{"id": "2602.22484", "categories": ["cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2602.22484", "abs": "https://arxiv.org/abs/2602.22484", "authors": ["K. Asakura", "K. Yamamoto", "A. Koga"], "title": "Impact of Stealthy Hyperuniform Magnetic Impurity Configurations on Bulk Magnetism in a Two-dimensional Heisenberg Model", "comment": "9 pages, 6 figures", "summary": "We investigate an antiferromagnetic quantum Heisenberg model on a square lattice with high-spin magnetic impurities to clarify how random and stealthy hyperuniform impurity configurations influence the bulk magnetic properties. Stealthy hyperuniform configurations are generated using generalized cost functions that interpolate between square-lattice-like and triangular-lattice-like arrangements. Using linear spin-wave theory for the mixed-spin model, we demonstrate that triangular-lattice-like arrangements yield a larger average staggered magnetization than both random and square-lattice-like cases. This enhancement originates from sublattice effects: while the square-lattice-like configuration enforces nearest-neighbor impurities to occupy opposite sublattices due to its bipartite structure, the triangular-lattice-like arrangement allows same-sublattice nearest-neighbor pairs, thereby strengthening cooperative magnetic enhancement.", "AI": {"tldr": "\u7814\u7a76\u9ad8\u81ea\u65cb\u78c1\u6027\u6742\u8d28\u5728\u53cd\u94c1\u78c1\u91cf\u5b50\u6d77\u68ee\u5821\u6a21\u578b\u4e2d\u7684\u6392\u5217\u65b9\u5f0f\u5bf9\u4f53\u78c1\u6027\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u7c7b\u4e09\u89d2\u6676\u683c\u6392\u5217\u6bd4\u968f\u673a\u548c\u7c7b\u6b63\u65b9\u6676\u683c\u6392\u5217\u4ea7\u751f\u66f4\u5927\u7684\u4ea4\u9519\u78c1\u5316\u5f3a\u5ea6\uff0c\u8fd9\u6e90\u4e8e\u4e9a\u6676\u683c\u6548\u5e94", "motivation": "\u63a2\u7a76\u968f\u673a\u548c\u9690\u8eab\u8d85\u5747\u5300\u6742\u8d28\u6784\u578b\u5982\u4f55\u5f71\u54cd\u5757\u72b6\u78c1\u6027\u6027\u8d28", "method": "\u4f7f\u7528\u7ebf\u6027\u81ea\u65cb\u6ce2\u7406\u8bba\u7814\u7a76\u6df7\u5408\u81ea\u65cb\u6a21\u578b\uff0c\u901a\u8fc7\u5e7f\u4e49\u6210\u672c\u51fd\u6570\u751f\u6210\u5728\u6b63\u65b9\u6676\u683c\u548c\u4e09\u89d2\u6676\u683c\u6392\u5217\u4e4b\u95f4\u63d2\u503c\u7684\u9690\u8eab\u8d85\u5747\u5300\u6784\u578b", "result": "\u7c7b\u4e09\u89d2\u6676\u683c\u6392\u5217\u6bd4\u968f\u673a\u548c\u7c7b\u6b63\u65b9\u6676\u683c\u60c5\u51b5\u4ea7\u751f\u66f4\u5927\u7684\u5e73\u5747\u4ea4\u9519\u78c1\u5316\u5f3a\u5ea6\uff0c\u8fd9\u6e90\u4e8e\u4e9a\u6676\u683c\u6548\u5e94\uff1a\u6b63\u65b9\u6676\u683c\u7ed3\u6784\u8feb\u4f7f\u6700\u8fd1\u90bb\u6742\u8d28\u5360\u636e\u76f8\u53cd\u4e9a\u6676\u683c\uff0c\u800c\u4e09\u89d2\u6676\u683c\u6392\u5217\u5141\u8bb8\u540c\u4e9a\u6676\u683c\u6700\u8fd1\u90bb\u5bf9", "conclusion": "\u4e09\u89d2\u6676\u683c\u72b6\u6392\u5217\u901a\u8fc7\u5141\u8bb8\u540c\u4e9a\u6676\u683c\u6700\u8fd1\u90bb\u5bf9\u6765\u589e\u5f3a\u534f\u540c\u78c1\u589e\u5f3a\uff0c\u4ece\u800c\u83b7\u5f97\u66f4\u5927\u7684\u4ea4\u9519\u78c1\u5316\u5f3a\u5ea6"}}
{"id": "2602.22657", "categories": ["cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2602.22657", "abs": "https://arxiv.org/abs/2602.22657", "authors": ["Hidetoshi Nishimori"], "title": "Exact mapping of a spin glass with correlated disorder to the pure Ising model", "comment": "9 pages", "summary": "We introduce an Ising spin-glass model with correlated disorder which continuously interpolates between the pure ferromagnetic Ising model and the Edwards-Anderson model with symmetric disorder. For this model, we prove that physical quantities on the Nishimori line (NL) can be expressed exactly in terms of those of the pure Ising model at an effective temperature on any lattice in any dimension. For example, the energy on the NL is equal to the energy of the pure Ising model at the effective temperature up to a constant and a trivial factor. More remarkably, the specific heat on the NL equals the energy, not the specific heat, of the pure Ising model at the effective temperature, again up to a constant and a trivial factor. Gauge-noninvariant quantities such as the magnetization and correlation functions are exactly equal to the corresponding quantities of the pure Ising model at the effective temperature. These exact relations imply that the leading critical behavior at that multicritical point for the disorder-correlated model is pure-Ising-like, in contrast to the conventional multicritical universality class of the standard Edwards-Anderson model. Our results motivate further investigations of the relatively unexplored topic of correlations in disorder in spin glasses and related problems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u76f8\u5173\u65e0\u5e8f\u7684\u4f0a\u8f9b\u81ea\u65cb\u73bb\u7483\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u5728Nishimori\u7ebf\u4e0a\u7269\u7406\u91cf\u53ef\u7cbe\u786e\u8868\u793a\u4e3a\u7eaf\u4f0a\u8f9b\u6a21\u578b\u5728\u6709\u6548\u6e29\u5ea6\u4e0b\u7684\u5bf9\u5e94\u91cf\uff0c\u63ed\u793a\u4e86\u4e34\u754c\u884c\u4e3a\u5448\u73b0\u7eaf\u4f0a\u8f9b\u666e\u9002\u7c7b\u800c\u975e\u4f20\u7edf\u591a\u4e34\u754c\u884c\u4e3a\u3002", "motivation": "\u7814\u7a76\u81ea\u65cb\u73bb\u7483\u4e2d\u65e0\u5e8f\u5173\u8054\u6027\u7684\u5f71\u54cd\uff0c\u6311\u6218\u6807\u51c6Edwards-Anderson\u6a21\u578b\u7684\u591a\u4e34\u754c\u666e\u9002\u7c7b\u7406\u8bba\uff0c\u63a2\u7d22\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u65e0\u5e8f\u76f8\u5173\u6027\u5728\u81ea\u65cb\u73bb\u7483\u53ca\u76f8\u5173\u95ee\u9898\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u5728\u7eaf\u94c1\u78c1\u4f0a\u8f9b\u6a21\u578b\u548c\u5bf9\u79f0\u65e0\u5e8fEdwards-Anderson\u6a21\u578b\u4e4b\u95f4\u8fde\u7eed\u63d2\u503c\u7684\u81ea\u65cb\u73bb\u7483\u6a21\u578b\uff0c\u5229\u7528\u89c4\u8303\u5bf9\u79f0\u6027\u5206\u6790\uff0c\u4e25\u683c\u8bc1\u660eNishimori\u7ebf\u4e0a\u7269\u7406\u91cf\u4e0e\u7eaf\u4f0a\u8f9b\u6a21\u578b\u7684\u7cbe\u786e\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "\u5728Nishimori\u7ebf\u4e0a\uff1a1) \u80fd\u91cf\u7b49\u4e8e\u7eaf\u4f0a\u8f9b\u6a21\u578b\u5728\u6709\u6548\u6e29\u5ea6\u4e0b\u7684\u80fd\u91cf\uff1b2) \u6bd4\u70ed\u7b49\u4e8e\u7eaf\u4f0a\u8f9b\u6a21\u578b\u5728\u76f8\u540c\u6e29\u5ea6\u4e0b\u7684\u80fd\u91cf\u800c\u975e\u6bd4\u70ed\uff1b3) \u78c1\u5316\u5f3a\u5ea6\u548c\u5173\u8054\u51fd\u6570\u4e0e\u7eaf\u4f0a\u8f9b\u6a21\u578b\u5728\u6709\u6548\u6e29\u5ea6\u4e0b\u4e25\u683c\u76f8\u7b49\uff1b4) \u591a\u4e34\u754c\u70b9\u7684\u4e3b\u5bfc\u4e34\u754c\u884c\u4e3a\u5448\u73b0\u7eaf\u4f0a\u8f9b\u666e\u9002\u7c7b\u7279\u5f81\u3002", "conclusion": "\u65e0\u5e8f\u76f8\u5173\u6027\u5bfc\u81f4\u8be5\u6a21\u578b\u7684\u591a\u4e34\u754c\u70b9\u666e\u9002\u7c7b\u4e0e\u6807\u51c6Edwards-Anderson\u6a21\u578b\u4e0d\u540c\uff0c\u8868\u73b0\u4e3a\u7eaf\u4f0a\u8f9b\u578b\u4e34\u754c\u884c\u4e3a\uff0c\u8fd9\u4e00\u53d1\u73b0\u4e3a\u81ea\u65cb\u73bb\u7483\u4e2d\u65e0\u5e8f\u5173\u8054\u6027\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u5e76\u6fc0\u52b1\u4e86\u76f8\u5173\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002"}}
{"id": "2602.23272", "categories": ["nlin.CD"], "pdf": "https://arxiv.org/pdf/2602.23272", "abs": "https://arxiv.org/abs/2602.23272", "authors": ["Meiyazhagan Jaganathan", "Vikram Pakrashi", "Aasifa Rounak"], "title": "Interplay of Nonsmoothness, Time Delay, and Stochasticity in Turning Dynamics", "comment": null, "summary": "The stochastic dynamics of orthogonal metal cutting with both regenerative and nonsmooth frictional effects are investigated numerically in this paper. The shortcomings of neglecting nonsmoothness in frictional and stochastic effects in modeling the dynamics of such a machining process are demonstrated. Dynamics of the tool motion is observed to exhibit rich nonlinear phenomena such as stick-slip during chatter, with stochastic perturbations in cutting forces adding further complexity, leading to the occurrence of stochastic P and D bifurcations. Measures of entropy are found to be effective in quantifying the dynamical transitions occurring in the dynamics of the tool. Subsequently, basin stability analyses, modified to account for stochasticity and time-delays, are carried out to systematically investigate the dynamics of the cutting tool across multiple surface roughness profiles of the workpiece. Basin stability analyses indicate that chatter can be controlled by restricting initial tool displacement and controlling initial workpiece surface roughness, suggesting practical strategies to improve machining outcomes for precision manufacturing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u6570\u503c\u65b9\u6cd5\u7814\u7a76\u6b63\u4ea4\u91d1\u5c5e\u5207\u524a\u7684\u968f\u673a\u52a8\u529b\u5b66\uff0c\u8bc1\u660e\u5ffd\u7565\u975e\u5149\u6ed1\u6469\u64e6\u548c\u968f\u673a\u6548\u5e94\u7684\u6a21\u578b\u5b58\u5728\u7f3a\u9677\uff0c\u53d1\u73b0\u968f\u673a\u6270\u52a8\u4f1a\u5bfc\u81f4P/D\u5206\u5c94\uff0c\u53ef\u7528\u71b5\u5ea6\u91cf\u91cf\u5316\u8f6c\u53d8\uff0c\u5e76\u901a\u8fc7\u6539\u8fdb\u7684\u6d41\u57df\u7a33\u5b9a\u6027\u5206\u6790\u63ed\u793a\u901a\u8fc7\u63a7\u5236\u521d\u59cb\u4f4d\u79fb\u548c\u5de5\u4ef6\u7c97\u7cd9\u5ea6\u6765\u6291\u5236\u98a4\u632f\u7684\u5b9e\u7528\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u91d1\u5c5e\u5207\u524a\u52a8\u529b\u5b66\u6a21\u578b\u5e38\u5ffd\u7565\u975e\u5149\u6ed1\u6469\u64e6\u6548\u5e94\u548c\u968f\u673a\u6270\u52a8\uff0c\u5bfc\u81f4\u5bf9\u98a4\u632f\u7b49\u590d\u6742\u975e\u7ebf\u6027\u73b0\u8c61\u7684\u9884\u6d4b\u4e0d\u51c6\u786e\uff0c\u5f71\u54cd\u7cbe\u5bc6\u5236\u9020\u8d28\u91cf\u3002\u672c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u8fd9\u4e9b\u88ab\u5ffd\u7565\u56e0\u7d20\u7684\u5b9e\u9645\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u6570\u503c\u4eff\u771f\u5efa\u7acb\u5305\u542b\u518d\u751f\u6548\u5e94\u548c\u975e\u5149\u6ed1\u6469\u64e6\u7684\u968f\u673a\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u8fd0\u7528\u71b5\u5ea6\u91cf\u91cf\u5316\u52a8\u6001\u8f6c\u53d8\uff0c\u5f00\u5c55\u9488\u5bf9\u968f\u673a\u6027\u548c\u65f6\u6ede\u7684\u6539\u8fdb\u6d41\u57df\u7a33\u5b9a\u6027\u5206\u6790\uff0c\u7cfb\u7edf\u7814\u7a76\u4e0d\u540c\u5de5\u4ef6\u8868\u9762\u7c97\u7cd9\u5ea6\u4e0b\u7684\u5200\u5177\u52a8\u6001\u884c\u4e3a\u3002", "result": "\u5200\u5177\u8fd0\u52a8\u5448\u73b0\u4e30\u5bcc\u7684\u975e\u7ebf\u6027\u73b0\u8c61\uff08\u5982\u98a4\u632f\u4e2d\u7684\u7c98\u6ed1\u8fd0\u52a8\uff09\uff0c\u968f\u673a\u6270\u52a8\u5bfc\u81f4\u968f\u673aP/D\u5206\u5c94\uff1b\u71b5\u5ea6\u91cf\u80fd\u6709\u6548\u6355\u6349\u7cfb\u7edf\u8f6c\u53d8\uff1b\u6d41\u57df\u7a33\u5b9a\u6027\u5206\u6790\u8868\u660e\u53ef\u901a\u8fc7\u9650\u5236\u521d\u59cb\u5200\u5177\u4f4d\u79fb\u548c\u63a7\u5236\u5de5\u4ef6\u521d\u59cb\u7c97\u7cd9\u5ea6\u6765\u6291\u5236\u98a4\u632f\u3002", "conclusion": "\u975e\u5149\u6ed1\u6469\u64e6\u548c\u968f\u673a\u6548\u5e94\u5fc5\u987b\u7eb3\u5165\u5207\u524a\u52a8\u529b\u5b66\u6a21\u578b\uff1b\u71b5\u662f\u91cf\u5316\u7cfb\u7edf\u590d\u6742\u6027\u7684\u6709\u6548\u5de5\u5177\uff1b\u901a\u8fc7\u8c03\u63a7\u521d\u59cb\u6761\u4ef6\u53ef\u5b9e\u73b0\u98a4\u632f\u63a7\u5236\uff0c\u4e3a\u7cbe\u5bc6\u5236\u9020\u63d0\u4f9b\u5b9e\u7528\u7b56\u7565\uff0c\u6539\u5584\u52a0\u5de5\u8d28\u91cf\u3002"}}
{"id": "2602.22310", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.22310", "abs": "https://arxiv.org/abs/2602.22310", "authors": ["Pedro M. C\u00f4nsoli", "Ezra Day-Roberts", "Johannes Knolle", "Antia S. Botana", "Onur Erten"], "title": "Tuning the magnetic properties of Kitaev materials via the antiferromagnetic proximity effect: Novel phases and application to an $\u03b1$-RuCl$_3$/MnPS$_3$ bilayer", "comment": "20+9 pages (single-column formatting), 5+3 figures", "summary": "In recent years, the increasing level of control over van der Waals (vdW) heterostructures has opened new routes to tune the properties of quantum materials. Motivated by these developments, we examine the potential consequences of interfacing a Kitaev honeycomb magnet, such as $\u03b1$-RuCl$_3$, with a nearly lattice-matched vdW antiferromagnet. By combining perturbation theory, exact diagonalization, and a classical energy-minimization method, we show that an effective staggered magnetic field originating from the vdW antiferromagnet can drive a monolayer of a Kitaev material into various novel phases, including an antichiral Kitaev spin liquid, a nonmagnetic nematic phase, and different types of skyrmion crystals. We then apply first-principle simulations to assess the prospect of concretely realizing this setup in a heterobilayer of $\u03b1$-RuCl$_3$ and the easy-axis antiferromagnet MnPS$_3$.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u8303\u5fb7\u534e\u5f02\u8d28\u7ed3\u6784\u4e2d\uff0c\u5c06Kitaev\u8702\u7a9d\u78c1\u4f53\uff08\u5982\u03b1-RuCl\u2083\uff09\u4e0e\u6676\u683c\u5339\u914d\u7684\u8303\u5fb7\u534e\u53cd\u94c1\u78c1\u4f53\u7ed3\u5408\u65f6\uff0c\u7531\u53cd\u94c1\u78c1\u4f53\u4ea7\u751f\u7684\u6709\u6548\u4ea4\u9519\u78c1\u573a\u53ef\u9a71\u52a8Kitaev\u6750\u6599\u8fdb\u5165\u591a\u79cd\u65b0\u7269\u76f8\uff0c\u5305\u62ec\u53cd\u624b\u6027Kitaev\u81ea\u65cb\u6db2\u4f53\u3001\u975e\u78c1\u6027\u5411\u5217\u76f8\u548c\u4e0d\u540c\u7c7b\u578b\u7684\u65af\u683c\u660e\u5b50\u6676\u4f53\u3002\u901a\u8fc7\u7b2c\u4e00\u6027\u539f\u7406\u6a21\u62df\uff0c\u8bc4\u4f30\u4e86\u5728\u03b1-RuCl\u2083/MnPS\u2083\u5f02\u8d28\u53cc\u5c42\u4e2d\u5b9e\u73b0\u8be5\u4f53\u7cfb\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u5bf9\u8303\u5fb7\u534e\uff08vdW\uff09\u5f02\u8d28\u7ed3\u6784\u63a7\u5236\u6c34\u5e73\u7684\u63d0\u5347\u4e3a\u8c03\u63a7\u91cf\u5b50\u6750\u6599\u7279\u6027\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002\u53d7\u6b64\u542f\u53d1\uff0c\u672c\u7814\u7a76\u63a2\u7a76\u4e86\u5728Kitaev\u8702\u7a9d\u78c1\u4f53\u4e0e\u8fd1\u6676\u683c\u5339\u914d\u7684vdW\u53cd\u94c1\u78c1\u4f53\u754c\u9762\u5904\u53ef\u80fd\u4ea7\u751f\u7684\u65b0\u9896\u91cf\u5b50\u7269\u76f8\u3002", "method": "\u7ed3\u5408\u5fae\u6270\u7406\u8bba\u3001\u7cbe\u786e\u5bf9\u89d2\u5316\u65b9\u6cd5\u548c\u7ecf\u5178\u80fd\u91cf\u6700\u5c0f\u5316\u65b9\u6cd5\u8fdb\u884c\u7406\u8bba\u8ba1\u7b97\uff0c\u5e76\u5e94\u7528\u7b2c\u4e00\u6027\u539f\u7406\u6a21\u62df\u8bc4\u4f30\u5177\u4f53\u6750\u6599\u4f53\u7cfb\uff08\u03b1-RuCl\u2083\u548cMnPS\u2083\uff09\u7684\u5b9e\u73b0\u524d\u666f\u3002", "result": "\u53d1\u73b0\u6765\u81eavdW\u53cd\u94c1\u78c1\u4f53\u7684\u6709\u6548\u4ea4\u9519\u78c1\u573a\u53ef\u5c06Kitaev\u5355\u5c42\u9a71\u52a8\u81f3\u591a\u79cd\u65b0\u7269\u76f8\uff1a\u53cd\u624b\u6027Kitaev\u81ea\u65cb\u6db2\u4f53\u3001\u975e\u78c1\u6027\u5411\u5217\u76f8\u4ee5\u53ca\u4e0d\u540c\u7c7b\u578b\u7684\u65af\u683c\u660e\u5b50\u6676\u4f53\u3002", "conclusion": "\u8be5\u5f02\u8d28\u7ed3\u6784\u8bbe\u8ba1\uff08\u7279\u522b\u662f\u03b1-RuCl\u2083/MnPS\u2083\u53cc\u5c42\uff09\u4e3a\u5b9e\u9a8c\u5b9e\u73b0\u8fd9\u4e9b\u65b0\u9896\u91cf\u5b50\u7269\u76f8\u63d0\u4f9b\u4e86\u53ef\u884c\u9014\u5f84\uff0c\u5c55\u793a\u4e86\u8303\u5fb7\u534e\u5f02\u8d28\u7ed3\u6784\u5728\u8c03\u63a7\u91cf\u5b50\u6750\u6599\u7269\u6027\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2602.22227", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22227", "abs": "https://arxiv.org/abs/2602.22227", "authors": ["Yicheng Bao", "Xuhong Wang", "Xin Tan"], "title": "To Deceive is to Teach? Forging Perceptual Robustness via Adversarial Reinforcement Learning", "comment": null, "summary": "Despite their impressive capabilities, Multimodal Large Language Models (MLLMs) exhibit perceptual fragility when confronted with visually complex scenes. This weakness stems from a reliance on finite training datasets, which are prohibitively expensive to scale and impose a ceiling on model robustness. We introduce \\textbf{AOT-SFT}, a large-scale adversarial dataset for bootstrapping MLLM robustness. Building on this, we propose \\textbf{AOT (Adversarial Opponent Training)}, a self-play framework that forges MLLM robustness by creating its own training data. Our method orchestrates a co-evolution between an image-editing Attacker and a Defender MLLM, where the Attacker generates a diverse and dynamic curriculum of image manipulations, forcing the Defender to adapt and improve. Extensive experiments demonstrate that AOT enhances the Defender's perceptual robustness and reduces hallucinations, establishing a scalable paradigm for training more reliable MLLMs.", "AI": {"tldr": "This paper proposes AOT, a self-play framework with an image-editing Attacker and Defender MLLM to forge robustness through co-evolution, using AOT-SFT adversarial dataset.", "motivation": "Multimodal Large Language Models (MLLMs) show perceptual fragility in complex visual scenes, limited by finite and expensive-to-scale training datasets that cap model robustness.", "method": "Introduces AOT (Adversarial Opponent Training), a self-play co-evolution framework where an image-editing Attacker generates dynamic adversarial image manipulations that force the Defender MLLM to adapt and improve.", "result": "AOT significantly enhances the Defender's perceptual robustness and reduces hallucinations, demonstrating scalable MLLM training.", "conclusion": "The work establishes a scalable paradigm for training more reliable MLLMs by bootstrapping robustness through self-generated adversarial training data."}}
{"id": "2602.22372", "categories": ["physics.comp-ph", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.22372", "abs": "https://arxiv.org/abs/2602.22372", "authors": ["Gianluca Grosso", "Marc K. Ritter", "Stefan Rohshap", "Samuel Badr", "Anna Kauch", "Markus Wallerberger", "Jan von Delft", "Hiroshi Shinaoka"], "title": "Adaptive Patching for Tensor Train Computations", "comment": "38 pages, 21 figures, codes at https://tensor4all.org", "summary": "Quantics Tensor Train (QTT) operations such as matrix product operator contractions are prohibitively expensive for large bond dimensions. We propose an adaptive patching scheme that exploits block-sparse QTT structures to reduce costs through divide-and-conquer, adaptively partitioning tensors into smaller patches with reduced bond dimensions. We demonstrate substantial improvements for sharply localized functions and show efficient computation of bubble diagrams and Bethe-Salpeter equations, opening the door to practical large-scale QTT-based computations previously beyond reach.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u5206\u5757\u65b9\u6848\uff0c\u5229\u7528\u5757\u7a00\u758fQTT\u7ed3\u6784\u901a\u8fc7\u5206\u6cbb\u6cd5\u964d\u4f4e\u5927\u952e\u7ef4\u5ea6\u4e0b\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u5b9e\u73b0\u5c40\u90e8\u51fd\u6570\u663e\u8457\u52a0\u901f\u548c\u6c14\u6ce1\u56fe/BSE\u9ad8\u6548\u8ba1\u7b97\uff0c\u5f00\u542f\u5927\u89c4\u6a21QTT\u8ba1\u7b97\u65b0\u53ef\u80fd", "motivation": "\u5927\u952e\u7ef4\u5ea6\u4e0b\u91cf\u5b50\u5f20\u91cf\u5217\u8f66\uff08QTT\uff09\u64cd\u4f5c\uff08\u5982\u77e9\u9635\u4e58\u79ef\u7b97\u7b26\u6536\u7f29\uff09\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u96be\u4ee5\u8fdb\u884c\u5927\u89c4\u6a21\u8ba1\u7b97", "method": "\u5f00\u53d1\u81ea\u9002\u5e94\u5206\u5757\u65b9\u6848\uff0c\u5229\u7528\u5757\u7a00\u758fQTT\u7ed3\u6784\uff0c\u901a\u8fc7\u5206\u6cbb\u6cd5\u5c06\u5f20\u91cf\u81ea\u9002\u5e94\u5206\u5272\u4e3a\u952e\u7ef4\u5ea6\u66f4\u5c0f\u7684\u5b50\u5757", "result": "\u5bf9\u5c16\u9510\u5c40\u57df\u5316\u51fd\u6570\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u53ef\u9ad8\u6548\u8ba1\u7b97\u6c14\u6ce1\u56fe\u548cBethe-Salpeter\u65b9\u7a0b", "conclusion": "\u8be5\u65b9\u6cd5\u7a81\u7834\u4e86\u539f\u6709\u5927\u89c4\u6a21QTT\u8ba1\u7b97\u7684\u5c40\u9650\uff0c\u4e3a\u6b64\u524d\u65e0\u6cd5\u5b9e\u73b0\u7684\u5b9e\u9645\u5927\u89c4\u6a21QTT\u8ba1\u7b97\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84"}}
{"id": "2602.22215", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.22215", "abs": "https://arxiv.org/abs/2602.22215", "authors": ["Pengzhen Xie", "Huizhi Liang"], "title": "Graph Your Way to Inspiration: Integrating Co-Author Graphs with Retrieval-Augmented Generation for Large Language Model Based Scientific Idea Generation", "comment": "15 pages, 10 figures. Submitted to [RAAI]", "summary": "Large Language Models (LLMs) demonstrate potential in the field of scientific idea generation. However, the generated results often lack controllable academic context and traceable inspiration pathways. To bridge this gap, this paper proposes a scientific idea generation system called GYWI, which combines author knowledge graphs with retrieval-augmented generation (RAG) to form an external knowledge base to provide controllable context and trace of inspiration path for LLMs to generate new scientific ideas. We first propose an author-centered knowledge graph construction method and inspiration source sampling algorithms to construct external knowledge base. Then, we propose a hybrid retrieval mechanism that is composed of both RAG and GraphRAG to retrieve content with both depth and breadth knowledge. It forms a hybrid context. Thirdly, we propose a Prompt optimization strategy incorporating reinforcement learning principles to automatically guide LLMs optimizing the results based on the hybrid context. To evaluate the proposed approaches, we constructed an evaluation dataset based on arXiv (2018-2023). This paper also develops a comprehensive evaluation method including empirical automatic assessment in multiple-choice question task, LLM-based scoring, human evaluation, and semantic space visualization analysis. The generated ideas are evaluated from the following five dimensions: novelty, feasibility, clarity, relevance, and significance. We conducted experiments on different LLMs including GPT-4o, DeepSeek-V3, Qwen3-8B, and Gemini 2.5. Experimental results show that GYWI significantly outperforms mainstream LLMs in multiple metrics such as novelty, reliability, and relevance.", "AI": {"tldr": "GYWI\u7cfb\u7edf\u7ed3\u5408\u4f5c\u8005\u77e5\u8bc6\u56fe\u8c31\u4e0eRAG\u6280\u672f\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u751f\u6210\u79d1\u5b66\u521b\u610f\u7684\u521b\u65b0\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u521b\u610f\u751f\u6210\u9886\u57df\u5c55\u73b0\u6f5c\u529b\uff0c\u4f46\u751f\u6210\u7684\u7ed3\u679c\u5f80\u5f80\u7f3a\u4e4f\u53ef\u63a7\u7684\u5b66\u672f\u80cc\u666f\u548c\u8ffd\u6eaf\u7684\u7075\u611f\u8def\u5f84\u3002", "method": "\u63d0\u51faGYWI\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f5c\u8005\u4e2d\u5fc3\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u548c\u7075\u611f\u6e90\u91c7\u6837\u7b97\u6cd5\u5efa\u7acb\u5916\u90e8\u77e5\u8bc6\u5e93\uff1b\u91c7\u7528RAG\u4e0eGraphRAG\u6df7\u5408\u68c0\u7d22\u673a\u5236\uff1b\u5f15\u5165\u5f3a\u5316\u5b66\u4e60\u539f\u7406\u4f18\u5316Prompt\u7b56\u7565\uff1b\u5728arXiv(2018-2023)\u6570\u636e\u96c6\u4e0a\u901a\u8fc7\u591a\u9009\u9898\u4efb\u52a1\u3001LLM\u8bc4\u5206\u3001\u4eba\u5de5\u8bc4\u4f30\u548c\u8bed\u4e49\u7a7a\u95f4\u53ef\u89c6\u5316\u8fdb\u884c\u4e94\u7ef4\u5ea6\u8bc4\u4f30(\u65b0\u9896\u6027\u3001\u53ef\u884c\u6027\u3001\u6e05\u6670\u5ea6\u3001\u76f8\u5173\u6027\u3001\u91cd\u8981\u6027)\u3002", "result": "\u5728GPT-4o\u3001DeepSeek-V3\u3001Qwen3-8B\u548cGemini 2.5\u7b49\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGYWI\u5728\u521b\u65b0\u6027\u3001\u53ef\u9760\u6027\u548c\u76f8\u5173\u6027\u7b49\u591a\u4e2a\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "GYWI\u7cfb\u7edf\u901a\u8fc7\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u4e0e\u68c0\u7d22\u589e\u5f3a\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u79d1\u5b66\u521b\u610f\u751f\u6210\u4e2d\u80cc\u666f\u4e0d\u53ef\u63a7\u548c\u8def\u5f84\u4e0d\u53ef\u8ffd\u6eaf\u7684\u95ee\u9898\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2602.22257", "categories": ["physics.data-an", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2602.22257", "abs": "https://arxiv.org/abs/2602.22257", "authors": ["Griffin M Kearney", "Kasey M Laurent", "Makan Fardad"], "title": "Maximum Likelihood Particle Tracking in Turbulent Flows via Sparse Optimization", "comment": null, "summary": "Lagrangian particle tracking is essential for characterizing turbulent flows, but inferring particle acceleration from inherently noisy position data remains a significant challenge. Fluid particles in turbulence experience extreme, intermittent accelerations, resulting in heavy-tailed probability density functions (PDFs) that deviate strongly from Gaussian predictions. Existing filtering techniques, such as Gaussian kernels and penalized B-splines, implicitly assume Gaussian-distributed jerk, thereby penalizing sparse, high-magnitude acceleration changes and artificially suppressing the intermittent tails. In this work, we develop a novel maximum likelihood estimation (MLE) framework that explicitly accounts for this non-Gaussian intermittency. By formulating a modified Gaussian process to model the random incremental forcing, we introduce a sparse optimization scheme utilizing a convex 1-norm relaxation. To overcome the numerical stiffness associated with high-order difference operators, the problem is efficiently solved using an iteratively reweighted least squares (IRLS) algorithm. The proposed filter is evaluated against direct numerical simulation (DNS) data of homogeneous, isotropic turbulence (Re approx. 310). Results demonstrate that the IRLS approach consistently outperforms state-of-the-art discrete MLE, continuous MLE, and B-spline methods, yielding systematic reductions in root-mean-squared error (RMSE) across position, velocity, and acceleration. Most importantly, the proposed framework succeeds in better recovering the heavy-tailed statistical structure of both acceleration and acceleration differences (jerk) across temporal scales, preserving the physical intermittency characteristic of high-Reynoldsnumber turbulent flows that baseline methods severely attenuate.", "AI": {"tldr": "\u9488\u5bf9\u6e4d\u6d41\u4e2d\u62c9\u683c\u6717\u65e5\u7c92\u5b50\u52a0\u901f\u5ea6\u63a8\u65ad\u96be\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u663e\u5f0f\u8003\u8651\u975e\u9ad8\u65af\u95f4\u6b47\u6027\u7684\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u7a00\u758f\u4f18\u5316\u548c\u8fed\u4ee3\u91cd\u52a0\u6743\u6700\u5c0f\u4e8c\u4e58\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u52a0\u901f\u5ea6\u7edf\u8ba1\u7279\u6027\u6062\u590d\u80fd\u529b\uff0c\u4fdd\u7559\u6e4d\u6d41\u7269\u7406\u95f4\u6b47\u6027\u3002", "motivation": "\u73b0\u6709\u6ee4\u6ce2\u65b9\u6cd5\uff08\u9ad8\u65af\u6838\u3001\u60e9\u7f5aB\u6837\u6761\uff09\u9690\u542b\u5047\u8bbe\u52a0\u52a0\u901f\u5ea6\u4e3a\u9ad8\u65af\u5206\u5e03\uff0c\u5bfc\u81f4\u7a00\u758f\u9ad8\u5e45\u503c\u52a0\u901f\u5ea6\u53d8\u5316\u88ab\u6291\u5236\uff0c\u65e0\u6cd5\u6062\u590d\u6e4d\u6d41\u4e2d\u91cd\u5c3e\u5206\u5e03\u7684\u975e\u9ad8\u65af\u95f4\u6b47\u6027\u7279\u5f81\u3002", "method": "\u6784\u5efa\u6539\u8fdb\u9ad8\u65af\u8fc7\u7a0b\u5efa\u6a21\u968f\u673a\u589e\u91cf\u5f3a\u8feb\uff0c\u5f15\u5165\u51f81-\u8303\u6570\u677e\u5f1b\u7684\u7a00\u758f\u4f18\u5316\u65b9\u6848\uff0c\u91c7\u7528\u8fed\u4ee3\u91cd\u52a0\u6743\u6700\u5c0f\u4e8c\u4e58\u6cd5(IRLS)\u6c42\u89e3\u9ad8\u9636\u68af\u5ea6\u7b97\u5b50\u6570\u503c\u521a\u6027\u95ee\u9898\u3002", "result": "\u5728Re\u2248310\u5404\u5411\u540c\u6027\u6e4d\u6d41DNS\u6570\u636e\u6d4b\u8bd5\u4e2d\uff0c\u65b0\u65b9\u6cd5\u5728\u4f4d\u7f6e/\u901f\u5ea6/\u52a0\u901f\u5ea6\u7684\u5747\u65b9\u6839\u8bef\u5dee\u4e0a\u5168\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5173\u952e\u6210\u529f\u6062\u590d\u4e86\u52a0\u901f\u5ea6\u548c\u52a0\u52a0\u901f\u5ea6\u8de8\u65f6\u95f4\u5c3a\u5ea6\u7684\u91cd\u5c3e\u7edf\u8ba1\u7ed3\u6784\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u4fdd\u7559\u4e86\u9ad8\u96f7\u8bfa\u6570\u6e4d\u6d41\u7684\u7269\u7406\u95f4\u6b47\u6027\u7279\u5f81\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u95f4\u6b47\u6027\u5c3e\u90e8\u7684\u4e25\u91cd\u8870\u51cf\u95ee\u9898\uff0c\u4e3a\u6e4d\u6d41\u7cbe\u7ec6\u7ed3\u6784\u7814\u7a76\u63d0\u4f9b\u65b0\u5de5\u5177\u3002"}}
{"id": "2602.22530", "categories": ["cs.CC", "cs.CL", "math-ph", "math.DS", "math.HO"], "pdf": "https://arxiv.org/pdf/2602.22530", "abs": "https://arxiv.org/abs/2602.22530", "authors": ["Michael Stephen Fiske"], "title": "Dynamic Level Sets", "comment": "7 pages", "summary": "A mathematical concept is identified and analyzed that is implicit in the 2012 paper Turing Incomputable Computation, presented at the Alan Turing Centenary Conference (Turing 100, Manchester). The concept, called dynamic level sets, is distinct from mathematical concepts in the standard literature on dynamical systems, topology, and computability theory. A new mathematical object is explained and why it may have escaped prior characterizations, including the classical result of de Leeuw, Moore, Shannon, and Shapiro (1956) that probabilistic Turing machines compute no more than deterministic ones.", "AI": {"tldr": "The paper identifies and analyzes a novel mathematical concept called \"dynamic level sets\" implicit in a 2012 Turing Centenary Conference paper, explaining why this concept eluded prior characterization, including classical results on the computational equivalence of probabilistic and deterministic Turing machines.", "motivation": "To explicate a mathematical concept that was implicit but not explicitly identified in prior work on Turing incomputable computation, and to understand why it was overlooked despite extensive study in dynamical systems, topology, and computability theory.", "method": "Conceptual analysis of existing literature to identify an implicit mathematical structure, with examination of its relationship to classical computability theory results.", "result": "Dynamic level sets constitute a distinct mathematical object not previously characterized in standard literature, suggesting that certain computational phenomena may have been missed by classical frameworks.", "conclusion": "The discovery of dynamic level sets reveals a potential limitation in classical computability theory and opens new avenues for understanding computational processes that may circumvent traditional boundaries between probabilistic and deterministic computation."}}
{"id": "2602.22331", "categories": ["quant-ph", "cond-mat.dis-nn", "hep-th"], "pdf": "https://arxiv.org/pdf/2602.22331", "abs": "https://arxiv.org/abs/2602.22331", "authors": ["Priesh Roy", "Sumilan Banerjee"], "title": "Schwinger-Keldysh field theory for operator R\u00e9nyi entropy and entanglement growth in non-interacting systems with sub-ballistic transports", "comment": "18 pages, 18 figures", "summary": "The notion of operator growth in quantum systems furnishes a bridge between transport and the generation of entanglement between different parts of the system under quantum dynamics. We define a measure of operator growth in terms of subsystem operator R\u00e9nyi entropy, which provides a state-independent measure of operator growth, unlike entanglement entropies, and the usual measures of operator growth like out-of-time-order correlators. We show that the subsystem operator R\u00e9nyi entropy encodes both spatial and temporal information, and thus can directly connect to transport for a local operator related to a conserved quantity. We construct a unified Schwinger-Keldysh (SK) field theory formalism for the time evolution of operator R\u00e9nyi entropy and entanglement entropies of initial pure states. We use the SK field theory to obtain the operator R\u00e9nyi and state entanglement entropies in terms of infinite-temperature and vacuum Keldysh Green's functions, respectively, for non-interacting systems. We apply the method to explore the connection between operator and entanglement growth, and transport in non-interacting systems with quasiperiodic and random disorder, like the one- and two-dimensional Aubry-Andr\u00e9 models and the two-dimensional Anderson model. In particular, we show that the growth of subsystem operator R\u00e9nyi entropy and state von Neumann and R\u00e9nyi entanglement entropies can capture both ballistic and sub-ballistic transport behaviors, like diffusive and anomalous diffusive transport, as well as localization in these systems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5b50\u7cfb\u7edf\u7b97\u7b26\u96f7\u5c3c\u71b5\u7684\u91cf\u5b50\u7b97\u7b26\u589e\u957f\u5ea6\u91cf\uff0c\u5efa\u7acb\u7b97\u7b26\u589e\u957f\u4e0e\u8f93\u8fd0\u884c\u4e3a\u7684\u76f4\u63a5\u8054\u7cfb\uff0c\u63ed\u793a\u65e0\u5e8f\u7cfb\u7edf\u4e2d\u7ea0\u7f20\u589e\u957f\u4e0e\u53cd\u5e38\u8f93\u8fd0\u7684\u7edf\u4e00\u63cf\u8ff0", "motivation": "\u4f20\u7edf\u7b97\u7b26\u589e\u957f\u5ea6\u91cf\uff08\u5982OTOC\uff09\u548c\u7ea0\u7f20\u71b5\u7f3a\u4e4f\u72b6\u6001\u65e0\u5173\u6027\uff0c\u4e14\u96be\u4ee5\u76f4\u63a5\u5173\u8054\u5c40\u57df\u5b88\u6052\u91cf\u7684\u65f6\u7a7a\u8f93\u8fd0\u884c\u4e3a\uff0c\u9700\u5efa\u7acb\u7edf\u4e00\u6846\u67b6\u8fde\u63a5\u91cf\u5b50\u52a8\u529b\u5b66\u4e2d\u7684\u7b97\u7b26\u589e\u957f\u3001\u7ea0\u7f20\u751f\u6210\u4e0e\u8f93\u8fd0\u73b0\u8c61", "method": "1. \u5b9a\u4e49\u5b50\u7cfb\u7edf\u7b97\u7b26\u96f7\u5c3c\u71b5\u4f5c\u4e3a\u72b6\u6001\u65e0\u5173\u7684\u7b97\u7b26\u589e\u957f\u5ea6\u91cf\uff1b2. \u6784\u5efa\u7edf\u4e00\u7684Schwinger-Keldysh\u573a\u8bba\u5f62\u5f0f\u5316\u4f53\u7cfb\uff0c\u5c06\u7b97\u7b26\u96f7\u5c3c\u71b5\u4e0e\u65e0\u9650\u6e29\u5ea6\u683c\u6797\u51fd\u6570\u3001\u771f\u7a7a\u7ea0\u7f20\u71b5\u4e0e\u57fa\u6001\u683c\u6797\u51fd\u6570\u5173\u8054\uff1b3. \u5e94\u7528\u4e8e\u51c6\u5468\u671f/\u65e0\u5e8f\u7cfb\u7edf\uff08AA\u6a21\u578b\u3001\u5b89\u5fb7\u68ee\u6a21\u578b\uff09", "result": "1. \u5b50\u7cfb\u7edf\u7b97\u7b26\u96f7\u5c3c\u71b5\u540c\u65f6\u7f16\u7801\u7a7a\u95f4\u4e0e\u65f6\u95f4\u4fe1\u606f\uff1b2. \u5728\u65e0\u5e8f\u7cfb\u7edf\u4e2d\uff0c\u7b97\u7b26\u96f7\u5c3c\u71b5\u3001\u51af\u8bfa\u4f9d\u66fc\u71b5\u53ca\u96f7\u5c3c\u71b5\u7684\u589e\u957f\u53ef\u5b9a\u91cf\u523b\u753b\u5f39\u9053\u8f93\u8fd0\u3001\u6269\u6563/\u53cd\u5e38\u6269\u6563\u53ca\u5c40\u57df\u5316\u884c\u4e3a\uff1b3. \u5b9e\u73b0\u7b97\u7b26\u589e\u957f\u3001\u7ea0\u7f20\u589e\u957f\u4e0e\u8f93\u8fd0\u52a8\u529b\u5b66\u7684\u7edf\u4e00\u63cf\u8ff0", "conclusion": "\u5b50\u7cfb\u7edf\u7b97\u7b26\u96f7\u5c3c\u71b5\u662f\u8fde\u63a5\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u7b97\u7b26\u52a8\u529b\u5b66\u3001\u7ea0\u7f20\u751f\u6210\u4e0e\u5b8f\u89c2\u8f93\u8fd0\u7684\u666e\u9002\u6307\u6807\uff0c\u4e3a\u7814\u7a76\u975e\u5e73\u8861\u91cf\u5b50\u8f93\u8fd0\u4e0e\u91cf\u5b50\u4fe1\u606f\u4f20\u64ad\u63d0\u4f9b\u65b0\u7406\u8bba\u5de5\u5177"}}
{"id": "2602.22317", "categories": ["quant-ph", "cond-mat.stat-mech", "nlin.CD"], "pdf": "https://arxiv.org/pdf/2602.22317", "abs": "https://arxiv.org/abs/2602.22317", "authors": ["Rohan Banerjee", "Shahyad Khamnei", "Anatoli Polkovnikov", "Stewart Morawetz"], "title": "Partial Reversibility and Counterdiabatic Driving in Nearly Integrable Systems", "comment": "10 pages, 6 figures", "summary": "Adiabatic (or reversible) processes are the key concept unifying our understanding of thermodynamics and dynamical systems. Reversibility in the thermodynamic sense is understood as entropy-preserving processes, such as in the idealized Carnot engine, whereas in integrable dynamical systems it is understood as the conservation of the action variables. Between these two idealized limits, however, where the phase space can become mixed, things are much less clear. In this work, we first determine the extent to which reversible processes are even possible in this regime. We then explore how the dissipative losses resulting from rapidly driving these kinds of systems can be fought by approximate counterdiabatic driving. Finally, we argue that much of the phenomenology should be the same for quantum many-body systems with large degeneracy in the presence of integrability breaking perturbations.", "AI": {"tldr": "\u7814\u7a76\u6df7\u5408\u76f8\u7a7a\u95f4\u4e2d\u53ef\u9006\u8fc7\u7a0b\u7684\u6781\u9650\uff0c\u63a2\u7d22\u53cd\u7edd\u70ed\u9a71\u52a8\u5bf9\u6297\u8017\u6563\u635f\u5931\uff0c\u5e76\u5c06\u73b0\u8c61\u63a8\u5e7f\u5230\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf", "motivation": "\u70ed\u529b\u5b66\u53ef\u9006\u6027\uff08\u71b5\u5b88\u6052\uff09\u4e0e\u52a8\u529b\u5b66\u53ef\u9006\u6027\uff08\u4f5c\u7528\u91cf\u5b88\u6052\uff09\u4e4b\u95f4\u5b58\u5728\u8ba4\u77e5\u9e3f\u6c9f\uff0c\u6df7\u5408\u76f8\u7a7a\u95f4\u4e2d\u53ef\u9006\u8fc7\u7a0b\u7684\u53ef\u884c\u6027\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u5efa\u7acb\u7edf\u4e00\u7406\u89e3\u6846\u67b6", "method": "\u7406\u8bba\u5206\u6790\u6df7\u5408\u76f8\u7a7a\u95f4\u53ef\u9006\u8fc7\u7a0b\u6781\u9650\uff0c\u7814\u7a76\u5feb\u901f\u9a71\u52a8\u4e0b\u7684\u8017\u6563\u635f\u5931\uff0c\u63d0\u51fa\u8fd1\u4f3c\u53cd\u7edd\u70ed\u9a71\u52a8\u65b9\u6848\uff0c\u5e76\u63a8\u5e7f\u81f3\u5177\u6709\u7b80\u5e76\u6027\u548c\u53ef\u79ef\u6027\u7834\u574f\u7684\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf", "result": "\u786e\u5b9a\u4e86\u6df7\u5408\u76f8\u7a7a\u95f4\u4e2d\u53ef\u9006\u8fc7\u7a0b\u7684\u53ef\u884c\u8303\u56f4\uff0c\u8bc1\u5b9e\u8fd1\u4f3c\u53cd\u7edd\u70ed\u9a71\u52a8\u80fd\u6709\u6548\u6291\u5236\u8017\u6563\u635f\u5931\uff0c\u8bba\u8bc1\u4e86\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u4e2d\u5b58\u5728\u76f8\u4f3c\u73b0\u8c61\u5b66\u7279\u5f81", "conclusion": "\u5b9e\u73b0\u4e86\u7ecf\u5178\u4e0e\u91cf\u5b50\u7cfb\u7edf\u53ef\u9006\u8fc7\u7a0b\u7684\u7406\u8bba\u7edf\u4e00\uff0c\u4e3a\u5728\u975e\u7406\u60f3\u6761\u4ef6\u4e0b\u7ef4\u6301\u7edd\u70ed\u6027\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u62d3\u5c55\u4e86\u7edd\u70ed\u70ed\u529b\u5b66\u4e0e\u52a8\u529b\u5b66\u7cfb\u7edf\u7684\u4ea4\u53c9\u7814\u7a76\u8fb9\u754c"}}
{"id": "2602.22878", "categories": ["cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.22878", "abs": "https://arxiv.org/abs/2602.22878", "authors": ["Debraj Das", "Andrea Gambassi", "Stefano Iubini", "Stefano Lepri"], "title": "Dephasing-induced relaxation in tight-binding chains with linear and nonlinear defects", "comment": "17 pages, 10 figures", "summary": "We investigate thermalization in a tight-binding chain with an on-site defect subject to local dephasing noise implemented as random phase kicks. For a single linear defect of strength $\u03b5$, we obtain an exact analytical description of the system spectrum and formulate the dephasing-induced dynamics in the eigenstate basis. We derive an approximate kinetic equation for mode populations that describes a continuous-time random walk in action space. The walk transition rates are defined by the overlap matrix encoding the spatial structure of eigenstates that can be computed exactly. Analyzing the spectral properties of the equation, we show that defect-induced localized modes act as bottlenecks that strongly slow down relaxation, with rates scaling as $\u03b5^{-2}$ for strong defects. Using large-deviation theory, we characterize rare dynamical trajectories and identify distinct relaxation pathways associated with low- and high-activity regimes in action space. We provide numerical evidence that the large-deviation function exhibits a dynamical phase transition in the limit $\u03b5\\to \\infty$. We then extend our analysis to the nonlinear case, considering a single nonlinear defect embedded in either a linear or a fully nonlinear discrete Schr\u00f6dinger equation. Numerical simulations reveal a qualitatively faster approach to equilibrium driven by the amplitude-dependent weakening of the defect. Our results provide a unified framework for understanding thermalization, rare fluctuations, and relaxation pathways in stochastic tight-binding systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u4e2a\u7d27\u675f\u7f1a\u94fe\u4e2d\u70ed\u5316\u3001\u7f55\u89c1\u6da8\u843d\u4e0e\u5f1b\u8c6b\u8def\u5f84\u7684\u7edf\u4e00\u7406\u8bba\u6846\u67b6\uff0c\u63ed\u793a\u5c40\u57df\u7f3a\u9677\u4f5c\u4e3a\u70ed\u5316\u74f6\u9888\u7684\u6807\u5ea6\u5f8b\u548c\u975e\u7ebf\u6027\u52a0\u901f\u673a\u5236", "motivation": "\u63a2\u7a76\u542b\u5c40\u57df\u7f3a\u9677\u548c\u9000\u76f8\u5e72\u566a\u58f0\u7684\u7d27\u675f\u7f1a\u7cfb\u7edf\u4e2d\u70ed\u5316\u52a8\u529b\u5b66\uff0c\u7279\u522b\u5173\u6ce8\u7f3a\u9677\u5982\u4f55\u5f71\u54cd\u5f1b\u8c6b\u8fc7\u7a0b\u3001\u7f55\u89c1\u8f68\u8ff9\u53ca\u4f5c\u7528\u7a7a\u95f4\u4e2d\u7684\u4e0d\u540c\u5f1b\u8c6b\u8def\u5f84", "method": "\u5bf9\u7ebf\u6027\u7f3a\u9677\uff1a\u6c42\u89e3\u7cbe\u786e\u80fd\u8c31\u5e76\u6784\u5efa\u672c\u5f81\u6001\u57fa\u4e0b\u7684\u52a8\u529b\u5b66\uff1b\u63a8\u5bfc\u6a21\u7c92\u5b50\u6570\u7684\u52a8\u529b\u5b66\u65b9\u7a0b\uff08\u4f5c\u7528\u7a7a\u95f4\u8fde\u7eed\u65f6\u95f4\u968f\u673a\u6e38\u8d70\uff09\uff0c\u5176\u8dc3\u8fc1\u7387\u7531\u672c\u5f81\u6001\u7a7a\u95f4\u7ed3\u6784\u7684\u91cd\u53e0\u77e9\u9635\u7cbe\u786e\u786e\u5b9a\uff1b\u7ed3\u5408\u5927\u504f\u5dee\u7406\u8bba\u5206\u6790\u7f55\u89c1\u8f68\u8ff9", "result": "1. \u5f3a\u7f3a\u9677\u4e0b\u5c40\u57df\u6a21\u6210\u4e3a\u5f1b\u8c6b\u74f6\u9888\uff0c\u5f1b\u8c6b\u901f\u7387\u6309\u03b5\u207b\u00b2\u6807\u5ea6\u51cf\u7f13\uff1b2. \u03b5\u2192\u221e\u65f6\u5927\u504f\u5dee\u51fd\u6570\u5448\u73b0\u52a8\u529b\u5b66\u76f8\u53d8\uff1b3. \u975e\u7ebf\u6027\u7f3a\u9677\uff08\u65e0\u8bba\u5d4c\u5165\u7ebf\u6027\u6216\u5168\u975e\u7ebf\u6027\u4f53\u7cfb\uff09\u56e0\u632f\u5e45\u4f9d\u8d56\u7684\u7f3a\u9677\u51cf\u5f31\u6548\u5e94\u800c\u663e\u8457\u52a0\u901f\u70ed\u5316", "conclusion": "\u4e3a\u968f\u673a\u7d27\u675f\u7f1a\u7cfb\u7edf\u7684\u70ed\u5316\u673a\u5236\u3001\u7f55\u89c1\u6da8\u843d\u53ca\u591a\u8def\u5f84\u5f1b\u8c6b\u63d0\u4f9b\u4e86\u666e\u9002\u7406\u8bba\u63cf\u8ff0\uff0c\u63ed\u793a\u4e86\u7f3a\u9677\u5f3a\u5ea6\u4e0e\u975e\u7ebf\u6027\u7684\u5173\u952e\u8c03\u63a7\u4f5c\u7528"}}
{"id": "2602.22228", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22228", "abs": "https://arxiv.org/abs/2602.22228", "authors": ["Jiyeong Kim", "Stephen P. Ma", "Nirali Vora", "Nicholas W. Larsen", "Julia Adler-Milstein", "Jonathan H. Chen", "Selen Bozkurt", "Abeed Sarker", "Juhee Cho", "Jindeok Joo", "Natali Pageler", "Fatima Rodriguez", "Christopher Sharp", "Eleni Linos"], "title": "Patient-Centered, Graph-Augmented Artificial Intelligence-Enabled Passive Surveillance for Early Stroke Risk Detection in High-Risk Individuals", "comment": null, "summary": "Stroke affected millions annually, yet poor symptom recognition often delayed care-seeking. To address risk recognition gap, we developed a passive surveillance system for early stroke risk detection using patient-reported symptoms among individuals with diabetes. Constructing a symptom taxonomy grounded in patients own language and a dual machine learning pipeline (heterogeneous GNN and EN/LASSO), we identified symptom patterns associated with subsequent stroke. We translated findings into a hybrid risk screening system integrating symptom relevance and temporal proximity, evaluated across 3-90 day windows through EHR-based simulations. Under conservative thresholds, intentionally designed to minimize false alerts, the screening system achieved high specificity (1.00) and prevalence-adjusted positive predictive value (1.00), with good sensitivity (0.72), an expected trade-off prioritizing precision, that was highest in 90-day window. Patient-reported language alone supported high-precision, low-burden early stroke risk detection, that could offer a valuable time window for clinical evaluation and intervention for high-risk individuals.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u60a3\u8005\u62a5\u544a\u8bed\u8a00\u7684\u88ab\u52a8\u76d1\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u53cc\u673a\u5668\u5b66\u4e60\u7ba1\u9053\u8bc6\u522b\u7cd6\u5c3f\u75c5\u60a3\u8005\u7684\u8111\u5352\u4e2d\u524d\u9a71\u75c7\u72b6\u6a21\u5f0f\uff0c\u5728\u4fdd\u5b88\u9608\u503c\u4e0b\u5b9e\u73b0\u9ad8\u7279\u5f02\u6027(1.00)\u548c\u9633\u6027\u9884\u6d4b\u503c(1.00)\uff0c\u4e3a\u9ad8\u98ce\u9669\u4eba\u7fa4\u521b\u9020\u4e34\u5e8a\u5e72\u9884\u65f6\u95f4\u7a97", "motivation": "\u8111\u5352\u4e2d\u6bcf\u5e74\u5f71\u54cd\u6570\u767e\u4e07\u4eba\uff0c\u4f46\u75c7\u72b6\u8bc6\u522b\u4e0d\u8db3\u5bfc\u81f4\u5c31\u533b\u5ef6\u8fdf\uff1b\u9488\u5bf9\u7cd6\u5c3f\u75c5\u60a3\u8005\u8fd9\u4e00\u9ad8\u98ce\u9669\u7fa4\u4f53\uff0c\u9700\u89e3\u51b3\u65e9\u671f\u98ce\u9669\u8bc6\u522b\u7f3a\u53e3\u4ee5\u7f29\u77ed\u6551\u6cbb\u65f6\u95f4\u7a97", "method": "\u6784\u5efa\u57fa\u4e8e\u60a3\u8005\u539f\u751f\u8bed\u8a00\u7684\u75c7\u58ee taxonomy\uff0c\u91c7\u7528\u5f02\u6784GNN\u4e0eEN/LASSO\u53cc\u673a\u5668\u5b66\u4e60\u7ba1\u9053\u8bc6\u522b\u75c7\u72b6\u6a21\u5f0f\uff1b\u5f00\u53d1\u878d\u5408\u75c7\u72b6\u76f8\u5173\u6027\u548c\u65f6\u95f4\u4e34\u8fd1\u6027\u7684\u6df7\u5408\u7b5b\u67e5\u7cfb\u7edf\uff0c\u901a\u8fc7\u7535\u5b50\u75c5\u5386\u6a21\u62df\u8bc4\u4f303-90\u5929\u7a97\u53e3\u671f", "result": "\u5728\u4fdd\u5b88\u9608\u503c\u4e0b\uff08\u6700\u5c0f\u5316\u5047\u8b66\u62a5\uff09\uff0c\u7cfb\u7edf\u8fbe\u5230\u9ad8\u7279\u5f02\u6027(1.00)\u548c\u60a3\u75c5\u7387\u6821\u6b63\u9633\u6027\u9884\u6d4b\u503c(1.00)\uff0c\u7075\u654f\u5ea60.72\uff1b90\u5929\u7a97\u53e3\u671f\u7cbe\u5ea6\u6700\u9ad8\uff0c\u4ec5\u4f9d\u8d56\u60a3\u8005\u62a5\u544a\u8bed\u8a00\u5b9e\u73b0\u4f4e\u8d1f\u62c5\u9ad8\u7cbe\u5ea6\u68c0\u6d4b", "conclusion": "\u60a3\u8005\u62a5\u544a\u8bed\u8a00\u652f\u6301\u7684\u7b5b\u67e5\u7cfb\u7edf\u53ef\u4e3a\u9ad8\u98ce\u9669\u4e2a\u4f53\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u4e34\u5e8a\u8bc4\u4f30\u4e0e\u5e72\u9884\u65f6\u95f4\u7a97\uff0c\u5728\u4f18\u5148\u4fdd\u8bc1\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\u5e73\u8861\u654f\u611f\u5ea6\uff0c\u9002\u7528\u4e8e\u65e9\u671f\u8111\u5352\u4e2d\u98ce\u9669\u9884\u8b66"}}
{"id": "2602.22967", "categories": ["physics.comp-ph", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22967", "abs": "https://arxiv.org/abs/2602.22967", "authors": ["Yifeng Guan", "Chuyi Liu", "Dongzhan Zhou", "Lei Bai", "Wan-jian Yin", "Jingyuan Li", "Mao Su"], "title": "Discovery of Interpretable Physical Laws in Materials via Language-Model-Guided Symbolic Regression", "comment": null, "summary": "Discovering interpretable physical laws from high-dimensional data is a fundamental challenge in scientific research. Traditional methods, such as symbolic regression, often produce complex, unphysical formulas when searching a vast space of possible forms. We introduce a framework that guides the search process by leveraging the embedded scientific knowledge of large language models, enabling efficient identification of physical laws in the data. We validate our approach by modeling key properties of perovskite materials. Our method mitigates the combinatorial explosion commonly encountered in traditional symbolic regression, reducing the effective search space by a factor of approximately $10^5$. A set of novel formulas for bulk modulus, band gap, and oxygen evolution reaction activity are identified, which not only provide meaningful physical insights but also outperform previous formulas in accuracy and simplicity.", "AI": {"tldr": "\u63d0\u51fa\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u7684\u79d1\u5b66\u77e5\u8bc6\u5f15\u5bfc\u7b26\u53f7\u56de\u5f52\u641c\u7d22\u8fc7\u7a0b\uff0c\u9ad8\u6548\u53d1\u73b0\u9499\u949b\u77ff\u6750\u6599\u6027\u8d28\u7684\u53ef\u89e3\u91ca\u7269\u7406\u5b9a\u5f8b\uff0c\u5c06\u641c\u7d22\u7a7a\u95f4\u7f29\u5c0f\u7ea610^5\u500d\uff0c\u83b7\u5f97\u66f4\u51c6\u786e\u7b80\u6d01\u7684\u65b0\u516c\u5f0f", "motivation": "\u4f20\u7edf\u7b26\u53f7\u56de\u5f52\u5728\u641c\u7d22\u9ad8\u7ef4\u6570\u636e\u4e2d\u7684\u7269\u7406\u5b9a\u5f8b\u65f6\u6613\u4ea7\u751f\u590d\u6742\u65e0\u7269\u7406\u610f\u4e49\u7684\u516c\u5f0f\uff0c\u4e14\u9762\u4e34\u7ec4\u5408\u7206\u70b8\u95ee\u9898\uff0c\u4e9f\u9700\u9ad8\u6548\u53ef\u89e3\u91ca\u7684\u81ea\u52a8\u5316\u53d1\u73b0\u65b9\u6cd5", "method": "\u6784\u5efa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f15\u5bfc\u6846\u67b6\uff0c\u5229\u7528\u5176\u5185\u7f6e\u79d1\u5b66\u77e5\u8bc6\u7ea6\u675f\u641c\u7d22\u7a7a\u95f4\uff0c\u9488\u5bf9\u9499\u949b\u77ff\u6750\u6599\u7684\u4f53\u5f39\u6a21\u91cf\u3001\u5e26\u9699\u548c\u6c27\u6790\u51fa\u53cd\u5e94\u6d3b\u6027\u8fdb\u884c\u5efa\u6a21", "result": "\u5c06\u4f20\u7edf\u65b9\u6cd5\u7684\u7ec4\u5408\u7206\u70b8\u641c\u7d22\u7a7a\u95f4\u7f29\u5c0f\u7ea610^5\u500d\uff0c\u6210\u529f\u8bc6\u522b\u51fa\u4e09\u4e2a\u65b0\u9896\u7269\u7406\u516c\u5f0f\uff0c\u5728\u51c6\u786e\u6027\u548c\u7b80\u6d01\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u516c\u5f0f", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u7b26\u53f7\u56de\u5f52\u7684\u641c\u7d22\u74f6\u9888\uff0c\u751f\u6210\u7684\u516c\u5f0f\u517c\u5177\u7269\u7406\u610f\u4e49\u548c\u9884\u6d4b\u6027\u80fd\uff0c\u4e3a\u81ea\u52a8\u5316\u79d1\u5b66\u53d1\u73b0\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f"}}
{"id": "2602.22273", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22273", "abs": "https://arxiv.org/abs/2602.22273", "authors": ["Xiyuan Zhang", "Huihang Wu", "Jiayu Guo", "Zhenlin Zhang", "Yiwei Zhang", "Liangyu Huo", "Xiaoxiao Ma", "Jiansong Wan", "Xuewei Jiao", "Yi Jing", "Jian Xie"], "title": "FIRE: A Comprehensive Benchmark for Financial Intelligence and Reasoning Evaluation", "comment": null, "summary": "We introduce FIRE, a comprehensive benchmark designed to evaluate both the theoretical financial knowledge of LLMs and their ability to handle practical business scenarios. For theoretical assessment, we curate a diverse set of examination questions drawn from widely recognized financial qualification exams, enabling evaluation of LLMs deep understanding and application of financial knowledge. In addition, to assess the practical value of LLMs in real-world financial tasks, we propose a systematic evaluation matrix that categorizes complex financial domains and ensures coverage of essential subdomains and business activities. Based on this evaluation matrix, we collect 3,000 financial scenario questions, consisting of closed-form decision questions with reference answers and open-ended questions evaluated by predefined rubrics. We conduct comprehensive evaluations of state-of-the-art LLMs on the FIRE benchmark, including XuanYuan 4.0, our latest financial-domain model, as a strong in-domain baseline. These results enable a systematic analysis of the capability boundaries of current LLMs in financial applications. We publicly release the benchmark questions and evaluation code to facilitate future research.", "AI": {"tldr": "This paper introduces FIRE, a comprehensive financial benchmark evaluating LLMs' theoretical knowledge and practical skills across 3,000 scenarios, finding current LLMs have clear capability boundaries in financial applications.", "motivation": "To create a comprehensive benchmark that evaluates both theoretical financial knowledge and practical business scenario handling capabilities of LLMs, addressing the need for systematic assessment of their real-world value in financial tasks.", "method": "Curated financial exam questions for theoretical assessment, developed a systematic evaluation matrix covering complex financial domains, and collected 3,000 financial scenario questions (closed-form with reference answers and open-ended with rubrics) to evaluate state-of-the-art LLMs including XuanYuan 4.0.", "result": "Comprehensive evaluation revealing systematic capability boundaries of current LLMs in financial applications, with XuanYuan 4.0 serving as a strong in-domain baseline; publicly released benchmark and evaluation code for future research.", "conclusion": "The FIRE benchmark provides a comprehensive framework for evaluating LLMs in finance, systematically identifies their capability boundaries, and establishes a valuable resource to advance future research in financial AI applications."}}
{"id": "2602.22312", "categories": ["quant-ph", "physics.atom-ph"], "pdf": "https://arxiv.org/pdf/2602.22312", "abs": "https://arxiv.org/abs/2602.22312", "authors": ["Twesh Upadhyaya", "Yifan Hong", "T. C. Mooney", "Alexey V. Gorshkov"], "title": "Robustness-Runtime Tradeoff for Quantum State Transfer", "comment": "16 pages, 5 figures", "summary": "Quantum state transfer is the primitive of transporting an unknown state on one site of a lattice to another. Using power-law interactions, recent state transfer protocols achieve speedup by utilizing the intermediate ancilla sites. However, these protocols require the ancillas to be in a perfectly initialized state, which, due to noise or imperfect control, may not be the case. In this work we introduce the $\\textit{robustness}$ of a state transfer protocol, which quantifies the protocol's tolerance to error in the initial ancilla state. In the Heisenberg picture, state transfer grows operators supported on the final site such that they no longer commute with all operators on the starting site. We prove that this robustness tightly bounds the Schatten $p$-norms of these commutators between initial and final-site operators. This generalizes the known cases of $p=\\infty$ and $p=2$, which govern completely state-dependent and state-independent state transfer respectively, demonstrating that intermediate values of $p$ govern partially state-dependent state transfer. In conjunction with existing power-law light cones, our result gives new minimum runtimes for partially state-dependent protocols which, in certain regimes, are parametrically better than existing bounds. We introduce new robust state transfer protocols, charting the landscape between complete state-dependence and state-independence.", "AI": {"tldr": "\u63d0\u51fa\u91cf\u5b50\u6001\u4f20\u8f93\u534f\u8bae\u7684\u9c81\u68d2\u6027\u5b9a\u4e49\uff0c\u91cf\u5316\u5176\u5bf9\u521d\u59cb\u8f85\u52a9\u6001\u8bef\u5dee\u7684\u5bb9\u5fcd\u5ea6\uff0c\u8bc1\u660e\u9c81\u68d2\u6027\u4e0e\u4ea4\u6362\u5b50Schatten p-\u8303\u6570\u7684\u7d27\u5bc6\u8fb9\u754c\uff0c\u5e76\u7ed9\u51fa\u90e8\u5206\u4f9d\u8d56\u6001\u4f20\u8f93\u7684\u65b0\u8fd0\u884c\u65f6\u95f4\u754c\u9650\u53ca\u66f4\u4f18\u534f\u8bae\u3002", "motivation": "\u73b0\u6709\u5229\u7528\u5e42\u5f8b\u76f8\u4e92\u4f5c\u7528\u7684\u91cf\u5b50\u6001\u4f20\u8f93\u534f\u8bae\u4f9d\u8d56\u5b8c\u7f8e\u521d\u59cb\u5316\u7684\u8f85\u52a9\u4f4d\uff0c\u4f46\u5b9e\u9645\u4e2d\u566a\u58f0\u548c\u63a7\u5236\u4e0d\u5b8c\u7f8e\u4f1a\u5bfc\u81f4\u521d\u59cb\u6001\u5b58\u5728\u8bef\u5dee\uff0c\u4e9f\u9700\u91cf\u5316\u534f\u8bae\u5bf9\u8fd9\u7c7b\u8bef\u5dee\u7684\u5bb9\u5fcd\u80fd\u529b\uff08\u5373\u9c81\u68d2\u6027\uff09\u3002", "method": "\u5728Heisenberg\u56fe\u50cf\u4e2d\u5206\u6790\u7b97\u7b26\u589e\u957f\uff0c\u5c06\u9c81\u68d2\u6027\u4e0e\u521d\u59cb/\u672b\u6001\u7b97\u7b26\u4ea4\u6362\u5b50\u7684Schatten p-\u8303\u6570\u5efa\u7acb\u4e25\u683c\u8fb9\u754c\u5173\u7cfb\uff0c\u63a8\u5e7fp=\u221e\uff08\u5b8c\u5168\u4f9d\u8d56\u6001\uff09\u548cp=2\uff08\u5b8c\u5168\u72ec\u7acb\u6001\uff09\u7684\u5df2\u77e5\u7ed3\u8bba\u3002", "result": "1) \u8bc1\u660e\u9c81\u68d2\u6027\u7d27\u7ea6\u675f\u4ea4\u6362\u5b50p-\u8303\u6570\uff1b2) \u63ed\u793ap\u503c\u63a7\u5236\u6001\u4f9d\u8d56\u7a0b\u5ea6\uff08p\u5c45\u4e2d\u5bf9\u5e94\u90e8\u5206\u4f9d\u8d56\u6001\uff09\uff1b3) \u7ed3\u5408\u5149\u9525\u7406\u8bba\u7ed9\u51fa\u65b0\u534f\u8bae\u7684\u6700\u5c0f\u8fd0\u884c\u65f6\u95f4\uff0c\u5728\u7279\u5b9a\u533a\u95f4\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u754c\u9650\uff1b4) \u63d0\u51fa\u65b0\u578b\u9c81\u68d2\u4f20\u8f93\u534f\u8bae\u3002", "conclusion": "\u5efa\u7acb\u4e86\u91cf\u5b50\u6001\u4f20\u8f93\u9c81\u68d2\u6027\u7684\u7406\u8bba\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u6001\u4f9d\u8d56/\u72ec\u7acb\u4f20\u8f93\u7684\u6781\u7aef\u60c5\u51b5\uff0c\u4e3a\u566a\u58f0\u73af\u5883\u4e0b\u7684\u9ad8\u6548\u91cf\u5b50\u4fe1\u606f\u4f20\u8f93\u63d0\u4f9b\u4e86\u65b0\u534f\u8bae\u548c\u4f18\u5316\u8def\u5f84\u3002"}}
{"id": "2602.23131", "categories": ["physics.data-an", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.23131", "abs": "https://arxiv.org/abs/2602.23131", "authors": ["Ronald G. Dixson", "Adam L. Pintar", "R. Joseph. Kline", "Thomas A. Germer", "J. Alexander Liddle", "John S. Villarrubia", "Samuel M. Stavis"], "title": "Titanic overconfidence -- dark uncertainty can sink hybrid metrology for semiconductor manufacturing", "comment": "This manuscript has been submitted to the IEEE for possible publication", "summary": "Hybrid metrology for semiconductor manufacturing is on a collision course with dark uncertainty. An IEEE technology roadmap for this venture has targeted a linewidth uncertainty of +/- 0.17 nm at 95 % coverage and advised the hybridization of results from different measurement methods to hit this target. Related studies have applied statistical models that require consistent results to compel a lower uncertainty, whereas inconsistent results are prevalent. We illuminate this lurking issue, studying how standard methods of uncertainty evaluation fail to account for the causes and effects of dark uncertainty. We revisit a comparison of imaging and scattering methods to measure linewidths of approximately 13 nm, applying contrasting statistical models to highlight the potential effect of dark uncertainty on hybrid metrology. A random effects model allows the combination of inconsistent results, accounting for dark uncertainty and estimating a total uncertainty of +/- 0.8 nm at 95 % coverage. In contrast, a common mean model requires consistent results for combination, ignoring dark uncertainty and underestimating the total uncertainty by as much as a factor of five. To avoid such titanic overconfidence, which can sink a venture, we outline good practices to reduce dark uncertainty and guide the combination of indeterminately consistent results.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63ed\u793a\u4e86\u534a\u5bfc\u4f53\u6df7\u5408\u8ba1\u91cf\u4e2d\u7684\"\u6697\u4e0d\u786e\u5b9a\u6027\"\u95ee\u9898\u3002\u901a\u8fc7\u5bf9\u6bd4\u4e24\u79cd\u7edf\u8ba1\u6a21\u578b\u5bf913nm\u7ebf\u5bbd\u6d4b\u91cf\u7684\u5206\u6790\uff0c\u53d1\u73b0\u5ffd\u7565\u6697\u4e0d\u786e\u5b9a\u6027\u7684\u6a21\u578b\u4f1a\u4e25\u91cd\u4f4e\u4f30\u603b\u4e0d\u786e\u5b9a\u6027\u8fbe5\u500d\uff0c\u800c\u8003\u8651\u6697\u4e0d\u786e\u5b9a\u6027\u7684\u968f\u673a\u6548\u5e94\u6a21\u578b\u66f4\u7b26\u5408\u5b9e\u9645\u3002\u8bba\u6587\u63d0\u51fa\u4e86\u51cf\u5c11\u6697\u4e0d\u786e\u5b9a\u6027\u7684\u826f\u597d\u5b9e\u8df5\u3002", "motivation": "\u534a\u5bfc\u4f53\u5236\u9020\u4e2d\u7684\u6df7\u5408\u8ba1\u91cf\u9762\u4e34\"\u6697\u4e0d\u786e\u5b9a\u6027\"\u6311\u6218\u3002IEEE\u6280\u672f\u8def\u7ebf\u56fe\u8981\u6c42\u572895%\u7f6e\u4fe1\u5ea6\u4e0b\u7ebf\u5bbd\u4e0d\u786e\u5b9a\u5ea6\u8fbe\u5230\u00b10.17 nm\uff0c\u4f46\u73b0\u6709\u7edf\u8ba1\u6a21\u578b\u9700\u8981\u4e00\u81f4\u7ed3\u679c\u624d\u80fd\u964d\u4f4e\u4e0d\u786e\u5b9a\u5ea6\uff0c\u800c\u5b9e\u9645\u4e2d\u4e0d\u4e00\u81f4\u7ed3\u679c\u666e\u904d\u5b58\u5728\u3002\u6807\u51c6\u4e0d\u786e\u5b9a\u5ea6\u8bc4\u4f30\u65b9\u6cd5\u672a\u80fd\u89e3\u91ca\u6697\u4e0d\u786e\u5b9a\u6027\u7684\u56e0\u679c\uff0c\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u8fc7\u5ea6\u81ea\u4fe1\u3002", "method": "\u91cd\u65b0\u5206\u6790\u6210\u50cf\u548c\u6563\u5c04\u65b9\u6cd5\u6d4b\u91cf\u7ea613 nm\u7ebf\u5bbd\u7684\u5bf9\u6bd4\u6570\u636e\uff0c\u5e94\u7528\u4e24\u79cd\u5bf9\u6bd4\u7edf\u8ba1\u6a21\u578b\uff1a\u968f\u673a\u6548\u5e94\u6a21\u578b\u548c\u5171\u540c\u5747\u503c\u6a21\u578b\u3002\u968f\u673a\u6548\u5e94\u6a21\u578b\u5141\u8bb8\u5408\u5e76\u4e0d\u4e00\u81f4\u7ed3\u679c\u5e76\u8003\u8651\u6697\u4e0d\u786e\u5b9a\u6027\uff0c\u800c\u5171\u540c\u5747\u503c\u6a21\u578b\u8981\u6c42\u7ed3\u679c\u4e00\u81f4\u624d\u80fd\u5408\u5e76\uff0c\u5ffd\u7565\u6697\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u968f\u673a\u6548\u5e94\u6a21\u578b\u572895%\u7f6e\u4fe1\u5ea6\u4e0b\u7ed9\u51fa\u00b10.8 nm\u7684\u603b\u4e0d\u786e\u5b9a\u5ea6\uff0c\u5408\u7406\u8003\u8651\u4e86\u6697\u4e0d\u786e\u5b9a\u6027\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5171\u540c\u5747\u503c\u6a21\u578b\u5ffd\u7565\u6697\u4e0d\u786e\u5b9a\u6027\uff0c\u4e25\u91cd\u4f4e\u4f30\u603b\u4e0d\u786e\u5b9a\u5ea6\u9ad8\u8fbe5\u500d\u3002\u8fd9\u63ed\u793a\u4e86\u6807\u51c6\u65b9\u6cd5\u7684\u91cd\u5927\u7f3a\u9677\u3002", "conclusion": "\u4e3a\u907f\u514d\u56e0\u8fc7\u5ea6\u81ea\u4fe1\u5bfc\u81f4\u9879\u76ee\u5931\u8d25\uff0c\u5fc5\u987b\u91cd\u89c6\u6697\u4e0d\u786e\u5b9a\u6027\u3002\u8bba\u6587\u63d0\u51fa\u51cf\u5c11\u6697\u4e0d\u786e\u5b9a\u6027\u7684\u826f\u597d\u5b9e\u8df5\uff0c\u5e76\u6307\u5bfc\u5982\u4f55\u5408\u7406\u5408\u5e76\u4e0d\u786e\u5b9a\u4e00\u81f4\u7684\u7ed3\u679c\uff0c\u4ee5\u786e\u4fdd\u6df7\u5408\u8ba1\u91cf\u80fd\u591f\u8fbe\u5230\u534a\u5bfc\u4f53\u5236\u9020\u7684\u4e25\u683c\u7cbe\u5ea6\u8981\u6c42\u3002"}}
{"id": "2602.22429", "categories": ["quant-ph", "cond-mat.mes-hall", "physics.optics"], "pdf": "https://arxiv.org/pdf/2602.22429", "abs": "https://arxiv.org/abs/2602.22429", "authors": ["Daigo Oue"], "title": "Macroscopic Quantum Electrodynamics with Gain: Modified Fluctuations and Their Consequences", "comment": null, "summary": "Macroscopic quantum electrodynamics (MQED) provides a unified framework to describe quantum electromagnetic fields in the presence of arbitrary macroscopic environments. Central to this theory is the field correlation, which governs both radiative (e.g., Lamb shifts and the Purcell effect) and mechanical phenomena, such as van der Waals and Casimir forces. In this tutorial, we provide an overview of MQED and its extension to active media, highlighting fluctuation-induced forces as manifestations of gain-modified field correlations.", "AI": {"tldr": "This tutorial introduces Macroscopic Quantum Electrodynamics (MQED) as a unified framework for quantum electromagnetic fields in macroscopic environments, emphasizing field correlations and their extension to active media to explain phenomena like Casimir forces.", "motivation": "To provide a comprehensive theoretical framework that describes quantum electromagnetic phenomena in arbitrary macroscopic environments, bridging quantum field behavior with classical material responses.", "method": "MQED framework utilizing field correlation functions to unify the description of radiative effects (Lamb shifts, Purcell effect) and mechanical phenomena (van der Waals and Casimir forces), extended to active media with gain-modified correlations.", "result": "Demonstrates that fluctuation-induced forces in both passive and active media can be fundamentally understood as manifestations of modified field correlation functions, with gain media altering these correlations.", "conclusion": "MQED serves as an essential tutorial framework for understanding quantum electromagnetic phenomena in complex environments, with particular value in explaining how material properties (including gain) modify quantum field correlations and associated forces."}}
{"id": "2602.22500", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22500", "abs": "https://arxiv.org/abs/2602.22500", "authors": ["Anastasija Mensikova", "Donna M. Rizzo", "Kathryn Hinkelman"], "title": "Mapping the Landscape of Artificial Intelligence in Life Cycle Assessment Using Large Language Models", "comment": null, "summary": "Integration of artificial intelligence (AI) into life cycle assessment (LCA) has accelerated in recent years, with numerous studies successfully adapting machine learning algorithms to support various stages of LCA. Despite this rapid development, comprehensive and broad synthesis of AI-LCA research remains limited. To address this gap, this study presents a detailed review of published work at the intersection of AI and LCA, leveraging large language models (LLMs) to identify current trends, emerging themes, and future directions. Our analyses reveal that as LCA research continues to expand, the adoption of AI technologies has grown dramatically, with a noticeable shift toward LLM-driven approaches, continued increases in ML applications, and statistically significant correlations between AI approaches and corresponding LCA stages. By integrating LLM-based text-mining methods with traditional literature review techniques, this study introduces a dynamic and effective framework capable of capturing both high-level research trends and nuanced conceptual patterns (themes) across the field. Collectively, these findings demonstrate the potential of LLM-assisted methodologies to support large-scale, reproducible reviews across broad research domains, while also evaluating pathways for computationally-efficient LCA in the context of rapidly developing AI technologies. In doing so, this work helps LCA practitioners incorporate state-of-the-art tools and timely insights into environmental assessments that can enhance the rigor and quality of sustainability-driven decisions and decision-making processes.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u5bf9AI\u4e0e\u751f\u547d\u5468\u671f\u8bc4\u4f30(LCA)\u4ea4\u53c9\u9886\u57df\u7684\u7814\u7a76\u8fdb\u884c\u5168\u9762\u7efc\u8ff0\uff0c\u8bc6\u522b\u5f53\u524d\u8d8b\u52bf\u3001\u65b0\u5174\u4e3b\u9898\u548c\u672a\u6765\u65b9\u5411\u3002\u7814\u7a76\u53d1\u73b0AI\u6280\u672f\u5728LCA\u4e2d\u7684\u5e94\u7528\u5feb\u901f\u589e\u957f\uff0c\u7279\u522b\u662fLLM\u9a71\u52a8\u65b9\u6cd5\u663e\u8457\u589e\u52a0\uff0c\u4e14AI\u65b9\u6cd5\u4e0e\u5e94\u7528\u7684LCA\u9636\u6bb5\u5b58\u5728\u663e\u8457\u76f8\u5173\u6027\u3002\u8be5\u7814\u7a76\u63d0\u51fa\u4e86LLM\u4e0e\u4f20\u7edf\u6587\u732e\u7efc\u8ff0\u76f8\u7ed3\u5408\u7684\u9ad8\u6548\u6846\u67b6\uff0c\u4e3a\u53ef\u6301\u7eed\u6027\u51b3\u7b56\u63d0\u4f9b\u4e86\u66f4\u4e25\u8c28\u7684\u73af\u5883\u8bc4\u4f30\u5de5\u5177\u3002", "motivation": "\u5c3d\u7ba1AI\u4e0eLCA\u7684\u6574\u5408\u8fd1\u5e74\u6765\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u5bf9\u8be5\u4ea4\u53c9\u9886\u57df\u7684\u5168\u9762\u548c\u5e7f\u6cdb\u7efc\u5408\u7814\u7a76\u4ecd\u7136\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u7cfb\u7edf\u6027\u7efc\u8ff0\u5206\u6790AI\u5728LCA\u4e2d\u7684\u5e94\u7528\u73b0\u72b6\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u6e05\u6670\u7684\u7814\u7a76\u8109\u7edc\u548c\u53d1\u5c55\u65b9\u5411\u3002", "method": "\u7814\u7a76\u91c7\u7528\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u9a71\u52a8\u6587\u672c\u6316\u6398\u6280\u672f\uff0c\u7ed3\u5408\u4f20\u7edf\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5bf9AI-LCA\u9886\u57df\u7684\u5df2\u53d1\u8868\u6587\u732e\u8fdb\u884c\u52a8\u6001\u5206\u6790\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u540c\u65f6\u6355\u6349\u5b8f\u89c2\u7814\u7a76\u8d8b\u52bf\u548c\u5fae\u89c2\u6982\u5ff5\u6a21\u5f0f(\u4e3b\u9898)\uff0c\u5f62\u6210\u53ef\u5927\u89c4\u6a21\u3001\u53ef\u91cd\u590d\u7684\u7efc\u8ff0\u6846\u67b6\u3002", "result": "\u5206\u6790\u8868\u660e\uff1a1) AI\u6280\u672f\u5728LCA\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u6025\u5267\u589e\u957f\uff1b2) \u7814\u7a76\u91cd\u5fc3\u660e\u663e\u5411LLM\u9a71\u52a8\u65b9\u6cd5\u8f6c\u79fb\uff1b3) \u673a\u5668\u5b66\u4e60\u5e94\u7528\u6301\u7eed\u589e\u52a0\uff1b4) AI\u65b9\u6cd5\u4e0e\u7279\u5b9aLCA\u9636\u6bb5\u4e4b\u95f4\u5b58\u5728\u7edf\u8ba1\u663e\u8457\u6027\u76f8\u5173\u6027\uff1b5) \u6240\u63d0\u51fa\u7684LLM\u589e\u5f3a\u578b\u7efc\u8ff0\u6846\u67b6\u80fd\u6709\u6548\u8bc6\u522b\u9886\u57df\u5168\u8c8c\u3002", "conclusion": "LLM\u8f85\u52a9\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u3001\u53ef\u91cd\u590d\u7684\u8de8\u5b66\u79d1\u7efc\u8ff0\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u5e76\u4e3a\u8ba1\u7b97\u9ad8\u6548\u7684LCA\u5b9e\u65bd\u63d0\u4f9b\u4e86\u8def\u5f84\u3002\u672c\u7814\u7a76\u5e2e\u52a9LCA\u4ece\u4e1a\u8005\u6574\u5408\u524d\u6cbfAI\u5de5\u5177\uff0c\u5c06\u53ca\u65f6\u6d1e\u5bdf\u878d\u5165\u73af\u5883\u8bc4\u4f30\uff0c\u4ece\u800c\u63d0\u5347\u53ef\u6301\u7eed\u6027\u51b3\u7b56\u7684\u4e25\u8c28\u6027\u548c\u8d28\u91cf\u3002"}}
{"id": "2602.22540", "categories": ["quant-ph", "cs.ET", "cs.PF"], "pdf": "https://arxiv.org/pdf/2602.22540", "abs": "https://arxiv.org/abs/2602.22540", "authors": ["Timothy Proctor", "Robin Blume-Kohout", "Andrew Baczewski"], "title": "The Road to Useful Quantum Computers", "comment": "This article was written by invitation for the National Academy of Engineering (NAE)'s magazine, The Bridge, based on a talk given by the first author at The Grainger Foundation Frontiers of Engineering 2025 Symposium. It was written for a general audience of engineers. It is close to the published article, which can be found at https://www.nae.edu/344306/the-road-to-useful-quantum-computers", "summary": "Building a useful quantum computer is a grand science and engineering challenge, currently pursued intensely by teams around the world. In the 1980s, Richard Feynman and Yuri Manin observed independently that computers based on quantum mechanics might enable better simulations of quantum phenomena. Their vision remained an intellectual curiosity until Peter Shor published his famous quantum algorithm for integer factoring, and shortly thereafter a proof that errors in quantum computations can be corrected. Since then, quantum computing R&D has progressed rapidly, from small-scale experiments in university physics laboratories to well-funded industrial efforts and prototypes. Hype notwithstanding, quantum computers have yet to solve scientifically or practically important problems -- a target often called quantum utility. In this article, we describe the capabilities of contemporary quantum computers, compare them to the requirements of quantum utility, and illustrate how to track progress from today to utility. We highlight key science and engineering challenges on the road to quantum utility, touching on relevant aspects of our own research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u91cf\u5b50\u8ba1\u7b97\u7684\u73b0\u72b6\u4e0e\u5b9e\u7528\u5316\uff08\u91cf\u5b50\u6548\u7528\uff09\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5206\u6790\u4e86\u5f53\u524d\u6280\u672f\u80fd\u529b\u3001\u5173\u952e\u6311\u6218\u53ca\u5b9e\u73b0\u5b9e\u7528\u5316\u7684\u8def\u5f84\u3002", "motivation": "\u6784\u5efa\u5b9e\u7528\u91cf\u5b50\u8ba1\u7b97\u673a\u662f\u91cd\u5927\u79d1\u5b66\u4e0e\u5de5\u7a0b\u6311\u6218\uff0c\u867d\u81ea1980\u5e74\u4ee3\u8d39\u66fc\u7b49\u4eba\u63d0\u51fa\u6982\u5ff5\u3001\u8096\u5c14\u7b97\u6cd5\u53ca\u91cf\u5b50\u7ea0\u9519\u7406\u8bba\u63a8\u52a8\u53d1\u5c55\uff0c\u4f46\u5f53\u524d\u91cf\u5b50\u8ba1\u7b97\u673a\u5c1a\u672a\u89e3\u51b3\u5177\u6709\u79d1\u5b66\u6216\u5b9e\u8df5\u4ef7\u503c\u7684\u95ee\u9898\uff0c\u5b58\u5728 hype \u4e0e\u73b0\u5b9e\u7684\u9e3f\u6c9f\u3002", "method": "\u901a\u8fc7\u63cf\u8ff0\u5f53\u4ee3\u91cf\u5b50\u8ba1\u7b97\u673a\u7684\u80fd\u529b\uff0c\u5bf9\u6bd4\u91cf\u5b50\u6548\u7528\u7684\u8981\u6c42\uff0c\u63d0\u51fa\u4ece\u5f53\u524d\u72b6\u6001\u5411\u5b9e\u7528\u5316\u8fdb\u5c55\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u4f5c\u8005\u7814\u7a76\u5f3a\u8c03\u5173\u952e\u79d1\u5b66\u4e0e\u5de5\u7a0b\u6311\u6218\u3002", "result": "\u91cf\u5b50\u8ba1\u7b97\u673a\u5c1a\u672a\u5b9e\u73b0\"\u91cf\u5b50\u6548\u7528\"\uff08\u5373\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\uff09\uff0c\u73b0\u6709\u6280\u672f\u4ecd\u9762\u4e34\u80fd\u529b\u4e0e\u9700\u6c42\u95f4\u7684\u663e\u8457\u5dee\u8ddd\uff0c\u9700\u7a81\u7834\u5173\u952e\u6311\u6218\u624d\u80fd\u8fbe\u6210\u76ee\u6807\u3002", "conclusion": "\u5b9e\u73b0\u91cf\u5b50\u6548\u7528\u9700\u6301\u7eed\u653b\u514b\u79d1\u5b66\u4e0e\u5de5\u7a0b\u96be\u9898\uff0c\u8bba\u6587\u4e3a\u8fdb\u5c55\u8ddf\u8e2a\u63d0\u4f9b\u6846\u67b6\uff0c\u5e76\u547c\u5401\u805a\u7126\u6838\u5fc3\u6311\u6218\u4ee5\u63a8\u52a8\u4ece\u5b9e\u9a8c\u5ba4\u539f\u578b\u5411\u5b9e\u7528\u5316\u8fc7\u6e21\u3002"}}
{"id": "2602.22718", "categories": ["cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.22718", "abs": "https://arxiv.org/abs/2602.22718", "authors": ["Rui Wei", "Hanfei Yu", "Shubham Jain", "Yogarajan Sivakumar", "Devesh Tiwari", "Jian Li", "Seung-Jong Park", "Hao Wang"], "title": "RLHFless: Serverless Computing for Efficient RLHF", "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) has been widely applied to Large Language Model (LLM) post-training to align model outputs with human preferences. Recent models, such as DeepSeek-R1, have also shown RLHF's potential to improve LLM reasoning on complex tasks. In RL, inference and training co-exist, creating dynamic resource demands throughout the workflow. Compared to traditional RL, RLHF further challenges training efficiency due to expanding model sizes and resource consumption. Several RLHF frameworks aim to balance flexible abstraction and efficient execution. However, they rely on serverful infrastructures, which struggle with fine-grained resource variability. As a result, during synchronous RLHF training, idle time between or within RL components often causes overhead and resource wastage.\n  To address these issues, we present RLHFless, the first scalable training framework for synchronous RLHF, built on serverless computing environments. RLHFless adapts to dynamic resource demands throughout the RLHF pipeline, pre-computes shared prefixes to avoid repeated computation, and uses a cost-aware actor scaling strategy that accounts for response length variation to find sweet spots with lower cost and higher speed. In addition, RLHFless assigns workloads efficiently to reduce intra-function imbalance and idle time. Experiments on both physical testbeds and a large-scale simulated cluster show that RLHFless achieves up to 1.35x speedup and 44.8% cost reduction compared to the state-of-the-art baseline.", "AI": {"tldr": "This paper proposes RLHFless, a serverless computing framework for synchronous RLHF training that addresses dynamic resource demands and resource wastage through prefix pre-computation, cost-aware actor scaling, and efficient workload assignment, achieving 1.35x speedup and 44.8% cost reduction.", "motivation": "RLHF training faces efficiency challenges from expanding model sizes and dynamic resource demands. Existing serverful frameworks struggle with fine-grained resource variability, causing significant idle time and resource wastage between synchronous RL components.", "method": "RLHFless is built on serverless computing with three key techniques: adapting to dynamic resource demands, pre-computing shared prefixes to avoid repeated computation, and employing a cost-aware actor scaling strategy that accounts for response length variation with efficient workload assignment.", "result": "Experiments on physical testbeds and large-scale simulated clusters show RLHFless achieves up to 1.35x speedup and 44.8% cost reduction compared to state-of-the-art baselines.", "conclusion": "RLHFless demonstrates serverless computing as an effective platform for scalable synchronous RLHF training, significantly improving both performance and cost-efficiency over traditional serverful approaches."}}
{"id": "2602.22334", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.22334", "abs": "https://arxiv.org/abs/2602.22334", "authors": ["Yuda Bi", "Wenjun Xiao", "Linhao Bai", "Vince D Calhoun"], "title": "A 1/R Law for Kurtosis Contrast in Balanced Mixtures", "comment": null, "summary": "Kurtosis-based Independent Component Analysis (ICA) weakens in wide, balanced mixtures. We prove a sharp redundancy law: for a standardized projection with effective width $R_{\\mathrm{eff}}$ (participation ratio), the population excess kurtosis obeys $|\u03ba(y)|=O(\u03ba_{\\max}/R_{\\mathrm{eff}})$, yielding the order-tight $O(c_b\u03ba_{\\max}/R)$ under balance (typically $c_b=O(\\log R)$). As an impossibility screen, under standard finite-moment conditions for sample kurtosis estimation, surpassing the $O(1/\\sqrt{T})$ estimation scale requires $R\\lesssim \u03ba_{\\max}\\sqrt{T}$. We also show that \\emph{purification} -- selecting $m\\!\\ll\\!R$ sign-consistent sources -- restores $R$-independent contrast $\u03a9(1/m)$, with a simple data-driven heuristic. Synthetic experiments validate the predicted decay, the $\\sqrt{T}$ crossover, and contrast recovery.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.22367", "categories": ["cs.LG", "cs.AI", "math.NA", "q-bio.TO"], "pdf": "https://arxiv.org/pdf/2602.22367", "abs": "https://arxiv.org/abs/2602.22367", "authors": ["Arsenii Dokuchaev", "Francesca Bonizzoni", "Stefano Pagani", "Francesco Regazzoni", "Simone Pezzuto"], "title": "Learning geometry-dependent lead-field operators for forward ECG modeling", "comment": "20 pages, 9 figures", "summary": "Modern forward electrocardiogram (ECG) computational models rely on an accurate representation of the torso domain. The lead-field method enables fast ECG simulations while preserving full geometric fidelity. Achieving high anatomical accuracy in torso representation is, however, challenging in clinical practice, as imaging protocols are typically focused on the heart and often do not include the entire torso. In addition, the computational cost of the lead-field method scales linearly with the number of electrodes, limiting its applicability in high-density recording settings. To date, no existing approach simultaneously achieves high anatomical fidelity, low data requirements and computational efficiency. In this work, we propose a shape-informed surrogate model of the lead-field operator that serves as a drop-in replacement for the full-order model in forward ECG simulations. The proposed framework consists of two components: a geometry-encoding module that maps anatomical shapes into a low-dimensional latent space, and a geometry-conditioned neural surrogate that predicts lead-field gradients from spatial coordinates, electrode positions and latent codes. The proposed method achieves high accuracy in approximating lead fields both within the torso (mean angular error 5\u00b0) and inside the heart, resulting in highly accurate ECG simulations (relative mean squared error <2.5%. The surrogate consistently outperforms the widely used pseudo lead-field approximation while preserving negligible inference cost. Owing to its compact latent representation, the method does not require a fully detailed torso segmentation and can therefore be deployed in data-limited settings while preserving high-fidelity ECG simulations.", "AI": {"tldr": "A shape-informed neural surrogate model for forward ECG simulation that achieves high accuracy with low computational cost and minimal data requirements.", "motivation": "Current lead-field methods for forward ECG simulation face challenges: (1) obtaining complete torso anatomical data is difficult in clinical practice as imaging focuses on the heart, and (2) computational cost scales linearly with electrode number, limiting high-density applications. No existing method simultaneously provides high anatomical fidelity, low data requirements, and computational efficiency.", "method": "The authors propose a two-component framework: (1) a geometry-encoding module that maps anatomical shapes to a low-dimensional latent space, and (2) a geometry-conditioned neural surrogate that predicts lead-field gradients from spatial coordinates, electrode positions, and latent codes. This serves as a drop-in replacement for the full-order lead-field model.", "result": "The method achieves high accuracy with mean angular error of 5\u00b0 for lead field approximation and relative mean squared error <2.5% for ECG simulations. It outperforms pseudo lead-field approximation while maintaining negligible inference cost.", "conclusion": "The proposed shape-informed surrogate model enables high-fidelity ECG simulations in data-limited clinical settings without requiring detailed torso segmentation, addressing key limitations of traditional lead-field methods."}}
{"id": "2602.22412", "categories": ["cs.LG", "cs.HC", "cs.IT", "econ.GN"], "pdf": "https://arxiv.org/pdf/2602.22412", "abs": "https://arxiv.org/abs/2602.22412", "authors": ["Ruiqi Zhou", "Donghao Zhu", "Houcai Shen"], "title": "A Learning-Based Hybrid Decision Framework for Matching Systems with User Departure Detection", "comment": "Accepted at HCII 2026", "summary": "In matching markets such as kidney exchanges and freight exchanges, delayed matching has been shown to improve overall market efficiency. The benefits of delay are highly sensitive to participants' sojourn times and departure behavior, and delaying matches can impose significant costs, including longer waiting times and increased market congestion. These competing effects make fixed matching policies inherently inflexible in dynamic environments. We propose a learning-based Hybrid framework that adaptively combines immediate and delayed matching. The framework continuously collects data on user departures over time, estimates the underlying departure distribution via regression, and determines whether to delay matching in the subsequent period based on a decision threshold that governs the system's tolerance for matching efficiency loss. The proposed framework can substantially reduce waiting times and congestion while sacrificing only a limited amount of matching efficiency. By dynamically adjusting its matching strategy, the Hybrid framework enables system performance to flexibly interpolate between purely greedy and purely patient policies, offering a robust and adaptive alternative to static matching mechanisms.", "AI": {"tldr": "\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u5339\u914d\u5e02\u573a\uff0c\u901a\u8fc7\u4f30\u8ba1\u79bb\u5f00\u5206\u5e03\u548c\u4f7f\u7528\u51b3\u7b56\u9608\u503c\u6765\u81ea\u9002\u5e94\u5730\u7ed3\u5408\u5373\u65f6\u5339\u914d\u548c\u5ef6\u8fdf\u5339\u914d\uff0c\u4ee5\u5e73\u8861\u6548\u7387\u3001\u7b49\u5f85\u65f6\u95f4\u548c\u62e5\u5835\u3002", "motivation": "\u5728\u52a8\u6001\u5339\u914d\u5e02\u573a\u4e2d\uff0c\u56fa\u5b9a\u5339\u914d\u7b56\u7565\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u800c\u5ef6\u8fdf\u5339\u914d\u7684\u6536\u76ca\u5bf9\u53c2\u4e0e\u8005\u505c\u7559\u65f6\u95f4\u548c\u79bb\u5f00\u884c\u4e3a\u9ad8\u5ea6\u654f\u611f\uff0c\u5bfc\u81f4\u6548\u7387\u3001\u7b49\u5f85\u65f6\u95f4\u548c\u62e5\u5835\u4e4b\u95f4\u7684\u6743\u8861\u3002", "method": "\u6df7\u5408\u6846\u67b6\u6301\u7eed\u6536\u96c6\u7528\u6237\u79bb\u5f00\u6570\u636e\uff0c\u901a\u8fc7\u56de\u5f52\u4f30\u8ba1\u6f5c\u5728\u79bb\u5f00\u5206\u5e03\uff0c\u5e76\u57fa\u4e8e\u51b3\u7b56\u9608\u503c\u51b3\u5b9a\u5728\u540e\u7eed\u671f\u95f4\u662f\u5426\u5ef6\u8fdf\u5339\u914d\uff0c\u8be5\u9608\u503c\u63a7\u5236\u7cfb\u7edf\u5bf9\u5339\u914d\u6548\u7387\u635f\u5931\u7684\u5bb9\u5fcd\u5ea6\u3002", "result": "\u8be5\u6846\u67b6\u5728\u4ec5\u727a\u7272\u6709\u9650\u5339\u914d\u6548\u7387\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u7b49\u5f85\u65f6\u95f4\u548c\u62e5\u5835\uff0c\u4f7f\u7cfb\u7edf\u6027\u80fd\u80fd\u591f\u5728\u7eaf\u8d2a\u5fc3\u548c\u7eaf\u8010\u5fc3\u7b56\u7565\u4e4b\u95f4\u7075\u6d3b\u63d2\u503c\u3002", "conclusion": "\u6df7\u5408\u6846\u67b6\u901a\u8fc7\u57fa\u4e8e\u5b9e\u65f6\u6570\u636e\u52a8\u6001\u8c03\u6574\u5339\u914d\u7b56\u7565\uff0c\u4e3a\u9759\u6001\u5339\u914d\u673a\u5236\u63d0\u4f9b\u4e86\u7a33\u5065\u4e14\u81ea\u9002\u5e94\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2602.22842", "categories": ["cs.AI", "cs.CE", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.22842", "abs": "https://arxiv.org/abs/2602.22842", "authors": ["Tan Bui-Thanh"], "title": "The AI Research Assistant: Promise, Peril, and a Proof of Concept", "comment": "11 pages, 1 figure", "summary": "Can artificial intelligence truly contribute to creative mathematical research, or does it merely automate routine calculations while introducing risks of error? We provide empirical evidence through a detailed case study: the discovery of novel error representations and bounds for Hermite quadrature rules via systematic human-AI collaboration.\n  Working with multiple AI assistants, we extended results beyond what manual work achieved, formulating and proving several theorems with AI assistance. The collaboration revealed both remarkable capabilities and critical limitations. AI excelled at algebraic manipulation, systematic proof exploration, literature synthesis, and LaTeX preparation. However, every step required rigorous human verification, mathematical intuition for problem formulation, and strategic direction.\n  We document the complete research workflow with unusual transparency, revealing patterns in successful human-AI mathematical collaboration and identifying failure modes researchers must anticipate. Our experience suggests that, when used with appropriate skepticism and verification protocols, AI tools can meaningfully accelerate mathematical discovery while demanding careful human oversight and deep domain expertise.", "AI": {"tldr": "This paper investigates whether AI can contribute to creative mathematical research through a case study on Hermite quadrature rules. It finds that AI excels at algebraic manipulation and proof exploration but requires rigorous human verification and domain expertise, suggesting AI can accelerate discovery when used with appropriate oversight.", "motivation": "Can artificial intelligence truly contribute to creative mathematical research, or does it merely automate routine calculations while introducing risks of error?", "method": "A detailed case study on discovering novel error representations and bounds for Hermite quadrature rules via systematic human-AI collaboration, working with multiple AI assistants to extend results beyond manual work and formulate/prove several theorems.", "result": "AI excelled at algebraic manipulation, systematic proof exploration, literature synthesis, and LaTeX preparation, but every step required rigorous human verification, mathematical intuition for problem formulation, and strategic direction. The collaboration successfully extended mathematical results.", "conclusion": "When used with appropriate skepticism and verification protocols, AI tools can meaningfully accelerate mathematical discovery while demanding careful human oversight and deep domain expertise. The study reveals patterns for successful collaboration and identifies failure modes."}}
{"id": "2602.23163", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.IT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.23163", "abs": "https://arxiv.org/abs/2602.23163", "authors": ["Usman Anwar", "Julianna Piskorz", "David D. Baek", "David Africa", "Jim Weatherall", "Max Tegmark", "Christian Schroeder de Witt", "Mihaela van der Schaar", "David Krueger"], "title": "A Decision-Theoretic Formalisation of Steganography With Applications to LLM Monitoring", "comment": "First two authors contributed equally", "summary": "Large language models are beginning to show steganographic capabilities. Such capabilities could allow misaligned models to evade oversight mechanisms. Yet principled methods to detect and quantify such behaviours are lacking. Classical definitions of steganography, and detection methods based on them, require a known reference distribution of non-steganographic signals. For the case of steganographic reasoning in LLMs, knowing such a reference distribution is not feasible; this renders these approaches inapplicable. We propose an alternative, \\textbf{decision-theoretic view of steganography}. Our central insight is that steganography creates an asymmetry in usable information between agents who can and cannot decode the hidden content (present within a steganographic signal), and this otherwise latent asymmetry can be inferred from the agents' observable actions. To formalise this perspective, we introduce generalised $\\mathcal{V}$-information: a utilitarian framework for measuring the amount of usable information within some input. We use this to define the \\textbf{steganographic gap} -- a measure that quantifies steganography by comparing the downstream utility of the steganographic signal to agents that can and cannot decode the hidden content. We empirically validate our formalism, and show that it can be used to detect, quantify, and mitigate steganographic reasoning in LLMs.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u9690\u5199\u80fd\u529b\u53ef\u80fd\u89c4\u907f\u76d1\u7763\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u51b3\u7b56\u8bba\u7684\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u5e7f\u4e49V-\u4fe1\u606f\u5b9a\u4e49\u9690\u5199\u95f4\u9699\uff0c\u65e0\u9700\u53c2\u8003\u5206\u5e03\u5373\u53ef\u68c0\u6d4b\u3001\u91cf\u5316\u548c\u7f13\u89e3LLM\u9690\u5199\u63a8\u7406\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5c55\u73b0\u51fa\u9690\u5199\u80fd\u529b\uff0c\u53ef\u80fd\u88ab\u672a\u5bf9\u9f50\u6a21\u578b\u7528\u4e8e\u9003\u907f\u76d1\u7763\u673a\u5236\uff0c\u4f46\u73b0\u6709\u57fa\u4e8e\u53c2\u8003\u5206\u5e03\u7684\u7ecf\u5178\u9690\u5199\u68c0\u6d4b\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8eLLM\u63a8\u7406\u573a\u666f\uff0c\u4e9f\u9700\u65b0\u7684\u68c0\u6d4b\u8303\u5f0f\u3002", "method": "\u63d0\u51fa\u51b3\u7b56\u8bba\u89c6\u89d2\uff1a\u9690\u5199\u672f\u5728\u53ef\u89e3\u7801\u4e0e\u4e0d\u53ef\u89e3\u7801\u667a\u80fd\u4f53\u95f4\u521b\u9020\u53ef\u7528\u4fe1\u606f\u4e0d\u5bf9\u79f0\u6027\u3002\u5f15\u5165\u5e7f\u4e49V-\u4fe1\u606f\u4f5c\u4e3a\u529f\u5229\u4e3b\u4e49\u6846\u67b6\u6d4b\u91cf\u8f93\u5165\u53ef\u7528\u4fe1\u606f\uff0c\u5e76\u5b9a\u4e49\u9690\u5199\u95f4\u9699\u2014\u2014\u901a\u8fc7\u6bd4\u8f83\u4e24\u7c7b\u667a\u80fd\u4f53\u5bf9\u9690\u5199\u4fe1\u53f7\u7684\u4e0b\u6e38\u6548\u7528\u5dee\u5f02\u6765\u91cf\u5316\u9690\u5199\u7a0b\u5ea6\u3002", "result": "\u5b9e\u8bc1\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u5f62\u5f0f\u5316\u65b9\u6cd5\u80fd\u6709\u6548\u68c0\u6d4b\u3001\u91cf\u5316\u548c\u7f13\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u9690\u5199\u63a8\u7406\u884c\u4e3a\uff0c\u4e3a\u76d1\u7763LLM\u63d0\u4f9b\u4e86\u53ef\u884c\u5de5\u5177\u3002", "conclusion": "\u51b3\u7b56\u8bba\u65b9\u6cd5\u7a81\u7834\u4e86\u7ecf\u5178\u9690\u5199\u68c0\u6d4b\u5bf9\u5df2\u77e5\u53c2\u8003\u5206\u5e03\u7684\u4f9d\u8d56\uff0c\u4e3a\u76d1\u63a7\u6f5c\u5728\u672a\u5bf9\u9f50\u7684LLM\u9690\u5199\u884c\u4e3a\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6846\u67b6\u3002"}}
{"id": "2602.23329", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.23329", "abs": "https://arxiv.org/abs/2602.23329", "authors": ["Chen Bo Calvin Zhang", "Christina Q. Knight", "Nicholas Kruus", "Jason Hausenloy", "Pedro Medeiros", "Nathaniel Li", "Aiden Kim", "Yury Orlovskiy", "Coleman Breen", "Bryce Cai", "Jasper G\u00f6tting", "Andrew Bo Liu", "Samira Nedungadi", "Paula Rodriguez", "Yannis Yiming He", "Mohamed Shaaban", "Zifan Wang", "Seth Donoughe", "Julian Michael"], "title": "LLM Novice Uplift on Dual-Use, In Silico Biology Tasks", "comment": "59 pages, 33 figures", "summary": "Large language models (LLMs) perform increasingly well on biology benchmarks, but it remains unclear whether they uplift novice users -- i.e., enable humans to perform better than with internet-only resources. This uncertainty is central to understanding both scientific acceleration and dual-use risk. We conducted a multi-model, multi-benchmark human uplift study comparing novices with LLM access versus internet-only access across eight biosecurity-relevant task sets. Participants worked on complex problems with ample time (up to 13 hours for the most involved tasks). We found that LLM access provided substantial uplift: novices with LLMs were 4.16 times more accurate than controls (95% CI [2.63, 6.87]). On four benchmarks with available expert baselines (internet-only), novices with LLMs outperformed experts on three of them. Perhaps surprisingly, standalone LLMs often exceeded LLM-assisted novices, indicating that users were not eliciting the strongest available contributions from the LLMs. Most participants (89.6%) reported little difficulty obtaining dual-use-relevant information despite safeguards. Overall, LLMs substantially uplift novices on biological tasks previously reserved for trained practitioners, underscoring the need for sustained, interactive uplift evaluations alongside traditional benchmarks.", "AI": {"tldr": "LLMs provide massive uplift for novices on biosecurity tasks, making them 4x more accurate than internet-only users and outperforming experts in some cases, raising dual-use concerns.", "motivation": "It's unclear whether LLMs genuinely enable novice users to perform better on biology tasks compared to internet-only resources, which is crucial for understanding scientific acceleration and dual-use risks.", "method": "Multi-model, multi-benchmark human study comparing novices with LLM access versus internet-only access across eight biosecurity-relevant task sets, with ample time (up to 13 hours).", "result": "Novices with LLMs were 4.16\u00d7 more accurate than controls (95% CI [2.63, 6.87]); outperformed experts on 3/4 benchmarks; standalone LLMs often exceeded assisted novices; 89.6% reported little difficulty obtaining dual-use information.", "conclusion": "LLMs dramatically uplift novices on specialized biological tasks, underscoring the need for sustained, interactive uplift evaluations alongside traditional benchmarks to assess dual-use risks."}}
{"id": "2602.23330", "categories": ["cs.AI", "q-fin.TR"], "pdf": "https://arxiv.org/pdf/2602.23330", "abs": "https://arxiv.org/abs/2602.23330", "authors": ["Kunihiro Miyazaki", "Takanobu Kawahara", "Stephen Roberts", "Stefan Zohren"], "title": "Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks", "comment": "14 pages, 3 figures", "summary": "The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis into fine-grained tasks, rather than providing coarse-grained instructions. We evaluate the proposed framework using Japanese stock data, including prices, financial statements, news, and macro information, under a leakage-controlled backtesting setting. Experimental results show that fine-grained task decomposition significantly improves risk-adjusted returns compared to conventional coarse-grained designs. Crucially, further analysis of intermediate agent outputs suggests that alignment between analytical outputs and downstream decision preferences is a critical driver of system performance. Moreover, we conduct standard portfolio optimization, exploiting low correlation with the stock index and the variance of each system's output. This approach achieves superior performance. These findings contribute to the design of agent structure and task configuration when applying LLM agents to trading systems in practical settings.", "AI": {"tldr": "\u63d0\u51fa\u7ec6\u7c92\u5ea6\u4efb\u52a1\u5206\u89e3\u7684\u591aAgent LLM\u4ea4\u6613\u6846\u67b6\uff0c\u5728\u63a7\u5236\u6570\u636e\u6cc4\u9732\u7684\u56de\u6d4b\u4e2d\u663e\u8457\u63d0\u5347\u65e5\u672c\u80a1\u7968\u4ea4\u6613\u7684\u98ce\u9669\u8c03\u6574\u6536\u76ca\uff0c\u53d1\u73b0\u5206\u6790\u8f93\u51fa\u4e0e\u51b3\u7b56\u504f\u597d\u7684\u5bf9\u9f50\u662f\u6027\u80fd\u5173\u952e\u9a71\u52a8\u56e0\u7d20", "motivation": "\u73b0\u6709\u591aAgent\u4ea4\u6613\u7cfb\u7edf\u4f9d\u8d56\u62bd\u8c61\u6307\u4ee4\uff0c\u5ffd\u89c6\u771f\u5b9e\u5de5\u4f5c\u6d41\u7ec6\u8282\uff0c\u5bfc\u81f4\u63a8\u7406\u6027\u80fd\u4e0b\u964d\u548c\u51b3\u7b56\u4e0d\u900f\u660e", "method": "\u5c06\u6295\u8d44\u5206\u6790\u663e\u5f0f\u5206\u89e3\u4e3a\u7ec6\u7c92\u5ea6\u4efb\u52a1\uff08\u800c\u975e\u7c97\u7c92\u5ea6\u6307\u4ee4\uff09\uff0c\u4f7f\u7528\u65e5\u672c\u80a1\u7968\u6570\u636e\uff08\u4ef7\u683c/\u8d22\u62a5/\u65b0\u95fb/\u5b8f\u89c2\u4fe1\u606f\uff09\u5728\u9632\u6cc4\u9732\u56de\u6d4b\u73af\u5883\u4e2d\u9a8c\u8bc1", "result": "\u7ec6\u7c92\u5ea6\u5206\u89e3\u663e\u8457\u63d0\u5347\u98ce\u9669\u8c03\u6574\u6536\u76ca\uff1bAgent\u8f93\u51fa\u4e0e\u51b3\u7b56\u504f\u597d\u7684\u5bf9\u9f50\u5ea6\u662f\u7cfb\u7edf\u6027\u80fd\u5173\u952e\u9a71\u52a8\u56e0\u7d20\uff1b\u7ed3\u5408\u4f4e\u76f8\u5173\u6027\u6295\u8d44\u7ec4\u5408\u4f18\u5316\u5b9e\u73b0\u66f4\u4f18\u8868\u73b0", "conclusion": "\u4e3aLLM Agent\u5728\u4ea4\u6613\u7cfb\u7edf\u4e2d\u7684\u7ed3\u6784\u8bbe\u8ba1\u548c\u4efb\u52a1\u914d\u7f6e\u63d0\u4f9b\u5b9e\u8df5\u6307\u5bfc\uff0c\u5f3a\u8c03\u4efb\u52a1\u7c92\u5ea6\u7ec6\u5316\u4e0e\u51b3\u7b56\u504f\u597d\u5bf9\u9f50\u7684\u91cd\u8981\u6027"}}
{"id": "2602.22673", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2602.22673", "abs": "https://arxiv.org/abs/2602.22673", "authors": ["Md Tanvir Hasan Turja"], "title": "Forecasting Antimicrobial Resistance Trends Using Machine Learning on WHO GLASS Surveillance Data: A Retrieval-Augmented Generation Approach for Policy Decision Support", "comment": "18 pages, 4 figures, code and data available at https://github.com/TanvirTurja", "summary": "Antimicrobial resistance (AMR) is a growing global crisis projected to cause 10 million deaths per year by 2050. While the WHO Global Antimicrobial Resistance and Use Surveillance System (GLASS) provides standardized surveillance data across 44 countries, few studies have applied machine learning to forecast population-level resistance trends from this data. This paper presents a two-component framework for AMR trend forecasting and evidence-grounded policy decision support. We benchmark six models -- Naive, Linear Regression, Ridge Regression, XGBoost, LightGBM, and LSTM -- on 5,909 WHO GLASS observations across six WHO regions (2021-2023). XGBoost achieved the best performance with a test MAE of 7.07% and R-squared of 0.854, outperforming the naive baseline by 83.1%. Feature importance analysis identified the prior-year resistance rate as the dominant predictor (50.5% importance), while regional MAE ranged from 4.16% (European Region) to 10.14% (South-East Asia Region). We additionally implemented a Retrieval-Augmented Generation (RAG) pipeline combining a ChromaDB vector store of WHO policy documents with a locally deployed Phi-3 Mini language model, producing source-attributed, hallucination-constrained policy answers. Code and data are available at https://github.com/TanvirTurja", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u53cc\u7ec4\u4ef6\u6846\u67b6\uff0c\u7528\u4e8e\u57fa\u4e8eWHO GLASS\u6570\u636e\u9884\u6d4b\u6297\u83cc\u836f\u7269\u8010\u836f\u6027(AMR)\u8d8b\u52bf\u3002XGBoost\u8868\u73b0\u6700\u4f73\uff08\u6d4b\u8bd5MAE\u4e3a7.07%\uff0cR\u00b2\u4e3a0.854\uff09\uff0c\u4e0a\u4e00\u5e74\u7684\u8010\u836f\u7387\u662f\u6700\u91cd\u8981\u7684\u9884\u6d4b\u56e0\u5b50\u3002\u7ed3\u5408ChromaDB\u548cPhi-3 Mini\u7684RAG\u7ba1\u9053\u53ef\u751f\u6210\u57fa\u4e8e\u8bc1\u636e\u7684\u653f\u7b56\u5efa\u8bae\u3002", "motivation": "\u6297\u83cc\u836f\u7269\u8010\u836f\u6027(AMR)\u662f\u4e00\u4e2a\u65e5\u76ca\u4e25\u91cd\u7684\u5168\u7403\u6027\u5371\u673a\uff0c\u9884\u8ba1\u52302050\u5e74\u6bcf\u5e74\u5c06\u5bfc\u81f41000\u4e07\u4eba\u6b7b\u4ea1\u3002\u867d\u7136WHO GLASS\u572844\u4e2a\u56fd\u5bb6\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u76d1\u6d4b\u6570\u636e\uff0c\u4f46\u5f88\u5c11\u6709\u7814\u7a76\u5e94\u7528\u673a\u5668\u5b66\u4e60\u4ece\u8be5\u6570\u636e\u9884\u6d4b\u4eba\u7fa4\u6c34\u5e73\u7684\u8010\u836f\u8d8b\u52bf\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u53cc\u7ec4\u4ef6\u6846\u67b6\uff1a(1) AMR\u8d8b\u52bf\u9884\u6d4b\uff0c\u5728\u6765\u81ea\u516d\u4e2aWHO\u533a\u57df2021-2023\u5e74\u76845,909\u6761WHO GLASS\u89c2\u6d4b\u6570\u636e\u4e0a\uff0c\u5bf9\u516d\u79cd\u6a21\u578b\uff08Naive\u3001\u7ebf\u6027\u56de\u5f52\u3001\u5cad\u56de\u5f52\u3001XGBoost\u3001LightGBM\u548cLSTM\uff09\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff1b(2) \u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7ba1\u9053\uff0c\u7ed3\u5408WHO\u653f\u7b56\u6587\u6863\u7684ChromaDB\u5411\u91cf\u5b58\u50a8\u548c\u672c\u5730\u90e8\u7f72\u7684Phi-3 Mini\u8bed\u8a00\u6a21\u578b\u6765\u751f\u6210\u653f\u7b56\u5efa\u8bae\u3002", "result": "XGBoost\u8868\u73b0\u6700\u4f73\uff0c\u6d4b\u8bd5MAE\u4e3a7.07%\uff0cR\u00b2\u4e3a0.854\uff0c\u6bd4\u6734\u7d20\u57fa\u7ebf\u9ad8\u51fa83.1%\u3002\u4e0a\u4e00\u5e74\u7684\u8010\u836f\u7387\u662f\u4e3b\u8981\u9884\u6d4b\u56e0\u5b50\uff0850.5%\u7684\u91cd\u8981\u6027\uff09\u3002\u533a\u57dfMAE\u4ece\u6b27\u6d32\u533a\u57df\u76844.16%\u5230\u4e1c\u5357\u4e9a\u533a\u57df\u768410.14%\u4e0d\u7b49\u3002RAG\u7ba1\u9053\u6210\u529f\u751f\u6210\u4e86\u6709\u6765\u6e90\u5f52\u5c5e\u3001\u5e7b\u89c9\u53d7\u9650\u7684\u653f\u7b56\u7b54\u6848\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u6709\u6548\u7684AMR\u8d8b\u52bf\u9884\u6d4b\u548c\u57fa\u4e8e\u8bc1\u636e\u7684\u653f\u7b56\u51b3\u7b56\u652f\u6301\u3002\u4ee3\u7801\u548c\u6570\u636e\u53ef\u5728https://github.com/TanvirTurja\u516c\u5f00\u83b7\u53d6\u3002"}}
{"id": "2602.21585", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.21585", "abs": "https://arxiv.org/abs/2602.21585", "authors": ["Sweta Karlekar", "Carolina Zheng", "Magnus Saebo", "Nicolas Beltran-Velez", "Shuyang Yu", "John Bowlan", "Michal Kucer", "David Blei"], "title": "Duel-Evolve: Reward-Free Test-Time Scaling via LLM Self-Preferences", "comment": null, "summary": "Many applications seek to optimize LLM outputs at test time by iteratively proposing, scoring, and refining candidates over a discrete output space. Existing methods use a calibrated scalar evaluator for the target objective to guide search, but for many tasks such scores are unavailable, too sparse, or unreliable. Pairwise comparisons, by contrast, are often easier to elicit, still provide useful signal on improvement directions, and can be obtained from the LLM itself without external supervision. Building on this observation, we introduce Duel-Evolve, an evolutionary optimization algorithm that replaces external scalar rewards with pairwise preferences elicited from the same LLM used to generate candidates. Duel-Evolve aggregates these noisy candidate comparisons via a Bayesian Bradley-Terry model, yielding uncertainty-aware estimates of candidate quality. These quality estimates guide allocation of the comparison budget toward plausible optima using Double Thompson Sampling, as well as selection of high-quality parents to generate improved candidates. We evaluate Duel-Evolve on MathBench, where it achieves 20 percentage points higher accuracy over existing methods and baselines, and on LiveCodeBench, where it improves over comparable iterative methods by over 12 percentage points. Notably, the method requires no reward model, no ground-truth labels during search, and no hand-crafted scoring function. Results show that pairwise self-preferences provide strong optimization signal for test-time improvement over large, discrete output spaces.", "AI": {"tldr": "\u8fd9\u662f\u4e00\u4e2a\u5173\u4e8eDuel-Evolve\u7684\u8bba\u6587\u6458\u8981\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7LLM\u81ea\u8eab\u7684\u6210\u5bf9\u504f\u597d\u800c\u975e\u5916\u90e8\u5956\u52b1\u6765\u4f18\u5316LLM\u8f93\u51fa\uff0c\u4f7f\u7528\u8d1d\u53f6\u65afBradley-Terry\u6a21\u578b\u548c\u53cc\u91cd\u6c64\u666e\u68ee\u91c7\u6837\uff0c\u5728\u6570\u5b66\u548c\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u4f18\u5316LLM\u8f93\u51fa\u7684\u65b9\u6cd5\u4f9d\u8d56\u6821\u51c6\u7684\u6807\u91cf\u8bc4\u4f30\u5668\uff0c\u4f46\u8fd9\u4e9b\u8bc4\u4f30\u5668\u5f80\u5f80\u4e0d\u53ef\u7528\u3001\u8fc7\u4e8e\u7a00\u758f\u6216\u4e0d\u53ef\u9760\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u6210\u5bf9\u6bd4\u8f83\u66f4\u5bb9\u6613\u83b7\u53d6\uff0c\u80fd\u63d0\u4f9b\u6539\u8fdb\u65b9\u5411\u7684\u4fe1\u53f7\uff0c\u4e14\u65e0\u9700\u5916\u90e8\u76d1\u7763\u5373\u53ef\u4eceLLM\u81ea\u8eab\u83b7\u5f97\u3002", "method": "Duel-Evolve\u662f\u4e00\u79cd\u8fdb\u5316\u4f18\u5316\u7b97\u6cd5\uff0c\u7528LLM\u81ea\u8eab\u4ea7\u751f\u7684\u6210\u5bf9\u504f\u597d\u66ff\u4ee3\u5916\u90e8\u6807\u91cf\u5956\u52b1\uff1b\u901a\u8fc7\u8d1d\u53f6\u65afBradley-Terry\u6a21\u578b\u805a\u5408\u8fd9\u4e9b\u5e26\u566a\u58f0\u7684\u6bd4\u8f83\uff0c\u5f97\u5230\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u5019\u9009\u8d28\u91cf\u4f30\u8ba1\uff1b\u4f7f\u7528\u53cc\u91cd\u6c64\u666e\u68ee\u91c7\u6837\u6307\u5bfc\u6bd4\u8f83\u9884\u7b97\u5206\u914d\uff0c\u5e76\u9009\u62e9\u9ad8\u8d28\u91cf\u7236\u4ee3\u751f\u6210\u6539\u8fdb\u7684\u5019\u9009\u3002", "result": "\u5728MathBench\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad8\u51fa20\u4e2a\u767e\u5206\u70b9\u51c6\u786e\u7387\uff1b\u5728LiveCodeBench\u4e0a\u6bd4\u53ef\u6bd4\u8fed\u4ee3\u65b9\u6cd5\u9ad8\u51fa\u8d85\u8fc712\u4e2a\u767e\u5206\u70b9\uff1b\u65e0\u9700\u5956\u52b1\u6a21\u578b\u3001\u641c\u7d22\u671f\u95f4\u65e0\u9700\u771f\u5b9e\u6807\u7b7e\u3001\u65e0\u9700\u624b\u5de5\u8bbe\u8ba1\u8bc4\u5206\u51fd\u6570\u3002", "conclusion": "\u6210\u5bf9\u81ea\u504f\u597d\u4e3a\u5927\u578b\u79bb\u6563\u8f93\u51fa\u7a7a\u95f4\u4e0a\u7684\u6d4b\u8bd5\u65f6\u6539\u8fdb\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u4f18\u5316\u4fe1\u53f7\u3002"}}
{"id": "2602.22681", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22681", "abs": "https://arxiv.org/abs/2602.22681", "authors": ["Shuchen Zhu", "Rizhen Hu", "Mingze Wang", "Mou Sun", "Xue Wang", "Kun Yuan", "Zaiwen Wen"], "title": "Accelerating LLM Pre-Training through Flat-Direction Dynamics Enhancement", "comment": null, "summary": "Pre-training Large Language Models requires immense computational resources, making optimizer efficiency essential. The optimization landscape is highly anisotropic, with loss reduction driven predominantly by progress along flat directions. While matrix-based optimizers such as Muon and SOAP leverage fine-grained curvature information to outperform AdamW, their updates tend toward isotropy -- relatively conservative along flat directions yet potentially aggressive along sharp ones. To address this limitation, we first establish a unified Riemannian Ordinary Differential Equation (ODE) framework that elucidates how common adaptive algorithms operate synergistically: the preconditioner induces a Riemannian geometry that mitigates ill-conditioning, while momentum serves as a Riemannian damping term that promotes convergence. Guided by these insights, we propose LITE, a generalized acceleration strategy that enhances training dynamics by applying larger Hessian damping coefficients and learning rates along flat trajectories. Extensive experiments demonstrate that LITE significantly accelerates both Muon and SOAP across diverse architectures (Dense, MoE), parameter scales (130M--1.3B), datasets (C4, Pile), and learning-rate schedules (cosine, warmup-stable-decay). Theoretical analysis confirms that LITE facilitates faster convergence along flat directions in anisotropic landscapes, providing a principled approach to efficient LLM pre-training. The code is available at https://github.com/SHUCHENZHU/LITE.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faLITE\u4f18\u5316\u7b56\u7565\uff0c\u901a\u8fc7Riemannian ODE\u6846\u67b6\u5206\u6790\u81ea\u9002\u5e94\u4f18\u5316\u5668\uff0c\u5728\u5404\u5411\u5f02\u6027\u7684LLM\u8bad\u7ec3 landscape\u4e0a\u6cbf\u5e73\u5766\u65b9\u5411\u5e94\u7528\u66f4\u5927\u7684\u963b\u5c3c\u548c\u5b66\u4e60\u7387\uff0c\u663e\u8457\u52a0\u901fMuon\u548cSOAP\u7b49\u4f18\u5316\u5668\u3002", "motivation": "LLM\u9884\u8bad\u7ec3\u9700\u5de8\u5927\u7b97\u529b\uff0c\u4f18\u5316\u5668\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u77e9\u9635\u4f18\u5316\u5668\uff08\u5982Muon\u3001SOAP\uff09\u867d\u5229\u7528\u66f2\u7387\u4fe1\u606f\uff0c\u4f46\u66f4\u65b0\u8d8b\u4e8e\u5404\u5411\u540c\u6027\u2014\u2014\u5728\u5e73\u5766\u65b9\u5411\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u5728\u5c16\u9510\u65b9\u5411\u8fc7\u4e8e\u6fc0\u8fdb\uff0c\u5b58\u5728\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "\u5efa\u7acb\u7edf\u4e00Riemannian ODE\u6846\u67b6\uff1a\u9884\u5904\u7406\u5b50\u6784\u5efaRiemannian\u51e0\u4f55\u6539\u5584\u75c5\u6001\u6761\u4ef6\uff0c\u52a8\u91cf\u4f5c\u4e3a\u963b\u5c3c\u9879\u4fc3\u8fdb\u6536\u655b\u3002\u57fa\u4e8e\u6b64\u63d0\u51faLITE\u7b56\u7565\uff0c\u5728\u5e73\u5766\u8f68\u8ff9\u4e0a\u5e94\u7528\u66f4\u5927\u7684Hessian\u963b\u5c3c\u7cfb\u6570\u4e0e\u5b66\u4e60\u7387\u3002", "result": "LITE\u663e\u8457\u52a0\u901fMuon\u548cSOAP\uff0c\u5728Dense/MoE\u67b6\u6784\u3001130M-1.3B\u53c2\u6570\u89c4\u6a21\u3001C4/Pile\u6570\u636e\u96c6\u53ca\u591a\u79cd\u5b66\u4e60\u7387\u8c03\u5ea6\u4e0b\u5747\u6709\u6548\u3002", "conclusion": "\u7406\u8bba\u5206\u6790\u8bc1\u5b9eLITE\u5728\u5e73\u5766\u65b9\u5411\u5b9e\u73b0\u66f4\u5feb\u6536\u655b\uff0c\u4e3a\u9ad8\u6548LLM\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u65b9\u6cd5\u3002"}}
{"id": "2602.22251", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22251", "abs": "https://arxiv.org/abs/2602.22251", "authors": ["Alex Morehead", "Miruna Cretu", "Antonia Panescu", "Rishabh Anand", "Maurice Weiler", "Tynan Perez", "Samuel Blau", "Steven Farrell", "Wahid Bhimji", "Anubhav Jain", "Hrushikesh Sahasrabuddhe", "Pietro Lio", "Tommi Jaakkola", "Rafael Gomez-Bombarelli", "Rex Ying", "N. Benjamin Erichson", "Michael W. Mahoney"], "title": "Zatom-1: A Multimodal Flow Foundation Model for 3D Molecules and Materials", "comment": null, "summary": "General-purpose 3D chemical modeling encompasses molecules and materials, requiring both generative and predictive capabilities. However, most existing AI approaches are optimized for a single domain (molecules or materials) and a single task (generation or prediction), which limits representation sharing and transfer. We introduce Zatom-1, the first foundation model that unifies generative and predictive learning of 3D molecules and materials. Zatom-1 is a Transformer trained with a multimodal flow matching objective that jointly models discrete atom types and continuous 3D geometries. This approach supports scalable pretraining with predictable gains as model capacity increases, while enabling fast and stable sampling. We use joint generative pretraining as a universal initialization for downstream multi-task prediction of properties, energies, and forces. Empirically, Zatom-1 matches or outperforms specialized baselines on both generative and predictive benchmarks, while reducing the generative inference time by more than an order of magnitude. Our experiments demonstrate positive predictive transfer between chemical domains from joint generative pretraining: modeling materials during pretraining improves molecular property prediction accuracy.", "AI": {"tldr": "Zatom-1\u662f\u9996\u4e2a\u7edf\u4e003D\u5206\u5b50\u548c\u6750\u6599\u751f\u6210\u4e0e\u9884\u6d4b\u7684\u57fa\u7840\u6a21\u578b\uff0c\u91c7\u7528\u591a\u6a21\u6001\u6d41\u5339\u914dTransformer\u67b6\u6784\uff0c\u5728\u751f\u6210\u548c\u9884\u6d4b\u4efb\u52a1\u4e0a\u5339\u914d\u6216\u8d85\u8d8a\u4e13\u7528\u57fa\u7ebf\uff0c\u5e76\u5c06\u751f\u6210\u63a8\u7406\u65f6\u95f4\u7f29\u77ed10\u500d\u4ee5\u4e0a\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u8de8\u5316\u5b66\u57df\u7684\u6b63\u5411\u77e5\u8bc6\u8fc1\u79fb\u3002", "motivation": "\u901a\u75283D\u5316\u5b66\u5efa\u6a21\u9700\u8981\u751f\u6210\u548c\u9884\u6d4b\u53cc\u91cd\u80fd\u529b\uff0c\u4f46\u73b0\u6709AI\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u4e00\u9886\u57df(\u5206\u5b50\u6216\u6750\u6599)\u548c\u5355\u4e00\u4efb\u52a1(\u751f\u6210\u6216\u9884\u6d4b)\uff0c\u9650\u5236\u4e86\u8868\u5f81\u5171\u4eab\u548c\u8fc1\u79fb\u5b66\u4e60\u6548\u679c\u3002", "method": "Zatom-1\u91c7\u7528\u591a\u6a21\u6001\u6d41\u5339\u914d\u76ee\u6807\u7684Transformer\uff0c\u8054\u5408\u5efa\u6a21\u79bb\u6563\u539f\u5b50\u7c7b\u578b\u548c\u8fde\u7eed3D\u51e0\u4f55\u7ed3\u6784\uff0c\u652f\u6301\u53ef\u6269\u5c55\u9884\u8bad\u7ec3\uff0c\u5e76\u4f5c\u4e3a\u4e0b\u6e38\u591a\u4efb\u52a1\u9884\u6d4b(\u6027\u8d28\u3001\u80fd\u91cf\u3001\u529b)\u7684\u901a\u7528\u521d\u59cb\u5316\u3002", "result": "\u5728\u751f\u6210\u548c\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5339\u914d\u6216\u8d85\u8d8a\u4e13\u7528\u57fa\u7ebf\uff0c\u751f\u6210\u63a8\u7406\u65f6\u95f4\u51cf\u5c11\u4e00\u4e2a\u6570\u91cf\u7ea7\u4ee5\u4e0a\uff1b\u9884\u8bad\u7ec3\u4e2d\u5efa\u6a21\u6750\u6599\u80fd\u63d0\u5347\u5206\u5b50\u6027\u8d28\u9884\u6d4b\u7cbe\u5ea6\uff0c\u8bc1\u5b9e\u8de8\u5316\u5b66\u57df\u6b63\u5411\u8fc1\u79fb\u3002", "conclusion": "\u8054\u5408\u751f\u6210\u5f0f\u9884\u8bad\u7ec3\u6210\u529f\u5b9e\u73b0\u4e86\u5206\u5b50\u548c\u6750\u6599\u9886\u57df\u7684\u7edf\u4e00\u5efa\u6a21\u4e0e\u77e5\u8bc6\u8fc1\u79fb\uff0c\u4e3a\u5f00\u53d1\u901a\u7528\u5316\u5b66AI\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2602.22847", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.22847", "abs": "https://arxiv.org/abs/2602.22847", "authors": ["Anna Van Elst", "Kerrian Le Caillec", "Igor Colin", "Stephan Cl\u00e9men\u00e7on"], "title": "Decentralized Ranking Aggregation: Gossip Algorithms for Borda and Copeland Consensus", "comment": "8 pages, 2 figures", "summary": "The concept of ranking aggregation plays a central role in preference analysis, and numerous algorithms for calculating median rankings, often originating in social choice theory, have been documented in the literature, offering theoretical guarantees in a centralized setting, i.e., when all the ranking data to be aggregated can be brought together in a single computing unit. For many technologies (e.g. peer-to-peer networks, IoT, multi-agent systems), extending the ability to calculate consensus rankings with guarantees in a decentralized setting, i.e., when preference data is initially distributed across a communicating network, remains a major methodological challenge. Indeed, in recent years, the literature on decentralized computation has mainly focused on computing or optimizing statistics such as arithmetic means using gossip algorithms. The purpose of this article is precisely to study how to achieve reliable consensus on collective rankings using classical rules (e.g. Borda, Copeland) in a decentralized setting, thereby raising new questions, robustness to corrupted nodes, and scalability through reduced communication costs in particular. The approach proposed and analyzed here relies on random gossip communication, allowing autonomous agents to compute global ranking consensus using only local interactions, without coordination or central authority.\n  We provide rigorous convergence guarantees, including explicit rate bounds, for the Borda and Copeland consensus methods. Beyond these rules, we also provide a decentralized implementation of consensus according to the median rank rule and local Kemenization. Extensive empirical evaluations on various network topologies and real and synthetic ranking datasets demonstrate that our algorithms converge quickly and reliably to the correct ranking aggregation.", "AI": {"tldr": "This paper develops decentralized ranking aggregation algorithms using gossip communication, providing theoretical convergence guarantees for Borda/Copeland methods and demonstrating fast, reliable consensus across distributed networks.", "motivation": "Traditional ranking aggregation algorithms work well in centralized settings but face challenges in decentralized environments like P2P networks, IoT, and multi-agent systems. Existing decentralized literature focuses mainly on computing arithmetic means, leaving ranking consensus underexplored despite its importance in preference analysis.", "method": "Proposes a random gossip communication framework where autonomous agents achieve global ranking consensus through local interactions using classical rules (Borda, Copeland, median rank, local Kemenization) without central coordination.", "result": "Provides rigorous convergence guarantees with explicit rate bounds for Borda and Copeland methods, decentralized implementations of median rank rule and local Kemenization, and empirical validation showing fast, reliable convergence across various network topologies and real/synthetic datasets.", "conclusion": "The algorithms successfully enable decentralized ranking aggregation with strong theoretical guarantees and practical performance, addressing robustness to corrupted nodes and scalability through reduced communication costs."}}
{"id": "2602.22428", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22428", "abs": "https://arxiv.org/abs/2602.22428", "authors": ["Daniel Geyfman", "Felix Draxler", "Jan Groeneveld", "Hyunsoo Lee", "Theofanis Karaletsos", "Stephan Mandt"], "title": "Calibrated Test-Time Guidance for Bayesian Inference", "comment": "Preprint. Under review", "summary": "Test-time guidance is a widely used mechanism for steering pretrained diffusion models toward outcomes specified by a reward function. Existing approaches, however, focus on maximizing reward rather than sampling from the true Bayesian posterior, leading to miscalibrated inference. In this work, we show that common test-time guidance methods do not recover the correct posterior distribution and identify the structural approximations responsible for this failure. We then propose consistent alternative estimators that enable calibrated sampling from the Bayesian posterior. We significantly outperform previous methods on a set of Bayesian inference tasks, and match state-of-the-art in black hole image reconstruction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6307\u51fa\u5f53\u524d\u6d4b\u8bd5\u65f6\u5f15\u5bfc\u65b9\u6cd5\u65e0\u6cd5\u6b63\u786e\u91c7\u6837\u8d1d\u53f6\u65af\u540e\u9a8c\u5206\u5e03\uff0c\u63d0\u51fa\u4e86\u4e00\u81f4\u7684\u66ff\u4ee3\u4f30\u8ba1\u5668\u4ee5\u5b9e\u73b0\u6821\u51c6\u91c7\u6837\uff0c\u5728\u8d1d\u53f6\u65af\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u8d85\u8d8a\u5148\u524d\u65b9\u6cd5\uff0c\u5e76\u5728\u9ed1\u6d1e\u56fe\u50cf\u91cd\u5efa\u4e2d\u8fbe\u5230\u9876\u5c16\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u5f15\u5bfc\u65b9\u6cd5\u4e13\u6ce8\u4e8e\u6700\u5927\u5316\u5956\u52b1\u51fd\u6570\u800c\u975e\u4ece\u771f\u5b9e\u8d1d\u53f6\u65af\u540e\u9a8c\u91c7\u6837\uff0c\u5bfc\u81f4\u63a8\u7406\u7ed3\u679c\u4e0d\u6821\u51c6\u3002", "method": "\u8bc6\u522b\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u5bfc\u81f4\u540e\u9a8c\u5206\u5e03\u9519\u8bef\u7684\u7684\u7ed3\u6784\u6027\u8fd1\u4f3c\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u5177\u6709\u4e00\u81f4\u6027\u7684\u66ff\u4ee3\u4f30\u8ba1\u5668\u6765\u542f\u7528\u8d1d\u53f6\u65af\u540e\u9a8c\u7684\u6821\u51c6\u91c7\u6837\u3002", "result": "\u5728\u8d1d\u53f6\u65af\u63a8\u7406\u4efb\u52a1\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u5e76\u5728\u9ed1\u6d1e\u56fe\u50cf\u91cd\u5efa\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6b63\u786e\u5730\u4ece\u8d1d\u53f6\u65af\u540e\u9a8c\u5206\u5e03\u4e2d\u8fdb\u884c\u6821\u51c6\u91c7\u6837\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6d4b\u8bd5\u65f6\u5f15\u5bfc\u65b9\u6cd5\u7684\u6838\u5fc3\u7f3a\u9677\u3002"}}
{"id": "2602.22438", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22438", "abs": "https://arxiv.org/abs/2602.22438", "authors": ["Uttamasha Anjally Oyshi", "Susan Gauch"], "title": "From Bias to Balance: Fairness-Aware Paper Recommendation for Equitable Peer Review", "comment": null, "summary": "Despite frequent double-blind review, systemic biases related to author demographics still disadvantage underrepresented groups. We start from a simple hypothesis: if a post-review recommender is trained with an explicit fairness regularizer, it should increase inclusion without degrading quality. To test this, we introduce Fair-PaperRec, a Multi-Layer Perceptron (MLP) with a differentiable fairness loss over intersectional attributes (e.g., race, country) that re-ranks papers after double-blind review. We first probe the hypothesis on synthetic datasets spanning high, moderate, and near-fair biases. Across multiple randomized runs, these controlled studies map where increasing the fairness weight strengthens macro/micro diversity while keeping utility approximately stable, demonstrating robustness and adaptability under varying disparity levels. We then carry the hypothesis into the original setting, conference data from ACM Special Interest Group on Computer-Human Interaction (SIGCHI), Designing Interactive Systems (DIS), and Intelligent User Interfaces (IUI). In this real-world scenario, an appropriately tuned configuration of Fair-PaperRec achieves up to a 42.03% increase in underrepresented-group participation with at most a 3.16% change in overall utility relative to the historical selection. Taken together, the synthetic-to-original progression shows that fairness regularization can act as both an equity mechanism and a mild quality regularizer, especially in highly biased regimes. By first analyzing the behavior of the fairness parameters under controlled conditions and then validating them on real submissions, Fair-PaperRec offers a practical, equity-focused framework for post-review paper selection that preserves, and in some settings can even enhance, measured scholarly quality.", "AI": {"tldr": "Fair-PaperRec uses differentiable fairness regularization in an MLP model to re-rank academic papers post double-blind review, increasing underrepresented group participation by up to 42% with minimal impact on overall quality.", "motivation": "Systemic biases against underrepresented groups persist in double-blind academic paper reviews; the authors hypothesize that explicit fairness regularization in post-review recommenders can improve inclusion without sacrificing quality.", "method": "Proposes Fair-PaperRec, a Multi-Layer Perceptron (MLP) with a differentiable fairness loss function over intersectional attributes (e.g., race, country) to re-rank papers. Tests on synthetic datasets with controlled bias levels and real-world conference data (SIGCHI, DIS, IUI).", "result": "Synthetic tests show fairness regularization boosts macro/micro diversity while maintaining utility across varying bias regimes. Real-world deployment achieves up to 42.03% increase in underrepresented-group participation with \u22643.16% change in overall utility compared to historical selection.", "conclusion": "Fairness regularization acts as both an equity mechanism and mild quality regularizer, especially in highly biased contexts. Fair-PaperRec provides a practical framework for equitable paper selection that preserves or enhances scholarly quality."}}
{"id": "2602.23008", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23008", "abs": "https://arxiv.org/abs/2602.23008", "authors": ["Zeyuan Liu", "Jeonghye Kim", "Xufang Luo", "Dongsheng Li", "Yuqing Yang"], "title": "Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization", "comment": "Accepted to ICLR 2026", "summary": "Exploration remains the key bottleneck for large language model agents trained with reinforcement learning. While prior methods exploit pretrained knowledge, they fail in environments requiring the discovery of novel states. We propose Exploratory Memory-Augmented On- and Off-Policy Optimization (EMPO$^2$), a hybrid RL framework that leverages memory for exploration and combines on- and off-policy updates to make LLMs perform well with memory while also ensuring robustness without it. On ScienceWorld and WebShop, EMPO$^2$ achieves 128.6% and 11.3% improvements over GRPO, respectively. Moreover, in out-of-distribution tests, EMPO$^2$ demonstrates superior adaptability to new tasks, requiring only a few trials with memory and no parameter updates. These results highlight EMPO$^2$ as a promising framework for building more exploratory and generalizable LLM-based agents.", "AI": {"tldr": "EMPO\u00b2\u6846\u67b6\u901a\u8fc7\u8bb0\u5fc6\u589e\u5f3a\u548c\u6df7\u5408\u7b56\u7565\u4f18\u5316\u89e3\u51b3LLM\u667a\u80fd\u4f53\u7684\u63a2\u7d22\u74f6\u9888\uff0c\u5728ScienceWorld\u548cWebShop\u4e0a\u663e\u8457\u8d85\u8d8aGRPO\uff0c\u5e76\u5c55\u73b0\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684LLM\u667a\u80fd\u4f53\u5728\u63a2\u7d22\u65b0\u72b6\u6001\u65f6\u5b58\u5728\u74f6\u9888\uff0c\u4f9d\u8d56\u9884\u8bad\u7ec3\u77e5\u8bc6\u7684\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94\u9700\u53d1\u73b0\u65b0\u72b6\u6001\u7684\u73af\u5883\u3002", "method": "\u63d0\u51fa\u8bb0\u5fc6\u589e\u5f3a\u7684\u6df7\u5408\u7b56\u7565\u4f18\u5316\u6846\u67b6\uff08EMPO\u00b2\uff09\uff0c\u5229\u7528\u8bb0\u5fc6\u4fc3\u8fdb\u63a2\u7d22\uff0c\u7ed3\u5408\u5728\u7ebf\u548c\u79bb\u7ebf\u7b56\u7565\u66f4\u65b0\u63d0\u5347\u8bb0\u5fc6\u5b58\u5728/\u7f3a\u5931\u65f6\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728ScienceWorld\u548cWebShop\u4e0a\u5206\u522b\u53d6\u5f97128.6%\u548c11.3%\u7684\u6027\u80fd\u63d0\u5347\uff1bOOD\u6d4b\u8bd5\u4e2d\u4ec5\u9700\u5c11\u91cf\u8bb0\u5fc6\u8bd5\u9519\u5373\u53ef\u9002\u5e94\u65b0\u4efb\u52a1\uff0c\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u3002", "conclusion": "EMPO\u00b2\u4e3a\u6784\u5efa\u66f4\u5177\u63a2\u7d22\u6027\u548c\u6cdb\u5316\u6027\u7684LLM\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2602.23116", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.23116", "abs": "https://arxiv.org/abs/2602.23116", "authors": ["Junghyun Lee", "Minju Hong", "Kwang-Sung Jun", "Chulhee Yun", "Se-Young Yun"], "title": "Regularized Online RLHF with Generalized Bilinear Preferences", "comment": "43 pages, 1 table", "summary": "We consider the problem of contextual online RLHF with general preferences, where the goal is to identify the Nash Equilibrium. We adopt the Generalized Bilinear Preference Model (GBPM) to capture potentially intransitive preferences via low-rank, skew-symmetric matrices. We investigate general preference learning with any strongly convex regularizer (where $\u03b7^{-1}$ is the regularization strength), generalizing beyond prior works limited to reverse KL-regularization. Central to our analysis is proving that the dual gap of the greedy policy is bounded by the square of the estimation error - a result derived solely from strong convexity and the skew-symmetricity of GBPM.Building on this insight and a feature diversity assumption, we establish two regret bounds via two simple algorithms: (1) Greedy Sampling achieves polylogarithmic, $e^{O(\u03b7)}$-free regret $\\tilde{O}(\u03b7d^4 (\\log T)^2)$. (2) Explore-Then-Commit achieves $\\mathrm{poly}(d)$-free regret $\\tilde{O}(\\sqrt{\u03b7r T})$ by exploiting the low-rank structure; this is the first statistically efficient guarantee for online RLHF in high-dimensions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5e7f\u4e49\u504f\u597d\u4e0b\u7684\u4e0a\u4e0b\u6587\u5728\u7ebfRLHF\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8eGBPM\u6a21\u578b\u7684\u8d2a\u5a6a\u91c7\u6837\u548c\u63a2\u7d22-\u7136\u540e-\u627f\u8bfa\u7b97\u6cd5\uff0c\u5206\u522b\u5b9e\u73b0\u4e86\u591a\u9879\u5f0f\u5bf9\u6570\u9057\u61be\u754c\u548c\u9996\u4e2a\u9ad8\u7ef4\u7edf\u8ba1\u9ad8\u6548\u4fdd\u8bc1\u3002", "motivation": "\u89e3\u51b3\u5177\u6709\u5e7f\u4e49\u504f\u597d\u7684\u4e0a\u4e0b\u6587\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4eba\u7c7b\u53cd\u9988\u95ee\u9898\uff0c\u76ee\u6807\u662f\u627e\u5230\u7eb3\u4ec0\u5747\u8861\uff0c\u5e76\u5c06\u5148\u524d\u4ec5\u9650\u4e8e\u53cd\u5411KL\u6b63\u5219\u5316\u7684\u7814\u7a76\u63a8\u5e7f\u5230\u4efb\u610f\u5f3a\u51f8\u6b63\u5219\u5316\u5668\u3002", "method": "\u91c7\u7528\u5e7f\u4e49\u53cc\u7ebf\u6027\u504f\u597d\u6a21\u578b\uff08GBPM\uff09\u901a\u8fc7\u4f4e\u79e9\u3001\u659c\u5bf9\u79f0\u77e9\u9635\u6355\u6349\u975e\u4f20\u9012\u6027\u504f\u597d\uff1b\u57fa\u4e8e\u5f3a\u51f8\u6027\u548c\u659c\u5bf9\u79f0\u6027\u8bc1\u660e\u8d2a\u5a6a\u7b56\u7565\u5bf9\u5076\u95f4\u9699\u53d7\u4f30\u8ba1\u8bef\u5dee\u5e73\u65b9\u7684\u7ea6\u675f\uff1b\u5728\u7279\u5f81\u591a\u6837\u6027\u5047\u8bbe\u4e0b\u8bbe\u8ba1\u4e24\u79cd\u7b97\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u6838\u5fc3\u7406\u8bba\u6d1e\u5bdf\u2014\u2014\u5bf9\u5076\u95f4\u9699\u4e0e\u4f30\u8ba1\u8bef\u5dee\u7684\u5e73\u65b9\u5173\u7cfb\uff1b\u8d2a\u5a6a\u91c7\u6837\u7b97\u6cd5\u8fbe\u5230\u00d5(\u03b7d^4 (log T)^2)\u7684\u6307\u6570\u65e0\u5173\u9057\u61be\u754c\uff1b\u63a2\u7d22-\u7136\u540e-\u627f\u8bfa\u7b97\u6cd5\u8fbe\u5230\u00d5(\u221a(\u03b7rT))\u7684\u4f4e\u79e9\u65e0\u5173\u9057\u61be\u754c\uff0c\u662f\u5728\u7ebfRLHF\u5728\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u7684\u9996\u4e2a\u7edf\u8ba1\u9ad8\u6548\u7ed3\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u7a81\u7834\u4e86\u5728\u7ebfRLHF\u7684\u7406\u8bba\u9650\u5236\uff0c\u4e3a\u5e7f\u4e49\u504f\u597d\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5206\u6790\u5de5\u5177\u548c\u7b97\u6cd5\u4fdd\u8bc1\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7ef4\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u7edf\u8ba1\u6548\u7387\u3002"}}
{"id": "2602.23349", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23349", "abs": "https://arxiv.org/abs/2602.23349", "authors": ["Jose Javier Gonzalez Ortiz", "Abhay Gupta", "Chris Renard", "Davis Blalock"], "title": "FlashOptim: Optimizers for Memory Efficient Training", "comment": "Source code is available at https://github.com/databricks/flashoptim", "summary": "Standard mixed-precision training of neural networks requires many bytes of accelerator memory for each model parameter. These bytes reflect not just the parameter itself, but also its gradient and one or more optimizer state variables. With each of these values typically requiring 4 bytes, training even a 7 billion parameter model can be impractical for researchers with less than 100GB of accelerator memory.\n  We introduce FlashOptim, a suite of optimizations that reduces per-parameter memory by over 50% while preserving model quality and API compatibility. Our approach introduces two key techniques. First, we improve master weight splitting by finding and exploiting a tight bound on its quantization error. Second, we design companding functions that greatly reduce the error in 8-bit optimizer state quantization. Together with 16-bit gradients, these techniques reduce AdamW memory from 16 bytes to 7 bytes per parameter, or 5 bytes with gradient release. They also cut model checkpoint sizes by more than half.\n  Experiments with FlashOptim applied to SGD, AdamW, and Lion show no measurable quality degradation on any task from a collection of standard vision and language benchmarks, including Llama-3.1-8B finetuning.", "AI": {"tldr": "FlashOptim\u662f\u4e00\u79cd\u5185\u5b58\u4f18\u5316\u65b9\u6848\uff0c\u53ef\u5c06\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u4e2d\u6bcf\u53c2\u6570\u5185\u5b58\u5360\u7528\u964d\u4f4e50%\u4ee5\u4e0a\uff0c\u4f7fAdamW\u4ece\u6bcf\u53c2\u657016\u5b57\u8282\u964d\u81f37\u5b57\u8282\uff08\u91ca\u653e\u68af\u5ea6\u540e\u4ec55\u5b57\u8282\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\u548cAPI\u517c\u5bb9\u6027\uff0c\u8ba970\u4ebf\u53c2\u6570\u6a21\u578b\u5728\u4e0d\u8db3100GB\u663e\u5b58\u7684\u8bbe\u5907\u4e0a\u8bad\u7ec3\u6210\u4e3a\u53ef\u80fd\u3002", "motivation": "\u6807\u51c6\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u6bcf\u53c2\u6570\u9700\u5b58\u50a8\u53c2\u6570\u3001\u68af\u5ea6\u548c\u4f18\u5316\u5668\u72b6\u6001\uff08\u901a\u5e38\u54044\u5b57\u8282\uff09\uff0cAdamW\u603b\u8ba116\u5b57\u8282/\u53c2\u6570\uff0c\u5bfc\u81f470\u4ebf\u53c2\u6570\u6a21\u578b\u8bad\u7ec3\u5bf9\u663e\u5b58\u4e0d\u8db3100GB\u7684\u7814\u7a76\u8005\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u63d0\u51fa\u4e24\u9879\u5173\u952e\u6280\u672f\uff1a(1) \u901a\u8fc7\u91cf\u5316\u8bef\u5dee\u7d27\u7ea6\u675f\u6539\u8fdb\u4e3b\u6743\u91cd\u5206\u5272\uff1b(2) \u8bbe\u8ba1\u538b\u7f29\u6269\u5c55\u51fd\u6570\u5b9e\u73b08\u4f4d\u4f18\u5316\u5668\u72b6\u6001\u91cf\u5316\uff0c\u7ed3\u540816\u4f4d\u68af\u5ea6\u3002", "result": "AdamW\u5185\u5b58\u4ece16\u5b57\u8282/\u53c2\u6570\u964d\u81f37\u5b57\u8282\uff08\u91ca\u653e\u68af\u5ea6\u540e5\u5b57\u8282\uff09\uff0c\u68c0\u67e5\u70b9\u5927\u5c0f\u51cf\u534a\uff0c\u5728Llama-3.1-8B\u5fae\u8c03\u7b49\u89c6\u89c9\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u65e0\u8d28\u91cf\u635f\u5931\u3002", "conclusion": "FlashOptim\u901a\u8fc7\u521b\u65b0\u91cf\u5316\u6280\u672f\u6709\u6548\u89e3\u51b3\u5927\u6a21\u578b\u8bad\u7ec3\u5185\u5b58\u74f6\u9888\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u4f7f\u5927\u89c4\u6a21\u8bad\u7ec3\u66f4\u666e\u53ca\u3002"}}
{"id": "2602.23353", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23353", "abs": "https://arxiv.org/abs/2602.23353", "authors": ["Simon Roschmann", "Paul Krzakala", "Sonia Mazelet", "Quentin Bouniot", "Zeynep Akata"], "title": "SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport", "comment": "Preprint", "summary": "The Platonic Representation Hypothesis posits that neural networks trained on different modalities converge toward a shared statistical model of the world. Recent work exploits this convergence by aligning frozen pretrained vision and language models with lightweight alignment layers, but typically relies on contrastive losses and millions of paired samples. In this work, we ask whether meaningful alignment can be achieved with substantially less supervision. We introduce a semi-supervised setting in which pretrained unimodal encoders are aligned using a small number of image-text pairs together with large amounts of unpaired data. To address this challenge, we propose SOTAlign, a two-stage framework that first recovers a coarse shared geometry from limited paired data using a linear teacher, then refines the alignment on unpaired samples via an optimal-transport-based divergence that transfers relational structure without overconstraining the target space. Unlike existing semi-supervised methods, SOTAlign effectively leverages unpaired images and text, learning robust joint embeddings across datasets and encoder pairs, and significantly outperforming supervised and semi-supervised baselines.", "AI": {"tldr": "\u63d0\u51faSOTAlign\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\uff08\u7ebf\u6027\u6559\u5e08\u6a21\u578b+\u6700\u4f18\u4f20\u8f93\uff09\u5229\u7528\u5c11\u91cf\u914d\u5bf9\u6570\u636e\u548c\u5927\u91cf\u672a\u914d\u5bf9\u6570\u636e\uff0c\u5b9e\u73b0\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u534a\u76d1\u7763\u5bf9\u9f50\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u6570\u767e\u4e07\u914d\u5bf9\u6837\u672c\u548c\u5bf9\u6bd4\u635f\u5931\u6765\u5bf9\u9f50\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u6210\u672c\u9ad8\u6602\uff1b\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u7528\u66f4\u5c11\u76d1\u7763\u5b9e\u73b0\u6709\u610f\u4e49\u5bf9\u9f50", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u7528\u7ebf\u6027\u6559\u5e08\u4ece\u6709\u9650\u914d\u5bf9\u6570\u636e\u6062\u590d\u7c97\u7565\u5171\u4eab\u51e0\u4f55\u7ed3\u6784\uff1b2\uff09\u7528\u6700\u4f18\u4f20\u8f93\u6563\u5ea6\u5728\u672a\u914d\u5bf9\u6837\u672c\u4e0a\u7ec6\u5316\u5bf9\u9f50\uff0c\u8f6c\u79fb\u5173\u7cfb\u7ed3\u6784\u800c\u4e0d\u8fc7\u5ea6\u7ea6\u675f\u76ee\u6807\u7a7a\u95f4", "result": "\u6709\u6548\u5229\u7528\u672a\u914d\u5bf9\u56fe\u50cf\u6587\u672c\uff0c\u8de8\u6570\u636e\u96c6\u548c\u7f16\u7801\u5668\u5b66\u4e60\u5230\u9c81\u68d2\u7684\u8054\u5408\u5d4c\u5165\uff0c\u663e\u8457\u4f18\u4e8e\u76d1\u7763\u548c\u534a\u76d1\u7763\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "SOTAlign\u6210\u529f\u5b9e\u73b0\u4e86\u7528\u6781\u5c11\u76d1\u7763\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u8bc1\u660e\u4e86\u5229\u7528\u5927\u91cf\u672a\u914d\u5bf9\u6570\u636e\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u964d\u4f4e\u6570\u636e\u6807\u6ce8\u6210\u672c\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848"}}
{"id": "2602.23303", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23303", "abs": "https://arxiv.org/abs/2602.23303", "authors": ["Ilya Balabin", "Thomas M. Kaiser"], "title": "Inferential Mechanics Part 1: Causal Mechanistic Theories of Machine Learning in Chemical Biology with Implications", "comment": null, "summary": "Machine learning techniques are now routinely encountered in research laboratories across the globe. Impressive progress has been made through ML and AI techniques with regards to large data set processing. This progress has increased the ability of the experimenter to digest data and make novel predictions regarding phenomena of interest. However, machine learning predictors generated from data sets taken from the natural sciences are often treated as black boxes which are used broadly and generally without detailed consideration of the causal structure of the data set of interest. Work has been attempted to bring causality into discussions of machine learning models of natural phenomena; however, a firm and unified theoretical treatment is lacking. This series of three papers explores the union of chemical theory, biological theory, probability theory and causality that will correct current causal flaws of machine learning in the natural sciences. This paper, Part 1 of the series, provides the formal framework of the foundational causal structure of phenomena in chemical biology and is extended to machine learning through the novel concept of focus, defined here as the ability of a machine learning algorithm to narrow down to a hidden underpinning mechanism in large data sets. Initial proof of these principles on a family of Akt inhibitors is also provided. The second paper containing Part 2 will provide a formal exploration of chemical similarity, and Part 3 will present extensive experimental evidence of how hidden causal structures weaken all machine learning in chemical biology. This series serves to establish for chemical biology a new kind of mathematical framework for modeling mechanisms in Nature without the need for the tools of reductionism: inferential mechanics.", "AI": {"tldr": "A three-part series proposes a theoretical framework integrating causality with machine learning for chemical biology. Part 1 introduces \"focus\" - ML's ability to identify underlying mechanisms - and tests it on Akt inhibitors.", "motivation": "Machine learning models in natural sciences are often treated as black boxes without considering causal structures, leading to causal flaws. A firm, unified theoretical treatment is lacking.", "method": "Establishes a formal framework for causal structure in chemical biology and extends it to ML through the novel concept of \"focus\" (ML's ability to narrow down to hidden mechanisms). Initial proof is provided on Akt inhibitors.", "result": "Initial proof of the \"focus\" concept principles demonstrated on a family of Akt inhibitors. The series aims to establish \"inferential mechanics\" as a new mathematical framework for modeling mechanisms without reductionism.", "conclusion": "The series will provide a unified theoretical framework for causality-aware machine learning in chemical biology, addressing current causal flaws and establishing inferential mechanics for modeling natural mechanisms."}}
{"id": "2602.23336", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.23336", "abs": "https://arxiv.org/abs/2602.23336", "authors": ["Camilo Gomez", "Pengyang Wang", "Liansheng Tang"], "title": "Differentiable Zero-One Loss via Hypersimplex Projections", "comment": "To appear in PAKDD 2026 (Pacific-Asia Conference on Knowledge Discovery and Data Mining), 12 pages", "summary": "Recent advances in machine learning have emphasized the integration of structured optimization components into end-to-end differentiable models, enabling richer inductive biases and tighter alignment with task-specific objectives. In this work, we introduce a novel differentiable approximation to the zero-one loss-long considered the gold standard for classification performance, yet incompatible with gradient-based optimization due to its non-differentiability. Our method constructs a smooth, order-preserving projection onto the n,k-dimensional hypersimplex through a constrained optimization framework, leading to a new operator we term Soft-Binary-Argmax. After deriving its mathematical properties, we show how its Jacobian can be efficiently computed and integrated into binary and multiclass learning systems. Empirically, our approach achieves significant improvements in generalization under large-batch training by imposing geometric consistency constraints on the output logits, thereby narrowing the performance gap traditionally observed in large-batch training.", "AI": {"tldr": "\u63d0\u51faSoft-Binary-Argmax\u7b97\u5b50\uff0c\u901a\u8fc7\u7ea6\u675f\u4f18\u5316\u5b9e\u73b0\u96f6\u4e00\u635f\u5931\u7684\u53ef\u5fae\u8fd1\u4f3c\uff0c\u89e3\u51b3\u5927\u6279\u91cf\u8bad\u7ec3\u6cdb\u5316\u5dee\u8ddd\u95ee\u9898", "motivation": "\u96f6\u4e00\u635f\u5931\u662f\u5206\u7c7b\u4efb\u52a1\u7684\u91d1\u6807\u51c6\u4f46\u4e0d\u53ef\u5fae\uff0c\u73b0\u6709\u68af\u5ea6\u4f18\u5316\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\uff1b\u5927\u6279\u91cf\u8bad\u7ec3\u65f6\u6a21\u578b\u6cdb\u5316\u6027\u80fd\u663e\u8457\u4e0b\u964d", "method": "\u6784\u5efan\u7ef4k-\u5355\u5f62\u7684\u5e73\u6ed1\u4fdd\u5e8f\u6295\u5f71\uff0c\u8bbe\u8ba1\u53ef\u5fae\u7684Soft-Binary-Argmax\u7b97\u5b50\uff1b\u63a8\u5bfc\u5176\u96c5\u53ef\u6bd4\u77e9\u9635\u5e76\u96c6\u6210\u5230\u5206\u7c7b\u7cfb\u7edf", "result": "\u5728\u5927\u6279\u91cf\u8bad\u7ec3\u4e0b\u663e\u8457\u63d0\u5347\u6cdb\u5316\u6027\u80fd\uff0c\u901a\u8fc7\u8f93\u51falogits\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u7ea6\u675f\u7f29\u5c0f\u4f20\u7edf\u6027\u80fd\u5dee\u8ddd", "conclusion": "\u9996\u6b21\u5b9e\u73b0\u96f6\u4e00\u635f\u5931\u7684\u6709\u6548\u53ef\u5fae\u8fd1\u4f3c\uff0c\u4e3a\u7ed3\u6784\u5316\u4f18\u5316\u7ec4\u4ef6\u96c6\u6210\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u7279\u522b\u6539\u5584\u5927\u6279\u91cf\u8bad\u7ec3\u6548\u679c"}}
{"id": "2602.22287", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22287", "abs": "https://arxiv.org/abs/2602.22287", "authors": ["Willem Schooltink", "Fabio Massimo Zennaro"], "title": "Multi-Level Causal Embeddings", "comment": null, "summary": "Abstractions of causal models allow for the coarsening of models such that relations of cause and effect are preserved. Whereas abstractions focus on the relation between two models, in this paper we study a framework for causal embeddings which enable multiple detailed models to be mapped into sub-systems of a coarser causal model. We define causal embeddings as a generalization of abstraction, and present a generalized notion of consistency. By defining a multi-resolution marginal problem, we showcase the relevance of causal embeddings for both the statistical marginal problem and the causal marginal problem; furthermore, we illustrate its practical use in merging datasets coming from models with different representations.", "AI": {"tldr": "\u63d0\u51fa\u56e0\u679c\u5d4c\u5165\u6846\u67b6\uff0c\u5c06\u591a\u4e2a\u7cbe\u7ec6\u56e0\u679c\u6a21\u578b\u6620\u5c04\u5230\u7c97\u7c92\u5ea6\u6a21\u578b\u7684\u5b50\u7cfb\u7edf\uff0c\u89e3\u51b3\u591a\u5206\u8fa8\u7387\u8fb9\u9645\u95ee\u9898\uff0c\u5b9e\u73b0\u5f02\u6784\u6570\u636e\u96c6\u5408\u5e76", "motivation": "\u73b0\u6709\u56e0\u679c\u6a21\u578b\u62bd\u8c61\u65b9\u6cd5\u4ec5\u5173\u6ce8\u4e24\u4e2a\u6a21\u578b\u95f4\u5173\u7cfb\uff0c\u9700\u6269\u5c55\u6846\u67b6\u4ee5\u652f\u6301\u591a\u4e2a\u6a21\u578b\u5d4c\u5165\u5230\u7edf\u4e00\u7c97\u7c92\u5ea6\u7cfb\u7edf\uff0c\u5e76\u5904\u7406\u7edf\u8ba1\u4e0e\u56e0\u679c\u5c42\u9762\u7684\u8fb9\u9645\u95ee\u9898", "method": "\u5b9a\u4e49\u56e0\u679c\u5d4c\u5165\u4f5c\u4e3a\u62bd\u8c61\u7684\u6cdb\u5316\u5f62\u5f0f\uff0c\u63d0\u51fa\u4e00\u81f4\u6027\u65b0\u6982\u5ff5\uff0c\u6784\u5efa\u591a\u5206\u8fa8\u7387\u8fb9\u9645\u95ee\u9898\u6846\u67b6", "result": "\u8bc1\u660e\u8be5\u6846\u67b6\u540c\u65f6\u9002\u7528\u4e8e\u7edf\u8ba1\u8fb9\u9645\u95ee\u9898\u4e0e\u56e0\u679c\u8fb9\u9645\u95ee\u9898\uff0c\u652f\u6301\u4e0d\u540c\u8868\u793a\u6a21\u578b\u7684\u6570\u636e\u96c6\u878d\u5408", "conclusion": "\u56e0\u679c\u5d4c\u5165\u4e3a\u591a\u6a21\u578b\u96c6\u6210\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\uff0c\u5728\u5f02\u6784\u6570\u636e\u5408\u5e76\u573a\u666f\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c"}}
{"id": "2602.22441", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22441", "abs": "https://arxiv.org/abs/2602.22441", "authors": ["Yingqian Cui", "Zhenwei Dai", "Bing He", "Zhan Shi", "Hui Liu", "Rui Sun", "Zhiji Liu", "Yue Xing", "Jiliang Tang", "Benoit Dumoulin"], "title": "How Do Latent Reasoning Methods Perform Under Weak and Strong Supervision?", "comment": null, "summary": "Latent reasoning has been recently proposed as a reasoning paradigm and performs multi-step reasoning through generating steps in the latent space instead of the textual space. This paradigm enables reasoning beyond discrete language tokens by performing multi-step computation in continuous latent spaces. Although there have been numerous studies focusing on improving the performance of latent reasoning, its internal mechanisms remain not fully investigated. In this work, we conduct a comprehensive analysis of latent reasoning methods to better understand the role and behavior of latent representation in the process. We identify two key issues across latent reasoning methods with different levels of supervision. First, we observe pervasive shortcut behavior, where they achieve high accuracy without relying on latent reasoning. Second, we examine the hypothesis that latent reasoning supports BFS-like exploration in latent space, and find that while latent representations can encode multiple possibilities, the reasoning process does not faithfully implement structured search, but instead exhibits implicit pruning and compression. Finally, our findings reveal a trade-off associated with supervision strength: stronger supervision mitigates shortcut behavior but restricts the ability of latent representations to maintain diverse hypotheses, whereas weaker supervision allows richer latent representations at the cost of increased shortcut behavior.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u6f5c\u5728\u63a8\u7406\u5b58\u5728\u4e24\u4e2a\u6838\u5fc3\u95ee\u9898\uff1a\u4e00\u662f\u6a21\u578b\u666e\u904d\u5b58\u5728\u6377\u5f84\u884c\u4e3a\uff08\u4e0d\u771f\u6b63\u4f7f\u7528\u6f5c\u5728\u63a8\u7406\u5373\u53ef\u83b7\u5f97\u9ad8\u7cbe\u5ea6\uff09\uff0c\u4e8c\u662f\u6f5c\u5728\u63a8\u7406\u5e76\u975e\u5982\u5047\u8bbe\u822c\u5b9e\u73b0\u7c7bBFS\u7ed3\u6784\u5316\u641c\u7d22\uff0c\u800c\u662f\u8868\u73b0\u51fa\u9690\u6027\u526a\u679d\u4e0e\u538b\u7f29\u3002\u5173\u952e\u53d1\u73b0\u662f\u76d1\u7763\u5f3a\u5ea6\u5b58\u5728\u6743\u8861\uff1a\u5f3a\u76d1\u7763\u51cf\u5c11\u6377\u5f84\u4f46\u9650\u5236\u5047\u8bbe\u591a\u6837\u6027\uff0c\u5f31\u76d1\u7763\u5219\u76f8\u53cd\u3002", "motivation": "\u5c3d\u7ba1\u6f5c\u5728\u63a8\u7406\u4f5c\u4e3a\u5728\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u591a\u6b65\u63a8\u7406\u7684\u65b0\u8303\u5f0f\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5185\u90e8\u673a\u5236\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7a76\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5168\u9762\u5206\u6790\u6f5c\u5728\u63a8\u7406\u65b9\u6cd5\uff0c\u6df1\u5165\u7406\u89e3\u6f5c\u5728\u8868\u5f81\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u771f\u5b9e\u4f5c\u7528\u4e0e\u884c\u4e3a\u3002", "method": "\u4f5c\u8005\u5bf9\u5177\u6709\u4e0d\u540c\u76d1\u7763\u7a0b\u5ea6\u7684\u6f5c\u5728\u63a8\u7406\u65b9\u6cd5\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u5206\u6790\uff0c\u91cd\u70b9\u68c0\u9a8c\u4e86\u6f5c\u5728\u63a8\u7406\u662f\u5426\u652f\u6301\u7c7b\u4f3c\u5e7f\u5ea6\u4f18\u5148\u641c\u7d22\uff08BFS\uff09\u7684\u5047\u8bbe\u3002\u901a\u8fc7\u63a7\u5236\u76d1\u7763\u5f3a\u5ea6\u53d8\u91cf\uff0c\u63a2\u7a76\u6f5c\u5728\u8868\u5f81\u7684\u591a\u6837\u6027\u4e0e\u63a8\u7406\u771f\u5b9e\u6027\u7684\u5173\u7cfb\u3002", "result": "1) \u53d1\u73b0\u666e\u904d\u5b58\u5728\u7684\u6377\u5f84\u884c\u4e3a\uff1a\u6a21\u578b\u65e0\u9700\u4f9d\u8d56\u771f\u5b9e\u6f5c\u5728\u63a8\u7406\u5373\u53ef\u83b7\u5f97\u9ad8\u51c6\u786e\u7387\uff1b2) \u6f5c\u5728\u8868\u5f81\u867d\u80fd\u7f16\u7801\u591a\u79cd\u53ef\u80fd\u6027\uff0c\u4f46\u63a8\u7406\u8fc7\u7a0b\u5e76\u672a\u5fe0\u5b9e\u5b9e\u73b0\u7ed3\u6784\u5316\u641c\u7d22\uff0c\u800c\u662f\u8868\u73b0\u51fa\u9690\u6027\u526a\u679d\u4e0e\u538b\u7f29\uff1b3) \u63ed\u793a\u76d1\u7763\u5f3a\u5ea6\u6743\u8861\uff1a\u5f3a\u76d1\u7763\u51cf\u8f7b\u6377\u5f84\u4f46\u9650\u5236\u6f5c\u5728\u8868\u5f81\u7684\u591a\u6837\u6027\uff0c\u5f31\u76d1\u7763\u5219\u5141\u8bb8\u66f4\u4e30\u5bcc\u7684\u8868\u5f81\u4f46\u4ee5\u589e\u52a0\u6377\u5f84\u4e3a\u4ee3\u4ef7\u3002", "conclusion": "\u6f5c\u5728\u63a8\u7406\u7684\u5185\u90e8\u673a\u5236\u6bd4\u9884\u671f\u66f4\u590d\u6742\uff0c\u6377\u5f84\u884c\u4e3a\u662f\u5176\u6839\u672c\u6311\u6218\u3002\u76d1\u7763\u5f3a\u5ea6\u5728\u63a8\u7406\u8d28\u91cf\u4e0e\u8868\u5f81\u591a\u6837\u6027\u4e4b\u95f4\u5b58\u5728\u6839\u672c\u6027\u6743\u8861\uff0c\u5f53\u524d\u65b9\u6cd5\u53ef\u80fd\u5e76\u672a\u5b9e\u73b0\u771f\u6b63\u610f\u4e49\u4e0a\u7684\u591a\u6b65\u8ba1\u7b97\uff0c\u8fd9\u4e3a\u672a\u6765\u6539\u8fdb\u6f5c\u5728\u63a8\u7406\u67b6\u6784\u548c\u76d1\u7763\u7b56\u7565\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2602.22480", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22480", "abs": "https://arxiv.org/abs/2602.22480", "authors": ["Varun Ursekar", "Apaar Shanker", "Veronica Chatrath", "Yuan", "Xue", "Sam Denton"], "title": "VeRO: An Evaluation Harness for Agents to Optimize Agents", "comment": null, "summary": "An important emerging application of coding agents is agent optimization: the iterative improvement of a target agent through edit-execute-evaluate cycles. Despite its relevance, the community lacks a systematic understanding of coding agent performance on this task. Agent optimization differs fundamentally from conventional software engineering: the target agent interleaves deterministic code with stochastic LLM completions, requiring structured capture of both intermediate reasoning and downstream execution outcomes. To address these challenges, we introduce VERO (Versioning, Rewards, and Observations), which provides (1) a reproducible evaluation harness with versioned agent snapshots, budget-controlled evaluation, and structured execution traces, and (2) a benchmark suite of target agents and tasks with reference evaluation procedures. Using VERO, we conduct an empirical study comparing optimizer configurations across tasks and analyzing which modifications reliably improve target agent performance. We release VERO to support research on agent optimization as a core capability for coding agents.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86VERO\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u4f18\u5316\u7f16\u7801\u667a\u80fd\u4f53\u3002\u901a\u8fc7\u7248\u672c\u63a7\u5236\u3001\u5956\u52b1\u673a\u5236\u548c\u7ed3\u6784\u5316\u89c2\u5bdf\uff0c\u8be5\u6846\u67b6\u89e3\u51b3\u4e86\u667a\u80fd\u4f53\u4f18\u5316\u4e2d\u53ef\u590d\u73b0\u6027\u548c\u8bc4\u4f30\u6807\u51c6\u5316\u7684\u6311\u6218\uff0c\u5e76\u63d0\u4f9b\u4e86\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u6765\u6bd4\u8f83\u4e0d\u540c\u4f18\u5316\u914d\u7f6e\u7684\u6548\u679c\u3002", "motivation": "\u7f16\u7801\u667a\u80fd\u4f53\u5728\u901a\u8fc7\u7f16\u8f91-\u6267\u884c-\u8bc4\u4f30\u5faa\u73af\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\u65b9\u9762\u5b58\u5728\u7cfb\u7edf\u6027\u7406\u89e3\u7f3a\u5931\u3002\u4e0e\u5e38\u89c4\u8f6f\u4ef6\u5de5\u7a0b\u4e0d\u540c\uff0c\u667a\u80fd\u4f53\u4f18\u5316\u9700\u8981\u540c\u65f6\u5904\u7406\u786e\u5b9a\u6027\u4ee3\u7801\u548c\u968f\u673aLLM\u751f\u6210\uff0c\u5e76\u6355\u83b7\u4e2d\u95f4\u63a8\u7406\u4e0e\u6267\u884c\u7ed3\u679c\uff0c\u8fd9\u9700\u8981\u4e13\u95e8\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faVERO\uff08\u7248\u672c\u63a7\u5236\u3001\u5956\u52b1\u4e0e\u89c2\u5bdf\uff09\u6846\u67b6\uff0c\u5305\u542b\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u5de5\u5177\u548c\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u3002\u8bc4\u4f30\u5de5\u5177\u63d0\u4f9b\u7248\u672c\u5316\u667a\u80fd\u4f53\u5feb\u7167\u3001\u9884\u7b97\u63a7\u5236\u548c\u7ed3\u6784\u5316\u6267\u884c\u8ffd\u8e2a\uff1b\u57fa\u51c6\u5957\u4ef6\u5305\u542b\u76ee\u6807\u667a\u80fd\u4f53\u548c\u4efb\u52a1\u53ca\u53c2\u8003\u8bc4\u4f30\u6d41\u7a0b\u3002", "result": "\u5229\u7528VERO\u8fdb\u884c\u4e86\u5b9e\u8bc1\u7814\u7a76\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u4f18\u5316\u914d\u7f6e\u5728\u5404\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u4e86\u54ea\u4e9b\u4fee\u6539\u80fd\u53ef\u9760\u63d0\u5347\u76ee\u6807\u667a\u80fd\u4f53\u6027\u80fd\u3002", "conclusion": "VERO\u4e3a\u7f16\u7801\u667a\u80fd\u4f53\u7684\u6838\u5fc3\u4f18\u5316\u80fd\u529b\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u8bbe\u65bd\uff0c\u652f\u6301\u793e\u533a\u5728\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2602.22897", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.22897", "abs": "https://arxiv.org/abs/2602.22897", "authors": ["Xiaoxi Li", "Wenxiang Jiao", "Jiarui Jin", "Shijian Wang", "Guanting Dong", "Jiajie Jin", "Hao Wang", "Yinuo Wang", "Ji-Rong Wen", "Yuan Lu", "Zhicheng Dou"], "title": "OmniGAIA: Towards Native Omni-Modal AI Agents", "comment": null, "summary": "Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world. However, current multi-modal LLMs are primarily confined to bi-modal interactions (e.g., vision-language), lacking the unified cognitive capabilities required for general AI assistants. To bridge this gap, we introduce OmniGAIA, a comprehensive benchmark designed to evaluate omni-modal agents on tasks necessitating deep reasoning and multi-turn tool execution across video, audio, and image modalities. Constructed via a novel omni-modal event graph approach, OmniGAIA synthesizes complex, multi-hop queries derived from real-world data that require cross-modal reasoning and external tool integration. Furthermore, we propose OmniAtlas, a native omni-modal foundation agent under tool-integrated reasoning paradigm with active omni-modal perception. Trained on trajectories synthesized via a hindsight-guided tree exploration strategy and OmniDPO for fine-grained error correction, OmniAtlas effectively enhances the tool-use capabilities of existing open-source models. This work marks a step towards next-generation native omni-modal AI assistants for real-world scenarios.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
