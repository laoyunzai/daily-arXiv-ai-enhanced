<div id=toc></div>

# Table of Contents

- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 4]
- [cs.LG](#cs.LG) [Total: 42]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 5]
- [cs.AI](#cs.AI) [Total: 15]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [quant-ph](#quant-ph) [Total: 28]
- [nlin.CD](#nlin.CD) [Total: 2]


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [1] [Lévy walkers inside spherical shells with absorbing boundaries: Towards settling the optimal Lévy walk strategy for random searches](https://arxiv.org/abs/2601.10731)
*L. G. P. Caramês,Y. B. Matos,F. Bartumeus,C. G. Bezerra,T. Macrì,M. G. E. da Luz,E. P. Raposo,G. M. Viswanathan*

Main category: cond-mat.stat-mech

TL;DR: 该论文证明了在无限时间、无限制重访的二维及以上维度觅食场景中，逆平方Lévy飞行是最优搜索策略


<details>
  <summary>Details</summary>
Motivation: Lévy飞行觅食假说认为生物进化出了利用Lévy行走搜索策略的适应性，但尽管广泛接受逆平方Lévy行走在无限制重访觅食中优化搜索效率，对于D≥2维度的数学严格证明仍然缺乏

Method: 研究Lévy行走在具有吸收边界的环形或球壳区域内的相关问题，在对应于无限制重访觅食的极限情况下进行分析

Result: 在对应于无限制重访觅食的极限情况下，证明了逆平方Lévy行走优化了搜索效率，这是迄今为止支持逆平方Lévy行走搜索策略最优性的最强形式结果

Conclusion: 该研究为Lévy飞行觅食假说提供了重要的数学支持，证明了在二维及以上维度、无限制重访的觅食场景中，逆平方Lévy行走确实是最优搜索策略

Abstract: The Lévy flight foraging hypothesis states that organisms must have evolved adaptations to exploit Lévy walk search strategies. Indeed, it is widely accepted that inverse square Lévy walks optimize the search efficiency in foraging with unrestricted revisits (also known as non-destructive foraging). However, a mathematically rigorous demonstration of this for dimensions $D \geq 2$ is still lacking. Here we study the very closely related problem of a Lévy walker inside annuli or spherical shells with absorbing boundaries. In the limit that corresponds to the foraging with unrestricted revisits, we show that inverse square Lévy walks optimize the search. This constitutes the strongest formal result to date supporting the optimality of inverse square Lévy walks search strategies.

</details>


### [2] [Eigen Microstate Condensation and Critical Phenomena in the Lennard-Jones Fluid](https://arxiv.org/abs/2601.10741)
*Lan Yang,Zhaorong Pang,Chongzhi Qiao,Gaoke Hu,Jiaqi Dong,Rui Shi,Xiaosong Chen*

Main category: cond-mat.stat-mech

TL;DR: 使用本征微观态理论（EMT）研究Lennard-Jones流体中的液-气连续相变，通过本征微观态的概率幅作为序参数，同时确定了临界温度和密度，并获得了与Ising普适类一致的临界指数。


<details>
  <summary>Details</summary>
Motivation: 尽管对液-气相变进行了广泛研究，但通过直接模拟准确确定流体系统中的临界点和临界指数仍然是一个挑战。需要一种有效的方法来研究复杂流体系统的临界现象。

Method: 采用本征微观态理论（EMT）在正则系综中研究Lennard-Jones流体的液-气相变。使用本征微观态的概率幅作为序参数，通过概率幅的有限尺寸标度分析来确定临界参数。

Result: 同时确定了临界温度Tc=1.188(2)和临界密度ρc=0.320(4)。获得了临界指数β=0.32(2)和ν=0.64(3)，与Ising普适类高度一致。该方法还揭示了临界区域中流体的三维空间构型。

Conclusion: EMT为研究复杂流体系统的临界现象提供了有力工具，证实了临界区域中本征微观态概率幅的有限尺寸标度行为，并能够表征临界区域中流体的介观结构。

Abstract: Despite extensive study of the liquid-gas phase transition, accurately determining the critical point and the critical exponents in fluid systems through direct simulation remains a challenge. We employ the eigen microstate theory (EMT) to investigate the liquid-gas continuous phase transition in the Lennard-Jones (LJ) fluid within the canonical ensemble. In EMT, the probability amplitudes of eigen microstates serve as the order parameter. Using finite-size scaling of probability amplitudes, we simultaneously determine the critical temperature, $T_c = 1.188(2)$, and critical density, $ρ_c = 0.320(4)$. Furturemore, we obtain critical exponents of the LJ fluid, $β= 0.32(2)$ and $ν= 0.64(3)$, which demonstrate a great agreement with the Ising universality class. This method also reveals the mesoscopic structure of the emergent phase, characterizing the three-dimensional (3D) spatial configuration of the fluid in the critical region. This work also confirms the finite-size scaling behavior of the probability amplitudes of the eigen microstates in the critical region. The EMT provides a powerful tool for studying the critical phenomena of complex fluid system.

</details>


### [3] [Exact solution of a two-dimensional (2D) Ising model with the next nearest interactions](https://arxiv.org/abs/2601.10749)
*Zhidong Zhang*

Main category: cond-mat.stat-mech

TL;DR: 本文推导了零磁场下具有次近邻相互作用的二维伊辛模型的精确解，通过三种表示方法分析系统的拓扑结构，获得了配分函数和自发磁化强度，揭示了相互作用数量和拓扑贡献对临界点的影响。


<details>
  <summary>Details</summary>
Motivation: 研究二维伊辛模型具有次近邻相互作用的精确解，旨在理解二维磁性材料的物理性质，特别是拓扑结构对系统临界行为的影响。

Method: 采用三种表示方法分析转移矩阵：克利福德代数表示、转移张量表示和示意图表示；将系统等效为三角伊辛模型加z轴相互作用，并修改三维伊辛模型的求解方法以适应本系统。

Result: 获得了配分函数和自发磁化强度的精确解；发现单位胞内相互作用数量的增加或拓扑贡献的存在/增强会提高伊辛晶格的临界点。

Conclusion: 本文成功推导了具有次近邻相互作用的二维伊辛模型的精确解，揭示了拓扑结构对临界行为的影响，为理解二维磁性材料的物理性质提供了理论依据。

Abstract: The exact solution of a two-dimensional (2D) Ising model with the next nearest interactions at zero magnetic field is derived. At first, the transfer matrices are analyzed in three representations, i.e., Clifford algebraic representation, transfer tensor representation and schematic representation, to inspect nontrivial topological structures in this system. The system is equivalent to a triangular Ising model plus an interaction along the z axis, so that the approaches developed for the 3D Ising model are modified to be appropriable for solving the exact solution of the 2D Ising model with the next nearest interactions. The partition function and the spontaneous magnetization are obtained. The comparison with the exact solutions of other Ising lattices reveals that either the increase of the number of interactions in a unit cell or the presence/increase of topological contributions enhances the critical point of the Ising lattices. The results obtained in this work are helpful for understanding the physical properties of the 2D magnetic materials.

</details>


### [4] [Irreversible Kinetics Emerges from Bayesian Inference over Admissible Histories](https://arxiv.org/abs/2601.10763)
*Manas V. Upadhyay*

Main category: cond-mat.stat-mech

TL;DR: 提出了一种不可逆动力学的概率公式，通过Gibbs型测度对增量可接受历史进行加权，该测度由能量耗散作用和观测约束构建，Theta控制认知不确定性。该测度可解释为历史的贝叶斯后验分布。


<details>
  <summary>Details</summary>
Motivation: 传统确定性动力学方法在处理非凸能量或时间耦合约束时存在局限性，需要一种能够处理认知不确定性和多重竞争历史的概率框架。

Method: 引入基于能量耗散作用和观测约束的Gibbs型概率测度，将历史视为随机变量。在零不确定性极限下，该测度集中于最大后验历史，恢复经典确定性演化。

Result: 在七个不同的前向时间示例中展示了该框架的涌现特性，并通过全局约束最小作用原理解决了从稀疏观测推断未知历史的逆推理问题。

Conclusion: 提出的概率框架统一了经典确定性演化和贝叶斯推理，能够处理非凸能量、时间耦合约束和认知不确定性，为不可逆动力学提供了更一般的数学基础。

Abstract: A probabilistic formulation of irreversible kinetics is introduced in which incrementally admissible histories are weighted by a Gibbs-type measure built from an energy-dissipation action and observation constraints, with Theta controlling epistemic uncertainty. This measure can be interpreted as a Bayesian posterior over histories. In the zero-uncertainty limit, it concentrates on maximum-a-posteriori (MAP) histories, recovering classical deterministic evolution by incremental minimization in the convex generalized-standard-material setting, while allowing multiple competing MAP histories for non-convex energies or temporally coupled constraints. This emergence is demonstrated across seven distinct forward-in-time examples and an inverse inference problem of unknown histories from sparse observations via a global constrained minimum-action principle.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [Analytic Bijections for Smooth and Interpretable Normalizing Flows](https://arxiv.org/abs/2601.10774)
*Mathis Gerdes,Miranda C. N. Cheng*

Main category: cs.LG

TL;DR: 提出了三种全局平滑、定义在全实数域且解析可逆的双射函数族（三次有理、sinh、三次多项式），以及径向流新架构，在保持训练稳定性的同时大幅减少参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有归一化流设计面临权衡：仿射变换平滑且解析可逆但表达能力有限；单调样条提供局部控制但仅分段平滑且定义在有界域；残差流平滑但需要数值求逆。需要结合各种方法优点的解决方案。

Method: 引入三种解析双射函数族：三次有理、sinh和三次多项式，它们全局平滑（C^∞）、定义在全实数域且解析可逆。此外开发径向流架构，通过直接参数化变换径向坐标同时保持角度方向不变。

Result: 这些双射函数可作为耦合流中的即插即用替代品，匹配或超越样条性能。径向流表现出卓越的训练稳定性，产生几何可解释的变换，对于具有径向结构的目标，能以1000倍更少的参数达到与耦合流相当的质量。

Conclusion: 提出的解析双射函数结合了现有方法的优点，而径向流架构为具有特定几何结构的问题提供了高效解决方案。在φ^4晶格场理论实验中，这些方法超越了仿射基线，并能针对模式塌陷问题设计特定解决方案。

Abstract: A key challenge in designing normalizing flows is finding expressive scalar bijections that remain invertible with tractable Jacobians. Existing approaches face trade-offs: affine transformations are smooth and analytically invertible but lack expressivity; monotonic splines offer local control but are only piecewise smooth and act on bounded domains; residual flows achieve smoothness but need numerical inversion. We introduce three families of analytic bijections -- cubic rational, sinh, and cubic polynomial -- that are globally smooth ($C^\infty$), defined on all of $\mathbb{R}$, and analytically invertible in closed form, combining the favorable properties of all prior approaches. These bijections serve as drop-in replacements in coupling flows, matching or exceeding spline performance. Beyond coupling layers, we develop radial flows: a novel architecture using direct parametrization that transforms the radial coordinate while preserving angular direction. Radial flows exhibit exceptional training stability, produce geometrically interpretable transformations, and on targets with radial structure can achieve comparable quality to coupling flows with $1000\times$ fewer parameters. We provide comprehensive evaluation on 1D and 2D benchmarks, and demonstrate applicability to higher-dimensional physics problems through experiments on $φ^4$ lattice field theory, where our bijections outperform affine baselines and enable problem-specific designs that address mode collapse.

</details>


### [6] [Transient learning dynamics drive escape from sharp valleys in Stochastic Gradient Descent](https://arxiv.org/abs/2601.10962)
*Ning Yang,Yikuan Zhang,Qi Ouyang,Chao Tang,Yuhai Tu*

Main category: cs.LG

TL;DR: SGD通过非平衡机制选择平坦解，噪声重塑损失景观为有效势能，训练后期出现瞬态冻结机制，噪声强度延迟冻结促进收敛到更平坦极小值


<details>
  <summary>Details</summary>
Motivation: 研究SGD偏好平坦、更可泛化解的动态起源，理解学习动态、损失景观几何与泛化能力之间的物理联系

Method: 通过分析SGD学习动态，使用数值实验观察轨迹行为，建立可处理的物理模型，研究噪声对损失景观的重塑作用

Result: 发现SGD存在瞬态探索阶段，噪声将景观重塑为偏好平坦解的有效势能，训练后期出现瞬态冻结机制，噪声强度延迟冻结促进收敛到平坦极小值

Conclusion: 为学习动态、损失景观几何和泛化能力提供了统一的物理框架，为设计更有效的优化算法提供了原则

Abstract: Stochastic gradient descent (SGD) is central to deep learning, yet the dynamical origin of its preference for flatter, more generalizable solutions remains unclear. Here, by analyzing SGD learning dynamics, we identify a nonequilibrium mechanism governing solution selection. Numerical experiments reveal a transient exploratory phase in which SGD trajectories repeatedly escape sharp valleys and transition toward flatter regions of the loss landscape. By using a tractable physical model, we show that the SGD noise reshapes the landscape into an effective potential that favors flat solutions. Crucially, we uncover a transient freezing mechanism: as training proceeds, growing energy barriers suppress inter-valley transitions and ultimately trap the dynamics within a single basin. Increasing the SGD noise strength delays this freezing, which enhances convergence to flatter minima. Together, these results provide a unified physical framework linking learning dynamics, loss-landscape geometry, and generalization, and suggest principles for the design of more effective optimization algorithms.

</details>


### [7] [Unified Optimization of Source Weights and Transfer Quantities in Multi-Source Transfer Learning: An Asymptotic Framework](https://arxiv.org/abs/2601.10779)
*Qingyue Zhang,Chang Chu,Haohao Fu,Tianren Peng,Yanru Wu,Guanbo Huang,Yang Li,Shao-Lun Huang*

Main category: cs.LG

TL;DR: UOWQ框架统一优化多源迁移学习中的源权重和转移数量，通过理论分析证明在权重调整后使用所有源样本最优，并提供闭式解和凸优化方法，实验验证优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统迁移学习方法通常单独优化源权重或转移样本数量，忽略了二者的联合考虑。多源迁移中简单的均匀转移可能导致负迁移，需要平衡异构源任务的贡献。

Method: 提出UOWQ理论框架，将多源迁移学习建模为基于KL散度泛化误差度量的参数估计问题。框架联合确定每个源任务的最优权重和最优转移数量。证明权重调整后使用所有源样本最优，为单源情况提供闭式解，为多源情况开发基于凸优化的数值方法。

Result: 理论分析表明权重调整后使用所有可用源样本总是最优的。在DomainNet和Office-Home等真实基准测试上的广泛实验显示，UOWQ始终优于强基线方法。

Conclusion: UOWQ框架通过统一优化源权重和转移数量，有效解决了多源迁移学习中的负迁移问题，理论预测得到验证，实际效果显著优于现有方法。

Abstract: Transfer learning plays a vital role in improving model performance in data-scarce scenarios. However, naive uniform transfer from multiple source tasks may result in negative transfer, highlighting the need to properly balance the contributions of heterogeneous sources. Moreover, existing transfer learning methods typically focus on optimizing either the source weights or the amount of transferred samples, while largely neglecting the joint consideration of the other. In this work, we propose a theoretical framework, Unified Optimization of Weights and Quantities (UOWQ), which formulates multi-source transfer learning as a parameter estimation problem grounded in an asymptotic analysis of a Kullback-Leibler divergence-based generalization error measure. The proposed framework jointly determines the optimal source weights and optimal transfer quantities for each source task. Firstly, we prove that using all available source samples is always optimal once the weights are properly adjusted, and we provide a theoretical explanation for this phenomenon. Moreover, to determine the optimal transfer weights, our analysis yields closed-form solutions in the single-source setting and develops a convex optimization-based numerical procedure for the multi-source case. Building on the theoretical results, we further propose practical algorithms for both multi-source transfer learning and multi-task learning settings. Extensive experiments on real-world benchmarks, including DomainNet and Office-Home, demonstrate that UOWQ consistently outperforms strong baselines. The results validate both the theoretical predictions and the practical effectiveness of our framework.

</details>


### [8] [Mugi: Value Level Parallelism For Efficient LLMs](https://arxiv.org/abs/2601.10823)
*Daniel Price,Prabhu Vellaisamy,John Shen,Di Wu*

Main category: cs.LG

TL;DR: 该论文提出了一种名为Mugi的新型VLP架构，用于优化大型语言模型中的非线性操作和小批量GEMM，显著提升了性能、能效和可持续性。


<details>
  <summary>Details</summary>
Motivation: 现有VLP技术主要针对大批量、低精度的GEMM操作，但在Transformer架构的LLMs中存在更复杂的非线性操作和小批量GEMM需求，需要探索VLP如何更好地应用于LLMs。

Method: 1. 将VLP推广到非线性近似，采用以值为中心的方法，为重要值分配更高精度；2. 优化VLP用于非对称输入的小批量GEMM，结合权重量化、KV缓存量化和组查询注意力等LLM优化技术；3. 设计新的VLP架构Mugi来封装上述创新。

Result: Mugi在非线性softmax操作上实现了高达45倍的吞吐量提升和668倍的能效提升，在LLMs上实现了2.07倍的吞吐量提升和3.11倍的能效提升，同时将LLM运行碳足迹降低1.45倍，硬件碳足迹降低1.48倍。

Conclusion: VLP技术可以有效优化LLMs中的复杂操作，Mugi架构通过创新的VLP方法在性能、能效和可持续性方面都取得了显著改进，为LLM硬件加速提供了有前景的解决方案。

Abstract: Value level parallelism (VLP) has been proposed to improve the efficiency of large-batch, low-precision general matrix multiply (GEMM) between symmetric activations and weights. In transformer based large language models (LLMs), there exist more sophisticated operations beyond activation-weight GEMM. In this paper, we explore how VLP benefits LLMs. First, we generalize VLP for nonlinear approximations, outperforming existing nonlinear approximations in end-to-end LLM accuracy, performance, and efficiency. Our VLP approximation follows a value-centric approach, where important values are assigned with greater accuracy. Second, we optimize VLP for small-batch GEMMs with asymmetric inputs efficiently, which leverages timely LLM optimizations, including weight-only quantization, key-value (KV) cache quantization, and group query attention. Finally, we design a new VLP architecture, Mugi, to encapsulate the innovations above and support full LLM workloads, while providing better performance, efficiency and sustainability. Our experimental results show that Mugi can offer significant improvements on throughput and energy efficiency, up to $45\times$ and $668\times$ for nonlinear softmax operations, and $2.07\times$ and $3.11\times$ for LLMs, and also decrease operational carbon for LLM operation by $1.45\times$ and embodied carbon by $1.48\times$.

</details>


### [9] [AI-Guided Human-In-the-Loop Inverse Design of High Performance Engineering Structures](https://arxiv.org/abs/2601.10859)
*Dat Quoc Ha,Md Ferdous Alam,Markus J. Buehler,Faez Ahmed,Josephine V. Carstensen*

Main category: cs.LG

TL;DR: 本文提出了一种AI协同设计方法，通过机器学习预测用户在拓扑优化中的偏好区域，减少人工迭代次数，提高设计效率。


<details>
  <summary>Details</summary>
Motivation: 传统拓扑优化计算时间长且缺乏用户交互性，现有的人机协同方法依赖耗时的迭代区域选择过程，需要减少迭代次数以提高效率。

Method: 采用U-Net架构的图像分割模型，在合成数据集上训练预测用户偏好区域（最长拓扑构件或最复杂结构连接），作为AI推荐提供给用户。

Result: 模型能成功预测合理的修改区域，在多样化非标准拓扑优化问题中表现出泛化能力，集成AI协同的人机循环方法可将线性屈曲载荷提高39%，总设计时间仅增加15秒。

Conclusion: AI协同设计方法有效减少了人机循环拓扑优化的迭代次数，提高了设计效率，在保持计算时间基本不变的情况下显著改善了结构性能。

Abstract: Inverse design tools such as Topology Optimization (TO) can achieve new levels of improvement for high-performance engineered structures. However, widespread use is hindered by high computational times and a black-box nature that inhibits user interaction. Human-in-the-loop TO approaches are emerging that integrate human intuition into the design generation process. However, these rely on the time-consuming bottleneck of iterative region selection for design modifications. To reduce the number of iterative trials, this contribution presents an AI co-pilot that uses machine learning to predict the user's preferred regions. The prediction model is configured as an image segmentation task with a U-Net architecture. It is trained on synthetic datasets where human preferences either identify the longest topological member or the most complex structural connection. The model successfully predicts plausible regions for modification and presents them to the user as AI recommendations. The human preference model demonstrates generalization across diverse and non-standard TO problems and exhibits emergent behavior outside the single-region selection training data. Demonstration examples show that the new human-in-the-loop TO approach that integrates the AI co-pilot can improve manufacturability or improve the linear buckling load by 39% while only increasing the total design time by 15 sec compared to conventional simplistic TO.

</details>


### [10] [Beyond Accuracy: A Stability-Aware Metric for Multi-Horizon Forecasting](https://arxiv.org/abs/2601.10863)
*Chutian Ma,Grigorii Pomazkin,Giacinto Paolo Saggese,Paul Smith*

Main category: cs.LG

TL;DR: 该论文提出了一种新的概率多步预测评估指标——预测准确性与一致性评分（forecast AC score），该指标同时考虑多步预测的准确性和时间一致性，并通过可微目标函数优化季节性ARIMA模型，在M4小时数据集上显著降低了预测波动性。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测方法仅优化准确性，忽略了时间一致性（即模型在不同预测起点对同一未来事件的预测一致性）。这种单一目标可能导致预测结果不稳定，在实际应用中产生问题。

Method: 提出了预测准确性与一致性评分（forecast AC score），该评分同时衡量多步预测的准确性和稳定性，并允许用户通过指定权重来平衡这两个需求。将该评分实现为可微目标函数，用于训练季节性ARIMA模型。

Result: 在M4小时基准数据集上的实验结果表明，与传统最大似然估计相比，AC优化模型在保持相当或改进的点预测准确性的同时，对相同目标时间戳的预测波动性降低了75%。

Conclusion: 提出的forecast AC score能够有效平衡预测准确性和时间一致性，通过可微优化框架显著改善了预测的稳定性，为时间序列预测提供了更全面的质量评估和优化方法。

Abstract: Traditional time series forecasting methods optimize for accuracy alone. This objective neglects temporal consistency, in other words, how consistently a model predicts the same future event as the forecast origin changes. We introduce the forecast accuracy and coherence score (forecast AC score for short) for measuring the quality of probabilistic multi-horizon forecasts in a way that accounts for both multi-horizon accuracy and stability. Our score additionally provides for user-specified weights to balance accuracy and consistency requirements. As an example application, we implement the score as a differentiable objective function for training seasonal ARIMA models and evaluate it on the M4 Hourly benchmark dataset. Results demonstrate substantial improvements over traditional maximum likelihood estimation. Our AC-optimized models achieve a 75\% reduction in forecast volatility for the same target timestamps while maintaining comparable or improved point forecast accuracy.

</details>


### [11] [Action Shapley: A Training Data Selection Metric for World Model in Reinforcement Learning](https://arxiv.org/abs/2601.10905)
*Rajat Ghosh,Debojyoti Dutta*

Main category: cs.LG

TL;DR: 提出Action Shapley作为训练数据选择的公平度量方法，通过随机动态算法降低计算复杂度，在数据受限的真实场景中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，世界模型的质量对系统效能和可解释性至关重要，而训练数据质量直接影响世界模型性能。现有训练数据选择方法缺乏系统性和公平性，需要一种无偏的数据选择度量方法。

Method: 提出Action Shapley作为训练数据选择的公平度量标准，并设计随机动态算法来降低传统Shapley值计算的指数复杂度。该方法不依赖于特定模型，适用于各种世界模型。

Result: 算法计算效率比传统指数时间计算提升超过80%。在五个数据受限的真实案例研究中，基于Action Shapley的训练数据选择策略始终优于临时性数据选择方法。

Conclusion: Action Shapley为训练数据选择提供了有效的公平度量方法，其高效计算算法使其在实际应用中具有可行性，能够显著提升世界模型的训练效果。

Abstract: Numerous offline and model-based reinforcement learning systems incorporate world models to emulate the inherent environments. A world model is particularly important in scenarios where direct interactions with the real environment is costly, dangerous, or impractical. The efficacy and interpretability of such world models are notably contingent upon the quality of the underlying training data. In this context, we introduce Action Shapley as an agnostic metric for the judicious and unbiased selection of training data. To facilitate the computation of Action Shapley, we present a randomized dynamic algorithm specifically designed to mitigate the exponential complexity inherent in traditional Shapley value computations. Through empirical validation across five data-constrained real-world case studies, the algorithm demonstrates a computational efficiency improvement exceeding 80\% in comparison to conventional exponential time computations. Furthermore, our Action Shapley-based training data selection policy consistently outperforms ad-hoc training data selection.

</details>


### [12] [Realistic Curriculum Reinforcement Learning for Autonomous and Sustainable Marine Vessel Navigation](https://arxiv.org/abs/2601.10911)
*Zhang Xiaocai,Xiao Zhe,Liang Maohan,Liu Tao,Li Haijiang,Zhang Wenbin*

Main category: cs.LG

TL;DR: 提出一个结合课程强化学习、数据驱动海洋仿真环境和机器学习燃料预测的框架，用于实现可持续和安全的船舶导航


<details>
  <summary>Details</summary>
Motivation: 海事运输的可持续性日益重要，传统船舶导航依赖人工经验，缺乏自主性和排放意识，容易因人为错误影响安全

Method: 提出课程强化学习框架，集成数据驱动的海洋仿真环境（使用真实船舶运动数据和扩散模型模拟动态条件）、基于机器学习的燃料消耗预测模块，以及轻量级策略型CRL智能体，采用综合考虑安全、排放、及时性和目标完成的奖励机制

Result: 在印度洋海域验证了该方法的有效性，能够实现可持续和安全的船舶导航

Conclusion: 该框架能够渐进处理复杂任务，在连续动作空间中确保稳定高效的学习，为可持续船舶导航提供了有效解决方案

Abstract: Sustainability is becoming increasingly critical in the maritime transport, encompassing both environmental and social impacts, such as Greenhouse Gas (GHG) emissions and navigational safety. Traditional vessel navigation heavily relies on human experience, often lacking autonomy and emission awareness, and is prone to human errors that may compromise safety. In this paper, we propose a Curriculum Reinforcement Learning (CRL) framework integrated with a realistic, data-driven marine simulation environment and a machine learning-based fuel consumption prediction module. The simulation environment is constructed using real-world vessel movement data and enhanced with a Diffusion Model to simulate dynamic maritime conditions. Vessel fuel consumption is estimated using historical operational data and learning-based regression. The surrounding environment is represented as image-based inputs to capture spatial complexity. We design a lightweight, policy-based CRL agent with a comprehensive reward mechanism that considers safety, emissions, timeliness, and goal completion. This framework effectively handles complex tasks progressively while ensuring stable and efficient learning in continuous action spaces. We validate the proposed approach in a sea area of the Indian Ocean, demonstrating its efficacy in enabling sustainable and safe vessel navigation.

</details>


### [13] [FAConvLSTM: Factorized-Attention ConvLSTM for Efficient Feature Extraction in Multivariate Climate Data](https://arxiv.org/abs/2601.10914)
*Francis Ndikum Nji,Jianwu Wang*

Main category: cs.LG

TL;DR: FAConvLSTM是一种改进的ConvLSTM2D模型，通过因子化注意力机制、多尺度深度可分离卷积和轴向空间注意力，在保持循环动态的同时显著降低计算成本，提高对地球观测数据的时空建模能力。


<details>
  <summary>Details</summary>
Motivation: 传统ConvLSTM2D在处理高分辨率多元地球观测数据时面临挑战：密集卷积门控计算成本高，局部感受野限制了对长程空间结构和解耦气候动态的建模能力。

Method: 提出FAConvLSTM作为ConvLSTM2D的直接替代方案，采用因子化门控计算（轻量级1×1瓶颈和共享深度空间混合）、多尺度扩张深度可分离分支、挤压-激励重校准、窥视孔连接，以及轻量级轴向空间注意力机制和专用子空间头。

Result: 在多元时空气候数据上的实验表明，FAConvLSTM比标准ConvLSTM产生更稳定、可解释和鲁棒的潜在表示，同时显著降低计算开销。

Conclusion: FAConvLSTM通过因子化注意力设计，在效率、空间表达能力和物理可解释性方面同时改进，为地球观测数据的时空建模提供了更优的解决方案。

Abstract: Learning physically meaningful spatiotemporal representations from high-resolution multivariate Earth observation data is challenging due to strong local dynamics, long-range teleconnections, multi-scale interactions, and nonstationarity. While ConvLSTM2D is a commonly used baseline, its dense convolutional gating incurs high computational cost and its strictly local receptive fields limit the modeling of long-range spatial structure and disentangled climate dynamics. To address these limitations, we propose FAConvLSTM, a Factorized-Attention ConvLSTM layer designed as a drop-in replacement for ConvLSTM2D that simultaneously improves efficiency, spatial expressiveness, and physical interpretability. FAConvLSTM factorizes recurrent gate computations using lightweight [1 times 1] bottlenecks and shared depthwise spatial mixing, substantially reducing channel complexity while preserving recurrent dynamics. Multi-scale dilated depthwise branches and squeeze-and-excitation recalibration enable efficient modeling of interacting physical processes across spatial scales, while peephole connections enhance temporal precision. To capture teleconnection-scale dependencies without incurring global attention cost, FAConvLSTM incorporates a lightweight axial spatial attention mechanism applied sparsely in time. A dedicated subspace head further produces compact per timestep embeddings refined through temporal self-attention with fixed seasonal positional encoding. Experiments on multivariate spatiotemporal climate data shows superiority demonstrating that FAConvLSTM yields more stable, interpretable, and robust latent representations than standard ConvLSTM, while significantly reducing computational overhead.

</details>


### [14] [HOSL: Hybrid-Order Split Learning for Memory-Constrained Edge Training](https://arxiv.org/abs/2601.10940)
*Aakriti,Zhe Li,Dandan Liang,Chao Huang,Rui Li,Haibo Yang*

Main category: cs.LG

TL;DR: HOSL提出了一种混合阶分割学习框架，通过在客户端使用零阶优化减少内存占用，在服务器端使用一阶优化保证性能，解决了分割学习中内存效率与优化效果之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有分割学习系统主要依赖一阶优化，需要客户端存储激活值等中间量，导致内存开销大，削弱了模型分割的优势。零阶优化虽然减少内存使用，但收敛慢且性能下降。需要解决内存效率与优化效果之间的根本权衡。

Method: 提出HOSL混合阶分割学习框架：客户端采用内存高效的零阶梯度估计，消除反向传播和激活存储；服务器端采用一阶优化确保快速收敛和竞争性能。理论上证明收敛率取决于客户端模型维度而非完整模型维度。

Result: 在OPT模型（125M和1.3B参数）的6个任务上实验表明：HOSL相比一阶方法减少客户端GPU内存达3.7倍，精度仅下降0.20%-4.23%；相比零阶基线性能提升达15.55%，验证了混合策略在边缘设备内存高效训练中的有效性。

Conclusion: HOSL通过客户端零阶优化和服务器端一阶优化的策略性集成，有效解决了分割学习中内存效率与优化效果之间的权衡，为资源受限边缘设备上的大语言模型协作训练提供了实用解决方案。

Abstract: Split learning (SL) enables collaborative training of large language models (LLMs) between resource-constrained edge devices and compute-rich servers by partitioning model computation across the network boundary. However, existing SL systems predominantly rely on first-order (FO) optimization, which requires clients to store intermediate quantities such as activations for backpropagation. This results in substantial memory overhead, largely negating benefits of model partitioning. In contrast, zeroth-order (ZO) optimization eliminates backpropagation and significantly reduces memory usage, but often suffers from slow convergence and degraded performance. In this work, we propose HOSL, a novel Hybrid-Order Split Learning framework that addresses this fundamental trade-off between memory efficiency and optimization effectiveness by strategically integrating ZO optimization on the client side with FO optimization on the server side. By employing memory-efficient ZO gradient estimation at the client, HOSL eliminates backpropagation and activation storage, reducing client memory consumption. Meanwhile, server-side FO optimization ensures fast convergence and competitive performance. Theoretically, we show that HOSL achieves a $\mathcal{O}(\sqrt{d_c/TQ})$ rate, which depends on client-side model dimension $d_c$ rather than the full model dimension $d$, demonstrating that convergence improves as more computation is offloaded to the server. Extensive experiments on OPT models (125M and 1.3B parameters) across 6 tasks demonstrate that HOSL reduces client GPU memory by up to 3.7$\times$ compared to the FO method while achieving accuracy within 0.20%-4.23% of this baseline. Furthermore, HOSL outperforms the ZO baseline by up to 15.55%, validating the effectiveness of our hybrid strategy for memory-efficient training on edge devices.

</details>


### [15] [Toward Adaptive Grid Resilience: A Gradient-Free Meta-RL Framework for Critical Load Restoration](https://arxiv.org/abs/2601.10973)
*Zain ul Abdeen,Waris Gill,Ming Jin*

Main category: cs.LG

TL;DR: 提出MGF-RL框架，结合元学习和进化策略，用于配电网故障恢复，能在不确定性和非线性约束下快速适应新场景


<details>
  <summary>Details</summary>
Motivation: 极端事件后恢复关键负荷需要自适应控制，但可再生能源不确定性、可调度资源有限和非线性动态使有效恢复困难。标准强化学习泛化能力差，需要大量重新训练

Method: 提出元引导的无梯度强化学习框架，结合一阶元学习和进化策略，从历史故障经验学习可迁移初始化，快速适应未见场景，无需梯度计算

Result: 在IEEE 13总线和123总线测试系统中，MGF-RL优于标准RL、基于MAML的元RL和模型预测控制，在可靠性、恢复速度和适应效率方面表现更好，泛化能力强且需要更少微调

Conclusion: MGF-RL框架适用于可再生能源丰富的配电网实时负荷恢复，提供次线性遗憾界，支持实证增益并激励实际应用

Abstract: Restoring critical loads after extreme events demands adaptive control to maintain distribution-grid resilience, yet uncertainty in renewable generation, limited dispatchable resources, and nonlinear dynamics make effective restoration difficult. Reinforcement learning (RL) can optimize sequential decisions under uncertainty, but standard RL often generalizes poorly and requires extensive retraining for new outage configurations or generation patterns. We propose a meta-guided gradient-free RL (MGF-RL) framework that learns a transferable initialization from historical outage experiences and rapidly adapts to unseen scenarios with minimal task-specific tuning. MGF-RL couples first-order meta-learning with evolutionary strategies, enabling scalable policy search without gradient computation while accommodating nonlinear, constrained distribution-system dynamics. Experiments on IEEE 13-bus and IEEE 123-bus test systems show that MGF-RL outperforms standard RL, MAML-based meta-RL, and model predictive control across reliability, restoration speed, and adaptation efficiency under renewable forecast errors. MGF-RL generalizes to unseen outages and renewable patterns while requiring substantially fewer fine-tuning episodes than conventional RL. We also provide sublinear regret bounds that relate adaptation efficiency to task similarity and environmental variation, supporting the empirical gains and motivating MGF-RL for real-time load restoration in renewable-rich distribution grids.

</details>


### [16] [Reasoning Distillation for Lightweight Automated Program Repair](https://arxiv.org/abs/2601.10987)
*Aanand Balasubramanian,Sashank Silwal*

Main category: cs.LG

TL;DR: 轻量级符号推理监督能提升紧凑型自动程序修复模型的修复类型分类性能，通过大模型提供结构化推理标签来增强小模型的学习能力。


<details>
  <summary>Details</summary>
Motivation: 小型代码模型在资源受限环境下具有吸引力，但它们通常只产生单一预测，难以判断是否真正学习了程序结构还是依赖浅层相关性。需要提升轻量级程序修复模型的可解释性和鲁棒性。

Method: 提出推理蒸馏方法：使用大型教师模型为修复类型标签提供结构化符号推理标签，这些标签捕获bug的高层因果属性而不依赖自由形式解释。在IntroClass基准上训练CodeT5基础的学生模型，比较纯标签训练和推理蒸馏训练的效果。

Result: 推理监督持续提升宏平均性能，特别是在较少出现的bug类别上表现更佳，且不增加模型大小或复杂度。正确推理轨迹与正确预测强相关，但不完全决定预测结果。

Conclusion: 符号推理蒸馏是提升轻量级程序修复模型可解释性和鲁棒性的实用方法，能够帮助小模型更好地学习程序结构而非依赖浅层相关性。

Abstract: We study whether lightweight symbolic reasoning supervision can improve fix type classification in compact automated program repair models. Small code models are attractive for resource-constrained settings, but they typically produce only a single prediction, making it unclear whether they learn meaningful program structure or rely on shallow correlations. We propose a reasoning distillation approach in which a large teacher model provides structured symbolic reasoning tags alongside fix-type labels. These tags capture high-level causal properties of bugs without relying on free-form explanations. We train a CodeT5-based student model under label-only and reasoning-distilled settings on the IntroClass benchmark. Reasoning supervision consistently improves macro averaged performance, particularly on less frequent bug categories, without increasing model size or complexity. We further analyze the relationship between reasoning accuracy and fix-type prediction, showing that correct reasoning traces strongly correlate with correct predictions, while not fully determining them. Our results suggest that symbolic reasoning distillation is a practical way to improve interpretability and robustness in lightweight program repair models.

</details>


### [17] [Constant Metric Scaling in Riemannian Computation](https://arxiv.org/abs/2601.10992)
*Kisung You*

Main category: cs.LG

TL;DR: 本文探讨了黎曼度量常数缩放的计算意义，区分了受缩放影响的量（范数、距离、体积、梯度大小）和保持不变的几何对象（Levi-Civita联络、测地线、指数映射等），并讨论了在黎曼优化中的实际应用。


<details>
  <summary>Details</summary>
Motivation: 在计算环境中，黎曼度量的常数缩放经常出现，但这一操作的影响在实践中并不总是清晰，可能与曲率变化、流形结构变化或坐标表示变化相混淆。本文旨在澄清常数度量缩放对黎曼流形的影响，帮助计算实践者正确理解这一基本操作。

Method: 本文采用理论分析方法，系统研究了常数度量缩放对黎曼几何中各种数学对象的影响。通过区分受缩放影响的量（如范数、距离、体积元素、梯度大小）和保持不变的几何对象（如Levi-Civita联络、测地线、指数映射、对数映射、平行移动），提供了完整的理论框架。

Result: 分析表明，常数度量缩放会改变范数、距离、体积元素和梯度大小等度量相关量，但不会改变Levi-Civita联络、测地线、指数映射、对数映射和平行移动等几何结构。在黎曼优化中，这种缩放通常可以解释为步长的全局重新缩放，而不是底层几何的修改。

Conclusion: 本文澄清了常数度量缩放的计算意义，表明可以在不改变底层几何结构的情况下引入全局度量尺度参数。这对于黎曼计算实践具有重要意义，帮助研究者正确区分几何不变性和度量依赖性，避免对计算结果的误解。

Abstract: Constant rescaling of a Riemannian metric appears in many computational settings, often through a global scale parameter that is introduced either explicitly or implicitly. Although this operation is elementary, its consequences are not always made clear in practice and may be confused with changes in curvature, manifold structure, or coordinate representation. In this note we provide a short, self-contained account of constant metric scaling on arbitrary Riemannian manifolds. We distinguish between quantities that change under such a scaling, including norms, distances, volume elements, and gradient magnitudes, and geometric objects that remain invariant, such as the Levi--Civita connection, geodesics, exponential and logarithmic maps, and parallel transport. We also discuss implications for Riemannian optimization, where constant metric scaling can often be interpreted as a global rescaling of step sizes rather than a modification of the underlying geometry. The goal of this note is purely expository and is intended to clarify how a global metric scale parameter can be introduced in Riemannian computation without altering the geometric structures on which these methods rely.

</details>


### [18] [Backdoor Attacks on Multi-modal Contrastive Learning](https://arxiv.org/abs/2601.11006)
*Simi D Kuniyilh,Rita Machacy*

Main category: cs.LG

TL;DR: 本文对对比学习中的后门攻击进行了全面比较性综述，分析了威胁模型、攻击方法、目标领域和现有防御措施，总结了该领域最新进展，强调了对比学习特有的脆弱性，并讨论了挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 对比学习已成为跨领域自监督表示学习的主要方法，但最近研究表明对比学习容易受到后门和数据投毒攻击。攻击者可以通过操纵预训练数据或模型更新来插入隐藏的恶意行为。本文旨在系统分析对比学习中的后门攻击问题。

Method: 采用系统性文献综述方法，对对比学习中的后门攻击进行全面的比较分析。分析内容包括威胁模型、攻击方法、目标领域（视觉、多模态、图、联邦学习等）以及可用的防御措施。

Result: 总结了对比学习后门攻击领域的最新进展，识别了对比学习特有的脆弱性，分析了现有防御方法的有效性，并指出了当前研究中的关键挑战。

Conclusion: 对比学习中的后门攻击是一个严重的安全威胁，特别是在工业和分布式环境中。需要进一步研究来开发更有效的防御机制，确保对比学习系统的安全部署。本文为未来研究提供了重要的方向和启示。

Abstract: Contrastive learning has become a leading self- supervised approach to representation learning across domains, including vision, multimodal settings, graphs, and federated learning. However, recent studies have shown that contrastive learning is susceptible to backdoor and data poisoning attacks. In these attacks, adversaries can manipulate pretraining data or model updates to insert hidden malicious behavior. This paper offers a thorough and comparative review of backdoor attacks in contrastive learning. It analyzes threat models, attack methods, target domains, and available defenses. We summarize recent advancements in this area, underline the specific vulnerabilities inherent to contrastive learning, and discuss the challenges and future research directions. Our findings have significant implications for the secure deployment of systems in industrial and distributed environments.

</details>


### [19] [Combating Spurious Correlations in Graph Interpretability via Self-Reflection](https://arxiv.org/abs/2601.11021)
*Kecheng Cai,Chenyang Xu,Chao Peng*

Main category: cs.LG

TL;DR: 本文提出一种自反思框架，用于提升图学习模型在Spurious-Motif基准数据集上的可解释性，通过反馈机制迭代评估节点和边的重要性分数。


<details>
  <summary>Details</summary>
Motivation: Spurious-Motif基准数据集因包含虚假相关性而极具挑战性，现有方法在该数据集上表现显著较差。需要提升图学习模型在存在强虚假相关性的数据集上的可解释性，以更好地区分真正相关结构与误导性模式。

Method: 提出自反思框架，可与现有可解释图学习方法集成。当这些方法生成节点和边的重要性分数后，框架将这些预测反馈给原始方法进行第二轮评估，形成迭代过程。同时从图表示学习角度分析改进原因，提出基于此反馈机制的微调训练方法。

Result: 自反思技术能有效提升模型在具有强虚假相关性的Spurious-Motif数据集上的可解释性表现，通过迭代评估机制改善重要性分数的准确性。

Conclusion: 借鉴大语言模型的自反思技术可成功应用于图学习领域，特别是在处理具有虚假相关性的挑战性数据集时，通过反馈机制能显著提升模型的可解释性。

Abstract: Interpretable graph learning has recently emerged as a popular research topic in machine learning. The goal is to identify the important nodes and edges of an input graph that are crucial for performing a specific graph reasoning task. A number of studies have been conducted in this area, and various benchmark datasets have been proposed to facilitate evaluation. Among them, one of the most challenging is the Spurious-Motif benchmark, introduced at ICLR 2022. The datasets in this synthetic benchmark are deliberately designed to include spurious correlations, making it particularly difficult for models to distinguish truly relevant structures from misleading patterns. As a result, existing methods exhibit significantly worse performance on this benchmark compared to others.
  In this paper, we focus on improving interpretability on the challenging Spurious-Motif datasets. We demonstrate that the self-reflection technique, commonly used in large language models to tackle complex tasks, can also be effectively adapted to enhance interpretability in datasets with strong spurious correlations. Specifically, we propose a self-reflection framework that can be integrated with existing interpretable graph learning methods. When such a method produces importance scores for each node and edge, our framework feeds these predictions back into the original method to perform a second round of evaluation. This iterative process mirrors how large language models employ self-reflective prompting to reassess their previous outputs. We further analyze the reasons behind this improvement from the perspective of graph representation learning, which motivates us to propose a fine-tuning training method based on this feedback mechanism.

</details>


### [20] [AVP-Pro: An Adaptive Multi-Modal Fusion and Contrastive Learning Approach for Comprehensive Two-Stage Antiviral Peptide Identification](https://arxiv.org/abs/2601.11028)
*Xinru Wen,Weizhong Lin,zi liu,Xuan Xiao*

Main category: cs.LG

TL;DR: AVP-Pro是一个两阶段抗病毒肽预测框架，通过自适应特征融合和对比学习提高识别准确性，在通用AVP识别和功能亚型预测中都优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有抗病毒肽识别方法在捕捉复杂序列依赖性和区分高相似度样本方面存在局限，需要更精确的预测工具来支持新药开发

Method: 提出两阶段预测框架：1)构建包含10种描述符的全景特征空间，设计分层融合架构整合自注意力和自适应门控机制；2)采用基于BLOSUM62增强的OHEM对比学习策略，结合迁移学习进行功能亚型预测

Result: 在通用AVP识别阶段，准确率达0.9531，MCC为0.9064，优于现有SOTA方法；在功能亚型预测阶段，实现了6个病毒家族和8种特定病毒在小样本条件下的准确分类

Conclusion: AVP-Pro为抗病毒药物高通量筛选提供了强大且可解释的新工具，并开发了用户友好的Web界面

Abstract: The accurate identification of antiviral peptides (AVPs) is crucial for novel drug development. However, existing methods still have limitations in capturing complex sequence dependencies and distinguishing confusing samples with high similarity. To address these challenges, we propose AVP-Pro, a novel two-stage predictive framework that integrates adaptive feature fusion and contrastive learning. To comprehensively capture the physicochemical properties and deep-seated patterns of peptide sequences, we constructed a panoramic feature space encompassing 10 distinct descriptors and designed a hierarchical fusion architecture. This architecture integrates self-attention and adaptive gating mechanisms to dynamically modulate the weights of local motifs extracted by CNNs and global dependencies captured by BiLSTMs based on sequence context. Targeting the blurred decision boundary caused by the high similarity between positive and negative sample sequences, we adopted an Online Hard Example Mining (OHEM)-driven contrastive learning strategy enhanced by BLOSUM62. This approach significantly sharpened the model's discriminative power. Model evaluation results show that in the first stage of general AVP identification, the model achieved an accuracy of 0.9531 and an MCC of 0.9064, outperforming existing state-of-the-art (SOTA) methods. In the second stage of functional subtype prediction, combined with a transfer learning strategy, the model realized accurate classification of 6 viral families and 8 specific viruses under small-sample conditions. AVP-Pro provides a powerful and interpretable new tool for the high-throughput screening of antiviral drugs. To further enhance accessibility for users, we have developed a user-friendly web interface, which is available at https://wwwy1031-avp-pro.hf.space.

</details>


### [21] [OpFML: Pipeline for ML-based Operational Forecasting](https://arxiv.org/abs/2601.11046)
*Shahbaz Alvi,Giusy Fedele,Gabriele Accarino,Italo Epicoco,Ilenia Manco,Pasquale Schiano*

Main category: cs.LG

TL;DR: OpFML是一个可配置、可适应的机器学习管道，用于周期性预测，特别应用于每日火灾危险指数预测


<details>
  <summary>Details</summary>
Motivation: 机器学习在气候和地球科学中应用广泛，传统火灾风险评估方法经常高估风险，需要更准确的数据驱动预测系统

Method: 开发了OpFML（Operational Forecasting with Machine Learning）管道，这是一个可配置、可适应的机器学习模型服务框架，支持周期性预测任务

Result: 成功开发了OpFML管道，并展示了其在每日火灾危险指数预测中的应用能力，突出了管道的各种功能特性

Conclusion: OpFML为机器学习模型的周期性预测提供了一个灵活、可配置的解决方案，在火灾危险评估等气候科学应用中具有实用价值

Abstract: Machine learning is finding its application in a multitude of areas in science and research, and Climate and Earth Sciences is no exception to this trend. Operational forecasting systems based on data-driven approaches and machine learning methods deploy models for periodic forecasting. Wildfire danger assessment using machine learning has garnered significant interest in the last decade, as conventional methods often overestimate the risk of wildfires. In this work, we present the code OpFML: Operational Forecasting with Machine Learning. OpFML is a configurable and adaptable pipeline that can be utilized to serve a machine learning model for periodic forecasting. We further demonstrate the capabilities of the pipeline through its application to daily Fire Danger Index forecasting and outline its various features.

</details>


### [22] [Soft Bayesian Context Tree Models for Real-Valued Time Series](https://arxiv.org/abs/2601.11079)
*Shota Saito,Yuta Nakahara,Toshiyasu Matsushima*

Main category: cs.LG

TL;DR: 提出Soft-BCT模型，用于实值时间序列，采用软（概率）分割而非硬分割，基于变分推断学习，在真实数据集上表现优于或与先前BCT相当


<details>
  <summary>Details</summary>
Motivation: 传统的贝叶斯上下文树（BCT）模型对实值时间序列使用硬（确定性）分割，这可能限制了模型的表达能力。为了更灵活地建模上下文空间，需要一种能够进行软（概率）分割的BCT模型。

Method: 提出软贝叶斯上下文树模型（Soft-BCT），采用概率分割而非确定性分割来划分上下文空间。基于变分推断开发了相应的学习算法。

Result: 在多个真实世界数据集上，Soft-BCT表现出与先前BCT相当或更优的性能。

Conclusion: Soft-BCT通过引入软分割机制，为实值时间序列建模提供了更灵活的贝叶斯上下文树框架，在保持或提升性能的同时增强了模型的表达能力。

Abstract: This paper proposes the soft Bayesian context tree model (Soft-BCT), which is a novel BCT model for real-valued time series. The Soft-BCT considers soft (probabilistic) splits of the context space, instead of hard (deterministic) splits of the context space as in the previous BCT for real-valued time series. A learning algorithm of the Soft-BCT is proposed based on the variational inference. For some real-world datasets, the Soft-BCT demonstrates almost the same or superior performance to the previous BCT.

</details>


### [23] [Differentially Private Subspace Fine-Tuning for Large Language Models](https://arxiv.org/abs/2601.11113)
*Lele Zheng,Xiang Wang,Tao Zhang,Yang Cao,Ke Cheng,Yulong Shen*

Main category: cs.LG

TL;DR: DP-SFT：一种两阶段子空间微调方法，通过将DP噪声仅注入到任务特定的低维子空间，在保证差分隐私的同时显著降低噪声幅度，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在下游任务上的微调通常依赖敏感数据，引发隐私担忧。传统差分隐私方法在高维参数空间中注入噪声会产生大范数扰动，导致性能下降和训练不稳定。

Method: 提出两阶段子空间微调方法：第一阶段通过分析主梯度方向识别任务特定的低维子空间；第二阶段将完整梯度投影到该子空间，添加DP噪声，然后将扰动后的梯度映射回原始参数空间进行模型更新。

Result: 在多个数据集上的实验表明，DP-SFT在严格的DP约束下提高了准确性和稳定性，加速了收敛速度，相比DP微调基线取得了显著提升。

Conclusion: DP-SFT通过将DP噪声限制在任务相关的低维子空间，在保证形式化隐私保护的同时显著降低了噪声影响，为大语言模型的隐私保护微调提供了有效解决方案。

Abstract: Fine-tuning large language models on downstream tasks is crucial for realizing their cross-domain potential but often relies on sensitive data, raising privacy concerns. Differential privacy (DP) offers rigorous privacy guarantees and has been widely adopted in fine-tuning; however, naively injecting noise across the high-dimensional parameter space creates perturbations with large norms, degrading performance and destabilizing training. To address this issue, we propose DP-SFT, a two-stage subspace fine-tuning method that substantially reduces noise magnitude while preserving formal DP guarantees. Our intuition is that, during fine-tuning, significant parameter updates lie within a low-dimensional, task-specific subspace, while other directions change minimally. Hence, we only inject DP noise into this subspace to protect privacy without perturbing irrelevant parameters. In phase one, we identify the subspace by analyzing principal gradient directions to capture task-specific update signals. In phase two, we project full gradients onto this subspace, add DP noise, and map the perturbed gradients back to the original parameter space for model updates, markedly lowering noise impact. Experiments on multiple datasets demonstrate that DP-SFT enhances accuracy and stability under rigorous DP constraints, accelerates convergence, and achieves substantial gains over DP fine-tuning baselines.

</details>


### [24] [Optimized Algorithms for Text Clustering with LLM-Generated Constraints](https://arxiv.org/abs/2601.11118)
*Chaoqi Jia,Weihong Wu,Longkun Guo,Zhigang Lu,Chao Chen,Kok-Leong Ong*

Main category: cs.LG

TL;DR: 提出了一种基于大语言模型的约束生成方法，通过生成约束集而非传统成对约束来减少资源消耗，同时设计了专门的约束聚类算法来处理可能不准确的约束。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法通过人工标注的must-link和cannot-link约束来提高准确性，但这种方法成本高且难以扩展。随着大语言模型的发展，研究者开始探索利用LLM自动生成约束，但现有方法在查询效率和约束准确性方面仍有改进空间。

Method: 1) 提出新的约束生成方法，生成约束集而非成对约束，减少LLM查询次数；2) 设计专门的约束聚类算法，包含置信度阈值和惩罚机制来处理可能不准确的约束；3) 在五个文本数据集上进行评估，同时考虑约束生成成本和聚类性能。

Result: 该方法在保持与最先进算法相当的聚类准确性的同时，将LLM查询次数减少了20倍以上，显著降低了资源消耗。

Conclusion: 提出的基于LLM的约束生成和聚类方法能够有效提高聚类效率，在减少资源消耗的同时保持高质量的聚类结果，为文本聚类任务提供了实用的解决方案。

Abstract: Clustering is a fundamental tool that has garnered significant interest across a wide range of applications including text analysis. To improve clustering accuracy, many researchers have incorporated background knowledge, typically in the form of must-link and cannot-link constraints, to guide the clustering process. With the recent advent of large language models (LLMs), there is growing interest in improving clustering quality through LLM-based automatic constraint generation. In this paper, we propose a novel constraint-generation approach that reduces resource consumption by generating constraint sets rather than using traditional pairwise constraints. This approach improves both query efficiency and constraint accuracy compared to state-of-the-art methods. We further introduce a constrained clustering algorithm tailored to the characteristics of LLM-generated constraints. Our method incorporates a confidence threshold and a penalty mechanism to address potentially inaccurate constraints. We evaluate our approach on five text datasets, considering both the cost of constraint generation and the overall clustering performance. The results show that our method achieves clustering accuracy comparable to the state-of-the-art algorithms while reducing the number of LLM queries by more than 20 times.

</details>


### [25] [Shape-morphing programming of soft materials on complex geometries via neural operator](https://arxiv.org/abs/2601.11126)
*Lu Chen,Gengxiang Chen,Xu Liu,Jingyan Su,Xuhao Lyu,Lihui Wang,Yingguang Li*

Main category: cs.LG

TL;DR: 提出S2NO神经算子，通过谱空间卷积实现复杂几何体上的高保真形变预测，结合进化算法优化材料分布，实现超分辨率形变设计


<details>
  <summary>Details</summary>
Motivation: 现有形状变形软材料设计主要针对简单几何体，难以在复杂几何体上实现精确多样的形变设计，限制了在植入物部署、空气动力学变形等高级应用中的潜力

Method: 提出谱空间神经算子(S2NO)，集成拉普拉斯特征函数编码和空间卷积，有效捕捉不规则计算域上的全局和局部变形行为；结合进化算法优化体素级材料分布

Result: S2NO能够在复杂几何体（不规则边界形状、多孔结构、薄壁结构）上实现高保真形变预测，神经算子的离散不变性支持超分辨率材料分布设计

Conclusion: 该方法显著提高了复杂形状变形编程的效率和能力，为高级应用如共形植入物部署和空气动力学变形提供了有效解决方案

Abstract: Shape-morphing soft materials can enable diverse target morphologies through voxel-level material distribution design, offering significant potential for various applications. Despite progress in basic shape-morphing design with simple geometries, achieving advanced applications such as conformal implant deployment or aerodynamic morphing requires accurate and diverse morphing designs on complex geometries, which remains challenging. Here, we present a Spectral and Spatial Neural Operator (S2NO), which enables high-fidelity morphing prediction on complex geometries. S2NO effectively captures global and local morphing behaviours on irregular computational domains by integrating Laplacian eigenfunction encoding and spatial convolutions. Combining S2NO with evolutionary algorithms enables voxel-level optimisation of material distributions for shape morphing programming on various complex geometries, including irregular-boundary shapes, porous structures, and thin-walled structures. Furthermore, the neural operator's discretisation-invariant property enables super-resolution material distribution design, further expanding the diversity and complexity of morphing design. These advancements significantly improve the efficiency and capability of programming complex shape morphing.

</details>


### [26] [FSL-BDP: Federated Survival Learning with Bayesian Differential Privacy for Credit Risk Modeling](https://arxiv.org/abs/2601.11134)
*Sultan Amed,Tanmay Sen,Sayantan Banerjee*

Main category: cs.LG

TL;DR: 论文提出了一个联邦生存学习框架FSL-BDP，用于在保护隐私的前提下进行跨机构的信用风险建模，解决了传统违约预测忽略违约时间和数据共享限制的问题。


<details>
  <summary>Details</summary>
Motivation: 信用风险模型对金融机构至关重要，但数据保护法规（如GDPR、CCPA）限制了跨境借款人数据共享，而传统违约预测存在两个局限：二元分类忽略了违约时间（将早期违约者与晚期违约者等同对待），集中式训练违反了新兴监管约束。

Method: 提出了联邦生存学习框架FSL-BDP，结合贝叶斯差分隐私，在不集中敏感数据的情况下建模违约时间轨迹。该框架提供贝叶斯（数据依赖）差分隐私保证，同时使机构能够共同学习风险动态。

Result: 在三个真实信用数据集（LendingClub、SBA、Bondora）上的实验表明，联邦学习从根本上改变了隐私机制的相对有效性。虽然经典差分隐私在集中式设置中表现优于贝叶斯差分隐私，但后者从联邦学习中获益更大（+7.0% vs +1.4%），在大多数参与客户中达到接近非私有性能并优于经典差分隐私。

Conclusion: 隐私机制选择应在目标部署架构中评估，而非基于集中式基准。这些发现为在受监管的多机构环境中设计隐私保护决策支持系统的从业者提供了可操作的指导。

Abstract: Credit risk models are a critical decision-support tool for financial institutions, yet tightening data-protection rules (e.g., GDPR, CCPA) increasingly prohibit cross-border sharing of borrower data, even as these models benefit from cross-institution learning. Traditional default prediction suffers from two limitations: binary classification ignores default timing, treating early defaulters (high loss) equivalently to late defaulters (low loss), and centralized training violates emerging regulatory constraints. We propose a Federated Survival Learning framework with Bayesian Differential Privacy (FSL-BDP) that models time-to-default trajectories without centralizing sensitive data. The framework provides Bayesian (data-dependent) differential privacy (DP) guarantees while enabling institutions to jointly learn risk dynamics. Experiments on three real-world credit datasets (LendingClub, SBA, Bondora) show that federation fundamentally alters the relative effectiveness of privacy mechanisms. While classical DP performs better than Bayesian DP in centralized settings, the latter benefits substantially more from federation (+7.0\% vs +1.4\%), achieving near parity of non-private performance and outperforming classical DP in the majority of participating clients. This ranking reversal yields a key decision-support insight: privacy mechanism selection should be evaluated in the target deployment architecture, rather than centralized benchmarks. These findings provide actionable guidance for practitioners designing privacy-preserving decision support systems in regulated, multi-institutional environments.

</details>


### [27] [Context-aware Graph Causality Inference for Few-Shot Molecular Property Prediction](https://arxiv.org/abs/2601.11135)
*Van Thuy Hoang,O-Joun Lee*

Main category: cs.LG

TL;DR: CaMol是一个基于因果推断的上下文感知图学习框架，用于小样本分子性质预测，通过编码化学知识、解耦因果子结构和应用后门调整来提高预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前基于上下文学习的分子性质预测方法存在两个主要限制：1) 未能充分利用与性质因果相关的官能团先验知识；2) 难以识别与性质直接相关的关键子结构。需要一种能够从因果推断角度解决这些挑战的新方法。

Method: CaMol采用因果推断框架，假设每个分子包含决定特定性质的潜在因果结构。方法包括：1) 构建编码化学知识的上下文图，连接官能团、分子和性质；2) 提出可学习的原子掩码策略，从混杂子结构中解耦因果子结构；3) 引入分布干预器，通过将因果子结构与化学基础混杂因子结合应用后门调整。

Result: 在多个分子数据集上的实验表明，CaMol在小样本任务中实现了优越的准确性和样本效率，展示了其对未见性质的泛化能力。发现的因果子结构与官能团的化学知识高度一致，支持了模型的可解释性。

Conclusion: CaMol通过因果推断视角有效解决了小样本分子性质预测中的关键挑战，不仅提高了预测性能，还增强了模型的可解释性，为基于Web的蛋白质结构预测和药物发现等服务提供了有力工具。

Abstract: Molecular property prediction is becoming one of the major applications of graph learning in Web-based services, e.g., online protein structure prediction and drug discovery. A key challenge arises in few-shot scenarios, where only a few labeled molecules are available for predicting unseen properties. Recently, several studies have used in-context learning to capture relationships among molecules and properties, but they face two limitations in: (1) exploiting prior knowledge of functional groups that are causally linked to properties and (2) identifying key substructures directly correlated with properties. We propose CaMol, a context-aware graph causality inference framework, to address these challenges by using a causal inference perspective, assuming that each molecule consists of a latent causal structure that determines a specific property. First, we introduce a context graph that encodes chemical knowledge by linking functional groups, molecules, and properties to guide the discovery of causal substructures. Second, we propose a learnable atom masking strategy to disentangle causal substructures from confounding ones. Third, we introduce a distribution intervener that applies backdoor adjustment by combining causal substructures with chemically grounded confounders, disentangling causal effects from real-world chemical variations. Experiments on diverse molecular datasets showed that CaMol achieved superior accuracy and sample efficiency in few-shot tasks, showing its generalizability to unseen properties. Also, the discovered causal substructures were strongly aligned with chemical knowledge about functional groups, supporting the model interpretability.

</details>


### [28] [Clustering High-dimensional Data: Balancing Abstraction and Representation Tutorial at AAAI 2026](https://arxiv.org/abs/2601.11160)
*Claudia Plant,Lena G. M. Bauer,Christian Böhm*

Main category: cs.LG

TL;DR: 聚类算法需要在抽象和表示之间取得平衡：抽象去除冗余细节，表示强调区分特征。K-means高度抽象但表示简单，子空间和深度聚类支持更丰富的表示，但需要明确强制抽象以避免仅学习表示。未来研究需更自适应地平衡两者。


<details>
  <summary>Details</summary>
Motivation: 解决大规模真实数据集自然分组的核心挑战：如何在聚类过程中平衡抽象（去除冗余细节）和表示（强调区分特征）。现有算法在这两个目标之间存在冲突，需要更好的平衡策略。

Method: 分析不同聚类算法的抽象-表示权衡：K-means（高抽象、简单表示）、子空间聚类（学习聚类相关和无关的潜在空间）、深度聚类（通过基于质心和密度的聚类损失强制抽象）。

Result: 不同聚类算法实现了不同的抽象-表示权衡。随着表示表达能力增强，需要在目标函数中明确强制抽象以确保进行聚类而非仅表示学习。子空间聚类通过分离相关和无关信息空间提供有益思路。

Conclusion: 聚类需要在抽象和表示之间找到最佳平衡点。未来研究应更自适应地平衡两者以提高性能、能效和可解释性。人脑在这方面表现出色，为算法改进提供了方向。

Abstract: How to find a natural grouping of a large real data set? Clustering requires a balance between abstraction and representation. To identify clusters, we need to abstract from superfluous details of individual objects. But we also need a rich representation that emphasizes the key features shared by groups of objects that distinguish them from other groups of objects.
  Each clustering algorithm implements a different trade-off between abstraction and representation. Classical K-means implements a high level of abstraction - details are simply averaged out - combined with a very simple representation - all clusters are Gaussians in the original data space. We will see how approaches to subspace and deep clustering support high-dimensional and complex data by allowing richer representations. However, with increasing representational expressiveness comes the need to explicitly enforce abstraction in the objective function to ensure that the resulting method performs clustering and not just representation learning. We will see how current deep clustering methods define and enforce abstraction through centroid-based and density-based clustering losses. Balancing the conflicting goals of abstraction and representation is challenging. Ideas from subspace clustering help by learning one latent space for the information that is relevant to clustering and another latent space to capture all other information in the data.
  The tutorial ends with an outlook on future research in clustering. Future methods will more adaptively balance abstraction and representation to improve performance, energy efficiency and interpretability. By automatically finding the sweet spot between abstraction and representation, the human brain is very good at clustering and other related tasks such as single-shot learning. So, there is still much room for improvement.

</details>


### [29] [Assesing the Viability of Unsupervised Learning with Autoencoders for Predictive Maintenance in Helicopter Engines](https://arxiv.org/abs/2601.11154)
*P. Sánchez,K. Reyes,B. Radu,E. Fernández*

Main category: cs.LG

TL;DR: 比较直升机发动机预测性维护的两种方法：有监督分类与基于自编码器的无监督异常检测，评估它们在真实数据集上的表现和适用场景


<details>
  <summary>Details</summary>
Motivation: 直升机发动机的意外故障会导致严重的运营中断、安全风险和昂贵的维修费用，需要有效的预测性维护策略来降低这些风险

Method: 比较两种方法：1) 有监督分类管道，依赖正常和故障行为的标注示例；2) 基于自编码器的无监督异常检测方法，仅使用健康发动机数据学习正常操作模型，将偏差标记为潜在故障

Result: 有监督模型在有标注故障数据时表现出色，而自编码器无需故障标签也能实现有效检测，特别适用于故障数据稀缺或不完整的场景

Conclusion: 比较揭示了准确性、数据可用性和部署可行性之间的实际权衡，强调了无监督学习作为航空航天应用早期故障检测可行解决方案的潜力

Abstract: Unplanned engine failures in helicopters can lead to severe operational disruptions, safety hazards, and costly repairs. To mitigate these risks, this study compares two predictive maintenance strategies for helicopter engines: a supervised classification pipeline and an unsupervised anomaly detection approach based on autoencoders (AEs). The supervised method relies on labelled examples of both normal and faulty behaviour, while the unsupervised approach learns a model of normal operation using only healthy engine data, flagging deviations as potential faults. Both methods are evaluated on a real-world dataset comprising labelled snapshots of helicopter engine telemetry. While supervised models demonstrate strong performance when annotated failures are available, the AE achieves effective detection without requiring fault labels, making it particularly well suited for settings where failure data are scarce or incomplete. The comparison highlights the practical trade-offs between accuracy, data availability, and deployment feasibility, and underscores the potential of unsupervised learning as a viable solution for early fault detection in aerospace applications.

</details>


### [30] [FAQ: Mitigating Quantization Error via Regenerating Calibration Data with Family-Aware Quantization](https://arxiv.org/abs/2601.11200)
*Haiyang Xiao,Weiqing Li,Jinyue Guo,Guochao Jiang,Guohua Liu,Yuewei Zhang*

Main category: cs.LG

TL;DR: FAQ提出了一种基于同系列大语言模型先验知识的校准数据再生框架，通过更大模型生成高保真校准样本，显著提升后训练量化的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统后训练量化方法依赖有限校准样本，难以捕捉推理阶段的激活分布，导致量化参数存在偏差。校准数据的代表性和通用性是决定量化精度的核心瓶颈。

Method: FAQ框架利用同系列LLM的先验知识，将原始校准样本输入更大的同系列模型，生成高保真校准数据。这些数据包含思维链推理并符合预期激活分布，通过专家指导下的组间竞争选择最佳样本，再进行重归一化以增强标准PTQ效果。

Result: 在包括Qwen3-8B在内的多个模型系列上的实验表明，FAQ相比使用原始校准数据的基线方法，将精度损失降低了高达28.5%。

Conclusion: FAQ通过利用同系列模型的知识系统生成高质量校准数据，有效解决了传统PTQ中校准数据代表性不足的问题，展示了强大的应用潜力和贡献。

Abstract: Although post-training quantization (PTQ) provides an efficient numerical compression scheme for deploying large language models (LLMs) on resource-constrained devices, the representativeness and universality of calibration data remain a core bottleneck in determining the accuracy of quantization parameters. Traditional PTQ methods typically rely on limited samples, making it difficult to capture the activation distribution during the inference phase, leading to biases in quantization parameters. To address this, we propose \textbf{FAQ} (Family-Aware Quantization), a calibration data regeneration framework that leverages prior knowledge from LLMs of the same family to generate high-fidelity calibration samples. Specifically, FAQ first inputs the original calibration samples into a larger LLM from the same family as the target model, regenerating a series of high-fidelity calibration data using a highly consistent knowledge system. Subsequently, this data, carrying Chain-of-Thought reasoning and conforming to the expected activation distribution, undergoes group competition under expert guidance to select the best samples, which are then re-normalized to enhance the effectiveness of standard PTQ. Experiments on multiple model series, including Qwen3-8B, show that FAQ reduces accuracy loss by up to 28.5\% compared to the baseline with original calibration data, demonstrating its powerful potential and contribution.

</details>


### [31] [SDFLoRA: Selective Dual-Module LoRA for Federated Fine-tuning with Heterogeneous Clients](https://arxiv.org/abs/2601.11219)
*Zhikang Shen,Jianrong Lu,Haiyuan Wan,Jianhai Chen*

Main category: cs.LG

TL;DR: SDFLoRA：一种解决联邦学习中LoRA适配器秩异构问题的选择性双模块方法，通过分离全局和本地模块实现鲁棒聚合与隐私保护


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的LoRA方法面临秩异构问题，不同客户端使用不同低秩配置导致直接聚合存在偏差和不稳定。现有解决方案强制统一秩或将异构更新对齐到共享子空间，这会过度约束客户端特定语义、限制个性化，并在差分隐私噪声下提供弱保护。

Method: 提出选择性双模块联邦LoRA（SDFLoRA），将每个客户端适配器分解为捕获可转移知识的全局模块和保留客户端特定适应的本地模块。全局模块在客户端间选择性对齐和聚合，本地模块保持私有。该设计支持在秩异构下的鲁棒学习，并通过仅向全局模块注入差分隐私噪声实现隐私感知优化。

Result: 在GLUE基准测试上的实验表明，SDFLoRA优于代表性的联邦LoRA基线方法，并实现了更好的效用-隐私权衡。

Conclusion: SDFLoRA通过分离全局和本地模块的设计，有效解决了联邦学习中LoRA适配器的秩异构问题，同时支持鲁棒聚合和隐私保护，在保持个性化能力的同时实现了更好的性能。

Abstract: Federated learning (FL) for large language models (LLMs) has attracted increasing attention as a way to enable privacy-preserving adaptation over distributed data. Parameter-efficient methods such as LoRA are widely adopted to reduce communication and memory costs. Despite these advances, practical FL deployments often exhibit rank heterogeneity, since different clients may use different low-rank configurations. This makes direct aggregation of LoRA updates biased and unstable. Existing solutions typically enforce unified ranks or align heterogeneous updates into a shared subspace, which over-constrains client-specific semantics, limits personalization, and provides weak protection of local client information under differential privacy noise. To address this issue, we propose Selective Dual-module Federated LoRA (SDFLoRA), which decomposes each client adapter into a global module that captures transferable knowledge and a local module that preserves client-specific adaptations. The global module is selectively aligned and aggregated across clients, while local modules remain private. This design enables robust learning under rank heterogeneity and supports privacy-aware optimization by injecting differential privacy noise exclusively into the global module. Experiments on GLUE benchmarks demonstrate that SDFLoRA outperforms representative federated LoRA baselines and achieves a better utility-privacy trade-off.

</details>


### [32] [Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation](https://arxiv.org/abs/2601.11258)
*Pingzhi Tang,Yiding Wang,Muhan Zhang*

Main category: cs.LG

TL;DR: PaST框架通过提取技能向量实现模块化技能转移，解决LLMs知识更新问题，在多个基准测试中显著提升性能


<details>
  <summary>Details</summary>
Motivation: 大型语言模型面临"知识截止"挑战，其冻结的参数化记忆无法直接内化新信息。监督微调虽然常用于更新模型知识，但往往只更新事实内容而无法可靠提升模型使用新信息进行问答或决策的能力。强化学习对于获取推理技能至关重要，但其高计算成本使得在线适应不切实际。

Method: 提出参数化技能转移框架，基于SFT和RL参数更新几乎正交的观察，从源域提取领域无关的技能向量，在目标模型经过轻量级SFT后，线性注入知识操作技能。

Result: 在知识整合问答基准上，PaST在SQuAD上比最先进的自我编辑SFT基线高出9.9分；在LooGLE长上下文问答上获得8.0分的绝对准确率提升；在ToolBench工具使用基准上，零样本成功率平均提高10.3分，跨工具类别表现一致。

Conclusion: PaST框架通过模块化技能转移有效解决了LLMs知识适应问题，技能向量展现出强大的可扩展性和跨领域可迁移性，为高效知识更新提供了新途径。

Abstract: Large Language Models (LLMs) face the "knowledge cutoff" challenge, where their frozen parametric memory prevents direct internalization of new information. While Supervised Fine-Tuning (SFT) is commonly used to update model knowledge, it often updates factual content without reliably improving the model's ability to use the newly incorporated information for question answering or decision-making. Reinforcement Learning (RL) is essential for acquiring reasoning skills; however, its high computational cost makes it impractical for efficient online adaptation. We empirically observe that the parameter updates induced by SFT and RL are nearly orthogonal. Based on this observation, we propose Parametric Skill Transfer (PaST), a framework that supports modular skill transfer for efficient and effective knowledge adaptation. By extracting a domain-agnostic Skill Vector from a source domain, we can linearly inject knowledge manipulation skills into a target model after it has undergone lightweight SFT on new data. Experiments on knowledge-incorporation QA (SQuAD, LooGLE) and agentic tool-use benchmarks (ToolBench) demonstrate the effectiveness of our method. On SQuAD, PaST outperforms the state-of-the-art self-editing SFT baseline by up to 9.9 points. PaST further scales to long-context QA on LooGLE with an 8.0-point absolute accuracy gain, and improves zero-shot ToolBench success rates by +10.3 points on average with consistent gains across tool categories, indicating strong scalability and cross-domain transferability of the Skill Vector.

</details>


### [33] [GMM-COMET: Continual Source-Free Universal Domain Adaptation via a Mean Teacher and Gaussian Mixture Model-Based Pseudo-Labeling](https://arxiv.org/abs/2601.11161)
*Pascal Schlachter,Bin Yang*

Main category: cs.LG

TL;DR: 该论文提出了首个持续源自由通用域自适应（continual SF-UniDA）方法GMM-COMET，用于处理多个不同未标记目标域的序列适应问题。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，源数据在适应过程中可能不再可用，且目标域标签空间可能与源域不同。现有SF-UniDA方法仅假设单一域偏移，而实际应用中模型需要连续适应多个不同目标域。

Method: 结合高斯混合模型伪标签和均值教师框架，引入一致性损失以提高鲁棒性。在在线SF-UniDA方法基础上改进，增强长期适应序列的稳定性。

Result: GMM-COMET在所有评估场景中持续优于仅使用源数据的模型，为持续SF-UniDA提供了首个强基线方法。

Conclusion: 该研究首次探索了持续SF-UniDA问题，提出的GMM-COMET方法在多个不同目标域的序列适应中表现出色，为这一新兴领域建立了基准。

Abstract: Unsupervised domain adaptation tackles the problem that domain shifts between training and test data impair the performance of neural networks in many real-world applications. Thereby, in realistic scenarios, the source data may no longer be available during adaptation, and the label space of the target domain may differ from the source label space. This setting, known as source-free universal domain adaptation (SF-UniDA), has recently gained attention, but all existing approaches only assume a single domain shift from source to target. In this work, we present the first study on continual SF-UniDA, where the model must adapt sequentially to a stream of multiple different unlabeled target domains. Building upon our previous methods for online SF-UniDA, we combine their key ideas by integrating Gaussian mixture model-based pseudo-labeling within a mean teacher framework for improved stability over long adaptation sequences. Additionally, we introduce consistency losses for further robustness. The resulting method GMM-COMET provides a strong first baseline for continual SF-UniDA and is the only approach in our experiments to consistently improve upon the source-only model across all evaluated scenarios. Our code is available at https://github.com/pascalschlachter/GMM-COMET.

</details>


### [34] [FEATHer: Fourier-Efficient Adaptive Temporal Hierarchy Forecaster for Time-Series Forecasting](https://arxiv.org/abs/2601.11350)
*Jaehoon Lee,Seungwoo Lee,Younghwi Kim,Dohee Kim,Sunghyun Sim*

Main category: cs.LG

TL;DR: FEATHer是一种用于边缘设备时间序列预测的轻量级模型，仅需400个参数，在严格资源限制下实现准确长期预测


<details>
  <summary>Details</summary>
Motivation: 工业领域（如制造业和智能工厂）需要时间序列预测模型能在边缘设备（PLC、微控制器）上运行，这些设备有严格的延迟和内存限制，通常只能容纳几千个参数，传统深度架构在此不实用

Method: FEATHer采用：1）超轻量级多尺度频率路径分解；2）共享密集时间核（投影-深度卷积-投影结构，无循环或注意力）；3）基于频谱特征的频率感知分支门控；4）稀疏周期核通过周期下采样重建输出以捕捉季节性

Result: 在8个基准测试中取得最佳排名，获得60个第一名结果，平均排名2.05，在紧凑架构（最少400个参数）下超越基线模型

Conclusion: 可靠的长期预测在受限边缘硬件上是可行的，为工业实时推理提供了实用方向

Abstract: Time-series forecasting is fundamental in industrial domains like manufacturing and smart factories. As systems evolve toward automation, models must operate on edge devices (e.g., PLCs, microcontrollers) with strict constraints on latency and memory, limiting parameters to a few thousand. Conventional deep architectures are often impractical here. We propose the Fourier-Efficient Adaptive Temporal Hierarchy Forecaster (FEATHer) for accurate long-term forecasting under severe limits. FEATHer introduces: (i) ultra-lightweight multiscale decomposition into frequency pathways; (ii) a shared Dense Temporal Kernel using projection-depthwise convolution-projection without recurrence or attention; (iii) frequency-aware branch gating that adaptively fuses representations based on spectral characteristics; and (iv) a Sparse Period Kernel reconstructing outputs via period-wise downsampling to capture seasonality. FEATHer maintains a compact architecture (as few as 400 parameters) while outperforming baselines. Across eight benchmarks, it achieves the best ranking, recording 60 first-place results with an average rank of 2.05. These results demonstrate that reliable long-range forecasting is achievable on constrained edge hardware, offering a practical direction for industrial real-time inference.

</details>


### [35] [TimeMar: Multi-Scale Autoregressive Modeling for Unconditional Time Series Generation](https://arxiv.org/abs/2601.11184)
*Xiangyu Xu,Qingsong Zhong,Jilin Hu*

Main category: cs.LG

TL;DR: 提出了一种结构解耦的多尺度时间序列生成框架，通过双路径VQ-VAE分离趋势和季节性成分，采用从粗到细的自回归生成方式，在减少参数量的同时生成高质量长序列。


<details>
  <summary>Details</summary>
Motivation: 时间序列分析面临数据稀缺和隐私挑战，而现有方法未能充分处理时间序列的多尺度时间模式和异构结构复杂性。

Method: 1) 在多时间分辨率下将序列编码为离散标记，进行从粗到细的自回归生成；2) 引入双路径VQ-VAE解耦趋势和季节性成分；3) 提出基于引导的重建策略，利用粗粒度季节性信号作为先验指导细粒度季节性模式重建。

Result: 在六个数据集上的实验表明，该方法比现有方法生成更高质量的时间序列，在显著减少参数数量的情况下实现强性能，并展现出生成高质量长序列的优越能力。

Conclusion: 该结构解耦的多尺度生成框架有效解决了时间序列的结构复杂性，为数据稀缺和隐私挑战提供了有前景的解决方案。

Abstract: Generative modeling offers a promising solution to data scarcity and privacy challenges in time series analysis. However, the structural complexity of time series, characterized by multi-scale temporal patterns and heterogeneous components, remains insufficiently addressed. In this work, we propose a structure-disentangled multiscale generation framework for time series. Our approach encodes sequences into discrete tokens at multiple temporal resolutions and performs autoregressive generation in a coarse-to-fine manner, thereby preserving hierarchical dependencies. To tackle structural heterogeneity, we introduce a dual-path VQ-VAE that disentangles trend and seasonal components, enabling the learning of semantically consistent latent representations. Additionally, we present a guidance-based reconstruction strategy, where coarse seasonal signals are utilized as priors to guide the reconstruction of fine-grained seasonal patterns. Experiments on six datasets show that our approach produces higher-quality time series than existing methods. Notably, our model achieves strong performance with a significantly reduced parameter count and exhibits superior capability in generating high-quality long-term sequences. Our implementation is available at https://anonymous.4open.science/r/TimeMAR-BC5B.

</details>


### [36] [MetaboNet: The Largest Publicly Available Consolidated Dataset for Type 1 Diabetes Management](https://arxiv.org/abs/2601.11505)
*Miriam K. Wolff,Peter Calhoun,Eleonora Maria Aiello,Yao Qin,Sam F. Royston*

Main category: cs.LG

TL;DR: 该研究整合了多个公开的1型糖尿病数据集，创建了名为MetaboNet的统一数据资源，包含3135名患者和1228患者年的CGM与胰岛素数据，旨在解决现有数据集碎片化和标准化不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前1型糖尿病算法开发受到现有数据集碎片化和缺乏标准化的限制。不同数据集结构差异大，访问和处理耗时，阻碍了数据整合，降低了算法开发的可比性和泛化性。

Method: 整合多个公开的1型糖尿病数据集，要求同时包含连续血糖监测数据和胰岛素泵剂量记录。保留碳水化合物摄入和体力活动等辅助信息。数据集分为完全公开子集和需数据使用协议限制的子集，后者提供处理管道自动转换为标准化格式。

Result: 创建了MetaboNet数据集，包含3135名受试者和1228患者年的重叠CGM和胰岛素数据，规模远超现有独立基准数据集。数据集涵盖广泛的血糖谱和人口统计学特征，可通过https://metabo-net.org/立即下载公开部分。

Conclusion: 提出了一个统一的1型糖尿病研究公共数据集，描述了其无限制和受数据使用协议管理组件的访问途径。该数据集能够产生比单个数据集更具泛化性的算法性能。

Abstract: Progress in Type 1 Diabetes (T1D) algorithm development is limited by the fragmentation and lack of standardization across existing T1D management datasets. Current datasets differ substantially in structure and are time-consuming to access and process, which impedes data integration and reduces the comparability and generalizability of algorithmic developments. This work aims to establish a unified and accessible data resource for T1D algorithm development. Multiple publicly available T1D datasets were consolidated into a unified resource, termed the MetaboNet dataset. Inclusion required the availability of both continuous glucose monitoring (CGM) data and corresponding insulin pump dosing records. Additionally, auxiliary information such as reported carbohydrate intake and physical activity was retained when present. The MetaboNet dataset comprises 3135 subjects and 1228 patient-years of overlapping CGM and insulin data, making it substantially larger than existing standalone benchmark datasets. The resource is distributed as a fully public subset available for immediate download at https://metabo-net.org/ , and with a Data Use Agreement (DUA)-restricted subset accessible through their respective application processes. For the datasets in the latter subset, processing pipelines are provided to automatically convert the data into the standardized MetaboNet format. A consolidated public dataset for T1D research is presented, and the access pathways for both its unrestricted and DUA-governed components are described. The resulting dataset covers a broad range of glycemic profiles and demographics and thus can yield more generalizable algorithmic performance than individual datasets.

</details>


### [37] [Operator learning on domain boundary through combining fundamental solution-based artificial data and boundary integral techniques](https://arxiv.org/abs/2601.11222)
*Haochen Wu,Heng Wu,Benzhuo Lu*

Main category: cs.LG

TL;DR: 提出MAD-BNO框架，仅使用边界数据学习偏微分方程算子，通过数学人工数据生成训练样本，无需全区域采样或数值模拟


<details>
  <summary>Details</summary>
Motivation: 传统算子学习方法需要全区域采样数据，计算成本高；现有方法依赖外部测量或数值模拟；需要开发仅使用边界数据的高效算子学习框架

Method: 结合数学人工数据方法，从基本解直接合成边界数据对；学习边界到边界的映射；训练后通过边界积分公式恢复内部解；支持多种边界条件和源项

Result: 在二维Laplace、Poisson和Helmholtz方程的基准测试中，精度达到或优于现有神经算子方法，同时显著减少训练时间

Conclusion: MAD-BNO框架为已知基本解的线性偏微分方程提供了高效的数据驱动算子学习方法，可扩展到三维问题和复杂几何

Abstract: For linear partial differential equations with known fundamental solutions, this work introduces a novel operator learning framework that relies exclusively on domain boundary data, including solution values and normal derivatives, rather than full-domain sampling. By integrating the previously developed Mathematical Artificial Data (MAD) method, which enforces physical consistency, all training data are synthesized directly from the fundamental solutions of the target problems, resulting in a fully data-driven pipeline without the need for external measurements or numerical simulations. We refer to this approach as the Mathematical Artificial Data Boundary Neural Operator (MAD-BNO), which learns boundary-to-boundary mappings using MAD-generated Dirichlet-Neumann data pairs. Once trained, the interior solution at arbitrary locations can be efficiently recovered through boundary integral formulations, supporting Dirichlet, Neumann, and mixed boundary conditions as well as general source terms. The proposed method is validated on benchmark operator learning tasks for two-dimensional Laplace, Poisson, and Helmholtz equations, where it achieves accuracy comparable to or better than existing neural operator approaches while significantly reducing training time. The framework is naturally extensible to three-dimensional problems and complex geometries.

</details>


### [38] [Sample-Near-Optimal Agnostic Boosting with Improved Running Time](https://arxiv.org/abs/2601.11265)
*Arthur da Cunha,Miakel Møller Høgsgaard,Andrea Paudice*

Main category: cs.LG

TL;DR: 提出了第一个具有接近最优样本复杂度的不可知提升算法，在固定其他参数时运行时间为多项式时间


<details>
  <summary>Details</summary>
Motivation: 提升方法能将弱学习器转化为高精度的强学习器，但在不可知设置（不对数据做任何假设）下理解较少。现有算法虽然解决了样本复杂度问题，但运行时间是指数级的

Method: 提出了一种新的不可知提升算法，在固定其他参数的情况下，运行时间相对于样本大小是多项式的

Result: 该算法具有接近最优的样本复杂度，是第一个在多项式时间内实现这一目标的不可知提升算法

Conclusion: 这项工作填补了不可知提升领域的重要空白，提供了既具有理论保证又实用的算法

Abstract: Boosting is a powerful method that turns weak learners, which perform only slightly better than random guessing, into strong learners with high accuracy. While boosting is well understood in the classic setting, it is less so in the agnostic case, where no assumptions are made about the data. Indeed, only recently was the sample complexity of agnostic boosting nearly settled arXiv:2503.09384, but the known algorithm achieving this bound has exponential running time. In this work, we propose the first agnostic boosting algorithm with near-optimal sample complexity, running in time polynomial in the sample size when considering the other parameters of the problem fixed.

</details>


### [39] [Metabolomic Biomarker Discovery for ADHD Diagnosis Using Interpretable Machine Learning](https://arxiv.org/abs/2601.11283)
*Nabil Belacel,Mohamed Rachid Boulassel*

Main category: cs.LG

TL;DR: 研究开发了一种结合尿液代谢组学和可解释机器学习的方法，用于ADHD的客观诊断，通过14种代谢物的特征面板实现了高准确率（AUC>0.97）


<details>
  <summary>Details</summary>
Motivation: ADHD是一种常见的神经发育障碍，目前缺乏客观的诊断工具，迫切需要基于生物学的客观诊断框架来推进精准精神病学

Method: 整合尿液代谢组学与可解释机器学习框架，使用Closest Resemblance分类器结合特征选择，分析52名ADHD患者和46名对照者的靶向代谢组学数据

Result: CR模型优于随机森林和K最近邻分类器，基于14种代谢物的简化面板实现了AUC>0.97的高准确率，这些代谢物涉及多巴胺能神经传递和氨基酸代谢通路

Conclusion: 该研究展示了一个结合代谢组学和可解释机器学习的转化框架，为ADHD提供了客观、生物学信息化的诊断策略，具有透明决策边界和低计算成本的优势

Abstract: Attention Deficit Hyperactivity Disorder (ADHD) is a prevalent neurodevelopmental disorder with limited objective diagnostic tools, highlighting the urgent need for objective, biology-based diagnostic frameworks in precision psychiatry. We integrate urinary metabolomics with an interpretable machine learning framework to identify biochemical signatures associated with ADHD. Targeted metabolomic profiles from 52 ADHD and 46 control participants were analyzed using a Closest Resemblance (CR) classifier with embedded feature selection. The CR model outperformed Random Forest and K-Nearest Neighbor classifiers, achieving an AUC > 0.97 based on a reduced panel of 14 metabolites. These metabolites including dopamine 4-sulfate, N-acetylaspartylglutamic acid, and citrulline map to dopaminergic neurotransmission and amino acid metabolism pathways, offering mechanistic insight into ADHD pathophysiology. The CR classifier's transparent decision boundaries and low computational cost support integration into targeted metabolomic assays and future point of care diagnostic platforms. Overall, this work demonstrates a translational framework combining metabolomics and interpretable machine learning to advance objective, biologically informed diagnostic strategies for ADHD.

</details>


### [40] [FORESTLLM: Large Language Models Make Random Forest Great on Few-shot Tabular Learning](https://arxiv.org/abs/2601.11311)
*Zhihan Yang,Jiaqi Wei,Xiang Zhang,Haoyu Dong,Yiwen Wang,Xiaoke Guo,Pengkun Zhang,Yiwei Xu,Chenyu You*

Main category: cs.LG

TL;DR: FORESTLLM结合决策树的结构归纳偏置与大语言模型的语义推理能力，在少样本表格数据学习中实现最先进性能，训练时使用LLM设计轻量级可解释森林模型，测试时无需LLM推理。


<details>
  <summary>Details</summary>
Motivation: 表格数据在金融、医疗和科学发现等领域用于关键决策，但在少样本场景下学习面临挑战。传统树方法依赖统计纯度指标，在有限监督下不稳定且易过拟合；直接应用大语言模型常忽略数据结构导致性能不佳。

Method: 提出FORESTLLM框架：1) 语义分割准则：LLM基于标记和未标记数据的连贯性评估候选分割，在少样本监督下诱导更鲁棒和可泛化的树结构；2) 一次性上下文推理机制：LLM将决策路径和支持样本提炼为简洁确定性预测，用语义信息输出替代噪声经验估计。

Result: 在多样化的少样本分类和回归基准测试中，FORESTLLM实现了最先进的性能。

Conclusion: FORESTLLM成功统一了决策森林的结构归纳偏置与LLMs的语义推理能力，在少样本表格数据学习中表现出色，同时保持了模型的轻量级和可解释性。

Abstract: Tabular data high-stakes critical decision-making in domains such as finance, healthcare, and scientific discovery. Yet, learning effectively from tabular data in few-shot settings, where labeled examples are scarce, remains a fundamental challenge. Traditional tree-based methods often falter in these regimes due to their reliance on statistical purity metrics, which become unstable and prone to overfitting with limited supervision. At the same time, direct applications of large language models (LLMs) often overlook its inherent structure, leading to suboptimal performance. To overcome these limitations, we propose FORESTLLM, a novel framework that unifies the structural inductive biases of decision forests with the semantic reasoning capabilities of LLMs. Crucially, FORESTLLM leverages the LLM only during training, treating it as an offline model designer that encodes rich, contextual knowledge into a lightweight, interpretable forest model, eliminating the need for LLM inference at test time. Our method is two-fold. First, we introduce a semantic splitting criterion in which the LLM evaluates candidate partitions based on their coherence over both labeled and unlabeled data, enabling the induction of more robust and generalizable tree structures under few-shot supervision. Second, we propose a one-time in-context inference mechanism for leaf node stabilization, where the LLM distills the decision path and its supporting examples into a concise, deterministic prediction, replacing noisy empirical estimates with semantically informed outputs. Across a diverse suite of few-shot classification and regression benchmarks, FORESTLLM achieves state-of-the-art performance.

</details>


### [41] [Unlocking the Potentials of Retrieval-Augmented Generation for Diffusion Language Models](https://arxiv.org/abs/2601.11342)
*Chuanyue Yu,Jiahui Wang,Yuhan Li,Heng Chang,Ge Lan,Qingyun Sun,Jia Li,Jianxin Li,Ziwei Zhang*

Main category: cs.LG

TL;DR: 本文提出SPREAD框架，通过查询相关性引导的去噪策略解决扩散语言模型在检索增强生成中的语义漂移问题，显著提高了生成精度。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型在自然语言处理任务中表现出色，但检索增强生成在增强大语言模型方面的成功潜力尚未在DLMs中得到充分探索，主要因为LLM和DLM解码机制存在根本差异。研究发现DLMs结合RAG虽然对上下文信息有更强依赖性，但存在生成精度有限的问题，核心原因是响应语义漂移。

Method: 提出SPREAD（Semantic-Preserving REtrieval-Augmented Diffusion）框架，引入查询相关性引导的去噪策略，主动引导去噪轨迹，确保生成内容始终锚定在查询语义上，有效抑制语义漂移。

Result: 实验结果表明，SPREAD显著提高了RAG框架内生成答案的精度，有效缓解了响应语义漂移问题。

Conclusion: SPREAD框架成功解决了扩散语言模型在检索增强生成中的语义漂移问题，通过查询引导的去噪策略保持了语义一致性，为DLMs在RAG中的应用提供了有效解决方案。

Abstract: Diffusion Language Models (DLMs) have recently demonstrated remarkable capabilities in natural language processing tasks. However, the potential of Retrieval-Augmented Generation (RAG), which shows great successes for enhancing large language models (LLMs), has not been well explored, due to the fundamental difference between LLM and DLM decoding. To fill this critical gap, we systematically test the performance of DLMs within the RAG framework. Our findings reveal that DLMs coupled with RAG show promising potentials with stronger dependency on contextual information, but suffer from limited generation precision. We identify a key underlying issue: Response Semantic Drift (RSD), where the generated answer progressively deviates from the query's original semantics, leading to low precision content. We trace this problem to the denoising strategies in DLMs, which fail to maintain semantic alignment with the query throughout the iterative denoising process. To address this, we propose Semantic-Preserving REtrieval-Augmented Diffusion (SPREAD), a novel framework that introduces a query-relevance-guided denoising strategy. By actively guiding the denoising trajectory, SPREAD ensures the generation remains anchored to the query's semantics and effectively suppresses drift. Experimental results demonstrate that SPREAD significantly enhances the precision and effectively mitigates RSD of generated answers within the RAG framework.

</details>


### [42] [Latent Space Inference via Paired Autoencoders](https://arxiv.org/abs/2601.11397)
*Emma Hart,Bas Peters,Julianne Chung,Matthias Chung*

Main category: cs.LG

TL;DR: 提出了一种基于配对自编码器的数据驱动潜在空间推理框架，用于处理反问题中的观测不一致性。该方法通过两个自编码器分别处理参数空间和观测空间，并在潜在空间之间学习映射关系，实现正则化反演和优化。


<details>
  <summary>Details</summary>
Motivation: 解决反问题中观测数据不一致性的挑战，包括部分数据、噪声数据或分布外数据的情况，同时保持与底层物理模型的一致性。

Method: 使用两个自编码器分别处理参数空间和观测空间，在潜在空间之间学习映射关系。通过这种配对自编码器框架，能够重建损坏的数据，然后使用重建数据进行参数估计。

Result: 与单独的配对自编码器和相同架构的端到端编码器-解码器相比，该方法在数据不一致的场景下能够产生更准确的重建结果。在医学断层扫描和地球物理地震波形反演两个成像示例中验证了方法的有效性。

Conclusion: 该框架能够灵活处理各种数据不一致性问题，同时保持物理一致性，适用于科学和工程应用中的多种反问题。

Abstract: This work describes a novel data-driven latent space inference framework built on paired autoencoders to handle observational inconsistencies when solving inverse problems. Our approach uses two autoencoders, one for the parameter space and one for the observation space, connected by learned mappings between the autoencoders' latent spaces. These mappings enable a surrogate for regularized inversion and optimization in low-dimensional, informative latent spaces. Our flexible framework can work with partial, noisy, or out-of-distribution data, all while maintaining consistency with the underlying physical models. The paired autoencoders enable reconstruction of corrupted data, and then use the reconstructed data for parameter estimation, which produces more accurate reconstructions compared to paired autoencoders alone and end-to-end encoder-decoders of the same architecture, especially in scenarios with data inconsistencies. We demonstrate our approaches on two imaging examples in medical tomography and geophysical seismic-waveform inversion, but the described approaches are broadly applicable to a variety of inverse problems in scientific and engineering applications.

</details>


### [43] [Forcing and Diagnosing Failure Modes of Fourier Neural Operators Across Diverse PDE Families](https://arxiv.org/abs/2601.11428)
*Lennon Shikhman*

Main category: cs.LG

TL;DR: FNO在PDE求解中表现良好，但在分布偏移、长期推演和结构扰动下的鲁棒性不足。研究通过系统压力测试框架，在五类PDE上揭示了FNO的故障模式，发现参数或边界条件变化可使误差增加一个数量级，分辨率变化主要影响高频模式。


<details>
  <summary>Details</summary>
Motivation: 虽然傅里叶神经算子（FNOs）在学习偏微分方程（PDEs）的解映射方面表现出色，但其在分布偏移、长期推演和结构扰动下的鲁棒性仍然缺乏深入理解。需要系统评估FNOs在不同PDE家族中的故障模式，以揭示其潜在脆弱性。

Method: 提出了一个系统化的压力测试框架，在五类不同性质的PDE家族（色散、椭圆、多尺度流体、金融和混沌系统）上进行评估。设计了受控压力测试，包括参数偏移、边界或终端条件变化、分辨率外推与谱分析、迭代推演等，以暴露谱偏差、积分误差累积和边界条件过拟合等脆弱性。共训练了1,000个模型进行大规模评估。

Result: 大规模评估显示：参数或边界条件的分布偏移可使误差增加一个数量级以上；分辨率变化主要将误差集中在高频模式；输入扰动通常不会放大误差，但最坏情况（如局部泊松扰动）仍然具有挑战性。这些发现为算子学习的鲁棒性改进提供了比较性故障模式图谱和可操作见解。

Conclusion: 该研究通过系统压力测试揭示了FNOs在不同PDE家族中的鲁棒性限制，提供了故障模式的比较性图谱，为改进算子学习的鲁棒性提供了具体方向和见解。研究强调了在评估算子学习方法时，除了传统精度指标外，还需要考虑分布偏移和结构扰动下的性能表现。

Abstract: Fourier Neural Operators (FNOs) have shown strong performance in learning solution maps of partial differential equations (PDEs), but their robustness under distribution shifts, long-horizon rollouts, and structural perturbations remains poorly understood. We present a systematic stress-testing framework that probes failure modes of FNOs across five qualitatively different PDE families: dispersive, elliptic, multi-scale fluid, financial, and chaotic systems. Rather than optimizing in-distribution accuracy, we design controlled stress tests--including parameter shifts, boundary or terminal condition changes, resolution extrapolation with spectral analysis, and iterative rollouts--to expose vulnerabilities such as spectral bias, compounding integration errors, and overfitting to restricted boundary regimes. Our large-scale evaluation (1{,}000 trained models) reveals that distribution shifts in parameters or boundary conditions can inflate errors by more than an order of magnitude, while resolution changes primarily concentrate error in high-frequency modes. Input perturbations generally do not amplify error, though worst-case scenarios (e.g., localized Poisson perturbations) remain challenging. These findings provide a comparative failure-mode atlas and actionable insights for improving robustness in operator learning.

</details>


### [44] [When Are Two Scores Better Than One? Investigating Ensembles of Diffusion Models](https://arxiv.org/abs/2601.11444)
*Raphaël Razafindralambo,Rémy Sun,Frédéric Precioso,Damien Garreau,Pierre-Alexandre Mattei*

Main category: cs.LG

TL;DR: 扩散模型集成方法虽然能改善评分匹配损失和模型似然，但在图像数据集上并不能一致提升感知质量指标如FID


<details>
  <summary>Details</summary>
Motivation: 尽管集成方法在监督模型中已被证明有效，但在无条件评分扩散模型中的应用尚未充分探索，本文旨在研究集成是否能为生成建模带来实际好处

Method: 使用深度集成、蒙特卡洛Dropout等方法，在CIFAR-10和FFHQ数据集上测试多种聚合规则，同时通过随机森林研究表格数据，并提供评分模型求和的理论分析

Result: 集成评分通常能改善评分匹配损失和模型似然，但无法一致提升FID等感知质量指标；在表格数据中发现一种聚合策略优于其他方法

Conclusion: 扩散模型集成在理论指标上有益，但在实际感知质量提升上有限，理论分析为模型集成和组合技术提供了新的见解

Abstract: Diffusion models now generate high-quality, diverse samples, with an increasing focus on more powerful models. Although ensembling is a well-known way to improve supervised models, its application to unconditional score-based diffusion models remains largely unexplored. In this work we investigate whether it provides tangible benefits for generative modelling. We find that while ensembling the scores generally improves the score-matching loss and model likelihood, it fails to consistently enhance perceptual quality metrics such as FID on image datasets. We confirm this observation across a breadth of aggregation rules using Deep Ensembles, Monte Carlo Dropout, on CIFAR-10 and FFHQ. We attempt to explain this discrepancy by investigating possible explanations, such as the link between score estimation and image quality. We also look into tabular data through random forests, and find that one aggregation strategy outperforms the others. Finally, we provide theoretical insights into the summing of score models, which shed light not only on ensembling but also on several model composition techniques (e.g. guidance).

</details>


### [45] [Low-Rank Key Value Attention](https://arxiv.org/abs/2601.11471)
*James O'Neill,Robert Clancy,Mariia Matskevichus,Fergal Reid*

Main category: cs.LG

TL;DR: LRKV是一种通过低秩KV适配减少Transformer KV缓存内存占用的注意力机制，在保持完整token分辨率的同时利用注意力头间的冗余性，显著降低训练和推理的计算需求。


<details>
  <summary>Details</summary>
Motivation: Transformer预训练面临内存和计算资源限制，其中KV缓存成为训练和自回归解码的主要瓶颈。需要一种方法在减少KV缓存内存的同时保持模型性能。

Method: 提出低秩KV适配（LRKV），修改多头注意力机制，使用共享的全秩KV投影加上低秩、头特定的残差，实现KV缓存内存的连续权衡，完全兼容标准多头注意力。

Result: 在大规模预训练实验中，LRKV相比标准注意力、MQA/GQA和MLA，实现了更快的损失下降、更低的验证困惑度和更强的下游任务性能。在2.5B规模下，使用约一半的KV缓存就能超越标准注意力，达到相同模型质量可减少20-25%的训练计算量。

Conclusion: LRKV通过分析注意力头在算子空间的结构，证明其几乎保留了所有功能头多样性，而更激进的KV共享机制依赖查询专业化补偿。LRKV成为内存和计算受限环境下扩展Transformer预训练的实用有效注意力机制。

Abstract: Transformer pretraining is increasingly constrained by memory and compute requirements, with the key-value (KV) cache emerging as a dominant bottleneck during training and autoregressive decoding. We propose \textit{low-rank KV adaptation} (LRKV), a simple modification of multi-head attention that reduces KV cache memory by exploiting redundancy across attention heads while preserving full token-level resolution. Each layer uses a shared full-rank KV projection augmented with low-rank, head-specific residuals, yielding a continuous trade-off between complete sharing and fully independent attention.
  LRKV is a drop-in replacement for standard multi-head attention and directly subsumes query-sharing approaches such as multi-query and grouped-query attention, while remaining distinct from latent-compression methods such as multi-latent attention (MLA). Across large-scale pretraining experiments, LRKV consistently achieves faster loss reduction, lower validation perplexity, and stronger downstream task performance than standard attention, MQA/GQA, and MLA. At the 2.5B scale, LRKV outperforms standard attention while using roughly half the KV cache, and reaches equivalent model quality with up to \textbf{20-25\% less training compute} when measured in cumulative FLOPs. To explain these gains, we analyze attention head structure in operator space and show that LRKV preserves nearly all functional head diversity relative to standard attention, whereas more aggressive KV-sharing mechanisms rely on compensatory query specialization. Together, these results establish LRKV as a practical and effective attention mechanism for scaling Transformer pretraining under memory- and compute-constrained regimes.

</details>


### [46] [Extractive summarization on a CMOS Ising machine](https://arxiv.org/abs/2601.11491)
*Ziqing Zeng,Abhimanyu Kumar,Chris H. Kim,Ulya R. Karpuzcu,Sachin S. Sapatnekar*

Main category: cs.LG

TL;DR: 本文探索在低功耗CMOS耦合振荡器伊辛机上实现McDonald式抽取式摘要，通过硬件感知的伊辛公式化、随机舍入和分解策略，在CNN/DailyMail数据集上实现3-4.5倍加速和2-3个数量级的能耗降低。


<details>
  <summary>Details</summary>
Motivation: 现代抽取式摘要系统虽然准确率高，但依赖CPU/GPU基础设施，能耗高且不适合资源受限环境的实时推理。本文旨在探索在低功耗CMOS耦合振荡器伊辛机上实现抽取式摘要的可行性。

Method: 1) 提出硬件感知的伊辛公式化，减少局部场和耦合项之间的尺度不平衡，提高系数量化的鲁棒性；2) 开发完整的抽取式摘要流程，包括随机舍入和迭代精炼来补偿精度损失；3) 采用分解策略将大型摘要问题划分为可在COBI上高效求解的较小伊辛子问题。

Result: 在CNN/DailyMail数据集上，该流程仅使用有限精度的整数耦合伊辛硬件就能生成高质量摘要。COBI相比暴力方法实现3-4.5倍运行时间加速，与软件Tabu搜索相当，同时能耗降低2-3个数量级，且保持有竞争力的摘要质量。

Conclusion: 这些结果突显了CMOS伊辛求解器在边缘设备上实现实时、低能耗文本摘要的潜力，为资源受限环境中的高效摘要系统提供了新途径。

Abstract: Extractive summarization (ES) aims to generate a concise summary by selecting a subset of sentences from a document while maximizing relevance and minimizing redundancy. Although modern ES systems achieve high accuracy using powerful neural models, their deployment typically relies on CPU or GPU infrastructures that are energy-intensive and poorly suited for real-time inference in resource-constrained environments. In this work, we explore the feasibility of implementing McDonald-style extractive summarization on a low-power CMOS coupled oscillator-based Ising machine (COBI) that supports integer-valued, all-to-all spin couplings. We first propose a hardware-aware Ising formulation that reduces the scale imbalance between local fields and coupling terms, thereby improving robustness to coefficient quantization: this method can be applied to any problem formulation that requires k of n variables to be chosen. We then develop a complete ES pipeline including (i) stochastic rounding and iterative refinement to compensate for precision loss, and (ii) a decomposition strategy that partitions a large ES problem into smaller Ising subproblems that can be efficiently solved on COBI and later combined. Experimental results on the CNN/DailyMail dataset show that our pipeline can produce high-quality summaries using only integer-coupled Ising hardware with limited precision. COBI achieves 3-4.5x runtime speedups compared to a brute-force method, which is comparable to software Tabu search, and two to three orders of magnitude reductions in energy, while maintaining competitive summary quality. These results highlight the potential of deploying CMOS Ising solvers for real-time, low-energy text summarization on edge devices.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [47] [In search of diabolical critical points](https://arxiv.org/abs/2601.10783)
*Naren Manjunath,Dominic V. Else*

Main category: cond-mat.str-el

TL;DR: 该论文研究了相图拓扑缺陷的高余维情形，引入了"魔鬼临界点"概念，作为连续相变的高余维类比


<details>
  <summary>Details</summary>
Motivation: 研究相图中高余维拓扑缺陷的结构和性质，探索平衡态在缺陷周围发生非平凡缠绕的现象

Method: 分析经典统计力学系统中的拓扑缺陷，提出"魔鬼临界点"的定义和稳定性条件，讨论(1+1)维量子系统中的具体例子

Result: 证明了即使在经典统计力学系统中也存在高余维拓扑缺陷，提出了DCP的稳定性条件，并展示了(1+1)维量子系统中的稳定DCP实例

Conclusion: 相图中的高余维拓扑缺陷具有重要的理论意义，魔鬼临界点作为连续相变的高余维类比，为理解多体系统的相变拓扑结构提供了新视角

Abstract: A phase transition is an example of a ``topological defect'' in the space of parameters of a quantum or classical many-body systems. In this paper, we consider phase diagram topological defects of higher codimension. These have the property that equilibrium states undergo some kind of non-trivial winding as one moves around the defect. We show that such topological defects exist even in classical statistical mechanical systems, and describe their general structure in this context. We then introduce the term ``diabolical critical point'' (DCP), which is a higher-codimension analog of a continuous phase transition, with the proximate phases of matter replaced by the non-trivial winding of the proximate equilibrium states. We propose conditions under which a system can have a stable DCP. We also discuss some examples of stable DCPs in (1+1)-dimensional quantum systems.

</details>


### [48] [Surface Functional Renormalization Group for Layered Quantum Materials](https://arxiv.org/abs/2601.11055)
*Lennart Klebl,Dante M. Kennes*

Main category: cond-mat.str-el

TL;DR: 该研究扩展了二维功能重整化群方法，用于处理三维系统表面或界面处的相互作用，并应用于半无限二维方晶格堆叠系统，研究了表面层Hubbard相互作用与交替层间耦合下强关联态的演化。


<details>
  <summary>Details</summary>
Motivation: 开发一种有效处理三维系统表面或界面相互作用的计算方法，研究表面层强关联态在层间耦合作用下的演化，探索新型量子态如手性自旋键序的实现可能性。

Method: 扩展二维功能重整化群方法，应用于半无限二维方晶格堆叠系统，包含表面层Hubbard相互作用和交替SSH型层间耦合，通过相图分析研究不同参数区域下的量子态演化。

Result: 在大部分相图区域，二维系统的物理特性占主导，包括反铁磁、d波超导和铁磁关联；但在中等层间耦合区域，中等相互作用强度的超导态被非公度自旋密度波和自旋键序小区域分隔，可能实现手性自旋键序。

Conclusion: 扩展的功能重整化群方法能有效处理表面/界面相互作用，表面层强关联态在层间耦合下展现出丰富的相图，中等耦合区域可能实现手性自旋键序等新型量子态，为界面量子材料研究提供了理论工具。

Abstract: We present an extension to the two-dimensional functional renormalization group to efficiently treat interactions on the surface or at interfaces of three-dimensional systems. As an application, we consider a semi-infinite stack of two-dimensional square lattices, including a Hubbard interaction on the surface layer and an alternating interlayer coupling. We investigate how strongly correlated states of the decoupled two-dimensional Hubbard model on the surface evolve under inclusion of such an SSH-like interlayer coupling. For large parts of the phase diagram as a function of the interlayer hopping parameters, the physics of the two-dimensional system prevails, with antiferromagnetic, superconducting d-wave, and ferromagnetic correlations taking center stage. However, for intermediate interlayer couplings the superconducting state at intermediate interaction strengths separates into two regimes by a small region of incommensurate spin-density-wave and spin-bond order, enabling the potential realization of chiral spin-bond order.

</details>


### [49] [Spontaneous Anomalous Hall Effect at Room Temperature in Antiferromagnetic Material NbMnAs](https://arxiv.org/abs/2601.11088)
*Yuki Arai,Junichi Hayashi,Keiki Takeda,Hideki Tou,Eiichi Matsuoka,Hitoshi Sugawara,Hisashi Kotegawa*

Main category: cond-mat.str-el

TL;DR: NbMnAs是一种新型反铁磁材料，在室温下表现出大的反常霍尔效应，尽管只有很小的净磁化强度。


<details>
  <summary>Details</summary>
Motivation: 研究具有与铁磁体相同对称性破缺的反铁磁材料，这些材料能够产生足够大的铁磁响应，探索在室温下具有显著铁磁响应的新型反铁磁材料。

Method: 制备了多晶和单晶NbMnAs样品，多晶样品接近化学计量比，单晶样品通过助熔剂法生长但存在As位点缺陷。测量了材料的磁化强度、反常霍尔效应和奈尔温度。

Result: 多晶NbMnAs在TN=354K以下表现出反铁磁态，自发磁化强度约为6×10⁻³μB/Mn，并在室温下显示出大的反常霍尔效应。单晶样品由于As缺陷导致TN降低和自发磁化强度增加。

Conclusion: NbMnAs是一种能够在室温下从反铁磁性衍生出显著铁磁响应的新型材料，尽管单晶生长仍需改进，但该研究为开发室温反铁磁电子器件提供了有前景的材料平台。

Abstract: Recent studies have shown that certain antiferromagnetic (AFM) materials with the same symmetry breaking as ferromagnets can generate sufficiently large ferromagnetic (FM) responses. Here, we report that the new AFM material NbMnAs exhibits a large anomalous Hall effect (AHE) at zero field and at room temperature, despite having only a small net magnetization. A polycrystalline sample of NbMnAs, likely close to stoichiometric composition, exhibited an AFM state with a small spontaneous magnetization of approximately $6 \times 10^{-3} μ_{\rm B}$/Mn and the AHE below $T_{\rm N}=354\,{\rm K}$. In contrast, single crystals of NbMnAs obtained by a flux method exhibited a deficiency at the As site, {which resulted} in a decrease in $T_{\rm N}$ and an increase in spontaneous magnetization. Although improvement of the single-crystal growth is still required, our study reveals that NbMnAs is a novel material capable of exhibiting significant FM responses derived from antiferromagnetism at room temperature.

</details>


### [50] [Three-dimensional topological insulator feature of ternary chalcogenide Ge2Bi2Te5](https://arxiv.org/abs/2601.11339)
*Shangjie Tian,Yuchong Zhang,Chenhao Liang,Yuqing Cao,Wenxin Lv,Xingyu Lv,Zhijun Wang,Tian Qian,Hechang Lei,Shouguo Wang*

Main category: cond-mat.str-el

TL;DR: Ge2Bi2Te5被鉴定为三维拓扑绝缘体，具有非平庸拓扑不变量，其表面态狄拉克点位于费米能级以上290meV处。


<details>
  <summary>Details</summary>
Motivation: 探索超越二元硫族化合物的新型拓扑绝缘体，以寻找奇异量子态和器件应用。

Method: 通过实验观测Ge2Bi2Te5的体电子结构和表面态，结合理论计算验证其拓扑性质。

Result: Ge2Bi2Te5具有三维拓扑绝缘体特征：体电子结构在费米能级处显示空穴型费米面，表面态狄拉克点位于费米能级以上290meV，理论计算确认其具有非平庸Z2拓扑不变量(000;1)。

Conclusion: 层状类辉铋矿三元化合物材料家族是探索奇异拓扑现象的重要平台。

Abstract: The exploration of novel topological insulators (TIs) beyond binary chalcogenides has been accelerated in pursuit of exotic quantum states and device applications. Here, the layered ternary chalcogenide Ge2Bi2Te5 is identified as a three-dimensional TI. The bulk electronic structure of Ge2Bi2Te5 features a hole-type Fermi surface at Fermi level EF, which dominates the transport properties. Moreover, an unoccupied topological surface state with a Dirac point located at 290 meV above EF has been observed. Theoretical calculations confirm a bulk bandgap and a nontrivial Z2 topological invariant (000;1). The present study demonstrates that the material family of layered tetradymite-like ternary compounds is an important platform to explore exotic topological phenomena.

</details>


### [51] [Visualization of Tunable Electronic Structure of Monolayer TaIrTe$_4$](https://arxiv.org/abs/2601.11504)
*Sandy Adhitia Ekahana,Aalok Tiwari,Souvik Sasmal,Zefeng Cai,Ravi Kumar Bandapelli,I-Hsuan Kao,Jian Tang,Chenbo Min,Tiema Qian,Kenji Watanabe,Takashi Taniguchi,Ni Ni,Qiong Ma,Chris Jozwiak,Eli Rotenberg,Aaron Bostwick,Simranjeet Singh,Noa Marom,Jyoti Katoch*

Main category: cond-mat.str-el

TL;DR: 该研究使用微米级分辨率的微角分辨光电子能谱直接测量了单层TaIrTe₄的能带结构，发现其绝缘基态与密度泛函理论计算一致，没有强电子关联证据，并揭示了电子-空穴掺杂的不对称响应。


<details>
  <summary>Details</summary>
Motivation: 单层TaIrTe₄是研究拓扑和强电子关联现象的重要平台，但尽管其能带结构对解释拓扑相至关重要，直接实验测量其本征电子结构一直未能实现。

Method: 使用空间分辨的微角分辨光电子能谱（microARPES）直接测量单层TaIrTe₄的能带结构，结合Heyd-Scuseria-Ernzerhof杂化泛函的密度泛函理论计算进行对比分析。

Result: 实验观测到的色散与理论计算定量一致，确认了绝缘基态且没有强电子关联的证据；发现了显著的电子-空穴不对称性：空穴掺杂容易通过静电门控实现，而电子掺杂则不会导致费米能级的刚性上移，而是驱动能带重整化并缩小带隙。

Conclusion: 掺杂可以根本上改变单层TaIrTe₄的电子结构，超越了通常假设的刚性带行为，揭示了诱导电荷重塑能带拓扑的微观机制。

Abstract: Monolayer TaIrTe$_4$ has emerged as an attractive material platform to study intriguing phenomena related to topology and strong electron correlations. Recently, strong interactions have been demonstrated to induce strain and dielectric screening tunable topological phases such as quantum spin Hall insulator (QSHI), trivial insulator, higher-order topological insulator, and metallic phase, in the ground state of monolayer TaIrTe$_4$. Moreover, charge dosing has been demonstrated to convert the QSHI into a dual QSHI state. Although the band structure of monolayer TaIrTe$_4$ is central to interpreting its topological phases in transport experiments, direct experimental access to its intrinsic electronic structure has so far remained elusive. Here we report direct measurements of the monolayer TaIrTe$_4$ band structure using spatially resolved micro-angle-resolved photoemission spectroscopy (microARPES) with micrometre-scale resolution. The observed dispersions show quantitative agreement with density functional theory calculations using the Heyd-Scuseria-Ernzerhof hybrid functional, establishing the insulating ground state and revealing no evidence for strong electronic correlations. We further uncover a pronounced electron-hole asymmetry in the doping response. Whereas hole doping is readily induced by electrostatic gating, attempts to introduce electrons via gating or alkali metal deposition do not yield a rigid upward shift of the Fermi level. Fractional charge calculations demonstrate that added electrons instead drive band renormalization and shrink the band gap. Taken together, our experimental and theoretical results identify the microscopic mechanism by which induced charges reshape the band topology of monolayer TaIrTe$_4$, showing that doping can fundamentally alter the electronic structure beyond the rigid band behaviour that is typically assumed.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [52] [Do You Trust Me? Cognitive-Affective Signatures of Trustworthiness in Large Language Models](https://arxiv.org/abs/2601.10719)
*Gerard Yeo,Svetlana Churina,Kokil Jaidka*

Main category: cs.AI

TL;DR: LLMs在预训练中隐式编码了心理可信度信号，无需显式监督就能区分高/低可信度文本，为可信AI系统设计提供了表征基础。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究大型语言模型是否以心理一致的方式表示在线信息的可信度，这对于嵌入搜索、推荐和对话系统的LLMs至关重要。

Method: 使用PEACE-Reviews数据集分析指令调优的LLMs（Llama 3.1 8B, Qwen 2.5 7B, Mistral 7B），通过层和头级别的激活差异分析，以及探测分析来研究可信度信号的编码方式。

Result: 研究发现：1）模型在预训练中隐式编码了可信度线索；2）可信度信号可线性解码；3）微调会精炼而非重构这些表征；4）最强关联出现在公平性、确定性和自我问责等人类信任形成的核心维度。

Conclusion: 现代LLMs无需显式监督就能内化心理基础的可信度信号，这为设计可信、透明和值得信赖的Web生态系统AI系统提供了表征基础。

Abstract: Perceived trustworthiness underpins how users navigate online information, yet it remains unclear whether large language models (LLMs),increasingly embedded in search, recommendation, and conversational systems, represent this construct in psychologically coherent ways. We analyze how instruction-tuned LLMs (Llama 3.1 8B, Qwen 2.5 7B, Mistral 7B) encode perceived trustworthiness in web-like narratives using the PEACE-Reviews dataset annotated for cognitive appraisals, emotions, and behavioral intentions. Across models, systematic layer- and head-level activation differences distinguish high- from low-trust texts, revealing that trust cues are implicitly encoded during pretraining. Probing analyses show linearly de-codable trust signals and fine-tuning effects that refine rather than restructure these representations. Strongest associations emerge with appraisals of fairness, certainty, and accountability-self -- dimensions central to human trust formation online. These findings demonstrate that modern LLMs internalize psychologically grounded trust signals without explicit supervision, offering a representational foundation for designing credible, transparent, and trust-worthy AI systems in the web ecosystem. Code and appendix are available at: https://github.com/GerardYeo/TrustworthinessLLM.

</details>


### [53] [CTHA: Constrained Temporal Hierarchical Architecture for Stable Multi-Agent LLM Systems](https://arxiv.org/abs/2601.10738)
*Percy Jardine*

Main category: cs.AI

TL;DR: CTHA是一个约束时间分层架构，通过结构化流形投影和仲裁机制解决多时间尺度智能体架构中的协调稳定性问题，显著减少失败级联并提升样本效率。


<details>
  <summary>Details</summary>
Motivation: 多时间尺度智能体架构虽然提升了性能，但破坏了统一智能体系统的协调稳定性，导致层间冲突、误差传播无界和可扩展性受限等问题。

Method: 提出约束时间分层架构(CTHA)，通过三个关键约束：1)消息契约约束，通过类型化摘要、计划和策略包形式化层间信息流；2)权限流形约束，根据时间范围限制每层的决策空间；3)仲裁器解决约束，保证多层决策的无冲突组合。

Result: CTHA在复杂任务执行中有效，相比无约束分层基线减少了47%的失败级联，提升了2.3倍的样本效率，并展现出优越的可扩展性。

Conclusion: CTHA作为时间分层架构的原则性扩展，有助于深入理解多智能体协调，并为稳健自主系统的演进指明了有前景的方向。

Abstract: Recently, multi-time-scale agent architectures have extended the ubiquitous single-loop paradigm by introducing temporal hierarchies with distinct cognitive layers. While yielding substantial performance gains, this diversification fundamentally compromises the coordination stability intrinsic to unified agent systems, which causes severe inter-layer conflicts, unbounded error propagation, and restricted scalability. To address these challenges, we propose Constrained Temporal Hierarchical Architecture (CTHA), a general framework that projects the inter-layer communication space onto structured manifolds to restore coordination stability, while incorporating principled arbitration mechanisms to ensure coherent decision-making. Specifically, CTHA enforces three key constraints: (1) Message Contract Constraints that formalize information flow between layers via typed summary, plan, and policy packets; (2) Authority Manifold Constraints that bound each layer's decision space according to its temporal scope; and (3) Arbiter Resolution Constraints that guarantee conflict-free composition of multi-layer decisions. Empirical experiments demonstrate that CTHA is effective for complex task execution at scale, offering 47% reduction in failure cascades, 2.3x improvement in sample efficiency, and superior scalability compared to unconstrained hierarchical baselines. We anticipate that CTHA, as a principled extension of temporal hierarchies, will contribute to a deeper understanding of multi-agent coordination and suggest promising directions for the evolution of robust autonomous systems.

</details>


### [54] [Optimisation of complex product innovation processes based on trend models with three-valued logic](https://arxiv.org/abs/2601.10768)
*Nina Bočková,Barbora Volná,Mirko Dohnal*

Main category: cs.AI

TL;DR: 该研究使用基于启发式的趋势模型分析复杂产品创新过程，通过简单的增加、减少或恒定趋势作为最小信息强度量化器，避免依赖数值或粗糙集，并通过转换图表示可能的系统行为路径。


<details>
  <summary>Details</summary>
Motivation: 研究复杂产品创新过程需要有效的分析工具，传统方法可能过于依赖数值数据或复杂模型，需要一种更简洁、信息强度最小化的量化方法来理解和预测创新系统的动态行为。

Method: 采用基于启发式的趋势模型，每个启发式通过简单的增加、减少或恒定趋势表达，作为最小信息强度的量化器。构建趋势模型的解决方案定义为包含可能转换的场景集合，用转换图表示，系统任何可能的未来或过去行为都可以表示为图中的路径。

Result: 开发了一种基于趋势的建模框架，能够以最小信息强度描述复杂产品创新过程，通过转换图可视化系统行为的可能路径，为理解和预测创新动态提供了结构化方法。

Conclusion: 基于启发式的趋势模型为分析复杂产品创新过程提供了有效的简化框架，通过最小信息强度的量化方法和转换图表示，能够捕捉系统行为的本质特征，为创新管理决策提供支持。

Abstract: This paper investigates complex product-innovation processes using models grounded in a set of heuristics. Each heuristic is expressed through simple trends -- increasing, decreasing, or constant -- which serve as minimally information-intensive quantifiers, avoiding reliance on numerical values or rough sets. A solution to a trend model is defined as a set of scenarios with possible transitions between them, represented by a transition graph. Any possible future or past behaviour of the system under study can thus be depicted by a path within this graph.

</details>


### [55] [ARC Prize 2025: Technical Report](https://arxiv.org/abs/2601.10904)
*François Chollet,Mike Knoop,Gregory Kamradt,Bryan Landers*

Main category: cs.AI

TL;DR: ARC-AGI-2竞赛显示AI推理能力仍受限于知识覆盖，精炼循环成为2025年关键方法，但前沿AI系统在抽象推理上仍有根本性限制。


<details>
  <summary>Details</summary>
Motivation: 分析ARC-AGI-2竞赛结果，探讨精炼循环在AGI进展中的作用，研究知识依赖性过拟合问题，为下一代ARC-AGI-3基准做准备。

Method: 通过调查ARC-AGI-2竞赛中表现最佳的方法，分析精炼循环（包括进化程序合成和商业AI系统应用层优化）的作用，并研究零预训练深度学习方法。

Result: 竞赛最高得分24%，精炼循环成为主导方法，前沿AI实验室开始将ARC-AGI作为行业标准基准，但当前AI推理性能仍受限于知识覆盖而非真正抽象推理。

Conclusion: ARC-AGI已成为AI推理的行业标准基准，但当前前沿AI系统在抽象推理上仍有根本性限制，需要开发ARC-AGI-3来评估交互式推理能力。

Abstract: The ARC-AGI benchmark series serves as a critical measure of few-shot generalization on novel tasks, a core aspect of intelligence. The ARC Prize 2025 global competition targeted the newly released ARC-AGI-2 dataset, which features greater task complexity compared to its predecessor. The Kaggle competition attracted 1,455 teams and 15,154 entries, with the top score reaching 24% on the ARC-AGI-2 private evaluation set. Paper submissions nearly doubled year-over-year to 90 entries, reflecting the growing research interest in fluid intelligence and abstract reasoning. The defining theme of 2025 is the emergence of the refinement loop -- a per-task iterative program optimization loop guided by a feedback signal. Refinement loops come in a variety of forms, in particular evolutionary program synthesis approaches and application-layer refinements to commercial AI systems. Such refinement loops are also possible in weight space, as evidenced by zero-pretraining deep learning methods which are now achieving competitive performance with remarkably small networks (7M parameters). In parallel, four frontier AI labs (Anthropic, Google DeepMind, OpenAI, and xAI) reported ARC-AGI performance in public model cards in 2025, establishing ARC-AGI as an industry standard benchmark for AI reasoning. However, our analysis indicates that current frontier AI reasoning performance remains fundamentally constrained to knowledge coverage, giving rise to new forms of benchmark contamination. In this paper, we survey the top-performing methods, examine the role of refinement loops in AGI progress, discuss knowledge-dependent overfitting, and preview ARC-AGI-3, which introduces interactive reasoning challenges that require exploration, planning, memory, goal acquisition, and alignment capabilities.

</details>


### [56] [What Matters in Data Curation for Multimodal Reasoning? Insights from the DCVLR Challenge](https://arxiv.org/abs/2601.10922)
*Yosub Shin,Michael Buriek,Boris Sobolev,Pavel Bushuyeu,Vikas Kumar,Haoyang Xu,Samuel Watson,Igor Molybog*

Main category: cs.AI

TL;DR: 该研究通过NeurIPS 2025 DCVLR挑战赛探索多模态推理的数据策展，发现基于难度的样本选择是性能提升的主要因素，数据集大小增加主要减少方差而非提升平均准确率。


<details>
  <summary>Details</summary>
Motivation: 研究多模态推理中的数据策展问题，通过固定模型和训练协议的挑战赛设置，专门研究数据集选择对性能的影响。

Method: 使用基于Walton多模态冷启动数据集的紧凑策展数据集，在DCVLR挑战赛中提交方案，并通过赛后消融实验分析不同数据策展策略的效果。

Result: 基于对齐基础数据集的难度样本选择是性能提升的主要驱动力；增加数据集大小主要减少运行间方差而非提升平均准确率；常用的多样性和合成增强启发式方法无额外益处甚至降低性能。

Conclusion: DCVLR是一个饱和状态评估，对齐性和难度在多模态推理的数据效率中起核心作用。

Abstract: We study data curation for multimodal reasoning through the NeurIPS 2025 Data Curation for Vision-Language Reasoning (DCVLR) challenge, which isolates dataset selection by fixing the model and training protocol. Using a compact curated dataset derived primarily from Walton Multimodal Cold Start, our submission placed first in the challenge. Through post-competition ablations, we show that difficulty-based example selection on an aligned base dataset is the dominant driver of performance gains. Increasing dataset size does not reliably improve mean accuracy under the fixed training recipe, but mainly reduces run-to-run variance, while commonly used diversity and synthetic augmentation heuristics provide no additional benefit and often degrade performance. These results characterize DCVLR as a saturation-regime evaluation and highlight the central role of alignment and difficulty in data-efficient multimodal reasoning.

</details>


### [57] [ReCreate: Reasoning and Creating Domain Agents Driven by Experience](https://arxiv.org/abs/2601.11100)
*Zhezheng Hao,Hong Wang,Jian Luo,Jianqing Zhang,Yuyan Zhou,Qiang Lin,Can Wang,Hande Dong,Jiawei Chen*

Main category: cs.AI

TL;DR: ReCreate是一个基于经验的自动领域智能体创建框架，通过分析智能体交互历史来优化智能体设计，相比传统黑盒方法能更高效地创建和适应领域智能体。


<details>
  <summary>Details</summary>
Motivation: 当前大多数实际应用的智能体仍需人工设计，因为不同任务差异大，构建成本高。现有自动化方法将智能体生成视为黑盒过程，仅依赖最终性能指标，忽略了成功/失败的关键证据，且计算成本高。

Method: 提出ReCreate框架，采用智能体即优化器范式，包含三个核心组件：1) 经验存储与检索机制用于按需检查；2) 推理-创建协同管道将执行经验映射到脚手架编辑；3) 层次化更新将实例级细节抽象为可重用领域模式。

Result: 在多个不同领域的实验中，ReCreate始终优于人工设计的智能体和现有自动化智能体生成方法，即使从最小种子脚手架开始也能取得良好效果。

Conclusion: ReCreate通过系统利用智能体交互历史中的具体信号，能够更有效地自动创建和适应领域智能体，解决了现有方法的局限性，为智能体自动化创建提供了新范式。

Abstract: Large Language Model agents are reshaping the industrial landscape. However, most practical agents remain human-designed because tasks differ widely, making them labor-intensive to build. This situation poses a central question: can we automatically create and adapt domain agents in the wild? While several recent approaches have sought to automate agent creation, they typically treat agent generation as a black-box procedure and rely solely on final performance metrics to guide the process. Such strategies overlook critical evidence explaining why an agent succeeds or fails, and often require high computational costs. To address these limitations, we propose ReCreate, an experience-driven framework for the automatic creation of domain agents. ReCreate systematically leverages agent interaction histories, which provide rich concrete signals on both the causes of success or failure and the avenues for improvement. Specifically, we introduce an agent-as-optimizer paradigm that effectively learns from experience via three key components: (i) an experience storage and retrieval mechanism for on-demand inspection; (ii) a reasoning-creating synergy pipeline that maps execution experience into scaffold edits; and (iii) hierarchical updates that abstract instance-level details into reusable domain patterns. In experiments across diverse domains, ReCreate consistently outperforms human-designed agents and existing automated agent generation methods, even when starting from minimal seed scaffolds.

</details>


### [58] [Do We Always Need Query-Level Workflows? Rethinking Agentic Workflow Generation for Multi-Agent Systems](https://arxiv.org/abs/2601.11147)
*Zixu Wang,Bingbing Xu,Yige Yuan,Huawei Shen,Xueqi Cheng*

Main category: cs.AI

TL;DR: SCALE框架通过任务级工作流生成和自预测评估，在保持性能的同时大幅降低token消耗


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统工作流生成方法在任务级和查询级各有优劣，但成本效益不明确，且基于执行的评估token成本高且不可靠

Method: 提出SCALE框架：通过少量样本校准的自预测评估替代完整验证执行，实现低成本的任务级工作流生成

Result: SCALE在多个数据集上平均性能仅下降0.61%，同时将总体token使用量减少高达83%

Conclusion: 查询级工作流生成并非总是必要，任务级方法配合高效评估机制可以在保持性能的同时显著降低成本

Abstract: Multi-Agent Systems (MAS) built on large language models typically solve complex tasks by coordinating multiple agents through workflows. Existing approaches generates workflows either at task level or query level, but their relative costs and benefits remain unclear. After rethinking and empirical analyses, we show that query-level workflow generation is not always necessary, since a small set of top-K best task-level workflows together already covers equivalent or even more queries. We further find that exhaustive execution-based task-level evaluation is both extremely token-costly and frequently unreliable. Inspired by the idea of self-evolution and generative reward modeling, we propose a low-cost task-level generation framework \textbf{SCALE}, which means \underline{\textbf{S}}elf prediction of the optimizer with few shot \underline{\textbf{CAL}}ibration for \underline{\textbf{E}}valuation instead of full validation execution. Extensive experiments demonstrate that \textbf{SCALE} maintains competitive performance, with an average degradation of just 0.61\% compared to existing approach across multiple datasets, while cutting overall token usage by up to 83\%.

</details>


### [59] [TANDEM: Temporal-Aware Neural Detection for Multimodal Hate Speech](https://arxiv.org/abs/2601.11178)
*Girish A. Koushik,Helen Treharne,Diptesh Kanojia*

Main category: cs.AI

TL;DR: TANDEM是一个统一框架，将音视频仇恨检测从二元分类任务转变为结构化推理问题，通过跨模态上下文优化实现精确的时间戳和目标识别，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前社交媒体长格式多模态内容中，有害叙事通过音频、视觉和文本线索的复杂交互构建。虽然自动化系统能高精度标记仇恨言论，但通常作为"黑箱"运行，无法提供人类参与审核所需的可解释证据，如精确时间戳和目标身份。

Method: 引入TANDEM框架，采用新颖的串联强化学习策略，让视觉-语言和音频-语言模型通过自约束跨模态上下文相互优化，稳定处理长时间序列推理，无需密集帧级监督。

Result: 在三个基准数据集上的实验显示，TANDEM显著优于零样本和上下文增强基线，在HateMM数据集上目标识别F1达到0.73（比最先进方法提升30%），同时保持精确的时间定位。二元检测稳健，但在多类别设置中区分冒犯性和仇恨内容仍具挑战性。

Conclusion: 研究表明，即使在复杂多模态环境中，结构化、可解释的对齐也是可实现的，为下一代透明且可操作的在线安全审核工具提供了蓝图。

Abstract: Social media platforms are increasingly dominated by long-form multimodal content, where harmful narratives are constructed through a complex interplay of audio, visual, and textual cues. While automated systems can flag hate speech with high accuracy, they often function as "black boxes" that fail to provide the granular, interpretable evidence, such as precise timestamps and target identities, required for effective human-in-the-loop moderation. In this work, we introduce TANDEM, a unified framework that transforms audio-visual hate detection from a binary classification task into a structured reasoning problem. Our approach employs a novel tandem reinforcement learning strategy where vision-language and audio-language models optimize each other through self-constrained cross-modal context, stabilizing reasoning over extended temporal sequences without requiring dense frame-level supervision. Experiments across three benchmark datasets demonstrate that TANDEM significantly outperforms zero-shot and context-augmented baselines, achieving 0.73 F1 in target identification on HateMM (a 30% improvement over state-of-the-art) while maintaining precise temporal grounding. We further observe that while binary detection is robust, differentiating between offensive and hateful content remains challenging in multi-class settings due to inherent label ambiguity and dataset imbalance. More broadly, our findings suggest that structured, interpretable alignment is achievable even in complex multimodal settings, offering a blueprint for the next generation of transparent and actionable online safety moderation tools.

</details>


### [60] [Beyond Model Scaling: Test-Time Intervention for Efficient Deep Reasoning](https://arxiv.org/abs/2601.11252)
*Qianyue Wang,Jinwu Hu,Yufeng Wang,Huanxiang Lin,Bolin Chen,Zhiquan Wen,Yaofo Chen,Mingkui Tan*

Main category: cs.AI

TL;DR: Think-with-Me是一种新型的测试时交互推理范式，通过在推理过程中引入外部反馈干预，解决大型推理模型中存在的过度思考和过度推理问题，在有限上下文窗口下实现准确性与推理长度的更好平衡。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在多步推理方面表现出色，但经常存在推理过程效率低下的问题，如过度思考和过度推理，这会增加计算成本并降低性能。现有的高效推理方法以闭环方式运行，缺乏外部干预机制来指导推理过程。

Method: 提出Think-with-Me范式，在过渡连词处暂停推理以获取外部反馈，通过多标准评估（合理性和完整性）生成反馈，使用Group Relative Policy Optimization训练目标模型适应交互模式，自适应地延长或终止推理以减少冗余。

Result: 在AIME24上，Think-with-Me在8K窗口下比QwQ-32B准确率提高7.19%，同时平均推理长度减少81%。该范式在有限上下文窗口下实现了准确性与推理长度的优越平衡，对安全和创造性任务也有益处。

Conclusion: Think-with-Me通过引入外部反馈干预的交互式推理范式，有效解决了大型推理模型中的过度推理问题，在保持准确性的同时显著减少了推理长度，为高效推理提供了新的解决方案。

Abstract: Large Reasoning Models (LRMs) excel at multi-step reasoning but often suffer from inefficient reasoning processes like overthinking and overshoot, where excessive or misdirected reasoning increases computational cost and degrades performance. Existing efficient reasoning methods operate in a closed-loop manner, lacking mechanisms for external intervention to guide the reasoning process. To address this, we propose Think-with-Me, a novel test-time interactive reasoning paradigm that introduces external feedback intervention into the reasoning process. Our key insights are that transitional conjunctions serve as natural points for intervention, signaling phases of self-validation or exploration and using transitional words appropriately to prolong the reasoning enhances performance, while excessive use affects performance. Building on these insights, Think-with-Me pauses reasoning at these points for external feedback, adaptively extending or terminating reasoning to reduce redundancy while preserving accuracy. The feedback is generated via a multi-criteria evaluation (rationality and completeness) and comes from either human or LLM proxies. We train the target model using Group Relative Policy Optimization (GRPO) to adapt to this interactive mode. Experiments show that Think-with-Me achieves a superior balance between accuracy and reasoning length under limited context windows. On AIME24, Think-with-Me outperforms QwQ-32B by 7.19% in accuracy while reducing average reasoning length by 81% under an 8K window. The paradigm also benefits security and creative tasks.

</details>


### [61] [XChoice: Explainable Evaluation of AI-Human Alignment in LLM-based Constrained Choice Decision Making](https://arxiv.org/abs/2601.11286)
*Weihong Qi,Fan Huang,Rasika Muralidharan,Jisun An,Haewoon Kwak*

Main category: cs.AI

TL;DR: XChoice是一个可解释的框架，用于评估约束决策中AI与人类的对齐程度，超越传统准确性指标，通过机制建模分析决策因素、约束敏感性和权衡关系。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要关注结果一致性（如准确率、F1分数），缺乏对决策机制的深入理解，无法诊断AI与人类在约束决策中的根本对齐差异。

Method: XChoice通过拟合基于机制的决策模型到人类数据和LLM生成的决策中，恢复可解释参数（决策因素相对重要性、约束敏感性、隐含权衡），通过比较参数向量评估对齐度。

Result: 在美国人日常时间分配研究中，发现模型与人类对齐存在异质性，黑人和已婚群体中存在显著错位；通过不变性分析验证鲁棒性，RAG干预可针对性缓解错位。

Conclusion: XChoice提供基于机制的指标，能够诊断错位并支持超越表面结果匹配的知情改进，为AI与人类对齐评估提供更深入的分析框架。

Abstract: We present XChoice, an explainable framework for evaluating AI-human alignment in constrained decision making. Moving beyond outcome agreement such as accuracy and F1 score, XChoice fits a mechanism-based decision model to human data and LLM-generated decisions, recovering interpretable parameters that capture the relative importance of decision factors, constraint sensitivity, and implied trade-offs. Alignment is assessed by comparing these parameter vectors across models, options, and subgroups. We demonstrate XChoice on Americans' daily time allocation using the American Time Use Survey (ATUS) as human ground truth, revealing heterogeneous alignment across models and activities and salient misalignment concentrated in Black and married groups. We further validate robustness of XChoice via an invariance analysis and evaluate targeted mitigation with a retrieval augmented generation (RAG) intervention. Overall, XChoice provides mechanism-based metrics that diagnose misalignment and support informed improvements beyond surface outcome matching.

</details>


### [62] [AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems](https://arxiv.org/abs/2601.11354)
*Weiyi Wang,Xinchi Chen,Jingjing Gong,Xuanjing Huang,Xipeng Qiu*

Main category: cs.AI

TL;DR: AstroReason-Bench是一个用于评估智能体在空间规划问题中规划能力的基准测试，该基准整合了多种调度机制，发现当前智能体在现实约束下的表现远不及专用求解器。


<details>
  <summary>Details</summary>
Motivation: 现有智能体基准主要关注符号化或弱接地环境，缺乏对物理约束现实世界领域的评估，需要建立一个能评估智能体在具有异质目标、严格物理约束和长时程决策的高风险空间规划问题中表现的综合基准。

Method: 提出了AstroReason-Bench基准，整合了地面站通信和敏捷地球观测等多种调度机制，提供了统一的智能体导向交互协议，并在多种最先进的开源和闭源智能体LLM系统上进行了评估。

Result: 评估发现当前智能体在现实约束下的表现显著低于专用求解器，突显了通用规划器在现实约束下的关键局限性。

Conclusion: AstroReason-Bench为未来智能体研究提供了一个具有挑战性和诊断性的测试平台，有助于推动智能体在物理约束现实世界领域的发展。

Abstract: Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.

</details>


### [63] [Hyperparameter Optimization of Constraint Programming Solvers](https://arxiv.org/abs/2601.11389)
*Hedieh Haddad,Thibault Falque,Pierre Talbot,Pascal Bouvry*

Main category: cs.AI

TL;DR: 提出"探针与求解"两阶段框架，用于约束规划求解器的自动超参数优化，通过贝叶斯优化等方法在时间预算内先探索最优配置再求解问题。


<details>
  <summary>Details</summary>
Motivation: 约束规划求解器的性能对超参数选择高度敏感，手动寻找最佳配置需要专家知识且耗时，需要自动化超参数优化方法。

Method: 提出两阶段框架：1) 探针阶段使用可配置的超参数优化方法（贝叶斯优化和汉明距离搜索）探索不同超参数集；2) 求解阶段使用找到的最佳配置在剩余时间内解决问题。

Result: 在114个组合问题实例上测试，贝叶斯优化方法优于求解器默认配置：ACE求解器在25.4%实例中提升解质量，57.9%实例持平；Choco求解器在38.6%实例中表现更优。贝叶斯优化也始终优于汉明距离搜索。

Conclusion: 探针与求解算法为约束求解器调优提供了实用、资源感知的方法，能在多种问题类型上实现稳健的性能提升，模型化探索优于简单局部搜索。

Abstract: The performance of constraint programming solvers is highly sensitive to the choice of their hyperparameters. Manually finding the best solver configuration is a difficult, time-consuming task that typically requires expert knowledge. In this paper, we introduce probe and solve algorithm, a novel two-phase framework for automated hyperparameter optimization integrated into the CPMpy library. This approach partitions the available time budget into two phases: a probing phase that explores different sets of hyperparameters using configurable hyperparameter optimization methods, followed by a solving phase where the best configuration found is used to tackle the problem within the remaining time.
  We implement and compare two hyperparameter optimization methods within the probe and solve algorithm: Bayesian optimization and Hamming distance search. We evaluate the algorithm on two different constraint programming solvers, ACE and Choco, across 114 combinatorial problem instances, comparing their performance against the solver's default configurations.
  Results show that using Bayesian optimization, the algorithm outperforms the solver's default configurations, improving solution quality for ACE in 25.4% of instances and matching the default performance in 57.9%, and for Choco, achieving superior results in 38.6% of instances. It also consistently surpasses Hamming distance search within the same framework, confirming the advantage of model-based exploration over simple local search. Overall, the probe and solve algorithm offers a practical, resource-aware approach for tuning constraint solvers that yields robust improvements across diverse problem types.

</details>


### [64] [Exploring LLM Features in Predictive Process Monitoring for Small-Scale Event-Logs](https://arxiv.org/abs/2601.11468)
*Alessandro Padella,Massimiliano de Leoni,Marlon Dumas*

Main category: cs.AI

TL;DR: 本文扩展了基于LLM的预测性流程监控框架，从仅关注总时间预测扩展到多KPI评估，在数据稀缺场景下（仅100条轨迹）LLM表现优于基准方法，并展示了LLM利用先验知识和训练轨迹内部相关性进行高阶推理的能力。


<details>
  <summary>Details</summary>
Motivation: 预测性流程监控旨在预测进行中流程的结果，传统方法使用机器学习和深度学习架构。本文旨在扩展先前基于LLM的预测性流程监控框架，全面评估其泛化能力、语义利用和推理机制，并扩展到多个关键绩效指标。

Method: 扩展了基于LLM的预测性流程监控框架，从仅通过提示进行总时间预测扩展到全面评估。在三个不同的事件日志上进行实证评估，涵盖总时间和活动发生预测两个关键绩效指标。特别关注数据稀缺设置（仅100条轨迹）。

Result: 在数据稀缺设置中（仅100条轨迹），LLM在所有评估的关键绩效指标上均超越了基准方法。实验表明LLM既利用了其内在的先验知识，也利用了训练轨迹之间的内部相关性。模型不简单复制现有预测方法，而是执行高阶推理来生成预测。

Conclusion: 基于LLM的预测性流程监控框架在数据稀缺场景下表现出色，能够有效利用先验知识和训练数据相关性，并通过高阶推理机制生成预测，为流程监控提供了新的有效方法。

Abstract: Predictive Process Monitoring is a branch of process mining that aims to predict the outcome of an ongoing process. Recently, it leveraged machine-and-deep learning architectures. In this paper, we extend our prior LLM-based Predictive Process Monitoring framework, which was initially focused on total time prediction via prompting. The extension consists of comprehensively evaluating its generality, semantic leverage, and reasoning mechanisms, also across multiple Key Performance Indicators. Empirical evaluations conducted on three distinct event logs and across the Key Performance Indicators of Total Time and Activity Occurrence prediction indicate that, in data-scarce settings with only 100 traces, the LLM surpasses the benchmark methods. Furthermore, the experiments also show that the LLM exploits both its embodied prior knowledge and the internal correlations among training traces. Finally, we examine the reasoning strategies employed by the model, demonstrating that the LLM does not merely replicate existing predictive methods but performs higher-order reasoning to generate the predictions.

</details>


### [65] [Health Facility Location in Ethiopia: Leveraging LLMs to Integrate Expert Knowledge into Algorithmic Planning](https://arxiv.org/abs/2601.11479)
*Yohai Trabelsi,Guojun Xiong,Fentabil Getnet,Stéphane Verguet,Milind Tambe*

Main category: cs.AI

TL;DR: 提出LEG混合框架，结合LLM和优化算法，在资源有限条件下优先升级埃塞俄比亚卫生站，平衡人口覆盖和专家偏好


<details>
  <summary>Details</summary>
Motivation: 埃塞俄比亚卫生部升级农村卫生站资源有限，需要平衡人口覆盖最大化和专家/利益相关者的多样化偏好，传统优化方法难以处理自然语言表达的定性标准

Method: 开发LEG框架：结合可证明近似算法优化人口覆盖，使用LLM驱动的迭代细化，通过人机对齐确保解决方案反映专家定性指导，同时保持覆盖保证

Result: 在埃塞俄比亚三个地区的真实数据实验中验证了框架的有效性，证明能够为公平、数据驱动的卫生系统规划提供信息

Conclusion: LEG框架成功弥合了理论优化和实际专家知识之间的差距，为资源受限环境下的卫生设施升级决策提供了系统化方法

Abstract: Ethiopia's Ministry of Health is upgrading health posts to improve access to essential services, particularly in rural areas. Limited resources, however, require careful prioritization of which facilities to upgrade to maximize population coverage while accounting for diverse expert and stakeholder preferences. In collaboration with the Ethiopian Public Health Institute and Ministry of Health, we propose a hybrid framework that systematically integrates expert knowledge with optimization techniques. Classical optimization methods provide theoretical guarantees but require explicit, quantitative objectives, whereas stakeholder criteria are often articulated in natural language and difficult to formalize. To bridge these domains, we develop the Large language model and Extended Greedy (LEG) framework. Our framework combines a provable approximation algorithm for population coverage optimization with LLM-driven iterative refinement that incorporates human-AI alignment to ensure solutions reflect expert qualitative guidance while preserving coverage guarantees. Experiments on real-world data from three Ethiopian regions demonstrate the framework's effectiveness and its potential to inform equitable, data-driven health system planning.

</details>


### [66] [BoxMind: Closed-loop AI strategy optimization for elite boxing validated in the 2024 Olympics](https://arxiv.org/abs/2601.11492)
*Kaiwen Wang,Kaili Zheng,Rongrong Deng,Qingmin Fan,Milin Zhang,Zongrui Li,Xuesi Zhou,Bo Han,Liren Chen,Chenyi Guo,Ji Wu*

Main category: cs.AI

TL;DR: BoxMind是一个用于拳击战术分析的闭环AI专家系统，通过定义原子击打事件、构建层次化技术战术指标，并基于图模型预测比赛结果，最终生成可执行的战术调整建议。


<details>
  <summary>Details</summary>
Motivation: 格斗类运动如拳击在AI驱动分析方面发展不足，主要因为动作动态复杂且缺乏结构化战术表示。需要将非结构化视频数据转化为战略智能，弥合计算机视觉与竞技体育决策支持之间的差距。

Method: 1. 定义具有精确时间边界、空间和技术属性的原子击打事件；2. 将比赛录像解析为18个层次化技术战术指标；3. 提出基于图的预测模型，融合显性技术战术特征与可学习的时间变化潜在嵌入；4. 将比赛结果建模为技术战术指标的可微分函数，将获胜概率梯度转化为可执行的战术调整。

Result: 1. 结果预测模型在BoxerGraph测试集上达到69.8%准确率，在奥运比赛上达到87.5%准确率；2. 系统生成的战略建议达到与人类专家相当的水平；3. 在2024年巴黎奥运会闭环部署中，直接助力中国国家队取得3金2银的历史性成绩。

Conclusion: BoxMind建立了将非结构化视频数据转化为战略智能的可复制范式，成功弥合了计算机视觉与竞技体育决策支持之间的差距，为格斗类运动的AI驱动战术分析提供了有效解决方案。

Abstract: Competitive sports require sophisticated tactical analysis, yet combat disciplines like boxing remain underdeveloped in AI-driven analytics due to the complexity of action dynamics and the lack of structured tactical representations. To address this, we present BoxMind, a closed-loop AI expert system validated in elite boxing competition. By defining atomic punch events with precise temporal boundaries and spatial and technical attributes, we parse match footage into 18 hierarchical technical-tactical indicators. We then propose a graph-based predictive model that fuses these explicit technical-tactical profiles with learnable, time-variant latent embeddings to capture the dynamics of boxer matchups. Modeling match outcome as a differentiable function of technical-tactical indicators, we turn winning probability gradients into executable tactical adjustments. Experiments show that the outcome prediction model achieves state-of-the-art performance, with 69.8% accuracy on BoxerGraph test set and 87.5% on Olympic matches. Using this predictive model as a foundation, the system generates strategic recommendations that demonstrate proficiency comparable to human experts. BoxMind is validated through a closed-loop deployment during the 2024 Paris Olympics, directly contributing to the Chinese National Team's historic achievement of three gold and two silver medals. BoxMind establishes a replicable paradigm for transforming unstructured video data into strategic intelligence, bridging the gap between computer vision and decision support in competitive sports.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [67] [Disorder effects in two-dimensional flat-band system with next-nearest-neighbor hopping](https://arxiv.org/abs/2601.10932)
*Yue Heng Liu,Zi-Xiang Hu,Qi Li*

Main category: cond-mat.dis-nn

TL;DR: 该研究探讨了二维Lieb晶格中，在存在复数次近邻跃迁的情况下，平带的局域化机制。研究发现拓扑边缘态可以缓解弱无序下的几何局域化，而相关无序会诱导逆安德森转变，使拓扑边缘态在强无序下保持稳定。


<details>
  <summary>Details</summary>
Motivation: 研究二维Lieb晶格中平带的局域化机制，特别是在存在复数次近邻跃迁的情况下。探索拓扑边缘态如何影响平带的局域化行为，以及无序对拓扑相变和平带系统的影响。

Method: 采用转移矩阵方法研究存在复数次近邻跃迁时的平带局域化机制。通过计算陈数来确认拓扑边缘态在强无序下的鲁棒性。

Result: 1. 拓扑边缘态可以缓解弱无序下的平带几何局域化；2. 相关无序会诱导逆安德森转变，使拓扑边缘态在强无序下保持稳定；3. 陈数计算证实了这种现象的鲁棒性。

Conclusion: 该研究建立了一个统一的平台，用于研究拓扑相变、平带和无序效应之间的相互作用，揭示了拓扑边缘态在无序环境中对平带局域化的调节作用。

Abstract: For two-dimensional Lieb lattice, while intrinsic spin-orbit coupling is responsible for opening the gap that exhibits the quantum spin Hall effect, topological phase transitions are driven by a real next-nearest-neighbor (NNN) hopping. In this work, we utilize the transfer matrix method to study the flat-band localization mechanism in the presence of complex NNN hoppings. We demonstrate that the geometric localization in flat bands can be alleviated by topological edge states under weak disorder. Furthermore, correlated disorders are shown to induce inverse Anderson transition with the topological edge states persisting under strong disorder, a robustness confirmed by Chern number calculations, which identifies the root cause of this phenomenon. These findings establish a unified platform for investigating topological phase transitions, flat bands, and disorder effects.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [68] [Exponential gain in clock precision using quantum correlated ticks](https://arxiv.org/abs/2601.10785)
*Florian Meier,Yuri Minoguchi,Gianmichele Blasi,Géraldine Haack,Marcus Huber*

Main category: quant-ph

TL;DR: 该论文提出了一种基于量子关联实现自主自校正的新型时钟范式，通过量子排斥原理在超短时间尺度上获得指数级精度优势


<details>
  <summary>Details</summary>
Motivation: 传统时钟设计面临超短时间尺度精度限制的挑战，要么依赖振荡器长期稳定性但短时精度受限，要么追求单个随机滴答的精度。需要探索新的时钟设计范式来突破超短时间尺度的时间测量极限。

Method: 提出自主自校正时钟概念，利用量子关联实现连续滴答的量子级校正。通过完全求解耦合量子系统模型，展示量子排斥原理如何在量子层面关联时钟，并利用量子输运理论的优势。

Result: 理论证明量子关联时钟在超短时间尺度上可获得指数级精度优势。通过包含现实缺陷的仿真验证，这种精度增益在非理想条件下依然保持稳定，为当代量子技术实现提供了路线图。

Conclusion: 量子自主自校正时钟为超短时间尺度的时间测量提供了全新范式，结合量子关联和量子输运理论，有望突破传统时钟设计的精度限制，具有实际实现的可行性。

Abstract: Creating precise timing devices at ultra-short time scales is not just an important technological challenge, but confronts us with foundational questions about timekeeping's ultimate precision limits. Research on clocks has either focused on long-term stability using an oscillator stabilized by a level transition, limiting precision at short timescales, or on making individual stochastic ticks as precise as possible. Here, we prove the viability of a conceptually different avenue: the autonomous self-correction of consecutive ticks by quantum correlations. This provides a new paradigm that integrates the advantages and insights from quantum transport theory to operate clocks at ultra-short timescales. We fully solve a model of coupled quantum systems and show how the emergent Pauli exclusion principle correlates the clock at the quantum level yielding an exponential advantage in precision. We furthermore demonstrate through simulations with realistic imperfections that this remarkable gain in precision remains stable providing a roadmap for implementation with contemporary quantum technologies.

</details>


### [69] [Elevator Codes: Concatenation for resource-efficient quantum memory under biased noise](https://arxiv.org/abs/2601.10786)
*Peter Shanahan,Diego Ruiz*

Main category: quant-ph

TL;DR: 提出一种针对偏置噪声的2D局部量子纠错码构造，在噪声偏置η≥7×10⁴时优于现有方案，可将量子比特开销降低50%以上


<details>
  <summary>Details</summary>
Motivation: 现有偏置噪声量子比特纠错方案存在局限性：矩形表面码阈值较低，XZZX码需要两倍物理量子比特来维持相同码距。需要开发更高效的纠错方案来降低量子比特开销

Method: 采用两层经典码级联构造：内层为重复相位翻转码，外层为高编码率比特翻转码。通过逻辑层实现外层码，规避设备连接性限制，针对相位翻转和比特翻转错误在不同编码层处理

Result: 在p_Z=10⁻³、η=2×10⁶条件下，实现逻辑错误率10⁻¹²时，量子比特开销降低超过50%。内层码专注于相位翻转错误的高阈值，外层码优化编码率效率

Conclusion: 在足够强的偏置噪声下，在不同编码层分别处理相位翻转和比特翻转错误具有优势。高编码率外层码使纠正残余比特翻转错误的开销与重复码本身相当，显著低于先前方法

Abstract: Biased-noise qubits, in which one type of error (e.g. $X$- and $Y$-type errors) is significantly suppressed relative to the other (e.g. $Z$-type errors), can significantly reduce the overhead of quantum error correction. Codes such as the rectangular surface code or XZZX code substantially reduce the qubit overhead under biased noise, but they still face challenges. The rectangular surface code suffers from a relatively low threshold, while the XZZX code requires twice as many physical qubits to maintain the same code distance as the surface code. In this work, we introduce a 2D local code construction that outperforms these codes for noise biases $η\ge 7\times10^{4}$, reducing the qubit overhead by over 50% at $p_Z=10^{-3}$ and $η= 2 \times 10^6$ to achieve a logical error rate of $10^{-12}$. Our construction relies on the concatenation of two classical codes. The inner codes are repetition phase-flip codes while the outer codes are high-rate bit-flip codes enabled by their implementation at the logical level, which circumvents device connectivity constraints. These results indicate that under sufficiently biased noise, it is advantageous to address phase-flip and bit-flip errors at different layers of the coding scheme. The inner code should prioritize a high threshold for phase-flip errors, while the bit-flip outer code should optimize for encoding rate efficiency. In the strong biased-noise regime, high-rate outer codes keep the overhead for correcting residual bit-flip errors comparable to that of the repetition code itself, meaningfully lower than that required by earlier approaches.

</details>


### [70] [Critical non-equilibrium phases from noisy topological memories](https://arxiv.org/abs/2601.10792)
*Amir-Reza Negari,Subhayan Sahu,Jan Behrends,Benjamin Béri,Timothy H. Hsieh*

Main category: quant-ph

TL;DR: 该研究发现了表面码在随机泡利测量信道下的扩展非平衡临界相，其特征是条件互信息的亚指数衰减，该相与环模型的Goldstone相相关，保留部分逻辑信息但无法通过准局域解码器恢复。


<details>
  <summary>Details</summary>
Motivation: 研究表面码在随机测量下的非平衡相变行为，探索量子纠错码在噪声环境中的信息保持能力，特别是理解哪些信息可以被准局域解码器恢复。

Method: 将混合态映射到正方形晶格上的完全填充环模型，利用环模型的理论结果分析条件互信息衰减，引入"穿孔相干信息"作为准局域解码的必要条件诊断工具。

Result: 发现了扩展临界相，其条件互信息呈多对数衰减，该相保留部分逻辑信息但只能通过全局解码器恢复，无法通过任何准局域解码器恢复。

Conclusion: 表面码在随机测量下存在扩展临界相，该相具有独特的非平衡特性，为理解量子纠错码在噪声环境中的信息保持和恢复机制提供了新视角。

Abstract: We demonstrate the existence of an extended non-equilibrium critical phase, characterized by sub-exponential decay of conditional mutual information (CMI), in the surface code subject to heralded random Pauli measurement channels. By mapping the resulting mixed state to the ensemble of completely packed loops on a square lattice, we relate the extended phase to the Goldstone phase of the loop model. In particular, CMI is controlled by the characteristic length scale of loops, and we use analytic results of the latter to establish polylogarithmic decay of CMI in the critical phase. We find that the critical phase retains partial logical information that can be recovered by a global decoder, but not by any quasi-local decoder. To demonstrate this, we introduce a diagnostic called punctured coherent information which provides a necessary condition for quasi-local decoding.

</details>


### [71] [Parent Hamiltonians for stabilizer quantum many-body scars](https://arxiv.org/abs/2601.10805)
*Shane Dooley*

Main category: quant-ph

TL;DR: 提出了一种将稳定子态嵌入为量子多体疤痕的通用构造方法，基于泡利弦的可分解性，能够系统构建具有零能量稳定子疤痕的局域哈密顿量。


<details>
  <summary>Details</summary>
Motivation: 量子多体疤痕在弱遍历性破缺中起重要作用，但缺乏系统构造方法。本文旨在开发一个通用框架，能够将各种稳定子态嵌入为局域哈密顿量的量子多体疤痕。

Method: 基于泡利弦在晶格上的可分解性概念，将稳定子元素转换为局域的、少体算符，这些算符湮灭稳定子态。利用这种方法系统构建具有零能量稳定子疤痕的父哈密顿量。

Result: 方法统一再现了多个已知结果，包括"彩虹"疤痕和纠缠反足贝尔对态等体积律纠缠疤痕。成功构造了更复杂纠缠结构的稳定子疤痕，如簇态、环面码态和反足环面码态。

Conclusion: 提出了一个通用框架，能够系统地将稳定子态构造为量子多体疤痕，为理解弱遍历性破缺提供了新工具，并展示了该方法在复杂纠缠结构中的应用潜力。

Abstract: Quantum many-body scars (QMBS) have attracted considerable interest due to their role in weak ergodicity breaking in many-body systems. We present a general construction that embeds stabilizer states as QMBS of local Hamiltonians. The method relies on a notion of factorizability of Pauli strings on a lattice, which is used to convert stabilizer elements into local, few-body operators that annihilate the stabilizer state. This enables the systematic construction of parent Hamiltonians with zero-energy stabilizer QMBS typically near the middle of the spectrum. The method reproduces several known results in a unified framework, including recent examples of volume-law entangled QMBS, such as the ``rainbow'' QMBS and the entangled antipodal Bell pair state. We also apply the framework to construct examples of stabilizer QMBS with a more complex entanglement structure, such as the cluster state, the toric code state, and a volume-law entangled state we dub the antipodal toric code (ATC) state. Exact diagonalization confirms our results and reveal the stabilizer states as exact eigenstates of their parent Hamiltonian.

</details>


### [72] [Charging a quantum battery from the Bloch sphere](https://arxiv.org/abs/2601.10844)
*C. A. Downing,M. S. Ukhtary*

Main category: quant-ph

TL;DR: 研究重新审视了由充电量子比特和单电池量子比特组成的简单量子电池模型的充电过程量子能量学和量子热力学，发现存储能量、功容和容量都依赖于初始布洛赫球极角，类似于量子面积定理。


<details>
  <summary>Details</summary>
Motivation: 研究量子电池充电过程的量子能量学和热力学特性，探索初始量子态对电池性能的影响，为基于耦合两能级系统的量子能量科学实验提供理论指导。

Method: 采用由充电量子比特和单电池量子比特组成的简单两分量量子电池模型，允许充电器初始量子态位于布洛赫球表面的任意位置，推导广义解析表达式来描述存储能量、功容和容量。

Result: 发现存储能量、功容和容量都依赖于初始布洛赫球极角，类似于量子面积定理；功容的产生源于量子相干性和布居反转的平衡；功容充电功率及其最佳充电时间与传统忽略热力学考虑的结果存在显著偏差。

Conclusion: 该理论研究为基于耦合两能级系统的量子能量科学实验提供了理论基础，揭示了量子相干性和布居反转在量子电池性能中的关键作用，以及热力学考虑对充电过程的重要影响。

Abstract: We reconsider the quantum energetics and quantum thermodynamics of the charging process of a simple, two-component quantum battery model made up of a charger qubit and a single--cell battery qubit. We allow for the initial quantum state of the charger to lie anywhere on the surface of the Bloch sphere, and find the generalized analytical expressions describing the stored energy, ergotropy and capacity of the battery, all of which depend upon the initial Bloch sphere polar angle in a manner evocative of the quantum area theorem. The origin of the ergotropy produced, as well as the genesis of the battery capacity, can be readily traced back to the quantum coherences and population inversions generated (and the balance between these two mechanisms is contingent upon the starting Bloch polar angle). Importantly, the ergotropic charging power and its associated optimal charging time display notable deviations from standard results which disregard thermodynamic considerations. Our theoretical groundwork may be useful for guiding forthcoming experiments in quantum energy science based upon coupled two-level systems.

</details>


### [73] [Efficient Quantum Circuits for the Hilbert Transform](https://arxiv.org/abs/2601.10876)
*Henry Zhang,Joseph Li*

Main category: quant-ph

TL;DR: 提出了一种量子希尔伯特变换的高效实现，相比经典算法指数级减少操作，适用于非平稳信号和异常检测


<details>
  <summary>Details</summary>
Motivation: 希尔伯特变换在非平稳信号和异常检测中比量子傅里叶变换和量子小波变换更强大，但之前没有高效的量子实现方案

Method: 构建了量子希尔伯特变换的多对数规模和对数深度实现，将算法推广到任意d维希尔伯特变换，深度为O(d log N)

Result: 实现了信号长度N的多对数规模和指数级减少的操作，模拟显示在电力系统控制和图像处理等任务中有效，与经典结果完全一致

Conclusion: 成功开发了高效的量子希尔伯特变换算法，为量子信息处理提供了新的强大工具

Abstract: The quantum Fourier transform and quantum wavelet transform have been cornerstones of quantum information processing. However, for non-stationary signals and anomaly detection, the Hilbert transform can be a more powerful tool, yet no prior work has provided efficient quantum implementations for the discrete Hilbert transform. This letter presents a novel construction for a quantum Hilbert transform in polylogarithmic size and logarithmic depth for a signal of length $N$, exponentially fewer operations than classical algorithms for the same mapping. We generalize this algorithm to create any $d$-dimensional Hilbert transform in depth $O(d\log N)$. Simulations demonstrate effectiveness for tasks such as power systems control and image processing, with exact agreement with classical results.

</details>


### [74] [Two-tooth bosonic quantum comb for temporal-correlation sensing](https://arxiv.org/abs/2601.10916)
*Shaojiang Zhu,Xinyuan You,Alexander Romanenko,Anna Grassellino*

Main category: quant-ph

TL;DR: 该论文提出了一种双齿玻色子量子梳，用于研究热吸收体与长寿命相干探针之间的顺序相互作用，通过干涉效应实现玻色子噪声光谱分析。


<details>
  <summary>Details</summary>
Motivation: 研究热吸收体与相干探针之间的相互作用，特别是如何通过探针记录瞬时涨落及其时间相关性，从而实现对玻色子环境的探测。

Method: 采用双齿玻色子量子梳模型，利用过程张量公式推导闭式表达式，通过扫描两个相互作用窗口之间的时间间隔来采样吸收体的关联函数。

Result: 发现干涉效应产生非单调记忆响应，反映了热布居数与动力学关联之间的竞争，能够区分马尔可夫温度噪声与慢速或谱结构涨落。

Conclusion: 该方法与电路QED平台兼容，为探测涨落玻色子环境提供了一种通用方法，实现了玻色子噪声光谱分析。

Abstract: We introduce a two-tooth bosonic quantum comb that captures the sequential interactions between a thermal absorber and a long-lived coherent probe. The comb provides a causal, multi-time description of coherence transport, tracking how the probe records both instantaneous fluctuations and their temporal correlations. Using a process-tensor formulation, we derive closed form expressions showing that interference between the two interaction windows generates a non-monotonic memory response that reflects a fundamental competition between the absorbers thermal population and its dynamical correlations. By sweeping the temporal separation between the interaction windows, the probe directly samples the absorbers population correlator, enabling bosonic noise spectroscopy that discriminates Markovian temperature noise from slow or spectrally structured fluctuations. The approach is readily compatible with circuit-QED platforms and offers a general method for probing fluctuating bosonic environments.

</details>


### [75] [Quantum trajectories for time-binned data and their closeness to fully conditioned quantum trajectories](https://arxiv.org/abs/2601.10937)
*Nattaphong Wonglakhon,Areeya Chantasri,Howard M. Wiseman*

Main category: quant-ph

TL;DR: 该论文研究了量子轨迹的有限时间间隔动力学映射，比较了不同映射方法在从测量电流数据重构量子态时的误差标度。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解当只能获得有限时间间隔内的平均电流测量数据时，如何更准确地重构量子系统的条件态。虽然已有研究发现使用平均电流数据可以得到几乎纯的量子态，但其与真实条件态的距离仍然较大。

Method: 论文提出了两种有限时间间隔动力学映射方法：1) 基于平均电流I_t的映射，产生条件态ρ；2) 引入额外统计量φ_t的Φ-映射，产生条件态ψ_Φ。通过数值模拟比较了这两种映射与Itô映射及其他高阶映射的误差标度。

Result: 研究发现：1) 基于平均电流的映射产生的条件态与真实条件态的距离标度为(Δt)^{3/2}；2) 新提出的Φ-映射产生的条件态与真实条件态的距离标度为(Δt)^{2}，误差更小；3) 数值验证了这些标度关系，表明Φ-映射在提取额外统计量φ_t时具有最优性能。

Conclusion: 对于一般量子系统，如果实验中除了平均电流I_t外还能提取统计量φ_t，那么Φ-映射相比其他映射方法能提供更小的重构误差，为量子轨迹的有限时间间隔分析提供了更精确的工具。

Abstract: Quantum trajectories are dynamical equations for quantum states conditioned on the results of a time-continuous measurement, such as a continuous-in-time current $\vec y_t$. Recently there has been renewed interest in dynamical maps for quantum trajectories with time-intervals of finite size $Δt$. Guilmin \emph{et al.} (unpublished) derived such a dynamical map for the (experimentally relevant) case where only the average current $I_t$ over each interval is available. Surprisingly, this binned data still generates a conditioned state $ρ_\text{\faFaucet}$ that is almost pure (for efficient measurements), with an impurity scaling as $(Δt)^{3}$. We show that, nevertheless, the typical distance of $ρ_\text{\faFaucet}$ from $\hatψ_{\text{F}; \vec y_t}$ -- the projector for the pure state conditioned on the full current -- is as large as $(Δt)^{3/2}$. We introduce another finite-interval dynamical map (``$Φ$-map''), which requires only one additional real statistic, $φ_t$, of the current in the interval, that gives a conditioned state $\hatψ_Φ$ which is only $(Δt)^{2}$-distant from $\hatψ_{\text{F}; \vec y_t}$. We numerically verify these scalings of the error (distance from the true states) for these two maps, as well as for the lowest-order (Itô) map and two other higher-order maps. Our results show that, for a generic system, if the statistic $φ_t$ can be extracted from experiment along with $I_t$, then the $Φ$-map gives a smaller error than any other.

</details>


### [76] [The Hilbert-Schmidt norms of quantum channels and matrix integrals over the unit sphere](https://arxiv.org/abs/2601.10943)
*Yuan Li,Zhengli Chen,Zhihua Guo,Yongfeng Pang*

Main category: quant-ph

TL;DR: 研究量子通道的希尔伯特-施密特范数平方与其互补通道范数平方之和的可能取值范围，并给出达到极值的量子通道的等价刻画


<details>
  <summary>Details</summary>
Motivation: 量子系统的动力学通常由量子通道族描述，研究量子通道的希尔伯特-施密特范数与其互补通道范数之间的关系有助于深入理解量子信息处理中的通道特性

Method: 主要研究量子通道的希尔伯特-施密特范数平方与其互补通道范数平方之和的所有可能取值范围，给出达到最大值和最小值的量子通道的等价刻画，同时研究无限维系统中保持纯态的完全正映射，并证明单位球上几种矩阵积分的等价性

Result: 确定了量子通道的希尔伯特-施密特范数平方与其互补通道范数平方之和的取值范围，给出了达到极值的量子通道的等价刻画，获得了无限维系统中保持纯态的完全正映射的具体描述，证明了单位球上几种矩阵积分的等价性并得到了一些扩展结果

Conclusion: 该研究深入探讨了量子通道的希尔伯特-施密特范数特性，为量子信息理论提供了重要的数学工具和理论结果，特别是在量子通道的极值特性和矩阵积分等价性方面取得了有价值的进展

Abstract: The dynamics of quantum systems are generally described by a family of quantum channels (linear, completely positive and trace preserving maps). In this note, we mainly study the range of all possible values of $\|\mathcal{E}\|_2^2+\|\widetilde{\mathcal{E}}\|_2^2$ for quantum channels $\mathcal{E}$ and give the equivalent characterizations for quantum channels that achieve these maximum and minimum values, respectively, where $\|\mathcal{E}\|_2$ is the Hilbert-Schmidt norm of $\mathcal{E}$ and $\widetilde{\mathcal{E}}$ is a complementary channel of $\mathcal{E}.$ Also, we get a concrete description of completely positive maps on infinite dimensional systems preserving pure states. Moreover, the equivalency of several matrix integrals over the unit sphere is demonstrated and some extensions of these matrix integrals are obtained.

</details>


### [77] [The two-time Leggett-Garg inequalities of a superconducting qubit interacting with thermal photons in a cavity](https://arxiv.org/abs/2601.10946)
*Hiroo Azuma*

Main category: quant-ph

TL;DR: 该研究探讨了约瑟夫森结量子比特与外部磁通相互作用模型中双时间Leggett-Garg不等式的违反情况，发现温度升高会减弱不等式违反，且违反程度与温度呈幂律关系，其指数取决于耦合常数。


<details>
  <summary>Details</summary>
Motivation: 研究量子光学模型中Leggett-Garg不等式的违反行为，特别关注约瑟夫森结量子比特与光子相互作用的扩展模型，该模型的光子相互作用部分是线性组合的平方，而非传统可解模型中的线性组合。

Method: 采用二阶微扰近似方法研究不可解模型的时间演化，通过数值计算分析双时间Leggett-Garg不等式的违反情况与温度和耦合常数的关系。

Result: 数值计算显示：1）LG不等式的违反程度随温度升高而减弱；2）违反程度与温度呈幂律关系；3）幂律指数随耦合常数变化；4）耦合常数越大，不等式违反程度越小且对温度越不敏感。

Conclusion: 该研究揭示了在扩展的量子光学模型中，Leggett-Garg不等式的违反行为受到温度和耦合常数的显著影响，为理解量子系统在热环境下的宏观实在性提供了新的见解。

Abstract: In this paper, we study the two-time Leggett-Garg (LG) inequalities of a quantum optical model that appears in the Josephson-junction quantum bit (qubit) interacting with an external magnetic flux. This model is a natural extension of an exactly solvable model whose interaction between a qubit and single-mode photons is given by a product of the Pauli $z$ operator of the qubit and a linear combination of annihilation and creation operators of the photons. By contrast, a photon's part of the interaction of our model is given by the square of the linear combination. Because our model is not solvable, we approximately investigate its time evolution up to the second-order perturbation. Our numerical calculations show that violation of the LG inequality diminishes as the temperature increases. Moreover, it exhibits power laws of the temperature, whose exponents vary depending on the coupling constant of the interaction between the qubit and photons. The violation of the LG inequality decreases and becomes less sensitive to the temperature as the coupling constant of the interaction gets larger.

</details>


### [78] [Faithful Simulation of Broadcast Measurements](https://arxiv.org/abs/2601.10947)
*Anders Høst-Madsen*

Main category: quant-ph

TL;DR: 论文研究三方量子测量模拟中的通信效率问题，中心服务器Charlie进行POVM测量，Alice和Bob只需模拟各自的部分结果，探索所需通信量的可达区域。


<details>
  <summary>Details</summary>
Motivation: 研究在量子测量模拟场景中，当三方共享共同随机性时，Alice和Bob为忠实模拟各自测量结果所需的最小通信量，旨在优化量子通信协议的效率。

Method: 建立三方量子测量模拟框架：Charlie对量子系统C执行POVM测量，Alice和Bob分别关注部分结果g_A(x)和g_B(x)。利用共享共同随机性，分析Alice和Bob为模拟测量所需通信量的可达区域。

Result: 论文推导出了Alice和Bob所需通信量的可达区域，为三方量子测量模拟提供了通信效率的理论界限。

Conclusion: 该研究为量子测量模拟中的通信优化提供了理论框架，确定了Alice和Bob在共享共同随机性条件下忠实模拟测量所需通信量的可达区域。

Abstract: In this paper a central server Charlie has access to a quantum system C and measures it with a POVM $\{Λ_x\}$. Alice and Bob are only interested in the partial results $g_A(x)$ respectively $g_B(x)$. Alice, Bob, and Charlie share common randomness and Alice and Bob only need to faithfully simulate their measurements. The paper develops to achievable regions for the amount of communication needed to Alice and Bob.

</details>


### [79] [Stabilizer Code-Generic Universal Fault-Tolerant Quantum Computation](https://arxiv.org/abs/2601.10964)
*Nicholas J. C. Papadopoulos,Ramin Ayanzadeh*

Main category: quant-ph

TL;DR: 该论文提出了一种通用的、确定性的逻辑门实现方法，使任何稳定子码都能实现通用量子计算，无需修改底层代码或消耗辅助寄存器。


<details>
  <summary>Details</summary>
Motivation: 当前量子纠错码单独无法实现通用量子计算，传统方法如代码级联、代码切换或魔术态蒸馏成本高且仅适用于特定代码，需要更通用的解决方案。

Method: 通过新颖的辅助寄存器介导协议实现逻辑Clifford和T门，利用辅助寄存器中的辅助代码和中间电路测量，不修改底层数据代码或寄存器。

Result: 实现了通用容错量子门集，该方法具有确定性、不消耗辅助寄存器、不修改底层代码、适用于所有稳定子码的特性。

Conclusion: 任何单个代码都能通过该方法实现通用量子计算，同时支持异构稳定子码之间的通信，为现有和未来代码的规模化、异构共存开辟了新可能。

Abstract: Fault-tolerant quantum computation allows quantum computations to be carried out while resisting unwanted noise. Several error correcting codes have been developed to achieve this task, but none alone are capable of universal quantum computation. This universality is highly desired and often achieved using additional techniques such as code concatenation, code switching, or magic state distillation, which can be costly and only work for specific codes. This work implements logical Clifford and T gates through novel ancilla-mediated protocols to construct a universal fault-tolerant quantum gate set. Unlike traditional techniques, our implementation is deterministic, does not consume ancilla registers, does not modify the underlying data codes or registers, and is generic over all stabilizer codes. Thus, any single code becomes capable of universal quantum computation by leveraging helper codes in ancilla registers and mid-circuit measurements. Furthermore, since these logical gates are stabilizer code-generic, these implementations enable communication between heterogeneous stabilizer codes. These features collectively open the door to countless possibilities for existing and undiscovered codes as well as their scalable, heterogeneous coexistence.

</details>


### [80] [Noise-Aware Quantum Architecture Search Based on NSGA-II Algorithm](https://arxiv.org/abs/2601.10965)
*Chenlu Li,Hui Zeng,Dazhi Ding*

Main category: quant-ph

TL;DR: 提出了一种噪声感知量子架构搜索框架，通过将噪声模型融入参数化量子电路训练，在噪声条件下自动设计高性能量子电路


<details>
  <summary>Details</summary>
Motivation: 量子架构搜索旨在自动化设计特定任务和硬件约束下的高性能量子电路，但现有方法在噪声条件下的鲁棒性不足，需要开发能够识别噪声鲁棒架构的搜索框架

Method: 提出噪声感知量子架构搜索框架，将噪声模型融入参数化量子电路训练；采用混合哈密顿量ε-greedy策略优化评估成本并避免局部最优；使用增强型可变深度NSGA-II算法在广阔搜索空间中导航，实现架构表达性与量子硬件开销的自动权衡

Result: 在噪声条件下的二元分类和鸢尾花多分类任务中验证了框架有效性，相比现有方法，能够在噪声条件下搜索到性能更优、资源效率更高的量子架构

Conclusion: 提出的噪声感知量子架构搜索框架能够有效自动化设计噪声鲁棒的量子电路，在保持性能的同时提高资源效率，为实际量子计算应用提供了实用解决方案

Abstract: Quantum architecture search (QAS) has emerged to automate the design of high-performance quantum circuits under specific tasks and hardware constraints. We propose a noise-aware quantum architecture search (NA-QAS) framework based on variational quantum circuit design. By incorporating a noise model into the training of parameterized quantum circuits (PQCs) , the proposed framework identifies the noise-robust architectures. We introduce a hybrid Hamiltonian $\varepsilon$ -greedy strategy to optimize evaluation costs and circumvent local optima. Furthermore, an enhanced variable-depth NSGA-II algorithm is employed to navigate the vast search space, enabling an automated trade-off between architectural expressibility and quantum hardware overhead. The effectiveness of the framework is validated through binary classification and iris multi-classification tasks under a noisy condition. Compared to existing approaches, our framework can search for quantum architectures with superior performance and greater resource efficiency under a noisy condition.

</details>


### [81] [Off-resonant preservation and generation of imaginarity in distributed scenarios](https://arxiv.org/abs/2601.10979)
*Si-Min Wang,Ming-Liang Hu,Heng Fan*

Main category: quant-ph

TL;DR: 研究量子虚性在分布式场景中的非局域优势（NAQI）和可蒸馏虚性辅助（DIA），发现在大对称失谐条件下，这些虚性资源能在耗散腔中长时间保持，且非共振相互作用能从初始乘积态产生高NAQI和DIA。


<details>
  <summary>Details</summary>
Motivation: 探索虚性作为量子资源在分布式场景中的应用，研究如何在耗散环境中有效控制和保持虚性资源，特别是通过非共振相互作用来增强虚性的非局域优势。

Method: 研究两个量子比特与耗散腔的相互作用，分析在量子比特与腔之间存在大对称失谐条件下的NAQI和DIA行为。通过分析腔模诱导的有效耦合来解释物理机制。

Result: 发现当量子比特与腔之间存在大对称失谐时，NAQI和DIA都能长时间保持。非共振相互作用能从具有相同失谐但不等耦合的初始乘积态产生高NAQI和DIA。

Conclusion: 非共振相互作用在分布式场景中有效控制虚性方面发挥重要作用，为量子虚性资源的实际应用提供了新策略。

Abstract: We study the nonlocal advantage of quantum imaginarity (NAQI) and distillable imaginarity of assistance (DIA), which treat imaginarity as a resource in distributed scenarios. For two qubits interacting with a lossy cavity, it is shown that both the NAQI and DIA can be well preserved for long times in the presence of large and symmetric detuning between the qubits and the cavity. Moreover, the off-resonant interaction generates a high degree of NAQI and DIA from the initial product states of two qubits having the same detunings and unequal couplings to the cavity. Based on the effective coupling of the qubits induced by the cavity mode, we explain the physical mechanism underlying the validity of this strategy. Our findings shed light on the role that off-resonant interactions have in the efficient control of imaginarity in distributed scenarios.

</details>


### [82] [Certifying entanglement dimensionality by random Pauli sampling](https://arxiv.org/abs/2601.11040)
*Changhao Yi*

Main category: quant-ph

TL;DR: 提出基于泡利测量的算法，用于认证n量子比特纯态的施密特数，平均样本复杂度为多项式级别，相比最坏情况指数级复杂度有显著改进。


<details>
  <summary>Details</summary>
Motivation: 高维纠缠认证通常需要指数级样本复杂度，限制了实际应用。需要开发可扩展的方法来高效认证量子态的施密特数。

Method: 使用基于泡利测量的算法，结合局部伪随机幺正变换，将最坏情况转化为平均情况，实现多项式样本复杂度。

Result: 算法平均样本复杂度为O(poly(n)χ²)，相比最坏情况O(2ⁿχ)有显著改进，为高维纠缠认证提供了可扩展方法。

Conclusion: 该工作建立了可扩展的高维纠缠认证方法，并引入了随机泡利采样的证明框架，为量子信息处理提供了实用工具。

Abstract: We introduce a Pauli-measurement-based algorithm to certify the Schmidt number of $n$-qubit pure states. Our protocol achieves an average-case sample complexity of $\caO(\mathrm{poly}(n)χ^2)$, a substantial improvement over the $\caO(2^n χ)$ worst-case bound. By utilizing local pseudorandom unitaries, we ensure the worst case can be transformed into the average-case with high probability. This work establishes a scalable approach to high-dimensional entanglement certification and introduces a proof framework for random Pauli sampling.

</details>


### [83] [Converting qubit relaxation into erasures with a single fluxonium](https://arxiv.org/abs/2601.11086)
*Chenlu Liu,Yulong Li,Jiahui Wang,Quan Guan,Lijing Jin,Lu Ma,Ruizi Hu,Tenghui Wang,Xing Zhu,Hai-Feng Yu,Chunqing Deng,Xizheng Ma*

Main category: quant-ph

TL;DR: 在零磁通条件下操作的单fluxonium量子比特中实现了擦除错误转换，通过精心设计的谐振器同时提供中间电路擦除检测和终端逻辑测量，将逻辑寿命从193μs提升到869μs


<details>
  <summary>Details</summary>
Motivation: 传统双轨编码的擦除量子比特虽然能实现高保真度操作，但需要额外的硬件开销和电路复杂度。本研究旨在解决这些限制，在单个fluxonium量子比特中实现擦除转换，减少资源需求

Method: 在零磁通条件下操作单个fluxonium量子比特，将逻辑状态编码在0-2子空间中。使用单个精心设计的谐振器同时提供中间电路擦除检测和终端逻辑测量。通过后选择非擦除结果来提升逻辑寿命

Result: 逻辑寿命从193μs显著提升到869μs，增加了超过四倍。每个擦除检查仅贡献7.2×10⁻⁵的微小误差。测量引起的逻辑退相干随测量功率和频率变化得到表征

Conclusion: 整数fluxonium是一种有前景、资源高效的擦除错误缓解平台，无需额外硬件，为容错量子计算提供了更简洁的解决方案

Abstract: Qubits that experience predominantly erasure errors offer distinct advantages for fault-tolerant operation. Indeed, dual-rail encoded erasure qubits in superconducting cavities and transmons have demonstrated high-fidelity operations by converting physical-qubit relaxation into logical-qubit erasures, but this comes at the cost of increased hardware overhead and circuit complexity. Here, we address these limitations by realizing erasure conversion in a single fluxonium operated at zero flux, where the logical state is encoded in its 0-2 subspace. A single, carefully engineered resonator provides both mid-circuit erasure detection and end-of-line (EOL) logical measurement. Post-selection on non-erasure outcomes results in more than four-fold increase of the logical lifetime, from $193~μ$s to $869~μ$s. Finally, we characterize measurement-induced logical dephasing as a function of measurement power and frequency, and infer that each erasure check contributes a negligible error of $7.2\times 10^{-5}$. These results establish integer-fluxonium as a promising, resource-efficient platform for erasure-based error mitigation, without requiring additional hardware.

</details>


### [84] [UAV-Deployed OAM-BB84 QKD: Turbulence- and Misalignment-Resilient Decoy-State Finite-Key Security with AI-Assisted Calibration](https://arxiv.org/abs/2601.11117)
*Linxier Deng*

Main category: quant-ph

TL;DR: 该论文提出了在无人机平台上使用轨道角动量编码BB84协议的量子密钥分发理论框架，通过物理信息学习模块提升密钥率10-30%，同时保持可组合安全性。


<details>
  <summary>Details</summary>
Motivation: 随着量子通信技术的发展，需要探索在移动平台（如无人机）上实现安全量子密钥分发的可能性。传统地面量子通信受限于固定基础设施，而无人机平台能够提供灵活部署和移动通信能力，但面临大气湍流、指向误差等动态信道挑战。

Method: 1. 建立统一信道模型，包含Kolmogorov湍流、指向误差和有限孔径截断效应；2. 采用弱真空诱骗态方案，推导包含统计波动、探测器暗计数、效率失配和纠错泄漏的可组合有限密钥率下界；3. 引入轻量级物理信息学习模块，结合物理先验和测量链路统计，分类有效脉冲、拒绝损坏数据并推荐解码策略。

Result: 在中等湍流和毫弧度级指向抖动条件下，模拟显示所提出的AI辅助方法可将秘密密钥率提高10%至30%，同时保持可组合安全性。论文还提供了完整的评估流程，包括无人机系统架构、湍流驱动的QBER映射、诱骗态优化、有限密钥缩放和AI校准指标。

Conclusion: 该研究成功建立了无人机平台上轨道角动量编码量子密钥分发的理论框架，通过物理信息学习模块有效应对动态飞行条件下的信道变化，显著提升了系统性能。该方法为移动量子通信系统的实际部署提供了理论基础和技术路径。

Abstract: We present a theoretical framework for quantum key distribution (QKD) using orbital angular momentum (OAM) encoded BB84 on an unmanned aerial vehicle (UAV) platform. A unified channel model captures Kolmogorov turbulence, pointing induced misalignment, and finite aperture clipping, enabling quantitative predictions of inter mode crosstalk and the resulting quantum bit error rate (QBER). Using a weak plus vacuum decoy state formulation, we derive composable finite key lower bounds on the secret key rate that incorporate statistical fluctuations, detector dark counts, efficiency mismatch, and error correction leakage. To stabilize performance under non stationary flight conditions, we introduce a lightweight physics informed learning module that combines physical priors with measured link statistics to classify valid pulses, reject corrupted data, and recommend decoding strategies. We outline a complete evaluation pipeline including UAV system architecture, turbulence driven QBER maps, decoy optimization, finite key scaling, and AI calibration metrics. Simulations indicate that under moderate turbulence and milliradian level pointing jitter, the proposed AI assisted method can improve the secret key rate by 10 percent to 30 percent while preserving composable security.

</details>


### [85] [From three-body resonances to bound states in a continuum: pole trajectories](https://arxiv.org/abs/2601.11188)
*Lucas Happ*

Main category: quant-ph

TL;DR: 研究三维连续谱中束缚态的形成机制，通过追踪复能面极点轨迹，发现参数变化可导致至少一个三维BIC形成，且形成机制对运动学结构比对两体相互作用细节更敏感。


<details>
  <summary>Details</summary>
Motivation: 研究三维连续谱中束缚态的形成机制，探索系统参数变化如何影响BIC的形成，理解BIC形成的物理本质。

Method: 使用一维模型，包含两个全同玻色子和一个可区分粒子，通过高斯势相互作用。系统改变相互作用强度、相互作用范围和质比，追踪复能面极点轨迹。

Result: 证实了BIC的参数性质，发现相互作用参数和质比变化都能导致至少一个三维BIC形成。质比变化产生更规则的模式和多个BIC位置，轨迹形状各异。

Conclusion: 三维BIC形成机制对问题的运动学结构比对两体相互作用的具体细节更敏感。

Abstract: We investigate the formation of three-body bound states in the continuum by tracing pole trajectories in the complex energy plane under variation of system parameters. Using a one-dimensional model of two identical bosons and a distinguishable particle interacting via Gaussian potentials, we systematically vary the interaction strength, interaction range, and mass ratio. Our results confirm the parametric nature of few-body bound states in a continuum (BIC) and extend this characterization to a broader set of system parameters. Specifically, we find that variations of both interaction parameters and the mass ratio can lead to the formation of at least one three-body BIC. However, the exact shape of trajectories differs, and for the mass ratio variation we find a more regular pattern with multiple BIC locations. These results suggest that the mechanism of few-body BIC formation is more sensitive to the kinematic structure of the problem than to the specific details of the two-body interaction.

</details>


### [86] [Concatenated continuous driving of silicon qubit by amplitude and phase modulation](https://arxiv.org/abs/2601.11245)
*Takuma Kuno,Takeru Utsugi,Andrew J. Ramsay,Normann Mertig,Noriyuki Lee,Itaru Yanagi,Toshiyuki Mine,Nobuhiro Kusuno,Hideo Arimoto,Sofie Beyne,Julien Jussot,Stefan Kubicek,Yann Canvel,Clement Godfrin,Bart Raes,Yosuke Shimura,Roger Loo,Sylvain Baudot,Danny Wan,Kristiaan De Greve,Shinichi Saito,Digh Hisamoto,Ryuta Tsuchiya,Tetsuo Kodera,Hiroyuki Mizuno*

Main category: quant-ph

TL;DR: 提出一种新型的级联连续驱动方案，通过同时调制驱动场的幅度和相位，在载波频率旋转坐标系中产生圆偏振场，消除第二旋转坐标系中的反向旋转项，从而提高量子比特门保真度。


<details>
  <summary>Details</summary>
Motivation: 传统的级联连续驱动（CCD）方案通过连续驱动抑制噪声并操控缀饰态，但在快速门操作中，由于旋转波近似不完美会产生系统性的脉冲面积误差。需要一种新方案来消除这种误差，提高量子门操作的保真度。

Method: 提出圆调制级联连续驱动（CM-CCD）方案，同时调制驱动场的幅度和相位，在载波频率旋转坐标系中产生圆偏振场。这种方法在第二旋转坐标系中消除了反向旋转项，从而避免了由于旋转波近似不完美导致的脉冲面积误差。

Result: 数值模拟显示CM-CCD比传统CCD方案获得更高的门保真度。在硅量子点电子自旋量子比特实验中，CM-CCD对静态失谐和Rabi频率误差表现出显著增强的鲁棒性，相比标准Rabi驱动有显著改进。

Conclusion: CM-CCD方案有效提高了量子门操作的保真度和鲁棒性，特别适用于存在量子比特频率变化、驱动耦合变化和低频噪声的量子比特阵列。该方案可广泛应用于多种物理系统，包括囚禁原子、冷原子、超导量子比特和NV色心等。

Abstract: The rate of coherence loss is lower for a qubit under Rabi drive compared to a freely evolving qubit, $T_{2}^{\rm{Rabi}}>T_{2}^*$.
  Building on this principle, concatenated continuous driving (CCD) keeps the qubit under continuous drive to suppress noise and manipulate dressed states by either phase or amplitude modulation.
  In this work, we propose a new variant of CCD which simultaneously modulates both the amplitude and phase of the driving field to generate a circularly-polarized field in the rotating frame of the carrier frequency.
  This circular-modulated (CM)-CCD cancels the counter-rotating term in the second rotating frame, eliminating a systematic pulse-area error that arises from an imperfect rotating wave approximation for fast gates.
  Numerical simulations demonstrate that the proposed CMCCD achieves higher gate fidelity than conventional CCD schemes.
  We further implement and compare different CCD protocols using an electron spin-qubit in an isotopically purified $^{28}$Si-MOS quantum dot and evaluate its robustness by applying static detuning and Rabi frequency errors.
  The robustness is significantly improved compared to standard Rabi-drive, showing the effectiveness of this scheme for qubit arrays with variation in qubit frequency, coupling to Rabi drive, and low frequency noise.
  The proposed scheme can be applied to various physical systems, including trapped atoms, cold atoms, superconducting qubits, and NV-centers.

</details>


### [87] [Shortcuts to adiabaticity, unexciting backgrounds, and reflectionless potentials](https://arxiv.org/abs/2601.11256)
*Fernando C. Lombardo,Francisco D. Mazzitelli*

Main category: quant-ph

TL;DR: 该论文分析了量子谐振子和量子场论中的绝热捷径及其完成，利用散射理论中的Bogoliubov系数对应关系，将STA协议与传输共振和反射less势联系起来。


<details>
  <summary>Details</summary>
Motivation: 研究量子系统在非平稳背景下的绝热捷径及其完成机制，探索量子谐振子和量子场论中STA的物理对应关系，为量子控制和量子信息处理提供理论基础。

Method: 利用一维量子力学类比和Bogoliubov系数与散射理论中传输/反射振幅的对应关系，将QHO中的STA协议映射到传输共振，将QFT中的STA映射到反射less势，并通过压缩态和粒子产生的关系分析STA完成机制。

Result: 建立了QHO中STA协议与传输共振的等价关系，QFT中STA与反射less势的对应关系，并证明STA完成可以通过反压缩算符来理解，为量子系统控制提供了新的理论框架。

Conclusion: 通过散射理论框架，成功将量子谐振子和量子场论中的绝热捷径统一描述，揭示了STA的物理本质及其与压缩态操作的深层联系，为量子技术应用提供了重要理论支持。

Abstract: We analyze shortcuts to adiabaticity (STA) and their completions for the quantum harmonic oscillator (QHO) with time-dependent frequency, as well as for quantum field theory (QFT) in non-stationary backgrounds. We exploit the analogy with one-dimensional quantum mechanics, and the well known correspondence between Bogoliubov coefficients in the QHO and transmission/reflection amplitudes in scattering theory. Within this framework, STA protocols for the QHO are equivalent to transmission resonances, while STA in QFT with homogeneous backgrounds correspond to reflectionless potentials. Moreover, using the connection between particle creation and squeezed states, we show how STA completions can be understood in terms of the anti-squeezing operator.

</details>


### [88] [Controlled Parity of Cooper Pair Tunneling in a Hybrid Superconducting Qubit](https://arxiv.org/abs/2601.11303)
*David Feldstein-Bofill,Leo Uhre Jacobsen,Ksenia Shagalov,Zhenhai Sun,Casper Wied,Shikhar Singh,Anders Kringhøj,Jacob Hastrup,András Gyenis,Karsten Flensberg,Svend Krøjer,Morten Kjaergaard*

Main category: quant-ph

TL;DR: 研究人员开发了一种"谐波奇偶量子比特"(HPQ)，通过将两个铝氧化物隧道结与门可调InAs/Al纳米线结并联形成SQUID，实现了对约瑟夫森势能中奇偶谐波分量的控制，特别在半个磁通量子点能够将奇次谐波抑制两个数量级。


<details>
  <summary>Details</summary>
Motivation: 约瑟夫森结的能量-相位关系除了基本的cosφ项外，还包含高阶傅里叶谐波cos(kφ)，对应k个库珀对的关联隧穿。奇偶主导隧穿过程会产生本质上不同的性质，控制这一特性在超导技术中有广泛应用。然而，实现偶数主导的体系一直很困难，通常需要复杂的多结或全混合结构。

Method: 研究人员设计了一种简单的"谐波奇偶量子比特"(HPQ)，将两个铝氧化物隧道结与门可调的InAs/Al纳米线结并联形成SQUID结构。通过测量85个门电压点的通量谱，重建了其能量-相位关系。

Result: 在半个磁通量子点，约瑟夫森势能中的奇次谐波相对于偶次谐波可被抑制达两个数量级，产生以偶次谐波为主的双势阱，最小值位于±π/2附近。这种控制谐波奇偶性的能力实现了由库珀对对的超电流传输。

Conclusion: 谐波奇偶量子比特提供了一种简单的方法来控制约瑟夫森势能中的谐波奇偶性，为超导电路中的傅里叶工程提供了新的构建模块，能够实现由库珀对对的超电流传输。

Abstract: Superconducting quantum circuits derive their nonlinearity from the Josephson energy-phase relation. Besides the fundamental $\cosφ$ term, this relation can also contain higher Fourier harmonics $\cos(kφ)$ corresponding to correlated tunneling of $k$ Cooper pairs. The parity of the dominant tunneling process, i.e.~whether an odd or even number of Cooper pairs tunnel, results in qualitatively different properties, and controlling this opens up a wide range of applications in superconducting technology. However, access to even-dominated regimes has remained challenging and has so far relied on complex multi-junction or all-hybrid architectures. Here, we demonstrate a simple "harmonic parity qubit" (HPQ); an element that combines two aluminum-oxide tunnel junctions in parallel to a gate-tunable InAs/Al nanowire junction forming a SQUID, and use spectroscopy versus flux to reconstruct its energy-phase relation at 85 gate voltage points. At half flux quantum, the odd harmonics of the Josephson potential can be suppressed by up to two orders of magnitude relative to the even harmonics, producing a double-well potential dominated by even harmonics with minima near $\pmπ/2$. The ability to control harmonic parity enables supercurrent carried by pairs of Cooper pairs and provides a new building block for Fourier engineering in superconducting circuits.

</details>


### [89] [Dressed-state relaxation in coupled qubits as a source of two-qubit gate errors](https://arxiv.org/abs/2601.11316)
*Ruixia Wang,Jiayu Ding,Chenlu Wang,Yujia Zhang,He Wang,Wuerkaixi Nuerbolati,Zhen Yang,Xuehui Liang,Weijie Sun,Haifeng Yu,Fei Yan*

Main category: quant-ph

TL;DR: 该论文研究了双量子比特门操作中的误差机制，发现与耦合强度匹配的噪声频率会诱导独特的弛豫通道，从而降低门性能。


<details>
  <summary>Details</summary>
Motivation: 理解双量子比特门操作中的误差机制对于构建高保真度量子处理器至关重要。先前研究主要将退相噪声视为马尔可夫或低频噪声，但实际量子比特环境表现出结构化、频率相关的频谱特性。

Method: 通过理论分析和在具有工程化噪声谱的超导量子比特上的实验验证，研究噪声在频率匹配于裸态能量分裂（由量子比特间耦合强度g设定）时的影响。

Result: 双量子比特门误差与频率2g处的噪声功率谱密度呈可预测的比例关系，将T1ρ弛豫概念扩展到相互作用系统，这种频率选择性弛豫机制在多个平台上具有普适性。

Conclusion: 该频率选择性弛豫机制丰富了我们对门操作期间退相干路径的理解，并为双轨或单重态-三重态编码设置了相干性限制，这一机制在多个量子平台上具有普适性。

Abstract: Understanding error mechanisms in two-qubit gate operations is essential for building high-fidelity quantum processors. While prior studies predominantly treat dephasing noise as either Markovian or predominantly low-frequency, realistic qubit environments exhibit structured, frequency-dependent spectra. Here we demonstrate that noise at frequencies matching the dressed-state energy splitting--set by the inter-qubit coupling strength g--induces a distinct relaxation channel that degrades gate performance. Through combined theoretical analysis and experimental verification on superconducting qubits with engineered noise spectra, we show that two-qubit gate errors scale predictably with the noise power spectral density at frequency 2g, extending the concept of $T_{1ρ}$ relaxation to interacting systems. This frequency-selective relaxation mechanism, universal across platforms, enriches our understanding of decoherence pathways during gate operations. The same mechanism sets coherence limits for dual-rail or singlet-triplet encodings.

</details>


### [90] [Skyrmion Quantum Diode Prototype: Bridging Micromagnetic Simulations and Quantum Models](https://arxiv.org/abs/2601.11341)
*Haowen Yang,Gerald Bissell,Han Zhong,Peter Van Kirk,Tiger Cao,Pengcheng Lu,Yingying Wu*

Main category: quant-ph

TL;DR: 该研究提出了一种基于斯格明子量子比特的新型器件——斯格明子量子二极管，通过结合经典微磁模拟和量子电路模型，实现了从20nm到3nm尺度的单向斯格明子传输，为混合量子平台提供了可行性验证。


<details>
  <summary>Details</summary>
Motivation: 斯格明子因其拓扑保护特性而具有抗扰动鲁棒性，是理想的信息载体。研究旨在解决量子计算中的关键挑战：在不同类型量子比特之间建立可靠的单向连接，实现低耗散、单向的量子信息传输。

Method: 采用概念验证研究方法，结合经典微磁模拟（实现小至3nm的斯格明子直径）和受超导量子比特启发的量子电路模型。通过不对称结中的斯格明子霍尔效应实现单向传输。

Result: 展示了：(1) 在20nm到3nm尺度范围内的单向斯格明子传输；(2) 与通量可调量子架构的潜在兼容性；(3) 斯格明子基量子比特系统中的非谐性初步见解。验证了混合斯格明子-量子平台的操作可行性和缩放行为。

Conclusion: 该工作为将斯格明子基量子组件集成到实际器件架构中铺平了道路，实现了低耗散、单向量子信息传输。这对于可扩展量子计算、自旋电子逻辑和混合量子系统至关重要，并为芯片级无泵隔离器和定向量子链路开辟了机会。

Abstract: Magnetic skyrmions are topologically protected spin textures known for their robustness against perturbations. Their topological stability makes them robust information carriers, ideal for tackling a key challenge in quantum computing: creating reliable, one-way links between different types of qubits. In this proof-of-concept study, we introduce a novel device - the skyrmion quantum diode - based on skyrmion qubits. Our approach combines classical micromagnetic simulations, achieving skyrmion diameters as small as 3 nm, with quantum circuit models inspired by superconducting qubits. In this work, we demonstrate: (i) unidirectional skyrmion transport via the skyrmion Hall effect in asymmetric junctions, spanning length scales from 20 nm down to 3 nm; (ii) potential compatibility with flux-tunable quantum architectures; and (iii) preliminary insights into anharmonicity in skyrmion-based qubit systems. These results establish both the operational feasibility and the scaling behavior necessary for a hybrid skyrmion-quantum platform. Our work outlines a path toward integrating skyrmion based quantum components into practical device architectures, enabling low-dissipation, unidirectional quantum information transport. This capability is crucial for scalable quantum computing, spintronic logic, and hybrid quantum systems, and opens opportunities for chipscale, pump-free isolators and directional quantum links that enhance readout fidelity, reduce cryogenic load, and support modular skyrmion-superconducting processors

</details>


### [91] [No quantum solutions to linear constraint systems from monomial measurement-based quantum computation in odd prime dimension](https://arxiv.org/abs/2601.11367)
*Markus Frembs,Cihan Okay,Ho Yiu Chung*

Main category: quant-ph

TL;DR: 该研究探讨了基于测量的量子计算中的上下文性与线性约束系统量子解之间的关系，发现对于一大类MBQC，其测量算符不会产生状态无关的上下文性，从而区分了状态依赖和状态无关的上下文性形式。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于比较基于测量的量子计算中上下文性的两种视角：状态依赖的上下文性作为量子优势的关键资源，以及测量算符满足经典不可满足线性约束所体现的更强形式的上下文性。研究者希望探索这两种上下文性形式之间的关联程度。

Method: 方法是将某些表现出强形式状态依赖上下文性的MBQC与线性约束系统关联起来，然后询问这些MBQC中的测量算符是否会产生其关联LCS的量子解形式的状态无关上下文性。

Result: 主要结果排除了对于一大类MBQC存在这样的量子解。这一发现既区分了状态依赖和状态无关的上下文性形式，又推广了在有限奇数（素数）维度下线性约束系统不存在量子解的结果。

Conclusion: 研究结论表明，对于一大类基于测量的量子计算，其测量算符不会产生状态无关的上下文性，这进一步明确了状态依赖和状态无关上下文性之间的区别，并扩展了关于线性约束系统量子解不存在性的理论结果。

Abstract: We combine the study of resources in measurement-based quantum computation (MBQC) with that of quantum solutions to linear constraint systems (LCS). Contextuality of the input state in MBQC has been identified as a key resource for quantum advantage, and in a stronger form, underlies algebraic relations between (measurement) operators which obey classically unsatisfiable (linear) constraints. Here, we compare these two perspectives on contextuality, and study to what extent they are related. More precisely, we associate a LCS to certain MBQC which exhibit strong forms of state-dependent contextuality, and ask if the measurement operators in such MBQC give rise to state-independent contextuality in the form of quantum solutions of its associated LCS. Our main result rules out such quantum solutions for a large class of MBQC. This both sharpens the distinction between state-dependent and state-independent forms of contextuality, and further generalises results on the non-existence of quantum solutions to LCS in finite odd (prime) dimension.

</details>


### [92] [Quantum-enhanced optimization for patient stratification in clinical trials](https://arxiv.org/abs/2601.11413)
*Laia Domingo,Christine Johnson*

Main category: quant-ph

TL;DR: 量子增强的患者分层优化方法，通过最小化协变量不平衡，显著提升临床试验统计功效和计算效率


<details>
  <summary>Details</summary>
Motivation: 临床试验失败率高、成本昂贵，主要原因是生物不确定性导致患者分层不当，造成治疗组间协变量不平衡，掩盖真实治疗效果，降低统计功效

Method: 提出基于优化的量子增强患者分层方法，在不改变试验方案或终点的情况下，最小化数值和分类变量的协变量不平衡，采用混合量子-经典优化方法

Result: 量子增强方法比经典方法计算效率提升100倍以上，在真实临床试验数据中实现高质量分层，使治疗效果估计的统计显著性提升高达5倍

Conclusion: 优化驱动的患者分层能加强临床试验设计，提高下游决策信心，降低后期失败风险，量子计算为此类优化问题提供了可扩展的解决方案

Abstract: Clinical trials are notorious for their high failure rates and steep costs, leading to wasted time and resources spend, prolonged development timelines, and delayed patient access to new therapies. A key contributor to these failures is biological uncertainty, which complicates trial design and weakens the ability to detect true treatment effects. In particular, inadequate patient stratification often results in covariate imbalances across treatment arms, masking treatment effects and reducing statistical power, even when therapies are effective for specific patient subpopulations. This work presents an optimization-based, quantum-enhanced approach to patient stratification that explicitly minimizes covariate imbalance across numerical and categorical variables, without altering protocol design or trial endpoints. Using real clinical trial data, we demonstrate that hybrid quantum-classical optimization methods achieve high-quality stratification while scaling efficiently to larger cohorts. In our benchmark study, the quantum-enhanced pipeline delivered over a 100x improvement in computational efficiency compared to classical approaches, enabling faster iteration and practical deployment at scale. This report shows how improved stratification can lead to decision-relevant gains, including up to a fivefold increase in statistical significance in treatment effect estimation, reducing treatment-effect dilution and increasing trial sensitivity. Together, these results show that optimization-driven stratification can strengthen clinical trial design, improve confidence in downstream decisions, and reduce the risk of costly late-stage failure.

</details>


### [93] [Simulating Quantum Walk Hamiltonians without Pauli Decomposition](https://arxiv.org/abs/2601.11418)
*Mostafa Atallah,Alvin Gonzales,Daniel Dilley,Igor Gaidai,Zain H. Saleem,Rebekah Herrman*

Main category: quant-ph

TL;DR: 提出一种名为"匹配分解"的新算法，用于在任意简单稀疏图上高效生成连续时间量子行走的量子电路，相比传统Pauli分解方法可显著减少控制门数量和电路深度。


<details>
  <summary>Details</summary>
Motivation: 传统基于Pauli分解的量子行走模拟方法资源消耗较大，需要开发更高效的量子电路生成算法来减少门操作数量和电路深度，提高量子行走模拟的效率。

Method: 1) 将连续时间量子行走哈密顿量分解为对应图中匹配的可精确实现哈密顿量集合；2) 使用新颖的图压缩算法合并图中的边；3) 将行走转换为电路，并对这些组件进行Trotter化；4) 在匹配中的每条边上，行走动力学通过CX和CRx门序列实现，不使用Pauli分解。

Result: 与标准Pauli分解模拟流程相比，匹配分解算法在不同图家族中一致实现显著资源减少：最多减少43%的控制门和54%的电路深度。同时展示了匹配分解能够精确模拟图上连续时间量子行走的实例和理论结果。

Conclusion: 匹配分解算法为量子行走的高效量子电路实现提供了一种有前景的方法，相比传统Pauli分解方法在资源效率方面具有显著优势，特别是在稀疏图上表现突出。

Abstract: In this work, we present a new algorithm for generating quantum circuits that efficiently implement continuous time quantum walks on arbitrary simple sparse graphs. The algorithm, called matching decomposition, works by decomposing a continuous-time quantum walk Hamiltonian into a collection of exactly implementable Hamiltonians corresponding to matchings in the underlying graph followed by a novel graph compression algorithm that merges edges in the graph. Lastly, we convert the walks to a circuit and Trotterize over these components. The dynamics of the walker on each edge in the matching can be implemented in the circuit model as sequences of CX and CRx gates. We do not use Pauli decomposition when implementing walks along each matching. Furthermore, we compare matching decomposition to a standard Pauli-based simulation pipeline and find that matching decomposition consistently yields substantial resource reductions, requiring up to 43% fewer controlled gates and up to 54% shallower circuits than Pauli decomposition across multiple graph families. Finally, we also present examples and theoretical results for when matching decomposition can exactly simulate a continuous-time quantum walk on a graph.

</details>


### [94] [Coupling free electrons to a trapped-ion quantum computer](https://arxiv.org/abs/2601.11446)
*Elias Pescoller,Santiago Beltrán-Romero,Sebastian Egginger,Nicolas Jungwirth,Martino Zanetti,Dominik Hornof,Michael S. Seifner,Iva Březinová,Philipp Haslinger,Thomas Juffmann,Johannes Kofler,Philipp Schindler,Dennis Rätzel*

Main category: quant-ph

TL;DR: 提出了一种将电子显微镜中的自由电子与囚禁离子量子处理器相干耦合的方案，实现非破坏性、量子相干的探测，支持量子增强的剂量高效电子显微镜应用。


<details>
  <summary>Details</summary>
Motivation: 自由电子可以作为量子探针与其他量子系统相干关联，为先进计量学提供资源。目前需要建立能够实现自由电子与量子处理器相干耦合的平台，以实现非破坏性、量子相干的探测和信息积累。

Method: 提出了一种将电子显微镜中的自由电子与囚禁离子量子处理器相干耦合的实验装置。该方案通过相干耦合机制，使自由电子能够非破坏性地与量子比特相互作用，实现量子相干探测。

Result: 分析表明，单个电子可以诱导可分辨的量子比特激发，这为量子增强的剂量高效电子显微镜等实际应用建立了平台基础。

Conclusion: 该方案成功建立了自由电子与囚禁离子量子处理器之间的相干耦合平台，为实现量子增强的电子显微镜等先进计量应用提供了可行的技术路径。

Abstract: Freely propagating electrons may serve as quantum probes that can become coherently correlated with other quantum systems, offering access to advanced metrological resources. We propose a setup that coherently couples free electrons in an electron microscope to a trapped-ion quantum processor, enabling non-destructive, quantum-coherent detection and the accumulation of information across multiple electrons. Our analysis shows that single electrons can induce resolvable qubit excitations, establishing a platform for practical applications such as quantum-enhanced, dose-efficient electron microscopy.

</details>


### [95] [Heat, work, and fluctuations in a driven quantum resonator](https://arxiv.org/abs/2601.11480)
*Riya Baruah,Pedro Portugal,Jun-Zhe Chen,Joachim Wabnig,Christian Flindt*

Main category: quant-ph

TL;DR: 研究驱动量子谐振器的热力学特性，通过调制其固有频率来控制温度，分析外部驱动所做的功和热流，并计算光子交换的全分布统计量。


<details>
  <summary>Details</summary>
Motivation: 量子谐振器作为纳米热机的工作介质，能够精确控制和耦合热库，是探索量子热力学过程的理想平台。研究其热力学特性有助于理解热、功和涨落之间的相互作用。

Method: 通过调制量子谐振器的固有频率来控制其温度，评估外部驱动所做的功和由此产生的谐振器与环境之间的热流，在线性响应和非线性区域进行分析，并确定谐振器与环境之间光子交换的全分布统计特征。

Result: 获得了关于热、功和涨落之间相互作用的定量见解，包括光子交换分布的前几个累积量，为理解量子热力学过程提供了具体数据。

Conclusion: 研究结果为量子谐振器作为热机工作介质的热力学行为提供了深入理解，有助于未来热机设计，特别是在纳米尺度量子系统中热功转换的优化。

Abstract: A central building block of a heat engine is the working fluid, which mediates the conversion of heat into work. In nanoscale heat engines, the working fluid can be a quantum system whose behavior and dynamics are non-classical. A particularly versatile realization is a quantum resonator, which allows for precise control and coupling to thermal reservoirs, making it an ideal platform for exploring quantum thermodynamic processes. Here, we investigate the thermodynamic properties of a driven quantum resonator whose temperature is controlled by modulating its natural frequency. We evaluate the work performed by the external drive and the resulting heat flow between the resonator and its environment, both within linear response and beyond. To further elucidate these processes, we determine the full distribution of photon exchanges between the resonator and its environment, characterized by its first few cumulants. Our results provide quantitative insights into the interplay between heat, work, and fluctuations, and may help in designing future heat engines.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [96] [Experimental study of coupled quantum billiards with integrable and chaotic classical dynamics and test of a special Rosenzweig-Porter model](https://arxiv.org/abs/2601.11212)
*Xiaodong Zhang,Jiongning Che,Barbara Dietz*

Main category: nlin.CD

TL;DR: 研究两个量子台球系统（一个混沌、一个可积）通过开口耦合时的谱特性，并与Rosenzweig-Porter模型比较，发现该模型能描述实验数据并确定耦合强度


<details>
  <summary>Details</summary>
Motivation: 研究不同动力学特性（混沌与可积）的量子系统通过开口耦合时的谱特性变化，探索耦合强度对系统整体动力学行为的影响

Method: 实验研究两个量子台球系统（一个混沌、一个可积）通过共同壁上的开口耦合；同时使用Rosenzweig-Porter模型，该模型由两个对角块（分别模拟两个量子台球的谱特性）组成，通过可调参数耦合

Result: Rosenzweig-Porter模型适合描述实验数据，可用于确定耦合强度；随着耦合增强，本征模式重叠增加，可积量子台球的对称性被破坏，谱特性偏离典型的可积和混沌系统，在足够大耦合下接近完全混沌系统

Conclusion: 通过开口耦合混沌和可积量子系统时，耦合强度决定谱特性变化；Rosenzweig-Porter模型能有效描述这种耦合系统，与之前通过随机势能诱导混沌转变的研究方法形成对比

Abstract: We report on the experimental study of the spectral properties of quantum systems consisting of two quantum billiards (QBs), one with chaotic, the other one with integrable classical dynamics, that are coupled to each other via an opening in a common wall. They are compared to those of a special case of the Rosenzweig-Porter model with random matrices composed of two diagonal blocks modeling the spectral properties of the QBs, that are coupled with a tunable parameter. We demonstrate that this model is suitable for the description of the experimental data and thus may be employed to determine the strength of the coupling. It results from the increasing overlap of eigenmodes in the QBs penetrating through the opening into the other one, leading to a mixing of their eigenstates, and the breaking of the symmetry present in the QB with integrable dynamics. This implicates deviations of the spectral properties from those of typical quantum systems with integrable and chaotic dynamics, respectively, and approaches those of a fully chaotic system for sufficiently large coupling strength. In contrast in previous studies the transition from integrable to chaotic dynamics was induced by introducing a random potential of increasing strength into such a QB and applicability of a variant of the Rosenzweig-Porter model was tested.

</details>


### [97] [Nonrelativistic versus relativistic quantum scars in billiard systems](https://arxiv.org/abs/2601.11224)
*Barbara Dietz,Dung Xuan Nguyen,Tilen Čadež*

Main category: nlin.CD

TL;DR: 该研究比较了相对论性中微子台球、石墨烯台球、Haldane石墨烯台球与非相对论量子台球中疤痕态的特征，发现石墨烯台球的疤痕态性质与非相对论系统一致，而非相对论性系统。


<details>
  <summary>Details</summary>
Motivation: 研究不同物理系统中疤痕态的特征，特别是要证明石墨烯台球中观察到的量子疤痕态并不具有相对论性系统的特性，而是与非相对论量子台球的性质一致。

Method: 1. 研究全体育场和四分之一体育场形状的量子台球、中微子台球、石墨烯台球和Haldane石墨烯台球中的疤痕态特征；2. 应用半经典方法分析谱密度，推导与疤痕波函数相关的周期轨道的半经典迹公式；3. 分析动量分布和Husimi函数来分类疤痕态；4. 比较对称性、谱特性和波函数性质。

Result: 1. 所有系统中疤痕态都沿着相同类型的周期轨道局域化，最显著的是弹跳球轨道；2. 石墨烯台球的半经典方法、谱特性、对称性和波函数性质都与非相对论量子台球一致；3. Haldane石墨烯台球的特性与中微子台球（相对论系统）吻合良好；4. 石墨烯台球中观察到的量子疤痕不是相对论性的。

Conclusion: 石墨烯台球中的疤痕态表现出与非相对论量子系统相同的特征，而非相对论性系统的特征，这表明石墨烯台球中观察到的量子疤痕现象本质上是非相对论性的。

Abstract: We study the features of scarred eigenstates of relativistic neutrino billiards (NBs), graphene billiards (GBs) and Haldane graphene billiards (HGBs) and recapitulate those for nonrelativistic quantum billiards (QBs) with the shapes of a full- and quarter-stadium billiard. Here, we restrict for the GBs and HGBs to the region of linear dispersion around the Fermi energy, where they are effectively described by the same Dirac equation for massless spin-1/2 particles as NBs. Scarred wave functions of the nonrelativistic billiards and spinor functions of the relativistic ones are localized along the same types of periodic orbits, the most prominent ones being bouncing-ball orbits. The objective is to demonstrate that the properties of the scarred eigenstates observed in the full- and quarter-stadium GB do not comply with those of relativistic quantum systems. For this we apply the semiclassical approach associated with such non-generic contributions, which was developed for the spectral density of QBs and NBs. It provides semiclassical trace formulas in terms of the periodic orbits associated with a scarred wave function and a procedure to extract such contributions from the eigenvalue spectra. Furthermore, we analyze momentum distributions and Husimi functions of such scarred states and employ them to classify scarred wave functions according to the periodic orbits along which they are localized. We show that for the GB the semiclassical approach, the spectral properties, the symmetry properties and generally properties of the wave functions all comply with those of the nonrelativistic QB, whereas for the HGB they agree well with those of the NB, implying that the quantum scars observed in GBs are not relativistic.

</details>
