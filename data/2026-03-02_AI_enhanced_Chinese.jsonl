{"id": "2602.23420", "categories": ["cond-mat.str-el", "cond-mat.stat-mech", "hep-th", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23420", "abs": "https://arxiv.org/abs/2602.23420", "authors": ["Thomas T. Dumitrescu", "Pierluigi Niro", "Ryan Thorngren"], "title": "From QED$_3$ to Self-Dual Multicriticality in the Fradkin-Shenker Model", "comment": null, "summary": "We consider the Fradkin-Shenker ${\\mathbb Z}_2$ gauge-Higgs lattice model in 2+1 dimensions, i.e. the toric code deformed by an in-plane magnetic field. Its phase diagram contains a multicritical CFT with gapless, mutually non-local electric and magnetic particles, exchanged by a ${\\mathbb Z}_2^{\\mathsf{D}}$ self-duality symmetry. We introduce a staggered generalization of the model in which these particles carry global $U(1)_e$ and $U(1)_m$ charges, respectively, and we propose a continuum QFT description in terms of QED$_3$ with $N_f = 2$ Dirac fermion flavors and a charge-two Higgs field with Yukawa couplings. The conjectured phase diagram harbors a multicritical CFT with $(O(2)_e \\times O(2)_m)\\rtimes\\mathbb{Z}_2^\\mathsf{D}$ symmetry, some of which is emergent in the QFT description. We compute the scaling dimensions of some operators using a large-$N_f$ expansion and find agreement with the emergent selection rules. The staggered model admits a deformation to the original Fradkin-Shenker model, which maps to unit-charge monopole operators in Higgs-Yukawa-QED$_3$ that break the $U(1)_e \\times U(1)_m$ symmetry. We show explicitly that this deformation reproduces all features of the Fradkin-Shenker phase diagram. Finally, we propose a multicritical duality between Higgs-Yukawa-QED$_3$ and the easy-plane $\\mathbb{ CP}^1$ model (i.e. two-flavor scalar QED$_3$ with a suitable potential), which describes spin-1/2 anti-ferromagnets on a square lattice. This duality implies a first-order line of N\u00e9el-VBS transitions ending in a deconfined quantum multicritical point, described by the same $O(2)_e \\times O(2)_m$ symmetric CFT that arises in the staggered Fradkin-Shenker model, which separates it from a gapped ${\\mathbb Z}_2$ spin liquid phase.", "AI": {"tldr": "This paper studies a staggered generalization of the Fradkin-Shenker Z\u2082 gauge-Higgs model in 2+1 dimensions, introducing global U(1) symmetries for electric and magnetic charges. It proposes a continuum field theory (Higgs-Yukawa-QED\u2083) and establishes a duality to the easy-plane CP\u00b9 model, revealing a multicritical conformal field theory (CFT) with O(2)\u2091 \u00d7 O(2)\u2098 symmetry that describes deconfined quantum critical points in antiferromagnets.", "motivation": "To generalize the original Fradkin-Shenker model (toric code with in-plane magnetic field) by incorporating continuous U(1) symmetries for electric/magnetic particles, enabling a richer phase diagram and connections to continuum field theories. This aims to bridge lattice gauge models with deconfined quantum critical phenomena in condensed matter systems, particularly spin-1/2 antiferromagnets.", "method": "1. Introduces a staggered lattice model with explicit U(1)\u2091 and U(1)\u2098 global symmetries.  \n2. Proposes Higgs-Yukawa-QED\u2083 (QED\u2083 with N_f=2 Dirac fermions and a charge-2 Higgs field with Yukawa couplings) as the continuum description.  \n3. Uses large-N_f expansion to compute operator scaling dimensions.  \n4. Deforms the staggered model to recover the original Fradkin-Shenker model via monopole operators.  \n5. Constructs a duality between Higgs-Yukawa-QED\u2083 and the easy-plane CP\u00b9 model (scalar QED\u2083 with a potential).", "result": "1. Identifies a multicritical CFT with emergent (O(2)\u2091 \u00d7 O(2)\u2098) \u22ca Z\u2082\u1d30 symmetry in the staggered model.  \n2. Confirms scaling dimensions from large-N_f expansion match emergent symmetry selection rules.  \n3. Shows deformation to Fradkin-Shenker model via monopoles reproduces its full phase diagram.  \n4. Proposes duality revealing a first-order N\u00e9el-VBS transition line ending at a deconfined quantum multicritical point (same CFT as staggered model), separating these phases from a Z\u2082 spin liquid.", "conclusion": "The staggered generalization successfully links discrete gauge models to continuum QFTs with continuous symmetries, providing a unified framework for multicritical phenomena. The duality to CP\u00b9 model offers new insights into deconfined quantum criticality in antiferromagnets, confirming the universality of the O(2)\u2091 \u00d7 O(2)\u2098 CFT across different physical systems."}}
{"id": "2602.23477", "categories": ["cond-mat.str-el", "cond-mat.mes-hall", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2602.23477", "abs": "https://arxiv.org/abs/2602.23477", "authors": ["Sayan Mitra", "Fang Xie", "Marek Kolmer", "Qimiao Si", "Chandan Setty"], "title": "Signatures of Green's function zeros and their topology using impurity spectroscopy", "comment": "8 pages, 5 figures", "summary": "Topology without quasiparticles has emerged as a key framework for understanding Mott insulators, where Green's-function zeros encode nontrivial topological structure. Yet, experimental detection of these zeros represents a challenge. Using exact diagonalization of the one-dimensional Hubbard model with an impurity and Zeeman field, supported by exact analytic results, we show that Green's-function zeros manifest as an in-gap spectral weight in the unitary scattering regime. In this limit, we map the impurity problem onto a doped Mott insulator and identify the resulting in-gap state as a \"zeron\" excitation which is a localized doublon (holon) for an attractive (repulsive) potential. The zeron spectral weight and its associated zero vanish above a critical Zeeman field. Our results imply that Green's function zeros have in fact already been observed in experiments, and establish impurity and magnetic-field tuning as practical tools for controlling their topology.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u7cbe\u786e\u5bf9\u89d2\u5316\u4e00\u7ef4Hubbard\u6a21\u578b\uff0c\u53d1\u73b0\u683c\u6797\u51fd\u6570\u96f6\u70b9\u5728\u5e7a\u6b63\u6563\u5c04 regime \u4e2d\u8868\u73b0\u4e3a\u53ef\u89c2\u6d4b\u7684\"zeron\"\u6fc0\u53d1\u6001\uff0c\u5176\u8c31\u6743\u91cd\u968f\u585e\u66fc\u573a\u589e\u5f3a\u800c\u6d88\u5931\uff0c\u4e3a\u5b9e\u9a8c\u63a2\u6d4b\u62d3\u6251\u7ed3\u6784\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "motivation": "\u5728\u975e\u76f8\u4e92\u4f5c\u7528\u4f53\u7cfb\u4e2d\u62d3\u6251\u7ed3\u6784\u7531\u80fd\u5e26\u8868\u5f81\uff0c\u4f46Mott\u7edd\u7f18\u4f53\u4e2d\u7684\u62d3\u6251\u6027\u8d28\u7531\u683c\u6797\u51fd\u6570\u96f6\u70b9\u7f16\u7801\uff0c\u800c\u5b9e\u9a8c\u4e0a\u76f4\u63a5\u63a2\u6d4b\u8fd9\u4e9b\u96f6\u70b9\u5b58\u5728\u5de8\u5927\u6311\u6218\u3002", "method": "\u91c7\u7528\u7cbe\u786e\u5bf9\u89d2\u5316\u65b9\u6cd5\u7814\u7a76\u542b\u6742\u8d28\u548c\u585e\u66fc\u573a\u7684\u4e00\u7ef4Hubard\u6a21\u578b\uff0c\u7ed3\u5408\u89e3\u6790\u5206\u6790\uff0c\u5c06\u6742\u8d28\u95ee\u9898\u6620\u5c04\u5230\u63ba\u6742Mott\u7edd\u7f18\u4f53\u4f53\u7cfb\u3002", "result": "\u5728\u5e7a\u6b63\u6563\u5c04\u6781\u9650\u4e0b\uff0c\u683c\u6797\u51fd\u6570\u96f6\u70b9\u8868\u73b0\u4e3a\u9699\u95f4\u8c31\u6743\u91cd\uff08\u79f0\u4e3a\"zeron\"\u6fc0\u53d1\uff09\uff1a\u5438\u5f15\u52bf\u5bf9\u5e94\u5c40\u57df\u53cc\u5360\u636e\u6001\uff0c\u6392\u65a5\u52bf\u5bf9\u5e94\u7a7a\u7a74\u6001\uff1b\u8be5\u6001\u5728\u4e34\u754c\u585e\u66fc\u573a\u4ee5\u4e0a\u6d88\u5931\u3002", "conclusion": "\u683c\u6797\u51fd\u6570\u96f6\u70b9\u53ef\u80fd\u5df2\u5728\u5b9e\u9a8c\u4e2d\u88ab\u89c2\u6d4b\u5230\uff0c\u901a\u8fc7\u8c03\u63a7\u6742\u8d28\u548c\u78c1\u573a\u53ef\u64cd\u7eb5\u5176\u62d3\u6251\u7ed3\u6784\uff0c\u4e3a\u7814\u7a76Mott\u7edd\u7f18\u4f53\u62d3\u6251\u6027\u8d28\u63d0\u4f9b\u4e86\u53ef\u884c\u5b9e\u9a8c\u65b9\u6848\u3002"}}
{"id": "2602.23484", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.23484", "abs": "https://arxiv.org/abs/2602.23484", "authors": ["Shuangyuan Lu", "Lucas Q Silveira", "Yizhi You"], "title": "Nonequilibrium topological response under charge dephasing", "comment": "23 pages, 8 figures", "summary": "We explore nonequilibrium topological responses of symmetry-protected topological (SPT) states in open quantum systems subject to decoherence. For SPT wavefunctions protected by a product symmetry G $\\times$ S , where G defects are decorated with S charge, we show that local dephasing of the S charge density generically induces spontaneous strong-to-weak symmetry breaking (SWSSB) of G in the resulting mixed-state ensemble. We extend this mechanism to SPT phases protected by higher-form and spatially modulated symmetries, and further to gapless SPT states, demonstrating that dephasing-induced SWSSB persists well beyond conventional gapped 0-form settings. Our results provide a qualitative, channel-defined fingerprint of SPT order that is intrinsic to open-system dynamics and goes beyond equilibrium linear response.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u4e2d\u5bf9\u79f0\u4fdd\u62a4\u62d3\u6251\u6001\u5728\u9000\u76f8\u5e72\u4f5c\u7528\u4e0b\u7684\u975e\u5e73\u8861\u62d3\u6251\u54cd\u5e94\uff0c\u53d1\u73b0\u5c40\u57df\u9000\u76f8\u4f4d\u4f1a\u5f15\u53d1\u5f3a\u5bf9\u79f0\u6027\u5230\u5f31\u5bf9\u79f0\u6027\u7684\u81ea\u53d1\u7834\u7f3a\uff0c\u5e76\u8bc1\u660e\u8be5\u673a\u5236\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u5bf9\u79f0\u6027\u7c7b\u578b\u3002", "motivation": "\u63a2\u7d22\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\uff08\u53d7\u9000\u76f8\u5e72\u5f71\u54cd\uff09\u4e2d\u5bf9\u79f0\u4fdd\u62a4\u62d3\u6251\u6001\u7684\u975e\u5e73\u8861\u62d3\u6251\u54cd\u5e94\u7279\u6027\uff0c\u7279\u522b\u662f\u8d85\u8d8a\u4f20\u7edf\u5e73\u8861\u6001\u7ebf\u6027\u54cd\u5e94\u7684\u5185\u5728\u54cd\u5e94\u673a\u5236\u3002", "method": "\u9488\u5bf9\u7531\u4e58\u79ef\u5bf9\u79f0\u6027G\u00d7S\u4fdd\u62a4\u7684\u62d3\u6251\u6ce2\u51fd\u6570\uff0c\u5206\u6790S\u7535\u8377\u5bc6\u5ea6\u7684\u5c40\u57df\u9000\u76f8\u4f4d\u5982\u4f55\u5bfc\u81f4\u6df7\u5408\u6001\u7cfb\u7efc\u4e2d\u7684\u5f3a-\u5f31\u5bf9\u79f0\u6027\u81ea\u53d1\u7834\u7f3a\uff08SWSSB\uff09\uff0c\u5e76\u5c06\u673a\u5236\u63a8\u5e7f\u81f3\u9ad8\u5f62\u5f0f\u5bf9\u79f0\u6027\u3001\u7a7a\u95f4\u8c03\u5236\u5bf9\u79f0\u6027\u53ca\u65e0\u80fd\u9699\u62d3\u6251\u6001\u3002", "result": "\u8bc1\u5b9e\u5c40\u57df\u9000\u76f8\u4f4d\u4f1a\u666e\u904d\u8bf1\u5bfcG\u5bf9\u79f0\u6027\u7684\u5f3a-\u5f31\u5bf9\u79f0\u6027\u81ea\u53d1\u7834\u7f3a\uff1b\u8be5\u673a\u5236\u4e0d\u4ec5\u9650\u4e8e\u4f20\u7edf\u6709\u96990-form\u5bf9\u79f0\u6027\u4f53\u7cfb\uff0c\u8fd8\u9002\u7528\u4e8e\u9ad8\u5f62\u5f0f\u5bf9\u79f0\u6027\u3001\u7a7a\u95f4\u8c03\u5236\u5bf9\u79f0\u6027\u53ca\u65e0\u80fd\u9699\u5bf9\u79f0\u4fdd\u62a4\u62d3\u6251\u6001\u3002", "conclusion": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u91cf\u5b50\u901a\u9053\u5b9a\u4e49\u7684\u5bf9\u79f0\u4fdd\u62a4\u62d3\u6251\u5e8f\u7684\u5b9a\u6027\u6307\u7eb9\uff0c\u8be5\u6307\u7eb9\u5185\u751f\u4e8e\u5f00\u653e\u7cfb\u7edf\u52a8\u529b\u5b66\uff0c\u8d85\u8d8a\u4e86\u5e73\u8861\u6001\u7ebf\u6027\u54cd\u5e94\u7406\u8bba\uff0c\u4e3a\u5b9e\u9a8c\u68c0\u6d4b\u62d3\u6251\u5e8f\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2602.23522", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.23522", "abs": "https://arxiv.org/abs/2602.23522", "authors": ["Nitin Kaushal", "Adarsh S. Patri", "Marcel Franz"], "title": "Spontaneous altermagnetism in multi-orbital correlated electron systems", "comment": null, "summary": "Altermagnets have attracted considerable attention in recent years owing to their potential technological applications in spintronics and magnonics. Recently, a new class of spontaneous altermagnets has been theoretically predicted in a correlated two orbital model, driven by the coexistence of antiferromagnetic spin and staggered orbital ordering, thus broadening the scope of altermagnetic phenomena to systems with strong correlations. It has been noted, however, that the required spin and orbital order violates the well-established Goodenough-Kanamori (GK) rules, which underlie much of our understanding of magnetism in complex systems. Here we show that materials with three active orbitals may offer a more realistic route to this exotic state. Specifically, we consider a two-dimensional system with $t_{2g}^{2}$ electrons and identify a novel microscopic mechanism that allows the formation of a spontaneous altermagnetic Mott insulator. We explain how the GK rules are circumvented and provide the stability criteria by employing unbiased mean-field and density matrix renormalization group calculations. In addition, for the first time, we uncover the presence and microscopic origin of chirally split magnons in these spontaneous altermagnets, with experimentally measurable spin conductivities. Finally, we predict that the application of a small in-plane magnetic field induces, in the presence of weak atomic spin-orbit coupling, an as-yet unreported hybrid chiral magnon-orbiton mode with a non-zero orbital polarization giving rise to finite longitudinal and transverse orbital conductivities under a thermal gradient.", "AI": {"tldr": "\u63d0\u51fa\u4e09\u8f68\u9053\u7cfb\u7edf\u53ef\u5b9e\u73b0\u81ea\u53d1\u53cd\u94c1\u78c1\u83ab\u7279\u7edd\u7f18\u4f53\uff0c\u901a\u8fc7\u89c4\u907f\u53e4\u5fb7\u91cc\u5947-\u91d1\u68ee\u89c4\u5219\uff0c\u9996\u6b21\u63ed\u793a\u624b\u6027\u5206\u88c2\u78c1\u632f\u5b50\u53ca\u78c1\u573a\u8bf1\u5bfc\u7684\u8f68\u9053-\u78c1\u632f\u5b50\u6df7\u5408\u6a21\u5f0f", "motivation": "\u73b0\u6709\u4e24\u8f68\u9053\u6a21\u578b\u4e2d\u81ea\u53d1\u53cd\u94c1\u78c1\u6001\u8fdd\u80cc\u53e4\u5fb7\u91cc\u5947-\u91d1\u68ee(GK)\u89c4\u5219\uff0c\u9700\u5bfb\u627e\u66f4\u5408\u7406\u7684\u5f3a\u5173\u8054\u4f53\u7cfb\u5b9e\u73b0\u8be5\u62d3\u6251\u78c1\u6001", "method": "\u91c7\u7528\u65e0\u504f\u5e73\u5747\u573a\u548c\u5bc6\u5ea6\u77e9\u9635\u91cd\u6b63\u5316\u7fa4\u8ba1\u7b97\uff0c\u5206\u6790\u4e8c\u7ef4t\u2082g\u00b2\u7535\u5b50\u7cfb\u7edf\uff0c\u7a81\u7834\u4f20\u7edfGK\u89c4\u5219\u9650\u5236", "result": "1) \u8bc1\u5b9e\u4e09\u8f68\u9053\u7cfb\u7edf\u53ef\u7a33\u5b9a\u5f62\u6210\u53cd\u94c1\u78c1\u83ab\u7279\u7edd\u7f18\u4f53 2) \u9996\u6b21\u53d1\u73b0\u624b\u6027\u5206\u88c2\u78c1\u632f\u5b50\u53ca\u5176\u81ea\u65cb\u7535\u5bfc\u7279\u5f81 3) \u9884\u6d4b\u5f31\u78c1\u573a\u4e0b\u4ea7\u751f\u975e\u96f6\u8f68\u9053\u6781\u5316\u7684\u6742\u5316\u624b\u6027\u78c1\u632f\u5b50-\u8f68\u9053\u5b50\u6a21\u5f0f", "conclusion": "\u4e3a\u5f3a\u5173\u8054\u4f53\u7cfb\u4e2d\u7684\u53cd\u94c1\u78c1\u62d3\u6251\u6001\u63d0\u4f9b\u65b0\u5b9e\u73b0\u8def\u5f84\uff0c\u9884\u8a00\u7684\u53ef\u89c2\u6d4b\u8f68\u9053\u8f93\u8fd0\u6548\u5e94\u5c06\u63a8\u52a8\u591a\u94c1\u6027\u5668\u4ef6\u4e0e\u8f68\u9053\u7535\u5b50\u5b66\u7814\u7a76"}}
{"id": "2602.23391", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23391", "abs": "https://arxiv.org/abs/2602.23391", "authors": ["Nazanin Mohammadi Sepahvand", "Eleni Triantafillou", "Hugo Larochelle", "Doina Precup", "Daniel M. Roy", "Gintare Karolina Dziugaite"], "title": "Detoxifying LLMs via Representation Erasure-Based Preference Optimization", "comment": null, "summary": "Large language models (LLMs) trained on webscale data can produce toxic outputs, raising concerns for safe deployment. Prior defenses, based on applications of DPO, NPO, and similar algorithms, reduce the likelihood of harmful continuations, but not robustly so: they are vulnerable to adversarial prompting and easily undone by fine-tuning-based relearning attacks. Indeed, research has shown that these edits to the model are superficial: linear probing reveals that harmful \"directions\" remain present in representations. To address this, we propose Representation Erasure-based Preference Optimization (REPO), reformulating detoxification as a token-level preference problem. Using a novel objective with preference data, we force the representations of toxic continuations to converge toward their benign counterparts. Our mechanistic analysis reveals that this granular approach is critical: unlike baselines, REPO induces deep, localized edits to toxicity-encoding neurons while preserving general model utility. Exhaustive evaluations show that REPO achieves state-of-the-art robustness, stopping sophisticated threats-including relearning attacks and enhanced GCG jailbreaks-where existing representation- and output-based methods fail.", "AI": {"tldr": "\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ea7\u751f\u6709\u5bb3\u8f93\u51fa\u7684\u95ee\u9898\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\uff08\u5982DPO\u3001NPO\uff09\u5b58\u5728\u8106\u5f31\u6027\uff0c\u6613\u53d7\u5bf9\u6297\u6027\u63d0\u793a\u548c\u91cd\u65b0\u5b66\u4e60\u653b\u51fb\u5f71\u54cd\u3002\u672c\u7814\u7a76\u63d0\u51faREPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ee4\u724c\u7ea7\u504f\u597d\u4f18\u5316\u4f7f\u6709\u6bd2\u8868\u793a\u5411\u826f\u6027\u8868\u793a\u6536\u655b\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6548\u7528\u7684\u540c\u65f6\u5b9e\u73b0\u5bf9\u6bd2\u6027\u7f16\u7801\u795e\u7ecf\u5143\u7684\u6df1\u5ea6\u672c\u5730\u5316\u7f16\u8f91\uff0c\u5728\u62b5\u5fa1\u9ad8\u7ea7\u5a01\u80c1\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u90e8\u7f72\u4e2d\u5b58\u5728\u751f\u6210\u6709\u6bd2\u5185\u5bb9\u7684\u98ce\u9669\uff0c\u73b0\u6709\u57fa\u4e8eDPO\u3001NPO\u7b49\u7b97\u6cd5\u7684\u9632\u5fa1\u63aa\u65bd\u5b58\u5728\u6839\u672c\u6027\u7f3a\u9677\uff1a\u5b83\u4eec\u65e0\u6cd5\u62b5\u5fa1\u5bf9\u6297\u6027\u63d0\u793a\uff0c\u4e14\u6613\u901a\u8fc7\u5fae\u8c03\u91cd\u65b0\u5b66\u4e60\u653b\u51fb\u800c\u5931\u6548\u3002\u7814\u7a76\u8868\u660e\u8fd9\u4e9b\u7f16\u8f91\u53ea\u662f\u8868\u5c42\u4fee\u6539\uff0c\u6709\u5bb3\u7684\"\u65b9\u5411\"\u4ecd\u7136\u5b58\u5728\u4e8e\u6a21\u578b\u8868\u793a\u4e2d\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9c81\u68d2\u3001\u66f4\u6df1\u5c42\u6b21\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u8868\u793a\u64e6\u9664\u504f\u597d\u4f18\u5316\uff08REPO\uff09\uff0c\u5c06\u53bb\u6bd2\u5316\u91cd\u65b0\u8868\u8ff0\u4e3a\u4ee4\u724c\u7ea7\u504f\u597d\u95ee\u9898\u3002\u901a\u8fc7\u65b0\u9896\u7684\u504f\u597d\u76ee\u6807\u51fd\u6570\uff0c\u5f3a\u5236\u6709\u6bd2\u7eed\u4f53\u7684\u8868\u793a\u5411\u5176\u826f\u6027\u5bf9\u5e94\u4f53\u6536\u655b\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u7ec6\u7c92\u5ea6\u7b56\u7565\uff0c\u9488\u5bf9\u6027\u5730\u4fee\u6539\u7f16\u7801\u6bd2\u6027\u7684\u795e\u7ecf\u5143\uff0c\u540c\u65f6\u4fdd\u7559\u6a21\u578b\u7684\u901a\u7528\u6548\u7528\u3002", "result": "REPO\u5728\u5168\u9762\u8bc4\u4f30\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u9c81\u68d2\u6027\uff0c\u6210\u529f\u62b5\u5fa1\u4e86\u73b0\u6709\u8868\u793a\u7ea7\u548c\u8f93\u51fa\u7ea7\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u7684\u590d\u6742\u5a01\u80c1\uff0c\u5305\u62ec\u91cd\u65b0\u5b66\u4e60\u653b\u51fb\u548c\u589e\u5f3a\u7684GCG\u8d8a\u72f1\u653b\u51fb\u3002\u673a\u5236\u5206\u6790\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u4e0d\u540c\uff0cREPO\u80fd\u591f\u8bf1\u5bfc\u5bf9\u6bd2\u6027\u7f16\u7801\u795e\u7ecf\u5143\u7684\u6df1\u5ea6\u3001\u672c\u5730\u5316\u7f16\u8f91\u3002", "conclusion": "REPO\u4ee3\u8868\u4e86LLM\u5b89\u5168\u5fae\u8c03\u7684\u91cd\u8981\u7a81\u7834\uff0c\u8bc1\u660e\u901a\u8fc7\u8868\u793a\u7a7a\u95f4\u4e2d\u7684\u7ec6\u7c92\u5ea6\u5e72\u9884\u53ef\u5b9e\u73b0\u6df1\u5c42\u3001\u6301\u4e45\u7684\u53bb\u6bd2\u5316\u6548\u679c\u3002\u8be5\u65b9\u6cd5\u4e3a\u6784\u5efa\u66f4\u5b89\u5168\u3001\u66f4\u53ef\u9760\u7684\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u62b5\u5fa1\u9ad8\u7ea7\u5bf9\u6297\u6027\u653b\u51fb\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2602.24221", "categories": ["nlin.CD"], "pdf": "https://arxiv.org/pdf/2602.24221", "abs": "https://arxiv.org/abs/2602.24221", "authors": ["Alexei A. Mailybaev", "Luca Moriconi"], "title": "Renormalization-group perspective on spontaneous stochasticity", "comment": "28 pages, 10 figures", "summary": "We present a renormalization-group perspective on spontaneous stochasticity in hydrodynamic turbulence, viewed through the lens of multiscale dynamical systems. Building on previously established results for a solvable multiscale Arnold's cat model, we show that spontaneous stochasticity emerges as a universal fixed point of an RG transformation acting on Markov kernels, independent of the microscopic regularization. Classical examples - including the Feigenbaum equation, the central limit theorem, and hierarchical spin models - are reinterpreted within the same framework, placing spontaneous stochasticity alongside other universality phenomena.", "AI": {"tldr": "This paper develops a renormalization-group framework to show that spontaneous stochasticity in hydrodynamic turbulence is a universal fixed point phenomenon, analogous to other universality classes in physics.", "motivation": "To provide a theoretical perspective on spontaneous stochasticity in hydrodynamic turbulence by connecting it to multiscale dynamical systems and universality phenomena through renormalization-group methods.", "method": "Uses renormalization-group (RG) transformations acting on Markov kernels within a multiscale dynamical systems framework, building on previous results from Arnold's cat model.", "result": "Demonstrates that spontaneous stochasticity emerges as a universal fixed point of the RG transformation, independent of microscopic details. Reinterprets classical examples (Feigenbaum equation, central limit theorem, hierarchical spin models) within this unified framework.", "conclusion": "Spontaneous stochasticity belongs to the same category of universal phenomena as other well-established physics universality classes, providing a broader theoretical context for understanding turbulence and stochastic dynamics."}}
{"id": "2602.23988", "categories": ["cond-mat.stat-mech", "cond-mat.soft"], "pdf": "https://arxiv.org/pdf/2602.23988", "abs": "https://arxiv.org/abs/2602.23988", "authors": ["Kristian St\u00f8levik Olsen", "Mitsusuke Tarama", "Hartmut L\u00f6wen"], "title": "Information bound on navigation speed in smart active matter", "comment": null, "summary": "Intelligent behavior in life-like systems often arises from the ability to gather, process, and act on information. While active matter provides a framework for studying life-like dynamics, it typically omits internal information-processing and decision-making. Here we introduce an adaptive active particle model that uses minimal information processing capabilities in order to navigate towards a distant target. By combining renewal-based intermittent motion with the Cram\u00e9r-Rao inequality, we derive a bound on the navigation speed valid for a wide range of information processing strategies. The framework captures hallmark features of cognitive systems, including optimal sensing durations and a speed-accuracy trade-off that balances noise and reliability. Allowing stored information to degrade before action reveals that although deterioration slows navigation, the trade-off remains governed primarily by external orientational noise and is remarkably insensitive to memory decay.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u81ea\u9002\u5e94\u6d3b\u6027\u7c92\u5b50\u6a21\u578b\uff0c\u7ed3\u5408\u4fe1\u606f\u5904\u7406\u4e0e\u8fd0\u52a8\u63a7\u5236\uff0c\u63a8\u5bfc\u51fa\u901a\u7528\u5bfc\u822a\u901f\u5ea6\u8fb9\u754c\uff0c\u63ed\u793a\u8ba4\u77e5\u7cfb\u7edf\u7279\u5f81\uff08\u6700\u4f18\u611f\u77e5\u65f6\u957f\u3001\u901f\u5ea6-\u7cbe\u5ea6\u6743\u8861\uff09\uff0c\u53d1\u73b0\u8bb0\u5fc6\u8870\u51cf\u5bf9\u5bfc\u822a\u5f71\u54cd\u6709\u9650\u3002", "motivation": "\u4f20\u7edf\u6d3b\u6027\u7269\u8d28\u6a21\u578b\u7f3a\u4e4f\u5185\u90e8\u4fe1\u606f\u5904\u7406\u4e0e\u51b3\u7b56\u80fd\u529b\uff0c\u800c\u751f\u547d\u7cfb\u7edf\u7684\u667a\u80fd\u884c\u4e3a\u4f9d\u8d56\u4e8e\u4fe1\u606f\u611f\u77e5\u3001\u5904\u7406\u548c\u54cd\u5e94\u3002\u672c\u7814\u7a76\u65e8\u5728\u5efa\u7acb\u8fde\u63a5\u6d3b\u6027\u7269\u8d28\u4e0e\u8ba4\u77e5\u7cfb\u7edf\u7684\u7406\u8bba\u6846\u67b6\u3002", "method": "\u7ed3\u5408\u57fa\u4e8e\u66f4\u65b0\u7684\u95f4\u6b47\u8fd0\u52a8\u6a21\u578b\u4e0e\u514b\u62c9\u7f8e-\u62c9\u5965\u4e0d\u7b49\u5f0f\uff0c\u63a8\u5bfc\u9002\u7528\u4e8e\u5e7f\u6cdb\u4fe1\u606f\u5904\u7406\u7b56\u7565\u7684\u5bfc\u822a\u901f\u5ea6\u8fb9\u754c\u3002\u901a\u8fc7\u5141\u8bb8\u5b58\u50a8\u4fe1\u606f\u9000\u5316\u6765\u6a21\u62df\u8bb0\u5fc6\u8870\u51cf\u6548\u5e94\u3002", "result": "1) \u63a8\u5bfc\u51fa\u5bfc\u822a\u901f\u5ea6\u7684\u666e\u9002\u8fb9\u754c\uff1b2) \u91cd\u73b0\u8ba4\u77e5\u7cfb\u7edf\u6838\u5fc3\u7279\u5f81\uff1a\u6700\u4f18\u611f\u77e5\u65f6\u957f\u3001\u901f\u5ea6-\u7cbe\u5ea6\u6743\u8861\uff1b3) \u53d1\u73b0\u5bfc\u822a\u8fc7\u7a0b\u4e3b\u8981\u53d7\u5916\u90e8\u65b9\u5411\u566a\u58f0\u652f\u914d\uff0c\u5bf9\u8bb0\u5fc6\u8870\u51cf\u4e0d\u654f\u611f\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5c06\u4fe1\u606f\u5904\u7406\u673a\u5236\u6574\u5408\u5230\u6d3b\u6027\u7269\u8d28\u7cfb\u7edf\u4e2d\uff0c\u63ed\u793a\u4e86\u751f\u7269\u5bfc\u822a\u7684\u57fa\u672c\u539f\u7406\uff0c\u8868\u660e\u8bb0\u5fc6\u8870\u9000\u867d\u4f1a\u8f7b\u5fae\u51cf\u6162\u5bfc\u822a\uff0c\u4f46\u6838\u5fc3\u6743\u8861\u5173\u7cfb\u4ecd\u7531\u5916\u90e8\u73af\u5883\u566a\u58f0\u4e3b\u5bfc\u3002"}}
{"id": "2602.23367", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.23367", "abs": "https://arxiv.org/abs/2602.23367", "authors": ["Shubh Laddha", "Lucas Changbencharoen", "Win Kuptivej", "Surya Shringla", "Archana Vaidheeswaran", "Yash Bhaskar"], "title": "HumanMCP: A Human-Like Query Dataset for Evaluating MCP Tool Retrieval Performance", "comment": "4 pages, 2 figures, 3 tables", "summary": "Model Context Protocol (MCP) servers contain a collection of thousands of open-source standardized tools, linking LLMs to external systems; however, existing datasets and benchmarks lack realistic, human-like user queries, remaining a critical gap in evaluating the tool usage and ecosystems of MCP servers. Existing datasets often do contain tool descriptions but fail to represent how different users portray their requests, leading to poor generalization and inflated reliability of certain benchmarks. This paper introduces the first large-scale MCP dataset featuring diverse, high-quality diverse user queries generated specifically to match 2800 tools across 308 MCP servers, developing on the MCP Zero dataset. Each tool is paired with multiple unique user personas that we have generated, to capture varying levels of user intent ranging from precise task requests, and ambiguous, exploratory commands, reflecting the complexity of real-world interaction patterns.", "AI": {"tldr": "This paper introduces the first large-scale MCP dataset with diverse, realistic user queries across 2,800 tools from 308 servers to address the gap in evaluating LLM tool usage.", "motivation": "Existing MCP datasets lack realistic, human-like user queries, leading to poor generalization and inflated benchmark reliability for evaluating tool usage and ecosystems.", "method": "Developed a large-scale dataset building on MCP Zero, pairing 2,800 tools from 308 MCP servers with multiple unique user personas that generate diverse queries ranging from precise tasks to ambiguous, exploratory commands.", "result": "Created the first MCP dataset with diverse, high-quality user queries that better reflect real-world interaction patterns across a wide range of tools and servers.", "conclusion": "This dataset addresses a critical gap in MCP evaluation by providing realistic user queries that enable more accurate assessment of LLM tool usage capabilities and ecosystem performance."}}
{"id": "2602.23422", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23422", "abs": "https://arxiv.org/abs/2602.23422", "authors": ["Adway Kumar Das", "Achilleas Lazarides"], "title": "Ground state and persistent oscillations in the quantum East model", "comment": "11 pages, 8 figures", "summary": "For the 1D quantum East model with open boundaries, we show that in the limit $s \\to -\\infty$, the ground state is accurately captured by a simple spin-coherent product state. We further identify a low-entanglement excited eigenstate that differs from the ground state only by a $\u03c0$-rotation of the boundary spin, remaining well approximated by a spin-coherent state. For a range of $-\\infty<s<0$, the edge-coherent product state overlaps with two eigenstates separated by a size-independent energy gap, leading to persistent coherent oscillations of both global and local observables in the thermodynamic limit. These oscillations originate from boundary physics and are distinct from quantum many-body scars or hypercube-like Fock-space mechanisms.", "AI": {"tldr": "\u57281D\u91cf\u5b50East\u6a21\u578b\u4e2d\uff0c\u5f53\u53c2\u6570s\u2192-\u221e\u65f6\uff0c\u7cfb\u7edf\u7684\u57fa\u6001\u548c\u6fc0\u53d1\u6001\u5747\u53ef\u7531\u7b80\u5355\u7684\u81ea\u65cb\u76f8\u5e72\u6001\u51c6\u786e\u63cf\u8ff0\uff0c\u8fb9\u754c\u81ea\u65cb\u7684\u03c0\u65cb\u8f6c\u5bfc\u81f4\u4f4e\u7ea0\u7f20\u6fc0\u53d1\u6001\uff1b\u5f53-\u221e<s<0\u65f6\uff0c\u8fd9\u79cd\u8fb9\u754c\u76f8\u5e72\u6027\u5bfc\u81f4\u7cfb\u7edf\u51fa\u73b0\u6301\u7eed\u7684\u5168\u5c40\u4e0e\u5c40\u57df\u89c2\u6d4b\u91cf\u7684\u76f8\u5e72\u632f\u8361\uff0c\u5176\u7269\u7406\u673a\u5236\u4e0d\u540c\u4e8e\u4f20\u7edf\u591a\u4f53\u75a4\u75d5\u6216Fock\u7a7a\u95f4\u8d85\u7acb\u65b9\u4f53\u673a\u5236\u3002", "motivation": "\u7406\u89e3\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u5728\u7279\u5b9a\u6781\u9650\u4e0b\u7684\u4f4e\u80fd\u7269\u7406\u884c\u4e3a\uff0c\u7279\u522b\u662f\u8fb9\u754c\u6548\u5e94\u5bfc\u81f4\u7684\u975e\u5e73\u51e1\u52a8\u529b\u5b66\u73b0\u8c61\uff0c\u5e76\u533a\u5206\u5176\u4e0e\u5df2\u77e5\u673a\u5236\uff08\u5982\u591a\u4f53\u75a4\u75d5\uff09\u7684\u5dee\u5f02\u3002", "method": "\u89e3\u6790\u7814\u7a761D\u91cf\u5b50East\u6a21\u578b\u5728\u5f00\u653e\u8fb9\u754c\u6761\u4ef6\u4e0b\u7684\u672c\u5f81\u6001\u7ed3\u6784\uff0c\u5206\u6790s\u2192-\u221e\u6781\u9650\u4e0b\u7684\u57fa\u6001\u548c\u4f4e\u6fc0\u53d1\u6001\u6027\u8d28\uff0c\u5e76\u8003\u5bdf\u6709\u9650s\u503c\u4e0b\u8fb9\u7f18\u76f8\u5e72\u6001\u4e0e\u4e24\u4e2a\u672c\u5f81\u6001\u7684\u91cd\u53e0\u53ca\u80fd\u9699\u884c\u4e3a\u3002", "result": "1. s\u2192-\u221e\u65f6\u57fa\u6001\u53ef\u7528\u81ea\u65cb\u76f8\u5e72\u4e58\u79ef\u6001\u7cbe\u786e\u8fd1\u4f3c\uff1b2. \u5b58\u5728\u7531\u8fb9\u754c\u81ea\u65cb\u03c0\u65cb\u8f6c\u5bfc\u81f4\u7684\u4f4e\u7ea0\u7f20\u6fc0\u53d1\u6001\uff0c\u540c\u6837\u53ef\u7528\u81ea\u65cb\u76f8\u5e72\u6001\u63cf\u8ff0\uff1b3. -\u221e<s<0\u533a\u95f4\u5185\uff0c\u8fb9\u7f18\u76f8\u5e72\u6001\u4e0e\u4e24\u4e2a\u5206\u79bb\u7684\u672c\u5f81\u6001\u91cd\u53e0\uff0c\u5176\u95f4\u80fd\u9699\u4e0e\u7cfb\u7edf\u5c3a\u5bf8\u65e0\u5173\uff1b4. \u70ed\u529b\u5b66\u6781\u9650\u4e0b\u51fa\u73b0\u6301\u7eed\u76f8\u5e72\u632f\u8361\uff1b5. \u8be5\u673a\u5236\u6e90\u4e8e\u8fb9\u754c\u7269\u7406\uff0c\u4e0d\u540c\u4e8e\u591a\u4f53\u75a4\u75d5\u6216Fock\u7a7a\u95f4\u8d85\u7acb\u65b9\u4f53\u673a\u5236\u3002", "conclusion": "1D\u91cf\u5b50East\u6a21\u578b\u7684\u8fb9\u754c\u81ea\u7531\u5ea6\u5728\u7279\u5b9a\u53c2\u6570\u533a\u95f4\u7684\u76f8\u5e72\u6f14\u5316\u5bfc\u81f4\u4e86\u53ef\u89c2\u6d4b\u7684\u6301\u7eed\u632f\u8361\u73b0\u8c61\uff0c\u8fd9\u4e00\u53d1\u73b0\u63ed\u793a\u4e86\u4e00\u79cd\u65b0\u7684\u975e\u5e73\u8861\u52a8\u529b\u5b66\u673a\u5236\uff0c\u4e3a\u7814\u7a76\u91cf\u5b50\u591a\u4f53\u8fb9\u754c\u7269\u7406\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2602.23582", "categories": ["physics.comp-ph", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.23582", "abs": "https://arxiv.org/abs/2602.23582", "authors": ["Zhen Jiang", "Jiuyang Liang", "Qi Zhou"], "title": "Random batch sum-of-Gaussians method for molecular dynamics simulation of particle systems in the NPT ensemble", "comment": "29 pages, 8 pages", "summary": "In this work, we develop a random batch sum-of-Gaussians (RBSOG) method for molecular dynamics simulations of charged systems in the isothermal-isobaric (NPT) ensemble. We introduce an SOG splitting of the pressure-related $1/r^3$ kernel, yielding a smooth short-/long-range decomposition for instantaneous pressure evaluation. The long-range part is treated in Fourier space by random-batch importance sampling. Because the radial and non-radial pressure components favor different proposals, direct sampling either increases structure-factor evaluations and communication or leads to substantial variance inflation. To address this tradeoff, we introduce a measure-recalibration strategy that reuses Fourier modes drawn from the radial proposal and corrects them for the non-radial target, producing an unbiased pressure estimator with significantly reduced variance and negligible extra cost. The resulting method mitigates pressure artifacts caused by cutoff discontinuities in traditional Ewald-based treatments while preserving near-optimal $O(N)$ complexity. We provide theoretical evidence on pressure decomposition error, consistency of stochastic approximation, and convergence of RBSOG-based MD. Numerical experiments on bulk water, LiTFSI ionic liquids, and DPPC membranes show that RBSOG accurately reproduces key structural and dynamical observables with small batch sizes ($P\\sim 100$). In large-scale benchmarks up to $10^7$ atoms on $2048$ CPU cores, RBSOG achieves about an order-of-magnitude speedup over particle-particle particle-mesh in electrostatic calculations for NPT simulations, together with a consistent $4\\times$ variance reduction relative to random batch Ewald and excellent weak/strong scalability. Overall, RBSOG provides a practical and scalable route to reduce time-to-solution and communication cost in large-scale NPT simulations.", "AI": {"tldr": "\u63d0\u51fa\u968f\u673a\u6279\u91cf\u9ad8\u65af\u53e0\u52a0(RBSOG)\u65b9\u6cd5\u7528\u4e8eNPT\u7cfb\u7efc\u5e26\u7535\u4f53\u7cfb\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\uff0c\u901a\u8fc7\u538b\u529b\u6838SOG\u5206\u88c2\u548c\u6d4b\u5ea6\u91cd\u6821\u51c6\u7b56\u7565\uff0c\u5728\u4fdd\u6301O(N)\u590d\u6742\u5ea6\u7684\u540c\u65f6\u5b9e\u73b010\u500d\u52a0\u901f\u548c4\u500d\u65b9\u5dee\u964d\u4f4e\uff0c\u53ef\u6269\u5c55\u81f3\u5343\u4e07\u539f\u5b50\u89c4\u6a21\u3002", "motivation": "\u4f20\u7edfEwald\u65b9\u6cd5\u5728NPT\u6a21\u62df\u4e2d\u5b58\u5728\u622a\u65ad\u4e0d\u8fde\u7eed\u5bfc\u81f4\u7684\u538b\u529b\u4f2a\u5f71\uff0c\u4e14\u8ba1\u7b97\u901a\u4fe1\u6210\u672c\u9ad8\uff1b\u73b0\u6709\u968f\u673a\u6279\u91cf\u65b9\u6cd5\u5728\u5f84\u5411/\u975e\u5f84\u5411\u538b\u529b\u5206\u91cf\u91c7\u6837\u4e2d\u5b58\u5728\u65b9\u5dee\u4e0e\u8ba1\u7b97\u5f00\u9500\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u63d0\u51faRBSOG\u65b9\u6cd5\uff1a(1)\u5bf91/r^3\u538b\u529b\u6838\u8fdb\u884cSOG\u5206\u88c2\uff0c\u5b9e\u73b0\u5149\u6ed1\u7684\u77ed\u7a0b/\u957f\u7a0b\u5206\u89e3\uff1b(2)\u7528\u968f\u673a\u6279\u91cf\u91cd\u8981\u6027\u91c7\u6837\u5904\u7406\u957f\u7a0b\u90e8\u5206\uff1b(3)\u5f15\u5165\u6d4b\u5ea6\u91cd\u6821\u51c6\u7b56\u7565\uff0c\u91cd\u7528\u5f84\u5411\u63d0\u6848\u7684\u5085\u91cc\u53f6\u6a21\u5e76\u6821\u6b63\u975e\u5f84\u5411\u76ee\u6807\uff0c\u83b7\u5f97\u65e0\u504f\u4e14\u4f4e\u65b9\u5dee\u7684\u538b\u529b\u4f30\u8ba1\u5668\u3002", "result": "\u83b7\u5f97\u8fd1\u6700\u4f18O(N)\u590d\u6742\u5ea6\uff1b\u5728\u4f53\u76f8\u6c34\u3001\u79bb\u5b50\u6db2\u4f53\u548cDPPC\u819c\u4f53\u7cfb\u4e2d\u51c6\u786e\u91cd\u73b0\u5173\u952e\u89c2\u6d4b\u91cf\uff1b\u5343\u4e07\u539f\u5b50\u89c4\u6a21\u4e0b\u5b9e\u73b0\u6bd4\u7c92\u5b50\u7f51\u683c\u6cd510\u500d\u52a0\u901f\uff1b\u76f8\u6bd4\u968f\u673a\u6279\u91cfEwald\u65b9\u5dee\u964d\u4f4e4\u500d\uff1b\u57282048\u6838\u4e0a\u5c55\u73b0\u4f18\u5f02\u5f31/\u5f3a\u6269\u5c55\u6027\u3002", "conclusion": "RBSOG\u4e3a\u5927\u89c4\u6a21NPT\u6a21\u62df\u63d0\u4f9b\u4e86\u5b9e\u7528\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u6c42\u89e3\u65f6\u95f4\u548c\u901a\u4fe1\u6210\u672c\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u538b\u529b\u4f2a\u5f71\u95ee\u9898\u3002"}}
{"id": "2602.23411", "categories": ["cs.CC", "cs.DM", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.23411", "abs": "https://arxiv.org/abs/2602.23411", "authors": ["Yongjian Zhan"], "title": "Microscopic Structure of Random 3-SAT: A Discrete Geometric Approach to Phase Transitions and Algorithmic Complexity", "comment": "13 pages", "summary": "The structural phase transitions and computational complexity of random 3-SAT instances are traditionally described using thermodynamic analogies from statistical physics, such as Replica Symmetry Breaking and energy landscapes. While providing profound macroscopic insights, these theories lack a discrete microscopic structure. In this paper, we propose a complementary, strictly discrete geometric model that maps these phenomena directly to the combinatorial topology of an $N$-dimensional Boolean hypercube. By defining the problem space purely through valid solutions rather than abstract energy states, we establish deterministic mechanics for clustering and freezing, driven by the progressive elimination of vertices and Hamming distance bridges. Furthermore, we derive absolute structural boundaries for 3-SAT, identifying a minimal unsatisfiability limit at constraint density $\u03b1= \\frac{8}{N}$ populated by at least $\\frac{N(N-1)(N-2)}{6}$ distinct unsatisfiable cores, and a maximal satisfiability limit at $\u03b1= \\frac{7}{6}(N-1)(N-2)$ populated by $2^N$ maximal satisfiable instances. These combinatorial extremes mathematically elucidate why the average-case Satisfiability Threshold Conjecture holds only ``almost surely.'' Finally, we apply this topological framework to explain the ``easy-hard-easy'' algorithmic complexity curve. We demonstrate that the efficiency of Depth-First Search is governed by the geometric transition from an abundance of valid search paths (the under-constrained easy phase) to a high density of structurally ``removed variables'' that force immediate contradictions (the over-constrained easy phase). This microscopic perspective bridges theoretical phase transitions with the concrete mechanics of complete search algorithms.", "AI": {"tldr": "A discrete geometric model mapping 3-SAT to Boolean hypercube topology reveals absolute structural limits (minimal unsatisfiability at \u03b1=8/N, maximal satisfiability at \u03b1=(7/6)(N-1)(N-2)), explains why satisfiability thresholds hold \"almost surely,\" and bridges phase transitions with algorithmic complexity through combinatorial geometry of vertex elimination and Hamming distance bridges.", "motivation": "Traditional statistical physics analogies for random 3-SAT lack a discrete microscopic structure to describe phase transitions and computational complexity.", "method": "Proposing a strictly discrete geometric model that maps 3-SAT phenomena to combinatorial topology of N-dimensional Boolean hypercube, defining problem space through valid solutions and establishing deterministic mechanics via progressive vertex elimination and Hamming distance bridges.", "result": "Deriving absolute structural boundaries: minimal unsatisfiability limit at constraint density \u03b1=8/N with at least N(N-1)(N-2)/6 unsatisfiable cores; maximal satisfiability limit at \u03b1=(7/6)(N-1)(N-2) with 2^N maximal satisfiable instances; explaining the \"easy-hard-easy\" algorithmic complexity curve through geometric transitions in Depth-First Search path availability.", "conclusion": "This topological framework provides a microscopic perspective that mathematically elucidates phase transitions and bridges theoretical complexity with concrete search algorithm mechanics."}}
{"id": "2602.23554", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.23554", "abs": "https://arxiv.org/abs/2602.23554", "authors": ["T. R. Kirkpatrick", "D. Belitz"], "title": "Generic Long-Range Order-Parameter Correlations in Metallic Quantum Magnets", "comment": "26pp, 13 figs", "summary": "It is shown that in all types of metallic magnets the coupling of the order parameter to the conduction electrons leads to an order-parameter susceptibility that is long-ranged at zero temperature. This is true for all known classes of ferromagnets, and also for antiferromagnets and spin-density wave systems, helimagnets, magnetic nematics, and altermagnets. The consequences for the magnetic quantum phase transition vary between different classes of magnets. In almost all 3-d systems with a homogeneous magnetization, as well as in magnetic nematics and in altermagnets, the long-ranged correlations generically modify the nature of the magnetic quantum phase transition from second order to first order. The only exception are non-centrosymmetric ferromagnets with a strong spin-orbit interaction, where the correlations change the order of the transition in 2-d systems, but not in 3-d ones. In helimagnets, spin-wave systems, and N{\u00e9}el antiferromagnets their effect is even weaker and does not change the order of the transition if the ordering wave number is sufficiently large. In systems with quenched disorder the transition generically is of second order, but the correlations modify the critical behavior. These conclusions are reached by very simple considerations that are based entirely on the single-particle excitations in the nonmagnetic phase and their modifications by a field conjugate to the order parameter, augmented by renormalization-group considerations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63ed\u793a\u6240\u6709\u91d1\u5c5e\u78c1\u4f53\u4e2d\u5e8f\u53c2\u91cf\u4e0e\u7535\u5b50\u7684\u8026\u5408\u5728\u96f6\u6e29\u4e0b\u4ea7\u751f\u957f\u7a0b\u78c1\u5316\u7387\uff0c\u5e76\u7cfb\u7edf\u7814\u7a76\u4e86\u5176\u5bf9\u78c1\u91cf\u5b50\u76f8\u53d8\u7684\u5f71\u54cd\uff1a\u5728\u591a\u6570\u4e09\u7ef4\u5747\u5300\u78c1\u4f53\u3001\u78c1\u5411\u5217\u76f8\u548c\u4ea4\u9519\u78c1\u4f53\u4e2d\uff0c\u76f8\u53d8\u4ece\u4e8c\u7ea7\u53d8\u4e3a\u4e00\u7ea7\uff1b\u5728\u975e\u4e2d\u5fc3\u5bf9\u79f0\u94c1\u78c1\u4f53\u4e2d\uff0c\u4ec5\u5728\u4e8c\u7ef4\u4f53\u7cfb\u6539\u53d8\u76f8\u53d8\u7ea7\u6570\uff1b\u5728\u5177\u6709\u5927\u6709\u5e8f\u6ce2\u77e2\u7684\u87ba\u65cb\u78c1\u4f53\u3001\u81ea\u65cb\u6ce2\u4f53\u7cfb\u548c\u5948\u5c14\u53cd\u94c1\u78c1\u4f53\u4e2d\u6548\u5e94\u8f83\u5f31\u4e14\u4e0d\u6539\u53d8\u76f8\u53d8\u7ea7\u6570\uff1b\u5b58\u5728\u6dec\u706b\u65e0\u5e8f\u65f6\u76f8\u53d8\u4fdd\u6301\u4e8c\u7ea7\u4f46\u4e34\u754c\u884c\u4e3a\u88ab\u4fee\u6b63\u3002", "motivation": "\u63a2\u7a76\u5e8f\u53c2\u91cf\u4e0e\u5bfc\u7535\u7535\u5b50\u7684\u8026\u5408\u662f\u5426\u4ee5\u53ca\u5982\u4f55\u5f71\u54cd\u5404\u7c7b\u91d1\u5c5e\u78c1\u4f53\u7684\u91cf\u5b50\u76f8\u53d8\u6027\u8d28\u3002", "method": "\u57fa\u4e8e\u975e\u78c1\u76f8\u5355\u7c92\u5b50\u6fc0\u53d1\u53ca\u5176\u5bf9\u5171\u8f6d\u5916\u573a\u7684\u54cd\u5e94\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u5e76\u7ed3\u5408\u91cd\u6b63\u5316\u7fa4\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u957f\u7a0b\u5e8f\u53c2\u91cf\u78c1\u5316\u7387\u7684\u666e\u9002\u5b58\u5728\uff1b\u5efa\u7acb\u4e86\u4e0d\u540c\u78c1\u4f53\u7c7b\u578b\u4e2d\u76f8\u53d8\u7ea7\u6570\u6539\u53d8\u7684\u5206\u7c7b\u89c4\u5f8b\uff1b\u786e\u5b9a\u4e86\u7ef4\u5ea6\u3001\u5bf9\u79f0\u6027\u548c\u6709\u5e8f\u6ce2\u77e2\u5bf9\u76f8\u53d8\u884c\u4e3a\u7684\u5173\u952e\u5f71\u54cd\uff1b\u63ed\u793a\u4e86\u65e0\u5e8f\u4f53\u7cfb\u4e2d\u4e34\u754c\u884c\u4e3a\u7684\u4fee\u6b63\u3002", "conclusion": "\u5e8f\u53c2\u91cf-\u7535\u5b50\u8026\u5408\u662f\u5f71\u54cd\u91d1\u5c5e\u78c1\u4f53\u91cf\u5b50\u76f8\u53d8\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5176\u6548\u5e94\u5177\u6709\u666e\u9002\u6027\u4f46\u5177\u4f53\u8868\u73b0\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u4f53\u7cfb\u7684\u7ed3\u6784\u548c\u5bf9\u79f0\u6027\u7279\u5f81\uff0c\u4e3a\u7406\u89e3\u590d\u6742\u78c1\u4f53\u7684\u76f8\u53d8\u884c\u4e3a\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u3002"}}
{"id": "2602.23400", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23400", "abs": "https://arxiv.org/abs/2602.23400", "authors": ["Zezheng Wu", "Rui Wang", "Xinghe Cheng", "Yang Shao", "Qing Yang", "Jiapu Wang", "Jingwei Zhang"], "title": "U-CAN: Utility-Aware Contrastive Attenuation for Efficient Unlearning in Generative Recommendation", "comment": null, "summary": "Generative Recommendation (GenRec) typically leverages Large Language Models (LLMs) to redefine personalization as an instruction-driven sequence generation task. However, fine-tuning on user logs inadvertently encodes sensitive attributes into model parameters, raising critical privacy concerns. Existing Machine Unlearning (MU) techniques struggle to navigate this tension due to the Polysemy Dilemma, where neurons superimpose sensitive data with general reasoning patterns, leading to catastrophic utility loss under traditional gradient or pruning methods. To address this, we propose Utility-aware Contrastive AttenuatioN (U-CAN), a precision unlearning framework that operates on low-rank adapters. U-CAN quantifies risk by contrasting activations and focuses on neurons with asymmetric responses that are highly sensitive to the forgetting set but suppressed on the retention set. To safeguard performance, we introduce a utility-aware calibration mechanism that combines weight magnitudes with retention-set activation norms, assigning higher utility scores to dimensions that contribute strongly to retention performance. Unlike binary pruning, which often fragments network structure, U-CAN develop adaptive soft attenuation with a differentiable decay function to selectively down-scale high-risk parameters on LoRA adapters, suppressing sensitive retrieval pathways and preserving the topological connectivity of reasoning circuits. Experiments on two public datasets across seven metrics demonstrate that U-CAN achieves strong privacy forgetting, utility retention, and computational efficiency.", "AI": {"tldr": "\u63d0\u51faU-CAN\u6846\u67b6\u89e3\u51b3\u751f\u6210\u5f0f\u63a8\u8350\u4e2d\u7684\u9690\u79c1\u9057\u5fd8\u4e0e\u6548\u7528\u4fdd\u7559\u77db\u76fe\uff0c\u901a\u8fc7\u4f4e\u79e9\u9002\u914d\u5668\u4e0a\u7684\u5bf9\u6bd4\u8870\u51cf\u5b9e\u73b0\u7cbe\u51c6\u673a\u5668\u9057\u5fd8", "motivation": "\u751f\u6210\u5f0f\u63a8\u8350\uff08GenRec\uff09\u901a\u8fc7LLM\u5b9e\u73b0\u4e2a\u6027\u5316\u5e8f\u5217\u751f\u6210\uff0c\u4f46\u7528\u6237\u65e5\u5fd7\u5fae\u8c03\u4f1a\u610f\u5916\u7f16\u7801\u654f\u611f\u5c5e\u6027\u5230\u6a21\u578b\u53c2\u6570\u4e2d\u5f15\u53d1\u9690\u79c1\u98ce\u9669\uff0c\u800c\u73b0\u6709\u673a\u5668\u9057\u5fd8\u6280\u672f\u56e0\u591a\u4e49\u6027\u56f0\u5883\u5bfc\u81f4\u6548\u7528\u4e25\u91cd\u635f\u5931", "method": "\u63d0\u51fa\u6548\u7528\u611f\u77e5\u5bf9\u6bd4\u8870\u51cf\uff08U-CAN\uff09\u6846\u67b6\uff1a\u5728\u4f4e\u79e9\u9002\u914d\u5668\u4e0a\u91cf\u5316\u98ce\u9669\uff0c\u8bc6\u522b\u5bf9\u9057\u5fd8\u96c6\u654f\u611f\u4f46\u5bf9\u4fdd\u7559\u96c6\u6291\u5236\u7684\u975e\u5bf9\u79f0\u54cd\u5e94\u795e\u7ecf\u5143\uff1b\u7ed3\u5408\u6743\u91cd\u5e45\u503c\u4e0e\u4fdd\u7559\u96c6\u6fc0\u6d3b\u8303\u6570\u8bbe\u8ba1\u6548\u7528\u611f\u77e5\u6821\u51c6\u673a\u5236\uff1b\u91c7\u7528\u53ef\u5fae\u5206\u8870\u51cf\u51fd\u6570\u5b9e\u73b0\u81ea\u9002\u5e94\u8f6f\u8870\u51cf\uff0c\u9009\u62e9\u6027\u4e0b\u8c03\u9ad8\u98ce\u9669\u53c2\u6570", "result": "\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u4e03\u9879\u6307\u6807\u5b9e\u9a8c\u8868\u660e\uff0cU-CAN\u5728\u9690\u79c1\u9057\u5fd8\u3001\u6548\u7528\u4fdd\u7559\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u8868\u73b0\u4f18\u5f02", "conclusion": "U-CAN\u901a\u8fc7\u7cbe\u51c6\u6291\u5236\u654f\u611f\u68c0\u7d22\u8def\u5f84\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u7535\u8def\u62d3\u6251\u8fde\u63a5\uff0c\u5b9e\u73b0\u4e86\u5f3a\u9690\u79c1\u9057\u5fd8\u4e0e\u9ad8\u6548\u7528\u4fdd\u7559\u7684\u5e73\u8861"}}
{"id": "2602.24008", "categories": ["cond-mat.stat-mech", "cond-mat.quant-gas", "cond-mat.str-el", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.24008", "abs": "https://arxiv.org/abs/2602.24008", "authors": ["Kazuya Fujimoto", "Taiki Ishiyama", "Taiga Kurose", "Takato Yoshimura", "Tomohiro Sasamoto"], "title": "Exact Anomalous Current Fluctuations in Quantum Many-Body Dynamics", "comment": "35 pages, 4 figures", "summary": "Fluctuations of integrated currents have attracted considerable interest over the past decades in the context of statistical mechanics. Recently, anomalous current fluctuations, characterized by the M-Wright function, were obtained exactly in a classical automaton [$\u017d$. Krajnik et al., Phys. Rev. Lett. 128, 160601 (2022)], and previous studies have shown that the anomalous behavior can arise in a variety of classical systems. Despite the rapidly growing interest in such anomalous behaviors, which capture a universal aspect of one-dimensional many-body transport, the exact derivation of the M-Wright function in quantum many-body systems has remained elusive. In this Letter, we present the first exact microscopic derivation of the M-Wright function in quantum many-body dynamics by analyzing the integrated spin current in a one-dimensional Fermi-Hubbard model with infinitely strong repulsive interactions. Our results lay the groundwork for exploring anomalous integrated currents in a broad class of quantum many-body systems.", "AI": {"tldr": "\u9996\u6b21\u5728\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u4e2d\u7cbe\u786e\u63a8\u5bfc\u51fa\u63cf\u8ff0\u53cd\u5e38\u7535\u6d41\u6da8\u843d\u7684M-Wright\u51fd\u6570\uff0c\u4e3a\u7814\u7a76\u91cf\u5b50\u591a\u4f53\u8f93\u8fd0\u53cd\u5e38\u73b0\u8c61\u5960\u5b9a\u57fa\u7840", "motivation": "\u4e00\u7ef4\u591a\u4f53\u8f93\u8fd0\u4e2d\u5b58\u5728\u666e\u9002\u6027\u53cd\u5e38\u7535\u6d41\u6da8\u843d\u73b0\u8c61\uff08\u7531M-Wright\u51fd\u6570\u63cf\u8ff0\uff09\uff0c\u4f46\u6b64\u524d\u4ec5\u5728\u7ecf\u5178\u7cfb\u7edf\u4e2d\u7cbe\u786e\u6c42\u89e3\uff0c\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u4e2d\u7684\u7cbe\u786e\u63a8\u5bfc\u4ecd\u5c5e\u96be\u9898", "method": "\u901a\u8fc7\u5206\u6790\u65e0\u9650\u5f3a\u6392\u65a5\u76f8\u4e92\u4f5c\u7528\u4e00\u7ef4\u8d39\u7c73-\u54c8\u4f2f\u5fb7\u6a21\u578b\u4e2d\u7684\u79ef\u5206\u81ea\u65cb\u7535\u6d41\uff0c\u8fdb\u884c\u5fae\u89c2\u63a8\u5bfc", "result": "\u5728\u91cf\u5b50\u591a\u4f53\u52a8\u529b\u5b66\u4e2d\u9996\u6b21\u83b7\u5f97M-Wright\u51fd\u6570\u7684\u7cbe\u786e\u5fae\u89c2\u63a8\u5bfc\uff0c\u8bc1\u5b9e\u8be5\u53cd\u5e38\u884c\u4e3a\u5728\u91cf\u5b50\u4f53\u7cfb\u4e2d\u540c\u6837\u5b58\u5728", "conclusion": "\u8be5\u7ed3\u679c\u4e3a\u5728\u5404\u7c7b\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u4e2d\u63a2\u7d22\u53cd\u5e38\u79ef\u5206\u7535\u6d41\u73b0\u8c61\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840"}}
{"id": "2602.23491", "categories": ["quant-ph", "physics.hist-ph"], "pdf": "https://arxiv.org/pdf/2602.23491", "abs": "https://arxiv.org/abs/2602.23491", "authors": ["Gy\u0151z\u0151 Egri", "Marton Gomori", "Balazs Gyenis", "G\u00e1bor Hofer-Szab\u00f3"], "title": "Trajectory of Probabilities, Probability on Trajectories, and the Stochastic-Quantum Correspondence", "comment": null, "summary": "The probabilistic description of the time evolution of a physical system can take two conceptually distinct forms: a trajectory of probabilities, which specifies how probabilities evolve over time, and a probability on trajectories, which assigns probabilities to possible histories. A lack of a clear distinction between these two probabilistic descriptions has given rise to a number of conceptual difficulties, particularly in recent analyses of stochastic-quantum correspondence. This paper provides a systematic account of their relationship. We define probability dynamics and stochastic process families together with a precise notion of implementation that connects the two descriptions. We show that implementations are generically non-unique, that every probability dynamics admits a Markovian implementation, and characterize when non-Markovian implementations are possible. We expose fallacies in common arguments for the linearity of probability dynamics based on the law of total probability and clarify the proper interpretation of ``transition matrices'' by distinguishing dynamics-level maps from the conditional probability matrices of implementing processes. We further introduce decomposability as the appropriate general notion of stepwise evolution for (possibly nonlinear) probability dynamics, relate it to divisibility in the linear case -- showing that the two can come apart -- and disentangle both notions from Markovianity and time-homogeneity. Finally, we connect these results to what we call statistical dynamics, in which linearity is indeed physically motivated, and contrast the framework with quantum mechanics.", "AI": {"tldr": "This paper systematically clarifies the distinction between two probabilistic descriptions of physical systems (probability trajectories vs. probability on trajectories) by defining probability dynamics and stochastic implementations, showing implementations are generically non-unique, every dynamics admits a Markovian implementation, and exposing fallacies in common arguments for linearity.", "motivation": "The lack of clear distinction between probability trajectories and probability on trajectories has caused conceptual difficulties, particularly in stochastic-quantum correspondence. A systematic account of their relationship is needed to resolve these issues.", "method": "Defines probability dynamics and stochastic process families with a precise notion of implementation connecting them. Distinguishes dynamics-level maps from conditional probability matrices. Introduces decomposability as general stepwise evolution and relates it to divisibility in linear cases.", "result": "Shows implementations are generically non-unique; every probability dynamics admits a Markovian implementation; characterizes when non-Markovian implementations exist; exposes fallacies in linearity arguments based on total probability; clarifies transition matrix interpretation; demonstrates decomposability and divisibility can diverge; disentangles both from Markovianity and time-homogeneity.", "conclusion": "The framework resolves conceptual difficulties in stochastic-quantum correspondence by clarifying the relationship between probabilistic descriptions, distinguishing key concepts, and offering proper interpretation of transition matrices, while contrasting with quantum mechanics where linearity is physically motivated."}}
{"id": "2602.24087", "categories": ["physics.comp-ph", "astro-ph.IM", "hep-ex", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2602.24087", "abs": "https://arxiv.org/abs/2602.24087", "authors": ["Luca Di Bella", "Jan B\u00fcrger", "Markus Demleitner", "Torsten En\u00dflin", "Johannes Erdmann", "Martin Erdmann", "Benjamin Fischer", "Martin Gasthuber", "Gabriele Gramelsberger", "Wolfgang Gr\u00fcndinger", "Prateek Gupta", "Johannes Hartl", "Maximilian Horzela", "Vijay Kartik", "Stefan Krischer", "Eva Kr\u00f6ll", "Thomas Kuhr", "Katharina K\u00fcrschner", "Inga Lakomiec", "Valerie Lang", "Kristin Lohwasser", "Thomas Metcalf", "Martin M\u00f6ller", "Saskia Nagel", "Susanne Pfalzner", "Rebecca Redlin", "Christopher Schrader", "Kathrin Schulz", "Markus Schumacher", "Kilian Schwarz", "Fabian Sigler", "Dwayne Spiteri", "Achim Stahl", "Judith Steinfeld", "Wim Vanderbauwhede", "Cyrus Walther", "Angela Warkentin", "Peter Wissmann", "Eoin Woods"], "title": "Shaping the Digital Future of ErUM Research: Sustainability & Ethics", "comment": "32 pages, no figures, report for workshop \"Shaping the Digital Future of ErUM Research: Sustainability & Ethics\", 28 July to 1 August 2025, Aachen, Germany, see https://indico.desy.de/event/47133/", "summary": "This workshop report from \"Shaping the Digital Future of ErUM Research: Sustainability & Ethics\" (Aachen, 2025) reviews progress on sustainability measures in data-intensive ErUM-Data research since the 2023 call-to-action on resource-aware research. It evaluates short-, medium-, and long-term actions around monitoring and reducing CO2 emissions, improving data and software FAIRness, optimizing workflows and computing infrastructures, and aligning operations with low-carbon energy availability, including concepts such as \"breathing\" computing centers, long-term data storage strategies, and software efficiency certification. The report stresses the need for systematic teaching, training, mentoring, and new support formats to establish sustainable coding and computing practices, particularly among students and early-career researchers, and highlights the importance of dedicated steering and funding instruments to embed sustainability in project planning. Ethical discussions focus on the transformative use of AI in ErUM-Data, addressing autonomy, bias, transparency, explainability, attribution of responsibility, and the risk of deskilling, while reaffirming that accountability for scientific outcomes remains with human researchers. Finally, the report emphasizes that sustainable transformation requires not only technical measures but also targeted awareness-building, communication strategies, incentives, and community-driven initiatives to move from awareness to action and to integrate sustainability and ethics into everyday scientific practice.", "AI": {"tldr": "2025\u5e74\u57c3\u6717\u6839\u7814\u8ba8\u4f1a\u62a5\u544a\u56de\u987e\u81ea2023\u5e74\u8d44\u6e90\u611f\u77e5\u7814\u7a76\u5021\u8bae\u4ee5\u6765\uff0c\u6570\u636e\u5bc6\u96c6\u578bErUM\u7814\u7a76\u7684\u53ef\u6301\u7eed\u8fdb\u5c55\uff0c\u8bc4\u4f30\u4e86\u51cf\u5c11CO2\u6392\u653e\u3001\u63d0\u9ad8\u6570\u636e\u8f6f\u4ef6FAIR\u6027\u3001\u4f18\u5316\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\u3001AI\u4f26\u7406\u7b49\u77ed\u4e2d\u957f\u671f\u884c\u52a8\uff0c\u5f3a\u8c03\u9700\u901a\u8fc7\u57f9\u8bad\u3001\u8d44\u91d1\u673a\u5236\u548c\u793e\u533a\u5021\u8bae\u5c06\u53ef\u6301\u7eed\u6027\u4e0e\u4f26\u7406\u5d4c\u5165\u65e5\u5e38\u79d1\u7814\u5b9e\u8df5\u3002", "motivation": "\u8bc4\u4f30\u6570\u636e\u5bc6\u96c6\u578bErUM\u7814\u7a76\u5728\u53ef\u6301\u7eed\u6027\u548c\u4f26\u7406\u65b9\u9762\u7684\u8fdb\u5c55\uff0c\u89e3\u51b3\u8d44\u6e90\u6d88\u8017\u3001AI\u5e94\u7528\u5e26\u6765\u7684\u4f26\u7406\u6311\u6218\uff0c\u63a8\u52a8\u79d1\u7814\u5b9e\u8df5\u4ece\u610f\u8bc6\u5230\u884c\u52a8\u7684\u8f6c\u53d8\u3002", "method": "\u901a\u8fc7\u7814\u8ba8\u4f1a\u5f62\u5f0f\uff0c\u7cfb\u7edf\u8bc4\u4f30\u77ed\u3001\u4e2d\u3001\u957f\u671f\u884c\u52a8\u65b9\u6848\uff0c\u6db5\u76d6CO2\u76d1\u6d4b\u4e0e\u51cf\u6392\u3001FAIR\u539f\u5219\u5b9e\u65bd\u3001\u5de5\u4f5c\u6d41\u4e0e\u57fa\u7840\u8bbe\u65bd\u4f18\u5316\u3001\u4f4e\u80fd\u8017\u8ba1\u7b97\u4e2d\u5fc3(\"\u547c\u5438\"\u4e2d\u5fc3)\u3001\u8f6f\u4ef6\u6548\u7387\u8ba4\u8bc1\u3001AI\u4f26\u7406\u6846\u67b6\u7b49\u7ef4\u5ea6\u3002", "result": "\u63d0\u51fa\u9700\u5efa\u7acb\u7cfb\u7edf\u6027\u6559\u5b66\u4e0e\u57f9\u8bad\u4f53\u7cfb\uff0c\u5f00\u53d1\u4e13\u9879\u6307\u5bfc\u4e0e\u8d44\u52a9\u5de5\u5177\uff0c\u63a8\u5e7f\"\u547c\u5438\u5f0f\"\u8ba1\u7b97\u4e2d\u5fc3\u548c\u8f6f\u4ef6\u6548\u7387\u8ba4\u8bc1\uff0c\u660e\u786eAI\u5e94\u7528\u4e2d\u7684\u4eba\u7c7b\u95ee\u8d23\u5236\uff0c\u5e76\u5f3a\u8c03\u6280\u672f\u63aa\u65bd\u4e0e\u6fc0\u52b1\u3001\u4f20\u64ad\u7b56\u7565\u76f8\u7ed3\u5408\u3002", "conclusion": "\u5b9e\u73b0\u79d1\u7814\u53ef\u6301\u7eed\u8f6c\u578b\u9700\u6280\u672f\u3001\u6559\u80b2\u548c\u793e\u533a\u4e09\u7ba1\u9f50\u4e0b\uff0c\u901a\u8fc7\u76ee\u6807\u660e\u786e\u7684\u610f\u8bc6\u5efa\u8bbe\u3001\u6c9f\u901a\u7b56\u7565\u3001\u6fc0\u52b1\u673a\u5236\u548c\u793e\u533a\u9a71\u52a8\u5021\u8bae\uff0c\u5c06\u53ef\u6301\u7eed\u6027\u4e0e\u4f26\u7406\u539f\u5219\u6df1\u5ea6\u878d\u5165\u65e5\u5e38\u79d1\u7814\u5b9e\u8df5\u3002"}}
{"id": "2602.23503", "categories": ["cs.CC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23503", "abs": "https://arxiv.org/abs/2602.23503", "authors": ["Lianna Hambardzumyan", "Konstantin Myasnikov", "Artur Riazanov", "Morgan Shirley", "Adi Shraibman"], "title": "Spiky Rank and Its Applications to Rigidity and Circuits", "comment": null, "summary": "We introduce spiky rank, a new matrix parameter that enhances blocky rank by combining the combinatorial structure of the latter with linear-algebraic flexibility. A spiky matrix is block-structured with diagonal blocks that are arbitrary rank-one matrices, and the spiky rank of a matrix is the minimum number of such matrices required to express it as a sum. This measure extends blocky rank to real matrices and is more robust for problems with both combinatorial and algebraic character.\n  Our conceptual contribution is as follows: we propose spiky rank as a well-behaved candidate matrix complexity measure and demonstrate its potential through applications. We show that large spiky rank implies high matrix rigidity, and that spiky rank lower bounds yield lower bounds for depth-2 ReLU circuits, the basic building blocks of neural networks. On the technical side, we establish tight bounds for random matrices and develop a framework for explicit lower bounds, applying it to Hamming distance matrices and spectral expanders. Finally, we relate spiky rank to other matrix parameters, including blocky rank, sparsity, and the $\u03b3_2$-norm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faspiky rank\uff0c\u4e00\u79cd\u65b0\u7684\u77e9\u9635\u590d\u6742\u5ea6\u5ea6\u91cf\uff0c\u5b83\u901a\u8fc7\u7ed3\u5408blocky rank\u7684\u7ec4\u5408\u7ed3\u6784\u548c\u7ebf\u6027\u4ee3\u6570\u7075\u6d3b\u6027\u6765\u589e\u5f3a\u540e\u8005\uff0c\u5e76\u5e94\u7528\u4e8e\u77e9\u9635\u521a\u6027\u548c\u6df1\u5ea6-2 ReLU\u7535\u8def\u4e0b\u754c\u3002", "motivation": "\u4e3a\u4e86\u5904\u7406\u540c\u65f6\u5177\u6709\u7ec4\u5408\u548c\u4ee3\u6570\u7279\u6027\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u77e9\u9635\u590d\u6742\u5ea6\u5ea6\u91cf\u3002Spiky rank\u65e8\u5728\u6269\u5c55blocky rank\u81f3\u5b9e\u77e9\u9635\uff0c\u5e76\u63d0\u4f9b\u66f4\u597d\u7684\u7406\u8bba\u6027\u8d28\u3002", "method": "\u5b9a\u4e49spiky rank\u4e3a\u7528\u5bf9\u89d2\u5757\u4e3a\u4efb\u610f\u79e9-1\u77e9\u9635\u7684\u5206\u5757\u77e9\u9635\u8868\u793a\u76ee\u6807\u77e9\u9635\u6240\u9700\u7684\u6700\u5c11\u9879\u6570\uff1b\u5efa\u7acb\u4e0e\u77e9\u9635\u521a\u6027\u548c\u7535\u8def\u590d\u6742\u5ea6\u7684\u7406\u8bba\u8054\u7cfb\uff1b\u63a8\u5bfc\u968f\u673a\u77e9\u9635\u7684\u7d27\u754c\uff1b\u5f00\u53d1\u663e\u5f0f\u4e0b\u754c\u6846\u67b6\u5e76\u5e94\u7528\u4e8eHamming\u8ddd\u79bb\u77e9\u9635\u548c\u8c31\u6269\u5c55\u5668\uff1b\u6bd4\u8f83spiky rank\u4e0e\u5176\u4ed6\u77e9\u9635\u53c2\u6570\u3002", "result": "1) \u9ad8spiky rank\u8574\u542b\u9ad8\u77e9\u9635\u521a\u6027\uff1b2) spiky rank\u4e0b\u754c\u7ed9\u51fa\u6df1\u5ea6-2 ReLU\u7535\u8def\u89c4\u6a21\u4e0b\u754c\uff1b3) \u83b7\u5f97\u968f\u673a\u77e9\u9635\u7684\u7d27spiky rank\u754c\uff1b4) \u5bf9Hamming\u8ddd\u79bb\u77e9\u9635\u548c\u8c31\u6269\u5c55\u5668\u5efa\u7acb\u663e\u5f0f\u4e0b\u754c\uff1b5) \u63ed\u793aspiky rank\u4e0eblocky rank\u3001\u7a00\u758f\u6027\u3001\u03b3\u2082-\u8303\u6570\u7684\u5173\u7cfb\u3002", "conclusion": "Spiky rank\u4f5c\u4e3a\u77e9\u9635\u590d\u6742\u5ea6\u5ea6\u91cf\u662f\u6709\u6548\u7684\uff0c\u5b83\u5728\u7406\u8bba\u8ba1\u7b97\u673a\u79d1\u5b66\uff08\u7279\u522b\u662f\u7535\u8def\u590d\u6742\u5ea6\u548c\u77e9\u9635\u521a\u6027\uff09\u4e2d\u5177\u6709\u5e94\u7528\u4ef7\u503c\uff0c\u5e76\u4e3a\u76f8\u5173\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2602.23600", "categories": ["cond-mat.str-el", "cond-mat.mes-hall", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23600", "abs": "https://arxiv.org/abs/2602.23600", "authors": ["Li Chen", "Zhiping Yao"], "title": "Second-quantized approach to the study of Halperin state in fractional quantum Hall effect", "comment": "9 pages", "summary": "We give a recursion relation for the second-quantized fermionic (bosonic) Halperin state, which avoids exact diagonalization of its two-component first-quantized parent Hamiltonian. We validate this formula by proving that the second-quantized Halperin state, as recursively defined in this formula, is indeed a zero mode of the corresponding second-quantized parent Hamiltonian and that it has the correct filling factor.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u4e8c\u6b21\u91cf\u5b50\u5316\u8d39\u7c73\uff08\u73bb\u8272\uff09Halperin\u6001\u7684\u9012\u63a8\u5173\u7cfb\uff0c\u907f\u514d\u5bf9\u5176\u53cc\u5206\u91cf\u4e00\u6b21\u91cf\u5b50\u5316\u7236\u54c8\u5bc6\u987f\u91cf\u8fdb\u884c\u7cbe\u786e\u5bf9\u89d2\u5316\uff0c\u5e76\u901a\u8fc7\u8bc1\u660e\u8be5\u9012\u63a8\u5b9a\u4e49\u7684\u72b6\u6001\u662f\u7236\u54c8\u5bc6\u987f\u91cf\u7684\u96f6\u6a21\u4e14\u5177\u6709\u6b63\u786e\u586b\u5145\u56e0\u5b50\u6765\u9a8c\u8bc1\u516c\u5f0f\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u7cbe\u786e\u5bf9\u89d2\u5316\u65b9\u6cd5\u5728\u5904\u7406\u53cc\u5206\u91cf\u4e00\u6b21\u91cf\u5b50\u5316\u7236\u54c8\u5bc6\u987f\u91cf\u65f6\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7684\u95ee\u9898\uff0c\u5bfb\u6c42\u66f4\u9ad8\u6548\u7684\u6570\u5b66\u63cf\u8ff0\u65b9\u5f0f\u3002", "method": "\u6784\u5efa\u4e8c\u6b21\u91cf\u5b50\u5316\u6846\u67b6\u4e0b\u7684\u9012\u63a8\u5173\u7cfb\uff0c\u901a\u8fc7\u9012\u5f52\u65b9\u5f0f\u751f\u6210Halperin\u6001\uff0c\u7ed5\u5f00\u76f4\u63a5\u5bf9\u89d2\u5316\u7236\u54c8\u5bc6\u987f\u91cf\u3002", "result": "\u9a8c\u8bc1\u4e86\u9012\u63a8\u5b9a\u4e49\u7684Halperin\u6001\u662f\u7236\u54c8\u5bc6\u987f\u91cf\u7684\u96f6\u6a21\uff08\u80fd\u91cf\u672c\u5f81\u503c\u4e3a\u96f6\uff09\uff0c\u5e76\u786e\u8ba4\u5176\u6ee1\u8db3\u7406\u8bba\u9884\u671f\u7684\u586b\u5145\u56e0\u5b50\u3002", "conclusion": "\u6240\u63d0\u9012\u63a8\u516c\u5f0f\u6709\u6548\u89c4\u907f\u4e86\u7cbe\u786e\u5bf9\u89d2\u5316\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u4e3aHalperin\u6001\u7684\u89e3\u6790\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7269\u7406\u7279\u6027\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2602.23409", "categories": ["cs.LG", "cs.AI", "cs.ET", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23409", "abs": "https://arxiv.org/abs/2602.23409", "authors": ["Michael Poppel", "Jonas Stein", "Sebastian W\u00f6lckert", "Markus Baumann", "Claudia Linnhoff-Popien"], "title": "Long Range Frequency Tuning for QML", "comment": null, "summary": "Quantum machine learning models using angle encoding naturally represent truncated Fourier series, providing universal function approximation capabilities with sufficient circuit depth. For unary fixed-frequency encodings, circuit depth scales as O(omega_max * (omega_max + epsilon^{-2})) with target frequency magnitude omega_max and precision epsilon. Trainable-frequency approaches theoretically reduce this to match the target spectrum size, requiring only as many encoding gates as frequencies in the target spectrum. Despite this compelling efficiency, their practical effectiveness hinges on a key assumption: that gradient-based optimization can drive prefactors to arbitrary target values. We demonstrate through systematic experiments that frequency prefactors exhibit limited trainability: movement is constrained to approximately +/-1 units with typical learning rates. When target frequencies lie outside this reachable range, optimization frequently fails. To overcome this frequency reachability limitation, we propose grid-based initialization using ternary encodings, which generate dense integer frequency spectra. While this approach requires O(log_3(omega_max)) encoding gates -- more than the theoretical optimum but exponentially fewer than fixed-frequency methods -- it ensures target frequencies lie within the locally reachable range. On synthetic targets with three shifted high frequencies, ternary grid initialization achieves a median R^2 score of 0.9969, compared to 0.1841 for the trainable-frequency baseline. For the real-world Flight Passengers dataset, ternary grid initialization achieves a median R^2 score of 0.9671, representing a 22.8% improvement over trainable-frequency initialization (median R^2 = 0.7876).", "AI": {"tldr": "\u53ef\u8bad\u7ec3\u9891\u7387\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5728\u5b9e\u9645\u4e2d\u56e0\u9891\u7387\u9884\u56e0\u5b50\u8bad\u7ec3\u53d7\u9650\u800c\u5931\u8d25\uff1b\u4e09\u5143\u7f51\u683c\u521d\u59cb\u5316\u7528O(log\u2083(\u03c9_max))\u95e8\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u5728\u5408\u6210\u6570\u636e\u4e0a\u8fbe\u52300.997 R\u00b2\uff0c\u771f\u5b9e\u6570\u636e\u4e0a\u8fbe\u52300.967 R\u00b2\u3002", "motivation": "\u53ef\u8bad\u7ec3\u9891\u7387\u7f16\u7801\u7684\u7406\u8bba\u6548\u7387\u5047\u8bbe\u68af\u5ea6\u4e0b\u964d\u53ef\u4efb\u610f\u4f18\u5316\u9891\u7387\u9884\u56e0\u5b50\uff0c\u4f46\u8fd9\u4e00\u5047\u8bbe\u672a\u7ecf\u68c0\u9a8c\u4e14\u53ef\u80fd\u9519\u8bef\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u5b9e\u9a8c\u8bc6\u522b\u8bad\u7ec3\u9650\u5236\uff08\u7ea6\u00b11\u5355\u4f4d\u8303\u56f4\uff09\uff0c\u7136\u540e\u63d0\u51fa\u4e09\u5143\u7f51\u683c\u521d\u59cb\u5316\uff0c\u751f\u6210\u5bc6\u96c6\u6574\u6570\u9891\u7387\u8c31\uff0c\u786e\u4fdd\u76ee\u6807\u9891\u7387\u5728\u5c40\u90e8\u53ef\u8fbe\u8303\u56f4\u5185\u3002", "result": "\u4e09\u5143\u7f51\u683c\u521d\u59cb\u5316\u5728\u542b\u4e09\u4e2a\u9ad8\u9891\u7684\u5408\u6210\u76ee\u6807\u4e0a\u4e2d\u4f4dR\u00b2\u8fbe0.9969\uff08\u5bf9\u6bd4\u57fa\u7ebf0.1841\uff09\uff0c\u5728Flight Passengers\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u4e2d\u4f4dR\u00b2\u8fbe0.9671\uff08\u5bf9\u6bd4\u57fa\u7ebf0.7876\uff0c\u63d0\u534722.8%\uff09\u3002", "conclusion": "\u9891\u7387\u53ef\u8fbe\u6027\u95ee\u9898\u771f\u5b9e\u5b58\u5728\u4e14\u53ef\u901a\u8fc7\u7f51\u683c\u521d\u59cb\u5316\u514b\u670d\uff0c\u4f7f\u53ef\u8bad\u7ec3\u9891\u7387\u7f16\u7801\u4ee5\u4ec5O(log\u2083(\u03c9_max))\u95e8\u6570\u5b9e\u73b0\u5b9e\u9645\u53ef\u884c\u6027\u3002"}}
{"id": "2602.24242", "categories": ["cond-mat.stat-mech", "cond-mat.quant-gas", "cond-mat.str-el", "nlin.SI"], "pdf": "https://arxiv.org/pdf/2602.24242", "abs": "https://arxiv.org/abs/2602.24242", "authors": ["Takato Yoshimura", "\u017diga Krajnik", "Alvise Bastianello", "Enej Ilievski"], "title": "Anomalous hydrodynamic fluctuations in the quantum XXZ spin chain", "comment": "9+2 pages, 3 figures", "summary": "The quantum XXZ spin-1/2 chain features non-Gaussian spin current fluctuations in the regime of easy-axis anisotropy. Using ballistic macroscopic fluctuation theory, we derive the exact probability distribution of typical spin-current fluctuations in thermal equilibrium. The obtained nested Gaussian distribution is fully characterized by its variance which we analytically relate to the spin diffusion constant and static spin susceptibility, and compare our with numerical simulations. By unveiling how the same mechanism which leads to anomalous charge current fluctuations in single-file systems manifests in the XXZ chain, our approach establishes the universal hydrodynamic origin of the observed anomalous fluctuations.", "AI": {"tldr": "\u91cf\u5b50XXZ\u81ea\u65cb-1/2\u94fe\u5728\u6613\u8f74\u5404\u5411\u5f02\u6027\u533a\u57df\u5448\u73b0\u975e\u9ad8\u65af\u81ea\u65cb\u6d41\u5f02\u5e38\u6da8\u843d\uff0c\u5176\u5206\u5e03\u4e3a\u5d4c\u5957\u9ad8\u65af\u578b\uff0c\u65b9\u5dee\u4e0e\u81ea\u65cb\u6269\u6563\u5e38\u6570\u548c\u9759\u6001\u81ea\u65cb\u78c1\u5316\u7387\u76f8\u5173\uff0c\u63ed\u793a\u4e86\u4e0e\u5355\u5217\u7cfb\u7edf\u7535\u8377\u6d41\u6da8\u843d\u7684\u666e\u9002\u6d41\u4f53\u529b\u5b66\u8d77\u6e90\u3002", "motivation": "\u7814\u7a76\u91cf\u5b50XXZ\u81ea\u65cb-1/2\u94fe\u5728\u6613\u8f74\u5404\u5411\u5f02\u6027\u4e0b\u7684\u975e\u9ad8\u65af\u81ea\u65cb\u6d41\u6da8\u843d\u73b0\u8c61\uff0c\u9610\u660e\u5176\u7269\u7406\u8d77\u6e90\uff0c\u5e76\u4e0e\u5355\u5217\u7cfb\u7edf\u4e2d\u7684\u5f02\u5e38\u7535\u8377\u6d41\u6da8\u843d\u5efa\u7acb\u8054\u7cfb\u3002", "method": "\u5e94\u7528\u5f39\u9053\u5b8f\u89c2\u6da8\u843d\u7406\u8bba\u63a8\u5bfc\u70ed\u5e73\u8861\u6001\u4e0b\u5178\u578b\u81ea\u65cb\u6d41\u6da8\u843d\u7684\u7cbe\u786e\u6982\u7387\u5206\u5e03\u3002", "result": "\u83b7\u5f97\u5d4c\u5957\u9ad8\u65af\u5206\u5e03\uff0c\u5176\u65b9\u5dee\u53ef\u89e3\u6790\u5730\u4e0e\u81ea\u65cb\u6269\u6563\u5e38\u6570\u548c\u9759\u6001\u81ea\u65cb\u78c1\u5316\u7387\u76f8\u5173\u8054\uff0c\u5e76\u4e0e\u6570\u503c\u6a21\u62df\u7ed3\u679c\u76f8\u7b26\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86XXZ\u94fe\u4e2d\u89c2\u5bdf\u5230\u7684\u5f02\u5e38\u6da8\u843d\u4e0e\u5355\u5217\u7cfb\u7edf\u7535\u8377\u6d41\u5f02\u5e38\u6da8\u843d\u5177\u6709\u76f8\u540c\u7684\u6d41\u4f53\u529b\u5b66\u673a\u5236\uff0c\u786e\u7acb\u4e86\u5176\u666e\u9002\u6027\u6d41\u4f53\u529b\u5b66\u8d77\u6e90\u3002"}}
{"id": "2602.23541", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23541", "abs": "https://arxiv.org/abs/2602.23541", "authors": ["Arvind Raghavan", "Elias Bareinboim"], "title": "Causal Identification from Counterfactual Data: Completeness and Bounding Results", "comment": null, "summary": "Previous work establishing completeness results for $\\textit{counterfactual identification}$ has been circumscribed to the setting where the input data belongs to observational or interventional distributions (Layers 1 and 2 of Pearl's Causal Hierarchy), since it was generally presumed impossible to obtain data from counterfactual distributions, which belong to Layer 3. However, recent work (Raghavan & Bareinboim, 2025) has formally characterized a family of counterfactual distributions which can be directly estimated via experimental methods - a notion they call $\\textit{counterfactual realizabilty}$. This leaves open the question of what $\\textit{additional}$ counterfactual quantities now become identifiable, given this new access to (some) Layer 3 data. To answer this question, we develop the CTFIDU+ algorithm for identifying counterfactual queries from an arbitrary set of Layer 3 distributions, and prove that it is complete for this task. Building on this, we establish the theoretical limit of which counterfactuals can be identified from physically realizable distributions, thus implying the $\\textit{fundamental limit to exact causal inference in the non-parametric setting}$. Finally, given the impossibility of identifying certain critical types of counterfactuals, we derive novel analytic bounds for such quantities using realizable counterfactual data, and corroborate using simulations that counterfactual data helps tighten the bounds for non-identifiable quantities in practice.", "AI": {"tldr": "This paper extends counterfactual identification beyond observational/interventional data by introducing CTFIDU+ algorithm that uses experimentally realizable Layer 3 counterfactual distributions, proving its completeness, establishing theoretical limits for causal inference, and deriving practical bounds for non-identifiable quantities.", "motivation": "Previous work on counterfactual identification was limited to Layers 1 and 2 of Pearl's Causal Hierarchy (observational/interventional data). Recent work introduced \"counterfactual realizability\" - some Layer 3 distributions can be experimentally estimated. This opens the question: what additional counterfactual quantities become identifiable with Layer 3 data access?", "method": "Developed CTFIDU+ algorithm for identifying counterfactual queries from arbitrary Layer 3 distributions, and derived novel analytic bounds for non-identifiable quantities using realizable counterfactual data.", "result": "CTFIDU+ is proven complete for this task. The paper establishes theoretical limits of counterfactual identification from physically realizable distributions (fundamental limit to exact causal inference). Simulations confirm counterfactual data tightens bounds for non-identifiable quantities in practice.", "conclusion": "There exists a fundamental limit to exact causal inference in the non-parametric setting, but access to realizable counterfactual distributions enables identification of additional quantities and provides tighter bounds for those that remain non-identifiable."}}
{"id": "2502.01476", "categories": ["cs.LG", "math.NA", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2502.01476", "abs": "https://arxiv.org/abs/2502.01476", "authors": ["Orestis Oikonomou", "Levi Lingsch", "Dana Grund", "Siddhartha Mishra", "Georgios Kissas"], "title": "Neuro-Symbolic AI for Analytical Solutions of Differential Equations", "comment": "Updates the method and added extra results", "summary": "Analytical solutions to differential equations offer exact, interpretable insight but are rarely available because discovering them requires expert intuition or exhaustive search in combinatorial spaces. We introduce SIGS, a neuro-symbolic framework that automates this process. SIGS uses a formal grammar to generate only syntactically valid building blocks, embeds these expressions into a continuous space, and then searches this space to assemble, score, and refine candidate closed-form solutions by minimizing a physics-based residual. This design unifies symbolic reasoning with numerical optimization; the grammar constrains candidate solution blocks to be proper by construction, while the latent search makes exploration tractable and data-free. SIGS is the first neuro-symbolic method to (i) analytically solve coupled systems of nonlinear PDEs, (ii) discover solutions under grammar misspecification, and (iii) produce accurate symbolic approximations for PDEs lacking known closed-form solutions. Overall, SIGS achieves orders-of-magnitude improvements in accuracy and efficiency over existing symbolic methods on standard benchmarks.", "AI": {"tldr": "SIGS\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u8bed\u6cd5\u751f\u6210\u6709\u6548\u8868\u8fbe\u5f0f\u5757\uff0c\u5728\u8fde\u7eed\u7a7a\u95f4\u4e2d\u641c\u7d22\u5e76\u4f18\u5316\u5019\u9009\u89e3\uff0c\u9996\u6b21\u5b9e\u73b0\u8026\u5408\u975e\u7ebf\u6027PDEs\u7684\u89e3\u6790\u6c42\u89e3\u3001\u8bed\u6cd5\u8bef\u8bbe\u4e0b\u7684\u89e3\u53d1\u73b0\uff0c\u4ee5\u53ca\u5bf9\u65e0\u95ed\u5f0f\u89e3PDEs\u7684\u7b26\u53f7\u8fd1\u4f3c\uff0c\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7b26\u53f7\u65b9\u6cd5\u3002", "motivation": "\u89e3\u6790\u6c42\u89e3\u5fae\u5206\u65b9\u7a0b\u80fd\u63d0\u4f9b\u7cbe\u786e\u53ef\u89e3\u91ca\u7684\u6d1e\u89c1\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e13\u5bb6\u76f4\u89c9\u6216\u7ec4\u5408\u7a7a\u95f4\u7a77\u4e3e\u641c\u7d22\uff0c\u6548\u7387\u4f4e\u4e0b\u4e14\u96be\u4ee5\u81ea\u52a8\u5316\u3002", "method": "SIGS\u91c7\u7528\u5f62\u5f0f\u5316\u8bed\u6cd5\u751f\u6210\u8bed\u6cd5\u6b63\u786e\u7684\u8868\u8fbe\u5f0f\u5757\uff0c\u5c06\u5176\u5d4c\u5165\u8fde\u7eed\u7a7a\u95f4\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u7269\u7406\u6b8b\u5dee\u641c\u7d22\u3001\u7ec4\u88c5\u5e76\u4f18\u5316\u5019\u9009\u95ed\u5f0f\u89e3\uff0c\u7edf\u4e00\u7b26\u53f7\u63a8\u7406\u4e0e\u6570\u503c\u4f18\u5316\u3002", "result": "SIGS\u9996\u6b21\u5b9e\u73b0\uff1a(i) \u8026\u5408\u975e\u7ebf\u6027PDEs\u7684\u89e3\u6790\u6c42\u89e3\uff1b(ii) \u8bed\u6cd5\u8bef\u8bbe\u4e0b\u7684\u89e3\u53d1\u73b0\uff1b(iii) \u65e0\u5df2\u77e5\u95ed\u5f0f\u89e3PDEs\u7684\u7b26\u53f7\u8fd1\u4f3c\uff0c\u5e76\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7cbe\u5ea6\u4e0e\u6548\u7387\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u6570\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "SIGS\u901a\u8fc7\u8bed\u6cd5\u7ea6\u675f\u4e0e\u6f5c\u7a7a\u95f4\u641c\u7d22\u7684\u7ed3\u5408\uff0c\u6210\u529f\u81ea\u52a8\u5316\u4e86\u5fae\u5206\u65b9\u7a0b\u89e3\u6790\u89e3\u7684\u6784\u5efa\u8fc7\u7a0b\uff0c\u4e3a\u590d\u6742\u7cfb\u7edf\u5efa\u6a21\u63d0\u4f9b\u4e86\u9ad8\u6548\u7cbe\u786e\u7684\u795e\u7ecf\u7b26\u53f7\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23763", "categories": ["cs.CC"], "pdf": "https://arxiv.org/pdf/2602.23763", "abs": "https://arxiv.org/abs/2602.23763", "authors": ["Zihan Hao", "Zikuan Huang", "Qipeng Liu"], "title": "On the Need for (Quantum) Memory with Short Outputs", "comment": null, "summary": "In this work, we establish the first separation between computation with bounded and unbounded space, for problems with short outputs (i.e., working memory can be exponentially larger than output size), both in the classical and the quantum setting. Towards that, we introduce a problem called nested collision finding, and show that optimal query complexity can not be achieved without exponential memory. Our result is based on a novel ``two-oracle recording'' technique, where one oracle ``records'' the computation's long outputs under the other oracle, effectively reducing the time-space trade-off for short-output problems to that of long-output problems. We believe this technique will be of independent interest for establishing time-space tradeoffs in other short-output settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u5728\u6709\u754c\u7a7a\u95f4\u4e0e\u65e0\u754c\u7a7a\u95f4\u4e4b\u95f4\u5bf9\u77ed\u8f93\u51fa\u95ee\u9898\uff08\u5de5\u4f5c\u5185\u5b58\u53ef\u6307\u6570\u500d\u4e8e\u8f93\u51fa\u5927\u5c0f\uff09\u8fdb\u884c\u4e86\u5206\u79bb\uff0c\u7ecf\u5178\u4e0e\u91cf\u5b50\u8bbe\u7f6e\u5747\u6210\u7acb\u3002\u901a\u8fc7\u5d4c\u5957\u78b0\u649e\u53d1\u73b0\u95ee\u9898\uff0c\u8bc1\u660e\u6700\u4f18\u67e5\u8be2\u590d\u6742\u5ea6\u9700\u6307\u6570\u5185\u5b58\uff0c\u5e76\u63d0\u51fa\u201c\u53cc\u9884\u8a00\u8bb0\u5f55\u201d\u6280\u672f\uff0c\u5c06\u77ed\u8f93\u51fa\u95ee\u9898\u7684\u65f6\u95f4\u2011\u7a7a\u95f4\u6743\u8861\u5f52\u7ea6\u4e3a\u957f\u8f93\u51fa\u95ee\u9898\u3002", "motivation": "\u77ed\u8f93\u51fa\u95ee\u9898\u7684\u65f6\u95f4\u2011\u7a7a\u95f4\u6743\u8861\u5728\u7ecf\u5178\u548c\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7814\u7a76\u8f83\u5c11\uff0c\u5efa\u7acb\u6709\u754c\u4e0e\u65e0\u754c\u7a7a\u95f4\u7684\u5206\u79bb\u6709\u52a9\u4e8e\u7406\u89e3\u8ba1\u7b97\u8d44\u6e90\u7684\u6781\u9650\uff0c\u5e76\u4e3a\u91cf\u5b50\u7b97\u6cd5\u4e0b\u754c\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002", "method": "\u5f15\u5165\u5d4c\u5957\u78b0\u649e\u53d1\u73b0\u95ee\u9898\uff0c\u4f7f\u7528\u201c\u53cc\u9884\u8a00\u8bb0\u5f55\u201d\u6280\u672f\uff0c\u8ba9\u4e00\u4e2a\u9884\u8a00\u8bb0\u5f55\u53e6\u4e00\u4e2a\u9884\u8a00\u7684\u957f\u8f93\u51fa\uff0c\u4ece\u800c\u5c06\u77ed\u8f93\u51fa\u95ee\u9898\u7684\u65f6\u95f4\u2011\u7a7a\u95f4\u6743\u8861\u8f6c\u5316\u4e3a\u957f\u8f93\u51fa\u95ee\u9898\u7684\u6743\u8861\u3002", "result": "\u8bc1\u660e\u5d4c\u5957\u78b0\u649e\u627e\u95ee\u9898\u7684\u6700\u4f18\u67e5\u8be2\u590d\u6742\u5ea6\u5fc5\u987b\u4f9d\u8d56\u6307\u6570\u7ea7\u5185\u5b58\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u6709\u754c\u4e0e\u65e0\u754c\u7a7a\u95f4\u4e4b\u95f4\u7684\u5206\u79bb\uff0c\u4e14\u5728\u7ecf\u5178\u548c\u91cf\u5b50\u8bbe\u7f6e\u4e0b\u5747\u6210\u7acb\u3002", "conclusion": "\u201c\u53cc\u9884\u8a00\u8bb0\u5f55\u201d\u6280\u672f\u4e3a\u77ed\u8f93\u51fa\u95ee\u9898\u7684\u65f6\u95f4\u2011\u7a7a\u95f4\u6743\u8861\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u9884\u8ba1\u5c06\u5728\u5176\u4ed6\u76f8\u5173\u7814\u7a76\u4e2d\u53d1\u6325\u91cd\u8981\u4f5c\u7528\u3002"}}
{"id": "2602.23925", "categories": ["cond-mat.str-el", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2602.23925", "abs": "https://arxiv.org/abs/2602.23925", "authors": ["Afreen Anamul Haque", "Rishabh Saraswat", "Aniket Singha", "Rekha Verma", "Sitangshu Bhattacharya"], "title": "Phonon-Assisted Photoluminescence and Ultrafast Exciton Dynamics in Two-Dimensional Silicon Carbide", "comment": "6 figures", "summary": "Phonon assisted photoluminescence provides a direct window into exciton phonon interactions in low dimensional semiconductors. Using fully ab initio many body perturbation theory, including finite momentum Bethe Salpeter calculations, we investigate phonon assisted emission and exciton dynamics in two dimensional hexagonal silicon carbide and benchmark its response against 2D hexagonal boron nitride. By explicitly resolving exciton phonon matrix elements, we identify high energy optical TO LO phonons as the dominant contributors to sideband formation and quantify their spectral weights. h SiC exhibits pronounced phonon assisted sidebands comparable to h BN, despite a smaller exciton phonon energy separation and fewer resolved replicas. The bright K K exciton governs near UV zero phonon emission, while intervalley excitons acquire radiative character through symmetry allowed optical-phonon coupling. Temperature dependent scattering rates reveal an ultrashort bright exciton lifetime of approximately 300 fs at 10 K, highlighting rapid exciton relaxation driven by intrinsic phonon channels. These results establish monolayer SiC as a symmetry-activated platform for efficient, strain-free phonon-assisted emission and provide a quantitative framework for ultrafast exciton dynamics in wide bandgap 2D semiconductors.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\u63ed\u793a\u5355\u5c42\u516d\u65b9\u78b3\u5316\u7845\uff08h-SiC\uff09\u5177\u6709\u663e\u8457\u58f0\u5b50\u8f85\u52a9\u53d1\u5149\u8fb9\u5e26\uff0c\u5176\u8d85\u5feb\u6fc0\u5b50\u52a8\u529b\u5b66\uff0810K\u4e0b\u4eae\u6fc0\u5b50\u5bff\u547d\u4ec5300\u98de\u79d2\uff09\u6e90\u4e8e\u5bf9\u79f0\u6027\u5141\u8bb8\u7684\u5149\u5b66\u58f0\u5b50\u8026\u5408\uff0c\u4e3a\u5bbd\u7981\u5e26\u4e8c\u7ef4\u534a\u5bfc\u4f53\u63d0\u4f9b\u4e86\u5b9a\u91cf\u7406\u8bba\u6846\u67b6", "motivation": "\u63a2\u7a76\u4f4e\u7ef4\u534a\u5bfc\u4f53\u4e2d\u6fc0\u5b50-\u58f0\u5b50\u76f8\u4e92\u4f5c\u7528\u673a\u5236\uff0c\u7a81\u7834\u4f20\u7edf\u5149\u8c31\u6280\u672f\u5bf9\u58f0\u5b50\u8f85\u52a9\u53d1\u5149\u8fb9\u5e26\u5fae\u89c2\u8d77\u6e90\u7684\u8ba4\u77e5\u5c40\u9650\uff0c\u4e3a\u4e8c\u7ef4\u5bbd\u7981\u5e26\u6750\u6599\u7684\u5149\u7535\u5e94\u7528\u63d0\u4f9b\u7406\u8bba\u57fa\u7840", "method": "\u91c7\u7528\u5168 ab initio \u591a\u4f53\u5fae\u6270\u7406\u8bba\u7ed3\u5408\u6709\u9650\u52a8\u91cf Bethe-Salpeter \u65b9\u7a0b\uff0c\u663e\u5f0f\u89e3\u6790\u6fc0\u5b50-\u58f0\u5b50\u77e9\u9635\u5143\uff0c\u5bf9\u6bd4\u5206\u6790 h-SiC \u4e0e h-BN \u7684\u58f0\u5b50\u8f85\u52a9\u53d1\u5c04\u52a8\u529b\u5b66", "result": "1) \u9ad8\u80fd\u91cf TO/LO \u5149\u5b66\u58f0\u5b50\u662f\u8fb9\u5e26\u5f62\u6210\u4e3b\u5bfc\u56e0\u7d20\uff1b2) h-SiC \u867d\u6fc0\u5b50-\u58f0\u5b50\u80fd\u9699\u66f4\u5c0f\u3001\u58f0\u5b50\u590d\u5236\u5cf0\u66f4\u5c11\uff0c\u4f46\u8fb9\u5e26\u5f3a\u5ea6\u4e0e h-BN \u76f8\u5f53\uff1b3) K-K \u4eae\u6fc0\u5b50\u4e3b\u5bfc\u8fd1\u7d2b\u5916\u96f6\u58f0\u5b50\u53d1\u5c04\uff0c\u8c37\u95f4\u6fc0\u5b50\u901a\u8fc7\u5bf9\u79f0\u6027\u5141\u8bb8\u7684\u5149\u5b66\u58f0\u5b50\u8026\u5408\u83b7\u5f97\u8f90\u5c04\u7279\u6027\uff1b4) 10K \u65f6\u4eae\u6fc0\u5b50\u5bff\u547d\u4ec5 300 \u98de\u79d2", "conclusion": "\u5355\u5c42 h-SiC \u662f\u5bf9\u79f0\u6027\u6fc0\u6d3b\u7684\u9ad8\u6548\u65e0\u5e94\u53d8\u58f0\u5b50\u8f85\u52a9\u53d1\u5149\u5e73\u53f0\uff0c\u5efa\u7acb\u7684\u5b9a\u91cf\u6846\u67b6\u4e3a\u8d85\u5feb\u6fc0\u5b50\u52a8\u529b\u5b66\u7814\u7a76\u63d0\u4f9b\u65b0\u65b9\u6cd5\uff0c\u63a8\u52a8\u5bbd\u7981\u5e26\u4e8c\u7ef4\u534a\u5bfc\u4f53\u5728\u5149\u7535\u5668\u4ef6\u4e2d\u7684\u5e94\u7528"}}
{"id": "2602.23410", "categories": ["cs.LG", "cs.AI", "eess.SP", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2602.23410", "abs": "https://arxiv.org/abs/2602.23410", "authors": ["Hanning Guo", "Farah Abdellatif", "Hanwen Bi", "Andrei Galbenus", "Jon. N. Shah", "Abigail Morrison", "J\u00fcrgen Dammers"], "title": "Brain-OF: An Omnifunctional Foundation Model for fMRI, EEG and MEG", "comment": null, "summary": "Brain foundation models have achieved remarkable advances across a wide range of neuroscience tasks. However, most existing models are limited to a single functional modality, restricting their ability to exploit complementary spatiotemporal dynamics and the collective data scale across imaging techniques. To address this limitation, we propose Brain-OF, the first omnifunctional brain foundation model jointly pretrained on fMRI, EEG and MEG, capable of handling both unimodal and multimodal inputs within a unified framework. To reconcile heterogeneous spatiotemporal resolutions, we introduce the Any-Resolution Neural Signal Sampler, which projects diverse brain signals into a shared semantic space.To further manage semantic shifts, the Brain-OF backbone integrates DINT attention with a Sparse Mixture of Experts, where shared experts capture modality-invariant representations and routed experts specialize in modality-specific semantics. Furthermore, we propose Masked Temporal-Frequency Modeling, a dual-domain pretraining objective that jointly reconstructs brain signals in both the time and frequency domains. Brain-OF is pretrained on a large-scale corpus comprising around 40 datasets and demonstrates superior performance across diverse downstream tasks, highlighting the benefits of joint multimodal integration and dual-domain pretraining.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u591a\u6a21\u6001\u8111\u57fa\u7840\u6a21\u578bBrain-OF\uff0c\u901a\u8fc7\u878d\u5408fMRI/EEG/MEG\u6570\u636e\u5e76\u91c7\u7528\u53cc\u57df\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u795e\u7ecf\u79d1\u5b66\u4efb\u52a1\u6027\u80fd", "motivation": "\u73b0\u6709\u8111\u57fa\u7840\u6a21\u578b\u5c40\u9650\u4e8e\u5355\u4e00\u529f\u80fd\u6a21\u6001\uff0c\u65e0\u6cd5\u5229\u7528\u591a\u6210\u50cf\u6280\u672f\u7684\u4e92\u8865\u65f6\u7a7a\u52a8\u6001\u7279\u5f81\u548c\u96c6\u4f53\u6570\u636e\u89c4\u6a21", "method": "1) \u91c7\u7528\u4efb\u610f\u5206\u8fa8\u7387\u795e\u7ecf\u4fe1\u53f7\u91c7\u6837\u5668\u7edf\u4e00\u4e0d\u540c\u65f6\u7a7a\u5206\u8fa8\u7387\u7684\u8111\u4fe1\u53f7 2) \u96c6\u6210DINT\u6ce8\u610f\u529b\u4e0e\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u5206\u79bb\u6a21\u6001\u4e0d\u53d8/\u7279\u5b9a\u8868\u5f81 3) \u8bbe\u8ba1\u65f6\u9891\u63a9\u7801\u5efa\u6a21\u53cc\u57df\u9884\u8bad\u7ec3\u76ee\u6807", "result": "\u5728\u5305\u542b\u7ea640\u4e2a\u6570\u636e\u96c6\u7684\u5927\u89c4\u6a21\u8bed\u6599\u5e93\u4e0a\u9884\u8bad\u7ec3\uff0c\u5728\u591a\u7c7b\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u591a\u6a21\u6001\u8054\u5408\u8bad\u7ec3\u4f18\u52bf", "conclusion": "\u9996\u6b21\u5b9e\u73b0fMRI/EEG/MEG\u7684\u901a\u7528\u6846\u67b6\u5efa\u6a21\uff0c\u4e3a\u5f00\u53d1\u66f4\u5168\u9762\u7684\u8111\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2602.24276", "categories": ["cond-mat.stat-mech", "hep-th", "math-ph", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.24276", "abs": "https://arxiv.org/abs/2602.24276", "authors": ["Samuel H. Pickering", "Bruno Bertini"], "title": "Asymptotically Solvable Quantum Circuits", "comment": null, "summary": "The discovery of chaotic quantum circuits with (partially) solvable dynamics has played a key role in our understanding of non-equilibrium quantum matter and, at the same time, has helped the development of concrete platforms for quantum computation. It was shown that solvability does not prevent the generation of chaotic dynamics, however, it imposes non-trivial constraints on the generated correlations. A natural question is then whether it is possible to gain insight into the generic case despite the latter being very hard to access. To address this question here we introduce a family of 'asymptotically solvable' quantum circuits where the solvability constraints only affect correlations on length scales beyond a tuneable threshold. This means that their dynamics are only solvable for long enough times: for times shorter than the threshold they are generic. We show this by computing both their dynamical correlations on the equilibrium (infinite temperature) state and their thermalisation dynamics following quantum quenches from compatible (asymptotically solvable) non-equilibrium initial states. The class of systems we introduce is generically ergodic but contains a non-interacting point, which we use to provide exact analytical results, complementing those of numerical experiments, on the non-solvable early time regime.", "AI": {"tldr": "\u63d0\u51fa\"\u6e10\u8fd1\u53ef\u89e3\"\u91cf\u5b50\u7535\u8def\u6a21\u578b\uff0c\u901a\u8fc7\u8c03\u8282\u9608\u503c\u5c3a\u5ea6\uff0c\u5728\u65e9\u671f\u8868\u73b0\u51fa\u4e00\u822c\u6df7\u6c8c\u884c\u4e3a\uff0c\u5728\u540e\u671f\u624d\u5448\u73b0\u53ef\u89e3\u7279\u6027\uff0c\u4e3a\u7814\u7a76\u4e00\u822c\u975e\u5e73\u8861\u91cf\u5b50\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "motivation": "\u7406\u89e3\u4e00\u822c\u6df7\u6c8c\u91cf\u5b50\u7cfb\u7edf\u7684\u56f0\u96be\u6027\uff0c\u4ee5\u53ca\u53ef\u89e3\u6027\u5e26\u6765\u7684\u975e\u5e73\u51e1\u7ea6\u675f\uff0c\u65e8\u5728\u5efa\u7acb\u53ef\u89e3\u6a21\u578b\u4e0e\u4e00\u822c\u6df7\u6c8c\u7cfb\u7edf\u4e4b\u95f4\u7684\u6865\u6881\uff0c\u63a2\u7d22\u5728\u96be\u4ee5\u76f4\u63a5\u7814\u7a76\u7684\u666e\u904d\u60c5\u51b5\u4e0b\u83b7\u53d6\u6d1e\u89c1\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u4e00\u7c7b\"\u6e10\u8fd1\u53ef\u89e3\"\u91cf\u5b50\u7535\u8def\u5bb6\u65cf\uff0c\u5176\u53ef\u89e3\u6027\u7ea6\u675f\u4ec5\u4f5c\u7528\u4e8e\u8d85\u8fc7\u53ef\u8c03\u9608\u503c\u7684\u5173\u8054\u5c3a\u5ea6\u3002\u901a\u8fc7\u8ba1\u7b97\u5e73\u8861\u6001\uff08\u65e0\u7a77\u6e29\u5ea6\uff09\u4e0b\u7684\u52a8\u529b\u5b66\u5173\u8054\u51fd\u6570\u548c\u91cf\u5b50\u731d\u706d\u540e\u7684\u70ed\u5316\u52a8\u529b\u5b66\uff0c\u5e76\u7ed3\u5408\u975e\u76f8\u4e92\u4f5c\u7528\u70b9\u4e0a\u7684\u7cbe\u786e\u89e3\u6790\u7ed3\u679c\u4e0e\u6570\u503c\u5b9e\u9a8c\u8fdb\u884c\u4e92\u8865\u7814\u7a76\u3002", "result": "\u8be5\u7535\u8def\u7c7b\u5177\u6709\u666e\u904d\u904d\u5386\u6027\uff0c\u4f46\u5305\u542b\u4e00\u4e2a\u975e\u76f8\u4e92\u4f5c\u7528\u70b9\uff0c\u53ef\u5728\u4e0d\u53ef\u89e3\u7684\u65e9\u671f\u65f6\u95f4\u533a\u57df\u63d0\u4f9b\u7cbe\u786e\u89e3\u6790\u7ed3\u679c\uff0c\u8865\u5145\u4e86\u6570\u503c\u5b9e\u9a8c\u7684\u4e0d\u8db3\u3002", "conclusion": "\u6e10\u8fd1\u53ef\u89e3\u91cf\u5b50\u7535\u8def\u4e3a\u7814\u7a76\u4e00\u822c\u6df7\u6c8c\u91cf\u5b50\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u65e9\u671f\u65f6\u95f4\u4fdd\u6301\u4e00\u822c\u6027\u800c\u5728\u540e\u671f\u5448\u73b0\u53ef\u89e3\u6027\uff0c\u6210\u529f\u8fde\u63a5\u4e86\u53ef\u89e3\u6a21\u578b\u4e0e\u4e00\u822c\u6df7\u6c8c\u52a8\u529b\u5b66\uff0c\u6df1\u5316\u4e86\u5bf9\u975e\u5e73\u8861\u91cf\u5b50\u7269\u8d28\u7684\u7406\u89e3\u3002"}}
{"id": "2602.23545", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23545", "abs": "https://arxiv.org/abs/2602.23545", "authors": ["Matteo Ceriscioli", "Karthika Mohan"], "title": "Planning under Distribution Shifts with Causal POMDPs", "comment": "To appear at the 36th International Conference on Automated Planning and Scheduling (ICAPS-26)", "summary": "In the real world, planning is often challenged by distribution shifts. As such, a model of the environment obtained under one set of conditions may no longer remain valid as the distribution of states or the environment dynamics change, which in turn causes previously learned strategies to fail. In this work, we propose a theoretical framework for planning under partial observability using Partially Observable Markov Decision Processes (POMDPs) formulated using causal knowledge. By representing shifts in the environment as interventions on this causal POMDP, the framework enables evaluating plans under hypothesized changes and actively identifying which components of the environment have been altered. We show how to maintain and update a belief over both the latent state and the underlying domain, and we prove that the value function remains piecewise linear and convex (PWLC) in this augmented belief space. Preservation of PWLC under distribution shifts has the advantage of maintaining the tractability of planning via $\u03b1$-vector-based POMDP methods.", "AI": {"tldr": "The paper proposes a causal POMDP framework to handle distribution shifts in planning by representing shifts as interventions, maintaining beliefs over both state and domain changes, and proving the value function remains tractable (PWLC) under these shifts.", "motivation": "Real-world planning is challenged by distribution shifts where models become invalid as environment conditions change, causing previously learned strategies to fail.", "method": "Proposes a theoretical framework using causal Partially Observable Markov Decision Processes (POMDPs) where distribution shifts are represented as interventions. The method maintains and updates beliefs over both latent state and underlying domain, and proves the value function remains piecewise linear and convex (PWLC) in this augmented belief space.", "result": "Shows how to maintain beliefs over state and domain simultaneously, and proves that PWLC property is preserved under distribution shifts, enabling tractable planning with \u03b1-vector-based POMDP methods.", "conclusion": "The framework enables evaluating plans under hypothesized changes and actively identifying altered environment components while maintaining computational tractability."}}
{"id": "2602.23510", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23510", "abs": "https://arxiv.org/abs/2602.23510", "authors": ["Emma Tien Hwai Medlock", "Vinod N. Rao", "Ry Render", "Timothy Spiller", "Rupesh Kumar"], "title": "Continuous variable quantum key distribution channel emulator for the SPOQC mission", "comment": "11 pages, 8 figures", "summary": "In a free space optical (FSO) communication link from satellite to ground, the losses in the channel will be dynamic. Thus, the characterization of the FSO channel is of great importance and this can be emulated in the lab to evaluate the realistic performance of a satellite payload. In this work, we introduce a novel optical channel emulator capable of replicating these dynamics, especially for Low Earth Orbit based CubeSats. We demonstrate its ability to accurately emulate a satellite-to-ground optical communications channel under various atmospheric turbulence strengths, satellite trajectories, and optical ground station parameters at a given optical wavelength of interest. Our satellite channel emulator was designed to test and benchmark the performance of the continuous variable quantum key distribution payload for the Satellite Platform for Optical Quantum Communications mission - an in-orbit demonstrator for the UK's Quantum Communication Hub, to be launched in early 2026.", "AI": {"tldr": "\u5f00\u53d1\u65b0\u578b\u5149\u5b66\u4fe1\u9053\u4eff\u771f\u5668\uff0c\u7528\u4e8e\u5728\u5b9e\u9a8c\u5ba4\u73af\u5883\u4e0b\u7cbe\u786e\u6a21\u62df\u4f4e\u8f68\u536b\u661f\u5230\u5730\u9762\u7684\u81ea\u7531\u7a7a\u95f4\u5149\u901a\u4fe1\u4fe1\u9053\u52a8\u6001\u7279\u6027\uff0c\u7279\u522b\u662f\u9488\u5bf9\u7acb\u65b9\u661f\u91cf\u5b50\u901a\u4fe1\u8f7d\u8377\u7684\u6027\u80fd\u6d4b\u8bd5", "motivation": "\u536b\u661f\u5bf9\u5730\u81ea\u7531\u7a7a\u95f4\u5149\u901a\u4fe1\u4fe1\u9053\u5b58\u5728\u52a8\u6001\u635f\u8017\uff0c\u51c6\u786e\u8868\u5f81\u4fe1\u9053\u7279\u6027\u5bf9\u8bc4\u4f30\u536b\u661f\u8f7d\u8377\u5b9e\u9645\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u9700\u5728\u5b9e\u9a8c\u5ba4\u5b9e\u73b0\u9ad8\u4fdd\u771f\u4fe1\u9053\u4eff\u771f", "method": "\u8bbe\u8ba1\u65b0\u578b\u5149\u5b66\u4fe1\u9053\u4eff\u771f\u5668\uff0c\u53ef\u6a21\u62df\u4e0d\u540c\u5927\u6c14\u6e4d\u6d41\u5f3a\u5ea6\u3001\u536b\u661f\u8f68\u9053\u8f68\u8ff9\u53ca\u5730\u9762\u7ad9\u53c2\u6570\u4e0b\u7684\u536b\u661f\u5bf9\u5730\u5149\u4fe1\u9053\u7279\u6027\uff0c\u5de5\u4f5c\u4e8e\u7279\u5b9a\u5149\u5b66\u6ce2\u957f", "result": "\u6210\u529f\u6784\u5efa\u53ef\u7cbe\u786e\u590d\u73b0\u4f4e\u5730\u7403\u8f68\u9053\u7acb\u65b9\u661f\u5bf9\u5730\u91cf\u5b50\u901a\u4fe1\u4fe1\u9053\u52a8\u6001\u7684\u4eff\u771f\u7cfb\u7edf\uff0c\u652f\u6301\u8fde\u7eed\u53d8\u91cf\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\u8f7d\u8377\u7684\u6027\u80fd\u6d4b\u8bd5\u4e0e\u57fa\u51c6\u8bc4\u4f30", "conclusion": "\u8be5\u4eff\u771f\u5668\u5c06\u4e3a2026\u5e74\u82f1\u56fd\u91cf\u5b50\u901a\u4fe1\u4e2d\u5fc3\"\u5149\u5b66\u91cf\u5b50\u901a\u4fe1\u536b\u661f\u5e73\u53f0\"\u4efb\u52a1\u7684\u6709\u6548\u8f7d\u8377\u63d0\u4f9b\u5173\u952e\u6d4b\u8bd5\u9a8c\u8bc1\uff0c\u63a8\u52a8\u5929\u57fa\u91cf\u5b50\u901a\u4fe1\u6280\u672f\u53d1\u5c55"}}
{"id": "2602.23809", "categories": ["cs.CC"], "pdf": "https://arxiv.org/pdf/2602.23809", "abs": "https://arxiv.org/abs/2602.23809", "authors": ["Pavel Hub\u00e1\u010dek"], "title": "Black-Box PWPP Is Not Turing-Closed", "comment": null, "summary": "We establish that adaptive collision-finding queries are strictly more powerful than non-adaptive ones by proving that the complexity class PWPP (Polynomial Weak Pigeonhole Principle) is not closed under adaptive Turing reductions relative to a random oracle. Previously, PWPP was known to be closed under non-adaptive Turing reductions (Je\u0159\u00e1bek 2016). We demonstrate this black-box separation by introducing the NESTED-COLLISION problem, a natural collision-finding problem defined on a pair of shrinking functions. We show that while this problem is solvable via two adaptive calls to a PWPP oracle, its random instances cannot be solved via a black-box non-adaptive reduction to the canonical PWPP-complete problem COLLISION.", "AI": {"tldr": "\u8bc1\u660e\u81ea\u9002\u5e94\u78b0\u649e\u67e5\u627e\u67e5\u8be2\u6bd4\u975e\u81ea\u9002\u5e94\u67e5\u8be2\u66f4\u5f3a\u5927\uff0c\u901a\u8fc7\u8bc1\u660e PWPP \u590d\u6742\u5ea6\u7c7b\u5728\u968f\u673a\u9884\u8a00\u673a\u4e0b\u4e0d\u5bf9\u81ea\u9002\u5e94\u56fe\u7075\u5f52\u7ea6\u5c01\u95ed", "motivation": "\u5efa\u7acb\u81ea\u9002\u5e94\u78b0\u649e\u67e5\u627e\u67e5\u8be2\u4e25\u683c\u5f3a\u4e8e\u975e\u81ea\u9002\u5e94\u67e5\u8be2\u7684\u8bc1\u636e\uff0c\u56e0\u4e3a PWPP \u5df2\u77e5\u5bf9\u975e\u81ea\u9002\u5e94\u56fe\u7075\u5f52\u7ea6\u5c01\u95ed\uff0c\u4f46\u81ea\u9002\u5e94\u60c5\u51b5\u672a\u77e5", "method": "\u5f15\u5165 NESTED-COLLISION \u95ee\u9898\uff0c\u8bc1\u660e\u8be5\u95ee\u9898\u53ef\u901a\u8fc7\u4e24\u6b21\u81ea\u9002\u5e94 PWPP \u9884\u8a00\u673a\u8c03\u7528\u6c42\u89e3\uff0c\u4f46\u968f\u673a\u5b9e\u4f8b\u65e0\u6cd5\u901a\u8fc7\u9ed1\u76d2\u975e\u81ea\u9002\u5e94\u5f52\u7ea6\u5230\u6807\u51c6 PWPP-\u5b8c\u5168\u95ee\u9898 COLLISION", "result": "\u5b9e\u73b0\u4e86\u9996\u4e2a\u9ed1\u76d2\u5206\u79bb\uff0c\u8868\u660e PWPP \u5728\u968f\u673a\u9884\u8a00\u673a\u4e0b\u4e0d\u5bf9\u81ea\u9002\u5e94\u56fe\u7075\u5f52\u7ea6\u5c01\u95ed\uff0c\u8bc1\u5b9e\u81ea\u9002\u5e94\u67e5\u8be2\u7684\u4f18\u8d8a\u6027", "conclusion": "\u81ea\u9002\u5e94\u78b0\u649e\u67e5\u627e\u67e5\u8be2\u786e\u5b9e\u4e25\u683c\u6bd4\u975e\u81ea\u9002\u5e94\u67e5\u8be2\u66f4\u5f3a\u5927\uff0c\u8fd9\u5bf9\u5bc6\u7801\u5b66\u57fa\u7840\u7406\u8bba\u548c\u590d\u6742\u6027\u7c7b\u7406\u89e3\u6709\u91cd\u8981\u610f\u4e49"}}
{"id": "2602.23986", "categories": ["cond-mat.str-el", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2602.23986", "abs": "https://arxiv.org/abs/2602.23986", "authors": ["GiBaik Sim", "Stephan Rachel"], "title": "Quantum spin models of commensurate $p$-wave magnets", "comment": "8 pages, 4 figures", "summary": "The $p$-wave magnet has emerged as a new type of magnetism exhibiting odd-parity, time-reversal-symmetric spin splitting in momentum space, and has attracted considerable interest as a promising platform for spintronic applications. However, the theoretical understanding of the fundamental mechanism responsible for stabilizing this phase remains limited. In this work, we identify a microscopic interacting model that realizes the $p$-wave magnet as its ground state. We first introduce a Hubbard model and derive the corresponding low-energy spin Hamiltonian. At the classical level, we find that the $p$-wave magnet is stabilized but remains energetically degenerate with competing noncoplanar states. Quantum fluctuations lift this degeneracy, selecting the $p$-wave magnet as the unique ground state. The resulting electronic structure exhibits finite spin accumulation via the Edelstein effect, highlighting the potential of $p$-wave magnetism for spintronic applications. We further discuss the relevance of our theory to quasi-two-dimensional honeycomb magnets such as Ni$_2$Mo$_3$O$_8$. Our findings establish the possibility of spontaneous $p$-wave magnetism.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u5fae\u89c2\u76f8\u4e92\u4f5c\u7528\u6a21\u578b\uff0c\u63ed\u793a\u4e86p\u6ce2\u78c1\u4f53\u7684\u7a33\u5b9a\u673a\u5236\uff1a\u91cf\u5b50\u6da8\u843d\u4f7f\u5176\u4ece\u7ecf\u5178\u7b80\u5e76\u6001\u4e2d\u6210\u4e3a\u72ec\u7279\u57fa\u6001\uff0c\u5e76\u8bc1\u5b9e\u5176\u5177\u5907\u81ea\u65cb\u7535\u5b50\u5b66\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "p\u6ce2\u78c1\u4f53\u4f5c\u4e3a\u65b0\u578b\u78c1\u6027\u76f8\uff0c\u867d\u5728\u81ea\u65cb\u7535\u5b50\u5b66\u4e2d\u6f5c\u529b\u663e\u8457\uff0c\u4f46\u5176\u7a33\u5b9a\u673a\u5236\u7684\u7406\u8bba\u7406\u89e3\u4ecd\u4e0d\u5145\u5206\uff0c\u9650\u5236\u4e86\u5e94\u7528\u63a2\u7d22\u3002", "method": "\u5f15\u5165\u54c8\u4f2f\u5fb7\u6a21\u578b\u5e76\u63a8\u5bfc\u4f4e\u80fd\u6709\u6548\u81ea\u65cb\u54c8\u5bc6\u987f\u91cf\uff0c\u901a\u8fc7\u7ecf\u5178\u4e0e\u91cf\u5b50\u5c42\u9762\u5206\u6790\uff0c\u63a2\u7a76p\u6ce2\u78c1\u4f53\u7684\u57fa\u6001\u7a33\u5b9a\u673a\u5236\u3002", "result": "\u7ecf\u5178\u5c42\u9762p\u6ce2\u78c1\u4f53\u4e0e\u5176\u4ed6\u975e\u5171\u7ebf\u6001\u80fd\u91cf\u7b80\u5e76\uff0c\u91cf\u5b50\u6da8\u843d\u89e3\u9664\u7b80\u5e76\u5e76\u4f7f\u5176\u6210\u4e3a\u552f\u4e00\u57fa\u6001\uff1b\u540c\u65f6\u89c2\u5bdf\u5230\u57c3\u5fb7\u65af\u5766\u6548\u5e94\u5bfc\u81f4\u7684\u81ea\u65cb\u79ef\u7d2f\u3002", "conclusion": "\u9996\u6b21\u4ece\u5fae\u89c2\u6a21\u578b\u8bc1\u5b9e\u81ea\u53d1p\u6ce2\u78c1\u6027\u7684\u5b58\u5728\uff0c\u7406\u8bba\u6846\u67b6\u53ef\u89e3\u91caNi\u2082Mo\u2083O\u2088\u7b49\u51c6\u4e8c\u7ef4\u8702\u7a9d\u6676\u683c\u78c1\u4f53\uff0c\u4e3a\u65b0\u578b\u81ea\u65cb\u7535\u5b50\u5668\u4ef6\u8bbe\u8ba1\u63d0\u4f9b\u57fa\u7840\u3002"}}
{"id": "2602.23413", "categories": ["cs.LG", "cs.CL", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.23413", "abs": "https://arxiv.org/abs/2602.23413", "authors": ["Shu Liu", "Shubham Agarwal", "Monishwaran Maheswaran", "Mert Cemri", "Zhifei Li", "Qiuyang Mang", "Ashwin Naren", "Ethan Boneh", "Audrey Cheng", "Melissa Z. Pan", "Alexander Du", "Kurt Keutzer", "Alexandros G. Dimakis", "Koushik Sen", "Matei Zaharia", "Ion Stoica"], "title": "EvoX: Meta-Evolution for Automated Discovery", "comment": null, "summary": "Recent work such as AlphaEvolve has shown that combining LLM-driven optimization with evolutionary search can effectively improve programs, prompts, and algorithms across domains. In this paradigm, previously evaluated solutions are reused to guide the model toward new candidate solutions. Crucially, the effectiveness of this evolution process depends on the search strategy: how prior solutions are selected and varied to generate new candidates. However, most existing methods rely on fixed search strategies with predefined knobs (e.g., explore-exploit ratios) that remain static throughout execution. While effective in some settings, these approaches often fail to adapt across tasks, or even within the same task as the search space changes over time. We introduce EvoX, an adaptive evolution method that optimizes its own evolution process. EvoX jointly evolves candidate solutions and the search strategies used to generate them, continuously updating how prior solutions are selected and varied based on progress. This enables the system to dynamically shift between different search strategies during the optimization process. Across nearly 200 real-world optimization tasks, EvoX outperforms existing AI-driven evolutionary methods including AlphaEvolve, OpenEvolve, GEPA, and ShinkaEvolve on the majority of tasks.", "AI": {"tldr": "EvoJ is an adaptive evolutionary method that co-evolves solutions and search strategies, achieving superior performance across 200+ real-world optimization tasks.", "motivation": "Existing LLM-driven evolutionary optimization methods rely on static search strategies with fixed parameters, limiting their adaptability across tasks and within tasks as the search space evolves.", "method": "EvoJ jointly evolves both candidate solutions and the search strategies themselves, continuously adapting solution selection and variation mechanisms based on real-time optimization progress.", "result": "Across nearly 200 real-world optimization tasks, EvoJ outperformed state-of-the-art baselines including AlphaEvolve, OpenEvolve, GEPA, and ShinkaEvolve on the majority of tasks.", "conclusion": "The adaptive optimization of evolution strategies is superior to fixed strategies, demonstrating that evolving the search process itself significantly enhances performance."}}
{"id": "2602.23579", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23579", "abs": "https://arxiv.org/abs/2602.23579", "authors": ["Guillem Rodr\u00edguez-Corominas", "Maria J. Blesa", "Christian Blum"], "title": "Construct, Merge, Solve & Adapt with Reinforcement Learning for the min-max Multiple Traveling Salesman Problem", "comment": null, "summary": "The Multiple Traveling Salesman Problem (mTSP) extends the Traveling Salesman Problem to m tours that start and end at a common depot and jointly visit all customers exactly once. In the min-max variant, the objective is to minimize the longest tour, reflecting workload balance. We propose a hybrid approach, Construct, Merge, Solve & Adapt with Reinforcement Learning (RL-CMSA), for the symmetric single-depot min-max mTSP. The method iteratively constructs diverse solutions using probabilistic clustering guided by learned pairwise q-values, merges routes into a compact pool, solves a restricted set-covering MILP, and refines solutions via inter-route remove, shift, and swap moves. The q-values are updated by reinforcing city-pair co-occurrences in high-quality solutions, while the pool is adapted through ageing and pruning. This combination of exact optimization and reinforcement-guided construction balances exploration and exploitation. Computational results on random and TSPLIB instances show that RL-CMSA consistently finds (near-)best solutions and outperforms a state-of-the-art hybrid genetic algorithm under comparable time limits, especially as instance size and the number of salesmen increase.", "AI": {"tldr": "A hybrid RL-CMSA method combining reinforcement learning with exact optimization effectively solves min-max mTSP by balancing exploration-exploitation, outperforming genetic algorithms especially for large-scale instances.", "motivation": "Address workload imbalance in multi-salesman routing by minimizing the longest tour (min-max objective) rather than total distance, crucial for fair resource allocation in logistics and scheduling.", "method": "RL-CMSA framework: (1) Probabilistically clusters cities using learned q-values to build diverse solutions; (2) Merges routes into a pool; (3) Solves restricted set-covering MILP; (4) Refines via inter-route moves; (5) Updates q-values by reinforcing city-pair co-occurrences in high-quality solutions; (6) Adapts pool via aging/pruning.", "result": "Outperforms state-of-the-art hybrid genetic algorithm on random and TSPLIB instances, achieving (near-)optimal solutions with significant advantages as problem size and salesman count increase.", "conclusion": "The integration of reinforcement-guided construction with exact optimization successfully balances exploration of diverse solutions and exploitation of high-quality patterns, establishing RL-CMSA as a superior approach for min-max mTSP."}}
{"id": "2602.23544", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23544", "abs": "https://arxiv.org/abs/2602.23544", "authors": ["Jihee Yang", "Thomas J. Carroll", "Philip Mason", "Robert Schwartz", "Kenneth M. O'Hara", "Jennifer Lund", "Michael Gottschalk", "Timothy Stephenson", "Lawrence H. Friedman", "Francisco Yumiceva", "Justin Hackley", "Aurelius L. Graninger", "Chris Rotella", "Pat Warner", "Jonathan M. Cochran", "Adam V. Bruce", "Melody Wagner", "James Wenner", "Stan Steers", "Christopher Moore", "Alex Marakov", "Bradley G. Christensen"], "title": "High-Temporal-Resolution Measurements of the Impacts of Ionizing Radiation on Superconducting Qubits", "comment": null, "summary": "We measure the effect of ionizing radiation on superconducting qubits with a timing resolution of 1 $\u03bcs$ using microwave kinetic inductance detectors (MKIDs) fabricated on the same substrate. We observe no correlation between two-level system (TLS) scrambling events and ionizing radiation events detected with the MKIDs, suggesting TLS scrambling events may not arise from ionizing radiation and instead the previously reported apparent correlation may be due to events without sufficient energy to trigger our MKIDs. We characterize the fast-time system recovery of transmons following a radiation event, where we observe the recovery of the enhanced qubit relaxation and excitation to be well-described by an exponential recovery to the baseline quasiparticle density, with a characteristic time of $13\\pm1\\ \u03bc$s, and a peak quasiparticle density at the junction per deposited energy of $240/\u03bcm^3/MeV$. The fast recovery is consistent with literature reported values for Nb-based devices with direct injection of 2$\u0394_{\\text{Al}}$ phonons, demonstrating the recovery is strongly dependent on the proximity of niobium to the junction.", "AI": {"tldr": "\u5229\u7528\u4e0e\u8d85\u5bfc\u91cf\u5b50\u6bd4\u7279\u540c\u57fa\u5e95\u7684\u5fae\u6ce2\u52a8\u80fd\u611f\u5e94\u63a2\u6d4b\u5668\uff08MKID\uff09\uff0c\u4ee51\u5fae\u79d2\u65f6\u95f4\u5206\u8fa8\u7387\u7814\u7a76\u7535\u79bb\u8f90\u5c04\u5bf9\u91cf\u5b50\u6bd4\u7279\u7684\u5f71\u54cd\uff0c\u53d1\u73b0TLS scrambling\u4e8b\u4ef6\u4e0e\u8f90\u5c04\u4e8b\u4ef6\u65e0\u76f8\u5173\u6027\uff0c\u4e14\u91cf\u5b50\u6bd4\u7279\u5728\u8f90\u5c04\u540e\u7ea613\u5fae\u79d2\u5feb\u901f\u6062\u590d\u3002", "motivation": "\u63a2\u7a76\u7535\u79bb\u8f90\u5c04\u662f\u5426\u76f4\u63a5\u5bfc\u81f4\u8d85\u5bfc\u91cf\u5b50\u6bd4\u7279\u4e2d\u7684\u4e8c\u80fd\u7ea7\u7cfb\u7edf\uff08TLS\uff09scrambling\u73b0\u8c61\uff0c\u4ee5\u6f84\u6e05\u5148\u524d\u7814\u7a76\u4e2d\u89c2\u5bdf\u5230\u7684\u8868\u89c2\u76f8\u5173\u6027\u662f\u5426\u771f\u5b9e\u5b58\u5728\u3002", "method": "\u5728\u76f8\u540c\u57fa\u5e95\u4e0a\u96c6\u6210\u5fae\u6ce2\u52a8\u80fd\u611f\u5e94\u63a2\u6d4b\u5668\uff08MKID\uff09\u4e0e\u8d85\u5bfc\u91cf\u5b50\u6bd4\u7279\uff0c\u5229\u7528MKID\u4ee51\u5fae\u79d2\u65f6\u95f4\u5206\u8fa8\u7387\u540c\u6b65\u76d1\u6d4b\u7535\u79bb\u8f90\u5c04\u4e8b\u4ef6\u4e0e\u91cf\u5b50\u6bd4\u7279\u72b6\u6001\u53d8\u5316\u3002", "result": "1. \u672a\u89c2\u6d4b\u5230TLS scrambling\u4e8b\u4ef6\u4e0e\u7535\u79bb\u8f90\u5c04\u4e8b\u4ef6\u7684\u5173\u8054\u6027\uff1b\n2. \u8f90\u5c04\u540e\u91cf\u5b50\u6bd4\u7279\u5f1b\u8c6b\u4e0e\u6fc0\u53d1\u6062\u590d\u8fc7\u7a0b\u7b26\u5408\u6307\u6570\u8870\u51cf\u6a21\u578b\uff0c\u7279\u5f81\u6062\u590d\u65f6\u95f413\u00b11\u5fae\u79d2\uff1b\n3. \u6bcf\u4e2a\u6c89\u79ef\u80fd\u91cf\u5355\u4f4d\uff08MeV\uff09\u5728\u7ed3\u533a\u4ea7\u751f240/\u03bcm\u00b3\u7684\u51c6\u7c92\u5b50\u5cf0\u503c\u5bc6\u5ea6\uff1b\n4. \u5feb\u901f\u6062\u590d\u73b0\u8c61\u4e0e\u94cc\u6750\u6599\u9760\u8fd1\u7ed3\u533a\u7684\u5668\u4ef6\u6587\u732e\u503c\u4e00\u81f4\u3002", "conclusion": "TLS scrambling\u53ef\u80fd\u5e76\u975e\u7531\u7535\u79bb\u8f90\u5c04\u76f4\u63a5\u5f15\u53d1\uff0c\u5148\u524d\u62a5\u9053\u7684\u76f8\u5173\u6027\u6216\u6e90\u4e8e\u80fd\u91cf\u4e0d\u8db3\u89e6\u53d1\u63a2\u6d4b\u5668\u7684\u4f4e\u80fd\u4e8b\u4ef6\uff1b\u91cf\u5b50\u6bd4\u7279\u6062\u590d\u901f\u5ea6\u5f3a\u70c8\u4f9d\u8d56\u4e8e\u94cc\u4e0e\u7ed3\u533a\u7684 proximity\uff08\u90bb\u8fd1\u6027\uff09\u3002"}}
{"id": "2602.24178", "categories": ["cs.LG", "cs.CC"], "pdf": "https://arxiv.org/pdf/2602.24178", "abs": "https://arxiv.org/abs/2602.24178", "authors": ["Adam R. Klivans", "Konstantinos Stavropoulos", "Arsen Vasilyan"], "title": "Sandwiching Polynomials for Geometric Concepts with Low Intrinsic Dimension", "comment": "30 pages", "summary": "Recent work has shown the surprising power of low-degree sandwiching polynomial approximators in the context of challenging learning settings such as learning with distribution shift, testable learning, and learning with contamination. A pair of sandwiching polynomials approximate a target function in expectation while also providing pointwise upper and lower bounds on the function's values. In this paper, we give a new method for constructing low-degree sandwiching polynomials that yield greatly improved degree bounds for several fundamental function classes and marginal distributions. In particular, we obtain degree $\\mathrm{poly}(k)$ sandwiching polynomials for functions of $k$ halfspaces under the Gaussian distribution, improving exponentially over the prior $2^{O(k)}$ bound. More broadly, our approach applies to function classes that are low-dimensional and have smooth boundary.\n  In contrast to prior work, our proof is relatively simple and directly uses the smoothness of the target function's boundary to construct sandwiching Lipschitz functions, which are amenable to results from high-dimensional approximation theory. For low-dimensional polynomial threshold functions (PTFs) with respect to Gaussians, we obtain doubly exponential improvements without applying the FT-mollification method of Kane used in the best previous result.", "AI": {"tldr": "\u63d0\u51fa\u6784\u9020\u4f4e\u6b21\u5939\u903c\u591a\u9879\u5f0f\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u51fd\u6570\u7c7b\u7684\u8fd1\u4f3c\u6b21\u6570\uff0c\u5bf9k\u534a\u7a7a\u95f4\u9ad8\u65af\u51fd\u6570\u5b9e\u73b0poly(k)\u6b21\uff0c\u8f83\u5148\u524d2^O(k)\u6b21\u6307\u6570\u7ea7\u63d0\u5347", "motivation": "\u9488\u5bf9\u5206\u5e03\u504f\u79fb\u3001\u53ef\u6d4b\u8bd5\u5b66\u4e60\u53ca\u6c61\u67d3\u6570\u636e\u7b49\u6311\u6218\u6027\u5b66\u4e60\u573a\u666f\uff0c\u6539\u8fdb\u4f4e\u6b21\u5939\u903c\u591a\u9879\u5f0f\u5bf9\u57fa\u7840\u51fd\u6570\u7c7b\u7684\u8fd1\u4f3c\u6b21\u6570\u8fb9\u754c", "method": "\u5229\u7528\u76ee\u6807\u51fd\u6570\u8fb9\u754c\u7684\u5e73\u6ed1\u6027\u76f4\u63a5\u6784\u9020\u5939\u903cLipschitz\u51fd\u6570\uff0c\u907f\u514d\u590d\u6742\u6280\u5de7\uff08\u5982FT-\u78e8\u5149\u6cd5\uff09\uff0c\u7ed3\u5408\u9ad8\u7ef4\u903c\u8fd1\u7406\u8bba", "result": "\u9ad8\u65af\u5206\u5e03\u4e0bk\u534a\u7a7a\u95f4\u51fd\u6570\uff1a\u6b21\u6570\u4ece2^O(k)\u964d\u81f3poly(k)\uff1b\u4f4e\u7ef4\u591a\u9879\u5f0f\u9608\u503c\u51fd\u6570\u5b9e\u73b0\u53cc\u6307\u6570\u7ea7\u63d0\u5347", "conclusion": "\u5e73\u6ed1\u8fb9\u754c\u7279\u6027\u4f7f\u7b80\u5355\u6784\u9020\u6cd5\u6709\u6548\uff0c\u4e3a\u4f4e\u7ef4\u5149\u6ed1\u8fb9\u754c\u51fd\u6570\u7c7b\u63d0\u4f9b\u666e\u9002\u6027\u4f18\u5316\u6846\u67b6"}}
{"id": "2602.23989", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.23989", "abs": "https://arxiv.org/abs/2602.23989", "authors": ["Huimei Liu", "Giniyat Khaliullin"], "title": "Triplon-mediated pairing and the superconducting gap structure in bilayer nickelates", "comment": "6 pages, 4 figures", "summary": "We investigate the superconducting gap structure in bilayer nickelates within a model where conduction bands of dx2-y2 symmetry coexist with localized d3z2-r2 spins. Strong interlayer coupling drives the local moments into a singlet ground state, whose virtual singlet-triplet excitations (\"triplons\") mediate the pairing interaction between conduction electrons. This yields interband s+- pairing, with opposite signs of the order parameter on the bonding beta and antibonding alpha bands. Our theory naturally explains two key experimental features: a larger gap on the alpha band despite its smaller density of states, and pronounced gap anisotropy arising from momentum-dependent nonlocal Kondo coupling. These results support triplon-mediated pairing as the microscopic origin of superconductivity in bilayer nickelates.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u954d\u9178\u76d0\u4e2d\u4e09\u91cd\u6001\u6fc0\u53d1\u5a92\u4ecb\u7684s+-\u6ce2\u914d\u5bf9\u673a\u5236\uff0c\u89e3\u91ca\u03b1\u80fd\u5e26\u66f4\u5927\u80fd\u9699\u53ca\u5f3a\u5404\u5411\u5f02\u6027\u5b9e\u9a8c\u73b0\u8c61", "motivation": "\u63ed\u793a\u53cc\u954d\u9178\u76d0\u8d85\u5bfc\u4f53\u4e2d\u914d\u5bf9\u76f8\u4e92\u4f5c\u7528\u7684\u5fae\u89c2\u8d77\u6e90\uff0c\u89e3\u91ca\u5b9e\u9a8c\u89c2\u6d4b\u5230\u7684\u80fd\u9699\u53cd\u5e38\u7279\u5f81", "method": "\u5efa\u7acbdx\u00b2-y\u00b2\u4f20\u5bfc\u5e26\u4e0e\u5c40\u57dfd\u00b3z\u00b2-r\u00b2\u81ea\u65cb\u5171\u5b58\u6a21\u578b\uff0c\u901a\u8fc7\u5f3a\u5c42\u95f4\u8026\u5408\u4f7f\u5c40\u57df\u77e9\u5f62\u6210\u5355\u6001\u57fa\u6001\uff0c\u5176\u865a\u6fc0\u53d1\uff08\u4e09\u91cd\u6001\uff09\u5a92\u4ecb\u7535\u5b50\u914d\u5bf9", "result": "\u9884\u8a00\u5c42\u95f4s+-\u6ce2\u914d\u5bf9\uff08\u6210\u952e\u03b2\u4e0e\u53cd\u952e\u03b1\u80fd\u5e26\u5e8f\u53c2\u91cf\u7b26\u53f7\u76f8\u53cd\uff09\uff0c\u03b1\u80fd\u5e26\u867d\u6001\u5bc6\u5ea6\u66f4\u4f4e\u5374\u5448\u73b0\u66f4\u5927\u80fd\u9699\uff0c\u4e14\u975e\u5c40\u57dfKondo\u8026\u5408\u5bfc\u81f4\u5f3a\u52a8\u91cf\u4f9d\u8d56\u7684\u5404\u5411\u5f02\u6027", "conclusion": "\u4e09\u91cd\u6001\u6fc0\u53d1\u5a92\u4ecb\u7684\u914d\u5bf9\u673a\u5236\u53ef\u5408\u7406\u89e3\u91ca\u53cc\u954d\u9178\u76d0\u8d85\u5bfc\u4f53\u7684\u5173\u952e\u5b9e\u9a8c\u7279\u5f81\uff0c\u652f\u6301\u5176\u4f5c\u4e3a\u8be5\u7c7b\u6750\u6599\u8d85\u5bfc\u5fae\u89c2\u8d77\u6e90\u7684\u7406\u8bba\u6846\u67b6"}}
{"id": "2602.23446", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23446", "abs": "https://arxiv.org/abs/2602.23446", "authors": ["Alejandro Rodriguez Dominguez"], "title": "Human Supervision as an Information Bottleneck: A Unified Theory of Error Floors in Human-Guided Learning", "comment": "Proceedings from IEEE CAI 2026, Conference on Artificial Intelligence, 8-10 May, Granada, Spain. 8 Pages, 3 Figures, 7 Tables", "summary": "Large language models are trained primarily on human-generated data and feedback, yet they exhibit persistent errors arising from annotation noise, subjective preferences, and the limited expressive bandwidth of natural language. We argue that these limitations reflect structural properties of the supervision channel rather than model scale or optimization. We develop a unified theory showing that whenever the human supervision channel is not sufficient for a latent evaluation target, it acts as an information-reducing channel that induces a strictly positive excess-risk floor for any learner dominated by it. We formalize this Human-Bounded Intelligence limit and show that across six complementary frameworks (operator theory, PAC-Bayes, information theory, causal inference, category theory, and game-theoretic analyses of reinforcement learning from human feedback), non-sufficiency yields strictly positive lower bounds arising from the same structural decomposition into annotation noise, preference distortion, and semantic compression. The theory explains why scaling alone cannot eliminate persistent human-aligned errors and characterizes conditions under which auxiliary non-human signals (e.g., retrieval, program execution, tools) increase effective supervision capacity and collapse the floor by restoring information about the latent target. Experiments on real preference data, synthetic known-target tasks, and externally verifiable benchmarks confirm the predicted structural signatures: human-only supervision exhibits a persistent floor, while sufficiently informative auxiliary channels strictly reduce or eliminate excess error.", "AI": {"tldr": "The paper argues that persistent errors in large language models stem from fundamental limitations in human supervision channels, not model scale, creating an unavoidable \"excess-risk floor\" that can only be overcome with non-human auxiliary signals.", "motivation": "Large language models exhibit persistent errors despite training on human data/feedback. The authors hypothesize these errors arise from structural properties of the human supervision channel itself (annotation noise, subjective preferences, limited language bandwidth), not model scale or optimization.", "method": "Develops a unified theory called \"Human-Bounded Intelligence limit\" and validates it through six complementary frameworks: operator theory, PAC-Bayes, information theory, causal inference, category theory, and game-theoretic RLHF analysis. Uses experiments on real preference data, synthetic tasks, and verifiable benchmarks.", "result": "Confirms that insufficient human supervision creates a strictly positive lower bound (excess-risk floor) for errors. Non-human auxiliary signals (retrieval, program execution, tools) can collapse this floor by providing missing information about the latent evaluation target.", "conclusion": "Scaling alone cannot eliminate persistent human-aligned errors due to information-reducing properties of human supervision. Progress requires augmenting supervision with sufficiently informative non-human channels to increase effective supervision capacity."}}
{"id": "2602.23751", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.23751", "abs": "https://arxiv.org/abs/2602.23751", "authors": ["Morteza Zarei", "Mohammad Hossein Zarei"], "title": "Spin stiffness and resilience phase transition in a noisy toric-rotor code", "comment": "11 pages, 7 figures, Submitted to Physical Review A", "summary": "We use a quantum formalism for the partition function of the classical $XY$ model to identify a resilience phase transition in a noisy toric-rotor code. Specifically, we consider the toric-rotor code under phase-shift noise described by a von Mises probability distribution and show that the fidelity between the final state after noise and the initial state is proportional to the partition function of the $XY$ model. We map the temperature of the $XY$ model to the width of the noise in the toric-rotor code, such that a Kosterlitz--Thouless phase transition at a critical temperature $T_{c}$ corresponds to a mixed-state phase transition at a critical width $\u03c3_c$. To characterize this phase transition, we develop a quantum formalism for the spin stiffness in the $XY$ model and show that it is mapped to the gate fidelity in the logical subspace of the toric-rotor code. In particular, we introduce a topological order parameter that characterizes the resilience of the toric-rotor code to decoherence within the logical subspace. We show that the logical subspace does not exhibit complete resilience to noise, which is a necessary condition for correctability. However, it exhibits partial resilience to noise for widths less than $\u03c3_c\\approx 0.89$, where the resilience order parameter takes values near $1$ and then drops to zero at $\u03c3_c$. We also use our results to shed light on the correctability of toric-rotor codes in higher dimensions $d > 2$. Our work shows that the quantum formalism for partition functions provides a mathematically rigorous framework for studying correctability in continuous-variable quantum codes.", "AI": {"tldr": "This paper establishes a quantum formalism linking the partition function of the classical XY model to the resilience phase transition in toric-rotor codes under phase-shift noise, identifying a critical noise width \u03c3_c\u22480.89 where partial resilience is lost.", "motivation": "To develop a mathematically rigorous framework for studying correctability in continuous-variable quantum codes by leveraging connections between quantum error correction and statistical physics models.", "method": "Maps the toric-rotor code's noise-induced fidelity decay to the XY model's partition function, equating noise width \u03c3 to temperature T; quantifies resilience via a quantum spin stiffness formalism mapped to logical gate fidelity.", "result": "Identifies a Kosterlitz-Thouless-like phase transition at \u03c3_c\u22480.89: the logical subspace shows near-complete resilience (order parameter \u22481) for \u03c3<\u03c3_c but loses all resilience at \u03c3_c, proving toric-rotor codes lack full correctability.", "conclusion": "The XY model partition function framework rigorously characterizes partial resilience in continuous-variable codes; toric-rotor codes exhibit correctability limits in 2D but may inform higher-dimensional (d>2) code analysis."}}
{"id": "2602.23605", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23605", "abs": "https://arxiv.org/abs/2602.23605", "authors": ["Zongzhe Xu", "Zitao Shuai", "Eideen Mozaffari", "Ravi S. Aysola", "Rajesh Kumar", "Yuzhe Yang"], "title": "SleepLM: Natural-Language Intelligence for Human Sleep", "comment": null, "summary": "We present SleepLM, a family of sleep-language foundation models that enable human sleep alignment, interpretation, and interaction with natural language. Despite the critical role of sleep, learning-based sleep analysis systems operate in closed label spaces (e.g., predefined stages or events) and fail to describe, query, or generalize to novel sleep phenomena. SleepLM bridges natural language and multimodal polysomnography, enabling language-grounded representations of sleep physiology. To support this alignment, we introduce a multilevel sleep caption generation pipeline that enables the curation of the first large-scale sleep-text dataset, comprising over 100K hours of data from more than 10,000 individuals. Furthermore, we present a unified pretraining objective that combines contrastive alignment, caption generation, and signal reconstruction to better capture physiological fidelity and cross-modal interactions. Extensive experiments on real-world sleep understanding tasks verify that SleepLM outperforms state-of-the-art in zero-shot and few-shot learning, cross-modal retrieval, and sleep captioning. Importantly, SleepLM also exhibits intriguing capabilities including language-guided event localization, targeted insight generation, and zero-shot generalization to unseen tasks. All code and data will be open-sourced.", "AI": {"tldr": "\u63d0\u51faSleepLM\u7761\u7720\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u7cfb\u5217\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5b9e\u73b0\u7761\u7720\u5bf9\u9f50\u3001\u89e3\u8bfb\u4e0e\u4ea4\u4e92\uff0c\u6784\u5efa\u9996\u4e2a\u8d8510\u4e07\u5c0f\u65f6\u7684\u5927\u89c4\u6a21\u7761\u7720-\u6587\u672c\u6570\u636e\u96c6\uff0c\u5728\u591a\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u6280\u672f", "motivation": "\u73b0\u6709\u5b66\u4e60\u5f0f\u7761\u7720\u5206\u6790\u7cfb\u7edf\u5c40\u9650\u4e8e\u5c01\u95ed\u6807\u7b7e\u7a7a\u95f4\uff08\u5982\u9884\u8bbe\u9636\u6bb5\u6216\u4e8b\u4ef6\uff09\uff0c\u65e0\u6cd5\u63cf\u8ff0\u3001\u67e5\u8be2\u6216\u6cdb\u5316\u5230\u65b0\u7761\u7720\u73b0\u8c61\uff0c\u7f3a\u4e4f\u4e0e\u751f\u7406\u4fe1\u53f7\u5bf9\u9f50\u7684\u81ea\u7136\u8bed\u8a00\u63a5\u53e3", "method": "1) \u5f00\u53d1\u591a\u7ea7\u7761\u7720\u5b57\u5e55\u751f\u6210\u6d41\u7a0b\uff0c\u6784\u5efa\u542b10\u4e07+\u5c0f\u65f6\u6570\u636e\u7684\u5927\u89c4\u6a21\u7761\u7720-\u6587\u672c\u6570\u636e\u96c6\uff1b2) \u8bbe\u8ba1\u7edf\u4e00\u9884\u8bad\u7ec3\u76ee\u6807\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5bf9\u9f50\u3001\u5b57\u5e55\u751f\u6210\u4e0e\u4fe1\u53f7\u91cd\u5efa\uff1b3) \u8bad\u7ec3\u591a\u6a21\u6001\u7761\u7720-\u8bed\u8a00\u57fa\u7840\u6a21\u578b", "result": "\u5728\u96f6\u6837\u672c/\u5c11\u6837\u672c\u5b66\u4e60\u3001\u8de8\u6a21\u6001\u68c0\u7d22\u3001\u7761\u7720\u5b57\u5e55\u751f\u6210\u4efb\u52a1\u4e0a\u5747\u8d85\u8d8a\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u5c55\u73b0\u51fa\u8bed\u8a00\u5f15\u5bfc\u4e8b\u4ef6\u5b9a\u4f4d\u3001\u5b9a\u5411\u6d1e\u5bdf\u751f\u6210\u53ca\u5bf9\u672a\u89c1\u4efb\u52a1\u7684\u96f6\u6837\u672c\u6cdb\u5316\u7b49\u65b0\u5174\u80fd\u529b", "conclusion": "SleepLM\u6210\u529f\u6865\u63a5\u81ea\u7136\u8bed\u8a00\u4e0e\u591a\u6a21\u6001\u591a\u5bfc\u7761\u7720\u56fe\uff0c\u4e3a\u7761\u7720\u751f\u7406\u63d0\u4f9b\u8bed\u8a00\u63a5\u5730\u8868\u793a\uff0c\u4ee3\u7801\u4e0e\u6570\u636e\u5c06\u5f00\u6e90\uff0c\u63a8\u52a8\u7761\u7720\u7814\u7a76\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u53ef\u4ea4\u4e92\u6027\u53d1\u5c55"}}
{"id": "2602.23625", "categories": ["quant-ph", "hep-th"], "pdf": "https://arxiv.org/pdf/2602.23625", "abs": "https://arxiv.org/abs/2602.23625", "authors": ["N. L. Diaz"], "title": "From quantum time to manifestly covariant QFT: on the need for a quantum-action-based quantization", "comment": "27 pages, 2 figures", "summary": "In quantum time (QT) schemes, time is promoted to a degree of freedom, allowing Lorentz covariance to be made explicit for single particles. We ask whether this can be lifted to QFT, so that Lorentz covariance becomes manifest at the Hilbert-space level, rather than being hidden as in the standard canonical formulation. We address this question by proposing a second-quantized approach in which the elementary particle is the QT particle itself, leading naturally to the notion of spacetime field algebras and of quantum action. We show, however, that a naive many-body construction runs into inconsistencies. To pinpoint their origin we introduce a classical counterpart of the second-quantized formalism, spacetime classical mechanics (SCM), and prove a no-go theorem: Dirac quantization of SCM collapses back to standard QFT and therefore hides covariance. We circumvent this problem by presenting a quantum-action--based quantization that yields a spacetime version of quantum mechanics (SQM), making covariance manifest for (interacting) QFTs. Finally, we show that this resolution is tied to a genuine spacetime generalization of the notion of quantum state, required by causality and closely connected to recent ``states over time'' proposals and, in dS/CFT-motivated settings, to microscopic notions of timelike entanglement and emergent time.", "AI": {"tldr": "\u5c06\u91cf\u5b50\u65f6\u95f4\u65b9\u6848\u6269\u5c55\u5230\u91cf\u5b50\u573a\u8bba\u4ee5\u5b9e\u73b0\u660e\u663e\u7684\u6d1b\u4f26\u5179\u534f\u53d8\u6027\uff0c\u901a\u8fc7\u65b0\u7684\u91cf\u5b50\u5316\u65b9\u6cd5\u514b\u670d\u65e0\u6cd5\u5b9a\u7406", "motivation": "\u6807\u51c6\u91cf\u5b50\u573a\u8bba\u4e2d\u6d1b\u4f26\u5179\u534f\u53d8\u6027\u88ab\u9690\u85cf\uff0c\u800c\u91cf\u5b50\u65f6\u95f4\u65b9\u6848\u4f7f\u5355\u7c92\u5b50\u534f\u53d8\u6027\u663e\u5f0f\uff0c\u56e0\u6b64\u5e0c\u671b\u63a8\u5e7f\u5230\u91cf\u5b50\u573a\u8bba\u5c42\u9762\uff0c\u4f7f\u534f\u53d8\u6027\u5728\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u7ea7\u522b\u663e\u5f0f", "method": "\u63d0\u51fa\u4e8c\u6b21\u91cf\u5b50\u5316\u65b9\u6cd5\uff0c\u4ee5\u91cf\u5b50\u65f6\u95f4\u7c92\u5b50\u4e3a\u57fa\u672c\u5355\u5143\uff1b\u5f15\u5165\u65f6\u7a7a\u7ecf\u5178\u529b\u5b66\u4f5c\u4e3a\u7ecf\u5178\u5bf9\u5e94\uff1b\u8bc1\u660e\u72c4\u62c9\u514b\u91cf\u5b50\u5316\u7684\u65e0\u6cd5\u5b9a\u7406\uff1b\u5f00\u53d1\u57fa\u4e8e\u91cf\u5b50\u4f5c\u7528\u7684\u91cf\u5b50\u5316\u65b9\u6848", "result": "\u6734\u7d20\u591a\u4f53\u6784\u9020\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\uff1b\u72c4\u62c9\u514b\u91cf\u5b50\u5316\u65f6\u7a7a\u7ecf\u5178\u529b\u5b66\u9000\u5316\u4e3a\u6807\u51c6\u91cf\u5b50\u573a\u8bba\uff1b\u800c\u57fa\u4e8e\u91cf\u5b50\u4f5c\u7528\u7684\u91cf\u5b50\u5316\u80fd\u4ea7\u751f\u65f6\u7a7a\u91cf\u5b50\u529b\u5b66\uff0c\u4f7f\u76f8\u4e92\u4f5c\u7528\u91cf\u5b50\u573a\u8bba\u5177\u6709\u663e\u5f0f\u534f\u53d8\u6027", "conclusion": "\u8fd9\u9700\u8981\u65f6\u7a7a\u91cf\u5b50\u6001\u7684\u63a8\u5e7f\uff0c\u4e0e\u56e0\u679c\u6027\u548c\u201c\u65f6\u95f4\u4e0a\u7684\u6001\u201d\u7b49\u6982\u5ff5\u76f8\u5173\uff0c\u5728dS/CFT\u80cc\u666f\u4e0b\u6d89\u53ca\u65f6\u7a7a\u7ea0\u7f20\u548c\u6d8c\u73b0\u65f6\u95f4"}}
{"id": "2602.24051", "categories": ["cond-mat.str-el", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.24051", "abs": "https://arxiv.org/abs/2602.24051", "authors": ["Benjamin Canals"], "title": "Emergence of geometric order from topological constraints in a three-dimensional Coulomb phase", "comment": "5 pages, 4 figures", "summary": "The emergence of order and geometric limit shapes in a three-dimensional (3D) Coulomb phase subject to domain wall boundary conditions (DWBC) is investigated. While the arctic circle phenomenon -- the spatial segregation of frozen and fluctuating degrees of freedom -- is well-established in the two-dimensional six-vertex model (square ice), its extension to 3D remains largely unexplored. A cubic lattice model with Ising degrees of freedom living on the edges, whose ground state manifold is governed by a divergence-free (3-in/3-out) local constraint, is considered. In the bulk, this model realizes a classical spin liquid characterized by algebraic correlations and pinch-point singularities in reciprocal space. It is demonstrated that applying DWBC partially lifts the extensive ground state degeneracy, inducing long-range magnetic order in the thermodynamic limit. Despite this ordering, it is found that the system retains a fluctuating component that exhibits the signature of a Coulomb phase. Finally, by mapping the local vertex polarization density, compelling numerical support is provided for a 3D generalization of the arctic limit shape, bridging the gap between topological constraints and emergent geometry in higher dimensions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e09\u7ef4\u5e93\u4ed1\u76f8\u5728\u57df\u58c1\u8fb9\u754c\u6761\u4ef6\u4e0b\u7684\u51e0\u4f55\u6781\u9650\u5f62\u72b6\u548c\u6709\u5e8f\u73b0\u8c61\uff0c\u9996\u6b21\u8bc1\u5b9e\u4e09\u7ef4\u4f53\u7cfb\u4e2d\u4e5f\u5b58\u5728\u7c7b\u4f3c\u4e8c\u7ef4\"\u5317\u6781\u5708\"\u7684\u6781\u9650\u5f62\u72b6\uff0c\u63ed\u793a\u4e86\u62d3\u6251\u7ea6\u675f\u4e0e\u9ad8\u7ef4\u6d8c\u73b0\u51e0\u4f55\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "motivation": "\u4e8c\u7ef4\u516d\u9876\u70b9\u6a21\u578b\uff08\u65b9\u683c\u51b0\uff09\u4e2d\u5df2\u786e\u7acb\"\u5317\u6781\u5708\"\u73b0\u8c61\uff08\u7a7a\u95f4\u5206\u79bb\u51bb\u7ed3\u4e0e\u6da8\u843d\u81ea\u7531\u5ea6\uff09\uff0c\u4f46\u8be5\u73b0\u8c61\u5411\u4e09\u7ef4\u7684\u62d3\u5c55\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u5b58\u5728\u7406\u8bba\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u4e09\u7ef4\u7acb\u65b9\u6676\u683c\u6a21\u578b\uff0c\u8fb9\u4f4d\u70b9\u8bbe\u7f6eIsing\u81ea\u65cb\uff0c\u65bd\u52a0\u6563\u5ea6\u4e3a\u96f6\u7684\u5c40\u90e83-in/3-out\u7ea6\u675f\uff1b\u901a\u8fc7\u57df\u58c1\u8fb9\u754c\u6761\u4ef6\u90e8\u5206\u89e3\u9664\u57fa\u6001\u7b80\u5e76\uff0c\u7ed3\u5408\u9876\u70b9\u6781\u5316\u5bc6\u5ea6\u6620\u5c04\u8fdb\u884c\u6570\u503c\u6a21\u62df\u3002", "result": "1) \u57df\u58c1\u8fb9\u754c\u8bf1\u5bfc\u70ed\u529b\u5b66\u6781\u9650\u4e0b\u7684\u957f\u7a0b\u78c1\u5e8f\uff1b2) \u4f53\u7cfb\u4fdd\u7559\u5e93\u4ed1\u76f8\u6da8\u843d\u7279\u5f81\uff08\u4ee3\u6570\u5173\u8054\u4e0e\u5012\u7a7a\u95f4\u9488\u5c16\u5947\u70b9\uff09\uff1b3) \u83b7\u5f97\u4e09\u7ef4\"\u5317\u6781\u6781\u9650\u5f62\u72b6\"\u7684\u6570\u503c\u8bc1\u636e\u3002", "conclusion": "\u6210\u529f\u5efa\u7acb\u4e09\u7ef4\u5e93\u4ed1\u76f8\u4e2d\u62d3\u6251\u7ea6\u675f\u4e0e\u6d8c\u73b0\u51e0\u4f55\u7684\u6865\u6881\uff0c\u8bc1\u5b9e\u4e86\u9ad8\u7ef4\u4f53\u7cfb\u4e2d\u51e0\u4f55\u6781\u9650\u5f62\u72b6\u7684\u5b58\u5728\u6027\uff0c\u4e3a\u7406\u89e3\u9ad8\u7ef4\u81ea\u65cb\u6db2\u4f53\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2602.23459", "categories": ["cs.LG", "q-bio.QM", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.23459", "abs": "https://arxiv.org/abs/2602.23459", "authors": ["Eric V. Strobl"], "title": "Global Interpretability via Automated Preprocessing: A Framework Inspired by Psychiatric Questionnaires", "comment": null, "summary": "Psychiatric questionnaires are highly context sensitive and often only weakly predict subsequent symptom severity, which makes the prognostic relationship difficult to learn. Although flexible nonlinear models can improve predictive accuracy, their limited interpretability can erode clinical trust. In fields such as imaging and omics, investigators commonly address visit- and instrument-specific artifacts by extracting stable signal through preprocessing and then fitting an interpretable linear model. We adopt the same strategy for questionnaire data by decoupling preprocessing from prediction: we restrict nonlinear capacity to a baseline preprocessing module that estimates stable item values, and then learn a linear mapping from these stabilized baseline items to future severity. We refer to this two-stage method as REFINE (Redundancy-Exploiting Follow-up-Informed Nonlinear Enhancement), which concentrates nonlinearity in preprocessing while keeping the prognostic relationship transparently linear and therefore globally interpretable through a coefficient matrix, rather than through post hoc local attributions. In experiments, REFINE outperforms other interpretable approaches while preserving clear global attribution of prognostic factors across psychiatric and non-psychiatric longitudinal prediction tasks.", "AI": {"tldr": "\u63d0\u51faREFINE\u65b9\u6cd5\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u9884\u5904\u7406\u7a33\u5b9a\u95ee\u5377\u6761\u76ee+\u7ebf\u6027\u9884\u6d4b\u6a21\u578b\uff0c\u89e3\u51b3\u7cbe\u795e\u75be\u75c5\u9884\u540e\u9884\u6d4b\u4e2d\u51c6\u786e\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u7684\u77db\u76fe", "motivation": "\u7cbe\u795e\u75be\u75c5\u95ee\u5377\u9884\u6d4b\u75c7\u72b6\u4e25\u91cd\u7a0b\u5ea6\u65f6\u5b58\u5728\u4e0a\u4e0b\u6587\u654f\u611f\u6027\u9ad8\u3001\u9884\u6d4b\u529b\u5f31\u7684\u95ee\u9898\uff1b\u975e\u7ebf\u6027\u6a21\u578b\u867d\u63d0\u5347\u7cbe\u5ea6\u4f46\u7f3a\u4e4f\u4e34\u5e8a\u4fe1\u4efb\u6240\u9700\u7684\u89e3\u91ca\u6027", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u7528\u975e\u7ebf\u6027\u6a21\u5757\u4f30\u8ba1\u7a33\u5b9a\u57fa\u7ebf\u6761\u76ee\u503c\uff08REFINE\u9884\u5904\u7406\uff09 2) \u7528\u7ebf\u6027\u6a21\u578b\u6620\u5c04\u5230\u672a\u6765\u4e25\u91cd\u7a0b\u5ea6\uff0c\u5c06\u975e\u7ebf\u6027\u9650\u5236\u5728\u9884\u5904\u7406\u9636\u6bb5\uff0c\u4fdd\u6301\u9884\u6d4b\u5173\u7cfb\u5168\u5c40\u53ef\u89e3\u91ca", "result": "\u5728\u591a\u9879\u7cbe\u795e\u53ca\u975e\u7cbe\u795e\u7eb5\u5411\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cREFINE\u8d85\u8d8a\u5176\u4ed6\u53ef\u89e3\u91ca\u65b9\u6cd5\uff0c\u540c\u65f6\u63d0\u4f9b\u6e05\u6670\u7684\u9884\u540e\u56e0\u7d20\u5168\u5c40\u5f52\u56e0\uff08\u901a\u8fc7\u7cfb\u6570\u77e9\u9635\uff09", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u89e3\u8026\u9884\u5904\u7406\u4e0e\u9884\u6d4b\uff0c\u5728\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u7684\u540c\u65f6\u4fdd\u6301\u900f\u660e\u7ebf\u6027\u5173\u7cfb\uff0c\u4e3a\u4e34\u5e8a\u53ef\u4fe1\u8d56\u7684\u9884\u540e\u6a21\u578b\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2602.23632", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23632", "abs": "https://arxiv.org/abs/2602.23632", "authors": ["Lun Zhan", "Feng Xiong", "Huanyong Liu", "Feng Zhang", "Yuhui Yin"], "title": "MMKG-RDS: Reasoning Data Synthesis via Deep Mining of Multimodal Knowledge Graphs", "comment": null, "summary": "Synthesizing high-quality training data is crucial for enhancing domain models' reasoning abilities. Existing methods face limitations in long-tail knowledge coverage, effectiveness verification, and interpretability. Knowledge-graph-based approaches still fall short in functionality, granularity, customizability, and evaluation. To address these issues, we propose MMKG-RDS, a flexible framework for reasoning data synthesis that leverages multimodal knowledge graphs. It supports fine-grained knowledge extraction, customizable path sampling, and multidimensional data quality scoring. We validate MMKG-RDS with the MMKG-RDS-Bench dataset, covering five domains, 17 task types, and 14,950 samples. Experimental results show fine-tuning Qwen3 models (0.6B/8B/32B) on a small number of synthesized samples improves reasoning accuracy by 9.2%. The framework also generates distinct data, challenging existing models on tasks involving tables and formulas, useful for complex benchmark construction. The dataset and code are available at https://github.com/360AILAB-NLP/MMKG-RDS", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMMKG-RDS\u6846\u67b6\uff0c\u5229\u7528\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u5408\u6210\u9ad8\u8d28\u91cf\u63a8\u7406\u8bad\u7ec3\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u5c3e\u77e5\u8bc6\u8986\u76d6\u3001\u6548\u679c\u9a8c\u8bc1\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u5c40\u9650\u3002\u5b9e\u9a8c\u8868\u660e\u4f7f\u7528\u5c11\u91cf\u5408\u6210\u6570\u636e\u5fae\u8c03Qwen3\u6a21\u578b\u5373\u53ef\u63d0\u53479.2%\u63a8\u7406\u51c6\u786e\u7387\uff0c\u5e76\u53d1\u5e03\u6db5\u76d65\u4e2a\u9886\u57df\u300114,950\u4e2a\u6837\u672c\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u8bad\u7ec3\u6570\u636e\u5408\u6210\u65b9\u6cd5\u5728\u957f\u5c3e\u77e5\u8bc6\u8986\u76d6\u3001\u6709\u6548\u6027\u9a8c\u8bc1\u548c\u89e3\u91ca\u6027\u65b9\u9762\u5b58\u5728\u9650\u5236\uff1b\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u65b9\u6cd5\u5728\u529f\u80fd\u6027\u3001\u9897\u7c92\u5ea6\u3001\u53ef\u5b9a\u5236\u6027\u548c\u8bc4\u4f30\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\uff0c\u4e9f\u9700\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u5347\u9886\u57df\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faMMKG-RDS\u6846\u67b6\uff0c\u91c7\u7528\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u4f5c\u4e3a\u57fa\u7840\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u62bd\u53d6\u3001\u53ef\u5b9a\u5236\u8def\u5f84\u91c7\u6837\u548c\u591a\u7ef4\u5ea6\u6570\u636e\u8d28\u91cf\u8bc4\u5206\u3002\u6784\u5efaMMKG-RDS-Bench\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d65\u4e2a\u9886\u57df\u300117\u79cd\u4efb\u52a1\u7c7b\u578b\u548c14,950\u4e2a\u6837\u672c\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728Qwen3\u7cfb\u5217\u6a21\u578b(0.6B/8B/32B)\u4e0a\u4f7f\u7528\u5c11\u91cf\u5408\u6210\u6837\u672c\u8fdb\u884c\u5fae\u8c03\uff0c\u63a8\u7406\u51c6\u786e\u7387\u5e73\u5747\u63d0\u53479.2%\u3002\u751f\u6210\u7684\u6570\u636e\u5bf9\u8868\u683c\u548c\u516c\u5f0f\u4efb\u52a1\u6784\u6210\u6311\u6218\uff0c\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "conclusion": "MMKG-RDS\u6846\u67b6\u4e3a\u63a8\u7406\u6570\u636e\u5408\u6210\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u751f\u6210\u7684\u6570\u636e\u8d28\u91cf\u9ad8\u4e14\u591a\u6837\u5316\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u590d\u6742\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63a8\u52a8\u9886\u57df\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\u3002"}}
{"id": "2602.23650", "categories": ["quant-ph", "cond-mat.mes-hall", "hep-th"], "pdf": "https://arxiv.org/pdf/2602.23650", "abs": "https://arxiv.org/abs/2602.23650", "authors": ["Xu Zhang", "Qiang Gu"], "title": "Perfect transmission of a Dirac particle in one-dimension double square barrier", "comment": "9 pages, 5 figures", "summary": "Dirac particles can undergo perfect transmission through a sufficiently high potential barrier in the Klein zone. Although the perfect Klein tunneling (often referred to as the Klein paradox) is similar to the non-relativistic resonant transmission which occurs only when the kinetic energy exceeds the barrier, the underlying mechanism is believed to be fundamentally distinct. In this work, we show that for the relativistic double-barrier model the perfect-transmission curve can pass continuously from the above-barrier zone to the Klein zone. Additionally, in the Klein zone, perfect transmission occurs even for subcritical barrier heights, supported by both bound-state analysis and wave-packet dynamics. These findings suggest a connection between perfect Klein tunneling and resonant transmission, and provide new insights into the physical nature of the Klein paradox.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63ed\u793a\u76f8\u5bf9\u8bba\u6027\u53cc\u52bf\u5792\u6a21\u578b\u4e2d\u5b8c\u7f8eKlein\u96a7\u7a7f\u4e0e\u5171\u632f\u900f\u5c04\u5b58\u5728\u8fde\u7eed\u8fc7\u6e21\uff0c\u4e14Klein\u533a\u5728\u4e9a\u4e34\u754c\u52bf\u5792\u9ad8\u5ea6\u4e0b\u4ecd\u53ef\u5b9e\u73b0\u5b8c\u7f8e\u900f\u5c04\uff0c\u6311\u6218\u4e86\u4e8c\u8005\u673a\u5236\u6839\u672c\u4e0d\u540c\u7684\u4f20\u7edf\u89c2\u70b9\u3002", "motivation": "\u4f20\u7edf\u7406\u8bba\u8ba4\u4e3aKlein\u96a7\u7a7f\uff08\u514b\u83b1\u56e0\u6096\u8bba\uff09\u4e0e\u975e\u76f8\u5bf9\u8bba\u6027\u5171\u632f\u900f\u5c04\u673a\u5236\u5b58\u5728\u672c\u8d28\u533a\u522b\uff0c\u4f46\u4e8c\u8005\u5747\u8868\u73b0\u4e3a\u52bf\u5792\u4e0a\u7684\u5b8c\u7f8e\u900f\u5c04\u73b0\u8c61\uff0c\u5176\u5185\u5728\u5173\u8054\u6027\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u91c7\u7528\u76f8\u5bf9\u8bba\u6027\u53cc\u52bf\u5792\u6a21\u578b\uff0c\u7ed3\u5408\u675f\u7f1a\u6001\u5206\u6790\u4e0e\u6ce2\u5305\u52a8\u529b\u5b66\u6a21\u62df\uff0c\u7cfb\u7edf\u7814\u7a76\u900f\u5c04\u7279\u6027\u968f\u52bf\u5792\u53c2\u6570\u7684\u6f14\u5316\u89c4\u5f8b\u3002", "result": "\u53d1\u73b0\u5b8c\u7f8e\u900f\u5c04\u9891\u8c31\u66f2\u7dda\u53ef\u4ece\"above-barrier\"\u533a\u8fde\u7eed\u8fc7\u6e21\u81f3Klein\u533a\uff1b\u5728Klein\u533a\u4e2d\uff0c\u5373\u4f7f\u52bf\u5792\u9ad8\u5ea6\u4f4e\u4e8e\u4e34\u754c\u503c\uff08subcritical\uff09\uff0c\u4ecd\u53ef\u5b9e\u73b0\u5b8c\u7f8e\u900f\u5c04\u3002", "conclusion": "\u8bc1\u5b9eKlein\u96a7\u7a7f\u4e0e\u5171\u632f\u900f\u5c04\u5b58\u5728\u7269\u7406\u673a\u5236\u4e0a\u7684\u5173\u8054\u6027\uff0c\u4e3a\u7406\u89e3\u514b\u83b1\u56e0\u6096\u8bba\u7684\u672c\u8d28\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u8868\u660e\u5176\u53ef\u80fd\u6e90\u4e8e\u76f8\u540c\u7684\u91cf\u5b50\u5e72\u6d89\u6548\u5e94\u3002"}}
{"id": "2602.24135", "categories": ["cond-mat.str-el", "cond-mat.mes-hall", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2602.24135", "abs": "https://arxiv.org/abs/2602.24135", "authors": ["Bingbing Wang", "Yongpan Li", "Yichen Liu", "Cheng-Cheng Liu"], "title": "Spontaneous Fully Compensated Ferrimagnetism", "comment": null, "summary": "We propose a general mechanism for the spontaneous emergence of filling-enforced fully compensated ferrimagnetism (fFIM), characterized by zero net magnetization yet ferromagnetic-like spin-split band structures. Using Hartree-Fock mean-field calculations of the Hubbard model, we map out the stability regime of spontaneous fFIM over a broad parameter space of interaction strength and staggered potential. We show the unique quantum-geometry-governed optical selection rules and the abundant valley- and spin-related physics of electronics and optics arising from the emergence of fFIM order, with tunable spin-polarized and valley-contrasting charge and spin currents. Furthermore, based on our theory, we demonstrate that spontaneous fFIM can be realized in nominally nonmagnetic graphene via defect engineering. Our results establish a unified framework for the mechanism, emergent properties, and materials realization of spontaneous fFIM, opening new opportunities for spintronic, valleytronic, and optoelectronic applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u53d1\u4ea7\u751f\u5b8c\u5168\u8865\u507f\u94c1\u78c1\u6027\u7684\u901a\u7528\u673a\u5236\uff0c\u8fd9\u79cd\u78c1\u6027\u6001\u5177\u6709\u96f6\u51c0\u78c1\u5316\u5f3a\u5ea6\u4f46\u7c7b\u4f3c\u94c1\u78c1\u4f53\u7684\u81ea\u65cb\u5288\u88c2\u80fd\u5e26\u7ed3\u6784\uff0c\u5728\u77f3\u58a8\u70ef\u7b49\u6750\u6599\u4e2d\u53ef\u901a\u8fc7\u7f3a\u9677\u5de5\u7a0b\u5b9e\u73b0\uff0c\u4e3a\u81ea\u65cb\u7535\u5b50\u5b66\u548c\u8c37\u7535\u5b50\u5b66\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u6846\u67b6\u3002", "motivation": "\u5bfb\u627e\u5177\u6709\u96f6\u51c0\u78c1\u5316\u5f3a\u5ea6\u4f46\u4fdd\u7559\u81ea\u65cb\u76f8\u5173\u7279\u6027\u7684\u65b0\u578b\u78c1\u6027\u6001\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u94c1\u78c1\u4f53\u5728\u81ea\u65cb\u7535\u5b50\u5668\u4ef6\u4e2d\u56e0\u51c0\u78c1\u5316\u5e26\u6765\u7684\u7a33\u5b9a\u6027\u548c\u96c6\u6210\u5ea6\u95ee\u9898\u3002", "method": "\u91c7\u7528\u54c8\u7279\u91cc-\u798f\u514b\u5e73\u5747\u573a\u65b9\u6cd5\u5bf9\u54c8\u4f2f\u5fb7\u6a21\u578b\u8fdb\u884c\u8ba1\u7b97\uff0c\u7cfb\u7edf\u7814\u7a76\u4e86\u76f8\u4e92\u4f5c\u7528\u5f3a\u5ea6\u548c\u4ea4\u9519\u52bf\u53c2\u6570\u7a7a\u95f4\u4e2d\u81ea\u53d1fFIM\u7684\u7a33\u5b9a\u6027\u533a\u57df\u3002", "result": "\u63ed\u793a\u4e86fFIM\u72ec\u7279\u7684\u91cf\u5b50\u51e0\u4f55\u5149\u5b66\u9009\u62e9\u89c4\u5219\uff0c\u9884\u8a00\u4e86\u4e30\u5bcc\u7684\u8c37\u81ea\u65cb\u76f8\u5173\u7684\u7535\u5b50\u5149\u5b66\u7279\u6027\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u7f3a\u9677\u5de5\u7a0b\u53ef\u5728\u77f3\u58a8\u70ef\u4e2d\u5b9e\u73b0\u8be5\u78c1\u6027\u6001\u3002", "conclusion": "\u5efa\u7acb\u4e86\u81ea\u53d1fFIM\u7684\u673a\u5236\u3001\u7269\u6027\u8868\u5f81\u4e0e\u6750\u6599\u5b9e\u73b0\u76f8\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u4e3a\u5f00\u53d1\u65b0\u578b\u81ea\u65cb\u7535\u5b50\u3001\u8c37\u7535\u5b50\u548c\u5149\u7535\u5668\u4ef6\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.23495", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23495", "abs": "https://arxiv.org/abs/2602.23495", "authors": ["Yangyi Li", "Mengdi Huai"], "title": "Uncertainty-aware Language Guidance for Concept Bottleneck Models", "comment": null, "summary": "Concept Bottleneck Models (CBMs) provide inherent interpretability by first mapping input samples to high-level semantic concepts, followed by a combination of these concepts for the final classification. However, the annotation of human-understandable concepts requires extensive expert knowledge and labor, constraining the broad adoption of CBMs. On the other hand, there are a few works that leverage the knowledge of large language models (LLMs) to construct concept bottlenecks. Nevertheless, they face two essential limitations: First, they overlook the uncertainty associated with the concepts annotated by LLMs and lack a valid mechanism to quantify uncertainty about the annotated concepts, increasing the risk of errors due to hallucinations from LLMs. Additionally, they fail to incorporate the uncertainty associated with these annotations into the learning process for concept bottleneck models. To address these limitations, we propose a novel uncertainty-aware CBM method, which not only rigorously quantifies the uncertainty of LLM-annotated concept labels with valid and distribution-free guarantees, but also incorporates quantified concept uncertainty into the CBM training procedure to account for varying levels of reliability across LLM-annotated concepts. We also provide the theoretical analysis for our proposed method. Extensive experiments on the real-world datasets validate the desired properties of our proposed methods.", "AI": {"tldr": "A novel uncertainty-aware Concept Bottleneck Model that quantifies and incorporates LLM annotation uncertainty into training to improve reliability and interpretability.", "motivation": "Manual concept annotation for CBMs is labor-intensive and requires expert knowledge. While LLMs can automate this, their outputs suffer from hallucinations and uncertainty that existing methods fail to properly quantify and account for during training.", "method": "Proposes an uncertainty-aware CBM that rigorously quantifies LLM-annotated concept uncertainty with distribution-free guarantees, then incorporates this uncertainty into the CBM training procedure to handle varying reliability levels across concepts.", "result": "Provides theoretical analysis of the method and validates its desired properties through extensive experiments on real-world datasets.", "conclusion": "The proposed method successfully addresses key limitations of LLM-based concept annotation by explicitly handling uncertainty, leading to more robust and reliable interpretable models with theoretical guarantees."}}
{"id": "2602.23643", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23643", "abs": "https://arxiv.org/abs/2602.23643", "authors": ["Judah Goldfeder", "Philippe Wyder", "Yann LeCun", "Ravid Shwartz Ziv"], "title": "AI Must Embrace Specialization via Superhuman Adaptable Intelligence", "comment": null, "summary": "Everyone from AI executives and researchers to doomsayers, politicians, and activists is talking about Artificial General Intelligence (AGI). Yet, they often don't seem to agree on its exact definition. One common definition of AGI is an AI that can do everything a human can do, but are humans truly general? In this paper, we address what's wrong with our conception of AGI, and why, even in its most coherent formulation, it is a flawed concept to describe the future of AI. We explore whether the most widely accepted definitions are plausible, useful, and truly general. We argue that AI must embrace specialization, rather than strive for generality, and in its specialization strive for superhuman performance, and introduce Superhuman Adaptable Intelligence (SAI). SAI is defined as intelligence that can learn to exceed humans at anything important that we can do, and that can fill in the skill gaps where humans are incapable. We then lay out how SAI can help hone a discussion around AI that was blurred by an overloaded definition of AGI, and extrapolate the implications of using it as a guide for the future.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6279\u5224\u4e86AGI\uff08\u901a\u7528\u4eba\u5de5\u667a\u80fd\uff09\u6982\u5ff5\u7684\u7f3a\u9677\uff0c\u63d0\u51faSAI\uff08\u8d85\u4eba\u7c7b\u9002\u5e94\u667a\u80fd\uff09\u4f5c\u4e3a\u66f4\u5408\u9002\u7684\u672a\u6765AI\u53d1\u5c55\u65b9\u5411\uff0c\u5f3a\u8c03\u4e13\u4e1a\u5316\u800c\u975e\u901a\u7528\u6027\uff0c\u5e76\u8ffd\u6c42\u8d85\u4eba\u7c7b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5bf9AGI\u7684\u5b9a\u4e49\u7f3a\u4e4f\u5171\u8bc6\uff0c\u4e14\u5176\u6838\u5fc3\u7406\u5ff5\u5b58\u5728\u95ee\u9898\uff1b\u4eba\u7c7b\u672c\u8eab\u5e76\u975e\u771f\u6b63\u7684\u901a\u7528\u667a\u80fd\uff0c\u800cAGI\u8fd9\u4e00\u6a21\u7cca\u6982\u5ff5\u5df2\u6df7\u6dc6\u4e86AI\u8ba8\u8bba\uff0c\u65e0\u6cd5\u6709\u6548\u6307\u5bfc\u672a\u6765\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u6279\u5224\u6027\u5206\u6790\u73b0\u6709AGI\u5b9a\u4e49\u7684\u5408\u7406\u6027\u3001\u5b9e\u7528\u6027\u548c\u771f\u6b63\u901a\u7528\u6027\uff0c\u6307\u51fa\u5176\u6982\u5ff5\u7f3a\u9677\uff1b\u8fdb\u800c\u63d0\u51faSAI\u6846\u67b6\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\uff0c\u5e76\u63a8\u6f14\u5176\u5e94\u7528\u4ef7\u503c\u3002", "result": "\u8bba\u8bc1\u4e86AGI\u5373\u4f7f\u5728\u6700\u8fde\u8d2f\u7684\u8868\u8ff0\u4e0b\u4ecd\u662f\u4e00\u4e2a\u63cf\u8ff0AI\u672a\u6765\u7684\u6709\u7f3a\u9677\u6982\u5ff5\uff1bSAI\u88ab\u5b9a\u4e49\u4e3a\u80fd\u591f\u5b66\u4e60\u5e76\u8d85\u8d8a\u4eba\u7c7b\u5b8c\u6210\u4efb\u4f55\u91cd\u8981\u4e8b\u52a1\uff0c\u5e76\u80fd\u586b\u8865\u4eba\u7c7b\u80fd\u529b\u7a7a\u767d\u7684\u667a\u80fd\u5f62\u5f0f\u3002", "conclusion": "AI\u53d1\u5c55\u5e94\u62e5\u62b1\u4e13\u4e1a\u5316\u800c\u975e\u8ffd\u6c42\u901a\u7528\u6027\uff0c\u5728\u4e13\u4e1a\u5316\u4e2d\u8ffd\u6c42\u8d85\u4eba\u7c7b\u6027\u80fd\uff1bSAI\u6846\u67b6\u6709\u52a9\u4e8e\u6f84\u6e05\u88abAGI\u8fc7\u8f7d\u5b9a\u4e49\u6a21\u7cca\u7684AI\u8ba8\u8bba\uff0c\u5e76\u4e3a\u672a\u6765AI\u53d1\u5c55\u63d0\u4f9b\u66f4\u6e05\u6670\u7684\u6307\u5bfc\u65b9\u5411\u3002"}}
{"id": "2602.23664", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23664", "abs": "https://arxiv.org/abs/2602.23664", "authors": ["Benjamin Rempfer", "Parker Kuklinski", "Justin Elenewski", "Kevin Obenland"], "title": "Harmonic sequence state-preparation", "comment": null, "summary": "We demonstrate an efficient circuit to prepare a quantum state with amplitudes proportional to a harmonic sequence. We do this by first preparing a large quantum state with linearly related amplitudes and then applying a quantum Fourier transform; this has a direct analogy to the fact that the Fourier coefficients of a sawtooth wave follow a harmonic sequence. We then consider an extension of this problem by block-encoding a matrix with a harmonic sequence along its diagonal. The cost of both circuits is dominated by the costs associated with the quantum Fourier transform.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u91cf\u5b50\u7535\u8def\uff0c\u901a\u8fc7\u5148\u5236\u5907\u7ebf\u6027\u632f\u5e45\u91cf\u5b50\u6001\u518d\u5e94\u7528\u91cf\u5b50\u5085\u91cc\u53f6\u53d8\u6362\uff08QFT\uff09\uff0c\u5b9e\u73b0\u632f\u5e45\u6b63\u6bd4\u4e8e\u8c03\u548c\u5e8f\u5217\u7684\u91cf\u5b50\u6001\u5236\u5907\uff0c\u5e76\u5c06\u5176\u6269\u5c55\u81f3\u5bf9\u89d2\u7ebf\u4e3a\u8c03\u548c\u5e8f\u5217\u7684\u77e9\u9635\u5757\u7f16\u7801\uff0c\u7535\u8def\u6210\u672c\u4e3b\u8981\u7531QFT\u51b3\u5b9a\u3002", "motivation": "\u8c03\u548c\u5e8f\u5217\u632f\u5e45\u91cf\u5b50\u6001\u5728\u91cf\u5b50\u7b97\u6cd5\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\uff0c\u4f8b\u5982\u4e0e\u7ecf\u5178\u4fe1\u53f7\u5904\u7406\u4e2d\u7684\u952f\u9f7f\u6ce2\u5085\u91cc\u53f6\u7cfb\u6570\u7c7b\u6bd4\uff0c\u9700\u9ad8\u6548\u5236\u5907\u65b9\u6cd5\u4ee5\u652f\u6301\u76f8\u5173\u91cf\u5b50\u8ba1\u7b97\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u6cd5\uff1a1\uff09\u5236\u5907\u5177\u6709\u7ebf\u6027\u76f8\u5173\u632f\u5e45\u7684\u521d\u59cb\u91cf\u5b50\u6001\uff1b2\uff09\u5e94\u7528\u91cf\u5b50\u5085\u91cc\u53f6\u53d8\u6362\u5c06\u7ebf\u6027\u632f\u5e45\u8f6c\u6362\u4e3a\u8c03\u548c\u5e8f\u5217\u632f\u5e45\u3002\u8fdb\u4e00\u6b65\u5c06\u5bf9\u89d2\u7ebf\u4e3a\u8c03\u548c\u5e8f\u5217\u7684\u77e9\u9635\u901a\u8fc7\u5757\u7f16\u7801\u6280\u672f\u5d4c\u5165\u91cf\u5b50\u7535\u8def\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u8c03\u548c\u5e8f\u5217\u632f\u5e45\u91cf\u5b50\u6001\u7684\u9ad8\u6548\u5236\u5907\u53ca\u77e9\u9635\u5757\u7f16\u7801\uff0c\u7535\u8def\u6574\u4f53\u8d44\u6e90\u6d88\u8017\u4e3b\u8981\u7531\u91cf\u5b50\u5085\u91cc\u53f6\u53d8\u6362\u7684\u590d\u6742\u5ea6\u4e3b\u5bfc\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8c03\u548c\u5e8f\u5217\u76f8\u5173\u91cf\u5b50\u6001\u5236\u5907\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u65b9\u6848\uff0c\u5176\u6548\u7387\u53d6\u51b3\u4e8eQFT\u5b9e\u73b0\uff0c\u4e3a\u91cf\u5b50\u7b97\u6cd5\u4e2d\u7279\u5b9a\u632f\u5e45\u7ed3\u6784\u7684\u6784\u5efa\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.24145", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.24145", "abs": "https://arxiv.org/abs/2602.24145", "authors": ["S. D. Semenov", "A. I. Lichtenstein", "A. N. Rubtsov"], "title": "A Unified Approach to Strong Local Correlations and Collective Fluctuations: Eliminating Divergence in the Spin Channel", "comment": null, "summary": "Dynamical mean-field theory (DMFT) provides an optimal local approximation for correlated lattice systems by mapping the lattice onto a self-consistent effective impurity model. To account for the missing long-range correlations, we propose a novel extended approach, which we term fluctuating dynamical mean-field theory (fDMFT). It incorporates collective fluctuations of auxiliary impurity models across different sites via functional integration. Technically, this method involves obtaining a family of DMFT solutions on a grid for a self-consistent auxiliary classical field applied to the lattice. While the result can, in principle, be improved diagrammatically, we find that the minimal version of the theory already yields accurate results, with lowest-order diagrammatic corrections offering only minor improvements. This consistent framework, based on our fluctuating local field concept, demonstrates superior performance for the nearly half-filled Hubbard model compared to other known diagrammatic extensions of DMFT.", "AI": {"tldr": "The paper proposes fDMFT, a novel extension of DMFT that incorporates long-range correlations via functional integration of collective fluctuations across sites, demonstrating superior performance for the nearly half-filled Hubbard model.", "motivation": "Standard DMFT provides an optimal local approximation for correlated lattice systems but fails to capture missing long-range correlations, necessitating an extended approach.", "method": "Proposes fluctuating DMFT (fDMFT) which incorporates collective fluctuations of auxiliary impurity models across different sites via functional integration, technically implemented by obtaining a family of DMFT solutions on a grid for a self-consistent auxiliary classical field.", "result": "The minimal version of fDMFT already yields accurate results with only minor improvements from lowest-order diagrammatic corrections, showing superior performance for the nearly half-filled Hubbard model.", "conclusion": "The fDMFT framework based on the fluctuating local field concept outperforms other known diagrammatic extensions of DMFT for the nearly half-filled Hubbard model."}}
{"id": "2602.23504", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.23504", "abs": "https://arxiv.org/abs/2602.23504", "authors": ["Anik Pramanik", "Murat Kantarcioglu", "Vincent Oria", "Shantanu Sharma"], "title": "FedDAG: Clustered Federated Learning via Global Data and Gradient Integration for Heterogeneous Environments", "comment": "This paper has been accepted in ICLR 2026", "summary": "Federated Learning (FL) enables a group of clients to collaboratively train a model without sharing individual data, but its performance drops when client data are heterogeneous. Clustered FL tackles this by grouping similar clients. However, existing clustered FL approaches rely solely on either data similarity or gradient similarity; however, this results in an incomplete assessment of client similarities. Prior clustered FL approaches also restrict knowledge and representation sharing to clients within the same cluster. This prevents cluster models from benefiting from the diverse client population across clusters. To address these limitations, FedDAG introduces a clustered FL framework, FedDAG, that employs a weighted, class-wise similarity metric that integrates both data and gradient information, providing a more holistic measure of similarity during clustering. In addition, FedDAG adopts a dual-encoder architecture for cluster models, comprising a primary encoder trained on its own clients' data and a secondary encoder refined using gradients from complementary clusters. This enables cross-cluster feature transfer while preserving cluster-specific specialization. Experiments on diverse benchmarks and data heterogeneity settings show that FedDAG consistently outperforms state-of-the-art clustered FL baselines in accuracy.", "AI": {"tldr": "FedDAG\u901a\u8fc7\u7ed3\u5408\u6570\u636e\u548c\u68af\u5ea6\u76f8\u4f3c\u6027\u5ea6\u91cf\u6765\u6539\u8fdb\u805a\u7c7b\u8054\u90a6\u5b66\u4e60\uff0c\u91c7\u7528\u53cc\u7f16\u7801\u5668\u67b6\u6784\u5b9e\u73b0\u8de8\u96c6\u7fa4\u77e5\u8bc6\u8fc1\u79fb\uff0c\u540c\u65f6\u5728\u4fdd\u6301\u96c6\u7fa4\u7279\u5b9a\u4e13\u4e1a\u5316\u7684\u540c\u65f6\u63d0\u5347\u6a21\u578b\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u5728\u6570\u636e\u5f02\u6784\u6027\u4e0b\u6027\u80fd\u4e0b\u964d\u3002\u73b0\u6709\u805a\u7c7b\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u6570\u636e\u6216\u68af\u5ea6\u76f8\u4f3c\u6027\uff0c\u5bfc\u81f4\u8bc4\u4f30\u4e0d\u5168\u9762\uff0c\u4e14\u9650\u5236\u77e5\u8bc6\u5171\u4eab\u4e8e\u96c6\u7fa4\u5185\u90e8\uff0c\u65e0\u6cd5\u5229\u7528\u8de8\u96c6\u7fa4\u7684\u591a\u6837\u6027\u4fe1\u606f\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51faFedDAG\u6846\u67b6\uff0c\u5f15\u5165\uff1a(1)\u52a0\u6743\u7c7b\u7ea7\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u6574\u5408\u6570\u636e\u548c\u68af\u5ea6\u4fe1\u606f\u4ee5\u8fdb\u884c\u66f4\u5168\u9762\u5ba2\u6237\u7aef\u805a\u7c7b\uff1b(2)\u53cc\u7f16\u7801\u5668\u67b6\u6784\uff0c\u4e3b\u7f16\u7801\u5668\u5728\u672c\u5730\u5ba2\u6237\u7aef\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u6b21\u7f16\u7801\u5668\u5229\u7528\u4e92\u8865\u96c6\u7fa4\u7684\u68af\u5ea6\u8fdb\u884c\u4f18\u5316\uff0c\u5b9e\u73b0\u8de8\u96c6\u7fa4\u7279\u5f81\u8fc1\u79fb\u3002", "result": "\u5728\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u548c\u6570\u636e\u5f02\u6784\u6027\u8bbe\u7f6e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFedDAG\u5728\u51c6\u786e\u6027\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u805a\u7c7b\u8054\u90a6\u5b66\u4e60\u57fa\u7ebf\u3002", "conclusion": "FedDAG\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u5ba2\u6237\u7aef\u805a\u7c7b\u65b9\u6cd5\uff0c\u5e76\u5b9e\u73b0\u4e86\u6709\u76ca\u7684\u8de8\u96c6\u7fa4\u77e5\u8bc6\u8fc1\u79fb\uff0c\u5728\u5f02\u6784\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.23687", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23687", "abs": "https://arxiv.org/abs/2602.23687", "authors": ["Daichi Kagamihara", "Shunji Tsuchiya"], "title": "Stabilizer R\u00e9nyi entropy of 3-uniform hypergraph states", "comment": "10 pages, 3 figures", "summary": "Nonstabilizerness, also known as magic, plays a central role in universal quantum computation. Hypergraph states are nonstabilizer generalizations of graph states and constitute a key class of quantum states in various areas of quantum physics, such as the demonstration of quantum advantage, measurement-based quantum computation, and the study of topological phases. In this work, we investigate nonstabilizerness of 3-uniform hypergraph states, which are solely generated by controlled-controlled-Z gates, in terms of the stabilizer R\u00e9nyi entropy (SRE). We find that the SRE of 3-uniform hypergraph states can be expressed using the matrix rank, which reduces computational cost from $\\mathcal{O}(2^{3N})$ to $\\mathcal{O}(N^3 2^{N})$ for $N$-qubit states. Based on this result, we exactly evaluate SREs of one-dimensional hypergraph states. We also present numerical results of SREs of several large-scale 3-uniform hypergraph states. Our results would contribute to an understanding of the role of nonstabilizerness in a wide range of physical settings where hypergraph states are employed.", "AI": {"tldr": "\u7814\u7a763-\u4e00\u81f4\u8d85\u56fe\u6001\u7684\u975e\u7a33\u5b9a\u5b50\u6027\uff08\u9b54\u6027\uff09\uff0c\u5229\u7528\u7a33\u5b9a\u5b50\u96f7\u5c3c\u71b5(SRE)\u5ea6\u91cf\uff0c\u53d1\u73b0\u5176\u53ef\u901a\u8fc7\u77e9\u9635\u79e9\u8868\u8fbe\uff0c\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5e76\u8ba1\u7b97\u4e86\u4e00\u7ef4\u548c\u5927\u89c4\u6a21\u8d85\u56fe\u6001\u7684SRE\u3002", "motivation": "\u975e\u7a33\u5b9a\u5b50\u6027\u662f\u5b9e\u73b0\u901a\u7528\u91cf\u5b50\u8ba1\u7b97\u7684\u6838\u5fc3\u8d44\u6e90\u3002\u8d85\u56fe\u6001\u662f\u56fe\u6001\u7684\u975e\u7a33\u5b9a\u5b50\u63a8\u5e7f\uff0c\u5728\u91cf\u5b50\u4f18\u52bf\u3001\u6d4b\u91cf\u578b\u91cf\u5b50\u8ba1\u7b97\u548c\u62d3\u6251\u76f8\u7814\u7a76\u4e2d\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5982\u4f55\u9ad8\u6548\u91cf\u5316\u8d85\u56fe\u6001\u7684\u975e\u7a33\u5b9a\u5b50\u6027\u4ecd\u662f\u5f00\u653e\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7a33\u5b9a\u5b50\u96f7\u5c3c\u71b5(SRE)\u4f5c\u4e3a\u5ea6\u91cf\u5de5\u5177\uff0c\u9488\u5bf9\u4ec5\u7531CCZ\u95e8\u751f\u6210\u76843-\u4e00\u81f4\u8d85\u56fe\u6001\uff0c\u5efa\u7acbSRE\u4e0e\u77e9\u9635\u79e9\u7684\u89e3\u6790\u5173\u7cfb\u3002", "result": "\u53d1\u73b03-\u4e00\u81f4\u8d85\u56fe\u6001\u7684SRE\u53ef\u7528\u77e9\u9635\u79e9\u7cbe\u786e\u8868\u8fbe\uff0c\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u4eceO(2^(3N))\u964d\u81f3O(N^3 * 2^N)\uff1b\u89e3\u6790\u8ba1\u7b97\u4e86\u4e00\u7ef4\u8d85\u56fe\u6001\u7684SRE\uff1b\u7ed9\u51fa\u4e86\u591a\u79cd\u5927\u89c4\u6a213-\u4e00\u81f4\u8d85\u56fe\u6001\u7684\u6570\u503c\u7ed3\u679c\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u7406\u89e3\u8d85\u56fe\u6001\u975e\u7a33\u5b9a\u5b50\u6027\u63d0\u4f9b\u4e86\u9ad8\u6548\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5c06\u4fc3\u8fdb\u5176\u5728\u91cf\u5b50\u8ba1\u7b97\u3001\u91cf\u5b50\u4f18\u52bf\u548c\u62d3\u6251\u7269\u6001\u7b49\u9886\u57df\u7684\u5e94\u7528\u7814\u7a76\u3002"}}
{"id": "2602.24203", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.24203", "abs": "https://arxiv.org/abs/2602.24203", "authors": ["Md Zahid Ansari", "Souvik Kundu", "Kedar Damle"], "title": "Vacancy-induced local moments in quantum paramagnetic phases: An SU($N$) designer Hamiltonian study", "comment": "11 pages, 9 figures", "summary": "We explore the effects of non-magnetic impurities (vacancy disorder) on the quantum paramagnetic phases stabilized by SU($N$) designer Hamiltonians on bipartite lattices. Using the results of our quantum Monte Carlo simulations, we demonstrate that isolated vacancies seed emergent spin $S=1/2$ moments in their vicinity when the low-temperature state has valence bond solid order. Indeed, our quantum Monte Carlo results for the low-temperature susceptibility in such regimes shows clear evidence of the vacancy-induced Curie tails associated with these emergent moments, and our zero-temperature projector Monte Carlo results on the ground-state wavefunction in the valence bond basis provide additional evidence in support of this picture. Further, for such designer Hamiltonians on the Lieb lattice with two additional sites on each bond of a square lattice, we identify a low-temperature spin liquid-like regime with no sign of spin or valence bond order. This liquid-like regime serves as a test bed for validating a recently-developed argument concerning the effects of vacancy disorder in such low temperature regimes. Consistent with this argument, we find that isolated vacancies do not seed emergent local moments in such spin liquids. Instead, in the presence of vacancy disorder, emergent local moments are associated with the presence of monomers in maximum-density dimer packings of the corresponding diluted lattice.", "AI": {"tldr": "Using quantum Monte Carlo simulations, we find that non-magnetic vacancies induce emergent spin-1/2 moments in valence bond solid phases but not in spin liquid phases; instead, moments in spin liquids arise from monomers in maximum-density dimer packings of the diluted lattice.", "motivation": "To understand the effects of non-magnetic impurity (vacancy) disorder on quantum paramagnetic phases stabilized by SU(N) designer Hamiltonians on bipartite lattices, and to test a recent argument about vacancy disorder effects in low-temperature spin liquids.", "method": "Quantum Monte Carlo simulations (finite-temperature and zero-temperature projector Monte Carlo) on SU(N) designer Hamiltonians on bipartite and Lieb lattices, analyzing low-temperature susceptibility and ground-state wavefunction in valence bond basis.", "result": "In valence bond solid ordered phases, isolated vacancies seed emergent S=1/2 moments, evidenced by Curie tails in susceptibility and ground-state wavefunction analysis. In the spin liquid-like regime on the Lieb lattice, vacancies do not seed local moments; emergent moments are instead associated with monomers in maximum-density dimer packings of the diluted lattice.", "conclusion": "The impact of vacancy disorder depends on the nature of the low-temperature phase: in ordered phases (VBS) vacancies create local moments, while in spin liquids they do not, supporting the recent argument and highlighting the role of lattice geometry and dimer coverings."}}
{"id": "2602.23507", "categories": ["cs.LG", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.23507", "abs": "https://arxiv.org/abs/2602.23507", "authors": ["Diana Shamsutdinova", "Felix Zimmer", "Oyebayo Ridwan Olaniran", "Sarah Markham", "Daniel Stahl", "Gordon Forbes", "Ewan Carr"], "title": "Sample Size Calculations for Developing Clinical Prediction Models: Overview and pmsims R package", "comment": "26 pages, 4 figures, 1 table, preprint", "summary": "Background: Clinical prediction models are increasingly used to inform healthcare decisions, but determining the minimum sample size for their development remains a critical and unresolved challenge. Inadequate sample sizes can lead to overfitting, poor generalisability, and biased predictions. Existing approaches, such as heuristic rules, closed-form formulas, and simulation-based methods, vary in flexibility and accuracy, particularly for complex data structures and machine learning models. Methods: We review current methodologies for sample size estimation in prediction modelling and introduce a conceptual framework that distinguishes between mean-based and assurance-based criteria. Building on this, we propose a novel simulation-based approach that integrates learning curves, Gaussian Process optimisation, and assurance principles to identify sample sizes that achieve target performance with high probability. This approach is implemented in pmsims, an open-source, model-agnostic R package. Results: Through case studies, we demonstrate that sample size estimates vary substantially across methods, performance metrics, and modelling strategies. Compared to existing tools, pmsims provides flexible, efficient, and interpretable solutions that accommodate diverse models and user-defined metrics while explicitly accounting for variability in model performance. Conclusions: Our framework and software advance sample size methodology for clinical prediction modelling by combining flexibility with computational efficiency. Future work should extend these methods to hierarchical and multimodal data, incorporate fairness and stability metrics, and address challenges such as missing data and complex dependency structures.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u4e34\u5e8a\u9884\u6d4b\u6a21\u578b\u7684\u6700\u5c0f\u6837\u672c\u91cf\u786e\u5b9a\u96be\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5b66\u4e60\u66f2\u7ebf\u3001\u9ad8\u65af\u8fc7\u7a0b\u4f18\u5316\u548c\u4fdd\u8bc1\u539f\u5219\u7684\u6a21\u62df\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86\u5f00\u6e90R\u5305pmsims\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4e34\u5e8a\u9884\u6d4b\u6a21\u578b\u7684\u6837\u672c\u91cf\u4e0d\u8db3\u4f1a\u5bfc\u81f4\u8fc7\u62df\u5408\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u9884\u6d4b\u504f\u5dee\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u542f\u53d1\u5f0f\u89c4\u5219\u3001\u95ed\u5f0f\u516c\u5f0f\u3001\u6a21\u62df\u65b9\u6cd5\uff09\u5728\u590d\u6742\u6570\u636e\u7ed3\u6784\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u65b9\u9762\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u7efc\u8ff0\u73b0\u6709\u65b9\u6cd5\u5e76\u5efa\u7acb\u5747\u503c\u578b\u4e0e\u4fdd\u8bc1\u578b\u51c6\u5219\u7684\u6982\u5ff5\u6846\u67b6\u3002\u63d0\u51fa\u65b0\u9896\u7684\u6a21\u62df\u65b9\u6cd5\uff0c\u6574\u5408\u5b66\u4e60\u66f2\u7ebf\u3001\u9ad8\u65af\u8fc7\u7a0b\u4f18\u5316\u548c\u4fdd\u8bc1\u539f\u5219\uff0c\u4ee5\u786e\u5b9a\u80fd\u4ee5\u9ad8\u6982\u7387\u8fbe\u5230\u76ee\u6807\u6027\u80fd\u7684\u6837\u672c\u91cf\u3002\u8be5\u65b9\u6cd5\u5728\u5f00\u6e90\u3001\u6a21\u578b\u65e0\u5173\u7684R\u5305pmsims\u4e2d\u5b9e\u73b0\u3002", "result": "\u6837\u672c\u91cf\u4f30\u8ba1\u5728\u4e0d\u540c\u65b9\u6cd5\u3001\u6027\u80fd\u6307\u6807\u548c\u5efa\u6a21\u7b56\u7565\u95f4\u5dee\u5f02\u663e\u8457\u3002pmsims\u63d0\u4f9b\u7075\u6d3b\u3001\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u9002\u5e94\u4e0d\u540c\u6a21\u578b\u548c\u7528\u6237\u5b9a\u4e49\u6307\u6807\uff0c\u5e76\u660e\u786e\u8003\u8651\u6a21\u578b\u6027\u80fd\u7684\u53d8\u5f02\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u548c\u8f6f\u4ef6\u901a\u8fc7\u7ed3\u5408\u7075\u6d3b\u6027\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u63a8\u8fdb\u4e86\u4e34\u5e8a\u9884\u6d4b\u6a21\u578b\u7684\u6837\u672c\u91cf\u65b9\u6cd5\u5b66\u3002\u672a\u6765\u9700\u6269\u5c55\u5230\u5c42\u6b21\u5316\u548c\u591a\u6a21\u6001\u6570\u636e\uff0c\u7eb3\u5165\u516c\u5e73\u6027\u548c\u7a33\u5b9a\u6027\u6307\u6807\uff0c\u5e76\u89e3\u51b3\u7f3a\u5931\u6570\u636e\u548c\u590d\u6742\u4f9d\u8d56\u7ed3\u6784\u7b49\u6311\u6218\u3002"}}
{"id": "2602.23681", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23681", "abs": "https://arxiv.org/abs/2602.23681", "authors": ["Siyuan Ma", "Bo Gao", "Xiaojun Jia", "Simeng Qin", "Tianlin Li", "Ke Ma", "Xiaoshuang Jia", "Wenqi Ren", "Yang Liu"], "title": "ODAR: Principled Adaptive Routing for LLM Reasoning via Active Inference", "comment": null, "summary": "The paradigm of large language model (LLM) reasoning is shifting from parameter scaling to test-time compute scaling, yet many existing approaches still rely on uniform brute-force sampling (for example, fixed best-of-N or self-consistency) that is costly, hard to attribute, and can trigger overthinking with diminishing returns. We propose ODAR-Expert, an adaptive routing framework that optimizes the accuracy-efficiency trade-off via principled resource allocation. ODAR uses a difficulty estimator grounded in amortized active inference to dynamically route queries between a heuristic Fast Agent and a deliberative Slow Agent. We further introduce a free-energy-principled, risk-sensitive fusion mechanism that selects answers by minimizing a variational free energy objective, balancing log-likelihood with epistemic uncertainty (varentropy) as a principled alternative to ad hoc voting over heterogeneous candidates. Extensive evaluation across 23 benchmarks shows strong and consistent gains, including 98.2% accuracy on MATH and 54.8% on Humanity's Last Exam (HLE), while improving the compute-accuracy frontier under compute-matched settings. We also validate reproducibility on a fully open-source stack (Llama 4 + DeepSeek), where ODAR surpasses homogeneous sampling strategies while reducing computational costs by 82%. Overall, our results suggest that thinking-optimal scaling requires adaptive resource allocation with free-energy-based decision-making rather than simply increasing test-time compute.", "AI": {"tldr": "\u63d0\u51faODAR-Expert\u81ea\u9002\u5e94\u8def\u7531\u6846\u67b6\uff0c\u901a\u8fc7\u96be\u5ea6\u4f30\u8ba1\u548c\u57fa\u4e8e\u81ea\u7531\u80fd\u7684\u878d\u5408\u673a\u5236\uff0c\u5728\u5feb\u6162\u667a\u80fd\u4f53\u95f4\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u964d\u4f4e82%\u7684\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5f53\u524dLLM\u63a8\u7406\u65b9\u6cd5\u4f9d\u8d56\u5747\u5300\u66b4\u529b\u91c7\u6837\uff08\u5982best-of-N\u3001\u81ea\u6d3d\u6027\uff09\uff0c\u5b58\u5728\u6210\u672c\u9ad8\u3001\u96be\u4ee5\u5f52\u56e0\u3001\u968f\u7740\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6269\u5c55\u4f1a\u51fa\u73b0\u6536\u76ca\u9012\u51cf\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\u3002", "method": "ODAR-Expert\u91c7\u7528\u57fa\u4e8e\u644a\u9500\u4e3b\u52a8\u63a8\u7406\u7684\u96be\u5ea6\u4f30\u8ba1\u5668\uff0c\u52a8\u6001\u8def\u7531\u67e5\u8be2\u81f3\u542f\u53d1\u5f0f\u5feb\u667a\u80fd\u4f53\u548c\u6df1\u601d\u719f\u8651\u6162\u667a\u80fd\u4f53\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u81ea\u7531\u80fd\u7684\u878d\u5408\u673a\u5236\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u53d8\u5206\u81ea\u7531\u80fd\u76ee\u6807\u6765\u5e73\u8861\u5bf9\u6570\u4f3c\u7136\u4e0e\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u572823\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cMATH\u8fbe\u523098.2%\u51c6\u786e\u7387\uff0cHLE\u8fbe\u523054.8%\uff0c\u5728\u5f00\u6e90\u6280\u672f\u6808\uff08Llama 4 + DeepSeek\uff09\u4e0a\u8d85\u8d8a\u5747\u5300\u91c7\u6837\u7b56\u7565\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e82%\u3002", "conclusion": "\u6700\u4f18\u63a8\u7406\u6269\u5c55\u9700\u8981\u57fa\u4e8e\u81ea\u7531\u80fd\u7684\u51b3\u7b56\u81ea\u9002\u5e94\u8d44\u6e90\u5206\u914d\uff0c\u800c\u975e\u7b80\u5355\u5730\u589e\u52a0\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u91cf\u3002"}}
{"id": "2602.23528", "categories": ["cs.LG", "cs.CE", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.23528", "abs": "https://arxiv.org/abs/2602.23528", "authors": ["Yicen Li", "Jose Antonio Lara Benitez", "Ruiyang Hong", "Anastasis Kratsios", "Paul David McNicholas", "Maarten Valentijn de Hoop"], "title": "Neural Operators Can Discover Functional Clusters", "comment": null, "summary": "Operator learning is reshaping scientific computing by amortizing inference across infinite families of problems. While neural operators (NOs) are increasingly well understood for regression, far less is known for classification and its unsupervised analogue: clustering. We prove that sample-based neural operators can learn any finite collection of classes in an infinite-dimensional reproducing kernel Hilbert space, even when the classes are neither convex nor connected, under mild kernel sampling assumptions. Our universal clustering theorem shows that any $K$ closed classes can be approximated to arbitrary precision by NO-parameterized classes in the upper Kuratowski topology on closed sets, a notion that can be interpreted as disallowing false-positive misclassifications.\n  Building on this, we develop an NO-powered clustering pipeline for functional data and apply it to unlabeled families of ordinary differential equation (ODE) trajectories. Discretized trajectories are lifted by a fixed pre-trained encoder into a continuous feature map and mapped to soft assignments by a lightweight trainable head. Experiments on diverse synthetic ODE benchmarks show that the resulting practical SNO recovers latent dynamical structure in regimes where classical methods fail, providing evidence consistent with our universal clustering theory.", "AI": {"tldr": "This paper proves neural operators can universally learn classification and clustering of infinite-dimensional functional data, even for non-convex and disconnected classes, and demonstrates this with an ODE trajectory clustering pipeline that outperforms classical methods.", "motivation": "While neural operators excel at regression tasks in scientific computing, their theoretical foundations and practical applications for classification and unsupervised clustering of functional data remain underexplored, particularly for complex dynamical systems like ODE trajectories where traditional methods fail.", "method": "The authors establish universal approximation theorems in reproducing kernel Hilbert spaces, then build a pipeline that lifts discretized ODE trajectories via a pre-trained encoder into continuous feature maps, which are clustered by a lightweight trainable head parameterized as a neural operator.", "result": "(1) Universal classification theorem: sample-based neural operators can learn any finite class collection in infinite-dimensional RKHS; (2) Universal clustering theorem: any K closed classes are approximable in Kuratowski topology, disallowing false positives; (3) Empirical validation shows their approach recovers latent dynamical structure where classical methods fail on synthetic ODE benchmarks.", "conclusion": "Neural operators provide a theoretically-grounded, powerful framework for functional data clustering that handles pathological class geometries and reveals hidden structures in scientific computing applications."}}
{"id": "2602.23701", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.23701", "abs": "https://arxiv.org/abs/2602.23701", "authors": ["Yawen Wang", "Wenjie Wu", "Junjie Wang", "Qing Wang"], "title": "From Flat Logs to Causal Graphs: Hierarchical Failure Attribution for LLM-based Multi-Agent Systems", "comment": null, "summary": "LLM-powered Multi-Agent Systems (MAS) have demonstrated remarkable capabilities in complex domains but suffer from inherent fragility and opaque failure mechanisms. Existing failure attribution methods, whether relying on direct prompting, costly replays, or supervised fine-tuning, typically treat execution logs as flat sequences. This linear perspective fails to disentangle the intricate causal links inherent to MAS, leading to weak observability and ambiguous responsibility boundaries. To address these challenges, we propose CHIEF, a novel framework that transforms chaotic trajectories into a structured hierarchical causal graph. It then employs hierarchical oracle-guided backtracking to efficiently prune the search space via sybthesized virtual oracles. Finally, it implements counterfactual attribution via a progressive causal screening strategy to rigorously distinguish true root causes from propagated symptoms. Experiments on Who&When benchmark show that CHIEF outperforms eight strong and state-of-the-art baselines on both agent- and step-level accuracy. Ablation studies further confirm the critical role of each proposed module.", "AI": {"tldr": "\u63d0\u51faCHIEF\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u5206\u5c42\u56e0\u679c\u56fe\u3001\u5c42\u7ea7\u5316\u5f15\u5bfc\u56de\u6eaf\u548c\u53cd\u4e8b\u5b9e\u5f52\u56e0\uff0c\u89e3\u51b3LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6545\u969c\u6839\u56e0\u5b9a\u4f4d\u95ee\u9898\uff0c\u5728Who&When\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u4f18\u4e8e8\u79cd\u5148\u8fdb\u65b9\u6cd5", "motivation": "LLM\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf(MAS)\u867d\u80fd\u529b\u5f3a\u4f46\u5b58\u5728\u8106\u5f31\u6027\u548c\u4e0d\u900f\u660e\u6545\u969c\u673a\u5236\u3002\u73b0\u6709\u5f52\u56e0\u65b9\u6cd5\u5c06\u6267\u884c\u65e5\u5fd7\u89c6\u4e3a\u7ebf\u6027\u5e8f\u5217\uff0c\u65e0\u6cd5\u63ed\u793aMAS\u5185\u5728\u7684\u590d\u6742\u56e0\u679c\u8054\u7cfb\uff0c\u5bfc\u81f4\u53ef\u89c2\u6d4b\u6027\u5f31\u3001\u8d23\u4efb\u8fb9\u754c\u6a21\u7cca\uff0c\u96be\u4ee5\u5b9a\u4f4d\u771f\u5b9e\u6839\u56e0", "method": "CHIEF\u6846\u67b6\u91c7\u7528\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a(1)\u5c06\u6df7\u6c8c\u8f68\u8ff9\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u5206\u5c42\u56e0\u679c\u56fe\uff1b(2)\u901a\u8fc7\u5c42\u7ea7\u5316oracle\u5f15\u5bfc\u56de\u6eaf\uff0c\u5229\u7528\u5408\u6210\u865a\u62dforacle\u9ad8\u6548\u526a\u679d\u641c\u7d22\u7a7a\u95f4\uff1b(3)\u5b9e\u65bd\u57fa\u4e8e\u6e10\u8fdb\u56e0\u679c\u7b5b\u9009\u7684\u53cd\u4e8b\u5b9e\u5f52\u56e0\uff0c\u4e25\u683c\u533a\u5206\u6839\u56e0\u4e0e\u4f20\u64ad\u75c7\u72b6", "result": "\u5728Who&When\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cCHIEF\u5728\u667a\u80fd\u4f53\u7ea7\u548c\u6b65\u9aa4\u7ea7\u51c6\u786e\u7387\u5747\u8d85\u8d8a8\u79cd\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff1b\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u6bcf\u4e2a\u6a21\u5757\u7684\u5173\u952e\u4f5c\u7528", "conclusion": "CHIEF\u901a\u8fc7\u7ed3\u6784\u5316\u56e0\u679c\u5efa\u6a21\u548c\u5206\u5c42\u63a8\u7406\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86MAS\u6545\u969c\u5f52\u56e0\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u6784\u5efa\u66f4\u5065\u58ee\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848"}}
{"id": "2602.23529", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23529", "abs": "https://arxiv.org/abs/2602.23529", "authors": ["Martin \u010cern\u00fd", "David Sychrovsk\u00fd", "Filip \u00daradn\u00edk", "Jakub \u010cern\u00fd"], "title": "Active Value Querying to Minimize Additive Error in Subadditive Set Function Learning", "comment": null, "summary": "Subadditive set functions play a pivotal role in computational economics (especially in combinatorial auctions), combinatorial optimization or artificial intelligence applications such as interpretable machine learning. However, specifying a set function requires assigning values to an exponentially large number of subsets in general, a task that is often resource-intensive in practice, particularly when the values derive from external sources such as retraining of machine learning models. A~simple omission of certain values introduces ambiguity that becomes even more significant when the incomplete set function has to be further optimized over. Motivated by the well-known result about inapproximability of subadditive functions using deterministic value queries with respect to a multiplicative error, we study a problem of approximating an unknown subadditive (or a subclass of thereof) set function with respect to an additive error -- i. e., we aim to efficiently close the distance between minimal and maximal completions. Our contributions are threefold: (i) a thorough exploration of minimal and maximal completions of different classes of set functions with missing values and an analysis of their resulting distance; (ii) the development of methods to minimize this distance over classes of set functions with a known prior, achieved by disclosing values of additional subsets in both offline and online manner; and (iii) empirical demonstrations of the algorithms' performance in practical scenarios.", "AI": {"tldr": "\u9488\u5bf9\u542b\u7f3a\u5931\u503c\u7684\u5b50\u6a21\u96c6\u51fd\u6570\uff0c\u63d0\u51fa\u57fa\u4e8e\u52a0\u6027\u8bef\u5dee\u7684\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u901a\u8fc7\u79bb\u7ebf\u548c\u5728\u7ebf\u67e5\u8be2\u7b56\u7565\u6700\u5c0f\u5316\u6700\u5c0f/\u6700\u5927\u8865\u5168\u95f4\u7684\u8ddd\u79bb\uff0c\u5e76\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027", "motivation": "\u5b50\u6a21\u96c6\u51fd\u6570\u5728\u7ec4\u5408\u4f18\u5316\u548c\u673a\u5668\u5b66\u4e60\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5b8c\u6574\u5b9a\u4e49\u9700\u6307\u6570\u7ea7\u5b50\u96c6\u4f30\u503c\uff0c\u5b9e\u9645\u4e2d\u5e38\u56e0\u5916\u90e8\u6570\u636e\uff08\u5982\u6a21\u578b\u91cd\u8bad\u7ec3\uff09\u5bfc\u81f4\u4f30\u503c\u7f3a\u5931\uff0c\u5f15\u53d1\u4f18\u5316\u6b67\u4e49\uff1b\u5df2\u6709\u7814\u7a76\u8bc1\u660e\u57fa\u4e8e\u786e\u5b9a\u6027\u67e5\u8be2\u7684\u4e58\u6cd5\u8bef\u5dee\u8fd1\u4f3c\u5b58\u5728\u4e0d\u53ef\u8fd1\u4f3c\u6027\uff0c\u6545\u8f6c\u5411\u52a0\u6027\u8bef\u5dee\u6846\u67b6", "method": "1) \u7cfb\u7edf\u5206\u6790\u4e0d\u540c\u7c7b\u522b\u96c6\u51fd\u6570\u7f3a\u5931\u503c\u7684\u6700\u5c0f/\u6700\u5927\u8865\u5168\u53ca\u5176\u8ddd\u79bb\uff1b2) \u7ed3\u5408\u5148\u9a8c\u77e5\u8bc6\u8bbe\u8ba1\u79bb\u7ebf\u548c\u5728\u7ebf\u5b50\u96c6\u67e5\u8be2\u7b56\u7565\u4ee5\u538b\u7f29\u8865\u5168\u8ddd\u79bb\uff1b3) \u901a\u8fc7\u5b9e\u9645\u573a\u666f\u5b9e\u9a8c\u9a8c\u8bc1\u7b97\u6cd5\u6027\u80fd", "result": "\u7406\u8bba\u5c42\u9762\u63ed\u793a\u5404\u7c7b\u96c6\u51fd\u6570\u8865\u5168\u8ddd\u79bb\u7684\u7279\u6027\uff1b\u5b9e\u8df5\u5c42\u9762\u5f00\u53d1\u51fa\u80fd\u6709\u6548\u964d\u4f4e\u6b67\u4e49\u7684\u67e5\u8be2\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u663e\u793a\u5728\u7ec4\u5408\u62cd\u5356\u548c\u673a\u5668\u5b66\u4e60\u7b49\u573a\u666f\u4e2d\u6027\u80fd\u826f\u597d", "conclusion": "\u901a\u8fc7\u52a0\u6027\u8bef\u5dee\u6846\u67b6\u548c\u4e3b\u52a8\u67e5\u8be2\u673a\u5236\uff0c\u663e\u8457\u7f13\u89e3\u4e86\u7f3a\u5931\u503c\u5bfc\u81f4\u7684\u4f18\u5316\u6b67\u4e49\u95ee\u9898\uff0c\u4e3a\u5b50\u6a21\u51fd\u6570\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u5e94\u7528\u63d0\u4f9b\u65b0\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.23716", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23716", "abs": "https://arxiv.org/abs/2602.23716", "authors": ["Jiangyuan Wang", "Kejun Xiao", "Huaipeng Zhao", "Tao Luo", "Xiaoyi Zeng"], "title": "ProductResearch: Training E-Commerce Deep Research Agents via Multi-Agent Synthetic Trajectory Distillation", "comment": null, "summary": "Large Language Model (LLM)-based agents show promise for e-commerce conversational shopping, yet existing implementations lack the interaction depth and contextual breadth required for complex product research. Meanwhile, the Deep Research paradigm, despite advancing information synthesis in web search, suffers from domain gaps when transferred to e-commerce. We propose ProductResearch, a multi-agent framework that synthesizes high-fidelity, long-horizon tool-use trajectories for training robust e-commerce shopping agents. The framework employs a User Agent to infer nuanced shopping intents from behavioral histories, and a Supervisor Agent that orchestrates iterative collaboration with a Research Agent to generate synthetic trajectories culminating in comprehensive, insightful product research reports. These trajectories are rigorously filtered and distilled through a reflective internalization process that consolidates multi-agent supervisory interactions into coherent single-role training examples, enabling effective fine-tuning of LLM agents for complex shopping inquiries. Extensive experiments show that a compact MoE model fine-tuned on our synthetic data achieves substantial improvements over its base model in response comprehensiveness, research depth, and user-perceived utility, approaching the performance of frontier proprietary deep research systems and establishing multi-agent synthetic trajectory training as an effective and scalable paradigm for enhancing LLM-based shopping assistance.", "AI": {"tldr": "\u63d0\u51faProductResearch\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u9ad8\u8d28\u91cf\u5de5\u5177\u4f7f\u7528\u8f68\u8ff9\u8bad\u7ec3\u7535\u5546\u8d2d\u7269\u667a\u80fd\u4f53\uff0c\u4f7f\u5fae\u8c03\u540e\u7684\u7d27\u51d1\u6a21\u578b\u5728\u54cd\u5e94\u5168\u9762\u6027\u3001\u7814\u7a76\u6df1\u5ea6\u548c\u7528\u6237\u6548\u7528\u4e0a\u63a5\u8fd1\u524d\u6cbf\u4e13\u6709\u7cfb\u7edf", "motivation": "\u73b0\u6709LLM\u7535\u5546\u8d2d\u7269\u667a\u80fd\u4f53\u7f3a\u4e4f\u590d\u6742\u4ea7\u54c1\u7814\u7a76\u6240\u9700\u7684\u4ea4\u4e92\u6df1\u5ea6\u548c\u4e0a\u4e0b\u6587\u5e7f\u5ea6\uff0c\u800cDeep Research\u8303\u5f0f\u5728\u8fc1\u79fb\u5230\u7535\u5546\u9886\u57df\u65f6\u5b58\u5728\u9886\u57df\u9e3f\u6c9f", "method": "\u6784\u5efa\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff1aUser Agent\u4ece\u884c\u4e3a\u5386\u53f2\u63a8\u65ad\u8d2d\u7269\u610f\u56fe\uff0cSupervisor Agent\u534f\u8c03Research Agent\u8fdb\u884c\u8fed\u4ee3\u534f\u4f5c\u751f\u6210\u5408\u6210\u8f68\u8ff9\uff0c\u518d\u901a\u8fc7\u53cd\u601d\u5185\u5316\u8fc7\u7a0b\u5c06\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u8f6c\u5316\u4e3a\u5355\u89d2\u8272\u8bad\u7ec3\u793a\u4f8b", "result": "\u5728\u5408\u6210\u6570\u636e\u4e0a\u5fae\u8c03\u7684\u7d27\u51d1MoE\u6a21\u578b\u5728\u54cd\u5e94\u5168\u9762\u6027\u3001\u7814\u7a76\u6df1\u5ea6\u548c\u7528\u6237\u611f\u77e5\u6548\u7528\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u6027\u80fd\u63a5\u8fd1\u524d\u6cbf\u4e13\u6709\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf", "conclusion": "\u591a\u667a\u80fd\u4f53\u5408\u6210\u8f68\u8ff9\u8bad\u7ec3\u662f\u63d0\u5347LLM\u8d2d\u7269\u52a9\u624b\u80fd\u529b\u7684\u6709\u6548\u4e14\u53ef\u6269\u5c55\u8303\u5f0f"}}
{"id": "2602.23764", "categories": ["quant-ph", "math-ph", "math.CV"], "pdf": "https://arxiv.org/pdf/2602.23764", "abs": "https://arxiv.org/abs/2602.23764", "authors": ["Snehasis Bera", "Sourav Das", "Abhijit Banerjee"], "title": "A new class of coherent states involving Fox-Wright functions and their generalization in the bicomplex framework", "comment": null, "summary": "In this work, an extensive class of coherent states is introduced by taking the Fox Wright function as the normalization function. It is demonstrated that these states satisfy the key requirements of continuity, normalizability and resolution of unity. Furthermore, coherent states associated with the continuous spectrum are obtained through a discrete to continuous limiting procedure. Moreover, FW generalized multi parameter nu function is introduced and shown to act as the normalization function for the Fox Wright coherent states in the continuous spectrum. Later the Fox Wright function with bicomplex arguments has been introduced and its existence has been investigated. Bicomplex Fox Wright coherent states are also developed for the discrete spectrum based on this new function and their properties are analyzed. Subsequently, the results regarding Fox Wright coherent states are generalized to the bicomplex setting. In addition, a bicomplex FW generalized multi-parameter nu function is defined to demonstrate that it provides the normalization for these states in the continuous spectrum.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8eFox Wright\u51fd\u6570\u7684\u76f8\u5e72\u6001\u5b8c\u6574\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8be5\u51fd\u6570\u4f5c\u4e3a\u5f52\u4e00\u5316\u51fd\u6570\uff0c\u7cfb\u7edf\u6027\u5730\u53d1\u5c55\u4e86\u79bb\u6563\u8c31\u548c\u8fde\u7eed\u8c31\u76f8\u5e72\u6001\uff0c\u5e76\u6210\u529f\u5c06\u5176\u63a8\u5e7f\u5230\u53cc\u590d\u6570\u9886\u57df\uff0c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u6001\u6ee1\u8db3\u8fde\u7eed\u6027\u3001\u53ef\u5f52\u4e00\u5316\u548c\u5355\u4f4d\u5206\u89e3\u6027\u7b49\u6838\u5fc3\u6570\u5b66\u8981\u6c42\u3002", "motivation": "\u63a8\u5e7f\u76f8\u5e72\u6001\u7406\u8bba\uff0c\u63a2\u7d22Fox Wright\u51fd\u6570\u4f5c\u4e3a\u5f52\u4e00\u5316\u51fd\u6570\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5c06\u7406\u8bba\u6846\u67b6\u6269\u5c55\u5230\u53cc\u590d\u6570\u57df\u4ee5\u62d3\u5c55\u5176\u6570\u5b66\u5e94\u7528\u8303\u56f4\u3002", "method": "\u91c7\u7528Fox Wright\u51fd\u6570\u4f5c\u4e3a\u6838\u5fc3\u6570\u5b66\u5de5\u5177\uff0c\u901a\u8fc7\u79bb\u6563\u8c31\u5230\u8fde\u7eed\u8c31\u7684\u6781\u9650\u8fc7\u7a0b\u6784\u5efa\u8fde\u7eed\u8c31\u76f8\u5e72\u6001\uff0c\u5e76\u5f15\u5165\u53cc\u590d\u6570\u53c2\u6570\u5316\u65b9\u6cd5\u5b9e\u73b0\u7406\u8bba\u7684\u5168\u57df\u63a8\u5e7f\u3002", "result": "\u6210\u529f\u6784\u9020\u51fa\u6ee1\u8db3\u57fa\u672c\u7269\u7406\u8981\u6c42\u7684Fox Wright\u76f8\u5e72\u6001\uff1b\u63a8\u5bfc\u51fa\u9002\u7528\u4e8e\u8fde\u7eed\u8c31\u7684FW\u5e7f\u4e49\u591a\u53c2\u6570\u03bd\u51fd\u6570\uff1b\u5efa\u7acb\u4e86\u53cc\u590d\u6570Fox Wright\u51fd\u6570\u7406\u8bba\u5e76\u9a8c\u8bc1\u5176\u5b58\u5728\u6027\uff1b\u5b9e\u73b0\u4e86\u76f8\u5e72\u6001\u7406\u8bba\u4ece\u5b9e\u6570\u57df\u5230\u53cc\u590d\u6570\u57df\u7684\u5b8c\u6574\u63a8\u5e7f\u3002", "conclusion": "Fox Wright\u51fd\u6570\u4e3a\u76f8\u5e72\u6001\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u6570\u5b66\u57fa\u7840\uff0c\u53cc\u590d\u6570\u6269\u5c55\u4e0d\u4ec5\u4fdd\u6301\u4e86\u539f\u6709\u7406\u8bba\u7684\u5168\u90e8\u6838\u5fc3\u6027\u8d28\uff0c\u8fd8\u4e3a\u91cf\u5b50\u529b\u5b66\u548c\u6570\u5b66\u7269\u7406\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2602.23720", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23720", "abs": "https://arxiv.org/abs/2602.23720", "authors": ["Sheng Cao", "Zhao Chang", "Chang Li", "Hannan Li", "Liyao Fu", "Ji Tang"], "title": "The Auton Agentic AI Framework", "comment": null, "summary": "The field of Artificial Intelligence is undergoing a transition from Generative AI -- probabilistic generation of text and images -- to Agentic AI, in which autonomous systems execute actions within external environments on behalf of users. This transition exposes a fundamental architectural mismatch: Large Language Models (LLMs) produce stochastic, unstructured outputs, whereas the backend infrastructure they must control -- databases, APIs, cloud services -- requires deterministic, schema-conformant inputs. The present paper describes the Auton Agentic AI Framework, a principled architecture for standardizing the creation, execution, and governance of autonomous agent systems. The framework is organized around a strict separation between the Cognitive Blueprint, a declarative, language-agnostic specification of agent identity and capabilities, and the Runtime Engine, the platform-specific execution substrate that instantiates and runs the agent. This separation enables cross-language portability, formal auditability, and modular tool integration via the Model Context Protocol (MCP). The paper formalizes the agent execution model as an augmented Partially Observable Markov Decision Process (POMDP) with a latent reasoning space, introduces a hierarchical memory consolidation architecture inspired by biological episodic memory systems, defines a constraint manifold formalism for safety enforcement via policy projection rather than post-hoc filtering, presents a three-level self-evolution framework spanning in-context adaptation through reinforcement learning, and describes runtime optimizations -- including parallel graph execution, speculative inference, and dynamic context pruning -- that reduce end-to-end latency for multi-step agent workflows.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.23773", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23773", "abs": "https://arxiv.org/abs/2602.23773", "authors": ["Ying Chen", "Hongwei Yu", "Jiawei Hu"], "title": "Entanglement dynamics for atoms near a reflecting boundary: enhancement and suppression by environment-induced interactions", "comment": null, "summary": "We investigate how environment-induced interactions influence the entanglement dynamics of two static atoms placed near a perfectly reflecting boundary. In this setting, the environment-induced interactions include both atom-boundary contributions (position-dependent Lamb shifts) and the induced atom-atom interaction mediated by the field. We show that, regardless of the initial two-atom state, the entanglement dynamics differs qualitatively and quantitatively from predictions that neglect these energy-shift effects. Depending on the geometry and parameter regime, the environment-induced interactions can either enhance entanglement generation -- yielding a larger maximum concurrence and a longer entanglement lifetime -- or suppress it, reducing both the peak concurrence and the survival time. This behavior contrasts sharply with the free-space case, where the environment-induced atom-atom interaction affects entanglement generation only for a restricted class of initial states and does so in an exclusively assisting manner.", "AI": {"tldr": "\u7814\u7a76\u53cd\u5c04\u8fb9\u754c\u9644\u8fd1\u4e24\u539f\u5b50\u7684\u7ea0\u7f20\u52a8\u529b\u5b66\uff0c\u53d1\u73b0\u73af\u5883\u8bf1\u5bfc\u76f8\u4e92\u4f5c\u7528\u53ef\u589e\u5f3a\u6216\u6291\u5236\u7ea0\u7f20\uff0c\u4e0e\u81ea\u7531\u7a7a\u95f4\u60c5\u51b5\u622a\u7136\u4e0d\u540c\u3002", "motivation": "\u63a2\u7a76\u53cd\u5c04\u8fb9\u754c\u8bf1\u5bfc\u7684\u73af\u5883\u76f8\u4e92\u4f5c\u7528\u5982\u4f55\u5f71\u54cd\u4e24\u9759\u6001\u539f\u5b50\u7684\u7ea0\u7f20\u52a8\u529b\u5b66\uff0c\u63ed\u793a\u5176\u4e0e\u81ea\u7531\u7a7a\u95f4\u9884\u6d4b\u7684\u672c\u8d28\u5dee\u5f02\u3002", "method": "\u7406\u8bba\u5206\u6790\u4e24\u9759\u6001\u539f\u5b50\u5728\u7406\u60f3\u53cd\u5c04\u8fb9\u754c\u9644\u8fd1\u7684\u7cfb\u7edf\uff0c\u8ba1\u5165\u539f\u5b50-\u8fb9\u754c\u76f8\u4e92\u4f5c\u7528\uff08\u4f4d\u7f6e\u4f9d\u8d56\u7684Lamb\u4f4d\u79fb\uff09\u548c\u573a\u4ecb\u5bfc\u7684\u539f\u5b50-\u539f\u5b50\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u65e0\u8bba\u521d\u59cb\u6001\u5982\u4f55\uff0c\u7ea0\u7f20\u52a8\u529b\u5b66\u4e0e\u5ffd\u7565\u80fd\u91cf\u79fb\u52a8\u6548\u5e94\u7684\u9884\u6d4b\u5b58\u5728\u8d28\u548c\u91cf\u7684\u5dee\u5f02\uff1b\u51e0\u4f55\u53c2\u6570\u51b3\u5b9a\u73af\u5883\u76f8\u4e92\u4f5c\u7528\u662f\u589e\u5f3a\uff08\u63d0\u5347\u6700\u5927\u5e76\u53d1\u5ea6\u548c\u7ea0\u7f20\u5bff\u547d\uff09\u8fd8\u662f\u6291\u5236\u7ea0\u7f20\u3002", "conclusion": "\u4e0e\u81ea\u7531\u7a7a\u95f4\u4ec5\u5728\u67d0\u4e9b\u521d\u6001\u4e0b\u8f85\u52a9\u7ea0\u7f20\u4e0d\u540c\uff0c\u8fb9\u754c\u8bf1\u5bfc\u7684\u76f8\u4e92\u4f5c\u7528\u53ef\u5728\u4efb\u610f\u521d\u6001\u4e0b\u589e\u5f3a\u6216\u6291\u5236\u7ea0\u7f20\uff0c\u8868\u73b0\u51fa\u5168\u65b0\u7684\u7269\u7406\u884c\u4e3a\u3002"}}
{"id": "2602.23565", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.23565", "abs": "https://arxiv.org/abs/2602.23565", "authors": ["Adhyyan Narang", "Sarah Dean", "Lillian J Ratliff", "Maryam Fazel"], "title": "Dynamics of Learning under User Choice: Overspecialization and Peer-Model Probing", "comment": null, "summary": "In many economically relevant contexts where machine learning is deployed, multiple platforms obtain data from the same pool of users, each of whom selects the platform that best serves them. Prior work in this setting focuses exclusively on the \"local\" losses of learners on the distribution of data that they observe. We find that there exist instances where learners who use existing algorithms almost surely converge to models with arbitrarily poor global performance, even when models with low full-population loss exist. This happens through a feedback-induced mechanism, which we call the overspecialization trap: as learners optimize for users who already prefer them, they become less attractive to users outside this base, which further restricts the data they observe. Inspired by the recent use of knowledge distillation in modern ML, we propose an algorithm that allows learners to \"probe\" the predictions of peer models, enabling them to learn about users who do not select them. Our analysis characterizes when probing succeeds: this procedure converges almost surely to a stationary point with bounded full-population risk when probing sources are sufficiently informative, e.g., a known market leader or a majority of peers with good global performance. We verify our findings with semi-synthetic experiments on the MovieLens, Census, and Amazon Sentiment datasets.", "AI": {"tldr": "Multiple platforms learning from self-selecting users can fall into an \"overspecialization trap\" with arbitrarily poor global performance; the authors propose a knowledge-distillation-inspired probing algorithm that learns from peer models to achieve bounded global risk when peers are sufficiently informative.", "motivation": "In multi-platform settings where users self-select their preferred platform, prior work only optimizes local losses on observed data, which can create feedback loops where platforms become overspecialized to their existing user base, leading to arbitrarily poor global performance despite the existence of good models.", "method": "Inspired by knowledge distillation, the proposed algorithm allows platforms to \"probe\" predictions of peer models (e.g., market leaders or high-performing peers) to learn about users who don't select them, thereby escaping the data restriction caused by self-selection bias.", "result": "Theoretically, probing converges almost surely to a stationary point with bounded full-population risk when probing sources are sufficiently informative; experimentally verified on MovieLens, Census, and Amazon Sentiment datasets showing the algorithm effectively mitigates overspecialization.", "conclusion": "The overspecialization trap is a fundamental limitation of local learning in multi-platform user selection settings, but can be resolved through strategic probing of informative peer models, enabling platforms to achieve bounded global risk and avoid arbitrarily poor performance."}}
{"id": "2602.23730", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23730", "abs": "https://arxiv.org/abs/2602.23730", "authors": ["Longyin Zhang", "Shuo Sun", "Yingxu He", "Won Cheng Yi Lewis", "Muhammad Huzaifah Bin Md Shahrin", "Hardik Bhupendra Sailor", "Heng Meng Jeremy Wong", "Tarun Kumar Vangani", "Yi Ma", "Qiongqiong Wang", "Minh Duc Pham", "Ridong Jiang", "Jingtao Li", "Jingyi Liao", "Zhuohan Liu", "Yanfeng Lu", "Manas Gupta", "Ai Ti Aw"], "title": "Unlocking Cognitive Capabilities and Analyzing the Perception-Logic Trade-off", "comment": null, "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) pursue omni-perception capabilities, yet integrating robust sensory grounding with complex reasoning remains a challenge, particularly for underrepresented regions. In this report, we introduce the research preview of MERaLiON2-Omni (Alpha), a 10B-parameter multilingual omni-perception tailored for Southeast Asia (SEA). We present a progressive training pipeline that explicitly decouples and then integrates \"System 1\" (Perception) and \"System 2\" (Reasoning) capabilities. First, we establish a robust Perception Backbone by aligning region-specific audio-visual cues (e.g., Singlish code-switching, local cultural landmarks) with a multilingual LLM through orthogonal modality adaptation. Second, to inject cognitive capabilities without large-scale supervision, we propose a cost-effective Generate-Judge-Refine pipeline. By utilizing a Super-LLM to filter hallucinations and resolve conflicts via a consensus mechanism, we synthesize high-quality silver data that transfers textual Chain-of-Thought reasoning to multimodal scenarios.\n  Comprehensive evaluation on our newly introduced SEA-Omni Benchmark Suite reveals an Efficiency-Stability Paradox: while reasoning acts as a non-linear amplifier for abstract tasks (boosting mathematical and instruction-following performance significantly), it introduces instability in low-level sensory processing. Specifically, we identify Temporal Drift in long-context audio, where extended reasoning desynchronizes the model from acoustic timestamps, and Visual Over-interpretation, where logic overrides pixel-level reality. This report details the architecture, the data-efficient training recipe, and a diagnostic analysis of the trade-offs between robust perception and structured reasoning.", "AI": {"tldr": "\u672c\u6587\u63a8\u51fa\u9762\u5411\u4e1c\u5357\u4e9a\u768410B\u53c2\u6570\u591a\u8bed\u8a00\u5168\u77e5\u6a21\u578bMERaLiON2-Omni (Alpha)\uff0c\u63ed\u793a\u63a8\u7406\u80fd\u529b\u867d\u80fd\u653e\u5927\u62bd\u8c61\u4efb\u52a1\u6027\u80fd\uff0c\u5374\u4f1a\u5f15\u53d1\u4f4e\u5c42\u6b21\u611f\u5b98\u5904\u7406\u4e0d\u7a33\u5b9a\u6027\u7684\u6548\u7387-\u7a33\u5b9a\u6027\u6096\u8bba\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6574\u5408\u7a33\u5065\u611f\u5b98\u63a5\u5730\u4e0e\u590d\u6742\u63a8\u7406\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u5c24\u5176\u5728\u8d44\u6e90\u4e0d\u8db3\u7684\u4e1c\u5357\u4e9a\u5730\u533a\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u9002\u5408\u8be5\u533a\u57df\u768410B\u53c2\u6570\u591a\u8bed\u8a00\u5168\u77e5\u611f\u77e5\u6a21\u578b\u3002", "method": "\u91c7\u7528\u89e3\u8026\u4e0e\u878d\u5408\"\u7cfb\u7edf1\"\u611f\u77e5\u548c\"\u7cfb\u7edf2\"\u63a8\u7406\u7684\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u6d41\u7a0b\u3002\u5148\u901a\u8fc7\u6b63\u4ea4\u6a21\u6001\u9002\u914d\u6784\u5efa\u611f\u77e5\u9aa8\u5e72\uff0c\u5bf9\u9f50\u533a\u57df\u97f3\u89c6\u9891\u7ebf\u7d22\uff1b\u518d\u901a\u8fc7\u751f\u6210-\u8bc4\u5224-\u7cbe\u70bc\u7ba1\u9053\uff0c\u5229\u7528\u8d85\u7ea7\u5927\u6a21\u578b\u5408\u6210\u9ad8\u8d28\u91cf\u4f2a\u6570\u636e\uff0c\u5b9e\u73b0\u6587\u672c\u94fe\u5f0f\u63a8\u7406\u5411\u591a\u6a21\u6001\u8fc1\u79fb\u3002", "result": "\u5728SEA-Omni\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d1\u73b0\u6548\u7387-\u7a33\u5b9a\u6027\u6096\u8bba\uff1a\u63a8\u7406\u663e\u8457\u63d0\u5347\u6570\u5b66\u548c\u6307\u4ee4\u9075\u5faa\u7b49\u62bd\u8c61\u4efb\u52a1\u6027\u80fd\uff0c\u4f46\u5bfc\u81f4\u957f\u97f3\u9891\u65f6\u95f4\u6f02\u79fb\u548c\u89c6\u89c9\u8fc7\u5ea6\u89e3\u8bfb\u7b49\u4f4e\u5c42\u6b21\u611f\u77e5\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "conclusion": "\u62a5\u544a\u8be6\u8ff0\u4e86\u6a21\u578b\u67b6\u6784\u3001\u6570\u636e\u9ad8\u6548\u8bad\u7ec3\u65b9\u6848\uff0c\u5e76\u5bf9\u7a33\u5065\u611f\u77e5\u4e0e\u7ed3\u6784\u5316\u63a8\u7406\u95f4\u7684\u6743\u8861\u8fdb\u884c\u4e86\u8bca\u65ad\u5206\u6790\u3002"}}
{"id": "2602.23848", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23848", "abs": "https://arxiv.org/abs/2602.23848", "authors": ["Yusuke Kimura", "Yutaka Takita"], "title": "MAFFT-inspired Quantum Shift-based Sequence Alignment and its Efficient Simulation on Decision Diagrams", "comment": "11 pages", "summary": "Multiple sequence alignment (MSA) is a core operation for comparing genome sequences and is widely used in bio-informatics. MAFFT, a practical MSA tool, repeatedly shifts a pair of sequences and computes a distance. Because the number of sequence pairs grows quadratically with the number of sequences, this procedure can become a bottleneck.\n  We propose Quantum Shift-based Sequence Alignment (QShift-SA), which implements this ``shift-wise score computation'' as a gate-based quantum circuit and searches over shift amounts and sequence pairs using Grover algorithm. QShift-SA constructs an oracle circuit that compute the Hamming distance (the number of mismatches) between two sequences with data encoding, controlled shift, comparison, and addition. This oracle can search for candidates with small distances. QShift-SA does not aim to replace the full MSA workflow; instead, it targets the screening steps that often dominate the runtime in classical MAFFT as stated above.\n  We evaluate circuit resources (number of qubits, gate count, and depth) and benchmark simulation time across multiple quantum circuit simulators. We find that a decision diagram (DD)-based quantum circuit simulator runs more than 1,000$\\times$ faster than state-vector and MPS simulators and can handle larger circuits.", "AI": {"tldr": "\u63d0\u51fa\u91cf\u5b50\u7b97\u6cd5QShift-SA\uff0c\u5229\u7528Grover\u641c\u7d22\u52a0\u901f\u591a\u5e8f\u5217\u6bd4\u5bf9\u4e2d\u7684\u4f4d\u79fb\u8ba1\u7b97\u74f6\u9888\uff0c\u901a\u8fc7\u91cf\u5b50\u7535\u8def\u5b9e\u73b0\u6c49\u660e\u8ddd\u79bb\u8ba1\u7b97\uff0cDD\u6a21\u62df\u5668\u6bd4\u4f20\u7edf\u6a21\u62df\u5668\u5feb1000\u500d", "motivation": "\u7ecf\u5178MAFFT\u5de5\u5177\u5728\u591a\u5e8f\u5217\u6bd4\u5bf9\u4e2d\u9700\u8ba1\u7b97\u6240\u6709\u5e8f\u5217\u5bf9\u7684\u4f4d\u79fb\uff0c\u8ba1\u7b97\u91cf\u968f\u5e8f\u5217\u6570\u5448\u4e8c\u6b21\u589e\u957f\u6210\u4e3a\u6027\u80fd\u74f6\u9888", "method": "\u8bbe\u8ba1\u91cf\u5b50\u4f4d\u79fb\u5e8f\u5217\u5bf9\u9f50(QShift-SA)\u7b97\u6cd5\uff1a\u6784\u5efa\u91cf\u5b50Oracle\u7535\u8def\u8ba1\u7b97\u5e8f\u5217\u95f4\u6c49\u660e\u8ddd\u79bb\uff0c\u7ed3\u5408Grover\u7b97\u6cd5\u641c\u7d22\u6700\u5c0f\u8ddd\u79bb\u7684\u4f4d\u79fb\u7ec4\u5408", "result": "\u51b3\u7b56\u56fe(DD)\u91cf\u5b50\u7535\u8def\u6a21\u62df\u5668\u6bd4\u72b6\u6001\u5411\u91cf\u548cMPS\u6a21\u62df\u5668\u5feb1000\u500d\u4ee5\u4e0a\uff0c\u53ef\u5904\u7406\u66f4\u5927\u89c4\u6a21\u7535\u8def", "conclusion": "\u8be5\u65b9\u6848\u9488\u5bf9\u6027\u52a0\u901fMAFFT\u7684\u8017\u65f6\u6548\u6b65\u9aa4\u800c\u975e\u6574\u4e2a\u6d41\u7a0b\uff0c\u4e3a\u91cf\u5b50\u8ba1\u7b97\u5728\u751f\u7269\u4fe1\u606f\u5b66\u4e2d\u7684\u5b9e\u7528\u5316\u63d0\u4f9b\u53ef\u884c\u8def\u5f84"}}
{"id": "2602.23777", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23777", "abs": "https://arxiv.org/abs/2602.23777", "authors": ["Zhipeng Xu", "Zilong Wang", "Xinyang Jiang", "Dongsheng Li", "De Cheng", "Nannan Wang"], "title": "Reasoning-Driven Multimodal LLM for Domain Generalization", "comment": "Accepted at ICLR 2026 (Poster)", "summary": "This paper addresses the domain generalization (DG) problem in deep learning. While most DG methods focus on enforcing visual feature invariance, we leverage the reasoning capability of multimodal large language models (MLLMs) and explore the potential of constructing reasoning chains that derives image categories to achieve more robust predictions under domain shift. To this end, we systematically study the role of reasoning in DG using DomainBed-Reasoning, a newly constructed extension of DomainBed dataset, in which each sample is paired with class-relevant reasoning chains. Our analysis reveals two key challenges: (i) fine-tuning MLLMs with reasoning chains for classification is more challenging than direct label supervision, since the model must optimize complex reasoning sequences before label prediction; and (ii) mismatches in reasoning patterns between supervision signals and fine-tuned MLLMs lead to a trade-off between semantic richness (informative but harder to optimize) and optimization efficiency (easier to optimize but less informative). To address these issues, we propose RD-MLDG (Reasoning-Driven Multimodal LLM for Domain Generalization), a framework with two components: (i) MTCT (Multi-Task Cross-Training), which introduces an additional direct classification pathway to guide reasoning supervision; and (ii) SARR (Self-Aligned Reasoning Regularization), which preserves the semantic richness of reasoning chains while mitigating reasoning-pattern mismatches via iterative self-labeling. Experiments on standard DomainBed datasets (PACS, VLCS, OfficeHome, TerraInc) demonstrate that RD-MLDG achieves state-of-the-art performances, highlighting reasoning as a promising complementary signal for robust out-of-domain generalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRD-MLDG\u6846\u67b6\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u89e3\u51b3\u9886\u57df\u6cdb\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u4ea4\u53c9\u8bad\u7ec3\u548c\u81ea\u5bf9\u9f50\u63a8\u7406\u6b63\u5219\u5316\u5b9e\u73b0\u66f4\u9c81\u68d2\u7684\u8de8\u9886\u57df\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9\u7279\u5f81\u4e0d\u53d8\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002\u672c\u6587\u63a2\u7d22\u5229\u7528\u63a8\u7406\u94fe\u6765\u63d0\u5347\u6a21\u578b\u5728\u9886\u57df\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u7cfb\u7edf\u7814\u7a76\u63a8\u7406\u5728\u9886\u57df\u6cdb\u5316\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u63d0\u51faRD-MLDG\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u7ec4\u4ef6\uff1a1) MTCT\uff08\u591a\u4efb\u52a1\u4ea4\u53c9\u8bad\u7ec3\uff09\uff0c\u5f15\u5165\u989d\u5916\u5206\u7c7b\u8def\u5f84\u6307\u5bfc\u63a8\u7406\u76d1\u7763\uff1b2) SARR\uff08\u81ea\u5bf9\u9f50\u63a8\u7406\u6b63\u5219\u5316\uff09\uff0c\u901a\u8fc7\u8fed\u4ee3\u81ea\u6807\u6ce8\u5728\u4fdd\u6301\u63a8\u7406\u8bed\u4e49\u4e30\u5bcc\u6027\u7684\u540c\u65f6\u7f13\u89e3\u63a8\u7406\u6a21\u5f0f\u4e0d\u5339\u914d\u3002\u5728\u81ea\u5efa\u6570\u636e\u96c6DomainBed-Reasoning\u4e0a\u8fdb\u884c\u7cfb\u7edf\u7814\u7a76\u3002", "result": "\u5728\u6807\u51c6DomainBed\u6570\u636e\u96c6\uff08PACS\u3001VLCS\u3001OfficeHome\u3001TerraInc\uff09\u4e0a\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u63a8\u7406\u4f5c\u4e3a\u4e92\u8865\u4fe1\u53f7\u5bf9\u9886\u57df\u5916\u6cdb\u5316\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u662f\u9886\u57df\u6cdb\u5316\u7684\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u63a8\u7406\u94fe\u53ef\u4ee5\u4f5c\u4e3a\u6709\u6548\u7684\u76d1\u7763\u4fe1\u53f7\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.23865", "categories": ["quant-ph", "cs.LO", "math.CT"], "pdf": "https://arxiv.org/pdf/2602.23865", "abs": "https://arxiv.org/abs/2602.23865", "authors": ["Matt Wilson", "James Hefford", "Timoth\u00e9e Hoffreumon"], "title": "Supermaps on generalised theories", "comment": null, "summary": "Categorical supermaps generalise higher-order quantum operations from finite-dimensional quantum theory to arbitrary circuit theories. In this paper, we establish the Yoneda lemma for categorical supermaps, which states that whenever a physical theory has a suitable notion of channel-state duality, then categorical supermaps on that theory can be concretely represented in terms of that duality. This lemma eliminates any guesswork or ambiguity when defining the appropriate notion of supermap for these theories. As a concrete application, we show that the recently proposed higher-order processes on boxworld can be obtained as a particular instance of categorical supermaps, and put forward a stable definition of higher-order real quantum theory.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7Yoneda\u5f15\u7406\u4e3a\u8303\u7574\u8d85\u56fe\u5efa\u7acb\u4e86\u7cfb\u7edf\u5316\u7684\u6570\u5b66\u6846\u67b6\uff0c\u5c06\u4efb\u610f\u7269\u7406\u7406\u8bba\u4e2d\u7684\u9ad8\u9636\u64cd\u4f5c\u5b9a\u4e49\u4e3a\u901a\u9053-\u6001\u5bf9\u5076\u6027\u7684\u5177\u4f53\u8868\u793a\uff0c\u6d88\u9664\u4e86\u5b9a\u4e49\u6a21\u7cca\u6027\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u76d2\u4e16\u754c\u548c\u9ad8\u9636\u5b9e\u91cf\u5b50\u7406\u8bba\u3002", "motivation": "\u5728\u4efb\u610f\u7535\u8def\u7406\u8bba\u4e2d\u5b9a\u4e49\u5408\u9002\u7684\u9ad8\u9636\u8d85\u56fe\u64cd\u4f5c\u5b58\u5728\u731c\u6d4b\u6027\u548c\u6a21\u7cca\u6027\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u6570\u5b66\u57fa\u7840\u3002", "method": "\u8fd0\u7528\u8303\u7574\u8bba\u4e2d\u7684Yoneda\u5f15\u7406\uff0c\u5c06\u8303\u7574\u8d85\u56fe\u901a\u8fc7\u901a\u9053-\u6001\u5bf9\u5076\u6027\u8fdb\u884c\u5177\u4f53\u5316\u8868\u793a\u3002", "result": "\u5efa\u7acb\u4e86Yoneda\u5f15\u7406\u4e0e\u8303\u7574\u8d85\u56fe\u7684\u5173\u7cfb\uff1b\u8bc1\u660e\u4e86\u76d2\u4e16\u754c\u7684\u9ad8\u9636\u8fc7\u7a0b\u662f\u5176\u7279\u4f8b\uff1b\u63d0\u51fa\u4e86\u7a33\u5b9a\u9ad8\u9636\u5b9e\u91cf\u5b50\u7406\u8bba\u7684\u5b9a\u4e49\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5404\u7c7b\u7269\u7406\u7406\u8bba\u4e2d\u7684\u8d85\u56fe\u63d0\u4f9b\u4e86\u65e0\u6b67\u4e49\u3001\u7cfb\u7edf\u5316\u7684\u5b9a\u4e49\u65b9\u6cd5\uff0c\u7edf\u4e00\u4e86\u4e0d\u540c\u7406\u8bba\u95f4\u7684\u9ad8\u9636\u64cd\u4f5c\u63cf\u8ff0\u3002"}}
{"id": "2602.23578", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23578", "abs": "https://arxiv.org/abs/2602.23578", "authors": ["Junghoon Justin Park", "Maria Pak", "Sebin Lee", "Samuel Yen-Chi Chen", "Shinjae Yoo", "Huan-Hsin Tseng", "Jiook Cha"], "title": "Hybrid Quantum Temporal Convolutional Networks", "comment": null, "summary": "Quantum machine learning models for sequential data face scalability challenges with complex multivariate signals. We introduce the Hybrid Quantum Temporal Convolutional Network (HQTCN), which combines classical temporal windowing with a quantum convolutional neural network core. By applying a shared quantum circuit across temporal windows, HQTCN captures long-range dependencies while achieving significant parameter reduction. Evaluated on synthetic NARMA sequences and high-dimensional EEG time-series, HQTCN performs competitively with classical baselines on univariate data and outperforms all baselines on multivariate tasks. The model demonstrates particular strength under data-limited conditions, maintaining high performance with substantially fewer parameters than conventional approaches. These results establish HQTCN as a parameter-efficient approach for multivariate time-series analysis.", "AI": {"tldr": "Proposes HQTCN: a hybrid quantum-classical model for multivariate time-series analysis that uses shared quantum circuits across temporal windows to reduce parameters while outperforming classical baselines.", "motivation": "Quantum machine learning models struggle with scalability when processing complex multivariate sequential signals.", "method": "Hybrid Quantum Temporal Convolutional Network (HQTCN) combining classical temporal windowing with a quantum convolutional neural network core, applying shared quantum circuits across windows.", "result": "Competitive on univariate tasks, outperforms classical baselines on multivariate EEG and NARMA data, maintains high performance with substantially fewer parameters, especially effective in data-limited scenarios.", "conclusion": "HQTCN establishes a parameter-efficient quantum approach for multivariate time-series analysis."}}
{"id": "2602.23802", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23802", "abs": "https://arxiv.org/abs/2602.23802", "authors": ["Yiyang Fang", "Wenke Huang", "Pei Fu", "Yihao Yang", "Kehua Su", "Zhenbo Luo", "Jian Luan", "Mang Ye"], "title": "EMO-R3: Reflective Reinforcement Learning for Emotional Reasoning in Multimodal Large Language Models", "comment": "Accepted by CVPR 2026", "summary": "Multimodal Large Language Models (MLLMs) have shown remarkable progress in visual reasoning and understanding tasks but still struggle to capture the complexity and subjectivity of human emotions. Existing approaches based on supervised fine-tuning often suffer from limited generalization and poor interpretability, while reinforcement learning methods such as Group Relative Policy Optimization fail to align with the intrinsic characteristics of emotional cognition. To address these challenges, we propose Reflective Reinforcement Learning for Emotional Reasoning (EMO-R3), a framework designed to enhance the emotional reasoning ability of MLLMs. Specifically, we introduce Structured Emotional Thinking to guide the model to perform step-by-step emotional reasoning in a structured and interpretable manner, and design a Reflective Emotional Reward that enables the model to re-evaluate its reasoning based on visual-text consistency and emotional coherence. Extensive experiments demonstrate that EMO-R3 significantly improves both the interpretability and emotional intelligence of MLLMs, achieving superior performance across multiple visual emotional understanding benchmarks.", "AI": {"tldr": "\u63d0\u51faEMO-R3\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u60c5\u611f\u601d\u8003\u548c\u53cd\u601d\u6027\u60c5\u611f\u5956\u52b1\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u60c5\u611f\u63a8\u7406\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027", "motivation": "\u73b0\u6709MLLMs\u5728\u60c5\u611f\u7406\u89e3\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u76d1\u7763\u5fae\u8c03\u6cdb\u5316\u6027\u5dee\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e0d\u7b26\u5408\u60c5\u611f\u8ba4\u77e5\u5185\u5728\u7279\u5f81", "method": "\u63d0\u51fa\u53cd\u601d\u6027\u5f3a\u5316\u5b66\u4e60\u6846\u67b6EMO-R3\uff0c\u5305\u542b\u7ed3\u6784\u5316\u60c5\u611f\u601d\u8003\uff08\u6307\u5bfc\u9010\u6b65\u63a8\u7406\uff09\u548c\u53cd\u601d\u6027\u60c5\u611f\u5956\u52b1\uff08\u57fa\u4e8e\u89c6\u89c9-\u6587\u672c\u4e00\u81f4\u6027\u548c\u60c5\u611f\u4e00\u81f4\u6027\u8fdb\u884c\u91cd\u8bc4\u4f30\uff09", "result": "\u663e\u8457\u63d0\u5347MLLMs\u7684\u53ef\u89e3\u91ca\u6027\u548c\u60c5\u611f\u667a\u80fd\uff0c\u5728\u591a\u4e2a\u89c6\u89c9\u60c5\u611f\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02", "conclusion": "EMO-R3\u6846\u67b6\u6709\u6548\u589e\u5f3a\u4e86MLLMs\u7684\u60c5\u611f\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u60c5\u611f\u8ba4\u77e5\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2602.23868", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23868", "abs": "https://arxiv.org/abs/2602.23868", "authors": ["Zhichen Huang", "Chunxiao Du", "Yang Zhou", "Zhisong Xiao"], "title": "Non-commutative Index of Measurement-only Entanglement Phase Transition", "comment": "10 pages, 11 figures", "summary": "Measurement-only models offer an ideal platform for exploring entanglement dynamics in the absence of unitary evolution. Despite extensive numerical evidence for entanglement phase transitions in measurement-only dynamics, the underlying mechanism attributed to non-commutativity among multi-site projective measurements has remained qualitative and coarse-grained. In this work, we identify a quantitative non-commutative index. By applying this index into three representative measurement-only models, we elucidate the role of non-commutativity in measurement-only dynamics: the emergence of a volume-law phase is governed by the non-commutative structure of the measurement ensemble, while the transition point is quantitatively determined by the amount of critical non-commutativity. More strikingly, the critical non-commutativity exhibits a universal linear scaling with the measurement range, independent of the microscopic details of the measurement ensembles. Our findings deepen the understanding of the fundamental mechanism behind the measurement-only entanglement phase transition.", "AI": {"tldr": "This paper develops a quantitative non-commutative index to explain entanglement phase transitions in measurement-only models, revealing universal scaling laws between critical non-commutativity and measurement range.", "motivation": "While measurement-only models show clear entanglement phase transitions, the fundamental mechanism of non-commutativity among multi-site measurements has only been qualitatively understood, lacking a quantitative framework.", "method": "The authors introduce a quantitative non-commutative index and apply it to three representative measurement-only models to systematically analyze the role of non-commutativity in entanglement dynamics.", "result": "The volume-law phase emergence is governed by non-commutative structure; transition points are determined by critical non-commutativity levels; universal linear scaling exists between critical non-commutativity and measurement range, independent of microscopic details.", "conclusion": "This work provides a quantitative foundation for understanding measurement-only entanglement phase transitions, revealing the universal role of non-commutativity across different models."}}
{"id": "2602.23581", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23581", "abs": "https://arxiv.org/abs/2602.23581", "authors": ["Xiang Ao"], "title": "SDMixer: Sparse Dual-Mixer for Time Series Forecasting", "comment": "12pages,2 figures", "summary": "Multivariate time series forecasting is widely applied in fields such as transportation, energy, and finance. However, the data commonly suffers from issues of multi-scale characteristics, weak correlations, and noise interference, which limit the predictive performance of existing models. This paper proposes a dual-stream sparse Mixer prediction framework that extracts global trends and local dynamic features from sequences in both the frequency and time domains, respectively. It employs a sparsity mechanism to filter out invalid information, thereby enhancing the accuracy of cross-variable dependency modeling. Experimental results demonstrate that this method achieves leading performance on multiple real-world scenario datasets, validating its effectiveness and generality. The code is available at https://github.com/SDMixer/SDMixer", "AI": {"tldr": "Dual-stream sparse Mixer framework for multivariate time series forecasting using frequency/time domains and sparsity to improve accuracy", "motivation": "Existing models underperform due to multi-scale characteristics, weak correlations, and noise interference in real-world multivariate time series data", "method": "Proposes a dual-stream sparse Mixer framework extracting global trends (frequency domain) and local dynamics (time domain) with sparsity-based noise filtering", "result": "Achieves state-of-the-art performance across multiple real-world datasets (transportation, energy, finance)", "conclusion": "Effectively enhances cross-variable dependency modeling and demonstrates strong generality for practical forecasting applications"}}
{"id": "2602.23883", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23883", "abs": "https://arxiv.org/abs/2602.23883", "authors": ["Nripendra Majumdar"], "title": "Four Party Absolutely Maximal Contextual Correlations", "comment": "12 pages, 0 figure, 2 tables", "summary": "The Kochen Specker theorem revealed contextuality as a fundamental nonclassical feature of nature. Nonlocality arises as a special case of contextuality, where entangled states shared by space like separated parties exhibit nonlocal correlations. The notion of maximality in correlations, analogous to maximal entanglement, is less explored in multipartite systems. In our work, we have defined maximal correlations in terms of contextual models, which are analogous to absolutely maximally entangled (AME) states. Employing the sheaf theoretic framework, we introduce maximal contextual correlations associated with the corresponding maximal contextual model. The formalism introduces the contextual fraction CF as a measure of contextuality, taking values from 0 (noncontextual) to 1 (fully contextual). This enables the formulation of a new class of correlations termed absolutely maximal contextual correlations (AMCC), which are both maximally contextual and maximal marginals. In the bipartite setting, the canonical example is the Popescu Rohrlich (PR) box, while in the tripartite case, it includes Greenberger Horne Zeilinger (GHZ) correlations and three way nonlocal correlations. In this work, we extend these findings to four party correlations. Notably, no AME state exists for four qubits, which introduces a subtle difference between AMCC and AME. The construction follows the constraint satisfaction problem (CSP) and parity check methods. In particular, the explicit realization of a non AMCC correlation that is maximally contextual yet not maximal marginal is obtained within the CSP framework.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u91cf\u5b50\u975e\u5c40\u57df\u6027\u62d3\u5c55\u81f3\u4e0a\u4e0b\u6587\u6027\u6846\u67b6\uff0c\u63d0\u51fa\"\u7edd\u5bf9\u6700\u5927\u4e0a\u4e0b\u6587\u5173\u8054\"\uff08AMCC\uff09\u6982\u5ff5\u4ee5\u7c7b\u6bd4\u6700\u5927\u7ea0\u7f20\u6001\uff0c\u63ed\u793a\u591a\u4f53\u7cfb\u7edf\u4e2d\u5173\u8054\u6027\u7684\u65b0\u5206\u7c7b\u3002\u901a\u8fc7\u5c42\u8bba\u4e0e\u7ea6\u675f\u6ee1\u8db3\u65b9\u6cd5\uff0c\u53d1\u73b0\u56db\u4f53\u7cfb\u7edf\u4e2d\u4e0a\u4e0b\u6587\u5173\u8054\u4e0e\u7ea0\u7f20\u6001\u5b58\u5728\u672c\u8d28\u5dee\u5f02\uff0c\u5e76\u6784\u9020\u51fa\u9996\u4e2a\"\u6700\u5927\u4e0a\u4e0b\u6587\u4f46\u975e\u6700\u5927\u8fb9\u7f18\"\u7684\u5173\u8054\u5b9e\u4f8b\u3002", "motivation": "\u4f20\u7edf\u7814\u7a76\u805a\u7126\u4e8e\u91cf\u5b50\u7ea0\u7f20\uff0c\u4f46Kochen-Specker\u5b9a\u7406\u8868\u660e\u4e0a\u4e0b\u6587\u6027\u624d\u662f\u66f4\u57fa\u7840\u7684\u975e\u7ecf\u5178\u7279\u6027\u3002\u73b0\u6709\u6587\u732e\u5bf9\u591a\u4f53\u7cfb\u7edf\u4e2d\"\u6700\u5927\u5173\u8054\"\u7684\u5b9a\u4e49\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u89c6\u89d2\uff0c\u4e14\u672a\u89e3\u51b3\u56db\u4f53\u4ee5\u4e0a\u7cfb\u7edf\u7684\u5173\u8054\u5206\u7c7b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5c42\u8bba\uff08sheaf theory\uff09\u6846\u67b6\u5f62\u5f0f\u5316\u4e0a\u4e0b\u6587\u5173\u8054\uff0c\u5b9a\u4e49\u4e0a\u4e0b\u6587\u5206\u6570\uff08CF\uff09\u91cf\u5316\u8bed\u5883\u6027\u7a0b\u5ea6\uff1b\u7ed3\u5408\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\uff08CSP\uff09\u4e0e\u5947\u5076\u6821\u9a8c\u65b9\u6cd5\uff0c\u6784\u5efa\u591a\u4f53\u5173\u8054\u6a21\u578b\u5e76\u5206\u6790\u5176\u6781\u503c\u7279\u6027\u3002", "result": "1) \u63d0\u51faAMCC\u6982\u5ff5\uff08\u517c\u5177\u6700\u5927\u4e0a\u4e0b\u6587\u6027\u4e0e\u6700\u5927\u8fb9\u7f18\u6027\uff09\\n2) \u53d1\u73b0\u56db\u4f53\u7cfb\u7edf\u4e2dAMCC\u4e0eAME\u6001\u4e0d\u5b58\u5728\u5bf9\u5e94\u5173\u7cfb\\n3) \u901a\u8fc7CSP\u6846\u67b6\u6784\u9020\u51fa\u65b0\u578b\u5173\u8054\uff1a\u6700\u5927\u4e0a\u4e0b\u6587\u4f46\u975e\u6700\u5927\u8fb9\u7f18\u7684\u5b9e\u4f8b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5efa\u7acb\u4e86\u591a\u4f53\u4e0a\u4e0b\u6587\u5173\u8054\u7684\u7406\u8bba\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u4e0a\u4e0b\u6587\u6027\u4e0e\u7ea0\u7f20\u7684\u672c\u8d28\u5dee\u5f02\uff0c\u4e3a\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u8d44\u6e90\u5206\u7c7b\u6807\u51c6\uff0c\u540c\u65f6\u8868\u660e\u7ea6\u675f\u6ee1\u8db3\u65b9\u6cd5\u662f\u7814\u7a76\u590d\u6742\u91cf\u5b50\u5173\u8054\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2602.23599", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23599", "abs": "https://arxiv.org/abs/2602.23599", "authors": ["Dang Sy Duy", "Nguyen Duy Chien", "Kapil Dev", "Jeff Nijsse"], "title": "Normalisation and Initialisation Strategies for Graph Neural Networks in Blockchain Anomaly Detection", "comment": "14 pages, 5 figures", "summary": "Graph neural networks (GNNs) offer a principled approach to financial fraud detection by jointly learning from node features and transaction graph topology. However, their effectiveness on real-world anti-money laundering (AML) benchmarks depends critically on training practices such as specifically weight initialisation and normalisation that remain underexplored. We present a systematic ablation of initialisation and normalisation strategies across three GNN architectures (GCN, GAT, and GraphSAGE) on the Elliptic Bitcoin dataset. Our experiments reveal that initialisation and normalisation are architecture-dependent: GraphSAGE achieves the strongest performance with Xavier initialisation alone, GAT benefits most from combining GraphNorm with Xavier initialisation, while GCN shows limited sensitivity to these modifications. These findings offer practical, architecture-specific guidance for deploying GNNs in AML pipelines for datasets with severe class imbalance. We release a reproducible experimental framework with temporal data splits, seeded runs, and full ablation results.", "AI": {"tldr": "This paper systematically studies how weight initialization and normalization affect three GNN architectures for Bitcoin anti-money laundering detection, finding that optimal strategies are architecture-specific and releasing a reproducible framework.", "motivation": "GNNs show promise for financial fraud detection but their performance on real-world AML benchmarks heavily depends on underexplored training practices like weight initialization and normalization, especially given severe class imbalance in transaction data.", "method": "Conducted a systematic ablation study comparing initialization (Xavier) and normalization (GraphNorm) strategies across three GNN architectures (GCN, GAT, GraphSAGE) using the Elliptic Bitcoin dataset with temporal splits and seeded runs for reproducibility.", "result": "Found that performance is architecture-dependent: GraphSAGE achieves best results with Xavier initialization alone, GAT benefits most from combining GraphNorm with Xavier initialization, and GCN shows minimal sensitivity to these changes.", "conclusion": "Provides practical architecture-specific guidance for GNN deployment in AML pipelines, addressing class imbalance challenges, and releases a reproducible experimental framework with full results."}}
{"id": "2602.23876", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23876", "abs": "https://arxiv.org/abs/2602.23876", "authors": ["Ning Gao", "Xiuhui Zhang", "Xingyu Jiang", "Mukang You", "Mohan Zhang", "Yue Deng"], "title": "RF-Agent: Automated Reward Function Design via Language Agent Tree Search", "comment": "39 pages, 9 tables, 11 figures, Project page see https://github.com/deng-ai-lab/RF-Agent", "summary": "Designing efficient reward functions for low-level control tasks is a challenging problem. Recent research aims to reduce reliance on expert experience by using Large Language Models (LLMs) with task information to generate dense reward functions. These methods typically rely on training results as feedback, iteratively generating new reward functions with greedy or evolutionary algorithms. However, they suffer from poor utilization of historical feedback and inefficient search, resulting in limited improvements in complex control tasks. To address this challenge, we propose RF-Agent, a framework that treats LLMs as language agents and frames reward function design as a sequential decision-making process, enhancing optimization through better contextual reasoning. RF-Agent integrates Monte Carlo Tree Search (MCTS) to manage the reward design and optimization process, leveraging the multi-stage contextual reasoning ability of LLMs. This approach better utilizes historical information and improves search efficiency to identify promising reward functions. Outstanding experimental results in 17 diverse low-level control tasks demonstrate the effectiveness of our method. The source code is available at https://github.com/deng-ai-lab/RF-Agent.", "AI": {"tldr": "\u63d0\u51faRF-Agent\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u4f18\u5316\u4f4e\u9636\u63a7\u5236\u4efb\u52a1\u7684\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\uff0c\u572817\u9879\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u6709\u6548", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5956\u52b1\u51fd\u6570\u7684\u65b9\u6cd5\u5b58\u5728\u5386\u53f2\u53cd\u9988\u5229\u7528\u7387\u4f4e\u3001\u641c\u7d22\u6548\u7387\u5dee\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u63a7\u5236\u4efb\u52a1", "method": "\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8bed\u8a00\u667a\u80fd\u4f53\uff0c\u91c7\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7ba1\u7406\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u4e0e\u4f18\u5316\u6d41\u7a0b\uff0c\u5229\u7528\u5927\u6a21\u578b\u7684\u591a\u9636\u6bb5\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u63d0\u5347\u641c\u7d22\u6548\u7387", "result": "\u572817\u4e2a\u591a\u6837\u5316\u4f4e\u9636\u63a7\u5236\u4efb\u52a1\u4e2d\u53d6\u5f97\u4f18\u5f02\u5b9e\u9a8c\u7ed3\u679c", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6539\u8fdb\u5386\u53f2\u4fe1\u606f\u5229\u7528\u548c\u641c\u7d22\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86\u590d\u6742\u63a7\u5236\u4efb\u52a1\u4e2d\u5956\u52b1\u51fd\u6570\u7684\u8bbe\u8ba1\u6548\u7387"}}
{"id": "2602.23888", "categories": ["quant-ph", "cond-mat.supr-con"], "pdf": "https://arxiv.org/pdf/2602.23888", "abs": "https://arxiv.org/abs/2602.23888", "authors": ["Rangga P. Budoyo", "Rasanayagam S. Kajen", "Bing Wen Cheah", "Long H. Nguyen", "Rainer Dumke"], "title": "Characterization of Josephson Junction Aging and Annealing Under Different Environments", "comment": "9 pages, 4 figures", "summary": "Understanding the aging behavior of Josephson junctions and the effect of annealing on junction resistances is important in building large-scale superconducting quantum processors. Here we study the effects of aging of Josephson junctions under different storage conditions from immediately after fabrication up to 2 to 3 months. We find that the aging curve follows a logarithmic curve, with the aging amplitude mainly determined by fabrication conditions and the aging speed determined by storage conditions. Junctions stored at ambient laboratory conditions aged faster compared to junctions stored in a nitrogen atmosphere or vacuum, with the aging speed appreciably changes when the storage condition changed. We also compared the effect of thermal annealing under nitrogen environment with annealing under ambient conditions up to 250$^\\circ$ C. We find that under nitrogen environment, the resistances decreased at all temperatures tested, while under ambient environment the resistances increased at 200$^\\circ$ C and decreased at 250$^\\circ$ C instead. We were unable to decrease the resistance below the initial-time resistance, suggesting a lower limit on the range of resistance tuning.", "AI": {"tldr": "\u7ea6\u745f\u592b\u68ee\u7ed3\u7684\u7535\u963b\u8001\u5316\u9075\u5faa\u5bf9\u6570\u89c4\u5f8b\uff0c\u5b58\u50a8\u73af\u5883\u663e\u8457\u5f71\u54cd\u8001\u5316\u901f\u5ea6\uff0c\u6c2e\u6c14\u73af\u5883\u4e2d\u9000\u706b\u53ef\u964d\u4f4e\u7535\u963b\u800c\u7a7a\u6c14\u73af\u5883\u5728200\u00b0C\u65f6\u7535\u963b\u53cd\u5e38\u5347\u9ad8\uff0c\u4e14\u7535\u963b\u65e0\u6cd5\u964d\u81f3\u521d\u59cb\u503c\u4ee5\u4e0b\u3002", "motivation": "\u7406\u89e3\u7ea6\u745f\u592b\u68ee\u7ed3\u7684\u8001\u5316\u884c\u4e3a\u53ca\u9000\u706b\u5bf9\u7ed3\u7535\u963b\u7684\u5f71\u54cd\uff0c\u5bf9\u6784\u5efa\u5927\u89c4\u6a21\u8d85\u5bfc\u91cf\u5b50\u5904\u7406\u5668\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5bf9\u6bd4\u7814\u7a76\u4e0d\u540c\u5b58\u50a8\u6761\u4ef6\uff08\u7a7a\u6c14/\u6c2e\u6c14/\u771f\u7a7a\uff09\u4e0b\u7ed3\u7535\u963b\u57282-3\u4e2a\u6708\u5185\u7684\u8001\u5316\u89c4\u5f8b\uff0c\u5e76\u6d4b\u8bd5\u6c2e\u6c14\u4e0e\u7a7a\u6c14\u73af\u5883\u4e2d250\u00b0C\u4ee5\u5185\u7684\u70ed\u9000\u706b\u5bf9\u7535\u963b\u7684\u8c03\u63a7\u6548\u679c\u3002", "result": "1) \u8001\u5316\u66f2\u7ebf\u5448\u5bf9\u6570\u5173\u7cfb\uff0c\u8001\u5316\u5e45\u5ea6\u7531\u5236\u5907\u6761\u4ef6\u51b3\u5b9a\uff0c\u8001\u5316\u901f\u5ea6\u7531\u5b58\u50a8\u6761\u4ef6\u51b3\u5b9a\uff1b\u7a7a\u6c14\u5b58\u50a8\u8001\u5316\u6700\u5feb\uff1b2) \u6c2e\u6c14\u9000\u706b\u5728\u6240\u6709\u6d4b\u8bd5\u6e29\u5ea6\u4e0b\u5747\u964d\u4f4e\u7535\u963b\uff0c\u7a7a\u6c14\u9000\u706b\u5728200\u00b0C\u65f6\u7535\u963b\u5347\u9ad8\u800c\u5728250\u00b0C\u65f6\u964d\u4f4e\uff1b3) \u7535\u963b\u65e0\u6cd5\u964d\u81f3\u521d\u59cb\u503c\u4ee5\u4e0b\uff0c\u5b58\u5728\u8c03\u63a7\u4e0b\u9650\u3002", "conclusion": "\u5b58\u50a8\u6761\u4ef6\u662f\u63a7\u5236\u8001\u5316\u901f\u5ea6\u7684\u5173\u952e\uff0c\u9000\u706b\u73af\u5883\u663e\u8457\u5f71\u54cd\u7535\u963b\u8c03\u63a7\u65b9\u5411\uff0c\u4e14\u7ed3\u7535\u963b\u5b58\u5728\u4e0d\u53ef\u7a81\u7834\u7684\u4e0b\u9650\uff0c\u8fd9\u5bf9\u91cf\u5b50\u5904\u7406\u5668\u5236\u5907\u5de5\u827a\u4f18\u5316\u5177\u6709\u91cd\u8981\u6307\u5bfc\u610f\u4e49\u3002"}}
{"id": "2602.23614", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23614", "abs": "https://arxiv.org/abs/2602.23614", "authors": ["Kejing Yin", "Haizhou Xu", "Wenfang Yao", "Chen Liu", "Zijie Chen", "Yui Haang Cheung", "William K. Cheung", "Jing Qin"], "title": "When Does Multimodal Learning Help in Healthcare? A Benchmark on EHR and Chest X-Ray Fusion", "comment": null, "summary": "Machine learning holds promise for advancing clinical decision support, yet it remains unclear when multimodal learning truly helps in practice, particularly under modality missingness and fairness constraints. In this work, we conduct a systematic benchmark of multimodal fusion between Electronic Health Records (EHR) and chest X-rays (CXR) on standardized cohorts from MIMIC-IV and MIMIC-CXR, aiming to answer four fundamental questions: when multimodal fusion improves clinical prediction, how different fusion strategies compare, how robust existing methods are to missing modalities, and whether multimodal models achieve algorithmic fairness. Our study reveals several key insights. Multimodal fusion improves performance when modalities are complete, with gains concentrating in diseases that require complementary information from both EHR and CXR. While cross-modal learning mechanisms capture clinically meaningful dependencies beyond simple concatenation, the rich temporal structure of EHR introduces strong modality imbalance that architectural complexity alone cannot overcome. Under realistic missingness, multimodal benefits rapidly degrade unless models are explicitly designed to handle incomplete inputs. Moreover, multimodal fusion does not inherently improve fairness, with subgroup disparities mainly arising from unequal sensitivity across demographic groups. To support reproducible and extensible evaluation, we further release a flexible benchmarking toolkit that enables plug-and-play integration of new models and datasets. Together, this work provides actionable guidance on when multimodal learning helps, when it fails, and why, laying the foundation for developing clinically deployable multimodal systems that are both effective and reliable. The open-source toolkit can be found at https://github.com/jakeykj/CareBench.", "AI": {"tldr": "This paper benchmarks multimodal fusion of EHR and chest X-rays for clinical prediction, finding that benefits are limited to diseases needing both data types, degrade under missing modalities, and don't automatically improve fairness.", "motivation": "Unclear when multimodal learning truly helps in clinical decision support, especially under modality missingness and fairness constraints.", "method": "Systematic benchmark of EHR-CXR fusion on MIMIC-IV/CXR cohorts to answer four questions about performance improvement, strategy comparison, missingness robustness, and algorithmic fairness.", "result": "Fusion helps only for diseases requiring complementary info; cross-modal learning captures dependencies but EHR temporal structure causes imbalance; benefits rapidly degrade under missingness without explicit design; fusion doesn't inherently improve fairness. Released CareBench toolkit.", "conclusion": "Provides actionable guidance on when multimodal learning helps/fails for developing clinically deployable systems."}}
{"id": "2602.23974", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23974", "abs": "https://arxiv.org/abs/2602.23974", "authors": ["Fan Zhang", "Baoru Huang", "Xin Zhang"], "title": "Pessimistic Auxiliary Policy for Offline Reinforcement Learning", "comment": null, "summary": "Offline reinforcement learning aims to learn an agent from pre-collected datasets, avoiding unsafe and inefficient real-time interaction. However, inevitable access to out-ofdistribution actions during the learning process introduces approximation errors, causing the error accumulation and considerable overestimation. In this paper, we construct a new pessimistic auxiliary policy for sampling reliable actions. Specifically, we develop a pessimistic auxiliary strategy by maximizing the lower confidence bound of the Q-function. The pessimistic auxiliary strategy exhibits a relatively high value and low uncertainty in the vicinity of the learned policy, avoiding the learned policy sampling high-value actions with potentially high errors during the learning process. Less approximation error introduced by sampled action from pessimistic auxiliary strategy leads to the alleviation of error accumulation. Extensive experiments on offline reinforcement learning benchmarks reveal that utilizing the pessimistic auxiliary strategy can effectively improve the efficacy of other offline RL approaches.", "AI": {"tldr": "\u9488\u5bf9\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u5206\u5e03\u5916\u52a8\u4f5c\u5bfc\u81f4\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u60b2\u89c2\u8f85\u52a9\u7b56\u7565\uff0c\u901a\u8fc7\u6700\u5927\u5316Q\u51fd\u6570\u7684\u4e0b\u7f6e\u4fe1\u754c\u6765\u91c7\u6837\u53ef\u9760\u52a8\u4f5c\uff0c\u6709\u6548\u63d0\u5347\u79bb\u7ebfRL\u6027\u80fd\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u867d\u907f\u514d\u4e86\u5b9e\u65f6\u4ea4\u4e92\u7684\u5b89\u5168\u98ce\u9669\uff0c\u4f46\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u4e0d\u53ef\u907f\u514d\u5730\u4f1a\u8bbf\u95ee\u5206\u5e03\u5916\u52a8\u4f5c\uff0c\u5f15\u5165\u8fd1\u4f3c\u8bef\u5dee\u5e76\u9020\u6210\u8bef\u5dee\u7d2f\u79ef\u4e0e\u8fc7\u5ea6\u4f30\u8ba1\uff0c\u9650\u5236\u4e86\u5b66\u4e60\u6548\u679c\u3002", "method": "\u6784\u5efa\u60b2\u89c2\u8f85\u52a9\u7b56\u7565\uff0c\u901a\u8fc7\u6700\u5927\u5316Q\u51fd\u6570\u7684\u4e0b\u7f6e\u4fe1\u754c\u6765\u9009\u62e9\u52a8\u4f5c\u3002\u8be5\u7b56\u7565\u5728\u5df2\u5b66\u4e60\u7b56\u7565\u9644\u8fd1\u8868\u73b0\u51fa\u9ad8\u503c\u4f4e\u4e0d\u786e\u5b9a\u6027\u7684\u7279\u70b9\uff0c\u907f\u514d\u91c7\u6837\u9ad8\u4ef7\u503c\u4f46\u6f5c\u5728\u9ad8\u8bef\u5dee\u7684\u52a8\u4f5c\u3002", "result": "\u5728\u591a\u4e2a\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u91c7\u7528\u8be5\u60b2\u89c2\u8f85\u52a9\u7b56\u7565\u80fd\u663e\u8457\u63d0\u5347\u5176\u4ed6\u79bb\u7ebfRL\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u51cf\u5c11\u8fd1\u4f3c\u8bef\u5dee\u548c\u7f13\u89e3\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u4e3a\u63d0\u5347\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23970", "categories": ["quant-ph", "math.FA", "math.PR"], "pdf": "https://arxiv.org/pdf/2602.23970", "abs": "https://arxiv.org/abs/2602.23970", "authors": ["Ce Wang"], "title": "Continuous-Time Quantum Walk on Locally Infinite Graph", "comment": "13 pages", "summary": "Time-reversal symmetry is of fundamental importance to physics. In the classical theory of time-reversal symmetry, the time-reversal symmetry of a quantum system is described by an anti-unitary operator, which is known as the time-reversal operator of the system. In this paper, we introduce and study a model of continuous-time quantum walk on a special locally infinite graph. After examining its spectral property, we investigate the time-reversal symmetry of the model. To our surprise, we find that its time-reversal symmetry can be described directly by a unitary operator, which contrasts sharply with that in the classical theory of time-reversal symmetry. Some other related results are also proven.", "AI": {"tldr": "\u7814\u7a76\u8fde\u7eed\u65f6\u95f4\u91cf\u5b50\u884c\u8d70\u6a21\u578b\u7684\u65f6\u95f4\u53cd\u6f14\u5bf9\u79f0\u6027\uff0c\u53d1\u73b0\u5176\u53ef\u7531\u9149\u7b97\u5b50\u800c\u975e\u7ecf\u5178\u7684\u53cd\u9149\u7b97\u5b50\u63cf\u8ff0", "motivation": "\u65f6\u95f4\u53cd\u6f14\u5bf9\u79f0\u6027\u662f\u7269\u7406\u5b66\u57fa\u672c\u6982\u5ff5\uff0c\u7ecf\u5178\u7406\u8bba\u4e2d\u91cf\u5b50\u7cfb\u7edf\u7684\u65f6\u95f4\u53cd\u6f14\u5bf9\u79f0\u6027\u7531\u53cd\u9149\u7b97\u5b50\u63cf\u8ff0\u3002\u672c\u8bba\u6587\u65e8\u5728\u63a2\u7d22\u7279\u6b8a\u5c40\u90e8\u65e0\u7a77\u56fe\u4e0a\u8fde\u7eed\u65f6\u95f4\u91cf\u5b50\u884c\u8d70\u6a21\u578b\u662f\u5426\u9075\u5faa\u8fd9\u4e00\u7ecf\u5178\u7406\u8bba\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u7279\u6b8a\u5c40\u90e8\u65e0\u7a77\u56fe\u4e0a\u7684\u8fde\u7eed\u65f6\u95f4\u91cf\u5b50\u884c\u8d70\u6a21\u578b\uff0c\u5206\u6790\u5176\u8c31\u6027\u8d28\uff0c\u5e76\u7814\u7a76\u8be5\u6a21\u578b\u7684\u65f6\u95f4\u53cd\u6f14\u5bf9\u79f0\u6027\u3002", "result": "\u53d1\u73b0\u8be5\u6a21\u578b\u7684\u65f6\u95f4\u53cd\u6f14\u5bf9\u79f0\u6027\u53ef\u76f4\u63a5\u7528\u9149\u7b97\u5b50\u63cf\u8ff0\uff0c\u4e0e\u7ecf\u5178\u7406\u8bba\u4e2d\u7684\u53cd\u9149\u7b97\u5b50\u5f62\u6210\u9c9c\u660e\u5bf9\u6bd4\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u4ed6\u76f8\u5173\u7ed3\u8bba\u3002", "conclusion": "\u7279\u5b9a\u91cf\u5b50\u884c\u8d70\u6a21\u578b\u7684\u65f6\u95f4\u53cd\u6f14\u5bf9\u79f0\u6027\u53ef\u7531\u9149\u7b97\u5b50\u63cf\u8ff0\uff0c\u8fd9\u4e00\u53cd\u76f4\u89c9\u7ed3\u679c\u63ed\u793a\u4e86\u65f6\u95f4\u53cd\u6f14\u5bf9\u79f0\u6027\u5728\u91cf\u5b50\u884c\u8d70\u7cfb\u7edf\u4e2d\u7684\u65b0\u8868\u73b0\u5f62\u5f0f\uff0c\u62d3\u5c55\u4e86\u5bf9\u79f0\u6027\u7406\u8bba\u7684\u9002\u7528\u8303\u56f4\u3002"}}
{"id": "2602.24037", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24037", "abs": "https://arxiv.org/abs/2602.24037", "authors": ["Vanya Priscillia Bendatu", "Yao Lu"], "title": "Portfolio Reinforcement Learning with Scenario-Context Rollout", "comment": null, "summary": "Market regime shifts induce distribution shifts that can degrade the performance of portfolio rebalancing policies. We propose macro-conditioned scenario-context rollout (SCR) that generates plausible next-day multivariate return scenarios under stress events. However, doing so faces new challenges, as history will never tell what would have happened differently. As a result, incorporating scenario-based rewards from rollouts introduces a reward--transition mismatch in temporal-difference learning, destabilizing RL critic training.\n  We analyze this inconsistency and show it leads to a mixed evaluation target. Guided by this analysis, we construct a counterfactual next state using the rollout-implied continuations and augment the critic agent's bootstrap target. Doing so stabilizes the learning and provides a viable bias-variance tradeoff.\n  In out-of-sample evaluations across 31 distinct universes of U.S. equity and ETF portfolios, our method improves Sharpe ratio by up to 76% and reduces maximum drawdown by up to 53% compared with classic and RL-based portfolio rebalancing baselines.", "AI": {"tldr": "A macro-conditioned scenario rollout method with counterfactual state augmentation stabilizes RL for portfolio rebalancing, improving Sharpe ratio by up to 76% and reducing drawdown by up to 53%.", "motivation": "Market regime shifts cause distribution shifts that degrade portfolio rebalancing performance, and incorporating scenario-based rewards from rollouts creates a reward-transition mismatch that destabilizes RL critic training.", "method": "Propose macro-conditioned scenario-context rollout (SCR) to generate stress-event scenarios, analyze the reward-transition mismatch inconsistency, construct counterfactual next states using rollout-implied continuations, and augment critic bootstrap targets to stabilize learning.", "result": "Out-of-sample testing across 31 U.S. equity/ETF portfolio universes shows up to 76% Sharpe ratio improvement and 53% maximum drawdown reduction compared to classic and RL-based baselines.", "conclusion": "The counterfactual augmentation approach stabilizes RL training while providing an effective bias-variance tradeoff, delivering significant performance gains for portfolio rebalancing under market stress."}}
{"id": "2602.23975", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23975", "abs": "https://arxiv.org/abs/2602.23975", "authors": ["Madan Mohan Mahana", "Gunjan Yadav", "Tarak Nath Dey"], "title": "Coherent Control of Population and Quantum Coherence in Superconducting Circuits", "comment": null, "summary": "Quantum mechanics, with its counterintuitive principles and probabilistic nature, has long been confined to the microscopic realm of atoms and photons. Yet, recent breakthroughs have pushed the boundaries of quantum behavior into the macroscopic world, where objects are visible to the naked eye and governed by classical physics. This review article traces the extraordinary progress toward achieving coherent control of population distributions among multiple quantum levels, as well as manipulation of absorption and refractive index, in such large-scale quantum systems, a feat once considered beyond reach.", "AI": {"tldr": "\u5b8f\u89c2\u91cf\u5b50\u7cfb\u7edf\u7814\u7a76\u8fdb\u5c55", "motivation": "\u7a81\u7834\u91cf\u5b50\u529b\u5b66\u4ec5\u9650\u5fae\u89c2\u9886\u57df\u7684\u4f20\u7edf\u8ba4\u77e5\uff0c\u63a2\u7d22\u5b8f\u89c2\u5c3a\u5ea6\u4e0b\u91cf\u5b50\u76f8\u5e72\u63a7\u5236\u7684\u53ef\u884c\u6027", "method": "\u7efc\u8ff0\u8fd1\u5e74\u6765\u5728\u5927\u578b\u91cf\u5b50\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u7c92\u5b50\u6570\u5206\u5e03\u76f8\u5e72\u8c03\u63a7\u53ca\u5438\u6536/\u6298\u5c04\u7387\u64cd\u63a7\u7684\u7814\u7a76\u8fdb\u5c55", "result": "\u8bc1\u5b9e\u4e86\u5b8f\u89c2\u53ef\u89c1\u7269\u4f53\u4e2d\u5b9e\u73b0\u591a\u80fd\u7ea7\u91cf\u5b50\u6001\u76f8\u5e72\u63a7\u5236\u53ca\u5149\u5b66\u6027\u8d28\u8c03\u63a7\u7684\u6280\u672f\u7a81\u7834", "conclusion": "\u5b8f\u89c2\u91cf\u5b50\u7cfb\u7edf\u64cd\u63a7\u5df2\u4ece\u7406\u8bba\u8bbe\u60f3\u8fdb\u5165\u5b9e\u9a8c\u5b9e\u73b0\u9636\u6bb5\uff0c\u4e3a\u91cf\u5b50\u6280\u672f\u5411\u5b8f\u89c2\u5c3a\u5ea6\u62d3\u5c55\u5960\u5b9a\u57fa\u7840"}}
{"id": "2602.23633", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23633", "abs": "https://arxiv.org/abs/2602.23633", "authors": ["Yubo Zhou", "Luo Luo", "Guang Dai", "Haishan Ye"], "title": "On the Convergence of Single-Loop Stochastic Bilevel Optimization with Approximate Implicit Differentiation", "comment": null, "summary": "Stochastic Bilevel Optimization has emerged as a fundamental framework for meta-learning and hyperparameter optimization. Despite the practical prevalence of single-loop algorithms--which update lower and upper variables concurrently--their theoretical understanding, particularly in the stochastic regime, remains significantly underdeveloped compared to their multi-loop counterparts. Existing analyses often yield suboptimal convergence rates or obscure the critical dependence on the lower-level condition number $\u03ba$, frequently burying it within generic Lipschitz constants. In this paper, we bridge this gap by providing a refined convergence analysis of the Single-loop Stochastic Approximate Implicit Differentiation (SSAID) algorithm. We prove that SSAID achieves an $\u03b5$-stationary point with an oracle complexity of $\\mathcal{O}(\u03ba^7 \u03b5^{-2})$. Our result is noteworthy in two aspects: (i) it matches the optimal $\\mathcal{O}(\u03b5^{-2})$ rate of state-of-the-art multi-loop methods (e.g., stocBiO) while maintaining the computational efficiency of a single-loop update; and (ii) it provides the first explicit, fine-grained characterization of the $\u03ba$-dependence for stochastic AID-based single-loop methods. This work demonstrates that SSAID is not merely a heuristic approach, but admits a rigorous theoretical foundation with convergence guarantees competitive with mainstream multi-loop frameworks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u968f\u673a\u53cc\u5c42\u4f18\u5316\u4e2d\u5355\u5faa\u73af\u7b97\u6cd5\u7406\u8bba\u5206\u6790\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u6539\u8fdb\u4e86SSAID\u7b97\u6cd5\u7684\u6536\u655b\u6027\u5206\u6790\uff0c\u8bc1\u660e\u5176\u8fbe\u5230O(\u03ba\u2077\u03b5\u207b\u00b2)\u7684oracle\u590d\u6742\u5ea6\uff0c\u9996\u6b21\u663e\u5f0f\u523b\u753b\u4e86\u6761\u4ef6\u6570\u03ba\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u4e14\u6536\u655b\u901f\u7387\u4e0e\u591a\u5faa\u73af\u65b9\u6cd5\u6700\u4f18\u6c34\u5e73\u4e00\u81f4\u3002", "motivation": "\u968f\u673a\u53cc\u5c42\u4f18\u5316\u662f\u5143\u5b66\u4e60\u548c\u8d85\u53c2\u6570\u4f18\u5316\u7684\u57fa\u7840\u6846\u67b6\uff0c\u4f46\u5355\u5faa\u73af\u7b97\u6cd5\uff08\u540c\u65f6\u66f4\u65b0\u4e0a\u4e0b\u5c42\u53d8\u91cf\uff09\u5728\u968f\u673a\u573a\u666f\u4e0b\u7684\u7406\u8bba\u7406\u89e3\u8fdc\u843d\u540e\u4e8e\u591a\u5faa\u73af\u7b97\u6cd5\uff0c\u73b0\u6709\u5206\u6790\u5b58\u5728\u6536\u655b\u901f\u7387\u6b21\u4f18\u6216\u9690\u85cf\u5173\u952e\u6761\u4ef6\u6570\u03ba\u4f9d\u8d56\u7684\u95ee\u9898\u3002", "method": "\u5bf9\u5355\u5faa\u73af\u968f\u673a\u8fd1\u4f3c\u9690\u5f0f\u5fae\u5206\u7b97\u6cd5\uff08SSAID\uff09\u8fdb\u884c\u7cbe\u7ec6\u5316\u6536\u655b\u5206\u6790\uff0c\u91cd\u70b9\u63ed\u793a\u4e0b\u5c42\u6761\u4ef6\u6570\u03ba\u5bf9\u6536\u655b\u901f\u7387\u7684\u663e\u5f0f\u5f71\u54cd\u3002", "result": "\u8bc1\u660eSSAID\u4ee5O(\u03ba\u2077\u03b5\u207b\u00b2)\u7684oracle\u590d\u6742\u5ea6\u8fbe\u5230\u03b5-\u9a7b\u70b9\uff0c\u8be5\u7ed3\u679c\uff08i\uff09\u4e0e\u591a\u5faa\u73af\u65b9\u6cd5\uff08\u5982stocBiO\uff09\u7684\u6700\u4f18O(\u03b5\u207b\u00b2)\u901f\u7387\u4e00\u81f4\uff1b\uff08ii\uff09\u9996\u6b21\u7ed9\u51fa\u968f\u673aAID\u5355\u5faa\u73af\u65b9\u6cd5\u4e2d\u03ba\u4f9d\u8d56\u7684\u663e\u5f0f\u523b\u753b\u3002", "conclusion": "SSAID\u4e0d\u4ec5\u5177\u6709\u8ba1\u7b97\u6548\u7387\u4f18\u52bf\uff0c\u66f4\u5177\u5907\u4e0e\u4e3b\u6d41\u591a\u5faa\u73af\u6846\u67b6\u76f8\u7ade\u4e89\u7684\u4e25\u683c\u7406\u8bba\u4fdd\u8bc1\uff0c\u786e\u7acb\u4e86\u5355\u5faa\u73af\u65b9\u6cd5\u5728\u968f\u673a\u53cc\u5c42\u4f18\u5316\u4e2d\u7684\u7406\u8bba\u5408\u7406\u6027\u3002"}}
{"id": "2602.24055", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.24055", "abs": "https://arxiv.org/abs/2602.24055", "authors": ["Reva Schwartz", "Carina Westling", "Morgan Briggs", "Marzieh Fadaee", "Isar Nejadgholi", "Matthew Holmes", "Fariza Rashid", "Maya Carlyle", "Afaf Ta\u00efk", "Kyra Wilson", "Peter Douglas", "Theodora Skeadas", "Gabriella Waters", "Rumman Chowdhury", "Thiago Lacerda"], "title": "CIRCLE: A Framework for Evaluating AI from a Real-World Lens", "comment": "Accepted at Intelligent Systems Conference (IntelliSys) 2026", "summary": "This paper proposes CIRCLE, a six-stage, lifecycle-based framework to bridge the reality gap between model-centric performance metrics and AI's materialized outcomes in deployment. While existing frameworks like MLOps focus on system stability and benchmarks measure abstract capabilities, decision-makers outside the AI stack lack systematic evidence about the behavior of AI technologies under real-world user variability and constraints. CIRCLE operationalizes the Validation phase of TEVV (Test, Evaluation, Verification, and Validation) by formalizing the translation of stakeholder concerns outside the stack into measurable signals. Unlike participatory design, which often remains localized, or algorithmic audits, which are often retrospective, CIRCLE provides a structured, prospective protocol for linking context-sensitive qualitative insights to scalable quantitative metrics. By integrating methods such as field testing, red teaming, and longitudinal studies into a coordinated pipeline, CIRCLE produces systematic knowledge: evidence that is comparable across sites yet sensitive to local context. This can enable governance based on materialized downstream effects rather than theoretical capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCIRCLE\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u516d\u9636\u6bb5\u751f\u547d\u5468\u671f\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u73b0\u573a\u6d4b\u8bd5\u3001\u7ea2\u961f\u6d4b\u8bd5\u548c\u7eb5\u5411\u7814\u7a76\u7b49\u65b9\u6cd5\uff0c\u5c06\u5229\u76ca\u76f8\u5173\u8005\u5173\u6ce8\u70b9\u8f6c\u5316\u4e3a\u53ef\u6d4b\u91cf\u4fe1\u53f7\uff0c\u4ece\u800c\u5f25\u5408\u6a21\u578b\u4e2d\u5fc3\u6027\u80fd\u6307\u6807\u4e0eAI\u5b9e\u9645\u90e8\u7f72\u6548\u679c\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u6cbb\u7406\u63d0\u4f9b\u53ef\u6bd4\u8f83\u4e14\u7b26\u5408\u672c\u5730\u8bed\u5883\u7684\u8bc1\u636e\u3002", "motivation": "\u73b0\u6709AI\u6846\u67b6\uff08\u5982MLOps\uff09\u5173\u6ce8\u7cfb\u7edf\u7a33\u5b9a\u6027\uff0c\u57fa\u51c6\u6d4b\u8bd5\u8861\u91cf\u62bd\u8c61\u80fd\u529b\uff0c\u4f46AI\u6808\u5916\u7684\u51b3\u7b56\u8005\u7f3a\u4e4f\u5173\u4e8eAI\u5728\u771f\u5b9e\u4e16\u754c\u7528\u6237\u53d8\u5316\u548c\u7ea6\u675f\u4e0b\u884c\u4e3a\u7684\u7cfb\u7edf\u6027\u8bc1\u636e\uff0c\u9700\u8981\u7ed3\u6784\u5316\u3001\u524d\u77bb\u6027\u7684\u534f\u8bae\u5c06\u5b9a\u6027\u6d1e\u5bdf\u4e0e\u5b9a\u91cf\u6307\u6807\u8054\u7cfb\u8d77\u6765\u3002", "method": "CIRCLE\u662f\u4e00\u4e2a\u516d\u9636\u6bb5\u751f\u547d\u5468\u671f\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5229\u76ca\u76f8\u5173\u8005\u5173\u6ce8\u70b9\u5f62\u5f0f\u5316\u8f6c\u5316\u4e3a\u53ef\u6d4b\u91cf\u4fe1\u53f7\uff0c\u5b9e\u65bdTEVV\uff08\u6d4b\u8bd5\u3001\u8bc4\u4f30\u3001\u9a8c\u8bc1\u548c\u786e\u8ba4\uff09\u7684\u9a8c\u8bc1\u9636\u6bb5\u3002\u5b83\u6574\u5408\u4e86\u73b0\u573a\u6d4b\u8bd5\u3001\u7ea2\u961f\u6d4b\u8bd5\u548c\u7eb5\u5411\u7814\u7a76\u65b9\u6cd5\uff0c\u5f62\u6210\u534f\u8c03\u4e00\u81f4\u7684\u6d41\u7a0b\u3002", "result": "\u8be5\u6846\u67b6\u4ea7\u751f\u7cfb\u7edf\u6027\u77e5\u8bc6\uff0c\u5176\u8bc1\u636e\u5728\u4e0d\u540c\u90e8\u7f72\u5730\u70b9\u95f4\u5177\u6709\u53ef\u6bd4\u6027\uff0c\u540c\u65f6\u5bf9\u672c\u5730\u8bed\u5883\u4fdd\u6301\u654f\u611f\uff0c\u4f7f\u6cbb\u7406\u80fd\u591f\u57fa\u4e8e\u5b9e\u9645\u4e0b\u6e38\u5f71\u54cd\u800c\u975e\u7406\u8bba\u80fd\u529b\u8fdb\u884c\u51b3\u7b56\u3002", "conclusion": "CIRCLE\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u3001\u524d\u77bb\u6027\u7684\u534f\u8bae\uff0c\u53ef\u5f25\u5408\u6a21\u578b\u4e2d\u5fc3\u6307\u6807\u4e0e\u5b9e\u9645\u90e8\u7f72\u7ed3\u679c\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4ece\u800c\u652f\u6301\u57fa\u4e8eAI\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u5b9e\u9645\u5f71\u54cd\u7684\u6cbb\u7406\u65b9\u5f0f\u3002"}}
{"id": "2602.23976", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23976", "abs": "https://arxiv.org/abs/2602.23976", "authors": ["Alejandro Gomez Cadavid", "Ananth Kaushik", "Pranav Chandarana", "Miguel Angel Lopez-Ruiz", "Gaurav Dev", "Willie Aboumrad", "Qi Zhang", "Claudio Girotto", "Sebasti\u00e1n V. Romero", "Martin Roetteler", "Enrique Solano", "Marco Pistoia", "Narendra N. Hegade"], "title": "Large-scale portfolio optimization on a trapped-ion quantum computer", "comment": "10 pages, 6 figures", "summary": "We present an end-to-end pipeline for large-scale portfolio selection with cardinality constraints and experimentally demonstrate it on trapped-ion quantum processors using hardware-aware decomposition. Building on RMT-based correlation-matrix denoising and community detection, we identify correlated asset groups and introduce a correlation-guided greedy splitting scheme that caps each cluster by the executable qubit budget. Each cluster defines a hardware-embeddable QUBO subproblem that we solve using bias-field digitized counterdiabatic quantum optimization (BF-DCQO), a non-variational method that avoids classical parameter-training loops. We recombine low-energy candidates into global portfolios and enforce feasibility with a two-stage post-processing routine: fast repair followed by a cardinality-preserving swap local search. We benchmark the workflow on a 250-asset universe taken from the S&P 500 and execute subproblems on a 64-qubit Barium development system similar to the forthcoming IonQ Tempo line. We observe that larger executable subproblem sizes reduce decomposition error and systematically improve final objective values and risk-return trade-offs relative to randomized baselines under identical post-processing. Overall, the results establish a hardware-tested route for scaling financial optimization problems, defined by a trade space in which executable problem size and circuit cost are balanced against the resulting solution quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u786c\u4ef6\u611f\u77e5\u5206\u89e3\u7684\u91cf\u5b50\u8ba1\u7b97\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u89c4\u6a21\u5e26\u57fa\u6570\u7ea6\u675f\u7684\u6295\u8d44\u7ec4\u5408\u9009\u62e9\u95ee\u9898\uff0c\u901a\u8fc7\u76f8\u5173\u6027\u5f15\u5bfc\u7684\u805a\u7c7b\u548cBF-DCQO\u7b97\u6cd5\u572864\u91cf\u5b50\u6bd4\u7279trapped-ion\u5904\u7406\u5668\u4e0a\u5b9e\u73b0\u4e86\u4f18\u4e8e\u968f\u673a\u57fa\u7ebf\u7684\u98ce\u9669\u6536\u76ca\u6743\u8861", "motivation": "\u5927\u89c4\u6a21\u5e26\u57fa\u6570\u7ea6\u675f\u7684\u6295\u8d44\u7ec4\u5408\u4f18\u5316\u662f\u8ba1\u7b97\u96be\u9898\uff0c\u7ecf\u5178\u65b9\u6cd5\u6269\u5c55\u6027\u6709\u9650\u3002\u91cf\u5b50\u8ba1\u7b97\u867d\u5177\u6f5c\u529b\uff0c\u4f46\u9700\u9002\u914d\u786c\u4ef6\u91cf\u5b50\u6bd4\u7279\u9650\u5236\uff0c\u901a\u8fc7\u5206\u89e3\u65b9\u6cd5\u5b9e\u73b0\u89c4\u6a21\u5316\u6c42\u89e3", "method": "\u7aef\u5230\u7aef\u6d41\u7a0b\uff1aRMT\u53bb\u566a\u4e0e\u793e\u533a\u68c0\u6d4b\u8bc6\u522b\u8d44\u4ea7\u7ec4\u2192\u76f8\u5173\u6027\u5f15\u5bfc\u8d2a\u5fc3\u5206\u5272\u63a7\u5236\u96c6\u7fa4\u89c4\u6a21\u2192BF-DCQO\u91cf\u5b50\u7b97\u6cd5\u6c42\u89e3QUBO\u5b50\u95ee\u9898\u2192\u4e24\u9636\u6bb5\u540e\u5904\u7406\uff08\u5feb\u901f\u4fee\u590d+\u57fa\u6570\u4fdd\u6301\u4ea4\u6362\u641c\u7d22\uff09\u3002\u5728250\u4e2aS&P 500\u8d44\u4ea7\u4e0a\u5b9e\u9a8c\uff0c64\u91cf\u5b50\u6bd4\u7279Barium\u7cfb\u7edf\u6267\u884c", "result": "\u66f4\u5927\u53ef\u6267\u884c\u5b50\u95ee\u9898\u89c4\u6a21\u964d\u4f4e\u5206\u89e3\u8bef\u5dee\uff0c\u7cfb\u7edf\u6027\u63d0\u5347\u76ee\u6807\u51fd\u6570\u503c\u548c\u98ce\u9669\u6536\u76ca\u6bd4\uff0c\u663e\u8457\u4f18\u4e8e\u76f8\u540c\u540e\u5904\u7406\u4e0b\u7684\u968f\u673a\u57fa\u7ebf", "conclusion": "\u5efa\u7acb\u4e86\u786c\u4ef6\u9a8c\u8bc1\u7684\u91cf\u5b50\u91d1\u878d\u4f18\u5316\u6269\u5c55\u8def\u5f84\uff0c\u63ed\u793a\u4e86\u95ee\u9898\u89c4\u6a21\u3001\u7535\u8def\u6210\u672c\u4e0e\u89e3\u8d28\u91cf\u4e4b\u95f4\u7684\u6838\u5fc3\u6743\u8861\u5173\u7cfb"}}
{"id": "2602.23636", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23636", "abs": "https://arxiv.org/abs/2602.23636", "authors": ["Zhihao Ding", "Jinming Li", "Ze Lu", "Jieming Shi"], "title": "FlexGuard: Continuous Risk Scoring for Strictness-Adaptive LLM Content Moderation", "comment": null, "summary": "Ensuring the safety of LLM-generated content is essential for real-world deployment. Most existing guardrail models formulate moderation as a fixed binary classification task, implicitly assuming a fixed definition of harmfulness. In practice, enforcement strictness - how conservatively harmfulness is defined and enforced - varies across platforms and evolves over time, making binary moderators brittle under shifting requirements. We first introduce FlexBench, a strictness-adaptive LLM moderation benchmark that enables controlled evaluation under multiple strictness regimes. Experiments on FlexBench reveal substantial cross-strictness inconsistency in existing moderators: models that perform well under one regime can degrade substantially under others, limiting their practical usability. To address this, we propose FlexGuard, an LLM-based moderator that outputs a calibrated continuous risk score reflecting risk severity and supports strictness-specific decisions via thresholding. We train FlexGuard via risk-alignment optimization to improve score-severity consistency and provide practical threshold selection strategies to adapt to target strictness at deployment. Experiments on FlexBench and public benchmarks demonstrate that FlexGuard achieves higher moderation accuracy and substantially improved robustness under varying strictness. We release the source code and data to support reproducibility.", "AI": {"tldr": "The paper introduces FlexBench, a benchmark for evaluating LLM moderators across varying safety strictness levels, and proposes FlexGuard, a continuous risk score-based moderator that adapts to different strictness requirements via thresholding, demonstrating superior robustness over binary classification approaches.", "motivation": "Existing LLM moderation models use fixed binary classification assuming static harmfulness definitions, but real-world deployment requires adapting to varying enforcement strictness across platforms and over time, making binary moderators brittle under shifting requirements.", "method": "They created FlexBench for controlled multi-strictness evaluation and developed FlexGuard that outputs calibrated continuous risk scores, trained via risk-alignment optimization with practical threshold selection strategies for strictness adaptation.", "result": "Experiments revealed substantial cross-strictness inconsistency in existing moderators. FlexGuard achieved higher moderation accuracy and substantially improved robustness under varying strictness regimes on both FlexBench and public benchmarks.", "conclusion": "Continuous risk score-based moderation with strictness-adaptive thresholding is more practical and robust than binary classification for real-world LLM safety applications."}}
{"id": "2602.24080", "categories": ["cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.24080", "abs": "https://arxiv.org/abs/2602.24080", "authors": ["Xiang Li", "Jiabao Gao", "Sipei Lin", "Xuan Zhou", "Chi Zhang", "Bo Cheng", "Jiale Han", "Benyou Wang"], "title": "Human or Machine? A Preliminary Turing Test for Speech-to-Speech Interaction", "comment": "Accepted by ICLR 2026 Conference", "summary": "The pursuit of human-like conversational agents has long been guided by the Turing test. For modern speech-to-speech (S2S) systems, a critical yet unanswered question is whether they can converse like humans. To tackle this, we conduct the first Turing test for S2S systems, collecting 2,968 human judgments on dialogues between 9 state-of-the-art S2S systems and 28 human participants. Our results deliver a clear finding: no existing evaluated S2S system passes the test, revealing a significant gap in human-likeness. To diagnose this failure, we develop a fine-grained taxonomy of 18 human-likeness dimensions and crowd-annotate our collected dialogues accordingly. Our analysis shows that the bottleneck is not semantic understanding but stems from paralinguistic features, emotional expressivity, and conversational persona. Furthermore, we find that off-the-shelf AI models perform unreliably as Turing test judges. In response, we propose an interpretable model that leverages the fine-grained human-likeness ratings and delivers accurate and transparent human-vs-machine discrimination, offering a powerful tool for automatic human-likeness evaluation. Our work establishes the first human-likeness evaluation for S2S systems and moves beyond binary outcomes to enable detailed diagnostic insights, paving the way for human-like improvements in conversational AI systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5bf9\u8bed\u97f3\u5230\u8bed\u97f3(S2S)\u7cfb\u7edf\u8fdb\u884c\u4e86\u56fe\u7075\u6d4b\u8bd5\uff0c\u53d1\u73b0\u73b0\u6709\u7cfb\u7edf\u5747\u672a\u901a\u8fc7\u6d4b\u8bd5\uff0c\u4e3b\u8981\u5dee\u8ddd\u5728\u4e8e\u526f\u8bed\u8a00\u7279\u5f81\u3001\u60c5\u611f\u8868\u8fbe\u548c\u5bf9\u8bdd\u4e2a\u6027\uff0c\u800c\u975e\u8bed\u4e49\u7406\u89e3\u3002\u7814\u7a76\u8fd8\u63d0\u51fa\u4e86\u53ef\u89e3\u91ca\u7684\u81ea\u52a8\u8bc4\u4f30\u6a21\u578b\u3002", "motivation": "\u73b0\u4ee3\u8bed\u97f3\u5230\u8bed\u97f3\u7cfb\u7edf\u80fd\u5426\u50cf\u4eba\u7c7b\u4e00\u6837\u5bf9\u8bdd\u4ecd\u662f\u672a\u89e3\u4e4b\u8c1c\u3002\u4f20\u7edf\u56fe\u7075\u6d4b\u8bd5\u4e3a\u8bc4\u4f30\u4eba\u673a\u5bf9\u8bdd\u76f8\u4f3c\u6027\u63d0\u4f9b\u4e86\u6846\u67b6\uff0c\u4f46\u6b64\u524d\u5c1a\u672a\u6709\u9488\u5bf9S2S\u7cfb\u7edf\u7684\u7cfb\u7edf\u6027\u6d4b\u8bd5\u3002", "method": "\u6536\u96c62,968\u6761\u4eba\u7c7b\u5bf99\u4e2a\u5148\u8fdbS2S\u7cfb\u7edf\u548c28\u540d\u4eba\u7c7b\u53c2\u4e0e\u8005\u5bf9\u8bdd\u7684\u8bc4\u5224\uff1b\u5efa\u7acb\u5305\u542b18\u4e2a\u7ef4\u5ea6\u7684\u7ec6\u7c92\u5ea6\u4eba\u7c7b\u76f8\u4f3c\u6027\u5206\u7c7b\u4f53\u7cfb\u5e76\u8fdb\u884c\u4f17\u5305\u6807\u6ce8\uff1b\u5f00\u53d1\u53ef\u89e3\u91ca\u6a21\u578b\u7528\u4e8e\u81ea\u52a8\u8bc4\u4f30\u3002", "result": "1) \u6240\u6709\u88ab\u8bc4\u4f30\u7684S2S\u7cfb\u7edf\u5747\u672a\u901a\u8fc7\u56fe\u7075\u6d4b\u8bd5\uff1b2) \u74f6\u9888\u5728\u4e8e\u526f\u8bed\u8a00\u7279\u5f81\u3001\u60c5\u611f\u8868\u8fbe\u548c\u5bf9\u8bdd\u4e2a\u6027\uff0c\u800c\u975e\u8bed\u4e49\u7406\u89e3\uff1b3) \u73b0\u6709AI\u6a21\u578b\u4f5c\u4e3a\u8bc4\u5224\u8005\u4e0d\u53ef\u9760\uff1b4) \u65b0\u6a21\u578b\u80fd\u51c6\u786e\u533a\u5206\u4eba\u673a\u5bf9\u8bdd\u3002", "conclusion": "\u5efa\u7acb\u4e86\u9996\u4e2aS2S\u7cfb\u7edf\u4eba\u7c7b\u76f8\u4f3c\u6027\u8bc4\u4f30\u57fa\u51c6\uff0c\u8d85\u8d8a\u4e86\u4e8c\u5143\u7ed3\u679c\u63d0\u4f9b\u8bca\u65ad\u6027\u6d1e\u5bdf\uff0c\u4e3a\u5f00\u53d1\u7c7b\u4eba\u5bf9\u8bddAI\u7cfb\u7edf\u6307\u660e\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2602.24003", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.24003", "abs": "https://arxiv.org/abs/2602.24003", "authors": ["Waqas Ahmad", "Gioele Consani", "Mohammad Tasnimul Haque", "Jacob Dunstan", "Brian Vlastakis"], "title": "3D Integrated Embedded Filters for Superconducting Quantum Circuits", "comment": null, "summary": "Microwave filtering for superconducting qubits is a key element of quantum computing technology, enabling high coherence and fast state detection. This work presents the design and implementation of novel microwave Purcell filters for superconducting quantum circuits, integrated within a multilayer printed circuit board (PCB). The off-chip design removes all filter components from the qubit substrate, reducing device complexity, improving layout footprint and allowing better scalability to large qubit counts. Each embedded filter can couple up to nine readout resonators, enabling efficient multiplexed readout. Electromagnetic simulations of the filter predict a thousand-fold improvement in qubit isolation from the readout port. The design was experimentally validated under cryogenic conditions in conjunction with a 35-qubit device, demonstrating compatibility of the PCB-based filter with high-coherence superconducting qubits. The comparison of the measured qubit median T1 of 84 $\u03bc$s with the expected radiative limit from electromagnetic simulations validated the presence of Purcell filtering in the system.", "AI": {"tldr": "This paper presents a novel off-chip microwave Purcell filter integrated into a multilayer PCB for superconducting qubits, which removes filters from the qubit substrate, improves scalability, and achieves 1000x better qubit isolation while maintaining high coherence (T1 of 84 \u03bcs) in a 35-qubit device.", "motivation": "Microwave filtering is crucial for superconducting qubits to achieve high coherence and fast state detection. Existing on-chip filters limit scalability and increase device complexity. There's a need for off-chip solutions that can support large-scale quantum processors with multiplexed readout capabilities.", "method": "The researchers designed and implemented an embedded microwave Purcell filter in a multilayer printed circuit board (PCB) that is separate from the qubit substrate. They used electromagnetic simulations to predict performance and experimentally validated the design under cryogenic conditions with a 35-qubit superconducting quantum processor.", "result": "The PCB-based filter successfully coupled up to nine readout resonators for multiplexed readout. Simulations predicted a thousand-fold improvement in qubit isolation. Experimental results showed a median qubit T1 coherence time of 84 \u03bcs, which matched the expected radiative limit, confirming effective Purcell filtering and compatibility with high-coherence qubits.", "conclusion": "The off-chip PCB integration approach successfully addresses scalability challenges in superconducting quantum computing by reducing qubit substrate complexity while maintaining high coherence through effective Purcell filtering, demonstrating a viable path toward large-scale quantum processors."}}
{"id": "2602.23638", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23638", "abs": "https://arxiv.org/abs/2602.23638", "authors": ["Haoran Zhang", "Dongjun Kim", "Seohyeon Cha", "Haris Vikalo"], "title": "FedRot-LoRA: Mitigating Rotational Misalignment in Federated LoRA", "comment": "preprint", "summary": "Federated LoRA provides a communication-efficient mechanism for fine-tuning large language models on decentralized data. In practice, however, a discrepancy between the factor-wise averaging used to preserve low rank and the mathematically correct aggregation of local updates can cause significant aggregation error and unstable training. We argue that a major source of this problem is rotational misalignment, arising from the rotational invariance of low-rank factorizations -- semantically equivalent updates can be represented in different latent subspaces across clients since $(B_i R_i)(R_i^\\top A_i) = B_i A_i$. When such misaligned factors are averaged directly, they interfere destructively and degrade the global update. To address this issue, we propose FedRot-LoRA, a federated LoRA framework that aligns client updates via orthogonal transformations prior to aggregation. This alignment preserves the semantic update while reducing cross-client subspace mismatch, without increasing communication cost or restricting model expressivity. We provide a convergence analysis that examines the aggregation error induced by factor-wise averaging and shows how rotational alignment yields a tighter upper bound on this error. Extensive experiments on natural language understanding and generative tasks demonstrate that FedRot-LoRA consistently outperforms existing federated LoRA baselines across a range of heterogeneity levels and LoRA ranks.", "AI": {"tldr": "FedRot-LoRA\u901a\u8fc7\u6b63\u4ea4\u53d8\u6362\u5bf9\u9f50\u5ba2\u6237\u7aef\u4f4e\u79e9\u66f4\u65b0\uff0c\u89e3\u51b3\u65cb\u8f6c\u4e0d\u53d8\u6027\u5bfc\u81f4\u7684\u805a\u5408\u8bef\u5dee\u95ee\u9898\uff0c\u63d0\u5347\u8054\u90a6\u5927\u6a21\u578b\u5fae\u8c03\u6027\u80fd\u3002", "motivation": "\u8054\u90a6LoRA\u5b58\u5728\u65cb\u8f6c\u5931\u914d\u95ee\u9898\uff1a\u56e0\u4f4e\u79e9\u5206\u89e3\u7684\u65cb\u8f6c\u4e0d\u53d8\u6027\uff0c\u4e0d\u540c\u5ba2\u6237\u7aef\u5bf9\u76f8\u540c\u8bed\u4e49\u66f4\u65b0\u7684\u8868\u793a\u53ef\u80fd\u5904\u4e8e\u4e0d\u540c\u5b50\u7a7a\u95f4\uff0c\u76f4\u63a5\u5e73\u5747\u4f1a\u5bfc\u81f4\u7834\u574f\u6027\u5e72\u6270\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002", "method": "\u5728\u805a\u5408\u524d\u4f7f\u7528\u6b63\u4ea4\u53d8\u6362\u5bf9\u9f50\u5ba2\u6237\u7aef\u66f4\u65b0\u56e0\u5b50\uff0c\u4fdd\u6301\u8bed\u4e49\u66f4\u65b0\u4e0d\u53d8\u7684\u540c\u65f6\u51cf\u5c11\u8de8\u5ba2\u6237\u7aef\u5b50\u7a7a\u95f4\u4e0d\u5339\u914d\uff0c\u4e0d\u589e\u52a0\u901a\u4fe1\u5f00\u9500\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u65cb\u8f6c\u5bf9\u9f50\u80fd\u6536\u7d27\u805a\u5408\u8bef\u5dee\u4e0a\u754c\uff1bNLU\u548c\u751f\u6210\u4efb\u52a1\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFedRot-LoRA\u5728\u5404\u79cd\u5f02\u6784\u7a0b\u5ea6\u548cLoRA\u79e9\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "FedRot-LoRA\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6LoRA\u7684\u805a\u5408\u8bef\u5dee\u95ee\u9898\uff0c\u4e3a\u53bb\u4e2d\u5fc3\u5316\u6570\u636e\u4e0a\u7684\u9ad8\u6548\u5927\u6a21\u578b\u5fae\u8c03\u63d0\u4f9b\u4e86\u53ef\u9760\u65b9\u6848\u3002"}}
{"id": "2602.24097", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24097", "abs": "https://arxiv.org/abs/2602.24097", "authors": ["Yue Xie", "Zizhen Xu", "William Beazley", "Fumiya Iida"], "title": "Bi-level RL-Heuristic Optimization for Real-world Winter Road Maintenance", "comment": null, "summary": "Winter road maintenance is critical for ensuring public safety and reducing environmental impacts, yet existing methods struggle to manage large-scale routing problems effectively and mostly reply on human decision. This study presents a novel, scalable bi-level optimization framework, validated on real operational data on UK strategic road networks (M25, M6, A1), including interconnected local road networks in surrounding areas for vehicle traversing, as part of the highway operator's efforts to solve existing planning challenges. At the upper level, a reinforcement learning (RL) agent strategically partitions the road network into manageable clusters and optimally allocates resources from multiple depots. At the lower level, a multi-objective vehicle routing problem (VRP) is solved within each cluster, minimizing the maximum vehicle travel time and total carbon emissions. Unlike existing approaches, our method handles large-scale, real-world networks efficiently, explicitly incorporating vehicle-specific constraints, depot capacities, and road segment requirements. Results demonstrate significant improvements, including balanced workloads, reduced maximum travel times below the targeted two-hour threshold, lower emissions, and substantial cost savings. This study illustrates how advanced AI-driven bi-level optimization can directly enhance operational decision-making in real-world transportation and logistics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u591a\u76ee\u6807\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u7684\u53cc\u5c42\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u51ac\u5b63\u9053\u8def\u7ef4\u62a4\u8def\u7ebf\u89c4\u5212\uff0c\u5728\u82f1\u56fd\u771f\u5b9e\u8def\u7f51\u6570\u636e\u4e0a\u9a8c\u8bc1\u53ef\u5b9e\u73b0\u8d1f\u8f7d\u5747\u8861\u3001\u964d\u4f4e\u78b3\u6392\u653e\u548c\u6210\u672c\u8282\u7ea6\u3002", "motivation": "\u73b0\u6709\u51ac\u5b63\u9053\u8def\u7ef4\u62a4\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u5927\u89c4\u6a21\u8def\u5f84\u89c4\u5212\u95ee\u9898\u4e14\u8fc7\u5ea6\u4f9d\u8d56\u4eba\u5de5\u51b3\u7b56\uff0c\u65e0\u6cd5\u517c\u987e\u516c\u5171\u5b89\u5168\u548c\u73af\u5883\u5f71\u54cd\u7684\u5e73\u8861\u3002", "method": "\u4e0a\u5c42\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u5bf9\u8def\u7f51\u8fdb\u884c\u5206\u533a\u4e0e\u591a\u4ed3\u5e93\u8d44\u6e90\u5206\u914d\uff1b\u4e0b\u5c42\u5728\u6bcf\u4e2a\u96c6\u7fa4\u5185\u6c42\u89e3\u591a\u76ee\u6807\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u6700\u5927\u8f66\u8f86\u884c\u9a76\u65f6\u95f4\u548c\u603b\u78b3\u6392\u653e\u91cf\uff0c\u5e76\u7eb3\u5165\u8f66\u8f86\u7ea6\u675f\u3001\u4ed3\u5e93\u5bb9\u91cf\u548c\u8def\u6bb5\u9700\u6c42\u3002", "result": "\u5728M25\u3001M6\u3001A1\u7b49\u82f1\u56fd\u6218\u7565\u8def\u7f51\u9a8c\u8bc1\u663e\u793a\uff1a\u5de5\u4f5c\u8d1f\u8f7d\u663e\u8457\u5747\u8861\uff0c\u6700\u5927\u884c\u9a76\u65f6\u95f4\u964d\u81f32\u5c0f\u65f6\u9608\u503c\u4ee5\u4e0b\uff0c\u78b3\u6392\u653e\u964d\u4f4e\uff0c\u5e76\u5b9e\u73b0\u53ef\u89c2\u6210\u672c\u8282\u7ea6\u3002", "conclusion": "\u8be5AI\u9a71\u52a8\u7684\u53cc\u5c42\u4f18\u5316\u6846\u67b6\u4e3a\u5b9e\u9645\u4ea4\u901a\u7269\u6d41\u8fd0\u8425\u51b3\u7b56\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5148\u8fdb\u4f18\u5316\u6280\u672f\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.24048", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2602.24048", "abs": "https://arxiv.org/abs/2602.24048", "authors": ["Jo\u00e3o P. R. Leonel", "Paulo A. Brand\u00e3o"], "title": "Saturable nonlinearities in a driven-dissipative bosonic quantum battery", "comment": null, "summary": "We investigate the charging of a nonlinear quantum battery consisting of a single bosonic mode subject to a saturable nonlinearity, coherent driving, and dissipation. In contrast to Kerr-type anharmonicities, the saturable interaction induces a bounded and nonlinear distortion of the energy spectrum, leading to a progressive increase in the density of energy levels. We analyze the time evolution of the energy and ergotropy of the battery by solving a Lindblad master equation and show that the nonlinear spectral structure significantly affects both transient charging behavior and steady-state properties. Our results reveal that, for a broad range of parameters, the saturable nonlinearity enhances the maximum stored energy and modifies the ergotropy generation in the presence of losses. The interplay between dissipation and bounded spectral nonlinearity provides a controllable mechanism to tune energy storage and work extraction in bosonic quantum batteries.", "AI": {"tldr": "This paper studies how saturable nonlinearity affects quantum battery performance, finding it enhances maximum stored energy and provides controllable tuning of work extraction despite dissipation losses.", "motivation": "To understand how saturable nonlinearity (distinct from Kerr-type) impacts charging dynamics and ergotropy in a realistic quantum battery with dissipation.", "method": "Solving a Lindblad master equation to analyze time evolution of energy and ergotropy in a single bosonic mode quantum battery with saturable nonlinearity, coherent driving, and dissipation.", "result": "The saturable nonlinearity creates a bounded, nonlinear energy spectrum with increased level density, enhancing maximum stored energy and modifying ergotropy generation in the presence of losses. It significantly affects both transient charging behavior and steady-state properties across a broad parameter range.", "conclusion": "The interplay between dissipation and bounded spectral nonlinearity offers a controllable mechanism to tune energy storage and work extraction in bosonic quantum batteries, demonstrating the beneficial role of saturable nonlinearities for quantum energy storage design."}}
{"id": "2602.23662", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23662", "abs": "https://arxiv.org/abs/2602.23662", "authors": ["Kohei Obata", "Zheng Chen", "Yasuko Matsubara", "Lingwei Zhu", "Yasushi Sakurai"], "title": "Selective Denoising Diffusion Model for Time Series Anomaly Detection", "comment": null, "summary": "Time series anomaly detection (TSAD) has been an important area of research for decades, with reconstruction-based methods, mostly based on generative models, gaining popularity and demonstrating success. Diffusion models have recently attracted attention due to their advanced generative capabilities. Existing diffusion-based methods for TSAD rely on a conditional strategy, which reconstructs input instances from white noise with the aid of the conditioner. However, this poses challenges in accurately reconstructing the normal parts, resulting in suboptimal detection performance. In response, we propose a novel diffusion-based method, named AnomalyFilter, which acts as a selective filter that only denoises anomaly parts in the instance while retaining normal parts. To build such a filter, we mask Gaussian noise during the training phase and conduct the denoising process without adding noise to the instances. The synergy of the two simple components greatly enhances the performance of naive diffusion models. Extensive experiments on five datasets demonstrate that AnomalyFilter achieves notably low reconstruction error on normal parts, providing empirical support for its effectiveness in anomaly detection. AnomalyFilter represents a pioneering approach that focuses on the noise design of diffusion models specifically tailored for TSAD.", "AI": {"tldr": "\u63d0\u51faAnomalyFilter\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a9\u7801\u9ad8\u65af\u566a\u58f0\u8bad\u7ec3\u548c\u53bb\u566a\u8fc7\u7a0b\u4e0d\u6dfb\u52a0\u566a\u58f0\u7684\u521b\u65b0\u566a\u58f0\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4ec5\u5bf9\u5f02\u5e38\u90e8\u5206\u53bb\u566a\u5e76\u4fdd\u7559\u6b63\u5e38\u90e8\u5206\uff0c\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u964d\u4f4e\u4e86\u6b63\u5e38\u90e8\u5206\u7684\u91cd\u6784\u8bef\u5dee\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u91c7\u7528\u6761\u4ef6\u7b56\u7565\u4ece\u767d\u566a\u58f0\u91cd\u6784\u8f93\u5165\uff0c\u4f46\u5728\u51c6\u786e\u91cd\u6784\u6b63\u5e38\u90e8\u5206\u65f6\u9762\u4e34\u6311\u6218\uff0c\u5bfc\u81f4\u68c0\u6d4b\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51faAnomalyFilter\u4f5c\u4e3a\u9009\u62e9\u6027\u6ee4\u6ce2\u5668\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u9636\u6bb5\u63a9\u7801\u9ad8\u65af\u566a\u58f0\u5e76\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u4e0d\u5bf9\u5b9e\u4f8b\u6dfb\u52a0\u566a\u58f0\uff0c\u4ec5\u5bf9\u5b9e\u4f8b\u7684\u5f02\u5e38\u90e8\u5206\u8fdb\u884c\u53bb\u566a\u800c\u4fdd\u7559\u6b63\u5e38\u90e8\u5206\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cAnomalyFilter\u5728\u6b63\u5e38\u90e8\u5206\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u964d\u4f4e\u7684\u91cd\u6784\u8bef\u5dee\uff0c\u4e3a\u5176\u5f02\u5e38\u68c0\u6d4b\u6709\u6548\u6027\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u652f\u6301\u3002", "conclusion": "AnomalyFilter\u4ee3\u8868\u4e86\u9488\u5bf9TSAD\u4e13\u95e8\u5b9a\u5236\u7684\u6269\u6563\u6a21\u578b\u566a\u58f0\u8bbe\u8ba1\u7684\u5f00\u521b\u6027\u65b9\u6cd5\u3002"}}
{"id": "2602.24100", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24100", "abs": "https://arxiv.org/abs/2602.24100", "authors": ["Richard Csaky"], "title": "Artificial Agency Program: Curiosity, compression, and communication in agents", "comment": "This is a working draft. Feedback and criticism is most welcome", "summary": "This paper presents the Artificial Agency Program (AAP), a position and research agenda for building AI systems as reality embedded, resource-bounded agents whose development is driven by curiosity-as-learning-progress under physical and computational constraints. The central thesis is that AI is most useful when treated as part of an extended human--tool system that increases sensing, understanding, and actuation capability while reducing friction at the interface between people, tools, and environments. The agenda unifies predictive compression, intrinsic motivation, empowerment and control, interface quality (unification), and language/self-communication as selective information bottlenecks. We formulate these ideas as a falsifiable program with explicit costs, staged experiments, and a concrete multimodal tokenized testbed in which an agent allocates limited budget among observation, action, and deliberation. The aim is to provide a conceptual and experimental framework that connects intrinsic motivation, information theory, thermodynamics, bounded rationality, and modern reasoning systems", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4eba\u5de5\u667a\u80fd\u4f53\u8ba1\u5212\uff08AAP\uff09\uff0c\u4e3b\u5f20\u5c06AI\u6784\u5efa\u4e3a\u5d4c\u5165\u73b0\u5b9e\u3001\u8d44\u6e90\u53d7\u9650\u7684\u667a\u80fd\u4f53\uff0c\u5176\u53d1\u5c55\u7531\u597d\u5947\u5fc3\u9a71\u52a8\u7684\u5b66\u4e60\u8fdb\u5c55\u51b3\u5b9a\u3002\u6838\u5fc3\u8bba\u70b9\u662fAI\u4f5c\u4e3a\u4eba\u673a\u5de5\u5177\u7cfb\u7edf\u7684\u5ef6\u4f38\u6700\u6709\u7528\uff0c\u80fd\u589e\u5f3a\u611f\u77e5\u3001\u7406\u89e3\u4e0e\u6267\u884c\u80fd\u529b\u5e76\u51cf\u5c11\u4ea4\u4e92\u6469\u64e6\u3002\u8be5\u8ba1\u5212\u5c06\u9884\u6d4b\u538b\u7f29\u3001\u5185\u5728\u52a8\u673a\u3001\u6388\u6743\u63a7\u5236\u3001\u63a5\u53e3\u8d28\u91cf\u53ca\u8bed\u8a00/\u81ea\u6211\u6c9f\u901a\u7edf\u4e00\u4e3a\u9009\u62e9\u6027\u4fe1\u606f\u74f6\u9888\uff0c\u901a\u8fc7\u53ef\u8bc1\u4f2a\u7a0b\u5e8f\u3001\u660e\u786e\u6210\u672c\u3001\u5206\u9636\u6bb5\u5b9e\u9a8c\u548c\u591a\u6a21\u6001\u6807\u8bb0\u5316\u6d4b\u8bd5\u5e8a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u65e8\u5728\u5efa\u7acb\u8fde\u63a5\u5185\u5728\u52a8\u673a\u3001\u4fe1\u606f\u8bba\u3001\u70ed\u529b\u5b66\u3001\u6709\u9650\u7406\u6027\u4e0e\u73b0\u4ee3\u63a8\u7406\u7cfb\u7edf\u7684\u6982\u5ff5\u4e0e\u5b9e\u9a8c\u6846\u67b6\u3002", "motivation": "\u73b0\u6709AI\u7814\u7a76\u7f3a\u4e4f\u5bf9\u7269\u7406\u4e0e\u8ba1\u7b97\u7ea6\u675f\u4e0b\u5d4c\u5165\u5f0f\u667a\u80fd\u4f53\u7684\u7edf\u4e00\u8003\u91cf\uff0c\u4e14\u672a\u80fd\u5145\u5206\u5c06\u5176\u5b9a\u4f4d\u4e3a\u4eba\u673a\u534f\u4f5c\u7cfb\u7edf\u7684\u5ef6\u4f38\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6784\u5efa\u8d44\u6e90\u53d7\u9650\u3001\u597d\u5947\u5fc3\u9a71\u52a8\u7684\u667a\u80fd\u4f53\uff0c\u6700\u5927\u5316AI\u4f5c\u4e3a\u4eba\u7c7b\u8ba4\u77e5\u4e0e\u884c\u52a8\u5ef6\u4f38\u7684\u6548\u7528\uff0c\u51cf\u5c11\u4eba\u4e0e\u5de5\u5177\u53ca\u73af\u5883\u95f4\u7684\u4ea4\u4e92\u6469\u64e6\uff0c\u63d0\u5347\u6574\u4f53\u7cfb\u7edf\u7684\u611f\u77e5\u3001\u7406\u89e3\u548c\u6267\u884c\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u53ef\u8bc1\u4f2a\u7684\u7814\u7a76\u8ba1\u5212\uff0c\u5c06\u9884\u6d4b\u538b\u7f29\u3001\u5185\u5728\u52a8\u673a\u3001\u6388\u6743\u63a7\u5236\u3001\u63a5\u53e3\u8d28\u91cf\u3001\u8bed\u8a00/\u81ea\u6211\u6c9f\u901a\u7b49\u6982\u5ff5\u7edf\u4e00\u5efa\u6a21\u4e3a\u9009\u62e9\u6027\u4fe1\u606f\u74f6\u9888\uff1b\u8bbe\u5b9a\u660e\u786e\u6210\u672c\u51fd\u6570\uff1b\u89c4\u5212\u5206\u9636\u6bb5\u5b9e\u9a8c\uff1b\u5f00\u53d1\u591a\u6a21\u6001\u6807\u8bb0\u5316\u6d4b\u8bd5\u73af\u5883\uff0c\u5176\u4e2d\u667a\u80fd\u4f53\u9700\u5728\u89c2\u5bdf\u3001\u884c\u52a8\u4e0e deliberation \u95f4\u5206\u914d\u6709\u9650\u9884\u7b97\u3002", "result": "\u63d0\u51fa\u4e86AAP\u7684\u5b8c\u6574\u7814\u7a76\u8bae\u7a0b\uff0c\u5305\u62ec\u6838\u5fc3\u7406\u8bba\u6846\u67b6\u3001\u7edf\u4e00\u7684\u6982\u5ff5\u6a21\u578b\u3001\u53ef\u8bc1\u4f2a\u7684\u7814\u7a76\u8ba1\u5212\u3001\u5206\u9636\u6bb5\u5b9e\u9a8c\u65b9\u6848\u4ee5\u53ca\u5177\u4f53\u7684\u591a\u6a21\u6001\u6d4b\u8bd5\u5e8a\u8bbe\u8ba1\uff0c\u4e3a\u8fde\u63a5\u5185\u5728\u52a8\u673a\u3001\u4fe1\u606f\u8bba\u3001\u70ed\u529b\u5b66\u3001\u6709\u9650\u7406\u6027\u4e0e\u73b0\u4ee3\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u6982\u5ff5\u4e0e\u5b9e\u9a8c\u84dd\u56fe\u3002", "conclusion": "\u8be5\u7814\u7a76\u8ba1\u5212\u901a\u8fc7\u6574\u5408\u591a\u4e2aAI\u6838\u5fc3\u6982\u5ff5\u5e76\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u6846\u67b6\uff0c\u4e3a\u5f00\u53d1\u66f4\u5b9e\u7528\u3001\u9ad8\u6548\u7684\u5d4c\u5165\u5f0fAI\u667a\u80fd\u4f53\u6307\u660e\u4e86\u65b9\u5411\uff0c\u6709\u671b\u63a8\u52a8AI\u4ece\u5b64\u7acb\u7cfb\u7edf\u5411\u4eba\u7c7b\u8ba4\u77e5\u4e0e\u884c\u52a8\u7684\u5ef6\u4f38\u5de5\u5177\u8f6c\u53d8\uff0c\u5efa\u7acb\u8de8\u5b66\u79d1\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.24053", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.24053", "abs": "https://arxiv.org/abs/2602.24053", "authors": ["Viacheslav Dubovitskii", "Filippo Utro", "Aritra Bose", "Laxmi Parida", "Sabrina Maniscalco", "Sergey N. Filippov"], "title": "Experimental implementation of a discrete-time quantum walk on biological networks", "comment": "14 pages, 6 figures", "summary": "Quantum walks provide a versatile framework for probing the structural and dynamical properties of complex systems ranging from biological networks to synthetic materials. However, their realization on current noisy pre-fault-tolerant quantum computers is fundamentally limited by decoherence. Conventional dense encodings of graph structures require prohibitively deep circuits, making them incompatible with existing hardware. Here we introduce an algorithm that leverages symmetry-sector encoding and trades circuit depth for qubits, while integrating symmetry-respecting postselection as an effective noise-mitigation strategy. This combination enables us to execute practical quantum-walk circuits for biological networks on actual quantum hardware. We benchmark the proposed methodology against known state-of-the-art circuit architectures, highlighting significant reduction of circuit depth in our approach at the cost of moderate qubit overhead. Utilizing 40 qubits, we implement quantum walks on complex graphs containing up to 17 nodes and 20 edges -- the largest experiment on superconducting hardware to date, with the Hellinger fidelity exceeding 87% throughout 7 steps. We present a case study that illustrates how experimentally obtained quantum-walk dynamics on a protein-protein-interaction network can be applied to prioritizing disease-associated genes. We discuss the framework scalability in the pre-fault-tolerant era and its potential for studying larger biological networks.", "AI": {"tldr": "This paper introduces a symmetry-sector encoding algorithm with noise mitigation that enables quantum walk experiments on biological networks using current quantum hardware, achieving the largest such implementation to date.", "motivation": "Current noisy quantum computers cannot run conventional quantum walk algorithms due to decoherence and prohibitively deep circuits required for dense graph encodings.", "method": "Leverages symmetry-sector encoding to trade circuit depth for qubits, combined with symmetry-respecting postselection as a noise-mitigation strategy.", "result": "Executed quantum walks on graphs up to 17 nodes/20 edges using 40 qubits with 87% Hellinger fidelity over 7 steps - the largest superconducting quantum walk experiment. Demonstrated application in prioritizing disease-associated genes via protein interaction networks.", "conclusion": "The framework offers a scalable approach for near-term quantum study of biological networks, with potential for investigating larger networks in the pre-fault-tolerant quantum computing era."}}
{"id": "2602.23663", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23663", "abs": "https://arxiv.org/abs/2602.23663", "authors": ["Kohei Obata", "Taichi Murayama", "Zheng Chen", "Yasuko Matsubara", "Yasushi Sakurai"], "title": "Disentangled Mode-Specific Representations for Tensor Time Series via Contrastive Learning", "comment": null, "summary": "Multi-mode tensor time series (TTS) can be found in many domains, such as search engines and environmental monitoring systems. Learning representations of a TTS benefits various applications, but it is also challenging since the complexities inherent in the tensor hinder the realization of rich representations. In this paper, we propose a novel representation learning method designed specifically for TTS, namely MoST. Specifically, MoST uses a tensor slicing approach to reduce the complexity of the TTS structure and learns representations that can be disentangled into individual non-temporal modes. Each representation captures mode-specific features, which are the relationship between variables within the same mode, and mode-invariant features, which are in common in representations of different modes. We employ a contrastive learning framework to learn parameters; the loss function comprises two parts intended to learn representation in a mode-specific way and mode-invariant way, effectively exploiting disentangled representations as augmentations. Extensive experiments on real-world datasets show that MoST consistently outperforms the state-of-the-art methods in terms of classification and forecasting accuracy. Code is available at https://github.com/KoheiObata/MoST.", "AI": {"tldr": "MoST\u63d0\u51fa\u5f20\u91cf\u5207\u7247\u4e0e\u5bf9\u6bd4\u5b66\u4e60\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u89e3\u8026\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u7684\u6a21\u5f0f\u7279\u5f02\u6027/\u4e0d\u53d8\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u5206\u7c7b\u9884\u6d4b\u7cbe\u5ea6", "motivation": "\u591a\u6a21\u6001\u5f20\u91cf\u65f6\u95f4\u5e8f\u5217\uff08\u5982\u641c\u7d22\u5f15\u64ce\u3001\u73af\u5883\u76d1\u6d4b\u6570\u636e\uff09\u56e0\u9ad8\u7ef4\u590d\u6742\u6027\u5bfc\u81f4\u8868\u5f81\u5b66\u4e60\u56f0\u96be\uff0c\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd", "method": "\u91c7\u7528\u5f20\u91cf\u5207\u7247\u964d\u7ef4\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u89e3\u8026\u8868\u5f81\uff1a\u2460 \u6a21\u5f0f\u7279\u5f02\u6027\u7279\u5f81\uff08\u540c\u6a21\u6001\u53d8\u91cf\u5173\u7cfb\uff09\u2461 \u6a21\u5f0f\u4e0d\u53d8\u7279\u5f81\uff08\u8de8\u6a21\u6001\u5171\u6027\uff09\uff0c\u53cc\u8def\u5f84\u635f\u5931\u51fd\u6570\u8054\u5408\u4f18\u5316", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5206\u7c7b\u4e0e\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cMoST\u5168\u9762\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u7cbe\u5ea6\u63d0\u5347\u663e\u8457", "conclusion": "\u901a\u8fc7\u89e3\u8026\u8868\u5f81\u4f5c\u4e3a\u6570\u636e\u589e\u5f3a\u624b\u6bb5\uff0cMoST\u6709\u6548\u5229\u7528\u591a\u6a21\u6001\u7279\u6027\uff0c\u4e3a\u5f20\u91cf\u65f6\u5e8f\u5206\u6790\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2602.24110", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.24110", "abs": "https://arxiv.org/abs/2602.24110", "authors": ["Yanwei Ren", "Haotian Zhang", "Likang Xiao", "Xikai Zhang", "Jiaxing Huang", "Jiayan Qiu", "Baosheng Yu", "Quan Chen", "Liu Liu"], "title": "Recycling Failures: Salvaging Exploration in RLVR via Fine-Grained Off-Policy Guidance", "comment": null, "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a powerful paradigm for enhancing the complex reasoning capabilities of Large Reasoning Models. However, standard outcome-based supervision suffers from a critical limitation that penalizes trajectories that are largely correct but fail due to several missteps as heavily as completely erroneous ones. This coarse feedback signal causes the model to discard valuable largely correct rollouts, leading to a degradation in rollout diversity that prematurely narrows the exploration space. Process Reward Models have demonstrated efficacy in providing reliable step-wise verification for test-time scaling, naively integrating these signals into RLVR as dense rewards proves ineffective.Prior methods attempt to introduce off-policy guided whole-trajectory replacement that often outside the policy model's distribution, but still fail to utilize the largely correct rollouts generated by the model itself and thus do not effectively mitigate the narrowing of the exploration space. To address these issues, we propose SCOPE (Step-wise Correction for On-Policy Exploration), a novel framework that utilizes Process Reward Models to pinpoint the first erroneous step in suboptimal rollouts and applies fine-grained, step-wise off-policy rectification. By applying precise refinement on partially correct rollout, our method effectively salvages partially correct trajectories and increases diversity score by 13.5%, thereby sustaining a broad exploration space. Extensive experiments demonstrate that our approach establishes new state-of-the-art results, achieving an average accuracy of 46.6% on math reasoning and exhibiting robust generalization with 53.4% accuracy on out-of-distribution reasoning tasks.", "AI": {"tldr": "\u9488\u5bf9RLVR\u4e2d\u7ed3\u679c\u76d1\u7763\u5bf9\"\u57fa\u672c\u6b63\u786e\u4f46\u6709\u5c0f\u9519\"\u8f68\u8ff9\u60e9\u7f5a\u8fc7\u91cd\u5bfc\u81f4\u63a2\u7d22\u7a7a\u95f4\u8fc7\u65e9\u7f29\u5c0f\u7684\u95ee\u9898\uff0c\u63d0\u51faSCOPE\u6846\u67b6\uff0c\u5229\u7528\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u5b9a\u4f4d\u9996\u4e2a\u9519\u8bef\u6b65\u9aa4\u5e76\u9010\u6b65\u4fee\u6b63\uff0c\u633d\u6551\u90e8\u5206\u6b63\u786e\u8f68\u8ff9\uff0c\u591a\u6837\u6027\u63d0\u534713.5%\uff0c\u5728\u6570\u5b66\u63a8\u7406\u548c\u5206\u5e03\u5916\u6cdb\u5316\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u6807\u51c6\u7ed3\u679c\u76d1\u7763\u5c06\u57fa\u672c\u6b63\u786e\u4f46\u5b58\u5728\u5c11\u6570\u9519\u8bef\u7684\u8f68\u8ff9\u4e0e\u5b8c\u5168\u9519\u8bef\u8f68\u8ff9\u540c\u7b49\u91cd\u7f5a\uff0c\u5bfc\u81f4\u6a21\u578b\u4e22\u5f03\u5927\u91cf\u6709\u4ef7\u503c\u7684\u90e8\u5206\u6b63\u786e\u8f68\u8ff9\uff0c\u9020\u6210\u63a2\u7d22\u591a\u6837\u6027\u4e25\u91cd\u4e0b\u964d\u5e76\u8fc7\u65e9\u7f29\u5c0f\u641c\u7d22\u7a7a\u95f4\u3002\u73b0\u6709\u65b9\u6cd5\u7b80\u5355\u96c6\u6210\u8fc7\u7a0b\u5956\u52b1\u65e0\u6548\uff0c\u6216\u91c7\u7528\u79bb\u7b56\u7565\u6574\u6761\u8f68\u8ff9\u66ff\u6362\u4f46\u8d85\u51fa\u7b56\u7565\u5206\u5e03\u4e14\u672a\u5229\u7528\u6a21\u578b\u81ea\u8eab\u751f\u6210\u7684\u90e8\u5206\u6b63\u786e\u8f68\u8ff9\u3002", "method": "\u63d0\u51faSCOPE\uff08Step-wise Correction for On-Policy Exploration\uff09\u6846\u67b6\uff1a\u5229\u7528\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u7cbe\u51c6\u8bc6\u522b\u6b21\u4f18\u8f68\u8ff9\u4e2d\u7684\u9996\u4e2a\u9519\u8bef\u6b65\u9aa4\uff0c\u7136\u540e\u5bf9\u8be5\u6b65\u9aa4\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7684\u79bb\u7b56\u7565\u4fee\u6b63\uff0c\u751f\u6210\u4fee\u6b63\u540e\u8f68\u8ff9\u5e76\u4fdd\u7559\u81f3\u8bad\u7ec3\u96c6\uff0c\u4ece\u800c\u6709\u6548\u633d\u6551\u90e8\u5206\u6b63\u786e\u8f68\u8ff9\u5e76\u4fdd\u6301\u5e7f\u6cdb\u63a2\u7d22\u7a7a\u95f4\u3002", "result": "\u591a\u6837\u6027\u5206\u6570\u63d0\u534713.5%\uff1b\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u5b9e\u73b046.6%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff08\u65b0SOTA\uff09\uff1b\u5728\u5206\u5e03\u5916\u63a8\u7406\u4efb\u52a1\u4e0a\u5c55\u73b0\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\uff0c\u8fbe\u523053.4%\u51c6\u786e\u7387\u3002", "conclusion": "SCOPE\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u6b65\u9aa4\u7ea7\u4fee\u6b63\u6709\u6548\u7f13\u89e3\u4e86\u63a2\u7d22\u7a7a\u95f4\u8fc7\u65e9\u7f29\u5c0f\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\uff0c\u4e3aRLVR\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u8303\u5f0f\u3002"}}
{"id": "2602.23696", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23696", "abs": "https://arxiv.org/abs/2602.23696", "authors": ["Yongzhong Xu"], "title": "Optimizer-Induced Low-Dimensional Drift and Transverse Dynamics in Transformer Training", "comment": "18 pages, 4 figures", "summary": "We study the geometry of training trajectories in small transformer models and find that parameter updates organize into a dominant drift direction with transverse residual dynamics. Using uncentered, row-normalized trajectory PCA, we show that a single direction captures a large fraction of cumulative parameter movement early in training, while remaining components encode oscillatory behavior in auxiliary probe performance. Instantaneous gradients exhibit little alignment with this dominant direction, indicating that it arises from accumulated optimizer updates rather than per-batch gradient structure. Comparing AdamW with SGD variants at matched loss levels reveals substantial differences in trajectory geometry: AdamW develops multi-dimensional drift structure, whereas SGD-family optimizers produce nearly colinear parameter evolution and weaker probe dynamics. Reheating selectively perturbs transverse components with minimal effect on the dominant drift coordinate. These findings suggest that optimizer choice shapes the effective dimensionality and structure of learning trajectories beyond what is apparent from loss values alone.", "AI": {"tldr": "\u7814\u7a76\u5c0f\u578bTransformer\u8bad\u7ec3\u8f68\u8ff9\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u53d1\u73b0\u53c2\u6570\u66f4\u65b0\u5f62\u6210\u4e3b\u5bfc\u6f02\u79fb\u65b9\u5411\u4e0e\u6a2a\u5411\u6b8b\u4f59\u52a8\u529b\u5b66\uff0c\u63ed\u793aAdamW\u4e0eSGD\u5728\u8f68\u8ff9\u7ef4\u5ea6\u4e0a\u7684\u672c\u8d28\u5dee\u5f02\uff0c\u8868\u660e\u4f18\u5316\u5668\u9009\u62e9\u4ee5\u8d85\u8d8a\u635f\u5931\u503c\u7684\u65b9\u5f0f\u5851\u9020\u5b66\u4e60\u8f68\u8ff9\u7684\u6709\u6548\u7ef4\u5ea6\u3002", "motivation": "\u63a2\u7a76Transformer\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u53c2\u6570\u7684\u6f14\u5316\u89c4\u5f8b\uff0c\u4ee5\u53ca\u4e0d\u540c\u4f18\u5316\u5668\u5982\u4f55\u5f71\u54cd\u8bad\u7ec3\u8f68\u8ff9\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u7406\u89e3\u4f18\u5316\u5668\u9009\u62e9\u5bf9\u5b66\u4e60\u52a8\u529b\u5b66\u7684\u5f71\u54cd\u673a\u5236\u3002", "method": "\u91c7\u7528\u975e\u4e2d\u5fc3\u5316\u3001\u884c\u5f52\u4e00\u5316\u7684\u8f68\u8ff9\u4e3b\u6210\u5206\u5206\u6790\uff0c\u5bf9\u6bd4AdamW\u4e0eSGD\u53d8\u4f53\u5728\u5339\u914d\u635f\u5931\u6c34\u5e73\u4e0b\u7684\u8868\u73b0\uff0c\u5206\u6790\u53c2\u6570\u66f4\u65b0\u3001\u77ac\u65f6\u68af\u5ea6\u4e0e\u8f85\u52a9\u63a2\u9488\u6027\u80fd\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u53c2\u6570\u66f4\u65b0\u5728\u8bad\u7ec3\u65e9\u671f\u7531\u5355\u4e00\u4e3b\u5bfc\u65b9\u5411\u6355\u83b7\u5927\u90e8\u5206\u7d2f\u79ef\u8fd0\u52a8\uff0c\u5176\u4f59\u5206\u91cf\u7f16\u7801\u632f\u8361\u6027\u63a2\u9488\u6027\u80fd\uff1b\u77ac\u65f6\u68af\u5ea6\u4e0e\u4e3b\u5bfc\u65b9\u5411\u5bf9\u9f50\u5ea6\u4f4e\uff1bAdamW\u4ea7\u751f\u591a\u7ef4\u6f02\u79fb\u7ed3\u6784\uff0c\u800cSGD\u65cf\u4f18\u5316\u5668\u4ea7\u751f\u8fd1\u4e4e\u5171\u7ebf\u7684\u53c2\u6570\u6f14\u5316\uff1b\u91cd\u52a0\u70ed\u9009\u62e9\u6027\u5730\u6270\u52a8\u6a2a\u5411\u5206\u91cf\u3002", "conclusion": "\u4f18\u5316\u5668\u9009\u62e9\u5851\u9020\u4e86\u5b66\u4e60\u8f68\u8ff9\u7684\u6709\u6548\u7ef4\u5ea6\u548c\u7ed3\u6784\uff0c\u8fd9\u79cd\u5f71\u54cd\u65e0\u6cd5\u4ec5\u4ece\u635f\u5931\u503c\u4e2d\u89c2\u5bdf\u5230\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u4f18\u5316\u7b97\u6cd5\u5728\u53c2\u6570\u7a7a\u95f4\u63a2\u7d22\u65b9\u5f0f\u7684\u672c\u8d28\u5dee\u5f02\u3002"}}
{"id": "2602.24173", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24173", "abs": "https://arxiv.org/abs/2602.24173", "authors": ["Antoine Peyronnet", "Fabian Gloeckle", "Amaury Hayat"], "title": "LemmaBench: A Live, Research-Level Benchmark to Evaluate LLM Capabilities in Mathematics", "comment": "15 pages, 3 figures, 5 Tables", "summary": "We present a new approach for benchmarking Large Language Model (LLM) capabilities on research-level mathematics. Existing benchmarks largely rely on static, hand-curated sets of contest or textbook-style problems as proxies for mathematical research. Instead, we establish an updatable benchmark evaluating models directly on the latest research results in mathematics. This consists of an automatic pipeline that extracts lemmas from arXiv and rewrites them into self-contained statements by making all assumptions and required definitions explicit. It results in a benchmark that can be updated regularly with new problems taken directly from human mathematical research, while previous instances can be used for training without compromising future evaluations. We benchmark current state-of-the-art LLMs, which obtain around 10-15$\\%$ accuracy in theorem proving (pass@1) depending on the model, showing that there is currently a large margin of progression for LLMs to reach human-level proving capabilities in a research context.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8earXiv\u6700\u65b0\u6570\u5b66\u7814\u7a76\u8bba\u6587\u7684\u52a8\u6001\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u7814\u7a76\u7ea7\u6570\u5b66\u4efb\u52a1\u4e2d\u7684\u5b9a\u7406\u8bc1\u660e\u80fd\u529b\uff0c\u5f53\u524d\u6a21\u578b\u51c6\u786e\u7387\u4ec510-15%", "motivation": "\u73b0\u6709\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4f9d\u8d56\u9759\u6001\u7684\u7ade\u8d5b/\u6559\u79d1\u4e66\u9898\u76ee\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u7814\u7a76\u573a\u666f\uff1b\u9700\u8981\u5efa\u7acb\u80fd\u968f\u6570\u5b66\u7814\u7a76\u8fdb\u5c55\u6301\u7eed\u66f4\u65b0\u7684\u8bc4\u4f30\u4f53\u7cfb", "method": "\u5f00\u53d1\u81ea\u52a8\u6d41\u6c34\u7ebf\u4ecearXiv\u63d0\u53d6\u6570\u5b66\u5f15\u7406\uff0c\u91cd\u5199\u4e3a\u5305\u542b\u6240\u6709\u663e\u5f0f\u5047\u8bbe\u548c\u5b9a\u4e49\u7684\u72ec\u7acb\u547d\u9898\uff0c\u5b9e\u73b0\u57fa\u51c6\u6d4b\u8bd5\u7684\u6301\u7eed\u66f4\u65b0\u4e0e\u8bad\u7ec3/\u8bc4\u4f30\u6570\u636e\u9694\u79bb", "result": "\u5f53\u524dSOTA\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b9a\u7406\u8bc1\u660e\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u4ec510-15%\uff08pass@1\uff09\uff0c\u4e0e\u4eba\u7c7b\u7814\u7a76\u7ea7\u8bc1\u660e\u80fd\u529b\u5b58\u5728\u663e\u8457\u5dee\u8ddd", "conclusion": "\u65b0\u57fa\u51c6\u66f4\u771f\u5b9e\u53cd\u6620LLM\u5728\u6570\u5b66\u7814\u7a76\u4e2d\u7684\u5b9e\u9645\u6c34\u5e73\uff0c\u663e\u793a\u5176\u8ddd\u4eba\u7c7b\u7ea7\u7814\u7a76\u80fd\u529b\u4ecd\u6709\u5de8\u5927\u63d0\u5347\u7a7a\u95f4"}}
{"id": "2602.24062", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.24062", "abs": "https://arxiv.org/abs/2602.24062", "authors": ["Michele Bandini", "Davide Ferrari", "Stefano Carretta", "Michele Amoretti"], "title": "Optimized Compilation for Distributed Quantum Computing", "comment": null, "summary": "In many practical applications, quantum algorithms require several qubits, significantly more than those available with current noisy intermediate-scale quantum processors. Distributed quantum computing (DQC) is considered a scalable approach to increasing the number of available qubits for computational tasks. In the DQC setting, a quantum compiler must find the best partitioning for the quantum algorithm and then perform smart non-local operations scheduling to optimize the consumption of Einstein-Podolsky-Rosen (EPR) pairs. In this work, the focus is on minimizing the use of EPR pairs when the circuit structure allows for multiple non-local gates to utilize a single TeleGate operation. This is achieved by using a greedy algorithm that explores the circuit and groups together the gates that could share an EPR pair while also changing the order of commutative gates when necessary. With this preliminary pass, the compiled circuits show reduced depth and EPR usage. Since the quality of each EPR pair quickly deteriorates, the number of non-local gates using the same EPR pair should also be bounded. This means that, depending on the features of the target quantum network, the user can achieve different levels of optimization. Here, it is shown that this approach brings benefits even while assuming a low EPR pair lifetime.", "AI": {"tldr": "This paper proposes a greedy algorithm for distributed quantum computing (DQC) that minimizes EPR pair usage by grouping non-local gates to share TeleGate operations and reordering commutative gates, significantly reducing circuit depth and EPR consumption even under low EPR pair lifetime conditions.", "motivation": "Current quantum processors have limited qubits, while many practical quantum algorithms require more qubits. Distributed quantum computing (DQC) is a scalable solution, but optimizing EPR pair consumption (critical for inter-processor communication) is essential due to their rapid quality deterioration.", "method": "A greedy algorithm that: (1) explores circuit structure to group non-local gates sharing a single TeleGate operation (reducing EPR pairs), and (2) dynamically reorders commutative gates to maximize EPR sharing opportunities.", "result": "Compiled circuits show reduced depth and EPR usage. The approach delivers optimization benefits even under low EPR pair lifetime assumptions, and allows adjustable optimization levels based on target quantum network characteristics.", "conclusion": "The proposed greedy grouping method effectively minimizes EPR consumption in DQC, making it practical for near-term quantum networks with limited entanglement resources. This compiler optimization is crucial for scaling quantum computations beyond single-processor limits."}}
{"id": "2602.23737", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23737", "abs": "https://arxiv.org/abs/2602.23737", "authors": ["Hanping Zhang", "Yuhong Guo"], "title": "Bridging Dynamics Gaps via Diffusion Schr\u00f6dinger Bridge for Cross-Domain Reinforcement Learning", "comment": null, "summary": "Cross-domain reinforcement learning (RL) aims to learn transferable policies under dynamics shifts between source and target domains. A key challenge lies in the lack of target-domain environment interaction and reward supervision, which prevents direct policy learning. To address this challenge, we propose Bridging Dynamics Gaps for Cross-Domain Reinforcement Learning (BDGxRL), a novel framework that leverages Diffusion Schr\u00f6dinger Bridge (DSB) to align source transitions with target-domain dynamics encoded in offline demonstrations. Moreover, we introduce a reward modulation mechanism that estimates rewards based on state transitions, applying to DSB-aligned samples to ensure consistency between rewards and target-domain dynamics. BDGxRL performs target-oriented policy learning entirely within the source domain, without access to the target environment or its rewards. Experiments on MuJoCo cross-domain benchmarks demonstrate that BDGxRL outperforms state-of-the-art baselines and shows strong adaptability under transition dynamics shifts.", "AI": {"tldr": "BDGxRL uses Diffusion Schr\u00f6dinger Bridge and reward modulation to align source and target domain dynamics with offline demos, enabling cross-domain RL without target environment interaction, outperforming baselines on MuJoCo benchmarks.", "motivation": "Cross-domain RL struggles with dynamics shifts between source and target domains due to lack of target environment interaction and reward supervision, preventing direct policy learning.", "method": "Proposes BDGxRL: (1) Uses Diffusion Schr\u00f6dinger Bridge (DSB) to align source transitions with target dynamics from offline demos; (2) Introduces reward modulation to estimate rewards via state transitions for DSB-aligned samples; (3) Performs target-oriented policy learning entirely in source domain.", "result": "Outperforms state-of-the-art baselines on MuJoCo cross-domain benchmarks and demonstrates strong adaptability under transition dynamics shifts.", "conclusion": "BDGxRL effectively bridges domain dynamics gaps without target environment access, enabling efficient cross-domain policy transfer through dynamics alignment and reward modulation."}}
{"id": "2602.24077", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.24077", "abs": "https://arxiv.org/abs/2602.24077", "authors": ["Yun-Long Cao", "Xiao-Ye Xu", "Chuan-Feng Li", "Guang-Can Guo"], "title": "Gaussian resource based heralded entangled state generation enhanced by photon addition and subtraction", "comment": null, "summary": "We propose a heralded entanglement generation scheme based on Gaussian sources augmented with photon addition and subtraction operations. By combining single-mode squeezing, linear interferometers, and conditional photon-number measurements on ancillary modes, our model probabilistically generates dual-rail encoded Bell, GHZ, and W states. We systematically optimize the squeezing parameters and interferometer settings to maximize both the heralding success probability and the fidelity with the target states. Our results show that the inclusion of photon addition and subtraction significantly enhances the non-classicality of the output states, leading to improved generation performance, while maintaining computational efficiency comparable to single-photon source models. We further analyze the robustness of the scheme under parameter perturbations, demonstrating stable performance against realistic experimental imperfections. This work provides a versatile and experimentally feasible framework for scalable heralded entanglement generation using Gaussian resources with non-Gaussian operations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u6e90\u5e76\u7ed3\u5408\u5149\u5b50\u589e\u51cf\u64cd\u4f5c\u7684\u4fe1 heralded \u7ea0\u7f20\u751f\u6210\u65b9\u6848\u3002\u901a\u8fc7\u5355\u6a21\u538b\u7f29\u3001\u7ebf\u6027\u5e72\u6d89\u4eea\u548c\u8f85\u52a9\u6a21\u5f0f\u6761\u4ef6\u5149\u5b50\u6570\u6d4b\u91cf\uff0c\u6982\u7387\u6027\u751f\u6210\u53cc\u8f68\u7f16\u7801\u7684Bell\u3001GHZ\u548cW\u6001\u3002\u5149\u5b50\u589e\u51cf\u663e\u8457\u589e\u5f3a\u4e86\u8f93\u51fa\u6001\u7684\u975e\u7ecf\u5178\u6027\uff0c\u63d0\u9ad8\u4e86\u751f\u6210\u6027\u80fd\uff0c\u540c\u65f6\u5bf9\u53c2\u6570\u6270\u52a8\u5177\u6709\u9c81\u68d2\u6027\u3002", "motivation": "\u5229\u7528\u9ad8\u65af\u8d44\u6e90\u7ed3\u5408\u975e\u9ad8\u65af\u64cd\u4f5c\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u5b9e\u9a8c\u53ef\u884c\u7684\u9ad8\u4fdd\u771f\u5ea6\u7ea0\u7f20\u751f\u6210\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6848\u5728\u6210\u529f\u6982\u7387\u3001\u8ba1\u7b97\u6548\u7387\u548c\u6297\u566a\u58f0\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u5355\u6a21\u538b\u7f29\u3001\u7ebf\u6027\u5e72\u6d89\u4eea\u548c\u8f85\u52a9\u6a21\u5f0f\u6761\u4ef6\u5149\u5b50\u6570\u6d4b\u91cf\uff0c\u901a\u8fc7\u5149\u5b50\u589e\u51cf\u64cd\u4f5c\u589e\u5f3a\u9ad8\u65af\u6e90\u7684\u91cf\u5b50\u7279\u6027\uff0c\u7cfb\u7edf\u4f18\u5316\u538b\u7f29\u53c2\u6570\u548c\u5e72\u6d89\u4eea\u8bbe\u7f6e\u4ee5\u6700\u5927\u5316\u6210\u529f\u6982\u7387\u548c\u4fdd\u771f\u5ea6\u3002", "result": "\u5149\u5b50\u589e\u51cf\u64cd\u4f5c\u663e\u8457\u63d0\u5347\u8f93\u51fa\u6001\u975e\u7ecf\u5178\u6027\uff0c\u7ea0\u7f20\u751f\u6210\u6027\u80fd\u4f18\u4e8e\u7eaf\u9ad8\u65af\u65b9\u6848\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u5355\u5149\u5b50\u6e90\u6a21\u578b\u76f8\u5f53\u7684\u8ba1\u7b97\u6548\u7387\uff1b\u65b9\u6848\u5bf9\u5b9e\u9a8c\u53c2\u6570\u6270\u52a8\u5177\u6709\u7a33\u5b9a\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5229\u7528\u9ad8\u65af\u8d44\u6e90\u548c\u975e\u9ad8\u65af\u64cd\u4f5c\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u591a\u7c92\u5b50\u7ea0\u7f20\u751f\u6210\u63d0\u4f9b\u4e86\u901a\u7528\u4e14\u5b9e\u9a8c\u53ef\u884c\u7684\u6846\u67b6\u3002"}}
{"id": "2602.23761", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23761", "abs": "https://arxiv.org/abs/2602.23761", "authors": ["Yuyu Geng", "Lei Sun", "Yao Gao", "Xinxin Hu", "Zhonghua Yi", "Xiaolong Qian", "Weijian Hu", "Jian Bai", "Kaiwei Wang"], "title": "OPTIAGENT: A Physics-Driven Agentic Framework for Automated Optical Design", "comment": null, "summary": "Optical design is the process of configuring optical elements to precisely manipulate light for high-fidelity imaging. It is inherently a highly non-convex optimization problem that relies heavily on human heuristic expertise and domain-specific knowledge. While Large Language Models (LLMs) possess extensive optical knowledge, their capabilities in leveraging the knowledge in designing lens system remain significantly constrained. This work represents the first attempt to employ LLMs in the field of optical design. We bridge the expertise gap by enabling users without formal optical training to successfully develop functional lens systems. Concretely, we curate a comprehensive dataset, named OptiDesignQA, which encompasses both classical lens systems sourced from standard optical textbooks and novel configurations generated by automated design algorithms for training and evaluation. Furthermore, we inject domain-specific optical expertise into the LLM through a hybrid objective of full-system synthesis and lens completion. To align the model with optical principles, we employ Group Relative Policy Optimization Done Right (DrGRPO) guided by Optical Lexicographic Reward for physics-driven policy alignment. This reward system incorporates structural format rewards, physical feasibility rewards, light-manipulation accuracy, and LLM-based heuristics. Finally, our model integrates with specialized optical optimization routines for end-to-end fine-tuning and precision refinement. We benchmark our proposed method against both traditional optimization-based automated design algorithms and LLM counterparts, and experimental results show the superiority of our method.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e8e\u5149\u5b66\u8bbe\u8ba1\u9886\u57df\uff0c\u901a\u8fc7\u6784\u5efa\u4e13\u7528\u6570\u636e\u96c6OptiDesignQA\u3001\u6ce8\u5165\u9886\u57df\u77e5\u8bc6\u7684\u6df7\u5408\u8bad\u7ec3\u76ee\u6807\u548c\u57fa\u4e8e\u7269\u7406\u7684\u5956\u52b1\u7cfb\u7edf\uff0c\u4f7f\u975e\u5149\u5b66\u4e13\u4e1a\u7528\u6237\u80fd\u591f\u6210\u529f\u8bbe\u8ba1\u529f\u80fd\u6027\u955c\u5934\u7cfb\u7edf\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u4f18\u5316\u7b97\u6cd5\u3002", "motivation": "\u5149\u5b66\u8bbe\u8ba1\u662f\u4e00\u4e2a\u9ad8\u5ea6\u975e\u51f8\u7684\u4f18\u5316\u95ee\u9898\uff0c\u4e25\u91cd\u4f9d\u8d56\u4eba\u7c7b\u542f\u53d1\u5f0f\u4e13\u4e1a\u77e5\u8bc6\u548c\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5177\u5907\u4e30\u5bcc\u7684\u5149\u5b66\u77e5\u8bc6\uff0c\u4f46\u5728\u955c\u5934\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\u80fd\u529b\u4ecd\u4e25\u91cd\u53d7\u9650\uff0c\u9700\u8981\u586b\u8865\u4e13\u4e1a\u77e5\u8bc6\u9e3f\u6c9f\u3002", "method": "1. \u6784\u5efa\u5305\u542b\u7ecf\u5178\u548c\u65b0\u9896\u955c\u5934\u7cfb\u7edf\u7684OptiDesignQA\u6570\u636e\u96c6\uff1b2. \u901a\u8fc7\u5168\u7cfb\u7edf\u5408\u6210\u548c\u955c\u5934\u8865\u5168\u7684\u6df7\u5408\u76ee\u6807\u6ce8\u5165\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff1b3. \u91c7\u7528\u57fa\u4e8eGroup Relative Policy Optimization Done Right (DrGRPO)\u7684\u5149\u5b66\u8bcd\u5178\u5956\u52b1\u673a\u5236\uff0c\u5b9e\u73b0\u7269\u7406\u9a71\u52a8\u7684\u4f18\u5316\uff1b4. \u96c6\u6210\u4e13\u4e1a\u5149\u5b66\u4f18\u5316\u5de5\u5177\u8fdb\u884c\u7aef\u5230\u7aef\u5fae\u8c03\u548c\u7cbe\u5ea6\u4f18\u5316\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edf\u57fa\u4e8e\u4f18\u5316\u7684\u81ea\u52a8\u5316\u8bbe\u8ba1\u7b97\u6cd5\u548cLLM\u57fa\u7ebf\u6a21\u578b\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u6210\u529f\u5b9e\u73b0\u4e86\u9996\u4e2aLLM\u9a71\u52a8\u7684\u5149\u5b66\u8bbe\u8ba1\u6846\u67b6\uff0c\u4f7f\u672a\u7ecf\u6b63\u5f0f\u5149\u5b66\u57f9\u8bad\u7684\u7528\u6237\u4e5f\u80fd\u5f00\u53d1\u529f\u80fd\u6027\u955c\u5934\u7cfb\u7edf\uff0c\u4e3a\u5149\u5b66\u8bbe\u8ba1\u6c11\u4e3b\u5316\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.24195", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24195", "abs": "https://arxiv.org/abs/2602.24195", "authors": ["Gregory Kang Ruey Lau", "Hieu Dao", "Nicole Kan Hui Lin", "Bryan Kian Hsiang Low"], "title": "Uncertainty Quantification for Multimodal Large Language Models with Incoherence-adjusted Semantic Volume", "comment": "Earlier versions presented at ICLR 2025 QUESTION workshop and ICML 2025 R2-FM workshop", "summary": "Despite their capabilities, Multimodal Large Language Models (MLLMs) may produce plausible but erroneous outputs, hindering reliable deployment. Accurate uncertainty metrics could enable escalation of unreliable queries to human experts or larger models for improved performance. However, existing uncertainty metrics have practical constraints, such as being designed only for specific modalities, reliant on external tools, or computationally expensive. We introduce UMPIRE, a training-free uncertainty quantification framework for MLLMs that works efficiently across various input and output modalities without external tools, relying only on the models' own internal modality features. UMPIRE computes the incoherence-adjusted semantic volume of sampled MLLM responses for a given task instance, effectively capturing both the global semantic diversity of samples and the local incoherence of responses based on internal model confidence. We propose uncertainty desiderata for MLLMs and provide theoretical analysis motivating UMPIRE's design. Extensive experiments show that UMPIRE consistently outperforms baseline metrics in error detection and uncertainty calibration across image, audio, and video-text benchmarks, including adversarial and out-of-distribution settings. We also demonstrate UMPIRE's generalization to non-text output tasks, including image and audio generation.", "AI": {"tldr": "UMPIRE\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\uff0c\u4ec5\u5229\u7528\u6a21\u578b\u5185\u90e8\u7279\u5f81\u8ba1\u7b97\u91c7\u6837\u54cd\u5e94\u7684\u4e0d\u8fde\u8d2f\u8c03\u6574\u8bed\u4e49\u4f53\u79ef\uff0c\u5728\u5404\u7c7b\u4efb\u52a1\u548c\u6a21\u6001\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u5f3a\u5927\uff0c\u4f46\u5176\u53ef\u80fd\u4ea7\u751f\u770b\u4f3c\u5408\u7406\u5374\u9519\u8bef\u7684\u8f93\u51fa\uff0c\u963b\u788d\u53ef\u9760\u90e8\u7f72\u3002\u73b0\u6709\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\uff1a\u4ec5\u9002\u7528\u4e8e\u7279\u5b9a\u6a21\u6001\u3001\u4f9d\u8d56\u5916\u90e8\u5de5\u5177\u6216\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "\u4f5c\u8005\u63d0\u51faUMPIRE\u6846\u67b6\uff0c\u901a\u8fc7\u8ba1\u7b97\u7ed9\u5b9a\u4efb\u52a1\u5b9e\u4f8b\u7684\u91c7\u6837MLLM\u54cd\u5e94\u7684\u4e0d\u8fde\u8d2f\u8c03\u6574\u8bed\u4e49\u4f53\u79ef\uff0c\u4ec5\u5229\u7528\u6a21\u578b\u5185\u90e8\u6a21\u6001\u7279\u5f81\u6765\u6355\u83b7\u91c7\u6837\u7ed3\u679c\u7684\u5168\u5c40\u8bed\u4e49\u591a\u6837\u6027\u548c\u57fa\u4e8e\u6a21\u578b\u7f6e\u4fe1\u5ea6\u7684\u5c40\u90e8\u4e0d\u8fde\u8d2f\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cUMPIRE\u5728\u56fe\u50cf\u3001\u97f3\u9891\u548c\u89c6\u9891\u6587\u672c\u57fa\u51c6\u6d4b\u8bd5\u7684\u9519\u8bef\u68c0\u6d4b\u548c\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u6307\u6807\uff0c\u5305\u62ec\u5bf9\u6297\u6027\u548c\u5206\u5e03\u5916\u573a\u666f\u3002\u8be5\u6846\u67b6\u8fd8\u6210\u529f\u63a8\u5e7f\u5230\u56fe\u50cf\u548c\u97f3\u9891\u751f\u6210\u7b49\u975e\u6587\u672c\u8f93\u51fa\u4efb\u52a1\u3002", "conclusion": "UMPIRE\u4e3aMLLMs\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u5916\u90e8\u4f9d\u8d56\u5373\u53ef\u8de8\u591a\u79cd\u6a21\u6001\u548c\u4efb\u52a1\u5de5\u4f5c\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u90e8\u7f72\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2602.23770", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23770", "abs": "https://arxiv.org/abs/2602.23770", "authors": ["Chenxing Lin", "Xinhui Gao", "Haipeng Zhang", "Xinran Li", "Haitao Wang", "Songzhu Mei", "Chenglu Wen", "Weiquan Liu", "Siqi Shen", "Cheng Wang"], "title": "MAGE: Multi-scale Autoregressive Generation for Offline Reinforcement Learning", "comment": "ICLR2026", "summary": "Generative models have gained significant traction in offline reinforcement learning (RL) due to their ability to model complex trajectory distributions. However, existing generation-based approaches still struggle with long-horizon tasks characterized by sparse rewards. Some hierarchical generation methods have been developed to mitigate this issue by decomposing the original problem into shorter-horizon subproblems using one policy and generating detailed actions with another. While effective, these methods often overlook the multi-scale temporal structure inherent in trajectories, resulting in suboptimal performance. To overcome these limitations, we propose MAGE, a Multi-scale Autoregressive GEneration-based offline RL method. MAGE incorporates a condition-guided multi-scale autoencoder to learn hierarchical trajectory representations, along with a multi-scale transformer that autoregressively generates trajectory representations from coarse to fine temporal scales. MAGE effectively captures temporal dependencies of trajectories at multiple resolutions. Additionally, a condition-guided decoder is employed to exert precise control over short-term behaviors. Extensive experiments on five offline RL benchmarks against fifteen baseline algorithms show that MAGE successfully integrates multi-scale trajectory modeling with conditional guidance, generating coherent and controllable trajectories in long-horizon sparse-reward settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86MAGE\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u81ea\u56de\u5f52\u751f\u6210\u6846\u67b6\u89e3\u51b3\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u957f\u671f\u7a00\u758f\u5956\u52b1\u4efb\u52a1\u7684\u6311\u6218\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u591a\u5c3a\u5ea6\u81ea\u52a8\u7f16\u7801\u5668\u548cTransformer\uff0c\u4ece\u7c97\u5230\u7ec6\u751f\u6210\u8f68\u8ff9\uff0c\u5e76\u901a\u8fc7\u6761\u4ef6\u5f15\u5bfc\u5b9e\u73b0\u7cbe\u786e\u63a7\u5236\uff0c\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u751f\u6210\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u957f\u89c6\u91ce\u7a00\u758f\u5956\u52b1\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5c42\u6b21\u5316\u751f\u6210\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u8f68\u8ff9\u4e2d\u7684\u591a\u5c3a\u5ea6\u65f6\u95f4\u7ed3\u6784\uff0c\u5bfc\u81f4\u6027\u80fd\u6b21\u4f18\u3002", "method": "MAGE\u91c7\u7528\u6761\u4ef6\u5f15\u5bfc\u7684\u591a\u5c3a\u5ea6\u81ea\u52a8\u7f16\u7801\u5668\u5b66\u4e60\u5c42\u6b21\u5316\u8f68\u8ff9\u8868\u793a\uff0c\u4f7f\u7528\u591a\u5c3a\u5ea6Transformer\u4ece\u7c97\u5230\u7ec6\u81ea\u56de\u5f52\u751f\u6210\u8f68\u8ff9\u8868\u793a\uff0c\u5e76\u5f15\u5165\u6761\u4ef6\u5f15\u5bfc\u89e3\u7801\u5668\u7cbe\u786e\u63a7\u5236\u77ed\u671f\u884c\u4e3a\u3002", "result": "\u5728\u4e94\u4e2a\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e15\u79cd\u57fa\u7ebf\u7b97\u6cd5\u5bf9\u6bd4\uff0cMAGE\u6210\u529f\u878d\u5408\u4e86\u591a\u5c3a\u5ea6\u8f68\u8ff9\u5efa\u6a21\u4e0e\u6761\u4ef6\u5f15\u5bfc\uff0c\u751f\u6210\u4e86\u8fde\u8d2f\u4e14\u53ef\u63a7\u7684\u8f68\u8ff9\u3002", "conclusion": "MAGE\u6709\u6548\u6355\u6349\u4e86\u591a\u5206\u8fa8\u7387\u4e0b\u7684\u8f68\u8ff9\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u5728\u957f\u89c6\u91ce\u7a00\u758f\u5956\u52b1\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7684\u8f68\u8ff9\u751f\u6210\u548c\u63a7\u5236\u3002"}}
{"id": "2602.24273", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24273", "abs": "https://arxiv.org/abs/2602.24273", "authors": ["Borja Requena Pozo", "Austin Letson", "Krystian Nowakowski", "Izan Beltran Ferreiro", "Leopoldo Sarra"], "title": "A Minimal Agent for Automated Theorem Proving", "comment": null, "summary": "We propose a minimal agentic baseline that enables systematic comparison across different AI-based theorem prover architectures. This design implements the core features shared among state-of-the-art systems: iterative proof refinement, library search and context management. We evaluate our baseline using qualitatively different benchmarks and compare various popular models and design choices, and demonstrate competitive performance compared to state-of-the-art approaches, while using a significantly simpler architecture. Our results demonstrate consistent advantages of an iterative approach over multiple single-shot generations, especially in terms of sample efficiency and cost effectiveness. The implementation is released open-source as a candidate reference for future research and as an accessible prover for the community.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6700\u5c0f\u5316\u7684AI\u5b9a\u7406\u8bc1\u660e\u667a\u80fd\u4f53\u57fa\u7ebf\uff0c\u901a\u8fc7\u5b9e\u73b0\u6838\u5fc3\u529f\u80fd\uff08\u8fed\u4ee3\u8bc1\u660e\u4f18\u5316\u3001\u5e93\u641c\u7d22\u548c\u4e0a\u4e0b\u6587\u7ba1\u7406\uff09\uff0c\u5728\u4fdd\u6301\u7b80\u5355\u67b6\u6784\u7684\u540c\u65f6\u8fbe\u5230\u4e0e\u9876\u5c16\u7cfb\u7edf\u76f8\u5f53\u7684\u7ade\u4e89\u6027\u80fd\uff0c\u5e76\u8bc1\u660e\u4e86\u8fed\u4ee3\u65b9\u6cd5\u5728\u6837\u672c\u6548\u7387\u548c\u6210\u672c\u6548\u76ca\u4e0a\u7684\u4f18\u52bf\u3002", "motivation": "\u4e3a\u4e86\u5728\u4e0d\u540cAI\u5b9a\u7406\u8bc1\u660e\u5668\u67b6\u6784\u4e4b\u95f4\u5b9e\u73b0\u7cfb\u7edf\u6027\u6bd4\u8f83\uff0c\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u6700\u5c0f\u5316\u7684\u57fa\u51c6\u7cfb\u7edf\u6765\u8bc4\u4f30\u548c\u5bf9\u6bd4\u5404\u79cd\u8bbe\u8ba1\u9009\u62e9\u3002", "method": "\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u4e00\u4e2a\u5305\u542b\u4e09\u5927\u6838\u5fc3\u529f\u80fd\u7684\u57fa\u7ebf\u7cfb\u7edf\uff1a\u8fed\u4ee3\u8bc1\u660e\u4f18\u5316\u3001\u5e93\u641c\u7d22\u548c\u4e0a\u4e0b\u6587\u7ba1\u7406\uff0c\u5e76\u5bf9\u4e0d\u540c\u6a21\u578b\u548c\u8bbe\u8ba1\u65b9\u6848\u8fdb\u884c\u4e86\u6bd4\u8f83\u8bc4\u4f30\u3002", "result": "\u5728\u5b9a\u6027\u4e0d\u540c\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u5177\u6709\u7ade\u4e89\u529b\uff0c\u867d\u7136\u67b6\u6784\u66f4\u7b80\u5355\u4f46\u6027\u80fd\u63a5\u8fd1\u9876\u5c16\u65b9\u6cd5\uff1b\u8bc1\u660e\u4e86\u8fed\u4ee3\u65b9\u6cd5\u76f8\u6bd4\u5355\u6b21\u751f\u6210\u5728\u6837\u672c\u6548\u7387\u548c\u6210\u672c\u6548\u76ca\u4e0a\u5177\u6709\u4e00\u81f4\u6027\u4f18\u52bf\u3002", "conclusion": "\u5f00\u6e90\u53d1\u5e03\u5b9e\u73b0\u4f5c\u4e3a\u672a\u6765\u7814\u7a76\u7684\u5019\u9009\u53c2\u8003\u57fa\u51c6\u548c\u793e\u533a\u53ef\u8bbf\u95ee\u7684\u8bc1\u660e\u5668\uff0c\u4e3aAI\u5b9a\u7406\u8bc1\u660e\u9886\u57df\u63d0\u4f9b\u6807\u51c6\u5316\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2602.24102", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.24102", "abs": "https://arxiv.org/abs/2602.24102", "authors": ["Kai-Xuan Wen", "Dong-Long Hu", "Shengyong Li", "Ze-Liang Xiang"], "title": "Estimating the performance boundary of Gottesman-Kitaev-Preskill codes and number-phase codes", "comment": null, "summary": "Bosonic quantum error-correcting codes encode logical information in a harmonic oscillator, with the Gottesman-Kitaev-Preskill (GKP) and number-phase (NP) codes representing two fundamentally different encoding paradigms. Although both have been extensively studied, it remains unclear under what physical noise conditions (including photon loss and dephasing) one encoding intrinsically outperforms the other. Here we estimate a quantitative performance boundary between GKP and NP codes under general photon loss-dephasing noise. By optimizing code parameters within each encoding family, we identify the noise regimes in which each code exhibits a fundamental advantage. In particular, we find that the crossover occurs when the dephasing strength is approximately two orders of magnitude smaller than the loss strength, revealing a sharp separation between operational regimes. Beyond this specific comparison, our work establishes a practical and extensible methodology for benchmarking bosonic codes and optimizing their parameters, providing concrete guidance for the experimental selection and deployment of bosonic encodings in realistic noise environments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u91cf\u5316\u6bd4\u8f83\u4e86GKP\u548cNP\u73bb\u8272\u91cf\u5b50\u7ea0\u9519\u7801\u5728\u5149\u5b50\u635f\u8017-\u9000\u76f8\u4f4d\u566a\u58f0\u4e0b\u7684\u6027\u80fd\u8fb9\u754c\uff0c\u53d1\u73b0\u5f53\u9000\u76f8\u4f4d\u5f3a\u5ea6\u6bd4\u635f\u8017\u5f3a\u5ea6\u4f4e\u7ea6\u4e24\u4e2a\u6570\u91cf\u7ea7\u65f6\u5b58\u5728\u660e\u663e\u6027\u80fd\u4ea4\u53c9\u70b9\uff0c\u5e76\u63d0\u51fa\u4e86\u53ef\u6269\u5c55\u7684\u73bb\u8272\u7801\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1GKP\u548cNP\u7801\u4e24\u79cd\u73bb\u8272\u91cf\u5b50\u7ea0\u9519\u7f16\u7801\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5728\u5b9e\u9645\u7269\u7406\u566a\u58f0\uff08\u5982\u5149\u5b50\u635f\u8017\u548c\u9000\u76f8\u4f4d\uff09\u6761\u4ef6\u4e0b\uff0c\u4f55\u79cd\u7f16\u7801\u5177\u6709\u672c\u8d28\u6027\u80fd\u4f18\u52bf\u4ecd\u4e0d\u660e\u786e\uff0c\u7f3a\u4e4f\u5b9a\u91cf\u6027\u80fd\u8fb9\u754c\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u4f18\u5316\u4e24\u7c7b\u7f16\u7801\u7684\u53c2\u6570\uff0c\u5728\u4e00\u822c\u5149\u5b50\u635f\u8017-\u9000\u76f8\u4f4d\u566a\u58f0\u6a21\u578b\u4e0b\u8fdb\u884c\u5b9a\u91cf\u6027\u80fd\u6bd4\u8f83\uff0c\u5efa\u7acb\u6027\u80fd\u8fb9\u754c\u5206\u6790\u6846\u67b6\u3002", "result": "\u53d1\u73b0\u9000\u76f8\u4f4d\u5f3a\u5ea6\u7ea6\u4e3a\u5149\u5b50\u635f\u8017\u5f3a\u5ea6\u76841/100\u65f6\u51fa\u73b0\u6027\u80fd\u4ea4\u53c9\u70b9\uff0c\u4e24\u7c7b\u7f16\u7801\u5728\u4e0d\u540c\u566a\u58f0 regime \u4e2d\u5c55\u73b0\u51fa\u660e\u786e\u7684\u5206\u754c\u4f18\u52bf\uff1a\u635f\u8017\u4e3b\u5bfc\u65f6GKP\u66f4\u4f18\uff0c\u9000\u76f8\u4f4d\u76f8\u5bf9\u663e\u8457\u65f6NP\u66f4\u4f18\u3002", "conclusion": "\u5de5\u4f5c\u4e0d\u4ec5\u63ed\u793a\u4e86\u4e24\u79cd\u7f16\u7801\u7684\u672c\u8d28\u9002\u7528\u8fb9\u754c\uff0c\u66f4\u5efa\u7acb\u4e86\u5b9e\u7528\u7684\u73bb\u8272\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e0e\u53c2\u6570\u4f18\u5316\u65b9\u6cd5\u8bba\uff0c\u4e3a\u5b9e\u9a8c\u4e2d\u9009\u62e9\u548c\u90e8\u7f72\u6297\u566a\u58f0\u73bb\u8272\u7f16\u7801\u63d0\u4f9b\u4e86\u5177\u4f53\u6307\u5bfc\u3002"}}
{"id": "2602.24288", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.24288", "abs": "https://arxiv.org/abs/2602.24288", "authors": ["Fan Shu", "Yite Wang", "Ruofan Wu", "Boyi Liu", "Zhewei Yao", "Yuxiong He", "Feng Yan"], "title": "DARE-bench: Evaluating Modeling and Instruction Fidelity of LLMs in Data Science", "comment": "Published as a conference paper at ICLR 2026. 10 pages plus appendix", "summary": "The fast-growing demands in using Large Language Models (LLMs) to tackle complex multi-step data science tasks create an emergent need for accurate benchmarking. There are two major gaps in existing benchmarks: (i) the lack of standardized, process-aware evaluation that captures instruction adherence and process fidelity, and (ii) the scarcity of accurately labeled training data. To bridge these gaps, we introduce DARE-bench, a benchmark designed for machine learning modeling and data science instruction following. Unlike many existing benchmarks that rely on human- or model-based judges, all tasks in DARE-bench have verifiable ground truth, ensuring objective and reproducible evaluation. To cover a broad range of tasks and support agentic tools, DARE-bench consists of 6,300 Kaggle-derived tasks and provides both large-scale training data and evaluation sets. Extensive evaluations show that even highly capable models such as gpt-o4-mini struggle to achieve good performance, especially in machine learning modeling tasks. Using DARE-bench training tasks for fine-tuning can substantially improve model performance. For example, supervised fine-tuning boosts Qwen3-32B's accuracy by 1.83x and reinforcement learning boosts Qwen3-4B's accuracy by more than 8x. These significant improvements verify the importance of DARE-bench both as an accurate evaluation benchmark and critical training data.", "AI": {"tldr": "DARE-bench\u662f\u4e00\u4e2a\u9762\u5411\u6570\u636e\u79d1\u5b66\u6307\u4ee4\u9075\u5faa\u7684\u65b0\u57fa\u51c6\uff0c\u901a\u8fc76,300\u4e2aKaggle\u4efb\u52a1\u63d0\u4f9b\u53ef\u9a8c\u8bc1ground truth\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u7f3a\u4e4f\u8fc7\u7a0b\u611f\u77e5\u8bc4\u4f30\u548c\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5b9e\u9a8c\u663e\u793a\u8be5\u57fa\u51c6\u65e2\u80fd\u6709\u6548\u8bc4\u4f30\u6a21\u578b\uff08\u5373\u4f7f\u662fgpt-o4-mini\u4e5f\u8868\u73b0\u4e0d\u4f73\uff09\uff0c\u53c8\u80fd\u4f5c\u4e3a\u5173\u952e\u8bad\u7ec3\u6570\u636e\u4f7f\u6a21\u578b\u6027\u80fd\u83b7\u5f97\u9ad8\u8fbe8\u500d\u7684\u63d0\u5347\u3002", "motivation": "LLM\u5904\u7406\u590d\u6742\u591a\u6b65\u6570\u636e\u79d1\u5b66\u4efb\u52a1\u7684\u9700\u6c42\u5feb\u901f\u589e\u957f\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u5b58\u5728\u4e24\u5927\u7f3a\u9677\uff1a(i)\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u8fc7\u7a0b\u611f\u77e5\u8bc4\u4f30\uff0c\u65e0\u6cd5\u51c6\u786e\u8861\u91cf\u6307\u4ee4\u9075\u5faa\u548c\u8fc7\u7a0b\u4fdd\u771f\u5ea6\uff1b(ii)\u51c6\u786e\u6807\u8bb0\u7684\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u3002", "method": "\u63d0\u51faDARE-bench\u57fa\u51c6\uff0c\u4e13\u4e3a\u673a\u5668\u5b66\u4e60\u5efa\u6a21\u548c\u6570\u636e\u79d1\u5b66\u6307\u4ee4\u9075\u5faa\u8bbe\u8ba1\u3002\u6240\u6709\u4efb\u52a1\u5747\u5177\u6709\u53ef\u9a8c\u8bc1\u7684ground truth\uff0c\u786e\u4fdd\u5ba2\u89c2\u53ef\u91cd\u590d\u7684\u8bc4\u4f30\u3002\u57fa\u51c6\u5305\u542b6,300\u4e2aKaggle\u884d\u751f\u4efb\u52a1\uff0c\u540c\u65f6\u63d0\u4f9b\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u548c\u8bc4\u4f30\u96c6\u3002", "result": "\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0c\u5373\u4f7fgpt-o4-mini\u7b49\u9ad8\u80fd\u529b\u6a21\u578b\u5728\u673a\u5668\u5b66\u4e60\u5efa\u6a21\u4efb\u52a1\u4e0a\u4e5f\u8868\u73b0\u4e0d\u4f73\u3002\u4f7f\u7528DARE-bench\u8bad\u7ec3\u4efb\u52a1\u8fdb\u884c\u5fae\u8c03\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\uff1a\u76d1\u7763\u5fae\u8c03\u4f7fQwen3-32B\u51c6\u786e\u7387\u63d0\u53471.83\u500d\uff0c\u5f3a\u5316\u5b66\u4e60\u4f7fQwen3-4B\u51c6\u786e\u7387\u63d0\u5347\u8d85\u8fc78\u500d\u3002", "conclusion": "\u8fd9\u4e9b\u663e\u8457\u6539\u8fdb\u9a8c\u8bc1\u4e86DARE-bench\u4f5c\u4e3a\u51c6\u786e\u8bc4\u4f30\u57fa\u51c6\u548c\u91cd\u8981\u8bad\u7ec3\u6570\u636e\u7684\u5173\u952e\u4ef7\u503c\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u7a7a\u767d\u3002"}}
{"id": "2602.23785", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23785", "abs": "https://arxiv.org/abs/2602.23785", "authors": ["Zhiwei Han", "Stefan Matthes", "Hao Shen"], "title": "Provable Subspace Identification of Nonlinear Multi-view CCA", "comment": null, "summary": "We investigate the identifiability of nonlinear Canonical Correlation Analysis (CCA) in a multi-view setup, where each view is generated by an unknown nonlinear map applied to a linear mixture of shared latents and view-private noise. Rather than attempting exact unmixing, a problem proven to be ill-posed, we instead reframe multi-view CCA as a basis-invariant subspace identification problem. We prove that, under suitable latent priors and spectral separation conditions, multi-view CCA recovers the pairwise correlated signal subspaces up to view-wise orthogonal ambiguity. For $N \\geq 3$ views, the objective provably isolates the jointly correlated subspaces shared across all views while eliminating view-private variations. We further establish finite-sample consistency guarantees by translating the concentration of empirical cross-covariances into explicit subspace error bounds via spectral perturbation theory. Experiments on synthetic and rendered image datasets validate our theoretical findings and confirm the necessity of the assumed conditions.", "AI": {"tldr": "\u672c\u6587\u89e3\u51b3\u4e86\u975e\u7ebf\u6027\u591a\u89c6\u89d2CCA\u7684\u53ef\u8bc6\u522b\u6027\u95ee\u9898\uff0c\u8bc1\u660e\u5728\u9002\u5f53\u6761\u4ef6\u4e0b\uff0c\u8be5\u65b9\u6cd5\u80fd\u5728\u6b63\u4ea4\u6b67\u4e49\u6027\u8303\u56f4\u5185\u6062\u590d\u5171\u4eab\u6f5c\u5728\u5b50\u7a7a\u95f4\uff0c\u22653\u89c6\u89d2\u65f6\u53ef\u5206\u79bb\u6240\u6709\u89c6\u89d2\u5171\u4eab\u7684\u8054\u5408\u5b50\u7a7a\u95f4\uff0c\u5e76\u83b7\u5f97\u6709\u9650\u6837\u672c\u7406\u8bba\u4fdd\u8bc1\u3002", "motivation": "\u975e\u7ebf\u6027\u591a\u89c6\u89d2CCA\u4e2d\uff0c\u6bcf\u4e2a\u89c6\u89d2\u7531\u672a\u77e5\u975e\u7ebf\u6027\u6620\u5c04\u751f\u6210\uff0c\u7cbe\u786e\u89e3\u6df7\u88ab\u8bc1\u660e\u662f\u4e0d\u9002\u5b9a\u95ee\u9898\uff0c\u9700\u8981\u65b0\u7684\u53ef\u8bc6\u522b\u6027\u7406\u8bba\u3002", "method": "\u5c06\u95ee\u9898\u91cd\u65b0\u6846\u67b6\u4e3a\u57fa\u4e0d\u53d8\u5b50\u7a7a\u95f4\u8bc6\u522b\uff0c\u5229\u7528\u8c31\u6270\u52a8\u7406\u8bba\u5c06\u7ecf\u9a8c\u4e92\u534f\u65b9\u5dee\u6d53\u5ea6\u8f6c\u5316\u4e3a\u5b50\u7a7a\u95f4\u8bef\u5dee\u754c\u3002", "result": "\u5728\u6f5c\u5728\u5148\u9a8c\u548c\u8c31\u5206\u79bb\u6761\u4ef6\u4e0b\uff0c\u65b9\u6cd5\u4ee5\u89c6\u89d2\u6b63\u4ea4\u6b67\u4e49\u6027\u6062\u590d\u76f8\u5173\u5b50\u7a7a\u95f4\uff1bN\u22653\u89c6\u89d2\u65f6\u53ef\u5206\u79bb\u5171\u4eab\u8054\u5408\u5b50\u7a7a\u95f4\u5e76\u6d88\u9664\u79c1\u6709\u53d8\u5316\uff0c\u83b7\u5f97\u6709\u9650\u6837\u672c\u4e00\u81f4\u6027\u4fdd\u8bc1\u3002", "conclusion": "\u7406\u8bba\u7ed3\u679c\u5728\u5408\u6210\u548c\u56fe\u50cf\u6570\u636e\u4e0a\u5f97\u5230\u9a8c\u8bc1\uff0c\u8bc1\u5b9e\u5047\u8bbe\u6761\u4ef6\u7684\u5fc5\u8981\u6027\u548c\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.24164", "categories": ["quant-ph", "cs.LO"], "pdf": "https://arxiv.org/pdf/2602.24164", "abs": "https://arxiv.org/abs/2602.24164", "authors": ["Anuj Dawar", "Nihil Shah"], "title": "Complexity of Satisfiability in Kochen-Specker Partial Boolean Algebras", "comment": null, "summary": "The Kochen-Specker no-go theorem established that hidden-variable theories in quantum mechanics necessarily admit contextuality. This theorem is formally stated in terms of the partial Boolean algebra structure of projectors on a Hilbert space. Each partial Boolean algebra provides a semantics for interpreting propositional logic. In this paper, we examine the complexity of propositional satisfiablity for various classes of partial Boolean algebras. We first show that the satisfiability problem for the class of non-trivial partial Boolean algebras is NP-complete. Next, we consider the satisfiability problem for the class of partial Boolean algebras arising from projectors on finite dimensional Hilbert spaces. For real Hilbert spaces of dimension greater 2 and any complex Hilbert spaces of dimension greater than 3, we demonstrate that the satisfiablity problem is complete for the existential theory of the reals. Interestingly, the proofs of these results make use of Kochen-Specker sets as gadgets. As a corollary, we conclude that deciding quantum homomorphism in these fixed dimensions are also complete for the existential theory of the reals. Finally, we show that the satisfiability problems for the class of all Hilbert spaces and all finite-dimensional Hilbert spaces is undecidable.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u91cf\u5b50\u529b\u5b66\u4e2d\u90e8\u5206\u5e03\u5c14\u4ee3\u6570\u7684\u547d\u9898\u53ef\u6ee1\u8db3\u6027\u95ee\u9898\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u8bc1\u660e\u5176\u590d\u6742\u5ea6\u968f\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u7ef4\u5ea6\u548c\u7c7b\u578b\u4e0d\u540c\uff0c\u4eceNP\u5b8c\u5168\u5230\u5b9e\u6570\u5b58\u5728\u7406\u8bba\u5b8c\u5168\uff0c\u76f4\u81f3\u4e0d\u53ef\u5224\u5b9a\u3002", "motivation": "Kochen-Specker\u5b9a\u7406\u901a\u8fc7\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u6295\u5f71\u7b97\u7b26\u7684\u90e8\u5206\u5e03\u5c14\u4ee3\u6570\u7ed3\u6784\u786e\u7acb\u4e86\u91cf\u5b50\u9690\u53d8\u91cf\u7406\u8bba\u4e2d\u7684\u8bed\u5883\u6027\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76\u8fd9\u4e9b\u91cf\u5b50\u903b\u8f91\u7ed3\u6784\u7684\u547d\u9898\u53ef\u6ee1\u8db3\u6027\u95ee\u9898\u6240\u9762\u4e34\u7684\u8ba1\u7b97\u590d\u6742\u6027\u3002", "method": "\u5206\u6790\u4e09\u7c7b\u90e8\u5206\u5e03\u5c14\u4ee3\u6570\u7684\u53ef\u6ee1\u8db3\u6027\u95ee\u9898\uff1a\u975e\u5e73\u51e1\u90e8\u5206\u5e03\u5c14\u4ee3\u6570\u3001\u6709\u9650\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u6295\u5f71\u7b97\u7b26\u751f\u6210\u7684\u90e8\u5206\u5e03\u5c14\u4ee3\u6570\uff08\u5b9e\u6570\u548c\u590d\u6570\u60c5\u5f62\uff09\uff0c\u5e76\u5728\u8bc1\u660e\u4e2d\u4f7f\u7528Kochen-Specker\u96c6\u5408\u4f5c\u4e3a\u8ba1\u7b97\u5f52\u7ea6\u7684\u88c5\u7f6e\u3002", "result": "\u4e3b\u8981\u7ed3\u679c\uff1a1) \u975e\u5e73\u51e1\u90e8\u5206\u5e03\u5c14\u4ee3\u6570\u7684\u53ef\u6ee1\u8db3\u6027\u662fNP\u5b8c\u5168\u7684\u30022) \u5b9e\u6570\u57df\u4e0a\u7ef4\u6570>2\u4e14\u590d\u6570\u57df\u4e0a\u7ef4\u6570>3\u7684\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\uff0c\u5176\u53ef\u6ee1\u8db3\u6027\u95ee\u9898\u662f\u5b9e\u6570\u5b58\u5728\u7406\u8bba\u5b8c\u5168\u7684\u30023) \u76f8\u5e94\u7ef4\u5ea6\u4e0b\u7684\u91cf\u5b50\u540c\u6001\u5224\u5b9a\u95ee\u9898\u4e5f\u5177\u6709\u76f8\u540c\u7684\u590d\u6742\u5ea6\u30024) \u6240\u6709\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u548c\u6240\u6709\u6709\u9650\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u7684\u53ef\u6ee1\u8db3\u6027\u95ee\u9898\u4e0d\u53ef\u5224\u5b9a\u3002", "conclusion": "\u91cf\u5b50\u547d\u9898\u903b\u8f91\u7684\u53ef\u6ee1\u8db3\u6027\u590d\u6742\u5ea6\u5448\u73b0\u51fa\u4e30\u5bcc\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u63ed\u793a\u4e86\u91cf\u5b50\u8bed\u5883\u6027\u4e0e\u8ba1\u7b97\u903b\u8f91\u57fa\u7840\u95ee\u9898\u4e4b\u95f4\u7684\u6df1\u523b\u8054\u7cfb\u3002\u8be5\u95ee\u9898\u5728\u4f4e\u7ef4\u65f6\u662f\u591a\u9879\u5f0f\u65f6\u95f4\u53ef\u89e3\u7684\uff0c\u4f46\u5728\u7a0d\u9ad8\u7ef4\u5ea6\u5c31\u8dc3\u5347\u81f3\u5b9e\u6570\u5b58\u5728\u7406\u8bba\u5c42\u7ea7\uff0c\u800c\u4e00\u822c\u60c5\u5f62\u4e0b\u5219\u662f\u4e0d\u53ef\u5224\u5b9a\u7684\u3002"}}
{"id": "2602.23789", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23789", "abs": "https://arxiv.org/abs/2602.23789", "authors": ["Aleksandr Ananikian", "Daniil Drozdov", "Konstantin Yakovlev"], "title": "UPath: Universal Planner Across Topological Heterogeneity For Grid-Based Pathfinding", "comment": null, "summary": "The performance of search algorithms for grid-based pathfinding, e.g. A*, critically depends on the heuristic function that is used to focus the search. Recent studies have shown that informed heuristics that take the positions/shapes of the obstacles into account can be approximated with the deep neural networks. Unfortunately, the existing learning-based approaches mostly rely on the assumption that training and test grid maps are drawn from the same distribution (e.g., city maps, indoor maps, etc.) and perform poorly on out-of-distribution tasks. This naturally limits their application in practice when often a universal solver is needed that is capable of efficiently handling any problem instance. In this work, we close this gap by designing an universal heuristic predictor: a model trained once, but capable of generalizing across a full spectrum of unseen tasks. Our extensive empirical evaluation shows that the suggested approach halves the computational effort of A* by up to a factor of 2.2, while still providing solutions within 3% of the optimal cost on average altogether on the tasks that are completely different from the ones used for training $\\unicode{x2013}$ a milestone reached for the first time by a learnable solver.", "AI": {"tldr": "This paper proposes a universal heuristic predictor for grid-based pathfinding that generalizes across different map distributions, reducing A* computation by 2.2x while staying within 3% of optimal cost.", "motivation": "Existing learning-based heuristics for A* pathfinding fail on out-of-distribution maps, requiring retraining for each map type and limiting practical deployment where a universal solver is needed.", "method": "Design a universal heuristic predictor: a deep neural network trained once to generalize across diverse unseen tasks by learning obstacle-aware heuristics that work beyond the i.i.d. distribution assumption.", "result": "Reduces A* computational effort by up to 2.2x while maintaining solutions within 3% of optimal cost on completely unseen map distributions, achieving the first successful generalization for learnable solvers.", "conclusion": "Successfully creates a learnable universal heuristic that bridges the generalization gap, marking a milestone for practical pathfinding applications requiring robust cross-distribution performance."}}
{"id": "2602.24280", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.24280", "abs": "https://arxiv.org/abs/2602.24280", "authors": ["Arnaud Coatanhay", "Ang\u00e9lique Dr\u00e9meau"], "title": "Geometric Resilience of Quantum LiDAR in Turbulent Media: A Wasserstein Distance Approach", "comment": "8 pages, 4 figures. Submitted to Physical Review A", "summary": "Quantum-enhanced LiDAR, exploiting squeezed states of light, promises significant sensitivity gains over classical protocols. However, in realistic scenarios characterized by high optical losses and atmospheric turbulence, standard figures of merit, such as quantum fidelity or the quantum Chernoff bound, saturate rapidly, failing to provide a usable gradient for system optimization. In this work, we propose the Quantum Wasserstein Distance of order 2 ($W_2$) as a robust geometric metric for lossy quantum sensing. Unlike overlap-based measures, $W_2$ quantifies the transport cost in phase space and maintains a linear response to channel transmissivity, even in regimes where the quantum state is virtually indistinguishable from thermal noise. We derive an analytical threshold for the quantum advantage, demonstrating that squeezing is only beneficial when the transmissivity exceeds a critical value determined by the environmental noise-to-signal ratio. Furthermore, using Monte-Carlo simulations of a fading channel, we show that $W_2$ acts as a high-fidelity estimator of instantaneous link quality, exhibiting a wide dynamic range immune to the numerical instabilities of fidelity-based metrics. This geometric framework bridges the gap between quantum optimal transport and practical receiver design, paving the way for adaptive sensing in scattering media.", "AI": {"tldr": "\u63d0\u51fa\u91cf\u5b50\u74e6\u745f\u65af\u5766\u8ddd\u79bb(W\u2082)\u4f5c\u4e3a\u9ad8\u635f\u8017\u91cf\u5b50\u4f20\u611f\u7684\u65b0\u5ea6\u91cf\u6807\u51c6\uff0c\u89e3\u51b3\u4f20\u7edf\u4fdd\u771f\u5ea6\u6307\u6807\u5728\u6e4d\u6d41\u73af\u5883\u4e2d\u5feb\u901f\u9971\u548c\u5931\u6548\u7684\u95ee\u9898\uff0c\u63a8\u5bfc\u51fa\u91cf\u5b50\u4f18\u52bf\u4e34\u754c\u9608\u503c\u5e76\u9a8c\u8bc1\u5176\u5728\u8870\u843d\u4fe1\u9053\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u91cf\u5b50LiDAR\u5728\u771f\u5b9e\u9ad8\u635f\u8017/\u6e4d\u6d41\u573a\u666f\u4e2d\uff0c\u6807\u51c6\u91cf\u5b50\u5ea6\u91cf(\u5982\u4fdd\u771f\u5ea6\u3001Chernoff\u754c)\u4f1a\u5feb\u901f\u9971\u548c\uff0c\u65e0\u6cd5\u63d0\u4f9b\u7cfb\u7edf\u4f18\u5316\u6240\u9700\u7684\u68af\u5ea6\u4fe1\u53f7\u3002", "method": "\u91c7\u7528\u91cf\u5b50\u6700\u4f18\u4f20\u8f93\u7406\u8bba\uff0c\u63d0\u51fa\u57fa\u4e8e\u76f8\u4f4d\u7a7a\u95f4\u4f20\u8f93\u6210\u672c\u7684W\u2082\u8ddd\u79bb\u5ea6\u91cf\uff1b\u901a\u8fc7\u89e3\u6790\u63a8\u5bfc\u786e\u5b9a\u91cf\u5b50\u4f18\u52bf\u4e34\u754c\u9608\u503c\uff1b\u5229\u7528\u8499\u7279\u5361\u6d1b\u6a21\u62df\u9a8c\u8bc1\u5176\u5728\u8870\u843d\u4fe1\u9053\u4e2d\u7684\u52a8\u6001\u8303\u56f4\u4f18\u52bf\u3002", "result": "1) W\u2082\u4fdd\u6301\u5bf9\u4fe1\u9053\u900f\u5c04\u7387\u7684\u7ebf\u6027\u54cd\u5e94(\u5373\u4f7f\u5728\u91cf\u5b50\u6001\u63a5\u8fd1\u70ed\u566a\u58f0\u65f6)\uff1b2) \u63a8\u5bfc\u51fa\u900f\u5c04\u7387\u4e34\u754c\u9608\u503c\u516c\u5f0f(\u7531\u73af\u5883\u566a\u58f0/\u4fe1\u53f7\u6bd4\u51b3\u5b9a)\uff1b3) \u5728\u8499\u7279\u5361\u6d1b\u6a21\u62df\u4e2d\u5c55\u73b0\u5bbd\u52a8\u6001\u8303\u56f4\u4e14\u65e0\u4fdd\u771f\u5ea6\u6307\u6807\u7684\u6570\u503c\u4e0d\u7a33\u5b9a\u6027\u3002", "conclusion": "W\u2082\u4e3a\u6563\u5c04\u4ecb\u8d28\u4e2d\u7684\u81ea\u9002\u5e94\u4f20\u611f\u63d0\u4f9b\u51e0\u4f55\u6846\u67b6\uff0c\u8fde\u63a5\u91cf\u5b50\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u4e0e\u5b9e\u9645\u63a5\u6536\u673a\u8bbe\u8ba1\uff0c\u63a8\u52a8\u91cf\u5b50\u4f20\u611f\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2602.23795", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23795", "abs": "https://arxiv.org/abs/2602.23795", "authors": ["Wenwu Tang", "Dong Wang", "Lothar Thiele", "Olga Saukh"], "title": "GRAIL: Post-hoc Compensation by Linear Reconstruction for Compressed Networks", "comment": "Conference on Parsimony and Learning (CPAL)", "summary": "Structured deep model compression methods are hardware-friendly and substantially reduce memory and inference costs. However, under aggressive compression, the resulting accuracy degradation often necessitates post-compression finetuning, which can be impractical due to missing labeled data or high training cost. We propose post-hoc blockwise compensation, called GRAIL, a simple zero-finetuning step applied after model compression that restores each block's input-output behavior using a small calibration set. The method summarizes hidden activations via a Gram matrix and applies ridge regression to linearly reconstruct the original hidden representation from the reduced one. The resulting reconstruction map is absorbed into the downstream projection weights, while the upstream layer is compressed. The approach is selector-agnostic (Magnitude, Wanda, Gram-based selection, or folding), data-aware (requiring only a few forward passes without gradients or labels), and recovers classic pruning or folding when the Gram matrix is near identity, indicating weak inter-channel correlations. Across ResNets, ViTs, and decoder-only LLMs, GRAIL consistently improves accuracy or perplexity over data-free and data-aware pruning or folding baselines in practical compression regimes, with manageable overhead and no backpropagation. The code is available at https://github.com/TWWinde/GRAIL.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faGRAIL\uff0c\u4e00\u79cd\u65e0\u9700\u5fae\u8c03\u7684\u5757\u7ea7\u540e\u8865\u507f\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u5c0f\u89c4\u6a21\u6821\u51c6\u96c6\u4e0a\u4f7f\u7528\u683c\u62c9\u59c6\u77e9\u9635\u548c\u5cad\u56de\u5f52\u6765\u6062\u590d\u538b\u7f29\u6a21\u578b\u7684\u7cbe\u5ea6\u3002", "motivation": "\u6fc0\u8fdb\u7684\u6a21\u578b\u538b\u7f29\u4f1a\u5bfc\u81f4\u7cbe\u5ea6\u663e\u8457\u4e0b\u964d\uff0c\u901a\u5e38\u9700\u8981\u5fae\u8c03\u6765\u6062\u590d\u6027\u80fd\uff0c\u4f46\u5fae\u8c03\u5728\u5b9e\u9645\u4e2d\u5f80\u5f80\u4e0d\u53ef\u884c\u2014\u2014\u8981\u4e48\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\uff0c\u8981\u4e48\u8bad\u7ec3\u6210\u672c\u8fc7\u9ad8\u3002\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u65e0\u9700\u6216\u4f4e\u4ee3\u4ef7\u7684\u540e\u538b\u7f29\u6062\u590d\u65b9\u6cd5\u3002", "method": "GRAIL\u5728\u538b\u7f29\u540e\u5e94\u7528\u96f6\u5fae\u8c03\u6b65\u9aa4\u3002\u5bf9\u6bcf\u4e2a\u5757\uff0c\u4f7f\u7528\u5c0f\u89c4\u6a21\u6821\u51c6\u96c6\u8ba1\u7b97\u9690\u85cf\u6fc0\u6d3b\u7684\u683c\u62c9\u59c6\u77e9\u9635\uff0c\u7136\u540e\u901a\u8fc7\u5cad\u56de\u5f52\u5b66\u4e60\u4ece\u538b\u7f29\u8868\u793a\u5230\u539f\u59cb\u8868\u793a\u7684\u7ebf\u6027\u91cd\u6784\u6620\u5c04\u3002\u8be5\u6620\u5c04\u88ab\u5438\u6536\u5230\u4e0b\u6e38\u6295\u5f71\u6743\u91cd\u4e2d\u3002\u8be5\u65b9\u6cd5\u5bf9\u9009\u62e9\u5668\u4e0d\u53ef\u77e5\uff08\u652f\u6301\u5e45\u5ea6\u3001Wanda\u3001Gram-based\u6216\u6298\u53e0\uff09\uff0c\u4ec5\u9700\u524d\u5411\u4f20\u64ad\u4e14\u65e0\u9700\u68af\u5ea6\u6216\u6807\u7b7e\u3002", "result": "\u5728ResNet\u3001ViT\u548c\u89e3\u7801\u5668-only\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGRAIL\u5728\u5b9e\u7528\u538b\u7f29 regime \u4e0b\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65e0\u6570\u636e\u548c\u542b\u6570\u636e\u526a\u679d/\u6298\u53e0\u57fa\u7ebf\uff0c\u7cbe\u5ea6\u6216\u56f0\u60d1\u5ea6\u5f97\u5230\u63d0\u5347\uff0c\u5f00\u9500\u53ef\u63a7\u4e14\u65e0\u9700\u53cd\u5411\u4f20\u64ad\u3002", "conclusion": "GRAIL\u662f\u4e00\u79cd\u6709\u6548\u4e14\u5b9e\u7528\u7684\u540e\u538b\u7f29\u8865\u507f\u65b9\u6cd5\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u6062\u590d\u6a21\u578b\u6027\u80fd\uff0c\u4f7f\u6fc0\u8fdb\u538b\u7f29\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u66f4\u52a0\u53ef\u884c\u3002"}}
{"id": "2602.23798", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.23798", "abs": "https://arxiv.org/abs/2602.23798", "authors": ["Tiantong Wang", "Xinyu Yan", "Tiantong Wu", "Yurong Hao", "Yong Jiang", "Fei Huang", "Wei Yang Bryan Lim"], "title": "MPU: Towards Secure and Privacy-Preserving Knowledge Unlearning for Large Language Models", "comment": null, "summary": "Machine unlearning for large language models often faces a privacy dilemma in which strict constraints prohibit sharing either the server's parameters or the client's forget set. To address this dual non-disclosure constraint, we propose MPU, an algorithm-agnostic privacy-preserving Multiple Perturbed Copies Unlearning framework that primarily introduces two server-side modules: Pre-Process for randomized copy generation and Post-Process for update aggregation. In Pre-Process, the server distributes multiple perturbed and reparameterized model instances, allowing the client to execute unlearning locally on its private forget set without accessing the server's exact original parameters. After local unlearning, the server performs Post-Process by inverting the reparameterization and aggregating updates with a harmonic denoising procedure to alleviate the impact of perturbation. Experiments with seven unlearning algorithms show that MPU achieves comparable unlearning performance to noise-free baselines, with most algorithms' average degradation well below 1% under 10% noise, and can even outperform the noise-free baseline for some algorithms under 1% noise. Code is available at https://github.com/Tristan-SHU/MPU.", "AI": {"tldr": "MPU\u662f\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u673a\u5668\u9057\u5fd8\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4e2a\u6270\u52a8\u6a21\u578b\u526f\u672c\u5b9e\u73b0\u65e0\u9700\u5171\u4eab\u670d\u52a1\u5668\u53c2\u6570\u6216\u5ba2\u6237\u7aef\u9057\u5fd8\u96c6\u7684\u672c\u5730\u9057\u5fd8\uff0c\u6027\u80fd\u63a5\u8fd1\u65e0\u566a\u58f0\u57fa\u7ebf\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u673a\u5668\u9057\u5fd8\u9762\u4e34\u9690\u79c1\u56f0\u5883\uff1a\u4e25\u683c\u9650\u5236\u7981\u6b62\u5171\u4eab\u670d\u52a1\u5668\u53c2\u6570\u6216\u5ba2\u6237\u7aef\u9057\u5fd8\u96c6\uff08\u53cc\u91cd\u975e\u62ab\u9732\u7ea6\u675f\uff09\u3002", "method": "\u63d0\u51faMPU\u7b97\u6cd5\u65e0\u5173\u7684\u9690\u79c1\u4fdd\u62a4\u591a\u6270\u52a8\u526f\u672c\u9057\u5fd8\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u670d\u52a1\u5668\u7aef\u6a21\u5757\uff1a\u9884\u5904\u7406\uff08\u968f\u673a\u5316\u526f\u672c\u751f\u6210\uff0c\u5206\u53d1\u591a\u4e2a\u6270\u52a8\u4e14\u91cd\u53c2\u6570\u5316\u7684\u6a21\u578b\u5b9e\u4f8b\uff09\u548c\u540e\u7eed\u5904\u7406\uff08\u91cd\u53c2\u6570\u5316\u9006\u53d8\u6362\u4e0e\u8c10\u6ce2\u53bb\u566a\u805a\u5408\u66f4\u65b0\uff09\u3002", "result": "\u4e03\u4e2a\u9057\u5fd8\u7b97\u6cd5\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMPU\u6027\u80fd\u4e0e\u65e0\u566a\u58f0\u57fa\u7ebf\u76f8\u5f53\uff0c\u572810%\u566a\u58f0\u4e0b\u5927\u591a\u6570\u7b97\u6cd5\u5e73\u5747\u9000\u5316\u4f4e\u4e8e1%\uff0c\u57281%\u566a\u58f0\u4e0b\u67d0\u4e9b\u7b97\u6cd5\u751a\u81f3\u4f18\u4e8e\u65e0\u566a\u58f0\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u6210\u529f\u89e3\u51b3\u53cc\u91cd\u975e\u62ab\u9732\u9690\u79c1\u7ea6\u675f\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u4f18\u5f02\u7684\u9057\u5fd8\u6027\u80fd\u3002"}}
{"id": "2602.23811", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23811", "abs": "https://arxiv.org/abs/2602.23811", "authors": ["Xiang Li", "Nan Jiang", "Yuheng Zhang"], "title": "Beyond State-Wise Mirror Descent: Offline Policy Optimization with Parameteric Policies", "comment": null, "summary": "We investigate the theoretical aspects of offline reinforcement learning (RL) under general function approximation. While prior works (e.g., Xie et al., 2021) have established the theoretical foundations of learning a good policy from offline data via pessimism, existing algorithms that are computationally tractable (often in an oracle-efficient sense), such as PSPI, only apply to finite and small action spaces. Moreover, these algorithms rely on state-wise mirror descent and require actors to be implicitly induced from the critic functions, failing to accommodate standalone policy parameterization which is ubiquitous in practice. In this work, we address these limitations and extend the theoretical guarantees to parameterized policy classes over large or continuous action spaces. When extending mirror descent to parameterized policies, we identify contextual coupling as the core difficulty, and show how connecting mirror descent to natural policy gradient leads to novel analyses, guarantees, and algorithmic insights, including a surprising unification between offline RL and imitation learning.", "AI": {"tldr": "This paper extends offline RL theory to parameterized policies in large/continuous action spaces by connecting mirror descent to natural policy gradient, revealing a novel unification between offline RL and imitation learning while overcoming prior limitations.", "motivation": "Existing offline RL algorithms (like PSPI) only work for finite/small action spaces and fail to accommodate practical standalone policy parameterization. The core challenge is extending theoretical guarantees to parameterized policy classes over large or continuous action spaces.", "method": "The authors extend mirror descent to parameterized policies, identify contextual coupling as the key difficulty, and connect mirror descent to natural policy gradient to derive novel analyses, guarantees, and algorithms.", "result": "The work provides theoretical guarantees for offline RL with parameterized policies in large/continuous action spaces, offers novel algorithmic insights, and discovers a surprising unification between offline RL and imitation learning.", "conclusion": "This research bridges the gap between theoretical offline RL foundations and practical implementations with parameterized policies, enabling broader applicability while revealing fundamental connections between offline RL and imitation learning."}}
{"id": "2602.23816", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23816", "abs": "https://arxiv.org/abs/2602.23816", "authors": ["George Papadopoulos", "George A. Vouros"], "title": "Learning to maintain safety through expert demonstrations in settings with unknown constraints: A Q-learning perspective", "comment": "Accepted for publication at AAMAS 2026", "summary": "Given a set of trajectories demonstrating the execution of a task safely in a constrained MDP with observable rewards but with unknown constraints and non-observable costs, we aim to find a policy that maximizes the likelihood of demonstrated trajectories trading the balance between being conservative and increasing significantly the likelihood of high-rewarding trajectories but with potentially unsafe steps. Having these objectives, we aim towards learning a policy that maximizes the probability of the most $promising$ trajectories with respect to the demonstrations. In so doing, we formulate the ``promise\" of individual state-action pairs in terms of $Q$ values, which depend on task-specific rewards as well as on the assessment of states' safety, mixing expectations in terms of rewards and safety. This entails a safe Q-learning perspective of the inverse learning problem under constraints: The devised Safe $Q$ Inverse Constrained Reinforcement Learning (SafeQIL) algorithm is compared to state-of-the art inverse constraint reinforcement learning algorithms to a set of challenging benchmark tasks, showing its merits.", "AI": {"tldr": "\u63d0\u51faSafeQIL\u7b97\u6cd5\uff0c\u901a\u8fc7\u5b89\u5168Q\u5b66\u4e60\u89e3\u51b3\u7ea6\u675f\u9006\u5411\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u4ece\u6f14\u793a\u8f68\u8ff9\u4e2d\u5b66\u4e60\u517c\u987e\u5956\u52b1\u6700\u5927\u5316\u548c\u5b89\u5168\u6027\u7684\u7b56\u7565", "motivation": "\u5728\u672a\u77e5\u7ea6\u675f\u548c\u90e8\u5206\u53ef\u89c2\u4ee3\u4ef7\u7684\u53d7\u9650MDP\u4e2d\uff0c\u4ece\u5b89\u5168\u6f14\u793a\u8f68\u8ff9\u5b66\u4e60\u7b56\u7565\uff0c\u9700\u5e73\u8861\u4fdd\u5b88\u6027\u4e0e\u9ad8\u56de\u62a5\u4f46\u53ef\u80fd\u4e0d\u5b89\u5168\u7684\u52a8\u4f5c\uff0c\u6700\u5927\u5316\"\u6709\u524d\u666f\"\u8f68\u8ff9\u7684\u6982\u7387", "method": "\u57fa\u4e8eQ\u503c\u5b9a\u4e49\u72b6\u6001-\u52a8\u4f5c\u5bf9\u7684\"\u524d\u666f\"\uff0c\u878d\u5408\u4efb\u52a1\u5956\u52b1\u548c\u72b6\u6001\u5b89\u5168\u8bc4\u4f30\uff0c\u63d0\u51faSafe Q Inverse Constrained Reinforcement Learning (SafeQIL)\u7b97\u6cd5", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u4efb\u52a1\u4e0a\uff0c\u4e0e\u5148\u8fdb\u9006\u5411\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u76f8\u6bd4\u5c55\u73b0\u51fa\u4f18\u52bf", "conclusion": "SafeQIL\u7b97\u6cd5\u80fd\u6709\u6548\u4ece\u6f14\u793a\u4e2d\u5b66\u4e60\u5b89\u5168\u4e14\u9ad8\u6027\u80fd\u7684\u7b56\u7565\uff0c\u5728\u7ea6\u675f\u672a\u77e5\u73af\u5883\u4e0b\u5177\u6709\u663e\u8457\u4f18\u52bf"}}
{"id": "2602.24220", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.24220", "abs": "https://arxiv.org/abs/2602.24220", "authors": ["Miras Seilkhan", "Adilbek Taizhanov"], "title": "Comparing Classical and Quantum Variational Classifiers on the XOR Problem", "comment": "32 pages, 17 figures. Code and experiment scripts available at https://github.com/mseilkhan/XOR-research-Quantum-ML-vs-Classic", "summary": "Quantum machine learning applies principles such as superposition and entanglement to data processing and optimization. Variational quantum models operate on qubits in high-dimensional Hilbert spaces and provide an alternative approach to model expressivity. We compare classical models and a variational quantum classifier on the XOR problem. Logistic regression, a one-hidden-layer multilayer perceptron, and a two-qubit variational quantum classifier with circuit depths 1 and 2 are evaluated on synthetic XOR datasets with varying Gaussian noise and sample sizes using accuracy and binary cross-entropy.\n  Performance is determined primarily by model expressivity. Logistic regression and the depth-1 quantum circuit fail to represent XOR reliably, whereas the multilayer perceptron and the depth-2 quantum circuit achieve perfect test accuracy under representative conditions. Robustness analyses across noise levels, dataset sizes, and random seeds confirm that circuit depth is decisive for quantum performance on this task. Despite matching accuracy, the multilayer perceptron achieves lower binary cross-entropy and substantially shorter training time. Hardware execution preserves the global XOR structure but introduces structured deviations in the decision function. Overall, deeper variational quantum classifiers can match classical neural networks in accuracy on low-dimensional XOR benchmarks, but no clear empirical advantage in robustness or efficiency is observed in the examined settings.", "AI": {"tldr": "This paper compares variational quantum classifiers (with circuit depths 1 and 2) against classical models (logistic regression and MLP) on the XOR problem. While depth-2 quantum circuits match MLP's perfect accuracy under ideal conditions, they show higher loss values, longer training times, and no robustness/efficiency advantages over classical methods. Circuit depth is critical for quantum performance, but no empirical quantum advantage is observed.", "motivation": "To evaluate whether variational quantum models can match or surpass classical neural networks in expressivity and robustness on the fundamental non-linear XOR problem, testing quantum machine learning's practical potential against established classical baselines.", "method": "Systematic comparison of logistic regression, one-hidden-layer MLP, and two-qubit variational quantum classifiers (depths 1/2) on synthetic XOR datasets. Models are evaluated across varying Gaussian noise levels, sample sizes, and random seeds using accuracy and binary cross-entropy metrics, with hardware execution validation.", "result": "Depth-2 quantum circuits achieve perfect test accuracy matching MLP, while logistic regression and depth-1 quantum circuits fail. However, MLP exhibits lower binary cross-entropy and significantly faster training. Quantum hardware preserves XOR structure but introduces structured decision function deviations. No quantum advantage in robustness or efficiency is observed.", "conclusion": "Deeper variational quantum classifiers can match classical neural networks in accuracy on low-dimensional XOR tasks, but they do not demonstrate superior robustness, efficiency, or loss performance. Circuit depth is essential for quantum expressivity, yet classical methods remain more practical for this benchmark."}}
{"id": "2602.23824", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23824", "abs": "https://arxiv.org/abs/2602.23824", "authors": ["Pavlin G. Poli\u010dar", "Dalibor Stanimirovi\u0107", "Bla\u017e Zupan"], "title": "Inferring Chronic Treatment Onset from ePrescription Data: A Renewal Process Approach", "comment": null, "summary": "Longitudinal electronic health record (EHR) data are often left-censored, making diagnosis records incomplete and unreliable for determining disease onset. In contrast, outpatient prescriptions form renewal-based trajectories that provide a continuous signal of disease management. We propose a probabilistic framework to infer chronic treatment onset by modeling prescription dynamics as a renewal process and detecting transitions from sporadic to sustained therapy via change-point detection between a baseline Poisson (sporadic prescribing) regime and a regime-specific Weibull (sustained therapy) renewal model. Using a nationwide ePrescription dataset of 2.4 million individuals, we show that the approach yields more temporally plausible onset estimates than naive rule-based triggering, substantially reducing implausible early detections under strong left censoring. Detection performance varies across diseases and is strongly associated with prescription density, highlighting both the strengths and limits of treatment-based onset inference.", "AI": {"tldr": "\u9488\u5bf9EHR\u6570\u636e\u5de6\u5220\u5931\u95ee\u9898\uff0c\u672c\u7814\u7a76\u5229\u7528\u95e8\u8bca\u5904\u65b9\u7eed\u671f\u8f68\u8ff9\uff0c\u63d0\u51fa\u57fa\u4e8e\u7eed\u671f\u8fc7\u7a0b\u548c\u53d8\u70b9\u68c0\u6d4b\u7684\u6982\u7387\u6846\u67b6\u6765\u63a8\u65ad\u6162\u6027\u6cbb\u7597\u8d77\u59cb\u65f6\u95f4\u3002\u5728240\u4e07\u4eba\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u6bd4\u4f20\u7edf\u89c4\u5219\u65b9\u6cd5\u66f4\u51c6\u786e\uff0c\u6027\u80fd\u53d7\u5904\u65b9\u5bc6\u5ea6\u5f71\u54cd\u3002", "motivation": "\u7eb5\u5411\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55(EHR)\u6570\u636e\u901a\u5e38\u5b58\u5728\u5de6\u5220\u5931\u95ee\u9898\uff0c\u5bfc\u81f4\u8bca\u65ad\u8bb0\u5f55\u4e0d\u5b8c\u6574\u4e14\u4e0d\u53ef\u9760\uff0c\u96be\u4ee5\u51c6\u786e\u5224\u65ad\u75be\u75c5\u53d1\u75c5\u65f6\u95f4\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u95e8\u8bca\u5904\u65b9\u5f62\u6210\u57fa\u4e8e\u7eed\u671f\u7684\u8f68\u8ff9\uff0c\u63d0\u4f9b\u4e86\u75be\u75c5\u7ba1\u7406\u7684\u8fde\u7eed\u4fe1\u53f7\uff0c\u4f46\u9700\u8981\u66f4\u597d\u7684\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6982\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5904\u65b9\u52a8\u6001\u5efa\u6a21\u4e3a\u7eed\u671f\u8fc7\u7a0b\uff0c\u5e76\u91c7\u7528\u53d8\u70b9\u68c0\u6d4b\u65b9\u6cd5\u5728\u57fa\u7ebf\u6cca\u677e\u5206\u5e03\uff08\u96f6\u661f\u5904\u65b9\uff09\u548c\u7279\u5b9a\u5a01\u5e03\u5c14\u5206\u5e03\uff08\u6301\u7eed\u6cbb\u7597\uff09\u7eed\u671f\u6a21\u578b\u4e4b\u95f4\u68c0\u6d4b\u4ece\u96f6\u661f\u5230\u6301\u7eed\u6cbb\u7597\u7684\u8f6c\u53d8\uff0c\u4ece\u800c\u63a8\u65ad\u6162\u6027\u6cbb\u7597\u8d77\u59cb\u65f6\u95f4\u3002", "result": "\u4f7f\u7528240\u4e07\u4eba\u7684\u5168\u56fd\u6027\u7535\u5b50\u5904\u65b9\u6570\u636e\u96c6\u9a8c\u8bc1\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u6bd4\u7b80\u5355\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u89e6\u53d1\u65b9\u6cd5\u4ea7\u751f\u66f4\u5408\u7406\u7684\u65f6\u95f4\u8d77\u59cb\u4f30\u8ba1\uff0c\u5728\u5f3a\u5de6\u5220\u5931\u60c5\u51b5\u4e0b\u663e\u8457\u51cf\u5c11\u4e86\u4e0d\u5408\u7406\u65e9\u671f\u68c0\u6d4b\u3002\u68c0\u6d4b\u6027\u80fd\u56e0\u75be\u75c5\u800c\u5f02\uff0c\u4e14\u4e0e\u5904\u65b9\u5bc6\u5ea6\u5bc6\u5207\u76f8\u5173\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7a81\u51fa\u4e86\u57fa\u4e8e\u6cbb\u7597\u7684\u53d1\u75c5\u63a8\u65ad\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u8868\u660e\u5176\u6709\u6548\u6027\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u5904\u65b9\u6570\u636e\u7684\u5bc6\u5ea6\u3002"}}
{"id": "2602.23827", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23827", "abs": "https://arxiv.org/abs/2602.23827", "authors": ["Junkang Liu", "Fanhua Shang", "Yuxuan Tian", "Hongying Liu", "Yuanyuan Liu"], "title": "FedNSAM:Consistency of Local and Global Flatness for Federated Learning", "comment": null, "summary": "In federated learning (FL), multi-step local updates and data heterogeneity usually lead to sharper global minima, which degrades the performance of the global model. Popular FL algorithms integrate sharpness-aware minimization (SAM) into local training to address this issue. However, in the high data heterogeneity setting, the flatness in local training does not imply the flatness of the global model. Therefore, minimizing the sharpness of the local loss surfaces on the client data does not enable the effectiveness of SAM in FL to improve the generalization ability of the global model. We define the \\textbf{flatness distance} to explain this phenomenon. By rethinking the SAM in FL and theoretically analyzing the \\textbf{flatness distance}, we propose a novel \\textbf{FedNSAM} algorithm that accelerates the SAM algorithm by introducing global Nesterov momentum into the local update to harmonize the consistency of global and local flatness. \\textbf{FedNSAM} uses the global Nesterov momentum as the direction of local estimation of client global perturbations and extrapolation. Theoretically, we prove a tighter convergence bound than FedSAM by Nesterov extrapolation. Empirically, we conduct comprehensive experiments on CNN and Transformer models to verify the superior performance and efficiency of \\textbf{FedNSAM}. The code is available at https://github.com/junkangLiu0/FedNSAM.", "AI": {"tldr": "\u9488\u5bf9\u8054\u90a6\u5b66\u4e60\u4e2d\u6570\u636e\u5f02\u8d28\u6027\u5bfc\u81f4\u5168\u5c40\u6a21\u578b\u5c16\u9510\u6700\u5c0f\u503c\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faFedNSAM\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5168\u5c40Nesterov\u52a8\u91cf\u6765\u534f\u8c03\u5168\u5c40\u4e0e\u5c40\u90e8\u5e73\u5766\u5ea6\u7684\u4e00\u81f4\u6027\uff0c\u5728\u7406\u8bba\u4e0a\u83b7\u5f97\u66f4\u7d27\u7684\u6536\u655b\u754c\uff0c\u5728\u5b9e\u9a8c\u4e0a\u5c55\u73b0\u51fa\u66f4\u4f18\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\uff0c\u591a\u6b65\u672c\u5730\u66f4\u65b0\u548c\u6570\u636e\u5f02\u8d28\u6027\u901a\u5e38\u4f1a\u5bfc\u81f4\u66f4\u5c16\u9510\u7684\u5168\u5c40\u6700\u5c0f\u503c\uff0c\u4ece\u800c\u964d\u4f4e\u5168\u5c40\u6a21\u578b\u7684\u6027\u80fd\u3002\u867d\u7136\u73b0\u6709\u7b97\u6cd5\u5c06\u9510\u5ea6\u611f\u77e5\u6700\u5c0f\u5316\uff08SAM\uff09\u96c6\u6210\u5230\u672c\u5730\u8bad\u7ec3\u4e2d\uff0c\u4f46\u5728\u9ad8\u6570\u636e\u5f02\u8d28\u6027\u8bbe\u7f6e\u4e0b\uff0c\u5c40\u90e8\u8bad\u7ec3\u7684\u5e73\u5766\u5ea6\u5e76\u4e0d\u610f\u5473\u7740\u5168\u5c40\u6a21\u578b\u7684\u5e73\u5766\u5ea6\uff0c\u56e0\u6b64\u65e0\u6cd5\u6709\u6548\u63d0\u5347\u5168\u5c40\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684FedNSAM\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c06\u5168\u5c40Nesterov\u52a8\u91cf\u5f15\u5165\u672c\u5730\u66f4\u65b0\u6765\u534f\u8c03\u5168\u5c40\u4e0e\u5c40\u90e8\u5e73\u5766\u5ea6\u7684\u4e00\u81f4\u6027\u3002\u8be5\u7b97\u6cd5\u4f7f\u7528\u5168\u5c40Nesterov\u52a8\u91cf\u4f5c\u4e3a\u5ba2\u6237\u7aef\u5168\u5c40\u6270\u52a8\u548c extrapolation \u7684\u5c40\u90e8\u4f30\u8ba1\u65b9\u5411\u3002", "result": "\u7406\u8bba\u65b9\u9762\uff0c\u901a\u8fc7Nesterov\u5916\u63a8\u6cd5\u8bc1\u660e\u4e86\u6bd4FedSAM\u66f4\u7d27\u7684\u6536\u655b\u754c\uff1b\u5b9e\u8bc1\u65b9\u9762\uff0c\u5728CNN\u548cTransformer\u6a21\u578b\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u9a8c\u8bc1\u4e86FedNSAM\u7684\u4f18\u8d8a\u6027\u80fd\u548c\u6548\u7387\u3002", "conclusion": "FedNSAM\u7b97\u6cd5\u901a\u8fc7\u534f\u8c03\u5168\u5c40\u4e0e\u5c40\u90e8\u5e73\u5766\u5ea6\u7684\u4e00\u81f4\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u6570\u636e\u5f02\u8d28\u6027\u4e0b\u7684\u8054\u90a6\u5b66\u4e60\u4f18\u5316\u95ee\u9898\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u66f4\u7d27\u7684\u7406\u8bba\u4fdd\u8bc1\u548c\u66f4\u597d\u7684\u5b9e\u9645\u8868\u73b0\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.23852", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.23852", "abs": "https://arxiv.org/abs/2602.23852", "authors": ["Zhaowen Wang", "Dongdong Zhou", "Qi Xu", "Fengyu Cong", "Mohammad Al-Sa'd", "Jenni Raitoharju"], "title": "ULW-SleepNet: An Ultra-Lightweight Network for Multimodal Sleep Stage Scoring", "comment": "Accepted to ICASSP 2026", "summary": "Automatic sleep stage scoring is crucial for the diagnosis and treatment of sleep disorders. Although deep learning models have advanced the field, many existing models are computationally demanding and designed for single-channel electroencephalography (EEG), limiting their practicality for multimodal polysomnography (PSG) data. To overcome this, we propose ULW-SleepNet, an ultra-lightweight multimodal sleep stage scoring framework that efficiently integrates information from multiple physiological signals. ULW-SleepNet incorporates a novel Dual-Stream Separable Convolution (DSSC) Block, depthwise separable convolutions, channel-wise parameter sharing, and global average pooling to reduce computational overhead while maintaining competitive accuracy. Evaluated on the Sleep-EDF-20 and Sleep-EDF-78 datasets, ULW-SleepNet achieves accuracies of 86.9% and 81.4%, respectively, with only 13.3K parameters and 7.89M FLOPs. Compared to state-of-the-art methods, our model reduces parameters by up to 98.6% with only marginal performance loss, demonstrating its strong potential for real-time sleep monitoring on wearable and IoT devices. The source code for this study is publicly available at https://github.com/wzw999/ULW-SLEEPNET.", "AI": {"tldr": "\u63d0\u51faULW-SleepNet\u8d85\u8f7b\u91cf\u591a\u6a21\u6001\u7761\u7720\u5206\u671f\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0DSSC\u6a21\u5757\u548c\u53c2\u6570\u4f18\u5316\u6280\u672f\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\uff0886.9%/81.4%\uff09\u7684\u540c\u65f6\u5c06\u53c2\u6570\u91cf\u538b\u7f29\u81f313.3K\uff0c\u8f83SOTA\u51cf\u5c1198.6%\uff0c\u9002\u7528\u4e8e\u53ef\u7a7f\u6234\u8bbe\u5907\u5b9e\u65f6\u76d1\u6d4b", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u7761\u7720\u5206\u671f\u6a21\u578b\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u4e14\u4ec5\u652f\u6301\u5355\u901a\u9053EEG\uff0c\u96be\u4ee5\u5904\u7406\u591a\u6a21\u6001PSG\u6570\u636e\u5e76\u90e8\u7f72\u4e8e\u8d44\u6e90\u53d7\u9650\u7684 wearable/IoT \u8bbe\u5907", "method": "\u8bbe\u8ba1ULW-SleepNet\u6846\u67b6\uff1a\u91c7\u7528\u53cc\u901a\u9053\u53ef\u5206\u79bb\u5377\u79ef(DSSC)\u6a21\u5757\u3001\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u3001\u901a\u9053\u7ea7\u53c2\u6570\u5171\u4eab\u548c\u5168\u5c40\u5e73\u5747\u6c60\u5316\u6280\u672f\uff0c\u5b9e\u73b0\u591a\u751f\u7406\u4fe1\u53f7\u9ad8\u6548\u878d\u5408\u4e0e\u8ba1\u7b97\u5f00\u9500\u4f18\u5316", "result": "\u5728Sleep-EDF-20\u548cSleep-EDF-78\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523086.9%\u548c81.4%\u51c6\u786e\u7387\uff0c\u4ec5\u542b13.3K\u53c2\u6570\u548c7.89M FLOPs\uff0c\u8f83SOTA\u65b9\u6cd5\u53c2\u6570\u51cf\u5c1198.6%\u4e14\u7cbe\u5ea6\u635f\u5931\u6781\u5c0f", "conclusion": "\u8be5\u6a21\u578b\u663e\u8457\u5e73\u8861\u4e86\u7cbe\u5ea6\u4e0e\u6548\u7387\uff0c\u4e3a\u53ef\u7a7f\u6234\u8bbe\u5907\u5b9e\u65f6\u7761\u7720\u76d1\u6d4b\u63d0\u4f9b\u53ef\u884c\u65b9\u6848\uff0c\u5f00\u6e90\u4ee3\u7801\u5c06\u4fc3\u8fdb\u4e34\u5e8a\u5e94\u7528\u843d\u5730"}}
{"id": "2602.23880", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23880", "abs": "https://arxiv.org/abs/2602.23880", "authors": ["Zhang Wan", "Tingting Mu", "Samuel Kaski"], "title": "A Theory of Random Graph Shift in Truncated-Spectrum vRKHS", "comment": null, "summary": "This paper develops a theory of graph classification under domain shift through a random-graph generative lens, where we consider intra-class graphs sharing the same random graph model (RGM) and the domain shift induced by changes in RGM components. While classic domain adaptation (DA) theories have well-underpinned existing techniques to handle graph distribution shift, the information of graph samples, which are itself structured objects, is less explored. The non-Euclidean nature of graphs and specialized architectures for graph learning further complicate a fine-grained analysis of graph distribution shifts. In this paper, we propose a theory that assumes RGM as the data generative process, exploiting its connection to hypothesis complexity in function space perspective for such fine-grained analysis. Building on a vector-valued reproducing kernel Hilbert space (vRKHS) formulation, we derive a generalization bound whose shift penalty admits a factorization into (i) a domain discrepancy term, (ii) a spectral-geometry term summarized by the accessible truncated spectrum, and (iii) an amplitude term that aggregates convergence and construction-stability effects. We empirically verify the insights on these terms in both real data and simulations.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u968f\u673a\u56fe\u6a21\u578b(RGM)\u7684\u56fe\u5206\u7c7b\u57df\u504f\u79fb\u7406\u8bba\u6846\u67b6\uff0c\u63a8\u5bfc\u51fa\u53ef\u5206\u89e3\u4e3a\u57df\u5dee\u5f02\u3001\u8c31\u51e0\u4f55\u548c\u5e45\u5ea6\u4e09\u9879\u7684\u6cdb\u5316\u754c", "motivation": "\u7ecf\u5178\u57df\u9002\u5e94\u7406\u8bba\u672a\u5145\u5206\u5229\u7528\u56fe\u6837\u672c\u7684\u7ed3\u6784\u4fe1\u606f\uff0c\u4e14\u56fe\u7684Non-Euclidean\u7279\u6027\u4f7f\u7ec6\u7c92\u5ea6\u5206\u6790\u590d\u6742\u5316", "method": "\u5047\u8bbeRGM\u4e3a\u6570\u636e\u751f\u6210\u8fc7\u7a0b\uff0c\u57fa\u4e8e\u5411\u91cf\u503c\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4(vRKHS)\u5efa\u7acb\u7406\u8bba\uff0c\u63a8\u5bfc\u6cdb\u5316\u754c", "result": "\u6cdb\u5316\u754c\u7684\u504f\u79fb\u60e9\u7f5a\u53ef\u5206\u89e3\u4e3a\uff1a(i)\u57df\u5dee\u5f02\u9879(ii)\u53ef\u53ca\u622a\u65ad\u8c31\u603b\u7ed3\u7684\u8c31\u51e0\u4f55\u9879(iii)\u6536\u655b\u4e0e\u6784\u9020\u7a33\u5b9a\u6027\u5e45\u5ea6\u9879", "conclusion": "\u8be5\u7406\u8bba\u5b9e\u73b0\u4e86\u5bf9\u56fe\u5206\u5e03\u504f\u79fb\u7684\u7ec6\u7c92\u5ea6\u5206\u6790\uff0c\u5e76\u901a\u8fc7\u771f\u5b9e\u6570\u636e\u548c\u6a21\u62df\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u6d1e\u5bdf"}}
{"id": "2602.23981", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23981", "abs": "https://arxiv.org/abs/2602.23981", "authors": ["Xianglong Shi", "Ziheng Chen", "Yunhan Jiang", "Nicu Sebe"], "title": "Intrinsic Lorentz Neural Network", "comment": "Published in ICLR 2026", "summary": "Real-world data frequently exhibit latent hierarchical structures, which can be naturally represented by hyperbolic geometry. Although recent hyperbolic neural networks have demonstrated promising results, many existing architectures remain partially intrinsic, mixing Euclidean operations with hyperbolic ones or relying on extrinsic parameterizations. To address it, we propose the \\emph{Intrinsic Lorentz Neural Network} (ILNN), a fully intrinsic hyperbolic architecture that conducts all computations within the Lorentz model. At its core, the network introduces a novel \\emph{point-to-hyperplane} fully connected layer (FC), replacing traditional Euclidean affine logits with closed-form hyperbolic distances from features to learned Lorentz hyperplanes, thereby ensuring that the resulting geometric decision functions respect the inherent curvature. Around this fundamental layer, we design intrinsic modules: GyroLBN, a Lorentz batch normalization that couples gyro-centering with gyro-scaling, consistently outperforming both LBN and GyroBN while reducing training time. We additionally proposed a gyro-additive bias for the FC output, a Lorentz patch-concatenation operator that aligns the expected log-radius across feature blocks via a digamma-based scale, and a Lorentz dropout layer. Extensive experiments conducted on CIFAR-10/100 and two genomic benchmarks (TEB and GUE) illustrate that ILNN achieves state-of-the-art performance and computational cost among hyperbolic models and consistently surpasses strong Euclidean baselines. The code is available at \\href{https://github.com/Longchentong/ILNN}{\\textcolor{magenta}{this url}}.", "AI": {"tldr": "The paper proposes ILNN, a fully intrinsic hyperbolic neural network architecture using the Lorentz model that introduces novel components like point-to-hyperplane layers and GyroLBN, achieving state-of-the-art performance on CIFAR and genomic benchmarks.", "motivation": "Real-world data exhibits hierarchical structures naturally represented by hyperbolic geometry, but existing hyperbolic networks are only partially intrinsic by mixing Euclidean/hyperbolic operations or using extrinsic parameterizations.", "method": "ILNN is a fully intrinsic hyperbolic architecture conducting all computations in the Lorentz model. Key innovations: 1) point-to-hyperplane FC layer using hyperbolic distances to Lorentz hyperplanes, 2) GyroLBN batch normalization coupling gyro-centering/scaling, 3) gyro-additive bias, 4) Lorentz patch-concatenation with digamma-based scaling, and 5) Lorentz dropout.", "result": "Experiments on CIFAR-10/100 and genomic benchmarks (TEB/GUE) show ILNN achieves state-of-the-art performance among hyperbolic models, consistently surpasses Euclidean baselines, and improves computational efficiency while reducing training time.", "conclusion": "ILNN successfully creates a fully intrinsic hyperbolic neural network that respects geometric curvature, demonstrating superior performance and efficiency across multiple domains compared to existing hyperbolic and Euclidean approaches."}}
{"id": "2602.23994", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23994", "abs": "https://arxiv.org/abs/2602.23994", "authors": ["Vrushank Ahire", "Yogesh Kumar", "Anouck Girard", "M. A. Ganaie"], "title": "MINT: Multimodal Imaging-to-Speech Knowledge Transfer for Early Alzheimer's Screening", "comment": null, "summary": "Alzheimer's disease is a progressive neurodegenerative disorder in which mild cognitive impairment (MCI) marks a critical transition between aging and dementia. Neuroimaging modalities, such as structural MRI, provide biomarkers of this transition; however, their high costs and infrastructure needs limit their deployment at a population scale. Speech analysis offers a non-invasive alternative, but speech-only classifiers are developed independently of neuroimaging, leaving decision boundaries biologically ungrounded and limiting reliability on the subtle CN-versus-MCI distinction. We propose MINT (Multimodal Imaging-to-Speech Knowledge Transfer), a three-stage cross-modal framework that transfers biomarker structure from MRI into a speech encoder at training time. An MRI teacher, trained on 1,228 subjects, defines a compact neuroimaging embedding space for CN-versus-MCI classification. A residual projection head aligns speech representations to this frozen imaging manifold via a combined geometric loss, adapting speech to the learned biomarker space while preserving imaging encoder fidelity. The frozen MRI classifier, which is never exposed to speech, is applied to aligned embeddings at inference and requires no scanner. Evaluation on ADNI-4 shows aligned speech achieves performance comparable to speech-only baselines (AUC 0.720 vs 0.711) while requiring no imaging at inference, demonstrating that MRI-derived decision boundaries can ground speech representations. Multimodal fusion improves over MRI alone (0.973 vs 0.958). Ablation studies identify dropout regularization and self-supervised pretraining as critical design decisions. To our knowledge, this is the first demonstration of MRI-to-speech knowledge transfer for early Alzheimer's screening, establishing a biologically grounded pathway for population-level cognitive triage without neuroimaging at inference.", "AI": {"tldr": "\u63d0\u51faMINT\u6846\u67b6\uff0c\u901a\u8fc7\u5c06MRI\u751f\u7269\u6807\u5fd7\u7269\u7ed3\u6784\u8f6c\u79fb\u5230\u8bed\u97f3\u7f16\u7801\u5668\uff0c\u5b9e\u73b0\u65e0\u9700\u626b\u63cf\u7684\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u65e9\u671f\u7b5b\u67e5\uff0c\u6027\u80fd\u5ab2\u7f8e\u7eaf\u8bed\u97f3\u65b9\u6cd5", "motivation": "\u73b0\u6709\u795e\u7ecf\u5f71\u50cf\u5b66\u65b9\u6cd5\uff08\u5982MRI\uff09\u6210\u672c\u9ad8\u4e14\u57fa\u7840\u8bbe\u65bd\u8981\u6c42\u4e25\u683c\uff0c\u96be\u4ee5\u5927\u89c4\u6a21\u90e8\u7f72\uff1b\u800c\u7eaf\u8bed\u97f3\u5206\u6790\u7f3a\u4e4f\u751f\u7269\u5b66\u4f9d\u636e\uff0c\u5728\u533a\u5206\u8ba4\u77e5\u6b63\u5e38\u4e0e\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\u65f6\u53ef\u9760\u6027\u6709\u9650", "method": "\u4e09\u9636\u6bb5\u8de8\u6a21\u6001\u6846\u67b6\uff1a1) \u8bad\u7ec3MRI\u6559\u5e08\u6a21\u578b\u5efa\u7acb\u795e\u7ecf\u5f71\u50cf\u5d4c\u5165\u7a7a\u95f4\uff1b2) \u901a\u8fc7\u6b8b\u5dee\u6295\u5f71\u5934\u548c\u51e0\u4f55\u635f\u5931\u5c06\u8bed\u97f3\u8868\u5f81\u5bf9\u9f50\u5230\u51bb\u7ed3\u7684\u5f71\u50cf\u6d41\u5f62\uff1b3) \u63a8\u7406\u65f6\u76f4\u63a5\u5e94\u7528\u51bb\u7ed3\u7684MRI\u5206\u7c7b\u5668\u5230\u5bf9\u9f50\u540e\u7684\u8bed\u97f3\u5d4c\u5165", "result": "\u5728ADNI-4\u6570\u636e\u96c6\u4e0a\uff0c\u5bf9\u9f50\u540e\u7684\u8bed\u97f3\u8fbe\u5230\u4e0e\u7eaf\u8bed\u97f3\u57fa\u7ebf\u76f8\u5f53\u7684\u6027\u80fd(AUC 0.720 vs 0.711)\uff0c\u591a\u6a21\u6001\u878d\u5408\u8d85\u8d8a\u5355\u72ecMRI(0.973 vs 0.958)\uff1b\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9edropout\u6b63\u5219\u5316\u548c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u662f\u5173\u952e\u8bbe\u8ba1", "conclusion": "\u9996\u6b21\u5b9e\u73b0MRI\u5230\u8bed\u97f3\u7684\u77e5\u8bc6\u8f6c\u79fb\uff0c\u4e3a\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u65e9\u671f\u7b5b\u67e5\u5efa\u7acb\u4e86\u751f\u7269\u5b66\u57fa\u7840\u7684\u7fa4\u4f53\u7ea7\u8ba4\u77e5\u5206\u8bca\u8def\u5f84\uff0c\u63a8\u7406\u9636\u6bb5\u65e0\u9700\u795e\u7ecf\u5f71\u50cf\u8bbe\u5907"}}
{"id": "2602.23997", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23997", "abs": "https://arxiv.org/abs/2602.23997", "authors": ["Florent Delgrange"], "title": "Foundation World Models for Agents that Learn, Verify, and Adapt Reliably Beyond Static Environments", "comment": "AAMAS 2026, Blue Sky Idea Track. 4 pages, 1 Figure", "summary": "The next generation of autonomous agents must not only learn efficiently but also act reliably and adapt their behavior in open worlds. Standard approaches typically assume fixed tasks and environments with little or no novelty, which limits world models' ability to support agents that must evolve their policies as conditions change. This paper outlines a vision for foundation world models: persistent, compositional representations that unify reinforcement learning, reactive/program synthesis, and abstraction mechanisms. We propose an agenda built around four components: (i) learnable reward models from specifications to support optimization with clear objectives; (ii) adaptive formal verification integrated throughout learning; (iii) online abstraction calibration to quantify the reliability of the model's predictions; and (iv) test-time synthesis and world-model generation guided by verifiers. Together, these components enable agents to synthesize verifiable programs, derive new policies from a small number of interactions, and maintain correctness while adapting to novelty. The resulting framework positions foundation world models as a substrate for learning, reasoning, and adaptation, laying the groundwork for agents that not only act well but can explain and justify the behavior they adopt.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\"\u57fa\u7840\u4e16\u754c\u6a21\u578b\"\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u81ea\u4e3b\u667a\u80fd\u4f53\u5728\u5f00\u653e\u4e16\u754c\u4e2d\u65e0\u6cd5\u9002\u5e94\u65b0\u73af\u5883\u7684\u7f3a\u9677\u3002\u901a\u8fc7\u6574\u5408\u53ef\u5b66\u4e60\u5956\u52b1\u6a21\u578b\u3001\u81ea\u9002\u5e94\u5f62\u5f0f\u5316\u9a8c\u8bc1\u3001\u5728\u7ebf\u62bd\u8c61\u6821\u51c6\u548c\u6d4b\u8bd5\u65f6\u5408\u6210\u56db\u5927\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u667a\u80fd\u4f53\u65e2\u80fd\u9ad8\u6548\u5b66\u4e60\uff0c\u53c8\u80fd\u53ef\u9760\u884c\u52a8\u5e76\u89e3\u91ca\u5176\u884c\u4e3a\u3002", "motivation": "\u6807\u51c6\u65b9\u6cd5\u5047\u8bbe\u4efb\u52a1\u548c\u73af\u5883\u56fa\u5b9a\u3001\u7f3a\u4e4f\u65b0\u610f\uff0c\u9650\u5236\u4e86\u4e16\u754c\u6a21\u578b\u652f\u6301\u667a\u80fd\u4f53\u968f\u6761\u4ef6\u53d8\u5316\u800c\u6f14\u5316\u7684\u80fd\u529b\u3002\u4e0b\u4e00\u4ee3\u81ea\u4e3b\u667a\u80fd\u4f53\u4e0d\u4ec5\u9700\u8981\u9ad8\u6548\u5b66\u4e60\uff0c\u8fd8\u5fc5\u987b\u5728\u5f00\u653e\u4e16\u754c\u4e2d\u53ef\u9760\u884c\u52a8\u5e76\u9002\u5e94\u65b0\u60c5\u51b5\u3002", "method": "\u63d0\u51fa\u5305\u542b\u56db\u4e2a\u7ec4\u4ef6\u7684\u7814\u7a76\u8bae\u7a0b\uff1a(i) \u4ece\u89c4\u7ea6\u5b66\u4e60\u53ef\u5b66\u4e60\u7684\u5956\u52b1\u6a21\u578b\u4ee5\u652f\u6301\u76ee\u6807\u4f18\u5316\uff1b(ii) \u5c06\u81ea\u9002\u5e94\u5f62\u5f0f\u5316\u9a8c\u8bc1\u8d2f\u7a7f\u4e8e\u5b66\u4e60\u8fc7\u7a0b\uff1b(iii) \u5728\u7ebf\u62bd\u8c61\u6821\u51c6\u4ee5\u91cf\u5316\u6a21\u578b\u9884\u6d4b\u7684\u53ef\u9760\u6027\uff1b(iv) \u7531\u9a8c\u8bc1\u5668\u5f15\u5bfc\u7684\u6d4b\u8bd5\u65f6\u5408\u6210\u4e0e\u4e16\u754c\u6a21\u578b\u751f\u6210\u3002\u7edf\u4e00\u5f3a\u5316\u5b66\u4e60\u3001\u7a0b\u5e8f\u5408\u6210\u548c\u62bd\u8c61\u673a\u5236\u3002", "result": "\u6784\u5efa\u7684\u6846\u67b6\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\uff1a\u5408\u6210\u53ef\u9a8c\u8bc1\u7a0b\u5e8f\u3001\u4ece\u5c11\u91cf\u4ea4\u4e92\u4e2d\u63a8\u5bfc\u65b0\u7b56\u7565\u3001\u5728\u9002\u5e94\u65b0\u73af\u5883\u65f6\u4fdd\u6301\u6b63\u786e\u6027\uff0c\u5e76\u80fd\u89e3\u91ca\u548c\u8bc1\u660e\u5176\u884c\u4e3a\u3002\u4f46\u672c\u6587\u4e3b\u8981\u63d0\u51fa\u7814\u7a76\u613f\u666f\u800c\u975e\u5b9e\u8bc1\u7ed3\u679c\u3002", "conclusion": "\u57fa\u7840\u4e16\u754c\u6a21\u578b\u53ef\u4f5c\u4e3a\u5b66\u4e60\u3001\u63a8\u7406\u548c\u9002\u5e94\u7684\u5e95\u5c42\u67b6\u6784\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u81ea\u4e3b\u667a\u80fd\u4f53\u5960\u5b9a\u57fa\u7840\u2014\u2014\u8fd9\u4e9b\u667a\u80fd\u4f53\u4e0d\u4ec5\u8868\u73b0\u826f\u597d\uff0c\u8fd8\u80fd\u5728\u5f00\u653e\u4e16\u754c\u4e2d\u53ef\u9760\u9002\u5e94\u5e76\u89e3\u91ca\u5176\u884c\u4e3a\u51b3\u7b56\u3002"}}
{"id": "2602.24012", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.24012", "abs": "https://arxiv.org/abs/2602.24012", "authors": ["Roy Betser", "Eyal Gofer", "Meir Yossef Levi", "Guy Gilboa"], "title": "InfoNCE Induces Gaussian Distribution", "comment": "Accepted to ICLR 2026, Oral", "summary": "Contrastive learning has become a cornerstone of modern representation learning, allowing training with massive unlabeled data for both task-specific and general (foundation) models. A prototypical loss in contrastive training is InfoNCE and its variants. In this work, we show that the InfoNCE objective induces Gaussian structure in representations that emerge from contrastive training. We establish this result in two complementary regimes. First, we show that under certain alignment and concentration assumptions, projections of the high-dimensional representation asymptotically approach a multivariate Gaussian distribution. Next, under less strict assumptions, we show that adding a small asymptotically vanishing regularization term that promotes low feature norm and high feature entropy leads to similar asymptotic results. We support our analysis with experiments on synthetic and CIFAR-10 datasets across multiple encoder architectures and sizes, demonstrating consistent Gaussian behavior. This perspective provides a principled explanation for commonly observed Gaussianity in contrastive representations. The resulting Gaussian model enables principled analytical treatment of learned representations and is expected to support a wide range of applications in contrastive learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc1\u660e\u4e86InfoNCE\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u4f1a\u5728\u8868\u5f81\u4e2d\u8bf1\u5bfc\u51fa\u9ad8\u65af\u7ed3\u6784\uff0c\u4e3a\u89c2\u5bdf\u5230\u7684\u5bf9\u6bd4\u8868\u5f81\u9ad8\u65af\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\uff0c\u5e76\u652f\u6301\u4e86\u540e\u7eed\u7684\u5206\u6790\u5e94\u7528\u3002", "motivation": "\u5bf9\u6bd4\u5b66\u4e60\u5df2\u6210\u4e3a\u73b0\u4ee3\u8868\u5f81\u5b66\u4e60\u7684\u57fa\u77f3\uff0c\u4f46\u5176\u5b66\u4e60\u5230\u7684\u8868\u5f81\u4e3a\u4f55\u5e38\u8868\u73b0\u51fa\u9ad8\u65af\u5206\u5e03\u7279\u6027\u4ecd\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7406\u8bba\u89e3\u91ca\u3002\u7406\u89e3\u8fd9\u4e00\u73b0\u8c61\u5bf9\u4e8e\u5206\u6790\u548c\u6539\u8fdb\u5bf9\u6bd4\u5b66\u4e60\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u4e24\u79cd\u4e92\u8865\u7684\u7406\u8bba\u6846\u67b6\u8fdb\u884c\u5206\u6790\uff1a1\uff09\u5728\u7279\u5b9a\u5bf9\u9f50\u548c\u96c6\u4e2d\u5047\u8bbe\u4e0b\uff0c\u8bc1\u660e\u9ad8\u7ef4\u8868\u5f81\u7684\u6295\u5f71\u6e10\u8fd1\u8d8b\u4e8e\u591a\u5143\u9ad8\u65af\u5206\u5e03\uff1b2\uff09\u5728\u8f83\u5f31\u5047\u8bbe\u4e0b\uff0c\u901a\u8fc7\u6dfb\u52a0\u5fae\u5c0f\u7684\u6e10\u6d88\u6b63\u5219\u9879\uff08\u4fc3\u8fdb\u4f4e\u7279\u5f81\u8303\u6570\u548c\u9ad8\u7279\u5f81\u71b5\uff09\u83b7\u5f97\u7c7b\u4f3c\u6e10\u8fd1\u7ed3\u679c\u3002\u5e76\u5728\u5408\u6210\u6570\u636e\u548cCIFAR-10\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u591a\u67b6\u6784\u3001\u591a\u5c3a\u5bf8\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86InfoNCE\u76ee\u6807\u8bf1\u5bfc\u51fa\u7684\u9ad8\u65af\u7ed3\u6784\u7279\u6027\uff0c\u5b9e\u9a8c\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u89c2\u5bdf\u5230\u4e86\u7a33\u5b9a\u7684\u4e00\u81f4\u6027\u9ad8\u65af\u884c\u4e3a\u3002\u8be5\u53d1\u73b0\u63ed\u793a\u4e86\u5bf9\u6bd4\u8868\u5f81\u7684\u5185\u5728\u7edf\u8ba1\u89c4\u5f8b\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5bf9\u6bd4\u8868\u5f81\u4e2d\u7684\u9ad8\u65af\u73b0\u8c61\u63d0\u4f9b\u4e86\u539f\u7406\u6027\u89e3\u91ca\uff0c\u5efa\u7acb\u7684\u9ad8\u65af\u6a21\u578b\u4f7f\u8868\u5f81\u7684\u89e3\u6790\u5904\u7406\u6210\u4e3a\u53ef\u80fd\uff0c\u9884\u8ba1\u5c06\u652f\u6301\u5bf9\u6bd4\u5b66\u4e60\u5728\u5e7f\u6cdb\u9886\u57df\u7684\u5e94\u7528\u3002"}}
{"id": "2602.24040", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.24040", "abs": "https://arxiv.org/abs/2602.24040", "authors": ["Daniel Yang", "Samuel Stante", "Florian Redhardt", "Lena Libon", "Parnian Kassraie", "Ido Hakimi", "Barna P\u00e1sztor", "Andreas Krause"], "title": "RewardUQ: A Unified Framework for Uncertainty-Aware Reward Models", "comment": null, "summary": "Reward models are central to aligning large language models (LLMs) with human preferences. Yet most approaches rely on pointwise reward estimates that overlook the epistemic uncertainty in reward models arising from limited human feedback. Recent work suggests that quantifying this uncertainty can reduce the costs of human annotation via uncertainty-guided active learning and mitigate reward overoptimization in LLM post-training. However, uncertainty-aware reward models have so far been adopted without thorough comparison, leaving them poorly understood. This work introduces a unified framework, RewardUQ, to systematically evaluate uncertainty quantification for reward models. We compare common methods along standard metrics measuring accuracy and calibration, and we propose a new ranking strategy incorporating both dimensions for a simplified comparison. Our experimental results suggest that model size and initialization have the most meaningful impact on performance, and most prior work could have benefited from alternative design choices. To foster the development and evaluation of new methods and aid the deployment in downstream applications, we release our open-source framework as a Python package. Our code is available at https://github.com/lasgroup/rewarduq.", "AI": {"tldr": "\u63d0\u51faRewardUQ\u6846\u67b6\u7cfb\u7edf\u8bc4\u4f30\u5956\u52b1\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u53d1\u73b0\u6a21\u578b\u5c3a\u5bf8\u548c\u521d\u59cb\u5316\u5bf9\u6027\u80fd\u5f71\u54cd\u6700\u5927\uff0c\u5e76\u5f00\u6e90\u4ee3\u7801\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76", "motivation": "\u73b0\u6709\u5956\u52b1\u6a21\u578b\u5ffd\u7565\u4eba\u7c7b\u53cd\u9988\u6709\u9650\u6027\u5bfc\u81f4\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u4e14\u7f3a\u4e4f\u7cfb\u7edf\u6bd4\u8f83\u65b9\u6cd5\uff0c\u5f71\u54cdLLM\u5bf9\u9f50\u6548\u679c\u4e0e\u6210\u672c\u6548\u76ca", "method": "\u6784\u5efa\u7edf\u4e00\u6846\u67b6RewardUQ\uff0c\u901a\u8fc7\u6807\u51c6\u6307\u6807\uff08\u51c6\u786e\u6027/\u6821\u51c6\u6027\uff09\u548c\u65b0\u9896\u53cc\u7ef4\u5ea6\u6392\u5e8f\u7b56\u7565\u5bf9\u6bd4\u5e38\u89c1\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5", "result": "\u5b9e\u9a8c\u8868\u660e\u6a21\u578b\u5c3a\u5bf8\u548c\u521d\u59cb\u5316\u5bf9\u6027\u80fd\u5f71\u54cd\u6700\u663e\u8457\uff0c\u591a\u6570\u5148\u524d\u7814\u7a76\u53ef\u901a\u8fc7\u6539\u8fdb\u8bbe\u8ba1\u9009\u62e9\u83b7\u5f97\u66f4\u597d\u6548\u679c", "conclusion": "\u5f00\u6e90Python\u6846\u67b6\u4ee5\u4fc3\u8fdb\u65b0\u65b9\u6cd5\u5f00\u53d1\uff0c\u52a9\u529b\u4e0b\u6e38\u5e94\u7528\u90e8\u7f72\uff0c\u4ee3\u7801\u5df2\u53d1\u5e03\u4e8eGitHub"}}
{"id": "2602.24066", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24066", "abs": "https://arxiv.org/abs/2602.24066", "authors": ["Tobias Nygaard"], "title": "pathsig: A GPU-Accelerated Library for Truncated and Projected Path Signatures", "comment": null, "summary": "Path signatures provide a rich representation of sequential data, with strong theoretical guarantees and good performance in a variety of machine-learning tasks. While signatures have progressed from fixed feature extractors to trainable components of machine-learning models, existing libraries often lack the required scalability for large-scale, gradient-based learning. To address this gap, this paper introduces pathsig, a PyTorch-native library that computes path signatures directly in the word basis. By using CUDA kernels to update signature coefficients in parallel over prefix-closed word sets, pathsig achieves high GPU throughput and near-minimal peak memory. Compared with other libraries, pathsig achieves 10-30x speedups for computation of truncated signatures and up to 4-10x speedups in training that require backpropagation through the signature. Beyond regular truncation, pathsig supports projections of the (infinite-dimensional) signature onto user-specified sets of words and anisotropic truncation motivated by inhomogeneous path regularity, enabling more compact representations that can reduce dimensionality, redundancy, and computational cost.", "AI": {"tldr": "The paper introduces pathsig, a PyTorch-native library that efficiently computes path signatures using CUDA kernels in the word basis, achieving 10-30x speedups for truncated signatures and 4-10x speedups in gradient-based training while supporting flexible projection and truncation methods.", "motivation": "Existing path signature libraries lack the scalability required for large-scale gradient-based learning, limiting their integration into modern machine learning pipelines despite their theoretical promise and empirical performance.", "method": "The authors developed pathsig, a PyTorch-native library that computes path signatures directly in the word basis using custom CUDA kernels to update signature coefficients in parallel over prefix-closed word sets, optimizing for GPU throughput and memory efficiency.", "result": "Compared to other libraries, pathsig achieves 10-30x speedups for truncated signature computation and 4-10x speedups during backpropagation training, with near-minimal peak memory usage, while supporting word set projections and anisotropic truncation for compact representations.", "conclusion": "Pathsig enables scalable gradient-based learning with path signatures and provides flexible truncation schemes that reduce dimensionality and computational cost, making it practical for large-scale machine learning applications involving sequential data."}}
{"id": "2602.24069", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2602.24069", "abs": "https://arxiv.org/abs/2602.24069", "authors": ["Ryan DeWolfe"], "title": "Leveraging Non-linear Dimension Reduction and Random Walk Co-occurrence for Node Embedding", "comment": "13 pages, 6 figures", "summary": "Leveraging non-linear dimension reduction techniques, we remove the low dimension constraint from node embedding and propose COVE, an explainable high dimensional embedding that, when reduced to low dimension with UMAP, slightly increases performance on clustering and link prediction tasks. The embedding is inspired by neural embedding methods that use co-occurrence on a random walk as an indication of similarity, and is closely related to a diffusion process. Extending on recent community detection benchmarks, we find that a COVE UMAP HDBSCAN pipeline performs similarly to the popular Louvain algorithm.", "AI": {"tldr": "\u63d0\u51faCOVE\u2014\u2014\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u9ad8\u7ef4\u8282\u70b9\u5d4c\u5165\u65b9\u6cd5\uff0c\u901a\u8fc7UMAP\u964d\u7ef4\u540e\u8f7b\u5fae\u63d0\u5347\u805a\u7c7b\u4e0e\u94fe\u63a5\u9884\u6d4b\u6027\u80fd\uff0c\u4e14COVE+UMAP+HDBSCAN\u6d41\u7a0b\u5728\u793e\u533a\u68c0\u6d4b\u4e0a\u53ef\u5ab2\u7f8eLouvain\u7b97\u6cd5\u3002", "motivation": "\u4f20\u7edf\u8282\u70b9\u5d4c\u5165\u53d7\u9650\u4e8e\u4f4e\u7ef4\u8868\u793a\uff0c\u53ef\u80fd\u9650\u5236\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u57fa\u4e8e\u968f\u673a\u6e38\u8d70\u5171\u73b0\u548c\u6269\u6563\u8fc7\u7a0b\u7684\u9ad8\u7ef4\u5d4c\u5165\uff0c\u5229\u7528UMAP\u8fdb\u884c\u975e\u7ebf\u6027\u964d\u7ef4\u3002", "result": "\u964d\u7ef4\u540e\u805a\u7c7b\u4e0e\u94fe\u63a5\u9884\u6d4b\u6027\u80fd\u5c0f\u5e45\u63d0\u5347\uff1b\u5b8c\u6574\u6d41\u7a0b\u5728\u793e\u533a\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0eLouvain\u7b97\u6cd5\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "\u9ad8\u7ef4\u5d4c\u5165\u914d\u5408\u975e\u7ebf\u6027\u964d\u7ef4\u4e3a\u56fe\u6316\u6398\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u4e14\u5177\u7ade\u4e89\u529b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.24083", "categories": ["cs.LG", "math.PR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.24083", "abs": "https://arxiv.org/abs/2602.24083", "authors": ["Xinlong Du", "Harsha Honnappa", "Vinayak Rao"], "title": "Neural Diffusion Intensity Models for Point Process Data", "comment": null, "summary": "Cox processes model overdispersed point process data via a latent stochastic intensity, but both nonparametric estimation of the intensity model and posterior inference over intensity paths are typically intractable, relying on expensive MCMC methods. We introduce Neural Diffusion Intensity Models, a variational framework for Cox processes driven by neural SDEs. Our key theoretical result, based on enlargement of filtrations, shows that conditioning on point process observations preserves the diffusion structure of the latent intensity with an explicit drift correction. This guarantees the variational family contains the true posterior, so that ELBO maximization coincides with maximum likelihood estimation under sufficient model capacity. We design an amortized encoder architecture that maps variable-length event sequences to posterior intensity paths by simulating the drift-corrected SDE, replacing repeated MCMC runs with a single forward pass. Experiments on synthetic and real-world data demonstrate accurate recovery of latent intensity dynamics and posterior paths, with orders-of-magnitude speedups over MCMC-based methods.", "AI": {"tldr": "The paper proposes Neural Diffusion Intensity Models, a variational framework for Cox processes using neural SDEs, with a theoretical guarantee that the variational family contains the true posterior, enabling efficient amortized inference that replaces MCMC with a single forward pass.", "motivation": "Cox processes model overdispersed point process data via latent stochastic intensity, but nonparametric estimation and posterior inference are computationally intractable and rely on expensive MCMC methods.", "method": "They introduce a variational framework based on neural SDEs, with a key theoretical result using enlargement of filtrations showing that conditioning on observations preserves diffusion structure with an explicit drift correction. They design an amortized encoder architecture that maps event sequences to posterior paths by simulating the drift-corrected SDE.", "result": "Experiments on synthetic and real-world data demonstrate accurate recovery of latent intensity dynamics and posterior paths, with orders-of-magnitude speedups over MCMC-based methods.", "conclusion": "The proposed Neural Diffusion Intensity Models provide a theoretically-grounded, computationally efficient alternative to MCMC for Cox process inference, achieving accurate results with dramatically reduced computational cost."}}
{"id": "2602.24146", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24146", "abs": "https://arxiv.org/abs/2602.24146", "authors": ["Zitian Li", "Wang Chi Cheung"], "title": "Learning with a Budget: Identifying the Best Arm with Resource Constraints", "comment": "A preliminary version of this work, titled 'Best Arm Identification with Resource Constraints,' was presented at the 27th International Conference on Artificial Intelligence and Statistics (AISTATS 2024). This manuscript extends the original conference paper by providing improved theoretical results and more generalized conclusions, aiming for future journal submission. arXiv admin note: substantial text overlap with arXiv:2402.19090", "summary": "In many applications, evaluating the effectiveness of different alternatives comes with varying costs or resource usage. Motivated by such heterogeneity, we study the Best Arm Identification with Resource Constraints (BAIwRC) problem, where an agent seeks to identify the best alternative (aka arm) in the presence of resource constraints. Each arm pull consumes one or more types of limited resources. We make two key contributions. First, we propose the Successive Halving with Resource Rationing (SH-RR) algorithm, which integrates resource-aware allocation into the classical successive halving framework on best arm identification. The SH-RR algorithm unifies the theoretical analysis for both the stochastic and deterministic consumption settings, with a new \\textit{effective consumption measure", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u8d44\u6e90\u7ea6\u675f\u4e0b\u7684\u6700\u4f73\u81c2\u8bc6\u522b\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u8d44\u6e90\u914d\u7ed9\u7684\u8fde\u7eed\u51cf\u534a\u7b97\u6cd5(SH-RR)\uff0c\u901a\u8fc7\u65b0\u7684\u6709\u6548\u6d88\u8017\u5ea6\u91cf\u7edf\u4e00\u4e86\u968f\u673a\u548c\u786e\u5b9a\u6027\u8d44\u6e90\u6d88\u8017\u7684\u7406\u8bba\u5206\u6790\u6846\u67b6\u3002", "motivation": "\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5907\u9009\u65b9\u6848\u8bc4\u4f30\u5b58\u5728\u5f02\u8d28\u6027\u6210\u672c\u4e0e\u8d44\u6e90\u6d88\u8017\uff0c\u4f20\u7edf\u6700\u4f73\u81c2\u8bc6\u522b\u6a21\u578b\u672a\u8003\u8651\u6709\u9650\u8d44\u6e90\u7ea6\u675f\uff0c\u5bfc\u81f4\u7b97\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u3002", "method": "\u63d0\u51fa\"\u8fde\u7eed\u51cf\u534a+\u8d44\u6e90\u914d\u7ed9\"(SH-RR)\u7b97\u6cd5\uff0c\u5c06\u8d44\u6e90\u611f\u77e5\u5206\u914d\u673a\u5236\u878d\u5165\u7ecf\u5178\u8fde\u7eed\u51cf\u534a\u6846\u67b6\uff0c\u5f15\u5165\u521b\u65b0\u7684\u6709\u6548\u6d88\u8017\u5ea6\u91cf\u6765\u5904\u7406\u591a\u7c7b\u578b\u8d44\u6e90\u7ea6\u675f\uff0c\u652f\u6301\u968f\u673a\u4e0e\u786e\u5b9a\u6027\u4e24\u79cd\u6d88\u8017\u6a21\u578b\u3002", "result": "\u8be5\u7b97\u6cd5\u7edf\u4e00\u4e86\u4e0d\u540c\u8d44\u6e90\u6d88\u8017\u8bbe\u7f6e\u4e0b\u7684\u7406\u8bba\u5206\u6790\uff0c\u5728\u968f\u673a\u548c\u786e\u5b9a\u6027\u6d88\u8017\u573a\u666f\u4e0b\u5747\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u6027\u80fd\u4fdd\u8bc1\uff08\u5177\u4f53\u8fb9\u754c\u89c1\u539f\u6587\u5b8c\u6574\u8bc1\u660e\uff09\u3002", "conclusion": "SH-RR\u4e3a\u8d44\u6e90\u7ea6\u675f\u4e0b\u7684\u6700\u4f73\u81c2\u8bc6\u522b\u63d0\u4f9b\u4e86\u901a\u7528\u4e14\u7406\u8bba\u575a\u5b9e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u62d3\u5c55\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u9002\u7528\u8303\u56f4\u3002"}}
{"id": "2602.24209", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24209", "abs": "https://arxiv.org/abs/2602.24209", "authors": ["Mohsen Tajgardan", "Atena Shiranzaei", "Mahdi Rabbani", "Reza Khoshkangini", "Mahtab Jamali"], "title": "An Efficient Unsupervised Federated Learning Approach for Anomaly Detection in Heterogeneous IoT Networks", "comment": null, "summary": "Federated learning (FL) is an effective paradigm for distributed environments such as the Internet of Things (IoT), where data from diverse devices with varying functionalities remains localized while contributing to a shared global model. By eliminating the need to transmit raw data, FL inherently preserves privacy. However, the heterogeneous nature of IoT data, stemming from differences in device capabilities, data formats, and communication constraints, poses significant challenges to maintaining both global model performance and privacy. In the context of IoT-based anomaly detection, unsupervised FL offers a promising means to identify abnormal behavior without centralized data aggregation. Nevertheless, feature heterogeneity across devices complicates model training and optimization, hindering effective implementation. In this study we propose an efficient unsupervised FL framework that enhances anomaly detection by leveraging shared features from two distinct IoT datasets: one focused on anomaly detection and the other on device identification, while preserving dataset-specific features. To improve transparency and interpretability, we employ explainable AI techniques, such as SHAP, to identify key features influencing local model decisions. Experiments conducted on real-world IoT datasets demonstrate that the proposed method significantly outperforms conventional FL approaches in anomaly detection accuracy. This work underscores the potential of using shared features from complementary datasets to optimize unsupervised federated learning and achieve superior anomaly detection results in decentralized IoT environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u4e24\u4e2a\u7269\u8054\u7f51\u6570\u636e\u96c6\uff08\u5f02\u5e38\u68c0\u6d4b\u4e0e\u8bbe\u5907\u8bc6\u522b\uff09\u7684\u5171\u4eab\u7279\u5f81\uff0c\u5728\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u5206\u5e03\u5f0f\u7269\u8054\u7f51\u73af\u5883\u4e0b\u7684\u5f02\u5e38\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u7269\u8054\u7f51\u8bbe\u5907\u7684\u5f02\u6784\u6027\u5bfc\u81f4\u6570\u636e\u7279\u5f81\u5206\u5e03\u5dee\u5f02\u5927\uff0c\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u5728\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u9762\u4e34\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u53cc\u91cd\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3\u7279\u5f81\u5f02\u8d28\u6027\u5e26\u6765\u7684\u8bad\u7ec3\u96be\u9898\u3002", "method": "\u63d0\u51fa\u65e0\u76d1\u7763\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u4e24\u4e2a\u4e92\u8865\u7269\u8054\u7f51\u6570\u636e\u96c6\uff08\u5f02\u5e38\u68c0\u6d4b\u6570\u636e\u96c6\u548c\u8bbe\u5907\u8bc6\u522b\u6570\u636e\u96c6\uff09\u7684\u5171\u4eab\u7279\u5f81\u8fdb\u884c\u8054\u5408\u8bad\u7ec3\uff0c\u4fdd\u7559\u6570\u636e\u96c6\u7279\u6709\u7279\u5f81\uff1b\u91c7\u7528SHAP\u7b49\u53ef\u89e3\u91caAI\u6280\u672f\u8bc6\u522b\u5f71\u54cd\u672c\u5730\u6a21\u578b\u51b3\u7b56\u7684\u5173\u952e\u7279\u5f81\u3002", "result": "\u5728\u771f\u5b9e\u7269\u8054\u7f51\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u5f02\u5e38\u68c0\u6d4b\u51c6\u786e\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u65b9\u6848\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u4e92\u8865\u6570\u636e\u96c6\u7684\u5171\u4eab\u7279\u5f81\u4f18\u5316\u65e0\u76d1\u7763\u8054\u90a6\u5b66\u4e60\uff0c\u53ef\u5728\u53bb\u4e2d\u5fc3\u5316\u7684\u7269\u8054\u7f51\u73af\u5883\u4e2d\u5b9e\u73b0\u66f4\u4f18\u7684\u5f02\u5e38\u68c0\u6d4b\u6548\u679c\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2602.24281", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24281", "abs": "https://arxiv.org/abs/2602.24281", "authors": ["Ali Behrouz", "Zeman Li", "Yuan Deng", "Peilin Zhong", "Meisam Razaviyayn", "Vahab Mirrokni"], "title": "Memory Caching: RNNs with Growing Memory", "comment": null, "summary": "Transformers have been established as the de-facto backbones for most recent advances in sequence modeling, mainly due to their growing memory capacity that scales with the context length. While plausible for retrieval tasks, it causes quadratic complexity and so has motivated recent studies to explore viable subquadratic recurrent alternatives. Despite showing promising preliminary results in diverse domains, such recurrent architectures underperform Transformers in recall-intensive tasks, often attributed to their fixed-size memory. In this paper, we introduce Memory Caching (MC), a simple yet effective technique that enhances recurrent models by caching checkpoints of their memory states (a.k.a. hidden states). Memory Caching allows the effective memory capacity of RNNs to grow with sequence length, offering a flexible trade-off that interpolates between the fixed memory (i.e., $O(L)$ complexity) of RNNs and the growing memory (i.e., $O(L^2)$ complexity) of Transformers. We propose four variants of MC, including gated aggregation and sparse selective mechanisms, and discuss their implications on both linear and deep memory modules. Our experimental results on language modeling, and long-context understanding tasks show that MC enhances the performance of recurrent models, supporting its effectiveness. The results of in-context recall tasks indicate that while Transformers achieve the best accuracy, our MC variants show competitive performance, close the gap with Transformers, and performs better than state-of-the-art recurrent models.", "AI": {"tldr": "Proposes Memory Caching (MC) to enhance recurrent models by caching hidden states, enabling flexible memory scaling between RNNs' O(L) and Transformers' O(L\u00b2) complexity, closing performance gap in recall tasks.", "motivation": "Transformers suffer quadratic complexity from growing memory, while subquadratic recurrent alternatives underperform in recall-intensive tasks due to fixed-size memory, creating a need for scalable recurrent architectures.", "method": "Introduces Memory Caching (MC)\u2014caching checkpoints of RNN hidden states\u2014with four variants (gated aggregation, sparse selection) to flexibly expand effective memory capacity proportionally to sequence length.", "result": "MC-enhanced recurrent models improve language modeling and long-context tasks; in recall tasks, MC variants achieve near-Transformer accuracy (though Transformers lead) and outperform state-of-the-art recurrent models.", "conclusion": "MC provides an effective trade-off between RNNs' efficiency and Transformers' memory capacity, making recurrent models competitive for long-context and recall-intensive applications."}}
{"id": "2602.24182", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24182", "abs": "https://arxiv.org/abs/2602.24182", "authors": ["Sikata Sengupta", "Guangyi Liu", "Omer Gottesman", "Joseph W Durham", "Michael Kearns", "Aaron Roth", "Michael Caldara"], "title": "Multi-Objective Reinforcement Learning for Large-Scale Tote Allocation in Human-Robot Collaborative Fulfillment Centers", "comment": null, "summary": "Optimizing the consolidation process in container-based fulfillment centers requires trading off competing objectives such as processing speed, resource usage, and space utilization while adhering to a range of real-world operational constraints. This process involves moving items between containers via a combination of human and robotic workstations to free up space for inbound inventory and increase container utilization. We formulate this problem as a large-scale Multi-Objective Reinforcement Learning (MORL) task with high-dimensional state spaces and dynamic system behavior. Our method builds on recent theoretical advances in solving constrained RL problems via best-response and no-regret dynamics in zero-sum games, enabling principled minimax policy learning. Policy evaluation on realistic warehouse simulations shows that our approach effectively trades off objectives, and we empirically observe that it learns a single policy that simultaneously satisfies all constraints, even if this is not theoretically guaranteed. We further introduce a theoretical framework to handle the problem of error cancellation, where time-averaged solutions display oscillatory behavior. This method returns a single iterate whose Lagrangian value is close to the minimax value of the game. These results demonstrate the promise of MORL in solving complex, high-impact decision-making problems in large-scale industrial systems.", "AI": {"tldr": "\u5c06\u5bb9\u5668\u5316\u5c65\u7ea6\u4e2d\u5fc3\u7684\u8d27\u7269\u6574\u5408\u95ee\u9898\u5efa\u6a21\u4e3a\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\uff0c\u901a\u8fc7\u96f6\u548c\u535a\u5f08\u4e2d\u7684\u65e0\u9057\u61be\u52a8\u6001\u7406\u8bba\u5b9e\u73b0\u7ea6\u675f\u4f18\u5316\uff0c\u5728\u4ed3\u5e93\u4eff\u771f\u4e2d\u6709\u6548\u5e73\u8861\u591a\u4e2a\u76ee\u6807\u5e76\u6ee1\u8db3\u7ea6\u675f\u3002", "motivation": "\u5bb9\u5668\u5316\u5c65\u7ea6\u4e2d\u5fc3\u9700\u8981\u901a\u8fc7\u4eba\u673a\u534f\u540c\u5de5\u4f5c\u7ad9\u79fb\u52a8\u8d27\u7269\u6765\u4f18\u5316\u7a7a\u95f4\u5229\u7528\u7387\u548c\u5904\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u9762\u4e34\u8d44\u6e90\u4f7f\u7528\u7b49\u591a\u76ee\u6807\u6743\u8861\u548c\u73b0\u5b9e\u8fd0\u8425\u7ea6\u675f\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u8fd9\u79cd\u5927\u89c4\u6a21\u3001\u9ad8\u7ef4\u5ea6\u7684\u52a8\u6001\u51b3\u7b56\u95ee\u9898\u3002", "method": "\u5c06\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u5927\u89c4\u6a21\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\uff0c\u57fa\u4e8e\u96f6\u548c\u535a\u5f08\u4e2d\u7684\u6700\u4f73\u54cd\u5e94\u548c\u65e0\u9057\u61be\u52a8\u6001\u7406\u8bba\u5b9e\u73b0\u7ea6\u675fRL\u6c42\u89e3\uff0c\u63d0\u51fa\u6781\u5c0f\u6781\u5927\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u8bef\u5dee\u62b5\u6d88\u7406\u8bba\u6846\u67b6\u6765\u89e3\u51b3\u65f6\u95f4\u5e73\u5747\u89e3\u7684\u632f\u8361\u95ee\u9898\u3002", "result": "\u5728\u771f\u5b9e\u4ed3\u5e93\u4eff\u771f\u73af\u5883\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6743\u8861\u591a\u4e2a\u7ade\u4e89\u76ee\u6807\uff0c\u5b66\u4e60\u51fa\u80fd\u540c\u65f6\u6ee1\u8db3\u6240\u6709\u7ea6\u675f\u7684\u5355\u4e00\u7b56\u7565\uff1b\u8bef\u5dee\u62b5\u6d88\u6846\u67b6\u8fd4\u56de\u7684\u5355\u4e00\u8fed\u4ee3\u503c\u5176\u62c9\u683c\u6717\u65e5\u503c\u63a5\u8fd1\u535a\u5f08\u7684\u6781\u5c0f\u6781\u5927\u503c\uff0c\u907f\u514d\u4e86\u632f\u8361\u884c\u4e3a\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u5728\u89e3\u51b3\u5927\u89c4\u6a21\u5de5\u4e1a\u7cfb\u7edf\u590d\u6742\u51b3\u7b56\u95ee\u9898\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u5bb9\u5668\u5316\u5c65\u7ea6\u4e2d\u5fc3\u7684\u667a\u80fd\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u8df5\u65b9\u6cd5\u3002"}}
{"id": "2602.24283", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.24283", "abs": "https://arxiv.org/abs/2602.24283", "authors": ["Zhengbo Wang", "Jian Liang", "Ran He", "Zilei Wang", "Tieniu Tan"], "title": "Taming Momentum: Rethinking Optimizer States Through Low-Rank Approximation", "comment": "Camera-ready version. Accepted as Oral at ICLR 2026", "summary": "Modern optimizers like Adam and Muon are central to training large language models, but their reliance on first- and second-order momenta introduces significant memory overhead, which constrains scalability and computational efficiency. In this work, we reframe the exponential moving average (EMA) used in these momenta as the training of a linear regressor via online gradient flow. Building on this equivalence, we introduce LoRA-Pre, a novel low-rank optimizer designed for efficient pre-training. Specifically, LoRA-Pre reduces the optimizer's memory footprint by decomposing the full momentum matrix into a compact low-rank subspace within the online linear learner, thereby maintaining optimization performance while improving memory efficiency. We empirically validate LoRA-Pre's efficacy by pre-training models from the Llama architecture family, scaling from 60M to 1B parameters. LoRA-Pre achieves the highest performance across all model sizes. Notably, LoRA-Pre demonstrates remarkable rank efficiency, achieving comparable or superior results using only 1/8 the rank of baseline methods. Beyond pre-training, we evaluate LoRA-Pre's effectiveness in fine-tuning scenarios. With the same rank, LoRA-Pre consistently outperforms all efficient fine-tuning baselines. Specifically, compared to standard LoRA, LoRA-Pre achieves substantial improvements of 3.14 points on Llama-3.1-8B and 6.17 points on Llama-2-7B, validating our approach's effectiveness across both pre-training and fine-tuning paradigms. Our code is publicly available at https://github.com/mrflogs/LoRA-Pre.", "AI": {"tldr": "\u63d0\u51faLoRA-Pre\u4f4e\u79e9\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u5206\u89e3\u52a8\u91cf\u77e9\u9635\u4e3a\u4f4e\u79e9\u5b50\u7a7a\u95f4\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u5f00\u9500\uff0c\u5728\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u4e2d\u5747\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u4ee3\u4f18\u5316\u5668\u5982Adam\u548cMuon\u4f9d\u8d56\u52a8\u91cf\u9020\u6210\u663e\u8457\u5185\u5b58\u5f00\u9500\uff0c\u9650\u5236\u4e86\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u7684\u53ef\u6269\u5c55\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u5c06\u6307\u6570\u79fb\u52a8\u5e73\u5747(EMA)\u91cd\u65b0\u5efa\u6a21\u4e3a\u5728\u7ebf\u68af\u5ea6\u6d41\u8bad\u7ec3\u7684\u7ebf\u6027\u56de\u5f52\u5668\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u8bbe\u8ba1\u4f4e\u79e9\u4f18\u5316\u5668LoRA-Pre\uff0c\u901a\u8fc7\u4f4e\u79e9\u5206\u89e3\u538b\u7f29\u52a8\u91cf\u77e9\u9635\u3002", "result": "\u572860M-1B\u53c2\u6570\u7684Llama\u6a21\u578b\u9884\u8bad\u7ec3\u4e2d\u5168\u9762\u9886\u5148\uff1b\u4ec5\u7528\u57fa\u7ebf1/8\u7684\u79e9\u5373\u8fbe\u5230\u76f8\u5f53\u6216\u66f4\u4f18\u6027\u80fd\uff1b\u5fae\u8c03\u65f6\u76f8\u6bd4\u6807\u51c6LoRA\u5728Llama-3.1-8B\u63d0\u53473.14\u5206\uff0cLlama-2-7B\u63d0\u53476.17\u5206\u3002", "conclusion": "LoRA-Pre\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u65b0\u578b\u4f18\u5316\u5668\uff0c\u5728\u4fdd\u6301\u4f18\u5316\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u5185\u5b58\u5360\u7528\uff0c\u9002\u7528\u4e8e\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u573a\u666f\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.24201", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24201", "abs": "https://arxiv.org/abs/2602.24201", "authors": ["Egor Antipov", "Alessandro Palma", "Lorenzo Consoli", "Stephan G\u00fcnnemann", "Andrea Dittadi", "Fabian J. Theis"], "title": "Flow-Based Density Ratio Estimation for Intractable Distributions with Applications in Genomics", "comment": null, "summary": "Estimating density ratios between pairs of intractable data distributions is a core problem in probabilistic modeling, enabling principled comparisons of sample likelihoods under different data-generating processes across conditions and covariates. While exact-likelihood models such as normalizing flows offer a promising approach to density ratio estimation, naive flow-based evaluations are computationally expensive, as they require simulating costly likelihood integrals for each distribution separately. In this work, we leverage condition-aware flow matching to derive a single dynamical formulation for tracking density ratios along generative trajectories. We demonstrate competitive performance on simulated benchmarks for closed-form ratio estimation, and show that our method supports versatile tasks in single-cell genomics data analysis, where likelihood-based comparisons of cellular states across experimental conditions enable treatment effect estimation and batch correction evaluation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u611f\u77e5\u6d41\u5339\u914d\u7684\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u4e00\u52a8\u6001\u516c\u5f0f\u8ffd\u8e2a\u751f\u6210\u8f68\u8ff9\u4e2d\u7684\u5bc6\u5ea6\u6bd4\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u5728\u6a21\u62df\u57fa\u51c6\u6d4b\u8bd5\u548c\u5355\u7ec6\u80de\u57fa\u56e0\u7ec4\u5b66\u5206\u6790\u4e2d\u5c55\u73b0\u7ade\u4e89\u529b\u3002", "motivation": "\u73b0\u6709\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u65b9\u6cd5\uff08\u5982\u5f52\u4e00\u5316\u6d41\uff09\u9700\u4e3a\u6bcf\u4e2a\u5206\u5e03\u5355\u72ec\u6a21\u62df\u9ad8\u6210\u672c\u4f3c\u7136\u79ef\u5206\uff0c\u8ba1\u7b97\u6548\u7387\u4f4e\uff1b\u9700\u652f\u6301\u8de8\u6761\u4ef6/\u534f\u53d8\u91cf\u7684\u6837\u672c\u4f3c\u7136\u5bf9\u6bd4\uff0c\u5c24\u5176\u5728\u5355\u7ec6\u80de\u57fa\u56e0\u7ec4\u5b66\u4e2d\u9700\u9ad8\u6548\u6bd4\u8f83\u7ec6\u80de\u72b6\u6001\u4ee5\u8bc4\u4f30\u5904\u7406\u6548\u5e94\u548c\u6279\u6b21\u6821\u6b63\u3002", "method": "\u5229\u7528\u6761\u4ef6\u611f\u77e5\u6d41\u5339\u914d\u6280\u672f\uff0c\u63a8\u5bfc\u51fa\u53ef\u6cbf\u751f\u6210\u8f68\u8ff9\u8ddf\u8e2a\u5bc6\u5ea6\u6bd4\u7684\u7edf\u4e00\u52a8\u529b\u5b66\u516c\u5f0f\uff0c\u907f\u514d\u4e3a\u6bcf\u4e2a\u5206\u5e03\u72ec\u7acb\u8ba1\u7b97\u4f3c\u7136\u79ef\u5206\uff0c\u5b9e\u73b0\u5355\u6b21\u6a21\u62df\u5b8c\u6210\u591a\u5206\u5e03\u5bf9\u6bd4\u3002", "result": "\u5728\u95ed\u5f0f\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u7684\u6a21\u62df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u5177\u6709\u7ade\u4e89\u529b\uff1b\u6210\u529f\u5e94\u7528\u4e8e\u5355\u7ec6\u80de\u57fa\u56e0\u7ec4\u5b66\uff0c\u652f\u6301\u8de8\u5b9e\u9a8c\u6761\u4ef6\u7684\u7ec6\u80de\u72b6\u6001\u4f3c\u7136\u5bf9\u6bd4\uff0c\u5b9e\u73b0\u5904\u7406\u6548\u5e94\u4f30\u8ba1\u548c\u6279\u6b21\u6821\u6b63\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5355\u4e00\u52a8\u6001\u5efa\u6a21\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u590d\u6742\u9886\u57df\uff08\u5982\u57fa\u56e0\u7ec4\u5b66\uff09\u4e2d\u7684\u6982\u7387\u5206\u5e03\u5bf9\u6bd4\u63d0\u4f9b\u53ef\u6269\u5c55\u5de5\u5177\uff0c\u5e73\u8861\u4e86\u8ba1\u7b97\u6210\u672c\u4e0e\u6027\u80fd\u3002"}}
{"id": "2602.24286", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24286", "abs": "https://arxiv.org/abs/2602.24286", "authors": ["Weinan Dai", "Hanlin Wu", "Qiying Yu", "Huan-ang Gao", "Jiahao Li", "Chengquan Jiang", "Weiqiang Lou", "Yufan Song", "Hongli Yu", "Jiaze Chen", "Wei-Ying Ma", "Ya-Qin Zhang", "Jingjing Liu", "Mingxuan Wang", "Xin Liu", "Hao Zhou"], "title": "CUDA Agent: Large-Scale Agentic RL for High-Performance CUDA Kernel Generation", "comment": null, "summary": "GPU kernel optimization is fundamental to modern deep learning but remains a highly specialized task requiring deep hardware expertise. Despite strong performance in general programming, large language models (LLMs) remain uncompetitive with compiler-based systems such as torch.compile for CUDA kernel generation. Existing CUDA code generation approaches either rely on training-free refinement or fine-tune models within fixed multi-turn execution-feedback loops, but both paradigms fail to fundamentally improve the model's intrinsic CUDA optimization ability, resulting in limited performance gains. We present CUDA Agent, a large-scale agentic reinforcement learning system that develops CUDA kernel expertise through three components: a scalable data synthesis pipeline, a skill-augmented CUDA development environment with automated verification and profiling to provide reliable reward signals, and reinforcement learning algorithmic techniques enabling stable training. CUDA Agent achieves state-of-the-art results on KernelBench, delivering 100\\%, 100\\%, and 92\\% faster rate over torch.compile on KernelBench Level-1, Level-2, and Level-3 splits, outperforming the strongest proprietary models such as Claude Opus 4.5 and Gemini 3 Pro by about 40\\% on the hardest Level-3 setting.", "AI": {"tldr": "CUDA Agent uses large-scale reinforcement learning to automatically optimize CUDA kernels, achieving up to 100% speedup over torch.compile and outperforming top proprietary models like Claude Opus 4.5 by ~40% on the hardest tasks.", "motivation": "GPU kernel optimization requires deep hardware expertise, and existing LLM approaches fail to fundamentally improve intrinsic CUDA optimization ability, remaining uncompetitive with compiler-based systems like torch.compile.", "method": "A three-component agentic RL system: (1) scalable data synthesis pipeline, (2) skill-augmented CUDA environment with automated verification/profiling for reward signals, and (3) RL algorithms for stable training.", "result": "State-of-the-art performance on KernelBench: 100%, 100%, and 92% faster than torch.compile on Level-1/2/3 splits, outperforming Claude Opus 4.5 and Gemini 3 Pro by ~40% on hardest Level-3 tasks.", "conclusion": "CUDA Agent successfully develops CUDA kernel expertise through reinforcement learning, demonstrating that systematic RL training can bridge the performance gap between LLMs and specialized compiler systems for hardware optimization."}}
{"id": "2602.24207", "categories": ["cs.LG", "cs.CY", "cs.GT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.24207", "abs": "https://arxiv.org/abs/2602.24207", "authors": ["Gabriele Farina", "Juan Carlos Perdomo"], "title": "The Stability of Online Algorithms in Performative Prediction", "comment": null, "summary": "The use of algorithmic predictions in decision-making leads to a feedback loop where the models we deploy actively influence the data distributions we see, and later use to retrain on. This dynamic was formalized by Perdomo et al. 2020 in their work on performative prediction. Our main result is an unconditional reduction showing that any no-regret algorithm deployed in performative settings converges to a (mixed) performatively stable equilibrium: a solution in which models actively shape data distributions in ways that their own predictions look optimal in hindsight. Prior to our work, all positive results in this area made strong restrictions on how models influenced distributions. By using a martingale argument and allowing randomization, we avoid any such assumption and sidestep recent hardness results for finding stable models. Lastly, on a more conceptual note, our connection sheds light on why common algorithms, like gradient descent, are naturally stabilizing and prevent runaway feedback loops. We hope our work enables future technical transfer of ideas between online optimization and performativity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc1\u660e\u4e86\u4efb\u610f\u65e0\u6094\u7b97\u6cd5\u5728\u611f\u6027\u51b3\u7b56\u73af\u5883\u4e2d\u90fd\u4f1a\u6536\u655b\u81f3\u4e00\u4e2a\uff08\u6df7\u5408\uff09\u611f\u6027\u7a33\u5b9a\u5747\u8861\uff0c\u901a\u8fc7\u9785\u8bba\u8bba\u8bc1\u548c\u968f\u673a\u5316\u65b9\u6cd5\u7a81\u7834\u4ee5\u5f80\u9650\u5236\uff0c\u5e76\u89e3\u91ca\u4e86\u4e3a\u4f55\u68af\u5ea6\u4e0b\u964d\u7b49\u5e38\u89c1\u7b97\u6cd5\u5929\u7136\u5177\u6709\u7a33\u5b9a\u7279\u6027\uff0c\u9632\u6b62\u53cd\u9988\u5931\u63a7\u3002", "motivation": "\u7b97\u6cd5\u9884\u6d4b\u5728\u51b3\u7b56\u4e2d\u4f7f\u7528\u65f6\u4f1a\u5f62\u6210\u53cd\u9988\u5faa\u73af\uff0c\u90e8\u7f72\u7684\u6a21\u578b\u4f1a\u5f71\u54cd\u6240\u89c1\u7684\u6570\u636e\u5206\u5e03\uff0c\u8fdb\u800c\u5f71\u54cd\u540e\u7eed\u8bad\u7ec3\u3002\u8fd9\u4e00\u52a8\u6001\u7531Perdomo\u7b49\u4eba2020\u5e74\u5728\u611f\u6027\u9884\u6d4b\u7814\u7a76\u4e2d\u5f62\u5f0f\u5316\u3002\u6b64\u524d\u6240\u6709\u6b63\u9762\u7ed3\u679c\u90fd\u5bf9\u8be5\u6a21\u578b\u5982\u4f55\u5f71\u54cd\u5206\u5e03\u65bd\u52a0\u4e86\u5f3a\u9650\u5236\u3002", "method": "\u901a\u8fc7\u9785\u8bba\u8bba\u8bc1\u5e76\u5141\u8bb8\u968f\u673a\u5316\uff0c\u5efa\u7acb\u65e0\u6761\u4ef6\u5f52\u7ea6\uff0c\u907f\u514d\u4e86\u4efb\u4f55\u6b64\u7c7b\u5047\u8bbe\uff0c\u7ed5\u8fc7\u4e86\u5bfb\u627e\u7a33\u5b9a\u6a21\u578b\u7684\u8fd1\u671f\u56f0\u96be\u7ed3\u679c\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u611f\u6027\u73af\u5883\u4e2d\uff0c\u4efb\u4f55\u65e0\u6094\u7b97\u6cd5\u90fd\u4f1a\u6536\u655b\u5230\u4e00\u4e2a\uff08\u6df7\u5408\uff09\u611f\u6027\u7a33\u5b9a\u5747\u8861\uff1a\u6a21\u578b\u4e3b\u52a8\u5851\u9020\u6570\u636e\u5206\u5e03\uff0c\u4f7f\u5176\u81ea\u8eab\u9884\u6d4b\u5728\u4e8b\u540e\u770b\u6765\u662f\u6700\u4f73\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8fd9\u4e00\u8054\u7cfb\u9610\u660e\u4e86\u4e3a\u4f55\u68af\u5ea6\u4e0b\u964d\u7b49\u5e38\u89c1\u7b97\u6cd5\u5929\u7136\u5177\u6709\u7a33\u5b9a\u7279\u6027\uff0c\u80fd\u9632\u6b62\u5931\u63a7\u53cd\u9988\u5faa\u73af\u3002\u671f\u671b\u8be5\u5de5\u4f5c\u80fd\u4fc3\u8fdb\u5728\u7ebf\u4f18\u5316\u4e0e\u611f\u6027\u51b3\u7b56\u4e24\u4e2a\u9886\u57df\u4e4b\u95f4\u7684\u6280\u672f\u601d\u60f3\u8f6c\u79fb\u3002"}}
{"id": "2602.24231", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24231", "abs": "https://arxiv.org/abs/2602.24231", "authors": ["Hongrui Xie", "Junyu Cao", "Kan Xu"], "title": "Adaptive Combinatorial Experimental Design: Pareto Optimality for Decision-Making and Inference", "comment": "30 pages, 3 figure, AISTATS 2026 accepted paper", "summary": "In this paper, we provide the first investigation into adaptive combinatorial experimental design, focusing on the trade-off between regret minimization and statistical power in combinatorial multi-armed bandits (CMAB). While minimizing regret requires repeated exploitation of high-reward arms, accurate inference on reward gaps requires sufficient exploration of suboptimal actions. We formalize this trade-off through the concept of Pareto optimality and establish equivalent conditions for Pareto-efficient learning in CMAB. We consider two relevant cases under different information structures, i.e., full-bandit feedback and semi-bandit feedback, and propose two algorithms MixCombKL and MixCombUCB respectively for these two cases. We provide theoretical guarantees showing that both algorithms are Pareto optimal, achieving finite-time guarantees on both regret and estimation error of arm gaps. Our results further reveal that richer feedback significantly tightens the attainable Pareto frontier, with the primary gains arising from improved estimation accuracy under our proposed methods. Taken together, these findings establish a principled framework for adaptive combinatorial experimentation in multi-objective decision-making.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u7814\u7a76\u81ea\u9002\u5e94\u7ec4\u5408\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u5728\u7ec4\u5408\u591a\u81c2\u8001\u864e\u673a\u4e2d\u5efa\u7acb\u9057\u61be\u6700\u5c0f\u5316\u4e0e\u7edf\u8ba1\u529f\u6548\u4e4b\u95f4\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u6743\u8861\u6846\u67b6\uff0c\u63d0\u51fa\u4e24\u79cd\u7b97\u6cd5\u5e76\u8bc1\u660e\u5176\u7406\u8bba\u4f18\u8d8a\u6027\u3002", "motivation": "\u4f20\u7edf\u7ec4\u5408\u5b9e\u9a8c\u8bbe\u8ba1\u9762\u4e34\u6838\u5fc3\u77db\u76fe\uff1a\u6700\u5c0f\u5316\u9057\u61be\u9700\u91cd\u590d\u5229\u7528\u9ad8\u56de\u62a5\u7b56\u7565\uff0c\u800c\u51c6\u786e\u63a8\u65ad\u5956\u52b1\u5dee\u8ddd\u9700\u5145\u5206\u63a2\u7d22\u6b21\u4f18\u7b56\u7565\u3002\u73b0\u6709\u7814\u7a76\u672a\u7cfb\u7edf\u89e3\u51b3\u8fd9\u4e00\u591a\u76ee\u6807\u6743\u8861\u95ee\u9898\u3002", "method": "1) \u901a\u8fc7\u5e15\u7d2f\u6258\u6700\u4f18\u6027\u5f62\u5f0f\u5316\u6743\u8861\u5173\u7cfb\uff1b2) \u9488\u5bf9\u5168\u81c2\u53cd\u9988\u548c\u534a\u81c2\u53cd\u9988\u4e24\u79cd\u4fe1\u606f\u7ed3\u6784\uff0c\u5206\u522b\u63d0\u51faMixCombKL\u548cMixCombUCB\u7b97\u6cd5\uff1b3) \u5efa\u7acb\u6709\u9650\u65f6\u95f4\u7406\u8bba\u4fdd\u8bc1\u3002", "result": "1) \u8bc1\u660e\u4e24\u79cd\u7b97\u6cd5\u5747\u8fbe\u5230\u5e15\u7d2f\u6258\u6700\u4f18\uff1b2) \u63ed\u793a\u66f4\u4e30\u5bcc\u7684\u53cd\u9988\u80fd\u663e\u8457\u6536\u7d27\u5e15\u7d2f\u6258\u524d\u6cbf\uff1b3) \u6838\u5fc3\u6539\u8fdb\u6e90\u4e8e\u6240\u63d0\u65b9\u6cd5\u5728\u4f30\u8ba1\u7cbe\u5ea6\u4e0a\u7684\u63d0\u5347\u3002", "conclusion": "\u5efa\u7acb\u4e86\u81ea\u9002\u5e94\u7ec4\u5408\u5b9e\u9a8c\u7684\u591a\u76ee\u6807\u51b3\u7b56\u7406\u8bba\u6846\u67b6\uff0c\u4e3a\u5e73\u8861\u63a2\u7d22-\u5229\u7528\u4e0e\u7edf\u8ba1\u63a8\u65ad\u9700\u6c42\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.24238", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24238", "abs": "https://arxiv.org/abs/2602.24238", "authors": ["Javier Pulido", "Filipe Rodrigues"], "title": "Time Series Foundation Models as Strong Baselines in Transportation Forecasting: A Large-Scale Benchmark Analysis", "comment": "6 pages", "summary": "Accurate forecasting of transportation dynamics is essential for urban mobility and infrastructure planning. Although recent work has achieved strong performance with deep learning models, these methods typically require dataset-specific training, architecture design and hyper-parameter tuning. This paper evaluates whether general-purpose time-series foundation models can serve as forecasters for transportation tasks by benchmarking the zero-shot performance of the state-of-the-art model, Chronos-2, across ten real-world datasets covering highway traffic volume and flow, urban traffic speed, bike-sharing demand, and electric vehicle charging station data. Under a consistent evaluation protocol, we find that, even without any task-specific fine-tuning, Chronos-2 delivers state-of-the-art or competitive accuracy across most datasets, frequently outperforming classical statistical baselines and specialized deep learning architectures, particularly at longer horizons. Beyond point forecasting, we evaluate its native probabilistic outputs using prediction-interval coverage and sharpness, demonstrating that Chronos-2 also provides useful uncertainty quantification without dataset-specific training. In general, this study supports the adoption of time-series foundation models as a key baseline for transportation forecasting research.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578bChronos-2\u5728\u4ea4\u901a\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u53d1\u73b0\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\uff0c\u8be5\u6a21\u578b\u572810\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5373\u53ef\u8fbe\u5230\u9876\u5c16\u6216\u6781\u5177\u7ade\u4e89\u529b\u7684\u7cbe\u5ea6\uff0c\u652f\u6301\u5c06\u5176\u4f5c\u4e3a\u4ea4\u901a\u9884\u6d4b\u7814\u7a76\u7684\u57fa\u51c6\u65b9\u6cd5\u3002", "motivation": "\u4ea4\u901a\u52a8\u6001\u7684\u7cbe\u51c6\u9884\u6d4b\u5bf9\u57ce\u5e02\u51fa\u884c\u548c\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u867d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u9700\u8981\u9488\u5bf9\u6bcf\u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u4e13\u95e8\u8bad\u7ec3\u3001\u67b6\u6784\u8bbe\u8ba1\u548c\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002", "method": "\u5728\u4e00\u81f4\u7684\u8bc4\u4f30\u534f\u8bae\u4e0b\uff0c\u5bf9Chronos-2\u6a21\u578b\u8fdb\u884c\u96f6\u6837\u672c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d610\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\uff08\u5305\u62ec\u9ad8\u901f\u516c\u8def\u4ea4\u901a\u91cf/\u6d41\u91cf\u3001\u57ce\u5e02\u4ea4\u901a\u901f\u5ea6\u3001\u5171\u4eab\u5355\u8f66\u9700\u6c42\u548c\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u7ad9\u6570\u636e\uff09\uff0c\u5e76\u8bc4\u4f30\u5176\u70b9\u9884\u6d4b\u548c\u6982\u7387\u9884\u6d4b\u80fd\u529b\u3002", "result": "Chronos-2\u5728\u5927\u591a\u6570\u6570\u636e\u96c6\u4e0a\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u8fbe\u5230\u9876\u5c16\u6216\u5177\u7ade\u4e89\u529b\u7684\u7cbe\u5ea6\uff0c\u5e38\u4f18\u4e8e\u7ecf\u5178\u7edf\u8ba1\u57fa\u7ebf\u548c\u4e13\u7528\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u5c24\u5176\u5728\u66f4\u957f\u9884\u6d4b\u89c6\u754c\u8868\u73b0\u7a81\u51fa\uff1b\u540c\u65f6\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u672c\u5730\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "conclusion": "\u8be5\u7814\u7a76\u652f\u6301\u5c06\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u4ea4\u901a\u9884\u6d4b\u7814\u7a76\u7684\u5173\u952e\u57fa\u51c6\uff0c\u4e3a\u5176\u5728\u4ea4\u901a\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\u3002"}}
{"id": "2602.24245", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24245", "abs": "https://arxiv.org/abs/2602.24245", "authors": ["Hainan Xu", "Vladimir Bataev", "Travis M. Bartley", "Jagadeesh Balam"], "title": "Chunk-wise Attention Transducers for Fast and Accurate Streaming Speech-to-Text", "comment": "Accepted at ICASSP 2026", "summary": "We propose Chunk-wise Attention Transducer (CHAT), a novel extension to RNN-T models that processes audio in fixed-size chunks while employing cross-attention within each chunk. This hybrid approach maintains RNN-T's streaming capability while introducing controlled flexibility for local alignment modeling. CHAT significantly reduces the temporal dimension that RNN-T must handle, yielding substantial efficiency improvements: up to 46.2% reduction in peak training memory, up to 1.36X faster training, and up to 1.69X faster inference. Alongside these efficiency gains, CHAT achieves consistent accuracy improvements over RNN-T across multiple languages and tasks -- up to 6.3% relative WER reduction for speech recognition and up to 18.0% BLEU improvement for speech translation. The method proves particularly effective for speech translation, where RNN-T's strict monotonic alignment hurts performance. Our results demonstrate that the CHAT model offers a practical solution for deploying more capable streaming speech models without sacrificing real-time constraints.", "AI": {"tldr": "CHAT is a novel extension to RNN-T models that processes audio in fixed-size chunks with cross-attention, achieving significant efficiency gains (up to 46.2% less memory, 1.69X faster inference) and accuracy improvements (up to 6.3% WER reduction for ASR, 18.0% BLEU gain for translation) while maintaining streaming capability.", "motivation": "RNN-T's strict monotonic alignment limits performance for speech translation, and the model's computational demands are high. The paper aims to maintain streaming capability while introducing controlled flexibility for local alignment modeling to improve both efficiency and accuracy.", "method": "Chunk-wise Attention Transducer (CHAT): processes audio in fixed-size chunks with cross-attention within each chunk. This hybrid approach reduces temporal dimension that RNN-T must handle, combining streaming capability with local alignment flexibility.", "result": "Up to 46.2% reduction in peak training memory, 1.36X faster training, 1.69X faster inference. Up to 6.3% relative WER reduction for speech recognition and 18.0% BLEU improvement for speech translation across multiple languages/tasks.", "conclusion": "CHAT provides a practical solution for deploying more capable streaming speech models without sacrificing real-time constraints, particularly effective for speech translation where monotonic alignment is limiting."}}
{"id": "2602.24278", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24278", "abs": "https://arxiv.org/abs/2602.24278", "authors": ["Shruti Joshi", "Th\u00e9o Saulus", "Wieland Brendel", "Philippe Brouillard", "Dhanya Sridhar", "Patrik Reizinger"], "title": "Who Guards the Guardians? The Challenges of Evaluating Identifiability of Learned Representations", "comment": null, "summary": "Identifiability in representation learning is commonly evaluated using standard metrics (e.g., MCC, DCI, R^2) on synthetic benchmarks with known ground-truth factors. These metrics are assumed to reflect recovery up to the equivalence class guaranteed by identifiability theory. We show that this assumption holds only under specific structural conditions: each metric implicitly encodes assumptions about both the data-generating process (DGP) and the encoder. When these assumptions are violated, metrics become misspecified and can produce systematic false positives and false negatives. Such failures occur both within classical identifiability regimes and in post-hoc settings where identifiability is most needed. We introduce a taxonomy separating DGP assumptions from encoder geometry, use it to characterise the validity domains of existing metrics, and release an evaluation suite for reproducible stress testing and comparison.", "AI": {"tldr": "Standard metrics for evaluating identifiability in representation learning (e.g., MCC, DCI, R\u00b2) are unreliable because they rely on hidden assumptions about data generation and encoder structure; when violated, they produce systematic errors. The authors introduce a taxonomy to clarify these assumptions, map the validity domains of existing metrics, and release a stress-testing suite.", "motivation": "Existing evaluation metrics for identifiability are assumed to correctly measure factor recovery up to theoretical equivalence classes, but this assumption lacks rigorous justification. The paper seeks to uncover the implicit assumptions behind these metrics and demonstrate that they can fail systematically, especially in post-hoc evaluation settings where identifiability is most critical.", "method": "The authors develop a taxonomy that disentangles two types of assumptions: those about the data-generating process (DGP) and those about the encoder geometry. Using this framework, they systematically analyze and characterize the conditions under which standard metrics (MCC, DCI, R\u00b2) remain valid, revealing hidden failure modes. They also create an open-source evaluation suite for reproducible stress testing.", "result": "The study reveals that standard metrics produce systematic false positives and false negatives when their implicit assumptions are violated, even within classical identifiability regimes. This demonstrates that metric validity is not universal but depends on specific structural conditions. The evaluation suite quantifies these failures across diverse settings.", "conclusion": "Evaluation of identifiability using standard metrics is more fragile than previously recognized. Researchers must explicitly verify metric assumptions rather than treating them as black boxes. The proposed taxonomy clarifies validity domains, and the released evaluation suite enables more rigorous, reproducible assessment of identifiability in representation learning."}}
