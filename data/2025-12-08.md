<div id=toc></div>

# Table of Contents

- [quant-ph](#quant-ph) [Total: 26]
- [cs.LG](#cs.LG) [Total: 40]
- [cs.AI](#cs.AI) [Total: 19]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 10]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 6]


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [1] [A Mutual Information-based Metric for Temporal Expressivity and Trainability Estimation in Quantum Policy Gradient Pipelines](https://arxiv.org/abs/2512.05157)
*Jaehun Jeong,Donghwa Ji,Junghee Ryu,Kabgyun Jeong*

Main category: quant-ph

TL;DR: 该研究为量子强化学习中的参数化量子电路定义了新的表达能力概念，并提出互信息作为评估表达能力和可训练性的指标，为电路选择和学习进度估计提供依据。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习的局限性推动了强化学习及量子强化学习的发展。在量子强化学习中，基于梯度的方法（特别是策略梯度方法）具有优势，可以通过参数化量子电路实现。然而，尽管已有研究量化PQCs的表达能力和可训练性，但在强化学习背景下缺乏明确的研究。因此需要定义适合强化学习的表达能力概念，并找到评估表达能力和可训练性的指标。

Method: 研究新定义了适合强化学习的表达能力概念，并证明动作分布与奖励信号分布之间的互信息可以在某些方面指示表达能力和可训练性信息。该方法为选择不同PQCs提供了简单标准，并能在黑盒设置中间接估计学习进度。

Result: 研究表明互信息可以作为评估参数化量子电路在强化学习中表达能力和可训练性的有效指标。这为量子强化学习中的电路选择提供了实用标准，并解决了黑盒环境中学习进度难以评估的问题。

Conclusion: 该研究为量子强化学习中的参数化量子电路提供了新的表达能力定义和评估框架，通过互信息指标为电路选择和学习进度监控提供了实用工具，对量子强化学习的实际应用具有重要意义。

Abstract: In recent years, various limitations of conventional supervised learning have been highlighted, leading to the emergence of reinforcement learning -- and, further, quantum reinforcement learning that exploits quantum resources such as entanglement and superposition -- as promising alternatives. Among the various reinforcement learning methodologies, gradient-based approaches, particularly policy gradient methods, are considered to have many benefits. Moreover, in the quantum regime, they also have a profit in that they can be readily implemented through parameterized quantum circuits (PQCs). From the perspective of learning, two indicators can be regarded as most crucial: expressivity and, for gradient-based methods, trainability. While a number of attempts have been made to quantify the expressivity and trainability of PQCs, clear efforts in the context of reinforcement learning have so far been lacking. Therefore, in this study, we newly define the notion of expressivity suited to reinforcement learning and demonstrate that the mutual information between action distribution and reward-signal distribution can, in certain respects, indicate information about both expressivity and trainability. Such research is valuable in that it provides an easy criterion for choosing among various PQCs employed in reinforcement learning, and further, enables the indirect estimation of learning progress even in black-box settings where the agent's achievement aligned with the episodes cannot be explicitly evaluated.

</details>


### [2] [From Kinematics to Interference: Operational Requirements for the Quantum Principle of Relativity](https://arxiv.org/abs/2512.05164)
*Mikołaj Sienicki,Krzysztof Sienicki*

Main category: quant-ph

TL;DR: 该论文提出将量子相对性原理（QPR）的讨论分为三个层次：运动学、操作内容和动力学/桥梁，旨在为"相对论推导量子理论"这一宏大目标提供清晰的检查清单。


<details>
  <summary>Details</summary>
Motivation: 量子相对性原理提出通过引入形式上的超光速洛伦兹型映射来扩展狭义相对论，并将由此产生的一致性约束视为量子理论结构形成的线索。然而，现有讨论存在混淆：坐标映射不等于物理理论，量子叠加需要操作定义。本文旨在澄清这些概念层次。

Method: 采用组织性方法，将量子相对性原理的讨论明确分为三个独立层次：(K)运动学（存在哪些映射及其守恒量）、(O)操作内容（实验必须重现什么，特别是闭合环路干涉）、(D/B)动力学和桥梁（振幅和概率如何产生，亚光速和超光速区域如何连接）。

Result: 提出了一个清晰的框架，将量子相对性原理的复杂讨论系统化，为"相对论推导量子理论"这一研究计划提供了结构化的检查清单，明确了需要补充的内容。

Conclusion: 本文的目标不是证明相对论可以推导出量子理论，而是为这一宏大目标提供一个清晰的概念框架，明确区分运动学、操作内容和动力学/桥梁三个层次，使相关研究计划能够成为定义明确的研究项目。

Abstract: The quantum principle of relativity (QPR) puts forward an ambitious idea: extend special relativity with a formally superluminal branch of Lorentz-type maps, and treat the resulting consistency constraints as hints about why quantum theory has the structure it does [1]. The discussion that followed has emphasized a basic point: writing down coordinate maps is not the same thing as providing a physical theory. In particular, quantum superposition is not operationally defined by drawing multiple paths on paper: it is defined by what happens when alternatives recombine in an interference loop [2, 3]. In parallel, careful 1+1 analyses have clarified how sign conventions and time-orientation choices enter the superluminal formulas [4]. Finally, tachyonic QFT proposals suggest a possible mathematical bridge via an enlarged (twin) Hilbert space [5], although this proposal remains contested (e.g., on commutator covariance and microcausality grounds) [6]. The aim of this short note is organizational. We keep three layers separate: (K) kinematics (which maps exist and what they preserve), (O) operational content (what an experiment must actually reproduce, especially closed-loop interference), and (D/B) dynamics and bridges (how amplitudes and probabilities are generated, and how subluminal and superluminal sectors might be linked). The goal is not relativity derives quantum theory, but a clear checklist of what must be added for that ambition to become a well-posed programme.

</details>


### [3] [Quantum compilation framework for data loading](https://arxiv.org/abs/2512.05183)
*Guillermo Alonso-Linaje,Utkarsh Azad,Jay Soni,Jarrett Smalley,Leigh Lapworth,Juan Miguel Arrazola*

Main category: quant-ph

TL;DR: 提出自动化量子数据加载编译框架，通过误差预算分配在精度和近似间权衡，显著减少量子资源消耗


<details>
  <summary>Details</summary>
Motivation: 经典数据到量子电路的编码效率直接影响量子算法的可扩展性，需要资源感知的自动化编译方法

Method: 开发自动化编译框架，系统分配总误差预算到精度和近似误差，支持多种先进方法（多路复用器、QROM、稀疏编码、MPS、FSL、Walsh变换等）

Result: 框架在多个应用中揭示非显而易见的资源高效策略，在计算流体动力学中实现超过4个数量级的资源减少，并开发了更高效的d-对角矩阵电路和动能算子优化块编码

Conclusion: 自动化、近似感知的编译对于在资源受限硬件上实现大规模量子算法至关重要

Abstract: Efficient encoding of classical data into quantum circuits is a critical challenge that directly impacts the scalability of quantum algorithms. In this work, we present an automated compilation framework for resource-aware quantum data loading tailored to a given input vector and target error tolerance. By explicitly exploiting the trade-off between exact and approximate state preparation, our approach systematically partitions the total error budget between precision and approximation errors, thereby minimizing quantum resource costs. The framework supports a comprehensive suite of state-of-the-art methods, including multiplexer-based loaders, quantum read-only memory (QROM) constructions, sparse encodings, matrix product states (MPS), Fourier series loaders (FSL), and Walsh transform-based diagonal operators. We demonstrate the effectiveness of our framework across several applications, where it consistently uncovers non-obvious, resource-efficient strategies enabled by controlled approximation. In particular, we analyze a computational fluid dynamics workflow where the automated selection of MPS state preparation and Walsh transform-based encoding, combined with a novel Walsh-based measurement technique, leads to resource reductions of over four orders of magnitude compared to previous approaches. We also introduce two independent advances developed through the framework: a more efficient circuit for d-diagonal matrices, and an optimized block encoding for kinetic energy operators. Our results underscore the indispensable role of automated, approximation-aware compilation in making large-scale quantum algorithms feasible on resource-constrained hardware.

</details>


### [4] [The deep Hilbert space of all-to-all interacting SU(3) atoms: from quantum to classical](https://arxiv.org/abs/2512.05184)
*Federico Balducci,Aleksandra A. Ziolkowska*

Main category: quant-ph

TL;DR: 该研究探讨了全对全相互作用多能级原子系统中的混沌涌现，重点关注3能级Tavis-Cummings模型，揭示了其希尔伯特空间的分裂结构和不同对称性扇区的动力学行为。


<details>
  <summary>Details</summary>
Motivation: 研究全对全相互作用多能级原子系统中的混沌动力学，特别是超越完全对称子空间的结构，探索量子-经典对应关系，揭示多能级全对全相互作用模型的丰富结构。

Method: 使用Schur-Weyl对偶性分析3能级Tavis-Cummings模型在远失谐极限下的希尔伯特空间结构，枚举所有不同的动力学扇区；提出基于自旋相干态的半经典描述来解释混沌或规则动力学的起源。

Result: 发现由于非阿贝尔对称性导致的强希尔伯特空间分裂，某些扇区显示规则动力学而其他扇区呈现混沌；在经典极限下，许多置换对称性扇区对动力学有贡献，超越了通常研究的完全对称子空间。

Conclusion: 该工作为混沌系统中的量子-经典对应关系研究做出贡献，揭示了多能级全对全相互作用模型的丰富结构，并通过简单的几何论证解释了混沌或规则动力学的起源。

Abstract: We study the emergence of chaos in multilevel atoms with all-to-all interactions, inspired by cavity QED. Focusing on a 3-level Tavis-Cummings model in a far detuned limit, we detail its deep Hilbert space structure -- i.e. we enumerate all distinct dynamical sectors, beyond the totally symmetric subspace -- by using the Schur-Weyl duality, which is applicable thanks to the permutation symmetry in the all-to-all Hamiltonian. Strong Hilbert space fragmentation ensues from the non-abelian nature of the symmetry, with some sectors displaying regular dynamics and others being chaotic. We uncover that many permutation symmetry sectors contribute to the dynamics in the classical limit, in addition to the commonly studied totally symmetric subspace. To elucidate the dynamical responses in each of the symmetry sectors, we propose a semiclassical description in terms of spin coherent states, which is also able to explain the origin of chaotic or regular dynamics with a simple geometrical argument. Our work contributes to the study of the quantum-classical correspondence in chaotic systems, and uncovers a rich structure in multilevel all-to-all interacting models.

</details>


### [5] [Multimode equilibrium approximations in light-matter systems from weak to strong coupling](https://arxiv.org/abs/2512.05196)
*Davis M. Welakuh,Vasil Rokaj,Michael Ruggenthaler,Angel Rubio*

Main category: quant-ph

TL;DR: 本文提出了在长波近似下高效处理多模光子环境的多种方法，用于非相对论量子电动力学中的光-物质耦合系统平衡性质研究。


<details>
  <summary>Details</summary>
Motivation: 研究动机是开发能够准确描述复杂光子环境中量子光-物质系统平衡性质的高效计算方法，为真实材料在腔量子电动力学中的第一性原理模拟奠定基础。

Method: 提出了两种主要方法：1) 在长度规范表述中仅保留电场的极化部分；2) 使用少量有效模式近似。这些方法适用于从弱耦合到强耦合的各种复杂光子环境。

Result: 方法成功应用于原子和分子模型以及二维量子环系统，证明了该方法的通用性和有效性，能够准确捕捉多模光子环境对物质系统的影响。

Conclusion: 本文提出的近似方法为高效处理多模光子环境中的光-物质耦合系统平衡性质提供了有效工具，为真实材料的腔量子电动力学第一性原理模拟奠定了基础。

Abstract: In this work, we detail different approaches to treat multi-mode photonic environments within non-relativistic quantum electrodynamics in the long-wavelength approximation efficiently. Specifically we show that for equilibrium properties of coupled light-matter systems, we can approximately capture the effects of multi-mode photonic environments on matter systems by either only keeping the polarization part of the electric field in the length-gauge formulation or by a few effective modes. We present a comprehensive set of approximation methods designed to accurately capture equilibrium phenomena in quantum light-matter systems across a range of complex photonic environments, from weak to strong coupling. These methods are applied to atomic and molecular models as well as to a two-dimensional quantum ring, demonstrating the versatility of our approach and laying the groundwork for first-principles simulations of real materials in cavity quantum electrodynamics.

</details>


### [6] [Constraint-oriented biased quantum search for linear constrained combinatorial optimization problems](https://arxiv.org/abs/2512.05205)
*Sören Wilkening,Timo Ziegler,Maximilian Hess*

Main category: quant-ph

TL;DR: 本文扩展了先前提出的基于Grover的启发式方法，用于解决具有线性约束的一般组合优化问题，并将其描述为一个可通过电路优化和机器学习技术提升性能的框架。


<details>
  <summary>Details</summary>
Motivation: 扩展Grover搜索算法以解决更广泛的组合优化问题，特别是那些带有线性约束的问题，探索量子计算在优化问题上的潜力。

Method: 扩展先前提出的基于Grover的启发式方法，将其构建为一个框架，该框架允许通过电路优化和机器学习技术来提升算法性能。

Result: 与最先进的经典求解器比较表明，该算法在速度方面具有实现量子优势的潜力，前提是有适当的量子硬件支持。

Conclusion: 提出的框架为组合优化问题的量子求解提供了有前景的途径，通过电路优化和机器学习技术可以进一步提升性能，有望在未来量子硬件成熟时实现量子优势。

Abstract: In this paper, we extend a previously presented Grover-based heuristic to tackle general combinatorial optimization problems with linear constraints. We further describe the introduced method as a framework that enables performance improvements through circuit optimization and machine learning techniques. Comparisons with state-of-the-art classical solvers further demonstrate the algorithm's potential to achieve a quantum advantage in terms of speed, given appropriate quantum hardware.

</details>


### [7] [A Framework for Quantum Simulations of Energy-Loss and Hadronization in Non-Abelian Gauge Theories: SU(2) Lattice Gauge Theory in 1+1D](https://arxiv.org/abs/2512.05210)
*Zhiyao Li,Marc Illa,Martin J. Savage*

Main category: quant-ph

TL;DR: 该论文建立了在量子计算机上模拟能量损失和强子化的框架，应用于重夸克在SU(2)格点上的运动，并成功在IBM量子计算机上实现了18量子位的模拟。


<details>
  <summary>Details</summary>
Motivation: 能量损失和强子化模拟对于理解非平衡强相互作用物质现象至关重要，但传统计算方法面临挑战，需要开发量子计算方法来模拟非阿贝尔理论。

Method: 建立了量子计算框架，将重夸克映射为量子比特，使用强子算符实现颜色纠缠，采用域分解进行量子态制备，设计可扩展量子电路处理非阿贝尔荷异质性，使用费米子SWAP操作实现重夸克离散运动。

Result: 在IBM ibm_pittsburgh量子计算机上使用18个量子比特成功模拟了L=3空间格点系统，状态制备、运动和二阶Trotter时间演化电路的双量子比特深度为398，通过误差缓解技术获得与经典模拟一致的结果。

Conclusion: 该框架成功实现了非阿贝尔理论在量子计算机上的模拟，验证了量子计算在强相互作用物理中的可行性，并可推广到包括量子色动力学SU(3)在内的其他非阿贝尔群。

Abstract: Simulations of energy loss and hadronization are essential for understanding a range of phenomena in non-equilibrium strongly-interacting matter. We establish a framework for performing such simulations on a quantum computer and apply it to a heavy quark moving across a modest-sized 1+1D SU(2) lattice of light quarks. Conceptual advances with regard to simulations of non-Abelian versus Abelian theories are developed, allowing for the evolution of the energy in light quarks, of their local non-Abelian charge densities, and of their multi-partite entanglement to be computed. The non-trivial action of non-Abelian charge operators on arbitrary states suggests mapping the heavy quarks to qubits alongside the light quarks, and limits the heavy-quark motion to discrete steps among spatial lattice sites. Further, the color entanglement among the heavy quarks and light quarks is implemented using hadronic operators, and Domain Decomposition is shown to be effective in quantum state preparation. Scalable quantum circuits that account for the heterogeneity of non-Abelian charge sectors across the lattice are used to prepare the interacting ground-state wavefunction in the presence of heavy quarks. The discrete motion of heavy quarks between adjacent spatial sites is implemented using fermionic SWAP operations. Quantum simulations of the dynamics of a system on $L=3$ spatial sites are performed using IBM's ${\tt ibm\_pittsburgh}$ quantum computer using 18 qubits, for which the circuits for state preparation, motion, and one second-order Trotter step of time evolution have a two-qubit depth of 398. A suite of error mitigation techniques are used to extract the observables from the simulations, providing results that are in good agreement with classical simulations. The framework presented here generalizes straightforwardly to other non-Abelian groups, including SU(3) for quantum chromodynamics.

</details>


### [8] [Analog quantum simulation of the Lipkin-Meshkov-Glick model in a transmon qudit](https://arxiv.org/abs/2512.05237)
*Elizabeth Champion,Annie Schwartz,Muhammad A. Ijaz,Xiaohui Xu,Steve Campbell,Gabriel T. Landi,Machiel S. Blok*

Main category: quant-ph

TL;DR: 该研究首次在单个超导transmon qudit（d=9能级）上实验实现了Lipkin-Meshkov-Glick模型的模拟，展示了五种量子临界性前兆，为模拟多体物理提供了新路径。


<details>
  <summary>Details</summary>
Motivation: 虽然量子模拟是多数量子计算机追求的重要应用，但现有的模拟大多基于量子比特处理器，而许多物理系统更自然地用qudit（d能级系统）表示。研究旨在探索基于qudit的模拟器在模拟多体物理方面的潜力。

Method: 通过转换到旋转框架，利用多个同时驱动实现任意时间依赖局域场和单轴扭曲演化。结合qudit态的通用控制和单次读取，设计了无需系统本征态先验知识的协议来提取相关性质。

Result: 成功在单个超导transmon qudit（d=9）上实现了LMG模型的模拟，详细研究了五种量子临界性前兆：动力学相变、能隙闭合、Kibble-Zurek类动力学、序参量统计和激发态相变。

Conclusion: 高维transmon qudit为模拟多体物理提供了有前景的路径，所开发的方法可扩展到更高维度或更复杂模型，无需系统本征态的先验知识。

Abstract: The simulation of large-scale quantum systems is one of the most sought-after applications of quantum computers. Of particular interest for near-term demonstrations of quantum computational advantage are analog quantum simulations, which employ analog controls instead of digitized gates. Most analog quantum simulations to date, however, have been performed using qubit-based processors, despite the fact that many physical systems are more naturally represented in terms of qudits (i.e., $d$-level systems). Motivated by this, we present an experimental realization of the Lipkin-Meshkov-Glick (LMG) model using an analog simulator based on a single superconducting transmon qudit with up to $d = 9$ levels. This is accomplished by moving to a rotated frame in which evolution under any time-dependent local field and one-axis twisting can be realized by the application of multiple simultaneous drives. Combining this analog drive scheme with universal control and single-shot readout of the qudit state, we provide a detailed study of five finite-size precursors of quantum criticality in the LMG model: dynamical phase transitions, closing of the energy gap, Kibble-Zurek-like dynamics, statistics of the order parameter, and excited-state phase transitions. For each experiment we devise a protocol for extracting the relevant properties which does not require any prior knowledge of the system eigenstates, and can therefore be readily extended to higher dimensions or more complicated models. Our results cement high-dimensional transmon qudits as an exciting path towards simulating many-body physics.

</details>


### [9] [Boosting Work Extraction in Quantum Batteries via Continuous Environment Monitoring](https://arxiv.org/abs/2512.05244)
*Gabriele Cenedese,Giuliano Benenti,Dario Ferraro,Marco G. Genoni*

Main category: quant-ph

TL;DR: 通过耦合可连续监测的额外环境，可以减弱量子电池与充电器之间的相关性，从而提升可提取功，超越理想封闭系统的极限。


<details>
  <summary>Details</summary>
Motivation: 量子电池与充电器之间通常会产生量子相关性，这会减少从电池中可提取的功量。需要找到方法来减弱这些相关性以提高工作提取效率。

Method: 通过将系统与一个可连续监测的额外环境耦合，利用环境监测来减弱量子电池与充电器之间的相关性。使用腔介导的自旋-自旋模型和Dicke量子电池模型来验证这一机制。

Result: 该方法能够有效减弱量子相关性，使可提取功超越理想封闭系统所能达到的极限，在两种量子电池模型中都得到了验证。

Conclusion: 通过耦合可连续监测的额外环境来减弱量子相关性，是提高量子电池工作提取效率的有效通用机制。

Abstract: Quantum correlations that typically develop between a quantum battery and its charger reduce the amount of work extractable from the battery. We show that by coupling the system with an additional environment that can be continuously monitored, one can weaken these correlations and enhance work extraction beyond what is achievable in the ideal (closed system) limit. This general mechanism is illustrated using both a cavity-mediated spin-spin and Dicke quantum battery models.

</details>


### [10] [Real-time optimal quantum control for atomic magnetometers with decoherence](https://arxiv.org/abs/2512.05265)
*Julia Amoros-Binefa*

Main category: quant-ph

TL;DR: 该研究将连续量子测量和估计理论应用于光学原子磁力计，推导了量子噪声对瞬态场传感的基本限制，开发了可扩展的量子动力学模型和实时估计控制架构，实现了接近量子极限的磁场跟踪。


<details>
  <summary>Details</summary>
Motivation: 量子纠缠（如自旋压缩）已知能提高原子传感器对静态或缓慢变化场的灵敏度，但瞬态事件传感面临不同挑战，在自旋进动磁力测量等实际重要场景中尚未证明能从纠缠中受益。需要准确建模、解释测量数据、控制动力学并实现最优灵敏度。

Method: 1. 推导量子噪声对瞬态场传感的基本限制；2. 开发基于随机主方程的共动高斯近似的可扩展量子动力学模型，包含测量反作用和退相干；3. 设计集成扩展卡尔曼滤波器和线性二次调节器的实时估计控制架构；4. 模拟磁力计性能并验证策略有效性。

Result: 1. 发现量子噪声限制的灵敏度最佳按传感时间和原子数N线性缩放，排除任何超经典缩放；2. 该限制与初始状态、测量、估计器和基于测量的反馈无关，仅取决于退相干模型和场波动强度；3. 开发的EKF+LQR策略能实现接近量子极限的恒定和波动场跟踪；4. 该策略还能跟踪生物相关信号（如心跳波形）并驱动原子系综进入纠缠态。

Conclusion: 该研究为光学原子磁力计建立了量子极限传感的理论框架和实用控制策略，证明了量子极限的瞬态场跟踪在当前原子磁力计中是可实现的，为实际量子增强传感应用提供了重要工具。

Abstract: Quantum entanglement, in the form of spin squeezing, is known to improve the sensitivity of atomic sensors to static or slowly varying fields. Sensing transient events presents a distinct challenge, requires different analysis tools, and has not been shown to benefit from entanglement in practically important scenarios such as spin-precession magnetometry. To address this, we apply concepts from continuous quantum measurements and estimation theory to optical atomic magnetometers, aiming to accurately model these devices, interpret their measurement data, control their dynamics, and achieve optimal sensitivity. Quantifying this optimal performance requires determining a fundamental quantum limit on sensitivity. We derive this limit, imposed by noise, and show that it scales at best linearly with sensing time and atom number N, ruling out any super-classical scaling. This limit is independent of the initial state, measurement, estimator, and measurement-based feedback, and depends only on the decoherence model and the strength of field fluctuations. Thus, finding an estimator that attains this bound proves the sensing strategy optimal. To approach this limit, we develop a quantum dynamical model scalable with N, based on a co-moving Gaussian approximation of the stochastic master equation, which includes measurement backaction and decoherence. This enables a real-time estimation and control architecture integrating an extended Kalman filter with a linear quadratic regulator. Simulating the magnetometer with our model and EKF+LQR strategy shows that quantum-limited tracking of constant and fluctuating fields is within reach of current atomic magnetometers. Our sensing strategy can also track biologically relevant signals, such as heartbeat-like waveforms, and drive the atomic ensemble into an entangled state, even when the measurement record is used for feedback but later discarded.

</details>


### [11] [Squeezing Classical Antiferromagnets into Quantum Spin Liquids via Global Cavity Fluctuations](https://arxiv.org/abs/2512.05630)
*Charlie-Ray Mann,Mark A. Oehlgrien,Błażej Jaworowski,Giuseppe Calajó,Jamir Marino,Kyung S. Choi,Darrick E. Chang*

Main category: quant-ph

TL;DR: 腔量子电动力学与原子系综结合，利用腔介导的均匀相互作用将系统投影到总自旋单态子空间，从而将经典反铁磁体压缩成量子自旋液体，实现强关联量子物质。


<details>
  <summary>Details</summary>
Motivation: 传统腔QED研究通常关注集体自旋现象（如超辐射和自旋压缩），但本文发现腔QED可以成为诱导强关联现象的新资源。作者旨在探索如何利用腔介导的相互作用产生新的强关联量子物质形式。

Method: 将里德堡原子阵列耦合到单模腔中，利用均匀的腔介导相互作用将系统能量投影到总自旋单态子空间（S=0）。在这个高度纠缠的子空间中，物理完全由腔涨落主导，全局腔涨落可以有效地将经典反铁磁体压缩成量子自旋液体。

Result: 研究表明，腔QED可以诱导出具有非局域纠缠、分数化激发和涌现规范场的量子自旋液体。这种强关联现象可以在新一代的混合光镊-腔平台中进行实验探索。

Conclusion: 腔QED不仅是研究集体自旋现象的平台，更是诱导强关联量子物质的有力资源。通过将系统投影到总自旋单态子空间，可以利用腔涨落产生新的量子相，为在混合光镊-腔平台中探索强关联现象开辟了新途径。

Abstract: Cavity quantum electrodynamics with atomic ensembles is typically associated with collective spin phenomena, such as superradiance and spin squeezing, in which the atoms evolve collectively as a macroscopic spin ($S\sim N/2$) on the Bloch sphere. Surprisingly, we show that the tendency toward a collective spin description need not imply collective spin phenomena; rather, it can be exploited to generate new forms of strongly correlated quantum matter. The key idea is to use uniform cavity-mediated interactions to energetically project the system into the total-spin singlet sector ($S=0$) - a highly entangled subspace where the physics is governed entirely by cavity fluctuations. Focusing on Rydberg atom arrays coupled to a single-mode cavity, we show that global cavity fluctuations can effectively squeeze classical antiferromagnets into quantum spin liquids, characterized by non-local entanglement, fractionalized excitations, and emergent gauge fields. This work suggests that cavity QED can be a surprising resource for inducing strongly correlated phenomena, which could be explored in the new generation of hybrid tweezer-cavity platforms.

</details>


### [12] [Non-equilibrium quantum field theory of the free-electron laser in Keldysh formalism](https://arxiv.org/abs/2512.05266)
*Loris Di Cairano*

Main category: quant-ph

TL;DR: 基于Preparata模型，采用实时Keldysh形式体系，建立了自由电子激光的非平衡量子场论，将FEL阈值解释为激光普适类中的连续非平衡相变。


<details>
  <summary>Details</summary>
Motivation: 为自由电子激光提供一个统一的微观量子场论描述，将增益、色散和噪声纳入单一的自能框架，而不是分离的现象学成分，从而揭示FEL阈值的相变本质。

Method: 从相对论电子束与单辐射模式耦合的微观拉格朗日量出发，构建Keldysh泛函积分，进行大N重标度，积分掉电子自由度，得到FEL模式的有效作用量。

Result: 获得了平稳高斯束流下自能推迟分量和Keldysh分量的闭合解析表达式，直接编码频率牵引、能量展宽导致的增益降低以及场经历的噪声谱。低频下理论简化为具有质量、增长率、非线性和噪声强度的单复模Landau-Ginzburg-Keldysh描述。

Conclusion: 该理论将FEL阈值解释为激光普适类中的连续非平衡相变，相干场振幅作为序参量，临界涨落幅度由微观噪声核固定，为Vlasov-Maxwell FEL理论提供了最小开放量子场论类比。

Abstract: We develop a non-equilibrium quantum field theory of the free-electron laser based on the Preparata model, using the real-time Keldysh formalism. Starting from a microscopic Lagrangian for a relativistic electron beam coupled to a single radiation mode, we construct a Keldysh functional integral, perform the large-N rescaling, and integrate out the electronic degrees of freedom. This yields an effective action for the FEL mode in which dispersion, gain, and noise are all generated by a single electronic self-energy built from the current correlations of the beam. For a stationary Gaussian beam, we obtain closed analytic expressions for the retarded and Keldysh components of the self-energy, which directly encode frequency pulling, gain reduction due to energy spread, and the noise spectrum experienced by the field. At low frequency, the theory reduces to a Landau-Ginzburg-Keldysh description of a single complex mode with a mass, growth rate, nonlinearity, and noise strength fully determined by beam current, energy spread, and detuning. In this framework, the FEL threshold appears as a continuous non-equilibrium phase transition in the laser universality class: the coherent field amplitude plays the role of an order parameter, while the amplitude of critical fluctuations is fixed by the microscopic noise kernel. The result is a minimal open quantum field theory analog of Vlasov-Maxwell FEL theory, in which gain, dispersion, and noise arise from a unified self-energy framework rather than from separate phenomenological ingredients.

</details>


### [13] [Photoelectrical detection and characterization of divacancy and PL5-PL7 spins in silicon carbide](https://arxiv.org/abs/2512.05283)
*Naoya Morioka,Tetsuri Nishikawa,Hiroshi Abe,Takeshi Ohshima,Norikazu Mizuochi*

Main category: quant-ph

TL;DR: 该研究展示了室温下对PL3、PL5、PL6和PL7自旋缺陷的光电检测磁共振，发现PL7和PL5比PL6更适合电学读出，并揭示了PL7的次级共振特性，为零场分裂参数测定和量子电子器件应用奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 光电检测磁共振为半导体中自旋缺陷提供了一种可扩展的替代光学读出的方法，特别适用于近红外发射器，因为在这些波长下光电检测通常具有挑战性。研究旨在探索这些缺陷在室温下的相干光电检测磁共振性能。

Method: 使用光电检测磁共振技术在室温下对PL3（双空位）、PL5、PL6和PL7自旋缺陷进行检测，通过拉比振荡和双频光谱学分析自旋特性，测定零场分裂参数。

Result: 成功实现了室温下对PL3、PL5、PL6和PL7自旋缺陷的相干光电检测磁共振；发现PL7和PL5的光电检测磁共振信号比PL6更强，表明它们具有更高的电离效率和更适合电学读出的特性；发现了PL7的次级共振；测定了PL7的零场分裂参数，并将最近报道的PL3a缺陷确定为PL7。

Conclusion: 这些近红外缺陷的光电检测磁共振演示是量子电子器件发展的关键进展，澄清的自旋参数和电离特性为利用这些缺陷推进量子技术提供了坚实基础，无论采用何种检测方案。

Abstract: Photoelectrical detection of magnetic resonance (PDMR) offers a scalable alternative to optical readout of spin defects in semiconductors and is particularly promising for near-infrared (NIR) emitters, where photodetection is often challenging. Here, we demonstrate room-temperature coherent PDMR of PL3 (divacancy), PL5, PL6, and PL7 spins. PL7 and PL5 exhibit notably stronger PDMR than PL6 as opposed to optical detection, indicating higher ionization efficiency and suitability for electrical readout. Rabi oscillation and two-frequency spectroscopy reveal a previously undiscovered secondary resonance of PL7. We determine the zero-field splitting parameters of PL7 and assign the recently reported PL3a defect to PL7. The demonstrated PDMR of these NIR defects constitutes a key advancement toward quantum electronic devices. Also, the clarified spin parameters and ionization characteristics provide a solid foundation for advancing quantum technologies utilizing these defects regardless of the detection schemes.

</details>


### [14] [Comparison of Nb and Ta Pentoxide Loss Tangents for Superconducting Quantum Devices](https://arxiv.org/abs/2512.05407)
*D. P. Goronzy,W. W. Mah,P. G. Lim,T. Guess,S. Majumder,D. A. Garcia-Wetten,M. J. Walker,J. Ramirez,W. -R. Syong,D. Bennett,M. Vissers,R. dos Reis,T. Pham,V. P. Dravid,M. C. Hersam,M. J. Bedzyk,C. R. H. McRae*

Main category: quant-ph

TL;DR: 比较Nb2O5和Ta2O5五氧化物的介电损耗，发现Nb2O5的TLS损耗比Ta2O5高约30%，表明使用Nb布线的量子比特受更高损耗影响


<details>
  <summary>Details</summary>
Motivation: 超导transmon量子比特通常使用Nb薄膜布线，但最近研究表明Ta布线性能更好。本研究旨在通过比较Nb2O5和Ta2O5的介电损耗，进一步理解量子比特中的限制性损耗机制

Method: 通过脉冲激光沉积在相同的共面波导谐振器上沉积三种厚度的Nb和Ta五氧化物，测量谐振器诱导的单光子、毫开尔文介电损耗

Result: Nb2O5的双能级系统(TLS)损耗比Ta2O5高约30%，表明使用Nb布线的量子比特受到更高损耗影响，这主要源于原生五氧化物本身，可能还包括亚氧化物的存在（在Ta中基本不存在）

Conclusion: Nb布线量子比特的更高损耗源于Nb2O5比Ta2O5更高的TLS损耗，这解释了为什么Ta布线量子比特通常表现出更好的性能

Abstract: Superconducting transmon qubits are commonly made with thin-film Nb wiring, but recent studies have shown increased performance with Ta wiring. In this work, we compare the resonator-induced single photon, millikelvin dielectric loss for pentoxides of Nb (Nb2O5) and Ta (Ta2O5) in order to further understand limiting losses in qubits. Nb and Ta pentoxides of three thicknesses are deposited via pulsed laser deposition onto identical coplanar waveguide resonators. The two-level system (TLS) loss in Nb2O5 is determined to be about 30% higher than that of Ta2O5. This work indicates that qubits with Nb wiring are affected by higher loss arising from the native pentoxide itself, likely in addition to the presence of suboxides, which are largely absent in Ta.

</details>


### [15] [Nonlinear Classical Dynamics described by a Density Matrix in the Classical Limit](https://arxiv.org/abs/2512.05423)
*Gaspar Gonzalez,Angelo Plastino,Andrés Kowalski*

Main category: quant-ph

TL;DR: 在MaxEnt框架下研究非线性半经典混合系统的经典极限，证明经典极限由代表单一状态的纯密度矩阵表征，并能重现经典对应系统的动力学。


<details>
  <summary>Details</summary>
Motivation: 研究非线性半经典混合系统的经典极限问题，探讨在MaxEnt框架下混合动力学的一致性条件，理解量子系统如何过渡到经典行为。

Method: 采用MaxEnt框架分析非线性半经典混合系统，推导混合动力学的一致性所需的代数约束和平滑条件，通过解析方法研究经典极限特性，并重新审视和综合两个先前研究的例子进行说明。

Result: 经典极限由代表单一状态的纯密度矩阵表征，能够精确重现经典对应系统的动力学行为，证明了混合系统在经典极限下的合理性和一致性。

Conclusion: 在MaxEnt框架下，非线性半经典混合系统的经典极限具有明确的数学特征，表现为纯密度矩阵，为理解量子-经典过渡提供了理论框架和验证方法。

Abstract: We examine the classical limit of a fairly general nonlinear semiclassical hybrid system within a MaxEnt framework. The consistency of the hybrid dynamics requires algebraic constraints on quantum operators and smoothness conditions for the classical variables. Analytically, we demonstrate that the classical limit is characterized by a pure density matrix representing a single state, which reproduces the dynamics of its classical analogue. To illustrate the methodology, we revisit and synthesize two previously studied examples.

</details>


### [16] [Concentrated Monte Carlo sampling for local observables in quantum spin chains](https://arxiv.org/abs/2512.05440)
*Wenxuan Zhang,Dingzu Wang,Dario Poletti*

Main category: quant-ph

TL;DR: 提出了一种集中蒙特卡洛采样方法，通过优先采样局部可观测量周围区域来提高短程关联系统中局部可观测量估计的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统蒙特卡洛方法在估计量子多体系统可观测量时需要大量样本才能达到足够精度。针对短程关联系统，局部可观测量主要受其周围区域影响，因此需要更高效的采样策略。

Method: 提出集中蒙特卡洛采样方法：对局部可观测量周围区域考虑所有可能构型，而对剩余系统使用马尔可夫链蒙特卡洛进行唯一采样。优先获取可观测量周围区域的详细信息。

Result: 在自旋-1/2倾斜伊辛模型基态的不同相中，以及自旋-1双线性-双二次模型的热态中测试。结果表明CMCS在短程关联态中对局部可观测量具有更高精度，且所需样本数显著减少。

Conclusion: 集中蒙特卡洛采样方法能够在短程关联系统中以更少样本获得局部观测量的更高精度估计，展示了在哪些体系中可以加速期望值计算。

Abstract: Monte Carlo methods are widely used to estimate observables in many-body quantum systems. However, conventional sampling schemes often require a large number of samples to achieve sufficient accuracy. In this work we propose the concentrated Monte Carlo sampling approach, which builds on the idea that in systems with only short range correlations, to obtain accurate expectation values for local observables, one would favor detailed information in the surroundings of this observable compared to far away from it. In this approach we consider all possible configurations in the surroundings of a local observable, and unique samples from the remaining of the setup drawn using Markov chain Monte Carlo. We have tested the performance of this approach for ground states of the spin-1/2 tilted Ising model in different phases, and also for thermal states in the a spin-1 bilinear-biquadratic model. Our results demonstrate that CMCS yields higher accuracy for local observables in short-range correlated states while requiring substantially fewer samples, showcasing in which regimes one can obtain acceleration for the evaluation of expectation values.

</details>


### [17] [Shadow Tomography Against Adversaries](https://arxiv.org/abs/2512.05451)
*Maryam Aliakbarpour,Vladimir Braverman,Nai-Hui Chia,Chia-Ying Lin,Yuhan Liu,Aadil Oufkir,Yu-Ching Shen*

Main category: quant-ph

TL;DR: 该论文研究了对抗性噪声下的单拷贝影子层析成像，提出了一个鲁棒算法，在γ比例测量结果被恶意篡改的情况下，能以Õ(γ·max_i||O_i||_HS)误差学习M个可观测量的期望值，样本复杂度与无噪声情况下的经典影子算法相同。


<details>
  <summary>Details</summary>
Motivation: 经典影子层析成像算法在对抗性噪声下表现不佳，当γ比例的测量结果被恶意篡改时，现有算法会产生较大误差。需要设计能够在对抗性环境中鲁棒工作的影子层析成像算法。

Method: 设计了一个概念简单、易于实现的鲁棒算法，仅需n=1/γ²·log(M/δ)个副本就能以至少1-δ的概率达到目标误差。算法通过适当的测量策略和处理对抗性噪声的机制来保证鲁棒性。

Result: 算法实现了ε=Õ(γ·max_i||O_i||_HS)的误差，当M≥d时几乎匹配最坏情况误差下界。对于秩r的状态，实现了ε=Õ(γ√r)的渐近误差和Õ(dr²/ε²)=Õ(dr/γ²)的副本复杂度，填补了先前工作中的空白。

Conclusion: 该工作首次为对抗性鲁棒影子层析成像提供了接近最优的算法，样本复杂度与无噪声情况相同，且在保真度估计的经典模拟中表现出比[HKP20]更强的鲁棒性，为量子态层析成像提供了重要进展。

Abstract: We study single-copy shadow tomography in the adversarial robust setting, where the goal is to learn the expectation values of $M$ observables $O_1, \ldots, O_M$ with $\varepsilon$ accuracy, but $γ$-fraction of the outcomes can be arbitrarily corrupted by an adversary. We show that all non-adaptive shadow tomography algorithms must incur an error of $\varepsilon=\tildeΩ(γ\min\{\sqrt{M}, \sqrt{d}\})$ for some choice of observables, even with unlimited copies. Unfortunately, the classical shadows algorithm by [HKP20] and naive algorithms that directly measure each observable suffer even more. We design an algorithm that achieves an error of $\varepsilon=\tilde{O}(γ\max_{i\in[M]}\|O_i\|_{HS})$, which nearly matches our worst-case error lower bound for $M\ge d$ and guarantees better accuracy when the observables have stronger structure. Remarkably, the algorithm only needs $n=\frac{1}{γ^2}\log(M/δ)$ copies to achieve that error with probability at least $1-δ$, matching the sample complexity of the classical shadows algorithm that achieves the same error without corrupted measurement outcomes. Our algorithm is conceptually simple and easy to implement. Classical simulation for fidelity estimation shows that our algorithm enjoys much stronger robustness than [HKP20] under adversarial noise. Finally, based on a reduction from full-state tomography to shadow tomography, we prove that for rank $r$ states, both the near-optimal asymptotic error of $\varepsilon=\tilde{O}(γ\sqrt{r})$ and copy complexity $\tilde{O}(dr^2/\varepsilon^2)=\tilde{O}(dr/γ^2)$ can be achieved for adversarially robust state tomography, closing the large gap in [ABCL25] where optimal error can only be achieved using pseudo-polynomial number of copies in $d$.

</details>


### [18] [Observability Architecture for Quantum-Centric Supercomputing Workflows](https://arxiv.org/abs/2512.05484)
*Naoki Kanazawa,Yuto Morohoshi,Hitomi Takahashi,Yukio Kawashima,Hiroshi Horii,Kengo Nakajima*

Main category: quant-ph

TL;DR: 提出面向量子中心超级计算工作流的可观测性架构，解决量子算法执行难解释、性能难监控的问题，通过解耦遥测收集与工作流执行，实现跨系统层和算法层的持久监控。


<details>
  <summary>Details</summary>
Motivation: 量子中心超级计算工作流中的混合经典-量子算法具有概率性，在远程量子硬件上执行，难以解释和监控运行时性能。量子电路执行成本高，大规模高性能计算基础设施限制了试验次数，需要全面评估执行结果以支持迭代开发。

Method: 提出专门为QCSC工作流设计的可观测性架构，将遥测收集与工作流执行解耦，实现跨系统层和算法层的持久监控，保留详细的执行数据用于可重现和回顾性分析，避免冗余运行。

Result: 将该系统应用于涉及基于样本的量子对角化的代表性工作流，揭示了求解器在多次迭代中的行为。该方法增强了QCSC环境的透明度和可重现性。

Conclusion: 该可观测性架构支持基础设施感知的算法设计和系统化实验，为量子中心超级计算工作流提供了有效的监控和分析解决方案。

Abstract: Quantum-centric supercomputing (QCSC) workflows often involve hybrid classical-quantum algorithms that are inherently probabilistic and executed on remote quantum hardware, making them difficult to interpret and limiting the ability to monitor runtime performance and behavior. The high cost of quantum circuit execution and large-scale high-performance computing (HPC) infrastructure further restricts the number of feasible trials, making comprehensive evaluation of execution results essential for iterative development. We propose an observability architecture tailored for QCSC workflows that decouples telemetry collection from workload execution, enabling persistent monitoring across system and algorithmic layers and retaining detailed execution data for reproducible and retrospective analysis, eliminating redundant runs. Applied to a representative workflow involving sample-based quantum diagonalization, our system reveals solver behavior across multiple iterations. This approach enhances transparency and reproducibility in QCSC environments, supporting infrastructure-aware algorithm design and systematic experimentation.

</details>


### [19] [Frequency-matching quantum key distribution](https://arxiv.org/abs/2512.05496)
*Hao-Tao Zhu,Yizhi Huang,Abdullah Rasmita,Chao Ding,Xiangbin Cai,Haoran Zhang,Xiongfeng Ma,Weibo Gao*

Main category: quant-ph

TL;DR: 提出使用经典光电二极管补偿独立激光器频率差异的方法，解决量子密钥分发中的相位不稳定性问题，并在模式配对QKD系统中实现接近理论极限的性能


<details>
  <summary>Details</summary>
Motivation: 量子密钥分发(QKD)虽然能提供信息论安全通信，但在双场QKD和测量设备无关QKD等方案中，相位不稳定性仍然是一个挑战。最主要的问题来源是独立激光器之间的频率偏移

Method: 使用经典光电二极管来补偿激光器频率差异。将该技术应用于模式配对QKD系统，通过频率匹配实现相位稳定

Result: 在296.8公里光纤距离上实现了接近理论极限的错误率，并超越了线性密钥率界限

Conclusion: 该方法为独立激光器之间的频率匹配提供了实用解决方案，并且可以扩展到其他需要精确相位稳定的领域

Abstract: Quantum key distribution (QKD) enables information-theoretically secure communication against eavesdropping. However, phase instability remains a challenge across many QKD applications, particularly in schemes such as twin-field QKD and measurement-device-independent QKD. The most dominant source of phase fluctuation arises from the frequency offset between independent lasers. Here we propose a method to address this issue by employing a classical photodiode to compensate for the laser frequency difference. As an application of this method, we implement this technique in a mode-pairing QKD system, achieving an error rate approaching the theoretical limit and surpassing the linear key-rate bound over a fiber distance of 296.8 km. This approach provides a practical solution for frequency matching between independent lasers and can be extended to other fields requiring precise phase stabilization.

</details>


### [20] [Quantum advantages in multiparty communication](https://arxiv.org/abs/2512.05538)
*Ankush Pandit*

Main category: quant-ph

TL;DR: 研究两个发送者和一个接收者的多方通信场景，在维度和可区分性约束下，量子通信能系统性地超越经典极限


<details>
  <summary>Details</summary>
Motivation: 研究多方通信场景中，在维度和可区分性约束下，量子通信相对于经典通信的优势

Method: 基于Phys.Rev.A83, 062112和arXiv:2506.07699的研究框架，分析两个发送者和一个接收者的通信场景；使用半定层次工具专门针对双发送者单接收者设置进行实现

Result: 明确描述了在维度和可区分性约束下可实现的经典相关性；证明了量子通信即使在没有预共享纠缠且接收者没有输入选择的情况下，也能系统性地超越经典极限

Conclusion: 在维度和可区分性约束下，多方通信存在明显的量子优势

Abstract: We investigate two senders and one receiver multiparty communication scenario. Following Phys.Rev.A83, 062112 and arXiv : 2506.07699, we study multiparty communication bounded by dimension and distinguishability. We provide an explicit characterization of the classical correlations achievable under these constraints. We then demonstrate that quantum communication systematically exceeds these classical limits, even in the absence of preshared entanglement and without any input choice for the receiver. Furthermore, we implement semidefinite hierarchy tools tailored to the two-sender, one-receiver setting for both types of constraints considered. Our results reveal a clear quantum advantage in multiparty communication under those restrictions.

</details>


### [21] [Measurement-based Initial Point Smoothing and Control Approach to Quantum Memory Systems](https://arxiv.org/abs/2512.05586)
*Igor G. Vladimirov,Ian R. Petersen,Guodong Shi*

Main category: quant-ph

TL;DR: 该论文研究了一种量子记忆系统，通过测量反馈的经典控制器来补偿环境噪声引起的量子信息存储偏差，使用线性二次高斯控制和固定点平滑技术最小化量子变量与初始值的偏差。


<details>
  <summary>Details</summary>
Motivation: 量子记忆系统在环境噪声下存储量子信息时会出现偏差，需要设计控制方案来补偿这种偏差，保持量子信息的完整性。

Method: 将系统建模为开放量子谐振子，其海森堡演化由线性Hudson-Parthasarathy量子随机微分方程描述。控制器设计为经典线性时变系统，采用线性二次高斯控制和固定点平滑技术，在二阶矩层面进行优化，控制器具有分离结构并包含对初始量子变量的连续更新估计。

Result: 设计了包含量子经典动态变量的闭环系统，通过初始点平滑器形成执行器信号，最小化量子记忆系统变量在给定时间范围内与初始值的均方偏差以及控制信号的积分二次惩罚。

Conclusion: 该研究提出了一种有效的量子记忆控制方案，通过测量反馈的经典控制器补偿环境噪声影响，为量子信息存储提供了实用的控制方法。

Abstract: This paper is concerned with a quantum memory system for storing quantum information in the form of its initial dynamic variables in the presence of environmental noise. In order to compensate for the deviation from the initial conditions, the classical parameters of the system Hamiltonian are affected by the actuator output of a measurement-based classical controller. The latter uses an observation process produced by a measuring apparatus from the quantum output field of the memory system. The underlying system is modelled as an open quantum harmonic oscillator whose Heisenberg evolution is governed by linear Hudson-Parthasarathy quantum stochastic differential equations. The controller is organised as a classical linear time-varying system, so that the resulting closed-loop system has quantum and classical dynamic variables. We apply linear-quadratic-Gaussian control and fixed-point smoothing at the level of the first two moments and consider controllers with a separation structure which involve a continuously updated estimate for the initial quantum variables. The initial-point smoother is used for actuator signal formation so as to minimise the sum of a mean-square deviation of the quantum memory system variables at a given time horizon from their initial values and an integral quadratic penalty on the control signal.

</details>


### [22] [Heisenberg-Weyl bosonic phase spaces: emergence, constraints and quantum informational resources](https://arxiv.org/abs/2512.05603)
*Eloi Descamps,Astghik Saharyan,Arne Keller,Pérola Milman*

Main category: quant-ph

TL;DR: 该论文提出了一个连接物理相空间与计算相空间的一般框架，揭示了参考系选择对量子计算可模拟性的关键作用，并分析了平面相空间极限下的经典可模拟行为。


<details>
  <summary>Details</summary>
Motivation: 相空间负性通常被认为是量子计算优势的必要条件，但在玻色系统中，即使编码态具有大的负性，相应的架构仍可能被经典模拟。现有框架未能一致地连接物理相空间与计算相空间的非负性关系。

Method: 引入一个连接玻色系统物理相空间结构与任意维度编码计算表示的一般框架，强调参考系（真空选择）在定义计算基和连接相空间可模拟性方面的关键作用。

Result: 建立了物理相空间与计算相空间之间的系统对应关系，揭示了参考系选择如何影响量子计算的可模拟性，并分析了平面相空间极限下量子特征逐渐消失、产生经典可模拟行为的机制。

Conclusion: 该框架为理解玻色系统中量子计算优势与相空间负性的关系提供了统一的理论基础，特别强调了参考系选择在连接物理系统与计算表示中的核心作用，有助于更准确地评估量子架构的经典可模拟性。

Abstract: Phase space quasi-probability functions provide powerful representations of quantum states and operators, as well as criteria for assessing quantum computational resources. In discrete, odd-dimensional systems (qudits), protocols involving only non-negative phase space distributions can be efficiently classically simulated. For bosonic systems, defined in continuous variables, phase space negativities are likewise necessary to prevent efficient classical simulation of the underlying physical processes. However, when quantum information is encoded in bosonic systems, this connection becomes subtler: as negativity is only a necessary property for potential quantum advantage, encoding (i.e., physical) states may exhibit large negativities while still corresponding to architectures that remain classically simulable. Several frameworks have attempted to relate non-negativity of states and gates in the computational phase space to non-negativity of processes in the physical bosonic phase space, but a consistent correspondence remains elusive. Here, we introduce a general framework that connects the physical phase space structure of bosonic systems to their encoded computational representations across arbitrary dimensions and encodings. This framework highlights the key role of the reference frame-equivalently, the choice of vacuum-in defining the computational basis and linking its phase space simulability properties to those of the physical system. Finally, we provide computational and physical interpretations of the planar (quadrature-like) phase space limit, where genuinely quantum features may gradually vanish, yielding classically simulable behavior.

</details>


### [23] [Randomness quantification in spontaneous emission](https://arxiv.org/abs/2512.05713)
*Chenxu Li,Shengfan Liu,Xiongfeng Ma*

Main category: quant-ph

TL;DR: 该研究为基于自发辐射的量子随机数生成器建立了全面的量子信息理论框架，分析了两种窃听策略下的安全性，发现不同检测方式的QRNG具有不同的安全特性。


<details>
  <summary>Details</summary>
Motivation: 量子相干性是产生内在随机性的基本资源，但基于自发辐射的量子随机数生成器的随机性量化主要停留在现象学层面。现有分析缺乏严格的对抗模型，也没有清晰描述量子相干性在这些系统中的作用。

Method: 开发了一个全面的量子信息理论框架来分析自发辐射过程中的随机性生成。表征了两种不同的窃听策略：一种是攻击者直接访问原子系综，另一种是攻击者只访问其纯化系统。分析了基于单光子检测和时间模式测量与基于空间模式检测和相位波动的不同QRNG方案。

Result: 分析表明：基于单光子检测和时间模式测量的QRNG对第一种攻击者场景是脆弱的，但对第二种攻击者场景即使原子信息最大泄露也能保证内在随机性的下界。而基于空间模式检测和相位波动的QRNG对两种攻击者都具有安全性，能提供鲁棒的随机性生成。研究还提供了这些自发辐射QRNG方案内在随机性的定量计算。

Conclusion: 该研究为基于自发辐射的量子随机数生成器建立了严格的量子信息理论框架，明确了不同检测方案的安全特性差异，为设计更安全的QRNG提供了理论指导。

Abstract: Quantum coherence serves as a fundamental resource for generating intrinsic randomness, yet the quantification of randomness in quantum random number generators (QRNGs) based on spontaneous emission has remained largely phenomenological. Existing randomness analysis lacks rigorous adversarial models and a clear characterization of the role of quantum coherence in these systems. In this work, we develop a comprehensive quantum information-theoretic framework for randomness generation in spontaneous emission processes. We characterize two distinct eavesdropping strategies: one where the adversary directly accesses the atom ensemble, and the other where the adversary accesses only its purification. Our analysis reveals that when randomness is generated through single-photon detection and temporal mode measurements, the QRNG is vulnerable to the first adversary scenario, though it still guarantees a lower bound on intrinsic randomness against the second adversary scenario even under maximal information leakage from the atoms. In contrast, QRNGs based on spatial mode detection and phase fluctuations demonstrate security against both types of adversaries, providing robust randomness generation. Furthermore, we provide a quantitative calculation of intrinsic randomness for these spontaneous-emission-based QRNG schemes.

</details>


### [24] [Continuous operations on non-Markovian processes](https://arxiv.org/abs/2512.05884)
*Fabio Costa,Jing Yang*

Main category: quant-ph

TL;DR: 该论文提出了一个基于过程和操作泛函的连续时间多时间量子过程框架，用于描述非马尔可夫环境下的连续测量，无需指定微观测量模型。


<details>
  <summary>Details</summary>
Motivation: 连续测量在量子控制和传感中至关重要，但缺乏模型无关的操作描述，现有离散时间框架无法表示有限持续时间的测量，限制了在非马尔可夫过程中的应用。

Method: 引入基于过程和操作泛函的连续时间扩展，推广了Feynman-Vernon影响泛函，提出了连续Born规则，清晰分离过程和操作，提供了非马尔可夫动力学在连续监测下的一致表示。

Result: 该框架为连续时间中的马尔可夫性提供了自然定义，并通过分析广义Caldeira-Leggett模型中的连续测量，证明了其在现实非马尔可夫场景中的适用性。

Conclusion: 提出的连续时间多时间量子过程框架为连续测量提供了模型无关的描述方法，能够处理任意非马尔可夫过程，填补了现有离散时间框架的局限性。

Abstract: Continuous measurements are central to quantum control and sensing, yet lack a model-independent operational description that can be applied to arbitrary non-Markovian processes without specifying a microscopic measurement model. Existing multi-time frameworks, such as process matrices, allow for an arbitrary sequence of operations to be applied on a general process, but are restricted to interventions at discrete times and cannot represent measurements of finite duration. We introduce a continuous-time extension of multi-time quantum processes based on process and operation functionals, which generalize the Feynman-Vernon influence functional and yield a continuous Born rule that cleanly separates processes from operations. This framework provides a consistent representation of non-Markovian dynamics under continuous monitoring and leads to a natural definition of Markovianity in continuous time. We illustrate the formalism by analyzing continuous measurements in a generalized Caldeira-Leggett model, demonstrating its applicability to realistic non-Markovian scenarios.

</details>


### [25] [Spectroscopy and Coherent Control of Two-Level System Defect Ensembles Using a Broadband 3D Waveguide](https://arxiv.org/abs/2512.05934)
*Qianxu Wang,Juan S. Salcedo-Gallo,Salil Bedkihal,Tian Xia,Maciej W. Olszewski,Valla Fatemi,Mattias Fitzpatrick*

Main category: quant-ph

TL;DR: 开发宽带低温瞬态介电光谱技术，研究非晶电介质中二能级系统缺陷的集体动力学，揭示量子干涉效应和记忆依赖动力学，为量子器件退相干源诊断提供新方法。


<details>
  <summary>Details</summary>
Motivation: 固态材料中的缺陷对量子技术的相干性、稳定性和性能有重要影响。虽然窄带技术可以高精度探测特定共振，但宽带光谱方法能捕获缺陷性质和动力学的完整谱。非晶电介质中的二能级系统缺陷是特别重要的例子，因为它们是超导量子器件中退相干和能量损失的主要来源。然而，访问和表征它们的集体动力学比探测单个缺陷更具挑战性。

Method: 基于先前开发的宽带低温瞬态介电光谱技术，研究3-5 GHz宽频范围内二能级系统缺陷系综的相干控制和时间分辨动力学，无需完整器件制造。使用驱动的最小自旋模型与偶极-偶极相互作用来定性捕捉观察到的行为。

Result: 揭示了量子干涉效应、记忆依赖动力学和缺陷浴中的修饰态演化。光谱响应显示对应于裸本征模频率的独特V形结构。提取了硅样品在4.1-4.6 GHz范围内的缺陷谱密度为84 GHz^-1。系统研究了多个温度和脉冲间延迟下的幅度和相位控制干涉条纹，提供了相干动力学和控制的直接证据。

Conclusion: BCTDS技术建立了宽带缺陷光谱的多功能平台，为诊断和减轻退相干源、工程多体动力学以及探索无序量子系统中的非平衡现象提供了新能力。

Abstract: Defects in solid-state materials play a central role in determining coherence, stability, and performance in quantum technologies. Although narrowband techniques can probe specific resonances with high precision, a broadband spectroscopic approach captures the full spectrum of defect properties and dynamics. Two-level system (TLS) defects in amorphous dielectrics are a particularly important example because they are major sources of decoherence and energy loss in superconducting quantum devices. However, accessing and characterizing their collective dynamics remains far more challenging than probing individual TLS defects. Building on our previously developed Broadband Cryogenic Transient Dielectric Spectroscopy (BCTDS) technique, we study the coherent control and time-resolved dynamics of TLS defect ensembles over a wide frequency range of 3-5 GHz without requiring full device fabrication, revealing quantum interference effects, memory-dependent dynamics, and dressed-state evolution within the TLS defect bath. The spectral response reveals distinct V-shaped structures corresponding to the bare eigenmode frequencies. Using these features, we extract a TLS defect spectral density of 84 GHz^-1 for a silicon sample, across a 4.1-4.6 GHz span. Furthermore, we systematically investigate amplitude- and phase-controlled interference fringes for multiple temperatures and inter-pulse delays, providing direct evidence of coherent dynamics and control. A driven minimal spin model with dipole-dipole interactions that qualitatively capture the observed behavior is presented. Our results establish BCTDS as a versatile platform for broadband defect spectroscopy, offering new capabilities for diagnosing and mitigating sources of decoherence, engineering many-body dynamics, and exploring non-equilibrium phenomena in disordered quantum systems.

</details>


### [26] [Entanglement-Enhanced Quantum Nano-Vibrometry](https://arxiv.org/abs/2512.05961)
*Colin P. Lualdi,Joshua Rapp,Spencer J. Johnson,Michael Vayninger,Paul G. Kwiat*

Main category: quant-ph

TL;DR: 该研究利用极端能量纠缠的双光子干涉技术，实现了纳米尺度动态系统的高频振动测量，在损耗和背景噪声下表现出显著的量子优势。


<details>
  <summary>Details</summary>
Motivation: 纳米尺度动态系统的研究需要高分辨率测量，但传统方法难以在存在损耗和背景噪声的情况下实现快速测量。量子双光子干涉技术虽然具有损耗和背景噪声的鲁棒性，但实现快速高分辨率测量仍面临挑战。

Method: 引入极端能量纠缠的光子对进行干涉，采用通量探测分析技术，能够恢复高达21 kHz的振动信号频率。

Result: 实现了纳米尺度的测量精度和准确度验证，在存在损耗和背景噪声的测量条件下观察到显著的量子优势。

Conclusion: 极端能量纠缠的双光子干涉技术为纳米尺度动态系统的快速高分辨率测量提供了有效解决方案，特别是在损耗和背景噪声环境下展现出量子技术的优越性。

Abstract: The study of dynamic systems at the nanometer scale can benefit from the loss and background resilience offered by quantum two-photon interference. However, fast measurements with the required resolution are difficult to realize. As a solution, we introduce extreme energy entanglement between the photons undergoing interference. Using a flux probing analysis technique, we recover vibrational signals with frequencies as high as 21 kHz. Along with validating nanometer-scale precision and accuracy, we observe a significant quantum advantage when measuring in the presence of loss and background.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [27] [Coefficient of Variation Masking: A Volatility-Aware Strategy for EHR Foundation Models](https://arxiv.org/abs/2512.05216)
*Rajna Fani,Rafi Al Attrach,David Restrepo,Yugang Jia,Leo Anthony Celi,Peter Schüffler*

Main category: cs.LG

TL;DR: 提出CV-Masking方法，根据实验室测试指标的波动性调整掩码概率，相比传统随机掩码方法在EHR表示学习中表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有MAE方法在EHR数据上通常采用均匀随机掩码，假设所有特征同等可预测。但实际上实验室测试指标存在显著异质性波动性：有些生物标志物（如钠）保持稳定，而其他（如乳酸）波动较大且更难建模。临床上，波动性强的生物标志物通常提示急性病理生理变化，需要更复杂的建模来捕捉其复杂时间模式。

Method: 提出波动性感知预训练策略CV-Masking（变异系数掩码），根据每个特征的内在变异性自适应调整掩码概率。结合与临床工作流程对齐的仅值掩码目标，该方法相比随机和基于方差的策略有系统性改进。

Result: 在大规模实验室测试面板上的实验表明，CV-Masking增强了重建能力，改善了下游预测性能，加速了收敛速度，产生了更稳健且具有临床意义的EHR表示。

Conclusion: CV-Masking方法通过考虑实验室测试指标的波动性异质性，为EHR表示学习提供了更有效的预训练策略，能够捕捉临床相关的复杂时间模式。

Abstract: Masked autoencoders (MAEs) are increasingly applied to electronic health records (EHR) for learning general-purpose representations that support diverse clinical tasks. However, existing approaches typically rely on uniform random masking, implicitly assuming all features are equally predictable. In reality, laboratory tests exhibit substantial heterogeneity in volatility: some biomarkers (e.g., sodium) remain stable, while others (e.g., lactate) fluctuate considerably and are more difficult to model. Clinically, volatile biomarkers often signal acute pathophysiology and require more sophisticated modeling to capture their complex temporal patterns. We propose a volatility-aware pretraining strategy, Coefficient of Variation Masking (CV-Masking), that adaptively adjusts masking probabilities according to the intrinsic variability of each feature. Combined with a value-only masking objective aligned with clinical workflows, CV-Masking yields systematic improvements over random and variance-based strategies. Experiments on a large panel of laboratory tests show that CV-Masking enhances reconstruction, improves downstream predictive performance, and accelerates convergence, producing more robust and clinically meaningful EHR representations.

</details>


### [28] [Rethinking Tokenization for Clinical Time Series: When Less is More](https://arxiv.org/abs/2512.05217)
*Rafi Al Attrach,Rajna Fani,David Restrepo,Yugang Jia,Peter Schüffler*

Main category: cs.LG

TL;DR: 系统评估临床时间序列建模中的分词策略，发现时间编码无显著效益，值特征重要性因任务而异，冻结预训练编码器表现更优且参数更少。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录的分词策略影响模型处理效果，但缺乏公平比较。研究旨在系统评估不同分词方法在临床时间序列建模中的有效性，揭示任务依赖性的发现。

Method: 使用基于Transformer的架构，在MIMIC-IV数据集上对四种临床预测任务进行受控消融实验，比较不同分词策略（包括时间编码、值特征等）的效果。

Result: 1) 显式时间编码对评估的下游任务无一致统计显著效益；2) 值特征重要性任务依赖（影响死亡率预测但不影响再入院预测）；3) 冻结预训练代码编码器显著优于可训练版本且参数更少；4) 更大的临床编码器在所有任务中表现一致更好。

Conclusion: 更简单、参数高效的方法在许多情况下也能达到强性能，但最优分词策略仍取决于具体任务。受控评估为公平比较分词策略提供了框架。

Abstract: Tokenization strategies shape how models process electronic health records, yet fair comparisons of their effectiveness remain limited. We present a systematic evaluation of tokenization approaches for clinical time series modeling using transformer-based architectures, revealing task-dependent and sometimes counterintuitive findings about temporal and value feature importance. Through controlled ablations across four clinical prediction tasks on MIMIC-IV, we demonstrate that explicit time encodings provide no consistent statistically significant benefit for the evaluated downstream tasks. Value features show task-dependent importance, affecting mortality prediction but not readmission, suggesting code sequences alone can carry sufficient predictive signal. We further show that frozen pretrained code encoders dramatically outperform their trainable counterparts while requiring dramatically fewer parameters. Larger clinical encoders provide consistent improvements across tasks, benefiting from frozen embeddings that eliminate computational overhead. Our controlled evaluation enables fairer tokenization comparisons and demonstrates that simpler, parameter-efficient approaches can, in many cases, achieve strong performance, though the optimal tokenization strategy remains task-dependent.

</details>


### [29] [Variance Matters: Improving Domain Adaptation via Stratified Sampling](https://arxiv.org/abs/2512.05226)
*Andrea Napoli,Paul White*

Main category: cs.LG

TL;DR: 提出VaRDASS方法，通过分层采样减少UDA中域差异估计的方差，提高模型在目标域的性能


<details>
  <summary>Details</summary>
Motivation: 无监督域适应中的域差异估计在随机设置下存在高方差问题，这会阻碍方法理论优势的实现，需要专门的方差减少技术

Method: 提出VaRDASS方法，针对相关性对齐和最大均值差异两种差异度量，推导分层采样目标，并设计k-means风格的优化算法

Result: 在三个域偏移数据集上实验，显示提高了差异估计准确性和目标域性能，证明了MMD目标在特定假设下的理论最优性

Conclusion: VaRDASS是首个专门针对UDA的随机方差减少技术，能有效减少域差异估计方差，提升模型在目标域的适应性

Abstract: Domain shift remains a key challenge in deploying machine learning models to the real world. Unsupervised domain adaptation (UDA) aims to address this by minimising domain discrepancy during training, but the discrepancy estimates suffer from high variance in stochastic settings, which can stifle the theoretical benefits of the method. This paper proposes Variance-Reduced Domain Adaptation via Stratified Sampling (VaRDASS), the first specialised stochastic variance reduction technique for UDA. We consider two specific discrepancy measures -- correlation alignment and the maximum mean discrepancy (MMD) -- and derive ad hoc stratification objectives for these terms. We then present expected and worst-case error bounds, and prove that our proposed objective for the MMD is theoretically optimal (i.e., minimises the variance) under certain assumptions. Finally, a practical k-means style optimisation algorithm is introduced and analysed. Experiments on three domain shift datasets demonstrate improved discrepancy estimation accuracy and target domain performance.

</details>


### [30] [When unlearning is free: leveraging low influence points to reduce computational costs](https://arxiv.org/abs/2512.05254)
*Anat Kleiman,Robert Fisher,Ben Deaner,Udi Wieder*

Main category: cs.LG

TL;DR: 论文提出了一种高效的机器学习遗忘框架，通过识别对模型输出影响可忽略的训练数据子集，在遗忘前减少数据集规模，从而显著降低计算成本（最高约50%）


<details>
  <summary>Details</summary>
Motivation: 随着机器学习中数据隐私问题的日益突出，从训练模型中"遗忘"或移除特定数据点的能力变得越来越重要。现有遗忘方法通常平等对待遗忘集中的所有数据点，但并非所有数据点对模型学习都有显著影响。

Method: 通过跨语言和视觉任务的比较分析，使用影响函数识别对模型输出影响可忽略的训练数据子集。基于这一洞察，提出了一种高效的遗忘框架，在遗忘前减少数据集规模。

Result: 该方法在真实世界经验示例中实现了显著的计算节省（最高约50%），通过减少需要处理的数据量来优化遗忘过程。

Conclusion: 并非所有训练数据点都需要被遗忘，识别和筛选对模型影响可忽略的数据子集可以显著提高遗忘效率，为隐私保护机器学习提供了更实用的解决方案。

Abstract: As concerns around data privacy in machine learning grow, the ability to unlearn, or remove, specific data points from trained models becomes increasingly important. While state of the art unlearning methods have emerged in response, they typically treat all points in the forget set equally. In this work, we challenge this approach by asking whether points that have a negligible impact on the model's learning need to be removed. Through a comparative analysis of influence functions across language and vision tasks, we identify subsets of training data with negligible impact on model outputs. Leveraging this insight, we propose an efficient unlearning framework that reduces the size of datasets before unlearning leading to significant computational savings (up to approximately 50 percent) on real world empirical examples.

</details>


### [31] [Bridging Interpretability and Optimization: Provably Attribution-Weighted Actor-Critic in Reproducing Kernel Hilbert Spaces](https://arxiv.org/abs/2512.05291)
*Na Li,Hangguan Shan,Wei Ni,Wenjie Zhang,Xinyu Li*

Main category: cs.LG

TL;DR: 提出RSA2C算法，将核希尔伯特空间与SHAP归因结合到Actor-Critic框架中，通过状态特征归因指导训练，提升可解释性、效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有Actor-Critic方法可解释性有限，大多数可解释RL方法未能利用状态归因辅助训练，忽视了不同状态维度对奖励的异质性影响。

Method: 提出RSA2C算法：1) Actor在向量值再生核希尔伯特空间中使用马氏距离加权算子值核；2) Value Critic和Advantage Critic在标量RKHS中；3) 通过RKHS-SHAP计算状态归因，转换为马氏门控权重来调节Actor梯度和Advantage Critic目标；4) 使用稀疏化字典。

Result: 理论上推导了状态扰动下的全局非渐近收敛界，显示通过扰动误差项实现稳定性，通过收敛误差项实现效率。在三个标准连续控制环境中的实验结果表明算法实现了效率、稳定性和可解释性。

Conclusion: RSA2C算法成功将状态归因整合到Actor-Critic框架中，通过核方法和SHAP归因实现了可解释的强化学习，在保持性能的同时提供了训练过程的透明度。

Abstract: Actor-critic (AC) methods are a cornerstone of reinforcement learning (RL) but offer limited interpretability. Current explainable RL methods seldom use state attributions to assist training. Rather, they treat all state features equally, thereby neglecting the heterogeneous impacts of individual state dimensions on the reward. We propose RKHS--SHAP-based Advanced Actor--Critic (RSA2C), an attribution-aware, kernelized, two-timescale AC algorithm, including Actor, Value Critic, and Advantage Critic. The Actor is instantiated in a vector-valued reproducing kernel Hilbert space (RKHS) with a Mahalanobis-weighted operator-valued kernel, while the Value Critic and Advantage Critic reside in scalar RKHSs. These RKHS-enhanced components use sparsified dictionaries: the Value Critic maintains its own dictionary, while the Actor and Advantage Critic share one. State attributions, computed from the Value Critic via RKHS--SHAP (kernel mean embedding for on-manifold expectations and conditional mean embedding for off-manifold expectations), are converted into Mahalanobis-gated weights that modulate Actor gradients and Advantage Critic targets. Theoretically, we derive a global, non-asymptotic convergence bound under state perturbations, showing stability through the perturbation-error term and efficiency through the convergence-error term. Empirical results on three standard continuous-control environments show that our algorithm achieves efficiency, stability, and interpretability.

</details>


### [32] [Robustness Test for AI Forecasting of Hurricane Florence Using FourCastNetv2 and Random Perturbations of the Initial Condition](https://arxiv.org/abs/2512.05323)
*Adam Lizerbram,Shane Stevenson,Iman Khadir,Matthew Tu,Samuel S. P. Shen*

Main category: cs.LG

TL;DR: 测试NVIDIA FourCastNetv2天气预测模型对输入噪声的鲁棒性，通过注入高斯噪声和完全随机初始条件来评估模型对飓风预测的敏感性。


<details>
  <summary>Details</summary>
Motivation: 评估AI天气预测模型在输入噪声和不确定性下的鲁棒性对于保证极端天气事件（如飓风）预测输出的可靠性至关重要。

Method: 进行两个实验：1）在飓风Florence的ERA5初始条件中注入不同水平的高斯噪声，观察对预测轨迹和风暴强度的影响；2）使用完全随机初始条件启动模型，观察模型对无意义输入的反应。

Result: FCNv2在低到中等噪声水平下能准确保持飓风特征；即使在高噪声下，也能维持风暴轨迹和结构，但位置精度下降。模型在所有噪声水平下都持续低估风暴强度和持续性。完全随机初始条件下，模型在几个时间步后能生成平滑连贯的预测。

Conclusion: FCNv2对输入噪声表现出良好的鲁棒性，倾向于生成稳定平滑的输出。该方法简单且可移植到其他数据驱动的AI天气预测模型。

Abstract: Understanding the robustness of a weather forecasting model with respect to input noise or different uncertainties is important in assessing its output reliability, particularly for extreme weather events like hurricanes. In this paper, we test sensitivity and robustness of an artificial intelligence (AI) weather forecasting model: NVIDIAs FourCastNetv2 (FCNv2). We conduct two experiments designed to assess model output under different levels of injected noise in the models initial condition. First, we perturb the initial condition of Hurricane Florence from the European Centre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset (September 13-16, 2018) with varying amounts of Gaussian noise and examine the impact on predicted trajectories and forecasted storm intensity. Second, we start FCNv2 with fully random initial conditions and observe how the model responds to nonsensical inputs. Our results indicate that FCNv2 accurately preserves hurricane features under low to moderate noise injection. Even under high levels of noise, the model maintains the general storm trajectory and structure, although positional accuracy begins to degrade. FCNv2 consistently underestimates storm intensity and persistence across all levels of injected noise. With full random initial conditions, the model generates smooth and cohesive forecasts after a few timesteps, implying the models tendency towards stable, smoothed outputs. Our approach is simple and portable to other data-driven AI weather forecasting models.

</details>


### [33] [Non-Convex Federated Optimization under Cost-Aware Client Selection](https://arxiv.org/abs/2512.05327)
*Xiaowen Jiang,Anton Rodomanov,Sebastian U. Stich*

Main category: cs.LG

TL;DR: 论文提出了一种新的联邦优化模型，能够量化通信和本地计算成本，并区分不同的客户端选择策略。在此基础上，作者开发了一种基于SAGA梯度估计器和递归梯度技术的新算法，在非凸优化中实现了目前已知最佳的通信和本地计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有联邦优化算法使用不同的客户端选择策略（随机采样、全客户端通信或混合方案），但这些策略在实践中产生不同的通信成本。现有比较指标通常不区分这些策略，导致无法准确评估算法的实际通信效率。

Method: 1. 引入一个简单自然的联邦优化模型，量化通信和本地计算复杂度，支持多种客户端选择策略并为每种策略分配明确成本。
2. 提出基于不精确复合梯度方法的新算法，采用精心构建的梯度估计器和辅助子问题求解程序。
3. 基于SAGA方差缩减梯度估计器，推导新的方差界限，证明SAGA能利用函数相似性。
4. 引入递归梯度技术作为改进条件无偏梯度估计器误差界限的通用方法，应用于SAGA得到RG-SAGA估计器。

Result: 新算法在非凸联邦优化中实现了目前已知最佳的通信和本地计算复杂度。RG-SAGA估计器相比原始SAGA具有改进的误差界限，递归梯度技术可应用于SAGA和SVRG等多种梯度估计器。

Conclusion: 论文提出的联邦优化模型能够更准确地评估不同客户端选择策略的实际通信成本，而基于SAGA和递归梯度技术的新算法在非凸优化中达到了最优的通信和计算复杂度，为联邦学习优化提供了更高效的解决方案。

Abstract: Different federated optimization algorithms typically employ distinct client-selection strategies: some methods communicate only with a randomly sampled subset of clients at each round, while others need to periodically communicate with all clients or use a hybrid scheme that combines both strategies. However, existing metrics for comparing optimization methods typically do not distinguish between these strategies, which often incur different communication costs in practice. To address this disparity, we introduce a simple and natural model of federated optimization that quantifies communication and local computation complexities. This new model allows for several commonly used client-selection strategies and explicitly associates each with a distinct cost. Within this setting, we propose a new algorithm that achieves the best-known communication and local complexities among existing federated optimization methods for non-convex optimization. This algorithm is based on the inexact composite gradient method with a carefully constructed gradient estimator and a special procedure for solving the auxiliary subproblem at each iteration. The gradient estimator is based on SAGA, a popular variance-reduced gradient estimator. We first derive a new variance bound for it, showing that SAGA can exploit functional similarity. We then introduce the Recursive-Gradient technique as a general way to potentially improve the error bound of a given conditionally unbiased gradient estimator, including both SAGA and SVRG. By applying this technique to SAGA, we obtain a new estimator, RG-SAGA, which has an improved error bound compared to the original one.

</details>


### [34] [PathFinder: MCTS and LLM Feedback-based Path Selection for Multi-Hop Question Answering](https://arxiv.org/abs/2512.05336)
*Durga Prasad Maram,Kalpa Gunaratna,Vijay Srinivasan,Haris Jeelani,Srinivas Chappidi*

Main category: cs.LG

TL;DR: PATHFINDER使用蒙特卡洛树搜索生成训练路径轨迹，通过子答案召回和LLM作为裁判验证过滤错误轨迹，改进多跳问答性能


<details>
  <summary>Details</summary>
Motivation: 多跳问答任务中，现有基于训练的方法仍受LLM幻觉和错误推理路径影响，导致性能受限

Method: 使用蒙特卡洛树搜索生成训练路径轨迹；通过子答案召回和LLM作为裁判验证过滤错误和冗长轨迹；重新表述子查询处理检索失败情况

Result: PATHFINDER在公共基准数据集上提高了多跳问答的性能

Conclusion: 通过改进训练数据质量和处理检索失败，PATHFINDER能有效提升多跳问答系统的性能

Abstract: Multi-hop question answering is a challenging task in which language models must reason over multiple steps to reach the correct answer. With the help of Large Language Models and their reasoning capabilities, existing systems are able to think and decompose an input question over multiple steps to analyze, retrieve, and reason. However, training-based approaches for this problem still suffer from LLM hallucinations and incorrect reasoning paths that hinder performance. Hence, we propose PATHFINDER, an approach that: (i) uses Monte Carlo Tree Search to generate training path traces, (ii) improves training data quality by filtering erroneous and lengthy traces using sub-answer recall and LLM-as-a-judge verification, and (iii) reformulates sub-queries to handle failed retrieval cases. By following these steps, we demonstrate that PATHFINDER improves the performance of multi-hop QA over public benchmark datasets.

</details>


### [35] [Taxonomy-Adaptive Moderation Model with Robust Guardrails for Large Language Models](https://arxiv.org/abs/2512.05339)
*Mahesh Kumar Nandwana,Youngwan Lim,Joseph Liu,Alex Yang,Varun Notibala,Nishchaie Khanna*

Main category: cs.LG

TL;DR: Roblox Guard 1.0是基于Llama-3.1-8B-Instruct的指令微调LLM，通过输入输出双重审核增强LLM系统安全性，并发布了RobloxGuard-Eval评估基准。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在训练后进行了安全对齐，但仍可能生成不当输出，存在用户风险，需要跨输入输出的鲁棒安全防护机制。

Method: 基于Llama-3.1-8B-Instruct进行指令微调，使用合成和开源安全数据集，结合思维链推理和输入反转增强上下文理解和决策能力，采用LLM管道提升审核能力。

Result: 模型能够泛化到未见过的安全分类体系，在域外安全基准上表现优异，同时发布了可扩展安全分类的RobloxGuard-Eval评估基准。

Conclusion: Roblox Guard 1.0通过综合的输入输出审核机制有效提升LLM系统安全性，为LLM安全防护提供了新的解决方案和评估框架。

Abstract: Large Language Models (LLMs) are typically aligned for safety during the post-training phase; however, they may still generate inappropriate outputs that could potentially pose risks to users. This challenge underscores the need for robust safeguards that operate across both model inputs and outputs. In this work, we introduce Roblox Guard 1.0, a state-of-the-art instruction fine-tuned LLM designed to enhance the safety of LLM systems through comprehensive input-output moderation, using a pipeline of LLMs to enhance moderation capability. Built on the Llama-3.1-8B-Instruct backbone, our model is instruction fine-tuned to generalize across previously unseen safety taxonomies and demonstrates strong performance on out-of-domain safety benchmarks. The instruction fine-tuning process uses a mix of synthetic and open-source safety datasets, augmented with chain-of-thought (CoT) rationales and input inversion to enhance contextual understanding and decision making. To support systematic evaluation, we also release RobloxGuard-Eval, a new benchmark featuring an extensible safety taxonomy to assess the effectiveness of LLM guardrails and moderation frameworks.

</details>


### [36] [When Forgetting Builds Reliability: LLM Unlearning for Reliable Hardware Code Generation](https://arxiv.org/abs/2512.05341)
*Yiwen Liang,Qiufeng Li,Shikai Wang,Weidong Cao*

Main category: cs.LG

TL;DR: 论文提出了一种针对硬件代码生成的LLM遗忘学习框架，通过语法保持的遗忘策略和细粒度选择性损失函数，有效移除LLM中的问题知识而不损害代码生成能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在硬件设计代码生成中存在可靠性问题，包括对专有知识产权、受污染基准和不安全编码模式的记忆，需要一种方法来安全移除这些有问题的知识。

Method: 结合语法保持的遗忘策略（保护硬件代码结构完整性）和细粒度floor-aware选择性损失函数（精确高效移除问题知识），实现有效遗忘而不降低代码生成能力。

Result: 实验表明该框架支持高达3倍大的遗忘集，通常只需单次训练轮次，同时保持RTL代码的语法正确性和功能完整性。

Conclusion: 该工作为可靠的LLM辅助硬件设计开辟了新途径，通过专门设计的遗忘学习框架解决了LLM在硬件代码生成中的可靠性挑战。

Abstract: Large Language Models (LLMs) have shown strong potential in accelerating digital hardware design through automated code generation. Yet, ensuring their reliability remains a critical challenge, as existing LLMs trained on massive heterogeneous datasets often exhibit problematic memorization of proprietary intellectual property (IP), contaminated benchmarks, and unsafe coding patterns. To mitigate these risks, we propose a novel unlearning framework tailored for LLM-based hardware code generation. Our method combines (i) a syntax-preserving unlearning strategy that safeguards the structural integrity of hardware code during forgetting, and (ii) a fine-grained floor-aware selective loss that enables precise and efficient removal of problematic knowledge. This integration achieves effective unlearning without degrading LLM code generation capabilities. Extensive experiments show that our framework supports forget sets up to 3x larger, typically requiring only a single training epoch, while preserving both syntactic correctness and functional integrity of register-transfer level (RTL) codes. Our work paves an avenue towards reliable LLM-assisted hardware design.

</details>


### [37] [Text Rationalization for Robust Causal Effect Estimation](https://arxiv.org/abs/2512.05373)
*Lijinghua Zhang,Hengrui Cai*

Main category: cs.LG

TL;DR: CATR框架通过选择稀疏的必要文本标记子集来解决文本数据因果推断中的混杂偏倚和正性假设违反问题，提高因果效应估计的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 高维文本数据虽然能编码丰富的上下文信息，但在因果推断中带来独特挑战：正性假设在观测层面经常被违反，冗余或虚假文本特征会膨胀维度，导致极端倾向得分、权重不稳定和效应估计方差膨胀。

Method: 提出混杂感知标记合理化（CATR）框架，使用残差独立性诊断选择稀疏的必要标记子集，旨在保留足够用于无混杂性的混杂信息，丢弃无关文本同时保留关键信号。

Result: 在合成数据和MIMIC-III数据库的真实世界研究中，CATR比现有基线方法产生更准确、稳定和可解释的因果效应估计。

Conclusion: CATR框架通过选择稀疏文本标记子集有效缓解观测层面的正性假设违反问题，稳定下游因果效应估计器，为文本数据因果推断提供了实用解决方案。

Abstract: Recent advances in natural language processing have enabled the increasing use of text data in causal inference, particularly for adjusting confounding factors in treatment effect estimation. Although high-dimensional text can encode rich contextual information, it also poses unique challenges for causal identification and estimation. In particular, the positivity assumption, which requires sufficient treatment overlap across confounder values, is often violated at the observational level, when massive text is represented in feature spaces. Redundant or spurious textual features inflate dimensionality, producing extreme propensity scores, unstable weights, and inflated variance in effect estimates. We address these challenges with Confounding-Aware Token Rationalization (CATR), a framework that selects a sparse necessary subset of tokens using a residual-independence diagnostic designed to preserve confounding information sufficient for unconfoundedness. By discarding irrelevant texts while retaining key signals, CATR mitigates observational-level positivity violations and stabilizes downstream causal effect estimators. Experiments on synthetic data and a real-world study using the MIMIC-III database demonstrate that CATR yields more accurate, stable, and interpretable causal effect estimates than existing baselines.

</details>


### [38] [China Regional 3km Downscaling Based on Residual Corrective Diffusion Model](https://arxiv.org/abs/2512.05377)
*Honglu Sun,Hao Jing,Zhixiang Dai,Sa Xiao,Wei Xue,Jian Sun,Qifeng Lu*

Main category: cs.LG

TL;DR: 该研究基于CorrDiff扩散模型框架，将统计降尺度方法应用于中国区域天气预测，将25km全球预报降尺度至3km分辨率，相比传统区域模型CMA-MESO在MAE指标上表现更优。


<details>
  <summary>Details</summary>
Motivation: 数值天气预报中高效生成高分辨率预报是一个基本挑战。传统方法包括动力降尺度和统计降尺度，本研究专注于统计降尺度，利用深度学习建立低分辨率与高分辨率历史数据之间的统计关系，以生成更精细的天气预报。

Method: 基于CorrDiff扩散模型的降尺度框架，将研究区域扩大近20倍，不仅考虑地表变量，还包含六个气压层的高层变量作为目标降尺度变量。添加全局残差连接以提高准确性。使用CMA-GFS（25km全球网格预报）和SFF（基于球形傅里叶神经算子的数据驱动深度学习天气模型）作为输入，生成中国区域3km预报。

Result: 实验结果表明，该方法降尺度后的预报在目标变量的MAE指标上普遍优于CMA-MESO的直接预报。雷达组合反射率预报显示，CorrDiff作为生成模型能够生成更精细的细节，相比确定性回归模型产生更真实的预测。

Conclusion: 基于CorrDiff扩散模型的统计降尺度方法能够有效生成高分辨率天气预报，在扩大区域范围和包含多层变量的情况下仍保持良好性能，为数值天气预报提供了一种高效的降尺度解决方案。

Abstract: A fundamental challenge in numerical weather prediction is to efficiently produce high-resolution forecasts. A common solution is applying downscaling methods, which include dynamical downscaling and statistical downscaling, to the outputs of global models. This work focuses on statistical downscaling, which establishes statistical relationships between low-resolution and high-resolution historical data using statistical models. Deep learning has emerged as a powerful tool for this task, giving rise to various high-performance super-resolution models, which can be directly applied for downscaling, such as diffusion models and Generative Adversarial Networks. This work relies on a diffusion-based downscaling framework named CorrDiff. In contrast to the original work of CorrDiff, the region considered in this work is nearly 20 times larger, and we not only consider surface variables as in the original work, but also encounter high-level variables (six pressure levels) as target downscaling variables. In addition, a global residual connection is added to improve accuracy. In order to generate the 3km forecasts for the China region, we apply our trained models to the 25km global grid forecasts of CMA-GFS, an operational global model of the China Meteorological Administration (CMA), and SFF, a data-driven deep learning-based weather model developed from Spherical Fourier Neural Operators (SFNO). CMA-MESO, a high-resolution regional model, is chosen as the baseline model. The experimental results demonstrate that the forecasts downscaled by our method generally outperform the direct forecasts of CMA-MESO in terms of MAE for the target variables. Our forecasts of radar composite reflectivity show that CorrDiff, as a generative model, can generate fine-scale details that lead to more realistic predictions compared to the corresponding deterministic regression models.

</details>


### [39] [Generalization Beyond Benchmarks: Evaluating Learnable Protein-Ligand Scoring Functions on Unseen Targets](https://arxiv.org/abs/2512.05386)
*Jakub Kopko,David Graber,Saltuk Mustafa Eyrilmez,Stanislav Mazurenko,David Bednar,Jiri Sedlar,Josef Sivic*

Main category: cs.LG

TL;DR: 评估蛋白质-配体评分函数在新靶点上的泛化能力，发现现有基准测试无法反映真实挑战，探索自监督预训练和有限测试数据利用方法


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在分子设计中日益重要，需要确保可学习的蛋白质-配体评分函数在新蛋白质靶点上的可靠性。尽管许多评分函数在标准基准测试中表现良好，但其在训练数据之外的泛化能力仍然是一个重大挑战。

Method: 1) 在模拟有限已知结构和实验亲和力测量的靶点数据集分割上评估最先进评分函数的泛化能力；2) 研究大规模自监督预训练是否能弥合泛化差距；3) 探索利用有限测试靶点数据改进评分函数性能的简单方法。

Result: 分析显示常用基准测试不能反映泛化到新靶点的真实挑战；提供了自监督预训练潜力的初步证据；测试了利用有限测试数据改进性能的方法。

Conclusion: 研究结果强调了需要更严格的评估协议，并为设计具有扩展到新蛋白质靶点预测能力的评分函数提供了实用指导。

Abstract: As machine learning becomes increasingly central to molecular design, it is vital to ensure the reliability of learnable protein-ligand scoring functions on novel protein targets. While many scoring functions perform well on standard benchmarks, their ability to generalize beyond training data remains a significant challenge. In this work, we evaluate the generalization capability of state-of-the-art scoring functions on dataset splits that simulate evaluation on targets with a limited number of known structures and experimental affinity measurements. Our analysis reveals that the commonly used benchmarks do not reflect the true challenge of generalizing to novel targets. We also investigate whether large-scale self-supervised pretraining can bridge this generalization gap and we provide preliminary evidence of its potential. Furthermore, we probe the efficacy of simple methods that leverage limited test-target data to improve scoring function performance. Our findings underscore the need for more rigorous evaluation protocols and offer practical guidance for designing scoring functions with predictive power extending to novel protein targets.

</details>


### [40] [Smart Timing for Mining: A Deep Learning Framework for Bitcoin Hardware ROI Prediction](https://arxiv.org/abs/2512.05402)
*Sithumi Wickramasinghe,Bikramjit Das,Dorien Herremans*

Main category: cs.LG

TL;DR: 本文提出MineROI-Net，一个基于Transformer的模型，用于预测比特币挖矿硬件购买的盈利性，帮助矿工在波动的市场中做出更明智的投资决策。


<details>
  <summary>Details</summary>
Motivation: 比特币挖矿硬件采购面临市场波动大、技术快速过时、协议驱动收益周期等挑战，但现有研究缺乏何时购买ASIC硬件的指导，也没有计算框架解决这一决策问题。

Method: 将硬件采购问题建模为时间序列分类任务，预测购买ASIC机器在一年内是否盈利。提出MineROI-Net，一个开源的基于Transformer的架构，旨在捕捉挖矿盈利性的多尺度时间模式。

Result: 在2015-2024年间发布的20种ASIC矿机数据上评估，MineROI-Net在多种市场环境下优于LSTM和TSLANet基线，达到83.7%准确率和83.1%宏F1分数。模型在经济相关性方面表现强劲，对无盈利时期的检测精度达93.6%，对盈利时期达98.5%。

Conclusion: MineROI-Net为挖矿硬件采购时机提供了一个实用的数据驱动工具，有望降低资本密集型挖矿操作的财务风险，模型已在GitHub开源。

Abstract: Bitcoin mining hardware acquisition requires strategic timing due to volatile markets, rapid technological obsolescence, and protocol-driven revenue cycles. Despite mining's evolution into a capital-intensive industry, there is little guidance on when to purchase new Application-Specific Integrated Circuit (ASIC) hardware, and no prior computational frameworks address this decision problem. We address this gap by formulating hardware acquisition as a time series classification task, predicting whether purchasing ASIC machines yields profitable (Return on Investment (ROI) >= 1), marginal (0 < ROI < 1), or unprofitable (ROI <= 0) returns within one year. We propose MineROI-Net, an open source Transformer-based architecture designed to capture multi-scale temporal patterns in mining profitability. Evaluated on data from 20 ASIC miners released between 2015 and 2024 across diverse market regimes, MineROI-Net outperforms LSTM-based and TSLANet baselines, achieving 83.7% accuracy and 83.1% macro F1-score. The model demonstrates strong economic relevance, achieving 93.6% precision in detecting unprofitable periods and 98.5% precision for profitable ones, while avoiding misclassification of profitable scenarios as unprofitable and vice versa. These results indicate that MineROI-Net offers a practical, data-driven tool for timing mining hardware acquisitions, potentially reducing financial risk in capital-intensive mining operations. The model is available through: https://github.com/AMAAI-Lab/MineROI-Net.

</details>


### [41] [RevoNAD: Reflective Evolutionary Exploration for Neural Architecture Design](https://arxiv.org/abs/2512.05403)
*Gyusam Chang,Jeongyoon Yoon,Shin han yi,JaeHyeok Lee,Sujin Jang,Sangpil Kim*

Main category: cs.LG

TL;DR: RevoNAD是一个结合大语言模型与进化算法的神经架构设计系统，通过多轮多专家共识、自适应反射探索和帕累托进化选择，实现了反馈对齐的架构搜索，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的神经架构设计系统面临挑战：令牌级设计循环是离散且不可微分的，导致反馈无法平滑指导架构改进。这些方法通常会出现模式坍塌到冗余结构或漂移到不可行设计的问题，当构造性推理缺乏良好基础时尤为明显。

Method: 1. 多轮多专家共识：将孤立的设计规则转化为有意义的架构线索；2. 自适应反射探索：利用奖励方差调整探索程度，在反馈不确定时探索，在稳定性达到时优化；3. 帕累托引导的进化选择：共同优化准确性、效率、延迟、置信度和结构多样性的架构。

Result: 在CIFAR10、CIFAR100、ImageNet16-120、COCO-5K和Cityscape等多个数据集上实现了最先进的性能。消融和迁移研究进一步验证了RevoNAD在实际可靠和可部署神经架构设计方面的有效性。

Conclusion: RevoNAD通过将基于大语言模型的推理与反馈对齐的架构搜索有效结合，解决了现有LLM驱动生成方法的局限性，实现了实用可靠且可部署的神经架构设计。

Abstract: Recent progress in leveraging large language models (LLMs) has enabled Neural Architecture Design (NAD) systems to generate new architecture not limited from manually predefined search space. Nevertheless, LLM-driven generation remains challenging: the token-level design loop is discrete and non-differentiable, preventing feedback from smoothly guiding architectural improvement. These methods, in turn, commonly suffer from mode collapse into redundant structures or drift toward infeasible designs when constructive reasoning is not well grounded. We introduce RevoNAD, a reflective evolutionary orchestrator that effectively bridges LLM-based reasoning with feedback-aligned architectural search. First, RevoNAD presents a Multi-round Multi-expert Consensus to transfer isolated design rules into meaningful architectural clues. Then, Adaptive Reflective Exploration adjusts the degree of exploration leveraging reward variance; it explores when feedback is uncertain and refines when stability is reached. Finally, Pareto-guided Evolutionary Selection effectively promotes architectures that jointly optimize accuracy, efficiency, latency, confidence, and structural diversity. Across CIFAR10, CIFAR100, ImageNet16-120, COCO-5K, and Cityscape, RevoNAD achieves state-of-the-art performance. Ablation and transfer studies further validate the effectiveness of RevoNAD in allowing practically reliable, and deployable neural architecture design.

</details>


### [42] [TS-HINT: Enhancing Semiconductor Time Series Regression Using Attention Hints From Large Language Model Reasoning](https://arxiv.org/abs/2512.05419)
*Jonathan Adam Rico,Nagarajan Raghavan,Senthilnath Jayavelu*

Main category: cs.LG

TL;DR: 提出TS-Hint框架，结合时间序列基础模型和思维链推理，通过注意力提示改进半导体制造中材料去除率的预测，在有限数据下实现有效学习。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法从时间序列中提取静态特征来近似半导体制造过程中的材料去除率，但这种方法会丢失时间动态信息，且需要大量数据进行有效训练。

Method: 提出TS-Hint框架，整合时间序列基础模型与思维链推理，基于注意力机制数据和显著性数据在训练过程中提供注意力提示。

Result: 实验结果表明该模型在有限数据设置下通过少样本学习表现出有效性，并能直接从多元时间序列特征中学习。

Conclusion: TS-Hint框架能够有效解决现有方法在时间动态信息丢失和大量数据需求方面的局限性，为半导体制造过程监控提供了更高效的解决方案。

Abstract: Existing data-driven methods rely on the extraction of static features from time series to approximate the material removal rate (MRR) of semiconductor manufacturing processes such as chemical mechanical polishing (CMP). However, this leads to a loss of temporal dynamics. Moreover, these methods require a large amount of data for effective training. In this paper, we propose TS-Hint, a Time Series Foundation Model (TSFM) framework, integrated with chain-of-thought reasoning which provides attention hints during training based on attention mechanism data and saliency data. Experimental results demonstrate the effectiveness of our model in limited data settings via few-shot learning and can learn directly from multivariate time series features.

</details>


### [43] [IdealTSF: Can Non-Ideal Data Contribute to Enhancing the Performance of Time Series Forecasting Models?](https://arxiv.org/abs/2512.05442)
*Hua Wang,Jinghao Lu,Fan Zhang*

Main category: cs.LG

TL;DR: IdealTSF框架利用非理想负样本增强时间序列预测，通过预训练、训练和优化三阶段，结合对抗扰动机制，在噪声数据场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测中，缺失值和异常值等非理想数据阻碍深度学习发展。现有研究主要关注从序列数据提取特征或将非理想数据作为正样本进行知识迁移，但更有效的方法是利用这些非理想负样本来增强事件预测能力。

Method: 提出IdealTSF框架，包含三个渐进步骤：1) 预训练阶段从负样本数据提取知识；2) 训练阶段将序列数据转换为理想正样本；3) 应用带有对抗扰动的负优化机制。框架整合了理想正样本和非理想负样本。

Result: 大量实验表明，负样本数据在基础注意力架构中释放了时间序列预测的显著潜力。IdealTSF特别适用于具有噪声样本或低质量数据的应用场景。

Conclusion: IdealTSF框架通过有效利用非理想负样本，增强了时间序列预测模型的性能，特别是在处理噪声数据和低质量数据时表现突出，为时间序列预测领域提供了新的思路和方法。

Abstract: Deep learning has shown strong performance in time series forecasting tasks. However, issues such as missing values and anomalies in sequential data hinder its further development in prediction tasks. Previous research has primarily focused on extracting feature information from sequence data or addressing these suboptimal data as positive samples for knowledge transfer. A more effective approach would be to leverage these non-ideal negative samples to enhance event prediction. In response, this study highlights the advantages of non-ideal negative samples and proposes the IdealTSF framework, which integrates both ideal positive and negative samples for time series forecasting. IdealTSF consists of three progressive steps: pretraining, training, and optimization. It first pretrains the model by extracting knowledge from negative sample data, then transforms the sequence data into ideal positive samples during training. Additionally, a negative optimization mechanism with adversarial disturbances is applied. Extensive experiments demonstrate that negative sample data unlocks significant potential within the basic attention architecture for time series forecasting. Therefore, IdealTSF is particularly well-suited for applications with noisy samples or low-quality data.

</details>


### [44] [How Ensemble Learning Balances Accuracy and Overfitting: A Bias-Variance Perspective on Tabular Data](https://arxiv.org/abs/2512.05469)
*Zubair Ahmed Mohammad*

Main category: cs.LG

TL;DR: 集成模型在表格分类任务中如何平衡准确率和过拟合的研究，发现集成方法通过降低方差实现高准确率且保持小泛化差距，但在不同数据特性下效果不同


<details>
  <summary>Details</summary>
Motivation: 研究集成模型在保持高准确率的同时控制过拟合的能力，理解集成方法在不同类型表格数据上的泛化表现，为实际应用中的模型选择提供指导

Method: 使用重复分层交叉验证和统计显著性检验，在四个表格分类任务（乳腺癌、心脏病、皮马糖尿病、信用卡欺诈）上比较线性模型、单决策树和九种集成方法

Result: 集成方法通过平均或受控提升降低方差，能在保持泛化差距低于3%的同时提高测试准确率5-7个百分点；在线性清洁数据上集成增益有限，在非线性结构数据上效果显著，在噪声或高度不平衡数据上需要正则化

Conclusion: 集成模型能够有效平衡准确率和过拟合，但效果取决于数据特性；数据集复杂度指标（线性度评分、Fisher比率、噪声估计）可预测集成控制方差的效果，为实际表格应用中的模型选择提供实用指导

Abstract: Ensemble models often achieve higher accuracy than single learners, but their ability to maintain small generalization gaps is not always well understood. This study examines how ensembles balance accuracy and overfitting across four tabular classification tasks: Breast Cancer, Heart Disease, Pima Diabetes, and Credit Card Fraud. Using repeated stratified cross validation with statistical significance testing, we compare linear models, a single decision tree, and nine ensemble methods. The results show that ensembles can reach high accuracy without large gaps by reducing variance through averaging or controlled boosting. On nearly linear and clean data, linear models already generalize well and ensembles offer little additional benefit. On datasets with meaningful nonlinear structure, tree based ensembles increase test accuracy by 5 to 7 points while keeping gaps below 3 percent. On noisy or highly imbalanced datasets, ensembles remain competitive but require regularization to avoid fitting noise or majority class patterns. We also compute simple dataset complexity indicators, such as linearity score, Fisher ratio, and noise estimate, which explain when ensembles are likely to control variance effectively. Overall, the study provides a clear view of how and when ensembles maintain high accuracy while keeping overfitting low, offering practical guidance for model selection in real world tabular applications.

</details>


### [45] [PERM EQ x GRAPH EQ: Equivariant Neural Networks for Quantum Molecular Learning](https://arxiv.org/abs/2512.05475)
*Saumya Biswas,Jiten Oswal*

Main category: cs.LG

TL;DR: 比较不同几何量子机器学习模型在分子几何结构学习中的性能，发现置换对称嵌入是最具泛化性的量子机器学习模型


<details>
  <summary>Details</summary>
Motivation: 研究分子几何结构层次中不同对称性量子机器学习模型的性能差异，为几何数据集选择合适模型提供标准

Method: 使用两个分子数据集（线性LiH分子和三角锥形NH3分子），比较无对称性、旋转和置换等变性、图嵌入置换等变性等量子机器学习模型，以经典等变模型为基准

Result: 图嵌入特征被证明是提高几何数据集可训练性的有效途径，置换对称嵌入被发现是几何学习中最具泛化性的量子机器学习模型

Conclusion: 分子几何结构与模型性能差异揭示了模型泛化性的选择标准，置换对称嵌入模型在几何学习中表现最优

Abstract: In hierarchal order of molecular geometry, we compare the performances of Geometric Quantum Machine Learning models. Two molecular datasets are considered: the simplistic linear shaped LiH-molecule and the trigonal pyramidal molecule NH3. Both accuracy and generalizability metrics are considered. A classical equivariant model is used as a baseline for the performance comparison. The comparative performance of Quantum Machine Learning models with no symmetry equivariance, rotational and permutational equivariance, and graph embedded permutational equivariance is investigated. The performance differentials and the molecular geometry in question reveals the criteria for choice of models for generalizability. Graph embedding of features is shown to be an effective pathway to greater trainability for geometric datasets. Permutational symmetric embedding is found to be the most generalizable quantum Machine Learning model for geometric learning.

</details>


### [46] [Credal and Interval Deep Evidential Classifications](https://arxiv.org/abs/2512.05526)
*Michele Caprio,Shireen K. Manchingal,Fabio Cuzzolin*

Main category: cs.LG

TL;DR: 本文提出了两种新的不确定性量化方法CDEC和IDEC，用于分类任务中的不确定性评估，能够区分认知和偶然不确定性，并在不确定性过高时拒绝分类，在可接受范围内提供具有概率保证的标签集合。


<details>
  <summary>Details</summary>
Motivation: 不确定性量化在人工智能领域至关重要，影响决策制定、风险评估和模型可靠性。现有方法存在不足，需要新的方法来系统评估认知不确定性和偶然不确定性，避免对训练数据的过拟合。

Method: 提出了两种新方法：CDEC（基于信度集的深度证据分类）和IDEC（基于区间的深度证据分类）。CDEC使用信度集（概率的闭凸集），IDEC使用证据预测分布的区间。两种方法都使用标准反向传播和基于证据理论的损失函数进行训练。

Result: 在MNIST、CIFAR-10、CIFAR-100及其自然OoD偏移数据集上的实验表明，CDEC和IDEC实现了有竞争力的预测准确性、在认知不确定性和总不确定性下的最先进OoD检测能力，以及紧密、校准良好的预测区域。消融实验显示CDEC仅需小规模集成就能获得稳定的不确定性估计。

Conclusion: CDEC和IDEC克服了先前工作的不足，扩展了当前证据深度学习的文献，为分类任务提供了有效的不确定性量化方法，能够在不确定性过高时拒绝分类，在可接受范围内提供具有概率保证的预测。

Abstract: Uncertainty Quantification (UQ) presents a pivotal challenge in the field of Artificial Intelligence (AI), profoundly impacting decision-making, risk assessment and model reliability. In this paper, we introduce Credal and Interval Deep Evidential Classifications (CDEC and IDEC, respectively) as novel approaches to address UQ in classification tasks. CDEC and IDEC leverage a credal set (closed and convex set of probabilities) and an interval of evidential predictive distributions, respectively, allowing us to avoid overfitting to the training data and to systematically assess both epistemic (reducible) and aleatoric (irreducible) uncertainties. When those surpass acceptable thresholds, CDEC and IDEC have the capability to abstain from classification and flag an excess of epistemic or aleatoric uncertainty, as relevant. Conversely, within acceptable uncertainty bounds, CDEC and IDEC provide a collection of labels with robust probabilistic guarantees. CDEC and IDEC are trained using standard backpropagation and a loss function that draws from the theory of evidence. They overcome the shortcomings of previous efforts, and extend the current evidential deep learning literature. Through extensive experiments on MNIST, CIFAR-10 and CIFAR-100, together with their natural OoD shifts (F-MNIST/K-MNIST, SVHN/Intel, TinyImageNet), we show that CDEC and IDEC achieve competitive predictive accuracy, state-of-the-art OoD detection under epistemic and total uncertainty, and tight, well-calibrated prediction regions that expand reliably under distribution shift. An ablation over ensemble size further demonstrates that CDEC attains stable uncertainty estimates with only a small ensemble.

</details>


### [47] [IDK-S: Incremental Distributional Kernel for Streaming Anomaly Detection](https://arxiv.org/abs/2512.05531)
*Yang Xu,Yixiao Ma,Kaifeng Zhang,Zuliang Yang,Kai Ming Ting*

Main category: cs.LG

TL;DR: IDK-S是一种用于数据流异常检测的增量分布核方法，通过动态核均值嵌入表示，在保持高检测精度的同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 数据流异常检测面临两大挑战：需要在分布不断演变的情况下保持高检测精度，同时确保实时效率。现有方法往往难以同时满足这两个要求。

Method: IDK-S基于隔离分布核（IDK）框架，采用增量更新机制。它继承了IDK使用数据依赖核的优势，同时通过轻量级增量更新策略避免完全模型重训练，显著降低计算复杂度。

Result: 在13个基准测试上的实验表明，IDK-S在检测精度上优于现有方法，同时运行速度显著更快（通常快一个数量级），且统计上等价于完全重训练的模型。

Conclusion: IDK-S成功解决了数据流异常检测中精度与效率的平衡问题，通过创新的增量分布核方法实现了高精度实时检测，为流数据异常检测提供了有效的解决方案。

Abstract: Anomaly detection on data streams presents significant challenges, requiring methods to maintain high detection accuracy among evolving distributions while ensuring real-time efficiency. Here we introduce $\mathcal{IDK}$-$\mathcal{S}$, a novel $\mathbf{I}$ncremental $\mathbf{D}$istributional $\mathbf{K}$ernel for $\mathbf{S}$treaming anomaly detection that effectively addresses these challenges by creating a new dynamic representation in the kernel mean embedding framework. The superiority of $\mathcal{IDK}$-$\mathcal{S}$ is attributed to two key innovations. First, it inherits the strengths of the Isolation Distributional Kernel, an offline detector that has demonstrated significant performance advantages over foundational methods like Isolation Forest and Local Outlier Factor due to the use of a data-dependent kernel. Second, it adopts a lightweight incremental update mechanism that significantly reduces computational overhead compared to the naive baseline strategy of performing a full model retraining. This is achieved without compromising detection accuracy, a claim supported by its statistical equivalence to the full retrained model. Our extensive experiments on thirteen benchmarks demonstrate that $\mathcal{IDK}$-$\mathcal{S}$ achieves superior detection accuracy while operating substantially faster, in many cases by an order of magnitude, than existing state-of-the-art methods.

</details>


### [48] [SCoNE: Spherical Consistent Neighborhoods Ensemble for Effective and Efficient Multi-View Anomaly Detection](https://arxiv.org/abs/2512.05540)
*Yang Xu,Hang Zhang,Yixiao Ma,Ye Zhu,Kai Ming Ting*

Main category: cs.LG

TL;DR: SCoNE提出了一种新的多视图异常检测方法，通过直接使用多视图实例表示一致邻域，无需中间表示，实现了O(N)时间复杂度，并在大规模数据集上显著提升了检测精度和运行速度。


<details>
  <summary>Details</summary>
Motivation: 现有多视图异常检测方法存在两个关键问题：1) 在不同视图中密度变化区域难以有效捕捉一致邻域，导致检测精度低；2) 学习过程计算复杂度高(O(N²))，不适用于大规模数据集。

Method: 提出SCoNE方法，具有两个独特特征：1) 直接使用多视图实例表示一致邻域，无需中间表示；2) 邻域具有数据依赖特性，在稀疏区域形成大邻域，在密集区域形成小邻域，无需学习过程。

Result: 实验评估表明，SCoNE具有优越的检测精度，在大规模数据集上运行速度比现有方法快几个数量级。

Conclusion: SCoNE通过直接表示数据依赖的一致邻域，解决了多视图异常检测中的一致邻域表示和计算效率问题，实现了高效准确的异常检测。

Abstract: The core problem in multi-view anomaly detection is to represent local neighborhoods of normal instances consistently across all views. Recent approaches consider a representation of local neighborhood in each view independently, and then capture the consistent neighbors across all views via a learning process. They suffer from two key issues. First, there is no guarantee that they can capture consistent neighbors well, especially when the same neighbors are in regions of varied densities in different views, resulting in inferior detection accuracy. Second, the learning process has a high computational cost of $\mathcal{O}(N^2)$, rendering them inapplicable for large datasets. To address these issues, we propose a novel method termed \textbf{S}pherical \textbf{C}onsistent \textbf{N}eighborhoods \textbf{E}nsemble (SCoNE). It has two unique features: (a) the consistent neighborhoods are represented with multi-view instances directly, requiring no intermediate representations as used in existing approaches; and (b) the neighborhoods have data-dependent properties, which lead to large neighborhoods in sparse regions and small neighborhoods in dense regions. The data-dependent properties enable local neighborhoods in different views to be represented well as consistent neighborhoods, without learning. This leads to $\mathcal{O}(N)$ time complexity. Empirical evaluations show that SCoNE has superior detection accuracy and runs orders-of-magnitude faster in large datasets than existing approaches.

</details>


### [49] [Improving Local Fidelity Through Sampling and Modeling Nonlinearity](https://arxiv.org/abs/2512.05556)
*Sanjeev Shrestha,Rahul Dubey,Hui Liu*

Main category: cs.LG

TL;DR: 本文提出了一种基于多元自适应回归样条（MARS）的新解释方法，通过建模非线性局部边界来改进LIME，在三个UCI数据集上实现了更高的保真度解释。


<details>
  <summary>Details</summary>
Motivation: 随着黑盒机器学习模型在关键领域的应用增加，提供模型预测解释变得至关重要。现有的LIME方法假设局部决策边界是线性的，无法捕捉非线性关系，导致解释不准确。

Method: 提出一种新方法：1）使用多元自适应回归样条（MARS）建模非线性局部边界，更好地捕捉参考模型的基础行为；2）采用N-ball采样技术，直接从期望分布中采样而非像LIME那样重新加权样本。

Result: 在三个UCI数据集上对不同分类器和不同核宽度进行评估，结果显示该方法相比基线方法产生更忠实的解释，平均减少37%的均方根误差，显著提高了局部保真度。

Conclusion: 该方法通过MARS建模非线性局部边界和N-ball采样技术，能够生成更高保真度的解释，有效解决了LIME方法在捕捉非线性关系方面的局限性。

Abstract: With the increasing complexity of black-box machine learning models and their adoption in high-stakes areas, it is critical to provide explanations for their predictions. Local Interpretable Model-agnostic Explanation (LIME) is a widely used technique that explains the prediction of any classifier by learning an interpretable model locally around the predicted instance. However, it assumes that the local decision boundary is linear and fails to capture the non-linear relationships, leading to incorrect explanations. In this paper, we propose a novel method that can generate high-fidelity explanations. Multivariate adaptive regression splines (MARS) is used to model non-linear local boundaries that effectively captures the underlying behavior of the reference model, thereby enhancing the local fidelity of the explanation. Additionally, we utilize the N-ball sampling technique, which samples directly from the desired distribution instead of reweighting samples as done in LIME, further improving the faithfulness score. We evaluate our method on three UCI datasets across different classifiers and varying kernel widths. Experimental results show that our method yields more faithful explanations compared to baselines, achieving an average reduction of 37% in root mean square error, significantly improving local fidelity.

</details>


### [50] [Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning](https://arxiv.org/abs/2512.05591)
*Zhenpeng Su,Leiyu Pan,Minxuan Lv,Tiehua Mei,Zijia Lin,Yuntao Li,Wenping Hu,Ruiming Tang,Kun Gai,Guorui Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种新的熵比剪裁机制，通过约束当前策略与先前策略之间的熵比来稳定强化学习训练，解决了PPO-Clip无法调节未采样动作概率分布偏移的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的后训练依赖强化学习来提高模型能力和对齐质量，但离策略训练范式引入了分布偏移，导致训练不稳定，表现为策略熵波动和梯度不稳定。虽然PPO-Clip通过重要性剪裁缓解了这个问题，但仍忽略了动作的全局分布偏移。

Method: 提出使用当前策略与先前策略之间的熵比作为新的全局度量，有效量化策略探索的相对变化。基于此度量，引入了熵比剪裁机制，对熵比施加双向约束，在全局分布层面稳定策略更新，并弥补PPO-clip无法调节未采样动作概率偏移的不足。将ERC集成到DAPO和GPPO强化学习算法中。

Result: 在多个基准测试上的实验表明，ERC能够持续提升性能。

Conclusion: 熵比剪裁机制通过全局分布层面的约束有效稳定了强化学习训练，解决了现有方法在调节未采样动作概率分布偏移方面的局限性，为大型语言模型的后训练提供了更稳定的优化方法。

Abstract: Large language model post-training relies on reinforcement learning to improve model capability and alignment quality. However, the off-policy training paradigm introduces distribution shift, which often pushes the policy beyond the trust region, leading to training instabilities manifested as fluctuations in policy entropy and unstable gradients. Although PPO-Clip mitigates this issue through importance clipping, it still overlooks the global distributional shift of actions. To address these challenges, we propose using the entropy ratio between the current and previous policies as a new global metric that effectively quantifies the relative change in policy exploration throughout updates. Building on this metric, we introduce an \textbf{Entropy Ratio Clipping} (ERC) mechanism that imposes bidirectional constraints on the entropy ratio. This stabilizes policy updates at the global distribution level and compensates for the inability of PPO-clip to regulate probability shifts of un-sampled actions. We integrate ERC into both DAPO and GPPO reinforcement learning algorithms. Experiments across multiple benchmarks show that ERC consistently improves performance.

</details>


### [51] [Modular Jets for Supervised Pipelines: Diagnosing Mirage vs Identifiability](https://arxiv.org/abs/2512.05638)
*Suman Sanyal*

Main category: cs.LG

TL;DR: 论文提出"模块化喷射"概念，用于评估机器学习模型中模块分解的唯一性，区分可识别和幻象分解状态


<details>
  <summary>Details</summary>
Motivation: 传统监督学习主要通过预测风险评估模型，但无法确定模型内部分解是否由数据和评估设计唯一确定。需要一种方法来评估模块分解的唯一性

Method: 引入模块化喷射概念，估计经验喷射（局部线性响应映射），描述模块对输入微小结构化扰动的反应。提出MoJet算法进行经验喷射估计和幻象诊断

Result: 在双模块线性回归管道中证明了喷射可识别性定理：在温和秩假设和模块级喷射可访问条件下，内部分解是唯一确定的。而仅基于风险的评估则允许大量幻象分解

Conclusion: 模块化喷射提供了一种评估模型内部分解唯一性的新方法，区分了可识别和幻象分解状态，为理解机器学习模型内部结构提供了新视角

Abstract: Classical supervised learning evaluates models primarily via predictive risk on hold-out data. Such evaluations quantify how well a function behaves on a distribution, but they do not address whether the internal decomposition of a model is uniquely determined by the data and evaluation design. In this paper, we introduce \emph{Modular Jets} for regression and classification pipelines. Given a task manifold (input space), a modular decomposition, and access to module-level representations, we estimate empirical jets, which are local linear response maps that describe how each module reacts to small structured perturbations of the input. We propose an empirical notion of \emph{mirage} regimes, where multiple distinct modular decompositions induce indistinguishable jets and thus remain observationally equivalent, and contrast this with an \emph{identifiable} regime, where the observed jets single out a decomposition up to natural symmetries. In the setting of two-module linear regression pipelines we prove a jet-identifiability theorem. Under mild rank assumptions and access to module-level jets, the internal factorisation is uniquely determined, whereas risk-only evaluation admits a large family of mirage decompositions that implement the same input-to-output map. We then present an algorithm (MoJet) for empirical jet estimation and mirage diagnostics, and illustrate the framework using linear and deep regression as well as pipeline classification.

</details>


### [52] [Hyperparameter Transfer Enables Consistent Gains of Matrix-Preconditioned Optimizers Across Scales](https://arxiv.org/abs/2512.05620)
*Shikai Qiu,Zixi Chen,Hoang Phan,Qi Lei,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 该研究探讨了如何通过超参数迁移来扩展预条件优化器（如Shampoo、SOAP、Muon）的规模，提出了学习率和权重衰减随模型宽度和深度变化的缩放规则，并在Llama架构语言模型上验证了这些优化器相对于AdamW的加速效果。


<details>
  <summary>Details</summary>
Motivation: 近期一些基于矩阵级预条件的深度学习优化器在小规模实验中显示出比AdamW更快的速度，但其效果在更大规模验证时结果不一致。为了理解这些优化器在大规模应用中的有效性，需要研究如何通过超参数迁移来扩展预条件优化器。

Method: 研究学习率和权重衰减应如何随模型宽度和深度进行缩放，涵盖Shampoo、SOAP、Muon等多种优化器。考虑了常用的技术如分块（blocking）和嫁接（grafting）的影响，并基于μP理论进行扩展。通过实验验证缩放规则在Llama架构语言模型（190M到1.4B参数）上的效果。

Result: 研究发现：1）按μP缩放学习率能改善迁移效果，但仍存在有限宽度偏差导致最优学习率漂移，可通过分块和显式谱归一化缓解；2）对于计算最优缩放，权重衰减按1/宽度缩放几乎对所有优化器都是最优的；3）应用这些缩放规则后，Muon和Shampoo分别在训练Llama模型时获得1.4倍和1.3倍相对于AdamW的加速，而错误缩放会使加速效果随规模增大迅速消失。

Conclusion: 研究最优超参数迁移对于在现实调优预算下可靠比较大规模优化器至关重要。正确的缩放规则能使预条件优化器在大规模语言模型训练中保持相对于AdamW的显著加速优势。

Abstract: Several recently introduced deep learning optimizers utilizing matrix-level preconditioning have shown promising speedups relative to the current dominant optimizer AdamW, particularly in relatively small-scale experiments. However, efforts to validate and replicate their successes have reported mixed results. To better understand the effectiveness of these optimizers at scale, in this work we investigate how to scale preconditioned optimizers via hyperparameter transfer, building on prior works such as $μ$P. We study how the optimal learning rate and weight decay should scale with model width and depth for a wide range of optimizers, including Shampoo, SOAP, and Muon, accounting for the impact of commonly used techniques such as blocking and grafting. We find that scaling the learning rate according to $μ$P improves transfer, but can still suffer from significant finite-width deviations that cause drifting optimal learning rates, which we show can be mitigated by blocking and explicit spectral normalization. For compute-optimal scaling, we find scaling independent weight decay as $1/\mathrm{width}$ is nearly optimal across optimizers. Applying these scaling rules, we show Muon and Shampoo consistently achieve $1.4\times$ and $1.3\times$ speedup over AdamW for training Llama-architecture language models of sizes ranging from $190$M to $1.4$B, whereas the speedup vanishes rapidly with scale under incorrect scaling. Based on these results and further ablations, we argue that studying optimal hyperparameter transfer is essential for reliably comparing optimizers at scale given a realistic tuning budget.

</details>


### [53] [Feasibility of AI-Assisted Programming for End-User Development](https://arxiv.org/abs/2512.05666)
*Irene Weber*

Main category: cs.LG

TL;DR: AI辅助的终端用户编码可能成为低代码/无代码平台的有力补充或替代方案，让非程序员通过自然语言提示开发应用


<details>
  <summary>Details</summary>
Motivation: 探索AI辅助终端用户编码是否可行，以及它是否能补充或替代现有的低代码/无代码平台，推动组织数字化转型

Method: 通过案例研究，让非程序员与AI助手交互开发基础网页应用，分析任务完成情况和参与者反馈

Result: 大多数参与者能在合理时间内成功完成任务，并支持AI辅助终端用户编码作为可行的终端用户开发方法

Conclusion: AI辅助终端用户编码是可行的终端用户开发范式，可能补充甚至替代低代码/无代码平台，具有实践、研究和教学意义

Abstract: End-user development,where non-programmers create or adapt their own digital tools, can play a key role in driving digital transformation within organizations. Currently, low-code/no-code platforms are widely used to enable end-user development through visual programming, minimizing the need for manual coding. Recent advancements in generative AI, particularly large language model-based assistants and "copilots", open new possibilities, as they may enable end users to generate and refine programming code and build apps directly from natural language prompts. This approach, here referred to as AI-assisted end-user coding, promises greater flexibility, broader applicability, faster development, improved reusability, and reduced vendor lock-in compared to the established visual LCNC platforms. This paper investigates whether AI-assisted end-user coding is a feasible paradigm for end-user development, which may complement or even replace the LCNC model in the future. To explore this, we conducted a case study in which non-programmers were asked to develop a basic web app through interaction with AI assistants.The majority of study participants successfully completed the task in reasonable time and also expressed support for AI-assisted end-user coding as a viable approach for end-user development. The paper presents the study design, analyzes the outcomes, and discusses potential implications for practice, future research, and academic teaching.

</details>


### [54] [Bounded Graph Clustering with Graph Neural Networks](https://arxiv.org/abs/2512.05623)
*Kibidi Neocosmos,Diego Baptista,Nicole Ludwig*

Main category: cs.LG

TL;DR: 提出一种灵活的GNN社区检测框架，允许用户指定社区数量的范围或精确值，解决了传统GNN方法无法可靠控制社区数量的限制


<details>
  <summary>Details</summary>
Motivation: 传统社区检测方法需要预先指定聚类数量，而GNN方法即使指定了期望数量也经常无法准确返回该数量，存在设计上的局限性

Method: 引入灵活的框架，允许用户指定社区数量的合理范围，并在训练过程中强制执行这些边界约束；同时也支持指定精确的社区数量并可靠返回

Result: 该方法能够可靠地控制GNN发现的社区数量，无论是指定范围还是精确数量，都能在训练过程中有效执行约束条件

Conclusion: 提出的框架解决了GNN在社区检测中无法可靠控制社区数量的关键限制，提供了更灵活和原则性的方法来管理聚类数量

Abstract: In community detection, many methods require the user to specify the number of clusters in advance since an exhaustive search over all possible values is computationally infeasible. While some classical algorithms can infer this number directly from the data, this is typically not the case for graph neural networks (GNNs): even when a desired number of clusters is specified, standard GNN-based methods often fail to return the exact number due to the way they are designed. In this work, we address this limitation by introducing a flexible and principled way to control the number of communities discovered by GNNs. Rather than assuming the true number of clusters is known, we propose a framework that allows the user to specify a plausible range and enforce these bounds during training. However, if the user wants an exact number of clusters, it may also be specified and reliably returned.

</details>


### [55] [Mechanistic Interpretability of Antibody Language Models Using SAEs](https://arxiv.org/abs/2512.05794)
*Rebonto Haque,Oliver M. Turnbull,Anisha Parsan,Nithin Parsan,John J. Yang,Charlotte M. Deane*

Main category: cs.LG

TL;DR: TopK和Ordered稀疏自编码器用于分析抗体语言模型p-IgGen，TopK能揭示生物学特征但控制生成效果有限，Ordered能可靠识别可操控特征但激活模式更复杂。


<details>
  <summary>Details</summary>
Motivation: 研究稀疏自编码器在蛋白质语言模型中的机制可解释性应用，特别是探索如何有效解释和操控抗体语言模型p-IgGen的生成过程。

Method: 使用TopK和Ordered两种稀疏自编码器分析自回归抗体语言模型p-IgGen，比较它们在特征识别和生成操控方面的表现。

Result: TopK SAEs能揭示有生物学意义的潜在特征，但高特征概念相关性不能保证对生成的因果控制；Ordered SAEs通过分层结构能可靠识别可操控特征，但激活模式更复杂且可解释性降低。

Conclusion: TopK SAEs适用于将潜在特征映射到概念，而Ordered SAEs在需要精确生成操控时更优，这推进了领域特定蛋白质语言模型的机制可解释性研究。

Abstract: Sparse autoencoders (SAEs) are a mechanistic interpretability technique that have been used to provide insight into learned concepts within large protein language models. Here, we employ TopK and Ordered SAEs to investigate an autoregressive antibody language model, p-IgGen, and steer its generation. We show that TopK SAEs can reveal biologically meaningful latent features, but high feature concept correlation does not guarantee causal control over generation. In contrast, Ordered SAEs impose an hierarchical structure that reliably identifies steerable features, but at the expense of more complex and less interpretable activation patterns. These findings advance the mechanistic interpretability of domain-specific protein language models and suggest that, while TopK SAEs are sufficient for mapping latent features to concepts, Ordered SAEs are preferable when precise generative steering is required.

</details>


### [56] [Approximation of Box Decomposition Algorithm for Fast Hypervolume-Based Multi-Objective Optimization](https://arxiv.org/abs/2512.05825)
*Shuhei Watanabe*

Main category: cs.LG

TL;DR: 本文填补了多目标贝叶斯优化中HV近似算法文献空白，提供了完整的数学和算法描述


<details>
  <summary>Details</summary>
Motivation: 超体积（HV）贝叶斯优化中，获取函数的优化计算成本高，主要源于HV改进计算的昂贵开销。虽然HV盒分解能应对频繁的精确改进计算，但其在最坏情况下具有超多项式内存复杂度。现有近似算法缺乏严格的算法描述。

Method: 提供Couckuyt等人（2012）提出的近似算法的全面数学和算法细节，填补文献中该算法缺乏严格描述的空白

Result: 本文提供了该近似算法的完整数学推导和算法实现细节，为多目标贝叶斯优化领域提供了重要的算法资源

Conclusion: 通过提供HV近似算法的全面描述，本文解决了多目标贝叶斯优化中计算效率的关键问题，填补了文献空白，为后续研究和应用提供了基础

Abstract: Hypervolume (HV)-based Bayesian optimization (BO) is one of the standard approaches for multi-objective decision-making. However, the computational cost of optimizing the acquisition function remains a significant bottleneck, primarily due to the expense of HV improvement calculations. While HV box-decomposition offers an efficient way to cope with the frequent exact improvement calculations, it suffers from super-polynomial memory complexity $O(MN^{\lfloor \frac{M + 1}{2} \rfloor})$ in the worst case as proposed by Lacour et al. (2017). To tackle this problem, Couckuyt et al. (2012) employed an approximation algorithm. However, a rigorous algorithmic description is currently absent from the literature. This paper bridges this gap by providing comprehensive mathematical and algorithmic details of this approximation algorithm.

</details>


### [57] [Beyond Data Filtering: Knowledge Localization for Capability Removal in LLMs](https://arxiv.org/abs/2512.05648)
*Igor Shilov,Alex Cloud,Aryo Pradipta Gema,Jacob Goldman-Wetzler,Nina Panickssery,Henry Sleight,Erik Jones,Cem Anil*

Main category: cs.LG

TL;DR: SGTM（选择性梯度掩码）是一种改进的梯度路由技术，通过在预训练时选择性屏蔽梯度，将目标知识定位到专用参数子集中，从而在存在标签噪声的情况下更有效地移除有害内容。


<details>
  <summary>Details</summary>
Motivation: 大语言模型具有双重用途风险，传统的数据过滤方法面临两个主要挑战：大规模标注有害数据成本高昂，以及随着模型规模增大样本效率提高，即使少量错误标注内容也可能导致危险能力。需要一种在标签噪声不可避免的情况下仍能有效移除有害知识的预训练时缓解方法。

Method: 提出选择性梯度掩码（SGTM），这是梯度路由技术的改进变体。该方法对选定梯度进行零掩码，使得目标领域示例仅更新其专用参数，从而将目标知识定位到可后续移除的参数子集中。

Result: 在两个应用中验证SGTM有效性：1）从双语合成数据集训练的模型中移除一种语言知识；2）从英文维基百科训练的模型中移除生物学知识。SGTM在存在标签错误的情况下，相比数据过滤和先前梯度路由实现，提供了更好的保留/遗忘权衡。SGTM对对抗性微调表现出强鲁棒性，需要比基于微调的遗忘方法（RMU）多7倍的微调步骤才能达到遗忘集上的基线性能。

Conclusion: SGTM为现有安全缓解措施提供了一个有前景的预训练时补充，特别是在标签噪声不可避免的场景中。相比浅层遗忘方法，SGTM对对抗性微调具有更强的鲁棒性。

Abstract: Large Language Models increasingly possess capabilities that carry dual-use risks. While data filtering has emerged as a pretraining-time mitigation, it faces significant challenges: labeling whether data is harmful is expensive at scale, and given improving sample efficiency with larger models, even small amounts of mislabeled content could give rise to dangerous capabilities. To address risks associated with mislabeled harmful content, prior work proposed Gradient Routing (Cloud et al., 2024) -- a technique that localizes target knowledge into a dedicated subset of model parameters so they can later be removed. We explore an improved variant of Gradient Routing, which we call Selective GradienT Masking (SGTM), with particular focus on evaluating its robustness to label noise. SGTM zero-masks selected gradients such that target domain examples only update their dedicated parameters. We test SGTM's effectiveness in two applications: removing knowledge of one language from a model trained on a bilingual synthetic dataset, and removing biology knowledge from a model trained on English Wikipedia. In both cases SGTM provides better retain/forget trade-off in the presence of labeling errors compared to both data filtering and a previously proposed instantiation of Gradient Routing. Unlike shallow unlearning approaches that can be quickly undone through fine-tuning, SGTM exhibits strong robustness to adversarial fine-tuning, requiring seven times more fine-tuning steps to reach baseline performance on the forget set compared to a finetuning-based unlearning method (RMU). Our results suggest SGTM provides a promising pretraining-time complement to existing safety mitigations, particularly in settings where label noise is unavoidable.

</details>


### [58] [NEAT: Neighborhood-Guided, Efficient, Autoregressive Set Transformer for 3D Molecular Generation](https://arxiv.org/abs/2512.05844)
*Daniel Rose,Roxane Axel Jacob,Johannes Kirchmair,Thierry Langer*

Main category: cs.LG

TL;DR: NEAT是一种基于邻域引导的高效自回归集合变换器，将分子图视为原子集合，通过自回归流模型学习图边界上可接受标记的顺序无关分布，实现了3D分子生成的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 自回归模型是3D分子结构生成中扩散模型的有前景替代方案，但传统方法假设标记顺序存在，而分子图中的下一个标记预测应该对原子排列不变。先前工作通过使用规范顺序或焦点原子来回避这种不匹配，作者认为这是不必要的。

Method: NEAT（邻域引导、高效、自回归、集合变换器）将分子图视为原子集合，使用自回归流模型学习图边界上可接受标记的顺序无关分布，实现了原子级排列不变性。

Result: NEAT在3D分子生成方面达到了最先进的性能，具有高计算效率和原子级排列不变性，为可扩展的分子设计建立了实用基础。

Conclusion: NEAT通过将分子图视为集合并学习顺序无关分布，解决了自回归模型中顺序假设与分子图排列不变性之间的不匹配问题，为高效、可扩展的3D分子生成提供了实用解决方案。

Abstract: Autoregressive models are a promising alternative to diffusion-based models for 3D molecular structure generation. However, a key limitation is the assumption of a token order: while text has a natural sequential order, the next token prediction given a molecular graph prefix should be invariant to atom permutations. Previous works sidestepped this mismatch by using canonical orders or focus atoms. We argue that this is unnecessary. We introduce NEAT, a Neighborhood-guided, Efficient, Autoregressive, Set Transformer that treats molecular graphs as sets of atoms and learns the order-agnostic distribution over admissible tokens at the graph boundary with an autoregressive flow model. NEAT approaches state-of-the-art performance in 3D molecular generation with high computational efficiency and atom-level permutation invariance, establishing a practical foundation for scalable molecular design.

</details>


### [59] [Sparse Attention Post-Training for Mechanistic Interpretability](https://arxiv.org/abs/2512.05865)
*Florent Draye,Anson Lei,Ingmar Posner,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: 提出一种简单的后训练方法，可使Transformer注意力变得稀疏而不损失性能，将注意力连接减少到约0.3%，同时保持原始预训练损失


<details>
  <summary>Details</summary>
Motivation: 现有稀疏注意力方法主要关注计算效率，而本文利用稀疏性作为结构先验，旨在揭示更有序、可解释的连接模式，简化全局电路结构

Method: 采用简单的后训练方法，在约束损失目标下应用灵活的稀疏正则化，使Transformer注意力稀疏化

Result: 在高达10亿参数的模型上，保持原始预训练损失的同时将注意力连接减少到约0.3%，任务特定电路涉及的组件（注意力头和MLP）大幅减少，连接边减少达100倍

Conclusion: Transformer注意力可以变得稀疏数个数量级，表明其大部分计算是冗余的，稀疏性可作为构建更结构化、可解释模型的指导原则

Abstract: We introduce a simple post-training method that makes transformer attention sparse without sacrificing performance. Applying a flexible sparsity regularisation under a constrained-loss objective, we show on models up to 1B parameters that it is possible to retain the original pretraining loss while reducing attention connectivity to $\approx 0.3 \%$ of its edges. Unlike sparse-attention methods designed for computational efficiency, our approach leverages sparsity as a structural prior: it preserves capability while exposing a more organized and interpretable connectivity pattern. We find that this local sparsity cascades into global circuit simplification: task-specific circuits involve far fewer components (attention heads and MLPs) with up to 100x fewer edges connecting them. These results demonstrate that transformer attention can be made orders of magnitude sparser, suggesting that much of its computation is redundant and that sparsity may serve as a guiding principle for more structured and interpretable models.

</details>


### [60] [Neural Coherence : Find higher performance to out-of-distribution tasks from few samples](https://arxiv.org/abs/2512.05880)
*Simon Guiroy,Mats Richter,Sarath Chandar,Christopher Pal*

Main category: cs.LG

TL;DR: 提出基于神经一致性的模型选择方法，仅需少量无标注目标域数据即可在分布外场景下有效选择预训练模型检查点


<details>
  <summary>Details</summary>
Motivation: 当前微调预训练大视觉模型时，如何从大量训练检查点中选择最佳起点仍是一个开放问题，特别是在目标域数据稀缺、无标注且分布外的情况下，传统基于验证集的方法不可靠或不适用

Method: 提出神经一致性概念，通过表征模型在源域和目标域的激活统计特性，设计高数据效率的模型选择方法，仅需少量无标注目标域示例

Result: 在ImageNet1K预训练模型上，针对Food-101、PlantNet-300K和iNaturalist等目标域进行实验，同时在元学习设置中评估，相比基线方法显著提升了跨域泛化性能

Conclusion: 神经一致性是一个强大的原则，不仅可用于模型选择，还展示了在训练数据选择中的有效性，为解决分布外场景下的模型选择问题提供了新思路

Abstract: To create state-of-the-art models for many downstream tasks, it has become common practice to fine-tune a pre-trained large vision model. However, it remains an open question of how to best determine which of the many possible model checkpoints resulting from a large training run to use as the starting point. This becomes especially important when data for the target task of interest is scarce, unlabeled and out-of-distribution. In such scenarios, common methods relying on in-distribution validation data become unreliable or inapplicable. This work proposes a novel approach for model selection that operates reliably on just a few unlabeled examples from the target task. Our approach is based on a novel concept: Neural Coherence, which entails characterizing a model's activation statistics for source and target domains, allowing one to define model selection methods with high data-efficiency. We provide experiments where models are pre-trained on ImageNet1K and examine target domains consisting of Food-101, PlantNet-300K and iNaturalist. We also evaluate it in many meta-learning settings. Our approach significantly improves generalization across these different target domains compared to established baselines. We further demonstrate the versatility of Neural Coherence as a powerful principle by showing its effectiveness in training data selection.

</details>


### [61] [MaxShapley: Towards Incentive-compatible Generative Search with Fair Context Attribution](https://arxiv.org/abs/2512.05958)
*Sara Patel,Mingxun Zhou,Giulia Fanti*

Main category: cs.LG

TL;DR: MaxShapley是一种用于生成式搜索引擎的高效公平归因算法，基于可分解的最大和效用函数，将Shapley值计算复杂度从指数级降低到线性级


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的生成式搜索引擎取代传统搜索，信息提供者的补偿方式发生根本变化。需要公平机制来根据内容提供者对生成答案的贡献进行归因和补偿，以维持生态系统

Method: 提出MaxShapley算法，这是Shapley值的一个特例，利用可分解的最大和效用函数来计算文档归因。该方法将计算复杂度从指数级降低到线性级，特别适用于检索增强生成（RAG）管道

Result: 在三个多跳问答数据集（HotPotQA、MuSiQUE、MS MARCO）上评估，MaxShapley在保持与精确Shapley计算相当的归因质量的同时，显著减少资源消耗。例如，在相同归因准确度下，比先前最先进方法减少高达8倍的资源消耗

Conclusion: MaxShapley为生成式搜索引擎提供了一种高效、公平的内容归因机制，能够以线性计算成本实现高质量的归因，有助于建立可持续的生成式搜索生态系统

Abstract: Generative search engines based on large language models (LLMs) are replacing traditional search, fundamentally changing how information providers are compensated. To sustain this ecosystem, we need fair mechanisms to attribute and compensate content providers based on their contributions to generated answers. We introduce MaxShapley, an efficient algorithm for fair attribution in generative search pipelines that use retrieval-augmented generation (RAG). MaxShapley is a special case of the celebrated Shapley value; it leverages a decomposable max-sum utility function to compute attributions with linear computation in the number of documents, as opposed to the exponential cost of Shapley values. We evaluate MaxShapley on three multi-hop QA datasets (HotPotQA, MuSiQUE, MS MARCO); MaxShapley achieves comparable attribution quality to exact Shapley computation, while consuming a fraction of its tokens--for instance, it gives up to an 8x reduction in resource consumption over prior state-of-the-art methods at the same attribution accuracy.

</details>


### [62] [Whatever Remains Must Be True: Filtering Drives Reasoning in LLMs, Shaping Diversity](https://arxiv.org/abs/2512.05962)
*Germán Kruszewski,Pierre Erbacher,Jos Rozen,Marc Dymetman*

Main category: cs.LG

TL;DR: 该论文提出使用α-散度来优化LLM的推理能力，解决传统强化学习导致的多样性损失问题，在Lean定理证明基准上实现了覆盖率和精度的帕累托最优。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在调优LLM进行推理任务时会导致显著的多样性损失，这是因为RL隐式优化了"模式寻求"的反向KL散度，使模型集中在目标分布的高概率区域而忽略其他区域。

Method: 从显式目标分布出发（通过过滤错误答案同时保留正确答案的相对概率），使用α-散度族来近似这个目标分布，α-散度统一了先前方法并允许通过插值在模式寻求和质量覆盖散度之间直接控制精度-多样性权衡。

Result: 在Lean定理证明基准上，该方法在覆盖率-精度帕累托前沿上实现了最先进的性能，在覆盖率轴上优于所有先前方法。

Conclusion: 通过α-散度方法可以更好地控制LLM推理任务中的精度与多样性平衡，解决了传统RL方法导致的多样性损失问题，在定理证明等需要多样推理路径的任务中表现优异。

Abstract: Reinforcement Learning (RL) has become the de facto standard for tuning LLMs to solve tasks involving reasoning. However, growing evidence shows that models trained in such way often suffer from a significant loss in diversity. We argue that this arises because RL implicitly optimizes the "mode-seeking" or "zero-forcing" Reverse KL to a target distribution causing the model to concentrate mass on certain high-probability regions of the target while neglecting others. In this work, we instead begin from an explicit target distribution, obtained by filtering out incorrect answers while preserving the relative probabilities of correct ones. Starting from a pre-trained LLM, we approximate this target distribution using the $α$-divergence family, which unifies prior approaches and enables direct control of the precision-diversity trade-off by interpolating between mode-seeking and mass-covering divergences. On a Lean theorem-proving benchmark, our method achieves state-of-the-art performance along the coverage-precision Pareto frontier, outperforming all prior methods on the coverage axis.

</details>


### [63] [Utility Boundary of Dataset Distillation: Scaling and Configuration-Coverage Laws](https://arxiv.org/abs/2512.05817)
*Zhengquan Luo,Zhiqiang Xu*

Main category: cs.LG

TL;DR: 论文提出了一个统一的理论框架来分析数据集蒸馏方法，揭示了性能随蒸馏样本数量增加的缩放规律，以及所需样本量与配置多样性之间的线性关系。


<details>
  <summary>Details</summary>
Motivation: 数据集蒸馏方法虽然取得了快速进展，但缺乏统一的理论基础。现有方法基于不同的替代目标和优化假设，难以分析其共同原理或提供通用保证。同时，不清楚在训练配置（如优化器、架构或数据增强）变化时，蒸馏数据如何保持有效性。

Method: 提出了配置-动态-误差分析统一理论框架，将主要数据集蒸馏方法重新表述在共同的泛化误差视角下。该框架提供了两个主要结果：缩放定律和覆盖定律。

Result: 缩放定律提供了单一配置的上界，描述了误差如何随蒸馏样本数量增加而减小，解释了常见的性能饱和效应。覆盖定律表明所需蒸馏样本数量与配置多样性呈线性比例关系，具有可证明匹配的上界和下界。

Conclusion: 统一分析揭示了各种匹配方法是可互换的替代方法，都能减少相同的泛化误差。这解释了为什么它们都能实现数据集蒸馏，并为替代选择如何影响样本效率和鲁棒性提供了指导。实验证实了推导出的定律，为数据集蒸馏奠定了理论基础，并支持理论驱动的紧凑、配置鲁棒的数据集蒸馏设计。

Abstract: Dataset distillation (DD) aims to construct compact synthetic datasets that allow models to achieve comparable performance to full-data training while substantially reducing storage and computation. Despite rapid empirical progress, its theoretical foundations remain limited: existing methods (gradient, distribution, trajectory matching) are built on heterogeneous surrogate objectives and optimization assumptions, which makes it difficult to analyze their common principles or provide general guarantees. Moreover, it is still unclear under what conditions distilled data can retain the effectiveness of full datasets when the training configuration, such as optimizer, architecture, or augmentation, changes. To answer these questions, we propose a unified theoretical framework, termed configuration--dynamics--error analysis, which reformulates major DD approaches under a common generalization-error perspective and provides two main results: (i) a scaling law that provides a single-configuration upper bound, characterizing how the error decreases as the distilled sample size increases and explaining the commonly observed performance saturation effect; and (ii) a coverage law showing that the required distilled sample size scales linearly with configuration diversity, with provably matching upper and lower bounds. In addition, our unified analysis reveals that various matching methods are interchangeable surrogates, reducing the same generalization error, clarifying why they can all achieve dataset distillation and providing guidance on how surrogate choices affect sample efficiency and robustness. Experiments across diverse methods and configurations empirically confirm the derived laws, advancing a theoretical foundation for DD and enabling theory-driven design of compact, configuration-robust dataset distillation.

</details>


### [64] [Computational Design of Low-Volatility Lubricants for Space Using Interpretable Machine Learning](https://arxiv.org/abs/2512.05870)
*Daniel Miliate,Ashlie Martini*

Main category: cs.LG

TL;DR: 利用机器学习预测蒸汽压，筛选适用于太空机械组件的液体润滑剂


<details>
  <summary>Details</summary>
Motivation: 太空机械组件需要低蒸汽压的液体润滑剂，但现有选择有限且各有局限性，限制了机械设计

Method: 采用数据驱动的机器学习方法，结合分子动力学模拟和实验数据库数据训练模型，注重模型可解释性

Result: 建立了能够预测蒸汽压的机器学习模型，识别了化学结构与蒸汽压之间的关系，并提出了多个有潜力的候选分子

Conclusion: 机器学习方法能够有效筛选适用于太空机械组件的新型液体润滑剂，为太空润滑剂开发提供了新途径

Abstract: The function and lifetime of moving mechanical assemblies (MMAs) in space depend on the properties of lubricants. MMAs that experience high speeds or high cycles require liquid based lubricants due to their ability to reflow to the point of contact. However, only a few liquid-based lubricants have vapor pressures low enough for the vacuum conditions of space, each of which has limitations that add constraints to MMA designs. This work introduces a data-driven machine learning (ML) approach to predicting vapor pressure, enabling virtual screening and discovery of new space-suitable liquid lubricants. The ML models are trained with data from both high-throughput molecular dynamics simulations and experimental databases. The models are designed to prioritize interpretability, enabling the relationships between chemical structure and vapor pressure to be identified. Based on these insights, several candidate molecules are proposed that may have promise for future space lubricant applications in MMAs.

</details>


### [65] [KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity](https://arxiv.org/abs/2512.05916)
*Damien Lesens,Beheshteh T. Rakhshan,Guillaume Rabusseau*

Main category: cs.LG

TL;DR: KQ-SVD：一种直接对注意力矩阵进行最优低秩分解的新方法，通过闭式解解决KV缓存压缩问题，相比现有方法能更准确地保持注意力输出


<details>
  <summary>Details</summary>
Motivation: 随着序列长度和批处理大小的增长，Transformer大语言模型中的键值缓存成为主要内存瓶颈。现有压缩方法通常只对键进行低秩分解或尝试联合嵌入查询和键，但这些方法忽略了注意力本质上依赖于它们的内积

Method: 提出KQ-SVD方法，直接对注意力矩阵进行最优低秩分解，通过闭式解实现。该方法针对冗余的真正来源，在压缩下能更高保真度地保持注意力输出

Result: 在LLaMA和Mistral模型上的广泛评估表明，该方法在投影质量方面始终优于现有方法

Conclusion: KQ-SVD是一种简单且计算高效的方法，通过直接分解注意力矩阵来解决KV缓存压缩问题，相比现有策略能提供更优的注意力近似

Abstract: The Key-Value (KV) cache is central to the efficiency of transformer-based large language models (LLMs), storing previously computed vectors to accelerate inference. Yet, as sequence length and batch size grow, the cache becomes a major memory bottleneck. Prior compression methods typically apply low-rank decomposition to keys alone or attempt to jointly embed queries and keys, but both approaches neglect that attention fundamentally depends on their inner products. In this work, we prove that such strategies are suboptimal for approximating the attention matrix. We introduce KQ-SVD, a simple and computationally efficient method that directly performs an optimal low-rank decomposition of the attention matrix via a closed-form solution. By targeting the true source of redundancy, KQ-SVD preserves attention outputs with higher fidelity under compression. Extensive evaluations on LLaMA and Mistral models demonstrate that our approach consistently delivers superior projection quality.

</details>


### [66] [Developing synthetic microdata through machine learning for firm-level business surveys](https://arxiv.org/abs/2512.05948)
*Jorge Cisneros Paz,Timothy Wojan,Matthew Williams,Jennifer Ozawa,Robert Chew,Kimberly Janda,Timothy Navarro,Michael Floyd,Christine Task,Damon Streat*

Main category: cs.LG

TL;DR: 该论文探讨了使用机器学习模型为美国年度商业调查创建合成公共使用微数据样本，以解决商业数据匿名化挑战，并通过实证研究验证了合成数据的真实性。


<details>
  <summary>Details</summary>
Motivation: 随着计算能力提升和大数据可用性增加，传统匿名化方法面临重新识别风险，可能违反调查受访者的保密承诺。商业数据尤其具有挑战性，因为企业缺乏匿名性且某些行业在特定地理区域容易被识别。

Method: 使用机器学习模型构建基于年度商业调查的合成公共使用微数据样本，并开发了多种质量评估指标。同时为2007年企业主调查创建了两个合成数据集作为验证案例。

Result: 通过复制《小企业经济学》上发表的高影响力分析进行计量经济学验证，证明合成数据与真实数据具有高度相似性。虽然ABS PUMS仍在完善中且结果保密，但验证案例展示了合成数据的实用性。

Conclusion: 合成数据方法能够保护调查受访者隐私同时保留关键统计特征，为商业调查数据的公共使用提供了可行解决方案，并讨论了ABS合成数据的潜在应用场景。

Abstract: Public-use microdata samples (PUMS) from the United States (US) Census Bureau on individuals have been available for decades. However, large increases in computing power and the greater availability of Big Data have dramatically increased the probability of re-identifying anonymized data, potentially violating the pledge of confidentiality given to survey respondents. Data science tools can be used to produce synthetic data that preserve critical moments of the empirical data but do not contain the records of any existing individual respondent or business. Developing public-use firm data from surveys presents unique challenges different from demographic data, because there is a lack of anonymity and certain industries can be easily identified in each geographic area. This paper briefly describes a machine learning model used to construct a synthetic PUMS based on the Annual Business Survey (ABS) and discusses various quality metrics. Although the ABS PUMS is currently being refined and results are confidential, we present two synthetic PUMS developed for the 2007 Survey of Business Owners, similar to the ABS business data. Econometric replication of a high impact analysis published in Small Business Economics demonstrates the verisimilitude of the synthetic data to the true data and motivates discussion of possible ABS use cases.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [67] [Semantic Faithfulness and Entropy Production Measures to Tame Your LLM Demons and Manage Hallucinations](https://arxiv.org/abs/2512.05156)
*Igor Halperin*

Main category: cs.AI

TL;DR: 该论文提出了两种基于信息论和热力学的无监督度量方法，用于评估大型语言模型在给定任务中的忠实度。方法将LLM视为二分信息引擎，通过主题转换矩阵的KL散度量化语义忠实度，并引入热力学语义熵产生度量，两者可用于LLM评估和幻觉控制。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型对给定任务的忠实度是一个复杂挑战，需要有效的无监督度量方法来量化模型输出的可靠性，特别是检测和控制幻觉问题。

Method: 将LLM视为二分信息引擎，隐藏层作为麦克斯韦妖控制上下文C通过提示Q转换为答案A。将QCA三元组建模为共享主题上的概率分布，主题转换通过转移矩阵Q和A表示查询目标和实际结果。通过凸优化同时推断这两个矩阵，计算KL散度作为语义忠实度度量，并映射到[0,1]区间。此外提出基于热力学的语义熵产生度量。

Result: 提出的语义忠实度(SF)和语义熵产生(SEP)度量可用于联合或单独评估LLM。在LLM总结公司SEC 10-K文件的实验中展示了该框架的有效性，表明高忠实度通常对应低熵产生。

Conclusion: 该研究提供了基于信息论和热力学的无监督度量框架，能够有效评估LLM的忠实度并控制幻觉，为LLM可靠性评估提供了新方法。

Abstract: Evaluating faithfulness of Large Language Models (LLMs) to a given task is a complex challenge. We propose two new unsupervised metrics for faithfulness evaluation using insights from information theory and thermodynamics. Our approach treats an LLM as a bipartite information engine where hidden layers act as a Maxwell demon controlling transformations of context $C $ into answer $A$ via prompt $Q$. We model Question-Context-Answer (QCA) triplets as probability distributions over shared topics. Topic transformations from $C$ to $Q$ and $A$ are modeled as transition matrices ${\bf Q}$ and ${\bf A}$ encoding the query goal and actual result, respectively. Our semantic faithfulness (SF) metric quantifies faithfulness for any given QCA triplet by the Kullback-Leibler (KL) divergence between these matrices. Both matrices are inferred simultaneously via convex optimization of this KL divergence, and the final SF metric is obtained by mapping the minimal divergence onto the unit interval [0,1], where higher scores indicate greater faithfulness. Furthermore, we propose a thermodynamics-based semantic entropy production (SEP) metric in answer generation, and show that high faithfulness generally implies low entropy production. The SF and SEP metrics can be used jointly or separately for LLM evaluation and hallucination control. We demonstrate our framework on LLM summarization of corporate SEC 10-K filings.

</details>


### [68] [Bridging Traditional Machine Learning and Large Language Models: A Two-Part Course Design for Modern AI Education](https://arxiv.org/abs/2512.05167)
*Fang Li*

Main category: cs.AI

TL;DR: 提出一种创新的AI与数据科学教学方法，系统性地将传统机器学习技术与现代大语言模型相结合，通过两部分课程设计帮助学生全面理解AI发展并掌握实用技能。


<details>
  <summary>Details</summary>
Motivation: 为了帮助学生全面理解人工智能的发展历程，同时掌握传统机器学习和现代大语言模型技术，更好地适应快速发展的AI行业需求。

Method: 采用两部分课程设计：第一部分教授基础机器学习概念，第二部分专注于当代大语言模型应用。课程包括架构设计、实施策略、评估方法，并在两个七周的夏季学期中实施。

Result: 这种整合方法增强了学生对AI领域的理解，更好地为他们应对行业需求做好准备，从夏季课程的实施中获得了积极的学习成果。

Conclusion: 这种将传统机器学习与现代LLM相结合的教学方法能够有效提升学生对AI领域的全面理解，为他们应对快速发展的AI行业需求做好充分准备。

Abstract: This paper presents an innovative pedagogical approach for teaching artificial intelligence and data science that systematically bridges traditional machine learning techniques with modern Large Language Models (LLMs). We describe a course structured in two sequential and complementary parts: foundational machine learning concepts and contemporary LLM applications. This design enables students to develop a comprehensive understanding of AI evolution while building practical skills with both established and cutting-edge technologies. We detail the course architecture, implementation strategies, assessment methods, and learning outcomes from our summer course delivery spanning two seven-week terms. Our findings demonstrate that this integrated approach enhances student comprehension of the AI landscape and better prepares them for industry demands in the rapidly evolving field of artificial intelligence.

</details>


### [69] [On the Computability of Artificial General Intelligence](https://arxiv.org/abs/2512.05212)
*Georgios Mappouras,Charalambos Rossides*

Main category: cs.AI

TL;DR: 该论文通过形式化证明指出，任何算法（包括AI模型）都无法创造出初始算法本身不具备的新功能能力，因此无法实现真正意义上的创造性突破。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能快速发展，人们开始探讨何时能实现人工通用智能（AGI）。本文旨在从计算理论的角度，探讨任何机器可计算过程（算法）的能力上限，特别是关于创造性的本质。

Method: 采用先前研究中关于AGI的定义（即在某个领域展现创造性并解锁新的未知功能能力），然后通过形式化证明方法，论证任何算法都无法产生超出其初始设计的新功能能力。

Result: 形式化证明表明：1）没有算法能够展示初始算法本身不具备的新功能能力；2）AI模型只能展示现有功能能力及其组合与排列；3）因此，任何算法（包括AI）都无法在科学、工程、艺术等领域实现真正的创造性突破。

Conclusion: 该证明对AI发展的未来具有重要意义，表明AI无法实现真正的创造性突破，只能基于现有能力进行组合。同时，这一结论也引发了关于人类智能起源本质的深刻思考。

Abstract: In recent years we observed rapid and significant advancements in artificial intelligence (A.I.). So much so that many wonder how close humanity is to developing an A.I. model that can achieve human level of intelligence, also known as artificial general intelligence (A.G.I.). In this work we look at this question and we attempt to define the upper bounds, not just of A.I., but rather of any machine-computable process (a.k.a. an algorithm). To answer this question however, one must first precisely define A.G.I. We borrow prior work's definition of A.G.I. [1] that best describes the sentiment of the term, as used by the leading developers of A.I. That is, the ability to be creative and innovate in some field of study in a way that unlocks new and previously unknown functional capabilities in that field. Based on this definition we draw new bounds on the limits of computation. We formally prove that no algorithm can demonstrate new functional capabilities that were not already present in the initial algorithm itself. Therefore, no algorithm (and thus no A.I. model) can be truly creative in any field of study, whether that is science, engineering, art, sports, etc. In contrast, A.I. models can demonstrate existing functional capabilities, as well as combinations and permutations of existing functional capabilities. We conclude this work by discussing the implications of this proof both as it regards to the future of A.I. development, as well as to what it means for the origins of human intelligence.

</details>


### [70] [Resolving Zadehs Paradox Axiomatic Possibility Theory as a Foundation for Reliable Artificial Intelligence](https://arxiv.org/abs/2512.05257)
*Bychkov Oleksii,Bychkova Sophia,Lytvynchuk Khrystyna*

Main category: cs.AI

TL;DR: 本文论证可能性理论是解决Dempster-Shafer理论悖论的根本方案，通过可能性与必要性测度的二元框架建立逻辑一致的不确定性处理基础，避免DST的逻辑陷阱。


<details>
  <summary>Details</summary>
Motivation: 针对Dempster-Shafer理论（DST）在处理不确定性时出现的悖论和逻辑不一致问题，作者认为现有许多修复Dempster规则的尝试不够根本，需要从基础理论层面解决DST的危机。

Method: 采用Bychkov文章中发展的公理化方法，基于可能性与必要性测度的二元框架，从零开始构建逻辑一致且数学严谨的不确定性处理基础。通过比较分析概率论、证据理论和可能性理论三种范式，并以经典医疗诊断困境为例进行验证。

Result: 可能性理论能够正确处理矛盾数据，避免DST的逻辑陷阱，使形式推理更接近自然智能的逻辑。研究表明可能性理论不仅是DST的替代方案，更是解决其悖论的根本方案。

Conclusion: 可能性理论为解决Dempster-Shafer理论的危机提供了根本性解决方案，其公理化方法和二元测度框架能够建立逻辑一致的不确定性处理基础，使形式推理更符合人类自然智能的逻辑。

Abstract: This work advances and substantiates the thesis that the resolution of this crisis lies in the domain of possibility theory, specifically in the axiomatic approach developed in Bychkovs article. Unlike numerous attempts to fix Dempster rule, this approach builds from scratch a logically consistent and mathematically rigorous foundation for working with uncertainty, using the dualistic apparatus of possibility and necessity measures. The aim of this work is to demonstrate that possibility theory is not merely an alternative, but provides a fundamental resolution to DST paradoxes. A comparative analysis of three paradigms will be conducted probabilistic, evidential, and possibilistic. Using a classic medical diagnostic dilemma as an example, it will be shown how possibility theory allows for correct processing of contradictory data, avoiding the logical traps of DST and bringing formal reasoning closer to the logic of natural intelligence.

</details>


### [71] [AI & Human Co-Improvement for Safer Co-Superintelligence](https://arxiv.org/abs/2512.05356)
*Jason Weston,Jakob Foerster*

Main category: cs.AI

TL;DR: 论文主张将AI研究目标从"自我改进"转向"协同改进"，即人类研究者与AI系统合作进行AI研究，共同实现超级智能


<details>
  <summary>Details</summary>
Motivation: 当前AI领域的自我改进目标存在危险且难以完全实现，需要更可行、更安全的研究方向。作者认为应该关注人类与AI的协作改进，通过共生关系实现更安全的超级智能

Method: 提出"协同改进"框架，专注于提升AI系统与人类研究者合作进行AI研究的能力，涵盖从构思到实验的完整研究流程，将人类研究改进纳入循环

Result: 协同改进既能加速AI研究进展，又能通过人类与AI的共生关系赋予双方更安全的超级智能能力

Conclusion: 关注人类与AI的协同改进比单纯追求AI自我改进更可行、更安全，能够更快实现超级智能目标，同时确保安全性

Abstract: Self-improvement is a goal currently exciting the field of AI, but is fraught with danger, and may take time to fully achieve. We advocate that a more achievable and better goal for humanity is to maximize co-improvement: collaboration between human researchers and AIs to achieve co-superintelligence. That is, specifically targeting improving AI systems' ability to work with human researchers to conduct AI research together, from ideation to experimentation, in order to both accelerate AI research and to generally endow both AIs and humans with safer superintelligence through their symbiosis. Focusing on including human research improvement in the loop will both get us there faster, and more safely.

</details>


### [72] [ChipMind: Retrieval-Augmented Reasoning for Long-Context Circuit Design Specifications](https://arxiv.org/abs/2512.05371)
*Changwen Xing,SamZaak Wong,Xinlai Wan,Yanfeng Lu,Mengli Zhang,Zebin Ma,Lei Qi,Zhengxiong Li,Nan Guan,Zhe Jiang,Xi Wang,Jun Yang*

Main category: cs.AI

TL;DR: ChipMind是一个基于知识图谱增强的推理框架，专门用于处理冗长的集成电路规格文档，通过将电路规格转换为领域知识图谱并结合自适应检索机制，显著提升了LLM在硬件设计中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在集成电路开发自动化方面具有巨大潜力，但实际部署受到上下文窗口限制的制约。现有的上下文扩展方法难以对复杂冗长的电路规格进行有效的语义建模和多跳推理。

Method: 提出ChipMind框架：1）通过电路语义感知知识图谱构建方法将电路规格转换为领域特定知识图谱ChipKG；2）采用ChipKG增强推理机制，结合信息论自适应检索动态追踪逻辑依赖关系，以及意图感知语义过滤去除无关噪声。

Result: 在工业级规格推理基准测试中，ChipMind显著优于现有最先进基线方法，平均提升34.59%（最高达72.73%）。

Conclusion: ChipMind框架填补了LLM辅助硬件设计在学术研究和实际工业部署之间的关键空白，为集成电路开发自动化提供了有效的解决方案。

Abstract: While Large Language Models (LLMs) demonstrate immense potential for automating integrated circuit (IC) development, their practical deployment is fundamentally limited by restricted context windows. Existing context-extension methods struggle to achieve effective semantic modeling and thorough multi-hop reasoning over extensive, intricate circuit specifications. To address this, we introduce ChipMind, a novel knowledge graph-augmented reasoning framework specifically designed for lengthy IC specifications. ChipMind first transforms circuit specifications into a domain-specific knowledge graph ChipKG through the Circuit Semantic-Aware Knowledge Graph Construction methodology. It then leverages the ChipKG-Augmented Reasoning mechanism, combining information-theoretic adaptive retrieval to dynamically trace logical dependencies with intent-aware semantic filtering to prune irrelevant noise, effectively balancing retrieval completeness and precision. Evaluated on an industrial-scale specification reasoning benchmark, ChipMind significantly outperforms state-of-the-art baselines, achieving an average improvement of 34.59% (up to 72.73%). Our framework bridges a critical gap between academic research and practical industrial deployment of LLM-aided Hardware Design (LAD).

</details>


### [73] [BEAVER: An Efficient Deterministic LLM Verifier](https://arxiv.org/abs/2512.05439)
*Tarun Suresh,Nalin Wadhwa,Debangshu Banerjee,Gagandeep Singh*

Main category: cs.AI

TL;DR: BEAVER是一个为大型语言模型输出约束满足提供确定性概率边界的首个实用框架，相比采样方法提供可证明的保证，在相同计算预算下获得6-8倍更紧的概率边界。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型从研究原型转向生产系统，从业者需要可靠的方法来验证模型输出是否满足所需约束。基于采样的估计只能提供模型行为的直觉，但无法提供可靠的保证。

Method: BEAVER框架使用新颖的token trie和frontier数据结构，系统性地探索生成空间，为任何前缀封闭的语义约束维护可证明的边界。该方法形式化了验证问题，并证明了方法的可靠性。

Result: 在正确性验证、隐私验证和安全代码生成任务上，BEAVER在相同计算预算下实现了6-8倍更紧的概率边界，并识别出比基线方法多3-4倍的高风险实例。

Conclusion: BEAVER能够提供松散边界或经验评估无法实现的精确特征描述和风险评估，为LLM约束满足提供了首个实用的确定性概率边界计算框架。

Abstract: As large language models (LLMs) transition from research prototypes to production systems, practitioners often need reliable methods to verify that model outputs satisfy required constraints. While sampling-based estimates provide an intuition of model behavior, they offer no sound guarantees. We present BEAVER, the first practical framework for computing deterministic, sound probability bounds on LLM constraint satisfaction. Given any prefix-closed semantic constraint, BEAVER systematically explores the generation space using novel token trie and frontier data structures, maintaining provably sound bounds at every iteration. We formalize the verification problem, prove soundness of our approach, and evaluate BEAVER on correctness verification, privacy verification and secure code generation tasks across multiple state of the art LLMs. BEAVER achieves 6 to 8 times tighter probability bounds and identifies 3 to 4 times more high risk instances compared to baseline methods under identical computational budgets, enabling precise characterization and risk assessment that loose bounds or empirical evaluation cannot provide.

</details>


### [74] [MIND: Multi-rationale INtegrated Discriminative Reasoning Framework for Multi-modal Large Models](https://arxiv.org/abs/2512.05530)
*Chuang Yu,Jinmiao Zhao,Mingxuan Zhao,Yunpeng Liu,Xiujun Shu,Yuanhao Feng,Bo Wang,Xiangyu Yue*

Main category: cs.AI

TL;DR: MIND框架通过"理解-重新思考-纠正"的认知过程，将MLLMs从被动模仿推理转变为主动判别推理，在多个数据集上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在多理由语义建模、逻辑鲁棒性方面存在局限，在复杂场景中容易受到误导性解释的影响，需要提升其认知推理能力。

Method: 提出MIND推理框架，包含：1)RAD范式自动扩展数据集生成多样化理由；2)P2CL策略分两阶段增强多理由正学习和主动逻辑判别纠正；3)MCA优化策略解决多理由语义空间表示纠缠问题。

Result: 在涵盖科学、常识和数学场景的多个公共数据集上实现了最先进的性能表现。

Conclusion: MIND框架为推进MLLMs向更高层次认知智能发展提供了新视角，实现了从被动模仿推理到主动判别推理的范式演进。

Abstract: Recently, multimodal large language models (MLLMs) have been widely applied to reasoning tasks. However, they suffer from limited multi-rationale semantic modeling, insufficient logical robustness, and are susceptible to misleading interpretations in complex scenarios. Therefore, we propose a Multi-rationale INtegrated Discriminative (MIND) reasoning framework, which is designed to endow MLLMs with human-like cognitive abilities of "Understand -> Rethink -> Correct", and achieves a paradigm evolution from passive imitation-based reasoning to active discriminative reasoning. Specifically, we introduce a Rationale Augmentation and Discrimination (RAD) paradigm, which automatically and efficiently expands existing datasets by generating diverse rationales, providing a unified and extensible data foundation. Meanwhile, we design a Progressive Two-stage Correction Learning (P2CL) strategy. The first phase enhances multi-rationale positive learning, while the second phase enables active logic discrimination and correction. In addition, to mitigate representation entanglement in the multi-rationale semantic space, we propose a Multi-rationale Contrastive Alignment (MCA) optimization strategy, which achieves semantic aggregation of correct reasoning and boundary separation of incorrect reasoning. Extensive experiments demonstrate that the proposed MIND reasoning framework achieves state-of-the-art (SOTA) performance on multiple public datasets covering scientific, commonsense, and mathematical scenarios. It provides a new perspective for advancing MLLMs towards higher levels of cognitive intelligence. Our code is available at https://github.com/YuChuang1205/MIND

</details>


### [75] [CureAgent: A Training-Free Executor-Analyst Framework for Clinical Reasoning](https://arxiv.org/abs/2512.05576)
*Ting-Ting Xie,Yixin Zhang*

Main category: cs.AI

TL;DR: 本文提出Executor-Analyst框架解决临床智能体中的上下文利用失败问题，通过分离工具执行和临床推理，采用分层集成策略，在无需端到端微调的情况下实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于小型LLM的临床智能体（如TxAgent）存在"上下文利用失败"问题：模型能够成功检索生物医学证据，但无法基于这些信息进行诊断推理。需要解决这种推理缺陷。

Method: 提出Executor-Analyst框架：1）模块化架构，将工具执行的语法精度与临床推理的语义鲁棒性解耦；2）专用TxAgent（执行器）与长上下文基础模型（分析师）协同工作；3）分层集成策略，保留证据多样性，避免信息瓶颈。

Result: 1）在CURE-Bench上实现最先进性能；2）发现上下文-性能悖论：推理上下文超过12k token会引入噪声降低准确性；3）发现行动空间维度诅咒：工具集扩展需要分层检索策略；4）无需昂贵的端到端微调。

Conclusion: 通过免训练的架构工程方法，为下一代可信赖的AI驱动治疗提供了可扩展、敏捷的基础，证明了模块化设计在解决临床智能体推理缺陷方面的有效性。

Abstract: Current clinical agent built on small LLMs, such as TxAgent suffer from a \textit{Context Utilization Failure}, where models successfully retrieve biomedical evidence due to supervised finetuning but fail to ground their diagnosis in that information. In this work, we propose the Executor-Analyst Framework, a modular architecture that decouples the syntactic precision of tool execution from the semantic robustness of clinical reasoning. By orchestrating specialized TxAgents (Executors) with long-context foundation models (Analysts), we mitigate the reasoning deficits observed in monolithic models. Beyond simple modularity, we demonstrate that a Stratified Ensemble strategy significantly outperforms global pooling by preserving evidentiary diversity, effectively addressing the information bottleneck. Furthermore, our stress tests reveal critical scaling insights: (1) a \textit{Context-Performance Paradox}, where extending reasoning contexts beyond 12k tokens introduces noise that degrades accuracy; and (2) the \textit{Curse of Dimensionality} in action spaces, where expanding toolsets necessitates hierarchical retrieval strategies. Crucially, our approach underscores the potential of training-free architectural engineering, achieving state-of-the-art performance on CURE-Bench without the need for expensive end-to-end finetuning. This provides a scalable, agile foundation for the next generation of trustworthy AI-driven therapeutics. Code has been released on https://github.com/June01/CureAgent.

</details>


### [76] [Ontology Learning with LLMs: A Benchmark Study on Axiom Identification](https://arxiv.org/abs/2512.05594)
*Roos M. Bakker,Daan L. Di Scala,Maaike H. T. de Boer,Stephan A. Raaijmakers*

Main category: cs.AI

TL;DR: 本文提出OntoAxiom基准测试，系统评估大型语言模型在识别本体公理方面的性能，发现Axiom-by-Axiom提示策略优于直接方法，但性能因公理类型和本体领域而异。


<details>
  <summary>Details</summary>
Motivation: 本体开发需要大量建模和领域专业知识，虽然自然语言处理和大型语言模型的发展推动了本体学习自动化，但在识别公理（定义类和属性间逻辑关系的基本组件）方面仍面临挑战。

Method: 创建OntoAxiom基准测试，包含9个中等规模本体共17,118个三元组和2,771个公理，聚焦子类、不相交、子属性、定义域和值域公理。评估12个LLM在三种few-shot设置和两种提示策略（直接查询所有公理vs逐个公理查询）下的性能。

Result: Axiom-by-Axiom提示策略的F1分数高于直接方法；性能因公理类型而异；领域影响显著（如FOAF本体子类公理得分0.642，音乐本体仅0.218）；大型模型优于小型模型，但小型模型在资源受限环境下仍可用。

Conclusion: 虽然LLM性能不足以完全自动化公理识别，但能为本体工程师提供有价值的候选公理，支持本体开发和精化。Axiom-by-Axiom策略更有效，性能差异表明某些公理类型和领域更具挑战性。

Abstract: Ontologies are an important tool for structuring domain knowledge, but their development is a complex task that requires significant modelling and domain expertise. Ontology learning, aimed at automating this process, has seen advancements in the past decade with the improvement of Natural Language Processing techniques, and especially with the recent growth of Large Language Models (LLMs). This paper investigates the challenge of identifying axioms: fundamental ontology components that define logical relations between classes and properties. In this work, we introduce an Ontology Axiom Benchmark OntoAxiom, and systematically test LLMs on that benchmark for axiom identification, evaluating different prompting strategies, ontologies, and axiom types. The benchmark consists of nine medium-sized ontologies with together 17.118 triples, and 2.771 axioms. We focus on subclass, disjoint, subproperty, domain, and range axioms. To evaluate LLM performance, we compare twelve LLMs with three shot settings and two prompting strategies: a Direct approach where we query all axioms at once, versus an Axiom-by-Axiom (AbA) approach, where each prompt queries for one axiom only. Our findings show that the AbA prompting leads to higher F1 scores than the direct approach. However, performance varies across axioms, suggesting that certain axioms are more challenging to identify. The domain also influences performance: the FOAF ontology achieves a score of 0.642 for the subclass axiom, while the music ontology reaches only 0.218. Larger LLMs outperform smaller ones, but smaller models may still be viable for resource-constrained settings. Although performance overall is not high enough to fully automate axiom identification, LLMs can provide valuable candidate axioms to support ontology engineers with the development and refinement of ontologies.

</details>


### [77] [Enhancing Local Search for MaxSAT with Deep Differentiation Clause Weighting](https://arxiv.org/abs/2512.05619)
*Menghua Jiang,Haokai Gao,Shuhao Chen,Yin Chen*

Main category: cs.AI

TL;DR: 提出DeepDist算法，针对PMS和WPMS问题设计新的子句权重方案，首次区分两种问题的权重更新条件，结合新的初始化方法和消减技术，在MaxSAT评估中超越现有最优解算器。


<details>
  <summary>Details</summary>
Motivation: 现有随机局部搜索算法主要关注子句权重方案设计，但未能充分区分部分最大可满足性和加权部分最大可满足性，通常采用统一的权重更新策略，忽视了这两种问题类型之间的关键结构差异。

Method: 提出新颖的子句权重方案，首次根据PMS和WPMS实例的不同条件更新子句权重；引入新的初始化方法以适应两种实例类型的独特特征；提出消减方法优先满足单元子句和硬子句，与权重方案形成互补。

Result: 在最近MaxSAT评估的基准测试中，DeepDist超越了最先进的SLS解算器；与TT-Open-WBO-Inc结合的混合解算器超越了MaxSAT评估2024的获胜者SPB-MaxSAT-c-Band和SPB-MaxSAT-c-FPS。

Conclusion: 提出的方法有效区分了PMS和WPMS问题，DeepDist算法在性能上超越了现有最优解算器，证明了该方法的有效性，代码已开源。

Abstract: Partial Maximum Satisfiability (PMS) and Weighted Partial Maximum Satisfiability (WPMS) generalize Maximum Satisfiability (MaxSAT), with broad real-world applications. Recent advances in Stochastic Local Search (SLS) algorithms for solving (W)PMS have mainly focused on designing clause weighting schemes. However, existing methods often fail to adequately distinguish between PMS and WPMS, typically employing uniform update strategies for clause weights and overlooking critical structural differences between the two problem types. In this work, we present a novel clause weighting scheme that, for the first time, updates the clause weights of PMS and WPMS instances according to distinct conditions. This scheme also introduces a new initialization method, which better accommodates the unique characteristics of both instance types. Furthermore, we propose a decimation method that prioritizes satisfying unit and hard clauses, effectively complementing our proposed clause weighting scheme. Building on these methods, we develop a new SLS solver for (W)PMS named DeepDist. Experimental results on benchmarks from the anytime tracks of recent MaxSAT Evaluations show that DeepDist outperforms state-of-the-art SLS solvers. Notably, a hybrid solver combining DeepDist with TT-Open-WBO-Inc surpasses the performance of the MaxSAT Evaluation 2024 winners, SPB-MaxSAT-c-Band and SPB-MaxSAT-c-FPS, highlighting the effectiveness of our approach. The code is available at https://github.com/jmhmaxsat/DeepDist

</details>


### [78] [A Fast Anti-Jamming Cognitive Radar Deployment Algorithm Based on Reinforcement Learning](https://arxiv.org/abs/2512.05753)
*Wencheng Cai,Xuchao Gao,Congying Han,Mingqiang Li,Tiande Guo*

Main category: cs.AI

TL;DR: FARDA框架使用深度强化学习快速部署认知雷达对抗干扰，相比进化算法速度提升约7000倍，同时保持相当的覆盖性能。


<details>
  <summary>Details</summary>
Motivation: 现代战争中快速部署认知雷达对抗干扰是关键挑战，现有基于进化算法的方法耗时且易陷入局部最优，需要更高效的解决方案。

Method: 将雷达部署问题建模为端到端任务，采用深度强化学习算法，设计集成神经模块感知热图信息，并开发新的奖励格式。

Result: 实验结果表明，FARDA方法在覆盖性能上与进化算法相当，但部署速度提升约7000倍，消融实验证实了各组件的重要性。

Conclusion: FARDA框架通过深度强化学习实现了快速高效的雷达部署，显著提升了抗干扰雷达部署的速度和效率。

Abstract: The fast deployment of cognitive radar to counter jamming remains a critical challenge in modern warfare, where more efficient deployment leads to quicker detection of targets. Existing methods are primarily based on evolutionary algorithms, which are time-consuming and prone to falling into local optima. We tackle these drawbacks via the efficient inference of neural networks and propose a brand new framework: Fast Anti-Jamming Radar Deployment Algorithm (FARDA). We first model the radar deployment problem as an end-to-end task and design deep reinforcement learning algorithms to solve it, where we develop integrated neural modules to perceive heatmap information and a brand new reward format. Empirical results demonstrate that our method achieves coverage comparable to evolutionary algorithms while deploying radars approximately 7,000 times faster. Further ablation experiments confirm the necessity of each component of FARDA.

</details>


### [79] [Evolutionary System 2 Reasoning: An Empirical Proof](https://arxiv.org/abs/2512.05760)
*Zeyuan Ma,Wenqi Huang,Guo-Huan Song,Hongshu Guo,Sijie Ma,Zhiguang Cao,Yue-Jiao Gong*

Main category: cs.AI

TL;DR: 本文提出进化推理优化（ERO）框架，通过进化算法在LLM群体中筛选出具有强大推理能力的个体，发现最新LLM的系统2推理能力有限，但通过简单进化循环可显著增强较弱模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在特定任务上表现出色，但在通用智能方面仍有不足。受智能与系统2推理（慢思考）相关性的启发，本文研究机器智能（如LLM）是否能像人类一样进化获得推理能力（而非特定技能）。

Method: 提出进化推理优化（ERO）框架：首先初始化多个LLM作为种群，然后采用进化策略对种群进行演化，以最大化最佳个体的量化推理分数，实现"适者生存"的筛选过程。

Result: 实验发现两个令人惊讶的实证结果：1）最新LLM（如GPT-5）仍表现出有限的系统2推理能力；2）通过ERO的简单进化循环，相对较弱的模型（Qwen-7B）可被增强并涌现出强大的推理能力。

Conclusion: ERO框架证明通过进化方法可以显著提升LLM的推理能力，为机器智能获得类似人类的推理能力提供了可行路径，项目代码已在GitHub开源供复现使用。

Abstract: Machine intelligence marks the ultimate dream of making machines' intelligence comparable to human beings. While recent progress in Large Language Models (LLMs) show substantial specific skills for a wide array of downstream tasks, they more or less fall shorts in general intelligence. Following correlation between intelligence and system 2 reasoning (slow thinking), in this paper, we aim to answering a worthwhile research question: could machine intelligence such as LLMs be evolved to acquire reasoning ability (not specific skill) just like our human beings? To this end, we propose evolutionary reasoning optimization (ERO) framework which performs survival of the fittest over a population of LLMs to search for individual with strong reasoning ability. Given a reasoning task, ERO first initializes multiple LLMs as a population, after which an evolutionary strategy evolves the population to maximize quantified reasoning score of the best individual. Based on experiments on representative testsuites, we claim two surprising empirical discoveries: i) the latest LLMs such as GPT-5 still show limited system 2 reasoning ability; ii) with simple evolution-loop of ERO, a relatively weak model (Qwen-7B) could be enhanced to emerge powerful reasoning ability. Our project can be accessed at https://github.com/MetaEvo/ERO for reproduction needs.

</details>


### [80] [The Missing Layer of AGI: From Pattern Alchemy to Coordination Physics](https://arxiv.org/abs/2512.05765)
*Edward Y. Chang*

Main category: cs.AI

TL;DR: 论文反驳了"LLMs只是模式匹配器，无法实现真正推理"的批评，提出缺失的是System-2协调层，并形式化了UCCT理论和MACI架构来实现推理能力。


<details>
  <summary>Details</summary>
Motivation: 针对当前对大型语言模型的批评——认为它们只是"模式匹配器"，结构上无法进行推理或规划——论文认为这种批评混淆了问题本质。真正的瓶颈不是模式匹配本身，而是缺乏一个能够选择、约束和绑定这些模式的System-2协调层。

Method: 提出了UCCT（语义锚定理论），将推理建模为由有效支持度(rho_d)、表征不匹配(d_r)和自适应锚定预算(gamma log k)控制的相变过程。在此基础上设计了MACI架构，包含诱饵（行为调制的辩论）、过滤（苏格拉底式判断）和持久性（事务性记忆）三个协调组件。

Result: 通过理论形式化和架构设计，展示了如何将常见的反对意见重新解释为可测试的协调失败，而不是LLMs的根本限制。论证了在LLMs基础上构建AGI的可行性路径。

Conclusion: 通往AGI的道路是通过LLMs而不是绕过它们。模式存储库是必要的System-1基础，而缺失的System-2协调层可以通过UCCT理论和MACI架构来实现，从而使推理能力从模式匹配中涌现。

Abstract: Influential critiques argue that Large Language Models (LLMs) are a dead end for AGI: "mere pattern matchers" structurally incapable of reasoning or planning. We argue this conclusion misidentifies the bottleneck: it confuses the ocean with the net. Pattern repositories are the necessary System-1 substrate; the missing component is a System-2 coordination layer that selects, constrains, and binds these patterns. We formalize this layer via UCCT, a theory of semantic anchoring that models reasoning as a phase transition governed by effective support (rho_d), representational mismatch (d_r), and an adaptive anchoring budget (gamma log k). Under this lens, ungrounded generation is simply an unbaited retrieval of the substrate's maximum likelihood prior, while "reasoning" emerges when anchors shift the posterior toward goal-directed constraints. We translate UCCT into architecture with MACI, a coordination stack that implements baiting (behavior-modulated debate), filtering (Socratic judging), and persistence (transactional memory). By reframing common objections as testable coordination failures, we argue that the path to AGI runs through LLMs, not around them.

</details>


### [81] [Multimodal Oncology Agent for IDH1 Mutation Prediction in Low-Grade Glioma](https://arxiv.org/abs/2512.05824)
*Hafsa Akebli,Adam Shephard,Vincenzo Della Mea,Nasir Rajpoot*

Main category: cs.AI

TL;DR: 本文提出了一个多模态肿瘤学智能体（MOA），整合了基于TITAN基础模型的IDH1突变预测组织学工具，结合临床和基因组数据的结构化推理，在低级别胶质瘤中实现了高精度的IDH1突变预测。


<details>
  <summary>Details</summary>
Motivation: 低级别胶质瘤中IDH1突变定义了具有特定预后和治疗意义的临床亚组，准确预测这些突变对临床决策至关重要。现有方法可能无法充分利用多模态信息，需要整合组织学、临床和基因组数据以提高预测准确性。

Method: 开发了多模态肿瘤学智能体（MOA），整合了基于TITAN基础模型的IDH1突变预测组织学工具，通过PubMed、Google Search和OncoKB对结构化临床和基因组输入进行推理。在TCGA-LGG队列的488名患者上评估了MOA报告。

Result: MOA（无组织学工具）的F1分数为0.826，优于临床基线（0.798）。当与组织学特征融合时，MOA达到最高性能，F1分数为0.912，超过了组织学基线（0.894）和融合的组织学-临床基线（0.897）。

Conclusion: MOA通过整合外部生物医学资源，捕获了互补的突变相关信息，能够准确预测低级别胶质瘤中的IDH1突变，为临床决策提供了有价值的工具。

Abstract: Low-grade gliomas frequently present IDH1 mutations that define clinically distinct subgroups with specific prognostic and therapeutic implications. This work introduces a Multimodal Oncology Agent (MOA) integrating a histology tool based on the TITAN foundation model for IDH1 mutation prediction in low-grade glioma, combined with reasoning over structured clinical and genomic inputs through PubMed, Google Search, and OncoKB. MOA reports were quantitatively evaluated on 488 patients from the TCGA-LGG cohort against clinical and histology baselines. MOA without the histology tool outperformed the clinical baseline, achieving an F1-score of 0.826 compared to 0.798. When fused with histology features, MOA reached the highest performance with an F1-score of 0.912, exceeding both the histology baseline at 0.894 and the fused histology-clinical baseline at 0.897. These results demonstrate that the proposed agent captures complementary mutation-relevant information enriched through external biomedical sources, enabling accurate IDH1 mutation prediction.

</details>


### [82] [To Err Is Human: Systematic Quantification of Errors in Published AI Papers via LLM Analysis](https://arxiv.org/abs/2512.05925)
*Federico Bianchi,Yongchan Kwon,Zachary Izzo,Linjun Zhang,James Zou*

Main category: cs.AI

TL;DR: 基于GPT-5的论文正确性检查器发现，已发表的AI论文包含不可忽视的客观错误数量，且错误率随时间增长。人类专家验证显示AI检查器精度达83.2%，并能对75.8%的错误提出正确修正。


<details>
  <summary>Details</summary>
Motivation: 同行评审出版物是研究知识的基础，但其中的错误会在文献中传播，造成后续研究的混淆和可重复性问题。研究加速和同行评审压力使得错误更难被发现和避免。

Method: 开发基于GPT-5的论文正确性检查器，系统识别顶级AI会议和期刊已发表论文中的客观错误（如公式、推导、计算、图表错误），排除主观评价。人类专家验证AI识别的潜在错误。

Result: 发现已发表论文包含显著数量的客观错误，且每篇论文平均错误数随时间增长：NeurIPS从2021年的3.8个增加到2025年的5.9个（增长55.3%）；ICLR从2018年的4.1个增加到2025年的5.2个；TMLR从2022/23年的5.0个增加到2025年的5.5个。人类专家确认263/316个潜在错误为真实错误，精度83.2%。AI检查器能为75.8%的错误提出正确修正。

Conclusion: 前沿大语言模型在检测和修正已发表论文中的客观错误方面具有潜力，有助于建立更坚实的知识基础。虽然大多数错误相对较小，但修正它们可以减少文献混淆并增强可重复性。

Abstract: How many mistakes do published AI papers contain? Peer-reviewed publications form the foundation upon which new research and knowledge are built. Errors that persist in the literature can propagate unnoticed, creating confusion in follow-up studies and complicating reproducibility. The accelerating pace of research and the increasing demands on the peer-review system make such mistakes harder to detect and avoid. To address this, we developed a Paper Correctness Checker based on GPT-5 to systematically identify mistakes in papers previously published at top AI conferences and journals. Our analysis focuses on objective mistakes-e.g., errors in formulas, derivations, calculations, figures, and tables-that have a clearly verifiable ground truth. We intentionally exclude subjective considerations such as novelty, importance, or writing quality. We find that published papers contain a non-negligible number of objective mistakes and that the average number of mistakes per paper has increased over time-from 3.8 in NeurIPS 2021 to 5.9 in NeurIPS 2025 (55.3% increase); from 4.1 in ICLR 2018 to 5.2 in ICLR 2025; and from 5.0 in TMLR 2022/23 to 5.5 in TMLR 2025. Human experts reviewed 316 potential mistakes identified by the AI Checker and confirmed that 263 were actual mistakes, corresponding to a precision of 83.2%. While most identified issues are relatively minor, correcting them would reduce confusion in the literature and strengthen reproducibility. The AI Checker also surfaced potentially more substantive mistakes that could affect the interpretation of results. Moreover, we show that the AI Checker can propose correct fixes for 75.8% of the identified mistakes. Overall, this study highlights the potential of frontier LLMs to detect and correct objective mistakes in published papers, helping to establish a firmer foundation of knowledge.

</details>


### [83] [TRACE: A Framework for Analyzing and Enhancing Stepwise Reasoning in Vision-Language Models](https://arxiv.org/abs/2512.05943)
*Shima Imani,Seungwhan Moon,Lambert Mathias,Lu Zhang,Babak Damavandi*

Main category: cs.AI

TL;DR: TRACE是一个透明推理和一致性评估框架，通过辅助推理集分解复杂问题，评估中间步骤一致性，诊断推理轨迹而非仅最终答案。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在可靠数学和科学推理方面仍面临挑战，标准最终答案评估往往掩盖推理错误，导致无声失败持续存在。

Method: TRACE框架利用辅助推理集（紧凑的子问题-答案对）分解复杂问题，通过基于一致性的指标评估中间步骤，并暴露标准评估忽略的失败。

Result: 实验表明，ARS间的一致性与最终答案正确性相关，有助于精确定位推理失败步骤，为模型改进提供可操作的信号。TRACE还定义了置信区域，区分可靠与不可靠的推理路径。

Conclusion: TRACE通过透明推理轨迹诊断和一致性评估，解决了大型视觉语言模型推理评估的局限性，支持有效的过滤、调试和模型改进。

Abstract: Reliable mathematical and scientific reasoning remains an open challenge for large vision-language models. Standard final-answer evaluation often masks reasoning errors, allowing silent failures to persist. To address this gap, we introduce TRACE, a framework for Transparent Reasoning And Consistency Evaluation that diagnoses reasoning trajectories rather than only end results. At its core, TRACE leverages Auxiliary Reasoning Sets, compact sub question answer pairs that decompose complex problems, evaluate intermediate steps through consistency-based metrics, and expose failures overlooked by standard evaluation. Our experiments show that consistency across ARS correlates with final-answer correctness and helps pinpoint the reasoning steps where failures arise, offering actionable signals for model improvement. Furthermore, TRACE defines confidence regions that distinguish reliable from unreliable reasoning paths, supporting effective filtering, debugging, and model refinement.

</details>


### [84] [Variational Quantum Rainbow Deep Q-Network for Optimizing Resource Allocation Problem](https://arxiv.org/abs/2512.05946)
*Truong Thanh Hung Nguyen,Truong Thinh Nguyen,Hung Cao*

Main category: cs.AI

TL;DR: VQR-DQN结合变分量子电路与Rainbow DQN，在人力资源分配问题上比传统方法提升4.9-13.4%性能


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习在资源分配问题上受限于经典函数逼近器的表达能力，而量子计算中的叠加和纠缠特性有望提升表示能力

Method: 提出VQR-DQN，将环拓扑变分量子电路与Rainbow DQN集成，将人力资源分配问题建模为基于人员能力、事件调度和转移时间的MDP

Result: 在四个HRAP基准测试中，VQR-DQN相比随机基线减少26.8%标准化完工时间，比Double DQN和经典Rainbow DQN提升4.9-13.4%

Conclusion: 量子增强的深度强化学习在大规模资源分配问题上具有潜力，电路表达能力、纠缠与策略质量的理论联系得到验证

Abstract: Resource allocation remains NP-hard due to combinatorial complexity. While deep reinforcement learning (DRL) methods, such as the Rainbow Deep Q-Network (DQN), improve scalability through prioritized replay and distributional heads, classical function approximators limit their representational power. We introduce Variational Quantum Rainbow DQN (VQR-DQN), which integrates ring-topology variational quantum circuits with Rainbow DQN to leverage quantum superposition and entanglement. We frame the human resource allocation problem (HRAP) as a Markov decision process (MDP) with combinatorial action spaces based on officer capabilities, event schedules, and transition times. On four HRAP benchmarks, VQR-DQN achieves 26.8% normalized makespan reduction versus random baselines and outperforms Double DQN and classical Rainbow DQN by 4.9-13.4%. These gains align with theoretical connections between circuit expressibility, entanglement, and policy quality, demonstrating the potential of quantum-enhanced DRL for large-scale resource allocation. Our implementation is available at: https://github.com/Analytics-Everywhere-Lab/qtrl/.

</details>


### [85] [SymPyBench: A Dynamic Benchmark for Scientific Reasoning with Executable Python Code](https://arxiv.org/abs/2512.05954)
*Shima Imani,Seungwhan Moon,Adel Ahmadyan,Lu Zhang,Kirmani Ahmed,Babak Damavandi*

Main category: cs.AI

TL;DR: SymPyBench是一个包含15,045个大学物理问题的大规模合成基准，支持无限参数配置，包含三种问题类型和创新的评估指标，用于测试语言模型的科学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准在测试语言模型的科学推理能力方面存在局限性，需要更全面、动态的评估工具来量化模型在物理问题解决中的表现、一致性和不确定性。

Method: 创建了15,045个完全参数化的大学物理问题（90/10训练/测试分割），每个问题都附带结构化逐步推理和可执行的Python代码。包含三种问题类型：MC-Symbolic、MC-Numerical和自由形式。引入了三个新颖的评估指标：一致性分数、失败率和混淆率。

Result: 通过最先进的指令调优语言模型的实验，揭示了模型在科学推理方面的优势和局限性，表明SymPyBench能够有效评估模型在不同问题变体中的表现变化和不确定性。

Conclusion: SymPyBench为开发更稳健和可解释的推理系统奠定了基础，通过其动态、代码驱动的特性和创新的评估指标，能够全面评估语言模型在科学推理任务中的能力。

Abstract: We introduce, a large-scale synthetic benchmark of 15,045 university-level physics problems (90/10% train/test split). Each problem is fully parameterized, supporting an effectively infinite range of input configurations, and is accompanied by structured, step-by-step reasoning and executable Python code that produces the ground-truth solution for any parameter set. The benchmark contains three question types: MC-Symbolic (multiple-choice with symbolic options), MC-Numerical (multiple-choice with numerical options), and free-form (open-ended responses). These diverse formats test complementary reasoning skills. By leveraging the dynamic, code-driven nature of the benchmark, we introduce three novel evaluation metrics in addition to standard accuracy: Consistency Score, Failure Rate, and Confusion Rate, that quantify variability and uncertainty across problem variants. Experiments with state-of-the-art instruction-tuned language models reveal both strengths and limitations in scientific reasoning, positioning SymPyBench as a foundation for developing more robust and interpretable reasoning systems

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [86] [Interaction-Induced Breakdown of Anderson Localization: Thermodynamic Segregation disguised as the Skin Effect](https://arxiv.org/abs/2512.05393)
*Ali Tozar*

Main category: cond-mat.str-el

TL;DR: 强相互作用可以克服无序诱导的局域化，在一维费米-哈伯德模型中驱动系统进入宏观分离相，自旋种类在相反边界积累，这种现象挑战了无序-相互作用竞争的传统理解。


<details>
  <summary>Details</summary>
Motivation: 研究无序和排斥相互作用在一维费米-哈伯德模型中的相互作用，挑战无序通常增强局域化、相互作用在多体局域化中加强局域化的传统范式。

Method: 在一维费米-哈伯德模型中，在开放边界条件下研究强无序和排斥相互作用，通过严格的对照实验在厄米极限下验证，分析能量最小化机制。

Result: 强排斥相互作用可以克服无序诱导的局域化，驱动系统进入宏观分离相，自旋种类在相反边界积累，这种现象在厄米极限下仍然存在，表现为尖锐的热力学交叉，具有发散的能量敏感性。

Conclusion: 相互作用诱导的分离现象挑战了无序-相互作用竞争的传统理解，揭示了多体能量最小化是主要驱动力，而非非厄米皮肤效应，为开放量子系统中的无序-相互作用竞争提供了新视角。

Abstract: We investigate the interplay between strong disorder and repulsive interactions in the one-dimensional Fermi-Hubbard model under open boundary conditions. While uncorrelated disorder is widely accepted to localize all single-particle eigenstates, a phenomenon typically reinforced by interactions in the Many-Body Localization (MBL) regime, we report a counter-intuitive breakdown of this paradigm. We demonstrate that strong repulsive interactions can overcome disorder-induced localization, driving the system into a macroscopically segregated phase where spin species accumulate at opposite boundaries. Although this boundary accumulation phenomenologically mimics the Non-Hermitian Skin Effect (NHSE) observed in non-reciprocal systems, our comprehensive analysis reveals a fundamentally different origin. By performing a rigorous control experiment in the Hermitian limit, we prove that the segregation persists without non-reciprocity, identifying many-body energy minimization as the primary driver. This "interaction-induced segregation" manifests as a sharp thermodynamic crossover, characterized by a divergent energy susceptibility, challenging the conventional understanding of disorder-interaction competition in open quantum systems.

</details>


### [87] [Sub 1 K Adiabatic Demagnetization Refrigeration with Rare-Earth Borates Ba$_3$XB$_9$O$_{18}$ and Ba$_3$XB$_3$O$_9$, X = (Yb, Gd)](https://arxiv.org/abs/2512.05550)
*Marvin Klinger,Tim Treu,Felix Kreisberger,Christian Heil,Anna Klinger,Anton Jesche,Philipp Gegenwart*

Main category: cond-mat.str-el

TL;DR: 该论文研究了稀土硼酸盐Ba₃XB₉O₁₈和Ba₃XB₃O₉（X=Yb,Gd）的结构、磁性和热力学性质，评估其在低于1K温度下的绝热去磁制冷性能。


<details>
  <summary>Details</summary>
Motivation: 随着全球氦-3供应日益紧张，绝热去磁制冷（ADR）在低于1K温度制冷方面重新获得关注。传统的水合顺磁盐存在局限性，而受挫稀土氧化物具有更高的熵密度和实用优势（加热或抽真空时不降解）。

Method: 研究了稀土硼酸盐Ba₃XB₉O₁₈和Ba₃XB₃O₉（X=Yb,Gd）的结构、磁性和热力学性质，分析了从2K起始、在5T磁场下的绝热去磁制冷性能，并与文献结果进行比较。

Result: 除Ba₃GdB₉O₁₈在108mK有序外，其他三种材料在最低测量温度下保持顺磁性。评估了这些材料在绝热去磁制冷中的性能表现。

Conclusion: 稀土硼酸盐材料在低于1K温度制冷中具有潜力，特别是那些保持顺磁性的材料，为氦-3短缺背景下的低温制冷提供了有前景的替代方案。

Abstract: Adiabatic demagnetization refrigeration (ADR) is regaining relevance for the refrigeration to temperatures below 1 K as global helium-3 supply is increasingly strained. While ADR at these temperatures is long established with paramagnetic hydrated salts, more recently frustrated rare-earth oxides were found to offer higher entropy densities and practical advantages since they do not degrade under heating or evacuation. We report structural, magnetic and thermodynamic properties of the rare-earth borates Ba$_3$XB$_9$O$_{18}$ and Ba$_3$XB$_3$O$_9$ with X = (Yb, Gd). Except for Ba$_3$GdB$_9$O$_{18}$, which orders at 108 mK, the three other materials remain paramagnetic down to their lowest measured temperatures. ADR performance starting at 2 K in a field of 5 T is analyzed and compared to literature results.

</details>


### [88] [Interplay of Orbital Degeneracy and Vacancies in Stabilizing Collinear Magnetic Order in Cr$_{1+δ}$Te$_2$](https://arxiv.org/abs/2512.05563)
*Prasanta Chowdhury,Jyotirmoy Sau,Mohamad Numan,Jhuma Sannigrahi,Matthias Gutmann,Gangadhar Das,D. T. Adroja,Saurav Giri,Manoranjan Kumar,Subham Majumdar*

Main category: cond-mat.str-el

TL;DR: Cr1.33Te2的磁性结构争议：通过中子衍射、X射线吸收光谱和第一性原理计算，发现结构空位导致局域对称性破缺，提升Cr 3d轨道简并度，从而稳定了共线磁基态而非非共线磁结构。


<details>
  <summary>Details</summary>
Motivation: Cr1+δTe2作为一种二维范德华铁磁体，其磁性结构存在争议，介于共线和非共线自旋构型之间。需要从微观层面澄清其磁性结构，并理解控制磁各向异性的机制。

Method: 结合单晶中子衍射、X射线吸收光谱和第一性原理计算，从实验和理论两方面研究Cr1.33Te2的微观磁性结构。

Result: 中子衍射显示明显的共线自旋排列，光谱分析揭示Cr和Te位点存在固有结构空位。这些空位导致局域对称性破缺，提升Cr 3d轨道简并度，从而改变磁晶各向异性，稳定了共线磁基态。

Conclusion: 空位驱动机制能够控制层状铁磁体中的自旋各向异性和磁有序，为设计可调控的二维磁性材料提供了新思路。

Abstract: Cr$_{1+δ}$Te$_2$, a two-dimensional van der Waals ferromagnet, displays a contested magnetic structure, poised between collinear and non-collinear spin configurations. In this work, we investigate the magnetic structure of Cr$_{1.33}$Te$_2$ at the microscopic level by combining single-crystal neutron diffraction, X-ray absorption spectroscopy, and first-principles calculations. Neutron diffraction measurements reveal a distinct collinear spin alignment, whereas spectroscopic analyses reveal inherent structural vacancies at both Cr and Te sites. These vacancies lead to local symmetry breaking that elevates the orbital degeneracy of the Cr 3$d$ states, as demonstrated by our first-principles analysis. The resulting modification of magnetocrystalline anisotropy emerges as the key mechanism stabilising the collinear magnetic ground state over the non-collinear one in the presence of vacancies. Our findings uncover a vacancy-driven route to control spin anisotropy and magnetic ordering in layered ferromagnets, offering new insights into the design of tunable 2D magnetic materials.

</details>


### [89] [Excitonic Charge Density Waves in Moire Ladders](https://arxiv.org/abs/2512.05696)
*Paula Mellado,Francisco Muñoz,Javiera Cabezas-Escares*

Main category: cond-mat.str-el

TL;DR: 该研究探讨了非公度电荷密度波在四能带梯子模型中的特性，发现层间错位会显著改变电荷有序模式和低能玻色子激发，并表明HfTe3中的电荷密度波相具有激子性质。


<details>
  <summary>Details</summary>
Motivation: 研究层间错位如何影响层状材料中的电荷有序模式和低能激发，特别是理解非公度电荷密度波的物理特性及其在真实材料（如HfTe3）中的表现。

Method: 采用半填充的四能带梯子模型，引入层间错位δ=p/q形成超晶胞，考虑短程库仑相互作用，研究激子型非公度电荷密度波态的形成及其集体激发模式。

Result: 层间错位导致莫尔势使费米能级附近的微带变窄，产生额外的态密度峰；短程相互作用诱导激子型非公度电荷密度波态，其振幅振荡对应有隙的Higgs集体模式和最低能量的Goldstone模式（中性相位子）。

Conclusion: 即使微小的层间错位也能显著改变层状材料的电荷有序模式和低能玻色子激发，HfTe3中的电荷密度波相很可能是激子性质的，相位子的传播速度由层间错位和隧穿振幅控制。

Abstract: An incommensurate charge density wave (CDW) is a periodic modulation of charge that breaks translational symmetry incongruently with the underlying lattice. Its low-energy excitations, the phason, are collective, gapless phase fluctuations. We study a half-filled, four-band ladder model where a shift $δ=p/q$ between the legs leads to a supercell of q composite cells. The moiré potential narrows minibands near the Fermi level, resulting in additional peaks in the density of states, whose separation is controlled by $δ$. The inclusion of short-range Coulomb interactions leads to an excitonic incommensurate CDW state. We identify the oscillations in its amplitude with a gapped Higgs collective mode and a lowest-energy Goldstone mode, realized by long-lived neutral phasons whose propagation velocity is governed by the shift δ and the inter-leg tunneling amplitude. Our results show that even the slightest interlayer mismatches can strongly modify both charge-ordering patterns and low-energy bosonic excitations in layered materials, and suggest that the enigmatic CDW phase in the quasi-one-dimensional compound HfTe3 is excitonic in nature.

</details>


### [90] [Sliding phasons in Moiré Ladders](https://arxiv.org/abs/2512.05735)
*Paula Mellado,Francisco Muñoz,Javiera Cabezas-Escares*

Main category: cond-mat.str-el

TL;DR: 研究通过构建具有相对位移的梯子模型，揭示了层间不匹配如何通过莫尔势压缩能带形成平带，在库仑相互作用下产生非公度电荷密度波相，其集体激发为声学相位子。


<details>
  <summary>Details</summary>
Motivation: 旨在揭示层状材料中非公度电荷有序形成的机制，特别是研究层间不匹配（如莫尔结构）如何影响电荷密度波的形成。

Method: 构建半填充的四能带紧束缚梯子模型，其中一条腿发生二聚化导致两腿间存在相对位移δ=p/q。位移产生包含q个复合单元的莫尔超晶胞和调制的层间隧穿。莫尔势将腿能带压缩成费米能级附近的平带，引入库仑相互作用后研究电荷有序相。

Result: 发现非公度电荷密度波相，其中电荷调制在两腿间反相。该态的集体激发是长寿命的中性声学相位子，其速度由莫尔参数δ和层间隧穿振幅控制。

Conclusion: 该模型阐明了层间不匹配在范德华材料和异质结构材料中激子电荷有序相形成中的作用，莫尔结构通过压缩能带促进低能态密度峰，从而稳定非公度电荷密度波。

Abstract: An incommensurate charge density wave is a periodic modulation of charge that breaks translational symmetry at a momentum that does not coincide with the primitive lattice vectors. Its Goldstone excitation, the phason, comprises collective gapless phase fluctuations. Aiming to unveil the mechanism behind the onset of incommensurate charge order in layered materials, we study a half-filled, four-band tight-binding model on a ladder with a relative shift $δ=p/q$ between the legs, induced by the dimerization of one of them. The shift results in a moiré supercell comprising q composite cells and a modulated inter-leg tunneling. The moiré potential compresses the leg bands into flat minibands near the Fermi level, resulting in additional low-energy peaks in the density of states. Including Coulomb interactions, we find an incommensurate charge-density-wave phase in which the charge modulation is out of phase between the legs. The collective excitations of this state are long-lived neutral, acoustic phasons whose speed is controlled by the moiré parameter $δ$ and the inter-leg tunneling amplitude. This model sheds light on the role of interlayer incongruities in the formation of excitonic charge-ordered phases in van der Waals and heterostructured materials.

</details>


### [91] [Irreversible phase reconfiguration and thermal-memory effects in a highly-correlated manganite](https://arxiv.org/abs/2512.05778)
*Guilherme Kuhl-Soares,Otávio Canton,Eduardo Granado,Diego Carranza-Célis,Marcelo Knobel,Gabriel Gomide,Juan Gabriel Ramirez,Diego Muraca*

Main category: cond-mat.str-el

TL;DR: 通过温度循环拉曼光谱在LPCMO锰氧化物中发现了一种新的结构不可逆机制，揭示了晶格畸变与相分离、电荷轨道有序温度之间的相互作用，编码了热记忆效应。


<details>
  <summary>Details</summary>
Motivation: 研究相分离锰氧化物中竞争电子序和结构序的动力学，探索强关联材料中的亚稳态和记忆现象，为量子材料中的自适应和神经形态功能开辟途径。

Method: 使用温度循环拉曼光谱技术研究La₀.₂₇₅Pr₀.₃₅Ca₀.₃₇₅MnO₃ (LPCMO)材料，结合相关的磁性和输运响应测量，分析晶格与电子自由度之间的耦合。

Result: 发现了先前未知的结构不可逆机制，这种不可逆行为编码了热记忆效应，反映了系统依赖于历史过程的能量景观。磁性和输运响应证实了晶格与电子自由度之间的耦合，揭示了混合价氧化物中非平衡相动力学的新形式。

Conclusion: 这些结果推进了对强关联材料中亚稳态和记忆现象的理解，为量子材料中的自适应和神经形态功能开辟了新途径。

Abstract: Phase separated manganites provide a unique platform to study the dynamics of competing electronic and structural orders in correlated systems. In $La_{0.275}Pr_{0.35}Ca_{0.375}MnO_{3}$ (LPCMO), we use temperature cycling Raman spectroscopy to uncover a previously unidentified regime of structural irreversibility, emerging from the interplay between lattice distortions and phase competition across the phase separation and charge and orbital ordering temperatures. This irreversible behavior encodes a thermal memory effect reflecting the system's history dependent energy landscape. Correlated magnetic and transport responses confirm the coupling between lattice and electronic degrees of freedom, revealing a nem form of nonequilibrium phase dynamics in mixed valence oxides. These results advance the understanding of metastability and memory phenomena in strongly correlated materials, opening pathways toward adaptive and neuromorphic functionalities in quantum materials.

</details>


### [92] [Topological spin multipolization and linear magnetoelectric coupling in two-dimensional antiferromagnets](https://arxiv.org/abs/2512.05862)
*Jörn W. F. Venderbos,Paola Gentile,Carmine Ortix*

Main category: cond-mat.str-el

TL;DR: 该论文预测二维反铁磁体的磁电响应由基态拓扑决定，这种拓扑磁电响应源于二维Dirac费米子，与三维拓扑绝缘体不同，是二维系统特有的现象。


<details>
  <summary>Details</summary>
Motivation: 研究二维反铁磁体中的磁电响应与拓扑性质之间的关系，探索不同于三维拓扑绝缘体的二维特有拓扑磁电效应。

Method: 通过微观计算磁电极化率，分析两个最小晶格模型：自旋轨道耦合的Néel反铁磁体和自旋轨道自由的非共线双Q自旋序反铁磁体。同时考虑一维变体模型来展示维度层次结构。

Result: 揭示了磁电效应的拓扑起源可追溯到二维拓扑半金属的电磁响应，最终由一维强拓扑不变量控制。磁电极化率显示出明显的非平凡晶体拓扑特征。

Conclusion: 二维反铁磁体的磁电响应由基态拓扑决定，这种拓扑磁电效应是二维系统特有的，与三维拓扑绝缘体不同，并讨论了可能的材料实现方案。

Abstract: In this paper we predict that the magnetoelectric response of two-dimensional (2D) antiferromagnets is determined by the topology of the ground state. This topological magnetoelectric response, encoded in the spin magnetoelectric polarizability and its closely related spin multipolization, occurs when the electronic structure of the antiferromagnetic insulator is described by massive 2D Dirac fermions, and is therefore native to 2D, unlike the topological magnetoelectric effect of three-dimensional topological insulators. To demonstrate the topological contribution to the (spin) magnetoelectric polarizability, we compute the magnetoelectric polarizability microscopically for two distinct minimal lattice models: a spin-orbit coupled Néel antiferromagnet and a spin-orbit-free noncollinear antiferromagnet with double-$Q$ spin order. We show that the topological origin of the revealed magnetoelectric effect can be traced back to the electromagnetic response of topological semimetals in two dimensions, and hence is ultimately governed by a strong topological invariant in one dimension. Given this dimensional hierarchy, we further consider two minimal lattice models in one dimension, both one-dimensional variants of the 2D lattice models, and show that the magnetoelectric polarizability exhibits a clear signature of nontrivial crystalline topology. Possible material realizations are discussed.

</details>


### [93] [Layer dipole magnetoelectric polarizability of antiferromagnetic bilayers](https://arxiv.org/abs/2512.05875)
*H. Radhakrishnan,C. Ortix,J. W. F. Venderbos*

Main category: cond-mat.str-el

TL;DR: 该论文研究了二维磁性双层中的磁电效应，引入了层偶极磁电极化率的概念，描述了垂直电场引起的磁化响应，并发现二维反铁磁体存在与层赝自旋自由度相关的拓扑磁电响应。


<details>
  <summary>Details</summary>
Motivation: 研究二维磁性双层中的磁电效应，特别是垂直于双层平面的电场引起的磁化响应，探索二维反铁磁体中的拓扑磁电响应特性。

Method: 从垂直位移场产生的轨道磁化出发，推导层偶极磁电极化率的微观表达式，并将其应用于两种双层磁体最小模型：屈曲方晶格模型和磁性拓扑绝缘体模型。

Result: 在屈曲方晶格模型中，层偶极磁电极化率具有（准）拓扑贡献，揭示了二维反铁磁体存在与层赝自旋自由度相关的拓扑磁电响应。

Conclusion: 层偶极磁电极化率是描述二维磁性双层中磁电效应的关键物理量，二维反铁磁体表现出与层赝自旋自由度相关的拓扑磁电响应特性。

Abstract: In this paper we study magnetoelectric effects in two-dimensional magnetic bilayers and introduce the notion of a layer dipole magnetoelectric polarizability. This magnetoelectric polarizability describes the magnetization response to an applied electric field perpendicular to the bilayer. As such, it represents the electric analog of the spin magnetoelectric polarizability, governing the charge polarization response to an applied Zeeman field. Starting from the orbital magnetization produced by a perpendicular displacement field, we derive a microscopic expression for the layer dipole magnetoelectric polarizability and apply it to two minimal models for bilayer magnets, i.e., a buckled square lattice model and a magnetic topological insulator model. In the case of the buckled square lattice model we show that the layer dipole magnetoelectric polarizability has a (quasi-)topological contribution, revealing a topological magnetoelectric response of two-dimensional antiferromagnets associated with the layer pseudospin degree of freedom.

</details>


### [94] [Uncovering surface states of the Dirac semimetal BaMg2Bi2](https://arxiv.org/abs/2512.05886)
*A. De Vita,J. Bakkelund,H. Świątek,M. J. Winiarski,S. Malick,C. V. B. Nielsen,F. Bertran,A. J. H. Jones,P. Majchrzak,F. Miletto Granozio,J. A. Miwa,R. Ernstorfer,T. Pincelli,T. Klimczuk,C. Bigi,F. Mazzola*

Main category: cond-mat.str-el

TL;DR: 该研究通过高分辨率ARPES和DFT计算，揭示了BaMg2Bi2狄拉克半金属中先前未观测到的表面态，发现这些表面态是拓扑平庸的，但有助于解决实验与理论之间的差异。


<details>
  <summary>Details</summary>
Motivation: BaMg2Bi2是一种具有简单狄拉克锥的狄拉克半金属，与SrMg2Bi2一起被认为是化学驱动拓扑开关的有力候选材料。然而，先前的研究受限于高光子能量，导致分辨率降低和矩阵元素调制，难以与DFT计算直接比较，需要更详细地理解其电子结构。

Method: 结合高分辨率角分辨光电子能谱（ARPES）和密度泛函理论（DFT）计算，对BaMg2Bi2的价带态进行综合分析，获得低能电子结构的完整图像。

Result: 测量揭示了先前未观测到的表面态，这些表面态被证明是拓扑平庸的。这一发现提供了对材料行为的更全面理解，并解决了先前实验与理论之间的差异。

Conclusion: 通过高分辨率ARPES和DFT计算的结合，获得了BaMg2Bi2低能电子结构的全面图像，发现了拓扑平庸的表面态，这些表面态虽然非拓扑但有助于理解材料性质并调和实验与理论的差异。

Abstract: BaMg2Bi2 is a Dirac semimetal characterized by a simple Dirac cone crossing the Fermi level at the center of the Brillouin zone, protected by C3 rotational symmetry. Together with its Sr-based analogue SrMg2Bi2, it has been proposed as a promising candidate for a chemically driven topological switch: while SrMg2Bi2 is an insulator, BaMg2Bi2 exhibits non-trivial topological features. A detailed understanding of its electronic structure is essential to elucidate its electronic and transport properties. Previous photoemission studies confirmed the Dirac nature of BaMg2Bi2, but were limited to high photon energies, which hindered direct comparison with density functional theory calculations (DFT), due to reduced resolution and higher-frequency matrix-element modulation in that regime. In this work, we combine high-resolution angle-resolved photoemission spectroscopy (ARPES) and DFT calculations to get full insight on the valence band states, providing a comprehensive picture of the low-energy electronic structure. Our measurements reveal the presence of previously unobserved surface states. We found that they are topologically trivial, but they unlock a more comprehensive understanding of the material's behavior, reconciling previous discrepancies between experiment and theory.

</details>


### [95] [Minimal two band model and experimental proposals to distinguish pairing mechanisms of the high-T$_c$ superconductor La$_3$Ni$_2$O$_7$](https://arxiv.org/abs/2512.05956)
*Zheng-Duo Fan,Ashvin Vishwanath*

Main category: cond-mat.str-el

TL;DR: 该论文提出通过施加垂直电场来区分La₃Ni₂O₇中不同超导配对机制的方法，并设计了三个具体实验方案。


<details>
  <summary>Details</summary>
Motivation: La₃Ni₂O₇中发现的高温超导性开辟了不同于铜基和铁基超导体的新路径，但目前缺乏实验可验证的方案来区分不同的配对机制。

Method: 构建了一个最小二带模型，重现了ARPES测量和DFT计算中观察到的费米面拓扑结构，分析了两种不同配对机制产生的超导性，并研究了它们对垂直电场的响应差异。

Result: 发现两种配对机制对垂直电场的响应存在显著差异，这为区分不同配对方案提供了清晰的方法。

Conclusion: La₃Ni₂O₇为干净地区分不同配对机制提供了独特机会，论文提出了三个具体的实验建议来识别最相关的配对机制。

Abstract: The discovery of high-T$_c$ superconductivity in La$_3$Ni$_2$O$_7$ has opened the door to a new route to high temperature superconductivity, distinct from that in cuprates and iron-based materials. Yet, despite intense recent activity, we lack experimentally testable protocols for distinguishing between different pairing scenarios. In this Letter, we construct a minimal two-band model that reproduces the Fermi-surface topology observed in recent ARPES measurements and DFT calculations, and we analyze superconductivity arising from two distinct pairing mechanisms. We show that these mechanisms yield sharply different responses to an applied perpendicular electric field. Thus, La$_3$Ni$_2$O$_7$ offers the unique opportunity to cleanly distinguish between different pairing scenarios. Finally, we propose three concrete experimental proposals designed to distinguish these scenarios and thereby identify the pairing mechanism most relevant to the real material.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [96] [Thermal stability originates the vanishing of the specific heats at the absolute zero](https://arxiv.org/abs/2512.05129)
*Martín-Olalla,José María*

Main category: cond-mat.stat-mech

TL;DR: 研究热容量在T→0⁺时趋于零与热稳定性的关系，发现热容量至少以T的速度趋于零才能维持标准热稳定性判据，而弱趋于零则标志着T=0时的临界条件。


<details>
  <summary>Details</summary>
Motivation: 探讨热力学第三定律中热容量在绝对零度附近趋于零的现象与热稳定性判据之间的关系，澄清这一现象是否代表新的物理定律。

Method: 通过分析热容量在T→0⁺时的渐近行为与内能U对熵S的二阶导数（U_ss）的关系，研究热稳定性判据的数学条件，并以边际费米液体为例进行说明。

Result: 热容量必须至少以T的速度趋于零才能维持标准热稳定性判据U_ss>0；若热容量弱趋于零（慢于T），则表明在T=0时存在临界条件，内能曲线在T=0处失去曲率。

Conclusion: 热容量在绝对零度附近趋于零的现象不应被视为新的物理定律，而是现有热力学框架的确认性结果，反映了热稳定性判据在低温极限下的数学要求。

Abstract: The relationship between the vanishing of the heat capacities as $T\to0^+$ and the thermal stability is examined. The heat capacities vanish as fast as or faster than $T$ as $T\to0^+$ for states at the phase space boundary ($T=0$) to sustain the standard thermal stability criterion $U_{ss}>0$. Conversely, weakly vanishing heat capacities, which signify a loss of curvature in $U(S)$ at $T=0$, are the signature of a critical condition precisely at $T=0$, as exemplified in marginal Fermi liquids. Therefore, the vanishing of the specific heat should be viewed not as a new law but as a confirmatory result of the existing framework of thermodynamics.

</details>


### [97] [Some frameworks for dissipative evolution in multiscale non-equilibrium thermodynamics](https://arxiv.org/abs/2512.05168)
*Miroslav Grmela,Michal Pavelka*

Main category: cond-mat.stat-mech

TL;DR: 本文回顾并比较了非平衡热力学中耗散的几个框架，包括经典不可逆热力学、梯度动力学、瑞利耗散势、耗散达朗贝尔框架以及泊松括号生成的耗散演化框架。


<details>
  <summary>Details</summary>
Motivation: 非平衡热力学中的耗散现象需要系统化的理论框架来描述，本文旨在梳理和比较现有的主要耗散框架，阐明它们之间的关系和适用性。

Method: 首先概述经典不可逆热力学和梯度动力学，然后讨论瑞利耗散势和耗散达朗贝尔框架及其与梯度动力学的关系，最后探讨由泊松括号生成的耗散演化框架。

Result: 通过系统比较，揭示了不同耗散框架之间的内在联系和数学结构，为理解和应用非平衡热力学中的耗散理论提供了清晰的框架体系。

Conclusion: 不同耗散框架在描述非平衡热力学现象时各有优势，它们之间存在密切的数学联系，为研究复杂耗散系统提供了多样化的理论工具。

Abstract: In this paper, we review and compare some frameworks for dissipation in non-equilibrium thermodynamics. We start with a brief overview of classical irreversible thermodynamics and gradient dynamics. Then we discuss several specific frameworks including Rayleigh dissipation potential and the dissipative d'Alembert framework, showing their relations with gradient dynamics. Finally, we discuss frameworks for dissipative evolution generated from Poisson brackets.

</details>


### [98] [Dynamic hysteresis and transitions induced by potential asymmetry](https://arxiv.org/abs/2512.05465)
*Samudro Ghosh,Moupriya Das*

Main category: cond-mat.stat-mech

TL;DR: 研究双稳态系统中外部周期场引起的动态磁滞现象，发现通过调控势能不对称性可以在温和条件下诱导磁滞回线对称性破缺和动态相变。


<details>
  <summary>Details</summary>
Motivation: 动态磁滞现象在技术中有广泛应用，环境噪声对其有重要调控作用。传统上对称性破缺只在噪声强度、周期力振幅和频率的极端条件下出现，本研究旨在探索在温和条件下通过调控势能不对称性来诱导对称性破缺的可能性。

Method: 在双稳态系统中引入周期场，研究系统响应函数与场之间的磁滞回线。通过设计底层势能的不对称性，分析磁滞回线对称性破缺的条件。定量研究磁滞回线面积和序参数随势能不对称程度的变化规律。

Result: 研究发现通过适当设计势能的不对称性，可以在温和条件下诱导磁滞回线的对称性破缺，观察到动态相变。建立了磁滞回线面积和序参数与势能不对称程度的定量关系，表明通过调控内在势能的不对称性可以有效控制动态磁滞过程。

Conclusion: 该研究提出了一种通过调控内在势能不对称性来控制动态磁滞过程的新方法，为涉及磁滞现象的技术进步开辟了新路径，并为理解底层势能在调控这一基础过程中的精确作用提供了基本认识。

Abstract: The influence of an external periodic field in a bistable system gives rise to a very interesting phenomenon of dynamic hysteresis, which has extensive applications in technologies. Environmental noise also has a crucial role in controlling this class of hysteresis. The relaxational delay of the system's response towards the periodic drive manifests in the response function-field hysteresis loops. These hysteresis loops are generally symmetric in the range of the external force and the response function. Symmetry breaking is observed under extreme conditions of the noise strength, amplitude and frequency of the periodic force. This symmetric-asymmetric dynamic transition is interpreted in terms of the relevant order parameter defined for the system under study. In our present work, we importantly identify that it is possible to induce symmetry breaking in the hysteresis loops, and consequently to observe dynamic transitions under moderate conditions as well if appropriate asymmetry is implemented in the design of the underlying potential. We develop a quantitative understanding of the variation of the hysteresis loop area and the order parameter with respect to the degree of asymmetry in the potential. Our study suggests an ingenious way to get the desired outputs in the processes related to dynamic hysteresis by effectively controlling the asymmetry of the intrinsic potential governing the dynamics. These findings not only direct a new and impactful path for technological advances involving hysteresis but also provide the basic understanding of the precise role of the underlying potential in regulating this fundamental process, which manifests frequently in natural and designed systems.

</details>


### [99] [Observation-Time-Induced Crossover from Fluctuating Diffusivity](https://arxiv.org/abs/2512.05471)
*Masahiro Shirataki,Takuma Akimoto*

Main category: cond-mat.stat-mech

TL;DR: 该研究提出蛋白质动力学转变可解释为观测时间诱导的交叉现象，源于朗之万框架中的涨落扩散系数，而非真正的相变。


<details>
  <summary>Details</summary>
Motivation: 蛋白质动力学转变（在特征温度下均方位移突然增加）在实验和模拟中广泛观察到，但其物理起源仍不清楚。研究旨在解释这一现象的本质。

Method: 采用朗之万框架中的涨落扩散系数模型，通过分析和数值方法研究观测时间诱导的交叉现象，揭示其产生机制和条件。

Result: 研究发现有效扩散系数表现出温度依赖的转变，其交叉点随观测时间移动。这种观测时间诱导的交叉现象为蛋白质动力学转变提供了统一的非平衡解释。

Conclusion: 蛋白质动力学转变可视为这种一般交叉机制的一个实例，而非真正的相变，这为理解蛋白质动力学行为提供了新的理论框架。

Abstract: A dynamical transition -- seen as a sudden increase in the mean-squared displacement at a characteristic temperature that depends on the observation time -- is widely reported in neutron-scattering experiments and molecular dynamics simulations of hydrated proteins. However, its physical origin remains elusive. We show that fluctuating diffusivity in a Langevin framework leads to an observation-time-induced crossover, where the effective diffusion coefficient exhibits a temperature-dependent transition whose crossover point shifts with observation time. Analytical and numerical analyses reveal the mechanism of this crossover and delineate the conditions under which it emerges. Our findings provide a unified nonequilibrium interpretation for observation-time-induced crossover, and suggest that the protein dynamical transition can be viewed as an instance of this general crossover mechanism.

</details>


### [100] [Sticky eigenstates in systems with sharply-divided phase space](https://arxiv.org/abs/2512.05627)
*Hua Yan*

Main category: cond-mat.stat-mech

TL;DR: 研究具有尖锐分割相空间的系统中混合本征态的特性，使用分段线性映射，发现粘性本征态在MUPO情况下按ℏ¹ᐟ²标度，在准周期情况下围绕此代数行为振荡，揭示了非KAM系统中经典粘性的量子特征。


<details>
  <summary>Details</summary>
Motivation: 研究具有尖锐分割相空间的系统中混合本征态的特性，特别是经典粘性在量子系统中的表现，以及非KAM系统中粘性本征态的标度行为。

Method: 使用具有不同规则-混沌边界的分段线性映射，边界由边际不稳定周期轨道(MUPOs)或准周期轨道形成。通过重叠指数和熵局域化长度对混合本征态进行分类，分析动力学隧穿贡献和粘性本征态的标度行为。

Result: 动力学隧穿贡献按∼ℏ exp(-b/ℏ)标度，b>0与规则区域相对大小相关。粘性本征态在MUPO情况下按ℏ¹ᐟ²标度，在准周期情况下围绕此代数行为振荡。对于研究的分段线性映射，γ=2，这与经典粘性在累积RTDs中t⁻²的代数衰减相对应。

Conclusion: 研究结果揭示了非KAM系统中经典粘性的清晰量子特征，推广了KAM系统中分层态标度行为ℏ¹⁻¹ᐟγ的预测，对于分段线性映射γ=2，粘性本征态标度为ℏ¹ᐟ²。

Abstract: We investigate mixed eigenstates in systems with sharply-divided phase space, using different piecewise-linear maps whose regular-chaotic boundaries are formed by marginally unstable periodic orbits (MUPOs) or by quasi-periodic orbits. With the overlap index and the entropy localization length, we classify mixed eigenstates and show that the contribution from dynamical tunneling scales as $\sim \hbar\, \exp(-b/\hbar)$, with $b>0$ associated with the relative size of the regular region. The dominant fraction of states that remain sticky to the boundaries, referred to as sticky eigenstates, scales as $\hbar^{1/2}$ in the MUPO case and oscillates around this algebraic behavior in the quasi-periodic case. This behavior generalizes established predictions for hierarchical states in KAM systems, which scale as $\hbar^{1 - 1/γ}$, with $γ$ set by the corresponding classical stickiness reflected in the algebraic decay of cumulative RTDs $t^{-γ}$. For the piecewise-linear maps studied here, $γ= 2$. These results reveal a clear quantum signature of classical stickiness in non-KAM systems.

</details>


### [101] [Geometric control of boundary-catalytic branching processes](https://arxiv.org/abs/2512.05637)
*Denis S. Grebenkov,Yilin Ye*

Main category: cond-mat.stat-mech

TL;DR: 研究边界催化分支过程的几何控制，通过吸收补偿分支增殖，建立临界线区分种群指数增长与灭绝


<details>
  <summary>Details</summary>
Motivation: 边界催化分支过程广泛存在于自然现象中，扩散粒子在复杂环境中的催化边界上自发进行二元分支（如分裂、裂变），需要研究如何通过几何控制来调节种群增长

Method: 通过识别适当的Steklov谱问题，建立该非平衡随机过程的相图，利用主特征值确定临界线

Result: 建立了计算最优吸收率的强大工具，该吸收率能平衡分支和吸收的相反效应，实现扩散反应系统的稳态行为；同时发现了临界催化速率，超过该速率则无法补偿，种群将持续指数增长

Conclusion: 该框架为理解和控制各种边界催化分支过程提供了有前景的视角，在物理学和生命科学中具有应用价值

Abstract: Boundary-catalytic branching processes describe a broad class of natural phenomena where the population of diffusing particles grows due to their spontaneous binary branching (e.g., division, fission or splitting) on a catalytic boundary located in a complex environment. We investigate the possibility of geometric control of the population growth by compensating the proliferation of particles due to branching events by their absorptions in the bulk or on boundary absorbing regions. We identify an appropriate Steklov spectral problem to obtain the phase diagram of this out-of-equilibrium stochastic process. The principal eigenvalue determines the critical line that separates an exponential growth of the population from its extinction. In other words, we establish a powerful tool for calculating the optimal absorption rate that equilibrates the opposite effects of branching and absorption events and thus results in steady-state behavior of this diffusion-reaction system. Moreover, we show the existence of a critical catalytic rate above which no compensation is possible, so that the population cannot be controlled and keeps growing exponentially. The proposed framework opens promising perspectives for better understanding, modeling and control of various boundary-catalytic branching processes, with applications in physics and life sciences.

</details>
