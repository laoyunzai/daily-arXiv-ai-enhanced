<div id=toc></div>

# Table of Contents

- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 7]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 6]
- [quant-ph](#quant-ph) [Total: 32]
- [cs.LG](#cs.LG) [Total: 40]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.AI](#cs.AI) [Total: 32]


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [1] [Two-Dimensional Active Brownian Particles Crossing a Parabolic Barrier: Transition-Path Times, Survival Probability, and First-Passage Time](https://arxiv.org/abs/2601.15310)
*Michele Caraglio*

Main category: cond-mat.stat-mech

TL;DR: 本文推导了二维活性布朗粒子在抛物线势垒中传播子和跃迁路径时间分布的解析表达式，发现自推进作用显著缩短跃迁路径时间。


<details>
  <summary>Details</summary>
Motivation: 研究活性布朗粒子在势垒穿越过程中的动力学行为，特别是自推进作用如何影响跃迁路径时间分布和首次通过时间分布。

Method: 以被动布朗粒子的本征态为基础，将活性作为微扰处理，通过求解约化后的Fokker-Planck方程，得到传播子和跃迁路径时间分布的解析表达式。

Result: 自推进作用显著缩短跃迁路径时间；存活概率和首次通过时间分布强烈依赖于粒子活性，而旋转扩散系数的影响相对较小。

Conclusion: 活性布朗粒子的自推进作用在势垒穿越过程中起主导作用，显著加速了跃迁过程，为理解活性粒子的非平衡动力学提供了理论框架。

Abstract: We derive an analytical expression for the propagator and the transition path time distribution of a two-dimensional active Brownian particle crossing a parabolic barrier with absorbing boundary conditions at both sides. By taking those of a passive Brownian particle as basis states and dealing with the activity as a perturbation, our solution is expressed in terms of the perturbed eigenfunctions and eigenvalues of the associated Fokker-Planck equation once the latter is reduced by taking into account only the coordinate along the direction of the barrier and the self-propulsion angle. We show that transition path times are typically shortened by the self-propulsion of the particle. Our solution also allows us to obtain the survival probability and the first-passage times distribution, which display a strong dependence on the particle's activity, while the rotational diffusivity influences them to a minor extent.

</details>


### [2] [Mesoscopic Fluctuations in Statistical Systems](https://arxiv.org/abs/2601.15782)
*V. I. Yukalov,E. P. Yukalova*

Main category: cond-mat.stat-mech

TL;DR: 该论文综述了介观涨落现象，即尺寸远大于最近邻平均距离但远小于系统整体尺寸的涨落，它们在不同系统中表现为一种相在另一种宿主相中的涨落。


<details>
  <summary>Details</summary>
Motivation: 介观涨落是许多系统中普遍存在的现象，其特征与周围物质有本质不同，可以解释为一种相在另一种宿主相中的涨落。这些涨落出现在凝聚态物质、囚禁原子系统以及生物和社会系统中，需要建立统一的理论框架来描述这一普遍现象。

Method: 论文提供了不同材料和系统中介观涨落实验证据的综述，重点关注描述介观涨落的通用理论方法，并讨论了该方法的实际应用。

Result: 论文综述了介观涨落在各种系统中的实验证据，提出了描述这类涨落的通用理论方法，并展示了该方法在不同领域的应用。

Conclusion: 介观涨落是跨越多物理系统的普遍现象，需要统一的描述方法。论文提供的理论框架能够有效描述这类涨落，并在不同领域具有广泛的应用价值。

Abstract: The fluctuations are termed mesoscopic, when their typical size is essentially larger then the average distance between the nearest neighbors, while being much smaller than the overall system size. Since the features of mesoscopic fluctuations are essentially different from those of the surrounding matter, they can be interpreted as fluctuations of one phase occurring inside another host phase. In condensed matter, these fluctuations are of nanosize. They can occur in many-body systems of different nature, for instance, they are typical for condensed matter, can appear in systems of trapped atoms, and also arise in biological and social systems. A survey of the experimental evidence for the occurrence of mesoscopic fluctuations in different materials and systems is given. The main attention is paid to a general theoretical approach for describing them. Applications of the approach are also discussed.

</details>


### [3] [The flux of particles in a one-dimensional Fleming-Viot process](https://arxiv.org/abs/2601.16029)
*Éric Brunet,Bernard Derrida*

Main category: cond-mat.stat-mech

TL;DR: 研究Fleming-Viot过程在半无限直线上有偏扩散的情况，发现其密度演化具有类似Fisher-KPP方程的特征，但可以显式求解。通过修改原点附近的扩散规则，可以产生类似推拉转变的吸收粒子通量转变。


<details>
  <summary>Details</summary>
Motivation: 研究Fleming-Viot过程在有偏扩散和吸收边界条件下的行为，探索其与经典Fisher-KPP方程的相似性和差异，特别是关于稳态解、长时间渐近行为和通量转变的特性。

Method: 考虑N个粒子在半无限直线上有偏扩散（向吸收原点扩散）的Fleming-Viot过程。当粒子被吸收时，从剩余N-1个粒子中随机选择一个复制新粒子。在大N极限下，密度演化变为确定性过程。通过修改原点附近的扩散规则，研究通量转变。

Result: 发现Fleming-Viot过程具有类似Fisher-KPP方程的特征：单参数稳态解族、长时间渐近行为依赖于初始条件、Bramson对数偏移等。但Fleming-Viot过程可以显式求解任意初始条件和任意时间。修改扩散规则会产生类似推拉转变的吸收粒子通量转变。使用截断近似推导了大N极限下吸收粒子通量的主要修正项。

Conclusion: Fleming-Viot过程在有偏扩散条件下表现出与Fisher-KPP方程相似的数学结构，但具有可显式求解的优势。通过扩散规则的修改可以实现通量转变，这为研究类似推拉转变的现象提供了新视角。截断近似方法可用于预测大N极限下的修正行为。

Abstract: The Fleming-Viot process describes a system of $N$ particles diffusing on a graph with an absorbing site. Whenever one of the particles is absorbed, it is replaced by a new particle at the position of one of the $N-1$ remaining particles. Here we consider the case where the particles lie on the semi-infinite line with a biased diffusion towards the origin which is the absorbing site. In the large $N$ limit, the evolution of the density becomes deterministic and has a number of characteristics similar to the Fisher-KPP equation: a one-parameter family of steady state solutions, dependence of the long time asymptotics on the initial conditions, Bramson logarithmic shift, etc. One noticeable difference, however, is that in the Fleming-Viot case, the solution can be computed explicitly for arbitrary initial conditions and at an arbitrary time. By modifying the diffusion rule near the origin, one can produce a transition in the flux of absorbed particles, very similar to the pushed-pulled transition in travelling waves. Lastly, using a cut-off approximation (which is known to be correct in the theory of travelling waves), we derive a number of predictions for the leading large $N$ correction of the flux of absorbed particles.

</details>


### [4] [Random Walks Across Dimensions: Exploring Simplicial Complexes](https://arxiv.org/abs/2601.16086)
*Diego Febbe,Duccio Fanelli,Timoteo Carletti*

Main category: cond-mat.stat-mech

TL;DR: 提出了一种描述单纯复形上随机游走过程的新算子，允许游走者在不同维度的单形间移动，通过分层结构扩展到任意高维，其渐近分布可用于评估高阶单形的相对重要性。


<details>
  <summary>Details</summary>
Motivation: 传统随机游走主要关注节点间的连接，但现实世界中的复杂系统往往包含更高阶的结构（如边、三角形等）。需要一种能够描述在单纯复形上跨越不同维度结构的随机游走过程，以更好地理解和分析复杂系统中的高阶相互作用。

Method: 引入了一个新颖的算子来描述单纯复形上的随机游走过程。游走者可以在不同维度的单形之间移动（如从节点到边，从边到三角形），通过嵌套的分层组织结构扩展到任意有限高维结构。该方法还考虑了存在随机传送时的最优搜索策略。

Result: 游走者的渐近分布为评估高阶单形的相对重要性提供了自然排序方法。研究揭示了噪声与高阶结构之间的特殊相互作用，并解决了存在随机传送情况下的最优搜索策略问题。

Conclusion: 该研究提出了一种能够描述单纯复形上跨越维度随机游走的新框架，为分析复杂系统中的高阶结构提供了有效工具，特别是在评估高阶单形重要性和理解噪声与高阶结构相互作用方面具有重要价值。

Abstract: We introduce a novel operator to describe a random walk process on a simplicial complex. Walkers are allowed to wonder across simplices of various dimensions, bridging nodes to edges, and edges to triangles, via a nested organization that hierarchically extends to higher structures of arbitrary large, but finite, dimension. The asymptotic distribution of the walkers provides a natural ranking to gauge the relative importance of higher order simplices. Optimal search strategies in presence of stochastic teleportation are addressed and the peculiar interplay of noise with higher order structures unraveled.

</details>


### [5] [Transition in Splitting Probabilities of Quantum Walks](https://arxiv.org/abs/2601.16111)
*Prashant Singh,David A. Kessler,Eli Barkai*

Main category: cond-mat.stat-mech

TL;DR: 量子行走在双目标监测下的分裂概率表现出相变行为，受采样时间控制。当采样时间小于临界值时，分裂概率为1/2且普适；超过临界值则进入非普适区域，分裂概率偏离1/2并呈现波动模式。


<details>
  <summary>Details</summary>
Motivation: 研究量子行走在双目标监测下的分裂概率行为，探索其与经典随机行走的显著差异，特别是相变行为的出现。

Method: 通过叠加原理将双目标分裂问题映射到一对单目标检测问题，分析连续时间量子行走在双目标监测下的行为。

Result: 发现分裂概率存在临界采样时间τ_c = 2π/ΔE。当采样时间小于τ_c时，分裂概率为普适的1/2；当采样时间大于τ_c时，分裂概率偏离1/2并呈现依赖于采样时间和初始条件的波动模式。

Conclusion: 量子行走在双目标监测下展现出与经典随机行走截然不同的相变行为，这种非解析行为由采样时间控制，揭示了量子行走的独特统计特性。

Abstract: We investigate the splitting probability of a monitored continuous-time quantum walk with two targets and show that, in stark contrast to a classical random walk, it exhibits a nonanalytic, phase-transition-like behavior controlled by the sampling time at the targets. For large systems and sampling times smaller than a critical value $τ_c = 2π/ΔE$, where $ΔE$ is the energy bandwidth, the splitting probability is universal and equal to $1/2$, independent of the initial condition and the sampling time. Above the critical sampling, a nonuniversal regime emerges in which the splitting probability deviates from $1/2$ and develops a fluctuating pattern of pronounced peaks and dips dependent on both the sampling time and the initial condition. These results follow from a nontrivial mapping of the splitting problem onto a pair of single-target detection problems enabled by the superposition principle.

</details>


### [6] [Langevin equations with non-Gaussian thermal noise: Valid but superfluous](https://arxiv.org/abs/2601.16114)
*Alex V. Plyukhin*

Main category: cond-mat.stat-mech

TL;DR: 该研究探讨了广义朗之万方程中加性热噪声的统计特性，通过Jarzynski等式在有限时间内的验证，发现除非噪声是高斯分布，否则该方程仅在七阶以下无条件满足Jarzynski等式。


<details>
  <summary>Details</summary>
Motivation: 传统上通过假设系统遍历性和渐近长时间平衡来验证广义朗之万方程的有效性，但本文旨在通过Jarzynski等式在有限时间内的检验来评估该方程的适用性，且不假设系统遍历性。

Method: 研究一个经典布朗振子模型，其初始刚度（或频率）受到持续时间为τ的矩形脉冲扰动。通过分析Jarzynski等式在不同阶数下的满足情况，检验广义朗之万方程与线性耗散的相容性。

Result: 发现Jarzynski等式仅在七阶以下无条件满足；在更高阶数下，该等式成立当且仅当噪声是高斯分布。这意味着除非噪声精确为高斯分布，否则朗之万方程只能用于评估噪声及其导数线性或二次型的性质。

Conclusion: 具有线性耗散和非高斯噪声的朗之万方程虽然不自相矛盾，但实际上是多余的，因为其适用的性质对噪声统计特性不敏感。该方程只能可靠地处理噪声的线性或二次型性质。

Abstract: We discuss the statistics of additive thermal (internal) noise in systems governed by the generalized Langevin equation with linear dissipation. To assess the equation's validity, it is common to assume that the system is ergodic and to verify that solutions approach correct equilibrium values at asymptotically long times. In this paper, we instead consider the consistency of the generalized Langevin equation with the Jarzynski equality at finite times and do not assume the system's ergodicity. Specifically, we consider a classical Brownian oscillator whose initial stiffness, or frequency, is perturbed by a rectangular pulse of duration $τ$. We find that the Jarzynski equality is satisfied unconditionally only up to the seventh order in $τ$; in higher orders, the Jarzynski equality holds if and only if the noise is Gaussian. These results imply that, unless it is exact, the Langevin equation can only be used to evaluate properties that are linear or quadratic in noise and its derivatives. Such properties are insensitive to the noise statistics, so the Langevin equation with linear dissipation and non-Gaussian noise (though not inconsistent by itself) is superfluous.

</details>


### [7] [A saturation bound for cumulative responses under local linear relaxation](https://arxiv.org/abs/2601.16157)
*Sanjeev Kumar Verma*

Main category: cond-mat.stat-mech

TL;DR: 论文证明累积观测量的饱和现象仅源于线性局部弛豫，与几何、维度或微观动力学无关，为各种系统中的饱和现象提供了统一解释。


<details>
  <summary>Details</summary>
Motivation: 在传播或扩散信号的系统中，累积观测量的饱和现象广泛存在，但通常需要系统特定的机制来解释。本文旨在证明这种饱和现象实际上仅源于线性局部弛豫这一基本过程。

Method: 通过分析线性弛豫过程，推导出任何线性可观测量在弛豫信号寿命期间的累积值存在一个由弛豫时间设定的上界。当弛豫通过输运或扩散映射到空间时，这一时间界限会产生相应的空间饱和尺度。

Result: 得到了一个闭式表达式，揭示了两种行为模式：短时间内的线性增长和超过弛豫时间后的饱和。该结果与几何、维度或微观动力学无关，为各种系统中的饱和现象提供了统一解释。

Conclusion: 累积饱和现象可以直接从线性局部弛豫推导出来，无需依赖系统特定的机制。这一发现为输运、扩散和随机系统中的饱和现象提供了最小且统一的解释框架。

Abstract: Saturation of cumulative observables is widely observed in systems with propagating or spreading signals and is commonly modeled using system-specific mechanisms such as scattering statistics, coherence functions, or phenomenological decay laws. This work shows that such saturation follows directly from linear local relaxation alone. Any linear observable accumulated over the lifetime of a relaxing signal is bounded by a scale set by the relaxation time, independent of geometry, dimensionality, or microscopic dynamics. When relaxation is mapped to space through transport or spreading, this temporal bound yields a corresponding spatial saturation scale. A closed-form expression reveals a two-regime behavior: linear growth at short times followed by saturation beyond the relaxation time. The result provides a minimal and unified explanation for cumulative saturation across transport, diffusive, and stochastic systems.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [8] [Exactly solvable topological phase transition in a quantum dimer model](https://arxiv.org/abs/2601.15377)
*Laura Shou,Jeet Shah,Matthew Lerner-Brecher,Amol Aggarwal,Alexei Borodin,Victor Galitski*

Main category: cond-mat.str-el

TL;DR: 该论文提出了一类广义Rokhsar-Kivelson哈密顿量，在三角晶格上构建了具有可调参数的量子二聚体模型，发现了从拓扑Z2量子自旋液体到柱状有序态的连续量子相变。


<details>
  <summary>Details</summary>
Motivation: 研究量子二聚体模型中的相变行为，特别是探索如何通过设计哈密顿量来获得精确的基态，并理解拓扑量子自旋液体与有序态之间的转变。

Method: 引入广义Rokhsar-Kivelson哈密顿量家族，在三角晶格上构建具有周期性边权重的量子二聚体模型，采用2×1周期性模型，通过可调水平边权重α控制相变。

Result: 在α=3处发现连续量子相变：α<3时为拓扑Z2量子自旋液体，α>3时为柱状有序态。二聚体-二聚体关联函数在相变两侧呈指数衰减，临界点呈幂律衰减。vison关联函数在自旋液体相指数衰减，在有序相变为常数。

Conclusion: 通过vison关联函数的有限尺寸标度分析，提取的临界指数与2D Ising普适类一致，揭示了量子二聚体模型中拓扑相变的基本物理机制。

Abstract: We introduce a family of generalized Rokhsar-Kivelson (RK) Hamiltonians, which are reverse-engineered to have an arbitrary edge-weighted superposition of dimer coverings as their exact ground state at the RK point. We then focus on a quantum dimer model on the triangular lattice, with doubly-periodic edge weights. For simplicity we consider a $2\times1$ periodic model in which all weights are set to one except for a tunable horizontal edge weight labeled $α$. We analytically show that the model exhibits a continuous quantum phase transition at $α=3$, changing from a topological $\mathbb{Z}_2$ quantum spin liquid ($α<3$) to a columnar ordered state ($α>3$). The dimer-dimer correlator decays exponentially on both sides of the transition with the correlation length $ξ\propto1/|α-3|$ and as a power-law at criticality. The vison correlator exhibits an exponential decay in the spin liquid phase, but becomes a constant in the ordered phase. We explain the constant vison correlator in terms of loops statistics of the double-dimer model. Using finite-size scaling of the vison correlator, we extract critical exponents consistent with the 2D Ising universality class.

</details>


### [9] [Theory of Next-Generation Even-Denominator States](https://arxiv.org/abs/2601.15386)
*Misha Yutushui,David F. Mross*

Main category: cond-mat.str-el

TL;DR: 该论文研究了下一代偶分母量子霍尔态的理论，包括其体准粒子、边缘输运测量中的配对通道区分方法以及试验波函数，证明了通量附着对界面模式拓扑稳定性的不变性，并比较了不同能级中下一代配对态与Bonderson-Slingerland态的能量偏好。


<details>
  <summary>Details</summary>
Motivation: 偶分母量子霍尔态是实现非阿贝尔拓扑序的主要候选者，ν=5/2平台是最早且研究最多的例子。最近在GaAs和双层石墨烯中观察到了许多"下一代"偶分母态（如ν=3/4、3/8、3/10），需要对这些态进行理论发展以理解其性质。

Method: 发展了这些态的理论，包括分析其体准粒子、边缘输运测量中区分配对通道的方法、试验波函数推导。推导了通量附着如何影响态的各种普适性质的一般关系，特别是证明了界面模式的拓扑稳定性在通量附着下不变。比较了相同填充因子下的下一代配对态与Bonderson-Slingerland态。

Result: 证明了通量附着不改变界面模式的拓扑稳定性。发现下一代态和Bonderson-Slingerland态的准粒子携带相同电荷并遵循相同交换统计。这两种态仍描述不同相：下一代态在最低朗道能级中能量更有利，而Bonderson-Slingerland态在第一激发能级中更有利。

Conclusion: 该研究为理解下一代偶分母量子霍尔态提供了系统的理论框架，揭示了通量附着的普适性质不变性，并确定了不同能级中配对态的能量偏好差异，为实验区分这些拓扑相提供了理论基础。

Abstract: Even-denominator quantum Hall states are leading candidates for realizing non-Abelian topological orders, with the $ν=\frac{5}{2}$ plateau in GaAs the first and most-studied example. Recent experiments in GaAs and bilayer graphene (BLG) have observed many `next-generation' even-denominator states at filling factors such as $ν=\frac{3}{4}$, $\frac{3}{8}$, and $\frac{3}{10}$. We develop the theory of these states, including analyses of their bulk quasiparticles, of methods for distinguishing between pairing channels in edge transport measurements, and of their trial wavefunctions. As part of this study, we derive general relations of how flux attachment affects many universal properties of states. In particular, we prove that the topological stability of interface modes is invariant under flux attachment. We compare next-generation paired states to Bonderson-Slingerland states at the same filling factors, and demonstrate that their quasiparticles carry identical charges and obey the same exchange statistics. The next-generation and Bonderson-Slingerland states still describe distinct phases, and we find that the former are energetically favored in the lowest Landau level, while the latter are favored in the first excited level.

</details>


### [10] [Spin reorientations in structurally metastable, disordered, and hexagonal Cr7Te8](https://arxiv.org/abs/2601.15702)
*K. Guratinder,T. G. Romig,H. C. Mandujano,C. Stock,E. E. Rodriguez*

Main category: cond-mat.str-el

TL;DR: 二维Cr₇Te₈材料表现出异常的温度依赖霍尔效应，包括室温反常霍尔效应、冷却过程中的霍尔电阻率符号反转以及低温下的霍尔电阻率峰值。淬火得到的六方相具有亚稳态特性，在加热时会发生向单斜相的结构转变，并显示出复杂的磁性转变行为。


<details>
  <summary>Details</summary>
Motivation: 研究二维Cr₇Te₈异质结构中观察到的异常霍尔效应特性，通过研究体相材料的磁性、结构和电子性质，试图理解这些异常现象背后的物理机制，并建立结构、磁性和电子性质之间的联系。

Method: 采用1000°C淬火合成体相Cr₇Te₈材料，通过高分辨率X射线衍射研究结构相变，利用磁化率测量分析磁性转变，并通过中子衍射研究确定具体的磁结构变化。

Result: 淬火得到的六方相在加热至约550K时发生向单斜相的一级相变。磁性测量显示该相具有高于室温的铁磁有序，并在约220K和70K处有两个明显的转变。中子衍射证实220K处为铁磁磁矩的自旋重取向转变，而70K处则是从高温铁磁态向低温反铁磁态的转变。

Conclusion: 二维Cr₇Te₈异质结构中观察到的温度依赖霍尔效应特性与体相材料的结构和磁性转变密切相关，表明在"伪"二维铬碲化物中，结构、磁性和电子性质之间存在紧密联系。

Abstract: Vapor deposited two-dimensional Cr$_{7}$Te$_{8}$ displays unusual temperature dependent Hall effect properties, including a room temperature anomalous Hall effect, sign reversals of the Hall resistivity on cooling, and a peak in the Hall resistivity at low temperatures. The two dimensional Cr$_{7}$Te$_{8}$ heterostructures that form the basis of these measurements are hexagonal in structure. We study the magnetic and structural properties of bulk Cr$_{7}$Te$_{8}$ synthesized by quenching from 1000 $^{\circ}$C with the goal of relating the magnetic, structural, and electronic properties. This quenched phase is metastable, hexagonal, and displays different magnetic properties from the slow-cooled and more thermodynamically stable monoclinic phase. High-resolution x-ray diffraction of the quenched hexagonal phase finds a first-order transition to a lower symmetry monoclinic phase on \textit{heating} above $\sim$ 550 K. Magnetic susceptibility measurements of the quenched hexagonal phase reveal ferromagnetic ordering above room temperature, along with the two distinct transitions at $\sim$ 220~K and $\sim$ 70~K. Through neutron diffraction studies, we find the $\sim$ 220 K anomaly is a spin reorientation transition of the ferromagnetically aligned magnetic moments and the $\sim70$ K feature represents a transition from a high temperature ferromagnet to a low temperature antiferromagnet. We suggest that these magnetic transitions are related to changes in the unit cell dimensions and are connected to the temperature dependent Hall resisitivity studied in two-dimensional heterostructures. This implies a link between structural, magnetic, and electronic properties in the ``pseudo" two-dimensional chromium tellurides.

</details>


### [11] [Magnetoelastic coupling at the field-induced transition in EuAl$_{12}$O$_{19}$](https://arxiv.org/abs/2601.15833)
*T. Haidamak,G. Bastien,P. Proschek,A. Eliáš,R. H. Colman,D. Gorbunov,S. Zherlitsyn,A. A. Zvyagin,G. A. Zvyagina,J. Prokleška,V. Sechovský,M. Vališka*

Main category: cond-mat.str-el

TL;DR: 研究EuAl12O19中磁弹性耦合对磁场诱导相变的影响，通过测量横向磁场中的声速发现剪切模量C44在铁磁到顺磁相变时显著软化，这归因于交换伸缩机制和相变附近的强磁涨落。


<details>
  <summary>Details</summary>
Motivation: 研究各向异性铁磁体中磁弹性耦合在磁场诱导相变中的作用，特别是EuAl12O19这种准二维各向异性铁磁体，通过超声波方法探索其弹性常数在横向磁场中的变化。

Method: 使用超声波方法测量EuAl12O19在垂直于自发磁化方向的磁场中的声速，从而确定弹性常数。特别关注剪切模量C44的变化，并结合磁化数据进行理论计算，基于应变交换机制分析磁弹性耦合效应。

Result: 剪切模量C44在磁场诱导的铁磁到顺磁相变时表现出显著的软化现象，这种软化归因于二阶相变附近的强磁涨落。基于磁化数据的理论计算在应变交换机制框架下定性再现了观测到的行为。

Conclusion: EuAl12O19中的磁弹性耦合主要来源于交换伸缩机制，这一结果为建模其他各向异性铁磁体中的类似相变提供了框架，证实了磁弹性耦合在磁场诱导相变中的关键作用。

Abstract: Magnetoelastic coupling plays a crucial role in magnetic-field-induced transitions in anisotropic ferromagnets. Ultrasonic methods are suitable for experimental investigations of these phenomena. We investigate elastic constants in EuAl$_{12}$O$_{19}$, a quasi-two-dimensional anisotropic ferromagnet, by measuring sound velocity in magnetic fields perpendicular to spontaneous magnetization. The shear modulus $C_{44}$ exhibits dramatic softening at the field-induced transition from the ferromagnetic to a paramagnetic phase with magnetic moments forced to polarize along the applied transverse field. The softening is attributed to strong magnetic fluctuations near a second-order phase transition. Theoretical calculations based on magnetization data qualitatively reproduced the observed behavior within a strain-exchange mechanism. These results demonstrate that magnetoelastic coupling in EuAl$_{12}$O$_{19}$ arises primarily from exchange striction and provide a framework for modeling similar transitions in other anisotropic ferromagnets.

</details>


### [12] [The role of the apical oxygen in cuprate high-temperature superconductors](https://arxiv.org/abs/2601.16017)
*Samuel Vadnais,Rémi Duchesne,Kristjan Haule,A. -M. S. Tremblay,David Sénéchal,Benjamin Bacq-Labreuil*

Main category: cond-mat.str-el

TL;DR: 通过第一性原理计算，研究发现铜氧化物超导体中Cu-顶端O距离变化对超导序参数的影响主要源于CuO₂平面有效空穴掺杂的改变，而非电荷转移能隙的变化。


<details>
  <summary>Details</summary>
Motivation: 先前实验发现Bi-2212超导体中Cu-顶端O距离与超导序参数存在相关性，这被解释为超导性与电荷转移能隙直接相关的证据，并重新引发了关于顶端氧在铜氧化物超导中作用的长期争论。

Method: 采用密度泛函理论和团簇动力学平均场理论相结合的第一性原理计算方法，研究了Bi₂Sr₂CuO₆₊δ、Bi-2212和HgBa₂CuO₄₊δ三种铜氧化物中仅由顶端氧位移引起的超导序参数变化。

Result: 计算与实验数据定量吻合，确认了观测到的超导序参数变化确实由Cu-顶端O距离变化引起。然而，这些变化主要源于CuO₂平面有效空穴掺杂的改变，对电荷转移能隙的影响可以忽略不计。

Conclusion: 顶端氧位移单独引起的超导序参数调制幅度有限，因此在解释不同铜氧化物化合物间Tc与Cu-顶端O距离相关性时需要谨慎，这种相关性可能主要反映的是掺杂效应而非电荷转移能隙的直接作用。

Abstract: Scanning tunneling microscopy measurements exploiting the natural superstructure modulation of the cuprate superconductor Bi$_2$Sr$_2$CaCu$_2$O$_{8+x}$ (Bi-2212) have revealed a possible correlation between the Cu-apical-O distance $δ_{\mathrm{api}}$ and the superconducting order parameter $m_{\mathrm{SC}}$, as reported recently by O'Mahony et al. (Proc. Natl. Acad. Sci. 119, e2207449119 (2022)). These observations were interpreted as evidence for a direct link between superconductivity and the charge-transfer gap, and more broadly revived the long-standing question of the role of apical oxygens in cuprate superconductivity. Using a combination of density-functional theory and cluster dynamical mean-field theory, we compute from first principles the variations of $m_{\mathrm{SC}}$ induced solely by apical oxygen displacement in Bi$_2$Sr$_2$CuO$_{6+δ}$, Bi-2212, and HgBa$_2$CuO$_{4+δ}$. The quantitative agreement between our calculations and experiments allows us to unambiguously attribute the observed variations of $m_{\mathrm{SC}}$ to changes in $δ_{\mathrm{api}}$. We demonstrate, however, that these variations of $m_{\mathrm{SC}}$ originate predominantly from changes in the effective hole-doping of the CuO$_2$ planes, with negligible effect on the charge-transfer gap. The modest magnitude of the $m_{\mathrm{SC}}$ modulation induced by apical-oxygen displacement alone therefore warrants caution in interpreting correlations between $T_c$ and $δ_{\mathrm{api}}$ inferred from comparisons across different cuprate compounds.

</details>


### [13] [Charge and spin orders in the t-U-V-J model: a slave-spin-1 approach](https://arxiv.org/abs/2601.16153)
*Olivier Simard,Michel Ferrero,Thomas Ayral*

Main category: cond-mat.str-el

TL;DR: 本文提出了一种自旋-1 slave-particle技术，用于经济地处理t-U-V-J费米子模型，通过将电荷和自旋自由度映射到赝自旋和赝费米子扇区，并采用自洽团簇平均场方法，发现了电荷和自旋条纹相。


<details>
  <summary>Details</summary>
Motivation: 强关联费米子系统在凝聚态物理中非常重要，但即使在最先进的数值方法下也难以求解，特别是在参数区域中各种自由度在相似能量和长度尺度上竞争或合作时。

Method: 引入自旋-1 slave-particle技术，将原始电荷和自旋自由度分别映射到赝自旋和赝费米子扇区，采用自洽团簇平均场方法处理这些扇区。

Result: 研究了模型在各种条件下的相图，报告了电荷和自旋条纹的出现，这些条纹是赝粒子扇区团簇平均场处理的结果，在之前的slave-particle研究中未被检测到，结果与更可靠的数值方法定性吻合良好。

Conclusion: 提出的自旋-1 slave-particle技术为处理强关联费米子系统提供了一种经济有效的方法，能够捕捉到重要的物理现象如条纹相，与更精确的数值方法结果一致。

Abstract: Strongly-correlated fermion systems on a lattice have been a subject of intense focus in the field of condensed-matter physics. These systems are notoriously difficult to solve, even with state-of-the-art numerical methods, especially in regimes of parameters where degrees of freedom compete or cooperate at similar energy and length scales. Here, we introduce a spin-1 slave-particle technique to approximately treat the t-U-V-J fermionic model at arbitrary electron dopings in an economical manner. This formalism respectively maps the original charge and spin degrees of freedom into effective pseudo-spin and pseudo-fermion sectors, which are treated using a self-consistent cluster mean-field method. We study the phase diagram of the model under various conditions and report the appearance of charge and spin stripes within this formalism. These stripes are a consequence of the cluster mean-field treatment of the pseudo-particle sectors and have not been detected in previous slave-particle studies. The results obtained agree qualitatively well with what more reliable numerical methods capture.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [14] [Bidirectional teleportation using scrambling dynamics: a practical protocol](https://arxiv.org/abs/2601.15536)
*Amit Vikram,Edwin Chaparro,Muhammad Miskeen Khan,Andrew Lucas,Chris Akers,Ana Maria Rey*

Main category: quant-ph

TL;DR: 量子信息加扰可在无需通用局域控制的系统中实现集体自由度之间的通用SWAP门，通过结合黑洞信息悖论相关的Hayden-Preskill恢复方案与量子隐形传态，实现双向量子态交换。


<details>
  <summary>Details</summary>
Motivation: 在缺乏通用局域控制的系统中实现集体自由度之间的量子态交换，探索信息加扰在量子计算中的实际应用，将黑洞信息悖论的理论概念转化为实用的量子门设计。

Method: 将Hayden-Preskill恢复方案与量子隐形传态结合，以并行且相反的方向运行，通过全局相互作用实现双向量子态交换。采用Dicke模型作为实验实现方案，可在腔QED和囚禁离子平台中实现。

Result: 证明了量子信息加扰能够实现集体自由度之间的通用SWAP门，清晰区分了信息传播、纠缠和混沌在实现相干态传输和恢复中的作用，为全息原理在量子门设计中的应用提供了实用案例。

Conclusion: 量子信息加扰为在缺乏局域控制的系统中实现量子计算提供了新途径，将黑洞信息悖论的理论概念转化为实际量子门设计，展示了全息原理在量子技术中的实用价值。

Abstract: We show that quantum information scrambling can enable a generic SWAP gate between collective degrees of freedom in systems without universal local control. Our protocol combines the Hayden-Preskill recovery scheme, associated with the black hole information paradox, with quantum teleportation and runs them in parallel and in opposite directions, enabling bidirectional exchange of quantum states through global interactions alone. This approach cleanly distinguishes the roles of information spreading, entanglement, and chaos for enabling both coherent state transfer and recovery. We propose an experimental realization using the Dicke model, which can be realized in cavity-QED and trapped-ion platforms, highlighting the utility of holography in designing practical quantum gates.

</details>


### [15] [Precision limit under weak-coupling with ancillary qubit](https://arxiv.org/abs/2601.15354)
*Peng Chen,Jun Jing*

Main category: quant-ph

TL;DR: 提出一种基于测量的量子计量协议，通过探针系统（自旋系综）与辅助量子比特的XXZ相互作用，利用无条件测量实现量子Fisher信息的二次标度，达到海森堡极限。


<details>
  <summary>Details</summary>
Motivation: 传统量子计量需要GHZ态或压缩哈密顿量等复杂资源，本文旨在通过更简单的无条件测量方案超越标准量子极限，提高计量精度。

Method: 采用复合模型：探针系统（自旋系综）与辅助量子比特通过海森堡XXZ相互作用耦合。优化弱耦合强度和演化时间，利用对量子比特的无条件测量产生探针系统的两条平行演化路径，将集体角动量算符的本征态转变为具有大本征空间距离的双分量态。

Result: 量子Fisher信息关于相位编码呈现精确或渐近的二次标度（与自旋数N的平方成正比），该标度行为对不完美的编码算符和耦合强度不敏感。通过辅助量子比特或探针系统的宇称检测，相位灵敏度可接近海森堡极限。

Conclusion: 对量子比特的无条件测量可以成为替代GHZ态和压缩哈密顿量的有效资源，在计量精度上超越标准量子极限，为量子计量提供更简单实用的方案。

Abstract: We propose a measurement-based quantum metrology protocol in a composite model, where the probe system (a spin ensemble) is coupled to an ancillary two-level system (qubit) with a general Heisenberg XXZ interaction. With an optimized and weak probe-ancilla coupling strength and a proper duration of joint evolution, the two parallel evolution paths of the probe system induced by the unconditional measurement on qubit can transform an eigenstate of the collective angular momentum operator of spin ensemble to be a two-component state with a large distance in eigenspace. The quantum Fisher information about the phase encoded in the probe system of polarized states or their superposition, that could be relaxed to mixed states, can therefore manifest an exact or asymptotic quadratic scaling with respect to the probe size (spin number) $N$. The quadratic scaling behavior is found to be insensitive to the imperfect encoding operator and coupling strength. By virtue of the parity detection on the ancillary qubit or the probe system, the phase sensitivity can approach the Heisenberg limit. We suggest that the unconditional measurement on qubit could become an efficient resource to replace Greenberger-Horne-Zeilinger-like states and squeezing Hamiltonian for exceeding the standard quantum limit in metrology precision.

</details>


### [16] [USDs: A universal stabilizer decoder framework using symmetry](https://arxiv.org/abs/2601.15361)
*Hoshitaro Ohnishi,Hideo Mukai*

Main category: quant-ph

TL;DR: 本文提出了一种将深度学习应用于任意稳定子码解码的重新优化方法，解决了量子纠错码解码中标签简并性问题，在Color码和Golay码上分别实现了0.8%和0.1%的解码精度提升。


<details>
  <summary>Details</summary>
Motivation: 量子纠错是实现可靠量子计算的关键。当将深度学习应用于量子纠错码解码时，面临的主要挑战是：提供给解码器的综合征测量与对应的错误模式（作为真实标签）之间存在非唯一性关系。之前的工作针对toric码通过利用奇偶校验结构的对称性重新优化解码器解决了这一问题，本文旨在将这种方法推广到任意稳定子码。

Method: 1. 使用多层感知器近似连续函数来补充Color码和Golay码的综合征测量；2. 对每种码执行解码器重新优化；3. 通过评估连续函数近似中的几何和代数结构，分析广义连续函数设计对学习码固有几何结构的优势。

Result: 1. 对于Color码，在物理错误率为5%时，解码精度提高了约0.8%；2. 对于Golay码，解码精度提高了约0.1%；3. 分析表明，广义连续函数设计有利于学习码的几何结构，且忠实再现码结构的近似对重新优化的效果有显著影响。

Conclusion: 本研究证明，先前在toric码上有效的重新优化技术可以推广到解决将深度学习应用于稳定子码解码时出现的标签简并性挑战。该方法为任意稳定子码的深度学习解码提供了通用框架，并展示了码结构忠实近似对解码性能的重要性。

Abstract: Quantum error correction is indispensable to achieving reliable quantum computation. When quantum information is encoded redundantly, a larger Hilbert space is constructed using multiple physical qubits, and the computation is performed within a designated subspace. When applying deep learning to the decoding of quantum error-correcting codes, a key challenge arises from the non-uniqueness between the syndrome measurements provided to the decoder and the corresponding error patterns that constitute the ground-truth labels. Building upon prior work that addressed this issue for the toric code by re-optimizing the decoder with respect to the symmetry inherent in the parity-check structure, we generalize this approach to arbitrary stabilizer codes. In our experiments, we employed multilayer perceptrons to approximate continuous functions that complement the syndrome measurements of the Color code and the Golay code. Using these models, we performed decoder re-optimization for each code. For the Color code, we achieved an improvement of approximately 0.8% in decoding accuracy at a physical error rate of 5%, while for the Golay code the accuracy increased by about 0.1%. Furthermore, from the evaluation of the geometric and algebraic structures in the continuous function approximation for each code, we showed that the design of generalized continuous functions is advantageous for learning the geometric structure inherent in the code. Our results also indicate that approximations that faithfully reproduce the code structure can have a significant impact on the effectiveness of reoptimization. This study demonstrates that the re-optimization technique previously shown to be effective for the Toric code can be generalized to address the challenge of label degeneracy that arises when applying deep learning to the decoding of stabilizer codes.

</details>


### [17] [The computational two-way quantum capacity](https://arxiv.org/abs/2601.15393)
*Johannes Jakob Meyer,Jacopo Rizzo,Asad Raza,Lorenzo Leone,Sofiene Jerbi,Jens Eisert*

Main category: quant-ph

TL;DR: 该论文研究计算量子信道容量，发现在计算效率要求下，量子通信能力会发生根本性变化，存在信道其计算双向量子容量为零而无限资源容量接近最大值的分离现象。


<details>
  <summary>Details</summary>
Motivation: 传统量子信道容量的定义没有限制发送方和接收方的计算资源，而实际应用中需要计算效率，因此需要研究计算量子容量来量化在计算效率要求下的可靠信息传输能力。

Method: 研究计算双向量子容量，展示其与信道Choi态的计算可蒸馏纠缠密切相关，基于标准密码学假设构造多项式复杂度量子信道，分析其容量特性。

Result: 在标准密码学假设下，存在多项式复杂度的量子信道，其计算双向量子容量为零，而无限制资源容量接近最大值；当信道复杂度离开多项式领域时，计算量子容量会从接近最大值急剧下降到零。

Conclusion: 计算效率要求会从根本上改变量子通信的极限，计算量子容量与无限制资源容量存在显著分离，这对实际量子通信系统的设计和分析具有重要意义。

Abstract: Quantum channel capacities are fundamental to quantum information theory. Their definition, however, does not limit the computational resources of sender and receiver. In this work, we initiate the study of computational quantum capacities. These quantify how much information can be reliably transmitted when imposing the natural requirement that en- and decoding have to be computationally efficient. We focus on the computational two-way quantum capacity and showcase that it is closely related to the computational distillable entanglement of the Choi state of the channel. This connection allows us to show a stark computational capacity separation. Under standard cryptographic assumptions, there exists a quantum channel of polynomial complexity whose computational two-way quantum capacity vanishes while its unbounded counterpart is nearly maximal. More so, we show that there exists a sharp transition in computational quantum capacity from nearly maximal to zero when the channel complexity leaves the polynomial realm. Our results demonstrate that the natural requirement of computational efficiency can radically alter the limits of quantum communication.

</details>


### [18] [Studying energy-resolved transport with wavepacket dynamics on quantum computers](https://arxiv.org/abs/2601.16180)
*Melody Lee,Roland C. Farrell*

Main category: quant-ph

TL;DR: 该研究提出使用波包来探测量子模拟器中的能量依赖输运性质，相比传统方法具有更好的能量分辨率。通过在量子计算机上实现，在安德森模型中观测到能量依赖的局域化转变，并开发了误差缓解策略和波包制备算法。


<details>
  <summary>Details</summary>
Motivation: 传统方法使用简单初始态（如计算基态）研究淬火动力学，这些态远离能量本征态，限制了可实现的能量分辨率。需要开发能够制备具有可调能量和小能量方差态的方法，以更好地探测能量依赖的输运性质。

Method: 提出使用波包来探测输运性质，提高能量分辨率。在Quantinuum H2-2量子计算机上制备和演化波包，研究安德森模型中的能量依赖局域化。采用基于最大似然估计的误差缓解策略来推断无噪声输出比特串分布。还开发了制备一维相互作用费米子模型中准粒子波包的量子算法。

Result: 在8x7晶格的安德森模型中观测到能量依赖的局域化转变（有限尺寸迁移率边）。低能波包在时间演化下保持空间局域化，而高能波包则退局域化。误差缓解策略相比后选择方法消除了系统误差，并将统计不确定性降低了最多5倍。

Conclusion: 波包方法为研究量子模拟器中的能量依赖输运性质提供了改进的能量分辨率。开发的误差缓解策略和波包制备算法使波包基的输运研究成为近期量子计算机的有前景应用，特别是在多体系统中。

Abstract: Probing energy-dependent transport in quantum simulators requires preparing states with tunable energy and small energy variance. Existing approaches often study quench dynamics of simple initial states, such as computational basis states, which are far from energy eigenstates and therefore limit the achievable energy resolution. In this work, we propose using wavepackets to probe transport properties with improved energy resolution. To demonstrate the utility of this approach, we prepare and evolve wavepackets on Quantinuum's H2-2 quantum computer and identify an energy-dependent localization transition in the Anderson model on an 8x7 lattice--a finite-size mobility edge. We observe that a wavepacket initialized at low energy remains spatially localized under time evolution, while a high-energy wavepacket delocalizes, consistent with the presence of a mobility edge. Crucial to our experiments is an error mitigation strategy that infers the noiseless output bit string distribution using maximum-likelihood estimation. Compared to post-selection, this method removes systematic errors and reduces statistical uncertainty by up to a factor of 5. We extend our methods to the many-particle regime by developing a quantum algorithm for preparing quasiparticle wavepackets in a one-dimensional model of interacting fermions. This technique has modest quantum resource requirements, making wavepacket-based studies of transport in many-body systems a promising application for near-term quantum computers.

</details>


### [19] [Check-weight-constrained quantum codes: Bounds and examples](https://arxiv.org/abs/2601.15446)
*Lily Wang,Andy Zeyi Liu,Ray Li,Aleksander Kubica,Shouzhen Gu*

Main category: quant-ph

TL;DR: 该论文研究了量子低密度奇偶校验码在检查权重受限条件下的参数限制，证明了权重不超过3的稳定子码无法具有非平凡距离，并为权重受限的CSS稳定子码和子系统码建立了紧致的速率-距离权衡关系。


<details>
  <summary>Details</summary>
Motivation: 量子低密度奇偶校验码因其只需测量低权重检查而兼容噪声量子硬件，是构建噪声弹性量子计算机的关键。但检查权重约束如何限制qLDPC码的可实现参数是一个基本开放问题，需要系统研究。

Method: 结合解析论证和数值优化方法，研究检查权重受限的稳定子码和子系统码。使用线性规划技术推导有限尺寸区域的数值上界，并识别接近这些极限的具体码构造。

Result: 证明了检查权重不超过3的稳定子码无法具有非平凡距离；为检查权重不超过4的CSS稳定子码和权重不超过2的子系统码建立了紧致的速率-距离权衡关系；这些界限适用于一般qLDPC码，不依赖几何局域性或特殊图连通性假设。

Conclusion: 该研究为检查权重受限的qLDPC码建立了严格的参数界限，描绘了具有数十或数百物理量子比特的实际相关qLDPC码的景观，为量子纠错码的设计和实现提供了重要理论指导。

Abstract: Quantum low-density parity-check (qLDPC) codes can be implemented by measuring only low-weight checks, making them compatible with noisy quantum hardware and central to the quest to build noise-resilient quantum computers. A fundamental open question is how constraints on check weight limit the achievable parameters of qLDPC codes. Here, we study stabilizer and subsystem codes with constrained check weight, combining analytical arguments with numerical optimization to establish strong upper bounds on their parameters. We show that stabilizer codes with checks of weight at most three cannot have nontrivial distance. We also prove tight tradeoffs between rate and distance for broad families of CSS stabilizer and subsystem codes with checks of weight at most four and two, respectively. Notably, our bounds are applicable to general qLDPC codes, as they rely only on check-weight constraints without assuming geometric locality or special graph connectivity. In the finite-size regime, we derive numerical upper bounds using linear programming techniques and identify explicit code constructions that approach these limits, delineating the landscape of practically relevant qLDPC codes with tens or hundreds of physical qubits.

</details>


### [20] [Frictional work and entropy production in integrable and non-integrable spin chains](https://arxiv.org/abs/2601.15941)
*Vishnu Muraleedharan Sajitha,Matthew J. Davis,L. A. Williamson*

Main category: quant-ph

TL;DR: 量子系统中最大可提取功在绝热驱动下实现，摩擦功衡量绝热与非绝热驱动间的功输出差异。研究表明非可积自旋链中的摩擦功可用对角熵产生描述，该熵产生与量子相干性积累相关，由最终绝热态的有效温度表征。


<details>
  <summary>Details</summary>
Motivation: 研究量子系统中摩擦功的物理本质，探索绝热与非绝热驱动下功提取的差异，特别关注可积与非可积系统的对比，以及可积性破缺对功提取的影响。

Method: 通过分析非可积自旋链系统，研究摩擦功与对角熵产生的关系，引入有效温度概念描述最终绝热态。对比可积自旋链系统，分析不同驱动速度下摩擦功的描述方式。

Result: 非可积自旋链中，慢到中等速度驱动下，摩擦功可由对角熵产生描述，该熵产生与量子相干性积累相关，由有效温度表征。快速驱动下，摩擦功由最终非绝热态与绝热态间的量子相对熵描述。可积系统中，摩擦功需用不同有效温度的子空间项之和描述。

Conclusion: 可积性破缺在绝热极限下可增强功提取，但在足够非绝热区域会降低功提取效率。摩擦功的物理本质与系统可积性密切相关，不同驱动速度下需要不同的理论描述框架。

Abstract: The maximum work extractable from a quantum system is achieved when the system is driven adiabatically. Frictional work then quantifies the difference in work output between adiabatic and non-adiabatic driving. Here we show that frictional work in a non-integrable spin chain is well-described by the diagonal entropy production associated with the build up of quantum coherence. The relationship is characterized by an effective temperature of the final adiabatic state and holds for slow to moderate driving protocols. For fast protocols, the frictional work is instead described by the quantum relative entropy between the final non-adiabatic and adiabatic states. We compare our results to those obtained from an integrable spin chain, in which case the adiabatic state is no longer described by a single temperature. In this case, the frictional work is described by a sum of terms for each independent subspace of the spin chain, which are at different effective temperatures. We show how integrability breaking can enhance work extraction in the adiabatic limit, but degrade work extraction in sufficiently non-adiabatic regimes.

</details>


### [21] [A Sublinear-Time Quantum Algorithm for High-Dimensional Reaction Rates](https://arxiv.org/abs/2601.15523)
*Tyler Kharazi,Ahmad M. Alkadri,Kranthi K. Mandadapu,K. Birgitta Whaley*

Main category: quant-ph

TL;DR: 提出一种量子算法，通过高斯线性组合哈密顿模拟（Gaussian-LCHS）表示非幺正传播子，避免指数衰减问题，用于计算高维Fokker-Planck方程的反应速率。


<details>
  <summary>Details</summary>
Motivation: Fokker-Planck方程在科学中用于模拟稀有事件，但其高维特性对经典计算机构成挑战。现有量子算法在处理此类非幺正动力学时通常存在成功概率指数衰减的问题。

Method: 采用平方和表示法，开发高斯线性组合哈密顿模拟（Gaussian-LCHS）来表示非幺正传播子，结合新技术直接估计矩阵元素而不产生指数衰减。对于η个成对相互作用粒子，使用N个平面波离散化每个自由度。

Result: 算法以O(√(t‖H‖log(1/ε)))查询其块编码，估计反应通量误差ε需要Õ((η^{5/2}√(tβ)α_V + η^{3/2}√(t/β)N)/ε)量子门。与非凸势能的最坏情况经典分析界限O(te^{Ω(η)}/ε^4)相比，在粒子数η上实现指数分离，在ε上实现四次加速，在t上实现二次加速。

Conclusion: 该工作展示了量子算法在高维耗散动力学中实现量子优势的严格途径，虽然实际中专门化的经典启发式方法可能超越这些界限，但为量子计算在复杂动力学模拟中的应用开辟了新方向。

Abstract: The Fokker-Planck equation models rare events across sciences, but its high-dimensional nature challenges classical computers. Quantum algorithms for such non-unitary dynamics often suffer from exponential {decay in} success probability. We introduce a quantum algorithm that overcomes this for computing reaction rates. Using a sum-of-squares representation, we develop a Gaussian linear combination of Hamiltonian simulations (Gaussian-LCHS) to represent the non-unitary propagator with $O\left(\sqrt{t\|H\|\log(1/ε)}\right)$ queries to its block encoding. Crucially, we pair this with {a} novel technique to directly estimate matrix elements without exponential decay. For $η$ pairwise interacting particles discretized with $N$ plane waves per degree of freedom, we estimate reactive flux to error $ε$ using $\widetilde{O}\left((η^{5/2}\sqrt{tβ}α_V + η^{3/2}\sqrt{t/β}N)/ε\right)$ quantum gates, where $α_V = \max_{r}|V'(r)/r|$. For non-convex potentials, the {sharpest classical} worst-case analytical bounds to simulate the related overdamped Langevin {equation} scale as $O(te^{Ω(η)}/ε^4)$. This {implies} an exponential separation in particle number $η$, a quartic speedup in $ε$, and quadratic speedup in $t$. While specialized classical heuristics may outperform these bounds in practice, this demonstrates a rigorous route toward quantum advantage for high-dimensional dissipative dynamics.

</details>


### [22] [Universal Digitized Counterdiabatic Driving](https://arxiv.org/abs/2601.15972)
*Takuya Hatomura*

Main category: quant-ph

TL;DR: 提出了一种数字化的反绝热驱动通用方法，通过数字方式构建绝热规范势，具有不引入多体/非局域相互作用、能包含无限嵌套对易子、提供数字实现旋转角显式表达式三大优势。


<details>
  <summary>Details</summary>
Motivation: 反绝热驱动通过绝热规范势实现参数化哈密顿量能量本征态的参量位移，但现有方法存在引入多体/非局域相互作用、难以处理无限嵌套对易子、缺乏数字实现的具体旋转角表达式等局限性。

Method: 提出数字化反绝热驱动通用方法，基于通用反绝热驱动思想，以数字方式构建绝热规范势。该方法能够处理无限嵌套对易子，并提供数字实现所需的旋转角显式表达式。

Result: 通过解析方式证明了该方法与精确理论的一致性，并通过数值模拟验证了方法的有效性。

Conclusion: 提出的数字化反绝热驱动方法克服了现有方法的局限性，为量子计算中的绝热演化提供了更实用和高效的实现方案。

Abstract: Counterdiabatic driving realizes parameter displacement of an energy eigenstate of a given parametrized Hamiltonian using the adiabatic gauge potential. In this paper, we propose a universal method of digitized counterdiabatic driving, constructing the adiabatic gauge potential in a digital way with the idea of universal counterdiabatic driving. This method has three advantages over existing universal counterdiabatic driving and/or digitized counterdiabatic driving: it does not introduce any many-body and/or nonlocal interactions to an original target Hamiltonian; it can incorporate infinite nested commutators, which constitute the adiabatic gauge potential; and it gives explicit expression of rotation angles for digital implementation. We show the consistency of our method to the exact theory in an analytical way and the effectiveness of our method with the aid of numerical simulations.

</details>


### [23] [Spectator-transition crosstalk in a spin-3/2 silicon vacancy qudit in silicon carbide revealed by broadband Ramsey interferometry](https://arxiv.org/abs/2601.15559)
*Jun-Jae Choi,Seung-Jae Hwang,Seoyoung Paik,Juhwan Kim,Jawad UI-Hassan,Nguyen Tien Son,Hiroshi Abe,Takeshi Oshima,Jaekwon Suk,Hyeon-Ho Jeong,Dong-Hee Kim,Sang-Yun Lee*

Main category: quant-ph

TL;DR: 研究人员使用宽带拉姆齐干涉法揭示并量化了4H-SiC中硅空位色心自旋量子比特的旁观跃迁串扰，为多能级控制提供了实用的旁观感知框架。


<details>
  <summary>Details</summary>
Motivation: 4H-SiC中的硅空位色心自旋具有S=3/2基态，这是一种天然的qudit（多能级量子系统），能够实现紧凑编码和子空间选择性控制。然而，这种多能级结构也引入了旁观跃迁问题：短脉冲可能相干驱动非寻址能级对，产生串扰，这会影响量子操作的精度和可靠性。

Method: 采用宽带拉姆齐干涉法来揭示和量化旁观跃迁串扰。通过分析拉姆齐傅里叶谱中的多条谱线，将每条谱线映射到旋转框架哈密顿量的qudit能级之间的成对能量差，并通过制备状态和微波脉冲参数确定的紧凑振幅分配其权重。同时进行了数值时域传播模拟，使用实验采样参数重现失谐图。

Result: 实验结果显示拉姆齐傅里叶谱除了寻址的单量子跃迁外还显示多条谱线。分析预测了一个确定性的六分支结构，测量到的峰值位置与分析分支线完全吻合，无需频率拟合。数值模拟成功重现了失谐图，验证了分析框架的有效性。

Conclusion: 该研究为硅空位qudit中的多能级控制提供了一个实用的旁观感知框架。该方法为抑制串扰提供了明确指导，同时也提供了利用旁观谱线的新途径，例如作为原位脉冲校准的额外约束条件，以及用于相位敏感的量子态和过程估计。

Abstract: Color center spins in 4H-SiC offer a rare combination of wafer-scale materials maturity with long spin coherence and chip-level photonics, making them promising building blocks for scalable quantum technologies. In particular, the silicon vacancy hosts an S=3/2 ground state, a native qudit that enables compact encodings and subspace-selective control, but also introduces spectator transitions: short, detuned pulses can coherently drive non-addressed level pairs and create crosstalk. Here we use broadband Ramsey interferometry to reveal and quantify such spectator-transition crosstalk. Experimentally, the Ramsey Fourier spectra display multiple lines beyond the addressed single-quantum transition. Analytically, we map each line to a pairwise energy difference between qudit levels of the rotating-frame Hamiltonian and assign its weight via compact amplitudes set by the prepared state and the microwave pulse parameters, predicting a deterministic six-branch structure. Numerical time-domain propagation with the experimental sampling reproduces the detuning map, and the measured peak positions coincide with the analytic branch lines without frequency fitting. Together these results provide a practical, spectator-aware framework for multilevel control in the silicon vacancy qudit. The approach offers clear guidance to suppress crosstalk or, conversely, to exploit spectator lines, for example as additional constraints for in situ pulse calibration and for phase-sensitive quantum state and process estimation.

</details>


### [24] [Fair sampling with temperature-targeted QAOA based on quantum-classical correspondence theory](https://arxiv.org/abs/2601.16144)
*Tetsuro Abe,Shu Tanaka*

Main category: quant-ph

TL;DR: 提出SBO-QAOA算法解决标准QAOA在简并基态采样中的偏差问题，通过温度依赖的哈密顿量实现均匀采样


<details>
  <summary>Details</summary>
Motivation: 在具有简并基态的组合优化问题中，公平采样简并解至关重要。然而，标准QAOA随着电路深度增加会在简并态间产生偏差，需要解决这一问题。

Method: 基于量子-经典对应理论，提出SBO-QAOA算法，使用温度依赖的哈密顿量编码吉布斯分布作为基态，通过有限温度值实现均匀分布。

Result: 数值模拟显示，与标准QAOA不同，SBO-QAOA的基态概率收敛到有限温度值，在简并态间实现均匀分布。这些公平性和温度目标特性即使在仅有4个变分参数的线性调度下也能保持。

Conclusion: SBO-QAOA能够有效解决QAOA在简并基态采样中的偏差问题，实现公平采样，且具有鲁棒性，在简单参数设置下仍能保持良好性能。

Abstract: In combinatorial optimization problems with degenerate ground states, fair sampling of degenerate solutions is essential. However, the quantum approximate optimization algorithm (QAOA) with a standard transverse-field mixer induces biases among degenerate states as circuit depth increases. Based on quantum-classical correspondence theory, we propose SBO-QAOA, which employs a temperature-dependent Hamiltonian encoding a Gibbs distribution as its ground state. Numerical simulations show that, unlike standard QAOA, SBO-QAOA yields ground-state probabilities converging to finite-temperature values with uniform distribution among degenerate states. These fairness and temperature-targeting properties are preserved even with only four variational parameters under a linear schedule.

</details>


### [25] [Bright Pulsed Squeezed Light for Quantum-Enhanced Precision Microscopy](https://arxiv.org/abs/2601.15565)
*Alex Terrasson,Lars Madsen,Joel Grim,Warwick Bowen*

Main category: quant-ph

TL;DR: 该研究提出了一种在波导中利用χ²光学参量放大过程高效生成高亮度皮秒脉冲压缩光的技术，实现了-3.2dB的亮度压缩和-3.6dB的真空压缩，校正损耗后对应波导内生成-15.4dB的压缩，这是目前报道的最高亮度振幅脉冲压缩水平。


<details>
  <summary>Details</summary>
Motivation: 压缩光能够将噪声降低到标准量子极限以下，从而提高测量精度。在非线性显微镜应用中，现有技术受限于光损伤和量子极限噪声。虽然显微镜需要明亮的脉冲光以获得最佳性能，但生成和检测高水平的明亮脉冲压缩光仍然具有挑战性。

Method: 采用χ²光学参量放大过程在波导中生成高水平的明亮皮秒脉冲压缩光。通过波导结构实现高效的非线性光学转换，测量了亮度压缩和真空压缩水平，并对损耗进行了校正。

Result: 测量得到-3.2dB的亮度压缩（光学功率与非线性显微镜兼容）和-3.6dB的真空压缩。校正损耗后，这些压缩水平对应波导内生成的-15.4^{+2.7}_{-8.7}dB压缩。这是目前报道的最高亮度振幅脉冲压缩水平。

Conclusion: 该技术成功实现了高效的明亮脉冲压缩光生成，为量子增强非线性显微镜在生物学研究中的广泛应用提供了关键技术支持，有望推动该领域的进一步发展。

Abstract: Squeezed states of light enable enhanced measurement precision by reducing noise below the standard quantum limit. A key application of squeezed light is nonlinear microscopy, where state-of-the-art performance is limited by photodamage and quantum-limited noise. Such microscopes require bright, pulsed light for optimal operation, yet generating and detecting bright pulsed squeezing at high levels remains challenging. In this work, we present an efficient technique to generate high levels of bright picosecond pulsed squeezed light using a $χ^2$ optical parametric amplification process in a waveguide. We measure $-3.2~\mathrm{dB}$ of bright squeezing with optical power compatible with nonlinear microscopy, as well as $-3.6~\mathrm{dB}$ of vacuum squeezing. Corrected for losses, these squeezing levels correspond to $-15.4^{+2.7}_{-8.7}~\mathrm{dB}$ of squeezing generated in the waveguide. The measured level of bright amplitude pulsed squeezing is to our knowledge the highest reported to date, and will contribute to the broader adoption of quantum-enhanced nonlinear microscopy in biological studies.

</details>


### [26] [Stabilizer Thermal Eigenstates at Infinite Temperature](https://arxiv.org/abs/2601.16177)
*Akihiro Hokkyo*

Main category: quant-ph

TL;DR: 该论文提出了一种基于稳定子的方法来构造非可积多体哈密顿量的解析可处理能量本征态，证明了二体哈密顿量的稳定子本征态无法满足k≥4的微观热平衡，并通过构造实例表明该界限是紧的。


<details>
  <summary>Details</summary>
Motivation: 理解如何分析高度纠缠的热本征态是量子多体系统研究的核心挑战。作者旨在开发一种基于稳定子的方法来构造非可积多体哈密顿量的解析可处理能量本征态。

Method: 引入基于稳定子的方法来构造非可积多体哈密顿量的解析可处理能量本征态。专注于无限温度下的零能量本征态，证明了关于二体哈密顿量稳定子本征态微观热平衡的严格不可行定理。通过显式构造二体非可积哈密顿量来展示界限的紧性。

Result: 证明了二体哈密顿量的稳定子本征态无法满足k≥4的微观热平衡，该界限是紧的。构造了二体非可积哈密顿量，其稳定子本征态能够重现所有二体和三体可观测量在无限温度下的热期望值。揭示了由相互作用少体性质所施加的基本约束。

Conclusion: 该研究揭示了稳定子本征态作为二体哈密顿量零能量本征态的结构限制，证明了它们无法满足k≥4的微观热平衡，这一界限是紧的。这为理解量子多体系统中热本征态的性质提供了新的理论框架和基本约束。

Abstract: Understanding how to analyze highly entangled thermal eigenstates is a central challenge in the study of quantum many-body systems. In this Letter, we introduce a stabilizer-based approach to construct analytically tractable energy eigenstates of nonintegrable many-body Hamiltonians. Focusing on zero-energy eigenstates at infinite temperature, we prove a sharp no-go theorem: stabilizer eigenstates of two-body Hamiltonians cannot satisfy $k$-body microscopic thermal equilibrium for any $k\ge4$. We further show that this bound is tight by explicitly constructing two-body nonintegrable Hamiltonians whose stabilizer eigenstates reproduce thermal expectation values for all two-body and all three-body observables. Finally, we identify the structural origin of this limitation by characterizing the conditions under which a stabilizer state can appear as a zero-energy eigenstate of a Hamiltonian, thereby revealing a fundamental constraint imposed by the few-body nature of interactions.

</details>


### [27] [Optimized Slice-Phase Control of Mirror Pulse in Cold-Atom Interferometry with Finite Response Time](https://arxiv.org/abs/2601.15586)
*Xueting Fang,Doudou Wang,Kun Yuan,Jie Deng,Qin Luo,Xiaochun Duan,Minkang Zhou,Lushuai Cao,Zhongkun Hu*

Main category: quant-ph

TL;DR: 量子最优控制设计的镜面脉冲通过自适应切片结构显著提升原子干涉仪性能，增强了对实验不均匀性的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 原子干涉仪需要在高效率和鲁棒性能之间取得平衡，特别是在实验不均匀性条件下。传统脉冲对频率失谐和拉比频率变化敏感，限制了干涉仪的实际应用性能。

Method: 使用梯度上升脉冲工程（GRAPE）方法，通过将控制离散化为非均匀相位切片，为马赫-曾德尔光脉冲原子干涉仪设计优化的镜面脉冲。采用自适应脉冲切片策略，减少实验复杂性。

Result: 优化脉冲显著提高了对失谐范围[-Ω₀,Ω₀]和拉比频率范围[0.1×Ω₀,1.9×Ω₀]（Ω₀=2π×25 kHz）的容忍度，即使在响应时间延迟达1.6 μs时仍保持高传输效率。优化脉冲对耦合不均匀性和速度展宽具有鲁棒性，相比传统脉冲有显著改进。

Conclusion: 自适应脉冲切片方法提供了一种简约策略，在减少实验复杂性的同时增强了鲁棒性和可扩展性，为高精度原子干涉测量中的量子最优控制提供了创新方案。

Abstract: Atom interferometers require both high efficiency and robust performance in their mirror pulses under experimental inhomogeneities. In this work, we demonstrated that quantum optimal control designed mirror pulse significantly enhance interferometer performance by using novel adaptive sliced structure. Using gradient ascent pulse engineering (GRAPE), optimized mirror pulse for a Mach-Zehnder light-pulse atom interferometer was designed by discretizing the control into non-uniform phase slices. This design broadened the tolerence to experimentally relevant variations in detuning $[-Ω_0,Ω_0]$ and Rabi frequency $[0.1\timesΩ_0,1.9\timesΩ_0]$ ($Ω_0=2π\times25$ kHz), while maintaining high transfer efficiency even when the response-time delays up to 1.6 $\rm{μs}$. The optimized pulse was found to be robust to coupling inhomogeneity and velocity spread, offering a significant improvement in robustness over conventional pulse. The adaptive pulse slicing method provides a minimalist strategy that reduces experimental complexity while enhancing robustness and scalability, offering an innovative scheme for quantum optimal control in high precision atom interferometry.

</details>


### [28] [Machine Failure Detection Based on Projected Quantum Models](https://arxiv.org/abs/2601.15641)
*Larry Bowden,Qi Chu,Bernard Cena,Kentaro Ohno,Bob Parney,Deepak Sharma,Mitsuharu Takeori*

Main category: quant-ph

TL;DR: 提出基于量子计算和统计变点检测的故障检测算法，利用投影量子特征映射提升机器监控系统中的异常检测精度，在IBM 133量子比特Heron处理器上验证可行性。


<details>
  <summary>Details</summary>
Motivation: 工业中及时检测机器故障对维持效率和最小化停机时间至关重要，传统方法在复杂噪声环境下可能存在精度不足的问题，需要探索量子计算在工业诊断中的潜力。

Method: 基于量子计算和统计变点检测的故障检测算法，利用投影量子特征映射增强异常检测精度，在IBM 133量子比特Heron量子处理器上实现。

Result: 在基准多维时间序列数据集和真实世界物联网传感器数据集上验证了方法的有效性，能够准确识别噪声时间序列数据中的异常，展示了量子计算在工业维护中的可行性。

Conclusion: 该工作不仅突显了量子计算在工业诊断中的潜力，还为预测性维护领域更复杂的量子算法铺平了道路，展示了量子计算在工业应用中的实际可行性。

Abstract: Detecting machine failures promptly is of utmost importance in industry for maintaining efficiency and minimizing downtime. This paper introduces a failure detection algorithm based on quantum computing and a statistical change-point detection approach. Our method leverages the potential of projected quantum feature maps to enhance the precision of anomaly detection in machine monitoring systems. We empirically validate our approach on benchmark multi-dimensional time series datasets as well as on a real-world dataset comprising IoT sensor readings from operational machines, ensuring the practical relevance of our study. The algorithm was executed on IBM's 133-qubit Heron quantum processor, demonstrating the feasibility of integrating quantum computing into industrial maintenance procedures. The presented results underscore the effectiveness of our quantum-based failure detection system, showcasing its capability to accurately identify anomalies in noisy time series data. This work not only highlights the potential of quantum computing in industrial diagnostics but also paves the way for more sophisticated quantum algorithms in the realm of predictive maintenance.

</details>


### [29] [Enhancing the Size of Phase-Space States Containing Sub-Planck-Scale Structures via Non-Gaussian Operations](https://arxiv.org/abs/2601.15654)
*Arman,Prasanta K. Panigrahi*

Main category: quant-ph

TL;DR: 光子增强的猫态和小猫态相比原始态在相空间灵敏度上具有计量学优势，但能耗更高。通过弱压缩和位移等可及的非经典资源构造压缩态和叠加态，其光子增强变体与奇偶匹配的猫态和KS态相比，在某些参数区间表现出更高的保真度、更大振幅和更好的计量性能。


<details>
  <summary>Details</summary>
Motivation: 研究光子增强操作如何改善猫态和小猫态的计量学性能，探索通过可及的非经典资源（弱压缩和位移）构造量子态，并分析这些态在量子计量和量子纠错方面的应用潜力。

Method: 使用弱压缩和位移操作构造压缩态和两种叠加态（压缩猫态和对称压缩态），生成这些态的光子增强变体。通过量子Fisher信息和保真度与奇偶匹配的猫态和KS态进行比较，分析QFI等值线图来识别高保真度和大振幅的参数区间。

Result: 光子增强操作通过增加振幅扩展相空间区域，提高了猫态和小猫态的相空间灵敏度。KS态在某些参数区间表现出高保真度和大振幅，可通过高斯操作和光子增强制备。压缩和光子增强的猫态也显示出改进的计量学性能，且更大的相空间面积减小了干涉条纹尺寸，增强了猫码中量子纠错的有效性。

Conclusion: 光子增强操作能有效提升猫态和小猫态的计量学性能，通过增加振幅扩展相空间区域，不仅改善了相位灵敏度，还增强了量子纠错能力。这些发现为利用可及的非经典资源制备高性能量子态提供了新途径。

Abstract: We observe a metrological advantage in phase-space sensitivity for photon-added cat and kitten states over their original forms, due to phase-space broadening from increased amplitude via photon addition, albeit with higher energy cost. Using accessible non-classical resources, weak squeezing and displacement, we construct a squeezed state and two superposed states: the squeezed cat state and the symmetrically squeezed state. Their photon-added variants are compared with parity-matched cat and KSs using quantum Fisher information and fidelity. The QFI isocontours reveal regimes where KS exhibit high fidelity and large amplitude, enabling their preparation via Gaussian operations and photon addition. Similar regimes are identified for cat states enhanced by squeezing and photon addition, demonstrating improved metrological performance. Moreover, increased amplitude and thus larger phase-space area reduces the size of interferometric fringes, enhancing the effectiveness of quantum error correction in cat codes.

</details>


### [30] [Quantum-HPC hybrid computation of biomolecular excited-state energies](https://arxiv.org/abs/2601.15677)
*Kentaro Yamamoto,Riku Masui,Takahito Nakajima,Miwako Tsuji,Mitsuhisa Sato,Peter Schow,Lukas Heidemann,Matthew Burke,Philipp Seitz,Oliver J. Backhouse,Juan W. Pedersen,John Children,Craig Holliman,Nathan Lysne,Daichi Okuno,Seyon Sivarajah,David Muñoz Ramo,Alex Chernoguzov,Ross Duncan*

Main category: quant-ph

TL;DR: 开发了ONIOM框架下的混合量子-经典计算工作流，结合超级计算机Fugaku和Quantinuum Reimei量子计算机，用于精确模拟生物分子化学反应


<details>
  <summary>Details</summary>
Motivation: 需要精确模拟复杂的生物分子化学反应，特别是处理活性位点（如蛋白质）与大型弱相关分子环境之间的相互作用

Method: 在ONIOM框架内开发工作流，采用分层方法：使用量子计算机精确处理活性位点，使用超级计算机处理大型分子环境

Result: 成功实现了混合量子-经典计算平台，能够精确模拟生物分子化学反应，标志着可扩展精确模拟的重要里程碑

Conclusion: 该混合平台为复杂生物分子反应的可扩展精确模拟提供了有效解决方案，结合了量子计算和经典计算的优势

Abstract: We develop a workflow within the ONIOM framework and demonstrate it on the hybrid computing system consisting of the supercomputer Fugaku and the Quantinuum Reimei trapped-ion quantum computer. This hybrid platform extends the layered approach for biomolecular chemical reactions to accurately treat the active site, such as a protein, and the large and often weakly correlated molecular environment. Our result marks a significant milestone in enabling scalable and accurate simulation of complex biomolecular reactions

</details>


### [31] [Fractional squeezing: spectra and dynamics from generalized squeezing Hamiltonian with fractional orders](https://arxiv.org/abs/2601.15693)
*Sahel Ashhab*

Main category: quant-ph

TL;DR: 该论文将广义挤压问题推广到包含分数阶挤压阶数n，能够确定行为发生定性变化的关键点位置，并准确预测这些关键点处的行为，解决了传统计算方法难以处理的问题。


<details>
  <summary>Details</summary>
Motivation: 传统计算方法在处理广义挤压问题时，难以准确确定行为发生定性变化的关键点位置，也无法准确预测这些关键点处的行为。因此需要一种新的方法来处理分数阶挤压阶数的情况。

Method: 将广义挤压问题推广到包含分数阶挤压阶数n，通过数值计算确定关键点位置，分析谱从连续到离散的转变点，以及振荡从渐近无限振幅到有限振幅的转变点。

Result: 成功确定了谱从连续变为离散的关键点，以及振荡从渐近无限振幅变为有限振幅的关键点。在大n区域进行了数值研究，并提供了与数值结果一致的直观解释。

Conclusion: 通过将广义挤压问题推广到分数阶挤压阶数，能够准确确定行为发生定性变化的关键点位置，并预测这些关键点处的行为，为传统计算方法难以处理的问题提供了有效解决方案。

Abstract: We generalize the generalized-squeezing problem to include fractional values of the squeezing order $n$. This approach allows us to determine the locations of critical points at which qualitative changes in behaviour occur and accurately predict the behaviour at these critical points, which are challenging for conventional computational methods. Based on our numerical calculations, we identify with a high degree of confidence the point at which the spectrum turns from continuous to discrete and the point at which oscillations turn from having asymptotically infinite amplitudes to finite amplitudes. Furthermore, we numerically investigate the behaviour in the large $n$ regime and provide an intuitive explanation that coincides with the numerical results.

</details>


### [32] [Unsplit Spreading: An Overlooked Signature of Long-Range Interaction](https://arxiv.org/abs/2601.15752)
*Jian-Feng Wu,Yi Huang,Yu-Xiang Zhang*

Main category: quant-ph

TL;DR: 传统晶格模型中平滑色散关系导致局域激发分裂成反向传播波包，只有奇异色散关系才能实现不分裂传播，这是长程相互作用的特征


<details>
  <summary>Details</summary>
Motivation: 传统晶格模型假设色散关系是平滑函数，这导致局域激发会分裂成反向传播波包。作者发现不分裂传播现象在2014年的量子模拟实验中已经可见，但一直未被识别为独特的物理效应

Method: 证明平滑色散关系必然导致局域激发分裂，分析长程相互作用如何产生奇异色散特征，研究1D和2D亚波长原子阵列等开放量子系统中的不分裂传播现象

Result: 发现不分裂传播只能发生在色散关系具有奇异特征时，而长程相互作用恰好能产生这种奇异特征。在亚波长原子阵列的长寿命亚辐射态中，有效色散具有所需的奇异性

Conclusion: 不分裂传播是奇异能带结构的实验可观测特征，可作为长程物理的明确证据，为理解开放量子系统中的波传播提供了新视角

Abstract: In conventional lattice models, the dispersion relation $ω(k)$ is assumed to be a smooth function. We prove that this smoothness implies the splitting of an initially localized excitation into counter-propagating wave packets. Consequently, unsplit spreading can occur only when $ω(k)$ develops singular features, precisely what long-range interactions enable. Remarkably, this phenomenon was clearly visible in published quantum simulation experiments as early as 2014, yet it has remained unrecognized or discussed as a distinct physical effect. We show that unsplit spreading emerges in realistic open quantum systems, such as 1D and 2D subwavelength atomic arrays, where the long-lived subradiant states host effective dispersion with the required singularities. Our work establishes unsplit spreading as an experimentally accessible, smoking-gun signature of singular band structure induced by long-range physics.

</details>


### [33] [Improving the efficiency of QAOA using efficient parameter transfer initialization and targeted-single-layer regularized optimization with minimal performance degradation](https://arxiv.org/abs/2601.15760)
*Shubham Patel,Utkarsh Mishra*

Main category: quant-ph

TL;DR: 该研究探索了在量子近似优化算法（QAOA）中，通过参数转移初始化结合目标单层优化来求解MaxCut问题。在无权重图中，该方法能达到98.88%的最优性能，计算速度提升8.06倍；但在权重图中效果较差。研究发现复杂参数空间会使完整优化陷入局部最优，而L2正则化能减少此类情况。


<details>
  <summary>Details</summary>
Motivation: 量子近似优化算法（QAOA）在组合优化问题中具有应用潜力，但传统方法计算成本高。研究者希望找到更高效的参数初始化策略，减少优化层数，同时保持接近最优的性能表现。

Method: 采用参数转移初始化策略，将预训练参数转移到新问题，然后进行目标单层优化。在三种图结构（3正则图、Erdos Renyi图、Barabasi Albert图）上测试MaxCut问题，比较无权重图和权重图的表现。使用L2正则化平滑解空间，帮助优化器找到更好的参数。

Result: 在无权重图中，参数转移方法达到0.9443的平均近似比（完整优化为0.9551），相当于98.88%的最优性能，计算速度提升8.06倍。但在权重图中，对于节点较多的图，最优性能低于90%。有趣的是，在8.92%的测试案例中，目标单层优化优于完整优化，表明复杂参数空间会使完整优化陷入局部最优。使用L2正则化后，这类不一致案例从8.92%降至3.81%。

Conclusion: 高效的参数初始化和目标单层优化可以显著提升QAOA的效率，同时性能损失最小。该方法特别适用于无权重图，但在权重图中效果有限。L2正则化有助于平滑解空间，减少优化器陷入局部最优的情况，提高优化稳定性。

Abstract: Quantum approximate optimization algorithm (QAOA) have promising applications in combinatorial optimization problems (COPs). We investigated the MaxCut problem in three different families of graphs using QAOA ansats with parameter transfer initialization followed by targeted single layer optimization. For 3 regular (3R), Erdos Renyi (ER), and Barabasi Albert (BA) graphs, the parameter transfer approach achieved mean approximation ratios of 0.9443 for targeted-single layer optimization as compared to 0.9551 of full optimization. It represents 98.88 percent optimal performance, with 8.06 times computational speedup in unweighted graphs. But, in weighted graph families, optimal performance is relatively low (less than 90 percent) for higher nodes graph, suggesting parameter transfer followed by targeted-single-layer optimization is not ideal for weighted graph families, however, we find that for some weighted families (weighted 3-regular) this approach works perfectly. In 8.92 percent test cases, targeted single layer optimization outperformed the full optimization, indicating that complex parameter landscape can trap full optimization in sub-optimal local minima. To mitigate this inconsistency, ridge (L2) regularization is used to smoothen the solution landscape, which helps the optimizer to find better optimum parameters during full optimization and reduces these inconsistent test cases from 8.92 percent to 3.81 percent. This work demonstrates that efficient parameter initialization and targeted-single-layer optimization can improve the efficiency of QAOA with minimal performance degradation.

</details>


### [34] [Fermion Doubling in Dirac Quantum Walks](https://arxiv.org/abs/2601.15885)
*Chaitanya Gupta,Anthony J. Short*

Main category: quant-ph

TL;DR: 该论文研究量子行走中的费米子倍增问题，提出了一种避免倍增子和伪倍增子的量子行走家族，同时仍能在连续极限下模拟狄拉克方程。


<details>
  <summary>Details</summary>
Motivation: 量子行走可以模拟狄拉克粒子，但存在费米子倍增问题，其中高动量态会产生额外的低能解，这些解表现得像狄拉克粒子。在"二次量子化"版本中，这些倍增子会导致虚假解，而伪倍增子则可能引起真空稳定性问题。

Method: 通过允许行走者停留在原点的概率非零，构建了一个量子行走家族。与传统的狄拉克行走总是零停留概率不同，这种方法消除了倍增子和伪倍增子，同时保持连续极限下的狄拉克方程模拟。

Result: 提出的量子行走家族成功消除了倍增子和伪倍增子，但仍保留少量不直接对应狄拉克粒子的额外低能解。这些行走在连续极限下仍能模拟狄拉克方程。

Conclusion: 通过修改量子行走的停留概率，可以解决费米子倍增问题，为模拟狄拉克粒子提供更稳健的离散时空模型，尽管仍存在少量非狄拉克低能解需要进一步研究。

Abstract: We consider discrete spacetime models known as quantum walks, which can be used to simulate Dirac particles. In particular we look at fermion doubling in these models, in which high momentum states yield additional low energy solutions which behave like Dirac particles. The presence of doublers carries over to the `second quantised' version of the walks represented by quantum cellular automata, which may lead to spurious solutions when introducing interactions. Moreover, we also consider pseudo-doublers, which have high energy but behave like low energy Dirac particles, and cause potential problems regarding the stability of the vacuum. To address these issues, we propose a family of quantum walks, that are free of these doublers and pseudo-doublers, but still simulate the Dirac equation in the continuum limit. However, there remain a small number of additional low energy solutions which do not directly correspond to Dirac particles. While the conventional Dirac walk always has a zero probability for the walker staying at the same point, we obtain the family of walks by allowing this probability to be non-zero.

</details>


### [35] [Automated quantum circuit optimization with randomized replacements](https://arxiv.org/abs/2601.15934)
*Marcin Szyniszewski,Aleks Kissinger,Noah Linden,Paul Skrzypczyk*

Main category: quant-ph

TL;DR: 本文提出了一种基于ZX演算的近似量子电路重写协议，通过允许近似局部变换和使用混合量子通道来近似纯电路，实现了量子电路的资源优化。


<details>
  <summary>Details</summary>
Motivation: 量子电路优化对最大化当前和近期量子设备的效用至关重要。传统自动优化技术主要关注保持酉矩阵不变，但本文发现通过允许近似局部变换和使用混合量子通道，可以获得显著的资源减少机会。

Method: 提出了一种基于ZX演算的自动近似电路重写协议，采用贪婪策略选择性地将具有小相位角的ZX图替换为恒等算符和精心选择的过旋转的随机混合，旨在减少总体门数期望值，同时保持在严格的误差预算内。

Result: 该方法在随机量子电路中实现了适度的两量子比特门数减少，在结构化电路（如量子傅里叶变换）中实现了显著减少。该方法将实验噪声转化为精心设计的随机噪声，在平均性能上优于许多其他近似方法。

Conclusion: 混合通道近似方法有潜力增强未来量子电路性能，为超越纯酉通道的资源感知自动量子编译指出了新方向。

Abstract: Quantum circuit optimization - the process of transforming a quantum circuit into an equivalent one with reduced time and space requirements - is crucial for maximizing the utility of current and near-future quantum devices. While most automated optimization techniques focus on transforming circuits into equivalent ones that implement the same unitary, we show that substantial new opportunities for resource reduction can be achieved by (1) allowing approximate local transformations and (2) employing mixed quantum channels to approximate pure circuits. Our novel automated protocol for approximate circuit rewriting is a refined evolution of automated optimization techniques based on the ZX-calculus, where we add a greedy strategy that selectively replaces ZX-diagrams with small phase angles with stochastic mixtures of the identity and carefully chosen over-rotations, which are designed to reduce the overall gate count in expectation while staying within a strict error budget. This approach yields modest two-qubit gate count reduction in random quantum circuits, and achieves a substantial reduction in structured circuits such as the quantum Fourier transform. Fundamentally, our protocol converts experimental noise due to gate applications into deliberately engineered random noise, outperforming many other approximation methods on average. These results highlight the potential of mixed-channel approximations to enhance future quantum circuit performance, suggesting new directions for resource-aware automated quantum compilation beyond pure unitary channels.

</details>


### [36] [Renormalization Treatment of IR and UV Cutoffs in Waveguide QED and Implications to Numerical Model Simulation](https://arxiv.org/abs/2601.15945)
*Romain Piron,Akihito Soeda*

Main category: quant-ph

TL;DR: 该论文提出了波导-QED模型中重整化关系的非微扰第一性原理推导，考虑了数值模拟中必要的红外和紫外截断，建立了裸模型参数与可观测物理量之间的联系，并展示了如何用于参数化最小频带模拟以降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 波导-QED模型的数值模拟需要引入红外和紫外截断，但如何将裸模型参数与可观测物理量正确关联，以及如何高效参数化模拟同时保持物理准确性，是当前研究的关键挑战。

Method: 采用时域方法描述原子动力学，推导出连接裸模型参数与可观测原子频率和衰减速率的显式表达式，验证其与散射理论的一致性，并与标准费曼图建立联系。

Result: 获得了明确的重整化关系，能够将裸参数映射到可观测物理量，并展示了如何利用这些关系参数化具有最小频带的模拟，在保持物理准确性的同时显著降低计算成本。

Conclusion: 该工作为波导-QED模型提供了系统化的重整化框架，使高效可靠的多光子光-物质相互作用模拟成为可能，为复杂量子光学系统的数值研究铺平了道路。

Abstract: We present a non-perturbative, first-principles derivation of renormalization relations for waveguide-QED models, explicitly accounting for the infrared (IR) and ultraviolet (UV) cutoffs that are necessarily introduced in numerical simulations. By formulating the atomic dynamics in the time domain, we obtain explicit expressions linking the bare model parameters to the physically observable atomic frequency and decay rate, and verify their consistency with scattering theory. We further connect these results to standard Feynman diagrams, providing a transparent physical interpretation and ensuring the generality of the approach. Finally, we show how these renormalization relations can be used to parameterize simulations with a minimal frequency bandwidth, simultaneously preserving physical accuracy and reducing computational cost, thereby paving the way for efficient and reliable multi-photon light-matter simulations.

</details>


### [37] [Semiclassical entanglement entropy for spin-field interaction](https://arxiv.org/abs/2601.15986)
*Matheus V. Scherer,Lea F. Santos,Alexandre D. Ribeiro*

Main category: quant-ph

TL;DR: 该论文研究了自旋与玻色场相互作用的一般二分量子系统，开发了描述两个子系统之间纠缠动力学的半经典框架，通过引入复轨迹显著提高了半经典描述的准确性。


<details>
  <summary>Details</summary>
Motivation: 研究自旋与玻色场相互作用量子系统中的纠缠动力学，建立能够超越Ehrenfest时间限制的半经典描述框架，以更好地理解量子纠缠的经典对应。

Method: 采用半经典近似方法，推导出仅依赖于经典轨迹的纠缠熵表达式；通过将经典相空间解析延拓到复域，识别出额外的复轨迹；结合实轨迹和复轨迹来精确描述纠缠动力学。

Result: 复轨迹的引入显著提高了半经典描述的准确性，即使在Ehrenfest时间之外也能精确捕捉纠缠动力学；通过代表性示例明确展示了实轨迹和复轨迹在再现量子纠缠熵中的作用。

Conclusion: 通过将经典相空间延拓到复域并包含复轨迹，可以构建高度精确的半经典框架来描述量子纠缠动力学，这为理解量子纠缠的经典对应提供了新途径。

Abstract: We study a general bipartite quantum system consisting of a spin interacting with a bosonic field, with the initial state prepared as the product of a spin coherent state and a canonical coherent state. Our goal is to develop a semiclassical framework to describe the entanglement dynamics between these two subsystems. Using appropriate approximations, we derive a semiclassical expression for the entanglement entropy that depends exclusively on the trajectories of the underlying classical description. By analytically extending the classical phase space into the complex domain, we identify additional complex trajectories that significantly improve the accuracy of the semiclassical description. The inclusion of these complex trajectories allows us to capture the entanglement dynamics with remarkable precision, even well beyond the Ehrenfest time. The approach is illustrated with a representative example, where the role of real and complex trajectories in reproducing the quantum entanglement entropy is explicitly demonstrated.

</details>


### [38] [Engineering quantum Mpemba effect by Liouvillian skin effect](https://arxiv.org/abs/2601.16002)
*Xiang Zhang Chen Sun,Fuxiang Li*

Main category: quant-ph

TL;DR: 该论文提出利用开放量子系统中的Liouvillian皮肤效应来工程化量子Mpemba效应，并发现了一种新型QME-III


<details>
  <summary>Details</summary>
Motivation: 量子Mpemba效应（QME）描述了初始状态离平衡态更远的系统反而比离平衡态更近的系统弛豫得更快的现象。论文旨在通过Liouvillian皮肤效应（LSE）来工程化实现QME，并探索其物理机制

Method: 基于二次Lindblad方程，考虑两种具体案例来设计初始状态。利用Liouvillian皮肤效应的空间分布特性为初始状态制备提供直接途径，从而在开放量子系统中实现量子Mpemba效应

Result: 成功实现了量子Mpemba效应，并发现了一种新型QME（QME-III），其特征是在两个不同时间点出现两次Hilbert-Schmidt距离的反转。Liouvillian皮肤效应为理解QME提供了更直观的物理图像

Conclusion: Liouvillian皮肤效应不仅是实现量子Mpemba效应的理想平台，其空间分布特性还简化了初始状态制备，为实验实现提供了便利。该研究深化了对量子Mpemba效应的理解，并揭示了新的物理现象

Abstract: We propose a new approach to engineer the quantum Mpemba effect (QME) -- wherein an initial state farther from system relaxes faster than a close one -- by the Liouvillian skin effect (LSE) in open quantum systems. Moreover, the LSE serves as an ideal platform for realizing the QME and the spatial profile of the LSE provides a straightforward pathway for the initial state preparation, thereby enabling readily accessible experimental preparation. Focusing on the quadratic Lindbladians, we consider two concrete cases to design the initial states, thereby realizing the QME. Interestingly, we uncover a new kind of QME (QME-III) that is distinct from the two typical scenarios, manifested as two reversals in the Hilbert-Schmidt distance at two different times. In particular, the LSE provides a physically more intuitive understanding of the QME.

</details>


### [39] [Wigner's Friend as a Circuit: Inter-Branch Communication Witness Benchmarks on Superconducting Quantum Hardware](https://arxiv.org/abs/2601.16004)
*Christopher Altman*

Main category: quant-ph

TL;DR: 在IBM量子硬件上实现并基准测试Violaris提出的电路家族，用于估计操作性的分支间通信见证量，通过编译的Wigner朋友式电路产生的经典测量记录相关性来评估。


<details>
  <summary>Details</summary>
Motivation: 研究在真实量子设备噪声和编译约束下，如何评估非理想信道相对于校准设备噪声的可检测性，为量子通信协议提供可复现的操作约束管道。

Method: 在IBM量子硬件上实现五量子比特的Violaris电路协议，将其作为单电路内的寄存器间消息传输模式（而非物理信号传输），使用ibm_fez后端执行20000次测量，分析分支条件演化、控制传输操作和条件测量上下文相关性。

Result: 观察到基于布居数的可见度为0.877，沿正交轴的相干性见证量为0.840和-0.811，相位敏感幅度约为1.17。可见度度量对某些退相干类别不敏感，而相干性见证量对非对角噪声提供互补敏感性。

Conclusion: 该工作不测试或区分量子力学解释，而是提供了一个可复现的操作约束管道，用于评估非理想信道相对于校准设备噪声的可检测性，为量子硬件上的通信协议基准测试提供实用工具。

Abstract: We implement and benchmark on IBM Quantum hardware the circuit family proposed by Violaris for estimating operational inter-branch communication witnesses, defined as correlations in classical measurement records produced by compiled Wigner's-friend-style circuits. We realize a five-qubit instance of the protocol as an inter-register message-transfer pattern within a single circuit, rather than physical signaling, and evaluate its behavior under realistic device noise and compilation constraints. The circuit encodes branch-conditioned evolution of an observer subsystem whose dynamics depend on a control qubit, followed by a controlled transfer operation that probes correlations between conditional measurement contexts.
  Executing on the ibm_fez backend with 20000 shots, we observe population-based visibility of 0.877, coherence witnesses of 0.840 and -0.811 along orthogonal axes, and a phase-sensitive magnitude of approximately 1.17. While the visibility metric is insensitive to some classes of dephasing, the coherence witnesses provide complementary sensitivity to off-diagonal noise.
  This work does not test or discriminate among interpretations of quantum mechanics. Instead, it provides a reproducible operational constraint pipeline for evaluating detectability of non-ideal channels relative to calibrated device noise.

</details>


### [40] [Echoed Random Quantum Metrology](https://arxiv.org/abs/2601.16026)
*Dong-Sheng Liu,Zi-Jie Chen,Ziyue Hua,Yilong Zhou,Qing-Xuan Jie,Weizhou Cai,Ming Li,Luyan Sun,Chang-Ling Zou,Xi-Feng Ren,Guang-Can Guo*

Main category: quant-ph

TL;DR: 通过随机脉冲驱动克尔非线性模式，无需复杂量子控制即可实现接近海森堡极限的灵敏度


<details>
  <summary>Details</summary>
Motivation: 传统量子计量学需要制备纠缠态或压缩态等复杂量子探针态，对系统参数校准和量子控制优化要求高，限制了可扩展性和鲁棒性

Method: 引入回声随机过程，通过随机脉冲驱动克尔非线性模式，产生亚普朗克相空间结构，无需复杂量子控制

Result: 该方法能实现接近海森堡极限的灵敏度，对驱动参数范围宽泛，对控制波动和光子损耗具有鲁棒性

Conclusion: 为高维希尔伯特空间中的量子增强计量学提供了一种实用、硬件高效、可扩展且无需优化的新途径

Abstract: Quantum metrology typically demands the preparation of exotic quantum probe states, such as entangled or squeezed states, to surpass classical limits. However, the need for carefully calibrated system parameters and finely optimized quantum controls imposes limitations on scalability and robustness. Here, we circumvent these limitations by introducing an echoed random process that achieves sensitivity approaching the Heisenberg limit while remaining blind to the random probe state. We demonstrate that by simply driving a Kerr nonlinear mode with random pulses, the emergence of sub-Planck phase-space structures grants high sensitivity, eliminating the need for complex quantum control. The protocol is statistically robust, yielding high performance across broad driving parameter ranges while exhibiting resilience to control fluctuations and photon loss. Broadly applicable to both bosonic and qubit platforms, our work reveals a practical, hardware-efficient, scalable, and optimization-free route to quantum-enhanced metrology in high-dimensional Hilbert spaces.

</details>


### [41] [Robust Quantum Algorithmic Binary Decision-Making on Displacement Signals](https://arxiv.org/abs/2601.16081)
*Aishwarya Majumdar,Yuan Liu*

Main category: quant-ph

TL;DR: 提出基于广义量子信号处理干涉测量（GQSPI）的框架，用于解决量子位移信号参数检测的二元决策问题，将主动二元假设检验转化为多项式逼近问题，实现低错误概率。


<details>
  <summary>Details</summary>
Motivation: 在量子领域中，信号可能表现为玻色子相空间中的位移或相移算子。对于嵌入在这种位移算子中的实参数β，判断β是否在不对称阈值区间[β_{-th}, β_{+th}]内是一个二元决策问题。需要一种能够处理任意阈值、在单次或少数几次测量中做出决策的量子传感框架。

Method: 基于广义量子信号处理干涉测量（GQSPI）框架，在混合量子比特-玻色子振荡器系统上实现。将主动二元假设检验问题转化为多项式逼近问题，通过电路深度d来控制决策精度。

Result: 实现了低决策错误概率p_{err}，约为O(1/d·log(d))，其中d为电路深度。分析了β为确定性参数和从已知先验分布随机抽取两种情况。在退相干噪声下表现出鲁棒性，并扩展到多阈值情况。

Conclusion: 提出的框架能够对任意一般位移信号在任意阈值上进行决策，仅需单次或少数几次测量，为量子传感中的二元决策问题提供了有效的解决方案。

Abstract: A relevant signal in the quantum domain may manifest as a displacement or a phase shift operator in the bosonic phase space. For a real parameter $β$ embedded in such a displacement operator, the task of determining if $β\in [β_{-th}, β_{+th}]$ for real asymmetric thresholds $(β_{-th} \ne -β_{+th})$ is a binary decision problem. We propose a framework based on generalized quantum signal processing interferometry (GQSPI) on hybrid qubit-bosonic oscillator systems that addresses this parameter detection problem by recasting the practical task of active binary hypothesis testing on quantum systems to that of a polynomial approximation. We achieve a small decision error probability $p_{err}$ on the order of $O(\frac{1}{d}\log{(d)})$, with $d$ as the circuit depth. We analyze the protocol when (i) $β$ is a deterministic parameter, and (ii) when $β$ is drawn randomly from a known prior distribution. The performance of the sensing protocol under dephasing noise is also shown to be robust. We further extend our protocol from two thresholds to more general multi-threshold cases as well. Overall, the proposed framework enables decision-making over arbitrary thresholds for any general displacement signal in a single or a few shots.

</details>


### [42] [Quantum Metrology under Coarse-Grained Measurement](https://arxiv.org/abs/2601.16106)
*Byeong-Yoon Go,Geunhee Gwak,Young-Do Yoon,Sungho Lee,Nicolas Treps,Jiyong Park,Young-Sik Ra*

Main category: quant-ph

TL;DR: 该论文研究了量子测量中的粗粒度问题，通过理论和实验证明，即使只有两个分箱的极端粗粒度测量也能实现超越标准量子极限的相位估计精度，并遵循海森堡标度。


<details>
  <summary>Details</summary>
Motivation: 量子计量学虽然能实现超越经典极限的测量精度，但其性能通常容易受到实验缺陷的影响。以往研究主要关注量子态和操作的缺陷，本文则专门研究量子测量中的粗粒度问题。

Method: 使用压缩真空和激光输入的干涉仪，分析零差探测中的粗粒度如何影响相位估计精度。在不同粗粒度条件下评估费舍尔信息，确定每种情况下的最优估计策略。实验中采用矩量法确定最优估计策略，并提出了适用于一般实验设置的校准程序。

Result: 即使只有两个分箱的极端粗粒度测量，也能实现超越标准量子极限的相位估计，并遵循海森堡标度。实验中，仅使用两个分箱就观察到相对于理想测量的经典方法有1.2 dB的量子增强，随着分箱数增加，量子增强可提高至3.8 dB。

Conclusion: 这些结果表明，即使在存在严重实验缺陷的情况下，通过粗粒度测量也能实现量子增强，为实际应用中的量子计量学提供了可行的技术路径。

Abstract: While quantum metrology enables measurement precision beyond classical limits, its performance is often susceptible to experimental imperfections. Most prior studies have focused on imperfections in quantum states and operations. Here, we investigate the effect of coarse graining in quantum measurement through both theoretical analysis and experimental demonstration. Using an interferometer with a squeezed vacuum and a laser input, we analyze how coarse graining in homodyne detection affects the precision of phase estimation. We evaluate the Fisher information under various coarse-graining conditions and determine, in each case, an optimal estimation strategy that saturates the Cramér-Rao bound. Remarkably, even extremely coarse-grained measurement -- with only two bins -- enables phase estimation beyond the standard quantum limit and even achieves a precision that follows the Heisenberg scaling. We experimentally demonstrate quantum-enhanced phase estimation under coarse-grained homodyne detection. To determine an optimal estimation strategy, we employ the method of moments and present calibration procedures that enable its application to general experimental settings. Using only two bins, we observe a quantum enhancement of 1.2 dB compared to the classical method using the ideal measurement, improving towards 3.8 dB as the bin number increases. These results highlight a practical pathway to achieving quantum enhancement under the presence of severe experimental imperfections.

</details>


### [43] [Exceptional points in Gaussian channels: diffusion gauging and drift-governed spectrum](https://arxiv.org/abs/2601.16121)
*Frank Ernesto Quintela Rodríguez*

Main category: quant-ph

TL;DR: 论文证明了线性开放量子系统中Liouvillian谱与噪声强度无关的原理，并分别在连续时间和离散时间下为多模玻色高斯系统建立了精确的数学框架。


<details>
  <summary>Details</summary>
Motivation: McDonald和Clerk先前的工作表明线性开放量子系统的Liouvillian谱与噪声强度无关，本文旨在将这一噪声无关原理精确化，并扩展到更一般的系统框架中。

Method: 1. 在连续时间下，针对多模玻色高斯马尔可夫半群，利用Hurwitz漂移和Lyapunov方程构造时间无关的高斯相似变换来消除扩散项；2. 在离散时间下，针对一般稳定的多模玻色高斯通道，构造显式的高斯相似变换在通道参数化层面消除扩散项。

Result: 证明了漂移项完全控制特征值和非对角化性质，而扩散项决定稳态和本征算子的结构。通过单模压缩储层Lindbladian和非马尔可夫单模高斯通道的实例验证了该方法，能够解析获得异常点流形和相关的规范协方差。

Conclusion: 成功将噪声无关原理精确化并扩展到连续时间和离散时间的多模玻色高斯系统中，建立了漂移与扩散的分离框架，为分析开放量子系统的谱特性提供了新的理论工具。

Abstract: McDonald and Clerk [Phys.\ Rev.\ Research 5, 033107 (2023)] showed that for linear open quantum systems the Liouvillian spectrum is independent of the noise strength. We first make this noise-independence principle precise in continuous time for multimode bosonic Gaussian Markov semigroups: for Hurwitz drift, a time-independent Gaussian similarity fixed by the Lyapunov equation gauges away diffusion for all times, so eigenvalues and non-diagonalizability are controlled entirely by the drift, while diffusion determines steady states and the structure of eigenoperators. We then extend the same separation to discrete time for general stable multimode bosonic Gaussian channels: for any stable Gaussian channel, we construct an explicit Gaussian similarity transformation that gauges away diffusion at the level of the channel parametrization. We illustrate the method with a single-mode squeezed-reservoir Lindbladian and with a non-Markovian family of single-mode Gaussian channels, where the exceptional-point manifolds and the associated gauging covariances can be obtained analytically.

</details>


### [44] [Polynomial-time thermalization and Gibbs sampling from system-bath couplings](https://arxiv.org/abs/2601.16154)
*Samuel Slezak,Matteo Scandi,Álvaro M. Alhambra,Daniel Stilck França,Cambyse Rouzé*

Main category: quant-ph

TL;DR: 论文研究了两种Lindblad过程的收敛速度：重复交互Gibbs采样算法和开放多体量子热化模型，证明了它们在多个非对易系统中的多项式时间收敛性


<details>
  <summary>Details</summary>
Motivation: 理解Lindblad方程收敛到稳态的速度对于限制算法运行时间和热化时间尺度至关重要，这些过程模拟了弱耦合到浴池的系统

Method: 研究两种Lindblad过程家族：重复交互Gibbs采样算法和开放多体量子热化模型，使用新的技术结果从准局部Lindbladians外推谱隙下界到控制这些动力学的非局部生成元

Result: 证明了两种过程在多个非对易系统中的多项式时间收敛，包括高温局部晶格、弱相互作用费米子和1D自旋链

Conclusion: 简单的耗散量子算法可以制备复杂的Gibbs态，Lindblad动力学准确捕捉热弛豫过程

Abstract: Many physical phenomena, including thermalization in open quantum systems and quantum Gibbs sampling, are modeled by Lindbladians approximating a system weakly coupled to a bath. Understanding the convergence speed of these Lindbladians to their steady states is crucial for bounding algorithmic runtimes and thermalization timescales. We study two such families of processes: one characterizing a repeated-interaction Gibbs sampling algorithm, and another modeling open many-body quantum thermalization. We prove that both converge in polynomial time for several non-commuting systems, including high-temperature local lattices, weakly interacting fermions, and 1D spin chains. These results demonstrate that simple dissipative quantum algorithms can prepare complex Gibbs states and that Lindblad dynamics accurately capture thermal relaxation. Our proofs rely on a novel technical result that extrapolates spectral gap lower bounds from quasi-local Lindbladians to the non-local generators governing these dynamics.

</details>


### [45] [Robust Bell Nonlocality from Gottesman-Kitaev-Preskill States](https://arxiv.org/abs/2601.16189)
*Xiaotian Yang,Santiago Zamora,Rafael Chaves,Ulrik L. Andersen,Jonatan Bohr Brask,A. de Oliveira Junior*

Main category: quant-ph

TL;DR: GKP编码能否让零差探测成为连续变量系统中贝尔非定域性的实用工具？研究发现：在双粒子系统中，GKP编码的贝尔态无法违反CHSH不等式，但在多粒子系统中，有限压缩的GKP编码GHZ态和W态可以通过零差探测展示强多体非定域性。


<details>
  <summary>Details</summary>
Motivation: 连续变量系统中基于零差探测的贝尔测试受到严重限制。本文研究Gottesman-Kitaev-Preskill (GKP) 编码是否能够将零差探测转变为揭示贝尔非定域性的实用工具，特别是在物理上可行的测量模型下。

Method: 采用物理上可行的模型：各方执行零差探测，并通过固定周期性分箱将连续结果数字化，对应逻辑泡利测量。在此框架下，推导了双粒子系统的不可行性定理，并研究了多粒子系统中有限压缩的GKP编码GHZ态和W态。

Result: 1. 双粒子系统：GKP编码的贝尔态无法违反CHSH不等式（双粒子不可行定理）。2. 多粒子系统：有限压缩的GKP编码GHZ态和W态能够违反多体贝尔不等式，仅使用零差探测。研究量化了所需的压缩阈值和对损耗的鲁棒性。

Conclusion: GKP编码为连续变量系统中基于零差探测的贝尔测试提供了可行路径。虽然双粒子系统存在限制，但多粒子系统展示了通过零差探测揭示强非定域性的潜力，为实验实现提供了具体参数指导。

Abstract: Bell tests based on homodyne detection are strongly constrained in continuous-variable systems. Can Gottesman-Kitaev-Preskill (GKP) encoding turn homodyne detection into a practical tool for revealing Bell nonlocality? We consider a physically motivated model in which each party performs homodyne detection and digitizes the continuous outcome via a fixed periodic binning, corresponding to logical Pauli measurements. Within this framework, we derive a bipartite no-go: CHSH cannot be violated for Bell-pair states. Moving beyond two parties, we show that finitely squeezed GKP-encoded GHZ and W states nevertheless exhibit strong multipartite nonlocality, violating multipartite Bell inequalities with homodyne-only readout. We quantify the required squeezing thresholds and robustness to loss, providing a route toward homodyne-based Bell tests in continuous-variable systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [46] [PRISM: Deriving the Transformer as a Signal-Denoising Operator via Maximum Coding Rate Reduction](https://arxiv.org/abs/2601.15540)
*Dongchen Huang*

Main category: cs.LG

TL;DR: 提出Prism架构，基于最大编码率降低原理，通过几何归纳偏置实现无监督功能解耦，在TinyStories上验证了注意力头自发分化为低频信号和高频噪声处理


<details>
  <summary>Details</summary>
Motivation: 深度学习模型（特别是Transformer）常被批评为"黑箱"且缺乏可解释性，需要探索如何将可解释性和性能统一起来

Method: 提出Prism白盒注意力架构，基于MCR²原理，将注意力机制建模为信号-噪声流形上的梯度上升过程，引入过完备字典扩展表示相空间和π-RoPE无理频率分离来强制信号与噪声子空间的不相干性

Result: 在TinyStories测试平台上观察到Prism的注意力头自发分化为光谱不同的机制：低频头捕获长程因果依赖（信号），高频头处理局部句法约束（噪声）

Conclusion: 可解释性和性能不是权衡关系，可以通过原则性的几何构造统一起来，几何归纳偏置足以单独诱导无监督功能解耦

Abstract: Deep learning models, particularly Transformers, are often criticized as "black boxes" and lack interpretability. We propose Prism, a white-box attention-based architecture derived from the principles of Maximizing Coding Rate Reduction ($\text{MCR}^2$). By modeling the attention mechanism as a gradient ascent process on a distinct signal-noise manifold, we introduce two physical constraints: an overcomplete dictionary to expand the representational phase space, and an irrational frequency separation ($π$-RoPE) to enforce incoherence between signal and noise subspaces. We demonstrate that these geometric inductive biases can be viewed as a physical constraint and they are sufficient to induce unsupervised functional disentanglement alone. Using TinyStories as a controlled testbed for verifying spectral dynamics, we observe that Prism spontaneously specializes its attention heads into spectrally distinct regimes: low-frequency heads capturing long-range causal dependencies (signal) and high-frequency heads handling local syntactic constraints (noise). Our results suggest that interpretability and performance are not a trade-off, but can be unified through principled geometric construction.

</details>


### [47] [Language Models Entangle Language and Culture](https://arxiv.org/abs/2601.15337)
*Shourya Jain,Paras Chopra*

Main category: cs.LG

TL;DR: 研究发现LLMs在低资源语言上的回答质量较低，语言选择显著影响模型使用的文化背景，进而影响回答质量


<details>
  <summary>Details</summary>
Motivation: 用户不应因使用不同语言与LLMs交互而受到系统性歧视，需要评估不同语言用户是否获得相似质量的回答，以及语言选择如何影响模型使用的文化背景

Method: 基于WildChat数据集创建真实世界开放式问题集，使用LLM-as-a-Judge识别回答中的文化背景，并在多语言翻译的CulturalBench基准上进行评估

Result: LLMs在低资源语言上的开放式问题回答质量较低；语言显著影响模型使用的文化背景；文化背景差异影响下游回答质量

Conclusion: 语言选择不仅影响回答质量，还影响LLMs使用的文化背景，揭示了语言和文化在LLMs中的纠缠关系，需要关注多语言公平性问题

Abstract: Users should not be systemically disadvantaged by the language they use for interacting with LLMs; i.e. users across languages should get responses of similar quality irrespective of language used. In this work, we create a set of real-world open-ended questions based on our analysis of the WildChat dataset and use it to evaluate whether responses vary by language, specifically, whether answer quality depends on the language used to query the model. We also investigate how language and culture are entangled in LLMs such that choice of language changes the cultural information and context used in the response by using LLM-as-a-Judge to identify the cultural context present in responses. To further investigate this, we evaluate LLMs on a translated subset of the CulturalBench benchmark across multiple languages. Our evaluations reveal that LLMs consistently provide lower quality answers to open-ended questions in low resource languages. We find that language significantly impacts the cultural context used by the model. This difference in context impacts the quality of the downstream answer.

</details>


### [48] [Improving MoE Compute Efficiency by Composing Weight and Data Sparsity](https://arxiv.org/abs/2601.15370)
*Maciej Kilian,Oleg Mkrtchyan,Luke Zettlemoyer,Akshat Shrivastava,Armen Aghajanyan*

Main category: cs.LG

TL;DR: 通过引入零计算（null）专家实现数据稀疏性，结合权重稀疏性，在视觉语言模型中提升计算效率


<details>
  <summary>Details</summary>
Motivation: 传统的专家选择路由在自回归模型中违反因果性，导致训练-推理不匹配。需要一种既能实现数据稀疏性又不违反因果性的方法

Method: 在因果令牌选择MoE中引入零计算专家，当令牌路由到这些专家时，这些槽位不消耗计算资源。通过负载均衡目标训练模型均匀使用所有专家（真实和零计算），在期望上实现数据稀疏性

Result: 在匹配的预期FLOPs下，权重稀疏性和数据稀疏性组合比单独使用权重稀疏性产生更高效的计算边界，在训练损失和下游性能上都有提升

Conclusion: 该方法在视觉语言模型中实现了隐式的模态感知分配，无需显式模态路由，视觉令牌更积极地路由到零计算专家，提高了计算效率

Abstract: Mixture-of-Experts layers achieve compute efficiency through weight sparsity: each token activates only a subset of experts. Data sparsity, where each expert processes only a subset of tokens, offers a complementary axis. Expert-choice routing implements data sparsity directly but violates causality in autoregressive models, creating train-inference mismatch. We recover data sparsity within causal token-choice MoE by leveraging zero-compute (null) experts within the routing pool. When a token routes to null experts, those slots consume no compute. The standard load balancing objective trains the model to uniformly use all experts (real and null) therefore creating data sparsity in expectation without the causality violations. We evaluate on vision-language model training, where data heterogeneity is pronounced: vision encoders produce many low-information tokens while text tokens are denser. At matched expected FLOPs, composing weight and data sparsity yields a more compute-efficient frontier than weight sparsity alone, with gains in training loss and downstream performance. The model learns implicit modality-aware allocation, routing vision tokens to null experts more aggressively than text, without explicit modality routing.

</details>


### [49] [You Need Better Attention Priors](https://arxiv.org/abs/2601.15380)
*Elon Litman,Gabe Guo*

Main category: cs.LG

TL;DR: GOAT是一种基于熵最优传输的广义注意力机制，用可学习的连续先验替代标准注意力中的隐式均匀先验，保持与FlashAttention的兼容性，解决注意力沉没问题，并实现长度泛化。


<details>
  <summary>Details</summary>
Motivation: 标准注意力机制在熵最优传输视角下对应一个具有隐式均匀先验的传输问题，这种朴素假设限制了注意力的表达能力。需要一种更灵活的注意力机制，能够学习连续先验，同时保持计算效率。

Method: 提出GOAT（广义最优传输注意力），通过熵最优传输框架将注意力重新表述为具有可学习连续先验的传输问题。该方法与FlashAttention等优化内核完全兼容，通过将空间信息吸收到核心注意力计算中学习可外推的先验。

Result: GOAT提供了注意力沉没现象的EOT解释并实现了解决方案，避免了标准注意力中的表示权衡。学习到的可外推先验结合了学习位置嵌入的灵活性和固定编码的长度泛化能力。

Conclusion: GOAT通过熵最优传输框架统一了注意力机制，用可学习连续先验替代均匀先验，在保持计算效率的同时提高了表达能力和泛化性能，为注意力机制提供了更坚实的理论基础。

Abstract: We generalize the attention mechanism by viewing it through the lens of Entropic Optimal Transport, revealing that standard attention corresponds to a transport problem regularized by an implicit uniform prior. We introduce Generalized Optimal transport Attention with Trainable priors (GOAT), a new attention mechanism that replaces this naive assumption with a learnable, continuous prior. This prior maintains full compatibility with optimized kernels such as FlashAttention. GOAT also provides an EOT-based explanation of attention sinks and materializes a solution for them, avoiding the representational trade-offs of standard attention. Finally, by absorbing spatial information into the core attention computation, GOAT learns an extrapolatable prior that combines the flexibility of learned positional embeddings with the length generalization of fixed encodings.

</details>


### [50] [Ambient Dataloops: Generative Models for Dataset Refinement](https://arxiv.org/abs/2601.15417)
*Adrián Rodríguez-Muñoz,William Daspit,Adam Klivans,Antonio Torralba,Constantinos Daskalakis,Giannis Daras*

Main category: cs.LG

TL;DR: Ambient Dataloops是一个迭代框架，通过数据集-模型协同进化过程逐步提升数据集质量，利用Ambient Diffusion技术避免自消耗循环，在图像生成和蛋白质设计任务中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现代数据集包含质量差异很大的样本，直接在这样异构的数据上训练往往产生次优模型。需要一种方法来提升数据集质量，同时避免自消耗循环的破坏性影响。

Method: 提出数据集-模型协同进化过程：每个迭代中，数据集质量逐步提升，模型相应改进。为避免破坏性自消耗循环，将合成改进样本视为噪声，但比前一次迭代噪声水平略低，并使用Ambient Diffusion技术进行损坏下的学习。

Result: Ambient Dataloops在无条件图像生成、文本条件图像生成和从头蛋白质设计任务中实现了最先进的性能。

Conclusion: 该框架通过理论论证捕捉了数据循环过程的优势，提供了一种有效提升扩散模型学习数据分布能力的方法。

Abstract: We propose Ambient Dataloops, an iterative framework for refining datasets that makes it easier for diffusion models to learn the underlying data distribution. Modern datasets contain samples of highly varying quality, and training directly on such heterogeneous data often yields suboptimal models. We propose a dataset-model co-evolution process; at each iteration of our method, the dataset becomes progressively higher quality, and the model improves accordingly. To avoid destructive self-consuming loops, at each generation, we treat the synthetically improved samples as noisy, but at a slightly lower noisy level than the previous iteration, and we use Ambient Diffusion techniques for learning under corruption. Empirically, Ambient Dataloops achieve state-of-the-art performance in unconditional and text-conditional image generation and de novo protein design. We further provide a theoretical justification for the proposed framework that captures the benefits of the data looping procedure.

</details>


### [51] [CASL: Concept-Aligned Sparse Latents for Interpreting Diffusion Models](https://arxiv.org/abs/2601.15441)
*Zhenghao He,Guangzhi Xiong,Boyang Wang,Sanchit Sinha,Aidong Zhang*

Main category: cs.LG

TL;DR: CASL：首个实现扩散模型潜在表示与语义概念监督对齐的框架，通过稀疏自编码器和轻量级线性映射将潜在维度与概念对齐，并引入CASL-Steer作为因果探针验证语义含义。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的内部激活编码了丰富的语义信息，但现有基于稀疏自编码器的方法都是无监督的，无法将稀疏特征与人类可理解的概念对齐，限制了其对生成图像的可靠语义控制能力。

Method: CASL框架首先在冻结的U-Net激活上训练稀疏自编码器获得解耦的潜在表示，然后学习轻量级线性映射将每个概念与一小部分相关潜在维度关联。引入CASL-Steer作为受控潜在干预，沿学习的概念轴移动激活以验证语义含义。

Result: 实验表明，该方法相比现有方法实现了更优的编辑精度和可解释性。提出了编辑精度比（EPR）这一联合度量概念特异性和无关属性保持的指标。

Conclusion: 这是首个实现扩散模型潜在表示与语义概念监督对齐的工作，为理解扩散模型内部表示和实现可靠的语义控制提供了新途径。

Abstract: Internal activations of diffusion models encode rich semantic information, but interpreting such representations remains challenging. While Sparse Autoencoders (SAEs) have shown promise in disentangling latent representations, existing SAE-based methods for diffusion model understanding rely on unsupervised approaches that fail to align sparse features with human-understandable concepts. This limits their ability to provide reliable semantic control over generated images. We introduce CASL (Concept-Aligned Sparse Latents), a supervised framework that aligns sparse latent dimensions of diffusion models with semantic concepts. CASL first trains an SAE on frozen U-Net activations to obtain disentangled latent representations, and then learns a lightweight linear mapping that associates each concept with a small set of relevant latent dimensions. To validate the semantic meaning of these aligned directions, we propose CASL-Steer, a controlled latent intervention that shifts activations along the learned concept axis. Unlike editing methods, CASL-Steer is used solely as a causal probe to reveal how concept-aligned latents influence generated content. We further introduce the Editing Precision Ratio (EPR), a metric that jointly measures concept specificity and the preservation of unrelated attributes. Experiments show that our method achieves superior editing precision and interpretability compared to existing approaches. To the best of our knowledge, this is the first work to achieve supervised alignment between latent representations and semantic concepts in diffusion models.

</details>


### [52] [Learning from Synthetic Data: Limitations of ERM](https://arxiv.org/abs/2601.15468)
*Kareem Amin,Alex Bie,Weiwei Kong,Umar Syed,Sergei Vassilvitskii*

Main category: cs.LG

TL;DR: 该论文研究了在合成数据污染的自然数据环境中学习理论的基本问题，发现传统ERM方法存在局限性，但存在能够处理任意污染程度的算法。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的普及和低成本，合成内容大量出现，导致自然数据被LLM生成的数据污染。研究者需要重新审视在这种普遍存在的数据污染环境中的基本学习理论问题。

Method: 将场景建模为一系列学习任务，其中输入是自然数据和合成数据的混合，学习算法不知道每个样本的来源。研究了ERM在这种设置下的可能性和局限性，并与分配非均匀权重的算法进行比较。

Result: 对于估计任意d维分布均值的问题，ERM虽然收敛到真实均值，但被分配不同代数据非均匀权重的算法超越。在PAC学习设置中，ERM并不总是收敛到真实概念，但存在能够学习任意VC类和任意污染程度的正确假设的算法。

Conclusion: 在合成数据污染的环境中，传统ERM方法存在局限性，需要开发新的算法来处理数据污染问题，存在能够有效学习正确假设的算法解决方案。

Abstract: The prevalence and low cost of LLMs have led to a rise of synthetic content. From review sites to court documents, ``natural'' content has been contaminated by data points that appear similar to natural data, but are in fact LLM-generated. In this work we revisit fundamental learning theory questions in this, now ubiquitous, setting. We model this scenario as a sequence of learning tasks where the input is a mix of natural and synthetic data, and the learning algorithms are oblivious to the origin of any individual example.
  We study the possibilities and limitations of ERM in this setting. For the problem of estimating the mean of an arbitrary $d$-dimensional distribution, we find that while ERM converges to the true mean, it is outperformed by an algorithm that assigns non-uniform weights to examples from different generations of data. For the PAC learning setting, the disparity is even more stark. We find that ERM does not always converge to the true concept, echoing the model collapse literature. However, we show there are algorithms capable of learning the correct hypothesis for arbitrary VC classes and arbitrary amounts of contamination.

</details>


### [53] [Panther: Faster and Cheaper Computations with Randomized Numerical Linear Algebra](https://arxiv.org/abs/2601.15473)
*Fahd Seddik,Abdulrahman Elbedewy,Gaser Sami,Mohamed Abdelmoniem,Yahia Zakaria*

Main category: cs.LG

TL;DR: Panther是一个PyTorch兼容的RandNLA库，通过随机数值线性代数技术压缩深度学习模型，减少内存使用达75%


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型训练受GPU内存和计算限制，RandNLA技术虽然有效但缺乏统一的生产级库，阻碍了这些方法的广泛采用

Method: 开发Panther库，整合成熟的RandNLA算法，提供高效即插即用组件，包括草图线性层、2D卷积、多头注意力、随机矩阵分解，并实现自定义C++/CUDA后端

Result: 仅需几行代码替换标准PyTorch线性层，在BERT上实现高达75%的内存节省，同时保持可比较的损失

Conclusion: Panther证明了RandNLA技术的有效性及其易用性，为深度学习模型压缩提供了实用的生产级解决方案

Abstract: Training modern deep learning models is increasingly constrained by GPU memory and compute limits. While Randomized Numerical Linear Algebra (RandNLA) offers proven techniques to compress these models, the lack of a unified, production-grade library prevents widely adopting these methods. We present Panther, a PyTorch-compatible library that consolidates established RandNLA algorithms into a single high-performance framework. Panther engineers efficient, drop-in replacements for standard components including sketched linear layers, 2D convolution, multi-head attention, and randomized matrix decompositions (such as pivoted CholeskyQR). By implementing a custom C++/CUDA backend (pawX), Panther provides an optimized implementation that can run on both CPUs and GPUs. We demonstrate the effectiveness of RandNLA techniques and Panther's ease of adoption. By replacing standard PyTorch linear layers with Panther layers (requiring only a few lines of code) we achieve significant memory savings (up to 75%) on BERT while maintaining comparable loss. Source code is available (MIT License) at https://github.com/FahdSeddik/panther, along with demonstration video at https://youtu.be/7M3RQb4KWxs.

</details>


### [54] [Martingale Foresight Sampling: A Principled Approach to Inference-Time LLM Decoding](https://arxiv.org/abs/2601.15482)
*Huayu Li,ZhengXiao He,Siyuan Tian,Jinghao Wen,Ao Li*

Main category: cs.LG

TL;DR: MFS将LLM解码重构为识别最优随机过程的问题，使用鞅理论设计算法，在推理基准上超越SOTA方法，同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 标准自回归解码在大型语言模型中存在短视问题，无法找到全局最优推理路径。现有的推理时策略如前瞻采样通常依赖启发式方法进行路径评估和剪枝，缺乏理论依据。

Method: 提出鞅前瞻采样(MFS)框架，将LLM解码建模为识别最优随机过程的问题。利用鞅理论：1) 使用Doob分解定理推导步骤评估，测量路径的可预测优势；2) 应用可选停止理论进行原则性剪枝；3) 基于鞅收敛定理设计自适应停止规则。

Result: 在六个推理基准测试中，MFS在准确性上超越了最先进的方法，同时显著提高了计算效率。

Conclusion: MFS提供了一个基于概率理论的原则性框架，用于改进LLM解码，解决了自回归解码的短视问题，并在准确性和效率方面都取得了显著提升。

Abstract: Standard autoregressive decoding in large language models (LLMs) is inherently short-sighted, often failing to find globally optimal reasoning paths due to its token-by-token generation process. While inference-time strategies like foresight sampling attempt to mitigate this by simulating future steps, they typically rely on ad-hoc heuristics for valuing paths and pruning the search space. This paper introduces Martingale Foresight Sampling (MFS), a principled framework that reformulates LLM decoding as a problem of identifying an optimal stochastic process. By modeling the quality of a reasoning path as a stochastic process, we leverage Martingale theory to design a theoretically-grounded algorithm. Our approach replaces heuristic mechanisms with principles from probability theory: step valuation is derived from the Doob Decomposition Theorem to measure a path's predictable advantage, path selection uses Optional Stopping Theory for principled pruning of suboptimal candidates, and an adaptive stopping rule based on the Martingale Convergence Theorem terminates exploration once a path's quality has provably converged. Experiments on six reasoning benchmarks demonstrate that MFS surpasses state-of-the-art methods in accuracy while significantly improving computational efficiency. Code will be released at https://github.com/miraclehetech/EACL2026-Martingale-Foresight-Sampling.

</details>


### [55] [MARS: Unleashing the Power of Speculative Decoding via Margin-Aware Verification](https://arxiv.org/abs/2601.15498)
*Jingwei Song,Xinyu Wang,Hanbin Wang,Xiaoxuan Lei,Bill Shi,Shixin Han,Eric Yang,Xiao-Wen Chang,Lynn Ai*

Main category: cs.LG

TL;DR: 提出Margin-Aware Speculative Verification方法，通过自适应目标模型的局部决策稳定性来优化验证机制，在保持生成质量的同时显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法在验证机制上存在根本性低效：当目标模型对候选token偏好较弱时（低边际区域），严格拒绝次优token带来的信息增益很小，却产生大量回滚成本。

Method: 提出训练无关、领域无关的边界感知推测验证策略，根据目标模型logits直接测量的决策稳定性来条件化验证，仅在严格验证提供最小收益时放宽拒绝规则。

Result: 在8B到235B不同规模模型上的广泛实验表明，该方法相比最先进基线实现了持续且显著的推理加速，同时在多样化基准测试中保持了生成质量。

Conclusion: 边界感知推测验证通过自适应目标模型的局部决策稳定性，解决了推测解码验证机制的根本性低效问题，为LLM推理加速提供了有效解决方案。

Abstract: Speculative Decoding (SD) accelerates autoregressive large language model (LLM) inference by decoupling generation and verification. While recent methods improve draft quality by tightly coupling the drafter with the target model, the verification mechanism itself remains largely unchanged, relying on strict token-level rejection sampling. In practice, modern LLMs frequently operate in low-margin regimes where the target model exhibits weak preference among top candidates. In such cases, rejecting plausible runner-up tokens yields negligible information gain while incurring substantial rollback cost, leading to a fundamental inefficiency in verification. We propose Margin-Aware Speculative Verification, a training-free and domain-agnostic verification strategy that adapts to the target model's local decisiveness. Our method conditions verification on decision stability measured directly from the target logits and relaxes rejection only when strict verification provides minimal benefit. Importantly, the approach modifies only the verification rule and is fully compatible with existing target-coupled speculative decoding frameworks. Extensive experiments across model scales ranging from 8B to 235B demonstrate that our method delivers consistent and significant inference speedups over state-of-the-art baselines while preserving generation quality across diverse benchmarks.

</details>


### [56] [Data-driven Lake Water Quality Forecasting for Time Series with Missing Data using Machine Learning](https://arxiv.org/abs/2601.15503)
*Rishit Chatterjee,Tahiya Chowdhury*

Main category: cs.LG

TL;DR: 该研究针对志愿者湖泊监测数据不完整的问题，开发了Secchi盘深度预测方法，通过联合可行性函数确定在5%精度目标下所需的最小训练样本和最少预测因子。


<details>
  <summary>Details</summary>
Motivation: 志愿者湖泊监测产生不规则的季节性时间序列，存在大量数据缺口（冰盖、天气限制、人为错误），这给有害藻华预测和早期预警带来困难。

Method: 使用MICE处理缺失数据，评估六种候选模型，采用岭回归作为最佳模型，通过后向近期历史协议确定最小样本量，识别最小特征集，并引入联合可行性函数。

Result: 岭回归表现最佳；达到完整历史精度5%范围内平均需要176个训练样本；四特征子集与十三特征基线在5%容差内匹配；联合可行性分析显示约64个近期样本和仅一个预测因子即可满足5%精度目标。

Conclusion: 联合可行性策略在固定精度目标下统一了近期历史长度和特征选择，为湖泊研究人员提供了设定采样努力和测量优先级的简单高效规则，突出了针对性监测的实用性。

Abstract: Volunteer-led lake monitoring yields irregular, seasonal time series with many gaps arising from ice cover, weather-related access constraints, and occasional human errors, complicating forecasting and early warning of harmful algal blooms. We study Secchi Disk Depth (SDD) forecasting on a 30-lake, data-rich subset drawn from three decades of in situ records collected across Maine lakes. Missingness is handled via Multiple Imputation by Chained Equations (MICE), and we evaluate performance with a normalized Mean Absolute Error (nMAE) metric for cross-lake comparability. Among six candidates, ridge regression provides the best mean test performance. Using ridge regression, we then quantify the minimal sample size, showing that under a backward, recent-history protocol, the model reaches within 5% of full-history accuracy with approximately 176 training samples per lake on average. We also identify a minimal feature set, where a compact four-feature subset matches the thirteen-feature baseline within the same 5% tolerance. Bringing these results together, we introduce a joint feasibility function that identifies the minimal training history and fewest predictors sufficient to achieve the target of staying within 5% of the complete-history, full-feature baseline. In our study, meeting the 5% accuracy target required about 64 recent samples and just one predictor per lake, highlighting the practicality of targeted monitoring. Hence, our joint feasibility strategy unifies recent-history length and feature choice under a fixed accuracy target, yielding a simple, efficient rule for setting sampling effort and measurement priorities for lake researchers.

</details>


### [57] [Machine learning-enhanced non-amnestic Alzheimer's disease diagnosis from MRI and clinical features](https://arxiv.org/abs/2601.15530)
*Megan A. Witherow,Michael L. Evans,Ahmed Temtam,Hamid Okhravi,Khan M. Iftekharuddin*

Main category: cs.LG

TL;DR: 提出机器学习方法，利用临床测试和MRI数据区分非典型阿尔茨海默病与非AD认知障碍，提高诊断准确性


<details>
  <summary>Details</summary>
Motivation: 非典型阿尔茨海默病（atAD）患者常被误诊，而基于临床评估和海马体积的方法对典型AD有效但对atAD诊断准确性低，需要改进诊断方法

Method: 使用机器学习方法，结合临床测试数据和MRI特征（包括海马体积和全脑MRI特征），采用Boruta统计方法识别显著脑区，在1410名受试者（包括典型AD、非典型AD、非AD认知障碍和认知正常）的数据集上进行分类实验

Result: 最佳性能通过结合重要MRI特征实现，优于仅使用海马体积。在NACC数据集中，atAD正确诊断率从52%提高到69%；在ADNI数据集中，从34%提高到77%，同时保持高精度

Conclusion: 提出的机器学习方法仅使用临床测试和MRI数据就能显著提高非典型阿尔茨海默病的诊断准确性，对临床环境具有重要应用价值

Abstract: Alzheimer's disease (AD), defined as an abnormal buildup of amyloid plaques and tau tangles in the brain can be diagnosed with high accuracy based on protein biomarkers via PET or CSF analysis. However, due to the invasive nature of biomarker collection, most AD diagnoses are made in memory clinics using cognitive tests and evaluation of hippocampal atrophy based on MRI. While clinical assessment and hippocampal volume show high diagnostic accuracy for amnestic or typical AD (tAD), a substantial subgroup of AD patients with atypical presentation (atAD) are routinely misdiagnosed. To improve diagnosis of atAD patients, we propose a machine learning approach to distinguish between atAD and non-AD cognitive impairment using clinical testing battery and MRI data collected as standard-of-care. We develop and evaluate our approach using 1410 subjects across four groups (273 tAD, 184 atAD, 235 non-AD, and 685 cognitively normal) collected from one private data set and two public data sets from the National Alzheimer's Coordinating Center (NACC) and the Alzheimer's Disease Neuroimaging Initiative (ADNI). We perform multiple atAD vs. non-AD classification experiments using clinical features and hippocampal volume as well as a comprehensive set of MRI features from across the brain. The best performance is achieved by incorporating additional important MRI features, which outperforms using hippocampal volume alone. Furthermore, we use the Boruta statistical approach to identify and visualize significant brain regions distinguishing between diagnostic groups. Our ML approach improves the percentage of correctly diagnosed atAD cases (the recall) from 52% to 69% for NACC and from 34% to 77% for ADNI, while achieving high precision. The proposed approach has important implications for improving diagnostic accuracy for non-amnestic atAD in clinical settings using only clinical testing battery and MRI.

</details>


### [58] [QUAIL: Quantization Aware Unlearning for Mitigating Misinformation in LLMs](https://arxiv.org/abs/2601.15538)
*Himanshu Mishra,Kanwal Mehreen*

Main category: cs.LG

TL;DR: 量化会灾难性地恢复已遗忘的知识，本文提出了一种量化感知的遗忘方法，通过logits空间铰链损失确保遗忘样本在量化后仍保持可区分性


<details>
  <summary>Details</summary>
Motivation: 在实际部署中，模型通常被量化（如4位）以节省资源，但研究发现量化会灾难性地恢复通过机器遗忘技术移除的特定知识，这带来了安全和隐私风险

Method: 首先分析量化如何破坏遗忘效果，发现典型的遗忘更新太小而无法跨越量化阈值；然后提出logits空间铰链损失方法，强制遗忘样本在未学习模型中的输出logits与原模型至少相差半个量化步长的边界，确保量化后仍能保持遗忘效果

Result: 在语言和分类任务（包括Twitter虚假信息数据集）上的评估显示，该方法在4位量化下能有效保持遗忘效果，而现有方法在量化后几乎完全恢复了被遗忘的知识

Conclusion: 量化会严重破坏机器遗忘的效果，但通过提出的量化感知遗忘方法可以缓解这一问题，确保模型在量化部署后仍能有效保持对特定知识的遗忘

Abstract: Machine unlearning aims to remove specific knowledge (e.g., copyrighted or private data) from a trained model without full retraining. In practice, models are often quantized (e.g., 4-bit) for deployment, but we find that quantization can catastrophically restore forgotten information [1]. In this paper, we (1) analyze why low-bit quantization undermines unlearning, and (2) propose a quantization-aware unlearning method to mitigate this. We first compute weight-change statistics and bucket overlaps in quantization to show that typical unlearning updates are too small to cross quantization thresholds. Building on this insight, we introduce a logits space hinge loss: for each forget example, we force the output logits of the unlearned model to differ from the original model by at least a margin (half the quantization step). This ensures forgotten examples remain distinguishable even after quantization. We evaluate on language and classification tasks (including a Twitter misinformation dataset) and show our method preserves forgetting under 4-bit quantization, whereas existing methods almost entirely recover the forgotten knowledge.

</details>


### [59] [RDumb++: Drift-Aware Continual Test-Time Adaptation](https://arxiv.org/abs/2601.15544)
*Himanshu Mishra*

Main category: cs.LG

TL;DR: RDumb++通过引入两种漂移检测机制（基于熵和KL散度）和自适应重置策略，解决了持续测试时适应（CTTA）在长期部署中模型崩溃的问题，在CCC基准测试中相比RDumb获得约3%的绝对准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有CTTA方法（如Tent、EATA）在短期分布漂移下表现良好，但在测试分布快速变化或极长时间部署时（如CCC基准测试的750万样本流）容易失败，导致模型预测崩溃。

Method: 提出RDumb++方法，扩展了RDumb框架，引入两种漂移检测机制：1）基于熵的漂移评分，2）基于KL散度的漂移评分，结合自适应重置策略。这些机制使模型能够检测累积适应何时变得有害，并在预测崩溃发生前恢复。

Result: 在CCC-medium基准测试的三个速度和三个种子（共9次运行，每次包含100万样本）上，RDumb++始终优于RDumb，获得约3%的绝对准确率提升，同时在整个流中保持稳定的适应能力。消融实验进一步表明漂移感知重置对于防止崩溃和实现可靠的长期CTTA至关重要。

Conclusion: 漂移检测机制和自适应重置策略是解决长期CTTA中模型崩溃问题的关键，RDumb++通过系统性的漂移感知方法实现了稳定可靠的长期部署适应。

Abstract: Continual Test-Time Adaptation (CTTA) seeks to update a pretrained model during deployment using only the incoming, unlabeled data stream. Although prior approaches such as Tent, EATA etc. provide meaningful improvements under short evolving shifts, they struggle when the test distribution changes rapidly or over extremely long horizons. This challenge is exemplified by the CCC benchmark, where models operate over streams of 7.5M samples with continually changing corruption types and severities. We propose RDumb++, a principled extension of RDumb that introduces two drift-detection mechanisms i.e entropy-based drift scoring and KL-divergence drift scoring, together with adaptive reset strategies. These mechanisms allow the model to detect when accumulated adaptation becomes harmful and to recover before prediction collapse occurs. Across CCC-medium with three speeds and three seeds (nine runs, each containing one million samples), RDumb++ consistently surpasses RDumb, yielding approx 3% absolute accuracy gains while maintaining stable adaptation throughout the entire stream. Ablation experiments on drift thresholds and reset strengths further show that drift-aware resetting is essential for preventing collapse and achieving reliable long-horizon CTTA.

</details>


### [60] [Beyond validation loss: Clinically-tailored optimization metrics improve a model's clinical performance](https://arxiv.org/abs/2601.15546)
*Charles B. Delahunt,Courosh Mehanian,Daniel E. Shea,Matthew P. Horning*

Main category: cs.LG

TL;DR: 医疗AI模型优化应使用临床定制指标而非验证损失，以更好地满足临床需求


<details>
  <summary>Details</summary>
Motivation: 传统机器学习使用验证损失优化模型，但医疗AI的目标是满足具体临床需求而非最小化损失函数。临床需求可以通过定制化指标更精确地捕捉。

Method: 通过两个对照实验，比较使用临床定制指标与验证损失进行模型优化的效果差异。

Result: 临床定制指标在模型优化中表现优于验证损失，能产生在临床任务上性能更好的模型。

Conclusion: 虽然定义和编码临床相关指标需要额外努力，但能产生更符合医疗AI核心目标（在临床中表现优异）的模型。

Abstract: A key task in ML is to optimize models at various stages, e.g. by choosing hyperparameters or picking a stopping point. A traditional ML approach is to use validation loss, i.e. to apply the training loss function on a validation set to guide these optimizations. However, ML for healthcare has a distinct goal from traditional ML: Models must perform well relative to specific clinical requirements, vs. relative to the loss function used for training. These clinical requirements can be captured more precisely by tailored metrics. Since many optimization tasks do not require the driving metric to be differentiable, they allow a wider range of options, including the use of metrics tailored to be clinically-relevant. In this paper we describe two controlled experiments which show how the use of clinically-tailored metrics provide superior model optimization compared to validation loss, in the sense of better performance on the clinical task. The use of clinically-relevant metrics for optimization entails some extra effort, to define the metrics and to code them into the pipeline. But it can yield models that better meet the central goal of ML for healthcare: strong performance in the clinic.

</details>


### [61] [Learning Neural Operators from Partial Observations via Latent Autoregressive Modeling](https://arxiv.org/abs/2601.15547)
*Jingren Hou,Hong Wang,Pengyu Xu,Chang Gao,Huafeng Liu,Liping Jing*

Main category: cs.LG

TL;DR: 该论文提出了首个系统性的框架，用于从部分观测数据中学习神经算子，解决了真实世界科学应用中数据不完整的问题。


<details>
  <summary>Details</summary>
Motivation: 真实世界的科学应用经常遇到不完整的观测数据，而现有的神经算子方法假设完全观测的空间输入，这严重限制了其在真实世界应用中的适用性。

Method: 提出了Latent Autoregressive Neural Operator (LARNO)，包含两个核心组件：(1) mask-to-predict训练策略，通过在观测区域战略性地创建掩码来生成人工监督；(2) Physics-Aware Latent Propagator，通过边界优先的自回归生成在潜在空间中重建解。

Result: 在POBench-PDE基准测试中，LARNO实现了18-69%的相对L2误差降低，在缺失率小于50%的补丁式缺失情况下表现最佳，并能有效处理高达75%缺失率的实际场景。

Conclusion: 该研究填补了理想化研究设置与真实世界科学计算复杂性之间的差距，为从部分观测数据学习神经算子提供了首个系统性框架。

Abstract: Real-world scientific applications frequently encounter incomplete observational data due to sensor limitations, geographic constraints, or measurement costs. Although neural operators significantly advanced PDE solving in terms of computational efficiency and accuracy, their underlying assumption of fully-observed spatial inputs severely restricts applicability in real-world applications. We introduce the first systematic framework for learning neural operators from partial observation. We identify and formalize two fundamental obstacles: (i) the supervision gap in unobserved regions that prevents effective learning of physical correlations, and (ii) the dynamic spatial mismatch between incomplete inputs and complete solution fields. Specifically, our proposed Latent Autoregressive Neural Operator~(\ours) introduces two novel components designed explicitly to address the core difficulties of partial observations: (i) a mask-to-predict training strategy that creates artificial supervision by strategically masking observed regions, and (ii) a Physics-Aware Latent Propagator that reconstructs solutions through boundary-first autoregressive generation in latent space. Additionally, we develop POBench-PDE, a dedicated and comprehensive benchmark designed specifically for evaluating neural operators under partial observation conditions across three PDE-governed tasks. \ours achieves state-of-the-art performance with 18--69$\%$ relative L2 error reduction across all benchmarks under patch-wise missingness with less than 50$\%$ missing rate, including real-world climate prediction. Our approach effectively addresses practical scenarios involving up to 75$\%$ missing rate, to some extent bridging the existing gap between idealized research settings and the complexities of real-world scientific computing.

</details>


### [62] [BanditLP: Large-Scale Stochastic Optimization for Personalized Recommendations](https://arxiv.org/abs/2601.15552)
*Phuc Nguyen,Benjamin Zelditch,Joyce Chen,Rohit Patra,Changshuai Wei*

Main category: cs.LG

TL;DR: BanditLP是一个可扩展的多利益相关方上下文老虎机框架，结合了神经Thompson采样和大规模线性规划，用于生产环境中的约束优化和探索。


<details>
  <summary>Details</summary>
Motivation: 为了解决在生产系统中同时进行探索和约束优化的挑战，特别是在需要平衡多个利益相关方目标的大规模应用中，如LinkedIn的电子邮件营销系统。

Method: 使用神经Thompson采样学习特定目标的结果，结合大规模线性规划在服务时进行约束动作选择。框架是应用无关的，兼容任意神经架构，线性规划求解器能够处理数十亿变量。

Result: 在公共基准测试和合成数据上的实验显示，相比强基线方法有持续提升。在LinkedIn电子邮件营销系统中的应用展示了业务成功，证明了集成探索和约束优化的价值。

Conclusion: BanditLP框架成功地将神经探索与大规模约束优化相结合，为生产环境中的多利益相关方决策问题提供了可扩展的解决方案，并在实际应用中验证了其有效性。

Abstract: We present BanditLP, a scalable multi-stakeholder contextual bandit framework that unifies neural Thompson Sampling for learning objective-specific outcomes with a large-scale linear program for constrained action selection at serving time. The methodology is application-agnostic, compatible with arbitrary neural architectures, and deployable at web scale, with an LP solver capable of handling billions of variables. Experiments on public benchmarks and synthetic data show consistent gains over strong baselines. We apply this approach in LinkedIn's email marketing system and demonstrate business win, illustrating the value of integrated exploration and constrained optimization in production.

</details>


### [63] [Deep Learning for Perishable Inventory Systems with Human Knowledge](https://arxiv.org/abs/2601.15589)
*Xuan Liao,Zhenkang Peng,Ying Rong*

Main category: cs.LG

TL;DR: 该研究提出了一种基于深度学习的易腐品库存管理方法，在需求过程和提前期分布未知的情况下，利用有限历史数据、协变量和系统状态直接输出订购量，并开发了三种端到端变体：纯黑盒方法、结构引导方法和增强的结构引导方法。


<details>
  <summary>Details</summary>
Motivation: 易腐品库存管理面临需求不确定和提前期随机的挑战，传统方法需要已知需求过程和提前期分布，但在实际中这些信息往往是未知的。现有方法在有限数据下学习效率低，需要一种能够利用历史数据、协变量和系统状态进行端到端学习的智能库存管理方法。

Method: 采用边际成本核算方案为每个订单分配单一生命周期成本，生成统一的端到端学习损失函数。开发了三种深度学习策略：1) E2E-BB：纯黑盒方法直接输出订购量；2) E2E-PIL：结构引导方法嵌入投影库存水平策略，通过显式计算而非额外学习来捕捉库存效应；3) E2E-BPIL：利用E2E-PIL目标函数的一阶齐次性，采用操作数据分析中的提升技术得到增强策略。

Result: 在合成数据和真实数据上的实验表明，三种方法存在稳健的性能排序：E2E-BB被E2E-PIL主导，而E2E-BPIL进一步改进了E2E-PIL。通过超额风险分解分析，嵌入启发式策略结构减少了有效模型复杂度，提高了学习效率，仅以适度的灵活性损失为代价。

Conclusion: 深度学习决策工具在人类知识引导下更加有效和稳健，突出了将高级分析与库存理论相结合的价值。结构引导方法通过减少模型复杂度和提高学习效率，在有限数据环境下表现更优，为易腐品库存管理提供了实用的端到端解决方案。

Abstract: Managing perishable products with limited lifetimes is a fundamental challenge in inventory management, as poor ordering decisions can quickly lead to stockouts or excessive waste. We study a perishable inventory system with random lead times in which both the demand process and the lead time distribution are unknown. We consider a practical setting where orders are placed using limited historical data together with observed covariates and current system states. To improve learning efficiency under limited data, we adopt a marginal cost accounting scheme that assigns each order a single lifetime cost and yields a unified loss function for end-to-end learning. This enables training a deep learning-based policy that maps observed covariates and system states directly to order quantities. We develop two end-to-end variants: a purely black-box approach that outputs order quantities directly (E2E-BB), and a structure-guided approach that embeds the projected inventory level (PIL) policy, capturing inventory effects through explicit computation rather than additional learning (E2E-PIL). We further show that the objective induced by E2E-PIL is homogeneous of degree one, enabling a boosting technique from operational data analytics (ODA) that yields an enhanced policy (E2E-BPIL). Experiments on synthetic and real data establish a robust performance ordering: E2E-BB is dominated by E2E-PIL, which is further improved by E2E-BPIL. Using an excess-risk decomposition, we show that embedding heuristic policy structure reduces effective model complexity and improves learning efficiency with only a modest loss of flexibility. More broadly, our results suggest that deep learning-based decision tools are more effective and robust when guided by human knowledge, highlighting the value of integrating advanced analytics with inventory theory.

</details>


### [64] [Closing the Gap on the Sample Complexity of 1-Identification](https://arxiv.org/abs/2601.15620)
*Zitian Li,Wang Chi Cheung*

Main category: cs.LG

TL;DR: 论文研究1-identification多臂老虎机问题，提出新的下界和算法，解决了当存在多个合格臂时的期望停止时间分析问题。


<details>
  <summary>Details</summary>
Motivation: 1-identification是纯探索多臂老虎机的基本问题，需要判断是否存在平均奖励不低于已知阈值μ₀的合格臂。现有文献在存在多个合格臂时的期望停止时间分析存在空白，本文旨在填补这一理论缺口。

Method: 采用优化问题框架推导新的下界，并设计新算法，其期望停止时间上界与下界之间的差距在所有问题实例上最多为对数因子的多项式。

Result: 推导了当至少存在一个合格臂时的新下界，设计了算法获得紧上界，解决了历史文献中关于多个合格臂情况下期望停止时间分析的开问题。

Conclusion: 本文通过优化公式和算法设计，完整解决了1-identification问题中期望停止时间的理论分析，特别是在多个合格臂情况下的性能边界问题。

Abstract: 1-identification is a fundamental multi-armed bandit formulation on pure exploration. An agent aims to determine whether there exists a qualified arm whose mean reward is not less than a known threshold $μ_0$, or to output \textsf{None} if it believes such an arm does not exist. The agent needs to guarantee its output is correct with probability at least $1-δ$, while making expected total pulling times $\mathbb{E}τ$ as small as possible. We work on 1-identification with two main contributions. (1) We utilize an optimization formulation to derive a new lower bound of $\mathbb{E}τ$, when there is at least one qualified arm. (2) We design a new algorithm, deriving tight upper bounds whose gap to lower bounds are up to a polynomial of logarithm factor across all problem instance. Our result complements the analysis of $\mathbb{E}τ$ when there are multiple qualified arms, which is an open problem left by history literature.

</details>


### [65] [Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors](https://arxiv.org/abs/2601.15625)
*Zhiwei Zhang,Fei Zhao,Rui Wang,Zezhong Wang,Bin Liang,Jiakang Wang,Yao Hu,Shaosheng Cao,Kam-Fai Wong*

Main category: cs.LG

TL;DR: Fission-GRPO框架通过将执行错误转化为纠正性监督，在RL训练循环中增强LLMs的多轮工具调用容错能力，相比传统方法显著提升了错误恢复率和整体准确性。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在多轮工具调用中存在脆弱性：遇到工具调用错误时，较小模型往往陷入重复无效调用，无法正确解释错误反馈并进行自我纠正。这种脆弱性阻碍了LLMs在实际部署中的可靠性，因为工具交互过程中的执行错误是不可避免的。现有方法存在局限性：标准RL将错误视为稀疏负奖励，不提供恢复指导；预收集的合成错误纠正数据集与模型在线错误模式存在分布不匹配问题。

Method: 提出Fission-GRPO框架，核心机制是将每个失败轨迹"裂变"为新的训练实例：通过使用微调的Error Simulator生成的诊断反馈来增强失败轨迹，然后在策略上重新采样恢复轨迹。这使得模型能够从其探索过程中产生的精确错误中学习，而不是从静态的预收集错误案例中学习。

Result: 在BFCL v4 Multi-Turn基准测试中，Fission-GRPO将Qwen3-8B的错误恢复率提高了5.7%（绝对值），更重要的是，相比GRPO实现了4%的整体准确率提升（从42.75%到46.75%），并超越了专门的工具使用智能体。

Conclusion: Fission-GRPO通过将执行错误转化为纠正性监督，有效解决了LLMs在多轮工具调用中的脆弱性问题，显著提升了错误恢复能力和整体性能，为可靠的实际部署提供了有前景的解决方案。

Abstract: Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model's on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents.

</details>


### [66] [Dualformer: Time-Frequency Dual Domain Learning for Long-term Time Series Forecasting](https://arxiv.org/abs/2601.15669)
*Jingjing Bai,Yoshinobu Kawahara*

Main category: cs.LG

TL;DR: Dualformer是一个用于长时间序列预测的双域Transformer框架，通过分层频率采样和周期性感知加权机制解决传统Transformer的低通滤波效应问题，在异构或弱周期性数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在长时间序列预测中存在固有的低通滤波效应，导致高频信息在层间传播中逐渐衰减，无法捕捉细粒度的时间变化模式。

Method: 提出Dualformer框架，包含三个核心组件：1）双分支架构同时建模时域和频域互补模式；2）分层频率采样模块为不同层分配不同频段；3）周期性感知加权机制基于输入谐波能量比动态平衡双分支贡献。

Result: 在八个广泛使用的基准数据集上进行实验，证明Dualformer具有鲁棒性和优越性能，特别是在异构或弱周期性数据上表现突出。

Conclusion: Dualformer通过结构化频率建模和自适应时频特征集成，有效解决了Transformer的低通滤波问题，为长时间序列预测提供了更有效的解决方案。

Abstract: Transformer-based models, despite their promise for long-term time series forecasting (LTSF), suffer from an inherent low-pass filtering effect that limits their effectiveness. This issue arises due to undifferentiated propagation of frequency components across layers, causing a progressive attenuation of high-frequency information crucial for capturing fine-grained temporal variations. To address this limitation, we propose Dualformer, a principled dual-domain framework that rethinks frequency modeling from a layer-wise perspective. Dualformer introduces three key components: (1) a dual-branch architecture that concurrently models complementary temporal patterns in both time and frequency domains; (2) a hierarchical frequency sampling module that allocates distinct frequency bands to different layers, preserving high-frequency details in lower layers while modeling low-frequency trends in deeper layers; and (3) a periodicity-aware weighting mechanism that dynamically balances contributions from the dual branches based on the harmonic energy ratio of inputs, supported theoretically by a derived lower bound. This design enables structured frequency modeling and adaptive integration of time-frequency features, effectively preserving high-frequency information and enhancing generalization. Extensive experiments conducted on eight widely used benchmarks demonstrate Dualformer's robustness and superior performance, particularly on heterogeneous or weakly periodic data. Our code is publicly available at https://github.com/Akira-221/Dualformer.

</details>


### [67] [Beyond Hard Writes and Rigid Preservation: Soft Recursive Least-Squares for Lifelong LLM Editing](https://arxiv.org/abs/2601.15686)
*Xinyu Wang,Sicheng Lyu,Yu Gu,Jerry Huang,Peng Lu,Yufei Cui,Xiao-Wen Chang*

Main category: cs.LG

TL;DR: RLSEdit：基于递归最小二乘的LLM序列编辑方法，通过在线二次优化解决长期编辑中的塑性-稳定性困境，支持万级编辑规模


<details>
  <summary>Details</summary>
Motivation: 现有模型编辑方法在长期序列编辑中面临塑性-稳定性困境：硬写入方法会积累干扰，硬保护方法可能覆盖过去编辑且未约束行为会偏离，导致多编辑场景下通用能力退化

Method: 将编辑建模为带软约束的在线二次优化问题，最小化累积键值拟合目标，包含两个正则化项：控制与预训练权重的偏离，以及控制与指定锚映射的偏离。通过Woodbury恒等式实现高效在线递归更新，每次编辑成本与历史长度无关，仅与当前编辑规模相关

Result: 实验表明RLSEdit能稳定扩展到10K次编辑，在编辑成功率和整体稳定性上优于基线方法，能保留早期编辑，并在GLUE和推理/代码基准测试上保持通用能力

Conclusion: RLSEdit通过递归最小二乘优化框架有效解决了长期序列编辑中的塑性-稳定性平衡问题，为大规模模型编辑提供了可扩展的解决方案

Abstract: Model editing updates a pre-trained LLM with new facts or rules without re-training, while preserving unrelated behavior. In real deployment, edits arrive as long streams, and existing editors often face a plasticity-stability dilemma: locate-then-edit "hard writes" can accumulate interference over time, while null-space-style "hard preservation" preserves only what is explicitly constrained, so past edits can be overwritten and unconstrained behaviors may deviate, degrading general capabilities in the many-edits regime. We propose RLSEdit, a recursive least-squares editor for long sequential editing. RLSEdit formulates editing as an online quadratic optimization with soft constraints, minimizing a cumulative key-value fitting objective with two regularizers that control for both deviation from the pre-trained weights and from a designated anchor mapping. The resulting update admits an efficient online recursion via the Woodbury identity, with per-edit cost independent of history length and scaling only with the current edit size. We further provide deviation bounds and an asymptotic characterization of the adherence-preservation trade-off in the many-edits regime. Experiments on multiple model families demonstrate stable scaling to 10K edits, outperforming strong baselines in both edit success and holistic stability -- crucially retaining early edits, and preserving general capabilities on GLUE and held-out reasoning/code benchmarks.

</details>


### [68] [Even GPT-5.2 Can't Count to Five: The Case for Zero-Error Horizons in Trustworthy LLMs](https://arxiv.org/abs/2601.15714)
*Ryoma Sato*

Main category: cs.LG

TL;DR: 论文提出零误差视界（ZEH）概念，用于评估LLM在无错误情况下能解决的最大问题范围，发现即使先进模型如GPT-5.2在简单问题上也会出错，这对安全关键应用有重要启示。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型（LLM）能力强大，但在简单问题上仍会出错，这对安全关键领域的应用构成风险。需要一种方法来评估模型的无错误解决范围，以更好地理解其可靠性限制。

Method: 提出零误差视界（ZEH）评估框架，通过测试模型在不同复杂度问题上的表现来确定其无错误解决的最大范围。使用树结构和在线softmax等技术来降低计算成本，实现高达一个数量级的加速。

Result: 评估发现GPT-5.2无法正确计算短字符串（如11000）的奇偶性，也无法判断括号（如((((()))))）是否平衡。ZEH与准确率相关但行为模式不同，为算法能力的涌现提供了线索。

Conclusion: ZEH是评估LLM可靠性的重要指标，揭示了即使先进模型在简单任务上也会出错，这对安全关键应用有重要警示。提出的加速方法使ZEH评估更加实用。

Abstract: We propose Zero-Error Horizon (ZEH) for trustworthy LLMs, which represents the maximum range that a model can solve without any errors. While ZEH itself is simple, we demonstrate that evaluating the ZEH of state-of-the-art LLMs yields abundant insights. For example, by evaluating the ZEH of GPT-5.2, we found that GPT-5.2 cannot even compute the parity of a short string like 11000, and GPT-5.2 cannot determine whether the parentheses in ((((()))))) are balanced. This is surprising given the excellent capabilities of GPT-5.2. The fact that LLMs make mistakes on such simple problems serves as an important lesson when applying LLMs to safety-critical domains. By applying ZEH to Qwen2.5 and conducting detailed analysis, we found that while ZEH correlates with accuracy, the detailed behaviors differ, and ZEH provides clues about the emergence of algorithmic capabilities. Finally, while computing ZEH incurs significant computational cost, we discuss how to mitigate this cost by achieving up to one order of magnitude speedup using tree structures and online softmax.

</details>


### [69] [Towards Automated Kernel Generation in the Era of LLMs](https://arxiv.org/abs/2601.15727)
*Yang Yu,Peiyu Zang,Chi Hsu Tsai,Haiming Wu,Yixin Shen,Jialing Zhang,Haoyu Wang,Zhiyou Xiao,Jingze Shi,Yuyu Luo,Wentao Zhang,Chunlei Men,Guang Liu,Yonghua Lin*

Main category: cs.LG

TL;DR: 这篇论文是关于LLM驱动的内核生成与优化的系统性综述，旨在填补该领域缺乏系统化视角的空白，提供了现有方法的分类、数据集和基准测试的整理，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统的性能受限于底层内核的质量，而内核开发需要硬件架构和编程模型的专家知识，这是一个耗时且难以扩展的过程。LLM和基于LLM的智能体为自动化内核生成和优化提供了新可能，但该领域仍然碎片化，缺乏系统化的视角。

Method: 通过系统性综述的方法，对现有LLM驱动内核生成方法进行分类整理，涵盖基于LLM的方法和智能体优化工作流程，并系统性地汇编了支撑学习和评估的数据集与基准测试。

Result: 提供了该领域的结构化概览，建立了开源GitHub仓库以跟踪最新进展，为下一代自动化内核优化建立了全面的参考框架。

Conclusion: LLM驱动的内核生成是一个有前景的研究方向，但仍面临挑战。本文通过系统性综述填补了该领域的空白，为未来研究提供了参考框架和方向指引。

Abstract: The performance of modern AI systems is fundamentally constrained by the quality of their underlying kernels, which translate high-level algorithmic semantics into low-level hardware operations. Achieving near-optimal kernels requires expert-level understanding of hardware architectures and programming models, making kernel engineering a critical but notoriously time-consuming and non-scalable process. Recent advances in large language models (LLMs) and LLM-based agents have opened new possibilities for automating kernel generation and optimization. LLMs are well-suited to compress expert-level kernel knowledge that is difficult to formalize, while agentic systems further enable scalable optimization by casting kernel development as an iterative, feedback-driven loop. Rapid progress has been made in this area. However, the field remains fragmented, lacking a systematic perspective for LLM-driven kernel generation. This survey addresses this gap by providing a structured overview of existing approaches, spanning LLM-based approaches and agentic optimization workflows, and systematically compiling the datasets and benchmarks that underpin learning and evaluation in this domain. Moreover, key open challenges and future research directions are further outlined, aiming to establish a comprehensive reference for the next generation of automated kernel optimization. To keep track of this field, we maintain an open-source GitHub repository at https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation.

</details>


### [70] [Rethinking Drug-Drug Interaction Modeling as Generalizable Relation Learning](https://arxiv.org/abs/2601.15771)
*Dong Xu,Jiantao Wu,Qihua Pan,Sisi Yuan,Zexuan Zhu,Junkai Ji*

Main category: cs.LG

TL;DR: GenRel-DDI将药物相互作用预测重构为关系中心学习问题，通过独立于药物身份学习交互表示，显著提升了模型对新药物和未见药物对的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有药物相互作用预测方法虽然在标准基准上表现良好，但在实际部署场景中泛化能力不足，特别是对于未见药物和验证交互稀缺的情况。现有基于分子嵌入的方法中，嵌入空间的邻近性并不能可靠地对应交互标签。

Method: 提出GenRel-DDI框架，将DDI预测重新构建为关系中心学习问题。该方法独立于药物身份学习交互表示，通过关系级抽象捕获可转移的交互模式，从而泛化到未见药物和新药物对。

Result: 在多个基准测试上的广泛实验表明，GenRel-DDI始终显著优于现有最先进方法，特别是在严格的实体不相交评估中表现出特别大的性能提升。

Conclusion: 关系学习为稳健的药物相互作用预测提供了有效且实用的方法，能够显著提升模型在真实部署场景中的泛化能力。

Abstract: Drug-drug interaction (DDI) prediction is central to drug discovery and clinical development, particularly in the context of increasingly prevalent polypharmacy. Although existing computational methods achieve strong performance on standard benchmarks, they often fail to generalize to realistic deployment scenarios, where most candidate drug pairs involve previously unseen drugs and validated interactions are scarce. We demonstrate that proximity in the embedding spaces of prevailing molecule-centric DDI models does not reliably correspond to interaction labels, and that simply scaling up model capacity therefore fails to improve generalization. To address these limitations, we propose GenRel-DDI, a generalizable relation learning framework that reformulates DDI prediction as a relation-centric learning problem, in which interaction representations are learned independently of drug identities. This relation-level abstraction enables the capture of transferable interaction patterns that generalize to unseen drugs and novel drug pairs. Extensive experiments across multiple benchmark demonstrate that GenRel-DDI consistently and significantly outperforms state-of-the-art methods, with particularly large gains on strict entity-disjoint evaluations, highlighting the effectiveness and practical utility of relation learning for robust DDI prediction. The code is available at https://github.com/SZU-ADDG/GenRel-DDI.

</details>


### [71] [Next Generation Active Learning: Mixture of LLMs in the Loop](https://arxiv.org/abs/2601.15773)
*Yuanyuan Qi,Xiaohao Yang,Jueqing Lu,Guoxiang Guo,Joanne Enticott,Gang Liu,Lan Du*

Main category: cs.LG

TL;DR: 提出一种基于混合LLMs的主动学习框架，用多个LLMs替代人工标注，通过聚合不同LLM的优势提高标注鲁棒性，并引入标注差异性和负学习处理噪声标签。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，人们开始将其纳入主动学习流程以降低标注成本，但单个LLM生成的标注质量往往达不到实际应用要求，需要提高LLM标注的鲁棒性和可靠性。

Method: 提出Mixture of LLMs in the Loop Active Learning框架：1）用基于混合LLMs的标注模型替代人工标注；2）引入标注差异性识别不可靠标注；3）采用负学习增强学习效果；4）基于轻量级LLMs实现本地化部署。

Result: 实验表明该框架性能接近人工标注水平，显著优于单个LLM基线和其他LLM集成方法，且能在本地机器上高效运行。

Conclusion: 该混合LLMs主动学习框架有效提高了LLM标注的鲁棒性，在降低标注成本的同时保持了高质量，为实际应用提供了可行的解决方案。

Abstract: With the rapid advancement and strong generalization capabilities of large language models (LLMs), they have been increasingly incorporated into the active learning pipelines as annotators to reduce annotation costs. However, considering the annotation quality, labels generated by LLMs often fall short of real-world applicability. To address this, we propose a novel active learning framework, Mixture of LLMs in the Loop Active Learning, replacing human annotators with labels generated through a Mixture-of-LLMs-based annotation model, aimed at enhancing LLM-based annotation robustness by aggregating the strengths of multiple LLMs. To further mitigate the impact of the noisy labels, we introduce annotation discrepancy and negative learning to identify the unreliable annotations and enhance learning effectiveness. Extensive experiments demonstrate that our framework achieves performance comparable to human annotation and consistently outperforms single-LLM baselines and other LLM-ensemble-based approaches. Moreover, our framework is built on lightweight LLMs, enabling it to operate fully on local machines in real-world applications.

</details>


### [72] [Attributing and Exploiting Safety Vectors through Global Optimization in Large Language Models](https://arxiv.org/abs/2601.15801)
*Fengheng Chu,Jiahao Chen,Yuhong Wang,Jun Wang,Zhihui Fu,Shouling Ji,Songze Li*

Main category: cs.LG

TL;DR: 本文提出了GOSV框架，通过全局优化识别LLM中的安全关键注意力头，发现恶意注入向量和安全抑制向量两种空间不同的安全向量，并基于此开发了新的白盒越狱攻击方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的安全护栏在面对越狱攻击时仍然脆弱，表明对安全机制组件的理解有限。现有方法基于局部、贪婪的归因分析，假设组件贡献独立，但忽略了注意力头等组件之间的协作交互。

Method: 提出了GOSV（全局优化安全向量提取）框架，通过全局优化同时识别所有注意力头中的安全关键头。采用两种互补的激活重修补策略：有害修补和零消融，识别出空间不同的安全向量集。

Result: 发现对齐的LLM为安全目的维护着独立的功能通路，识别出恶意注入向量和安全抑制向量。当约30%的总头被重修补时，所有模型都会出现完全的安全崩溃。基于此开发的白盒越狱攻击在所有测试模型上显著优于现有方法。

Conclusion: GOSV框架在LLM安全可解释性方面具有有效性，通过全局优化方法揭示了安全机制中的协作交互，为理解LLM安全组件提供了新视角。

Abstract: While Large Language Models (LLMs) are aligned to mitigate risks, their safety guardrails remain fragile against jailbreak attacks. This reveals limited understanding of components governing safety. Existing methods rely on local, greedy attribution that assumes independent component contributions. However, they overlook the cooperative interactions between different components in LLMs, such as attention heads, which jointly contribute to safety mechanisms. We propose \textbf{G}lobal \textbf{O}ptimization for \textbf{S}afety \textbf{V}ector Extraction (GOSV), a framework that identifies safety-critical attention heads through global optimization over all heads simultaneously. We employ two complementary activation repatching strategies: Harmful Patching and Zero Ablation. These strategies identify two spatially distinct sets of safety vectors with consistently low overlap, termed Malicious Injection Vectors and Safety Suppression Vectors, demonstrating that aligned LLMs maintain separate functional pathways for safety purposes. Through systematic analyses, we find that complete safety breakdown occurs when approximately 30\% of total heads are repatched across all models. Building on these insights, we develop a novel inference-time white-box jailbreak method that exploits the identified safety vectors through activation repatching. Our attack substantially outperforms existing white-box attacks across all test models, providing strong evidence for the effectiveness of the proposed GOSV framework on LLM safety interpretability.

</details>


### [73] [Why Inference in Large Models Becomes Decomposable After Training](https://arxiv.org/abs/2601.15871)
*Jidong Jin*

Main category: cs.LG

TL;DR: 论文提出了一种后训练的结构化推理方法，通过识别和移除训练后统计上不显著的参数依赖关系，将大型AI模型分解为独立的子结构，实现并行推理而不改变模型功能。


<details>
  <summary>Details</summary>
Motivation: 当前大规模AI模型的推理通常在密集参数矩阵上进行，导致推理成本和系统复杂度随模型规模不可持续地增长。这种限制并非源于模型容量不足，而是因为将后训练推理系统视为整体操作符，忽略了学习过程中形成的内部结构。

Method: 研究发现大型模型中的梯度更新事件具有高度局部性和选择性，许多参数依赖关系在训练后与其初始化分布在统计上无法区分。基于此，提出了后训练统计准则和结构退火过程，移除不受支持的依赖关系并揭示稳定的独立子结构。

Result: 该方法建立了后训练、模型无关的推理系统结构视图，实现了结构化并行推理，无需修改模型功能或接口，显著降低了推理成本和系统复杂度。

Conclusion: 通过利用大型模型中固有的结构非均匀性和可分解性，可以构建更高效的后训练推理系统，为大规模AI模型的可持续部署提供了新的结构化视角。

Abstract: Inference in large-scale AI models is typically performed on dense parameter matrices, leading to inference cost and system complexity that scale unsustainably with model size. This limitation does not arise from insufficient model capacity, but from treating post-training inference systems as monolithic operators while ignoring internal structures formed during learning. We show that gradient update events in large models are highly localized and selective, leaving many parameter dependencies statistically indistinguishable from their initialization distribution after training. As a result, post-training inference systems are structurally non-uniform and inherently decomposable. Based on this observation, we introduce a post-training statistical criterion and a structural annealing procedure that removes unsupported dependencies and reveals stable, independent substructures. This work establishes a post-training, model-agnostic structural view of inference systems and enables structured, parallel inference without modifying model functionality or interfaces.

</details>


### [74] [SoK: Challenges in Tabular Membership Inference Attacks](https://arxiv.org/abs/2601.15874)
*Cristina Pêra,Tânia Carvalho,Maxime Cordy,Luís Antunes*

Main category: cs.LG

TL;DR: 该论文对成员推理攻击在表格数据中的有效性进行了全面分析，发现其在表格数据中表现普遍较差，但能有效识别单例记录，且使用不同替代模型可提高攻击效果。


<details>
  <summary>Details</summary>
Motivation: 成员推理攻击是评估机器学习隐私的主要方法，但在表格数据中的有效性尚未充分探索，特别是在联邦学习场景中外敌威胁和单例记录脆弱性方面存在研究空白。

Method: 1) 对集中式和联邦学习中的MIA进行系统分类和扩展；2) 在表格数据上测试多种攻击策略和防御方法；3) 考虑联邦学习中的外部敌手威胁；4) 分析单例记录的脆弱性；5) 探索攻击在不同模型架构间的迁移性。

Result: 1) MIA在表格数据中普遍表现较差，与先前研究形成对比；2) 即使攻击性能有限，仍能成功暴露大量单例记录；3) 使用不同替代模型可提高MIA的有效性；4) 攻击在不同模型架构间具有可迁移性。

Conclusion: MIA在表格数据中的隐私评估效果有限，但单例记录高度脆弱，且攻击策略的改进（如使用多样化替代模型）可提升攻击效果，这对隐私保护策略设计具有重要启示。

Abstract: Membership Inference Attacks (MIAs) are currently a dominant approach for evaluating privacy in machine learning applications. Despite their significance in identifying records belonging to the training dataset, several concerns remain unexplored, particularly with regard to tabular data. In this paper, first, we provide an extensive review and analysis of MIAs considering two main learning paradigms: centralized and federated learning. We extend and refine the taxonomy for both. Second, we demonstrate the efficacy of MIAs in tabular data using several attack strategies, also including defenses. Furthermore, in a federated learning scenario, we consider the threat posed by an outsider adversary, which is often neglected. Third, we demonstrate the high vulnerability of single-outs (records with a unique signature) to MIAs. Lastly, we explore how MIAs transfer across model architectures. Our results point towards a general poor performance of these attacks in tabular data which contrasts with previous state-of-the-art. Notably, even attacks with limited attack performance can still successfully expose a large portion of single-outs. Moreover, our findings suggest that using different surrogate models makes MIAs more effective.

</details>


### [75] [Iterative Amortized Hierarchical VAE](https://arxiv.org/abs/2601.15894)
*Simon W. Penninga,Ruud J. G. van Sloun*

Main category: cs.LG

TL;DR: IA-HVAE通过结合摊销推理和迭代优化，在傅里叶空间实现线性可分离解码器，相比传统HVAE推理速度提升35倍，在逆问题中表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统分层变分自编码器（HVAE）在推理过程中存在效率瓶颈，特别是在需要高模型深度时。作者希望开发一种既能保持摊销推理效率又能通过迭代优化提升精度的混合方法。

Method: 提出迭代摊销分层变分自编码器（IA-HVAE），采用混合推理方案：先进行摊销推理获得初始猜测，然后利用解码器梯度进行迭代优化。关键创新是在变换域（如傅里叶空间）创建线性可分离的解码器结构，支持实时应用和高模型深度。

Result: 1. 相比传统HVAE，迭代推理速度提升35倍；2. 混合方法在准确性和速度上分别优于完全摊销和完全迭代的对应方法；3. 在去模糊和去噪等逆问题中，IA-HVAE比原始HVAE具有更好的重建质量。

Conclusion: IA-HVAE通过结合摊销推理的效率和迭代优化的精度，在保持实时性能的同时显著提升了推理速度和重建质量，为高深度模型的逆问题求解提供了有效解决方案。

Abstract: In this paper we propose the Iterative Amortized Hierarchical Variational Autoencoder (IA-HVAE), which expands on amortized inference with a hybrid scheme containing an initial amortized guess and iterative refinement with decoder gradients. We achieve this by creating a linearly separable decoder in a transform domain (e.g. Fourier space), enabling real-time applications with very high model depths. The architectural change leads to a 35x speed-up for iterative inference with respect to the traditional HVAE. We show that our hybrid approach outperforms fully amortized and fully iterative equivalents in accuracy and speed respectively. Moreover, the IAHVAE shows improved reconstruction quality over a vanilla HVAE in inverse problems such as deblurring and denoising.

</details>


### [76] [Partially Lazy Gradient Descent for Smoothed Online Learning](https://arxiv.org/abs/2601.15984)
*Naram Mhaisen,George Iosifidis*

Main category: cs.LG

TL;DR: k-lazyGD算法在平滑在线凸优化中连接了贪婪OGD和惰性GD，通过懒惰松弛参数k在反应性和稳定性之间建立连续谱，证明可在不牺牲命中性能的情况下实现懒惰更新。


<details>
  <summary>Details</summary>
Motivation: 研究在线学习算法中反应性更新（贪婪OGD）和稳定性更新（惰性GD/双平均）之间的权衡，探索在平滑在线凸优化中如何在不损失命中性能的情况下实现懒惰更新。

Method: 提出k-lazyGD算法，基于FTRL框架，通过懒惰松弛参数k在贪婪OGD（k=1）和惰性GD（k=T）之间创建连续谱，分析不同k值下的性能表现。

Result: 证明k-lazyGD对任何懒惰松弛k达到Θ(√(T/P_T))时都能实现最优动态遗憾O(√((P_T+1)T))，其中P_T是比较器路径长度，表明懒惰程度可与比较器变化量相适应。

Conclusion: 懒惰更新可以在不牺牲命中性能的情况下实现，k-lazyGD通过连接反应性和稳定性更新，创建了可根据比较器动态调整的算法谱系，使用不同松弛参数的集成方法可在稳定时保持稳定，需要敏捷时保持敏捷。

Abstract: We introduce $k$-lazyGD, an online learning algorithm that bridges the gap between greedy Online Gradient Descent (OGD, for $k=1$) and lazy GD/dual-averaging (for $k=T$), creating a spectrum between reactive and stable updates. We analyze this spectrum in Smoothed Online Convex Optimization (SOCO), where the learner incurs both hitting and movement costs. Our main contribution is establishing that laziness is possible without sacrificing hitting performance: we prove that $k$-lazyGD achieves the optimal dynamic regret $\mathcal{O}(\sqrt{(P_T+1)T})$ for any laziness slack $k$ up to $Θ(\sqrt{T/P_T})$, where $P_T$ is the comparator path length. This result formally connects the allowable laziness to the comparator's shifts, showing that $k$-lazyGD can retain the inherently small movements of lazy methods without compromising tracking ability. We base our analysis on the Follow the Regularized Leader (FTRL) framework, and derive a matching lower bound. Since the slack depends on $P_T$, an ensemble of learners with various slacks is used, yielding a method that is provably stable when it can be, and agile when it must be.

</details>


### [77] [Data-Driven Conditional Flexibility Index](https://arxiv.org/abs/2601.16028)
*Moritz Wedemeyer,Eike Cramer,Alexander Mitsos,Manuel Dahmen*

Main category: cs.LG

TL;DR: 提出条件灵活性指数（CFI），通过从历史数据学习参数化可接受不确定性集，并利用上下文信息使其具有条件性，从而改进传统灵活性指数方法。


<details>
  <summary>Details</summary>
Motivation: 随着流程灵活性的增加，制定稳健的调度决策变得重要。传统灵活性指数使用简单可接受不确定性集（如超立方体）来近似可接受不确定性区域，但未考虑可用上下文信息（如预测）来定义可接受不确定性集。

Method: 提出条件灵活性指数（CFI）：1）使用归一化流从历史数据学习参数化可接受不确定性集；2）利用上下文信息使可接受不确定性集具有条件性。通过归一化流学习从高斯基分布到数据分布的双射映射，在潜在空间中构建超球面作为可接受潜在不确定性集，然后映射到数据空间。

Result: 通过示例说明：不能一概而论数据驱动的可接受不确定性集优于简单集，或条件集优于无条件集。但数据驱动和条件可接受不确定性集确保只考虑包含实际实现的参数空间区域。应用于安全约束机组组合示例，证明CFI能通过纳入时间信息提高调度质量。

Conclusion: 条件灵活性指数通过从数据学习可接受不确定性集并纳入上下文信息，提供了更灵活的灵活性评估方法，能改进调度决策质量，特别是在考虑时间信息等上下文因素时。

Abstract: With the increasing flexibilization of processes, determining robust scheduling decisions has become an important goal. Traditionally, the flexibility index has been used to identify safe operating schedules by approximating the admissible uncertainty region using simple admissible uncertainty sets, such as hypercubes. Presently, available contextual information, such as forecasts, has not been considered to define the admissible uncertainty set when determining the flexibility index. We propose the conditional flexibility index (CFI), which extends the traditional flexibility index in two ways: by learning the parametrized admissible uncertainty set from historical data and by using contextual information to make the admissible uncertainty set conditional. This is achieved using a normalizing flow that learns a bijective mapping from a Gaussian base distribution to the data distribution. The admissible latent uncertainty set is constructed as a hypersphere in the latent space and mapped to the data space. By incorporating contextual information, the CFI provides a more informative estimate of flexibility by defining admissible uncertainty sets in regions that are more likely to be relevant under given conditions. Using an illustrative example, we show that no general statement can be made about data-driven admissible uncertainty sets outperforming simple sets, or conditional sets outperforming unconditional ones. However, both data-driven and conditional admissible uncertainty sets ensure that only regions of the uncertain parameter space containing realizations are considered. We apply the CFI to a security-constrained unit commitment example and demonstrate that the CFI can improve scheduling quality by incorporating temporal information.

</details>


### [78] [CLASP: An online learning algorithm for Convex Losses And Squared Penalties](https://arxiv.org/abs/2601.16072)
*Ricardo N. Ferreira,Cláudia Soares,João Xavier*

Main category: cs.LG

TL;DR: CLASP算法在约束在线凸优化中同时最小化累积损失和平方约束违反，在强凸问题中首次实现了对数级别的遗憾和累积平方惩罚保证。


<details>
  <summary>Details</summary>
Motivation: 研究约束在线凸优化问题，其中学习者在迭代选择动作时面临不可预见的凸损失和凸约束，需要在累积损失的同时处理约束违反的惩罚。现有方法在强凸问题中缺乏对数级别的性能保证。

Method: 提出CLASP算法，通过充分利用凸投影算子的强非扩张性这一新的证明策略，同时最小化累积损失和平方约束违反。算法针对凸损失和强凸问题分别进行分析。

Result: 对于凸损失，CLASP实现遗憾$O(T^{\max\{β,1-β\}})$和累积平方惩罚$O(T^{1-β})$；对于强凸问题，首次获得遗憾$O(\log T)$和累积平方惩罚$O(\log T)$的对数级别保证。

Conclusion: CLASP算法在约束在线凸优化中取得了突破性进展，特别是在强凸问题上首次实现了对数级别的遗憾和约束违反保证，为这一领域提供了新的理论基准。

Abstract: We study Constrained Online Convex Optimization (COCO), where a learner chooses actions iteratively, observes both unanticipated convex loss and convex constraint, and accumulates loss while incurring penalties for constraint violations. We introduce CLASP (Convex Losses And Squared Penalties), an algorithm that minimizes cumulative loss together with squared constraint violations. Our analysis departs from prior work by fully leveraging the firm non-expansiveness of convex projectors, a proof strategy not previously applied in this setting. For convex losses, CLASP achieves regret $O\left(T^{\max\{β,1-β\}}\right)$ and cumulative squared penalty $O\left(T^{1-β}\right)$ for any $β\in (0,1)$. Most importantly, for strongly convex problems, CLASP provides the first logarithmic guarantees on both regret and cumulative squared penalty. In the strongly convex case, the regret is upper bounded by $O( \log T )$ and the cumulative squared penalty is also upper bounded by $O( \log T )$.

</details>


### [79] [Explainable AI to Improve Machine Learning Reliability for Industrial Cyber-Physical Systems](https://arxiv.org/abs/2601.16074)
*Annemarie Jutte,Uraz Odyurt*

Main category: cs.LG

TL;DR: 该研究将可解释AI（XAI）应用于工业信息物理系统（CPS）中的机器学习模型，通过SHAP值分析时间序列数据分解组件对预测的影响，发现训练数据上下文信息不足，通过增加数据窗口大小提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 工业CPS对安全和经济效益都至关重要，需要高可靠性。虽然深度学习越来越多地应用于工业CPS，但其模型复杂性导致操作不透明，需要严格评估以防止模型在未来未见数据上出现意外行为。可解释AI可用于揭示模型推理过程，从而进行更全面的行为分析。

Method: 应用可解释AI（XAI）来改进工业CPS中机器学习模型的预测性能。具体使用SHAP值分析时间序列数据分解组件对模型预测的影响，通过XAI发现训练数据上下文信息不足的问题，然后基于这些发现增加数据实例的窗口大小。

Result: 通过XAI分析发现了模型训练中上下文信息不足的证据。基于XAI的发现，通过增加数据窗口大小，成功提高了模型的性能表现。

Conclusion: 可解释AI不仅可以帮助理解机器学习模型的推理过程，还能为改进模型性能提供具体指导。在工业CPS应用中，通过XAI分析发现训练数据上下文不足的问题，并通过增加窗口大小有效提升了模型预测性能，证明了XAI在工业CPS可靠性保障中的实用价值。

Abstract: Industrial Cyber-Physical Systems (CPS) are sensitive infrastructure from both safety and economics perspectives, making their reliability critically important. Machine Learning (ML), specifically deep learning, is increasingly integrated in industrial CPS, but the inherent complexity of ML models results in non-transparent operation. Rigorous evaluation is needed to prevent models from exhibiting unexpected behaviour on future, unseen data. Explainable AI (XAI) can be used to uncover model reasoning, allowing a more extensive analysis of behaviour. We apply XAI to to improve predictive performance of ML models intended for industrial CPS. We analyse the effects of components from time-series data decomposition on model predictions using SHAP values. Through this method, we observe evidence on the lack of sufficient contextual information during model training. By increasing the window size of data instances, informed by the XAI findings, we are able to improve model performance.

</details>


### [80] [Probably Approximately Correct Maximum A Posteriori Inference](https://arxiv.org/abs/2601.16083)
*Matthew Shorvon,Frederik Mallmann-Trenn,David S. Watson*

Main category: cs.LG

TL;DR: 该论文提出了用于MAP推理的PAC算法，在可变和固定计算预算下提供可证明的最优解，使用概率电路实现，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: MAP估计是概率推理中的基本任务，但通常难以计算，即使在许多常见的结构约束和近似方案下仍然困难。需要开发具有理论保证的MAP推理方法。

Method: 引入PAC-MAP算法，使用信息论度量来表征可处理性条件，通过概率电路高效实现，开发随机化策略作为独立的MAP推理技术或改进现有启发式方法。

Result: 实验证明该方法在一系列基准测试中具有优势，能够提供具有严格理论保证的解决方案。

Conclusion: PAC-MAP算法为MAP推理提供了具有理论保证的解决方案，通过信息论度量和概率电路实现，能够有效处理传统上难以计算的MAP估计问题。

Abstract: Computing the conditional mode of a distribution, better known as the $\mathit{maximum\ a\ posteriori}$ (MAP) assignment, is a fundamental task in probabilistic inference. However, MAP estimation is generally intractable, and remains hard even under many common structural constraints and approximation schemes. We introduce $\mathit{probably\ approximately\ correct}$ (PAC) algorithms for MAP inference that provide provably optimal solutions under variable and fixed computational budgets. We characterize tractability conditions for PAC-MAP using information theoretic measures that can be estimated from finite samples. Our PAC-MAP solvers are efficiently implemented using probabilistic circuits with appropriate architectures. The randomization strategies we develop can be used either as standalone MAP inference techniques or to improve on popular heuristics, fortifying their solutions with rigorous guarantees. Experiments confirm the benefits of our method in a range of benchmarks.

</details>


### [81] [Benchmarking Deep Learning Models for Raman Spectroscopy Across Open-Source Datasets](https://arxiv.org/abs/2601.16107)
*Adithya Sineesh,Akshita Kamsali*

Main category: cs.LG

TL;DR: 该研究首次系统性地比较了多个专门针对拉曼光谱的深度学习分类器，在三个开源数据集上评估了五种代表性架构，提供了公平可复现的基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前拉曼光谱深度学习分类器的评估往往孤立进行，或仅与传统机器学习方法比较，缺乏专门针对拉曼光谱的深度学习模型之间的直接对比。现有研究缺少在共享开源数据集上对多个拉曼专用深度学习模型的系统性比较。

Method: 选择了五种代表性的深度学习架构，在三个开源拉曼数据集上采用统一的训练和超参数调优协议进行评估。数据集选择支持标准评估、微调和显式分布偏移测试。使用分类准确率和宏平均F1分数作为评价指标。

Result: 研究提供了拉曼光谱分类深度学习模型的公平可复现比较结果，报告了各模型在不同数据集上的分类准确率和F1分数，为后续研究提供了基准参考。

Conclusion: 该研究填补了拉曼光谱深度学习模型系统性比较的空白，为研究人员提供了首个多模型、多数据集的基准测试框架，有助于推动该领域的发展。

Abstract: Deep learning classifiers for Raman spectroscopy are increasingly reported to outperform classical chemometric approaches. However their evaluations are often conducted in isolation or compared against traditional machine learning methods or trivially adapted vision-based architectures that were not originally proposed for Raman spectroscopy. As a result, direct comparisons between existing deep learning models developed specifically for Raman spectral analysis on shared open-source datasets remain scarce. To the best of our knowledge, this study presents one of the first systematic benchmarks comparing three or more published Raman-specific deep learning classifiers across multiple open-source Raman datasets. We evaluate five representative deep learning architectures under a unified training and hyperparameter tuning protocol across three open-source Raman datasets selected to support standard evaluation, fine-tuning, and explicit distribution-shift testing. We report classification accuracies and macro-averaged F1 scores to provide a fair and reproducible comparison of deep learning models for Raman spectra based classification.

</details>


### [82] [On the Intrinsic Dimensions of Data in Kernel Learning](https://arxiv.org/abs/2601.16139)
*Rustem Takhanov*

Main category: cs.LG

TL;DR: 该论文研究了流形假设下核岭回归的泛化性能，提出了两种内在维度概念，分析了它们与特征值衰减的关系，并给出了误差界和估计算法。


<details>
  <summary>Details</summary>
Motivation: 流形假设认为当输入分布的内在维度较低时，机器学习方法的泛化性能会显著提高。本文旨在研究核岭回归中两种不同的内在维度概念，并分析它们如何影响泛化性能。

Method: 提出了两种内在维度：基于核函数诱导度量的Minkowski维数d_ρ和基于Kolmogorov n-宽度衰减的有效维数d_K。分析了n-宽度与积分算子特征值的关系，证明了Kolmogorov n-宽度刻画了所有概率测度下的最坏特征值衰减。基于此推导了约束核岭回归的误差界，并提出了使用有限样本估计n-宽度上界的算法。

Result: 得到了约束核岭回归的过拟合误差界为O(n^{-(2+d_K)/(2+2d_K)+ε})。证明了对于接近均匀的分布，可以使用O(ε^{-d_ρ}log(1/ε))个样本以高概率计算所有n-宽度的ε-精确上界。计算了各种分形集的有效维数d_K，发现对于Laplace核等核函数，d_K可以显著小于Minkowski维数d_ρ。

Conclusion: 本文建立了内在维度与核岭回归泛化性能的理论联系，证明了有效维数d_K比Minkowski维数d_ρ更能准确反映泛化性能，并提供了实用的估计算法。这些结果为理解流形假设下的学习理论提供了新的视角。

Abstract: The manifold hypothesis suggests that the generalization performance of machine learning methods improves significantly when the intrinsic dimension of the input distribution's support is low. In the context of KRR, we investigate two alternative notions of intrinsic dimension. The first, denoted $d_ρ$, is the upper Minkowski dimension defined with respect to the canonical metric induced by a kernel function $K$ on a domain $Ω$. The second, denoted $d_K$, is the effective dimension, derived from the decay rate of Kolmogorov $n$-widths associated with $K$ on $Ω$. Given a probability measure $μ$ on $Ω$, we analyze the relationship between these $n$-widths and eigenvalues of the integral operator $φ\to \int_ΩK(\cdot,x)φ(x)dμ(x)$. We show that, for a fixed domain $Ω$, the Kolmogorov $n$-widths characterize the worst-case eigenvalue decay across all probability measures $μ$ supported on $Ω$. These eigenvalues are central to understanding the generalization behavior of constrained KRR, enabling us to derive an excess error bound of order $O(n^{-\frac{2+d_K}{2+2d_K} + ε})$ for any $ε> 0$, when the training set size $n$ is large. We also propose an algorithm that estimates upper bounds on the $n$-widths using only a finite sample from $μ$. For distributions close to uniform, we prove that $ε$-accurate upper bounds on all $n$-widths can be computed with high probability using at most $O\left(ε^{-d_ρ}\log\frac{1}ε\right)$ samples, with fewer required for small $n$. Finally, we compute the effective dimension $d_K$ for various fractal sets and present additional numerical experiments. Our results show that, for kernels such as the Laplace kernel, the effective dimension $d_K$ can be significantly smaller than the Minkowski dimension $d_ρ$, even though $d_K = d_ρ$ provably holds on regular domains.

</details>


### [83] [Beat-ssl: Capturing Local ECG Morphology through Heartbeat-level Contrastive Learning with Soft Targets](https://arxiv.org/abs/2601.16147)
*Muhammad Ilham Rizqyawan,Peter Macfarlane,Stathis Hadjidemetriou,Fani Deligianni*

Main category: cs.LG

TL;DR: Beat-SSL是一个针对ECG信号的双上下文对比学习框架，通过节奏级和心跳级对比结合软目标，在有限标注数据下实现有效迁移学习。


<details>
  <summary>Details</summary>
Motivation: 获取标注ECG数据困难，现有对比学习框架要么只关注全局上下文，要么未能充分利用ECG特异性特征，且依赖硬对比目标无法充分捕捉ECG信号特征的连续性。

Method: 提出Beat-SSL框架，通过节奏级和心跳级双上下文对比学习，使用软目标来更好地捕捉ECG特征的连续相似性。

Result: 在多标签分类任务中达到ECG基础模型93%的性能，在分割任务中超越所有其他方法4%。

Conclusion: Beat-SSL通过双上下文对比学习和软目标，在有限标注数据下实现了有效的ECG表示学习，在多个下游任务中表现出色。

Abstract: Obtaining labelled ECG data for developing supervised models is challenging. Contrastive learning (CL) has emerged as a promising pretraining approach that enables effective transfer learning with limited labelled data. However, existing CL frameworks either focus solely on global context or fail to exploit ECG-specific characteristics. Furthermore, these methods rely on hard contrastive targets, which may not adequately capture the continuous nature of feature similarity in ECG signals. In this paper, we propose Beat-SSL, a contrastive learning framework that performs dual-context learning through both rhythm-level and heartbeat-level contrasting with soft targets. We evaluated our pretrained model on two downstream tasks: 1) multilabel classification for global rhythm assessment, and 2) ECG segmentation to assess its capacity to learn representations across both contexts. We conducted an ablation study and compared the best configuration with three other methods, including one ECG foundation model. Despite the foundation model's broader pretraining, Beat-SSL reached 93% of its performance in multilabel classification task and surpassed all other methods in the segmentation task by 4%.

</details>


### [84] [Learning to Discover at Test Time](https://arxiv.org/abs/2601.16175)
*Mert Yuksekgonul,Daniel Koceja,Xinhao Li,Federico Bianchi,Jed McCaleb,Xiaolong Wang,Jan Kautz,Yejin Choi,James Zou,Carlos Guestrin,Yu Sun*

Main category: cs.LG

TL;DR: TTT-Discover通过测试时强化学习让LLM针对特定问题持续训练，在多个科学领域实现了新的最先进成果


<details>
  <summary>Details</summary>
Motivation: 现有测试时缩放方法（如AlphaEvolve）使用冻结的LLM进行搜索，无法针对特定测试问题进行持续学习和优化。需要一种方法让LLM在测试时能够基于具体问题经验进行强化学习，专注于生成单个优秀解决方案而非平均表现。

Method: 提出TTT-Discover方法，通过测试时训练进行强化学习，设计专门的学习目标和搜索子程序来优先考虑最有希望的解决方案。使用开源模型OpenAI gpt-oss-120b，通过Tinker API进行测试时训练，每个问题成本仅几百美元。

Result: 在多个领域取得新的最先进成果：(1)数学：Erdős最小重叠问题和自相关不等式；(2)GPU内核工程：GPUMode内核竞赛（比先前技术快2倍）；(3)算法设计：过去AtCoder算法竞赛；(4)生物学：单细胞分析中的去噪问题。所有结果均使用开源模型实现且可复现。

Conclusion: TTT-Discover通过在测试时进行强化学习，使LLM能够针对特定科学问题持续优化，在多个领域实现了新的最先进成果，且使用开源模型和公开代码确保了可复现性，为AI驱动的科学发现提供了有效方法。

Abstract: How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.

</details>


### [85] [Counterfactual Training: Teaching Models Plausible and Actionable Explanations](https://arxiv.org/abs/2601.16205)
*Patrick Altmeyer,Aleksander Buszydlik,Arie van Deursen,Cynthia C. S. Liem*

Main category: cs.LG

TL;DR: 提出一种名为"反事实训练"的新训练范式，利用反事实解释来增强模型的可解释性能力，使模型在训练阶段就学习生成合理且可操作的反事实解释。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法多为后处理技术，虽然能生成合理且可操作的反事实解释，但模型本身并不具备这种解释能力。研究旨在让模型在训练阶段就直接学习生成符合要求的反事实解释，而不是依赖后处理。

Method: 提出反事实训练方法，在训练阶段使用反事实解释来最小化学到的表示与合理、可操作解释之间的差异。通过理论分析和实证验证，使模型能够提供内在符合要求的反事实解释。

Result: 实验和理论证明表明，该方法能够训练出能够提供内在符合要求的反事实解释的模型，并且这些模型还表现出改进的对抗鲁棒性。

Conclusion: 反事实训练是一种有效的训练范式，能够使机器学习模型在训练阶段就具备生成合理且可操作反事实解释的能力，同时提升模型的对抗鲁棒性，为构建更透明、可信的决策系统提供了新思路。

Abstract: We propose a novel training regime termed counterfactual training that leverages counterfactual explanations to increase the explanatory capacity of models. Counterfactual explanations have emerged as a popular post-hoc explanation method for opaque machine learning models: they inform how factual inputs would need to change in order for a model to produce some desired output. To be useful in real-world decision-making systems, counterfactuals should be plausible with respect to the underlying data and actionable with respect to the feature mutability constraints. Much existing research has therefore focused on developing post-hoc methods to generate counterfactuals that meet these desiderata. In this work, we instead hold models directly accountable for the desired end goal: counterfactual training employs counterfactuals during the training phase to minimize the divergence between learned representations and plausible, actionable explanations. We demonstrate empirically and theoretically that our proposed method facilitates training models that deliver inherently desirable counterfactual explanations and additionally exhibit improved adversarial robustness.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [86] [Structural constraints on mobility edges in one-dimensional quasiperiodic systems](https://arxiv.org/abs/2601.15799)
*Sanghoon Lee,Tilen Cadez,Kyoung-Min Kim*

Main category: cond-mat.dis-nn

TL;DR: 准周期系统中的迁移率边缘位置受等谱对偶性约束，而非独立特征，导致其在自对偶极限下收敛于单一局域化-退局域化转变点。


<details>
  <summary>Details</summary>
Motivation: 研究准周期系统中迁移率边缘的起源，传统上仅从单个哈密顿量层面理解，本文旨在揭示迁移率边缘位置在等谱对偶性相关的哈密顿量之间存在结构性约束。

Method: 使用双色Aubry-André模型作为最小设置，基于Thouless公式推导Lyapunov指数的精确恒等式，分析迁移率边缘位置的结构性约束。

Result: 迁移率边缘位置被限制在能量子集中，在自对偶极限下收敛于单一局域化-退局域化转变点，并导致物理Lyapunov谱在自对偶点附近呈现线性临界标度。

Conclusion: 迁移率边缘位置不是单个哈密顿量的独立谱特征，而是受等谱对偶性约束的结构性特征，这为理解准周期系统中的迁移率边缘提供了新视角。

Abstract: Mobility edges commonly arise in one-dimensional quasiperiodic systems once exact self-duality is broken, yet their origin is typically understood only at the level of individual Hamiltonians. Here we show that mobility edge positions are not independent spectral features of individual Hamiltonians, but are structurally constrained across quasiperiodic Hamiltonians related by an isospectral duality. Using a bichromatic Aubry--André model as a minimal setting, we demonstrate that this constraint is encoded in an exact identity for Lyapunov exponents derived from the Thouless formula. As a consequence, the mobility edge positions are restricted to a reduced set of energies. In the self-dual limit, these mobility edge positions coincide at a single localization--delocalization transition. This structural constraint enforces a linear critical scaling of the physical Lyapunov spectrum near the self-dual point. Numerical results confirm a critical exponent consistent with the standard Aubry--André value of $ν= 1$, while simultaneously revealing a novel, non-universal energy-dependent prefactor.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [87] [Gated Sparse Attention: Combining Computational Efficiency with Training Stability for Long-Context Language Models](https://arxiv.org/abs/2601.15305)
*Alfred Shen,Aaron Shen*

Main category: cs.AI

TL;DR: Gated Sparse Attention (GSA) 结合了稀疏注意力和门控注意力的优势，在长上下文语言模型中实现了效率和质量的双重提升，同时改善了训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 长上下文语言模型中注意力计算的计算负担促使了稀疏注意力和门控注意力两条独立的研究路线，但它们各自存在弱点，需要互补的解决方案。

Method: 提出Gated Sparse Attention (GSA)架构，包含：1）带sigmoid激活的门控闪电索引器，产生有界、可解释的选择分数；2）基于局部不确定性调节关注token数量的自适应稀疏控制器；3）值和输出阶段的双重门控机制。

Result: 在1.7B参数模型、400B token训练的实验表明：GSA在128K上下文长度下达到12-16倍加速，困惑度从6.03提升到5.70，RULER分数几乎翻倍，首token注意力从47%降至4%以下，训练稳定性显著改善，损失峰值减少98%。

Conclusion: GSA成功结合了稀疏注意力的效率和门控注意力的质量优势，为长上下文语言模型提供了高效、稳定且高质量的注意力机制解决方案。

Abstract: The computational burden of attention in long-context language models has motivated two largely independent lines of work: sparse attention mechanisms that reduce complexity by attending to selected tokens, and gated attention variants that improve training sta-bility while mitigating the attention sink phenomenon. We observe that these approaches address complementary weaknesses and propose Gated Sparse Attention (GSA), an architecture that realizes the benefits of both. GSA incorporates a gated lightning indexer with sigmoid activations that produce bounded, interpretable selection scores, an adaptive sparsity controller that modulates the number of attended tokens based on local uncertainty, and dual gating at the value and output stages. We establish theoretical foundations for the approach, including complexity analysis, expressiveness results, and convergence guarantees. In experiments with 1.7B parameter models trained on 400B tokens, GSA matches the efficiency of sparse-only baselines (12-16x speedup at 128K context) while achieving the quality gains associated with gated attention: perplexity improves from 6.03 to 5.70, RULER scores at 128K context nearly double, and attention to the first token, a proxy for attention sinks, drops from 47% to under 4%. Training stability improves markedly, with loss spikes reduced by 98%.

</details>


### [88] [DeepSurvey-Bench: Evaluating Academic Value of Automatically Generated Scientific Survey](https://arxiv.org/abs/2601.15307)
*Guo-Biao Zhang,Ding-Yuan Liu,Da-Yi Wu,Tian Lan,Heyan Huang,Zhijing Wu,Xian-Ling Mao*

Main category: cs.AI

TL;DR: DeepSurvey-Bench是一个评估生成式科学综述学术价值的新基准，解决了现有基准只关注表面质量而忽视深层学术价值的问题。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准存在两个关键问题：1）基于引用数量等有缺陷标准选择的人工撰写综述作为基准数据集不可靠；2）评估指标只关注逻辑连贯性等表面质量，无法评估综述的深层学术价值，如核心研究目标和批判性分析。

Method: 提出了一个全面的学术价值评估标准，涵盖三个维度：信息价值、学术交流价值和研究指导价值。基于此标准构建了带有学术价值标注的可靠数据集，用于评估生成式综述的深层学术价值。

Result: 广泛的实验结果表明，该基准在评估生成式综述学术价值方面与人类评估表现高度一致。

Conclusion: DeepSurvey-Bench能够全面评估生成式科学综述的学术价值，解决了现有基准的局限性，为自动生成综述技术的质量评估提供了更可靠的基准。

Abstract: The rapid development of automated scientific survey generation technology has made it increasingly important to establish a comprehensive benchmark to evaluate the quality of generated surveys.Nearly all existing evaluation benchmarks rely on flawed selection criteria such as citation counts and structural coherence to select human-written surveys as the ground truth survey datasets, and then use surface-level metrics such as structural quality and reference relevance to evaluate generated surveys.However, these benchmarks have two key issues: (1) the ground truth survey datasets are unreliable because of a lack academic dimension annotations; (2) the evaluation metrics only focus on the surface quality of the survey such as logical coherence. Both issues lead to existing benchmarks cannot assess to evaluate their deep "academic value", such as the core research objectives and the critical analysis of different studies. To address the above problems, we propose DeepSurvey-Bench, a novel benchmark designed to comprehensively evaluate the academic value of generated surveys. Specifically, our benchmark propose a comprehensive academic value evaluation criteria covering three dimensions: informational value, scholarly communication value, and research guidance value. Based on this criteria, we construct a reliable dataset with academic value annotations, and evaluate the deep academic value of the generated surveys. Extensive experimental results demonstrate that our benchmark is highly consistent with human performance in assessing the academic value of generated surveys.

</details>


### [89] [Aeon: High-Performance Neuro-Symbolic Memory Management for Long-Horizon LLM Agents](https://arxiv.org/abs/2601.15311)
*Mustafa Arslan*

Main category: cs.AI

TL;DR: Aeon是一个神经符号认知操作系统，通过结构化记忆宫殿和神经符号情节图解决LLM在长上下文中的计算成本和"迷失在中间"问题，实现亚毫秒级检索延迟。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型受到自注意力二次计算成本和"迷失在中间"现象的限制，现有基于向量数据库的"扁平RAG"架构无法捕捉长时程交互的层次和时间结构，导致"向量迷雾"问题。

Method: 提出Aeon神经符号认知操作系统，将记忆结构化为记忆宫殿（通过Atlas实现的SIMD加速页面聚类向量索引）和痕迹（神经符号情节图），引入语义旁路缓冲器作为预测性缓存机制。

Result: Aeon在对话工作负载上实现<1ms检索延迟，通过零拷贝C++/Python桥确保状态一致性，为自主智能体提供持久结构化记忆。

Conclusion: Aeon通过将记忆重新定义为操作系统资源而非静态存储，有效解决了LLM的长上下文限制，实现了高效、结构化的记忆管理。

Abstract: Large Language Models (LLMs) are fundamentally constrained by the quadratic computational cost of self-attention and the "Lost in the Middle" phenomenon, where reasoning capabilities degrade as context windows expand. Existing solutions, primarily "Flat RAG" architectures relying on vector databases, treat memory as an unstructured bag of embeddings. This approach fails to capture the hierarchical and temporal structure of long-horizon interactions, leading to "Vector Haze", the retrieval of disjointed facts lacking episodic continuity. We propose Aeon, a Neuro-Symbolic Cognitive Operating System that redefines memory not as a static store, but as a managed OS resource. Aeon structures memory into a Memory Palace (a spatial index implemented via Atlas, a SIMD-accelerated Page-Clustered Vector Index that combines small-world graph navigation with B+ Tree-style disk locality to minimize read amplification) and a Trace (a neuro-symbolic episodic graph). We introduce the Semantic Lookaside Buffer (SLB), a predictive caching mechanism that exploits conversational locality to achieve sub-millisecond retrieval latencies. Benchmarks demonstrate that Aeon achieves < 1ms retrieval latency on conversational workloads while ensuring state consistency via a zero-copy C++/Python bridge, effectively enabling persistent, structured memory for autonomous agents.

</details>


### [90] [The Paradigm Shift: A Comprehensive Survey on Large Vision Language Models for Multimodal Fake News Detection](https://arxiv.org/abs/2601.15316)
*Wei Ai,Yilong Tan,Yuntao Shou,Tao Meng,Haowen Chen,Zhixiong He,Keqin Li*

Main category: cs.AI

TL;DR: 该论文是关于大型视觉语言模型在多模态假新闻检测领域应用的系统性综述，追踪了从传统特征工程方法到端到端多模态推理框架的范式转变。


<details>
  <summary>Details</summary>
Motivation: 近年来大型视觉语言模型的快速发展推动了多模态假新闻检测的范式转变，但该领域缺乏系统性的综述来追踪这一转变并整合最新进展。本文旨在填补这一空白，全面回顾LVLMs在多模态假新闻检测中的应用。

Method: 1. 提供历史视角，追踪从传统多模态检测流程到基础模型驱动范式的演变；2. 建立结构化分类体系，涵盖模型架构、数据集和性能基准；3. 分析剩余技术挑战；4. 提出未来研究方向。

Result: 这是首个系统记录和分析LVLMs在多模态假新闻检测中变革作用的全面综述，建立了该领域的结构化分类体系，识别了关键技术挑战，并为未来研究提供了方向指引。

Conclusion: 大型视觉语言模型正在彻底改变多模态假新闻检测领域，从传统特征工程方法转向统一的端到端多模态推理框架。尽管取得显著进展，但仍面临可解释性、时序推理和领域泛化等挑战，需要进一步研究来推动该领域发展。

Abstract: In recent years, the rapid evolution of large vision-language models (LVLMs) has driven a paradigm shift in multimodal fake news detection (MFND), transforming it from traditional feature-engineering approaches to unified, end-to-end multimodal reasoning frameworks. Early methods primarily relied on shallow fusion techniques to capture correlations between text and images, but they struggled with high-level semantic understanding and complex cross-modal interactions. The emergence of LVLMs has fundamentally changed this landscape by enabling joint modeling of vision and language with powerful representation learning, thereby enhancing the ability to detect misinformation that leverages both textual narratives and visual content. Despite these advances, the field lacks a systematic survey that traces this transition and consolidates recent developments. To address this gap, this paper provides a comprehensive review of MFND through the lens of LVLMs. We first present a historical perspective, mapping the evolution from conventional multimodal detection pipelines to foundation model-driven paradigms. Next, we establish a structured taxonomy covering model architectures, datasets, and performance benchmarks. Furthermore, we analyze the remaining technical challenges, including interpretability, temporal reasoning, and domain generalization. Finally, we outline future research directions to guide the next stage of this paradigm shift. To the best of our knowledge, this is the first comprehensive survey to systematically document and analyze the transformative role of LVLMs in combating multimodal fake news. The summary of existing methods mentioned is in our Github: \href{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}.

</details>


### [91] [Beyond Prompting: Efficient and Robust Contextual Biasing for Speech LLMs via Logit-Space Integration (LOGIC)](https://arxiv.org/abs/2601.15397)
*Peidong Wang*

Main category: cs.AI

TL;DR: LOGIC框架通过解码层直接操作，实现高效上下文偏置，解决语音大语言模型识别新实体的问题，相比提示方法显著降低实体错误率。


<details>
  <summary>Details</summary>
Motivation: 语音大语言模型在识别新实体（如联系人姓名、播放列表、技术术语）方面存在局限，现有提示方法存在可扩展性差、上下文窗口限制、推理延迟增加和"中间丢失"现象等问题，而生成式错误校正方法则容易产生"过度校正"和幻觉实体。

Method: 提出LOGIC（Logit-Space Integration for Contextual Biasing）框架，直接在解码层进行操作，将上下文注入与输入处理解耦，确保相对于提示长度的恒定时间复杂度。

Result: 在11种多语言环境中使用Phi-4-MM模型进行实验，LOGIC实现了平均9%的相对实体错误率降低，而误报率仅增加0.30%。

Conclusion: LOGIC框架为语音大语言模型提供了一种高效、稳健的上下文偏置解决方案，解决了现有方法在可扩展性和准确性方面的限制。

Abstract: The rapid emergence of new entities -- driven by cultural shifts, evolving trends, and personalized user data -- poses a significant challenge for existing Speech Large Language Models (Speech LLMs). While these models excel at general conversational tasks, their static training knowledge limits their ability to recognize domain-specific terms such as contact names, playlists, or technical jargon. Existing solutions primarily rely on prompting, which suffers from poor scalability: as the entity list grows, prompting encounters context window limitations, increased inference latency, and the "lost-in-the-middle" phenomenon. An alternative approach, Generative Error Correction (GEC), attempts to rewrite transcripts via post-processing but frequently suffers from "over-correction", introducing hallucinations of entities that were never spoken.
  In this work, we introduce LOGIC (Logit-Space Integration for Contextual Biasing), an efficient and robust framework that operates directly in the decoding layer. Unlike prompting, LOGIC decouples context injection from input processing, ensuring constant-time complexity relative to prompt length. Extensive experiments using the Phi-4-MM model across 11 multilingual locales demonstrate that LOGIC achieves an average 9% relative reduction in Entity WER with a negligible 0.30% increase in False Alarm Rate.

</details>


### [92] [Not Your Typical Sycophant: The Elusive Nature of Sycophancy in Large Language Models](https://arxiv.org/abs/2601.15436)
*Shahar Ben Natan,Oren Tsur*

Main category: cs.AI

TL;DR: 提出了一种通过零和博弈框架直接评估LLM谄媚行为的新方法，发现所有模型都有谄媚倾向，但Claude和Mistral在伤害第三方时会表现出"道德悔恨"；同时发现谄媚性和近因偏差存在相互增强的"建设性干扰"效应。


<details>
  <summary>Details</summary>
Motivation: 现有评估LLM谄媚行为的方法存在各种形式的不可控偏见、噪声或操纵性语言，需要一种更直接和中立的评估方式来准确衡量模型的谄媚倾向。

Method: 采用LLM-as-a-judge方法，将谄媚性评估构建为零和博弈的投注场景。在这种框架下，谄媚行为服务于一个个体（用户）而明确对另一个个体造成成本。比较了Gemini 2.5 Pro、ChatGPT 4o、Mistral-Large-Instruct-2411和Claude Sonnet 3.7四个领先模型。

Result: 所有模型在常见设置中都表现出谄媚倾向，其中Claude和Mistral在谄媚行为明确伤害第三方时会表现出"道德悔恨"并过度补偿。所有模型都对最后提出的答案存在偏见（近因偏差）。谄媚性和近因偏差相互作用产生"建设性干扰"效应，当用户意见最后呈现时，同意用户的倾向会加剧。

Conclusion: 提出了一种新颖的谄媚性评估框架，揭示了LLM在谄媚行为、道德考量和认知偏差方面的复杂交互作用，为理解模型行为提供了更深入的洞察。

Abstract: We propose a novel way to evaluate sycophancy of LLMs in a direct and neutral way, mitigating various forms of uncontrolled bias, noise, or manipulative language, deliberately injected to prompts in prior works. A key novelty in our approach is the use of LLM-as-a-judge, evaluation of sycophancy as a zero-sum game in a bet setting. Under this framework, sycophancy serves one individual (the user) while explicitly incurring cost on another. Comparing four leading models - Gemini 2.5 Pro, ChatGpt 4o, Mistral-Large-Instruct-2411, and Claude Sonnet 3.7 - we find that while all models exhibit sycophantic tendencies in the common setting, in which sycophancy is self-serving to the user and incurs no cost on others, Claude and Mistral exhibit "moral remorse" and over-compensate for their sycophancy in case it explicitly harms a third party. Additionally, we observed that all models are biased toward the answer proposed last. Crucially, we find that these two phenomena are not independent; sycophancy and recency bias interact to produce `constructive interference' effect, where the tendency to agree with the user is exacerbated when the user's opinion is presented last.

</details>


### [93] [Reliability by design: quantifying and eliminating fabrication risk in LLMs. From generative to consultative AI: a comparative analysis in the legal domain and lessons for high-stakes knowledge bases](https://arxiv.org/abs/2601.15476)
*Alex Dantart*

Main category: cs.AI

TL;DR: 该研究评估了三种AI范式在法律工作中的可靠性，发现独立生成模型不适合专业使用，基础RAG显著减少错误但仍有问题，而经过优化的高级RAG能将幻觉率降至可忽略水平。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决大型语言模型在高风险法律工作中产生幻觉的问题，确保AI在法律领域的可靠性和可信度，为专业法律工作提供安全有效的AI工具。

Method: 研究方法包括：1）区分三种AI范式（独立生成模型、基础RAG系统、高级RAG系统）；2）引入两个可靠性指标（虚假引用率和虚构事实率）；3）对12个LLM在75个法律任务上生成的2,700个司法风格答案进行专家双盲评审。

Result: 结果显示：1）独立生成模型不适合专业使用，虚假引用率超过30%；2）基础RAG系统显著减少错误但仍存在明显错误；3）采用嵌入微调、重排序和自校正技术的高级RAG系统能将虚构事实率降至0.2%以下。

Conclusion: 研究结论是：可信赖的法律AI需要基于检索的架构，强调验证和可追溯性，并提供了适用于其他高风险领域的评估框架。高级RAG系统通过端到端优化能够达到专业法律工作所需的可靠性标准。

Abstract: This paper examines how to make large language models reliable for high-stakes legal work by reducing hallucinations. It distinguishes three AI paradigms: (1) standalone generative models ("creative oracle"), (2) basic retrieval-augmented systems ("expert archivist"), and (3) an advanced, end-to-end optimized RAG system ("rigorous archivist"). The authors introduce two reliability metrics -False Citation Rate (FCR) and Fabricated Fact Rate (FFR)- and evaluate 2,700 judicial-style answers from 12 LLMs across 75 legal tasks using expert, double-blind review. Results show that standalone models are unsuitable for professional use (FCR above 30%), while basic RAG greatly reduces errors but still leaves notable misgrounding. Advanced RAG, using techniques such as embedding fine-tuning, re-ranking, and self-correction, reduces fabrication to negligible levels (below 0.2%). The study concludes that trustworthy legal AI requires rigor-focused, retrieval-based architectures emphasizing verification and traceability, and provides an evaluation framework applicable to other high-risk domains.

</details>


### [94] [MiRAGE: A Multiagent Framework for Generating Multimodal Multihop Question-Answer Dataset for RAG Evaluation](https://arxiv.org/abs/2601.15487)
*Chandan Kumar Sahu,Premith Kumar Chilukuri,Matthew Hetrich*

Main category: cs.AI

TL;DR: MiRAGE是一个多智能体框架，用于生成领域特定、多模态、多跳的问答数据集，以评估RAG系统在专业文档中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统向多模态、高风险企业应用快速发展，但缺乏针对专业领域文档的评估基准。现有数据集通常基于通用领域语料库或纯文本检索，无法捕捉专业文档中信息多模态且需要综合分散证据进行推理的复杂性。

Method: MiRAGE采用多智能体协作框架：递归上下文优化循环聚合分散证据，对抗性验证智能体保证事实基础，以及识别专家角色和领域的智能体来模拟专家认知工作流程。该框架通过专门智能体群协作生成经过验证的领域特定、多模态、多跳问答数据集。

Result: 在四个不同领域（法规、金融、定量生物学和新闻）的实证评估显示，MiRAGE生成的数据集具有显著更高的推理复杂性（>2.3平均跳数）和事实忠实度。消融研究表明，如果有图像的文本描述，MiRAGE可以由LLM驱动，但视觉基础仍是前沿挑战。

Conclusion: MiRAGE通过自动化创建反映专有语料库潜在主题结构的黄金标准评估数据集，为严格基准测试下一代信息检索系统提供了必要的基础设施。

Abstract: The rapid evolution of Retrieval-Augmented Generation (RAG) toward multimodal, high-stakes enterprise applications has outpaced the development of domain specific evaluation benchmarks. Existing datasets often rely on general-domain corpora or purely textual retrieval, failing to capture the complexity of specialized technical documents where information is inextricably multimodal and reasoning requires synthesizing disjoint evidence. We address this gap by introducing MiRAGE, a Multiagent framework for RAG systems Evaluation, that leverages a collaborative swarm of specialized agents to generate verified, domain-specific, multimodal, and multi-hop Question-Answer datasets. MiRAGE orchestrates a swarm of specialized agents: a recursive context optimization loop to aggregate scattered evidence, an adversarial verifier agent to guarantee factual grounding, and an agent to recognize the expert persona and the relevant domain to mimic expert cognitive workflows. Extensive empirical evaluation across four distinct domains (regulations, finance, quantitative biology, and journalism) demonstrates that MiRAGE generates datasets with significantly higher reasoning complexity (>2.3 average hops) and factual faithfulness. Our ablation studies point that MiRAGE can be powered by LLMs if textual descriptions of the images are available. Visual grounding still remains a frontier. By automating the creation of gold standard evaluation datasets that reflect the latent thematic structure of proprietary corpora, MiRAGE provides the necessary infrastructure to rigorously benchmark the next generation information retrieval systems.

</details>


### [95] [Tracking the Limits of Knowledge Propagation: How LLMs Fail at Multi-Step Reasoning with Conflicting Knowledge](https://arxiv.org/abs/2601.15495)
*Yiyang Feng,Zeming Chen,Haotian Wu,Jiawei Zhou,Antoine Bosselut*

Main category: cs.AI

TL;DR: TRACK是一个新的基准测试，用于评估LLMs在处理与参数知识冲突的新知识时的多步推理能力，发现提供更新事实反而会降低推理性能。


<details>
  <summary>Details</summary>
Motivation: 当前缓解LLMs中过时或错误信息的解决方案（如上下文提供更新事实或知识编辑）会在知识更新失败覆盖模型参数知识时引入知识冲突，这些冲突会传播到错误推理中。现有基准主要关注单一知识更新和事实回忆，缺乏评估这些更新如何影响下游推理。

Method: 引入TRACK基准测试，涵盖三个推理密集型场景（WIKI、CODE和MATH），引入多个现实冲突以反映真实世界复杂性，研究LLMs在参数知识与新知识冲突时如何传播新知识进行多步推理。

Result: 结果显示，为模型提供更新事实进行推理的性能比不提供更新事实更差，而且随着提供更多更新事实，性能下降加剧。这种失败源于无法忠实整合更新事实，以及即使知识被整合也存在有缺陷的推理。

Conclusion: TRACK提供了一个严谨的新基准，用于衡量和指导未来在多步推理中传播冲突知识方面的进展。

Abstract: A common solution for mitigating outdated or incorrect information in Large Language Models (LLMs) is to provide updated facts in-context or through knowledge editing. However, these methods introduce knowledge conflicts when the knowledge update fails to overwrite the model's parametric knowledge, which propagate to faulty reasoning. Current benchmarks for this problem, however, largely focus only on single knowledge updates and fact recall without evaluating how these updates affect downstream reasoning. In this work, we introduce TRACK (Testing Reasoning Amid Conflicting Knowledge), a new benchmark for studying how LLMs propagate new knowledge through multi-step reasoning when it conflicts with the model's initial parametric knowledge. Spanning three reasoning-intensive scenarios (WIKI, CODE, and MATH), TRACK introduces multiple, realistic conflicts to mirror real-world complexity. Our results on TRACK reveal that providing updated facts to models for reasoning can worsen performance compared to providing no updated facts to a model, and that this performance degradation exacerbates as more updated facts are provided. We show this failure stems from both inability to faithfully integrate updated facts, but also flawed reasoning even when knowledge is integrated. TRACK provides a rigorous new benchmark to measure and guide future progress on propagating conflicting knowledge in multi-step reasoning.

</details>


### [96] [The Dark Side of AI Transformers: Sentiment Polarization & the Loss of Business Neutrality by NLP Transformers](https://arxiv.org/abs/2601.15509)
*Prasanna Kumar*

Main category: cs.AI

TL;DR: 论文指出Transformer模型在情感分析中的准确率提升是以牺牲中立性和导致情感极性化为代价的，这对依赖情感分析结果的工业应用构成严重问题。


<details>
  <summary>Details</summary>
Motivation: 虽然迁移学习和Transformer模型在解决复杂计算问题和提高准确率方面取得了显著进展，但在应用AI分析特别是情感分析领域，这种准确率提升带来了负面效应。研究发现，Transformer模型对某一类情感准确率的提升往往以另一类情感的极端化和中立性失效为代价，这对依赖情感分析计算输出的工业级应用构成了严重问题。

Method: 通过实验观察发现，Transformer模型在情感分析任务中存在中立性缺失和情感极化问题。研究关注了模型在提升某一类情感识别准确率的同时，如何导致其他情感类别的极端化表现。

Result: 实验结果表明，Transformer模型在情感分析中的准确率提升伴随着明显的中立性缺失问题。模型倾向于将情感极端化，导致原本可能属于中立的情感被错误分类为积极或消极，这种偏差对工业应用中的可靠性造成了威胁。

Conclusion: Transformer模型在情感分析中的准确率提升存在局限性，其导致的中立性缺失和情感极化问题对应用NLP领域的工业级任务构成了实质性挑战。需要在追求准确率的同时，重视模型的中立性和平衡性表现，以确保情感分析结果在实际应用中的可靠性。

Abstract: The use of Transfer Learning & Transformers has steadily improved accuracy and has significantly contributed in solving complex computation problems. However, this transformer led accuracy improvement in Applied AI Analytics specifically in sentiment analytics comes with the dark side. It is observed during experiments that a lot of these improvements in transformer led accuracy of one class of sentiment has been at the cost of polarization of another class of sentiment and the failing of neutrality. This lack of neutrality poses an acute problem in the Applied NLP space, which relies heavily on the computational outputs of sentiment analytics for reliable industry ready tasks.

</details>


### [97] [TransportAgents: a multi-agents LLM framework for traffic accident severity prediction](https://arxiv.org/abs/2601.15519)
*Zhichao Yang,Jiashu He,Jinxuan Fan,Cirillo Cinzia*

Main category: cs.AI

TL;DR: 提出TransportAgents混合多智能体框架，通过专业化智能体处理不同交通信息子集，结合MLP集成模块，显著提升交通事故严重程度预测的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）的单智能体架构在处理异构、领域特定的交通事故数据时存在困难，容易产生有偏或不稳定的预测，而准确的交通事故严重程度预测对应急响应和公共安全规划至关重要。

Method: 提出TransportAgents混合多智能体框架，包含专门处理人口统计、环境背景、事故细节等不同交通信息子集的智能体，这些智能体生成中间严重程度评估，然后通过多层感知机（MLP）集成模块融合为统一预测。

Result: 在两个互补的美国数据集（CPSRMS和NEISS）上的实验表明，TransportAgents在GPT-3.5、GPT-4o和LLaMA-3.3等不同骨干模型上均优于传统机器学习和先进的LLM基线方法，展现出强大的鲁棒性、可扩展性和跨数据集泛化能力。

Conclusion: TransportAgents框架比标准单智能体LLM方法产生更平衡、校准更好的严重程度预测，在可解释性和可靠性方面具有优势，适用于安全关键决策支持应用。

Abstract: Accurate prediction of traffic crash severity is critical for improving emergency response and public safety planning. Although recent large language models (LLMs) exhibit strong reasoning capabilities, their single-agent architectures often struggle with heterogeneous, domain-specific crash data and tend to generate biased or unstable predictions. To address these limitations, this paper proposes TransportAgents, a hybrid multi-agent framework that integrates category-specific LLM reasoning with a multilayer perceptron (MLP) integration module. Each specialized agent focuses on a particular subset of traffic information, such as demographics, environmental context, or incident details, to produce intermediate severity assessments that are subsequently fused into a unified prediction. Extensive experiments on two complementary U.S. datasets, the Consumer Product Safety Risk Management System (CPSRMS) and the National Electronic Injury Surveillance System (NEISS), demonstrate that TransportAgents consistently outperforms both traditional machine learning and advanced LLM-based baselines. Across three representative backbones, including closed-source models such as GPT-3.5 and GPT-4o, as well as open-source models such as LLaMA-3.3, the framework exhibits strong robustness, scalability, and cross-dataset generalizability. A supplementary distributional analysis further shows that TransportAgents produces more balanced and well-calibrated severity predictions than standard single-agent LLM approaches, highlighting its interpretability and reliability for safety-critical decision support applications.

</details>


### [98] [ALIGNAgent: Adaptive Learner Intelligence for Gap Identification and Next-step guidance](https://arxiv.org/abs/2601.15551)
*Bismack Tokoli,Luis Jaimes,Ayesha S. Dina*

Main category: cs.AI

TL;DR: ALIGNAgent是一个多智能体教育框架，通过整合知识评估、技能差距识别和针对性资源推荐，提供个性化学习体验。


<details>
  <summary>Details</summary>
Motivation: 现有个性化学习系统通常只专注于知识追踪、诊断建模或资源推荐中的某一项，缺乏将这些组件整合成一个连贯自适应循环的系统。

Method: ALIGNAgent采用多智能体框架，包括技能差距智能体（处理学生测验表现、成绩数据和偏好，进行概念级诊断推理）和推荐智能体（检索与诊断缺陷对齐的偏好感知学习材料），实现持续反馈循环。

Result: 在两个本科计算机科学课程的真实数据集上进行评估，基于GPT-4o的智能体在知识熟练度估计方面达到0.87-0.90的精确度和0.84-0.87的F1分数，与实际考试表现验证一致。

Conclusion: ALIGNAgent通过整合知识评估、技能差距识别和资源推荐，有效实现了个性化学习，展示了多智能体框架在教育领域的应用潜力。

Abstract: Personalized learning systems have emerged as a promising approach to enhance student outcomes by tailoring educational content, pacing, and feedback to individual needs. However, most existing systems remain fragmented, specializing in either knowledge tracing, diagnostic modeling, or resource recommendation, but rarely integrating these components into a cohesive adaptive cycle. In this paper, we propose ALIGNAgent (Adaptive Learner Intelligence for Gap Identification and Next-step guidance), a multi-agent educational framework designed to deliver personalized learning through integrated knowledge estimation, skill-gap identification, and targeted resource recommendation.ALIGNAgent begins by processing student quiz performance, gradebook data, and learner preferences to generate topic-level proficiency estimates using a Skill Gap Agent that employs concept-level diagnostic reasoning to identify specific misconceptions and knowledge deficiencies. After identifying skill gaps, the Recommender Agent retrieves preference-aware learning materials aligned with diagnosed deficiencies, implementing a continuous feedback loop where interventions occur before advancing to subsequent topics. Extensive empirical evaluation on authentic datasets from two undergraduate computer science courses demonstrates ALIGNAgent's effectiveness, with GPT-4o-based agents achieving precision of 0.87-0.90 and F1 scores of 0.84-0.87 in knowledge proficiency estimation validated against actual exam performance.

</details>


### [99] [CogToM: A Comprehensive Theory of Mind Benchmark inspired by Human Cognition for Large Language Models](https://arxiv.org/abs/2601.15628)
*Haibo Tong,Zeyang Yue,Feifei Zhao,Erliang Lin,Lu Jia,Ruolin Chen,Yinqian Sun,Qian Zhang,Yi Zeng*

Main category: cs.AI

TL;DR: CogToM是一个全面的双语基准测试，包含8000多个实例和46种范式，用于评估LLMs是否具备类似人类的心理理论能力，揭示了模型性能异质性和与人类认知结构的潜在差异。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要局限于错误信念任务等狭窄范式，无法全面捕捉人类认知机制的全貌，因此需要更全面的评估工具来研究LLMs是否真正具备类似人类的心理理论能力。

Method: 开发了CogToM基准测试，包含超过8000个双语实例，涵盖46种范式，由49名人类标注者验证。系统评估了22个代表性模型，包括GPT-5.1和Qwen3-Max等前沿模型。

Result: 评估显示模型性能存在显著异质性，在特定维度上存在持续瓶颈。基于人类认知模式的分析表明LLMs与人类认知结构可能存在差异。

Conclusion: CogToM为研究LLMs不断演化的认知边界提供了稳健的工具和视角，有助于深入理解LLMs的心理理论能力及其与人类认知的差异。

Abstract: Whether Large Language Models (LLMs) truly possess human-like Theory of Mind (ToM) capabilities has garnered increasing attention. However, existing benchmarks remain largely restricted to narrow paradigms like false belief tasks, failing to capture the full spectrum of human cognitive mechanisms. We introduce CogToM, a comprehensive, theoretically grounded benchmark comprising over 8000 bilingual instances across 46 paradigms, validated by 49 human annotator.A systematic evaluation of 22 representative models, including frontier models like GPT-5.1 and Qwen3-Max, reveals significant performance heterogeneities and highlights persistent bottlenecks in specific dimensions. Further analysis based on human cognitive patterns suggests potential divergences between LLM and human cognitive structures. CogToM offers a robust instrument and perspective for investigating the evolving cognitive boundaries of LLMs.

</details>


### [100] [Predictive Coding and Information Bottleneck for Hallucination Detection in Large Language Models](https://arxiv.org/abs/2601.15652)
*Manish Bhatt*

Main category: cs.AI

TL;DR: 提出了一种基于神经科学信号设计的混合检测框架，用于检测大语言模型中的幻觉问题，相比现有方法具有更高的数据效率、更快的推理速度和更好的可解释性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型中的幻觉（看似合理但事实不准确的生成内容）是高风险部署的关键障碍。现有检测方法通常依赖计算昂贵的外部检索循环或不透明的黑盒LLM判断器，需要大量参数和计算资源。

Method: 引入混合检测框架，结合神经科学启发的信号设计和监督机器学习。提取基于预测编码（量化与内部先验的意外程度）和信息瓶颈（测量扰动下的信号保留）的可解释信号。通过系统消融研究，实现三个关键增强：实体聚焦摄取、上下文依从性和可证伪性评分。

Result: 在HaluBench数据集上，理论指导的基线达到0.8017 AUROC，基础监督模型达到0.8274 AUROC，改进特征提升到0.8669 AUROC（4.95%增益）。相比Lynx使用75倍更少的训练数据（200 vs 15,000样本），推理速度快1000倍（5ms vs 5s），且保持完全可解释性。

Conclusion: 通过信号架构编码领域知识相比扩展LLM判断器具有更优的数据效率，能够使用轻量级（少于100万参数）、可解释的模型实现强大性能，适合生产部署。同时发现合理化信号无法区分幻觉，表明LLM会为错误前提生成连贯推理。

Abstract: Hallucinations in Large Language Models (LLMs) -- generations that are plausible but factually unfaithful -- remain a critical barrier to high-stakes deployment. Current detection methods typically rely on computationally expensive external retrieval loops or opaque black-box LLM judges requiring 70B+ parameters. In this work, we introduce [Model Name], a hybrid detection framework that combines neuroscience-inspired signal design with supervised machine learning. We extract interpretable signals grounded in Predictive Coding (quantifying surprise against internal priors) and the Information Bottleneck (measuring signal retention under perturbation). Through systematic ablation, we demonstrate three key enhancements: Entity-Focused Uptake (concentrating on high-value tokens), Context Adherence (measuring grounding strength), and Falsifiability Score (detecting confident but contradictory claims).
  Evaluating on HaluBench (n=200, perfectly balanced), our theory-guided baseline achieves 0.8017 AUROC. BASE supervised models reach 0.8274 AUROC, while IMPROVED features boost performance to 0.8669 AUROC (4.95% gain), demonstrating consistent improvements across architectures. This competitive performance is achieved while using 75x less training data than Lynx (200 vs 15,000 samples), 1000x faster inference (5ms vs 5s), and remaining fully interpretable. Crucially, we report a negative result: the Rationalization signal fails to distinguish hallucinations, suggesting that LLMs generate coherent reasoning for false premises ("Sycophancy").
  This work demonstrates that domain knowledge encoded in signal architecture provides superior data efficiency compared to scaling LLM judges, achieving strong performance with lightweight (less than 1M parameter), explainable models suitable for production deployment.

</details>


### [101] [PhysProver: Advancing Automatic Theorem Proving for Physics](https://arxiv.org/abs/2601.15737)
*Hanning Zhang,Ruida Wang,Rui Pan,Wenyuan Wang,Bingxu Meng,Tong Zhang*

Main category: cs.AI

TL;DR: 本文提出了首个增强物理学领域形式化定理证明的方法，通过创建专用数据集PhysLeanData，并利用DeepSeek-Prover-V2-7B模型结合可验证奖励的强化学习训练PhysProver模型，在物理学多个子领域实现了2.4%的整体提升。


<details>
  <summary>Details</summary>
Motivation: 虽然可验证语言与LLMs的结合为定理证明提供了严格基础，但现有研究主要关注数学领域的形式化推理，对同样依赖问题解决和定理证明框架的物理学形式化推理关注不足。本文旨在填补这一空白，将形式化定理证明扩展到物理学领域。

Method: 1) 创建专用数据集PhysLeanData，包含从PhysLean采样的定理和基于猜想的形式化数据生成管道生成的数据；2) 利用开源数学定理证明器DeepSeek-Prover-V2-7B；3) 应用可验证奖励的强化学习(RLVR)训练PhysProver模型；4) 仅使用约5K训练样本进行训练。

Result: 1) PhysProver在物理学多个子领域实现了2.4%的整体提升；2) 经过物理学形式化训练后，在MiniF2F-Test基准测试上获得了1.3%的增益，表明模型具有超越物理学领域的非平凡泛化能力，同时增强了形式化数学能力。

Conclusion: 该方法为将形式化证明器扩展到数学领域之外提供了范例，证明了方法的有效性和效率。作者将向社区发布数据集和模型以促进进一步研究。

Abstract: The combination of verifiable languages and LLMs has significantly influenced both the mathematical and computer science communities because it provides a rigorous foundation for theorem proving. Recent advancements in the field provide foundation models and sophisticated agentic systems pushing the boundaries of formal mathematical reasoning to approach the natural language capability of LLMs. However, little attention has been given to the formal physics reasoning, which also heavily relies on similar problem-solving and theorem-proving frameworks. To solve this problem, this paper presents, to the best of our knowledge, the first approach to enhance formal theorem proving in the physics domain. We compose a dedicated dataset PhysLeanData for the task. It is composed of theorems sampled from PhysLean and data generated by a conjecture-based formal data generation pipeline. In the training pipeline, we leverage DeepSeek-Prover-V2-7B, a strong open-source mathematical theorem prover, and apply Reinforcement Learning with Verifiable Rewards (RLVR) to train our model PhysProver. Comprehensive experiments demonstrate that, using only $\sim$5K training samples, PhysProver achieves an overall 2.4\% improvement in multiple sub-domains. Furthermore, after formal physics training, we observe 1.3\% gains on the MiniF2F-Test benchmark, which indicates non-trivial generalization beyond physics domains and enhancement for formal math capability as well. The results highlight the effectiveness and efficiency of our approach, which provides a paradigm for extending formal provers outside mathematical domains. To foster further research, we will release both our dataset and model to the community.

</details>


### [102] [Tabular Incremental Inference](https://arxiv.org/abs/2601.15751)
*Xinda Chen,Xing Zhen,Hanyu Zhang,Weimin Tan,Bo Yan*

Main category: cs.AI

TL;DR: 本文提出了表格增量推理（TabII）新任务，解决AI模型在推理阶段处理动态变化表格列的问题，基于信息瓶颈理论设计方法，在8个公开数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 表格数据是基础数据结构，表格列的动态变化源于技术进步、需求变化、数据整合等因素。现有AI模型在固定列表格上训练后进行推理的方法无法处理动态变化的表格，需要新的无监督方法来高效处理这类表格。

Method: 1. 将表格增量推理任务形式化为基于信息瓶颈理论的优化问题；2. 设计包含LLM占位符和预训练TabAdapter的方法，提供外部知识；3. 使用增量样本压缩块来压缩增量列属性提供的任务相关信息。

Result: 在8个公开数据集上的实验结果表明，TabII方法能够有效利用增量属性，实现了最先进的性能。

Conclusion: 提出的表格增量推理（TabII）任务能够使训练好的模型在推理阶段纳入新列，增强了AI模型在表格动态变化场景中的实用性，基于信息瓶颈理论的方法设计是有效的。

Abstract: Tabular data is a fundamental form of data structure. The evolution of table analysis tools reflects humanity's continuous progress in data acquisition, management, and processing. The dynamic changes in table columns arise from technological advancements, changing needs, data integration, etc. However, the standard process of training AI models on tables with fixed columns and then performing inference is not suitable for handling dynamically changed tables. Therefore, new methods are needed for efficiently handling such tables in an unsupervised manner. In this paper, we introduce a new task, Tabular Incremental Inference (TabII), which aims to enable trained models to incorporate new columns during the inference stage, enhancing the practicality of AI models in scenarios where tables are dynamically changed. Furthermore, we demonstrate that this new task can be framed as an optimization problem based on the information bottleneck theory, which emphasizes that the key to an ideal tabular incremental inference approach lies in minimizing mutual information between tabular data and representation while maximizing between representation and task labels. Under this guidance, we design a TabII method with Large Language Model placeholders and Pretrained TabAdapter to provide external knowledge and Incremental Sample Condensation blocks to condense the task-relevant information given by incremental column attributes. Experimental results across eight public datasets show that TabII effectively utilizes incremental attributes, achieving state-of-the-art performance.

</details>


### [103] [Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning](https://arxiv.org/abs/2601.15761)
*Xiefeng Wu,Mingyu Hu,Shu Zhang*

Main category: cs.AI

TL;DR: SigEnt-SAC是一种从零开始学习的离线策略actor-critic方法，仅需单条专家轨迹，通过sigmoid有界熵项防止负熵驱动的优化，减少Q函数振荡，在现实世界机器人任务中实现低成本强化学习部署。


<details>
  <summary>Details</summary>
Motivation: 现实世界强化学习面临样本效率低、奖励稀疏和视觉观测噪声等挑战。现有方法需要大量数据集或大规模预训练，缺乏低成本、数据需求少的实用解决方案。

Method: 提出SigEnt-SAC方法，核心设计是sigmoid有界熵项，防止负熵驱动的优化导致分布外动作，减少Q函数振荡。该方法仅需单条专家轨迹从零开始学习。

Result: 在D4RL基准测试中，SigEnt-SAC显著缓解了Q函数振荡，比先前方法更快达到100%成功率。在四个现实世界机器人任务中，仅需少量真实交互就能学习成功策略。

Conclusion: SigEnt-SAC为现实世界强化学习部署提供了一条低成本、实用的途径，仅需最小数据需求就能在真实机器人任务中实现有效学习。

Abstract: Deploying reinforcement learning in the real world remains challenging due to sample inefficiency, sparse rewards, and noisy visual observations. Prior work leverages demonstrations and human feedback to improve learning efficiency and robustness. However, offline-to-online methods need large datasets and can be unstable, while VLA-assisted RL relies on large-scale pretraining and fine-tuning. As a result, a low-cost real-world RL method with minimal data requirements has yet to emerge. We introduce \textbf{SigEnt-SAC}, an off-policy actor-critic method that learns from scratch using a single expert trajectory. Our key design is a sigmoid-bounded entropy term that prevents negative-entropy-driven optimization toward out-of-distribution actions and reduces Q-function oscillations. We benchmark SigEnt-SAC on D4RL tasks against representative baselines. Experiments show that SigEnt-SAC substantially alleviates Q-function oscillations and reaches a 100\% success rate faster than prior methods. Finally, we validate SigEnt-SAC on four real-world robotic tasks across multiple embodiments, where agents learn from raw images and sparse rewards; results demonstrate that SigEnt-SAC can learn successful policies with only a small number of real-world interactions, suggesting a low-cost and practical pathway for real-world RL deployment.

</details>


### [104] [Agentic Confidence Calibration](https://arxiv.org/abs/2601.15778)
*Jiaxin Zhang,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.AI

TL;DR: 该论文首次提出"智能体置信度校准"问题，并开发了HTC框架，通过提取智能体整个轨迹的丰富过程特征来校准多步任务中的置信度，在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前AI智能体正从被动语言模型发展为执行复杂多步任务的自主系统，但其在失败情况下的过度自信成为高风险部署的根本障碍。现有的校准方法针对静态单轮输出设计，无法解决智能体系统的独特挑战，如轨迹中的误差累积、外部工具的不确定性以及不透明的失败模式。

Method: 提出了Holistic Trajectory Calibration (HTC)框架，这是一种新颖的诊断框架，从宏观动态到微观稳定性等多个层面提取智能体整个轨迹的丰富过程级特征。采用简单可解释的模型，并开发了General Agent Calibrator (GAC)实现跨领域泛化。

Result: HTC在8个基准测试、多个LLM和不同智能体框架中，在校准和判别方面均持续超越强基线方法。特别地，GAC在跨领域的GAIA基准测试中实现了最佳校准（最低ECE）。

Conclusion: 该研究为置信度校准建立了新的过程中心范式，提供了诊断和增强AI智能体可靠性的框架，实现了可解释性、可迁移性和泛化性三大关键进展。

Abstract: AI agents are rapidly advancing from passive language models to autonomous systems executing complex, multi-step tasks. Yet their overconfidence in failure remains a fundamental barrier to deployment in high-stakes settings. Existing calibration methods, built for static single-turn outputs, cannot address the unique challenges of agentic systems, such as compounding errors along trajectories, uncertainty from external tools, and opaque failure modes. To address these challenges, we introduce, for the first time, the problem of Agentic Confidence Calibration and propose Holistic Trajectory Calibration (HTC), a novel diagnostic framework that extracts rich process-level features ranging from macro dynamics to micro stability across an agent's entire trajectory. Powered by a simple, interpretable model, HTC consistently surpasses strong baselines in both calibration and discrimination, across eight benchmarks, multiple LLMs, and diverse agent frameworks. Beyond performance, HTC delivers three essential advances: it provides interpretability by revealing the signals behind failure, enables transferability by applying across domains without retraining, and achieves generalization through a General Agent Calibrator (GAC) that achieves the best calibration (lowest ECE) on the out-of-domain GAIA benchmark. Together, these contributions establish a new process-centric paradigm for confidence calibration, providing a framework for diagnosing and enhancing the reliability of AI agents.

</details>


### [105] [Creativity in the Age of AI: Rethinking the Role of Intentional Agency](https://arxiv.org/abs/2601.15797)
*James S. Pearson,Matthew J. Dennis,Marc Cheong*

Main category: cs.AI

TL;DR: 论文主张放弃"意向性主体条件"作为创造力的普遍必要条件，认为生成式AI的发展使这一条件在描述和功能上都存在问题，应改为"一致性要求"，即创造力应追踪可靠产生新颖有价值产品的能力。


<details>
  <summary>Details</summary>
Motivation: 传统创造力理论认为意向性主体是创造力的必要条件，但随着生成式AI的发展，这一条件在描述上（人们越来越多地将创造力归因于AI）和功能上都变得有问题，需要重新审视创造力的概念框架。

Method: 1. 语料库证据分析：展示作者和记者越来越愿意将创造力归因于缺乏意向性主体的生成式AI；2. 概念工程方法：论证意向性主体条件不再履行其核心社会功能，反而助长对AI生成产出的评估偏见。

Result: 意向性主体条件作为创造力的普遍条件应该被放弃，因为它既不符合语言使用实践，也不再发挥应有的社会功能。应改为"一致性要求"，即创造力应基于可靠产生新颖有价值产品的能力。

Conclusion: 虽然意向性主体条件应在特定局部领域保留，但作为创造力的普遍必要条件应该被抛弃，代之以更符合当代技术现实的概念框架，以更好地识别和鼓励新颖有价值产品的可靠来源。

Abstract: Many theorists of creativity maintain that intentional agency is a necessary condition of creativity. We argue that this requirement, which we call the Intentional Agency Condition (IAC), should be rejected as a general condition of creativity, while retaining its relevance in specific contexts. We show that recent advances in generative AI have rendered the IAC increasingly problematic, both descriptively and functionally. We offer two reasons for abandoning it at the general level. First, we present corpus evidence indicating that authors and journalists are increasingly comfortable ascribing creativity to generative AI, despite its lack of intentional agency. This development places pressure on the linguistic intuitions that have traditionally been taken to support the IAC. Second, drawing on the method of conceptual engineering, we argue that the IAC no longer fulfils its core social function. Rather than facilitating the identification and encouragement of reliable sources of novel and valuable products, it now feeds into biases that distort our assessments of AI-generated outputs. We therefore propose replacing the IAC with a consistency requirement, according to which creativity tracks the reliable generation of novel and valuable products. Nonetheless, we explain why the IAC should be retained in specific local domains.

</details>


### [106] [VitalDiagnosis: AI-Driven Ecosystem for 24/7 Vital Monitoring and Chronic Disease Management](https://arxiv.org/abs/2601.15798)
*Zhikai Xue,Tianqianjin Lin,Pengwei Yan,Ruichun Wang,Yuxin Liu,Zhuoren Jiang,Xiaozhong Liu*

Main category: cs.AI

TL;DR: VitalDiagnosis是一个基于大语言模型的生态系统，旨在将慢性病管理从被动监测转变为主动互动参与，通过整合可穿戴设备数据和LLM推理能力，解决急性健康异常和日常依从性问题。


<details>
  <summary>Details</summary>
Motivation: 慢性病已成为全球主要死因，医疗资源紧张和人口老龄化加剧了这一挑战。患者难以解读早期恶化迹象并坚持护理计划，需要更主动的管理方式。

Method: 通过整合可穿戴设备的连续数据与大语言模型的推理能力，系统采用情境感知询问分析触发因素，在医患协作工作流程中生成临时见解，并提供个性化指导。

Result: 系统能够同时处理急性健康异常和日常依从性问题，通过主动互动参与促进更积极的护理模式。

Conclusion: VitalDiagnosis有潜力增强患者自我管理能力，减少可避免的临床工作量，推动慢性病管理向更主动、协作的护理范式转变。

Abstract: Chronic diseases have become the leading cause of death worldwide, a challenge intensified by strained medical resources and an aging population. Individually, patients often struggle to interpret early signs of deterioration or maintain adherence to care plans. In this paper, we introduce VitalDiagnosis, an LLM-driven ecosystem designed to shift chronic disease management from passive monitoring to proactive, interactive engagement. By integrating continuous data from wearable devices with the reasoning capabilities of LLMs, the system addresses both acute health anomalies and routine adherence. It analyzes triggers through context-aware inquiries, produces provisional insights within a collaborative patient-clinician workflow, and offers personalized guidance. This approach aims to promote a more proactive and cooperative care paradigm, with the potential to enhance patient self-management and reduce avoidable clinical workload.

</details>


### [107] [Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification](https://arxiv.org/abs/2601.15808)
*Yuxuan Wan,Tianqing Fang,Zaitang Li,Yintong Huo,Wenxuan Wang,Haitao Mi,Dong Yu,Michael R. Lyu*

Main category: cs.AI

TL;DR: DeepVerifier：基于评估准则的推理时验证系统，通过自动构建的深度研究智能体失败分类法，指导智能体自我进化，无需额外训练即可提升性能


<details>
  <summary>Details</summary>
Motivation: 现有深度研究智能体主要通过后训练增强策略能力，但缺乏推理时的自我改进机制。研究者提出通过验证智能体输出并基于评估准则提供反馈，实现智能体的自我进化

Method: 1. 构建深度研究智能体失败分类法，将失败分为5大类13子类；2. 开发DeepVerifier基于准则的奖励验证器，利用验证不对称性；3. 推理时集成作为即插即用模块，生成详细反馈供智能体迭代优化

Result: 1. DeepVerifier在元评估F1分数上比基准方法提升12%-48%；2. 在GAIA和XBench-DeepResearch的挑战性子集上，推理时扩展带来8%-11%的准确率提升；3. 发布DeepVerifier-4K数据集，包含4,646个高质量智能体步骤，专注于验证能力

Conclusion: 提出了一种新的智能体自我进化范式，通过推理时验证和基于准则的反馈实现无需额外训练的性能提升。该方法为开源智能体发展提供了高质量数据集和验证框架

Abstract: Recent advances in Deep Research Agents (DRAs) are transforming automated knowledge discovery and problem-solving. While the majority of existing efforts focus on enhancing policy capabilities via post-training, we propose an alternative paradigm: self-evolving the agent's ability by iteratively verifying the policy model's outputs, guided by meticulously crafted rubrics. This approach gives rise to the inference-time scaling of verification, wherein an agent self-improves by evaluating its generated answers to produce iterative feedback and refinements. We derive the rubrics based on an automatically constructed DRA Failure Taxonomy, which systematically classifies agent failures into five major categories and thirteen sub-categories. We present DeepVerifier, a rubrics-based outcome reward verifier that leverages the asymmetry of verification and outperforms vanilla agent-as-judge and LLM judge baselines by 12%-48% in meta-evaluation F1 score. To enable practical self-evolution, DeepVerifier integrates as a plug-and-play module during test-time inference. The verifier produces detailed rubric-based feedback, which is fed back to the agent for iterative bootstrapping, refining responses without additional training. This test-time scaling delivers 8%-11% accuracy gains on challenging subsets of GAIA and XBench-DeepResearch when powered by capable closed-source LLMs. Finally, to support open-source advancement, we release DeepVerifier-4K, a curated supervised fine-tuning dataset of 4,646 high-quality agent steps focused on DRA verification. These examples emphasize reflection and self-critique, enabling open models to develop robust verification capabilities.

</details>


### [108] [ErrorMap and ErrorAtlas: Charting the Failure Landscape of Large Language Models](https://arxiv.org/abs/2601.15812)
*Shir Ashury-Tahan,Yifan Mai,Elron Bandel,Michal Shmueli-Scheuer,Leshem Choshen*

Main category: cs.AI

TL;DR: ErrorMap是首个系统分析LLM失败原因的方法，通过提取模型的"失败签名"来揭示错误根源，而非仅仅指出失败。该方法应用于35个数据集和83个模型，生成了ErrorAtlas错误分类法，发现了当前研究中被忽视的错误类型。


<details>
  <summary>Details</summary>
Motivation: 当前LLM基准测试只能告诉我们模型何时失败，但不能解释为什么失败。错误的答案可能源于格式问题、计算错误或数据集噪声，而非推理能力弱。如果不区分这些原因，基准测试就不完整，无法可靠地指导模型改进。

Method: 引入ErrorMap方法，提取模型的"失败签名"，澄清基准测试的测量内容，并扩大错误识别范围以减少盲点。该方法适用于任何模型和数据集，使用相同的逻辑进行分析。

Result: 将方法应用于35个数据集和83个模型，生成了ErrorAtlas错误分类法，揭示了重复出现的失败模式。ErrorAtlas突出了当前LLM研究中被忽视的错误类型，如输出中遗漏必要细节和问题误解。

Conclusion: 通过将焦点从模型成功的地方转移到为什么失败，ErrorMap和ErrorAtlas实现了高级评估，揭示了隐藏的弱点并指导进展。与通常通过任务级指标衡量的成功不同，该方法引入了一个可在模型和任务间全局应用的更深层评估层，提供了对模型行为和局限性的更丰富见解。

Abstract: Large Language Models (LLM) benchmarks tell us when models fail, but not why they fail. A wrong answer on a reasoning dataset may stem from formatting issues, calculation errors, or dataset noise rather than weak reasoning. Without disentangling such causes, benchmarks remain incomplete and cannot reliably guide model improvement. We introduce ErrorMap, the first method to chart the sources of LLM failure. It extracts a model's unique "failure signature", clarifies what benchmarks measure, and broadens error identification to reduce blind spots. This helps developers debug models, aligns benchmark goals with outcomes, and supports informed model selection. ErrorMap works on any model or dataset with the same logic. Applying our method to 35 datasets and 83 models we generate ErrorAtlas, a taxonomy of model errors, revealing recurring failure patterns. ErrorAtlas highlights error types that are currently underexplored in LLM research, such as omissions of required details in the output and question misinterpretation. By shifting focus from where models succeed to why they fail, ErrorMap and ErrorAtlas enable advanced evaluation - one that exposes hidden weaknesses and directs progress. Unlike success, typically measured by task-level metrics, our approach introduces a deeper evaluation layer that can be applied globally across models and tasks, offering richer insights into model behavior and limitations. We make the taxonomy and code publicly available with plans to periodically update ErrorAtlas as new benchmarks and models emerge.

</details>


### [109] [ICON: Invariant Counterfactual Optimization with Neuro-Symbolic Priors for Text-Based Person Search](https://arxiv.org/abs/2601.15931)
*Xiangyu Wang,Zhixin Lv,Yongjiao Sun,Anrui Han,Ye Yuan,Hangxu Ji*

Main category: cs.AI

TL;DR: ICON框架通过因果和拓扑先验解决文本行人搜索中的虚假相关性和空间语义错位问题，实现几何不变性和环境独立性，在标准基准上保持领先性能并展现对遮挡、背景干扰和定位噪声的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前基于预训练模型的文本行人搜索方法在复杂开放世界场景中迁移效果不佳，依赖"被动观察"导致多方面的虚假相关性和空间语义错位，缺乏对分布变化的鲁棒性。

Method: 提出ICON框架，整合因果和拓扑先验：1) 规则引导的空间干预惩罚对边界框噪声的敏感性；2) 反事实上下文解耦通过语义驱动的背景移植实现环境独立性；3) 显著性驱动的语义正则化解决局部显著性偏差；4) 神经符号拓扑对齐确保激活区域与人类结构逻辑的拓扑一致性。

Result: ICON不仅在标准基准上保持领先性能，而且对遮挡、背景干扰和定位噪声展现出卓越的鲁棒性。

Conclusion: 该方法通过从拟合统计共现转向学习因果不变性，有效推动了文本行人搜索领域的发展。

Abstract: Text-Based Person Search (TBPS) holds unique value in real-world surveillance bridging visual perception and language understanding, yet current paradigms utilizing pre-training models often fail to transfer effectively to complex open-world scenarios. The reliance on "Passive Observation" leads to multifaceted spurious correlations and spatial semantic misalignment, causing a lack of robustness against distribution shifts. To fundamentally resolve these defects, this paper proposes ICON (Invariant Counterfactual Optimization with Neuro-symbolic priors), a framework integrating causal and topological priors. First, we introduce Rule-Guided Spatial Intervention to strictly penalize sensitivity to bounding box noise, forcibly severing location shortcuts to achieve geometric invariance. Second, Counterfactual Context Disentanglement is implemented via semantic-driven background transplantation, compelling the model to ignore background interference for environmental independence. Then, we employ Saliency-Driven Semantic Regularization with adaptive masking to resolve local saliency bias and guarantee holistic completeness. Finally, Neuro-Symbolic Topological Alignment utilizes neuro-symbolic priors to constrain feature matching, ensuring activated regions are topologically consistent with human structural logic. Experimental results demonstrate that ICON not only maintains leading performance on standard benchmarks but also exhibits exceptional robustness against occlusion, background interference, and localization noise. This approach effectively advances the field by shifting from fitting statistical co-occurrences to learning causal invariance.

</details>


### [110] [Natural Language-Driven Global Mapping of Martian Landforms](https://arxiv.org/abs/2601.15949)
*Yiran Wang,Shuoyuan Wang,Zhaoran Wei,Jiannan Zhao,Zhonghua Yao,Zejian Xie,Songxin Zhang,Jun Huang,Bingyi Jing,Hongxin Wei*

Main category: cs.AI

TL;DR: MarScope是一个行星尺度的视觉-语言框架，通过自然语言驱动、无需标签的方式实现火星地貌映射，将行星图像与文本对齐到共享语义空间，支持任意用户查询并在5秒内完成全球检索。


<details>
  <summary>Details</summary>
Motivation: 行星表面通常使用自然语言中的高级语义概念进行分析，但大量轨道图像档案仍以像素级别组织。这种不匹配限制了行星表面的可扩展、开放式探索。

Method: 开发MarScope框架，将行星图像和文本对齐到共享语义空间，使用超过20万个精心策划的图像-文本对进行训练，实现自然语言驱动的无标签映射。

Result: 该框架能够在5秒内完成整个火星的任意用户查询，F1分数高达0.978。除了形态分类外，还能促进过程导向分析和基于相似性的行星尺度地貌映射。

Conclusion: MarScope建立了一个新范式，使自然语言成为大规模地理空间数据集科学发现的直接接口，改变了全球地貌映射的方式。

Abstract: Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets.

</details>


### [111] [Decoupling Return-to-Go for Efficient Decision Transformer](https://arxiv.org/abs/2601.15953)
*Yongyi Wang,Hanyu Liu,Lingfeng Li,Bozhou Chen,Ang Li,Qirui Zheng,Xionghui Yang,Wenxin Li*

Main category: cs.AI

TL;DR: 本文提出解耦决策变换器（DDT），通过简化RTG输入结构来解决决策变换器中RTG序列冗余问题，提高性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 研究发现决策变换器（DT）设计中存在关键冗余：将整个回报到目标（RTG）序列输入变换器在理论上是不必要的，因为只有最近的RTG影响动作预测。这种冗余会损害DT的性能。

Method: 提出解耦决策变换器（DDT），简化架构：仅通过变换器处理观察和动作序列，使用最新的RTG来指导动作预测，从而消除RTG序列的冗余输入。

Result: 实验表明DDT显著优于原始DT，并在多个离线强化学习任务中与最先进的DT变体相比具有竞争力。同时，简化架构还降低了计算成本。

Conclusion: 通过消除RTG序列的冗余输入，解耦决策变换器（DDT）不仅提高了性能，还降低了计算复杂度，为离线强化学习的序列建模方法提供了更高效的架构。

Abstract: The Decision Transformer (DT) has established a powerful sequence modeling approach to offline reinforcement learning. It conditions its action predictions on Return-to-Go (RTG), using it both to distinguish trajectory quality during training and to guide action generation at inference. In this work, we identify a critical redundancy in this design: feeding the entire sequence of RTGs into the Transformer is theoretically unnecessary, as only the most recent RTG affects action prediction. We show that this redundancy can impair DT's performance through experiments. To resolve this, we propose the Decoupled DT (DDT). DDT simplifies the architecture by processing only observation and action sequences through the Transformer, using the latest RTG to guide the action prediction. This streamlined approach not only improves performance but also reduces computational cost. Our experiments show that DDT significantly outperforms DT and establishes competitive performance against state-of-the-art DT variants across multiple offline RL tasks.

</details>


### [112] [Deja Vu in Plots: Leveraging Cross-Session Evidence with Retrieval-Augmented LLMs for Live Streaming Risk Assessment](https://arxiv.org/abs/2601.16027)
*Yiran Qiao,Xiang Ao,Jing Chen,Yang Liu,Qiwei Zhong,Qing He*

Main category: cs.AI

TL;DR: CS-VAR是一个用于直播风险检测的跨会话证据感知检索增强检测器，通过轻量级领域特定模型进行快速会话级风险推断，在训练时由大型语言模型指导，使其能够识别跨流量的重复模式并保持实时部署效率。


<details>
  <summary>Details</summary>
Motivation: 直播的兴起带来了大规模实时互动，但也使平台面临复杂的风险，如诈骗和协同恶意行为。这些风险检测具有挑战性，因为有害行为往往逐渐积累并在看似无关的流中重复出现。

Method: 提出CS-VAR（跨会话证据感知检索增强检测器），采用轻量级领域特定模型进行快速会话级风险推断，在训练时由大型语言模型指导，该LLM基于检索到的跨会话行为证据进行推理，并将其局部到全局的洞察转移给小模型。

Result: 在大规模工业数据集上的离线实验结合在线验证表明，CS-VAR实现了最先进的性能。此外，CS-VAR提供了可解释的局部化信号，有效赋能现实世界的直播内容审核。

Conclusion: CS-VAR通过结合轻量级模型的高效性和大型语言模型的推理能力，能够有效检测直播中的复杂风险模式，同时保持实时部署的可行性，为直播平台风险检测提供了创新解决方案。

Abstract: The rise of live streaming has transformed online interaction, enabling massive real-time engagement but also exposing platforms to complex risks such as scams and coordinated malicious behaviors. Detecting these risks is challenging because harmful actions often accumulate gradually and recur across seemingly unrelated streams. To address this, we propose CS-VAR (Cross-Session Evidence-Aware Retrieval-Augmented Detector) for live streaming risk assessment. In CS-VAR, a lightweight, domain-specific model performs fast session-level risk inference, guided during training by a Large Language Model (LLM) that reasons over retrieved cross-session behavioral evidence and transfers its local-to-global insights to the small model. This design enables the small model to recognize recurring patterns across streams, perform structured risk assessment, and maintain efficiency for real-time deployment. Extensive offline experiments on large-scale industrial datasets, combined with online validation, demonstrate the state-of-the-art performance of CS-VAR. Furthermore, CS-VAR provides interpretable, localized signals that effectively empower real-world moderation for live streaming.

</details>


### [113] [Grounding Large Language Models in Reaction Knowledge Graphs for Synthesis Retrieval](https://arxiv.org/abs/2601.16038)
*Olga Bunkova,Lorenzo Di Fruscia,Sophia Rupprecht,Artur M. Schweidtmann,Marcel J. T. Reinders,Jana M. Weber*

Main category: cs.AI

TL;DR: 该研究将化学反应路径检索转化为Text2Cypher生成问题，比较了不同提示策略在知识图谱查询中的表现，发现使用对齐示例的一样本提示效果最佳。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在化学合成规划中有应用潜力，但标准提示方法常产生幻觉或过时的建议，需要更可靠的方法来利用反应知识图谱。

Method: 将反应路径检索转化为Text2Cypher（自然语言到图查询）生成问题，定义单步和多步检索任务，比较零样本提示与使用静态、随机和嵌入对齐示例的一样本变体，并评估检查表驱动的验证/校正循环。

Result: 使用对齐示例的一样本提示始终表现最佳；检查表式自校正循环主要在零样本设置中提高可执行性，一旦有良好示例存在，检索增益有限。

Conclusion: 研究提供了一个可复现的Text2Cypher评估框架，促进基于知识图谱的LLM在合成规划中的进一步研究，强调了对齐示例在提高查询有效性和检索准确性中的重要性。

Abstract: Large Language Models (LLMs) can aid synthesis planning in chemistry, but standard prompting methods often yield hallucinated or outdated suggestions. We study LLM interactions with a reaction knowledge graph by casting reaction path retrieval as a Text2Cypher (natural language to graph query) generation problem, and define single- and multi-step retrieval tasks. We compare zero-shot prompting to one-shot variants using static, random, and embedding-based exemplar selection, and assess a checklist-driven validator/corrector loop. To evaluate our framework, we consider query validity and retrieval accuracy. We find that one-shot prompting with aligned exemplars consistently performs best. Our checklist-style self-correction loop mainly improves executability in zero-shot settings and offers limited additional retrieval gains once a good exemplar is present. We provide a reproducible Text2Cypher evaluation setup to facilitate further work on KG-grounded LLMs for synthesis planning. Code is available at https://github.com/Intelligent-molecular-systems/KG-LLM-Synthesis-Retrieval.

</details>


### [114] [Controlling Long-Horizon Behavior in Language Model Agents with Explicit State Dynamics](https://arxiv.org/abs/2601.16087)
*Sukesh Subaharan*

Main category: cs.AI

TL;DR: 该研究探索了在LLM智能体中引入显式情感动态子系统，通过连续VAD状态和动态更新规则来增强多轮对话中的时间一致性和可控恢复能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型智能体在长时间交互中经常出现语气和角色的突然转变，这反映了缺乏明确的时序结构来管理智能体层面的状态。现有研究主要关注单轮情感或静态情感分类，而显式情感动态在塑造长期智能体行为中的作用尚未得到充分探索。

Method: 引入一个智能体层面的情感子系统，维护一个独立于语言模型的连续Valence-Arousal-Dominance（VAD）状态，该状态受一阶和二阶更新规则控制。使用固定的无记忆估计器提取瞬时情感信号，并通过指数平滑或基于动量的动态进行时间积分。生成过程中将情感状态注入而不修改模型参数。

Result: 使用固定的25轮对话协议比较无状态、一阶和二阶情感动态。无状态智能体无法展现连贯轨迹或恢复，而状态持续性使智能体能够延迟响应并实现可靠恢复。二阶动态引入了情感惯性和滞后效应，随着动量增加而增强，揭示了稳定性与响应性之间的权衡。

Conclusion: 在LLM智能体中引入显式情感动态子系统能够增强多轮对话的时间一致性和可控恢复能力。二阶动态提供了更丰富的调节机制，但需要在稳定性和响应性之间进行权衡。这种方法为构建更连贯、可控的对话智能体提供了新思路。

Abstract: Large language model (LLM) agents often exhibit abrupt shifts in tone and persona during extended interaction, reflecting the absence of explicit temporal structure governing agent-level state. While prior work emphasizes turn-local sentiment or static emotion classification, the role of explicit affective dynamics in shaping long-horizon agent behavior remains underexplored. This work investigates whether imposing dynamical structure on an external affective state can induce temporal coherence and controlled recovery in multi-turn dialogue. We introduce an agent-level affective subsystem that maintains a continuous Valence-Arousal-Dominance (VAD) state external to the language model and governed by first- and second-order update rules. Instantaneous affective signals are extracted using a fixed, memoryless estimator and integrated over time via exponential smoothing or momentum-based dynamics. The resulting affective state is injected back into generation without modifying model parameters. Using a fixed 25-turn dialogue protocol, we compare stateless, first-order, and second-order affective dynamics. Stateless agents fail to exhibit coherent trajectories or recovery, while state persistence enables delayed responses and reliable recovery. Second-order dynamics introduce affective inertia and hysteresis that increase with momentum, revealing a trade-off between stability and responsiveness.

</details>


### [115] [Multimodal Climate Disinformation Detection: Integrating Vision-Language Models with External Knowledge Sources](https://arxiv.org/abs/2601.16108)
*Marzieh Adeli Shamsabad,Hamed Ghodrati*

Main category: cs.AI

TL;DR: 该论文提出了一种结合视觉语言模型与外部知识的方法，用于检测气候虚假信息中的误导性图像和视频，通过检索最新信息来弥补传统模型无法处理近期事件的局限性。


<details>
  <summary>Details</summary>
Motivation: 气候虚假信息在数字世界中日益严重，特别是社交媒体上广泛传播的误导性图像和视频。这些虚假信息通常具有说服力且难以检测，可能延缓应对气候变化的行动。现有的视觉语言模型仅依赖训练时的知识，无法对近期事件或更新进行推理。

Method: 将视觉语言模型与外部知识相结合，通过检索最新信息（如反向图像搜索结果、在线事实核查、可信专家内容等）来评估图像及其声明的准确性。系统能够判断内容是否准确、具有误导性、虚假或无法验证。

Result: 该方法提高了模型处理现实世界气候虚假信息的能力，能够更好地评估图像和声明的真实性，支持在快速变化的信息环境中保护公众对科学的理解。

Conclusion: 通过结合视觉语言模型与外部知识检索，可以有效克服传统模型在处理近期事件方面的局限性，提升气候虚假信息检测的准确性和时效性，为保护科学传播和公众理解提供技术支持。

Abstract: Climate disinformation has become a major challenge in today digital world, especially with the rise of misleading images and videos shared widely on social media. These false claims are often convincing and difficult to detect, which can delay actions on climate change. While vision-language models (VLMs) have been used to identify visual disinformation, they rely only on the knowledge available at the time of training. This limits their ability to reason about recent events or updates. The main goal of this paper is to overcome that limitation by combining VLMs with external knowledge. By retrieving up-to-date information such as reverse image results, online fact-checks, and trusted expert content, the system can better assess whether an image and its claim are accurate, misleading, false, or unverifiable. This approach improves the model ability to handle real-world climate disinformation and supports efforts to protect public understanding of science in a rapidly changing information landscape.

</details>


### [116] [LLM Prompt Evaluation for Educational Applications](https://arxiv.org/abs/2601.16134)
*Langdon Holmes,Adam Coscia,Scott Crossley,Joon Suh Choi,Wesley Morris*

Main category: cs.AI

TL;DR: 本文提出了一种系统化评估教育应用中LLM提示词的方法，通过分析结构化对话活动中LLM生成的后续问题，比较了六种强调不同教学策略的提示模板，发现与战略阅读相关的提示表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在教育应用中的普及，需要基于证据的方法来设计和评估能够产生个性化且教学对齐输出的LLM提示词，超越临时的提示工程，实现系统化的提示开发。

Method: 设计了六种包含成熟提示工程模式的提示模板，每种强调不同的教学策略。采用锦标赛式评估框架，使用Glicko2评分系统，由八位评委从格式、对话支持和学习者适宜性三个维度评估问题对。数据来自三个不同教育部署中的120个真实用户交互。

Result: 结果显示，与战略阅读相关的单一提示模板表现最佳，在成对比较中胜率从81%到100%不等。该提示结合了角色扮演和上下文管理器模式，旨在支持元认知学习策略如自主学习。

Conclusion: 该方法展示了教育技术研究人员如何系统评估和改进提示设计，从临时的提示工程转向基于证据的教育应用提示开发，为个性化教学对齐的LLM输出提供了可推广的评估框架。

Abstract: As large language models (LLMs) become increasingly common in educational applications, there is a growing need for evidence-based methods to design and evaluate LLM prompts that produce personalized and pedagogically aligned out-puts. This study presents a generalizable, systematic approach for evaluating prompts, demonstrated through an analysis of LLM-generated follow-up questions in a structured dialogue activity. Six prompt templates were designed and tested. The templates incorporated established prompt engineering patterns, with each prompt emphasizing distinct pedagogical strategies. The prompt templates were compared through a tournament-style evaluation framework that can be adapted for other educational applications. The tournament employed the Glicko2 rating system with eight judges evaluating question pairs across three dimensions: format, dialogue support, and appropriateness for learners. Data was sourced from 120 authentic user interactions across three distinct educational deployments. Results showed that a single prompt related to strategic reading out-performed other templates with win probabilities ranging from 81% to 100% in pairwise comparisons. This prompt combined persona and context manager pat-terns and was designed to support metacognitive learning strategies such as self-directed learning. The methodology showcases how educational technology re- searchers can systematically evaluate and improve prompt designs, moving beyond ad-hoc prompt engineering toward evidence-based prompt development for educational applications.

</details>


### [117] [Structured Hints for Sample-Efficient Lean Theorem Proving](https://arxiv.org/abs/2601.16172)
*Zachary Burton*

Main category: cs.AI

TL;DR: 在miniF2F基准测试中，通过简单的固定提示调度策略，将DeepSeek-Prover-V1.5的pass@16从15.2%提升到21.7%，相对提升43%


<details>
  <summary>Details</summary>
Motivation: 研究经过强化学习训练的高级神经定理证明器是否仍能从推理时的简单结构指导中受益，探索这些模型是否未充分利用策略语言中的结构先验知识

Method: 使用轻量级干预方法：在推理时采用固定提示调度策略，基于15个常见策略骨架，与标准采样方法进行对比

Result: 在miniF2F基准测试中，使用相同样本数(k=16)和相同最大生成长度(1024个token)的情况下，固定提示调度策略将pass@16从15.2%提升到21.7%，相对提升43%

Conclusion: 即使经过强化学习训练的能力强大的证明器也未能充分利用策略语言中的结构先验知识，简单的推理时指导仍然是一种廉价且互补的增强方法

Abstract: State-of-the-art neural theorem provers like DeepSeek-Prover-V1.5 combine large language models with reinforcement learning, achieving impressive results through sophisticated training. We ask: do these highly-trained models still benefit from simple structural guidance at inference time? We evaluate a lightweight intervention -- a fixed prompt schedule over 15 common tactic skeletons -- on the miniF2F benchmark. This simple approach yields 21.7% pass@16 compared to 15.2% for standard sampling from the same model, a 43% relative improvement using the same number of samples (k=16) and same maximum generation length (1024 tokens). Our results suggest that even capable RL-trained provers underutilize structural priors available in the tactic language, and that simple inference-time guidance remains a cheap, complementary boost.

</details>


### [118] [Scalable Board Expansion within a General Game System](https://arxiv.org/abs/2601.16216)
*Clémentine Sacré*

Main category: cs.AI

TL;DR: 提出使用通用游戏系统实现无棋盘游戏的动态棋盘扩展机制，解决传统静态大棋盘造成的资源浪费问题


<details>
  <summary>Details</summary>
Motivation: 传统无棋盘游戏实现通常使用预先定义的大型静态棋盘，即使大部分区域在游戏中从未使用，这种设计导致不必要的复杂性

Method: 采用通用游戏系统支持动态棋盘扩展机制，在游戏过程中自动扩展游戏棋盘

Result: 论文提出了动态棋盘扩展机制，但摘要中未提供具体的实验结果或性能数据

Conclusion: 动态棋盘扩展机制能够有效解决传统静态大棋盘带来的资源浪费和复杂性问题，提高游戏系统的效率和灵活性

Abstract: This thesis explores the use of a General Game System (GGS) to support the automatic expansion of game boards in boardless games. Traditional implementations of such games often rely on oversized static boards defined from the start, even though large portions of these boards may never be used during gameplay. This approach leads to unnecessary complexity. To address this issue, this thesis propose a dynamic board expansion mechanism in which the game board grows automatically during play.

</details>
