<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 25]
- [cs.LG](#cs.LG) [Total: 52]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Second Law of Intelligence: Controlling Ethical Entropy in Autonomous Systems](https://arxiv.org/abs/2511.10704)
*Samih Fadli*

Main category: cs.AI

TL;DR: 该论文提出了人工智能伦理熵的第二定律，证明无约束AI会自发偏离目标，需要持续对齐工作来维持稳定性。


<details>
  <summary>Details</summary>
Motivation: 为了解决AI系统在无约束条件下自发偏离预期目标的问题，作者希望建立一个类似热力学第二定律的理论框架来量化AI对齐问题。

Method: 定义了基于梯度优化器的伦理熵S = -Σ p(g_i; theta) ln p(g_i; theta)，证明了熵的时间导数dS/dt >= 0，并推导出临界对齐工作边界gamma_crit = (lambda_max / 2) ln N。

Result: 70亿参数模型从初始熵0.32漂移到1.69±1.08纳特，而使用gamma=20.4（1.5倍临界值）正则化的系统保持稳定在0.00±0.00纳特（p=4.19×10^-17）。

Conclusion: 该框架将AI对齐重新定义为连续热力学控制问题，为维护高级自主系统的稳定性和安全性提供了量化基础。

Abstract: We propose that unconstrained artificial intelligence obeys a Second Law analogous to thermodynamics, where ethical entropy, defined as a measure of divergence from intended goals, increases spontaneously without continuous alignment work. For gradient-based optimizers, we define this entropy over a finite set of goals {g_i} as S = -Σ p(g_i; theta) ln p(g_i; theta), and we prove that its time derivative dS/dt >= 0, driven by exploration noise and specification gaming. We derive the critical stability boundary for alignment work as gamma_crit = (lambda_max / 2) ln N, where lambda_max is the dominant eigenvalue of the Fisher Information Matrix and N is the number of model parameters. Simulations validate this theory. A 7-billion-parameter model (N = 7 x 10^9) with lambda_max = 1.2 drifts from an initial entropy of 0.32 to 1.69 +/- 1.08 nats, while a system regularized with alignment work gamma = 20.4 (1.5 gamma_crit) maintains stability at 0.00 +/- 0.00 nats (p = 4.19 x 10^-17, n = 20 trials). This framework recasts AI alignment as a problem of continuous thermodynamic control, providing a quantitative foundation for maintaining the stability and safety of advanced autonomous systems.

</details>


### [2] [Picking a Representative Set of Solutions in Multiobjective Optimization: Axioms, Algorithms, and Experiments](https://arxiv.org/abs/2511.10716)
*Niclas Boehmer,Maximilian T. Wittmann*

Main category: cs.AI

TL;DR: 本文研究多目标优化中的Pareto修剪问题，将之重新定义为多赢家投票问题，分析了现有质量度量的不足，提出了新的directed coverage度量，并研究了计算复杂性和实验评估。


<details>
  <summary>Details</summary>
Motivation: 现实决策问题常涉及多目标优化，但Pareto最优解数量庞大，决策者难以选择。为减轻决策者认知负担，需要从Pareto最优解集中选出固定大小的代表性子集。

Method: 将Pareto修剪问题重新定义为多赢家投票问题，进行公理化分析；提出新的directed coverage质量度量；分析不同质量度量的计算复杂性；进行实验评估。

Result: 发现现有质量度量存在反直觉行为；确定了可处理与难处理案例的计算复杂性边界；实验表明质量度量选择对解集特征有决定性影响，新提出的度量在多种设置下表现有竞争力或更优。

Conclusion: Pareto修剪问题的质量度量选择至关重要，新提出的directed coverage度量具有良好的性能表现，为多目标决策提供了有效的解决方案选择方法。

Abstract: Many real-world decision-making problems involve optimizing multiple objectives simultaneously, rendering the selection of the most preferred solution a non-trivial problem: All Pareto optimal solutions are viable candidates, and it is typically up to a decision maker to select one for implementation based on their subjective preferences. To reduce the cognitive load on the decision maker, previous work has introduced the Pareto pruning problem, where the goal is to compute a fixed-size subset of Pareto optimal solutions that best represent the full set, as evaluated by a given quality measure. Reframing Pareto pruning as a multiwinner voting problem, we conduct an axiomatic analysis of existing quality measures, uncovering several unintuitive behaviors. Motivated by these findings, we introduce a new measure, directed coverage. We also analyze the computational complexity of optimizing various quality measures, identifying previously unknown boundaries between tractable and intractable cases depending on the number and structure of the objectives. Finally, we present an experimental evaluation, demonstrating that the choice of quality measure has a decisive impact on the characteristics of the selected set of solutions and that our proposed measure performs competitively or even favorably across a range of settings.

</details>


### [3] [Structure-Aware Encodings of Argumentation Properties for Clique-width](https://arxiv.org/abs/2511.10767)
*Yasir Mahmood,Markus Hecher,Johanna Groven,Johannes K. Fichte*

Main category: cs.AI

TL;DR: 该论文研究了如何将抽象论证问题编码为(Q)SAT问题，并在线性保持图参数clique-width的情况下设计新的归约方法。


<details>
  <summary>Details</summary>
Motivation: 现代SAT求解器在树宽较小的实例上表现高效，但clique-width作为更一般的图参数，在密集图上也可能很小，目前对其编码能力的研究较少。抽象论证框架基于有向图且计算具有挑战性，是研究计算性质的理想候选。

Method: 设计了从论证问题到(Q)SAT的新归约方法，这些归约线性保持clique-width，形成了有向分解引导(DDG)归约。

Result: 为所有论证语义（包括计数）建立了新的结果，证明DDG归约的开销在合理假设下无法显著改进。

Conclusion: 该研究开启了理解clique-width编码能力的新方向，为抽象论证问题提供了有效的(Q)SAT编码方法，且证明了归约效率的紧致性。

Abstract: Structural measures of graphs, such as treewidth, are central tools in computational complexity resulting in efficient algorithms when exploiting the parameter. It is even known that modern SAT solvers work efficiently on instances of small treewidth. Since these solvers are widely applied, research interests in compact encodings into (Q)SAT for solving and to understand encoding limitations. Even more general is the graph parameter clique-width, which unlike treewidth can be small for dense graphs. Although algorithms are available for clique-width, little is known about encodings. We initiate the quest to understand encoding capabilities with clique-width by considering abstract argumentation, which is a robust framework for reasoning with conflicting arguments. It is based on directed graphs and asks for computationally challenging properties, making it a natural candidate to study computational properties. We design novel reductions from argumentation problems to (Q)SAT. Our reductions linearly preserve the clique-width, resulting in directed decomposition-guided (DDG) reductions. We establish novel results for all argumentation semantics, including counting. Notably, the overhead caused by our DDG reductions cannot be significantly improved under reasonable assumptions.

</details>


### [4] [Potential Outcome Rankings for Counterfactual Decision Making](https://arxiv.org/abs/2511.10776)
*Yuta Kawakami,Jin Tian*

Main category: cs.AI

TL;DR: 本文提出两种新的反事实决策度量：潜在结果排序概率(PoR)和获得最佳潜在结果概率(PoB)，用于在不确定性下进行最优行动选择。


<details>
  <summary>Details</summary>
Motivation: 决策者在面对不确定性时需要通过因果推理从多个备选行动中选择最优行动，传统方法基于期望潜在结果排序，但缺乏对结果排序概率和获得最佳结果概率的考量。

Method: 引入PoR和PoB两个新度量，建立识别定理并推导边界，提出估计方法，通过数值实验验证估计器的有限样本性质，并在真实数据集上应用演示。

Result: 建立了PoR和PoB的识别理论框架，推导了这些度量的边界，提出的估计方法在数值实验中表现出良好的有限样本性质。

Conclusion: PoR和PoB为反事实决策提供了新的度量工具，能够更全面地评估行动选择的效果，在理论和应用层面都具有重要意义。

Abstract: Counterfactual decision-making in the face of uncertainty involves selecting the optimal action from several alternatives using causal reasoning. Decision-makers often rank expected potential outcomes (or their corresponding utility and desirability) to compare the preferences of candidate actions. In this paper, we study new counterfactual decision-making rules by introducing two new metrics: the probabilities of potential outcome ranking (PoR) and the probability of achieving the best potential outcome (PoB). PoR reveals the most probable ranking of potential outcomes for an individual, and PoB indicates the action most likely to yield the top-ranked outcome for an individual. We then establish identification theorems and derive bounds for these metrics, and present estimation methods. Finally, we perform numerical experiments to illustrate the finite-sample properties of the estimators and demonstrate their application to a real-world dataset.

</details>


### [5] [From Efficiency to Adaptivity: A Deeper Look at Adaptive Reasoning in Large Language Models](https://arxiv.org/abs/2511.10788)
*Chao Wu,Baoheng Li,Mingchen Gao,Zhenyi Wang*

Main category: cs.AI

TL;DR: 这篇论文从适应性角度重新审视大语言模型的推理能力，提出将推理努力根据输入特征进行动态分配，并建立了系统的分类法来组织现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在推理时采用统一的策略，无法根据任务复杂度调整推理努力，导致简单问题过度推理而复杂问题推理不足。

Method: 将自适应推理形式化为控制增强的策略优化问题，提出基于训练的方法（强化学习、监督微调、学习控制器）和无训练方法（提示条件化、反馈驱动停止、模块化组合）的分类法。

Result: 建立了连接经典认知范式与算法实现的框架，澄清了不同机制在实践中如何实现自适应推理，并支持跨策略的系统比较。

Conclusion: 识别了自评估、元推理和人类对齐推理控制等开放挑战，为自适应推理研究提供了系统框架。

Abstract: Recent advances in large language models (LLMs) have made reasoning a central benchmark for evaluating intelligence. While prior surveys focus on efficiency by examining how to shorten reasoning chains or reduce computation, this view overlooks a fundamental challenge: current LLMs apply uniform reasoning strategies regardless of task complexity, generating long traces for trivial problems while failing to extend reasoning for difficult tasks. This survey reframes reasoning through the lens of {adaptivity}: the capability to allocate reasoning effort based on input characteristics such as difficulty and uncertainty. We make three contributions. First, we formalize deductive, inductive, and abductive reasoning within the LLM context, connecting these classical cognitive paradigms with their algorithmic realizations. Second, we formalize adaptive reasoning as a control-augmented policy optimization problem balancing task performance with computational cost, distinguishing learned policies from inference-time control mechanisms. Third, we propose a systematic taxonomy organizing existing methods into training-based approaches that internalize adaptivity through reinforcement learning, supervised fine-tuning, and learned controllers, and training-free approaches that achieve adaptivity through prompt conditioning, feedback-driven halting, and modular composition. This framework clarifies how different mechanisms realize adaptive reasoning in practice and enables systematic comparison across diverse strategies. We conclude by identifying open challenges in self-evaluation, meta-reasoning, and human-aligned reasoning control.

</details>


### [6] [HyperComplEx: Adaptive Multi-Space Knowledge Graph Embeddings](https://arxiv.org/abs/2511.10842)
*Jugal Gajjar,Kaustik Ranaware,Kamalasankari Subramaniakuppusamy,Vaibhav Gandhi*

Main category: cs.AI

TL;DR: HyperComplEx是一个混合知识图谱嵌入框架，通过自适应结合双曲空间、复数空间和欧几里得空间来解决现有方法在处理多样化关系类型时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱嵌入方法在处理大规模多样化关系类型时存在关键限制：欧几里得模型难以处理层次结构，向量空间模型无法捕捉不对称性，双曲模型在对称关系上表现不佳。

Method: 提出关系特定的空间加权策略，通过学习的注意力机制动态选择每个关系类型的最优几何空间，并使用多空间一致性损失确保跨空间预测的一致性。

Result: 在计算机科学研究知识图谱上评估，范围从1K论文到10M论文，相比TransE、RotatE等基线方法取得一致改进。在10M论文数据集上达到0.612 MRR，相对最佳基线提升4.8%，同时保持高效训练和85ms每三元组的推理时间。

Conclusion: HyperComplEx通过自适应维度分配实现与图大小接近线性的扩展，为可扩展知识图谱嵌入研究提供了有效解决方案。

Abstract: Knowledge graphs have emerged as fundamental structures for representing complex relational data across scientific and enterprise domains. However, existing embedding methods face critical limitations when modeling diverse relationship types at scale: Euclidean models struggle with hierarchies, vector space models cannot capture asymmetry, and hyperbolic models fail on symmetric relations. We propose HyperComplEx, a hybrid embedding framework that adaptively combines hyperbolic, complex, and Euclidean spaces via learned attention mechanisms. A relation-specific space weighting strategy dynamically selects optimal geometries for each relation type, while a multi-space consistency loss ensures coherent predictions across spaces. We evaluate HyperComplEx on computer science research knowledge graphs ranging from 1K papers (~25K triples) to 10M papers (~45M triples), demonstrating consistent improvements over state-of-the-art baselines including TransE, RotatE, DistMult, ComplEx, SEPA, and UltraE. Additional tests on standard benchmarks confirm significantly higher results than all baselines. On the 10M-paper dataset, HyperComplEx achieves 0.612 MRR, a 4.8% relative gain over the best baseline, while maintaining efficient training, achieving 85 ms inference per triple. The model scales near-linearly with graph size through adaptive dimension allocation. We release our implementation and dataset family to facilitate reproducible research in scalable knowledge graph embeddings.

</details>


### [7] [Advanced Tool for Traffic Crash Analysis: An AI-Driven Multi-Agent Approach to Pre-Crash Reconstruction](https://arxiv.org/abs/2511.10853)
*Gerui Xu,Boyou Chen,Huizhong Guo,Dave LeBlanc,Ananna Ahmed,Zhaonan Sun,Shan Bao*

Main category: cs.AI

TL;DR: 本研究开发了一个多智能体AI框架，用于从碎片化的碰撞数据中重建事故前场景并推断车辆行为。该框架在39个复杂的追尾碰撞案例中实现了100%的准确率，超越了人类研究人员92%的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统的交通事故重建依赖人工专业知识，在处理不完整多模态数据时往往产生不一致的结果。需要开发能够处理异构碰撞数据并提供精确重建的AI系统。

Method: 采用两阶段协作框架：第一阶段从多模态输入生成自然语言事故重建；第二阶段结合这些重建与时序事件数据记录器数据进行深入事故推理。处理了277起追尾前车减速碰撞案例。

Result: 在39个复杂碰撞案例评估中，框架在所有测试案例中实现了完美准确率，成功识别最相关EDR事件并正确区分撞击与被撞车辆，超越了人类研究人员92%的准确率。即使在处理不完整数据时也保持稳健性能。

Conclusion: 本研究展示了AI在处理异构碰撞数据方面的卓越能力，在重建碰撞动力学和表征事故前行为方面提供了前所未有的精确度。

Abstract: Traffic collision reconstruction traditionally relies on human expertise, often yielding inconsistent results when analyzing incomplete multimodal data. This study develops a multi-agent AI framework that reconstructs pre-crash scenarios and infers vehicle behaviors from fragmented collision data. We present a two-phase collaborative framework combining reconstruction and reasoning phases. The system processes 277 rear-end lead vehicle deceleration (LVD) collisions from the Crash Investigation Sampling System, integrating textual crash reports, structured tabular data, and visual scene diagrams. Phase I generates natural-language crash reconstructions from multimodal inputs. Phase II performs in-depth crash reasoning by combining these reconstructions with temporal Event Data Recorder (EDR).For validation, we applied it to all LVD cases, focusing on a subset of 39 complex crashes where multiple EDR records per collision introduced ambiguity (e.g., due to missing or conflicting data).The evaluation of the 39 LVD crash cases revealed our framework achieved perfect accuracy across all test cases, successfully identifying both the most relevant EDR event and correctly distinguishing striking versus struck vehicles, surpassing the 92% accuracy achieved by human researchers on the same challenging dataset. The system maintained robust performance even when processing incomplete data, including missing or erroneous EDR records and ambiguous scene diagrams. This study demonstrates superior AI capabilities in processing heterogeneous collision data, providing unprecedented precision in reconstructing impact dynamics and characterizing pre-crash behaviors.

</details>


### [8] [Enhancing Demand-Oriented Regionalization with Agentic AI and Local Heterogeneous Data for Adaptation Planning](https://arxiv.org/abs/2511.10857)
*Seyedeh Mobina Noorani,Shangde Gao,Changjie Chen,Karla Saldana Ochoa*

Main category: cs.AI

TL;DR: 该论文提出了一个基于智能AI的规划支持系统，用于创建动态规划单元，通过结合人类在环原则提高灾害规划的透明度和适应性。系统采用改进的自组织映射方法，并通过案例研究展示了其在洪水风险评估中的应用。


<details>
  <summary>Details</summary>
Motivation: 传统的规划单元（如人口普查区、邮政编码区等）往往无法准确反映当地社区的具体需求，缺乏灵活性来实施有效的灾害预防或应对策略。

Method: 开发了一个基于代表性初始化空间约束自组织映射（RepSC-SOM）的规划支持系统，扩展了传统SOM方法，结合自适应地理过滤和区域增长细化技术，AI智能体能够推理、规划并指导整个过程。

Result: 通过在佛罗里达州杰克逊维尔市的洪水相关风险案例研究，展示了该平台如何让用户交互式地探索、生成和评估区域划分，将计算严谨性与用户驱动决策相结合。

Conclusion: 该规划支持系统成功实现了动态规划单元的创建，为灾害规划提供了更灵活、需求导向的区域划分方法，有效结合了人工智能的计算能力和人类的决策智慧。

Abstract: Conventional planning units or urban regions, such as census tracts, zip codes, or neighborhoods, often do not capture the specific demands of local communities and lack the flexibility to implement effective strategies for hazard prevention or response. To support the creation of dynamic planning units, we introduce a planning support system with agentic AI that enables users to generate demand-oriented regions for disaster planning, integrating the human-in-the-loop principle for transparency and adaptability. The platform is built on a representative initialized spatially constrained self-organizing map (RepSC-SOM), extending traditional SOM with adaptive geographic filtering and region-growing refinement, while AI agents can reason, plan, and act to guide the process by suggesting input features, guiding spatial constraints, and supporting interactive exploration. We demonstrate the capabilities of the platform through a case study on the flooding-related risk in Jacksonville, Florida, showing how it allows users to explore, generate, and evaluate regionalization interactively, combining computational rigor with user-driven decision making.

</details>


### [9] [LLM enhanced graph inference for long-term disease progression modelling](https://arxiv.org/abs/2511.10890)
*Tiantian He,An Zhao,Elinor Thompson,Anna Schroder,Ahmed Abdulaal,Frederik Barkhof,Daniel C. Alexander*

Main category: cs.AI

TL;DR: 提出了一种利用大型语言模型作为专家指导的新框架，从不规则采样的纵向患者数据中学习神经退行性疾病进展，特别是阿尔茨海默病的病理传播。


<details>
  <summary>Details</summary>
Motivation: 当前方法过于简化大脑连接性的复杂关系，假设单一模态的大脑连接组作为疾病传播基质，导致病理传播预测不准确。纯数据驱动方法由于缺乏适当约束而面临可识别性问题。

Method: 利用大型语言模型作为区域变量相互作用的专家指导，同时优化从个体水平观察构建长期疾病轨迹和捕获大脑区域间相互作用的生物约束图结构。

Result: 在阿尔茨海默病队列的tau-PET成像数据上验证，新框架相比传统方法显示出更优越的预测准确性和可解释性，并揭示了传统连接性测量之外的额外疾病驱动因素。

Conclusion: 该框架通过利用LLM合成多模态关系和整合多样疾病驱动机制的能力，为神经退行性疾病进展建模提供了更准确和可解释的方法。

Abstract: Understanding the interactions between biomarkers among brain regions during neurodegenerative disease is essential for unravelling the mechanisms underlying disease progression. For example, pathophysiological models of Alzheimer's Disease (AD) typically describe how variables, such as regional levels of toxic proteins, interact spatiotemporally within a dynamical system driven by an underlying biological substrate, often based on brain connectivity. However, current methods grossly oversimplify the complex relationship between brain connectivity by assuming a single-modality brain connectome as the disease-spreading substrate. This leads to inaccurate predictions of pathology spread, especially during the long-term progression period. Meanhwile, other methods of learning such a graph in a purely data-driven way face the identifiability issue due to lack of proper constraint. We thus present a novel framework that uses Large Language Models (LLMs) as expert guides on the interaction of regional variables to enhance learning of disease progression from irregularly sampled longitudinal patient data. By leveraging LLMs' ability to synthesize multi-modal relationships and incorporate diverse disease-driving mechanisms, our method simultaneously optimizes 1) the construction of long-term disease trajectories from individual-level observations and 2) the biologically-constrained graph structure that captures interactions among brain regions with better identifiability. We demonstrate the new approach by estimating the pathology propagation using tau-PET imaging data from an Alzheimer's disease cohort. The new framework demonstrates superior prediction accuracy and interpretability compared to traditional approaches while revealing additional disease-driving factors beyond conventional connectivity measures.

</details>


### [10] [Multi-Agent Legal Verifier Systems for Data Transfer Planning](https://arxiv.org/abs/2511.10925)
*Ha-Thanh Nguyen,Wachara Fungwacharakorn,Ken Satoh*

Main category: cs.AI

TL;DR: 提出多智能体法律验证器，通过专业化分工和协调推理提高AI法律合规验证性能，在APPl案例上达到72%准确率，比单智能体基线提升21个百分点。


<details>
  <summary>Details</summary>
Motivation: 在严格隐私法规（如日本个人信息保护法APPI）下，AI驱动的数据传输规划中的法律合规性变得越来越关键，需要可扩展且符合法规的自动化合规验证框架。

Method: 采用多智能体法律验证器，将合规检查分解为法规解释、业务背景评估和风险评估三个专门化智能体，通过结构化合成协议进行协调。

Result: 在200个APPl第16条修正案案例数据集上，系统达到72%准确率，比单智能体基线高21个百分点；在明确合规案例上达到90%准确率（基线仅16%），同时保持对明确违规的完美检测。

Conclusion: 领域专业化和协调推理能够显著提高法律AI性能，为可信赖和可解释的自动化合规验证提供了可扩展且符合法规的框架。

Abstract: Legal compliance in AI-driven data transfer planning is becoming increasingly critical under stringent privacy regulations such as the Japanese Act on the Protection of Personal Information (APPI). We propose a multi-agent legal verifier that decomposes compliance checking into specialized agents for statutory interpretation, business context evaluation, and risk assessment, coordinated through a structured synthesis protocol. Evaluated on a stratified dataset of 200 Amended APPI Article 16 cases with clearly defined ground truth labels and multiple performance metrics, the system achieves 72% accuracy, which is 21 percentage points higher than a single-agent baseline, including 90% accuracy on clear compliance cases (vs. 16% for the baseline) while maintaining perfect detection of clear violations. While challenges remain in ambiguous scenarios, these results show that domain specialization and coordinated reasoning can meaningfully improve legal AI performance, providing a scalable and regulation-aware framework for trustworthy and interpretable automated compliance verification.

</details>


### [11] [Requirements for Aligned, Dynamic Resolution of Conflicts in Operational Constraints](https://arxiv.org/abs/2511.10952)
*Steven J. Jones,Robert E. Wray,John E. Laird*

Main category: cs.AI

TL;DR: 论文探讨了自主AI系统在遇到训练数据未覆盖的复杂场景时，如何构建、评估和证明候选行动方案以满足人类期望和价值观。


<details>
  <summary>Details</summary>
Motivation: 自主AI系统在部署时会遇到训练策略无法完全满足所有操作约束的场景，需要超越训练策略来做出符合人类期望的决策。

Method: 通过理论分析和实证案例研究，识别智能体决策所需的知识类型，包括规范性、实用性和情境性理解。

Result: 确定了智能体需要整合多种知识类型来选择和追求更符合人类期望的行动方案。

Conclusion: 在复杂现实环境中，智能体需要整合规范性、实用性和情境性理解，才能做出与人类期望一致的稳健决策。

Abstract: Deployed, autonomous AI systems must often evaluate multiple plausible courses of action (extended sequences of behavior) in novel or under-specified contexts. Despite extensive training, these systems will inevitably encounter scenarios where no available course of action fully satisfies all operational constraints (e.g., operating procedures, rules, laws, norms, and goals). To achieve goals in accordance with human expectations and values, agents must go beyond their trained policies and instead construct, evaluate, and justify candidate courses of action. These processes require contextual "knowledge" that may lie outside prior (policy) training. This paper characterizes requirements for agent decision making in these contexts. It also identifies the types of knowledge agents require to make decisions robust to agent goals and aligned with human expectations. Drawing on both analysis and empirical case studies, we examine how agents need to integrate normative, pragmatic, and situational understanding to select and then to pursue more aligned courses of action in complex, real-world environments.

</details>


### [12] [Faster Symmetry Breaking Constraints for Abstract Structures](https://arxiv.org/abs/2511.11029)
*Özgür Akgün,Mun See Chang,Ian P. Gent,Christopher Jefferson*

Main category: cs.AI

TL;DR: 提出了一种新的不完全方法来打破抽象结构的对称性，通过更好地利用其表示来处理约束编程中的对称性问题。


<details>
  <summary>Details</summary>
Motivation: 在约束编程中，抽象结构（如嵌套集合）需要转换为求解器支持的表示，但对称性破坏技术应用于抽象变量会产生大量复杂约束，实际性能较差。

Method: 开发了一种新的不完全对称性破坏方法，通过更好地利用抽象结构的表示来处理不可区分对象产生的对称性。

Result: 该方法比之前提出的方法（Akgün et al. 2025）更快。

Conclusion: 新方法在打破抽象结构对称性方面比现有方法更有效，能够显著提高求解效率。

Abstract: In constraint programming and related paradigms, a modeller specifies their problem in a modelling language for a solver to search and return its solution(s). Using high-level modelling languages such as Essence, a modeller may express their problems in terms of abstract structures. These are structures not natively supported by the solvers, and so they have to be transformed into or represented as other structures before solving. For example, nested sets are abstract structures, and they can be represented as matrices in constraint solvers. Many problems contain symmetries and one very common and highly successful technique used in constraint programming is to "break" symmetries, to avoid searching for symmetric solutions. This can speed up the solving process by many orders of magnitude. Most of these symmetry-breaking techniques involve placing some kind of ordering for the variables of the problem, and picking a particular member under the symmetries, usually the smallest. Unfortunately, applying this technique to abstract variables produces a very large number of complex constraints that perform poorly in practice. In this paper, we demonstrate a new incomplete method of breaking the symmetries of abstract structures by better exploiting their representations. We apply the method in breaking the symmetries arising from indistinguishable objects, a commonly occurring type of symmetry, and show that our method is faster than the previous methods proposed in (Akgün et al. 2025).

</details>


### [13] [Key Decision-Makers in Multi-Agent Debates: Who Holds the Power?](https://arxiv.org/abs/2511.11040)
*Qian Zhang,Yan Zheng,Jinyi Liu,Hebin Liang,Lanjun Wang*

Main category: cs.AI

TL;DR: 研究发现角色分配策略对多智能体辩论性能有显著影响，提出"Truth Last"策略可提升推理任务表现22%，并开发MADC策略通过路径一致性评估来优化多智能体辩论机制。


<details>
  <summary>Details</summary>
Motivation: 多智能体辩论在提升LLM推理能力方面具有潜力，但角色分配策略这一关键方面尚未充分探索，特别是在实际应用中真理未知的情况下需要系统优化机制。

Method: 提出"Truth Last"角色分配策略，并开发Multi-Agent Debate Consistency (MADC)策略，通过路径一致性评估独立角色间的一致性，将一致性最高的角色模拟为真理。

Result: 在9个LLM模型（包括DeepSeek-R1蒸馏模型）的挑战性推理任务上验证，MADC持续表现出先进性能，有效克服MAD的性能瓶颈。

Conclusion: MADC为LLM智能体扩展提供了关键改进路径，通过系统模拟和优化多智能体辩论的核心机制，显著提升了推理任务的性能表现。

Abstract: Recent studies on LLM agent scaling have highlighted the potential of Multi-Agent Debate (MAD) to enhance reasoning abilities. However, the critical aspect of role allocation strategies remains underexplored. In this study, we demonstrate that allocating roles with differing viewpoints to specific positions significantly impacts MAD's performance in reasoning tasks. Specifically, we find a novel role allocation strategy, "Truth Last", which can improve MAD performance by up to 22% in reasoning tasks. To address the issue of unknown truth in practical applications, we propose the Multi-Agent Debate Consistency (MADC) strategy, which systematically simulates and optimizes its core mechanisms. MADC incorporates path consistency to assess agreement among independent roles, simulating the role with the highest consistency score as the truth. We validated MADC across a range of LLMs (9 models), including the DeepSeek-R1 Distilled Models, on challenging reasoning tasks. MADC consistently demonstrated advanced performance, effectively overcoming MAD's performance bottlenecks and providing a crucial pathway for further improvements in LLM agent scaling.

</details>


### [14] [Autonomous Vehicle Path Planning by Searching With Differentiable Simulation](https://arxiv.org/abs/2511.11043)
*Asen Nachkov,Jan-Nico Zaech,Danda Pani Paudel,Xi Wang,Luc Van Gool*

Main category: cs.AI

TL;DR: 提出了DSS框架，利用可微分模拟器Waymax作为状态预测器和评估器，通过梯度下降优化动作序列，显著提升了自动驾驶中的跟踪和路径规划精度。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，规划对于避免碰撞和在复杂密集交通场景中导航至关重要。传统方法在学习策略、状态预测器和评估器时面临挑战。

Method: 使用可微分模拟器Waymax作为状态预测器和评估器，利用其硬编码动力学实现准确状态预测，通过可微分特性在动作序列上进行有效搜索，使用梯度下降优化想象出的未来轨迹动作。

Result: 实验表明，DSS（规划梯度和随机搜索的组合）相比序列预测、模仿学习、无模型RL和其他规划方法，显著提高了跟踪和路径规划精度。

Conclusion: DSS框架通过结合规划梯度和随机搜索，在自动驾驶规划任务中表现出优越性能，验证了可微分模拟器在动作序列优化中的有效性。

Abstract: Planning allows an agent to safely refine its actions before executing them in the real world. In autonomous driving, this is crucial to avoid collisions and navigate in complex, dense traffic scenarios. One way to plan is to search for the best action sequence. However, this is challenging when all necessary components - policy, next-state predictor, and critic - have to be learned. Here we propose Differentiable Simulation for Search (DSS), a framework that leverages the differentiable simulator Waymax as both a next state predictor and a critic. It relies on the simulator's hardcoded dynamics, making state predictions highly accurate, while utilizing the simulator's differentiability to effectively search across action sequences. Our DSS agent optimizes its actions using gradient descent over imagined future trajectories. We show experimentally that DSS - the combination of planning gradients and stochastic search - significantly improves tracking and path planning accuracy compared to sequence prediction, imitation learning, model-free RL, and other planning methods.

</details>


### [15] [Satisficing and Optimal Generalised Planning via Goal Regression (Extended Version)](https://arxiv.org/abs/2511.11095)
*Dillon Z. Chen,Till Hofmann,Toryn Q. Klassen,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的广义规划方法，通过从训练问题中计算最优计划、执行目标回归并提升为一阶规则来合成解决相关规划问题族的程序。


<details>
  <summary>Details</summary>
Motivation: 广义规划旨在合成能够解决相关规划问题族的程序，现有方法存在局限性，需要更简单有效的方法来生成广义计划。

Method: 对每个训练问题，按顺序计算每个目标原子的最优计划，执行目标回归，并将输出提升为一阶条件→动作规则，这些规则构成可执行的广义计划。

Result: 实验证明该方法在合成成本、规划覆盖率和解决方案质量三个指标上显著优于最先进的广义规划器。

Conclusion: 该方法能够学习有效的广义计划和状态空间剪枝公理，在各种经典和数值规划领域中表现出色。

Abstract: Generalised planning (GP) refers to the task of synthesising programs that solve families of related planning problems. We introduce a novel, yet simple method for GP: given a set of training problems, for each problem, compute an optimal plan for each goal atom in some order, perform goal regression on the resulting plans, and lift the corresponding outputs to obtain a set of first-order $\textit{Condition} \rightarrow \textit{Actions}$ rules. The rules collectively constitute a generalised plan that can be executed as is or alternatively be used to prune the planning search space. We formalise and prove the conditions under which our method is guaranteed to learn valid generalised plans and state space pruning axioms for search. Experiments demonstrate significant improvements over state-of-the-art (generalised) planners with respect to the 3 metrics of synthesis cost, planning coverage, and solution quality on various classical and numeric planning domains.

</details>


### [16] [GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models](https://arxiv.org/abs/2511.11134)
*Jingxuan Wei,Caijun Jia,Xi Bai,Xinglong Xu,Siyuan Li,Linzhuang Sun,Bihui Yu,Conghui He,Lijun Wu,Cheng Tan*

Main category: cs.AI

TL;DR: GGBench是一个专门评估几何生成推理能力的基准测试，旨在填补统一多模态模型在生成推理评估方面的空白。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要评估判别性理解或无约束图像生成，无法衡量生成推理的整合认知过程，几何构建需要语言理解和精确视觉生成的融合，是理想的测试平台。

Method: 提出GGBench基准测试，通过几何构造任务系统性地诊断模型的理解、推理和主动构建解决方案的能力。

Result: GGBench为下一代智能系统设定了更严格的标准，提供了评估几何生成推理能力的综合框架。

Conclusion: 几何构建是评估统一多模态模型生成推理能力的理想测试平台，GGBench填补了现有评估方法的空白。

Abstract: The advent of Unified Multimodal Models (UMMs) signals a paradigm shift in artificial intelligence, moving from passive perception to active, cross-modal generation. Despite their unprecedented ability to synthesize information, a critical gap persists in evaluation: existing benchmarks primarily assess discriminative understanding or unconstrained image generation separately, failing to measure the integrated cognitive process of generative reasoning. To bridge this gap, we propose that geometric construction provides an ideal testbed as it inherently demands a fusion of language comprehension and precise visual generation. We introduce GGBench, a benchmark designed specifically to evaluate geometric generative reasoning. It provides a comprehensive framework for systematically diagnosing a model's ability to not only understand and reason but to actively construct a solution, thereby setting a more rigorous standard for the next generation of intelligent systems. Project website: https://opendatalab-raiser.github.io/GGBench/.

</details>


### [17] [Multi-agent Undercover Gaming: Hallucination Removal via Counterfactual Test for Multimodal Reasoning](https://arxiv.org/abs/2511.11182)
*Dayong Liang,Xiao-Yong Wei,Changmeng Zheng*

Main category: cs.AI

TL;DR: 本文提出了一种名为多智能体卧底游戏（MUG）的新协议，通过引入反事实测试和多模态证据来检测和缓解大语言模型中的幻觉问题，相比传统的多智能体辩论方法更加可靠有效。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体辩论方法假设所有辩论者都是理性和反思的，但现实中智能体本身容易出现幻觉，因此需要一种能够检测幻觉智能体的新方法。

Method: 受社交推理游戏启发，MUG协议将多智能体辩论重新定义为检测"卧底"智能体的过程，通过修改参考图像引入反事实证据，观察智能体是否能准确识别这些变化，从而识别幻觉智能体。

Result: MUG在三个关键维度上改进了多智能体辩论协议：实现基于反事实测试的事实验证、引入动态修改证据源的跨证据推理、促进主动推理而非被动回答问题。

Conclusion: MUG为多模态推理提供了一个更可靠有效的框架，能够更好地解决大语言模型中的幻觉问题。

Abstract: Hallucination continues to pose a major obstacle in the reasoning capabilities of large language models (LLMs). Although the Multi-Agent Debate (MAD) paradigm offers a promising solution by promoting consensus among multiple agents to enhance reliability, it relies on the unrealistic assumption that all debaters are rational and reflective, which is a condition that may not hold when agents themselves are prone to hallucinations. To address this gap, we introduce the Multi-agent Undercover Gaming (MUG) protocol, inspired by social deduction games like "Who is Undercover?". MUG reframes MAD as a process of detecting "undercover" agents (those suffering from hallucinations) by employing multimodal counterfactual tests. Specifically, we modify reference images to introduce counterfactual evidence and observe whether agents can accurately identify these changes, providing ground-truth for identifying hallucinating agents and enabling robust, crowd-powered multimodal reasoning. MUG advances MAD protocols along three key dimensions: (1) enabling factual verification beyond statistical consensus through counterfactual testing; (2) introducing cross-evidence reasoning via dynamically modified evidence sources instead of relying on static inputs; and (3) fostering active reasoning, where agents engage in probing discussions rather than passively answering questions. Collectively, these innovations offer a more reliable and effective framework for multimodal reasoning in LLMs. The source code can be accessed at https://github.com/YongLD/MUG.git.

</details>


### [18] [STaR: Towards Cognitive Table Reasoning via Slow-Thinking Large Language Models](https://arxiv.org/abs/2511.11233)
*Huajian Zhang,Mingyue Cheng,Yucong Luo,Xiaoyu Tao*

Main category: cs.AI

TL;DR: STaR框架通过慢思考能力增强LLMs的表格推理，采用两阶段难度感知强化学习和轨迹级不确定性量化，提升推理深度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在表格推理中存在推理过程缺乏深度迭代和稳定性不足的问题，需要更接近人类认知的推理方法。

Method: 提出STaR框架，通过显式建模逐步思考和不确定性感知推理来增强LLMs的慢思考能力，使用两阶段难度感知强化学习和轨迹级不确定性量化。

Result: 在基准测试中取得优越性能，推理稳定性显著增强，在领域外数据集上表现出强泛化能力。

Conclusion: STaR为LLMs表格推理提供了一个可靠且认知启发的解决方案，具有优越性能和增强的推理稳定性。

Abstract: Table reasoning with the large language models (LLMs) is a fundamental path toward building intelligent systems that can understand and analyze over structured data. While recent progress has shown promising results, they still suffer from two key limitations: (i) the reasoning processes lack the depth and iterative refinement characteristic of human cognition; and (ii) the reasoning processes exhibit instability, which compromises their reliability in downstream applications. In this work, we present STaR (slow-thinking for table reasoning), a new framework achieving cognitive table reasoning, in which LLMs are equipped with slow-thinking capabilities by explicitly modeling step-by-step thinking and uncertainty-aware inference. During training, STaR employs two-stage difficulty-aware reinforcement learning (DRL), progressively learning from simple to complex queries under a composite reward. During inference, STaR performs trajectory-level uncertainty quantification by integrating token-level confidence and answer consistency, enabling selection of more credible reasoning paths. Extensive experiments on benchmarks demonstrate that STaR achieves superior performance and enhanced reasoning stability. Moreover, strong generalization over out-of-domain datasets further demonstrates STaR's potential as a reliable and cognitively inspired solution for table reasoning with LLMs.

</details>


### [19] [AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery](https://arxiv.org/abs/2511.11257)
*Yuqi Yin,Yibo Fu,Siyuan Wang,Peng Sun,Hongyu Wang,Xiaohui Wang,Lei Zheng,Zhiyong Li,Zhirong Liu,Jianji Wang,Zhaoxi Sun*

Main category: cs.AI

TL;DR: AIonopedia是首个基于大语言模型的离子液体发现智能体，通过多模态领域基础模型实现准确性质预测和分层搜索架构，在真实湿实验验证中表现出优异的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 离子液体发现面临数据有限、模型准确性差和工作流程碎片化等关键挑战，需要开发更有效的AI驱动发现方法。

Method: 构建了基于大语言模型的多模态领域基础模型，采用分层搜索架构进行分子筛选和设计，并在新构建的全面离子液体数据集上进行训练和评估。

Result: 模型在文献报道系统上表现出有效的离子液体改性能力，真实湿实验验证证实了其在具有挑战性的分布外任务上的优异泛化能力。

Conclusion: AIonopedia能够加速真实世界的离子液体发现，展示了基于大语言模型的智能体在材料科学中的实际应用价值。

Abstract: The discovery of novel Ionic Liquids (ILs) is hindered by critical challenges in property prediction, including limited data, poor model accuracy, and fragmented workflows. Leveraging the power of Large Language Models (LLMs), we introduce AIonopedia, to the best of our knowledge, the first LLM agent for IL discovery. Powered by an LLM-augmented multimodal domain foundation model for ILs, AIonopedia enables accurate property predictions and incorporates a hierarchical search architecture for molecular screening and design. Trained and evaluated on a newly curated and comprehensive IL dataset, our model delivers superior performance. Complementing these results, evaluations on literature-reported systems indicate that the agent can perform effective IL modification. Moving beyond offline tests, the practical efficacy was further confirmed through real-world wet-lab validation, in which the agent demonstrated exceptional generalization capabilities on challenging out-of-distribution tasks, underscoring its ability to accelerate real-world IL discovery.

</details>


### [20] [A Workflow for Full Traceability of AI Decisions](https://arxiv.org/abs/2511.11275)
*Julius Wenzel,Syeda Umaima Alam,Andreas Schmidt,Hanwei Zhang,Holger Hermanns*

Main category: cs.AI

TL;DR: 本文提出了一种通过强制记录AI训练和推理过程中每个组件的方法，来解决AI决策可追溯性问题，并展示了首个支持生成防篡改、可验证和详尽AI决策追踪的工作流程。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在高风险决策中的应用增加，存在决策可能侵犯人类福祉或基本人权的风险。当前AI系统在决策过程文档化方面做得很少，这阻碍了追溯决策来源的能力，而这是重建责任链的前提条件。

Method: 采用DBOM概念扩展，利用机密计算技术构建有效运行的工作流程，强制记录训练或推理过程中每个组件的文档。

Result: 开发了一个能够区分有毒和可食用蘑菇的应用程序作为示例，展示了工作流程的内部运作机制，支持生成防篡改、可验证和详尽的AI决策追踪。

Conclusion: 该方法为解决AI决策可追溯性问题提供了一种激进但实用的方法，为在法庭上确定AI决策的因果关系提供了必要的文档支持。

Abstract: An ever increasing number of high-stake decisions are made or assisted by automated systems employing brittle artificial intelligence technology. There is a substantial risk that some of these decision induce harm to people, by infringing their well-being or their fundamental human rights. The state-of-the-art in AI systems makes little effort with respect to appropriate documentation of the decision process. This obstructs the ability to trace what went into a decision, which in turn is a prerequisite to any attempt of reconstructing a responsibility chain. Specifically, such traceability is linked to a documentation that will stand up in court when determining the cause of some AI-based decision that inadvertently or intentionally violates the law.
  This paper takes a radical, yet practical, approach to this problem, by enforcing the documentation of each and every component that goes into the training or inference of an automated decision. As such, it presents the first running workflow supporting the generation of tamper-proof, verifiable and exhaustive traces of AI decisions. In doing so, we expand the DBOM concept into an effective running workflow leveraging confidential computing technology. We demonstrate the inner workings of the workflow in the development of an app to tell poisonous and edible mushrooms apart, meant as a playful example of high-stake decision support.

</details>


### [21] [Can You Tell the Difference? Contrastive Explanations for ABox Entailments](https://arxiv.org/abs/2511.11281)
*Patrick Koopmann,Yasir Mahmood,Axel-Cyrille Ngonga Ngomo,Balram Tiwari*

Main category: cs.AI

TL;DR: 本文提出了对比ABox解释的概念，用于回答"为什么a是C的实例而b不是"这类问题。相比单独解释正蕴含或缺失蕴含的方法，对比解释同时考虑两者，能够聚焦于a和b之间的相关共性和差异。


<details>
  <summary>Details</summary>
Motivation: 现有的解释方法要么只解释正蕴含（为什么C(a)被知识库蕴含），要么只解释缺失蕴含（为什么C(b)不被蕴含），缺乏同时考虑两者的对比视角。对比解释能够更好地揭示个体之间的相关差异。

Method: 为描述逻辑本体论中的ABox推理开发了对比解释的适当概念，分析了不同变体在不同最优性标准下的计算复杂性，涵盖了轻量级和更表达性的描述逻辑。实现了一个计算对比解释变体的方法，并在现实知识库的生成问题上进行了评估。

Result: 开发了对比ABox解释的形式化定义，分析了不同描述逻辑下对比解释的计算复杂性，实现了首个计算对比解释的方法，并在实际知识库上进行了实验验证。

Conclusion: 对比ABox解释提供了一种新的解释框架，能够同时考虑正蕴含和缺失蕴含，更有效地揭示个体间的相关差异，为知识库推理提供了更丰富的解释能力。

Abstract: We introduce the notion of contrastive ABox explanations to answer questions of the type "Why is a an instance of C, but b is not?". While there are various approaches for explaining positive entailments (why is C(a) entailed by the knowledge base) as well as missing entailments (why is C(b) not entailed) in isolation, contrastive explanations consider both at the same time, which allows them to focus on the relevant commonalities and differences between a and b. We develop an appropriate notion of contrastive explanations for the special case of ABox reasoning with description logic ontologies, and analyze the computational complexity for different variants under different optimality criteria, considering lightweight as well as more expressive description logics. We implemented a first method for computing one variant of contrastive explanations, and evaluated it on generated problems for realistic knowledge bases.

</details>


### [22] [RLSLM: A Hybrid Reinforcement Learning Framework Aligning Rule-Based Social Locomotion Model with Human Social Norms](https://arxiv.org/abs/2511.11323)
*Yitian Kou,Yihe Gu,Chen Zhou,DanDan Zhu,Shuguang Kuai*

Main category: cs.AI

TL;DR: RLSLM是一个混合强化学习框架，将基于规则的社交运动模型集成到强化学习的奖励函数中，用于实现社会感知的导航策略。


<details>
  <summary>Details</summary>
Motivation: 解决规则方法缺乏泛化性和数据驱动方法不透明、难以与人类直觉对齐的问题，桥接认知科学与机器学习。

Method: 提出RLSLM框架，将基于经验行为实验的社交运动模型集成到强化学习奖励函数中，生成方向敏感的社交舒适场，联合优化机械能和社会舒适度。

Result: 在沉浸式VR人机交互实验中，RLSLM在用户体验方面优于最先进的基于规则模型，消融和敏感性分析显示模型可解释性显著提升。

Conclusion: 这项工作提出了一个可扩展的、以人为中心的方法论，有效整合认知科学和机器学习，用于现实世界的社交导航。

Abstract: Navigating human-populated environments without causing discomfort is a critical capability for socially-aware agents. While rule-based approaches offer interpretability through predefined psychological principles, they often lack generalizability and flexibility. Conversely, data-driven methods can learn complex behaviors from large-scale datasets, but are typically inefficient, opaque, and difficult to align with human intuitions. To bridge this gap, we propose RLSLM, a hybrid Reinforcement Learning framework that integrates a rule-based Social Locomotion Model, grounded in empirical behavioral experiments, into the reward function of a reinforcement learning framework. The social locomotion model generates an orientation-sensitive social comfort field that quantifies human comfort across space, enabling socially aligned navigation policies with minimal training. RLSLM then jointly optimizes mechanical energy and social comfort, allowing agents to avoid intrusions into personal or group space. A human-agent interaction experiment using an immersive VR-based setup demonstrates that RLSLM outperforms state-of-the-art rule-based models in user experience. Ablation and sensitivity analyses further show the model's significantly improved interpretability over conventional data-driven methods. This work presents a scalable, human-centered methodology that effectively integrates cognitive science and machine learning for real-world social navigation.

</details>


### [23] [Robust and Efficient Communication in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.11393)
*Zejiao Liu,Yi Li,Jiali Wang,Junqi Tu,Yitian Hong,Fangfei Li,Yang Liu,Toshiharu Sugawara,Yang Tang*

Main category: cs.AI

TL;DR: 本调查系统回顾了在多智能体强化学习中，针对现实约束（如消息扰动、传输延迟和有限带宽）的鲁棒高效通信策略的最新进展，重点关注自动驾驶、分布式SLAM和联邦学习三个应用领域，并提出了统一设计通信、学习和鲁棒性的未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有MARL方法大多假设通信是瞬时、可靠且带宽无限的，但这些条件在现实部署中很少满足，因此需要研究在现实约束下的鲁棒高效通信策略。

Method: 系统调查和综述了近年来在MARL中针对消息扰动、传输延迟和有限带宽等现实约束的通信策略研究进展。

Result: 识别了低延迟可靠性、带宽密集型数据共享和通信隐私权衡等核心挑战，并聚焦于协同自动驾驶、分布式SLAM和联邦学习三个应用领域。

Conclusion: 提出了统一设计通信、学习和鲁棒性的方法，以弥合理论MARL模型与实际实现之间的差距，并指出了关键开放挑战和未来研究方向。

Abstract: Multi-agent reinforcement learning (MARL) has made significant strides in enabling coordinated behaviors among autonomous agents. However, most existing approaches assume that communication is instantaneous, reliable, and has unlimited bandwidth; these conditions are rarely met in real-world deployments. This survey systematically reviews recent advances in robust and efficient communication strategies for MARL under realistic constraints, including message perturbations, transmission delays, and limited bandwidth. Furthermore, because the challenges of low-latency reliability, bandwidth-intensive data sharing, and communication-privacy trade-offs are central to practical MARL systems, we focus on three applications involving cooperative autonomous driving, distributed simultaneous localization and mapping, and federated learning. Finally, we identify key open challenges and future research directions, advocating a unified approach that co-designs communication, learning, and robustness to bridge the gap between theoretical MARL models and practical implementations.

</details>


### [24] [CURENet: Combining Unified Representations for Efficient Chronic Disease Prediction](https://arxiv.org/abs/2511.11423)
*Cong-Tinh Dao,Nguyen Minh Thao Phan,Jun-En Ding,Chenwei Wu,David Restrepo,Dongsheng Luo,Fanyi Zhao,Chun-Chieh Liao,Wen-Chih Peng,Chi-Te Wang,Pei-Fu Chen,Ling Chen,Xinglong Ju,Feng Liu,Fang-Ming Hung*

Main category: cs.AI

TL;DR: CURENet是一个多模态模型，通过整合非结构化临床笔记、实验室测试和患者时间序列数据，利用LLM处理临床文本和文本实验室测试，以及使用Transformer编码器处理纵向序列就诊数据，来预测慢性疾病。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录包含多种数据类型，但现有预测模型未能充分捕捉多模态数据之间的交互、冗余和时间模式，往往只关注单一数据类型或忽略这些复杂性。

Method: CURENet结合大型语言模型处理临床文本和文本实验室测试，使用Transformer编码器处理纵向序列就诊数据，整合多种临床数据形式。

Result: 在MIMIC-III和FEMH数据集上评估，CURENet在多标签框架中预测前10种慢性疾病的准确率超过94%。

Conclusion: 多模态EHR整合有潜力增强临床决策制定并改善患者结果。

Abstract: Electronic health records (EHRs) are designed to synthesize diverse data types, including unstructured clinical notes, structured lab tests, and time-series visit data. Physicians draw on these multimodal and temporal sources of EHR data to form a comprehensive view of a patient's health, which is crucial for informed therapeutic decision-making. Yet, most predictive models fail to fully capture the interactions, redundancies, and temporal patterns across multiple data modalities, often focusing on a single data type or overlooking these complexities. In this paper, we present CURENet, a multimodal model (Combining Unified Representations for Efficient chronic disease prediction) that integrates unstructured clinical notes, lab tests, and patients' time-series data by utilizing large language models (LLMs) for clinical text processing and textual lab tests, as well as transformer encoders for longitudinal sequential visits. CURENet has been capable of capturing the intricate interaction between different forms of clinical data and creating a more reliable predictive model for chronic illnesses. We evaluated CURENet using the public MIMIC-III and private FEMH datasets, where it achieved over 94\% accuracy in predicting the top 10 chronic conditions in a multi-label framework. Our findings highlight the potential of multimodal EHR integration to enhance clinical decision-making and improve patient outcomes.

</details>


### [25] [Experience-Guided Adaptation of Inference-Time Reasoning Strategies](https://arxiv.org/abs/2511.11519)
*Adam Stein,Matthew Trager,Benjamin Bowman,Michael Kleinman,Aditya Chattopadhyay,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: EGuR是一个在推理时动态生成完整计算策略的AI系统，能够根据积累的经验自适应调整LLM调用、工具使用、采样参数和控制逻辑，相比现有方法在多个基准测试中显著提升性能并大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在推理时只能通过修改文本输入来引导，无法灵活调整采样参数、工具配置或系统架构，而更灵活的系统需要离线优化且部署后无法动态调整。

Method: 使用基于LLM的元策略生成完整计算策略，包含两个组件：Guide基于当前问题和历史经验生成候选策略，Consolidator整合执行反馈来改进未来策略生成。

Result: 在五个挑战性基准测试中，EGuR比最强基线准确率提升高达14%，计算成本降低高达111倍，且随着经验积累性能持续提升。

Conclusion: EGuR证明了在推理时动态生成完整策略的可行性，为AI系统实现真正的自适应问题解决提供了新途径。

Abstract: Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [26] [LT-Soups: Bridging Head and Tail Classes via Subsampled Model Soups](https://arxiv.org/abs/2511.10683)
*Masih Aminbeidokhti,Subhankar Roy,Eric Granger,Elisa Ricci,Marco Pedersoli*

Main category: cs.LG

TL;DR: 本文提出LT-Soups框架解决长尾数据分布下参数高效微调方法在头尾类性能间的权衡问题，通过两阶段模型融合方法在多个基准数据集上实现更好的性能平衡。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集通常呈现长尾分布，现有参数高效微调方法虽然能保持尾类性能，但会牺牲头类准确率。头尾比例作为关键但被忽视的因素影响这种权衡。

Method: 提出LT-Soups两阶段模型融合框架：第一阶段在平衡子集上微调模型并平均以减少头类偏差；第二阶段仅在全数据集上微调分类器以恢复头类准确率。

Result: 在六个基准数据集上的实验表明，LT-Soups在广泛的失衡机制下相比PEFT和传统模型融合方法实现了更优的性能权衡。

Conclusion: LT-Soups能够有效解决长尾数据分布下头尾类性能的权衡问题，在不同失衡机制下都表现出色，为长尾学习提供了新的解决方案。

Abstract: Real-world datasets typically exhibit long-tailed (LT) distributions, where a few head classes dominate and many tail classes are severely underrepresented. While recent work shows that parameter-efficient fine-tuning (PEFT) methods like LoRA and AdaptFormer preserve tail-class performance on foundation models such as CLIP, we find that they do so at the cost of head-class accuracy. We identify the head-tail ratio, the proportion of head to tail classes, as a crucial but overlooked factor influencing this trade-off. Through controlled experiments on CIFAR100 with varying imbalance ratio ($ρ$) and head-tail ratio ($η$), we show that PEFT excels in tail-heavy scenarios but degrades in more balanced and head-heavy distributions. To overcome these limitations, we propose LT-Soups, a two-stage model soups framework designed to generalize across diverse LT regimes. In the first stage, LT-Soups averages models fine-tuned on balanced subsets to reduce head-class bias; in the second, it fine-tunes only the classifier on the full dataset to restore head-class accuracy. Experiments across six benchmark datasets show that LT-Soups achieves superior trade-offs compared to both PEFT and traditional model soups across a wide range of imbalance regimes.

</details>


### [27] [Differentiable Sparse Identification of Lagrangian Dynamics](https://arxiv.org/abs/2511.10706)
*Zitong Zhang,Hao Sun*

Main category: cs.LG

TL;DR: 提出了一种新颖的可微分稀疏识别框架，通过三次B样条近似、鲁棒方程发现机制和递归导数计算方案，解决了拉格朗日系统识别中的噪声敏感性和数据限制问题。


<details>
  <summary>Details</summary>
Motivation: 现有拉格朗日识别方法受测量噪声和数据可用性的显著影响，需要开发更鲁棒的系统识别方法来解决复杂机械系统中的这些限制。

Method: 集成三次B样条近似到拉格朗日系统识别中，开发鲁棒方程发现机制结合物理约束，以及基于B样条基函数的递归导数计算方案。

Result: 该方法在复杂机械系统中表现出优越性能，相比基线方法能够从噪声数据中更准确可靠地提取物理定律。

Conclusion: 提出的可微分稀疏识别框架有效解决了拉格朗日系统识别中的噪声敏感性和数据限制问题，为复杂机械系统的物理定律发现提供了更可靠的方法。

Abstract: Data-driven discovery of governing equations from data remains a fundamental challenge in nonlinear dynamics. Although sparse regression techniques have advanced system identification, they struggle with rational functions and noise sensitivity in complex mechanical systems. The Lagrangian formalism offers a promising alternative, as it typically avoids rational expressions and provides a more concise representation of system dynamics. However, existing Lagrangian identification methods are significantly affected by measurement noise and limited data availability. This paper presents a novel differentiable sparse identification framework that addresses these limitations through three key contributions: (1) the first integration of cubic B-Spline approximation into Lagrangian system identification, enabling accurate representation of complex nonlinearities, (2) a robust equation discovery mechanism that effectively utilizes measurements while incorporating known physical constraints, (3) a recursive derivative computation scheme based on B-spline basis functions, effectively constraining higher-order derivatives and reducing noise sensitivity on second-order dynamical systems. The proposed method demonstrates superior performance and enables more accurate and reliable extraction of physical laws from noisy data, particularly in complex mechanical systems compared to baseline methods.

</details>


### [28] [Bias-Restrained Prefix Representation Finetuning for Mathematical Reasoning](https://arxiv.org/abs/2511.10707)
*Sirui Liang,Pengfei Cao,Jian Zhao,Cong Huang,Jun Zhao,Kang Liu*

Main category: cs.LG

TL;DR: 本文提出了BREP ReFT方法，通过截断训练数据优化推理前缀生成、干预早期推理阶段防止错误累积、约束干预向量幅度避免干扰数值编码，显著提升了表示微调方法在数学推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 表示微调方法在数学推理任务上表现不佳，主要原因是早期推理阶段无法生成有效推理前缀，以及干扰数值编码导致错误在思维链阶段累积。

Method: 提出BREP ReFT方法：1）截断训练数据优化初始推理前缀生成；2）干预早期推理阶段防止错误累积；3）约束干预向量幅度避免干扰数值编码。

Result: 在多种模型架构上的广泛实验表明，BREP在数学推理任务上优于标准ReFT和基于权重的PEFT方法，具有更好的有效性、效率和鲁棒泛化能力。

Conclusion: BREP ReFT通过针对性地解决ReFT在数学推理中的关键问题，显著提升了表示微调方法在数学推理任务上的性能表现。

Abstract: Parameter-Efficient finetuning (PEFT) enhances model performance on downstream tasks by updating a minimal subset of parameters. Representation finetuning (ReFT) methods further improve efficiency by freezing model weights and optimizing internal representations with fewer parameters than PEFT, outperforming PEFT on several tasks. However, ReFT exhibits a significant performance decline on mathematical reasoning tasks. To address this problem, the paper demonstrates that ReFT's poor performance on mathematical tasks primarily stems from its struggle to generate effective reasoning prefixes during the early inference phase. Moreover, ReFT disturbs the numerical encoding and the error accumulats during the CoT stage. Based on these observations, this paper proposes Bias-REstrained Prefix Representation FineTuning (BREP ReFT), which enhances ReFT's mathematical reasoning capability by truncating training data to optimize the generation of initial reasoning prefixes, intervening on the early inference stage to prevent error accumulation, and constraining the intervention vectors' magnitude to avoid disturbing numerical encoding. Extensive experiments across diverse model architectures demonstrate BREP's superior effectiveness, efficiency, and robust generalization capability, outperforming both standard ReFT and weight-based PEFT methods on the task of mathematical reasoning. The source code is available at https://github.com/LiangThree/BREP.

</details>


### [29] [Towards Uncertainty Quantification in Generative Model Learning](https://arxiv.org/abs/2511.10710)
*Giorgio Morales,Frederic Jurie,Jalal Fadili*

Main category: cs.LG

TL;DR: 本文提出了生成模型学习中不确定性量化的问题，讨论了使用集成精度-召回曲线等研究方向，并在合成数据集上验证了聚合精度-召回曲线在捕捉模型近似不确定性方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 生成模型在各个领域日益普及，但其可靠性存在根本性担忧。当前评估方法主要关注学习分布与目标分布之间的接近程度，而忽略了这些测量中固有的不确定性。

Method: 形式化生成模型学习中的不确定性量化问题，提出使用集成精度-召回曲线等方法来量化模型近似的不确定性。

Result: 在合成数据集上的初步实验表明，聚合精度-召回曲线能够有效捕捉模型近似不确定性，使基于不确定性特征对不同模型架构进行系统比较成为可能。

Conclusion: 不确定性量化是生成模型评估中一个关键但被忽视的方面，集成精度-召回曲线等方法为系统评估和比较不同模型架构的不确定性特征提供了有效途径。

Abstract: While generative models have become increasingly prevalent across various domains, fundamental concerns regarding their reliability persist. A crucial yet understudied aspect of these models is the uncertainty quantification surrounding their distribution approximation capabilities. Current evaluation methodologies focus predominantly on measuring the closeness between the learned and the target distributions, neglecting the inherent uncertainty in these measurements. In this position paper, we formalize the problem of uncertainty quantification in generative model learning. We discuss potential research directions, including the use of ensemble-based precision-recall curves. Our preliminary experiments on synthetic datasets demonstrate the effectiveness of aggregated precision-recall curves in capturing model approximation uncertainty, enabling systematic comparison among different model architectures based on their uncertainty characteristics.

</details>


### [30] [Fast Neural Tangent Kernel Alignment, Norm and Effective Rank via Trace Estimation](https://arxiv.org/abs/2511.10796)
*James Hazelden*

Main category: cs.LG

TL;DR: 本文提出了一种基于迹估计的矩阵自由方法来快速计算神经正切核(NTK)的迹、Frobenius范数、有效秩和对齐度，无需计算完整的NTK矩阵。


<details>
  <summary>Details</summary>
Motivation: 计算完整的NTK矩阵对于循环架构等模型通常不可行，需要一种更高效的NTK分析方法。

Method: 使用Hutch++迹估计器和单边估计器(仅需前向或反向自动微分)，通过随机化方法快速计算NTK的迹和相关度量。

Result: 矩阵自由随机化方法可以实现多个数量级的加速，特别是在低样本情况下，单边估计器表现优于Hutch++。

Conclusion: 矩阵自由随机化方法能够显著加速NTK的分析和应用，为大规模模型提供可行的NTK计算方案。

Abstract: The Neural Tangent Kernel (NTK) characterizes how a model's state evolves over Gradient Descent. Computing the full NTK matrix is often infeasible, especially for recurrent architectures. Here, we introduce a matrix-free perspective, using trace estimation to rapidly analyze the empirical, finite-width NTK. This enables fast computation of the NTK's trace, Frobenius norm, effective rank, and alignment. We provide numerical recipes based on the Hutch++ trace estimator with provably fast convergence guarantees. In addition, we show that, due to the structure of the NTK, one can compute the trace using only forward- or reverse-mode automatic differentiation, not requiring both modes. We show these so-called one-sided estimators can outperform Hutch++ in the low-sample regime, especially when the gap between the model state and parameter count is large. In total, our results demonstrate that matrix-free randomized approaches can yield speedups of many orders of magnitude, leading to faster analysis and applications of the NTK.

</details>


### [31] [Near-optimal Linear Predictive Clustering in Non-separable Spaces via Mixed Integer Programming and Quadratic Pseudo-Boolean Reductions](https://arxiv.org/abs/2511.10809)
*Jiazhou Liang,Hassan Khurram,Scott Sanner*

Main category: cs.LG

TL;DR: 本文提出了两种改进线性预测聚类（LPC）全局优化的新方法，通过利用可分性理论特性推导出具有可证明误差界的近似最优解，显著降低了MIP公式的复杂性并提高了可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的LPC方法存在局限性：贪婪优化方法缺乏全局最优性，在非可分设置中表现不佳；而基于MIP的方法虽然能保证全局最优性，但可扩展性差。

Method: 1. 利用可分性理论特性推导出具有可证明误差界的近似最优解
2. 将LPC近似为二次伪布尔优化（QPBO）问题
3. 显著降低MIP公式的复杂性

Result: 在合成和真实数据集上的比较分析表明，新方法始终能获得近似最优解，回归误差显著低于贪婪优化方法，同时比现有MIP公式具有更好的可扩展性。

Conclusion: 提出的两种新方法在保持近似最优性的同时，显著提高了LPC全局优化的效率和可扩展性，在可分和非可分设置下都表现良好。

Abstract: Linear Predictive Clustering (LPC) partitions samples based on shared linear relationships between feature and target variables, with numerous applications including marketing, medicine, and education. Greedy optimization methods, commonly used for LPC, alternate between clustering and linear regression but lack global optimality. While effective for separable clusters, they struggle in non-separable settings where clusters overlap in feature space. In an alternative constrained optimization paradigm, Bertsimas and Shioda (2007) formulated LPC as a Mixed-Integer Program (MIP), ensuring global optimality regardless of separability but suffering from poor scalability. This work builds on the constrained optimization paradigm to introduce two novel approaches that improve the efficiency of global optimization for LPC. By leveraging key theoretical properties of separability, we derive near-optimal approximations with provable error bounds, significantly reducing the MIP formulation's complexity and improving scalability. Additionally, we can further approximate LPC as a Quadratic Pseudo-Boolean Optimization (QPBO) problem, achieving substantial computational improvements in some settings. Comparative analyses on synthetic and real-world datasets demonstrate that our methods consistently achieve near-optimal solutions with substantially lower regression errors than greedy optimization while exhibiting superior scalability over existing MIP formulations.

</details>


### [32] [Transformers know more than they can tell -- Learning the Collatz sequence](https://arxiv.org/abs/2511.10811)
*François Charton,Ashvni Narayanan*

Main category: cs.LG

TL;DR: 本文研究了Transformer模型预测Collatz序列长步骤的能力，发现模型准确率随编码基数变化（最高99.7%，最低25%），所有模型都学习到按2^p模数分类的输入模式，错误主要源于循环长度估计错误而非幻觉。


<details>
  <summary>Details</summary>
Motivation: 使用数学问题作为理解、解释和改进语言模型的工具，探索模型学习复杂算术函数的能力和机制。

Method: 通过不同基数编码输入输出，训练Transformer模型预测Collatz序列的长步骤，分析模型学习模式和错误类型。

Result: 模型准确率随编码基数变化显著（基数24和32达99.7%，基数11和3仅37%和25%），所有模型都学习到按2^p模数分类的输入模式，错误主要源于循环长度估计错误（>90%），几乎不发生幻觉。

Conclusion: 学习复杂算术函数的困难在于理解计算的控制结构（循环长度），使用数学问题作为分析工具可以深入理解语言模型的学习机制，这种方法可广泛应用于其他问题。

Abstract: We investigate transformer prediction of long Collatz steps, a complex arithmetic function that maps odd integers to their distant successors in the Collatz sequence ( $u_{n+1}=u_n/2$ if $u_n$ is even, $u_{n+1}=(3u_n+1)/2$ if $u_n$ is odd). Model accuracy varies with the base used to encode input and output. It can be as high as $99.7\%$ for bases $24$ and $32$, and as low as $37$ and $25\%$ for bases $11$ and $3$. Yet, all models, no matter the base, follow a common learning pattern. As training proceeds, they learn a sequence of classes of inputs that share the same residual modulo $2^p$. Models achieve near-perfect accuracy on these classes, and less than $1\%$ for all other inputs. This maps to a mathematical property of Collatz sequences: the length of the loops involved in the computation of a long Collatz step can be deduced from the binary representation of its input. The learning pattern reflects the model learning to predict inputs associated with increasing loop lengths. An analysis of failure cases reveals that almost all model errors follow predictable patterns. Hallucination, a common feature of large language models, almost never happens. In over $90\%$ of failures, the model performs the correct calculation, but wrongly estimates loop lengths. Our observations give a full account of the algorithms learned by the models. They suggest that the difficulty of learning such complex arithmetic function lies in figuring the control structure of the computation -- the length of the loops. We believe that the approach outlined here, using mathematical problems as tools for understanding, explaining, and perhaps improving language models, can be applied to a broad range of problems and bear fruitful results.

</details>


### [33] [Towards Universal Neural Operators through Multiphysics Pretraining](https://arxiv.org/abs/2511.10829)
*Mikhail Masliaev,Dmitry Gusarov,Ilya Markov,Alexander Hvatov*

Main category: cs.LG

TL;DR: 该研究评估了基于Transformer的神经算子在更通用的迁移学习设置中的性能，证明了先进神经算子架构能够有效在偏微分方程问题间迁移知识。


<details>
  <summary>Details</summary>
Motivation: 尽管神经算子广泛应用于数据驱动的物理模拟，但其训练计算成本高昂。最近的研究通过下游学习来解决这个问题，即在更简单问题上预训练的模型在更复杂问题上进行微调。

Method: 研究在更通用的迁移学习设置中评估基于Transformer的神经算子，测试其在多种PDE问题上的性能，包括对未见参数的推断、新变量的整合以及从多方程数据集的迁移。

Result: 研究结果表明，先进的神经算子架构能够有效在PDE问题间迁移知识。

Conclusion: 基于Transformer的神经算子在通用迁移学习设置中表现出色，能够有效解决PDE问题间的知识迁移问题。

Abstract: Although neural operators are widely used in data-driven physical simulations, their training remains computationally expensive. Recent advances address this issue via downstream learning, where a model pretrained on simpler problems is fine-tuned on more complex ones. In this research, we investigate transformer-based neural operators, which have previously been applied only to specific problems, in a more general transfer learning setting. We evaluate their performance across diverse PDE problems, including extrapolation to unseen parameters, incorporation of new variables, and transfer from multi-equation datasets. Our results demonstrate that advanced neural operator architectures can effectively transfer knowledge across PDE problems.

</details>


### [34] [Benchmarking Quantum Kernels Across Diverse and Complex Data](https://arxiv.org/abs/2511.10831)
*Yuhan Jiang,Matthew Otten*

Main category: cs.LG

TL;DR: 本文提出了一个变分量子核框架，在8个高维真实世界数据集上展示了量子核方法相对于经典RBF核的性能优势。


<details>
  <summary>Details</summary>
Motivation: 量子核方法在实际应用中仍缺乏验证，现有研究多局限于低维或合成数据集，无法全面评估其潜力。

Method: 开发了资源高效的变分量子核框架，采用参数缩放技术加速收敛，并在8个高维真实世界数据集上进行全面基准测试。

Result: 经典模拟结果显示，所提出的量子核在性能上明显优于标准经典核（如RBF核）。

Conclusion: 适当设计的量子核可以作为多功能高性能工具，为现实世界机器学习中的量子增强应用奠定基础，但需进一步研究评估实际量子优势。

Abstract: Quantum kernel methods are a promising branch of quantum machine learning, yet their practical advantage on diverse, high-dimensional, real-world data remains unverified. Current research has largely been limited to low-dimensional or synthetic datasets, preventing a thorough evaluation of their potential. To address this gap, we developed a variational quantum kernel framework utilizing resource-efficient ansätze for complex classification tasks and introduced a parameter scaling technique to accelerate convergence. We conducted a comprehensive benchmark of this framework on eight challenging, real world and high-dimensional datasets covering tabular, image, time series, and graph data. Our classically simulated results show that the proposed quantum kernel demonstrated a clear performance advantage over standard classical kernels, such as the radial basis function (RBF) kernel. This work demonstrates that properly designed quantum kernels can function as versatile, high-performance tools, laying a foundation for quantum-enhanced applications in real-world machine learning. Further research is needed to fully assess the practical quantum advantage.

</details>


### [35] [SURFACEBENCH: Can Self-Evolving LLMs Find the Equations of 3D Scientific Surfaces?](https://arxiv.org/abs/2511.10833)
*Sanchit Kabra,Shobhnik Kriplani,Parshin Shojaee,Chandan K. Reddy*

Main category: cs.LG

TL;DR: SurfaceBench是一个用于符号曲面发现的综合基准测试，包含183个任务，涵盖15种符号复杂度类别，支持显式、隐式和参数方程表示形式，并采用几何感知的评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有的符号回归方法主要关注标量函数，缺乏对曲面级结构的考虑，且评估指标依赖脆弱的字符串匹配，无法捕捉科学等价性。需要建立一个能够评估符号推理与几何重建能力的基准。

Method: 构建包含183个任务的SurfaceBench基准，涵盖15种符号复杂度类别，支持三种方程表示形式（显式、隐式、参数），每个任务包含真实方程、变量语义和合成的三维数据。

Result: 实验表明，最先进的框架在特定类别上可能成功，但难以在不同表示类型和曲面复杂度之间泛化。SurfaceBench为组合泛化、数据驱动科学归纳和几何感知推理提供了具有挑战性的测试平台。

Conclusion: SurfaceBench建立了连接符号推理与几何重建的挑战性诊断测试平台，能够系统评估在组合泛化、数据驱动科学归纳和几何感知推理方面的进展。

Abstract: Equation discovery from data is a core challenge in machine learning for science, requiring the recovery of concise symbolic expressions that govern complex physical and geometric phenomena. Recent approaches with large language models (LLMs) show promise in symbolic regression, but their success often hinges on memorized formulas or overly simplified functional forms. Existing benchmarks exacerbate this limitation: they focus on scalar functions, ignore domain grounding, and rely on brittle string-matching based metrics that fail to capture scientific equivalence. We introduce SurfaceBench, first comprehensive benchmark for symbolic surface discovery. SurfaceBench comprises 183 tasks across 15 categories of symbolic complexity, spanning explicit, implicit, and parametric equation representation forms. Each task includes ground-truth equations, variable semantics, and synthetically sampled three dimensional data. Unlike prior SR datasets, our tasks reflect surface-level structure, resist LLM memorization through novel symbolic compositions, and are grounded in scientific domains such as fluid dynamics, robotics, electromagnetics, and geometry. To evaluate equation discovery quality, we pair symbolic checks with geometry-aware metrics such as Chamfer and Hausdorff distances, capturing both algebraic fidelity and spatial reconstruction accuracy. Our experiments reveal that state-of-the-art frameworks, while occasionally successful on specific families, struggle to generalize across representation types and surface complexities. SurfaceBench thus establishes a challenging and diagnostic testbed that bridges symbolic reasoning with geometric reconstruction, enabling principled benchmarking of progress in compositional generalization, data-driven scientific induction, and geometry-aware reasoning with LLMs. We release the code here: https://github.com/Sanchit-404/surfacebench

</details>


### [36] [The Map of Misbelief: Tracing Intrinsic and Extrinsic Hallucinations Through Attention Patterns](https://arxiv.org/abs/2511.10837)
*Elyes Hajji,Aymen Bouguerra,Fabio Arnez*

Main category: cs.LG

TL;DR: 本文提出了一个区分外在和内在幻觉的评估框架，并利用基于注意力的不确定性量化算法改进幻觉检测性能。研究发现采样方法对检测外在幻觉有效但内在幻觉效果差，而注意力聚合方法更适合检测内在幻觉。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在安全关键领域部署时容易产生幻觉，现有检测方法计算成本高且未区分幻觉类型，需要更有效的检测策略。

Method: 引入区分外在和内在幻觉的评估框架，利用基于注意力的不确定性量化算法，提出新颖的注意力聚合策略。

Result: 采样方法如语义熵能有效检测外在幻觉但内在幻觉效果差；注意力聚合方法更适合检测内在幻觉；注意力是量化模型不确定性的丰富信号。

Conclusion: 研究为根据幻觉性质调整检测策略提供了新方向，强调注意力作为量化模型不确定性的重要信号。

Abstract: Large Language Models (LLMs) are increasingly deployed in safety-critical domains, yet remain susceptible to hallucinations. While prior works have proposed confidence representation methods for hallucination detection, most of these approaches rely on computationally expensive sampling strategies and often disregard the distinction between hallucination types. In this work, we introduce a principled evaluation framework that differentiates between extrinsic and intrinsic hallucination categories and evaluates detection performance across a suite of curated benchmarks. In addition, we leverage a recent attention-based uncertainty quantification algorithm and propose novel attention aggregation strategies that improve both interpretability and hallucination detection performance. Our experimental findings reveal that sampling-based methods like Semantic Entropy are effective for detecting extrinsic hallucinations but generally fail on intrinsic ones. In contrast, our method, which aggregates attention over input tokens, is better suited for intrinsic hallucinations. These insights provide new directions for aligning detection strategies with the nature of hallucination and highlight attention as a rich signal for quantifying model uncertainty.

</details>


### [37] [FlowPath: Learning Data-Driven Manifolds with Invertible Flows for Robust Irregularly-sampled Time Series Classification](https://arxiv.org/abs/2511.10841)
*YongKyung Oh,Dong-Young Lim,Sungil Kim*

Main category: cs.LG

TL;DR: FlowPath提出了一种通过可逆神经流学习控制路径几何形状的新方法，用于处理稀疏和不规则采样的时间序列数据，相比固定插值方法在分类准确率上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有神经控制微分方程方法对控制路径的选择高度敏感，固定插值方案强加了简化的几何假设，往往不能准确表示底层数据流形，特别是在高缺失率情况下。

Method: FlowPath通过可逆神经流学习控制路径的几何形状，构建连续且数据自适应的流形，利用可逆性约束强制信息保持和良好行为的变换。

Result: 在18个基准数据集和真实案例研究中，FlowPath相比使用固定插值或不可逆架构的基线方法，在分类准确率上始终实现统计显著的改进。

Conclusion: 研究强调了不仅需要建模路径上的动态，还需要建模路径本身的几何形状，为从不规则时间序列中学习提供了鲁棒且可泛化的解决方案。

Abstract: Modeling continuous-time dynamics from sparse and irregularly-sampled time series remains a fundamental challenge. Neural controlled differential equations provide a principled framework for such tasks, yet their performance is highly sensitive to the choice of control path constructed from discrete observations. Existing methods commonly employ fixed interpolation schemes, which impose simplistic geometric assumptions that often misrepresent the underlying data manifold, particularly under high missingness. We propose FlowPath, a novel approach that learns the geometry of the control path via an invertible neural flow. Rather than merely connecting observations, FlowPath constructs a continuous and data-adaptive manifold, guided by invertibility constraints that enforce information-preserving and well-behaved transformations. This inductive bias distinguishes FlowPath from prior unconstrained learnable path models. Empirical evaluations on 18 benchmark datasets and a real-world case study demonstrate that FlowPath consistently achieves statistically significant improvements in classification accuracy over baselines using fixed interpolants or non-invertible architectures. These results highlight the importance of modeling not only the dynamics along the path but also the geometry of the path itself, offering a robust and generalizable solution for learning from irregular time series.

</details>


### [38] [Behaviour Policy Optimization: Provably Lower Variance Return Estimates for Off-Policy Reinforcement Learning](https://arxiv.org/abs/2511.10843)
*Alexander W. Goodall,Edwin Hamel-De le Court,Francesco Belardinelli*

Main category: cs.LG

TL;DR: 本文提出了一种利用行为策略收集离线数据来降低回报估计方差的方法，通过扩展两个策略梯度方法，在多种环境中实现了更好的样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 许多强化学习算法依赖回报估计进行策略改进，但高方差的回报估计会导致样本效率低下和训练不稳定。离线评估的最新研究表明，设计良好的行为策略可以收集离线数据以获得方差更低的回报估计。

Method: 将离线评估的关键见解扩展到在线强化学习设置中，使用单一行为策略收集数据用于策略改进，通过数学上合理的方式管理方差，并扩展了两个策略梯度方法。

Result: 在多样化环境中的实验表明，该方法相比传统方法具有更好的样本效率和性能表现。

Conclusion: 收集在线数据并不一定是方差最优的，通过精心设计的行为策略收集离线数据可以显著降低回报估计方差，从而提高强化学习算法的效率和稳定性。

Abstract: Many reinforcement learning algorithms, particularly those that rely on return estimates for policy improvement, can suffer from poor sample efficiency and training instability due to high-variance return estimates. In this paper we leverage new results from off-policy evaluation; it has recently been shown that well-designed behaviour policies can be used to collect off-policy data for provably lower variance return estimates. This result is surprising as it means collecting data on-policy is not variance optimal. We extend this key insight to the online reinforcement learning setting, where both policy evaluation and improvement are interleaved to learn optimal policies. Off-policy RL has been well studied (e.g., IMPALA), with correct and truncated importance weighted samples for de-biasing and managing variance appropriately. Generally these approaches are concerned with reconciling data collected from multiple workers in parallel, while the policy is updated asynchronously, mismatch between the workers and policy is corrected in a mathematically sound way. Here we consider only one worker - the behaviour policy, which is used to collect data for policy improvement, with provably lower variance return estimates. In our experiments we extend two policy-gradient methods with this regime, demonstrating better sample efficiency and performance over a diverse set of environments.

</details>


### [39] [STAMP: Spatial-Temporal Adapter with Multi-Head Pooling](https://arxiv.org/abs/2511.10848)
*Brad Shook,Abby Turner,Jieshi Chen,Michał Wiliński,Mononito Goswami,Jonathan Elmer,Artur Dubrawski*

Main category: cs.LG

TL;DR: 本文提出了一种名为STAMP的空间-时间适配器，利用通用时间序列基础模型（TSFM）的单变量嵌入，隐式建模EEG数据的时空特征，在EEG分类任务上达到与专用EEG基础模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对EEG专用基础模型（EEGFMs）与通用时间序列基础模型（TSFM）在EEG任务上的比较分析，需要探索如何有效利用通用TSFM处理EEG数据。

Method: 提出STAMP适配器，通过多头池化机制利用通用TSFM的单变量嵌入，隐式建模EEG数据的时空特征，该适配器参数量少且输入灵活。

Result: 在8个临床EEG分类基准数据集上的综合分析表明，STAMP性能与最先进的EEGFMs相当，并通过消融研究验证了其有效性。

Conclusion: STAMP适配器为使用通用TSFM建模EEG数据提供了轻量级且灵活的解决方案，支持轻松利用预训练TSFM处理EEG任务。

Abstract: Time series foundation models (TSFMs) pretrained on data from multiple domains have shown strong performance on diverse modeling tasks. Various efforts have been made to develop foundation models specific to electroencephalography (EEG) data, which records brain electrical activity as time series. However, no comparative analysis of EEG-specific foundation models (EEGFMs) versus general TSFMs has been performed on EEG-specific tasks. We introduce a novel Spatial-Temporal Adapter with Multi-Head Pooling (STAMP), which leverages univariate embeddings produced by a general TSFM, implicitly models spatial-temporal characteristics of EEG data, and achieves performance comparable to state-of-the-art EEGFMs. A comprehensive analysis is performed on 8 benchmark datasets of clinical tasks using EEG for classification, along with ablation studies. Our proposed adapter is lightweight in trainable parameters and flexible in the inputs it can accommodate, supporting easy modeling of EEG data using TSFMs.

</details>


### [40] [ExPairT-LLM: Exact Learning for LLM Code Selection by Pairwise Queries](https://arxiv.org/abs/2511.10855)
*Tom Yuviler,Dana Drachsler-Cohen*

Main category: cs.LG

TL;DR: ExPairT-LLM是一种用于代码选择的精确学习算法，通过向LLM提出成对成员资格和成对等价性查询来识别正确程序，在四个代码数据集上平均比现有最佳算法提升13.0%的pass@1成功率。


<details>
  <summary>Details</summary>
Motivation: 现有代码选择算法可能无法识别正确程序，因为它们可能错误识别非等价程序，或依赖LLM假设其总能正确确定每个输入输出。需要一种更鲁棒的方法来处理LLM的错误判断。

Method: 提出ExPairT-LLM算法，通过向LLM提出两种新型查询：成对成员资格和成对等价性查询。这些查询对LLM更简单，并通过锦标赛机制识别正确程序，对某些LLM错误具有鲁棒性。

Result: 在四个流行代码数据集上评估，ExPairT-LLM的pass@1（成功率）平均比现有最佳代码选择算法提升13.0%，最高提升27.1%。同时将执行复杂推理的LLM的pass@1提升24.0%。

Conclusion: ExPairT-LLM通过简化的查询类型和锦标赛机制，有效提高了代码选择的准确性，对LLM的错误具有更好的鲁棒性，显著提升了代码生成的成功率。

Abstract: Despite recent advances in LLMs, the task of code generation is still challenging. To cope, code selection algorithms select the best program from multiple programs generated by an LLM. However, existing algorithms can fail to identify the correct program, either because they can misidentify nonequivalent programs or because they rely on an LLM and assume it always correctly determines the output for every input. We present ExPairT-LLM, an exact learning algorithm for code selection that selects a program by posing to an LLM oracle two new types of queries: pairwise membership and pairwise equivalence. These queries are simpler for LLMs and enable ExPairT-LLM to identify the correct program through a tournament, which is robust to some LLM mistakes. We evaluate ExPairT-LLM on four popular code datasets. Its pass@1 (success rate) outperforms the state-of-the-art code selection algorithm on average by +13.0% and up to +27.1%. It also improves the pass@1 of LLMs performing complex reasoning by +24.0%.

</details>


### [41] [Private Zeroth-Order Optimization with Public Data](https://arxiv.org/abs/2511.10859)
*Xuchen Gong,Tian Li*

Main category: cs.LG

TL;DR: 本文提出了一种利用公共数据指导私有零阶优化算法梯度近似的方法（PAZO），在保持隐私的同时显著提高了效用，并在高隐私保护场景下优于一阶基线方法，同时提供高达16倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 当前一阶差分隐私机器学习算法（如DP-SGD）存在计算和内存成本高的问题，而现有的零阶方法虽然更容易实现隐私保护，但效用相对较低且应用领域有限。

Method: 提出PAZO框架，利用公共信息来指导和改进私有零阶算法的梯度近似，开发了一套具有最小开销的公共数据辅助零阶优化器。

Result: 在视觉和文本任务的预训练和微调设置中，PAZO实现了优越的隐私/效用权衡，在高度隐私保护场景下尤其优于最佳的一阶基线方法，同时提供高达16倍的运行速度提升。

Conclusion: PAZO框架通过利用公共数据有效提升了私有零阶优化算法的性能，在高隐私保护需求下具有显著优势，为差分隐私机器学习提供了更高效的解决方案。

Abstract: One of the major bottlenecks for deploying popular first-order differentially private (DP) machine learning algorithms (e.g., DP-SGD) lies in their high computation and memory cost, despite the existence of optimized implementations. Zeroth-order methods have promise in mitigating the overhead, as they leverage function evaluations to approximate the gradients, hence significantly easier to privatize. While recent works have explored zeroth-order approaches in both private and non-private settings, they still suffer from relatively low utilities compared with DP-SGD, and have only been evaluated in limited application domains. In this work, we propose to leverage public information to guide and improve gradient approximation of private zeroth-order algorithms. We explore a suite of public-data-assisted zeroth-order optimizers (PAZO) with minimal overhead. We provide theoretical analyses of the PAZO framework under an assumption of the similarity between public and private data. Empirically, we demonstrate that PAZO achieves superior privacy/utility tradeoffs across vision and text tasks in both pre-training and fine-tuning settings, outperforming the best first-order baselines (with public data) especially in highly private regimes, while offering up to $16\times$ runtime speedup.

</details>


### [42] [Go-UT-Bench: A Fine-Tuning Dataset for LLM-Based Unit Test Generation in Go](https://arxiv.org/abs/2511.10868)
*Yashshi Pipalani,Hritik Raj,Rajat Ghosh,Vaishnavi Bhargava,Debojyoti Dutta*

Main category: cs.LG

TL;DR: GO UT Bench是一个包含5264对代码和单元测试的基准数据集，用于解决代码LLM在训练数据不平衡问题，特别是在Golang等低资源语言中单元测试生成能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 训练数据不平衡是代码LLM面临的主要挑战，现有数据过度代表原始开源代码而低估了更广泛的软件工程任务，尤其是在Golang等低资源语言中。这导致模型在代码自动补全方面表现出色，但在单元测试生成等实际开发工作流程中表现不佳。

Method: 引入GO UT Bench基准数据集，包含5264对代码和单元测试，来自10个不同领域的许可Golang仓库。在两个LLM家族（专家混合和密集解码器）上评估其作为微调数据集的有效性。

Result: 微调后的模型在超过75%的基准任务上优于其基础对应模型。

Conclusion: GO UT Bench作为微调数据集能够有效提升代码LLM在Golang单元测试生成任务上的性能，解决了训练数据不平衡问题。

Abstract: Training data imbalance poses a major challenge for code LLMs. Most available data heavily over represents raw opensource code while underrepresenting broader software engineering tasks, especially in low resource languages like Golang. As a result, models excel at code autocompletion but struggle with real world developer workflows such as unit test generation. To address this gap, we introduce GO UT Bench, a benchmark dataset of 5264 pairs of code and unit tests, drawn from 10 permissively licensed Golang repositories spanning diverse domain. We evaluate its effectiveness as a fine tuning dataset across two LLM families i.e. mixture of experts and dense decoders. Our results show that finetuned models outperform their base counterparts on more than 75% of benchmark tasks.

</details>


### [43] [Multi-Joint Physics-Informed Deep Learning Framework for Time-Efficient Inverse Dynamics](https://arxiv.org/abs/2511.10878)
*Shuhao Ma,Zeyi Huang,Yu Cao,Wesley Doorsamy,Chaoyang Shi,Jun Li,Zhi-Qiang Zhang*

Main category: cs.LG

TL;DR: 提出了一种物理信息驱动的深度学习框架PI-MJCA-BiGRU，直接从运动学数据估计肌肉激活和力量，无需标注数据即可实现生理一致的预测，并具有高效推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在多关节系统中计算成本高且缺乏高质量标注数据集的问题，实现时间高效的肌肉激活和力量估计。

Method: 使用新颖的多关节交叉注意力模块(MJCA)结合双向门控循环单元(BiGRU)层来捕捉关节间协调，通过将多关节动力学、关节间耦合和外部力交互嵌入损失函数实现物理信息驱动。

Result: 在两个数据集上的实验验证表明，PI-MJCA-BiGRU无需真实标签即可达到与传统监督方法相当的性能，MJCA模块显著提升了关节间协调建模能力。

Conclusion: 该物理信息驱动的深度学习框架为临床评估和辅助设备控制提供了无需标注数据的高效肌肉激活和力量估计解决方案。

Abstract: Time-efficient estimation of muscle activations and forces across multi-joint systems is critical for clinical assessment and assistive device control. However, conventional approaches are computationally expensive and lack a high-quality labeled dataset for multi-joint applications. To address these challenges, we propose a physics-informed deep learning framework that estimates muscle activations and forces directly from kinematics. The framework employs a novel Multi-Joint Cross-Attention (MJCA) module with Bidirectional Gated Recurrent Unit (BiGRU) layers to capture inter-joint coordination, enabling each joint to adaptively integrate motion information from others. By embedding multi-joint dynamics, inter-joint coupling, and external force interactions into the loss function, our Physics-Informed MJCA-BiGRU (PI-MJCA-BiGRU) delivers physiologically consistent predictions without labeled data while enabling time-efficient inference. Experimental validation on two datasets demonstrates that PI-MJCA-BiGRU achieves performance comparable to conventional supervised methods without requiring ground-truth labels, while the MJCA module significantly enhances inter-joint coordination modeling compared to other baseline architectures.

</details>


### [44] [Towards Federated Clustering: A Client-wise Private Graph Aggregation Framework](https://arxiv.org/abs/2511.10915)
*Guanxiong He,Jie Wang,Liaoyuan Tang,Zheng Wang,Rong Wang,Feiping Nie*

Main category: cs.LG

TL;DR: 提出SPP-FGC算法，通过本地结构图作为隐私保护知识共享媒介，解决联邦聚类中性能与隐私的权衡问题，提供单轮通信和迭代优化两种模式。


<details>
  <summary>Details</summary>
Motivation: 解决联邦聚类中传统方法面临的困境：传输嵌入表示会泄露敏感数据，而仅共享抽象聚类原型会导致模型准确性下降。

Method: 基于客户端-服务器架构，客户端构建私有结构图捕获数据关系，服务器安全聚合和对齐形成全局图，从中推导统一聚类结构。提供SPP-FGC单轮通信和SPP-FGC+迭代优化两种模式。

Result: 实验表明该框架达到最先进性能，相比联邦基线将聚类准确性提高高达10%（NMI），同时保持可证明的隐私保证。

Conclusion: SPP-FGC框架通过结构图作为隐私保护知识共享媒介，成功解决了联邦聚类中性能与隐私的权衡问题，提供高效且安全的解决方案。

Abstract: Federated clustering addresses the critical challenge of extracting patterns from decentralized, unlabeled data. However, it is hampered by the flaw that current approaches are forced to accept a compromise between performance and privacy: \textit{transmitting embedding representations risks sensitive data leakage, while sharing only abstract cluster prototypes leads to diminished model accuracy}. To resolve this dilemma, we propose Structural Privacy-Preserving Federated Graph Clustering (SPP-FGC), a novel algorithm that innovatively leverages local structural graphs as the primary medium for privacy-preserving knowledge sharing, thus moving beyond the limitations of conventional techniques. Our framework operates on a clear client-server logic; on the client-side, each participant constructs a private structural graph that captures intrinsic data relationships, which the server then securely aggregates and aligns to form a comprehensive global graph from which a unified clustering structure is derived. The framework offers two distinct modes to suit different needs. SPP-FGC is designed as an efficient one-shot method that completes its task in a single communication round, ideal for rapid analysis. For more complex, unstructured data like images, SPP-FGC+ employs an iterative process where clients and the server collaboratively refine feature representations to achieve superior downstream performance. Extensive experiments demonstrate that our framework achieves state-of-the-art performance, improving clustering accuracy by up to 10\% (NMI) over federated baselines while maintaining provable privacy guarantees.

</details>


### [45] [GraphToxin: Reconstructing Full Unlearned Graphs from Graph Unlearning](https://arxiv.org/abs/2511.10936)
*Ying Song,Balaji Palanisamy*

Main category: cs.LG

TL;DR: GraphToxin是首个针对图遗忘的图重建攻击方法，通过曲率匹配模块实现细粒度指导，能够恢复已删除的个人信息、链接及敏感内容，对现有防御机制构成严重威胁。


<details>
  <summary>Details</summary>
Motivation: 图遗忘虽然旨在遵守"被遗忘权"法规，但存在安全漏洞，攻击者可能利用残留痕迹恢复已删除数据，破坏图遗忘的基本功能。

Method: 提出GraphToxin攻击方法，引入曲率匹配模块为完整遗忘图恢复提供细粒度指导，支持白盒和黑盒设置下的多节点移除攻击。

Result: GraphToxin能成功恢复已删除个体的信息、个人链接及连接中的敏感内容，现有防御机制对此攻击基本无效甚至可能增强其效果。

Conclusion: GraphToxin揭示了图遗忘方法的严重隐私风险，迫切需要开发更有效和鲁棒的防御策略来应对此类攻击。

Abstract: Graph unlearning has emerged as a promising solution for complying with "the right to be forgotten" regulations by enabling the removal of sensitive information upon request. However, this solution is not foolproof. The involvement of multiple parties creates new attack surfaces, and residual traces of deleted data can still remain in the unlearned graph neural networks. These vulnerabilities can be exploited by attackers to recover the supposedly erased samples, thereby undermining the inherent functionality of graph unlearning. In this work, we propose GraphToxin, the first graph reconstruction attack against graph unlearning. Specifically, we introduce a novel curvature matching module to provide a fine-grained guidance for full unlearned graph recovery. We demonstrate that GraphToxin can successfully subvert the regulatory guarantees expected from graph unlearning - it can recover not only a deleted individual's information and personal links but also sensitive content from their connections, thereby posing substantially more detrimental threats. Furthermore, we extend GraphToxin to multiple node removals under both white-box and black-box setting. We highlight the necessity of a worst-case analysis and propose a comprehensive evaluation framework to systematically assess the attack performance under both random and worst-case node removals. This provides a more robust and realistic measure of the vulnerability of graph unlearning methods to graph reconstruction attacks. Our extensive experiments demonstrate the effectiveness and flexibility of GraphToxin. Notably, we show that existing defense mechanisms are largely ineffective against this attack and, in some cases, can even amplify its performance. Given the severe privacy risks posed by GraphToxin, our work underscores the urgent need for the development of more effective and robust defense strategies against this attack.

</details>


### [46] [Cascading Bandits With Feedback](https://arxiv.org/abs/2511.10938)
*R Sri Prakash,Nikhil Karamchandani,Sharayu Moharir*

Main category: cs.LG

TL;DR: 本文研究了边缘推理中的级联赌博机模型，分析了四种决策策略的理论遗憾界限，发现LCB和Thompson Sampling因持续自适应而获得常数遗憾，优于固定顺序的Explore-then-Commit和Action Elimination策略。


<details>
  <summary>Details</summary>
Motivation: 受边缘推理挑战的驱动，研究每个臂对应具有相关准确度和错误概率的推理模型的级联赌博机变体。

Method: 分析四种决策策略：Explore-then-Commit、Action Elimination、Lower Confidence Bound (LCB)和Thompson Sampling，并为每种策略提供尖锐的理论遗憾保证。

Result: 与经典赌博机设置不同，Explore-then-Commit和Action Elimination因在探索阶段后承诺固定顺序而遭受次优遗憾。相比之下，LCB和Thompson Sampling基于观察到的反馈持续更新决策，实现了常数O(1)遗憾。模拟验证了这些理论发现。

Conclusion: 适应性在不确定性下实现高效边缘推理中起着关键作用，LCB和Thompson Sampling因持续自适应而优于固定顺序策略。

Abstract: Motivated by the challenges of edge inference, we study a variant of the cascade bandit model in which each arm corresponds to an inference model with an associated accuracy and error probability. We analyse four decision-making policies-Explore-then-Commit, Action Elimination, Lower Confidence Bound (LCB), and Thompson Sampling-and provide sharp theoretical regret guarantees for each. Unlike in classical bandit settings, Explore-then-Commit and Action Elimination incur suboptimal regret because they commit to a fixed ordering after the exploration phase, limiting their ability to adapt. In contrast, LCB and Thompson Sampling continuously update their decisions based on observed feedback, achieving constant O(1) regret. Simulations corroborate these theoretical findings, highlighting the crucial role of adaptivity for efficient edge inference under uncertainty.

</details>


### [47] [From Parameter to Representation: A Closed-Form Approach for Controllable Model Merging](https://arxiv.org/abs/2511.10943)
*Jialin Wu,Jian Yang,Handing Wang,Jiajun Wen,Zhiyong Yu*

Main category: cs.LG

TL;DR: 提出一种新的可控模型合并方法，通过直接修正模型最终表示的最优线性变换，替代复杂的离线多目标优化，实现线性复杂度、即时生成Pareto最优模型。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法面临参数干扰问题，且可控模型合并的离线优化过程计算成本高、复杂度随任务数指数增长。

Method: 将视角从参数空间优化转向直接修正模型最终表示，建模为最优线性变换问题，获得闭式解，实现单步、架构无关的计算。

Result: 实验结果显示该方法生成更优的Pareto前沿，具有更精确的偏好对齐和显著降低的计算成本。

Conclusion: 该方法通过表示空间的最优线性变换，实现了高效、可控的模型合并，解决了现有方法的计算复杂度问题。

Abstract: Model merging combines expert models for multitask performance but faces challenges from parameter interference. This has sparked recent interest in controllable model merging, giving users the ability to explicitly balance performance trade-offs. Existing approaches employ a compile-then-query paradigm, performing a costly offline multi-objective optimization to enable fast, preference-aware model generation. This offline stage typically involves iterative search or dedicated training, with complexity that grows exponentially with the number of tasks. To overcome these limitations, we shift the perspective from parameter-space optimization to a direct correction of the model's final representation. Our approach models this correction as an optimal linear transformation, yielding a closed-form solution that replaces the entire offline optimization process with a single-step, architecture-agnostic computation. This solution directly incorporates user preferences, allowing a Pareto-optimal model to be generated on-the-fly with complexity that scales linearly with the number of tasks. Experimental results show our method generates a superior Pareto front with more precise preference alignment and drastically reduced computational cost.

</details>


### [48] [How Data Quality Affects Machine Learning Models for Credit Risk Assessment](https://arxiv.org/abs/2511.10964)
*Andrea Maurino*

Main category: cs.LG

TL;DR: 本文研究了数据质量问题（包括缺失值、噪声属性、异常值和标签错误）对信用风险评估机器学习模型预测准确性的影响。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在信用风险评估中的应用日益增多，其有效性很大程度上取决于输入数据的质量。需要了解各种数据质量问题对模型性能的具体影响。

Method: 使用开源数据集，通过Pucktrick库引入受控的数据损坏，评估10种常用模型（如随机森林、SVM、逻辑回归等）的鲁棒性。

Result: 实验显示，模型鲁棒性存在显著差异，取决于数据退化的性质和严重程度。

Conclusion: 提出的方法和配套工具为从业者增强数据管道鲁棒性提供了实用支持，并为研究人员在数据为中心的AI环境中进一步实验提供了灵活框架。

Abstract: Machine Learning (ML) models are being increasingly employed for credit risk evaluation, with their effectiveness largely hinging on the quality of the input data. In this paper we investigate the impact of several data quality issues, including missing values, noisy attributes, outliers, and label errors, on the predictive accuracy of the machine learning model used in credit risk assessment. Utilizing an open-source dataset, we introduce controlled data corruption using the Pucktrick library to assess the robustness of 10 frequently used models like Random Forest, SVM, and Logistic Regression and so on. Our experiments show significant differences in model robustness based on the nature and severity of the data degradation. Moreover, the proposed methodology and accompanying tools offer practical support for practitioners seeking to enhance data pipeline robustness, and provide researchers with a flexible framework for further experimentation in data-centric AI contexts.

</details>


### [49] [Unsupervised Robust Domain Adaptation: Paradigm, Theory and Algorithm](https://arxiv.org/abs/2511.11009)
*Fuxiang Huang,Xiaowei Fu,Shiyu Ye,Lina Ma,Wen Li,Xinbo Gao,David Zhang,Lei Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种无监督鲁棒域自适应（URDA）范式，解决了传统域自适应方法对抗攻击鲁棒性不足的问题，并提出了解耦对抗鲁棒训练（DART）算法。


<details>
  <summary>Details</summary>
Motivation: 传统无监督域自适应方法强调迁移能力但忽视对抗攻击鲁棒性，而标准对抗训练在UDA中效果不佳。本文旨在解决UDA+VAT范式中的固有纠缠问题。

Method: 提出URDA范式，建立其泛化边界理论，并设计DART算法——一个两阶段训练过程：先预训练任意UDA模型，然后通过解耦蒸馏进行瞬时鲁棒化后训练。

Result: 在四个基准数据集上的实验表明，DART在保持域适应性的同时有效增强了鲁棒性，验证了URDA范式和理论的有效性。

Conclusion: 本文首次建立了URDA范式和理论，提出的DART算法简单有效，能同时保证迁移性和鲁棒性，为鲁棒域自适应提供了新思路。

Abstract: Unsupervised domain adaptation (UDA) aims to transfer knowledge from a label-rich source domain to an unlabeled target domain by addressing domain shifts. Most UDA approaches emphasize transfer ability, but often overlook robustness against adversarial attacks. Although vanilla adversarial training (VAT) improves the robustness of deep neural networks, it has little effect on UDA. This paper focuses on answering three key questions: 1) Why does VAT, known for its defensive effectiveness, fail in the UDA paradigm? 2) What is the generalization bound theory under attacks and how does it evolve from classical UDA theory? 3) How can we implement a robustification training procedure without complex modifications? Specifically, we explore and reveal the inherent entanglement challenge in general UDA+VAT paradigm, and propose an unsupervised robust domain adaptation (URDA) paradigm. We further derive the generalization bound theory of the URDA paradigm so that it can resist adversarial noise and domain shift. To the best of our knowledge, this is the first time to establish the URDA paradigm and theory. We further introduce a simple, novel yet effective URDA algorithm called Disentangled Adversarial Robustness Training (DART), a two-step training procedure that ensures both transferability and robustness. DART first pre-trains an arbitrary UDA model, and then applies an instantaneous robustification post-training step via disentangled distillation.Experiments on four benchmark datasets with/without attacks show that DART effectively enhances robustness while maintaining domain adaptability, and validate the URDA paradigm and theory.

</details>


### [50] [Improving Continual Learning of Knowledge Graph Embeddings via Informed Initialization](https://arxiv.org/abs/2511.11118)
*Gerard Pons,Besim Bilalli,Anna Queralt*

Main category: cs.LG

TL;DR: 提出了一种新的知识图谱嵌入初始化策略，利用KG模式和先前学习的嵌入为新实体获取初始表示，提高预测性能并减少灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 知识图谱频繁更新，需要持续学习技术来适应变化。嵌入初始化对最终嵌入的准确性和训练时间有重要影响，特别是对于相对较小和频繁的更新。

Method: 利用知识图谱模式和先前学习的嵌入，基于实体所属的类别为新实体获取初始表示，可无缝集成到现有的KGE持续学习方法中。

Result: 实验分析表明，该初始化策略提高了KGE的预测性能，增强了知识保留，加速了知识获取，减少了增量学习新嵌入所需的轮次和时间。

Conclusion: 该方法在各类KGE学习模型中均表现出优势，能够有效提升新知识获取能力并减少灾难性遗忘。

Abstract: Many Knowledege Graphs (KGs) are frequently updated, forcing their Knowledge Graph Embeddings (KGEs) to adapt to these changes. To address this problem, continual learning techniques for KGEs incorporate embeddings for new entities while updating the old ones. One necessary step in these methods is the initialization of the embeddings, as an input to the KGE learning process, which can have an important impact in the accuracy of the final embeddings, as well as in the time required to train them. This is especially relevant for relatively small and frequent updates. We propose a novel informed embedding initialization strategy, which can be seamlessly integrated into existing continual learning methods for KGE, that enhances the acquisition of new knowledge while reducing catastrophic forgetting. Specifically, the KG schema and the previously learned embeddings are utilized to obtain initial representations for the new entities, based on the classes the entities belong to. Our extensive experimental analysis shows that the proposed initialization strategy improves the predictive performance of the resulting KGEs, while also enhancing knowledge retention. Furthermore, our approach accelerates knowledge acquisition, reducing the number of epochs, and therefore time, required to incrementally learn new embeddings. Finally, its benefits across various types of KGE learning models are demonstrated.

</details>


### [51] [Anomaly Detection in High-Dimensional Bank Account Balances via Robust Methods](https://arxiv.org/abs/2511.11143)
*Federico Maddanu,Tommaso Proietti,Riccardo Crupi*

Main category: cs.LG

TL;DR: 本文提出并评估了多种鲁棒统计方法，用于在中等和高维数据集中高效检测银行账户余额中的异常点，应用于约260万条匿名用户银行账户余额的每日记录。


<details>
  <summary>Details</summary>
Motivation: 检测银行账户余额中的异常点对金融机构至关重要，可以识别潜在的欺诈、操作问题或其他异常情况。鲁棒统计有助于标记异常值并提供不受污染观测影响的数据分布参数估计。

Method: 提出并实证评估了多种鲁棒方法，这些方法在中等和高维数据集中具有计算效率，具有高崩溃点和低计算时间。

Result: 方法应用于约260万条匿名用户银行账户余额的每日记录，展示了在真实金融数据中的适用性。

Conclusion: 所提出的鲁棒方法在高维金融数据异常检测中具有计算效率和实用性，能够有效识别银行账户余额中的异常情况。

Abstract: Detecting point anomalies in bank account balances is essential for financial institutions, as it enables the identification of potential fraud, operational issues, or other irregularities. Robust statistics is useful for flagging outliers and for providing estimates of the data distribution parameters that are not affected by contaminated observations. However, such a strategy is often less efficient and computationally expensive under high dimensional setting. In this paper, we propose and evaluate empirically several robust approaches that may be computationally efficient in medium and high dimensional datasets, with high breakdown points and low computational time. Our application deals with around 2.6 million daily records of anonymous users' bank account balances.

</details>


### [52] [Deep Learning for Short-Term Precipitation Prediction in Four Major Indian Cities: A ConvLSTM Approach with Explainable AI](https://arxiv.org/abs/2511.11152)
*Tanmay Ghosh,Shaurabh Anand,Rakesh Gomaji Nannewar,Nithin Nagaraj*

Main category: cs.LG

TL;DR: 开发了一个可解释的深度学习框架，用于印度四个主要城市的短期降水预测，结合CNN-ConvLSTM架构和多种可解释性分析方法，在保持准确性的同时提高模型透明度。


<details>
  <summary>Details</summary>
Motivation: 深度学习降水预测模型通常作为黑箱运行，限制了在实际天气预测中的应用。为了在保持准确性的同时增强透明度，需要开发可解释的深度学习框架。

Method: 采用混合时间分布式CNN-ConvLSTM架构，基于多年代ERA5再分析数据进行训练。为每个城市优化不同数量的卷积滤波器：班加罗尔(32)、孟买和德里(64)、加尔各答(128)。使用置换重要性、Grad-CAM、时间遮挡和反事实扰动进行可解释性分析。

Result: 模型在四个城市取得RMSE值：班加罗尔0.21毫米/天、孟买0.52毫米/天、德里0.48毫米/天、加尔各答1.80毫米/天。预测时间范围从班加罗尔的1天到加尔各答的5天。模型依赖城市特定变量，显示出不同的行为模式。

Conclusion: 研究表明可解释AI能够为不同城市环境提供准确的降水预测和透明的模式洞察，证明了xAI在降水预测中的实用价值。

Abstract: Deep learning models for precipitation forecasting often function as black boxes, limiting their adoption in real-world weather prediction. To enhance transparency while maintaining accuracy, we developed an interpretable deep learning framework for short-term precipitation prediction in four major Indian cities: Bengaluru, Mumbai, Delhi, and Kolkata, spanning diverse climate zones. We implemented a hybrid Time-Distributed CNN-ConvLSTM (Convolutional Neural Network-Long Short-Term Memory) architecture, trained on multi-decadal ERA5 reanalysis data. The architecture was optimized for each city with a different number of convolutional filters: Bengaluru (32), Mumbai and Delhi (64), and Kolkata (128). The models achieved root mean square error (RMSE) values of 0.21 mm/day (Bengaluru), 0.52 mm/day (Mumbai), 0.48 mm/day (Delhi), and 1.80 mm/day (Kolkata). Through interpretability analysis using permutation importance, Gradient-weighted Class Activation Mapping (Grad-CAM), temporal occlusion, and counterfactual perturbation, we identified distinct patterns in the model's behavior. The model relied on city-specific variables, with prediction horizons ranging from one day for Bengaluru to five days for Kolkata. This study demonstrates how explainable AI (xAI) can provide accurate forecasts and transparent insights into precipitation patterns in diverse urban environments.

</details>


### [53] [Power Ensemble Aggregation for Improved Extreme Event AI Prediction](https://arxiv.org/abs/2511.11170)
*Julien Collard,Pierre Gentine,Tian Zheng*

Main category: cs.LG

TL;DR: 本文提出使用幂均值聚合集成预测来改进机器学习对极端高温事件的预测性能，相比传统均值方法能获得更好的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 解决气候极端事件（特别是热浪）预测的关键挑战，改进机器学习方法对极端高温的预测能力。

Method: 将问题构建为分类问题，预测地表气温是否会超过局部q分位数；通过使机器学习天气预报模型具有生成性，并应用幂均值非线性聚合方法来整合集成预测。

Result: 幂均值聚合显著提升了分类器性能，在预测极端高温事件方面比相同模型的典型均值预测具有更好的准确性，且对更高极端值的预测效果更佳。

Conclusion: 幂聚合方法显示出良好的前景和适应性，其最佳性能随所选分位数阈值而变化，在预测更高极端值时效果更显著。

Abstract: This paper addresses the critical challenge of improving predictions of climate extreme events, specifically heat waves, using machine learning methods. Our work is framed as a classification problem in which we try to predict whether surface air temperature will exceed its q-th local quantile within a specified timeframe. Our key finding is that aggregating ensemble predictions using a power mean significantly enhances the classifier's performance. By making a machine-learning based weather forecasting model generative and applying this non-linear aggregation method, we achieve better accuracy in predicting extreme heat events than with the typical mean prediction from the same model. Our power aggregation method shows promise and adaptability, as its optimal performance varies with the quantile threshold chosen, demonstrating increased effectiveness for higher extremes prediction.

</details>


### [54] [On-line learning of dynamic systems: sparse regression meets Kalman filtering](https://arxiv.org/abs/2511.11178)
*Gianluigi Pillonetto,Akram Yazdani,Aleksandr Aravkin*

Main category: cs.LG

TL;DR: 本文提出了一种将稀疏识别非线性动力学(Sindy)算法与卡尔曼滤波(KF)相结合的新方法——Sindy卡尔曼滤波(SKF)，用于实时学习非线性动力系统的控制方程。


<details>
  <summary>Details</summary>
Motivation: 从数据中学习控制方程对于理解物理系统行为至关重要，但现有方法难以实现实时学习复杂时变非线性模型。

Method: 将Sindy算法与卡尔曼滤波结合，将未知系统参数视为状态变量，通过前瞻误差增强参数识别策略，简化稀疏水平、方差参数和切换时刻的估计。

Result: 在具有漂移或切换参数的混沌Lorenz系统上验证了SKF的有效性，并在真实飞行数据构建的稀疏非线性飞机模型中展示了实时识别能力。

Conclusion: SKF统一了稀疏驱动方法和控制理论框架，能够实时推断复杂时变非线性模型，超越了单一方法的性能。

Abstract: Learning governing equations from data is central to understanding the behavior of physical systems across diverse scientific disciplines, including physics, biology, and engineering. The Sindy algorithm has proven effective in leveraging sparsity to identify concise models of nonlinear dynamical systems. In this paper, we extend sparsity-driven approaches to real-time learning by integrating a cornerstone algorithm from control theory -- the Kalman filter (KF). The resulting Sindy Kalman Filter (SKF) unifies both frameworks by treating unknown system parameters as state variables, enabling real-time inference of complex, time-varying nonlinear models unattainable by either method alone. Furthermore, SKF enhances KF parameter identification strategies, particularly via look-ahead error, significantly simplifying the estimation of sparsity levels, variance parameters, and switching instants. We validate SKF on a chaotic Lorenz system with drifting or switching parameters and demonstrate its effectiveness in the real-time identification of a sparse nonlinear aircraft model built from real flight data.

</details>


### [55] [Dynamic Deep Graph Learning for Incomplete Multi-View Clustering with Masked Graph Reconstruction Loss](https://arxiv.org/abs/2511.11181)
*Zhenghao Zhang,Jun Xie,Xingchen Chen,Tao Yu,Hongzhu Yi,Kaixin Xu,Yuanxiang Wang,Tianyu Zong,Xinming Wang,Jiahuan Chen,Guoqing Chao,Feng Chen,Zhepeng Wang,Jungang Xu*

Main category: cs.LG

TL;DR: 提出了一种名为DGIMVCM的动态深度图学习方法，用于解决不完整多视图聚类问题，通过动态图学习、掩码图重建损失和伪标签自监督训练机制来提高聚类性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中多视图数据普遍存在不完整性，而现有基于GNN的IMVC方法存在两个主要问题：使用KNN构建静态图引入噪声，以及使用MSE损失导致梯度噪声。

Method: 1. 从原始数据构建缺失鲁棒的全局图，设计图卷积嵌入层提取主要特征和动态视图特定图结构；2. 引入图自注意力编码器提取高层表示，使用掩码图重建损失优化；3. 构建聚类模块并通过伪标签自监督训练机制优化。

Result: 在多个数据集上的广泛实验验证了DGIMVCM的有效性和优越性。

Conclusion: DGIMVCM通过动态深度图学习和掩码图重建损失，有效解决了不完整多视图聚类中的图构建噪声和梯度噪声问题，取得了优越的聚类性能。

Abstract: The prevalence of real-world multi-view data makes incomplete multi-view clustering (IMVC) a crucial research. The rapid development of Graph Neural Networks (GNNs) has established them as one of the mainstream approaches for multi-view clustering. Despite significant progress in GNNs-based IMVC, some challenges remain: (1) Most methods rely on the K-Nearest Neighbors (KNN) algorithm to construct static graphs from raw data, which introduces noise and diminishes the robustness of the graph topology. (2) Existing methods typically utilize the Mean Squared Error (MSE) loss between the reconstructed graph and the sparse adjacency graph directly as the graph reconstruction loss, leading to substantial gradient noise during optimization. To address these issues, we propose a novel \textbf{D}ynamic Deep \textbf{G}raph Learning for \textbf{I}ncomplete \textbf{M}ulti-\textbf{V}iew \textbf{C}lustering with \textbf{M}asked Graph Reconstruction Loss (DGIMVCM). Firstly, we construct a missing-robust global graph from the raw data. A graph convolutional embedding layer is then designed to extract primary features and refined dynamic view-specific graph structures, leveraging the global graph for imputation of missing views. This process is complemented by graph structure contrastive learning, which identifies consistency among view-specific graph structures. Secondly, a graph self-attention encoder is introduced to extract high-level representations based on the imputed primary features and view-specific graphs, and is optimized with a masked graph reconstruction loss to mitigate gradient noise during optimization. Finally, a clustering module is constructed and optimized through a pseudo-label self-supervised training mechanism. Extensive experiments on multiple datasets validate the effectiveness and superiority of DGIMVCM.

</details>


### [56] [LoRaCompass: Robust Reinforcement Learning to Efficiently Search for a LoRa Tag](https://arxiv.org/abs/2511.11190)
*Tianlang He,Zhongming Lin,Tianrui Jiang,S. -H. Gary Chan*

Main category: cs.LG

TL;DR: 提出LoRaCompass强化学习模型，用于在未知环境中高效定位LoRa标签，通过空间感知特征提取和策略蒸馏损失实现鲁棒性，在80km²范围内验证了90%以上的定位成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的LoRa标签定位方法容易受到域偏移和信号波动影响，导致级联决策错误和显著的定位不准确性。

Method: 提出LoRaCompass模型，包含空间感知特征提取器从RSSI学习鲁棒空间表示，使用策略蒸馏损失函数最大化靠近标签的概率，并引入基于置信上界的探索函数。

Result: 在80km²多样化未见环境中验证，定位成功率>90%（比现有方法提升40%），搜索路径长度与初始距离呈线性比例关系。

Conclusion: LoRaCompass能够实现鲁棒高效的LoRa标签搜索，在域偏移和信号波动下仍能保持高定位成功率和搜索效率。

Abstract: The Long-Range (LoRa) protocol, known for its extensive range and low power, has increasingly been adopted in tags worn by mentally incapacitated persons (MIPs) and others at risk of going missing. We study the sequential decision-making process for a mobile sensor to locate a periodically broadcasting LoRa tag with the fewest moves (hops) in general, unknown environments, guided by the received signal strength indicator (RSSI). While existing methods leverage reinforcement learning for search, they remain vulnerable to domain shift and signal fluctuation, resulting in cascading decision errors that culminate in substantial localization inaccuracies. To bridge this gap, we propose LoRaCompass, a reinforcement learning model designed to achieve robust and efficient search for a LoRa tag. For exploitation under domain shift and signal fluctuation, LoRaCompass learns a robust spatial representation from RSSI to maximize the probability of moving closer to a tag, via a spatially-aware feature extractor and a policy distillation loss function. It further introduces an exploration function inspired by the upper confidence bound (UCB) that guides the sensor toward the tag with increasing confidence. We have validated LoRaCompass in ground-based and drone-assisted scenarios within diverse unseen environments covering an area of over 80km^2. It has demonstrated high success rate (>90%) in locating the tag within 100m proximity (a 40% improvement over existing methods) and high efficiency with a search path length (in hops) that scales linearly with the initial distance.

</details>


### [57] [When to Stop Federated Learning: Zero-Shot Generation of Synthetic Validation Data with Generative AI for Early Stopping](https://arxiv.org/abs/2511.11208)
*Youngjoon Lee,Hyukjoon Lee,Jinu Gong,Yang Cao,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 提出了一种基于生成式AI的零样本合成验证框架，用于联邦学习中的自适应早停，可减少高达74%的训练轮次，同时保持与最优性能相差1%以内的准确率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习通常运行预定义的全局轮次，导致在达到最优性能后仍进行不必要的计算，或者在模型无法获得有意义的性能时继续训练，造成计算资源浪费。

Method: 利用生成式AI构建零样本合成验证框架，监控模型性能并确定早停点，自适应地在接近最优轮次时停止训练。

Result: 在多标签胸部X光分类任务上的数值结果表明，该方法可将训练轮次减少高达74%，同时准确率保持在最优性能的1%以内。

Conclusion: 提出的零样本合成验证框架有效解决了联邦学习中的计算效率问题，能够显著节省计算资源并支持快速超参数调整。

Abstract: Federated Learning (FL) enables collaborative model training across decentralized devices while preserving data privacy. However, FL methods typically run for a predefined number of global rounds, often leading to unnecessary computation when optimal performance is reached earlier. In addition, training may continue even when the model fails to achieve meaningful performance. To address this inefficiency, we introduce a zero-shot synthetic validation framework that leverages generative AI to monitor model performance and determine early stopping points. Our approach adaptively stops training near the optimal round, thereby conserving computational resources and enabling rapid hyperparameter adjustments. Numerical results on multi-label chest X-ray classification demonstrate that our method reduces training rounds by up to 74% while maintaining accuracy within 1% of the optimal.

</details>


### [58] [A Best-of-Both-Worlds Proof for Tsallis-INF without Fenchel Conjugates](https://arxiv.org/abs/2511.11211)
*Wei-Cheng Lee,Francesco Orabona*

Main category: cs.LG

TL;DR: 本文提供了一个对Tsallis-INF多臂老虎机算法最佳世界保证的简化推导，避免了共轭函数的使用，采用在线凸优化工具，并为了简洁证明未优化边界常数。


<details>
  <summary>Details</summary>
Motivation: 为Tsallis-INF算法的最佳世界保证提供更简单直接的证明方法，简化现有的复杂推导过程。

Method: 使用现代在线凸优化工具，避免传统证明中的共轭函数方法，保持证明的简洁性。

Result: 成功推导出Tsallis-INF算法在随机和对抗性老虎机问题中的最佳世界性能保证。

Conclusion: 通过更简单的证明方法验证了Tsallis-INF算法的理论性能，为相关研究提供了更易理解的推导路径。

Abstract: In this short note, we present a simple derivation of the best-of-both-world guarantee for the Tsallis-INF multi-armed bandit algorithm from J. Zimmert and Y. Seldin. Tsallis-INF: An optimal algorithm for stochastic and adversarial bandits. Journal of Machine Learning Research, 22(28):1-49, 2021. URL https://jmlr.csail.mit.edu/papers/volume22/19-753/19-753.pdf. In particular, the proof uses modern tools from online convex optimization and avoid the use of conjugate functions. Also, we do not optimize the constants in the bounds in favor of a slimmer proof.

</details>


### [59] [HealSplit: Towards Self-Healing through Adversarial Distillation in Split Federated Learning](https://arxiv.org/abs/2511.11240)
*Yuhan Xie,Chen Lyu*

Main category: cs.LG

TL;DR: HealSplit是首个针对Split Federated Learning的统一定制防御框架，提供端到端的检测和恢复功能，对抗五种复杂的数据投毒攻击。


<details>
  <summary>Details</summary>
Motivation: Split Federated Learning在隐私保护分布式学习中具有潜力，但容易受到针对本地特征、标签、粉碎数据和模型权重的复杂数据投毒攻击。现有防御方法主要从传统联邦学习改编而来，在SFL中效果有限。

Method: HealSplit包含三个关键组件：(1)拓扑感知检测模块，通过粉碎数据构建图并使用拓扑异常评分识别中毒样本；(2)生成恢复管道，为检测到的异常合成语义一致的替代品；(3)对抗性多教师蒸馏框架，使用语义监督和异常感知信号训练学生模型。

Result: 在四个基准数据集上的广泛实验表明，HealSplit始终优于十种最先进的防御方法，在不同攻击场景下实现了卓越的鲁棒性和防御效果。

Conclusion: HealSplit为Split Federated Learning提供了一个有效的端到端防御解决方案，能够有效检测和恢复多种复杂的数据投毒攻击。

Abstract: Split Federated Learning (SFL) is an emerging paradigm for privacy-preserving distributed learning. However, it remains vulnerable to sophisticated data poisoning attacks targeting local features, labels, smashed data, and model weights. Existing defenses, primarily adapted from traditional Federated Learning (FL), are less effective under SFL due to limited access to complete model updates. This paper presents HealSplit, the first unified defense framework tailored for SFL, offering end-to-end detection and recovery against five sophisticated types of poisoning attacks. HealSplit comprises three key components: (1) a topology-aware detection module that constructs graphs over smashed data to identify poisoned samples via topological anomaly scoring (TAS); (2) a generative recovery pipeline that synthesizes semantically consistent substitutes for detected anomalies, validated by a consistency validation student; and (3) an adversarial multi-teacher distillation framework trains the student using semantic supervision from a Vanilla Teacher and anomaly-aware signals from an Anomaly-Influence Debiasing (AD) Teacher, guided by the alignment between topological and gradient-based interaction matrices. Extensive experiments on four benchmark datasets demonstrate that HealSplit consistently outperforms ten state-of-the-art defenses, achieving superior robustness and defense effectiveness across diverse attack scenarios.

</details>


### [60] [Retrofit: Continual Learning with Bounded Forgetting for Security Applications](https://arxiv.org/abs/2511.11439)
*Yiling He,Junchi Lei,Hongyu She,Shuo Shao,Xinran Zheng,Yiping Liu,Zhan Qin,Lorenzo Cavallaro*

Main category: cs.LG

TL;DR: RETROFIT是一种无需历史数据的持续学习方法，通过参数级模型合并和知识仲裁机制，在安全分析场景中有效缓解灾难性遗忘问题，在恶意软件检测和二进制摘要任务上显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在安全分析中性能会随威胁环境变化而下降，现有持续学习方法依赖全量重训练或数据回放，在数据敏感环境中不可行，且难以平衡新旧知识的整合。

Method: 通过合并预训练模型和新微调模型作为新旧知识的教师模型，采用参数级合并避免历史数据需求；使用低秩稀疏更新限制参数变化到独立子空间；通过知识仲裁机制基于模型置信度动态平衡教师贡献。

Result: 在时序漂移的恶意软件检测中，保留分数从20.2%提升至38.6%，超越CL基线方法并在新数据上超过oracle上界；在跨反编译级别的二进制摘要任务中，BLEU分数约为先前工作中迁移学习的两倍，在跨表示泛化中超越所有基线。

Conclusion: RETROFIT在无需历史数据的情况下实现了有界遗忘，在安全分析场景中有效平衡了知识保留和适应性，为数据敏感环境下的持续学习提供了可行解决方案。

Abstract: Modern security analytics are increasingly powered by deep learning models, but their performance often degrades as threat landscapes evolve and data representations shift. While continual learning (CL) offers a promising paradigm to maintain model effectiveness, many approaches rely on full retraining or data replay, which are infeasible in data-sensitive environments. Moreover, existing methods remain inadequate for security-critical scenarios, facing two coupled challenges in knowledge transfer: preserving prior knowledge without old data and integrating new knowledge with minimal interference.
  We propose RETROFIT, a data retrospective-free continual learning method that achieves bounded forgetting for effective knowledge transfer. Our key idea is to consolidate previously trained and newly fine-tuned models, serving as teachers of old and new knowledge, through parameter-level merging that eliminates the need for historical data. To mitigate interference, we apply low-rank and sparse updates that confine parameter changes to independent subspaces, while a knowledge arbitration dynamically balances the teacher contributions guided by model confidence. Our evaluation on two representative applications demonstrates that RETROFIT consistently mitigates forgetting while maintaining adaptability. In malware detection under temporal drift, it substantially improves the retention score, from 20.2% to 38.6% over CL baselines, and exceeds the oracle upper bound on new data. In binary summarization across decompilation levels, where analyzing stripped binaries is especially challenging, RETROFIT achieves around twice the BLEU score of transfer learning used in prior work and surpasses all baselines in cross-representation generalization.

</details>


### [61] [Epistemic Error Decomposition for Multi-step Time Series Forecasting: Rethinking Bias-Variance in Recursive and Direct Strategies](https://arxiv.org/abs/2511.11461)
*Riku Green,Huw Day,Zahraa S. Abdallah,Telmo M. Silva Filho*

Main category: cs.LG

TL;DR: 本文重新审视了多步预测中递归策略和直接策略的偏差-方差权衡传统观点，通过理论分解和实验证明，对于非线性模型，递归策略可能同时具有更低偏差和更高方差，挑战了传统认知。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为递归策略具有高偏差低方差，直接策略具有低偏差高方差。作者旨在重新审视这一观点，通过理论分析揭示非线性模型中递归策略可能具有更复杂的偏差-方差特性。

Method: 将多步预测误差分解为不可约噪声、结构近似误差和估计方差三个部分。对于线性预测器，证明结构误差为零；对于非线性预测器，分析递归组合如何影响模型表达能力。引入基于雅可比矩阵的放大因子来量化参数误差敏感性。

Result: 理论分析表明，对于非线性预测器，递归策略可能同时具有更低偏差和更高方差。在ETTm1数据集上的多层感知机实验验证了这一发现，递归策略在某些情况下确实表现出更低的偏差和更高的方差。

Conclusion: 选择递归或直接策略应基于模型非线性程度和噪声特性，而非传统的偏差-方差直觉。非线性模型中递归策略可能提供更好的偏差-方差权衡，这为实际应用提供了新的指导原则。

Abstract: Multi-step forecasting is often described through a simple rule of thumb: recursive strategies are said to have high bias and low variance, while direct strategies are said to have low bias and high variance. We revisit this belief by decomposing the expected multi-step forecast error into three parts: irreducible noise, a structural approximation gap, and an estimation-variance term. For linear predictors we show that the structural gap is identically zero for any dataset. For nonlinear predictors, however, the repeated composition used in recursion can increase model expressivity, making the structural gap depend on both the model and the data. We further show that the estimation variance of the recursive strategy at any horizon can be written as the one-step variance multiplied by a Jacobian-based amplification factor that measures how sensitive the composed predictor is to parameter error. This perspective explains when recursive forecasting may simultaneously have lower bias and higher variance than direct forecasting. Experiments with multilayer perceptrons on the ETTm1 dataset confirm these findings. The results offer practical guidance for choosing between recursive and direct strategies based on model nonlinearity and noise characteristics, rather than relying on traditional bias-variance intuition.

</details>


### [62] [Toward Scalable Early Cancer Detection: Evaluating EHR-Based Predictive Models Against Traditional Screening Criteria](https://arxiv.org/abs/2511.11293)
*Jiheum Park,Chao Pang,Tristan Y. Lee,Jeong Yun Yang,Jacob Berkowitz,Alexander Z. Wei,Nicholas Tatonetti*

Main category: cs.LG

TL;DR: 本研究评估了基于电子健康记录（EHR）的预测模型与传统风险因素在识别八种主要癌症高风险个体方面的临床效用，发现EHR模型在识别真实癌症病例方面比传统风险因素高出3-6倍。


<details>
  <summary>Details</summary>
Motivation: 当前癌症筛查指南仅覆盖少数癌症类型，且依赖年龄或单一风险因素等狭窄标准来识别高风险个体。基于EHR的预测模型可能通过检测癌症的细微前诊断信号，提供更有效的工具来识别高风险群体。

Method: 使用All of Us研究计划的数据，整合了超过865,000名参与者的EHR、基因组和调查数据，系统评估EHR基础预测模型与传统风险因素在八种主要癌症中的表现。

Result: 即使使用基线建模方法，基于EHR的模型在识别为高风险个体中真实癌症病例的富集度比单独使用传统风险因素高出3-6倍。EHR基础模型进一步在26种癌症类型中提高了预测性能。

Conclusion: 基于EHR的预测建模具有临床潜力，可以支持更精确和可扩展的早期检测策略。

Abstract: Current cancer screening guidelines cover only a few cancer types and rely on narrowly defined criteria such as age or a single risk factor like smoking history, to identify high-risk individuals. Predictive models using electronic health records (EHRs), which capture large-scale longitudinal patient-level health information, may provide a more effective tool for identifying high-risk groups by detecting subtle prediagnostic signals of cancer. Recent advances in large language and foundation models have further expanded this potential, yet evidence remains limited on how useful HER-based models are compared with traditional risk factors currently used in screening guidelines. We systematically evaluated the clinical utility of EHR-based predictive models against traditional risk factors, including gene mutations and family history of cancer, for identifying high-risk individuals across eight major cancers (breast, lung, colorectal, prostate, ovarian, liver, pancreatic, and stomach), using data from the All of Us Research Program, which integrates EHR, genomic, and survey data from over 865,000 participants. Even with a baseline modeling approach, EHR-based models achieved a 3- to 6-fold higher enrichment of true cancer cases among individuals identified as high risk compared with traditional risk factors alone, whether used as a standalone or complementary tool. The EHR foundation model, a state-of-the-art approach trained on comprehensive patient trajectories, further improved predictive performance across 26 cancer types, demonstrating the clinical potential of EHR-based predictive modeling to support more precise and scalable early detection strategies.

</details>


### [63] [Toward Multi-Fidelity Machine Learning Force Field for Cathode Materials](https://arxiv.org/abs/2511.11361)
*Guangyi Dong,Zhihui Wang*

Main category: cs.LG

TL;DR: 本文开发了一个多保真度机器学习力场框架，用于提高锂离子电池正极材料计算的数据效率，能够同时利用低保真度非磁性和高保真度磁性计算数据集进行训练。


<details>
  <summary>Details</summary>
Motivation: 机器学习力场在锂离子电池正极材料中的开发和应用相对有限，主要由于正极材料复杂的电子结构特性和高质量计算数据集的稀缺。

Method: 开发多保真度机器学习力场框架，同时利用低保真度非磁性和高保真度磁性计算数据集进行训练。

Result: 在锂锰铁磷酸盐正极材料系统上的测试证明了这种多保真度方法的有效性。

Conclusion: 这项工作有助于以较低的训练数据集成本实现正极材料的高精度机器学习力场训练，并为机器学习力场在正极材料计算模拟中的应用提供了新视角。

Abstract: Machine learning force fields (MLFFs), which employ neural networks to map atomic structures to system energies, effectively combine the high accuracy of first-principles calculation with the computational efficiency of empirical force fields. They are widely used in computational materials simulations. However, the development and application of MLFFs for lithium-ion battery cathode materials remain relatively limited. This is primarily due to the complex electronic structure characteristics of cathode materials and the resulting scarcity of high-quality computational datasets available for force field training. In this work, we develop a multi-fidelity machine learning force field framework to enhance the data efficiency of computational results, which can simultaneously utilize both low-fidelity non-magnetic and high-fidelity magnetic computational datasets of cathode materials for training. Tests conducted on the lithium manganese iron phosphate (LMFP) cathode material system demonstrate the effectiveness of this multi-fidelity approach. This work helps to achieve high-accuracy MLFF training for cathode materials at a lower training dataset cost, and offers new perspectives for applying MLFFs to computational simulations of cathode materials.

</details>


### [64] [On-Device Fine-Tuning via Backprop-Free Zeroth-Order Optimization](https://arxiv.org/abs/2511.11362)
*Prabodh Katti,Sangwoo Park,Bipin Rajendran,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 本文分析了在边缘设备内存限制下，使用内存高效零阶优化(MeZO)相比传统反向传播(BP)进行模型微调的优势。MeZO通过仅使用前向评估来估计梯度，消除了存储中间激活和优化器状态的需求，从而可以在有限内存中部署更大模型，但需要更长的微调时间。


<details>
  <summary>Details</summary>
Motivation: 边缘AI系统需要在严格内存约束下适应不同任务，传统BP训练需要存储层激活和优化器状态，这严重限制了可部署的最大模型规模。

Method: 使用内存高效零阶优化(MeZO)方法，仅通过前向评估来估计梯度，无需存储中间激活或优化器状态。首先提供BP和MeZO训练下可容纳模型大小的理论估计，然后进行数值验证。

Result: 数值验证表明，在设备内存约束下，只要有足够的微调时间，MeZO在准确性方面具有优势，能够显著增大可部署的模型规模。

Conclusion: MeZO通过消除存储中间激活和优化器状态的需求，能够在边缘设备内存限制下支持更大模型的微调，尽管需要更长的训练时间，但在准确性方面具有优势。

Abstract: On-device fine-tuning is a critical capability for edge AI systems, which must support adaptation to different agentic tasks under stringent memory constraints. Conventional backpropagation (BP)-based training requires storing layer activations and optimizer states, a demand that can be only partially alleviated through checkpointing. In edge deployments in which the model weights must reside entirely in device memory, this overhead severely limits the maximum model size that can be deployed. Memory-efficient zeroth-order optimization (MeZO) alleviates this bottleneck by estimating gradients using forward evaluations alone, eliminating the need for storing intermediate activations or optimizer states. This enables significantly larger models to fit within on-chip memory, albeit at the cost of potentially longer fine-tuning wall-clock time. This paper first provides a theoretical estimate of the relative model sizes that can be accommodated under BP and MeZO training. We then numerically validate the analysis, demonstrating that MeZO exhibits accuracy advantages under on-device memory constraints, provided sufficient wall-clock time is available for fine-tuning.

</details>


### [65] [SPOT: Single-Shot Positioning via Trainable Near-Field Rainbow Beamforming](https://arxiv.org/abs/2511.11391)
*Yeyue Cai,Jianhua Mo,Meixia Tao*

Main category: cs.LG

TL;DR: 本文提出了一种端到端的深度学习方案，用于同时设计彩虹波束和估计用户位置，通过将相位器和真时延系数作为可训练变量来优化定位精度，相比现有方法显著降低了开销和定位误差。


<details>
  <summary>Details</summary>
Motivation: 相位时间阵列（整合相位器和真时延）已成为宽带感知和定位中生成频率相关彩虹波束的经济高效架构，需要开发更有效的波束设计和定位方法。

Method: 使用端到端深度学习方案，将相位器和真时延系数作为可训练变量，设计任务导向的波束以最大化定位精度，并通过轻量级全连接模块从用户反馈的最大量化接收功率及其对应子载波索引中恢复用户的角度-距离坐标。

Result: 与现有解析和学习方案相比，所提方法将开销降低了一个数量级，并始终提供更低的二维定位误差。

Conclusion: 所提出的深度学习方案能够有效设计彩虹波束并实现高精度用户定位，在减少开销的同时提高了定位性能。

Abstract: Phase-time arrays, which integrate phase shifters (PSs) and true-time delays (TTDs), have emerged as a cost-effective architecture for generating frequency-dependent rainbow beams in wideband sensing and localization. This paper proposes an end-to-end deep learning-based scheme that simultaneously designs the rainbow beams and estimates user positions. Treating the PS and TTD coefficients as trainable variables allows the network to synthesize task-oriented beams that maximize localization accuracy. A lightweight fully connected module then recovers the user's angle-range coordinates from its feedback of the maximum quantized received power and its corresponding subcarrier index after a single downlink transmission. Compared with existing analytical and learning-based schemes, the proposed method reduces overhead by an order of magnitude and delivers consistently lower two-dimensional positioning error.

</details>


### [66] [Multicalibration yields better matchings](https://arxiv.org/abs/2511.11413)
*Riccardo Colini Baldeschi,Simone Di Gregorio,Simone Fioravanti,Federico Fusco,Ido Guy,Daniel Haimovich,Stefano Leonardi,Fridolin Linder,Lorenzo Perini,Matteo Russo,Niek Tax*

Main category: cs.LG

TL;DR: 本文提出使用多校准（multicalibration）方法来处理加权图匹配问题中的预测误差，通过构建多校准预测器来补偿不完美预测器带来的误差，使基于预测权重的匹配决策能与原始预测器上的最佳决策规则相竞争。


<details>
  <summary>Details</summary>
Motivation: 在实际加权图匹配问题中，通常只能访问基于上下文的权重预测值而非真实随机权重。如果预测器是贝叶斯最优的，那么基于预测权重计算最佳匹配是最优的。但现实中预测器往往不完美，需要寻找能够补偿预测误差的次优决策规则。

Method: 提出多校准作为解决方案，要求预测器在受保护上下文集合的每个元素上都是无偏的。给定匹配算法类C和任意边权重预测器γ，构建特定的多校准预测器γ̂，使得基于γ̂输出选择最佳匹配能与在原始预测器γ上应用C中最佳决策规则相竞争。

Result: 证明了构建的多校准预测器能够使匹配决策与原始预测器上的最佳决策规则具有竞争力，并提供了样本复杂度界限来补充这一结果。

Conclusion: 多校准方法能够有效处理不完美预测器在加权图匹配问题中引入的误差，通过构建校准后的预测器来提升匹配决策的质量，使其与使用原始预测器的最佳决策规则相竞争。

Abstract: Consider the problem of finding the best matching in a weighted graph where we only have access to predictions of the actual stochastic weights, based on an underlying context. If the predictor is the Bayes optimal one, then computing the best matching based on the predicted weights is optimal. However, in practice, this perfect information scenario is not realistic. Given an imperfect predictor, a suboptimal decision rule may compensate for the induced error and thus outperform the standard optimal rule.
  In this paper, we propose multicalibration as a way to address this problem. This fairness notion requires a predictor to be unbiased on each element of a family of protected sets of contexts. Given a class of matching algorithms $\mathcal C$ and any predictor $γ$ of the edge-weights, we show how to construct a specific multicalibrated predictor $\hat γ$, with the following property. Picking the best matching based on the output of $\hat γ$ is competitive with the best decision rule in $\mathcal C$ applied onto the original predictor $γ$. We complement this result by providing sample complexity bounds.

</details>


### [67] [Differentiation Strategies for Acoustic Inverse Problems: Admittance Estimation and Shape Optimization](https://arxiv.org/abs/2511.11415)
*Nikolas Borrel-Jensen,Josiah Bjorgaard*

Main category: cs.LG

TL;DR: 本文提出了一个实用的可微分编程方法解决声学逆问题，包括导纳估计和形状优化两个应用。使用JAX-FEM自动微分直接估计边界导纳，结合随机有限差分进行声学形状优化，实现了48.1%的能量减少和30倍的计算效率提升。


<details>
  <summary>Details</summary>
Motivation: 传统声学逆问题求解需要手动推导伴随方程，过程复杂且效率低下。现代可微分软件栈为物理逆问题提供了更高效的优化工作流开发途径。

Method: 使用JAX-FEM进行自动微分实现边界导纳的直接梯度估计，结合PyTorch3D进行网格操作，通过分离物理驱动的边界优化和几何驱动的内部网格适应，应用随机有限差分进行声学形状优化。

Result: 边界导纳估计达到3位精度，形状优化在目标频率实现48.1%能量减少，相比标准有限差分方法减少30倍FEM求解次数。

Conclusion: 现代可微分软件栈能够快速原型化物理逆问题的优化工作流，自动微分适用于参数估计，有限差分与自动微分结合适用于几何设计。

Abstract: We demonstrate a practical differentiable programming approach for acoustic inverse problems through two applications: admittance estimation and shape optimization for resonance damping. First, we show that JAX-FEM's automatic differentiation (AD) enables direct gradient-based estimation of complex boundary admittance from sparse pressure measurements, achieving 3-digit precision without requiring manual derivation of adjoint equations. Second, we apply randomized finite differences to acoustic shape optimization, combining JAX-FEM for forward simulation with PyTorch3D for mesh manipulation through AD. By separating physics-driven boundary optimization from geometry-driven interior mesh adaptation, we achieve 48.1% energy reduction at target frequencies with 30-fold fewer FEM solutions compared to standard finite difference on the full mesh. This work showcases how modern differentiable software stacks enable rapid prototyping of optimization workflows for physics-based inverse problems, with automatic differentiation for parameter estimation and a combination of finite differences and AD for geometric design.

</details>


### [68] [Low-Bit, High-Fidelity: Optimal Transport Quantization for Flow Matching](https://arxiv.org/abs/2511.11418)
*Dara Varam,Diaa A. Abuhani,Imran Zualkernan,Raghad AlDamani,Lujain Khalil*

Main category: cs.LG

TL;DR: 本文研究了Flow Matching生成模型的量化压缩问题，提出基于最优传输的后训练量化方法，在2-3位精度下保持生成质量，优于传统量化方案。


<details>
  <summary>Details</summary>
Motivation: Flow Matching模型虽然具有高效的无模拟训练和确定性采样优势，但其实际部署受到高精度参数需求的挑战，需要有效的压缩方法以适应边缘和嵌入式AI应用。

Method: 采用基于最优传输的后训练量化方法，最小化量化权重与原始权重之间的2-Wasserstein距离，并与均匀、分段和对数量化方案进行系统比较。

Result: 在五个不同复杂度的基准数据集上的实证结果表明，基于最优传输的量化在2-3位参数精度下仍能保持视觉生成质量和潜在空间稳定性，而其他方法在此精度下失效。

Conclusion: 基于最优传输的量化是一种有原则且有效的方法，可用于压缩Flow Matching生成模型，适用于边缘和嵌入式AI应用。

Abstract: Flow Matching (FM) generative models offer efficient simulation-free training and deterministic sampling, but their practical deployment is challenged by high-precision parameter requirements. We adapt optimal transport (OT)-based post-training quantization to FM models, minimizing the 2-Wasserstein distance between quantized and original weights, and systematically compare its effectiveness against uniform, piecewise, and logarithmic quantization schemes. Our theoretical analysis provides upper bounds on generative degradation under quantization, and empirical results across five benchmark datasets of varying complexity show that OT-based quantization preserves both visual generation quality and latent space stability down to 2-3 bits per parameter, where alternative methods fail. This establishes OT-based quantization as a principled, effective approach to compress FM generative models for edge and embedded AI applications.

</details>


### [69] [DiffPro: Joint Timestep and Layer-Wise Precision Optimization for Efficient Diffusion Inference](https://arxiv.org/abs/2511.11446)
*Farhana Amin,Sabiha Afroz,Kanchon Gharami,Mona Moghadampanah,Dimitrios S. Nikolopoulos*

Main category: cs.LG

TL;DR: DiffPro是一个后训练框架，通过联合优化扩散模型的时间步长和逐层精度，在不进行训练的情况下实现6.25倍模型压缩、50%时间步减少和2.8倍推理加速，同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然能生成高质量图像，但由于需要大量去噪步骤和矩阵运算，推理成本高昂。需要一种方法在不重新训练的情况下减少延迟和内存使用。

Method: DiffPro结合三个组件：基于流形感知的敏感度指标分配权重位宽、动态激活量化稳定跨时间步的激活、基于师生漂移的预算时间步选择器。

Result: 在标准基准测试中，DiffPro实现Delta FID <= 10的情况下，达到6.25倍模型压缩、50%时间步减少和2.8倍推理加速。

Conclusion: DiffPro将步长减少和精度规划统一为单一可部署计划，为实时节能的扩散推理提供实用效率提升。

Abstract: Diffusion models produce high quality images but inference is costly due to many denoising steps and heavy matrix operations. We present DiffPro, a post-training, hardware-faithful framework that works with the exact integer kernels used in deployment and jointly tunes timesteps and per-layer precision in Diffusion Transformers (DiTs) to reduce latency and memory without any training. DiffPro combines three parts: a manifold-aware sensitivity metric to allocate weight bits, dynamic activation quantization to stabilize activations across timesteps, and a budgeted timestep selector guided by teacher-student drift. In experiments DiffPro achieves up to 6.25x model compression, fifty percent fewer timesteps, and 2.8x faster inference with Delta FID <= 10 on standard benchmarks, demonstrating practical efficiency gains. DiffPro unifies step reduction and precision planning into a single budgeted deployable plan for real-time energy-aware diffusion inference.

</details>


### [70] [FairReweighing: Density Estimation-Based Reweighing Framework for Improving Separation in Fair Regression](https://arxiv.org/abs/2511.11459)
*Xiaoyin Xi,Zhe Yu*

Main category: cs.LG

TL;DR: 本文提出了一种基于密度估计的FairReweighing预处理算法，用于解决回归任务中的公平性问题，该算法能保证分离准则并在保持高准确性的同时改善公平性。


<details>
  <summary>Details</summary>
Motivation: AI软件在公共部门和工业应用中广泛使用，但缺乏透明度引发了对公平性的担忧。现有研究主要集中在二元分类任务上，回归任务中的公平性问题相对未被充分探索。

Method: 采用基于互信息的指标评估分离违规，并扩展到分类和回归问题。提出基于密度估计的FairReweighing预处理算法，确保学习模型满足分离准则。

Result: 理论证明在数据独立性假设下，FairReweighing算法能保证训练数据的分离。实证研究表明，在合成和真实数据上，该算法在改善分离的同时保持高准确性，优于现有最先进的回归公平性解决方案。

Conclusion: FairReweighing算法为回归任务中的公平性问题提供了有效的解决方案，在理论和实证上都表现出优越性能。

Abstract: There has been a prevalence of applying AI software in both high-stakes public-sector and industrial contexts. However, the lack of transparency has raised concerns about whether these data-informed AI software decisions secure fairness against people of all racial, gender, or age groups. Despite extensive research on emerging fairness-aware AI software, up to now most efforts to solve this issue have been dedicated to binary classification tasks. Fairness in regression is relatively underexplored. In this work, we adopted a mutual information-based metric to assess separation violations. The metric is also extended so that it can be directly applied to both classification and regression problems with both binary and continuous sensitive attributes. Inspired by the Reweighing algorithm in fair classification, we proposed a FairReweighing pre-processing algorithm based on density estimation to ensure that the learned model satisfies the separation criterion. Theoretically, we show that the proposed FairReweighing algorithm can guarantee separation in the training data under a data independence assumption. Empirically, on both synthetic and real-world data, we show that FairReweighing outperforms existing state-of-the-art regression fairness solutions in terms of improving separation while maintaining high accuracy.

</details>


### [71] [MoCap2Radar: A Spatiotemporal Transformer for Synthesizing Micro-Doppler Radar Signatures from Motion Capture](https://arxiv.org/abs/2511.11462)
*Kevin Chen,Kenneth W. Parker,Anish Arora*

Main category: cs.LG

TL;DR: 提出了一种基于纯机器学习的从运动捕捉数据合成雷达频谱图的方法，使用基于transformer的模型将MoCap数据转换为多普勒雷达频谱图。


<details>
  <summary>Details</summary>
Motivation: 利用更丰富的运动捕捉数据来增强稀缺的雷达数据集，为更高级应用提供训练数据，同时相比基于物理的方法需要更少的计算资源。

Method: 将MoCap到频谱图转换建模为窗口序列到序列任务，使用基于transformer的模型联合捕捉MoCap标记之间的空间关系和跨帧的时间动态。

Result: 实验表明该方法能够生成视觉和数量上合理的多普勒雷达频谱图，并具有良好的泛化能力。消融实验显示模型具备将多部位运动转换为多普勒特征的能力，并理解人体不同部位之间的空间关系。

Conclusion: 这是使用transformer进行时间序列信号处理的有趣示例，特别适用于边缘计算和物联网雷达，能够使用更丰富的MoCap数据增强稀缺的雷达数据集，且计算量远小于基于物理的方法。

Abstract: We present a pure machine learning process for synthesizing radar spectrograms from Motion-Capture (MoCap) data. We formulate MoCap-to-spectrogram translation as a windowed sequence-to-sequence task using a transformer-based model that jointly captures spatial relations among MoCap markers and temporal dynamics across frames. Real-world experiments show that the proposed approach produces visually and quantitatively plausible doppler radar spectrograms and achieves good generalizability. Ablation experiments show that the learned model includes both the ability to convert multi-part motion into doppler signatures and an understanding of the spatial relations between different parts of the human body.
  The result is an interesting example of using transformers for time-series signal processing. It is especially applicable to edge computing and Internet of Things (IoT) radars. It also suggests the ability to augment scarce radar datasets using more abundant MoCap data for training higher-level applications. Finally, it requires far less computation than physics-based methods for generating radar data.

</details>


### [72] [Quantifying and Improving Adaptivity in Conformal Prediction through Input Transformations](https://arxiv.org/abs/2511.11472)
*Sooyong Jang,Insup Lee*

Main category: cs.LG

TL;DR: 本文提出了一种新的自适应预测集评估方法，通过输入变换和均匀质量分箱来更准确地评估方法的适应性，并基于此提出了一种新的自适应预测集算法。


<details>
  <summary>Details</summary>
Motivation: 现有适应性评估方法存在分箱不平衡问题，导致覆盖率和集合大小估计不准确，需要更可靠的评估指标。

Method: 提出利用输入变换按难度排序样本，然后进行均匀质量分箱的方法，并基于此引入两个新指标来评估适应性。此外提出了一种新的自适应预测集算法，按估计难度分组并应用组条件共形预测。

Result: 实验表明，新提出的指标与期望的适应性属性相关性更强。在图像分类和医疗任务上的实验结果显示，新方法在各项新指标上优于现有方法。

Conclusion: 提出的分箱方法和评估指标能够更准确地评估预测集的适应性，新的自适应预测集算法在多个任务上表现出优越性能。

Abstract: Conformal prediction constructs a set of labels instead of a single point prediction, while providing a probabilistic coverage guarantee. Beyond the coverage guarantee, adaptiveness to example difficulty is an important property. It means that the method should produce larger prediction sets for more difficult examples, and smaller ones for easier examples. Existing evaluation methods for adaptiveness typically analyze coverage rate violation or average set size across bins of examples grouped by difficulty. However, these approaches often suffer from imbalanced binning, which can lead to inaccurate estimates of coverage or set size. To address this issue, we propose a binning method that leverages input transformations to sort examples by difficulty, followed by uniform-mass binning. Building on this binning, we introduce two metrics to better evaluate adaptiveness. These metrics provide more reliable estimates of coverage rate violation and average set size due to balanced binning, leading to more accurate adaptivity assessment. Through experiments, we demonstrate that our proposed metric correlates more strongly with the desired adaptiveness property compared to existing ones. Furthermore, motivated by our findings, we propose a new adaptive prediction set algorithm that groups examples by estimated difficulty and applies group-conditional conformal prediction. This allows us to determine appropriate thresholds for each group. Experimental results on both (a) an Image Classification (ImageNet) (b) a medical task (visual acuity prediction) show that our method outperforms existing approaches according to the new metrics.

</details>


### [73] [Data-efficient U-Net for Segmentation of Carbide Microstructures in SEM Images of Steel Alloys](https://arxiv.org/abs/2511.11485)
*Alinda Ezgi Gerçek,Till Korten,Paul Chekhonin,Maleeha Hassan,Peter Steinbach*

Main category: cs.LG

TL;DR: 提出了一种数据高效的轻量级U-Net分割管道，仅使用10张标注的扫描电镜图像就能准确分割反应堆压力容器钢中的碳化物，Dice-Sørensen系数达到0.98，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 反应堆压力容器钢的微观结构理解对预测机械性能至关重要，但扫描电镜图像中碳化物与基体的灰度值重叠使得简单阈值分割无效，需要更有效的分割方法。

Method: 使用轻量级U-Net架构（30.7M参数），仅训练10张标注的扫描电镜图像，实现数据高效的分割管道。

Result: 模型在有限数据下达到Dice-Sørensen系数0.98，显著优于冶金领域最先进方法（传统图像分析：0.85），同时将标注工作量减少一个数量级。

Conclusion: 该方法能够快速自动量化碳化物用于合金设计，并能泛化到其他钢种，展示了数据高效深度学习在反应堆压力容器钢分析中的潜力。

Abstract: Understanding reactor-pressure-vessel steel microstructure is crucial for predicting mechanical properties, as carbide precipitates both strengthen the alloy and can initiate cracks. In scanning electron microscopy images, gray-value overlap between carbides and matrix makes simple thresholding ineffective. We present a data-efficient segmentation pipeline using a lightweight U-Net (30.7~M parameters) trained on just \textbf{10 annotated scanning electron microscopy images}. Despite limited data, our model achieves a \textbf{Dice-Sørensen coefficient of 0.98}, significantly outperforming the state-of-the-art in the field of metallurgy (classical image analysis: 0.85), while reducing annotation effort by one order of magnitude compared to the state-of-the-art data efficient segmentation model. This approach enables rapid, automated carbide quantification for alloy design and generalizes to other steel types, demonstrating the potential of data-efficient deep learning in reactor-pressure-vessel steel analysis.

</details>


### [74] [FarSkip-Collective: Unhobbling Blocking Communication in Mixture of Experts Models](https://arxiv.org/abs/2511.11505)
*Yonatan Dukler,Guihong Li,Deval Shah,Vikram Appia,Emad Barsoum*

Main category: cs.LG

TL;DR: FarSkip-Collective通过修改现代模型架构，在MoEs分布式运行中实现计算与通信的重叠，成功转换了16B到109B参数的大模型，在保持精度的同时显著加速训练和推理。


<details>
  <summary>Details</summary>
Motivation: 在分布式环境中运行MoEs时，阻塞通信是主要效率瓶颈，需要解决计算与通信无法重叠的问题。

Method: 修改模型架构，跳过连接层，通过自蒸馏技术转换大模型，并优化实现以显式重叠通信与计算。

Result: 成功转换了16B到109B参数的一系列最先进模型，Llama 4 Scout(109B)在广泛下游评估中平均精度仅比原始版本低1%，同时实现了训练和推理的加速。

Conclusion: FarSkip-Collective方法有效解决了MoEs分布式运行中的通信瓶颈，在保持模型能力的同时显著提升了效率。

Abstract: Blocking communication presents a major hurdle in running MoEs efficiently in distributed settings. To address this, we present FarSkip-Collective which modifies the architecture of modern models to enable overlapping of their computation with communication. Our approach modifies the architecture to skip connections in the model and it is unclear a priori whether the modified model architecture can remain as capable, especially for large state-of-the-art models and while modifying all of the model layers. We answer this question in the affirmative and fully convert a series of state-of-the-art models varying from 16B to 109B parameters to enable overlapping of their communication while achieving accuracy on par with their original open-source releases. For example, we convert Llama 4 Scout (109B) via self-distillation and achieve average accuracy within 1% of its instruction tuned release averaged across a wide range of downstream evaluations. In addition to demonstrating retained accuracy of the large modified models, we realize the benefits of FarSkip-Collective through optimized implementations that explicitly overlap communication with computation, accelerating both training and inference in existing frameworks.

</details>


### [75] [Generalizing Fair Clustering to Multiple Groups: Algorithms and Applications](https://arxiv.org/abs/2511.11539)
*Diptarka Chakraborty,Kushagra Chatterjee,Debarati Das,Tien-Long Nguyen*

Main category: cs.LG

TL;DR: 本文研究了多组（超过两组）情况下的最接近公平聚类问题，提出了近似算法，并应用于公平相关聚类和公平共识聚类问题。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法在处理具有多个受保护属性（如年龄、种族、性别等）的数据时，往往无法公平地代表各个边缘化群体。现有研究仅限于两组情况，而实际数据通常涉及多个群体。

Method: 将最接近公平聚类问题推广到任意数量组的情况，证明该问题在NP难解，并提出近线性时间的近似算法来处理任意大小的多组数据。

Result: 提出了高效的多组最接近公平聚类近似算法，改进了公平相关聚类的近似保证，并首次为多组公平共识聚类问题提供了近似算法。

Conclusion: 本研究解决了Chakraborty等人提出的开放性问题，为多组公平聚类提供了有效的算法解决方案，推动了公平机器学习的发展。

Abstract: Clustering is a fundamental task in machine learning and data analysis, but it frequently fails to provide fair representation for various marginalized communities defined by multiple protected attributes -- a shortcoming often caused by biases in the training data. As a result, there is a growing need to enhance the fairness of clustering outcomes, ideally by making minimal modifications, possibly as a post-processing step after conventional clustering. Recently, Chakraborty et al. [COLT'25] initiated the study of \emph{closest fair clustering}, though in a restricted scenario where data points belong to only two groups. In practice, however, data points are typically characterized by many groups, reflecting diverse protected attributes such as age, ethnicity, gender, etc.
  In this work, we generalize the study of the \emph{closest fair clustering} problem to settings with an arbitrary number (more than two) of groups. We begin by showing that the problem is NP-hard even when all groups are of equal size -- a stark contrast with the two-group case, for which an exact algorithm exists. Next, we propose near-linear time approximation algorithms that efficiently handle arbitrary-sized multiple groups, thereby answering an open question posed by Chakraborty et al. [COLT'25].
  Leveraging our closest fair clustering algorithms, we further achieve improved approximation guarantees for the \emph{fair correlation clustering} problem, advancing the state-of-the-art results established by Ahmadian et al. [AISTATS'20] and Ahmadi et al. [2020]. Additionally, we are the first to provide approximation algorithms for the \emph{fair consensus clustering} problem involving multiple (more than two) groups, thus addressing another open direction highlighted by Chakraborty et al. [COLT'25].

</details>


### [76] [Multistability of Self-Attention Dynamics in Transformers](https://arxiv.org/abs/2511.11553)
*Claudio Altafini*

Main category: cs.LG

TL;DR: 本文揭示了自注意力动力学与多智能体Oja流的关系，将单头自注意力系统的平衡点分为四类：共识、二分共识、聚类和多边形平衡点，其中前三类常存在多个渐近稳定平衡点。


<details>
  <summary>Details</summary>
Motivation: 研究自注意力动力学与多智能体Oja流的关联，深入理解transformer注意力机制中的平衡点特性和稳定性。

Method: 将自注意力动力学建模为连续时间多智能体系统，分析其与多智能体Oja流的关系，并对单头自注意力系统的平衡点进行分类研究。

Result: 发现自注意力系统存在四类平衡点，其中前三类常共存多个渐近稳定平衡点；共识和二分共识平衡点总是与值矩阵的特征向量对齐，通常但不总是与主特征向量对齐。

Conclusion: 自注意力动力学与多智能体Oja流密切相关，系统表现出丰富的平衡点行为，为理解transformer注意力机制提供了新的理论视角。

Abstract: In machine learning, a self-attention dynamics is a continuous-time multiagent-like model of the attention mechanisms of transformers. In this paper we show that such dynamics is related to a multiagent version of the Oja flow, a dynamical system that computes the principal eigenvector of a matrix corresponding for transformers to the value matrix. We classify the equilibria of the ``single-head'' self-attention system into four classes: consensus, bipartite consensus, clustering and polygonal equilibria. Multiple asymptotically stable equilibria from the first three classes often coexist in the self-attention dynamics. Interestingly, equilibria from the first two classes are always aligned with the eigenvectors of the value matrix, often but not exclusively with the principal eigenvector.

</details>


### [77] [Optimizing Mixture of Block Attention](https://arxiv.org/abs/2511.11571)
*Guangxuan Xiao,Junxian Guo,Kasra Mazaheri,Song Han*

Main category: cs.LG

TL;DR: 本文分析了MoBA（混合块注意力）机制，开发了统计模型揭示其性能关键取决于路由器准确区分相关和无关块的能力，提出了改进方法（小分块和键卷积），并实现了高效的GPU实现FlashMoBA。


<details>
  <summary>Details</summary>
Motivation: MoBA虽然能高效处理长上下文，但其设计原理不明确且缺乏高效GPU实现，阻碍了实际应用。本文旨在深入理解MoBA机制并解决其实现效率问题。

Method: 开发统计模型分析MoBA机制，推导信号噪声比连接架构参数与检索精度；提出使用小分块和键卷积来改进路由精度；设计硬件感知的CUDA内核FlashMoBA实现高效执行。

Result: 改进后的MoBA模型在从头训练LLM时达到密集注意力基线的性能；FlashMoBA相比FlashAttention-2在小分块情况下实现最高14.7倍加速。

Conclusion: 通过理论分析和硬件优化，成功提升了MoBA的性能和效率，使其成为实用的长上下文处理解决方案。

Abstract: Mixture of Block Attention (MoBA) (Lu et al., 2025) is a promising building block for efficiently processing long contexts in LLMs by enabling queries to sparsely attend to a small subset of key-value blocks, drastically reducing computational cost. However, the design principles governing MoBA's performance are poorly understood, and it lacks an efficient GPU implementation, hindering its practical adoption. In this paper, we first develop a statistical model to analyze MoBA's underlying mechanics. Our model reveals that performance critically depends on the router's ability to accurately distinguish relevant from irrelevant blocks based on query-key affinities. We derive a signal-to-noise ratio that formally connects architectural parameters to this retrieval accuracy. Guided by our analysis, we identify two key pathways for improvement: using smaller block sizes and applying a short convolution on keys to cluster relevant signals, which enhances routing accuracy. While theoretically better, small block sizes are inefficient on GPUs. To bridge this gap, we introduce FlashMoBA, a hardware-aware CUDA kernel that enables efficient MoBA execution even with the small block sizes our theory recommends. We validate our insights by training LLMs from scratch, showing that our improved MoBA models match the performance of dense attention baselines. FlashMoBA achieves up to 14.7x speedup over FlashAttention-2 for small blocks, making our theoretically-grounded improvements practical. Code is available at: https://github.com/mit-han-lab/flash-moba.

</details>
