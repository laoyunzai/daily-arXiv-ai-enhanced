{"id": "2602.20174", "categories": ["cond-mat.str-el", "hep-th"], "pdf": "https://arxiv.org/pdf/2602.20174", "abs": "https://arxiv.org/abs/2602.20174", "authors": ["C. A. Lütken"], "title": "Elliptic mirror of the quantum Hall effect", "comment": "31 pages, 12 figures", "summary": "Toroidal sigma models of magneto-transport are analyzed, in which integer and fractional quantum Hall effects automatically are unified by a {holomorphic modular symmetry}. By exploiting a quantum equivalence called \\emph{mirror symmetry}, these models are mapped to tractable mirror models (also elliptic), in which topological protection is provided by more familiar winding numbers. Phase diagrams and scaling properties of elliptic models are compared to some of the experimental and numerical data accumulated over the past three decades. The geometry of scaling flows extracted from quantum Hall experiments is in good agreement with modular predictions, including the location of many quantum critical points. One conspicuous model %(arguably the simplest and most natural one) has a critical delocalization exponent $ν_{\\rm tor} = 18 \\ln 2 /(π^2 G^4) = 2.6051\\dots$ ($G$ is Gauss' constant) that is in excellent agreement with the value $ν_{\\rm num} = 2.607\\pm\\,.004$ calculated in the numerical Chalker-Coddington model, suggesting that these models are in the same universality class. The real delocalization exponent may be disentangled from other scaling exponents in finite size scaling experiments, giving an experimental value $ν_{\\rm exp} = 2.3\\pm 0.2$. The modular model suggests how these theoretical and experimental results may be reconciled, but in order to determine if these theoretical models really are in the quantum Hall universality class, improved finite size scaling experiments are urgently needed."}
{"id": "2602.20560", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.20560", "abs": "https://arxiv.org/abs/2602.20560", "authors": ["Yi-Ming Liu", "Wei-Qiang Chen", "Zheng-Cheng Gu"], "title": "Real-space construction and classification for time-reversal symmetric crystalline superconductors in 2D interacting fermionic systems", "comment": null, "summary": "Crystalline symmetry and time-reversal symmetry are commonly present in real superconducting materials. However, the topological classification of systems respecting these symmetries, particularly for interacting fermions, remains incomplete. In this work, we systematically classify time-reversal symmetry-protected crystalline topological superconductors in two-dimensional interacting fermionic systems using an explicit real-space construction. Among the resulting phases, we identify intrinsically interacting fermionic topological superconductors, i.e., phases that cannot be realized in either free-fermion or interacting bosonic systems. For spinless fermions with protecting symmetry group $C_4 \\times Z_2^T$ or $D_4 \\times Z_2^T$ (plus fermion parity), the intrinsic sector has a $Z_4$ classification. The corresponding root phases generating this $Z_4$ classification admit a transparent real-space construction in terms of decorated 1D blocks. These blocks are 1D fermionic symmetry-protected topological (FSPT) phases, realizable as double Majorana chains. We further find the corresponding $Z_4$ spinless intrinsic phases for wallpaper groups $p4$, $p4m$, and $p4g$. We also find an additional $Z_2$ intrinsically interacting phase for spinless fermions with wallpaper group $pm$, which is absent with the corresponding point-group symmetry alone. Moreover, these intrinsic phases naturally give rise to higher-order FSPT phases that support corner zero modes. Finally, we verify the crystalline equivalence principle for generic 2D interacting FSPT systems with both crystalline and internal symmetries."}
{"id": "2602.20600", "categories": ["cond-mat.str-el", "cond-mat.mes-hall", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.20600", "abs": "https://arxiv.org/abs/2602.20600", "authors": ["Debraj Debata", "Abhirup Mukherjee", "Siddhartha Lal"], "title": "Kondo breakdown as an entanglement transition driven by continuous measurement", "comment": "28 pages, 19 Figures", "summary": "We study the breakdown of Kondo screening by a local magnetic field from the perspective of a measurement-driven entanglement transition in a monitored quantum system. Here, the Kondo coupling leads to the growth in entanglement of an impurity spin with it's fermionic environment, while the local field plays the role of a continuous observer. Using a non-perturbative Unitary Renormalization Group (URG) approach, we derive coupled renormalization-group flow equations for the Kondo exchange and the local field, and obtain a field-dependent RG phase diagram. The RG flows separate a low-energy Kondo-screened phase, where the impurity is absorbed into the Fermi sea and forms an entangled singlet with the conduction bath, from a polarized local-moment phase in which screening is frustrated and impurity-bath entanglement is suppressed. We identify the fixed-point Hamiltonians governing the two phases and the critical regime, and relate the transition to the emergence of a novel non-Fermi liquid. Various impurity signatures such as the spectral function and thermalisation of impurity observables are used to characterise this entanglement transition. These results offer insight into the interplay of decoherence and measurement in governing the dynamics of a prototypical quantum system."}
{"id": "2602.20814", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.20814", "abs": "https://arxiv.org/abs/2602.20814", "authors": ["Dmitry M. Korotin", "Anna A. Anisimova", "Vladimir I. Anisimov"], "title": "Parameterizing DFT+U+V from Hybrid Functionals: A Wannier-Function-Based Approach for Strongly Correlated Materials", "comment": null, "summary": "We present an approach to parameterize DFT+$U$+$V$ from hybrid-functional calculations using Wannier-function projections. The method constructs a common localized Wannier basis for both semilocal DFT and hybrid-functional calculations, then determines effective on-site ($U$) and intersite ($V$) Hubbard parameters by minimizing the Hamiltonian mismatch within the correlated subspace. This procedure yields interaction parameters that reproduce the hybrid-functional electronic structure at a fraction of the computational cost and allow efficient structural relaxations and further many-body calculations. We validate the workflow on three oxide systems with different electronic characters: MgO (wide-gap insulator), NiO (antiferromagnetic charge-transfer insulator), and V$_2$O$_5$ (d$^0$ transition-metal oxide). In all cases, the mapped DFT+$U$+$V$ parameters reproduce hybrid-functional band gaps, densities of states, and magnetic moments and improve upon semilocal DFT while maintaining computational efficiency."}
{"id": "2602.20303", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20303", "abs": "https://arxiv.org/abs/2602.20303", "authors": ["Joyanta Jyoti Mondal"], "title": "Multilevel Determinants of Overweight and Obesity Among U.S. Children Aged 10-17: Comparative Evaluation of Statistical and Machine Learning Approaches Using the 2021 National Survey of Children's Health", "comment": null, "summary": "Background: Childhood and adolescent overweight and obesity remain major public health concerns in the United States and are shaped by behavioral, household, and community factors. Their joint predictive structure at the population level remains incompletely characterized. Objectives: The study aims to identify multilevel predictors of overweight and obesity among U.S. adolescents and compare the predictive performance, calibration, and subgroup equity of statistical, machine-learning, and deep-learning models. Data and Methods: We analyze 18,792 children aged 10-17 years from the 2021 National Survey of Children's Health. Overweight/obesity is defined using BMI categories. Predictors included diet, physical activity, sleep, parental stress, socioeconomic conditions, adverse experiences, and neighborhood characteristics. Models include logistic regression, random forest, gradient boosting, XGBoost, LightGBM, multilayer perceptron, and TabNet. Performance is evaluated using AUC, accuracy, precision, recall, F1 score, and Brier score. Results: Discrimination range from 0.66 to 0.79. Logistic regression, gradient boosting, and MLP showed the most stable balance of discrimination and calibration. Boosting and deep learning modestly improve recall and F1 score. No model was uniformly superior. Performance disparities across race and poverty groups persist across algorithms. Conclusion: Increased model complexity yields limited gains over logistic regression. Predictors consistently span behavioral, household, and neighborhood domains. Persistent subgroup disparities indicate the need for improved data quality and equity-focused surveillance rather than greater algorithmic complexity."}
{"id": "2602.20171", "categories": ["quant-ph", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.20171", "abs": "https://arxiv.org/abs/2602.20171", "authors": ["Shangzhou Xia", "Haitao Fu", "Jianjun Zhao"], "title": "QSolver: A Quantum Constraint Solver", "comment": null, "summary": "With the growing interest in quantum programs, ensuring their correctness is a fundamental challenge. Although constraint-solving techniques can overcome some limitations of traditional testing and verification, they have not yet been sufficiently explored in the context of quantum programs. To address this gap, we present QSolver, the first quantum constraint solver. QSolver provides a structured framework for handling five types of quantum constraints and incorporates an automated assertion generation module to verify quantum states. QSolver transforms quantum programs and multi-moment constraints into symbolic representations, and utilizes an SMT solver to obtain quantum states that satisfy these constraints. To validate the correctness of the generated input states, QSolver automatically generates assertion programs corresponding to each constraint. Experimental results show that QSolver efficiently processes commonly used quantum gates and demonstrates good scalability across quantum programs of different sizes."}
{"id": "2602.20175", "categories": ["cs.LG", "math.OC", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.20175", "abs": "https://arxiv.org/abs/2602.20175", "authors": ["Ryo Sakai", "Chen-Yu Liu"], "title": "Tensor Network Generator-Enhanced Optimization for Traveling Salesman Problem", "comment": "11 pages, 7 figures", "summary": "We present an application of the tensor network generator-enhanced optimization (TN-GEO) framework to address the traveling salesman problem (TSP), a fundamental combinatorial optimization challenge. Our approach employs a tensor network Born machine based on automatically differentiable matrix product states (MPS) as the generative model, using the Born rule to define probability distributions over candidate solutions. Unlike approaches based on binary encoding, which require $N^2$ variables and penalty terms to enforce valid tour constraints, we adopt a permutation-based formulation with integer variables and use autoregressive sampling with masking to guarantee that every generated sample is a valid tour by construction. We also introduce a $k$-site MPS variant that learns distributions over $k$-grams (consecutive city subsequences) using a sliding window approach, enabling parameter-efficient modeling for larger instances. Experimental validation on TSPLIB benchmark instances with up to 52 cities demonstrates that TN-GEO can outperform classical heuristics including swap and 2-opt hill-climbing. The $k$-site variants, which put more focus on local correlations, show better results compared to the full-MPS case."}
{"id": "2602.20620", "categories": ["cond-mat.dis-nn", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.20620", "abs": "https://arxiv.org/abs/2602.20620", "authors": ["Munetaka Sasaki"], "title": "Construction of a Neural Network with Temperature-Dependent Recall Patterns", "comment": "6 pages, 10 figures", "summary": "We present a simple model that recalls two different patterns depending on the temperature. To realize a change in recall pattern due to temperature change, we embed two patterns to different graphs: the first pattern into a fully connected graph and the second pattern into a sparse graph. Because a fully connected graph is more resistant to thermal fluctuations than a sparse graph, we can realize a change in recall pattern by tuning relative weights of the two patterns properly. We demonstrate by equilibrium Monte-Carlo simulations that such a temperature-dependent change in recall patterns does occur in our model. Simulation results strongly indicate that the system undergoes a first-order phase transition when the change in recall patterns occurs. It is also demonstrated by annealing simulations that the system fails to recall the pattern embedded in the sparse graph at low temperatures if the free-energy barrier is too high to overcome within the given simulation timescale."}
{"id": "2602.20682", "categories": ["nlin.CD"], "pdf": "https://arxiv.org/pdf/2602.20682", "abs": "https://arxiv.org/abs/2602.20682", "authors": ["L. Salasnich", "F. Sattin"], "title": "Geometric investigation of chaos unfolding in Hamiltonian systems", "comment": null, "summary": "In this work we revisit the geometric approach to chaos in Hamiltonian dynamics, by means of the Jacobi-Levi-Civita equation (JLCE). We inspect numerically two low-dimensional dynamical systems; show that, along chaotic orbits, the exponential divergence between nearby trajectories quantified by the JLCE does not unfold in a continuous manner, rather is closer to a multiplicative discrete process: in correspondence of each turning point, where the trajectory bounces away from the boundary of the energetically allowed region, the relative separation increases sharply and abruptly. We highlight through analytical and numerical arguments that the chaotic rather than regular nature of the trajectory is determined by the details of the scattering with the boundary, and interpret these results in terms of parametric resonance theory, and specifically the Mathieu equation."}
{"id": "2602.20505", "categories": ["nlin.AO"], "pdf": "https://arxiv.org/pdf/2602.20505", "abs": "https://arxiv.org/abs/2602.20505", "authors": ["Yoshiki Kuramoto"], "title": "Half a century of the theory of synchronization", "comment": "10 pages, no figure, article submitted to the Statphys 19", "summary": "This review offers a retrospective of the development of the theory of coupled oscillators and synchronization over the past half century. Among the various works made by myself during this period, the following three specific works will be focused on, serving as some key points to illustrate the field's evolution. They are the derivation of (1) a simple partial differential equation exhibiting spatio-tempoeral chaos (Kuramoto-Sivashinsky equaiton), (2) a solvable mathematical model describing synchronization phase transition (Kuramoto model), and the discovery of (3) coexistence of coherence and incoherence in nonlocally coupled oscillators (chimera states). It is emphasized that all these works resulted fron the phase reduction of the complex Ginzburg-Landau equation (or its variants), the equation which was derived with a coworker in 1974 from a certain reaction-diffusion model. A quick overview will also be made on how the above three works influenced the subsequent development of the field of coupled oscillators and synchronization. Finally, a few comments will be made on how the methods of dynamical reduction, such as the center-manifold reduction and phase reduction, are crucial for exploring this field in depth. This article is a largely faithful reproduction of the content presented in my award lecture."}
{"id": "2602.20256", "categories": ["cond-mat.stat-mech", "cond-mat.dis-nn", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.20256", "abs": "https://arxiv.org/abs/2602.20256", "authors": ["Feng He", "Arthur Hutsalyuk", "Giuseppe Mussardo", "Andrea Stampiggi"], "title": "Spectral Decimation of Quantum Many-Body Hamiltonians", "comment": "16+6 pages; 5+3 figures", "summary": "We develop a systematic theory of spectral decimation for quantum many-body Hamiltonians and show that it provides a quantitative probe of emergent symmetries in statistically mixed spectra. Building on an analytical description of statistical mixtures, we derive an explicit expression for the size of a characteristic symmetry sector (CSS), defined as the largest subsequence of levels exhibiting non-Poissonian correlations. The CSS dimension is shown to be the size-biased average of the underlying symmetry sectors, establishing a direct link between spectral statistics and Hilbert-space structure. We apply this framework to two paradigmatic settings: Hilbert-space fragmentation and disorder-induced many-body localization (MBL). In fragmented systems, the CSS reproduces the mixture prediction and isolates correlated subsectors even when the full spectrum appears nearly Poissonian. In the disordered Heisenberg chain, spectral decimation reveals the gradual emergence of integrability through a shrinking CSS, whose statistics exhibit signatures consistent with local integrals of motion. We introduce a characteristic symmetry entropy (CSE) as a finite-size scaling observable and extract, within accessible system sizes, the crossover exponents. Our results establish spectral decimation as a controlled, unbiased and computationally inexpensive diagnostic of hidden structure in many-body spectra, capable of distinguishing between chaotic dynamics, statistical mixtures, and emergent integrability."}
{"id": "2602.20832", "categories": ["cs.CC"], "pdf": "https://arxiv.org/pdf/2602.20832", "abs": "https://arxiv.org/abs/2602.20832", "authors": ["Amir Shpilka", "Yann Tal"], "title": "Polynomial Identity Testing and Reconstruction for Depth-4 Powering Circuits of High Degree", "comment": null, "summary": "We study deterministic polynomial identity testing (PIT) and reconstruction algorithms for depth-$4$ arithmetic circuits of the form \\[ Σ^{[r]}\\!\\wedge^{[d]}\\!Σ^{[s]}\\!Π^{[δ]}. \\] This model generalizes Waring decompositions and diagonal circuits, and captures sums of powers of low-degree sparse polynomials. Specifically, each circuit computes a sum of $r$ terms, where each term is a $d$-th power of an $s$-sparse polynomial of degree $δ$. This model also includes algebraic representations that arise in tensor decomposition and moment-based learning tasks such as mixture models and subspace learning.\n  We give deterministic worst-case algorithms for PIT and reconstruction in this model. Our PIT construction applies when $d>r^2$ and yields explicit hitting sets of size $O(r^4 s^4 n^2 d δ^3)$. The reconstruction algorithm runs in time $\\textrm{poly}(n,s,d)$ under the condition $d=Ω(r^4δ)$, and in particular it tolerates polynomially large top fan-in $r$ and bottom degree $δ$.\n  Both results hold over fields of characteristic zero and over fields of sufficiently large characteristic. These algorithms provide the first polynomial-time deterministic solutions for depth-$4$ powering circuits with unbounded top fan-in. In particular, the reconstruction result improves upon previous work which required non-degeneracy or average-case assumptions.\n  The PIT construction relies on the ABC theorem for function fields (Mason-Stothers theorem), which ensures linear independence of high-degree powers of sparse polynomials after a suitable projection. The reconstruction algorithm combines this with Wronskian-based differential operators, structural properties of their kernels, and a robust version of the Klivans-Spielman hitting set."}
{"id": "2602.20990", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.20990", "abs": "https://arxiv.org/abs/2602.20990", "authors": ["Min-Chul Cha", "Hoon Beom Kwon", "Ji-Woo Lee", "Myung-Hoon Chung"], "title": "Entanglement Properties of the One-Dimensional Dimerized Fermi-Hubbard Model", "comment": "6 pages and 5 figures", "summary": "We study the entanglement properties of the one-dimensional dimerized Fermi-Hubbard model. Using a matrix-product-state approach, we compute the ground state and identify two insulating phases at 1/2- and 3/4-filling, along with a metallic phase, whose mechanisms can be characterized by their entanglement spectra. Our findings indicate that the two insulating phases are distinct, implying that the phase at 1/2-filling has a charge gap arising from the band gap, which is enhanced by repulsive interactions, while the phase at 3/4-filling exhibits a Mott gap resulting from particle interactions. This difference between the two insulating phases is reflected in the scaling properties of the half-chain entanglement entropy and the distribution of the entanglement spectrum."}
{"id": "2602.20186", "categories": ["quant-ph", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.20186", "abs": "https://arxiv.org/abs/2602.20186", "authors": ["Frederick Dehmel", "Shilun Li"], "title": "A Symplectic Proof of the Quantum Singleton Bound", "comment": "6 pages", "summary": "We present a symplectic linear-algebraic proof of the Quantum Singleton Bound for stabiliser quantum error-correcting codes together with a Lean4 formalisation of the linear-algebraic argument. The proof is formulated in the language of finite-dimensional symplectic vector spaces modelling Pauli operators and relies on distance-based erasure correctability and the cleaning lemma. Using a dimension-counting argument within the symplectic stabiliser framework, we derive the bound \\( k + 2(d - 1) \\le n \\) for any [[n, k, d]] stabiliser code. This approach isolates the algebraic structure underlying the bound and avoids the heavier analytic machinery that appears in entropy-based proofs, while remaining well-suited to formal verification."}
{"id": "2602.20191", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20191", "abs": "https://arxiv.org/abs/2602.20191", "authors": ["Dongwei Wang", "Jinhee Kim", "Seokho Han", "Denis Gudovskiy", "Yohei Nakata", "Tomoyuki Okuno", "KhayTze Peong", "Kang Eun Jeon", "Jong Hwan Ko", "Yiran Chen", "Huanrui Yang"], "title": "MoBiQuant: Mixture-of-Bits Quantization for Token-Adaptive Elastic LLMs", "comment": "17 pages, 12 figures", "summary": "Changing runtime complexity on cloud and edge devices necessitates elastic large language model (LLM) deployment, where an LLM can be inferred with various quantization precisions based on available computational resources. However, it has been observed that the calibration parameters for quantization are typically linked to specific precisions, which presents challenges during elastic-precision calibration and precision switching at runtime. In this work, we attribute the source of varying calibration parameters to the varying token-level sensitivity caused by a precision-dependent outlier migration phenomenon.Motivated by this observation, we propose \\texttt{MoBiQuant}, a novel Mixture-of-Bits quantization framework that adjusts weight precision for elastic LLM inference based on token sensitivity. Specifically, we propose the many-in-one recursive residual quantization that can iteratively reconstruct higher-precision weights and the token-aware router to dynamically select the number of residual bit slices. MoBiQuant enables smooth precision switching while improving generalization for the distribution of token outliers. Experimental results demonstrate that MoBiQuant exhibits strong elasticity, enabling it to match the performance of bit-specific calibrated PTQ on LLaMA3-8B without repeated calibration."}
{"id": "2602.20256", "categories": ["cond-mat.stat-mech", "cond-mat.dis-nn", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.20256", "abs": "https://arxiv.org/abs/2602.20256", "authors": ["Feng He", "Arthur Hutsalyuk", "Giuseppe Mussardo", "Andrea Stampiggi"], "title": "Spectral Decimation of Quantum Many-Body Hamiltonians", "comment": "16+6 pages; 5+3 figures", "summary": "We develop a systematic theory of spectral decimation for quantum many-body Hamiltonians and show that it provides a quantitative probe of emergent symmetries in statistically mixed spectra. Building on an analytical description of statistical mixtures, we derive an explicit expression for the size of a characteristic symmetry sector (CSS), defined as the largest subsequence of levels exhibiting non-Poissonian correlations. The CSS dimension is shown to be the size-biased average of the underlying symmetry sectors, establishing a direct link between spectral statistics and Hilbert-space structure. We apply this framework to two paradigmatic settings: Hilbert-space fragmentation and disorder-induced many-body localization (MBL). In fragmented systems, the CSS reproduces the mixture prediction and isolates correlated subsectors even when the full spectrum appears nearly Poissonian. In the disordered Heisenberg chain, spectral decimation reveals the gradual emergence of integrability through a shrinking CSS, whose statistics exhibit signatures consistent with local integrals of motion. We introduce a characteristic symmetry entropy (CSE) as a finite-size scaling observable and extract, within accessible system sizes, the crossover exponents. Our results establish spectral decimation as a controlled, unbiased and computationally inexpensive diagnostic of hidden structure in many-body spectra, capable of distinguishing between chaotic dynamics, statistical mixtures, and emergent integrability."}
{"id": "2602.20308", "categories": ["cond-mat.stat-mech", "cond-mat.soft"], "pdf": "https://arxiv.org/pdf/2602.20308", "abs": "https://arxiv.org/abs/2602.20308", "authors": ["Raphaël Maire"], "title": "Hyperuniformity in active fluids reshape nucleation and capillary-wave dynamics", "comment": "15 pages, 2 figures", "summary": "While nucleation in typical active and driven fluids often appears equilibrium-like, striking departures emerge when large-scale fluctuations are strongly suppressed. Here, we investigate nucleation in nonequilibrium hyperuniform fluids by projecting the full density-field dynamics onto relevant collective variables. We demonstrate that nucleation is governed by a nonequilibrium quasi-potential rather than the reversible work of formation. Surprisingly, because of the reduced hyperuniform fluctuations, the nucleation probability no longer separates into the usual surface and volume contributions. Furthermore, accounting for capillary waves reveals a clear breakdown of detailed balance driven by nonreciprocal dynamics. More broadly, our framework can be readily extended to identify nonequilibrium signatures in conventional active fluids."}
{"id": "2602.20333", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20333", "abs": "https://arxiv.org/abs/2602.20333", "authors": ["Samarth KaPatel", "Sofia Nikiforova", "Giacinto Paolo Saggese", "Paul Smith"], "title": "DMCD: Semantic-Statistical Framework for Causal Discovery", "comment": null, "summary": "We present DMCD (DataMap Causal Discovery), a two-phase causal discovery framework that integrates LLM-based semantic drafting from variable metadata with statistical validation on observational data. In Phase I, a large language model proposes a sparse draft DAG, serving as a semantically informed prior over the space of possible causal structures. In Phase II, this draft is audited and refined via conditional independence testing, with detected discrepancies guiding targeted edge revisions.\n  We evaluate our approach on three metadata-rich real-world benchmarks spanning industrial engineering, environmental monitoring, and IT systems analysis. Across these datasets, DMCD achieves competitive or leading performance against diverse causal discovery baselines, with particularly large gains in recall and F1 score. Probing and ablation experiments suggest that these improvements arise from semantic reasoning over metadata rather than memorization of benchmark graphs. Overall, our results demonstrate that combining semantic priors with principled statistical verification yields a high-performing and practically effective approach to causal structure learning."}
{"id": "2602.20234", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.20234", "abs": "https://arxiv.org/abs/2602.20234", "authors": ["Tyler D. Kharazi", "Stepan Fomichev", "Shu Kanno", "Takao Kobayashi", "Juan Miguel Arrazola", "Qi Gao", "Torin F. Stetina"], "title": "Quantum Simulations for Extreme Ultraviolet Photolithography", "comment": null, "summary": "Extreme Ultraviolet (EUV) lithography is the state-of-the-art process in semiconductor fabrication, yet its spatial resolution is fundamentally limited by the ``blur'' originating from absorption of photons at 92 eV, which induce physical and chemical changes in the photoresist via excited state processes and electron cascades. Accurate modeling of these phenomena requires precise ab initio data for high-energy decay channels, specifically photoabsorption and photoelectron emission. These are computationally difficult for classical methods due to prohibitive scaling in simulating electron dynamics, or due to the inability to resolve the ionization continuum in an efficient manner. In this work, we present quantum simulation algorithms to compute these key observables. First, we introduce a coherent time-domain spectroscopy algorithm optimized to resolve the photoabsorption cross-section at the 92 eV operating frequency. Second, we develop a first-quantized plane-wave simulation to compute the photoelectron kinetic energy spectrum, utilizing real-time dynamics and energy windowing to treat bound and delocalized scattering states on equal footing. Additionally, we provide logical resource estimation for a model photoresist monomer, 4-iodo-2-methylphenol (IMePh), and demonstrate that 92 eV absorption sensitivity can be resolved using roughly $200$ logical qubits and $10^{9}$ total non-Clifford gates per circuit with approximately $10^3$ shots for the smallest instance. The more sophisticated photoemission algorithm that models the continuum explicitly, incurs gate costs of $\\geq 10^{13}$ total non-Clifford gates per circuit, $10^4$ shots, and requires a few thousand logical qubits. These results establish high-fidelity quantum simulations as a key component to parameterize the multi-scale macroscopic models required to overcome the electron blur bottleneck in semiconductor miniaturization."}
{"id": "2602.20194", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.20194", "abs": "https://arxiv.org/abs/2602.20194", "authors": ["Takato Yasuno"], "title": "FedAvg-Based CTMC Hazard Model for Federated Bridge Deterioration Assessment", "comment": "10 pages, 4 figures, 2 tables", "summary": "Bridge periodic inspection records contain sensitive information about public infrastructure, making cross-organizational data sharing impractical under existing data governance constraints. We propose a federated framework for estimating a Continuous-Time Markov Chain (CTMC) hazard model of bridge deterioration, enabling municipalities to collaboratively train a shared benchmark model without transferring raw inspection records. Each User holds local inspection data and trains a log-linear hazard model over three deterioration-direction transitions -- Good$\\to$Minor, Good$\\to$Severe, and Minor$\\to$Severe -- with covariates for bridge age, coastline distance, and deck area. Local optimization is performed via mini-batch stochastic gradient descent on the CTMC log-likelihood, and only a 12-dimensional pseudo-gradient vector is uploaded to a central server per communication round. The server aggregates User updates using sample-weighted Federated Averaging (FedAvg) with momentum and gradient clipping. All experiments in this paper are conducted on fully synthetic data generated from a known ground-truth parameter set with region-specific heterogeneity, enabling controlled evaluation of federated convergence behaviour. Simulation results across heterogeneous Users show consistent convergence of the average negative log-likelihood, with the aggregated gradient norm decreasing as User scale increases. Furthermore, the federated update mechanism provides a natural participation incentive: Users who register their local inspection datasets on a shared technical-standard platform receive in return the periodically updated global benchmark parameters -- information that cannot be obtained from local data alone -- thereby enabling evidence-based life-cycle planning without surrendering data sovereignty."}
{"id": "2602.20422", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20422", "abs": "https://arxiv.org/abs/2602.20422", "authors": ["Hanping Zhang", "Yuhong Guo"], "title": "Diffusion Modulation via Environment Mechanism Modeling for Planning", "comment": null, "summary": "Diffusion models have shown promising capabilities in trajectory generation for planning in offline reinforcement learning (RL). However, conventional diffusion-based planning methods often fail to account for the fact that generating trajectories in RL requires unique consistency between transitions to ensure coherence in real environments. This oversight can result in considerable discrepancies between the generated trajectories and the underlying mechanisms of a real environment. To address this problem, we propose a novel diffusion-based planning method, termed as Diffusion Modulation via Environment Mechanism Modeling (DMEMM). DMEMM modulates diffusion model training by incorporating key RL environment mechanisms, particularly transition dynamics and reward functions. Experimental results demonstrate that DMEMM achieves state-of-the-art performance for planning with offline reinforcement learning."}
{"id": "2602.20238", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.20238", "abs": "https://arxiv.org/abs/2602.20238", "authors": ["Satoshi Yoshida", "Ethan Lake", "Hayata Yamasaki"], "title": "Proof of a finite threshold for the union-find decoder", "comment": "20 pages, 6 figures", "summary": "Fast decoders that achieve strong error suppression are essential for fault-tolerant quantum computation (FTQC) from both practical and theoretical perspectives. The union-find (UF) decoder for the surface code is widely regarded as a promising candidate, offering almost-linear time complexity and favorable empirical error suppression supported by numerical evidence. However, the lack of a rigorous threshold theorem has left open whether the UF decoder can achieve fault tolerance beyond the error models and parameter regimes tested in numerical simulations. Here, we provide a rigorous proof of a finite threshold for the UF decoder on the surface code under the circuit-level local stochastic error model. To this end, we develop a refined error-clustering framework that extends techniques previously used to analyze cellular-automaton and renormalization-group decoders, by showing that error clusters can be separated by substantially larger buffers, thereby enabling analytical control over the behavior of the UF decoder. Using this guarantee, we further prove a quasi-polylogarithmic upper bound on the average runtime of a parallel UF decoder in terms of the code size. We also show that this framework yields a finite threshold for the greedy decoder, a simpler decoder with lower complexity but weaker empirical error suppression. These results provide a solid theoretical foundation for the practical use of UF-based decoders in the development of fault-tolerant quantum computers, while offering a unified framework for studying fault tolerance across these practical decoders."}
{"id": "2602.20197", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20197", "abs": "https://arxiv.org/abs/2602.20197", "authors": ["Zhuoxu Huang", "Mengxi Jia", "Hao Sun", "Xuelong Li", "Jungong Han"], "title": "Controllable Exploration in Hybrid-Policy RLVR for Multi-Modal Reasoning", "comment": "Published as a conference paper at ICLR 2026", "summary": "Reinforcement Learning with verifiable rewards (RLVR) has emerged as a primary learning paradigm for enhancing the reasoning capabilities of multi-modal large language models (MLLMs). However, during RL training, the enormous state space of MLLM and sparse rewards often leads to entropy collapse, policy degradation, or over-exploitation of suboptimal behaviors. This necessitates an exploration strategy that maintains productive stochasticity while avoiding the drawbacks of uncontrolled random sampling, yielding inefficient exploration. In this paper, we propose CalibRL, a hybrid-policy RLVR framework that supports controllable exploration with expert guidance, enabled by two key mechanisms. First, a distribution-aware advantage weighting scales updates by group rareness to calibrate the distribution, therefore preserving exploration. Meanwhile, the asymmetric activation function (LeakyReLU) leverages the expert knowledge as a calibration baseline to moderate overconfident updates while preserving their corrective direction. CalibRL increases policy entropy in a guided manner and clarifies the target distribution by estimating the on-policy distribution through online sampling. Updates are driven by these informative behaviors, avoiding convergence to erroneous patterns. Importantly, these designs help alleviate the distributional mismatch between the model's policy and expert trajectories, thereby achieving a more stable balance between exploration and exploitation. Extensive experiments across eight benchmarks, including both in-domain and out-of-domain settings, demonstrate consistent improvements, validating the effectiveness of our controllable hybrid-policy RLVR training. Code is available at https://github.com/zhh6425/CalibRL."}
{"id": "2602.20424", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20424", "abs": "https://arxiv.org/abs/2602.20424", "authors": ["Ved Sirdeshmukh", "Marc Wetter"], "title": "Implicit Intelligence -- Evaluating Agents on What Users Don't Say", "comment": null, "summary": "Real-world requests to AI agents are fundamentally underspecified. Natural human communication relies on shared context and unstated constraints that speakers expect listeners to infer. Current agentic benchmarks test explicit instruction-following but fail to evaluate whether agents can reason about implicit requirements spanning accessibility needs, privacy boundaries, catastrophic risks, and contextual constraints. We present Implicit Intelligence, an evaluation framework testing whether AI agents can move beyond prompt-following to become genuine goal-fulfillers, paired with Agent-as-a-World (AaW), a harness where interactive worlds are defined in human-readable YAML files and simulated by language models. Our scenarios feature apparent simplicity in user requests, hidden complexity in correct solutions, and discoverability of constraints through environmental exploration. Evaluating 16 frontier and open-weight models across 205 scenarios, we find that even the best-performing model achieves only 48.3% scenario pass rate, revealing substantial room for improvement in bridging the gap between literal instruction-following and human-like contextual reasoning."}
{"id": "2602.20269", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.20269", "abs": "https://arxiv.org/abs/2602.20269", "authors": ["Mert Esencan", "A. I. Lvovsky", "Berislav Buča"], "title": "Time Crystals as Passively Protected Oscillating Qubits", "comment": "5 pages, 5 figures. Supplemental material included", "summary": "Protecting information against decoherence in open quantum systems remains a central challenge for quantum computing. In particular, passive error correction schemes have so far been limited to static memories rather than dynamical qubits. We demonstrate that a driven-dissipative bosonic system can encode a persistently oscillating qubit within a noiseless subsystem, realized explicitly in the Bose-Hubbard dimer (BHD). The strong parity symmetry of the model leads to degenerate stationary states. This symmetry is further broken into non-stationary states in the thermodynamic limit, which exhibit persistent oscillations. As the driving force increases, the Liouvillian spectrum of these states features a phase transition. Above the transition point, the non-stationary state encodes quantum information, preserving it in a noiseless subsystem. In addition to global loss that affects both bosonic modes identically, we further add global dephasing and show that the oscillating qubit is preserved. Finally, in order to gain additional physical insight, we study the effect of phase perturbation to both modes and observe that likewise they are passively protected, returning approximately to their initial configurations. These results establish dissipative time-crystalline dynamics as a mechanism for passive protection of dynamical quantum information, enabling autonomously stabilized oscillating qubits."}
{"id": "2602.20199", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20199", "abs": "https://arxiv.org/abs/2602.20199", "authors": ["Soufiane Bacha", "Laouni Djafri", "Sahraoui Dhelim", "Huansheng Ning"], "title": "IMOVNO+: A Regional Partitioning and Meta-Heuristic Ensemble Framework for Imbalanced Multi-Class Learning", "comment": "28 pages", "summary": "Class imbalance, overlap, and noise degrade data quality, reduce model reliability, and limit generalization. Although widely studied in binary classification, these issues remain underexplored in multi-class settings, where complex inter-class relationships make minority-majority structures unclear and traditional clustering fails to capture distribution shape. Approaches that rely only on geometric distances risk removing informative samples and generating low-quality synthetic data, while binarization approaches treat imbalance locally and ignore global inter-class dependencies. At the algorithmic level, ensembles struggle to integrate weak classifiers, leading to limited robustness. This paper proposes IMOVNO+ (IMbalance-OVerlap-NOise+ Algorithm-Level Optimization), a two-level framework designed to jointly enhance data quality and algorithmic robustness for binary and multi-class tasks. At the data level, first, conditional probability is used to quantify the informativeness of each sample. Second, the dataset is partitioned into core, overlapping, and noisy regions. Third, an overlapping-cleaning algorithm is introduced that combines Z-score metrics with a big-jump gap distance. Fourth, a smart oversampling algorithm based on multi-regularization controls synthetic sample proximity, preventing new overlaps. At the algorithmic level, a meta-heuristic prunes ensemble classifiers to reduce weak-learner influence. IMOVNO+ was evaluated on 35 datasets (13 multi-class, 22 binary). Results show consistent superiority over state-of-the-art methods, approaching 100% in several cases. For multi-class data, IMOVNO+ achieves gains of 37-57% in G-mean, 25-44% in F1-score, 25-39% in precision, and 26-43% in recall. In binary tasks, it attains near-perfect performance with improvements of 14-39%. The framework handles data scarcity and imbalance from collection and privacy limits."}
{"id": "2602.20426", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20426", "abs": "https://arxiv.org/abs/2602.20426", "authors": ["Ruocheng Guo", "Kaiwen Dong", "Xiang Gao", "Kamalika Das"], "title": "Learning to Rewrite Tool Descriptions for Reliable LLM-Agent Tool Use", "comment": "Preprint", "summary": "The performance of LLM-based agents depends not only on the agent itself but also on the quality of the tool interfaces it consumes. While prior work has focused heavily on agent fine-tuning, tool interfaces-including natural language descriptions and parameter schemas-remain largely human-oriented and often become a bottleneck, especially when agents must select from large candidate tool sets. Existing approaches to improving tool interfaces rely on execution traces, which are frequently unavailable in cold-start or privacy-constrained settings, and typically optimize each tool independently, limiting scalability and generalization to unseen tools. We propose Trace-Free+, a curriculum learning framework that progressively transfers supervision from trace-rich settings to trace-free deployment, encouraging the model to abstract reusable interface-usage patterns and tool usage outcomes. To support this approach, we construct a large-scale dataset of high-quality tool interfaces using a structured workflow over a diverse collection of tools. Experiments on StableToolBench and RestBench show consistent gains on unseen tools, strong cross-domain generalization, and robustness as the number of candidate tools scales to over 100, demonstrating that tool interface optimization is a practical and deployable complement to agent fine-tuning."}
{"id": "2602.20207", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20207", "abs": "https://arxiv.org/abs/2602.20207", "authors": ["Shrestha Datta", "Hongfu Liu", "Anshuman Chhabra"], "title": "Golden Layers and Where to Find Them: Improved Knowledge Editing for Large Language Models Via Layer Gradient Analysis", "comment": null, "summary": "Knowledge editing in Large Language Models (LLMs) aims to update the model's prediction for a specific query to a desired target while preserving its behavior on all other inputs. This process typically involves two stages: identifying the layer to edit and performing the parameter update. Intuitively, different queries may localize knowledge at different depths of the model, resulting in different sample-wise editing performance for a fixed editing layer. In this work, we hypothesize the existence of fixed golden layers that can achieve near-optimal editing performance similar to sample-wise optimal layers. To validate this hypothesis, we provide empirical evidence by comparing golden layers against ground-truth sample-wise optimal layers. Furthermore, we show that golden layers can be reliably identified using a proxy dataset and generalize effectively to unseen test set queries across datasets. Finally, we propose a novel method, namely Layer Gradient Analysis (LGA) that estimates golden layers efficiently via gradient-attribution, avoiding extensive trial-and-error across multiple editing runs. Extensive experiments on several benchmark datasets demonstrate the effectiveness and robustness of our LGA approach across different LLM types and various knowledge editing methods."}
{"id": "2602.20459", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20459", "abs": "https://arxiv.org/abs/2602.20459", "authors": ["Anirudh Ajith", "Amanpreet Singh", "Jay DeYoung", "Nadav Kunievsky", "Austin C. Kozlowski", "Oyvind Tafjord", "James Evans", "Daniel S. Weld", "Tom Hope", "Doug Downey"], "title": "PreScience: A Benchmark for Forecasting Scientific Contributions", "comment": "10 pages (53 with bibliography and appendix), 4 figures (13 with appendix), 4 tables (10 with appendix), 1 algorithm", "summary": "Can AI systems trained on the scientific record up to a fixed point in time forecast the scientific advances that follow? Such a capability could help researchers identify collaborators and impactful research directions, and anticipate which problems and methods will become central next. We introduce PreScience -- a scientific forecasting benchmark that decomposes the research process into four interdependent generative tasks: collaborator prediction, prior work selection, contribution generation, and impact prediction. PreScience is a carefully curated dataset of 98K recent AI-related research papers, featuring disambiguated author identities, temporally aligned scholarly metadata, and a structured graph of companion author publication histories and citations spanning 502K total papers. We develop baselines and evaluations for each task, including LACERScore, a novel LLM-based measure of contribution similarity that outperforms previous metrics and approximates inter-annotator agreement. We find substantial headroom remains in each task -- e.g. in contribution generation, frontier LLMs achieve only moderate similarity to the ground-truth (GPT-5, averages 5.6 on a 1-10 scale). When composed into a 12-month end-to-end simulation of scientific production, the resulting synthetic corpus is systematically less diverse and less novel than human-authored research from the same period."}
{"id": "2602.20208", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20208", "abs": "https://arxiv.org/abs/2602.20208", "authors": ["Longhua Li", "Lei Qi", "Qi Tian", "Xin Geng"], "title": "Model Merging in the Essential Subspace", "comment": "Accepted by CVPR 2026", "summary": "Model merging aims to integrate multiple task-specific fine-tuned models derived from a shared pre-trained checkpoint into a single multi-task model without additional training. Despite extensive research, task interference remains a major obstacle that often undermines the performance of merged models. In this paper, we propose ESM (Essential Subspace Merging) , a robust framework for effective model merging. We begin by performing Principal Component Analysis (PCA) on feature shifts induced by parameter updates. The resulting principal directions span an essential subspace that dominantly influences feature representations. Each task's parameter update matrix is projected onto its respective essential subspace for low-rank decomposition before merging. This methodology mitigates inter-task interference while preserving core task-specific functionality. Furthermore, we introduce a multi-level polarized scaling strategy that amplifies parameters containing critical knowledge and suppresses redundant ones, preventing essential knowledge from being overwhelmed during fusion. Extensive experiments across multiple task sets and model scales demonstrate that our method achieves state-of-the-art performance in multi-task model merging."}
{"id": "2602.20620", "categories": ["cond-mat.dis-nn", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.20620", "abs": "https://arxiv.org/abs/2602.20620", "authors": ["Munetaka Sasaki"], "title": "Construction of a Neural Network with Temperature-Dependent Recall Patterns", "comment": "6 pages, 10 figures", "summary": "We present a simple model that recalls two different patterns depending on the temperature. To realize a change in recall pattern due to temperature change, we embed two patterns to different graphs: the first pattern into a fully connected graph and the second pattern into a sparse graph. Because a fully connected graph is more resistant to thermal fluctuations than a sparse graph, we can realize a change in recall pattern by tuning relative weights of the two patterns properly. We demonstrate by equilibrium Monte-Carlo simulations that such a temperature-dependent change in recall patterns does occur in our model. Simulation results strongly indicate that the system undergoes a first-order phase transition when the change in recall patterns occurs. It is also demonstrated by annealing simulations that the system fails to recall the pattern embedded in the sparse graph at low temperatures if the free-energy barrier is too high to overcome within the given simulation timescale."}
{"id": "2602.20210", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20210", "abs": "https://arxiv.org/abs/2602.20210", "authors": ["Kiyoung Seong", "Sungsoo Ahn", "Sehui Han", "Changyoung Park"], "title": "Multimodal Crystal Flow: Any-to-Any Modality Generation for Unified Crystal Modeling", "comment": null, "summary": "Crystal modeling spans a family of conditional and unconditional generation tasks across different modalities, including crystal structure prediction (CSP) and \\emph{de novo} generation (DNG). While recent deep generative models have shown promising performance, they remain largely task-specific, lacking a unified framework that shares crystal representations across different generation tasks. To address this limitation, we propose \\emph{Multimodal Crystal Flow (MCFlow)}, a unified multimodal flow model that realizes multiple crystal generation tasks as distinct inference trajectories via independent time variables for atom types and crystal structures. To enable multimodal flow in a standard transformer model, we introduce a composition- and symmetry-aware atom ordering with hierarchical permutation augmentation, injecting strong compositional and crystallographic priors without explicit structural templates. Experiments on the MP-20 and MPTS-52 benchmarks show that MCFlow achieves competitive performance against task-specific baselines across multiple crystal generation tasks."}
{"id": "2602.20502", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20502", "abs": "https://arxiv.org/abs/2602.20502", "authors": ["Hongbin Zhong", "Fazle Faisal", "Luis França", "Tanakorn Leesatapornwongsa", "Adriana Szekeres", "Kexin Rong", "Suman Nath"], "title": "ActionEngine: From Reactive to Programmatic GUI Agents via State Machine Memory", "comment": null, "summary": "Existing Graphical User Interface (GUI) agents operate through step-by-step calls to vision language models--taking a screenshot, reasoning about the next action, executing it, then repeating on the new page--resulting in high costs and latency that scale with the number of reasoning steps, and limited accuracy due to no persistent memory of previously visited pages.\n  We propose ActionEngine, a training-free framework that transitions from reactive execution to programmatic planning through a novel two-agent architecture: a Crawling Agent that constructs an updatable state-machine memory of the GUIs through offline exploration, and an Execution Agent that leverages this memory to synthesize complete, executable Python programs for online task execution.\n  To ensure robustness against evolving interfaces, execution failures trigger a vision-based re-grounding fallback that repairs the failed action and updates the memory.\n  This design drastically improves both efficiency and accuracy: on Reddit tasks from the WebArena benchmark, our agent achieves 95% task success with on average a single LLM call, compared to 66% for the strongest vision-only baseline, while reducing cost by 11.8x and end-to-end latency by 2x.\n  Together, these components yield scalable and reliable GUI interaction by combining global programmatic planning, crawler-validated action templates, and node-level execution with localized validation and repair."}
{"id": "2602.20217", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20217", "abs": "https://arxiv.org/abs/2602.20217", "authors": ["Seongjin Cha", "Gyuwan Kim", "Dongsu Han", "Tao Yang", "Insu Han"], "title": "KnapSpec: Self-Speculative Decoding via Adaptive Layer Selection as a Knapsack Problem", "comment": null, "summary": "Self-speculative decoding (SSD) accelerates LLM inference by skipping layers to create an efficient draft model, yet existing methods often rely on static heuristics that ignore the dynamic computational overhead of attention in long-context scenarios. We propose KnapSpec, a training-free framework that reformulates draft model selection as a knapsack problem to maximize tokens-per-time throughput. By decoupling Attention and MLP layers and modeling their hardware-specific latencies as functions of context length, KnapSpec adaptively identifies optimal draft configurations on the fly via a parallel dynamic programming algorithm. Furthermore, we provide the first rigorous theoretical analysis establishing cosine similarity between hidden states as a mathematically sound proxy for the token acceptance rate. This foundation allows our method to maintain high drafting faithfulness while navigating the shifting bottlenecks of real-world hardware. Our experiments on Qwen3 and Llama3 demonstrate that KnapSpec consistently outperforms state-of-the-art SSD baselines, achieving up to 1.47x wall-clock speedup across various benchmarks. Our plug-and-play approach ensures high-speed inference for long sequences without requiring additional training or compromising the target model's output distribution."}
{"id": "2602.20223", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20223", "abs": "https://arxiv.org/abs/2602.20223", "authors": ["Wall Kim", "Chaeyoung Song", "Hanul Kim"], "title": "MultiModalPFN: Extending Prior-Data Fitted Networks for Multimodal Tabular Learning", "comment": "Accepted to CVPR 2026", "summary": "Recently, TabPFN has gained attention as a foundation model for tabular data. However, it struggles to integrate heterogeneous modalities such as images and text, which are common in domains like healthcare and marketing, thereby limiting its applicability. To address this, we present the Multi-Modal Prior-data Fitted Network (MMPFN), which extends TabPFN to handle tabular and non-tabular modalities in a unified manner. MMPFN comprises per-modality encoders, modality projectors, and pre-trained foundation models. The modality projectors serve as the critical bridge, transforming non-tabular embeddings into tabular-compatible tokens for unified processing. To this end, we introduce a multi-head gated MLP and a cross-attention pooler that extract richer context from non-tabular inputs while mitigates attention imbalance issue in multimodal learning. Extensive experiments on medical and general-purpose multimodal datasets demonstrate that MMPFN consistently outperforms competitive state-of-the-art methods and effectively exploits non-tabular modalities alongside tabular features. These results highlight the promise of extending prior-data fitted networks to the multimodal setting, offering a scalable and effective framework for heterogeneous data learning. The source code is available at https://github.com/too-z/MultiModalPFN."}
{"id": "2602.20558", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.20558", "abs": "https://arxiv.org/abs/2602.20558", "authors": ["Yucheng Shi", "Ying Li", "Yu Wang", "Yesu Feng", "Arjun Rao", "Rein Houthooft", "Shradha Sehgal", "Jin Wang", "Hao Zhen", "Ninghao Liu", "Linas Baltrunas"], "title": "From Logs to Language: Learning Optimal Verbalization for LLM-Based Recommendation in Production", "comment": "Work in progress", "summary": "Large language models (LLMs) are promising backbones for generative recommender systems, yet a key challenge remains underexplored: verbalization, i.e., converting structured user interaction logs into effective natural language inputs. Existing methods rely on rigid templates that simply concatenate fields, yielding suboptimal representations for recommendation. We propose a data-centric framework that learns verbalization for LLM-based recommendation. Using reinforcement learning, a verbalization agent transforms raw interaction histories into optimized textual contexts, with recommendation accuracy as the training signal. This agent learns to filter noise, incorporate relevant metadata, and reorganize information to improve downstream predictions. Experiments on a large-scale industrial streaming dataset show that learned verbalization delivers up to 93% relative improvement in discovery item recommendation accuracy over template-based baselines. Further analysis reveals emergent strategies such as user interest summarization, noise removal, and syntax normalization, offering insights into effective context construction for LLM-based recommender systems."}
{"id": "2602.20224", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20224", "abs": "https://arxiv.org/abs/2602.20224", "authors": ["Lana E. Yeganova", "Won G. Kim", "Shubo Tian", "Natalie Xie", "Donald C. Comeau", "W. John Wilbur", "Zhiyong Lu"], "title": "Exploring Anti-Aging Literature via ConvexTopics and Large Language Models", "comment": null, "summary": "The rapid expansion of biomedical publications creates challenges for organizing knowledge and detecting emerging trends, underscoring the need for scalable and interpretable methods. Common clustering and topic modeling approaches such as K-means or LDA remain sensitive to initialization and prone to local optima, limiting reproducibility and evaluation. We propose a reformulation of a convex optimization based clustering algorithm that produces stable, fine-grained topics by selecting exemplars from the data and guaranteeing a global optimum. Applied to about 12,000 PubMed articles on aging and longevity, our method uncovers topics validated by medical experts. It yields interpretable topics spanning from molecular mechanisms to dietary supplements, physical activity, and gut microbiota. The method performs favorably, and most importantly, its reproducibility and interpretability distinguish it from common clustering approaches, including K-means, LDA, and BERTopic. This work provides a basis for developing scalable, web-accessible tools for knowledge discovery."}
{"id": "2602.20571", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20571", "abs": "https://arxiv.org/abs/2602.20571", "authors": ["Ayush Sawarni", "Jiyuan Tan", "Vasilis Syrgkanis"], "title": "CausalReasoningBenchmark: A Real-World Benchmark for Disentangled Evaluation of Causal Identification and Estimation", "comment": null, "summary": "Many benchmarks for automated causal inference evaluate a system's performance based on a single numerical output, such as an Average Treatment Effect (ATE). This approach conflates two distinct steps in causal analysis: identification-formulating a valid research design under stated assumptions-and estimation-implementing that design numerically on finite data. We introduce CausalReasoningBenchmark, a benchmark of 173 queries across 138 real-world datasets, curated from 85 peer-reviewed research papers and four widely-used causal-inference textbooks. For each query a system must produce (i) a structured identification specification that names the strategy, the treatment, outcome, and control variables, and all design-specific elements, and (ii) a point estimate with a standard error. By scoring these two components separately, our benchmark enables granular diagnosis: it distinguishes failures in causal reasoning from errors in numerical execution. Baseline results with a state-of-the-art LLM show that, while the model correctly identifies the high-level strategy in 84 % of cases, full identification-specification correctness drops to only 30 %, revealing that the bottleneck lies in the nuanced details of research design rather than in computation. CausalReasoningBenchmark is publicly available on Hugging Face and is designed to foster the development of more robust automated causal-inference systems."}
{"id": "2602.20306", "categories": ["cs.LG", "cs.AI", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.20306", "abs": "https://arxiv.org/abs/2602.20306", "authors": ["Davide Carrara", "Marc Hirschvogel", "Francesca Bonizzoni", "Stefano Pagani", "Simone Pezzuto", "Francesco Regazzoni"], "title": "Shape-informed cardiac mechanics surrogates in data-scarce regimes via geometric encoding and generative augmentation", "comment": "39 pages, 19 figures", "summary": "High-fidelity computational models of cardiac mechanics provide mechanistic insight into the heart function but are computationally prohibitive for routine clinical use. Surrogate models can accelerate simulations, but generalization across diverse anatomies is challenging, particularly in data-scarce settings. We propose a two-step framework that decouples geometric representation from learning the physics response, to enable shape-informed surrogate modeling under data-scarce conditions. First, a shape model learns a compact latent representation of left ventricular geometries. The learned latent space effectively encodes anatomies and enables synthetic geometries generation for data augmentation. Second, a neural field-based surrogate model, conditioned on this geometric encoding, is trained to predict ventricular displacement under external loading. The proposed architecture performs positional encoding by using universal ventricular coordinates, which improves generalization across diverse anatomies. Geometric variability is encoded using two alternative strategies, which are systematically compared: a PCA-based approach suitable for working with point cloud representations of geometries, and a DeepSDF-based implicit neural representation learned directly from point clouds. Overall, our results, obtained on idealized and patient-specific datasets, show that the proposed approaches allow for accurate predictions and generalization to unseen geometries, and robustness to noisy or sparsely sampled inputs."}
{"id": "2602.20722", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20722", "abs": "https://arxiv.org/abs/2602.20722", "authors": ["Xu Wan", "Yansheng Wang", "Wenqi Huang", "Mingyang Sun"], "title": "Buffer Matters: Unleashing the Power of Off-Policy Reinforcement Learning in Large Language Model Reasoning", "comment": null, "summary": "Traditional on-policy Reinforcement Learning with Verifiable Rewards (RLVR) frameworks suffer from experience waste and reward homogeneity, which directly hinders learning efficiency on difficult samples during large language models post-training. In this paper, we introduce Batch Adaptation Policy Optimization (BAPO), an off-policy RLVR framework to improve the data efficiency in large language models post-training. It dynamically selects training batches by re-evaluating historically difficult samples and reusing high-quality ones, while holding a lower bound guarantee for policy improvement. Extensive experiments further demonstrate that BAPO achieves an average 12.5% improvement over GRPO across mathematics, planning, and visual reasoning tasks. Crucially, BAPO successfully resolves 40.7% of problems that base models consistently fail to solve."}
{"id": "2602.20370", "categories": ["cs.LG", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.20370", "abs": "https://arxiv.org/abs/2602.20370", "authors": ["Jonathan W. Siegel", "Snir Hordan", "Hannah Lawrence", "Ali Syed", "Nadav Dym"], "title": "Quantitative Approximation Rates for Group Equivariant Learning", "comment": null, "summary": "The universal approximation theorem establishes that neural networks can approximate any continuous function on a compact set. Later works in approximation theory provide quantitative approximation rates for ReLU networks on the class of $α$-Hölder functions $f: [0,1]^N \\to \\mathbb{R}$. The goal of this paper is to provide similar quantitative approximation results in the context of group equivariant learning, where the learned $α$-Hölder function is known to obey certain group symmetries. While there has been much interest in the literature in understanding the universal approximation properties of equivariant models, very few quantitative approximation results are known for equivariant models.\n  In this paper, we bridge this gap by deriving quantitative approximation rates for several prominent group-equivariant and invariant architectures. The architectures that we consider include: the permutation-invariant Deep Sets architecture; the permutation-equivariant Sumformer and Transformer architectures; joint invariance to permutations and rigid motions using invariant networks based on frame averaging; and general bi-Lipschitz invariant models. Overall, we show that equally-sized ReLU MLPs and equivariant architectures are equally expressive over equivariant functions. Thus, hard-coding equivariance does not result in a loss of expressivity or approximation power in these models."}
{"id": "2602.20732", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20732", "abs": "https://arxiv.org/abs/2602.20732", "authors": ["Chao Fei", "Guozhong Li", "Chenxi Liu", "Panos Kalnis"], "title": "CHESS: Context-aware Hierarchical Efficient Semantic Selection for Long-Context LLM Inference", "comment": null, "summary": "Long-context LLMs demand accurate inference at low latency, yet decoding becomes primarily constrained by KV cache as context grows. Prior pruning methods are largely context-agnostic: their token selection ignores step-wise relevance and local semantics, which undermines quality. Moreover, their irregular accesses and selection overheads yield only limited wall-clock speedups. To address this, we propose \\textbf{CHESS}, an \\textit{algorithm-system co-design} KV-cache management system. Algorithmically, CHESS introduces a context-aware, hierarchical selection policy that dynamically reconstructs a coherent context for the current decoding. System-wise, coarse granularity selection eliminates expensive data movement, fully realizing practical acceleration from theoretical sparsity. Extensive evaluations demonstrate that CHESS surpasses Full-KV quality using only \\textbf{1\\%} of the KV cache, delivers low-latency stable inference with up to \\textbf{4.56$\\times$} higher throughput, and consistently outperforms other strong baselines. Code is available at \\href{https://anonymous.4open.science/r/CHESS-9958/}{https://anonymous.4open.science/r/CHESS/}."}
{"id": "2602.20739", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20739", "abs": "https://arxiv.org/abs/2602.20739", "authors": ["Shitian Zhao", "Shaoheng Lin", "Ming Li", "Haoquan Zhang", "Wenshuo Peng", "Kaipeng Zhang", "Chen Wei"], "title": "PyVision-RL: Forging Open Agentic Vision Models via RL", "comment": "preprint", "summary": "Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents."}
{"id": "2602.20191", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20191", "abs": "https://arxiv.org/abs/2602.20191", "authors": ["Dongwei Wang", "Jinhee Kim", "Seokho Han", "Denis Gudovskiy", "Yohei Nakata", "Tomoyuki Okuno", "KhayTze Peong", "Kang Eun Jeon", "Jong Hwan Ko", "Yiran Chen", "Huanrui Yang"], "title": "MoBiQuant: Mixture-of-Bits Quantization for Token-Adaptive Elastic LLMs", "comment": "17 pages, 12 figures", "summary": "Changing runtime complexity on cloud and edge devices necessitates elastic large language model (LLM) deployment, where an LLM can be inferred with various quantization precisions based on available computational resources. However, it has been observed that the calibration parameters for quantization are typically linked to specific precisions, which presents challenges during elastic-precision calibration and precision switching at runtime. In this work, we attribute the source of varying calibration parameters to the varying token-level sensitivity caused by a precision-dependent outlier migration phenomenon.Motivated by this observation, we propose \\texttt{MoBiQuant}, a novel Mixture-of-Bits quantization framework that adjusts weight precision for elastic LLM inference based on token sensitivity. Specifically, we propose the many-in-one recursive residual quantization that can iteratively reconstruct higher-precision weights and the token-aware router to dynamically select the number of residual bit slices. MoBiQuant enables smooth precision switching while improving generalization for the distribution of token outliers. Experimental results demonstrate that MoBiQuant exhibits strong elasticity, enabling it to match the performance of bit-specific calibrated PTQ on LLaMA3-8B without repeated calibration."}
{"id": "2602.20197", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20197", "abs": "https://arxiv.org/abs/2602.20197", "authors": ["Zhuoxu Huang", "Mengxi Jia", "Hao Sun", "Xuelong Li", "Jungong Han"], "title": "Controllable Exploration in Hybrid-Policy RLVR for Multi-Modal Reasoning", "comment": "Published as a conference paper at ICLR 2026", "summary": "Reinforcement Learning with verifiable rewards (RLVR) has emerged as a primary learning paradigm for enhancing the reasoning capabilities of multi-modal large language models (MLLMs). However, during RL training, the enormous state space of MLLM and sparse rewards often leads to entropy collapse, policy degradation, or over-exploitation of suboptimal behaviors. This necessitates an exploration strategy that maintains productive stochasticity while avoiding the drawbacks of uncontrolled random sampling, yielding inefficient exploration. In this paper, we propose CalibRL, a hybrid-policy RLVR framework that supports controllable exploration with expert guidance, enabled by two key mechanisms. First, a distribution-aware advantage weighting scales updates by group rareness to calibrate the distribution, therefore preserving exploration. Meanwhile, the asymmetric activation function (LeakyReLU) leverages the expert knowledge as a calibration baseline to moderate overconfident updates while preserving their corrective direction. CalibRL increases policy entropy in a guided manner and clarifies the target distribution by estimating the on-policy distribution through online sampling. Updates are driven by these informative behaviors, avoiding convergence to erroneous patterns. Importantly, these designs help alleviate the distributional mismatch between the model's policy and expert trajectories, thereby achieving a more stable balance between exploration and exploitation. Extensive experiments across eight benchmarks, including both in-domain and out-of-domain settings, demonstrate consistent improvements, validating the effectiveness of our controllable hybrid-policy RLVR training. Code is available at https://github.com/zhh6425/CalibRL."}
{"id": "2602.20199", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20199", "abs": "https://arxiv.org/abs/2602.20199", "authors": ["Soufiane Bacha", "Laouni Djafri", "Sahraoui Dhelim", "Huansheng Ning"], "title": "IMOVNO+: A Regional Partitioning and Meta-Heuristic Ensemble Framework for Imbalanced Multi-Class Learning", "comment": "28 pages", "summary": "Class imbalance, overlap, and noise degrade data quality, reduce model reliability, and limit generalization. Although widely studied in binary classification, these issues remain underexplored in multi-class settings, where complex inter-class relationships make minority-majority structures unclear and traditional clustering fails to capture distribution shape. Approaches that rely only on geometric distances risk removing informative samples and generating low-quality synthetic data, while binarization approaches treat imbalance locally and ignore global inter-class dependencies. At the algorithmic level, ensembles struggle to integrate weak classifiers, leading to limited robustness. This paper proposes IMOVNO+ (IMbalance-OVerlap-NOise+ Algorithm-Level Optimization), a two-level framework designed to jointly enhance data quality and algorithmic robustness for binary and multi-class tasks. At the data level, first, conditional probability is used to quantify the informativeness of each sample. Second, the dataset is partitioned into core, overlapping, and noisy regions. Third, an overlapping-cleaning algorithm is introduced that combines Z-score metrics with a big-jump gap distance. Fourth, a smart oversampling algorithm based on multi-regularization controls synthetic sample proximity, preventing new overlaps. At the algorithmic level, a meta-heuristic prunes ensemble classifiers to reduce weak-learner influence. IMOVNO+ was evaluated on 35 datasets (13 multi-class, 22 binary). Results show consistent superiority over state-of-the-art methods, approaching 100% in several cases. For multi-class data, IMOVNO+ achieves gains of 37-57% in G-mean, 25-44% in F1-score, 25-39% in precision, and 26-43% in recall. In binary tasks, it attains near-perfect performance with improvements of 14-39%. The framework handles data scarcity and imbalance from collection and privacy limits."}
{"id": "2602.20207", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20207", "abs": "https://arxiv.org/abs/2602.20207", "authors": ["Shrestha Datta", "Hongfu Liu", "Anshuman Chhabra"], "title": "Golden Layers and Where to Find Them: Improved Knowledge Editing for Large Language Models Via Layer Gradient Analysis", "comment": null, "summary": "Knowledge editing in Large Language Models (LLMs) aims to update the model's prediction for a specific query to a desired target while preserving its behavior on all other inputs. This process typically involves two stages: identifying the layer to edit and performing the parameter update. Intuitively, different queries may localize knowledge at different depths of the model, resulting in different sample-wise editing performance for a fixed editing layer. In this work, we hypothesize the existence of fixed golden layers that can achieve near-optimal editing performance similar to sample-wise optimal layers. To validate this hypothesis, we provide empirical evidence by comparing golden layers against ground-truth sample-wise optimal layers. Furthermore, we show that golden layers can be reliably identified using a proxy dataset and generalize effectively to unseen test set queries across datasets. Finally, we propose a novel method, namely Layer Gradient Analysis (LGA) that estimates golden layers efficiently via gradient-attribution, avoiding extensive trial-and-error across multiple editing runs. Extensive experiments on several benchmark datasets demonstrate the effectiveness and robustness of our LGA approach across different LLM types and various knowledge editing methods."}
{"id": "2602.20208", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20208", "abs": "https://arxiv.org/abs/2602.20208", "authors": ["Longhua Li", "Lei Qi", "Qi Tian", "Xin Geng"], "title": "Model Merging in the Essential Subspace", "comment": "Accepted by CVPR 2026", "summary": "Model merging aims to integrate multiple task-specific fine-tuned models derived from a shared pre-trained checkpoint into a single multi-task model without additional training. Despite extensive research, task interference remains a major obstacle that often undermines the performance of merged models. In this paper, we propose ESM (Essential Subspace Merging) , a robust framework for effective model merging. We begin by performing Principal Component Analysis (PCA) on feature shifts induced by parameter updates. The resulting principal directions span an essential subspace that dominantly influences feature representations. Each task's parameter update matrix is projected onto its respective essential subspace for low-rank decomposition before merging. This methodology mitigates inter-task interference while preserving core task-specific functionality. Furthermore, we introduce a multi-level polarized scaling strategy that amplifies parameters containing critical knowledge and suppresses redundant ones, preventing essential knowledge from being overwhelmed during fusion. Extensive experiments across multiple task sets and model scales demonstrate that our method achieves state-of-the-art performance in multi-task model merging."}
{"id": "2602.20210", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20210", "abs": "https://arxiv.org/abs/2602.20210", "authors": ["Kiyoung Seong", "Sungsoo Ahn", "Sehui Han", "Changyoung Park"], "title": "Multimodal Crystal Flow: Any-to-Any Modality Generation for Unified Crystal Modeling", "comment": null, "summary": "Crystal modeling spans a family of conditional and unconditional generation tasks across different modalities, including crystal structure prediction (CSP) and \\emph{de novo} generation (DNG). While recent deep generative models have shown promising performance, they remain largely task-specific, lacking a unified framework that shares crystal representations across different generation tasks. To address this limitation, we propose \\emph{Multimodal Crystal Flow (MCFlow)}, a unified multimodal flow model that realizes multiple crystal generation tasks as distinct inference trajectories via independent time variables for atom types and crystal structures. To enable multimodal flow in a standard transformer model, we introduce a composition- and symmetry-aware atom ordering with hierarchical permutation augmentation, injecting strong compositional and crystallographic priors without explicit structural templates. Experiments on the MP-20 and MPTS-52 benchmarks show that MCFlow achieves competitive performance against task-specific baselines across multiple crystal generation tasks."}
{"id": "2602.20217", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20217", "abs": "https://arxiv.org/abs/2602.20217", "authors": ["Seongjin Cha", "Gyuwan Kim", "Dongsu Han", "Tao Yang", "Insu Han"], "title": "KnapSpec: Self-Speculative Decoding via Adaptive Layer Selection as a Knapsack Problem", "comment": null, "summary": "Self-speculative decoding (SSD) accelerates LLM inference by skipping layers to create an efficient draft model, yet existing methods often rely on static heuristics that ignore the dynamic computational overhead of attention in long-context scenarios. We propose KnapSpec, a training-free framework that reformulates draft model selection as a knapsack problem to maximize tokens-per-time throughput. By decoupling Attention and MLP layers and modeling their hardware-specific latencies as functions of context length, KnapSpec adaptively identifies optimal draft configurations on the fly via a parallel dynamic programming algorithm. Furthermore, we provide the first rigorous theoretical analysis establishing cosine similarity between hidden states as a mathematically sound proxy for the token acceptance rate. This foundation allows our method to maintain high drafting faithfulness while navigating the shifting bottlenecks of real-world hardware. Our experiments on Qwen3 and Llama3 demonstrate that KnapSpec consistently outperforms state-of-the-art SSD baselines, achieving up to 1.47x wall-clock speedup across various benchmarks. Our plug-and-play approach ensures high-speed inference for long sequences without requiring additional training or compromising the target model's output distribution."}
{"id": "2602.20175", "categories": ["cs.LG", "math.OC", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.20175", "abs": "https://arxiv.org/abs/2602.20175", "authors": ["Ryo Sakai", "Chen-Yu Liu"], "title": "Tensor Network Generator-Enhanced Optimization for Traveling Salesman Problem", "comment": "11 pages, 7 figures", "summary": "We present an application of the tensor network generator-enhanced optimization (TN-GEO) framework to address the traveling salesman problem (TSP), a fundamental combinatorial optimization challenge. Our approach employs a tensor network Born machine based on automatically differentiable matrix product states (MPS) as the generative model, using the Born rule to define probability distributions over candidate solutions. Unlike approaches based on binary encoding, which require $N^2$ variables and penalty terms to enforce valid tour constraints, we adopt a permutation-based formulation with integer variables and use autoregressive sampling with masking to guarantee that every generated sample is a valid tour by construction. We also introduce a $k$-site MPS variant that learns distributions over $k$-grams (consecutive city subsequences) using a sliding window approach, enabling parameter-efficient modeling for larger instances. Experimental validation on TSPLIB benchmark instances with up to 52 cities demonstrates that TN-GEO can outperform classical heuristics including swap and 2-opt hill-climbing. The $k$-site variants, which put more focus on local correlations, show better results compared to the full-MPS case."}
{"id": "2602.20223", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20223", "abs": "https://arxiv.org/abs/2602.20223", "authors": ["Wall Kim", "Chaeyoung Song", "Hanul Kim"], "title": "MultiModalPFN: Extending Prior-Data Fitted Networks for Multimodal Tabular Learning", "comment": "Accepted to CVPR 2026", "summary": "Recently, TabPFN has gained attention as a foundation model for tabular data. However, it struggles to integrate heterogeneous modalities such as images and text, which are common in domains like healthcare and marketing, thereby limiting its applicability. To address this, we present the Multi-Modal Prior-data Fitted Network (MMPFN), which extends TabPFN to handle tabular and non-tabular modalities in a unified manner. MMPFN comprises per-modality encoders, modality projectors, and pre-trained foundation models. The modality projectors serve as the critical bridge, transforming non-tabular embeddings into tabular-compatible tokens for unified processing. To this end, we introduce a multi-head gated MLP and a cross-attention pooler that extract richer context from non-tabular inputs while mitigates attention imbalance issue in multimodal learning. Extensive experiments on medical and general-purpose multimodal datasets demonstrate that MMPFN consistently outperforms competitive state-of-the-art methods and effectively exploits non-tabular modalities alongside tabular features. These results highlight the promise of extending prior-data fitted networks to the multimodal setting, offering a scalable and effective framework for heterogeneous data learning. The source code is available at https://github.com/too-z/MultiModalPFN."}
{"id": "2602.20256", "categories": ["cond-mat.stat-mech", "cond-mat.dis-nn", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.20256", "abs": "https://arxiv.org/abs/2602.20256", "authors": ["Feng He", "Arthur Hutsalyuk", "Giuseppe Mussardo", "Andrea Stampiggi"], "title": "Spectral Decimation of Quantum Many-Body Hamiltonians", "comment": "16+6 pages; 5+3 figures", "summary": "We develop a systematic theory of spectral decimation for quantum many-body Hamiltonians and show that it provides a quantitative probe of emergent symmetries in statistically mixed spectra. Building on an analytical description of statistical mixtures, we derive an explicit expression for the size of a characteristic symmetry sector (CSS), defined as the largest subsequence of levels exhibiting non-Poissonian correlations. The CSS dimension is shown to be the size-biased average of the underlying symmetry sectors, establishing a direct link between spectral statistics and Hilbert-space structure. We apply this framework to two paradigmatic settings: Hilbert-space fragmentation and disorder-induced many-body localization (MBL). In fragmented systems, the CSS reproduces the mixture prediction and isolates correlated subsectors even when the full spectrum appears nearly Poissonian. In the disordered Heisenberg chain, spectral decimation reveals the gradual emergence of integrability through a shrinking CSS, whose statistics exhibit signatures consistent with local integrals of motion. We introduce a characteristic symmetry entropy (CSE) as a finite-size scaling observable and extract, within accessible system sizes, the crossover exponents. Our results establish spectral decimation as a controlled, unbiased and computationally inexpensive diagnostic of hidden structure in many-body spectra, capable of distinguishing between chaotic dynamics, statistical mixtures, and emergent integrability."}
{"id": "2602.20224", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20224", "abs": "https://arxiv.org/abs/2602.20224", "authors": ["Lana E. Yeganova", "Won G. Kim", "Shubo Tian", "Natalie Xie", "Donald C. Comeau", "W. John Wilbur", "Zhiyong Lu"], "title": "Exploring Anti-Aging Literature via ConvexTopics and Large Language Models", "comment": null, "summary": "The rapid expansion of biomedical publications creates challenges for organizing knowledge and detecting emerging trends, underscoring the need for scalable and interpretable methods. Common clustering and topic modeling approaches such as K-means or LDA remain sensitive to initialization and prone to local optima, limiting reproducibility and evaluation. We propose a reformulation of a convex optimization based clustering algorithm that produces stable, fine-grained topics by selecting exemplars from the data and guaranteeing a global optimum. Applied to about 12,000 PubMed articles on aging and longevity, our method uncovers topics validated by medical experts. It yields interpretable topics spanning from molecular mechanisms to dietary supplements, physical activity, and gut microbiota. The method performs favorably, and most importantly, its reproducibility and interpretability distinguish it from common clustering approaches, including K-means, LDA, and BERTopic. This work provides a basis for developing scalable, web-accessible tools for knowledge discovery."}
{"id": "2602.20600", "categories": ["cond-mat.str-el", "cond-mat.mes-hall", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.20600", "abs": "https://arxiv.org/abs/2602.20600", "authors": ["Debraj Debata", "Abhirup Mukherjee", "Siddhartha Lal"], "title": "Kondo breakdown as an entanglement transition driven by continuous measurement", "comment": "28 pages, 19 Figures", "summary": "We study the breakdown of Kondo screening by a local magnetic field from the perspective of a measurement-driven entanglement transition in a monitored quantum system. Here, the Kondo coupling leads to the growth in entanglement of an impurity spin with it's fermionic environment, while the local field plays the role of a continuous observer. Using a non-perturbative Unitary Renormalization Group (URG) approach, we derive coupled renormalization-group flow equations for the Kondo exchange and the local field, and obtain a field-dependent RG phase diagram. The RG flows separate a low-energy Kondo-screened phase, where the impurity is absorbed into the Fermi sea and forms an entangled singlet with the conduction bath, from a polarized local-moment phase in which screening is frustrated and impurity-bath entanglement is suppressed. We identify the fixed-point Hamiltonians governing the two phases and the critical regime, and relate the transition to the emergence of a novel non-Fermi liquid. Various impurity signatures such as the spectral function and thermalisation of impurity observables are used to characterise this entanglement transition. These results offer insight into the interplay of decoherence and measurement in governing the dynamics of a prototypical quantum system."}
{"id": "2602.20306", "categories": ["cs.LG", "cs.AI", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.20306", "abs": "https://arxiv.org/abs/2602.20306", "authors": ["Davide Carrara", "Marc Hirschvogel", "Francesca Bonizzoni", "Stefano Pagani", "Simone Pezzuto", "Francesco Regazzoni"], "title": "Shape-informed cardiac mechanics surrogates in data-scarce regimes via geometric encoding and generative augmentation", "comment": "39 pages, 19 figures", "summary": "High-fidelity computational models of cardiac mechanics provide mechanistic insight into the heart function but are computationally prohibitive for routine clinical use. Surrogate models can accelerate simulations, but generalization across diverse anatomies is challenging, particularly in data-scarce settings. We propose a two-step framework that decouples geometric representation from learning the physics response, to enable shape-informed surrogate modeling under data-scarce conditions. First, a shape model learns a compact latent representation of left ventricular geometries. The learned latent space effectively encodes anatomies and enables synthetic geometries generation for data augmentation. Second, a neural field-based surrogate model, conditioned on this geometric encoding, is trained to predict ventricular displacement under external loading. The proposed architecture performs positional encoding by using universal ventricular coordinates, which improves generalization across diverse anatomies. Geometric variability is encoded using two alternative strategies, which are systematically compared: a PCA-based approach suitable for working with point cloud representations of geometries, and a DeepSDF-based implicit neural representation learned directly from point clouds. Overall, our results, obtained on idealized and patient-specific datasets, show that the proposed approaches allow for accurate predictions and generalization to unseen geometries, and robustness to noisy or sparsely sampled inputs."}
{"id": "2602.21198", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21198", "abs": "https://arxiv.org/abs/2602.21198", "authors": ["Yining Hong", "Huang Huang", "Manling Li", "Li Fei-Fei", "Jiajun Wu", "Yejin Choi"], "title": "Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs", "comment": null, "summary": "Embodied LLMs endow robots with high-level task reasoning, but they cannot reflect on what went wrong or why, turning deployment into a sequence of independent trials where mistakes repeat rather than accumulate into experience. Drawing upon human reflective practitioners, we introduce Reflective Test-Time Planning, which integrates two modes of reflection: \\textit{reflection-in-action}, where the agent uses test-time scaling to generate and score multiple candidate actions using internal reflections before execution; and \\textit{reflection-on-action}, which uses test-time training to update both its internal reflection model and its action policy based on external reflections after execution. We also include retrospective reflection, allowing the agent to re-evaluate earlier decisions and perform model updates with hindsight for proper long-horizon credit assignment. Experiments on our newly-designed Long-Horizon Household benchmark and MuJoCo Cupboard Fitting benchmark show significant gains over baseline models, with ablative studies validating the complementary roles of reflection-in-action and reflection-on-action. Qualitative analyses, including real-robot trials, highlight behavioral correction through reflection."}
{"id": "2602.21196", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.21196", "abs": "https://arxiv.org/abs/2602.21196", "authors": ["Ravi Ghadia", "Maksim Abraham", "Sergei Vorobyov", "Max Ryabinin"], "title": "Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking", "comment": "14 pages, 6 figures", "summary": "Efficiently processing long sequences with Transformer models usually requires splitting the computations across accelerators via context parallelism. The dominant approaches in this family of methods, such as Ring Attention or DeepSpeed Ulysses, enable scaling over the context dimension but do not focus on memory efficiency, which limits the sequence lengths they can support. More advanced techniques, such as Fully Pipelined Distributed Transformer or activation offloading, can further extend the possible context length at the cost of training throughput. In this paper, we present UPipe, a simple yet effective context parallelism technique that performs fine-grained chunking at the attention head level. This technique significantly reduces the activation memory usage of self-attention, breaking the activation memory barrier and unlocking much longer context lengths. Our approach reduces intermediate tensor memory usage in the attention layer by as much as 87.5$\\%$ for 32B Transformers, while matching previous context parallelism techniques in terms of training speed. UPipe can support the context length of 5M tokens when training Llama3-8B on a single 8$\\times$H100 node, improving upon prior methods by over 25$\\%$."}
{"id": "2602.21198", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21198", "abs": "https://arxiv.org/abs/2602.21198", "authors": ["Yining Hong", "Huang Huang", "Manling Li", "Li Fei-Fei", "Jiajun Wu", "Yejin Choi"], "title": "Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs", "comment": null, "summary": "Embodied LLMs endow robots with high-level task reasoning, but they cannot reflect on what went wrong or why, turning deployment into a sequence of independent trials where mistakes repeat rather than accumulate into experience. Drawing upon human reflective practitioners, we introduce Reflective Test-Time Planning, which integrates two modes of reflection: \\textit{reflection-in-action}, where the agent uses test-time scaling to generate and score multiple candidate actions using internal reflections before execution; and \\textit{reflection-on-action}, which uses test-time training to update both its internal reflection model and its action policy based on external reflections after execution. We also include retrospective reflection, allowing the agent to re-evaluate earlier decisions and perform model updates with hindsight for proper long-horizon credit assignment. Experiments on our newly-designed Long-Horizon Household benchmark and MuJoCo Cupboard Fitting benchmark show significant gains over baseline models, with ablative studies validating the complementary roles of reflection-in-action and reflection-on-action. Qualitative analyses, including real-robot trials, highlight behavioral correction through reflection."}
{"id": "2602.20303", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20303", "abs": "https://arxiv.org/abs/2602.20303", "authors": ["Joyanta Jyoti Mondal"], "title": "Multilevel Determinants of Overweight and Obesity Among U.S. Children Aged 10-17: Comparative Evaluation of Statistical and Machine Learning Approaches Using the 2021 National Survey of Children's Health", "comment": null, "summary": "Background: Childhood and adolescent overweight and obesity remain major public health concerns in the United States and are shaped by behavioral, household, and community factors. Their joint predictive structure at the population level remains incompletely characterized. Objectives: The study aims to identify multilevel predictors of overweight and obesity among U.S. adolescents and compare the predictive performance, calibration, and subgroup equity of statistical, machine-learning, and deep-learning models. Data and Methods: We analyze 18,792 children aged 10-17 years from the 2021 National Survey of Children's Health. Overweight/obesity is defined using BMI categories. Predictors included diet, physical activity, sleep, parental stress, socioeconomic conditions, adverse experiences, and neighborhood characteristics. Models include logistic regression, random forest, gradient boosting, XGBoost, LightGBM, multilayer perceptron, and TabNet. Performance is evaluated using AUC, accuracy, precision, recall, F1 score, and Brier score. Results: Discrimination range from 0.66 to 0.79. Logistic regression, gradient boosting, and MLP showed the most stable balance of discrimination and calibration. Boosting and deep learning modestly improve recall and F1 score. No model was uniformly superior. Performance disparities across race and poverty groups persist across algorithms. Conclusion: Increased model complexity yields limited gains over logistic regression. Predictors consistently span behavioral, household, and neighborhood domains. Persistent subgroup disparities indicate the need for improved data quality and equity-focused surveillance rather than greater algorithmic complexity."}
{"id": "2602.20324", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20324", "abs": "https://arxiv.org/abs/2602.20324", "authors": ["Cathy Shyr", "Yan Hu", "Rory J. Tinker", "Thomas A. Cassini", "Kevin W. Byram", "Rizwan Hamid", "Daniel V. Fabbri", "Adam Wright", "Josh F. Peterson", "Lisa Bastarache", "Hua Xu"], "title": "An artificial intelligence framework for end-to-end rare disease phenotyping from clinical notes using large language models", "comment": null, "summary": "Phenotyping is fundamental to rare disease diagnosis, but manual curation of structured phenotypes from clinical notes is labor-intensive and difficult to scale. Existing artificial intelligence approaches typically optimize individual components of phenotyping but do not operationalize the full clinical workflow of extracting features from clinical text, standardizing them to Human Phenotype Ontology (HPO) terms, and prioritizing diagnostically informative HPO terms. We developed RARE-PHENIX, an end-to-end AI framework for rare disease phenotyping that integrates large language model-based phenotype extraction, ontology-grounded standardization to HPO terms, and supervised ranking of diagnostically informative phenotypes. We trained RARE-PHENIX using data from 2,671 patients across 11 Undiagnosed Diseases Network clinical sites, and externally validated it on 16,357 real-world clinical notes from Vanderbilt University Medical Center. Using clinician-curated HPO terms as the gold standard, RARE-PHENIX consistently outperformed a state-of-the-art deep learning baseline (PhenoBERT) across ontology-based similarity and precision-recall-F1 metrics in end-to-end evaluation (i.e., ontology-based similarity of 0.70 vs. 0.58). Ablation analyses demonstrated performance improvements with the addition of each module in RARE-PHENIX (extraction, standardization, and prioritization), supporting the value of modeling the full clinical phenotyping workflow. By modeling phenotyping as a clinically aligned workflow rather than a single extraction task, RARE-PHENIX provides structured, ranked phenotypes that are more concordant with clinician curation and has the potential to support human-in-the-loop rare disease diagnosis in real-world settings."}
{"id": "2602.20422", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20422", "abs": "https://arxiv.org/abs/2602.20422", "authors": ["Hanping Zhang", "Yuhong Guo"], "title": "Diffusion Modulation via Environment Mechanism Modeling for Planning", "comment": null, "summary": "Diffusion models have shown promising capabilities in trajectory generation for planning in offline reinforcement learning (RL). However, conventional diffusion-based planning methods often fail to account for the fact that generating trajectories in RL requires unique consistency between transitions to ensure coherence in real environments. This oversight can result in considerable discrepancies between the generated trajectories and the underlying mechanisms of a real environment. To address this problem, we propose a novel diffusion-based planning method, termed as Diffusion Modulation via Environment Mechanism Modeling (DMEMM). DMEMM modulates diffusion model training by incorporating key RL environment mechanisms, particularly transition dynamics and reward functions. Experimental results demonstrate that DMEMM achieves state-of-the-art performance for planning with offline reinforcement learning."}
{"id": "2602.20502", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20502", "abs": "https://arxiv.org/abs/2602.20502", "authors": ["Hongbin Zhong", "Fazle Faisal", "Luis França", "Tanakorn Leesatapornwongsa", "Adriana Szekeres", "Kexin Rong", "Suman Nath"], "title": "ActionEngine: From Reactive to Programmatic GUI Agents via State Machine Memory", "comment": null, "summary": "Existing Graphical User Interface (GUI) agents operate through step-by-step calls to vision language models--taking a screenshot, reasoning about the next action, executing it, then repeating on the new page--resulting in high costs and latency that scale with the number of reasoning steps, and limited accuracy due to no persistent memory of previously visited pages.\n  We propose ActionEngine, a training-free framework that transitions from reactive execution to programmatic planning through a novel two-agent architecture: a Crawling Agent that constructs an updatable state-machine memory of the GUIs through offline exploration, and an Execution Agent that leverages this memory to synthesize complete, executable Python programs for online task execution.\n  To ensure robustness against evolving interfaces, execution failures trigger a vision-based re-grounding fallback that repairs the failed action and updates the memory.\n  This design drastically improves both efficiency and accuracy: on Reddit tasks from the WebArena benchmark, our agent achieves 95% task success with on average a single LLM call, compared to 66% for the strongest vision-only baseline, while reducing cost by 11.8x and end-to-end latency by 2x.\n  Together, these components yield scalable and reliable GUI interaction by combining global programmatic planning, crawler-validated action templates, and node-level execution with localized validation and repair."}
{"id": "2602.21201", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21201", "abs": "https://arxiv.org/abs/2602.21201", "authors": ["Tony Feng", "Junehyuk Jung", "Sang-hyun Kim", "Carlo Pagano", "Sergei Gukov", "Chiang-Chiang Tsai", "David Woodruff", "Adel Javanmard", "Aryan Mokhtari", "Dawsen Hwang", "Yuri Chervonyi", "Jonathan N. Lee", "Garrett Bingham", "Trieu H. Trinh", "Vahab Mirrokni", "Quoc V. Le", "Thang Luong"], "title": "Aletheia tackles FirstProof autonomously", "comment": "34 pages. Project page: https://github.com/google-deepmind/superhuman/tree/main/aletheia", "summary": "We report the performance of Aletheia (Feng et al., 2026b), a mathematics research agent powered by Gemini 3 Deep Think, on the inaugural FirstProof challenge. Within the allowed timeframe of the challenge, Aletheia autonomously solved 6 problems (2, 5, 7, 8, 9, 10) out of 10 according to majority expert assessments; we note that experts were not unanimous on Problem 8 (only). For full transparency, we explain our interpretation of FirstProof and disclose details about our experiments as well as our evaluation. Raw prompts and outputs are available at https://github.com/google-deepmind/superhuman/tree/main/aletheia."}
