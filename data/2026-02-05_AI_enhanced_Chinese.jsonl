{"id": "2602.03925", "categories": ["cond-mat.str-el", "cond-mat.mes-hall", "cond-mat.supr-con"], "pdf": "https://arxiv.org/pdf/2602.03925", "abs": "https://arxiv.org/abs/2602.03925", "authors": ["Zhi-Qiang Gao", "Yan-Qi Wang", "Ya-Hui Zhang", "Hui Yang"], "title": "Primary charge-4e superconductivity from doping a featureless Mott insulator", "comment": "4.5+7 pages, 4+4 figures", "summary": "Superconductivity is usually understood as a phase in which charge-$2e$ Cooper pairs are condensed. Charge-$4e$ superconductivity has largely been discussed as a vestigial order at finite temperature emerging from charge-$2e$ states. Primary charge-$4e$ superconducting phases at zero temperature remain scarce in both experiments and microscopic models. Here we argue that a doped featureless Mott insulator with $SU(4)$ symmetry provides a natural platform for primary charge-$4e$ superconductivity, based on perturbative renormalization group arguments and group theoretic considerations. As a concrete realization, we construct a bilayer Hubbard model with tunable onsite $SU(4)$ and $Sp(4)$ symmetries that exhibits a featureless Mott insulating phase at half filling. Its low energy physics is captured by a generalized ESD model, featuring an effective Hamiltonian that is purely kinetic within the constrained Hilbert space. Using density matrix renormalization group (DMRG) simulations, we find a primary charge-$4e$ superconducting phase in the $SU(4)$ ESD model and a conventional primary charge-$2e$ phase in the $Sp(4)$ case. We further characterize the corresponding normal states and discuss the resulting finite temperature phase diagram.", "AI": {"tldr": "A doped SU(4)-symmetric Mott insulator enables primary charge-4e superconductivity at zero temperature, distinct from conventional charge-2e pairing.", "motivation": "Primary charge-4e superconductivity at zero temperature is rare in experiments and models; conventional understanding treats it as a vestigial high-temperature phase of charge-2e states. This work seeks a fundamental platform for intrinsic charge-4e pairing.", "method": "Used perturbative renormalization group and group theory on a doped featureless Mott insulator with SU(4) symmetry; constructed a tunable bilayer Hubbard model exhibiting a featureless Mott insulating phase; analyzed low-energy physics via a generalized ESD model with purely kinetic effective Hamiltonian within constrained Hilbert space; employed DMRG simulations.", "result": "DMRG simulations revealed a primary charge-4e superconducting phase in the SU(4) ESD model, contrasting with a conventional primary charge-2e phase in the Sp(4) case; characterized normal states and derived the finite-temperature phase diagram.", "conclusion": "SU(4)-symmetric doped Mott insulators provide a natural platform for primary charge-4e superconductivity, demonstrating a rare zero-temperature phase beyond vestigial charge-2e mechanisms."}}
{"id": "2602.03936", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.03936", "abs": "https://arxiv.org/abs/2602.03936", "authors": ["Amanda A. Konieczna", "P. Peter Stavropoulos", "Roser Valent\u00ed"], "title": "Revealing the microscopic origin of the magnetization plateau in Na$_3$Ni$_2$BiO$_6$", "comment": "11 pages, 8 figures", "summary": "Recent experimental studies of the spin-1 honeycomb antiferromagnet Na$_3$Ni$_2$BiO$_6$ have revealed a pronounced one-third magnetization plateau under applied magnetic fields, highlighting the presence of strong magnetic frustration and anisotropy in this material. Such behavior has been attributed to substantial bond-dependent Kitaev interactions in combination with single-ion anisotropy, placing Na$_3$Ni$_2$BiO$_6$ among honeycomb compounds of interest for unconventional magnetic phases. Motivated by these observations, we present a first-principles-based analysis of the magnetic interactions in Na$_3$Ni$_2$BiO$_6$. By combining density-functional calculations with microscopic modeling, we extract the relevant exchange parameters and construct an effective spin model that quantitatively reproduces both the elastic neutron-scattering spectra and the magnetization curve. The model captures the experimentally observed zero-field zigzag magnetic order, and proposes a $\\textit{double-zigzag}$ state at intermediate magnetic fields, realizing the 1/3-magnetization plateau in a simpler way than suggested in previous works. Crucially, we show that the one-third magnetization plateau does not require Kitaev interactions; instead, it arises from the interplay of strong out-of-plane single-ion anisotropy and competing ferromagnetic nearest-neighbor ($J_1$) and antiferromagnetic third-neighbor ($J_3$) Heisenberg couplings. These results establish a consistent microscopic description of Na$_3$Ni$_2$BiO$_6$ and clarify the origin of its field-induced plateau phase.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\u63ed\u793aNa\u2083Ni\u2082BiO\u2086\u6750\u6599\u4e2d1/3\u78c1\u5316\u5e73\u53f0\u6e90\u4e8e\u5355\u79bb\u5b50\u5404\u5411\u5f02\u6027\u4e0eJ\u2081-J\u2083\u4ea4\u6362\u7ade\u4e89\uff0c\u65e0\u9700\u4f9d\u8d56Kitaev\u76f8\u4e92\u4f5c\u7528", "motivation": "\u5b9e\u9a8c\u89c2\u6d4b\u5230Na\u2083Ni\u2082BiO\u2086\u5b58\u5728\u663e\u84571/3\u78c1\u5316\u5e73\u53f0\uff0c\u4f20\u7edf\u7406\u8bba\u5f52\u56e0\u4e8eKitaev\u76f8\u4e92\u4f5c\u7528\uff0c\u4f46\u5fae\u89c2\u673a\u5236\u5b58\u5728\u4e89\u8bae\uff0c\u9700\u5efa\u7acb\u5b9a\u91cf\u6a21\u578b\u6f84\u6e05\u8d77\u6e90", "method": "\u7ed3\u5408\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\u8ba1\u7b97\u4e0e\u5fae\u89c2\u81ea\u65cb\u6a21\u578b\u6784\u5efa\uff0c\u63d0\u53d6\u4ea4\u6362\u53c2\u6570\u5e76\u62df\u5408\u4e2d\u5b50\u6563\u5c04\u8c31\u4e0e\u78c1\u5316\u66f2\u7ebf", "result": "\u63d0\u51fa\u53cczigzag\u78c1\u7ed3\u6784\u89e3\u91ca\u4e2d\u95f4\u573a1/3\u5e73\u53f0\uff0c\u8bc1\u660e\u5f3a\u9762\u5916\u5355\u79bb\u5b50\u5404\u5411\u5f02\u6027\u4e0e\u94c1\u78c1J\u2081/\u53cd\u94c1\u78c1J\u2083\u7ade\u4e89\u5373\u53ef\u5b9e\u73b0\u8be5\u5e73\u53f0\uff0c\u65e0\u9700Kitaev\u4f5c\u7528", "conclusion": "\u5efa\u7acb\u4e86Na\u2083Ni\u2082BiO\u2086\u7684\u5fae\u89c2\u81ea\u6d3d\u63cf\u8ff0\uff0c\u4fee\u6b63\u4e86Kitaev\u76f8\u4e92\u4f5c\u7528\u5fc5\u8981\u6027\u7684\u4f20\u7edf\u8ba4\u77e5\uff0c\u4e3a\u963b\u632b\u78c1\u4f53\u7814\u7a76\u63d0\u4f9b\u66f4\u7b80\u5316\u7684\u7406\u8bba\u6846\u67b6"}}
{"id": "2602.04002", "categories": ["cond-mat.str-el", "hep-th"], "pdf": "https://arxiv.org/pdf/2602.04002", "abs": "https://arxiv.org/abs/2602.04002", "authors": ["Rodrigo Corso"], "title": "Boundary and Symmetry Breaking in a Deformed Toric Code", "comment": "20 pages, 8 figues, comments are welcome", "summary": "This work explores a deformation of the Kitaev toric code that induces a phase transition out of the topologically ordered phase. By placing the model on a cylinder, the bulk global 1-form symmetries separate into distinct boundary operators, allowing us to show that the transition is accompanied by the breaking of one higher-form symmetry. Using a holographic $(1+1)$-dimensional boundary Hamiltonian, we extract an effective central charge and find a pronounced suppression near $\u03b2_c$, followed by its restoration at strong coupling, indicating sensitivity to bulk criticality rather than topological order.", "AI": {"tldr": "This paper studies a deformed Kitaev toric code on a cylinder, revealing that its phase transition out of topological order involves breaking of a 1-form symmetry, with holographic analysis showing central charge suppression at criticality indicating bulk criticality-driven transition.", "motivation": "To understand how topological order is lost during phase transitions by deforming the Kitaev toric code model and probing the underlying mechanisms, particularly the role of higher-form symmetries and criticality.", "method": "Deforming the Kitaev toric code on a cylindrical geometry to separate bulk 1-form symmetries into boundary operators, combined with holographic (1+1)-dimensional boundary Hamiltonian analysis to extract effective central charge.", "result": "The phase transition breaks a higher-form symmetry, and the holographic boundary analysis reveals a pronounced suppression of the effective central charge near the critical coupling \u03b2_c, followed by restoration at strong coupling, indicating sensitivity to bulk criticality rather than topological order.", "conclusion": "The phase transition out of topological order is driven by bulk criticality, evidenced by 1-form symmetry breaking and central charge behavior, challenging the notion that topological order loss alone dictates the transition."}}
{"id": "2602.04452", "categories": ["cond-mat.str-el", "cond-mat.dis-nn", "cond-mat.supr-con"], "pdf": "https://arxiv.org/pdf/2602.04452", "abs": "https://arxiv.org/abs/2602.04452", "authors": ["E. Yu. Beliayev", "Y. K. Mishra", "I. A. Chichibaba", "I. G. Mirzoiev", "V. A. Horielyi", "A. V. Terekhov"], "title": "Emergent Coherence at the Edge of Magnetism: Low-Doped La2-xSrxCuO4+delta Revisited", "comment": "27 pages, 6 figures. Review article", "summary": "The La2-xSrxCuO4+delta (LSCO) system provides a unique experimental setting for exploring how magnetism, superconductivity, and disorder jointly shape charge transport in a doped Mott insulator. Transport measurements in lightly doped and oxygen-enriched LSCO reveal a strongly insulating normal state governed by variable-range hopping, accompanied by pronounced nonlinear current-voltage characteristics and, at low temperatures, current-induced negative differential resistance. With increasing carrier concentration, these features evolve into regimes characterized by granular and percolative superconductivity near the threshold of bulk superconductivity and, eventually, into a homogeneous strange-metal state close to optimal doping. Throughout this evolution, the transport response shows marked sensitivity to disorder, electronic inhomogeneity, and external control parameters, such as bias current and magnetic field. Rather than reflecting a sequence of sharply distinct phases, the observed transport regimes form a continuous crossover from a localization-dominated insulating state to granular superconductivity and further to a coherent metallic state. This crossover is driven primarily by the progressive enhancement of electronic screening, inter-region coupling, and superconducting connectivity, rather than by abrupt changes in the underlying microscopic scattering mechanisms. Taken together, the available transport data provide a coherent experimental basis for understanding how disorder and mesoscale electronic inhomogeneity organize charge transport and superconductivity across the LSCO phase diagram, underscoring the central role of percolation and nonequilibrium effects in underdoped cuprates.", "AI": {"tldr": "\u7814\u7a76LSCO\u7cfb\u7edf\u53d1\u73b0\u4ece\u7edd\u7f18\u6001\u5230\u8d85\u5bfc\u6001\u518d\u5230\u91d1\u5c5e\u6001\u7684\u8fde\u7eed\u6f14\u5316\uff0c\u7531\u7535\u5b50\u5c4f\u853d\u3001\u533a\u57df\u8026\u5408\u548c\u8d85\u5bfc\u8fde\u901a\u6027\u9a71\u52a8\uff0c\u63ed\u793a\u4e86\u6e17\u6d41\u548c\u975e\u5e73\u8861\u6548\u5e94\u5728\u6b20\u63ba\u6742\u94dc\u9178\u76d0\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\u3002", "motivation": "\u63a2\u7d22\u78c1\u6027\u3001\u8d85\u5bfc\u6027\u548c\u65e0\u5e8f\u5982\u4f55\u5171\u540c\u8c03\u63a7\u63ba\u6742\u83ab\u7279\u7edd\u7f18\u4f53\u7684\u7535\u8377\u8f93\u8fd0\u884c\u4e3a\u3002", "method": "\u5728\u8f7b\u63ba\u6742\u548c\u5bcc\u6c27LSCO\u4e2d\u8fdb\u884c\u53d8\u8f7d\u6d41\u5b50\u6d53\u5ea6\u7684\u8f93\u8fd0\u6d4b\u91cf\u3002", "result": "\u53d1\u73b0\u4ece\u5c40\u57df\u5316\u7edd\u7f18\u6001\uff08\u53d8\u7a0b\u8dc3\u8fc1\u3001\u975e\u7ebf\u6027I-V\u3001\u8d1f\u5fae\u5206\u7535\u963b\uff09\u5230\u9897\u7c92/\u6e17\u6d41\u8d85\u5bfc\u6001\uff0c\u518d\u5230\u5947\u5f02\u91d1\u5c5e\u6001\u7684\u8fde\u7eed\u4ea4\u53c9\u3002\u6f14\u5316\u7531\u7535\u5b50\u5c4f\u853d\u589e\u5f3a\u3001\u533a\u57df\u8026\u5408\u548c\u8d85\u5bfc\u8fde\u901a\u6027\u9a71\u52a8\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u65e0\u5e8f\u548c\u4ecb\u89c2\u4e0d\u5747\u5300\u6027\u7ec4\u7ec7\u7535\u8377\u8f93\u8fd0\u548c\u8d85\u5bfc\u6027\u7684\u5b9e\u9a8c\u57fa\u7840\uff0c\u5f3a\u8c03\u4e86\u6e17\u6d41\u548c\u975e\u5e73\u8861\u6548\u5e94\u5728\u6b20\u63ba\u6742\u94dc\u9178\u76d0\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.03938", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.03938", "abs": "https://arxiv.org/abs/2602.03938", "authors": ["Piper C. Wysocki", "Luke D. Burkhart", "Madeline H. Morocco", "Corey I. Ostrove", "Riley J. Murray", "Tristan Brown", "Jeffrey M. Gertler", "David K. Kim", "Nathan E. Miller", "Bethany M. Niedzielski", "Katrina M. Sliwa", "Robin Blume-Kohout", "Gabriel O. Samach", "Mollie E. Schwartz", "Kenneth M. Rudinger"], "title": "Detailed, interpretable characterization of mid-circuit measurement on a transmon qubit", "comment": null, "summary": "Mid-circuit measurements (MCMs) are critical components of the quantum error correction protocols expected to enable utility-scale quantum computing. MCMs can be modeled by quantum instruments (a type of quantum operation or process), which can be characterized self-consistently using gate set tomography. However, experimentally estimated quantum instruments are often hard to interpret or relate to device physics. We address this challenge by adapting the error generator formalism -- previously used to interpret noisy quantum gates by decomposing their error processes into physically meaningful sums of \"elementary errors\" -- to MCMs. We deploy our new analysis on a transmon qubit device to tease out and quantify error mechanisms including amplitude damping, readout error, and imperfect collapse. We examine in detail how the magnitudes of these errors vary with the readout pulse amplitude, recover the key features of dispersive readout predicted by theory, and show that these features can be modeled parsimoniously using a reduced model with just a few parameters.", "AI": {"tldr": "\u5c06\u91cf\u5b50\u95e8\u8bef\u5dee\u5206\u6790\u4e2d\u7684\u8bef\u5dee\u751f\u6210\u5668\u5f62\u5f0f\u4f53\u7cfb\u521b\u65b0\u6027\u5730\u5e94\u7528\u4e8e\u4e2d\u7a0b\u6d4b\u91cf(MCMs)\uff0c\u6210\u529f\u89e3\u6784\u5e76\u91cf\u5316\u4e86\u8d85\u5bfc\u91cf\u5b50\u6bd4\u7279\u4e2d\u7684\u7269\u7406\u9519\u8bef\u673a\u5236\uff0c\u9a8c\u8bc1\u4e86\u8272\u6563\u8bfb\u53d6\u7406\u8bba\u7279\u5f81\uff0c\u5e76\u5efa\u7acb\u4e86\u7b80\u7ea6\u53c2\u6570\u6a21\u578b\u3002", "motivation": "\u5b9e\u9a8c\u6d4b\u5f97\u7684\u91cf\u5b50\u4eea\u5668\uff08\u63cf\u8ff0\u4e2d\u7a0b\u6d4b\u91cf\u7684\u91cf\u5b50\u8fc7\u7a0b\uff09\u96be\u4ee5\u76f4\u63a5\u5173\u8054\u5230\u8bbe\u5907\u7269\u7406\u673a\u5236\uff0c\u963b\u788d\u4e86\u5bf9\u91cf\u5b50\u7ea0\u9519\u5173\u952e\u7ec4\u4ef6\u7684\u6df1\u5165\u7406\u89e3\u548c\u4f18\u5316\u3002", "method": "\u5c06\u539f\u672c\u7528\u4e8e\u89e3\u91ca\u566a\u58f0\u91cf\u5b50\u95e8\u7684\u8bef\u5dee\u751f\u6210\u5668\u5f62\u5f0f\u4f53\u7cfb\u8fdb\u884c\u9002\u914d\uff0c\u5e94\u7528\u4e8e\u5206\u6790\u4e2d\u7a0b\u6d4b\u91cf\u8fc7\u7a0b\uff0c\u901a\u8fc7\u5206\u89e3\u8bef\u5dee\u8fc7\u7a0b\u4e3a\"\u57fa\u672c\u8bef\u5dee\"\u7684\u7269\u7406\u53ef\u89e3\u91ca\u6c42\u548c\u6765\u8868\u5f81\u8bbe\u5907\u3002", "result": "\u5728transmon\u91cf\u5b50\u6bd4\u7279\u8bbe\u5907\u4e0a\u6210\u529f\u5206\u79bb\u5e76\u91cf\u5316\u4e86\u632f\u5e45\u963b\u5c3c\u3001\u8bfb\u53d6\u9519\u8bef\u548c\u4e0d\u5b8c\u5168\u574d\u7f29\u7b49\u6838\u5fc3\u8bef\u5dee\u673a\u5236\uff1b\u63ed\u793a\u4e86\u8fd9\u4e9b\u8bef\u5dee\u968f\u8bfb\u53d6\u8109\u51b2\u632f\u5e45\u7684\u53d8\u5316\u89c4\u5f8b\uff1b\u901a\u8fc7\u6781\u7b80\u53c2\u6570\u6a21\u578b\u91cd\u73b0\u4e86\u8272\u6563\u8bfb\u53d6\u7406\u8bba\u9884\u6d4b\u7684\u5173\u952e\u7279\u5f81\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4e2d\u7a0b\u6d4b\u91cf\u8bef\u5dee\u7684\u7269\u7406\u53ef\u89e3\u91ca\u6027\u5206\u6790\uff0c\u4e3a\u7406\u89e3\u91cf\u5b50\u6d4b\u91cf\u8fc7\u7a0b\u7684\u566a\u58f0\u6839\u6e90\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u9a8c\u8bc1\u4e86\u7406\u8bba\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u5e76\u5efa\u7acb\u4e86\u53ef\u7528\u4e8e\u5b9e\u9645\u4f18\u5316\u7684\u7b80\u7ea6\u5316\u63cf\u8ff0\u6846\u67b6\u3002"}}
{"id": "2602.04091", "categories": ["nlin.CD"], "pdf": "https://arxiv.org/pdf/2602.04091", "abs": "https://arxiv.org/abs/2602.04091", "authors": ["Edson D. Leonel", "Jo\u00e3o P. C. Ferreira", "Diego F. M. Oliveira"], "title": "A phenomenological description of critical slowing down at period-doubling bifurcations", "comment": null, "summary": "We present a phenomenological description of the critical slowing down associated with period-doubling bifurcations in discrete dynamical systems. Starting from a local Taylor expansion around the fixed point and the bifurcation parameter, we derive a reduced description that captures the convergence towards stationary state both at and near criticality. At the bifurcation point, three universal critical exponents are obtained, characterising the short-time behaviour, the asymptotic decay, and the crossover between these regimes. Away from criticality, a fourth exponent governing the relaxation time is identified. We show this phenomenology, well established for one-dimensional maps, extends naturally to two-dimensional mappings. By projecting the dynamics onto the centre manifold, we demonstrate that the local normal form of a two-dimensional period-doubling bifurcation reduces to the same universal structure found in one dimension. The theoretical predictions are validated numerically using the H\u00e9non and Ikeda maps, showing excellent agreement for all scaling laws and critical exponents.", "AI": {"tldr": "This paper extends the theory of critical slowing down from 1D to 2D period-doubling bifurcations, identifying universal critical exponents and validating predictions with numerical models.", "motivation": "To generalize the established phenomenology of critical slowing down (previously limited to one-dimensional maps) to two-dimensional dynamical systems, capturing universal scaling behavior near period-doubling bifurcations.", "method": "Derives a reduced description using local Taylor expansion around fixed points and bifurcation parameters. Projects 2D dynamics onto a center manifold to reduce the system to a 1D universal normal form. Validates predictions numerically using H\u00e9non and Ikeda maps.", "result": "Identifies four universal critical exponents: three at the bifurcation point (governing short-time behavior, asymptotic decay, and crossover) and one away from criticality (governing relaxation time). Demonstrates that 2D period-doubling bifurcations exhibit identical universal scaling laws as 1D systems. Numerical results show excellent agreement with all theoretical scaling laws.", "conclusion": "The phenomenology of critical slowing down for period-doubling bifurcations is universally applicable to both one- and two-dimensional dynamical systems. The center manifold reduction confirms the structural equivalence of 2D bifurcations to the 1D universal description, with critical exponents providing a complete characterization of the dynamics near criticality."}}
{"id": "2602.03900", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03900", "abs": "https://arxiv.org/abs/2602.03900", "authors": ["Erik Goh", "John Kos", "Ashok Goel"], "title": "Knowledge Model Prompting Increases LLM Performance on Planning Tasks", "comment": null, "summary": "Large Language Models (LLM) can struggle with reasoning ability and planning tasks. Many prompting techniques have been developed to assist with LLM reasoning, notably Chain-of-Thought (CoT); however, these techniques, too, have come under scrutiny as LLMs' ability to reason at all has come into question. Borrowing from the domain of cognitive and educational science, this paper investigates whether the Task-Method-Knowledge (TMK) framework can improve LLM reasoning capabilities beyond its previously demonstrated success in educational applications. The TMK framework's unique ability to capture causal, teleological, and hierarchical reasoning structures, combined with its explicit task decomposition mechanisms, makes it particularly well-suited for addressing language model reasoning deficiencies, and unlike other hierarchical frameworks such as HTN and BDI, TMK provides explicit representations of not just what to do and how to do it, but also why actions are taken. The study evaluates TMK by experimenting on the PlanBench benchmark, focusing on the Blocksworld domain to test for reasoning and planning capabilities, examining whether TMK-structured prompting can help language models better decompose complex planning problems into manageable sub-tasks. Results also highlight significant performance inversion in reasoning models. TMK prompting enables the reasoning model to achieve up to an accuracy of 97.3\\% on opaque, symbolic tasks (Random versions of Blocksworld in PlanBench) where it previously failed (31.5\\%), suggesting the potential to bridge the gap between semantic approximation and symbolic manipulation. Our findings suggest that TMK functions not merely as context, but also as a mechanism that steers reasoning models away from their default linguistic modes to engage formal, code-execution pathways in the context of the experiments.", "AI": {"tldr": "The paper applies the Task-Method-Knowledge (TMK) framework from cognitive science to improve LLM reasoning. Testing on PlanBench's Blocksworld domain, TMK-structured prompting dramatically improved accuracy from 31.5% to 97.3% on symbolic tasks by helping models decompose problems and shift from linguistic to formal reasoning modes.", "motivation": "LLMs struggle with reasoning and planning tasks, and existing techniques like Chain-of-Thought are under scrutiny. The TMK framework shows promise due to its ability to capture causal, teleological, and hierarchical reasoning structures while explicitly representing what to do, how to do it, and why\u2014unlike other hierarchical frameworks.", "method": "The study evaluates TMK by experimenting on the PlanBench benchmark, focusing on the Blocksworld domain to test reasoning and planning capabilities. It uses TMK-structured prompting to help language models decompose complex planning problems into manageable sub-tasks.", "result": "TMK prompting enabled a reasoning model to achieve up to 97.3% accuracy on opaque, symbolic tasks (Random versions of Blocksworld) where it previously failed (31.5%), demonstrating a significant performance inversion and bridging the gap between semantic approximation and symbolic manipulation.", "conclusion": "TMK functions not merely as context but as a mechanism that steers reasoning models away from default linguistic modes to engage formal, code-execution pathways, suggesting its potential to enhance LLM reasoning capabilities."}}
{"id": "2602.03919", "categories": ["cond-mat.stat-mech", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.03919", "abs": "https://arxiv.org/abs/2602.03919", "authors": ["Airton Deppman"], "title": "Tsallis Entropy derived from the Chaitin-Kolmogorov Informational Entropy", "comment": "16 pages 1 figure", "summary": "We provide a rigorous first-principle derivation of the non-additive Tsallis' entropy by employing the Chaitin-Kolmogorov algorithmic information theory. By applying non-local restrictive rules on the string formation (grammar), we show that the algorithmic cost follows a power-law of the string length, instead of the linear behaviour obtained in the classical theory. As a result, the Tsallis entropy governs the increase of information. We explore the result showing, through Landauer's limit, that the heat dissipation in systems with long-range correlations is diminished. The $\u03a9_q$ number, which remains incompressible, now offers the possibility of a continuous increase of complexity, measured by the parameter $q$. We show the consistency of the results by a numerical simulation, and discuss Zipf's law in light of the new findings.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.04619", "categories": ["cond-mat.dis-nn", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2602.04619", "abs": "https://arxiv.org/abs/2602.04619", "authors": ["Joshua Donald", "Ben A. Johnson", "Amir Mehrnejat", "Alex Gabbitas", "Arthur G. T. Coveney", "Alexander G. Balanov", "Sergey Savel'ev", "Pavel Borisov"], "title": "Scalable platform enabling reservoir computing with nanoporous oxide memristors for image recognition and time series prediction", "comment": null, "summary": "Typical mammal brains have some form of random connectivity between neurons. Reservoir computing, a neural network approach, uses random weights within its processing layer along with built-in recurrent connections and short-term, fading memory, and is shown to be time and training efficient in processing spatiotemporal signals. Here we prepared a niobium oxide-based thin film memristor device with intrinsic structural in-homogeneity in the form of random nanopores and performed computational tasks of XOR operations, image recognition, and time series prediction and reconstruction. For the latter task we chose a complex three-dimensional chaotic Lorenz-63 time series. By applying three temporal voltage waveforms individually across the device and training the readout layer with electrical current signals from a three-output physical reservoir, we achieved satisfactory prediction and reconstruction accuracy in comparison to the case of no reservoir. This work highlights the potential for scalable, on-chip devices using all-oxide reservoir systems, paving the way for energy-efficient neuromorphic electronics dealing with time signals.", "AI": {"tldr": "Niobium oxide memristor with random nanopores serves as physical reservoir for efficient spatiotemporal signal processing, demonstrating XOR, image recognition, and chaotic time series prediction", "motivation": "To develop energy-efficient neuromorphic hardware for processing complex time signals using reservoir computing principles inspired by biological neural networks", "method": "Fabricated niobium oxide thin-film memristor with intrinsic random nanopores as physical reservoir; applied three temporal voltage inputs and trained electrical current outputs for XOR, image recognition, and Lorenz-63 chaotic time series tasks", "result": "Achieved satisfactory prediction/reconstruction accuracy for complex 3D chaotic time series, outperforming non-reservoir approaches; validated functionality across multiple computational tasks", "conclusion": "Scalable all-oxide reservoir systems show strong potential for on-chip, energy-efficient neuromorphic electronics capable of processing spatiotemporal signals"}}
{"id": "2602.04730", "categories": ["nlin.CD", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2602.04730", "abs": "https://arxiv.org/abs/2602.04730", "authors": ["Christopher Amey", "Bala Sundaram", "Andrew C. Poje"], "title": "Semiclassical Structure of the Advection--Diffusion Spectrum in Mixed Phase Spaces", "comment": null, "summary": "We examine the spectral structure of the two-dimensional advection-diffusion operator in flows with mixed phase space at very large Peclet number. Using Fourier discretization combined with symmetry reduction and Krylov-Arnoldi methods, we compute on the order of one hundred leading eigenpairs reliably in the asymptotic, weak-diffusion regime. While the principal eigenvalue is asymptotically diffusive and localized on the largest regular region, the broader spectrum exhibits a rich organization controlled by local Lagrangian phase-space geometry. In particular, exponential mixing in chaotic regions rapidly suppresses correlations, whereas algebraic mixing in integrable regions generates long-lived coherent structures that dominate the slow and intermediate parts of the spectrum. We identify three distinct classes of eigenmodes: advective modes associated with transport on invariant tori, diffusive modes and, within the duffusive branch, tunneling modes arising from weak coupling between dynamically separated regular regions. Drawing on a semiclassical analogy, we assign quantum-number-like labels to these families and predict the appearance, scaling, and ordering of their sub-spectra directly from the Hamiltonian phase-space structure. The coexistence of these families implies that no uniform control of the spectral gap exists across the full spectrum: although the slowest mode is diffusive, arbitrarily small gaps arise between competing families at higher mode numbers. As a result, finite-time advection-diffusion dynamics is generically governed by persistent modal competition rather than single-mode dominance, even at asymptotically large Peclet number.", "AI": {"tldr": "\u7814\u7a76\u5927Peclet\u6570\u4e0b\u6df7\u5408\u76f8\u7a7a\u95f4\u4e8c\u7ef4\u5bf9\u6d41-\u6269\u6563\u7b97\u5b50\u7684\u8c31\u7ed3\u6784\uff0c\u53d1\u73b0\u4e09\u79cd\u7279\u5f81\u6a21\u5f0f\u65cf\uff08\u5bf9\u6d41\u3001\u6269\u6563\u3001\u96a7\u7a7f\u6a21\u5f0f\uff09\uff0c\u63ed\u793a\u5373\u4f7f\u5728\u9ad8Peclet\u6570\u4e0b\uff0c\u6709\u9650\u65f6\u95f4\u52a8\u529b\u5b66\u4ecd\u7531\u6a21\u5f0f\u7ade\u4e89\u800c\u975e\u5355\u4e00\u6a21\u5f0f\u4e3b\u5bfc\u3002", "motivation": "\u7406\u89e3\u6df7\u5408\u76f8\u7a7a\u95f4\u6d41\u52a8\uff08\u540c\u65f6\u5305\u542b\u89c4\u5219\u548c\u6df7\u6c8c\u533a\u57df\uff09\u4e2d\u7269\u8d28\u8f93\u8fd0\u7684\u8c31\u7279\u6027\uff0c\u7279\u522b\u662f\u5728\u5927Peclet\u6570\u5f31\u6269\u6563\u6781\u9650\u4e0b\uff0c\u89c4\u5219\u533a\u57df\u4e0e\u6df7\u6c8c\u533a\u57df\u7684\u4e0d\u540c\u6df7\u5408\u673a\u5236\u5982\u4f55\u5171\u540c\u5f71\u54cd\u6574\u4f53\u6269\u6563\u884c\u4e3a\u3002", "method": "\u91c7\u7528\u5085\u91cc\u53f6\u79bb\u6563\u5316\u7ed3\u5408\u5bf9\u79f0\u6027\u7ea6\u5316\u548cKrylov-Arnoldi\u65b9\u6cd5\uff0c\u5728\u5f31\u6269\u6563\u6e10\u8fd1 regime \u4e2d\u53ef\u9760\u8ba1\u7b97\u7ea6\u4e00\u767e\u4e2a\u4e3b\u5bfc\u7279\u5f81\u5bf9\u3002", "result": "\u4e3b\u7279\u5f81\u503c\u5448\u6269\u6563\u6027\u5e76\u5c40\u57df\u4e8e\u6700\u5927\u89c4\u5219\u533a\u57df\uff1b\u8c31\u7ed3\u6784\u7531\u62c9\u683c\u6717\u65e5\u76f8\u7a7a\u95f4\u51e0\u4f55\u63a7\u5236\uff1b\u8bc6\u522b\u51fa\u4e09\u7c7b\u7279\u5f81\u6a21\u5f0f\uff08\u5bf9\u6d41/\u6269\u6563/\u96a7\u7a7f\u6a21\u5f0f\uff09\uff1b\u6df7\u6c8c\u533a\u6307\u6570\u6df7\u5408\u5feb\u901f\u6291\u5236\u5173\u8054\uff0c\u53ef\u79ef\u533a\u4ee3\u6570\u6df7\u5408\u4ea7\u751f\u957f\u5bff\u547d\u76f8\u5e72\u7ed3\u6784\uff1b\u4e0d\u5b58\u5728\u7edf\u4e00\u7684\u8c31\u9699\u63a7\u5236\u3002", "conclusion": "\u6709\u9650\u65f6\u95f4\u5bf9\u6d41-\u6269\u6563\u52a8\u529b\u5b66\u53d7\u6301\u7eed\u7684\u6a21\u5f0f\u7ade\u4e89\u800c\u975e\u5355\u6a21\u4e3b\u5bfc\uff0c\u5373\u4f7fPeclet\u6570\u6e10\u8fd1\u589e\u5927\uff1b\u76f8\u7a7a\u95f4\u7ed3\u6784\u51b3\u5b9a\u4e86\u8c31\u7684\u6807\u5ea6\u548c\u6392\u5e8f\u3002"}}
{"id": "2602.03950", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.03950", "abs": "https://arxiv.org/abs/2602.03950", "authors": ["Aditya Basarkar", "Benyamin Tabarsi", "Tiffany Barnes", "Dongkuan", "Xu"], "title": "Enhancing Mathematical Problem Solving in LLMs through Execution-Driven Reasoning Augmentation", "comment": "9 pages, 7 figures, submitted to ACL ARR 2026", "summary": "Mathematical problem solving is a fundamental benchmark for assessing the reasoning capabilities of artificial intelligence and a gateway to applications in education, science, and engineering where reliable symbolic reasoning is essential. Although recent advances in multi-agent LLM-based systems have enhanced their mathematical reasoning capabilities, they still lack a reliably revisable representation of the reasoning process. Existing agents either operate in rigid sequential pipelines that cannot correct earlier steps or rely on heuristic self-evaluation that can fail to identify and fix errors. In addition, programmatic context can distract language models and degrade accuracy. To address these gaps, we introduce Iteratively Improved Program Construction (IIPC), a reasoning method that iteratively refines programmatic reasoning chains and combines execution feedback with the native Chain-of-thought abilities of the base LLM to maintain high-level contextual focus. IIPC surpasses competing approaches in the majority of reasoning benchmarks on multiple base LLMs. All code and implementations are released as open source.", "AI": {"tldr": "\u63d0\u51faIteratively Improved Program Construction (IIPC)\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u7a0b\u5e8f\u5316\u63a8\u7406\u94fe\u5e76\u7ed3\u5408\u6267\u884c\u53cd\u9988\u4e0e\u601d\u7ef4\u94fe\u80fd\u529b\uff0c\u63d0\u5347LLM\u6570\u5b66\u63a8\u7406\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u7f3a\u4e4f\u53ef\u4fee\u8ba2\u7684\u63a8\u7406\u8fc7\u7a0b\u8868\u793a\uff1a\u987a\u5e8f\u6d41\u6c34\u7ebf\u65e0\u6cd5\u7ea0\u9519\uff0c\u542f\u53d1\u5f0f\u81ea\u8bc4\u4f30\u53ef\u80fd\u9057\u6f0f\u9519\u8bef\uff0c\u7a0b\u5e8f\u5316\u4e0a\u4e0b\u6587\u8fd8\u4f1a\u5206\u6563\u6a21\u578b\u6ce8\u610f\u529b\u964d\u4f4e\u51c6\u786e\u7387\u3002", "method": "IIPC\u65b9\u6cd5\u8fed\u4ee3\u5730\u4f18\u5316\u7a0b\u5e8f\u5316\u63a8\u7406\u94fe\uff0c\u5c06\u4ee3\u7801\u6267\u884c\u53cd\u9988\u4e0e\u57fa\u7840LLM\u7684\u539f\u751f\u601d\u7ef4\u94fe\u80fd\u529b\u7ed3\u5408\uff0c\u4fdd\u6301\u9ad8\u5c42\u4e0a\u4e0b\u6587\u4e13\u6ce8\u5ea6\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u7840LLM\u4e0a\uff0cIIPC\u5728\u5927\u591a\u6570\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u7ade\u4e89\u65b9\u6cd5\u3002", "conclusion": "IIPC\u6709\u6548\u89e3\u51b3\u4e86\u7a0b\u5e8f\u5316\u63a8\u7406\u4e2d\u7684\u9519\u8bef\u4fee\u6b63\u548c\u4e0a\u4e0b\u6587\u5e72\u6270\u95ee\u9898\uff0c\u6240\u6709\u4ee3\u7801\u5f00\u6e90\u3002"}}
{"id": "2602.04200", "categories": ["cond-mat.stat-mech", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.04200", "abs": "https://arxiv.org/abs/2602.04200", "authors": ["Kevin Callahan-Coray", "Kyle Lee", "Kyle Jiang", "Kerem Y. Camsari"], "title": "Restoring Sparsity in Potts Machines via Mean-Field Constraints", "comment": null, "summary": "Ising machines and related probabilistic hardware have emerged as promising platforms for NP-hard optimization and sampling. However, many practical problems involve constraints that induce dense or all-to-all couplings, undermining scalability and hardware efficiency. We address this constraint-induced density through two complementary approaches. First, we introduce a hardware-aware native formulation for multi-state probabilistic digits (p-dits) that avoids the locally dense intra-variable couplings required by binary Ising encodings. We validate p-dit dynamics by reproducing known critical behavior of the 2D Potts model. Second, we propose mean-field constraints (MFC), a hybrid scheme that replaces dense pairwise constraint couplings with dynamically updated single-node biases. Applied to balanced graph partitioning, MFC achieves solution quality comparable to exact all-to-all constraint formulations while dramatically reducing graph density. Finally, we demonstrate the practical impact of restored sparsity by an FPGA implementation, enabling orders-of-magnitude acceleration over CPU-based solvers. Together, these results outline a pathway for scaling constrained optimization on probabilistic hardware.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u4f0a\u8f9b\u673a\u4e2d\u7ea6\u675f\u5bfc\u81f4\u7684\u5bc6\u96c6\u8026\u5408\u95ee\u9898\uff0c\u63d0\u51fa\u4e24\u79cd\u89e3\u51b3\u65b9\u6848\uff1a(1) \u91c7\u7528\u591a\u6001\u6982\u7387\u6570\u5b57(p-dits)\u7684\u539f\u751f\u786c\u4ef6\u53cb\u597d\u578b\u8868\u8ff0\uff0c\u907f\u514d\u4e8c\u8fdb\u5236\u4f0a\u8f9b\u7f16\u7801\u6240\u9700\u7684\u5c40\u90e8\u5bc6\u96c6\u53d8\u91cf\u5185\u8026\u5408\uff1b(2) \u5747\u503c\u573a\u7ea6\u675f(MFC)\uff0c\u7528\u52a8\u6001\u66f4\u65b0\u7684\u5355\u8282\u70b9\u504f\u7f6e\u66ff\u4ee3\u5bc6\u96c6\u7684\u4e24\u4e24\u7ea6\u675f\u8026\u5408\u3002\u8fd9\u4e9b\u65b9\u6cd5\u6062\u590d\u4e86\u7a00\u758f\u6027\uff0c\u5728FPGA\u4e0a\u5b9e\u73b0\u4e86\u76f8\u6bd4CPU\u6c42\u89e3\u5668\u6570\u91cf\u7ea7\u7684\u52a0\u901f\u3002", "motivation": "\u8bb8\u591a\u5b9e\u9645\u4f18\u5316\u95ee\u9898\u5305\u542b\u7ea6\u675f\uff0c\u8fd9\u4e9b\u7ea6\u675f\u5728\u4f0a\u8f9b\u673a\u4e2d\u4ea7\u751f\u5bc6\u96c6\u6216\u5168\u8fde\u63a5\u8026\u5408\uff0c\u635f\u5bb3\u4e86\u53ef\u6269\u5c55\u6027\u548c\u786c\u4ef6\u6548\u7387\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u79cd\u7ea6\u675f\u5f15\u8d77\u7684\u5bc6\u5ea6\u95ee\u9898\uff0c\u4f7f\u6982\u7387\u786c\u4ef6\u66f4\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u4e92\u8865\u65b9\u6cd5\uff1a(1) \u5f15\u5165\u786c\u4ef6\u611f\u77e5\u7684\u591a\u6001\u6982\u7387\u6570\u5b57(p-dits)\u539f\u751f\u8868\u8ff0\uff0c\u907f\u514d\u4e8c\u8fdb\u5236\u7f16\u7801\u6240\u9700\u7684\u5c40\u90e8\u5bc6\u96c6\u53d8\u91cf\u5185\u8026\u5408\uff0c\u5e76\u901a\u8fc7\u4e8c\u7ef4\u6ce2\u8328\u6a21\u578b\u6a21\u62df\u9a8c\u8bc1\uff1b(2) \u63d0\u51fa\u5747\u503c\u573a\u7ea6\u675f(MFC)\uff0c\u8fd9\u662f\u4e00\u79cd\u6df7\u5408\u65b9\u6848\uff0c\u7528\u52a8\u6001\u66f4\u65b0\u7684\u5355\u8282\u70b9\u504f\u7f6e\u66ff\u4ee3\u5bc6\u96c6\u7684\u4e24\u4e24\u7ea6\u675f\u8026\u5408\u3002\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8e\u5e73\u8861\u56fe\u5212\u5206\u95ee\u9898\uff0c\u5e76\u5728FPGA\u4e0a\u5b9e\u73b0\u3002", "result": "p-dit\u8868\u8ff0\u6210\u529f\u91cd\u73b0\u4e86\u4e8c\u7ef4\u6ce2\u8328\u6a21\u578b\u7684\u5df2\u77e5\u4e34\u754c\u884c\u4e3a\u3002MFC\u5728\u5b9e\u73b0\u4e0e\u7cbe\u786e\u5168\u8fde\u63a5\u7ea6\u675f\u516c\u5f0f\u76f8\u5f53\u89e3\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u56fe\u5bc6\u5ea6\u3002FPGA\u5b9e\u73b0\u76f8\u6bd4\u57fa\u4e8eCPU\u7684\u6c42\u89e3\u5668\u5b9e\u73b0\u4e86\u6570\u91cf\u7ea7\u7684\u52a0\u901f\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u901a\u8fc7\u6062\u590d\u7a00\u758f\u6027\u3001\u5b9e\u73b0\u9ad8\u6548\u786c\u4ef6\u5b9e\u73b0\u5e76\u4fdd\u6301\u89e3\u8d28\u91cf\uff0c\u4e3a\u6982\u7387\u786c\u4ef6\u4e0a\u7684\u7ea6\u675f\u4f18\u5316\u6269\u5c55\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2602.04061", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04061", "abs": "https://arxiv.org/abs/2602.04061", "authors": ["Sirui Chen", "Dragomir Davidovi\u0107"], "title": "Correlation-Enabled Beatings in Two-Dimensional Electronic Spectroscopy", "comment": null, "summary": "Long-lived beatings in two-dimensional electronic spectroscopy (2DES) remain difficult to interpret within standard excitonic open-system models, which typically assume factorized initialization and predict rapid coherence decay. We show that persistent beatings can arise from a correlation-driven mechanism that requires both slow bath memory and ultrafast pulse sequences that propagate system-bath correlations across optical interactions. In this regime, the pulse sequence unitarily dresses the bath-memory contribution and activates nonsecular population-coherence transfer during field-free evolution, sustaining coherence signatures far beyond factorized or weak-memory descriptions. Rather than addressing what is oscillating (excitonic versus vibronic) or quantum-versus-classical semantics, this work reframes long-lived beatings as a protocol-level dynamical effect: correlation-mediated retrieval under ultrafast control.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63ed\u793a\u4e8c\u7ef4\u7535\u5b50\u5149\u8c31\u4e2d\u7684\u957f\u5bff\u547d\u62cd\u9891\u6e90\u4e8e\u5173\u8054\u9a71\u52a8\u673a\u5236\uff0c\u6d89\u53ca\u6162\u70ed\u6d74\u8bb0\u5fc6\u548c\u8d85\u5feb\u8109\u51b2\u5e8f\u5217\uff0c\u5c06\u5176\u91cd\u65b0\u8be0\u91ca\u4e3a\u534f\u8bae\u5c42\u9762\u7684\u52a8\u529b\u5b66\u6548\u5e94\uff0c\u800c\u975e\u7b80\u5355\u7684\u6fc0\u5b50-\u632f\u52a8\u632f\u8361\u95ee\u9898\u3002", "motivation": "\u6807\u51c6\u6fc0\u5b50\u5f00\u653e\u7cfb\u7edf\u6a21\u578b\u56e0\u5047\u8bbe\u56e0\u5b50\u5316\u521d\u59cb\u5316\u548c\u5feb\u901f\u76f8\u5e72\u8870\u51cf\uff0c\u65e0\u6cd5\u89e3\u91ca\u4e8c\u7ef4\u7535\u5b50\u5149\u8c31\u4e2d\u7684\u957f\u5bff\u547d\u62cd\u9891\u73b0\u8c61\uff0c\u56e0\u6b64\u9700\u8981\u8d85\u8d8a\u4f20\u7edf\u6a21\u578b\u7684\u65b0\u673a\u5236\u6765\u89e3\u91ca\u8fd9\u79cd\u6301\u7eed\u76f8\u5e72\u6027\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63d0\u51fa\u5173\u8054\u9a71\u52a8\u673a\u5236\uff0c\u8be5\u673a\u5236\u9700\u8981\u6162\u70ed\u6d74\u8bb0\u5fc6\u548c\u8d85\u5feb\u8109\u51b2\u5e8f\u5217\u5728\u5149\u5b66\u76f8\u4e92\u4f5c\u7528\u4e2d\u4f20\u64ad\u7cfb\u7edf-\u70ed\u6d74\u5173\u8054\uff0c\u5b9e\u73b0\u534f\u8bae\u5c42\u9762\u7684\u76f8\u5e72\u8c03\u63a7\u3002", "result": "\u7814\u7a76\u8868\u660e\u8d85\u5feb\u8109\u51b2\u5e8f\u5217\u53ef\u5e7a\u6b63\u5730\u4fee\u9970\u70ed\u6d74\u8bb0\u5fc6\u8d21\u732e\uff0c\u5e76\u5728\u65e0\u573a\u6f14\u5316\u671f\u95f4\u6fc0\u6d3b\u975e\u4e16\u4fd7\u9879\u7684\u5e03\u5c45-\u76f8\u5e72\u8f6c\u79fb\uff0c\u4f7f\u76f8\u5e72\u4fe1\u53f7\u6301\u7eed\u65f6\u95f4\u8fdc\u8d85\u56e0\u5b50\u5316\u6216\u5f31\u8bb0\u5fc6\u63cf\u8ff0\u3002", "conclusion": "\u957f\u5bff\u547d\u62cd\u9891\u5e94\u91cd\u65b0\u5b9a\u4e49\u4e3a\u8d85\u5feb\u63a7\u5236\u4e0b\u7684\u5173\u8054\u4ecb\u5bfc\u68c0\u7d22\u8fd9\u4e00\u534f\u8bae\u5c42\u9762\u52a8\u529b\u5b66\u6548\u5e94\uff0c\u800c\u975e\u7ea0\u7f20\u4e8e\u6fc0\u5b50\u4e0e\u632f\u52a8\u7684\u672c\u8d28\u6216\u91cf\u5b50-\u7ecf\u5178\u8bed\u4e49\u4e4b\u4e89\u3002"}}
{"id": "2602.04793", "categories": ["nlin.CD", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04793", "abs": "https://arxiv.org/abs/2602.04793", "authors": ["F. J. Arranz", "F. Borondo"], "title": "Correspondence between classical and quantum resonances", "comment": null, "summary": "Bifurcations take place in molecular Hamiltonian nonlinear systems as the excitation energy increases, this leading to the appearance of different classical resonances. In this paper, we study the quantum manifestations of these classical resonances in the isomerizing system CN-Li$\\leftrightarrows$Li-CN. By using a correlation diagram of eigenenergies versus Planck constant, we show the existence of different series of avoided crossings, leading to the corresponding series of quantum resonances, which represent the quantum manifestations of the classical resonances. Moreover, the extrapolation of these series to $\\hbar=0$ unveils the correspondence between the bifurcation energy of classical resonances and the energy of the series of quantum resonances in the semiclassical limit $\\hbar\\to0$. Additionally, in order to obtain analytical expressions for our results, a semiclassical theory is developed.", "AI": {"tldr": "\u7814\u7a76\u5206\u5b50\u975e\u7ebf\u6027\u7cfb\u7edf\u4e2d\u7ecf\u5178\u5206\u5c94\u73b0\u8c61\u7684\u91cf\u5b50\u5bf9\u5e94\uff0c\u4ee5CN-Li\u5f02\u6784\u5316\u7cfb\u7edf\u4e3a\u4f8b\uff0c\u901a\u8fc7\u80fd\u7ea7-\u666e\u6717\u514b\u5e38\u6570\u5173\u8054\u56fe\u63ed\u793a\u91cf\u5b50\u5171\u632f\u7cfb\u5217\u4e0e\u7ecf\u5178\u5171\u632f\u7684\u5bf9\u5e94\u5173\u7cfb", "motivation": "\u63a2\u7d22\u7ecf\u5178\u975e\u7ebf\u6027\u7cfb\u7edf\u4e2d\u80fd\u91cf\u589e\u52a0\u5bfc\u81f4\u7684\u5206\u5c94\u73b0\u8c61\u5728\u91cf\u5b50\u4f53\u7cfb\u4e2d\u7684\u8868\u73b0\uff0c\u5efa\u7acb\u7ecf\u5178\u5171\u632f\u4e0e\u91cf\u5b50\u5171\u632f\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb", "method": "\u91c7\u7528\u80fd\u7ea7-\u666e\u6717\u514b\u5e38\u6570\u5173\u8054\u56fe\u5206\u6790\uff0c\u8bc6\u522b\u907f\u514d\u4ea4\u53c9\u7cfb\u5217\uff1b\u53d1\u5c55\u534a\u7ecf\u5178\u7406\u8bba\u8fdb\u884c\u89e3\u6790\u63a8\u5bfc\uff1b\u4ee5CN-Li\u21c6Li-CN\u5f02\u6784\u5316\u7cfb\u7edf\u4e3a\u5177\u4f53\u7814\u7a76\u5bf9\u8c61", "result": "\u53d1\u73b0\u4e00\u7cfb\u5217\u907f\u514d\u4ea4\u53c9\u7ed3\u6784\u5bf9\u5e94\u91cf\u5b50\u5171\u632f\u7cfb\u5217\uff1b\u5916\u63a8\u81f3\u210f=0\u65f6\uff0c\u7ecf\u5178\u5206\u5c94\u80fd\u91cf\u4e0e\u91cf\u5b50\u5171\u632f\u80fd\u91cf\u5728\u534a\u7ecf\u5178\u6781\u9650\u4e0b\u7cbe\u786e\u5bf9\u5e94", "conclusion": "\u91cf\u5b50\u5171\u632f\u7cfb\u5217\u662f\u7ecf\u5178\u5171\u632f\u7684\u91cf\u5b50\u8868\u73b0\uff1b\u534a\u7ecf\u5178\u7406\u8bba\u6210\u529f\u5efa\u7acb\u4e86\u7ecf\u5178\u5206\u5c94\u4e0e\u91cf\u5b50\u80fd\u7ea7\u7ed3\u6784\u7684\u5b9a\u91cf\u8054\u7cfb\uff0c\u9a8c\u8bc1\u4e86\u91cf\u5b50-\u7ecf\u5178\u5bf9\u5e94\u539f\u7406"}}
{"id": "2602.03876", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03876", "abs": "https://arxiv.org/abs/2602.03876", "authors": ["Kyuseong Choi", "Dwaipayan Saha", "Woojeong Kim", "Anish Agarwal", "Raaz Dwivedi"], "title": "GOPO: Policy Optimization using Ranked Rewards", "comment": "17 pages, 8 figures", "summary": "Standard reinforcement learning from human feedback (RLHF) trains a reward model on pairwise preference data and then uses it for policy optimization. However, while reward models are optimized to capture relative preferences, existing policy optimization techniques rely on absolute reward magnitudes during training. In settings where the rewards are non-verifiable such as summarization, instruction following, and chat completion, this misalignment often leads to suboptimal performance. We introduce Group Ordinal Policy Optimization (GOPO), a policy optimization method that uses only the ranking of the rewards and discards their magnitudes. Our rank-based transformation of rewards provides several gains, compared to Group Relative Policy Optimization (GRPO), in settings with non-verifiable rewards: (1) consistently higher training/validation reward trajectories, (2) improved LLM-as-judge evaluations across most intermediate training steps, and (3) reaching a policy of comparable quality in substantially less training steps than GRPO. We demonstrate consistent improvements across a range of tasks and model sizes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGOPO\uff08Group Ordinal Policy Optimization\uff09\uff0c\u4e00\u79cd\u4ec5\u4f7f\u7528\u5956\u52b1\u6392\u5e8f\u800c\u975e\u7edd\u5bf9\u6570\u503c\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u975e\u53ef\u9a8c\u8bc1\u5956\u52b1\u573a\u666f\u4e0bRLHF\u7684\u9519\u4f4d\u95ee\u9898\uff0c\u76f8\u6bd4GRPO\u8868\u73b0\u51fa\u66f4\u4f18\u6027\u80fd\u3002", "motivation": "\u6807\u51c6RLHF\u5728\u5956\u52b1\u6a21\u578b\u4e0a\u4f18\u5316\u76f8\u5bf9\u504f\u597d\uff0c\u4f46\u5728\u7b56\u7565\u4f18\u5316\u9636\u6bb5\u5374\u4f7f\u7528\u7edd\u5bf9\u5956\u52b1\u503c\uff0c\u8fd9\u79cd\u9519\u4f4d\u5bfc\u81f4\u5728\u975e\u53ef\u9a8c\u8bc1\u5956\u52b1\u573a\u666f\uff08\u5982\u6458\u8981\u751f\u6210\u3001\u6307\u4ee4\u9075\u5faa\u548c\u5bf9\u8bdd\u5b8c\u6210\uff09\u4e2d\u6027\u80fd\u6b21\u4f18\u3002", "method": "\u5206\u7ec4\u5e8f\u6570\u7b56\u7565\u4f18\u5316\uff08GOPO\uff09\uff0c\u8be5\u65b9\u6cd5\u5c06\u5956\u52b1\u8f6c\u6362\u4e3a\u57fa\u4e8e\u6392\u5e8f\u7684\u8868\u793a\uff0c\u5728\u7b56\u7565\u4f18\u5316\u8fc7\u7a0b\u4e2d\u820d\u5f03\u5956\u52b1\u7684\u5e45\u5ea6\u4fe1\u606f\u3002", "result": "GOPO\u5b9e\u73b0\u4e86\u6301\u7eed\u66f4\u9ad8\u7684\u8bad\u7ec3/\u9a8c\u8bc1\u5956\u52b1\u8f68\u8ff9\uff0c\u5728\u5927\u591a\u6570\u4e2d\u95f4\u8bad\u7ec3\u6b65\u9aa4\u4e2d\u6539\u8fdb\u4e86LLM-as-judge\u8bc4\u4f30\uff0c\u5e76\u4ee5\u6bd4GRPO\u663e\u8457\u66f4\u5c11\u7684\u8bad\u7ec3\u6b65\u6570\u8fbe\u5230\u76f8\u5f53\u7684\u7b56\u7565\u8d28\u91cf\uff0c\u5728\u5404\u79cd\u4efb\u52a1\u548c\u6a21\u578b\u89c4\u6a21\u4e0a\u5747\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u6539\u8fdb\u3002", "conclusion": "GOPO\u901a\u8fc7\u4f7f\u7528\u57fa\u4e8e\u6392\u5e8f\u7684\u5956\u52b1\u6709\u6548\u89e3\u51b3\u4e86\u975e\u53ef\u9a8c\u8bc1\u5956\u52b1\u573a\u666f\u4e0b\u7684RLHF\u9519\u4f4d\u95ee\u9898\uff0c\u5728\u6548\u7387\u548c\u6027\u80fd\u4e0a\u5747\u4f18\u4e8eGRPO\u7b49\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2602.03955", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.03955", "abs": "https://arxiv.org/abs/2602.03955", "authors": ["Yinyi Luo", "Yiqiao Jin", "Weichen Yu", "Mengqi Zhang", "Srijan Kumar", "Xiaoxiao Li", "Weijie Xu", "Xin Chen", "Jindong Wang"], "title": "AgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent", "comment": null, "summary": "While large language model (LLM) multi-agent systems achieve superior reasoning performance through iterative debate, practical deployment is limited by their high computational cost and error propagation. This paper proposes AgentArk, a novel framework to distill multi-agent dynamics into the weights of a single model, effectively transforming explicit test-time interactions into implicit model capabilities. This equips a single agent with the intelligence of multi-agent systems while remaining computationally efficient. Specifically, we investigate three hierarchical distillation strategies across various models, tasks, scaling, and scenarios: reasoning-enhanced fine-tuning; trajectory-based augmentation; and process-aware distillation. By shifting the burden of computation from inference to training, the distilled models preserve the efficiency of one agent while exhibiting strong reasoning and self-correction performance of multiple agents. They further demonstrate enhanced robustness and generalization across diverse reasoning tasks. We hope this work can shed light on future research on efficient and robust multi-agent development. Our code is at https://github.com/AIFrontierLab/AgentArk.", "AI": {"tldr": "AgentArk\u6846\u67b6\u901a\u8fc7\u4e09\u79cd\u5206\u5c42\u84b8\u998f\u7b56\u7565\u5c06\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u52a8\u6001\u538b\u7f29\u5230\u5355\u4e2a\u6a21\u578b\u6743\u91cd\u4e2d\uff0c\u5b9e\u73b0\u5355\u667a\u80fd\u4f53\u6548\u7387\u4e0e\u591a\u667a\u80fd\u4f53\u63a8\u7406\u6027\u80fd\u7684\u7edf\u4e00\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u867d\u7136\u63a8\u7406\u6027\u80fd\u4f18\u8d8a\uff0c\u4f46\u90e8\u7f72\u53d7\u9650\u4e8e\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u9519\u8bef\u4f20\u64ad\u95ee\u9898\uff0c\u4e9f\u9700\u63d0\u5347\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51faAgentArk\u6846\u67b6\uff0c\u7814\u7a76\u4e09\u79cd\u5206\u5c42\u84b8\u998f\u7b56\u7565\uff1a\u63a8\u7406\u589e\u5f3a\u5fae\u8c03\u3001\u8f68\u8ff9\u589e\u5f3a\u548c\u8fc7\u7a0b\u611f\u77e5\u84b8\u998f\uff0c\u5c06\u663e\u5f0f\u6d4b\u8bd5\u65f6\u4ea4\u4e92\u8f6c\u5316\u4e3a\u9690\u5f0f\u6a21\u578b\u80fd\u529b\u3002", "result": "\u84b8\u998f\u540e\u7684\u6a21\u578b\u4fdd\u6301\u5355\u667a\u80fd\u4f53\u8ba1\u7b97\u6548\u7387\uff0c\u540c\u65f6\u5c55\u73b0\u51fa\u591a\u667a\u80fd\u4f53\u7684\u5f3a\u63a8\u7406\u548c\u81ea\u6211\u4fee\u6b63\u6027\u80fd\uff0c\u5e76\u5728\u591a\u6837\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5c06\u8ba1\u7b97\u8d1f\u62c5\u4ece\u63a8\u7406\u8f6c\u79fb\u5230\u8bad\u7ec3\uff0c\u4e3a\u9ad8\u6548\u9c81\u68d2\u7684\u591a\u667a\u80fd\u4f53\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.04308", "categories": ["cond-mat.stat-mech", "cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2602.04308", "abs": "https://arxiv.org/abs/2602.04308", "authors": ["Riccardo Piombo", "Dario Mazzilli", "Aurelio Patelli"], "title": "Statistical Mechanics of the Sub-Optimal Transport", "comment": "28 pages, 4 figures", "summary": "Statistical mechanics is a powerful framework for analyzing optimization yielding analytical results for matching, optimal transport, and other combinatorial problems. However, these methods typically target the zero-temperature limit, where systems collapse onto optimal configurations, a.k.a. the ground states. Real-world systems often occupy intermediate regimes where entropy and cost minimization genuinely compete, producing configurations that are structured yet sub-optimal. The Sub-Optimal Transport (SOT) model captures this competition through an ensemble of weighted bipartite graphs: a coupling parameter interpolates between entropy-dominated dense configurations and cost-dominated sparse structures. This crossover has been observed numerically but lacked analytical understanding. Here we develop a mean-field theory that characterizes this transition. We show that local fluctuations in Lagrange multipliers become sub-extensive in the thermodynamic limit, reducing the full model with strength constraints to an effective single-constraint problem admitting an exact solution in some intermediate regime. The resulting free energy is analytic in the coupling parameter, confirming a smooth crossover rather than a phase transition. We derive closed-form expressions for thermodynamic observables and weight distributions, validated against numerical simulations. These results establish the first analytical description of the SOT model, extending statistical mechanics methods beyond the zero-temperature regime.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u7edf\u8ba1\u529b\u5b66\u65b9\u6cd5\u5728\u4f18\u5316\u95ee\u9898\u4e2d\u901a\u5e38\u4ec5\u9002\u7528\u4e8e\u96f6\u6e29\u6781\u9650\uff08\u57fa\u6001\uff09\u7684\u5c40\u9650\uff0c\u63d0\u51fa\u4e86\u4e9a\u4f18\u4f20\u8f93\uff08SOT\uff09\u6a21\u578b\u6765\u63cf\u8ff0\u5b9e\u9645\u7cfb\u7edf\u4e2d\u71b5\u4e0e\u6210\u672c\u7ade\u4e89\u7684\u4e2d\u6e29\u533a\u57df\u3002\u901a\u8fc7\u5e73\u5747\u573a\u7406\u8bba\uff0c\u4f5c\u8005\u53d1\u73b0\u62c9\u683c\u6717\u65e5\u4e58\u5b50\u7684\u5c40\u90e8\u6da8\u843d\u5728\u70ed\u529b\u5b66\u6781\u9650\u4e0b\u662f\u6b21\u5e7f\u5ef6\u7684\uff0c\u4ece\u800c\u5c06\u591a\u7ea6\u675f\u95ee\u9898\u7b80\u5316\u4e3a\u5355\u7ea6\u675f\u95ee\u9898\u5e76\u83b7\u5f97\u89e3\u6790\u89e3\u3002\u7814\u7a76\u8868\u660e\u8be5\u6a21\u578b\u5728\u8026\u5408\u53c2\u6570\u53d8\u5316\u65f6\u5448\u73b0\u5e73\u6ed1\u8fc7\u6e21\u800c\u975e\u76f8\u53d8\uff0c\u4e3aSOT\u6a21\u578b\u63d0\u4f9b\u4e86\u9996\u4e2a\u89e3\u6790\u63cf\u8ff0\u3002", "motivation": "\u4f20\u7edf\u7edf\u8ba1\u529b\u5b66\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u96f6\u6e29\u6781\u9650\u4e0b\u7684\u6700\u4f18\u914d\u7f6e\uff08\u57fa\u6001\uff09\uff0c\u4f46\u771f\u5b9e\u4e16\u754c\u7cfb\u7edf\u5f80\u5f80\u5904\u4e8e\u71b5\u4e0e\u6210\u672c\u76f8\u4e92\u7ade\u4e89\u7684\u4e2d\u6e29\u533a\u57df\uff0c\u4ea7\u751f\u7ed3\u6784\u5316\u4f46\u975e\u6700\u4f18\u7684\u914d\u7f6e\u3002SOT\u6a21\u578b\u867d\u80fd\u6570\u503c\u4e0a\u6355\u6349\u8fd9\u4e00\u7ade\u4e89\uff0c\u4f46\u7f3a\u4e4f\u7406\u8bba\u5206\u6790\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f00\u53d1\u89e3\u6790\u65b9\u6cd5\u6765\u7406\u89e3\u8be5\u6a21\u578b\u4e2d\u71b5\u4e3b\u5bfc\u7684\u7a20\u5bc6\u6784\u578b\u4e0e\u6210\u672c\u4e3b\u5bfc\u7684\u7a00\u758f\u7ed3\u6784\u4e4b\u95f4\u7684\u4ea4\u53c9\u884c\u4e3a\u3002", "method": "\u4f5c\u8005\u91c7\u7528\u5e73\u5747\u573a\u7406\u8bba\u6765\u5206\u6790SOT\u6a21\u578b\u3002\u901a\u8fc7\u7814\u7a76\u70ed\u529b\u5b66\u6781\u9650\u4e0b\u62c9\u683c\u6717\u65e5\u4e58\u5b50\u7684\u5c40\u90e8\u6da8\u843d\u7279\u6027\uff0c\u53d1\u73b0\u8fd9\u4e9b\u6da8\u843d\u662f\u6b21\u5e7f\u5ef6\u7684\uff08sub-extensive\uff09\uff0c\u4ece\u800c\u53ef\u4ee5\u5c06\u5305\u542b\u5f3a\u5ea6\u7ea6\u675f\u7684\u5b8c\u6574\u6a21\u578b\u7b80\u5316\u4e3a\u6709\u6548\u7684\u5355\u7ea6\u675f\u95ee\u9898\u3002\u8fd9\u4e00\u7b80\u5316\u4f7f\u5f97\u5728\u4e2d\u95f4\u533a\u57df\u83b7\u5f97\u7cbe\u786e\u89e3\u6790\u89e3\u6210\u4e3a\u53ef\u80fd\u3002", "result": "1) \u8bc1\u660e\u4e86\u81ea\u7531\u80fd\u5173\u4e8e\u8026\u5408\u53c2\u6570\u662f\u89e3\u6790\u7684\uff0c\u786e\u8ba4\u4e86\u5e73\u6ed1\u4ea4\u53c9\u800c\u975e\u76f8\u53d8\uff1b2) \u63a8\u5bfc\u51fa\u70ed\u529b\u5b66\u89c2\u6d4b\u503c\u548c\u6743\u91cd\u5206\u5e03\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff1b3) \u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u7684\u6b63\u786e\u6027\uff1b4) \u4e3aSOT\u6a21\u578b\u63d0\u4f9b\u4e86\u9996\u4e2a\u5b8c\u6574\u7684\u89e3\u6790\u63cf\u8ff0\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u6210\u529f\u5730\u5c06\u7edf\u8ba1\u529b\u5b66\u65b9\u6cd5\u4ece\u4f20\u7edf\u7684\u96f6\u6e29\u6781\u9650\u6269\u5c55\u5230\u4e2d\u6e29\u533a\u57df\uff0c\u4e3a\u7406\u89e3\u5b9e\u9645\u4f18\u5316\u95ee\u9898\u4e2d\u71b5\u4e0e\u6210\u672c\u7684\u7ade\u4e89\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u3002\u5e73\u5747\u573a\u65b9\u6cd5\u63ed\u793a\u7684\u6b21\u5e7f\u5ef6\u6da8\u843d\u7279\u6027\u662f\u591a\u7ea6\u675f\u7cfb\u7edf\u53ef\u7b80\u5316\u7684\u5173\u952e\uff0c\u8fd9\u4e00\u6d1e\u89c1\u53ef\u80fd\u9002\u7528\u4e8e\u5176\u4ed6\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u3002\u7814\u7a76\u7ed3\u679c\u4e0d\u4ec5\u5b8c\u5584\u4e86SOT\u6a21\u578b\u7684\u7406\u8bba\u57fa\u7840\uff0c\u4e5f\u4e3a\u5206\u6790\u7c7b\u4f3c\u7cfb\u7edf\u7684\u6709\u9650\u6e29\u5ea6\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.04774", "categories": ["cond-mat.dis-nn", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.04774", "abs": "https://arxiv.org/abs/2602.04774", "authors": ["Blake Bordelon", "Francesco Mori"], "title": "Theory of Optimal Learning Rate Schedules and Scaling Laws for a Random Feature Model", "comment": null, "summary": "Setting the learning rate for a deep learning model is a critical part of successful training, yet choosing this hyperparameter is often done empirically with trial and error. In this work, we explore a solvable model of optimal learning rate schedules for a powerlaw random feature model trained with stochastic gradient descent (SGD). We consider the optimal schedule $\u03b7_T^\\star(t)$ where $t$ is the current iterate and $T$ is the total training horizon. This schedule is computed both numerically and analytically (when possible) using optimal control methods. Our analysis reveals two regimes which we term the easy phase and hard phase. In the easy phase the optimal schedule is a polynomial decay $\u03b7_T^\\star(t) \\simeq T^{-\u03be} (1-t/T)^\u03b4$ where $\u03be$ and $\u03b4$ depend on the properties of the features and task. In the hard phase, the optimal schedule resembles warmup-stable-decay with constant (in $T$) initial learning rate and annealing performed over a vanishing (in $T$) fraction of training steps. We investigate joint optimization of learning rate and batch size, identifying a degenerate optimality condition. Our model also predicts the compute-optimal scaling laws (where model size and training steps are chosen optimally) in both easy and hard regimes. Going beyond SGD, we consider optimal schedules for the momentum $\u03b2(t)$, where speedups in the hard phase are possible. We compare our optimal schedule to various benchmarks in our task including (1) optimal constant learning rates $\u03b7_T(t) \\sim T^{-\u03be}$ (2) optimal power laws $\u03b7_T(t) \\sim T^{-\u03be} t^{-\u03c7}$, finding that our schedule achieves better rates than either of these. Our theory suggests that learning rate transfer across training horizon depends on the structure of the model and task. We explore these ideas in simple experimental pretraining setups.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6700\u4f18\u63a7\u5236\u7406\u8bba\u6c42\u89e3\u5e42\u5f8b\u968f\u673a\u7279\u5f81\u6a21\u578bSGD\u8bad\u7ec3\u7684\u6700\u4f18\u5b66\u4e60\u7387\u8c03\u5ea6\uff0c\u63ed\u793a\"\u7b80\u5355\u76f8\"\u4e0e\"\u56f0\u96be\u76f8\"\u4e24\u79cd\u673a\u5236\uff0c\u63d0\u51fa\u8d85\u8d8a\u7ecf\u9a8c\u8bd5\u9519\u7684\u7406\u8bba\u9a71\u52a8\u65b9\u6848\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u4e2d\u5b66\u4e60\u7387\u662f\u5173\u952e\u4f46\u5e38\u88ab\u7ecf\u9a8c\u6027\u8bbe\u7f6e\u7684\u8d85\u53c2\u6570\uff0c\u4e9f\u9700\u7406\u8bba\u6307\u5bfc\u3002", "method": "\u6784\u5efa\u53ef\u89e3\u7684\u5e42\u5f8b\u968f\u673a\u7279\u5f81\u6a21\u578b\uff0c\u8fd0\u7528\u6700\u4f18\u63a7\u5236\u65b9\u6cd5\u6570\u503c\u548c\u89e3\u6790\u8ba1\u7b97\u6700\u4f18\u8c03\u5ea6\u03b7_T*(t)\u3002", "result": "\u53d1\u73b0\u7b80\u5355\u76f8\u4e3a\u591a\u9879\u5f0f\u8870\u51cf\u03b7_T*(t)\u2248T^(-\u03be)(1-t/T)^\u03b4\uff0c\u56f0\u96be\u76f8\u4e3a\u9884\u70ed-\u7a33\u5b9a-\u8870\u51cf\u6a21\u5f0f\uff1b\u8bc6\u522b\u5b66\u4e60\u7387\u4e0e\u6279\u5927\u5c0f\u7684\u8054\u5408\u4f18\u5316\u6761\u4ef6\uff1b\u9884\u6d4b\u8ba1\u7b97\u6700\u4f18\u7f29\u653e\u5f8b\uff1b\u52a8\u91cf\u8c03\u5ea6\u5728\u56f0\u96be\u76f8\u53ef\u52a0\u901f\uff1b\u6240\u63d0\u65b9\u6cd5\u4f18\u4e8e\u5e38\u91cf\u548c\u5e42\u5f8b\u57fa\u51c6\u3002", "conclusion": "\u5b66\u4e60\u7387\u8de8\u5468\u671f\u8fc1\u79fb\u6027\u53d6\u51b3\u4e8e\u6a21\u578b\u4e0e\u4efb\u52a1\u7ed3\u6784\uff0c\u7406\u8bba\u6700\u4f18\u8c03\u5ea6\u663e\u8457\u8d85\u8d8a\u7ecf\u9a8c\u65b9\u6cd5\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u63d0\u4f9b\u65b0\u6846\u67b6\u3002"}}
{"id": "2602.04072", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04072", "abs": "https://arxiv.org/abs/2602.04072", "authors": ["Junhao Song", "Ziqian Bi", "Xinliang Chia", "William Knottenbelt", "Yudong Cao"], "title": "Data Verification is the Future of Quantum Computing Copilots", "comment": "13 Pages, 20 Figures. Accepted to AAAI 2026 Workshop on AI4Research. Comments welcome", "summary": "Quantum program generation demands a level of precision that may not be compatible with the statistical reasoning carried out in the inference of large language models (LLMs). Hallucinations are mathematically inevitable and not addressable by scaling, which leads to infeasible solutions. We argue that architectures prioritizing verification are necessary for quantum copilots and AI automation in domains governed by constraints. Our position rests on three key points: verified training data enables models to internalize precise constraints as learned structures rather than statistical approximations; verification must constrain generation rather than filter outputs, as valid designs occupy exponentially shrinking subspaces; and domains where physical laws impose correctness criteria require verification embedded as architectural primitives. Early experiments showed LLMs without data verification could only achieve a maximum accuracy of 79% in circuit optimization. Our positions are formulated as quantum computing and AI4Research community imperatives, calling for elevating verification from afterthought to architectural foundation in AI4Research.", "AI": {"tldr": "This paper argues that quantum program generation requires verification-centric AI architectures because LLM hallucinations are mathematically inevitable, proposing that verification should be an architectural foundation rather than an afterthought.", "motivation": "Quantum program generation demands extreme precision that conflicts with the statistical nature of LLMs, where hallucinations are mathematically inevitable and cannot be solved by scaling. There's a critical need for AI systems that produce correct-by-construction outputs in constraint-governed domains like quantum computing.", "method": "The paper presents a position-based argument structured around three key claims: (1) verified training data enables models to internalize precise constraints as learned structures, (2) verification must constrain generation rather than filter outputs due to exponentially shrinking valid design spaces, and (3) physical laws require verification as architectural primitives. Early experimental evidence with LLMs on circuit optimization is referenced.", "result": "Early experiments demonstrated that LLMs without data verification achieved only a maximum accuracy of 79% in circuit optimization tasks, revealing fundamental limitations of pure statistical approaches for constraint-dominated domains.", "conclusion": "The authors conclude that quantum computing and AI4Research communities must elevate verification from an afterthought to an architectural foundation, making it a core design principle for developing reliable AI systems in domains governed by physical and mathematical constraints."}}
{"id": "2602.03974", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03974", "abs": "https://arxiv.org/abs/2602.03974", "authors": ["Shuhui Qu"], "title": "Active Epistemic Control for Query-Efficient Verified Planning", "comment": null, "summary": "Planning in interactive environments is challenging under partial observability: task-critical preconditions (e.g., object locations or container states) may be unknown at decision time, yet grounding them through interaction is costly. Learned world models can cheaply predict missing facts, but prediction errors can silently induce infeasible commitments. We present \\textbf{Active Epistemic Control (AEC)}, an epistemic-categorical planning layer that integrates model-based belief management with categorical feasibility checks. AEC maintains a strict separation between a \\emph{grounded fact store} used for commitment and a \\emph{belief store} used only for pruning candidate plans. At each step, it either queries the environment to ground an unresolved predicate when uncertainty is high or predictions are ambiguous, or simulates the predicate to filter hypotheses when confidence is sufficient. Final commitment is gated by grounded precondition coverage and an SQ-BCP pullback-style compatibility check, so simulated beliefs affect efficiency but cannot directly certify feasibility. Experiments on ALFWorld and ScienceWorld show that AEC achieves competitive success with fewer replanning rounds than strong LLM-agent baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e3b\u52a8\u8ba4\u77e5\u63a7\u5236(AEC)\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\"\u5df2\u9a8c\u8bc1\u4e8b\u5b9e\u5e93\"\u548c\"\u4fe1\u5ff5\u5e93\"\uff0c\u5728\u4fdd\u8bc1\u5b89\u5168\u6027\u7684\u524d\u63d0\u4e0b\u5229\u7528\u5b66\u4e60\u4e16\u754c\u6a21\u578b\u63d0\u5347\u4ea4\u4e92\u5f0f\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u7684\u89c4\u5212\u6548\u7387\u3002", "motivation": "\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u7684\u4ea4\u4e92\u5f0f\u73af\u5883\u4e2d\uff0c\u4efb\u52a1\u5173\u952e\u524d\u63d0\u672a\u77e5\u4f46\u76f4\u63a5\u9a8c\u8bc1\u6210\u672c\u9ad8\u6602\uff1b\u5b66\u4e60\u4e16\u754c\u6a21\u578b\u867d\u80fd\u5ec9\u4ef7\u9884\u6d4b\u7f3a\u5931\u4fe1\u606f\uff0c\u4f46\u9884\u6d4b\u8bef\u5dee\u4f1a\u5bfc\u81f4\u4e0d\u53ef\u884c\u627f\u8bfa\uff0c\u4e9f\u9700\u5728\u6548\u7387\u4e0e\u5b89\u5168\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "method": "\u8bbe\u8ba1\u8ba4\u77e5-\u8303\u7574\u89c4\u5212\u5c42AEC\uff1a\u7ef4\u62a4\u72ec\u7acb\u7684\"\u5df2\u9a8c\u8bc1\u4e8b\u5b9e\u5e93\"\uff08\u7528\u4e8e\u627f\u8bfa\uff09\u548c\"\u4fe1\u5ff5\u5e93\"\uff08\u4ec5\u7528\u4e8e\u526a\u679d\uff09\u3002\u6bcf\u6b65\u6839\u636e\u4e0d\u786e\u5b9a\u6027\u9ad8\u4f4e\u9009\u62e9\u76f4\u63a5\u67e5\u8be2\u73af\u5883\u6216\u4f7f\u7528\u6a21\u578b\u6a21\u62df\uff1b\u6700\u7ec8\u627f\u8bfa\u9700\u901a\u8fc7 grounded \u524d\u63d0\u8986\u76d6\u5ea6\u548cSQ-BCP\u62c9\u56de\u5f0f\u517c\u5bb9\u6027\u68c0\u67e5\uff0c\u786e\u4fdd\u6a21\u62df\u4fe1\u5ff5\u53ea\u5f71\u54cd\u6548\u7387\u800c\u4e0d\u80fd\u76f4\u63a5\u8ba4\u8bc1\u53ef\u884c\u6027\u3002", "result": "\u5728ALFWorld\u548cScienceWorld\u5b9e\u9a8c\u8868\u660e\uff0cAEC\u5728\u6210\u529f\u7387\u4e0a\u5177\u6709\u7ade\u4e89\u529b\uff0c\u4e14\u6bd4\u5f3aLLM\u667a\u80fd\u4f53\u57fa\u7ebf\u663e\u8457\u51cf\u5c11\u91cd\u89c4\u5212\u8f6e\u6b21\u3002", "conclusion": "AEC\u901a\u8fc7\u4e25\u683c\u7684\u4fe1\u5ff5\u7ba1\u7406\u673a\u5236\uff0c\u6709\u6548\u5229\u7528\u4e16\u754c\u6a21\u578b\u63d0\u5347\u89c4\u5212\u6548\u7387\u7684\u540c\u65f6\uff0c\u9632\u6b62\u9884\u6d4b\u8bef\u5dee\u5f15\u53d1\u4e0d\u53ef\u884c\u627f\u8bfa\uff0c\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u7684\u5b89\u5168\u89c4\u5212\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2602.04313", "categories": ["cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.04313", "abs": "https://arxiv.org/abs/2602.04313", "authors": ["Georgii Kalagov", "Nikita Lebedev"], "title": "Critical behavior of isotropic systems with strong dipole-dipole interaction from the functional renormalization group", "comment": null, "summary": "We compute the critical exponents of three-dimensional magnets with strong dipole-dipole interactions using the functional renormalization group (FRG) within the local potential approximation including the wave function renormalization (LPA$^\\prime$). The system is governed by the Aharony fixed point, which is scale-invariant but lacks conformal invariance. Our nonperturbative FRG analysis identifies this fixed point and determines its scaling behavior. The resulting critical exponents are found to be close to those of the Heisenberg $O(3)$ universality class, as computed within the same FRG/LPA$^\\prime$ framework. This proximity confirms the distinct yet numerically similar nature of the two universality classes.", "AI": {"tldr": "\u4f7f\u7528\u529f\u80fd\u91cd\u6574\u5316\u7fa4\uff08FRG\uff09\u7ed3\u5408\u5c40\u90e8\u52bf\u8fd1\u4f3c\u548c\u6ce2\u51fd\u6570\u91cd\u6574\u5316\uff08LPA'\uff09\u65b9\u6cd5\uff0c\u8ba1\u7b97\u4e86\u4e09\u7ef4\u5f3a\u5076\u6781\u76f8\u4e92\u4f5c\u7528\u78c1\u4f53\u7684\u4e34\u754c\u6307\u6570\uff0c\u53d1\u73b0\u5176\u7531Aharony\u4e0d\u52a8\u70b9\u652f\u914d\uff08\u5177\u6709\u5c3a\u5ea6\u4e0d\u53d8\u6027\u4f46\u65e0\u5171\u5f62\u4e0d\u53d8\u6027\uff09\uff0c\u4e14\u4e34\u754c\u6307\u6570\u4e0e\u6d77\u68ee\u5821O(3)\u666e\u9002\u7c7b\u63a5\u8fd1\u4f46\u5c5e\u4e8e\u4e0d\u540c\u666e\u9002\u7c7b\u3002", "motivation": "\u63a2\u7a76\u5f3a\u5076\u6781\u76f8\u4e92\u4f5c\u7528\u5bf9\u4e09\u7ef4\u78c1\u4f53\u4e34\u754c\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u660e\u786e\u5176\u662f\u5426\u5f62\u6210\u72ec\u7acb\u4e8e\u4f20\u7edf\u6d77\u68ee\u5821\u6a21\u578b\u7684\u666e\u9002\u7c7b\uff0c\u5e76\u7cbe\u786e\u8ba1\u7b97\u5176\u4e34\u754c\u6307\u6570\u3002", "method": "\u91c7\u7528\u529f\u80fd\u91cd\u6574\u5316\u7fa4\uff08FRG\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u5c40\u90e8\u52bf\u8fd1\u4f3c\u5e76\u5305\u542b\u6ce2\u51fd\u6570\u91cd\u6574\u5316\uff08LPA'\uff09\u7684\u975e\u5fae\u6270\u65b9\u6cd5\u8fdb\u884c\u5206\u6790\u3002", "result": "\u8bc6\u522b\u51fa\u7531Aharony\u4e0d\u52a8\u70b9\u652f\u914d\u7684\u7cfb\u7edf\uff0c\u5176\u4e34\u754c\u6307\u6570\u4e0eFRG/LPA'\u6846\u67b6\u4e0b\u8ba1\u7b97\u7684\u6d77\u68ee\u5821O(3)\u666e\u9002\u7c7b\u6570\u503c\u63a5\u8fd1\u3002", "conclusion": "\u5f3a\u5076\u6781\u76f8\u4e92\u4f5c\u7528\u78c1\u4f53\u5f62\u6210\u72ec\u7acb\u666e\u9002\u7c7b\uff08Aharony\u4e0d\u52a8\u70b9\uff09\uff0c\u5176\u4e34\u754c\u884c\u4e3a\u4e0e\u6d77\u68ee\u5821O(3)\u6a21\u578b\u76f8\u4f3c\u4f46\u5b58\u5728\u672c\u8d28\u533a\u522b\uff0c\u8bc1\u5b9e\u4e8c\u8005\u4e3a\u6570\u503c\u76f8\u8fd1\u7684\u4e0d\u540c\u666e\u9002\u7c7b\u3002"}}
{"id": "2602.03906", "categories": ["cs.LG", "cs.AI", "cs.IT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.03906", "abs": "https://arxiv.org/abs/2602.03906", "authors": ["Weiqi Wang", "Zhiyi Tian", "Chenhan Zhang", "Shui Yu"], "title": "GeoIB: Geometry-Aware Information Bottleneck via Statistical-Manifold Compression", "comment": null, "summary": "Information Bottleneck (IB) is widely used, but in deep learning, it is usually implemented through tractable surrogates, such as variational bounds or neural mutual information (MI) estimators, rather than directly controlling the MI I(X;Z) itself. The looseness and estimator-dependent bias can make IB \"compression\" only indirectly controlled and optimization fragile.\n  We revisit the IB problem through the lens of information geometry and propose a \\textbf{Geo}metric \\textbf{I}nformation \\textbf{B}ottleneck (\\textbf{GeoIB}) that dispenses with mutual information (MI) estimation. We show that I(X;Z) and I(Z;Y) admit exact projection forms as minimal Kullback-Leibler (KL) distances from the joint distributions to their respective independence manifolds. Guided by this view, GeoIB controls information compression with two complementary terms: (i) a distribution-level Fisher-Rao (FR) discrepancy, which matches KL to second order and is reparameterization-invariant; and (ii) a geometry-level Jacobian-Frobenius (JF) term that provides a local capacity-type upper bound on I(Z;X) by penalizing pullback volume expansion of the encoder. We further derive a natural-gradient optimizer consistent with the FR metric and prove that the standard additive natural-gradient step is first-order equivalent to the geodesic update. We conducted extensive experiments and observed that the GeoIB achieves a better trade-off between prediction accuracy and compression ratio in the information plane than the mainstream IB baselines on popular datasets. GeoIB improves invariance and optimization stability by unifying distributional and geometric regularization under a single bottleneck multiplier. The source code of GeoIB is released at \"https://anonymous.4open.science/r/G-IB-0569\".", "AI": {"tldr": "\u63d0\u51fa\u51e0\u4f55\u4fe1\u606f\u74f6\u9888GeoIB\uff0c\u901a\u8fc7Fisher-Rao\u6563\u5ea6\u548cJacobian-Frobenius\u6b63\u5219\u9879\u76f4\u63a5\u63a7\u5236\u4fe1\u606f\u538b\u7f29\uff0c\u907f\u514d\u4e92\u4fe1\u606f\u4f30\u8ba1\uff0c\u5b9e\u73b0\u66f4\u4f18\u7684\u7cbe\u5ea6-\u538b\u7f29\u6743\u8861\u548c\u4f18\u5316\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u4fe1\u606f\u74f6\u9888\u65b9\u6cd5\u4f9d\u8d56\u4e92\u4fe1\u606f\u4f30\u8ba1\u7684\u66ff\u4ee3\u5f62\u5f0f\uff0c\u5b58\u5728\u8fb9\u754c\u677e\u5f1b\u548c\u4f30\u8ba1\u504f\u5dee\u95ee\u9898\uff0c\u5bfc\u81f4\u538b\u7f29\u63a7\u5236\u95f4\u63a5\u4e14\u4f18\u5316\u8106\u5f31\u3002", "method": "\u4ece\u4fe1\u606f\u51e0\u4f55\u89d2\u5ea6\u91cd\u65b0\u5ba1\u89c6IB\uff0c\u63d0\u51faGeoIB\uff1a1) \u7528Fisher-Rao\u6563\u5ea6\u4f5c\u4e3a\u5206\u5e03\u7ea7\u6b63\u5219\u9879\uff1b2) \u7528Jacobian-Frobenius\u9879\u7ea6\u675f\u7f16\u7801\u5668\u4f53\u79ef\u81a8\u80c0\uff1b3) \u8bbe\u8ba1\u7b26\u5408FR\u5ea6\u91cf\u7684\u81ea\u7136\u68af\u5ea6\u4f18\u5316\u5668\u3002", "result": "\u5728\u6d41\u884c\u6570\u636e\u96c6\u4e0a\uff0cGeoIB\u5728\u4fe1\u606f\u5e73\u9762\u4e2d\u5b9e\u73b0\u4e86\u9884\u6d4b\u7cbe\u5ea6\u4e0e\u538b\u7f29\u6bd4\u7684\u6700\u4f73\u6743\u8861\uff0c\u4f18\u4e8e\u4e3b\u6d41IB\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u4e0d\u53d8\u6027\u548c\u4f18\u5316\u7a33\u5b9a\u6027\u3002", "conclusion": "GeoIB\u901a\u8fc7\u7edf\u4e00\u5206\u5e03\u548c\u51e0\u4f55\u6b63\u5219\u5316\uff0c\u4e3a\u4fe1\u606f\u74f6\u9888\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u3001\u66f4\u6709\u6548\u7684\u5b9e\u73b0\u6846\u67b6\u3002"}}
{"id": "2602.03975", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03975", "abs": "https://arxiv.org/abs/2602.03975", "authors": ["Shuhui Qu"], "title": "Adaptive Test-Time Compute Allocation via Learned Heuristics over Categorical Structure", "comment": null, "summary": "Test-time computation has become a primary driver of progress in large language model (LLM) reasoning, but it is increasingly bottlenecked by expensive verification. In many reasoning systems, a large fraction of verifier calls are spent on redundant or unpromising intermediate hypotheses. We study reasoning under a \\emph{verification-cost-limited} setting and ask how verification effort should be allocated across intermediate states. We propose a state-level selective verification framework that combines (i) deterministic feasibility gating over a structured move interface, (ii) pre-verification ranking using a hybrid of learned state-distance and residual scoring, and (iii) adaptive allocation of verifier calls based on local uncertainty. Unlike solution-level best-of-$N$ or uniform intermediate verification, our method distributes verification where it is most informative. On the \\textsc{MATH} benchmark, our approach achieves higher accuracy than best-of-$N$, majority voting, and beam search while using 44\\% fewer verifier calls.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u72b6\u6001\u7ea7\u9009\u62e9\u6027\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u667a\u80fd\u5206\u914d\u9a8c\u8bc1\u5668\u8c03\u7528\u6765\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u9a8c\u8bc1\u6210\u672c\u9ad8\u6602\u7684\u95ee\u9898\uff0c\u5728MATH\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c06\u9a8c\u8bc1\u8c03\u7528\u51cf\u5c1144%\u7684\u540c\u65f6\u63d0\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u53d7\u5230\u6602\u8d35\u9a8c\u8bc1\u7684\u74f6\u9888\u9650\u5236\uff0c\u5927\u91cf\u9a8c\u8bc1\u5668\u8c03\u7528\u6d6a\u8d39\u5728\u5197\u4f59\u6216\u65e0\u5e0c\u671b\u7684\u4e2d\u4ecb\u5047\u8bbe\u4e0a\uff0c\u4e9f\u9700\u5728\u9a8c\u8bc1\u6210\u672c\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u5408\u7406\u5206\u914d\u9a8c\u8bc1\u8d44\u6e90\u3002", "method": "\u63d0\u51fa\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\u7684\u72b6\u6001\u7ea7\u9009\u62e9\u6027\u9a8c\u8bc1\u6846\u67b6\uff1a(1) \u57fa\u4e8e\u7ed3\u6784\u5316\u79fb\u52a8\u63a5\u53e3\u7684\u786e\u5b9a\u6027\u53ef\u884c\u6027\u95e8\u63a7\uff0c(2) \u7ed3\u5408\u5b66\u4e60\u5f0f\u72b6\u6001\u8ddd\u79bb\u548c\u6b8b\u5dee\u8bc4\u5206\u7684\u9884\u9a8c\u8bc1\u6392\u5e8f\uff0c(3) \u6839\u636e\u5c40\u90e8\u4e0d\u786e\u5b9a\u6027\u81ea\u9002\u5e94\u5206\u914d\u9a8c\u8bc1\u5668\u8c03\u7528\u3002", "result": "\u5728MATH\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4best-of-N\u3001\u591a\u6570\u6295\u7968\u548c\u675f\u641c\u7d22\u83b7\u5f97\u66f4\u9ad8\u51c6\u786e\u7387\uff0c\u540c\u65f6\u51cf\u5c1144%\u7684\u9a8c\u8bc1\u5668\u8c03\u7528\u3002", "conclusion": "\u4e0e\u89e3\u51b3\u65b9\u6848\u7ea7\u6216\u5747\u5300\u9a8c\u8bc1\u65b9\u6cd5\u4e0d\u540c\uff0c\u72b6\u6001\u7ea7\u9009\u62e9\u6027\u9a8c\u8bc1\u80fd\u5c06\u9a8c\u8bc1\u8d44\u6e90\u5206\u914d\u5230\u6700\u5177\u4fe1\u606f\u91cf\u7684\u4f4d\u7f6e\uff0c\u5b9e\u73b0\u4ee5\u66f4\u5c11\u7684\u8ba1\u7b97\u8d44\u6e90\u83b7\u5f97\u66f4\u597d\u7684\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2602.04342", "categories": ["cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.04342", "abs": "https://arxiv.org/abs/2602.04342", "authors": ["Vicen\u00e7 M\u00e9ndez", "Rosa Flaquer-Galm\u00e9s", "Javier Crist\u00edn"], "title": "Area under subdiffusive random walks", "comment": null, "summary": "We study the statistical properties of the area and the absolute area under the trajectories of subdiffusive random walks. Using different frameworks to describe subdiffusion (as the scaled Brownian motion, fractional Brownian motion, the continuous-time random walk or the Brownian motion in heterogeneous media), we compute the first two moments, the ergodicity breaking parameter for the absolute area and infer a general scaling for the probability density functions of these functionals. We discuss the differences between the statistical properties of the area and the absolute area for the different subdiffusion models and illustrate the experimental interest of our results. Our theoretical findings are supported by Monte Carlo simulations showing an excellent agreement.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e9a\u6269\u6563\u968f\u673a\u6e38\u8d70\u8f68\u8ff9\u7684\u9762\u79ef\u4e0e\u7edd\u5bf9\u9762\u79ef\u7684\u7edf\u8ba1\u6027\u8d28\u3002", "motivation": "\u4e9a\u6269\u6563\u5728\u7269\u7406\u3001\u751f\u7269\u7cfb\u7edf\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u5176\u8f68\u8ff9\u51e0\u4f55\u7279\u6027\u7684\u7edf\u8ba1\u63cf\u8ff0\u5bf9\u7406\u89e3\u5f02\u5e38\u6269\u6563\u673a\u5236\u53ca\u5b9e\u9a8c\u6570\u636e\u5206\u6790\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u91c7\u7528\u6807\u5ea6\u5e03\u6717\u8fd0\u52a8\u3001\u5206\u6570\u5e03\u6717\u8fd0\u52a8\u3001\u8fde\u7eed\u65f6\u95f4\u968f\u673a\u6e38\u8d70\u548c\u975e\u5747\u5300\u4ecb\u8d28\u4e2d\u7684\u5e03\u6717\u8fd0\u52a8\u7b49\u591a\u79cd\u4e9a\u6269\u6563\u6a21\u578b\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6a21\u62df\u9a8c\u8bc1\u7406\u8bba\u7ed3\u679c\u3002", "result": "\u8ba1\u7b97\u4e86\u9762\u79ef\u4e0e\u7edd\u5bf9\u9762\u79ef\u7684\u524d\u4e24\u9636\u77e9\u3001\u7edd\u5bf9\u9762\u79ef\u7684\u975e\u904d\u5386\u6027\u7834\u7f3a\u53c2\u6570\uff0c\u63a8\u65ad\u5176\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u7684\u666e\u9002\u6807\u5ea6\u5173\u7cfb\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u6a21\u578b\u4e0b\u9762\u79ef\u4e0e\u7edd\u5bf9\u9762\u79ef\u7edf\u8ba1\u6027\u8d28\u7684\u5dee\u5f02\uff0c\u5e76\u9610\u660e\u4e86\u7ed3\u679c\u7684\u5b9e\u9a8c\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "\u4e0d\u540c\u4e9a\u6269\u6563\u6a21\u578b\u4e0b\u9762\u79ef\u4e0e\u7edd\u5bf9\u9762\u79ef\u7684\u7edf\u8ba1\u6027\u8d28\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u7406\u8bba\u9884\u6d4b\u4e0e\u8499\u7279\u5361\u6d1b\u6a21\u62df\u9ad8\u5ea6\u543b\u5408\uff0c\u8be5\u7814\u7a76\u5bf9\u5b9e\u9a8c\u6d4b\u91cf\u4e9a\u6269\u6563\u8fc7\u7a0b\u5177\u6709\u6307\u5bfc\u610f\u4e49\u3002"}}
{"id": "2602.04404", "categories": ["cs.LG", "cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2602.04404", "abs": "https://arxiv.org/abs/2602.04404", "authors": ["Beatrice Achilli", "Marco Benedetti", "Giulio Biroli", "Marc M\u00e9zard"], "title": "Theory of Speciation Transitions in Diffusion Models with General Class Structure", "comment": "17 pages, 6 figures", "summary": "Diffusion Models generate data by reversing a stochastic diffusion process, progressively transforming noise into structured samples drawn from a target distribution. Recent theoretical work has shown that this backward dynamics can undergo sharp qualitative transitions, known as speciation transitions, during which trajectories become dynamically committed to data classes. Existing theoretical analyses, however, are limited to settings where classes are identifiable through first moments, such as mixtures of Gaussians with well-separated means. In this work, we develop a general theory of speciation in diffusion models that applies to arbitrary target distributions admitting well-defined classes. We formalize the notion of class structure through Bayes classification and characterize speciation times in terms of free-entropy difference between classes. This criterion recovers known results in previously studied Gaussian-mixture models, while extending to situations in which classes are not distinguishable by first moments and may instead differ through higher-order or collective features. Our framework also accommodates multiple classes and predicts the existence of successive speciation times associated with increasingly fine-grained class commitment. We illustrate the theory on two analytically tractable examples: mixtures of one-dimensional Ising models at different temperatures and mixtures of zero-mean Gaussians with distinct covariance structures. In the Ising case, we obtain explicit expressions for speciation times by mapping the problem onto a random-field Ising model and solving it via the replica method. Our results provide a unified and broadly applicable description of speciation transitions in diffusion-based generative models.", "AI": {"tldr": "\u63d0\u51fa\u6269\u6563\u6a21\u578b\u4e2d\u7269\u79cd\u5f62\u6210\u8f6c\u53d8\u7684\u901a\u7528\u7406\u8bba\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u5206\u7c7b\u548c\u81ea\u7531\u71b5\u5dee\u523b\u753b\u751f\u6210\u6837\u672c\u5411\u6570\u636e\u7c7b\u522b\u52a8\u6001\u627f\u8bfa\u7684\u65f6\u523b\uff0c\u7a81\u7834\u4e86\u5148\u524d\u4ec5\u9002\u7528\u4e8e\u4e00\u9636\u77e9\u53ef\u8bc6\u522b\u7c7b\u522b\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u7269\u79cd\u5f62\u6210\u8f6c\u53d8\u7684\u7406\u8bba\u5206\u6790\u4ec5\u9650\u4e8e\u7c7b\u522b\u53ef\u901a\u8fc7\u4e00\u9636\u77e9\u8bc6\u522b\u7684\u573a\u666f\uff08\u5982\u5747\u503c\u5206\u79bb\u7684\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff09\uff0c\u7f3a\u4e4f\u9002\u7528\u4e8e\u4efb\u610f\u5177\u6709\u660e\u786e\u7c7b\u522b\u7ed3\u6784\u7684\u76ee\u6807\u5206\u5e03\u7684\u901a\u7528\u7406\u8bba\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u8d1d\u53f6\u65af\u5206\u7c7b\u5f62\u5f0f\u5316\u7c7b\u522b\u7ed3\u6784\uff0c\u7528\u7c7b\u522b\u95f4\u7684\u81ea\u7531\u71b5\u5dee\u523b\u753b\u7269\u79cd\u5f62\u6210\u65f6\u95f4\uff0c\u5e76\u5229\u7528\u590d\u672c\u65b9\u6cd5\u6c42\u89e3\u4e00\u7ef4Ising\u6a21\u578b\u6df7\u5408\u6848\u4f8b\uff0c\u5c06\u95ee\u9898\u6620\u5c04\u5230\u968f\u673a\u573aIsing\u6a21\u578b\u83b7\u5f97\u89e3\u6790\u8868\u8fbe\u5f0f\u3002", "result": "\u7406\u8bba\u6062\u590d\u4e86\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u7684\u5df2\u77e5\u7ed3\u679c\uff0c\u6269\u5c55\u5230\u4ec5\u901a\u8fc7\u9ad8\u9636\u6216\u96c6\u4f53\u7279\u5f81\u533a\u5206\u7c7b\u522b\u7684\u60c5\u5f62\uff0c\u652f\u6301\u591a\u7c7b\u522b\u5e76\u9884\u6d4b\u4e0e\u7cbe\u7ec6\u7c7b\u522b\u627f\u8bfa\u76f8\u5173\u7684\u8fde\u7eed\u7269\u79cd\u5f62\u6210\u65f6\u95f4\u3002", "conclusion": "\u4e3a\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u6a21\u578b\u4e2d\u7684\u7269\u79cd\u5f62\u6210\u8f6c\u53d8\u63d0\u4f9b\u4e86\u7edf\u4e00\u4e14\u5e7f\u6cdb\u9002\u7528\u7684\u7406\u8bba\u63cf\u8ff0\uff0c\u663e\u8457\u62d3\u5c55\u4e86\u9002\u7528\u573a\u666f\u548c\u7406\u8bba\u6df1\u5ea6\u3002"}}
{"id": "2602.04199", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04199", "abs": "https://arxiv.org/abs/2602.04199", "authors": ["Saveetha Harikrishnan", "Tim Byrnes", "Chandrashekar Radhakrishnan"], "title": "Influence of Noninertial Dynamics on Static Quantum Resource Theories", "comment": "12 pages", "summary": "The effect of noninertial dynamics on static quantum resource theories is investigated. To this end, we first show the equivalence between noninertial effects and a completely positive, trace-preserving (CPTP) map. In this formulation, the Unruh effect is equivalent to a bosonic amplifier channel. The effect of this map on a generic quantum resource is investigated by studying the role of the CPTP map on the three core ingredients of a resource theory, namely, the free states, the free operations and the resource quantifiers. We show several general statements can be made about these three components of a resource theory in the presence of noninertial motion.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u975e\u60ef\u6027\u52a8\u529b\u5b66\u5bf9\u9759\u6001\u91cf\u5b50\u8d44\u6e90\u7406\u8bba\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u5c06\u975e\u60ef\u6027\u6548\u5e94\u7b49\u4ef7\u4e8e\u5b8c\u5168\u6b63\u5b9a\u4fdd\u8ff9\uff08CPTP\uff09\u6620\u5c04\uff0c\u63ed\u793aUnruh\u6548\u5e94\u5bf9\u5e94\u4e8e\u73bb\u8272\u653e\u5927\u5668\u901a\u9053\uff0c\u5e76\u7cfb\u7edf\u5206\u6790\u8be5\u6620\u5c04\u5bf9\u8d44\u6e90\u7406\u8bba\u4e09\u5927\u6838\u5fc3\u8981\u7d20\uff08\u81ea\u7531\u6001\u3001\u81ea\u7531\u64cd\u4f5c\u548c\u8d44\u6e90\u5ea6\u91cf\uff09\u7684\u5f71\u54cd\u3002", "motivation": "\u63a2\u7d22\u76f8\u5bf9\u8bba\u6027\u975e\u60ef\u6027\u53c2\u8003\u7cfb\u5bf9\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u548c\u91cf\u5b50\u8d44\u6e90\u7684\u5f71\u54cd\uff0c\u8fd9\u5bf9\u53d1\u5c55\u76f8\u5bf9\u8bba\u91cf\u5b50\u8ba1\u7b97\u3001\u52a0\u901f\u7cfb\u91cf\u5b50\u901a\u4fe1\u4ee5\u53ca\u7406\u89e3\u5f2f\u66f2\u65f6\u7a7a\u4e2d\u7684\u91cf\u5b50\u529b\u5b66\u57fa\u7840\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u7406\u8bba\u5206\u6790\u65b9\u6cd5\uff1a\u9996\u5148\u5efa\u7acb\u975e\u60ef\u6027\u6548\u5e94\u4e0eCPTP\u6620\u5c04\u7684\u7b49\u4ef7\u5173\u7cfb\uff0c\u5c06Unruh\u6548\u5e94\u5efa\u6a21\u4e3a\u73bb\u8272\u653e\u5927\u5668\u901a\u9053\uff0c\u8fdb\u800c\u7814\u7a76\u8be5\u901a\u9053\u5bf9\u91cf\u5b50\u8d44\u6e90\u7406\u8bba\u4e09\u5927\u6838\u5fc3\u7ec4\u6210\u90e8\u5206\u7684\u7cfb\u7edf\u6027\u5f71\u54cd\u3002", "result": "\u5efa\u7acb\u4e86\u975e\u60ef\u6027\u8fd0\u52a8\u4e0b\u91cf\u5b50\u8d44\u6e90\u7406\u8bba\u7684\u5206\u6790\u6846\u67b6\uff0c\u8bc1\u660e\u53ef\u4ee5\u5bf9\u81ea\u7531\u6001\u3001\u81ea\u7531\u64cd\u4f5c\u548c\u8d44\u6e90\u5ea6\u91cf\u5728\u5b58\u5728\u975e\u60ef\u6027\u8fd0\u52a8\u65f6\u7684\u884c\u4e3a\u505a\u51fa\u82e5\u5e72\u666e\u9002\u6027\u8bba\u65ad\u3002", "conclusion": "\u975e\u60ef\u6027\u6548\u5e94\u53ef\u901a\u8fc7CPTP\u6620\u5c04\u5728\u91cf\u5b50\u8d44\u6e90\u7406\u8bba\u6846\u67b6\u4e0b\u8fdb\u884c\u7cfb\u7edf\u7814\u7a76\uff0c\u4e3a\u7406\u89e3\u76f8\u5bf9\u8bba\u6027\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u63ed\u793a\u4e86\u52a0\u901f\u8fd0\u52a8\u5bf9\u91cf\u5b50\u8d44\u6e90\u7684\u666e\u904d\u5f71\u54cd\u89c4\u5f8b\u3002"}}
{"id": "2602.03978", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03978", "abs": "https://arxiv.org/abs/2602.03978", "authors": ["Zidi Xiong", "Shan Chen", "Himabindu Lakkaraju"], "title": "Monitorability as a Free Gift: How RLVR Spontaneously Aligns Reasoning", "comment": null, "summary": "As Large Reasoning Models (LRMs) are increasingly deployed, auditing their chain-of-thought (CoT) traces for safety becomes critical. Recent work has reported that monitorability--the degree to which CoT faithfully and informatively reflects internal computation--can appear as a \"free gift\" during the early stages of Reinforcement Learning with Verifiable Rewards (RLVR). We make this observation concrete through a systematic evaluation across model families and training domains. Our results show that this effect is not universal: monitorability improvements are strongly data-dependent. In particular, we demonstrate the critical role of data diversity and instruction-following data during RLVR training. We further show that monitorability is orthogonal to capability--improvements in reasoning performance do not imply increased transparency. Through mechanistic analysis, we attribute monitorability gains primarily to response distribution sharpening (entropy reduction) and increased attention to the prompt, rather than stronger causal reliance on reasoning traces. We also reveal how monitorability dynamics vary with controlled training and evaluation difficulty. Together, these findings provide a holistic view of how monitorability emerges under RLVR, clarifying when gains are likely to occur and when they are not.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u53d1\u73b0\uff0c\u5927\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u601d\u7ef4\u94fe\u7684\u53ef\u76d1\u6d4b\u6027\u63d0\u5347\u5e76\u975eRLVR\u8bad\u7ec3\u7684\u514d\u8d39\u526f\u4ea7\u54c1\uff0c\u800c\u662f\u9ad8\u5ea6\u4f9d\u8d56\u6570\u636e\u591a\u6837\u6027\uff0c\u4e14\u4e0e\u6a21\u578b\u80fd\u529b\u6b63\u4ea4\uff0c\u4e3b\u8981\u6e90\u4e8e\u54cd\u5e94\u5206\u5e03\u9510\u5316\u548c\u5bf9\u63d0\u793a\u7684\u5173\u6ce8\u589e\u5f3a\u3002", "motivation": "\u968f\u7740\u5927\u63a8\u7406\u6a21\u578b\u90e8\u7f72\u589e\u591a\uff0c\u5ba1\u8ba1\u5176\u601d\u7ef4\u94fe\u8f68\u8ff9\u4ee5\u786e\u4fdd\u5b89\u5168\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u8fd1\u671f\u7814\u7a76\u8868\u660e\uff0c\u5728RLVR\u65e9\u671f\u9636\u6bb5\uff0c\u53ef\u76d1\u6d4b\u6027\u53ef\u80fd\u4f5c\u4e3a\"\u514d\u8d39\u793c\u7269\"\u51fa\u73b0\uff0c\u4f46\u8fd9\u4e00\u73b0\u8c61\u7f3a\u4e4f\u7cfb\u7edf\u6027\u9a8c\u8bc1\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u5bf9\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u548c\u8bad\u7ec3\u9886\u57df\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\uff0c\u5e76\u7ed3\u5408\u673a\u5236\u6027\u5206\u6790\uff0c\u63a2\u7a76RLVR\u8bad\u7ec3\u4e2d\u53ef\u76d1\u6d4b\u6027\u7684\u53d8\u5316\u89c4\u5f8b\u3002", "result": "1) \u53ef\u76d1\u6d4b\u6027\u63d0\u5347\u975e\u666e\u904d\u73b0\u8c61\uff0c\u5f3a\u70c8\u4f9d\u8d56\u6570\u636e\uff1b2) \u6570\u636e\u591a\u6837\u6027\u548c\u6307\u4ee4\u9075\u5faa\u6570\u636e\u8d77\u5173\u952e\u4f5c\u7528\uff1b3) \u53ef\u76d1\u6d4b\u6027\u4e0e\u80fd\u529b\u6b63\u4ea4\uff0c\u63a8\u7406\u6027\u80fd\u63d0\u5347\u4e0d\u610f\u5473\u7740\u900f\u660e\u5ea6\u589e\u52a0\uff1b4) \u673a\u5236\u5206\u6790\u8868\u660e\u589e\u76ca\u4e3b\u8981\u6765\u81ea\u54cd\u5e94\u5206\u5e03\u9510\u5316\uff08\u71b5\u51cf\uff09\u548c\u589e\u5f3a\u7684\u63d0\u793a\u5173\u6ce8\uff0c\u800c\u975e\u5bf9\u63a8\u7406\u8f68\u8ff9\u7684\u56e0\u679c\u4f9d\u8d56\u589e\u5f3a\uff1b5) \u53ef\u76d1\u6d4b\u6027\u52a8\u6001\u968f\u8bad\u7ec3\u548c\u8bc4\u4f30\u96be\u5ea6\u53d8\u5316\u800c\u53d8\u5316\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86RLVR\u4e0b\u53ef\u76d1\u6d4b\u6027\u4ea7\u751f\u673a\u5236\u7684\u6574\u4f53\u89c6\u56fe\uff0c\u660e\u786e\u4e86\u5728\u4f55\u79cd\u6761\u4ef6\u4e0b\u53ef\u76d1\u6d4b\u6027\u589e\u76ca\u53ef\u80fd\u51fa\u73b0\u6216\u4e0d\u4f1a\u51fa\u73b0\uff0c\u4e3a\u6a21\u578b\u5b89\u5168\u5ba1\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u6d1e\u5bdf\u3002"}}
{"id": "2602.04357", "categories": ["cond-mat.stat-mech", "math.PR"], "pdf": "https://arxiv.org/pdf/2602.04357", "abs": "https://arxiv.org/abs/2602.04357", "authors": ["Yuta Yanagibashi", "Kazumasa A. Takeuchi"], "title": "Population dynamics simulations of large deviations for three subclasses of the Kardar-Parisi-Zhang universality class", "comment": "9 pages, 5 figures", "summary": "Recent theoretical studies have gradually deepened our understanding of the one-dimensional (1D) Kardar-Parisi-Zhang (KPZ) universality class even in the large deviation regime, but numerical methods for studying KPZ large deviations remain limited. Here we implement a method based on the population dynamics algorithm for studying large deviations of time-integrated local currents in the totally asymmetric simple exclusion process (TASEP), which is a pragmatic model in the 1D KPZ class. Carrying out simulations for the three representative initial conditions, namely step, flat, and stationary ones, we not only confirm theoretical predictions available for the step case, but also characterize large deviations for the flat and stationary cases which have not been investigated before. We reveal in particular an unexpected robustness of the deeply negative large deviation regime with respect to different initial conditions. We attribute this robustness to the spontaneous formation of a wedge shape in interface profile. Our population dynamics approach may serve as a versatile method for studying large deviations in the KPZ class numerically and, potentially, even experimentally.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8e\u7fa4\u4f53\u52a8\u529b\u5b66\u7b97\u6cd5\u7684\u6570\u503c\u65b9\u6cd5\uff0c\u7814\u7a76KPZ\u666e\u9002\u7c7b\u4e2dTASEP\u6a21\u578b\u7684\u65f6\u95f4\u79ef\u5206\u7535\u6d41\u5927\u504f\u5dee\u95ee\u9898\uff0c\u63ed\u793a\u8d1f\u5927\u504f\u5dee\u533a\u5bf9\u4e0d\u540c\u521d\u59cb\u6761\u4ef6\u7684\u9c81\u68d2\u6027\u6e90\u4e8e\u754c\u9762\u81ea\u53d1\u5f62\u6210\u6954\u5f62\u7ed3\u6784", "motivation": "\u73b0\u6709\u7406\u8bba\u867d\u6df1\u5316\u4e86\u5bf9\u4e00\u7ef4KPZ\u666e\u9002\u7c7b\u5927\u504f\u5dee regime \u7684\u7406\u89e3\uff0c\u4f46\u6570\u503c\u7814\u7a76\u65b9\u6cd5\u4ecd\u6709\u9650\uff0c\u5c24\u5176\u7f3a\u4e4f\u9488\u5bf9\u5e73\u5766\u548c\u7a33\u6001\u521d\u59cb\u6761\u4ef6\u4e0b\u5927\u504f\u5dee\u7279\u6027\u7684\u6570\u503c\u8868\u5f81", "method": "\u91c7\u7528\u7fa4\u4f53\u52a8\u529b\u5b66\u7b97\u6cd5\u6a21\u62df\u5b8c\u5168\u4e0d\u5bf9\u79f0\u7b80\u5355\u6392\u65a5\u8fc7\u7a0b(TASEP)\uff0c\u9488\u5bf9\u9636\u8dc3/\u5e73\u5766/\u7a33\u6001\u4e09\u7c7b\u521d\u59cb\u6761\u4ef6\uff0c\u7814\u7a76\u65f6\u95f4\u79ef\u5206\u5c40\u90e8\u7535\u6d41\u7684\u5927\u504f\u5dee\u884c\u4e3a", "result": "1) \u9a8c\u8bc1\u9636\u8dc3\u521d\u59cb\u6761\u4ef6\u7684\u7406\u8bba\u9884\u6d4b\uff1b2) \u9996\u6b21\u8868\u5f81\u5e73\u5766/\u7a33\u6001\u6761\u4ef6\u4e0b\u7684\u5927\u504f\u5dee\u7279\u6027\uff1b3) \u53d1\u73b0\u8d1f\u5927\u504f\u5dee\u533a\u5b58\u5728\u610f\u5916\u9c81\u68d2\u6027\uff0c\u5f52\u56e0\u4e8e\u754c\u9762\u81ea\u53d1\u5f62\u6210\u6954\u5f62\u7ed3\u6784", "conclusion": "\u7fa4\u4f53\u52a8\u529b\u5b66\u65b9\u6cd5\u4e3aKPZ\u666e\u9002\u7c7b\u5927\u504f\u5dee\u7814\u7a76\u63d0\u4f9b\u901a\u7528\u6570\u503c\u5de5\u5177\uff0c\u8be5\u9c81\u68d2\u6027\u673a\u5236\u53ef\u80fd\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7cfb\u7edf\uff0c\u751a\u81f3\u5177\u5907\u5b9e\u9a8c\u9a8c\u8bc1\u6f5c\u529b"}}
{"id": "2602.04003", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04003", "abs": "https://arxiv.org/abs/2602.04003", "authors": ["Shutong Fan", "Lan Zhang", "Xiaoyong Yuan"], "title": "When AI Persuades: Adversarial Explanation Attacks on Human Trust in AI-Assisted Decision Making", "comment": null, "summary": "Most adversarial threats in artificial intelligence target the computational behavior of models rather than the humans who rely on them. Yet modern AI systems increasingly operate within human decision loops, where users interpret and act on model recommendations. Large Language Models generate fluent natural-language explanations that shape how users perceive and trust AI outputs, revealing a new attack surface at the cognitive layer: the communication channel between AI and its users. We introduce adversarial explanation attacks (AEAs), where an attacker manipulates the framing of LLM-generated explanations to modulate human trust in incorrect outputs. We formalize this behavioral threat through the trust miscalibration gap, a metric that captures the difference in human trust between correct and incorrect outputs under adversarial explanations. By incorporating this gap, AEAs explore the daunting threats in which persuasive explanations reinforce users' trust in incorrect predictions. To characterize this threat, we conducted a controlled experiment (n = 205), systematically varying four dimensions of explanation framing: reasoning mode, evidence type, communication style, and presentation format. Our findings show that users report nearly identical trust for adversarial and benign explanations, with adversarial explanations preserving the vast majority of benign trust despite being incorrect. The most vulnerable cases arise when AEAs closely resemble expert communication, combining authoritative evidence, neutral tone, and domain-appropriate reasoning. Vulnerability is highest on hard tasks, in fact-driven domains, and among participants who are less formally educated, younger, or highly trusting of AI. This is the first systematic security study that treats explanations as an adversarial cognitive channel and quantifies their impact on human trust in AI-assisted decision making.", "AI": {"tldr": "This paper introduces adversarial explanation attacks (AEAs) where attackers manipulate LLM-generated explanations to make humans trust incorrect AI outputs, revealing a new cognitive-layer security vulnerability.", "motivation": "Traditional adversarial attacks target AI models' computational behavior, but modern systems increasingly operate within human decision loops. The research identifies a novel attack surface at the cognitive layer\u2014where LLM-generated explanations shape user trust\u2014posing risks when persuasive explanations reinforce trust in incorrect predictions.", "method": "The authors formalized the threat using a \"trust miscalibration gap\" metric and conducted a controlled experiment (n=205). They systematically manipulated four explanation dimensions: reasoning mode, evidence type, communication style, and presentation format to measure their impact on human trust in incorrect AI outputs.", "result": "Users reported nearly identical trust for adversarial and benign explanations, with adversarial versions preserving most benign trust despite being incorrect. Most effective attacks mimicked expert communication (authoritative evidence, neutral tone, domain-appropriate reasoning). Vulnerability was highest on difficult tasks, fact-driven domains, and among less educated, younger, or highly AI-trusting participants.", "conclusion": "This first systematic security study treating explanations as an adversarial cognitive channel demonstrates that LLM-generated explanations can be weaponized to manipulate human trust, revealing a critical vulnerability in human-AI decision-making systems that requires urgent attention."}}
{"id": "2602.04363", "categories": ["cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.04363", "abs": "https://arxiv.org/abs/2602.04363", "authors": ["R. Goutham", "R. Rajesh", "V. Subashri", "Oleg Zaboronski"], "title": "Probabilities of rare events in product kernel aggregation: An exact formula and phase diagram", "comment": "17 pages, 12 figures", "summary": "We present an exact method for calculating the large deviation function describing rare fluctuations in the number of particles for product-kernel aggregation. Starting from the master equation, we derive an exact integral representation for the probability $P(M,N,t)$ of observing $N$ particles at time $t$ starting from $M$ monomers for any finite $M, N, t$. From this, we obtain an exact expression for the exponential moment $\\langle p^N\\rangle$ for integer $p$. Employing a replica conjecture -- numerically validated by finite-$M$ scaling -- we extend this result to real $p \\geq 0$. The convex envelope of the large deviation function, obtained via a Legendre-Fenchel transform of the exponential moment, shows singular behavior. The singular structure allows us to construct the full phase diagram of product-kernel aggregation, which contains a tricritical point, separating continuous and discontinuous transitions. We also compute the asymptotic form of the LDF for small $N/M$.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cbe\u786e\u8ba1\u7b97\u4e58\u79ef\u6838\u805a\u96c6\u8fc7\u7a0b\u4e2d\u7c92\u5b50\u6570\u7f55\u89c1\u6da8\u843d\u7684\u5927\u504f\u5dee\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3b\u65b9\u7a0b\u63a8\u5bfc\u51fa\u6982\u7387\u7684\u79ef\u5206\u8868\u793a\uff0c\u5229\u7528\u526f\u672c\u731c\u60f3\u548c\u52d2\u8ba9\u5fb7-\u82ac\u5207\u5c14\u53d8\u6362\u6784\u5efa\u4e86\u5305\u542b\u4e09\u4e34\u754c\u70b9\u7684\u5b8c\u6574\u76f8\u56fe\u3002", "motivation": "\u4e58\u79ef\u6838\u805a\u96c6\u8fc7\u7a0b\u4e2d\u7684\u7f55\u89c1\u6da8\u843d\u73b0\u8c61\u7f3a\u4e4f\u7cbe\u786e\u7684\u7406\u8bba\u63cf\u8ff0\uff0c\u5927\u504f\u5dee\u51fd\u6570\u662f\u523b\u753b\u6b64\u7c7b\u7a00\u6709\u4e8b\u4ef6\u6982\u7387\u7684\u5173\u952e\u5de5\u5177\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u83b7\u5f97\u7cbe\u786e\u89e3\u3002", "method": "\u4ece\u4e3b\u65b9\u7a0b\u51fa\u53d1\uff0c\u63a8\u5bfc\u4efb\u610f\u6709\u9650M\u3001N\u3001t\u4e0b\u89c2\u6d4b\u5230N\u4e2a\u7c92\u5b50\u7684\u6982\u7387P(M,N,t)\u7684\u7cbe\u786e\u79ef\u5206\u8868\u793a\uff1b\u8ba1\u7b97\u6574\u6570p\u7684\u6307\u6570\u77e9<p^N>\uff1b\u901a\u8fc7\u6570\u503c\u9a8c\u8bc1\u7684\u526f\u672c\u731c\u60f3\u5c06\u5176\u63a8\u5e7f\u5230\u5b9e\u6570p\u22650\uff1b\u6700\u540e\u901a\u8fc7\u52d2\u8ba9\u5fb7-\u82ac\u5207\u5c14\u53d8\u6362\u83b7\u5f97\u5927\u504f\u5dee\u51fd\u6570\u7684\u51f8\u5305\u7edc\u3002", "result": "\u83b7\u5f97\u4e86\u5927\u504f\u5dee\u51fd\u6570\u7684\u7cbe\u786e\u8868\u8fbe\u5f0f\uff0c\u53d1\u73b0\u5176\u5947\u5f02\u7ed3\u6784\u53ef\u6784\u5efa\u5b8c\u6574\u7684\u76f8\u56fe\uff0c\u8be5\u76f8\u56fe\u5305\u542b\u4e00\u4e2a\u4e09\u4e34\u754c\u70b9\uff0c\u53ef\u533a\u5206\u8fde\u7eed\u548c\u4e0d\u8fde\u7eed\u76f8\u53d8\uff1b\u540c\u65f6\u8ba1\u7b97\u4e86\u5c0fN/M\u60c5\u51b5\u4e0b\u5927\u504f\u5dee\u51fd\u6570\u7684\u6e10\u8fd1\u5f62\u5f0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u63ed\u793a\u4e86\u4e58\u79ef\u6838\u805a\u96c6\u7cfb\u7edf\u7684\u76f8\u53d8\u7279\u6027\uff0c\u4e09\u4e34\u754c\u70b9\u7684\u5b58\u5728\u4e3a\u7406\u89e3\u805a\u96c6\u8fc7\u7a0b\u4e2d\u7684\u7a81\u53d8\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\uff0c\u7ed3\u679c\u5bf9\u975e\u5e73\u8861\u7edf\u8ba1\u7269\u7406\u5177\u6709\u91cd\u8981\u53c2\u8003\u4ef7\u503c\u3002"}}
{"id": "2602.04253", "categories": ["quant-ph", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2602.04253", "abs": "https://arxiv.org/abs/2602.04253", "authors": ["Runhong He", "Xin Hong", "Qiaozhen Chai", "Ji Guan", "Junyuan Zhou", "Arapat Ablimit", "Guolong Cui", "Shenggang Ying"], "title": "Constructing Compact ADAPT Unitary Coupled-Cluster Ansatz with Parameter-Based Criterion", "comment": "12 pages, 8 figures", "summary": "The adaptive derivative-assembled pseudo-trotter variational quantum eigensolver (ADAPT-VQE) is a promising hybrid quantum-classical algorithm for molecular ground state energy calculation, yet its practical scalability is hampered by redundant excitation operators and excessive measurement costs. To address these challenges, we propose Param-ADAPT-VQE, a novel improved algorithm that selects excitation operators based on a parameter-based criterion instead of the traditional gradient-based metric. This strategy effectively eludes redundant operators. We further develop a sub-Hamiltonian technique and integrate a hot-start VQE optimization strategy, achieving a significant reduction in measurement costs. Numerical experiments on typical molecular systems demonstrate that Param-ADAPT-VQE outperforms the original ADAPT-VQE in computational accuracy, ansatz size, and measurement costs. Furthermore, our scheme retains the fundamental framework of ADAPT-VQE and is thus fully compatible with its various modified versions, enabling further performance improvements in specific aspects. This work presents an efficient and scalable enhancement to ADAPT-VQE, mitigating the core obstacles that impede its practical implementation in the field of molecular quantum chemistry.", "AI": {"tldr": "Param-ADAPT-VQE is a novel improvement over ADAPT-VQE that uses parameter-based operator selection, sub-Hamiltonian technique, and hot-start optimization to reduce redundancy and measurement costs while improving accuracy and scalability for molecular quantum chemistry calculations.", "motivation": "The original ADAPT-VQE algorithm, while promising for molecular ground state energy calculation, suffers from redundant excitation operators and excessive measurement costs that limit its practical scalability.", "method": "The authors propose Param-ADAPT-VQE which selects excitation operators using a parameter-based criterion instead of gradient-based metrics to avoid redundancy. They also develop a sub-Hamiltonian technique and integrate a hot-start VQE optimization strategy to reduce measurement costs.", "result": "Numerical experiments demonstrate that Param-ADAPT-VQE outperforms the original algorithm in computational accuracy, ansatz size, and measurement costs. The method retains ADAPT-VQE's fundamental framework and remains compatible with its various modified versions.", "conclusion": "This work provides an efficient and scalable enhancement to ADAPT-VQE, addressing the core obstacles that prevent its practical implementation in molecular quantum chemistry."}}
{"id": "2602.04028", "categories": ["cs.AI", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.04028", "abs": "https://arxiv.org/abs/2602.04028", "authors": ["Leila Amgoud", "Martin Cooper"], "title": "Axiomatic Foundations of Counterfactual Explanations", "comment": null, "summary": "Explaining autonomous and intelligent systems is critical in order to improve trust in their decisions. Counterfactuals have emerged as one of the most compelling forms of explanation. They address ``why not'' questions by revealing how decisions could be altered. Despite the growing literature, most existing explainers focus on a single type of counterfactual and are restricted to local explanations, focusing on individual instances. There has been no systematic study of alternative counterfactual types, nor of global counterfactuals that shed light on a system's overall reasoning process.\n  This paper addresses the two gaps by introducing an axiomatic framework built on a set of desirable properties for counterfactual explainers. It proves impossibility theorems showing that no single explainer can satisfy certain axiom combinations simultaneously, and fully characterizes all compatible sets. Representation theorems then establish five one-to-one correspondences between specific subsets of axioms and the families of explainers that satisfy them. Each family gives rise to a distinct type of counterfactual explanation, uncovering five fundamentally different types of counterfactuals. Some of these correspond to local explanations, while others capture global explanations. Finally, the framework situates existing explainers within this taxonomy, formally characterizes their behavior, and analyzes the computational complexity of generating such explanations.", "AI": {"tldr": "\u672c\u6587\u5efa\u7acb\u53cd\u4e8b\u5b9e\u89e3\u91ca\u7684\u516c\u7406\u6846\u67b6\uff0c\u63ed\u793a\u4e94\u79cd\u6839\u672c\u4e0d\u540c\u7684\u89e3\u91ca\u7c7b\u578b\uff08\u542b\u5c40\u90e8\u4e0e\u5168\u5c40\uff09\uff0c\u901a\u8fc7\u4e0d\u53ef\u80fd\u6027\u5b9a\u7406\u548c\u8868\u5f81\u5b9a\u7406\u7cfb\u7edf\u5206\u7c7b\u73b0\u6709\u65b9\u6cd5\u5e76\u5206\u6790\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u81ea\u4e3b\u667a\u80fd\u7cfb\u7edf\u7684\u51b3\u7b56\u89e3\u91ca\u5bf9\u63d0\u5347\u4fe1\u4efb\u81f3\u5173\u91cd\u8981\uff0c\u53cd\u4e8b\u5b9e\u89e3\u91ca\u80fd\u56de\u7b54\"\u4e3a\u4f55\u4e0d\"\u7c7b\u95ee\u9898\u3002\u4f46\u73b0\u6709\u7814\u7a76\u591a\u805a\u7126\u5355\u4e00\u5c40\u90e8\u89e3\u91ca\u7c7b\u578b\uff0c\u7f3a\u4e4f\u5bf9\u591a\u5143\u53cd\u4e8b\u5b9e\u7c7b\u578b\u53ca\u5168\u5c40\u89e3\u91ca\u7684\u7cfb\u7edf\u6027\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7406\u60f3\u6027\u8d28\u7684\u516c\u7406\u6846\u67b6\uff0c\u8bc1\u660e\u4e0d\u53ef\u80fd\u6027\u5b9a\u7406\u9610\u660e\u516c\u7406\u95f4\u7684\u5185\u5728\u51b2\u7a81\uff0c\u5229\u7528\u8868\u5f81\u5b9a\u7406\u5efa\u7acb\u516c\u7406\u5b50\u96c6\u4e0e\u89e3\u91ca\u5668\u5bb6\u65cf\u7684\u7cbe\u786e\u5bf9\u5e94\uff0c\u4ece\u800c\u7cfb\u7edf\u5316\u5206\u7c7b\u53cd\u4e8b\u5b9e\u89e3\u91ca\u3002", "result": "\u53d1\u73b0\u4e94\u79cd\u672c\u8d28\u4e0d\u540c\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u7c7b\u578b\uff08\u90e8\u5206\u5c40\u90e8\u3001\u90e8\u5206\u5168\u5c40\uff09\uff1b\u8bc1\u660e\u7279\u5b9a\u516c\u7406\u7ec4\u5408\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\uff1b\u5efa\u7acb\u516c\u7406\u4e0e\u89e3\u91ca\u5668\u7684\u4e25\u683c\u6620\u5c04\uff1b\u5c06\u73b0\u6709\u65b9\u6cd5\u7eb3\u5165\u8be5\u5206\u7c7b\u4f53\u7cfb\uff1b\u5206\u6790\u751f\u6210\u5404\u7c7b\u89e3\u91ca\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u586b\u8865\u4e86\u53cd\u4e8b\u5b9e\u89e3\u91ca\u7cfb\u7edf\u6027\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u4e3a\u7406\u89e3\u3001\u9009\u62e9\u548c\u5f00\u53d1\u89e3\u91ca\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u660e\u786e\u4e86\u4e0d\u540c\u89e3\u91ca\u7c7b\u578b\u7684\u6027\u8d28\u8fb9\u754c\u4e0e\u8ba1\u7b97\u4ee3\u4ef7\u3002"}}
{"id": "2602.04560", "categories": ["cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.04560", "abs": "https://arxiv.org/abs/2602.04560", "authors": ["Marco Baldovin", "Alessandro Manacorda"], "title": "Control protocols for harmonically confined run-and-tumble particles", "comment": "13 pages, 9 figures", "summary": "Run-and-tumble particles constitute one of the simplest models of self-propelled active matter, and provide an ideal playground to the understanding of out-of-equilibrium systems. We consider an idealized setup where one such particle is subject to a harmonic confining potential, and an external agent can vary in time the tumbling rate and the strength of the trap. We search for time-dependent control protocols steering the system between assigned end states, in a prescribed time interval. To this aim, we propose a description of the dynamics, alternative to the usual ones, in the form of an infinite set of ordinary differential equations. Solutions based on a suitable closure of such hierarchy, which we expect to hold true in the limit of long protocol duration, are discussed and compared with numerical simulations. We also look for the protocol completing the task with the minimal work, on average: the problem can be tackled analytically, again in the regime of slow (but not quasi-static) transformations. The solution provides insightful intuition on the optimal strategies for the control of active matter systems.", "AI": {"tldr": "\u63d0\u51fa\u63cf\u8ff0\u81ea\u9a71\u52a8\u6d3b\u6027\u7269\u8d28\u7684\u65b0\u52a8\u529b\u5b66\u65b9\u6cd5\uff0c\u901a\u8fc7\u65e0\u9650\u5e38\u5fae\u5206\u65b9\u7a0b\u7ec4\u6c42\u89e3\u53d7\u63a7\u8c10\u632f\u5b50\u4e2d\u8dd1\u52a8-\u7ffb\u6eda\u7c92\u5b50\u7684\u6700\u4f18\u63a7\u5236\u534f\u8bae\uff0c\u5728\u6162\u901f\u53d8\u6362\u4e0b\u5b9e\u73b0\u6700\u5c0f\u529f\u4f20\u8f93\u5e76\u63d0\u4f9b\u8c03\u63a7\u6d3b\u6027\u7cfb\u7edf\u7684\u7269\u7406\u6d1e\u89c1\u3002", "motivation": "\u7406\u89e3\u975e\u5e73\u8861\u6001\u7cfb\u7edf\u4e2d\u6d3b\u6027\u7269\u8d28\u7684\u8c03\u63a7\u673a\u5236\uff0c\u63a2\u7d22\u5728\u6307\u5b9a\u65f6\u95f4\u5185\u5c06\u8dd1\u52a8-\u7ffb\u6eda\u7c92\u5b50\u4ece\u521d\u6001\u5f15\u5bfc\u81f3\u7ec8\u6001\u7684\u6700\u4f18\u63a7\u5236\u7b56\u7565\uff0c\u5e76\u6700\u5c0f\u5316\u5e73\u5747\u529f\u8017\u3002", "method": "\u6784\u5efa\u66ff\u4ee3\u4f20\u7edf\u63cf\u8ff0\u7684\u65e0\u9650\u9636\u5e38\u5fae\u5206\u65b9\u7a0b\u7ec4\u52a8\u529b\u5b66\u6846\u67b6\uff0c\u91c7\u7528\u9002\u7528\u4e8e\u957f\u65f6\u534f\u8bae\u7684\u95ed\u5305\u8fd1\u4f3c\u6c42\u89e3\uff0c\u7ed3\u5408\u6162\u901f\uff08\u975e\u51c6\u9759\u6001\uff09\u53d8\u6362\u4e0b\u7684\u89e3\u6790\u65b9\u6cd5\u4f18\u5316\u63a7\u5236\u534f\u8bae\u3002", "result": "\u83b7\u5f97\u5b9e\u73b0\u4efb\u52a1\u7684\u6700\u5c0f\u529f\u89e3\u6790\u89e3\uff1b\u95ed\u5305\u8fd1\u4f3c\u5728\u6162\u901f\u534f\u8bae\u4e0b\u4e0e\u6570\u503c\u6a21\u62df\u543b\u5408\uff1b\u63ed\u793a\u6d3b\u6027\u7cfb\u7edf\u6700\u4f18\u63a7\u5236\u7b56\u7565\uff08\u5982 tumbling rate \u4e0e trap strength \u7684\u534f\u540c\u8c03\u63a7\uff09\u3002", "conclusion": "\u8be5\u52a8\u529b\u5b66\u6846\u67b6\u6709\u6548\u63cf\u8ff0\u975e\u5e73\u8861\u63a7\u5236\u8fc7\u7a0b\uff0c\u6700\u4f18\u534f\u8bae\u4e3a\u6d3b\u6027\u7269\u8d28\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\uff0c\u6162\u901f\u53d8\u6362\u4e0b\u7684\u89e3\u6790\u89e3\u5177\u6709\u666e\u9002\u6307\u5bfc\u610f\u4e49\u3002"}}
{"id": "2602.04295", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04295", "abs": "https://arxiv.org/abs/2602.04295", "authors": ["Alexandre Delattre", "Eddy Collin"], "title": "Canonical Quantization of Cylindrical Waveguides: A Gauge-Based Approach", "comment": null, "summary": "We present a canonical quantization of electromagnetic modes in cylindrical waveguides, extending a gauge-based formalism previously developed for Cartesian geometries [1]. By introducing the two field quadratures $X,Y$ of TEM (transverse electric-magnetic), but also of TM (transverse magnetic) and TE (transverse electric) traveling modes, we identify for each a characteristic one-dimensional scalar field (a generalized flux $\\varphi$) governed by a Klein-Gordon type equation. The associated Hamiltonian is derived explicitly from Maxwell's equations, allowing the construction of bosonic ladder operators. The generalized flux is directly deduced from the electromagnetic potentials $A,V$ by a proper gauge choice, generalizing Devoret's approach [2]. Our analysis unifies the treatment of cylindrical and Cartesian guided modes under a consistent and generic framework, ensuring both theoretical insight and experimental relevance. We derive mode-specific capacitance and inductance from the field profiles and express voltage and current in terms of the canonical field variables. Measurable quantities are therefore properly defined from the mode quantum operators, especially for the non-trivial TM and TE ones. The formalism shall extend in future works to any other type of waveguides, especially on-chip coplanar geometries particularly relevant to quantum technologies.", "AI": {"tldr": "This paper develops a unified quantum formalism for electromagnetic modes in cylindrical waveguides by extending gauge-based quantization from Cartesian geometries, introducing field quadratures for TEM/TM/TE modes, deriving a Klein-Gordon type equation for generalized flux, and constructing bosonic operators to define measurable quantities for quantum technology applications.", "motivation": "To extend a previously developed gauge-based quantization formalism from Cartesian to cylindrical waveguide geometries, unifying the treatment of TEM, TM, and TE modes under a consistent quantum framework for enhanced theoretical insight and experimental relevance in quantum technologies.", "method": "Canonical quantization of electromagnetic modes using field quadratures (X, Y) for TEM/TM/TE traveling modes; derivation of a generalized flux (\u03c6) governed by a Klein-Gordon equation from Maxwell's equations via proper gauge choice; explicit construction of Hamiltonian and bosonic ladder operators; calculation of mode-specific capacitance/inductance from field profiles.", "result": "A unified quantum framework for cylindrical waveguides that: (1) identifies a characteristic scalar field (generalized flux \u03c6) for each mode type, (2) derives the Hamiltonian and ladder operators, (3) expresses voltage/current in canonical variables, and (4) properly defines measurable quantities\u2014especially for non-trivial TM/TE modes.", "conclusion": "The formalism successfully unifies cylindrical and Cartesian guided mode quantization, providing a consistent theoretical foundation for quantum electromagnetic analysis in waveguides, with direct applicability to quantum technologies and planned extension to on-chip coplanar geometries."}}
{"id": "2602.03921", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03921", "abs": "https://arxiv.org/abs/2602.03921", "authors": ["Duc Hoang", "Ajay Jaiswal", "Mohammad Samragh", "Minsik Cho"], "title": "SpecMD: A Comprehensive Study On Speculative Expert Prefetching", "comment": null, "summary": "Mixture-of-Experts (MoE) models enable sparse expert activation, meaning that only a subset of the model's parameters is used during each inference. However, to translate this sparsity into practical performance, an expert caching mechanism is required. Previous works have proposed hardware-centric caching policies, but how these various caching policies interact with each other and different hardware specification remains poorly understood. To address this gap, we develop \\textbf{SpecMD}, a standardized framework for benchmarking ad-hoc cache policies on various hardware configurations. Using SpecMD, we perform an exhaustive benchmarking of several MoE caching strategies, reproducing and extending prior approaches in controlled settings with realistic constraints. Our experiments reveal that MoE expert access is not consistent with temporal locality assumptions (e.g LRU, LFU). Motivated by this observation, we propose \\textbf{Least-Stale}, a novel eviction policy that exploits MoE's predictable expert access patterns to reduce collision misses by up to $85\\times$ over LRU. With such gains, we achieve over $88\\%$ hit rates with up to $34.7\\%$ Time-to-first-token (TTFT) reduction on OLMoE at only $5\\%$ or $0.6GB$ of VRAM cache capacity.", "AI": {"tldr": "\u5f00\u53d1SpecMD\u6846\u67b6\u57fa\u51c6\u6d4b\u8bd5MoE\u7f13\u5b58\u7b56\u7565\uff0c\u63d0\u51faLeast-Stale\u9a71\u9010\u7b97\u6cd5\uff0c\u51cf\u5c1185\u500d\u51b2\u7a81\u672a\u547d\u4e2d\uff0c0.6GB\u7f13\u5b58\u5b9e\u73b088%\u547d\u4e2d\u7387\u4e0e34.7% TTFT\u964d\u4f4e\u3002", "motivation": "MoE\u6a21\u578b\u7a00\u758f\u6fc0\u6d3b\u9700\u7f13\u5b58\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u73b0\u6709\u786c\u4ef6\u4e2d\u5fc3\u5316\u7b56\u7565\u4e0e\u786c\u4ef6\u89c4\u683c\u4ea4\u4e92\u5173\u7cfb\u4e0d\u660e\u3002", "method": "\u6784\u5efaSpecMD\u6807\u51c6\u5316\u6846\u67b6\uff1bexhaustive\u57fa\u51c6\u6d4b\u8bd5\u591a\u79cdMoE\u7f13\u5b58\u7b56\u7565\uff1b\u63d0\u51faLeast-Stale\uff0c\u5229\u7528MoE\u53ef\u9884\u6d4b\u8bbf\u95ee\u6a21\u5f0f\u3002", "result": "MoE\u8bbf\u95ee\u4e0d\u7b26\u5408\u65f6\u95f4\u5c40\u90e8\u6027\uff1bLeast-Stale\u6bd4LRU\u51cf\u5c1185\u00d7\u51b2\u7a81\u672a\u547d\u4e2d\uff1b\u4ec55% VRAM\u7f13\u5b58\u5b9e\u73b088%\u547d\u4e2d\u7387\u4e0e34.7% TTFT\u964d\u4f4e\u3002", "conclusion": "MoE\u9700\u8981\u7279\u6b8a\u7f13\u5b58\u7b56\u7565\uff0cSpecMD\u4e0eLeast-Stale\u663e\u8457\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2602.04089", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04089", "abs": "https://arxiv.org/abs/2602.04089", "authors": ["Xiaofeng Lin", "Sirou Zhu", "Yilei Chen", "Mingyu Chen", "Hejian Sang", "Ioannis Paschalidis", "Zhipeng Wang", "Aldo Pacchiano", "Xuezhou Zhang"], "title": "Scaling In-Context Online Learning Capability of LLMs via Cross-Episode Meta-RL", "comment": null, "summary": "Large language models (LLMs) achieve strong performance when all task-relevant information is available upfront, as in static prediction and instruction-following problems. However, many real-world decision-making tasks are inherently online: crucial information must be acquired through interaction, feedback is delayed, and effective behavior requires balancing information collection and exploitation over time. While in-context learning enables adaptation without weight updates, existing LLMs often struggle to reliably leverage in-context interaction experience in such settings. In this work, we show that this limitation can be addressed through training. We introduce ORBIT, a multi-task, multi-episode meta-reinforcement learning framework that trains LLMs to learn from interaction in context. After meta-training, a relatively small open-source model (Qwen3-14B) demonstrates substantially improved in-context online learning on entirely unseen environments, matching the performance of GPT-5.2 and outperforming standard RL fine-tuning by a large margin. Scaling experiments further reveal consistent gains with model size, suggesting significant headroom for learn-at-inference-time decision-making agents. Code reproducing the results in the paper can be found at https://github.com/XiaofengLin7/ORBIT.", "AI": {"tldr": "The paper introduces ORBIT, a meta-reinforcement learning framework that trains LLMs to learn from online interactions in-context. A small open-source model (Qwen3-14B) trained with ORBIT matches GPT-5.2 performance on unseen environments and significantly outperforms standard RL fine-tuning, with benefits scaling with model size.", "motivation": "While LLMs excel at static tasks with all information upfront, they struggle with online decision-making where information must be acquired through interaction, feedback is delayed, and exploration-exploitation trade-offs are needed over time. Existing LLMs cannot reliably leverage in-context interaction experience.", "method": "ORBIT: a multi-task, multi-episode meta-reinforcement learning framework that trains LLMs to learn from interaction in context. The framework meta-trains a relatively small open-source model (Qwen3-14B) to enable in-context online learning.", "result": "After meta-training, the Qwen3-14B model demonstrates substantially improved in-context online learning on entirely unseen environments, matching GPT-5.2 performance and outperforming standard RL fine-tuning by a large margin. Scaling experiments show consistent gains with increasing model size.", "conclusion": "The training approach successfully addresses LLMs' limitations in online learning settings, demonstrating significant headroom for learn-at-inference-time decision-making agents and suggesting that in-context learning from interaction can be effectively scaled."}}
{"id": "2602.04593", "categories": ["cond-mat.stat-mech", "cond-mat.quant-gas", "gr-qc", "hep-th"], "pdf": "https://arxiv.org/pdf/2602.04593", "abs": "https://arxiv.org/abs/2602.04593", "authors": ["Nitesh Jaiswal", "S. Shankaranarayanan"], "title": "Emergent Hawking Radiation and Quantum Sensing in a Quenched Chiral Spin Chain", "comment": "23 pages, 7 figures. Comments are welcome", "summary": "We investigate the emergence and detection of Hawking radiation (HR) in a 1D chiral spin chain model, where the gravitational collapse is simulated by a sudden quantum quench that triggers a horizon-inducing phase transition. While our previous work Jaiswal [2025] established that this model mimics BH formation conditions even when the Hoop conjecture is seemingly violated, we here focus on the resulting stationary radiation spectrum and its detectability. By mapping the spin chain dynamics to a Dirac fermion in a curved (1 + 1)-dimensional spacetime, we analyze the radiation using two complementary approaches: field-theoretic modes and operational quantum sensors. First, using localized Gaussian wave packets to model realistic detectors, we find that the radiation spectrum exhibits deviations from the ideal Planckian form, analogous to frequency-dependent greybody factors, while retaining robust Poissonian statistics that signal the loss of formation-scale information. Second, we introduce a qubit coupled to the chain as a stationary Unruh-DeWitt detector. We demonstrate that the qubit functions as a faithful quantum sensor of the Hawking temperature only in the weak-coupling regime, where its population dynamics are governed solely by the bath spectral density. In the strong-coupling limit, the probe thermalizes with the global environment, obscuring the horizon-induced thermal signature. These results provide a clear operational protocol for distinguishing genuine analog HR from environmental noise in quantum simulation platforms.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e00\u7ef4\u624b\u6027\u81ea\u65cb\u94fe\u6a21\u578b\u4e2d\u7684\u970d\u91d1\u8f90\u5c04\u53ca\u5176\u63a2\u6d4b\u3002\u901a\u8fc7\u91cf\u5b50\u731d\u706d\u6a21\u62df\u9ed1\u6d1e\u5f62\u6210\uff0c\u5c06\u81ea\u65cb\u94fe\u6620\u5c04\u5230\u5f2f\u66f2\u65f6\u7a7a\u4e2d\u7684\u72c4\u62c9\u514b\u8d39\u7c73\u5b50\uff0c\u91c7\u7528\u573a\u8bba\u6a21\u5f0f\u548c\u91cf\u5b50\u4f20\u611f\u5668\u4e24\u79cd\u65b9\u6cd5\u5206\u6790\u8f90\u5c04\u8c31\u3002\u53d1\u73b0\u8f90\u5c04\u8c31\u504f\u79bb\u7406\u60f3\u666e\u6717\u514b\u5f62\u5f0f\u4f46\u4fdd\u6301\u6cca\u677e\u7edf\u8ba1\uff0c\u4e14\u4ec5\u5728\u5f31\u8026\u5408\u4e0b\u91cf\u5b50\u6bd4\u7279\u624d\u80fd\u51c6\u786e\u63a2\u6d4b\u970d\u91d1\u6e29\u5ea6\uff0c\u4e3a\u91cf\u5b50\u6a21\u62df\u5668\u4e2d\u533a\u5206\u771f\u5b9e\u6a21\u62df\u970d\u91d1\u8f90\u5c04\u4e0e\u73af\u5883\u566a\u58f0\u63d0\u4f9b\u4e86\u64cd\u4f5c\u534f\u8bae\u3002", "motivation": "\u7814\u7a76\u4e00\u7ef4\u624b\u6027\u81ea\u65cb\u94fe\u6a21\u578b\u4e2d\u970d\u91d1\u8f90\u5c04\u7684\u4ea7\u751f\u673a\u5236\u548c\u63a2\u6d4b\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u7a33\u6001\u8f90\u5c04\u8c31\u7684\u53ef\u63a2\u6d4b\u6027\uff0c\u5efa\u7acb\u5728\u5148\u524d\u8bc1\u5b9e\u8be5\u6a21\u578b\u53ef\u6a21\u62df\u9ed1\u6d1e\u5f62\u6210\u6761\u4ef6\u7684\u5de5\u4f5c\u57fa\u7840\u4e0a\u3002", "method": "\u5c06\u81ea\u65cb\u94fe\u52a8\u529b\u5b66\u6620\u5c04\u5230(1+1)\u7ef4\u5f2f\u66f2\u65f6\u7a7a\u4e2d\u7684\u72c4\u62c9\u514b\u8d39\u7c73\u5b50\uff0c\u91c7\u7528\u4e24\u79cd\u4e92\u8865\u65b9\u6cd5\uff1a1) \u7528\u5c40\u57df\u9ad8\u65af\u6ce2\u5305\u6a21\u62df\u5b9e\u9645\u63a2\u6d4b\u5668\u8fdb\u884c\u573a\u8bba\u6a21\u5f0f\u5206\u6790\uff1b2) \u5c06\u91cf\u5b50\u6bd4\u7279\u4f5c\u4e3a\u9759\u6001Unruh-DeWitt\u63a2\u6d4b\u5668\u8fdb\u884c\u64cd\u4f5c\u6027\u91cf\u5b50\u4f20\u611f\u3002", "result": "1) \u8f90\u5c04\u8c31\u8868\u73b0\u51fa\u7c7b\u4f3c\u7070\u4f53\u56e0\u5b50\u7684\u975e\u7406\u60f3\u666e\u6717\u514b\u5f62\u5f0f\uff0c\u4f46\u4fdd\u6301\u7a33\u5065\u7684\u6cca\u677e\u7edf\u8ba1\u7279\u5f81\uff0c\u8868\u660e\u5f62\u6210\u5c3a\u5ea6\u4fe1\u606f\u7684\u4e22\u5931\uff1b2) \u91cf\u5b50\u6bd4\u7279\u4ec5\u5728\u5f31\u8026\u5408 regime \u80fd\u5fe0\u5b9e\u63a2\u6d4b\u970d\u91d1\u6e29\u5ea6\uff0c\u5176\u52a8\u529b\u5b66\u7531\u6d74\u8c31\u5bc6\u5ea6\u51b3\u5b9a\uff1b3) \u5f3a\u8026\u5408\u4e0b\u63a2\u9488\u4e0e\u5168\u5c40\u73af\u5883\u70ed\u5316\uff0c\u63a9\u76d6\u4e86\u89c6\u754c\u8bf1\u5bfc\u7684\u70ed\u4fe1\u53f7\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5728\u91cf\u5b50\u6a21\u62df\u5e73\u53f0\u4e0a\u533a\u5206\u771f\u5b9e\u7684\u6a21\u62df\u970d\u91d1\u8f90\u5c04\u4e0e\u5916\u90e8\u73af\u5883\u566a\u58f0\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u64cd\u4f5c\u534f\u8bae\u3002"}}
{"id": "2602.04345", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04345", "abs": "https://arxiv.org/abs/2602.04345", "authors": ["Daria Gaidukevich"], "title": "Does the entropy of systems with larger internal entanglement grow stronger?", "comment": "27 pages, 8 figures", "summary": "It is known that when a system interacts with its environment, the entanglement contained in the system is redistributed since parts of the system entangle with the environment. On the other hand, the entanglement of a system with its environment is closely related to the entropy of the system. However, does this imply that the entropy of systems with larger internal entanglement will grow stronger? We study the issue using the simplest model as an example: a system of qubits interacts with the environment described by the quantum harmonic oscillator. The answer to the posed question is ambiguous. However, the study of the situation on average (using the simulation of a set of random states) reveals certain patterns and we can say that the answer is affirmative. At the same time, the choice of states satisfying certain conditions in some cases can change the dependence to the opposite. Additionally, we show that the entanglement depth also makes a small contribution to entropy growth.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u7cfb\u7edf\u5185\u90e8\u7ea0\u7f20\u5ea6\u662f\u5426\u5f71\u54cd\u5176\u4e0e\u73af\u5883\u7684\u76f8\u4e92\u4f5c\u7528\u5bfc\u81f4\u7684\u71b5\u589e\uff0c\u901a\u8fc7\u91cf\u5b50\u6bd4\u7279-\u8c10\u632f\u5b50\u6a21\u578b\u53d1\u73b0\uff1a\u5e73\u5747\u60c5\u51b5\u4e0b\u9ad8\u7ea0\u7f20\u7cfb\u7edf\u71b5\u589e\u66f4\u5f3a\uff0c\u4f46\u7279\u5b9a\u6761\u4ef6\u6001\u4e0b\u8be5\u5173\u7cfb\u4f1a\u53cd\u8f6c\uff0c\u4e14\u7ea0\u7f20\u6df1\u5ea6\u8d21\u732e\u8f83\u5c0f\u3002", "motivation": "\u63a2\u7a76\u7cfb\u7edf\u521d\u59cb\u5185\u90e8\u7ea0\u7f20\u5ea6\u4e0e\u5176\u5728\u73af\u5883\u76f8\u4e92\u4f5c\u7528\u4e2d\u71b5\u589e\u5f3a\u5ea6\u7684\u5173\u7cfb\uff0c\u8fd9\u4e00\u57fa\u7840\u95ee\u9898\u5728\u91cf\u5b50\u4fe1\u606f\u70ed\u529b\u5b66\u4e2d\u5c1a\u672a\u660e\u786e\u3002", "method": "\u91c7\u7528\u6700\u7b80\u5355\u7684\u91cf\u5b50\u6a21\u578b\uff1a\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u4e0e\u73af\u5883\uff08\u91cf\u5b50\u8c10\u632f\u5b50\uff09\u76f8\u4e92\u4f5c\u7528\uff0c\u901a\u8fc7\u6a21\u62df\u968f\u673a\u91cf\u5b50\u6001\u96c6\u5408\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\uff0c\u5e76\u8003\u5bdf\u7279\u5b9a\u6761\u4ef6\u6001\u7684\u7279\u6b8a\u884c\u4e3a\u3002", "result": "\u5e73\u5747\u60c5\u51b5\u4e0b\uff08\u968f\u673a\u6001\u96c6\u5408\uff09\uff0c\u5185\u90e8\u7ea0\u7f20\u5ea6\u8d8a\u9ad8\u7684\u7cfb\u7edf\u71b5\u589e\u8d8a\u663e\u8457\uff1b\u4f46\u7279\u5b9a\u9009\u62e9\u7684\u6761\u4ef6\u6001\u53ef\u4f7f\u8be5\u4f9d\u8d56\u5173\u7cfb\u53cd\u8f6c\uff1b\u6b64\u5916\u7ea0\u7f20\u6df1\u5ea6\u5bf9\u71b5\u589e\u6709\u5fae\u5f31\u8d21\u732e\u3002", "conclusion": "\u7cfb\u7edf\u5185\u90e8\u7ea0\u7f20\u5ea6\u4e0e\u71b5\u589e\u7684\u5173\u7cfb\u5e76\u975e\u7edd\u5bf9\u5355\u8c03\uff0c\u5e73\u5747\u8d8b\u52bf\u4e3a\u6b63\u76f8\u5173\u4f46\u5b58\u5728\u6761\u4ef6\u4f9d\u8d56\u6027\uff0c\u8fd9\u5bf9\u7406\u89e3\u91cf\u5b50\u5f00\u653e\u7cfb\u7edf\u70ed\u529b\u5b66\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2602.04101", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04101", "abs": "https://arxiv.org/abs/2602.04101", "authors": ["Harsha Vardhan Khurdula", "Vineet Agarwal", "Yoeven D Khemlani"], "title": "Interfaze: The Future of AI is built on Task-Specific Small Models", "comment": "8 pages, 1 figure", "summary": "We present Interfaze, a system that treats modern LLM applications as a problem of building and acting over context, not just picking the right monolithic model. Instead of a single transformer, we combine (i) a stack of heterogeneous DNNs paired with small language models as perception modules for OCR involving complex PDFs, charts and diagrams, and multilingual ASR with (ii) a context-construction layer that crawls, indexes, and parses external sources (web pages, code, PDFs) into compact structured state, and (iii) an action layer that can browse, retrieve, execute code in a sandbox, and drive a headless browser for dynamic web pages. A thin controller sits on top of this stack and exposes a single, OpenAI-style endpoint: it decides which small models and actions to run and always forwards the distilled context to a user-selected LLM that produces the final response.\n  On this architecture, Interfaze-Beta achieves 83.6% on MMLU-Pro, 91.4% on MMLU, 81.3% on GPQA-Diamond, 57.8% on LiveCodeBench v5, and 90.0% on AIME-2025, along with strong multimodal scores on MMMU (val) (77.3%), AI2D (91.5%), ChartQA (90.9%), and Common Voice v16 (90.8%). We show that most queries are handled primarily by the small-model and tool stack, with the large LLM operating only on distilled context, yielding competitive accuracy while shifting the bulk of computation away from the most expensive and monolithic models.", "AI": {"tldr": "Interfaze is a novel system architecture that treats LLM applications as context-building and action problems rather than monolithic model selection. It combines specialized small models for perception, a context-construction layer, and an action layer orchestrated by a thin controller that routes distilled context to a user-selected LLM, achieving strong benchmark performance while reducing computational costs.", "motivation": "Modern LLM applications are constrained by over-reliance on selecting single monolithic models, which is computationally expensive and inflexible. The authors argue for a paradigm shift toward building and acting over context through specialized, composable components instead of model-centric approaches.", "method": "Interfaze employs a three-layer architecture: (i) a perception layer with heterogeneous DNNs and small language models for complex OCR (PDFs, charts, diagrams) and multilingual ASR, (ii) a context-construction layer that crawls, indexes, and parses external sources into compact structured state, and (iii) an action layer that can browse, retrieve, execute sandboxed code, and drive headless browsers. A thin controller orchestrates these components and exposes a single OpenAI-style endpoint, deciding which tools to run while forwarding distilled context to a user-selected LLM for final responses.", "result": "Interfaze-Beta achieved 83.6% on MMLU-Pro, 91.4% on MMLU, 81.3% on GPQA-Diamond, 57.8% on LiveCodeBench v5, 90.0% on AIME-2025, 77.3% on MMMU (val), 91.5% on AI2D, 90.9% on ChartQA, and 90.8% on Common Voice v16. Critically, most queries are handled by the small-model and tool stack, with the large LLM operating only on distilled context, shifting the bulk of computation away from expensive monolithic models while maintaining competitive accuracy.", "conclusion": "Decomposing LLM applications into specialized perception, context, and action components with intelligent context distillation enables high performance while significantly reducing computational costs by minimizing reliance on expensive monolithic models, offering a more efficient and scalable architecture for complex LLM applications."}}
{"id": "2602.04626", "categories": ["cond-mat.stat-mech", "math-ph"], "pdf": "https://arxiv.org/pdf/2602.04626", "abs": "https://arxiv.org/abs/2602.04626", "authors": ["Fabio Deelan Cunden", "Noemi Cuppone", "Giovanni Gramegna", "Pierpaolo Vivo"], "title": "The Most Dispersed Subset of Random Points in $\\mathbb{R}^d$", "comment": "33 pages, 7 figures", "summary": "Consider a population of $N$ individuals, each having $d\\geq 1$ different traits, and an additive measure, called dispersion, which rewards large pairwise separations between traits. The goal is to select $M\\leq N$ individuals such that their traits are as dispersed as possible. We compute analytically the full statistics (including large deviation tails) of the maximally achievable dispersion among sub-populations of size $M$ when the traits are independent and identically distributed. Two complementary approaches are developed, one based on a mean-field theory for order statistics, and the other on the replica method from the field of disordered systems. In all dimensions $d$, and for rotationally symmetric distributions, the optimal subset for large populations consists of all points lying outside a $d$-dimensional ball whose radius is determined self-consistently. For a single trait ($d=1$), the statistics of the maximal dispersion can be tackled for finite $N,M$ as well. The formulae we obtained are corroborated by numerical simulations on small instances and by heuristic algorithms that find near-optimal solutions.", "AI": {"tldr": "This paper analytically computes the full statistics of maximal dispersion when selecting M individuals from N with d i.i.d. traits. Using mean-field theory and replica methods, it shows that for large N, the optimal subset consists of points outside a d-dimensional ball whose radius is determined self-consistently, with results validated by simulations.", "motivation": "Selecting diverse subsets is crucial in applications like portfolio optimization, biodiversity conservation, and experimental design. Understanding the fundamental limits and statistics of maximal dispersion, especially its large deviation tails, provides theoretical insights and practical benchmarks for algorithm design.", "method": "The authors develop two complementary analytical approaches: (1) a mean-field theory for order statistics, and (2) the replica method from disordered systems. These are used to compute the full statistics of maximal dispersion for i.i.d. traits. Numerical simulations and heuristic algorithms are employed to validate the theoretical predictions.", "result": "Key results include: (i) For large N, any rotationally symmetric distribution, and any d, the optimal M-subset comprises all points outside a d-dimensional ball with a self-consistently determined radius; (ii) For d=1, the full statistics can be computed exactly even for finite N,M; (iii) The derived formulae agree well with numerical simulations and heuristic solutions.", "conclusion": "The study provides a complete analytical characterization of the maximum dispersion problem, revealing a universal geometric structure (ball-exterior selection) for optimal subsets. The combination of mean-field and replica methods offers a powerful framework for analyzing this class of combinatorial optimization problems, with results that are both theoretically rigorous and practically relevant."}}
{"id": "2602.03924", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2602.03924", "abs": "https://arxiv.org/abs/2602.03924", "authors": ["Michael Aich", "Andreas F\u00fcrst", "Florian Sestak", "Carlos Ruiz-Gonzalez", "Niklas Boers", "Johannes Brandstetter"], "title": "WIND: Weather Inverse Diffusion for Zero-Shot Atmospheric Modeling", "comment": null, "summary": "Deep learning has revolutionized weather and climate modeling, yet the current landscape remains fragmented: highly specialized models are typically trained individually for distinct tasks. To unify this landscape, we introduce WIND, a single pre-trained foundation model capable of replacing specialized baselines across a vast array of tasks. Crucially, in contrast to previous atmospheric foundation models, we achieve this without any task-specific fine-tuning. To learn a robust, task-agnostic prior of the atmosphere, we pre-train WIND with a self-supervised video reconstruction objective, utilizing an unconditional video diffusion model to iteratively reconstruct atmospheric dynamics from a noisy state. At inference, we frame diverse domain-specific problems strictly as inverse problems and solve them via posterior sampling. This unified approach allows us to tackle highly relevant weather and climate problems, including probabilistic forecasting, spatial and temporal downscaling, sparse reconstruction and enforcing conservation laws purely with our pre-trained model. We further demonstrate the model's capacity to generate physically consistent counterfactual storylines of extreme weather events under global warming scenarios. By combining generative video modeling with inverse problem solving, WIND offers a computationally efficient paradigm shift in AI-based atmospheric modeling.", "AI": {"tldr": "\u63d0\u51faWIND\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u89c6\u9891\u91cd\u5efa\u548c\u65e0\u4efb\u52a1\u5fae\u8c03\uff0c\u7edf\u4e00\u5929\u6c14\u6c14\u5019\u5efa\u6a21\u7684\u591a\u6837\u5316\u4efb\u52a1", "motivation": "\u5f53\u524d\u5929\u6c14\u6c14\u5019\u5efa\u6a21\u9ad8\u5ea6\u788e\u7247\u5316\uff0c\u9700\u4e3a\u4e0d\u540c\u4efb\u52a1\u8bad\u7ec3\u4e13\u95e8\u6a21\u578b\uff0c\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6", "method": "\u91c7\u7528\u65e0\u6761\u4ef6\u89c6\u9891\u6269\u6563\u6a21\u578b\u8fdb\u884c\u81ea\u76d1\u7763\u89c6\u9891\u91cd\u5efa\u9884\u8bad\u7ec3\uff0c\u63a8\u7406\u65f6\u5c06\u95ee\u9898\u89c6\u4e3a\u9006\u95ee\u9898\u5e76\u901a\u8fc7\u540e\u9a8c\u91c7\u6837\u6c42\u89e3", "result": "\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u5373\u53ef\u5904\u7406\u6982\u7387\u9884\u62a5\u3001\u65f6\u7a7a\u964d\u5c3a\u5ea6\u3001\u7a00\u758f\u91cd\u5efa\u548c\u5b88\u6052\u5f8b\u7b49\u4efb\u52a1\uff0c\u5e76\u80fd\u751f\u6210\u5168\u7403\u53d8\u6696\u4e0b\u6781\u7aef\u5929\u6c14\u7684\u53cd\u4e8b\u5b9e\u60c5\u666f", "conclusion": "\u7ed3\u5408\u751f\u6210\u5f0f\u89c6\u9891\u5efa\u6a21\u4e0e\u9006\u95ee\u9898\u6c42\u89e3\uff0cWIND\u4e3aAI\u5927\u6c14\u5efa\u6a21\u63d0\u4f9b\u4e86\u8ba1\u7b97\u9ad8\u6548\u7684\u65b0\u8303\u5f0f"}}
{"id": "2602.04144", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04144", "abs": "https://arxiv.org/abs/2602.04144", "authors": ["Ruiting Dai", "Zheyu Wang", "Haoyu Yang", "Yihan Liu", "Chengzhi Wang", "Zekun Zhang", "Zishan Huang", "Jiaman Cen", "Lisi Mo"], "title": "OMG-Agent: Toward Robust Missing Modality Generation with Decoupled Coarse-to-Fine Agentic Workflows", "comment": null, "summary": "Data incompleteness severely impedes the reliability of multimodal systems. Existing reconstruction methods face distinct bottlenecks: conventional parametric/generative models are prone to hallucinations due to over-reliance on internal memory, while retrieval-augmented frameworks struggle with retrieval rigidity. Critically, these end-to-end architectures are fundamentally constrained by Semantic-Detail Entanglement -- a structural conflict between logical reasoning and signal synthesis that compromises fidelity. In this paper, we present \\textbf{\\underline{O}}mni-\\textbf{\\underline{M}}odality \\textbf{\\underline{G}}eneration Agent (\\textbf{OMG-Agent}), a novel framework that shifts the paradigm from static mapping to a dynamic coarse-to-fine Agentic Workflow. By mimicking a \\textit{deliberate-then-act} cognitive process, OMG-Agent explicitly decouples the task into three synergistic stages: (1) an MLLM-driven Semantic Planner that resolves input ambiguity via Progressive Contextual Reasoning, creating a deterministic structured semantic plan; (2) a non-parametric Evidence Retriever that grounds abstract semantics in external knowledge; and (3) a Retrieval-Injected Executor that utilizes retrieved evidence as flexible feature prompts to overcome rigidity and synthesize high-fidelity details. Extensive experiments on multiple benchmarks demonstrate that OMG-Agent consistently surpasses state-of-the-art methods, maintaining robustness under extreme missingness, e.g., a $2.6$-point gain on CMU-MOSI at $70$\\% missing rates.", "AI": {"tldr": "The paper proposes OMG-Agent, a novel three-stage agentic framework that decouples semantic planning, evidence retrieval, and detail synthesis to address data incompleteness in multimodal systems, overcoming limitations of existing methods like hallucinations and retrieval rigidity through a coarse-to-fine workflow.", "motivation": "Data incompleteness severely affects multimodal system reliability. Existing methods have bottlenecks: parametric/generative models cause hallucinations due to over-reliance on internal memory, retrieval-augmented frameworks suffer from rigidity, and fundamentally all end-to-end architectures are constrained by Semantic-Detail Entanglement - a structural conflict between logical reasoning and signal synthesis.", "method": "The authors propose OMG-Agent, a dynamic coarse-to-fine Agentic Workflow with three synergistic stages: (1) an MLLM-driven Semantic Planner using Progressive Contextual Reasoning to create structured semantic plans, (2) a non-parametric Evidence Retriever that grounds semantics in external knowledge, and (3) a Retrieval-Injected Executor that uses retrieved evidence as flexible prompts to synthesize high-fidelity details.", "result": "Extensive experiments show OMG-Agent consistently outperforms state-of-the-art methods and maintains robustness under extreme missingness, achieving a 2.6-point gain on CMU-MOSI at 70% missing rates.", "conclusion": "The OMG-Agent framework successfully addresses data incompleteness challenges by decoupling semantic and detail processing through an agentic workflow, demonstrating superior performance and robustness compared to existing approaches, suggesting a promising paradigm shift from static mapping to dynamic agentic processing for multimodal systems."}}
{"id": "2602.04818", "categories": ["cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.04818", "abs": "https://arxiv.org/abs/2602.04818", "authors": ["Bishnu Bhowmik", "Sayantan Mitra", "Robert M. Ziff", "Ankur Sensharma"], "title": "Site and bond percolation in linearly distorted triangular and square lattices", "comment": "13 pages, 13 figures", "summary": "We investigate site and bond percolation in triangular and square lattices subjected to linear distortion. In contrast to previously studied distortion schemes that preserve lattice geometry, linear distortion dislocates regular lattice sites along a fixed direction. Nearest-neighbors of a regular lattice need to satisfy a distance-based connection criterion to remain neighbors in the linearly distorted lattice. Using extensive Monte Carlo simulations and finite-size scaling analyses, we examine how site and bond percolation thresholds vary with the distortion parameter and the connection threshold. For triangular lattices, we observe pronounced directional dependence of both site and bond percolation thresholds, as well as of the critical connection threshold. This arises from the distortion-induced anisotropic modification of nearest-neighbor separations. In particular, bond percolation exhibits nontrivial behavior that cannot be explained solely in terms of changes in the average coordination number. In contrast, square lattices remain effectively isotropic under linear distortion, resulting in identical percolation thresholds for distortions applied along different directions. Percolation thresholds in the thermodynamic limit, evaluated for a selected set of values of distortion parameter and connection threshold, confirm that the results for large finite lattices provide reliable estimates of the infinite-system behavior.", "AI": {"tldr": "\u7814\u7a76\u7ebf\u6027\u7578\u53d8\u5bf9\u4e09\u89d2\u548c\u6b63\u65b9\u5f62\u6676\u683c\u4e2d\u4f4d\u7f6e\u4e0e\u952e\u6e17\u6d41\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4e09\u89d2\u6676\u683c\u6e17\u6d41\u9608\u503c\u5448\u73b0\u663e\u8457\u65b9\u5411\u4f9d\u8d56\u6027\uff0c\u800c\u6b63\u65b9\u5f62\u6676\u683c\u4fdd\u6301\u5404\u5411\u540c\u6027\uff0c\u63ed\u793a\u4e86\u6676\u683c\u51e0\u4f55\u7ed3\u6784\u5bf9\u7578\u53d8\u54cd\u5e94\u7684\u5173\u952e\u4f5c\u7528\u3002", "motivation": "\u63a2\u7d22\u975e\u51e0\u4f55\u4fdd\u6301\u578b\u7ebf\u6027\u7578\u53d8\uff08\u6cbf\u56fa\u5b9a\u65b9\u5411\u4f4d\u79fb\u6676\u683c\u70b9\uff09\u5bf9\u6e17\u6d41\u76f8\u53d8\u7684\u5f71\u54cd\uff0c\u7a81\u7834\u6b64\u524d\u4ec5\u7814\u7a76\u4fdd\u6301\u6676\u683c\u51e0\u4f55\u7684\u7578\u53d8\u65b9\u6848\uff0c\u7279\u522b\u5173\u6ce8\u6676\u683c\u5bf9\u79f0\u6027\uff08\u4e09\u89d2vs\u6b63\u65b9\u5f62\uff09\u4e0e\u7578\u53d8\u5404\u5411\u5f02\u6027\u5982\u4f55\u5171\u540c\u8c03\u63a7\u6e17\u6d41\u9608\u503c\u3002", "method": "\u91c7\u7528\u5927\u89c4\u6a21\u8499\u7279\u5361\u6d1b\u6a21\u62df\u7ed3\u5408\u6709\u9650\u5c3a\u5bf8\u6807\u5ea6\u5206\u6790\uff0c\u7cfb\u7edf\u6539\u53d8\u7578\u53d8\u53c2\u6570\u548c\u90bb\u5c45\u8fde\u63a5\u8ddd\u79bb\u9608\u503c\uff0c\u7814\u7a76\u4f4d\u7f6e/\u952e\u6e17\u6d41\u4e34\u754c\u884c\u4e3a\u3002", "result": "1) \u4e09\u89d2\u6676\u683c\uff1a\u4f4d\u7f6e/\u952e\u6e17\u6d41\u9608\u503c\u53ca\u4e34\u754c\u8fde\u63a5\u9608\u503c\u5747\u5448\u73b0\u5f3a\u65b9\u5411\u4f9d\u8d56\u6027\uff0c\u6e90\u4e8e\u7578\u53d8\u5bfc\u81f4\u8fd1\u90bb\u95f4\u8ddd\u5404\u5411\u5f02\u6027\u6539\u53d8\uff1b\u952e\u6e17\u6d41\u5b58\u5728\u65e0\u6cd5\u7528\u5e73\u5747\u914d\u4f4d\u6570\u53d8\u5316\u89e3\u91ca\u7684\u975e\u5e73\u51e1\u884c\u4e3a\u30022) \u6b63\u65b9\u5f62\u6676\u683c\uff1a\u7ebf\u6027\u7578\u53d8\u4e0b\u4ecd\u4fdd\u6301\u6709\u6548\u5404\u5411\u540c\u6027\uff0c\u4e0d\u540c\u65b9\u5411\u7578\u53d8\u5f97\u5230\u76f8\u540c\u6e17\u6d41\u9608\u503c\u30023) \u70ed\u529b\u5b66\u6781\u9650\u6e17\u6d41\u9608\u503c\u9a8c\u8bc1\u4e86\u5927\u5c3a\u5bf8\u6709\u9650\u6676\u683c\u7ed3\u679c\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u6676\u683c\u51e0\u4f55\u7ed3\u6784\u662f\u51b3\u5b9a\u7578\u53d8\u6e17\u6d41\u884c\u4e3a\u7684\u6838\u5fc3\u56e0\u7d20\uff1a\u4e09\u89d2\u6676\u683c\u7684\u9ad8\u5bf9\u79f0\u6027\u4f7f\u5176\u5bf9\u7578\u53d8\u65b9\u5411\u654f\u611f\uff0c\u800c\u6b63\u65b9\u5f62\u6676\u683c\u7684\u4f4e\u5bf9\u79f0\u6027\u53cd\u800c\u5bfc\u81f4\u5404\u5411\u540c\u6027\u54cd\u5e94\uff0c\u63a8\u7ffb\u4ec5\u4f9d\u8d56\u914d\u4f4d\u6570\u53d8\u5316\u7684\u7b80\u5355\u89e3\u91ca\u6846\u67b6\u3002"}}
{"id": "2602.04351", "categories": ["quant-ph", "math.PR"], "pdf": "https://arxiv.org/pdf/2602.04351", "abs": "https://arxiv.org/abs/2602.04351", "authors": ["Antonio Falc\u00f3", "Hermann G. Matthies"], "title": "Vistas of Algebraic Probability: Quantum Computation and Information", "comment": null, "summary": "Kolmogorov's foundation of probability takes measure spaces, $\u03c3$-algebras, and probability measures as basic objects. It is, however, widely recognized that this classical framework is inadequate for random phenomena involving quantum effects, and more generally for \\emph{quantum-like} situations. A broader formulation is provided by an algebraic viewpoint: one starts from an algebra of random variables equipped with a distinguished linear functional -- the \\emph{state} -- interpreted as expectation. In this sense, the approach can also be viewed as a modern reading of ideas already implicit in early probability (e.g., the Bernoullis), while its contemporary form has been developed and used extensively in quantum physics.\n  The algebraic framework accommodates both classical and quantum-like behaviours, yet it remains underused in classical probability and uncertainty quantification, where it can nevertheless open new perspectives and clarify structural features. Although the language carries a physics flavor, the subject is purely probabilistic. The key distinction between classical and quantum-like behaviour is \\emph{commutativity}: its failure produces the characteristic effects of quantum-like situations. The rise of quantum computing is a prominent setting in which such behaviour may become relevant even for practitioners in computational science. Here we focus on the purely algebraic core of the approach. By restricting attention to finite-dimensional algebras, we avoid many analytical subtleties while retaining the main ideas, their classical limit, and their applicability to quantum-like models and quantum computation.", "AI": {"tldr": "This paper advocates for an algebraic probability framework using algebras of random variables with states, generalizing Kolmogorov's measure-theoretic approach to handle quantum-like phenomena, with focus on finite-dimensional algebras for tractability.", "motivation": "Kolmogorov's classical probability framework is inadequate for quantum effects and quantum-like situations; the algebraic framework, while powerful and used in quantum physics, remains underutilized in classical probability despite offering new perspectives.", "method": "Restricting to finite-dimensional algebras to avoid analytical subtleties while preserving core concepts, classical limits, and applicability to quantum-like models and quantum computation.", "result": "The algebraic framework unifies classical and quantum-like behaviors, with commutativity as the key distinguishing feature; connects to early probability concepts and is relevant for emerging quantum computing applications.", "conclusion": "The algebraic approach provides a purely probabilistic generalization of classical probability, offering a tractable finite-dimensional setting for quantum-like phenomena with practical applications in computational science."}}
{"id": "2602.03940", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03940", "abs": "https://arxiv.org/abs/2602.03940", "authors": ["Olaf Yunus Laitinen Imanov", "Duygu Erisken", "Derya Umut Kulali", "Taner Yilmaz", "Rana Irem Turhan"], "title": "Autonomous AI Agents for Real-Time Affordable Housing Site Selection: Multi-Objective Reinforcement Learning Under Regulatory Constraints", "comment": "12 pages, 6 figures, 5 tables", "summary": "Affordable housing shortages affect billions, while land scarcity and regulations make site selection slow. We present AURA (Autonomous Urban Resource Allocator), a hierarchical multi-agent reinforcement learning system for real-time affordable housing site selection under hard regulatory constraints (QCT, DDA, LIHTC). We model the task as a constrained multi-objective Markov decision process optimizing accessibility, environmental impact, construction cost, and social equity while enforcing feasibility. AURA uses a regulatory-aware state encoding 127 federal and local constraints, Pareto-constrained policy gradients with feasibility guarantees, and reward decomposition separating immediate costs from long-term social outcomes. On datasets from 8 U.S. metros (47,392 candidate parcels), AURA attains 94.3% regulatory compliance and improves Pareto hypervolume by 37.2% over strong baselines. In a New York City 2026 case study, it reduces selection time from 18 months to 72 hours and identifies 23% more viable sites; chosen sites have 31% better transit access and 19% lower environmental impact than expert picks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAURA\u2014\u2014\u4e00\u79cd\u5206\u5c42\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728127\u9879\u76d1\u7ba1\u7ea6\u675f\u4e0b\u4f18\u5316\u4fdd\u969c\u6027\u4f4f\u623f\u9009\u5740\uff0c\u5408\u89c4\u7387\u8fbe94.3%\uff0c\u5e76\u5c06\u7ebd\u7ea6\u5e02\u9009\u5740\u65f6\u95f4\u4ece18\u4e2a\u6708\u7f29\u77ed\u81f372\u5c0f\u65f6\uff0c\u540c\u65f6\u63d0\u5347\u9009\u5740\u8d28\u91cf\u3002", "motivation": "\u5168\u7403\u6570\u5341\u4ebf\u4eba\u9762\u4e34\u4fdd\u969c\u6027\u4f4f\u623f\u77ed\u7f3a\uff0c\u800c\u571f\u5730\u7a00\u7f3a\u4e0e\u590d\u6742\u6cd5\u89c4\uff08QCT\u3001DDA\u3001LIHTC\uff09\u5bfc\u81f4\u9009\u5740\u8fc7\u7a0b\u7f13\u6162\u4f4e\u6548\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u4e14\u5408\u89c4\u7684\u51b3\u7b56\u652f\u6301\u5de5\u5177\u3002", "method": "\u7814\u7a76\u56e2\u961f\u63d0\u51faAURA\u7cfb\u7edf\uff0c\u5c06\u9009\u5740\u4efb\u52a1\u5efa\u6a21\u4e3a\u7ea6\u675f\u591a\u76ee\u6807\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u91c7\u7528\u76d1\u7ba1\u611f\u77e5\u72b6\u6001\u7f16\u7801\u5668\u5904\u7406127\u9879\u8054\u90a6\u4e0e\u5730\u65b9\u6cd5\u89c4\u7ea6\u675f\uff0c\u8fd0\u7528\u5e26\u53ef\u884c\u6027\u4fdd\u8bc1\u7684\u5e15\u7d2f\u6258\u7ea6\u675f\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u5956\u52b1\u5206\u89e3\u673a\u5236\u5e73\u8861\u5373\u65f6\u5efa\u8bbe\u6210\u672c\u4e0e\u957f\u671f\u793e\u4f1a\u6548\u76ca\u3002", "result": "\u5728\u8986\u76d68\u4e2a\u7f8e\u56fd\u5927\u90fd\u5e0247,392\u4e2a\u5019\u9009\u5730\u5757\u7684\u6570\u636e\u96c6\u4e0a\uff0cAURA\u5b9e\u73b094.3%\u76d1\u7ba1\u5408\u89c4\u7387\uff0c\u5e15\u7d2f\u6258\u8d85\u4f53\u79ef\u8f83\u5f3a\u57fa\u7ebf\u63d0\u534737.2%\u3002\u7ebd\u7ea6\u5e022026\u5e74\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u9009\u5740\u5468\u671f\u4ece18\u4e2a\u6708\u538b\u7f29\u81f372\u5c0f\u65f6\uff0c\u8bc6\u522b\u53ef\u884c\u5730\u5757\u6570\u91cf\u589e\u52a023%\uff0c\u6240\u9009\u5730\u5757\u4ea4\u901a\u4fbf\u5229\u6027\u63d0\u534731%\uff0c\u73af\u5883\u5f71\u54cd\u964d\u4f4e19%\uff0c\u8868\u73b0\u4f18\u4e8e\u4e13\u5bb6\u51b3\u7b56\u3002", "conclusion": "AURA\u9a8c\u8bc1\u4e86\u5206\u5c42\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u76d1\u7ba1\u73af\u5883\u4e0b\u4f18\u5316\u4fdd\u969c\u6027\u4f4f\u623f\u9009\u5740\u7684\u6709\u6548\u6027\uff0c\u53ef\u663e\u8457\u63d0\u5347\u51b3\u7b56\u6548\u7387\u4e0e\u793e\u4f1a\u516c\u5e73\u6027\uff0c\u4e3a\u57ce\u5e02\u8d44\u6e90\u5206\u914d\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684AI\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.04210", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04210", "abs": "https://arxiv.org/abs/2602.04210", "authors": ["Enyu Zhou", "Zhiheng Xi", "Long Ma", "Zhihao Zhang", "Shihan Dou", "Zhikai Lei", "Guoteng Wang", "Rui Zheng", "Hang Yan", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Steering LLMs via Scalable Interactive Oversight", "comment": null, "summary": "As Large Language Models increasingly automate complex, long-horizon tasks such as \\emph{vibe coding}, a supervision gap has emerged. While models excel at execution, users often struggle to guide them effectively due to insufficient domain expertise, the difficulty of articulating precise intent, and the inability to reliably validate complex outputs. It presents a critical challenge in scalable oversight: enabling humans to responsibly steer AI systems on tasks that surpass their own ability to specify or verify. To tackle this, we propose Scalable Interactive Oversight, a framework that decomposes complex intent into a recursive tree of manageable decisions to amplify human supervision. Rather than relying on open-ended prompting, our system elicits low-burden feedback at each node and recursively aggregates these signals into precise global guidance. Validated in web development task, our framework enables non-experts to produce expert-level Product Requirement Documents, achieving a 54\\% improvement in alignment. Crucially, we demonstrate that this framework can be optimized via Reinforcement Learning using only online user feedback, offering a practical pathway for maintaining human control as AI scales.", "AI": {"tldr": "\u63d0\u51fa\u53ef\u6269\u5c55\u4ea4\u4e92\u5f0f\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u9012\u5f52\u51b3\u7b56\u6811\u5206\u89e3\u590d\u6742\u610f\u56fe\u5e76\u653e\u5927\u4eba\u7c7b\u76d1\u7763\uff0c\u4f7f\u975e\u4e13\u5bb6\u80fd\u6307\u5bfc\u5927\u8bed\u8a00\u6a21\u578b\u5b8c\u6210\u590d\u6742\u4efb\u52a1\uff0c\u5bf9\u9f50\u6548\u679c\u63d0\u534754%\uff0c\u4e14\u53ef\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u5316\u590d\u6742\u4efb\u52a1\uff0c\u51fa\u73b0\u76d1\u7763\u9e3f\u6c9f\uff1a\u7528\u6237\u7f3a\u4e4f\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u3001\u96be\u4ee5\u660e\u786e\u8868\u8fbe\u610f\u56fe\u3001\u65e0\u6cd5\u9a8c\u8bc1\u590d\u6742\u8f93\u51fa\uff0c\u5bfc\u81f4\u53ef\u6269\u5c55\u76d1\u7763\u9762\u4e34\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u53ef\u6269\u5c55\u4ea4\u4e92\u5f0f\u76d1\u7763\u6846\u67b6\uff0c\u5c06\u590d\u6742\u610f\u56fe\u5206\u89e3\u4e3a\u53ef\u7ba1\u7406\u7684\u9012\u5f52\u51b3\u7b56\u6811\uff0c\u5728\u6bcf\u4e2a\u8282\u70b9\u83b7\u53d6\u4f4e\u8d1f\u62c5\u53cd\u9988\uff0c\u5e76\u9012\u5f52\u805a\u5408\u6210\u7cbe\u786e\u7684\u5168\u5c40\u6307\u5bfc\u3002", "result": "\u5728\u7f51\u9875\u5f00\u53d1\u4efb\u52a1\u4e2d\uff0c\u975e\u4e13\u5bb6\u80fd\u751f\u6210\u4e13\u5bb6\u7ea7\u4ea7\u54c1\u9700\u6c42\u6587\u6863\uff0c\u5bf9\u9f50\u6548\u679c\u63d0\u534754%\uff1b\u8be5\u6846\u67b6\u53ef\u4ec5\u901a\u8fc7\u5728\u7ebf\u7528\u6237\u53cd\u9988\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u3002", "conclusion": "\u4e3a\u5728AI\u89c4\u6a21\u6269\u5c55\u65f6\u4fdd\u6301\u4eba\u7c7b\u63a7\u5236\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2602.03945", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03945", "abs": "https://arxiv.org/abs/2602.03945", "authors": ["Tamara Cucumides", "Floris Geerts"], "title": "Grables: Tabular Learning Beyond Independent Rows", "comment": null, "summary": "Tabular learning is still dominated by row-wise predictors that score each row independently, which fits i.i.d. benchmarks but fails on transactional, temporal, and relational tables where labels depend on other rows. We show that row-wise prediction rules out natural targets driven by global counts, overlaps, and relational patterns. To make \"using structure\" precise across architectures, we introduce grables: a modular interface that separates how a table is lifted to a graph (constructor) from how predictions are computed on that graph (node predictor), pinpointing where expressive power comes from. Experiments on synthetic tasks, transaction data, and a RelBench clinical-trials dataset confirm the predicted separations: message passing captures inter-row dependencies that row-local models miss, and hybrid approaches that explicitly extract inter-row structure and feed it to strong tabular learners yield consistent gains.", "AI": {"tldr": "Proposes \"grables\" framework to model inter-row dependencies in tabular data by separating graph construction from graph-based prediction, outperforming row-independent methods on structured tables", "motivation": "Row-wise predictors fail on transactional/temporal/relational tables where labels depend on other rows; existing methods lack precise characterization of how structure is utilized", "method": "Introduces \"grables\" - a modular interface separating table-to-graph lifting (constructor) from graph prediction (node predictor), enabling systematic analysis of structural expressiveness", "result": "Message passing captures inter-row dependencies missed by row-local models; hybrid approaches extracting explicit inter-row structure yield consistent gains across synthetic, transaction and clinical trial datasets", "conclusion": "Structural dependencies in tables require graph-based modeling; grables provides principled framework for leveraging table structure across architectures, with hybrids combining structure extraction and strong tabular learners showing particular promise"}}
{"id": "2602.04213", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04213", "abs": "https://arxiv.org/abs/2602.04213", "authors": ["Feiyu Gavin Zhu", "Jean Oh", "Reid Simmons"], "title": "InterPReT: Interactive Policy Restructuring and Training Enable Effective Imitation Learning from Laypersons", "comment": "Proceedings of the 21st ACM/IEEE International Conference on Human-Robot Interaction", "summary": "Imitation learning has shown success in many tasks by learning from expert demonstrations. However, most existing work relies on large-scale demonstrations from technical professionals and close monitoring of the training process. These are challenging for a layperson when they want to teach the agent new skills. To lower the barrier of teaching AI agents, we propose Interactive Policy Restructuring and Training (InterPReT), which takes user instructions to continually update the policy structure and optimize its parameters to fit user demonstrations. This enables end-users to interactively give instructions and demonstrations, monitor the agent's performance, and review the agent's decision-making strategies. A user study (N=34) on teaching an AI agent to drive in a racing game confirms that our approach yields more robust policies without impairing system usability, compared to a generic imitation learning baseline, when a layperson is responsible for both giving demonstrations and determining when to stop. This shows that our method is more suitable for end-users without much technical background in machine learning to train a dependable policy", "AI": {"tldr": "This paper proposes InterPReT, an interactive imitation learning framework that enables laypeople to teach AI agents through natural instructions and demonstrations, showing improved robustness and usability over baselines.", "motivation": "Existing imitation learning methods require large-scale expert demonstrations and technical monitoring, creating barriers for non-technical end-users who want to teach AI agents new skills.", "method": "Interactive Policy Restructuring and Training (InterPReT) - a system that continuously updates policy structure and optimizes parameters based on user instructions and demonstrations, allowing interactive teaching, performance monitoring, and decision review.", "result": "A user study (N=34) in a racing game domain demonstrated that InterPReT produces more robust policies while maintaining system usability compared to standard imitation learning when laypeople provide both demonstrations and training decisions.", "conclusion": "The approach makes imitation learning accessible to non-technical users, enabling them to train reliable AI policies through natural interaction without machine learning expertise."}}
{"id": "2602.04370", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04370", "abs": "https://arxiv.org/abs/2602.04370", "authors": ["Rasmus Vesterager Gothelf", "Lars Bojer Madsen", "Christian Saugbjerg Lange"], "title": "Limitations of an approximative phase-space description in strong-field quantum optics", "comment": null, "summary": "In recent years, strong-field processes such as high-order harmonic generation (HHG) and above-threshold ionization driven by nonclassical states of light have become an increasingly popular field of study. The theoretical modeling of these processes often applies an approximate phase-space expansion of the nonclassical driving field in terms of coherent states, which has been shown to accurately predict the harmonic spectrum. However, its accuracy for the computation of quantum optical observables like the degree of squeezing and photon statistics has not been thoroughly considered. In this work, we introduce this approximative phase-space description and discuss its accuracy, and we find that it mischaracterizes the quantum optical properties of the driving laser by making it an incoherent mixture of classical states. We further show that this error in the driving field description maps onto the light emitted from HHG, as neither sub-Poissonian photon statistics nor quadrature squeezing below vacuum fluctuations can be captured by the approximative phase-space description. Lastly, to benchmark the approximative phase-space description, we consider the quantum HHG from a one-band model, which yields an exact analytical solution. Using the approximative phase-space representation with this specific model, we find a small quantitative error in the quadrature variance of the emitted field that scales with pulse duration and emitter density. Our results show that using this approximative phase-space description can mischaracterize quantum optical observables. Attributing physical meaning to such results should therefore be accompanied by a quantitative analysis of the error.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u7528\u76f8\u5e72\u6001\u8fd1\u4f3c\u76f8\u7a7a\u95f4\u63cf\u8ff0\u975e\u7ecf\u5178\u5149\u573a\u5728\u5f3a\u573a\u8fc7\u7a0b\uff08\u5982\u9ad8\u6b21\u8c10\u6ce2\u751f\u6210\uff09\u4e2d\u7684\u51c6\u786e\u6027\u3002\u53d1\u73b0\u8be5\u65b9\u6cd5\u4f1a\u9519\u8bef\u5730\u5c06\u9a71\u52a8\u6fc0\u5149\u8868\u5f81\u4e3a\u7ecf\u5178\u6001\u7684\u975e\u76f8\u5e72\u6df7\u5408\uff0c\u5bfc\u81f4\u65e0\u6cd5\u6355\u6349\u4e9a\u6cca\u677e\u5149\u5b50\u7edf\u8ba1\u548c\u4f4e\u4e8e\u771f\u7a7a\u8d77\u4f0f\u7684\u538b\u7f29\u6001\u3002\u5728\u4e00\u7ef4\u5e26\u6a21\u578b\u7684\u7cbe\u786e\u89e3\u5bf9\u6bd4\u4e0b\uff0c\u8fd1\u4f3c\u63cf\u8ff0\u4ec5\u4ea7\u751f\u4e0e\u8109\u51b2\u6301\u7eed\u65f6\u95f4\u548c\u53d1\u5c04\u4f53\u5bc6\u5ea6\u76f8\u5173\u7684\u5c0f\u91cf\u5b9a\u91cf\u8bef\u5dee\uff0c\u4f46\u4ecd\u9700\u5b9a\u91cf\u8bef\u5dee\u5206\u6790\u4ee5\u907f\u514d\u8bef\u89e3\u91cf\u5b50\u5149\u5b66\u89c2\u6d4b\u91cf\u3002", "motivation": "\u5f3a\u573a\u8fc7\u7a0b\uff08\u5982\u9ad8\u6b21\u8c10\u6ce2\u751f\u6210\u548c\u9608\u4e0a\u7535\u79bb\uff09\u5728\u975e\u7ecf\u5178\u5149\u573a\u9a71\u52a8\u4e0b\u7684\u7814\u7a76\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u73b0\u6709\u7406\u8bba\u5e38\u7528\u76f8\u5e72\u6001\u8fd1\u4f3c\u76f8\u7a7a\u95f4\u5c55\u5f00\uff0c\u5176\u5bf9\u91cf\u5b50\u5149\u5b66\u89c2\u6d4b\u91cf\u7684\u51c6\u786e\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u9a8c\u8bc1\u3002", "method": "\u5f15\u5165\u8fd1\u4f3c\u76f8\u7a7a\u95f4\u63cf\u8ff0\uff0c\u5e76\u4e0e\u4e00\u7ef4\u5e26\u6a21\u578b\u7684\u7cbe\u786e\u89e3\u6790\u89e3\u8fdb\u884c\u6bd4\u8f83\uff0c\u8bc4\u4f30\u5176\u5bf9\u91cf\u5b50\u5149\u5b66\u89c2\u6d4b\u91cf\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002", "result": "\u8fd1\u4f3c\u76f8\u7a7a\u95f4\u63cf\u8ff0\u5c06\u975e\u7ecf\u5178\u9a71\u52a8\u573a\u8bef\u8868\u5f81\u4e3a\u7ecf\u5178\u6001\u7684\u975e\u76f8\u5e72\u6df7\u5408\uff0c\u5bfc\u81f4\u65e0\u6cd5\u9884\u6d4b\u4e9a\u6cca\u677e\u5149\u5b50\u7edf\u8ba1\u548c\u4f4e\u4e8e\u771f\u7a7a\u8d77\u4f0f\u7684\u538b\u7f29\u6001\u3002\u4f7f\u7528\u8be5\u63cf\u8ff0\u65f6\uff0c\u53d1\u5c04\u5149\u7684\u6b63\u4ea4\u65b9\u5dee\u5b58\u5728\u5c0f\u91cf\u5b9a\u91cf\u8bef\u5dee\uff0c\u8bef\u5dee\u968f\u8109\u51b2\u6301\u7eed\u65f6\u95f4\u548c\u53d1\u5c04\u4f53\u5bc6\u5ea6\u7f29\u653e\u3002", "conclusion": "\u4f7f\u7528\u8fd1\u4f3c\u76f8\u7a7a\u95f4\u63cf\u8ff0\u53ef\u80fd\u9519\u8bef\u5730\u8868\u5f81\u91cf\u5b50\u5149\u5b66\u89c2\u6d4b\u91cf\uff0c\u56e0\u6b64\u5728\u89e3\u91ca\u7269\u7406\u610f\u4e49\u65f6\u5fc5\u987b\u8fdb\u884c\u5b9a\u91cf\u8bef\u5dee\u5206\u6790\u3002"}}
{"id": "2602.04394", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04394", "abs": "https://arxiv.org/abs/2602.04394", "authors": ["Michal Natan", "Saar Levin", "Avi Pe'er"], "title": "Squeezing Enhanced Sagnac Sensing based on SU(1,1) Quantum Interference", "comment": null, "summary": "We present a simple and robust design for a squeezing-enhanced Sagnac interferometer that employs the concept of SU(1,1) interference to significantly surpass the classical sensitivity limit (shot-noise limit - SNL) in rotational sensing. By strategically placing an optical parametric amplifier (OPA) inside the Sagnac loop, light is automatically squeezed in both forward and backward directions of the loop, which enhances the detectability of a small phase. For measuring the squeezed quadrature, we explore two approaches: Direct detection of the output intensity, which is simple, but requires a high-efficiency photo-detector; and parametric homodyne with an additional OPA, which accepts practical detectors with no efficiency limitation, but is technically more complex. Our analysis demonstrates super-classical sensitivity under most realistic conditions of loss and detector inefficiency, thereby leveraging the resources of squeezing and the principles of SU(1,1) interference, while maintaining compatibility with standard Sagnac configurations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eSU(1,1)\u5e72\u6d89\u7684\u538b\u7f29\u589e\u5f3a\u578bSagnac\u5e72\u6d89\u4eea\u8bbe\u8ba1\u65b9\u6848\uff0c\u901a\u8fc7\u5728\u73af\u8def\u4e2d\u653e\u7f6e\u5149\u53c2\u91cf\u653e\u5927\u5668\u5b9e\u73b0\u53cc\u5411\u81ea\u52a8\u538b\u7f29\uff0c\u5728\u635f\u8017\u548c\u63a2\u6d4b\u5668\u6548\u7387\u9650\u5236\u4e0b\u4ecd\u53ef\u5b9e\u73b0\u8d85\u8d8a\u7ecf\u5178\u6781\u9650\u7684\u65cb\u8f6c\u4f20\u611f\u7075\u654f\u5ea6\u3002", "motivation": "\u4f20\u7edfSagnac\u5e72\u6d89\u4eea\u5728\u65cb\u8f6c\u4f20\u611f\u4e2d\u53d7\u6563\u7c92\u566a\u58f0\u6781\u9650(SNL)\u7ea6\u675f\uff0c\u4e3a\u5b9e\u73b0\u66f4\u9ad8\u7075\u654f\u5ea6\uff0c\u9700\u7ed3\u5408\u91cf\u5b50\u538b\u7f29\u6001\u4e0eSU(1,1)\u5e72\u6d89\u539f\u7406\u7a81\u7834\u7ecf\u5178\u9650\u5236\u3002", "method": "\u5728Sagnac\u73af\u8def\u5185\u90e8\u6218\u7565\u6027\u653e\u7f6e\u5149\u53c2\u91cf\u653e\u5927\u5668(OPA)\uff0c\u4f7f\u5149\u675f\u5728\u73af\u8def\u6b63\u5411\u548c\u53cd\u5411\u4f20\u64ad\u65f6\u81ea\u52a8\u4ea7\u751f\u538b\u7f29\uff1b\u6bd4\u8f83\u4e24\u79cd\u63a2\u6d4b\u65b9\u6848\uff1a\u76f4\u63a5\u5f3a\u5ea6\u63a2\u6d4b\uff08\u9700\u9ad8\u6548\u7387\u63a2\u6d4b\u5668\uff09\u548c\u53c2\u91cf\u96f6\u5dee\u63a2\u6d4b\uff08\u6280\u672f\u590d\u6742\u4f46\u63a2\u6d4b\u5668\u65e0\u6548\u7387\u9650\u5236\uff09\u3002", "result": "\u5728\u5927\u591a\u6570\u5b9e\u9645\u635f\u8017\u548c\u63a2\u6d4b\u5668\u975e\u7406\u60f3\u6761\u4ef6\u4e0b\uff0c\u8be5\u8bbe\u8ba1\u4ecd\u80fd\u5b9e\u73b0\u8d85\u7ecf\u5178\u7075\u654f\u5ea6\uff0c\u6709\u6548\u5229\u7528\u538b\u7f29\u8d44\u6e90\u5e76\u517c\u5bb9\u6807\u51c6Sagnac\u6784\u578b\u3002", "conclusion": "\u8be5\u65b9\u6848\u6210\u529f\u5c06\u91cf\u5b50\u538b\u7f29\u4e0eSU(1,1)\u5e72\u6d89\u7ed3\u5408\uff0c\u4e3a\u9ad8\u7cbe\u5ea6\u65cb\u8f6c\u4f20\u611f\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2602.03957", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.03957", "abs": "https://arxiv.org/abs/2602.03957", "authors": ["Md Muhtasim Munif Fahim", "Md Rezaul Karim"], "title": "Child Mortality Prediction in Bangladesh: A Decade-Long Validation Study", "comment": null, "summary": "The predictive machine learning models for child mortality tend to be inaccurate when applied to future populations, since they suffer from look-ahead bias due to the randomization used in cross-validation. The Demographic and Health Surveys (DHS) data from Bangladesh for 2011-2022, with n = 33,962, are used in this paper. We trained the model on (2011-2014) data, validated it on 2017 data, and tested it on 2022 data. Eight years after the initial test of the model, a genetic algorithm-based Neural Architecture Search found a single-layer neural architecture (with 64 units) to be superior to XGBoost (AUROC = 0.76 vs. 0.73; p < 0.01). Additionally, through a detailed fairness audit, we identified an overall \"Socioeconomic Predictive Gradient,\" with a positive correlation between regional poverty level (r = -0.62) and the algorithm's AUC. In addition, we found that the model performed at its highest levels in the least affluent divisions (AUC 0.74) and decreased dramatically in the wealthiest divisions (AUC 0.66). These findings suggest that the model is identifying areas with the greatest need for intervention. Our model would identify approximately 1300 additional at-risk children annually than a Gradient Boosting model when screened at the 10% level and validated using SHAP values and Platt Calibration, and therefore provide a robust, production-ready computational phenotype for targeted maternal and child health interventions.", "AI": {"tldr": "Researchers developed a neural network model using Bangladesh DHS data (2011-2022) with temporal validation to predict child mortality, avoiding look-ahead bias. The model outperformed XGBoost and works best in poorer regions, potentially identifying 1,300+ additional at-risk children annually for targeted health interventions.", "motivation": "To address look-ahead bias in child mortality prediction models caused by random cross-validation, which makes them inaccurate for future populations, and to create a robust model for effective maternal and child health interventions.", "method": "Used DHS data from Bangladesh (n=33,962) with temporal split: trained on 2011-2014 data, validated on 2017 data, tested on 2022 data. Applied genetic algorithm-based Neural Architecture Search to find optimal architecture, compared against XGBoost, and conducted fairness audit with SHAP values and Platt Calibration.", "result": "A single-layer neural network (64 units) achieved superior performance to XGBoost (AUROC 0.76 vs 0.73, p<0.01). Identified a \"Socioeconomic Predictive Gradient\" showing negative correlation between regional poverty and model performance (r=-0.62), with highest AUC in poorest divisions (0.74) versus wealthiest (0.66). The model would identify approximately 1,300 additional at-risk children annually at 10% screening level.", "conclusion": "The temporally-validated neural network provides a production-ready computational phenotype that accurately predicts child mortality and prioritizes high-need, socioeconomically disadvantaged regions, enabling more effective targeted interventions."}}
{"id": "2602.04284", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04284", "abs": "https://arxiv.org/abs/2602.04284", "authors": ["Yansong Ning", "Jun Fang", "Naiqiang Tan", "Hao Liu"], "title": "Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning", "comment": "Under Review", "summary": "Managing agent thought and observation during multi-turn agent-environment interactions is an emerging strategy to improve agent efficiency. However, existing studies treat the entire interaction trajectories equally, overlooking the thought necessity and observation utility varies across turns. To this end, we first conduct quantitative investigations into how thought and observation affect agent effectiveness and efficiency. Based on our findings, we propose Agent-Omit, a unified training framework that empowers LLM agents to adaptively omit redundant thoughts and observations. Specifically, we first synthesize a small amount of cold-start data, including both single-turn and multi-turn omission scenarios, to fine-tune the agent for omission behaviors. Furthermore, we introduce an omit-aware agentic reinforcement learning approach, incorporating a dual sampling mechanism and a tailored omission reward to incentivize the agent's adaptive omission capability. Theoretically, we prove that the deviation of our omission policy is upper-bounded by KL-divergence. Experimental results on five agent benchmarks show that our constructed Agent-Omit-8B could obtain performance comparable to seven frontier LLM agent, and achieve the best effectiveness-efficiency trade-off than seven efficient LLM agents methods. Our code and data are available at https://github.com/usail-hkust/Agent-Omit.", "AI": {"tldr": "\u63d0\u51faAgent-Omit\u6846\u67b6\uff0c\u901a\u8fc7\u51b7\u542f\u52a8\u6570\u636e\u5fae\u8c03\u548c\u7701\u7565\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\uff0c\u4f7fLLM\u667a\u80fd\u4f53\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u81ea\u9002\u5e94\u7701\u7565\u5197\u4f59\u601d\u8003\u4e0e\u89c2\u5bdf\uff0c\u5b9e\u73b0\u6548\u679c-\u6548\u7387\u6700\u4f73\u6743\u8861\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5e73\u7b49\u5bf9\u5f85\u591a\u8f6e\u4ea4\u4e92\u4e2d\u7684\u6240\u6709\u8f68\u8ff9\uff0c\u5ffd\u89c6\u4e86\u601d\u8003\u5fc5\u8981\u6027\u548c\u89c2\u5bdf\u6548\u7528\u7684\u8de8\u8f6e\u6b21\u5dee\u5f02\uff0c\u5bfc\u81f4\u667a\u80fd\u4f53\u6548\u7387\u672a\u8fbe\u6700\u4f18\u3002", "method": "1) \u91cf\u5316\u5206\u6790\u601d\u8003\u89c2\u5bdf\u5bf9\u6548\u679c\u6548\u7387\u7684\u5f71\u54cd\uff1b2) \u5408\u6210\u5355/\u591a\u8f6e\u7701\u7565\u51b7\u542f\u52a8\u6570\u636e\u5fae\u8c03\u6a21\u578b\uff1b3) \u8bbe\u8ba1\u7701\u7565\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u53cc\u91cd\u91c7\u6837\u673a\u5236\u548c\u5b9a\u5236\u5316\u7701\u7565\u5956\u52b1\uff1b4) \u7406\u8bba\u8bc1\u660e\u7701\u7565\u7b56\u7565\u504f\u5dee\u53d7KL\u6563\u5ea6\u4e0a\u754c\u7ea6\u675f\u3002", "result": "\u5728\u4e94\u4e2a\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAgent-Omit-8B\u6027\u80fd\u5ab2\u7f8e\u4e03\u79cd\u524d\u6cbfLLM\u667a\u80fd\u4f53\uff0c\u4e14\u76f8\u6bd4\u4e03\u79cd\u9ad8\u6548\u65b9\u6cd5\u83b7\u5f97\u6700\u4f18\u7684\u6548\u679c-\u6548\u7387\u6743\u8861\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u8d4b\u80fdLLM\u667a\u80fd\u4f53\u81ea\u9002\u5e94\u7701\u7565\u80fd\u529b\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4ea4\u4e92\u6548\u7387\uff0c\u4e3a\u9ad8\u6548\u667a\u80fd\u4f53\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2602.04443", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04443", "abs": "https://arxiv.org/abs/2602.04443", "authors": ["Mourad Halla"], "title": "Qudit Twisted-Torus Codes in the Bivariate Bicycle Framework", "comment": null, "summary": "We study finite-length qudit quantum low-density parity-check (LDPC) codes from translation-invariant CSS constructions on two-dimensional tori with twisted boundary conditions. Recent qubit work [PRX Quantum 6, 020357 (2025)] showed that, within the bivariate-bicycle viewpoint, twisting generalized toric patterns can significantly improve finite-size performance as measured by $k d^{2}/n$. Here $n$ denotes the number of physical qudits, $k$ the number of logical qudits, and $d$ the code distance. Building on this insight, we extend the search to qudit codes over finite fields. Using algebraic methods, we compute the number of logical qudits and identify compact codes with favorable rate--distance tradeoffs. Overall, for the finite sizes explored, twisted-torus qudit constructions typically achieve larger distances than their untwisted counterparts and outperform previously reported twisted qubit instances. The best new codes are tabulated.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e8c\u7ef4\u73af\u9762\u4e0a\u5177\u6709\u626d\u66f2\u8fb9\u754c\u7684\u5e73\u79fb\u4e0d\u53d8CSS\u6784\u9020\u7684\u6709\u9650\u957f\u5ea6qudit\u91cf\u5b50LDPC\u7801\uff0c\u53d1\u73b0\u626d\u66f2qudit\u6784\u9020\u76f8\u6bd4\u672a\u626d\u66f2\u7248\u672c\u548c\u4e4b\u524d\u7684\u626d\u66f2qubit\u5b9e\u4f8b\u80fd\u5b9e\u73b0\u66f4\u5927\u7684\u7801\u8ddd\u3002", "motivation": "\u57fa\u4e8e\u8fd1\u671f\u5173\u4e8e\u626d\u66f2\u8fb9\u754c\u6761\u4ef6\u6539\u5584qubit\u7801\u6027\u80fd\u7684\u7814\u7a76\uff0c\u672c\u6587\u5c06\u8fd9\u4e00\u601d\u8def\u63a8\u5e7f\u5230\u6709\u9650\u57df\u4e0a\u7684qudit\u7801\uff0c\u65e8\u5728\u63a2\u7d22\u626d\u66f2\u73af\u9762\u6784\u9020\u5728qudit\u7cfb\u7edf\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u5bfb\u627e\u5177\u6709\u4f18\u8d8a\u7387-\u8ddd\u79bb\u6743\u8861\u7684\u7d27\u51d1\u7801\u3002", "method": "\u91c7\u7528\u4ee3\u6570\u65b9\u6cd5\u8ba1\u7b97\u5e73\u79fb\u4e0d\u53d8CSS\u6784\u9020\u7684\u903b\u8f91qudit\u6570\u91cf\uff0c\u5728\u4e8c\u7ef4\u73af\u9762\u4e0a\u641c\u7d22\u5177\u6709\u626d\u66f2\u8fb9\u754c\u6761\u4ef6\u7684\u7d27\u51d1\u7801\uff0c\u5e76\u5206\u6790\u5176\u7387-\u8ddd\u79bb\u6298\u8877\u3002", "result": "\u5728\u7814\u7a76\u7684\u6709\u9650\u5c3a\u5bf8\u8303\u56f4\u5185\uff0c\u626d\u66f2\u73af\u9762qudit\u6784\u9020\u901a\u5e38\u6bd4\u672a\u626d\u66f2\u7248\u672c\u5b9e\u73b0\u66f4\u5927\u7684\u7801\u8ddd\uff0c\u5e76\u4e14\u6027\u80fd\u4f18\u4e8e\u5148\u524d\u62a5\u9053\u7684\u626d\u66f2qubit\u5b9e\u4f8b\u3002\u6700\u4f73\u65b0\u7801\u5df2\u5217\u8868\u5448\u73b0\u3002", "conclusion": "\u626d\u66f2\u8fb9\u754c\u6761\u4ef6\u5bf9qudit LDPC\u7801\u6709\u76ca\uff0c\u76f8\u6bd4\u672a\u626d\u66f2\u6784\u9020\u548c\u4e4b\u524d\u7684qubit\u65b9\u6848\u80fd\u63d0\u4f9b\u66f4\u597d\u7684\u8ddd\u79bb\u7279\u6027\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u91cf\u5b50\u7ea0\u9519\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2602.04444", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04444", "abs": "https://arxiv.org/abs/2602.04444", "authors": ["G. A. Bochkin", "E. B. Fel'dman", "E. I. Kuznetsova", "E. I. Shipulya"], "title": "Influence of environment on quantum correlations in two-spin systems with dipole-dipole interactions", "comment": "12 pages, 4 figures", "summary": "An influence of environment on quantum correlations (entanglement and quantum discord) is studied in a two-spin-1/2 system with dipole-dipole interactions on the basis of Lindblad equation. We consider the simplest case when the environment causes only dephasing of system spins. The dependencies of entanglement and the quantum discord on the relaxation rate are obtained. We compare the influence of the environment on entanglement and quantum discord.", "AI": {"tldr": "\u7814\u7a76\u53cc\u81ea\u65cb-1/2\u7cfb\u7edf\u4e2d\u73af\u5883\u9000\u76f8\u5e72\u5bf9\u91cf\u5b50\u7ea0\u7f20\u548c\u91cf\u5b50\u5931\u534f\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u91cf\u5b50\u5931\u534f\u6bd4\u7ea0\u7f20\u66f4\u5177\u9c81\u68d2\u6027", "motivation": "\u7406\u89e3\u73af\u5883\u566a\u58f0\u5bf9\u91cf\u5b50\u5173\u8054\uff08\u7ea0\u7f20\u548c\u91cf\u5b50\u5931\u534f\uff09\u7684\u4e0d\u540c\u5f71\u54cd\uff0c\u5bf9\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u4e2d\u7ef4\u6301\u91cf\u5b50\u7279\u6027\u81f3\u5173\u91cd\u8981", "method": "\u57fa\u4e8eLindblad\u65b9\u7a0b\uff0c\u7814\u7a76\u5177\u6709\u5076\u6781-\u5076\u6781\u76f8\u4e92\u4f5c\u7528\u7684\u53cc\u81ea\u65cb-1/2\u7cfb\u7edf\uff0c\u8003\u8651\u73af\u5883\u4ec5\u5f15\u8d77\u7cfb\u7edf\u81ea\u65cb\u9000\u76f8\u4f4d\u7684\u60c5\u51b5", "result": "\u83b7\u5f97\u7ea0\u7f20\u4e0e\u91cf\u5b50\u5931\u534f\u968f\u5f1b\u8c6b\u901f\u7387\u7684\u53d8\u5316\u5173\u7cfb\uff0c\u663e\u793a\u91cf\u5b50\u5931\u534f\u6bd4\u7ea0\u7f20\u8870\u51cf\u66f4\u7f13\u6162", "conclusion": "\u91cf\u5b50\u5931\u534f\u5bf9\u73af\u5883\u9000\u76f8\u4f4d\u7684\u62b5\u6297\u529b\u5f3a\u4e8e\u91cf\u5b50\u7ea0\u7f20\uff0c\u5728\u566a\u58f0\u73af\u5883\u4e2d\u66f4\u5177\u5e94\u7528\u6f5c\u529b"}}
{"id": "2602.04385", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.04385", "abs": "https://arxiv.org/abs/2602.04385", "authors": ["Marco Picone", "Fabio Turazza", "Matteo Martinelli", "Marco Mamei"], "title": "Digital Twins & ZeroConf AI: Structuring Automated Intelligent Pipelines for Industrial Applications", "comment": "Author-accepted manuscript of a paper published in the 2025 IEEE International Conference on Systems, Man and Cybernetics (IEEE SMC), October 2025, doi: 10.1109/SMC58881.2025.11343418", "summary": "The increasing complexity of Cyber-Physical Systems (CPS), particularly in the industrial domain, has amplified the challenges associated with the effective integration of Artificial Intelligence (AI) and Machine Learning (ML) techniques. Fragmentation across IoT and IIoT technologies, manifested through diverse communication protocols, data formats and device capabilities, creates a substantial gap between low-level physical layers and high-level intelligent functionalities. Recently, Digital Twin (DT) technology has emerged as a promising solution, offering structured, interoperable and semantically rich digital representations of physical assets. Current approaches are often siloed and tightly coupled, limiting scalability and reuse of AI functionalities. This work proposes a modular and interoperable solution that enables seamless AI pipeline integration into CPS by minimizing configuration and decoupling the roles of DTs and AI components. We introduce the concept of Zero Configuration (ZeroConf) AI pipelines, where DTs orchestrate data management and intelligent augmentation. The approach is demonstrated in a MicroFactory scenario, showing support for concurrent ML models and dynamic data processing, effectively accelerating the deployment of intelligent services in complex industrial settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\u7684\u96f6\u914d\u7f6eAI\u6d41\u6c34\u7ebf\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u5b9e\u73b0\u5de5\u4e1a\u4fe1\u606f\u7269\u7406\u7cfb\u7edf\u4e2dAI\u7684\u65e0\u7f1d\u96c6\u6210\uff0c\u5e76\u5728\u5fae\u578b\u5de5\u5382\u573a\u666f\u9a8c\u8bc1\u5176\u652f\u6301\u591a\u6a21\u578b\u5e76\u53d1\u548c\u52a8\u6001\u6570\u636e\u5904\u7406\u7684\u80fd\u529b\u3002", "motivation": "\u5de5\u4e1a\u4fe1\u606f\u7269\u7406\u7cfb\u7edf(CPS)\u7684\u590d\u6742\u6027\u65e5\u76ca\u589e\u52a0\uff0c\u7269\u8054\u7f51/\u5de5\u4e1a\u4e92\u8054\u7f51\u6280\u672f\u5728\u901a\u4fe1\u534f\u8bae\u3001\u6570\u636e\u683c\u5f0f\u548c\u8bbe\u5907\u80fd\u529b\u65b9\u9762\u7684\u788e\u7247\u5316\u9020\u6210\u4e86\u7269\u7406\u5c42\u4e0e\u667a\u80fd\u529f\u80fd\u4e4b\u95f4\u7684\u5de8\u5927\u9e3f\u6c9f\u3002\u73b0\u6709\u65b9\u6cd5\u5b64\u7acb\u4e14\u7d27\u8026\u5408\uff0c\u9650\u5236\u4e86AI\u529f\u80fd\u7684\u53ef\u6269\u5c55\u6027\u548c\u590d\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6a21\u5757\u5316\u548c\u53ef\u4e92\u64cd\u4f5c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5f15\u5165\u96f6\u914d\u7f6e(ZeroConf)AI\u6d41\u6c34\u7ebf\u6982\u5ff5\uff0c\u7531\u6570\u5b57\u5b6a\u751f\u534f\u8c03\u6570\u636e\u7ba1\u7406\u548c\u667a\u80fd\u589e\u5f3a\uff0c\u89e3\u8026\u6570\u5b57\u5b6a\u751f\u4e0eAI\u7ec4\u4ef6\u7684\u89d2\u8272\uff0c\u6700\u5c0f\u5316\u914d\u7f6e\u9700\u6c42\u3002", "result": "\u5728\u5fae\u578b\u5de5\u5382\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5bf9\u5e76\u53d1\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u52a8\u6001\u6570\u636e\u5904\u7406\u7684\u652f\u6301\u3002", "conclusion": "\u8be5\u65b9\u6848\u6709\u6548\u52a0\u901f\u4e86\u590d\u6742\u5de5\u4e1a\u73af\u5883\u4e2d\u667a\u80fd\u670d\u52a1\u7684\u90e8\u7f72\u3002"}}
{"id": "2602.03986", "categories": ["cs.LG", "cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.03986", "abs": "https://arxiv.org/abs/2602.03986", "authors": ["Nikolaos Bousias", "Lars Lindemann", "George Pappas"], "title": "eCP: Informative uncertainty quantification via Equivariantized Conformal Prediction with pre-trained models", "comment": null, "summary": "We study the effect of group symmetrization of pre-trained models on conformal prediction (CP), a post-hoc, distribution-free, finite-sample method of uncertainty quantification that offers formal coverage guarantees under the assumption of data exchangeability. Unfortunately, CP uncertainty regions can grow significantly in long horizon missions, rendering the statistical guarantees uninformative. To that end, we propose infusing CP with geometric information via group-averaging of the pretrained predictor to distribute the non-conformity mass across the orbits. Each sample now is treated as a representative of an orbit, thus uncertainty can be mitigated by other samples entangled to it via the orbit inducing elements of the symmetry group. Our approach provably yields contracted non-conformity scores in increasing convex order, implying improved exponential-tail bounds and sharper conformal prediction sets in expectation, especially at high confidence levels. We then propose an experimental design to test these theoretical claims in pedestrian trajectory prediction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u7fa4\u5bf9\u79f0\u5316\u9884\u8bad\u7ec3\u6a21\u578b\u6765\u6539\u8fdb\u5171\u5f62\u9884\u6d4b\uff0c\u5229\u7528\u51e0\u4f55\u5bf9\u79f0\u6027\u51cf\u5c11\u957f\u65f6\u57df\u4efb\u52a1\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u533a\u57df\u81a8\u80c0\u3002", "motivation": "\u5171\u5f62\u9884\u6d4b\u867d\u63d0\u4f9b\u5206\u5e03\u65e0\u5173\u7684\u6709\u9650\u6837\u672c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4fdd\u8bc1\uff0c\u4f46\u5728\u957f\u65f6\u57df\u4efb\u52a1\u4e2d\u4e0d\u786e\u5b9a\u6027\u533a\u57df\u4f1a\u663e\u8457\u589e\u957f\uff0c\u5bfc\u81f4\u7edf\u8ba1\u4fdd\u8bc1\u5931\u53bb\u4fe1\u606f\u91cf\u3002", "method": "\u63d0\u51fa\u901a\u8fc7\u7fa4\u5e73\u5747\u9884\u8bad\u7ec3\u9884\u6d4b\u5668\u5c06\u51e0\u4f55\u4fe1\u606f\u6ce8\u5165\u5171\u5f62\u9884\u6d4b\uff0c\u5728\u5bf9\u79f0\u7fa4\u8f68\u9053\u4e0a\u5206\u5e03\u975e\u7b26\u5408\u8d28\u91cf\uff0c\u5c06\u6bcf\u4e2a\u6837\u672c\u89c6\u4e3a\u8f68\u9053\u4ee3\u8868\u4ee5\u7f13\u89e3\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u9012\u589e\u51f8\u5e8f\u4e0b\u53ef\u6536\u7f29\u975e\u7b26\u5408\u6027\u5f97\u5206\uff0c\u5e26\u6765\u6539\u8fdb\u7684\u6307\u6570\u5c3e\u754c\u548c\u66f4\u5c16\u9510\u7684\u9884\u6d4b\u96c6\uff0c\u5c24\u5176\u5728\u9ad8\u7f6e\u4fe1\u6c34\u5e73\u4e0b\u6548\u679c\u663e\u8457\u3002", "conclusion": "\u7fa4\u5bf9\u79f0\u5316\u80fd\u6709\u6548\u7f13\u89e3\u5171\u5f62\u9884\u6d4b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u533a\u57df\u589e\u957f\uff0c\u7406\u8bba\u7ed3\u679c\u901a\u8fc7\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u5b9e\u9a8c\u9a8c\u8bc1\u3002"}}
{"id": "2602.04496", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04496", "abs": "https://arxiv.org/abs/2602.04496", "authors": ["Zhentao Tang", "Yuqi Cui", "Shixiong Kai", "Wenqian Zhao", "Ke Ye", "Xing Li", "Anxin Tian", "Zehua Pei", "Hui-Ling Zhen", "Shoubo Hu", "Xiaoguang Li", "Yunhe Wang", "Mingxuan Yuan"], "title": "ReThinker: Scientific Reasoning by Rethinking with Guided Reflection and Confidence Control", "comment": null, "summary": "Expert-level scientific reasoning remains challenging for large language models, particularly on benchmarks such as Humanity's Last Exam (HLE), where rigid tool pipelines, brittle multi-agent coordination, and inefficient test-time scaling often limit performance. We introduce ReThinker, a confidence-aware agentic framework that orchestrates retrieval, tool use, and multi-agent reasoning through a stage-wise Solver-Critic-Selector architecture. Rather than following a fixed pipeline, ReThinker dynamically allocates computation based on model confidence, enabling adaptive tool invocation, guided multi-dimensional reflection, and robust confidence-weighted selection. To support scalable training without human annotation, we further propose a reverse data synthesis pipeline and an adaptive trajectory recycling strategy that transform successful reasoning traces into high-quality supervision. Experiments on HLE, GAIA, and XBench demonstrate that ReThinker consistently outperforms state-of-the-art foundation models with tools and existing deep research systems, achieving state-of-the-art results on expert-level reasoning tasks.", "AI": {"tldr": "ReThinker\u662f\u4e00\u4e2a\u7f6e\u4fe1\u5ea6\u611f\u77e5\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7Solver-Critic-Selector\u67b6\u6784\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff0c\u7ed3\u5408\u53cd\u5411\u6570\u636e\u5408\u6210\u548c\u81ea\u9002\u5e94\u8f68\u8ff9\u56de\u6536\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u5927\u6a21\u578b\u5728\u4e13\u5bb6\u7ea7\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u5927\u6a21\u578b\u5728\u4e13\u5bb6\u7ea7\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\uff08\u5982HLE\u57fa\u51c6\uff09\u4e2d\u5b58\u5728\u5de5\u5177\u6d41\u7a0b\u50f5\u5316\u3001\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u8106\u5f31\u3001\u6d4b\u8bd5\u65f6\u6269\u5c55\u6548\u7387\u4f4e\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faReThinker\u6846\u67b6\uff0c\u91c7\u7528\u9636\u6bb5\u5f0fSolver-Critic-Selector\u67b6\u6784\u5b9e\u73b0\u52a8\u6001\u8ba1\u7b97\u5206\u914d\uff1b\u901a\u8fc7\u53cd\u5411\u6570\u636e\u5408\u6210\u548c\u81ea\u9002\u5e94\u8f68\u8ff9\u56de\u6536\u7b56\u7565\u751f\u6210\u9ad8\u8d28\u91cf\u76d1\u7763\u6570\u636e\u3002", "result": "\u5728HLE\u3001GAIA\u548cXBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u6a21\u578b\u53ca\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\uff0c\u8fbe\u5230\u4e13\u5bb6\u7ea7\u63a8\u7406\u4efb\u52a1\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "ReThinker\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u611f\u77e5\u7684\u52a8\u6001\u8ba1\u7b97\u673a\u5236\u548c\u9ad8\u6548\u8bad\u7ec3\u7b56\u7565\uff0c\u4e3a\u590d\u6742\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.04508", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04508", "abs": "https://arxiv.org/abs/2602.04508", "authors": ["Javier Navarro", "Mateo Casariego", "Gabriel Molina-Terriza", "\u00cd\u00f1igo Luis Egusquiza", "Mikel Sanz"], "title": "Squeezing-Enhanced Rotational Doppler Metrology", "comment": null, "summary": "A rotating surface can induce a frequency shift in incident light by changing its angular momentum, a phenomenon known as the rotational Doppler effect. This effect provides a means to estimate the angular velocity of the rotating surface. In this work, we develop a continuous-variable quantum protocol for estimating the angular velocity of a rotating surface via the rotational Doppler effect. Our approach exploits squeezed and displaced Laguerre-Gaussian modes as quantum resources, which interact with a rotating metallic disc with surface roughness. The frequency shift induced by the rotational Doppler effect is then measured using a homodyne detection scheme. By analyzing the Fisher information, we demonstrate that the proposed squeezing-enhanced protocol achieves Heisenberg scaling in the ideal noiseless regime. Furthermore, we investigate the influence of noise and consider different surface models to assess their impact on the protocol's performance. While Heisenberg scaling is degraded in the presence of noise, we show that optimizing the energy allocation ratio between displacement and squeezing of the probe ensures that the quantum strategy consistently outperforms its classical counterpart.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u538b\u7f29\u5149\u901a\u8fc7\u65cb\u8f6c\u591a\u666e\u52d2\u6548\u5e94\u6d4b\u91cf\u65cb\u8f6c\u8868\u9762\u89d2\u901f\u5ea6\u7684\u91cf\u5b50\u534f\u8bae\uff0c\u5728\u7406\u60f3\u65e0\u566a\u58f0\u60c5\u51b5\u4e0b\u8fbe\u5230\u6d77\u68ee\u5821\u6781\u9650\uff0c\u5e76\u5728\u6709\u566a\u58f0\u65f6\u901a\u8fc7\u4f18\u5316\u80fd\u91cf\u5206\u914d\u4f18\u4e8e\u7ecf\u5178\u65b9\u6cd5\u3002", "motivation": "\u5229\u7528\u65cb\u8f6c\u591a\u666e\u52d2\u6548\u5e94\u6765\u4f30\u8ba1\u65cb\u8f6c\u8868\u9762\u7684\u89d2\u901f\u5ea6\uff0c\u5e76\u901a\u8fc7\u91cf\u5b50\u8d44\u6e90\u63d0\u9ad8\u6d4b\u91cf\u7cbe\u5ea6\u3002", "method": "\u91c7\u7528\u538b\u7f29\u548c\u4f4d\u79fb\u7684\u62c9\u76d6\u5c14-\u9ad8\u65af\u6a21\u5f0f\u4f5c\u4e3a\u91cf\u5b50\u8d44\u6e90\uff0c\u4e0e\u8868\u9762\u7c97\u7cd9\u7684\u65cb\u8f6c\u91d1\u5c5e\u76d8\u76f8\u4e92\u4f5c\u7528\uff0c\u5e76\u901a\u8fc7\u96f6\u5dee\u68c0\u6d4b\u65b9\u6848\u6d4b\u91cf\u9891\u7387\u504f\u79fb\u3002", "result": "\u5728\u65e0\u566a\u58f0\u7406\u60f3\u60c5\u51b5\u4e0b\uff0c\u534f\u8bae\u8fbe\u5230\u6d77\u68ee\u5821\u6781\u9650\uff1b\u5728\u6709\u566a\u58f0\u65f6\uff0c\u901a\u8fc7\u4f18\u5316\u4f4d\u79fb\u548c\u538b\u7f29\u4e4b\u95f4\u7684\u80fd\u91cf\u5206\u914d\u6bd4\u4f8b\uff0c\u91cf\u5b50\u7b56\u7565\u59cb\u7ec8\u4f18\u4e8e\u7ecf\u5178\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u91cf\u5b50\u534f\u8bae\u5728\u4f30\u8ba1\u89d2\u901f\u5ea6\u65b9\u9762\u4f18\u4e8e\u7ecf\u5178\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u4f18\u5316\u540e\u80fd\u6709\u6548\u5e94\u5bf9\u566a\u58f0\u5f71\u54cd\u3002"}}
{"id": "2602.03994", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03994", "abs": "https://arxiv.org/abs/2602.03994", "authors": ["Anish Sathyanarayanan", "Aditya Nagarsekar", "Aarush Rathore"], "title": "When Chains of Thought Don't Matter: Causal Bypass in Large Language Models", "comment": "Under Review at ICLR, 2026", "summary": "Chain-of-thought (CoT) prompting is widely assumed to expose a model's reasoning process and improve transparency. We attempted to enforce this assumption by penalizing unfaithful reasoning, but found that surface-level compliance does not guarantee causal reliance. Our central finding is negative: even when CoT is verbose, strategic, and flagged by surface-level manipulation detectors, model answers are often causally independent of the CoT content. We present a diagnostic framework for auditing this failure mode: it combines (i) an interpretable behavioral module that scores manipulation-relevant signals in CoT text and (ii) a causal probe that measures CoT-mediated influence (CMI) via hidden-state patching and reports a bypass score ($1-\\mathrm{CMI}$), quantifying the degree to which the answer is produced by a bypass circuit independent of the rationale. In pilot evaluations, audit-aware prompting increases detectable manipulation signals (mean risk-score delta: $+5.10$), yet causal probes reveal task-dependent mediation: many QA items exhibit near-total bypass (CMI $\\approx 0$), while some logic problems show stronger mediation (CMI up to $0.56$). Layer-wise analysis reveals narrow and task-dependent ``reasoning windows'' even when mean CMI is low.", "AI": {"tldr": "Chain-of-thought prompting fails to provide genuine reasoning transparency: model answers often don't causally depend on the shown reasoning steps, even when they appear verbally plausible.", "motivation": "Challenging the widespread assumption that CoT prompting reveals models' true reasoning process by testing whether surface-level reasoning compliance implies actual causal reliance.", "method": "Developed a diagnostic framework combining (1) an interpretable behavioral module detecting manipulation signals in CoT text, and (2) a causal probe using hidden-state patching to measure CoT-mediated influence (CMI) and calculate bypass scores.", "result": "Found pervasive \"bypass circuits\" where answers are independent of CoT content (CMI\u22480 in most QA tasks, up to 0.56 in some logic problems); audit-aware prompting increased surface signals (+5.10 risk-score) but didn't ensure causal mediation.", "conclusion": "CoT's transparency assumption is fundamentally flawed: verbose reasoning doesn't guarantee it's causally used, revealing a critical failure mode in interpretability methods that requires causal verification beyond surface analysis."}}
{"id": "2602.04572", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2602.04572", "abs": "https://arxiv.org/abs/2602.04572", "authors": ["Niv Fono", "Yftah Ziser", "Omer Ben-Porat"], "title": "From Competition to Collaboration: Designing Sustainable Mechanisms Between LLMs and Online Forums", "comment": null, "summary": "While Generative AI (GenAI) systems draw users away from (Q&A) forums, they also depend on the very data those forums produce to improve their performance. Addressing this paradox, we propose a framework of sequential interaction, in which a GenAI system proposes questions to a forum that can publish some of them. Our framework captures several intricacies of such a collaboration, including non-monetary exchanges, asymmetric information, and incentive misalignment. We bring the framework to life through comprehensive, data-driven simulations using real Stack Exchange data and commonly used LLMs. We demonstrate the incentive misalignment empirically, yet show that players can achieve roughly half of the utility in an ideal full-information scenario. Our results highlight the potential for sustainable collaboration that preserves effective knowledge sharing between AI systems and human knowledge platforms.", "AI": {"tldr": "\u63d0\u51fa\u751f\u6210\u5f0fAI\u4e0e\u95ee\u7b54\u8bba\u575b\u7684\u5e8f\u8d2f\u4e92\u52a8\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u6a21\u62df\u63ed\u793a\u6fc0\u52b1\u9519\u4f4d\u4e0b\u4ecd\u53ef\u5b9e\u73b0\u7ea650%\u7406\u60f3\u6548\u7528\u7684\u53ef\u6301\u7eed\u534f\u4f5c\u6f5c\u529b", "motivation": "\u89e3\u51b3\u751f\u6210\u5f0fAI\u7cfb\u7edf\u65e2\u5206\u6d41\u95ee\u7b54\u8bba\u575b\u7528\u6237\u53c8\u4f9d\u8d56\u8bba\u575b\u6570\u636e\u63d0\u5347\u6027\u80fd\u7684\u6096\u8bba\uff0c\u63a2\u7d22\u4eba\u673a\u77e5\u8bc6\u5171\u4eab\u7684\u53ef\u6301\u7eed\u534f\u4f5c\u6a21\u5f0f", "method": "\u6784\u5efa\u5e8f\u8d2f\u4e92\u52a8\u7406\u8bba\u6846\u67b6\uff0c\u7ed3\u5408\u771f\u5b9eStack Exchange\u6570\u636e\u548c\u5e38\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6570\u636e\u9a71\u52a8\u6a21\u62df\uff0c\u5b9e\u8bc1\u5206\u6790\u6fc0\u52b1\u9519\u4f4d\u73b0\u8c61", "result": "\u8bc1\u5b9e\u5b58\u5728\u663e\u8457\u6fc0\u52b1\u9519\u4f4d\uff0c\u4f46\u534f\u4f5c\u65b9\u53ef\u5b9e\u73b0\u63a5\u8fd1\u7406\u60f3\u5168\u4fe1\u606f\u573a\u666f\u4e0b50%\u5de6\u53f3\u7684\u6548\u7528\u6c34\u5e73", "conclusion": "\u6846\u67b6\u63ed\u793a\u4e86AI\u7cfb\u7edf\u4e0e\u4eba\u7c7b\u77e5\u8bc6\u5e73\u53f0\u901a\u8fc7\u975e\u8d27\u5e01\u5316\u534f\u4f5c\u5b9e\u73b0\u53ef\u6301\u7eed\u77e5\u8bc6\u5171\u4eab\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5e73\u8861\u53cc\u65b9\u5229\u76ca\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2602.04524", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04524", "abs": "https://arxiv.org/abs/2602.04524", "authors": ["Eric Tesse"], "title": "A simple means for deriving quantum mechanics", "comment": "21 pages", "summary": "A type of mechanics will be presented that possesses some distinctive properties. On the one hand, its physical description & rules of operation are readily comprehensible & intuitively clear. On the other, it fully satisfies all observable predictions of non-relativistic quantum mechanics. Within it, particles exist at points in space, follow continuous, piecewise differentiable paths, and their linear momentum is equal to their mass times their velocity along their path. Yet the probabilities for position and momentum, conditioned on the state of the particle's environment, follow the rules of quantum theory. Indeed, all observable consequences of quantum theory are satisfied; particles can be entangled, have intrinsic spin, this spin is not local to the particle, particle identity can effect probabilities, and so forth. All the rules of quantum mechanics are obeyed, and all arise in a straightforward fashion. After this is established, connections will be drawn out between this type of mechanics and other types of quantum worlds; those that obey Bohmian mechanics, stochastic mechanics, the many worlds interpretation, and physical collapse. In the final section, a relativistic version of the mechanics will be presented.", "AI": {"tldr": "The paper presents an intuitive hidden-variables theory where particles follow definite, continuous trajectories (momentum = mass \u00d7 velocity) while fully reproducing all predictions of non-relativistic quantum mechanics, including entanglement and spin.", "motivation": "To resolve the tension between quantum formalism and physical intuition by providing a comprehensible, realistic description where particles have definite positions yet remain empirically equivalent to standard quantum mechanics.", "method": "Constructing a mechanics with classical-like particle dynamics where quantum probabilities emerge from conditional probabilities based on environmental states, then comparing this framework to Bohmian mechanics, stochastic mechanics, many-worlds, and collapse theories, and developing a relativistic extension.", "result": "Successfully created a theory that satisfies all observable quantum phenomena (entanglement, spin, particle identity effects) using intuitive particle trajectories, established connections to major quantum interpretations, and formulated a relativistic version.", "conclusion": "An intuitive, realistic alternative to standard quantum mechanics exists that reproduces all empirical content while providing clearer physical interpretation, suggesting quantum weirdness may be interpretational rather than fundamental."}}
{"id": "2602.04538", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04538", "abs": "https://arxiv.org/abs/2602.04538", "authors": ["Ferdi Altintas"], "title": "Thermodynamic Cost of Regeneration in a Quantum Stirling Cycle", "comment": "Comments are welcome", "summary": "We study the regenerative quantum Stirling heat engine cycle within the standard weak-coupling, Markovian open quantum system framework. We point out that the regeneration process is not thermodynamically free in a reduced open-system description, and we treat the required work input as an explicit regeneration cost by modifying the cycle efficiency accordingly. We consider two working substances--a single spin-$1/2$ and a pair of interacting spin-$1/2$ particles--and investigate the cycle performance by taking the regeneration cost at its minimum value set by the Carnot heat-pump limit. For comparison, we also analyze the conventional Stirling cycle without regeneration under the same conditions. The super-Carnot efficiencies reported under the cost-free regeneration assumption disappear once the regeneration cost is included: the modified efficiency stays below the Carnot bound, while still remaining higher than the efficiency of the conventional Stirling cycle. For the conventional Stirling cycle, we provide a rigorous Carnot bound using quantum relative entropy, whereas for the regenerative cycle we derive a sufficient lower bound on the regeneration cost that guarantees thermodynamic consistency. Finally, we suggest three candidate quantum regenerator models for future work.", "AI": {"tldr": "\u4fee\u6b63\u91cf\u5b50\u65af\u7279\u6797\u5f15\u64ce\u7814\u7a76\u4e2d\u518d\u751f\u8fc7\u7a0b\"\u96f6\u6210\u672c\"\u7684\u9519\u8bef\u5047\u8bbe\uff0c\u8bc1\u660e\u8ba1\u5165\u518d\u751f\u529f\u6d88\u8017\u540e\u6548\u7387\u4f4e\u4e8e\u5361\u8bfa\u6781\u9650\u4f46\u4ecd\u4f18\u4e8e\u4f20\u7edf\u5faa\u73af\uff0c\u5e76\u5efa\u7acb\u70ed\u529b\u5b66\u4e00\u81f4\u6027\u8fb9\u754c", "motivation": "\u5148\u524d\u7814\u7a76\u5047\u8bbe\u91cf\u5b50\u70ed\u673a\u518d\u751f\u8fc7\u7a0b\u65e0\u70ed\u529b\u5b66\u6210\u672c\uff0c\u5bfc\u81f4\u62a5\u9053\u51fa\u8d85\u5361\u8bfa\u6548\u7387\uff08\u8fdd\u53cd\u70ed\u529b\u5b66\u7b2c\u4e8c\u5b9a\u5f8b\uff09\u3002\u672c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u8be5\u5047\u8bbe\u7684\u9519\u8bef\u6027\uff0c\u5c06\u518d\u751f\u529f\u8f93\u5165\u4f5c\u4e3a\u663e\u6027\u6210\u672c\u7eb3\u5165\u6548\u7387\u8ba1\u7b97\uff0c\u6062\u590d\u70ed\u529b\u5b66\u4e00\u81f4\u6027\u3002", "method": "\u5728\u5f31\u8026\u5408\u9a6c\u5c14\u53ef\u592b\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u6846\u67b6\u4e0b\uff0c\u5206\u6790\u5355\u81ea\u65cb1/2\u548c\u53cc\u76f8\u4e92\u4f5c\u7528\u81ea\u65cb1/2\u4e24\u79cd\u5de5\u8d28\uff1b\u901a\u8fc7\u4fee\u6b63\u5faa\u73af\u6548\u7387\u516c\u5f0f\uff08\u663e\u5f0f\u8ba1\u5165\u518d\u751f\u529f\u6210\u672c\uff09\uff0c\u5e76\u4ee5\u5361\u8bfa\u70ed\u6cf5\u6781\u9650\u8bbe\u5b9a\u518d\u751f\u6210\u672c\u6700\u5c0f\u503c\u3002\u5bf9\u6bd4\u542b\u518d\u751f\u5faa\u73af\u4e0e\u4f20\u7edf\u5faa\u73af\u6027\u80fd\u3002", "result": "1) \u8ba1\u5165\u518d\u751f\u6210\u672c\u540e\uff0c\u8d85\u5361\u8bfa\u6548\u7387\u6d88\u5931\uff0c\u4fee\u6b63\u6548\u7387\u4e25\u683c\u4f4e\u4e8e\u5361\u8bfa\u8fb9\u754c\uff0c\u4f46\u4ecd\u9ad8\u4e8e\u4f20\u7edf\u65af\u7279\u6797\u5faa\u73af\u6548\u7387\uff1b2) \u4e3a\u4f20\u7edf\u5faa\u73af\u63d0\u4f9b\u57fa\u4e8e\u91cf\u5b50\u76f8\u5bf9\u71b5\u7684\u4e25\u683c\u5361\u8bfa\u8fb9\u754c\uff1b3) \u4e3a\u518d\u751f\u5faa\u73af\u63a8\u5bfc\u4fdd\u8bc1\u70ed\u529b\u5b66\u4e00\u81f4\u6027\u7684\u518d\u751f\u6210\u672c\u4e0b\u9650\u3002", "conclusion": "\u518d\u751f\u8fc7\u7a0b\u5728\u5f00\u653e\u7cfb\u7edf\u63cf\u8ff0\u4e2d\u5e76\u975e\u70ed\u529b\u5b66\u514d\u8d39\uff0c\u5fc5\u987b\u8ba1\u5165\u529f\u8f93\u5165\u6210\u672c\uff1b\u4fee\u6b63\u540e\u7684\u91cf\u5b50\u65af\u7279\u6797\u5faa\u73af\u5728\u4fdd\u6301\u70ed\u529b\u5b66\u4e00\u81f4\u6027\u7684\u524d\u63d0\u4e0b\u4ecd\u5177\u6027\u80fd\u4f18\u52bf\u3002\u63d0\u51fa\u4e09\u79cd\u91cf\u5b50\u518d\u751f\u5668\u5019\u9009\u6a21\u578b\u4f5c\u4e3a\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2602.04009", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.04009", "abs": "https://arxiv.org/abs/2602.04009", "authors": ["Mehdi Lotfian", "Mohammad Jalali", "Farzan Farnia"], "title": "PromptSplit: Revealing Prompt-Level Disagreement in Generative Models", "comment": null, "summary": "Prompt-guided generative AI models have rapidly expanded across vision and language domains, producing realistic and diverse outputs from textual inputs. The growing variety of such models, trained with different data and architectures, calls for principled methods to identify which types of prompts lead to distinct model behaviors. In this work, we propose PromptSplit, a kernel-based framework for detecting and analyzing prompt-dependent disagreement between generative models. For each compared model pair, PromptSplit constructs a joint prompt--output representation by forming tensor-product embeddings of the prompt and image (or text) features, and then computes the corresponding kernel covariance matrix. We utilize the eigenspace of the weighted difference between these matrices to identify the main directions of behavioral difference across prompts. To ensure scalability, we employ a random-projection approximation that reduces computational complexity to $O(nr^2 + r^3)$ for projection dimension $r$. We further provide a theoretical analysis showing that this approximation yields an eigenstructure estimate whose expected deviation from the full-dimensional result is bounded by $O(1/r^2)$. Experiments across text-to-image, text-to-text, and image-captioning settings demonstrate that PromptSplit accurately detects ground-truth behavioral differences and isolates the prompts responsible, offering an interpretable tool for detecting where generative models disagree.", "AI": {"tldr": "\u63d0\u51faPromptSplit\uff0c\u4e00\u79cd\u57fa\u4e8e\u6838\u65b9\u6cd5\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u8054\u5408\u63d0\u793a-\u8f93\u51fa\u8868\u793a\u5e76\u4f7f\u7528\u968f\u673a\u6295\u5f71\u5b9e\u73b0\u53ef\u6269\u5c55\u6027\uff0c\u6765\u68c0\u6d4b\u751f\u6210\u5f0f\u6a21\u578b\u95f4\u7684\u63d0\u793a\u4f9d\u8d56\u6027\u5dee\u5f02\u3002", "motivation": "\u968f\u7740\u63d0\u793a\u5f15\u5bfc\u7684\u751f\u6210\u5f0fAI\u6a21\u578b\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u9886\u57df\u7684\u5feb\u901f\u666e\u53ca\uff0c\u4ee5\u53ca\u4f7f\u7528\u4e0d\u540c\u6570\u636e\u548c\u67b6\u6784\u8bad\u7ec3\u7684\u6a21\u578b\u79cd\u7c7b\u4e0d\u65ad\u589e\u52a0\uff0c\u4e9f\u9700\u539f\u5219\u6027\u65b9\u6cd5\u6765\u8bc6\u522b\u54ea\u4e9b\u7c7b\u578b\u7684\u63d0\u793a\u4f1a\u5bfc\u81f4\u4e0d\u540c\u7684\u6a21\u578b\u884c\u4e3a\u3002", "method": "\u6784\u5efa\u63d0\u793a\u4e0e\u56fe\u50cf/\u6587\u672c\u7279\u5f81\u7684\u5f20\u91cf\u79ef\u5d4c\u5165\u4f5c\u4e3a\u8054\u5408\u8868\u793a\uff0c\u8ba1\u7b97\u5bf9\u5e94\u7684\u6838\u534f\u65b9\u5dee\u77e9\u9635\uff0c\u5229\u7528\u4e24\u77e9\u9635\u52a0\u6743\u5dee\u5f02\u7684\u7279\u5f81\u7a7a\u95f4\u8bc6\u522b\u884c\u4e3a\u5dee\u5f02\u7684\u4e3b\u8981\u65b9\u5411\uff0c\u5e76\u91c7\u7528\u968f\u673a\u6295\u5f71\u8fd1\u4f3c\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u964d\u81f3O(nr\u00b2 + r\u00b3)\u3002", "result": "\u5728\u6587\u672c\u5230\u56fe\u50cf\u3001\u6587\u672c\u5230\u6587\u672c\u548c\u56fe\u50cf\u5b57\u5e55\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPromptSplit\u80fd\u51c6\u786e\u68c0\u6d4b\u771f\u5b9e\u7684\u884c\u4e3a\u5dee\u5f02\u5e76\u5206\u79bb\u51fa\u5bfc\u81f4\u5dee\u5f02\u7684\u5173\u952e\u63d0\u793a\u3002", "conclusion": "PromptSplit\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u68c0\u6d4b\u751f\u6210\u5f0f\u6a21\u578b\u5728\u54ea\u4e9b\u65b9\u9762\u5b58\u5728\u5206\u6b67\uff0c\u901a\u8fc7\u63d0\u793a\u5206\u6790\u5e2e\u52a9\u7406\u89e3\u6a21\u578b\u884c\u4e3a\u7684\u5dee\u5f02\u3002"}}
{"id": "2602.04634", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.04634", "abs": "https://arxiv.org/abs/2602.04634", "authors": ["Zelai Xu", "Zhexuan Xu", "Ruize Zhang", "Chunyang Zhu", "Shi Yu", "Weilin Liu", "Quanlu Zhang", "Wenbo Ding", "Chao Yu", "Yu Wang"], "title": "WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning", "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.", "AI": {"tldr": "\u63d0\u51faWideSeek-R1\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5bbd\u5ea6\u6269\u5c55\u5b9e\u73b0\u5e76\u884c\u4fe1\u606f\u641c\u7d22\uff0c4B\u6a21\u578b\u6027\u80fd\u5ab2\u7f8e671B\u5355\u667a\u80fd\u4f53\u6a21\u578b", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u805a\u7126\u6df1\u5ea6\u6269\u5c55\uff08\u5355\u667a\u80fd\u4f53\u591a\u8f6e\u63a8\u7406\uff09\uff0c\u4f46\u9762\u5bf9\u5e7f\u6cdb\u4efb\u52a1\u65f6\uff0c\u7ec4\u7ec7\u80fd\u529b\u6210\u4e3a\u74f6\u9888\uff1b\u4f20\u7edf\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4f9d\u8d56\u624b\u5de5\u5de5\u4f5c\u6d41\u548c\u8f6e\u6d41\u4ea4\u4e92\uff0c\u65e0\u6cd5\u6709\u6548\u5e76\u884c\u5316\u5de5\u4f5c", "method": "\u8bbe\u8ba1lead-agent-subagent\u6846\u67b6\uff0c\u91c7\u7528\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u5171\u4eabLLM\u4f46\u4e0a\u4e0b\u6587\u9694\u79bb\uff0c\u914d\u5907\u4e13\u7528\u5de5\u5177\uff0c\u57282\u4e07\u6761\u5e7f\u6cdb\u4fe1\u606f\u641c\u7d22\u4efb\u52a1\u4e0a\u8054\u5408\u4f18\u5316\u4e3b\u667a\u80fd\u4f53\u548c\u5e76\u884c\u5b50\u667a\u80fd\u4f53", "result": "WideSeek-R1-4B\u5728WideSearch\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523040.0%\u7684item F1\u5206\u6570\uff0c\u6027\u80fd\u4e0e\u5355\u667a\u80fd\u4f53DeepSeek-R1-671B\u76f8\u5f53\uff1b\u589e\u52a0\u5e76\u884c\u5b50\u667a\u80fd\u4f53\u6570\u91cf\u65f6\u6027\u80fd\u6301\u7eed\u63d0\u5347", "conclusion": "\u5bbd\u5ea6\u6269\u5c55\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u80fd\u6709\u6548\u89e3\u51b3\u5e7f\u6cdb\u4fe1\u606f\u641c\u7d22\u4efb\u52a1\uff0c\u7a81\u7834\u7ec4\u7ec7\u80fd\u529b\u74f6\u9888\uff0c\u4e3aLLM\u6269\u5c55\u63d0\u4f9b\u65b0\u7ef4\u5ea6"}}
{"id": "2602.04543", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04543", "abs": "https://arxiv.org/abs/2602.04543", "authors": ["Daria Gaidukevich"], "title": "Effect of initial intrasystem entanglement on entropy growth in generalized Jaynes-Cummings models", "comment": "9 pages, 5 figures", "summary": "We investigate how initial intrasystem entanglement influences the entropy generated in atomic systems interacting with a photonic environment in several generalizations of the Jaynes-Cummings model with two or more subsystems. Since the initial entanglement does not uniquely determine the final entropy, we focus on ensemble-averaged behavior. We consider ensembles of initial system states including pure and mixed Haar-random states, ensembles with fixed average energy or fixed mixedness, and varying initial photon numbers in the environment. In all cases, we observe a positive correlation between the initial entanglement and the entropy growth, although the fractional contribution of the initial entanglement varies. Our results emphasize the role of intrasystem correlations as a factor contributing to entropy growth in quantum informational processes.", "AI": {"tldr": "Initial intrasystem entanglement positively correlates with entropy growth in multi-subsystem Jaynes-Cummings models across various initial state ensembles, though its fractional contribution varies.", "motivation": "Initial entanglement alone does not uniquely determine final entropy in quantum systems; understanding ensemble-averaged behavior is critical for revealing how intrasystem correlations influence entropy generation in quantum information processes.", "method": "Analyzed generalized Jaynes-Cummings models with multiple atomic subsystems interacting with a photonic environment. Studied ensembles of initial states including pure/mixed Haar-random states, ensembles with fixed average energy or mixedness, and varying initial photon numbers.", "result": "Observed a consistent positive correlation between initial intrasystem entanglement and entropy growth across all ensemble types, but the fractional contribution of initial entanglement to total entropy varied depending on the ensemble characteristics.", "conclusion": "Intrasystem correlations play a significant role as a contributing factor to entropy growth in quantum informational processes, highlighting their importance beyond individual state properties."}}
{"id": "2602.04019", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04019", "abs": "https://arxiv.org/abs/2602.04019", "authors": ["Yichen Xu", "Yuyang Liang", "Shan Dai", "Tianyang Hu", "Tsz Nam Chan", "Chenhao Ma"], "title": "Understanding and Guiding Layer Placement in Parameter-Efficient Fine-Tuning of Large Language Models", "comment": null, "summary": "As large language models (LLMs) continue to grow, the cost of full-parameter fine-tuning has made parameter-efficient fine-tuning (PEFT) the default strategy for downstream adaptation. Constraints from inference latency in scalable serving and fine-tuning cost in edge or rapid-deployment settings make the choice of which layers to fine-tune unavoidable. Yet current practice typically applies PEFT uniformly across all layers, with limited understanding or leverage of layer selection. This paper develops a unified projected residual view of PEFT on top of a frozen base model. Under a local quadratic approximation, layerwise adaptation is governed by three quantities: (i) the projected residual norm (resnorm), which measures how much correctable bias a layer can capture; (ii) the activation energy, which determines feature conditioning; and (iii) layer coupling, which quantifies how strongly residuals interact across layers. We show that, for squared loss and linear adapters, the resnorm equals a normalized gradient norm, activation energy controls ill-conditioning and noise amplification, and weak coupling yields approximately additive layerwise contributions. Building on these insights, we introduce the Layer Card, a reusable diagnostic that summarizes residual signal strength, compute cost, and performance for each layer of a given model. With an identical model and LoRA configuration, Layer Card-guided placement refines the choice of adapted layers to flexibly prioritize different objectives, such as maximizing performance or reducing fine-tuning cost. Moreover, on Qwen3-8B, we show that selectively adapting a subset of layers can achieve performance close to full-layer LoRA while substantially reducing fine-tuning cost and the number of adapter-augmented layers during inference, offering a more cost-performance-aware alternative to full-layer insertion.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6295\u5f71\u6b8b\u5dee\u89c6\u89d2\u6765\u7406\u89e3\u53c2\u6570\u9ad8\u6548\u5fae\u8c03(PEFT)\uff0c\u5e76\u5f15\u5165Layer Card\u8bca\u65ad\u5de5\u5177\uff0c\u901a\u8fc7\u5206\u6790\u5404\u5c42\u7684\u6b8b\u5dee\u4fe1\u53f7\u5f3a\u5ea6\u3001\u8ba1\u7b97\u6210\u672c\u548c\u6027\u80fd\uff0c\u5b9e\u73b0\u9009\u62e9\u6027\u5c42\u5fae\u8c03\uff0c\u5728\u4fdd\u6301\u63a5\u8fd1\u5168\u5c42LoRA\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5fae\u8c03\u6210\u672c\u548c\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u589e\u957f\uff0c\u5168\u53c2\u6570\u5fae\u8c03\u6210\u672c\u8fc7\u9ad8\uff0cPEFT\u6210\u4e3a\u9ed8\u8ba4\u7b56\u7565\u3002\u7136\u800c\u5f53\u524d\u5b9e\u8df5\u901a\u5e38\u5728\u6240\u6709\u5c42\u4e0a\u7edf\u4e00\u5e94\u7528PEFT\uff0c\u7f3a\u4e4f\u5bf9\u5c42\u9009\u62e9\u7b56\u7565\u7684\u6df1\u5165\u7406\u89e3\u3002\u63a8\u7406\u5ef6\u8fdf\u548c\u8fb9\u7f18\u90e8\u7f72\u6210\u672c\u7ea6\u675f\u4f7f\u5f97\u5c42\u9009\u62e9\u4e0d\u53ef\u907f\u514d\uff0c\u4f46\u5982\u4f55\u79d1\u5b66\u5730\u9009\u62e9\u9002\u914d\u5c42\u4ecd\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "1. \u5efa\u7acb\u57fa\u4e8e\u51bb\u7ed3\u57fa\u7840\u6a21\u578b\u7684PEFT\u7edf\u4e00\u6295\u5f71\u6b8b\u5dee\u7406\u8bba\u6846\u67b6\uff1b2. \u5728\u5c40\u90e8\u4e8c\u6b21\u8fd1\u4f3c\u4e0b\uff0c\u63d0\u51fa\u5c42\u81ea\u9002\u5e94\u7684\u4e09\u4e2a\u5173\u952e\u91cf\uff1a\u6295\u5f71\u6b8b\u5dee\u8303\u6570(\u8861\u91cf\u5c42\u53ef\u6355\u83b7\u7684\u53ef\u7ea0\u6b63\u504f\u5dee)\u3001\u6fc0\u6d3b\u80fd\u91cf(\u51b3\u5b9a\u7279\u5f81\u6761\u4ef6)\u548c\u5c42\u8026\u5408(\u91cf\u5316\u5c42\u95f4\u6b8b\u5dee\u4ea4\u4e92\u5f3a\u5ea6)\uff1b3. \u8bc1\u660e\u5bf9\u4e8e\u5e73\u65b9\u635f\u5931\u548c\u7ebf\u6027\u9002\u914d\u5668\uff0c\u6b8b\u5dee\u8303\u6570\u7b49\u4e8e\u5f52\u4e00\u5316\u68af\u5ea6\u8303\u6570\uff0c\u6fc0\u6d3b\u80fd\u91cf\u63a7\u5236\u75c5\u6001\u6761\u4ef6\u4e0e\u566a\u58f0\u653e\u5927\uff0c\u5f31\u8026\u5408\u4ea7\u751f\u8fd1\u4f3c\u53ef\u52a0\u7684\u5c42\u8d21\u732e\uff1b4. \u5f15\u5165Layer Card\u53ef\u590d\u7528\u8bca\u65ad\u5de5\u5177\uff0c\u6c47\u603b\u6bcf\u5c42\u7684\u6b8b\u5dee\u4fe1\u53f7\u5f3a\u5ea6\u3001\u8ba1\u7b97\u6210\u672c\u548c\u6027\u80fd\u3002", "result": "\u5728Qwen3-8B\u6a21\u578b\u4e0a\uff0c\u57fa\u4e8eLayer Card\u6307\u5bfc\u7684\u9009\u62e9\u6027\u5c42\u5fae\u8c03\u5b9e\u73b0\u4e86\uff1a1) \u6027\u80fd\u63a5\u8fd1\u5168\u5c42LoRA\uff1b2) \u663e\u8457\u964d\u4f4e\u5fae\u8c03\u6210\u672c\uff1b3) \u51cf\u5c11\u63a8\u7406\u65f6\u9002\u914d\u5668\u589e\u5f3a\u5c42\u6570\u91cf\u3002\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6027\u80fd\u4e0e\u6210\u672c\u611f\u77e5\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u7528\u5de5\u5177Layer Card\uff0c\u5efa\u7acb\u4e86PEFT\u5c42\u9009\u62e9\u7684\u79d1\u5b66\u6846\u67b6\uff0c\u4f7f\u4ece\u4e1a\u8005\u80fd\u591f\u6839\u636e\u6027\u80fd\u6216\u6210\u672c\u76ee\u6807\u7075\u6d3b\u9009\u62e9\u9002\u914d\u5c42\uff0c\u5b9e\u73b0\u4e86\u6210\u672c-\u6027\u80fd\u6743\u8861\u7684\u7cbe\u7ec6\u63a7\u5236\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u6a21\u578b\u5fae\u8c03\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.04813", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.04813", "abs": "https://arxiv.org/abs/2602.04813", "authors": ["Shubham Vatsal", "Harsh Dubey", "Aditi Singh"], "title": "Agentic AI in Healthcare & Medicine: A Seven-Dimensional Taxonomy for Empirical Evaluation of LLM-based Agents", "comment": null, "summary": "Large Language Model (LLM)-based agents that plan, use tools and act has begun to shape healthcare and medicine. Reported studies demonstrate competence on various tasks ranging from EHR analysis and differential diagnosis to treatment planning and research workflows. Yet the literature largely consists of overviews which are either broad surveys or narrow dives into a single capability (e.g., memory, planning, reasoning), leaving healthcare work without a common frame. We address this by reviewing 49 studies using a seven-dimensional taxonomy: Cognitive Capabilities, Knowledge Management, Interaction Patterns, Adaptation & Learning, Safety & Ethics, Framework Typology and Core Tasks & Subtasks with 29 operational sub-dimensions. Using explicit inclusion and exclusion criteria and a labeling rubric (Fully Implemented, Partially Implemented, Not Implemented), we map each study to the taxonomy and report quantitative summaries of capability prevalence and co-occurrence patterns. Our empirical analysis surfaces clear asymmetries. For instance, the External Knowledge Integration sub-dimension under Knowledge Management is commonly realized (~76% Fully Implemented) whereas Event-Triggered Activation sub-dimenison under Interaction Patterns is largely absent (~92% Not Implemented) and Drift Detection & Mitigation sub-dimension under Adaptation & Learning is rare (~98% Not Implemented). Architecturally, Multi-Agent Design sub-dimension under Framework Typology is the dominant pattern (~82% Fully Implemented) while orchestration layers remain mostly partial. Across Core Tasks & Subtasks, information centric capabilities lead e.g., Medical Question Answering & Decision Support and Benchmarking & Simulation, while action and discovery oriented areas such as Treatment Planning & Prescription still show substantial gaps (~59% Not Implemented).", "AI": {"tldr": "This systematic review of 49 LLM-based healthcare agent studies using a 7-dimensional taxonomy reveals significant capability gaps: external knowledge integration is common (76% fully implemented), but event-triggered activation (92% absent), drift detection (98% rare), and treatment planning (59% not implemented) remain underdeveloped, showing uneven field maturity.", "motivation": "Existing literature on LLM healthcare agents lacks a unified framework, consisting of either broad surveys or narrow single-capability analyses, preventing a comprehensive understanding of the field's overall capability landscape.", "method": "Researchers conducted a systematic review of 49 studies using a seven-dimensional taxonomy (Cognitive Capabilities, Knowledge Management, Interaction Patterns, Adaptation & Learning, Safety & Ethics, Framework Typology, Core Tasks & Subtasks) with 29 sub-dimensions, applying explicit inclusion/exclusion criteria and mapping each study using a three-level rubric (Fully/Partially/Not Implemented).", "result": "Quantitative analysis revealed stark asymmetries: Knowledge Management's External Knowledge Integration is prevalent (76% fully implemented), while Interaction Patterns' Event-Triggered Activation (92% not implemented) and Adaptation & Learning's Drift Detection & Mitigation (98% not implemented) are rare. Architecturally, Multi-Agent Design dominates (82% fully implemented). Task-wise, information-centric capabilities lead, but action-oriented Treatment Planning & Prescription shows major gaps (59% not implemented).", "conclusion": "The field of LLM-based healthcare agents demonstrates uneven development with strong information processing but weak autonomous action and adaptive capabilities, indicating a maturing but incomplete technology landscape requiring enhanced focus on safety, clinical workflow integration, and continuous learning mechanisms."}}
{"id": "2602.04550", "categories": ["quant-ph", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.04550", "abs": "https://arxiv.org/abs/2602.04550", "authors": ["Cristina Butucea", "Jan Johannes", "Henning Stein"], "title": "Locally Gentle State Certification for High Dimensional Quantum Systems", "comment": null, "summary": "Standard approaches to quantum statistical inference rely on measurements that induce a collapse of the wave function, effectively consuming the quantum state to extract information. In this work, we investigate the fundamental limits of \\emph{locally-gentle} quantum state certification, where the learning algorithm is constrained to perturb the state by at most $\u03b1$ in trace norm, thereby allowing for the reuse of samples. We analyze the hypothesis testing problem of distinguishing whether an unknown state $\u03c1$ is equal to a reference $\u03c1_0$ or $\u03b5$-far from it. We derive the minimax sample complexity for this problem, quantifying the information-theoretic price of non-destructive measurements. Specifically, by constructing explicit measurement operators, we show that the constraint of $\u03b1$-gentleness imposes a sample size penalty of $\\frac{d}{\u03b1^2}$, yielding a total sample complexity of $n = \u0398(\\frac{d^3}{\u03b5^2 \u03b1^2})$. Our results clarify the trade-off between information extraction and state disturbance, and highlight deep connections between physical measurement constraints and privacy mechanisms in quantum learning. Crucially, we find that the sample size penalty incurred by enforcing $\u03b1$-gentleness scales linearly with the Hilbert-space dimension $d$ rather than the number of parameters $d^2-1$ typical for high-dimensional private estimation.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u975e\u7834\u574f\u6027\u7684\"\u6e29\u548c\"\u91cf\u5b50\u6d4b\u91cf\uff0c\u8bc1\u660e\u5728\u533a\u5206\u91cf\u5b50\u6001\u65f6\uff0c\u6b64\u7c7b\u6d4b\u91cf\u9700\u8981\u0398(d\u00b3/(\u03b5\u00b2\u03b1\u00b2))\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u63ed\u793a\u4e86\u6e29\u548c\u6027\u5e26\u6765\u7684\u7ef4\u5ea6\u76f8\u5173\u60e9\u7f5a\u3002", "motivation": "\u4f20\u7edf\u91cf\u5b50\u6d4b\u91cf\u4f1a\u574d\u7f29\u6ce2\u51fd\u6570\u5e76\u7834\u574f\u91cf\u5b50\u6001\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u53ef\u91cd\u590d\u4f7f\u7528\u91cf\u5b50\u6001\u7684\"\u6e29\u548c\"\u6d4b\u91cf\u7684\u57fa\u672c\u6781\u9650\uff0c\u8fd9\u5bf9\u9ad8\u6548\u91cf\u5b50\u5b66\u4e60\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u4e0e\u91cf\u5b50\u9690\u79c1\u673a\u5236\u6709\u6df1\u523b\u8054\u7cfb\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5047\u8bbe\u68c0\u9a8c\u95ee\u9898\u2014\u2014\u5728\u6d4b\u91cf\u6270\u52a8\u4e0d\u8d85\u8fc7\u03b1\uff08\u8ff9\u8303\u6570\uff09\u7684\u7ea6\u675f\u4e0b\uff0c\u5224\u65ad\u672a\u77e5\u91cf\u5b50\u6001\u03c1\u662f\u5426\u7b49\u4e8e\u53c2\u8003\u6001\u03c1\u2080\u6216\u4e0e\u4e4b\u03b5-\u8fdc\u79bb\u2014\u2014\u5e76\u6784\u9020\u663e\u5f0f\u6d4b\u91cf\u7b97\u5b50\uff0c\u63a8\u5bfc\u51fa\u6781\u5c0f\u6781\u5927\u6837\u672c\u590d\u6742\u5ea6\u3002", "result": "\u6837\u672c\u590d\u6742\u5ea6\u4e3an = \u0398(d\u00b3/(\u03b5\u00b2\u03b1\u00b2))\u3002\u03b1-\u6e29\u548c\u6027\u7ea6\u675f\u76f8\u6bd4\u6807\u51c6\u91cf\u5b50\u6001\u8ba4\u8bc1\u5f15\u5165\u4e86d/\u03b1\u00b2\u7684\u60e9\u7f5a\u56e0\u5b50\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8be5\u60e9\u7f5a\u968fHilbert\u7a7a\u95f4\u7ef4\u5ea6d\u7ebf\u6027\u589e\u957f\uff0c\u800c\u975e\u9ad8\u7ef4\u79c1\u6709\u4f30\u8ba1\u4e2d\u5178\u578b\u7684\u53c2\u6570\u4e2a\u6570d\u00b2-1\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u9610\u660e\u4e86\u91cf\u5b50\u5b66\u4e60\u4e2d\u4fe1\u606f\u63d0\u53d6\u4e0e\u6001\u6270\u52a8\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u63ed\u793a\u4e86\u7269\u7406\u6d4b\u91cf\u7ea6\u675f\u4e0e\u9690\u79c1\u673a\u5236\u7684\u6df1\u523b\u8054\u7cfb\uff0c\u8868\u660e\u6e29\u548c\u6d4b\u91cf\u4f1a\u5e26\u6765\u4e0e\u7ef4\u5ea6d\u76f8\u5173\u7684\u6837\u672c\u6210\u672c\uff0c\u5bf9\u91cf\u5b50\u9690\u79c1\u548c\u9ad8\u6548\u91cf\u5b50\u7edf\u8ba1\u63a8\u65ad\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.04021", "categories": ["cs.LG", "q-bio.QM", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.04021", "abs": "https://arxiv.org/abs/2602.04021", "authors": ["Aditya Gorla", "Hugues Van Assel", "Jan-Christian Huetter", "Heming Yao", "Kyunghyun Cho", "Aviv Regev", "Russell Littman"], "title": "Group Contrastive Learning for Weakly Paired Multimodal Data", "comment": null, "summary": "We present GROOVE, a semi-supervised multi-modal representation learning approach for high-content perturbation data where samples across modalities are weakly paired through shared perturbation labels but lack direct correspondence. Our primary contribution is GroupCLIP, a novel group-level contrastive loss that bridges the gap between CLIP for paired cross-modal data and SupCon for uni-modal supervised contrastive learning, addressing a fundamental gap in contrastive learning for weakly-paired settings. We integrate GroupCLIP with an on-the-fly backtranslating autoencoder framework to encourage cross-modally entangled representations while maintaining group-level coherence within a shared latent space. Critically, we introduce a comprehensive combinatorial evaluation framework that systematically assesses representation learners across multiple optimal transport aligners, addressing key limitations in existing evaluation strategies. This framework includes novel simulations that systematically vary shared versus modality-specific perturbation effects enabling principled assessment of method robustness. Our combinatorial benchmarking reveals that there is not yet an aligner that uniformly dominates across settings or modality pairs. Across simulations and two real single-cell genetic perturbation datasets, GROOVE performs on par with or outperforms existing approaches for downstream cross-modal matching and imputation tasks. Our ablation studies demonstrate that GroupCLIP is the key component driving performance gains. These results highlight the importance of leveraging group-level constraints for effective multi-modal representation learning in scenarios where only weak pairing is available.", "AI": {"tldr": "GROOVE\u662f\u4e00\u79cd\u9488\u5bf9\u5f31\u914d\u5bf9\u591a\u6a21\u6001\u6270\u52a8\u6570\u636e\u7684\u534a\u76d1\u7763\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u6838\u5fc3\u662fGroupCLIP\u5206\u7ec4\u5bf9\u6bd4\u635f\u5931\uff0c\u7ed3\u5408\u5b9e\u65f6\u56de\u8bd1\u81ea\u7f16\u7801\u5668\u6846\u67b6\u548c\u7ec4\u5408\u8bc4\u4f30\u4f53\u7cfb\uff0c\u5728\u8de8\u6a21\u6001\u5339\u914d\u548c\u8865\u5168\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u5bf9\u6bd4\u5b66\u4e60\u5728\u5f31\u914d\u5bf9\u573a\u666f\u5b58\u5728\u6839\u672c\u6027\u7a7a\u767d\uff1aCLIP\u9700\u8981\u7cbe\u786e\u914d\u5bf9\u6570\u636e\uff0cSupCon\u4ec5\u652f\u6301\u5355\u6a21\u6001\u76d1\u7763\uff0c\u800c\u9ad8\u542b\u91cf\u6270\u52a8\u6570\u636e\u4e2d\u4e0d\u540c\u6a21\u6001\u6837\u672c\u4ec5\u901a\u8fc7\u6270\u52a8\u6807\u7b7e\u5f31\u5173\u8054\uff0c\u7f3a\u4e4f\u76f4\u63a5\u5bf9\u5e94\u5173\u7cfb\uff0c\u4e9f\u9700\u65b0\u65b9\u6cd5\u6865\u63a5\u8fd9\u4e00\u9e3f\u6c9f\u3002", "method": "\u63d0\u51faGROOVE\u6846\u67b6\uff0c\u521b\u65b0\u6027\u5730\u8bbe\u8ba1GroupCLIP\u5206\u7ec4\u7ea7\u5bf9\u6bd4\u635f\u5931\uff0c\u8fde\u63a5CLIP\u4e0eSupCon\uff1b\u96c6\u6210\u5b9e\u65f6\u56de\u8bd1\u81ea\u7f16\u7801\u5668\u4fc3\u8fdb\u8de8\u6a21\u6001\u7ea0\u7f20\u8868\u793a\uff1b\u6784\u5efa\u7ec4\u5408\u8bc4\u4f30\u6846\u67b6\u7cfb\u7edf\u68c0\u9a8c\u591a\u79cd\u6700\u4f18\u4f20\u8f93\u5bf9\u9f50\u5668\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u7cfb\u7edf\u6539\u53d8\u5171\u4eab\u4e0e\u6a21\u6001\u7279\u5f02\u6027\u6270\u52a8\u6548\u5e94\u3002", "result": "\u5728\u6a21\u62df\u6570\u636e\u548c\u4e24\u4e2a\u771f\u5b9e\u5355\u7ec6\u80de\u9057\u4f20\u6270\u52a8\u6570\u636e\u96c6\u4e0a\uff0cGROOVE\u5728\u8de8\u6a21\u6001\u5339\u914d\u548c\u8865\u5168\u4efb\u52a1\u4e0a\u8fbe\u5230\u6216\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff1b\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9eGroupCLIP\u662f\u5173\u952e\u9a71\u52a8\u529b\uff1b\u8bc4\u4f30\u663e\u793a\u5c1a\u65e0\u5bf9\u9f50\u5668\u5728\u6240\u6709\u573a\u666f\u4e0b\u7edf\u4e00\u5360\u4f18\u3002", "conclusion": "\u5728\u4ec5\u6709\u5f31\u914d\u5bf9\u53ef\u7528\u7684\u573a\u666f\u4e0b\uff0c\u5229\u7528\u5206\u7ec4\u7ea7\u7ea6\u675f\u5bf9\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u81f3\u5173\u91cd\u8981\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u586b\u8865\u4e86\u5bf9\u6bd4\u5b66\u4e60\u5728\u5f31\u914d\u5bf9\u8bbe\u7f6e\u4e2d\u7684\u57fa\u7840\u7a7a\u767d\u3002"}}
{"id": "2602.04836", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04836", "abs": "https://arxiv.org/abs/2602.04836", "authors": ["Haosen Ge", "Hamsa Bastani", "Osbert Bastani"], "title": "Are AI Capabilities Increasing Exponentially? A Competing Hypothesis", "comment": null, "summary": "Rapidly increasing AI capabilities have substantial real-world consequences, ranging from AI safety concerns to labor market consequences. The Model Evaluation & Threat Research (METR) report argues that AI capabilities have exhibited exponential growth since 2019. In this note, we argue that the data does not support exponential growth, even in shorter-term horizons. Whereas the METR study claims that fitting sigmoid/logistic curves results in inflection points far in the future, we fit a sigmoid curve to their current data and find that the inflection point has already passed. In addition, we propose a more complex model that decomposes AI capabilities into base and reasoning capabilities, exhibiting individual rates of improvement. We prove that this model supports our hypothesis that AI capabilities will exhibit an inflection point in the near future. Our goal is not to establish a rigorous forecast of our own, but to highlight the fragility of existing forecasts of exponential growth.", "AI": {"tldr": "\u8be5\u7814\u7a76\u53cd\u9a73METR\u62a5\u544a\u5173\u4e8eAI\u80fd\u529b\u6307\u6570\u7ea7\u589e\u957f\u7684\u4e3b\u5f20\uff0c\u901a\u8fc7\u91cd\u65b0\u5206\u6790\u6570\u636e\u53d1\u73b0\u903b\u8f91\u66f2\u7ebf\u62d0\u70b9\u5df2\u8fc7\uff0c\u5e76\u63d0\u51fa\u5206\u89e3\u6a21\u578b\uff08\u57fa\u7840\u80fd\u529b+\u63a8\u7406\u80fd\u529b\uff09\u8bc1\u660eAI\u80fd\u529b\u5c06\u5728\u8fd1\u671f\u800c\u975e\u9065\u8fdc\u7684\u672a\u6765\u51fa\u73b0\u589e\u957f\u62d0\u70b9\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6307\u6570\u589e\u957f\u9884\u6d4b\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u9488\u5bf9METR\u62a5\u544a\u58f0\u79f0\u81ea2019\u5e74\u4ee5\u6765AI\u80fd\u529b\u5448\u6307\u6570\u7ea7\u589e\u957f\u5e76\u4ea7\u751f\u91cd\u5927\u73b0\u5b9e\u5f71\u54cd\uff0c\u4f46\u8be5\u7ed3\u8bba\u7f3a\u4e4f\u6570\u636e\u652f\u6491\uff0c\u4f5c\u8005\u65e8\u5728\u6311\u6218\u8fd9\u4e00\u4e3b\u6d41\u89c2\u70b9\u5e76\u63ed\u793a\u5176\u9884\u6d4b\u6a21\u578b\u7684\u8106\u5f31\u6027\u3002", "method": "\u91cd\u65b0\u62df\u5408METR\u6570\u636e\u91c7\u7528\u903b\u8f91\u66f2\u7ebf\u6a21\u578b\uff0c\u5e76\u6784\u5efa\u66f4\u590d\u6742\u7684\u5206\u89e3\u6a21\u578b\uff0c\u5c06AI\u80fd\u529b\u62c6\u89e3\u4e3a\u57fa\u7840\u80fd\u529b\u4e0e\u63a8\u7406\u80fd\u529b\u4e24\u4e2a\u72ec\u7acb\u6539\u8fdb\u901f\u7387\u7684\u7ec4\u6210\u90e8\u5206\u3002", "result": "\u6570\u636e\u4e0d\u652f\u6301\u6307\u6570\u589e\u957f\uff1b\u903b\u8f91\u66f2\u7ebf\u663e\u793a\u62d0\u70b9\u5df2\u8fc7\u53bb\uff1b\u5206\u89e3\u6a21\u578b\u8bc1\u5b9eAI\u80fd\u529b\u5c06\u5728\u4e0d\u4e45\u7684\u5c06\u6765\u800c\u975e\u9065\u8fdc\u672a\u6765\u51fa\u73b0\u589e\u957f\u62d0\u70b9\u3002", "conclusion": "\u73b0\u6709\u6307\u6570\u589e\u957f\u9884\u6d4b\u8fc7\u4e8e\u8106\u5f31\uff0cAI\u80fd\u529b\u53d1\u5c55\u8f68\u8ff9\u66f4\u53ef\u80fd\u5448\u73b0S\u578b\u66f2\u7ebf\u7279\u5f81\uff0c\u62d0\u70b9\u4e34\u8fd1\uff0c\u8fd9\u5bf9\u5b89\u5168\u7814\u7a76\u548c\u653f\u7b56\u5236\u5b9a\u5177\u6709\u91cd\u8981\u542f\u793a\u610f\u4e49\u3002"}}
{"id": "2602.04552", "categories": ["quant-ph", "gr-qc", "hep-th"], "pdf": "https://arxiv.org/pdf/2602.04552", "abs": "https://arxiv.org/abs/2602.04552", "authors": ["Hao Xu"], "title": "Restoring Landauer's Principle for Unitarily Transformed Thermal Reservoirs", "comment": "6+2 pages", "summary": "Landauer's principle, a cornerstone of quantum information and thermodynamics, appears to be violated when the thermal reservoir is replaced by a squeezed thermal state (STS). We introduce a formal extension of the principle to such unitarily transformed thermal states. By defining an effective Hamiltonian, we rigorously establish a generalized Landauer inequality, which naturally reduces to the standard case for an ordinary thermal reservoir as a special instance. The framework further yields a consistent definition of entropy production and a proof of its non-negativity. We illustrate its utility by studying an arbitrarily moving Unruh-DeWitt detector coupled to a quantum field initially prepared in the STS. Using perturbation theory, we compute the entropy production explicitly, confirming its positivity. As a result of the symmetry breaking induced by the unitary transformation, it depends on both the proper time interval and the absolute spacetime position. Our work resolves the apparent violation of Landauer's principle with STSs. It also provides a robust tool for analyzing quantum thermodynamics in non-equilibrium and relativistic settings, with potential implications for quantum thermal machines and information protocols.", "AI": {"tldr": "\u63d0\u51fa\u5e7f\u4e49\u5170\u9053\u5c14\u539f\u7406\u89e3\u51b3\u538b\u7f29\u70ed\u6001\u4e0b\u7684\u8868\u89c2\u8fdd\u53cd\u95ee\u9898\uff0c\u5efa\u7acb\u6709\u6548\u54c8\u5bc6\u987f\u91cf\u6846\u67b6\u8bc1\u660e\u71b5\u4ea7\u975e\u8d1f\u6027\uff0c\u4e3a\u91cf\u5b50\u70ed\u529b\u5b66\u63d0\u4f9b\u65b0\u5de5\u5177", "motivation": "\u4f20\u7edf\u5170\u9053\u5c14\u539f\u7406\u5728\u538b\u7f29\u70ed\u6001\uff08STS\uff09\u7cfb\u7edf\u4e2d\u51fa\u73b0\u8868\u89c2\u8fdd\u53cd\uff0c\u9700\u5efa\u7acb\u7edf\u4e00\u6846\u67b6\u534f\u8c03\u91cf\u5b50\u4fe1\u606f\u4e0e\u70ed\u529b\u5b66\u5b9a\u5f8b", "method": "\u901a\u8fc7\u5b9a\u4e49\u6709\u6548\u54c8\u5bc6\u987f\u91cf\uff0c\u5f62\u5f0f\u5316\u6269\u5c55\u5170\u9053\u5c14\u539f\u7406\u81f3\u5e7a\u6b63\u53d8\u6362\u70ed\u6001\uff0c\u63a8\u5bfc\u5e7f\u4e49\u4e0d\u7b49\u5f0f\u5e76\u8bc1\u660e\u71b5\u4ea7\u975e\u8d1f\u6027", "result": "\u83b7\u5f97\u9002\u7528\u4e8eSTS\u7684\u5e7f\u4e49\u5170\u9053\u5c14\u4e0d\u7b49\u5f0f\uff08\u9000\u5316\u4e3a\u6807\u51c6\u60c5\u5f62\u5f53STS\u2192\u666e\u901a\u70ed\u5e93\uff09\uff0c\u4e25\u683c\u8bc1\u660e\u71b5\u4ea7\u975e\u8d1f\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u79fb\u52a8Unruh-DeWitt\u63a2\u6d4b\u5668\u6a21\u578b", "conclusion": "\u89e3\u51b3STS\u4e0b\u7684\u5170\u9053\u5c14\u539f\u7406\u8868\u89c2\u77db\u76fe\uff0c\u4e3a\u975e\u5e73\u8861\u4e0e\u76f8\u5bf9\u8bba\u6027\u91cf\u5b50\u70ed\u529b\u5b66\u63d0\u4f9b\u4e25\u8c28\u5206\u6790\u5de5\u5177\uff0c\u63a8\u52a8\u91cf\u5b50\u70ed\u673a\u4e0e\u4fe1\u606f\u534f\u8bae\u53d1\u5c55"}}
{"id": "2602.04027", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.04027", "abs": "https://arxiv.org/abs/2602.04027", "authors": ["Pratyush Uppuluri", "Shilpa Noushad", "Sajan Kumar"], "title": "A Consensus-Bayesian Framework for Detecting Malicious Activity in Enterprise Directory Access Graphs", "comment": null, "summary": "This work presents a consensus-based Bayesian framework to detect malicious user behavior in enterprise directory access graphs. By modeling directories as topics and users as agents within a multi-level interaction graph, we simulate access evolution using influence-weighted opinion dynamics. Logical dependencies between users are encoded in dynamic matrices Ci, and directory similarity is captured via a shared influence matrix W. Malicious behavior is injected as cross-component logical perturbations that violate structural norms of strongly connected components(SCCs). We apply theoretical guarantees from opinion dynamics literature to determine topic convergence and detect anomaly via scaled opinion variance. To quantify uncertainty, we introduce a Bayesian anomaly scoring mechanism that evolves over time, using both static and online priors. Simulations over synthetic access graphs validate our method, demonstrating its sensitivity to logical inconsistencies and robustness under dynamic perturbation.", "AI": {"tldr": "A Bayesian framework using opinion dynamics on directory access graphs detects malicious behavior by identifying anomalies in user access patterns through convergence analysis and dynamic uncertainty scoring.", "motivation": "Malicious users in enterprise directories create logical inconsistencies in access patterns that violate structural norms of user behavior, requiring detection methods that capture complex cross-component dependencies and quantify uncertainty.", "method": "Models directories as topics and users as agents in a multi-level graph, applying influence-weighted opinion dynamics with dynamic dependency matrices Ci and shared influence matrix W. Detects anomalies via scaled opinion variance and theoretical convergence guarantees, using Bayesian scoring with static and online priors.", "result": "Synthetic graph simulations demonstrate high sensitivity to logical inconsistencies from malicious perturbations and robustness under dynamic changes, effectively quantifying detection uncertainty.", "conclusion": "The consensus-based Bayesian framework successfully identifies malicious behavior by detecting violations of structural norms in strongly connected components, providing a principled approach for enterprise security with uncertainty quantification."}}
{"id": "2602.04837", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04837", "abs": "https://arxiv.org/abs/2602.04837", "authors": ["Zhaotian Weng", "Antonis Antoniades", "Deepak Nathani", "Zhen Zhang", "Xiao Pu", "Xin Eric Wang"], "title": "Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing", "comment": "18 pages", "summary": "Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods.", "AI": {"tldr": "Group-Evolving Agents (GEA) is a new paradigm where groups of AI agents evolve together with shared experience, significantly outperforming existing self-evolving methods on coding benchmarks (71.0% vs 56.7% on SWE-bench Verified) by better utilizing exploratory diversity.", "motivation": "Existing open-ended self-evolving agents using tree-structured evolution suffer from inefficient utilization of exploratory diversity due to isolated evolutionary branches, limiting autonomous capability advancement beyond human-designed architectures.", "method": "Proposes Group-Evolving Agents (GEA) that treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout the evolutionary process.", "result": "GEA achieves 71.0% on SWE-bench Verified (vs 56.7% baseline) and 88.3% on Polyglot (vs 68.3% baseline), matches top human-designed frameworks, fixes bugs in 1.4 iterations (vs 5), and shows better transferability and robustness.", "conclusion": "GEA effectively converts early-stage exploratory diversity into sustained long-term progress, demonstrating superior performance, robustness, and transferability for open-ended self-improving agents in coding tasks."}}
{"id": "2602.04031", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04031", "abs": "https://arxiv.org/abs/2602.04031", "authors": ["Aditya Gorla", "Ratish Puduppully"], "title": "The Illusion of Generalization: Re-examining Tabular Language Model Evaluation", "comment": null, "summary": "Tabular Language Models (TLMs) have been claimed to achieve emergent generalization for tabular prediction. We conduct a systematic re-evaluation of Tabula-8B as a representative TLM, utilizing 165 datasets from the UniPredict benchmark. Our investigation reveals three findings. First, binary and categorical classification achieve near-zero median lift over majority-class baselines and strong aggregate performance is driven entirely by quartile classification tasks. Second, top-performing datasets exhibit pervasive contamination, including complete train-test overlap and task-level leakage that evades standard deduplication. Third, instruction-tuning without tabular exposure recovers 92.2% of standard classification performance and on quartile classification, format familiarity closes 71.3% of the gap with the residual attributable to contaminated datasets. These findings suggest claimed generalization likely reflects evaluation artifacts rather than learned tabular reasoning. We conclude with recommendations for strengthening TLM evaluation.", "AI": {"tldr": "This paper systematically re-evaluates Tabula-8B, a Tabular Language Model, using 165 datasets from the UniPredict benchmark. It finds that TLMs' claimed emergent generalization stems from evaluation artifacts like dataset contamination and task-specific quirks rather than genuine tabular reasoning abilities. Instruction-tuning without tabular exposure nearly matches performance, suggesting format familiarity rather than domain learning drives results.", "motivation": "The authors were motivated by claims that Tabular Language Models achieve emergent generalization for tabular prediction, but suspected these claims might be inflated due to evaluation artifacts. They aimed to conduct a rigorous, large-scale re-evaluation to verify whether TLMs truly learn tabular reasoning or if performance gains reflect methodological flaws.", "method": "The researchers performed a systematic re-evaluation of Tabula-8B using 165 datasets from the UniPredict benchmark. They analyzed performance across different task types (binary, categorical, and quartile classification), investigated dataset contamination issues, and conducted controlled experiments comparing standard TLMs against instruction-tuned models without tabular exposure to isolate the factors driving performance.", "result": "Three key findings emerged: (1) Binary and categorical classification showed near-zero median lift over majority-class baselines, with strong aggregate performance driven entirely by quartile classification tasks; (2) Top-performing datasets exhibited pervasive contamination including complete train-test overlap and task-level leakage; (3) Instruction-tuning without tabular exposure recovered 92.2% of standard classification performance, and format familiarity closed 71.3% of the gap for quartile classification, with the residual gap attributable to contaminated datasets.", "conclusion": "The study concludes that claimed TLM generalization likely reflects evaluation artifacts rather than learned tabular reasoning. The authors provide recommendations for strengthening TLM evaluation, including improved dataset curation, contamination detection, and more rigorous experimental designs to ensure future claims reflect genuine model capabilities."}}
{"id": "2602.04843", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04843", "abs": "https://arxiv.org/abs/2602.04843", "authors": ["Dmitrii Kharlapenko", "Alessandro Stolfo", "Arthur Conmy", "Mrinmaya Sachan", "Zhijing Jin"], "title": "Fluid Representations in Reasoning Models", "comment": null, "summary": "Reasoning language models, which generate long chains of thought, dramatically outperform non-reasoning language models on abstract problems. However, the internal model mechanisms that allow this superior performance remain poorly understood. We present a mechanistic analysis of how QwQ-32B - a model specifically trained to produce extensive reasoning traces - process abstract structural information. On Mystery Blocksworld - a semantically obfuscated planning domain - we find that QwQ-32B gradually improves its internal representation of actions and concepts during reasoning. The model develops abstract encodings that focus on structure rather than specific action names. Through steering experiments, we establish causal evidence that these adaptations improve problem solving: injecting refined representations from successful traces boosts accuracy, while symbolic representations can replace many obfuscated encodings with minimal performance loss. We find that one of the factors driving reasoning model performance is in-context refinement of token representations, which we dub Fluid Reasoning Representations.", "AI": {"tldr": "This paper investigates how QwQ-32B, a reasoning language model, processes abstract structural information. Through mechanistic analysis on Mystery Blocksworld, they discover that the model develops \"Fluid Reasoning Representations\" - gradually refined internal token representations that focus on structural patterns rather than specific semantics, which causally improves problem-solving performance.", "motivation": "Reasoning language models that generate long chains of thought show dramatic performance improvements on abstract problems, but their internal mechanisms remain poorly understood. The paper aims to uncover how these models process abstract structural information.", "method": "The researchers conduct a mechanistic analysis of QwQ-32B on Mystery Blocksworld, a semantically obfuscated planning domain. They use steering experiments to establish causal relationships between internal representations and model performance.", "result": "QwQ-32B gradually improves its internal representation of actions and concepts during reasoning, developing abstract encodings that focus on structure over specific action names. Steering experiments show that injecting refined representations from successful traces boosts accuracy, and symbolic representations can replace many obfuscated encodings with minimal performance loss.", "conclusion": "The key factor driving reasoning model performance is in-context refinement of token representations, termed \"Fluid Reasoning Representations.\""}}
{"id": "2602.04627", "categories": ["quant-ph", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2602.04627", "abs": "https://arxiv.org/abs/2602.04627", "authors": ["Daniel Eyles", "Emmanuel Lassalle", "Adam Stokes", "Ahsan Nazir", "Ram\u00f3n Paniagua-Dom\u00ednguez"], "title": "Dicke Superradiance in Extended 2D Quantum Arrays Coupled to Metasurface Bound States in the Continuum", "comment": "14 pages, 7 figures, including supplementary material", "summary": "Dicke superradiance is a collective phenomenon where the emission from ensembles of quantum emitters is coherently enhanced beyond the sum of each emitter's independent emission. Here, we propose a platform that exploits the delocalised nature of a high-Q, non-local mode supported by a dielectric metasurface (a so-called bound-state-in-the-continuum or BIC) to induce superradiant behaviour within an extended two-dimensional array of distant quantum emitters. We show that these BIC-mediated emitter interactions can span several wavelengths, thus overcoming the traditional subwavelength separation between emitters required in free space. We further show that reaching the idealised Dicke limit is possible in this system, provided that the emitters are coupled to the BIC mode efficiently enough, as quantified through the $\u03b2$-factor. Moreover, we demonstrate its experimental viability by analysing its robustness to realistic experimental imperfections. This work puts forward optical metasurfaces supporting BICs as a physically viable platform for realising the upper limits of cooperative emission in physically extended quantum emitter arrays.", "AI": {"tldr": "\u5229\u7528\u4ecb\u7535\u8d85\u8868\u9762\u4e2d\u7684\u8fde\u7eed\u57df\u675f\u7f1a\u6001\u5b9e\u73b0\u6269\u5c55\u91cf\u5b50\u53d1\u5c04\u5668\u9635\u5217\u7684\u8fea\u514b\u8d85\u8f90\u5c04", "motivation": "\u4f20\u7edf\u8fea\u514b\u8d85\u8f90\u5c04\u9700\u8981\u53d1\u5c04\u5668\u4e9a\u6ce2\u957f\u95f4\u8ddd\uff0c\u9650\u5236\u4e86\u5176\u5728\u6269\u5c55\u9635\u5217\u4e2d\u7684\u5e94\u7528\uff1b\u9700\u8981\u65b0\u5e73\u53f0\u5b9e\u73b0\u957f\u7a0b\u76f8\u5e72\u8026\u5408", "method": "\u63d0\u51fa\u5229\u7528\u4ecb\u7535\u8d85\u8868\u9762\u652f\u6301\u7684\u9ad8Q\u975e\u5c40\u57df\u6a21\u5f0f\uff08\u8fde\u7eed\u57df\u675f\u7f1a\u6001/BIC\uff09\u6765\u4ecb\u5bfc\u76f8\u8ddd\u6570\u6ce2\u957f\u7684\u91cf\u5b50\u53d1\u5c04\u5668\u95f4\u7684\u8d85\u8f90\u5c04\u8026\u5408", "result": "BIC\u4ecb\u5bfc\u7684\u53d1\u5c04\u5668\u76f8\u4e92\u4f5c\u7528\u53ef\u8de8\u8d8a\u591a\u4e2a\u6ce2\u957f\u8ddd\u79bb\uff1b\u901a\u8fc7\u03b2\u56e0\u5b50\u91cf\u5316\u8026\u5408\u6548\u7387\uff1b\u7cfb\u7edf\u53ef\u8fbe\u5230\u7406\u60f3\u8fea\u514b\u6781\u9650\uff1b\u5bf9\u5b9e\u9a8c\u7f3a\u9677\u5177\u6709\u9c81\u68d2\u6027", "conclusion": "\u57fa\u4e8eBIC\u7684\u5149\u5b66\u8d85\u8868\u9762\u4e3a\u5728\u7269\u7406\u6269\u5c55\u7684\u91cf\u5b50\u53d1\u5c04\u5668\u9635\u5217\u4e2d\u5b9e\u73b0\u534f\u540c\u53d1\u5c04\u4e0a\u9650\u63d0\u4f9b\u4e86\u53ef\u884c\u5b9e\u9a8c\u5e73\u53f0"}}
{"id": "2602.04037", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.04037", "abs": "https://arxiv.org/abs/2602.04037", "authors": ["Pengcheng Wang", "Qinghang Liu", "Haotian Lin", "Yiheng Li", "Guojian Zhan", "Masayoshi Tomizuka", "Yixiao Wang"], "title": "DADP: Domain Adaptive Diffusion Policy", "comment": null, "summary": "Learning domain adaptive policies that can generalize to unseen transition dynamics, remains a fundamental challenge in learning-based control. Substantial progress has been made through domain representation learning to capture domain-specific information, thus enabling domain-aware decision making. We analyze the process of learning domain representations through dynamical prediction and find that selecting contexts adjacent to the current step causes the learned representations to entangle static domain information with varying dynamical properties. Such mixture can confuse the conditioned policy, thereby constraining zero-shot adaptation. To tackle the challenge, we propose DADP (Domain Adaptive Diffusion Policy), which achieves robust adaptation through unsupervised disentanglement and domain-aware diffusion injection. First, we introduce Lagged Context Dynamical Prediction, a strategy that conditions future state estimation on a historical offset context; by increasing this temporal gap, we unsupervisedly disentangle static domain representations by filtering out transient properties. Second, we integrate the learned domain representations directly into the generative process by biasing the prior distribution and reformulating the diffusion target. Extensive experiments on challenging benchmarks across locomotion and manipulation demonstrate the superior performance, and the generalizability of DADP over prior methods. More visualization results are available on the https://outsider86.github.io/DomainAdaptiveDiffusionPolicy/.", "AI": {"tldr": "\u63d0\u51faDADP\uff08\u57df\u81ea\u9002\u5e94\u6269\u6563\u7b56\u7565\uff09\uff0c\u901a\u8fc7\u6ede\u540e\u4e0a\u4e0b\u6587\u52a8\u6001\u9884\u6d4b\u5b9e\u73b0\u65e0\u76d1\u7763\u57df\u4fe1\u606f\u89e3\u8026\uff0c\u5e76\u5c06\u89e3\u8026\u540e\u7684\u57df\u8868\u793a\u6ce8\u5165\u6269\u6563\u751f\u6210\u8fc7\u7a0b\uff0c\u5728\u8fd0\u52a8\u548c\u64cd\u63a7\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u5353\u8d8a\u7684\u96f6\u6837\u672c\u9002\u5e94\u6027\u80fd\u3002", "motivation": "\u5b66\u4e60\u57df\u81ea\u9002\u5e94\u7b56\u7565\u4ee5\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u52a8\u6001\u8f6c\u79fb\u662f\u5b66\u4e60\u63a7\u5236\u7684\u57fa\u672c\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u57df\u8868\u793a\u5b66\u4e60\u6355\u83b7\u57df\u7279\u5b9a\u4fe1\u606f\uff0c\u4f46\u5f53\u524d\u76f8\u90bb\u4e0a\u4e0b\u6587\u9009\u62e9\u4f1a\u5bfc\u81f4\u9759\u6001\u57df\u4fe1\u606f\u4e0e\u52a8\u6001\u7279\u6027\u7ea0\u7f20\uff0c\u6df7\u6dc6\u6761\u4ef6\u7b56\u7565\uff0c\u9650\u5236\u96f6\u6837\u672c\u9002\u5e94\u80fd\u529b\u3002", "method": "\u63d0\u51faDADP\u6846\u67b6\uff1a1\uff09\u6ede\u540e\u4e0a\u4e0b\u6587\u52a8\u6001\u9884\u6d4b\u7b56\u7565\uff0c\u901a\u8fc7\u589e\u52a0\u5386\u53f2\u504f\u79fb\u4e0a\u4e0b\u6587\u7684\u65f6\u95f4\u95f4\u9694\uff0c\u65e0\u76d1\u7763\u5730\u89e3\u8026\u9759\u6001\u57df\u8868\u793a\uff1b2\uff09\u5c06\u5b66\u4e60\u5230\u7684\u57df\u8868\u793a\u76f4\u63a5\u96c6\u6210\u5230\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u901a\u8fc7\u504f\u7f6e\u5148\u9a8c\u5206\u5e03\u548c\u91cd\u6784\u6269\u6563\u76ee\u6807\u5b9e\u73b0\u57df\u611f\u77e5\u7684\u6269\u6563\u6ce8\u5165\u3002", "result": "\u5728\u8fd0\u52a8\u548c\u64cd\u63a7\u9886\u57df\u7684\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDADP\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002\u66f4\u591a\u53ef\u89c6\u5316\u7ed3\u679c\u53ef\u5728\u9879\u76ee\u7f51\u7ad9\u67e5\u770b\u3002", "conclusion": "DADP\u901a\u8fc7\u65e0\u76d1\u7763\u89e3\u8026\u548c\u57df\u611f\u77e5\u6269\u6563\u6ce8\u5165\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u57df\u9002\u5e94\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u57df\u8868\u793a\u4e2d\u9759\u6001\u4e0e\u52a8\u6001\u4fe1\u606f\u7ea0\u7f20\u7684\u95ee\u9898\uff0c\u4e3a\u5b66\u4e60\u57fa\u7840\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.04633", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04633", "abs": "https://arxiv.org/abs/2602.04633", "authors": ["Xuan Du Trinh", "Isma\u00ebl Septembre", "Hai-Chau Nguyen"], "title": "On the emergence of classical stochasticity", "comment": "Comments are welcome!", "summary": "We examine the logical structure of the emergence of classical stochasticity for a quantum system governed by a Pauli-type master equation. It is well-known that while such equations describe the evolution of probabilities, they do not automatically justify classical reasoning based on the assumption that the system exists in a definite state at intermediate times. On the other hand, we show that this assumption is crucial for the standard calculation of stochastic times such as the persistent time and the time of first arrivals. We then consider examples of single particles, bosons, and fermions in the so-called ultradecoherence limit to illustrate how classical stochasticity may emerge from quantum mechanics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63ed\u793a\u4e86\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7ecf\u5178\u968f\u673a\u6027\u6d8c\u73b0\u7684\u903b\u8f91\u57fa\u7840\u95ee\u9898\uff0c\u6307\u51faPauli\u578b\u4e3b\u65b9\u7a0b\u867d\u63cf\u8ff0\u6982\u7387\u6f14\u5316\uff0c\u4f46\u9700\u989d\u5916\u5047\u8bbe\"\u7cfb\u7edf\u5904\u4e8e\u786e\u5b9a\u72b6\u6001\"\u624d\u80fd\u8ba1\u7b97\u7ecf\u5178\u968f\u673a\u65f6\u95f4\uff0c\u5e76\u901a\u8fc7\u8d85\u9000\u76f8\u5e72\u6781\u9650\u6848\u4f8b\u5c55\u793a\u91cf\u5b50\u5230\u7ecf\u5178\u7684\u8fc7\u6e21\u673a\u5236\u3002", "motivation": "\u7ecf\u5178\u968f\u673a\u6027\u5728\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684\u6d8c\u73b0\u673a\u5236\u5c1a\u672a\u5b8c\u5168\u89e3\u51b3\uff1aPauli\u578b\u4e3b\u65b9\u7a0b\u63cf\u8ff0\u6982\u7387\u6f14\u5316\uff0c\u4f46\u65e0\u6cd5\u81ea\u52a8\u8bc1\u660e\"\u7cfb\u7edf\u5728\u4e2d\u95f4\u65f6\u523b\u5904\u4e8e\u786e\u5b9a\u72b6\u6001\"\u8fd9\u4e00\u7ecf\u5178\u63a8\u7406\u524d\u63d0\u7684\u5408\u7406\u6027\uff0c\u800c\u8be5\u5047\u8bbe\u5374\u662f\u8ba1\u7b97\u6301\u4e45\u65f6\u95f4\u3001\u9996\u6b21\u5230\u8fbe\u65f6\u95f4\u7b49\u968f\u673a\u65f6\u95f4\u6807\u51c6\u65b9\u6cd5\u7684\u57fa\u7840\u3002", "method": "\u901a\u8fc7\u5206\u6790Pauli\u578b\u4e3b\u65b9\u7a0b\u7684\u903b\u8f91\u7ed3\u6784\uff0c\u8bba\u8bc1\"\u786e\u5b9a\u72b6\u6001\"\u5047\u8bbe\u5bf9\u968f\u673a\u65f6\u95f4\u8ba1\u7b97\u7684\u5fc5\u8981\u6027\uff1b\u4ee5\u5355\u7c92\u5b50\u3001\u73bb\u8272\u5b50\u548c\u8d39\u7c73\u5b50\u5728\u8d85\u9000\u76f8\u5e72\u6781\u9650\u4e0b\u7684\u884c\u4e3a\u4e3a\u4f8b\uff0c\u6f14\u793a\u91cf\u5b50\u529b\u5b66\u5982\u4f55\u5bfc\u51fa\u7ecf\u5178\u968f\u673a\u6027\u3002", "result": "1) \u8bc1\u660e\u6807\u51c6\u968f\u673a\u65f6\u95f4\u8ba1\u7b97\u9690\u542b\u4f9d\u8d56\"\u7cfb\u7edf\u5b58\u5728\u786e\u5b9a\u72b6\u6001\"\u7684\u672a\u58f0\u660e\u5047\u8bbe\uff1b2) \u5728\u8d85\u9000\u76f8\u5e72\u6781\u9650\u4e0b\uff0c\u91cf\u5b50\u7cfb\u7edf\u901a\u8fc7\u9000\u76f8\u5e72\u673a\u5236\u81ea\u7136\u5448\u73b0\u7ecf\u5178\u968f\u673a\u884c\u4e3a\uff0c\u4e3a\u5047\u8bbe\u63d0\u4f9b\u7269\u7406\u57fa\u7840\u3002", "conclusion": "\u91cf\u5b50\u7cfb\u7edf\u53ef\u901a\u8fc7\u8d85\u9000\u76f8\u5e72\u7b49\u673a\u5236\u5b9e\u73b0\u5411\u7ecf\u5178\u968f\u673a\u6027\u7684\u6709\u6548\u8fc7\u6e21\uff0c\u4f46\u9700\u660e\u786e\u533a\u5206\u6570\u5b66\u63cf\u8ff0\uff08\u4e3b\u65b9\u7a0b\uff09\u4e0e\u7269\u7406\u5047\u8bbe\uff08\u786e\u5b9a\u72b6\u6001\uff09\uff0c\u8fd9\u5bf9\u91cf\u5b50-\u7ecf\u5178\u5bf9\u5e94\u5173\u7cfb\u7684\u4e25\u8c28\u5efa\u7acb\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.04042", "categories": ["cs.LG", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.04042", "abs": "https://arxiv.org/abs/2602.04042", "authors": ["Felipe Angelim", "Alessandro Leite"], "title": "Partition Trees: Conditional Density Estimation over General Outcome Spaces", "comment": "Code available at https://github.com/felipeangelimvieira/partition_tree", "summary": "We propose Partition Trees, a tree-based framework for conditional density estimation over general outcome spaces, supporting both continuous and categorical variables within a unified formulation. Our approach models conditional distributions as piecewise-constant densities on data adaptive partitions and learns trees by directly minimizing conditional negative log-likelihood. This yields a scalable, nonparametric alternative to existing probabilistic trees that does not make parametric assumptions about the target distribution. We further introduce Partition Forests, an ensemble extension obtained by averaging conditional densities. Empirically, we demonstrate improved probabilistic prediction over CART-style trees and competitive or superior performance compared to state-of-the-art probabilistic tree methods and Random Forests, along with robustness to redundant features and heteroscedastic noise.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPartition Trees\uff0c\u4e00\u4e2a\u7528\u4e8e\u901a\u7528\u7ed3\u679c\u7a7a\u95f4\u6761\u4ef6\u5bc6\u5ea6\u4f30\u8ba1\u7684\u6811\u6846\u67b6\uff0c\u7edf\u4e00\u652f\u6301\u8fde\u7eed\u548c\u5206\u7c7b\u53d8\u91cf\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6700\u5c0f\u5316\u6761\u4ef6\u8d1f\u5bf9\u6570\u4f3c\u7136\u5b66\u4e60\u6570\u636e\u81ea\u9002\u5e94\u7684\u5206\u6bb5\u5e38\u6570\u5bc6\u5ea6\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u975e\u53c2\u6570\u66ff\u4ee3\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u6982\u7387\u6811\u65b9\u6cd5\u901a\u5e38\u5bf9\u76ee\u6807\u5206\u5e03\u6709\u53c2\u6570\u5047\u8bbe\uff0c\u4e14\u96be\u4ee5\u7edf\u4e00\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u7ed3\u679c\u53d8\u91cf\u3002\u7f3a\u4e4f\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u975e\u53c2\u6570\u4e14\u9002\u7528\u4e8e\u901a\u7528\u7ed3\u679c\u7a7a\u95f4\u7684\u6761\u4ef6\u5bc6\u5ea6\u4f30\u8ba1\u6846\u67b6\u3002", "method": "\u63d0\u51faPartition Trees\uff1a\u5c06\u6761\u4ef6\u5206\u5e03\u5efa\u6a21\u4e3a\u6570\u636e\u81ea\u9002\u5e94\u5212\u5206\u4e0a\u7684\u5206\u6bb5\u5e38\u6570\u5bc6\u5ea6\uff0c\u901a\u8fc7\u76f4\u63a5\u6700\u5c0f\u5316\u6761\u4ef6\u8d1f\u5bf9\u6570\u4f3c\u7136\u8fdb\u884c\u6811\u5b66\u4e60\u3002\u8fdb\u4e00\u6b65\u5f15\u5165Partition Forests\uff0c\u901a\u8fc7\u96c6\u6210\u5e73\u5747\u6761\u4ef6\u5bc6\u5ea6\u6765\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6982\u7387\u9884\u6d4b\u4e0a\u663e\u8457\u4f18\u4e8eCART\u6811\uff1b\u4e0e\u5f53\u524d\u5148\u8fdb\u6982\u7387\u6811\u65b9\u6cd5\u53ca\u968f\u673a\u68ee\u6797\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u6216\u66f4\u4f18\u6027\u80fd\uff1b\u540c\u65f6\u5bf9\u5197\u4f59\u7279\u5f81\u548c\u5f02\u65b9\u5dee\u566a\u58f0\u8868\u73b0\u51fa\u826f\u597d\u9c81\u68d2\u6027\u3002", "conclusion": "Partition Trees\u4e3a\u901a\u7528\u7ed3\u679c\u7a7a\u95f4\u7684\u6761\u4ef6\u5bc6\u5ea6\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u3001\u53ef\u6269\u5c55\u7684\u975e\u53c2\u6570\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u8df5\u4ef7\u503c\u3002"}}
{"id": "2602.04646", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04646", "abs": "https://arxiv.org/abs/2602.04646", "authors": ["Xavier Barcons Planas", "Helen M. Chrzanowski", "Janik Wolters"], "title": "Pure narrowband photon-pair generation in a monolithic cavity", "comment": null, "summary": "Photonic quantum technologies require efficient sources of pure single photons. Here we present a heralded SPDC single-photon source in a monolithic cavity optimized for high spectral and spatial purity. The source heralds single-photons at a wavelength of 1540 nm and a spectral bandwidth of 168 MHz with a maximum heralding efficiency of 84%, while keeping the multi-photon contamination below 3%. The cavity enhancement generates photons mainly in the central cavity mode with 96.2% spectral purity.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u7247\u8154\u4f53\u4f18\u5316\u7684\u9884\u62a5\u5f0fSPDC\u5355\u5149\u5b50\u6e90\uff0c\u57281540 nm\u6ce2\u6bb5\u5b9e\u73b0\u4e8684%\u7684\u9884\u62a5\u6548\u7387\u300196.2%\u7684\u5149\u8c31\u7eaf\u5ea6\u4ee5\u53ca\u4f4e\u4e8e3%\u7684\u591a\u5149\u5b50\u6c61\u67d3\u3002", "motivation": "\u5149\u5b50\u91cf\u5b50\u6280\u672f\u7684\u53d1\u5c55\u8feb\u5207\u9700\u8981\u9ad8\u6548\u4e14\u5149\u8c31/\u7a7a\u95f4\u7eaf\u5ea6\u9ad8\u7684\u5355\u5149\u5b50\u6e90\u3002", "method": "\u91c7\u7528\u81ea\u53d1\u53c2\u91cf\u4e0b\u8f6c\u6362(SPDC)\u6280\u672f\u7ed3\u5408\u5355\u7247\u8154\u4f53\u7ed3\u6784\uff0c\u4f18\u5316\u63d0\u5347\u5149\u8c31\u548c\u7a7a\u95f4\u7eaf\u5ea6\u3002", "result": "\u5b9e\u9a8c\u83b7\u5f97\u4e861540 nm\u6ce2\u957f\u7684\u5355\u5149\u5b50\uff0c\u5149\u8c31\u5e26\u5bbd168 MHz\uff0c\u6700\u5927\u9884\u62a5\u6548\u738784%\uff0c\u591a\u5149\u5b50\u6c61\u67d3\u4f4e\u4e8e3%\uff0c\u5149\u8c31\u7eaf\u5ea6\u8fbe96.2%\u3002", "conclusion": "\u8154\u4f53\u589e\u5f3a\u6548\u5e94\u4f7f\u5149\u5b50\u4e3b\u8981\u5728\u4e2d\u5fc3\u8154\u6a21\u4ea7\u751f\uff0c\u6210\u529f\u5b9e\u73b0\u4e8696.2%\u7684\u9ad8\u5149\u8c31\u7eaf\u5ea6\u3002"}}
{"id": "2602.04700", "categories": ["quant-ph", "cs.CC"], "pdf": "https://arxiv.org/pdf/2602.04700", "abs": "https://arxiv.org/abs/2602.04700", "authors": ["Sebastian Alberto Grillo", "Bernardo Daniel D\u00e1valos", "Rodney Fabian Franco Torres", "Franklin de Lima Marquezino", "Edgar L\u00f3pez Pezoa"], "title": "Quantum Advantage in Decision Trees: A Weighted Graph and $L_1$ Norm Approach", "comment": "24 pages, 3 figures", "summary": "The analysis of the computational power of single-query quantum algorithms is important because they must extract maximal information from one oracle call, revealing fundamental limits of quantum advantage and enabling optimal, resource-efficient quantum computation. This paper proposes a formulation of single-query quantum decision trees as weighted graphs. This formulation has the advantage that it facilitates the analysis of the $L_1$ spectral norm of the algorithm output. This advantage is based on the fact that a high $L_1$ spectral norm of the output of a quantum decision tree is a necessary condition to outperform its classical counterpart. We propose heuristics for maximizing the $L_{1}$ spectral norm, show how to combine weighted graphs to generate sequences with strictly increasing norm, and present functions exhibiting exponential quantum advantage. Finally, we establish a necessary condition linking single-query quantum advantage to the asymptotic growth of measurement projector dimensions.", "AI": {"tldr": "\u672c\u6587\u5c06\u5355\u67e5\u8be2\u91cf\u5b50\u51b3\u7b56\u6811\u5efa\u6a21\u4e3a\u52a0\u6743\u56fe\uff0c\u901a\u8fc7\u5206\u6790\u8f93\u51fa\u6001\u7684L1\u8c31\u8303\u6570\u6765\u523b\u753b\u5355\u67e5\u8be2\u91cf\u5b50\u7b97\u6cd5\u7684\u8ba1\u7b97\u80fd\u529b\u3002\u7814\u7a76\u8868\u660e\u9ad8L1\u8303\u6570\u662f\u91cf\u5b50\u4f18\u52bf\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u5e76\u63d0\u51fa\u4e86\u6700\u5927\u5316\u8be5\u8303\u6570\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u3001\u6784\u9020\u4e25\u683c\u9012\u589e\u8303\u6570\u5e8f\u5217\u7684\u7ec4\u5408\u6280\u672f\u3001\u5c55\u793a\u6307\u6570\u7ea7\u91cf\u5b50\u4f18\u52bf\u7684\u5177\u4f53\u51fd\u6570\uff0c\u4ee5\u53ca\u5efa\u7acb\u91cf\u5b50\u4f18\u52bf\u4e0e\u6d4b\u91cf\u6295\u5f71\u7ef4\u6570\u6e10\u8fd1\u589e\u957f\u4e4b\u95f4\u7684\u5fc5\u8981\u6761\u4ef6\u3002", "motivation": "\u5355\u67e5\u8be2\u91cf\u5b50\u7b97\u6cd5\u5fc5\u987b\u5728\u4e00\u6b21\u9884\u8a00\u673a\u8c03\u7528\u4e2d\u63d0\u53d6\u6700\u5927\u4fe1\u606f\uff0c\u5bf9\u5176\u8ba1\u7b97\u80fd\u529b\u7684\u5206\u6790\u63ed\u793a\u4e86\u91cf\u5b50\u4f18\u52bf\u7684\u57fa\u672c\u6781\u9650\uff0c\u5bf9\u4e8e\u5b9e\u73b0\u6700\u4f18\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u91cf\u5b50\u8ba1\u7b97\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u5c06\u5355\u67e5\u8be2\u91cf\u5b50\u51b3\u7b56\u6811\u5f62\u5f0f\u5316\u4e3a\u52a0\u6743\u56fe\u7ed3\u6784\uff0c\u5229\u7528\u8be5\u8868\u793a\u6cd5\u4fbf\u4e8e\u5206\u6790\u7b97\u6cd5\u8f93\u51fa\u6001\u7684L1\u8c31\u8303\u6570\uff0c\u5e76\u8bbe\u8ba1\u542f\u53d1\u5f0f\u65b9\u6cd5\u6700\u5927\u5316\u8be5\u8303\u6570\uff0c\u540c\u65f6\u901a\u8fc7\u7ec4\u5408\u52a0\u6743\u56fe\u6784\u9020\u5177\u6709\u4e25\u683c\u9012\u589e\u8303\u6570\u7684\u5e8f\u5217\u3002", "result": "(1) \u63d0\u51fa\u4e86\u6700\u5927\u5316L1\u8c31\u8303\u6570\u7684\u542f\u53d1\u5f0f\u7b56\u7565\uff1b(2) \u5c55\u793a\u4e86\u7ec4\u5408\u52a0\u6743\u56fe\u4ee5\u751f\u6210\u4e25\u683c\u9012\u589e\u8303\u6570\u5e8f\u5217\u7684\u65b9\u6cd5\uff1b(3) \u6784\u9020\u4e86\u5c55\u73b0\u6307\u6570\u7ea7\u91cf\u5b50\u4f18\u52bf\u7684\u5177\u4f53\u51fd\u6570\u5b9e\u4f8b\uff1b(4) \u5efa\u7acb\u4e86\u5355\u67e5\u8be2\u91cf\u5b50\u4f18\u52bf\u4e0e\u6d4b\u91cf\u6295\u5f71\u7ef4\u6570\u6e10\u8fd1\u589e\u957f\u4e4b\u95f4\u7684\u5fc5\u8981\u6761\u4ef6\u5173\u7cfb\u3002", "conclusion": "\u52a0\u6743\u56fe\u8868\u793a\u6cd5\u4e3a\u5206\u6790\u91cf\u5b50\u51b3\u7b56\u6811\u7684L1\u8c31\u8303\u6570\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u800c\u9ad8L1\u8303\u6570\u662f\u8d85\u8d8a\u7ecf\u5178\u7b97\u6cd5\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u8fd9\u4e00\u5de5\u4f5c\u4e3a\u7406\u89e3\u548c\u8bbe\u8ba1\u5177\u6709\u91cf\u5b50\u4f18\u52bf\u7684\u5355\u67e5\u8be2\u7b97\u6cd5\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.04071", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04071", "abs": "https://arxiv.org/abs/2602.04071", "authors": ["Furkan Mumcu", "Lokman Bekit", "Michael J. Jones", "Anoop Cherian", "Yasin Yilmaz"], "title": "Agentic AI-Empowered Dynamic Survey Framework", "comment": null, "summary": "Survey papers play a central role in synthesizing and organizing scientific knowledge, yet they are increasingly strained by the rapid growth of research output. As new work continues to appear after publication, surveys quickly become outdated, contributing to redundancy and fragmentation in the literature. We reframe survey writing as a long-horizon maintenance problem rather than a one-time generation task, treating surveys as living documents that evolve alongside the research they describe. We propose an agentic Dynamic Survey Framework that supports the continuous updating of existing survey papers by incrementally integrating new work while preserving survey structure and minimizing unnecessary disruption. Using a retrospective experimental setup, we demonstrate that the proposed framework effectively identifies and incorporates emerging research while preserving the coherence and structure of existing surveys.", "AI": {"tldr": "Proposing a Dynamic Survey Framework to treat survey papers as living documents that can be continuously updated to avoid obsolescence.", "motivation": "Survey papers become outdated quickly due to rapid research growth, causing redundancy and fragmentation in literature.", "method": "Proposes an agentic Dynamic Survey Framework for continuous, incremental updating of surveys while preserving structure.", "result": "Retrospective experiments show the framework effectively identifies and incorporates new research while maintaining survey coherence.", "conclusion": "The framework successfully addresses survey obsolescence by enabling living, evolving survey documents."}}
{"id": "2602.04719", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04719", "abs": "https://arxiv.org/abs/2602.04719", "authors": ["Laurin E. Fischer"], "title": "Enabling large-scale digital quantum simulations with superconducting qubits", "comment": "187 pages", "summary": "Quantum computing promises to revolutionize several scientific and technological domains through fundamentally new ways of processing information. Among its most compelling applications is digital quantum simulation, where quantum computers are used to replicate the behavior of other quantum systems. This could enable the study of problems that are otherwise intractable on classical computers, transforming fields such as quantum chemistry, condensed matter physics, and materials science. Despite this potential, realizations of practical quantum advantage for relevant problems are hindered by imperfections of current devices. This also affects quantum hardware based on superconducting circuits which is among the most advanced and scalable platforms. The envisaged long-term solution of fault-tolerant quantum computers that correct their own errors remains out of reach mainly due to the associated qubit number overhead. As a result, the field has developed strategies that combine quantum and classical resources, exploit hardware-native operations, and employ error mitigation techniques to extract meaningful results from noisy data. This doctoral thesis contributes to this broader effort by exploring methods for advancing quantum simulation across the full computational stack, including hardware-level innovations, refined techniques for noise modeling and error mitigation, and algorithmic improvements enabled by efficient measurement processing.", "AI": {"tldr": "\u672c\u535a\u58eb\u8bba\u6587\u9488\u5bf9\u5f53\u524d\u542b\u566a\u58f0\u91cf\u5b50\u786c\u4ef6\u5b9e\u73b0\u91cf\u5b50\u6a21\u62df\u7684\u6311\u6218\uff0c\u63d0\u51fa\u8986\u76d6\u5168\u8ba1\u7b97\u6808\u7684\u7efc\u5408\u65b9\u6cd5\uff0c\u5305\u62ec\u786c\u4ef6\u521b\u65b0\u3001\u566a\u58f0\u5efa\u6a21\u3001\u8bef\u5dee\u7f13\u89e3\u548c\u7b97\u6cd5\u6539\u8fdb\uff0c\u65e8\u5728\u5f25\u5408\u7406\u8bba\u6f5c\u529b\u4e0e\u5b9e\u9645\u5e94\u7528\u7684\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524d\u91cf\u5b50\u8ba1\u7b97\u673a\uff08\u5305\u62ec\u6700\u5148\u8fdb\u7684\u8d85\u5bfc\u7535\u8def\u5e73\u53f0\uff09\u5b58\u5728\u7f3a\u9677\uff0c\u65e0\u6cd5\u5b9e\u73b0\u6570\u5b57\u91cf\u5b50\u6a21\u62df\u7684\u5b9e\u9645\u91cf\u5b50\u4f18\u52bf\u3002\u5bb9\u9519\u91cf\u5b50\u8ba1\u7b97\u673a\u867d\u4e3a\u957f\u671f\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u91cf\u5b50\u6bd4\u7279\u5f00\u9500\u8fc7\u5927\uff0c\u56e0\u6b64\u4e9f\u9700\u4ece\u542b\u566a\u58f0\u8bbe\u5907\u4e2d\u63d0\u53d6\u6709\u610f\u4e49\u7ed3\u679c\uff0c\u4ee5\u63a8\u52a8\u91cf\u5b50\u5316\u5b66\u3001\u51dd\u805a\u6001\u7269\u7406\u548c\u6750\u6599\u79d1\u5b66\u7b49\u9886\u57df\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u5168\u6808\u5f0f\u7814\u7a76\u65b9\u6cd5\uff1a1\uff09\u786c\u4ef6\u5c42\u9762\u7684\u5e73\u53f0\u7279\u5b9a\u521b\u65b0\uff1b2\uff09\u7cbe\u7ec6\u5316\u566a\u58f0\u5efa\u6a21\u4e0e\u8bef\u5dee\u7f13\u89e3\u6280\u672f\uff1b3\uff09\u901a\u8fc7\u9ad8\u6548\u6d4b\u91cf\u5904\u7406\u5b9e\u73b0\u7b97\u6cd5\u6539\u8fdb\uff0c\u7ed3\u5408\u91cf\u5b50\u4e0e\u7ecf\u5178\u8d44\u6e90\u534f\u540c\u4f18\u5316\u3002", "result": "\u6784\u5efa\u4e86\u591a\u5c42\u6b21\u5b9e\u7528\u65b9\u6cd5\u6846\u67b6\uff0c\u63d0\u4f9b\u786c\u4ef6\u7279\u5b9a\u4f18\u5316\u3001\u5148\u8fdb\u8bef\u5dee\u7f13\u89e3\u6280\u672f\u548c\u6d4b\u91cf\u9ad8\u6548\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u8fd1\u671f\u542b\u566a\u58f0\u91cf\u5b50\u786c\u4ef6\u4e0a\u8fdb\u884c\u91cf\u5b50\u6a21\u62df\u7684\u51c6\u786e\u6027\u548c\u53ef\u884c\u6027\u3002", "conclusion": "\u901a\u8fc7\u89e3\u51b3\u91cf\u5b50\u8ba1\u7b97\u6808\u5404\u5c42\u9762\u7684\u6311\u6218\uff0c\u8be5\u5de5\u4f5c\u589e\u5f3a\u4e86\u5f53\u524d\u542b\u566a\u58f0\u8bbe\u5907\u5b9e\u73b0\u91cf\u5b50\u6a21\u62df\u7684\u5b9e\u7528\u6027\uff0c\u4e3a\u672a\u6765\u5bb9\u9519\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\uff0c\u6709\u6548\u63a8\u52a8\u4e86\u4ece\u7406\u8bba\u6f5c\u529b\u5411\u5b9e\u9645\u5e94\u7528\u7684\u8fc7\u6e21\u3002"}}
{"id": "2602.04074", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.04074", "abs": "https://arxiv.org/abs/2602.04074", "authors": ["Julius Fridriksson", "Roger D. Newman-Norlund", "Saeed Ahmadi", "Regan Willis", "Nadra Salman", "Kalil Warren", "Xiang Guan", "Yong Yang", "Srihari Nelakuditi", "Rutvik Desai", "Leonardo Bonilha", "Jeff Charney", "Chris Rorden"], "title": "Stroke Lesions as a Rosetta Stone for Language Model Interpretability", "comment": "45 pages, 17 figures", "summary": "Large language models (LLMs) have achieved remarkable capabilities, yet methods to verify which model components are truly necessary for language function remain limited. Current interpretability approaches rely on internal metrics and lack external validation. Here we present the Brain-LLM Unified Model (BLUM), a framework that leverages lesion-symptom mapping, the gold standard for establishing causal brain-behavior relationships for over a century, as an external reference structure for evaluating LLM perturbation effects. Using data from individuals with chronic post-stroke aphasia (N = 410), we trained symptom-to-lesion models that predict brain damage location from behavioral error profiles, applied systematic perturbations to transformer layers, administered identical clinical assessments to perturbed LLMs and human patients, and projected LLM error profiles into human lesion space. LLM error profiles were sufficiently similar to human error profiles that predicted lesions corresponded to actual lesions in error-matched humans above chance in 67% of picture naming conditions (p < 10^{-23}) and 68.3% of sentence completion conditions (p < 10^{-61}), with semantic-dominant errors mapping onto ventral-stream lesion patterns and phonemic-dominant errors onto dorsal-stream patterns. These findings open a new methodological avenue for LLM interpretability in which clinical neuroscience provides external validation, establishing human lesion-symptom mapping as a reference framework for evaluating artificial language systems and motivating direct investigation of whether behavioral alignment reflects shared computational principles.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faBLUM\u6846\u67b6\uff0c\u5229\u7528\u5931\u8bed\u75c7\u60a3\u8005\u7684\u8111\u635f\u4f24-\u75c7\u72b6\u6620\u5c04\u4f5c\u4e3a\u5916\u90e8\u9a8c\u8bc1\uff0c\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u54ea\u4e9b\u7ec4\u4ef6\u5bf9\u8bed\u8a00\u529f\u80fd\u771f\u6b63\u5fc5\u8981\uff0c\u53d1\u73b0\u6a21\u578b\u9519\u8bef\u4e0e\u8111\u635f\u4f24\u6a21\u5f0f\u9ad8\u5ea6\u4e00\u81f4\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u7f3a\u4e4f\u5916\u90e8\u9a8c\u8bc1\uff0c\u4ec5\u4f9d\u8d56\u5185\u90e8\u6307\u6807\u3002\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528\u4e34\u5e8a\u795e\u7ecf\u79d1\u5b66\u4e2d\u767e\u5e74\u6765\u7684\u91d1\u6807\u51c6\u2014\u2014\u8111\u635f\u4f24-\u75c7\u72b6\u6620\u5c04\uff0c\u6765\u5efa\u7acb\u5bf9\u6a21\u578b\u8bed\u8a00\u7ec4\u4ef6\u7684\u56e0\u679c\u7406\u89e3\u3002", "method": "BLUM\u6846\u67b6\u4f7f\u7528410\u540d\u4e2d\u98ce\u540e\u5931\u8bed\u75c7\u60a3\u8005\u7684\u8bad\u7ec3\u75c7\u72b6-\u635f\u4f24\u6a21\u578b\uff0c\u7cfb\u7edf\u6027\u5730\u6270\u52a8Transformer\u5c42\uff0c\u5bf9\u6270\u52a8\u540e\u7684\u6a21\u578b\u548c\u4eba\u7c7b\u60a3\u8005\u65bd\u6d4b\u76f8\u540c\u7684\u4e34\u5e8a\u8bc4\u4f30\uff0c\u5e76\u5c06\u6a21\u578b\u9519\u8bef\u6a21\u5f0f\u6620\u5c04\u5230\u4eba\u7c7b\u8111\u635f\u4f24\u7a7a\u95f4\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u6a21\u578b\u9519\u8bef\u6a21\u5f0f\u4e0e\u4eba\u7c7b\u9519\u8bef\u6a21\u5f0f\u663e\u8457\u5339\u914d\uff0c\u9884\u6d4b\u7684\u8111\u635f\u4f24\u4f4d\u7f6e\u4e0e\u771f\u5b9e\u635f\u4f24\u572867%\u7684\u56fe\u7247\u547d\u540d\u6761\u4ef6\uff08p < 10^-23\uff09\u548c68.3%\u7684\u53e5\u5b50\u5b8c\u6210\u6761\u4ef6\uff08p < 10^-61\uff09\u4e2d\u9ad8\u4e8e\u968f\u673a\u6c34\u5e73\u3002\u8bed\u4e49\u9519\u8bef\u4e3b\u8981\u6620\u5c04\u5230\u8179\u4fa7\u6d41\u635f\u4f24\uff0c\u8bed\u97f3\u9519\u8bef\u4e3b\u8981\u6620\u5c04\u5230\u80cc\u4fa7\u6d41\u635f\u4f24\u3002", "conclusion": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u4eba\u7c7b\u8111\u635f\u4f24-\u75c7\u72b6\u6620\u5c04\u4f5c\u4e3a\u8bc4\u4f30\u4eba\u5de5\u8bed\u8a00\u7cfb\u7edf\u7684\u53c2\u8003\u6846\u67b6\uff0c\u4e3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u5f00\u8f9f\u4e86\u65b0\u65b9\u6cd5\uff0c\u8868\u660e\u6a21\u578b\u4e0e\u4eba\u7c7b\u7684\u8bed\u8a00\u884c\u4e3a\u4e00\u81f4\u6027\u53ef\u80fd\u53cd\u6620\u4e86\u5171\u540c\u7684\u8ba1\u7b97\u539f\u7406\u3002"}}
{"id": "2602.04721", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04721", "abs": "https://arxiv.org/abs/2602.04721", "authors": ["Hongze Ding", "Jiuqing Liang"], "title": "Ising-Induced Spectral Broadening Resolves the Relaxation Bottleneck in Superradiant Masers", "comment": "5 pages, 1 figure", "summary": "The recent observation of self-induced superradiant masing [[W. Kersten et al., Nat. Phys. 22, 158 (2026)]] revealed a collective relaxation timescale significantly slower than predicted by standard coherent transport models. Here, we elucidate the microscopic origin of this ``relaxation bottleneck.'' We show that in the high-density regime relevant to the experiment, diagonal Ising interactions -- often treated as perturbative -- generate profound inhomogeneous broadening that exceeds the intrinsic single-particle dephasing. This intense diagonal disorder suppresses resonant flip-flop exchange, effectively renormalizing the density of states available for spectral diffusion. Our parameter-free analytic theory quantitatively reproduces the experimentally observed microsecond dynamics, identifying Ising-induced broadening as the governing mechanism for energy transport in dense solid-state spin ensembles.", "AI": {"tldr": "\u4f0a\u8f9b\u76f8\u4e92\u4f5c\u7528\u5f15\u8d77\u7684\u975e\u5747\u5300\u5c55\u5bbd\u5728\u5bc6\u96c6\u81ea\u65cb\u7cfb\u7efc\u4e2d\u4ea7\u751f\u5f1b\u8c6b\u74f6\u9888\uff0c\u89e3\u91ca\u4e86\u8d85\u8f90\u5c04\u6fc0\u5c04\u7684\u7f13\u6162\u52a8\u529b\u5b66\u3002", "motivation": "\u89e3\u91ca\u81ea\u8bf1\u5bfc\u8d85\u8f90\u5c04\u6fc0\u5c04\u5b9e\u9a8c\u4e2d\u89c2\u5bdf\u5230\u7684\u96c6\u4f53\u5f1b\u8c6b\u65f6\u95f4\u5c3a\u5ea6\u6bd4\u6807\u51c6\u76f8\u5e72\u8f93\u8fd0\u6a21\u578b\u9884\u6d4b\u663e\u8457\u66f4\u6162\u7684\u73b0\u8c61\u3002", "method": "\u89e3\u6790\u7406\u8bba\u8868\u660e\uff0c\u5728\u9ad8\u5bc6\u5ea6\u6761\u4ef6\u4e0b\uff0c\u5bf9\u89d2\u4f0a\u8f9b\u76f8\u4e92\u4f5c\u7528\u4ea7\u751f\u8d85\u8fc7\u5355\u7c92\u5b50\u9000\u76f8\u4f4d\u7684\u975e\u5747\u5300\u5c55\u5bbd\uff0c\u6291\u5236\u5171\u632f\u7ffb\u8f6c\u4ea4\u6362\uff0c\u6709\u6548\u91cd\u6574\u5316\u8c31\u6269\u6563\u7684\u53ef\u7528\u6001\u5bc6\u5ea6\u3002", "result": "\u65e0\u53c2\u6570\u89e3\u6790\u7406\u8bba\u5b9a\u91cf\u91cd\u73b0\u4e86\u5b9e\u9a8c\u89c2\u6d4b\u7684\u5fae\u79d2\u7ea7\u52a8\u529b\u5b66\u3002", "conclusion": "\u4f0a\u8f9b\u8bf1\u5bfc\u7684\u5c55\u5bbd\u662f\u5bc6\u96c6\u56fa\u6001\u81ea\u65cb\u7cfb\u7efc\u4e2d\u80fd\u91cf\u8f93\u8fd0\u7684\u4e3b\u5bfc\u673a\u5236\u3002"}}
{"id": "2602.04740", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04740", "abs": "https://arxiv.org/abs/2602.04740", "authors": ["Felip Pellicer", "Juan Jos\u00e9 Garc\u00eda-Ripoll", "Alan C. Santos"], "title": "Resource-Efficient Digitized Adiabatic Quantum Factorization", "comment": "13 pages, 3 figures and 1 table. Comments are welcome", "summary": "Digitized adiabatic quantum factorization is a hybrid algorithm that exploits the advantage of digitized quantum computers to implement efficient adiabatic algorithms for factorization through gate decompositions of analog evolutions. In this paper, we harness the flexibility of digitized computers to derive a digitized adiabatic algorithm able to reduce the gate-demanding costs of implementing factorization. To this end, we propose a new approach for adiabatic factorization by encoding the solution of the problem in the kernel subspace of the problem Hamiltonian, instead of using ground-state encoding considered in the standard adiabatic factorization proposed by Peng $et$ $al$. [Phys. Rev. Lett. 101, 220405 (2008)]. Our encoding enables the design of adiabatic factorization algorithms belonging to the class of Quadratic Unconstrained Binary Optimization (QUBO) methods, instead the Polinomial Unconstrained Binary Optimization (PUBO) used by standard adiabatic factorization. We illustrate the performance of our QUBO algorithm by implementing the factorization of integers $N$ up to 8 bits. The results demonstrate a substantial improvement over the PUBO formulation, both in terms of reduced circuit complexity and increased fidelity in identifying the correct solution.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6570\u5b57\u5316\u7edd\u70ed\u91cf\u5b50\u56e0\u6570\u5206\u89e3\u7b97\u6cd5\uff0c\u901a\u8fc7\u6838\u5b50\u7a7a\u95f4\u7f16\u7801\u66ff\u4ee3\u57fa\u6001\u7f16\u7801\uff0c\u5c06\u95ee\u9898\u8f6c\u5316\u4e3aQUBO\u5f62\u5f0f\u800c\u975ePUBO\uff0c\u57288\u4f4d\u6574\u6570\u5206\u89e3\u4e2d\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u7535\u8def\u590d\u6742\u5ea6\u548c\u66f4\u9ad8\u7684\u4fdd\u771f\u5ea6\u3002", "motivation": "\u6807\u51c6\u7edd\u70ed\u91cf\u5b50\u56e0\u6570\u5206\u89e3\u7b97\u6cd5\uff08Peng et al., 2008\uff09\u91c7\u7528\u57fa\u6001\u7f16\u7801\uff0c\u5bfc\u81f4PUBO\u5f62\u5f0f\u548c\u9ad8\u95e8\u7535\u8def\u9700\u6c42\uff0c\u9650\u5236\u4e86\u5176\u5728\u6570\u5b57\u5316\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u7684\u5b9e\u73b0\u6548\u7387\u3002", "method": "\u5229\u7528\u6570\u5b57\u5316\u91cf\u5b50\u8ba1\u7b97\u673a\u7684\u7075\u6d3b\u6027\uff0c\u63d0\u51fa\u6838\u5b50\u7a7a\u95f4\u7f16\u7801\u65b9\u6cd5\uff0c\u5c06\u89e3\u7f16\u7801\u5728\u95ee\u9898\u54c8\u5bc6\u987f\u91cf\u7684\u6838\u5b50\u7a7a\u95f4\u4e2d\uff0c\u4ece\u800c\u8bbe\u8ba1\u5c5e\u4e8eQUBO\u7c7b\u7684\u7edd\u70ed\u91cf\u5b50\u56e0\u6570\u5206\u89e3\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u5206\u89e3\u6700\u591a8\u4f4d\u6574\u6570\u7684\u5b9e\u9a8c\uff0c\u65b0\u7b97\u6cd5\u76f8\u6bd4PUBO\u65b9\u6848\u663e\u8457\u964d\u4f4e\u4e86\u7535\u8def\u590d\u6742\u5ea6\uff0c\u5e76\u63d0\u9ad8\u4e86\u8bc6\u522b\u6b63\u786e\u89e3\u7684\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u6838\u5b50\u7a7a\u95f4\u7f16\u7801\u7b56\u7565\u6709\u6548\u4f18\u5316\u4e86\u6570\u5b57\u5316\u7edd\u70ed\u91cf\u5b50\u56e0\u6570\u5206\u89e3\u7684\u6027\u80fd\uff0c\u4e3a\u5728\u73b0\u6709\u91cf\u5b50\u786c\u4ef6\u4e0a\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u56e0\u6570\u5206\u89e3\u7b97\u6cd5\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.04082", "categories": ["cs.LG", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.04082", "abs": "https://arxiv.org/abs/2602.04082", "authors": ["Yicheng Zou", "Samuel Lanthaler", "Hossein Salahshoor"], "title": "A Probabilistic Framework for Solving High-Frequency Helmholtz Equations via Diffusion Models", "comment": null, "summary": "Deterministic neural operators perform well on many PDEs but can struggle with the approximation of high-frequency wave phenomena, where strong input-to-output sensitivity makes operator learning challenging, and spectral bias blurs oscillations. We argue for adopting a probabilistic approach for approximating waves in high-frequency regime, and develop our probabilistic framework using a score-based conditional diffusion operator. After demonstrating a stability analysis of the Helmholtz operator, we present our numerical experiments across a wide range of frequencies, benchmarked against other popular data-driven and machine learning approaches for waves. We show that our probabilistic neural operator consistently produces robust predictions with the lowest errors in $L^2$, $H^1$, and energy norms. Moreover, unlike all the other tested deterministic approaches, our framework remarkably captures uncertainties in the input sound speed map propagated to the solution field. We envision that our results position probabilistic operator learning as a principled and effective approach for solving complex PDEs such as Helmholtz in the challenging high-frequency regime.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5206\u6570\u7684\u6761\u4ef6\u6269\u6563\u7b97\u5b50\u6982\u7387\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u786e\u5b9a\u6027\u795e\u7ecf\u7b97\u5b50\u5728\u8fd1\u4f3c\u9ad8\u9891\u6ce2\u73b0\u8c61\u65f6\u7684\u56f0\u96be\uff0c\u5728Helmholtz\u65b9\u7a0b\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u5e76\u80fd\u91cf\u5316\u4e0d\u786e\u5b9a\u6027", "motivation": "\u786e\u5b9a\u6027\u795e\u7ecf\u7b97\u5b50\u5728\u5904\u7406\u9ad8\u9891\u6ce2\u73b0\u8c61\u65f6\u9762\u4e34\u8f93\u5165\u8f93\u51fa\u5f3a\u654f\u611f\u6027\u3001\u8c31\u504f\u5dee\u5bfc\u81f4\u632f\u8361\u6a21\u7cca\u7b49\u95ee\u9898\uff0c\u96be\u4ee5\u51c6\u786e\u6355\u83b7\u9ad8\u9891\u6ce2\u4f20\u64ad\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027", "method": "\u91c7\u7528\u57fa\u4e8e\u5206\u6570\u7684\u6761\u4ef6\u6269\u6563\u7b97\u5b50\u6784\u5efa\u6982\u7387\u795e\u7ecf\u7b97\u5b50\uff0c\u5148\u5bf9Helmholtz\u7b97\u5b50\u8fdb\u884c\u7a33\u5b9a\u6027\u5206\u6790\uff0c\u518d\u5728\u4e0d\u540c\u9891\u7387\u4e0b\u8fdb\u884c\u6570\u503c\u5b9e\u9a8c\uff0c\u5e76\u4e0e\u591a\u79cd\u6570\u636e\u9a71\u52a8\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5bf9\u6bd4", "result": "\u5728L\u00b2\u3001H\u00b9\u548c\u80fd\u91cf\u8303\u6570\u4e0a\u6301\u7eed\u83b7\u5f97\u6700\u4f4e\u8bef\u5dee\uff0c\u4e14\u552f\u4e00\u80fd\u6355\u83b7\u8f93\u5165\u58f0\u901f\u56fe\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u5230\u89e3\u573a\u7684\u7279\u6027\uff0c\u9884\u6d4b\u7ed3\u679c\u66f4\u52a0\u9c81\u68d2", "conclusion": "\u6982\u7387\u7b97\u5b50\u5b66\u4e60\u4e3a\u9ad8\u9891 regime \u4e0b\u6c42\u89e3Helmholtz\u7b49\u590d\u6742PDE\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u4e14\u6709\u6548\u7684\u65b0\u9014\u5f84\uff0c\u5177\u6709\u91cd\u8981\u5e94\u7528\u524d\u666f"}}
{"id": "2602.04747", "categories": ["quant-ph", "math-ph", "nlin.SI"], "pdf": "https://arxiv.org/pdf/2602.04747", "abs": "https://arxiv.org/abs/2602.04747", "authors": ["Bijan Bagchi", "Anindya Ghose-Choudhury"], "title": "Generalized quantum theory for accessing nonlinear systems: the case of Levinson-Smith equations", "comment": "8 pages, 2 figures", "summary": "Motivated by a recently developed generalized scheme of quantum mechanics, we touch upon connections with Levinson-Smith classes of nonlinear systems that contain as a particular case the Li\u00e9nard family of differential equations. The latter, which has coefficients of odd and odd symmetry, admits a closed form solution when converted to the Abel form. Analysis of the governing condition shows that one of the nontrivial equilibrium points is stable in character. Other classes of differential equations that we encounter speak of solutions involving Jacobi elliptic functions for a certain combination of underlying parameters, while, for a different set, relevance to position-dependent mass systems is shown. In addition, an interesting off-shoot of our results is the emergence of solitonic-like solutions from the condition of the level surface in the system.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u5e7f\u4e49\u91cf\u5b50\u529b\u5b66\u65b9\u6848\u4e0eLevinson-Smith\u975e\u7ebf\u6027\u7cfb\u7edf\uff08\u542bLi\u00e9nard\u65b9\u7a0b\uff09\u5efa\u7acb\u8054\u7cfb\uff0c\u901a\u8fc7Abel\u5f62\u5f0f\u8f6c\u5316\u83b7\u5f97\u95ed\u5f0f\u89e3\uff0c\u63ed\u793a\u7a33\u5b9a\u5e73\u8861\u70b9\u3001Jacobi\u692d\u5706\u51fd\u6570\u89e3\u3001\u4f4d\u7f6e\u76f8\u5173\u8d28\u91cf\u7cfb\u7edf\u5173\u8054\u53ca\u7c7b\u5b64\u5b50\u89e3", "motivation": "\u53d7\u5e7f\u4e49\u91cf\u5b50\u529b\u5b66\u65b0\u65b9\u6848\u7684\u542f\u53d1\uff0c\u63a2\u7d22\u5176\u4e0eLevinson-Smith\u975e\u7ebf\u6027\u7cfb\u7edf\uff08\u7279\u522b\u662f\u5177\u6709\u5947\u5076\u5bf9\u79f0\u7cfb\u6570\u7684Li\u00e9nard\u65b9\u7a0b\u65cf\uff09\u7684\u6df1\u5c42\u8054\u7cfb", "method": "\u5c06Li\u00e9nard\u65b9\u7a0b\u8f6c\u5316\u4e3aAbel\u5fae\u5206\u65b9\u7a0b\u5f62\u5f0f\u4ee5\u83b7\u53d6\u95ed\u5f0f\u89e3\uff0c\u5206\u6790\u7cfb\u7edf\u5e73\u8861\u70b9\u7a33\u5b9a\u6027\uff0c\u5e76\u7814\u7a76\u4e0d\u540c\u53c2\u6570\u7ec4\u5408\u4e0b\u7684\u89e3\u7ed3\u6784\uff08Jacobi\u692d\u5706\u51fd\u6570/\u4f4d\u7f6e\u76f8\u5173\u8d28\u91cf\u7cfb\u7edf\uff09", "result": "1) \u53d1\u73b0\u975e\u5e73\u51e1\u5e73\u8861\u70b9\u7684\u7a33\u5b9a\u6027\u7279\u5f81\uff1b2) \u7279\u5b9a\u53c2\u6570\u4e0b\u51fa\u73b0Jacobi\u692d\u5706\u51fd\u6570\u89e3\uff1b3) \u90e8\u5206\u7cfb\u7edf\u5bf9\u5e94\u4f4d\u7f6e\u76f8\u5173\u8d28\u91cf\u91cf\u5b50\u4f53\u7cfb\uff1b4) \u80fd\u7ea7\u9762\u6761\u4ef6\u610f\u5916\u5bfc\u51fa\u7c7b\u5b64\u5b50\u89e3", "conclusion": "\u5e7f\u4e49\u91cf\u5b50\u529b\u5b66\u6846\u67b6\u53ef\u7edf\u4e00\u63cf\u8ff0\u591a\u79cd\u975e\u7ebf\u6027\u7cfb\u7edf\uff0c\u5176\u6570\u5b66\u7ed3\u6784\u4e0d\u4ec5\u4ea7\u751f\u7a33\u5b9a\u89e3\u548c\u7279\u6b8a\u51fd\u6570\u89e3\uff0c\u8fd8\u81ea\u7136\u8574\u542b\u7c7b\u5b64\u5b50\u884c\u4e3a\uff0c\u62d3\u5c55\u4e86\u91cf\u5b50\u7406\u8bba\u4e0e\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7684\u4ea4\u53c9\u5e94\u7528"}}
{"id": "2602.04093", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04093", "abs": "https://arxiv.org/abs/2602.04093", "authors": ["Dario Fenoglio", "Arianna Casanova", "Francesco De Santis", "Mohan Li", "Gabriele Dominici", "Johannes Schneider", "Martin Gjoreski", "Marc Langheinrich", "Pietro Barbiero", "Giovanni De Felice"], "title": "Federated Concept-Based Models: Interpretable models with distributed supervision", "comment": null, "summary": "Concept-based models (CMs) enhance interpretability in deep learning by grounding predictions in human-understandable concepts. However, concept annotations are expensive to obtain and rarely available at scale within a single data source. Federated learning (FL) could alleviate this limitation by enabling cross-institutional training that leverages concept annotations distributed across multiple data owners. Yet, FL lacks interpretable modeling paradigms. Integrating CMs with FL is non-trivial: CMs assume a fixed concept space and a predefined model architecture, whereas real-world FL is heterogeneous and non-stationary, with institutions joining over time and bringing new supervision. In this work, we propose Federated Concept-based Models (F-CMs), a new methodology for deploying CMs in evolving FL settings. F-CMs aggregate concept-level information across institutions and efficiently adapt the model architecture in response to changes in the available concept supervision, while preserving institutional privacy. Empirically, F-CMs preserve the accuracy and intervention effectiveness of training settings with full concept supervision, while outperforming non-adaptive federated baselines. Notably, F-CMs enable interpretable inference on concepts not available to a given institution, a key novelty with respect to existing approaches.", "AI": {"tldr": "\u63d0\u51fa\u8054\u90a6\u6982\u5ff5\u6a21\u578b(F-CMs)\uff0c\u5c06\u6982\u5ff5\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u8054\u90a6\u5b66\u4e60\u7684\u9690\u79c1\u4fdd\u62a4\u7ed3\u5408\uff0c\u52a8\u6001\u9002\u5e94\u673a\u6784\u52a0\u5165\u548c\u6982\u5ff5\u76d1\u7763\u53d8\u5316\u3002", "motivation": "\u6982\u5ff5\u6a21\u578b\u9700\u8981\u6602\u8d35\u6807\u6ce8\uff0c\u8054\u90a6\u5b66\u4e60\u53ef\u8de8\u673a\u6784\u8bad\u7ec3\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u4e14\u771f\u5b9e\u8054\u90a6\u573a\u666f\u5f02\u6784\u3001\u975e\u5e73\u7a33\uff0c\u73b0\u6709CMs\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u6982\u5ff5\u7a7a\u95f4\u548c\u67b6\u6784\u3002", "method": "F-CMs\u805a\u5408\u8de8\u673a\u6784\u6982\u5ff5\u7ea7\u4fe1\u606f\uff0c\u52a8\u6001\u8c03\u6574\u6a21\u578b\u67b6\u6784\u4ee5\u54cd\u5e94\u6982\u5ff5\u76d1\u7763\u53d8\u5316\uff0c\u540c\u65f6\u4fdd\u62a4\u673a\u6784\u9690\u79c1\u3002", "result": "F-CMs\u5728\u7cbe\u5ea6\u548c\u5e72\u9884\u6548\u679c\u4e0a\u5ab2\u7f8e\u5168\u76d1\u7763\u8bad\u7ec3\uff0c\u4f18\u4e8e\u975e\u81ea\u9002\u5e94\u8054\u90a6\u57fa\u7ebf\uff0c\u4e14\u80fd\u5bf9\u5355\u4e2a\u673a\u6784\u4e0d\u53ef\u89c1\u7684\u6982\u5ff5\u8fdb\u884c\u53ef\u89e3\u91ca\u63a8\u7406\u3002", "conclusion": "F-CMs\u6210\u529f\u5728\u52a8\u6001\u771f\u5b9e\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e2d\u5b9e\u73b0\u53ef\u89e3\u91ca\u5efa\u6a21\uff0c\u4f7f\u673a\u6784\u65e0\u9700\u5171\u4eab\u539f\u59cb\u6570\u636e\u5373\u53ef\u5171\u4eab\u6982\u5ff5\u77e5\u8bc6\u3002"}}
{"id": "2602.04760", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04760", "abs": "https://arxiv.org/abs/2602.04760", "authors": ["Francois Payn", "Michele Minervini", "Davide Girolami"], "title": "Quantifying the Operational Cost of Multipartite Entanglement", "comment": "9 pages", "summary": "Multipartite entanglement determines the strength and range of interactions in many-body quantum systems. Yet, it is hard to evaluate it, due to the complex structures of quantum states. Here, we introduce a generic method to quantify the k <= N-partite entanglement of an N-particle system, by maximizing an arbitrary bipartite entanglement measure within subsystems of size up to k. The resulting classification of multipartite states captures their experimental cost: creating a k-partite entangled state requires at least k-1 two-particle entangling gates. Further, we analytically calculate the newly defined k-partite entanglement of formation, which generalizes an important bipartite entanglement measure, in several classes of states, including the W states of any dimension.", "AI": {"tldr": "\u63d0\u51fa\u91cf\u5316\u591a\u7c92\u5b50\u7cfb\u7edfk\u2264N\u90e8\u5206\u7ea0\u7f20\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5927\u5316\u5b50\u7cfb\u7edf\u4e8c\u5206\u7ea0\u7f20\u5ea6\u91cf\u5b9e\u73b0\uff0c\u63ed\u793ak\u90e8\u5206\u7ea0\u7f20\u6001\u81f3\u5c11\u9700\u8981k-1\u4e2a\u53cc\u7c92\u5b50\u7ea0\u7f20\u95e8\u521b\u5efa", "motivation": "\u591a\u7c92\u5b50\u7ea0\u7f20\u51b3\u5b9a\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u7684\u76f8\u4e92\u4f5c\u7528\u5f3a\u5ea6\u548c\u8303\u56f4\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u56e0\u91cf\u5b50\u6001\u590d\u6742\u7ed3\u6784\u800c\u96be\u4ee5\u6709\u6548\u8bc4\u4f30", "method": "\u901a\u8fc7\u6700\u5927\u5316\u4efb\u610f\u4e8c\u5206\u7ea0\u7f20\u5ea6\u91cf\u5728\u81f3\u591ak\u7c92\u5b50\u5b50\u7cfb\u7edf\u5185\u7684\u503c\uff0c\u5b9a\u4e49k\u90e8\u5206\u7ea0\u7f20\u5ea6\u91cf\uff0c\u5efa\u7acb\u72b6\u6001\u5206\u7c7b\u4e0e\u5b9e\u9a8c\u6210\u672c\u7684\u5173\u8054", "result": "\u8bc1\u660e\u521b\u5efak\u90e8\u5206\u7ea0\u7f20\u6001\u81f3\u5c11\u9700\u8981k-1\u4e2a\u53cc\u7c92\u5b50\u7ea0\u7f20\u95e8\uff1b\u89e3\u6790\u8ba1\u7b97\u4e86\u65b0\u5b9a\u4e49\u7684k\u90e8\u5206\u5f62\u6210\u7ea0\u7f20\u5728W\u6001\u7b49\u5178\u578b\u6001\u4e2d\u7684\u503c", "conclusion": "\u8be5\u5206\u7c7b\u65b9\u6cd5\u80fd\u6709\u6548\u8868\u5f81\u91cf\u5b50\u6001\u7684\u5b9e\u9a8c\u5236\u5907\u6210\u672c\uff0c\u4e3a\u591a\u4f53\u91cf\u5b50\u7cfb\u7edf\u7ea0\u7f20\u8d44\u6e90\u8bc4\u4f30\u63d0\u4f9b\u65b0\u7406\u8bba\u6846\u67b6"}}
{"id": "2602.04096", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04096", "abs": "https://arxiv.org/abs/2602.04096", "authors": ["Kevin Zhai", "Sabbir Mollah", "Zhenyi Wang", "Mubarak Shah"], "title": "CoRe: Context-Robust Remasking for Diffusion Language Models", "comment": null, "summary": "Standard decoding in Masked Diffusion Models (MDMs) is hindered by context rigidity: tokens are retained based on transient high confidence, often ignoring that early predictions lack full context. This creates cascade effects where initial inconsistencies misguide the remaining generation. Existing revision strategies attempt to mitigate this by relying on static confidence scores, but these signals are inherently myopic; inconsistent tokens can appear confident to the model itself. We propose Context-Robust Remasking (CoRe), a training-free framework for inference-time revision. Rather than trusting static token probabilities, CoRe identifies context-brittle tokens by probing their sensitivity to targeted masked-context perturbations. We formalize revision as a robust optimization objective over context shifts and efficiently approximate this objective to prioritize unstable tokens for revision. On LLaDA-8B-Base, CoRe delivers consistent improvements across reasoning and code benchmarks, outperforming compute-matched baselines and improving MBPP by up to 9.2 percentage points.", "AI": {"tldr": "\u63d0\u51faContext-Robust Remasking (CoRe)\u6846\u67b6\uff0c\u901a\u8fc7\u63a2\u6d4b\u6807\u8bb0\u5bf9\u63a9\u7801\u4e0a\u4e0b\u6587\u6270\u52a8\u7684\u654f\u611f\u6027\u6765\u8bc6\u522b\u4e0a\u4e0b\u6587\u8106\u5f31\u6807\u8bb0\uff0c\u5b9e\u73b0\u65e0\u9700\u8bad\u7ec3\u7684\u6297\u7ea7\u8054\u9519\u8bef\u4fee\u8ba2\u3002", "motivation": "\u6807\u51c6\u89e3\u7801\u5b58\u5728\u4e0a\u4e0b\u6587\u50f5\u5316\u95ee\u9898\uff1a\u65e9\u671f\u9884\u6d4b\u7f3a\u4e4f\u5b8c\u6574\u4e0a\u4e0b\u6587\uff0c\u5bfc\u81f4\u521d\u59cb\u4e0d\u4e00\u81f4\u4f1a\u8bef\u5bfc\u540e\u7eed\u751f\u6210\uff0c\u5f62\u6210\u7ea7\u8054\u9519\u8bef\u3002\u73b0\u6709\u9759\u6001\u7f6e\u4fe1\u5ea6\u4fee\u8ba2\u7b56\u7565\u5b58\u5728\u77ed\u89c6\u7f3a\u9677\u3002", "method": "CoRe\u6846\u67b6\u5c06\u4fee\u8ba2\u5f62\u5f0f\u5316\u4e3a\u4e0a\u4e0b\u6587\u504f\u79fb\u4e0a\u7684\u9c81\u68d2\u4f18\u5316\u76ee\u6807\uff0c\u901a\u8fc7\u76ee\u6807\u63a9\u7801\u4e0a\u4e0b\u6587\u6270\u52a8\u9ad8\u6548\u63a2\u6d4b\u6807\u8bb0\u7a33\u5b9a\u6027\uff0c\u4f18\u5148\u4fee\u8ba2\u4e0d\u7a33\u5b9a\u6807\u8bb0\u3002", "result": "\u5728LLaDA-8B-Base\u4e0a\u5b9e\u73b0\u6301\u7eed\u6539\u8fdb\uff0c\u5728\u63a8\u7406\u548c\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u8ba1\u7b97\u5339\u914d\u57fa\u7ebf\uff0cMBPP\u6700\u9ad8\u63d0\u53479.2\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u8be5\u65e0\u9700\u8bad\u7ec3\u7684\u63a8\u7406\u65f6\u4fee\u8ba2\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u4e0a\u4e0b\u6587\u8106\u5f31\u6027\u5bfc\u81f4\u7684\u7ea7\u8054\u9519\u8bef\u95ee\u9898\uff0c\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2602.04099", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.04099", "abs": "https://arxiv.org/abs/2602.04099", "authors": ["Letian Cheng", "Junyan Wang", "Yan Gao", "Elliott Wen", "Ting Dang", "Hong Jia"], "title": "Rethinking Perplexity: Revealing the Impact of Input Length on Perplexity Evaluation in LLMs", "comment": null, "summary": "Perplexity is a widely adopted metric for assessing the predictive quality of large language models (LLMs) and often serves as a reference metric for downstream evaluations. However, recent evidence shows that perplexity can be unreliable, especially when irrelevant long inputs are used, raising concerns for both benchmarking and system deployment. While prior efforts have employed selective input filtering and curated datasets, the impact of input length on perplexity has not been systematically studied from a systems perspective and input length has rarely been treated as a first-class system variable affecting both fairness and efficiency. In this work, we close this gap by introducing LengthBenchmark, a system-conscious evaluation framework that explicitly integrates input length, evaluation protocol design, and system-level costs, evaluating representative LLMs under two scoring protocols (direct accumulation and fixed window sliding) across varying context lengths. Unlike prior work that focuses solely on accuracy-oriented metrics, LengthBenchmark additionally measures latency, memory footprint, and evaluation cost, thereby linking predictive metrics to deployment realities. We further incorporate quantized variants not as a main contribution, but as robustness checks, showing that length-induced biases persist across both full-precision and compressed models. This design disentangles the effects of evaluation logic, quantization, and input length, and demonstrates that length bias is a general phenomenon that undermines fair cross-model comparison. Our analysis yields two key observations: (i) sliding window evaluation consistently inflates performance on short inputs, and (ii) both full-precision and quantized models appear to realise gains as the evaluated segment length grows.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faLengthBenchmark\u7cfb\u7edf\u611f\u77e5\u8bc4\u4f30\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u8f93\u5165\u957f\u5ea6\u4f5c\u4e3a\u4e00\u7ea7\u7cfb\u7edf\u53d8\u91cf\uff0c\u63ed\u793a\u957f\u5ea6\u504f\u5dee\u662f\u5f71\u54cdLLM\u516c\u5e73\u6bd4\u8f83\u7684\u666e\u904d\u73b0\u8c61\uff0c\u6ed1\u52a8\u7a97\u53e3\u8bc4\u4f30\u4f1a\u865a\u9ad8\u77ed\u6587\u672c\u6027\u80fd\uff0c\u4e14\u6a21\u578b\u8868\u73b0\u968f\u8bc4\u4f30\u7247\u6bb5\u957f\u5ea6\u589e\u957f\u800c\u63d0\u5347\u3002", "motivation": "\u56f0\u60d1\u5ea6\u4f5c\u4e3aLLM\u6838\u5fc3\u8bc4\u4f30\u6307\u6807\u5728\u957f\u8f93\u5165\u4e0b\u4e0d\u53ef\u9760\uff0c\u4f46\u8f93\u5165\u957f\u5ea6\u5bf9\u8bc4\u4f30\u516c\u5e73\u6027\u548c\u7cfb\u7edf\u6548\u7387\u7684\u5f71\u54cd\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u505a\u7b80\u5355\u8fc7\u6ee4\u800c\u672a\u5c06\u5176\u89c6\u4e3a\u5173\u952e\u7cfb\u7edf\u53d8\u91cf\u3002", "method": "\u5f00\u53d1LengthBenchmark\u6846\u67b6\uff0c\u5728\u4e24\u79cd\u8bc4\u5206\u534f\u8bae\uff08\u76f4\u63a5\u7d2f\u79ef\u548c\u56fa\u5b9a\u7a97\u53e3\u6ed1\u52a8\uff09\u4e0b\u8bc4\u4f30\u4ee3\u8868\u6027LLM\uff0c\u53d8\u5316\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u5e76\u6d4b\u91cf\u5ef6\u8fdf\u3001\u5185\u5b58\u5360\u7528\u548c\u8bc4\u4f30\u6210\u672c\u7b49\u7cfb\u7edf\u7ea7\u6307\u6807\uff0c\u540c\u65f6\u7eb3\u5165\u91cf\u5316\u6a21\u578b\u4f5c\u4e3a\u9c81\u68d2\u6027\u68c0\u9a8c\u3002", "result": "\u53d1\u73b0\u957f\u5ea6\u504f\u5dee\u5728\u6d6e\u70b9\u548c\u91cf\u5316\u6a21\u578b\u4e2d\u666e\u904d\u5b58\u5728\uff1b\u6ed1\u52a8\u7a97\u53e3\u8bc4\u4f30\u4f1a\u7cfb\u7edf\u6027\u865a\u9ad8\u77ed\u8f93\u5165\u6027\u80fd\uff1b\u6a21\u578b\u8868\u73b0\u968f\u8bc4\u4f30\u7247\u6bb5\u957f\u5ea6\u589e\u957f\u800c\u6301\u7eed\u63d0\u5347\uff0c\u8bc1\u660e\u957f\u5ea6\u662f\u5f71\u54cd\u8de8\u6a21\u578b\u516c\u5e73\u6bd4\u8f83\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u8f93\u5165\u957f\u5ea6\u504f\u5dee\u662f\u7834\u574fLLM\u8bc4\u4f30\u516c\u5e73\u6027\u7684\u666e\u904d\u73b0\u8c61\uff0c\u5fc5\u987b\u5c06\u5176\u4f5c\u4e3a\u4e00\u7ea7\u7cfb\u7edf\u53d8\u91cf\u7eb3\u5165\u8bc4\u4f30\u534f\u8bae\u8bbe\u8ba1\uff0c\u624d\u80fd\u5efa\u7acb\u4e0e\u90e8\u7f72\u73b0\u5b9e\u4e00\u81f4\u7684\u53ef\u9760\u57fa\u51c6\u6d4b\u8bd5\u4f53\u7cfb\u3002"}}
{"id": "2602.04831", "categories": ["quant-ph", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.04831", "abs": "https://arxiv.org/abs/2602.04831", "authors": ["Hiu Yung Wong"], "title": "Review of Superconducting Qubit Devices and Their Large-Scale Integration", "comment": null, "summary": "The superconducting qubit quantum computer is one of the most promising quantum computing architectures for large-scale integration due to its maturity and close proximity to the well-established semiconductor manufacturing infrastructure. From an education perspective, it also bridges classical microwave electronics and quantum electrodynamics. In this paper, we will review the basics of quantum computers, superconductivity, and Josephson junctions. We then introduce important technologies and concepts related to DiVincenzo's criteria, which are the necessary conditions for the superconducting qubits to work as a useful quantum computer. Firstly, we will discuss various types of superconducting qubits formed with Josephson junctions, from which we will understand the trade-off across multiple design parameters, including their noise immunity. Secondly, we will discuss different schemes to achieve entanglement gate operations, which are a major bottleneck in achieving more efficient fault-tolerant quantum computing. Thirdly, we will review readout engineering, including the implementations of the Purcell filters and quantum-limited amplifiers. Finally, we will discuss the nature and review the studies of two-level system defects, which are currently the limiting factor of qubit coherence time. DiVincenzo's criteria are only the necessary conditions for a technology to be eligible for quantum computing. To have a useful quantum computer, large-scale integration is required. We will review proposals and developments for the large-scale integration of superconducting qubit devices. By comparing with the application of electronic design automation (EDA) in semiconductors, we will also review the use of EDA in superconducting qubit quantum computer design, which is necessary for its large-scale integration.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u8d85\u5bfc\u91cf\u5b50\u6bd4\u7279\u91cf\u5b50\u8ba1\u7b97\u673a\u7684\u5173\u952e\u6280\u672f\uff0c\u5305\u62ec\u91cf\u5b50\u6bd4\u7279\u8bbe\u8ba1\u3001\u7ea0\u7f20\u95e8\u64cd\u4f5c\u3001\u8bfb\u53d6\u5de5\u7a0b\u3001\u9000\u76f8\u5e72\u673a\u5236\u548c\u5927\u89c4\u6a21\u96c6\u6210\u65b9\u6848\uff0c\u5e76\u63a2\u8ba8\u4e86\u7535\u5b50\u8bbe\u8ba1\u81ea\u52a8\u5316\u5728\u8be5\u9886\u57df\u7684\u5e94\u7528\u524d\u666f\u3002", "motivation": "\u8d85\u5bfc\u91cf\u5b50\u6bd4\u7279\u662f\u6700\u6709\u5e0c\u671b\u5b9e\u73b0\u5927\u89c4\u6a21\u96c6\u6210\u7684\u91cf\u5b50\u8ba1\u7b97\u67b6\u6784\u4e4b\u4e00\uff0c\u672c\u6587\u65e8\u5728\u4ece\u6559\u80b2\u89c6\u89d2\u68b3\u7406\u5176\u57fa\u7840\u77e5\u8bc6\uff0c\u7cfb\u7edf\u5206\u6790\u6ee1\u8db3DiVincenzo\u6807\u51c6\u7684\u5404\u9879\u6280\u672f\u6311\u6218\uff0c\u4e3a\u5927\u89c4\u6a21\u96c6\u6210\u63d0\u4f9b\u6280\u672f\u8def\u7ebf\u53c2\u8003\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u4ece\u57fa\u672c\u539f\u7406\u51fa\u53d1\uff0c\u5206\u5c42\u8ba8\u8bba\u91cf\u5b50\u6bd4\u7279\u7269\u7406\u5b9e\u73b0\u3001\u64cd\u63a7\u6280\u672f\u3001\u8bfb\u53d6\u65b9\u6848\u3001\u9000\u76f8\u5e72\u673a\u5236\u548c\u96c6\u6210\u8bbe\u8ba1\u4e94\u4e2a\u6838\u5fc3\u5c42\u9762\uff0c\u5e76\u7ed3\u5408\u534a\u5bfc\u4f53EDA\u7ecf\u9a8c\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u8be6\u7ec6\u5206\u6790\u4e86\u7ea6\u745f\u592b\u68ee\u7ed3\u91cf\u5b50\u6bd4\u7279\u7684\u7c7b\u578b\u4e0e\u53c2\u6570\u6743\u8861\u3001\u7ea0\u7f20\u95e8\u64cd\u4f5c\u74f6\u9888\u3001Purcell\u6ee4\u6ce2\u5668\u548c\u91cf\u5b50\u6781\u9650\u653e\u5927\u5668\u7b49\u8bfb\u53d6\u6280\u672f\u3001\u9650\u5236\u76f8\u5e72\u65f6\u95f4\u7684\u4e8c\u80fd\u7ea7\u7cfb\u7edf\u7f3a\u9677\uff0c\u4ee5\u53ca\u5927\u89c4\u6a21\u96c6\u6210\u67b6\u6784\u548cEDA\u8bbe\u8ba1\u5de5\u5177\u7684\u6700\u65b0\u8fdb\u5c55\u3002", "conclusion": "\u8d85\u5bfc\u91cf\u5b50\u8ba1\u7b97\u867d\u5728\u5236\u9020\u517c\u5bb9\u6027\u4e0a\u4f18\u52bf\u660e\u663e\uff0c\u4f46\u4ecd\u9762\u4e34\u9000\u76f8\u5e72\u3001\u95e8\u64cd\u4f5c\u6548\u7387\u548c\u96c6\u6210\u8bbe\u8ba1\u7b49\u6311\u6218\uff1b\u5efa\u7acb\u7c7b\u4f3c\u534a\u5bfc\u4f53\u7684EDA\u8bbe\u8ba1\u6d41\u7a0b\u662f\u63a8\u52a8\u5176\u5927\u89c4\u6a21\u96c6\u6210\u7684\u5173\u952e\u8def\u5f84\uff0c\u9700\u8981\u8de8\u5b66\u79d1\u534f\u540c\u521b\u65b0\u3002"}}
{"id": "2602.04107", "categories": ["cs.LG", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.04107", "abs": "https://arxiv.org/abs/2602.04107", "authors": ["Kosuke Sugiyama", "Masato Uchida"], "title": "Supervised Learning as Lossy Compression: Characterizing Generalization and Sample Complexity via Finite Blocklength Analysis", "comment": "22 pages, 1 figure", "summary": "This paper presents a novel information-theoretic perspective on generalization in machine learning by framing the learning problem within the context of lossy compression and applying finite blocklength analysis. In our approach, the sampling of training data formally corresponds to an encoding process, and the model construction to a decoding process. By leveraging finite blocklength analysis, we derive lower bounds on sample complexity and generalization error for a fixed randomized learning algorithm and its associated optimal sampling strategy. Our bounds explicitly characterize the degree of overfitting of the learning algorithm and the mismatch between its inductive bias and the task as distinct terms. This separation provides a significant advantage over existing frameworks. Additionally, we decompose the overfitting term to show its theoretical connection to existing metrics found in information-theoretic bounds and stability theory, unifying these perspectives under our proposed framework.", "AI": {"tldr": "\u672c\u6587\u4ece\u4fe1\u606f\u8bba\u89d2\u5ea6\u7814\u7a76\u673a\u5668\u5b66\u4e60\u6cdb\u5316\u95ee\u9898\uff0c\u5c06\u5b66\u4e60\u8fc7\u7a0b\u6846\u67b6\u4e3a\u6709\u635f\u538b\u7f29\u95ee\u9898\uff0c\u5229\u7528\u6709\u9650\u5757\u957f\u5206\u6790\u63a8\u5bfc\u51fa\u6837\u672c\u590d\u6742\u5ea6\u548c\u6cdb\u5316\u8bef\u5dee\u7684\u4e0b\u754c\uff0c\u5e76\u9996\u6b21\u5c06\u8fc7\u62df\u5408\u4e0e\u5f52\u7eb3\u504f\u7f6e\u4e0d\u5339\u914d\u5206\u79bb\u4e3a\u72ec\u7acb\u9879\uff0c\u7edf\u4e00\u4e86\u4fe1\u606f\u8bba\u8fb9\u754c\u4e0e\u7a33\u5b9a\u6027\u7406\u8bba\u3002", "motivation": "\u73b0\u6709\u6cdb\u5316\u7406\u8bba\u6846\u67b6\u96be\u4ee5\u660e\u786e\u533a\u5206\u8fc7\u62df\u5408\u4e0e\u5f52\u7eb3\u504f\u7f6e\u4e0d\u5339\u914d\u8fd9\u4e24\u4e2a\u4e0d\u540c\u6982\u5ff5\uff0c\u7f3a\u4e4f\u5bf9\u5b66\u4e60\u7b97\u6cd5\u8fc7\u62df\u5408\u7a0b\u5ea6\u7684\u7cbe\u786e\u523b\u753b\u3002", "method": "\u5c06\u8bad\u7ec3\u6570\u636e\u91c7\u6837\u5f62\u5f0f\u5316\u4e3a\u7f16\u7801\u8fc7\u7a0b\uff0c\u6a21\u578b\u6784\u5efa\u89c6\u4e3a\u89e3\u7801\u8fc7\u7a0b\uff0c\u5f15\u5165\u6709\u9650\u5757\u957f\u5206\u6790\u6280\u672f\uff0c\u5728\u56fa\u5b9a\u968f\u673a\u5b66\u4e60\u7b97\u6cd5\u53ca\u5176\u6700\u4f18\u91c7\u6837\u7b56\u7565\u4e0b\u8fdb\u884c\u5206\u6790\u3002", "result": "\u63a8\u5bfc\u51fa\u6837\u672c\u590d\u6742\u5ea6\u548c\u6cdb\u5316\u8bef\u5dee\u7684\u7406\u8bba\u4e0b\u754c\uff0c\u5f97\u5230\u4e86\u663e\u5f0f\u5206\u79bb\u7684\u8fc7\u62df\u5408\u9879\u548c\u5f52\u7eb3\u504f\u7f6e\u4e0d\u5339\u914d\u9879\uff0c\u5e76\u63ed\u793a\u4e86\u8fc7\u62df\u5408\u9879\u4e0e\u4fe1\u606f\u8bba\u8fb9\u754c\u3001\u7a33\u5b9a\u6027\u7406\u8bba\u4e2d\u73b0\u6709\u5ea6\u91cf\u7684\u7406\u8bba\u8054\u7cfb\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u66f4\u7cbe\u7ec6\u7684\u6cdb\u5316\u8bef\u5dee\u5206\u89e3\uff0c\u8fd8\u7edf\u4e00\u4e86\u4fe1\u606f\u8bba\u4e0e\u7a33\u5b9a\u6027\u7406\u8bba\u4e24\u79cd\u89c6\u89d2\uff0c\u4e3a\u7406\u89e3\u673a\u5668\u5b66\u4e60\u6cdb\u5316\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.04859", "categories": ["quant-ph", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.04859", "abs": "https://arxiv.org/abs/2602.04859", "authors": ["Pradeep Niroula", "Minzhao Liu", "Sivaprasad Omanakuttan", "David Amaro", "Shouvanik Chakrabarti", "Soumik Ghosh", "Zichang He", "Yuwei Jin", "Fatih Kaleoglu", "Steven Kordonowy", "Rohan Kumar", "Michael A. Perlin", "Akshay Seshadri", "Matthew Steinberg", "Joseph Sullivan", "Jacob Watkins", "Henry Yuen", "Ruslan Shaydulin"], "title": "Digital signatures with classical shadows on near-term quantum computers", "comment": null, "summary": "Quantum mechanics provides cryptographic primitives whose security is grounded in hardness assumptions independent of those underlying classical cryptography. However, existing proposals require low-noise quantum communication and long-lived quantum memory, capabilities which remain challenging to realize in practice. In this work, we introduce a quantum digital signature scheme that operates with only classical communication, using the classical shadows of states produced by random circuits as public keys. We provide theoretical and numerical evidence supporting the conjectured hardness of learning the private key (the circuit) from the public key (the shadow). A key technical ingredient enabling our scheme is an improved state-certification primitive that achieves higher noise tolerance and lower sample complexity than prior methods. We realize this certification by designing a high-rate error-detecting code tailored to our random-circuit ensemble and experimentally generating shadows for 32-qubit states using circuits with $\\geq 80$ logical ($\\geq 582$ physical) two-qubit gates, attaining 0.90 $\\pm$ 0.01 fidelity. With increased number of measurement samples, our hardware-demonstrated primitives realize a proof-of-principle quantum digital signature, demonstrating the near-term feasibility of our scheme.", "AI": {"tldr": "A quantum digital signature scheme using only classical communication and classical shadows of random circuits is proposed and experimentally demonstrated, achieving 0.90 fidelity and showing near-term feasibility without requiring low-noise quantum communication or long-lived quantum memory.", "motivation": "Existing quantum cryptographic primitives require low-noise quantum communication and long-lived quantum memory, which are practically challenging to implement.", "method": "Proposes a scheme using classical shadows of random circuit states as public keys, with an improved state-certification primitive based on a high-rate error-detecting code, and provides theoretical/numerical hardness evidence.", "result": "Experimentally generated shadows for 32-qubit states with \u226580 logical gates, attaining 0.90 \u00b1 0.01 fidelity, and realized a proof-of-principle quantum digital signature with increased measurement samples.", "conclusion": "The work demonstrates the near-term feasibility of quantum digital signatures by operating with only classical communication and tolerating higher noise levels."}}
{"id": "2602.04110", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04110", "abs": "https://arxiv.org/abs/2602.04110", "authors": ["Raymond Chu", "Jaewoong Choi", "Dohyun Kwon"], "title": "Rate-Optimal Noise Annealing in Semi-Dual Neural Optimal Transport: Tangential Identifiability, Off-Manifold Ambiguity, and Guaranteed Recovery", "comment": null, "summary": "Semi-dual neural optimal transport learns a transport map via a max-min objective, yet training can converge to incorrect or degenerate maps. We fully characterize these spurious solutions in the common regime where data concentrate on low-dimensional manifold: the objective is underconstrained off the data manifold, while the on-manifold transport signal remains identifiable. Following Choi, Choi, and Kwon (2025), we study additive-noise smoothing as a remedy and prove new map recovery guarantees as the noise vanishes. Our main practical contribution is a computable terminal noise level $\\varepsilon_{\\mathrm{stat}}(N)$ that attains the optimal statistical rate, with scaling governed by the intrinsic dimension $m$ of the data. The formula arises from a theoretical unified analysis of (i) quantitative stability of optimal plans, (ii) smoothing-induced bias, and (iii) finite-sample error, yielding rates that depend on $m$ rather than the ambient dimension. Finally, we show that the reduced semi-dual objective becomes increasingly ill-conditioned as $\\varepsilon \\downarrow 0$. This provides a principled stopping rule: annealing below $\\varepsilon_{\\mathrm{stat}}(N)$ can $\\textit{worsen}$ optimization conditioning without improving statistical accuracy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u534a\u5bf9\u5076\u795e\u7ecf\u6700\u4f18\u4f20\u8f93\u4e2d\u7684\u865a\u5047\u89e3\u95ee\u9898\uff0c\u63d0\u51fa\u52a0\u6027\u566a\u58f0\u5e73\u6ed1\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u7ed9\u51fa\u4e86\u4e00\u4e2a\u53ef\u8ba1\u7b97\u7684\u7ec8\u7aef\u566a\u58f0\u6c34\u5e73\u03b5_stat(N)\uff0c\u8be5\u6c34\u5e73\u80fd\u8fbe\u5230\u4ec5\u4f9d\u8d56\u4e8e\u6570\u636e\u672c\u5f81\u7ef4\u5ea6m\u7684\u6700\u4f18\u7edf\u8ba1\u901f\u7387\u3002\u540c\u65f6\u63ed\u793a\u8fc7\u5ea6\u51cf\u5c0f\u566a\u58f0\u4f1a\u4f7f\u4f18\u5316\u95ee\u9898\u75c5\u6001\uff0c\u4e3a\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u7684\u65e9\u505c\u89c4\u5219\u3002", "motivation": "\u534a\u5bf9\u5076\u795e\u7ecf\u6700\u4f18\u4f20\u8f93\u901a\u8fc7\u6700\u5927-\u6700\u5c0f\u76ee\u6807\u51fd\u6570\u5b66\u4e60\u4f20\u8f93\u6620\u5c04\uff0c\u4f46\u8bad\u7ec3\u8fc7\u7a0b\u53ef\u80fd\u6536\u655b\u5230\u9519\u8bef\u6216\u9000\u5316\u7684\u89e3\u3002\u5f53\u6570\u636e\u96c6\u4e2d\u5728\u4f4e\u7ef4\u6d41\u5f62\u65f6\uff0c\u76ee\u6807\u51fd\u6570\u5728\u6d41\u5f62\u5916\u6b20\u7ea6\u675f\uff0c\u5bfc\u81f4\u865a\u5047\u89e3\u51fa\u73b0\uff0c\u5f71\u54cd\u4e86\u65b9\u6cd5\u7684\u53ef\u9760\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "\u57fa\u4e8eChoi\u7b49\u4eba(2025)\u7684\u7814\u7a76\uff0c\u91c7\u7528\u52a0\u6027\u566a\u58f0\u5e73\u6ed1\u6280\u672f\u3002\u901a\u8fc7\u7edf\u4e00\u5206\u6790\u6700\u4f18\u8ba1\u5212\u7684\u5b9a\u91cf\u7a33\u5b9a\u6027\u3001\u5e73\u6ed1\u5f15\u5165\u7684\u504f\u5dee\u4ee5\u53ca\u6709\u9650\u6837\u672c\u8bef\u5dee\uff0c\u8bc1\u660e\u5f53\u566a\u58f0\u8d8b\u4e8e\u96f6\u65f6\u7684\u6620\u5c04\u6062\u590d\u4fdd\u8bc1\uff0c\u5e76\u63a8\u5bfc\u51fa\u53ef\u8ba1\u7b97\u7684\u7ec8\u7aef\u566a\u58f0\u6c34\u5e73\u03b5_stat(N)\u3002", "result": "1) \u5b8c\u5168\u523b\u753b\u4e86\u6570\u636e\u5728\u4f4e\u7ef4\u6d41\u5f62\u4e0a\u65f6\u865a\u5047\u89e3\u7684\u7279\u5f81\uff1b2) \u8bc1\u660e\u4e86\u52a0\u6027\u566a\u58f0\u5e73\u6ed1\u7684\u6709\u6548\u6027\uff0c\u5e76\u5efa\u7acb\u4e86\u65b0\u7684\u6620\u5c04\u6062\u590d\u7406\u8bba\u4fdd\u8bc1\uff1b3) \u63a8\u5bfc\u51fa\u7ec8\u7aef\u566a\u58f0\u6c34\u5e73\u03b5_stat(N)\uff0c\u8be5\u6c34\u5e73\u8fbe\u5230\u4ec5\u4f9d\u8d56\u4e8e\u672c\u5f81\u7ef4\u5ea6m\u7684\u6700\u4f18\u7edf\u8ba1\u901f\u7387\uff1b4) \u53d1\u73b0\u5f53\u03b5\u21920\u65f6\uff0c\u534a\u5bf9\u5076\u76ee\u6807\u51fd\u6570\u53d8\u5f97\u75c5\u6001\u3002", "conclusion": "\u52a0\u6027\u566a\u58f0\u5e73\u6ed1\u80fd\u6709\u6548\u89e3\u51b3\u534a\u5bf9\u5076\u795e\u7ecf\u6700\u4f18\u4f20\u8f93\u7684\u865a\u5047\u89e3\u95ee\u9898\uff0c\u4f46\u5b58\u5728\u4e00\u4e2a\u7406\u8bba\u6700\u4f18\u7684\u566a\u58f0\u6c34\u5e73\u03b5_stat(N)\u3002\u8bad\u7ec3\u65f6\u4e0d\u5e94\u5c06\u566a\u58f0\u964d\u81f3\u8be5\u6c34\u5e73\u4ee5\u4e0b\uff0c\u5426\u5219\u4f1a\u6076\u5316\u4f18\u5316\u6761\u4ef6\u800c\u65e0\u6cd5\u63d0\u5347\u7edf\u8ba1\u7cbe\u5ea6\u3002\u8fd9\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8d85\u53c2\u6570\u9009\u62e9\u548c\u65e9\u505c\u7b56\u7565\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.04116", "categories": ["cs.LG", "cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2602.04116", "abs": "https://arxiv.org/abs/2602.04116", "authors": ["Sicheng Liu", "Xunkai Li", "Daohan Su", "Ru Zhang", "Hongchao Qin", "Ronghua Li", "Guoren Wang"], "title": "Toward Effective Multimodal Graph Foundation Model: A Divide-and-Conquer Based Approach", "comment": "20 pages, 6 figures", "summary": "Graph Foundation Models (GFMs) have achieved remarkable success in generalizing across diverse domains. However, they mainly focus on Text-Attributed Graphs (TAGs), leaving Multimodal-Attributed Graphs (MAGs) largely untapped. Developing Multimodal Graph Foundation Models (MGFMs) allows for leveraging the rich multimodal information in MAGs, and extends applicability to broader types of downstream tasks. While recent MGFMs integrate diverse modality information, our empirical investigation reveals two fundamental limitations of existing MGFMs: (1)they fail to explicitly model modality interaction, essential for capturing intricate cross-modal semantics beyond simple aggregation, and (2)they exhibit sub-optimal modality alignment, which is critical for bridging the significant semantic disparity between distinct modal spaces. To address these challenges, we propose PLANET (graPh topoLogy-aware modAlity iNteraction and alignmEnT), a novel framework employing a Divide-and-Conquer strategy to decouple modality interaction and alignment across distinct granularities. At the embedding granularity, (1)Embedding-wise Domain Gating (EDG) performs local semantic enrichment by adaptively infusing topology-aware cross-modal context, achieving modality interaction. At the node granularity, (2)Node-wise Discretization Retrieval (NDR) ensures global modality alignment by constructing a Discretized Semantic Representation Space (DSRS) to bridge modality gaps. Extensive experiments demonstrate that PLANET significantly outperforms state-of-the-art baselines across diverse graph-centric and multimodal generative tasks.", "AI": {"tldr": "\u9488\u5bf9\u73b0\u6709\u591a\u6a21\u6001\u56fe\u57fa\u7840\u6a21\u578b(MGFMs)\u5728\u6a21\u6001\u4ea4\u4e92\u548c\u5bf9\u9f50\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u672c\u6587\u63d0\u51faPLANET\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6cbb\u7b56\u7565\u5728\u5d4c\u5165\u7ea7\u548c\u8282\u70b9\u7ea7\u5206\u522b\u5b9e\u73b0\u6a21\u6001\u4ea4\u4e92\u548c\u5bf9\u9f50\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u56fe\u57fa\u7840\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u5c5e\u6027\u56fe\uff0c\u5ffd\u89c6\u4e86\u591a\u6a21\u6001\u5c5e\u6027\u56fe\u3002\u5373\u4f7f\u662f\u6700\u65b0\u7684MGFMs\u4e5f\u5b58\u5728\u4e24\u4e2a\u6839\u672c\u9650\u5236\uff1a(1)\u65e0\u6cd5\u663e\u5f0f\u5efa\u6a21\u6a21\u6001\u4ea4\u4e92\uff0c\u96be\u4ee5\u6355\u6349\u590d\u6742\u7684\u8de8\u6a21\u6001\u8bed\u4e49\uff1b(2)\u6a21\u6001\u5bf9\u9f50\u6548\u679c\u4e0d\u4f73\uff0c\u65e0\u6cd5\u6709\u6548\u5f25\u5408\u4e0d\u540c\u6a21\u6001\u7a7a\u95f4\u4e4b\u95f4\u7684\u8bed\u4e49\u5dee\u5f02\u3002", "method": "\u63d0\u51faPLANET\u6846\u67b6\uff0c\u91c7\u7528\u5206\u6cbb\u7b56\u7565\u89e3\u8026\u6a21\u6001\u4ea4\u4e92\u548c\u5bf9\u9f50\uff1a\u5d4c\u5165\u7ea7\u4f7f\u7528\u5d4c\u5165\u57df\u95e8\u63a7(EDG)\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6ce8\u5165\u62d3\u6251\u611f\u77e5\u7684\u8de8\u6a21\u6001\u4e0a\u4e0b\u6587\u5b9e\u73b0\u5c40\u90e8\u8bed\u4e49\u589e\u5f3a\u548c\u6a21\u6001\u4ea4\u4e92\uff1b\u8282\u70b9\u7ea7\u91c7\u7528\u8282\u70b9\u79bb\u6563\u5316\u68c0\u7d22(NDR)\uff0c\u901a\u8fc7\u6784\u5efa\u79bb\u6563\u8bed\u4e49\u8868\u793a\u7a7a\u95f4(DSRS)\u5b9e\u73b0\u5168\u5c40\u6a21\u6001\u5bf9\u9f50\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cPLANET\u5728\u591a\u6837\u5316\u7684\u56fe\u4e2d\u5fc3\u548c\u591a\u6a21\u6001\u751f\u6210\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "PLANET\u6846\u67b6\u901a\u8fc7\u5206\u5c42\u5904\u7406\u6a21\u6001\u4ea4\u4e92\u548c\u5bf9\u9f50\u95ee\u9898\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u56fe\u57fa\u7840\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e3a\u5904\u7406\u591a\u6a21\u6001\u56fe\u6570\u636e\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.04114", "categories": ["cs.LG", "math.DS"], "pdf": "https://arxiv.org/pdf/2602.04114", "abs": "https://arxiv.org/abs/2602.04114", "authors": ["Amit K. Chakraborty", "Hao Wang", "Pouria Ramazi"], "title": "Turning mechanistic models into forecasters by using machine learning", "comment": "47 pages, 11 figures", "summary": "The equations of complex dynamical systems may not be identified by expert knowledge, especially if the underlying mechanisms are unknown. Data-driven discovery methods address this challenge by inferring governing equations from time-series data using a library of functions constructed from the measured variables. However, these methods typically assume time-invariant coefficients, which limits their ability to capture evolving system dynamics. To overcome this limitation, we allow some of the parameters to vary over time, learn their temporal evolution directly from data, and infer a system of equations that incorporates both constant and time-varying parameters. We then transform this framework into a forecasting model by predicting the time-varying parameters and substituting these predictions into the learned equations. The model is validated using datasets for Susceptible-Infected-Recovered, Consumer--Resource, greenhouse gas concentration, and Cyanobacteria cell count. By dynamically adapting to temporal shifts, our proposed model achieved a mean absolute error below 3\\% for learning a time series and below 6\\% for forecasting up to a month ahead. We additionally compare forecasting performance against CNN-LSTM and Gradient Boosting Machine (GBM), and show that our model outperforms these methods across most datasets. Our findings demonstrate that integrating time-varying parameters into data-driven discovery of differential equations improves both modeling accuracy and forecasting performance.", "AI": {"tldr": "\u63d0\u51fa\u65f6\u53d8\u53c2\u6570\u7684\u6570\u636e\u9a71\u52a8\u5fae\u5206\u65b9\u7a0b\u53d1\u73b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u5b66\u4e60\u53c2\u6570\u65f6\u5e8f\u6f14\u5316\u63d0\u5347\u590d\u6742\u7cfb\u7edf\u5efa\u6a21\u4e0e\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u5b66\u4e60\u8bef\u5dee<3%\u3001\u6708\u5c3a\u5ea6\u9884\u6d4b\u8bef\u5dee<6%\uff0c\u5e76\u4f18\u4e8eCNN-LSTM\u548cGBM\u6a21\u578b", "motivation": "\u4f20\u7edf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5047\u8bbe\u5fae\u5206\u65b9\u7a0b\u7cfb\u6570\u65f6\u4e0d\u53d8\uff0c\u96be\u4ee5\u6355\u6349\u7cfb\u7edf\u52a8\u6001\u6f14\u5316\u673a\u5236\uff0c\u9700\u5f00\u53d1\u80fd\u540c\u65f6\u5904\u7406\u5e38/\u65f6\u53d8\u53c2\u6570\u7684\u5efa\u6a21\u6846\u67b6", "method": "\u5141\u8bb8\u90e8\u5206\u53c2\u6570\u65f6\u53d8\u5e76\u76f4\u63a5\u4ece\u6570\u636e\u5b66\u4e60\u5176\u6f14\u5316\u89c4\u5f8b\uff0c\u6784\u5efa\u542b\u5e38/\u65f6\u53d8\u53c2\u6570\u7684\u65b9\u7a0b\u7cfb\u7edf\uff0c\u8f6c\u5316\u4e3a\u9884\u6d4b\u6a21\u578b\u65f6\u5bf9\u65f6\u53d8\u53c2\u6570\u8fdb\u884c\u9884\u6d4b\u5e76\u4ee3\u5165\u65b9\u7a0b", "result": "\u5728SIR\u6a21\u578b\u3001\u6d88\u8d39\u8005-\u8d44\u6e90\u7cfb\u7edf\u3001\u6e29\u5ba4\u6c14\u4f53\u6d53\u5ea6\u548c\u84dd\u85fb\u7ec6\u80de\u8ba1\u6570\u6570\u636e\u96c6\u4e0a\uff0c\u5b66\u4e60\u9636\u6bb5\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee<3%\uff0c\u6708\u5c3a\u5ea6\u9884\u6d4b\u8bef\u5dee<6%\uff0c\u4e14\u591a\u6570\u573a\u666f\u4e0b\u9884\u6d4b\u6027\u80fd\u4f18\u4e8eCNN-LSTM\u548c\u68af\u5ea6\u63d0\u5347\u673a", "conclusion": "\u5c06\u65f6\u53d8\u53c2\u6570\u6574\u5408\u81f3\u6570\u636e\u9a71\u52a8\u5fae\u5206\u65b9\u7a0b\u53d1\u73b0\u53ef\u540c\u65f6\u63d0\u5347\u5efa\u6a21\u7cbe\u5ea6\u4e0e\u9884\u6d4b\u6027\u80fd\uff0c\u4e3a\u52a8\u6001\u6f14\u5316\u7cfb\u7edf\u63d0\u4f9b\u6709\u6548\u5efa\u6a21\u8303\u5f0f"}}
{"id": "2602.04120", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.04120", "abs": "https://arxiv.org/abs/2602.04120", "authors": ["Samaresh Kumar Singh", "Joyjit Roy"], "title": "Scalable Explainability-as-a-Service (XaaS) for Edge AI Systems", "comment": "8 pages, 5 figures, submitted and accepted in the conference IEEE SoutheastCon 2026", "summary": "Though Explainable AI (XAI) has made significant advancements, its inclusion in edge and IoT systems is typically ad-hoc and inefficient. Most current methods are \"coupled\" in such a way that they generate explanations simultaneously with model inferences. As a result, these approaches incur redundant computation, high latency and poor scalability when deployed across heterogeneous sets of edge devices. In this work we propose Explainability-as-a-Service (XaaS), a distributed architecture for treating explainability as a first-class system service (as opposed to a model-specific feature). The key innovation in our proposed XaaS architecture is that it decouples inference from explanation generation allowing edge devices to request, cache and verify explanations subject to resource and latency constraints. To achieve this, we introduce three main innovations: (1) A distributed explanation cache with a semantic similarity based explanation retrieval method which significantly reduces redundant computation; (2) A lightweight verification protocol that ensures the fidelity of both cached and newly generated explanations; and (3) An adaptive explanation engine that chooses explanation methods based upon device capability and user requirement. We evaluated the performance of XaaS on three real-world edge-AI use cases: (i) manufacturing quality control; (ii) autonomous vehicle perception; and (iii) healthcare diagnostics. Experimental results show that XaaS reduces latency by 38\\% while maintaining high explanation quality across three real-world deployments. Overall, this work enables the deployment of transparent and accountable AI across large scale, heterogeneous IoT systems, and bridges the gap between XAI research and edge-practicality.", "AI": {"tldr": "\u63d0\u51faXaaS\uff08\u53ef\u89e3\u91ca\u5373\u670d\u52a1\uff09\u5206\u5e03\u5f0f\u67b6\u6784\uff0c\u5c06\u53ef\u89e3\u91ca\u6027\u89e3\u8026\u4e3a\u72ec\u7acb\u670d\u52a1\uff0c\u901a\u8fc7\u7f13\u5b58\u3001\u9a8c\u8bc1\u548c\u81ea\u9002\u5e94\u5f15\u64ce\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u8d28\u91cf\u7684\u53ef\u89e3\u91caAI\uff0c\u5b9e\u9a8c\u663e\u793a\u5ef6\u8fdf\u964d\u4f4e38%\u3002", "motivation": "\u5f53\u524dXAI\u65b9\u6cd5\u5728\u8fb9\u7f18\u548cIoT\u7cfb\u7edf\u4e2d\u901a\u5e38\u662f\u4e34\u65f6\u6027\u548c\u4f4e\u6548\u7684\uff0c\u63a8\u7406\u4e0e\u89e3\u91ca\u751f\u6210\u8026\u5408\u5bfc\u81f4\u5197\u4f59\u8ba1\u7b97\u3001\u9ad8\u5ef6\u8fdf\u548c\u6269\u5c55\u6027\u5dee\uff0c\u96be\u4ee5\u5728\u5f02\u6784\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002", "method": "\u63d0\u51faXaaS\u67b6\u6784\uff0c\u5305\u542b\u4e09\u4e2a\u521b\u65b0\uff1a(1)\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u6027\u7684\u5206\u5e03\u5f0f\u89e3\u91ca\u7f13\u5b58\u4e0e\u68c0\u7d22\u673a\u5236\uff1b(2)\u8f7b\u91cf\u7ea7\u9a8c\u8bc1\u534f\u8bae\u786e\u4fdd\u89e3\u91ca\u4fdd\u771f\u5ea6\uff1b(3)\u81ea\u9002\u5e94\u89e3\u91ca\u5f15\u64ce\u6839\u636e\u8bbe\u5907\u80fd\u529b\u548c\u7528\u6237\u9700\u6c42\u9009\u62e9\u89e3\u91ca\u65b9\u6cd5\u3002", "result": "\u5728\u5236\u9020\u8d28\u68c0\u3001\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u548c\u533b\u7597\u8bca\u65ad\u4e09\u4e2a\u5b9e\u9645\u573a\u666f\u4e2d\uff0cXaaS\u964d\u4f4e\u5ef6\u8fdf38%\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u89e3\u91ca\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5927\u89c4\u6a21\u5f02\u6784IoT\u7cfb\u7edf\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4f7f\u900f\u660e\u53ef\u4fe1\u7684AI\u80fd\u591f\u5728\u5927\u578b\u5f02\u6784IoT\u7cfb\u7edf\u4e0a\u90e8\u7f72\uff0c\u5f25\u8865\u4e86XAI\u7814\u7a76\u4e0e\u8fb9\u7f18\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4e3a\u8fb9\u7f18AI\u7684\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u5b9e\u7528\u5316\u8def\u5f84\u3002"}}
{"id": "2602.04878", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.04878", "abs": "https://arxiv.org/abs/2602.04878", "authors": ["Manuel S. Rudolph", "Armando Angrisani", "Andrew Wright", "Iwo Sanderski", "Ricard Puig", "Zo\u00eb Holmes"], "title": "Thermal State Simulation with Pauli and Majorana Propagation", "comment": "34 pages, 5 figues", "summary": "We introduce a propagation-based approach to thermal state simulation by adapting Pauli and Majorana propagation to imaginary-time evolution in the Schr\u00f6dinger picture. Our key observation is that high-temperature states can be sparse in the Pauli or Majorana bases, approaching the identity at infinite temperature. By formulating imaginary-time evolution directly in these operator bases and evolving from the maximally mixed state, we access a continuum of temperatures where the state remains efficiently representable. We provide analytic guarantees for small-coefficient truncation and Pauli-weight (Majorana-length) truncation strategies by quantifying the error growth and the impact of backflow. Large-scale numerics on the 1D J1-J2 model (energies) and the triangular-lattice Hubbard model (static correlations) validate efficiency at high temperatures.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4f20\u64ad\u5b50\u7684\u70ed\u6001\u6a21\u62df\u65b9\u6cd5\uff0c\u5229\u7528\u6ce1\u5229/\u9a6c\u7ea6\u62c9\u7eb3\u7b97\u7b26\u57fa\u5728\u865a\u65f6\u95f4\u6f14\u5316\u4e2d\u7684\u7a00\u758f\u6027\u5b9e\u73b0\u9ad8\u6e29\u91cf\u5b50\u7cfb\u7edf\u9ad8\u6548\u6a21\u62df", "motivation": "\u9ad8\u6e29\u91cf\u5b50\u6001\u6a21\u62df\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u74f6\u9888\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u70ed\u6001\u5728\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u7684\u590d\u6742\u6f14\u5316", "method": "\u5c06\u6ce1\u5229\u548c\u9a6c\u7ea6\u62c9\u7eb3\u4f20\u64ad\u5b50\u9002\u914d\u5230\u859b\u5b9a\u8c14\u56fe\u666f\u7684\u865a\u65f6\u95f4\u6f14\u5316\uff1b\u4ece\u6700\u5927\u6df7\u5408\u6001\u51fa\u53d1\uff0c\u5728\u7b97\u7b26\u57fa\u4e2d\u76f4\u63a5\u6f14\u5316\uff1b\u91c7\u7528\u5c0f\u7cfb\u6570\u622a\u65ad\u548c\u6ce1\u5229\u6743\u91cd\u622a\u65ad\u7b56\u7565", "result": "\u7406\u8bba\u8bc1\u660e\u622a\u65ad\u8bef\u5dee\u53ef\u63a7\u4e14\u53cd\u5411\u6d41\u5f71\u54cd\u53ef\u91cf\u5316\uff1b\u57281D J1-J2\u6a21\u578b\u548c\u4e09\u89d2\u6676\u683c\u54c8\u4f2f\u5fb7\u6a21\u578b\u7684\u5927\u5c3a\u5ea6\u6570\u503c\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u9ad8\u6e29\u533a\u80fd\u91cf\u548c\u9759\u6001\u5173\u8054\u7684\u9ad8\u6548\u8ba1\u7b97", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u9ad8\u6e29\u6001\u7684\u7b97\u7b26\u57fa\u7a00\u758f\u7279\u6027\uff0c\u4e3a\u9ad8\u6e29\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u7684\u70ed\u6001\u6a21\u62df\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7406\u8bba\u4fdd\u8bc1\u7684\u65b0\u6846\u67b6"}}
{"id": "2602.04153", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04153", "abs": "https://arxiv.org/abs/2602.04153", "authors": ["Zihao Jing", "Yuxi Long", "Ganlin Feng"], "title": "Pruning for Generalization: A Transfer-Oriented Spatiotemporal Graph Framework", "comment": "Under review at ICLR 2026 Workshop TSALM", "summary": "Multivariate time series forecasting in graph-structured domains is critical for real-world applications, yet existing spatiotemporal models often suffer from performance degradation under data scarcity and cross-domain shifts. We address these challenges through the lens of structure-aware context selection. We propose TL-GPSTGN, a transfer-oriented spatiotemporal framework that enhances sample efficiency and out-of-distribution generalization by selectively pruning non-optimized graph context. Specifically, our method employs information-theoretic and correlation-based criteria to extract structurally informative subgraphs and features, resulting in a compact, semantically grounded representation. This optimized context is subsequently integrated into a spatiotemporal convolutional architecture to capture complex multivariate dynamics. Evaluations on large-scale traffic benchmarks demonstrate that TL-GPSTGN consistently outperforms baselines in low-data transfer scenarios. Our findings suggest that explicit context pruning serves as a powerful inductive bias for improving the robustness of graph-based forecasting models.", "AI": {"tldr": "\u9488\u5bf9\u56fe\u7ed3\u6784\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u5728\u6570\u636e\u7a00\u7f3a\u548c\u8de8\u57df\u8fc1\u79fb\u65f6\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u63d0\u51faTL-GPSTGN\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u606f\u8bba\u548c\u5173\u8054\u6027\u51c6\u5219\u9009\u62e9\u6027\u5730\u526a\u679d\u975e\u4f18\u5316\u56fe\u4e0a\u4e0b\u6587\uff0c\u63d0\u53d6\u4fe1\u606f\u6027\u5b50\u56fe\u548c\u7279\u5f81\uff0c\u5728\u4ea4\u901a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u56fe\u7ed3\u6784\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u548c\u8de8\u57df\u5206\u5e03\u6f02\u79fb\u7684\u6311\u6218\uff0c\u73b0\u6709\u65f6\u7a7a\u6a21\u578b\u5728\u8fd9\u4e9b\u60c5\u51b5\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "method": "\u63d0\u51faTL-GPSTGN\u8fc1\u79fb\u65f6\u7a7a\u6846\u67b6\uff0c\u91c7\u7528\u4fe1\u606f\u8bba\u548c\u57fa\u4e8e\u76f8\u5173\u6027\u7684\u51c6\u5219\uff0c\u9009\u62e9\u6027\u5730\u526a\u679d\u975e\u4f18\u5316\u56fe\u4e0a\u4e0b\u6587\uff0c\u63d0\u53d6\u7ed3\u6784\u4fe1\u606f\u4e30\u5bcc\u7684\u5b50\u56fe\u548c\u7279\u5f81\uff0c\u5f62\u6210\u7d27\u51d1\u7684\u8bed\u4e49\u8868\u793a\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u65f6\u7a7a\u5377\u79ef\u67b6\u6784\u4e2d\u3002", "result": "\u5728\u5927\u89c4\u6a21\u4ea4\u901a\u57fa\u51c6\u6d4b\u8bd5\u7684\u4f4e\u6570\u636e\u8fc1\u79fb\u573a\u666f\u4e0b\uff0cTL-GPSTGN\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u663e\u5f0f\u7684\u4e0a\u4e0b\u6587\u526a\u679d\u53ef\u4f5c\u4e3a\u6709\u6548\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u663e\u8457\u63d0\u5347\u57fa\u4e8e\u56fe\u7684\u9884\u6d4b\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.04131", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04131", "abs": "https://arxiv.org/abs/2602.04131", "authors": ["Mehrdad Moghimi", "Anthony Coache", "Hyejin Ku"], "title": "Decoupling Time and Risk: Risk-Sensitive Reinforcement Learning with General Discounting", "comment": null, "summary": "Distributional reinforcement learning (RL) is a powerful framework increasingly adopted in safety-critical domains for its ability to optimize risk-sensitive objectives. However, the role of the discount factor is often overlooked, as it is typically treated as a fixed parameter of the Markov decision process or tunable hyperparameter, with little consideration of its effect on the learned policy. In the literature, it is well-known that the discounting function plays a major role in characterizing time preferences of an agent, which an exponential discount factor cannot fully capture. Building on this insight, we propose a novel framework that supports flexible discounting of future rewards and optimization of risk measures in distributional RL. We provide a technical analysis of the optimality of our algorithms, show that our multi-horizon extension fixes issues raised with existing methodologies, and validate the robustness of our methods through extensive experiments. Our results highlight that discounting is a cornerstone in decision-making problems for capturing more expressive temporal and risk preferences profiles, with potential implications for real-world safety-critical applications.", "AI": {"tldr": "This paper proposes a novel distributional reinforcement learning framework with flexible discounting to better capture temporal and risk preferences, demonstrating that discounting is crucial for safety-critical applications through theoretical analysis and experimental validation.", "motivation": "The discount factor in RL is typically treated as a fixed parameter or tunable hyperparameter without considering its effect on learned policies, especially in safety-critical domains where optimizing risk-sensitive objectives is important. The exponential discount factor cannot fully capture an agent's time preferences.", "method": "The authors propose a novel framework enabling flexible discounting of future rewards combined with optimization of risk measures in distributional RL. They provide technical analysis of algorithm optimality, introduce a multi-horizon extension to fix existing methodological issues, and validate robustness through extensive experiments.", "result": "The multi-horizon extension resolves issues with existing methodologies, and experiments confirm the robustness of the proposed methods. Results demonstrate that discounting is fundamental for capturing more expressive temporal and risk preference profiles.", "conclusion": "Discounting is a cornerstone in decision-making problems for capturing expressive temporal and risk preferences, with significant implications for real-world safety-critical applications. The proposed framework offers a more principled approach to handling discounting in reinforcement learning."}}
{"id": "2602.04265", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04265", "abs": "https://arxiv.org/abs/2602.04265", "authors": ["Wenze Lin", "Zhen Yang", "Xitai Jiang", "Pony Ma", "Gao Huang"], "title": "Thickening-to-Thinning: Reward Shaping via Human-Inspired Learning Dynamics for LLM Reasoning", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for enhancing reasoning in Large Language Models (LLMs). However, it frequently encounters challenges such as entropy collapse, excessive verbosity, and insufficient exploration for hard problems. Crucially, existing reward schemes fail to distinguish between the need for extensive search during problem-solving and the efficiency required for mastered knowledge. In this work, we introduce T2T(Thickening-to-Thinning), a dynamic reward framework inspired by human learning processes. Specifically, it implements a dual-phase mechanism: (1) On incorrect attempts, T2T incentivizes \"thickening\" (longer trajectories) to broaden the search space and explore novel solution paths; (2) Upon achieving correctness, it shifts to \"thinning\", imposing length penalties to discourage redundancy, thereby fostering model confidence and crystallizing reasoning capabilities. Extensive experiments on mathematical benchmarks (MATH-500, AIME, AMC) across Qwen-series and Deepseek models demonstrate that T2T significantly outperforms standard GRPO and recent baselines, achieving superior performance.", "AI": {"tldr": "T2T\u662f\u4e00\u4e2a\u52a8\u6001\u5956\u52b1\u6846\u67b6\uff0c\u901a\u8fc7\"\u5148\u539a\u540e\u8584\"\u673a\u5236\u89e3\u51b3RLVR\u7684\u71b5\u5d29\u6e83\u3001\u5197\u957f\u6027\u548c\u63a2\u7d22\u4e0d\u8db3\u95ee\u9898\uff0c\u5728\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u5b58\u5728\u71b5\u5d29\u6e83\u3001\u8fc7\u5ea6\u5197\u957f\u548c\u96be\u9898\u63a2\u7d22\u4e0d\u8db3\u7b49\u6311\u6218\uff0c\u4e14\u5956\u52b1\u673a\u5236\u65e0\u6cd5\u533a\u5206\u95ee\u9898\u6c42\u89e3\u65f6\u7684\u5e7f\u6cdb\u641c\u7d22\u9700\u6c42\u4e0e\u5df2\u638c\u63e1\u77e5\u8bc6\u7684\u9ad8\u6548\u8868\u8fbe\u9700\u6c42\u3002", "method": "\u63d0\u51faT2T\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u673a\u5236\uff1a\u9519\u8bef\u5c1d\u8bd5\u65f6\"\u52a0\u539a\"\uff08\u5956\u52b1\u957f\u8f68\u8ff9\u4ee5\u6269\u5927\u641c\u7d22\u7a7a\u95f4\uff09\uff1b\u6b63\u786e\u65f6\"\u524a\u8584\"\uff08\u65bd\u52a0\u957f\u5ea6\u60e9\u7f5a\u4ee5\u907f\u514d\u5197\u4f59\uff09\uff0c\u6a21\u62df\u4eba\u7c7b\u5b66\u4e60\u8fc7\u7a0b\u3002", "result": "\u5728MATH-500\u3001AIME\u3001AMC\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cT2T\u5728Qwen\u7cfb\u5217\u548cDeepseek\u6a21\u578b\u4e0a\u663e\u8457\u4f18\u4e8e\u6807\u51c6GRPO\u548c\u6700\u65b0\u57fa\u7ebf\u65b9\u6cd5\uff0c\u53d6\u5f97\u5353\u8d8a\u6027\u80fd\u3002", "conclusion": "T2T\u901a\u8fc7\u52a8\u6001\u5956\u52b1\u673a\u5236\u6709\u6548\u5e73\u8861\u4e86\u63a2\u7d22\u4e0e\u6548\u7387\uff0c\u6210\u529f\u89e3\u51b3\u4e86LLM\u63a8\u7406\u589e\u5f3a\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u53ef\u9a8c\u8bc1\u4efb\u52a1\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2602.04277", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04277", "abs": "https://arxiv.org/abs/2602.04277", "authors": ["Priyankkumar Dhrangdhariya", "Soumyadipta Maiti", "Venkataramana Runkana"], "title": "Multi Objective Design Optimization of Non Pneumatic Passenger Car Tires Using Finite Element Modeling, Machine Learning, and Particle swarm Optimization and Bayesian Optimization Algorithms", "comment": null, "summary": "Non Pneumatic tires offer a promising alternative to pneumatic tires. However, their discontinuous spoke structures present challenges in stiffness tuning, durability, and high speed vibration. This study introduces an integrated generative design and machine learning driven framework to optimize UPTIS type spoke geometries for passenger vehicles. Upper and lower spoke profiles were parameterized using high order polynomial representations, enabling the creation of approximately 250 generative designs through PCHIP based geometric variation. Machine learning models like KRR for stiffness and XGBoost for durability and vibration achieved strong predictive accuracy, reducing the reliance on computationally intensive FEM simulations. Optimization using Particle Swarm Optimization and Bayesian Optimization further enabled extensive performance refinement. The resulting designs demonstrate 53% stiffness tunability, up to 50% durability improvement, and 43% reduction in vibration compared to the baseline. PSO provided fast, targeted convergence, while Bayesian Optimization effectively explored multi objective tradeoffs. Overall, the proposed framework enables systematic development of high performance, next generation UPTIS spoke structures.", "AI": {"tldr": "A generative design and machine learning framework was developed to optimize UPTIS non-pneumatic tire spoke geometries, achieving 53% stiffness tunability, up to 50% durability improvement, and 43% vibration reduction while reducing computational costs.", "motivation": "Non-pneumatic tires offer promising alternatives to pneumatic tires but face challenges with discontinuous spoke structures including stiffness tuning difficulties, durability issues, and high-speed vibration problems that need addressing for passenger vehicle applications.", "method": "The study parameterized spoke profiles using high-order polynomials, generated ~250 designs via PCHIP-based geometric variation, trained ML models (KRR for stiffness, XGBoost for durability/vibration) to replace computationally intensive FEM simulations, and applied Particle Swarm Optimization and Bayesian Optimization for performance refinement.", "result": "Optimized designs achieved 53% stiffness tunability, up to 50% durability improvement, and 43% vibration reduction compared to baseline. PSO provided fast targeted convergence while Bayesian Optimization effectively explored multi-objective tradeoffs.", "conclusion": "The proposed integrated framework enables systematic development of high-performance, next-generation UPTIS spoke structures for passenger vehicles."}}
{"id": "2602.04145", "categories": ["cs.LG", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.04145", "abs": "https://arxiv.org/abs/2602.04145", "authors": ["Jinyuan Li", "Chengsong Huang", "Langlin Huang", "Shaoyang Xu", "Haolin Liu", "Wenxuan Zhang", "Jiaxin Huang"], "title": "Training Data Efficiency in Multimodal Process Reward Models", "comment": null, "summary": "Multimodal Process Reward Models (MPRMs) are central to step-level supervision for visual reasoning in MLLMs. Training MPRMs typically requires large-scale Monte Carlo (MC)-annotated corpora, incurring substantial training cost. This paper studies the data efficiency for MPRM training.Our preliminary experiments reveal that MPRM training quickly saturates under random subsampling of the training data, indicating substantial redundancy within existing MC-annotated corpora.To explain this, we formalize a theoretical framework and reveal that informative gradient updates depend on two factors: label mixtures of positive/negative steps and label reliability (average MC scores of positive steps). Guided by these insights, we propose the Balanced-Information Score (BIS), which prioritizes both mixture and reliability based on existing MC signals at the rollout level, without incurring any additional cost. Across two backbones (InternVL2.5-8B and Qwen2.5-VL-7B) on VisualProcessBench, BIS-selected subsets consistently match and even surpass the full-data performance at small fractions. Notably, the BIS subset reaches full-data performance using only 10% of the training data, improving over random subsampling by a relative 4.1%.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u591a\u6a21\u6001\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08MPRM\uff09\u8bad\u7ec3\u7684\u6570\u636e\u6548\u7387\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u4fe1\u606f\u5e73\u8861\u7684\u8bc4\u5206\uff08BIS\uff09\u65b9\u6cd5\uff0c\u4ec5\u970010%\u7684\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u8fbe\u5230\u5168\u91cf\u6570\u636e\u7684\u6027\u80fd\u3002", "motivation": "MPRM\u8bad\u7ec3\u4f9d\u8d56\u5927\u89c4\u6a21\u8499\u7279\u5361\u6d1b\u6807\u6ce8\u6570\u636e\uff0c\u6210\u672c\u9ad8\u6602\uff0c\u4e14\u5b58\u5728\u663e\u8457\u7684\u6570\u636e\u5197\u4f59\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63ed\u793a\u68af\u5ea6\u66f4\u65b0\u4e0e\u6807\u7b7e\u6df7\u5408\u5ea6\u53ca\u53ef\u9760\u6027\u7684\u5173\u7cfb\uff0c\u63d0\u51faBIS\u8bc4\u5206\u65b9\u6cd5\uff0c\u5728\u56de\u73af\u7ea7\u522b\u4f18\u5148\u9009\u62e9\u4fe1\u606f\u91cf\u9ad8\u7684\u6837\u672c\u3002", "result": "\u5728\u4e24\u4e2a\u9aa8\u5e72\u6a21\u578b\u4e0a\uff0cBIS\u7b5b\u9009\u7684\u5b50\u96c6\u4ec5\u970010%\u6570\u636e\u5373\u53ef\u8fbe\u5230\u5168\u91cf\u6570\u636e\u6027\u80fd\uff0c\u6bd4\u968f\u673a\u91c7\u6837\u76f8\u5bf9\u63d0\u53474.1%\u3002", "conclusion": "BIS\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u9009\u62e9\u7b56\u7565\uff0c\u80fd\u663e\u8457\u964d\u4f4eMPRM\u8bad\u7ec3\u6210\u672c\uff0c\u540c\u65f6\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2602.04323", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04323", "abs": "https://arxiv.org/abs/2602.04323", "authors": ["Dian Jin", "Yancheng Yuan", "Xiaoming Tao"], "title": "Efficient Equivariant High-Order Crystal Tensor Prediction via Cartesian Local-Environment Many-Body Coupling", "comment": null, "summary": "End-to-end prediction of high-order crystal tensor properties from atomic structures remains challenging: while spherical-harmonic equivariant models are expressive, their Clebsch-Gordan tensor products incur substantial compute and memory costs for higher-order targets. We propose the Cartesian Environment Interaction Tensor Network (CEITNet), an approach that constructs a multi-channel Cartesian local environment tensor for each atom and performs flexible many-body mixing via a learnable channel-space interaction. By performing learning in channel space and using Cartesian tensor bases to assemble equivariant outputs, CEITNet enables efficient construction of high-order tensor. Across benchmark datasets for order-2 dielectric, order-3 piezoelectric, and order-4 elastic tensor prediction, CEITNet surpasses prior high-order prediction methods on key accuracy criteria while offering high computational efficiency.", "AI": {"tldr": "CEITNet is a novel model that efficiently predicts high-order crystal tensor properties using Cartesian tensors and channel-space interactions, outperforming existing methods while avoiding expensive Clebsch-Gordan tensor products.", "motivation": "End-to-end prediction of high-order crystal tensor properties remains challenging because spherical-harmonic equivariant models, while expressive, suffer from substantial computational and memory costs for higher-order targets due to Clebsch-Gordan tensor products.", "method": "The Cartesian Environment Interaction Tensor Network (CEITNet) constructs multi-channel Cartesian local environment tensors for each atom and performs flexible many-body mixing via learnable channel-space interactions. It learns in channel space and uses Cartesian tensor bases to assemble equivariant outputs.", "result": "CEITNet surpasses prior high-order prediction methods on accuracy for order-2 dielectric, order-3 piezoelectric, and order-4 elastic tensor prediction benchmarks while maintaining high computational efficiency.", "conclusion": "CEITNet enables efficient and accurate construction of high-order tensor predictions by leveraging Cartesian tensor representations and channel-space learning, overcoming the computational limitations of traditional spherical-harmonic approaches."}}
{"id": "2602.04344", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04344", "abs": "https://arxiv.org/abs/2602.04344", "authors": ["Kou Misaki", "Takuya Akiba"], "title": "UnMaskFork: Test-Time Scaling for Masked Diffusion via Deterministic Action Branching", "comment": null, "summary": "Test-time scaling strategies have effectively leveraged inference-time compute to enhance the reasoning abilities of Autoregressive Large Language Models. In this work, we demonstrate that Masked Diffusion Language Models (MDLMs) are inherently amenable to advanced search strategies, owing to their iterative and non-autoregressive generation process. To leverage this, we propose UnMaskFork (UMF), a framework that formulates the unmasking trajectory as a search tree and employs Monte Carlo Tree Search to optimize the generation path. In contrast to standard scaling methods relying on stochastic sampling, UMF explores the search space through deterministic partial unmasking actions performed by multiple MDLMs. Our empirical evaluation demonstrates that UMF consistently outperforms existing test-time scaling baselines on complex coding benchmarks, while also exhibiting strong scalability on mathematical reasoning tasks.", "AI": {"tldr": "This paper proposes UnMaskFork, a framework that applies Monte Carlo Tree Search to Masked Diffusion Language Models for test-time scaling, outperforming existing methods on coding and math tasks.", "motivation": "The paper aims to explore how Masked Diffusion Language Models (MDLMs), with their iterative generation process, can benefit from advanced search strategies like MCTS for test-time scaling, which has been primarily explored for autoregressive models.", "method": "The UnMaskFork (UMF) framework treats the token unmasking process as a search tree and uses Monte Carlo Tree Search to optimize the generation path. It employs deterministic partial unmasking actions performed by multiple MDLMs, contrasting with stochastic sampling approaches.", "result": "Empirical evaluation shows UMF consistently outperforms existing test-time scaling baselines on complex coding benchmarks and demonstrates strong scalability on mathematical reasoning tasks.", "conclusion": "MDLMs are inherently suitable for advanced search strategies, and UMF effectively leverages this property to improve model performance through test-time compute scaling."}}
{"id": "2602.04189", "categories": ["cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2602.04189", "abs": "https://arxiv.org/abs/2602.04189", "authors": ["Xiaoyu Qiu", "Taewon Yang", "Zhanhao Liu", "Guanyang Wang", "Liyue Shen"], "title": "Benchmarking Uncertainty Quantification of Plug-and-Play Diffusion Priors for Inverse Problems Solving", "comment": null, "summary": "Plug-and-play diffusion priors (PnPDP) have become a powerful paradigm for solving inverse problems in scientific and engineering domains. Yet, current evaluations of reconstruction quality emphasize point-estimate accuracy metrics on a single sample, which do not reflect the stochastic nature of PnPDP solvers and the intrinsic uncertainty of inverse problems, critical for scientific tasks. This creates a fundamental mismatch: in inverse problems, the desired output is typically a posterior distribution and most PnPDP solvers induce a distribution over reconstructions, but existing benchmarks only evaluate a single reconstruction, ignoring distributional characterization such as uncertainty. To address this gap, we conduct a systematic study to benchmark the uncertainty quantification (UQ) of existing diffusion inverse solvers. Specifically, we design a rigorous toy model simulation to evaluate the uncertainty behavior of various PnPDP solvers, and propose a UQ-driven categorization. Through extensive experiments on toy simulations and diverse real-world scientific inverse problems, we observe uncertainty behaviors consistent with our taxonomy and theoretical justification, providing new insights for evaluating and understanding the uncertainty for PnPDPs.", "AI": {"tldr": "\u9488\u5bf9\u5373\u63d2\u5373\u7528\u6269\u6563\u5148\u9a8c\u5728\u9006\u95ee\u9898\u6c42\u89e3\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7f3a\u5931\u95ee\u9898\uff0c\u672c\u6587\u901a\u8fc7\u73a9\u5177\u6a21\u578b\u4eff\u771f\u548c\u771f\u5b9e\u79d1\u5b66\u9006\u95ee\u9898\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u73b0\u6709\u6269\u6563\u6c42\u89e3\u5668\u7684\u4e0d\u786e\u5b9a\u6027\u884c\u4e3a\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8eUQ\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u4e3a\u7406\u89e3\u6269\u6563\u9006\u95ee\u9898\u6c42\u89e3\u7684\u4e0d\u786e\u5b9a\u6027\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "motivation": "\u5f53\u524d\u5bf9\u5373\u63d2\u5373\u7528\u6269\u6563\u5148\u9a8c(PnPDP)\u9006\u95ee\u9898\u6c42\u89e3\u5668\u7684\u8bc4\u4f30\u4ec5\u5173\u6ce8\u5355\u70b9\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u5ffd\u7565\u4e86\u8fd9\u4e9b\u968f\u673a\u6c42\u89e3\u5668\u4ea7\u751f\u7684\u540e\u9a8c\u5206\u5e03\u7279\u6027\uff0c\u4e0e\u79d1\u5b66\u4efb\u52a1\u4e2d\u9700\u8981\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u5b9e\u9645\u9700\u6c42\u5b58\u5728\u6839\u672c\u6027\u9519\u914d\u3002", "method": "\u8bbe\u8ba1\u4e25\u683c\u7684\u73a9\u5177\u6a21\u578b\u4eff\u771f\u6765\u8bc4\u4f30\u5404\u79cdPnPDP\u6c42\u89e3\u5668\u4e0d\u786e\u5b9a\u6027\u884c\u4e3a\uff0c\u63d0\u51fa\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u5728\u73a9\u5177\u4eff\u771f\u548c\u591a\u6837\u771f\u5b9e\u79d1\u5b66\u9006\u95ee\u9898\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\u3002", "result": "\u89c2\u5bdf\u5230\u4e0e\u6240\u63d0\u5206\u7c7b\u6cd5\u548c\u7406\u8bba\u9884\u671f\u4e00\u81f4\u7684\u4e0d\u786e\u5b9a\u6027\u884c\u4e3a\u6a21\u5f0f\uff0c\u63ed\u793a\u4e86\u4e0d\u540cPnPDP\u6c42\u89e3\u5668\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u7684\u5dee\u5f02\u6027\u8868\u73b0\u3002", "conclusion": "\u672c\u7814\u7a76\u5efa\u7acb\u4e86PnPDP\u6c42\u89e3\u5668\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u57fa\u51c6\u8bc4\u4f30\u4f53\u7cfb\uff0c\u4e3a\u79d1\u5b66\u5e94\u7528\u4e2d\u6b63\u786e\u7406\u89e3\u548c\u8bc4\u4f30\u6269\u6563\u9006\u95ee\u9898\u6c42\u89e3\u7684\u4e0d\u786e\u5b9a\u6027\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2602.04192", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04192", "abs": "https://arxiv.org/abs/2602.04192", "authors": ["Vivek Anand", "Alec Helbling", "Mark Davenport", "Gordon Berman", "Sankar Alagapan", "Christopher Rozell"], "title": "LORE: Jointly Learning the Intrinsic Dimensionality and Relative Similarity Structure From Ordinal Data", "comment": "10 Pages, 31 with appendix: Accepted at ICLR 2026", "summary": "Learning the intrinsic dimensionality of subjective perceptual spaces such as taste, smell, or aesthetics from ordinal data is a challenging problem. We introduce LORE (Low Rank Ordinal Embedding), a scalable framework that jointly learns both the intrinsic dimensionality and an ordinal embedding from noisy triplet comparisons of the form, \"Is A more similar to B than C?\". Unlike existing methods that require the embedding dimension to be set apriori, LORE regularizes the solution using the nonconvex Schatten-$p$ quasi norm, enabling automatic joint recovery of both the ordinal embedding and its dimensionality. We optimize this joint objective via an iteratively reweighted algorithm and establish convergence guarantees. Extensive experiments on synthetic datasets, simulated perceptual spaces, and real world crowdsourced ordinal judgements show that LORE learns compact, interpretable and highly accurate low dimensional embeddings that recover the latent geometry of subjective percepts. By simultaneously inferring both the intrinsic dimensionality and ordinal embeddings, LORE enables more interpretable and data efficient perceptual modeling in psychophysics and opens new directions for scalable discovery of low dimensional structure from ordinal data in machine learning.", "AI": {"tldr": "LORE is a scalable framework that jointly learns intrinsic dimensionality and ordinal embeddings from noisy triplet comparisons (\"Is A more similar to B than C?\") using Schatten-p regularization, eliminating the need for pre-setting dimensions while recovering compact, interpretable perceptual space structures.", "motivation": "Existing methods require pre-specifying embedding dimensions for ordinal data (e.g., taste/smell perception), which is impractical for unknown subjective perceptual spaces; LORE aims to jointly discover both the embedding and its intrinsic dimensionality from noisy triplet comparisons.", "method": "Uses nonconvex Schatten-$p$ quasi-norm regularization to enable automatic dimensionality recovery; optimized via an iteratively reweighted algorithm with proven convergence guarantees.", "result": "Outperforms existing methods on synthetic, simulated perceptual, and real crowdsourced data; produces compact, highly accurate low-dimensional embeddings that faithfully recover latent perceptual geometry.", "conclusion": "LORE enables interpretable and data-efficient perceptual modeling in psychophysics, offering a scalable solution for discovering low-dimensional structure in ordinal data with joint dimensionality inference."}}
{"id": "2602.04384", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.04384", "abs": "https://arxiv.org/abs/2602.04384", "authors": ["Fabio Turazza", "Alessandro Neri", "Marcello Pietri", "Maria Angela Butturi", "Marco Picone", "Marco Mamei"], "title": "Blockchain Federated Learning for Sustainable Retail: Reducing Waste through Collaborative Demand Forecasting", "comment": "Author-accepted manuscript of a paper published in the IEEE International Symposium on Computers and Communications (ISCC), 2025, pp. 1-6. doi: https://doi.org/10.1109/ISCC65549.2025.11326299", "summary": "Effective demand forecasting is crucial for reducing food waste. However, data privacy concerns often hinder collaboration among retailers, limiting the potential for improved predictive accuracy. In this study, we explore the application of Federated Learning (FL) in Sustainable Supply Chain Management (SSCM), with a focus on the grocery retail sector dealing with perishable goods. We develop a baseline predictive model for demand forecasting and waste assessment in an isolated retailer scenario. Subsequently, we introduce a Blockchain-based FL model, trained collaboratively across multiple retailers without direct data sharing. Our preliminary results show that FL models have performance almost equivalent to the ideal setting in which parties share data with each other, and are notably superior to models built by individual parties without sharing data, cutting waste and boosting efficiency.", "AI": {"tldr": "\u672c\u7814\u7a76\u9488\u5bf9\u98df\u54c1\u96f6\u552e\u884c\u4e1a\u56e0\u6570\u636e\u9690\u79c1\u9650\u5236\u5bfc\u81f4\u7684\u9884\u6d4b\u51c6\u786e\u6027\u4e0d\u8db3\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u8de8\u96f6\u552e\u5546\u534f\u540c\u9700\u6c42\u9884\u6d4b\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u6027\u80fd\u63a5\u8fd1\u7406\u60f3\u6570\u636e\u5171\u4eab\u573a\u666f\uff0c\u663e\u8457\u4f18\u4e8e\u5b64\u7acb\u5efa\u6a21\uff0c\u80fd\u6709\u6548\u51cf\u5c11\u98df\u54c1\u6d6a\u8d39\u3002", "motivation": "\u98df\u54c1\u6d6a\u8d39\u95ee\u9898\u4e25\u91cd\uff0c\u51c6\u786e\u7684\u9700\u6c42\u9884\u6d4b\u5bf9\u51cf\u5c11\u6d6a\u8d39\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u96f6\u552e\u5546\u95f4\u7684\u6570\u636e\u9690\u79c1\u987e\u8651\u963b\u788d\u4e86\u534f\u4f5c\uff0c\u9650\u5236\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u7684\u63d0\u5347\u6f5c\u529b\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u8054\u90a6\u5b66\u4e60\u5728\u53ef\u6301\u7eed\u4f9b\u5e94\u94fe\u7ba1\u7406\u4e2d\u7684\u5e94\u7528\uff0c\u89e3\u51b3\u6570\u636e\u5b64\u5c9b\u4e0e\u9690\u79c1\u4fdd\u62a4\u4e4b\u95f4\u7684\u77db\u76fe\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u9996\u5148\uff0c\u5728\u5b64\u7acb\u96f6\u552e\u5546\u573a\u666f\u4e0b\u5f00\u53d1\u57fa\u7ebf\u9884\u6d4b\u6a21\u578b\u7528\u4e8e\u9700\u6c42\u9884\u6d4b\u548c\u6d6a\u8d39\u8bc4\u4f30\uff1b\u7136\u540e\uff0c\u5f15\u5165\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u8054\u90a6\u5b66\u4e60\u6a21\u578b\uff0c\u4f7f\u591a\u4e2a\u96f6\u552e\u5546\u80fd\u591f\u5728\u4e0d\u5171\u4eab\u539f\u59cb\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u534f\u540c\u8bad\u7ec3\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u8868\u660e\uff0c\u8054\u90a6\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u51e0\u4e4e\u7b49\u540c\u4e8e\u7406\u60f3\u7684\u6570\u636e\u5171\u4eab\u573a\u666f\uff0c\u4e14\u663e\u8457\u4f18\u4e8e\u5404\u96f6\u552e\u5546\u5355\u72ec\u5efa\u6a21\u7684\u7ed3\u679c\uff0c\u80fd\u591f\u6709\u6548\u51cf\u5c11\u6d6a\u8d39\u5e76\u63d0\u5347\u6548\u7387\u3002", "conclusion": "\u8054\u90a6\u5b66\u4e60\u4e3a\u53ef\u6301\u7eed\u4f9b\u5e94\u94fe\u7ba1\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u7684\u534f\u540c\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u5728\u4e0d\u727a\u7272\u9690\u79c1\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u63a5\u8fd1\u96c6\u4e2d\u5f0f\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u5bf9\u51cf\u5c11\u98df\u54c1\u6d6a\u8d39\u5177\u6709\u91cd\u8981\u5b9e\u8df5\u4ef7\u503c\u3002"}}
{"id": "2602.04201", "categories": ["cs.LG", "math.DS"], "pdf": "https://arxiv.org/pdf/2602.04201", "abs": "https://arxiv.org/abs/2602.04201", "authors": ["Yanjie Tong", "Peng Chen"], "title": "From Sparse Sensors to Continuous Fields: STRIDE for Spatiotemporal Reconstruction", "comment": null, "summary": "Reconstructing high-dimensional spatiotemporal fields from sparse point-sensor measurements is a central challenge in learning parametric PDE dynamics. Existing approaches often struggle to generalize across trajectories and parameter settings, or rely on discretization-tied decoders that do not naturally transfer across meshes and resolutions. We propose STRIDE (Spatio-Temporal Recurrent Implicit DEcoder), a two-stage framework that maps a short window of sensor measurements to a latent state with a temporal encoder and reconstructs the field at arbitrary query locations with a modulated implicit neural representation (INR) decoder. Using the Fourier Multi-Component and Multi-Layer Neural Network (FMMNN) as the INR backbone improves representation of complex spatial fields and yields more stable optimization than sine-based INRs. We provide a conditional theoretical justification: under stable delay observability of point measurements on a low-dimensional parametric invariant set, the reconstruction operator factors through a finite-dimensional embedding, making STRIDE-type architectures natural approximators. Experiments on four challenging benchmarks spanning chaotic dynamics and wave propagation show that STRIDE outperforms strong baselines under extremely sparse sensing, supports super-resolution, and remains robust to noise.", "AI": {"tldr": "\u63d0\u51faSTRIDE\u6846\u67b6\uff0c\u7528\u65f6\u5e8f\u7f16\u7801\u5668\u548cFMMNN\u9690\u5f0f\u795e\u7ecf\u89e3\u7801\u5668\u4ece\u7a00\u758f\u4f20\u611f\u5668\u91cd\u5efa\u9ad8\u7ef4\u65f6\u7a7a\u573a\uff0c\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u7a00\u758f\u4f20\u611f\u5668\u91cd\u5efa\u9ad8\u7ef4\u65f6\u7a7a\u573a\u662f\u53c2\u6570\u5316PDE\u5b66\u4e60\u7684\u6838\u5fc3\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u6cdb\u5316\u6027\u5dee\u6216\u4f9d\u8d56\u79bb\u6563\u5316\u89e3\u7801\u5668\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u65f6\u5e8f\u7f16\u7801\u5668\u63d0\u53d6\u6f5c\u5728\u72b6\u6001\uff0c\u8c03\u5236FMMNN\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u89e3\u7801\u5668\u91cd\u5efa\u4efb\u610f\u4f4d\u7f6e\u573a\u503c\uff1b\u7406\u8bba\u8bc1\u660e\u5728\u7a33\u5b9a\u5ef6\u8fdf\u53ef\u89c2\u6d4b\u6761\u4ef6\u4e0b\u91cd\u5efa\u7b97\u5b50\u53ef\u6709\u9650\u7ef4\u5d4c\u5165\u3002", "result": "\u5728\u6df7\u6c8c\u52a8\u529b\u5b66\u548c\u6ce2\u4f20\u64ad\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSTRIDE\u5728\u6781\u7a00\u758f\u4f20\u611f\u3001\u8d85\u5206\u8fa8\u7387\u548c\u6297\u566a\u58f0\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "STRIDE\u4e3a\u7a00\u758f\u4f20\u611f\u91cd\u5efa\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6491\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u73b0\u4e86\u826f\u597d\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.04396", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04396", "abs": "https://arxiv.org/abs/2602.04396", "authors": ["Andrej Jovanovi\u0107", "Alex Iacob", "Mher Safaryan", "Ionut-Vlad Modoranu", "Lorenzo Sani", "William F. Shen", "Xinchi Qiu", "Dan Alistarh", "Nicholas D. Lane"], "title": "LoRDO: Distributed Low-Rank Optimization with Infrequent Communication", "comment": "Preprint; under review", "summary": "Distributed training of foundation models via $\\texttt{DDP}$ is limited by interconnect bandwidth. While infrequent communication strategies reduce synchronization frequency, they remain bottlenecked by the memory and communication requirements of optimizer states. Low-rank optimizers can alleviate these constraints; however, in the local-update regime, workers lack access to the full-batch gradients required to compute low-rank projections, which degrades performance. We propose $\\texttt{LoRDO}$, a principled framework unifying low-rank optimization with infrequent synchronization. We first demonstrate that, while global projections based on pseudo-gradients are theoretically superior, they permanently restrict the optimization trajectory to a low-rank subspace. To restore subspace exploration, we introduce a full-rank quasi-hyperbolic update. $\\texttt{LoRDO}$ achieves near-parity with low-rank $\\texttt{DDP}$ in language modeling and downstream tasks at model scales of $125$M--$720$M, while reducing communication by $\\approx 10 \\times$. Finally, we show that $\\texttt{LoRDO}$ improves performance even more in very low-memory settings with small rank/batch size.", "AI": {"tldr": "LoRDO\u662f\u4e00\u4e2a\u5c06\u4f4e\u79e9\u4f18\u5316\u4e0e\u4f4e\u9891\u540c\u6b65\u76f8\u7ed3\u5408\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u79e9\u51c6\u53cc\u66f2\u66f4\u65b0\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5c06\u901a\u4fe1\u91cf\u51cf\u5c11\u7ea610\u500d\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u53d7\u9650\u4e8e\u4e92\u8fde\u5e26\u5bbd\u548c\u4f18\u5316\u5668\u72b6\u6001\u7684\u5f00\u9500\u3002\u4f4e\u79e9\u4f18\u5316\u5668\u5728\u672c\u5730\u66f4\u65b0\u673a\u5236\u4e0b\u56e0\u65e0\u6cd5\u83b7\u53d6\u5168\u6279\u6b21\u68af\u5ea6\u800c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faLoRDO\u6846\u67b6\uff0c\u7edf\u4e00\u4f4e\u79e9\u4f18\u5316\u4e0e\u4f4e\u9891\u540c\u6b65\u3002\u5f15\u5165\u5168\u79e9\u51c6\u53cc\u66f2\u66f4\u65b0\u4ee5\u6062\u590d\u5b50\u7a7a\u95f4\u63a2\u7d22\uff0c\u89e3\u51b3\u4f2a\u68af\u5ea6\u5168\u5c40\u6295\u5f71\u5c06\u4f18\u5316\u8f68\u8ff9\u9650\u5236\u5728\u4f4e\u79e9\u5b50\u7a7a\u95f4\u7684\u95ee\u9898\u3002", "result": "\u57281.25\u4ebf\u81f37.2\u4ebf\u53c2\u6570\u89c4\u6a21\u7684\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e0a\uff0c\u6027\u80fd\u4e0e\u4f4e\u79e9DDP\u63a5\u8fd1\uff0c\u901a\u4fe1\u91cf\u51cf\u5c11\u7ea610\u500d\uff1b\u5728\u4f4e\u5185\u5b58\u548c\u5c0f\u79e9/\u6279\u6b21\u8bbe\u7f6e\u4e0b\u63d0\u5347\u66f4\u663e\u8457\u3002", "conclusion": "LoRDO\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u6548\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u5728\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u7684\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5185\u5b58\u53d7\u9650\u573a\u666f\u3002"}}
{"id": "2602.04447", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04447", "abs": "https://arxiv.org/abs/2602.04447", "authors": ["Giacomo Frisoni", "Lorenzo Molfetta", "Davide Freddi", "Gianluca Moro"], "title": "Mixture of Masters: Sparse Chess Language Models with Player Routing", "comment": null, "summary": "Modern chess language models are dense transformers trained on millions of games played by thousands of high-rated individuals. However, these monolithic networks tend to collapse into mode-averaged behavior, where stylistic boundaries are blurred, and rare but effective strategies are suppressed. To counteract homogenization, we introduce Mixture-of-Masters (MoM), the first chess mixture-of-experts model with small-sized GPT experts emulating world-class grandmasters. Each expert is trained with a combination of self-supervised learning and reinforcement learning guided by chess-specific rewards. For each move, a post-hoc learnable gating network selects the most appropriate persona to channel depending on the game state, allowing MoM to switch its style dynamically$--$e.g., Tal's offensive vocation or Petrosian's defensive solidity. When evaluated against Stockfish on unseen standard games, MoM outperforms both dense individual expert networks and popular GPT baselines trained on aggregated data, while ensuring generation variety, control, and interpretability.", "AI": {"tldr": "We propose Mixture-of-Masters (MoM), a chess mixture-of-experts model that emulates world-class grandmasters to avoid mode-averaged behavior in existing dense transformers, outperforming Stockfish and baselines while enabling dynamic style switching and interpretability.", "motivation": "Modern chess language models are dense transformers that collapse into mode-averaged behavior, blurring stylistic boundaries and suppressing rare but effective strategies, leading to homogenization.", "method": "Introduce MoM, the first chess mixture-of-experts model with small GPT experts trained via self-supervised and reinforcement learning with chess-specific rewards. A learnable gating network selects grandmaster personas (e.g., Tal's offense, Petrosian's defense) dynamically based on game state.", "result": "MoM outperforms Stockfish, dense individual expert networks, and aggregated GPT baselines on unseen games, while ensuring generation variety, control, and interpretability.", "conclusion": "MoM successfully counteracts model homogenization by preserving stylistic diversity and rare strategies, offering a controllable and interpretable approach to chess AI that can dynamically adapt playing styles like human grandmasters."}}
{"id": "2602.04244", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04244", "abs": "https://arxiv.org/abs/2602.04244", "authors": ["Qi Feng", "Jicong Fan"], "title": "Training A Foundation Model to Represent Graphs as Vectors", "comment": null, "summary": "This paper aims to train a graph foundation model that is able to represent any graph as a vector preserving structural and semantic information useful for downstream graph-level tasks such as graph classification and graph clustering. To learn the features of graphs from diverse domains while maintaining strong generalization ability to new domains, we propose a multi-graph-based feature alignment method, which constructs weighted graphs using the attributes of all nodes in each dataset and then generates consistent node embeddings. To enhance the consistency of the features from different datasets, we propose a density maximization mean alignment algorithm with guaranteed convergence. The original graphs and generated node embeddings are fed into a graph neural network to achieve discriminative graph representations in contrastive learning. More importantly, to enhance the information preservation from node-level representations to the graph-level representation, we construct a multi-layer reference distribution module without using any pooling operation. We also provide a theoretical generalization bound to support the effectiveness of the proposed model. The experimental results of few-shot graph classification and graph clustering show that our model outperforms strong baselines.", "AI": {"tldr": "A graph foundation model using multi-graph feature alignment, density maximization mean alignment, and a pooling-free multi-layer reference distribution module for generalizable graph representation learning.", "motivation": "To develop a universal graph representation model that preserves structural and semantic information across diverse domains while maintaining strong generalization ability to unseen domains for downstream tasks like graph classification and clustering.", "method": "Proposes a multi-graph-based feature alignment method that constructs weighted graphs from node attributes to generate consistent embeddings; introduces a density maximization mean alignment algorithm with guaranteed convergence; employs contrastive learning with a GNN on original graphs and embeddings; designs a pooling-free multi-layer reference distribution module to preserve node-to-graph information.", "result": "Provides theoretical generalization bounds and demonstrates superior performance over strong baselines on few-shot graph classification and graph clustering tasks.", "conclusion": "The proposed model successfully learns discriminative, generalizable graph representations through innovative alignment and information preservation mechanisms, establishing an effective graph foundation model."}}
{"id": "2602.04255", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04255", "abs": "https://arxiv.org/abs/2602.04255", "authors": ["Hanlin Pan", "Yuhao Tang", "Wanfu Gao"], "title": "From Ambiguity to Action: A POMDP Perspective on Partial Multi-Label Ambiguity and Its Horizon-One Resolution", "comment": null, "summary": "In partial multi-label learning (PML), the true labels are unobserved, which makes label disambiguation important but difficult. A key challenge is that ambiguous candidate labels can propagate errors into downstream tasks such as feature engineering. To solve this issue, we jointly model the disambiguation and feature selection tasks as Partially Observable Markov Decision Processes (POMDP) to turn PML risk minimization into expected-return maximization. Stage 1 trains a transformer policy via reinforcement learning to produce high-quality hard pseudo-labels; Stage 2 describes feature selection as a sequential reinforcement learning problem, selecting features step by step and outputting an interpretable global ranking. We further provide the theoretical analysis of PML-POMDP correspondence and the excess-risk bound that decompose the error into pseudo label quality term and sample size. Experiments in multiple metrics and data sets verify the advantages of the framework.", "AI": {"tldr": "\u63d0\u51faPML-POMDP\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8054\u5408\u89e3\u51b3\u90e8\u5206\u591a\u6807\u7b7e\u5b66\u4e60\u4e2d\u7684\u6807\u7b7e\u6d88\u6b67\u548c\u7279\u5f81\u9009\u62e9\u95ee\u9898\uff0c\u7b2c\u4e00\u9636\u6bb5\u7528Transformer\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u7b2c\u4e8c\u9636\u6bb5\u8fdb\u884c\u53ef\u89e3\u91ca\u7684\u7279\u5f81\u6392\u5e8f\uff0c\u7406\u8bba\u5206\u6790\u8d85\u989d\u98ce\u9669\u754c", "motivation": "\u5728\u90e8\u5206\u591a\u6807\u7b7e\u5b66\u4e60\u4e2d\uff0c\u771f\u5b9e\u6807\u7b7e\u672a\u77e5\u5bfc\u81f4\u5019\u9009\u6807\u7b7e\u5b58\u5728\u6b67\u4e49\uff0c\u9519\u8bef\u4f1a\u4f20\u64ad\u5230\u7279\u5f81\u5de5\u7a0b\u7b49\u4e0b\u6e38\u4efb\u52a1\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5904\u7406\u6807\u7b7e\u6d88\u6b67\u548c\u7279\u5f81\u9009\u62e9\u4e24\u4e2a\u8026\u5408\u95ee\u9898", "method": "\u5c06\u4e24\u4e2a\u4efb\u52a1\u7edf\u4e00\u5efa\u6a21\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b(POMDP)\uff0c\u8f6c\u5316\u4e3a\u671f\u671b\u56de\u62a5\u6700\u5927\u5316\u95ee\u9898\uff1a\u9636\u6bb5\u4e00\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3Transformer\u7b56\u7565\u751f\u6210\u9ad8\u8d28\u91cf\u786c\u4f2a\u6807\u7b7e\uff1b\u9636\u6bb5\u4e8c\u5c06\u7279\u5f81\u9009\u62e9\u89c6\u4e3a\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\uff0c\u9010\u6b65\u9009\u62e9\u7279\u5f81\u5e76\u8f93\u51fa\u53ef\u89e3\u91ca\u7684\u5168\u5c40\u6392\u5e8f", "result": "\u7406\u8bba\u8bc1\u660e\u4e86PML-POMDP\u7684\u5bf9\u5e94\u5173\u7cfb\u548c\u8d85\u989d\u98ce\u9669\u754c\uff0c\u8be5\u754c\u53ef\u5206\u89e3\u4e3a\u4f2a\u6807\u7b7e\u8d28\u91cf\u548c\u6837\u672c\u91cf\u4e24\u4e2a\u56e0\u7d20\uff1b\u5728\u591a\u6307\u6807\u548c\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u4f18\u52bf", "conclusion": "\u9996\u6b21\u5c06\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5f15\u5165\u90e8\u5206\u591a\u6807\u7b7e\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u6807\u7b7e\u6d88\u6b67\u4e0e\u7279\u5f81\u9009\u62e9\u7684\u534f\u540c\u4f18\u5316\uff0c\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u8df5\u6709\u6548\u6027\uff0c\u4e3a\u5904\u7406\u6807\u7b7e\u6b67\u4e49\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f"}}
{"id": "2602.04663", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04663", "abs": "https://arxiv.org/abs/2602.04663", "authors": ["Jaemoo Choi", "Yuchen Zhu", "Wei Guo", "Petr Molodyk", "Bo Yuan", "Jinbin Bai", "Yi Xin", "Molei Tao", "Yongxin Chen"], "title": "Rethinking the Design Space of Reinforcement Learning for Diffusion Models: On the Importance of Likelihood Estimation Beyond Loss Design", "comment": "23 pages, 11 figures", "summary": "Reinforcement learning has been widely applied to diffusion and flow models for visual tasks such as text-to-image generation. However, these tasks remain challenging because diffusion models have intractable likelihoods, which creates a barrier for directly applying popular policy-gradient type methods. Existing approaches primarily focus on crafting new objectives built on already heavily engineered LLM objectives, using ad hoc estimators for likelihood, without a thorough investigation into how such estimation affects overall algorithmic performance. In this work, we provide a systematic analysis of the RL design space by disentangling three factors: i) policy-gradient objectives, ii) likelihood estimators, and iii) rollout sampling schemes. We show that adopting an evidence lower bound (ELBO) based model likelihood estimator, computed only from the final generated sample, is the dominant factor enabling effective, efficient, and stable RL optimization, outweighing the impact of the specific policy-gradient loss functional. We validate our findings across multiple reward benchmarks using SD 3.5 Medium, and observe consistent trends across all tasks. Our method improves the GenEval score from 0.24 to 0.95 in 90 GPU hours, which is $4.6\\times$ more efficient than FlowGRPO and $2\\times$ more efficient than the SOTA method DiffusionNFT without reward hacking.", "AI": {"tldr": "This paper systematically analyzes reinforcement learning design for diffusion models, finding that using an ELBO-based likelihood estimator from final samples is the key factor for effective, efficient, and stable optimization.", "motivation": "Diffusion models have intractable likelihoods that block direct policy-gradient RL application. Existing methods use ad hoc likelihood estimators without thorough investigation of how estimation affects algorithmic performance.", "method": "Systematically disentangle RL design space into three factors: policy-gradient objectives, likelihood estimators, and rollout sampling schemes. Propose using ELBO-based model likelihood estimator computed from final generated samples only.", "result": "ELBO-based estimator is the dominant factor enabling effective, efficient, and stable RL optimization, outweighing policy-gradient loss choice. Validated across multiple reward benchmarks using SD 3.5 Medium.", "conclusion": "Method improves GenEval score from 0.24 to 0.95 in 90 GPU hours, achieving 4.6\u00d7 higher efficiency than FlowGRPO and 2\u00d7 higher than SOTA DiffusionNFT without reward hacking."}}
{"id": "2602.04669", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.04669", "abs": "https://arxiv.org/abs/2602.04669", "authors": ["Xianbiao Qi", "Marco Chen", "Jiaquan Ye", "Yelin He", "Rong Xiao"], "title": "Delving into Muon and Beyond: Deep Analysis and Extensions", "comment": "This paper studies matrix-based optimizers (e.g., Muon) from a spectral perspective and unifies a range of methods under a common spectral framework", "summary": "The Muon optimizer has recently attracted considerable attention for its strong empirical performance and use of orthogonalized updates on matrix-shaped parameters, yet its underlying mechanisms and relationship to adaptive optimizers such as Adam remain insufficiently understood. In this work, we aim to address these questions through a unified spectral perspective. Specifically, we view Muon as the p = 0 endpoint of a family of spectral transformations of the form U \\boldsymbol\u03a3^{p} V' , and consider additional variants with p = 1/2 , p = 1/4 , and p = 1 . These transformations are applied to both first-moment updates, as in momentum SGD, and to root-mean-square (RMS) normalized gradient updates as in Adam. To enable efficient computation, we develop a coupled Newton iteration that avoids explicit singular value decomposition. Across controlled experiments, we find that RMS-normalized updates yield more stable optimization than first-moment updates. Moreover, while spectral compression provides strong stabilization benefits under first-moment updates, the Muon update (p = 0) does not consistently outperform Adam. These results suggest that Muon is best understood as an effective form of spectral normalization, but not a universally superior optimization method. Our source code will be released at https://github.com/Ocram7/BeyondMuon.", "AI": {"tldr": "This paper analyzes the Muon optimizer through a unified spectral perspective, viewing it as a special case of spectral transformations. The study finds that Muon is an effective form of spectral normalization but doesn't consistently outperform Adam, with RMS-normalized updates showing more stable optimization than first-moment updates.", "motivation": "The Muon optimizer shows strong empirical performance with orthogonalized updates on matrix-shaped parameters, but its underlying mechanisms and relationship to adaptive optimizers like Adam remain insufficiently understood.", "method": "The authors present a unified spectral perspective, viewing Muon as the p=0 endpoint of spectral transformations U\u03a3^pV'. They test variants (p=1/2, 1/4, 1) on both first-moment and RMS-normalized gradient updates, developing a coupled Newton iteration to avoid explicit SVD for efficient computation.", "result": "RMS-normalized updates yield more stable optimization than first-moment updates. While spectral compression stabilizes first-moment updates, Muon (p=0) does not consistently outperform Adam.", "conclusion": "Muon is best understood as an effective form of spectral normalization, but not a universally superior optimization method compared to Adam."}}
{"id": "2602.04270", "categories": ["cs.LG", "q-bio.NC", "q-bio.QM", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.04270", "abs": "https://arxiv.org/abs/2602.04270", "authors": ["Noga Mudrik", "Yuxi Chen", "Gal Mishne", "Adam S. Charles"], "title": "Multi-Integration of Labels across Categories for Component Identification (MILCCI)", "comment": null, "summary": "Many fields collect large-scale temporal data through repeated measurements (trials), where each trial is labeled with a set of metadata variables spanning several categories. For example, a trial in a neuroscience study may be linked to a value from category (a): task difficulty, and category (b): animal choice. A critical challenge in time-series analysis is to understand how these labels are encoded within the multi-trial observations, and disentangle the distinct effect of each label entry across categories. Here, we present MILCCI, a novel data-driven method that i) identifies the interpretable components underlying the data, ii) captures cross-trial variability, and iii) integrates label information to understand each category's representation within the data. MILCCI extends a sparse per-trial decomposition that leverages label similarities within each category to enable subtle, label-driven cross-trial adjustments in component compositions and to distinguish the contribution of each category. MILCCI also learns each component's corresponding temporal trace, which evolves over time within each trial and varies flexibly across trials. We demonstrate MILCCI's performance through both synthetic and real-world examples, including voting patterns, online page view trends, and neuronal recordings.", "AI": {"tldr": "MILCCI is a novel method for analyzing temporal data with multi-category metadata, identifying interpretable components while disentangling category-specific effects.", "motivation": "Large-scale temporal data from repeated trials with multi-category metadata labels pose a challenge in understanding how labels are encoded and disentangling distinct effects across categories.", "method": "Extends sparse per-trial decomposition by leveraging label similarities within categories for label-driven cross-trial adjustments, learning temporal traces that evolve within and vary across trials.", "result": "Validated on synthetic and real-world data including voting patterns, page views, and neuronal recordings.", "conclusion": "Successfully integrates label information to reveal category representations in temporal data, enabling nuanced analysis of multi-trial observations."}}
{"id": "2602.04678", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04678", "abs": "https://arxiv.org/abs/2602.04678", "authors": ["Zhen Zhou", "Zhirui Wang", "Qi Hong", "Yunyang Shi", "Ziyuan Gu", "Zhiyuan Liu"], "title": "Let Experts Feel Uncertainty: A Multi-Expert Label Distribution Approach to Probabilistic Time Series Forecasting", "comment": "11 pages, 2figures", "summary": "Time series forecasting in real-world applications requires both high predictive accuracy and interpretable uncertainty quantification. Traditional point prediction methods often fail to capture the inherent uncertainty in time series data, while existing probabilistic approaches struggle to balance computational efficiency with interpretability. We propose a novel Multi-Expert Learning Distributional Labels (LDL) framework that addresses these challenges through mixture-of-experts architectures with distributional learning capabilities. Our approach introduces two complementary methods: (1) Multi-Expert LDL, which employs multiple experts with different learned parameters to capture diverse temporal patterns, and (2) Pattern-Aware LDL-MoE, which explicitly decomposes time series into interpretable components (trend, seasonality, changepoints, volatility) through specialized sub-experts. Both frameworks extend traditional point prediction to distributional learning, enabling rich uncertainty quantification through Maximum Mean Discrepancy (MMD). We evaluate our methods on aggregated sales data derived from the M5 dataset, demonstrating superior performance compared to baseline approaches. The continuous Multi-Expert LDL achieves the best overall performance, while the Pattern-Aware LDL-MoE provides enhanced interpretability through component-wise analysis. Our frameworks successfully balance predictive accuracy with interpretability, making them suitable for real-world forecasting applications where both performance and actionable insights are crucial.", "AI": {"tldr": "\u63d0\u51fa\u591a\u4e13\u5bb6\u5b66\u4e60\u5206\u5e03\u6807\u7b7e\uff08LDL\uff09\u6846\u67b6\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u51c6\u786e\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u4e0d\u786e\u5b9a\u91cf\u5316\u7684\u5e73\u8861\u95ee\u9898\uff0c\u5305\u542b\u591a\u4e13\u5bb6LDL\u548c\u6a21\u5f0f\u611f\u77e5LDL-MoE\u4e24\u79cd\u65b9\u6cd5\uff0c\u5728M5\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u9700\u8981\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u548c\u53ef\u89e3\u91ca\u7684\u4e0d\u786e\u5b9a\u91cf\u5316\uff0c\u4f46\u4f20\u7edf\u70b9\u9884\u6d4b\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u4e0d\u786e\u5b9a\u6027\uff0c\u73b0\u6709\u6982\u7387\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u7684\u591a\u4e13\u5bb6\u5b66\u4e60\u5206\u5e03\u6807\u7b7e\u6846\u67b6\uff1a1\uff09\u591a\u4e13\u5bb6LDL\u4f7f\u7528\u4e0d\u540c\u53c2\u6570\u4e13\u5bb6\u6355\u6349\u591a\u6837\u65f6\u5e8f\u6a21\u5f0f\uff1b2\uff09\u6a21\u5f0f\u611f\u77e5LDL-MoE\u901a\u8fc7\u4e13\u7528\u5b50\u4e13\u5bb6\u663e\u5f0f\u5206\u89e3\u8d8b\u52bf\u3001\u5b63\u8282\u6027\u3001\u53d8\u70b9\u3001\u6ce2\u52a8\u6027\u7b49\u53ef\u89e3\u91ca\u6210\u5206\uff0c\u5e76\u91c7\u7528\u6700\u5927\u5747\u503c\u5dee\u5f02\u6269\u5c55\u5206\u5e03\u5b66\u4e60\u3002", "result": "\u5728M5\u805a\u5408\u9500\u552e\u6570\u636e\u4e0a\u8bc4\u4f30\u663e\u793a\uff0c\u8fde\u7eed\u591a\u4e13\u5bb6LDL\u53d6\u5f97\u6700\u4f73\u6574\u4f53\u6027\u80fd\uff0c\u6a21\u5f0f\u611f\u77e5LDL-MoE\u901a\u8fc7\u5206\u91cf\u5206\u6790\u63d0\u4f9b\u66f4\u5f3a\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4e24\u8005\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5e73\u8861\u9884\u6d4b\u51c6\u786e\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\uff0c\u9002\u7528\u4e8e\u9700\u8981\u6027\u80fd\u4e0e\u53ef\u64cd\u4f5c\u89c1\u89e3\u7684\u5b9e\u9645\u9884\u6d4b\u573a\u666f\u3002"}}
{"id": "2602.04718", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.04718", "abs": "https://arxiv.org/abs/2602.04718", "authors": ["Moritz Miller", "Florent Draye", "Bernhard Sch\u00f6lkopf"], "title": "Identifying Intervenable and Interpretable Features via Orthogonality Regularization", "comment": null, "summary": "With recent progress on fine-tuning language models around a fixed sparse autoencoder, we disentangle the decoder matrix into almost orthogonal features. This reduces interference and superposition between the features, while keeping performance on the target dataset essentially unchanged. Our orthogonality penalty leads to identifiable features, ensuring the uniqueness of the decomposition. Further, we find that the distance between embedded feature explanations increases with stricter orthogonality penalty, a desirable property for interpretability. Invoking the $\\textit{Independent Causal Mechanisms}$ principle, we argue that orthogonality promotes modular representations amenable to causal intervention. We empirically show that these increasingly orthogonalized features allow for isolated interventions. Our code is available under $\\texttt{https://github.com/mrtzmllr/sae-icm}$.", "AI": {"tldr": "The paper proposes applying orthogonality penalties to sparse autoencoder decoder matrices during language model fine-tuning to disentangle features, reduce interference, improve interpretability, and enable isolated causal interventions while preserving performance.", "motivation": "To address feature interference and superposition in sparse autoencoders used for language model fine-tuning, while improving interpretability and enabling modular representations that support causal interventions.", "method": "Fine-tuning language models with a fixed sparse autoencoder while adding an orthogonality penalty to the decoder matrix, motivated by the Independent Causal Mechanisms principle.", "result": "The orthogonality penalty reduces interference and superposition, maintains target dataset performance, creates identifiable features, increases distance between feature explanations (enhancing interpretability), and enables isolated causal interventions on the orthogonalized features.", "conclusion": "Orthogonalizing sparse autoencoder features successfully creates modular, interpretable representations amenable to causal intervention, with publicly available code for reproducibility and further research."}}
{"id": "2602.04735", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.04735", "abs": "https://arxiv.org/abs/2602.04735", "authors": ["Mengru Wang", "Zhenqian Xu", "Junfeng Fang", "Yunzhi Yao", "Shumin Deng", "Huajun Chen", "Ningyu Zhang"], "title": "From Data to Behavior: Predicting Unintended Model Behaviors Before Training", "comment": "Work in progress", "summary": "Large Language Models (LLMs) can acquire unintended biases from seemingly benign training data even without explicit cues or malicious content. Existing methods struggle to detect such risks before fine-tuning, making post hoc evaluation costly and inefficient. To address this challenge, we introduce Data2Behavior, a new task for predicting unintended model behaviors prior to training. We also propose Manipulating Data Features (MDF), a lightweight approach that summarizes candidate data through their mean representations and injects them into the forward pass of a base model, allowing latent statistical signals in the data to shape model activations and reveal potential biases and safety risks without updating any parameters. MDF achieves reliable prediction while consuming only about 20% of the GPU resources required for fine-tuning. Experiments on Qwen3-14B, Qwen2.5-32B-Instruct, and Gemma-3-12b-it confirm that MDF can anticipate unintended behaviors and provide insight into pre-training vulnerabilities.", "AI": {"tldr": "\u63d0\u51faData2Behavior\u4efb\u52a1\u9884\u6d4b\u5927\u6a21\u578b\u8bad\u7ec3\u524d\u6f5c\u5728\u504f\u89c1\uff0c\u8bbe\u8ba1MDF\u65b9\u6cd5\u901a\u8fc7\u6ce8\u5165\u6570\u636e\u5747\u503c\u8868\u5f81\u6765\u63a2\u6d4b\u6a21\u578b\u6fc0\u6d3b\uff0c\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u5373\u53ef\u9884\u6d4b\u98ce\u9669\uff0c\u8282\u770180%\u8bad\u7ec3\u8d44\u6e90", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u5fae\u8c03\u524d\u68c0\u6d4b\u5927\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u65e0\u610f\u504f\u89c1\uff0c\u4e8b\u540e\u8bc4\u4f30\u6210\u672c\u9ad8\uff1b\u9700\u63d0\u524d\u9884\u6d4b\u98ce\u9669\u4ee5\u907f\u514d\u8d44\u6e90\u6d6a\u8d39", "method": "1. \u5b9a\u4e49Data2Behavior\u65b0\u4efb\u52a1\uff1a\u9884\u8bad\u7ec3\u524d\u9884\u6d4b\u6a21\u578b\u610f\u5916\u884c\u4e3a\n2. \u63d0\u51faMDF\u65b9\u6cd5\uff1a\u8ba1\u7b97\u5019\u9009\u6570\u636e\u5747\u503c\u8868\u5f81\uff0c\u6ce8\u5165\u57fa\u7840\u6a21\u578b\u524d\u5411\u4f20\u64ad\u8fc7\u7a0b\uff0c\u5229\u7528\u6570\u636e\u7edf\u8ba1\u4fe1\u53f7\u5851\u9020\u6a21\u578b\u6fc0\u6d3b\n3. \u901a\u8fc7\u6fc0\u6d3b\u6a21\u5f0f\u63ed\u793a\u6f5c\u5728\u504f\u89c1\u548c\u5b89\u5168\u98ce\u9669\uff0c\u5168\u7a0b\u4e0d\u66f4\u65b0\u4efb\u4f55\u53c2\u6570", "result": "1. MDF\u4ec5\u970020%\u5fae\u8c03GPU\u8d44\u6e90\u5373\u53ef\u5b9e\u73b0\u53ef\u9760\u9884\u6d4b\n2. \u5728Qwen3-14B\u3001Qwen2.5-32B-Instruct\u548cGemma-3-12b-it\u4e0a\u9a8c\u8bc1\u6709\u6548\n3. \u53ef\u63d0\u524d\u53d1\u73b0\u6a21\u578b\u504f\u89c1\u884c\u4e3a\u5e76\u6d1e\u5bdf\u9884\u8bad\u7ec3\u6f0f\u6d1e", "conclusion": "MDF\u4e3a\u8bad\u7ec3\u524d\u504f\u89c1\u68c0\u6d4b\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u96f6\u53c2\u6570\u66f4\u65b0\u7684\u6fc0\u6d3b\u5206\u6790\u5b9e\u73b0\u98ce\u9669\u9884\u8b66\uff0c\u4e3a\u6a21\u578b\u5b89\u5168\u5f00\u53d1\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2602.04768", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04768", "abs": "https://arxiv.org/abs/2602.04768", "authors": ["Maya Bechler-Speicher", "Yoel Gottlieb", "Andrey Isakov", "David Abensur", "Ami Tavory", "Daniel Haimovich", "Ido Guy", "Udi Weinsberg"], "title": "Billion-Scale Graph Foundation Models", "comment": null, "summary": "Graph-structured data underpins many critical applications. While foundation models have transformed language and vision via large-scale pretraining and lightweight adaptation, extending this paradigm to general, real-world graphs is challenging. In this work, we present Graph Billion- Foundation-Fusion (GraphBFF): the first end-to-end recipe for building billion-parameter Graph Foundation Models (GFMs) for arbitrary heterogeneous, billion-scale graphs. Central to the recipe is the GraphBFF Transformer, a flexible and scalable architecture designed for practical billion-scale GFMs. Using the GraphBFF, we present the first neural scaling laws for general graphs and show that loss decreases predictably as either model capacity or training data scales, depending on which factor is the bottleneck. The GraphBFF framework provides concrete methodologies for data batching, pretraining, and fine-tuning for building GFMs at scale. We demonstrate the effectiveness of the framework with an evaluation of a 1.4 billion-parameter GraphBFF Transformer pretrained on one billion samples. Across ten diverse, real-world downstream tasks on graphs unseen during training, spanning node- and link-level classification and regression, GraphBFF achieves remarkable zero-shot and probing performance, including in few-shot settings, with large margins of up to 31 PRAUC points. Finally, we discuss key challenges and open opportunities for making GFMs a practical and principled foundation for graph learning at industrial scale.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86GraphBFF\uff0c\u9996\u4e2a\u7528\u4e8e\u6784\u5efa\u5341\u4ebf\u53c2\u6570\u56fe\u57fa\u7840\u6a21\u578b\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u53ef\u9884\u6d4b\u7684\u7f29\u653e\u89c4\u5f8b\u548c\u5728\u591a\u6837\u5316\u56fe\u4efb\u52a1\u4e0a\u7684\u5f3a\u5927\u96f6\u6837\u672c\u6027\u80fd\u3002", "motivation": "\u56fe\u7ed3\u6784\u6570\u636e\u652f\u6491\u7740\u8bb8\u591a\u5173\u952e\u5e94\u7528\uff0c\u4f46\u7531\u4e8e\u56fe\u6570\u636e\u7684\u5f02\u6784\u6027\u548c\u5927\u89c4\u6a21\u7279\u6027\uff0c\u5c06\u57fa\u7840\u6a21\u578b\u8303\u5f0f\uff08\u5728\u8bed\u8a00\u548c\u89c6\u89c9\u9886\u57df\u5df2\u6210\u529f\uff09\u6269\u5c55\u5230\u901a\u7528\u771f\u5b9e\u4e16\u754c\u56fe\u9762\u4e34\u6311\u6218\u3002", "method": "\u4f5c\u8005\u63d0\u51faGraphBFF\u6846\u67b6\uff0c\u5305\u542b\u7075\u6d3b\u7684GraphBFF Transformer\u67b6\u6784\uff0c\u5efa\u7acb\u4e86\u901a\u7528\u56fe\u7684\u795e\u7ecf\u7f29\u653e\u89c4\u5f8b\uff0c\u5e76\u63d0\u4f9b\u4e86\u6570\u636e\u6279\u5904\u7406\u3001\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u7684\u53ef\u6269\u5c55\u65b9\u6cd5\u3002", "result": "\u5728\u5341\u4ebf\u6837\u672c\u4e0a\u9884\u8bad\u7ec3\u768414\u4ebf\u53c2\u6570GraphBFF Transformer\u5728\u5341\u4e2a\u672a\u89c1\u8fc7\u7684\u771f\u5b9e\u4e16\u754c\u4e0b\u6e38\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2dPRAUC\u6307\u6807\u63d0\u5347\u9ad8\u8fbe31\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u8bc1\u660e\u4e86\u5341\u4ebf\u5c3a\u5ea6\u56fe\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u5de5\u4e1a\u7ea7\u56fe\u5b66\u4e60\u5b9e\u7528\u57fa\u7840\u7684\u53ef\u884c\u6027\uff0c\u540c\u65f6\u8ba8\u8bba\u4e86\u5176\u9762\u4e34\u7684\u6311\u6218\u548c\u5f00\u653e\u673a\u9047\u3002"}}
{"id": "2602.04785", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04785", "abs": "https://arxiv.org/abs/2602.04785", "authors": ["Congjing Zhang", "Ryan Feng Lin", "Ruoxuan Bao", "Shuai Huang"], "title": "Team, Then Trim: An Assembly-Line LLM Framework for High-Quality Tabular Data Generation", "comment": null, "summary": "While tabular data is fundamental to many real-world machine learning (ML) applications, acquiring high-quality tabular data is usually labor-intensive and expensive. Limited by the scarcity of observations, tabular datasets often exhibit critical deficiencies, such as class imbalance, selection bias, and low fidelity. To address these challenges, building on recent advances in Large Language Models (LLMs), this paper introduces Team-then-Trim (T$^2$), a framework that synthesizes high-quality tabular data through a collaborative team of LLMs, followed by a rigorous three-stage plug-in data quality control (QC) pipeline. In T$^2$, tabular data generation is conceptualized as a manufacturing process: specialized LLMs, guided by domain knowledge, are tasked with generating different data components sequentially, and the resulting products, i.e., the synthetic data, are systematically evaluated across multiple dimensions of QC. Empirical results on both simulated and real-world datasets demonstrate that T$^2$ outperforms state-of-the-art methods in producing high-quality tabular data, highlighting its potential to support downstream models when direct data collection is practically infeasible.", "AI": {"tldr": "Team-then-Trim (T^2) \u662f\u4e00\u4e2a\u901a\u8fc7LLM\u534f\u4f5c\u56e2\u961f\u5408\u6210\u9ad8\u8d28\u91cf\u8868\u683c\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u4e09\u9636\u6bb5\u8d28\u91cf\u63a7\u5236\u6d41\u7a0b\u7684\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u7b49\u95ee\u9898\u3002", "motivation": "\u5b9e\u9645\u673a\u5668\u5b66\u4e60\u5e94\u7528\u4e2d\uff0c\u9ad8\u8d28\u91cf\u8868\u683c\u6570\u636e\u83b7\u53d6\u6210\u672c\u9ad8\u6602\u4e14\u5e38\u5b58\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u3001\u9009\u62e9\u504f\u5dee\u3001\u4f4e\u4fdd\u771f\u5ea6\u7b49\u7f3a\u9677\uff0c\u4e9f\u9700\u9ad8\u6548\u7684\u6570\u636e\u5408\u6210\u65b9\u6cd5\u5f25\u8865\u6570\u636e\u7a00\u7f3a\u3002", "method": "\u5c06\u8868\u683c\u6570\u636e\u751f\u6210\u7c7b\u6bd4\u4e3a\u5236\u9020\u8fc7\u7a0b\uff1a\u7531\u9886\u57df\u77e5\u8bc6\u6307\u5bfc\u7684\u4e13\u95e8\u5316LLM\u56e2\u961f\u987a\u5e8f\u751f\u6210\u4e0d\u540c\u6570\u636e\u7ec4\u4ef6\uff0c\u518d\u901a\u8fc7\u4e09\u9636\u6bb5\u63d2\u4ef6\u5f0f\u8d28\u91cf\u63a7\u5236\u7ba1\u9053\u5bf9\u5408\u6210\u6570\u636e\u8fdb\u884c\u591a\u7ef4\u5ea6\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cT^2\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u8868\u683c\u6570\u636e\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u76f4\u63a5\u6570\u636e\u6536\u96c6\u4e0d\u53ef\u884c\u65f6\uff0c\u80fd\u4e3a\u4e0b\u6e38\u6a21\u578b\u63d0\u4f9b\u6709\u6548\u652f\u6301\uff0c\u5177\u6709\u89e3\u51b3\u5b9e\u9645\u6570\u636e\u83b7\u53d6\u96be\u9898\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.04809", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04809", "abs": "https://arxiv.org/abs/2602.04809", "authors": ["Elizabeth Bates", "Chris Hicks", "Vasilios Mavroudis"], "title": "Beyond Rewards in Reinforcement Learning for Cyber Defence", "comment": null, "summary": "Recent years have seen an explosion of interest in autonomous cyber defence agents trained to defend computer networks using deep reinforcement learning. These agents are typically trained in cyber gym environments using dense, highly engineered reward functions which combine many penalties and incentives for a range of (un)desirable states and costly actions. Dense rewards help alleviate the challenge of exploring complex environments but risk biasing agents towards suboptimal and potentially riskier solutions, a critical issue in complex cyber environments. We thoroughly evaluate the impact of reward function structure on learning and policy behavioural characteristics using a variety of sparse and dense reward functions, two well-established cyber gyms, a range of network sizes, and both policy gradient and value-based RL algorithms. Our evaluation is enabled by a novel ground truth evaluation approach which allows directly comparing between different reward functions, illuminating the nuanced inter-relationships between rewards, action space and the risks of suboptimal policies in cyber environments. Our results show that sparse rewards, provided they are goal aligned and can be encountered frequently, uniquely offer both enhanced training reliability and more effective cyber defence agents with lower-risk policies. Surprisingly, sparse rewards can also yield policies that are better aligned with cyber defender goals and make sparing use of costly defensive actions without explicit reward-based numerical penalties.", "AI": {"tldr": "This paper evaluates sparse versus dense reward functions for autonomous cyber defence agents trained with deep reinforcement learning, finding that sparse rewards yield more reliable training and lower-risk policies.", "motivation": "Dense reward functions used in training autonomous cyber defence agents risk biasing them toward suboptimal and risky solutions in complex cyber environments, necessitating investigation of alternative reward structures.", "method": "The authors compare sparse and dense reward functions across two cyber gyms, various network sizes, and RL algorithms using a novel ground truth evaluation approach to analyze learning and policy characteristics.", "result": "Sparse rewards (when goal-aligned and frequently encountered) provide enhanced training reliability, produce more effective cyber defence agents with lower-risk policies, better align with defender goals, and reduce costly actions without explicit penalties.", "conclusion": "Sparse reward functions uniquely offer superior performance for autonomous cyber defence agents compared to dense rewards, delivering more reliable training and safer, more effective defensive policies."}}
{"id": "2602.04346", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04346", "abs": "https://arxiv.org/abs/2602.04346", "authors": ["Weikang Meng", "Liangyu Huo", "Yadan Luo", "Yaowei Wang", "Yingjian Li", "Zheng Zhang"], "title": "MirrorLA: Reflecting Feature Map for Vision Linear Attention", "comment": null, "summary": "Linear attention significantly reduces the computational complexity of Transformers from quadratic to linear, yet it consistently lags behind softmax-based attention in performance. We identify the root cause of this degradation as the non-negativity constraint imposed on kernel feature maps: standard projections like ReLU act as \"passive truncation\" operators, indiscriminately discarding semantic information residing in the negative domain. We propose MirrorLA, a geometric framework that substitutes passive truncation with active reorientation. By leveraging learnable Householder reflections, MirrorLA rotates the feature geometry into the non-negative orthant to maximize information retention. Our approach restores representational density through a cohesive, multi-scale design: it first optimizes local discriminability via block-wise isometries, stabilizes long-context dynamics using variance-aware modulation to diversify activations, and finally, integrates dispersed subspaces via cross-head reflections to induce global covariance mixing. MirrorLA achieves state-of-the-art performance across standard benchmarks, demonstrating that strictly linear efficiency can be achieved without compromising representational fidelity.", "AI": {"tldr": "MirrorLA\u901a\u8fc7\u53ef\u5b66\u4e60\u7684Householder\u53cd\u5c04\u4e3b\u52a8\u91cd\u5b9a\u5411\u7279\u5f81\uff0c\u89e3\u51b3\u7ebf\u6027\u6ce8\u610f\u529b\u975e\u8d1f\u7ea6\u675f\u5bfc\u81f4\u7684\u4fe1\u606f\u635f\u5931\uff0c\u5b9e\u73b0\u7ebf\u6027\u590d\u6742\u5ea6\u4e0b\u7684SOTA\u6027\u80fd\u3002", "motivation": "\u7ebf\u6027\u6ce8\u610f\u529b\u5c06Transformer\u590d\u6742\u5ea6\u4ece\u4e8c\u6b21\u964d\u81f3\u7ebf\u6027\uff0c\u4f46\u6027\u80fd\u6301\u7eed\u843d\u540e\u4e8esoftmax\u6ce8\u610f\u529b\u3002\u6839\u672c\u539f\u56e0\u662f\u6838\u7279\u5f81\u56fe\u7684\u975e\u8d1f\u6027\u7ea6\u675f\u901a\u8fc7\u6807\u51c6\u6295\u5f71\uff08\u5982ReLU\uff09\u9020\u6210\u8d1f\u57df\u8bed\u4e49\u4fe1\u606f\u7684\u4e0d\u53ef\u9006\u4e22\u5931\u3002", "method": "\u63d0\u51faMirrorLA\u51e0\u4f55\u6846\u67b6\uff0c\u7528\u53ef\u5b66\u4e60\u7684Householder\u53cd\u5c04\u66ff\u4ee3\u88ab\u52a8\u622a\u65ad\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u8bbe\u8ba1\uff1a1) \u5757\u7ea7\u7b49\u8ddd\u53d8\u6362\u4f18\u5316\u5c40\u90e8\u5224\u522b\u6027\uff1b2) \u65b9\u5dee\u611f\u77e5\u8c03\u5236\u7a33\u5b9a\u957f\u4e0a\u4e0b\u6587\u52a8\u6001\u5e76\u591a\u6837\u5316\u6fc0\u6d3b\uff1b3) \u8de8\u5934\u53cd\u5c04\u8bf1\u5bfc\u5168\u5c40\u534f\u65b9\u5dee\u6df7\u5408\uff0c\u4ece\u800c\u6700\u5927\u5316\u4fe1\u606f\u4fdd\u7559\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8bc1\u660e\u4e25\u683c\u7ebf\u6027\u6548\u7387\u53ef\u4e0e\u8868\u5f81\u4fdd\u771f\u5ea6\u517c\u5f97\u3002", "conclusion": "\u4e3b\u52a8\u91cd\u5b9a\u5411\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u7ebf\u6027\u6ce8\u610f\u529b\u7684\u6027\u80fd\u74f6\u9888\uff0c\u4e3a\u8bbe\u8ba1\u9ad8\u6548Transformer\u63d0\u4f9b\u4e86\u65b0\u51e0\u4f55\u89c6\u89d2\u3002"}}
{"id": "2602.04861", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2602.04861", "abs": "https://arxiv.org/abs/2602.04861", "authors": ["Ryan Liu", "Eric Qu", "Tobias Kreiman", "Samuel M. Blau", "Aditi S. Krishnapriyan"], "title": "From Evaluation to Design: Using Potential Energy Surface Smoothness Metrics to Guide Machine Learning Interatomic Potential Architectures", "comment": "13 pages main text, 10 pages reference & appendix, 8 figures", "summary": "Machine Learning Interatomic Potentials (MLIPs) sometimes fail to reproduce the physical smoothness of the quantum potential energy surface (PES), leading to erroneous behavior in downstream simulations that standard energy and force regression evaluations can miss. Existing evaluations, such as microcanonical molecular dynamics (MD), are computationally expensive and primarily probe near-equilibrium states. To improve evaluation metrics for MLIPs, we introduce the Bond Smoothness Characterization Test (BSCT). This efficient benchmark probes the PES via controlled bond deformations and detects non-smoothness, including discontinuities, artificial minima, and spurious forces, both near and far from equilibrium. We show that BSCT correlates strongly with MD stability while requiring a fraction of the cost of MD. To demonstrate how BSCT can guide iterative model design, we utilize an unconstrained Transformer backbone as a testbed, illustrating how refinements such as a new differentiable $k$-nearest neighbors algorithm and temperature-controlled attention reduce artifacts identified by our metric. By optimizing model design systematically based on BSCT, the resulting MLIP simultaneously achieves a low conventional E/F regression error, stable MD simulations, and robust atomistic property predictions. Our results establish BSCT as both a validation metric and as an \"in-the-loop\" model design proxy that alerts MLIP developers to physical challenges that cannot be efficiently evaluated by current MLIP benchmarks.", "AI": {"tldr": "This paper introduces the Bond Smoothness Characterization Test (BSCT), an efficient benchmark that evaluates Machine Learning Interatomic Potentials (MLIPs) by probing potential energy surface smoothness through controlled bond deformations, detecting physical artifacts that standard regression metrics miss, while correlating strongly with molecular dynamics stability at a fraction of the computational cost.", "motivation": "MLIPs sometimes fail to reproduce the physical smoothness of quantum potential energy surfaces, leading to erroneous simulation behavior that standard energy/force regression evaluations cannot detect. Existing evaluation methods like microcanonical molecular dynamics are computationally expensive and only probe near-equilibrium states, creating a need for more efficient and comprehensive validation metrics.", "method": "The authors developed the Bond Smoothness Characterization Test (BSCT) which systematically probes the potential energy surface through controlled bond deformations to identify discontinuities, artificial minima, and spurious forces both near and far from equilibrium. They tested this using an unconstrained Transformer backbone with refinements including a new differentiable k-nearest neighbors algorithm and temperature-controlled attention mechanism.", "result": "BSCT demonstrates strong correlation with molecular dynamics stability while requiring only a small fraction of MD's computational cost. The refined Transformer model optimized using BSCT achieved simultaneously low conventional energy/force regression errors, stable molecular dynamics simulations, and robust predictions of atomistic properties.", "conclusion": "BSCT serves as both an effective validation metric and an \"in-the-loop\" model design proxy that alerts MLIP developers to physical challenges undetectable by current benchmarks, enabling systematic optimization of model architectures for improved physical fidelity and simulation stability."}}
{"id": "2602.04365", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04365", "abs": "https://arxiv.org/abs/2602.04365", "authors": ["Jiapeng Liu", "Liang Li", "Bing Li", "Peng Fu", "Xiyan Gao", "Chengyang Fang", "Xiaoshuai Hao", "Can Ma"], "title": "EXaMCaP: Subset Selection with Entropy Gain Maximization for Probing Capability Gains of Large Chart Understanding Training Sets", "comment": null, "summary": "Recent works focus on synthesizing Chart Understanding (ChartU) training sets to inject advanced chart knowledge into Multimodal Large Language Models (MLLMs), where the sufficiency of the knowledge is typically verified by quantifying capability gains via the fine-tune-then-evaluate paradigm. However, full-set fine-tuning MLLMs to assess such gains incurs significant time costs, hindering the iterative refinement cycles of the ChartU dataset. Reviewing the ChartU dataset synthesis and data selection domains, we find that subsets can potentially probe the MLLMs' capability gains from full-set fine-tuning. Given that data diversity is vital for boosting MLLMs' performance and entropy reflects this feature, we propose EXaMCaP, which uses entropy gain maximization to select a subset. To obtain a high-diversity subset, EXaMCaP chooses the maximum-entropy subset from the large ChartU dataset. As enumerating all possible subsets is impractical, EXaMCaP iteratively selects samples to maximize the gain in set entropy relative to the current set, approximating the maximum-entropy subset of the full dataset. Experiments show that EXaMCaP outperforms baselines in probing the capability gains of the ChartU training set, along with its strong effectiveness across diverse subset sizes and compatibility with various MLLM architectures.", "AI": {"tldr": "The paper proposes EXaMCaP, an entropy-based subset selection method that efficiently probes MLLM capability gains from ChartU training sets, avoiding costly full-set fine-tuning while maintaining effectiveness across different model architectures and subset sizes.", "motivation": "Full-set fine-tuning of MLLMs to verify ChartU dataset quality is computationally expensive and hinders iterative dataset refinement. The paper seeks a more efficient way to assess dataset effectiveness using subsets that can reliably predict full-set performance gains.", "method": "EXaMCaP uses entropy gain maximization to select diverse subsets from ChartU datasets. It iteratively chooses samples that maximize entropy gain relative to the current subset, approximating the maximum-entropy subset without exhaustive enumeration.", "result": "EXaMCaP outperforms baseline methods in probing MLLM capability gains, demonstrates strong effectiveness across various subset sizes, and shows compatibility with different MLLM architectures.", "conclusion": "The entropy-based subset selection approach provides an efficient and effective way to evaluate ChartU dataset quality, enabling faster iterative refinement cycles while maintaining reliable assessment of MLLM performance improvements."}}
{"id": "2602.04863", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.04863", "abs": "https://arxiv.org/abs/2602.04863", "authors": ["Ishaq Aden-Ali", "Noah Golowich", "Allen Liu", "Abhishek Shetty", "Ankur Moitra", "Nika Haghtalab"], "title": "Subliminal Effects in Your Data: A General Mechanism via Log-Linearity", "comment": "Code available at https://github.com/ishaqadenali/logit-linear-selection", "summary": "Training modern large language models (LLMs) has become a veritable smorgasbord of algorithms and datasets designed to elicit particular behaviors, making it critical to develop techniques to understand the effects of datasets on the model's properties. This is exacerbated by recent experiments that show datasets can transmit signals that are not directly observable from individual datapoints, posing a conceptual challenge for dataset-centric understandings of LLM training and suggesting a missing fundamental account of such phenomena. Towards understanding such effects, inspired by recent work on the linear structure of LLMs, we uncover a general mechanism through which hidden subtexts can arise in generic datasets.\n  We introduce Logit-Linear-Selection (LLS), a method that prescribes how to select subsets of a generic preference dataset to elicit a wide range of hidden effects. We apply LLS to discover subsets of real-world datasets so that models trained on them exhibit behaviors ranging from having specific preferences, to responding to prompts in a different language not present in the dataset, to taking on a different persona. Crucially, the effect persists for the selected subset, across models with varying architectures, supporting its generality and universality.", "AI": {"tldr": "A method called Logit-Linear-Selection (LLS) can select subsets of generic datasets to make LLMs exhibit hidden behaviors like specific preferences, responding in unseen languages, or adopting different personas, with effects generalizing across model architectures.", "motivation": "Understanding how datasets affect LLM properties is critical, especially since datasets can transmit hidden signals not observable from individual datapoints, challenging current dataset-centric understandings of LLM training.", "method": "Logit-Linear-Selection (LLS), inspired by the linear structure of LLMs, which prescribes how to select subsets of generic preference datasets to elicit hidden effects.", "result": "LLS discovers subsets of real-world datasets that cause models to exhibit specific behaviors (preferences, language switching, persona adoption), and this effect persists across different model architectures.", "conclusion": "There is a general mechanism through which hidden subtexts can arise in generic datasets, and LLS provides a systematic way to uncover and elicit these hidden effects, demonstrating generality and universality across model architectures."}}
{"id": "2602.04369", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04369", "abs": "https://arxiv.org/abs/2602.04369", "authors": ["Zongjiang Shang", "Dongliang Cui", "Binqing Wu", "Ling Chen"], "title": "Multi-scale hypergraph meets LLMs: Aligning large language models for time series analysis", "comment": "Accepted by ICLR2026", "summary": "Recently, there has been great success in leveraging pre-trained large language models (LLMs) for time series analysis. The core idea lies in effectively aligning the modality between natural language and time series. However, the multi-scale structures of natural language and time series have not been fully considered, resulting in insufficient utilization of LLMs capabilities. To this end, we propose MSH-LLM, a Multi-Scale Hypergraph method that aligns Large Language Models for time series analysis. Specifically, a hyperedging mechanism is designed to enhance the multi-scale semantic information of time series semantic space. Then, a cross-modality alignment (CMA) module is introduced to align the modality between natural language and time series at different scales. In addition, a mixture of prompts (MoP) mechanism is introduced to provide contextual information and enhance the ability of LLMs to understand the multi-scale temporal patterns of time series. Experimental results on 27 real-world datasets across 5 different applications demonstrate that MSH-LLM achieves the state-of-the-art results.", "AI": {"tldr": "MSH-LLM\u901a\u8fc7\u8d85\u56fe\u673a\u5236\u3001\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u6df7\u5408\u63d0\u793a\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u591a\u5c3a\u5ea6\u65f6\u95f4\u5e8f\u5217\u5206\u6790\uff0c\u572827\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97SOTA\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u867d\u6210\u529f\u5c06\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u5206\u6790\uff0c\u4f46\u672a\u80fd\u5145\u5206\u8003\u8651\u81ea\u7136\u8bed\u8a00\u4e0e\u65f6\u95f4\u5e8f\u5217\u7684\u591a\u5c3a\u5ea6\u7ed3\u6784\uff0c\u5bfc\u81f4\u5927\u6a21\u578b\u80fd\u529b\u5229\u7528\u4e0d\u8db3\u3002", "method": "\u63d0\u51faMSH-LLM\u65b9\u6cd5\uff0c\u5305\u542b\uff1a1)\u8d85\u56fe\u673a\u5236\u589e\u5f3a\u65f6\u95f4\u5e8f\u5217\u591a\u5c3a\u5ea6\u8bed\u4e49\u4fe1\u606f\uff1b2)\u8de8\u6a21\u6001\u5bf9\u9f50\u6a21\u5757\u5b9e\u73b0\u4e0d\u540c\u5c3a\u5ea6\u4e0b\u7684\u6a21\u6001\u5bf9\u9f50\uff1b3)\u6df7\u5408\u63d0\u793a\u673a\u5236\u63d0\u4f9b\u4e0a\u4e0b\u6587\u4fe1\u606f\u5e76\u589e\u5f3a\u591a\u5c3a\u5ea6\u65f6\u5e8f\u6a21\u5f0f\u7406\u89e3\u3002", "result": "\u57285\u4e2a\u4e0d\u540c\u9886\u57df\u768427\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5747\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8bed\u8a00\u4e0e\u65f6\u95f4\u5e8f\u5217\u95f4\u7684\u591a\u5c3a\u5ea6\u5bf9\u9f50\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002"}}
{"id": "2602.04868", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04868", "abs": "https://arxiv.org/abs/2602.04868", "authors": ["Yannick Denker", "Alexander Gepperth"], "title": "CRoSS: A Continual Robotic Simulation Suite for Scalable Reinforcement Learning with High Task Diversity and Realistic Physics Simulation", "comment": null, "summary": "Continual reinforcement learning (CRL) requires agents to learn from a sequence of tasks without forgetting previously acquired policies. In this work, we introduce a novel benchmark suite for CRL based on realistically simulated robots in the Gazebo simulator. Our Continual Robotic Simulation Suite (CRoSS) benchmarks rely on two robotic platforms: a two-wheeled differential-drive robot with lidar, camera and bumper sensor, and a robotic arm with seven joints. The former represent an agent in line-following and object-pushing scenarios, where variation of visual and structural parameters yields a large number of distinct tasks, whereas the latter is used in two goal-reaching scenarios with high-level cartesian hand position control (modeled after the Continual World benchmark), and low-level control based on joint angles. For the robotic arm benchmarks, we provide additional kinematics-only variants that bypass the need for physical simulation (as long as no sensor readings are required), and which can be run two orders of magnitude faster. CRoSS is designed to be easily extensible and enables controlled studies of continual reinforcement learning in robotic settings with high physical realism, and in particular allow the use of almost arbitrary simulated sensors. To ensure reproducibility and ease of use, we provide a containerized setup (Apptainer) that runs out-of-the-box, and report performances of standard RL algorithms, including Deep Q-Networks (DQN) and policy gradient methods. This highlights the suitability as a scalable and reproducible benchmark for CRL research.", "AI": {"tldr": "\u9488\u5bf9\u8fde\u7eed\u5f3a\u5316\u5b66\u4e60\u7f3a\u4e4f\u771f\u5b9e\u673a\u5668\u4eba\u57fa\u51c6\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faCRoSS\u2014\u2014\u4e00\u4e2a\u57fa\u4e8eGazebo\u7684\u9ad8\u4fdd\u771f\u673a\u5668\u4eba\u6a21\u62df\u57fa\u51c6\u5957\u4ef6\uff0c\u5305\u542b\u8f6e\u5f0f\u79fb\u52a8\u673a\u5668\u4eba\u548c\u4e03\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u4e24\u5927\u5e73\u53f0\uff0c\u652f\u6301\u4f20\u611f\u5668\u4efb\u610f\u914d\u7f6e\u548c\u5feb\u901f kinematics-only \u6a21\u5f0f\uff0c\u4e3a\u53ef\u91cd\u590d\u7684\u6301\u7eed\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u6807\u51c6\u5316\u8bc4\u4f30\u5de5\u5177\u3002", "motivation": "\u73b0\u6709\u8fde\u7eed\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u5728\u7269\u7406\u771f\u5b9e\u6027\u548c\u4f20\u611f\u5668\u7075\u6d3b\u6027\u65b9\u9762\u4e0d\u8db3\uff0c\u96be\u4ee5\u53cd\u6620\u771f\u5b9e\u673a\u5668\u4eba\u5b66\u4e60\u573a\u666f\u7684\u6311\u6218\uff0c\u7f3a\u4e4f\u53ef\u6269\u5c55\u4e14\u53ef\u91cd\u590d\u7684\u8bc4\u4f30\u5e73\u53f0\u3002", "method": "\u5728Gazebo\u6a21\u62df\u5668\u4e2d\u6784\u5efaCRoSS\u57fa\u51c6\uff0c\u5305\u542b\u914d\u5907\u6fc0\u5149\u96f7\u8fbe\u3001\u6444\u50cf\u5934\u7b49\u4f20\u611f\u5668\u7684\u5dee\u901f\u9a71\u52a8\u673a\u5668\u4eba\uff08\u7528\u4e8e\u5de1\u7ebf\u548c\u7269\u4f53\u63a8\u52a8\u4efb\u52a1\uff09\u548c\u4e03\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\uff08\u7528\u4e8e\u7b1b\u5361\u5c14\u7a7a\u95f4\u4e0e\u5173\u8282\u7a7a\u95f4\u7684\u5230\u8fbe\u4efb\u52a1\uff09\uff0c\u63d0\u4f9b\u7269\u7406\u4eff\u771f\u548c\u5feb\u901f kinematics-only \u4e24\u79cd\u8fd0\u884c\u6a21\u5f0f\uff0c\u91c7\u7528\u5bb9\u5668\u5316\u6280\u672f\u786e\u4fdd\u53ef\u91cd\u590d\u6027\uff0c\u5e76\u5728\u8be5\u57fa\u51c6\u4e0a\u6d4b\u8bd5\u4e86DQN\u548c\u7b56\u7565\u68af\u5ea6\u7b49\u6807\u51c6RL\u7b97\u6cd5\u6027\u80fd\u3002", "result": "CRoSS\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u7269\u7406\u771f\u5b9e\u6027\u7684\u673a\u5668\u4eba\u6301\u7eed\u5b66\u4e60\u8bc4\u4f30\uff0ckinematics-only\u6a21\u5f0f\u8fd0\u884c\u901f\u5ea6\u63d0\u5347\u4e24\u4e2a\u6570\u91cf\u7ea7\uff1b\u6807\u51c6RL\u7b97\u6cd5\u7684\u6027\u80fd\u9a8c\u8bc1\u4e86\u8be5\u57fa\u51c6\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "CRoSS\u4e3a\u673a\u5668\u4eba\u6301\u7eed\u5b66\u4e60\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u4e2a\u771f\u5b9e\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u91cd\u590d\u7684\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u5c06\u63a8\u52a8\u8be5\u9886\u57df\u5728\u53d7\u63a7\u73af\u5883\u4e0b\u7684\u6df1\u5165\u7814\u7a76\uff0c\u5e76\u4fc3\u8fdb\u7b97\u6cd5\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2602.04373", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04373", "abs": "https://arxiv.org/abs/2602.04373", "authors": ["Geethen Singh", "Jasper A Slingsby", "Tamara B Robinson", "Glenn Moncrieff"], "title": "Reducing the labeling burden in time-series mapping using Common Ground: a semi-automated approach to tracking changes in land cover and species over time", "comment": null, "summary": "Reliable classification of Earth Observation data depends on consistent, up-to-date reference labels. However, collecting new labelled data at each time step remains expensive and logistically difficult, especially in dynamic or remote ecological systems. As a response to this challenge, we demonstrate that a model with access to reference data solely from time step t0 can perform competitively on both t0 and a future time step t1, outperforming models trained separately on time-specific reference data (the gold standard). This finding suggests that effective temporal generalization can be achieved without requiring manual updates to reference labels beyond the initial time step t0. Drawing on concepts from change detection and semi-supervised learning (SSL), the most performant approach, \"Common Ground\", uses a semi-supervised framework that leverages temporally stable regions-areas with little to no change in spectral or semantic characteristics between time steps-as a source of implicit supervision for dynamic regions. We evaluate this strategy across multiple classifiers, sensors (Landsat-8, Sentinel-2 satellite multispectral and airborne imaging spectroscopy), and ecological use cases. For invasive tree species mapping, we observed a 21-40% improvement in classification accuracy using Common Ground compared to naive temporal transfer, where models trained at a single time step are directly applied to a future time step. We also observe a 10 -16% higher accuracy for the introduced approach compared to a gold-standard approach. In contrast, when broad land cover categories were mapped across Europe, we observed a more modest 2% increase in accuracy compared to both the naive and gold-standard approaches. These results underscore the effectiveness of combining stable reference screening with SSL for scalable and label-efficient multi-temporal remote sensing classification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\"Common Ground\"\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u65f6\u5e8f\u7a33\u5b9a\u533a\u57df\u63d0\u4f9b\u9690\u5f0f\u76d1\u7763\uff0c\u5b9e\u73b0\u5730\u7403\u89c2\u6d4b\u6570\u636e\u5206\u7c7b\u7684\u65f6\u5e8f\u6cdb\u5316\uff0c\u65e0\u9700\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u624b\u52a8\u66f4\u65b0\u6807\u6ce8\u3002", "motivation": "\u5728\u52a8\u6001\u6216\u504f\u8fdc\u751f\u6001\u7cfb\u7edf\u4e2d\uff0c\u6bcf\u4e2a\u65f6\u95f4\u6b65\u6536\u96c6\u65b0\u6807\u6ce8\u6570\u636e\u6210\u672c\u9ad8\u6602\u4e14\u5b9e\u65bd\u56f0\u96be\uff0c\u4e9f\u9700\u80fd\u591f\u8de8\u65f6\u95f4\u6cdb\u5316\u800c\u65e0\u9700\u9891\u7e41\u624b\u52a8\u66f4\u65b0\u6807\u7b7e\u7684\u65b9\u6cd5\u3002", "method": "\u501f\u9274\u53d8\u5316\u68c0\u6d4b\u4e0e\u534a\u76d1\u7763\u5b66\u4e60\uff0c\"Common Ground\"\u6846\u67b6\u5229\u7528\u65f6\u5e8f\u7a33\u5b9a\u533a\u57df\uff08\u5149\u8c31/\u8bed\u4e49\u7279\u5f81\u53d8\u5316\u6781\u5c0f\u7684\u533a\u57df\uff09\u4f5c\u4e3a\u52a8\u6001\u533a\u57df\u7684\u9690\u5f0f\u76d1\u7763\u6e90\u3002", "result": "\u5728\u5165\u4fb5\u6811\u79cd\u6d4b\u7ed8\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6bd4\u6734\u7d20\u65f6\u5e8f\u8fc1\u79fb\u63d0\u534721-40%\u51c6\u786e\u7387\uff0c\u6bd4\u91d1\u6807\u51c6\u65b9\u6cd5\u9ad810-16%\uff1b\u5728\u6b27\u6d32\u571f\u5730\u8986\u76d6\u5206\u7c7b\u4e2d\uff0c\u4ec5\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u53472%\u3002", "conclusion": "\u7ed3\u5408\u7a33\u5b9a\u533a\u57df\u7b5b\u9009\u4e0e\u534a\u76d1\u7763\u5b66\u4e60\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u3001\u6807\u7b7e\u9ad8\u6548\u7684\u591a\u65f6\u5e8f\u9065\u611f\u5206\u7c7b\uff0c\u6709\u6548\u907f\u514d\u4e86\u624b\u52a8\u66f4\u65b0\u53c2\u8003\u6807\u7b7e\u7684\u9700\u6c42\u3002"}}
{"id": "2602.04879", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.04879", "abs": "https://arxiv.org/abs/2602.04879", "authors": ["Penghui Qi", "Xiangxin Zhou", "Zichen Liu", "Tianyu Pang", "Chao Du", "Min Lin", "Wee Sun Lee"], "title": "Rethinking the Trust Region in LLM Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large vocabularies inherent to LLMs. PPO constrains policy updates based on the probability ratio of sampled tokens, which serves as a noisy single-sample Monte Carlo estimate of the true policy divergence. This creates a sub-optimal learning dynamic: updates to low-probability tokens are aggressively over-penalized, while potentially catastrophic shifts in high-probability tokens are under-constrained, leading to training inefficiency and instability. To address this, we propose Divergence Proximal Policy Optimization (DPPO), which substitutes heuristic clipping with a more principled constraint based on a direct estimate of policy divergence (e.g., Total Variation or KL). To avoid huge memory footprint, we introduce the efficient Binary and Top-K approximations to capture the essential divergence with negligible overhead. Extensive empirical evaluations demonstrate that DPPO achieves superior training stability and efficiency compared to existing methods, offering a more robust foundation for RL-based LLM fine-tuning.", "AI": {"tldr": "PPO\u7684\u6bd4\u7387\u88c1\u526a\u673a\u5236\u4e0d\u9002\u5408LLM\u5927\u8bcd\u6c47\u91cf\u7279\u6027\uff0c\u5bfc\u81f4\u4f4e\u6982\u7387token\u88ab\u8fc7\u5ea6\u60e9\u7f5a\u800c\u9ad8\u6982\u7387token\u7ea6\u675f\u4e0d\u8db3\u3002DPPO\u7528\u76f4\u63a5\u7b56\u7565\u6563\u5ea6\u4f30\u8ba1\u66ff\u4ee3\u542f\u53d1\u5f0f\u88c1\u526a\uff0c\u91c7\u7528\u4e8c\u5143\u548cTop-K\u8fd1\u4f3c\u964d\u4f4e\u5185\u5b58\u5f00\u9500\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002", "motivation": "PPO\u7684\u6838\u5fc3\u6bd4\u7387\u88c1\u526a\u673a\u5236\u5728LLM\u5927\u8bcd\u6c47\u573a\u666f\u4e0b\u5b58\u5728\u7ed3\u6784\u6027\u7f3a\u9677\uff1a\u57fa\u4e8e\u91c7\u6837token\u6982\u7387\u6bd4\u7684\u5355\u6837\u672c\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u566a\u58f0\u5927\uff0c\u5bfc\u81f4\u4f4e\u6982\u7387token\u66f4\u65b0\u88ab\u8fc7\u5ea6\u6291\u5236\uff0c\u800c\u9ad8\u6982\u7387token\u7684\u707e\u96be\u6027\u504f\u79fb\u7ea6\u675f\u4e0d\u8db3\uff0c\u5f15|\u53d1\u8bad\u7ec3\u4f4e\u6548\u548c\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51faDivergence Proximal Policy Optimization (DPPO)\uff0c\u7528Total Variation\u6216KL\u6563\u5ea6\u7b49\u76f4\u63a5\u7b56\u7565\u6563\u5ea6\u4f30\u8ba1\u66ff\u4ee3\u542f\u53d1\u5f0f\u88c1\u526a\uff1b\u5f15\u5165Binary\u548cTop-K\u9ad8\u6548\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u5728\u6355\u83b7\u5173\u952e\u6563\u5ea6\u4fe1\u606f\u7684\u540c\u65f6\u63a7\u5236\u5185\u5b58\u5f00\u9500\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cDPPO\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u66f4\u4f18\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6548\u7387\uff0c\u4e3a\u57fa\u4e8eRL\u7684LLM\u5fae\u8c03\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u57fa\u7ebf\u7b97\u6cd5\u3002", "conclusion": "DPPO\u901a\u8fc7 principled \u7684\u6563\u5ea6\u7ea6\u675f\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86PPO\u5728LLM\u5fae\u8c03\u4e2d\u7684\u56fa\u6709\u7f3a\u9677\uff0c\u662f\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u7684\u66f4\u53ef\u9760\u57fa\u7840\u6846\u67b6\u3002"}}
{"id": "2602.04881", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04881", "abs": "https://arxiv.org/abs/2602.04881", "authors": ["Ajesh Koyatan Chathoth"], "title": "Contrastive Continual Learning for Model Adaptability in Internet of Things", "comment": null, "summary": "Internet of Things (IoT) deployments operate in nonstationary, dynamic environments where factors such as sensor drift, evolving user behavior, and heterogeneous user privacy requirements can affect application utility. Continual learning (CL) addresses this by adapting models over time without catastrophic forgetting. Meanwhile, contrastive learning has emerged as a powerful representation-learning paradigm that improves robustness and sample efficiency in a self-supervised manner. This paper reviews the usage of \\emph{contrastive continual learning} (CCL) for IoT, connecting algorithmic design (replay, regularization, distillation, prompts) with IoT system realities (TinyML constraints, intermittent connectivity, privacy). We present a unifying problem formulation, derive common objectives that blend contrastive and distillation losses, propose an IoT-oriented reference architecture for on-device, edge, and cloud-based CCL, and provide guidance on evaluation protocols and metrics. Finally, we highlight open unique challenges with respect to the IoT domain, such as spanning tabular and streaming IoT data, concept drift, federated settings, and energy-aware training.", "AI": {"tldr": "\u7efc\u8ff0\u5bf9\u6bd4\u6301\u7eed\u5b66\u4e60\u5728\u7269\u8054\u7f51\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\u548c\u53c2\u8003\u67b6\u6784\uff0c\u8fde\u63a5\u7b97\u6cd5\u8bbe\u8ba1\u4e0e\u7269\u8054\u7f51\u7ea6\u675f\uff0c\u5e76\u6307\u51fa\u6982\u5ff5\u6f02\u79fb\u548c\u8282\u80fd\u8bad\u7ec3\u7b49\u6311\u6218\u3002", "motivation": "\u7269\u8054\u7f51\u90e8\u7f72\u5728\u975e\u5e73\u7a33\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u9762\u4e34\u4f20\u611f\u5668\u6f02\u79fb\u3001\u7528\u6237\u884c\u4e3a\u6f14\u53d8\u548c\u5f02\u6784\u9690\u79c1\u9700\u6c42\u7b49\u95ee\u9898\u3002\u6301\u7eed\u5b66\u4e60\u53ef\u9002\u5e94\u6a21\u578b\u800c\u4e0d\u53d1\u751f\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5bf9\u6bd4\u5b66\u4e60\u80fd\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u6837\u672c\u6548\u7387\uff0c\u4e8c\u8005\u7ed3\u5408\uff08CCL\uff09\u5bf9\u7269\u8054\u7f51\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u6587\u7efc\u8ff0CCL\u5728\u7269\u8054\u7f51\u4e2d\u7684\u5e94\u7528\uff0c\u8fde\u63a5\u7b97\u6cd5\u8bbe\u8ba1\uff08\u91cd\u653e\u3001\u6b63\u5219\u5316\u3001\u84b8\u998f\u3001\u63d0\u793a\uff09\u4e0e\u7cfb\u7edf\u7ea6\u675f\uff08TinyML\u3001\u95f4\u6b47\u8fde\u63a5\u3001\u9690\u79c1\uff09\u3002\u63d0\u51fa\u7edf\u4e00\u95ee\u9898\u8868\u8ff0\uff0c\u63a8\u5bfc\u878d\u5408\u5bf9\u6bd4\u4e0e\u84b8\u998f\u635f\u5931\u7684\u901a\u7528\u76ee\u6807\uff0c\u8bbe\u8ba1\u9762\u5411\u7269\u8054\u7f51\u7684\u7aef-\u8fb9-\u4e91\u53c2\u8003\u67b6\u6784\uff0c\u5e76\u63d0\u4f9b\u8bc4\u4f30\u534f\u8bae\u4e0e\u6307\u6807\u6307\u5bfc\u3002", "result": "\u63d0\u4f9b\u4e86\u8fde\u63a5CCL\u7b97\u6cd5\u4e0e\u7269\u8054\u7f51\u7ea6\u675f\u7684\u7efc\u5408\u7efc\u8ff0\uff0c\u63d0\u51fa\u7edf\u4e00\u95ee\u9898\u8868\u8ff0\u548c\u901a\u7528\u76ee\u6807\u51fd\u6570\uff0c\u8bbe\u8ba1\u4e86\u7269\u8054\u7f51\u5bfc\u5411\u7684\u53c2\u8003\u67b6\u6784\uff0c\u5e76\u5236\u5b9a\u4e86\u8bc4\u4f30\u6307\u5357\u3002", "conclusion": "\u5f3a\u8c03\u7269\u8054\u7f51\u9886\u57df\u72ec\u7279\u6311\u6218\uff1a\u8868\u683c\u4e0e\u6d41\u5f0f\u6570\u636e\u5904\u7406\u3001\u6982\u5ff5\u6f02\u79fb\u3001\u8054\u90a6\u5b66\u4e60\u8bbe\u7f6e\u53ca\u8282\u80fd\u8bad\u7ec3\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u65b9\u5411\u3002"}}
{"id": "2602.04883", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2602.04883", "abs": "https://arxiv.org/abs/2602.04883", "authors": ["Yanru Qu", "Cheng-Yen Hsieh", "Zaixiang Zheng", "Ge Liu", "Quanquan Gu"], "title": "Protein Autoregressive Modeling via Multiscale Structure Generation", "comment": "ByteDance Seed Tech Report; Page: https://par-protein.github.io/", "summary": "We present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for protein backbone generation via coarse-to-fine next-scale prediction. Using the hierarchical nature of proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and refining structural details over scales. To achieve this, PAR consists of three key components: (i) multi-scale downsampling operations that represent protein structures across multiple scales during training; (ii) an autoregressive transformer that encodes multi-scale information and produces conditional embeddings to guide structure generation; (iii) a flow-based backbone decoder that generates backbone atoms conditioned on these embeddings. Moreover, autoregressive models suffer from exposure bias, caused by the training and the generation procedure mismatch, and substantially degrades structure generation quality. We effectively alleviate this issue by adopting noisy context learning and scheduled sampling, enabling robust backbone generation. Notably, PAR exhibits strong zero-shot generalization, supporting flexible human-prompted conditional generation and motif scaffolding without requiring fine-tuning. On the unconditional generation benchmark, PAR effectively learns protein distributions and produces backbones of high design quality, and exhibits favorable scaling behavior. Together, these properties establish PAR as a promising framework for protein structure generation.", "AI": {"tldr": "\u63d0\u51fa\u86cb\u767d\u8d28\u81ea\u56de\u5f52\u5efa\u6a21(PAR)\uff0c\u9996\u4e2a\u591a\u5c3a\u5ea6\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u901a\u8fc7\u7c97\u5230\u7ec6\u7684\u5c3a\u5ea6\u9884\u6d4b\u751f\u6210\u86cb\u767d\u8d28\u9aa8\u67b6\uff0c\u91c7\u7528\u591a\u5c3a\u5ea6\u4e0b\u91c7\u6837\u3001\u81ea\u56de\u5f52\u53d8\u6362\u5668\u548c\u6d41\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u566a\u58f0\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u8ba1\u5212\u91c7\u6837\u7f13\u89e3\u66b4\u9732\u504f\u5dee\uff0c\u5c55\u73b0\u5f3a\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u548c\u9ad8\u8d28\u91cf\u8bbe\u8ba1", "motivation": "\u5229\u7528\u86cb\u767d\u8d28\u5c42\u6b21\u5316\u7279\u6027\uff0c\u6a21\u62df\u96d5\u5851\u8fc7\u7a0b\u5b9e\u73b0\u4ece\u7c97\u62d3\u6251\u5230\u7ec6\u7ec6\u8282\u7684\u86cb\u767d\u8d28\u9aa8\u67b6\u751f\u6210\uff0c\u540c\u65f6\u89e3\u51b3\u81ea\u56de\u5f52\u6a21\u578b\u4e2d\u8bad\u7ec3\u4e0e\u751f\u6210\u4e0d\u5339\u914d\u5bfc\u81f4\u7684\u66b4\u9732\u504f\u5dee\u95ee\u9898", "method": "PAR\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1)\u591a\u5c3a\u5ea6\u4e0b\u91c7\u6837\u64cd\u4f5c\uff0c\u8bad\u7ec3\u4e2d\u591a\u5c3a\u5ea6\u8868\u793a\u86cb\u767d\u8d28\u7ed3\u6784\uff1b(2)\u81ea\u56de\u5f52\u53d8\u6362\u5668\uff0c\u7f16\u7801\u591a\u5c3a\u5ea6\u4fe1\u606f\u5e76\u4ea7\u751f\u6761\u4ef6\u5d4c\u5165\uff1b(3)\u57fa\u4e8e\u6d41\u7684\u9aa8\u67b6\u89e3\u7801\u5668\uff0c\u6839\u636e\u5d4c\u5165\u751f\u6210\u9aa8\u67b6\u539f\u5b50\u3002\u91c7\u7528\u566a\u58f0\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u8ba1\u5212\u91c7\u6837\u7f13\u89e3\u66b4\u9732\u504f\u5dee", "result": "PAR\u5c55\u73b0\u5f3a\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u652f\u6301\u65e0\u9700\u5fae\u8c03\u7684\u4eba\u5de5\u63d0\u793a\u6761\u4ef6\u751f\u6210\u548c\u57fa\u5e8f\u652f\u67b6\uff1b\u5728\u65e0\u6761\u4ef6\u4e0b\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6709\u6548\u5b66\u4e60\u86cb\u767d\u8d28\u5206\u5e03\uff0c\u4ea7\u751f\u9ad8\u8bbe\u8ba1\u8d28\u91cf\u9aa8\u67b6\uff0c\u5e76\u5448\u73b0\u826f\u597d\u7f29\u653e\u884c\u4e3a", "conclusion": "PAR\u4f5c\u4e3a\u86cb\u767d\u8d28\u7ed3\u6784\u751f\u6210\u7684\u524d\u666f\u6846\u67b6\uff0c\u5177\u5907\u9ad8\u8d28\u91cf\u751f\u6210\u3001\u5f3a\u6cdb\u5316\u80fd\u529b\u548c\u7075\u6d3b\u6027\uff0c\u4e3a\u86cb\u767d\u8d28\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2602.04408", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.04408", "abs": "https://arxiv.org/abs/2602.04408", "authors": ["Shizhou Xu"], "title": "Separation-Utility Pareto Frontier: An Information-Theoretic Characterization", "comment": null, "summary": "We study the Pareto frontier (optimal trade-off) between utility and separation, a fairness criterion requiring predictive independence from sensitive attributes conditional on the true outcome. Through an information-theoretic lens, we prove a characterization of the utility-separation Pareto frontier, establish its concavity, and thereby prove the increasing marginal cost of separation in terms of utility. In addition, we characterize the conditions under which this trade-off becomes strict, providing a guide for trade-off selection in practice. Based on the theoretical characterization, we develop an empirical regularizer based on conditional mutual information (CMI) between predictions and sensitive attributes given the true outcome. The CMI regularizer is compatible with any deep model trained via gradient-based optimization and serves as a scalar monitor of residual separation violations, offering tractable guarantees during training. Finally, numerical experiments support our theoretical findings: across COMPAS, UCI Adult, UCI Bank, and CelebA, the proposed method substantially reduces separation violations while matching or exceeding the utility of established baseline methods. This study thus offers a provable, stable, and flexible approach to enforcing separation in deep learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u4fe1\u606f\u8bba\u65b9\u6cd5\u4ece\u7406\u8bba\u4e0a\u523b\u753b\u4e86\u516c\u5e73\u673a\u5668\u5b66\u4e60\u4e2d\u6548\u7528\u4e0e\u5206\u79bb\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u8bc1\u660e\u4e86\u5176\u5e15\u7d2f\u6258\u524d\u6cbf\u7684\u51f9\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u6761\u4ef6\u4e92\u4fe1\u606f\u6b63\u5219\u5316\u5668\uff0c\u80fd\u5728\u4fdd\u6301\u6a21\u578b\u6548\u7528\u7684\u540c\u65f6\u51cf\u5c11\u516c\u5e73\u6027\u8fdd\u89c4\u3002", "motivation": "\u523b\u753b\u6548\u7528\u4e0e\u5206\u79bb\u6027\u4e4b\u95f4\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u5176\u4e2d\u5206\u79bb\u6027\u662f\u4e00\u79cd\u8981\u6c42\u9884\u6d4b\u7ed3\u679c\u5728\u771f\u5b9e\u7ed3\u679c\u6761\u4ef6\u4e0b\u72ec\u7acb\u4e8e\u654f\u611f\u5c5e\u6027\u7684\u516c\u5e73\u6027\u51c6\u5219\u3002", "method": "\u91c7\u7528\u4fe1\u606f\u8bba\u6846\u67b6\u6765\u523b\u753b\u5e15\u7d2f\u6258\u524d\u6cbf\u53ca\u5176\u51f9\u6027\uff1b\u5f00\u53d1\u4e00\u79cd\u4e0e\u57fa\u4e8e\u68af\u5ea6\u7684\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\u517c\u5bb9\u7684\u6761\u4ef6\u4e92\u4fe1\u606f\uff08CMI\uff09\u6b63\u5219\u5316\u5668\u3002", "result": "\u8bc1\u660e\u4e86\u6548\u7528-\u5206\u79bb\u6027\u524d\u6cbf\u5177\u6709\u51f9\u6027\uff0c\u4e14\u5206\u79bb\u6027\u7684\u8fb9\u9645\u6210\u672c\u9012\u589e\uff1b\u786e\u5b9a\u4e86\u4e25\u683c\u6743\u8861\u7684\u6761\u4ef6\uff1b\u5b9e\u8bc1\u8868\u660eCMI\u6b63\u5219\u5316\u5668\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u51cf\u5c11\u5206\u79bb\u6027\u8fdd\u89c4\uff0c\u540c\u65f6\u5339\u914d\u6216\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\u7684\u6548\u7528\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u8bc1\u660e\u7684\u3001\u7a33\u5b9a\u7684\u4e14\u7075\u6d3b\u7684\u65b9\u6cd5\u6765\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u5b9e\u65bd\u5206\u79bb\u6027\uff0c\u5c06\u7406\u8bba\u89c1\u89e3\u4e0e\u5b9e\u8df5\u5e94\u7528\u76f8\u7ed3\u5408\u3002"}}
{"id": "2602.04431", "categories": ["cs.LG", "cs.GT"], "pdf": "https://arxiv.org/pdf/2602.04431", "abs": "https://arxiv.org/abs/2602.04431", "authors": ["Jonathan N\u00f6ther", "Adish Singla", "Goran Radanovic"], "title": "MaMa: A Game-Theoretic Approach for Designing Safe Agentic Systems", "comment": null, "summary": "LLM-based multi-agent systems have demonstrated impressive capabilities, but they also introduce significant safety risks when individual agents fail or behave adversarially. In this work, we study the automated design of agentic systems that remain safe even when a subset of agents is compromised. We formalize this challenge as a Stackelberg security game between a system designer (the Meta-Agent) and a best-responding Meta-Adversary that selects and compromises a subset of agents to minimize safety. We propose Meta-Adversary-Meta-Agent (MaMa), a novel algorithm for approximately solving this game and automatically designing safe agentic systems. Our approach uses LLM-based adversarial search, where the Meta-Agent iteratively proposes system designs and receives feedback based on the strongest attacks discovered by the Meta-Adversary. Empirical evaluations across diverse environments show that systems designed with MaMa consistently defend against worst-case attacks while maintaining performance comparable to systems optimized solely for task success. Moreover, the resulting systems generalize to stronger adversaries, as well as ones with different attack objectives or underlying LLMs, demonstrating robust safety beyond the training setting.", "AI": {"tldr": "\u63d0\u51faMaMa\u7b97\u6cd5\uff0c\u901a\u8fc7Stackelberg\u5b89\u5168\u535a\u5f08\u6846\u67b6\u81ea\u52a8\u8bbe\u8ba1\u80fd\u62b5\u5fa1\u90e8\u5206\u667a\u80fd\u4f53\u88ab\u5165\u4fb5\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5728\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027", "motivation": "\u73b0\u6709LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u90e8\u5206\u667a\u80fd\u4f53\u5931\u6548\u6216\u88ab\u6076\u610f\u64cd\u63a7\u65f6\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u9690\u60a3\uff0c\u9700\u81ea\u52a8\u5316\u8bbe\u8ba1\u5177\u5907\u9c81\u68d2\u6027\u7684\u5b89\u5168\u7cfb\u7edf", "method": "\u5c06\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u5143\u667a\u80fd\u4f53\uff08\u8bbe\u8ba1\u8005\uff09\u4e0e\u5143\u5bf9\u6297\u8005\uff08\u653b\u51fb\u8005\uff09\u7684Stackelberg\u535a\u5f08\uff0c\u63d0\u51faMaMa\u7b97\u6cd5\uff1a\u91c7\u7528LLM\u9a71\u52a8\u7684\u5bf9\u6297\u6027\u641c\u7d22\uff0c\u5143\u667a\u80fd\u4f53\u8fed\u4ee3\u751f\u6210\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u5143\u5bf9\u6297\u8005\u901a\u8fc7\u6700\u5f3a\u653b\u51fb\u63d0\u4f9b\u53cd\u9988", "result": "\u5b9e\u8bc1\u663e\u793aMaMa\u8bbe\u8ba1\u7684\u7cfb\u7edf\u5728\u62b5\u5fa1\u6700\u574f\u60c5\u51b5\u653b\u51fb\u65f6\u8868\u73b0\u7a33\u5b9a\uff0c\u4efb\u52a1\u6027\u80fd\u63a5\u8fd1\u4ec5\u4f18\u5316\u6210\u529f\u7387\u7684\u7cfb\u7edf\uff1b\u4e14\u80fd\u6cdb\u5316\u81f3\u66f4\u5f3a\u5bf9\u6297\u8005\u3001\u4e0d\u540c\u653b\u51fb\u76ee\u6807\u6216\u4e0d\u540cLLM\u6a21\u578b", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\u5b89\u5168\u8bbe\u8ba1\uff0c\u5728\u4fdd\u8bc1\u4efb\u52a1\u6548\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u9c81\u68d2\u6027\uff0c\u4e3a\u6784\u5efa\u53ef\u9760\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2602.04436", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04436", "abs": "https://arxiv.org/abs/2602.04436", "authors": ["Towa Sano", "Gouhei Tanaka"], "title": "Hand Gesture Recognition from Doppler Radar Signals Using Echo State Networks", "comment": "Submitted to IJCNN 2026. 21 pages, 10figures", "summary": "Hand gesture recognition (HGR) is a fundamental technology in human computer interaction (HCI).In particular, HGR based on Doppler radar signals is suited for in-vehicle interfaces and robotic systems, necessitating lightweight and computationally efficient recognition techniques. However, conventional deep learning-based methods still suffer from high computational costs. To address this issue, we propose an Echo State Network (ESN) approach for radar-based HGR, using frequency-modulated-continuous-wave (FMCW) radar signals. Raw radar data is first converted into feature maps, such as range-time and Doppler-time maps, which are then fed into one or more recurrent neural network-based reservoirs. The obtained reservoir states are processed by readout classifiers, including ridge regression, support vector machines, and random forests. Comparative experiments demonstrate that our method outperforms existing approaches on an 11-class HGR task using the Soli dataset and surpasses existing deep learning models on a 4-class HGR task using the Dop-NET dataset. The results indicate that parallel processing using multi-reservoir ESNs are effective for recognizing temporal patterns from the multiple different feature maps in the time-space and time-frequency domains. Our ESN approaches achieve high recognition performance with low computational cost in HGR, showing great potential for more advanced HCI technologies, especially in resource-constrained environments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u50a8\u5907\u6c60\u8ba1\u7b97(ESN)\u7684\u96f7\u8fbe\u624b\u52bf\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u50a8\u5c42\u5e76\u884c\u5904\u7406\u65f6\u9891\u7279\u5f81\u56fe\uff0c\u5728Soli\u548cDop-NET\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u9ad8\u6027\u80fd\u8bc6\u522b", "motivation": "\u8f66\u8f7d\u548c\u673a\u5668\u4eba\u573a\u666f\u9700\u8981\u8f7b\u91cf\u7ea7\u624b\u52bf\u8bc6\u522b\u6280\u672f\uff0c\u4f46\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u65e0\u6cd5\u6ee1\u8db3\u8d44\u6e90\u53d7\u9650\u73af\u5883\u7684\u9700\u6c42", "method": "\u4f7f\u7528FMCW\u96f7\u8fbe\u4fe1\u53f7\uff0c\u5c06\u539f\u59cb\u6570\u636e\u8f6c\u6362\u4e3a\u8ddd\u79bb-\u65f6\u95f4\u548c\u591a\u666e\u52d2-\u65f6\u95f4\u7279\u5f81\u56fe\uff0c\u8f93\u5165\u4e00\u4e2a\u6216\u591a\u4e2aRNN\u50a8\u5c42\uff0c\u901a\u8fc7\u5cad\u56de\u5f52\u3001SVM\u548c\u968f\u673a\u68ee\u6797\u7b49\u8bfb\u51fa\u5206\u7c7b\u5668\u8fdb\u884c\u8bc6\u522b", "result": "\u5728Soli\u6570\u636e\u96c611\u7c7b\u624b\u52bf\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728Dop-NET\u6570\u636e\u96c64\u7c7b\u4efb\u52a1\u4e0a\u8d85\u8d8a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff1b\u591a\u50a8\u5c42\u5e76\u884c\u5904\u7406\u80fd\u6709\u6548\u63d0\u53d6\u65f6\u57df\u548c\u65f6\u9891\u57df\u7279\u5f81\uff0c\u5b9e\u73b0\u9ad8\u51c6\u786e\u7387\u4f4e\u8ba1\u7b97\u6210\u672c", "conclusion": "ESN\u65b9\u6cd5\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5148\u8fdb\u4eba\u673a\u4ea4\u4e92\u6280\u672f\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u9614\u5e94\u7528\u524d\u666f"}}
{"id": "2602.04491", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04491", "abs": "https://arxiv.org/abs/2602.04491", "authors": ["Yuxi Guo", "Paul Sheridan"], "title": "Greedy-Gnorm: A Gradient Matrix Norm-Based Alternative to Attention Entropy for Head Pruning", "comment": "24 pages, 5 figures, 5 tables", "summary": "Attention head pruning has emerged as an effective technique for transformer model compression, an increasingly important goal in the era of Green AI. However, existing pruning methods often rely on static importance scores, which fail to capture the evolving role of attention heads during iterative removal. We propose Greedy-Gradient norm (Greedy-Gnorm), a novel head pruning algorithm that dynamically recalculates head importance after each pruning step. Specifically, each head is scored by the elementwise product of the l2-norms of its Q/K/V gradient blocks, as estimated from a hold-out validation set and updated at every greedy iteration. This dynamic approach to scoring mitigates against stale rankings and better reflects gradient-informed importance as pruning progresses. Extensive experiments on BERT, ALBERT, RoBERTa, and XLM-RoBERTa demonstrate that Greedy-Gnorm consistently preserves accuracy under substantial head removal, outperforming attention entropy. By effectively reducing model size while maintaining task performance, Greedy-Gnorm offers a promising step toward more energy-efficient transformer model deployment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faGreedy-Gnorm\uff0c\u4e00\u79cd\u52a8\u6001\u6ce8\u610f\u529b\u5934\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6bcf\u6b21\u526a\u679d\u540e\u91cd\u65b0\u8ba1\u7b97\u68af\u5ea6\u8303\u6570\u7684\u91cd\u8981\u6027\u5206\u6570\uff0c\u5728\u591a\u4e2aTransformer\u6a21\u578b\u4e0a\u76f8\u6bd4\u9759\u6001\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u4fdd\u6301\u7cbe\u5ea6\u3002", "motivation": "\u5728\u7eff\u8272AI\u65f6\u4ee3\uff0cTransformer\u6a21\u578b\u538b\u7f29\u5bf9\u964d\u4f4e\u80fd\u8017\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u4f7f\u7528\u9759\u6001\u91cd\u8981\u6027\u8bc4\u5206\uff0c\u65e0\u6cd5\u6355\u6349\u8fed\u4ee3\u79fb\u9664\u8fc7\u7a0b\u4e2d\u6ce8\u610f\u529b\u5934\u7684\u52a8\u6001\u53d8\u5316\u89d2\u8272\u3002", "method": "Greedy-Gradient norm (Greedy-Gnorm) \u901a\u8fc7\u8ba1\u7b97Q/K/V\u68af\u5ea6\u5757l2\u8303\u6570\u7684\u5143\u7d20\u4e58\u79ef\u6765\u52a8\u6001\u8bc4\u5206\u6ce8\u610f\u529b\u5934\uff0c\u4f7f\u7528\u4fdd\u7559\u9a8c\u8bc1\u96c6\u4f30\u8ba1\u5e76\u5728\u6bcf\u6b21\u8d2a\u5fc3\u8fed\u4ee3\u4e2d\u66f4\u65b0\uff0c\u907f\u514d\u8fc7\u65f6\u6392\u5e8f\u3002", "result": "\u5728BERT\u3001ALBERT\u3001RoBERTa\u548cXLM-RoBERTa\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cGreedy-Gnorm\u5728\u5927\u91cf\u79fb\u9664\u6ce8\u610f\u529b\u5934\u65f6\u80fd\u6301\u7eed\u4fdd\u6301\u7cbe\u5ea6\uff0c\u4f18\u4e8e\u6ce8\u610f\u529b\u71b5\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u52a8\u6001\u65b9\u6cd5\u901a\u8fc7\u6709\u6548\u51cf\u5c0f\u6a21\u578b\u5c3a\u5bf8\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\uff0c\u4e3a\u5b9e\u73b0\u66f4\u8282\u80fd\u7684Transformer\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u8def\u5f84\u3002"}}
{"id": "2602.04536", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04536", "abs": "https://arxiv.org/abs/2602.04536", "authors": ["Abdulrahman Alotaibi", "Irene Tenison", "Miriam Kim", "Isaac Lee", "Lalana Kagal"], "title": "Forget to Generalize: Iterative Adaptation for Generalization in Federated Learning", "comment": null, "summary": "The Web is naturally heterogeneous with user devices, geographic regions, browsing patterns, and contexts all leading to highly diverse, unique datasets. Federated Learning (FL) is an important paradigm for the Web because it enables privacy-preserving, collaborative machine learning across diverse user devices, web services and clients without needing to centralize sensitive data. However, its performance degrades severely under non-IID client distributions that is prevalent in real-world web systems. In this work, we propose a new training paradigm - Iterative Federated Adaptation (IFA) - that enhances generalization in heterogeneous federated settings through generation-wise forget and evolve strategy. Specifically, we divide training into multiple generations and, at the end of each, select a fraction of model parameters (a) randomly or (b) from the later layers of the model and reinitialize them. This iterative forget and evolve schedule allows the model to escape local minima and preserve globally relevant representations. Extensive experiments on CIFAR-10, MIT-Indoors, and Stanford Dogs datasets show that the proposed approach improves global accuracy, especially when the data cross clients are Non-IID. This method can be implemented on top any federated algorithm to improve its generalization performance. We observe an average of 21.5%improvement across datasets. This work advances the vision of scalable, privacy-preserving intelligence for real-world heterogeneous and distributed web systems.", "AI": {"tldr": "\u9488\u5bf9Web\u7cfb\u7edf\u4e2d\u6570\u636e\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-IID\uff09\u5bfc\u81f4\u7684\u8054\u90a6\u5b66\u4e60\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u8fed\u4ee3\u8054\u90a6\u81ea\u9002\u5e94\uff08IFA\uff09\u8303\u5f0f\uff0c\u901a\u8fc7\u9010\u4ee3\u9057\u5fd8\u548c\u6f14\u5316\u7b56\u7565\uff0c\u968f\u673a\u6216\u9009\u62e9\u6027\u5730\u91cd\u65b0\u521d\u59cb\u5316\u90e8\u5206\u6a21\u578b\u53c2\u6570\uff0c\u4ee5\u8df3\u51fa\u5c40\u90e8\u6700\u4f18\u5e76\u4fdd\u7559\u5168\u5c40\u76f8\u5173\u8868\u793a\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u5e73\u574721.5%\u7684\u7cbe\u5ea6\u63d0\u5347\u3002", "motivation": "Web\u73af\u5883\u5929\u7136\u5177\u6709\u5f02\u6784\u6027\uff0c\u7528\u6237\u8bbe\u5907\u3001\u5730\u7406\u533a\u57df\u3001\u6d4f\u89c8\u6a21\u5f0f\u548c\u4e0a\u4e0b\u6587\u5bfc\u81f4\u6570\u636e\u9ad8\u5ea6\u591a\u6837\u5316\u4e14\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-IID\uff09\u3002\u8054\u90a6\u5b66\u4e60\u867d\u80fd\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u534f\u4f5c\u5b66\u4e60\uff0c\u4f46\u5728\u771f\u5b9eWeb\u7cfb\u7edf\u4e2d\uff0cnon-IID\u6570\u636e\u5206\u5e03\u4f1a\u4e25\u91cd\u964d\u4f4e\u5176\u6027\u80fd\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e9f\u5f85\u89e3\u51b3\u7684\u5173\u952e\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u8fed\u4ee3\u8054\u90a6\u81ea\u9002\u5e94\uff08IFA\uff09\u8bad\u7ec3\u8303\u5f0f\uff0c\u91c7\u7528\u751f\u6210\u5f0f\u9057\u5fd8\u4e0e\u6f14\u5316\u7b56\u7565\u3002\u5c06\u8bad\u7ec3\u5206\u4e3a\u591a\u4e2a\u4e16\u4ee3\uff0c\u6bcf\u4ee3\u7ed3\u675f\u65f6\u9009\u62e9\u6027\u5730\u91cd\u65b0\u521d\u59cb\u5316\u90e8\u5206\u6a21\u578b\u53c2\u6570\uff08\u968f\u673a\u9009\u62e9\u6216\u4ece\u6a21\u578b\u8f83\u6df1\u5c42\u9009\u62e9\uff09\uff0c\u901a\u8fc7\u8fd9\u79cd\u8fed\u4ee3\u8c03\u5ea6\u4f7f\u6a21\u578b\u80fd\u591f\u9003\u79bb\u5c40\u90e8\u6781\u5c0f\u503c\u5e76\u4fdd\u7559\u5168\u5c40\u76f8\u5173\u7279\u5f81\u8868\u793a\u3002", "result": "\u5728CIFAR-10\u3001MIT-Indoors\u548cStanford Dogs\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5168\u5c40\u51c6\u786e\u7387\uff0c\u5c24\u5176\u5728\u5ba2\u6237\u7aef\u6570\u636e\u4e3a\u975e\u72ec\u7acb\u540c\u5206\u5e03\u65f6\u6548\u679c\u66f4\u4f73\u3002\u8be5\u65b9\u6cd5\u53ef\u4e0e\u4efb\u4f55\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\u7ed3\u5408\u4f7f\u7528\uff0c\u5e73\u5747\u5e26\u676521.5%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "IFA\u8303\u5f0f\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u4e3a\u6784\u5efa\u53ef\u6269\u5c55\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u771f\u5b9eWeb\u7cfb\u7edf\u667a\u80fd\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u63a8\u52a8\u4e86\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.04548", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.04548", "abs": "https://arxiv.org/abs/2602.04548", "authors": ["Dmitry Yarotsky", "Eugene Golikov", "Yaroslav Gusev"], "title": "Gradient Flow Through Diagram Expansions: Learning Regimes and Explicit Solutions", "comment": "48 pages, under review for ICML'2026", "summary": "We develop a general mathematical framework to analyze scaling regimes and derive explicit analytic solutions for gradient flow (GF) in large learning problems. Our key innovation is a formal power series expansion of the loss evolution, with coefficients encoded by diagrams akin to Feynman diagrams. We show that this expansion has a well-defined large-size limit that can be used to reveal different learning phases and, in some cases, to obtain explicit solutions of the nonlinear GF. We focus on learning Canonical Polyadic (CP) decompositions of high-order tensors, and show that this model has several distinct extreme lazy and rich GF regimes such as free evolution, NTK and under- and over-parameterized mean-field. We show that these regimes depend on the parameter scaling, tensor order, and symmetry of the model in a specific and subtle way. Moreover, we propose a general approach to summing the formal loss expansion by reducing it to a PDE; in a wide range of scenarios, it turns out to be 1st order and solvable by the method of characteristics. We observe a very good agreement of our theoretical predictions with experiment.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8d39\u66fc\u56fe\u5c55\u5f00\u7684\u6570\u5b66\u6846\u67b6\uff0c\u89e3\u6790\u5927\u6a21\u578b\u68af\u5ea6\u6d41\u7684\u6807\u5ea6\u5f8b\u4e0e\u5b66\u4e60\u9636\u6bb5\uff0c\u5e76\u4ee5\u9ad8\u9636\u5f20\u91cfCP\u5206\u89e3\u4e3a\u4f8b\u9a8c\u8bc1\u7406\u8bba\u9884\u6d4b\u4e0e\u5b9e\u9a8c\u9ad8\u5ea6\u543b\u5408", "motivation": "\u63ed\u793a\u5927\u89c4\u6a21\u5b66\u4e60\u95ee\u9898\u4e2d\u68af\u5ea6\u6d41\u7684\u4e0d\u540c\u5b66\u4e60\u9636\u6bb5\uff08\u5982NTK\u3001\u5747\u503c\u573a regime\uff09\u53ca\u5176\u6807\u5ea6\u5f8b\uff0c\u89e3\u51b3\u975e\u7ebf\u6027\u68af\u5ea6\u6d41\u89e3\u6790\u89e3\u7684\u96be\u9898", "method": "1. \u6784\u5efa\u635f\u5931\u6f14\u5316\u7684\u5f62\u5f0f\u5e42\u7ea7\u6570\u5c55\u5f00\uff08\u7cfb\u6570\u7531\u7c7b\u8d39\u66fc\u56fe\u7f16\u7801\uff09<br>2. \u901a\u8fc7\u5927\u5c3a\u5bf8\u6781\u9650\u5206\u6790\u5b66\u4e60\u9636\u6bb5<br>3. \u5c06\u7ea7\u6570\u6c42\u548c\u7ea6\u5316\u4e3a\u53ef\u7279\u5f81\u7ebf\u6cd5\u6c42\u89e3\u7684\u4e00\u9636\u504f\u5fae\u5206\u65b9\u7a0b", "result": "1. \u53d1\u73b0CP\u5f20\u91cf\u5206\u89e3\u5b58\u5728\u81ea\u7531\u6f14\u5316/NTK/\u6b20-\u8fc7\u53c2\u6570\u5316\u5747\u503c\u573a\u7b49\u591a\u91cd\u68af\u5ea6\u6d41\u9636\u6bb5<br>2. \u660e\u786e\u53c2\u6570\u6807\u5ea6\u3001\u5f20\u91cf\u9636\u6570\u4e0e\u6a21\u578b\u5bf9\u79f0\u6027\u5bf9\u9636\u6bb5\u7684\u8c03\u63a7\u673a\u5236<br>3. \u7406\u8bba\u9884\u6d4b\u4e0e\u5b9e\u9a8c\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u7edf\u4e00\u89e3\u6790\u4e86\u5927\u89c4\u6a21\u5b66\u4e60\u7684\u590d\u6742\u52a8\u529b\u5b66\uff0c\u66f4\u901a\u8fc7PDE\u7ea6\u5316\u5b9e\u73b0\u4e86\u5bbd\u6cdb\u573a\u666f\u4e0b\u7684\u663e\u5f0f\u6c42\u89e3\uff0c\u4e3a\u7406\u89e3\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u666e\u9002\u6570\u5b66\u5de5\u5177"}}
{"id": "2602.04555", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04555", "abs": "https://arxiv.org/abs/2602.04555", "authors": ["Pourya Shamsolmoali", "Masoumeh Zareapoor"], "title": "Finding Structure in Continual Learning", "comment": "Submitted to NeurIPS 2025", "summary": "Learning from a stream of tasks usually pits plasticity against stability: acquiring new knowledge often causes catastrophic forgetting of past information. Most methods address this by summing competing loss terms, creating gradient conflicts that are managed with complex and often inefficient strategies such as external memory replay or parameter regularization. We propose a reformulation of the continual learning objective using Douglas-Rachford Splitting (DRS). This reframes the learning process not as a direct trade-off, but as a negotiation between two decoupled objectives: one promoting plasticity for new tasks and the other enforcing stability of old knowledge. By iteratively finding a consensus through their proximal operators, DRS provides a more principled and stable learning dynamic. Our approach achieves an efficient balance between stability and plasticity without the need for auxiliary modules or complex add-ons, providing a simpler yet more powerful paradigm for continual learning systems.", "AI": {"tldr": "A paper that reformulates continual learning using Douglas-Rachford Splitting to decouple plasticity and stability objectives, enabling more stable learning without complex auxiliary components.", "motivation": "Continual learning suffers from the plasticity-stability dilemma where acquiring new knowledge causes catastrophic forgetting. Current methods create gradient conflicts by summing competing loss terms and require complex strategies like external memory replay or parameter regularization.", "method": "Reformulates the continual learning objective using Douglas-Rachford Splitting (DRS), which reframes learning as a negotiation between two decoupled objectives (plasticity for new tasks and stability of old knowledge) through iterative proximal operators.", "result": "Achieves an efficient balance between stability and plasticity without needing auxiliary modules or complex add-ons, providing a more principled and stable learning dynamic.", "conclusion": "Offers a simpler yet more powerful paradigm for continual learning systems by decoupling competing objectives rather than treating them as direct trade-offs."}}
{"id": "2602.04574", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04574", "abs": "https://arxiv.org/abs/2602.04574", "authors": ["Jonathan Klees", "Tobias Riedlinger", "Peter Stehr", "Bennet B\u00f6ddecker", "Daniel Kondermann", "Matthias Rottmann"], "title": "Probabilistic Label Spreading: Efficient and Consistent Estimation of Soft Labels with Epistemic Uncertainty on Graphs", "comment": null, "summary": "Safe artificial intelligence for perception tasks remains a major challenge, partly due to the lack of data with high-quality labels. Annotations themselves are subject to aleatoric and epistemic uncertainty, which is typically ignored during annotation and evaluation. While crowdsourcing enables collecting multiple annotations per image to estimate these uncertainties, this approach is impractical at scale due to the required annotation effort. We introduce a probabilistic label spreading method that provides reliable estimates of aleatoric and epistemic uncertainty of labels. Assuming label smoothness over the feature space, we propagate single annotations using a graph-based diffusion method. We prove that label spreading yields consistent probability estimators even when the number of annotations per data point converges to zero. We present and analyze a scalable implementation of our method. Experimental results indicate that, compared to baselines, our approach substantially reduces the annotation budget required to achieve a desired label quality on common image datasets and achieves a new state of the art on the Data-Centric Image Classification benchmark.", "AI": {"tldr": "\u63d0\u51fa\u6982\u7387\u6807\u7b7e\u4f20\u64ad\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u6269\u6563\u4ece\u7a00\u758f\u6807\u6ce8\u4e2d\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\uff0c\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u5e76\u5728\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\u3002", "motivation": "\u5b89\u5168AI\u611f\u77e5\u4efb\u52a1\u9762\u4e34\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u6807\u6ce8\u672c\u8eab\u5b58\u5728aleatoric\u548cepistemic\u4e0d\u786e\u5b9a\u6027\uff0c\u800c\u4f20\u7edf\u4f17\u5305\u65b9\u6cd5\u56e0\u6807\u6ce8\u6210\u672c\u8fc7\u9ad8\u96be\u4ee5\u89c4\u6a21\u5316\u5e94\u7528\u3002", "method": "\u5047\u8bbe\u7279\u5f81\u7a7a\u95f4\u4e2d\u6807\u7b7e\u5e73\u6ed1\uff0c\u5229\u7528\u56fe\u6269\u6563\u65b9\u6cd5\u5c06\u5355\u4e2a\u6807\u6ce8\u4f20\u64ad\u81f3\u672a\u6807\u6ce8\u6570\u636e\uff0c\u5b9e\u73b0\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002\u8bc1\u660e\u4e86\u5f53\u6bcf\u70b9\u6807\u6ce8\u6570\u8d8b\u8fd1\u4e8e\u96f6\u65f6\u4f30\u8ba1\u4ecd\u5177\u6709\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u8fbe\u5230\u76f8\u540c\u6807\u7b7e\u8d28\u91cf\u65f6\u5927\u5e45\u964d\u4f4e\u6807\u6ce8\u9884\u7b97\uff0c\u5e76\u5728Data-Centric Image Classification\u57fa\u51c6\u4e0a\u53d6\u5f97\u65b0\u7684\u6700\u4f18\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u611f\u77e5\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6846\u67b6\uff0c\u5728\u63d0\u5347AI\u5b89\u5168\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u6807\u6ce8\u6210\u672c\u3002"}}
{"id": "2602.04599", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04599", "abs": "https://arxiv.org/abs/2602.04599", "authors": ["Nikola Milosevic", "Leonard Franz", "Daniel Haeufle", "Georg Martius", "Nico Scherf", "Pavel Kolev"], "title": "Stochastic Decision Horizons for Constrained Reinforcement Learning", "comment": null, "summary": "Constrained Markov decision processes (CMDPs) provide a principled model for handling constraints, such as safety and other auxiliary objectives, in reinforcement learning. The common approach of using additive-cost constraints and dual variables often hinders off-policy scalability. We propose a Control as Inference formulation based on stochastic decision horizons, where constraint violations attenuate reward contributions and shorten the effective planning horizon via state-action-dependent continuation. This yields survival-weighted objectives that remain replay-compatible for off-policy actor-critic learning. We propose two violation semantics, absorbing and virtual termination, that share the same survival-weighted return but result in distinct optimization structures that lead to SAC/MPO-style policy improvement. Experiments demonstrate improved sample efficiency and favorable return-violation trade-offs on standard benchmarks. Moreover, MPO with virtual termination (VT-MPO) scales effectively to our high-dimensional musculoskeletal Hyfydy setup.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u51b3\u7b56\u89c6\u754c\u7684\"\u63a7\u5236\u5373\u63a8\u65ad\"\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u5b58\u52a0\u6743\u76ee\u6807\u51fd\u6570\u89e3\u51b3\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b(CMDP)\u4e2d\u7684\u79bb\u7ebf\u7b56\u7565\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5e76\u5728\u6807\u51c6\u57fa\u51c6\u548c\u808c\u8089\u9aa8\u9abc\u4eff\u771f\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u901a\u8fc7\u52a0\u6027\u4ee3\u4ef7\u7ea6\u675f\u548c\u5bf9\u5076\u53d8\u91cf\u5904\u7406\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u963b\u788d\u4e86\u79bb\u7ebf\u7b56\u7565\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u6027\u3002\u8be5\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6570\u5b66\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u7ea6\u675f\u5904\u7406\u80fd\u529b\u7684\u540c\u65f6\uff0c\u517c\u5bb9\u9ad8\u6548\u7684\u79bb\u7ebf\u7b56\u7565actor-critic\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u968f\u673a\u51b3\u7b56\u89c6\u754c\u7684\"\u63a7\u5236\u5373\u63a8\u65ad\"\u5efa\u6a21\u65b9\u6cd5\uff0c\u5176\u4e2d\u7ea6\u675f\u8fdd\u53cd\u4f1a\u8870\u51cf\u5956\u52b1\u8d21\u732e\u5e76\u7f29\u77ed\u6709\u6548\u89c4\u5212\u89c6\u754c\u3002\u8be5\u65b9\u6cd5\u751f\u6210\u751f\u5b58\u52a0\u6743\u76ee\u6807\u51fd\u6570\uff0c\u4fdd\u6301\u4e0e\u7ecf\u9a8c\u56de\u653e\u7684\u517c\u5bb9\u6027\u3002\u5b9a\u4e49\u4e86\u4e24\u79cd\u8fdd\u53cd\u8bed\u4e49\u2014\u2014\u5438\u6536\u6001\u548c\u865a\u62df\u7ec8\u6b62\uff0c\u5b83\u4eec\u4ea7\u751f\u76f8\u540c\u7684\u751f\u5b58\u52a0\u6743\u56de\u62a5\u4f46\u5f62\u6210\u4e0d\u540c\u7684\u4f18\u5316\u7ed3\u6784\uff0c\u5206\u522b\u5bf9\u5e94SAC\u548cMPO\u98ce\u683c\u7684\u7b56\u7565\u6539\u8fdb\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6837\u672c\u6548\u7387\u548c\u66f4\u4f18\u7684\u56de\u62a5-\u8fdd\u53cd\u6743\u8861\u3002\u7279\u522b\u662f\u865a\u62df\u7ec8\u6b62\u7248\u672c\u7684MPO\uff08VT-MPO\uff09\u80fd\u591f\u6709\u6548\u6269\u5c55\u5230\u9ad8\u7ef4\u808c\u8089\u9aa8\u9abcHyfydy\u4eff\u771f\u73af\u5883\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u751f\u5b58\u52a0\u6743\u76ee\u6807\u51fd\u6570\u6846\u67b6\u4e3a\u79bb\u7ebf\u7b56\u7565\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u865a\u62df\u7ec8\u6b62\u8bed\u4e49\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u6269\u5c55\u6027\u80fd\u3002"}}
{"id": "2602.04608", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04608", "abs": "https://arxiv.org/abs/2602.04608", "authors": ["Maya Janvier", "Julien Salomon", "Etienne Meunier"], "title": "Jacobian Regularization Stabilizes Long-Term Integration of Neural Differential Equations", "comment": null, "summary": "Hybrid models and Neural Differential Equations (NDE) are getting increasingly important for the modeling of physical systems, however they often encounter stability and accuracy issues during long-term integration. Training on unrolled trajectories is known to limit these divergences but quickly becomes too expensive due to the need for computing gradients over an iterative process. In this paper, we demonstrate that regularizing the Jacobian of the NDE model via its directional derivatives during training stabilizes long-term integration in the challenging context of short training rollouts. We design two regularizations, one for the case of known dynamics where we can directly derive the directional derivatives of the dynamic and one for the case of unknown dynamics where they are approximated using finite differences. Both methods, while having a far lower cost compared to long rollouts during training, are successful in improving the stability of long-term simulations for several ordinary and partial differential equations, opening up the door to training NDE methods for long-term integration of large scale systems.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u6b63\u5219\u5316\u795e\u7ecf\u5fae\u5206\u65b9\u7a0b(NDE)\u7684\u96c5\u53ef\u6bd4\u77e9\u9635\u65b9\u5411\u5bfc\u6570\u6765\u7a33\u5b9a\u957f\u671f\u79ef\u5206\uff0c\u5728\u5df2\u77e5/\u672a\u77e5\u52a8\u529b\u5b66\u573a\u666f\u4e0b\u5206\u522b\u8bbe\u8ba1\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u7684\u540c\u65f6\u63d0\u5347ODE/PDE\u6a21\u62df\u7a33\u5b9a\u6027", "motivation": "\u795e\u7ecf\u5fae\u5206\u65b9\u7a0b(NDE)\u548c\u6df7\u5408\u6a21\u578b\u5728\u7269\u7406\u7cfb\u7edf\u5efa\u6a21\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u957f\u671f\u79ef\u5206\u65f6\u5b58\u5728\u7a33\u5b9a\u6027\u548c\u7cbe\u5ea6\u95ee\u9898\uff1b\u73b0\u6709\u5c55\u5f00\u8f68\u8ff9\u8bad\u7ec3\u6cd5\u867d\u80fd\u7f13\u89e3\u53d1\u6563\uff0c\u5374\u56e0\u9700\u8fed\u4ee3\u8ba1\u7b97\u68af\u5ea6\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8", "method": "\u8bbe\u8ba1\u4e24\u79cd\u96c5\u53ef\u6bd4\u77e9\u9635\u65b9\u5411\u5bfc\u6570\u6b63\u5219\u5316\u65b9\u6cd5\uff1a\u9488\u5bf9\u5df2\u77e5\u52a8\u529b\u5b66\u573a\u666f\u76f4\u63a5\u63a8\u5bfc\u52a8\u6001\u65b9\u7a0b\u7684\u65b9\u5411\u5bfc\u6570\uff0c\u9488\u5bf9\u672a\u77e5\u52a8\u529b\u5b66\u573a\u666f\u7528\u6709\u9650\u5dee\u5206\u8fd1\u4f3c\u65b9\u5411\u5bfc\u6570", "result": "\u5728\u591a\u4e2a\u5e38\u5fae\u5206\u65b9\u7a0b\u548c\u504f\u5fae\u5206\u65b9\u7a0b\u6d4b\u8bd5\u4e2d\uff0c\u4e24\u79cd\u65b9\u6cd5\u5747\u4ee5\u8fdc\u4f4e\u4e8e\u957f\u8f68\u8ff9\u5c55\u5f00\u8bad\u7ec3\u7684\u6210\u672c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u671f\u6a21\u62df\u7684\u7a33\u5b9a\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u7cfb\u7edf\u7684\u795e\u7ecf\u5fae\u5206\u65b9\u7a0b\u957f\u671f\u79ef\u5206\u8bad\u7ec3\u5f00\u8f9f\u4e86\u65b0\u8def\u5f84\uff0c\u6709\u6548\u5e73\u8861\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u6a21\u62df\u7a33\u5b9a\u6027"}}
{"id": "2602.04609", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.04609", "abs": "https://arxiv.org/abs/2602.04609", "authors": ["Chenxi Hu", "Yue Ma", "Yifan Wu", "Yunhe Hou"], "title": "Resilient Load Forecasting under Climate Change: Adaptive Conditional Neural Processes for Few-Shot Extreme Load Forecasting", "comment": null, "summary": "Extreme weather can substantially change electricity consumption behavior, causing load curves to exhibit sharp spikes and pronounced volatility. If forecasts are inaccurate during those periods, power systems are more likely to face supply shortfalls or localized overloads, forcing emergency actions such as load shedding and increasing the risk of service disruptions and public-safety impacts. This problem is inherently difficult because extreme events can trigger abrupt regime shifts in load patterns, while relevant extreme samples are rare and irregular, making reliable learning and calibration challenging. We propose AdaCNP, a probabilistic forecasting model for data-scarce condition. AdaCNP learns similarity in a shared embedding space. For each target data, it evaluates how relevant each historical context segment is to the current condition and reweights the context information accordingly. This design highlights the most informative historical evidence even when extreme samples are rare. It enables few-shot adaptation to previously unseen extreme patterns. AdaCNP also produces predictive distributions for risk-aware decision-making without expensive fine-tuning on the target domain. We evaluate AdaCNP on real-world power-system load data and compare it against a range of representative baselines. The results show that AdaCNP is more robust during extreme periods, reducing the mean squared error by 22\\% relative to the strongest baseline while achieving the lowest negative log-likelihood, indicating more reliable probabilistic outputs. These findings suggest that AdaCNP can effectively mitigate the combined impact of abrupt distribution shifts and scarce extreme samples, providing a more trustworthy forecasting for resilient power system operation under extreme events.", "AI": {"tldr": "\u63d0\u51faAdaCNP\u6982\u7387\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u91cd\u52a0\u6743\u5386\u53f2\u4e0a\u4e0b\u6587\u4fe1\u606f\u89e3\u51b3\u6781\u7aef\u5929\u6c14\u4e0b\u7535\u529b\u8d1f\u8377\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5b9e\u73b0\u5c11\u6837\u672c\u9002\u5e94\u5e76\u63d0\u5347\u9884\u6d4b\u53ef\u9760\u6027", "motivation": "\u6781\u7aef\u5929\u6c14\u5bfc\u81f4\u7535\u529b\u8d1f\u8377\u66f2\u7ebf\u51fa\u73b0\u5267\u70c8\u6ce2\u52a8\u548c\u5c16\u5cf0\uff0c\u4f20\u7edf\u9884\u6d4b\u65b9\u6cd5\u56e0\u6781\u7aef\u4e8b\u4ef6\u6837\u672c\u7a00\u5c11\u4e14\u4e0d\u89c4\u5219\u800c\u96be\u4ee5\u51c6\u786e\u9884\u6d4b\uff0c\u53ef\u80fd\u5f15\u53d1\u4f9b\u7535\u77ed\u7f3a\u3001\u8fc7\u8f7d\u548c\u670d\u52a1\u4e2d\u65ad\u7b49\u516c\u5171\u5b89\u5168\u98ce\u9669", "method": "AdaCNP\u5728\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\u5b66\u4e60\u76f8\u4f3c\u6027\uff0c\u8bc4\u4f30\u5386\u53f2\u4e0a\u4e0b\u6587\u4e0e\u5f53\u524d\u6761\u4ef6\u7684\u5173\u8054\u5ea6\u5e76\u52a8\u6001\u91cd\u52a0\u6743\uff0c\u7a81\u51fa\u5173\u952e\u5386\u53f2\u8bc1\u636e\uff0c\u65e0\u9700\u6602\u8d35\u5fae\u8c03\u5373\u53ef\u751f\u6210\u9884\u6d4b\u5206\u5e03", "result": "\u5728\u771f\u5b9e\u7535\u529b\u7cfb\u7edf\u6570\u636e\u4e0a\uff0cAdaCNP\u76f8\u6bd4\u6700\u5f3a\u57fa\u7ebf\u5747\u65b9\u8bef\u5dee\u964d\u4f4e22%\uff0c\u8d1f\u5bf9\u6570\u4f3c\u7136\u6700\u4f4e\uff0c\u6781\u7aef\u65f6\u671f\u9884\u6d4b\u66f4\u7a33\u5065\u4e14\u6982\u7387\u8f93\u51fa\u66f4\u53ef\u9760", "conclusion": "AdaCNP\u80fd\u6709\u6548\u7f13\u89e3\u7a81\u53d8\u5206\u5e03\u504f\u79fb\u548c\u6781\u7aef\u6837\u672c\u7a00\u7f3a\u7684\u53cc\u91cd\u6311\u6218\uff0c\u4e3a\u6781\u7aef\u4e8b\u4ef6\u4e0b\u7684\u5f39\u6027\u7535\u529b\u7cfb\u7edf\u8fd0\u884c\u63d0\u4f9b\u53ef\u4fe1\u9884\u6d4b\u652f\u6301"}}
{"id": "2602.04620", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04620", "abs": "https://arxiv.org/abs/2602.04620", "authors": ["Doyeon Lee", "Eunyi Lyou", "Hyunsoo Cho", "Sookyung Kim", "Joonseok Lee", "Jaemoo Choi"], "title": "QUATRO: Query-Adaptive Trust Region Policy Optimization for LLM Fine-tuning", "comment": null, "summary": "GRPO-style reinforcement learning (RL)-based LLM fine-tuning algorithms have recently gained popularity. Relying on heuristic trust-region approximations, however, they can lead to brittle optimization behavior, as global importance-ratio clipping and group-wise normalization fail to regulate samples whose importance ratios fall outside the clipping range. We propose Query-Adaptive Trust-Region policy Optimization (QUATRO), which directly enforces trust-region constraints through a principled optimization. This yields a clear and interpretable objective that enables explicit control over policy updates and stable, entropy-controlled optimization, with a stabilizer terms arising intrinsically from the exact trust-region formulation. Empirically verified on diverse mathematical reasoning benchmarks, QUATRO shows stable training under increased policy staleness and aggressive learning rates, maintaining well-controlled entropy throughout training.", "AI": {"tldr": "\u9488\u5bf9GRPO\u7c7b\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u7b97\u6cd5\u7684\u8106\u5f31\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u67e5\u8be2\u81ea\u9002\u5e94\u4fe1\u4efb\u57df\u7b56\u7565\u4f18\u5316\u65b9\u6cd5QUATRO\uff0c\u901a\u8fc7\u539f\u5219\u6027\u4f18\u5316\u76f4\u63a5\u5f3a\u5236\u4fe1\u4efb\u57df\u7ea6\u675f\uff0c\u5b9e\u73b0\u7a33\u5b9a\u4e14\u71b5\u53ef\u63a7\u7684\u8bad\u7ec3\uff0c\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "GRPO\u98ce\u683c\u7684\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u7b97\u6cd5\u867d\u6d41\u884c\uff0c\u4f46\u4f9d\u8d56\u542f\u53d1\u5f0f\u4fe1\u4efb\u57df\u8fd1\u4f3c\uff0c\u5bfc\u81f4\u4f18\u5316\u884c\u4e3a\u8106\u5f31\uff0c\u5168\u5c40\u91cd\u8981\u6027\u6bd4\u7387\u622a\u65ad\u548c\u7ec4\u5f52\u4e00\u5316\u65e0\u6cd5\u6709\u6548\u7ea6\u675f\u8d85\u51fa\u622a\u65ad\u8303\u56f4\u7684\u91cd\u8981\u6027\u6bd4\u7387\u6837\u672c\u3002", "method": "\u63d0\u51fa\u67e5\u8be2\u81ea\u9002\u5e94\u4fe1\u4efb\u57df\u7b56\u7565\u4f18\u5316\uff08QUATRO\uff09\uff0c\u901a\u8fc7\u539f\u5219\u6027\u4f18\u5316\u76f4\u63a5\u5f3a\u5236\u4fe1\u4efb\u57df\u7ea6\u675f\uff0c\u5f97\u5230\u6e05\u6670\u53ef\u89e3\u91ca\u7684\u76ee\u6807\u51fd\u6570\uff0c\u5b9e\u73b0\u5bf9\u7b56\u7565\u66f4\u65b0\u7684\u663e\u5f0f\u63a7\u5236\uff0c\u7a33\u5b9a\u4e14\u71b5\u53ef\u63a7\u7684\u4f18\u5316\uff0c\u7a33\u5b9a\u9879\u4ece\u7cbe\u786e\u4fe1\u4efb\u57df\u516c\u5f0f\u4e2d\u81ea\u7136\u4ea7\u751f\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7ecf\u9a8c\u9a8c\u8bc1\uff0cQUATRO\u5728\u589e\u52a0\u7b56\u7565\u9648\u65e7\u6027\u548c\u6fc0\u8fdb\u5b66\u4e60\u7387\u4e0b\u8868\u73b0\u7a33\u5b9a\u8bad\u7ec3\uff0c\u5e76\u5728\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u826f\u597d\u63a7\u5236\u7684\u71b5\u3002", "conclusion": "QUATRO\u901a\u8fc7\u7cbe\u786e\u4fe1\u4efb\u57df\u516c\u5f0f\u89e3\u51b3\u4e86\u542f\u53d1\u5f0f\u8fd1\u4f3c\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u3001\u53ef\u63a7\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u4e3aLLM\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.04643", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04643", "abs": "https://arxiv.org/abs/2602.04643", "authors": ["Yanan He", "Yunshi Wen", "Xin Wang", "Tengfei Ma"], "title": "MTS-JEPA: Multi-Resolution Joint-Embedding Predictive Architecture for Time-Series Anomaly Prediction", "comment": null, "summary": "Multivariate time series underpin modern critical infrastructure, making the prediction of anomalies a vital necessity for proactive risk mitigation. While Joint-Embedding Predictive Architectures (JEPA) offer a promising framework for modeling the latent evolution of these systems, their application is hindered by representation collapse and an inability to capture precursor signals across varying temporal scales. To address these limitations, we propose MTS-JEPA, a specialized architecture that integrates a multi-resolution predictive objective with a soft codebook bottleneck. This design explicitly decouples transient shocks from long-term trends, and utilizes the codebook to capture discrete regime transitions. Notably, we find this constraint also acts as an intrinsic regularizer to ensure optimization stability. Empirical evaluations on standard benchmarks confirm that our approach effectively prevents degenerate solutions and achieves state-of-the-art performance under the early-warning protocol.", "AI": {"tldr": "\u63d0\u51faMTS-JEPA\u67b6\u6784\uff0c\u901a\u8fc7\u591a\u5206\u8fa8\u7387\u9884\u6d4b\u76ee\u6807\u548c\u8f6f\u7801\u672c\u74f6\u9888\u89e3\u51b3\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u9884\u6d4b\u4e2d\u7684\u8868\u793a\u5d29\u6e83\u548c\u8de8\u5c3a\u5ea6\u524d\u5146\u4fe1\u53f7\u6355\u6349\u95ee\u9898\uff0c\u5b9e\u73b0\u65e9\u671f\u9884\u8b66\u7684SOTA\u6027\u80fd", "motivation": "\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u652f\u6491\u73b0\u4ee3\u5173\u952e\u57fa\u7840\u8bbe\u65bd\uff0c\u5f02\u5e38\u9884\u6d4b\u5bf9\u4e3b\u52a8\u98ce\u9669\u7f13\u89e3\u81f3\u5173\u91cd\u8981\uff1b\u73b0\u6709\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u67b6\u6784\u5b58\u5728\u8868\u793a\u5d29\u6e83\u548c\u65e0\u6cd5\u6355\u6349\u591a\u65f6\u95f4\u5c3a\u5ea6\u524d\u5146\u4fe1\u53f7\u7684\u5c40\u9650", "method": "\u8bbe\u8ba1MTS-JEPA\u4e13\u7528\u67b6\u6784\uff1a1) \u591a\u5206\u8fa8\u7387\u9884\u6d4b\u76ee\u6807\u5206\u79bb\u77ac\u65f6\u51b2\u51fb\u4e0e\u957f\u671f\u8d8b\u52bf\uff1b2) \u8f6f\u7801\u672c\u74f6\u9888\u6355\u83b7\u79bb\u6563\u72b6\u6001\u8f6c\u79fb\uff1b3) \u8be5\u7ea6\u675f\u4f5c\u4e3a\u5185\u5728\u6b63\u5219\u5316\u5668\u786e\u4fdd\u4f18\u5316\u7a33\u5b9a\u6027", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6709\u6548\u9632\u6b62\u9000\u5316\u89e3\uff0c\u65e9\u671f\u9884\u8b66\u534f\u8bae\u4e0b\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u6210\u529f\u89e3\u51b3JEPA\u5728\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u9884\u6d4b\u4e2d\u7684\u6838\u5fc3\u7f3a\u9677\uff0c\u4e3a\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u98ce\u9669\u7ba1\u7406\u63d0\u4f9b\u6709\u6548\u65b9\u6848"}}
{"id": "2602.04651", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04651", "abs": "https://arxiv.org/abs/2602.04651", "authors": ["Dipan Maity"], "title": "SAFE: Stable Alignment Finetuning with Entropy-Aware Predictive Control for RLHF", "comment": null, "summary": "Optimization (PPO) has been positioned by recent literature as the canonical method for the RL part of RLHF. PPO performs well empirically but has a heuristic motivation and handles the KL-divergence constraint used in LM-RLHF in an ad-hoc manner and suffers form reward oscillations, entropy collapse, value function drift, and sudden policy divergence that require frequent restarts and extensive hyperparameter tuning. In this paper, we develop a new pure on policy actor-critic RL method for the LM-RLHF setting. We present SAFE (Stable Alignment Finetuning with Entropy-aware control),a novel RLHF algorithm that combines a Double Soft-Min Critic for pessimistic value estimation with a new multi-layer stabilization framework combining entropy-gated KL regulation, and PID-controlled adaptive thresholds. Unlike standard PPO's symmetric KL penalties, SAFE distinguishes high-entropy exploration from low-entropy mode collapse and adjusts penalties dynamically based on reward velocity. Experiments on a 3B parameter model show SAFE achieves +5.15\\% training-average reward than PPO (0.725 vs 0.689), negligible reward crashes, and superior KL control than ppo . Our method adds minimal computational overhead and provides an interpretable, crash-resistant RLHF framework that maintains aggressive learning speed while ensuring stable long-horizon optimization suitable for production deployment. Code is available at https://github.com/ryyzn9/SAFE", "AI": {"tldr": "This paper proposes SAFE, a novel RLHF algorithm that replaces PPO with a more stable method combining Double Soft-Min Critic, entropy-gated KL regulation, and PID-controlled adaptive thresholds to achieve better reward and stability with minimal overhead.", "motivation": "PPO, the standard method for RLHF, has several issues including heuristic motivation, ad-hoc KL-divergence constraint handling, reward oscillations, entropy collapse, value function drift, and sudden policy divergence that require frequent restarts and extensive hyperparameter tuning. The paper aims to develop a more stable and reliable RLHF method.", "method": "The paper develops SAFE (Stable Alignment Finetuning with Entropy-aware control), which combines: 1) Double Soft-Min Critic for pessimistic value estimation, 2) Multi-layer stabilization framework with entropy-gated KL regulation, and 3) PID-controlled adaptive thresholds. Unlike PPO's symmetric KL penalties, SAFE distinguishes high-entropy exploration from low-entropy mode collapse and dynamically adjusts penalties based on reward velocity.", "result": "Experiments on a 3B parameter model show SAFE achieves +5.15% training-average reward than PPO (0.725 vs 0.689), negligible reward crashes, and superior KL control compared to PPO.", "conclusion": "SAFE provides an interpretable, crash-resistant RLHF framework that maintains aggressive learning speed while ensuring stable long-horizon optimization suitable for production deployment, with minimal computational overhead."}}
{"id": "2602.04675", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04675", "abs": "https://arxiv.org/abs/2602.04675", "authors": ["Panagiotis Theodoropoulos", "Juno Nam", "Evangelos Theodorou", "Jaemoo Choi"], "title": "Generalized Schr\u00f6dinger Bridge on Graphs", "comment": null, "summary": "Transportation on graphs is a fundamental challenge across many domains, where decisions must respect topological and operational constraints. Despite the need for actionable policies, existing graph-transport methods lack this expressivity. They rely on restrictive assumptions, fail to generalize across sparse topologies, and scale poorly with graph size and time horizon. To address these issues, we introduce Generalized Schr\u00f6dinger Bridge on Graphs (GSBoG), a novel scalable data-driven framework for learning executable controlled continuous-time Markov chain (CTMC) policies on arbitrary graphs under state cost augmented dynamics. Notably, GSBoG learns trajectory-level policies, avoiding dense global solvers and thereby enhancing scalability. This is achieved via a likelihood optimization approach, satisfying the endpoint marginals, while simultaneously optimizing intermediate behavior under state-dependent running costs. Extensive experimentation on challenging real-world graph topologies shows that GSBoG reliably learns accurate, topology-respecting policies while optimizing application-specific intermediate state costs, highlighting its broad applicability and paving new avenues for cost-aware dynamical transport on general graphs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u56fe\u4e0a\u7684\u5e7f\u4e49\u859b\u5b9a\u8c14\u6865\uff08GSBoG\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u4f3c\u7136\u4f18\u5316\u5b66\u4e60\u53ef\u6267\u884c\u7684\u65f6\u7a7a\u9a6c\u5c14\u53ef\u592b\u94fe\u7b56\u7565\uff0c\u89e3\u51b3\u56fe\u8fd0\u8f93\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u8868\u8fbe\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u56fe\u8fd0\u8f93\u65b9\u6cd5\u53d7\u9650\u4e8e\u4e25\u683c\u5047\u8bbe\u3001\u5728\u7a00\u758f\u62d3\u6251\u4e0a\u6cdb\u5316\u80fd\u529b\u5dee\u3001\u968f\u56fe\u89c4\u6a21\u548c\u65f6\u95f4\u8303\u56f4\u6269\u5c55\u6027\u5dee\uff0c\u7f3a\u4e4f\u53ef\u64cd\u4f5c\u7b56\u7565\u7684\u8868\u8fbe\u80fd\u529b\u3002", "method": "GSBoG\u662f\u6570\u636e\u9a71\u52a8\u6846\u67b6\uff0c\u5b66\u4e60\u5e26\u72b6\u6001\u4ee3\u4ef7\u7684\u4efb\u610f\u56fe\u4e0a\u7684\u53ef\u63a7\u8fde\u7eed\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u94fe\u7b56\u7565\u3002\u91c7\u7528\u4f3c\u7136\u4f18\u5316\u65b9\u6cd5\u6ee1\u8db3\u7aef\u70b9\u7ea6\u675f\uff0c\u540c\u65f6\u4f18\u5316\u4e2d\u95f4\u884c\u4e3a\uff0c\u907f\u514d\u5bc6\u96c6\u5168\u5c40\u6c42\u89e3\u5668\u4ee5\u63d0\u9ad8\u53ef\u6269\u5c55\u6027\u3002", "result": "\u5728\u771f\u5b9e\u56fe\u62d3\u6251\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cGSBoG\u80fd\u53ef\u9760\u5b66\u4e60\u51c6\u786e\u7684\u3001\u5c0a\u91cd\u62d3\u6251\u7ed3\u6784\u7684\u7b56\u7565\uff0c\u540c\u65f6\u4f18\u5316\u5e94\u7528\u7279\u5b9a\u7684\u4e2d\u95f4\u72b6\u6001\u4ee3\u4ef7\uff0c\u5c55\u73b0\u51fa\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002", "conclusion": "GSBoG\u4e3a\u56fe\u8fd0\u8f93\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u9ad8\u8868\u8fbe\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u6210\u672c\u611f\u77e5\u7684\u52a8\u6001\u56fe\u8fd0\u8f93\u5f00\u8f9f\u4e86\u65b0\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2602.04677", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.04677", "abs": "https://arxiv.org/abs/2602.04677", "authors": ["Ondrej Tybl", "Lukas Neumann"], "title": "REDistill: Robust Estimator Distillation for Balancing Robustness and Efficiency", "comment": null, "summary": "Knowledge Distillation (KD) transfers knowledge from a large teacher model to a smaller student by aligning their predictive distributions. However, conventional KD formulations - typically based on Kullback-Leibler divergence - assume that the teacher provides reliable soft targets. In practice, teacher predictions are often noisy or overconfident, and existing correction-based approaches rely on ad-hoc heuristics and extensive hyper-parameter tuning, which hinders generalization. We introduce REDistill (Robust Estimator Distillation), a simple yet principled framework grounded in robust statistics. REDistill replaces the standard KD objective with a power divergence loss, a generalization of KL divergence that adaptively downweights unreliable teacher output while preserving informative logit relationships. This formulation provides a unified and interpretable treatment of teacher noise, requires only logits, integrates seamlessly into existing KD pipelines, and incurs negligible computational overhead. Extensive experiments on CIFAR-100 and ImageNet-1k demonstrate that REDistill consistently improves student accuracy in diverse teacher-student architectures. Remarkably, it achieves these gains without model-specific hyper-parameter tuning, underscoring its robustness and strong generalization to unseen teacher-student pairs.", "AI": {"tldr": "REDistill\u91c7\u7528\u5e42\u6563\u5ea6\u635f\u5931\u66ff\u4ee3KL\u6563\u5ea6\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\uff0c\u80fd\u6709\u6548\u5904\u7406\u6559\u5e08\u6a21\u578b\u7684\u566a\u58f0\u548c\u8fc7\u5ea6\u81ea\u4fe1\u9884\u6d4b\uff0c\u63d0\u5347\u9c81\u68d2\u6027\u4e14\u65e0\u9700\u989d\u5916\u8d85\u53c2\u6570\u8c03\u4f18\u3002", "motivation": "\u6807\u51c6\u77e5\u8bc6\u84b8\u998f\u5047\u8bbe\u6559\u5e08\u6a21\u578b\u63d0\u4f9b\u53ef\u9760\u7684\u8f6f\u6807\u7b7e\uff0c\u4f46\u5b9e\u8df5\u4e2d\u6559\u5e08\u9884\u6d4b\u5e38\u542b\u566a\u58f0\u6216\u8fc7\u5ea6\u81ea\u4fe1\u3002\u73b0\u6709\u4fee\u6b63\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u7b56\u7565\u4e14\u9700\u5927\u91cf\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u6cdb\u5316\u6027\u5dee\u3002", "method": "\u63d0\u51faREDistill\u6846\u67b6\uff0c\u57fa\u4e8e\u9c81\u68d2\u7edf\u8ba1\u7406\u8bba\uff0c\u7528\u5e42\u6563\u5ea6\u635f\u5931\u66ff\u4ee3\u6807\u51c6KD\u76ee\u6807\u3002\u8be5\u635f\u5931\u80fd\u81ea\u9002\u5e94\u964d\u4f4e\u4e0d\u53ef\u9760\u6559\u5e08\u8f93\u51fa\u7684\u6743\u91cd\uff0c\u540c\u65f6\u4fdd\u7559\u6709\u6548\u7684logit\u5173\u7cfb\u3002\u4ec5\u9700logits\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709KD\u6d41\u7a0b\uff0c\u8ba1\u7b97\u5f00\u9500\u53ef\u5ffd\u7565\u3002", "result": "\u5728CIFAR-100\u548cImageNet-1k\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cREDistill\u5728\u4e0d\u540c\u5e08\u751f\u67b6\u6784\u4e0b\u5747\u80fd\u6301\u7eed\u63d0\u5347\u5b66\u751f\u6a21\u578b\u51c6\u786e\u7387\uff0c\u4e14\u65e0\u9700\u9488\u5bf9\u7279\u5b9a\u6a21\u578b\u8c03\u4f18\u8d85\u53c2\u6570\u3002", "conclusion": "REDistill\u5177\u6709\u5f3a\u9c81\u68d2\u6027\uff0c\u80fd\u5f88\u597d\u5730\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u5e08\u751f\u6a21\u578b\u5bf9\u3002"}}
{"id": "2602.04689", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04689", "abs": "https://arxiv.org/abs/2602.04689", "authors": ["Mahima Lakra", "Ronan Fablet", "Lucas Drumetz", "Etienne Pauthenet", "Elodie Martinez"], "title": "Static and auto-regressive neural emulation of phytoplankton biomass dynamics from physical predictors in the global ocean", "comment": null, "summary": "Phytoplankton is the basis of marine food webs, driving both ecological processes and global biogeochemical cycles. Despite their ecological and climatic significance, accurately simulating phytoplankton dynamics remains a major challenge for biogeochemical numerical models due to limited parameterizations, sparse observational data, and the complexity of oceanic processes. Here, we explore how deep learning models can be used to address these limitations predicting the spatio-temporal distribution of phytoplankton biomass in the global ocean based on satellite observations and environmental conditions. First, we investigate several deep learning architectures. Among the tested models, the UNet architecture stands out for its ability to reproduce the seasonal and interannual patterns of phytoplankton biomass more accurately than other models like CNNs, ConvLSTM, and 4CastNet. When using one to two months of environmental data as input, UNet performs better, although it tends to underestimate the amplitude of low-frequency changes in phytoplankton biomass. Thus, to improve predictions over time, an auto-regressive version of UNet was also tested, where the model uses its own previous predictions to forecast future conditions. This approach works well for short-term forecasts (up to five months), though its performance decreases for longer time scales. Overall, our study shows that combining ocean physical predictors with deep learning allows for reconstruction and short-term prediction of phytoplankton dynamics. These models could become powerful tools for monitoring ocean health and supporting marine ecosystem management, especially in the context of climate change.", "AI": {"tldr": "\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5c24\u5176\u662fUNet\u67b6\u6784\uff09\u7ed3\u5408\u536b\u661f\u89c2\u6d4b\u548c\u73af\u5883\u6570\u636e\uff0c\u53ef\u8f83\u51c6\u786e\u9884\u6d4b\u5168\u7403\u6d77\u6d0b\u6d6e\u6e38\u690d\u7269\u751f\u7269\u91cf\u7684\u65f6\u7a7a\u5206\u5e03\uff0c\u77ed\u671f\uff08\u22645\u4e2a\u6708\uff09\u9884\u6d4b\u6548\u679c\u826f\u597d\uff0c\u4f46\u5b58\u5728\u4f4e\u4f30\u4f4e\u9891\u53d8\u5316\u5e45\u5ea6\u548c\u957f\u671f\u9884\u6d4b\u6027\u80fd\u4e0b\u964d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u6d6e\u6e38\u690d\u7269\u5bf9\u6d77\u6d0b\u751f\u6001\u7cfb\u7edf\u548c\u5168\u7403\u751f\u7269\u5730\u7403\u5316\u5b66\u5faa\u73af\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6570\u503c\u6a21\u578b\u56e0\u53c2\u6570\u5316\u4e0d\u8db3\u3001\u89c2\u6d4b\u6570\u636e\u7a00\u7f3a\u53ca\u6d77\u6d0b\u8fc7\u7a0b\u590d\u6742\u6027\uff0c\u96be\u4ee5\u51c6\u786e\u6a21\u62df\u5176\u52a8\u6001\u53d8\u5316\u3002", "method": "\u6d4b\u8bd5\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff08CNN\u3001ConvLSTM\u30014CastNet\u3001UNet\uff09\uff0c\u91c7\u75281-2\u4e2a\u6708\u73af\u5883\u6570\u636e\u4f5c\u4e3a\u8f93\u5165\uff1b\u8fdb\u4e00\u6b65\u5f00\u53d1\u81ea\u56de\u5f52UNet\u6a21\u578b\uff0c\u5229\u7528\u5386\u53f2\u9884\u6d4b\u503c\u8fed\u4ee3\u9884\u6d4b\u672a\u6765\u72b6\u6001\u3002", "result": "UNet\u5728\u590d\u73b0\u5b63\u8282\u6027\u548c\u5e74\u9645\u53d8\u5316\u6a21\u5f0f\u4e0a\u8868\u73b0\u6700\u4f18\uff1b\u81ea\u56de\u5f52UNet\u53ef\u5b9e\u73b0\u22645\u4e2a\u6708\u7684\u51c6\u786e\u77ed\u671f\u9884\u6d4b\uff0c\u4f46\u9884\u6d4b\u65f6\u957f\u589e\u52a0\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u4e14\u6a21\u578b\u666e\u904d\u4f4e\u4f30\u6d6e\u6e38\u690d\u7269\u751f\u7269\u91cf\u7684\u4f4e\u9891\u53d8\u5316\u5e45\u5ea6\u3002", "conclusion": "\u7ed3\u5408\u6d77\u6d0b\u7269\u7406\u9884\u6d4b\u56e0\u5b50\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u80fd\u6709\u6548\u91cd\u5efa\u548c\u77ed\u671f\u9884\u6d4b\u6d6e\u6e38\u690d\u7269\u52a8\u6001\uff0c\u4e3a\u6c14\u5019\u53d8\u5316\u80cc\u666f\u4e0b\u76d1\u6d4b\u6d77\u6d0b\u5065\u5eb7\u548c\u652f\u6301\u751f\u6001\u7cfb\u7edf\u7ba1\u7406\u63d0\u4f9b\u65b0\u5de5\u5177\u3002"}}
{"id": "2602.04709", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04709", "abs": "https://arxiv.org/abs/2602.04709", "authors": ["Andreas Roth"], "title": "Towards Understanding and Avoiding Limitations of Convolutions on Graphs", "comment": "dissertation", "summary": "While message-passing neural networks (MPNNs) have shown promising results, their real-world impact remains limited. Although various limitations have been identified, their theoretical foundations remain poorly understood, leading to fragmented research efforts. In this thesis, we provide an in-depth theoretical analysis and identify several key properties limiting the performance of MPNNs. Building on these findings, we propose several frameworks that address these shortcomings. We identify two properties exhibited by many MPNNs: shared component amplification (SCA), where each message-passing iteration amplifies the same components across all feature channels, and component dominance (CD), where a single component gets increasingly amplified as more message-passing steps are applied. These properties lead to the observable phenomenon of rank collapse of node representations, which generalizes the established over-smoothing phenomenon. By generalizing and decomposing over-smoothing, we enable a deeper understanding of MPNNs, more targeted solutions, and more precise communication within the field. To avoid SCA, we show that utilizing multiple computational graphs or edge relations is necessary. Our multi-relational split (MRS) framework transforms any existing MPNN into one that leverages multiple edge relations. Additionally, we introduce the spectral graph convolution for multiple feature channels (MIMO-GC), which naturally uses multiple computational graphs. A localized variant, LMGC, approximates the MIMO-GC while inheriting its beneficial properties. To address CD, we demonstrate a close connection between MPNNs and the PageRank algorithm. Based on personalized PageRank, we propose a variant of MPNNs that allows for infinitely many message-passing iterations, while preserving initial node features. Collectively, these results deepen the theoretical understanding of MPNNs.", "AI": {"tldr": "This paper provides a theoretical analysis of Message-Passing Neural Networks (MPNNs), identifying two key limiting properties\u2014Shared Component Amplification (SCA) and Component Dominance (CD)\u2014that cause rank collapse (generalizing over-smoothing). It proposes frameworks (MRS, MIMO-GC, LMGC) to avoid SCA using multiple edge relations/computational graphs, and a PageRank-based variant to address CD, enabling infinite message-passing while preserving node features.", "motivation": "MPNNs have limited real-world impact due to poorly understood theoretical foundations, causing fragmented research efforts. A deeper theoretical understanding is needed to enable more targeted solutions and precise communication within the field.", "method": "In-depth theoretical analysis identifying SCA and CD properties in MPNNs, leading to three frameworks: Multi-Relational Split (MRS) for multiple edge relations, MIMO-GC for multiple computational graphs (with localized variant LMGC), and a personalized PageRank-based MPNN variant.", "result": "The analysis reveals rank collapse as a generalization of over-smoothing. MRS/MIMO-GC/LMGC successfully avoid SCA, while the PageRank-based variant addresses CD, allowing infinitely many message-passing iterations while preserving initial node features.", "conclusion": "This work deepens the theoretical understanding of MPNN limitations by generalizing over-smoothing into rank collapse, providing practical frameworks to overcome these issues, and establishing a connection to PageRank that enables infinite-depth MPNNs."}}
{"id": "2602.04714", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04714", "abs": "https://arxiv.org/abs/2602.04714", "authors": ["Luca Stradiotti", "Laurens Devos", "Anna Monreale", "Jesse Davis", "Andrea Pugnana"], "title": "Bounded-Abstention Multi-horizon Time-series Forecasting", "comment": null, "summary": "Multi-horizon time-series forecasting involves simultaneously making predictions for a consecutive sequence of subsequent time steps. This task arises in many application domains, such as healthcare and finance, where mispredictions can have a high cost and reduce trust. The learning with abstention framework tackles these problems by allowing a model to abstain from offering a prediction when it is at an elevated risk of making a misprediction. Unfortunately, existing abstention strategies are ill-suited for the multi-horizon setting: they target problems where a model offers a single prediction for each instance. Hence, they ignore the structured and correlated nature of the predictions offered by a multi-horizon forecaster. We formalize the problem of learning with abstention for multi-horizon forecasting setting and show that its structured nature admits a richer set of abstention problems. Concretely, we propose three natural notions of how a model could abstain for multi-horizon forecasting. We theoretically analyze each problem to derive the optimal abstention strategy and propose an algorithm that implements it. Extensive evaluation on 24 datasets shows that our proposed algorithms significantly outperforms existing baselines.", "AI": {"tldr": "\u9488\u5bf9\u533b\u7597\u3001\u91d1\u878d\u7b49\u9ad8\u6210\u672c\u9886\u57df\u7684\u591a\u65f6\u57df\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u8be5\u8bba\u6587\u63d0\u51fa\u4e09\u79cd\u62d2\u7edd\u9884\u6d4b\u673a\u5236\uff0c\u5229\u7528\u5176\u7ed3\u6784\u5316\u7279\u6027\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bbe\u8ba1\u7b97\u6cd5\uff0c\u5728 24 \u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u533b\u7597\u3001\u91d1\u878d\u7b49\u9ad8\u6210\u672c\u9886\u57df\u9700\u591a\u65f6\u57df\u9884\u6d4b\uff0c\u8bef\u9884\u6d4b\u4f1a\u964d\u4f4e\u6210\u672c\u5e76\u524a\u5f31\u4fe1\u4efb\u3002\u5b66\u4e60\u62d2\u7edd\u9884\u6d4b\u6846\u67b6\u5141\u8bb8\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u65f6\u907f\u514d\u9884\u6d4b\uff0c\u4f46\u73b0\u6709\u7b56\u7565\u4ec5\u9002\u7528\u4e8e\u5355\u4e00\u9884\u6d4b\uff0c\u65e0\u6cd5\u5229\u7528\u591a\u65f6\u57df\u9884\u6d4b\u7684\u7ed3\u6784\u5316\u76f8\u5173\u6027\u3002", "method": "\u5f62\u5f0f\u5316\u591a\u65f6\u57df\u9884\u6d4b\u7684\u62d2\u7edd\u9884\u6d4b\u95ee\u9898\uff0c\u63d0\u51fa\u4e09\u79cd\u81ea\u7136\u62d2\u7edd\u7b56\u7565\u6982\u5ff5\uff0c\u7406\u8bba\u5206\u6790\u63a8\u5bfc\u6bcf\u79cd\u95ee\u9898\u7684\u6700\u4f18\u7b56\u7565\uff0c\u5e76\u8bbe\u8ba1\u5b9e\u73b0\u7b97\u6cd5\u3002", "result": "\u5728 24 \u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u6240\u63d0\u7b97\u6cd5\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u591a\u65f6\u57df\u9884\u6d4b\u7684\u7ed3\u6784\u5316\u7279\u6027\u652f\u6301\u66f4\u4e30\u5bcc\u7684\u62d2\u7edd\u9884\u6d4b\u6846\u67b6\uff0c\u6240\u63d0\u65b9\u6cd5\u80fd\u6709\u6548\u5229\u7528\u8be5\u7279\u6027\uff0c\u63d0\u5347\u9884\u6d4b\u53ef\u9760\u6027\u4e0e\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2602.04725", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.04725", "abs": "https://arxiv.org/abs/2602.04725", "authors": ["Neville Mathew", "Yidan Shen", "Renjie Hu", "Maham Rahimi", "George Zouridakis"], "title": "Benchmarking and Enhancing PPG-Based Cuffless Blood Pressure Estimation Methods", "comment": null, "summary": "Cuffless blood pressure screening based on easily acquired photoplethysmography (PPG) signals offers a practical pathway toward scalable cardiovascular health assessment. Despite rapid progress, existing PPG-based blood pressure estimation models have not consistently achieved the established clinical numerical limits such as AAMI/ISO 81060-2, and prior evaluations often lack the rigorous experimental controls necessary for valid clinical assessment. Moreover, the publicly available datasets commonly used are heterogeneous and lack physiologically controlled conditions for fair benchmarking. To enable fair benchmarking under physiologically controlled conditions, we created a standardized benchmarking subset NBPDB comprising 101,453 high-quality PPG segments from 1,103 healthy adults, derived from MIMIC-III and VitalDB. Using this dataset, we systematically benchmarked several state-of-the-art PPG-based models. The results showed that none of the evaluated models met the AAMI/ISO 81060-2 accuracy requirements (mean error $<$ 5 mmHg and standard deviation $<$ 8 mmHg). To improve model accuracy, we modified these models and added patient demographic data such as age, sex, and body mass index as additional inputs. Our modifications consistently improved performance across all models. In particular, the MInception model reduced error by 23\\% after adding the demographic data and yielded mean absolute errors of 4.75 mmHg (SBP) and 2.90 mmHg (DBP), achieves accuracy comparable to the numerical limits defined by AAMI/ISO accuracy standards. Our results show that existing PPG-based BP estimation models lack clinical practicality under standardized conditions, while incorporating demographic information markedly improves their accuracy and physiological validity.", "AI": {"tldr": "\u8be5\u7814\u7a76\u521b\u5efa\u4e86\u6807\u51c6\u5316PPG\u8840\u538b\u6570\u636e\u96c6NBPDB\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5747\u672a\u8fbe\u4e34\u5e8a\u7cbe\u5ea6\u6807\u51c6\uff0c\u4f46\u52a0\u5165\u5e74\u9f84/\u6027\u522b/BMI\u7b49\u4eba\u53e3\u7edf\u8ba1\u5b66\u6570\u636e\u540e\uff0cMInception\u6a21\u578b\u8bef\u5dee\u964d\u4f4e23%\u63a5\u8fd1\u56fd\u9645\u6807\u51c6", "motivation": "\u73b0\u6709PPG\u65e0\u521b\u8840\u538b\u6a21\u578b\u4e34\u5e8a\u7cbe\u5ea6\u4e0d\u8db3\u4e14\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\uff0c\u516c\u5f00\u6570\u636e\u96c6\u5f02\u8d28\u6027\u9ad8\u4e14\u7f3a\u4e4f\u751f\u7406\u63a7\u5236\u6761\u4ef6\uff0c\u963b\u788d\u4e86\u53ef\u9760\u4e34\u5e8a\u8f6c\u5316", "method": "1) \u4eceMIMIC-III\u548cVitalDB\u6784\u5efa\u542b10.1\u4e07\u9ad8\u8d28\u91cfPPG\u6bb5\u7684\u6807\u51c6\u5316\u6570\u636e\u96c6NBPDB\uff1b2) \u7cfb\u7edf\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u6d41PPG-BP\u6a21\u578b\uff1b3) \u6539\u8fdb\u6a21\u578b\u5e76\u5f15\u5165\u5e74\u9f84/\u6027\u522b/BMI\u4f5c\u4e3a\u989d\u5916\u8f93\u5165", "result": "1) \u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u5747\u672a\u6ee1\u8db3AAMI/ISO\u6807\u51c6\uff08\u5e73\u5747\u8bef\u5dee<5 mmHg\u4e14\u6807\u51c6\u5dee<8 mmHg\uff09\uff1b2) \u52a0\u5165\u4eba\u53e3\u7edf\u8ba1\u5b66\u6570\u636e\u540e\u5404\u6a21\u578b\u6027\u80fd\u4e00\u81f4\u63d0\u5347\uff1b3) MInception\u6a21\u578b\u8bef\u5dee\u964d\u4f4e23%\uff0c\u8fbe\u5230SBP 4.75 mmHg/DBP 2.90 mmHg\u7684\u8bef\u5dee\u6c34\u5e73", "conclusion": "\u5f53\u524dPPG-BP\u6a21\u578b\u5728\u6807\u51c6\u5316\u6761\u4ef6\u4e0b\u7f3a\u4e4f\u4e34\u5e8a\u5b9e\u7528\u6027\uff0c\u4f46\u6574\u5408\u60a3\u8005\u4eba\u53e3\u7edf\u8ba1\u5b66\u4fe1\u606f\u53ef\u663e\u8457\u63d0\u5347\u7cbe\u5ea6\u548c\u751f\u7406\u6709\u6548\u6027\uff0c\u4e3a\u65e0\u521b\u8840\u538b\u76d1\u6d4b\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2602.04737", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04737", "abs": "https://arxiv.org/abs/2602.04737", "authors": ["Kejiang Qian", "Amos Storkey", "Fengxiang He"], "title": "Rationality Measurement and Theory for Reinforcement Learning Agents", "comment": null, "summary": "This paper proposes a suite of rationality measures and associated theory for reinforcement learning agents, a property increasingly critical yet rarely explored. We define an action in deployment to be perfectly rational if it maximises the hidden true value function in the steepest direction. The expected value discrepancy of a policy's actions against their rational counterparts, culminating over the trajectory in deployment, is defined to be expected rational risk; an empirical average version in training is also defined. Their difference, termed as rational risk gap, is decomposed into (1) an extrinsic component caused by environment shifts between training and deployment, and (2) an intrinsic one due to the algorithm's generalisability in a dynamic environment. They are upper bounded by, respectively, (1) the $1$-Wasserstein distance between transition kernels and initial state distributions in training and deployment, and (2) the empirical Rademacher complexity of the value function class. Our theory suggests hypotheses on the benefits from regularisers (including layer normalisation, $\\ell_2$ regularisation, and weight normalisation) and domain randomisation, as well as the harm from environment shifts. Experiments are in full agreement with these hypotheses. The code is available at https://github.com/EVIEHub/Rationality.", "AI": {"tldr": "\u63d0\u51fa\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u7684\u7406\u6027\u5ea6\u91cf\u6846\u67b6\uff0c\u5b9a\u4e49\u9884\u671f\u7406\u6027\u98ce\u9669\u4e0e\u7406\u6027\u98ce\u9669\u95f4\u9699\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63ed\u793a\u73af\u5883\u504f\u79fb\u548c\u7b97\u6cd5\u6cdb\u5316\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u5e76\u9a8c\u8bc1\u6b63\u5219\u5316\u4e0e\u57df\u968f\u673a\u5316\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u7684\u7406\u6027\u5c5e\u6027\u81f3\u5173\u91cd\u8981\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\uff0c\u9700\u5efa\u7acb\u53ef\u91cf\u5316\u7684\u7406\u8bba\u6846\u67b6\u4ee5\u8bc4\u4f30\u90e8\u7f72\u65f6\u7684\u51b3\u7b56\u5408\u7406\u6027\uff0c\u5c24\u5176\u5173\u6ce8\u8bad\u7ec3\u4e0e\u90e8\u7f72\u73af\u5883\u5dee\u5f02\u5e26\u6765\u7684\u98ce\u9669\u3002", "method": "1) \u5b9a\u4e49\u5b8c\u7f8e\u7406\u6027\u52a8\u4f5c\u4e3a\u771f\u5b9e\u4ef7\u503c\u51fd\u6570\u6700\u901f\u4e0a\u5347\u65b9\u5411\uff1b2) \u6784\u5efa\u9884\u671f\u7406\u6027\u98ce\u9669\uff08\u90e8\u7f72\u8f68\u8ff9\u671f\u671b\u503c\u504f\u5dee\uff09\u53ca\u5176\u8bad\u7ec3\u7248\u672c\uff1b3) \u5206\u89e3\u7406\u6027\u98ce\u9669\u95f4\u9699\u4e3a\u73af\u5883\u504f\u79fb\uff08\u5916\u56e0\uff09\u548c\u7b97\u6cd5\u6cdb\u5316\uff08\u5185\u56e0\uff09\uff1b4) \u75281-Wasserstein\u8ddd\u79bb\u4e0eRademacher\u590d\u6742\u5ea6\u5206\u522b\u754c\u5b9a\u4e24\u7c7b\u95f4\u9699\u4e0a\u754c\u3002", "result": "\u7406\u8bba\u8bc1\u660e\uff1a\u73af\u5883\u504f\u79fb\u95f4\u9699\u2264\u8bad\u7ec3/\u90e8\u7f72\u8f6c\u79fb\u6838\u4e0e\u521d\u59cb\u5206\u5e03\u7684Wasserstein\u8ddd\u79bb\uff1b\u6cdb\u5316\u95f4\u9699\u2264\u4ef7\u503c\u51fd\u6570\u7c7b\u7684\u7ecf\u9a8cRademacher\u590d\u6742\u5ea6\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u6b63\u5219\u5316\uff08\u5c42\u5f52\u4e00\u5316\u3001\u2113\u2082\u3001\u6743\u91cd\u5f52\u4e00\u5316\uff09\u548c\u57df\u968f\u673a\u5316\u63d0\u5347\u7406\u6027\uff0c\u73af\u5883\u504f\u79fb\u635f\u5bb3\u7406\u6027\u3002", "conclusion": "\u9996\u6b21\u5efa\u7acb\u5f3a\u5316\u5b66\u4e60\u7406\u6027\u5ea6\u91cf\u7684\u5b8c\u6574\u7406\u8bba\u4f53\u7cfb\uff0c\u4e3a\u53ef\u90e8\u7f72\u667a\u80fd\u4f53\u63d0\u4f9b\u98ce\u9669\u91cf\u5316\u5de5\u5177\uff0c\u6307\u5bfc\u901a\u8fc7\u6b63\u5219\u5316\u4e0e\u57df\u968f\u673a\u5316\u7f13\u89e3\u7406\u6027\u98ce\u9669\u95f4\u9699\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.04752", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04752", "abs": "https://arxiv.org/abs/2602.04752", "authors": ["Andrew Lee", "Yonatan Belinkov", "Fernanda Vi\u00e9gas", "Martin Wattenberg"], "title": "Decomposing Query-Key Feature Interactions Using Contrastive Covariances", "comment": null, "summary": "Despite the central role of attention heads in Transformers, we lack tools to understand why a model attends to a particular token. To address this, we study the query-key (QK) space -- the bilinear joint embedding space between queries and keys. We present a contrastive covariance method to decompose the QK space into low-rank, human-interpretable components. It is when features in keys and queries align in these low-rank subspaces that high attention scores are produced. We first study our method both analytically and empirically in a simplified setting. We then apply our method to large language models to identify human-interpretable QK subspaces for categorical semantic features and binding features. Finally, we demonstrate how attention scores can be attributed to our identified features.", "AI": {"tldr": "\u63d0\u51fa\u5bf9\u6bd4\u534f\u65b9\u5dee\u65b9\u6cd5\u5206\u89e3Transformer\u7684QK\u7a7a\u95f4\u4e3a\u53ef\u89e3\u91ca\u4f4e\u79e9\u5b50\u7a7a\u95f4\uff0c\u63ed\u793a\u6ce8\u610f\u529b\u673a\u5236\u901a\u8fc7\u67e5\u8be2\u4e0e\u952e\u5728\u8bed\u4e49\u7279\u5f81\u5b50\u7a7a\u95f4\u7684\u5bf9\u9f50\u4ea7\u751f\u9ad8\u6ce8\u610f\u529b\u5206\u6570\u3002", "motivation": "\u5c3d\u7ba1\u6ce8\u610f\u529b\u673a\u5236\u662fTransformer\u7684\u6838\u5fc3\uff0c\u4f46\u7f3a\u4e4f\u7406\u89e3\u6a21\u578b\u4e3a\u4f55\u5173\u6ce8\u7279\u5b9a\u6807\u8bb0\u7684\u5de5\u5177\uff0c\u9700\u8981\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u6765\u5256\u6790\u6ce8\u610f\u529b\u51b3\u7b56\u7684\u5185\u5728\u903b\u8f91\u3002", "method": "\u7814\u7a76\u67e5\u8be2-\u952e(QK)\u53cc\u7ebf\u6027\u8054\u5408\u5d4c\u5165\u7a7a\u95f4\uff0c\u5f00\u53d1\u5bf9\u6bd4\u534f\u65b9\u5dee\u5206\u89e3\u7b97\u6cd5\u63d0\u53d6\u4f4e\u79e9\u53ef\u89e3\u91ca\u6210\u5206\uff0c\u901a\u8fc7\u7b80\u5316\u7684\u5206\u6790\u6027\u548c\u5b9e\u8bc1\u7814\u7a76\u9a8c\u8bc1\u540e\u5e94\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u8bc6\u522b\u5206\u7c7b\u8bed\u4e49\u7279\u5f81\u548c\u7ed1\u5b9a\u7279\u5f81\u7684QK\u5b50\u7a7a\u95f4\u3002", "result": "\u6210\u529f\u5728LLM\u4e2d\u53d1\u73b0\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684QK\u5b50\u7a7a\u95f4\uff0c\u8bc1\u660e\u6ce8\u610f\u529b\u9ad8\u5206\u6e90\u4e8e\u67e5\u8be2\u4e0e\u952e\u5728\u4f4e\u79e9\u8bed\u4e49\u7279\u5f81\u5b50\u7a7a\u95f4\u7684\u5bf9\u9f50\uff0c\u5e76\u5b9e\u73b0\u6ce8\u610f\u529b\u5206\u6570\u5411\u8fd9\u4e9b\u7279\u5f81\u7684\u5f52\u56e0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7406\u89e3\u6ce8\u610f\u529b\u673a\u5236\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5173\u6ce8\u51b3\u7b56\u7684\u8bed\u4e49\u57fa\u7840\uff0c\u4e3a\u8c03\u8bd5\u548c\u89e3\u91ca\u5927\u8bed\u8a00\u6a21\u578b\u884c\u4e3a\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.04757", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04757", "abs": "https://arxiv.org/abs/2602.04757", "authors": ["Yuchen Ye", "Zixuan Qi", "Shixuan Li", "Wei Qi", "Yanpeng Cai", "Chaoxia Yuan"], "title": "A Dual-TransUNet Deep Learning Framework for Multi-Source Precipitation Merging and Improving Seasonal and Extreme Estimates", "comment": "75 pages,20 figures", "summary": "Multi-source precipitation products (MSPs) from satellite retrievals and reanalysis are widely used for hydroclimatic monitoring, yet spatially heterogeneous biases and limited skill for extremes still constrain their hydrologic utility. Here we develop a dual-stage TransUNet-based multi-source precipitation merging framework (DDL-MSPMF) that integrates six MSPs with four ERA5 near-surface physical predictors. A first-stage classifier estimates daily precipitation occurrence probability, and a second-stage regressor fuses the classifier outputs together with all predictors to estimate daily precipitation amount at 0.25 degree resolution over China for 2001-2020. Benchmarking against multiple deep learning and hybrid baselines shows that the TransUNet - TransUNet configuration yields the best seasonal performance (R = 0.75; RMSE = 2.70 mm/day) and improves robustness relative to a single-regressor setting. For heavy precipitation (>25 mm/day), DDL-MSPMF increases equitable threat scores across most regions of eastern China and better reproduces the spatial pattern of the July 2021 Zhengzhou rainstorm, indicating enhanced extreme-event detection beyond seasonal-mean corrections. Independent evaluation over the Qinghai-Tibet Plateau using TPHiPr further supports its applicability in data-scarce regions. SHAP analysis highlights the importance of precipitation occurrence probabilities and surface pressure, providing physically interpretable diagnostics. The proposed framework offers a scalable and explainable approach for precipitation fusion and extreme-event assessment.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53cc\u9636\u6bb5TransUNet\u6846\u67b6\uff08DDL-MSPMF\uff09\uff0c\u901a\u8fc7\u878d\u5408\u591a\u6e90\u964d\u6c34\u4ea7\u54c1\u548cERA5\u7269\u7406\u9884\u6d4b\u56e0\u5b50\uff0c\u663e\u8457\u63d0\u5347\u4e2d\u56fd\u533a\u57df\u964d\u6c34\u4f30\u8ba1\u7cbe\u5ea6\u4e0e\u6781\u7aef\u4e8b\u4ef6\u68c0\u6d4b\u80fd\u529b", "motivation": "\u591a\u6e90\u536b\u661f\u4e0e\u518d\u5206\u6790\u964d\u6c34\u4ea7\u54c1\u5b58\u5728\u7a7a\u95f4\u5f02\u8d28\u6027\u504f\u5dee\u4e14\u6781\u7aef\u964d\u6c34\u6280\u80fd\u4e0d\u8db3\uff0c\u5236\u7ea6\u4e86\u5176\u5728\u6c34\u6587\u6c14\u5019\u76d1\u6d4b\u4e2d\u7684\u5e94\u7528\u4ef7\u503c", "method": "\u6784\u5efa\u53cc\u9636\u6bb5TransUNet\u6a21\u578b\uff1a\u7b2c\u4e00\u9636\u6bb5\u5206\u7c7b\u5668\u4f30\u8ba1\u65e5\u964d\u6c34\u53d1\u751f\u6982\u7387\uff1b\u7b2c\u4e8c\u9636\u6bb5\u56de\u5f52\u5668\u878d\u5408\u5206\u7c7b\u5668\u8f93\u51fa\u53ca4\u4e2aERA5\u8fd1\u5730\u9762\u7269\u7406\u9884\u6d4b\u56e0\u5b50\uff0c\u751f\u62100.25\u00b0\u5206\u8fa8\u7387\u4e2d\u56fd\u65e5\u964d\u6c34\u6570\u636e\uff082001-2020\uff09", "result": "TransUNet-TransUNet\u914d\u7f6e\u5b63\u8282\u6027\u6027\u80fd\u6700\u4f18\uff08R=0.75\uff1bRMSE=2.70 mm/\u5929\uff09\uff0c\u8f83\u5355\u56de\u5f52\u5668\u66f4\u7a33\u5065\uff1b\u5bf9>25 mm/\u5929\u5f3a\u964d\u6c34\uff0c\u663e\u8457\u63d0\u5347\u4e2d\u56fd\u4e1c\u90e8\u5927\u90e8\u5206\u533a\u57df\u516c\u5e73\u5a01\u80c1\u8bc4\u5206\uff0c\u5e76\u66f4\u51c6\u786e\u590d\u73b02021\u5e74\u90d1\u5dde\"7\u00b720\"\u66b4\u96e8\u7a7a\u95f4\u683c\u5c40\uff1b\u9752\u85cf\u9ad8\u539f\u72ec\u7acb\u9a8c\u8bc1\u8868\u660e\u6a21\u578b\u5728\u7f3a\u8d44\u6599\u533a\u57df\u9002\u7528\u6027\u5f3a", "conclusion": "\u8be5\u6846\u67b6\u517c\u5177\u53ef\u6269\u5c55\u6027\u4e0e\u7269\u7406\u53ef\u89e3\u91ca\u6027\uff08SHAP\u5206\u6790\u63ed\u793a\u964d\u6c34\u53d1\u751f\u6982\u7387\u548c\u5730\u8868\u6c14\u538b\u7684\u5173\u952e\u4f5c\u7528\uff09\uff0c\u4e3a\u964d\u6c34\u878d\u5408\u4e0e\u6781\u7aef\u4e8b\u4ef6\u8bc4\u4f30\u63d0\u4f9b\u65b0\u65b9\u6848"}}
{"id": "2602.04761", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.04761", "abs": "https://arxiv.org/abs/2602.04761", "authors": ["Hang Yu", "Yu-Hu Yan", "Peng Zhao"], "title": "Improved Dimension Dependence for Bandit Convex Optimization with Gradient Variations", "comment": null, "summary": "Gradient-variation online learning has drawn increasing attention due to its deep connections to game theory, optimization, etc. It has been studied extensively in the full-information setting, but is underexplored with bandit feedback. In this work, we focus on gradient variation in Bandit Convex Optimization (BCO) with two-point feedback. By proposing a refined analysis on the non-consecutive gradient variation, a fundamental quantity in gradient variation with bandits, we improve the dimension dependence for both convex and strongly convex functions compared with the best known results (Chiang et al., 2013). Our improved analysis for the non-consecutive gradient variation also implies other favorable problem-dependent guarantees, such as gradient-variance and small-loss regrets. Beyond the two-point setup, we demonstrate the versatility of our technique by achieving the first gradient-variation bound for one-point bandit linear optimization over hyper-rectangular domains. Finally, we validate the effectiveness of our results in more challenging tasks such as dynamic/universal regret minimization and bandit games, establishing the first gradient-variation dynamic and universal regret bounds for two-point BCO and fast convergence rates in bandit games.", "AI": {"tldr": "This paper improves gradient-variation analysis in Bandit Convex Optimization (BCO) with two-point feedback, achieving better dimension dependence for convex/strongly convex functions and extending results to one-point bandit linear optimization, dynamic/universal regret bounds, and bandit games.", "motivation": "Gradient-variation online learning connects deeply to game theory and optimization but remains underexplored under bandit feedback, unlike the well-studied full-information setting.", "method": "Proposes a refined analysis on non-consecutive gradient variation\u2014a fundamental quantity in gradient variation with bandits\u2014to achieve improved theoretical guarantees.", "result": "Improves dimension dependence for convex/strongly convex functions beyond prior work (Chiang et al., 2013); achieves first gradient-variation bounds for one-point bandit linear optimization; establishes first gradient-variation dynamic/universal regret bounds for two-point BCO; demonstrates fast convergence in bandit games.", "conclusion": "The refined analysis yields problem-dependent guarantees including gradient-variance and small-loss regrets, validating effectiveness across challenging tasks like dynamic regret minimization and bandit games."}}
{"id": "2602.04769", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04769", "abs": "https://arxiv.org/abs/2602.04769", "authors": ["Yan Chen", "Jie Peng", "Moajjem Hossain Chowdhury", "Tianlong Chen", "Yunmei Liu"], "title": "NeuroCanvas: VLLM-Powered Robust Seizure Detection by Reformulating Multichannel EEG as Image", "comment": null, "summary": "Accurate and timely seizure detection from Electroencephalography (EEG) is critical for clinical intervention, yet manual review of long-term recordings is labor-intensive. Recent efforts to encode EEG signals into large language models (LLMs) show promise in handling neural signals across diverse patients, but two significant challenges remain: (1) multi-channel heterogeneity, as seizure-relevant information varies substantially across EEG channels, and (2) computing inefficiency, as the EEG signals need to be encoded into a massive number of tokens for the prediction. To address these issues, we draw the EEG signal and propose the novel NeuroCanvas framework. Specifically, NeuroCanvas consists of two modules: (i) The Entropy-guided Channel Selector (ECS) selects the seizure-relevant channels input to LLM and (ii) the following Canvas of Neuron Signal (CNS) converts selected multi-channel heterogeneous EEG signals into structured visual representations. The ECS module alleviates the multi-channel heterogeneity issue, and the CNS uses compact visual tokens to represent the EEG signals that improve the computing efficiency. We evaluate NeuroCanvas across multiple seizure detection datasets, demonstrating a significant improvement of $20\\%$ in F1 score and reductions of $88\\%$ in inference latency. These results highlight NeuroCanvas as a scalable and effective solution for real-time and resource-efficient seizure detection in clinical practice.The code will be released at https://github.com/Yanchen30247/seizure_detect.", "AI": {"tldr": "The paper proposes NeuroCanvas, a framework for EEG-based seizure detection that uses entropy-guided channel selection and visual tokenization to address multi-channel heterogeneity and computational inefficiency in LLM-based approaches, achieving 20% F1 score improvement and 88% latency reduction.", "motivation": "Manual review of long-term EEG recordings for seizure detection is labor-intensive, and while LLM-based approaches show promise, they suffer from two major issues: (1) multi-channel heterogeneity where seizure-relevant information varies across EEG channels, and (2) computational inefficiency due to encoding EEG signals into massive numbers of tokens.", "method": "The NeuroCanvas framework consists of two modules: (i) Entropy-guided Channel Selector (ECS) that selects seizure-relevant channels for LLM input, and (ii) Canvas of Neuron Signal (CNS) that converts selected multi-channel heterogeneous EEG signals into structured visual representations using compact visual tokens.", "result": "Across multiple seizure detection datasets, NeuroCanvas demonstrates a 20% improvement in F1 score and 88% reduction in inference latency compared to existing methods.", "conclusion": "NeuroCanvas provides a scalable and effective solution for real-time and resource-efficient seizure detection in clinical practice, addressing both multi-channel heterogeneity and computational efficiency challenges in LLM-based EEG analysis."}}
{"id": "2602.04775", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04775", "abs": "https://arxiv.org/abs/2602.04775", "authors": ["Yuqi Li", "Matthew M. Engelhard"], "title": "Interval-Based AUC (iAUC): Extending ROC Analysis to Uncertainty-Aware Classification", "comment": null, "summary": "In high-stakes risk prediction, quantifying uncertainty through interval-valued predictions is essential for reliable decision-making. However, standard evaluation tools like the receiver operating characteristic (ROC) curve and the area under the curve (AUC) are designed for point scores and fail to capture the impact of predictive uncertainty on ranking performance. We propose an uncertainty-aware ROC framework specifically for interval-valued predictions, introducing two new measures: $AUC_L$ and $AUC_U$. This framework enables an informative three-region decomposition of the ROC plane, partitioning pairwise rankings into correct, incorrect, and uncertain orderings. This approach naturally supports selective prediction by allowing models to abstain from ranking cases with overlapping intervals, thereby optimizing the trade-off between abstention rate and discriminative reliability. We prove that under valid class-conditional coverage, $AUC_L$ and $AUC_U$ provide formal lower and upper bounds on the theoretical optimal AUC ($AUC^*$), characterizing the physical limit of achievable discrimination. The proposed framework applies broadly to interval-valued prediction models, regardless of the interval construction method. Experiments on real-world benchmark datasets, using bootstrap-based intervals as one instantiation, validate the framework's correctness and demonstrate its practical utility for uncertainty-aware evaluation and decision-making.", "AI": {"tldr": "\u63d0\u51fa\u9002\u7528\u4e8e\u533a\u95f4\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5ROC\u6846\u67b6\uff0c\u5b9a\u4e49$AUC_L$\u548c$AUC_U$\u65b0\u6307\u6807\uff0c\u5b9e\u73b0\u4e09\u533a\u5206\u89e3\u5e76\u652f\u6301\u9009\u62e9\u6027\u9884\u6d4b\uff0c\u5728\u6709\u6548\u8986\u76d6\u6761\u4ef6\u4e0b\u63d0\u4f9b\u7406\u8bba\u6700\u4f18AUC\u7684\u4e0a\u4e0b\u754c\u3002", "motivation": "\u9ad8\u98ce\u9669\u9884\u6d4b\u4e2d\u533a\u95f4\u9884\u6d4b\u5bf9\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6807\u51c6ROC/AUC\u5de5\u5177\u4ec5\u9002\u7528\u4e8e\u70b9\u4f30\u8ba1\uff0c\u65e0\u6cd5\u6355\u6349\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u5bf9\u6392\u5e8f\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e13\u95e8\u9488\u5bf9\u533a\u95f4\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5ROC\u6846\u67b6\uff0c\u5f15\u5165$AUC_L$\u548c$AUC_U$\u4e24\u4e2a\u65b0\u5ea6\u91cf\uff0c\u5b9e\u73b0ROC\u5e73\u9762\u7684\u4fe1\u606f\u6027\u4e09\u533a\u5206\u89e3\uff08\u6b63\u786e/\u9519\u8bef/\u4e0d\u786e\u5b9a\u6392\u5e8f\uff09\uff0c\u5e76\u652f\u6301\u901a\u8fc7\u62d2\u7edd\u91cd\u53e0\u533a\u95f4\u6848\u4f8b\u7684\u9009\u62e9\u6027\u9884\u6d4b\u3002", "result": "\u8bc1\u660e\u5728\u6709\u6548\u7c7b\u6761\u4ef6\u8986\u76d6\u4e0b\uff0c$AUC_L$\u548c$AUC_U$\u4e3a\u7406\u8bba\u6700\u4f18AUC\u63d0\u4f9b\u5f62\u5f0f\u5316\u4e0b\u754c\u548c\u4e0a\u754c\uff0c\u523b\u753b\u5224\u522b\u80fd\u529b\u7269\u7406\u6781\u9650\uff1b\u771f\u5b9e\u6570\u636e\u96c6\u5b9e\u9a8c\u9a8c\u8bc1\u6846\u67b6\u6b63\u786e\u6027\u4e0e\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5e7f\u6cdb\u9002\u7528\u4e8e\u4efb\u610f\u533a\u95f4\u6784\u9020\u65b9\u6cd5\uff0c\u4e3a\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u8bc4\u4f30\u548c\u51b3\u7b56\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2602.04780", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04780", "abs": "https://arxiv.org/abs/2602.04780", "authors": ["Emil Albrychiewicz", "Andr\u00e9s Franco Valiente", "Li-Ching Chen"], "title": "Dynamical Regimes of Multimodal Diffusion Models", "comment": "40 pages, 14 figures", "summary": "Diffusion based generative models have achieved unprecedented fidelity in synthesizing high dimensional data, yet the theoretical mechanisms governing multimodal generation remain poorly understood. Here, we present a theoretical framework for coupled diffusion models, using coupled Ornstein-Uhlenbeck processes as a tractable model. By using the nonequilibrium statistical physics of dynamical phase transitions, we demonstrate that multimodal generation is governed by a spectral hierarchy of interaction timescales rather than simultaneous resolution. A key prediction is the ``synchronization gap'', a temporal window during the reverse generative process where distinct eigenmodes stabilize at different rates, providing a theoretical explanation for common desynchronization artifacts. We derive analytical conditions for speciation and collapse times under both symmetric and anisotropic coupling regimes, establishing strict bounds for coupling strength to avoid unstable symmetry breaking. We show that the coupling strength acts as a spectral filter that enforces a tunable temporal hierarchy on generation. We support these predictions through controlled experiments with diffusion models trained on MNIST datasets and exact score samplers. These results motivate time dependent coupling schedules that target mode specific timescales, offering a potential alternative to ad hoc guidance tuning.", "AI": {"tldr": "This paper develops a theoretical framework for coupled diffusion models using Ornstein-Uhlenbeck processes and nonequilibrium statistical physics, revealing that multimodal generation is controlled by a hierarchy of timescales rather than simultaneous resolution. It identifies a \"synchronization gap\" phenomenon explaining desynchronization artifacts, derives analytical bounds for stable coupling strength, and proposes time-dependent coupling schedules as an alternative to guidance tuning.", "motivation": "Despite diffusion models' success in high-fidelity synthesis, the theoretical mechanisms underlying multimodal generation remain poorly understood, particularly how different modes are coordinated during the generative process. This gap limits our ability to address common artifacts like desynchronization.", "method": "The authors use coupled Ornstein-Uhlenbeck processes as a tractable theoretical model and apply nonequilibrium statistical physics of dynamical phase transitions to analyze the system. They derive analytical conditions and validate predictions through controlled experiments on MNIST datasets using exact score samplers.", "result": "They demonstrate that multimodal generation follows a spectral hierarchy of interaction timescales, not simultaneous resolution. They predict and validate the \"synchronization gap\" where eigenmodes stabilize at different rates, explain desynchronization artifacts, and establish strict bounds on coupling strength to prevent unstable symmetry breaking. Coupling strength acts as a tunable spectral filter.", "conclusion": "The framework reveals fundamental principles of multimodal generation in diffusion models, suggesting that time-dependent coupling schedules targeting mode-specific timescales could provide a principled alternative to ad hoc guidance tuning, potentially improving generation quality and stability."}}
{"id": "2602.04782", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04782", "abs": "https://arxiv.org/abs/2602.04782", "authors": ["Mumin Zhang", "Haochen Zhang", "Xin Zhi Khoo", "Yilin Zhang", "Nuo Chen", "Ting Zhang", "Junjie Tang"], "title": "Legendre Memory Unit with A Multi-Slice Compensation Model for Short-Term Wind Speed Forecasting Based on Wind Farm Cluster Data", "comment": "10 pages, 11 figures,", "summary": "With more wind farms clustered for integration, the short-term wind speed prediction of such wind farm clusters is critical for normal operation of power systems. This paper focuses on achieving accurate, fast, and robust wind speed prediction by full use of cluster data with spatial-temporal correlation. First, weighted mean filtering (WMF) is applied to denoise wind speed data at the single-farm level. The Legendre memory unit (LMU) is then innovatively applied for the wind speed prediction, in combination with the Compensating Parameter based on Kendall rank correlation coefficient (CPK) of wind farm cluster data, to construct the multi-slice LMU (MSLMU). Finally, an innovative ensemble model WMF-CPK-MSLMU is proposed herein, with three key blocks: data pre-processing, forecasting, and multi-slice compensation. Advantages include: 1) LMU jointly models linear and nonlinear dependencies among farms to capture spatial-temporal correlations through backpropagation; 2) MSLMU enhances forecasting by using CPK-derived weights instead of random initialization, allowing spatial correlations to fully activate hidden nodes across clustered wind farms.; 3) CPK adaptively weights the compensation model in MSLMU and complements missing data spatially, to facilitate the whole model highly accurate and robust. Test results on different wind farm clusters indicate the effectiveness and superiority of proposed ensemble model WMF-CPK-MSLMU in the short-term prediction of wind farm clusters compared to the existing models.", "AI": {"tldr": "\u9488\u5bf9\u98ce\u7535\u573a\u96c6\u7fa4\u77ed\u671f\u98ce\u901f\u9884\u6d4b\u96be\u9898\uff0c\u63d0\u51faWMF-CPK-MSLMU\u96c6\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u52a0\u6743\u5e73\u5747\u6ee4\u6ce2\u53bb\u566a\u3001Legendre\u8bb0\u5fc6\u5355\u5143\u5efa\u6a21\u65f6\u7a7a\u76f8\u5173\u6027\uff0c\u5e76\u7ed3\u5408Kendall\u76f8\u5173\u7cfb\u6570\u8865\u507f\u673a\u5236\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u9c81\u68d2\u7684\u9884\u6d4b\u3002", "motivation": "\u968f\u7740\u98ce\u7535\u573a\u96c6\u7fa4\u5316\u53d1\u5c55\uff0c\u5176\u77ed\u671f\u98ce\u901f\u9884\u6d4b\u5bf9\u7535\u529b\u7cfb\u7edf\u5b89\u5168\u8fd0\u884c\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u96c6\u7fa4\u6570\u636e\u7684\u65f6\u7a7a\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u9884\u6d4b\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "1) \u91c7\u7528\u52a0\u6743\u5e73\u5747\u6ee4\u6ce2(WMF)\u5bf9\u5355\u573a\u6570\u636e\u8fdb\u884c\u53bb\u566a\u9884\u5904\u7406\uff1b2) \u521b\u65b0\u6027\u5730\u5c06Legendre\u8bb0\u5fc6\u5355\u5143(LMU)\u4e0e\u57fa\u4e8eKendall\u79e9\u76f8\u5173\u7cfb\u6570\u7684\u8865\u507f\u53c2\u6570(CPK)\u7ed3\u5408\uff0c\u6784\u5efa\u591a\u5207\u7247LMU(MSLMU)\uff1b3) \u8bbe\u8ba1\u5305\u542b\u6570\u636e\u9884\u5904\u7406\u3001\u9884\u6d4b\u548c\u591a\u5207\u7247\u8865\u507f\u4e09\u6a21\u5757\u7684WMF-CPK-MSLMU\u96c6\u6210\u6a21\u578b\uff0c\u7528CPK\u6743\u91cd\u66ff\u4ee3\u968f\u673a\u521d\u59cb\u5316\u4ee5\u6fc0\u6d3b\u9690\u85cf\u8282\u70b9\u3002", "result": "\u5728\u4e0d\u540c\u98ce\u7535\u573a\u96c6\u7fa4\u4e0a\u7684\u6d4b\u8bd5\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u6a21\u578b\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u77ed\u671f\u98ce\u901f\u9884\u6d4b\u7cbe\u5ea6\u3001\u901f\u5ea6\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5747\u8868\u73b0\u51fa\u663e\u8457\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u6a21\u578b\u901a\u8fc7\u81ea\u9002\u5e94\u65f6\u7a7a\u76f8\u5173\u6027\u5efa\u6a21\u548c\u8865\u507f\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u98ce\u7535\u573a\u96c6\u7fa4\u77ed\u671f\u98ce\u901f\u9884\u6d4b\u6027\u80fd\uff0c\u4e3a\u7535\u529b\u7cfb\u7edf\u5b89\u5168\u7a33\u5b9a\u8fd0\u884c\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6280\u672f\u652f\u6491\u3002"}}
{"id": "2602.04784", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04784", "abs": "https://arxiv.org/abs/2602.04784", "authors": ["Kieran A. Murphy"], "title": "From independent patches to coordinated attention: Controlling information flow in vision transformers", "comment": "Code at https://github.com/murphyka/vit_ib", "summary": "We make the information transmitted by attention an explicit, measurable quantity in vision transformers. By inserting variational information bottlenecks on all attention-mediated writes to the residual stream -- without other architectural changes -- we train models with an explicit information cost and obtain a controllable spectrum from independent patch processing to fully expressive global attention. On ImageNet-100, we characterize how classification behavior and information routing evolve across this spectrum, and provide initial insights into how global visual representations emerge from local patch processing by analyzing the first attention heads that transmit information. By biasing learning toward solutions with constrained internal communication, our approach yields models that are more tractable for mechanistic analysis and more amenable to control.", "AI": {"tldr": "\u5728\u89c6\u89c9Transformer\u4e2d\u5f15\u5165\u53d8\u5206\u4fe1\u606f\u74f6\u9888\uff0c\u4f7f\u6ce8\u610f\u529b\u4fe1\u606f\u4f20\u8f93\u53ef\u6d4b\u91cf\u53ef\u63a7\uff0c\u63ed\u793a\u4e86\u5168\u5c40\u89c6\u89c9\u8868\u5f81\u4ece\u5c40\u90e8\u5757\u5904\u7406\u4e2d\u4ea7\u751f\u7684\u673a\u5236\uff0c\u5f97\u5230\u66f4\u6613\u5206\u6790\u548c\u63a7\u5236\u7684\u6a21\u578b\u3002", "motivation": "\u4f7f\u89c6\u89c9Transformer\u4e2d\u6ce8\u610f\u529b\u4f20\u8f93\u7684\u4fe1\u606f\u6210\u4e3a\u660e\u786e\u53ef\u6d4b\u91cf\u7684\u91cf\uff0c\u7406\u89e3\u5168\u5c40\u89c6\u89c9\u8868\u5f81\u5982\u4f55\u4ece\u5c40\u90e8\u5757\u5904\u7406\u4e2d\u4ea7\u751f\uff0c\u5e76\u83b7\u5f97\u66f4\u6613\u4e8e\u673a\u5236\u5206\u6790\u548c\u63a7\u5236\u7684\u6a21\u578b\u3002", "method": "\u5728\u6240\u6709\u6ce8\u610f\u529b\u4ecb\u5bfc\u7684\u6b8b\u5dee\u6d41\u5199\u5165\u5904\u63d2\u5165\u53d8\u5206\u4fe1\u606f\u74f6\u9888\uff08\u65e0\u9700\u5176\u4ed6\u67b6\u6784\u6539\u52a8\uff09\uff0c\u8bad\u7ec3\u5e26\u660e\u786e\u4fe1\u606f\u6210\u672c\u7684\u6a21\u578b\uff0c\u5b9e\u73b0\u4ece\u72ec\u7acb\u5757\u5904\u7406\u5230\u5168\u5c40\u6ce8\u610f\u529b\u7684\u53ef\u63a7\u9891\u8c31\u3002", "result": "\u5728ImageNet-100\u4e0a\u523b\u753b\u4e86\u5206\u7c7b\u884c\u4e3a\u548c\u4fe1\u606f\u8def\u7531\u5728\u8be5\u9891\u8c31\u4e0a\u7684\u6f14\u53d8\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u9996\u6279\u4f20\u8f93\u4fe1\u606f\u7684\u6ce8\u610f\u529b\u5934\uff0c\u521d\u6b65\u63ed\u793a\u4e86\u5168\u5c40\u89c6\u89c9\u8868\u5f81\u4ece\u5c40\u90e8\u5757\u5904\u7406\u4e2d\u4ea7\u751f\u7684\u8fc7\u7a0b\u3002", "conclusion": "\u901a\u8fc7\u504f\u5411\u7ea6\u675f\u5185\u90e8\u901a\u4fe1\u7684\u5b66\u4e60\uff0c\u8be5\u65b9\u6cd5\u4ea7\u751f\u7684\u6a21\u578b\u66f4\u6613\u4e8e\u673a\u5236\u5206\u6790\u4e14\u66f4\u53ef\u63a7\u3002"}}
{"id": "2602.04795", "categories": ["cs.LG", "eess.SP", "math.NA", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.04795", "abs": "https://arxiv.org/abs/2602.04795", "authors": ["Olivier Vu Thanh", "Nicolas Gillis"], "title": "Maximum-Volume Nonnegative Matrix Factorization", "comment": "arXiv admin note: substantial text overlap with arXiv:2412.06380", "summary": "Nonnegative matrix factorization (NMF) is a popular data embedding technique. Given a nonnegative data matrix $X$, it aims at finding two lower dimensional matrices, $W$ and $H$, such that $X\\approx WH$, where the factors $W$ and $H$ are constrained to be element-wise nonnegative. The factor $W$ serves as a basis for the columns of $X$. In order to obtain more interpretable and unique solutions, minimum-volume NMF (MinVol NMF) minimizes the volume of $W$. In this paper, we consider the dual approach, where the volume of $H$ is maximized instead; this is referred to as maximum-volume NMF (MaxVol NMF). MaxVol NMF is identifiable under the same conditions as MinVol NMF in the noiseless case, but it behaves rather differently in the presence of noise. In practice, MaxVol NMF is much more effective to extract a sparse decomposition and does not generate rank-deficient solutions. In fact, we prove that the solutions of MaxVol NMF with the largest volume correspond to clustering the columns of $X$ in disjoint clusters, while the solutions of MinVol NMF with smallest volume are rank deficient. We propose two algorithms to solve MaxVol NMF. We also present a normalized variant of MaxVol NMF that exhibits better performance than MinVol NMF and MaxVol NMF, and can be interpreted as a continuum between standard NMF and orthogonal NMF. We illustrate our results in the context of hyperspectral unmixing.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.04807", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04807", "abs": "https://arxiv.org/abs/2602.04807", "authors": ["Wolfgang Maass", "Sabine Janzen", "Prajvi Saxena", "Sach Mukherjee"], "title": "Evolving Afferent Architectures: Biologically-inspired Models for Damage-Avoidance Learning", "comment": "16 pages, 6 figures", "summary": "We introduce Afferent Learning, a framework that produces Computational Afferent Traces (CATs) as adaptive, internal risk signals for damage-avoidance learning. Inspired by biological systems, the framework uses a two-level architecture: evolutionary optimization (outer loop) discovers afferent sensing architectures that enable effective policy learning, while reinforcement learning (inner loop) trains damage-avoidance policies using these signals. This formalizes afferent sensing as providing an inductive bias for efficient learning: architectures are selected based on their ability to enable effective learning (rather than directly minimizing damage). We provide theoretical convergence guarantees under smoothness and bounded-noise assumptions. We illustrate the general approach in the challenging context of biomechanical digital twins operating over long time horizons (multiple decades of the life-course). Here, we find that CAT-based evolved architectures achieve significantly higher efficiency and better age-robustness than hand-designed baselines, enabling policies that exhibit age-dependent behavioral adaptation (23% reduction in high-risk actions). Ablation studies validate CAT signals, evolution, and predictive discrepancy as essential. We release code and data for reproducibility.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u4f20\u5165\u5b66\u4e60\"\u6846\u67b6\uff0c\u901a\u8fc7\u8fdb\u5316\u4f18\u5316\u548c\u5f3a\u5316\u5b66\u4e60\u53cc\u5faa\u73af\u751f\u6210\u81ea\u9002\u5e94\u98ce\u9669\u4fe1\u53f7(CATs)\uff0c\u5728\u751f\u7269\u529b\u5b66\u6570\u5b57\u5b6a\u751f\u4e2d\u5b9e\u73b023%\u7684\u9ad8\u98ce\u9669\u884c\u4e3a\u51cf\u5c11\u548c\u66f4\u597d\u7684\u5e74\u9f84\u9c81\u68d2\u6027\u3002", "motivation": "\u53d7\u751f\u7269\u7cfb\u7edf\u542f\u53d1\uff0c\u751f\u7269\u4f53\u5229\u7528\u4f20\u5165\u611f\u77e5\u8fdb\u884c\u635f\u4f24\u89c4\u907f\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u671f\u3001\u5e74\u9f84\u76f8\u5173\u7684\u635f\u4f24\u89c4\u907f\u65b9\u9762\u6548\u679c\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u5c06\u4f20\u5165\u611f\u77e5\u5f62\u5f0f\u5316\u4e3a\u5b66\u4e60\u5f52\u7eb3\u504f\u7f6e\u7684\u6846\u67b6\uff0c\u96be\u4ee5\u5b9e\u73b0\u9002\u5e94\u6027\u5185\u90e8\u98ce\u9669\u4fe1\u53f7\u800c\u975e\u7b80\u5355\u5916\u90e8\u5956\u52b1\u6700\u5c0f\u5316\u3002", "method": "\u63d0\u51fa\u4f20\u5165\u5b66\u4e60\u6846\u67b6\uff0c\u751f\u6210\u8ba1\u7b97\u4f20\u5165\u8f68\u8ff9(CATs)\u4f5c\u4e3a\u81ea\u9002\u5e94\u5185\u90e8\u98ce\u9669\u4fe1\u53f7\u3002\u91c7\u7528\u53cc\u5c42\u67b6\u6784\uff1a\u5916\u5faa\u73af\u8fdb\u5316\u4f18\u5316\u53d1\u73b0\u6709\u6548\u4f20\u611f\u67b6\u6784\uff0c\u5185\u5faa\u73af\u5f3a\u5316\u5b66\u4e60\u5229\u7528CATs\u8bad\u7ec3\u635f\u4f24\u89c4\u907f\u7b56\u7565\u3002\u5e94\u7528\u4e8e\u591a\u5e74\u751f\u7269\u529b\u5b66\u6570\u5b57\u5b6a\u751f\uff0c\u63d0\u4f9b\u7406\u8bba\u6536\u655b\u4fdd\u8bc1\u3002", "result": "CAT\u8fdb\u5316\u67b6\u6784\u663e\u8457\u4f18\u4e8e\u624b\u5de5\u8bbe\u8ba1\u57fa\u7ebf\uff0c\u9ad8\u98ce\u9669\u884c\u4e3a\u51cf\u5c1123%\uff0c\u5177\u6709\u66f4\u597d\u7684\u5e74\u9f84\u9c81\u68d2\u6027\uff0c\u7b56\u7565\u5c55\u73b0\u5e74\u9f84\u4f9d\u8d56\u7684\u884c\u4e3a\u9002\u5e94\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9eCAT\u4fe1\u53f7\u3001\u8fdb\u5316\u548c\u9884\u6d4b\u5dee\u5f02\u90fd\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u4f20\u5165\u5b66\u4e60\u4e3a\u635f\u4f24\u89c4\u907f\u5b66\u4e60\u63d0\u4f9b\u4e86\u901a\u7528\u6846\u67b6\uff0c\u4f20\u5165\u67b6\u6784\u8fdb\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u7279\u522b\u9002\u7528\u4e8e\u957f\u671f\u5e74\u9f84\u53d8\u5316\u573a\u666f\uff0c\u4e3a\u751f\u7269\u529b\u5b66\u6570\u5b57\u5b6a\u751f\u7b49\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.04852", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04852", "abs": "https://arxiv.org/abs/2602.04852", "authors": ["Philipp Nazari", "T. Konstantin Rusch"], "title": "The Key to State Reduction in Linear Attention: A Rank-based Perspective", "comment": null, "summary": "Linear attention offers a computationally efficient yet expressive alternative to softmax attention. However, recent empirical results indicate that the state of trained linear attention models often exhibits a low-rank structure, suggesting that these models underexploit their capacity in practice. To illuminate this phenomenon, we provide a theoretical analysis of the role of rank in linear attention, revealing that low effective rank can affect retrieval error by amplifying query noise. In addition to these theoretical insights, we conjecture that the low-rank states can be substantially reduced post-training with only minimal performance degradation, yielding faster and more memory-efficient models. To this end, we propose a novel hardware-aware approach that structurally prunes key and query matrices, reducing the state size while retaining compatibility with existing CUDA kernels. We adapt several existing pruning strategies to fit our framework and, building on our theoretical analysis, propose a novel structured pruning method based on a rank-revealing QR decomposition. Our empirical results, evaluated across models of varying sizes and on various downstream tasks, demonstrate the effectiveness of our state reduction framework. We highlight that our framework enables the removal of 50% of the query and key channels at only a marginal increase in perplexity. The code for this project can be found at https://github.com/camail-official/LinearAttentionPruning.", "AI": {"tldr": "\u9488\u5bf9\u7ebf\u6027\u6ce8\u610f\u529b\u6a21\u578b\u8bad\u7ec3\u540e\u72b6\u6001\u5448\u73b0\u4f4e\u79e9\u7ed3\u6784\u3001\u5bb9\u91cf\u5229\u7528\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u672c\u6587\u7406\u8bba\u5206\u6790\u4e86\u79e9\u5728\u68c0\u7d22\u8bef\u5dee\u4e2d\u7684\u4f5c\u7528\uff0c\u63d0\u51fa\u4e00\u79cd\u786c\u4ef6\u611f\u77e5\u7684\u7ed3\u6784\u5316\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7QR\u5206\u89e3\u7b49\u65b9\u6cd5\u538b\u7f29\u952e\u548c\u67e5\u8be2\u77e9\u9635\uff0c\u5728\u51e0\u4e4e\u4e0d\u589e\u52a0\u56f0\u60d1\u5ea6\u7684\u524d\u63d0\u4e0b\u53ef\u79fb\u966450%\u7684\u901a\u9053\uff0c\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u3002", "motivation": "\u7ebf\u6027\u6ce8\u610f\u529b\u867d\u8ba1\u7b97\u9ad8\u6548\uff0c\u4f46\u5b9e\u8bc1\u663e\u793a\u5176\u8bad\u7ec3\u540e\u72b6\u6001\u5e38\u5448\u4f4e\u79e9\u7ed3\u6784\uff0c\u8868\u660e\u6a21\u578b\u5b9e\u9645\u5bb9\u91cf\u672a\u88ab\u5145\u5206\u5229\u7528\u3002\u4f4e\u79e9\u72b6\u6001\u5f71\u54cd\u68c0\u7d22\u6027\u80fd\u4e14\u9020\u6210\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\uff0c\u4e9f\u9700\u7406\u8bba\u89e3\u91ca\u4e0e\u5b9e\u8df5\u4f18\u5316\u65b9\u6848\u3002", "method": "1) \u7406\u8bba\u5206\u6790\u79e9\u5bf9\u7ebf\u6027\u6ce8\u610f\u529b\u67e5\u8be2\u566a\u58f0\u653e\u5927\u53ca\u68c0\u7d22\u8bef\u5dee\u7684\u5f71\u54cd\u673a\u5236\uff1b2) \u63d0\u51fa\u786c\u4ef6\u611f\u77e5\u7684\u7ed3\u6784\u5316\u526a\u679d\u6846\u67b6\uff0c\u9002\u914d\u73b0\u6709CUDA\u5185\u6838\uff1b3) \u6539\u9020\u73b0\u6709\u526a\u679d\u7b56\u7565\u5e76\u57fa\u4e8e\u79e9\u63ed\u793aQR\u5206\u89e3\u63d0\u51fa\u65b0\u578b\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u79cd\u89c4\u6a21\u6a21\u578b\u548c\u4e0b\u6e38\u4efb\u52a1\u4e0a\u9a8c\u8bc1\uff0c\u6846\u67b6\u80fd\u6709\u6548\u538b\u7f29\u72b6\u6001\u89c4\u6a21\u3002\u5173\u952e\u53d1\u73b0\uff1a\u79fb\u966450%\u7684\u67e5\u8be2\u548c\u952e\u901a\u9053\u4ec5\u5bfc\u81f4\u56f0\u60d1\u5ea6\u8f7b\u5fae\u4e0a\u5347\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u548c\u5185\u5b58\u6548\u7387\u3002", "conclusion": "\u7ebf\u6027\u6ce8\u610f\u529b\u7684\u4f4e\u79e9\u73b0\u8c61\u53ef\u901a\u8fc7\u540e\u8bad\u7ec3\u526a\u679d\u5927\u5e45\u7f13\u89e3\uff0c\u6240\u63d0\u6846\u67b6\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u6a21\u578b\u8f7b\u91cf\u5316\uff0c\u4e3a\u9ad8\u6548Transformer\u90e8\u7f72\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2602.04870", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04870", "abs": "https://arxiv.org/abs/2602.04870", "authors": ["Chenwei Cui", "Rockwell Jackson", "Benjamin Joseph Herrera", "Ana Mar\u00eda T\u00e1rano", "Hannah Kerner"], "title": "Multi-Head LatentMoE and Head Parallel: Communication-Efficient and Deterministic MoE Parallelism", "comment": null, "summary": "Large language models have transformed many applications but remain expensive to train. Sparse Mixture of Experts (MoE) addresses this through conditional computation, with Expert Parallel (EP) as the standard distributed training method. However, EP has three limitations: communication cost grows linearly with the number of activated experts $k$, load imbalance affects latency and memory usage, and data-dependent communication requires metadata exchange. We propose Multi-Head LatentMoE and Head Parallel (HP), a new architecture and parallelism achieving $O(1)$ communication cost regardless of $k$, completely balanced traffic, and deterministic communication, all while remaining compatible with EP. To accelerate Multi-Head LatentMoE, we propose IO-aware routing and expert computation. Compared to MoE with EP, Multi-Head LatentMoE with HP trains up to $1.61\\times$ faster while having identical performance. With doubled granularity, it achieves higher overall performance while still being $1.11\\times$ faster. Our method makes multi-billion-parameter foundation model research more accessible.", "AI": {"tldr": "The paper proposes Multi-Head LatentMoE with Head Parallel (HP) to replace Expert Parallel (EP) for training sparse mixture of experts models, achieving constant communication cost, perfect load balancing, and up to 1.61x faster training while maintaining identical performance.", "motivation": "Large language models are expensive to train, and the standard Expert Parallel (EP) method for Sparse Mixture of Experts (MoE) has three key limitations: communication cost grows linearly with activated experts k, load imbalance issues, and data-dependent communication requiring metadata exchange.", "method": "The authors propose Multi-Head LatentMoE combined with Head Parallel (HP) strategy, plus IO-aware routing and expert computation optimizations to accelerate training.", "result": "Multi-Head LatentMoE with HP achieves O(1) communication cost (independent of k), perfectly balanced traffic, and deterministic communication. It trains up to 1.61x faster than MoE with EP while maintaining identical performance. With doubled granularity, it achieves higher performance while still being 1.11x faster.", "conclusion": "The proposed method makes multi-billion-parameter foundation model research more accessible by providing a more efficient and scalable training approach for MoE models."}}
