<div id=toc></div>

# Table of Contents

- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 5]
- [quant-ph](#quant-ph) [Total: 40]
- [cs.LG](#cs.LG) [Total: 69]
- [cs.AI](#cs.AI) [Total: 11]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 4]
- [nlin.CD](#nlin.CD) [Total: 2]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 6]


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [1] [Kappa Entropy and its Thermodynamic Connection](https://arxiv.org/abs/2512.03075)
*J. A. S. Lima,M. H. Benetti*

Main category: cond-mat.stat-mech

TL;DR: 提出了一种新的双参数非加性熵S_{κℓ}，与Kappa型幂律速度分布F_{κℓ}(v)相关。通过扩展的Neo-Boltzmannian微观状态计数和标准平均技术，证明在ℓ=-5/2时，无论κ参数取何值，热力学基本定律在该广义幂律框架下都得以保持。


<details>
  <summary>Details</summary>
Motivation: 从自下而上的角度出发，研究非加性熵与Kappa型幂律速度分布的关系，探索在广义幂律框架下保持热力学基本定律的条件。

Method: 提出双参数非加性熵S_{κℓ}，采用扩展的Neo-Boltzmannian微观状态计数程序，运用标准平均技术进行分析。

Result: 证明只有当ℓ=-5/2时，无论κ参数取何值，热力学基本定律在该广义幂律框架下都能得到保持。

Conclusion: 在Kappa型幂律速度分布的广义框架中，ℓ参数必须取-5/2这一特定值才能确保热力学定律的一致性，而κ参数可以自由变化。

Abstract: Adopting a bottom-up perspective, we propose a novel two-parametric nonadditive entropy, $S_{κ\ell}$, associated with a Kappa-type power-law velocity distribution, $F_{κ\ell}(v)$, recently derived in the literature. By formulating an extended Neo-Boltzmannian microstate counting procedure and employing standard averaging techniques, we demonstrate that the fundamental laws of thermodynamics are preserved within this generalized power-law framework only whether $\ell=-5/2$, regardless of the values assumed by the $κ$-parameter.

</details>


### [2] [Symmetry Breaking of Current Response in Disordered Exclusion Processes](https://arxiv.org/abs/2512.03316)
*Issei Sakai,Takuma Akimoto*

Main category: cond-mat.stat-mech

TL;DR: 本文研究了偏置反转对称性在无序系统中的表现，建立了该对称性成立的一般判据，揭示了键无序和点无序对对称性的不同影响。


<details>
  <summary>Details</summary>
Motivation: 偏置反转对称性是非平衡输运的重要特征，但在无序系统中，无序与粒子相互作用如何影响这一对称性尚不清楚。研究旨在理解环境无序和粒子间相互作用如何协同产生非对称输运。

Method: 建立了偏置反转对称性成立的一般判据：当且仅当局部左右键偏置比在空间上均匀时，该对称性成立。通过解析和数值分析，分别研究了键无序和点无序对对称性的影响。

Result: 键无序在线性响应之外仍保持对称性，而点无序通过异质性与粒子相互作用的相互作用破坏了对称性。研究结果揭示了无序和粒子相互作用如何协同产生非对称输运。

Conclusion: 研究为理解生物和人工纳米通道中的输运现象提供了统一的理论框架，阐明了环境无序和粒子间相互作用在产生非对称输运中的协同作用。

Abstract: The bias-reversal symmetry -- where reversing an external bias inverts the current without changing its magnitude -- is a hallmark of nonequilibrium transport. While this property holds in homogeneous systems such as the asymmetric simple exclusion process, how disorder and its interplay with particle interactions affect this symmetry has remained unclear. Here, we establish a general criterion showing that the bias-reversal symmetry holds if and only if the local left-right bond-bias ratio is spatially uniform. Analytical and numerical analyses reveal that bond disorder preserves the symmetry beyond linear response, whereas site disorder breaks it through an interplay between heterogeneity and particle interactions. Our results demonstrate how environmental disorder and interparticle interactions cooperate to generate asymmetric transport, thereby providing a unified theoretical framework relevant to transport through biological and artificial nanochannels.

</details>


### [3] [Tensor renormalization group calculations of partition-function ratios](https://arxiv.org/abs/2512.03395)
*Satoshi Morita,Naoki Kawashima*

Main category: cond-mat.stat-mech

TL;DR: 研究通过配分函数比值分析相变和临界现象，利用张量重整化群计算三种二维模型，验证了CFT预测的普适值


<details>
  <summary>Details</summary>
Motivation: 研究配分函数比值在临界点处的普适行为，验证共形场论对相变临界现象的预测能力

Method: 使用键加权张量重整化群方法，对三种二维模型（Ising模型、三态Potts模型、四态Potts模型）进行数值计算，分析配分函数比值的有限尺寸标度行为

Result: 配分函数比值遵循与Binder参数相同的有限尺寸标度形式，其临界值与CFT预测的普适值吻合良好；在四态Potts模型中观察到这些比值的系统尺寸依赖存在对数修正

Conclusion: 配分函数比值是研究相变临界现象的有效工具，其普适行为与共形场论预测一致，为理解不同普适类模型的临界性质提供了数值验证

Abstract: The behavior of dimensionless quantities defined as ratios of partition functions is analyzed to investigate phase transitions and critical phenomena. At criticality, the universal values of these ratios can be predicted from conformal field theory (CFT) through the modular-invariant partition functions on a torus. We perform numerical calculations using the bond-weighted tensor renormalization group for three two-dimensional models belonging to different universality classes: the Ising model, the three-state Potts model, and the four-state Potts model. The partition-function ratios obey the same finite-size scaling form as the Binder parameter, and their critical values agree well with the universal values predicted by CFT. In the four-state Potts model, we observe logarithmic corrections in the system-size dependence of these ratios.

</details>


### [4] [Classification of diffusion processes in dimension $d$ via the Carleman approach with applications to models involving additive, multiplicative or square-root noises](https://arxiv.org/abs/2512.03857)
*Cecile Monthus*

Main category: cond-mat.stat-mech

TL;DR: Carleman方法应用于随机微分方程系统，将多项式力场和扩散矩阵的d维随机系统转化为无限维线性系统，用于计算各阶矩的动力学演化。


<details>
  <summary>Details</summary>
Motivation: 将确定性经典动力学中著名的Carleman方法扩展到随机系统，为多项式力场和扩散矩阵的随机微分方程提供统一的矩演化分析框架。

Method: 将d维随机微分方程系统转化为无限维线性系统（Carleman矩阵），通过全局度n=n₁+...+n₄的块分解分析谱分解特性，应用于单噪声和多噪声模型。

Result: 在d=1维中，几何布朗运动的Carleman矩阵是对角矩阵，Pearson扩散族（包含Ornstein-Uhlenbeck等过程）是下三角矩阵；在d=2维中，Carleman矩阵按全局度分块，识别出块对角、块下三角和块上三角的最简模型。

Conclusion: Carleman方法为随机微分方程的矩演化分析提供了系统框架，通过矩阵结构分类揭示了不同随机过程的本质特性，特别适用于具有多项式系数的系统。

Abstract: The Carleman approach is well-known in the field of deterministic classical dynamics as a method to replace a finite number $d$ of non-linear differential equations by an infinite-dimensional linear system. Here this approach is applied to a system of $d$ stochastic differential equations for $[x_1(t),..,x_d(t)]$ when the forces and the diffusion-matrix elements are polynomials, in order to write the linear system governing the dynamics of the averaged values ${\mathbb E} ( x_1^{n_1}(t) x_2^{n_2}(t) ... x_d^{n_d}(t) )$ labelled by the $d$ integers $(n_1,..,n_d)$. The natural decomposition of the Carleman matrix into blocks associated to the global degree $n=n_1+n_2+..+n_d$ is useful to identify the models that have the simplest spectral decompositions in the bi-orthogonal basis of right and left eigenvectors. This analysis is then applied to models with a single noise per coordinate, that can be either additive or multiplicative or square-root, or with two types of noises per coordinate, with many examples in dimensions $d=1,2$. In $d=1$, the Carleman matrix governing the dynamics of the moments ${\mathbb E} ( x^{n}(t) )$ is diagonal for the Geometric Brownian motion, while it is lower-triangular for the family of Pearson diffusions containing the Ornstein-Uhlenbeck and the Square-Root processes, as well as the Kesten, the Fisher-Snedecor and the Student processes that converge towards steady states with power-law-tails. In dimension $d=2$, the Carleman matrix governing the dynamics of the correlations ${\mathbb E} ( x_1^{n_1}(t) x_2^{n_2}(t) )$ has a natural decomposition into blocks associated to the global degree $n=n_1+n_2$, and we discuss the simplest models where the Carleman matrix is either block-diagonal or block-lower-triangular or block-upper-triangular.

</details>


### [5] [Collective dynamics of trail-interacting particles](https://arxiv.org/abs/2512.03950)
*Paul Pineau,Samuel Bell,Raphaël Voituriez,Ram M. Adar*

Main category: cond-mat.stat-mech

TL;DR: 本文提出了一个最小化的多粒子轨迹相互作用模型，通过随机密度泛函理论推导出涨落流体动力学方程，揭示了记忆场与涨落耦合如何重塑集体动力学，在排斥情况下产生超扩散传播，在吸引情况下导致有限时间凝聚。


<details>
  <summary>Details</summary>
Motivation: 轨迹相互作用在自然界中普遍存在，但现有研究仅限于单粒子水平或唯象平均场理论。本文旨在将这一范式扩展到涨落集体水平，建立对多粒子轨迹相互作用系统的理解。

Method: 引入一个最小化的多粒子轨迹相互作用模型，粒子在扩散时沉积长寿命的排斥/吸引轨迹，形成共享记忆场。使用随机密度泛函理论推导涨落流体动力学方程，并通过解析和数值方法分析结果行为。

Result: 在排斥情况下，粒子密度显示超扩散传播特征，伴随瞬态聚类和弹道运动；在吸引情况下，系统在有限时间内凝聚成冻结的局域化状态。记忆与涨落的耦合从根本上重塑了集体动力学。

Conclusion: 研究结果为轨迹相互作用系统建立了通用原理，揭示了持久场如何产生新的不稳定性和自组织现象，将轨迹相互作用范式扩展到涨落集体水平。

Abstract: Trail interactions occur when past particle trajectories bias future motion, rendering the system out of thermodynamic equilibrium. While such systems are abundant in nature, their understanding is limited to the single-particle level or phenomenological mean-field theories. Here, we introduce a minimal model of many trail-interacting particles that extends this paradigm to the fluctuating collective level. Particles diffuse while depositing long-lasting repelling/attracting trails that act as a shared memory field, coupling their dynamics across time and space. Using stochastic density functional theory, we derive fluctuating hydrodynamic equations and analyze analytically and numerically the resulting behaviors. We show that memory, coupled with fluctuations, fundamentally reshapes collective dynamics; In the repulsive case, the particle density displays superdiffusive spreading characterized by transient clustering and ballistic motion; In the attractive case, the system condensates in finite time into frozen, localized states. Our results establish general principles for trail-interacting systems and reveal how persistent fields generate novel instabilities and self-organization.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [6] [New Identity for Cayley's First Hyperdeterminant with Applications to Symmetric Tensors and Entanglement](https://arxiv.org/abs/2512.03093)
*Isaac Dobes*

Main category: quant-ph

TL;DR: 本文提出了计算Cayley第一超行列式的新公式，并证明该公式可用于多项式时间内计算对称超矩阵的超行列式，最后讨论了在玻色子量子纠缠中的应用。


<details>
  <summary>Details</summary>
Motivation: 需要更高效的方法来计算对称超矩阵的超行列式，特别是在量子纠缠等物理应用中的需求。

Method: 1. 使用Levi-Civita符号给出了计算Cayley第一超行列式的新公式；2. 定义了超矩阵的消元和复制矩阵的推广形式；3. 在附录中推导了这些矩阵的显式公式。

Result: 1. 获得了计算超行列式的新公式；2. 实现了对称超矩阵超行列式的多项式时间计算（在固定边长条件下）；3. 成功应用于玻色子量子纠缠分析。

Conclusion: 本文提出的新公式和计算方法显著提高了对称超矩阵超行列式的计算效率，为量子纠缠等物理应用提供了有效的数学工具。

Abstract: In this article, a new formula for computing Cayley's first hyperdeterminant in terms of the Levi-Civita symbol is given. It is then shown that this formula can be used to compute the hyperdeterminant of symmetric hypermatrices in polynomial time with respect to their order (assuming fixed side length). Applications to the quantum entanglement of bosons are then discussed. Additionally, in order to obtain the fast calculation of the hyperdeterminant on symmetric hypermatrices, hypermatrix generalizations of elimination and duplication matrices are defined, and explicit formulas for them are derived in the appendix of this article.

</details>


### [7] [Generating redundantly encoded resource states for photonic quantum computing](https://arxiv.org/abs/2512.03131)
*Samuel J. Sheldon,Pieter Kok*

Main category: quant-ph

TL;DR: 该论文提出了一种使用单量子发射器确定性生成冗余编码光子资源态的方法，用于提升基于测量的量子计算中光子融合的成功率。


<details>
  <summary>Details</summary>
Motivation: 基于测量的量子计算需要大规模纠缠簇态作为通用资源，但构建这些态通常需要融合许多由量子发射器生成的小型资源态。标准的光子融合过程具有50%的成功概率，限制了效率。最近的研究表明，通过使用GHZ态对线性图态的顶点进行冗余编码，可以在低损耗极限下将融合成功率提升至接近100%。

Method: 提出了一种使用单量子发射器确定性生成冗余编码光子资源态的协议。研究了协议错误和光子损耗对生成的资源态以及II型光子融合的影响。

Result: 该工作为高效构建复杂纠缠光子量子比特态提供了一条途径，可用于光子量子计算和量子中继器。

Conclusion: 通过冗余编码和单量子发射器确定性生成资源态的方法，能够显著提升光子融合的成功率，为基于测量的量子计算提供了更高效的资源态构建方案。

Abstract: Measurement-based quantum computing relies on the generation of large entangled cluster states that act as a universal resource on which logical circuits can be imprinted and executed through local measurements. A number of strategies for constructing sufficiently large photonic cluster states propose fusing many smaller resource states generated by a series of quantum emitters. However, the fusion process is inherently probabilistic with a 50% success probability in standard guise. A recent proposal has shown that, in the limit of low loss, the probability of achieving successful fusion may be boosted to near unity by redundantly encoding the vertices of linear graph states using Greenberger-Horne-Zeilinger states [Quantum 7, 992 (2023)]. Here we present a protocol for deterministically generating redundantly encoded photonic resource states using single quantum emitters, and study the impact of protocol errors and photonic losses on the generated resource states and type-II photonic fusion. Our work provides a route for efficiently constructing complex entangled photonic qubit states for photonic quantum computing and quantum repeaters.

</details>


### [8] [Polylogarithmic-Depth Quantum Algorithm for Simulating the Extended Hubbard Model on a Two-Dimensional Lattice Using the Fast Multipole Method](https://arxiv.org/abs/2512.03898)
*Yu Wang,Martina Nibbi,Maxine Luo,Isabel Nha Minh Le,Yanbin Chen,J. Ignacio Cirac,Christian Mendl*

Main category: quant-ph

TL;DR: 提出了一种基于快速多极方法的量子算法，用于高效模拟二维扩展哈伯德模型的时间演化，电路深度随系统大小呈多对数增长。


<details>
  <summary>Details</summary>
Motivation: 二维扩展哈伯德模型包含长程相互作用，传统模拟方法面临挑战，需要开发高效的量子算法来解决这一问题。

Method: 受快速多极方法启发，通过层次化粗粒度盒子近似处理粒子间相互作用，利用二维中性原子量子计算平台支持的非局域操作（如长程门和穿梭操作）。

Result: 实现了每个Trotter步的电路深度随系统大小呈多对数增长，显著提高了模拟效率。

Conclusion: 该方法为模拟包含长程相互作用的二维扩展哈伯德模型提供了一种高效的量子算法，有望在近期量子计算平台上实现。

Abstract: The extended Hubbard model on a two-dimensional lattice captures key physical phenomena, but is challenging to simulate due to the presence of long-range interactions. In this work, we present an efficient quantum algorithm for simulating the time evolution of this model. Our approach, inspired by the fast multipole method, approximates pairwise interactions by interactions between hierarchical levels of coarse-graining boxes. We discuss how to leverage recent advances in two-dimensional neutral atom quantum computing, supporting non-local operations such as long-range gates and shuttling. The resulting circuit depth for a single Trotter step scales polylogarithmically with system size.

</details>


### [9] [Many-body symmetry-protected zero boundary modes of synthetic photo-magnonic crystals](https://arxiv.org/abs/2512.03135)
*Alan Gardin,Emilio Cobanera,Giuseppe C. Tettamanzi*

Main category: quant-ph

TL;DR: 该论文提出了一种受玻色子多体对称性保护的拓扑分类理论，识别了两种一维拓扑非平凡对称类，并设计了光子-磁子晶体作为实验平台来验证该理论。


<details>
  <summary>Details</summary>
Motivation: 费米子的拓扑分类（"十重方式"）在物理学多个领域产生了重要影响，但玻色子的类似分类存在困难。作者希望建立尽可能接近费米子原型的玻色子拓扑理论。

Method: 提出受玻色子多体对称操作（压缩变换、粒子数、玻色子时间反演）保护的自由玻色子拓扑理论。设计了光子-磁子晶体作为实验平台，通过电磁有限元建模模拟其反射和传输特性。

Result: 识别了两种一维拓扑非平凡对称类，包括玻色子Kitaev链和玻色子SSH模型。建立了光子-磁子晶体链的详细蓝图，展示了其边界模式的实验特征，并解决了向对称保护拓扑相的实验调谐问题。

Conclusion: 该研究为玻色子拓扑分类提供了理论框架和实验实现方案，光子-磁子晶体平台为验证玻色子拓扑物理提供了灵活的实验手段。

Abstract: The topological classification of insulators and superconductors, the "ten-fold way", is grounded on fermionic many-body symmetries and has had a dramatic impact on many fields of physics. Therefore, it seems equally important to investigate a similar approach for bosons as tightly analogous to the fermionic prototype as possible. There are, however, several obstacles coming from the fundamental physical differences between fermions and bosons. Here, we propose a potentially optimal way forward: a theory of free boson topology (topological classification and bulk-boundary correspondence) protected by bosonic many-body symmetry operations, namely, squeezing transformations, particle number, and bosonic time reversal. We identify two symmetry classes that are topologically non-trivial in one dimension. They include key models like the bosonic Kitaev chain, protected by a squeezing symmetry within our framework, and the celebrated bosonic SSH model, protected by a squeezing symmetry and particle number. To provide a robust experimental platform for testing our theory, we introduce a new quantum meta-material: photo-magnonic crystals. They consist of arrays of interconnected photo-magnonic cavities. They are remarkable for their experimental flexibility and natural affinity for displaying band topological physics at microwave frequencies. We engineer a many-body symmetry-protected topological photo-magnonic chain with boundary modes mandated by a Pfaffian invariant. Using an electromagnetic finite-element modelling, we simulate its reflection and transmission and identify experimental signatures of its boundary modes. The experimental tuning of the crystal to its symmetry-protected topological phase is also addressed. Our modelling of the photo-magnonic chain provides a thorough blueprint for its experimental realisation and the unambiguous observation of its exotic physics.

</details>


### [10] [The Pound-Drever-Hall Method for Superconducting-Qubit Readout](https://arxiv.org/abs/2512.03138)
*Ibukunoluwa Adisa,Won Chan Lee,Kevin C. Cox,Alicia J. Kollár*

Main category: quant-ph

TL;DR: 该论文提出了一种基于多音自相位参考Pound-Drever-Hall技术的超稳定超导量子比特读取方法，该方法对微波相位漂移不敏感，可实现单次读取，且不会引起测量诱导的状态跃迁。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算机向大规模发展，需要实现许多并行量子比特读取。传统读取方法存在相位漂移敏感性和测量诱导状态跃迁等问题，需要开发更稳定可靠的读取技术。

Method: 采用多音自相位参考Pound-Drever-Hall技术，该技术最初为光学腔开发。通过室温外差检测所有音调来重建PDH信号，对单个transmon量子比特进行PDH读取基准测试。

Result: PDH量子比特读取对微波相位漂移不敏感，在2小时内显示0.73°的相位稳定性；能够在超过量子比特状态引起的相移的相位误差下进行单次读取；PDH边带音调不会引起transmon量子比特的测量诱导状态跃迁；相比传统外差读取，潜在信号增强至少14dB。

Conclusion: PDH读取技术为大规模量子计算机提供了一种超稳定、相位不敏感、无测量诱导跃迁的量子比特读取解决方案，具有显著优于传统方法的性能优势。

Abstract: Scaling quantum computers to large sizes requires the implementation of many parallel qubit readouts. Here we present an ultrastable superconducting-qubit readout method using the multi-tone self-phase-referenced Pound-Drever-Hall (PDH) technique, originally developed for use with optical cavities. In this work, we benchmark PDH readout of a single transmon qubit, using room-temperature heterodyne detection of all tones to reconstruct the PDH signal. We demonstrate that PDH qubit readout is insensitive to microwave phase drift, displaying $0.73^\circ$ phase stability over 2 hours, and capable of single-shot readout in the presence of phase errors exceeding the phase shift induced by the qubit state. We show that the PDH sideband tones do not cause unwanted measurement-induced state transitions for a transmon qubit, leading to a potential signal enhancement of at least $14$~dB over traditional heterodyne readout.

</details>


### [11] [Classical Thermometry of Quantum Annealers](https://arxiv.org/abs/2512.03162)
*George Grattan,Pratik Sathe,Cristiano Nisoli*

Main category: quant-ph

TL;DR: 量子退火器作为可编程实验平台用于研究强关联自旋系统，但关键的吉布斯分布假设在大规模系统中未经验证。本文通过实验评估了跨越三个数量级的系统尺寸中的吉布斯采样保真度，发现有效温度标度律需要耦合无关的偏移量，揭示了残余非热效应但仍符合有效吉布斯描述。


<details>
  <summary>Details</summary>
Motivation: 量子退火器作为研究强关联自旋系统的实验平台正在兴起，但关键的吉布斯分布输出假设在大规模系统中尚未得到验证。需要定量评估量子退火器作为热力学实验平台的可行性。

Method: 通过实验方法评估吉布斯采样保真度，探索了耦合强度、系统尺寸、退火时间和D-Wave硬件架构等广泛参数空间。系统尺寸跨越三个数量级，使用不同的D-Wave硬件架构进行测试。

Result: 发现朴素假设的有效温度标度律需要非零的、耦合无关的偏移量，这种偏移在不同机器和参数体系中都很稳健。非理想性还体现在从采样系综推断的物理温度与设备名义低温之间的系统差异。

Conclusion: 研究系统评估了量子退火器作为经典热力学实验平台的可行性，修正了先前假设，并提供了物理基础的温度测量框架，为未来的热力学实验提供了基准测试方法。

Abstract: Quantum annealers are emerging as programmable, dynamical experimental platforms for probing strongly correlated spin systems. Yet key thermal assumptions, chiefly a Gibbs-distributed output ensemble, remain unverified in the large-scale regime. Here, we experimentally and quantitatively assess Gibbs sampling fidelity across system sizes spanning over three orders of magnitude. We explore a wide parameter space of coupling strengths, system sizes, annealing times, and D-wave hardware architectures. We find that the naively assumed scaling law for the effective temperature requires a non-negligible, coupling-independent offset that is robust across machines and parameter regimes, quantifying residual non-thermal effects that still conform to an effective Gibbs description. These non-idealities are further reflected in a systematic discrepancy between the physical temperature inferred from the sampled ensemble and the nominal cryogenic temperature of the device. Our results systematically assess the viability of quantum annealers as experimental platforms for probing classical thermodynamics, correct previous assumptions, and provide a physically grounded thermometry framework to benchmark these machines for future thermodynamic experiments.

</details>


### [12] [Excess work in counterdiabatic driving](https://arxiv.org/abs/2512.03274)
*Lucas P. Kamizaki,Marcus V. S. Bonança*

Main category: quant-ph

TL;DR: 该论文提出了一种新方法来量化反绝热驱动（CD）的能量成本，通过瞬时超额功和时间平均超额功来评估，解决了传统方法中CD看似能量成本为零的矛盾。


<details>
  <summary>Details</summary>
Motivation: 反绝热驱动方法长期以来被认为似乎没有能量成本，这挑战了量化其实际能量需求的任务。论文旨在解决这一矛盾，为CD控制提供合理的能量成本量化方法。

Method: 从Mandelstam-Tamm边界出发，将CD加速与总哈密顿量本征态之间的能量扩散联系起来。提出对CD哈密顿量参数的不同解释，使得超额功不为零，从而能够量化能量成本。使用Landau-Zener模型进行验证。

Result: 通过重新解释CD哈密顿量参数，成功获得了非零的超额功，使得超额功能够作为CD能量成本的量化指标。在Landau-Zener模型中验证了这一方法的有效性。

Conclusion: 反绝热驱动的能量成本可以通过瞬时超额功和时间平均超额功来量化，这解决了CD看似能量成本为零的长期矛盾，为量子控制系统的能量成本评估提供了新视角。

Abstract: Many years have passed since the conception of the quintessential method of shortcut to adiabaticity known as counterdiabatic driving (or transitionless quantum driving). Yet, this method appears to be energetically cost-free and thus continually challenges the task of quantifying the amount of energy it demands to be accomplished. This paper proposes that the energy cost of controlling a closed quantum system using the counterdiabatic method can also be assessed using the instantaneous excess work during the process and related quantities, as the time-averaged excess work. Starting from the Mandelstam-Tamm bound for driven dynamics, we have shown that the speed-up of counterdiabatic driving is linked with the spreading of energy between the eigenstates of the total Hamiltonian, which is necessarily accompanied by transitions between these eigenstates. Nonetheless, although excess work can be used to quantify energetically these transitions, it is well known that the excess work is zero throughout the entire process under counterdiabatic driving. To recover the excess work as an energetic cost quantifier for counterdiabatic driving, we will propose a different interpretation of the parameters of the counterdiabatic Hamiltonian, leading to an excess work different from zero. We have illustrated our findings with the Landau-Zener model.

</details>


### [13] [In Situ Quantum Analog Pulse Characterization via Structured Signal Processing](https://arxiv.org/abs/2512.03193)
*Yulong Dong,Christopher Kang,Murphy Yuezhen Niu*

Main category: quant-ph

TL;DR: 提出了一种用于模拟量子模拟器的脉冲轨迹原位学习表征算法，通过扩展量子信号处理框架分析时变脉冲，无需中间电路测量即可从时间有序传播子查询中重建平滑脉冲。


<details>
  <summary>Details</summary>
Motivation: 模拟量子模拟器能够直接模拟时变哈密顿动力学，但现有校准方案主要针对数字门表征设计，无法直接扩展到学习连续脉冲轨迹，需要开发专门针对模拟量子模拟器的表征方法。

Method: 扩展量子信号处理框架分析时变脉冲，结合逻辑级模拟-数字映射范式，直接从时间有序传播子查询中重建平滑脉冲，避免传统基于Trotter分解方法中因逻辑级分段增加导致的不可扩展性能退化。

Result: 通过严格理论分析和大量数值模拟证明，该方法具有高精度、强效率和鲁棒性，能够抵抗SPAM和退极化误差，为模拟量子模拟器提供轻量级且最优的验证协议，能够检测主要硬件故障。

Conclusion: 该方法为模拟量子模拟器提供了有效的脉冲轨迹原位学习表征方案，克服了传统方法的局限性，在精度、效率和鲁棒性方面表现出色，有望推动模拟量子模拟技术的发展和应用。

Abstract: Analog quantum simulators can directly emulate time-dependent Hamiltonian dynamics, enabling the exploration of diverse physical phenomena such as phase transitions, quench dynamics, and non-equilibrium processes. Realizing accurate analog simulations requires high-fidelity time-dependent pulse control, yet existing calibration schemes are tailored to digital gate characterization and cannot be readily extended to learn continuous pulse trajectories. We present a characterization algorithm for in situ learning of pulse trajectories by extending the Quantum Signal Processing (QSP) framework to analyze time-dependent pulses. By combining QSP with a logical-level analog-digital mapping paradigm, our method reconstructs a smooth pulse directly from queries of the time-ordered propagator, without requiring mid-circuit measurements or additional evolution. Unlike conventional Trotterization-based methods, our approach avoids unscalable performance degradation arising from accumulated local truncation errors as the logical-level segmentation increases. Through rigorous theoretical analysis and extensive numerical simulations, we demonstrate that our method achieves high accuracy with strong efficiency and robustness against SPAM as well as depolarizing errors, providing a lightweight and optimal validation protocol for analog quantum simulators capable of detecting major hardware faults.

</details>


### [14] [Quench dynamics of the quantum XXZ chain with staggered interactions: Exact results and simulations on digital quantum computers](https://arxiv.org/abs/2512.03341)
*Ching-Tai Huang,Yu-Cheng Lin,Ferenc Igloi*

Main category: quant-ph

TL;DR: 研究量子XXZ反铁磁链在平带极限下的淬火动力学，通过交换完全二聚化链的奇偶键强度，在贝尔基中推导出精确时间演化态，获得纠缠熵和Loschmidt回波的解析表达式，并在IBM-Q量子设备上进行数值实验验证。


<details>
  <summary>Details</summary>
Motivation: 研究量子多体系统中淬火动力学的精确可解模型，特别是纠缠熵和Loschmidt回波等非平衡动力学特性，同时探索在量子计算机上实现和验证这些理论结果的可能性。

Method: 1. 理论分析：在平带极限下，通过交换完全二聚化链的奇偶键强度，在贝尔基中推导精确时间演化态；2. 解析计算：获得冯·诺依曼熵、二阶Rényi纠缠熵和Loschmidt回波的封闭形式表达式；3. 量子实验：在IBM-Q设备上使用Hadamard测试估计贝尔基展开系数，以及实现无Trotter误差的时间演化电路结合随机泡利测量。

Result: 1. 获得了纠缠熵和Loschmidt回波的尺寸无关解析表达式；2. 确定了各向异性参数控制动力学可观测量周期性的精确条件；3. 在有限链中识别了Loschmidt零点；4. 量子实验结果显示，对于小系统，纠缠熵和Loschmidt回波的测量结果与精确解吻合良好。

Conclusion: 该研究成功建立了量子XXZ链淬火动力学的精确可解模型，获得了关键动力学量的解析结果，并在量子计算机上实现了对这些理论预测的实验验证，展示了量子计算在模拟非平衡量子动力学方面的潜力。

Abstract: We investigate quench dynamics in the quantum $S=1/2$ XXZ antiferromagnetic chain with staggered and anisotropic interactions in the flat-band limit. Our quench protocol interchanges the odd- and even-bond strengths of a fully dimerized chain, enabling us to derive exact time-dependent states for arbitrary even system sizes by working in the Bell basis. We obtain closed-form, size-independent expressions for the von Neumann and second-order Rényi entanglement entropies. We further calculate exact Loschmidt echoes and the corresponding return rate functions across various anisotropies and system sizes, and identify Loschmidt zeros in finite chains. Our analysis reveals the precise conditions on the anisotropy parameter that govern the periodicity of the dynamical observables. In addition to the analytic study, we perform two types of numerical experiments on IBM-Q quantum devices. First, we use the Hadamard test to estimate the Bell-basis expansion coefficients and reconstruct the dynamical states, achieving accurate entanglement entropies and the Loschmidt echo for small systems. Second, we implement Trotter-error-free time-evolution circuits combined with randomized Pauli measurements. Post-processing via statistical correlations and classical shadows yields reliable estimates of the second-order Rényi entanglement entropy and the Loschmidt echo, showing satisfactory agreement with exact results.

</details>


### [15] [Density of states of quantum systems from free probability theory: a brief overview](https://arxiv.org/abs/2512.03850)
*Keun-Young Kim,Kuntal Pal*

Main category: quant-ph

TL;DR: 本文综述了利用自由概率理论计算量子系统和随机矩阵哈密顿量态密度的方法，包括自由加性卷积的应用、成功案例、局限性以及基于从属公式的微扰方案。


<details>
  <summary>Details</summary>
Motivation: 量子系统和随机矩阵哈密顿量的态密度计算在理论物理中具有重要意义。自由概率理论提供了一种有效方法，能够从两个非对易算符的已知态密度推导出它们之和的态密度，这对于研究相互作用量子系统和随机矩阵模型很有价值。

Method: 主要方法包括：1) 假设两个非对易算符相互自由，使用自由加性卷积从已知的组分算符态密度推导哈密顿量态密度；2) 基于态密度柯西变换的从属公式开发微扰方案，用于获得各种模型的近似解析表达式。

Result: 该方法在许多相互作用量子系统和随机矩阵模型中能够提供相当准确的近似态密度。文中回顾了文献中该方法表现良好的例子，同时也讨论了该方法在某些情况下无法提供足够准确描述的局限性。

Conclusion: 自由概率理论为计算量子系统和随机矩阵哈密顿量的态密度提供了有效的近似方法，特别是在组分算符相互自由的假设下。基于从属公式的微扰方案进一步扩展了该方法的应用范围，使其能够处理更复杂的模型如Rosenzweig-Porter随机矩阵系综和具有在位无序的Anderson模型。

Abstract: We provide a brief overview of approaches for calculating the density of states of quantum systems and random matrix Hamiltonians using the tools of free probability theory. For a given Hamiltonian of a quantum system or a generic random matrix Hamiltonian, which can be written as a sum of two non-commutating operators, one can obtain an expression for the density of states of the Hamiltonian from the known density of states of the two component operators by assuming that these operators are mutually free and by using the free additive convolution. In many examples of interacting quantum systems and random matrix models, this procedure is known to provide a reasonably accurate approximation to the exact numerical density of states. We review some of the examples that are known in the literature where this procedure works very well, and also discuss some of the limitations of this method in situations where the free probability approximation fails to provide a sufficiently accurate description of the exact density of states. Subsequently, we describe a perturbation scheme that can be developed from the subordination formulas for the Cauchy transform of the density of states and use it to obtain approximate analytical expressions for the density of states in various models, such as the Rosenzweig-Porter random matrix ensemble and the Anderson model with on-site disorder.

</details>


### [16] [Hybridized-Mode Parametric Amplifier in Kinetic-Inductance Circuits](https://arxiv.org/abs/2512.03362)
*Danial Davoudi,Abdul Mohamed,Shabir Barzanjeh*

Main category: quant-ph

TL;DR: 本文展示了一种基于耦合动能电感谐振器的双模参量放大器，相比传统约瑟夫森结放大器具有更高饱和功率、更宽带宽和磁不敏感性


<details>
  <summary>Details</summary>
Motivation: 约瑟夫森结放大器虽然已成为超导量子电路的标准，但其存在磁敏感性、有限饱和功率和亚开尔文工作温度限制等问题，需要开发替代的非线性平台

Method: 使用NbTiN和NbN薄膜制造一对电容耦合的克尔非线性谐振器，利用材料的分布式克尔非线性实现非简并四波混频放大，并建立耦合模理论模型分析

Result: 实现了接近40dB的增益，增益带宽积达6.9MHz，1dB压缩功率比最先进的约瑟夫森放大器高2-3个数量级，NbN器件表现出更大的克尔系数和更优的增益带宽性能

Conclusion: 耦合动能电感谐振器为宽带、高功率和磁不敏感的量子极限放大提供了稳健平台，为超导量子比特、自旋系综、量子点等微波量子技术的先进读出提供了可扩展途径

Abstract: Parametric amplification is essential for quantum measurement, enabling the amplification of weak microwave signals with minimal added noise. While Josephson-junction-based amplifiers have become standard in superconducting quantum circuits, their magnetic sensitivity, limited saturation power, and sub-kelvin operating requirements motivate the development of alternative nonlinear platforms. Here we demonstrate a two-mode kinetic-inductance parametric amplifier based on a pair of capacitively coupled Kerr-nonlinear resonators fabricated from NbTiN and NbN thin films. The distributed Kerr nonlinearity of these materials enables nondegenerate four-wave-mixing amplification with gains approaching 40 dB, gain-bandwidth products up to 6.9 MHz, and 1-dB compression powers two to three orders of magnitude higher than those of state-of-the-art Josephson amplifiers. A coupled-mode theoretical model accurately captures the pump-induced modification of the hybridized modes and quantitatively reproduces the observed signal and idler responses. The NbN device exhibits a significantly larger Kerr coefficient and superior gain-bandwidth performance, highlighting the advantages of high-kinetic-inductance materials. Our results establish coupled kinetic-inductance resonators as a robust platform for broadband, high-power, and magnetically resilient quantum-limited amplification, offering a scalable route for advanced readout in superconducting qubits, spin ensembles, quantum dots, and other microwave-quantum technologies.

</details>


### [17] [Thermodynamics of an Open $\mathcal{PT-}$Symmetric Quantum System](https://arxiv.org/abs/2512.03935)
*Baibhab Bose,Devvrat Tiwari,Subhashish Banerjee*

Main category: quant-ph

TL;DR: 该论文研究了PT对称哈密顿量系统的量子热力学性质，特别是能量提取能力（ergotropy），包括封闭系统和开放系统两种情况。


<details>
  <summary>Details</summary>
Motivation: 研究PT对称量子系统的热力学性质，特别是能量提取能力，探索非厄米哈密顿量系统的热力学行为。

Method: 1. 为满足反对易关系的PT对称哈密顿量子类找到厄米基；2. 使用修正的投影算子构造广义密度矩阵；3. 计算封闭系统的ergotropy；4. 研究开放系统中PT对称系统的ergotropy；5. 分析热力学三定律的适用性。

Result: 1. 找到了PT对称哈密顿量子类的厄米基；2. 计算了PT对称演化的广义密度矩阵；3. 获得了封闭系统的ergotropy表达式；4. 分析了不同非厄米性机制下开放系统的ergotropy；5. 验证了热力学三定律在PT对称开放系统中的一致性。

Conclusion: 该研究为PT对称量子系统的热力学性质提供了理论基础，特别是能量提取能力，并验证了热力学定律在非厄米系统中的适用性。

Abstract: For a subclass of a general $\mathcal{PT}-$symmetric Hamiltonian obeying anti-commutation relation with its conjugate, a Hermitian basis is found that spans the bi-orthonormal energy eigenvectors. Using the modified projectors constructed from these eigenvectors, the generalized density matrix of the $\mathcal{PT}-$symmetric evolution is calculated, and subsequently, ergotropy for a closed system is obtained. The $\mathcal{PT}-$symmetric system, in an open system scenario, is studied to understand ergotropy under different regimes of non-Hermiticity of the Hamiltonian. The consistency of the three laws of thermodynamics for the $\mathcal{PT}-$symmetric system in an open system scenario is also analyzed.

</details>


### [18] [Engineering photonic dispersion relation and atomic dynamics in waveguide QED setup via long-range hoppings](https://arxiv.org/abs/2512.03423)
*Weijun Cheng,Da-Wei Wang,Yang Xue,Zhihai Wang,Liantuan Xiao*

Main category: quant-ph

TL;DR: 该论文研究通过设计一维耦合谐振腔波导中的长程跳跃，构建具有手性特性的线性色散关系，并展示其在控制原子辐射和吸收方面的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 光子波导中非平凡色散关系的工程化设计能够精确控制原子动力学，这一领域最近引起了广泛关注。研究者希望探索如何通过设计波导结构来实现特定的色散关系，从而为原子-环境耦合提供更灵活的控制手段。

Method: 研究采用一维耦合谐振腔波导系统，通过精心设计谐振腔之间的第j阶最近邻跳跃，构建具有手性特性的线性色散关系。通过分析高斯波包在波导中的传播保真度来量化线性程度，并展示这种波导结构如何实现定向原子辐射和吸收。

Result: 成功构建了具有手性特性的线性色散关系，并证明耦合谐振腔波导可以作为实现定向原子辐射和吸收的多功能平台。此外，通过定制第j阶最近邻跳跃，还可以实现更一般的色散关系，包括二次和三次关系。

Conclusion: 该研究为模拟具有任意色散关系的原子-环境耦合提供了一个统一框架，展示了通过工程化设计波导结构来实现精确原子动力学控制的潜力。

Abstract: Non-trivial dispersion relations engineered in photonic waveguide for the precise control of atomic dynamics has recently attracted considerable attention. Here, we study a system in which atoms are coupled to one-dimensional coupled-resonator waveguides with long-range hoppings. By carefully engineering the jth-order nearest neighbor (JNN) hoppings between resonators, we construct linear dispersion relations with the chiral characteristic. To quantify the degree of linearity, we analyze the propagation fidelities of Gaussian wave packets in these waveguides. Furthermore, we demonstrate that such coupled-resonator waveguides can serve as versatile platforms for enabling directional atomic radiation and absorption. Beyond linear dispersion relations, more general forms, including quadratic and cubic relations, can also be achieved through tailored JNN-hoppings. Our study thus provides a unified framework for simulating atom-environment couplings with arbitrary dispersion relations.

</details>


### [19] [Quantum Encrypted Control of Networked Systems](https://arxiv.org/abs/2512.03434)
*Zihao Ren,Daniel Quevedo,Salah Sukkarieh,Guodong Shi*

Main category: quant-ph

TL;DR: 本文提出了一种基于量子通信的高效加密控制框架，利用量子信道生成密钥，实现轻量级加密，并分析了量子状态误差对闭环稳定性的影响。


<details>
  <summary>Details</summary>
Motivation: 传统加密控制方案在单个密钥位错误时可能崩溃，且计算复杂度高。本文旨在利用量子通信技术提高加密控制的效率、安全性和鲁棒性。

Method: 1. 利用传感器与执行器之间的量子信道生成相同的秘密密钥，并通过量子密钥分发增强安全性；2. 开发基于量子密钥的线性系统状态反馈控制的新型加密-解密架构；3. 分析量子状态误差对闭环稳定性的影响，建立内在量子噪声的临界阈值；4. 采用量化技术处理实际通信比特受限的情况；5. 基于随机量化器实现量子密钥的隐私保护。

Result: 1. 证明了存在一个内在量子噪声的临界阈值，低于该阈值时可保证系统稳定性；2. 与传统加密控制方案相比，所提出的量子加密控制对密钥缺陷具有更强的鲁棒性；3. 通过量化技术有效处理了实际通信比特受限的情况；4. 实现了对量子密钥的隐私保护。

Conclusion: 即使以当前的技术成熟度，将量子技术以非平凡且有原则的方式集成到控制系统中，可以在降低计算复杂度、提高对密钥错误的鲁棒性以及确保对多种窃听源的安全性方面获得显著的性能提升。

Abstract: Encrypted control has been extensively studied to ensure the confidentiality of system states and control inputs for networked control systems. This paper presents a computationally efficient encrypted control framework for networked systems enabled by quantum communication. A quantum channel between sensors and actuators is used to generate identical secret keys, whose security is further enhanced through quantum key distribution. These keys enable lightweight encryption and decryption while preserving confidentiality and control accuracy. We develop a novel encryption-decryption architecture for state-feedback control of linear systems based on quantum keys, and characterize the impact of quantum state errors on closed-loop stability. In particular, we establish the existence of a critical threshold on intrinsic quantum noise below which stability is guaranteed. In contrast to classical encrypted control schemes, which may collapse under a single key-bit error, the proposed quantum encrypted control exhibits strong robustness to key imperfections. We further adopt quantization techniques to address the scenarios with limited communication bits in practical situations, and implement privacy protection for quantum keys based on a stochastic quantizer. These results demonstrate that integrating quantum technologies into control systems in a nontrivial and principled manner, even at their current level of maturity, can yield substantial performance gains in reducing computational complexity and improving resilience to key errors while ensuring security against multiple eavesdropping sources.

</details>


### [20] [Beyond Lindblad Dynamics: Rigorous Guarantees for Thermal and Ground State Preservation under System Bath Interactions](https://arxiv.org/abs/2512.03457)
*Ke Wang,Zhiyan Ding*

Main category: quant-ph

TL;DR: 该论文证明了系统-浴相互作用模型在量子热态和基态制备中的高效性和鲁棒性，突破了传统弱耦合极限的限制，即使在强耦合下也能实现精确状态制备。


<details>
  <summary>Details</summary>
Motivation: 现有分析主要依赖弱耦合Lindblad极限，需要O(ε)耦合强度才能达到ε精度，导致混合速度缓慢。本文旨在证明即使在超出这一限制的强耦合区域，精确状态制备仍然是可能的。

Method: 引入新技术控制戴森展开的所有阶次，并分析相关的多维算子傅里叶变换。通过理论证明和数值模拟（TFIM和Hubbard模型）验证系统-浴相互作用框架的鲁棒性。

Result: 即使累积耦合强度保持恒定而非趋近于零，诱导的量子通道仍能近似固定目标态。理论界限显著优于先前结果，数值模拟进一步证实了该框架在弱耦合和强耦合区域下的鲁棒性。

Conclusion: 系统-浴相互作用模型在量子热态和基态制备中具有超越传统弱耦合极限的高效性和鲁棒性，为量子计算中的状态制备提供了更强大的理论支持。

Abstract: We establish new theoretical results demonstrating the efficiency and robustness of system bath interaction models for quantum thermal and ground state preparation. Unlike existing analyses, which relies on the weak coupling Lindblad limit and require $O(ε)$ coupling strengths for $ε$ accuracy, leading to slow mixing, we rigorously show that accurate state preparation remains possible far beyond this regime. In particular, even when the cumulative coupling strength remains constant rather than vanishing, the induced quantum channel still approximately fixes the target state. Our proof introduces new techniques for controlling all orders of the Dyson expansion and for analyzing the associated multidimensional operator Fourier transforms. These bounds substantially improve upon prior results, and numerical simulations on the TFIM and Hubbard models further confirm the robustness of the system bath interaction framework across both weak and strong coupling regimes.

</details>


### [21] [Complex Wigner entropy and Fisher control of negativity in an oval quantum billiard](https://arxiv.org/abs/2512.03505)
*Kyu-Won Park,Jongin Jeong*

Main category: quant-ph

TL;DR: 提出复熵框架量化Wigner负性，应用于椭圆量子台球中的避免交叉现象，建立负性通道Fisher信息度量参数敏感性


<details>
  <summary>Details</summary>
Motivation: 需要量化量子相空间中的非经典性（Wigner负性），特别是在波混沌和介观系统中，理解避免交叉等量子现象如何影响相空间负性

Method: 开发复熵框架：将Gibbs-Shannon泛函扩展到复数域，虚部正比于Wigner负体积；引入符号分解分离总负权重与相空间分布；定义负性通道Fisher信息量化负瓣随参数变化的敏感性

Result: 在椭圆量子台球中，避免交叉显示增强的负性和放大的负性通道Fisher响应，提供了模式杂交的清晰相空间特征；建立了Cauchy-Schwarz界限制虚熵（Wigner负性）随参数变化的速度

Conclusion: 复熵框架为量化Wigner负性提供了通用工具，特别适用于波混沌和介观系统，揭示了避免交叉中模式杂交的相空间特征，建立了负性变化速率的基本限制

Abstract: We develop a complex-entropy framework for Wigner negativity and apply it to avoided crossings in an oval quantum billiard. For a real Wigner function the Gibbs--Shannon functional becomes complex; its imaginary part, proportional to the Wigner-negative volume, serves as an entropy-like measure of phase-space nonclassicality. A sign-resolved decomposition separates the total negative weight from its phase-space distribution and defines a negative-channel Fisher information that quantifies how sensitively the negative lobe reshapes as a control parameter is varied. This structure yields a Cauchy--Schwarz bound that limits how rapidly the imaginary entropy, and hence the Wigner negativity, can change with the parameter. In the oval billiard, avoided crossings display enhanced negativity and an amplified negative-channel Fisher response, providing a clear phase-space signature of mode hybridization. The construction is generic and extends to other wave-chaotic and mesoscopic systems with phase-space representations.

</details>


### [22] [Quantum Hash Function Based on Spectral Properties of Graphs and Discrete Walker Dynamics](https://arxiv.org/abs/2512.03581)
*Mohana Priya Thinesh Kumar,Pranavishvar Hariprakash*

Main category: quant-ph

TL;DR: 提出了一种基于量子图谱哈希的算法QGH-256，通过将消息映射为加权图，利用量子相位估计提取拉普拉斯矩阵的相位谱，生成256位高熵哈希指纹。


<details>
  <summary>Details</summary>
Motivation: 开发一种结合量子计算和图谱理论的新型哈希算法，利用量子相位估计提取消息诱导图的谱特征，生成具有强敏感性和结构丰富性的后量子哈希函数。

Method: 1. 将输入消息通过离散随机游走映射到n×n环面网格上的加权图；2. 使用量子相位估计提取图拉普拉斯矩阵的相位谱，采用节点基态的均匀叠加态而非特征向量；3. 将谱特征转换为256位摘要。

Result: 实现了QGH-256在4×4环面网格上的可行性验证，较小网格会出现碰撞，较大网格显著增加执行时间。使用Qiskit实现整个流程，通过种子状态向量模拟器获得稳定无噪声结果。

Conclusion: QGH-256算法通过量子相位估计提取消息诱导图的谱特征，生成高熵哈希指纹，对输入扰动具有强敏感性，为后量子哈希提供了结构丰富的基础。

Abstract: We present Quantum Graph Hash (QGH-256), a novel quantum spectral hashing algorithm that generates high-entropy fingerprints from message-induced graphs. Each input message is mapped to a weighted graph via a discrete random walk on an n X n toroidal grid, where the walk dynamics determine the edge weights. Quantum Phase Estimation (QPE) is then used to extract the phase spectrum of the graph Laplacian. Unlike standard QPE settings, the phase estimation is performed with respect to a superposition state (a uniform superposition over all node basis states) rather than an eigenvector, ensuring that all eigencomponents contribute to the resulting spectrum. This yields spectral features that distinguish even co-spectral but non-isomorphic message-induced graphs. The final spectral fingerprint is converted into a 256-bit digest, producing a compact representation of the input. As the fingerprint encodes both spectral and dynamical properties of the message-induced graph, the resulting hash exhibits strong sensitivity to input perturbations and provides a structurally rich foundation for post-quantum hashing. To demonstrate the feasibility of the approach, we implement QGH-256 on a 4 X 4 toroidal grid, chosen empirically: smaller grids exhibit collisions, whereas larger grids significantly increase execution time. The entire pipeline is implemented in Qiskit, and we use a seeded statevector simulator to obtain stable, noise-free results.

</details>


### [23] [Energy-Scaled Zero-Noise Extrapolation for Gottesman-Kitaev-Preskill Code](https://arxiv.org/abs/2512.03583)
*Gui-Zhong Luo,Matthew Otten*

Main category: quant-ph

TL;DR: 提出ES-ZNE协议，通过调节GKP码的平均光子数作为可调噪声参数，外推至理想无限能量极限，以缓解有限压缩对GKP码性能的限制。


<details>
  <summary>Details</summary>
Motivation: GKP码的性能受限于当前实验平台的有限压缩能力，需要一种方法来规避这种硬件需求，提升近态玻色子量子处理器的性能。

Method: 引入能量缩放零噪声外推（ES-ZNE）协议，使用GKP码的平均光子数作为可调有效噪声参数，在一系列可访问的有限能量下测量逻辑可观测量，并基于代码渐近误差缩放假设外推至理想无限能量极限。

Result: 通过模拟纯损耗通道下的GKP量子比特，ES-ZNE成功缓解了有限能量误差，在浅噪声区域恢复了理想期望值；通过计算去除有限能量编码产生的伪影，揭示了理想GKP码的内在性能，发现了一个尖锐的误差阈值。

Conclusion: ES-ZNE是一种实用的软件策略，通过采样开销换取高压缩等物理资源需求，可增强近态玻色子量子处理器的性能。

Abstract: The performance of Gottesman-Kitaev-Preskill (GKP) codes, an approach to hardware-efficient quantum error correction, is limited by the finite squeezing capabilities of current experimental platforms. To circumvent this hardware demand, we introduce Energy-Scaled Zero-Noise Extrapolation (ES-ZNE), a quantum error mitigation protocol that uses the mean photon number of the GKP code as a tunable effective noise parameter. The protocol measures logical observables at a series of accessible finite energies and extrapolates the results to the ideal, infinite-energy limit using an ansatz based on the code's asymptotic error scaling. Through simulating a GKP qubit under a pure-loss channel, we demonstrate that ES-ZNE successfully mitigates finite-energy errors, recovering the ideal expectation values (within numerical uncertainty) in the shallow-noise regime. Furthermore, by computationally removing artifacts arising from the finite-energy encoding, our method characterizes the intrinsic performance of the ideal GKP code, revealing a sharp error threshold beyond which the code's corrective power diminishes. These results establish ES-ZNE as a practical, software-based strategy for enhancing the performance of near-term bosonic quantum processors, trading sampling overhead for demanding physical resources like high squeezing.

</details>


### [24] [Experimental quantum voting using photonic GHZ states](https://arxiv.org/abs/2512.03659)
*Francis Marcellino,Mingsong Wu,Rob Thew*

Main category: quant-ph

TL;DR: 实验实现了一个基于量子GHZ态的四方选举协议，确保包括中央机构在内的任何人都无法获知投票者的偏好选择


<details>
  <summary>Details</summary>
Motivation: 量子通信协议利用量子系统的独特性质实现超越经典方法的协调或通信任务，特别是在需要强安全性和匿名性的选举场景中。选举需要确保合法性的强安全保障，而量子协议在这方面具有潜力。

Method: 实验实现了Centrone等人提出的选举协议，生成并分发四粒子GHZ态，通过量子态记录投票者的投票意向。协议确保包括潜在中央机构在内的任何人都无法获知任何投票者的偏好候选者。

Result: 成功进行了四方选举实验，生成的GHZ态保真度约为89%，成功记录投票者意向的成功率约为87%。

Conclusion: 该实验成功演示了量子选举协议的可行性，实现了强匿名性和安全性保证，为量子技术在选举系统中的应用提供了实验验证。

Abstract: Quantum communication protocols seek to leverage the unique properties of quantum systems for coordination or communication tasks, usually with guarantees of security or anonymity that exceed what is possible classically. One promising domain of application is elections, where strong such guarantees are essential to ensure legitimacy. We experimentally implement a recently proposed election protocol from Centrone et al. such that no one, including a potential central authority, can know the preferred candidate of any voter other than themself. We conduct a four-party election, generating and distributing four-partite GHZ states with $\approx 89\%$ fidelity and successfully recording voters' intentions $\approx 87\%$ of the time.

</details>


### [25] [Direct Equivalence between Dynamics of Quantum Walks and Coupled Classical Oscillators](https://arxiv.org/abs/2512.03681)
*Lilith Zschetzsche,Refik Mansuroglu,András Molnár,Norbert Schuch*

Main category: quant-ph

TL;DR: 本文建立了连续时间量子行走与谐振子系统动力学之间的直接映射关系，为量子计算提供了新的视角和算法转换框架。


<details>
  <summary>Details</summary>
Motivation: 连续时间量子行走和谐振子系统动力学都是BQP完全问题，但现有研究仅通过BQP完全性将它们联系起来。本文旨在建立两者之间更直接、透明的映射关系，以更好地利用这两个量子计算框架的优势。

Method: 通过建立连续时间量子行走与谐振子系统动力学之间的直接映射，保持底层图结构、初始化、读取和高效预言访问等特性，实现低空间和时间开销的转换。

Result: 成功建立了两个问题类之间的透明映射，该映射不仅适用于BQP完全实例，也适用于非BQP完全的限制子集，并提供了算法直接转换的配方。

Conclusion: 该映射为量子行走范式和谐振子系统之间的算法转换提供了直接途径，同时为谐振子系统问题的BQP完全性提供了更透明的证明方法，增强了两个量子计算框架之间的互操作性。

Abstract: Continuous time quantum walks on exponentially large, sparse graphs form a powerful paradigm for quantum computing: On the one hand, they can be efficiently simulated on a quantum computer. On the other hand, they are themselves BQP-complete, providing an alternative framework for thinking about quantum computing -- a perspective which has indeed led to a number of novel algorithms and oracle problems. Recently, simulating the dynamics of a system of harmonic oscillators (that is, masses and springs) was set forth as another BQP-complete problem defined on exponentially large, sparse graphs. In this work, we establish a direct and transparent mapping between these two classes of problems. As compared to linking the two classes of problems via their BQP-completeness, our mapping has several desirable features: It is transparent, in that it respects the structure of the problem, including the geometry of the underlying graph, initialization, read-out, and efficient oracle access, resulting in low overhead in terms of both space and time; it allows to map also between restricted subsets of instances of both problems which are not BQP-complete; it provides a recipe to directly translate any quantum algorithm designed in the quantum walk paradigm to harmonic oscillators (and vice versa); and finally, it provides an alternative, transparent way to prove BQP-completeness of the harmonic oscillator problem by mapping it to BQP-completeness construction for the quantum walk problem (or vice versa).

</details>


### [26] [Distributed Quantum Computing with Fan-Out Operations and Qudits: the Case of Distributed Global Gates (a Preliminary Study)](https://arxiv.org/abs/2512.03685)
*Seng W. Loke*

Main category: quant-ph

TL;DR: 该论文探讨了在分布式量子计算中如何利用多体纠缠资源（如GHZ态）和四维量子比特来实现分布式扇出操作和电路压缩，特别关注全局门（全局Mølmer-Sørensen门）的实现挑战。


<details>
  <summary>Details</summary>
Motivation: 分布式量子计算通常依赖于纠缠对和分布式双量子比特门，但本文关注如何通过单次操作实现节点间的多体纠缠，避免使用多个纠缠对生成多体纠缠态。研究探索多体纠缠资源在分布式扇出操作中的应用价值。

Method: 采用多体纠缠资源（如GHZ态）和四维量子比特（qudits）来实现分布式扇出操作和量子电路压缩。特别关注如何利用这些技术实现具有挑战性的全局门（全局Mølmer-Sørensen门），这些门涉及所有量子比特之间的两两相互作用。

Result: 研究表明多体纠缠资源和四维量子比特可以有效地用于分布式扇出操作和电路压缩，特别是在实现全局门这种具有挑战性的分布式量子计算任务中。这为减少电路深度、提高计算效率提供了新途径。

Conclusion: 多体纠缠资源和四维量子比特为分布式量子计算提供了有前景的工具，特别是在处理全局门等复杂操作时。研究结果对未来的量子电路编译和量子数据中心设计具有重要启示，展示了在"极端"分布式场景下的应用潜力。

Abstract: Much recent work on distributed quantum computing have focused on the use of entangled pairs and distributed two qubit gates. But there has also been work on efficient schemes for achieving multipartite entanglement between nodes in a single shot, removing the need to generate multipartite entangled states using many entangled pairs. This paper looks at how multipartite entanglement resources (e.g., GHZ states) can be useful for distributed fan-out operations; we also consider the use of qudits of dimension four for distributed quantum circuit compression. In particular, we consider how such fan-out operations and qudits can be used to implement circuits which are challenging for distributed quantum computation, involving pairwise qubit interactions, i.e., what has been called global gates (a.k.a. global Mølmer-Sørensen gates). Such gates have been explored to possibly yield more efficient computations via reduced circuit depth, and can be carried out efficiently in some types of quantum hardware (e.g., trapped-ion quantum computers); we consider this as an exploration of an ``extreme'' case for distribution given the global qubit-qubit interactions. We also conclude with some implications for future work on quantum circuit compilation and quantum data centre design.

</details>


### [27] [Sympathetic Cooling of Levitated Optomechanics through Nonreciprocal Coupling](https://arxiv.org/abs/2512.03690)
*Jialin Li,Guangyu Zhang,Zhang-qi Yin*

Main category: quant-ph

TL;DR: 提出了一种基于非厄米相互作用的非互易光机械冷却方案，通过两个悬浮纳米颗粒之间的非互易耦合实现比传统腔冷却更低的声子占据数


<details>
  <summary>Details</summary>
Motivation: 传统腔辅助冷却受限于腔耗散和环境噪声，限制了可达到的最低温度，需要探索新的冷却机制

Method: 提出非厄米光机械冷却方案，通过两个悬浮纳米颗粒之间的非互易耦合，其中一个颗粒被光学腔直接冷却，另一个通过非厄米相互作用间接冷却

Result: 增加非互易性增强了定向能量转移，使目标颗粒达到比传统腔冷却更低的声子占据数，理论和数值模拟均证实了这一点

Conclusion: 该研究展示了一种由非厄米相互作用驱动的新冷却机制，为实现悬浮光机械系统中的可控能量流和深度冷却提供了理论指导

Abstract: Optomechanical cooling of levitated nanoparticles has become an essential topic in modern quantum physics, providing a platform for exploring macroscopic quantum phenomena and high-precision sensing. However, conventional cavity-assisted cooling is fundamentally constrained by cavity dissipation and environmental noise, limiting the attainable minimum temperature. In this work, we propose a non-Hermitian optomechanical cooling scheme through nonreciprocal coupling between two levitated nanoparticles, where one particle is directly cooled by an optical cavity and the other is cooled indirectly through a non-Hermitian interaction. Both analytical solutions and numerical simulations reveal that increasing nonreciprocity enhances directional energy transfer, enabling the target particle to reach a lower phonon occupation than is achievable in conventional cavity cooling. This study demonstrates a new cooling mechanism driven by non-Hermitian interactions, offering theoretical guidance for realizing controllable energy flow and deep cooling in levitated optomechanical systems, and paving the way for future developments in quantum control and sensing technologies.

</details>


### [28] [Geometrical structure of the Wigner flow information quantifiers and hyperbolic stability in the phase-space framework](https://arxiv.org/abs/2512.03717)
*Alex E. Bernardini*

Main category: quant-ph

TL;DR: 该研究在Weyl-Wigner相空间框架下，从微分几何结构推导出平稳性、经典性、纯度和涡旋性的量化指标，并将其与经典和量子修正哈密顿非线性运动方程的双曲稳定性联系起来。


<details>
  <summary>Details</summary>
Motivation: 研究动机是建立相空间几何结构与哈密顿系统稳定性之间的定量联系，特别是量化量子涨落对相空间涡旋性出现的影响，从而更好地理解非线性动力学中的平衡和稳定性特性。

Method: 采用Weyl-Wigner相空间框架，从微分几何结构推导量化指标，分析自治常微分方程系统的平衡态，建立Wigner流特性与相空间双曲稳定性边界之间的对应关系，并针对量子高斯系综获得解析表达式。

Result: 获得了平衡稳定性参数的显式解析表达式，识别了由Wigner流驱动的信息量化指标，并通过Harper-like系统的应用验证了方法，建立了相空间涡旋性与哈密顿非线性动力学平衡稳定性之间的定量关系。

Conclusion: 该研究提供了一个自洽的分析框架，用于识别量子涨落对相空间涡旋性出现的影响，从而量化哈密顿非线性动力学的平衡和稳定性特性，为理解量子修正对经典非线性系统稳定性的影响提供了新视角。

Abstract: Quantifiers of stationarity, classicality, purity and vorticity are derived from phase-space differential geometrical structures within the Weyl-Wigner framework, after which they are related to the hyperbolic stability of classical and quantum-modified Hamiltonian (non-linear) equations of motion. By examining the equilibrium regime produced by such an autonomous system of ordinary differential equations, a correspondence between Wigner flow properties and hyperbolic stability boundaries in the phase-space is identified. Explicit analytical expressions for equilibrium-stability parameters are obtained for quantum Gaussian ensembles, wherein information quantifiers driven by Wigner currents are identified. Illustrated by an application to a Harper-like system, the results provide a self-contained analysis for identifying the influence of quantum fluctuations associated to the emergence of phase-space vorticity in order to quantify equilibrium and stability properties of Hamiltonian non-linear dynamics.

</details>


### [29] [Non-Gaussian Dissipative Quantum Thermometry Beyond Gaussian Bounds](https://arxiv.org/abs/2512.03735)
*Pritam Chattopadhyay*

Main category: quant-ph

TL;DR: 该论文研究了开放量子系统中温度传感的基本计量极限，特别关注非高斯量子资源的作用。通过分析非高斯态在耗散玻色演化下的量子费希尔信息，揭示了在相同能量约束下非高斯探针何时以及如何显著优于高斯态。


<details>
  <summary>Details</summary>
Motivation: 开放量子系统中温度传感的基本计量极限尚未完全解决，特别是非高斯量子资源在温度估计中的作用。需要明确在何种条件下非高斯探针能够超越高斯态的性能，为实际量子技术应用提供理论指导。

Method: 采用短时域时间局域主方程方法，推导了非高斯态在耗散玻色演化下量子费希尔信息的解析界限。通过精确的标度律分析，比较了非高斯态与高斯态的性能差异，并辅以精确数值模拟验证理论结果。

Result: 研究发现福克态展现出独特的线性时间量子费希尔信息增强，而高斯探针仅表现出较弱的二次标度。这一理论洞察通过精确数值模拟得到验证，并映射到电路量子电动力学等实验可访问平台。

Conclusion: 该研究不仅阐明了非高斯性在量子测温中的优势，还为在噪声量子技术中利用这一优势提供了现实可行的路径，为量子计量学的发展提供了重要理论指导。

Abstract: The fundamental metrological limits of temperature sensing in open quantum systems remain largely unresolved, particularly regarding the role of non-Gaussian quantum resources. In this letter, we establish analytic bounds on the quantum Fisher information (QFI) for temperature estimation using non-Gaussian states undergoing dissipative bosonic evolution. By focusing on the short-time regime governed by a time-local master equation, we derive precise scaling laws that elucidate when and how non-Gaussian probes decisively outperform Gaussian states under identical energy constraints. Our analysis uncovers a distinct linear-in-time QFI enhancement unique to Fock states, in contrast to the inherently weaker, quadratic scaling of Gaussian probes. These theoretical insights are substantiated through exact numerical simulations and mapped onto experimentally accessible platforms such as circuit QED. Our results not only clarify the quantum thermometric advantage of non-Gaussianity but also chart a realistic pathway toward harnessing it in noisy quantum technologies.

</details>


### [30] [Quantum Max Cut for complete tripartite graphs](https://arxiv.org/abs/2512.03740)
*Tea Štrekelj*

Main category: quant-ph

TL;DR: 本文解决了小局部维度(d≤3)下完全三部图的量子Max-d-Cut问题


<details>
  <summary>Details</summary>
Motivation: 量子Max-d-Cut问题是经典Max-d-Cut问题的量子类比，是2-局部哈密顿量问题的特例。近年来通过研究d-QMC哈密顿量的代数结构取得进展，本文在此基础上进一步推进。

Method: 基于d-QMC哈密顿量的代数结构方法，针对完全三部图在小局部维度(d≤3)情况下求解量子Max-d-Cut问题。

Result: 成功解决了d≤3时完全三部图的量子Max-d-Cut问题，找到了哈密顿量的最大本征值。

Conclusion: 通过代数结构方法，本文扩展了量子Max-d-Cut问题的求解范围，为小局部维度下完全三部图提供了解决方案。

Abstract: The Quantum Max-$d$-Cut ($d$-QMC) problem is a special instance of a $2$-local Hamiltonian problem, representing the quantum analog of the classical Max-$d$-Cut problem. The $d$-QMC problem seeks the largest eigenvalue of a Hamiltonian defined on a graph with $n$ vertices, where edges correspond to swap operators acting on $(\mathbb{C}^d)^{\otimes n}$. In recent years, progress has been made by investigating the algebraic structure of the $d$-QMC Hamiltonian. Building on this approach, this article solves the $d$-QMC problem for complete tripartite graphs for small local dimensions, $d \le 3$.

</details>


### [31] [Widefield Quantum Sensor for Vector Magnetic Field Imaging of Micromagnetic Structures](https://arxiv.org/abs/2512.03748)
*Orlando D. Cunha,Filipe Camarneiro,João P. Silva,Hariharan Nhalil,Ariel Zaig,Lior Klein,Jana B. Nieder*

Main category: quant-ph

TL;DR: 开发了一种基于氮空位中心的宽场矢量磁力计，可在标准显微镜上实现微米级空间分辨率的快速矢量磁场成像


<details>
  <summary>Details</summary>
Motivation: 许多自旋电子学、磁存储和神经形态器件依赖于空间变化的磁场，但现有技术要么分辨率高但扫描慢，要么宽场成像但矢量灵敏度有限或材料受限，缺乏实用的宽场矢量磁力计

Method: 改造商用宽场显微镜，采用相机兼容的脉冲光学检测磁共振协议，通过解析四个NV取向的塞曼位移来重建杂散场矢量

Result: 实现了约0.52微米的空间分辨率，83×83微米视场，峰值灵敏度(828±142)nT/√Hz，采集时间仅需几分钟，成功重建了微加工坡莫合金结构的杂散场矢量

Conclusion: 脉冲宽场NV磁力计成为实用且可扩展的工具，可用于常规复杂磁性器件的矢量分辨成像

Abstract: Many spintronic, magnetic-memory, and neuromorphic devices rely on spatially varying magnetic fields. Quantitatively imaging these fields with full vector information over extended areas remains a major challenge. Existing probes either offer nanoscale resolution at the cost of slow scanning, or widefield imaging with limited vector sensitivity or material constraints. Quantum sensing with nitrogen-vacancy (NV) centers in diamond promises to bridge this gap, but a practical camera-based vector magnetometry implementation on relevant microstructures has not been demonstrated. Here we adapt a commercial widefield microscope to implement a camera-compatible pulsed optically detected magnetic resonance protocol to reconstruct stray-field vectors from microscale devices. By resolving the Zeeman shifts of the four NV orientations, we reconstruct the stray-field vector generated by microfabricated permalloy structures that host multiple stable remanent states. Our implementation achieves a spatial resolution of $\approx 0.52 ~μ\mathrm{m}$ across an $83~μ\mathrm{m} \times 83~μ\mathrm{m}$ field of view and a peak sensitivity of $ (828 \pm 142)~\mathrm{nT\,Hz^{-1}}$, with acquisition times of only a few minutes. These results establish pulsed widefield NV magnetometry on standard microscopes as a practical and scalable tool for routine vector-resolved imaging of complex magnetic devices.

</details>


### [32] [An end-to-end quantum algorithm for nonlinear fluid dynamics with bounded quantum advantage](https://arxiv.org/abs/2512.03758)
*David Jennings,Kamil Korzekwa,Matteo Lostaglio,Richard Ashworth,Emanuele Marsili,Stephen Rolston*

Main category: quant-ph

TL;DR: 该论文分析了现有基于Carleman嵌入的量子CFD算法存在严重瓶颈，提出了一种新的不可压缩格子玻尔兹曼方程量子算法，发现仅在高误差容忍度下对特定观测值可能保持适度量子优势。


<details>
  <summary>Details</summary>
Motivation: 现有基于Carleman嵌入的量子CFD算法存在收敛性、时间步长要求、条件数扩展和数据提取效率等严重瓶颈，阻碍了量子优势的实现，需要开发新的量子算法来克服这些障碍。

Method: 开发了一种针对不可压缩格子玻尔兹曼方程的新量子算法，详细分析了算法复杂性，包括所有潜在的算法复杂度来源，并提供了门计数估计。

Result: 对于端到端问题，在高误差容忍度下对特定观测值可能保持适度量子优势。给出了量子算法的雷诺数缩放下界，在D维Kolmogorov微尺度分辨率下为O(Re^{3/4(1+D/2)}×q_M)，其中q_M是数据提取的乘法开销。数值研究表明D=2时的缩放估计为O(Re^{1.936}×q_M)。

Conclusion: 研究结果表明，在CFD领域可以实现小而重要的量子优势，但需要进一步开发严格的端到端量子算法。现有Carleman嵌入方法存在严重瓶颈，而新提出的格子玻尔兹曼方法可能提供更可行的路径。

Abstract: Computational fluid dynamics (CFD) is a cornerstone of classical scientific computing, and there is growing interest in whether quantum computers can accelerate such simulations. To date, the existing proposals for fault-tolerant quantum algorithms for CFD have almost exclusively been based on the Carleman embedding method, used to encode nonlinearities on a quantum computer. In this work, we begin by showing that these proposals suffer from a range of severe bottlenecks that negate conjectured quantum advantages: lack of convergence of the Carleman method, prohibitive time-stepping requirements, unfavorable condition number scaling, and inefficient data extraction. With these roadblocks clearly identified, we develop a novel algorithm for the incompressible lattice Boltzmann equation that circumvents these obstacles, and then provide a detailed analysis of our algorithm, including all potential sources of algorithmic complexity, as well as gate count estimates. We find that for an end-to-end problem, a modest quantum advantage may be preserved for selected observables in the high-error-tolerance regime. We lower bound the Reynolds number scaling of our quantum algorithm in dimension $D$ at Kolmogorov microscale resolution with $O(\mathrm{Re}^{\frac{3}{4}(1+\frac{D}{2})} \times q_M)$, where $q_M$ is a multiplicative overhead for data extraction with $q_M = O(\mathrm{Re}^{\frac{3}{8}})$ for the drag force. This upper bounds the scaling improvement over classical algorithms by $O(\mathrm{Re}^{\frac{3D}{8}})$. However, our numerical investigations suggest a lower speedup, with a scaling estimate of $O(\mathrm{Re}^{1.936} \times q_M)$ for $D=2$. Our results give robust evidence that small, but nontrivial, quantum advantages can be achieved in the context of CFD, and motivate the need for additional rigorous end-to-end quantum algorithm development.

</details>


### [33] [Metrological Sensitivity beyond Gaussian Limits with Cubic Phase States](https://arxiv.org/abs/2512.03769)
*Jiajie Guo,Shuheng Liu,Boxuan Jing,Qiongyi He,Manuel Gessner*

Main category: quant-ph

TL;DR: 立方相位态作为连续变量量子计算的关键非高斯资源，在量子计量学中展现出超越高斯态的相位传感灵敏度优势


<details>
  <summary>Details</summary>
Motivation: 立方相位态是连续变量量子计算的重要非高斯资源，但它们在量子计量学中的潜力尚未得到充分探索。研究旨在探索立方相位态在量子精密测量中是否能够超越高斯态的性能极限

Method: 通过理论分析立方相位态的量子计量学性能，研究其在相位传感中的灵敏度优势，分析其对损耗和检测噪声的鲁棒性，并识别最优测量策略和实验制备方案

Result: 立方相位态在相同平均光子数下超越了所有高斯态的相位传感灵敏度，仅需适度的初始压缩即可达到最优灵敏度，且非高斯优势对损耗和检测噪声具有鲁棒性。多个实验相关制备方案能够超越高斯极限，某些情况下甚至达到立方相位态的灵敏度水平

Conclusion: 立方相位态是超越高斯极限的量子增强精密测量的有前景资源，为量子计量学提供了新的非高斯资源选择

Abstract: Cubic phase states provide the essential non-Gaussian resource for continuous-variable quantum computing. We show that they also offer significant potential for quantum metrology, surpassing the phase-sensing sensitivity of all Gaussian states at equal average photon number. Optimal sensitivity requires only moderate initial squeezing, and the non-Gaussian advantage remains robust against loss and detection noise. We identify optimal measurement strategies and show that several experimentally relevant preparation schemes surpass Gaussian limits, in some cases reaching the sensitivity of cubic phase states. Our results establish cubic phase states as a promising resource for quantum-enhanced precision measurements beyond Gaussian limits.

</details>


### [34] [Quantum Algorithm for Searching for the Longest Segment and the Largest Empty Rectangle](https://arxiv.org/abs/2512.03788)
*Kamil Khadiev,Vladislav Remidovskii,Timur Bikmullin,Aliya Khadieva*

Main category: quant-ph

TL;DR: 该论文提出了量子算法解决二维地图中的最大空矩形问题，包括最大空正方形和固定宽度d的最大空矩形，相比经典算法获得了平方级加速。


<details>
  <summary>Details</summary>
Motivation: 研究二维地图中最大空矩形搜索问题及其一维版本（最大空线段）的量子算法，旨在利用量子计算优势显著降低查询复杂度，实现相对于经典算法的加速。

Method: 针对n×n矩形地图，设计了量子算法：对于最大空正方形问题，查询复杂度为Õ(n¹·⁵)；对于固定宽度d的最大空矩形问题，查询复杂度为Õ(n√d)。同时为一维版本问题设计了O(√n log n log log n)查询复杂度的量子算法。

Result: 量子算法相比经典算法获得显著加速：最大空正方形问题的经典下界为Ω(n²)，量子算法为Õ(n¹·⁵)；固定宽度d矩形问题的经典下界为Ω(nd)，量子算法为Õ(n√d)；一维版本问题的经典下界为Ω(n)，量子算法为O(√n log n log log n)，量子下界为Ω(√n)，基本达到最优。

Conclusion: 该研究成功为最大空矩形问题及其一维版本设计了高效的量子算法，实现了相对于经典算法的平方级加速，证明了量子计算在几何搜索问题上的优势。

Abstract: In the paper, we consider the problem of searching for the Largest empty rectangle in a 2D map, and the one-dimensional version of the problem is the problem of searching for the largest empty segment. We present a quantum algorithm for the Largest Empty Square problem and the Largest Empty Rectangle of a fixed width $d$ for $n\times n$-rectangular map. Query complexity of the algorithm is $\tilde{O}(n^{1.5})$ for the square case, and $\tilde{O}(n\sqrt{d})$ for the rectangle with a fixed width $d$ case, respectively. At the same time, the lower bounds for the classical case are $Ω(n^2)$, and $Ω(nd)$, respectively. The Quantum algorithm for the one-dimensional version of the problem has $O(\sqrt{n}\log n\log\log n)$ query complexity. The quantum lower bound for the problem is $Ω(\sqrt{n})$ which is almost equal to the upper bound up to a log factor. The classical lower bound is $Ω(n)$. So, we obtain the quadratic speed-up for the problem.

</details>


### [35] [Solution of the Electric Field Integral Equation Using a Hybrid Quantum-Classical Scheme: Investigation of Accuracy and Efficiency](https://arxiv.org/abs/2512.03808)
*Rui Chen,Teng-Yang Ma,Meng-Han Dou,Chao-Fu Wang*

Main category: quant-ph

TL;DR: 该论文提出了一种混合量子-经典方案，首次用于求解三维完美电导体电磁散射问题的电场积分方程，通过双层迭代策略结合量子算法降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统经典求解器在处理大规模电磁问题时面临内存瓶颈，而现有量子算法直接求解原始EFIE矩阵方程的能力有限。需要开发能够处理任意形状三维导体电磁散射问题的新方法。

Method: 采用混合量子-经典方案：首先设计预处理线性系统，然后使用双层迭代策略。外层迭代构建维度较小的子空间矩阵方程系统，内层迭代使用量子算法（HHL和VQLS）求解这些较小系统。在量子模拟器和量子计算机平台上执行。

Result: 理论时间复杂度分析表明，混合VQLS-经典方案的计算复杂度低于传统经典快速求解器。数值实验验证了混合方案的准确性和效率，表明其更适合分析大规模电磁问题。

Conclusion: 混合量子-经典方案为解决三维完美电导体电磁散射问题提供了有前景的途径，特别是混合VQLS-经典方案在计算复杂度方面优于传统方法，为大规模电磁问题分析开辟了新方向。

Abstract: Conventional classical solvers are commonly used for solving matrix equation systems resulting from the discretization of SIEs in computational electromagnetics (CEM). However, the memory requirement would become a bottleneck for classical computing as the electromagentic problems become much larger. As an alternative, quantum computing has a natural "parallelization" advantage with much lower storage complexity due to the superposition and entanglement in quantum mechanics. Even though several quantum algorithms have been applied for the SIEs-based methods in the literature, the size of the matrix equation systems solvable using them is still limited. In this work, we use a hybrid quantum-classical scheme to solve the EFIE for analyzing electromagentic scattering from three-dimensional (3D) perfect electrically conducting objects with arbitrary shapes in CEM for the first time. Instead of directly solving the original EFIE matrix equation system using the quantum algorithms, the hybrid scheme first designs the preconditioned linear system and then uses a double-layer iterative strategy for its solution, where the external iteration layer builds subspace matrix equation systems with smaller dimension and the internal iteration layer solves the smaller systems using the quantum algorithms. Two representative quantum algorithms, HHL and VQLS, are considered in this work, which are executed on the quantum simulator and quantum computer platforms. We present the theoretical time complexity analysis of the hybrid quantum-classical scheme and perform numerical experiments to investigate the accuracy and efficiency of the hybrid scheme. The results show that the computational complexity of the hybrid VQLS-classical scheme is lower than the conventional fast solvers in classical computing, which indicates the hybrid scheme is more promising for analyzing large-scale electromagnetic problems.

</details>


### [36] [Modelling the Impact of Device Imperfections on Electron Shuttling in SiMOS devices](https://arxiv.org/abs/2512.03853)
*Jack J. Turner,Christian W. Binder,Guido Burkard,Andrew J. Fisher*

Main category: quant-ph

TL;DR: 该研究通过3D模拟分析了SiMOS器件中传送带式电荷传输的性能，识别了关键挑战并找到了可靠的电荷传输操作区间。


<details>
  <summary>Details</summary>
Motivation: Si/SiGe系统中的电子传输已实现高保真度，但Si/SiO2（SiMOS）系统的演示仍处于早期阶段。需要深入研究SiMOS器件中传送带式电荷传输的实际性能。

Method: 在真实SiMOS器件上进行完整的3D模拟，求解泊松方程和含时薛定谔方程，分析不同传输速度和栅极电压下的性能，重点关注SiMOS器件的典型问题：氧化物界面粗糙度、栅极制造缺陷和传输路径上的电荷缺陷。

Result: 模拟发现：低栅极电压下，多层栅极结构中的额外氧化物屏蔽会导致传送带式传输崩溃为桶式传输，并引起显著的轨道激发；增加限制能恢复传送带操作，该操作对界面粗糙度、栅极错位和氧化物中的电荷缺陷具有鲁棒性；但位于Si/SiO2界面的缺陷会引起显著的轨道激发；在较低的传送带栅极偏压下，传输通道中的正电荷缺陷甚至能捕获通过的电子。

Conclusion: 研究识别了SiMOS架构中可靠电荷传输的关键挑战，并找到了合适的操作区间，为SiMOS系统中的电荷传输技术发展提供了重要指导。

Abstract: Extensive theoretical and experimental work has established high-fidelity electron shuttling in Si/SiGe systems, whereas demonstrations in Si/SiO2 (SiMOS) remain at an early stage. To help address this, we perform full 3D simulations of conveyor-belt charge shuttling in a realistic SiMOS device, building on earlier 2D modelling. We solve the Poisson and time-dependent Schrodinger equations for varying shuttling speeds and gate voltages, focusing on potential pitfalls of typical SiMOS devices such as oxide-interface roughness, gate fabrication imperfections, and charge defects along the transport path. The simulations reveal that for low clavier-gate voltages, the additional oxide screening in multi-layer gate architectures causes conveyor-belt shuttling to collapse to the bucket-brigade mode, inducing considerable orbital excitation in the process. Increasing the confinement restores conveyor-belt operation, which we find to be robust against interface roughness, gate misalignment, and charge defects buried in the oxide. However, our results indicate that defects located at the Si/SiO2-interface can induce considerable orbital excitation. For lower conveyor gate biases, positive defects in the transport channel can even capture passing electrons. Hence we identify key challenges and find operating regimes for reliable charge transport in SiMOS architectures.

</details>


### [37] [Towards Quantum Stochastic Optimization for Energy Systems under Uncertainty: Joint Chance Constraints with Quantum Annealing](https://arxiv.org/abs/2512.03925)
*David Ribes,Tatiana Gonzalez Grandon*

Main category: quant-ph

TL;DR: 量子退火平台在机会约束机组组合问题中的应用评估，混合量子经典求解器在大规模场景下具有竞争力，但当前量子退火硬件仍存在限制


<details>
  <summary>Details</summary>
Motivation: 现代电力系统中可再生能源和需求波动带来的不确定性使得随机优化变得必要，机会约束机组组合问题能捕捉这种不确定性，但随着场景数量增加计算变得困难，量子计算被提出作为克服这种扩展障碍的潜在途径

Method: 将机会约束机组组合问题重新表述为混合整数线性规划，使用DWave混合量子经典求解器和Gurobi进行求解，同时测试了QUBO重新表述方法

Result: 混合求解器在大规模场景集（实验中达15,000个）的严格运行时间限制下具有竞争力，而Gurobi在较小案例中表现更优；QUBO重新表述因硬件限制无法处理随机机组组合问题，确定性案例也受到嵌入开销的影响

Conclusion: 研究明确了机会约束机组组合问题在哪些方面已经可以通过混合量子经典方法解决，以及当前量子退火器在哪些方面仍然存在根本性限制，为量子计算在电力系统优化中的应用提供了实际指导

Abstract: Uncertainty is fundamental in modern power systems, where renewable generation and fluctuating demand make stochastic optimization indispensable. The chance constrained unit commitment problem (UCP) captures this uncertainty but rapidly becomes computationally challenging as the number of scenarios grows. Quantum computing has been proposed as a potential route to overcome such scaling barriers. In this work, we evaluate the applicability of quantum annealing platforms to the chance constrained UCP. Focusing on a scenario approximation, we reformulated the problem as a mixed integer linear program and solved it using DWave hybrid quantum classical solver alongside Gurobi. The hybrid solver proved competitive under strict runtime limits for large scenario sets (15,000 in our experiments), while Gurobi remained superior on smaller cases. QUBO reformulations were also tested, but current annealers cannot accommodate stochastic UCPs due to hardware limits, and deterministic cases suffered from embedding overhead. Our study delineates where chance constrained UCPs can already be addressed with hybrid quantum classical methods, and where current quantum annealers remain fundamentally limited.

</details>


### [38] [Rethinking Collapse: Coupling Quantum States to Classical Bits with quasi-probabilities](https://arxiv.org/abs/2512.03929)
*Dagomir Kaszlikowski,Pawel Kurzynski*

Main category: quant-ph

TL;DR: 该论文提出了一种在修正框架下描述量子测量的方法，将单个量子比特直接耦合到经典测量比特，通过准双随机过程实现量子测量，避免了冯·诺依曼无限耦合链问题。


<details>
  <summary>Details</summary>
Motivation: 传统量子测量理论中的冯·诺依曼链涉及无限耦合，导致测量问题复杂化。作者希望建立一种更直接的量子-经典耦合框架，将测量寄存器视为经典系统，同时通过准双随机结构捕捉测量的非经典特性。

Method: 将量子比特表示为两个经典比特的正概率分布p(aa')，测量装置用经典比特α描述，初始化为纯分布p(α)=½(1+α)。测量相互作用通过准双随机过程S(bb'β|aa'α)建模，这是一个可能包含负转移概率的双随机映射，但作用于完全正的状态空间。该过程作用于联合初始状态p(aa')p(α)，产生坍缩状态p(bb'|β)，得到测量结果β。

Result: 该方法能够产生正确的量子力学概率p(β)作为测量结果，成功地将量子比特与经典测量比特直接耦合，避免了传统测量理论中的无限耦合链问题，同时通过准双随机结构保持了测量的非经典特性。

Conclusion: 该研究提出了一种新颖的量子测量框架，通过将量子系统直接耦合到经典寄存器，并利用准双随机过程描述相互作用，为量子测量问题提供了新的理论视角，简化了传统测量理论的复杂性。

Abstract: We propose a formulation of quantum measurement within a modified framework of frames, in which a quantum system - a single qubit - is directly coupled to a classical measurement bit. The qubit is represented as a positive probability distribution over two classical bits, a and a', denoted by p(aa'). The measurement apparatus is described by a classical bit $α= \pm 1$, initialized in the pure distribution $p(α) = \frac{1}{2}(1 + α)$. The measurement interaction is modeled by a quasi-bistochastic process $ S(bb'β\mid aa'α)$ - a bistochastic map that may include negative transition probabilities, while acting on an entirely positive state space. When this process acts on the joint initial state $p(aa')p(α)$, it produces a collapsed state $p(bb'\midβ)$, yielding the measurement outcome $β$ with the correct quantum-mechanical probability $p(β)$. This approach bypasses the von Neumann chain of infinite couplings by treating the measurement register classically, while capturing the nonclassical nature of measurement through the quasi-bistochastic structure of the interaction.

</details>


### [39] [Phase-space open-systems dynamics of second-order nonlinear interactions with pulsed quantum light](https://arxiv.org/abs/2512.03933)
*Emanuel Hubenschmid,Victor Rueskov Christiansen*

Main category: quant-ph

TL;DR: 提出广义Bloch-Messiah分解框架，用于高效计算宽带多模量子脉冲在二阶非线性相互作用中的输入输出关系，特别适用于输入输出模式数量不同的开放量子系统。


<details>
  <summary>Details</summary>
Motivation: 宽带多模量子脉冲的二阶非线性相互作用理论描述复杂，但实际应用中通常只有少数宽带模式相关。需要一种高效框架来计算非线性元件输入输出量子态之间的关系，特别是当输入输出模式数量不同时。

Method: 引入广义Bloch-Messiah分解，将描述简化为相同数量的输入输出模式。通过将缩减输入量子脉冲的重新标度Wigner函数与多元高斯相空间函数卷积，计算输出态的多模Wigner函数。

Result: 以THz频率区的单宽带模式Fock态和双模压缩真空态为例，研究了卷积和纠缠断裂导致的热化对输出Wigner函数的影响，计算了输出Wigner函数的冯·诺依曼熵。

Conclusion: 该方法可用于优化宽带量子态的放大或频率转换，为超快时间尺度上光学量子态的生成和表征开辟了新途径。

Abstract: The theoretical description of broadband, multimode quantum pulses undergoing a second-order $χ^{(2)}$-nonlinear interaction can be quite intricate, due to the large dimensionality of the underlying phase space. However, in many cases only a few broadband (temporal) modes are relevant before and after the nonlinear interaction. Here we present an efficient framework to calculate the relation between the quantum states at the input and output of a nonlinear element in their respective relevant modes. Since the number of relevant input and output modes may differ, resulting in an open quantum system, we introduce the generalized Bloch-Messiah decomposition (GBMD), reducing the description to an equal number of input and output modes. The GBMD enables us to calculate the multimode Wigner function of the output state by convolving the rescaled Wigner function of the reduced input quantum pulse with a multivariate Gaussian phase-space function. We expand on this result by considering two examples input states: A Fock state in a single broadband mode and a two-mode squeezed vacuum, both in the THz-frequency regime, up-converted to a single output broadband mode of optical frequencies. We investigate the effect, the convolution and thermalization due to entanglement breakage have on the output Wigner function by calculating the von Neumann entropy of the output Wigner function. The methods presented here can be used to optimize the amplification or frequency conversion of broadband quantum states, opening an avenue to the generation and characterization of optical quantum states on ultrafast time scales.

</details>


### [40] [Image Theory for the Single Bounce Quantum Gravimeter](https://arxiv.org/abs/2512.03953)
*Joachim Guyomard,Serge Reynaud,Pierre Cladé*

Main category: quant-ph

TL;DR: 该研究为单反弹量子重力计开发了图像理论，通过连续能量基分解描述物质波包的自由落体和量子反弹，为量子干涉起源提供更清晰的解释，并提供了探索参数空间的新工具。


<details>
  <summary>Details</summary>
Motivation: 为最近提出的单反弹量子重力计建立理论基础，提供更清晰的物理解释，并开发分析工具来优化重力测量精度。

Method: 采用连续能量基分解方法描述物质波包的自由落体和量子反弹过程，结合半经典估计来分析量子干涉现象。

Result: 获得了量子干涉起源的更清晰解释，开发了探索参数空间的新工具，并讨论了自由落体加速度测量的预期精度。

Conclusion: 该图像理论为单反弹量子重力计提供了理论基础和分析框架，有助于理解和优化重力测量性能。

Abstract: We develop an image theory for the recently proposed single-bounce quantum gravimeter. Free fall and quantum bounce of a matter wave-packet are described through decompositions over a basis of continuous energies. This leads to a much clearer interpretation of the origin of quantum interferences, associated to semi-classical estimations. We then give new tools to explore the space of parameters, and discuss the expected accuracy of the free-fall acceleration measurement.

</details>


### [41] [Entanglement Detection with Rotationally Covariant Measurements - From Compton Scattering to Lemonade](https://arxiv.org/abs/2512.03984)
*Marlene Funck,Ilija Funk,Tizian Schmidt,René Schwonnek*

Main category: quant-ph

TL;DR: 该研究开发了基于旋转对称测量装置的偏振光子纠缠检测方法，推导了POVM公式，提出了SDP优化认证方法，证明了在旋转协变测量下虽然无法实现贝尔违规但可认证EPR导引，并通过软饮料散射实验验证了柠檬汁基检测器的适用性。


<details>
  <summary>Details</summary>
Motivation: 量子纠缠的准确高效检测是量子信息科学的核心挑战。本研究旨在解决仅由旋转对称性指定的测量装置下的偏振光子纠缠检测问题，探索在对称约束条件下的纠缠认证方法。

Method: 1. 推导了仅由旋转对称性指定的测量装置的显式正算子值测度(POVM)，证明此类设置可由单个实数参数r分类；2. 给出了康普顿散射中偏振光子的Klein-Nishina公式的POVM表述；3. 提出了基于半定规划(SDP)的纠缠认证方法，利用完整测量统计量给出紧致边界；4. 考虑了半设备独立场景；5. 设计了旋转协变展示实验，分析偏振光在软饮料中的散射。

Result: 1. 证明了旋转协变测量装置可由单个实数参数r完全表征；2. 开发了有效的SDP纠缠认证方法，在对称约束下给出紧致边界；3. 证明了在旋转协变测量下无法实现贝尔违规，但可认证EPR导引；4. 实验验证了柠檬汁基检测器适用于纠缠检测，软饮料散射实验展示了旋转协变测量的实际应用。

Conclusion: 该研究为旋转对称测量装置下的量子纠缠检测提供了系统框架，证明了在对称约束下仍可实现有效的纠缠认证，特别是EPR导引的认证。柠檬汁基检测器的成功应用展示了该方法的实际可行性，为量子信息处理中的纠缠检测提供了新工具。

Abstract: The accurate and efficient detection of quantum entanglement remains a central challenge in quantum information science. In this work, we study the detection of entanglement of polarized photons for measurement devices that are solely specified by rotational symmetry. We derive explicit positive operator valued measures (POVMs) showing that from a quantum information perspective any such setting is classified by one real measurable parameter r. In Particular, we give a POVM formulation of the Klein--Nishina formula for Compton scattering of polarized photons. We provide an SDP-based entanglement certification method that operates on the full measured statistics and gives tight bounds, also considering semi-device independent scenarios. Furthermore, we show that, while Bell violations are impossible with rotationally covariant measurements, EPR steering can still be certified under one-sided symmetry constraints. Finally, we present a rotationally covariant showcase experiment, analyzing the scattering of polarized optical light in a selection of soft drinks. Our results suggest that lemonade-based detectors are suitable for entanglement detection.

</details>


### [42] [Fully quantum theory of strong-field driven tunable entangled multi-photon states in HHG](https://arxiv.org/abs/2512.03987)
*Sebastián de-la-Peña,Heiko Appel,Angel Rubio,Ofer Neufeld*

Main category: quant-ph

TL;DR: 该论文提出了首个用于量子高次谐波生成（HHG）中纠缠度量的完整量子理论，解决了光-物质相互作用哈密顿量，并与实验数据达成定性一致，揭示了激光功率对纠缠特性的影响。


<details>
  <summary>Details</summary>
Motivation: 量子高次谐波生成是一个新兴研究领域，能够产生高光子数纠缠态光。然而，关于正确描述HHG量子方面（如压缩或纠缠）所需的理论水平存在开放争论。先前的方法采用非相互作用的经典轨迹系综或基于经典轨迹的微扰理论，忽略了关键的纠缠特征。

Method: 开发了用于HHG中纠缠度量的完整量子理论，精确求解光-物质相互作用哈密顿量，并用于评估不同谐波发射光子之间的纠缠。该方法首次考虑了经典自由度上的焦平均，这在以往的量子HHG理论中被忽略。

Result: 理论首次与实验达成定性一致，显示低于阈值谐波的R纠缠参数随激光功率增加而减小。发现激光功率微调可增强HHG纠缠特征，这些特征随驱动功率振荡并呈现局部非经典最大值结构。理论预测低于阈值谐波中观察到的纠缠振荡行为也出现在涉及高于阈值谐波的纠缠中。此外，驱动电子轨迹的长程行为可定性改变产生的纠缠，焦平均在纠缠度量中起关键作用并能改变可观测量定性行为。

Conclusion: 这项工作建立了探索HHG中纠缠特征的最先进理论框架，为分析和工程化XUV和超快领域中"真正量子"多光子态铺平了道路，可应用于更复杂的物质系统。

Abstract: Quantum high-harmonic generation (HHG) is a growing field of research with capabilities of providing high photon-number entangled states of light. However, there is an open debate regarding the theory level required for correctly describing the quantum aspects of HHG emission, such as squeezing or entanglement. Previous approaches have employed non-interacting classical ensembles of trajectories, or perturbation theory utilizing the classical trajectories as a starting point, missing out key entanglement features. In this Letter, we develop a full quantum theory for entanglement measures in HHG solving exactly the light-matter interaction Hamiltonian and employ it for evaluating the entanglement between emitted photons of different harmonics. For the first time, we reach qualitative agreement of theory with recent experiments showing that the R entanglement parameter decreases with increasing laser power for below-threshold harmonics. Our results indicate that fine-tuning the laser power could enhance HHG entanglement features, which are observed to oscillate with the driving power and exhibit local non-classical maxima structures. Similarly, our theory predicts that the oscillatory behavior of entanglement observed for below-threshold harmonics also appears for entanglement involving above-threshold harmonics. We also show that the long-range behavior of driven electronic trajectories can qualitatively change the resulting entanglement. Lastly, we show that focal averaging over classical degrees of freedom, which has thus far been ignored in quantum HHG theories, plays a key role in entanglement measures and can change the qualitative behavior of observables. Our work establishes the state-of-the art in exploring entanglement features in HHG, and paves way for analysis and engineering of 'truly-quantum' multi-photon states in the XUV and ultrafast regime for more complex matter systems.

</details>


### [43] [TARA Test-by-Adaptive-Ranks for Quantum Anomaly Detection with Conformal Prediction Guarantees](https://arxiv.org/abs/2512.04016)
*Davut Emre Tasar,Ceren Ocal Tasar*

Main category: quant-ph

TL;DR: TARA框架结合了保形预测和序列鞅测试，为量子异常检测提供分布无关的有效性保证，解决了现有量子密钥分发认证方法在有限样本和对抗场景下缺乏严格统计保证的问题。


<details>
  <summary>Details</summary>
Motivation: 现有量子密钥分发安全认证方法在有限样本条件和对抗场景下缺乏严格的统计保证，无法可靠区分真正的量子关联与经典窃听者模拟，需要一种具有分布无关有效性保证的认证框架。

Method: 提出TARA框架，结合保形预测和序列鞅测试。TARA-k基于Kolmogorov-Smirnov校准对抗局部隐变量零分布；TARA-m采用投注鞅进行流式检测，实现实时类型I错误控制。理论证明在（上下文条件）可交换性下，保形p值即使在强上下文量子数据中仍保持均匀分布。

Result: 在IBM Torino（超导，CHSH=2.725）和IonQ Forte Enterprise（离子阱，CHSH=2.716）量子处理器上验证，实现比经典CHSH界限2高出36%的安全裕度。发现同分布校准会使检测性能虚高44个百分点，表明先前使用标准训练测试分割的量子认证研究可能系统性高估了对抗鲁棒性。

Conclusion: TARA框架为量子异常检测提供了具有分布无关有效性保证的认证方法，揭示了量子认证中同分布校准可能导致性能虚高的问题，对量子认证及其他非经典数据应用具有重要影响。

Abstract: Quantum key distribution (QKD) security fundamentally relies on the ability to distinguish genuine quantum correlations from classical eavesdropper simulations, yet existing certification methods lack rigorous statistical guarantees under finite-sample conditions and adversarial scenarios. We introduce TARA (Test by Adaptive Ranks), a novel framework combining conformal prediction with sequential martingale testing for quantum anomaly detection that provides distribution-free validity guarantees. TARA offers two complementary approaches. TARA k, based on Kolmogorov Smirnov calibration against local hidden variable (LHV) null distributions, achieving ROC AUC = 0.96 for quantum-classical discrimination. And TARA-m, employing betting martingales for streaming detection with anytime valid type I error control that enables real time monitoring of quantum channels. We establish theoretical guarantees proving that under (context conditional) exchangeability, conformal p-values remain uniformly distributed even for strongly contextual quantum data, confirming that quantum contextuality does not break conformal prediction validity a result with implications beyond quantum certification to any application of distribution-free methods to nonclassical data. Extensive validation on both IBM Torino (superconducting, CHSH = 2.725) and IonQ Forte Enterprise (trapped ion, CHSH = 2.716) quantum processors demonstrates cross-platform robustness, achieving 36% security margins above the classical CHSH bound of 2. Critically, our framework reveals a methodological concern affecting quantum certification more broadly: same-distribution calibration can inflate detection performance by up to 44 percentage points compared to proper cross-distribution calibration, suggesting that prior quantum certification studies using standard train test splits may have systematically overestimated adversarial robustness.

</details>


### [44] [Thermalization from quenching in coupled oscillators](https://arxiv.org/abs/2512.04028)
*M. Harinarayanan,Karthik Rajeev*

Main category: quant-ph

TL;DR: 提出一种有限时间协议，无需宏观热浴即可将量子谐振子从其基态热化。该方法使用第二个谐振子作为有效环境，通过突然淬火改变频率和耦合来实现热化。


<details>
  <summary>Details</summary>
Motivation: 量子热力学实验和态制备需要快速、可控的热化方法。传统方法依赖宏观热浴，但本文旨在开发一种无需宏观热浴的有限时间热化协议。

Method: 使用第二个谐振子作为有效环境，通过突然淬火改变两个谐振子的频率和耦合强度。由于动力学的高斯性质，热化条件简化为三个可解方程，可获得精确解析解（针对离散温度集）和数值解（其他情况）。

Result: 获得了精确解析解（针对密集离散温度集）和数值解（其他情况）。任何目标温度都可以以任意精度近似，但需要在速度和精度之间权衡。协议简单，适用于量子热力学实验。

Conclusion: 该协议为量子热力学实验和态制备提供了一种快速、可控的热化工具，无需宏观热浴，通过简单操作即可实现精确热化。

Abstract: We introduce a finite-time protocol that thermalizes a quantum harmonic oscillator, initially in its ground state, without requiring a macroscopic bath. The method uses a second oscillator as an effective environment and implements sudden quenches of the oscillator frequencies and coupling. Owing to the Gaussian nature of the dynamics, the thermalization condition reduces to three solvable equations, yielding exact analytic solutions for a dense discrete set of temperatures and numerical solutions in all other cases. Any target temperature can be approximated with arbitrary precision, with a trade-off between speed and accuracy. The simplicity of the protocol makes it a promising tool for rapid, controlled thermalization in quantum thermodynamics experiments and state preparation.

</details>


### [45] [Closing the problem of which causal structures of up to six total nodes have a classical-quantum gap](https://arxiv.org/abs/2512.04058)
*Shashaank Khanna,Matthew Pusey,Roger Colbeck*

Main category: quant-ph

TL;DR: 该研究解决了6节点及以下因果结构中唯一未解决的问题，证明了在该特定因果结构中存在经典方法无法实现的量子关联，从而完成了6节点及以下因果结构中量子非经典关联的完整图景。


<details>
  <summary>Details</summary>
Motivation: 贝尔不等式揭示了量子关联无法用经典方法复现，这一发现在量子力学基础和实际应用中都很重要。虽然贝尔最初在简单的二分因果结构中证明了这一点，但类似结果在其他因果结构中也得到了展示。本研究旨在解决6节点及以下因果结构中唯一未解决的问题：是否存在量子关联无法用经典方法实现。

Method: 研究者通过施加额外的限制条件来分析特定因果结构中的关联性。这种方法涉及对关联施加约束条件，从而证明在该因果结构中存在量子关联无法用经典方法实现。

Result: 研究证明了在6节点及以下因果结构中唯一未解决的特定因果结构中，确实存在量子关联无法用经典方法实现。这一发现完成了6节点及以下因果结构中量子非经典关联的完整分类。

Conclusion: 该研究解决了6节点及以下因果结构中量子非经典关联存在的最后一个开放问题，为理解量子关联在不同因果结构中的表现提供了完整图景，并展示了通过施加额外限制条件分析因果结构的方法的有效性。

Abstract: The discovery of Bell that there exist quantum correlations that cannot be reproduced classically is one of the most important in the foundations of quantum mechanics, as well as having practical implications. Bell's result was originally proven in a simple bipartite causal structure, but analogous results have also been shown in further causal structures. Here we study the only causal structure with six or fewer nodes in which the question of whether or not there exist quantum correlations that cannot be achieved classically was open. In this causal structure we show that such quantum correlations exist using a method that involves imposing additional restrictions on the correlations. This hence completes the picture of which causal structures of up to six nodes support non-classical quantum correlations. We also provide further illustrations of our method using other causal structures.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [46] [Physics-Informed Machine Learning for Steel Development: A Computational Framework and CCT Diagram Modelling](https://arxiv.org/abs/2512.03050)
*Peter Hedström,Victor Lamelas Cubero,Jón Sigurdsson,Viktor Österberg,Satish Kolli,Joakim Odqvist,Ziyong Hou,Wangzhong Mu,Viswanadh Gowtham Arigela*

Main category: cs.LG

TL;DR: 本文提出了一种结合物理洞察与机器学习的计算框架，用于开发钢铁的物理信息连续冷却转变（CCT）模型，该模型在4100个图表数据集上训练，具有高计算效率和强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习在材料科学中已成为加速计算设计和生产的强大工具，但将通用机器学习框架应用于钢铁等复杂工业材料仍面临挑战。主要障碍在于准确捕捉化学成分、加工参数与最终微观结构和性能之间的复杂关系。

Method: 引入结合物理洞察与机器学习的计算框架，开发物理信息连续冷却转变（CCT）模型。该模型在4100个CCT图表数据集上训练，并针对文献和实验数据进行验证。

Result: 模型具有高计算效率，在5秒内生成包含100条冷却曲线的完整CCT图。对所有合金钢表现出强泛化能力，各相分类F1分数均超过88%。相变温度回归中，除贝氏体相（MAE为27°C）外，所有相的均方误差均低于20°C。

Conclusion: 该框架可通过添加通用和定制化机器学习模型扩展，建立热处理通用数字孪生平台。与补充模拟工具和目标实验的集成将进一步支持加速材料设计工作流程。

Abstract: Machine learning (ML) has emerged as a powerful tool for accelerating the computational design and production of materials. In materials science, ML has primarily supported large-scale discovery of novel compounds using first-principles data and digital twin applications for optimizing manufacturing processes. However, applying general-purpose ML frameworks to complex industrial materials such as steel remains a challenge. A key obstacle is accurately capturing the intricate relationship between chemical composition, processing parameters, and the resulting microstructure and properties. To address this, we introduce a computational framework that combines physical insights with ML to develop a physics-informed continuous cooling transformation (CCT) model for steels. Our model, trained on a dataset of 4,100 diagrams, is validated against literature and experimental data. It demonstrates high computational efficiency, generating complete CCT diagrams with 100 cooling curves in under 5 seconds. It also shows strong generalizability across alloy steels, achieving phase classification F1 scores above 88% for all phases. For phase transition temperature regression, it attains mean absolute errors (MAE) below 20 °C across all phases except bainite, which shows a slightly higher MAE of 27 °C. This framework can be extended with additional generic and customized ML models to establish a universal digital twin platform for heat treatment. Integration with complementary simulation tools and targeted experiments will further support accelerated materials design workflows.

</details>


### [47] [Energy-Efficient Federated Learning via Adaptive Encoder Freezing for MRI-to-CT Conversion: A Green AI-Guided Research](https://arxiv.org/abs/2512.03054)
*Ciro Benito Raggio,Lucia Migliorelli,Nils Skupien,Mathias Krohmer Zabaleta,Oliver Blanck,Francesco Cicone,Giuseppe Lucio Cascini,Paolo Zaffino,Maria Francesca Spadea*

Main category: cs.LG

TL;DR: 提出一种面向绿色AI的自适应层冻结策略，用于联邦学习中的MRI到CT图像转换，在保持模型性能的同时减少能耗和碳排放


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然能促进医疗健康领域的平等，但其高资源需求排除了计算基础设施有限的机构，加剧了医疗不平等。需要降低联邦学习的能耗和计算负担，使其更具可持续性和包容性。

Method: 提出自适应层冻结策略：基于轮次间编码器权重的相对差异监控，选择性冻结编码器权重；采用基于耐心的机制，仅在更新持续保持最小时才进行冻结；使用CodeCarbon库追踪能耗和碳排放。

Result: 相比未冻结的对照方法，训练时间、总能耗和CO2eq排放减少高达23%；MRI到CT转换性能保持稳定，平均绝对误差仅有微小变化；5种架构中有3种无统计显著差异，2种有统计显著改善。

Conclusion: 该工作推动了满足临床需求同时确保气候、社会和经济可持续性的深度学习框架研究范式，为新型联邦学习评估框架奠定基础，促进AI驱动医疗中的隐私、公平和正义。

Abstract: Federated Learning (FL) holds the potential to advance equality in health by enabling diverse institutions to collaboratively train deep learning (DL) models, even with limited data. However, the significant resource requirements of FL often exclude centres with limited computational infrastructure, further widening existing healthcare disparities. To address this issue, we propose a Green AI-oriented adaptive layer-freezing strategy designed to reduce energy consumption and computational load while maintaining model performance. We tested our approach using different federated architectures for Magnetic Resonance Imaging (MRI)-to-Computed Tomography (CT) conversion. The proposed adaptive strategy optimises the federated training by selectively freezing the encoder weights based on the monitored relative difference of the encoder weights from round to round. A patience-based mechanism ensures that freezing only occurs when updates remain consistently minimal. The energy consumption and CO2eq emissions of the federation were tracked using the CodeCarbon library. Compared to equivalent non-frozen counterparts, our approach reduced training time, total energy consumption and CO2eq emissions by up to 23%. At the same time, the MRI-to-CT conversion performance was maintained, with only small variations in the Mean Absolute Error (MAE). Notably, for three out of the five evaluated architectures, no statistically significant differences were observed, while two architectures exhibited statistically significant improvements. Our work aligns with a research paradigm that promotes DL-based frameworks meeting clinical requirements while ensuring climatic, social, and economic sustainability. It lays the groundwork for novel FL evaluation frameworks, advancing privacy, equity and, more broadly, justice in AI-driven healthcare.

</details>


### [48] [Delta Sampling: Data-Free Knowledge Transfer Across Diffusion Models](https://arxiv.org/abs/2512.03056)
*Zhidong Gao,Zimeng Pan,Yuhang Yao,Chenyue Xie,Wei Wei*

Main category: cs.LG

TL;DR: Delta Sampling (DS) 是一种无需原始训练数据、在推理时实现跨架构扩散模型知识迁移的新方法，通过利用模型适配前后的预测差异来指导新基础模型的去噪过程。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型生态系统中的参数高效适配器（如LoRA、LyCORIS、ControlNet）与特定基础模型紧密耦合，当基础模型升级时（如从SD 1.x到2.x），由于模型参数和架构的显著变化，这些适配组件难以重用。

Method: Delta Sampling (DS) 完全在推理时操作，通过利用"delta"——模型在适配前后的预测差异，然后将这个delta用于指导新基础模型的去噪过程，实现跨不同架构基础模型的知识迁移。

Result: DS在不同Stable Diffusion版本上进行了评估，结果表明DS在不同采样策略下都能在创建期望效果（如视觉风格、语义概念和结构）方面实现一致的改进。

Conclusion: DS被证明是一种有效、即插即用的机制，可用于扩散基图像合成中的知识迁移，解决了适配组件与基础模型紧密耦合的问题。

Abstract: Diffusion models like Stable Diffusion (SD) drive a vibrant open-source ecosystem including fully fine-tuned checkpoints and parameter-efficient adapters such as LoRA, LyCORIS, and ControlNet. However, these adaptation components are tightly coupled to a specific base model, making them difficult to reuse when the base model is upgraded (e.g., from SD 1.x to 2.x) due to substantial changes in model parameters and architecture. In this work, we propose Delta Sampling (DS), a novel method that enables knowledge transfer across base models with different architectures, without requiring access to the original training data. DS operates entirely at inference time by leveraging the delta: the difference in model predictions before and after the adaptation of a base model. This delta is then used to guide the denoising process of a new base model. We evaluate DS across various SD versions, demonstrating that DS achieves consistent improvements in creating desired effects (e.g., visual styles, semantic concepts, and structures) under different sampling strategies. These results highlight DS as an effective, plug-and-play mechanism for knowledge transfer in diffusion-based image synthesis. Code:~ https://github.com/Zhidong-Gao/DeltaSampling

</details>


### [49] [Dynamical Properties of Tokens in Self-Attention and Effects of Positional Encoding](https://arxiv.org/abs/2512.03058)
*Duy-Tung Pham,An The Nguyen,Viet-Hoang Tran,Nhan-Phu Chung,Xin T. Tong,Tan M. Nguyen,Thieu N. Vo*

Main category: cs.LG

TL;DR: 研究预训练Transformer模型中token的动态特性，分析其连续时间极限下的动力学系统，提出改进Transformer架构的简单方法。


<details>
  <summary>Details</summary>
Motivation: 研究预训练Transformer模型中token的动态行为，探索如何利用这些动力学特性来改进Transformer模型性能。

Method: 分析预训练模型的连续时间极限动力学系统，表征解的渐近行为，研究不同位置编码（绝对和旋转）对动力学机制的影响，并提出减轻收敛行为的架构改进。

Result: 发现收敛情景对模型性能有负面影响，提出了适用于绝对和旋转位置编码的简单Transformer架构改进方法，能有效减轻收敛行为。

Conclusion: 研究为改进Transformer模型提供了理论基础和设计原则，提出的简单架构改进能有效提升模型性能。

Abstract: This paper investigates the dynamical properties of tokens in pre-trained Transformer models and explores their application to improving Transformers. To this end, we analyze the dynamical system governing the continuous-time limit of the pre-trained model and characterize the asymptotic behavior of its solutions. Specifically, we characterize when tokens move closer to or farther from one another over time, depending on the model parameters. We provide sufficient conditions, based on these parameters, to identify scenarios where tokens either converge to zero or diverge to infinity. Unlike prior works, our conditions are broader in scope and more applicable to real-world models. Furthermore, we investigate how different forms of positional encoding -- specifically absolute and rotary -- affect these dynamical regimes. Empirical evidence reveals that the convergence scenario adversely impacts model performance. Motivated by these insights, we propose simple refinements to Transformer architectures that mitigate convergence behavior in models with absolute or rotary positional encoding. These findings support theoretical foundations and design principles for improving Transformer models.

</details>


### [50] [Safe and Sustainable Electric Bus Charging Scheduling with Constrained Hierarchical DRL](https://arxiv.org/abs/2512.03059)
*Jiaju Qi,Lei Lei,Thorsteinn Jonsson,Dusit Niyato*

Main category: cs.LG

TL;DR: 提出一种基于分层深度强化学习的安全框架，用于优化电动公交车充电调度，在光伏发电、动态电价等多源不确定性下，实现成本最小化和安全运行。


<details>
  <summary>Details</summary>
Motivation: 电动公交车与光伏等可再生能源结合是促进可持续低碳公共交通的有前景途径，但在实际运行中，光伏发电、动态电价、可变行驶时间等多源不确定性使得优化充电调度同时确保电池不耗尽具有挑战性。

Method: 将问题建模为带约束的马尔可夫决策过程，提出DAC-MAPPO-Lagrangian分层深度强化学习算法，高层采用集中式PPO-Lagrangian学习安全充电器分配策略，低层采用MAPPO-Lagrangian在CTDE范式下学习分散式充电功率决策。

Result: 基于真实数据的实验表明，该方法在成本最小化和安全合规方面优于现有基线方法，同时保持了较快的收敛速度。

Conclusion: 提出的安全分层深度强化学习框架能够有效解决多源不确定性下的电动公交车充电调度问题，为可持续公共交通系统提供了可行的优化方案。

Abstract: The integration of Electric Buses (EBs) with renewable energy sources such as photovoltaic (PV) panels is a promising approach to promote sustainable and low-carbon public transportation. However, optimizing EB charging schedules to minimize operational costs while ensuring safe operation without battery depletion remains challenging - especially under real-world conditions, where uncertainties in PV generation, dynamic electricity prices, variable travel times, and limited charging infrastructure must be accounted for. In this paper, we propose a safe Hierarchical Deep Reinforcement Learning (HDRL) framework for solving the EB Charging Scheduling Problem (EBCSP) under multi-source uncertainties. We formulate the problem as a Constrained Markov Decision Process (CMDP) with options to enable temporally abstract decision-making. We develop a novel HDRL algorithm, namely Double Actor-Critic Multi-Agent Proximal Policy Optimization Lagrangian (DAC-MAPPO-Lagrangian), which integrates Lagrangian relaxation into the Double Actor-Critic (DAC) framework. At the high level, we adopt a centralized PPO-Lagrangian algorithm to learn safe charger allocation policies. At the low level, we incorporate MAPPO-Lagrangian to learn decentralized charging power decisions under the Centralized Training and Decentralized Execution (CTDE) paradigm. Extensive experiments with real-world data demonstrate that the proposed approach outperforms existing baselines in both cost minimization and safety compliance, while maintaining fast convergence speed.

</details>


### [51] [A Large Scale Heterogeneous Treatment Effect Estimation Framework and Its Applications of Users' Journey at Snap](https://arxiv.org/abs/2512.03060)
*Jing Pan,Li Shi,Paul Lo*

Main category: cs.LG

TL;DR: 本文提出了一个大规模工业框架，用于使用来自数亿Snapchat用户的实验数据估计异质性处理效应（HTE），通过结合多个实验结果来发现潜在用户特征并产生稳定的处理效应估计。


<details>
  <summary>Details</summary>
Motivation: 传统方法假设处理效应对所有用户相同，但实际中用户对处理（如广告）的反应存在异质性。需要在大规模工业环境中准确估计异质性处理效应，以更好地理解用户特征并优化业务决策。

Method: 开发了一个大规模工业框架，包括实验选择、基础学习器设计和增量训练等核心组件。通过结合数百个实验的数据，使用机器学习方法估计条件平均处理效应（CATE），发现潜在用户特征。

Result: 框架成功应用于两个具体场景：用户对广告的影响性和用户对广告的敏感性。使用影响性分数进行定向的在线A/B测试显示，关键业务指标的改善幅度超过通常认为显著水平的六倍以上。

Conclusion: 该大规模HTE估计框架能够有效发现先前无法测量的潜在用户特征，产生稳定的处理效应估计，并在实际应用中显著改善业务指标，证明了在大规模工业环境中估计异质性处理效应的可行性和价值。

Abstract: Heterogeneous Treatment Effect (HTE) and Conditional Average Treatment Effect (CATE) models relax the assumption that treatment effects are the same for every user. We present a large scale industrial framework for estimating HTE using experimental data from hundreds of millions of Snapchat users. By combining results across many experiments, the framework uncovers latent user characteristics that were previously unmeasurable and produces stable treatment effect estimates at scale.
  We describe the core components that enabled this system, including experiment selection, base learner design, and incremental training. We also highlight two applications: user influenceability to ads and user sensitivity to ads. An online A/B test using influenceability scores for targeting showed an improvement on key business metrics that is more than six times larger than what is typically considered significant.

</details>


### [52] [Globally optimized SVD compression of LLMs via Fermi-function-based rank selection and gauge fixing](https://arxiv.org/abs/2512.03062)
*Roman Rausch,David Jansen,Sukhbinder Singh,Román Orús*

Main category: cs.LG

TL;DR: 提出两种基于物理启发的SVD LLM压缩改进方法：FermiGrad算法通过费米函数将离散奇异值截断转化为连续优化，确定全局最优的层间秩；PivGa利用参数化中的固有规范自由度对低秩因子进行无损压缩。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型对计算资源需求极高，低秩分解（如SVD）是LLM压缩的有前景方法，但面临层间秩选择和参数冗余消除等实际挑战。

Method: 提出两种改进：1) FermiGrad - 使用费米函数将离散奇异值截断松弛为连续优化，通过梯度下降确定全局最优层间秩；2) PivGa - 利用低秩因子参数化中的固有规范自由度进行无损压缩。

Result: 未在摘要中明确说明具体实验结果，但提出的方法旨在解决SVD LLM压缩中的实际障碍，包括层间秩选择和参数冗余问题。

Conclusion: 通过物理启发的FermiGrad和PivGa方法，改进了基于SVD的LLM压缩技术，能够更有效地确定最优压缩参数并减少冗余。

Abstract: Large Language Models (LLMs) are very demanding in terms of their computational resources. Low-rank decompositions of LLM weights, e.g. via Singular Value Decomposition (SVD), is a promising approach for LLM compression, but presents several practical hurdles, e.g. selecting appropriate layer-wise ranks and getting rid of its parameter redundancy. In this work, we present two physics-inspired improvements to SVD LLM compression: (1) \textbf{FermiGrad}, a gradient-descent algorithm that determines globally optimal layer-wise ranks by relaxing the discrete singular-value truncation into a continuous optimization using the Fermi function; (2) \textbf{PivGa}, an additional \textit{lossless} compression of the low-rank factors that exploits the intrinsic gauge freedom in their parametrization.

</details>


### [53] [Hierarchical clustering of complex energy systems using pretopology](https://arxiv.org/abs/2512.03069)
*Loup-Noe Levy,Jeremie Bosom,Guillaume Guerard,Soufian Ben Amor,Marc Bui,Hai Tran*

Main category: cs.LG

TL;DR: 本文提出使用预拓扑学建模建筑能耗分布，开发基于预拓扑空间特性的多准则分层分类算法，用于大规模建筑能耗优化管理。


<details>
  <summary>Details</summary>
Motivation: 对成千上万个建筑进行逐个深度能耗审计需要大量时间、资金和专业人员，因此需要开发自动化方法来建立有效的能耗推荐系统。

Method: 使用预拓扑学建模站点能耗分布，开发基于预拓扑空间特性的多准则分层分类算法，并实现为Python库。

Result: 在三个数据集上评估：2D空间点集、生成的时间序列和400个真实能耗站点的数据。算法能识别空间点簇和基于皮尔逊相关性的时间序列簇，调整兰德指数达到1。

Conclusion: 该方法能有效建模和分类大规模分布式区域的能耗分布，为建筑能耗优化管理提供自动化解决方案。

Abstract: This article attempts answering the following problematic: How to model and classify energy consumption profiles over a large distributed territory to optimize the management of buildings' consumption?
  Doing case-by-case in depth auditing of thousands of buildings would require a massive amount of time and money as well as a significant number of qualified people. Thus, an automated method must be developed to establish a relevant and effective recommendations system.
  To answer this problematic, pretopology is used to model the sites' consumption profiles and a multi-criterion hierarchical classification algorithm, using the properties of pretopological space, has been developed in a Python library.
  To evaluate the results, three data sets are used: A generated set of dots of various sizes in a 2D space, a generated set of time series and a set of consumption time series of 400 real consumption sites from a French Energy company.
  On the point data set, the algorithm is able to identify the clusters of points using their position in space and their size as parameter. On the generated time series, the algorithm is able to identify the time series clusters using Pearson's correlation with an Adjusted Rand Index (ARI) of 1.

</details>


### [54] [Mixed Data Clustering Survey and Challenges](https://arxiv.org/abs/2512.03070)
*Guillaume Guerard,Sonia Djebali*

Main category: cs.LG

TL;DR: 本文提出了一种基于预拓扑空间的混合数据聚类方法，用于处理大数据环境下的数值和分类变量混合数据集，并与传统聚类算法进行性能比较。


<details>
  <summary>Details</summary>
Motivation: 大数据时代带来了数据量、速度和多样性的挑战，混合数据聚类成为关键问题。传统聚类方法通常针对同质数据集设计，难以有效处理数值和分类变量混合的复杂数据，需要专门针对混合数据环境的创新方法。

Method: 提出了一种基于预拓扑空间的聚类方法，该方法能够处理混合数据类型（数值和分类变量），并采用分层和可解释的算法框架，提供结构化和可解释的聚类结果。

Result: 通过与传统数值聚类算法和现有预拓扑方法进行基准测试，评估了所提出方法的性能和有效性，验证了其在大数据范式下的适用性。

Conclusion: 基于预拓扑空间的聚类方法为解决大数据环境下的混合数据聚类问题提供了有效的解决方案，能够处理数据异质性并提供可解释的聚类结果，支持基于数据的决策制定。

Abstract: The advent of the big data paradigm has transformed how industries manage and analyze information, ushering in an era of unprecedented data volume, velocity, and variety. Within this landscape, mixed-data clustering has become a critical challenge, requiring innovative methods that can effectively exploit heterogeneous data types, including numerical and categorical variables. Traditional clustering techniques, typically designed for homogeneous datasets, often struggle to capture the additional complexity introduced by mixed data, underscoring the need for approaches specifically tailored to this setting. Hierarchical and explainable algorithms are particularly valuable in this context, as they provide structured, interpretable clustering results that support informed decision-making. This paper introduces a clustering method grounded in pretopological spaces. In addition, benchmarking against classical numerical clustering algorithms and existing pretopological approaches yields insights into the performance and effectiveness of the proposed method within the big data paradigm.

</details>


### [55] [PretopoMD: Pretopology-based Mixed Data Hierarchical Clustering](https://arxiv.org/abs/2512.03071)
*Loup-Noe Levy,Guillaume Guerard,Sonia Djebali,Soufian Ben Amor*

Main category: cs.LG

TL;DR: 提出基于预拓扑的新算法，无需降维即可聚类混合数据，使用析取范式构建可定制逻辑规则，通过层次树状图分析实现可解释的聚类结果。


<details>
  <summary>Details</summary>
Motivation: 解决混合数据聚类挑战，避免传统降维技术导致的信息损失，提高聚类结果的可解释性，为异构数据集提供定制化解决方案。

Method: 基于预拓扑的算法，利用析取范式构建可定制的逻辑规则和可调整的超参数，支持用户定义的层次聚类构建，直接从原始数据中识别聚类结构。

Result: 算法在保持数据完整性的同时，通过层次树状图分析和聚类指标比较，展现出优越性能，能够准确且可解释地划分聚类，构建有意义的聚类结构。

Conclusion: 该工作创新性地避免了传统降维技术，利用逻辑规则增强聚类形成和清晰度，为混合数据聚类领域贡献了重要进展，特别是在聚类可解释性方面具有潜力。

Abstract: This article presents a novel pretopology-based algorithm designed to address the challenges of clustering mixed data without the need for dimensionality reduction. Leveraging Disjunctive Normal Form, our approach formulates customizable logical rules and adjustable hyperparameters that allow for user-defined hierarchical cluster construction and facilitate tailored solutions for heterogeneous datasets. Through hierarchical dendrogram analysis and comparative clustering metrics, our method demonstrates superior performance by accurately and interpretably delineating clusters directly from raw data, thus preserving data integrity. Empirical findings highlight the algorithm's robustness in constructing meaningful clusters and reveal its potential in overcoming issues related to clustered data explainability. The novelty of this work lies in its departure from traditional dimensionality reduction techniques and its innovative use of logical rules that enhance both cluster formation and clarity, thereby contributing a significant advancement to the discourse on clustering mixed data.

</details>


### [56] [Risk-Entropic Flow Matching](https://arxiv.org/abs/2512.03078)
*Vahid R. Ramezani,Benjamin Englard*

Main category: cs.LG

TL;DR: 本文提出将倾斜（熵）风险应用于流匹配（FM），通过log-exponential变换改进标准FM损失，以更好地捕捉数据流形中的高阶统计信息和几何结构。


<details>
  <summary>Details</summary>
Motivation: 标准流匹配（FM）使用均方误差损失训练速度场，将所有到达同一时空点的速度目标压缩为单一条件均值，忽略了高阶条件信息（方差、偏度、多模态），这些信息编码了数据流形和少数分支的精细几何结构。

Method: 将标准风险敏感（log-exponential）变换应用于条件FM损失，得到倾斜风险损失，该损失是每个时空点上有意义的条件熵FM目标的上界。通过小阶展开，得到两个可解释的一阶修正项：FM残差的协方差预处理和偏好不对称或稀有分支的偏尾项。

Result: 在专门设计用于探测模糊性和尾部的合成数据上，风险敏感损失在统计指标上优于标准整流FM，并能更忠实地恢复几何结构。

Conclusion: 倾斜风险损失为流匹配提供了一种自然框架，能够更好地捕捉数据流形的高阶统计特性和几何结构，特别适用于处理模糊性和稀有事件。

Abstract: Tilted (entropic) risk, obtained by applying a log-exponential transform to a base loss, is a well established tool in statistics and machine learning for emphasizing rare or high loss events while retaining a tractable optimization problem. In this work, our aim is to interpret its structure for Flow Matching (FM). FM learns a velocity field that transports samples from a simple source distribution to data by integrating an ODE. In rectified FM, training pairs are obtained by linearly interpolating between a source sample and a data sample, and a neural velocity field is trained to predict the straight line displacement using a mean squared error loss. This squared loss collapses all velocity targets that reach the same space-time point into a single conditional mean, thereby ignoring higher order conditional information (variance, skewness, multi-modality) that encodes fine geometric structure about the data manifold and minority branches. We apply the standard risk-sensitive (log-exponential) transform to the conditional FM loss and show that the resulting tilted risk loss is a natural upper-bound on a meaningful conditional entropic FM objective defined at each space-time point. Furthermore, we show that a small order expansion of the gradient of this conditional entropic objective yields two interpretable first order corrections: covariance preconditioning of the FM residual, and a skew tail term that favors asymmetric or rare branches. On synthetic data designed to probe ambiguity and tails, the resulting risk-sensitive loss improves statistical metrics and recovers geometric structure more faithfully than standard rectified FM.

</details>


### [57] [ALARM: Automated MLLM-Based Anomaly Detection in Complex-EnviRonment Monitoring with Uncertainty Quantification](https://arxiv.org/abs/2512.03101)
*Congjing Zhang,Feng Lin,Xinyi Zhao,Pei Guo,Wei Li,Lin Chen,Chaoyue Zhao,Shuai Huang*

Main category: cs.LG

TL;DR: 本文提出了ALARM框架，这是一个支持不确定性量化的多模态大语言模型视觉异常检测系统，通过集成推理链、自我反思和模型集成等技术，在复杂环境中实现可靠决策。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，基于多模态大语言模型的视觉异常检测在复杂环境中的应用需求增加。然而，复杂环境中的异常往往具有高度上下文相关性和模糊性，因此不确定性量化成为这类系统成功的关键能力。

Method: ALARM框架将不确定性量化与质量保证技术（如推理链、自我反思和多模态大语言模型集成）相结合，基于严格的概率推理流程和计算过程设计，实现鲁棒且准确的性能。

Result: 通过在真实世界的智能家居基准数据和伤口图像分类数据上进行广泛实证评估，结果显示ALARM具有优越的性能，并在不同领域展现出通用的适用性，能够支持可靠决策。

Conclusion: ALARM框架成功地将不确定性量化集成到多模态大语言模型视觉异常检测中，为复杂环境下的可靠异常检测提供了有效的解决方案，具有跨领域的通用应用潜力。

Abstract: The advance of Large Language Models (LLMs) has greatly stimulated research interest in developing multi-modal LLM (MLLM)-based visual anomaly detection (VAD) algorithms that can be deployed in complex environments. The challenge is that in these complex environments, the anomalies are sometimes highly contextual and also ambiguous, and thereby, uncertainty quantification (UQ) is a crucial capacity for an MLLM-based VAD system to succeed. In this paper, we introduce our UQ-supported MLLM-based VAD framework called ALARM. ALARM integrates UQ with quality-assurance techniques like reasoning chain, self-reflection, and MLLM ensemble for robust and accurate performance and is designed based on a rigorous probabilistic inference pipeline and computational process. Extensive empirical evaluations are conducted using the real-world smart-home benchmark data and wound image classification data, which shows ALARM's superior performance and its generic applicability across different domains for reliable decision-making.

</details>


### [58] [Detecting AI Hallucinations in Finance: An Information-Theoretic Method Cuts Hallucination Rate by 92%](https://arxiv.org/abs/2512.03107)
*Mainak Singha*

Main category: cs.LG

TL;DR: ECLIPSE框架通过结合语义熵估计和困惑度分解来检测大语言模型的幻觉，将幻觉视为模型语义熵与可用证据容量之间的不匹配。


<details>
  <summary>Details</summary>
Motivation: 大语言模型会产生流畅但无根据的答案（幻觉），这限制了其在高风险领域的安全部署。需要一种机制来检测和减少幻觉。

Method: 提出ECLIPSE框架，将幻觉视为模型语义熵与可用证据容量之间的不匹配。结合多样本聚类的熵估计和新的困惑度分解方法，测量模型如何使用检索到的证据。

Result: 在受控金融问答数据集上，ECLIPSE达到ROC AUC 0.89和平均精度0.90，显著优于仅使用语义熵的基线（AUC 0.50）。在Claude-3-Haiku上的消融实验显示AUC降至0.59，系数幅度下降95%，证明ECLIPSE是依赖校准的token级不确定性的logprob原生机制。

Conclusion: ECLIPSE是一种有效的幻觉检测机制，其有效性依赖于校准的token级不确定性。困惑度分解特征具有最大的学习系数，证实证据利用是幻觉检测的核心。该研究是受控机制研究，跨领域和自然发生幻觉的更广泛验证是未来工作。

Abstract: Large language models (LLMs) produce fluent but unsupported answers - hallucinations - limiting safe deployment in high-stakes domains. We propose ECLIPSE, a framework that treats hallucination as a mismatch between a model's semantic entropy and the capacity of available evidence. We combine entropy estimation via multi-sample clustering with a novel perplexity decomposition that measures how models use retrieved evidence. We prove that under mild conditions, the resulting entropy-capacity objective is strictly convex with a unique stable optimum. We evaluate on a controlled financial question answering dataset with GPT-3.5-turbo (n=200 balanced samples with synthetic hallucinations), where ECLIPSE achieves ROC AUC of 0.89 and average precision of 0.90, substantially outperforming a semantic entropy-only baseline (AUC 0.50). A controlled ablation with Claude-3-Haiku, which lacks token-level log probabilities, shows AUC dropping to 0.59 with coefficient magnitudes decreasing by 95% - demonstrating that ECLIPSE is a logprob-native mechanism whose effectiveness depends on calibrated token-level uncertainties. The perplexity decomposition features exhibit the largest learned coefficients, confirming that evidence utilization is central to hallucination detection. We position this work as a controlled mechanism study; broader validation across domains and naturally occurring hallucinations remains future work.

</details>


### [59] [E-valuator: Reliable Agent Verifiers with Sequential Hypothesis Testing](https://arxiv.org/abs/2512.03109)
*Shuvom Sadhuka,Drew Prinster,Clara Fannjiang,Gabriele Scalia,Aviv Regev,Hanchen Wang*

Main category: cs.LG

TL;DR: e-valuator：一种将任意黑盒验证器评分转换为具有可证明误报率控制的决策规则的方法，用于评估智能体轨迹的成功概率。


<details>
  <summary>Details</summary>
Motivation: 当前评估智能体AI系统轨迹的验证器（如LLM评判器和过程奖励模型）虽然能提供启发式评分，但缺乏正确性保证，无法确保智能体最终会产生成功输出。

Method: 将区分成功轨迹与失败轨迹的问题构建为序贯假设检验问题，基于e-processes工具开发序贯假设检验，在智能体轨迹的每一步都保持统计有效性，支持对任意长动作序列的在线监控。

Result: 在6个数据集和3种智能体上的实验表明，e-valuator相比其他策略具有更高的统计功效和更好的误报率控制，并能快速终止问题轨迹以节省token消耗。

Conclusion: e-valuator提供了一个轻量级、模型无关的框架，将验证器启发式方法转换为具有统计保证的决策规则，使能部署更可靠的智能体系统。

Abstract: Agentic AI systems execute a sequence of actions, such as reasoning steps or tool calls, in response to a user prompt. To evaluate the success of their trajectories, researchers have developed verifiers, such as LLM judges and process-reward models, to score the quality of each action in an agent's trajectory. Although these heuristic scores can be informative, there are no guarantees of correctness when used to decide whether an agent will yield a successful output. Here, we introduce e-valuator, a method to convert any black-box verifier score into a decision rule with provable control of false alarm rates. We frame the problem of distinguishing successful trajectories (that is, a sequence of actions that will lead to a correct response to the user's prompt) and unsuccessful trajectories as a sequential hypothesis testing problem. E-valuator builds on tools from e-processes to develop a sequential hypothesis test that remains statistically valid at every step of an agent's trajectory, enabling online monitoring of agents over arbitrarily long sequences of actions. Empirically, we demonstrate that e-valuator provides greater statistical power and better false alarm rate control than other strategies across six datasets and three agents. We additionally show that e-valuator can be used for to quickly terminate problematic trajectories and save tokens. Together, e-valuator provides a lightweight, model-agnostic framework that converts verifier heuristics into decisions rules with statistical guarantees, enabling the deployment of more reliable agentic systems.

</details>


### [60] [Beyond Additivity: Sparse Isotonic Shapley Regression toward Nonlinear Explainability](https://arxiv.org/abs/2512.03112)
*Jialai She*

Main category: cs.LG

TL;DR: SISR框架通过同时学习单调变换恢复可加性和施加L0稀疏约束，解决了Shapley值在非可加性场景下的失真问题和高维稀疏解释的计算难题。


<details>
  <summary>Details</summary>
Motivation: 传统Shapley值框架假设可加性，但现实世界中的收益函数常违反此假设（非高斯分布、重尾、特征依赖等），导致归因失真；同时在高维空间中计算密集Shapley值再进行阈值处理成本过高且不一致。

Method: 提出Sparse Isotonic Shapley Regression (SISR)框架：同时学习单调变换恢复可加性（无需闭式指定）并施加L0稀疏约束；采用Pool-Adjacent-Violators进行高效保序回归和归一化硬阈值进行支持选择。

Result: SISR能在多种场景下恢复真实变换，在高噪声下实现强支持恢复；实验表明SISR能稳定不同收益方案下的归因，正确过滤无关特征，而标准Shapley值存在严重的排序和符号失真。

Conclusion: SISR通过统一非线性变换估计和稀疏性追求，推进了非线性可解释性的前沿，提供了理论可靠且实用的归因框架，首次证明无关特征和特征间依赖可导致收益函数显著偏离线性。

Abstract: Shapley values, a gold standard for feature attribution in Explainable AI, face two primary challenges. First, the canonical Shapley framework assumes that the worth function is additive, yet real-world payoff constructions--driven by non-Gaussian distributions, heavy tails, feature dependence, or domain-specific loss scales--often violate this assumption, leading to distorted attributions. Secondly, achieving sparse explanations in high dimensions by computing dense Shapley values and then applying ad hoc thresholding is prohibitively costly and risks inconsistency. We introduce Sparse Isotonic Shapley Regression (SISR), a unified nonlinear explanation framework. SISR simultaneously learns a monotonic transformation to restore additivity--obviating the need for a closed-form specification--and enforces an L0 sparsity constraint on the Shapley vector, enhancing computational efficiency in large feature spaces. Its optimization algorithm leverages Pool-Adjacent-Violators for efficient isotonic regression and normalized hard-thresholding for support selection, yielding implementation ease and global convergence guarantees. Analysis shows that SISR recovers the true transformation in a wide range of scenarios and achieves strong support recovery even in high noise. Moreover, we are the first to demonstrate that irrelevant features and inter-feature dependencies can induce a true payoff transformation that deviates substantially from linearity. Experiments in regression, logistic regression, and tree ensembles demonstrate that SISR stabilizes attributions across payoff schemes, correctly filters irrelevant features while standard Shapley values suffer severe rank and sign distortions. By unifying nonlinear transformation estimation with sparsity pursuit, SISR advances the frontier of nonlinear explainability, providing a theoretically grounded and practical attribution framework.

</details>


### [61] [Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models](https://arxiv.org/abs/2512.03125)
*Xiwen Wei,Mustafa Munir,Radu Marculescu*

Main category: cs.LG

TL;DR: MoDE提出了一种轻量级可扩展架构，通过解耦模态特定更新来缓解梯度冲突，有效解决了统一多模态生成模型中的模态间和模态内灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 统一多模态生成模型（UMGMs）在单一自回归框架中统一了视觉理解和图像生成，但其持续学习新任务的能力受到灾难性遗忘的严重阻碍，包括模态内遗忘和模态间遗忘。虽然模态内遗忘在先前持续学习研究中已有探讨，但模态间遗忘问题尚未得到充分研究。

Method: 提出Modality-Decoupled Experts（MoDE）架构：1）通过隔离模态特定更新来缓解模态梯度冲突；2）利用知识蒸馏防止灾难性遗忘并保留预训练能力；3）与先前保持模态耦合的方法不同，MoDE显式解耦模态以防止干扰。

Result: 在多样化基准测试中，MoDE显著缓解了模态间和模态内遗忘，在统一多模态生成设置中优于先前的持续学习基线方法。

Conclusion: MoDE通过解耦模态特定更新有效解决了UMGMs中的模态间和模态内灾难性遗忘问题，为统一多模态生成模型的持续学习提供了轻量级、可扩展的解决方案。

Abstract: Unified Multimodal Generative Models (UMGMs) unify visual understanding and image generation within a single autoregressive framework. However, their ability to continually learn new tasks is severely hindered by catastrophic forgetting, both within a modality (intra-modal) and across modalities (inter-modal). While intra-modal forgetting has been studied in prior continual learning (CL) work, inter-modal forgetting remains largely unexplored. In this paper, we identify and empirically validate this phenomenon in UMGMs and provide a theoretical explanation rooted in gradient conflict between modalities. To address both intra- and inter-modal forgetting, we propose Modality-Decoupled Experts (MoDE), a lightweight and scalable architecture that isolates modality-specific updates to mitigate the gradient conflict and leverages knowledge distillation to prevent catastrophic forgetting and preserve pre-trained capabilities. Unlike previous CL methods that remain modality-coupled and suffer from modality gradient conflict, MoDE explicitly decouples modalities to prevent interference. Experiments across diverse benchmarks demonstrate that MoDE significantly mitigates both inter- and intra-modal forgetting, outperforming prior CL baselines in unified multimodal generation settings. Codes will be publicly available: https://github.com/Christina200/MoDE-official.git

</details>


### [62] [Atomic Diffusion Models for Small Molecule Structure Elucidation from NMR Spectra](https://arxiv.org/abs/2512.03127)
*Ziyu Xiong,Yichi Zhang,Foyez Alauddin,Chu Xin Cheng,Joon Soo An,Mohammad R. Seyedsayamdost,Ellen D. Zhong*

Main category: cs.LG

TL;DR: ChefNMR是一个端到端框架，直接从1D NMR谱和化学式预测未知分子结构，使用原子扩散模型，在天然产物结构预测上达到超过65%的准确率。


<details>
  <summary>Details</summary>
Motivation: NMR谱解释是耗时且需要专业知识的传统过程，特别是在天然产物和临床治疗药物发现中，需要自动化工具来加速分子结构解析。

Method: 将结构解析构建为条件生成问题，使用基于非等变transformer架构的原子扩散模型，创建了包含11.1万种天然产物的模拟1D NMR谱数据集进行训练。

Result: ChefNMR在具有挑战性的天然产物化合物结构预测中达到了超过65%的准确率，超越了现有方法，显著推进了小分子结构自动解析的进程。

Conclusion: 该研究在自动化小分子结构解析这一重大挑战上迈出了重要一步，展示了深度学习在加速分子发现方面的潜力，为天然产物研究提供了强大的计算工具。

Abstract: Nuclear Magnetic Resonance (NMR) spectroscopy is a cornerstone technique for determining the structures of small molecules and is especially critical in the discovery of novel natural products and clinical therapeutics. Yet, interpreting NMR spectra remains a time-consuming, manual process requiring extensive domain expertise. We introduce ChefNMR (CHemical Elucidation From NMR), an end-to-end framework that directly predicts an unknown molecule's structure solely from its 1D NMR spectra and chemical formula. We frame structure elucidation as conditional generation from an atomic diffusion model built on a non-equivariant transformer architecture. To model the complex chemical groups found in natural products, we generated a dataset of simulated 1D NMR spectra for over 111,000 natural products. ChefNMR predicts the structures of challenging natural product compounds with an unsurpassed accuracy of over 65%. This work takes a significant step toward solving the grand challenge of automating small-molecule structure elucidation and highlights the potential of deep learning in accelerating molecular discovery. Code is available at https://github.com/ml-struct-bio/chefnmr.

</details>


### [63] [Contrastive Deep Learning for Variant Detection in Wastewater Genomic Sequencing](https://arxiv.org/abs/2512.03158)
*Adele Chinda,Richmond Azumah,Hemanth Demakethepalli Venkateswara*

Main category: cs.LG

TL;DR: 提出基于VQ-VAE的无监督病毒变异检测框架，用于废水基因组监测，无需参考基因组或变异标签，在SARS-CoV-2废水测序数据上取得高准确率。


<details>
  <summary>Details</summary>
Motivation: 废水基因组监测面临高测序噪声、低病毒覆盖率、片段化读取和缺乏变异标注等计算挑战，传统基于参考的变异检测方法难以处理新突变且计算资源需求大。

Method: 使用向量量化变分自编码器(VQ-VAE)从k-mer标记化序列中学习基因组模式的离散码本，无需参考基因组或变异标签。扩展基础架构包括掩码重建预训练以增强对缺失数据的鲁棒性，以及对比学习以获得高判别性嵌入。

Result: 在约10万条SARS-CoV-2废水测序数据上，VQ-VAE达到99.52%的平均标记级准确率和56.33%的精确序列匹配率，同时保持19.73%的码本利用率。对比微调显著改善聚类效果：64维嵌入的轮廓系数提升35%，128维嵌入提升42%。

Conclusion: 该无参考框架为基因组监测提供了可扩展、可解释的方法，可直接应用于公共卫生监测，有效解决了废水基因组监测中的计算挑战。

Abstract: Wastewater-based genomic surveillance has emerged as a powerful tool for population-level viral monitoring, offering comprehensive insights into circulating viral variants across entire communities. However, this approach faces significant computational challenges stemming from high sequencing noise, low viral coverage, fragmented reads, and the complete absence of labeled variant annotations. Traditional reference-based variant calling pipelines struggle with novel mutations and require extensive computational resources. We present a comprehensive framework for unsupervised viral variant detection using Vector-Quantized Variational Autoencoders (VQ-VAE) that learns discrete codebooks of genomic patterns from k-mer tokenized sequences without requiring reference genomes or variant labels. Our approach extends the base VQ-VAE architecture with masked reconstruction pretraining for robustness to missing data and contrastive learning for highly discriminative embeddings. Evaluated on SARS-CoV-2 wastewater sequencing data comprising approximately 100,000 reads, our VQ-VAE achieves 99.52% mean token-level accuracy and 56.33% exact sequence match rate while maintaining 19.73% codebook utilization (101 of 512 codes active), demonstrating efficient discrete representation learning. Contrastive fine-tuning with different projection dimensions yields substantial clustering improvements: 64-dimensional embeddings achieve +35% Silhouette score improvement (0.31 to 0.42), while 128-dimensional embeddings achieve +42% improvement (0.31 to 0.44), clearly demonstrating the impact of embedding dimensionality on variant discrimination capability. Our reference-free framework provides a scalable, interpretable approach to genomic surveillance with direct applications to public health monitoring.

</details>


### [64] [Plantain: Plan-Answer Interleaved Reasoning](https://arxiv.org/abs/2512.03176)
*Anthony Liang,Jonathan Berant,Adam Fisch,Abhimanyu Goyal,Kalpesh Krishna,Jacob Eisenstein*

Main category: cs.LG

TL;DR: 论文提出"交错推理"方法，让语言模型在思考过程中交替输出中间结果，替代传统的"先思考后回答"模式，从而减少用户感知延迟并允许早期干预。


<details>
  <summary>Details</summary>
Motivation: 传统推理模型在思考过程中不给用户任何提示，用户无法知道推理是否正确进行，也无法在推理出现错误时及时干预纠正，导致时间和体验浪费。相比之下，人类对话中会通过轻量级的增量确认来确保参与者理解一致。

Method: 提出交错推理方法，模型在思考过程中交替输出中间响应；进一步提出Plantain方法，首先生成明确的逐步执行计划作为第一个中间响应，允许用户干预并为后续推理步骤提供早期反馈。

Result: Plantain方法在多个数学推理和编程基准测试中实现了约6%的pass@1提升，同时将首次响应时间相对于"先思考后回答"基线减少了60%以上。

Conclusion: 交错推理方法能够显著改善用户体验，减少感知延迟，同时提高任务完成质量，为语言模型推理提供了更有效的人机交互范式。

Abstract: Reasoning models often spend a significant amount of time thinking before they generate a visible response. In the meantime, they do not give the user any hints as to whether their reasoning is on the right track, and do not give the user any recourse to stop and correct them if their reasoning is flawed. This creates a frustrating, but unfortunately common, experience: the user's time is wasted while the model reasons from a false premise that could have easily been corrected. In contrast, human speakers typically perform lightweight, incremental grounding acts to ensure that participants in the conversation are on the same page; here we ask if language models can learn to leverage a similar type of behavior? With this motivation, we propose interleaved reasoning (IR), in which the model alternates between thinking and surfacing intermediate responses, as an alternative to the standard "think-then-answer" approach. By providing useful information to the user earlier, IR reduces perceived latency, the time a user waits for an initial output, without compromising the quality of the final response. We further introduce a specialization of interleaved reasoning, Plantain (Plan-Thought-Answer Interleaving), where the first intermediate response is an explicit, step-by-step plan for executing the task. This plan-first strategy allows for user intervention and early feedback for subsequent reasoning steps. We demonstrate that Plantain yields an ~6% improvement in pass@1 across several challenging math reasoning and coding benchmarks, while reducing time-to-first-response by over 60% relative to think-then-answer baselines.

</details>


### [65] [Neighborhood density estimation using space-partitioning based hashing schemes](https://arxiv.org/abs/2512.03187)
*Aashi Jindal*

Main category: cs.LG

TL;DR: FiRE/FiRE.1是一种基于草图技术的异常检测算法，用于快速识别大规模单细胞RNA测序数据中的罕见细胞亚群；Enhash是一种快速资源高效的集成学习器，使用投影哈希检测流数据中的概念漂移。


<details>
  <summary>Details</summary>
Motivation: 需要快速有效地识别大规模单细胞RNA测序数据中的罕见细胞亚群，以及检测流数据中的概念漂移问题。

Method: FiRE/FiRE.1采用基于草图的算法进行异常检测；Enhash使用投影哈希的集成学习方法检测概念漂移。

Result: FiRE/FiRE.1在识别罕见细胞亚群方面表现出优于现有技术的性能；Enhash在各种漂移类型中在时间和准确性方面都表现出高度竞争力。

Conclusion: 提出的两种方法分别在单细胞RNA测序异常检测和流数据概念漂移检测方面提供了高效且性能优越的解决方案。

Abstract: This work introduces FiRE/FiRE.1, a novel sketching-based algorithm for anomaly detection to quickly identify rare cell sub-populations in large-scale single-cell RNA sequencing data. This method demonstrated superior performance against state-of-the-art techniques. Furthermore, the thesis proposes Enhash, a fast and resource-efficient ensemble learner that uses projection hashing to detect concept drift in streaming data, proving highly competitive in time and accuracy across various drift types.

</details>


### [66] [Scaling Internal-State Policy-Gradient Methods for POMDPs](https://arxiv.org/abs/2512.03204)
*Douglas Aberdeen,Jonathan Baxter*

Main category: cs.LG

TL;DR: 本文改进了在部分可观测环境中学习带记忆策略的算法，包括已知模型和模拟两种情况，并在大型POMDP问题上进行了测试。


<details>
  <summary>Details</summary>
Motivation: 策略梯度方法在部分可观测环境中学习无记忆策略已显示出潜力，但在需要记忆的情况下效果不佳，因此需要开发改进的带记忆策略学习算法。

Method: 开发了多种改进算法，用于在无限时域设置中学习带记忆策略：当环境模型已知时直接学习，否则通过模拟学习。

Result: 在大型POMDP问题上进行了比较测试，包括噪声机器人导航和多智能体问题。

Conclusion: 提出了改进的带记忆策略学习算法，为部分可观测环境中需要记忆的复杂问题提供了更有效的解决方案。

Abstract: Policy-gradient methods have received increased attention recently as a mechanism for learning to act in partially observable environments. They have shown promise for problems admitting memoryless policies but have been less successful when memory is required. In this paper we develop several improved algorithms for learning policies with memory in an infinite-horizon setting -- directly when a known model of the environment is available, and via simulation otherwise. We compare these algorithms on some large POMDPs, including noisy robot navigation and multi-agent problems.

</details>


### [67] [Perch 2.0 transfers 'whale' to underwater tasks](https://arxiv.org/abs/2512.03219)
*Andrea Burns,Lauren Harrell,Bart van Merriënboer,Vincent Dumoulin,Jenny Hamer,Tom Denton*

Main category: cs.LG

TL;DR: Perch 2.0作为生物声学基础模型，在海洋哺乳动物音频任务上通过少样本迁移学习表现出色，优于其他预训练模型


<details>
  <summary>Details</summary>
Motivation: 评估Perch 2.0基础模型在海洋哺乳动物和水下音频任务上的性能，尽管其训练数据中几乎没有包含这类数据

Method: 使用Perch 2.0生成的嵌入进行线性探测，并与多种预训练生物声学模型（包括Perch 1.0、SurfPerch、AVES-bio等）进行少样本迁移学习性能比较

Result: Perch 2.0的嵌入在少样本迁移学习中表现一致优秀，在大多数任务上优于其他嵌入模型

Conclusion: 当开发海洋哺乳动物分类的线性分类器且只有少量标注样本时，推荐使用Perch 2.0模型

Abstract: Perch 2.0 is a supervised bioacoustics foundation model pretrained on 14,597 species, including birds, mammals, amphibians, and insects, and has state-of-the-art performance on multiple benchmarks. Given that Perch 2.0 includes almost no marine mammal audio or classes in the training data, we evaluate Perch 2.0 performance on marine mammal and underwater audio tasks through few-shot transfer learning. We perform linear probing with the embeddings generated from this foundation model and compare performance to other pretrained bioacoustics models. In particular, we compare Perch 2.0 with previous multispecies whale, Perch 1.0, SurfPerch, AVES-bio, BirdAVES, and Birdnet V2.3 models, which have open-source tools for transfer-learning and agile modeling. We show that the embeddings from the Perch 2.0 model have consistently high performance for few-shot transfer learning, generally outperforming alternative embedding models on the majority of tasks, and thus is recommended when developing new linear classifiers for marine mammal classification with few labeled examples.

</details>


### [68] [SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning](https://arxiv.org/abs/2512.03244)
*Salman Rahman,Sruthi Gorantla,Arpit Gupta,Swastik Roy,Nanyun Peng,Yang Liu*

Main category: cs.LG

TL;DR: SPARK框架通过三阶段方法实现无参考的强化学习训练：1) 使用生成器和验证器模型产生多样解并进行并行和序列验证；2) 用验证输出作为合成数据训练生成式过程奖励模型；3) 将PRM-CoT作为奖励模型应用于数学推理RL，通过格式约束防止奖励攻击。


<details>
  <summary>Details</summary>
Motivation: 过程奖励模型(PRMs)虽然能提供密集的步骤级反馈，但其应用受到昂贵步骤级标注或真实参考需求的限制。需要开发一种无需地面真实参考的方法来训练过程奖励模型，以支持更广泛的领域应用。

Method: 提出SPARK三阶段框架：第一阶段，生成器模型产生多样解，验证器模型通过并行扩展(自一致性)和序列扩展(元批判)进行评估；第二阶段，用验证输出作为合成训练数据微调生成式过程奖励模型；第三阶段，将PRM-CoT作为奖励模型应用于RL训练，并引入格式约束防止奖励攻击。

Result: 在ProcessBench上达到67.5 F1分数，优于参考引导训练的66.4和GPT-4o的61.9；在六个数学推理基准测试中平均准确率达到47.4%，优于基于地面真实的RLVR的43.9%。

Conclusion: SPARK框架实现了超越地面真实方法的无参考RL训练，为缺乏可验证答案或可访问地面真实的领域开辟了新可能性，展示了合成训练数据在过程奖励模型训练中的有效性。

Abstract: Process reward models (PRMs) that provide dense, step-level feedback have shown promise for reinforcement learning, yet their adoption remains limited by the need for expensive step-level annotations or ground truth references. We propose SPARK: a three-stage framework where in the first stage a generator model produces diverse solutions and a verifier model evaluates them using parallel scaling (self-consistency) and sequential scaling (meta-critique). In the second stage, we use these verification outputs as synthetic training data to fine-tune generative process reward models, which subsequently serve as reward signals during training. We show that aggregating multiple independent verifications at the step level produces training data for process reward models that surpass ground-truth outcome supervision, achieving 67.5 F1 on ProcessBench (a benchmark for identifying erroneous steps in mathematical reasoning) compared to 66.4 for reference-guided training and 61.9 for GPT-4o. In the final stage, we apply our generative PRM with chain-of-thought verification (PRM-CoT) as the reward model in RL experiments on mathematical reasoning, and introduce format constraints to prevent reward hacking. Using Qwen2.5-Math-7B, we achieve 47.4% average accuracy across six mathematical reasoning benchmarks, outperforming ground-truth-based RLVR (43.9%). Our work enables reference-free RL training that exceeds ground-truth methods, opening new possibilities for domains lacking verifiable answers or accessible ground truth.

</details>


### [69] [Too Late to Recall: Explaining the Two-Hop Problem in Multimodal Knowledge Retrieval](https://arxiv.org/abs/2512.03276)
*Constantin Venhoff,Ashkan Khakzar,Sonia Joseph,Philip Torr,Neel Nanda*

Main category: cs.LG

TL;DR: 视觉语言模型在事实回忆任务上表现下降，主要原因是实体表示形成过晚，无法有效利用LLM已有的知识回忆机制。


<details>
  <summary>Details</summary>
Motivation: 许多视觉语言模型在事实回忆任务上表现不如其LLM骨干模型，研究者希望探究多模态微调在扩展LLM机制到视觉输入方面的有效性。

Method: 评估14个不同架构、规模和训练设置的VLM模型，使用归因修补、激活修补和探测技术分析性能差异，并测试两种性能恢复方法。

Result: 14个模型中有11个出现事实回忆性能下降；性能差的VLM在计算过程中实体表示形成过晚，无法有效利用LLM已有的知识回忆机制；通过修补实体表示或使用思维链提示可以恢复性能。

Conclusion: 早期实体表示形成的速度是VLM能否有效利用预训练LLM机制的关键因素，机制分析可以解释多模态对齐中的系统性失败。

Abstract: Training vision language models (VLMs) aims to align visual representations from a vision encoder with the textual representations of a pretrained large language model (LLM). However, many VLMs exhibit reduced factual recall performance compared to their LLM backbones, raising the question of how effective multimodal fine-tuning is at extending existing mechanisms within the LLM to visual inputs. We argue that factual recall based on visual inputs requires VLMs to solve a two-hop problem: (1) forming entity representations from visual inputs, and (2) recalling associated factual knowledge based on these entity representations. By benchmarking 14 VLMs with various architectures (LLaVA, Native, Cross-Attention), sizes (7B-124B parameters), and training setups on factual recall tasks against their original LLM backbone models, we find that 11 of 14 models exhibit factual recall degradation. We select three models with high and two models with low performance degradation, and use attribution patching, activation patching, and probing to show that degraded VLMs struggle to use the existing factual recall circuit of their LLM backbone, because they resolve the first hop too late in the computation. In contrast, high-performing VLMs resolve entity representations early enough to reuse the existing factual recall mechanism. Finally, we demonstrate two methods to recover performance: patching entity representations from the LLM backbone into the VLM, and prompting with chain-of-thought reasoning. Our results highlight that the speed of early entity resolution critically determines how effective VLMs are in using preexisting LLM mechanisms. More broadly, our work illustrates how mechanistic analysis can explain and unveil systematic failures in multimodal alignment.

</details>


### [70] [Adaptive Regime-Switching Forecasts with Distribution-Free Uncertainty: Deep Switching State-Space Models Meet Conformal Prediction](https://arxiv.org/abs/2512.03298)
*Echo Diyun LU,Charles Findling,Marianne Clausel,Alessandro Leite,Wei Gong,Pierric Kersaudy*

Main category: cs.LG

TL;DR: 该研究将自适应共形推理与深度切换状态空间模型结合，为存在制度转换的非平稳时间序列提供分布自由的预测不确定性量化方法。


<details>
  <summary>Details</summary>
Motivation: 制度转换经常破坏时间序列的平稳性，使得校准的不确定性量化与点预测精度同等重要。需要为存在制度切换的预测问题提供分布自由的置信区间方法。

Method: 将深度切换状态空间模型与自适应共形推理（ACI）及其聚合变体（AgACI）相结合。同时开发了一个统一的共形包装器，可应用于多种序列基线模型（S4、MC-Dropout GRU、稀疏高斯过程、变点局部模型），在非平稳性和模型误设条件下提供具有有限样本边际保证的在线预测区间。

Result: 在合成和真实数据集上，共形化预测器实现了接近名义水平的覆盖率，同时保持竞争性的预测精度，并且通常提高了区间效率。

Conclusion: 该方法为存在制度转换的非平稳时间序列预测提供了有效的分布自由不确定性量化框架，能够在模型误设和非平稳条件下保证预测区间的统计覆盖性质。

Abstract: Regime transitions routinely break stationarity in time series, making calibrated uncertainty as important as point accuracy. We study distribution-free uncertainty for regime-switching forecasting by coupling Deep Switching State Space Models with Adaptive Conformal Inference (ACI) and its aggregated variant (AgACI). We also introduce a unified conformal wrapper that sits atop strong sequence baselines including S4, MC-Dropout GRU, sparse Gaussian processes, and a change-point local model to produce online predictive bands with finite-sample marginal guarantees under nonstationarity and model misspecification. Across synthetic and real datasets, conformalized forecasters achieve near-nominal coverage with competitive accuracy and generally improved band efficiency.

</details>


### [71] [HydroDCM: Hydrological Domain-Conditioned Modulation for Cross-Reservoir Inflow Prediction](https://arxiv.org/abs/2512.03300)
*Pengfei Hu,Fan Ming,Xiaoxue Han,Chang Lu,Yue Ning,Dan Lu*

Main category: cs.LG

TL;DR: 本文提出HydroDCM框架，通过元数据构建伪域标签指导对抗学习，结合轻量级条件层实现跨水库流量预测的领域泛化。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在水库入库流量预测中表现良好，但在不同水库间应用时因分布差异（域偏移问题）性能下降。传统领域泛化方法在处理水文系统时面临挑战，因为每个水库具有独特的流量模式，而空间信息等元数据对预测有间接但重要的影响。

Method: 提出HydroDCM框架：1）利用水库空间元数据构建伪域标签指导对抗学习，提取域不变的时间特征；2）在推理阶段，通过轻量级条件层结合目标水库的元数据进行特征适配，平衡领域泛化的不变性与位置特异性适配。

Result: 在科罗拉多河上游流域30个真实水库上的实验结果表明，该方法在多域条件下显著优于最先进的领域泛化基线方法，且保持计算高效性。

Conclusion: HydroDCM框架通过元数据引导的对抗学习和轻量级条件适配，有效解决了水文系统中多域条件下的领域泛化问题，为跨水库流量预测提供了可扩展的解决方案。

Abstract: Deep learning models have shown promise in reservoir inflow prediction, yet their performance often deteriorates when applied to different reservoirs due to distributional differences, referred to as the domain shift problem. Domain generalization (DG) solutions aim to address this issue by extracting domain-invariant representations that mitigate errors in unseen domains. However, in hydrological settings, each reservoir exhibits unique inflow patterns, while some metadata beyond observations like spatial information exerts indirect but significant influence. This mismatch limits the applicability of conventional DG techniques to many-domain hydrological systems. To overcome these challenges, we propose HydroDCM, a scalable DG framework for cross-reservoir inflow forecasting. Spatial metadata of reservoirs is used to construct pseudo-domain labels that guide adversarial learning of invariant temporal features. During inference, HydroDCM adapts these features through light-weight conditioning layers informed by the target reservoir's metadata, reconciling DG's invariance with location-specific adaptation. Experiment results on 30 real-world reservoirs in the Upper Colorado River Basin demonstrate that our method substantially outperforms state-of-the-art DG baselines under many-domain conditions and remains computationally efficient.

</details>


### [72] [Robust Tabular Foundation Models](https://arxiv.org/abs/2512.03307)
*Matthew Peroni,Franck Le,Vadim Sheinin*

Main category: cs.LG

TL;DR: RTFM框架通过对抗性训练提升表格基础模型性能，使用合成数据生成器强调对模型具有挑战性的数据集，在TabPFN V2上实现最高6%的AUC提升


<details>
  <summary>Details</summary>
Motivation: 表格基础模型(TFMs)在结构化数据上展现出超越传统ML方法的潜力。现有研究主要关注设计高质量的数据生成器先验来提升预训练性能，但作者发现可以通过参数化生成器分布，从对抗鲁棒性角度出发，强调对模型具有挑战性的数据集来进一步提升性能。

Method: 提出RTFM（鲁棒表格基础模型）框架，引入最优性间隙度量（TFM性能与XGBoost、CatBoost、随机森林等强基线最佳性能的差异）。基于此，RTFM作为模型无关的对抗训练框架，在训练过程中调整生成器以强调对模型特别具有挑战性的数据集。

Result: 在TabPFN V2分类器上应用RTFM，相比原始TabPFN和其他基线算法，平均归一化AUC提升最高达6%，且仅需额外不到10万个合成数据集。这些结果展示了仅使用合成数据进行针对性对抗训练和微调TFMs的新方向。

Conclusion: RTFM框架通过对抗性训练方法有效提升了表格基础模型的性能，证明了通过参数化生成器分布并强调具有挑战性的数据集，可以在仅使用合成数据的情况下实现显著的性能改进，为TFMs的针对性训练开辟了有前景的新方向。

Abstract: The development of tabular foundation models (TFMs) has accelerated in recent years, showing strong potential to outperform traditional ML methods for structured data. A key finding is that TFMs can be pretrained entirely on synthetic datasets, opening opportunities to design data generators that encourage desirable model properties. Prior work has mainly focused on crafting high-quality priors over generators to improve overall pretraining performance. Our insight is that parameterizing the generator distribution enables an adversarial robustness perspective: during training, we can adapt the generator to emphasize datasets that are particularly challenging for the model. We formalize this by introducing an optimality gap measure, given by the difference between TFM performance and the best achievable performance as estimated by strong baselines such as XGBoost, CatBoost, and Random Forests. Building on this idea, we propose Robust Tabular Foundation Models (RTFM), a model-agnostic adversarial training framework. Applied to the TabPFN V2 classifier, RTFM improves benchmark performance, with up to a 6% increase in mean normalized AUC over the original TabPFN and other baseline algorithms, while requiring less than 100k additional synthetic datasets. These results highlight a promising new direction for targeted adversarial training and fine-tuning of TFMs using synthetic data alone.

</details>


### [73] [Single-Round Scalable Analytic Federated Learning](https://arxiv.org/abs/2512.03336)
*Alan T. L. Bacellar,Mustafa Munir,Felipe M. G. França,Priscila M. V. Lima,Radu Marculescu,Lizy K. John*

Main category: cs.LG

TL;DR: SAFLe框架通过引入结构化头部和稀疏分组嵌入，实现了可扩展的非线性表达能力，同时保持单轮聚合优势，在联邦学习中取得了新的最优性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临两大挑战：高通信开销和异构数据上的性能下降。现有方法要么局限于线性模型（AFL），要么需要多轮通信（DeepAFL），缺乏既保持单轮优势又具备非线性表达能力的高效解决方案。

Method: 提出SAFLe框架，引入结构化头部（bucketed features）和稀疏分组嵌入，证明这种非线性架构在数学上等价于高维线性回归，从而能够使用AFL的单轮不变聚合定律进行求解。

Result: SAFLe在联邦视觉任务中建立了新的最优性能，在所有基准测试中都显著优于线性AFL和多轮DeepAFL，实现了高效可扩展的解决方案。

Conclusion: SAFLe成功打破了联邦学习中单轮通信和非线性表达能力之间的权衡，提供了一种既高效又强大的联邦学习框架，为联邦视觉应用开辟了新途径。

Abstract: Federated Learning (FL) is plagued by two key challenges: high communication overhead and performance collapse on heterogeneous (non-IID) data. Analytic FL (AFL) provides a single-round, data distribution invariant solution, but is limited to linear models. Subsequent non-linear approaches, like DeepAFL, regain accuracy but sacrifice the single-round benefit. In this work, we break this trade-off. We propose SAFLe, a framework that achieves scalable non-linear expressivity by introducing a structured head of bucketed features and sparse, grouped embeddings. We prove this non-linear architecture is mathematically equivalent to a high-dimensional linear regression. This key equivalence allows SAFLe to be solved with AFL's single-shot, invariant aggregation law. Empirically, SAFLe establishes a new state-of-the-art for analytic FL, significantly outperforming both linear AFL and multi-round DeepAFL in accuracy across all benchmarks, demonstrating a highly efficient and scalable solution for federated vision.

</details>


### [74] [Breaking Determinism: Stochastic Modeling for Reliable Off-Policy Evaluation in Ad Auctions](https://arxiv.org/abs/2512.03354)
*Hongseon Yeom,Jaeyoul Shin,Soojin Min,Jeongmin Yoon,Seunghak Yu,Dongyeop Kang*

Main category: cs.LG

TL;DR: 提出首个确定性广告拍卖中的离策略评估框架，通过重新利用出价景观模型近似倾向得分，实现稳定评估，显著减少在线A/B测试的成本和风险。


<details>
  <summary>Details</summary>
Motivation: 在线A/B测试消耗大量工程资源且存在收入损失风险，而传统离策略评估方法在确定性拍卖环境中无法应用，因为非获胜广告的曝光概率为零。

Method: 重新利用出价景观模型近似倾向得分，推导稳健的近似倾向分数，使自归一化逆倾向评分等稳定估计器能够在确定性拍卖环境中进行反事实评估。

Result: 在AuctionNet模拟基准和大型工业平台2周在线A/B测试中验证，方法显示与在线结果显著一致，CTR预测的平均方向准确率达到92%，显著优于参数基线。

Conclusion: 贡献了首个实用且经验证的确定性拍卖环境可靠离策略评估框架，为昂贵且风险的在线实验提供了高效替代方案。

Abstract: Online A/B testing, the gold standard for evaluating new advertising policies, consumes substantial engineering resources and risks significant revenue loss from deploying underperforming variations. This motivates the use of Off-Policy Evaluation (OPE) for rapid, offline assessment. However, applying OPE to ad auctions is fundamentally more challenging than in domains like recommender systems, where stochastic policies are common. In online ad auctions, it is common for the highest-bidding ad to win the impression, resulting in a deterministic, winner-takes-all setting. This results in zero probability of exposure for non-winning ads, rendering standard OPE estimators inapplicable. We introduce the first principled framework for OPE in deterministic auctions by repurposing the bid landscape model to approximate the propensity score. This model allows us to derive robust approximate propensity scores, enabling the use of stable estimators like Self-Normalized Inverse Propensity Scoring (SNIPS) for counterfactual evaluation. We validate our approach on the AuctionNet simulation benchmark and against 2-weeks online A/B test from a large-scale industrial platform. Our method shows remarkable alignment with online results, achieving a 92\% Mean Directional Accuracy (MDA) in CTR prediction, significantly outperforming the parametric baseline. MDA is the most critical metric for guiding deployment decisions, as it reflects the ability to correctly predict whether a new model will improve or harm performance. This work contributes the first practical and validated framework for reliable OPE in deterministic auction environments, offering an efficient alternative to costly and risky online experiments.

</details>


### [75] [Better World Models Can Lead to Better Post-Training Performance](https://arxiv.org/abs/2512.03400)
*Prakhar Gupta,Henry Conklin,Sarah-Jane Leslie,Andrew Lee*

Main category: cs.LG

TL;DR: 研究显式世界建模目标如何影响Transformer在不同训练阶段的内部表示和下游能力，使用2x2x2魔方作为测试平台，发现显式世界建模能产生更线性可解码和因果可控的状态表示，并提升强化学习后训练的效果。


<details>
  <summary>Details</summary>
Motivation: 研究显式世界建模目标对Transformer内部表示和下游能力的影响，特别是在不同训练阶段如何影响模型的状态表示质量，以及这些表示如何影响强化学习后训练的效果。

Method: 使用2x2x2魔方作为测试环境，比较标准的下一个token预测与两种显式世界建模策略：(i)状态预测预训练，(ii)状态预测+下一个token联合目标。使用Group Relative Policy Optimization (GRPO)进行后训练，通过线性探测和因果干预评估表示质量。

Result: 显式世界建模产生更线性可解码和因果可控的状态表示。更重要的是，改进的状态表示能显著提升GRPO的效果，特别是在更难的魔方状态上。状态表示的锐化可以提升序列规划任务的后训练效果。

Conclusion: 显式世界建模目标能改善Transformer的状态表示质量，这些改进的表示能显著提升强化学习后训练的效果，特别是在复杂任务状态上，表明锐化状态表示可以增强序列规划任务的后训练有效性。

Abstract: In this work we study how explicit world-modeling objectives affect the internal representations and downstream capability of Transformers across different training stages. We use a controlled 2x2x2 Rubik's Cube and ask: (1) how does explicitly pretraining a world model affect the model's latent representations, and (2) how does world-model quality affect the model's performance after reinforcement learning post-training? We compare standard next-token prediction to two explicit world-modeling strategies -- (i) state-prediction pretraining and (ii) a joint state-prediction + next-token objective -- and assess task performance after Group Relative Policy Optimization (GRPO) is applied as post-training. We evaluate the representation quality with linear probes and causal interventions. We find that explicit world-modeling yields more linearly decodable and causally steerable state representations. More importantly, we find that improved state representations lead to higher gains for GRPO, especially on harder cube states. Our results indicate that sharpening state representations can improve the effectiveness of post-training for sequence-planning tasks.

</details>


### [76] [Tuning-Free Structured Sparse Recovery of Multiple Measurement Vectors using Implicit Regularization](https://arxiv.org/abs/2512.03393)
*Lakshmi Jayalal,Sheetal Kalyani*

Main category: cs.LG

TL;DR: 提出一种基于隐式正则化的免调参框架，用于恢复联合稀疏信号，无需先验知识或参数调整


<details>
  <summary>Details</summary>
Motivation: 传统多测量向量方法如M-OMP和M-FOCUSS需要仔细的参数调整或对信号稀疏度和噪声方差的先验知识，这在实际应用中存在限制

Method: 通过过参数化引入隐式正则化，将估计矩阵重新参数化为因子，分离共享行支持与个体向量条目，对标准最小二乘目标应用梯度下降

Result: 理论证明：在足够小且平衡的初始化下，优化动态呈现"动量效应"，真实支持中的行范数增长显著快于其他行；实证结果：性能与现有方法相当，无需先验信息或调参

Conclusion: 提出的免调参框架通过隐式正则化有效解决联合稀疏信号恢复问题，克服了传统方法对参数调整和先验知识的依赖

Abstract: Recovering jointly sparse signals in the multiple measurement vectors (MMV) setting is a fundamental problem in machine learning, but traditional methods like multiple measurement vectors orthogonal matching pursuit (M-OMP) and multiple measurement vectors FOCal Underdetermined System Solver (M-FOCUSS) often require careful parameter tuning or prior knowledge of the sparsity of the signal and/or noise variance. We introduce a novel tuning-free framework that leverages Implicit Regularization (IR) from overparameterization to overcome this limitation. Our approach reparameterizes the estimation matrix into factors that decouple the shared row-support from individual vector entries. We show that the optimization dynamics inherently promote the desired row-sparse structure by applying gradient descent to a standard least-squares objective on these factors. We prove that with a sufficiently small and balanced initialization, the optimization dynamics exhibit a "momentum-like" effect, causing the norms of rows in the true support to grow significantly faster than others. This formally guarantees that the solution trajectory converges towards an idealized row-sparse solution. Additionally, empirical results demonstrate that our approach achieves performance comparable to established methods without requiring any prior information or tuning.

</details>


### [77] [The promising potential of vision language models for the generation of textual weather forecasts](https://arxiv.org/abs/2512.03623)
*Edward C. C. Steele,Dinesh Mane,Emilio Monti,Luis Orus,Rebecca Chantrill-Cheyette,Matthew Couch,Kirstine I. Dale,Simon Eaton,Govindarajan Rangarajan,Amir Majlesi,Steven Ramsdale,Michael Sharpe,Craig Smith,Jonathan Smith,Rebecca Yates,Holly Ellis,Charles Ewen*

Main category: cs.LG

TL;DR: 使用视觉语言模型从视频编码的网格天气数据直接生成航运预报文本，探索多模态基础模型在气象产品服务中的应用


<details>
  <summary>Details</summary>
Motivation: 尽管多模态基础模型具有潜力，但在气象产品和服务生成方面的应用仍处于起步阶段。为了加速其应用和采纳，需要探索新的方法来提高生产效率和推动服务创新。

Method: 采用视觉语言模型，直接从视频编码的网格天气数据生成航运预报文本，这是一种创新的多模态应用方法。

Result: 早期结果显示，该方法为天气企业及其他领域提供了有前景的可扩展技术机会，能够提高生产效率和推动服务创新。

Conclusion: 这项研究展示了多模态基础模型在气象服务领域的应用潜力，为天气企业提供了新的技术路径，有望推动气象产品生成和服务创新的发展。

Abstract: Despite the promising capability of multimodal foundation models, their application to the generation of meteorological products and services remains nascent. To accelerate aspiration and adoption, we explore the novel use of a vision language model for writing the iconic Shipping Forecast text directly from video-encoded gridded weather data. These early results demonstrate promising scalable technological opportunities for enhancing production efficiency and service innovation within the weather enterprise and beyond.

</details>


### [78] [Full-Stack Alignment: Co-Aligning AI and Institutions with Thick Models of Value](https://arxiv.org/abs/2512.03399)
*Joe Edelman,Tan Zhi-Xuan,Ryan Lowe,Oliver Klingefjord,Vincent Wang-Mascianica,Matija Franklin,Ryan Othniel Kearns,Ellie Hain,Atrisha Sarkar,Michiel Bakker,Fazl Barez,David Duvenaud,Jakob Foerster,Iason Gabriel,Joseph Gubbels,Bryce Goodman,Andreas Haupt,Jobst Heitzig,Julian Jara-Ettinger,Atoosa Kasirzadeh,James Ravi Kirkpatrick,Andrew Koh,W. Bradley Knox,Philipp Koralus,Joel Lehman,Sydney Levine,Samuele Marro,Manon Revel,Toby Shorin,Morgan Sutherland,Michael Henry Tessler,Ivan Vendrov,James Wilken-Smith*

Main category: cs.LG

TL;DR: 论文主张需要"全栈对齐"——同时对齐AI系统及其塑造机构与人类价值观，提出"厚价值模型"来有效表示价值观，区别于传统方法如效用函数或偏好排序。


<details>
  <summary>Details</summary>
Motivation: 当前AI对齐方法存在局限：即使AI系统完美对齐其运营者的意图，如果该组织的目标与其他机构和个人的目标不一致，仍可能导致不良社会结果。传统价值观表示方法（如效用函数、偏好排序或无结构文本）难以有效区分价值观与其他信号、支持规范性推理和建模集体利益。

Method: 提出"厚价值模型"方法，结构化表示价值观和规范，使系统能够区分持久价值观与短暂偏好，建模个体选择的社会嵌入性，并进行规范性推理以在新领域中应用价值观。该方法不需要强加特定的个人或集体繁荣愿景。

Result: 在五个领域展示了该方法的应用：AI价值管理、规范能力强的智能体、双赢谈判系统、意义保持的经济机制和民主监管机构。这些应用展示了厚价值模型如何支持更有效的价值观对齐和社会协调。

Conclusion: 需要从单纯对齐单个AI系统转向"全栈对齐"，同时对齐AI系统及其塑造机构与人类价值观。厚价值模型为解决当前价值观表示方法的局限性提供了有前景的途径，能够支持更有效的规范性推理和社会协调，而无需强加特定的繁荣愿景。

Abstract: Beneficial societal outcomes cannot be guaranteed by aligning individual AI systems with the intentions of their operators or users. Even an AI system that is perfectly aligned to the intentions of its operating organization can lead to bad outcomes if the goals of that organization are misaligned with those of other institutions and individuals. For this reason, we need full-stack alignment, the concurrent alignment of AI systems and the institutions that shape them with what people value. This can be done without imposing a particular vision of individual or collective flourishing. We argue that current approaches for representing values, such as utility functions, preference orderings, or unstructured text, struggle to address these and other issues effectively. They struggle to distinguish values from other signals, to support principled normative reasoning, and to model collective goods. We propose thick models of value will be needed. These structure the way values and norms are represented, enabling systems to distinguish enduring values from fleeting preferences, to model the social embedding of individual choices, and to reason normatively, applying values in new domains. We demonstrate this approach in five areas: AI value stewardship, normatively competent agents, win-win negotiation systems, meaning-preserving economic mechanisms, and democratic regulatory institutions.

</details>


### [79] [Dynamically Scaled Activation Steering](https://arxiv.org/abs/2512.03661)
*Alex Ferrando,Xavier Suau,Jordi Gonzàlez,Pau Rodriguez*

Main category: cs.LG

TL;DR: DSAS是一种动态缩放激活引导框架，能够自适应地调整现有引导方法的强度，只在检测到不良行为时进行干预，从而在毒性缓解和模型效用之间实现更好的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有激活引导方法通常对所有输入统一应用干预，这在不必要引导时会降低模型性能。需要一种能够自适应判断何时引导以及如何引导的方法。

Method: DSAS将"何时引导"与"如何引导"解耦，在生成时计算上下文相关的缩放因子，选择性调整任何引导方法的强度。该方法还可与引导函数联合进行端到端优化。

Result: DSAS与现有引导方法结合时，能持续改进帕累托前沿，在毒性缓解和效用保持之间实现更好的权衡。应用于文本到图像扩散模型时，能有效调节特定概念。

Conclusion: DSAS提供了一种方法无关的动态激活引导框架，能以最小计算开销自适应调节引导强度，同时提高可解释性，识别哪些标记需要引导以及引导程度。

Abstract: Activation steering has emerged as a powerful method for guiding the behavior of generative models towards desired outcomes such as toxicity mitigation. However, most existing methods apply interventions uniformly across all inputs, degrading model performance when steering is unnecessary. We introduce Dynamically Scaled Activation Steering (DSAS), a method-agnostic steering framework that decouples when to steer from how to steer. DSAS adaptively modulates the strength of existing steering transformations across layers and inputs, intervening strongly only when undesired behavior is detected. At generation time, DSAS computes context-dependent scaling factors that selectively adjust the strength of any steering method. We also show how DSAS can be jointly optimized end-to-end together with the steering function. When combined with existing steering methods, DSAS consistently improves the Pareto front with respect to steering alone, achieving a better trade-off between toxicity mitigation and utility preservation. We further demonstrate DSAS's generality by applying it to a text-to-image diffusion model, showing how adaptive steering allows the modulation of specific concepts. Finally, DSAS introduces minimal computational overhead while improving interpretability, pinpointing which tokens require steering and by how much.

</details>


### [80] [GaussDetect-LiNGAM:Causal Direction Identification without Gaussianity test](https://arxiv.org/abs/2512.03428)
*Ziyi Ding,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: 提出GaussDetect-LiNGAM方法，利用前向模型噪声高斯性与反向模型残差独立性之间的等价关系，无需显式高斯性检验即可实现双变量因果发现。


<details>
  <summary>Details</summary>
Motivation: 传统LiNGAM方法依赖脆弱且对样本敏感的高斯性检验，限制了其在实际应用中的可靠性和可访问性。需要一种更稳健的方法来替代这些检验。

Method: 基于理论证明：在线性、无环和外生性假设下，前向模型噪声的高斯性等价于反向模型中回归变量与残差的独立性。利用这一等价关系，用稳健的基于核的独立性检验替代高斯性检验。

Result: 实验验证了等价关系的正确性，GaussDetect-LiNGAM在不同噪声类型和样本量下保持高一致性，同时减少了每个决策所需的检验次数(TPD)。

Conclusion: 该方法通过消除脆弱的高斯性检验需求，提高了因果推断的效率和实际应用性，使LiNGAM在现实场景中更加可靠和易于使用。

Abstract: We propose GaussDetect-LiNGAM, a novel approach for bivariate causal discovery that eliminates the need for explicit Gaussianity tests by leveraging a fundamental equivalence between noise Gaussianity and residual independence in the reverse regression. Under the standard LiNGAM assumptions of linearity, acyclicity, and exogeneity, we prove that the Gaussianity of the forward-model noise is equivalent to the independence between the regressor and residual in the reverse model. This theoretical insight allows us to replace fragile and sample-sensitive Gaussianity tests with robust kernel-based independence tests. Experimental results validate the equivalence and demonstrate that GaussDetect-LiNGAM maintains high consistency across diverse noise types and sample sizes, while reducing the number of tests per decision (TPD). Our method enhances both the efficiency and practical applicability of causal inference, making LiNGAM more accessible and reliable in real-world scenarios.

</details>


### [81] [DVPO: Distributional Value Modeling-based Policy Optimization for LLM Post-Training](https://arxiv.org/abs/2512.03847)
*Dingwei Zhu,Zhiheng Xi,Shihan Dou,Yuhui Wang,Sixian Li,Junjie Ye,Honglin Guo,Shichun Liu,Chenhao Huang,Yajie Yang,Junlin Shang,Senjie Jin,Ming Zhang,Jiazheng Zhang,Caishuang Huang,Yunke Zhang,Demei Yan,Yuran Wang,Tao Gui*

Main category: cs.LG

TL;DR: DVPO是一个新的RL框架，结合条件风险理论和分布价值建模，通过token级价值分布和不对称风险正则化来平衡鲁棒性和泛化性，在噪声监督下优于PPO、GRPO等方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界部署中常存在噪声或不完整的监督信号，复杂的监督信号会破坏训练稳定性并损害泛化能力。现有方法（如RFQI、CQL、PPO、GRPO）虽然能提高稳定性，但往往忽视泛化性，可能产生过于保守的策略，导致在不同实际场景中表现不均。

Method: DVPO结合条件风险理论与分布价值建模，学习token级价值分布以提供细粒度监督，并应用不对称风险正则化来塑造分布尾部：收缩下尾以抑制噪声负偏差，同时扩展上尾以保持探索多样性。

Result: 在多轮对话、数学推理和科学问答等广泛实验和分析中，DVPO在噪声监督下始终优于PPO、GRPO和基于鲁棒Bellman的PPO，显示出其在现实世界LLM后训练中的潜力。

Conclusion: DVPO通过分布价值建模和风险感知策略优化，在噪声监督下实现了鲁棒性和泛化性的更好平衡，为LLM后训练提供了有效的RL框架。

Abstract: Reinforcement learning (RL) has shown strong performance in LLM post-training, but real-world deployment often involves noisy or incomplete supervision. In such settings, complex and unreliable supervision signals can destabilize training and harm generalization. While existing approaches such as worst-case optimization (e.g., RFQI, CQL) and mean-based methods (e.g., PPO, GRPO) can improve stability, they often overlook generalization and may produce overly conservative policies, leading to uneven performance across diverse real scenarios. To this end, we introduce DVPO (Distributional Value Modeling with Risk-aware Policy Optimization), a new RL framework that combines conditional risk theory with distributional value modeling to better balance robustness and generalization. DVPO learns token-level value distributions to provide fine-grained supervision, and applies an asymmetric risk regularization to shape the distribution tails: it contracts the lower tail to dampen noisy negative deviations, while expanding the upper tail to preserve exploratory diversity. Across extensive experiments and analysis in multi-turn dialogue, math reasoning, and scientific QA, DVPO consistently outperforms PPO, GRPO, and robust Bellman-based PPO under noisy supervision, showing its potential for LLM post-training in the real-world.

</details>


### [82] [Grokked Models are Better Unlearners](https://arxiv.org/abs/2512.03437)
*Yuanbang Liang,Yang Li*

Main category: cs.LG

TL;DR: 研究发现，在模型完成"顿悟"（grokking）阶段后应用机器遗忘方法，相比在早期训练阶段应用，能实现更高效、更稳定、副作用更小的数据遗忘效果。


<details>
  <summary>Details</summary>
Motivation: 探索"顿悟"（grokking）现象是否有助于机器遗忘（machine unlearning），即在不完全重新训练的情况下移除特定数据的影响。研究比较在顿悟前后应用标准遗忘方法的效果差异。

Method: 在视觉（CNNs/ResNets在CIFAR、SVHN、ImageNet上）和语言（transformer在TOFU风格设置上）任务中，比较在顿悟前后应用标准遗忘方法的效果。分析特征和曲率以理解机制。

Result: 从顿悟后的检查点开始遗忘，相比早期停止的模型，能实现：(1) 更高效的遗忘（达到目标遗忘水平所需更新更少），(2) 更小的附带损害（在保留数据和测试集上的性能下降更小），(3) 更稳定的更新（跨种子的变化更小）。

Conclusion: 顿悟后的模型学习到更模块化的表示，减少了遗忘子集和保留子集之间的梯度对齐，从而促进选择性遗忘。模型何时被训练（顿悟前vs后）是影响遗忘效果的独立因素，为改进现有遗忘方法提供了实用方案。

Abstract: Grokking-delayed generalization that emerges well after a model has fit the training data-has been linked to robustness and representation quality. We ask whether this training regime also helps with machine unlearning, i.e., removing the influence of specified data without full retraining. We compare applying standard unlearning methods before versus after the grokking transition across vision (CNNs/ResNets on CIFAR, SVHN, and ImageNet) and language (a transformer on a TOFU-style setup). Starting from grokked checkpoints consistently yields (i) more efficient forgetting (fewer updates to reach a target forget level), (ii) less collateral damage (smaller drops on retained and test performance), and (iii) more stable updates across seeds, relative to early-stopped counterparts under identical unlearning algorithms. Analyses of features and curvature further suggest that post-grokking models learn more modular representations with reduced gradient alignment between forget and retain subsets, which facilitates selective forgetting. Our results highlight when a model is trained (pre- vs. post-grokking) as an orthogonal lever to how unlearning is performed, providing a practical recipe to improve existing unlearning methods without altering their algorithms.

</details>


### [83] [Multi-Modal Opinion Integration for Financial Sentiment Analysis using Cross-Modal Attention](https://arxiv.org/abs/2512.03464)
*Yujing Liu,Chen Yang*

Main category: cs.LG

TL;DR: 本文提出了一种用于金融情感分析的端到端深度学习框架，通过跨模态注意力机制整合时效性意见和流行性意见两种模态，显著提升了情感分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有金融情感分析方法难以有效整合多样化的意见模态并捕捉它们之间的细粒度交互。金融意见存在两种不同的信息渠道：时效性驱动的市场更新和流行性驱动的集体情感，需要专门的方法来处理这两种模态的融合。

Method: 提出端到端深度学习框架：1）使用BERT（Chinese-wwm-ext）进行特征嵌入；2）设计金融多头交叉注意力（FMHCA）结构促进两种意见模态间的信息交换；3）通过Transformer层优化处理特征；4）使用多模态因子双线性池化进行特征融合，分类为负面、中性和正面情感。

Result: 在涵盖837家公司的综合数据集上进行实验，模型准确率达到83.5%，显著优于包括BERT+Transformer在内的基线方法，提升幅度达21%。

Conclusion: 该框架通过有效整合时效性和流行性两种金融意见模态，显著提升了情感分析性能，有望支持更准确的金融决策和风险管理。

Abstract: In recent years, financial sentiment analysis of public opinion has become increasingly important for market forecasting and risk assessment. However, existing methods often struggle to effectively integrate diverse opinion modalities and capture fine-grained interactions across them. This paper proposes an end-to-end deep learning framework that integrates two distinct modalities of financial opinions: recency modality (timely opinions) and popularity modality (trending opinions), through a novel cross-modal attention mechanism specifically designed for financial sentiment analysis. While both modalities consist of textual data, they represent fundamentally different information channels: recency-driven market updates versus popularity-driven collective sentiment. Our model first uses BERT (Chinese-wwm-ext) for feature embedding and then employs our proposed Financial Multi-Head Cross-Attention (FMHCA) structure to facilitate information exchange between these distinct opinion modalities. The processed features are optimized through a transformer layer and fused using multimodal factored bilinear pooling for classification into negative, neutral, and positive sentiment. Extensive experiments on a comprehensive dataset covering 837 companies demonstrate that our approach achieves an accuracy of 83.5%, significantly outperforming baselines including BERT+Transformer by 21 percent. These results highlight the potential of our framework to support more accurate financial decision-making and risk management.

</details>


### [84] [Hyperdimensional Computing for Sustainable Manufacturing: An Initial Assessment](https://arxiv.org/abs/2512.03864)
*Danny Hoang,Anandkumar Patel,Ruimen Chen,Rajiv Malhotra,Farhad Imani*

Main category: cs.LG

TL;DR: 该研究比较了智能加工中几何质量预测的AI模型能耗，引入超维计算(HDC)作为替代方案，在保持精度的同时大幅降低能耗和提升速度。


<details>
  <summary>Details</summary>
Motivation: 智能制造虽能提高效率和降低能耗，但AI模型的高能耗可能抵消这些优势。需要寻找既能保持预测精度又能显著降低能耗的AI解决方案。

Method: 利用原位传感技术预测智能加工中的几何质量，比较常见AI模型的能耗、精度和速度。引入超维计算(HDC)作为替代方案，并与传统模型进行对比分析。

Result: HDC在保持与传统模型相当精度的同时，训练能耗降低200倍，推理能耗降低175-1000倍。训练时间减少200倍，推理时间减少300-600倍。

Conclusion: 超维计算(HDC)在智能制造的几何质量预测中展现出巨大潜力，既能保持预测精度，又能显著降低能耗和提升速度，为实现能源高效的智能制造提供了有前景的解决方案。

Abstract: Smart manufacturing can significantly improve efficiency and reduce energy consumption, yet the energy demands of AI models may offset these gains. This study utilizes in-situ sensing-based prediction of geometric quality in smart machining to compare the energy consumption, accuracy, and speed of common AI models. HyperDimensional Computing (HDC) is introduced as an alternative, achieving accuracy comparable to conventional models while drastically reducing energy consumption, 200$\times$ for training and 175 to 1000$\times$ for inference. Furthermore, HDC reduces training times by 200$\times$ and inference times by 300 to 600$\times$, showcasing its potential for energy-efficient smart manufacturing.

</details>


### [85] [Bayesian Event-Based Model for Disease Subtype and Stage Inference](https://arxiv.org/abs/2512.03467)
*Hongtao Hao,Joseph L. Austerweil*

Main category: cs.LG

TL;DR: 本文提出了一个贝叶斯亚型事件模型（BEBMS），在合成数据和真实阿尔茨海默病数据上均优于现有的SuStaIn方法，在排序、分期和亚型分配任务上表现更稳健。


<details>
  <summary>Details</summary>
Motivation: 慢性疾病在不同患者中的进展方式存在差异，通常存在少数几种疾病进展亚型。SuStaIn方法虽然被广泛应用，但其性能稳健性需要验证。本文旨在开发一个更稳健的贝叶斯亚型事件模型，并与SuStaIn进行系统比较。

Method: 开发了贝叶斯亚型事件模型（BEBMS），这是一个基于原则的贝叶斯亚型变体事件模型。通过多种合成数据实验，在不同程度的模型误设条件下，比较BEBMS与SuStaIn在排序、分期和亚型分配任务上的性能。最后将两种方法应用于真实的阿尔茨海默病数据集。

Result: 在合成数据实验中，BEBMS在排序、分期和亚型分配任务上显著优于SuStaIn。在真实阿尔茨海默病数据应用中，BEBMS的结果与科学界对阿尔茨海默病进展的共识更加一致。

Conclusion: BEBMS是一个比SuStaIn更稳健的疾病亚型分析模型，在模型误设条件下表现更好，且与科学共识更一致，为疾病亚型分析提供了更可靠的工具。

Abstract: Chronic diseases often progress differently across patients. Rather than randomly varying, there are typically a small number of subtypes for how a disease progresses across patients. To capture this structured heterogeneity, the Subtype and Stage Inference Event-Based Model (SuStaIn) estimates the number of subtypes, the order of disease progression for each subtype, and assigns each patient to a subtype from primarily cross-sectional data. It has been widely applied to uncover the subtypes of many diseases and inform our understanding of them. But how robust is its performance? In this paper, we develop a principled Bayesian subtype variant of the event-based model (BEBMS) and compare its performance to SuStaIn in a variety of synthetic data experiments with varied levels of model misspecification. BEBMS substantially outperforms SuStaIn across ordering, staging, and subtype assignment tasks. Further, we apply BEBMS and SuStaIn to a real-world Alzheimer's data set. We find BEBMS has results that are more consistent with the scientific consensus of Alzheimer's disease progression than SuStaIn.

</details>


### [86] [Guided Flow Policy: Learning from High-Value Actions in Offline Reinforcement Learning](https://arxiv.org/abs/2512.03973)
*Franki Nguimatsia Tiofack,Théotime Le Hellard,Fabian Schramm,Nicolas Perrin-Gilbert,Justin Carpentier*

Main category: cs.LG

TL;DR: GFP通过结合多步流匹配策略和蒸馏的单步演员，使用加权行为克隆来专注于复制数据集中的高价值动作，而非盲目模仿所有状态-动作对，在离线强化学习中实现了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中传统的行为正则化方法存在局限性，它们无法区分数据集中的高价值和低价值动作，对所有状态-动作对都进行同等程度的模仿，这限制了策略的性能提升。

Method: GFP结合了多步流匹配策略和蒸馏的单步演员。演员通过加权行为克隆指导流策略专注于复制数据集中的高价值动作，而流策略则约束演员保持与数据集最佳转换的一致性，同时最大化评论家的价值估计。

Result: GFP在OGBench、Minari和D4RL基准测试的144个状态和像素任务中实现了最先进的性能，特别是在次优数据集和挑战性任务上取得了显著提升。

Conclusion: GFP通过演员和流策略的相互指导机制，有效地区分并专注于数据集中的高价值动作，克服了传统行为正则化的局限性，为离线强化学习提供了强大的新方法。

Abstract: Offline reinforcement learning often relies on behavior regularization that enforces policies to remain close to the dataset distribution. However, such approaches fail to distinguish between high-value and low-value actions in their regularization components. We introduce Guided Flow Policy (GFP), which couples a multi-step flow-matching policy with a distilled one-step actor. The actor directs the flow policy through weighted behavior cloning to focus on cloning high-value actions from the dataset rather than indiscriminately imitating all state-action pairs. In turn, the flow policy constrains the actor to remain aligned with the dataset's best transitions while maximizing the critic. This mutual guidance enables GFP to achieve state-of-the-art performance across 144 state and pixel-based tasks from the OGBench, Minari, and D4RL benchmarks, with substantial gains on suboptimal datasets and challenging tasks. Webpage: https://simple-robotics.github.io/publications/guided-flow-policy/

</details>


### [87] [Joint Progression Modeling (JPM): A Probabilistic Framework for Mixed-Pathology Progression](https://arxiv.org/abs/2512.03475)
*Hongtao Hao,Joseph L. Austerweil*

Main category: cs.LG

TL;DR: JPM是一个概率框架，用于从横截面数据推断混合神经退行性疾病的联合进展，将单病轨迹视为部分排序并构建联合进展先验，相比单病模型提高了排序准确性。


<details>
  <summary>Details</summary>
Motivation: 标准事件模型假设个体只有单一疾病，但神经退行性疾病中混合病理很常见，需要能够处理多种疾病联合进展的模型。

Method: 提出联合进展模型(JPM)，将单病轨迹视为部分排序，构建联合进展先验，研究四种变体(Pairwise、Bradley-Terry、Plackett-Luce、Mallows)，分析校准性、分离性和锐度三个属性。

Result: 所有变体都具有校准性，分离性接近完美；锐度因变体而异且可通过输入部分排序的简单特征预测；在合成实验中，JPM比强基线(SA-EBM)提高约21%的排序准确性；在NACC数据中，Mallows变体和基线模型结果与AD和VaD混合病理进展的文献更一致。

Conclusion: JPM为从横截面数据推断混合神经退行性疾病的联合进展提供了一个有效的概率框架，能够处理多种病理的复杂相互作用，相比单病模型显著提高了准确性。

Abstract: Event-based models (EBMs) infer disease progression from cross-sectional data, and standard EBMs assume a single underlying disease per individual. In contrast, mixed pathologies are common in neurodegeneration. We introduce the Joint Progression Model (JPM), a probabilistic framework that treats single-disease trajectories as partial rankings and builds a prior over joint progressions. We study several JPM variants (Pairwise, Bradley-Terry, Plackett-Luce, and Mallows) and analyze three properties: (i) calibration -- whether lower model energy predicts smaller distance to the ground truth ordering; (ii) separation -- the degree to which sampled rankings are distinguishable from random permutations; and (iii) sharpness -- the stability of sampled aggregate rankings. All variants are calibrated, and all achieve near-perfect separation; sharpness varies by variant and is well-predicted by simple features of the input partial rankings (number and length of rankings, conflict, and overlap). In synthetic experiments, JPM improves ordering accuracy by roughly 21 percent over a strong EBM baseline (SA-EBM) that treats the joint disease as a single condition. Finally, using NACC, we find that the Mallows variant of JPM and the baseline model (SA-EBM) have results that are more consistent with prior literature on the possible disease progression of the mixed pathology of AD and VaD.

</details>


### [88] [MarkTune: Improving the Quality-Detectability Trade-off in Open-Weight LLM Watermarking](https://arxiv.org/abs/2512.04044)
*Yizhou Zhao,Zhiwei Steven Wu,Adam Block*

Main category: cs.LG

TL;DR: MarkTune是一种用于开放权重语言模型的理论化、基于策略的微调框架，通过将水印信号作为奖励并同时正则化文本质量下降，显著改善了GaussMark的质量-可检测性权衡，接近推理时水印的性能。


<details>
  <summary>Details</summary>
Motivation: 开放权重语言模型对水印技术提出了严峻挑战，因为一旦模型权重公开，就无法强制执行推理时干预。现有的开放权重水印技术（如GaussMark）通常需要对模型权重进行微小修改，这虽然能产生可检测信号，但达到与推理时水印相当的检测能力通常需要明显降低生成质量的权重扰动。

Method: MarkTune是一个理论化、基于策略的微调框架，将GaussMark信号作为奖励，同时通过正则化防止文本质量下降。该方法在模型的表示空间内引导更细粒度、水印感知的权重更新，同时保持生成质量。

Result: MarkTune将GaussMark的质量-可检测性前沿推近到接近推理时水印的水平，对改写和微调攻击保持鲁棒性，并表现出强大的泛化能力：在一个数据集上微调的模型在未见数据集上仍保持显著的水印检测能力。

Conclusion: MarkTune作为一种通用策略，能够将鲁棒、高质量的水印嵌入到开放权重语言模型中，解决了开放权重模型水印的关键挑战。

Abstract: Watermarking aims to embed hidden signals in generated text that can be reliably detected when given access to a secret key. Open-weight language models pose acute challenges for such watermarking schemes because the inference-time interventions that dominate contemporary approaches cannot be enforced once model weights are public. Existing watermaking techniques for open-weight models, such as the recently proposed GaussMark, typically rely on small modifications to model weights, which can yield signals detectable to those equipped with a secret key, but achieving detection power comparable to inference-time watermarks generally requires weight perturbations that noticeably reduce generation quality. We introduce MarkTune, a theoretically principled, on-policy fine-tuning framework that treats the GaussMark signal as a reward while simultaneously regularizing against degradation in text quality. We derive MarkTune as an improvement on GaussMark and demonstrate that MarkTune consistently improves the quality-detectability trade-off over GaussMark by steering finer-grained, watermark-aware weight updates within the model's representation space while preserving generation quality. Empirically, we show that MarkTune pushes the quality-detectability frontier of GaussMark close to that of inference-time watermarking, remains robust to paraphrasing and fine-tuning attacks, and exhibits strong generalization: a model fine-tuned on one dataset retains substantial watermark detection power on unseen datasets. Together, these results establish MarkTune as a general strategy for embedding robust, high-quality watermarks into open-weight LMs.

</details>


### [89] [Fare Comparison App of Uber, Ola and Rapido](https://arxiv.org/abs/2512.04065)
*Ashlesha Gopinath Sawant,Sahil S. Jadhav,Vidhan R. Jain,Shriraj S. Jagtap,Prachi Jadhav,Soham Jadhav,Ichha Raina*

Main category: cs.LG

TL;DR: 开发了一个网约车比价Web应用，通过API获取Ola、Uber、Rapido的实时价格，为用户提供最优出行选择


<details>
  <summary>Details</summary>
Motivation: 用户在日常出行中选择网约车服务时面临困难，难以找到既经济又高效的出行方案，需要透明化的比价工具来优化选择

Method: 构建Web应用程序，使用Python后端通过API获取各平台实时票价数据，结合Android Studio模拟器、Appium和位置比较技术处理数据访问挑战

Result: 成功开发了能够比较Ola、Uber、Rapido价格的Web应用，为用户提供透明化的票价对比和最优出行建议

Conclusion: 该项目提高了网约车服务的透明度，增强了用户体验和出行效率，解决了用户在选择合适出行方案时面临的实际问题

Abstract: In todays increasing world, it is very important to have good hailing services like Ola, Uber, and Rapido as it is very essential for our daily transportation. Users often face difficulties in choosing the most appropriate and efficient ride that would lead to both cost-effective and would take us to our destination in less time. This project provides you with the web application that helps you to select the most beneficial ride for you by providing users with the fare comparison between Ola, Uber, Rapido for the destination entered by the user. The backend is use to fetch the data, providing users with the fare comparison for the ride and finally providing with the best option using Python. This research paper also addresses the problem and challenges faced in accessing the data using APIs, Android Studios emulator, Appium and location comparison. Thus, the aim of the project is to provide transparency to the users in ride-hailing services and increase efficiency and provide users with better experience.

</details>


### [90] [Adaptive sampling using variational autoencoder and reinforcement learning](https://arxiv.org/abs/2512.03525)
*Adil Rasheed,Mikael Aleksander Jansen Shahly,Muhammad Faisal Aftab*

Main category: cs.LG

TL;DR: 提出自适应稀疏感知框架，结合变分自编码器先验和强化学习进行顺序测量选择，优于传统压缩感知、最优传感器布置和基于生成模型的方法。


<details>
  <summary>Details</summary>
Motivation: 传统压缩感知使用通用基和随机测量，效率和质量受限；最优传感器布置基于历史数据但使用固定线性基，无法适应非线性或样本特异性变化；基于生成模型的压缩感知虽改进重建但仍使用次优随机采样。

Method: 提出自适应稀疏感知框架，将变分自编码器先验与强化学习耦合，实现顺序测量选择。

Result: 实验表明该方法在稀疏测量重建方面优于压缩感知、最优传感器布置和基于生成模型的重建方法。

Conclusion: 自适应稀疏感知框架通过结合深度生成先验和顺序测量选择策略，显著提升了稀疏采样重建性能。

Abstract: Compressed sensing enables sparse sampling but relies on generic bases and random measurements, limiting efficiency and reconstruction quality. Optimal sensor placement uses historcal data to design tailored sampling patterns, yet its fixed, linear bases cannot adapt to nonlinear or sample-specific variations. Generative model-based compressed sensing improves reconstruction using deep generative priors but still employs suboptimal random sampling. We propose an adaptive sparse sensing framework that couples a variational autoencoder prior with reinforcement learning to select measurements sequentially. Experiments show that this approach outperforms CS, OSP, and Generative model-based reconstruction from sparse measurements.

</details>


### [91] [Parameter-Efficient Augment Plugin for Class-Incremental Learning](https://arxiv.org/abs/2512.03537)
*Zhiming Xu,Baile Xu,Jian Zhao,Furao Shen,Suorong Yang*

Main category: cs.LG

TL;DR: 本文提出了一种基于LoRA插件扩展的类增量学习方法DLC，通过在基础模型上添加任务特定的低秩适配器，实现了高效参数扩展和性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的类增量学习方法存在遗忘问题或稳定性-可塑性困境，而扩展方法虽然能提高准确率但需要大量参数增加。需要一种既高效又能保持性能的增量学习方案。

Method: 提出DLC方法：1) 将基于回放或蒸馏训练的特征提取器作为基础模型；2) 为每个任务使用LoRA在基础模型的深层注入任务特定的残差；3) 推理时聚合带任务特定残差的表示；4) 引入轻量级加权单元来减轻非目标LoRA插件的干扰。

Result: 在ImageNet-100上，仅使用标准ResNet-18 4%的参数，DLC实现了8%的准确率提升，表现出卓越的效率。在固定内存预算下超越了最先进方法。

Conclusion: DLC提供了一种类似软件可下载内容的即插即用增强方案，能够高效扩展基础方法，在类增量学习中实现了参数效率和性能的良好平衡。

Abstract: Existing class-incremental learning (CIL) approaches based on replay or knowledge distillation are often constrained by forgetting or the stability-plasticity dilemma. Some expansion-based approaches could achieve higher accuracy. However, they always require significant parameter increases. In this paper, we propose a plugin extension paradigm termed the Deployment of extra LoRA Components (DLC) for non-pre-trained CIL scenarios.We treat the feature extractor trained through replay or distillation as a base model with rich knowledge. For each task, we use Low-Rank Adaptation (LoRA) to inject task-specific residuals into the base model's deep layers. During inference, representations with task-specific residuals are aggregated to produce classification predictions. To mitigate interference from non-target LoRA plugins, we introduce a lightweight weighting unit. This unit learns to assign importance scores to different LoRA-tuned representations. Like downloadable contents in software, our method serves as a plug-and-play enhancement that efficiently extends the base methods. Remarkably, on the large-scale ImageNet-100, with merely 4 % of the parameters of a standard ResNet-18, our DLC model achieves a significant 8 % improvement in accuracy, demonstrating exceptional efficiency. Moreover, it could surpass state-of-the-art methods under the fixed memory budget.

</details>


### [92] [Towards Irreversible Machine Unlearning for Diffusion Models](https://arxiv.org/abs/2512.03564)
*Xun Yuan,Zilong Zhao,Jiayu Li,Aryan Pasikhani,Prosanta Gope,Biplab Sikdar*

Main category: cs.LG

TL;DR: 本文提出了一种针对扩散模型遗忘学习的攻击方法DiMRA，能够逆转基于微调的遗忘学习效果，同时提出了新的遗忘学习方法DiMUM来增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成合成图像方面表现出色，但存在安全、隐私和版权问题，需要遗忘学习技术来忘记特定训练数据。然而，现有的基于微调的遗忘学习方法存在安全漏洞，可能被攻击逆转。

Method: 提出了两种方法：1) DiMRA攻击方法：在不知道遗忘元素的情况下，通过在辅助数据集上优化已遗忘的扩散模型来逆转遗忘效果；2) DiMUM遗忘学习方法：通过记忆替代数据或特征来替换目标遗忘数据，而不是简单地忘记。

Result: 实验证明DiMRA能够有效逆转最先进的基于微调的扩散模型遗忘学习方法，揭示了现有技术的脆弱性。DiMUM在保持扩散模型生成性能的同时，显著增强了对抗DiMRA攻击的鲁棒性。

Conclusion: 基于微调的扩散模型遗忘学习方法存在被逆转攻击的风险，需要更鲁棒的解决方案。DiMUM通过记忆替代数据的方法提供了更强的安全性，为扩散模型遗忘学习提供了新的方向。

Abstract: Diffusion models are renowned for their state-of-the-art performance in generating synthetic images. However, concerns related to safety, privacy, and copyright highlight the need for machine unlearning, which can make diffusion models forget specific training data and prevent the generation of sensitive or unwanted content. Current machine unlearning methods for diffusion models are primarily designed for conditional diffusion models and focus on unlearning specific data classes or features. Among these methods, finetuning-based machine unlearning methods are recognized for their efficiency and effectiveness, which update the parameters of pre-trained diffusion models by minimizing carefully designed loss functions. However, in this paper, we propose a novel attack named Diffusion Model Relearning Attack (DiMRA), which can reverse the finetuning-based machine unlearning methods, posing a significant vulnerability of this kind of technique. Without prior knowledge of the unlearning elements, DiMRA optimizes the unlearned diffusion model on an auxiliary dataset to reverse the unlearning, enabling the model to regenerate previously unlearned elements. To mitigate this vulnerability, we propose a novel machine unlearning method for diffusion models, termed as Diffusion Model Unlearning by Memorization (DiMUM). Unlike traditional methods that focus on forgetting, DiMUM memorizes alternative data or features to replace targeted unlearning data or features in order to prevent generating such elements. In our experiments, we demonstrate the effectiveness of DiMRA in reversing state-of-the-art finetuning-based machine unlearning methods for diffusion models, highlighting the need for more robust solutions. We extensively evaluate DiMUM, demonstrating its superior ability to preserve the generative performance of diffusion models while enhancing robustness against DiMRA.

</details>


### [93] [Optimal Transportation and Alignment Between Gaussian Measures](https://arxiv.org/abs/2512.03579)
*Sanjit Dandapanthula,Aleksandr Podkopaev,Shiva Prasad Kasiviswanathan,Aaditya Ramdas,Ziv Goldfeld*

Main category: cs.LG

TL;DR: 该论文针对高斯分布在二次成本下的最优传输和Gromov-Wasserstein对齐问题，提供了封闭形式解和高效算法，填补了文献空白并扩展了应用范围。


<details>
  <summary>Details</summary>
Motivation: 最优传输和Gromov-Wasserstein对齐是数据科学中比较、转换和聚合异构数据集的重要几何框架，但计算成本高昂。现有方法主要依赖高斯分布在二次成本下的封闭形式解，但存在未解决的问题限制了应用范围。

Method: 1. 针对非中心化高斯分布的内积GW对齐问题，给出了封闭形式表达式（需通过酉算子进行二次优化），并推导了紧致的解析上下界；2. 当至少一个高斯分布中心化时，提供完全封闭形式解；3. 扩展了中心化高斯分布的IGW重心解析解；4. 将高斯多边际OT问题简化为可处理优化问题，并提出使用秩缺陷约束的高效算法。

Result: 1. 解决了可分离希尔伯特空间中非中心化高斯分布的IGW对齐开放问题；2. 为中心化高斯分布提供了完全封闭形式解；3. 推导了IGW重心的解析解；4. 开发了高斯多边际OT的高效算法；5. 在知识蒸馏和异构聚类任务中验证了方法的实用性。

Conclusion: 该工作全面解决了高斯分布二次成本下的OT和IGW对齐问题，填补了文献空白，提供了高效算法和解析解，显著扩展了这些几何框架在数据科学和机器学习中的应用范围。

Abstract: Optimal transport (OT) and Gromov-Wasserstein (GW) alignment provide interpretable geometric frameworks for comparing, transforming, and aggregating heterogeneous datasets -- tasks ubiquitous in data science and machine learning. Because these frameworks are computationally expensive, large-scale applications often rely on closed-form solutions for Gaussian distributions under quadratic cost. This work provides a comprehensive treatment of Gaussian, quadratic cost OT and inner product GW (IGW) alignment, closing several gaps in the literature to broaden applicability. First, we treat the open problem of IGW alignment between uncentered Gaussians on separable Hilbert spaces by giving a closed-form expression up to a quadratic optimization over unitary operators, for which we derive tight analytic upper and lower bounds. If at least one Gaussian measure is centered, the solution reduces to a fully closed-form expression, which we further extend to an analytic solution for the IGW barycenter between centered Gaussians. We also present a reduction of Gaussian multimarginal OT with pairwise quadratic costs to a tractable optimization problem and provide an efficient algorithm to solve it using a rank-deficiency constraint. To demonstrate utility, we apply our results to knowledge distillation and heterogeneous clustering on synthetic and real-world datasets.

</details>


### [94] [Federated Learning and Trajectory Compression for Enhanced AIS Coverage](https://arxiv.org/abs/2512.03584)
*Thomas Gräupl,Andreas Reisenbauer,Marcel Hecko,Anil Rasouli,Anita Graser,Melitta Dragaschnig,Axel Weissenfeld,Gilles Dejaegere,Mahmoud Sakr*

Main category: cs.LG

TL;DR: VesselEdge系统利用联邦学习和带宽受限轨迹压缩技术，通过扩展AIS覆盖范围来增强海上态势感知能力，将船舶转变为移动传感器，实现实时异常检测和低带宽下的高效数据传输。


<details>
  <summary>Details</summary>
Motivation: 当前海上态势感知面临AIS覆盖范围有限、数据传输带宽受限等挑战，需要一种能够有效扩展AIS覆盖、实现实时异常检测并适应低带宽环境的解决方案。

Method: 系统整合了M3fed联邦学习模型和BWC-DR-A轨迹压缩算法，将船舶转变为移动传感器，优先传输异常数据，在低带宽连接下实现高效数据传输。

Result: 初步结果表明，VesselEdge系统能够有效提高AIS覆盖范围和海上态势感知能力，使用历史数据验证了系统的有效性。

Conclusion: VesselEdge系统通过结合联邦学习和轨迹压缩技术，为解决海上态势感知中的覆盖范围和数据传输问题提供了一种创新解决方案，具有实际应用潜力。

Abstract: This paper presents the VesselEdge system, which leverages federated learning and bandwidth-constrained trajectory compression to enhance maritime situational awareness by extending AIS coverage. VesselEdge transforms vessels into mobile sensors, enabling real-time anomaly detection and efficient data transmission over low-bandwidth connections. The system integrates the M3fed model for federated learning and the BWC-DR-A algorithm for trajectory compression, prioritizing anomalous data. Preliminary results demonstrate the effectiveness of VesselEdge in improving AIS coverage and situational awareness using historical data.

</details>


### [95] [Observation-driven correction of numerical weather prediction for marine winds](https://arxiv.org/abs/2512.03606)
*Matteo Peduto,Qidong Yang,Jonathan Giezendanner,Devis Tuia,Sherrie Wang*

Main category: cs.LG

TL;DR: 该论文提出了一种基于Transformer的深度学习架构，通过同化最新现场观测数据来校正全球天气预报系统(GFS)的风速预报，显著提高了海洋风速预测精度。


<details>
  <summary>Details</summary>
Motivation: 海洋风速预报对航行安全、船舶路径规划和能源运营至关重要，但由于海洋观测数据稀疏、异构且时间变化大，准确预报仍然具有挑战性。现有数值天气预报模型存在系统性误差，需要有效的后处理方法来校正这些误差。

Method: 将风速预报重新定义为观测信息化的全球数值天气预报模型校正问题。提出基于Transformer的深度学习架构：1) 通过掩码和基于集合的注意力机制处理不规则和时间变化的观测集；2) 通过交叉注意力将预测条件化于最近的观测-预报对；3) 使用循环时间嵌入和坐标感知位置表示，实现在任意空间坐标的单次推理。模型能够处理多种观测平台数据。

Result: 在大西洋区域使用ICOADS观测数据评估，模型在所有48小时预报时效内均降低了GFS 10米风速的RMSE：1小时预报时效改善45%，48小时预报时效改善13%。空间分析显示在海岸线和航运路线等观测密集区域改进最为显著。架构能够同时生成站点特定预测和流域尺度网格化产品。

Conclusion: 该方法展示了一种实用、低延迟的后处理策略，通过校正系统性预报误差来补充数值天气预报，能够处理异构观测平台数据，在单次前向传播中生成多尺度预测产品。

Abstract: Accurate marine wind forecasts are essential for safe navigation, ship routing, and energy operations, yet they remain challenging because observations over the ocean are sparse, heterogeneous, and temporally variable. We reformulate wind forecasting as observation-informed correction of a global numerical weather prediction (NWP) model. Rather than forecasting winds directly, we learn local correction patterns by assimilating the latest in-situ observations to adjust the Global Forecast System (GFS) output. We propose a transformer-based deep learning architecture that (i) handles irregular and time-varying observation sets through masking and set-based attention mechanisms, (ii) conditions predictions on recent observation-forecast pairs via cross-attention, and (iii) employs cyclical time embeddings and coordinate-aware location representations to enable single-pass inference at arbitrary spatial coordinates. We evaluate our model over the Atlantic Ocean using observations from the International Comprehensive Ocean-Atmosphere Data Set (ICOADS) as reference. The model reduces GFS 10-meter wind RMSE at all lead times up to 48 hours, achieving 45% improvement at 1-hour lead time and 13% improvement at 48-hour lead time. Spatial analyses reveal the most persistent improvements along coastlines and shipping routes, where observations are most abundant. The tokenized architecture naturally accommodates heterogeneous observing platforms (ships, buoys, tide gauges, and coastal stations) and produces both site-specific predictions and basin-scale gridded products in a single forward pass. These results demonstrate a practical, low-latency post-processing approach that complements NWP by learning to correct systematic forecast errors.

</details>


### [96] [Cyclical Temporal Encoding and Hybrid Deep Ensembles for Multistep Energy Forecasting](https://arxiv.org/abs/2512.03656)
*Salim Khazem,Houssam Kanso*

Main category: cs.LG

TL;DR: 该论文提出了一种结合循环时间编码与混合LSTM-CNN架构的统一深度学习框架，用于提升多步电力消费预测精度，通过实验验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 准确的电力消费预测对于需求管理和智能电网运营至关重要。现有方法在处理周期性时间模式和长短期特征结合方面存在不足，需要更有效的预测框架。

Method: 1. 使用正弦余弦编码系统性地转换基于日历的属性，以保留周期性结构；2. 通过相关性分析评估预测相关性；3. 采用由LSTM、CNN和针对每个预测时域专门设计的MLP回归器元学习器组成的集成模型，以利用长期季节效应和短期局部模式；4. 使用一年的全国消费数据集进行实验研究。

Result: 在所有七个预测时域上均取得了一致的改进，混合模型在RMSE和MAE指标上优于单一架构和现有方法。消融分析显示循环编码和日历特征对性能提升有显著贡献。

Conclusion: 结合循环时间表示与互补的深度学习结构具有明显优势。据作者所知，这是首个在统一的短期能源预测框架中联合评估时间编码、日历特征和混合集成架构的工作。

Abstract: Accurate electricity consumption forecasting is essential for demand management and smart grid operations. This paper introduces a unified deep learning framework that integrates cyclical temporal encoding with hybrid LSTM-CNN architectures to enhance multistep energy forecasting. We systematically transform calendar-based attributes using sine cosine encodings to preserve periodic structure and evaluate their predictive relevance through correlation analysis. To exploit both long-term seasonal effects and short-term local patterns, we employ an ensemble model composed of an LSTM, a CNN, and a meta-learner of MLP regressors specialized for each forecast horizon. Using a one year national consumption dataset, we conduct an extensive experimental study including ablation analyses with and without cyclical encodings and calendar features and comparisons with established baselines from the literature. Results demonstrate consistent improvements across all seven forecast horizons, with our hybrid model achieving lower RMSE and MAE than individual architectures and prior methods. These findings confirm the benefit of combining cyclical temporal representations with complementary deep learning structures. To our knowledge, this is the first work to jointly evaluate temporal encodings, calendar-based features, and hybrid ensemble architectures within a unified short-term energy forecasting framework.

</details>


### [97] [Feature-aware Modulation for Learning from Temporal Tabular Data](https://arxiv.org/abs/2512.03678)
*Hao-Run Cai,Han-Jia Ye*

Main category: cs.LG

TL;DR: 该论文提出了一种特征感知的时间调制机制，通过调整特征的统计属性来应对表格数据中的时间分布偏移问题，在保持模型泛化能力的同时实现适应性。


<details>
  <summary>Details</summary>
Motivation: 现实世界表格机器学习部署中面临时间分布偏移的挑战，特征与标签之间的关系随时间不断演变。静态模型假设固定映射以保证泛化性，而自适应模型可能过度拟合瞬态模式，导致鲁棒性与适应性之间的困境。

Method: 提出特征感知的时间调制机制，基于时间上下文条件化特征表示，调制特征的统计属性（如尺度和偏度），通过跨时间对齐特征语义实现轻量级但强大的适应能力。

Result: 基准评估验证了该方法在处理表格数据时间偏移方面的有效性，能够有效平衡泛化性和适应性。

Conclusion: 通过分析特征语义演变（特别是客观和主观意义）引入的概念漂移，发现特征转换策略能够缓解跨时间阶段特征表示的差异，提出的特征感知时间调制机制为此提供了有效解决方案。

Abstract: While tabular machine learning has achieved remarkable success, temporal distribution shifts pose significant challenges in real-world deployment, as the relationships between features and labels continuously evolve. Static models assume fixed mappings to ensure generalization, whereas adaptive models may overfit to transient patterns, creating a dilemma between robustness and adaptability. In this paper, we analyze key factors essential for constructing an effective dynamic mapping for temporal tabular data. We discover that evolving feature semantics-particularly objective and subjective meanings-introduce concept drift over time. Crucially, we identify that feature transformation strategies are able to mitigate discrepancies in feature representations across temporal stages. Motivated by these insights, we propose a feature-aware temporal modulation mechanism that conditions feature representations on temporal context, modulating statistical properties such as scale and skewness. By aligning feature semantics across time, our approach achieves a lightweight yet powerful adaptation, effectively balancing generalizability and adaptability. Benchmark evaluations validate the effectiveness of our method in handling temporal shifts in tabular data.

</details>


### [98] [Unlocking the Invisible Urban Traffic Dynamics under Extreme Weather: A New Physics-Constrained Hamiltonian Learning Algorithm](https://arxiv.org/abs/2512.03744)
*Xuhui Lin,Qiuchen Lu*

Main category: cs.LG

TL;DR: 提出一种基于物理约束哈密顿学习算法的新方法，用于检测城市交通系统在极端天气后的"虚假恢复"现象，识别传统表面指标无法发现的隐藏结构损伤。


<details>
  <summary>Details</summary>
Motivation: 当前城市交通系统韧性评估方法依赖表面恢复指标，无法区分真正恢复和"虚假恢复"（交通指标正常化但系统动力学永久退化），需要能检测隐藏结构损伤的新方法。

Method: 开发物理约束哈密顿学习算法，结合"结构不可逆性检测"和"能量景观重构"，提取低维状态表示，通过物理约束优化识别准哈密顿结构，并通过能量景观比较量化结构变化。

Result: 对伦敦2021年极端降雨的分析显示，虽然表面指标完全恢复，但算法检测到64.8%的结构损伤被传统监测方法遗漏。

Conclusion: 该框架为主动结构风险评估提供工具，使基础设施投资能够基于真实的系统健康状况而非误导性的表面指标。

Abstract: Urban transportation systems face increasing resilience challenges from extreme weather events, but current assessment methods rely on surface-level recovery indicators that miss hidden structural damage. Existing approaches cannot distinguish between true recovery and "false recovery," where traffic metrics normalize, but the underlying system dynamics permanently degrade. To address this, a new physics-constrained Hamiltonian learning algorithm combining "structural irreversibility detection" and "energy landscape reconstruction" has been developed. Our approach extracts low-dimensional state representations, identifies quasi-Hamiltonian structures through physics-constrained optimization, and quantifies structural changes via energy landscape comparison. Analysis of London's extreme rainfall in 2021 demonstrates that while surface indicators were fully recovered, our algorithm detected 64.8\% structural damage missed by traditional monitoring. Our framework provides tools for proactive structural risk assessment, enabling infrastructure investments based on true system health rather than misleading surface metrics.

</details>


### [99] [Universally Converging Representations of Matter Across Scientific Foundation Models](https://arxiv.org/abs/2512.03750)
*Sathya Edamadaka,Soojung Yang,Ju Li,Rafael Gómez-Bombarelli*

Main category: cs.LG

TL;DR: 研究发现近60个科学模型在化学系统上表现出高度对齐的内部表示，表明基础模型可能学习到了物理现实的共同底层表示，但模型在训练数据之外的结构上仍存在局限性。


<details>
  <summary>Details</summary>
Motivation: 理解不同模态和架构的科学模型是否学习到相似的内部表示对于构建可靠泛化的科学基础模型至关重要。虽然语言和视觉领域已观察到表示收敛现象，但在科学领域尚未系统探索。

Method: 研究了近60个科学模型，涵盖字符串、图、3D原子结构和蛋白质等多种模态，分析它们在各种化学系统上的表示对齐情况。比较了在不同数据集上训练的模型，并观察机器学习原子间势能在表示空间中的收敛行为。

Result: 1) 在不同数据集上训练的小分子模型具有高度相似的表示；2) 机器学习原子间势能随着性能提升在表示空间中收敛；3) 发现两种不同的模型机制：在类似训练数据上，高性能模型紧密对齐，弱模型则发散到局部最优；在训练数据差异大的结构上，几乎所有模型都坍缩到低信息表示。

Conclusion: 科学模型确实学习到了共同的底层物理现实表示，但当前模型仍受限于训练数据和归纳偏置，尚未编码真正通用的结构。表示对齐可作为科学基础模型通用性的定量基准，有助于追踪通用表示的出现并选择跨模态、物质领域和科学任务的最佳模型。

Abstract: Machine learning models of vastly different modalities and architectures are being trained to predict the behavior of molecules, materials, and proteins. However, it remains unclear whether they learn similar internal representations of matter. Understanding their latent structure is essential for building scientific foundation models that generalize reliably beyond their training domains. Although representational convergence has been observed in language and vision, its counterpart in the sciences has not been systematically explored. Here, we show that representations learned by nearly sixty scientific models, spanning string-, graph-, 3D atomistic, and protein-based modalities, are highly aligned across a wide range of chemical systems. Models trained on different datasets have highly similar representations of small molecules, and machine learning interatomic potentials converge in representation space as they improve in performance, suggesting that foundation models learn a common underlying representation of physical reality. We then show two distinct regimes of scientific models: on inputs similar to those seen during training, high-performing models align closely and weak models diverge into local sub-optima in representation space; on vastly different structures from those seen during training, nearly all models collapse onto a low-information representation, indicating that today's models remain limited by training data and inductive bias and do not yet encode truly universal structure. Our findings establish representational alignment as a quantitative benchmark for foundation-level generality in scientific models. More broadly, our work can track the emergence of universal representations of matter as models scale, and for selecting and distilling models whose learned representations transfer best across modalities, domains of matter, and scientific tasks.

</details>


### [100] [Origin-Conditional Trajectory Encoding: Measuring Urban Configurational Asymmetries through Neural Decomposition](https://arxiv.org/abs/2512.03755)
*Stephen Law,Tao Yang,Nanjiang Chen,Xuhui Lin*

Main category: cs.LG

TL;DR: 提出条件轨迹编码器框架，联合学习空间和运动表征，解决轨迹分析中时空分离、方向不对称和过度依赖辅助数据的问题，量化城市形态造成的认知不平等。


<details>
  <summary>Details</summary>
Motivation: 当前城市轨迹分析方法存在三个主要问题：1) 空间表征与时间动态分离训练；2) 忽略导航中的方向不对称性（A→B ≠ B→A）；3) 过度依赖POI、图像等辅助数据而非城市空间基本几何特性。需要开发能联合学习时空表征并保持方向不对称性的方法。

Method: 提出条件轨迹编码器框架，使用双向LSTM处理可见性比率和曲率等几何特征，结合可学习的起点嵌入进行条件编码。通过对比学习将表征分解为共享城市模式和起点特定特征，保留起点依赖的方向不对称性。

Result: 在六个合成城市和北京西城区的真实数据验证表明，城市形态会产生系统性认知不平等。该框架能够量化不同起点位置的认知不对称性，为城市规划和导航系统提供分析工具。

Conclusion: 该研究为城市分析提供了量化认知不平等的方法，为城市规划者提供评估体验公平性的工具，为建筑师提供布局决策的认知影响洞察，并为导航系统实现起点感知分析。

Abstract: Urban analytics increasingly relies on AI-driven trajectory analysis, yet current approaches suffer from methodological fragmentation: trajectory learning captures movement patterns but ignores spatial context, while spatial embedding methods encode street networks but miss temporal dynamics. Three gaps persist: (1) lack of joint training that integrates spatial and temporal representations, (2) origin-agnostic treatment that ignores directional asymmetries in navigation ($A \to B \ne B \to A$), and (3) over-reliance on auxiliary data (POIs, imagery) rather than fundamental geometric properties of urban space. We introduce a conditional trajectory encoder that jointly learns spatial and movement representations while preserving origin-dependent asymmetries using geometric features. This framework decomposes urban navigation into shared cognitive patterns and origin-specific spatial narratives, enabling quantitative measurement of cognitive asymmetries across starting locations. Our bidirectional LSTM processes visibility ratio and curvature features conditioned on learnable origin embeddings, decomposing representations into shared urban patterns and origin-specific signatures through contrastive learning. Results from six synthetic cities and real-world validation on Beijing's Xicheng District demonstrate that urban morphology creates systematic cognitive inequalities. This provides urban planners quantitative tools for assessing experiential equity, offers architects insights into layout decisions' cognitive impacts, and enables origin-aware analytics for navigation systems.

</details>


### [101] [Forensic Activity Classification Using Digital Traces from iPhones: A Machine Learning-based Approach](https://arxiv.org/abs/2512.03786)
*Conor McCarthy,Jan Peter van Zandwijk,Marcel Worring,Zeno Geradts*

Main category: cs.LG

TL;DR: 基于智能手机和智能手表运动传感器数据，开发机器学习方法将数字痕迹转化为不同身体活动的似然比，用于法医调查中的活动识别和时序分析。


<details>
  <summary>Details</summary>
Motivation: 智能手机和智能手表在日常生活中的普及提供了丰富的用户行为信息，其内置运动传感器产生的数字痕迹为法医调查人员了解个人身体活动提供了机会。

Method: 提出基于机器学习的方法，将数字痕迹转化为不同身体活动的似然比（LRs），并在新的数据集NFI_FARED上进行评估，该数据集包含来自四种不同iPhone的数字痕迹，标记了19种活动。

Result: 在171种可能的活动配对中，该方法能够为167种配对产生有用的似然比系统。方法还扩展到同时分析多个活动（或活动组）的可能性，并创建活动时间线以辅助法医调查的早期和后期阶段。

Conclusion: 该方法在法医调查中具有实用价值，能够有效区分不同身体活动，数据集和代码已公开以促进该领域的进一步研究。

Abstract: Smartphones and smartwatches are ever-present in daily life, and provide a rich source of information on their users' behaviour. In particular, digital traces derived from the phone's embedded movement sensors present an opportunity for a forensic investigator to gain insight into a person's physical activities. In this work, we present a machine learning-based approach to translate digital traces into likelihood ratios (LRs) for different types of physical activities. Evaluating on a new dataset, NFI\_FARED, which contains digital traces from four different types of iPhones labelled with 19 activities, it was found that our approach could produce useful LR systems to distinguish 167 out of a possible 171 activity pairings. The same approach was extended to analyse likelihoods for multiple activities (or groups of activities) simultaneously and create activity timelines to aid in both the early and latter stages of forensic investigations. The dataset and all code required to replicate the results have also been made public to encourage further research on this topic.

</details>


### [102] [Adaptive Identification and Modeling of Clinical Pathways with Process Mining](https://arxiv.org/abs/2512.03787)
*Francesco Vitale,Nicola Mazzocca*

Main category: cs.LG

TL;DR: 该论文提出了一种基于过程挖掘的两阶段临床路径建模方法，通过一致性检查诊断扩展临床路径知识库，能够自动发现和建模疾病变体或组合的最佳治疗实践。


<details>
  <summary>Details</summary>
Motivation: 临床路径是标准化的医疗计划，但手动建模基于临床指南和领域专业知识很困难，且可能无法反映不同疾病变体或组合的实际最佳实践。需要一种自动化方法来扩展临床路径知识库。

Method: 采用两阶段过程挖掘方法：第一阶段收集特定疾病的历史数据，构建参考过程模型；第二阶段将新数据与参考模型进行一致性检查，基于检查结果扩展知识库，为新的疾病变体或组合创建更具体的模型。

Result: 使用Synthea基准数据集（模拟SARS-CoV-2感染和COVID-19并发症的患者治疗）进行验证，结果显示该方法能以足够的精度扩展临床路径知识库，AUC峰值达到95.62%，同时保持67.11%的弧度简洁性。

Conclusion: 提出的两阶段过程挖掘方法能够有效扩展临床路径知识库，自动发现和建模针对不同疾病变体或组合的最佳治疗实践，提高临床路径的适应性和实用性。

Abstract: Clinical pathways are specialized healthcare plans that model patient treatment procedures. They are developed to provide criteria-based progression and standardize patient treatment, thereby improving care, reducing resource use, and accelerating patient recovery. However, manual modeling of these pathways based on clinical guidelines and domain expertise is difficult and may not reflect the actual best practices for different variations or combinations of diseases. We propose a two-phase modeling method using process mining, which extends the knowledge base of clinical pathways by leveraging conformance checking diagnostics. In the first phase, historical data of a given disease is collected to capture treatment in the form of a process model. In the second phase, new data is compared against the reference model to verify conformance. Based on the conformance checking results, the knowledge base can be expanded with more specific models tailored to new variants or disease combinations. We demonstrate our approach using Synthea, a benchmark dataset simulating patient treatments for SARS-CoV-2 infections with varying COVID-19 complications. The results show that our method enables expanding the knowledge base of clinical pathways with sufficient precision, peaking to 95.62% AUC while maintaining an arc-degree simplicity of 67.11%.

</details>


### [103] [EfficientECG: Cross-Attention with Feature Fusion for Efficient Electrocardiogram Classification](https://arxiv.org/abs/2512.03804)
*Hanhui Deng,Xinglin Li,Jie Luo,Zhanpeng Jin,Di Wu*

Main category: cs.LG

TL;DR: 该论文提出了一种基于深度学习的ECG诊断模型，包括轻量级分类模型EfficientECG和跨注意力特征融合模型，用于准确分析多导联ECG数据并融合多特征信息。


<details>
  <summary>Details</summary>
Motivation: 心电图（ECG）是一种快速、无创且信息丰富的诊断信号，但现有ECG模型误诊率高。研究旨在开发能够自动提取ECG特征、准确快速诊断的深度学习模型，减轻医疗工作者负担。

Method: 1. 提出EfficientECG：基于EfficientNet的轻量级ECG分类模型，能有效处理高频长序列多导联ECG数据；2. 提出跨注意力特征融合模型：在EfficientECG基础上，融合多导联ECG数据与多特征信息（如性别、年龄）。

Result: 在代表性ECG数据集上的评估验证了所提模型在精度、多特征融合能力和轻量化方面的优越性，优于现有最先进方法。

Conclusion: 该研究提出的深度学习方法能够自动提取ECG特征，构建准确快速的诊断模型，有效降低误诊率并减轻医疗负担，为ECG分析提供了新的技术方案。

Abstract: Electrocardiogram is a useful diagnostic signal that can detect cardiac abnormalities by measuring the electrical activity generated by the heart. Due to its rapid, non-invasive, and richly informative characteristics, ECG has many emerging applications. In this paper, we study novel deep learning technologies to effectively manage and analyse ECG data, with the aim of building a diagnostic model, accurately and quickly, that can substantially reduce the burden on medical workers. Unlike the existing ECG models that exhibit a high misdiagnosis rate, our deep learning approaches can automatically extract the features of ECG data through end-to-end training. Specifically, we first devise EfficientECG, an accurate and lightweight classification model for ECG analysis based on the existing EfficientNet model, which can effectively handle high-frequency long-sequence ECG data with various leading types. On top of that, we next propose a cross-attention-based feature fusion model of EfficientECG for analysing multi-lead ECG data with multiple features (e.g., gender and age). Our evaluations on representative ECG datasets validate the superiority of our model against state-of-the-art works in terms of high precision, multi-feature fusion, and lightweights.

</details>


### [104] [Deep Reinforcement Learning for Dynamic Algorithm Configuration: A Case Study on Optimizing OneMax with the (1+($λ$,$λ$))-GA](https://arxiv.org/abs/2512.03805)
*Tai Nguyen,Phong Le,André Biedenkapp,Carola Doerr,Nguyen Dang*

Main category: cs.LG

TL;DR: 本文对深度强化学习在动态算法配置中的应用进行了系统性研究，重点分析了DDQN和PPO算法在控制(1+(λ,λ))-GA种群大小参数时的表现，揭示了可扩展性下降和学习不稳定性两大挑战，并提出了自适应奖励偏移机制等解决方案。


<details>
  <summary>Details</summary>
Motivation: 动态算法配置研究如何为参数化优化算法高效识别控制策略。虽然许多研究利用强化学习的鲁棒性来解决算法配置的优化挑战，但将RL应用于DAC存在困难且通常需要大量领域专业知识。本文旨在通过系统性分析深度RL算法在DAC中的表现，揭示其根本挑战并提供解决方案。

Method: 通过控制(1+(λ,λ))-GA在OneMax实例上的种群大小参数，对DDQN和PPO算法进行系统性分析。针对发现的探索不足和规划视野覆盖问题，提出了自适应奖励偏移机制（利用奖励分布统计增强DDQN探索）和无折扣学习等方法。同时分析了PPO的超参数依赖性。

Result: 研究发现DDQN和PPO在DAC中存在可扩展性下降和学习不稳定性两大挑战。自适应奖励偏移机制有效解决了探索不足问题，无需实例特定的超参数调优。无折扣学习解决了DDQN的规划视野覆盖问题，而PPO存在基本方差问题。配备自适应奖励偏移的DDQN实现了与理论推导策略相当的性能，样本效率大幅提升，比先前DAC方法高出几个数量级。

Conclusion: 深度强化学习在动态算法配置中面临可扩展性和稳定性挑战，但通过针对性的解决方案（如自适应奖励偏移和无折扣学习）可以有效克服。DDQN配合自适应奖励偏移策略在DAC中表现出色，为实际应用提供了高效可靠的解决方案。

Abstract: Dynamic Algorithm Configuration (DAC) studies the efficient identification of control policies for parameterized optimization algorithms. Numerous studies have leveraged the robustness of decision-making in Reinforcement Learning (RL) to address the optimization challenges in algorithm configuration. However, applying RL to DAC is challenging and often requires extensive domain expertise. We conduct a comprehensive study of deep-RL algorithms in DAC through a systematic analysis of controlling the population size parameter of the (1+($λ$,$λ$))-GA on OneMax instances. Our investigation of DDQN and PPO reveals two fundamental challenges that limit their effectiveness in DAC: scalability degradation and learning instability. We trace these issues to two primary causes: under-exploration and planning horizon coverage, each of which can be effectively addressed through targeted solutions. To address under-exploration, we introduce an adaptive reward shifting mechanism that leverages reward distribution statistics to enhance DDQN agent exploration, eliminating the need for instance-specific hyperparameter tuning and ensuring consistent effectiveness across different problem scales. In dealing with the planning horizon coverage problem, we demonstrate that undiscounted learning effectively resolves it in DDQN, while PPO faces fundamental variance issues that necessitate alternative algorithmic designs. We further analyze the hyperparameter dependencies of PPO, showing that while hyperparameter optimization enhances learning stability, it consistently falls short in identifying effective policies across various configurations. Finally, we demonstrate that DDQN equipped with our adaptive reward shifting strategy achieves performance comparable to theoretically derived policies with vastly improved sample efficiency, outperforming prior DAC approaches by several orders of magnitude.

</details>


### [105] [Log Probability Tracking of LLM APIs](https://arxiv.org/abs/2512.03816)
*Timothée Chauvin,Erwan Le Merrer,François Taïani,Gilles Tredan*

Main category: cs.LG

TL;DR: 提出了一种基于logprobs的廉价LLM API监控方法，仅需单个token输出就能检测微小模型变化，比现有方法敏感1000倍


<details>
  <summary>Details</summary>
Motivation: LLM API用户期望模型保持一致性，这对下游应用可靠性和研究可复现性至关重要。现有审计方法成本过高，无法定期监控广泛的LLM API，导致模型更新在实践中基本未被监控

Method: 利用LLM log probabilities（logprobs）作为基础，采用基于每个token logprob平均值的简单统计测试，仅需请求单个token输出。引入TinyChange基准来衡量审计方法对小型、现实模型变化的敏感性

Result: 该方法能够检测小至一次微调步骤的模型变化，比现有方法更敏感，同时成本降低1000倍

Conclusion: logprobs虽然通常是非确定性的，但仍可作为成本效益高的LLM API持续监控基础，为模型更新提供实用的监控解决方案

Abstract: When using an LLM through an API provider, users expect the served model to remain consistent over time, a property crucial for the reliability of downstream applications and the reproducibility of research. Existing audit methods are too costly to apply at regular time intervals to the wide range of available LLM APIs. This means that model updates are left largely unmonitored in practice. In this work, we show that while LLM log probabilities (logprobs) are usually non-deterministic, they can still be used as the basis for cost-effective continuous monitoring of LLM APIs. We apply a simple statistical test based on the average value of each token logprob, requesting only a single token of output. This is enough to detect changes as small as one step of fine-tuning, making this approach more sensitive than existing methods while being 1,000x cheaper. We introduce the TinyChange benchmark as a way to measure the sensitivity of audit methods in the context of small, realistic model changes.

</details>


### [106] [Probabilistic Foundations of Fuzzy Simplicial Sets for Nonlinear Dimensionality Reduction](https://arxiv.org/abs/2512.03899)
*Janis Keck,Lukas Silvester Barth,Fatemeh,Fahimi,Parvaneh Joharinad,Jürgen Jost*

Main category: cs.LG

TL;DR: 该论文提出了一个概率框架，将模糊单纯集解释为单纯集上概率测度的边际分布，为UMAP等降维方法提供了统一的理论基础。


<details>
  <summary>Details</summary>
Motivation: 模糊单纯集在降维和流形学习中很重要（如UMAP），但其代数拓扑定义缺乏清晰的概率解释，与这些领域常用的理论框架脱节。

Method: 引入一个框架，将模糊单纯集解释为单纯集上概率测度的边际分布，将UMAP的模糊权重解释为在随机尺度上采样Vietoris-Rips滤过的生成模型。

Result: 该框架连接了模糊单纯集与面偏序集上的概率模型，澄清了Kullback-Leibler散度与模糊交叉熵的关系，并通过底层单纯集的布尔运算恢复了标准t-范数和t-余范数。

Conclusion: 概率视角为模糊单纯集提供了统一的理论基础，阐明了UMAP在该框架中的作用，并能够系统推导新的降维方法（如使用Čech滤过和三元组采样的UMAP推广）。

Abstract: Fuzzy simplicial sets have become an object of interest in dimensionality reduction and manifold learning, most prominently through their role in UMAP. However, their definition through tools from algebraic topology without a clear probabilistic interpretation detaches them from commonly used theoretical frameworks in those areas. In this work we introduce a framework that explains fuzzy simplicial sets as marginals of probability measures on simplicial sets. In particular, this perspective shows that the fuzzy weights of UMAP arise from a generative model that samples Vietoris-Rips filtrations at random scales, yielding cumulative distribution functions of pairwise distances. More generally, the framework connects fuzzy simplicial sets to probabilistic models on the face poset, clarifies the relation between Kullback-Leibler divergence and fuzzy cross-entropy in this setting, and recovers standard t-norms and t-conorms via Boolean operations on the underlying simplicial sets. We then show how new embedding methods may be derived from this framework and illustrate this on an example where we generalize UMAP using Čech filtrations with triplet sampling. In summary, this probabilistic viewpoint provides a unified probabilistic theoretical foundation for fuzzy simplicial sets, clarifies the role of UMAP within this framework, and enables the systematic derivation of new dimensionality reduction methods.

</details>


### [107] [Density-Informed VAE (DiVAE): Reliable Log-Prior Probability via Density Alignment Regularization](https://arxiv.org/abs/2512.03928)
*Michele Alessi,Alessio Ansuini,Alex Rodriguez*

Main category: cs.LG

TL;DR: DiVAE是一种轻量级VAE正则化器，通过将VAE对数先验概率与数据估计的对数密度对齐来改进标准VAE，提升分布对齐、先验覆盖和OOD不确定性校准。


<details>
  <summary>Details</summary>
Motivation: 标准VAE将潜在变量匹配到简单先验分布，忽略了数据空间中的密度结构。这导致编码器无法根据数据空间密度合理分配后验质量，且先验分布可能无法覆盖高密度区域。

Method: DiVAE在ELBO中添加一个稳健的、精度加权的惩罚项，将VAE对数先验概率与从数据估计的对数密度对齐。该方法鼓励编码器根据数据空间密度按比例分配后验质量，并在先验可学习时推动先验向高密度区域移动。

Result: 在合成数据集上：1) 改进了潜在对数密度与其实对应物的分布对齐；2) 提高了先验覆盖；3) 获得了更好的OOD不确定性校准。在MNIST上：改进了先验与外部密度估计的对齐，提供更好的可解释性，并提高了可学习先验的OOD检测性能。

Conclusion: DiVAE是一种计算开销可忽略的轻量级正则化方法，通过将VAE先验与数据空间密度对齐，显著改善了分布对齐、先验覆盖和OOD不确定性校准，为VAE提供了更好的可解释性和OOD检测能力。

Abstract: We introduce Density-Informed VAE (DiVAE), a lightweight, data-driven regularizer that aligns the VAE log-prior probability $\log p_Z(z)$ with a log-density estimated from data. Standard VAEs match latents to a simple prior, overlooking density structure in the data-space. DiVAE encourages the encoder to allocate posterior mass in proportion to data-space density and, when the prior is learnable, nudges the prior toward high-density regions. This is realized by adding a robust, precision-weighted penalty to the ELBO, incurring negligible computational overhead. On synthetic datasets, DiVAE (i) improves distributional alignment of latent log-densities to its ground truth counterpart, (ii) improves prior coverage, and (iii) yields better OOD uncertainty calibration. On MNIST, DiVAE improves alignment of the prior with external estimates of the density, providing better interpretability, and improves OOD detection for learnable priors.

</details>


### [108] [Technical Report on Text Dataset Distillation](https://arxiv.org/abs/2512.03967)
*Keith Ando Ogawa,Bruno Lopes Yamamoto,Lucas Lauton de Alcantara,Victor Zacarias,Edson Bollis,Lucas Pellicer,Rosimeire Pereira Costa,Anna Helena Reali Costa,Artur Jordao*

Main category: cs.LG

TL;DR: 文本数据集蒸馏技术综述：从视觉领域迁移到独立发展，涵盖Transformer应用、离散文本生成和大规模模型蒸馏，但仍面临标准化、离散性处理、复杂任务和实际应用等挑战。


<details>
  <summary>Details</summary>
Motivation: 文本数据集蒸馏领域相对于视觉领域发展较慢，虽然从视觉方法中借鉴了思路，但文本模态的特殊性（如离散性）带来了独特挑战，需要专门的综述来梳理该领域的发展历程、关键贡献和未来方向。

Method: 本文采用综述研究方法，系统回顾文本数据集蒸馏的历史和最新进展，分析不同的蒸馏策略（包括基于Transformer的方法、离散文本生成技术、大规模解码器模型蒸馏等），并对比各种方法的优缺点。

Result: 文本数据集蒸馏经历了从视觉方法迁移到独立发展的过程，主要里程碑包括：Transformer模型的应用、离散合成文本的生成、以及参数超过10亿的解码器模型的扩展。然而，该领域仍处于成熟阶段，面临标准化不足、离散性处理困难、复杂任务适应性有限等挑战。

Conclusion: 文本数据集蒸馏是一个有前景但仍在发展的研究领域，需要在基准测试标准化、离散文本处理技术、复杂任务适应性和实际应用示范等方面进一步改进，以实现更广泛的应用价值。

Abstract: In the vision domain, dataset distillation arises as a technique to condense a large dataset into a smaller synthetic one that exhibits a similar result in the training process. While image data presents an extensive literature of distillation methods, text dataset distillation has fewer works in comparison. Text dataset distillation initially grew as an adaptation of efforts from the vision universe, as the particularities of the modality became clear obstacles, it rose into a separate branch of research. Several milestones mark the development of this area, such as the introduction of methods that use transformer models, the generation of discrete synthetic text, and the scaling to decoder-only models with over 1B parameters. Despite major advances in modern approaches, the field remains in a maturing phase, with room for improvement on benchmarking standardization, approaches to overcome the discrete nature of text, handling complex tasks, and providing explicit examples of real-world applications. In this report, we review past and recent advances in dataset distillation for text, highlighting different distillation strategies, key contributions, and general challenges.

</details>


### [109] [Training-Free Policy Violation Detection via Activation-Space Whitening in LLMs](https://arxiv.org/abs/2512.03994)
*Oren Rachmil,Roy Betser,Itay Gershon,Omer Hofman,Nitay Yakoby,Yuval Meron,Idan Yankelev,Asaf Shabtai,Yuval Elovici,Roman Vainshtein*

Main category: cs.LG

TL;DR: 提出一种无需训练的高效方法，将策略违规检测视为分布外检测问题，通过白化技术处理隐藏激活，使用欧几里得范数作为合规分数，在策略基准上达到SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在敏感领域（法律、金融、医疗）的部署，组织需要可靠的机制来检测内部策略违规，现有方法（如护栏、LLM-as-a-judge、微调）存在延迟高、可解释性差、缺乏鲁棒性等问题。

Method: 将策略违规检测视为分布外检测问题，受白化技术启发，对模型隐藏激活进行线性变换以去相关并标准化为零均值和单位方差，在变换空间中使用欧几里得范数作为合规分数进行检测。

Result: 在具有挑战性的策略基准测试中，该方法取得了最先进的结果，超越了现有的护栏和微调推理模型，且仅需策略文本和少量示例样本，轻量易部署。

Conclusion: 为组织提供了一个实用且统计基础的政策感知监督框架，推进了可部署AI治理的广泛目标，代码已开源。

Abstract: Aligning proprietary large language models (LLMs) with internal organizational policies has become an urgent priority as organizations increasingly deploy LLMs in sensitive domains such as legal support, finance, and medical services. Beyond generic safety filters, enterprises require reliable mechanisms to detect policy violations within their regulatory and operational frameworks, where breaches can trigger legal and reputational risks. Existing content moderation frameworks, such as guardrails, remain largely confined to the safety domain and lack the robustness to capture nuanced organizational policies. LLM-as-a-judge and fine-tuning approaches, though flexible, introduce significant latency and lack interpretability. To address these limitations, we propose a training-free and efficient method that treats policy violation detection as an out-of-distribution (OOD) detection problem. Inspired by whitening techniques, we apply a linear transformation to decorrelate the model's hidden activations and standardize them to zero mean and unit variance, yielding near-identity covariance matrix. In this transformed space, we use the Euclidean norm as a compliance score to detect policy violations. The method requires only the policy text and a small number of illustrative samples, which makes it light-weight and easily deployable. On a challenging policy benchmark, our approach achieves state-of-the-art results, surpassing both existing guardrails and fine-tuned reasoning models. This work provides organizations with a practical and statistically grounded framework for policy-aware oversight of LLMs, advancing the broader goal of deployable AI governance. Code is available at: https://tinyurl.com/policy-violation-detection

</details>


### [110] [Physics-Embedded Gaussian Process for Traffic State Estimation](https://arxiv.org/abs/2512.04004)
*Yanlin Chen,Kehua Chen,Yinhai Wang*

Main category: cs.LG

TL;DR: 提出了一种新的物理嵌入高斯过程（PEGP）框架，将经典交通流模型知识融入数据驱动方法中，以解决低渗透率下交通状态估计的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统纯数据驱动方法在观测数据稀疏时缺乏物理解释和泛化能力，而物理模型难以整合不确定性和捕捉真实交通复杂性。现有结合方法依赖惩罚调优且缺乏原则性不确定性校准，对模型误设敏感。

Method: 设计了两种基于经典交通流模型的多输出核函数，通过线性化微分算子的显式应用构建物理嵌入高斯过程（PEGP），将领域知识与数据驱动方法结合。

Result: 在HighD和NGSIM数据集上的实验显示，相比非物理基线方法有持续改进。PEGP-ARZ在稀疏观测下更可靠，PEGP-LWR在密集观测下误差更低。残差分析表明PEGP-ARZ更符合物理且不确定性可解释。

Conclusion: PEGP框架成功结合了物理先验和不确定性量化，为交通状态估计提供了可靠支持，特别是在低渗透率和稀疏观测场景下。

Abstract: Traffic state estimation (TSE) becomes challenging when probe-vehicle penetration is low and observations are spatially sparse. Pure data-driven methods lack physical explanations and have poor generalization when observed data is sparse. In contrast, physical models have difficulty integrating uncertainties and capturing the real complexity of traffic. To bridge this gap, recent studies have explored combining them by embedding physical structure into Gaussian process. These approaches typically introduce the governing equations as soft constraints through pseudo-observations, enabling the integration of model structure within a variational framework. However, these methods rely heavily on penalty tuning and lack principled uncertainty calibration, which makes them sensitive to model mis-specification. In this work, we address these limitations by presenting a novel Physics-Embedded Gaussian Process (PEGP), designed to integrate domain knowledge with data-driven methods in traffic state estimation. Specifically, we design two multi-output kernels informed by classic traffic flow models, constructed via the explicit application of the linearized differential operator. Experiments on HighD, NGSIM show consistent improvements over non-physics baselines. PEGP-ARZ proves more reliable under sparse observation, while PEGP-LWR achieves lower errors with denser observation. Ablation study further reveals that PEGP-ARZ residuals align closely with physics and yield calibrated, interpretable uncertainty, whereas PEGP-LWR residuals are more orthogonal and produce nearly constant variance fields. This PEGP framework combines physical priors, uncertainty quantification, which can provide reliable support for TSE.

</details>


### [111] [Efficient Public Verification of Private ML via Regularization](https://arxiv.org/abs/2512.04008)
*Zoë Ruha Bell,Anvith Thudi,Olive Franzese-McLaughlin,Nicolas Papernot,Shafi Goldwasser*

Main category: cs.LG

TL;DR: 提出首个差分隐私随机凸优化算法，其隐私验证成本远低于训练成本，同时保持接近最优的隐私-效用权衡


<details>
  <summary>Details</summary>
Motivation: 当前差分隐私算法的验证成本与训练成本相当，数据提供者和公众缺乏高效验证DP保证的方法，需要降低验证成本

Method: 通过私有化最小化一系列正则化目标，仅使用标准DP组合边界，实现接近最优的隐私-效用权衡

Result: 获得了接近最优的隐私-效用权衡，且验证成本远低于训练成本，显著减少大规模数据集上的验证开销

Conclusion: 设计了首个DP-SCO算法，其DP验证成本比训练成本更低，为差分隐私算法的实际部署提供了更可行的验证方案

Abstract: Training with differential privacy (DP) provides a guarantee to members in a dataset that they cannot be identified by users of the released model. However, those data providers, and, in general, the public, lack methods to efficiently verify that models trained on their data satisfy DP guarantees. The amount of compute needed to verify DP guarantees for current algorithms scales with the amount of compute required to train the model. In this paper we design the first DP algorithm with near optimal privacy-utility trade-offs but whose DP guarantees can be verified cheaper than training. We focus on DP stochastic convex optimization (DP-SCO), where optimal privacy-utility trade-offs are known. Here we show we can obtain tight privacy-utility trade-offs by privately minimizing a series of regularized objectives and only using the standard DP composition bound. Crucially, this method can be verified with much less compute than training. This leads to the first known DP-SCO algorithm with near optimal privacy-utility whose DP verification scales better than training cost, significantly reducing verification costs on large datasets.

</details>


### [112] [Domain Feature Collapse: Implications for Out-of-Distribution Detection and Solutions](https://arxiv.org/abs/2512.04034)
*Hong Yang,Devroop Kar,Qi Yu,Alex Ororbia,Travis Desell*

Main category: cs.LG

TL;DR: 该论文从信息论角度解释了为什么在单域数据集上训练的模型在OOD检测中会出现灾难性失败，提出了"域特征坍缩"理论，并通过域过滤方法验证了理论。


<details>
  <summary>Details</summary>
Motivation: 解释为什么在单域数据集上训练的最先进OOD检测方法会出现灾难性失败现象，从理论上理解这一令人困惑的经验观察。

Method: 使用信息论框架分析，证明监督学习在单域数据上必然导致域特征坍缩（I(x_d; z) = 0）。通过Fano不等式量化实际场景中的部分坍缩。引入Domain Bench基准，使用域过滤方法（利用预训练表示）验证理论。

Result: 理论分析表明单域训练会导致模型完全丢弃域特定信息。实验验证显示，通过域过滤保持I(x_d; z) > 0可以解决OOD检测失败问题（如MNIST上仅53% FPR@95）。域过滤的有效性为信息论框架提供了强有力的经验证据。

Conclusion: 该研究解释了OOD检测中的灾难性失败现象，揭示了监督学习在狭窄领域的根本局限性，对迁移学习以及何时微调与冻结预训练模型具有更广泛的意义。

Abstract: Why do state-of-the-art OOD detection methods exhibit catastrophic failure when models are trained on single-domain datasets? We provide the first theoretical explanation for this phenomenon through the lens of information theory. We prove that supervised learning on single-domain data inevitably produces domain feature collapse -- representations where I(x_d; z) = 0, meaning domain-specific information is completely discarded. This is a fundamental consequence of information bottleneck optimization: models trained on single domains (e.g., medical images) learn to rely solely on class-specific features while discarding domain features, leading to catastrophic failure when detecting out-of-domain samples (e.g., achieving only 53% FPR@95 on MNIST). We extend our analysis using Fano's inequality to quantify partial collapse in practical scenarios. To validate our theory, we introduce Domain Bench, a benchmark of single-domain datasets, and demonstrate that preserving I(x_d; z) > 0 through domain filtering (using pretrained representations) resolves the failure mode. While domain filtering itself is conceptually straightforward, its effectiveness provides strong empirical evidence for our information-theoretic framework. Our work explains a puzzling empirical phenomenon, reveals fundamental limitations of supervised learning in narrow domains, and has broader implications for transfer learning and when to fine-tune versus freeze pretrained models.

</details>


### [113] [Convergence for Discrete Parameter Updates](https://arxiv.org/abs/2512.04051)
*Paul Wilson,Fabio Zanasi,George Constantinides*

Main category: cs.LG

TL;DR: 提出一种离散更新规则的量化训练方法，避免传统量化训练中连续更新的离散化过程，为具有固有离散结构的模型开辟高效训练新途径。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型需要巨大的计算资源，促使低精度训练研究。传统量化训练通过低比特整数表示训练组件，但通常依赖于离散化实值更新，存在局限性。

Method: 引入离散更新规则方法，设计上避免连续更新的量化。建立此类离散方案的收敛保证，并提出多项分布更新规则作为具体实例。

Result: 建立了离散更新方案的一般收敛性保证，并通过实证评估支持多项分布更新规则的有效性。

Conclusion: 这种离散更新视角为高效训练开辟了新途径，特别适用于具有固有离散结构的模型，避免了传统量化训练中连续更新离散化的局限性。

Abstract: Modern deep learning models require immense computational resources, motivating research into low-precision training. Quantised training addresses this by representing training components in low-bit integers, but typically relies on discretising real-valued updates. We introduce an alternative approach where the update rule itself is discrete, avoiding the quantisation of continuous updates by design. We establish convergence guarantees for a general class of such discrete schemes, and present a multinomial update rule as a concrete example, supported by empirical evaluation. This perspective opens new avenues for efficient training, particularly for models with inherently discrete structure.

</details>


### [114] [Eval Factsheets: A Structured Framework for Documenting AI Evaluations](https://arxiv.org/abs/2512.04062)
*Florian Bordes,Candace Ross,Justine T Kao,Evangelia Spiliopoulou,Adina Williams*

Main category: cs.LG

TL;DR: Eval Factsheets：为AI系统评估方法设计的结构化文档框架，通过五个维度的分类和问卷式方法系统记录评估过程，提高透明度和可复现性。


<details>
  <summary>Details</summary>
Motivation: 当前AI领域基准测试激增，但评估方法缺乏系统化的文档标准，导致可复现性、透明度和决策制定方面存在挑战。与数据集和模型已有Datasheets、Model Cards等结构化文档框架不同，评估方法缺乏类似的标准化文档体系。

Method: 提出Eval Factsheets框架，通过五个基本维度组织评估特征：Context（评估者及时机）、Scope（评估内容）、Structure（评估构建要素）、Method（评估方法）、Alignment（可靠性/有效性/鲁棒性）。将该分类法实现为包含强制性和推荐性文档元素的五部分问卷。

Result: 通过对多个基准测试的案例研究，证明Eval Factsheets能够有效捕捉从传统基准测试到LLM-as-judge方法等多种评估范式，同时保持一致性和可比性。

Conclusion: Eval Factsheets应被纳入现有和新发布的评估框架中，以促进更高的透明度和可复现性，为AI系统评估提供系统化的文档标准。

Abstract: The rapid proliferation of benchmarks has created significant challenges in reproducibility, transparency, and informed decision-making. However, unlike datasets and models -- which benefit from structured documentation frameworks like Datasheets and Model Cards -- evaluation methodologies lack systematic documentation standards. We introduce Eval Factsheets, a structured, descriptive framework for documenting AI system evaluations through a comprehensive taxonomy and questionnaire-based approach. Our framework organizes evaluation characteristics across five fundamental dimensions: Context (Who made the evaluation and when?), Scope (What does it evaluate?), Structure (With what the evaluation is built?), Method (How does it work?) and Alignment (In what ways is it reliable/valid/robust?). We implement this taxonomy as a practical questionnaire spanning five sections with mandatory and recommended documentation elements. Through case studies on multiple benchmarks, we demonstrate that Eval Factsheets effectively captures diverse evaluation paradigms -- from traditional benchmarks to LLM-as-judge methodologies -- while maintaining consistency and comparability. We hope Eval Factsheets are incorporated into both existing and newly released evaluation frameworks and lead to more transparency and reproducibility.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [115] [Exploring Syntropic Frameworks in AI Alignment: A Philosophical Investigation](https://arxiv.org/abs/2512.03048)
*Austin Spizzirri*

Main category: cs.AI

TL;DR: 本文主张将AI对齐重新构想为通过过程性、多智能体、发展性机制构建熵减的、理由响应的智能体，而非编码固定的人类价值内容。


<details>
  <summary>Details</summary>
Motivation: 传统基于内容的价值规范方法存在结构不稳定性问题，包括"是-应该"鸿沟、价值多元论和扩展框架问题，这构成了"规范陷阱"。需要新的理论框架来理解多智能体对齐动态。

Method: 提出"熵减"作为信息理论框架，通过状态对齐递归减少智能体间的相互不确定性；基于兼容主义的指导控制理论区分真实与模拟的道德能力；建立具身实验范式和验证机制。

Result: 提出了具体的、可证伪的预测，包括价值涌现和人工系统中的道德主体性，但实证验证仍在进行中，是更广泛研究计划的哲学组成部分。

Conclusion: AI对齐应转向过程性、发展性的多智能体架构，通过熵减机制构建理由响应的智能体，而非试图编码固定的价值内容，这为解决传统对齐方法的局限性提供了新路径。

Abstract: I argue that AI alignment should be reconceived as architecting syntropic, reasons-responsive agents through process-based, multi-agent, developmental mechanisms rather than encoding fixed human value content. The paper makes three philosophical contributions. First, I articulate the ``specification trap'' argument demonstrating why content-based value specification appears structurally unstable due to the conjunction of the is-ought gap, value pluralism, and the extended frame problem. Second, I propose syntropy -- the recursive reduction of mutual uncertainty between agents through state alignment -- as an information-theoretic framework for understanding multi-agent alignment dynamics. Third, I establish a functional distinction between genuine and simulated moral capacity grounded in compatibilist theories of guidance control, coupled with an embodied experimental paradigm and verification regime providing operational criteria independent of phenomenological claims. This paper represents the philosophical component of a broader research program whose empirical validation is being developed in a separate project currently in preparation. While the framework generates specific, falsifiable predictions about value emergence and moral agency in artificial systems, empirical validation remains pending.

</details>


### [116] [Beyond the Black Box: A Cognitive Architecture for Explainable and Aligned AI](https://arxiv.org/abs/2512.03072)
*Hu Keyi*

Main category: cs.AI

TL;DR: 本文提出了一种名为"权重计算主义"的新型认知架构，基于第一原理构建，通过逻辑原子和基本操作实现可解释的决策，为AGI发展提供了新路径。


<details>
  <summary>Details</summary>
Motivation: 当前AI范式作为"体验架构师"面临可解释性和价值对齐的根本挑战，需要一种基于第一原理的认知架构来解决这些问题。

Method: 将认知解构为不可分割的逻辑原子和两个基本操作：指向和比较。通过可解释的权重计算模型（权重=收益×概率）形式化决策，所有值都可追溯到可审计的初始权重集。采用基于图算法的计算引擎和全局工作空间工作流实现。

Result: 该架构实现了透明、类人的推理能力，并在前所未有的场景中表现出强大的学习能力，为构建可信赖和对齐的AGI奠定了实践和理论基础。

Conclusion: 权重计算主义架构为解决AGI的可解释性和价值对齐问题提供了可行路径，通过原子分解实现了根本的可解释性、对新情境的内在通用性和可追溯的价值对齐。

Abstract: Current AI paradigms, as "architects of experience," face fundamental challenges in explainability and value alignment. This paper introduces "Weight-Calculatism," a novel cognitive architecture grounded in first principles, and demonstrates its potential as a viable pathway toward Artificial General Intelligence (AGI). The architecture deconstructs cognition into indivisible Logical Atoms and two fundamental operations: Pointing and Comparison. Decision-making is formalized through an interpretable Weight-Calculation model (Weight = Benefit * Probability), where all values are traceable to an auditable set of Initial Weights. This atomic decomposition enables radical explainability, intrinsic generality for novel situations, and traceable value alignment. We detail its implementation via a graph-algorithm-based computational engine and a global workspace workflow, supported by a preliminary code implementation and scenario validation. Results indicate that the architecture achieves transparent, human-like reasoning and robust learning in unprecedented scenarios, establishing a practical and theoretical foundation for building trustworthy and aligned AGI.

</details>


### [117] [When Do Symbolic Solvers Enhance Reasoning in Large Language Models?](https://arxiv.org/abs/2512.03272)
*Zhiyuan He,Dingmin Wang*

Main category: cs.AI

TL;DR: 该论文探讨了符号求解器集成方法何时能增强传统长思维链推理，发现符号求解器仅在问题需要有限隐式推理但涉及充分搜索空间时有效，特别是在需要重复回溯的约束满足问题上表现优异。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过生成长思维链在复杂推理任务上表现良好，但这种方法可能导致大量token开销，甚至因"过度思考"产生冗长推理链而导致错误答案。符号求解器集成方法利用LLM的代码生成能力将推理任务转化为可执行代码，然后用符号求解器求解，但何时这种传统长思维链能被符号求解器增强仍是一个开放问题。

Method: 研究采用符号求解器集成方法，利用LLM的代码生成能力将推理任务翻译成可执行代码，然后使用符号求解器进行求解。通过实验比较传统长思维链方法与符号求解器集成方法在不同类型问题上的表现。

Result: 实验结果显示：1）符号求解器集成方法仅在问题需要有限隐式推理但涉及充分搜索空间时有效；2）最新LLM（如GPT-4o）在推理深度较浅的演绎问题上表现更好；3）符号求解器集成方法在需要重复回溯的约束满足问题上显著提升LLM性能；4）当提供声明式示例时，即使是CodeLlama-13B也能在困难的斑马谜题上超越GPT-4o。

Conclusion: 符号求解器集成方法在特定类型推理问题上能有效增强传统长思维链方法，特别是在需要大量搜索和回溯的约束满足问题上。该方法为优化LLM推理效率提供了有前景的方向，但适用性取决于问题的推理特性和搜索空间特征。

Abstract: Large Reasoning Models (LRMs) achieve strong performance on complex reasoning tasks by generating long Chains of Thought (CoTs). However, this paradigm might incur substantial token overhead, especially when models "overthink" by producing lengthy reasoning chains, which can even lead to incorrect answers. A promising direction is the symbolic-solver-integrated approach, which leverages the code generation capabilities of LLMs to translate reasoning tasks into executable code and then solve them with a symbolic solver. In this paper, we explore an open question of when the conventional long-CoT can be enhanced by symbolic solvers. Our experimental results show that the symbolic-solver-integrated method only helps when the problem requires limited implicit reasoning but involves an ample search space. The latest LLMs, like GPT-4o, show better performance on deductive problems with shallow reasoning depth, while the symbolic-solver-integrated method significantly improves the LLMs' performance in constraint satisfaction problems that require repeated backtracks. When a declarative exemplar is provided, even CodeLlama-13B can outperform GPT-4o in difficult Zebra puzzles.

</details>


### [118] [Prior preferences in active inference agents: soft, hard, and goal shaping](https://arxiv.org/abs/2512.03293)
*Filippo Torresan,Ryota Kanai,Manuel Baltieri*

Main category: cs.AI

TL;DR: 该研究比较了主动推理中四种不同偏好分布定义方式对智能体在网格世界导航任务中性能的影响，发现目标塑造能提升整体性能但会牺牲对环境的探索学习。


<details>
  <summary>Details</summary>
Motivation: 主动推理使用期望自由能作为规划决策目标，但文献中对偏好分布如何具体定义及其对推理学习的影响关注不足。本研究旨在探讨不同偏好分布定义方式对智能体性能的影响。

Method: 考虑了四种定义偏好分布的方式：硬目标vs软目标，以及是否包含目标塑造（中间目标）。在网格世界导航任务中比较了四种智能体的性能。

Result: 目标塑造能够实现最佳整体性能（促进利用），但会牺牲对环境转移动态的学习（阻碍探索）。

Conclusion: 偏好分布的具体定义方式对主动推理智能体的性能有显著影响，目标塑造虽然能提升利用效率，但会以牺牲探索能力为代价，需要在两者之间权衡。

Abstract: Active inference proposes expected free energy as an objective for planning and decision-making to adequately balance exploitative and explorative drives in learning agents. The exploitative drive, or what an agent wants to achieve, is formalised as the Kullback-Leibler divergence between a variational probability distribution, updated at each inference step, and a preference probability distribution that indicates what states or observations are more likely for the agent, hence determining the agent's goal in a certain environment. In the literature, the questions of how the preference distribution should be specified and of how a certain specification impacts inference and learning in an active inference agent have been given hardly any attention. In this work, we consider four possible ways of defining the preference distribution, either providing the agents with hard or soft goals and either involving or not goal shaping (i.e., intermediate goals). We compare the performances of four agents, each given one of the possible preference distributions, in a grid world navigation task. Our results show that goal shaping enables the best performance overall (i.e., it promotes exploitation) while sacrificing learning about the environment's transition dynamics (i.e., it hampers exploration).

</details>


### [119] [Evaluating Generalization Capabilities of LLM-Based Agents in Mixed-Motive Scenarios Using Concordia](https://arxiv.org/abs/2512.03318)
*Chandler Smith,Marwa Abdulhai,Manfred Diaz,Marko Tesic,Rakshit S. Trivedi,Alexander Sasha Vezhnevets,Lewis Hammond,Jesse Clifton,Minsuk Chang,Edgar A. Duéñez-Guzmán,John P. Agapiou,Jayd Matyas,Danny Karmon,Akash Kundu,Aliaksei Korshuk,Ananya Ananya,Arrasy Rahman,Avinaash Anand Kulandaivel,Bain McHale,Beining Zhang,Buyantuev Alexander,Carlos Saith Rodriguez Rojas,Caroline Wang,Chetan Talele,Chenao Liu,Chichen Lin,Diana Riazi,Di Yang Shi,Emanuel Tewolde,Elizaveta Tennant,Fangwei Zhong,Fuyang Cui,Gang Zhao,Gema Parreño Piqueras,Hyeonggeun Yun,Ilya Makarov,Jiaxun Cui,Jebish Purbey,Jim Dilkes,Jord Nguyen,Lingyun Xiao,Luis Felipe Giraldo,Manuela Chacon-Chamorro,Manuel Sebastian Rios Beltran,Marta Emili García Segura,Mengmeng Wang,Mogtaba Alim,Nicanor Quijano,Nico Schiavone,Olivia Macmillan-Scott,Oswaldo Peña,Peter Stone,Ram Mohan Rao Kadiyala,Rolando Fernandez,Ruben Manrique,Sunjia Lu,Sheila A. McIlraith,Shamika Dhuri,Shuqing Shi,Siddhant Gupta,Sneheel Sarangi,Sriram Ganapathi Subramanian,Taehun Cha,Toryn Q. Klassen,Wenming Tu,Weijian Fan,Wu Ruiyang,Xue Feng,Yali Du,Yang Liu,Yiding Wang,Yipeng Kang,Yoonchang Sung,Yuxuan Chen,Zhaowei Zhang,Zhihan Wang,Zhiqiang Wu,Ziang Chen,Zilong Zheng,Zixia Jia,Ziyan Wang,Dylan Hadfield-Menell,Natasha Jaques,Tim Baarslag,Jose Hernandez-Orallo,Joel Z. Leibo*

Main category: cs.AI

TL;DR: 论文提出了一种评估LLM智能体在零样本、混合动机环境中合作能力的方法，使用Concordia多智能体模拟环境，揭示了当前智能体在合作泛化能力上的显著差距。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在社会互动方面展现出强大能力，并被部署到与人类和人工智能体交互的场景中。然而，现有评估方法无法衡量这些能力在新颖社交情境中的泛化表现，需要一种方法来评估智能体在零样本、混合动机环境中的合作能力。

Method: 引入基于Concordia自然语言多智能体模拟环境的评估方法，通过测试智能体在不同合作伙伴和情境中识别和利用互利机会的能力，来衡量一般合作智能。该方法在NeurIPS 2024 Concordia竞赛中进行了实证研究。

Result: 研究发现当前智能体能力与稳健合作所需的泛化能力之间存在显著差距，特别是在需要说服和规范执行的场景中。智能体在从谈判到集体行动问题等多样化情境中实现互利的能力有限。

Conclusion: LLM智能体在合作能力方面存在明显的泛化不足问题，特别是在复杂社交情境中。这为未来研究指出了改进方向，需要开发更具鲁棒性的合作智能体。

Abstract: Large Language Model (LLM) agents have demonstrated impressive capabilities for social interaction and are increasingly being deployed in situations where they might engage with both human and artificial agents. These interactions represent a critical frontier for LLM-based agents, yet existing evaluation methods fail to measure how well these capabilities generalize to novel social situations. In this paper, we introduce a method for evaluating the ability of LLM-based agents to cooperate in zero-shot, mixed-motive environments using Concordia, a natural language multi-agent simulation environment. Our method measures general cooperative intelligence by testing an agent's ability to identify and exploit opportunities for mutual gain across diverse partners and contexts. We present empirical results from the NeurIPS 2024 Concordia Contest, where agents were evaluated on their ability to achieve mutual gains across a suite of diverse scenarios ranging from negotiation to collective action problems. Our findings reveal significant gaps between current agent capabilities and the robust generalization required for reliable cooperation, particularly in scenarios demanding persuasion and norm enforcement.

</details>


### [120] [Multi-Agent Reinforcement Learning with Communication-Constrained Priors](https://arxiv.org/abs/2512.03528)
*Guang Yang,Tianpei Yang,Jingwen Qiao,Yanqing Wu,Jing Huo,Xingguo Chen,Yang Gao*

Main category: cs.AI

TL;DR: 提出了一种通信受限的多智能体强化学习框架，通过区分有损和无损消息，使用双重互信息估计器量化通信影响，并在多个基准测试中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中普遍存在有损通信问题，现有多智能体强化学习通信方法由于可扩展性和鲁棒性有限，难以应用于复杂动态的真实环境。

Method: 提出广义通信约束模型统一描述不同场景的通信条件；将其作为学习先验区分有损和无损消息；使用双重互信息估计器解耦有损和无损消息对分布式决策的影响；引入通信约束的多智能体强化学习框架，将通信消息影响量化为全局奖励。

Result: 在多个通信受限的基准测试中验证了方法的有效性。

Conclusion: 提出的通信约束多智能体强化学习框架能够有效处理现实世界中的有损通信问题，提高系统在复杂动态环境中的适应性和鲁棒性。

Abstract: Communication is one of the effective means to improve the learning of cooperative policy in multi-agent systems. However, in most real-world scenarios, lossy communication is a prevalent issue. Existing multi-agent reinforcement learning with communication, due to their limited scalability and robustness, struggles to apply to complex and dynamic real-world environments. To address these challenges, we propose a generalized communication-constrained model to uniformly characterize communication conditions across different scenarios. Based on this, we utilize it as a learning prior to distinguish between lossy and lossless messages for specific scenarios. Additionally, we decouple the impact of lossy and lossless messages on distributed decision-making, drawing on a dual mutual information estimatior, and introduce a communication-constrained multi-agent reinforcement learning framework, quantifying the impact of communication messages into the global reward. Finally, we validate the effectiveness of our approach across several communication-constrained benchmarks.

</details>


### [121] [EnCompass: Enhancing Agent Programming with Search Over Program Execution Paths](https://arxiv.org/abs/2512.03571)
*Zhening Li,Armando Solar-Lezama,Yisong Yue,Stephan Zheng*

Main category: cs.AI

TL;DR: 本文提出了一种名为"概率天使非确定性"（PAN）的新型智能体编程方法，通过分离核心工作流逻辑和推理时策略，使开发者能够独立设计和实验不同的推理策略。


<details>
  <summary>Details</summary>
Motivation: 当前智能体编程方法通常将核心工作流逻辑和推理时策略（如树搜索）紧密耦合，这种耦合限制了开发者在不同推理策略间的灵活切换和实验。

Method: 提出PAN编程模型，使用Python装饰器将智能体工作流程序编译为搜索空间，实现工作流逻辑与推理策略的解耦。在EnCompass框架中实现该模型。

Result: 通过三个案例研究表明，该框架使开发者能够快速提高智能体可靠性，轻松在不同推理策略间切换，且只需少量额外编码。

Conclusion: PAN编程模型和EnCompass框架为LLM-based智能体开发提供了更灵活、可维护的编程范式，显著降低了实验不同推理策略的成本。

Abstract: We introduce a new approach to agent programming, the development of LLM-based agents. Current approaches to agent programming often entangle two aspects of agent design: the core workflow logic and the inference-time strategy (e.g., tree search). We introduce "probabilistic angelic nondeterminism" ("PAN"), a programming model that disentangles these two concerns, allowing the programmer to describe the agent workflow and independently experiment with different inference-time strategies by simply changing a few inputs. We provide an implementation of PAN in Python as the EnCompass framework, which uses a Python decorator to compile agent workflow programs into a search space. We present three case studies that demonstrate how the framework lets the programmer quickly improve the reliability of an agent and easily switch between different inference-time strategies, all with little additional coding.

</details>


### [122] [DeepRule: An Integrated Framework for Automated Business Rule Generation via Deep Predictive Modeling and Hybrid Search Optimization](https://arxiv.org/abs/2512.03607)
*Yusen Wu,Xiaotie Deng*

Main category: cs.AI

TL;DR: DeepRule是一个用于零售品类和定价优化的自动化业务规则生成框架，通过三级架构解决理论模型与现实经济复杂性之间的不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 现有理论模型与现实经济复杂性存在系统性不匹配，具体表现为三个关键差距：1) 非结构化文本数据模态不匹配阻碍准确客户画像；2) 动态特征纠缠挑战非线性价格弹性和时变属性建模；3) 多层次业务约束导致操作不可行性。

Method: 采用三级架构：1) 混合知识融合引擎使用大语言模型深度语义解析非结构化文本，将分销协议和销售评估转化为结构化特征；2) 博弈论约束优化机制通过双边效用函数动态协调供应链利益；3) 可解释决策蒸馏接口利用LLM引导的符号回归优化定价策略和可审计业务规则。

Result: 在真实零售环境中验证了该框架，相比系统性B2C基线实现了更高的利润，同时确保了操作可行性。

Conclusion: 建立了一个闭环管道，统一了非结构化知识注入、多智能体优化和可解释策略合成，为真实经济智能提供了解决方案。

Abstract: This paper proposes DeepRule, an integrated framework for automated business rule generation in retail assortment and pricing optimization. Addressing the systematic misalignment between existing theoretical models and real-world economic complexities, we identify three critical gaps: (1) data modality mismatch where unstructured textual sources (e.g. negotiation records, approval documents) impede accurate customer profiling; (2) dynamic feature entanglement challenges in modeling nonlinear price elasticity and time-varying attributes; (3) operational infeasibility caused by multi-tier business constraints.
  Our framework introduces a tri-level architecture for above challenges. We design a hybrid knowledge fusion engine employing large language models (LLMs) for deep semantic parsing of unstructured text, transforming distributor agreements and sales assessments into structured features while integrating managerial expertise. Then a game-theoretic constrained optimization mechanism is employed to dynamically reconcile supply chain interests through bilateral utility functions, encoding manufacturer-distributor profit redistribution as endogenous objectives under hierarchical constraints. Finally an interpretable decision distillation interface leveraging LLM-guided symbolic regression to find and optimize pricing strategies and auditable business rules embeds economic priors (e.g. non-negative elasticity) as hard constraints during mathematical expression search. We validate the framework in real retail environments achieving higher profits versus systematic B2C baselines while ensuring operational feasibility. This establishes a close-loop pipeline unifying unstructured knowledge injection, multi-agent optimization, and interpretable strategy synthesis for real economic intelligence.

</details>


### [123] [RoCo: Role-Based LLMs Collaboration for Automatic Heuristic Design](https://arxiv.org/abs/2512.03762)
*Jiawei Xu,Fengfeng Wei,Weineng Chen*

Main category: cs.AI

TL;DR: RoCo是一个基于多智能体角色协作的系统，通过四个专门化的LLM智能体（探索者、利用者、批评者、整合者）协同工作，自动设计高质量的启发式算法来解决组合优化问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的自动启发式设计研究通常只考虑单一角色，限制了启发式算法的多样性和质量。为了增强自动启发式设计的多样性和质量，需要一种多角色协作的方法。

Method: 提出RoCo多智能体角色协作系统，包含四个专门化的LLM智能体：探索者（创造性、多样性驱动）、利用者（保守性、效率导向）、批评者（评估效果并提供反馈）、整合者（平衡创新与利用）。这些智能体通过结构化的多轮过程进行交互，包括反馈、精炼和精英变异。

Result: 在五种不同的组合优化问题上，无论是白盒还是黑盒设置下，RoCo都表现出优越性能，生成的启发式算法优于现有方法（包括ReEvo和HSEvo）。

Conclusion: 基于角色的协作范式为稳健且高性能的自动启发式设计建立了新标准，证明了多角色协作在增强启发式算法多样性和质量方面的有效性。

Abstract: Automatic Heuristic Design (AHD) has gained traction as a promising solution for solving combinatorial optimization problems (COPs). Large Language Models (LLMs) have emerged and become a promising approach to achieving AHD, but current LLM-based AHD research often only considers a single role. This paper proposes RoCo, a novel Multi-Agent Role-Based System, to enhance the diversity and quality of AHD through multi-role collaboration. RoCo coordinates four specialized LLM-guided agents-explorer, exploiter, critic, and integrator-to collaboratively generate high-quality heuristics. The explorer promotes long-term potential through creative, diversity-driven thinking, while the exploiter focuses on short-term improvements via conservative, efficiency-oriented refinements. The critic evaluates the effectiveness of each evolution step and provides targeted feedback and reflection. The integrator synthesizes proposals from the explorer and exploiter, balancing innovation and exploitation to drive overall progress. These agents interact in a structured multi-round process involving feedback, refinement, and elite mutations guided by both short-term and accumulated long-term reflections. We evaluate RoCo on five different COPs under both white-box and black-box settings. Experimental results demonstrate that RoCo achieves superior performance, consistently generating competitive heuristics that outperform existing methods including ReEvo and HSEvo, both in white-box and black-box scenarios. This role-based collaborative paradigm establishes a new standard for robust and high-performing AHD.

</details>


### [124] [Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning](https://arxiv.org/abs/2512.03783)
*Dongchao Yang,Songxiang Liu,Disong Wang,Yuanyuan Wang,Guanglu Wan,Helen Meng*

Main category: cs.AI

TL;DR: Omni-AutoThink是一个自适应推理框架，通过动态调整模型推理深度来提升多模态模型的推理能力，包含自适应监督微调和自适应强化学习两个阶段。


<details>
  <summary>Details</summary>
Motivation: 现有Omni模型在多模态感知和生成方面取得进展，但在推理行为上存在僵化问题：要么对简单问题过度思考，要么在需要推理时无法有效推理。需要解决这一局限性。

Method: 提出两阶段框架：1) 自适应监督微调阶段，使用大规模推理增强数据赋予模型基础推理能力；2) 自适应强化学习阶段，基于任务复杂度和奖励反馈优化推理行为。同时构建了涵盖文本、文本-音频、文本-视觉、文本-音频-视觉模态的综合自适应推理基准。

Result: 实验结果表明，提出的框架相比先前基线显著提升了自适应推理性能。所有基准数据和代码将公开。

Conclusion: Omni-AutoThink通过自适应调整推理深度，有效解决了多模态模型推理行为僵化的问题，提升了模型在不同难度任务上的推理能力。

Abstract: Recent advances in Omni models have enabled unified multimodal perception and generation. However, most existing systems still exhibit rigid reasoning behaviors, either overthinking simple problems or failing to reason when necessary. To address this limitation, we propose Omni-AutoThink, a novel adaptive reasoning framework that dynamically adjusts the model's reasoning depth according to task difficulty. Our framework comprises two stages: (1) an Adaptive Supervised Fine-Tuning (Adaptive SFT) stage, which endows the Omni model with fundamental reasoning capability using large-scale reasoning-augmented data, and (2) an Adaptive Reinforcement Learning (Adaptive GRPO) stage, which optimizes reasoning behaviors based on task complexity and reward feedback. We further construct a comprehensive adaptive reasoning benchmark that spans text-only, text-audio, text-visual, and text-audio-visual modalities, providing both training and evaluation splits for multimodal reasoning assessment. Experimental results demonstrate that our proposed framework significantly improves adaptive reasoning performance compared to previous baselines. All benchmark data and code will be publicly released.

</details>


### [125] [Benchmark for Planning and Control with Large Language Model Agents: Blocksworld with Model Context Protocol](https://arxiv.org/abs/2512.03955)
*Niklas Jobs,Luis Miguel Vieira da Silva,Jayanth Somashekaraiah,Maximilian Weigand,David Kube,Felix Gehlhoff*

Main category: cs.AI

TL;DR: 该研究提出了一个基于Blocksworld问题的可执行仿真基准测试环境，用于系统评估LLM智能体在工业自动化中的自适应规划与执行能力。


<details>
  <summary>Details</summary>
Motivation: 工业自动化需要能够适应变化任务和环境的灵活控制策略，基于大语言模型的智能体具有自适应规划和执行潜力，但缺乏标准化的基准测试进行系统比较。

Method: 引入一个包含可执行仿真环境的基准测试，基于Blocksworld问题提供五个复杂度类别；通过集成模型上下文协议（MCP）作为标准化工具接口，使不同智能体架构能够无需特定实现修改即可连接和评估。

Result: 通过单智能体实现展示了基准测试的适用性，建立了用于比较基于LLM的规划与执行方法的定量指标。

Conclusion: 该基准测试为系统评估和比较LLM智能体在工业自动化任务中的自适应规划与执行能力提供了标准化框架。

Abstract: Industrial automation increasingly requires flexible control strategies that can adapt to changing tasks and environments. Agents based on Large Language Models (LLMs) offer potential for such adaptive planning and execution but lack standardized benchmarks for systematic comparison. We introduce a benchmark with an executable simulation environment representing the Blocksworld problem providing five complexity categories. By integrating the Model Context Protocol (MCP) as a standardized tool interface, diverse agent architectures can be connected to and evaluated against the benchmark without implementation-specific modifications. A single-agent implementation demonstrates the benchmark's applicability, establishing quantitative metrics for comparison of LLM-based planning and execution approaches.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [126] [Symmetry-Protected Bipolar Skin Effect and its Topological Breakdown in Disordered Non-Hermitian Systems](https://arxiv.org/abs/2512.03283)
*Ali Tozar*

Main category: cond-mat.dis-nn

TL;DR: 该研究揭示了非厄米Rashba链中的Z_2拓扑双极趋肤效应，发现自旋向上和向下的本征态在相反边界局域化，并建立了拓扑层次性崩溃的全局相图。


<details>
  <summary>Details</summary>
Motivation: 非厄米拓扑与无序之间的相互作用是开放量子系统中的核心问题。虽然非厄米趋肤效应（NHSE）已知对弱扰动具有鲁棒性，但在强无序特别是存在自旋轨道耦合（SOC）的情况下，其命运尚未完全理解。

Method: 通过严格计算Lyapunov指数并引入双正交自旋分离指数，映射全局相图，揭示拓扑的层次性崩溃。研究Z_2趋肤效应在中等无序下的保护机制及其向平凡趋肤相的转变。

Result: 发现了Z_2拓扑双极趋肤效应，其中自旋向上和向下的本征态在相反边界局域化。该效应在中等无序下受到保护，但在最终发生Anderson局域化之前会崩溃为平凡趋肤相。建立了区别于平凡体极限和Anderson局域化相的无序鲁棒拓扑非互易性新机制。

Conclusion: 该研究建立了非厄米系统中无序鲁棒拓扑非互易性的独特机制，揭示了Z_2趋肤效应在无序作用下的层次性崩溃过程，为理解非厄米拓扑与无序的相互作用提供了新视角。

Abstract: The interplay between non-Hermitian topology and disorder remains a central puzzle in open quantum systems. While the non-Hermitian skin effect (NHSE) is known to be robust against weak perturbations, its fate under strong disorder, particularly in the presence of spin-orbit coupling (SOC), is not fully understood. Here, we uncover a Z_2 topological bipolar skin effect in a non-Hermitian Rashba chain, where spin-up and spin-down eigenstates localize at opposite boundaries. By strictly computing the Lyapunov exponents and introducing a biorthogonal spin-separation index, we map the global phase diagram and reveal a hierarchical breakdown of topology. We demonstrate that the Z_2 skin effect is protected against moderate disorder but collapses into a trivial skin phase before the ultimate onset of Anderson localization. Our results establish a distinct regime of disorder-robust topological non-reciprocity, distinguishable from both the trivial bulk limit and the Anderson localized phase.

</details>


### [127] [Stretched Exponential Scaling of Parity-Restricted Energy Gaps in a Random Transverse-Field Ising Model](https://arxiv.org/abs/2512.03526)
*G. -X. Tang,J. -Z. Zhuang,L. -M. Duan,Y. -K. Wu*

Main category: cond-mat.dis-nn

TL;DR: 一维随机横向场伊辛模型在宇称限制子空间中具有拉伸指数能隙缩放，与二维情况的多项式能隙形成对比


<details>
  <summary>Details</summary>
Motivation: 先前研究表明二维随机横向场伊辛模型在宇称限制子空间中具有多项式能隙，但不确定这一结果是否能推广到具有连续或偏置随机性的其他自旋玻璃模型

Method: 证明在一维随机横向场伊辛模型中，在交换能量服从一般独立同分布条件下，即使在宇称限制子空间中，能隙也遵循拉伸指数缩放

Result: 一维随机横向场伊辛模型在宇称限制子空间中具有拉伸指数能隙缩放，与二维情况的多项式能隙形成鲜明对比

Conclusion: 这一结果对量子退火问题具有重要意义，表明一维随机横向场伊辛模型的量子退火效率可能不如二维模型

Abstract: The success of a quantum annealing algorithm requires a polynomial scaling of the energy gap. Recently it was shown that a two-dimensional transverse-field Ising model on a square lattice with nearest-neighbor $\pm J$ random coupling has a polynomial energy gap in the symmetric subspace of the parity operator [Nature 631, 749-754 (2024)], indicating the efficient preparation of its ground states by quantum annealing. However, it is not clear if this result can be generalized to other spin glass models with continuous or biased randomness. Here we prove that under general independent and identical distributions (i.i.d.) of the exchange energies, the energy gap of a one-dimensional random transverse-field Ising model follows a stretched exponential scaling even in the parity-restricted subspace. We discuss the implication of this result to quantum annealing problems.

</details>


### [128] [A microscopic theory of Anderson localization of electrons in random lattices](https://arxiv.org/abs/2512.03917)
*Václav Janiš*

Main category: cond-mat.dis-nn

TL;DR: 本文提出了一种基于安德森量子模型的微观理论，用于描述无序电子系统中金属相和局域化相的统一框架，并推导了满足守恒定律和因果关系的格林函数近似方法。


<details>
  <summary>Details</summary>
Motivation: 虽然安德森局域化（由强随机性导致扩散消失）已在多种方式中得到证实，但基于安德森量子模型的系统方法尚未完全建立，该模型需要能够同时描述扩散和局域化两种状态。

Method: 基于近期研究，提出了一个微观理论，涵盖金属相（扩展布洛赫波）和局域化相（传播粒子与原点空穴形成量子束缚态）。该理论为构建受控近似的一粒子和二粒子格林函数提供了框架，这些函数在整个无序强度范围内满足必要的守恒定律和因果关系要求。

Result: 明确推导了二粒子不可约顶点的局域、类平均场近似，使得能够定量分析金属相和局域化相中解的性质，包括安德森局域化转变的临界行为。

Conclusion: 该理论为无序电子系统提供了一个统一的微观框架，能够同时处理金属相和局域化相，并为安德森局域化转变的定量分析提供了理论基础。

Abstract: The existence of Anderson localization, characterized by vanishing diffusion due to strong randomness, has been demonstrated in numerous ways. A systematic approach based on the Anderson quantum model of the Fermi gas in random lattices that can describe both diffusive and localized regimes has not yet been fully established. We build upon a recent publication \cite{Janis:2025ab} and present a microscopic theory of disordered electrons covering both the metallic phase with extended Bloch waves and the localized phase where the propagating particle forms a quantum bound state with the hole left behind at the origin. The general theory provides a framework for constructing controlled approximations to one-particle and two-particle Green functions that satisfy the necessary conservation laws and causality requirements in the whole range of disorder strength. It is used explicitly to derive a local, mean-field-like approximation for the two-particle irreducible vertices, enabling quantitative analysis of the solution's properties in both metallic and localized phases, including critical behavior at the Anderson localization transition.

</details>


### [129] [Testing the Localization Landscape Theory on the Bethe Lattice](https://arxiv.org/abs/2512.04037)
*Lorenzo Tonetti,Leticia F. Cugliandolo,Marco Tarzia*

Main category: cond-mat.dis-nn

TL;DR: 在Bethe晶格上精确测试局域化景观理论，发现其渗流转变与安德森局域化转变不重合，临界行为也不同，表明该理论不能准确描述量子局域化的临界性质。


<details>
  <summary>Details</summary>
Motivation: 局域化景观理论（LLT）通过引入有效约束势来经典描述安德森局域化，其渗流被认为与迁移率边重合。虽然在三维中数值结果良好，但其基本有效性尚未确定。本研究旨在通过Bethe晶格上的精确解来检验LLT的有效性。

Method: 在Bethe晶格上同时解析求解安德森局域化转变和LLT渗流问题，比较两者的转变点和临界行为。

Result: 发现两个转变不重合，临界行为显著不同：LLT渗流显示标准平均场渗流临界性，而Bethe晶格上的安德森转变具有独特的临界行为。

Conclusion: 虽然LLT在几何上直观，但不能捕捉局域化的真实量子临界性质。Bethe晶格上的精确解为LLT提供了基准测试，显示其与真正的量子临界行为存在根本差异。

Abstract: The Localization Landscape Theory (LLT) provides a classical picture of Anderson localization by introducing an effective confining potential whose percolation is proposed to coincide with the mobility edge. Although this proposal shows remarkable numerical agreement in three dimensions, its fundamental validity remains unsettled. Here we test the LLT analytically on the Bethe lattice, where both the Anderson localization transition and the LLT percolation problem are exactly solvable. We find that the two transitions do not coincide, and their critical behaviors differ markedly. In particular, LLT percolation displays standard mean-field percolation criticality that is fundamentally distinct from the peculiar critical behavior of the Anderson transition on the Bethe lattice. Our results provide an exact benchmark showing that, while geometrically intuitive, the LLT does not capture the true quantum critical properties of localization.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [130] [Transient chaos and Rayleigh particle escape out of a time modulated optical trap](https://arxiv.org/abs/2512.03403)
*E. N. Bulgakov,K. N. Pichugin,D. N. Maksimov*

Main category: nlin.CD

TL;DR: 光学陷阱中的瑞利粒子在特定参数下表现出瞬态混沌，导致粒子加速并被定向弹出陷阱，随后在斯托克斯力作用下返回，这一阈值效应可用于粒子分选。


<details>
  <summary>Details</summary>
Motivation: 研究周期性调制的光学陷阱中瑞利粒子的动力学行为，探索粒子在特定条件下出现的瞬态混沌现象及其潜在应用价值。

Method: 使用两个反向传播的高斯光束形成周期性调制的光学陷阱，分析瑞利粒子在该系统中的动力学行为，研究参数变化对粒子运动的影响。

Result: 在特定参数条件下，系统表现出瞬态混沌现象，导致粒子加速并被定向弹出陷阱，逃逸距离可达数百个波长，随后在斯托克斯力作用下返回陷阱，粒子逃逸表现为阈值效应。

Conclusion: 光学陷阱中的瑞利粒子在特定条件下会经历瞬态混沌导致的定向逃逸，这一阈值效应具有应用于粒子分选的潜力。

Abstract: We consider Rayleigh particles in a periodically modulated optical trap formed by two counter-propagating Gaussian beams. It is shown that for certain values of parameters the system exhibits transient chaos which manifests itself in particle acceleration and subsequent directional ejection out of the trap. The escape flights are terminated at the distance of hundreds wavelengths from the trap centrum and the particles return to the trap under the action of the Stokes force. The particle escape is shown to be a threshold effect that can be potentially employed for particle sorting.

</details>


### [131] [Adaptive Parameter Control Using AAN for Lower Limb Rehabilitation Exoskeletons](https://arxiv.org/abs/2512.03871)
*Zheng Sun,Wenkong Wang,Zizhong Wei,Xin Ma*

Main category: nlin.CD

TL;DR: 提出一种基于人机耦合动力学模型、人体扭矩动量观测器和自适应参数控制器的辅助即需控制算法，用于外骨骼康复训练，解决轨迹跟踪不精确、交互扭矩振荡和适应性不足等问题。


<details>
  <summary>Details</summary>
Motivation: 现有外骨骼控制策略存在轨迹跟踪不精确、交互扭矩振荡以及难以适应不同患者状况等问题，需要开发更有效的辅助即需控制算法来改善康复效果。

Method: 整合人机耦合动力学模型、人体扭矩动量观测器和自适应参数控制器。首先通过逆动力学计算康复轨迹所需的关节扭矩，然后通过HTMO估计患者关节扭矩并确定扭矩误差，外骨骼通过弹簧阻尼系统补偿误差生成目标轨迹，最后APC实现自适应辅助控制。

Result: 在MATLAB/Simulink环境中验证了所提方法的有效性，表明该算法能够改善外骨骼康复训练的控制性能。

Conclusion: 提出的辅助即需控制算法能够有效解决外骨骼康复训练中的控制挑战，为患者提供更精确、稳定和自适应的康复辅助。

Abstract: Exoskeletons play a crucial role in assisting patients with varying mobility levels during rehabilitation. However, existing control strategies face challenges such as imprecise trajectory tracking, interaction torque oscillations, and limited adaptability to diverse patient conditions. To address these issues, this paper proposes an assist-as-needed (AAN) control algorithm that integrates a human-robot coupling dynamics model, a human torque-momentum observer (HTMO), and an adaptive parameter controller (APC). The algorithm first employs inverse dynamics to compute the joint torques required for the rehabilitation trajectory. The HTMO then estimates the torque exerted by the patient's joints and determines the torque error, which the exoskeleton compensates for via a spring-damper system, ultimately generating the target trajectory. Finally, the APC ensures adaptive assistive control. The proposed method is validated for its effectiveness in MATLAB/Simulink.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [132] [Strain Response as a Probe of Spinons in Quantum Spin Liquids](https://arxiv.org/abs/2512.03137)
*Penghao Zhu,Archisman Panigrahi,Leonid Levitov,Nandini Trivedi*

Main category: cond-mat.str-el

TL;DR: 该论文提出利用晶格应变作为探测量子自旋液体中分数化费米子激发的有效方法，通过机械形变产生赝磁场和赝朗道能级，为不同QSL相提供可区分的实验特征。


<details>
  <summary>Details</summary>
Motivation: 量子自旋液体中的分数化费米子激发是电中性的，无法通过传统电磁探针检测，因此需要寻找新的实验方法来识别和区分这些激发态。

Method: 使用蜂窝晶格上的Kitaev模型，研究不同QSL相对晶格应变的响应。通过机械形变产生赝磁场，诱导赝朗道能级，分析半金属Kitaev自旋液体、有能隙手性自旋液体和Majorana金属相的应变响应差异。

Result: 半金属Kitaev自旋液体和有能隙手性自旋液体表现出明显的朗道量子化和类抗磁性响应，而Majorana金属相则表现出类顺磁性响应且不形成朗道能级。这些对比行为为实验识别不同QSL相提供了直接途径。

Conclusion: 晶格应变可作为探测量子自旋液体中分数化激发的有力工具，通过局部共振超声光谱可检测应变诱导的共振，为在候选材料中识别分数化激发提供了实用途径。

Abstract: Quantum spin liquids (QSLs) host emergent, fractionalized fermionic excitations that are charge-neutral. Identifying clear experimental signatures of these excitations remains a central challenge in the field of strongly correlated systems, as they do not couple to conventional electromagnetic probes. Here, we propose lattice strain as a powerful and tunable probe: Mechanical deformation of the lattice generates large pseudomagnetic fields, inducing pseudo-Landau levels that serve as distinctive spectroscopic signatures of these excitations. Using the Kitaev model on the honeycomb lattice, we show that distinct QSL phases exhibit strikingly different strain responses. The semimetallic Kitaev spin liquid and the gapped chiral spin liquid display pronounced Landau quantization and a diamagnetic-like response to strain, whereas the Majorana metal phase shows a paramagnetic-like response without forming Landau levels. These contrasting behaviors provide a direct route to experimentally identifying and distinguishing QSL phases hosting fractionalized excitations. We further outline how local resonant ultrasound spectroscopy can detect the strain-induced resonances associated with these responses, offering a practical pathway towards identifying fractionalized excitations in candidate materials.

</details>


### [133] [$α$-RuCl$_3$ intercalated into graphite: a new three-dimensional platform for exotic quantum phases](https://arxiv.org/abs/2512.03147)
*Aleksandar Razpopov,Shirin Mozaffari,Takahiro Matsuoka,Matthew Cothrine,Nan Huang,Roser Valentí,David Mandrus*

Main category: cond-mat.str-el

TL;DR: 石墨烯插层α-RuCl₃合成成功，为三维材料中的关联和拓扑态提供了新途径


<details>
  <summary>Details</summary>
Motivation: 多层石墨烯的不同堆叠序列已成为关联和拓扑相的有力平台，同时石墨烯与磁性或关联材料（如Kitaev候选材料α-RuCl₃）的异质结构研究取得了进展，这为工程量子系统创造了新机会。研究者希望探索三维类似物，将α-RuCl₃层直接插入石墨的范德华间隙中。

Method: 成功合成了α-RuCl₃插层石墨，并采用X射线衍射、量子振荡测量和第一性原理电子结构计算相结合的方法，系统研究了这些插层晶体的结构和电子性质。

Result: 研究结果表明，α-RuCl₃插层石墨为开发具有新颖关联和拓扑态的三维材料提供了稳健的途径。

Conclusion: α-RuCl₃插层石墨系统成功实现了三维材料中的关联和拓扑态工程，为量子材料研究开辟了新方向。

Abstract: Multilayer graphene with different stacking sequences has emerged as a powerful setting for correlated and topological phases. In parallel, progress in graphene heterostructures with magnetic or correlated materials-most notably the Kitaev candidate $α$-RuCl$_3$-has demonstrated charge transfer, magnetic proximity effects, and interfacial reconstruction, creating new opportunities for engineered quantum systems. Motivated by these developments, we explore a three-dimensional analogue in which $α$-RuCl$_3$ layers are inserted directly into the van der Waals gaps of graphite, forming an intercalated system. Here, we report the successful synthesis and comprehensive characterization of graphite intercalated with $α$-RuCl$_3$. Using a combination of X-ray diffraction, quantum oscillation measurements, and first-principles electronic structure calculations, we study the structural and electronic properties of these intercalated crystals. Our results demonstrate that graphite intercalated with $α$-RuCl$_3$ offers a robust route to develop three-dimensional materials with access to novel correlated and topological states.

</details>


### [134] [Proof that Momentum Mixing Hatsugai Kohmoto equals the Twisted Hubbard Model](https://arxiv.org/abs/2512.03148)
*Yuting Bai,Philip W. Phillips*

Main category: cond-mat.str-el

TL;DR: 本文正式证明了动量混合Hatsugai-Kohmoto模型(MMHK)是Hubbard模型的扭曲版本，并基于此证明了两个模型在电荷扇区具有相同的体物理性质。


<details>
  <summary>Details</summary>
Motivation: 研究MMHK模型与Hubbard模型之间的关系，探索不同模型之间是否存在等价性，特别是在电荷扇区的物理性质是否相同。

Method: 通过形式化证明MMHK模型是Hubbard模型的扭曲版本，然后利用Watanabe的定理：两个仅相差一个扭曲的模型必须具有相同的体物理性质。

Result: 证明了MMHK模型在电荷扇区与Hubbard模型等价，即MMHK=Hubbard在电荷扇区成立。

Conclusion: 动量混合Hatsugai-Kohmoto模型与Hubbard模型在电荷扇区具有相同的物理性质，这为理解两个模型之间的关系提供了理论基础。

Abstract: We prove formally that the momentum-mixing Hatsugai-Kohmoto model (MMHK) is the Hubbard model with a twist. With this result in tow, we rely on the proof of Watanabe's that two models which differ by a twist must have the same bulk physics. Consequently, we have proven that MMHK=Hubbard in the charge sector.

</details>


### [135] [Fermionic Critical Fluctuations: Potential Driver of Strange Metallicity and Violation of the Wiedemann-Franz Law in YbRh2Si2](https://arxiv.org/abs/2512.03618)
*Frank Steglich*

Main category: cond-mat.str-el

TL;DR: 该研究重新分析了重费米子化合物YbRh2Si2在磁场诱导量子临界点处的热电输运测量结果，揭示了奇异金属行为与维德曼-弗朗茨定律违反之间的关联，发现了一种新型的电荷和热载流子非弹性散射中心。


<details>
  <summary>Details</summary>
Motivation: 探索重费米子化合物YbRh2Si2在磁场诱导量子临界点处的奇异金属行为（表现为电导率和电子热导率的异常）与维德曼-弗朗茨定律在零温极限下的违反之间的关系。

Method: 重新分析YbRh2Si2在磁场诱导量子临界点处的热电输运测量结果，通过结合热导和电导测量来研究电荷和热载流子的输运行为。

Result: 检测到一种新型的电荷和热载流子非弹性散射中心，将其归因于小费米面到大费米面涨落。这些涨落作为费米子量子临界涨落，在近藤破坏量子临界点附近和该点处起作用，被认为是奇异金属行为和维德曼-弗朗茨定律违反的主要驱动因素。

Conclusion: 小费米面到大费米面涨落作为费米子量子临界涨落，是导致YbRh2Si2在量子临界点处奇异金属行为和维德曼-弗朗茨定律违反的根本原因，揭示了量子临界系统中非传统输运行为的新机制。

Abstract: Results of combined thermal and electrical transport measurements through the magnetic field-induced quantum critical point in the heavy-fermion compound YbRh2Si2 are revisited to explore the relationship between the strange-metal behavior, observed in both the electrical and electronic thermal resistivity, and the violation of the Wiedemann-Franz law in the zero-temperature limit. A new type of inelastic scattering center for the charge and heat carriers has been detected and ascribed to the small-to-large Fermi-surface fluctuations. These are operating in the vicinity of and at the Kondo-destroying quantum critical point as fermionic quantum critical fluctuations and are considered the primary driver of the strange-metal behavior and the violation of the Wiedemann-Franz law.

</details>


### [136] [More is uncorrelated: Tuning the local correlations of SU($N$) Fermi-Hubbard systems via controlled symmetry breaking](https://arxiv.org/abs/2512.03689)
*Edoardo Zavatti,Gabriele Bellomia,Samuele Giuli,Matteo Ferraretto,Massimo Capone*

Main category: cond-mat.str-el

TL;DR: 该研究探讨了冷原子实验中SU(N)对称性Hubbard模型的关联特性，发现SU(4)系统的关联性显著小于SU(2)系统，且对称性破缺可以增强关联强度。


<details>
  <summary>Details</summary>
Motivation: 研究冷原子实验中基于碱金属类原子的SU(N)对称性Hubbard模型，探索N值作为调节系统性质的新参数，特别是在全局半填充下的Mott转变和局域关联特性。

Method: 使用动力学平均场理论(DMFT)分析SU(N)系统，引入不同味之间的互信息来表征局域关联，并在原子极限下证明增加N会降低关联强度。通过Raman场逐渐打破对称性，从SU(4)过渡到SU(2)系统。

Result: SU(4)系统的关联性显著小于SU(2)系统；对称性破缺可以增强关联强度；在弱Raman耦合下，Mott态中突然恢复SU(2)模型的大关联特性；发现金属、带绝缘体和Mott绝缘体三相共存的三临界点。

Conclusion: 通过控制对称性破缺减少有效组分数可以增强系统的关联程度，这为冷原子实验中调控量子多体系统的关联特性提供了新途径，揭示了SU(N)对称性在量子相变中的重要作用。

Abstract: Cold-atom experiments based on alkali-like atoms provide us with a tool to experimentally realize Hubbard models with a large number $N$ of components. The value of $N$ can be seen as a new handle to tune the properties of the system, leading to new physics both in the case of fully SU($N$) symmetric systems, or in the presence of controlled symmetry breaking.
  We focus on the Mott transition at global half filling and we characterize local correlations between particles complementing conventional estimates with the inter-flavor mutual information. We prove that these correlations have classical nature and, using Dynamical Mean-Field Theory, we show that the SU(4) system has significantly smaller correlations than the SU(2) counterpart. In the atomic limit we prove that increasing $N$ further decreases the strength of the correlations. This suggests that a controlled reduction of the symmetry, reducing the number of effective components, can be used to enhance the degree of correlation.
  We confirm this scenario solving the model for $N=4$ and gradually breaking the symmetry via a Raman field, revealing an evolution from the SU(4) to the SU(2) Mott transition as the symmetry-breaking term increases, with a sudden recovery of the large correlations of the SU(2) model at weak Raman coupling in the Mott state. By further exploring the interplay between energy repulsion and the Raman field, we obtain a rich phase diagram with three different phases -- a metal, a band insulator, and a Mott insulator -- all coexisting at a single tricritical point.

</details>


### [137] [Sign-Resolved Statistics and the Origin of Bias in Quantum Monte Carlo](https://arxiv.org/abs/2512.04056)
*Ryan Larson,Rubem Mondaini,Richard T. Scalettar*

Main category: cond-mat.str-el

TL;DR: 该论文提出了一种新方法来诊断量子蒙特卡洛模拟中的符号问题，通过分析符号分辨的测量统计量来理解忽略符号权重导致的偏差，特别解释了d波配对敏感性。


<details>
  <summary>Details</summary>
Motivation: 量子模拟在探索强关联多体现象方面很强大，但受到费米子符号问题的限制，该问题导致配置权重变为负值，影响统计采样。在掺杂哈伯德模型的辅助场量子蒙特卡洛计算中，忽略权重符号会导致定性错误的结果，特别是d波配对在低温下的增强被错误地表现为抑制。

Method: 作者采用不同视角：不识别负权重路径，而是以符号分辨的方式检查测量可观测量（动能、反铁磁结构因子、配对磁化率）的统计量。通过分析符号为±1的配置的直方图，推导出忽略符号导致的偏差与符号分辨均值差Δμ和平均符号⟨S⟩之间的精确关系。

Result: 该框架为量子蒙特卡洛中的测量偏差提供了精确诊断工具，阐明了为什么像d波磁化率这样的可观测量对符号问题特别敏感。

Conclusion: 通过符号分辨的统计分析方法，可以更好地理解和诊断量子蒙特卡洛模拟中的符号问题，为解决这一长期存在的计算挑战提供了新思路。

Abstract: Quantum simulations are a powerful tool for exploring strongly correlated many-body phenomena. Yet, their reach is limited by the fermion sign problem, which causes configuration weights to become negative, compromising statistical sampling. In auxiliary-field Quantum Monte Carlo calculations of the doped Hubbard model, neglecting the sign ${\cal S}$ of the weight leads to qualitatively wrong results -- most notably, an apparent suppression rather than enhancement of $d$-wave pairing at low temperature. Here we approach the problem from a different perspective: instead of identifying negative-weight paths, we examine the statistics of measured observables in a sign-resolved manner. By analyzing histograms of key quantities (kinetic energy, antiferromagnetic structure factor, and pair susceptibilities) for configurations with ${\cal S}=\pm1$, we derive an exact relation linking the bias from ignoring the sign to the difference between sign-resolved means, $Δμ$, and the average sign, $\langle {\cal S}\rangle$. Our framework provides a precise diagnostic of the origin of measurement bias in Quantum Monte Carlo and clarifies why observables such as the $d$-wave susceptibility are especially sensitive to the sign problem.

</details>
