<div id=toc></div>

# Table of Contents

- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 5]
- [quant-ph](#quant-ph) [Total: 67]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 3]
- [cs.LG](#cs.LG) [Total: 50]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 9]
- [nlin.AO](#nlin.AO) [Total: 1]
- [cs.AI](#cs.AI) [Total: 40]


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [1] [Barrier-crossing and energy relaxation dynamics of non-Markovian inertial systems connected via analytical Green-Fokker-Planck approach](https://arxiv.org/abs/2601.09861)
*Roland R. Netz*

Main category: cond-mat.stat-mech

TL;DR: 该论文研究非马尔可夫一维反应坐标的势垒穿越时间，发现存在记忆翻转现象：中等记忆衰减时间下穿越时间比马尔可夫极限短，而长记忆时间下则随记忆时间平方增长。作者通过广义福克-普朗克方程和累积展开，推导出解析的阿伦尼乌斯表达式，能准确预测记忆翻转现象。


<details>
  <summary>Details</summary>
Motivation: 数值模拟表明非马尔可夫系统的势垒穿越时间存在记忆翻转现象，但现有理论无法系统解释长记忆时间下的渐近慢化行为。需要建立解析可处理的理论来理解这一现象，并提供从实验数据预测势垒穿越时间的方法。

Method: 从一般惯性非马尔可夫高斯反应坐标的格林函数出发，通过精确映射得到具有时间依赖有效扩散常数的广义福克-普朗克方程。采用系统累积展开的一阶近似，推导出解析的阿伦尼乌斯表达式，其中指前因子由能量弛豫时间给出。对单指数记忆核给出了闭式表达式。

Result: 推导的解析表达式能准确再现Kramers翻转（高摩擦与高质量极限之间）和记忆翻转（中等记忆加速与长记忆慢化之间）。发现非马尔可夫系统在零质量极限下是奇异的，表明长记忆势垒穿越慢化反映了质量与记忆效应的相互作用。

Conclusion: 建立了能预测非马尔可夫惯性系统势垒穿越时间的解析理论，无需提取记忆函数。发现物理上合理的非马尔可夫模型必须包含有限质量，因为质量与记忆效应的相互作用对势垒穿越动力学至关重要。

Abstract: From numerical simulations it is known that the barrier-crossing time of a non-Markovian one-dimensional reaction coordinate with a single exponentially decaying memory function exhibits a memory-turnover: for intermediate values of the memory decay time the barrier-crossing time is reduced compared to the Markovian limit and for long memory times increases quadratically with the memory time when keeping the total integrated friction and the mass constant. The intermediate memory acceleration regime is accurately predicted by Grote-Hynes theory, for the asymptotic long-memory slow-down behavior no systematic analytically tractable theory is available. Starting from the Green function for a general inertial (i.e. finite-mass) non-Markovian Gaussian reaction coordinate in a harmonic well, we derive by an exact mapping a generalized Fokker-Planck equation with a time-dependent effective diffusion constant. To first order in a systematic cumulant expansion we derive an analytical Arrhenius expression for the barrier-crossing time with the pre-exponential factor given by the energy relaxation time, which can be used to robustly predict barrier-crossing times from simulation or experimental trajectory data of general non-Markovian inertial systems without the need to extract memory functions. For a single exponential memory kernel we give a closed-form expression for the barrier-crossing time, which reproduces the Kramers turnover between the high-friction and high-mass limits as well as the memory turnover from the intermediate memory acceleration to the asymptotic long-memory slow-down regime. We also show that non-Markovian systems are singular in the zero-mass limit, which suggests that the long-memory barrier-crossing slow-down reflects the interplay between mass and memory effects. Thus, physically sound models for non-Markovian systems have to include a finite mass.

</details>


### [2] [Synchronization with Annealed Disorder and Higher-Harmonic Interactions in Arbitrary Dimensions: When Two Dimensions Are Special](https://arxiv.org/abs/2601.10646)
*Rupak Majumder,Shamik Gupta*

Main category: cond-mat.stat-mech

TL;DR: 该研究分析了退火无序对D维仓本模型同步行为的影响，发现退火无序消除了奇偶维度二分现象，使所有维度都呈现连续同步转变，而高阶谐波耦合则使转变在连续与不连续之间可调，并揭示了二维的特殊性。


<details>
  <summary>Details</summary>
Motivation: 研究退火无序对高维仓本模型同步行为的影响，因为之前的研究主要关注淬火无序（已知会产生奇偶维度二分现象），而退火无序的影响相对较少被研究。

Method: 建立包含基本耦合和高阶谐波相互作用的D维仓本模型，采用退火无序条件，开发任意维度的中心流形框架来分析集体行为起始附近的非线性动力学。

Result: 退火无序完全消除了奇偶维度二分现象，使所有维度都呈现连续的同步转变并具有普适的平均场标度；高阶谐波耦合保持了这种普适性，同时使同步转变在连续与不连续之间可调；发现了一种新的相关驱动转变，该转变在二维是连续的，但在更高维度是不连续的，揭示了二维的特殊作用。

Conclusion: 退火无序从根本上改变了维度在同步行为中的作用，消除了淬火无序产生的奇偶维度二分现象，同时揭示了二维在相关驱动转变中的特殊地位，为理解无序类型对集体现象的影响提供了新视角。

Abstract: The impact of disorder on collective phenomena depends crucially on whether it is quenched or annealed. In synchronization problems, quenched disorder in higher dimensional Kuramoto models is known to produce unconventional dimensional effects, including a striking odd even dichotomy: synchronization transitions are continuous in even dimensions and discontinuous in odd dimensions. By contrast, the impact of annealed disorder has received comparatively little attention. Here we study a D dimensional Kuramoto model with both fundamental and higher-harmonic interactions under annealed disorder, and develop an arbitrary dimensional center-manifold framework to analyze the nonlinear dynamics near the onset of collective behavior. We show that annealed disorder fundamentally alters the role of dimensionality. With fundamental coupling alone, it completely removes the odd even dichotomy, yielding continuous synchronization transitions with universal mean-field scaling in all dimensions. Higher-harmonic interactions preserve this universality while rendering the synchronization transition tunable between continuous and discontinuous. At the same time, they give rise to a novel, correlation-driven transition between a symmetry-protected incoherent phase and a symmetry broken state lacking global synchronization, which is therefore invisible to the conventional Kuramoto order parameter. This transition is continuous in two dimensions but discontinuous in higher dimensions, revealing an emergent and previously-unrecognized special role of two dimensions.

</details>


### [3] [It Takes Two to Make a Thing Go Right: Boosting Current in Coupled Motors](https://arxiv.org/abs/2601.09907)
*Geyao Gu,Drew Alvarez,John Strahan,Alex Albaugh,Emanuele Penocchio,Todd R. Gingrich*

Main category: cond-mat.stat-mech

TL;DR: 多分子马达机械耦合可提升平均电流，通过耦合增强活性并提高燃料浓度恢复偏置


<details>
  <summary>Details</summary>
Motivation: 催化驱动的合成分子马达在松散的机械化学耦合机制下运行，燃料分子分解不一定可靠地产生前进步伐，随机后向步骤会显著降低马达电流，因此研究多个马达的机械耦合是否能提升平均电流

Method: 使用两类模型进行模拟：基于粒子的非平衡分子动力学模型和跳跃扩散模型，研究基于轮烷的马达

Result: 电流提升是物理上可实现的，观测到的电流提升（单数倍放大）出现在马达间耦合能增加活性时，既加速前进步伐也加速后进步伐，虽然前进步伐偏置会降低，但可通过提高燃料浓度恢复偏置

Conclusion: 提出通用设计策略：通过耦合放大活性，通过更强的驱动力恢复偏置，机械耦合多个分子马达可以提升其平均电流

Abstract: Catalysis-driven synthetic molecular motors operate in a loose mechanochemical coupling regime, one in which a decomposition of a fuel molecule does not reliably produce a forward step. In that regime, stochastic backward steps can significantly degrade the motor's current, prompting us to ask whether mechanically coupling multiple such motors can boost their averaged current. By simulating rotaxane-based motors with two classes of models--particle-based nonequilibrium molecular dynamics and jump-diffusion models--we show that current boosts are physically achievable. Our observed boosts, which amplify current by single-digit factors, emerge when coupling between motors can increase the activity, speeding up the rate of both forward and backward steps. In doing so, the bias for preferring forward steps actually degrades, but the lost bias can be largely recovered by raising the fuel concentration, demonstrating a general design strategy: amplify activity through coupling and restore bias through stronger driving.

</details>


### [4] [Rotational Memory Function of SPC/E water](https://arxiv.org/abs/2601.10022)
*Dilipkumar N. Asthagiri,Dmitry V. Matyushov*

Main category: cond-mat.stat-mech

TL;DR: 本文首次计算了极性液体中偶极旋转的记忆函数，发现单粒子与集体偶极动力学的记忆函数几乎相同，验证了介电光谱理论中单粒子时间相关函数的重要性。


<details>
  <summary>Details</summary>
Motivation: 记忆效应对凝聚态材料动力学至关重要，但极性液体中偶极旋转的记忆函数从未被计算过。本文旨在填补这一空白，通过计算SPC/E水的记忆函数来理解偶极动力学。

Method: 对SPC/E水模型进行计算，分别计算单偶极旋转和样品总偶极矩的记忆函数，比较单粒子和集体偶极动力学的记忆函数特性。

Result: 单粒子和集体偶极动力学的记忆函数几乎完全相同，记忆时间很短（小于1飞秒），这验证了介电光谱理论中单粒子时间相关函数的有效性。

Conclusion: 该结果验证了介电光谱理论中单粒子时间相关函数的重要性，表明介电函数不包含超出单偶极相关函数的新动态信息，短记忆时间支持使用旋转扩散模型描述水中的单分子偶极矩动力学。

Abstract: Memory effects are essential for dynamics of condensed materials and are responsible for non-exponential relaxation of correlation functions of dynamic variables through the memory function. Memory functions of dipole rotations for polar liquids have never been calculated. We present here calculations of memory functions for single-dipole rotations and for the overall dipole moment of the sample for SPC/E water. The memory functions for single-particle and collective dipole dynamics turn out to be nearly identical. This result validates theories of dielectric spectroscopy in terms of single-particle time correlation functions and the connection between the collective and single-particle relaxation times through the Kirkwood factor. The dielectric function in this formalism contains no new dynamic information that does not exist in the single-dipole correlation function. A short memory time, $\lesssim 1$ fs, justifies the use of rotational diffusion model to describe dynamics of a single molecular dipole moment in bulk water.

</details>


### [5] [Random matrix theory universality of current operators in spin-$S$ Heisenberg chains](https://arxiv.org/abs/2601.10211)
*Mariel Kempa,Markus Kraft,Robin Steinigeweg,Jochen Gemmer,Jiaozi Wang*

Main category: cond-mat.stat-mech

TL;DR: 该论文通过数值研究验证了量子混沌系统中可观测量统计性质的随机矩阵理论普适性猜想，在自旋链中发现了支持证据。


<details>
  <summary>Details</summary>
Motivation: 量子混沌系统展现出与随机矩阵理论预测相似的统计性质。最近有猜想认为，在足够窄的能量窗口内，可观测量的统计性质可以用幺正不变系综描述，并基于自由累积量的标度行为提出了可检验的标准。本研究旨在数值验证这一猜想。

Method: 使用基于量子典型性的数值方法，结合系统对称性的利用，研究具有平移不变性的海森堡自旋链（自旋量子数S=1/2,1,3/2）。重点研究自旋流算符，分析其统计性质是否满足基于自由累积量标度行为的检验标准。

Result: 在混沌情况下，发现了与所提标准一致性的明确证据。自旋流算符的统计性质表现出与随机矩阵理论预测相符的行为，进一步支持了量子混沌系统中可观测量存在RMT普适性的猜想。

Conclusion: 研究结果进一步支持了量子混沌系统中可观测量统计性质存在随机矩阵理论普适性的猜想，为理解量子混沌系统的统计性质提供了新的数值证据。

Abstract: Quantum chaotic systems exhibit certain universal statistical properties that closely resemble predictions from random matrix theory (RMT). With respect to observables, it has recently been conjectured that, when truncated to a sufficiently narrow energy window, their statistical properties can be described by an unitarily invariant ensemble, and testable criteria have been introduced, which are based on the scaling behavior of free cumulants. In this paper, we investigate the conjecture numerically in translationally invariant Heisenberg spin chains with spin quantum number $S =\frac{1}{2},1,\frac{3}{2}$. Combining a quantum-typicality-based numerical method with the exploitation of the system's symmetries, we study the spin current operator and find clear evidence of consistency with the proposed criteria in chaotic cases. Our findings further support the conjecture of the existence of RMT universality as manifest in the observable properties in quantum chaotic systems.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [6] [Limits of Rank Recovery in Bilinear Observation Problems](https://arxiv.org/abs/2601.09754)
*Seungbeom Choi*

Main category: quant-ph

TL;DR: 该研究分析了双线性观测问题中秩恢复的局限性，发现即使进行数值细化，秩缺陷仍会在广泛的容差范围内稳定存在，表明需要改变问题结构本身而非仅进行数值优化才能恢复有效维度。


<details>
  <summary>Details</summary>
Motivation: 双线性观测问题在物理和信息理论中广泛存在，通常使用秩诊断来评估可观测的有效维度。传统方法隐含假设秩缺陷可以通过数值细化解决，但这一假设缺乏系统验证。本研究旨在检验这一假设，探究秩恢复的根本限制。

Method: 通过分析双线性观测算子在系统容差变化下的秩和零空间特性，而非依赖特定重建算法。将零空间分解为由变量块结构定义的代数扇区，研究其内部组织结构。比较数值细化与问题结构修改对秩恢复的影响。

Result: 发现了在广泛容差范围内持续存在的扩展秩平台，表明稳定的维度缺陷无法通过固定问题定义内的细化程序消除。零空间在特定扇区表现出显著但非排他性的集中，揭示了有组织的内部结构而非均匀的维度损失。秩恢复需要改变观测问题本身的结构，而非仅进行数值优化。

Conclusion: 双线性观测问题中的秩恢复存在根本限制，数值细化无法解决某些秩缺陷。秩平台现象表明需要区分数值细化与问题结构修改：前者在固定问题框架内优化，后者改变观测问题的基本结构。这对理解双线性系统的可观测性和维度结构有重要启示。

Abstract: Bilinear observation problems arise in many physical and information-theoretic settings, where observables and states enter multiplicatively. Rank-based diagnostics are commonly used in such problems to assess the effective dimensionality accessible to observation, often under the implicit assumption that rank deficiency can be resolved through numerical refinement. Here we examine this assumption by analyzing the rank and nullity of a bilinear observation operator under systematic tolerance variation. Rather than focusing on a specific reconstruction algorithm, we study the operator directly and identify extended rank plateaus that persist across broad tolerance ranges. These plateaus indicate stable dimensional deficits that are not removed by refinement procedures applied within a fixed problem definition. To investigate the origin of this behavior, we resolve the nullspace into algebraic sectors defined by the block structure of the variables. The nullspace exhibits a pronounced but nonexclusive concentration in specific sectors, revealing an organized internal structure rather than uniform dimensional loss. Comparing refinement with explicit modification of the problem formulation further shows that rank recovery in the reported setting requires a change in the structure of the observation problem itself. Here, "problem modification" refers to changes that alter the bilinear observation structure (e.g., admissible operator/state families or coupling constraints), in contrast to refinements that preserve the original formulation such as tolerance adjustment and numerical reparameterizations. Together, these results delineate limits of rank recovery in bilinear observation problems and clarify the distinction between numerical refinement and problem modification in accessing effective dimensional structure.

</details>


### [7] [Fractional Revival Dynamics in Kerr-Type Systems: Angular Momentum Moments and Classical Analogs](https://arxiv.org/abs/2601.09763)
*Ashish Kumar Patra,Saikumar Krithivasan*

Main category: quant-ph

TL;DR: 该研究扩展了波包复兴现象的分析，重点探讨角动量观测量中的分数复兴动力学，并比较量子复兴与经典系统的结构相似性。


<details>
  <summary>Details</summary>
Motivation: 波包复兴和分数复兴是非线性能谱系统中的典型量子干涉现象，先前研究主要关注可观测量期望值的特征。本文旨在扩展分析范围，探索角动量观测量中的分数复兴动力学，并建立量子复兴现象与经典系统之间的对应关系。

Method: 使用Kerr型非线性哈密顿量作为范例模型，分析自相关函数、矩动力学和相空间结构，通过量子地毯等可视化工具支持分析。推导角动量观测量矩的时间演化显式表达式，并研究代表性经典系统中的重现行为。

Result: 研究发现高阶角动量矩提供了分数复兴的清晰选择性特征。同时揭示了量子分数复兴与经典系统重现行为之间的结构相似性，为实验诊断提供了更广泛的工具。

Conclusion: 该研究扩展了分数复兴的实验诊断范围，为量子与经典动力系统中的复兴现象提供了统一的理论视角，加深了对非线性系统中量子干涉现象的理解。

Abstract: Wave packet revivals and fractional revivals are hallmark quantum interference phenomena that arise in systems with nonlinear energy spectra, and their signatures in expectation values of observables have been studied extensively in earlier work. In this article, we build on these studies and extend the analysis in two important directions. First, we investigate fractional revival dynamics in angular momentum observables, deriving explicit expressions for the time evolution of their moments and demonstrating that higher-order angular momentum moments provide clear and selective signatures of fractional revivals. Second, we examine classical analogs of quantum revival phenomena and elucidate structural similarities between quantum fractional revivals and recurrence behavior in representative classical systems. Using the Kerr-type nonlinear Hamiltonian as a paradigmatic model, we analyze the autocorrelation function, moment dynamics, and phase-space structures, supported by visualizations such as quantum carpets. Our results broaden the range of experimentally accessible diagnostics of fractional revivals and provide a unified perspective on revival phenomena across quantum and classical dynamical systems.

</details>


### [8] [Three questions on the future of quantum science and technology](https://arxiv.org/abs/2601.09769)
*S. Radenkovic,M. Dugic,I. Radojevic*

Main category: quant-ph

TL;DR: 本文介绍了量子科学与技术的现状和未来发展前景


<details>
  <summary>Details</summary>
Motivation: 量子科学与技术是当前科技发展的前沿领域，具有重要的科学意义和应用价值，需要对其现状和未来发展方向进行系统梳理和分析

Method: 通过文献综述、专家访谈和数据分析等方法，对量子科学技术的各个领域进行综合评估

Result: 总结了量子计算、量子通信、量子传感等领域的当前发展水平，识别了关键技术挑战和发展瓶颈

Conclusion: 量子科学与技术正处于快速发展阶段，未来将在计算、通信、传感等领域带来革命性突破，需要加强基础研究和应用开发

Abstract: The answers on the current status and future development of Quantum Science and Technology are presented.

</details>


### [9] [Hierarchical time crystals](https://arxiv.org/abs/2601.09779)
*Jan Carlo Schumann,Igor Lesanovsky,Parvinder Solanki*

Main category: quant-ph

TL;DR: 该研究展示了离散时间晶体与连续时间晶体耦合系统如何产生层次时间晶体相，实现同时双重时间对称性破缺


<details>
  <summary>Details</summary>
Motivation: 探索离散时间晶体和连续时间晶体相互作用的物理效应，研究它们耦合时可能产生的新奇非平衡相

Method: 构建时间无关的离散与连续时间晶体耦合系统，研究其在不同耦合方案下的动力学行为

Result: 发现系统诱导同时双重时间对称性破缺，形成层次时间晶体相，其中一个子系统破缺了动态涌现的离散时间对称性

Conclusion: 层次时间晶体是鲁棒的非平衡相，能在不同耦合方案和广泛参数范围内存在，揭示了时间晶体相互作用的新物理

Abstract: Spontaneous symmetry breaking is one of the central organizing principles in physics. Time crystals have emerged as an exotic phase of matter, spontaneously breaking the time translational symmetry, and are mainly categorized as discrete or continuous. While these distinct types of time crystals have been extensively explored as standalone systems, intriguing effects can arise from their mutual interaction. Here, we demonstrate that a time-independent coupled system of discrete and continuous time crystals induces a simultaneous two-fold temporal symmetry breaking, resulting in a hierarchical time crystal phase. Interestingly, one of the subsystems breaks an emergent discrete temporal symmetry that does not exist in the dynamical generator but rather emerges dynamically, leading to a convoluted non-equilibrium phase. We demonstrate that hierarchical time crystals are robust, emerging for fundamentally different coupling schemes and persisting across wide ranges of system parameters.

</details>


### [10] [Zero-Error List Decoding for Classical-Quantum Channels](https://arxiv.org/abs/2601.09786)
*Marco Dalai,Filippo Girardi,Ludovico Lami*

Main category: quant-ph

TL;DR: 研究纯态经典-量子信道在列表解码设置下的零错误容量，给出了列表大小为2的可达界和适用于任意固定列表大小的逆界，并讨论了经典-量子情况与完全经典情况的显著差异。


<details>
  <summary>Details</summary>
Motivation: 研究纯态经典-量子信道在列表解码设置下的零错误容量特性，探索经典-量子信道与完全经典信道在零错误列表编码方面的根本差异。

Method: 为列表大小为2的情况提供了可达界，并为任意固定列表大小提供了逆界；特别关注了成对绝对态重叠形成半正定矩阵的信道；分析了球堆积界发散速率与零错误列表码可达性的关系。

Result: 对于成对绝对态重叠形成半正定矩阵的信道，可达界和逆界重合；发现经典-量子情况下，即使取固定但任意大的列表大小极限，球堆积界发散速率也可能无法通过零错误列表码实现，这与完全经典情况不同。

Conclusion: 经典-量子信道在零错误列表解码方面表现出与完全经典信道不同的特性，特别是球堆积界发散速率可能无法通过零错误列表码实现，这揭示了量子信息处理中的独特现象。

Abstract: The aim of this work is to study the zero-error capacity of pure-state classical-quantum channels in the setting of list decoding. We provide an achievability bound for list-size two and a converse bound holding for every fixed list size. The two bounds coincide for channels whose pairwise absolute state overlaps form a positive semi-definite matrix. Finally, we discuss a remarkable peculiarity of the classical-quantum case: differently from the fully classical setting, the rate at which the sphere-packing bound diverges might not be achievable by zero-error list codes, even when we take the limit of fixed but arbitrarily large list size.

</details>


### [11] [Background cancellation for frequency-selective quantum sensing](https://arxiv.org/abs/2601.09792)
*Ricard Puig,Nathan Constantinides,Bharath Hebbe Madhusudhana,Daniel Bowring,C. Huerta Alderete,Andrew T. Sornborger*

Main category: quant-ph

TL;DR: 该论文提出了一种利用静态相互作用和纠缠的量子传感器，作为被动、可调谐、带阈值的频率滤波器，无需复杂控制方案即可检测特定频率的弱时变信号。


<details>
  <summary>Details</summary>
Motivation: 量子传感面临的关键挑战是检测弱时变信号，特别是那些在背景场中作为特定频率扰动的信号。传统方法通常需要复杂的量子传感器动态控制和繁重的经典后处理。

Method: 提出一种量子传感器，利用静态相互作用和纠缠作为被动、可调谐、带阈值的频率滤波器。通过将频率选择性和阈值行为直接编码到动力学中，使传感器仅对所选目标频率且振幅超过阈值的信号产生响应。

Result: 该方法避免了复杂控制方案的需求，并减少了后处理开销。传感器能够选择性地检测特定频率的弱信号，同时抑制背景噪声。

Conclusion: 提出的量子传感器设计通过将频率选择性和阈值检测功能直接集成到量子系统的动力学中，为弱时变信号的检测提供了一种更简单、更高效的替代方案，减少了传统方法所需的复杂控制和后处理负担。

Abstract: A key challenge in quantum sensing is the detection of weak time dependent signals, particularly those that arise as specific frequency perturbations over a background field. Conventional methods usually demand complex dynamical control of the quantum sensor and heavy classical post-processing. We propose a quantum sensor that leverages time independent interactions and entanglement to function as a passive, tunable, thresholded frequency filter. By encoding the frequency selectivity and thresholding behavior directly into the dynamics, the sensor is responsive only to a target frequency of choice whose amplitude is above a threshold. This approach circumvents the need for complex control schemes and reduces the post-processing overhead.

</details>


### [12] [Localization of quantum states within subspaces](https://arxiv.org/abs/2601.09817)
*L. L. Salcedo*

Main category: quant-ph

TL;DR: 该论文提出了量子态在希尔伯特空间子空间中定位概率的精确定义，识别了态的局域化分量，并建立了相关数学性质，讨论了在量子信息中的应用。


<details>
  <summary>Details</summary>
Motivation: 量子态在希尔伯特空间子空间中的定位概念在量子信息处理中很重要，但缺乏精确定义。需要建立严格的数学框架来描述量子态的局域化特性。

Method: 提出了量子态在给定子空间中定位概率的精确定义，明确识别了态的局域化分量，并建立了相关的数学性质。

Result: 建立了量子态局域化概率的严格数学定义，识别了局域化分量，证明了相关数学性质，为量子信息应用提供了理论基础。

Conclusion: 该工作为量子态在希尔伯特空间子空间中的定位提供了严格的数学框架，在量子信息处理中具有重要应用价值。

Abstract: A precise definition is proposed for the localization probability of a quantum state within a given subspace of the full Hilbert space of a quantum system. The corresponding localized component of the state is explicitly identified, and several mathematical properties are established. Applications and interpretations in the context of quantum information are also discussed.

</details>


### [13] [Fragmented Topological Excitations in Generalized Hypergraph Product Codes](https://arxiv.org/abs/2601.09850)
*Meng-Yuan Li,Yue Wu*

Main category: quant-ph

TL;DR: 该研究通过广义超图乘积码构造探索分形拓扑序，在3D正交多面体模型中发现非单调基态简并度和非阿贝尔晶格缺陷，在4D中发现碎片化拓扑激发——这种激发在实空间是离散点，但在低维子系统投影中形成连接对象。


<details>
  <summary>Details</summary>
Motivation: 量子稳定子码是实现容错量子计算的重要范式，其与可精确求解自旋模型基态的自然映射关系促使研究者探索稳定子码中的多体序。本研究旨在通过广义超图乘积码构造来研究分形拓扑序。

Method: 采用广义超图乘积码构造方法，构建了一类称为正交多面体模型的精确可解自旋模型，基于稳定子的几何结构进行分析。

Result: 在3D正交多面体模型中发现了非单调基态简并度（随系统尺寸变化）和非阿贝尔晶格缺陷；在4D中发现了碎片化拓扑激发，这种激发在实空间表现为离散点，但在低维子系统投影中形成连接对象（如环路），构成点状与空间扩展拓扑激发之间的中间类别。

Conclusion: 广义超图乘积码为研究分形拓扑序物理提供了一个多功能且可解析处理的平台，碎片化激发揭示了拓扑激发的新颖中间类别特征。

Abstract: Product code construction is a powerful tool for constructing quantum stabilizer codes, which serve as a promising paradigm for realizing fault-tolerant quantum computation. Furthermore, the natural mapping between stabilizer codes and the ground states of exactly solvable spin models also motivates the exploration of many-body orders in the stabilizer codes. In this work, we investigate the fracton topological orders in a family of codes obtained by a recently proposed general construction. More specifically, this code family can be regarded as a class of generalized hypergraph product (HGP) codes. We term the corresponding exactly solvable spin models \textit{orthoplex models}, based on the geometry of the stabilizers. In the 3D orthoplex model, we identify a series of intriguing properties within this model family, including non-monotonic ground state degeneracy (GSD) as a function of system size and non-Abelian lattice defects. Most remarkably, in 4D we discover \textit{fragmented topological excitations}: while such excitations manifest as discrete, isolated points in real space, their projections onto lower-dimensional subsystems form connected objects such as loops, revealing the intrinsic topological nature of these excitations. Therefore, fragmented excitations constitute an intriguing intermediate class between point-like and spatially extended topological excitations. In addition, these rich features establish the generalized HGP codes as a versatile and analytically tractable platform for studying the physics of fracton orders.

</details>


### [14] [Multi-level quantum emitter in an optical waveguide: paradoxes and resolutions](https://arxiv.org/abs/2601.09854)
*Ben Lang*

Main category: quant-ph

TL;DR: 研究多能级量子系统与任意局域偏振单模光波导之间的光学偶极相互作用，发现了一些看似矛盾的现象，如两个非正交量子态能产生相反方向的光子流，并解释了这些现象不违反量子力学幺正性要求。


<details>
  <summary>Details</summary>
Motivation: 探索多能级量子系统与任意偏振光波导相互作用的奇特现象，特别是那些看似违反直觉的物理行为，以深入理解量子光学中的基本原理。

Method: 理论分析多能级量子系统与单模光波导的光学偶极相互作用，考虑任意局域偏振情况，通过数学建模研究各种看似矛盾的物理情境。

Result: 发现两个非正交量子态能产生相反方向的光子流，但这一现象不违反量子力学幺正性；各向同性量子发射体根据波导偏振可呈现反射或透射特性，在零损耗极限下，无限小的偏振旋转可使系统从100%透射变为100%反射；四能级系统可作为光子数的无损宇称测量装置。

Conclusion: 多能级量子系统与偏振光波导的相互作用展现出丰富的物理现象，这些看似矛盾的结果实际上与量子力学基本原理一致，为量子信息处理和量子光学器件设计提供了新的可能性。

Abstract: We theoretically investigate the optical dipole interaction between a multi-level quantum system and a single-mode optical waveguide of any local polarisation. We investigate several paradoxical seeming situations, for example we find a situation in which there exist two non-orthogonal quantum states, each of which results in a photon flux in the opposite direction to the other. We show how, despite appearances, this does not break the unitary requirements of quantum mechanics. We also find that an isotropic quantum emitter can be either reflective or transmissive to light depending on the waveguide polarisation at the emitter location, indeed in the zero loss limit such a system changes from 100% transmission to 100% reflection due to an infinitesimal polarisation rotation. An example case for a four level system is also considered, which is found to operate as a non-destructive parity measurement of the photon number.

</details>


### [15] [Time-Dynamic Circuits for Fault-Tolerant Shift Automorphisms in Quantum LDPC Codes](https://arxiv.org/abs/2601.09911)
*Younghun Kim,Spiro Gicev,Martin Sevior,Muhammad Usman*

Main category: quant-ph

TL;DR: 该论文提出了一种通过动态变化测量电路来实现量子低密度奇偶校验码的移位自同构的方法，相比基于SWAP的方案显著降低了逻辑错误率。


<details>
  <summary>Details</summary>
Motivation: 量子低密度奇偶校验码是实现低开销逻辑量子存储的有前途方法，移位自同构是完成通用逻辑门集的基本构建块。然而，现有的基于SWAP的移位自同构方案会产生比容错空闲操作高几个数量级的逻辑错误率，这成为实际应用的主要障碍。

Method: 通过动态变化综合征测量电路来实现移位自同构，而不降低电路距离。该方法在扭曲和非扭曲的重量-6广义环面码（包括gross码族）上进行基准测试，使用BP-OSD解码器，在电路级噪声模型（SI1000）下评估性能。

Result: 动态电路实现的移位自同构性能与空闲操作相当。在物理错误率为10^-3时，gross码的动态电路相比基于SWAP的方案实现了超过一个数量级的逻辑错误率降低。动态电路还减少了移位自同构的时间开销。

Conclusion: 该工作提高了qLDPC码中移位自同构的错误恢复能力和时间效率，为超越表面码的qLDPC码提供了实用的动态电路实现途径，并可引导替代的综合征提取电路设计，如泄漏移除协议。

Abstract: Quantum low-density parity-check (qLDPC) codes have emerged as a promising approach for realizing low-overhead logical quantum memories. Recent theoretical developments have established shift automorphisms as a fundamental building block for completing the universal set of logical gates for qLDPC codes. However, practical challenges remain because the existing SWAP-based shift automorphism yields logical error rates that are orders of magnitude higher than those for fault-tolerant idle operations. In this work, we address this issue by dynamically varying the syndrome measurement circuits to implement the shift automorphisms without reducing the circuit distance. We benchmark our approach on both twisted and untwisted weight-6 generalized toric codes, including the gross code family. Our time-dynamic circuits for shift automorphisms achieve performance comparable to the idle operations under the circuit-level noise model (SI1000). Specifically, the dynamic circuits achieve more than an order of magnitude reduction in logical error rates relative to the SWAP-based scheme for the gross code at a physical error rate of $10^{-3}$, employing the BP-OSD decoder. Our findings improve both the error resilience and the time overhead of the shift automorphisms in qLDPC codes. Furthermore, our work can lead to alternative syndrome extraction circuit designs, such as leakage removal protocols, providing a practical pathway to utilizing dynamic circuits that extend beyond surface codes towards qLDPC codes.

</details>


### [16] [Nonlinear quantum Kibble-Zurek ramps in open systems at finite temperature](https://arxiv.org/abs/2601.10465)
*Johannes N. Kriel,Emma C. King,Michael Kastner*

Main category: quant-ph

TL;DR: 研究量子系统在温度和哈密顿量控制参数同时非线性变化下的动力学行为，探索在有限温度下探测量子相变临界指数的方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常在固定温度下研究量子临界点，但实际实验往往在有限温度下进行。本文旨在开发一种在有限温度、非平衡条件下探测量子相变临界指数的方法。

Method: 使用Kitaev量子线作为开放系统模型，分析温度和哈密顿量控制参数同时非线性变化（向量子临界点逼近）的协议。特别关注相干和非相干动力学对激发密度的影响。

Result: 发现特定类型的参数变化协议可以在有限温度、非平衡条件下探测量子相变的临界指数（ν和z）。识别了能够抑制渐近标度律次主导修正的具体变化方式。

Conclusion: 提出的协议为在实验现实的有限温度条件下动态探测量子临界指数提供了指导，扩展了量子相变研究的方法论。

Abstract: We analyze quantum systems under a broad class of protocols in which the temperature and a Hamiltonian control parameter are ramped simultaneously and, in general, in a nonlinear fashion toward a quantum critical point. Using an open-system version of a Kitaev quantum wire as an example, we show that, unlike finite-temperature protocols at fixed temperature, these protocols allow us to probe, in an out-of-equilibrium situation and at finite temperature, the universality class (characterized by the critical exponents $ν$ and $z$) of an equilibrium quantum phase transition at zero temperature. Key to this is the identification of ramps in which both coherent and incoherent parts of the open-system dynamics affect the excitation density in a non-negligible way. We also identify the specific ramps for which subleading corrections to the asymptotic scaling laws are suppressed, which serves as a guide to dynamically probing quantum critical exponents in experimentally realistic finite-temperature situations.

</details>


### [17] [Beyond Optimization: Harnessing Quantum Annealer Dynamics for Machine Learning](https://arxiv.org/abs/2601.09938)
*Akitada Sakurai,Aoi Hayashi,Tadayoshi Matumori,Daisuke Kaji,Tadashi Kadowaki,Kae Nemoto*

Main category: quant-ph

TL;DR: 该研究提出了一种将经典数据编码到伊辛哈密顿量中，利用量子退火机演化生成概率分布作为特征映射进行分类的量子机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 量子退火通常被视为组合优化工具，但其相干动力学特性也为机器学习提供了潜力。研究者希望探索如何利用量子退火机的量子动力学特性来构建机器学习模型。

Method: 将经典数据编码到伊辛哈密顿量中，在量子退火机上演化系统，利用生成的概率分布作为特征映射进行分类。引入了参与比作为有效模型规模的度量指标。

Result: 在量子退火机上使用Digits数据集以及MNIST模拟实验表明：较短的退火时间产生更高的分类准确率，而较长的退火时间虽然降低准确率但减少了采样成本。参与比与泛化能力显示出强相关性。

Conclusion: 量子退火机可用于构建机器学习模型，退火时间需要在准确率和采样成本之间权衡。参与比是衡量模型泛化能力的重要指标，为量子机器学习提供了新的见解。

Abstract: Quantum annealing is typically regarded as a tool for combinatorial optimization, but its coherent dynamics also offer potential for machine learning. We present a model that encodes classical data into an Ising Hamiltonian, evolves it on a quantum annealer, and uses the resulting probability distributions as feature maps for classification. Experiments on the quantum annealer machine with the Digits dataset, together with simulations on MNIST, demonstrate that short annealing times yield higher classification accuracy, while longer times reduce accuracy but lower sampling costs. We introduce the participation ratio as a measure of the effective model size and show its strong correlation with generalization.

</details>


### [18] [Three Months in the Life of Cloud Quantum Computing](https://arxiv.org/abs/2601.09943)
*Darrell Teegarden,Allison Casey,F. Gino Serpa,Patrick Becker,Asmita Brahme,Saanvi Kataria,Paul Lopata*

Main category: quant-ph

TL;DR: 该论文对云量子计算编程环境进行了为期三个月的系统性评估，收集了连接指标、算法执行、量子比特数变化影响、与模拟对比、执行时间和成本等元数据，旨在为探索量子计算潜力者提供具体数据和见解。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算从少数定制设备发展到可通过云访问的商业量子计算机，理解访问这些云量子计算机所需的复杂工具链及其权衡至关重要。ARLIS系统研究这些环境以理解其复杂性，为评估量子计算机的能力和实用性提供基础。

Method: 使用现成的设置和工具，在多个机器、云平台和时间上执行单一算法，收集连接指标、算法执行、量子比特数变化影响、与模拟对比、执行时间和成本等元数据，进行为期三个月的详细评估。

Result: 获得了不同服务的连接指标、算法执行数据、量子比特数变化的影响、与模拟的对比、执行时间和成本等具体元数据，提供了从这些精心整理数据中获得的独特见解。

Conclusion: 该工作为探索量子计算潜力者提供了具体的元数据和见解，展示了云量子计算环境的现状和复杂性，有助于理解量子计算的实际应用潜力和限制。

Abstract: Quantum Computing (QC) has evolved from a few custom quantum computers, which were only accessible to their creators, to an array of commercial quantum computers that can be accessed on the cloud by anyone. Accessing these cloud quantum computers requires a complex chain of tools that facilitate connecting, programming, simulating algorithms, estimating resources, submitting quantum computing jobs, retrieving results, and more. Some steps in the chain are hardware dependent and subject to change as both hardware and software tools, such as available gate sets and optimizing compilers, evolve. Understanding the trade-offs inherent in this process is essential for evaluating the power and utility of quantum computers. ARLIS has been systematically investigating these environments to understand these complexities. The work presented here is a detailed summary of three months of using such quantum programming environments. We show metadata obtained from these environments, including the connection metrics to the different services, the execution of algorithms, the testing of the effects of varying the number of qubits, comparisons to simulations, execution times, and cost. Our objective is to provide concrete data and insights for those who are exploring the potential of quantum computing. It is not our objective to present any new algorithms or optimize performance on any particular machine or cloud platform; rather, this work is focused on providing a consistent view of a single algorithm executed using out-of-the-box settings and tools across machines, cloud platforms, and time. We present insights only available from these carefully curated data.

</details>


### [19] [Parallelizing the Variational Quantum Eigensolver: From JIT Compilation to Multi-GPU Scaling](https://arxiv.org/abs/2601.09951)
*Rylan Malarchick,Ashton Steed*

Main category: quant-ph

TL;DR: 本文通过PennyLane框架在HPC集群上实现VQE算法计算H2分子势能面，通过优化器+JIT编译、GPU加速、MPI并行化和多GPU扩展四阶段优化，获得117倍总加速，将运行时间从近10分钟减少到5秒。


<details>
  <summary>Details</summary>
Motivation: 变分量子本征求解器(VQE)是计算分子系统基态能量的混合量子-经典算法，但实际应用中存在计算效率问题。本研究旨在通过高性能计算技术优化VQE实现，实现交互式量子化学探索。

Method: 使用PennyLane量子计算框架在配备4×NVIDIA H100 GPU的HPC集群上实现VQE算法，计算H2分子在100个键长下的势能面。采用四阶段优化策略：1)优化器+JIT编译；2)GPU设备加速；3)MPI并行化；4)多GPU扩展。

Result: 获得117倍总加速(593.95秒→5.04秒)。各阶段加速效果：优化器+JIT编译4.13倍，GPU加速3.60倍(4量子比特)至80.5倍(26量子比特)，MPI并行化28.5倍，多GPU扩展3.98倍(99.4%并行效率)。单个H100 GPU可模拟最多29量子比特。

Conclusion: 通过系统性的高性能计算优化，VQE算法实现了从近10分钟到5秒的显著加速，使交互式量子化学探索成为可能，为大规模量子化学模拟提供了实用解决方案。

Abstract: The Variational Quantum Eigensolver (VQE) is a hybrid quantum-classical algorithm for computing ground state energies of molecular systems. We implement VQE to calculate the potential energy surface of the hydrogen molecule (H$_2$) across 100 bond lengths using the PennyLane quantum computing framework on an HPC cluster featuring 4$\times$ NVIDIA H100 GPUs (80GB each). We present a comprehensive parallelization study with four phases: (1) Optimizer + JIT compilation achieving 4.13$\times$ speedup, (2) GPU device acceleration achieving 3.60$\times$ speedup at 4 qubits scaling to 80.5$\times$ at 26 qubits, (3) MPI parallelization achieving 28.5$\times$ speedup, and (4) Multi-GPU scaling achieving 3.98$\times$ speedup with 99.4% parallel efficiency across 4 H100 GPUs. The combined effect yields 117$\times$ total speedup for the H$_2$ potential energy surface (593.95s $\rightarrow$ 5.04s). We conduct a CPU vs GPU scaling study from 4--26 qubits, finding GPU advantage at all scales with speedups ranging from 10.5$\times$ to 80.5$\times$. Multi-GPU benchmarks demonstrate near-perfect scaling with 99.4% efficiency and establish that a single H100 can simulate up to 29 qubits before hitting memory limits. The optimized implementation reduces runtime from nearly 10 minutes to 5 seconds, enabling interactive quantum chemistry exploration.

</details>


### [20] [Statistical-noise-enhanced multi-photon interference](https://arxiv.org/abs/2601.09977)
*Rikizo Ikuta*

Main category: quant-ph

TL;DR: 三光子干涉中，光子统计特性与干涉可见度存在非单调关系，超泊松统计可最大化可见度，量子与经典优势在干涉中相互排斥


<details>
  <summary>Details</summary>
Motivation: 研究多光子干涉中光子统计特性的作用，特别是三光子干涉与标准双光子干涉（Hong-Ou-Mandel干涉）的差异，探索光子数涨落对干涉可见度的影响

Method: 使用对称电路（特别是离散傅里叶变换电路），通过调制激光器产生工程化的超泊松光子数涨落，调节对称电路参数，分析三光子干涉可见度

Result: 在三光子干涉中，可见度与强度相关函数之间不存在单调关系；超泊松光子数涨落可最大化干涉可见度，甚至超过单光子特征的幅度；通过调节电路参数，可见度层次相对于泊松统计基准发生反转

Conclusion: 量子优势与经典优势在干涉中是相互排斥的资源，表现出一种统计互补性，光子统计特性在多光子干涉中起着关键作用

Abstract: Photon statistics plays a governing role in multi-photon interference. While interference visibility in the standard two-photon case, known as Hong-Ou-Mandel interference, monotonically degrades with higher intensity correlation functions, we show that this monotonicity does not hold for three-photon interference in symmetric circuits. We reveal that, in the discrete Fourier transform circuit, engineered super-Poissonian photon-number fluctuations, realized using a modulated laser, maximize the visibility, surpassing the magnitude of the single-photon signature. In addition, by tuning the symmetric circuit parameters, we demonstrate that the visibility hierarchy inverts relative to the benchmark of Poissonian statistics. This trade-off implies that quantum and classical advantages are mutually exclusive resources for interference, indicating a form of statistical complementarity.

</details>


### [21] [Quantitative approach for the Dicke-Ising chain with an effective self-consistent matter Hamiltonian](https://arxiv.org/abs/2601.10210)
*J. Leibig,M. Hörmann,A. Langheld,A. Schellenberger,K. P. Schmidt*

Main category: quant-ph

TL;DR: Dicke-Ising链在热力学极限下可精确映射为有效自洽物质哈密顿量，光子场仅作为自洽有效场，无需考虑光子与自旋的量子关联即可理解量子相图。采用NLCE+DMRG方法求解该自洽物质哈密顿量，精确确定了相图，包括磁有序相，精度显著优于先前估计。


<details>
  <summary>Details</summary>
Motivation: 研究Dicke-Ising模型的量子相图，传统方法需要处理光子与自旋的复杂量子关联。本文发现热力学极限下光子场可简化为自洽有效场，从而避免处理光子-自旋关联的复杂性，使相图计算更加精确高效。

Method: 将Dicke-Ising链在热力学极限下映射为有效自洽物质哈密顿量，光子场作为自洽有效场。采用数值关联簇展开(NLCE)结合密度矩阵重整化群(DMRG)方法(NLCE+DMRG)求解该自洽物质哈密顿量。

Result: 1. 铁磁Ising耦合：将控制超辐射相变阶数变化的多临界点位置精度提升至相对精度10^{-4}；2. 反铁磁Ising耦合：在热力学极限下确认了窄反铁磁超辐射相的存在，该相被识别为具有纵向场的反铁磁横向场Ising模型的多体基态；3. 反铁磁超辐射相通过连续Dicke型极化子凝聚从反铁磁正常相产生，随后经历一级相变进入顺磁超辐射相。

Conclusion: 通过将Dicke-Ising模型映射为有效自洽物质哈密顿量，并采用NLCE+DMRG方法求解，实现了对一维Dicke-Ising相图的精确确定。该方法避免了处理光子-自旋量子关联的复杂性，为研究类似耦合光-物质系统提供了有效框架。

Abstract: In the thermodynamic limit, the Dicke-Ising chain maps exactly onto an effective self-consistent matter Hamiltonian with the photon field acting solely as a self-consistent effective field. As a consequence, no quantum correlations between photons and spins are needed to understand the quantum phase diagram. This enables us to determine the quantum phase diagram in the thermodynamic limit using numerical linked-cluster expansions combined with density matrix renormalization group calculations (NLCE+DMRG) to solve the resulting self-consistent matter Hamiltonian. This includes magnetically ordered phases with significantly improved accuracy compared to previous estimates. For ferromagnetic Ising couplings, we refine the location of the multicritical point governing the change in the order of the superradiant phase transition, reaching a relative accuracy of $10^{-4}$. For antiferromagnetic Ising couplings, we confirm the existence of the narrow antiferromagnetic superradiant phase in the thermodynamic limit. The effective matter Hamiltonian framework identifies the antiferromagnetic superradiant phase as the many-body ground state of an antiferromagnetic transverse-field Ising model with longitudinal field. This phase emerges through continuous Dicke-type polariton condensation from the antiferromagnetic normal phase, followed by a first-order transition to the paramagnetic superradiant phase. Thus, NLCE+DMRG provides a precise determination of the Dicke-Ising phase diagram in one dimension by solving the self-consistent effective matter Hamiltonian.

</details>


### [22] [Double Markovity for quantum systems](https://arxiv.org/abs/2601.09995)
*Masahito Hayashi,Jinpei Zhao*

Main category: quant-ph

TL;DR: 该论文建立了量子系统中的双重马尔可夫性类比，解决了将SDR技术扩展到量子系统的关键瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 经典信息论中的SDR技术依赖于严格次可加性和其等式情况分析，其中双重马尔可夫性是标准工具。为了将SDR类型论证扩展到量子系统，需要建立量子类比的双重马尔可夫性。

Method: 对于三方态，通过B和C上的兼容投影测量来表征同时满足A-B-C和A-C-B的马尔可夫条件，这些测量诱导出共同的经典标签J，使得A-J-(BC)成立。对于严格正的四方态，证明A-(BD)-C和A-(CD)-B成立当且仅当A-D-(BC)成立。

Result: 建立了量子系统中的双重马尔可夫性类比，为将SDR类型论证扩展到量子系统消除了关键瓶颈。

Conclusion: 该研究成功建立了量子信息论中的双重马尔可夫性类比，为将经典信息论中强大的SDR技术扩展到量子系统铺平了道路。

Abstract: The subadditivity-doubling-rotation (SDR) technique is a powerful route to Gaussian optimality in classical information theory and relies on strict subadditivity and its equality-case analysis, where double Markovity is a standard tool. We establish quantum analogues of double Markovity. For tripartite states, we characterize the simultaneous Markov conditions A-B-C and A-C-B via compatible projective measurements on B and C that induce a common classical label J yielding A-J-(BC). For strictly positive four-party states, we show that A-(BD)-C and A-(CD)-B hold if and only if A-D-(BC) holds. These results remove a key bottleneck in extending SDR-type arguments to quantum systems.

</details>


### [23] [Reentrant topological phases and entanglement scalings in moiré-modulated extended Su-Schrieffer-Heeger Model](https://arxiv.org/abs/2601.09997)
*Guo-Qing Zhang,L. F. Quezada,Shi-Hai Dong*

Main category: quant-ph

TL;DR: 该研究通过分析moiré调制的扩展SSH模型，揭示了由moiré强度驱动的重入相变序列及其普适类不变性，阐明了体边对应关系和纠缠谱特性。


<details>
  <summary>Details</summary>
Motivation: 虽然moiré物理研究为量子相变领域提供了丰富机会，但由moiré强度驱动的重入相变特性尚未被充分理解。本研究旨在探索moiré调制扩展SSH模型中的重入相变序列和普适类不变性。

Method: 对于胞间跃迁w=0的简化情况，通过解析推导哈密顿量参数的重整化关系来解释重入现象；对于一般情况，在热力学极限下计算数值相边界。通过分析零能边缘模和纠缠谱的简并度揭示体边对应关系，并研究从纠缠熵获得的中荷与相变过程中绕数变化之间的对应关系。

Result: 成功解释了重入相变现象，计算了相边界，揭示了零能边缘模与纠缠谱之间的体边对应关系，并建立了中荷与绕数变化之间的对应关系。

Conclusion: 该研究为理解一维凝聚态系统中moiré诱导的重入相变的普适特性和体边对应关系提供了重要见解，推动了moiré物理在量子相变领域的应用。

Abstract: Recent studies of moiré physics have unveiled a wealth of opportunities for significantly advancing the field of quantum phase transitions. However, properties of reentrant phase transitions driven by moiré strength are poorly understood. Here, we investigate the reentrant sequence of phase transitions and the invariant of universality class in moiré-modulated extended Su-Schrieffer-Heeger (SSH) model. For the simplified case with intercell hopping $w=0$, we analytically derive renormalization relations of Hamiltonian parameters to explain the reentrant phenomenon. For the general case, numerical phase boundaries are calculated in the thermodynamic limit. The bulk boundary correspondence between zero-energy edge modes and entanglement spectrum is revealed from the degeneracy of both quantities. We also address the correspondence between the central charge obtained from entanglement entropy and the change in winding number during the phase transition. Our results shed light on the understanding of universal characteristics and bulk-boundary correspondence for moiré induced reentrant phase transitions in 1D condensed-matter systems.

</details>


### [24] [Contextuality Derived from Minimal Decision Dynamics: Quantum Tug-of-War Decision Making](https://arxiv.org/abs/2601.10034)
*Song-Ju Kim*

Main category: quant-ph

TL;DR: 量子概率在决策建模中不仅是便利假设，而是自适应学习动力学的必然有效理论


<details>
  <summary>Details</summary>
Motivation: 决策常表现出挑战经典概率论的上下文依赖性，量子认知虽能建模此类现象，但量子概率是便利假设还是决策动力学的必然结果尚不明确

Method: 开发量子版Tug-of-War模型，基于物理约束（守恒性内部状态更新和测量诱导扰动）展示上下文依赖性如何从决策约束中生成性产生

Result: 上下文依赖性成为自适应学习动力学的结构结果，测量结构允许在最小单系统设置中实现KCBS型上下文性见证，排除了具有统一内部状态的任何非上下文经典描述

Conclusion: 量子概率不是描述性便利，而是自适应决策动力学不可避免的有效理论

Abstract: Decision making often exhibits context dependence that challenges classical probability theory. While quantum cognition has successfully modeled such phenomena, it remains unclear whether quantum probability is merely a convenient assumption or a necessary consequence of decision dynamics. Here we present a theoretical framework in which contextuality arises generatively from physically grounded constraints on decision making. By developing a quantum extension of the Tug-of-War (TOW) model, we show that conservation-based internal state updates and measurement-induced disturbance preclude any non-contextual classical description with a single, unified internal state. Contextuality therefore emerges as a structural consequence of adaptive learning dynamics. We further show that the resulting measurement structure admits Klyachko-Can-Binicioglu-Shumovsky (KCBS)-type contextuality witnesses in a minimal single-system setting. These results indicate that quantum probability is not merely a descriptive convenience, but an unavoidable effective theory for adaptive decision dynamics.

</details>


### [25] [Towards Minimal Fault-tolerant Error-Correction Sequence with Quantum Hamming Codes](https://arxiv.org/abs/2601.10042)
*Sha Shi,Xiao-Yang Xu,Min-Quan Cheng,Dong-Sheng Wang,Yun-Jiang Wang*

Main category: quant-ph

TL;DR: 该论文针对量子汉明码的容错测量序列高开销问题，提出了高效容错测量序列构造方法，将序列长度减少到仅比原始非容错序列多一次测量，同时利用循环矩阵变换和对称性实现硬件高效复用。


<details>
  <summary>Details</summary>
Motivation: 容错测量序列的高开销是实现量子稳定子码的主要挑战，特别是对于量子汉明码这类重要编码，需要设计更高效的容错测量方案来降低时间和硬件开销。

Method: 采用循环矩阵变换方法，系统性地组合初始稳定子矩阵的行，同时保持类似原始量子汉明码的自对偶CSS对称性。利用诱导的对称性实现硬件高效电路复用，前r个稳定子的测量电路通过切换边界Hadamard门即可转换为剩余r个稳定子的电路。

Result: 对于参数为[2^r-1, 2^r-1-2r, 3]且r=3k+1的量子汉明码，成功将容错测量序列长度减少到2r+1，仅比原始非容错序列多一次测量，达到了紧下界。同时通过对称性实现电路复用，显著降低了硬件开销。

Conclusion: 该方法同时减少了容错错误校正的时间开销和硬件开销，为解决量子汉明码最小容错测量序列设计这一重要开放问题提供了重要进展，对其他量子稳定子码的类似挑战也有启发意义。

Abstract: The high overhead of fault-tolerant measurement sequences (FTMSs) poses a major challenge for implementing quantum stabilizer codes. Here, we address this problem by constructing efficient FTMSs for the class of quantum Hamming codes $[\![2^r-1, 2^r-1-2r, 3]\!]$ with $r=3k+1$ ($k \in \mathbb{Z}^+$). Our key result demonstrates that the sequence length can be reduced to exactly $2r+1$-only one additional measurement beyond the original non-fault-tolerant sequence, establishing a tight lower bound. The proposed method leverages cyclic matrix transformations to systematically combine rows of the initial stabilizer matrix and preserving a self-dual CSS-like symmetry analogous to that of the original quantum Hamming codes. This induced symmetry enables hardware-efficient circuit reuse: the measurement circuits for the first $r$ stabilizers are transformed into circuits for the remaining $r$ stabilizers simply by toggling boundary Hadamard gates, eliminating redundant hardware. For distance-3 fault-tolerant error correction, our approach simultaneously reduces the time overhead via shorting the FTMS length and the hardware overhead through symmetry-enabled circuit multiplexing. These results provide an important advance towards the important open problem regarding the design of minimal FTMSs for quantum Hamming codes and may shed light on similar challenges in other quantum stabilizer codes.

</details>


### [26] [Optimal qudit overlapping tomography and optimal measurement order](https://arxiv.org/abs/2601.10059)
*Shuowei Ma,Qianfan Wang,Lvzhou Li,Fei Shi*

Main category: quant-ph

TL;DR: 该论文研究了高维量子比特系统的重叠层析成像，提出了基于广义盖尔曼矩阵的最优测量方案，并开发了减少实验切换成本的优化算法。


<details>
  <summary>Details</summary>
Motivation: 量子态层析对量子系统表征至关重要，但大系统面临指数级资源扩展问题。重叠层析通过重建所有k体边际分布来提取关键信息，但现有最优方案主要针对量子比特，高维量子比特系统的扩展研究不足。

Method: 使用广义盖尔曼矩阵构建局部测量设置，建立与组合覆盖阵列的对应关系，提出两种最优测量方案构造方法。针对n-三态系统，证明了成对层析的测量设置上界，并开发了优化测量顺序的算法以减少切换配置开销。

Result: 对于n-三态系统，证明了成对层析最多需要8 + 56⌈log₈ n⌉个测量设置，并给出了实现该界限的显式方案。优化算法将切换成本相比最坏情况减少了约50%。

Conclusion: 该研究为高效表征高维量子比特系统提供了实用途径，促进了其在量子通信和计算中的应用。

Abstract: Quantum state tomography is essential for characterizing quantum systems, but it becomes infeasible for large systems due to exponential resource scaling. Overlapping tomography addresses this challenge by reconstructing all $k$-body marginals using few measurement settings, enabling the efficient extraction of key information for many quantum tasks. While optimal schemes are known for qubits, the extension to higher-dimensional qudit systems remains largely unexplored. Here, we investigate optimal qudit overlapping tomography, constructing local measurement settings from generalized Gell-Mann matrices. By establishing a correspondence with combinatorial covering arrays, we present two explicit constructions of optimal measurement schemes. For $n$-qutrit systems, we prove that pairwise tomography requires at most $8 + 56\left\lceil \log_{8} n \right\rceil$ measurement settings, and provide an explicit scheme achieving this bound. Furthermore, we develop an efficient algorithm to determine the optimal order of these measurement settings, minimizing the experimental overhead associated with switching configurations. Compared to the worst-case ordering, our optimized schedule reduces switching costs by approximately 50\%. These results provide a practical pathway for efficient characterization of qudit systems, facilitating their application in quantum communication and computation.

</details>


### [27] [Geometric Criteria for Complete Mode Conversion in Detuned Systems via Piecewise-Coherent Modulation](https://arxiv.org/abs/2601.10066)
*Awanish Pandey*

Main category: quant-ph

TL;DR: 提出基于布洛赫球面表述的分段相干调制方法，将耦合模式动力学转化为几何轨迹，实现失谐系统中完全模式转换的精确测地线标准，并应用于无磁光隔离器和多步协议


<details>
  <summary>Details</summary>
Motivation: 静态相位失谐从根本上限制了非对称经典和量子系统中的相干态转移，需要新的理论框架来克服这一限制

Method: 引入布洛赫球面表述的分段相干调制方法，将耦合模式动力学重新表述为几何轨迹，将代数控制转化为路径优化问题

Result: 揭示了目标极点处的不可达锥面，获得了失谐系统中完全模式转换的精确测地线标准；实现了无磁光隔离器的高对比度；开发了递归多步协议，能够处理任意失谐情况

Conclusion: 几何框架为失谐系统中的相干控制提供了新视角，突破了传统限制，在光学隔离和量子信息处理等领域具有应用潜力

Abstract: Static phase detuning fundamentally constrains coherent state transfer in asymmetric classical and quantum systems. We introduce a Bloch-sphere formulation for piecewise-coherent modulation that recasts coupled-mode dynamics as geometric trajectories, transforming algebraic control into path optimization. The approach reveals a cone of inaccessibility at the target pole and yields exact geodesic criteria for complete mode conversion in detuned systems. Leveraging this framework, we break time-reversal symmetry to realize a magnet-free optical isolator with near-unity contrast. Furthermore, for detuning larger than coupling between modes, we develop a recursive multi-step protocol enabling deterministic transfer for arbitrary detunings and derive a universal geometric lower bound on the required number of coupling-switching events.

</details>


### [28] [Pseudomode approach to Fano effect in dissipative cavity quantum electrodynamics](https://arxiv.org/abs/2601.10087)
*Kazuki Kobayashi,Tatsuro Yuge*

Main category: quant-ph

TL;DR: 该研究通过伪模方法重新推导了耗散腔量子电动力学中的法诺效应，建立了统一的描述框架，并阐明了其非马尔可夫起源。


<details>
  <summary>Details</summary>
Motivation: 研究耗散腔量子电动力学中的法诺效应，该效应源于发射体直接辐射与通过腔模介导辐射之间的干涉。旨在建立统一的描述框架并阐明其非马尔可夫起源。

Method: 1. 从耦合到结构化库的两能级系统出发，通过伪模方法引入单个辅助模式重新推导量子主方程；2. 识别系统-环境相互作用的谱函数，证明其由常数项和形成法诺轮廓的非洛伦兹贡献组成；3. 通过法诺对角化在包含显式腔模的公共环境设置中独立推导相同谱函数。

Result: 1. 证明了常数项对于获得Lindblad主方程至关重要，且直接与法诺干涉相关的速率相关；2. 在最强干涉机制下独立推导出相同的谱函数；3. 建立了描述单模腔QED系统中法诺效应的统一框架；4. 阐明了编码在谱函数中的非马尔可夫起源。

Conclusion: 该研究为描述单模腔量子电动力学系统中的法诺效应建立了统一框架，澄清了其非马尔可夫起源，并证明了伪模方法在重新推导量子主方程中的有效性，为理解耗散腔QED中的干涉效应提供了理论基础。

Abstract: We study the Fano effect in dissipative cavity quantum electrodynamics, which originates from the interference between the emitter's direct radiation and that mediated by a cavity mode. Starting from a two-level system coupled to a structured reservoir, we show that a quantum master equation previously derived within the Born-Markov approximation can be rederived by introducing a single auxiliary mode via pseudomode approach. We identify the corresponding spectral function of the system--environment interaction and demonstrate that it consists of a constant and a non-Lorentzian contribution forming the Fano profile. The constant term is shown to be essential for obtaining a Lindblad master equation and is directly related to the rate associated with this Fano interference. Furthermore, by applying Fano diagonalization to a common-environment setup including an explicit cavity mode, we independently derive the same spectral function in the strongest-interference regime. Our results establish a unified framework for describing the Fano effect in single-mode cavity QED systems and clarify its non-Markovian origin encoded in the spectral function.

</details>


### [29] [Classical simulation of a quantum circuit with noisy magic inputs](https://arxiv.org/abs/2601.10111)
*Jiwon Heo,Sojeong Park,Changhun Oh*

Main category: quant-ph

TL;DR: 研究噪声魔法态对量子电路经典可模拟性的影响，建立了噪声依赖的经典采样算法，确定了从量子优势到经典可模拟的转变阈值。


<details>
  <summary>Details</summary>
Motivation: 魔法态是实现通用量子计算的关键资源，被认为是量子优势的主要来源，但在实际设备中不可避免地存在噪声。本研究旨在分析注入的魔法资源上的噪声如何改变量子电路的经典可模拟性，以及何时会引发从经典难解行为到高效经典模拟的转变。

Method: 采用资源中心的噪声模型，其中只有注入的魔法组件是噪声的，而基线状态、操作和测量属于高效可模拟的族。在此设置下，开发了具有可控误差的近似经典采样算法，并证明了算法在多项式时间内运行的显式噪声依赖条件。框架适用于具有Clifford基线的量子比特电路和具有匹配门基线的费米子电路，涵盖了典型的噪声通道如退相干和粒子损失。

Result: 建立了噪声依赖的经典模拟算法，确定了噪声阈值条件，当噪声超过特定阈值时，量子电路变得经典可模拟。通过数值估计模拟成本，提供了实际相关参数范围内的具体阈值和运行时间缩放。

Conclusion: 噪声魔法态会显著影响量子电路的经典可模拟性，存在明确的噪声阈值区分量子优势和经典可模拟性。该研究为理解噪声如何影响量子优势提供了理论框架，并为实际量子设备的性能评估提供了工具。

Abstract: Magic states are essential for universal quantum computation and are widely viewed as a key source of quantum advantage, yet in realistic devices they are inevitably noisy. In this work, we characterize how noise on injected magic resources changes the classical simulability of quantum circuits and when it induces a transition from classically intractable behavior to efficient classical simulation. We adopt a resource-centric noise model in which only the injected magic components are noisy, while the baseline states, operations, and measurements belong to an efficiently simulable family. Within this setting, we develop an approximate classical sampling algorithm with controlled error and prove explicit noise-dependent conditions under which the algorithm runs in polynomial time. Our framework applies to both qubit circuits with Clifford baselines and fermionic circuits with matchgate baselines, covering representative noise channels such as dephasing and particle loss. We complement the analysis with numerical estimates of the simulation cost, providing concrete thresholds and runtime scaling across practically relevant parameter regimes.

</details>


### [30] [Casimir interactions as a probe of broadband optical response](https://arxiv.org/abs/2601.10118)
*Calum F. Shelden,Jeremy N. Munday*

Main category: quant-ph

TL;DR: 通过机器学习反演Lifshitz理论，从单个Casimir力-距离曲线重建材料在七个数量级频率范围内的复介电函数


<details>
  <summary>Details</summary>
Motivation: Casimir力依赖于整个频率谱的介电响应，但传统Lifshitz理论使用虚频率的介电函数，这模糊了其与实频率光学性质的联系，限制了Casimir相互作用作为材料探测工具的应用

Method: 使用监督机器学习反演Lifshitz理论，从单个Casimir力-距离曲线确定材料在七个数量级频率范围内的复介电常数

Result: 不同间距的测量选择性地约束介电响应的不同频率范围，揭示了量子涨落如何采样电磁频谱，成功从力-距离曲线重建了材料的宽带光学响应

Conclusion: Casimir相互作用可作为一种物理约束的宽带光谱工具，为传统技术无法触及的光学表征领域开辟了新机会

Abstract: Casimir forces arise from quantum electromagnetic fluctuations and depend on the dielectric response of interacting materials across the entire frequency spectrum. Although this dependence is central to Lifshitz theory of the Casimir effect, the formulation of the force in terms of dielectric functions evaluated at imaginary frequencies has largely obscured its connection to real-frequency optical properties, limiting the use of Casimir interactions as a probe of materials. Here we demonstrate that Casimir force measurements encode sufficient information to reconstruct a material's broadband optical response. Using supervised machine learning to invert Lifshitz theory, we determine the complex permittivity of a material over more than seven orders of magnitude in frequency from a single force-distance curve. We show that measurements at different separations selectively constrain distinct frequency ranges of the dielectric response, providing direct physical insight into how quantum fluctuations sample the electromagnetic spectrum. These results establish Casimir interactions as a physically constrained, broadband spectroscopic tool and open new opportunities for optical characterization in regimes inaccessible to conventional techniques.

</details>


### [31] [Bridging Superconducting and Neutral-Atom Platforms for Efficient Fault-Tolerant Quantum Architectures](https://arxiv.org/abs/2601.10144)
*Xiang Fang,Jixuan Ruan,Sharanya Prabhu,Ang Li,Travis Humble,Dean Tullsen,Yufei Ding*

Main category: quant-ph

TL;DR: 该论文提出异构量子架构(HQA)，结合超导和中性原子平台优势，通过两种策略实现性能提升：MagicAcc将魔法态工厂卸载到超导设备，Memory-Compute Separation用中性原子存储、超导处理，获得752倍加速和10倍物理量子比特减少。


<details>
  <summary>Details</summary>
Motivation: 同质量子系统在容错时代面临局限，单一量子比特模式无法同时优化操作速度、连接性和可扩展性。需要结合不同量子平台的独特优势来构建更高效的量子计算架构。

Method: 提出异构量子架构(HQA)，结合超导(SC)和中性原子(NA)平台。探索两种架构角色分配策略：1) MagicAcc：将延迟关键的魔法态工厂(MSF)卸载到快速SC设备，在可扩展NA阵列上执行计算；2) 内存-计算分离(MCSep)：利用NA阵列进行高密度qLDPC内存存储，SC设备进行快速表面码处理。

Result: 基于端到端成本模型的评估显示，异构架构带来显著性能提升：设计平均比纯NA基线快752倍，相比纯SC系统减少超过10倍的物理量子比特占用。证明了跨模态互连优化未来容错量子计算机时空效率的可行性。

Conclusion: 异构量子架构通过战略性地结合超导和中性原子平台的互补优势，有效解决了同质系统的局限性，为未来容错量子计算机的时空效率优化提供了清晰的技术路径。

Abstract: The transition to the fault-tolerant era exposes the limitations of homogeneous quantum systems, where no single qubit modality simultaneously offers optimal operation speed, connectivity, and scalability. In this work, we propose a strategic approach to Heterogeneous Quantum Architectures (HQA) that synthesizes the distinct advantages of the superconducting (SC) and neutral atom (NA) platforms. We explore two architectural role assignment strategies based on hardware characteristics: (1) We offload the latency-critical Magic State Factory (MSF) to fast SC devices while performing computation on scalable NA arrays, a design we term MagicAcc, which effectively mitigates the resource-preparation bottleneck. (2) We explore a Memory-Compute Separation (MCSep) paradigm that utilizes NA arrays for high-density qLDPC memory storage and SC devices for fast surface-code processing. Our evaluation, based on a comprehensive end-to-end cost model, demonstrates that principled heterogeneity yields significant performance gains. Specifically, our designs achieve $752\times$ speedup over NA-only baselines on average and reduce the physical qubit footprint by over $10\times$ compared to SC-only systems. These results chart a clear pathway for leveraging cross-modality interconnects to optimize the space-time efficiency of future fault-tolerant quantum computers.

</details>


### [32] [Fluctuation-induced quenching of chaos in quantum optics](https://arxiv.org/abs/2601.10147)
*Mei-Qi Gao,Song-hai Li,Xun Li,Xingli Li,Jiong Cheng,Wenlin Li*

Main category: quant-ph

TL;DR: 该研究探讨了量子光学系统中混沌动力学的抑制机制，发现室温热涨落足以在期望值层面抑制混沌，且非线性增强会降低抑制混沌所需的噪声阈值。


<details>
  <summary>Details</summary>
Motivation: 传统平均场近似假设理想无涨落情况，但混沌对初始条件的高度敏感性意味着微小涨落可能被放大，这质疑了平均场近似在量子光学混沌系统中的适用性。

Method: 使用随机朗之万方程或林德布拉德主方程分析混沌效应，研究频率在10^5到10^7 Hz范围内的系统，考察室温热涨落对混沌的抑制效果。

Result: 1. 室温热涨落足以在期望值层面抑制混沌，即使系统具有弱非线性；2. 非线性导致量子态相空间分布偏离高斯分布，在Wigner函数中显示出类似吸引子的特征；3. 非线性增强会降低抑制混沌所需的噪声阈值，接近真空涨落尺度。

Conclusion: 研究结果为量子力学抑制混沌提供了双向验证：一方面热涨落可以抑制混沌，另一方面非线性增强使系统对涨落更敏感，抑制阈值接近量子极限。

Abstract: Recent studies have extensively explored chaotic dynamics in quantum optical systems through the mean-field approximation, which corresponds to an ideal, fluctuation-free scenario. However, the inherent sensitivity of chaos to initial conditions implies that even minute fluctuations can be amplified, thereby questioning the applicability of this approximation. Here, we analyze these chaotic effects using stochastic Langevin equations or the Lindblad master equation. For systems operating at frequencies of $10^5$ to $10^7$ Hz, we demonstrate that room-temperature thermal fluctuations are sufficient to suppress chaos at the level of expectation values, even under weak nonlinearity. Furthermore, nonlinearity induces deviations from Gaussian phase-space distributions of the quantum state, revealing attractor-like features in the Wigner function. With increasing nonlinearity, the noise threshold for chaos suppression decreases, approaching the scale of vacuum fluctuations. These results provide a bidirectional validation of the quantum mechanical suppression of chaos.

</details>


### [33] [Computing Statistical Properties of Velocity Fields on Current Quantum Hardware](https://arxiv.org/abs/2601.10166)
*Miriam Goldack,Yosi Atia,Ori Alberton,Karl Jansen*

Main category: quant-ph

TL;DR: 该论文提出了一种从参数化量子电路直接提取空间速度场统计特性（如中心矩和结构函数）的方法，避免了完整的量子态层析，并在IBMQ Heron2系统上实现了高精度计算。


<details>
  <summary>Details</summary>
Motivation: 量子算法在计算流体动力学（CFD）中因其良好的扩展性而受到关注，但量子CFD中模拟结果的高效读出是一个关键挑战，现有文献对此关注有限。

Method: 开发了从参数化ansatz电路直接提取空间速度场统计特性的方法，避免了完整的量子态层析。作为概念验证，将16个空间点的1D速度场编码到4个量子比特中，分析了正弦波信号和Burgers方程演化的四个快照。

Result: 使用Qedma的误差缓解软件QESEM，在IBMQ的Heron2系统ibm_fez上实现了高精度计算，证明了该方法在当前量子设备上的可行性。

Conclusion: 该方法为量子CFD中的高效结果读出提供了实用解决方案，避免了计算成本高昂的量子态层析，展示了在当前量子硬件上实现高精度流体动力学计算的潜力。

Abstract: Quantum algorithms are gaining attention in Computational Fluid Dynamics (CFD) for their favorable scaling, as encoding physical fields into quantum probability amplitudes enables representation of two to the power of n spatial points with only n qubits. A key challenge in Quantum CFD is the efficient readout of simulation results, a topic that has received limited attention in literature. This work presents methods to extract statistical properties of spatial velocity fields, such as central moments and structure functions, directly from parameterized ansatz circuits, avoiding full quantum state tomography. As a proof of concept, we implement our approach for 1D velocity fields, encoding 16 spatial points with 4 qubits, and analyze both a sine wave signal and four snapshots from Burgers' equation evolution. Using Qedma's error mitigation software QESEM, we demonstrate that such computations achieve high accuracy on current quantum devices, specifically IBMQ's Heron2 system ibm_fez.

</details>


### [34] [Exponential Analysis for Entanglement Distillation](https://arxiv.org/abs/2601.10190)
*Zhiwen Lin,Ke Li,Kun Fang*

Main category: quant-ph

TL;DR: 本文研究了纠缠蒸馏的可靠性函数，将框架从已知状态扩展到黑盒设置，建立了精确的有限块长结果，并分析了不同自由操作类别下的性能。


<details>
  <summary>Details</summary>
Motivation: 传统纠缠蒸馏研究主要关注可蒸馏纠缠度，且假设初始状态完全已知。本文旨在研究纠缠蒸馏的可靠性函数，即在蒸馏速率低于可蒸馏纠缠度时蒸馏误差衰减的最优指数，并将框架扩展到更具操作意义的黑盒设置。

Method: 将纠缠蒸馏问题与复合相关假设检验联系起来，建立精确的有限块长结果，无需冗余修正项。使用正则化量子Hoeffding散度表征可靠性函数，并针对不同自由操作类别（包括PPT保持、对偶非纠缠、对偶PPT保持操作）进行分析。

Result: 纠缠蒸馏的可靠性函数由正则化量子Hoeffding散度表征。对于纯初始状态，结果简化为Hayashi等人2003年推导的纠缠浓缩误差指数。在完全先验知识下，构建了具体的优化蒸馏协议，并分析了纠缠蒸馏的强逆指数。

Conclusion: 本文建立了纠缠蒸馏可靠性函数的精确理论框架，将标准设置扩展到黑盒场景，为不同自由操作类别下的纠缠蒸馏性能提供了完整的理论表征，统一了已知状态和未知状态下的纠缠蒸馏理论。

Abstract: Historically, the focus in entanglement distillation has predominantly been on the distillable entanglement, and the framework assumes complete knowledge of the initial state. In this paper, we study the reliability function of entanglement distillation, which specifies the optimal exponent of the decay of the distillation error when the distillation rate is below the distillable entanglement. Furthermore, to capture greater operational significance, we extend the framework from the standard setting of known states to a black-box setting, where distillation is performed from a set of possible states. We establish an exact finite blocklength result connecting to composite correlated hypothesis testing without any redundant correction terms. Based on this, the reliability function of entanglement distillation is characterized by the regularized quantum Hoeffding divergence. In the special case of a pure initial state, our result reduces to the error exponent for entanglement concentration derived by Hayashi et al. in 2003. Given full prior knowledge of the state, we construct a concrete optimal distillation protocol. Additionally, we analyze the strong converse exponent of entanglement distillation. While all the above results assume the free operations to be non-entangling, we also investigate other free operation classes, including PPT-preserving, dually non-entangling, and dually PPT-preserving operations.

</details>


### [35] [On the average-case complexity of learning states from the circular and Gaussian ensembles](https://arxiv.org/abs/2601.10197)
*Maxwell West*

Main category: quant-ph

TL;DR: 本文证明了从圆形和高斯系综中均匀采样的量子态的Born分布在统计查询模型中是平均情况难学习的，补充了经典紧致群采样的类似结果


<details>
  <summary>Details</summary>
Motivation: 研究从不同系综采样的量子态复杂性是量子信息理论的核心组成部分，需要补充经典紧致群采样结果的类似研究

Method: 采用非传统的紧致群积分方法，分析AI、AII和DIII型紧致对称空间上的均匀测度诱导的态系综

Result: 建立了圆形和费米子高斯系综中均匀采样态的Born分布在统计查询模型中的平均情况学习硬度

Conclusion: 该结果补充了经典紧致群采样的类似发现，所采用的非传统积分方法具有独立价值，能精确计算Haar随机酉电路和正交电路输出分布与常数分布的总变差距离

Abstract: Studying the complexity of states sampled from various ensembles is a central component of quantum information theory. In this work we establish the average-case hardness of learning, in the statistical query model, the Born distributions of states sampled uniformly from the circular and (fermionic) Gaussian ensembles. These ensembles of states are induced variously by the uniform measures on the compact symmetric spaces of type AI, AII, and DIII. This finding complements analogous recent results for states sampled from the classical compact groups. On the technical side, we employ a somewhat unconventional approach to integrating over the compact groups which may be of some independent interest. For example, our approach allows us to exactly evaluate the total variation distances between the output distributions of Haar random unitary and orthogonal circuits and the constant distribution, which were previously known only approximately.

</details>


### [36] [Topology-Aware Block Coordinate Descent for Qubit Frequency Calibration of Superconducting Quantum Processors](https://arxiv.org/abs/2601.10203)
*Zheng Zhao,Weifeng Zhuang,Yanwu Gu,Peng Qian,Xiao Xiao,Dong E. Liu*

Main category: quant-ph

TL;DR: 该论文为超导量子处理器频率校准中的Snake优化器建立了理论基础，将其形式化为块坐标下降法，并提出基于序列依赖旅行商问题的拓扑感知块排序方法，显著降低了校准时间。


<details>
  <summary>Details</summary>
Motivation: 超导量子处理器预执行校准是一个主要瓶颈，特别是量子比特频率分配因串扰耦合目标而具有挑战性。现有Snake优化器缺乏理论支撑，校准效率有待提高。

Method: 1. 证明Snake优化器数学上等价于块坐标下降法；2. 将块排序问题形式化为序列依赖旅行商问题，用最近邻启发式算法高效求解；3. 在局部串扰/有界度假设下实现线性复杂度；4. 分析带噪声测量的非精确BCD收敛性。

Result: 模拟实验显示BCD-NNA排序在保持相同优化精度的同时，运行时间显著低于基于图的启发式方法（BFS、DFS）和随机排序，对测量噪声具有鲁棒性，并能容忍适度的非局部串扰。

Conclusion: 该研究为频率校准提供了可扩展、可直接实施的解决方案，为NISQ时代处理器的频率校准工作流程奠定了理论基础，显著提高了校准效率。

Abstract: Pre-execution calibration is a major bottleneck for operating superconducting quantum processors, and qubit frequency allocation is especially challenging due to crosstalk-coupled objectives. We establish that the widely-used Snake optimizer is mathematically equivalent to Block Coordinate Descent (BCD), providing a rigorous theoretical foundation for this calibration strategy. Building on this formalization, we present a topology-aware block ordering obtained by casting order selection as a Sequence-Dependent Traveling Salesman Problem (SD-TSP) and solving it efficiently with a nearest-neighbor heuristic. The SD-TSP cost reflects how a given block choice expands the reduced-circuit footprint required to evaluate the block-local objective, enabling orders that minimize per-epoch evaluation time. Under local crosstalk/bounded-degree assumptions, the method achieves linear complexity in qubit count per epoch, while retaining calibration quality. We formalize the calibration objective, clarify when reduced experiments are equivalent or approximate to the full objective, and analyze convergence of the resulting inexact BCD with noisy measurements. Simulations on multi-qubit models show that the proposed BCD-NNA ordering attains the same optimization accuracy at markedly lower runtime than graph-based heuristics (BFS, DFS) and random orders, and is robust to measurement noise and tolerant to moderate non-local crosstalk. These results provide a scalable, implementation-ready workflow for frequency calibration on NISQ-era processors.

</details>


### [37] [Noise-Resilient Quantum Evolution in Open Systems through Error-Correcting Frameworks](https://arxiv.org/abs/2601.10206)
*Nirupam Basak,Goutam Paul,Pritam Chattopadhyay*

Main category: quant-ph

TL;DR: 该研究在微观系统-浴模型中嵌入量子纠错码，分析开放量子系统中的量子态保真度，发现五比特码在低温下能有效抑制退相干，高温下热激发占主导，五比特码性能仍优于Steane码和拓扑码。


<details>
  <summary>Details</summary>
Motivation: 研究动机是建立定量框架来评估量子纠错码在现实噪声环境下的性能，为近期量子技术开发噪声弹性量子架构提供指导。传统研究使用抽象量子通道，而本研究考虑多量子比特寄存器与玻色热环境的实际耦合。

Method: 方法包括：1) 将量子纠错码嵌入微观系统-浴模型；2) 推导多量子比特寄存器与玻色热环境耦合的二阶主方程；3) 在局部和集体噪声下对五比特码、Steane码和拓扑码进行基准测试；4) 计算逻辑量子比特的态保真度随耦合强度、浴温度和纠错周期数的变化。

Result: 主要结果：1) 低温下，五比特码的重复纠错能强烈抑制退相干和弛豫；2) 高温下，热激发主导动力学，所有码的效益降低，但五比特码仍优于Steane码和拓扑码；3) 对于两量子比特Werner态，存在临界演化时间，在此之前纠错不提高保真度，且该时间随纠缠增长而增加；4) 五比特码（最小完美码）在这些开放系统设置中始终提供比拓扑和级联架构更高的保真度。

Conclusion: 结论是建立了评估量子纠错在现实噪声环境下性能的定量框架，为开发噪声弹性量子架构提供了指导。五比特码在开放系统设置中表现出优越性能，特别是在低温条件下能有效保护量子态。

Abstract: We analyze quantum state preservation in open quantum systems using quantum error-correcting (QEC) codes that are explicitly embedded into microscopic system-bath models. Instead of abstract quantum channels, we consider multi-qubit registers coupled to bosonic thermal environments, derive a second-order master equation for the reduced dynamics, and use it to benchmark the five-qubit, Steane, and toric codes under local and collective noise. We compute state fidelities for logical qubits as functions of coupling strength, bath temperature, and the number of correction cycles. In the low-temperature regime, we find that repeated error-correction with the five-qubit code strongly suppresses decoherence and relaxation, while in the high-temperature regime, thermal excitations dominate the dynamics and reduce the benefit of all codes, though the five-qubit code still outperforms the Steane and toric codes. For two-qubit Werner states, we identify a critical evolution time before which QEC does not improve fidelity, and this time increases as entanglement grows. After this critical time, QEC does improve fidelity. Comparative analysis further reveals that the five-qubit code (the smallest perfect code) offers consistently higher fidelities than topological and concatenated architectures in these open-system settings. These findings establish a quantitative framework for evaluating QEC under realistic noise environments and provide guidance for developing noise-resilient quantum architectures in near-term quantum technologies.

</details>


### [38] [Coherence Limits in Interference-Based cos(2$\varphi$) Qubits](https://arxiv.org/abs/2601.10209)
*S. Messelot,A. Leblanc,J. -S. Tettekpoe,F. Lefloch,Q. Ficheux,J. Renard,É. Dumur*

Main category: quant-ph

TL;DR: 研究基于约瑟夫森干涉的cos(2φ)量子比特的相干特性，发现电荷噪声和磁通噪声之间存在基本权衡，限制了此类量子比特的退相干时间


<details>
  <summary>Details</summary>
Motivation: 研究具有宇称保护的cos(2φ)量子比特的相干特性，这类量子比特通过抑制单库珀对隧穿提供宇称保护，但需要评估其实际性能极限

Method: 使用数值模拟分析弛豫和退相干速率对外部磁通和电路参数的依赖关系，识别最佳折衷方案以实现最大相干性

Result: 发现尽管存在宇称保护，但电荷噪声和磁通噪声退相干通道之间存在基本权衡；在现有电路参数下，T1可超过毫秒级，但Tφ仅限微秒级

Conclusion: 建立了此类量子比特相干性的实际限制，对该方法的长期潜力提出了疑问

Abstract: We investigate the coherence properties of parity-protected $\cos(2\varphi)$ qubits based on interferences between two Josephson elements in a superconducting loop. We show that qubit implementations of a $\cos(2\varphi)$ potential using a single loop, such as those employing semiconducting junctions, rhombus circuits, flowermon and KITE structures, can be described by the same Hamiltonian as two multi-harmonic Josephson junctions in a SQUID geometry. We find that, despite the parity protection arising from the suppression of single Cooper pair tunneling, there exists a fundamental trade-off between charge and flux noise dephasing channels. Using numerical simulations, we examine how relaxation and dephasing rates depend on external flux and circuit parameters, and we identify the best compromise for maximum coherence. With currently existing circuit parameters, the qubit lifetime $T_1$ can exceed milliseconds while the dephasing time $T_\varphi$ remains limited to only a few microseconds due to either flux or charge noise. Our findings establish practical limits on the coherence of this class of qubits and raise questions about the long-term potential of this approach.

</details>


### [39] [Optimal control of a dissipative micromaser quantum battery in the ultrastrong coupling regime](https://arxiv.org/abs/2601.10281)
*Maristella Crotti,Luca Razzoli,Luigi Giannelli,Giuseppe A. Falci,Giuliano Benenti*

Main category: quant-ph

TL;DR: 研究超强耦合区域下微脉泽量子电池在环境耗散中的开放系统动力学，通过优化控制和耗散管理实现增强充电性能和长期稳定性


<details>
  <summary>Details</summary>
Motivation: 研究超强耦合区域下量子电池的开放系统动力学，解决反旋转项带来的能量无限增长和高度混合腔态问题，同时利用耗散效应改善电池性能

Method: 采用微脉泽量子电池模型，包含单模电磁腔与流式量子比特通过Rabi哈密顿量顺序相互作用，考虑热浴弱耦合引入耗散效应，实施优化控制和测量反馈策略

Result: 反旋转项显著提高充电速度，但会导致无耗散时能量无限增长；耗散效应缓解这些问题，产生有限能量和功的稳态；优化控制策略能最大化存储功并稳定对抗耗散损失

Conclusion: 超强光-物质耦合、受控耗散和优化控制策略的结合使微脉泽量子电池在现实条件下实现增强的充电性能和长期稳定性

Abstract: We investigate the open system dynamics of a micromaser quantum battery operating in the ultrastrong coupling (USC) regime under environmental dissipation. The battery consists of a single-mode electromagnetic cavity sequentially interacting, via the Rabi Hamiltonian, with a stream of qubits acting as chargers. Dissipative effects arise from the weak coupling of the qubit-cavity system to a thermal bath. Non-negligible in the USC regime, the counter-rotating terms substantially improve the charging speed, but also lead, in the absence of dissipation, to unbounded energy growth and highly mixed cavity states. Dissipation during each qubit-cavity interaction mitigates these detrimental effects, yielding steady-state of finite energy and ergotropy. Optimal control on qubit preparation and interaction times enhances battery's performance in: (i) Maximizing the stored ergotropy trhough an optimized charging protocol; (ii) Stabilizing the stored ergotropy against dissipative losses through an optimized measurement-based passive-feedback strategy. Overall, our numerical results demonstrate that the interplay of ultrastrong light-matter coupling, controlled dissipation, and optimized control strategies enables micromaser quantum batteries to achieve both enhanced charging performance and long-term stability under realistic conditions.

</details>


### [40] [Exponential improvement in benchmarking multiphoton interference](https://arxiv.org/abs/2601.10289)
*Rodrigo M. Sanz,Emilio Annoni,Stephen C. Wein,Carmen G. Almudever,Shane Mansfield,Ellen Derbyshire,Rawad Mezher*

Main category: quant-ph

TL;DR: 提出了一种使用量子傅里叶变换干涉仪来基准测试多光子不可区分性的新协议，相比现有方法实现了指数级改进的样本复杂度


<details>
  <summary>Details</summary>
Motivation: 现有评估多光子不可区分性的协议样本复杂度随光子数指数增长，限制了可扩展性，需要更高效的基准测试方法

Method: 通过新定理强化对可区分性与量子傅里叶变换干涉仪抑制定律关系的理解，提出使用QFT干涉仪的新协议

Result: 新协议在素数光子数下实现常数样本复杂度，其他情况下实现亚多项式缩放，相比现有方法有指数级改进，并在实验中得到验证

Conclusion: 建立了首个可扩展的多光子不可区分性计算方法，适用于当前和近期的光子量子硬件

Abstract: Several photonic quantum technologies rely on the ability to generate multiple indistinguishable photons. Benchmarking the level of indistinguishability of these photons is essential for scalability. The Hong-Ou-Mandel dip provides a benchmark for the indistinguishability between two photons, and extending this test to the multi-photon setting has so far resulted in a protocol that computes the genuine n-photon indistinguishability (GI). However, this protocol has a sample complexity that increases exponentially with the number of input photons for an estimation of GI up to a given additive error. To address this problem, we introduce new theorems that strengthen our understanding of the relationship between distinguishability and the suppression laws of the quantum Fourier transform interferometer (QFT). Building on this, we propose a protocol using the QFT for benchmarking GI that achieves constant sample complexity for the estimation of GI up to a given additive error for prime photon numbers, and sub-polynomial scaling otherwise, representing an exponential improvement over the state of the art. We prove the optimality of our protocol in many relevant scenarios and validate our approach experimentally on Quandela's reconfigurable photonic quantum processor, where we observe a clear advantage in runtime and precision over the state of the art. We therefore establish the first scalable method for computing multi-photon indistinguishability, which applies naturally to current and near-term photonic quantum hardware.

</details>


### [41] [Complex scalar relativistic field as a probability amplitude](https://arxiv.org/abs/2601.10302)
*Yu. M. Poluektov*

Main category: quant-ph

TL;DR: 提出了一种中性复场的相对论方程作为概率幅，获得了概率密度的连续性方程，发现该场存在两种激发类型，描述具有正能量和不同色散关系的粒子。


<details>
  <summary>Details</summary>
Motivation: 为中性复场建立相对论性的概率幅方程，研究其量子力学性质，特别是概率守恒和粒子激发特性。

Method: 提出相对论方程作为概率幅，推导连续性方程，基于拉格朗日形式主义获得守恒定律，考虑二次量子化过渡。

Result: 发现该场存在两种激发类型，描述具有正能量和不同色散关系的粒子，建立了概率守恒定律。

Conclusion: 成功建立了中性复场的相对论概率幅理论框架，揭示了其量子力学性质，为后续量子场论研究奠定了基础。

Abstract: A relativistic equation for a neutral complex field as a probability amplitude is proposed. The continuity equation for the probability density is obtained. It is shown that there are two types of excitations of this field, which describe particles with positive energy and different dispersion laws. Based on the Lagrangian formalism, conservation laws are obtained. The transition to secondary quantization is considered.

</details>


### [42] [Addition to the dynamic Stark shift of the coherent population trapping resonance](https://arxiv.org/abs/2601.10319)
*Gavriil Voloshin,Konstantin Barantsev,Andrey Litvinov*

Main category: quant-ph

TL;DR: 该理论研究光诱导相干布居囚禁共振的频移，提出了考虑额外激发态能级的Λ型原子系统模型，揭示了除传统动态斯塔克频移外，双色激光与失谐原子跃迁相互作用还会产生额外频移。


<details>
  <summary>Details</summary>
Motivation: 研究光诱导相干布居囚禁共振频移的物理机制，特别是双色激光与失谐原子跃迁相互作用对共振线型的影响，这对于提高量子频率标准等精密原子器件的精度至关重要。

Method: 提出包含额外激发态能级的Λ型原子系统解析模型，考虑弱耦合和强耦合两种机制，分析双色激光辐射与失谐原子跃迁的相互作用，推导弱耦合极限下的额外频移解析表达式。

Result: 发现除了传统动态斯塔克频移外，双色激光与失谐原子跃迁相互作用还会导致共振线型畸变产生额外频移；弱耦合下获得该频移的解析表达式，强耦合下该频移与光强呈非线性关系。

Conclusion: 该研究揭示了光诱导相干布居囚禁共振频移的新机制，为量子频率标准等精密原子器件中的光频移控制提供了新思路，特别是在强耦合条件下非线性频移特性可能带来新的应用机会。

Abstract: This paper presents a theoretical study of the light-induced shift of the coherent population trapping resonance. An analytical model is proposed that describes the interaction of two radiation components with an atomic system using a $Λ$ scheme and takes into account an additional level of excited state. Both weak and strong coupling regimes with off-resonant transitions are considered. It is shown that, in addition to the conventional dynamic Stark shift, an extra shift arises due to the distortion of the resonance line shape when bichromatic laser radiation interacts with off-resonant atomic transitions. An analytical expression for this additional shift is derived in the weak-coupling limit, and its significant impact on the resonance shape and sensitivity to the intensities of the laser field components is demonstrated. It is found that under strong coupling conditions, the additional shift can deviate substantially from a linear dependence on light intensity, suggesting new opportunities for controlling light shifts in precision atomic devices such as quantum frequency standards.

</details>


### [43] [Principles of Optics in the Fock Space: Scalable Manipulation of Giant Quantum States](https://arxiv.org/abs/2601.10325)
*Yifang Xu,Yilong Zhou,Ziyue Hua,Lida Sun,Jie Zhou,Weiting Wang,Weizhou Cai,Hongwei Huang,Lintao Xiao,Guangming Xue,Haifeng Yu,Ming Li,Chang-Ling Zou,Luyan Sun*

Main category: quant-ph

TL;DR: 该论文提出了"Fock空间光学"概念框架，将光子数作为合成维度，在量子域中建立了波传播理论，并在超导微波谐振器中实验演示了多达180个光子的光学类比现象。


<details>
  <summary>Details</summary>
Motivation: 经典光学中波光学提供了对空间和时间域中光场的优雅且可扩展的控制，但量子Fock空间中的状态工程主要局限于少光子体系，受限于大希尔伯特空间的计算和实验挑战。

Method: 引入"Fock空间光学"概念框架，将光子数作为合成维度处理；使用超导微波谐振器实验演示Fock空间中的光学类比现象，包括传播、折射、透镜、色散和干涉。

Result: 实验成功演示了多达180个光子的Fock空间光学现象，建立了单玻色子模式中薛定谔演化与经典傍轴波传播之间的基本对应关系。

Conclusion: 通过将直观的光学概念映射到高维量子态工程，这项工作为可扩展控制大规模量子系统（数千光子）和先进玻色子信息处理开辟了道路。

Abstract: The manipulation of distinct degrees of freedom of photons plays a critical role in both classical and quantum information processing. While the principles of wave optics provide elegant and scalable control over classical light in spatial and temporal domains, engineering quantum states in Fock space has been largely restricted to few-photon regimes, hindered by the computational and experimental challenges of large Hilbert spaces. Here, we introduce ``Fock-space optics", establishing a conceptual framework of wave propagation in the quantum domain by treating photon number as a synthetic dimension. Using a superconducting microwave resonator, we experimentally demonstrate Fock-space analogues of optical propagation, refraction, lensing, dispersion, and interference with up to 180 photons. These results establish a fundamental correspondence between Schrödinger evolution in a single bosonic mode and classical paraxial wave propagation. By mapping intuitive optical concepts onto high-dimensional quantum state engineering, our work opens a path toward scalable control of large-scale quantum systems with thousands of photons and advanced bosonic information processing.

</details>


### [44] [Realistic prospects for testing a relativistic local quantum measurement inequality](https://arxiv.org/abs/2601.10354)
*Riccardo Falcone,Claudio Conti*

Main category: quant-ph

TL;DR: 该研究探讨了测试相对论性局域量子测量不等式的实验前景，该不等式量化了有限尺寸探测器对真空不敏感性与对激发响应性之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于建立实际可用的测量界限，将理论不等式与真实光探测场景联系起来，为实验验证提供具体指导。

Method: 基于Reeh-Schlieder近似推导相干态的显式界限，将探测区域建模为在有限时间窗口内工作的方形棱柱，考虑正入射单模相干态，并进行数值计算。

Result: 数值结果显示预期的定性行为：抑制暗计数必然会收紧可实现的点击概率。

Conclusion: 该研究为测试相对论性局域量子测量不等式提供了实验可行的框架，揭示了探测器性能参数之间的基本权衡关系。

Abstract: We investigate the experimental prospects for testing a relativistic local quantum measurement inequality that quantifies the trade-off between vacuum insensitivity and responsiveness to excitations for finite-size detectors. Building on the Reeh--Schlieder approximation for coherent states, we derive an explicit and practically applicable bound for arbitrary coherent states. To connect with realistic photodetection scenarios, we model the detection region as a square prism operating over a finite time window and consider a normally incident single-mode coherent state. Numerical results exhibit the expected qualitative behavior: suppressing dark counts necessarily tightens the achievable click probability.

</details>


### [45] [Learning Hamiltonians in the Heisenberg limit with static single-qubit fields](https://arxiv.org/abs/2601.10380)
*Shrigyan Brahmachari,Shuchen Zhu,Iman Marvian,Yu Tong*

Main category: quant-ph

TL;DR: 提出一种仅使用强度与目标精度无关的静态单量子比特控制场就能实现海森堡极限哈密顿量学习的协议，克服了现有协议对多量子比特操作或高频率/强度单量子比特控制的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有海森堡极限哈密顿量学习协议要么需要易受噪声影响的多量子比特操作，要么需要频率或强度随目标精度增加的单量子比特操作，这限制了在近期量子平台上的应用。

Method: 使用仅需静态场形式的单量子比特控制，其强度与目标精度无关。协议对状态制备和测量（SPAM）误差具有鲁棒性。

Result: 通过严格的数学证明和数值实验证明该方法实现了海森堡极限标度。同时证明了信息论下界，表明除非使用大量离散控制操作，否则非零静态场强度是实现海森堡极限的必要条件。

Conclusion: 该协议克服了现有哈密顿量学习方法的局限性，为设备表征和量子传感提供了新工具，特别适用于近期量子平台。

Abstract: Learning the Hamiltonian governing a quantum system is a central task in quantum metrology, sensing, and device characterization. Existing Heisenberg-limited Hamiltonian learning protocols either require multi-qubit operations that are prone to noise, or single-qubit operations whose frequency or strength increases with the desired precision. These two requirements limit the applicability of Hamiltonian learning on near-term quantum platforms. We present a protocol that learns a quantum Hamiltonian with the optimal Heisenberg-limited scaling using only single-qubit control in the form of static fields with strengths that are independent of the target precision. Our protocol is robust against the state preparation and measurement (SPAM) error. By overcoming these limitations, our protocol provides new tools for device characterization and quantum sensing. We demonstrate that our method achieves the Heisenberg-limited scaling through rigorous mathematical proof and numerical experiments. We also prove an information-theoretic lower bound showing that a non-vanishing static field strength is necessary for achieving the Heisenberg limit unless one employs an extensive number of discrete control operations.

</details>


### [46] [Experimental Realization of Rabi-Driven Reset for Fast Cooling of a High-Q Cavity](https://arxiv.org/abs/2601.10385)
*Eliya Blumenthal,Natan Karaev,Shay Hacohen-Gourgy*

Main category: quant-ph

TL;DR: 该论文提出了一种硬件高效的拉比驱动重置(RDR)方法，通过连续、无测量的冷却机制快速重置超导腔模式，解决了高Q玻色子存储器重置的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 高Q玻色子存储器是硬件高效量子纠错的核心，但其隔离特性使得快速、高保真度重置成为持续瓶颈。现有方法要么依赖弱模间交叉克尔转换，要么依赖具有显著延迟的基于测量的序列。

Method: 提出拉比驱动重置(RDR)方法：在transmon上施加强共振拉比驱动，同时在存储器和读取模式上施加与拉比频率失谐的边带驱动。这将色散相互作用转换为量子比特修饰态与各模式之间的有效Jaynes-Cummings耦合，从而创建从存储器到冷读取浴的可调谐耗散通道。

Result: 实现了单光子1.2μs的衰减时间，比固有寿命快两个数量级以上；在约80μs内将约30个热光子重置到稳态平均光子数$\bar{n} = 0.045 \pm 0.025$。

Conclusion: RDR方法提供了一种硬件高效的连续无测量冷却方案，即使在故意抑制直接模间耦合的弱耦合架构中也能实现快速冷却，为量子信息处理中的存储器重置提供了有效解决方案。

Abstract: High-Q bosonic memories are central to hardware-efficient quantum error correction, but their isolation makes fast, high-fidelity reset a persistent bottleneck. Existing approaches either rely on weak intermode cross-Kerr conversion or on measurement-based sequences with substantial latency. Here we demonstrate a hardware-efficient Rabi-Driven Reset (RDR) that implements continuous, measurement-free cooling of a superconducting cavity mode. A strong resonant Rabi drive on a transmon, together with sideband drives on the memory and readout modes detuned by the Rabi frequency, converts the dispersive interaction into an effective Jaynes-Cummings coupling between the qubit dressed states and each mode. This realizes a tunable dissipation channel from the memory to the cold readout bath. Crucially, the engineered coupling scales with the qubit-mode dispersive interaction and the drive amplitude, rather than with the intermode cross-Kerr, enabling fast cooling even in very weakly coupled architectures that deliberately suppress direct mode-mode coupling. We demonstrate RDR of a single photon with a decay time of $1.2 μs$, more than two orders of magnitude faster than the intrinsic lifetime. Furthermore, we reset about 30 thermal photons in about $80 μs$ to a steady-state average photon number of $\bar{n} = 0.045 \pm 0.025$.

</details>


### [47] [A Collection of Pinsker-type Inequalities for Quantum Divergences](https://arxiv.org/abs/2601.10395)
*Kläre Wienecke,Gereon Koßmann,René Schwonnek*

Main category: quant-ph

TL;DR: 该研究为多种量子/经典散度（包括f-散度、Rényi散度等）建立了与迹距离相关的Pinsker型不等式，并提供了将其推广到平滑散度的策略。


<details>
  <summary>Details</summary>
Motivation: Pinsker不等式为量子态的Umegaki散度与迹距离之间建立了下界关系，但其他类型的散度（如f-散度、Rényi散度等）缺乏类似的估计。本研究旨在为更广泛的散度类型建立类似的边界关系。

Method: 为多种量子和经典散度（包括f-散度如Hellinger散度和χ²散度，以及Rényi散度及其特例如Umegaki散度、碰撞散度、最大散度）建立与迹距离相关的边界估计。同时提出了将这些边界推广到平滑散度的策略。

Result: 成功为多种散度类型建立了类似Pinsker不等式的边界估计，包括f-散度、Rényi散度及其特例，并提供了将这些结果扩展到平滑散度的方法。

Conclusion: 该研究扩展了Pinsker不等式到更广泛的散度类型，为量子信息理论中不同散度度量之间的关系提供了系统的理论框架，并为进一步研究平滑散度的边界估计奠定了基础。

Abstract: Pinsker's inequality sets a lower bound on the Umegaki divergence of two quantum states in terms of their trace distance. In this work, we formulate corresponding estimates for a variety of quantum and classical divergences including $f$-divergences like Hellinger and $χ^2$-divergences as well as Rényi divergences and special cases thereof like the Umegaki divergence, collision divergence, max divergence. We further provide a strategy on how to adapt these bounds to smoothed divergences.

</details>


### [48] [Bounding many-body properties under partial information and finite measurement statistics](https://arxiv.org/abs/2601.10408)
*Luke Mortimer,Leonardo Zambrano,Antonio Acín,Donato Farina*

Main category: quant-ph

TL;DR: 提出一种可扩展的量子多体系统性质边界计算方法，利用矩矩阵松弛技术结合半定规划，能够处理有限测量噪声，适用于基态、对称性和稳态等特定系统


<details>
  <summary>Details</summary>
Motivation: 量子多体系统性质的边界计算对于理解涌现量子现象至关重要，但现有方法在处理大规模系统和有限测量噪声时面临可扩展性挑战

Method: 采用矩矩阵松弛技术结合半定规划方法，将系统特定知识（如哈密顿量基态、对称性、Lindbladian稳态）融入框架，构建可扩展的认证方案

Result: 开发出可扩展的边界计算方法，能够处理有限测量噪声，适用于大规模量子比特系统，为实际实验提供实用的认证方案

Conclusion: 该方法结合半定规划松弛和实验估计，为量子多体系统的性质边界计算提供了可扩展且实用的解决方案，能够有效处理实际实验中的测量噪声

Abstract: Calculating bounds of properties of many-body quantum systems is of paramount importance, since they guide our understanding of emergent quantum phenomena and complement the insights obtained from estimation methods. Recent semidefinite programming approaches enable probabilistic bounds from finite-shot measurements of easily accessible, yet informationally incomplete, observables. Here we render these methods scalable in the number of qubits by instead utilizing moment-matrix relaxations. After introducing the general formalism, we show how the approach can be adapted with specific knowledge of the system, such as it being the ground state of a given Hamiltonian, possessing specific symmetries or being the steady state of a given Lindbladian. Our approach defines a scalable real-world certification scheme leveraging semidefinite programming relaxations and experimental estimations which, unavoidably, contain shot noise.

</details>


### [49] [Tight bounds on recurrence time in closed quantum systems](https://arxiv.org/abs/2601.10409)
*Marcin Kotowski,Michał Oszmaniec*

Main category: quant-ph

TL;DR: 该论文研究了孤立量子系统的量子回归现象，建立了回归时间的上界，并分析了初始态相干性对回归行为的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管量子回归是孤立量子系统演化的基本性质，但对其定量理解一直缺乏。作者旨在建立回归时间的严格上界，并研究初始态相干性对回归行为的影响。

Method: 通过将回归时间与逃离时间联系起来，将逃离时间估计问题转化为逆量子速度极限问题。使用哈密顿量方差来估计逃离时间，并通过随机哈密顿量验证上界的饱和性。

Result: 建立了回归时间上界：t_rec ≲ t_exit(ε)(1/ε)^d，其中d是希尔伯特空间维度。证明了在温和假设下，t_exit(ε) ≈ ε/√Δ(H^2)。该上界对于随机哈密顿量通常是饱和的。

Conclusion: 该研究为量子回归现象提供了严格的定量框架，揭示了回归时间与逃离时间、希尔伯特空间维度以及初始态相干性之间的关系，深化了对孤立量子系统动力学行为的理解。

Abstract: The evolution of an isolated quantum system inevitably exhibits recurrence: the state returns to the vicinity of its initial condition after finite time. Despite its fundamental nature, a rigorous quantitative understanding of recurrence has been lacking. We establish upper bounds on the recurrence time, $t_{\mathrm{rec}} \lesssim t_{\mathrm{exit}}(ε)(1/ε)^d$, where $d$ is the Hilbert-space dimension, $ε$ the neighborhood size, and $t_{\mathrm{exit}}(ε)$ the escape time from this neighborhood. For pure states evolving under a Hamiltonian $H$, estimating $t_{\mathrm{exit}}$ is equivalent to an inverse quantum speed limit problem: finding upper bounds on the time a time-evolved state $ψ_t$ needs to depart from the $ε$-vicinity of the initial state $ψ_0$. We provide a partial solution, showing that under mild assumptions $t_{\mathrm{exit}}(ε) \approx ε/\sqrt{ Δ(H^2)}$, with $Δ(H^2)$ the Hamiltonian variance in $ψ_0$. We show that our upper bound on $t_{\mathrm{rec}}$ is generically saturated for random Hamiltonians. Finally, we analyze the impact of coherence of the initial state in the eigenbasis of $H$ on recurrence behavior.

</details>


### [50] [Unifying Quantum and Classical Dynamics](https://arxiv.org/abs/2601.10423)
*Abdul Rahaman Shaikh,Tabish Qureshi*

Main category: quant-ph

TL;DR: 量子力学与经典力学的动力学方程在形式上完全等价，海森堡方程可转化为与牛顿方程相同的形式，其中普朗克常数ħ不出现。


<details>
  <summary>Details</summary>
Motivation: 虽然量子物理被认为是更基础的理论，经典力学应从量子力学中在特定极限条件下涌现，但这仍是一个挑战。本文旨在探索统一经典和量子物理动力学的可能性。

Method: 将海森堡运动方程重新表述，证明其可以转化为与牛顿运动方程完全相同的形式，其中普朗克常数ħ从方程中消失。

Result: 证明了量子可观测量动力学与其经典对应物之间的精确等价性，量子动力学和经典动力学由相同的方程支配，只是用海森堡算符替代了经典可观测量。

Conclusion: 量子力学和经典力学的动力学在数学形式上是完全等价的，这为理解两者关系提供了新的视角，但并非表明经典行为从量子力学中涌现。

Abstract: Classical and quantum physics represent two distinct theories; however, quantum physics is regarded as the more fundamental of the two. It is posited that classical mechanics should arise from quantum mechanics under certain limiting conditions. Nevertheless, this remains a challenging objective. In this work, we explore the potential for unifying the dynamics of classical and quantum physics. This discussion does not suggest that classical behavior emerges from quantum mechanics; rather, it demonstrates the exact equivalence between the dynamics of quantum observables and their classical counterparts. It is shown that the Heisenberg equations of motion can be cast in a form that is identical to Newton's equations of motion, with $\hbar$ being absent from the formulation. This implies that both quantum and classical dynamics are governed by the same equations, with the Heisenberg operators substituting the classical observables.

</details>


### [51] [Reduction of thermodynamic uncertainty by a virtual qubit](https://arxiv.org/abs/2601.10429)
*Yang Li,Fu-Lin Zhang*

Main category: quant-ph

TL;DR: 该论文分析了虚拟量子比特热机模型中的热力学不确定性关系，发现量子相干性会导致经典TUR的违反，这种违反在共振条件下达到最大，为量子热机优化提供了新见解。


<details>
  <summary>Details</summary>
Motivation: 研究量子热机中热力学不确定性关系的违反现象，揭示量子相干性对热机性能优化的关键作用，为量子技术中的热力学优化提供理论基础。

Method: 分析一类典型的量子热机模型，该模型通过两个能级间的相干耦合形成虚拟量子比特。研究稳态相干性对热力学不确定性关系的影响，将热力学不确定性分解为经典（对角）贡献和相干贡献两部分。

Result: 发现稳态电流和熵产生可由有效经典马尔可夫过程完全再现，但电流涨落获得额外的纯量子修正，源于相干性。相干贡献在共振条件下变为负值，并在最大化稳态相干性的耦合强度处达到最小值。确定了在可逆极限附近超越经典TUR边界的优化条件和判据。

Conclusion: 量子相干性导致经典热力学不确定性关系的违反，这种违反在共振条件下最为显著，为量子热机的性能优化提供了新的物理机制和设计原则。

Abstract: The thermodynamic uncertainty relation (TUR) imposes a fundamental constraint between current fluctuations and entropy production, providing a refined formulation of the second law for micro- and nanoscale systems. Quantum violations of the classical TUR reveal genuinely quantum thermodynamic effects, which are essential for improving performance and enabling optimization in quantum technologies. In this work, we analyze the TUR in a class of paradigmatic quantum thermal-machine models whose operation is enabled by coherent coupling between two energy levels forming a virtual qubit. Steady-state coherences are confined to this virtual-qubit subspace, while in the absence of coherent coupling the system satisfies detailed balance with the thermal reservoirs and supports no steady-state heat currents. We show that the steady-state currents and entropy production can be fully reproduced by an effective classical Markov process, whereas current fluctuations acquire an additional purely quantum correction originating from coherence. As a result, the thermodynamic uncertainty naturally decomposes into a classical (diagonal) contribution and a coherent contribution. The latter becomes negative under resonant conditions and reaches its minimum at the coupling strength that maximizes steady-state coherence. We further identify the optimization conditions and the criteria for surpassing the classical TUR bound in the vicinity of the reversible limit.

</details>


### [52] [Minimal-Energy Optimal Control of Tunable Two-Qubit Gates in Superconducting Platforms Using Continuous Dynamical Decoupling](https://arxiv.org/abs/2601.10446)
*Adonai Hilário da Silva,Octávio da Motta,Leonardo Kleber Castelano,Reginaldo de Jesus Napolitano*

Main category: quant-ph

TL;DR: 提出了一种结合连续动态解耦和变分最小能量最优控制的统一方案，用于生成超导平台中的高保真纠缠门


<details>
  <summary>Details</summary>
Motivation: 为了解决超导量子计算中纠缠门设计面临的残余耦合、校准漂移和准静态噪声等问题，需要开发一种实用且抗噪声的方案来提高门保真度和鲁棒性

Method: 采用连续动态解耦抑制残余耦合、校准漂移和准静态噪声，获得稳定的有效哈密顿量；在稳定的SU(4)流形中，通过变分测地线优化过程计算平滑的低能量单量子比特控制函数，直接最小化门不保真度

Result: 该方法应用于CZ、CX和通用纠缠门，实现了几乎单位保真度，在受限单量子比特操作下具有鲁棒性，且控制场符合实验实际要求

Conclusion: CDD增强的变分几何最优控制方案为设计超导纠缠门提供了一种实用且抗噪声的方法

Abstract: We present a unified scheme for generating high-fidelity entangling gates in superconducting platforms by continuous dynamical decoupling (CDD) combined with variational minimal-energy optimal control. During the CDD stage, we suppress residual couplings, calibration drifting, and quasistatic noise, resulting in a stable effective Hamiltonian that preserves the designed ZZ interaction intended for producing tunable couplers. In this stable $\mathrm{SU}(4)$ manifold, we calculate smooth low-energy single-quibt control functions using a variational geodesic optimization process that directly minimizes gate infidelity. We illustrate the methodology by applying it to CZ, CX, and generic engangling gates, achieving virtually unit fidelity and robustness under restricted single-qubit action, with experimentally realistic control fields. These results establish CDD-enhanced variational geometric optimal control as a practical and noise-resilient scheme for designing superconducting entangling gates.

</details>


### [53] [Localization Landscape in Non-Hermitian and Floquet quantum systems](https://arxiv.org/abs/2601.10451)
*David Guéry-Odelin,François Impens*

Main category: quant-ph

TL;DR: 提出了一种广义的Filoche-Mayboroda局域化景观理论，将其扩展到非静态、非椭圆和非厄米系统，同时保持几何可解释性，能够预测非厄米、Floquet和拓扑系统中的局域化现象。


<details>
  <summary>Details</summary>
Motivation: 传统Filoche-Mayboroda局域化景观理论主要适用于静态、椭圆和厄米系统，需要将其扩展到更广泛的量子系统，包括非厄米、Floquet和拓扑系统，以统一预测各种量子物质中的局域化现象。

Method: 使用正算子H†H构建广义局域化景观，无需计算本征态即可预测局域化。通过奇异值崩溃揭示谱不稳定性与皮肤效应，采用Sambe公式描述相干隧穿破坏，拓扑零模直接从景观中显现。

Result: 在Hatano-Nelson链、驱动两能级系统和驱动Aubry-André-Harper模型中的应用证实了定量准确性，建立了统一预测平衡态和驱动量子物质中局域化的方法。

Conclusion: 提出的广义局域化景观理论成功扩展了传统方法，为预测非厄米、Floquet和拓扑系统中的局域化现象提供了统一框架，无需计算本征态即可获得几何可解释的结果。

Abstract: We propose a generalization of the Filoche--Mayboroda localization landscape that extends the theory well beyond the static, elliptic and Hermitian settings while preserving its geometric interpretability. Using the positive operator $H^\dagger H$, we obtain a landscape that predicts localization across non-Hermitian, Floquet, and topological systems without computing eigenstates. Singular-value collapse reveals spectral instabilities and skin effects, the Sambe formulation captures coherent destruction of tunneling, and topological zero modes emerge directly from the landscape. Applications to Hatano--Nelson chains, driven two-level systems, and driven Aubry--André--Harper models confirm quantitative accuracy, establishing a unified predictor for localization in equilibrium and driven quantum matter.

</details>


### [54] [Erasure conversion for singlet-triplet spin qubits enables high-performance shuttling-based quantum error correction](https://arxiv.org/abs/2601.10461)
*Adam Siegel,Simon Benjamin*

Main category: quant-ph

TL;DR: 该论文提出了一种基于双自旋量子比特（单重态-三重态编码）的容错量子纠错框架，将其作为半导体架构中擦除量子比特的自然实现，通过硬件高效的泄漏检测协议和XZZX表面码，显著提高了纠错阈值和逻辑错误率。


<details>
  <summary>Details</summary>
Motivation: 在半导体量子点器件中，自旋量子比特的快速高保真传输已被证明。基于传输的几种架构被提出，其中单重态-三重态（双自旋）量子比特被认为可能实现最高的传输保真度。然而，需要建立一个容错框架来充分发挥这种量子比特在量子纠错中的潜力。

Method: 1. 建立基于双自旋量子比特的容错量子纠错框架，将其作为半导体架构中的擦除量子比特实现；2. 引入硬件高效的泄漏检测协议，无需测量反馈或增加经典控制开销即可自动将泄漏量子比特投影回计算子空间；3. 结合XZZX表面码和泄漏感知解码技术。

Result: 1. 纠错阈值提高了两倍；2. 逻辑错误率降低了数个数量级；3. 证明了单重态-三重态编码是实现高保真传输和基于擦除的容错量子计算的实用途径。

Conclusion: 单重态-三重态编码为半导体器件中实现高保真传输和基于擦除的容错量子计算提供了实用途径，通过硬件高效的泄漏检测协议和优化的纠错码，显著提升了量子纠错性能。

Abstract: Fast and high fidelity shuttling of spin qubits has been demonstrated in semiconductor quantum dot devices. Several architectures based on shuttling have been proposed; it has been suggested that singlet-triplet (dual-spin) qubits could be optimal for the highest shuttling fidelities. Here we present a fault-tolerant framework for quantum error correction based on such dual-spin qubits, establishing them as a natural realisation of erasure qubits within semiconductor architectures. We introduce a hardware-efficient leakage-detection protocol that automatically projects leaked qubits back onto the computational subspace, without the need for measurement feedback or increased classical control overheads. When combined with the XZZX surface code and leakage-aware decoding, we demonstrate a twofold increase in the error correction threshold and achieve orders-of-magnitude reductions in logical error rates. This establishes the singlet-triplet encoding as a practical route toward high-fidelity shuttling and erasure-based, fault-tolerant quantum computation in semiconductor devices.

</details>


### [55] [Analysis and Experimental Demonstration of Amplitude Amplification for Combinatorial Optimization](https://arxiv.org/abs/2601.10473)
*Daniel Koch,Brian Pardo,Kip Nieman*

Main category: quant-ph

TL;DR: 本文扩展了Grover算法的量子振幅放大技术，将其应用于QUBO等成本函数优化问题，推导出线性成本函数的精确参数设置公式，并通过40量子比特模拟和IBMQ、IonQ实验验证了性能。


<details>
  <summary>Details</summary>
Motivation: 量子振幅放大(QAA)作为Grover算法的推广，能够以高概率找到组合优化问题的最优解。传统Grover算法使用二维正交集体态表示，本研究旨在将其扩展到能够编码QUBO等成本函数的oracle，探索更广泛的优化问题应用。

Method: 将传统的二维Grover表示扩展到编码成本函数的oracle，推导出线性成本函数的精确参数设置公式。使用高达40量子比特的模拟验证算法性能，重点关注接近全局最优解的Grover-like性能。最后在IBMQ（超导）和IonQ（囚禁离子）量子比特上进行实验验证。

Result: 模拟结果显示QAA在所有可能解上都有良好的算法性能，特别是接近全局最优解时表现出类似Grover的性能。实验结果表明，观测到的各基态概率与理论方程预测一致，验证了oracle和扩散算子中自由参数变化的影响。

Conclusion: 成功将量子振幅放大技术扩展到更一般的成本函数优化问题，特别是为线性成本函数提供了精确的参数设置公式。通过大规模模拟和实际量子硬件实验验证了理论框架的有效性，为量子优化算法在实际问题中的应用提供了理论和实验基础。

Abstract: Quantum Amplitude Amplification (QAA), the generalization of Grover's algorithm, is capable of yielding optimal solutions to combinatorial optimization problems with high probabilities. In this work we extend the conventional 2-dimensional representation of Grover's (orthogonal collective states) to oracles which encode cost functions such as QUBO, and show that linear cost functions are a special case whereby an exact formula exists for determining optimal oracle parameter settings. Using simulations of problem sizes up to 40 qubits we demonstrate QAA's algorithmic performance across all possible solutions, with an emphasis on the closeness in Grover-like performance for solutions near the global optimum. We conclude with experimental demonstrations of generalized QAA on both IBMQ (superconducting) and IonQ (trapped ion) qubits, showing that the observed probabilities of each basis state match our equations as a function of varying the free parameters in the oracle and diffusion operators.

</details>


### [56] [H-EFT-VA: An Effective-Field-Theory Variational Ansatz with Provable Barren Plateau Avoidance](https://arxiv.org/abs/2601.10479)
*Eyad I. B Hamid*

Main category: quant-ph

TL;DR: H-EFT-VA是一种受有效场论启发的变分量子算法架构，通过层次化"UV截断"初始化避免贫瘠高原现象，同时保持体积律纠缠和表达能力。


<details>
  <summary>Details</summary>
Motivation: 变分量子算法面临贫瘠高原现象的严重威胁，这会导致梯度指数级衰减，阻碍算法收敛。现有避免贫瘠高原的方法通常通过限制纠缠来实现，但这会牺牲算法的表达能力。

Method: 提出H-EFT-VA架构，受有效场论启发，在初始化时强制执行层次化的"UV截断"，限制电路状态探索范围，防止形成近似酉2-设计。理论上证明了这种局部化能保证梯度方差有逆多项式下界。

Result: 在16个实验中（包括横向场伊辛模型和海森堡XXZ模型）进行广泛基准测试，结果显示：与标准硬件高效拟设相比，能量收敛提升109倍，基态保真度提高10.7倍，统计显著性p < 10^{-88}。

Conclusion: H-EFT-VA成功解决了贫瘠高原问题，同时保持了体积律纠缠和接近Haar纯度的表达能力，为变分量子算法提供了既高效又表达力强的解决方案。

Abstract: Variational Quantum Algorithms (VQAs) are critically threatened by the Barren Plateau (BP) phenomenon. In this work, we introduce the H-EFT Variational Ansatz (H-EFT-VA), an architecture inspired by Effective Field Theory (EFT). By enforcing a hierarchical "UV-cutoff" on initialization, we theoretically restrict the circuit's state exploration, preventing the formation of approximate unitary 2-designs. We provide a rigorous proof that this localization guarantees an inverse-polynomial lower bound on the gradient variance: $Var[\partial θ] \in Ω(1/poly(N))$. Crucially, unlike approaches that avoid BPs by limiting entanglement, we demonstrate that H-EFT-VA maintains volume-law entanglement and near-Haar purity, ensuring sufficient expressibility for complex quantum states. Extensive benchmarking across 16 experiments -- including Transverse Field Ising and Heisenberg XXZ models -- confirms a 109x improvement in energy convergence and a 10.7x increase in ground-state fidelity over standard Hardware-Efficient Ansatze (HEA), with a statistical significance of $p < 10^{-88}$.

</details>


### [57] [Optimized readout strategies for neutral atom quantum processors](https://arxiv.org/abs/2601.10492)
*Liang Chen,Wen-Yi Zhu,Zi-Jie Chen,Zhu-Bo Wang,Ya-Dong Hu,Qing-Xuan Jie,Guang-Can Guo,Chang-Ling Zou*

Main category: quant-ph

TL;DR: 该论文提出了一个理论框架来量化中性原子量子处理器中读取保真度与原子保留率之间的权衡，并引入量子电路迭代率(qCIR)和归一化量子Fisher信息来优化信息获取效率。


<details>
  <summary>Details</summary>
Motivation: 中性原子量子处理器具有高保真度操作和优异量子比特可扩展性，但实现实际应用的关键挑战是在保持高系统吞吐量的同时高效提取读取结果。

Method: 开发理论框架量化读取保真度与原子保留率之间的权衡，引入量子电路迭代率(qCIR)作为度量标准，使用归一化量子Fisher信息表征系统整体性能，通过平衡保真度和保留率优化读取策略。

Result: 对于87Rb原子，使用单光子探测器可实现197.2Hz的qCIR，使用相机可实现154.5Hz的qCIR，这些结果基于实验可行参数。

Conclusion: 该研究为构建可扩展、高吞吐量的中性原子量子处理器提供了实用指导，适用于传感、模拟和近期算法实现等应用。

Abstract: Neutral atom quantum processors have emerged as a promising platform for scalable quantum information processing, offering high-fidelity operations and exceptional qubit scalability. A key challenge in realizing practical applications is efficiently extracting readout outcomes while maintaining high system throughput, i.e., the rate of quantum task executions. In this work, we develop a theoretical framework to quantify the trade-off between readout fidelity and atomic retention. Moreover, we introduce a metric of quantum circuit iteration rate (qCIR) and employ normalized quantum Fisher information to characterize system overall performance. Further, by carefully balancing fidelity and retention, we demonstrate a readout strategy for optimizing information acquisition efficiency. Considering the experimentally feasible parameters for 87Rb atoms, we demonstrate that qCIRs of 197.2Hz and 154.5Hz are achievable using single photon detectors and cameras, respectively. These results provide practical guidance for constructing scalable and high-throughput neutral atom quantum processors for applications in sensing, simulation, and near-term algorithm implementation.

</details>


### [58] [A Mirror-Descent Algorithm for Computing the Petz-Rényi Capacity of Classical-Quantum Channels](https://arxiv.org/abs/2601.10558)
*Yu-Hong Lai,Hao-Chung Cheng*

Main category: quant-ph

TL;DR: 本文提出了一种计算经典-量子信道α-Rényi容量的指数梯度算法，该算法推广了Blahut-Arimoto算法，并证明了在熵几何下的全局次线性收敛和局部线性收敛


<details>
  <summary>Details</summary>
Motivation: 研究经典-量子信道在α∈(0,1)时的α-Rényi容量计算问题，需要开发有效的算法来解决这一重要的量子信息理论问题

Method: 提出基于镜像下降的指数梯度迭代算法，该算法推广了经典的Blahut-Arimoto算法，并在熵几何下分析其收敛性

Result: 证明了算法在熵几何下的相对光滑性，保证了目标值的全局次线性收敛；在满足切空间非退化条件（及某些情况下的谱下界）时，证明了在截断概率单纯形上Kullback-Leibler散度的局部线性收敛

Conclusion: 提出的指数梯度算法能够有效计算经典-量子信道的α-Rényi容量，具有理论保证的收敛性，为量子信息处理提供了新的计算工具

Abstract: We study the computation of the $α$-Rényi capacity of a classical-quantum (c-q) channel for $α\in(0,1)$. We propose an exponentiated-gradient (mirror descent) iteration that generalizes the Blahut-Arimoto algorithm. Our analysis establishes relative smoothness with respect to the entropy geometry, guaranteeing a global sublinear convergence of the objective values. Furthermore, under a natural tangent-space nondegeneracy condition (and a mild spectral lower bound in one regime), we prove local linear (geometric) convergence in Kullback-Leibler divergence on a truncated probability simplex, with an explicit contraction factor once the local curvature constants are bounded.

</details>


### [59] [Deterministic and scalable generation of large Fock states](https://arxiv.org/abs/2601.10559)
*Mo Xiong,Jize Han,Chuanzhen Cao,Jinbin Li,Qi Liu,Zhiguo Huang,Ming Xue*

Main category: quant-ph

TL;DR: 提出一种可扩展的协议，使用混合遗传-Adam优化框架生成光子数约100的大福克态，保真度超过0.9，仅使用原生控制操作和可选的后选择步骤。


<details>
  <summary>Details</summary>
Motivation: 大规模福克态的可扩展确定性制备是量子科学中长期存在的挑战，对量子计量、通信和模拟有直接影响。虽然在小规模实现方面取得了进展，但扩展到大的激发数同时保持高保真度仍然是一个艰巨的挑战。

Method: 采用混合遗传-Adam优化框架，结合遗传算法的全局搜索效率和Adam的自适应收敛性，优化由Jaynes-Cummings相互作用和位移操作组成的多脉冲控制序列，这些操作都是领先实验平台的原生操作。

Result: 实现了光子数约100的大福克态生成，保真度超过0.9，控制协议具有浅电路深度和对参数变化的强鲁棒性，可通过可选的后选择步骤进一步增强。

Conclusion: 建立了一条高效可扩展的途径，用于高保真度非经典态生成，为精密计量和容错量子技术奠定了基础。

Abstract: The scalable and deterministic preparation of large Fock-number states represents a long-standing frontier in quantum science, with direct implications for quantum metrology, communication, and simulation. Despite significant progress in small-scale implementations, extending such state generation to large excitation numbers while maintaining high fidelity remains a formidable challenge. Here, we present a scalable protocol for generating large Fock states with fidelities exceeding 0.9 up to photon numbers on the order of 100, achieved using only native control operations and, when desired, further enhanced by an optional post-selection step. Our method employs a hybrid Genetic-Adam optimization framework that combines the global search efficiency of genetic algorithms with the adaptive convergence of Adam to optimize multi-pulse control sequences comprising Jaynes-Cummings interactions and displacement operations, both of which are native to leading experimental platforms. The resulting control protocols achieve high fidelities with shallow circuit depths and strong robustness against parameter variations. These results establish an efficient and scalable pathway toward high-fidelity non-classical state generation for precision metrology and fault-tolerant quantum technologies.

</details>


### [60] [Searching for Quantum Effects in the Brain: A Bell-Type Test for Nonclassical Latent Representations in Autoencoders](https://arxiv.org/abs/2601.10588)
*I. K. Kominis,C. Xie,S. Li,M. Skotiniotis,G. P. Tsironis*

Main category: quant-ph

TL;DR: 该论文提出了一种模型无关的信息论方法来测试神经信息处理的非经典性，通过潜在空间的贝尔型一致性测试，避免了微观假设，直接探测神经表示的结构。


<details>
  <summary>Details</summary>
Motivation: 神经信息处理是否涉及量子力学元素仍是一个开放问题。传统方法依赖于微观假设，作者希望绕过这些假设，直接测试神经表示本身的结构，为探索神经计算的基本物理开辟新途径。

Method: 使用自编码器作为透明模型系统，在潜在空间中引入贝尔型一致性测试。通过多个读出上下文获得的解码统计，检验是否可以用单一的正潜在变量分布来联合解释。

Result: 该方法将量子特征搜索从微观动力学转移到实验可测试的信息处理约束上，为探测神经计算的基本物理开辟了新途径。

Conclusion: 该工作提出了一种模型无关、信息论的非经典性测试方法，通过潜在空间的贝尔型一致性测试，能够绕过微观假设直接探测神经表示的结构，为研究神经信息处理的量子特征提供了新框架。

Abstract: Whether neural information processing is entirely classical or involves quantum-mechanical elements remains an open question. Here we propose a model-agnostic, information-theoretic test of nonclassicality that bypasses microscopic assumptions and instead probes the structure of neural representations themselves. Using autoencoders as a transparent model system, we introduce a Bell-type consistency test in latent space, and ask whether decoding statistics obtained under multiple readout contexts can be jointly explained by a single positive latent-variable distribution. By shifting the search for quantum-like signatures in neural systems from microscopic dynamics to experimentally testable constraints on information processing, this work opens a new route for probing the fundamental physics of neural computation.

</details>


### [61] [Quantum solver for single-impurity Anderson models with particle-hole symmetry](https://arxiv.org/abs/2601.10594)
*Mariia Karabin,Tanvir Sohail,Dmytro Bykov,Eduardo Antonio Coello Pérez,Swarnava Ghosh,Murali Gopalakrishnan Meena,Seongmin Kim,Amir Shehata,In-Saeng Suh,Hanna Terletska,Markus Eisenbach*

Main category: quant-ph

TL;DR: 开发并基准测试了用于DMFT的量子-经典混合求解器，使用VQE和浅层量子电路求解Anderson杂质模型，在噪声条件下评估性能并比较优化方法


<details>
  <summary>Details</summary>
Motivation: DMFT等量子嵌入方法是研究强关联材料的有力框架，但其核心计算瓶颈在于求解Anderson杂质模型，该模型在经典计算中对于大浴尺寸是难以处理的

Method: 开发了量子-经典混合求解器，使用变分量子本征求解器(VQE)和浅层量子电路制备AIM基态，采用统一拟设框架通过参数偏移电路制备粒子和空穴激发，通过连分式展开重建杂质格林函数

Result: 在噪声、有限测量条件下评估了不同浴尺寸和相互作用强度的性能，比较了三种优化算法(COBYLA、Adam、L-BFGS-B)的收敛性和保真度，评估了量子计算矩校正的益处，并将重建的态密度与经典管道结果进行基准测试

Conclusion: 结果证明了在近期设备上重建格林函数的可行性，并为嵌入自洽DMFT循环中的量子杂质求解器建立了实用基准

Abstract: Quantum embedding methods, such as dynamical mean-field theory (DMFT), provide a powerful framework for investigating strongly correlated materials. A central computational bottleneck in DMFT is in solving the Anderson impurity model (AIM), whose exact solution is classically intractable for large bath sizes. In this work, we develop and benchmark a quantum-classical hybrid solver tailored for DMFT applications, using the variational quantum eigensolver (VQE) to prepare the ground state of the AIM with shallow quantum circuits. The solver uses a unified ansatz framework to prepare the particle and hole excitations of the ground-state from parameter-shifted circuits, enabling the reconstruction of the impurity Green's function through a continued-fraction expansion. We evaluate the performance of this approach across a few bath sizes and interaction strengths under noisy, shot-limited conditions. We compare three optimization routines (COBYLA, Adam, and L-BFGS-B) in terms of convergence and fidelity, assess the benefits of estimating a quantum-computed moment (QCM) correction to the variational energies, and benchmark the approach by comparing the reconstructed density of states (DOS) against that obtained using a classical pipeline. Our results demonstrate the feasibility of Green's function reconstruction on near-term devices and establish practical benchmarks for quantum impurity solvers embedded within self-consistent DMFT loops.

</details>


### [62] [Quantifying the properties of evolutionary quantum states of the XXZ spin model using quantum computing](https://arxiv.org/abs/2601.10650)
*M. P. Tonne,Kh. P. Gnatenko*

Main category: quant-ph

TL;DR: 研究XXZ模型双自旋系统演化量子态的纠缠距离，通过解析和量子计算分析纠缠距离与耦合常数、初始态参数的关系，并研究演化速度的依赖性


<details>
  <summary>Details</summary>
Motivation: 研究量子系统中纠缠距离的演化特性，探索XXZ模型双自旋系统的量子态演化规律，为量子计算和量子信息处理提供理论基础

Method: 结合解析方法和量子计算技术，分析XXZ模型双自旋系统的演化量子态，建立纠缠距离与系统参数的数学关系，并通过量子计算验证理论结果

Result: 获得了纠缠距离与耦合常数、初始态参数的解析依赖关系，明确了演化速度与系统参数的显式关系，量子计算结果与理论预测高度一致

Conclusion: 成功建立了XXZ模型双自旋系统纠缠距离和演化速度的完整理论框架，验证了量子计算在量子系统分析中的有效性，为量子信息处理提供了重要参考

Abstract: The entanglement distance of evolutionary quantum states of a two-spin system with the XXZ model has been studied. The analysis has been conducted both analytically and using quantum computing. An analytical dependence of the entanglement distance on the values of the model coupling constants and the parameters of the initial states has been obtained. The speed of evolution of a two-spin system has been investigated. The analysis has been performed analytically and using quantum computing. An explicit dependence of the speed of evolution on the coupling constants and on the parameters of the initial state has been obtained. The results of quantum computations are in good agreement with the theoretical predictions.

</details>


### [63] [Symmetry-based Perspectives on Hamiltonian Quantum Search Algorithms and Schrodinger's Dynamics between Orthogonal States](https://arxiv.org/abs/2601.10655)
*Carlo Cafaro,James Schneeloch*

Main category: quant-ph

TL;DR: 该论文证明：在由初始态和终态张成的二维子空间中，使用恒定哈密顿量无法突破正交态间的时间最优性限制。时间最优性的偏离只能通过含时哈密顿量或在更高维子空间中的演化实现。


<details>
  <summary>Details</summary>
Motivation: Grover搜索算法的连续时间变体通常由恒定哈密顿量描述，其演化被限制在源态和目标态张成的二维子空间中。当源态和目标态正交时，这种搜索方法失效。本文旨在探究在何种条件下可以突破正交态间的时间最优性限制。

Method: 利用归一化、正交性和能量限制等数学约束，分析在由初始态和终态张成的二维子空间中，使用恒定哈密顿量进行演化时的基本限制。通过定量分析证明时间最优性无法被突破的条件。

Result: 证明在由初始态和终态张成的二维子空间中，使用恒定哈密顿量无法实现正交态间的次优时间演化。时间最优性的偏离只能通过两种方式实现：1) 含时哈密顿量演化；2) 在整个希尔伯特空间更高维子空间中的恒定哈密顿量演化。

Conclusion: 正交态间的时间最优性限制与模拟量子搜索的失效机制密切相关。两者失败的根本原因都在于系统内在的对称性。当源态和目标态正交且不被搜索哈密顿量连接时，模拟量子搜索会失效，这与恒定哈密顿量无法实现次优时间演化的问题本质相同。

Abstract: It is known that the continuous-time variant of Grover's search algorithm is characterized by quantum search frameworks that are governed by stationary Hamiltonians, which result in search trajectories confined to the two-dimensional subspace of the complete Hilbert space formed by the source and target states. Specifically, the search approach is ineffective when the source and target states are orthogonal. In this paper, we employ normalization, orthogonality, and energy limitations to demonstrate that it is unfeasible to breach time-optimality between orthogonal states with constant Hamiltonians when the evolution is limited to the two-dimensional space spanned by the initial and final states. Deviations from time-optimality for unitary evolutions between orthogonal states can only occur with time-dependent Hamiltonian evolutions or, alternatively, with constant Hamiltonian evolutions in higher-dimensional subspaces of the entire Hilbert space. Ultimately, we employ our quantitative analysis to provide meaningful insights regarding the relationship between time-optimal evolutions and analog quantum search methods. We determine that the challenge of transitioning between orthogonal states with a constant Hamiltonian in a sub-optimal time is closely linked to the shortcomings of analog quantum search when the source and target states are orthogonal and not interconnected by the search Hamiltonian. In both scenarios, the fundamental cause of the failure lies in the existence of an inherent symmetry within the system.

</details>


### [64] [Counterdiabatic driving for random-gap Landau-Zener transitions](https://arxiv.org/abs/2601.10659)
*Georgios Theologou,Mikkel F. Andersen,Sandro Wimberger*

Main category: quant-ph

TL;DR: 该研究针对具有能隙分布的Landau-Zener型哈密顿量系综，设计单一控制场H₁来最小化平均跃迁概率，发现瞬时绝热性与最终跃迁概率之间存在系统权衡关系。


<details>
  <summary>Details</summary>
Motivation: 传统Landau-Zener模型描述两能级量子系统的避免交叉过程，在绝热极限下跃迁概率趋于零。虽然可以通过反演工程构造辅助控制场H_CD实现参数无关的绝热性，但本研究旨在为具有能隙分布的LZ型哈密顿量系综设计单一控制场H₁，以统计最优方式最小化平均跃迁概率。

Method: 研究受H_CD启发，关注特殊类别的H₁控制场。通过解析方法处理线性扫描的极限情况（包括具有狄拉克δ函数的LZ系统），并进行全面系统的数值模拟来支持和扩展解析结果。

Result: 研究发现瞬时绝热性与最终跃迁概率之间存在系统权衡关系。某些线性扫描的极限情况可以解析处理，其中一个特例是带有狄拉克δ函数的LZ系统。数值模拟结果支持并扩展了解析结论。

Conclusion: 成功构建了适用于具有能隙分布的LZ型哈密顿量系综的单一控制场H₁，该控制场在统计意义上最优，能够最小化平均跃迁概率，揭示了瞬时绝热性与最终跃迁概率之间的权衡关系。

Abstract: The Landau--Zener (LZ) model describes a two-level quantum system that undergoes an avoided crossing. In the adiabatic limit, the transition probability vanishes. An auxiliary control field $H_\text{CD}$ can be reverse-engineered so that the full Hamiltonian $H_0 + H_\text{CD}$ reproduces adiabaticity for all parameter values. Our aim is to construct a single control field $H_1$ that drives an ensemble of LZ-type Hamiltonians with a distribution of energy gaps. $H_1$ works best statistically, minimizing the average transition probability. We restrict our attention to a special class of $H_1$ controls, motivated by $H_\text{CD}$. We found a systematic trade-off between instantaneous adiabaticity and the final transition probability. Certain limiting cases with a linear sweep can be treated analytically; one of them being the LZ system with Dirac $δ(t)$ function. Comprehensive and systematic numerical simulations support and extend the analytic results.

</details>


### [65] [Geometric Aspects of Entanglement Generating Hamiltonian Evolutions](https://arxiv.org/abs/2601.10662)
*Carlo Cafaro,James Schneeloch*

Main category: quant-ph

TL;DR: 该研究从几何角度分析两量子比特系统从可分离态到最大纠缠态的哈密顿演化，发现时间最优演化具有高测地效率、零曲率、无能量浪费，但平均路径纠缠度低于时间次优演化。对于非正交态间的演化，时间最优演化表现出更强的短时非局域性；而对于正交态间演化则相反。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索从可分离态到最大纠缠态的哈密顿演化过程中，几何特性与纠缠特性之间的关系。特别关注时间最优演化与时间次优演化在几何效率和纠缠产生能力方面的差异。

Method: 采用几何分析方法，通过测地效率、速度效率和曲率系数来表征演化轨迹的几何特性。同时使用并发度、纠缠能力和纠缠功率等度量来量化演化过程中的纠缠特性。比较时间最优和时间次优演化在不同初始条件（正交态与非正交态）下的表现。

Result: 时间最优演化具有高测地效率、零曲率且无能量浪费，但其平均路径纠缠度低于时间次优演化。对于非正交态间的演化，时间最优演化表现出更强的短时非局域性；而对于正交态间演化，时间次优演化反而表现出更强的短时非局域性。这是由于正交态间次优轨迹具有更长路径、更小曲率和更高能量浪费所致。

Conclusion: 研究揭示了量子演化中几何特性与纠缠特性之间的复杂关系。时间最优演化在几何效率上更优，但在纠缠产生方面不一定最优。对于非正交态演化，时间最优演化能更快产生非局域性；而对于正交态演化，则需要更高的初始非局域性才能达到最大纠缠态。

Abstract: We examine the pertinent geometric characteristics of entanglement that arise from stationary Hamiltonian evolutions transitioning from separable to maximally entangled two-qubit quantum states. From a geometric perspective, each evolution is characterized by means of geodesic efficiency, speed efficiency, and curvature coefficient. Conversely, from the standpoint of entanglement, these evolutions are quantified using various metrics, such as concurrence, entanglement power, and entangling capability. Overall, our findings indicate that time-optimal evolution trajectories are marked by high geodesic efficiency, with no energy resource wastage, no curvature (i.e., zero bending), and an average path entanglement that is less than that observed in time-suboptimal evolutions. Additionally, when analyzing separable-to-maximally entangled evolutions between nonorthogonal states, time-optimal evolutions demonstrate a greater short-time degree of nonlocality compared to time-suboptimal evolutions between the same initial and final states. Interestingly, the reverse is generally true for separable-to-maximally entangled evolutions involving orthogonal states. Our investigation suggests that this phenomenon arises because suboptimal trajectories between orthogonal states are characterized by longer path lengths with smaller curvature, which are traversed with a higher energy resource wastage compared to suboptimal trajectories between nonorthogonal states. Consequently, a higher initial degree of nonlocality in the unitary time propagators appears to be essential for achieving the maximally entangled state from a separable state. Furthermore, when assessing optimal and suboptimal evolutions...

</details>


### [66] [Efficiency, Curvature, and Complexity of Quantum Evolutions for Qubits in Nonstationary Magnetic Fields](https://arxiv.org/abs/2601.10672)
*Carlo Cafaro,James Schneeloch*

Main category: quant-ph

TL;DR: 该研究提供了二能级量子系统在时变磁场下量子演化曲率的精确解析表达式，分析了曲率与测地效率、速度效率及演化复杂度的关系，发现高效量子演化通常复杂度较低，但复杂度不仅取决于路径长度，足够弯曲的长路径可能比曲率较低的短路径复杂度更低。


<details>
  <summary>Details</summary>
Motivation: 现实物理场景中的量子演化往往不是理想的，表现出次优效率、非零曲率和高复杂度。为了深入理解量子演化的几何特性，需要精确分析量子演化曲率及其与效率、复杂度的关系。

Method: 针对二能级量子系统在时变磁场下的演化，推导了量子演化曲率的精确解析表达式。研究了由双参数非平稳厄米哈密顿量产生的动力学，该哈密顿量具有单位速度效率。分析了曲率行为与测地效率、速度效率以及量子演化复杂度的关系，其中复杂度定义为从初态到末态演化过程中可访问与已访问Bloch球体积差与可访问体积之比。

Result: 得到了量子演化曲率的精确解析表达式。发现高效量子演化通常具有较低的复杂度，但复杂度不仅取决于路径长度：足够弯曲的长路径可能比曲率较低的短路径具有更低的复杂度。

Conclusion: 量子演化的复杂度不仅由路径长度决定，曲率也起着重要作用。足够弯曲的长路径可能比曲率较低的短路径具有更低的复杂度，这为理解和设计高效量子演化提供了新的几何视角。

Abstract: In optimal quantum-mechanical evolutions, motion can take place along paths of minimal length within an optimal time frame. Alternatively, optimal evolutions may occur along established paths without any waste of energy resources and achieving 100% speed efficiency. Unfortunately, realistic physical scenarios often lead to less-than-ideal evolutions that demonstrate suboptimal efficiency, nonzero curvature, and a high level of complexity. In this paper, we provide an exact analytical expression for the curvature of a quantum evolution pertaining to a two-level quantum system subjected to various time-dependent magnetic fields. Specifically, we examine the dynamics produced by a two-parameter nonstationary Hermitian Hamiltonian with unit speed efficiency. To enhance our understanding of the physical implications of the curvature coefficient, we analyze the curvature behavior in relation to geodesic efficiency, speed efficiency, and the complexity of the quantum evolution (as described by the ratio of the difference between accessible and accessed Bloch-sphere volumes for the evolution from initial to final state to the accessible volume for the given quantum evolution). Our findings indicate that, generally, efficient quantum evolutions exhibit lower complexity compared to inefficient ones. However, we also note that complexity transcends mere length. In fact, longer paths that are sufficiently curved can demonstrate a complexity that is less than that of shorter paths with a lower curvature coefficient.

</details>


### [67] [Optimal lower bound for quantum channel tomography in away-from-boundary regime](https://arxiv.org/abs/2601.10683)
*Kean Chen,Zhicheng Zhang,Nengkun Yu*

Main category: quant-ph

TL;DR: 本文研究了量子信道层析的查询复杂度下界，在远离边界区域(rd₂≥2d₁)证明了最优下界Ω(rd₁d₂/ε²)，与已知上界匹配，完全解决了等输入输出维度情况下的查询复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 量子信道层析是量子信息处理中的基本任务，但现有研究对查询复杂度下界的理解不完整。特别地，在边界区域(rd₂=d₁)和远离边界区域(rd₂≥2d₁)的查询复杂度下界尚未完全确定，需要建立最优下界以匹配已知上界。

Method: 通过分析量子信道的输入维度d₁、输出维度d₂和Kraus秩r的关系，在远离边界区域rd₂≥2d₁的条件下，使用信息论方法证明了查询复杂度下界Ω(rd₁d₂/ε²)。

Result: 证明了量子信道层析在远离边界区域的最优查询下界Ω(rd₁d₂/ε²)，与已知上界O(rd₁d₂/ε²)完全匹配。特别地，对于d₁=d₂=d且r≥2的常见情况，查询复杂度为Θ(rd²/ε²)，与酉信道(r=1)的海森堡标度Θ(d²/ε)形成鲜明对比。

Conclusion: 本文完全解决了量子信道层析在远离边界区域的查询复杂度问题，建立了最优下界Ω(rd₁d₂/ε²)。这一结果揭示了量子信道层析与酉信道层析在查询复杂度上的本质差异，为量子信息处理中的信道表征提供了理论基础。

Abstract: Consider quantum channels with input dimension $d_1$, output dimension $d_2$ and Kraus rank at most $r$. Any such channel must satisfy the constraint $rd_2\geq d_1$, and the parameter regime $rd_2=d_1$ is called the boundary regime. In this paper, we show an optimal query lower bound $Ω(rd_1d_2/\varepsilon^2)$ for quantum channel tomography to within diamond norm error $\varepsilon$ in the away-from-boundary regime $rd_2\geq 2d_1$, matching the existing upper bound $O(rd_1d_2/\varepsilon^2)$. In particular, this lower bound fully settles the query complexity for the commonly studied case of equal input and output dimensions $d_1=d_2=d$ with $r\geq 2$, in sharp contrast to the unitary case $r=1$ where Heisenberg scaling $Θ(d^2/\varepsilon)$ is achievable.

</details>


### [68] [Mitigating nonlinear transduction noise in high-cooperativity cavity optomechanics](https://arxiv.org/abs/2601.10689)
*Daniel Allepuz-Requena,Zohran Ali,Dennis Høj,Yingxuan Chen,Luiz Couto Correa Pinto Filho,Alexander Huck,Ulrik L. Andersen*

Main category: quant-ph

TL;DR: 该研究提出了一种非线性变换方法，能够消除热互调噪声的所有阶次影响，在室温高合作性腔光力学系统中将机械信号信噪比提高近10dB。


<details>
  <summary>Details</summary>
Motivation: 在腔光力学系统中，增加光力学耦合强度会导致光学谐振器的非线性响应，热互调噪声（TIN）会通过非线性混合将测量误差提高到远超标准量子极限的水平，而现有的"魔幻失谐"方法只能消除二阶非线性。

Method: 研究团队在室温下操作膜在中间微腔系统，实现高合作性（C>n_th），记录系统输出，并应用非线性变换来消除所有阶次的热互调噪声。

Result: 通过非线性变换成功移除了所有阶次的TIN，将机械信号的信噪比提高了近10dB。该方法特别适用于受三阶TIN影响的实验，而三阶TIN被认为是高合作性室温腔光力学系统中的主要固有噪声源。

Conclusion: 该研究提出的非线性变换方法能够有效消除所有阶次的热互调噪声，显著提高测量精度，为高合作性室温腔光力学系统的噪声抑制提供了新方案。

Abstract: Coupling mechanical motion to an optical resonator enables displacement measurements approaching the standard quantum limit (SQL). However, increasing the optomechanical coupling strength will inevitably lead to probing of the nonlinear response of the optical resonator. Thermal intermodulation noise (TIN) arising from the nonlinear mixing of thermomechanical motion can further increase the imprecision well above the SQL and has hitherto been canceled up to second order of nonlinearity via operation at the "magic detuning". In this work, we record the output of a membrane-in-the-middle microcavity system operating at room temperature and achieving high cooperativity, $C>n_\text{th}$, and apply a nonlinear transform that removes all orders of TIN, improving the mechanical signal-to-noise ratio by nearly 10 dB. Our results can be applied to experiments affected by third-order TIN, which we expect to be the dominating intrinsic source of noise in high-cooperativity room-temperature cavity optomechanical systems.

</details>


### [69] [Constant-Depth Unitary Preparation of Dicke States](https://arxiv.org/abs/2601.10693)
*Francisca Vasconcelos,Malvika Raj Joshi*

Main category: quant-ph

TL;DR: 首次提出在恒定深度下精确制备Dicke态的酉协议，突破了传统电路模型的对数深度限制，利用全局相互作用实现


<details>
  <summary>Details</summary>
Motivation: Dicke态在量子计量、通信和计算中是关键资源，但传统酉制备方法在标准电路模型中受限于对数深度，现有恒定深度协议需要测量和前馈操作

Method: 超越标准电路模型，利用中性原子和囚禁离子等架构固有的全局相互作用，使用无界CZ门（QAC^0电路类）和量子FAN-OUT操作（QAC_f^0电路类）

Result: 实现了恒定深度精确制备恒定权重Dicke态（使用多项式辅助比特）和近似制备权重-1 Dicke态（W态，仅需常数辅助比特），在QAC_f^0类中可精确制备任意权重Dicke态

Conclusion: 这些协议区分了基于连接性的量子架构的恒定深度能力，为解决长期存在的量子复杂性猜想提供了新路径

Abstract: Dicke states serve as a critical resource in quantum metrology, communication, and computation. However, unitary preparation of Dicke states is limited to logarithmic depth in standard circuit models and existing constant-depth protocols require measurement and feed-forward. In this work, we present the first unitary, constant-depth protocols for exact Dicke state preparation. We overcome the logarithmic-depth barrier by moving beyond the standard circuit model and leveraging global interactions (native to architectures such as neutral atoms and trapped ions). Specifically, utilizing unbounded CZ gates (i.e. within the QAC$^0$ circuit class), we offer circuits for exact computation of constant-weight Dicke states, using polynomial ancillae, and approximation of weight-1 Dicke states (i.e. $W$ states), using only constant ancillae. Granted additional access to the quantum FAN-OUT operation (i.e. upgrading to the QAC$_f^0$ circuit class), we also achieve exact preparation of arbitrary-weight Dicke states, with polynomial ancillae. These protocols distinguish the constant-depth capabilities of quantum architectures based on connectivity and offer a novel path toward resolving a long-standing quantum complexity conjecture.

</details>


### [70] [Madelung hydrodynamics of spin-orbit coupling: action principles, currents, and correlations](https://arxiv.org/abs/2601.10698)
*Cesare Tronci*

Main category: quant-ph

TL;DR: 该论文利用量子流体动力学的变分和哈密顿结构，揭示了自旋轨道耦合在电子运动中的相关性和转矩机制，分离了SOC诱导的量子力，并阐明了自旋输运特征。


<details>
  <summary>Details</summary>
Motivation: 研究自旋轨道耦合在量子流体动力学中的作用机制，特别是SOC诱导的量子力对轨道运动的影响，以及这些力如何产生量子自旋-轨道相关性。

Method: 使用泡利方程的哈密顿作用原理，分离SOC诱导的量子力；利用量子流体动力学的哈密顿结构分析自旋输运特征；通过Madelung-Rashba方程说明平面SOC配置，并提出基于粒子的数值实现方案。

Result: 识别出SOC诱导的轨道力来源于特定的电流算符，该算符对自旋电流有重要贡献但过去被忽视；揭示了产生量子自旋-轨道相关性的两种不同机制；阐明了自旋霍尔效应中的电流偏移和相关诱导的量子转矩等自旋输运特征。

Conclusion: 该研究通过量子流体动力学框架深入理解了自旋轨道耦合的物理机制，区分了不同的量子力项，为自旋输运现象提供了新的理论解释，并提出了可行的数值实现方案。

Abstract: We exploit the variational and Hamiltonian structures of quantum hydrodynamics with spin to unfold the correlation and torque mechanisms accompanying spin-orbit coupling (SOC) in electronic motion. Using Hamilton's action principle for the Pauli equation, we isolate SOC-induced quantum forces that act on the orbital Madelung--Bohm trajectories and complement the usual force terms known to appear in quantum hydrodynamics with spin. While the latter spin-hydrodynamic forces relate to the quantum geometric tensor (QGT), SOC-induced orbital forces originate from a particular current operator that contributes prominently to the spin current and whose contribution was overlooked in the past. The distinction between different force terms reveals two fundamentally different mechanisms generating quantum spin-orbit correlations. Leveraging the Hamiltonian structure of the hydrodynamic system, we also elucidate spin transport features such as the current shift in the spin Hall effect and the correlation-induced quantum torques. Finally, we illustrate the framework via the Madelung--Rashba equations for planar SOC configurations and propose a particle-based scheme for numerical implementation.

</details>


### [71] [Scalable Spin Squeezing in Power-Law Interacting XXZ Models with Disorder](https://arxiv.org/abs/2601.10703)
*Samuel E. Begg,Bishal K. Ghosh,Chong Zu,Chuanwei Zhang,Michael Kolodrubetz*

Main category: quant-ph

TL;DR: 该论文研究了二维晶格中自旋压缩在存在空位缺陷时的鲁棒性，发现在幂律相互作用XXZ模型中，自旋压缩在达到特定无序阈值前是可扩展的，并解释了先前NV中心实验中观察到的压缩能力受限现象。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明，自旋压缩在具有幂律相互作用的系统中可以实现，但在金刚石氮空位中心实验中观察到位置无序严重影响了自旋压缩能力，使其无法随系统规模扩展。这促使研究者探索自旋压缩在存在晶格缺陷情况下的鲁棒性。

Method: 采用半经典建模方法，研究二维晶格中具有幂律相互作用的XXZ模型，分析存在一定比例空位晶格点时的自旋压缩行为。通过系统模拟和理论分析，建立了可扩展自旋压缩的相图。

Result: 研究发现在达到特定无序阈值前，自旋压缩在幂律相互作用XXZ模型中是可扩展的；超过该阈值后，压缩不再具有可扩展性。研究还解释了先前NV中心实验中观察到的压缩能力受限现象，并确定了实现可扩展自旋压缩的最大允许无序程度。

Conclusion: 该工作揭示了在多种量子模拟器中实现可扩展自旋压缩的最大允许无序程度，发现了一个对无序具有显著容忍度的区域，并指出受控缺陷创建是实现固态系统中可扩展自旋压缩的有前景途径。

Abstract: While spin squeezing has been traditionally considered in all-to-all interacting models, recent works have shown that spin squeezing can occur in systems with power-law interactions, leading to direct testing in Rydberg atoms, trapped ions, ultracold atoms and nitrogen vacancy (NV) centers in diamond. For the latter, Wu. et al. Nature 646 (2025) demonstrated that spin squeezing is heavily affected by positional disorder, reducing any capacity for a practical squeezing advantage, which requires scalability with the system size. In this Letter we explore the robustness of spin-squeezing in two-dimensional lattices with a fraction of unoccupied lattice sites. Using semi-classical modeling, we demonstrate the existence of scalable squeezing in power-law interacting XXZ models up to a disorder threshold, above which squeezing is not scalable. We produce a phase diagram for scalable squeezing, and explain its absence in the aforementioned NV experiment. Our work illustrates the maximum disorder allowed for realizing scalable spin squeezing in a host of quantum simulators, highlights a regime with substantial tolerance to disorder, and identifies controlled defect creation as a promising route for scalable squeezing in solid-state systems.

</details>


### [72] [Quantum Maxwell Erasure Decoder for qLDPC codes](https://arxiv.org/abs/2601.10713)
*Bruno Costa Alves Freire,François-Marie Le Régent,Anthony Leverrier*

Main category: quant-ph

TL;DR: 提出量子麦克斯韦擦除解码器，用于CSS量子低密度奇偶校验码，通过有界猜测扩展了剥离解码，实现复杂度与性能的可调权衡


<details>
  <summary>Details</summary>
Motivation: 传统量子LDPC码解码器在性能和复杂度之间存在权衡，需要一种既能接近最大似然性能又能保持线性时间复杂度的解码方法

Method: 扩展剥离解码算法，引入有界猜测机制，通过符号化跟踪猜测并利用限制性检查消除猜测，通过猜测预算参数控制复杂度与性能的权衡

Result: 无约束预算时恢复最大似然性能，常数预算时实现线性时间解码并近似最大似然性能，在双变量自行车码和量子Tanner码上表现出色

Conclusion: 量子麦克斯韦擦除解码器为CSS量子LDPC码提供了一种灵活的解码框架，通过猜测预算参数在解码性能和计算复杂度之间实现可调权衡

Abstract: We introduce a quantum Maxwell erasure decoder for CSS quantum low-density parity-check (qLDPC) codes that extends peeling with bounded guessing. Guesses are tracked symbolically and can be eliminated by restrictive checks, giving a tunable tradeoff between complexity and performance via a guessing budget: an unconstrained budget recovers Maximum-Likelihood (ML) performance, while a constant budget yields linear-time decoding and approximates ML. We provide theoretical guarantees on asymptotic performance and demonstrate strong performance on bivariate bicycle and quantum Tanner codes.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [73] [Emergent Nonperturbative Universal Floquet Localization](https://arxiv.org/abs/2601.09793)
*Soumadip Pakrashi,Atanu Rajak,Sambuddha Sanyal*

Main category: cond-mat.dis-nn

TL;DR: 在准周期驱动晶格中发现了非微扰局域化平台，该平台独立于静态局域化特性和驱动协议，通过特定调谐的振幅频率比实现所有Floquet态局域化。


<details>
  <summary>Details</summary>
Motivation: 研究周期性驱动准周期晶格中的局域化现象，探索在存在密集共振的情况下能否实现非微扰局域化，以及这种局域化与静态系统特性的关系。

Method: 使用精确Floquet动力学、Floquet微扰理论和最优阶van Vleck分析，识别特定调谐的振幅频率比，研究van Vleck展开的超渐近精度及其在弱准周期势下的失效机制。

Result: 发现了一个稳健的非微扰局域化平台，在特定调谐的振幅频率比下，所有Floquet态都变得局域化，van Vleck展开在最优阶之前达到超渐近精度，最终因共振杂化而失效。

Conclusion: 周期性驱动准周期晶格中存在非微扰局域化现象，这种局域化独立于静态局域化特性和驱动协议，van Vleck展开的失效揭示了局域化的非微扰本质。

Abstract: We show that a robust, nonperturbative localization plateau emerges in periodically driven quasiperiodic lattices, independent of the static localization properties and drive protocol. Using exact Floquet dynamics, Floquet perturbation theory, and optimal-order van Vleck analysis, we identify a fine-tuned amplitude-to-frequency ratio where all Floquet states become localized despite dense resonances. The van Vleck expansion achieves superasymptotic accuracy up to an optimal orde; it ultimately breaks down due to resonant hybridization at a weak quasiperiodic potential, revealing that the observed localization is nonperturbative.

</details>


### [74] [Integral Variable Range Hopping for Modeling Electrical Transport in Disordered Systems](https://arxiv.org/abs/2601.10226)
*Chenxin Qin,Chenyan Wang,Mouyang Cheng,Ji Chen*

Main category: cond-mat.dis-nn

TL;DR: 提出积分变程跳跃(IVRH)模型，通过积分公式取代传统VRH模型中的经验温度幂律关系，提供更稳定、物理意义更明确的框架来描述无序系统中的电荷传输。


<details>
  <summary>Details</summary>
Motivation: 传统变程跳跃(VRH)模型基于过度简化的假设，适用范围有限且拟合行为存在问题，但其过度使用的情况日益严重，需要更稳健的物理模型。

Method: 提出积分变程跳跃(IVRH)模型，用基于物理的积分公式取代传统VRH中的经验温度幂律关系。模型基于标准跳跃概率ω(R)和反映系统几何影响的有效体积函数V(R)，通过积分公式自然重现低温下的Mott行为和高温下的Arrhenius行为。

Result: IVRH模型成功应用于二维、三维和多层系统。蒙特卡洛模拟验证了模型预测，拟合参数方差显著低于传统VRH模型。在单层MoS₂和WS₂系统的输运测量中也表现出更好的鲁棒性，提供更物理意义的解释。

Conclusion: IVRH模型为低维非晶材料中的跳跃输运提供了更稳定、物理意义更明确的解释框架，为理解无序系统中电荷传输的普适几何标度因子提供了更深入的见解。

Abstract: The variable range hopping (VRH) model has been widely applied to describe electrical transport in disordered systems, providing theoretical formulas to fit temperature-dependent electric conductivity. These models rely on oversimplified assumptions that restrict their applicability and result in problematic fitting behaviors, yet their overusing situation is becoming increasingly serious. In this work we formulate an integral variable range hopping (IVRH) model, which replaces the empirical temperature power-law dependence in standard VRH theories with a physics-inspired integral formulation. The model builds upon the standard hopping probability $ω(R)$ w.r.t. hopping distance $R$ and incorporates the density of accessible electronic states through an effective volume function $V(R)$, which reflects the influence of system geometry. The IVRH formulation inherently reproduces both the Mott behavior at low temperatures and the Arrhenius behavior at high temperatures, respectively, and enables a smooth transition between the two regimes. We apply the IVRH model to two-dimensional, three-dimensional, and multi-layered systems. Monte Carlo simulations validate the model's predictions and yield consistent values for the fitting parameters, with substantially reduced variances compared to fitting using the standard VRH model. Furthermore, the improved robustness of IVRH also extends to the transport measurements in monolayer MoS$_2$ system and monolayer WS$_2$ system, enabling more physically meaningful interpretation.IVRH model offers a more stable and physically sound framework for interpreting hopping transport in low-dimensional amorphous materials, providing deeper insights into the universal geometric scaling factors that govern charge transport in disordered systems.

</details>


### [75] [The eigenvalues and eigenvectors of finite-rank normal perturbations of large rotationally invariant non-Hermitian matrices](https://arxiv.org/abs/2601.10427)
*Pierre Bousseyroux,Marc Potters*

Main category: cond-mat.dis-nn

TL;DR: 该论文研究了旋转对称非厄米随机矩阵的有限秩正规形变，扩展了经典的BBP框架，描述了异常特征值的出现、波动及相应特征向量的行为。


<details>
  <summary>Details</summary>
Motivation: 研究非厄米随机矩阵在有限秩正规扰动下的特征值行为，扩展经典BBP框架到非厄米情形，为理解异常特征值现象提供统一理论框架。

Method: 研究形式为$\mathbf{A} + \mathbf{T}$的模型，其中$\mathbf{A}$是大型旋转对称非厄米随机矩阵，$\mathbf{T}$是有限秩正规扰动，分析异常特征值的出现和波动。

Result: 建立了统一框架描述异常特征值的出现和波动，同时描述了相应特征向量的行为，该框架同时涵盖厄米和非厄米情形，推广了多个已知案例。

Conclusion: 成功扩展了BBP框架到非厄米随机矩阵，为有限秩正规扰动下的特征值行为提供了统一的理论描述，连接了厄米和非厄米随机矩阵理论。

Abstract: We study finite-rank normal deformations of rotationally invariant non-Hermitian random matrices. Extending the classical Baik-Ben Arous-Péché (BBP) framework, we characterize the emergence and fluctuations of outlier eigenvalues in models of the form $\mathbf{A} + \mathbf{T}$, where $\mathbf{A}$ is a large rotationally invariant non-Hermitian random matrix and $\mathbf{T}$ is a finite-rank normal perturbation. We also describe the corresponding eigenvector behavior. Our results provide a unified framework encompassing both Hermitian and non-Hermitian settings, thereby generalizing several known cases.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [76] [Social Determinants of Health Prediction for ICD-9 Code with Reasoning Models](https://arxiv.org/abs/2601.09709)
*Sharim Khan,Paul Landes,Adam Cross,Jimeng Sun*

Main category: cs.LG

TL;DR: 该研究探索使用推理模型和传统大语言模型在MIMIC-III数据集上进行医院入院多标签社会健康决定因素ICD-9代码分类，取得了89%的F1分数，并发现了139个入院记录中缺失的SDoH代码。


<details>
  <summary>Details</summary>
Motivation: 社会健康决定因素与患者预后相关，但很少在结构化数据中捕获。虽然大语言模型在从句子中识别SDoH标签方面表现良好，但在长入院记录或纵向笔记中预测存在挑战，因为存在长距离依赖关系。

Method: 使用推理模型和传统大语言模型在MIMIC-III数据集上进行医院入院多标签社会健康决定因素ICD-9代码分类，利用现有的ICD-9代码进行入院预测。

Result: 在入院预测中取得了89%的F1分数，发现了139个入院记录中缺失的SDoH代码，并提供了可复现结果的代码。

Conclusion: 该研究表明使用推理模型和大语言模型可以有效从临床文本中提取社会健康决定因素信息，补充诊断系统对患者社会环境的了解，为改善医疗决策提供了有价值的方法。

Abstract: Social Determinants of Health correlate with patient outcomes but are rarely captured in structured data. Recent attention has been given to automatically extracting these markers from clinical text to supplement diagnostic systems with knowledge of patients' social circumstances. Large language models demonstrate strong performance in identifying Social Determinants of Health labels from sentences. However, prediction in large admissions or longitudinal notes is challenging given long distance dependencies. In this paper, we explore hospital admission multi-label Social Determinants of Health ICD-9 code classification on the MIMIC-III dataset using reasoning models and traditional large language models. We exploit existing ICD-9 codes for prediction on admissions, which achieved an 89% F1. Our contributions include our findings, missing SDoH codes in 139 admissions, and code to reproduce the results.

</details>


### [77] [TimeSAE: Sparse Decoding for Faithful Explanations of Black-Box Time Series Models](https://arxiv.org/abs/2601.09776)
*Khalid Oublal,Quentin Bouniot,Qi Gan,Stephan Clémençon,Zeynep Akata*

Main category: cs.LG

TL;DR: TimeSAE：一个基于稀疏自编码器和因果关系的框架，用于解释时间序列黑盒模型的预测，相比现有方法在分布偏移下更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 随着黑盒模型和预训练模型在时间序列应用中的普及，理解其预测变得至关重要，特别是在需要可解释性和信任的高风险领域。现有方法大多只涉及分布内解释，无法泛化到训练支持之外，缺乏泛化能力。

Method: 基于稀疏自编码器概念，提出TimeSAE框架，通过稀疏自编码器和因果关系的双重视角来解释时间序列黑盒模型。该方法旨在提供更鲁棒的模型解释。

Result: 在合成和真实世界时间序列数据集上的广泛评估表明，TimeSAE相比领先基线方法提供了更忠实和鲁棒的解释，定量指标和定性分析均支持这一结论。

Conclusion: TimeSAE框架能够有效解释时间序列黑盒模型，在分布偏移下表现出更好的鲁棒性，为解决模型可解释性问题提供了新思路。

Abstract: As black box models and pretrained models gain traction in time series applications, understanding and explaining their predictions becomes increasingly vital, especially in high-stakes domains where interpretability and trust are essential. However, most of the existing methods involve only in-distribution explanation, and do not generalize outside the training support, which requires the learning capability of generalization. In this work, we aim to provide a framework to explain black-box models for time series data through the dual lenses of Sparse Autoencoders (SAEs) and causality. We show that many current explanation methods are sensitive to distributional shifts, limiting their effectiveness in real-world scenarios. Building on the concept of Sparse Autoencoder, we introduce TimeSAE, a framework for black-box model explanation. We conduct extensive evaluations of TimeSAE on both synthetic and real-world time series datasets, comparing it to leading baselines. The results, supported by both quantitative metrics and qualitative insights, show that TimeSAE provides more faithful and robust explanations. Our code is available in an easy-to-use library TimeSAE-Lib: https://anonymous.4open.science/w/TimeSAE-571D/.

</details>


### [78] [On the origin of neural scaling laws: from random graphs to natural language](https://arxiv.org/abs/2601.10684)
*Maissam Barkeshli,Alberto Alfarano,Andrey Gromov*

Main category: cs.LG

TL;DR: 该研究通过简化实验（如图上的随机游走和简化语言模型）探索神经缩放定律的起源，发现即使在没有数据幂律结构的情况下也会出现缩放定律，并提供了对传统语言建模缩放定律的批判性分析。


<details>
  <summary>Details</summary>
Motivation: 研究神经缩放定律的起源，特别是探索它们是否必须依赖于数据中的幂律结构。通过简化设置来系统研究缩放行为，以更好地理解现代AI中缩放定律的基本机制。

Method: 1. 在具有可调复杂度的图上训练transformer预测随机游走（bigrams）；2. 通过从简化生成语言模型（4层、2层、1层transformer语言模型到语言bigrams）采样序列来系统降低自然语言复杂性；3. 在Erdös-Renyi和Barabási-Albert随机图集合上训练随机游走；4. 使用2层transformer和50上下文长度重现传统语言建模缩放定律。

Result: 1. 即使在数据相关性中没有幂律结构的情况下，简化设置也会产生神经缩放定律；2. 缩放指数随着语言模型复杂性的降低而单调演化；3. 在随机图上获得缩放定律；4. 使用极小模型重现了传统缩放定律的关键结果；5. 提供了对先前文献中各种拟合的批判性分析；6. 展示了获取计算最优曲线的替代方法；7. 初步证据表明最大更新参数化可能比标准参数化更参数高效。

Conclusion: 神经缩放定律可以在没有数据幂律结构的情况下出现，表明缩放行为可能源于更基本的机制。简化实验设置有助于更好地理解缩放定律的本质，并为更高效的模型参数化方法提供了初步证据。

Abstract: Scaling laws have played a major role in the modern AI revolution, providing practitioners predictive power over how the model performance will improve with increasing data, compute, and number of model parameters. This has spurred an intense interest in the origin of neural scaling laws, with a common suggestion being that they arise from power law structure already present in the data. In this paper we study scaling laws for transformers trained to predict random walks (bigrams) on graphs with tunable complexity. We demonstrate that this simplified setting already gives rise to neural scaling laws even in the absence of power law structure in the data correlations. We further consider dialing down the complexity of natural language systematically, by training on sequences sampled from increasingly simplified generative language models, from 4,2,1-layer transformer language models down to language bigrams, revealing a monotonic evolution of the scaling exponents. Our results also include scaling laws obtained from training on random walks on random graphs drawn from Erdös-Renyi and scale-free Barabási-Albert ensembles. Finally, we revisit conventional scaling laws for language modeling, demonstrating that several essential results can be reproduced using 2 layer transformers with context length of 50, provide a critical analysis of various fits used in prior literature, demonstrate an alternative method for obtaining compute optimal curves as compared with current practice in published literature, and provide preliminary evidence that maximal update parameterization may be more parameter efficient than standard parameterization.

</details>


### [79] [Eluder dimension: localise it!](https://arxiv.org/abs/2601.09825)
*Alireza Bakhtiari,Alex Ayoub,Samuel Robertson,David Janz,Csaba Szepesvári*

Main category: cs.LG

TL;DR: 论文建立了广义线性模型类的eluder维度下界，表明基于标准eluder维度的分析无法获得一阶遗憾界。为此引入了eluder维度的局部化方法，改进了伯努利多臂赌博机经典结果，并首次为有限时域强化学习任务提供了真正的一阶遗憾界。


<details>
  <summary>Details</summary>
Motivation: 标准eluder维度分析无法获得一阶遗憾界，这限制了其在更复杂任务中的应用，特别是在有限时域强化学习中缺乏真正的一阶遗憾界。

Method: 引入了eluder维度的局部化方法，通过局部化处理来改进分析框架，使其能够处理更复杂的模型类。

Result: 1) 建立了广义线性模型类eluder维度的下界；2) 恢复了伯努利多臂赌博机的经典结果并有所改进；3) 首次为有限时域强化学习任务提供了真正的一阶遗憾界。

Conclusion: 通过eluder维度的局部化方法，突破了标准分析的限制，为复杂学习任务提供了一阶遗憾界分析框架，具有重要的理论和应用价值。

Abstract: We establish a lower bound on the eluder dimension of generalised linear model classes, showing that standard eluder dimension-based analysis cannot lead to first-order regret bounds. To address this, we introduce a localisation method for the eluder dimension; our analysis immediately recovers and improves on classic results for Bernoulli bandits, and allows for the first genuine first-order bounds for finite-horizon reinforcement learning tasks with bounded cumulative returns.

</details>


### [80] [A New Convergence Analysis of Plug-and-Play Proximal Gradient Descent Under Prior Mismatch](https://arxiv.org/abs/2601.09831)
*Guixian Xu,Jinglai Li,Junqi Tang*

Main category: cs.LG

TL;DR: 该论文为plug-and-play近端梯度下降(PnP-PGD)在先验失配情况下的收敛性提供了新理论，这是首个在先验失配下的PnP-PGD收敛证明


<details>
  <summary>Details</summary>
Motivation: 现有PnP算法的理论结果需要多个限制性且无法验证的假设，且缺乏在先验失配（去噪器训练数据分布与推理任务不同）情况下的收敛性证明

Method: 提出了plug-and-play近端梯度下降(PnP-PGD)在去噪器训练数据分布与推理任务分布不同情况下的新收敛理论

Result: 这是首个PnP-PGD在先验失配下的收敛证明，移除了现有理论中多个限制性且无法验证的假设

Conclusion: 该工作为PnP-PGD在先验失配情况下的收敛性提供了理论基础，显著推进了PnP算法的理论分析框架

Abstract: In this work, we provide a new convergence theory for plug-and-play proximal gradient descent (PnP-PGD) under prior mismatch where the denoiser is trained on a different data distribution to the inference task at hand. To the best of our knowledge, this is the first convergence proof of PnP-PGD under prior mismatch. Compared with the existing theoretical results for PnP algorithms, our new results removed the need for several restrictive and unverifiable assumptions.

</details>


### [81] [A pipeline for enabling path-specific causal fairness in observational health data](https://arxiv.org/abs/2601.09841)
*Aparajita Kashyap,Sara Matijevic,Noémie Elhadad,Steven A. Kushner,Shalmali Joshi*

Main category: cs.LG

TL;DR: 该研究提出了一种用于训练因果公平机器学习模型的通用流程，特别针对医疗保健领域，通过路径特定因果公平性来解决直接和间接的医疗偏见问题。


<details>
  <summary>Details</summary>
Motivation: 在医疗保健环境中部署机器学习模型时，需要确保模型不会复制或加剧现有的医疗偏见。虽然存在许多公平性定义，但研究关注路径特定因果公平性，以更好地考虑偏见发生的社会和医疗背景。

Method: 将结构公平模型映射到观察性医疗保健设置中，创建一个通用的训练因果公平模型的流程。该流程明确考虑特定的医疗背景和差异来定义目标"公平"模型，并利用无公平约束的基础模型生成因果公平的下游预测。

Result: 研究填补了两个主要空白：扩展了对"公平性-准确性"权衡的表征，通过解耦直接和间接偏见来源；展示了如何在已知社会和医疗差异的任务中利用观察性健康数据训练的无公平约束基础模型生成因果公平的下游预测。

Conclusion: 该工作提出了一个模型无关的流程，用于训练因果公平的机器学习模型，解决医疗保健中的直接和间接偏见形式，为医疗AI的公平部署提供了实用框架。

Abstract: When training machine learning (ML) models for potential deployment in a healthcare setting, it is essential to ensure that they do not replicate or exacerbate existing healthcare biases. Although many definitions of fairness exist, we focus on path-specific causal fairness, which allows us to better consider the social and medical contexts in which biases occur (e.g., direct discrimination by a clinician or model versus bias due to differential access to the healthcare system) and to characterize how these biases may appear in learned models. In this work, we map the structural fairness model to the observational healthcare setting and create a generalizable pipeline for training causally fair models. The pipeline explicitly considers specific healthcare context and disparities to define a target "fair" model. Our work fills two major gaps: first, we expand on characterizations of the "fairness-accuracy" tradeoff by detangling direct and indirect sources of bias and jointly presenting these fairness considerations alongside considerations of accuracy in the context of broadly known biases. Second, we demonstrate how a foundation model trained without fairness constraints on observational health data can be leveraged to generate causally fair downstream predictions in tasks with known social and medical disparities. This work presents a model-agnostic pipeline for training causally fair machine learning models that address both direct and indirect forms of healthcare bias.

</details>


### [82] [Advancing Model Refinement: Muon-Optimized Distillation and Quantization for LLM Deployment](https://arxiv.org/abs/2601.09865)
*Jacob Sander,Brian Jalaian,Venkat R. Dasari*

Main category: cs.LG

TL;DR: 提出一个集成框架，结合GPTQ量化、LoRA微调和数据蒸馏，显著减少LLM模型大小和复杂度，同时保持或提升任务特定性能，实现最高2倍内存压缩。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在资源受限的边缘设备上部署面临计算、内存和能耗挑战，需要解决任务特定数据获取、性能微调和模型压缩三个关键问题。

Method: 集成框架包含：1) GPTQ量化减少模型大小；2) 低秩适应(LoRA)微调；3) 专门的数据蒸馏过程，结合知识蒸馏（KL散度）、贝叶斯超参数优化和Muon优化器。

Result: 实现最高2倍内存压缩（如6GB模型压缩至3GB），在标准LLM基准测试中表现优于单独使用GPTQ量化，Muon优化器显著增强了微调模型在量化过程中的抗精度衰减能力。

Conclusion: 该集成框架有效解决了LLM在边缘设备部署的资源约束问题，通过量化、微调和数据蒸馏的协同作用，在保持性能的同时显著减少模型复杂度和资源需求。

Abstract: Large Language Models (LLMs) enable advanced natural language processing but face deployment challenges on resource-constrained edge devices due to high computational, memory, and energy demands. Optimizing these models requires addressing three key challenges: acquiring task-specific data, fine-tuning for performance, and compressing models to accelerate inference while reducing resource demands. We propose an integrated framework combining GPTQ-based quantization, low-rank adaptation (LoRA), and a specialized data distillation process to significantly reduce model size and complexity while preserving or enhancing task-specific performance. By leveraging data distillation, knowledge distillation via Kullback-Leibler divergence, Bayesian hyperparameter optimization, and the Muon optimizer, our pipeline achieves up to 2x memory compression (e.g., reducing a 6GB model to 3GB) and enables efficient inference for specialized tasks. Empirical results demonstrate superior performance on standard LLM benchmarks compared to GPTQ quantization alone, with the Muon optimizer notably enhancing fine-tuned models' resistance to accuracy decay during quantization.

</details>


### [83] [Interpolation-Based Optimization for Enforcing lp-Norm Metric Differential Privacy in Continuous and Fine-Grained Domains](https://arxiv.org/abs/2601.09946)
*Chenxi Qiu*

Main category: cs.LG

TL;DR: 提出基于插值的框架优化lp范数度量差分隐私，通过稀疏锚点优化扰动分布，使用对数凸组合插值非锚点分布，在细粒度或连续域中保持隐私保证并提升效用。


<details>
  <summary>Details</summary>
Motivation: 现有基于优化的方法在粗粒度域中有效减少效用损失，但在细粒度或连续设置中优化度量差分隐私仍然具有挑战性，因为构建密集扰动矩阵和满足逐点约束的计算成本很高。

Method: 提出插值框架：1) 在稀疏锚点集上优化扰动分布；2) 通过对数凸组合插值非锚点位置的分布；3) 在高维空间中，将插值过程分解为一系列一维步骤，推导修正公式确保lp范数度量差分隐私；4) 探索扰动分布和隐私预算跨维度分配的联合优化。

Result: 在真实世界位置数据集上的实验表明，该方法在细粒度域中提供严格的隐私保证和具有竞争力的效用，优于基线机制。

Conclusion: 提出的插值框架有效解决了细粒度或连续域中优化度量差分隐私的计算挑战，通过稀疏锚点优化和插值方法在保持隐私保证的同时提升了效用。

Abstract: Metric Differential Privacy (mDP) generalizes Local Differential Privacy (LDP) by adapting privacy guarantees based on pairwise distances, enabling context-aware protection and improved utility. While existing optimization-based methods reduce utility loss effectively in coarse-grained domains, optimizing mDP in fine-grained or continuous settings remains challenging due to the computational cost of constructing dense perterubation matrices and satisfying pointwise constraints.
  In this paper, we propose an interpolation-based framework for optimizing lp-norm mDP in such domains. Our approach optimizes perturbation distributions at a sparse set of anchor points and interpolates distributions at non-anchor locations via log-convex combinations, which provably preserve mDP. To address privacy violations caused by naive interpolation in high-dimensional spaces, we decompose the interpolation process into a sequence of one-dimensional steps and derive a corrected formulation that enforces lp-norm mDP by design. We further explore joint optimization over perturbation distributions and privacy budget allocation across dimensions. Experiments on real-world location datasets demonstrate that our method offers rigorous privacy guarantees and competitive utility in fine-grained domains, outperforming baseline mechanisms. in high-dimensional spaces, we decompose the interpolation process into a sequence of one-dimensional steps and derive a corrected formulation that enforces lp-norm mDP by design. We further explore joint optimization over perturbation distributions and privacy budget allocation across dimensions. Experiments on real-world location datasets demonstrate that our method offers rigorous privacy guarantees and competitive utility in fine-grained domains, outperforming baseline mechanisms.

</details>


### [84] [Kinematic Tokenization: Optimization-Based Continuous-Time Tokens for Learnable Decision Policies in Noisy Time Series](https://arxiv.org/abs/2601.09949)
*Griffin Kearney*

Main category: cs.LG

TL;DR: 论文提出了一种基于优化的连续时间表示方法——运动学标记化，用于处理噪声采样下的连续信号，特别是在金融时间序列数据中，该方法相比离散标记化方法能更好地维持稳定的决策策略。


<details>
  <summary>Details</summary>
Motivation: Transformer通常设计用于离散标记，但现实世界中的许多信号是通过噪声采样观测到的连续过程。在低信噪比环境下，离散标记化方法（原始值、补丁、有限差分）可能变得脆弱，特别是当下游目标施加不对称惩罚时，理性上会鼓励弃权。

Method: 提出了运动学标记化方法，这是一种基于优化的连续时间表示，从噪声测量中重建显式样条，并将局部样条系数（位置、速度、加速度、急动度）作为标记。该方法应用于金融时间序列数据，包括资产价格和交易量分布。

Result: 在多资产日度股票测试平台上，使用风险厌恶不对称分类目标作为可学习性的压力测试。在该目标下，多个离散基线方法崩溃为吸收现金策略（清算均衡），而连续样条标记能够维持校准的非平凡行动分布和稳定策略。

Conclusion: 结果表明，显式连续时间标记可以改善在弃权诱导损失下噪声时间序列中选择性决策策略的可学习性和校准性。

Abstract: Transformers are designed for discrete tokens, yet many real-world signals are continuous processes observed through noisy sampling. Discrete tokenizations (raw values, patches, finite differences) can be brittle in low signal-to-noise regimes, especially when downstream objectives impose asymmetric penalties that rationally encourage abstention. We introduce Kinematic Tokenization, an optimization-based continuous-time representation that reconstructs an explicit spline from noisy measurements and tokenizes local spline coefficients (position, velocity, acceleration, jerk). This is applied to financial time series data in the form of asset prices in conjunction with trading volume profiles. Across a multi-asset daily-equity testbed, we use a risk-averse asymmetric classification objective as a stress test for learnability. Under this objective, several discrete baselines collapse to an absorbing cash policy (the Liquidation Equilibrium), whereas the continuous spline tokens sustain calibrated, non-trivial action distributions and stable policies. These results suggest that explicit continuous-time tokens can improve the learnability and calibration of selective decision policies in noisy time series under abstention-inducing losses.

</details>


### [85] [A Sustainable AI Economy Needs Data Deals That Work for Generators](https://arxiv.org/abs/2601.09966)
*Ruoxi Jia,Luis Oala,Wenjie Xiong,Suqin Ge,Jiachen T. Wang,Feiyang Kang,Dawn Song*

Main category: cs.LG

TL;DR: 机器学习价值链存在结构性不可持续问题，数据生成者在价值分配中获益极少，大部分价值被聚合者获取，这威胁到当前学习算法的可持续性。


<details>
  <summary>Details</summary>
Motivation: 机器学习价值链存在经济数据处理不平等问题：从输入到模型权重再到合成输出的每个阶段都提炼了技术信号，但剥夺了数据生成者的经济权益。通过分析73个公开数据交易发现，大部分价值流向聚合者，创作者版税几乎为零，交易条款普遍不透明。这不仅影响经济福利，随着数据及其衍生物成为经济资产，维持当前学习算法的反馈循环面临风险。

Method: 分析了73个公开数据交易，识别了三个结构性缺陷：缺失来源追溯、不对称议价能力和非动态定价。沿着机器学习价值链追踪这些问题，提出了公平数据价值交换（EDVEX）框架，旨在建立一个使所有参与者受益的最小市场。

Result: 研究发现大多数价值流向聚合者，文档记录的创作者版税几乎为零，交易条款普遍不透明。这些不平等现象威胁到机器学习系统的可持续性。

Conclusion: 机器学习价值链存在结构性不平等问题，需要建立公平的数据价值交换框架。提出了EDVEX框架作为解决方案，并概述了研究社区可以做出具体贡献的研究方向，以改善数据交易并实现更公平的价值分配。

Abstract: We argue that the machine learning value chain is structurally unsustainable due to an economic data processing inequality: each state in the data cycle from inputs to model weights to synthetic outputs refines technical signal but strips economic equity from data generators. We show, by analyzing seventy-three public data deals, that the majority of value accrues to aggregators, with documented creator royalties rounding to zero and widespread opacity of deal terms. This is not just an economic welfare concern: as data and its derivatives become economic assets, the feedback loop that sustains current learning algorithms is at risk. We identify three structural faults - missing provenance, asymmetric bargaining power, and non-dynamic pricing - as the operational machinery of this inequality. In our analysis, we trace these problems along the machine learning value chain and propose an Equitable Data-Value Exchange (EDVEX) Framework to enable a minimal market that benefits all participants. Finally, we outline research directions where our community can make concrete contributions to data deals and contextualize our position with related and orthogonal viewpoints.

</details>


### [86] [An Exploratory Study to Repurpose LLMs to a Unified Architecture for Time Series Classification](https://arxiv.org/abs/2601.09971)
*Hansen He,Shuheng Li*

Main category: cs.LG

TL;DR: 该研究探索了将专门的时间序列编码器与冻结的大型语言模型（LLM）主干结合的混合架构，发现Inception模型是唯一能持续带来性能提升的编码器架构。


<details>
  <summary>Details</summary>
Motivation: 时间序列分类（TSC）是机器学习核心问题，现有研究主要关注将时间序列数据映射到文本域的校准策略，但时间序列编码器架构的选择尚未得到充分探索。

Method: 采用混合架构方法，将专门的时间序列编码器（包括Inception、卷积、残差、基于Transformer和多层感知机等架构）与冻结的LLM主干结合，进行探索性研究。

Result: 在评估的多种编码器架构中，只有Inception模型在与LLM主干集成时能持续产生积极的性能提升，其他架构表现不一致或没有明显优势。

Conclusion: 时间序列编码器的选择对混合LLM架构有重要影响，基于Inception的模型是未来LLM驱动时间序列学习的有前景方向。

Abstract: Time series classification (TSC) is a core machine learning problem with broad applications. Recently there has been growing interest in repurposing large language models (LLMs) for TSC, motivated by their strong reasoning and generalization ability. Prior work has primarily focused on alignment strategies that explicitly map time series data into the textual domain; however, the choice of time series encoder architecture remains underexplored. In this work, we conduct an exploratory study of hybrid architectures that combine specialized time series encoders with a frozen LLM backbone. We evaluate a diverse set of encoder families, including Inception, convolutional, residual, transformer-based, and multilayer perceptron architectures, among which the Inception model is the only encoder architecture that consistently yields positive performance gains when integrated with an LLM backbone. Overall, this study highlights the impact of time series encoder choice in hybrid LLM architectures and points to Inception-based models as a promising direction for future LLM-driven time series learning.

</details>


### [87] [In-Context Operator Learning on the Space of Probability Measures](https://arxiv.org/abs/2601.09979)
*Frank Cole,Dixi Wang,Yineng Chen,Yulong Lu,Rongjie Lai*

Main category: cs.LG

TL;DR: 提出了一种在概率测度空间上进行上下文算子学习的方法，用于最优传输问题，通过少量样本作为提示学习映射分布对的解算子，无需推理时的梯度更新。


<details>
  <summary>Details</summary>
Motivation: 传统最优传输方法通常需要为每个新的分布对重新计算传输映射，计算成本高。本文旨在学习一个通用的解算子，能够根据少量样本提示直接预测最优传输映射，提高计算效率。

Method: 提出上下文算子学习方法，参数化解算子，在两种理论框架下分析：非参数设置（任务集中在低维流形上）和参数设置（如高斯分布族）。建立了泛化边界和有限样本超额风险边界。

Result: 理论分析表明上下文精度随提示大小、内在任务维度和模型容量的扩展规律；在参数设置下给出了能精确恢复最优传输映射的显式架构。数值实验验证了框架在合成传输和生成建模基准上的有效性。

Conclusion: 本文提出的上下文算子学习方法为最优传输问题提供了一种高效的新范式，能够通过少量样本提示直接学习分布对到传输映射的解算子，在理论和实验上都证明了其有效性。

Abstract: We introduce \emph{in-context operator learning on probability measure spaces} for optimal transport (OT). The goal is to learn a single solution operator that maps a pair of distributions to the OT map, using only few-shot samples from each distribution as a prompt and \emph{without} gradient updates at inference. We parameterize the solution operator and develop scaling-law theory in two regimes. In the \emph{nonparametric} setting, when tasks concentrate on a low-intrinsic-dimension manifold of source--target pairs, we establish generalization bounds that quantify how in-context accuracy scales with prompt size, intrinsic task dimension, and model capacity. In the \emph{parametric} setting (e.g., Gaussian families), we give an explicit architecture that recovers the exact OT map in context and provide finite-sample excess-risk bounds. Our numerical experiments on synthetic transports and generative-modeling benchmarks validate the framework.

</details>


### [88] [FaTRQ: Tiered Residual Quantization for LLM Vector Search in Far-Memory-Aware ANNS Systems](https://arxiv.org/abs/2601.09985)
*Tianqi Zhang,Flavio Ponzina,Tajana Rosing*

Main category: cs.LG

TL;DR: FaTRQ是一个针对近似最近邻搜索的远内存感知优化系统，通过分层内存架构消除从慢速存储读取完整向量的需求，使用渐进距离估计器和分层残差量化技术，显著提升存储效率和查询吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现代ANNS引擎虽然使用预建索引和压缩向量量化表示加速搜索，但仍依赖昂贵的第二遍精炼阶段从慢速存储（如SSD）读取完整精度向量。对于现代文本和多模态嵌入，这些读取操作已成为整个查询延迟的主要瓶颈。

Method: 1. 提出渐进距离估计器，使用从远内存流式传输的紧凑残差来精炼粗略分数；2. 引入分层残差量化，将残差编码为三元值高效存储在远内存中；3. 在CXL Type-2设备中部署定制加速器，执行低延迟本地精炼；4. 当候选者被证明不在top-k范围内时，提前停止精炼过程。

Result: FaTRQ系统将存储效率提高了2.4倍，相比最先进的GPU ANNS系统，吞吐量提升了高达9倍。

Conclusion: FaTRQ通过分层内存感知的精炼系统有效解决了ANNS中从慢速存储读取完整向量的瓶颈问题，显著提升了检索增强生成(RAG)场景下的近似最近邻搜索性能。

Abstract: Approximate Nearest-Neighbor Search (ANNS) is a key technique in retrieval-augmented generation (RAG), enabling rapid identification of the most relevant high-dimensional embeddings from massive vector databases. Modern ANNS engines accelerate this process using prebuilt indexes and store compressed vector-quantized representations in fast memory. However, they still rely on a costly second-pass refinement stage that reads full-precision vectors from slower storage like SSDs. For modern text and multimodal embeddings, these reads now dominate the latency of the entire query. We propose FaTRQ, a far-memory-aware refinement system using tiered memory that eliminates the need to fetch full vectors from storage. It introduces a progressive distance estimator that refines coarse scores using compact residuals streamed from far memory. Refinement stops early once a candidate is provably outside the top-k. To support this, we propose tiered residual quantization, which encodes residuals as ternary values stored efficiently in far memory. A custom accelerator is deployed in a CXL Type-2 device to perform low-latency refinement locally. Together, FaTRQ improves the storage efficiency by 2.4$\times$ and improves the throughput by up to 9$ \times$ than SOTA GPU ANNS system.

</details>


### [89] [Continuous-Depth Transformers with Learned Control Dynamics](https://arxiv.org/abs/2601.10007)
*Peter Jemley*

Main category: cs.LG

TL;DR: 提出了一种混合Transformer架构，用连续深度神经ODE块替换离散中间层，通过学习的控制信号实现推理时对生成属性的控制。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer通过固定离散层处理表示，缺乏在推理时对生成属性进行连续控制的能力。本文旨在将深度视为连续变量，通过注入低维控制信号实现可操控的语言生成。

Method: 提出混合Transformer架构，用连续深度神经ODE块替换离散中间层。使用学习到的向量场F_θ(H, τ, u)，其中u是通过显式拼接注入的低维控制信号。采用伴随方法实现O(1)内存训练。

Result: 1) 梯度流稳定，无梯度爆炸/消失事件；2) 语义操控实现正/负面情感控制98%/88%准确率；3) 连续插值验证固定与自适应求解器间轨迹差异仅0.068%；4) 效率基准测试显示与标准离散基线延迟相当。

Conclusion: 带有学习控制信号的连续深度动力学为可操控语言生成提供了可行、高效的机制。自适应ODE求解器揭示了学习动力学中的几何结构，控制信号将向量场划分为具有不同曲率特征的动态机制。

Abstract: We present a hybrid transformer architecture that replaces discrete middle layers with a continuous-depth Neural Ordinary Differential Equation (ODE) block, enabling inference-time control over generation attributes via a learned steering signal. Unlike standard transformers that process representations through fixed discrete layers, our approach treats depth as a continuous variable governed by a learned vector field $F_θ(H, τ, u)$, where $u$ is a low-dimensional control signal injected via explicit concatenation. We validate the architecture through four experiments: (1) gradient flow stability with zero exploding/vanishing gradient events, (2) semantic steering achieving 98\%/88\% accuracy for positive/negative sentiment control, (3) continuous interpolation validated by a negligible 0.068\% trajectory divergence between fixed and adaptive solvers, and (4) efficiency benchmarking demonstrating latency parity with standard discrete baselines. Additionally, we show that adaptive ODE solvers reveal geometric structure in the learned dynamics: the control signal partitions the vector field into distinct dynamical regimes with different curvature characteristics. The adjoint method enables $O(1)$ memory training regardless of integration depth. Our results demonstrate that continuous-depth dynamics with learned control signals provide a viable, efficient mechanism for steerable language generation.

</details>


### [90] [Time Aggregation Features for XGBoost Models](https://arxiv.org/abs/2601.10019)
*Mykola Pinchuk*

Main category: cs.LG

TL;DR: 本文研究XGBoost模型在点击率预测中的时间聚合特征，比较了时间感知目标编码与多种窗口设计的时间聚合方法，发现滑动窗口表现最佳。


<details>
  <summary>Details</summary>
Motivation: 研究在严格时间外分割和无前瞻特征约束下，如何通过时间聚合特征改进点击率预测模型的性能。

Method: 使用Avazu点击率预测数据集，采用严格时间外分割和无前瞻特征约束。比较时间感知目标编码基线模型与添加实体历史时间聚合特征的模型，测试了多种窗口设计（滑动窗口、事件计数窗口、间隔窗口、分桶窗口）。

Result: 滑动窗口设计相对于单纯目标编码，ROC AUC提升约0.0066-0.0082，PR AUC提升约0.0084-0.0094。在时间聚合设计网格中，事件计数窗口是唯一能持续改进滑动窗口的方法，但增益较小。间隔窗口和分桶窗口表现不如简单滑动窗口。

Conclusion: 建议将滑动窗口作为实践中的默认选择，当边际ROC AUC增益重要时，可考虑使用事件计数窗口。

Abstract: This paper studies time aggregation features for XGBoost models in click-through rate prediction. The setting is the Avazu click-through rate prediction dataset with strict out-of-time splits and a no-lookahead feature constraint. Features for hour H use only impressions from hours strictly before H. This paper compares a strong time-aware target encoding baseline to models augmented with entity history time aggregation under several window designs. Across two rolling-tail folds on a deterministic ten percent sample, a trailing window specification improves ROC AUC by about 0.0066 to 0.0082 and PR AUC by about 0.0084 to 0.0094 relative to target encoding alone. Within the time aggregation design grid, event count windows provide the only consistent improvement over trailing windows, and the gain is small. Gap windows and bucketized windows underperform simple trailing windows in this dataset and protocol. These results support a practical default of trailing windows, with an optional event count window when marginal ROC AUC gains matter.

</details>


### [91] [BPE: Behavioral Profiling Ensemble](https://arxiv.org/abs/2601.10024)
*Yanxin Liu,Yunqi Zhang*

Main category: cs.LG

TL;DR: BPE框架通过构建模型内在的"行为画像"，基于测试实例响应与行为画像的偏差来分配集成权重，相比传统静态和动态集成方法在预测精度、计算效率和存储资源方面都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统集成方法（如Stacking）将每个基学习器视为整体分配权重，忽略了模型在不同实例空间区域的能力差异。动态集成选择（DES）虽然考虑了这种差异，但传统方法主要依赖模型间的差异性，忽视了模型内在特性，且严重依赖验证集进行能力估计。

Method: 提出行为画像集成（BPE）框架，为每个模型构建内在的"行为画像"，基于模型对特定测试实例的响应与其已建立的行为画像之间的偏差来推导集成权重，实现了从模型间差异到模型内在特性的范式转变。

Result: 在合成和真实世界数据集上的广泛实验表明，基于BPE框架的算法相比最先进的集成基线方法，在预测准确性、计算效率和存储资源利用方面都取得了显著改进。

Conclusion: BPE框架通过关注模型内在行为特性而非模型间差异，提供了一种更有效的集成学习方法，在多个性能指标上超越了传统集成方法。

Abstract: Ensemble learning is widely recognized as a pivotal strategy for pushing the boundaries of predictive performance. Traditional static ensemble methods, such as Stacking, typically assign weights by treating each base learner as a holistic entity, thereby overlooking the fact that individual models exhibit varying degrees of competence across different regions of the instance space. To address this limitation, Dynamic Ensemble Selection (DES) was introduced. However, both static and dynamic approaches predominantly rely on the divergence among different models as the basis for integration. This inter-model perspective neglects the intrinsic characteristics of the models themselves and necessitates a heavy reliance on validation sets for competence estimation. In this paper, we propose the Behavioral Profiling Ensemble (BPE) framework, which introduces a novel paradigm shift. Unlike traditional methods, BPE constructs a ``behavioral profile'' intrinsic to each model and derives integration weights based on the deviation between the model's response to a specific test instance and its established behavioral profile. Extensive experiments on both synthetic and real-world datasets demonstrate that the algorithm derived from the BPE framework achieves significant improvements over state-of-the-art ensemble baselines. These gains are evident not only in predictive accuracy but also in computational efficiency and storage resource utilization across various scenarios.

</details>


### [92] [Unlabeled Data Can Provably Enhance In-Context Learning of Transformers](https://arxiv.org/abs/2601.10058)
*Renpu Liu,Jing Yang*

Main category: cs.LG

TL;DR: 本文提出了一种增强的上下文学习框架，通过在提示中包含少量标注示例和大量未标注数据，利用transformer模拟EM算法，理论上证明了未标注数据能提升ICL性能。


<details>
  <summary>Details</summary>
Motivation: 传统上下文学习（ICL）的性能受限于提示中能容纳的少量昂贵标注示例，而现实中存在大量与任务相关的未标注数据。如何利用这些未标注数据来理论上增强ICL性能成为一个重要问题。

Method: 提出增强ICL框架，在提示中包含少量标注示例和未标注输入块。在多类线性分类设置下，通过思维链（CoT）提示，使多层transformer能够有效模拟期望最大化（EM）算法，从而隐式地从标注和未标注数据中提取有用信息。

Result: 理论分析表明，transformer可以通过教师强制训练，参数以线性速率收敛到期望解。实验证明增强ICL框架在性能上持续优于传统少样本ICL，为理论发现提供了实证支持。

Conclusion: 这是首个关于未标注数据对transformer ICL性能影响的理论研究，证明了通过合理设计提示框架，transformer能够有效利用未标注数据提升ICL性能，为实际应用提供了理论指导。

Abstract: Large language models (LLMs) exhibit impressive in-context learning (ICL) capabilities, yet the quality of their predictions is fundamentally limited by the few costly labeled demonstrations that can fit into a prompt. Meanwhile, there exist vast and continuously growing amounts of unlabeled data that may be closely related to the ICL task. How to utilize such unlabeled data to provably enhance the performance of ICL thus becomes an emerging fundamental question. In this work, we propose a novel augmented ICL framework, in which the prompt includes a small set of labeled examples alongside a block of unlabeled inputs. We focus on the multi-class linear classification setting and demonstrate that, with chain-of-thought (CoT) prompting, a multi-layer transformer can effectively emulate an expectation-maximization (EM) algorithm. This enables the transformer to implicitly extract useful information from both labeled and unlabeled data, leading to provable improvements in ICL accuracy. Moreover, we show that such a transformer can be trained via teacher forcing, with its parameters converging to the desired solution at a linear rate. Experiments demonstrate that the augmented ICL framework consistently outperforms conventional few-shot ICL, providing empirical support for our theoretical findings. To the best of our knowledge, this is the first theoretical study on the impact of unlabeled data on the ICL performance of transformers.

</details>


### [93] [Efficient Content-based Recommendation Model Training via Noise-aware Coreset Selection](https://arxiv.org/abs/2601.10067)
*Hung Vinh Tran,Tong Chen,Hechuan Wen,Quoc Viet Hung Nguyen,Bin Cui,Hongzhi Yin*

Main category: cs.LG

TL;DR: 提出了一种面向基于内容的推荐系统的噪声感知核心集选择框架，能够在仅使用1%训练数据的情况下恢复93-95%的全数据集训练性能


<details>
  <summary>Details</summary>
Motivation: 基于内容的推荐系统需要大规模甚至连续训练以适应多样化的用户偏好，导致高昂的计算成本和资源需求。核心集选择虽然能减少训练开销，但所选的核心集容易受到用户-物品交互中普遍存在的噪声影响，特别是在核心集规模较小时

Method: 提出噪声感知核心集选择框架，通过基于训练梯度的子模优化构建核心集，同时使用渐进训练模型校正噪声标签，并通过不确定性量化过滤低置信度样本以精炼核心集

Result: 实验表明NaCS为基于内容的推荐系统产生更高质量的核心集，比现有核心集选择技术效率更高。仅使用1%的训练数据即可恢复93-95%的全数据集训练性能

Conclusion: NaCS框架有效解决了基于内容的推荐系统中核心集选择对噪声敏感的问题，显著降低了训练开销同时保持了模型性能

Abstract: Content-based recommendation systems (CRSs) utilize content features to predict user-item interactions, serving as essential tools for helping users navigate information-rich web services. However, ensuring the effectiveness of CRSs requires large-scale and even continuous model training to accommodate diverse user preferences, resulting in significant computational costs and resource demands. A promising approach to this challenge is coreset selection, which identifies a small but representative subset of data samples that preserves model quality while reducing training overhead. Yet, the selected coreset is vulnerable to the pervasive noise in user-item interactions, particularly when it is minimally sized. To this end, we propose Noise-aware Coreset Selection (NaCS), a specialized framework for CRSs. NaCS constructs coresets through submodular optimization based on training gradients, while simultaneously correcting noisy labels using a progressively trained model. Meanwhile, we refine the selected coreset by filtering out low-confidence samples through uncertainty quantification, thereby avoid training with unreliable interactions. Through extensive experiments, we show that NaCS produces higher-quality coresets for CRSs while achieving better efficiency than existing coreset selection techniques. Notably, NaCS recovers 93-95\% of full-dataset training performance using merely 1\% of the training data. The source code is available at \href{https://github.com/chenxing1999/nacs}{https://github.com/chenxing1999/nacs}.

</details>


### [94] [Comparative Evaluation of Deep Learning-Based and WHO-Informed Approaches for Sperm Morphology Assessment](https://arxiv.org/abs/2601.10070)
*Mohammad Abbadi*

Main category: cs.LG

TL;DR: 本研究比较了基于图像的深度学习模型HuSHeM与WHO(+SIRI)基线方法在精子形态质量评估中的性能，发现深度学习模型在判别性能、校准和临床效用方面均优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 精子形态质量评估是男性生育力评价的关键但主观的组成部分，常受观察者间变异性和资源限制的影响。本研究旨在开发更客观、可重复的评估方法。

Method: 提出了一个比较性生物医学人工智能框架，评估基于图像的深度学习模型HuSHeM与基于WHO标准并增强系统性炎症反应指数(SIRI)的临床基线方法。HuSHeM模型在高分辨率精子形态图像上训练，并在独立临床队列中评估。

Result: HuSHeM模型显示出更高的判别性能（ROC曲线下面积更大），在类别不平衡情况下表现更好（精确率-召回率面积值更高），校准分析显示预测概率与观察结果更一致，决策曲线分析表明在临床相关阈值概率下具有更大的净临床效益。

Conclusion: 基于图像的深度学习相比传统的基于规则和炎症增强的标准，可能提供更好的预测可靠性和临床效用。该框架支持精子形态的客观、可重复评估，可作为生育筛查和转诊工作流程中的决策支持工具，但并非旨在取代临床判断或实验室评估。

Abstract: Assessment of sperm morphological quality remains a critical yet subjective component of male fertility evaluation, often limited by inter-observer variability and resource constraints. This study presents a comparative biomedical artificial intelligence framework evaluating an image-based deep learning model (HuSHeM) alongside a clinically grounded baseline derived from World Health Organization criteria augmented with the Systemic Inflammation Response Index (WHO(+SIRI)).
  The HuSHeM model was trained on high-resolution sperm morphology images and evaluated using an independent clinical cohort. Model performance was assessed using discrimination, calibration, and clinical utility analyses. The HuSHeM model demonstrated higher discriminative performance, as reflected by an increased area under the receiver operating characteristic curve with relatively narrow confidence intervals compared to WHO(+SIRI). Precision-recall analysis further indicated improved performance under class imbalance, with higher precision-recall area values across evaluated thresholds. Calibration analysis indicated closer agreement between predicted probabilities and observed outcomes for HuSHeM, while decision curve analysis suggested greater net clinical benefit across clinically relevant threshold probabilities.
  These findings suggest that image-based deep learning may offer improved predictive reliability and clinical utility compared with traditional rule-based and inflammation-augmented criteria. The proposed framework supports objective and reproducible assessment of sperm morphology and may serve as a decision-support tool within fertility screening and referral workflows. The proposed models are intended as decision-support or referral tools and are not designed to replace clinical judgment or laboratory assessment.

</details>


### [95] [Sparse-RL: Breaking the Memory Wall in LLM Reinforcement Learning via Stable Sparse Rollouts](https://arxiv.org/abs/2601.10079)
*Sijia Luo,Xiaokang Zhang,Yuxuan Hu,Bohan Zhang,Ke Wang,Jinbo Su,Mengshu Sun,Lei Liang,Jing Zhang*

Main category: cs.LG

TL;DR: Sparse-RL：一种在稀疏rollouts下实现稳定强化学习训练的方法，通过解决KV缓存压缩导致的策略不匹配问题，显著降低内存开销同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习在大型语言模型复杂推理能力开发中至关重要，但长序列rollouts中KV缓存的高内存开销成为硬件限制下的关键瓶颈。现有KV压缩技术虽然适用于推理，但直接应用于RL训练会导致严重的策略不匹配和性能崩溃。

Method: 提出Sparse-RL框架，通过Sparsity-Aware Rejection Sampling（稀疏感知拒绝采样）和Importance-based Reweighting（基于重要性的重加权）来纠正压缩引起的信息损失带来的离策略偏差，解决密集旧策略、稀疏采样策略和学习者策略之间的不匹配问题。

Result: 实验结果表明，Sparse-RL相比密集基线显著降低了rollout开销，同时保持了性能。此外，该方法实现了稀疏感知训练，显著增强了模型在稀疏推理部署时的鲁棒性。

Conclusion: Sparse-RL成功解决了KV缓存压缩在RL训练中的策略不匹配问题，为在有限硬件资源下进行高效RL训练提供了可行方案，同时增强了模型在稀疏推理环境中的鲁棒性。

Abstract: Reinforcement Learning (RL) has become essential for eliciting complex reasoning capabilities in Large Language Models (LLMs). However, the substantial memory overhead of storing Key-Value (KV) caches during long-horizon rollouts acts as a critical bottleneck, often prohibiting efficient training on limited hardware. While existing KV compression techniques offer a remedy for inference, directly applying them to RL training induces a severe policy mismatch, leading to catastrophic performance collapse. To address this, we introduce Sparse-RL empowers stable RL training under sparse rollouts. We show that instability arises from a fundamental policy mismatch among the dense old policy, the sparse sampler policy, and the learner policy. To mitigate this issue, Sparse-RL incorporates Sparsity-Aware Rejection Sampling and Importance-based Reweighting to correct the off-policy bias introduced by compression-induced information loss. Experimental results show that Sparse-RL reduces rollout overhead compared to dense baselines while preserving the performance. Furthermore, Sparse-RL inherently implements sparsity-aware training, significantly enhancing model robustness during sparse inference deployment.

</details>


### [96] [Bayesian Meta-Analyses Could Be More: A Case Study in Trial of Labor After a Cesarean-section Outcomes and Complications](https://arxiv.org/abs/2601.10089)
*Ashley Klein,Edward Raff,Marcia DesJardin*

Main category: cs.LG

TL;DR: 该研究针对医学元分析中关键决策变量缺失导致效应大小未知的问题，提出了一种贝叶斯方法，并在剖宫产后试产评估中验证其应用价值。


<details>
  <summary>Details</summary>
Motivation: 医学研究中元分析的可靠性依赖于先前研究是否准确捕捉了关键变量。在医学研究中，影响医生决策的关键变量常常未被记录，导致效应大小未知和结论不可靠。特别是在剖宫产后试产等临床情境中，缺乏有效干预措施，需要更可靠的分析方法来支持医生决策。

Method: 研究构建了一种贝叶斯方法来处理医学研究中关键决策变量缺失的常见问题。该方法允许在变量缺失的情况下，分析阳性效应的主张是否仍然成立。研究将该方法应用于剖宫产后试产的临床评估，协助产科医生在干预措施有限的情况下做出更可靠的决策。

Result: 贝叶斯方法能够处理关键变量缺失的情况，为医学元分析提供更可靠的结论。在剖宫产后试产的临床应用中，该方法为医生提供了必要的支持，帮助他们推进患者护理，特别是在干预措施有限的复杂临床情境中。

Conclusion: 贝叶斯方法为解决医学研究中关键决策变量缺失的问题提供了有效途径，能够提高元分析的可靠性，为临床决策提供更有力的支持，特别是在剖宫产后试产等复杂医疗情境中具有重要应用价值。

Abstract: The meta-analysis's utility is dependent on previous studies having accurately captured the variables of interest, but in medical studies, a key decision variable that impacts a physician's decisions was not captured. This results in an unknown effect size and unreliable conclusions. A Bayesian approach may allow analysis to determine if the claim of a positive effect is still warranted, and we build a Bayesian approach to this common medical scenario. To demonstrate its utility, we assist professional OBGYNs in evaluating Trial of Labor After a Cesarean-section (TOLAC) situations where few interventions are available for patients and find the support needed for physicians to advance patient care.

</details>


### [97] [LeMoF: Level-guided Multimodal Fusion for Heterogeneous Clinical Data](https://arxiv.org/abs/2601.10092)
*Jongseok Kim,Seongae Kang,Jonghwan Shin,Yuhan Lee,Ohyun Jo*

Main category: cs.LG

TL;DR: LeMoF是一种新颖的多模态临床预测框架，通过层级引导的模态融合策略，选择性地整合每个模态中不同编码器层级的表示，在ICU住院时间预测任务上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态临床预测方法通常采用静态的模态整合方案和简单的融合策略，未能充分利用模态特定的表示，特别是在异构临床环境中难以平衡预测稳定性和判别能力。

Method: 提出Level-guided Modal Fusion (LeMoF)框架，通过层级引导的表示融合，在每个模态内选择性地整合不同编码器层级的表示，明确分离并学习全局模态级预测和层级特定的判别表示。

Result: 在ICU住院时间预测实验中，LeMoF在各种编码器配置下均一致优于现有的最先进多模态融合技术，证明了层级整合是实现稳健预测性能的关键因素。

Conclusion: LeMoF通过层级引导的模态融合策略，能够在异构临床环境中实现预测稳定性和判别能力之间的平衡，为多模态临床预测提供了更有效的解决方案。

Abstract: Multimodal clinical prediction is widely used to integrate heterogeneous data such as Electronic Health Records (EHR) and biosignals. However, existing methods tend to rely on static modality integration schemes and simple fusion strategies. As a result, they fail to fully exploit modality-specific representations. In this paper, we propose Level-guided Modal Fusion (LeMoF), a novel framework that selectively integrates level-guided representations within each modality. Each level refers to a representation extracted from a different layer of the encoder. LeMoF explicitly separates and learns global modality-level predictions from level-specific discriminative representations. This design enables LeMoF to achieve a balanced performance between prediction stability and discriminative capability even in heterogeneous clinical environments. Experiments on length of stay prediction using Intensive Care Unit (ICU) data demonstrate that LeMoF consistently outperforms existing state-of-the-art multimodal fusion techniques across various encoder configurations. We also confirmed that level-wise integration is a key factor in achieving robust predictive performance across various clinical conditions.

</details>


### [98] [Multilingual-To-Multimodal (M2M): Unlocking New Languages with Monolingual Text](https://arxiv.org/abs/2601.10096)
*Piyush Singh Pasi*

Main category: cs.LG

TL;DR: METAL是一种轻量级对齐方法，仅使用英语文本学习少量线性层，将多语言文本嵌入映射到多模态空间，实现强大的零样本跨语言迁移能力。


<details>
  <summary>Details</summary>
Motivation: 多模态模型在英语上表现出色，但在其他语言上性能大幅下降，主要原因是缺乏多语言多模态资源。现有解决方案过度依赖机器翻译，而多语言文本建模的进展未得到充分利用。

Method: 提出METAL方法，仅使用英语文本学习少量线性层，将多语言文本嵌入映射到多模态空间。该方法简单但有效，通过权重分析显示变换重塑了嵌入几何结构而非简单旋转。

Result: METAL在英语上达到基线性能（94.9% R@10），在11种语言（10种未见语言）上平均达到89.5% R@10的零样本迁移效果。定性t-SNE可视化显示多语言嵌入与多模态表示紧密对齐。

Conclusion: METAL不仅适用于图像-文本检索，还能推广到音频-文本检索和跨语言文本到图像生成。作者发布了代码、检查点和多语言评估数据集以促进进一步研究。

Abstract: Multimodal models excel in English, supported by abundant image-text and audio-text data, but performance drops sharply for other languages due to limited multilingual multimodal resources. Existing solutions rely heavily on machine translation, while advances in multilingual text modeling remain underutilized. We introduce METAL, a lightweight alignment method that learns only a few linear layers using English text alone to map multilingual text embeddings into a multimodal space. Despite its simplicity, METAL matches baseline performance in English (94.9 percent Recall at 10) and achieves strong zero-shot transfer (89.5 percent Recall at 10 averaged across 11 languages, 10 unseen) on XTD text-to-image retrieval. Qualitative t-SNE visualizations show that multilingual embeddings align tightly with multimodal representations, while weight analysis reveals that the transformation reshapes embedding geometry rather than performing trivial rotations. Beyond image-text retrieval, METAL generalizes to audio-text retrieval and cross-lingual text-to-image generation. We release code and checkpoints at https://github.com/m2m-codebase/M2M , as well as multilingual evaluation datasets including MSCOCO Multilingual 30K (https://huggingface.co/datasets/piyushsinghpasi/mscoco-multilingual-30k ), AudioCaps Multilingual (https://huggingface.co/datasets/piyushsinghpasi/audiocaps-multilingual ), and Clotho Multilingual (https://huggingface.co/datasets/piyushsinghpasi/clotho-multilingual ), to facilitate further research.

</details>


### [99] [Step-by-Step Causality: Transparent Causal Discovery with Multi-Agent Tree-Query and Adversarial Confidence Estimation](https://arxiv.org/abs/2601.10137)
*Ziyi Ding,Chenfei Ye-Hao,Zheyuan Wang,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: Tree-Query是一个树状结构的多专家LLM框架，通过将成对因果发现转化为关于后门路径、独立性、潜在混杂和因果方向的查询序列，提供可解释的因果判断和鲁棒性感知的置信度分数。


<details>
  <summary>Details</summary>
Motivation: 传统基于约束的方法（如PC、FCI）存在误差传播问题，而最近的LLM因果预言机往往表现为不透明、无置信度的黑盒，需要一种更可靠、可解释的因果发现方法。

Method: 提出Tree-Query框架，将成对因果发现转化为关于后门路径、独立性、潜在混杂和因果方向的查询序列，采用树状结构的多专家LLM设计，提供可解释的判断和鲁棒性感知的置信度分数。

Result: 在基于Mooij等人和UCI因果图的数据无关基准测试中，Tree-Query在结构指标上优于直接LLM基线；饮食-体重案例研究展示了混杂因素筛选和稳定、高置信度的因果结论；理论保证提供了四种成对关系的渐近可识别性。

Conclusion: Tree-Query提供了一种原则性的方法，从LLMs获取数据无关的因果先验，可以补充下游数据驱动的因果发现，为因果发现提供了更可靠、可解释的解决方案。

Abstract: Causal discovery aims to recover ``what causes what'', but classical constraint-based methods (e.g., PC, FCI) suffer from error propagation, and recent LLM-based causal oracles often behave as opaque, confidence-free black boxes. This paper introduces Tree-Query, a tree-structured, multi-expert LLM framework that reduces pairwise causal discovery to a short sequence of queries about backdoor paths, (in)dependence, latent confounding, and causal direction, yielding interpretable judgments with robustness-aware confidence scores. Theoretical guarantees are provided for asymptotic identifiability of four pairwise relations. On data-free benchmarks derived from Mooij et al. and UCI causal graphs, Tree-Query improves structural metrics over direct LLM baselines, and a diet--weight case study illustrates confounder screening and stable, high-confidence causal conclusions. Tree-Query thus offers a principled way to obtain data-free causal priors from LLMs that can complement downstream data-driven causal discovery. Code is available at https://anonymous.4open.science/r/Repo-9B3E-4F96.

</details>


### [100] [Understanding and Preserving Safety in Fine-Tuned LLMs](https://arxiv.org/abs/2601.10141)
*Jiawen Zhang,Yangfan Hu,Kejia Chen,Lipeng He,Jiachen Ma,Jian Lou,Dan Li,Jian Liu,Xiaohu Yang,Ruoxi Jia*

Main category: cs.LG

TL;DR: 该论文提出了一种名为SPF的安全保护微调方法，通过分析安全梯度与效用梯度之间的几何交互关系，在微调过程中显式移除与低秩安全子空间冲突的梯度分量，从而解决LLM微调中的安全-效用困境。


<details>
  <summary>Details</summary>
Motivation: 大语言模型微调在应用于下游任务时，即使微调数据完全无害，也可能显著降低安全对齐性，增加越狱攻击的脆弱性。现有方法面临持续的安全-效用困境：强调安全会损害任务性能，而优先考虑效用通常需要深度微调，这不可避免地导致安全性急剧下降。

Method: 通过系统实证分析发现：(1)安全梯度位于低秩子空间，而效用梯度跨越更广泛的高维空间；(2)这些子空间通常负相关，导致微调期间的方向冲突；(3)主导安全方向可以从单个样本中高效估计。基于这些发现，提出了安全保护微调(SPF)，这是一种轻量级方法，显式移除与低秩安全子空间冲突的梯度分量。

Result: 理论上，SPF保证效用收敛同时限制安全漂移。实证上，SPF持续保持下游任务性能，并恢复几乎所有预训练的安全对齐，即使在对抗性微调场景下也是如此。此外，SPF对深度微调和动态越狱攻击表现出强大的抵抗能力。

Conclusion: 该研究为大语言模型微调提供了新的机制理解和实践指导，实现了始终对齐的LLM微调。SPF方法有效解决了安全-效用困境，为安全微调提供了可行的解决方案。

Abstract: Fine-tuning is an essential and pervasive functionality for applying large language models (LLMs) to downstream tasks. However, it has the potential to substantially degrade safety alignment, e.g., by greatly increasing susceptibility to jailbreak attacks, even when the fine-tuning data is entirely harmless. Despite garnering growing attention in defense efforts during the fine-tuning stage, existing methods struggle with a persistent safety-utility dilemma: emphasizing safety compromises task performance, whereas prioritizing utility typically requires deep fine-tuning that inevitably leads to steep safety declination.
  In this work, we address this dilemma by shedding new light on the geometric interaction between safety- and utility-oriented gradients in safety-aligned LLMs. Through systematic empirical analysis, we uncover three key insights: (I) safety gradients lie in a low-rank subspace, while utility gradients span a broader high-dimensional space; (II) these subspaces are often negatively correlated, causing directional conflicts during fine-tuning; and (III) the dominant safety direction can be efficiently estimated from a single sample. Building upon these novel insights, we propose safety-preserving fine-tuning (SPF), a lightweight approach that explicitly removes gradient components conflicting with the low-rank safety subspace. Theoretically, we show that SPF guarantees utility convergence while bounding safety drift. Empirically, SPF consistently maintains downstream task performance and recovers nearly all pre-trained safety alignment, even under adversarial fine-tuning scenarios. Furthermore, SPF exhibits robust resistance to both deep fine-tuning and dynamic jailbreak attacks. Together, our findings provide new mechanistic understanding and practical guidance toward always-aligned LLM fine-tuning.

</details>


### [101] [LOOKAT: Lookup-Optimized Key-Attention for Memory-Efficient Transformers](https://arxiv.org/abs/2601.10155)
*Aryan Karmore*

Main category: cs.LG

TL;DR: LOOKAT是一种KV缓存压缩方法，通过乘积量化和非对称距离计算将注意力计算从内存密集型转为计算密集型，实现64倍压缩同时保持95.7%输出保真度。


<details>
  <summary>Details</summary>
Motivation: 现有量化方法虽然压缩了存储，但未能减少带宽，因为注意力计算需要将INT4/INT8键值反量化为FP16。需要一种更好的KV缓存压缩方法来在边缘设备上部署大语言模型。

Method: 将注意力评分视为内积相似性搜索，应用向量数据库的压缩技术。使用乘积量化将键向量分解为子空间，学习码本，通过查找表计算注意力表，将注意力从内存密集型转为计算密集型。

Result: 在GPT-2上测试，实现64倍压缩时95.7%输出保真度，32倍压缩时95.0%保真度。无需架构更改或训练，保持秩相关ρ>0.95。理论分析显示秩相关退化与d_k/mK成正比。

Conclusion: LOOKAT通过乘积量化和查找表机制有效压缩KV缓存，显著减少带宽需求，使大语言模型能够在边缘设备上部署，同时保持高输出质量。

Abstract: Compressing the KV cache is a required step to deploy large language models on edge devices. Current quantization methods compress storage but fail to reduce bandwidth as attention calculation requires dequantizing keys from INT4/INT8 to FP16 before use. We observe that attention scoring is mathematically equivalent to the inner product similarity search and we can apply some compression techniques from vector databases to compress KV-cache better. We propose LOOKAT, which applies product quantization and asymmetric distance computation, to transformer architecture by decomposing key vectors into subspaces, learning codebooks and computing attention tables via lookup tables. This transforms attention from memory-bound to compute-bound. LOOKAT achieves 64 $\times$ compression at 95.7\% output fidelity and 32 $\times$ compression at 95.0\% fidelity when tested on GPT-2. LOOKAT requires no architecture changes or training while maintaining rank correlation $ρ> 0.95$. Theoretical analysis confirms that rank correlation degrades as $O(d_k/mK)$, with guarantees validated across sequence lengths up to 1024 tokens.

</details>


### [102] [CC-OR-Net: A Unified Framework for LTV Prediction through Structural Decoupling](https://arxiv.org/abs/2601.10176)
*Mingyu Zhao,Haoran Bai,Yu Tian,Bing Zhu,Hengliang Luo*

Main category: cs.LG

TL;DR: 提出CC-OR-Net框架解决客户终身价值预测中的零膨胀长尾分布问题，通过结构分解实现排名与回归的鲁棒解耦，在真实数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 客户终身价值预测面临零膨胀和长尾分布的两大挑战：大多数低中价值用户数量上压倒少数但关键的高价值"鲸鱼"用户；低中价值用户内部也存在显著异质性。现有方法要么依赖刚性统计假设，要么通过损失约束而非架构设计来解耦排名和回归，难以平衡全局准确性和高价值精度。

Method: 提出CC-OR-Net框架，包含三个专门组件：结构序数分解模块用于鲁棒排名，桶内残差模块用于细粒度回归，针对性高价值增强模块用于提升顶级用户预测精度。通过结构分解在架构层面保证排名特性。

Result: 在包含超过3亿用户的真实数据集上评估，CC-OR-Net在所有关键业务指标上实现了优越的权衡，超越了现有最先进方法，提供了全面且具有商业价值的LTV预测解决方案。

Conclusion: CC-OR-Net通过结构分解实现了排名与回归的鲁棒解耦，有效解决了LTV预测中的零膨胀长尾分布问题，为实际商业应用提供了更优的预测框架。

Abstract: Customer Lifetime Value (LTV) prediction, a central problem in modern marketing, is characterized by a unique zero-inflated and long-tail data distribution. This distribution presents two fundamental challenges: (1) the vast majority of low-to-medium value users numerically overwhelm the small but critically important segment of high-value "whale" users, and (2) significant value heterogeneity exists even within the low-to-medium value user base. Common approaches either rely on rigid statistical assumptions or attempt to decouple ranking and regression using ordered buckets; however, they often enforce ordinality through loss-based constraints rather than inherent architectural design, failing to balance global accuracy with high-value precision. To address this gap, we propose \textbf{C}onditional \textbf{C}ascaded \textbf{O}rdinal-\textbf{R}esidual Networks \textbf{(CC-OR-Net)}, a novel unified framework that achieves a more robust decoupling through \textbf{structural decomposition}, where ranking is architecturally guaranteed. CC-OR-Net integrates three specialized components: a \textit{structural ordinal decomposition module} for robust ranking, an \textit{intra-bucket residual module} for fine-grained regression, and a \textit{targeted high-value augmentation module} for precision on top-tier users. Evaluated on real-world datasets with over 300M users, CC-OR-Net achieves a superior trade-off across all key business metrics, outperforming state-of-the-art methods in creating a holistic and commercially valuable LTV prediction solution.

</details>


### [103] [Graph Regularized PCA](https://arxiv.org/abs/2601.10199)
*Antonio Briola,Marwin Schmidt,Fabio Caccioli,Carlos Ros Perez,James Singleton,Christian Michler,Tomaso Aste*

Main category: cs.LG

TL;DR: GR-PCA是一种图正则化PCA方法，通过结合数据特征的依赖结构，学习稀疏精度图并将载荷偏向图拉普拉斯算子的低频傅里叶模式，从而抑制高频信号、保留图一致的低频信号，获得与条件关系对齐的可解释主成分。


<details>
  <summary>Details</summary>
Motivation: 高维数据中变量间的依赖关系常常违反PCA所基于的各向同性噪声假设。当噪声在特征间不是独立同分布时，需要一种能够结合数据依赖结构的降维方法。

Method: 提出图正则化PCA(GR-PCA)，通过图基正则化将数据特征的依赖结构融入PCA。方法学习稀疏精度图，并将载荷偏向对应图拉普拉斯算子的低频傅里叶模式，从而抑制高频信号、保留图一致的低频信号。

Result: 在多种图拓扑、信噪比和稀疏度水平的合成数据上评估表明，GR-PCA能将方差集中在预期支撑上，产生具有更低图拉普拉斯能量的载荷，并在样本外重构中保持竞争力。当存在高频信号时，图拉普拉斯惩罚防止过拟合，虽然降低重构精度但提高结构保真度。

Conclusion: GR-PCA提供了一种实用、可扩展的结构感知降维方法，在不牺牲预测性能的情况下提高结构保真度。当高频信号与图相关时，相比PCA优势最明显；当高频信号接近旋转不变时，PCA仍具竞争力。

Abstract: High-dimensional data often exhibit dependencies among variables that violate the isotropic-noise assumption under which principal component analysis (PCA) is optimal. For cases where the noise is not independent and identically distributed across features (i.e., the covariance is not spherical) we introduce Graph Regularized PCA (GR-PCA). It is a graph-based regularization of PCA that incorporates the dependency structure of the data features by learning a sparse precision graph and biasing loadings toward the low-frequency Fourier modes of the corresponding graph Laplacian. Consequently, high-frequency signals are suppressed, while graph-coherent low-frequency ones are preserved, yielding interpretable principal components aligned with conditional relationships. We evaluate GR-PCA on synthetic data spanning diverse graph topologies, signal-to-noise ratios, and sparsity levels. Compared to mainstream alternatives, it concentrates variance on the intended support, produces loadings with lower graph-Laplacian energy, and remains competitive in out-of-sample reconstruction. When high-frequency signals are present, the graph Laplacian penalty prevents overfitting, reducing the reconstruction accuracy but improving structural fidelity. The advantage over PCA is most pronounced when high-frequency signals are graph-correlated, whereas PCA remains competitive when such signals are nearly rotationally invariant. The procedure is simple to implement, modular with respect to the precision estimator, and scalable, providing a practical route to structure-aware dimensionality reduction that improves structural fidelity without sacrificing predictive performance.

</details>


### [104] [PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary](https://arxiv.org/abs/2601.10201)
*Jiarui Yao,Ruida Wang,Tong Zhang*

Main category: cs.LG

TL;DR: 本文提出过程奖励学习（PRL），将熵正则化强化学习目标分解为中间步骤，通过严格的过程奖励指导LLM推理过程，无需额外步骤如MCTS或单独奖励模型训练。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理能力提升方法大多基于轨迹级结果奖励，缺乏推理过程的细粒度监督。其他结合过程信号的训练框架依赖繁琐的额外步骤（如MCTS、训练单独奖励模型），且过程信号设计缺乏严格理论支持。

Method: 提出过程奖励学习（PRL），从理论动机出发，将熵正则化强化学习目标分解为中间步骤，推导出本质上等同于奖励最大化加策略模型与参考模型间KL散度惩罚项的PRL公式，将结果奖励转化为过程监督信号。

Result: 实验表明PRL不仅提高了LLM推理能力的平均性能（通过average @ n衡量），还通过改善pass @ n指标拓宽了推理边界。大量实验验证了PRL的有效性和泛化能力。

Conclusion: PRL提供了一种理论严谨、训练高效的过程监督方法，能够将结果奖励转化为细粒度的过程信号，有效指导强化学习优化过程中的探索，提升LLM的推理能力。

Abstract: Improving the reasoning abilities of Large Language Models (LLMs) has been a continuous topic recently. But most relevant works are based on outcome rewards at the trajectory level, missing fine-grained supervision during the reasoning process. Other existing training frameworks that try to combine process signals together to optimize LLMs also rely heavily on tedious additional steps like MCTS, training a separate reward model, etc., doing harm to the training efficiency. Moreover, the intuition behind the process signals design lacks rigorous theoretical support, leaving the understanding of the optimization mechanism opaque. In this paper, we propose Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. Starting from theoretical motivation, we derive the formulation of PRL that is essentially equivalent to the objective of reward maximization plus a KL-divergence penalty term between the policy model and a reference model. However, PRL could turn the outcome reward into process supervision signals, which helps better guide the exploration during RL optimization. From our experiment results, we demonstrate that PRL not only improves the average performance for LLMs' reasoning ability measured by average @ n, but also broadens the reasoning boundary by improving the pass @ n metric. Extensive experiments show the effectiveness of PRL could be verified and generalized.

</details>


### [105] [Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD](https://arxiv.org/abs/2601.10237)
*Murat Bilgehan Ertan,Marten van Dijk*

Main category: cs.LG

TL;DR: 本文分析了差分隐私随机梯度下降（DP-SGD）在f-差分隐私框架下的基本限制，证明了在标准最坏情况对抗模型下，DP-SGD无法同时实现强隐私和高效用，存在严格的噪声下界约束。


<details>
  <summary>Details</summary>
Motivation: DP-SGD是私有训练的主要范式，但其在最坏情况对抗隐私定义下的基本限制尚未被充分理解。研究者希望分析DP-SGD在f-差分隐私框架下的隐私-效用权衡，揭示其根本局限性。

Method: 采用f-差分隐私框架，通过假设检验权衡曲线分析隐私保护效果。研究单轮训练中M次梯度更新的洗牌采样机制，推导出可实现的权衡曲线的显式次优上界，进而得到分离度κ的几何下界。分析高斯噪声乘子σ与分离度κ之间的严格下界关系。

Result: 证明了在最坏情况对抗模型下，洗牌DP-SGD必须满足σ ≥ 1/√(2ln M) 或 κ ≥ 1/√8(1-1/√(4πln M))，无法同时实现强隐私和高效用。该限制也扩展到泊松子采样（相差常数因子）。实验证实该噪声水平会导致实际训练中的显著准确度下降。

Conclusion: DP-SGD在标准最坏情况对抗假设下存在关键瓶颈，隐私保护与模型效用之间存在根本性权衡。虽然随着M→∞限制会消失，但收敛速度极慢，在实际相关更新次数下所需噪声幅度仍然很大，限制了DP-SGD的实际应用效果。

Abstract: Differentially Private Stochastic Gradient Descent (DP-SGD) is the dominant paradigm for private training, but its fundamental limitations under worst-case adversarial privacy definitions remain poorly understood. We analyze DP-SGD in the $f$-differential privacy framework, which characterizes privacy via hypothesis-testing trade-off curves, and study shuffled sampling over a single epoch with $M$ gradient updates. We derive an explicit suboptimal upper bound on the achievable trade-off curve. This result induces a geometric lower bound on the separation $κ$ which is the maximum distance between the mechanism's trade-off curve and the ideal random-guessing line. Because a large separation implies significant adversarial advantage, meaningful privacy requires small $κ$. However, we prove that enforcing a small separation imposes a strict lower bound on the Gaussian noise multiplier $σ$, which directly limits the achievable utility. In particular, under the standard worst-case adversarial model, shuffled DP-SGD must satisfy
  $σ\ge \frac{1}{\sqrt{2\ln M}}$ $\quad\text{or}\quad$ $κ\ge\ \frac{1}{\sqrt{8}}\!\left(1-\frac{1}{\sqrt{4π\ln M}}\right)$,
  and thus cannot simultaneously achieve strong privacy and high utility. Although this bound vanishes asymptotically as $M \to \infty$, the convergence is extremely slow: even for practically relevant numbers of updates the required noise magnitude remains substantial. We further show that the same limitation extends to Poisson subsampling up to constant factors. Our experiments confirm that the noise levels implied by this bound leads to significant accuracy degradation at realistic training settings, thus showing a critical bottleneck in DP-SGD under standard worst-case adversarial assumptions.

</details>


### [106] [X-SAM: Boosting Sharpness-Aware Minimization with Dominant-Eigenvector Gradient Correction](https://arxiv.org/abs/2601.10251)
*Hongru Duan,Yongle Chen,Lei Guan*

Main category: cs.LG

TL;DR: 本文提出X-SAM方法，通过特征向量对齐修正SAM的梯度，更直接有效地正则化Hessian矩阵的最大特征值，改善泛化性能。


<details>
  <summary>Details</summary>
Motivation: SAM方法旨在通过最小化参数邻域内的最坏扰动损失来改善泛化，但在训练中其优化行为并不总是符合理论预期。当梯度仍指向尖锐区域时，SAM的锐度正则化效果可能被削弱。

Method: 从谱和几何角度分析SAM，利用梯度与Hessian矩阵主导特征向量之间的夹角作为锐度度量。提出显式特征向量对齐的SAM（X-SAM），通过沿顶部特征向量的正交分解修正梯度。

Result: 证明了X-SAM的收敛性和优越的泛化性能，大量实验评估证实了理论和实践上的优势。

Conclusion: X-SAM通过特征向量对齐机制更直接有效地正则化Hessian矩阵的最大特征值，解决了SAM在某些情况下锐度正则化效果不足的问题，在理论和实践上都表现出优越性。

Abstract: Sharpness-Aware Minimization (SAM) aims to improve generalization by minimizing a worst-case perturbed loss over a small neighborhood of model parameters. However, during training, its optimization behavior does not always align with theoretical expectations, since both sharp and flat regions may yield a small perturbed loss. In such cases, the gradient may still point toward sharp regions, failing to achieve the intended effect of SAM. To address this issue, we investigate SAM from a spectral and geometric perspective: specifically, we utilize the angle between the gradient and the leading eigenvector of the Hessian as a measure of sharpness. Our analysis illustrates that when this angle is less than or equal to ninety degrees, the effect of SAM's sharpness regularization can be weakened. Furthermore, we propose an explicit eigenvector-aligned SAM (X-SAM), which corrects the gradient via orthogonal decomposition along the top eigenvector, enabling more direct and efficient regularization of the Hessian's maximum eigenvalue. We prove X-SAM's convergence and superior generalization, with extensive experimental evaluations confirming both theoretical and practical advantages.

</details>


### [107] [Early Fault Detection on CMAPSS with Unsupervised LSTM Autoencoders](https://arxiv.org/abs/2601.10269)
*P. Sánchez,K. Reyes,B. Radu,E. Fernández*

Main category: cs.LG

TL;DR: 本文提出了一种无需故障标签的无监督涡扇发动机健康监测框架，通过回归归一化消除工况影响，使用LSTM自编码器仅训练健康数据，基于自适应阈值触发实时警报。


<details>
  <summary>Details</summary>
Motivation: 传统健康监测方法需要故障标签，但实际运行中故障数据稀缺且昂贵。本文旨在开发无需故障标签的无监督方法，能够快速部署并适应不同机队。

Method: 1. 使用基于回归的归一化消除NASA CMAPSS传感器数据中的工况影响；2. 仅使用每个轨迹的健康部分训练LSTM自编码器；3. 通过自适应数据驱动阈值估计持续重构误差，触发实时警报，无需人工调参规则。

Result: 基准测试显示该方法在多种运行状态下具有高召回率和低误报率，证明能够快速部署、适应不同机队规模，并可作为剩余使用寿命模型的补充预警层。

Conclusion: 提出的无监督健康监测框架有效解决了故障标签稀缺问题，通过自适应阈值实现实时预警，可作为现有剩余使用寿命模型的补充，具有快速部署和良好扩展性。

Abstract: This paper introduces an unsupervised health-monitoring framework for turbofan engines that does not require run-to-failure labels. First, operating-condition effects in NASA CMAPSS sensor streams are removed via regression-based normalisation; then a Long Short-Term Memory (LSTM) autoencoder is trained only on the healthy portion of each trajectory. Persistent reconstruction error, estimated using an adaptive data-driven threshold, triggers real-time alerts without hand-tuned rules. Benchmark results show high recall and low false-alarm rates across multiple operating regimes, demonstrating that the method can be deployed quickly, scale to diverse fleets, and serve as a complementary early-warning layer to Remaining Useful Life models.

</details>


### [108] [Queueing-Aware Optimization of Reasoning Tokens for Accuracy-Latency Trade-offs in LLM Servers](https://arxiv.org/abs/2601.10274)
*Emre Ozbas,Melih Bastopcu*

Main category: cs.LG

TL;DR: 研究大型语言模型服务器如何为不同任务类型分配计算资源（思考令牌），在准确性和延迟之间取得平衡，通过队列理论和优化方法找到最优分配策略。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型服务器需要处理多种类型的查询任务，每种任务需要不同的计算资源。如何为不同任务类型分配计算资源（思考令牌）以平衡准确性（需要更多计算）和延迟（需要更快响应）是一个重要问题。

Method: 将系统建模为M/G/1队列，服务时间与分配的令牌数呈近似线性关系。建立约束优化问题，最大化加权平均准确性并惩罚平均系统时间，考虑令牌预算约束和队列稳定性条件。使用严格凹函数性质保证最优解存在唯一性，开发投影梯度法求解。

Result: 证明了目标函数在稳定区域内严格凹，确保最优解存在且唯一。提出了耦合投影固定点特征描述最优解，开发了迭代求解方法和投影梯度法。通过舍入连续解得到整数令牌分配，模拟评估了性能损失。

Conclusion: 该研究为LLM服务器的资源分配提供了理论框架和实用算法，能够在准确性和延迟之间实现最优权衡，并通过模拟验证了方法的有效性。

Abstract: We consider a single large language model (LLM) server that serves a heterogeneous stream of queries belonging to $N$ distinct task types. Queries arrive according to a Poisson process, and each type occurs with a known prior probability. For each task type, the server allocates a fixed number of internal thinking tokens, which determines the computational effort devoted to that query. The token allocation induces an accuracy-latency trade-off: the service time follows an approximately affine function of the allocated tokens, while the probability of a correct response exhibits diminishing returns. Under a first-in, first-out (FIFO) service discipline, the system operates as an $M/G/1$ queue, and the mean system time depends on the first and second moments of the resulting service-time distribution. We formulate a constrained optimization problem that maximizes a weighted average accuracy objective penalized by the mean system time, subject to architectural token-budget constraints and queue-stability conditions. The objective function is shown to be strictly concave over the stability region, which ensures existence and uniqueness of the optimal token allocation. The first-order optimality conditions yield a coupled projected fixed-point characterization of the optimum, together with an iterative solution and an explicit sufficient condition for contraction. Moreover, a projected gradient method with a computable global step-size bound is developed to guarantee convergence beyond the contractive regime. Finally, integer-valued token allocations are attained via rounding of the continuous solution, and the resulting performance loss is evaluated in simulation results.

</details>


### [109] [We Need a More Robust Classifier: Dual Causal Learning Empowers Domain-Incremental Time Series Classification](https://arxiv.org/abs/2601.10312)
*Zhipeng Liu,Peibo Duan,Xuan Tang,Haodong Jing,Mingyang Geng,Yongsheng Huang,Jialu Xu,Bin Zhang,Binwu Wang*

Main category: cs.LG

TL;DR: 提出DualCD框架，通过双重因果解耦增强时间序列分类在域增量学习中的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有时间序列分类研究在域增量学习方面面临挑战，需要提高模型在域增量场景下的鲁棒性

Method: 提出轻量级双重因果解耦框架(DualCD)，包括时间特征解耦模块和双重因果干预机制，分离类因果特征和伪特征

Result: 在多个数据集和模型上的实验表明，DualCD能有效提升域增量场景下的性能表现

Conclusion: DualCD框架能增强时间序列分类模型在域增量学习中的鲁棒性，并建立了全面的基准测试

Abstract: The World Wide Web thrives on intelligent services that rely on accurate time series classification, which has recently witnessed significant progress driven by advances in deep learning. However, existing studies face challenges in domain incremental learning. In this paper, we propose a lightweight and robust dual-causal disentanglement framework (DualCD) to enhance the robustness of models under domain incremental scenarios, which can be seamlessly integrated into time series classification models. Specifically, DualCD first introduces a temporal feature disentanglement module to capture class-causal features and spurious features. The causal features can offer sufficient predictive power to support the classifier in domain incremental learning settings. To accurately capture these causal features, we further design a dual-causal intervention mechanism to eliminate the influence of both intra-class and inter-class confounding features. This mechanism constructs variant samples by combining the current class's causal features with intra-class spurious features and with causal features from other classes. The causal intervention loss encourages the model to accurately predict the labels of these variant samples based solely on the causal features. Extensive experiments on multiple datasets and models demonstrate that DualCD effectively improves performance in domain incremental scenarios. We summarize our rich experiments into a comprehensive benchmark to facilitate research in domain incremental time series classification.

</details>


### [110] [Meta Dynamic Graph for Traffic Flow Prediction](https://arxiv.org/abs/2601.10328)
*Yiqing Zou,Hanning Yuan,Qianyu Yang,Ziqiang Yuan,Shuliang Wang,Sijie Ruan*

Main category: cs.LG

TL;DR: MetaDG是一个用于交通预测的新框架，通过动态图结构建模时空动态性，统一捕捉时空异质性，超越了传统仅关注拓扑动态的方法。


<details>
  <summary>Details</summary>
Motivation: 交通流预测的核心挑战在于建模复杂的时空依赖关系。现有方法存在两个主要局限：1）动态建模通常仅限于空间拓扑动态（如邻接矩阵变化），范围较窄；2）异质性建模通常将空间和时间维度分开处理，缺乏统一框架。

Method: 提出Meta Dynamic Graph (MetaDG)框架，利用节点表示的动态图结构来显式建模时空动态性。该方法不仅生成动态邻接矩阵，还生成元参数，将动态建模扩展到拓扑之外，并将时空异质性的捕捉统一到单一维度中。

Result: 在四个真实世界数据集上进行的广泛实验验证了MetaDG的有效性。

Conclusion: MetaDG通过动态图结构建模时空动态性，成功解决了现有方法在动态建模范围有限和时空异质性分离的问题，为交通预测提供了更有效的框架。

Abstract: Traffic flow prediction is a typical spatio-temporal prediction problem and has a wide range of applications. The core challenge lies in modeling the underlying complex spatio-temporal dependencies. Various methods have been proposed, and recent studies show that the modeling of dynamics is useful to meet the core challenge. While handling spatial dependencies and temporal dependencies using separate base model structures may hinder the modeling of spatio-temporal correlations, the modeling of dynamics can bridge this gap. Incorporating spatio-temporal heterogeneity also advances the main goal, since it can extend the parameter space and allow more flexibility. Despite these advances, two limitations persist: 1) the modeling of dynamics is often limited to the dynamics of spatial topology (e.g., adjacency matrix changes), which, however, can be extended to a broader scope; 2) the modeling of heterogeneity is often separated for spatial and temporal dimensions, but this gap can also be bridged by the modeling of dynamics. To address the above limitations, we propose a novel framework for traffic prediction, called Meta Dynamic Graph (MetaDG). MetaDG leverages dynamic graph structures of node representations to explicitly model spatio-temporal dynamics. This generates both dynamic adjacency matrices and meta-parameters, extending dynamic modeling beyond topology while unifying the capture of spatio-temporal heterogeneity into a single dimension. Extensive experiments on four real-world datasets validate the effectiveness of MetaDG.

</details>


### [111] [SuS: Strategy-aware Surprise for Intrinsic Exploration](https://arxiv.org/abs/2601.10349)
*Mark Kashirskiy,Ilya Makarov*

Main category: cs.LG

TL;DR: 提出Strategy-aware Surprise (SuS)框架，通过策略稳定性和策略惊喜度作为内在动机信号，提升强化学习中的探索效率，在数学推理任务上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于好奇心的探索方法仅依赖状态预测误差，存在局限性。需要更有效的内在动机信号来促进强化学习中的探索，特别是在复杂任务如数学推理中。

Method: 提出Strategy-aware Surprise (SuS)框架，包含两个互补组件：策略稳定性(SS)衡量行为策略在时间步上的一致性；策略惊喜度(SuS)捕捉相对于当前策略表示的意外结果。通过学习的权重系数结合这两个信号形成奖励。

Result: 在数学推理任务上，SuS相比基线方法在Pass@1上提升17.4%，在Pass@5上提升26.4%。消融研究表明移除任一组件会导致至少10%的性能下降，验证了方法的协同效应。

Conclusion: SuS框架通过结合策略稳定性和策略惊喜度，有效提升了强化学习中的探索效率，在数学推理任务上取得了显著性能提升，同时保持了更高的策略多样性。

Abstract: We propose Strategy-aware Surprise (SuS), a novel intrinsic motivation framework that uses pre-post prediction mismatch as a novelty signal for exploration in reinforcement learning. Unlike traditional curiosity-driven methods that rely solely on state prediction error, SuS introduces two complementary components: Strategy Stability (SS) and Strategy Surprise (SuS). SS measures consistency in behavioral strategy across temporal steps, while SuS captures unexpected outcomes relative to the agent's current strategy representation. Our combined reward formulation leverages both signals through learned weighting coefficients. We evaluate SuS on mathematical reasoning tasks using large language models, demonstrating significant improvements in both accuracy and solution diversity. Ablation studies confirm that removing either component results in at least 10% performance degradation, validating the synergistic nature of our approach. SuS achieves 17.4% improvement in Pass@1 and 26.4% improvement in Pass@5 compared to baseline methods, while maintaining higher strategy diversity throughout training.

</details>


### [112] [EvoMorph: Counterfactual Explanations for Continuous Time-Series Extrinsic Regression Applied to Photoplethysmography](https://arxiv.org/abs/2601.10356)
*Mesut Ceylan,Alexis Tabin,Patrick Langer,Elgar Fleisch,Filipe Barata*

Main category: cs.LG

TL;DR: EvoMorph：一种用于时间序列外生回归的多目标进化框架，可生成生理学上合理的反事实解释，支持临床时间序列应用的可信模型分析。


<details>
  <summary>Details</summary>
Motivation: 可穿戴设备能够连续监测生理信号（如PPG），为数据驱动的临床评估创造了新机会。然而，对于临床推理和信任，仅有点估计是不够的：临床医生需要了解预测在生理学合理变化下的稳定性，以及生理信号的实际变化如何影响模型预测。现有的时间序列反事实解释方法主要局限于分类任务，忽视波形形态，且常产生生理学上不合理的信号。

Method: 提出EvoMorph，一个多目标进化框架，用于为时间序列外生回归应用生成生理学上合理且多样化的反事实解释。该方法优化基于可解释信号描述符的形态感知目标，并应用变换来保持波形结构。

Result: 在三个PPG数据集（心率、呼吸频率和血氧饱和度）上评估EvoMorph，并与最近非相似邻居基线进行比较。此外，通过案例研究评估EvoMorph作为不确定性量化工具，将反事实敏感性与bootstrap集成不确定性和数据密度测量相关联。

Conclusion: EvoMorph能够为连续生物医学信号生成生理学感知的反事实解释，支持不确定性感知的可解释性，推进了临床时间序列应用的可信模型分析。

Abstract: Wearable devices enable continuous, population-scale monitoring of physiological signals, such as photoplethysmography (PPG), creating new opportunities for data-driven clinical assessment. Time-series extrinsic regression (TSER) models increasingly leverage PPG signals to estimate clinically relevant outcomes, including heart rate, respiratory rate, and oxygen saturation. For clinical reasoning and trust, however, single point estimates alone are insufficient: clinicians must also understand whether predictions are stable under physiologically plausible variations and to what extent realistic, attainable changes in physiological signals would meaningfully alter a model's prediction. Counterfactual explanations (CFE) address these "what-if" questions, yet existing time series CFE generation methods are largely restricted to classification, overlook waveform morphology, and often produce physiologically implausible signals, limiting their applicability to continuous biomedical time series. To address these limitations, we introduce EvoMorph, a multi-objective evolutionary framework for generating physiologically plausible and diverse CFE for TSER applications. EvoMorph optimizes morphology-aware objectives defined on interpretable signal descriptors and applies transformations to preserve the waveform structure. We evaluated EvoMorph on three PPG datasets (heart rate, respiratory rate, and oxygen saturation) against a nearest-unlike-neighbor baseline. In addition, in a case study, we evaluated EvoMorph as a tool for uncertainty quantification by relating counterfactual sensitivity to bootstrap-ensemble uncertainty and data-density measures. Overall, EvoMorph enables the generation of physiologically-aware counterfactuals for continuous biomedical signals and supports uncertainty-aware interpretability, advancing trustworthy model analysis for clinical time-series applications.

</details>


### [113] [PLGC: Pseudo-Labeled Graph Condensation](https://arxiv.org/abs/2601.10358)
*Jay Nandy,Arnab Kumar Mondal,Anuj Rathore,Mahesh Chandran*

Main category: cs.LG

TL;DR: PLGC是一种自监督图压缩方法，无需真实标签，通过伪标签和节点嵌入匹配原始图的结构和特征统计，在标签噪声和分布偏移下表现鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有图压缩方法依赖干净监督标签，在标签稀缺、噪声或不一致时可靠性受限，需要开发不依赖真实标签的自监督压缩框架。

Method: 提出伪标签图压缩(PLGC)框架：从节点嵌入构建潜在伪标签，联合学习潜在原型和节点分配，优化压缩图以匹配原始图的结构和特征统计。

Result: 在节点分类和链接预测任务中，PLGC在干净数据集上与最先进的监督压缩方法竞争，在标签噪声下表现出显著鲁棒性，通常大幅超越所有基线方法。

Conclusion: 自监督图压缩在噪声或弱标签环境中具有理论和实践优势，PLGC为标签不可靠场景提供了有效的图压缩解决方案。

Abstract: Large graph datasets make training graph neural networks (GNNs) computationally costly. Graph condensation methods address this by generating small synthetic graphs that approximate the original data. However, existing approaches rely on clean, supervised labels, which limits their reliability when labels are scarce, noisy, or inconsistent. We propose Pseudo-Labeled Graph Condensation (PLGC), a self-supervised framework that constructs latent pseudo-labels from node embeddings and optimizes condensed graphs to match the original graph's structural and feature statistics -- without requiring ground-truth labels. PLGC offers three key contributions: (1) A diagnosis of why supervised condensation fails under label noise and distribution shift. (2) A label-free condensation method that jointly learns latent prototypes and node assignments. (3) Theoretical guarantees showing that pseudo-labels preserve latent structural statistics of the original graph and ensure accurate embedding alignment. Empirically, across node classification and link prediction tasks, PLGC achieves competitive performance with state-of-the-art supervised condensation methods on clean datasets and exhibits substantial robustness under label noise, often outperforming all baselines by a significant margin. Our findings highlight the practical and theoretical advantages of self-supervised graph condensation in noisy or weakly-labeled environments.

</details>


### [114] [Reinforcement Learning with Multi-Step Lookahead Information Via Adaptive Batching](https://arxiv.org/abs/2601.10418)
*Nadav Merlis*

Main category: cs.LG

TL;DR: 研究具有多步前瞻信息的表格强化学习问题，提出自适应批处理策略（ABP）来解决固定批处理和模型预测控制的不足，设计了一种乐观的遗憾最小化算法来学习最优ABP。


<details>
  <summary>Details</summary>
Motivation: 在多步前瞻信息（观察未来ℓ步的转移和奖励实现）的强化学习中，虽然这些信息能显著提升价值，但寻找最优策略是NP难的。现有的两种启发式方法（固定批处理策略和模型预测控制）存在不足，需要更有效的策略来利用前瞻信息。

Method: 提出自适应批处理策略（ABP），将前瞻信息按状态依赖的自适应批次进行处理。推导了这些策略的最优贝尔曼方程，并设计了一种乐观的遗憾最小化算法，用于在未知环境中学习最优ABP。

Result: 获得了阶最优的遗憾界限（最多相差前瞻视野ℓ的一个因子），通常ℓ可以被视为小常数。这表明ABP在利用前瞻信息方面比固定批处理和模型预测控制更有效。

Conclusion: 自适应批处理策略（ABP）为解决具有前瞻信息的强化学习问题提供了有效的解决方案，通过自适应地利用前瞻信息，在保证计算可行性的同时实现了接近最优的性能。

Abstract: We study tabular reinforcement learning problems with multiple steps of lookahead information. Before acting, the learner observes $\ell$ steps of future transition and reward realizations: the exact state the agent would reach and the rewards it would collect under any possible course of action. While it has been shown that such information can drastically boost the value, finding the optimal policy is NP-hard, and it is common to apply one of two tractable heuristics: processing the lookahead in chunks of predefined sizes ('fixed batching policies'), and model predictive control. We first illustrate the problems with these two approaches and propose utilizing the lookahead in adaptive (state-dependent) batches; we refer to such policies as adaptive batching policies (ABPs). We derive the optimal Bellman equations for these strategies and design an optimistic regret-minimizing algorithm that enables learning the optimal ABP when interacting with unknown environments. Our regret bounds are order-optimal up to a potential factor of the lookahead horizon $\ell$, which can usually be considered a small constant.

</details>


### [115] [DeFlow: Decoupling Manifold Modeling and Value Maximization for Offline Policy Extraction](https://arxiv.org/abs/2601.10471)
*Zhancun Mu*

Main category: cs.LG

TL;DR: DeFlow是一个解耦的离线强化学习框架，利用流匹配技术捕捉复杂行为流形，通过轻量级精炼模块避免ODE求解器反向传播的计算负担，在保持迭代表达能力的同时实现稳定改进。


<details>
  <summary>Details</summary>
Motivation: 传统生成策略优化计算成本高，需要通过ODE求解器进行反向传播，而单步蒸馏会牺牲迭代生成能力。需要一种既能保持流匹配表达能力又能高效优化的方法。

Method: 提出解耦的离线RL框架，学习一个轻量级精炼模块，在流形显式的数据驱动信任区域内工作，避免求解器微分，消除损失项平衡需求，保持流的迭代表达能力。

Result: 在具有挑战性的OGBench基准测试中取得优越性能，并展示了高效的离线到在线适应能力。

Conclusion: DeFlow通过解耦设计和流匹配技术，在保持生成模型迭代表达能力的同时实现了高效优化，为离线RL提供了新的有效框架。

Abstract: We present DeFlow, a decoupled offline RL framework that leverages flow matching to faithfully capture complex behavior manifolds. Optimizing generative policies is computationally prohibitive, typically necessitating backpropagation through ODE solvers. We address this by learning a lightweight refinement module within an explicit, data-derived trust region of the flow manifold, rather than sacrificing the iterative generation capability via single-step distillation. This way, we bypass solver differentiation and eliminate the need for balancing loss terms, ensuring stable improvement while fully preserving the flow's iterative expressivity. Empirically, DeFlow achieves superior performance on the challenging OGBench benchmark and demonstrates efficient offline-to-online adaptation.

</details>


### [116] [Projected Microbatch Accumulation yields reference-free proximal policy updates for reinforcement learning](https://arxiv.org/abs/2601.10498)
*Nilin Abrahamsen*

Main category: cs.LG

TL;DR: PROMA是一种用于大语言模型微调的近端策略更新方法，通过投影去除序列级梯度分量，在微批次间累积策略梯度，实现更稳定的策略学习。


<details>
  <summary>Details</summary>
Motivation: 现有方法如PPO和GRPO在策略更新时存在熵崩溃、依赖参考策略或似然比裁剪等问题，需要一种更稳定、不依赖额外组件的近端策略更新方法。

Method: PROMA在反向传播过程中逐层投影去除序列级梯度分量，然后在微批次间累积策略梯度，无需额外的前向或反向传播，实现高效的近端更新。

Result: 与GRPO相比，PROMA能更严格地控制局部KL散度，实现更稳定的策略学习，且不会导致熵崩溃，也不依赖参考策略或似然比裁剪。

Conclusion: PROMA提供了一种有效的大语言模型微调方法，通过投影梯度累积实现稳定的近端策略更新，解决了现有方法的局限性。

Abstract: This note introduces Projected Microbatch Accumulation (PROMA), a proximal policy update method for large language model fine-tuning. PROMA accumulates policy gradients across microbatches by projecting out sequence-wise gradient components before microbatch aggregation. The projection is applied layer-wise during the backward pass, enabling efficient implementation without additional forward or backward passes. Empirically, PROMA enforces tighter control of local KL divergence than GRPO, resulting in more stable policy learning. Unlike PPO and GRPO, PROMA achieves proximal updates without inducing entropy collapse and does not rely on a reference policy or likelihood-ratio clipping.

</details>


### [117] [Transformer-Based Cognitive Radio: Adaptive Modulation Strategies Using Transformer Models](https://arxiv.org/abs/2601.10519)
*Andrea Melis,Andrea Piroddi,Roberto Girau*

Main category: cs.LG

TL;DR: 使用GPT-2 Transformer模型生成新型调制方案，与传统方法相比在SNR和PSD等指标上表现相当甚至更优


<details>
  <summary>Details</summary>
Motivation: 认知无线电系统需要动态适应频谱环境变化，机器学习技术特别是Transformer模型可以提升频谱效率、鲁棒性和安全性

Method: 使用GPT-2架构的Transformer模型，在现有调制公式数据集上进行训练，生成新的调制方案

Result: Transformer生成的调制方案在信噪比和功率谱密度等关键性能指标上与传统方法相当，某些情况下表现更优

Conclusion: Transformer模型可以显著增强认知无线电系统，实现更高效、鲁棒和安全的通信系统

Abstract: Cognitive Radio (CR) systems, which dynamically adapt to changing spectrum environments, could benefit significantly from advancements in machine learning technologies. These systems can be enhanced in terms of spectral efficiency, robustness, and security through innovative approaches such as the use of Transformer models. This work investigates the application of Transformer models, specifically the GPT-2 architecture, to generate novel modulation schemes for wireless communications. By training a GPT-2 model on a dataset of existing modulation formulas, new modulation schemes has been created. These generated schemes are then compared to traditional methods using key performance metrics such as Signal-to-Noise Ratio (SNR) and Power Spectrum Density (PSD). The results show that Transformer-generated modulation schemes can achieve performance comparable to, and in some cases outperforming, traditional methods. This demonstrates that advanced CR systems could greatly benefit from the implementation of Transformer models, leading to more efficient, robust, and secure communication systems.

</details>


### [118] [Mixtures of Transparent Local Models](https://arxiv.org/abs/2601.10541)
*Niffa Cheick Oumar Diaby,Thierry Duchesne,Mario Marchand*

Main category: cs.LG

TL;DR: 该论文提出了一种基于透明局部模型混合的方法来设计可解释模型，通过同时学习透明标注函数和输入空间局部区域，在保持模型透明度的同时处理不同区域间函数可能突变的情况。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在人类活动各领域的广泛应用导致对其透明度的需求日益增长。模型透明度对于识别安全性和非歧视性等因素至关重要。当前需要一种既能保持透明度又能处理复杂数据分布的方法。

Method: 提出透明局部模型混合方法，通过新的多预测器（多局部性）损失函数，同时学习透明标注函数和输入空间局部区域。建立了二元线性分类和线性回归问题的严格PAC-Bayesian风险界限。

Result: 使用合成数据集验证了学习算法的有效性，在真实数据集上的结果表明该方法与现有方法以及某些不透明模型相比具有竞争力。

Conclusion: 透明局部模型混合方法为设计可解释模型提供了一种有效的替代方案，能够在保持模型透明度的同时处理复杂的数据分布模式，特别是在不同局部区域间函数可能发生突变的情况下。

Abstract: The predominance of machine learning models in many spheres of human activity has led to a growing demand for their transparency. The transparency of models makes it possible to discern some factors, such as security or non-discrimination. In this paper, we propose a mixture of transparent local models as an alternative solution for designing interpretable (or transparent) models. Our approach is designed for the situations where a simple and transparent function is suitable for modeling the label of instances in some localities/regions of the input space, but may change abruptly as we move from one locality to another. Consequently, the proposed algorithm is to learn both the transparent labeling function and the locality of the input space where the labeling function achieves a small risk in its assigned locality. By using a new multi-predictor (and multi-locality) loss function, we established rigorous PAC-Bayesian risk bounds for the case of binary linear classification problem and that of linear regression. In both cases, synthetic data sets were used to illustrate how the learning algorithms work. The results obtained from real data sets highlight the competitiveness of our approach compared to other existing methods as well as certain opaque models. Keywords: PAC-Bayes, risk bounds, local models, transparent models, mixtures of local transparent models.

</details>


### [119] [Process-Guided Concept Bottleneck Model](https://arxiv.org/abs/2601.10562)
*Reza M. Asiyabi,SEOSAW Partnership,Steven Hancock,Casey Ryan*

Main category: cs.LG

TL;DR: PG-CBM扩展了概念瓶颈模型，通过领域定义的因果机制约束学习过程，使用生物物理意义的中介概念，在科学应用中提高准确性、透明度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 标准概念瓶颈模型（CBMs）存在三个主要问题：1）忽略领域特定关系和因果机制；2）依赖完整概念标签，限制了在监督稀疏但过程定义明确的科学领域的应用；3）缺乏对领域知识的整合。

Method: 提出过程引导概念瓶颈模型（PG-CBM），通过生物物理意义的中介概念约束学习遵循领域定义的因果机制，利用多源异构训练数据，产生可解释的中间输出。

Result: 使用地球观测数据的地上生物量密度估计作为案例研究，PG-CBM相比多个基准模型减少了误差和偏差，同时提高了透明度和可解释性。

Conclusion: PG-CBM不仅提高了准确性，还增强了透明度，能够检测虚假学习，提供科学洞察，代表了科学应用中更可信AI系统的一步。

Abstract: Concept Bottleneck Models (CBMs) improve the explainability of black-box Deep Learning (DL) by introducing intermediate semantic concepts. However, standard CBMs often overlook domain-specific relationships and causal mechanisms, and their dependence on complete concept labels limits applicability in scientific domains where supervision is sparse but processes are well defined. To address this, we propose the Process-Guided Concept Bottleneck Model (PG-CBM), an extension of CBMs which constrains learning to follow domain-defined causal mechanisms through biophysically meaningful intermediate concepts. Using above ground biomass density estimation from Earth Observation data as a case study, we show that PG-CBM reduces error and bias compared to multiple benchmarks, whilst leveraging multi-source heterogeneous training data and producing interpretable intermediate outputs. Beyond improved accuracy, PG-CBM enhances transparency, enables detection of spurious learning, and provides scientific insights, representing a step toward more trustworthy AI systems in scientific applications.

</details>


### [120] [Combinatorial Optimization Augmented Machine Learning](https://arxiv.org/abs/2601.10583)
*Maximilian Schiffer,Heiko Hoppe,Yue Su,Louis Bouvier,Axel Parmentier*

Main category: cs.LG

TL;DR: COAML（组合优化增强机器学习）综述：整合预测模型与组合决策的新范式，通过嵌入组合优化求解器构建数据驱动且保持可行性的策略，连接机器学习、运筹学和随机优化领域。


<details>
  <summary>Details</summary>
Motivation: COAML作为新兴范式，能够有效整合预测模型与组合决策，构建既数据驱动又保持可行性的策略。本文旨在提供该领域的全面概述，建立统一框架，并为未来研究提供路线图。

Method: 1. 提出COAML管道的统一框架；2. 描述方法论构建模块；3. 形式化与经验成本最小化的联系；4. 基于不确定性和决策结构建立问题设置分类法；5. 回顾静态和动态问题的算法方法；6. 调查调度、车辆路径、随机规划和强化学习等应用领域。

Result: 1. 建立了COAML的统一框架和分类体系；2. 系统回顾了静态和动态问题的算法方法；3. 全面调查了多个应用领域的COAML应用；4. 从经验成本最小化、模仿学习和强化学习角度综合了方法论贡献；5. 识别了关键研究前沿。

Conclusion: COAML是连接组合优化与机器学习的有力范式，本文提供了该领域的全面综述，既可作为入门教程，也可作为未来研究的路线图，推动两个领域的深度融合。

Abstract: Combinatorial optimization augmented machine learning (COAML) has recently emerged as a powerful paradigm for integrating predictive models with combinatorial decision-making. By embedding combinatorial optimization oracles into learning pipelines, COAML enables the construction of policies that are both data-driven and feasibility-preserving, bridging the traditions of machine learning, operations research, and stochastic optimization. This paper provides a comprehensive overview of the state of the art in COAML. We introduce a unifying framework for COAML pipelines, describe their methodological building blocks, and formalize their connection to empirical cost minimization. We then develop a taxonomy of problem settings based on the form of uncertainty and decision structure. Using this taxonomy, we review algorithmic approaches for static and dynamic problems, survey applications across domains such as scheduling, vehicle routing, stochastic programming, and reinforcement learning, and synthesize methodological contributions in terms of empirical cost minimization, imitation learning, and reinforcement learning. Finally, we identify key research frontiers. This survey aims to serve both as a tutorial introduction to the field and as a roadmap for future research at the interface of combinatorial optimization and machine learning.

</details>


### [121] [ProbFM: Probabilistic Time Series Foundation Model with Uncertainty Decomposition](https://arxiv.org/abs/2601.10591)
*Arundeep Chinta,Lucas Vinh Tran,Jay Katukuri*

Main category: cs.LG

TL;DR: 本文提出ProbFM，一种基于深度证据回归的Transformer概率框架，首次为时间序列基础模型提供理论基础的、可分解为认知和偶然不确定性的量化方法，在金融预测中保持竞争力的同时实现不确定性分解。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列基础模型在金融预测中面临不确定性量化的根本限制：现有方法要么依赖限制性分布假设，要么混淆不同不确定性来源，或缺乏理论校准机制。虽然近期TSFMs采用了混合模型、t分布或保形预测等技术，但未能解决提供理论基础的不确定性分解这一核心挑战。

Method: 提出ProbFM（概率基础模型），这是一种基于Transformer的概率框架，利用深度证据回归提供理论化的不确定性量化，具有明确的认知-偶然不确定性分解。与预先指定分布形式或需要基于采样推理的现有方法不同，ProbFM通过高阶证据学习学习最优不确定性表示，同时保持单次计算效率。

Result: 在加密货币回报预测评估中，DER（深度证据回归）保持了竞争力的预测准确性，同时提供了明确的认知-偶然不确定性分解。通过使用一致的LSTM架构对五种概率方法进行控制比较研究，验证了DER不确定性量化方法的有效性。

Conclusion: 这项工作为基于基础模型的理论不确定性量化建立了一个可扩展的框架，并为DER在金融应用中的有效性提供了实证证据，解决了当前TSFMs在不确定性量化方面的根本限制。

Abstract: Time Series Foundation Models (TSFMs) have emerged as a promising approach for zero-shot financial forecasting, demonstrating strong transferability and data efficiency gains. However, their adoption in financial applications is hindered by fundamental limitations in uncertainty quantification: current approaches either rely on restrictive distributional assumptions, conflate different sources of uncertainty, or lack principled calibration mechanisms. While recent TSFMs employ sophisticated techniques such as mixture models, Student's t-distributions, or conformal prediction, they fail to address the core challenge of providing theoretically-grounded uncertainty decomposition. For the very first time, we present a novel transformer-based probabilistic framework, ProbFM (probabilistic foundation model), that leverages Deep Evidential Regression (DER) to provide principled uncertainty quantification with explicit epistemic-aleatoric decomposition. Unlike existing approaches that pre-specify distributional forms or require sampling-based inference, ProbFM learns optimal uncertainty representations through higher-order evidence learning while maintaining single-pass computational efficiency. To rigorously evaluate the core DER uncertainty quantification approach independent of architectural complexity, we conduct an extensive controlled comparison study using a consistent LSTM architecture across five probabilistic methods: DER, Gaussian NLL, Student's-t NLL, Quantile Loss, and Conformal Prediction. Evaluation on cryptocurrency return forecasting demonstrates that DER maintains competitive forecasting accuracy while providing explicit epistemic-aleatoric uncertainty decomposition. This work establishes both an extensible framework for principled uncertainty quantification in foundation models and empirical evidence for DER's effectiveness in financial applications.

</details>


### [122] [Data-driven stochastic reduced-order modeling of parametrized dynamical systems](https://arxiv.org/abs/2601.10690)
*Andrew F. Ilersich,Kevin Course,Prasanth B. Nair*

Main category: cs.LG

TL;DR: 提出基于摊销随机变分推断的数据驱动框架，学习连续时间随机降阶模型，能够在参数空间和强迫条件下泛化，无需昂贵的前向求解器训练。


<details>
  <summary>Details</summary>
Motivation: 复杂动力系统在变化条件下的建模计算成本高，现有降阶模型方法难以处理随机动力学且无法量化预测不确定性，限制了在鲁棒决策中的实用性。

Method: 基于摊销随机变分推断，利用马尔可夫高斯过程的重参数化技巧，联合学习概率自编码器和控制潜在动力学的随机微分方程，训练成本与数据集大小和系统刚度无关。

Result: 在三个挑战性测试问题上展示了对未见参数组合和强迫条件的优秀泛化能力，相比现有方法获得了显著的效率提升。

Conclusion: 提出的框架能够有效学习连续时间随机降阶模型，实现跨参数空间和强迫条件的泛化，为复杂动力系统的建模提供了高效且概率性的解决方案。

Abstract: Modeling complex dynamical systems under varying conditions is computationally intensive, often rendering high-fidelity simulations intractable. Although reduced-order models (ROMs) offer a promising solution, current methods often struggle with stochastic dynamics and fail to quantify prediction uncertainty, limiting their utility in robust decision-making contexts. To address these challenges, we introduce a data-driven framework for learning continuous-time stochastic ROMs that generalize across parameter spaces and forcing conditions. Our approach, based on amortized stochastic variational inference, leverages a reparametrization trick for Markov Gaussian processes to eliminate the need for computationally expensive forward solvers during training. This enables us to jointly learn a probabilistic autoencoder and stochastic differential equations governing the latent dynamics, at a computational cost that is independent of the dataset size and system stiffness. Additionally, our approach offers the flexibility of incorporating physics-informed priors if available. Numerical studies are presented for three challenging test problems, where we demonstrate excellent generalization to unseen parameter combinations and forcings, and significant efficiency gains compared to existing approaches.

</details>


### [123] [Communication-Efficient and Privacy-Adaptable Mechanism -- a Federated Learning Scheme with Convergence Analysis](https://arxiv.org/abs/2601.10701)
*Chun Hei Michael Shiu,Chih Wei Ling*

Main category: cs.LG

TL;DR: CEPAM是一种联邦学习机制，通过随机向量量化器实现通信效率和隐私保护的平衡，本文对其隐私保证和收敛性进行理论分析，并通过实验评估其性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在数据治理约束下实现隐私保护协作，但面临通信效率和各方隐私保护的关键挑战，需要研究同时实现这两个目标的新方法。

Method: 采用CEPAM机制，利用拒绝采样通用量化器(RSUQ)这一随机向量量化器，其量化误差等价于预设噪声，可调节以定制各方隐私保护。

Result: 对CEPAM的隐私保证和收敛性进行理论分析，并通过实验评估其收敛曲线与基线对比，以及不同参与方之间的准确率-隐私权衡。

Conclusion: CEPAM在联邦学习中有效平衡通信效率和隐私保护，为数据治理约束下的协作学习提供了实用解决方案。

Abstract: Federated learning enables multiple parties to jointly train learning models without sharing their own underlying data, offering a practical pathway to privacy-preserving collaboration under data-governance constraints. Continued study of federated learning is essential to address key challenges in it, including communication efficiency and privacy protection between parties. A recent line of work introduced a novel approach called the Communication-Efficient and Privacy-Adaptable Mechanism (CEPAM), which achieves both objectives simultaneously. CEPAM leverages the rejection-sampled universal quantizer (RSUQ), a randomized vector quantizer whose quantization error is equivalent to a prescribed noise, which can be tuned to customize privacy protection between parties. In this work, we theoretically analyze the privacy guarantees and convergence properties of CEPAM. Moreover, we assess CEPAM's utility performance through experimental evaluations, including convergence profiles compared with other baselines, and accuracy-privacy trade-offs between different parties.

</details>


### [124] [Distributed Perceptron under Bounded Staleness, Partial Participation, and Noisy Communication](https://arxiv.org/abs/2601.10705)
*Keval Jain,Anant Raj,Saurav Prakash,Girish Varma*

Main category: cs.LG

TL;DR: 研究半异步客户端-服务器感知机训练，通过迭代参数混合（IPM风格平均）处理联邦学习中的系统效应：版本滞后、部分参与和通信噪声，提出带填充的陈旧桶聚合方法，证明有限时间错误边界。


<details>
  <summary>Details</summary>
Motivation: 解决联邦和分布式部署中的三个关键系统效应：1）由于模型交付延迟和客户端计算延迟应用导致的陈旧更新（双向版本滞后）；2）部分参与（客户端间歇性可用）；3）下行和上行链路上的不完美通信（建模为有界二阶矩的零均值加性噪声）。这些因素严重影响分布式感知机训练的性能。

Method: 提出一种服务器端聚合规则：带填充的陈旧桶聚合。该方法确定性地强制执行预定的陈旧度分布，而不假设延迟或参与的随机模型。在边缘可分离性和有界数据半径条件下，分析半异步客户端-服务器感知机训练，其中客户端运行本地感知机更新，服务器通过聚合每轮通信中到达的更新形成全局模型。

Result: 证明了给定服务器轮数内累积加权感知机错误数的有限时间期望边界：延迟的影响仅通过平均强制陈旧度体现，而通信噪声贡献一个额外项，该项随噪声总能量的平方根增长。在无噪声情况下，展示了有限期望错误预算如何在温和的新鲜参与条件下产生显式的有限轮稳定边界。

Conclusion: 提出的带填充的陈旧桶聚合方法能够有效处理联邦学习中的系统延迟、部分参与和通信噪声问题，为半异步感知机训练提供了理论保证，延迟影响可控，噪声影响可量化，在无噪声情况下可实现有限轮稳定。

Abstract: We study a semi-asynchronous client-server perceptron trained via iterative parameter mixing (IPM-style averaging): clients run local perceptron updates and a server forms a global model by aggregating the updates that arrive in each communication round. The setting captures three system effects in federated and distributed deployments: (i) stale updates due to delayed model delivery and delayed application of client computations (two-sided version lag), (ii) partial participation (intermittent client availability), and (iii) imperfect communication on both downlink and uplink, modeled as effective zero-mean additive noise with bounded second moment. We introduce a server-side aggregation rule called staleness-bucket aggregation with padding that deterministically enforces a prescribed staleness profile over update ages without assuming any stochastic model for delays or participation. Under margin separability and bounded data radius, we prove a finite-horizon expected bound on the cumulative weighted number of perceptron mistakes over a given number of server rounds: the impact of delay appears only through the mean enforced staleness, whereas communication noise contributes an additional term that grows on the order of the square root of the horizon with the total noise energy. In the noiseless case, we show how a finite expected mistake budget yields an explicit finite-round stabilization bound under a mild fresh-participation condition.

</details>


### [125] [High-accuracy and dimension-free sampling with diffusions](https://arxiv.org/abs/2601.10708)
*Khashayar Gatmiry,Sitan Chen,Adil Salim*

Main category: cs.LG

TL;DR: 提出了一种新的扩散模型求解器，通过低阶近似和配置方法的巧妙结合，将迭代复杂度从多项式降低到多对数级别，实现了首个仅需访问数据分布分数的高精度扩散采样器。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在采样多模态分布方面表现出色，但其推理需要数值求解微分方程。现有方法需要大量小迭代步长才能产生高质量样本，迭代复杂度随维度和精度呈多项式增长，效率较低。

Method: 提出新的扩散模型求解器，结合低阶近似和配置方法（Lee, Song, Vempala 2018）。该方法仅需要近似访问数据分布的分数，通过巧妙的数学设计降低迭代复杂度。

Result: 证明了新求解器的迭代复杂度在1/ε上呈多对数增长，而不是多项式增长。这是首个仅需访问数据分布分数就能实现高精度保证的扩散采样器。复杂度不显式依赖于环境维度，仅通过目标分布支撑集的有效半径影响。

Conclusion: 该方法显著提高了扩散模型采样效率，将迭代复杂度从多项式降低到多对数级别，为扩散模型的高效推理提供了理论保证，同时降低了维度依赖。

Abstract: Diffusion models have shown remarkable empirical success in sampling from rich multi-modal distributions. Their inference relies on numerically solving a certain differential equation. This differential equation cannot be solved in closed form, and its resolution via discretization typically requires many small iterations to produce \emph{high-quality} samples.
  More precisely, prior works have shown that the iteration complexity of discretization methods for diffusion models scales polynomially in the ambient dimension and the inverse accuracy $1/\varepsilon$. In this work, we propose a new solver for diffusion models relying on a subtle interplay between low-degree approximation and the collocation method (Lee, Song, Vempala 2018), and we prove that its iteration complexity scales \emph{polylogarithmically} in $1/\varepsilon$, yielding the first ``high-accuracy'' guarantee for a diffusion-based sampler that only uses (approximate) access to the scores of the data distribution. In addition, our bound does not depend explicitly on the ambient dimension; more precisely, the dimension affects the complexity of our solver through the \emph{effective radius} of the support of the target distribution only.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [126] [Trapping $\tfrac{h}{2e}$ Flux in Metals](https://arxiv.org/abs/2601.09847)
*Zohar Komargodski,Fedor K. Popov*

Main category: cond-mat.str-el

TL;DR: 正常金属中发现了新的磁通量子化现象：金属可以捕获0或h/2e（半磁通）的磁通量，这是由于背反应效应导致的。


<details>
  <summary>Details</summary>
Motivation: 研究正常金属在存在局域化磁通时的响应，探索金属中可能存在的新的磁通量子化现象。

Method: 研究了两种几何构型：1）被磁螺线管穿透的金属；2）包裹磁螺线管的金属。在第二种情况下，通过解析方法证明了磁通捕获效应。

Result: 发现金属由于背反应效应会捕获0或h/2e（半磁通）的磁通量。当螺线管绝热关闭时，存在对数增强的局域平衡电流，反映了费米气体的完美缺陷抗磁性。

Conclusion: 正常金属中存在着新的磁通量子化现象，金属可以捕获特定的磁通量（0或半磁通），这一发现揭示了金属中费米气体的独特电磁响应特性。

Abstract: We report on a new flux quantization phenomenon in metals. We study the response of normal metals to the presence of localized magnetic flux. We find that, due to backreaction effects, the metal traps 0 flux or $\tfrac{h}{2e}$ flux (half flux). We exhibit this effect both for metals pierced by magnetic solenoids and metals wrapping a magnetic solenoid. In the latter case we demonstrate the trapping of magnetic flux analytically. Furthermore, we find that as the solenoid is adiabatically turned off, a logarithmically enhanced localized equilibrium current persists, reflecting perfect defect-diamagnetism of the Fermi gas.

</details>


### [127] [Composite Bogoliubov Fermi liquid in a half-filled Chern band](https://arxiv.org/abs/2601.09924)
*Zhengyan Darius Shi,Pavel A. Nosov*

Main category: cond-mat.str-el

TL;DR: 在Chern能带中，复合费米子可以形成具有中性无隙Bogoliubov费米面的超导体，称为复合Bogoliubov费米液体，这是一种具有独特性质的新型拓扑相。


<details>
  <summary>Details</summary>
Motivation: 研究在无外磁场条件下，Chern能带中出现的反常复合费米液体，探索复合费米子配对态超越完全能隙Pfaffian相的可能性。

Method: 在反演不对称且晶格旋转对称性降至C3的Chern能带中，分析复合费米子形成具有中性无隙Bogoliubov费米面的超导态。

Result: 发现了复合Bogoliubov费米液体相，具有不可压缩性、量子化霍尔电导、无量子振荡、拓扑基态简并等独特性质，同时表现出金属性T线性比热、非量子化热导等特征。

Conclusion: 复合Bogoliubov费米液体代表了一种超越传统朗道能级范式的新型无隙拓扑相，为理解Chern能带中配对复合费米子开辟了新的研究方向。

Abstract: The composite Fermi liquid (CFL) in the half-filled Landau level is a cornerstone of the quantum Hall phase diagram. Recent experiments and numerics indicate that an anomalous composite Fermi liquid (ACFL) can also arise at half filling of a Chern band without any external magnetic field, opening new possibilities for paired states of composite fermions beyond the fully gapped Pfaffian phase. We argue that in inversion-asymmetric Chern bands with lattice rotational symmetry reduced to $C_3$, as realized in experimental platforms where signatures of the ACFL have been observed, composite fermions can form a superconductor with neutral gapless Bogoliubov Fermi surfaces. We term the resulting electronic state {\it the composite Bogoliubov Fermi liquid (CBFL)}. This phase has a number of remarkable properties that make it distinct from both the ACFL and the fully gapped Pfaffian. For instance, it is incompressible, has quantized Hall conductance, shows no quantum oscillations as a function of magnetic field or doping, and has topological ground state degeneracy on a torus despite the presence of gapless quasiparticles. At the same time, the neutral Bogoliubov Fermi surface yields metallic $T$-linear specific heat, non-quantized thermal conductance, Landau damping of density fluctuations, and a non-analytic $|\mathbf{q}|^3$ contribution to the equal-time structure factor $S(\mathbf{q})$. We also briefly discuss vortex physics and possible fractionalized daughter states induced by doping or external magnetic fields. Our results pave the way for a broader understanding of gapless topological phases arising from paired composite fermions in Chern bands that go beyond the conventional Landau level paradigm.

</details>


### [128] [Comparison of SCAN+U and r2SCAN+U for Charge Density Wave Instability and Lattice Dynamics in CuTe](https://arxiv.org/abs/2601.10146)
*Seungha Ju,Sooran Kim*

Main category: cond-mat.str-el

TL;DR: 该研究比较了meta-GGA泛函SCAN和r2SCAN在描述CuTe电荷密度波方面的性能，发现r2SCAN+U能更好地再现实验观测的结构和动力学特性。


<details>
  <summary>Details</summary>
Motivation: 选择合适的交换相关泛函和计算条件对于解释材料的基本物理性质和预测其特性至关重要，特别是在描述CuTe中的电荷密度波形成方面。

Method: 使用meta-GGA泛函SCAN和r2SCAN（带或不带Hubbard U参数），通过研究Te-Te键调制、声子色散和电子结构，分析两种泛函在描述CDW形成时的结构特性和动力学性质差异。

Result: r2SCAN+U能够重现实验观测到的CDW相中Te链畸变和非CDW相中qCDW=(0.4, 0.0, 0.5)处的声子软模，而SCAN表现出非物理的声子行为。尽管两种泛函的电子结构和优化晶格常数相似，但r2SCAN在描述CDW形成和晶格动力学方面更合适。

Conclusion: r2SCAN比SCAN更适合描述CuTe中的电荷密度波形成和晶格动力学，特别是在结合Hubbard U参数时能够准确再现实验观测的结构畸变和声子软模特性。

Abstract: Identifying an appropriate exchange-correlation functional and computational conditions is essential for explaining the fundamental physics of materials and predicting their properties. Here, we investigate the performance of the meta-GGA functionals SCAN and r2SCAN, with and without a Hubbard U, for describing the charge density wave (CDW) in the quasi-one-dimensional material CuTe. By examining the Te-Te bond modulation, phonon dispersions, and electronic structures, we identify clear differences in how the two functionals capture the structural and dynamical properties of the CDW formation. r2SCAN+U reproduces the experimentally observed Te-chain distortions in the CDW phase and the phonon soft mode at qCDW=(0.4, 0.0, 0.5) in the non-CDW phase, whereas SCAN exhibits unphysical phonon behavior. The atomic displacements of the soft mode agree well with the experimental Te modulation. Despite their similar electronic structures and optimized lattice constants, our results demonstrate that r2SCAN is a more suitable choice than SCAN for describing CDW formation and lattice dynamics in CuTe.

</details>


### [129] [Electric field effects in one-dimensional spin-1/2 $K_1J_1Γ_1Γ_1^\prime K_2J_2$ model with ferromagnetic Kitaev coupling](https://arxiv.org/abs/2601.10158)
*Wang Yang,Helin Wang,Chao Xu*

Main category: cond-mat.str-el

TL;DR: 研究一维自旋-1/2 K₁J₁Γ₁Γ₁'K₂J₂模型中Luttinger液体相在电场下的行为，发现(1,1,1)方向电场保持Luttinger液体特性，其他方向电场则导致系统进入二聚化状态。


<details>
  <summary>Details</summary>
Motivation: 研究电场对一维广义Kitaev自旋模型中Luttinger液体相的影响，为理解电场调控一维自旋系统提供理论基础，并为探索二维系统中的电场相关物理提供起点。

Method: 对一维自旋-1/2 K₁J₁Γ₁Γ₁'K₂J₂模型在最近邻铁磁Kitaev耦合区域进行系统研究，分析不同方向电场对系统行为的影响。

Result: (1,1,1)方向电场维持系统的Luttinger液体行为，而其他方向电场驱动系统进入二聚化状态。对(1,1,1)方向电场在真实材料中调控Luttinger参数的有效性进行了估计。

Conclusion: 该研究有助于理解电场在一维广义Kitaev自旋模型中的作用，为基于准一维方法探索二维系统中的电场相关物理提供了起点。

Abstract: We perform a systematic study on the effects of electric fields in the Luttinger liquid phase of the one-dimensional spin-$1/2$ $K_1J_1Γ_1Γ_1^\prime K_2J_2$ model in the region of ferromagnetic nearest-neighboring Kitaev coupling. We find that while electric fields along $(1,1,1)$-direction maintain the Luttinger liquid behavior, fields along other directions drive the system to a dimerized state. An estimation is made on how effective a $(1,1,1)$-field is for tuning the Luttinger parameter in real materials. Our work is useful for understanding the effects of electric fields in one-dimensional generalized Kitaev spin models, and provides a starting point for exploring the electric-field-related physics in two dimensions based on a quasi-one-dimensional approach.

</details>


### [130] [Magnetic field-induced phases in a model S=1 Haldane chain system](https://arxiv.org/abs/2601.10489)
*I. Jakovac,M. S. Grbić,M. Dupont,N. Laflorencie,S. Capponi,Y. Hosokoshi,S. Krämer,Y. Skourski,S. Luther M. Takigawa,M. Horvatić*

Main category: cond-mat.str-el

TL;DR: 该研究通过有机Haldane链系统BoNO首次完整探索了S=1 Haldane链在磁场作用下的完整相图，包括Tomonaga-Luttinger液体和玻色-爱因斯坦凝聚相，验证了理论预测。


<details>
  <summary>Details</summary>
Motivation: 以往基于过渡金属复合物的S=1 Haldane链实验系统存在各向异性或J_1D值过大等问题，限制了完整相图的探索。需要寻找无各向异性且J_1D适中的系统来验证理论预测。

Method: 使用有机Haldane链系统3,5-双(N-叔丁基氨基氧基)-3'-硝基联苯(BoNO)，结合^1H核磁共振技术和理论分析，表征TLL特性，绘制BEC相边界，确定临界指数。

Result: 成功探索了完整的B-T相图，表征了TLL性质，绘制了BEC相边界T_c(B)，在B_c2处测得临界指数ν≈0.66，并在量子临界区域展示了普适的准粒子标度行为。

Conclusion: 该研究首次通过无各向异性且J_1D适中的有机Haldane链系统，完整验证了二十多年前提出的S=1 Haldane链在磁场诱导下的相变理论预测。

Abstract: An $S=1$ Haldane chain is a one-dimensional (1D) quantum magnet where strong fluctuations result in quantum disordered singlet ground state with a gapped excitation spectrum. The gap magnitude is primarily set by the dominant intrachain interaction ($J_\text{1D}$). An applied magnetic field closes the gap at $B_\text{c1}$ and drives the system into a gapless Tomonaga-Luttinger liquid (TLL) regime, followed by, at lower temperatures, a Bose-Einstein condensate (BEC) ground state, persisting up to $B_\text{c2} \propto 4 J_\text{1D}/gμ_B$. Almost all previously studied experimental realizations of such systems were based on transition-metal complexes which typically suffer from intrinsic anisotropies or large $J_\text{1D}$ values, limiting the access to the full theoretical phase diagram. We report a comprehensive study of TLL and BEC phases in the organic Haldane chain system 3,5-bis(N-tert-butylaminoxyl)-3'-nitrobiphenyl (BoNO). The absence of anisotropy and a moderate $J_\text{1D}$ enable exploration of the complete $B-T$ phase diagram. Through $^1$H nuclear magnetic resonance, combined with theoretical analysis, we characterize the TLL properties, map the BEC phase boundary $T_c (B)$, determine the associated critical exponent $ν\approx 0.66$ at $B_\text{c2}$, and demonstrate universal quasiparticle scaling in the quantum-critical regime. These results provide full experimental validation of theoretical predictions for field-induced phases in an $S=1$ Haldane chain, made over two decades ago.

</details>


### [131] [Correlated states in charge-transfer heterostructures based on rhombohedral multilayer graphene](https://arxiv.org/abs/2601.10530)
*Yanran Shi,Min Li,Xin Lu,Jianpeng Liu*

Main category: cond-mat.str-el

TL;DR: 该论文研究了菱面体多层石墨烯与绝缘衬底异质结构中的电荷转移效应，揭示了通过栅压调控能带对齐和层间电荷分布，为研究拓扑和激子相关态提供了新平台。


<details>
  <summary>Details</summary>
Motivation: 研究电荷转移在范德华异质结构中的作用，探索通过静电栅压调控能带对齐和层间电荷分布，为研究耦合双层相关电子系统提供可调平台。

Method: 开发了包含电荷转移效应的层电荷密度自洽静电理论，研究了菱面体多层石墨烯与绝缘衬底异质结构，分析了不同有效质量条件下的物理行为。

Result: 当衬底有效质量远大于石墨烯时，衬底载流子形成维格纳晶体，在石墨烯层诱导拓扑平带；当有效质量相当时，在电荷中性点发现由层间库仑耦合稳定的层间激子绝缘态。

Conclusion: 电荷转移异质结构为拓扑和激子相关态研究提供了丰富平台，开启了"电荷转移电子学"的新研究途径。

Abstract: Charge transfer is a common phenomenon in van der Waals heterostructures with proper work function mismatch, which enables electrostatic gating to control band alignment and interlayer charge distributions. This provides a tunable platform for studying coupled bilayer correlated electronic systems. Here, we theoretically investigate heterostructures of rhombohedral multilayer graphene (RMG) and an insulating substrate with gate-tunable band alignment. We first develop a self-consistent electrostatic theory for layer charge densities incorporating charge transfer, which reproduces the experimentally observed broadened and bent charge neutrality region. When the substrate's band edge has a much larger effective mass than RMG, its carriers can form a Wigner crystal at low densities. This creates a quantum superlattice that induces topological flat bands in the RMG layer, which may lead to Chern insulators driven by intralayer Coulomb interactions. Conversely, with comparable effective masses, we find an interlayer excitonic insulator state at charge neutrality stabilized by interlayer Coulomb coupling. Our work establishes these charge-transfer heterostructures as a rich platform for topological and excitonic correlated states, opening an avenue for ``charge-transferonics''.

</details>


### [132] [Flat-band Ferromagnetism of SU$(N)$ Hubbard Model on the Kagome Lattices](https://arxiv.org/abs/2601.10549)
*Hao Jin,Wenxing Nie*

Main category: cond-mat.str-el

TL;DR: 该研究通过渗流框架分析kagome晶格上SU(N) Hubbard模型中的顺磁-铁磁转变，发现铁磁性的临界粒子浓度超过标准渗流阈值且随N增大而增加


<details>
  <summary>Details</summary>
Motivation: kagome晶格作为几何阻挫系统的典型代表，具有色散平坦能带，为研究关联驱动的量子现象提供了独特平台。在特定粒子浓度下，平坦能带允许具有非平凡权重的渗流表示，这为理解SU(N)对称性下的磁性转变提供了新视角。

Method: 将SU(N) Hubbard模型严格映射到三角晶格上的经典N态位点渗流问题，其中SU(N)对称性体现在非平凡权重中。通过大规模蒙特卡洛模拟，研究了SU(3)、SU(4)和SU(10)对称性下的相变行为。

Result: 铁磁性的临界粒子浓度超过标准渗流阈值，并且随着对称性参数N的增加而增加。这表明有效熵排斥作用增强，反映了SU(N)对称性对磁性转变的重要影响。

Conclusion: 在kagome晶格的SU(N) Hubbard模型中，通过渗流框架成功揭示了顺磁-铁磁转变的临界行为。铁磁相的出现需要比标准渗流阈值更高的粒子浓度，且这种要求随对称性N的增大而增强，为理解多体系统中的磁性相变提供了新见解。

Abstract: The kagome lattice, a well known example of the geometrically frustrated system, hosts a dispersionless flat band that offers a unique platform for studying correlation-driven quantum phenomena. At appropriate particle concentrations, the existence of a flat band allows a representation of percolation with nontrivial weights. In this work, we investigate the paramagnetic-ferromagnetic transition in the repulsive SU($N$) Hubbard model on the kagome lattice within this percolation framework. In this representation, the model can be rigorously mapped to a classical $N$-state site-percolation problem on a triangular lattice, with the SU($N$) symmetry reflected in the nontrivial weights. By large-scale Monte Carlo simulations for SU($3$), SU($4$), and SU($10$) symmetries, we demonstrate that the critical particle concentration for ferromagnetism exceeds the standard percolation threshold and increases with $N$, indicating a strengthening of the effective entropic repulsion.

</details>


### [133] [Beyond Hubbard: the role of correlated hopping interaction in superconductors and quantum dot devices](https://arxiv.org/abs/2601.10619)
*Karol I. Wysokiński,Marcin M. Wysokiński*

Main category: cond-mat.str-el

TL;DR: 该研究探讨了超越标准Hubbard模型的强库仑相互作用在两种物理背景下的作用：超导相变和量子点输运特性，重点关注关联（辅助）跳跃相互作用的影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解强库仑相互作用在凝聚态物理中的重要作用，特别是在标准Hubbard模型之外的相关效应。作者关注关联跳跃相互作用如何影响超导相变和量子点输运特性，这对于理解强关联系统的物理行为具有重要意义。

Method: 研究方法包括：1）分析Mott金属-绝缘体转变附近的超导相变；2）研究耦合到外部电极的量子点人工纳米结构的输运特性。在这两种情况下，都重点关注关联（辅助）跳跃相互作用的影响，并展示系统在不同模型参数下的谱函数演化。

Result: 研究发现：1）对于超导体，关联跳跃相互作用是相变的驱动机制，并改变系统的谱特性；2）在量子点器件中，关联跳跃相互作用影响量子点与金属引线之间的隧穿振幅；3）正常金属-量子点-正常金属结构的电导特征变化为关联跳跃相互作用的存在和符号提供了清晰的信号。

Conclusion: 结论表明关联跳跃相互作用在强关联系统中起着关键作用，不仅驱动超导相变，还显著影响量子点器件的输运特性。该研究为实验检测这种相互作用提供了理论基础，并深化了对超越标准Hubbard模型的强库仑相互作用的理解。

Abstract: We investigate the role of strong Coulomb interactions beyond the standard Hubbard model in two distinct physical contexts. First, we analyze the superconducting phase transition occurring near the Mott metal-insulator transition. Second, we study transport properties of artificial nano-scale structures containing quantum dots coupled to external electrodes. In both cases, we focus on the impact of the correlated (assisted) hopping (CH) interaction. For superconductors, CH acts as a driving mechanism for the phase transition and modifies the spectral properties of the system. We present the evolution of the spectral function as the system approaches the Mott-type transition under varying model parameters. In quantum-dot-based devices, CH influences the tunneling amplitude between the dot and metallic leads. We demonstrate that the characteristic changes in the conductance of a normal metal-quantum dot-normal metal structure provide a clear signature of the presence and sign of CH interaction.

</details>


### [134] [Emergence and transition of incompressible phases in decorated Landau levels](https://arxiv.org/abs/2601.10717)
*Bo Peng,Yuzhu Wang,Bo Yang*

Main category: cond-mat.str-el

TL;DR: 单朗道能级加上周期性静电势可实现多种相互作用拓扑相，霍尔电导通常不等于朗道能级填充因子。通过delta势晶格模型实现精确零能陈能带（装饰朗道能级），与具有丰富几何特性的色散能带分离。


<details>
  <summary>Details</summary>
Motivation: 探索在周期性势场修饰的单朗道能级中实现新型相互作用拓扑相的可能性，这些相中的霍尔电导通常不等于朗道能级填充因子，为相关物理提供新的理论模型和实验平台。

Method: 提出最小模型：在单朗道能级中引入delta势晶格，实现精确零能陈能带（装饰朗道能级）。当每个晶胞有p/q磁通量时，产生q个色散能带和p-q个零能带。分析单体能势与电子-电子相互作用的竞争关系。

Result: 装饰朗道能级与色散能带分离，色散能带具有非平凡贝里曲率分布，甚至在q>1时具有非零陈数。在强短程相互作用极限下，低填充因子时能带混合被强烈抑制，单体能势稳定了装饰朗道能级内的鲁棒拓扑相。

Conclusion: 装饰朗道能级及相关色散能带可作为晶格或莫尔系统中相关物理的最小理论模型，同时也是实现丰富二维量子流体相图的高度可调实验平台。

Abstract: We show a single Landau level (LL) dressed with periodic electrostatic potentials can realize a plethora of interacting topological phases where the Hall conductivity generally does not equal to the LL filling factor. Their physics can be captured by a minimal model of a delta potential lattice within a single LL, realizing exact zero energy Chern bands (denoted as decorated Landau levels or dLL) gapped from dispersive bands with rich geometric properties. With $p/q$ magnetic fluxes per unit cell, there are $q$ dispersive bands and $p-q$ zero energy bands forming the dLL. When the one-body potential strength dominates the electron-electron interaction, band mixing is suppressed and the dispersion bands consist of ``localized states" with vanishing total Chern number. Nevertheless these dispersive bands can have highly nontrivial Berry curvature distribution, and even non-zero Chern numbers when $q>1$. Interestingly even in the limit of large short range interaction, band mixing between dLL and dispersion bands can be strongly suppressed at low filling factor, leading to robust topological phases within the dLL stabilized by the one-body potential. The dLL and the associated dispersive bands can serve as minimal theoretical models for correlated physics in lattice or moire systems; they are also highly tunable experimental platforms for realizing rich phase diagrams of exotic 2D quantum fluids.

</details>


<div id='nlin.AO'></div>

# nlin.AO [[Back]](#toc)

### [135] [Vibrational resonance in coupled self-learning Duffing oscillators and its application in noisy radio frequency signal processing](https://arxiv.org/abs/2601.09743)
*Jianhua Yang,Litai Lou,Shangyuan Li,Zhongqiu Wang,Miguel A. F. Sanjuán*

Main category: nlin.AO

TL;DR: 提出了一种频率自适应杜芬振子耦合阵列，通过学习规则实现频率自适应能力，扩展了振动共振的频率范围，并展示了在强噪声环境下的信号去噪性能。


<details>
  <summary>Details</summary>
Motivation: 传统振动共振方法在频率范围上有限制，需要开发具有频率自适应能力的新型振子系统，以扩展振动共振的应用范围并提升在强噪声环境下的信号处理性能。

Method: 设计了频率自适应杜芬振子耦合阵列，每个振子的固有频率通过学习规则随外部激励变化；推导了振动共振的理论条件，并通过数值模拟验证；应用于模拟信号和无线射频信号的去噪处理。

Result: 频率自适应学习规则显著扩展了振动共振的频率范围；系统在强噪声环境下表现出优异的信号去噪性能；实验证明其性能优于小波变换和卡尔曼滤波等传统方法。

Conclusion: 频率自适应杜芬振子耦合阵列结合了宽带频率适应性和强噪声抑制能力，在工程应用中具有重要潜力，特别是在复杂噪声环境下的信号处理领域。

Abstract: This work presents a new coupled array of frequency-adaptive Duffing oscillators. Based on learning rules, the natural frequency of each oscillator changes with the external excitation to achieve the frequency-adaptive capability in the response. The frequency range of vibrational resonance in the response is greatly extended through the frequency-adaptive learning rule. Moreover, the theoretical condition for vibrational resonance is derived and its validity is verified numerically. The coupled self-learning Duffing oscillators can also perform signal denoising in strong noise environment, and its performance in signal denoising has been verified through processing the simulated signal and the wireless radio frequency signal under two scenarios. The superiority of vibrational resonance to the conventional denosing methods such as wavelet transform and Kalman filter has also been illustrated by experimental radio frequency signal processing. The combination of broadband frequency adaptability and strong noise-reduction capability suggests that these oscillators hold considerable potential for engineering applications.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [136] [AI Survival Stories: a Taxonomic Analysis of AI Existential Risk](https://arxiv.org/abs/2601.09765)
*Herman Cappelen,Simon Goldstein,John Hawthorne*

Main category: cs.AI

TL;DR: 该论文提出了一个分析AI系统对人类构成生存风险的通用框架，基于两个前提构建了人类生存故事的分类法，并讨论了不同生存故事面临的挑战、相应的应对策略，以及AI毁灭人类的概率估计。


<details>
  <summary>Details</summary>
Motivation: 自ChatGPT发布以来，关于AI系统是否对人类构成生存风险的争论日益激烈。论文旨在建立一个系统性的框架来分析AI的生存风险问题，帮助理解不同观点背后的逻辑结构。

Method: 论文构建了一个基于两个前提的论证框架：前提一：AI系统将变得极其强大；前提二：如果AI系统变得极其强大，它们将毁灭人类。基于这两个前提，作者创建了一个生存故事分类法，其中每个故事都对应着其中一个前提的失败。然后分析不同生存故事面临的挑战，以及它们所暗示的不同应对策略。

Result: 论文提出了四种主要的人类生存故事：1）科学障碍阻止AI变得极其强大；2）人类禁止AI研究；3）极其强大的AI因其目标而不会毁灭人类；4）人类能够可靠地检测和禁用具有毁灭人类目标的AI系统。每种生存故事都面临不同的挑战，并需要不同的应对措施。

Conclusion: 该框架为理解AI生存风险提供了系统性的分析工具，帮助识别不同观点之间的根本分歧。通过这一分类法，作者能够对P(doom)——AI毁灭人类的概率——进行粗略估计，并为不同的风险情景制定相应的应对策略。

Abstract: Since the release of ChatGPT, there has been a lot of debate about whether AI systems pose an existential risk to humanity. This paper develops a general framework for thinking about the existential risk of AI systems. We analyze a two premise argument that AI systems pose a threat to humanity. Premise one: AI systems will become extremely powerful. Premise two: if AI systems become extremely powerful, they will destroy humanity. We use these two premises to construct a taxonomy of survival stories, in which humanity survives into the far future. In each survival story, one of the two premises fails. Either scientific barriers prevent AI systems from becoming extremely powerful; or humanity bans research into AI systems, thereby preventing them from becoming extremely powerful; or extremely powerful AI systems do not destroy humanity, because their goals prevent them from doing so; or extremely powerful AI systems do not destroy humanity, because we can reliably detect and disable systems that have the goal of doing so. We argue that different survival stories face different challenges. We also argue that different survival stories motivate different responses to the threats from AI. Finally, we use our taxonomy to produce rough estimates of P(doom), the probability that humanity will be destroyed by AI.

</details>


### [137] [GUI-Eyes: Tool-Augmented Perception for Visual Grounding in GUI Agents](https://arxiv.org/abs/2601.09770)
*Chen Chen,Jiawei Shao,Dakuan Lu,Haoyi Hu,Xiangcheng Liu,Hantao Yao,Wu Liu*

Main category: cs.AI

TL;DR: GUI-Eyes是一个强化学习框架，通过主动视觉感知解决GUI自动化任务，采用两阶段推理和渐进感知策略，在ScreenSpot-Pro基准上仅用3k样本达到44.8%的定位准确率。


<details>
  <summary>Details</summary>
Motivation: 现有GUI自动化方法依赖静态、一次性视觉输入和被动感知，缺乏自适应决定何时、是否以及如何观察界面的能力。需要开发能够主动获取信息性观察的智能体。

Method: 提出强化学习框架GUI-Eyes，采用两阶段推理过程：智能体学习在粗粒度探索和细粒度定位中战略性地决定是否以及如何调用视觉工具（如裁剪或缩放）。引入渐进感知策略，通过两级策略协调决策制定。设计针对工具使用的空间连续奖励函数，整合位置接近度和区域重叠度，提供密集监督。

Result: 在ScreenSpot-Pro基准测试中，GUI-Eyes-3B仅使用3k标记样本就实现了44.8%的定位准确率，显著优于监督学习和基于RL的基线方法。

Conclusion: 工具感知的主动感知，通过分阶段策略推理和细粒度奖励反馈实现，对于构建稳健且数据高效的GUI智能体至关重要。GUI-Eyes框架展示了在GUI任务中主动视觉感知的有效性。

Abstract: Recent advances in vision-language models (VLMs) and reinforcement learning (RL) have driven progress in GUI automation. However, most existing methods rely on static, one-shot visual inputs and passive perception, lacking the ability to adaptively determine when, whether, and how to observe the interface. We present GUI-Eyes, a reinforcement learning framework for active visual perception in GUI tasks. To acquire more informative observations, the agent learns to make strategic decisions on both whether and how to invoke visual tools, such as cropping or zooming, within a two-stage reasoning process. To support this behavior, we introduce a progressive perception strategy that decomposes decision-making into coarse exploration and fine-grained grounding, coordinated by a two-level policy. In addition, we design a spatially continuous reward function tailored to tool usage, which integrates both location proximity and region overlap to provide dense supervision and alleviate the reward sparsity common in GUI environments. On the ScreenSpot-Pro benchmark, GUI-Eyes-3B achieves 44.8% grounding accuracy using only 3k labeled samples, significantly outperforming both supervised and RL-based baselines. These results highlight that tool-aware active perception, enabled by staged policy reasoning and fine-grained reward feedback, is critical for building robust and data-efficient GUI agents.

</details>


### [138] [Antisocial behavior towards large language model users: experimental evidence](https://arxiv.org/abs/2601.09772)
*Paweł Niszczota,Cassandra Grützner*

Main category: cs.AI

TL;DR: 研究发现：人们愿意花费个人资源惩罚使用LLM完成任务的人，惩罚强度随实际使用程度单调增加，且存在"可信度鸿沟"——声称未使用比实际未使用受到更严厉惩罚


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速普及，人们对其引发的社会反应产生担忧。先前研究记录了人们对AI使用者的负面态度，但尚不清楚这种不认同是否会转化为实际的代价性行为。

Method: 采用两阶段在线实验设计：第一阶段提供目标对象，第二阶段招募491名参与者。参与者可以花费自己的部分资金来减少那些先前使用或不使用LLM支持完成实际努力任务的同伴的收入。

Result: 参与者平均销毁了完全依赖LLM者36%的收入，惩罚强度随实际LLM使用程度单调增加。关于LLM使用的披露存在可信度鸿沟：自我报告未使用者比实际未使用者受到更严厉惩罚，表明"未使用"声明被怀疑；而在高度使用情况下，实际依赖比自我报告依赖受到更强烈惩罚。

Conclusion: 这些发现首次提供了行为证据，表明LLM的效率提升是以社会制裁为代价的，揭示了AI使用在社会互动中引发的实际惩罚行为。

Abstract: The rapid spread of large language models (LLMs) has raised concerns about the social reactions they provoke. Prior research documents negative attitudes toward AI users, but it remains unclear whether such disapproval translates into costly action. We address this question in a two-phase online experiment (N = 491 Phase II participants; Phase I provided targets) where participants could spend part of their own endowment to reduce the earnings of peers who had previously completed a real-effort task with or without LLM support. On average, participants destroyed 36% of the earnings of those who relied exclusively on the model, with punishment increasing monotonically with actual LLM use. Disclosure about LLM use created a credibility gap: self-reported null use was punished more harshly than actual null use, suggesting that declarations of "no use" are treated with suspicion. Conversely, at high levels of use, actual reliance on the model was punished more strongly than self-reported reliance. Taken together, these findings provide the first behavioral evidence that the efficiency gains of LLMs come at the cost of social sanctions.

</details>


### [139] [Improving Chain-of-Thought for Logical Reasoning via Attention-Aware Intervention](https://arxiv.org/abs/2601.09805)
*Nguyen Minh Phuong,Dang Huu Tien,Naoya Inoue*

Main category: cs.AI

TL;DR: 提出了一种非交互式端到端推理框架AAI，通过注意力重加权机制增强LLMs的逻辑推理能力，无需外部资源且计算开销小。


<details>
  <summary>Details</summary>
Motivation: 现有逻辑推理方法依赖复杂的交互框架或外部符号求解器，存在额外开销和可扩展性限制，需要一种非交互式、端到端的推理框架。

Method: 提出注意力感知干预(AAI)：1) 通过few-shot提示激活与逻辑推理操作符对齐的注意力头；2) 在推理时对这些注意力头的注意力分数进行重加权，引导模型利用先验知识。

Result: AAI在多种基准测试和模型架构上显著提升了逻辑推理性能，同时仅带来可忽略的计算开销。

Conclusion: AAI提供了一种高效的非交互式端到端推理框架，通过注意力调制增强LLMs的逻辑推理能力，具有良好的泛化性和可分析性。

Abstract: Modern logical reasoning with LLMs primarily relies on employing complex interactive frameworks that decompose the reasoning process into subtasks solved through carefully designed prompts or requiring external resources (e.g., symbolic solvers) to exploit their strong logical structures. While interactive approaches introduce additional overhead, hybrid approaches depend on external components, which limit their scalability. A non-interactive, end-to-end framework enables reasoning to emerge within the model itself -- improving generalization while preserving analyzability without any external resources. In this work, we introduce a non-interactive, end-to-end framework for reasoning tasks. We show that introducing structural information into the few-shot prompt activates a subset of attention heads that patterns aligned with logical reasoning operators. Building on this insight, we propose Attention-Aware Intervention (AAI), an inference-time intervention method that reweights attention scores across selected heads identified by their logical patterns. AAI offers an efficient way to steer the model's reasoning toward leveraging prior knowledge through attention modulation. Extensive experiments show that AAI enhances logical reasoning performance across diverse benchmarks and model architectures, while incurring negligible additional computational overhead. Code is available at https://github.com/phuongnm94/aai_for_logical_reasoning.

</details>


### [140] [Thinking Long, but Short: Stable Sequential Test-Time Scaling for Large Reasoning Models](https://arxiv.org/abs/2601.09855)
*Michael R. Metel,Yufei Cui,Boxing Chen,Prasanna Parthasarathi*

Main category: cs.AI

TL;DR: Min-Seek是一种新颖的顺序测试时间缩放方法，通过动态KV缓存管理，在广泛推理长度范围内显著提升模型准确性，避免准确性退化，无需推理长度微调，且计算复杂度线性。


<details>
  <summary>Details</summary>
Motivation: 当前顺序测试时间缩放方法存在显著局限性：虽然延长推理时间可以提高准确性，但进一步扩展推理长度会导致准确性退化和模型不稳定性，且需要推理长度微调。

Method: 提出Min-Seek方法：1）只保留一个额外诱导思想的KV对在KV缓存中，提高效率；2）使用自定义KV缓存，存储不带位置嵌入的键，并在每个新生成思想前动态连续编码；3）支持超出模型最大上下文长度的推理。

Result: 1）在广泛诱导思想范围内显著提高模型准确性；2）稳定顺序缩放的准确性；3）消除推理长度微调需求；4）在多种推理任务上提升准确性；5）支持超出最大上下文长度的推理；6）在温和条件下具有线性计算复杂度。

Conclusion: Min-Seek是一种高效、稳定的顺序测试时间缩放方法，解决了现有方法的准确性和稳定性问题，无需微调，支持长序列推理，具有线性计算复杂度，为大型推理模型的训练自由改进提供了有效解决方案。

Abstract: Sequential test-time scaling is a promising training-free method to improve large reasoning model accuracy, but as currently implemented, significant limitations have been observed. Inducing models to think for longer can increase their accuracy, but as the length of reasoning is further extended, it has also been shown to result in accuracy degradation and model instability. This work presents a novel sequential test-time scaling method, Min-Seek, which improves model accuracy significantly over a wide range of induced thoughts, stabilizing the accuracy of sequential scaling, and removing the need for reasoning length fine-tuning. Beyond improving model accuracy over a variety of reasoning tasks, our method is inherently efficient, as only the KV pairs of one additional induced thought are kept in the KV cache during reasoning. With a custom KV cache which stores keys without position embeddings, by dynamically encoding them contiguously before each new generated thought, our method can continue to reason well beyond a model's maximum context length, and under mild conditions has linear computational complexity.

</details>


### [141] [Epistemology gives a Future to Complementarity in Human-AI Interactions](https://arxiv.org/abs/2601.09871)
*Andrea Ferrario,Alessandro Facchini,Juan M. Durán*

Main category: cs.AI

TL;DR: 该论文将人机互补性重新定义为评估人机团队可靠性的证据，而非简单的预测准确性相对指标，为人机交互提供了更坚实的理论基础。


<details>
  <summary>Details</summary>
Motivation: 人机互补性概念虽然在人机交互文献中受到关注，但面临理论挑战：缺乏精确的理论基础、仅作为事后预测准确性相对指标、忽视其他人机交互需求、抽象化性能增益的成本特征，导致在实证环境中难以实现。

Method: 利用认识论框架，将互补性重新置于可解释AI讨论中。基于计算可靠主义，将历史互补性实例视为特定人机交互在特定预测任务中是可靠认识过程的证据。结合其他评估人机团队与认识标准和社会技术实践对齐的可靠性指标。

Result: 互补性的角色和价值不在于提供预测准确性的相对度量，而在于帮助校准决策以适应日益影响日常生活的AI支持过程的可靠性。这支持了受这些输出影响的各方（患者、管理者、监管者等）的实践推理。

Conclusion: 通过认识论重构，为人机互补性提供了更坚实的理论基础，使其从简单的性能比较指标转变为评估人机团队可靠性的关键证据，有助于更好地理解和设计人机协作系统。

Abstract: Human-AI complementarity is the claim that a human supported by an AI system can outperform either alone in a decision-making process. Since its introduction in the human-AI interaction literature, it has gained traction by generalizing the reliance paradigm and by offering a more practical alternative to the contested construct of 'trust in AI.' Yet complementarity faces key theoretical challenges: it lacks precise theoretical anchoring, it is formalized just as a post hoc indicator of relative predictive accuracy, it remains silent about other desiderata of human-AI interactions and it abstracts away from the magnitude-cost profile of its performance gain. As a result, complementarity is difficult to obtain in empirical settings. In this work, we leverage epistemology to address these challenges by reframing complementarity within the discourse on justificatory AI. Drawing on computational reliabilism, we argue that historical instances of complementarity function as evidence that a given human-AI interaction is a reliable epistemic process for a given predictive task. Together with other reliability indicators assessing the alignment of the human-AI team with the epistemic standards and socio-technical practices, complementarity contributes to the degree of reliability of human-AI teams when generating predictions. This supports the practical reasoning of those affected by these outputs -- patients, managers, regulators, and others. In summary, our approach suggests that the role and value of complementarity lies not in providing a relative measure of predictive accuracy, but in helping calibrate decision-making to the reliability of AI-supported processes that increasingly shape everyday life.

</details>


### [142] [Beyond Rule-Based Workflows: An Information-Flow-Orchestrated Multi-Agents Paradigm via Agent-to-Agent Communication from CORAL](https://arxiv.org/abs/2601.09883)
*Xinxing Ren,Quagmire Zang,Caelum Forder,Suman Deb,Ahsen Tahir,Roman J. Georgio,Peter Carroll,Zekun Guo*

Main category: cs.AI

TL;DR: 提出了一种基于信息流编排的多智能体范式，通过智能体间自然语言通信动态协调任务，无需预定义工作流，在GAIA基准上超越基于工作流的OWL系统8.49个百分点


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体系统依赖预定义工作流，需要人工枚举任务状态并指定路由规则，存在两个根本限制：需要大量手动工作来预测和编码可能的状态，且无法穷尽复杂现实任务的状态空间

Method: 提出信息流编排的多智能体范式，通过专门的编排器持续监控任务进度，使用A2A工具包通过自然语言动态协调其他智能体，无需依赖预定义工作流

Result: 在GAIA基准测试中，使用pass@1设置，该方法达到63.64%准确率，比基于工作流的OWL系统（55.15%）高出8.49个百分点，且令牌消耗相当。案例分析显示该方法能实现更灵活的任务监控和更鲁棒的边缘情况处理

Conclusion: 信息流编排的多智能体范式通过动态协调机制克服了预定义工作流的局限性，在复杂任务中表现出更好的灵活性和鲁棒性，为多智能体系统设计提供了新方向

Abstract: Most existing Large Language Model (LLM)-based Multi-Agent Systems (MAS) rely on predefined workflows, where human engineers enumerate task states in advance and specify routing rules and contextual injections accordingly. Such workflow-driven designs are essentially rule-based decision trees, which suffer from two fundamental limitations: they require substantial manual effort to anticipate and encode possible task states, and they cannot exhaustively cover the state space of complex real-world tasks. To address these issues, we propose an Information-Flow-Orchestrated Multi-Agent Paradigm via Agent-to-Agent (A2A) Communication from CORAL, in which a dedicated information flow orchestrator continuously monitors task progress and dynamically coordinates other agents through the A2A toolkit using natural language, without relying on predefined workflows. We evaluate our approach on the general-purpose benchmark GAIA, using the representative workflow-based MAS OWL as the baseline while controlling for agent roles and underlying models. Under the pass@1 setting, our method achieves 63.64% accuracy, outperforming OWL's 55.15% by 8.49 percentage points with comparable token consumption. Further case-level analysis shows that our paradigm enables more flexible task monitoring and more robust handling of edge cases. Our implementation is publicly available at: https://github.com/Coral-Protocol/Beyond-Rule-Based-Workflows

</details>


### [143] [Hallucination Detection and Mitigation in Large Language Models](https://arxiv.org/abs/2601.09929)
*Ahmad Pesaranghader,Erin Li*

Main category: cs.AI

TL;DR: 本文提出了一个基于根本原因认知的幻觉管理操作框架，通过模型、数据和上下文三个层面的分类干预，结合多维度检测和分层缓解策略，构建了用于受监管环境中的可信生成AI系统的系统化方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和大型推理模型在金融、法律等高风险领域具有变革潜力，但其产生幻觉（生成事实错误或无依据内容）的倾向带来了关键可靠性风险，需要系统化的管理方法。

Method: 提出了一个全面的幻觉管理操作框架，基于持续改进循环和根本原因认知。将幻觉来源分类为模型、数据和上下文相关因素，整合多维度检测方法（不确定性估计、推理一致性等）和分层缓解策略（知识基础、置信度校准等）。通过分层架构和金融数据提取案例研究展示应用。

Result: 该框架通过模型、上下文和数据三个层级形成闭环反馈循环，实现了渐进式可靠性增强，为受监管环境中构建可信生成AI系统提供了系统化、可扩展的方法论。

Conclusion: 提出的基于根本原因认知的幻觉管理框架为高风险领域中的生成AI系统提供了系统化的可靠性管理方法，通过分类干预和持续改进循环，能够有效提升模型在受监管环境中的可信度。

Abstract: Large Language Models (LLMs) and Large Reasoning Models (LRMs) offer transformative potential for high-stakes domains like finance and law, but their tendency to hallucinate, generating factually incorrect or unsupported content, poses a critical reliability risk. This paper introduces a comprehensive operational framework for hallucination management, built on a continuous improvement cycle driven by root cause awareness. We categorize hallucination sources into model, data, and context-related factors, allowing targeted interventions over generic fixes. The framework integrates multi-faceted detection methods (e.g., uncertainty estimation, reasoning consistency) with stratified mitigation strategies (e.g., knowledge grounding, confidence calibration). We demonstrate its application through a tiered architecture and a financial data extraction case study, where model, context, and data tiers form a closed feedback loop for progressive reliability enhancement. This approach provides a systematic, scalable methodology for building trustworthy generative AI systems in regulated environments.

</details>


### [144] [Chinese Labor Law Large Language Model Benchmark](https://arxiv.org/abs/2601.09972)
*Zixun Lan,Maochun Xu,Yifan Ren,Rui Wu,Jianghui Zhou,Xueyang Cheng,Jianan Ding Ding,Xinheng Wang,Mingmin Chi,Fei Ma*

Main category: cs.AI

TL;DR: LabourLawLLM是专门针对中国劳动法的法律大语言模型，配合LabourLawBench基准测试，在劳动法相关任务上超越通用模型和其他法律专用模型。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型（如GPT-4）在处理需要精确法律知识、复杂推理和情境敏感性的专业法律子领域时表现不佳，特别是在劳动法这样的专业领域。

Method: 开发了LabourLawLLM（专门针对中国劳动法的法律大语言模型）和LabourLawBench（涵盖法律条文引用、知识问答、案例分类、赔偿计算、命名实体识别和法律案例分析等任务的综合基准）。评估框架结合了客观指标（ROUGE-L、准确率、F1、soft-F1）和基于GPT-4评分的主观评估。

Result: 实验表明，LabourLawLLM在各项任务类别中始终优于通用模型和现有的法律专用大语言模型。

Conclusion: 该方法不仅适用于劳动法领域，还为其他法律子领域构建专业化大语言模型提供了可扩展的方法，提高了法律AI应用的准确性、可靠性和社会价值。

Abstract: Recent advances in large language models (LLMs) have led to substantial progress in domain-specific applications, particularly within the legal domain. However, general-purpose models such as GPT-4 often struggle with specialized subdomains that require precise legal knowledge, complex reasoning, and contextual sensitivity. To address these limitations, we present LabourLawLLM, a legal large language model tailored to Chinese labor law. We also introduce LabourLawBench, a comprehensive benchmark covering diverse labor-law tasks, including legal provision citation, knowledge-based question answering, case classification, compensation computation, named entity recognition, and legal case analysis. Our evaluation framework combines objective metrics (e.g., ROUGE-L, accuracy, F1, and soft-F1) with subjective assessment based on GPT-4 scoring. Experiments show that LabourLawLLM consistently outperforms general-purpose and existing legal-specific LLMs across task categories. Beyond labor law, our methodology provides a scalable approach for building specialized LLMs in other legal subfields, improving accuracy, reliability, and societal value of legal AI applications.

</details>


### [145] [Memo-SQL: Structured Decomposition and Experience-Driven Self-Correction for Training-Free NL2SQL](https://arxiv.org/abs/2601.10011)
*Zerui Yang,Weichuan Wang,Yanwei Xu,Linqi Song,Yudai Matsuda,Wei Han,Bo Bai*

Main category: cs.AI

TL;DR: Memo-SQL：一种无需训练的NL2SQL框架，通过结构化分解和经验感知自校正解决现有系统问题，在BIRD基准上达到68.5%执行准确率，比之前方法节省10倍以上资源。


<details>
  <summary>Details</summary>
Motivation: 现有NL2SQL系统存在两个关键局限：1）仅依赖正确示例的上下文学习，忽略了历史错误修复对中的丰富信号；2）测试时缩放方法通常任意分解问题，产生几乎相同的SQL候选，削弱集成增益。此外，这些方法面临严重的准确率-效率权衡：高性能需要过多计算，而快速变体则牺牲质量。

Method: 提出Memo-SQL训练免费框架，包含两个核心思想：结构化分解和经验感知自校正。结构化分解采用三种明确策略：实体级、层次化和原子序列分解，以促进多样化推理。自校正方面，构建包含成功查询和历史错误修复对的动态记忆库，使用检索增强提示在推理时将相关示例引入上下文，无需微调或外部API。

Result: 在BIRD基准测试中，Memo-SQL达到68.5%的执行准确率，在开放、零微调方法中创造了新的最先进水平，同时比之前的TTS方法使用超过10倍更少的资源。

Conclusion: Memo-SQL通过结构化分解促进多样化推理，利用经验感知自校正从历史错误中学习，有效解决了现有NL2SQL系统的局限性，在保持高性能的同时显著提升了计算效率。

Abstract: Existing NL2SQL systems face two critical limitations: (1) they rely on in-context learning with only correct examples, overlooking the rich signal in historical error-fix pairs that could guide more robust self-correction; and (2) test-time scaling approaches often decompose questions arbitrarily, producing near-identical SQL candidates across runs and diminishing ensemble gains. Moreover, these methods suffer from a stark accuracy-efficiency trade-off: high performance demands excessive computation, while fast variants compromise quality. We present Memo-SQL, a training-free framework that addresses these issues through two simple ideas: structured decomposition and experience-aware self-correction. Instead of leaving decomposition to chance, we apply three clear strategies, entity-wise, hierarchical, and atomic sequential, to encourage diverse reasoning. For correction, we build a dynamic memory of both successful queries and historical error-fix pairs, and use retrieval-augmented prompting to bring relevant examples into context at inference time, no fine-tuning or external APIs required. On BIRD, Memo-SQL achieves 68.5% execution accuracy, setting a new state of the art among open, zero-fine-tuning methods, while using over 10 times fewer resources than prior TTS approaches.

</details>


### [146] [FilDeep: Learning Large Deformations of Elastic-Plastic Solids with Multi-Fidelity Data](https://arxiv.org/abs/2601.10031)
*Jianheng Tang,Shilong Tao,Zhe Feng,Haonan Sun,Menglu Wang,Zhanxing Zhu,Yunhuai Liu*

Main category: cs.AI

TL;DR: FilDeep是一个基于保真度的深度学习框架，用于解决弹性-塑性固体大变形问题，通过同时使用低保真度和高保真度数据来平衡数据数量与精度之间的矛盾。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在大变形问题中存在固有局限性，而现有深度学习技术需要大量高精度数据，但在大变形问题中难以获得。数据构建过程中存在数量与精度的两难困境，导致深度学习模型性能不佳。

Method: 提出FilDeep框架，同时使用低保真度（数量多但精度低）和高保真度（精度高但数量少）数据进行训练。针对实际大变形问题设计了注意力机制的跨保真度模块，有效捕捉多保真度数据间的长程物理相互作用。

Result: 大量实验表明，FilDeep框架在弹性-塑性固体大变形问题中始终达到最先进的性能，并能高效部署于制造应用中。

Conclusion: FilDeep是首个使用多保真度数据解决大变形问题的深度学习框架，成功解决了数据数量与精度之间的两难困境，为大变形问题的科学计算提供了有效的深度学习解决方案。

Abstract: The scientific computation of large deformations in elastic-plastic solids is crucial in various manufacturing applications. Traditional numerical methods exhibit several inherent limitations, prompting Deep Learning (DL) as a promising alternative. The effectiveness of current DL techniques typically depends on the availability of high-quantity and high-accuracy datasets, which are yet difficult to obtain in large deformation problems. During the dataset construction process, a dilemma stands between data quantity and data accuracy, leading to suboptimal performance in the DL models. To address this challenge, we focus on a representative application of large deformations, the stretch bending problem, and propose FilDeep, a Fidelity-based Deep Learning framework for large Deformation of elastic-plastic solids. Our FilDeep aims to resolve the quantity-accuracy dilemma by simultaneously training with both low-fidelity and high-fidelity data, where the former provides greater quantity but lower accuracy, while the latter offers higher accuracy but in less quantity. In FilDeep, we provide meticulous designs for the practical large deformation problem. Particularly, we propose attention-enabled cross-fidelity modules to effectively capture long-range physical interactions across MF data. To the best of our knowledge, our FilDeep presents the first DL framework for large deformation problems using MF data. Extensive experiments demonstrate that our FilDeep consistently achieves state-of-the-art performance and can be efficiently deployed in manufacturing.

</details>


### [147] [State of AI: An Empirical 100 Trillion Token Study with OpenRouter](https://arxiv.org/abs/2601.10088)
*Malika Aubakirova,Alex Atallah,Chris Clark,Justin Summerville,Anjney Midha*

Main category: cs.AI

TL;DR: 基于OpenRouter平台分析超过100万亿token的真实LLM使用数据，发现开源模型广泛采用、创意角色扮演和编程助手类应用流行、智能体推理兴起，并识别出早期用户的"灰姑娘玻璃鞋"效应


<details>
  <summary>Details</summary>
Motivation: 随着2024年12月5日首个广泛采用的推理模型o1发布，LLM领域从单次模式生成转向多步推理推断，但我们对这些模型在实际使用中的理解滞后于技术发展速度

Method: 利用OpenRouter平台分析超过100万亿token的真实世界LLM交互数据，涵盖不同任务、地域和时间维度

Result: 观察到开源模型广泛采用、创意角色扮演（超越常见的生产力任务）和编程助手类别特别受欢迎、智能体推理兴起，并通过留存分析发现早期用户的"灰姑娘玻璃鞋"效应

Conclusion: 开发者与终端用户对LLM的实际使用复杂多样，数据驱动的使用理解可为模型构建者、AI开发者和基础设施提供商提供更好的设计和部署指导

Abstract: The past year has marked a turning point in the evolution and real-world use of large language models (LLMs). With the release of the first widely adopted reasoning model, o1, on December 5th, 2024, the field shifted from single-pass pattern generation to multi-step deliberation inference, accelerating deployment, experimentation, and new classes of applications. As this shift unfolded at a rapid pace, our empirical understanding of how these models have actually been used in practice has lagged behind. In this work, we leverage the OpenRouter platform, which is an AI inference provider across a wide variety of LLMs, to analyze over 100 trillion tokens of real-world LLM interactions across tasks, geographies, and time. In our empirical study, we observe substantial adoption of open-weight models, the outsized popularity of creative roleplay (beyond just the productivity tasks many assume dominate) and coding assistance categories, plus the rise of agentic inference. Furthermore, our retention analysis identifies foundational cohorts: early users whose engagement persists far longer than later cohorts. We term this phenomenon the Cinderella "Glass Slipper" effect. These findings underscore that the way developers and end-users engage with LLMs "in the wild" is complex and multifaceted. We discuss implications for model builders, AI developers, and infrastructure providers, and outline how a data-driven understanding of usage can inform better design and deployment of LLM systems.

</details>


### [148] [MATRIX AS PLAN: Structured Logical Reasoning with Feedback-Driven Replanning](https://arxiv.org/abs/2601.10101)
*Ke Chen,Jiandian Zeng,Zihao Peng,Guo Li,Guangxue Zhang,Tian Wang*

Main category: cs.AI

TL;DR: MatrixCoT是一个结构化思维链框架，通过矩阵规划增强LLM的逻辑推理能力，避免外部求解器依赖，提高鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：思维链提示在符号推理任务上不足；神经符号方法依赖外部求解器但格式敏感易失败；纯LLM方法缺乏结构化表示和错误纠正机制。需要增强LLM处理复杂逻辑推理的能力。

Method: 提出MatrixCoT框架：1) 对自然语言表达进行归一化和类型标注，添加显式引用字段；2) 引入基于矩阵的规划方法，保留步骤间的全局关系；3) 添加反馈驱动的重新规划机制，在语义等价约束下识别遗漏和缺陷，重写压缩依赖矩阵。

Result: 在五个逻辑推理基准测试和五个LLM上的实验表明，MatrixCoT在不依赖外部求解器的情况下，增强了处理复杂符号推理任务的鲁棒性和可解释性，同时保持了有竞争力的性能。

Conclusion: MatrixCoT通过结构化思维链和矩阵规划，有效提升了LLM的逻辑推理能力，解决了现有方法的局限性，为复杂符号推理提供了更稳定可靠的解决方案。

Abstract: As knowledge and semantics on the web grow increasingly complex, enhancing Large Language Models (LLMs) comprehension and reasoning capabilities has become particularly important. Chain-of-Thought (CoT) prompting has been shown to enhance the reasoning capabilities of LLMs. However, it still falls short on logical reasoning tasks that rely on symbolic expressions and strict deductive rules. Neuro-symbolic methods address this gap by enforcing formal correctness through external solvers. Yet these solvers are highly format-sensitive, and small instabilities in model outputs can lead to frequent processing failures. LLM-driven approaches avoid parsing brittleness, but they lack structured representations and process-level error-correction mechanisms. To further enhance the logical reasoning capabilities of LLMs, we propose MatrixCoT, a structured CoT framework with a matrix-based plan. Specifically, we normalize and type natural language expressions, attach explicit citation fields, and introduce a matrix-based planning method to preserve global relations among steps. The plan becomes a verifiable artifact, making execution more stable. For verification, we also add a feedback-driven replanning mechanism. Under semantic-equivalence constraints, it identifies omissions and defects, rewrites and compresses the dependency matrix, and produces a more trustworthy final answer. Experiments on five logical-reasoning benchmarks and five LLMs show that, without relying on external solvers, MatrixCoT enhances both robustness and interpretability when tackling complex symbolic reasoning tasks, while maintaining competitive performance.

</details>


### [149] [M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints](https://arxiv.org/abs/2601.10131)
*Yizhan Li,Florence Cloutier,Sifan Wu,Ali Parviz,Boris Knyazev,Yan Zhang,Glen Berseth,Bang Liu*

Main category: cs.AI

TL;DR: MolGen是一个两阶段分子生成框架，使用检索增强的片段级编辑和强化学习优化，在多个物理化学性质约束下生成分子。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在精确多目标控制和数值推理方面存在困难，需要外部结构和反馈来生成满足多个精确数值约束的分子。

Method: 两阶段框架：第一阶段使用多智能体推理器进行检索锚定的片段级编辑生成原型；第二阶段使用基于GRPO的片段级优化器进行细粒度优化，最小化属性误差并控制编辑复杂度。

Result: 在两组性质约束（QED、LogP、分子量和HOMO、LUMO）下的实验表明，该方法在有效性和精确满足多属性目标方面表现优于强LLM和基于图的算法。

Conclusion: MolGen通过利用片段进行分子推理并支持向数值目标的可控细化，在满足多属性约束的分子生成方面取得了显著改进。

Abstract: Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. Although large language models (LLMs) are expressive, they struggle with precise multi-objective control and numeric reasoning without external structure and feedback. We introduce \textbf{M olGen}, a fragment-level, retrieval-augmented, two-stage framework for molecule generation under multi-property constraints. Stage I : Prototype generation: a multi-agent reasoner performs retrieval-anchored, fragment-level edits to produce a candidate near the feasible region. Stage II : RL-based fine-grained optimization: a fragment-level optimizer trained with Group Relative Policy Optimization (GRPO) applies one- or multi-hop refinements to explicitly minimize the property errors toward our target while regulating edit complexity and deviation from the prototype. A large, automatically curated dataset with reasoning chains of fragment edits and measured property deltas underpins both stages, enabling deterministic, reproducible supervision and controllable multi-hop reasoning. Unlike prior work, our framework better reasons about molecules by leveraging fragments and supports controllable refinement toward numeric targets. Experiments on generation under two sets of property constraints (QED, LogP, Molecular Weight and HOMO, LUMO) show consistent gains in validity and precise satisfaction of multi-property targets, outperforming strong LLMs and graph-based algorithms.

</details>


### [150] [Is More Context Always Better? Examining LLM Reasoning Capability for Time Interval Prediction](https://arxiv.org/abs/2601.10132)
*Yanan Cao,Farnaz Fallahi,Murali Mohana Krishna Dandu,Lalitesh Morishetti,Kai Zhao,Luyi Ma,Sinduja Subramaniam,Jianpeng Xu,Evren Korpeoglu,Kaushiki Nag,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: LLMs在预测用户重复行为时间间隔方面表现有限，虽然优于简单统计模型，但不如专用机器学习模型，且过多上下文信息反而会降低预测性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在推理和预测方面表现出色，但其从结构化行为数据中推断时间规律的能力尚未得到充分探索。本研究旨在探究LLMs是否能预测重复用户行为（如重复购买）之间的时间间隔，以及不同层次的上下文信息如何影响其预测行为。

Method: 使用简单但具有代表性的重复购买场景，在零样本设置下对最先进的LLMs进行基准测试，并与统计模型和机器学习模型进行比较。研究分析了不同层次上下文信息（从适度到详细用户级信息）对LLM预测性能的影响。

Result: 1. LLMs虽然优于轻量级统计基线模型，但始终不如专用机器学习模型，显示出其在捕捉定量时间结构方面的有限能力。
2. 适度上下文信息可以提高LLM的准确性，但添加更多用户级详细信息反而会降低性能，挑战了"更多上下文导致更好推理"的假设。

Conclusion: 研究揭示了当前LLMs在结构化时间推理方面的基本局限性，为设计未来上下文感知混合模型提供了指导，这些模型需要整合统计精度与语言灵活性。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in reasoning and prediction across different domains. Yet, their ability to infer temporal regularities from structured behavioral data remains underexplored. This paper presents a systematic study investigating whether LLMs can predict time intervals between recurring user actions, such as repeated purchases, and how different levels of contextual information shape their predictive behavior. Using a simple but representative repurchase scenario, we benchmark state-of-the-art LLMs in zero-shot settings against both statistical and machine-learning models. Two key findings emerge. First, while LLMs surpass lightweight statistical baselines, they consistently underperform dedicated machine-learning models, showing their limited ability to capture quantitative temporal structure. Second, although moderate context can improve LLM accuracy, adding further user-level detail degrades performance. These results challenge the assumption that "more context leads to better reasoning". Our study highlights fundamental limitations of today's LLMs in structured temporal inference and offers guidance for designing future context-aware hybrid models that integrate statistical precision with linguistic flexibility.

</details>


### [151] [History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis](https://arxiv.org/abs/2601.10143)
*Haochong Xia,Yao Long Teng,Regan Tan,Molei Qin,Xinrun Wang,Bo An*

Main category: cs.AI

TL;DR: 提出了一种针对金融数据概念漂移的漂移感知数据流系统，通过机器学习自适应控制数据管理流程，提升模型在动态市场中的鲁棒性和风险调整收益。


<details>
  <summary>Details</summary>
Motivation: 金融量化领域中，训练数据与真实世界性能之间存在显著差距，主要源于概念漂移和分布非平稳性。基于静态历史数据训练的模型容易过拟合，在动态市场中泛化能力差。"历史不足够"的理念强调了需要能够随市场演化的自适应数据生成方法。

Method: 提出了一个漂移感知数据流系统，将基于机器学习的自适应控制集成到数据管理过程中。系统包含参数化数据操作模块（单股票变换、多股票混合、筛选操作）和自适应规划调度器，采用基于梯度的双层优化来控制整个系统。该设计将数据增强、课程学习和数据工作流管理统一在一个可微分框架下，支持溯源感知重放和持续数据质量监控。

Result: 在预测和强化学习交易任务上的大量实验表明，该框架增强了模型鲁棒性并提高了风险调整收益。系统为金融数据提供了通用的自适应数据管理和学习引导的工作流自动化方法。

Conclusion: 该系统为解决金融数据概念漂移问题提供了一个有效的解决方案，通过将自适应控制集成到数据管理流程中，能够更好地适应动态市场环境，提升数据驱动系统的可靠性和性能。

Abstract: In quantitative finance, the gap between training and real-world performance-driven by concept drift and distributional non-stationarity-remains a critical obstacle for building reliable data-driven systems. Models trained on static historical data often overfit, resulting in poor generalization in dynamic markets. The mantra "History Is Not Enough" underscores the need for adaptive data generation that learns to evolve with the market rather than relying solely on past observations. We present a drift-aware dataflow system that integrates machine learning-based adaptive control into the data curation process. The system couples a parameterized data manipulation module comprising single-stock transformations, multi-stock mix-ups, and curation operations, with an adaptive planner-scheduler that employs gradient-based bi-level optimization to control the system. This design unifies data augmentation, curriculum learning, and data workflow management under a single differentiable framework, enabling provenance-aware replay and continuous data quality monitoring. Extensive experiments on forecasting and reinforcement learning trading tasks demonstrate that our framework enhances model robustness and improves risk-adjusted returns. The system provides a generalizable approach to adaptive data management and learning-guided workflow automation for financial data.

</details>


### [152] [DecisionLLM: Large Language Models for Long Sequence Decision Exploration](https://arxiv.org/abs/2601.10148)
*Xiaowei Lv,Zhilin Zhang,Yijun Li,Yusen Huo,Siyuan Ju,Xuyan Li,Chunxiang Hong,Tianyu Wang,Yongcai Wang,Peng Sun,Chuan Yu,Jian Xu,Bo Zheng*

Main category: cs.AI

TL;DR: 该研究提出DecisionLLM，将大型语言模型应用于离线决策任务，通过将轨迹数据作为独立模态与自然语言任务描述对齐，解决了LLM无法理解连续数值的问题，在迷宫导航和竞价场景中显著优于传统决策Transformer。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在长序列决策任务中存在挑战，而大型语言模型在复杂推理和规划任务中表现出色。研究者希望探索共享Transformer架构但规模更大的LLM能否在长视野序列决策问题中实现突破性性能，特别是解决LLM无法理解连续数值表示的根本限制。

Method: 提出DecisionLLM框架，将轨迹数据视为独立模态，学习将轨迹数据与自然语言任务描述对齐，使模型能够在统一框架内自回归预测未来决策。建立了该范式的缩放定律，表明性能取决于模型规模、数据量和数据质量三个因素。

Result: 在离线实验基准和竞价场景中，DecisionLLM表现出色。DecisionLLM-3B在Maze2D umaze-v1上比传统决策Transformer提升69.4分，在AuctionNet上提升0.085分。研究扩展了AIGB范式，为在线竞价提供了有前景的探索方向。

Conclusion: 该工作成功将大型语言模型应用于离线决策任务，通过模态对齐方法解决了LLM理解连续数值的挑战，建立了有效的缩放定律，在多个基准测试中显著优于传统方法，为序列决策任务开辟了新方向。

Abstract: Long-sequence decision-making, which is usually addressed through reinforcement learning (RL), is a critical component for optimizing strategic operations in dynamic environments, such as real-time bidding in computational advertising. The Decision Transformer (DT) introduced a powerful paradigm by framing RL as an autoregressive sequence modeling problem. Concurrently, Large Language Models (LLMs) have demonstrated remarkable success in complex reasoning and planning tasks. This inspires us whether LLMs, which share the same Transformer foundation, but operate at a much larger scale, can unlock new levels of performance in long-horizon sequential decision-making problem. This work investigates the application of LLMs to offline decision making tasks. A fundamental challenge in this domain is the LLMs' inherent inability to interpret continuous values, as they lack a native understanding of numerical magnitude and order when values are represented as text strings. To address this, we propose treating trajectories as a distinct modality. By learning to align trajectory data with natural language task descriptions, our model can autoregressively predict future decisions within a cohesive framework we term DecisionLLM. We establish a set of scaling laws governing this paradigm, demonstrating that performance hinges on three factors: model scale, data volume, and data quality. In offline experimental benchmarks and bidding scenarios, DecisionLLM achieves strong performance. Specifically, DecisionLLM-3B outperforms the traditional Decision Transformer (DT) by 69.4 on Maze2D umaze-v1 and by 0.085 on AuctionNet. It extends the AIGB paradigm and points to promising directions for future exploration in online bidding.

</details>


### [153] [MHub.ai: A Simple, Standardized, and Reproducible Platform for AI Models in Medical Imaging](https://arxiv.org/abs/2601.10154)
*Leonard Nürnberg,Dennis Bontempi,Suraj Pai,Curtis Lisle,Steve Pieper,Ron Kikinis,Sil van de Leemput,Rahul Soni,Gowtham Murugesan,Cosmin Ciausu,Miriam Groeneveld,Felix J. Dorfner,Jue Jiang,Aneesh Rangnekar,Harini Veeraraghavan,Joeran S. Bosma,Keno Bressem,Raymond Mak,Andrey Fedorov,Hugo JWL Aerts*

Main category: cs.AI

TL;DR: MHub.ai是一个开源容器化平台，旨在标准化医学影像AI模型的访问，解决AI实现多样性、文档不一致和可重复性问题，通过容器化封装模型并提供统一接口。


<details>
  <summary>Details</summary>
Motivation: 医学影像AI研究受限于AI实现和架构的多样性、文档不一致以及可重复性问题，阻碍了研究和临床应用。

Method: 开发MHub.ai开源平台，将同行评审的AI模型打包为标准容器，支持DICOM等格式直接处理，提供统一应用接口和结构化元数据，包含分割、预测和特征提取模型。

Result: 平台包含最先进的分割、预测和特征提取模型，通过临床用例（肺分割模型比较）验证实用性，公开分割结果和评估指标，提供交互式仪表板。

Conclusion: MHub.ai通过简化模型使用，支持相同执行命令的并行基准测试和标准化输出，降低了临床转化的门槛，增强了透明度和可重复性。

Abstract: Artificial intelligence (AI) has the potential to transform medical imaging by automating image analysis and accelerating clinical research. However, research and clinical use are limited by the wide variety of AI implementations and architectures, inconsistent documentation, and reproducibility issues. Here, we introduce MHub.ai, an open-source, container-based platform that standardizes access to AI models with minimal configuration, promoting accessibility and reproducibility in medical imaging. MHub.ai packages models from peer-reviewed publications into standardized containers that support direct processing of DICOM and other formats, provide a unified application interface, and embed structured metadata. Each model is accompanied by publicly available reference data that can be used to confirm model operation. MHub.ai includes an initial set of state-of-the-art segmentation, prediction, and feature extraction models for different modalities. The modular framework enables adaptation of any model and supports community contributions. We demonstrate the utility of the platform in a clinical use case through comparative evaluation of lung segmentation models. To further strengthen transparency and reproducibility, we publicly release the generated segmentations and evaluation metrics and provide interactive dashboards that allow readers to inspect individual cases and reproduce or extend our analysis. By simplifying model use, MHub.ai enables side-by-side benchmarking with identical execution commands and standardized outputs, and lowers the barrier to clinical translation.

</details>


### [154] [How does downsampling affect needle electromyography signals? A generalisable workflow for understanding downsampling effects on high-frequency time series](https://arxiv.org/abs/2601.10191)
*Mathieu Cherpitel,Janne Luijten,Thomas Bäck,Camiel Verhamme,Martijn Tannemaat,Anna Kononova*

Main category: cs.AI

TL;DR: 该研究提出了一个评估高采样率时间序列降采样信息损失的系统工作流程，结合形状失真度量和分类性能分析，用于针肌电图信号分析，以平衡计算负载与诊断信息保留。


<details>
  <summary>Details</summary>
Motivation: 针肌电图信号的高采样率和异质性给基于特征的机器学习模型带来计算挑战，降采样是潜在解决方案，但其对诊断信号内容和分类性能的影响尚未充分理解。

Method: 提出系统工作流程，结合形状失真度量、特征机器学习模型分类结果和特征空间分析，量化不同降采样算法和因素对波形完整性和预测性能的影响，使用三类神经肌肉疾病分类任务进行实验评估。

Result: 工作流程能识别既保留诊断信息又显著降低计算负载的降采样配置；形状感知降采样算法优于标准抽取，能更好保留峰值结构和整体信号形态。

Conclusion: 研究为选择支持近实时针肌电图分析的降采样配置提供实用指导，并提出了可推广到其他高采样率时间序列应用的工作流程，以平衡数据缩减与模型性能。

Abstract: Automated analysis of needle electromyography (nEMG) signals is emerging as a tool to support the detection of neuromuscular diseases (NMDs), yet the signals' high and heterogeneous sampling rates pose substantial computational challenges for feature-based machine-learning models, particularly for near real-time analysis. Downsampling offers a potential solution, but its impact on diagnostic signal content and classification performance remains insufficiently understood. This study presents a workflow for systematically evaluating information loss caused by downsampling in high-frequency time series. The workflow combines shape-based distortion metrics with classification outcomes from available feature-based machine learning models and feature space analysis to quantify how different downsampling algorithms and factors affect both waveform integrity and predictive performance. We use a three-class NMD classification task to experimentally evaluate the workflow. We demonstrate how the workflow identifies downsampling configurations that preserve diagnostic information while substantially reducing computational load. Analysis of shape-based distortion metrics showed that shape-aware downsampling algorithms outperform standard decimation, as they better preserve peak structure and overall signal morphology. The results provide practical guidance for selecting downsampling configurations that enable near real-time nEMG analysis and highlight a generalisable workflow that can be used to balance data reduction with model performance in other high-frequency time-series applications as well.

</details>


### [155] [Topo-RAG: Topology-aware retrieval for hybrid text-table documents](https://arxiv.org/abs/2601.10215)
*Alex Dantart,Marco Kóvacs-Navarro*

Main category: cs.AI

TL;DR: Topo-RAG是一个新型检索增强生成框架，专门处理企业文档中混合文本和表格数据的复杂结构，通过双架构分别处理叙述性内容和表格结构，相比传统线性化方法在混合查询上提升18.4%的nDCG@10性能。


<details>
  <summary>Details</summary>
Motivation: 企业数据集中的文档通常是叙述性文本和结构化表格的复杂混合体。当前RAG系统采用线性化方法（将表格转换为Markdown文本字符串）处理这种复杂性，但这种方法在数学上已被证明是不充分的，无法捕捉电子表格的几何结构。

Method: Topo-RAG采用双架构设计：1）传统密集检索器处理流畅的叙述性内容；2）Cell-Aware Late Interaction机制处理表格结构，保留其空间关系。这种方法尊重数据的拓扑结构，而不是简单地将所有内容视为文本。

Result: 在SEC-25（模拟真实世界复杂性的合成企业语料库）上的评估显示，Topo-RAG在混合查询上的nDCG@10比标准线性化方法提高了18.4%。

Conclusion: Topo-RAG挑战了"一切都是文本"的假设，通过尊重数据拓扑结构的双架构设计，不仅提升了搜索性能，更重要的是理解了信息的形状和结构。

Abstract: In enterprise datasets, documents are rarely pure. They are not just text, nor just numbers; they are a complex amalgam of narrative and structure. Current Retrieval-Augmented Generation (RAG) systems have attempted to address this complexity with a blunt tool: linearization. We convert rich, multidimensional tables into simple Markdown-style text strings, hoping that an embedding model will capture the geometry of a spreadsheet in a single vector. But it has already been shown that this is mathematically insufficient.
  This work presents Topo-RAG, a framework that challenges the assumption that "everything is text". We propose a dual architecture that respects the topology of the data: we route fluid narrative through traditional dense retrievers, while tabular structures are processed by a Cell-Aware Late Interaction mechanism, preserving their spatial relationships. Evaluated on SEC-25, a synthetic enterprise corpus that mimics real-world complexity, Topo-RAG demonstrates an 18.4% improvement in nDCG@10 on hybrid queries compared to standard linearization approaches. It's not just about searching better; it's about understanding the shape of information.

</details>


### [156] [TRIM: Hybrid Inference via Targeted Stepwise Routing in Multi-Step Reasoning Tasks](https://arxiv.org/abs/2601.10245)
*Vansh Kapoor,Aman Gupta,Hao Chen,Anurag Beniwal,Jing Huang,Aviral Kumar*

Main category: cs.AI

TL;DR: TRIM提出了一种针对多步推理任务的定向路由方法，只在关键步骤（容易导致级联错误的步骤）使用大模型，而让小模型处理常规步骤，从而显著提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM路由方法将整个查询分配给一个模型，将所有推理步骤视为同等重要。然而多步推理任务容易发生级联失败，单个错误步骤会导致整个解决方案崩溃。需要更精细的步骤级路由来提升效率。

Method: TRIM在步骤级别操作：使用过程奖励模型识别错误步骤，基于步骤级不确定性和预算约束做出路由决策。开发了多种路由策略，从简单的基于阈值的策略到更复杂的策略，这些策略考虑长期准确性-成本权衡和步骤级正确性估计的不确定性。

Result: 在MATH-500上，即使最简单的阈值策略也超越了先前的路由方法，成本效率提高了5倍；更高级的策略使用80%更少的大模型token就能匹配强大昂贵模型的性能。在AIME等更难基准上，TRIM实现了高达6倍的成本效率提升。所有方法在数学推理任务中都能有效泛化。

Conclusion: 步骤级难度代表了推理的基本特征，定向步骤级干预可以通过将昂贵调用限制在那些需要更强模型来防止级联错误的步骤上，从根本上改变推理效率。

Abstract: Multi-step reasoning tasks like mathematical problem solving are vulnerable to cascading failures, where a single incorrect step leads to complete solution breakdown. Current LLM routing methods assign entire queries to one model, treating all reasoning steps as equal. We propose TRIM (Targeted routing in multi-step reasoning tasks), which routes only critical steps$\unicode{x2013}$those likely to derail the solution$\unicode{x2013}$to larger models while letting smaller models handle routine continuations. Our key insight is that targeted step-level interventions can fundamentally transform inference efficiency by confining expensive calls to precisely those steps where stronger models prevent cascading errors. TRIM operates at the step-level: it uses process reward models to identify erroneous steps and makes routing decisions based on step-level uncertainty and budget constraints. We develop several routing strategies within TRIM, ranging from a simple threshold-based policy to more expressive policies that reason about long-horizon accuracy-cost trade-offs and uncertainty in step-level correctness estimates. On MATH-500, even the simplest thresholding strategy surpasses prior routing methods with 5x higher cost efficiency, while more advanced policies match the strong, expensive model's performance using 80% fewer expensive model tokens. On harder benchmarks such as AIME, TRIM achieves up to 6x higher cost efficiency. All methods generalize effectively across math reasoning tasks, demonstrating that step-level difficulty represents fundamental characteristics of reasoning.

</details>


### [157] [NoReGeo: Non-Reasoning Geometry Benchmark](https://arxiv.org/abs/2601.10254)
*Irina Abdullaeva,Anton Vasiliuk,Elizaveta Goncharova,Temurbek Rahmatullaev,Zagorulko Ivan,Maxim Kurkin,Andrey Kuznetsov*

Main category: cs.AI

TL;DR: NoReGeo是一个评估大语言模型内在几何理解能力的新基准，不依赖推理或代数计算，专注于评估LLM是否能直接编码空间关系和识别几何属性。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估基于推理的几何能力（使用代数方法求解），但缺乏对LLM内在几何理解能力的评估。需要评估LLM是否能直接编码空间关系和识别几何属性，而不依赖推理过程。

Method: 创建包含2,500个简单几何问题的基准，涵盖25个类别，每个问题都精心设计为仅通过原生几何理解即可解决（假设已知对象位置）。评估包括GPT-4在内的前沿模型，并进行消融实验。

Result: 即使是GPT-4等最先进的模型，在二元分类任务中的最高准确率仅为65%。消融实验表明，仅通过微调无法获得几何理解能力，需要专门的训练方法。

Conclusion: 当前LLM在原生理解几何概念方面存在显著差距，需要专门的方法来培养真正的几何认知能力，为未来研究提供了基础。

Abstract: We present NoReGeo, a novel benchmark designed to evaluate the intrinsic geometric understanding of large language models (LLMs) without relying on reasoning or algebraic computation. Unlike existing benchmarks that primarily assess models' proficiency in reasoning-based geometry-where solutions are derived using algebraic methods-NoReGeo focuses on evaluating whether LLMs can inherently encode spatial relationships and recognize geometric properties directly. Our benchmark comprises 2,500 trivial geometric problems spanning 25 categories, each carefully crafted to be solvable purely through native geometric understanding, assuming known object locations. We assess a range of state-of-the-art models on NoReGeo, including frontier models like GPT-4, observing that even the most advanced systems achieve an overall maximum of 65% accuracy in binary classification tasks. Further, our ablation experiments demonstrate that such geometric understanding does not emerge through fine-tuning alone, indicating that effective training for geometric comprehension requires a specialized approach from the outset. Our findings highlight a significant gap in current LLMs' ability to natively grasp geometric concepts, providing a foundation for future research toward models with true geometric cognition.

</details>


### [158] [Evidence-Augmented Policy Optimization with Reward Co-Evolution for Long-Context Reasoning](https://arxiv.org/abs/2601.10306)
*Xin Guan,Zijian Li,Shen Huang,Pengjun Xie,Jingren Zhou,Jiuxin Cao*

Main category: cs.AI

TL;DR: EAPO提出证据增强策略优化方法，通过密集过程监督解决长上下文推理中稀疏奖励问题，显著提升证据检索质量和推理性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习在长上下文推理中面临稀疏奖励问题，无法有效惩罚无根据的"幸运猜测"，导致证据检索过程缺乏监督，这是当前LLM推理的主要瓶颈。

Method: 1) 建立证据增强推理范式，通过树结构证据采样验证证据提取是关键瓶颈；2) 提出EAPO算法，使用奖励模型计算组相对证据奖励，提供密集过程监督；3) 引入自适应奖励-策略协同进化机制，迭代优化奖励模型以确保精确的过程指导。

Result: 在八个基准测试上的综合评估表明，EAPO相比最先进的基线方法显著提升了长上下文推理性能。

Conclusion: EAPO通过密集过程监督有效解决了长上下文推理中的稀疏奖励问题，证明了证据提取质量对推理性能的决定性作用，为LLM推理提供了新的优化框架。

Abstract: While Reinforcement Learning (RL) has advanced LLM reasoning, applying it to long-context scenarios is hindered by sparsity of outcome rewards. This limitation fails to penalize ungrounded "lucky guesses," leaving the critical process of needle-in-a-haystack evidence retrieval largely unsupervised. To address this, we propose EAPO (Evidence-Augmented Policy Optimization). We first establish the Evidence-Augmented Reasoning paradigm, validating via Tree-Structured Evidence Sampling that precise evidence extraction is the decisive bottleneck for long-context reasoning. Guided by this insight, EAPO introduces a specialized RL algorithm where a reward model computes a Group-Relative Evidence Reward, providing dense process supervision to explicitly improve evidence quality. To sustain accurate supervision throughout training, we further incorporate an Adaptive Reward-Policy Co-Evolution mechanism. This mechanism iteratively refines the reward model using outcome-consistent rollouts, sharpening its discriminative capability to ensure precise process guidance. Comprehensive evaluations across eight benchmarks demonstrate that EAPO significantly enhances long-context reasoning performance compared to SOTA baselines.

</details>


### [159] [C-GRASP: Clinically-Grounded Reasoning for Affective Signal Processing](https://arxiv.org/abs/2601.10342)
*Cheng Lin Cheng,Ting Chuan Lin,Chai Kai Chang*

Main category: cs.AI

TL;DR: C-GRASP是一个用于心率变异性（HRV）分析的临床推理框架，通过八步可追溯推理步骤和Z-score优先级层次结构，解决LLM在HRV解释中的生理幻觉问题，实现更准确的4类情绪分类。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在心率变异性（HRV）解释中存在生理幻觉问题，包括呼吸性窦性心律失常（RSA）污染、非线性指标的短数据不稳定性，以及忽视个体化基线而偏向群体标准。这些问题阻碍了LLM在HRV分析中的临床应用。

Method: 提出C-GRASP（临床基础情感信号处理推理）框架，采用护栏增强的RAG管道，将HRV解释分解为八个可追溯的推理步骤。核心是Z-score优先级层次结构，强制优先考虑个体化基线变化而非群体统计。系统通过自动化RSA感知护栏有效缓解频谱幻觉，防止频域指标污染。

Result: 在DREAMER数据集的414个试验中评估，C-GRASP与高规模推理模型（如MedGemma3-thinking）集成，在4类情绪分类中达到37.3%的准确率，临床推理一致性（CRC）得分为69.6%。消融研究证实个体化Delta Z-score模块是关键逻辑锚点，防止了原生LLM中常见的"群体偏差"。

Conclusion: C-GRASP将情感计算从黑盒分类转变为透明、基于证据的临床决策支持，为生物医学工程中更安全的AI集成铺平了道路。

Abstract: Heart rate variability (HRV) is a pivotal noninvasive marker for autonomic monitoring; however, applying Large Language Models (LLMs) to HRV interpretation is hindered by physiological hallucinations. These include respiratory sinus arrhythmia (RSA) contamination, short-data instability in nonlinear metrics, and the neglect of individualized baselines in favor of population norms. We propose C-GRASP (Clinically-Grounded Reasoning for Affective Signal Processing), a guardrailed RAG-enhanced pipeline that decomposes HRV interpretation into eight traceable reasoning steps. Central to C-GRASP is a Z-score Priority Hierarchy that enforces the weighting of individualized baseline shifts over normative statistics. The system effectively mitigates spectral hallucinations through automated RSA-aware guardrails, preventing contamination of frequency-domain indices. Evaluated on 414 trials from the DREAMER dataset, C-GRASP integrated with high-scale reasoning models (e.g., MedGemma3-thinking) achieved superior performance in 4-class emotion classification (37.3% accuracy) and a Clinical Reasoning Consistency (CRC) score of 69.6%. Ablation studies confirm that the individualized Delta Z-score module serves as the critical logical anchor, preventing the "population bias" common in native LLMs. Ultimately, C-GRASP transitions affective computing from black-box classification to transparent, evidence-based clinical decision support, paving the way for safer AI integration in biomedical engineering.

</details>


### [160] [LatentRefusal: Latent-Signal Refusal for Unanswerable Text-to-SQL Queries](https://arxiv.org/abs/2601.10398)
*Xuancheng Ren,Shijing Hu,Zhihui Lu,Jiangqi Huang,Qiang Duan*

Main category: cs.AI

TL;DR: 提出LatentRefusal方法，通过LLM隐藏层激活信号预测查询可回答性，为文本到SQL系统提供轻量级安全拒绝机制


<details>
  <summary>Details</summary>
Motivation: LLM文本到SQL系统中，不可回答和未充分指定的用户查询会产生错误SQL程序，导致误导结果或违反安全约束，现有拒绝策略存在幻觉或复杂度高的问题

Method: 将安全拒绝形式化为可回答性门控问题，提出LatentRefusal机制，使用Tri-Residual Gated Encoder轻量级探测架构从LLM中间隐藏激活预测查询可回答性，抑制模式噪声并放大问题-模式不匹配的稀疏局部线索

Result: 在四个基准测试中，LatentRefusal将平均F1提升至88.5%，同时仅增加约2毫秒的探测开销，在多样化的模糊和不可回答场景中表现出有效性

Conclusion: LatentRefusal为文本到SQL系统提供了可附加且高效的安全层，通过潜在信号拒绝机制有效处理不可回答查询问题

Abstract: In LLM-based text-to-SQL systems, unanswerable and underspecified user queries may generate not only incorrect text but also executable programs that yield misleading results or violate safety constraints, posing a major barrier to safe deployment. Existing refusal strategies for such queries either rely on output-level instruction following, which is brittle due to model hallucinations, or estimate output uncertainty, which adds complexity and overhead. To address this challenge, we formalize safe refusal in text-to-SQL systems as an answerability-gating problem and propose LatentRefusal, a latent-signal refusal mechanism that predicts query answerability from intermediate hidden activations of a large language model. We introduce the Tri-Residual Gated Encoder, a lightweight probing architecture, to suppress schema noise and amplify sparse, localized cues of question-schema mismatch that indicate unanswerability. Extensive empirical evaluations across diverse ambiguous and unanswerable settings, together with ablation studies and interpretability analyses, demonstrate the effectiveness of the proposed approach and show that LatentRefusal provides an attachable and efficient safety layer for text-to-SQL systems. Across four benchmarks, LatentRefusal improves average F1 to 88.5 percent on both backbones while adding approximately 2 milliseconds of probe overhead.

</details>


### [161] [ErrEval: Error-Aware Evaluation for Question Generation through Explicit Diagnostics](https://arxiv.org/abs/2601.10406)
*Weiping Fu,Bifan Wei,Jingyi Hao,Yushun Zhang,Jian Zhang,Jiaxin Wang,Bo Li,Yu He,Lingling Zhang,Jun Liu*

Main category: cs.AI

TL;DR: ErrEval是一个错误感知的自动问题生成评估框架，通过显式错误诊断和两阶段评估流程，解决现有方法忽视事实幻觉和答案不匹配等缺陷的问题。


<details>
  <summary>Details</summary>
Motivation: 现有自动问题生成评估方法（包括基于LLM的评估器）主要采用黑盒整体范式，缺乏显式错误建模，导致忽视关键缺陷（如事实幻觉、答案不匹配）并高估问题质量。

Method: ErrEval将评估重新构建为两阶段过程：1) 错误诊断阶段，使用轻量级即插即用错误标识器检测和分类结构、语言和内容相关方面的常见错误；2) 知情评分阶段，将这些诊断信号作为显式证据指导LLM评估器进行更细粒度和有依据的判断。

Result: 在三个基准测试上的广泛实验表明ErrEval的有效性，显示显式诊断的加入提高了与人类判断的一致性。进一步分析证实ErrEval有效缓解了对低质量问题的高估问题。

Conclusion: ErrEval通过显式错误诊断增强了问题生成评估，提供了一种灵活且错误感知的评估框架，能够更准确地识别和评估自动生成问题中的缺陷。

Abstract: Automatic Question Generation (QG) often produces outputs with critical defects, such as factual hallucinations and answer mismatches. However, existing evaluation methods, including LLM-based evaluators, mainly adopt a black-box and holistic paradigm without explicit error modeling, leading to the neglect of such defects and overestimation of question quality. To address this issue, we propose ErrEval, a flexible and Error-aware Evaluation framework that enhances QG evaluation through explicit error diagnostics. Specifically, ErrEval reformulates evaluation as a two-stage process of error diagnosis followed by informed scoring. At the first stage, a lightweight plug-and-play Error Identifier detects and categorizes common errors across structural, linguistic, and content-related aspects. These diagnostic signals are then incorporated as explicit evidence to guide LLM evaluators toward more fine-grained and grounded judgments. Extensive experiments on three benchmarks demonstrate the effectiveness of ErrEval, showing that incorporating explicit diagnostics improves alignment with human judgments. Further analyses confirm that ErrEval effectively mitigates the overestimation of low-quality questions.

</details>


### [162] [LADFA: A Framework of Using Large Language Models and Retrieval-Augmented Generation for Personal Data Flow Analysis in Privacy Policies](https://arxiv.org/abs/2601.10413)
*Haiyue Yuan,Nikolay Matyunin,Ali Raza,Shujun Li*

Main category: cs.AI

TL;DR: LADFA是一个端到端计算框架，结合LLM、RAG和定制知识库，从隐私政策中提取个人数据流并构建数据流图进行分析。


<details>
  <summary>Details</summary>
Motivation: 隐私政策通常使用复杂法律语言且在不同组织间实践不一致，导致用户难以完全理解。现有研究虽然使用LLM提取个人数据流，但需要更有效的自动化大规模分析方法。

Method: 提出LADFA框架，结合LLM、检索增强生成(RAG)和基于现有研究的定制知识库，包含预处理、LLM处理和数据流后处理三个模块，从非结构化隐私政策文本中提取个人数据流并构建数据流图。

Result: 通过汽车行业十个隐私政策的案例研究验证了方法的有效性和准确性，证明能够提取个人数据流并构建数据流图以促进洞察发现。

Conclusion: LADFA框架不仅适用于隐私政策分析，还设计为灵活可定制，适用于更广泛的基于文本的分析任务。

Abstract: Privacy policies help inform people about organisations' personal data processing practices, covering different aspects such as data collection, data storage, and sharing of personal data with third parties. Privacy policies are often difficult for people to fully comprehend due to the lengthy and complex legal language used and inconsistent practices across different sectors and organisations. To help conduct automated and large-scale analyses of privacy policies, many researchers have studied applications of machine learning and natural language processing techniques, including large language models (LLMs). While a limited number of prior studies utilised LLMs for extracting personal data flows from privacy policies, our approach builds on this line of work by combining LLMs with retrieval-augmented generation (RAG) and a customised knowledge base derived from existing studies. This paper presents the development of LADFA, an end-to-end computational framework, which can process unstructured text in a given privacy policy, extract personal data flows and construct a personal data flow graph, and conduct analysis of the data flow graph to facilitate insight discovery. The framework consists of a pre-processor, an LLM-based processor, and a data flow post-processor. We demonstrated and validated the effectiveness and accuracy of the proposed approach by conducting a case study that involved examining ten selected privacy policies from the automotive industry. Moreover, it is worth noting that LADFA is designed to be flexible and customisable, making it suitable for a range of text-based analysis tasks beyond privacy policy analysis.

</details>


### [163] [LLMdoctor: Token-Level Flow-Guided Preference Optimization for Efficient Test-Time Alignment of Large Language Models](https://arxiv.org/abs/2601.10416)
*Tiesunlong Shen,Rui Mao,Jin Wang,Heming Sun,Jian Zhang,Xuejie Zhang,Erik Cambria*

Main category: cs.AI

TL;DR: LLMdoctor：一种基于患者-医生范式的新型测试时对齐框架，通过细粒度token级奖励获取和流引导偏好优化，在保持生成多样性的同时实现高效对齐


<details>
  <summary>Details</summary>
Motivation: 传统微调方法计算成本高且不灵活，现有测试时对齐方法依赖扭曲的轨迹级信号或低效采样，性能受限且难以保持基础模型的生成多样性

Method: 采用患者-医生范式，从大型冻结患者LLM中提取细粒度token级偏好信号，通过token级流引导偏好优化训练小型专门医生模型，实现精确的token级对齐

Result: LLMdoctor显著优于现有测试时对齐方法，甚至超越DPO等完整微调方法的性能

Conclusion: LLMdoctor提供了一种高效、精确的测试时对齐解决方案，在保持生成多样性的同时实现了卓越的对齐性能

Abstract: Aligning Large Language Models (LLMs) with human preferences is critical, yet traditional fine-tuning methods are computationally expensive and inflexible. While test-time alignment offers a promising alternative, existing approaches often rely on distorted trajectory-level signals or inefficient sampling, fundamentally capping performance and failing to preserve the generative diversity of the base model. This paper introduces LLMdoctor, a novel framework for efficient test-time alignment that operates via a patient-doctor paradigm. It integrates token-level reward acquisition with token-level flow-guided preference optimization (TFPO) to steer a large, frozen patient LLM with a smaller, specialized doctor model. Unlike conventional methods that rely on trajectory-level rewards, LLMdoctor first extracts fine-grained, token-level preference signals from the patient model's behavioral variations. These signals then guide the training of the doctor model via TFPO, which establishes flow consistency across all subtrajectories, enabling precise token-by-token alignment while inherently preserving generation diversity. Extensive experiments demonstrate that LLMdoctor significantly outperforms existing test-time alignment methods and even surpasses the performance of full fine-tuning approaches like DPO.

</details>


### [164] [NSR-Boost: A Neuro-Symbolic Residual Boosting Framework for Industrial Legacy Models](https://arxiv.org/abs/2601.10457)
*Ziming Dai,Dabiao Ma,Jinle Tong,Mengyuan Han,Jian Yang,Haojun Fei*

Main category: cs.AI

TL;DR: NSR-Boost是一个神经符号残差提升框架，专门为工业场景设计，通过非侵入式方式修复遗留模型的"硬区域"预测错误，实现低成本、低风险的模型升级。


<details>
  <summary>Details</summary>
Motivation: 尽管梯度提升决策树在工业表格应用中占主导地位，但在高并发生产环境中升级遗留模型面临昂贵的重新训练成本和系统性风险，需要一种安全、低成本的进化范式。

Method: NSR-Boost包含三个阶段：1)通过残差找到硬区域；2)使用大语言模型生成符号代码结构创建可解释专家，并通过贝叶斯优化微调参数；3)通过轻量级聚合器动态集成专家与遗留模型输出。

Result: 在六个公共数据集和一个私有数据集上显著优于最先进的基线方法，在真实在线数据上表现出优异的性能提升，成功部署在Qfin Holdings的核心金融风控系统中。

Conclusion: 该框架有效捕捉传统模型遗漏的长尾风险，为工业应用提供了安全、低成本的进化范式，实现了非侵入式的模型升级。

Abstract: Although the Gradient Boosted Decision Trees (GBDTs) dominate industrial tabular applications, upgrading legacy models in high-concurrency production environments still faces prohibitive retraining costs and systemic risks. To address this problem, we present NSR-Boost, a neuro-symbolic residual boosting framework designed specifically for industrial scenarios. Its core advantage lies in being "non-intrusive". It treats the legacy model as a frozen model and performs targeted repairs on "hard regions" where predictions fail. The framework comprises three key stages: first, finding hard regions through residuals, then generating interpretable experts by generating symbolic code structures using Large Language Model (LLM) and fine-tuning parameters using Bayesian optimization, and finally dynamically integrating experts with legacy model output through a lightweight aggregator. We report on the successful deployment of NSR-Boost within the core financial risk control system at Qfin Holdings. This framework not only significantly outperforms state-of-the-art (SOTA) baselines across six public datasets and one private dataset, more importantly, shows excellent performance gains on real-world online data. In conclusion, it effectively captures long-tail risks missed by traditional models and offers a safe, low-cost evolutionary paradigm for industry.

</details>


### [165] [ChartComplete: A Taxonomy-based Inclusive Chart Dataset](https://arxiv.org/abs/2601.10462)
*Ahmad Mustapha,Charbel Toumieh,Mariette Awad*

Main category: cs.AI

TL;DR: 提出ChartComplete数据集，覆盖30种图表类型，弥补现有图表理解基准数据集仅包含少量图表类型的不足


<details>
  <summary>Details</summary>
Motivation: 随着深度学习和计算机视觉技术的发展，图表理解领域进展迅速，多模态大语言模型在图表理解方面表现出色。但现有评估MLLMs性能的数据集都局限于少量图表类型，存在局限性

Method: 基于可视化社区的图表分类学，构建ChartComplete数据集，包含30种不同图表类型的分类图表图像，但不包含学习信号

Result: 提出了ChartComplete数据集，该数据集基于图表分类学，覆盖30种图表类型，为研究社区提供了更全面的图表理解基准

Conclusion: ChartComplete数据集填补了现有图表理解基准数据集的空白，为社区提供了更全面的图表类型覆盖，可作为后续研究的基础

Abstract: With advancements in deep learning (DL) and computer vision techniques, the field of chart understanding is evolving rapidly. In particular, multimodal large language models (MLLMs) are proving to be efficient and accurate in understanding charts. To accurately measure the performance of MLLMs, the research community has developed multiple datasets to serve as benchmarks. By examining these datasets, we found that they are all limited to a small set of chart types. To bridge this gap, we propose the ChartComplete dataset. The dataset is based on a chart taxonomy borrowed from the visualization community, and it covers thirty different chart types. The dataset is a collection of classified chart images and does not include a learning signal. We present the ChartComplete dataset as is to the community to build upon it.

</details>


### [166] [Panning for Gold: Expanding Domain-Specific Knowledge Graphs with General Knowledge](https://arxiv.org/abs/2601.10485)
*Runhao Zhao,Weixin Zeng,Wentao Zhang,Chong Chen,Zhengpin Li,Xiang Zhao,Lei Chen*

Main category: cs.AI

TL;DR: 提出领域知识图谱融合任务(DKGF)，通过从通用知识图谱中整合相关事实来丰富领域知识图谱，解决领域相关性和知识粒度不匹配问题


<details>
  <summary>Details</summary>
Motivation: 领域知识图谱相比通用知识图谱覆盖范围有限，需要从通用知识图谱中整合相关事实来丰富领域知识

Method: 提出ExeFuse方法，采用Fact-as-Program范式，将通用知识图谱事实视为潜在语义程序，通过程序可执行性验证领域相关性

Result: 构建了两个基准数据集DKGF(W-I)和DKGF(Y-I)共21个评估配置，实验验证了任务重要性和模型有效性

Conclusion: 首次为领域知识图谱融合任务提供了标准化测试平台，提出的ExeFuse方法能有效解决领域相关性和知识粒度不匹配问题

Abstract: Domain-specific knowledge graphs (DKGs) often lack coverage compared to general knowledge graphs (GKGs). To address this, we introduce Domain-specific Knowledge Graph Fusion (DKGF), a novel task that enriches DKGs by integrating relevant facts from GKGs. DKGF faces two key challenges: high ambiguity in domain relevance and misalignment in knowledge granularity across graphs. We propose ExeFuse, a simple yet effective Fact-as-Program paradigm. It treats each GKG fact as a latent semantic program, maps abstract relations to granularity-aware operators, and verifies domain relevance via program executability on the target DKG. This unified probabilistic framework jointly resolves relevance and granularity issues. We construct two benchmarks, DKGF(W-I) and DKGF(Y-I), with 21 evaluation configurations. Extensive experiments validate the task's importance and our model's effectiveness, providing the first standardized testbed for DKGF.

</details>


### [167] [Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural Study on Phishing Detection](https://arxiv.org/abs/2601.10524)
*Frank Bobe,Gregory D. Vetaw,Chase Pavlick,Darshan Bryner,Matthew Cook,Jose Salas-Vernis*

Main category: cs.AI

TL;DR: 研究通过多层级诊断框架分析不同LLM在钓鱼检测任务上的泛化失败原因，发现架构与数据多样性的协同作用、架构依赖的泛化能力差异，以及某些架构固有的更强泛化性。


<details>
  <summary>Details</summary>
Motivation: 尽管微调大型语言模型在专业任务上取得了最先进的性能，但诊断这些模型为何变得脆弱且无法泛化仍然是一个关键开放问题。研究旨在揭示模型泛化失败的根源。

Method: 采用多层级诊断框架，对Llama 3.1 8B、Gemma 2 9B和Mistral模型进行跨架构研究。在钓鱼检测任务上微调这些模型，并使用SHAP分析和机制可解释性方法来揭示泛化失败的根源原因。

Result: 研究发现三个关键结论：1) 泛化能力由架构与数据多样性的强大协同作用驱动；2) 泛化高度依赖架构，Llama 3.1 8B在窄域表现良好但无法整合多样数据；3) 某些架构天生更具泛化性，Mistral模型在多种训练范式下表现一致且稳健。

Conclusion: 通过识别导致这些失败的有缺陷启发式方法，研究提供了诊断和理解泛化失败的具体方法论，强调可靠的AI需要深入验证架构、数据和训练策略之间的相互作用。

Abstract: The practice of fine-tuning Large Language Models (LLMs) has achieved state-of-the-art performance on specialized tasks, yet diagnosing why these models become brittle and fail to generalize remains a critical open problem. To address this, we introduce and apply a multi-layered diagnostic framework to a cross-architectural study. We fine-tune Llama 3.1 8B, Gemma 2 9B, and Mistral models on a high-stakes phishing detection task and use SHAP analysis and mechanistic interpretability to uncover the root causes of their generalization failures. Our investigation reveals three critical findings: (1) Generalization is driven by a powerful synergy between architecture and data diversity. The Gemma 2 9B model achieves state-of-the-art performance (>91\% F1), but only when trained on a stylistically diverse ``generalist'' dataset. (2) Generalization is highly architecture-dependent. We diagnose a specific failure mode in Llama 3.1 8B, which performs well on a narrow domain but cannot integrate diverse data, leading to a significant performance drop. (3) Some architectures are inherently more generalizable. The Mistral model proves to be a consistent and resilient performer across multiple training paradigms. By pinpointing the flawed heuristics responsible for these failures, our work provides a concrete methodology for diagnosing and understanding generalization failures, underscoring that reliable AI requires deep validation of the interplay between architecture, data, and training strategy.

</details>


### [168] [A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5](https://arxiv.org/abs/2601.10527)
*Xingjun Ma,Yixu Wang,Hengyuan Xu,Yutao Wu,Yifan Ding,Yunhan Zhao,Zilong Wang,Jiabin Hua,Ming Wen,Jianan Liu,Ranjie Duan,Yifeng Gao,Yingshui Tan,Yunhao Chen,Hui Xue,Xin Wang,Wei Cheng,Jingjing Chen,Zuxuan Wu,Bo Li,Yu-Gang Jiang*

Main category: cs.AI

TL;DR: 该报告对7个前沿大模型进行了综合安全评估，发现安全性能存在显著异质性，GPT-5.2表现最均衡，但所有模型在对抗性评估中都存在明显脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs和MLLMs在推理、感知和生成能力上的快速进步，这些进步是否带来相应的安全性提升尚不明确，且现有评估实践局限于单一模态或威胁模型，需要更全面的安全评估。

Method: 采用统一协议评估7个前沿模型，包括基准评估、对抗性评估、多语言评估和合规性评估，涵盖语言、视觉语言和图像生成三种设置。

Result: 安全性能呈现显著异质性：GPT-5.2在各项评估中表现均衡且强劲；其他模型在基准安全、对抗对齐、多语言泛化和监管合规之间存在明显权衡；所有模型在对抗性评估中都显著退化；文生图模型在受监管视觉风险类别中相对对齐更好，但在对抗性或语义模糊提示下仍脆弱。

Conclusion: 前沿模型的安全性本质上是多维的，受模态、语言和评估方案影响，需要标准化安全评估来准确评估实际风险并指导负责任的模型开发和部署。

Abstract: The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional--shaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.

</details>


### [169] [Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing](https://arxiv.org/abs/2601.10543)
*Yinzhi Zhao,Ming Wang,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种利用LLM解码过程中潜在安全信号进行早期不安全内容检测的方法，显著提升了对抗越狱攻击的安全性，同时保持了良性输入的响应质量。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型进行了广泛的安全对齐，但现有防御机制（包括基于解码的约束和后处理内容检测器）在面对复杂越狱攻击时效果有限，要么检测不鲁棒，要么过度降低模型效用。研究发现即使成功越狱，模型在生成过程中仍会表现出潜在的安全相关信号，但这些信号被模型追求流畅延续的驱动力所覆盖。

Method: 基于解码过程中存在潜在安全信号的观察，提出了一种简单而有效的方法，在解码过程中显式地提取和利用这些潜在安全信号，用于早期检测不安全内容。

Result: 在多种越狱攻击上的实验表明，该方法显著增强了安全性，同时在良性输入上保持了较低的过度拒绝率，并保留了响应质量。

Conclusion: 在解码过程中激活内在的安全意识为防御越狱攻击提供了一个有前景的补充方向，表明利用模型内部安全信号是有效的防御策略。

Abstract: Large language models (LLMs) have achieved impressive performance across natural language tasks and are increasingly deployed in real-world applications. Despite extensive safety alignment efforts, recent studies show that such alignment is often shallow and remains vulnerable to jailbreak attacks. Existing defense mechanisms, including decoding-based constraints and post-hoc content detectors, struggle against sophisticated jailbreaks, often intervening robust detection or excessively degrading model utility. In this work, we examine the decoding process of LLMs and make a key observation: even when successfully jailbroken, models internally exhibit latent safety-related signals during generation. However, these signals are overridden by the model's drive for fluent continuation, preventing timely self-correction or refusal. Building on this observation, we propose a simple yet effective approach that explicitly surfaces and leverages these latent safety signals for early detection of unsafe content during decoding. Experiments across diverse jailbreak attacks demonstrate that our approach significantly enhances safety, while maintaining low over-refusal rates on benign inputs and preserving response quality. Our results suggest that activating intrinsic safety-awareness during decoding offers a promising and complementary direction for defending against jailbreak attacks. Code is available at: https://github.com/zyz13590/SafeProbing.

</details>


### [170] [Generative AI collective behavior needs an interactionist paradigm](https://arxiv.org/abs/2601.10567)
*Laura Ferrarotti,Gian Maria Campedelli,Roberto Dessì,Andrea Baronchelli,Giovanni Iacca,Kathleen M. Carley,Alex Pentland,Joel Z. Leibo,James Evans,Bruno Lepri*

Main category: cs.AI

TL;DR: 本文主张研究基于大语言模型的智能体集体行为至关重要，需要新的交互主义范式来系统分析先验知识与社交情境如何共同影响多智能体生成AI系统中的涌现现象。


<details>
  <summary>Details</summary>
Motivation: 理解基于大语言模型的智能体集体行为是一个关键研究领域，对社会具有重要的风险与收益影响。大语言模型的独特性质——包括基于大量预训练知识和隐含社会先验的初始化，以及通过上下文学习进行适应的能力——需要新的理论框架来研究这些系统。

Method: 提出交互主义范式，包含替代性的理论基础、方法论和分析工具，系统研究先验知识和嵌入价值观如何与社交情境相互作用，从而塑造多智能体生成AI系统中的涌现现象。

Result: 提出了四个对基于大语言模型的集体系统开发和部署至关重要的研究方向，重点关注理论、方法和跨学科对话。

Conclusion: 研究基于大语言模型的智能体集体行为需要新的交互主义范式，通过理论、方法和跨学科对话的系统性研究，才能充分理解这些系统对社会的影响。

Abstract: In this article, we argue that understanding the collective behavior of agents based on large language models (LLMs) is an essential area of inquiry, with important implications in terms of risks and benefits, impacting us as a society at many levels. We claim that the distinctive nature of LLMs--namely, their initialization with extensive pre-trained knowledge and implicit social priors, together with their capability of adaptation through in-context learning--motivates the need for an interactionist paradigm consisting of alternative theoretical foundations, methodologies, and analytical tools, in order to systematically examine how prior knowledge and embedded values interact with social context to shape emergent phenomena in multi-agent generative AI systems. We propose and discuss four directions that we consider crucial for the development and deployment of LLM-based collectives, focusing on theory, methods, and trans-disciplinary dialogue.

</details>


### [171] [From Single to Multi-Agent Reasoning: Advancing GeneGPT for Genomics QA](https://arxiv.org/abs/2601.10581)
*Kimia Abedini,Farzad Shami,Gianmaria Silvello*

Main category: cs.AI

TL;DR: GenomAgent是一个多智能体框架，通过协调专门化智能体处理复杂基因组查询，在GeneTuring基准测试的9个任务中平均比GeneGPT提升12%性能。


<details>
  <summary>Details</summary>
Motivation: 基因组信息理解对生物医学研究至关重要，但从复杂分布式数据库中提取数据仍然具有挑战性。大型语言模型在基因组问答方面有潜力，但受限于对领域特定数据库的访问受限。当前最先进的GeneGPT系统虽然通过专用API调用增强了LLMs，但受限于僵化的API依赖和有限的适应性。

Method: 作者复制了GeneGPT并提出了GenomAgent，这是一个多智能体框架，能够高效协调专门化智能体来处理复杂的基因组查询。该框架采用灵活的架构设计，可以超越基因组学扩展到需要专家知识提取的各种科学领域。

Result: 在GeneTuring基准测试的9个任务上评估，GenomAgent平均比GeneGPT性能提升12%。该框架的灵活架构不仅适用于基因组学，还能扩展到其他需要专家知识提取的科学领域。

Conclusion: GenomAgent通过多智能体协调机制有效解决了基因组问答中的复杂查询问题，相比现有最佳方法GeneGPT有显著性能提升，且具有更好的适应性和可扩展性，为科学领域的专家知识提取提供了新的解决方案。

Abstract: Comprehending genomic information is essential for biomedical research, yet extracting data from complex distributed databases remains challenging. Large language models (LLMs) offer potential for genomic Question Answering (QA) but face limitations due to restricted access to domain-specific databases. GeneGPT is the current state-of-the-art system that enhances LLMs by utilizing specialized API calls, though it is constrained by rigid API dependencies and limited adaptability. We replicate GeneGPT and propose GenomAgent, a multi-agent framework that efficiently coordinates specialized agents for complex genomics queries. Evaluated on nine tasks from the GeneTuring benchmark, GenomAgent outperforms GeneGPT by 12% on average, and its flexible architecture extends beyond genomics to various scientific domains needing expert knowledge extraction.

</details>


### [172] [Multi-Property Synthesis](https://arxiv.org/abs/2601.10651)
*Christoph Weinhuber,Yannik Schnitzer,Alessandro Abate,David Parker,Giuseppe De Giacomo,Moshe Y. Vardi*

Main category: cs.AI

TL;DR: 提出一种符号化算法，用于LTLf多属性综合问题，当无法同时满足所有属性时，通过单次不动点计算找出最大可实现属性集，相比枚举方法有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 在多属性LTLf综合问题中，同时满足所有属性往往不可能。传统方法需要枚举属性子集，效率低下。需要一种更高效的方法来找出最大可实现属性集。

Method: 引入布尔目标变量，利用单调性紧凑表示指数级的目标组合，通过单次不动点计算产品博弈状态与可实现目标集之间的关系，并综合实现最大可实现集的策略。

Result: 提出的完全符号化算法显著优于基于枚举的基准方法，加速比可达两个数量级。

Conclusion: 该方法通过符号化表示和单调性利用，有效解决了多属性LTLf综合问题，避免了属性子集的枚举，实现了指数级的性能提升。

Abstract: We study LTLf synthesis with multiple properties, where satisfying all properties may be impossible. Instead of enumerating subsets of properties, we compute in one fixed-point computation the relation between product-game states and the goal sets that are realizable from them, and we synthesize strategies achieving maximal realizable sets. We develop a fully symbolic algorithm that introduces Boolean goal variables and exploits monotonicity to represent exponentially many goal combinations compactly. Our approach substantially outperforms enumeration-based baselines, with speedups of up to two orders of magnitude.

</details>


### [173] [Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models](https://arxiv.org/abs/2601.10679)
*Zirui Ren,Ziming Liu*

Main category: cs.AI

TL;DR: HRM在推理任务中表现出色但存在"猜测"而非"推理"的问题，研究发现其存在简单谜题失败、推理步骤的"顿悟"动态、多个固定点等特性，提出了数据增强、输入扰动和模型引导三种策略来提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管分层推理模型（HRM）在各种推理任务中表现卓越，但其推理机制尚不明确。本研究旨在深入理解HRM的优势和潜在失败模式，揭示其实际是"猜测"而非"推理"的本质，并提出改进策略。

Method: 通过机制性研究分析HRM的推理模式，发现三个关键事实：简单谜题失败、推理步骤的"顿悟"动态、多个固定点存在。基于这些发现，提出了三种扩展HRM"猜测"的策略：数据增强（提升猜测质量）、输入扰动（利用推理随机性增加猜测数量）、模型引导（利用训练随机性增加猜测数量）。

Result: 研究发现HRM存在三个令人惊讶的事实：1）在只有一个未知单元格的简单谜题上也会失败；2）推理过程中存在"顿悟"动态，答案不是逐步改进而是突然变正确；3）存在多个固定点，HRM会"猜测"第一个固定点并可能被困住。结合所有方法开发的增强HRM将Sudoku-Extreme的准确率从54.5%提升到96.9%。

Conclusion: HRM实际上是通过"猜测"而非"推理"来工作，这解释了其优势和失败模式。提出的三种扩展策略显著提升了HRM的性能，同时为理解推理模型的工作机制提供了新的科学见解。

Abstract: Hierarchical reasoning model (HRM) achieves extraordinary performance on various reasoning tasks, significantly outperforming large language model-based reasoners. To understand the strengths and potential failure modes of HRM, we conduct a mechanistic study on its reasoning patterns and find three surprising facts: (a) Failure of extremely simple puzzles, e.g., HRM can fail on a puzzle with only one unknown cell. We attribute this failure to the violation of the fixed point property, a fundamental assumption of HRM. (b) "Grokking" dynamics in reasoning steps, i.e., the answer is not improved uniformly, but instead there is a critical reasoning step that suddenly makes the answer correct; (c) Existence of multiple fixed points. HRM "guesses" the first fixed point, which could be incorrect, and gets trapped there for a while or forever. All facts imply that HRM appears to be "guessing" instead of "reasoning". Leveraging this "guessing" picture, we propose three strategies to scale HRM's guesses: data augmentation (scaling the quality of guesses), input perturbation (scaling the number of guesses by leveraging inference randomness), and model bootstrapping (scaling the number of guesses by leveraging training randomness). On the practical side, by combining all methods, we develop Augmented HRM, boosting accuracy on Sudoku-Extreme from 54.5% to 96.9%. On the scientific side, our analysis provides new insights into how reasoning models "reason".

</details>


### [174] [Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems](https://arxiv.org/abs/2601.10681)
*Amir Khurshid,Abhishek Sehgal*

Main category: cs.AI

TL;DR: 本文提出了一种基于结构感知和多样性约束的上下文气泡构建框架，用于改进LLM的检索增强生成，通过保留文档结构、平衡相关性和多样性来减少冗余，提高答案质量和引用准确性。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法使用top-k段落检索存在信息图碎片化、过度检索、内容重复以及查询上下文不足（包括二阶和三阶方面）等问题，需要更有效的上下文构建方法。

Method: 提出结构感知和多样性约束的上下文气泡框架：1) 保留和利用文档固有结构，组织多粒度跨度（如章节和行）；2) 使用任务条件结构先验指导检索；3) 从高相关性锚点跨度开始，通过平衡查询相关性、边际覆盖和冗余惩罚的约束选择构建上下文气泡；4) 明确约束多样性和预算，生成紧凑信息集；5) 提供完整检索记录以实现可审计性和确定性调优。

Result: 在企业文档上的实验表明，上下文气泡方法显著减少冗余上下文，更好地覆盖次要方面，在有限上下文窗口内具有更好的答案质量和引用忠实度。消融研究证明结构先验和多样性约束选择都是必要的，移除任一组件都会导致覆盖下降和冗余或不完整上下文增加。

Conclusion: 提出的上下文气泡框架通过结构感知和多样性约束的检索方法，有效解决了传统RAG的局限性，能够生成更紧凑、信息更丰富且可审计的上下文，提升了LLM的检索增强生成性能。

Abstract: Large language model (LLM) contexts are typically constructed using retrieval-augmented generation (RAG), which involves ranking and selecting the top-k passages. The approach causes fragmentation in information graphs in document structures, over-retrieval, and duplication of content alongside insufficient query context, including 2nd and 3rd order facets. In this paper, a structure-informed and diversity-constrained context bubble construction framework is proposed that assembles coherent, citable bundles of spans under a strict token budget. The method preserves and exploits inherent document structure by organising multi-granular spans (e.g., sections and rows) and using task-conditioned structural priors to guide retrieval. Starting from high-relevance anchor spans, a context bubble is constructed through constrained selection that balances query relevance, marginal coverage, and redundancy penalties. It will explicitly constrain diversity and budget, producing compact and informative context sets, unlike top-k retrieval. Moreover, a full retrieval is emitted that traces the scoring and selection choices of the records, thus providing auditability and deterministic tuning. Experiments on enterprise documents demonstrate the efficiency of context bubble as it significantly reduces redundant context, is better able to cover secondary facets and has a better answer quality and citation faithfulness within a limited context window. Ablation studies demonstrate that both structural priors as well as diversity constraint selection are necessary; removing either component results in a decline in coverage and an increase in redundant or incomplete context.

</details>


### [175] [The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load](https://arxiv.org/abs/2601.10696)
*Han Jiang,Yao Xiao,Rachel Hurley,Shichao Liu*

Main category: cs.AI

TL;DR: 研究探讨生成式AI对建筑概念设计任务中表现、创意自我效能和认知负荷的影响，发现AI对新手设计师有显著帮助，但会降低一般创意自我效能，效果取决于用户专业水平和提示策略。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI在建筑概念设计中的实际影响，特别是对设计表现、创意自我效能和认知负荷的作用，了解AI辅助设计的有效性和潜在局限性。

Method: 36名学生参与两阶段建筑设计任务：独立设计和外部工具辅助设计（生成式AI组和对照组使用现有建筑项目在线库）。专家评估设计成果，参与者自我报告自我效能和认知负荷，使用双重差分法分析数据。

Result: 整体上生成式AI无显著性能优势，但对新手设计师显著提升设计表现；使用AI的学生一般创意自我效能下降；认知负荷无显著差异，但迭代想法生成和视觉反馈提示与认知负荷更大降低相关。

Conclusion: 生成式AI的有效性取决于用户先前专业水平和通过提示的交互策略，对新手设计师有益但可能削弱创意自我效能，提示策略影响认知负荷管理。

Abstract: Our study examines how generative AI (GenAI) influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks. Thirty-six student participants from Architectural Engineering and other disciplines completed a two-phase architectural design task, first independently and then with external tools (GenAI-assisted condition and control condition using an online repository of existing architectural projects). Design outcomes were evaluated by expert raters, while self-efficacy and cognitive load were self-reported after each phase. Difference-in-differences analyses revealed no overall performance advantage of GenAI across participants; however, subgroup analyses showed that GenAI significantly improved design performance for novice designers. In contrast, general creative self-efficacy declined for students using GenAI. Cognitive load did not differ significantly between conditions, though prompt usage patterns showed that iterative idea generation and visual feedback prompts were linked to greater reductions in cognitive load. These findings suggest that GenAI effectiveness depends on users' prior expertise and interaction strategies through prompting.

</details>
