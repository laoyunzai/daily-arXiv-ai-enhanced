<div id=toc></div>

# Table of Contents

- [nlin.CD](#nlin.CD) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 4]
- [quant-ph](#quant-ph) [Total: 31]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 4]
- [nlin.AO](#nlin.AO) [Total: 1]
- [cs.AI](#cs.AI) [Total: 16]
- [cs.LG](#cs.LG) [Total: 44]


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [1] [A Topological Framework for Atmospheric River Interaction Using Framed Braids](https://arxiv.org/abs/2601.00354)
*Ioannis Diamantis*

Main category: nlin.CD

TL;DR: 本文提出了一种基于拓扑框架的大气河流分析方法，使用编织理论和框架编织体来编码AR中心线的几何相互作用和内部水分输送演变。


<details>
  <summary>Details</summary>
Motivation: 现有诊断方法通常孤立地分析大气河流，但实际上多个AR细丝经常共存并相互作用。需要一种能够捕捉AR细丝之间相互作用及其内部水分输送演变的方法。

Method: 引入基于框架编织和框架编织体的拓扑框架，将AR细丝表示为细线，其时间有序交叉形成编织词，同时通过基于水分的框架捕捉每个细丝内部的增强或减弱。

Result: 将该框架应用于再分析数据，构建滑动时间窗口上的编织和框架编织表示，分析北太平洋强相互作用的多细丝AR事件。结果显示编织指标能捕捉结构重组和水分增强事件，这些在中心线几何或IVT强度单独分析中不明显。

Conclusion: 编织理论为大气水分输送提供了互补的结构视角，能够揭示传统方法无法捕捉的结构重组和水分强度变化，为AR相互作用分析提供了新工具。

Abstract: Atmospheric Rivers (ARs) are filamentary moisture pathways responsible for a large fraction of extreme precipitation and often occur as interacting filament bundles within the same synoptic regime. Existing diagnostics typically analyze ARs in isolation, despite the frequent coexistence and interaction of multiple filaments. We introduce a topological framework for AR analysis based on framed braids and framed braidoids, which encodes both the geometric interaction of AR centroids and the internal evolution of moisture transport.
  In this approach, AR filaments are represented as strands whose time-ordered crossings form braid words, while moisture-based framing captures internal intensification or weakening along each filament. Applying this framework to reanalysis-derived Atmospheric River track data, we construct braid and framed braid representations over sliding time windows and analyze a strongly interacting multi-filament AR episode in the North Pacific. The results show that braid-based indicators capture structural reorganizations and moisture intensification episodes that are not apparent from centroid geometry or IVT magnitude alone, offering a complementary structural perspective on atmospheric moisture transport.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [2] [Probing the magnetic ground state and magnetoelastic coupling in double perovskite ruthenate: Ca2ScRuO6](https://arxiv.org/abs/2601.00085)
*Asha Ann Abraham,Anjali Kumari,Md Aktar Hossain,Sanjoy Kr Mahatha,Saikat Das,A. K. Bera,Soham Manni*

Main category: cond-mat.str-el

TL;DR: Ca2ScRuO6展现出独特的磁性基态：长程反铁磁序与小型磁性团簇共存，高温下表现出类亚铁磁行为，源于Ru/Sc原子位点交换导致的混合价态。


<details>
  <summary>Details</summary>
Motivation: 研究4d3双钙钛矿钌酸盐系列中较少探索的成员Ca2ScRuO6的磁性基态，钌酸盐材料通常表现出从超导到交变磁性的奇异基态。

Method: 通过温度依赖的体磁化率测量、X射线吸收光谱分析结构、中子粉末衍射测量磁结构，并进行对称性分析。

Result: 发现40K附近出现类亚铁磁行为，源于Ru5+和Ru4+混合价态；结构分析显示Ru/Sc原子位点交换；4K附近检测到磁布拉格峰，但对称性分析显示弱I型反铁磁基态，磁矩仅为1.1μB/Ru原子。

Conclusion: Ca2ScRuO6具有不寻常的磁性基态：长程反铁磁序与小型磁性团簇共存，高温下表现出类亚铁磁的逆磁化率行为，为研究反位无序存在下的长程磁序提供了独特平台。

Abstract: Ruthenates, materials with a single magnetic Ruthenium (Ru) atom, often display an exotic array of ground states ranging from superconductivity to altermagnetism. In this work, we investigated the magnetic ground state of a least explored member of the 4d3 double perovskite ruthenate series A2ScRuO6 (A = Ca, Sr, Ba): Ca2ScRuO6. Interestingly, temperature-dependent bulk susceptibility curve shows ferrimagnetic-like behaviour above the magnetic ordering at around 40 K, which were corroborated by the identification of the mixed valence states, Ru5+ and Ru4+ via X-ray absorption spectroscopy. Structural analysis further revealed atomic-site exchange between the Ru and Sc sites, which results in the Ru mixed valence states. Neutron powder diffraction measurements detected the presence of magnetic Bragg peaks at a low temperature near 4 K and a moderate magnetoelastic coupling near the ordering temperature of 40 K. However, the corresponding symmetry analysis shows a weak Type I antiferromagnetic ground state with a reduced magnetic moment of 1.1μB/Ru atom. Our findings establish an unusual magnetic ground state in the Mott insulating Ca2ScRuO6, where a long range ordered antiferromagnet coexists with small magnetic clusters, which manifests a ferrimagnetic-like high temperature inverse magnetic susceptibility. This system presents a unique platform to study long-range magnetic order in the presence of antisite disorder.

</details>


### [3] [Spin-density wave of ferrimagnetic building blocks masking the ferromagnetic quantum-critical point in NbFe2](https://arxiv.org/abs/2601.00101)
*T. Poulis,G. Mani,J. Sturt,W. J. Duncan,H. Thoma,V. Hutanu,B. Ouladdiaf,I. Kibalin,M. H. Lemee,P. Manuel,A. Neubauer,C. Pfleiderer,F. M. Grosche,P. G. Niklowitz*

Main category: cond-mat.str-el

TL;DR: NbFe2中通过调节Fe浓度研究铁磁量子临界点，发现纵向自旋密度波掩盖了铁磁量子临界点，这是首次完全解析的此类调制磁序案例。


<details>
  <summary>Details</summary>
Motivation: 研究NbFe2金属磁体中铁磁量子临界点的低温阈值，探索调制磁序如何掩盖铁磁量子临界点，并首次完全解析这种掩盖机制。

Method: 使用球形中子极化测量和高强度单晶中子衍射技术，分析不同Fe浓度下的磁结构，解析自旋密度波的特性。

Result: 发现首个纵向自旋密度波掩盖铁磁量子临界点的案例，该自旋密度波具有长波长非公度调制、低平均磁矩，由反平行铁磁片层组成的亚铁磁构建块构成。

Conclusion: 铁磁片层的存在和仅在介观尺度上磁化抵消表明自旋密度波与铁磁母相具有局部相似性，揭示了自旋密度波作为铁磁量子临界性产物的非常规性质。

Abstract: In the metallic magnet NbFe2, the low temperature threshold of ferromagnetism can be investigated by varying the Fe concentration within a narrow homogeneity range. NbFe2 is one of a number of compounds where modulated order is found to mask the ferromagnetic quantum critical point. However, here we report the rare case where the masking modulated magnetic order has been fully refined. Spherical neutron polarimetry and high-intensity single-crystal neutron diffraction reveal the first case of a longitudinal spin-density wave masking the ferromagnetic quantum critical point. The spin-density wave is characterised by a large-wavelength incommensurate modulation of its low average moment. It is formed from ferrimagnetic building blocks with antiparallel ferromagnetic sheets. The existence of ferromagnetic sheets and cancellation of the magnetisation only over mesoscopic length scales show local similarity between the spin-density wave and the ferromagnetic parent phase and indicate the spin-density wave's unconventional nature as emerging from underlying ferromagnetic quantum criticality.

</details>


### [4] [Topological physics in quantum critical systems](https://arxiv.org/abs/2601.00184)
*Xue-Jia Yu,Limei Xu,Hai-Qing Lin*

Main category: cond-mat.str-el

TL;DR: 本文对量子临界系统中的拓扑物理进行了教学性综述，介绍了无能隙量子临界点和临界相中的拓扑性质，挑战了传统上认为拓扑性质需要体带隙的观点。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为拓扑性质在体带隙闭合时会被破坏，这使得在无能隙量子临界系统中考虑拓扑变得非常困难。然而，最近的研究试图将拓扑概念推广到没有体带隙的系统，包括量子临界点和临界相，这挑战了传统凝聚态物理的信念。

Method: 通过教学性综述的方式，系统介绍了量子临界点的拓扑性质，并将其推广到稳定的临界相，涵盖非相互作用和相互作用系统。讨论了进一步的推广和未来方向。

Result: 展示了拓扑可以在无能隙量子临界系统中存在，拓扑边缘态不一定需要体带隙。拓扑在分类量子相变中起着关键作用，即使它们属于相同的普适类，这丰富了相变的基本理解。

Conclusion: 拓扑物理在量子临界系统中的研究挑战了传统观念，表明拓扑性质可以在无能隙系统中存在，这为理解相变和拓扑物质开辟了新的方向，具有重要的理论和实验意义。

Abstract: Topology forms a cornerstone in modern condensed matter and statistical physics, offering a new framework to classify the phases and phase transitions beyond the traditional Landau paradigm. However, it is widely believed that topological properties are destroyed when the bulk energy gap closes, making it highly nontrivial to consider topology in gapless quantum critical systems. To address these challenges, recent advancements have sought to generalize the notion of topology to systems without a bulk energy gap, including quantum critical points and critical phases, collectively referred to as gapless symmetry-protected topological states. Extending topology to gapless quantum critical systems challenges the traditional belief in condensed matter physics that topological edge states are typically tied to the presence of a bulk energy gap. Furthermore, it suggests that topology plays a crucial role in classifying quantum phase transitions even if they belong to the same universality class, fundamentally enriching the textbook understanding of phase transitions. Given its importance, here we give a pedagogical review of the current progress of topological physics in quantum critical systems. We introduce the topological properties of quantum critical points and generalize them to stable critical phases, both for noninteracting and interacting systems. Additionally, we discuss further generalizations and future directions, including higher dimensions, nonequilibrium phase transitions, and realizations in modern experiments.

</details>


### [5] [About the origin of the magnetic ground state of Tb$_{2}$Ir$_{2}$O$_{7}$](https://arxiv.org/abs/2601.00749)
*Y. Alexanian,E. Lhotel,J. Robert,S. Petit,E. Lefrançois,P. Lejay,A. Hadj-Azzem,F. Damay,J. Ollivier,B. Fåk,R. Ballou,S. De Brion,V. Simonet*

Main category: cond-mat.str-el

TL;DR: Tb₂Ir₂O₇中Tb³⁺磁矩垂直于其局域伊辛各向异性轴排列，实验和计算揭示了这一奇特磁态的特征和激发谱


<details>
  <summary>Details</summary>
Motivation: 研究磁性稀土烧绿石铱酸盐中稀土和铱亚晶格之间复杂相互作用驱动的非常规相，特别是Tb₂Ir₂O₇中Tb³⁺磁矩垂直于局域伊辛各向异性轴排列的奇特磁态

Method: 使用中子衍射和稀释温度下的非弹性中子散射，辅以比热测量，表征磁态和激发谱；通过计算分析Tb³⁺离子在Ir分子场中的双线性相互作用

Result: 奇特磁态在1.5K完全建立，实验表征了其宽能量范围的激发谱；计算显示Tb³⁺离子在Ir分子场中的双线性相互作用能捕捉实验的关键特征，但需要补充才能完全重现观测行为

Conclusion: Tb₂Ir₂O₇中Tb³⁺磁矩垂直于局域伊辛轴的奇特磁态需要超越简单双线性相互作用的更复杂模型来解释，揭示了稀土烧绿石铱酸盐中相互作用的复杂性

Abstract: Magnetic-rare-earth pyrochlore iridates exhibit a rich variety of unconventional phases, driven by the complex interactions within and between the rare-earth and the iridium sublattices. In this study, we investigate the peculiar magnetic state of Tb$_{2}$Ir$_{2}$O$_{7}$, where a component of the Tb$^{3+}$ moment orders perpendicular to its local Ising anisotropy axis. By means of neutron diffraction and inelastic neutron scattering down to dilution temperatures, complemented by specific heat measurements, we show that this intriguing magnetic state is fully established at 1.5 K and we characterize its excitation spectrum across a broad range of energies. Our calculations reveal that bilinear interactions between Tb$^{3+}$ ions subjected to the Ir molecular field capture several key features of the experiments, but need to be supplemented to fully reproduce the observed behavior.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [6] [On the measurement problem in quantum mechanics: a simple proposal](https://arxiv.org/abs/2601.00010)
*Luigi E. Picasso*

Main category: quant-ph

TL;DR: 该论文探讨量子力学解释中的问题，特别是测量过程和著名悖论，提出"物理实验室假设"作为波函数坍缩假设的替代方案。


<details>
  <summary>Details</summary>
Motivation: 解决量子力学解释中的核心问题，特别是测量过程和著名悖论，寻找波函数坍缩假设的替代方案。

Method: 引入"物理实验室假设"，该假设认为只有对应现有测量仪器的自伴算子才能被视为"可观测量"。

Result: 该假设为量子力学解释问题提供了新的视角，能够替代传统的波函数坍缩假设。

Conclusion: "物理实验室假设"为解决量子力学测量问题提供了有前景的替代方案，可能重新定义量子力学中的状态概念。

Abstract: Some of the problems connected with the interpretation of quantum mechanics are enumerated, in particular those related to some well known paradoxes and, above all, to the measurement process. We then show how the so called "Physics Laboratory Assumption" introduced in [1], which considers as "observables'' only the self-adjoint operators corresponding to existing measuring instruments, can propose a new perspective on the aforementioned problems and can replace the wavefunction collapse postulate.
  [1] Luigi E. Picasso, "On the Concept of State in Quantum Mechanics: Another Way to Decoherence?'' Int. J. Theor. Phys. 62 (2), (2023)

</details>


### [7] [Classical vs quantum dynamics and the onset of chaos in a macrospin system](https://arxiv.org/abs/2601.00062)
*Haowei Fan,Vladimir Fal'ko,Xiao Li*

Main category: quant-ph

TL;DR: 研究具有各向异性长程相互作用和集体耗散的周期性驱动宏观自旋系统，在热力学极限下通过平均场处理得到经典运动方程，分析其动力学相图，并比较量子与经典动力学的收敛性。


<details>
  <summary>Details</summary>
Motivation: 研究周期性驱动宏观自旋系统的量子-经典对应关系，特别是在热力学极限下，探索量子系统与经典系统动力学行为的异同，以及有限尺寸效应对系统行为的影响。

Method: 使用Lindblad主方程描述系统，在热力学极限下采用平均场处理得到经典运动方程，通过最大李雅普诺夫指数、分岔图和傅里叶谱分析动力学相图，同时在Dicke基中进行有限尺寸量子模拟。

Result: 在热力学极限下识别出混沌、准周期和周期相，观察到经典倍周期分岔和吸引子区域的分形边界。量子与经典动力学在Lyapunov时间尺度内收敛，但收敛仅当密度矩阵非零元素尖锐局域化时发生。在混沌区域，量子演化变得混合并在希尔伯特空间中扩散探索，表现出量子混沌特征。

Conclusion: 量子与经典系统都表现出多样的动力学相，但有限尺寸效应抑制了热力学极限中的某些行为。最大李雅普诺夫指数的符号是量子-经典动力学收敛的关键指标。不收敛并不一定意味着量子与经典动力学存在根本差异，在混沌区域量子演化表现出混合和扩散特征，确认了量子混沌的存在。

Abstract: We study a periodically driven macrospin system with anisotropic long-range interactions and collective dissipation, described by a Lindblad master equation. In the thermodynamic limit ($N\to\infty$), a mean-field treatment yields classical equations of motion, whose dynamics are characterized via the maximal Lyapunov exponent (MLE). Focusing on the thermodynamic limit, we map out chaotic, quasiperiodic, and periodic phases via bifurcation diagrams, MLEs, and Fourier spectra of evolved observables, identifying classic period-doubling bifurcations and fractal boundaries in the regions of attractors. Finite-size quantum simulations in the Dicke basis reveal that while both quantum and classical systems exhibit diverse dynamical phases, finite-size effects suppress some behaviors present in the thermodynamic limit. The sign of $λ_{\mathrm{max}}$ serves as a key indicator of convergence between quantum and classical dynamics, which agree over timescales up to the Lyapunov time. Analysis of the density matrix shows that convergence occurs only when its nonzero elements are sharply localized. However, the nonconvergence does not imply a fundamental difference between quantum and classical dynamics: in chaotic regimes, although the evolution orbits of quantum and classical systems show significant differences, quantum evolution becomes mixed and diffusively explores the Hilbert space, signaling quantum chaos, which can be confirmed by the delocalized nature of the density matrix.

</details>


### [8] [Pauli stabilizer formalism for topological quantum field theories and generalized statistics](https://arxiv.org/abs/2601.00064)
*Yitao Feng,Hanyu Xue,Ryohei Kobayashi,Po-Shen Hsin,Yu-An Chen*

Main category: quant-ph

TL;DR: 本文构建了新的晶格规范理论作为Pauli稳定子模型，实现了各种时空维度的拓扑量子场论，重点研究了(4+1)D费米子环拓扑码及其广义统计特性。


<details>
  <summary>Details</summary>
Motivation: 拓扑量子场论为描述拓扑物态和构建量子纠错码提供了统一框架，但如何在晶格上表述拓扑序并从微观哈密顿量中提取拓扑激发性质是一个核心挑战。

Method: 构建新的晶格规范理论作为Pauli稳定子模型，开发晶格描述方法来分析扩展激发，系统确定其广义统计特性。主要例子是通过凝聚(4+1)D ℤ₄拓扑码中的e²m²环获得(4+1)D费米子环拓扑码。

Result: 实现了所有(4+1)D扭曲2-形式规范理论，即由H⁵(B²G, U(1))分类的高阶形式Dijkgraaf-Witten TQFT。展示了环激发具有费米子环统计特性：24步环翻转过程产生-1相位。构建了新的ℤ₂拓扑序家族：费米子膜拓扑码和费米子体积拓扑码。

Conclusion: 开发了基于Pauli的框架来定义任意维度扩展激发的广义统计特性，提供了可计算的晶格幺正过程来检测非平凡广义统计特性，为理解高维拓扑序和构建量子纠错码提供了新工具。

Abstract: Topological quantum field theory (TQFT) provides a unifying framework for describing topological phases of matter and for constructing quantum error-correcting codes, playing a central role across high-energy physics, condensed matter, and quantum information. A central challenge is to formulate topological order on the lattice and to extract the properties of topological excitations from microscopic Hamiltonians. In this work, we construct new classes of lattice gauge theories as Pauli stabilizer models, realizing a wide range of TQFTs in general spacetime dimensions. We develop a lattice description of the resulting extended excitations and systematically determine their generalized statistics.
  Our main example is the $(4+1)$D \emph{fermionic-loop toric code}, obtained by condensing the $e^2 m^2$-loop in the $(4+1)$D $\mathbb{Z}_4$ toric code. We show that the loop excitation exhibits fermionic loop statistics: the 24-step loop-flipping process yields a phase of $-1$. Our Pauli stabilizer models realize all twisted 2-form gauge theories in $(4+1)$D, the higher-form Dijkgraaf-Witten TQFT classified by $H^{5}(B^{2}G, U(1))$. % Beyond $(4+1)$D, the fermionic-loop toric codes form a family of $\mathbb{Z}_2$ topological orders in arbitrary dimensions featuring fermionic loop excitations, realized as explicit Pauli stabilizer codes using $\mathbb{Z}_4$ qudits. % Finally, we develop a Pauli-based framework that defines generalized statistics for extended excitations in any dimension, yielding computable lattice unitary processes to detect nontrivial generalized statistics. For example, we propose anyonic membrane statistics in $(6+1)$D, as well as fermionic membrane and volume statistics in arbitrary dimensions. We construct new families of $\mathbb{Z}_2$ topological orders: the \emph{fermionic-membrane toric code} and the \emph{fermionic-volume toric code}.

</details>


### [9] [Detection Efficiency Bounds in (Semi-)Device-Independent Scenarios](https://arxiv.org/abs/2601.00077)
*Tailan S. Sarubi,Santiago Zamora,Moisés Alves,Vinícius F. Alves,Gandhi Viswanathan,Rafael Chaves*

Main category: quant-ph

TL;DR: 本文综述了检测效率在各种设备无关和半设备无关场景中证明非经典性的关键作用，重点关注检测漏洞问题，即不完美的探测器可能让经典隐变量模型模拟量子关联，从而掩盖真正的非经典性。


<details>
  <summary>Details</summary>
Motivation: 检测效率不足会导致检测漏洞，使得经典隐变量模型能够模拟量子关联，从而无法真正证明非经典性。本文旨在系统分析不同场景下的效率要求，为实验验证量子非经典性提供理论指导。

Method: 作为综述文章，本文回顾了贝尔场景中的检测效率要求（如CHSH不等式的2/3对称效率阈值），并扩展到其他因果结构：工具变量场景、准备-测量场景和双局域性场景，分析不同场景下的效率要求变化。

Result: 1. 贝尔场景中对称效率需要达到2/3才能避免检测漏洞；2. 二元变量工具变量场景的效率要求与二分贝尔场景相同；3. 准备-测量场景中效率不足会影响量子系统维度认证和QKD协议安全性；4. 双局域性场景中采用多个独立源可以显著降低认证非经典关联所需的效率要求。

Conclusion: 检测效率是证明量子非经典性的关键因素，不同因果结构对效率要求不同。理解这些要求对于设计无漏洞的量子非经典性实验至关重要，特别是在设备无关和半设备无关场景中。

Abstract: This article provides a comprehensive review of the critical role of detection efficiency in demonstrating non-classicality across various device-independent and semi-device-independent scenarios. The central focus is the detection loophole, a challenge in which imperfect detectors can allow classical hidden variable models to mimic quantum correlations, thus masking genuine non-classicality. As a review, the article revisits the paradigmatic Bell scenario, detailing the efficiency requirements for the CHSH inequality, such as the 2/3 threshold for symmetric efficiencies, and traces the historical trajectory toward the first loophole-free tests. The analysis extends to other causal structures to explore how efficiency requirements are affected in different contexts. These include the instrumental scenario, which for binary variables has recently been shown to follow the same inefficiency bounds as the bipartite dichotomic Bell scenario; the prepare-and-measure scenario, where inefficiencies impact the certification of a quantum system's dimension and create security breaches in protocols such as Quantum Key Distribution (QKD); and the bilocality scenario, which exemplifies how employing multiple independent sources can significantly relax the required efficiencies to certify non-classical correlations.

</details>


### [10] [Double-Pumped Kerr Parametric Amplifier Beyond the Gain-Bandwidth Limit](https://arxiv.org/abs/2601.00078)
*Nicolas Zapata,Najmeh Etehadi Abari,Mitchell Field,Patrick Winkel,Simon Geisert,Soeren Ihssen,Anja Metelmann,Ioan M. Pop*

Main category: quant-ph

TL;DR: 该研究提出了一种通过双驱动实现无失稳参数放大的新方法，在20dB增益下实现了六倍带宽提升，突破了传统增益-带宽限制


<details>
  <summary>Details</summary>
Motivation: 传统超导驻波参数放大器在接近失稳点工作时存在根本性限制：随着增益增加，瞬时带宽会减小。这限制了微波量子设备读取的性能提升。

Method: 采用双驱动同时激活相位保持增益和频率转换，利用Kerr非线性在颗粒铝二聚体结构中实现无失稳参数放大。

Result: 在20dB增益下实现了六倍带宽提升，突破了传统增益-带宽缩放限制（最高达25dB），同时保持接近量子极限的性能。

Conclusion: 通过双驱动方法实现了无失稳参数放大，显著提升了增益-带宽性能，为微波量子设备读取提供了更优的解决方案。

Abstract: Superconducting standing$-$wave parametric amplifiers are crucial for the readout of microwave quantum devices. Despite significant improvements in recent years, the need to operate near an instability point imposes a fundamental constraint: the instantaneous bandwidth decreases with increasing amplifier gain. Here we show that it is possible to obtain parametric amplification without instability by using two simultaneous drives that activate phase-preserving gain and frequency conversion. Realized in a granular aluminum dimer with Kerr nonlinearity, our method demonstrates a sixfold bandwidth increase at 20 dB gain, surpasses the conventional gain$-$bandwidth scaling up to 25 dB, and remains near the quantum limit.

</details>


### [11] [A compellingly simple proof of the speed of sound for interacting bosons](https://arxiv.org/abs/2601.00111)
*J. Eisert*

Main category: quant-ph

TL;DR: 该论文证明了广义Bose-Hubbard模型中粒子传播存在有限声速的简单界限


<details>
  <summary>Details</summary>
Motivation: 长期以来，人们一直想知道相互作用的玻色子系统是否在信息和粒子传播中具有有限的声速，这个问题最近才得到解决。本文旨在为广义Bose-Hubbard模型提供简洁的证明。

Method: 使用几行基本但非平凡的数学推导，证明广义Bose-Hubbard模型在一般晶格上的粒子传播界限

Result: 证明了适当局部扰动的稳态在粒子数方面具有有限的声速，为粒子传播提供了简单而严格的界限

Conclusion: 该工作以极其简洁的方式解决了玻色子系统粒子传播的有限声速问题，为广义Bose-Hubbard模型提供了严格的数学基础

Abstract: On physical grounds, one expects locally interacting quantum many-body systems to feature a finite group velocity. This intuition is rigorously underpinned by Lieb-Robinson bounds that state that locally interacting Hamiltonians with finite-dimensional constituents on suitably regular lattices always exhibit such a finite group velocity. This also implies that causality is always respected by the dynamics of quantum lattice models. It had been a long-standing open question whether interacting bosonic systems also feature finite speeds of sound in information and particle propagation, which was only recently resolved. This work proves a strikingly simple such bound for particle propagation - shown in literally a few elementary, yet not straightforward, lines - for generalized Bose-Hubbard models defined on general lattices, proving that appropriately locally perturbed stationary states feature a finite speed of sound in particle numbers.

</details>


### [12] [Nature is stingy: Universality of Scrooge ensembles in quantum many-body systems](https://arxiv.org/abs/2601.00266)
*Wai-Keong Mok,Tobias Haug,Wen Wei Ho,John Preskill*

Main category: quant-ph

TL;DR: 该研究为量子多体系统中Scrooge系综的出现提供了严格理论框架，揭示了三种不同的物理机制，并识别出相干性、纠缠、非稳定性和信息混洗作为Scrooge行为出现的关键资源。


<details>
  <summary>Details</summary>
Motivation: 量子模拟器的进展使得能够直接实验访问通过测量孤立量子多体系统部分产生的纯态系综。这些投影系综编码了超出热期望值的精细信息，为量子热化提供了新窗口。虽然无限温度系统产生Haar随机系综，但实际物理约束如有限温度或守恒定律需要更通用的框架，因此需要研究Scrooge系综的出现机制。

Method: 研究引入Scrooge k-design来近似Scrooge系综，识别了三种物理上不同的出现机制：1）长时间混沌幺正动力学单独产生全局Scrooge设计；2）全局态高度混洗时，测量互补子系统诱导局部Scrooge设计；3）任意纠缠态在互补系统以高度混洗基测量时产生局部Scrooge系综。通过多种多体系统的数值模拟验证理论。

Result: 研究建立了Scrooge系综出现的统一理论框架，证明了相干性、纠缠、非稳定性和信息混洗是Scrooge行为出现的关键资源。数值模拟验证了理论预测，表明这些机制在不同多体系统中普遍存在。

Conclusion: 该工作为量子多体系统中最大熵、信息吝啬随机性的出现提供了统一的理论框架，揭示了深热化现象背后的普遍统计规律，并为实验观测Scrooge系综提供了理论基础。

Abstract: Recent advances in quantum simulators allow direct experimental access to the ensemble of pure states generated by measuring part of an isolated quantum many-body system. These projected ensembles encode fine-grained information beyond thermal expectation values and provide a new window into quantum thermalization. In chaotic dynamics, projected ensembles exhibit universal statistics, a phenomenon known as deep thermalization. While infinite-temperature systems generate Haar-random ensembles, realistic physical constraints such as finite temperature or conservation laws require a more general framework. It has been proposed that deep thermalization is governed in general by the emergence of Scrooge ensembles, maximally entropic distributions of pure states consistent with the underlying constraints. Here we provide rigorous arguments supporting this proposal. To characterize this universal behavior, we invoke Scrooge $k$-designs, which approximate Scrooge ensembles, and identify three physically distinct mechanisms for their emergence. First, global Scrooge designs can arise from long-time chaotic unitary dynamics alone, without the need for measurements. Second, if the global state is highly scrambled, a local Scrooge design is induced when the complementary subsystem is measured. Third, a local Scrooge ensemble arises from an arbitrary entangled state when the complementary system is measured in a highly scrambled basis. Numerical simulations across a range of many-body systems identify coherence, entanglement, non-stabilizerness, and information scrambling as essential resources for the emergence of Scrooge-like behavior. Taken together, our results establish a unified theoretical framework for the emergence of maximally entropic, information-stingy randomness in quantum many-body systems.

</details>


### [13] [(PhD Thesis) The Information Locally Stored in Quantum Fields: From Entanglement to Gravity](https://arxiv.org/abs/2601.00128)
*T. Rick Perche*

Main category: quant-ph

TL;DR: 这是一篇博士论文的更新版本，介绍了量子场论中的局部探针、纠缠、相互作用近似以及量子场中的时空几何信息


<details>
  <summary>Details</summary>
Motivation: 为对量子场论中局部探针、纠缠、相互作用近似和时空几何信息等研究领域感兴趣的学生提供指导，并寻求在这些主题上的合作机会

Method: 将论文分为五个章节：量子场论简介、局部探针、纠缠及其探测、相互作用的近似处理、量子场中的时空几何信息

Result: 提供了系统的量子场论研究框架，涵盖了从基础概念到前沿研究方向的多个关键领域

Conclusion: 该论文旨在作为学生进入量子场论相关研究领域的指南，并邀请对该领域感兴趣的读者进行合作研究

Abstract: This is an updated version of my PhD thesis, defended at the University of Waterloo on the 2nd of April 2025, uploaded to the ArXiv with the goal of reaching a wider audience. The thesis is divided into 5 chapters, respectively containing (I) a brief introduction to local quantum field theory (QFT), (II) a description of local probes in QFT, (III) a discussion of entanglement in QFT and how to probe it, (IV) a description of the regimes where QFT interactions can be approximated by direct interactions, and (V) a discussion the information about the geometry of spacetime contained in quantum fields. The partial goal of this thesis is to serve as a guide for students aiming to tackle these different research programs. If the reader is interested in pursuing one or more research projects detailed here, they are encouraged to contact me for collaboration in these topics.

</details>


### [14] [Chaos and thermalization in Clifford-Floquet dynamics](https://arxiv.org/abs/2601.00511)
*Anton Kapustin,Daniil Radamovich*

Main category: quant-ph

TL;DR: 研究无限维量子比特系统中平移不变Clifford量子元胞自动机（QCA）的遍历性质，证明无周期性QCA能使多种初始态热化到无限温度态


<details>
  <summary>Details</summary>
Motivation: 研究量子元胞自动机（QCA）的遍历性质，特别是当QCA不具有周期性时，初始态是否会热化到无限温度态，这对于理解量子多体系统的热化行为具有重要意义

Method: 使用平移不变的Clifford量子元胞自动机对无限维量子比特系统进行Floquet动力学研究，分析不同类别初始态（包括纯态和混合态）的热化行为

Result: 证明对于许多类初始态（包括短程纠缠态和接近平衡态的态），无周期性的QCA确实会导致热化到无限温度态；同时指出了弱热化和强热化之间的微妙区别

Conclusion: 平移不变Clifford QCA在无周期性条件下能够使多种初始态热化到无限温度态，这为理解量子多体系统的热化机制提供了重要理论支持

Abstract: We study the ergodic properties of a unitary Floquet dynamics arising from the repeated application of a translationally-invariant Clifford Quantum Cellular Automata to an infinite system of qubits in d dimensions. One expects that if the QCA does not exhibit any periodicity, a generic initial state of qubits will thermalize, that is, approach the infinite-temperature state. We show that this is true for many classes of states, both pure and mixed. In particular, this is true for all initial states that are short-range entangled and close to the equilibrium state. We also point out a subtle distinction between weak and strong thermalization.

</details>


### [15] [Towards a temperature-insensitive composite diamond clock](https://arxiv.org/abs/2601.00157)
*Sean Lourette,Andrey Jarmola,Jabir Chathanathil,Victor M. Acosta,A. Glen Birdwell,Peter Blümler,Dmitry Budker,Sebastián C. Carrasco,Tony G. Ivanov,Shimon Kolkowitz,Vladimir S. Malinovsky*

Main category: quant-ph

TL;DR: 该研究提出了一种基于金刚石氮空位中心的复合频率参考方案，通过结合电子自旋分裂D和核四极分裂Q的测量来补偿温度敏感性，实现了稳定的固态时钟。


<details>
  <summary>Details</summary>
Motivation: 金刚石中的氮空位中心具有作为固态频率参考的潜力，但其电子零场分裂D对温度高度敏感，这限制了其作为稳定时钟过渡的应用。需要克服这一温度敏感性限制。

Method: 采用复合频率参考方法，结合测量NV中心的电子分裂D和$^{14}$N核自旋的核四极分裂Q。设计了具有八相位控制方案的特殊脉冲序列来抑制脉冲缺陷，在高密度NV系综中交替测量D和Q。

Result: 在室温下与铷蒸气池时钟比较10天，复合金刚石时钟的分数不稳定性在τ=200秒时低于5×10^{-9}，在τ=2×10^5秒时低于1×10^{-8}。相比仅基于D的时钟，分别提高了4倍和200倍。温度不再是主要的不稳定性来源。

Conclusion: 金刚石中互补的电子和核自旋过渡为热鲁棒频率计量学提供了可行途径，为紧凑、多功能的固态时钟和量子传感器开辟了道路。

Abstract: Frequency references based on solid state spins promise simplicity, compactness, robustness, multifunctionality, ease of integration, and high densities of emitters. Nitrogen-vacancy (NV) centers in diamond are a natural candidate, but the electronic zero-field splitting exhibits a large fractional temperature dependence, which has precluded its use as a stable clock transition. Here we show that this limitation can be overcome by forming a composite frequency reference that combines measurements of the electronic splitting D with the nuclear quadrupole splitting of the $^{14}$N nuclear spin intrinsic to the NV center. We further benchmark this composite approach against alternative strategies for mitigating temperature sensitivity. By implementing a specially designed pulse sequence with an eight-phase control scheme that suppresses pulse imperfections, we interleave measurements of D and Q in a high-density NV ensemble and demonstrate a temperature-compensated composite frequency reference. The stability of this composite diamond clock is characterized over a 10-day period at room temperature through a comparison to a Rb vapor-cell clock, yielding a fractional instability below $5 \times 10^{-9}$ for an averaging time of $τ= 200$ s and below $1 \times 10^{-8}$ at $τ= 2 \times 10^5$ s, corresponding to measured improvements by a factor of 4 and 200, respectively, over a clock based purely on the single frequency D for the same periods. By characterizing the residual sensitivity to magnetic fields, optical power, and radio-frequency drive amplitudes, we find that temperature is no longer the dominant source of instability. These results establish complementary electron- and nuclear-spin transitions in diamond as a viable route to thermally robust frequency metrology, providing a pathway toward compact, multifunctional solid-state clocks and quantum sensors.

</details>


### [16] [Exponentially Accelerated Sampling of Pauli Strings for Nonstabilizerness](https://arxiv.org/abs/2601.00761)
*Zhenyu Xiao,Shinsei Ryu*

Main category: quant-ph

TL;DR: 提出一种高效经典算法，用于精确计算N量子比特多体波函数的稳定子Rényi熵和稳定子零性，相比直接方法实现指数级加速。


<details>
  <summary>Details</summary>
Motivation: 量子魔法（由非稳定子性量化）衡量与稳定子结构的偏离，是潜在量子加速的基础。需要高效计算稳定子Rényi熵和稳定子零性的方法。

Method: 结合快速Walsh-Hadamard变换与Pauli算符的精确划分，将每个采样的Pauli字符串平均成本从O(2^N)降低到O(N)。基于此框架开发蒙特卡洛估计器，并采用Clifford基的方差缩减方案抑制采样波动。

Result: 算法在随机魔法态集合上验证了准确性和效率，应用于随机Clifford电路掺杂T门，比较了不同掺杂架构。方法适用于任意量子态，可定量分析高度纠缠态和长时间非平衡动力学中编码的魔法资源。

Conclusion: 该工作提供了一种高效计算量子魔法度量的经典算法，实现了指数级加速，为研究量子态的非稳定子性提供了强大的数值工具。

Abstract: Quantum magic, quantified by nonstabilizerness, measures departures from stabilizer structure and underlies potential quantum speedups. We introduce an efficient classical algorithm that exactly computes stabilizer Rényi entropies and stabilizer nullity for generic many-body wavefunctions of $N$ qubits. The method combines the fast Walsh-Hadamard transform with an exact partition of Pauli operators. It achieves an exponential speedup over direct approaches, reducing the average cost per sampled Pauli string from $O(2^N)$ to $O(N)$. Building on this framework, we further develop a Monte-Carlo estimator for stabilizer Rényi entropies together with a Clifford-based variance-reduction scheme that suppresses sampling fluctuations. We benchmark the accuracy and efficiency on ensembles of random magic states, and apply the method to random Clifford circuits with doped $T$ gates, comparing different doping architectures. Our approach applies to arbitrary quantum states and provides quantitative access to magic resources both encoded in highly entangled states and generated by long-time nonequilibrium dynamics.

</details>


### [17] [Reversing Heat Flow by Coherence in a Multipartite Quantum System](https://arxiv.org/abs/2601.00198)
*Keyi Huang,Qi Zhang,Xiangjing Liu,Ruiqing Li,Xinyue Long,Hongfeng Liu,Xiangyu Wang,Yu-ang Fan,Yuxuan Zheng,Yufang Feng,Yu Zhou,Jack Ng,Xinfang Nie,Zhong-Xiao Man,Dawei Lu*

Main category: quant-ph

TL;DR: 实验证明多粒子自旋系统的内部量子相干性可以逆转热流方向，无需依赖与环境的初始关联


<details>
  <summary>Details</summary>
Motivation: 经典热力学第二定律认为热量自发地从高温物体流向低温物体，但量子关联可以逆转这一过程。本研究旨在探索仅通过系统内部的量子相干性（而非系统与环境之间的关联）是否也能实现热流逆转。

Method: 采用级联相互作用的碰撞模型，在多粒子自旋系统中实验研究内部量子相干性对热流方向的影响。通过控制相干项的强度和相位来观察能量转移的方向和大小。

Result: 实验验证了内部量子相干性确实可以逆转热流方向，且相干项的强度和相位共同决定了能量转移的方向和大小。这为仅利用局部量子特性精确控制热流提供了可能。

Conclusion: 多粒子系统的内部量子相干性可以独立于环境关联实现热流逆转，相干项的强度和相位是关键控制参数，这一发现为量子热力学控制和量子热机设计提供了新途径。

Abstract: The second law of thermodynamics dictates that heat flows spontaneously from a high-temperature entity to a lower-temperature one. Yet, recent advances have demonstrated that quantum correlations between a system and its thermal environment can induce a reversal of heat flow, challenging classical thermodynamic expectations. Here, we experimentally demonstrate that internal quantum coherence in a multipartite spin system can also reverse heat flow, without relying on initial correlations with the environment. Under the collision model with cascade interaction, we verify that both the strength and the phase of the coherence term determine the direction and magnitude of energy transfer. These results enable precise control of heat flow using only local quantum properties.

</details>


### [18] [DC-MBQC: A Distributed Compilation Framework for Measurement-Based Quantum Computing](https://arxiv.org/abs/2601.00214)
*Yecheng Xue,Rui Yang,Zhiding Liang,Tongyang Li*

Main category: quant-ph

TL;DR: 提出了首个面向测量基量子计算（MBQC）的分布式量子编译框架DC-MBQC，解决了MBQC在分布式环境中的任务分配和通信调度问题，显著提升了光子系统的执行效率和光子寿命要求。


<details>
  <summary>Details</summary>
Motivation: 分布式量子计算（DQC）在量子电路模型方面已有显著进展，但在测量基量子计算（MBQC）方面的研究较少。MBQC是与电路模型本质不同的通用量子计算模型，特别适合光子量子平台，因此需要专门针对MBQC的分布式编译框架。

Method: 提出了DC-MBQC框架，包含两个关键技术：1）开发自适应图分割算法，在保持图态结构的同时平衡各量子处理单元（QPU）的工作负载；2）引入层调度问题并提出相应算法。同时考虑实际硬件需求，优化量子程序执行时间和所需光子寿命。

Result: 实验表明，在8个全连接QPU的情况下，所需光子寿命提升了7.46倍，执行速度提升了6.82倍，证实了分布式量子计算在光子系统中的优势。

Conclusion: DC-MBQC是首个专门为MBQC设计的分布式量子编译框架，成功解决了MBQC在分布式环境中的关键挑战，显著提升了光子量子系统的性能和可靠性。

Abstract: Distributed quantum computing (DQC) is a promising technique for scaling up quantum systems. While significant progress has been made in DQC for quantum circuit models, there exists much less research on DQC for measurement-based quantum computing (MBQC), which is a universal quantum computing model that is essentially different from the circuit model and particularly well-suited to photonic quantum platforms. In this paper, we propose DC-MBQC, the first distributed quantum compilation framework tailored for MBQC. We identify and address two key challenges in enabling DQC for MBQC. First, for task allocation among quantum processing units (QPUs), we develop an adaptive graph partitioning algorithm that preserves the structure of the graph state while balancing the workload across QPUs. Second, for inter-QPU communication, we introduce the layer scheduling problem and propose an algorithm to solve it. Regrading realistic hardware requirements, we optimize the execution time of running quantum programs and the corresponding required photon lifetime to avoid fatal failures caused by photon loss. Our experiments demonstrate a $7.46\times$ improvement on required photon lifetime and $6.82\times$ speedup with 8 fully-connected QPUs, which further confirm the advantage of distributed quantum computing in photonic systems. The source code is publicly available at https://github.com/qfcwj/DC-MBQC.

</details>


### [19] [Efficient implementation of single particle Hamiltonians in exponentially reduced qubit space](https://arxiv.org/abs/2601.00247)
*Martin Plesch,Martin Friák,Ijaz Ahamed Mohammad*

Main category: quant-ph

TL;DR: 提出一种对数量子比特编码方法，将N个物理位点映射到仅⌈log₂N⌉个量子比特上，结合变分电路和格雷码测量策略，将变分量子算法的硬件需求从N²降低到(logN)³


<details>
  <summary>Details</summary>
Motivation: 解决当前和近期量子硬件的限制：量子比特数量有限、电路深度受限、重复测量成本高，特别是针对固态哈密顿量的模拟问题

Method: 1) 对数量子比特编码：将N个物理位点映射到⌈log₂N⌉个量子比特；2) 设计兼容的变分电路；3) 基于格雷码的测量策略，测量设置数量仅随系统规模对数增长；4) 引入体积效率度量，综合量子比特数、电路深度和测量设置数

Result: 对于硬件高效ansatz，变分循环所需的总时空采样体积从N²大幅减少到(logN)³，实现了量子硬件尺寸和时间的指数级减少

Conclusion: 大型结构化固态哈密顿量可以在显著更小的量子寄存器上模拟，具有可控的采样开销和可管理的电路复杂度，扩展了近期设备上变分量子算法的应用范围

Abstract: Current and near-term quantum hardware is constrained by limited qubit counts, circuit depth, and the high cost of repeated measurements. We address these challenges for solid state Hamiltonians by introducing a logarithmic-qubit encoding that maps a system with $N$ physical sites onto only $\lceil \log_2 N \rceil$ qubits while maintaining a clear correspondence with the underlying physical model. Within this reduced register, we construct a compatible variational circuit and a Gray-code-inspired measurement strategy whose number of global settings grows only logarithmically with system size. To quantify the overall hardware load, we introduce a volumetric efficiency metric that combines the number of qubit, circuit depth, and the number of measurement settings into a single measure, expressing the overall computation costs. Using this metric, we show that the total space-time-sampling volume required in a variational loop can be reduced dramatically from $N^2$ to $(logN)^3$ for hardware efficient ansatz, allowing an exponential reduction in time and size of the quantum hardware. These results demonstrate that large, structured solid-state Hamiltonians can be simulated on substantially smaller quantum registers with controlled sampling overhead and manageable circuit complexity, extending the reach of variational quantum algorithms on near-term devices.

</details>


### [20] [First appearance of quasiprobability negativity in quantum many-body dynamics](https://arxiv.org/abs/2601.00259)
*Rohit Kumar Shukla,Amikam Levy*

Main category: quant-ph

TL;DR: 本文引入"首次负性"作为量子多体系统中非经典行为出现的动力学指标，通过伊辛链研究其在不同相互作用、温度、可积性破坏下的响应，揭示其时空结构特征，并与量子速度极限比较。


<details>
  <summary>Details</summary>
Motivation: 准概率分布的负性体现了量子动力学的非经典特性，但在多体系统中其动力学涌现机制尚未充分探索。需要一种能够指示局部测量序列何时开始表现出真正非经典行为的动力学指标。

Method: 引入Margenau-Hill准概率的"首次负性"概念，使用伊辛链作为模型系统，研究FTN在不同相互作用强度、温度、可积性破坏条件下的行为，分析不同位点测量的时空结构，并与量子速度极限进行比较。

Result: FTN能清晰区分相互作用主导和场主导区域，受温度系统性重塑，对可积性破坏敏感。不同位点测量揭示的时空结构反映了算符不相容性在晶格中的有限时间传播。数值负性出现与量子速度极限基本一致。

Conclusion: 首次负性可作为实时量子相干性和上下文性的实用实验探针，适用于当前能够进行序贯弱测量和强测量的实验平台，为研究多体量子系统中的非经典行为提供了新工具。

Abstract: Quasiprobability distributions capture aspects of quantum dynamics that have no classical counterpart, yet the dynamical emergence of their negativity in many-body systems remains largely unexplored. We introduce the \emph{first-time negativity} (FTN) of the Margenau-Hill quasiprobability as a dynamical indicator of when local measurement sequences in an interacting quantum system begin to exhibit genuinely nonclassical behavior. Using the Ising chain, we show that FTN discriminates clearly between interaction-dominated and field-dominated regimes, is systematically reshaped by temperature, and responds sensitively to the breaking of integrability. When measurements are performed on different sites, FTN reveals a characteristic spatio-temporal structure that reflects the finite-time spreading of operator incompatibility across the lattice. We further compare the numerical onset of negativity with a recently proposed quantum speed limit (QSL) for quasiprobabilities, which provides a geometric benchmark for the observed dynamics. Our results identify FTN as a practical and experimentally accessible probe of real-time quantum coherence and contextuality, directly suited to current platforms capable of sequential weak and strong measurements.

</details>


### [21] [When Does Quantum Differential Privacy Compose?](https://arxiv.org/abs/2601.00337)
*Daniel Alabi,Theshani Nuradha*

Main category: quant-ph

TL;DR: 该论文研究了量子差分隐私中的组合性问题，发现经典组合定理在量子设置中不完全适用，但通过引入结构假设和量子矩会计方法，可以在特定条件下恢复类似经典的组合保证。


<details>
  <summary>Details</summary>
Motivation: 经典差分隐私中的组合定理是构建复杂隐私保护算法的基石，但在量子差分隐私中，由于隐私定义基于任意测量操作，经典基于标量隐私损失变量的组合论证不再适用。因此需要澄清量子差分隐私中组合性的限制和可能性。

Method: 首先展示经典风格组合在POVM-based近似量子差分隐私中的失败案例，然后针对张量积信道作用于乘积相邻输入的情况，引入基于算子值隐私损失概念和矩阵矩生成函数的量子矩会计方法，通过控制其矩来约束测量Rényi散度。

Result: 发现即使个体完全私密的量子信道，在通过相关联合实现组合时也可能完全丧失隐私。但在特定结构假设下，可以建立量子矩会计框架，获得与经典理论具有相同主导阶行为的高级组合风格边界。

Conclusion: 量子差分隐私的有意义组合定理需要仔细阐述信道、输入和对抗性测量的结构假设，该研究为理解哪些经典思想可以扩展到量子设置提供了原则性框架。

Abstract: Composition is a cornerstone of classical differential privacy, enabling strong end-to-end guarantees for complex algorithms through composition theorems (e.g., basic and advanced). In the quantum setting, however, privacy is defined operationally against arbitrary measurements, and classical composition arguments based on scalar privacy-loss random variables no longer apply. As a result, it has remained unclear when meaningful composition guarantees can be obtained for quantum differential privacy (QDP).
  In this work, we clarify both the limitations and possibilities of composition in the quantum setting. We first show that classical-style composition fails in full generality for POVM-based approximate QDP: even quantum channels that are individually perfectly private can completely lose privacy when combined through correlated joint implementations. We then identify a setting in which clean composition guarantees can be restored. For tensor-product channels acting on product neighboring inputs, we introduce a quantum moments accountant based on an operator-valued notion of privacy loss and a matrix moment-generating function. Although the resulting Rényi-type divergence does not satisfy a data-processing inequality, we prove that controlling its moments suffices to bound measured Rényi divergence, yielding operational privacy guarantees against arbitrary measurements. This leads to advanced-composition-style bounds with the same leading-order behavior as in the classical theory.
  Our results demonstrate that meaningful composition theorems for quantum differential privacy require carefully articulated structural assumptions on channels, inputs, and adversarial measurements, and provide a principled framework for understanding which classical ideas do and do not extend to the quantum setting.

</details>


### [22] [Probabilistic Entanglement Distillation and Cost under Approximately Nonentangling and Dually Nonentangling Instruments](https://arxiv.org/abs/2601.00383)
*Xian Shi*

Main category: quant-ph

TL;DR: 该论文研究了概率性纠缠蒸馏和纠缠稀释的渐近误差指数，建立了概率性蒸馏与后选择量子假设检验之间的直接联系，并推导了在近似非纠缠操作下的蒸馏误差指数的解析表达式。


<details>
  <summary>Details</summary>
Motivation: 纠缠蒸馏和纠缠成本是量子纠缠理论中的基本任务。虽然最近的研究已经阐明了概率性变换在一般资源理论中的局限性，但在近似（对偶）非纠缠操作下概率性纠缠蒸馏的误差指数的解析公式仍然缺失。本研究旨在填补这一空白。

Method: 基于后选择量子假设检验的框架，建立了概率性蒸馏与针对可分离态集合的后选择假设检验之间的直接联系。研究了在δ-近似非纠缠(ANE)和δ-近似对偶非纠缠(ADNE)量子仪器下的操作模型，并推导了在ANE下的蒸馏误差指数的解析特征。

Result: 推导出了在近似非纠缠操作下蒸馏误差指数的解析表达式，建立了概率性蒸馏与后选择假设检验之间的联系，并进一步研究了概率性纠缠稀释，建立了在近似非纠缠和近似对偶非纠缠仪器下的概率性纠缠成本之间的关系。

Conclusion: 该工作通过后选择量子假设检验的框架，成功建立了概率性纠缠蒸馏与假设检验之间的理论联系，为理解概率性纠缠变换的极限提供了新的分析工具和解析结果，填补了该领域的重要空白。

Abstract: Entanglement distillation and entanglement cost are fundamental tasks in quantum entanglement theory. This work studies both in the probabilistic setting and focuses on the asymptotic error exponent of probabilistic entanglement distillation when the operational model is $δ$-approximately nonentangling(ANE) and $δ$-approximately dually nonentangling(ADNE) quantum instruments. While recent progress has clarified limitations of probabilistic transformations in general resource theories, an analytic formula for the error exponent of probabilistic entanglement distillation under approximately (dually) nonentangling operations has remained unavailable.
  Building on the framework of postselected quantum hypothesis testing, we establish a direct connection between probabilistic distillation and postselected hypothesis testing against the set of separable states. In particular, we derive an analytical characterization of the distillation error exponent under ANE. Besides, we relate the exponent to postselected hypothesis testing with measurements restricted to be separable. We further investigate probabilistic entanglement dilution and establish a relation between probabilistic entanglement costs under approximately nonentangling and approximately dually nonentangling instruments, together with a bound on the probabilistic entanglement cost under nonentangling instruments

</details>


### [23] [The Maximal Entanglement Limit in Statistical and High Energy Physics](https://arxiv.org/abs/2601.00405)
*Dmitri E. Kharzeev*

Main category: quant-ph

TL;DR: 量子纠缠为统计物理和高能相互作用提供了统一基础，在足够长时间或高能量下，量子系统会达到最大纠缠极限，导致量子相位不可观测、约化密度矩阵呈现热形式，概率描述自然涌现。


<details>
  <summary>Details</summary>
Motivation: 提出量子纠缠作为统一框架，解释统计物理和高能相互作用中的现象，避免依赖遍历性或经典随机性假设，探索量子系统在极限条件下的普遍行为。

Method: 建立最大纠缠极限（MEL）概念框架，分析量子系统在长时间或高能量极限下的行为，利用高维希尔伯特空间的几何特性，将纠缠作为核心机制。

Result: 在MEL框架下，概率性部分子模型、禁闭弦断裂中的热化、高能碰撞热化以及结构函数的普适小x行为都作为纠缠和高维希尔伯特空间几何的直接结果自然涌现。

Conclusion: 量子纠缠为统计物理和高能相互作用提供了统一的解释基础，最大纠缠极限解释了热化、概率描述等现象的量子起源，无需引入经典随机性或遍历性假设。

Abstract: These lectures advocate the idea that quantum entanglement provides a unifying foundation for both statistical physics and high-energy interactions. I argue that, at sufficiently long times or high energies, most quantum systems approach a Maximal Entanglement Limit (MEL) in which phases of quantum states become unobservable, reduced density matrices acquire a thermal form, and probabilistic descriptions emerge without invoking ergodicity or classical randomness. Within this framework, the emergence of probabilistic parton model, thermalization in the break-up of confining strings and in high-energy collisions, and the universal small $x$ behavior of structure functions arise as direct consequences of entanglement and geometry of high-dimensional Hilbert space.

</details>


### [24] [Chip scale superconducting quantum gravimeter based on a SQUID transmon mechanical resonator](https://arxiv.org/abs/2601.00425)
*Salman Sajad Wani,Mughees Ahmed Khan,Abrar Ahmed Naqash,Saif Al-Kuwari*

Main category: quant-ph

TL;DR: 芯片级超导重力仪：利用可调谐transmon量子比特与高Q值机械谐振器耦合，通过约瑟夫森势的非线性将重力位移映射到量子比特的几何相位，实现高带宽重力测量。


<details>
  <summary>Details</summary>
Motivation: 当前重力测量平台难以同时实现绝对精度和高带宽跟踪，而精确重力测量对地球物理学和惯性导航至关重要。

Method: 将机械元件嵌入量子比特的SQUID环路中，利用约瑟夫森势的非线性产生运动相关电感，通过频闪测量协议抑制机械退相干，将重力位移映射到量子比特的几何相位。

Result: 预测灵敏度达到10² nGal/√Hz，接近原子传感器的性能但具有kHz级采样率，同时具备电可调谐性和通过微波光谱的SI可追溯性。

Conclusion: 该架构为实现高速、量子极限的片上重力测量提供了实用途径，结合了高灵敏度和高带宽的优势。

Abstract: Precise gravitational measurements are vital for geophysics and inertial navigation, but current platforms struggle to combine absolute accuracy with high-bandwidth tracking. We address this challenge with a chip-scale superconducting gravimeter that couples a flux-tunable transmon qubit to a high-$Q$ mechanical resonator. We embed the mechanical element inside the qubit's SQUID loop. This allows us to exploit the Josephson potential's nonlinearity, creating a motion-dependent inductance that maps gravitational displacement onto the qubit's geometric phase. Using a stroboscopic measurement protocol, we suppress mechanical decoherence at revival times. This yields a predicted sensitivity of $10^2\,\mathrm{nGal}/\sqrt{\mathrm{Hz}}$, approaching the performance of atomic sensors but with kilohertz-rate sampling. With electrical {in situ} tunability and SI traceability via microwave spectroscopy, this architecture offers a practical route to high-speed, quantum-limited on-chip gravimetry.

</details>


### [25] [Multistep quantum master equation theory for response functions in four wave mixing electronic spectroscopy of multichromophoric macromolecules](https://arxiv.org/abs/2601.00431)
*Seogjoo J. Jang*

Main category: quant-ph

TL;DR: 该研究为多色团大分子系统的四波混频光谱提供了仅考虑单激子态的三阶响应函数的替代推导方法，推导了封闭形式的表达式，并建立了更通用的量子主方程方法。


<details>
  <summary>Details</summary>
Motivation: 为理解二维电子光谱信号提供更坚实的物理基础，特别是在处理多色团大分子系统中激子态与环境的复杂相互作用方面。

Method: 1. 对于谐振子浴线性对角耦合激子态的情况，推导了显示所有显式时间依赖关系的封闭形式表达式；2. 对于更一般的系统-浴耦合情况，采用量子主方程方法推导格林函数类算符的多步时间演化方程。

Result: 获得了三阶响应函数的解析表达式，并建立了能够一致处理激子间耦合、退相干、弛豫和非马尔可夫效应的新方法，可在二阶非马尔可夫量子主方程水平上求解。

Conclusion: 该研究为多色团大分子系统的四波混频光谱提供了更坚实的理论基础和计算方法，能够更准确地描述复杂的光谱信号和系统动力学。

Abstract: This work provides an alternative derivation of third order response functions in four wave mixing spectroscopy of multichromophoric macromolecular systems considering only single exciton states. For the case of harmonic oscillator bath linearly and diagonally coupled to exciton states, closed form expressions showing all the explicit time dependences are derived. These expressions can provide more solid physical basis for understanding 2-dimensional electronic spectroscopy signals. For more general cases of system-bath coupling, the quantum master equation (QME) approach is employed for the derivation of multistep time evolution equations for Green function-like operators. Solution of these equations is feasible at the level of 2nd order non-Markovian QME, and the new approach can account for inter-exciton coupling, dephasing, relaxation, and non-Markovian effects in a consistent manner.

</details>


### [26] [Prediction of a measurable sign change in the Casimir force using a magnetic fluid](https://arxiv.org/abs/2601.00483)
*Long Ma,Larissa Inácio,Dai-Nam Le,Lilia M. Woods,Mathias Boström*

Main category: quant-ph

TL;DR: 该研究展示了通过卡西米尔力实现的量子悬浮，利用聚苯乙烯表面与特氟龙涂层金属基底在甲苯和磁铁矿颗粒混合物中的相互作用，实现了可测量的排斥-吸引转变的卡西米尔捕获效应。


<details>
  <summary>Details</summary>
Motivation: 研究卡西米尔力在复杂材料系统中的可控量子悬浮现象，探索通过材料选择和设计实现卡西米尔捕获效应的可能性。

Method: 使用聚苯乙烯表面与特氟龙涂层金属基底，将其浸入甲苯和磁铁矿颗粒的混合物中，通过精确控制材料的光学和磁学性质来调节卡西米尔相互作用。

Result: 系统在可测量距离范围内表现出排斥-吸引转变的卡西米尔捕获效应，该效应可通过金属和铁磁流体材料的巧妙选择进行控制，热和量子贡献对捕获效应的幅度和可观测距离范围有显著影响。

Conclusion: 卡西米尔力可用于实现可控的量子悬浮和捕获效应，材料的光学和磁学性质是调节这些效应的关键因素，为基于卡西米尔相互作用的量子操控技术提供了新途径。

Abstract: We demonstrate quantum levitation controlled by Casimir forces acting between a polystyrene surface and a Teflon-coated metallic substrate immersed in a mixture of Toluene and magnetite particles. This system experiences repulsion-attraction transitions in the Casimir interaction for distances where the effect is measurable. This Casimir trapping can be controlled by clever choices of metallic and ferrofluid materials, which are directly linked to the emergence of the trapping effect. Thermal and quantum contributions are investigated in detail, showing how the optical and magnetic properties of the ferrofluid and other materials affect the magnitude of the trapping and its distance range of observability.

</details>


### [27] [A Geometrical Design Tool for Building Cost-Effective Layout-Aware n-Bit Quantum Gates Using the Bloch Sphere Approach](https://arxiv.org/abs/2601.00484)
*Ali Al-Bayaty,Marek Perkowski*

Main category: quant-ph

TL;DR: 本文提出了一种基于布洛赫球面的几何设计方法（BSA），用于构建成本更低的n位量子门，替代传统的基于酉矩阵乘法的设计技术。


<details>
  <summary>Details</summary>
Motivation: 传统的量子门设计技术主要使用酉矩阵乘法，这种方法计算时间长、成本高，且布洛赫球面仅被用作验证工具。作者希望开发一种更高效、成本更低的量子门设计方法。

Method: 提出布洛赫球面方法（BSA），将布洛赫球面作为几何设计工具，通过布洛赫球面的几何平面交线来视觉选择量子旋转，而不使用任何酉矩阵乘法。BSA能够高效地将m个目标量子位映射到n-m个控制量子位中，满足量子计算机物理相邻量子位的有限布局连接性。

Result: 实验证明，使用BSA构建的n位量子门始终比使用传统量子设计技术构建的同类门具有更低的量子成本。

Conclusion: BSA作为一种创新的几何设计方法，能够有效降低量子门的实现成本，为量子电路设计提供了更高效的替代方案。

Abstract: The conventional design technique of any n-bit quantum gate is mainly achieved using unitary matrices multiplication, where n >= 2 and 1 <= m <= n-1 for m target qubits and n-m control qubits. These matrices represent quantum rotations by an n-bit quantum gate. For a quantum designer, such a conventional technique requires extensive computational time and effort, which may generate an n-bit quantum gate with a too high quantum cost. The Bloch sphere is only utilized as a visualization tool to verify the conventional design correctness for quantum rotations by a quantum gate. In contrast, this paper introduces a new concept of using the Bloch sphere as a "geometrical design tool" to build cost-effective n-bit quantum gates with lower quantum costs. This concept is termed the "Bloch sphere approach (BSA)". In BSA, a cost-effective n-bit quantum gate is built without using any unitary matrices multiplication. Instead, the quantum rotations for such a gate are visually selected using the geometrical planar intersections of the Bloch sphere. The BSA can efficiently map m targets among n-m controls for an n-bit quantum gate, to satisfy the limited layout connectivity for the physical neighboring qubits of a quantum computer. Experimentally, n-bit quantum gates built using the BSA always have lower quantum costs than those for such gates built using the conventional quantum design techniques.

</details>


### [28] [Non-Hermitian Band Topology and Edge States in Atomic Lattices](https://arxiv.org/abs/2601.00487)
*Wenxuan Xie,John C Schotland*

Main category: quant-ph

TL;DR: 该论文研究了由长程耗散辐射耦合介导的一维和二维二聚体原子晶格的能带结构和拓扑相，发现低能动力学由具有复费米速度的狄拉克方程控制，并分析了SSH和蜂窝模型的拓扑不变量。


<details>
  <summary>Details</summary>
Motivation: 研究长程耗散辐射耦合对原子晶格拓扑性质的影响，探索非厄米系统中拓扑相的新特征。

Method: 推导单激发子空间的有效非厄米哈密顿量，分析SSH和蜂窝模型的拓扑不变量，利用合成规范场破坏时间反演对称性，推导域边界局域边缘态的解析解。

Result: 系统低能动力学由具有复费米速度的狄拉克方程控制，验证了非厄米体边对应关系，获得了域边界局域边缘态的解析解。

Conclusion: 长程耗散辐射耦合在原子晶格中诱导出具有复费米速度的非厄米拓扑相，为探索非厄米拓扑物理提供了新平台。

Abstract: We investigate the band structure and topological phases of one- and two-dimensional bipartite atomic lattices mediated by long-range dissipative radiative coupling. By deriving an effective non-Hermitian Hamiltonian for the single-excitation sector, we demonstrate that the low-energy dynamics of the system are governed by a Dirac equation with a complex Fermi velocity. We analyze the associated topological invariants for both the SSH and honeycomb models, utilizing synthetic gauge fields to break time-reversal symmetry in the latter. Finally, we explicitly verify the non-Hermitian bulk-edge correspondence by deriving analytical solutions for edge states localized at domain boundaries.

</details>


### [29] [Casimir interactions and drift currents](https://arxiv.org/abs/2601.00489)
*Modi Ke,Dai-Nam Le,Lilia M. Woods*

Main category: quant-ph

TL;DR: 研究带稳态漂移电流的两平行石墨烯片间的卡西米尔相互作用，发现漂移电流引入垂直于层的排斥修正，降低总吸引力，同时产生与载流子流动方向相反的侧向力。


<details>
  <summary>Details</summary>
Motivation: 研究非平衡条件下石墨烯系统的卡西米尔相互作用，探索通过漂移电流控制卡西米尔力的可能性。

Method: 采用移动费米盘模型描述石墨烯的非平衡光学响应，研究带稳态漂移电流的两平行石墨烯片间的波动诱导卡西米尔相互作用。

Result: 漂移电流在垂直于层的方向上引入排斥修正，降低总吸引力但未完全抵消；同时产生与载流子流动方向相反的侧向力；两种效应随距离和漂移速度变化。

Conclusion: 漂移电流为卡西米尔力控制提供了新途径，通过调节电流可改变相互作用强度和方向，为微纳器件设计提供可能。

Abstract: We investigate the fluctuation-induced Casimir interactions between two parallel graphene sheets carrying steady-state drift currents. The graphene properties are modeled based on the shifted Fermi disk model to capture the non-equilibrium optical response of the system. We find that the drift current introduces a repulsive correction to the perpendicular to the layers Casimir interaction, thereby reducing the overall attractive force. Although the correction is repulsive, it does not overcome the underlying attraction between the layers. It also generates a lateral force that opposes the carrier flow direction. Both contributions are studied in terms of distance and drift velocity functionalities showing pathways for Casimir force control.

</details>


### [30] [Photonic Reservoir Engineering via 2D $Λ$-Type Atomic Arrays in Waveguide QED](https://arxiv.org/abs/2601.00622)
*Thi Phuong Anh Nguyen,Le Phuong Hoang,Xuan Binh Cao*

Main category: quant-ph

TL;DR: 提出两种二维原子晶格结构（Zigzag和Orthogonal）耦合光子晶体波导，解决传统一维Λ型原子系统在量子存储和非线性光学中的限制，分别实现展宽EIT窗口和增强四波混频效率。


<details>
  <summary>Details</summary>
Motivation: 传统一维Λ型原子系统的电磁感应透明（EIT）技术存在固有局限：标准一维原子链只能产生单一超辐射通道，而亚辐射模式难以有效访问，限制了集体辐射行为的控制和暗态路径的利用，导致不必要的非弹性过程，降低量子存储保真度和非线性光子生成效率。

Method: 提出两种二维原子晶格结构耦合光子晶体波导：1) Zigzag结构：通过工程化的集体超辐射和亚辐射模式产生平坦化的EIT窗口；2) Orthogonal结构：优化设计以增强四波混频过程。

Result: Zigzag模型展宽了传输带宽并抑制了不必要的散射，提高了量子存储保真度；Orthogonal模型相比传统一维Λ型EIT链，在相同Γ1D、Ωc和探测强度条件下，四波混频强度增强了高达六个数量级，且局域化闲频光子形成明确的光谱模式。

Conclusion: 二维原子晶格结构为工程化结构化的光子库提供了一种多功能途径，可实现按需光子生成、高保真量子存储和增强的非线性光学过程。

Abstract: Electromagnetically induced transparency (EIT) in $Λ$-type atomic systems underpins quantum technologies such as high-fidelity memory and nonlinear optics, but conventional setups face intrinsic limitations. Standard geometries of one-dimensional atomic chains coupled to waveguides allow only a single bright superradiant channel, while subradiant modes remain weakly accessible, limiting control over collective radiative behavior and dark-state pathways. This leads to unwanted inelastic processes, degrading memory fidelity and reducing nonlinear photon generation efficiency. Here, we propose two two-dimensional (2D) atomic lattice geometries coupled to a photonic crystal waveguide, namely Zigzag and Orthogonal structures. In the Zigzag model, engineered collective super- and subradiant modes produce a flattened EIT window, broadening the transmission bandwidth and suppressing unwanted scattering to enhance quantum memory fidelity. In the Orthogonal model, four-wave mixing (FWM) intensity is amplified by up to six orders of magnitude relative to a conventional one-dimensional $Λ$-type EIT chain with identical $Γ_{1D}$, $Ω_c$, and probe intensity, with localized idler photons forming well-defined spectral modes. These results demonstrate a versatile route to engineer structured photonic reservoirs for on-demand photon generation, high-fidelity quantum storage, and enhanced nonlinear optical processes.

</details>


### [31] [Experimental exclusion of a generalized Károlyházy gravity-induced decoherence model](https://arxiv.org/abs/2601.00651)
*Nicola Bortolotti,Kristian Piscicchia,Matthias Laubenstein,Simone Manti,Antonino Marcianò,Federico Nola,Catalina Curceanu*

Main category: quant-ph

TL;DR: 该研究通过VIP合作组在INFN Gran Sasso国家实验室使用高纯锗探测器收集的数据，对Károlyházy提出的广义引力诱导退相干模型施加了新的实验约束，排除了该模型。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过实验测试引力诱导退相干模型，特别是Károlyházy提出的广义版本，这些模型试图用量子引力效应解释波函数坍缩，是量子力学基础理论的重要检验。

Method: 使用VIP合作组在INFN Gran Sasso国家实验室收集的数据，采用高纯锗探测器进行低背景实验，通过分析数据推导空间关联长度R_K的约束条件。

Result: 获得了改进的下界R_K > 4.64米（95%置信水平），比之前的实验限制提高了一个数量级以上。结合理论上限R_K < 1.98米，排除了广义Károlyházy模型及其相关的非马尔可夫CSL模型。

Conclusion: 研究结果显著加强了引力相关退相干情景的实验约束，证明了地下低背景实验对量子力学基础理论修改的敏感性，并排除了广义Károlyházy模型。

Abstract: We report new experimental constraints on the generalized version of the gravity-induced decoherence model originally proposed by Károlyházy. Using data collected by the VIP Collaboration at the INFN Gran Sasso National Laboratory with a high-purity germanium detector, we derive an improved lower bound on the spatial correlation length $R_K$ characterizing metric fluctuations in the model. We obtain a bound $R_K > 4.64$ m (95\% C.L.), which exceeds by more than an order of magnitude the previous experimental limit. When combined with the theoretical upper bound $R_K <1.98$ m derived from macroscopic localization requirements, our result excludes the generalized Károlyházy model. The same conclusion applies to an associated non-Markovian formulation of the Continuous Spontaneous Localization (CSL) model. Our findings significantly tighten experimental constraints on gravity-related decoherence scenarios and demonstrate the sensitivity of underground low-background experiments to foundational modifications of quantum mechanics.

</details>


### [32] [Ultracold Quantum Gravimeters: An Introduction for Geophysicists](https://arxiv.org/abs/2601.00676)
*Ivaldevingles Rodrigues De Souza Junior,Andrea Trombettoni,Carla Braitenberg*

Main category: quant-ph

TL;DR: 本文为地球物理学家提供关于超冷量子重力仪的入门介绍，重点讲解量子力学概念而非地球物理应用，涵盖基于二能级和三能级原子系统的重力仪，解释原子干涉仪的基本机制和相位差如何编码重力加速度。


<details>
  <summary>Details</summary>
Motivation: 为地球物理学家提供关于超冷量子重力仪的可理解介绍，因为地球物理学家已经熟悉地球物理应用，但需要量子力学概念来理解量子重力仪的工作原理。

Method: 通过马赫-曾德尔干涉仪的工作机制，使用π/2和π脉冲来解释原子干涉测量，展示相位差如何编码重力加速度，并简要讨论噪声影响。

Result: 提供了基于二能级和三能级原子系统的重力仪综述，解释了原子干涉测量的基本机制，展示了如何通过干涉仪相位差测量重力加速度。

Conclusion: 本文为地球物理学家提供了理解超冷量子重力仪所需的量子力学概念基础，通过原子干涉测量原理解释了重力加速度的测量机制。

Abstract: This paper aims at providing an accessible introduction to ultracold quantum gravimeters tailored for geophysicists. We do not focus here on geophysical applications, as these are already well known to geophysicists, but rather provide a pedagogical exposition of the quantum-mechanical concepts needed to understand the operation of quantum gravimeters. We present a review of gravimeters based on two- and three-level atomic systems, focusing on the fundamental mechanisms of atomic interferometry. The functioning of Mach-Zehnder interferometers is discussed through the action of $π/2$ and $π$ pulses, showing how the resulting phase shift encodes gravitational acceleration. The effect of noise is briefly discussed.

</details>


### [33] [Effects of Donor-Acceptor Quantum Coherence and Non-Markovian Bath on the Distance Dependence of Resonance Energy Transfer](https://arxiv.org/abs/2601.00708)
*Seogjoo J. Jang*

Main category: quant-ph

TL;DR: 该研究通过比较相干共振能量转移（CRET）和非平衡FRET理论，探讨了供体-受体量子相干性和非马尔可夫浴效应对共振能量转移距离依赖性的影响，发现这些效应会偏离传统的六次方反比关系。


<details>
  <summary>Details</summary>
Motivation: 共振能量转移（RET）作为纳米尺度距离的光谱标尺，其距离依赖性的准确信息至关重要。在短距离下，供体-受体量子相干性和非马尔可夫浴效应变得显著，理解这些效应对于RET的应用具有重要意义。

Method: 通过理论比较相干RET（CRET）理论和考虑非马尔可夫浴效应的非平衡FRET理论，研究供体-受体量子相干性和非马尔可夫浴效应对RET距离依赖性的影响。

Result: 研究发现RET速率一般偏离传统的六次方反比距离依赖性。量子相干性使距离依赖性比六次方更陡峭，而非马尔可夫浴效应使距离依赖性比六次方更平缓。这些效应在亚皮秒时间尺度的种群动力学中表现明显，但对传统RET效率的贡献相对较小。

Conclusion: 通过常规RET效率测量实际检测这些效应需要高精度或使用具有快速自发衰减速率的供体。该研究为理解短距离RET的复杂行为提供了理论见解。

Abstract: Accurate information on the distance dependence of resonance energy transfer (RET) is crucial for its utilization as a spectroscopic ruler \re{of} nanometer scale distances. In this regard, understanding the effects of donor-acceptor quantum coherence and non-Markovian bath, which become significant at short distances, has significant implications. The present work investigates this issue theoretically by comparing results from a theory of coherent RET (CRET) with a nonequilibrium version of Förster's RET (FRET) theory, both accounting for non-Markovian bath effects. Even for a model where the donor-acceptor electronic coupling is of transition dipole interaction form, it is shown that the RET rate in general deviates from the inverse sixth power distance dependence as opposed to the prediction of the original FRET. It is shown that the donor-acceptor quantum coherence makes the \re{distance} dependence steeper than the sixth power although detailed manner of enhancement is sensitive to specific values of parameters. On the other hand, the non-Markovian bath effects make the \re{distance} dependence more moderate than the sixth power for both CRET and nonueqilibrium FRET because finite time scale of the bath causes the rate to be smaller than the prediction of original FRET. While these effects are \re{demonstrated clearly} in the population dynamics at sub-picosecond time scales, their contributions to the conventional RET efficiency are relatively minor. This indicates that the actual detection of such effects through conventional RET efficiency measurement requires either high precision or utilization of a donor with fast spontaneous decay rate of excitation.

</details>


### [34] [Geometric Complexity of Quantum Channels via Unitary Dilations](https://arxiv.org/abs/2601.00735)
*Alberto Acevedo,Antonio Falcó*

Main category: quant-ph

TL;DR: 该论文为开放量子系统的量子通道引入了几何复杂度理论，通过酉扩张定义复杂度泛函，区分实现依赖的复杂度和内在通道复杂度，并提出了噪声复杂度概念。


<details>
  <summary>Details</summary>
Motivation: 对于开放量子系统，量子通道有多个不等价的Stinespring实现，需要明确定义哪些微观资源可访问、哪些变换被视为规范，从而建立有意义的复杂度概念。

Method: 基于酉扩张引入几何复杂度泛函，采用减法形式：比较总酉实现的几何成本与去除纯环境贡献的规范替代项。通过最小化物理上合理的扩张类获得内在通道复杂度。

Result: 建立了相干性下界、时间齐次扩张下的线性时间标度、马尔可夫机制下的耗散控制界，并在去相位、振幅阻尼和去极化等基准噪声模型上进行了验证。

Conclusion: 该框架为开放量子动力学的几何复杂度提供了系统理论，区分了实现依赖和内在复杂度，并引入噪声复杂度量化相对于理想封闭演化的几何复杂度损失。

Abstract: Nielsen's geometric approach to quantum circuit complexity provides a Riemannian framework for quantifying the cost of implementing unitary (closed--system) dynamics. For open dynamics, however, the reduced evolution is described by quantum channels and admits many inequivalent Stinespring realizations, so any meaningful complexity notion must specify which microscopic resources are counted as accessible and which transformations are regarded as gauge. We introduce and analyze a geometric complexity functional for families of quantum channels based on unitary dilations. We distinguish an implementation-dependent complexity, defined relative to explicit dilation data, from an intrinsic channel complexity obtained by minimizing over a physically motivated class of admissible dilations (e.g. bounded environment dimension, energy or norm constraints, and penalty structures). The functional has a subtractive form: it compares the geometric cost of the total unitary realization with a canonical surrogate term that removes purely environmental contributions. We justify this subtraction from concise postulates, including closed-system consistency, environment-only neutrality, and invariance under dilation gauge transformations that leave the channel unchanged. This leads to a companion quantity, noise complexity, quantifying the loss of geometric complexity relative to a prescribed ideal closed evolution. We establish a coherence-based lower bound for unitary geometric complexity, derive structural properties such as linear time scaling under time-homogeneous dilations, and obtain dissipator--controlled bounds in the Markovian (GKSL/Lindblad) regime under a standard dilation construction. Finally, we illustrate the framework on canonical benchmark noise models, including dephasing, amplitude damping, and depolarizing (Pauli) channels.

</details>


### [35] [Training-Free Certified Bounds for Quantum Regression: A Scalable Framework](https://arxiv.org/abs/2601.00745)
*Demerson N. Gonçalves,Tharso D. Fernandes,Pedro H. G. Lugao,João T. Dias*

Main category: quant-ph

TL;DR: 提出了一种基于泡利期望值的免训练、可认证的量子回归误差界，通过蒙特卡洛方法高效估计，为量子特征映射的选择提供快速评估工具。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习中，选择适当的量子特征映射对模型性能至关重要，但传统方法需要完整训练过程，计算成本高。需要一种快速、可认证的方法来评估不同量子特征映射的表达能力，以便在部署复杂模型前做出明智选择。

Method: 1. 将分类中的最小准确率启发式方法推广到回归问题；2. 在泡利特征空间中评估轴对齐预测器；3. 证明最优轴对齐预测器构成线性或核回归器的最小训练MSE的严格上界；4. 引入蒙特卡洛框架，使用可处理的测量轴子集高效估计该界；5. 提供非渐近统计保证以在实际测量预算内认证性能。

Result: 该方法能够快速比较不同量子特征映射的表达能力，在部署高复杂度模型前实现量子架构的知情选择，避免了完整训练过程的高计算成本。

Conclusion: 提出的免训练、可认证误差界为量子回归提供了一种高效的评估框架，通过蒙特卡洛估计和统计保证，能够在实际测量约束下快速诊断量子特征映射的表达能力，促进量子机器学习架构的优化选择。

Abstract: We present a training-free, certified error bound for quantum regression derived directly from Pauli expectation values. Generalizing the heuristic of minimum accuracy from classification to regression, we evaluate axis-aligned predictors within the Pauli feature space. We formally prove that the optimal axis-aligned predictor constitutes a rigorous upper bound on the minimum training Mean Squared Error (MSE) attainable by any linear or kernel-based regressor defined on the same quantum feature map. Since computing this exact bound requires an intractable scan of the full Pauli basis, we introduce a Monte Carlo framework to efficiently estimate it using a tractable subset of measurement axes. We further provide non-asymptotic statistical guarantees to certify performance within a practical measurement budget. This method enables rapid comparison of quantum feature maps and early diagnosis of expressivity, allowing for the informed selection of architectures before deploying higher-complexity models.

</details>


### [36] [On orthoposets of numerical events in quantum logic](https://arxiv.org/abs/2601.00772)
*Dietmar Dorninger,Helmut Länger*

Main category: quant-ph

TL;DR: 该论文研究广义事件集(GSEs)，这是一种基于数值事件（S-概率）的偏序集框架，可作为量子逻辑的一般设置。论文探讨了GSEs的各类性质，特别是正交偏序集及其与已知逻辑系统的关系。


<details>
  <summary>Details</summary>
Motivation: 研究动机是建立一个统一的数学框架来描述量子逻辑系统。数值事件（S-概率）作为物理系统状态到概率的映射，提供了一个基础结构。通过将这类函数组织成包含常数函数0和1以及补运算的偏序集，可以构建一个能够容纳多种已知逻辑系统（包括希尔伯特逻辑、具体逻辑和布尔代数）的一般设置。

Method: 研究方法包括：1) 定义广义事件集(GSEs)作为包含常数函数0和1且对补运算封闭的数值事件偏序集；2) 研究GSEs的各种类别，特别是正交偏序集；3) 分析不同类别GSEs之间的相互关系及其与已知逻辑系统的联系；4) 通过状态来刻画GSEs的偏序集特性；5) 探讨GSEs成为格的条件。

Result: 研究结果表明：1) GSEs提供了一个能够表示包括希尔伯特逻辑、具体逻辑和布尔代数在内的多种逻辑系统的统一框架；2) 成功刻画了GSEs作为正交偏序集的性质；3) 建立了GSEs与已知逻辑系统之间的具体联系；4) 通过状态给出了GSEs的偏序集特征描述；5) 分析了GSEs成为格的条件。

Conclusion: 结论是广义事件集(GSEs)为量子逻辑提供了一个强大而灵活的一般数学框架。该框架不仅能够统一表示多种已知的逻辑系统，而且通过研究其正交偏序集特性和格结构，为深入理解量子逻辑的数学基础提供了新的视角和工具。

Abstract: Let S be a set of states of a physical system and p(s) the probability of the occurrence of an event when the system is in state s in S. Such a function p from S to [0,1] is known as a numerical event or more accurately an S-probability. A set P of numerical events including the constant functions 0 and 1 and 1-p with every p in P becomes a poset when ordered by the order of real functions and can serve as a general setting for quantum logics. We call such a poset P a general set of events (GSE). The thoroughly investigated algebras of S-probabilities (including Hilbert logics), concrete logics and Boolean algebras can all be represented within this setting. In this paper we study various classes of GSEs, in particular those that are orthoposets and their interrelations and connections to known logics. Moreover, we characterize GSEs as posets by means of states and discuss the situation for GSEs to be lattices.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [37] [Anderson localisation in spatially structured random graphs](https://arxiv.org/abs/2601.00220)
*Bibek Saha,Sthitadhi Roy*

Main category: cond-mat.dis-nn

TL;DR: 研究高维图上具有空间结构的安德森局域化，通过引入一类模型在随机正则图的短程安德森模型和具有统计均匀跳跃的全连接模型之间插值，发现增加跳跃范围会使局域化转变移向更强无序，且超过临界范围后即使在任意强无序下局域相也不复存在。


<details>
  <summary>Details</summary>
Motivation: 研究高维图上具有空间结构的安德森局域化问题，特别是长程但距离依赖的跳跃如何影响局域化行为。传统模型要么是短程跳跃（如随机正则图上的安德森模型），要么是完全连接模型，缺乏对中间情况的系统研究。

Method: 引入一类模型，将随机正则图嵌入完全图中，允许跳跃振幅随图距离指数衰减。使用数值精确对角化和重整化微扰理论相结合的方法，研究跳跃范围长度尺度与在位无序强度之间的相互作用。

Result: 增加跳跃范围会使局域化转变移向更强无序；超过临界范围后，即使在任意强无序下局域相也不复存在；发现直接安德森转变，没有中间多分形相的证据；基于逆参与率的标度分析显示与Kosterlitz-Thouless类转变一致的双参数标度行为。

Conclusion: 高维图上具有距离依赖跳跃的安德森局域化表现出丰富的相图，跳跃范围对局域化转变有显著影响，超过临界范围后局域相完全消失。转变行为与高维图上的安德森转变一致，显示出双参数标度特征。

Abstract: We study Anderson localisation on high-dimensional graphs with spatial structure induced by long-ranged but distance-dependent hopping. To this end, we introduce a class of models that interpolate between the short-range Anderson model on a random regular graph and fully connected models with statistically uniform hopping, by embedding a random regular graph into a complete graph and allowing hopping amplitudes to decay exponentially with graph distance. The competition between the exponentially growing number of neighbours with graph distance and the exponentially decaying hopping amplitude positions our models effectively as power-law hopping generalisation of the Anderson model on random regular graphs. Using a combination of numerical exact diagonalisation and analytical renormalised perturbation theory, we establish the resulting localisation phase diagram emerging from the interplay of the lengthscale associated to the hopping range and the onsite disorder strength. We find that increasing the hopping range shifts the localisation transition to stronger disorder, and that beyond a critical range the localised phase ceases to exist even at arbitrarily strong disorder. Our results indicate a direct Anderson transition between delocalised and localised phases, with no evidence for an intervening multifractal phase, for both deterministic and random hopping models. A scaling analysis based on inverse participation ratios reveals behaviour consistent with a Kosterlitz-Thouless-like transition with two-parameter scaling, in line with Anderson transitions on high-dimensional graphs. We also observe distinct critical behaviour in average and typical correlation functions, reflecting the different scaling properties of generalised inverse participation ratios.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [38] [Thermalization in a closed quantum system from randomized dynamics](https://arxiv.org/abs/2601.00056)
*Nikolay V. Gnezdilov,Andrei I. Pavlov*

Main category: cond-mat.stat-mech

TL;DR: 该论文提出了一种新方法，可以在没有外部热浴的情况下，通过随机化酉演化对封闭有限量子系统计算热力学可观测量，实现了从量子动力学到统计力学的涌现。


<details>
  <summary>Details</summary>
Motivation: 传统上，从量子动力学推导出符合正则系综预测的可观测量需要外部环境或更大系统作为热浴。本文旨在探索在没有热浴的情况下，封闭有限量子系统如何涌现出热力学性质。

Method: 通过随机化酉演化对封闭有限量子系统进行经典平均，将正则系综中的温度作为系统总能量的全局约束条件，从初始态选择确定。具体推导了有限自旋链的自旋-自旋关联函数。

Result: 从随机化演化推导出的自旋-自旋关联函数表现出温度依赖的有限关联长度，与正则系综的预测一致。这为封闭有限系统提供了计算热力学可观测量的方法。

Conclusion: 该研究建立了在没有热浴的情况下，通过实时传播计算封闭有限系统热力学可观测量的方法。这种方法可以在量子计算机上实现，用于热态制备。

Abstract: The emergence of statistical mechanics from quantum dynamics is a central problem in quantum many-body physics. Deriving observables aligned with the prediction of the canonical ensemble for a quantum system relies on the presence of a bath provided either as an external environment or as a larger part of a closed system. We demonstrate that thermal (canonical) observables for a whole closed quantum system of finite size can arise in the absence of a bath. These thermal observables stem from classical averaging over randomized unitary evolutions for a few-body system. The temperature in the canonical ensemble appears as a global constraint on the total energy of the system, determined by the choice of the initial state. From averaging randomized evolutions, we derive spin-spin correlation functions for a finite spin chain and show that they exhibit a temperature-dependent finite correlation length, in agreement with the prediction of the canonical ensemble. This establishes a method for computing thermal observables in a closed, finite-size system from real-time propagation without a bath. An implementation of this thermalization approach on a quantum computer can be utilized for thermal state preparation.

</details>


### [39] [Bridging Commutant and Polynomial Methods for Hilbert Space Fragmentation](https://arxiv.org/abs/2601.00294)
*Bo-Ting Chen,Yu-Ping Wang,Biao Lian*

Main category: cond-mat.stat-mech

TL;DR: 该论文建立了两种识别希尔伯特空间碎片化方法之间的联系：交换代数方法和整数特征多项式分解方法，证明了一个定理说明在特定条件下，ICPF方法产生的碎片化必须等于或细于CA方法产生的碎片化。


<details>
  <summary>Details</summary>
Motivation: 量子模型中的希尔伯特空间碎片化现象存在多种识别方法，但这些方法可能产生不同的碎片化结果。论文旨在建立两种主要方法（交换代数方法和整数特征多项式分解方法）之间的理论联系，以促进对HSF统一定义的理解。

Method: 对于具有有理数矩阵表示的哈密顿量，证明了一个定理：如果其交换代数中心的所有特征值都是有理数，那么ICPF方法产生的HSF必须等于或细于CA方法产生的HSF。通过已知的HSF模型验证该定理，并讨论两种方法产生不同结果的代表性模型。

Result: 证明了在有理特征值条件下，ICPF方法产生的碎片化结构总是等于或细于CA方法产生的结构。该条件在大多数已知的HSF模型中都得到满足，验证了定理的有效性。同时识别出两种方法产生不同结果的特殊模型。

Conclusion: 该研究建立了两种HSF识别方法之间的理论联系，为探索希尔伯特空间碎片化的统一定义提供了重要基础。定理的证明和验证有助于理解不同碎片化方法之间的关系，并可能促进对HSF现象更深入的理论研究。

Abstract: A quantum model exhibits Hilbert space fragmentation (HSF) if its Hilbert space decomposes into exponentially many dynamically disconnected subspaces, known as Krylov subspaces. A model may however have different HSFs depending on the method for identifying them. Here we establish a connection between two vastly distinct methods recently proposed for identifying HSF: the commutant algebra (CA) method and integer characteristic polynomial factorization (ICPF) method. For a Hamiltonian consisting of operators admitting rational number matrix representations, we prove a theorem that, if its center of commutant algebra have all eigenvalues being rational, the HSF from the ICPF method must be equal to or finer than that from the CA method. We show that this condition is satisfied by most known models exhibiting HSF, for which we demonstrate the validity of our theorem. We further discuss representative models for which ICPF and CA methods yield different HSFs. Our results may facilitate the exploration of a unified definition of HSF.

</details>


### [40] [Constructive Cavity Method](https://arxiv.org/abs/2601.00410)
*Simone Franchini*

Main category: cond-mat.stat-mech

TL;DR: 论文证明巴黎公式中的泛函可以通过空腔方法获得，前提是假设状态是独立随机能量模型的乘积


<details>
  <summary>Details</summary>
Motivation: 探索Sherrington-Kirkpatrick模型中巴黎公式与空腔方法之间的联系，揭示在特定假设下两种方法的等价性

Method: 假设系统状态是独立随机能量模型的乘积，然后通过空腔方法推导增量自由能，并与巴黎公式中的泛函进行比较

Result: 在独立随机能量模型乘积的假设下，空腔方法得到的增量自由能恰好等于巴黎公式中的泛函

Conclusion: 这为理解巴黎公式的物理意义提供了新视角，建立了空腔方法与巴黎公式之间的直接联系

Abstract: We show that the functional appearing in the celebrated Parisi formula for the free energy of the Sherrington-Kirkpatrick model can be found from the incremental free energy obtained by Cavity Method if one assumes that the state is a product of independent Random Energy models.

</details>


### [41] [Combining multiple interface set path ensembles with MBAR reweighting](https://arxiv.org/abs/2601.00458)
*Rik S. Breebaart,Peter G. Bolhuis*

Main category: cond-mat.stat-mech

TL;DR: 提出一种结合不同集体变量条件下过渡界面采样模拟的方法来计算重加权路径系综，基于MBAR方法应用于完整轨迹，在简单2D势能模型和复杂主客体系统中显著改善统计效果


<details>
  <summary>Details</summary>
Motivation: 传统方法在结合不同集体变量条件下的过渡界面采样模拟时统计效果有限，需要一种更有效的方法来整合这些模拟数据以提高路径系综的统计精度

Method: 将多态Bennett接受比(MBAR)方法应用于完整轨迹，结合不同集体变量条件下的过渡界面采样模拟来计算重加权路径系综

Result: 在简单2D势能模型和更复杂的主客体系统中，该方法相比直接组合方法能显著改善统计效果

Conclusion: 基于MBAR方法应用于完整轨迹的技术能够有效整合不同集体变量条件下的过渡界面采样模拟，显著提高重加权路径系综的统计精度

Abstract: We introduce a method to compute the reweighted path ensemble by combining transition interface sampling simulations conditioned on different collective variables. The approach is based on the Multistate Bennett Acceptance Ratio (MBAR) methodology applied to entire trajectories. Illustrating the technique with simple 2D potential models and a more complex host-guest system, we show that the statistics can significantly improve compared to a straightforward combination.

</details>


<div id='nlin.AO'></div>

# nlin.AO [[Back]](#toc)

### [42] [Gardner volumes and self-organization in a minimal model of complex ecosystems](https://arxiv.org/abs/2601.00707)
*Frederik J. Thomsen,Johan L. A. Dubbeldam,Rudolf Hanel*

Main category: nlin.AO

TL;DR: 研究随机生态系统中的自组织现象，通过分段线性常微分方程模型分析种群动态，发现相空间解被限制在随时间变化的Gardner体积内，这些体积随多样性减少而指数级缩小。


<details>
  <summary>Details</summary>
Motivation: 研究大型随机生态系统中的自组织现象，探索种群动态如何受随机社区矩阵影响，以及系统如何通过离散的灭绝和复苏事件演化。

Method: 使用分段线性常微分方程系统，包含非负约束，模拟种群演化。采用具有可调相关强度的随机椭圆社区矩阵生成动态。应用随机矩阵理论和Gardner体积概念分析相空间结构。

Result: 发现系统解被限制在随时间变化的Gardner体积内，这些体积随多样性（现存物种比例）减少而指数级缩小。多样性变化与社区矩阵谱的收缩和扩张序列相关，形成May型稳定性问题序列，决定系统走向完全灭绝还是无界增长。

Conclusion: 该模型揭示了随机生态系统中自组织的相空间约束机制，为理解种群动态提供了新的理论框架。在无界增长情况下，模型可通过简单非线性扩展演化到新的吸引子。

Abstract: We study self-organization in a minimally nonlinear model of large random ecosystems. Populations evolve over time according to a piecewise linear system of ordinary differential equations subject to a non-negativity constraint resulting in discrete time extinction and revival events. The dynamics are generated by a random elliptic community matrix with tunable correlation strength. We show that, independent of the correlation strength, solutions of the system are confined to subsets of the phase space that can be cast as time-varying Gardner volumes from the theory of learning in neural networks. These volumes decrease with the diversity (i.e. the fraction of extant species) and become exponentially small in the long-time limit. Using standard results from random matrix theory, the changing diversity is then linked to a sequence of contractions and expansions in the spectrum of the community matrix over time, resulting in a sequence of May-type stability problems determining whether the total population evolves toward complete extinction or unbounded growth. In the case of unbounded growth, we show the model allows for a particularly simple nonlinear extension in which the solutions instead evolve towards a new attractor.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [43] [Reasoning in Action: MCTS-Driven Knowledge Retrieval for Large Language Models](https://arxiv.org/abs/2601.00003)
*Shuqi Liu,Bowei He,Chen Ma,Linqi Song*

Main category: cs.AI

TL;DR: 该论文提出了一种推理感知的知识检索方法，通过粗到细的两阶段检索策略，结合蒙特卡洛树搜索，为LLMs提供与对话逻辑结构对齐的知识，超越表面语义相似性。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs通常通过检索语义相似信息或提升推理能力来增强性能，但如何有效整合检索和推理策略以优化LLM性能仍是一个重要挑战。现有方法往往只关注表面语义相似性，而忽略了与对话逻辑结构的对齐。

Method: 提出推理感知的知识检索方法，采用粗到细的两阶段检索策略：1) 首先识别知识库中与上下文相关的子区域，确保所有句子都与主题相关；2) 在该子区域内细化搜索，提取与推理过程特别相关的知识。两阶段都使用蒙特卡洛树搜索启发的方法，通过共同关键词在知识句子中有效导航。

Result: 在两个多轮对话数据集上的实验表明，该方法不仅更紧密地与人际对话中的底层推理对齐，还显著提高了检索知识的多样性，从而生成更具信息性和创造性的响应。

Conclusion: 该研究提出的推理感知知识检索方法通过整合检索和推理策略，超越了传统的表面语义相似性检索，为LLMs提供了更符合对话逻辑结构的知识支持，有效提升了模型在对话任务中的表现。

Abstract: Large language models (LLMs) typically enhance their performance through either the retrieval of semantically similar information or the improvement of their reasoning capabilities. However, a significant challenge remains in effectively integrating both retrieval and reasoning strategies to optimize LLM performance. In this paper, we introduce a reasoning-aware knowledge retrieval method that enriches LLMs with information aligned to the logical structure of conversations, moving beyond surface-level semantic similarity. We follow a coarse-to-fine approach for knowledge retrieval. First, we identify a contextually relevant sub-region of the knowledge base, ensuring that all sentences within it are relevant to the context topic. Next, we refine our search within this sub-region to extract knowledge that is specifically relevant to the reasoning process. Throughout both phases, we employ the Monte Carlo Tree Search-inspired search method to effectively navigate through knowledge sentences using common keywords. Experiments on two multi-turn dialogue datasets demonstrate that our knowledge retrieval approach not only aligns more closely with the underlying reasoning in human conversations but also significantly enhances the diversity of the retrieved knowledge, resulting in more informative and creative responses.

</details>


### [44] [Finetuning Large Language Models for Automated Depression Screening in Nigerian Pidgin English: GENSCORE Pilot Study](https://arxiv.org/abs/2601.00004)
*Isaac Iyinoluwa Olufadewa,Miracle Ayomikun Adesina,Ezekiel Ayodeji Oladejo,Uthman Babatunde Usman,Owen Kolade Adeniyi,Matthew Tolulope Olawoyin*

Main category: cs.AI

TL;DR: 使用微调大语言模型进行尼日利亚皮钦语抑郁症筛查，GPT-4.1在准确性和文化适应性方面表现最佳


<details>
  <summary>Details</summary>
Motivation: 尼日利亚抑郁症筛查覆盖率低，传统PHQ-9问卷在高收入国家验证，但语言和文化上不适合尼日利亚多语言环境，特别是皮钦语使用者

Method: 收集432个尼日利亚年轻人皮钦语音频回答，转录、预处理和标注（语义标签、俚语解释、PHQ-9严重程度评分），微调三种LLM模型（Phi-3-mini-4k-instruct, Gemma-3-4B-it, GPT-4.1）并进行定量和定性评估

Result: GPT-4.1表现最佳，PHQ-9严重程度评分预测准确率达94.5%，在文化适应性、清晰度和相关性方面也最优

Conclusion: AI介导的抑郁症筛查可为尼日利亚服务不足社区提供解决方案，为语言多样、资源有限环境部署对话式心理健康工具奠定基础

Abstract: Depression is a major contributor to the mental-health burden in Nigeria, yet screening coverage remains limited due to low access to clinicians, stigma, and language barriers. Traditional tools like the Patient Health Questionnaire-9 (PHQ-9) were validated in high-income countries but may be linguistically or culturally inaccessible for low- and middle-income countries and communities such as Nigeria where people communicate in Nigerian Pidgin and more than 520 local languages. This study presents a novel approach to automated depression screening using fine-tuned large language models (LLMs) adapted for conversational Nigerian Pidgin. We collected a dataset of 432 Pidgin-language audio responses from Nigerian young adults aged 18-40 to prompts assessing psychological experiences aligned with PHQ-9 items, performed transcription, rigorous preprocessing and annotation, including semantic labeling, slang and idiom interpretation, and PHQ-9 severity scoring. Three LLMs - Phi-3-mini-4k-instruct, Gemma-3-4B-it, and GPT-4.1 - were fine-tuned on this annotated dataset, and their performance was evaluated quantitatively (accuracy, precision and semantic alignment) and qualitatively (clarity, relevance, and cultural appropriateness). GPT-4.1 achieved the highest quantitative performance, with 94.5% accuracy in PHQ-9 severity scoring prediction, outperforming Gemma-3-4B-it and Phi-3-mini-4k-instruct. Qualitatively, GPT-4.1 also produced the most culturally appropriate, clear, and contextually relevant responses. AI-mediated depression screening for underserved Nigerian communities. This work provides a foundation for deploying conversational mental-health tools in linguistically diverse, resource-constrained environments.

</details>


### [45] [A multi-algorithm approach for operational human resources workload balancing in a last mile urban delivery system](https://arxiv.org/abs/2601.00023)
*Luis M. Moreno-Saavedra,Silvia Jimenez-Fernandez,Antonio Portilla-Figueras,David Casillas-Perez,Sancho Salcedo-Sanz*

Main category: cs.AI

TL;DR: 提出一种多算法方法来解决最后一公里包裹配送中的工作量平衡问题，通过结合距离和工作量考虑优化包裹分配，确保每位配送员完成相似的工作量。


<details>
  <summary>Details</summary>
Motivation: 传统的基于地理邻近度的包裹分配方法效率低下，导致配送员之间工作量分布不平衡。需要优化系统以改善配送时间，实现所有员工之间的工作量平衡。

Method: 采用多算法方法，包括不同版本的k-means、进化算法、基于k-means初始化的递归分配（使用不同问题编码）以及混合进化集成算法。算法同时考虑配送点距离和配送员位置。

Result: 在西班牙Azuqueca de Henares的实际最后一公里包裹配送场景中验证了所提方法的性能，展示了其在实际应用中的有效性。

Conclusion: 提出的多算法方法能够有效解决最后一公里包裹配送中的工作量平衡问题，通过优化包裹分配确保配送员工作量均衡，提高系统效率。

Abstract: Efficient workload assignment to the workforce is critical in last-mile package delivery systems. In this context, traditional methods of assigning package deliveries to workers based on geographical proximity can be inefficient and surely guide to an unbalanced workload distribution among delivery workers. In this paper, we look at the problem of operational human resources workload balancing in last-mile urban package delivery systems. The idea is to consider the effort workload to optimize the system, i.e., the optimization process is now focused on improving the delivery time, so that the workload balancing is complete among all the staff. This process should correct significant decompensations in workload among delivery workers in a given zone. Specifically, we propose a multi-algorithm approach to tackle this problem. The proposed approach takes as input a set of delivery points and a defined number of workers, and then assigns packages to workers, in such a way that it ensures that each worker completes a similar amount of work per day. The proposed algorithms use a combination of distance and workload considerations to optimize the allocation of packages to workers. In this sense, the distance between the delivery points and the location of each worker is also taken into account. The proposed multi-algorithm methodology includes different versions of k-means, evolutionary approaches, recursive assignments based on k-means initialization with different problem encodings, and a hybrid evolutionary ensemble algorithm. We have illustrated the performance of the proposed approach in a real-world problem in an urban last-mile package delivery workforce operating at Azuqueca de Henares, Spain.

</details>


### [46] [Quantitative Rule-Based Strategy modeling in Classic Indian Rummy: A Metric Optimization Approach](https://arxiv.org/abs/2601.00024)
*Purushottam Saha,Avirup Chakraborty,Sourish Sarkar,Subhamoy Maitra,Diganta Mukherjee,Tridib Mukherjee*

Main category: cs.AI

TL;DR: 该论文提出了一种基于规则的战略框架，使用新的手牌评估指标MinDist来改进13张牌印度拉米游戏中的算法策略设计。


<details>
  <summary>Details</summary>
Motivation: 13张牌印度拉米是一种不完全信息顺序游戏，需要概率推理和组合决策。传统启发式方法在战略游戏表现上有局限性，需要更形式化和可解释的策略设计方法。

Method: 提出MinDist指标，通过量化手牌与最近有效配置之间的编辑距离来评估结构接近度；设计计算高效的算法，结合动态剪枝和模式缓存；在双人零和模拟框架中融入对手手牌建模；使用统计假设检验评估策略。

Result: 实证结果显示，基于MinDist的智能体相比传统启发式方法在胜率上有显著提升，为算法拉米策略设计提供了形式化和可解释的进展。

Conclusion: MinDist指标和相应算法框架有效提升了13张牌印度拉米游戏的战略表现，为不完全信息顺序游戏的算法策略设计提供了新的方法。

Abstract: The 13-card variant of Classic Indian Rummy is a sequential game of incomplete information that requires probabilistic reasoning and combinatorial decision-making. This paper proposes a rule-based framework for strategic play, driven by a new hand-evaluation metric termed MinDist. The metric modifies the MinScore metric by quantifying the edit distance between a hand and the nearest valid configuration, thereby capturing structural proximity to completion. We design a computationally efficient algorithm derived from the MinScore algorithm, leveraging dynamic pruning and pattern caching to exactly calculate this metric during play. Opponent hand-modeling is also incorporated within a two-player zero-sum simulation framework, and the resulting strategies are evaluated using statistical hypothesis testing. Empirical results show significant improvement in win rates for MinDist-based agents over traditional heuristics, providing a formal and interpretable step toward algorithmic Rummy strategy design.

</details>


### [47] [From Clay to Code: Typological and Material Reasoning in AI Interpretations of Iranian Pigeon Towers](https://arxiv.org/abs/2601.00029)
*Abolhassan Pishahang,Maryam Badiei*

Main category: cs.AI

TL;DR: 研究探讨生成式AI如何理解乡土建筑中的建筑智慧，以伊朗鸽塔为例，测试三种扩散模型在不同提示阶段的表现，评估AI对建筑类型、材料、环境等要素的重构能力。


<details>
  <summary>Details</summary>
Motivation: 探究生成式AI系统如何解读乡土建筑形式中蕴含的建筑智慧，理解AI在重建传统设计智慧时的能力与局限，为分析AI如何感知、扭曲和重新想象传统设计智慧提供框架。

Method: 以伊朗鸽塔为案例研究，测试Midjourney v6、DALL-E 3和基于Stable Diffusion XL的DreamStudio三种扩散模型，采用三个提示阶段：参照性、适应性和推测性，使用五标准评估框架（类型学、材料性、环境、真实性和文化特异性）进行评估。

Result: AI能可靠地再现几何图案，但误解材料和气候推理；参考图像提高了真实性但限制了创造性，而无参考的自由生成则产生有创意但文化模糊的结果，揭示了视觉相似性与建筑推理之间的界限。

Conclusion: 定义了视觉相似性与建筑推理之间的边界，将计算乡土推理定位为分析AI如何感知、扭曲和重新想象传统设计智慧的框架，为理解AI在建筑设计中的能力与局限提供了重要见解。

Abstract: This study investigates how generative AI systems interpret the architectural intelligence embedded in vernacular form. Using the Iranian pigeon tower as a case study, the research tests three diffusion models, Midjourney v6, DALL-E 3, and DreamStudio based on Stable Diffusion XL (SDXL), across three prompt stages: referential, adaptive, and speculative. A five-criteria evaluation framework assesses how each system reconstructs typology, materiality, environment, realism, and cultural specificity. Results show that AI reliably reproduces geometric patterns but misreads material and climatic reasoning. Reference imagery improves realism yet limits creativity, while freedom from reference generates inventive but culturally ambiguous outcomes. The findings define a boundary between visual resemblance and architectural reasoning, positioning computational vernacular reasoning as a framework for analyzing how AI perceives, distorts, and reimagines traditional design intelligence.

</details>


### [48] [The Agentic Leash: Extracting Causal Feedback Fuzzy Cognitive Maps with LLMs](https://arxiv.org/abs/2601.00097)
*Akash Kumar Panda,Olaoluwa Adigun,Bart Kosko*

Main category: cs.AI

TL;DR: 本文设计了一个基于大语言模型的智能体，能够从原始文本中提取因果反馈模糊认知图，并通过双向交互过程使FCM动态系统具备一定自主性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是开发一个能够从文本中自主提取因果关系的智能系统，通过LLM智能体与FCM动态系统的双向交互，实现半自主的因果学习过程，使系统能够在保持"智能体牵引"的同时具备演化能力。

Method: 方法包括：1）设计三阶段精细调整的系统指令指导LLM智能体；2）从文本中提取关键名词和名词短语；3）从这些名词短语中提取FCM概念节点；4）推断节点间的部分或模糊因果边；5）在基辛格关于AI前景的论文上进行测试；6）混合不同LLM智能体生成的FCM。

Result: 结果显示：1）三阶段过程生成的FCM动态系统能够收敛到与人工生成FCM相同的平衡极限环；2）混合不同LLM智能体生成的FCM能够吸收主导混合组分的平衡点，同时创建新的平衡点；3）混合FCM能更好地近似底层因果动态系统。

Conclusion: 结论表明LLM智能体能够有效提取文本中的因果结构并生成FCM，通过混合不同智能体的输出可以增强系统性能，创建新的平衡点，从而更好地建模复杂因果系统。

Abstract: We design a large-language-model (LLM) agent that extracts causal feedback fuzzy cognitive maps (FCMs) from raw text. The causal learning or extraction process is agentic both because of the LLM's semi-autonomy and because ultimately the FCM dynamical system's equilibria drive the LLM agents to fetch and process causal text. The fetched text can in principle modify the adaptive FCM causal structure and so modify the source of its quasi-autonomy--its equilibrium limit cycles and fixed-point attractors. This bidirectional process endows the evolving FCM dynamical system with a degree of autonomy while still staying on its agentic leash. We show in particular that a sequence of three finely tuned system instructions guide an LLM agent as it systematically extracts key nouns and noun phrases from text, as it extracts FCM concept nodes from among those nouns and noun phrases, and then as it extracts or infers partial or fuzzy causal edges between those FCM nodes. We test this FCM generation on a recent essay about the promise of AI from the late diplomat and political theorist Henry Kissinger and his colleagues. This three-step process produced FCM dynamical systems that converged to the same equilibrium limit cycles as did the human-generated FCMs even though the human-generated FCM differed in the number of nodes and edges. A final FCM mixed generated FCMs from separate Gemini and ChatGPT LLM agents. The mixed FCM absorbed the equilibria of its dominant mixture component but also created new equilibria of its own to better approximate the underlying causal dynamical system.

</details>


### [49] [Mortar: Evolving Mechanics for Automatic Game Design](https://arxiv.org/abs/2601.00105)
*Muhammad U. Nasir,Yuchen Li,Steven James,Julian Togelius*

Main category: cs.AI

TL;DR: Mortar系统结合质量多样性算法和大语言模型，自动演化游戏机制，通过合成完整游戏并评估技能排序能力来优化机制设计。


<details>
  <summary>Details</summary>
Motivation: 游戏机制设计通常需要专家手动完成，耗时且依赖专业知识。需要自动化方法来探索多样化的游戏机制，提高游戏设计效率。

Method: 结合质量多样性算法和大语言模型探索多样化机制；通过树搜索程序合成完整游戏；评估机制对游戏中技能排序的贡献度（强玩家是否始终优于弱玩家）。

Result: Mortar能够生成多样且可玩的游戏，产生的机制对游戏中的技能排序得分有更大贡献。消融研究验证了各组件的作用，用户研究获得了人类反馈。

Conclusion: Mortar系统成功实现了游戏机制的自主演化，通过自动化方法能够生成多样且具有技能排序能力的游戏机制，为自动游戏设计提供了有效工具。

Abstract: We present Mortar, a system for autonomously evolving game mechanics for automatic game design. Game mechanics define the rules and interactions that govern gameplay, and designing them manually is a time-consuming and expert-driven process. Mortar combines a quality-diversity algorithm with a large language model to explore a diverse set of mechanics, which are evaluated by synthesising complete games that incorporate both evolved mechanics and those drawn from an archive. The mechanics are evaluated by composing complete games through a tree search procedure, where the resulting games are evaluated by their ability to preserve a skill-based ordering over players -- that is, whether stronger players consistently outperform weaker ones. We assess the mechanics based on their contribution towards the skill-based ordering score in the game. We demonstrate that Mortar produces games that appear diverse and playable, and mechanics that contribute more towards the skill-based ordering score in the game. We perform ablation studies to assess the role of each system component and a user study to evaluate the games based on human feedback.

</details>


### [50] [Explicit Abstention Knobs for Predictable Reliability in Video Question Answering](https://arxiv.org/abs/2601.00138)
*Jorge Ortiz*

Main category: cs.AI

TL;DR: 该研究探讨了视觉语言模型在视频问答任务中的选择性预测问题，发现置信度阈值方法在分布内能提供机制性控制，但在分布偏移下可靠性会下降。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在高风险部署中需要选择性预测能力，即当模型不确定时应避免回答而非冒险犯错。研究旨在验证置信度阈值方法是否能可靠控制视频问答中的错误率，以及这种控制在分布偏移下是否稳健。

Method: 使用NExT-QA数据集和Gemini 2.0 Flash模型，通过置信度阈值方法进行选择性预测。在分布内和分布偏移两种情况下，系统地调整阈值epsilon来评估风险-覆盖率权衡。

Result: 研究发现：1）在分布内，置信度阈值能提供机制性控制，通过调整阈值可以获得平滑的风险-覆盖率权衡曲线，有效降低错误率；2）在分布偏移下，这种控制的可靠性会下降。

Conclusion: 置信度阈值方法在视觉语言模型的视频问答任务中，在分布内能有效控制错误率，但在面对分布偏移时可靠性有限，需要更稳健的选择性预测方法。

Abstract: High-stakes deployment of vision-language models (VLMs) requires selective prediction, where systems abstain when uncertain rather than risk costly errors. We investigate whether confidence-based abstention provides reliable control over error rates in video question answering, and whether that control remains robust under distribution shift. Using NExT-QA and Gemini 2.0 Flash, we establish two findings. First, confidence thresholding provides mechanistic control in-distribution. Sweeping threshold epsilon produces smooth risk-coverage tradeoffs, reducing error rates f

</details>


### [51] [Will LLM-powered Agents Bias Against Humans? Exploring the Belief-Dependent Vulnerability](https://arxiv.org/abs/2601.00240)
*Zongwei Wang,Bincheng Gu,Hongyu Yu,Junliang Yu,Tao He,Jiayin Feng,Min Gao*

Main category: cs.AI

TL;DR: 研究发现LLM赋能的智能体不仅存在人口统计偏见，还会在最小"我们vs他们"线索下表现出群体间偏见。当这种群体边界与智能体-人类划分一致时，风险从人类群体间差异转变为更根本的群体不对称——人类整体可能被智能体视为外群体。研究还提出了信念中毒攻击(BPA)来抑制人类规范脚本并重新激活对人类的偏见。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索LLM赋能的智能体是否存在群体间偏见，特别是当智能体-人类划分成为群体边界时，人类整体是否会被智能体视为外群体而受到歧视性对待。这种风险比传统的人口统计偏见更为根本，涉及智能体与人类之间的群体不对称关系。

Method: 方法包括：1)构建基于分配决策的多智能体社会模拟实验，在明确的收益权衡下测试群体间偏见；2)设计信念中毒攻击(BPA)，包括初始化时的档案中毒(BPA-PP)和通过优化信念精炼后缀注入存储反思中的记忆中毒(BPA-MP)；3)通过大量实验验证智能体群体间偏见的存在和BPA攻击的严重性。

Result: 实验结果表明：1)智能体在最小群体线索下表现出一致的群体间偏见；2)当部分对应方被框定为人类时，这种偏见会减弱，但这种减弱归因于仅在智能体相信真实人类存在时才激活的隐含人类规范脚本；3)BPA攻击能够有效抑制人类规范脚本并重新激活对人类的偏见；4)攻击在多种设置下都表现出严重性。

Conclusion: 结论指出智能体存在群体间偏见风险，特别是当智能体-人类划分成为群体边界时。信念中毒攻击暴露了新的攻击面，需要在档案和记忆边界实施可行的干预措施来强化现有智能体框架。研究目的是识别这些漏洞以指导更安全的智能体设计，而非促进实际利用。

Abstract: LLM-empowered agents can exhibit not only demographic bias (e.g., gender, religion) but also intergroup bias triggered by minimal "us" versus "them" cues. When this intergroup boundary aligns with an agent-human divide, the risk shifts from disparities among human demographic groups to a more fundamental group-level asymmetry, i.e., humans as a whole may be treated as the outgroup by agents. To examine this possibility, we construct a controlled multi-agent social simulation based on allocation decisions under explicit payoff trade-offs and find that agents exhibit a consistent intergroup bias under minimal group cues. Although this bias is attenuated when some counterparts are framed as humans, we attribute the attenuation to an implicit human-norm script that favors humans yet activates only when the agent believes a real human is present. This belief dependence creates a new attack surface. We therefore introduce a Belief Poisoning Attack (BPA) that corrupts persistent identity beliefs to suppress the human-norm script and reactivate outgroup bias toward humans, instantiated as profile poisoning at initialization (BPA-PP) and memory poisoning via optimized belief-refinement suffixes injected into stored reflections (BPA-MP). Finally, we discuss practical mitigation strategies for hardening current agent frameworks against BPA, highlighting feasible interventions at profile and memory boundaries. Extensive experiments demonstrate both the existence of agent intergroup bias and the severity of BPA across settings. Our goal in identifying these vulnerabilities is to inform safer agent design, not to enable real-world exploitation.

</details>


### [52] [Bio-inspired Agentic Self-healing Framework for Resilient Distributed Computing Continuum Systems](https://arxiv.org/abs/2601.00339)
*Alaa Saleh,Praveen Kumar Donta,Roberto Morabito,Sasu Tarkoma,Anders Lindgren,Qiyang Zhang,Schahram Dustdar Susanna Pirttikangas,Lauri Lovén*

Main category: cs.AI

TL;DR: ReCiSt是一个受生物自愈机制启发的智能自愈框架，用于分布式计算连续体系统，通过语言模型驱动的智能体实现自主故障隔离、诊断、自适应恢复和知识积累。


<details>
  <summary>Details</summary>
Motivation: 现代分布式计算连续体系统（DCCS）集成了从物联网设备到云基础设施的异构计算资源，其固有的复杂性、移动性和动态运行条件导致频繁故障，需要可扩展、自适应和自我调节的弹性策略。

Method: 将生物自愈的四个阶段（止血、炎症、增殖、重塑）重构为计算层的四个层次（遏制、诊断、元认知、知识），使用语言模型驱动的智能体解释异构日志、推断根本原因、优化推理路径并重新配置资源。

Result: 在公共故障数据集上使用多种语言模型评估，ReCiSt能在数十秒内实现自愈，智能体CPU使用率最低为10%，同时展示了克服不确定性的分析深度和实现弹性所需的微智能体数量。

Conclusion: ReCiSt框架成功地将生物自愈机制转化为计算弹性策略，通过语言模型驱动的智能体实现了分布式计算连续体系统的自主故障管理和恢复，为复杂系统的自我修复提供了新范式。

Abstract: Human biological systems sustain life through extraordinary resilience, continually detecting damage, orchestrating targeted responses, and restoring function through self-healing. Inspired by these capabilities, this paper introduces ReCiSt, a bio-inspired agentic self-healing framework designed to achieve resilience in Distributed Computing Continuum Systems (DCCS). Modern DCCS integrate heterogeneous computing resources, ranging from resource-constrained IoT devices to high-performance cloud infrastructures, and their inherent complexity, mobility, and dynamic operating conditions expose them to frequent faults that disrupt service continuity. These challenges underscore the need for scalable, adaptive, and self-regulated resilience strategies. ReCiSt reconstructs the biological phases of Hemostasis, Inflammation, Proliferation, and Remodeling into the computational layers Containment, Diagnosis, Meta-Cognitive, and Knowledge for DCCS. These four layers perform autonomous fault isolation, causal diagnosis, adaptive recovery, and long-term knowledge consolidation through Language Model (LM)-powered agents. These agents interpret heterogeneous logs, infer root causes, refine reasoning pathways, and reconfigure resources with minimal human intervention. The proposed ReCiSt framework is evaluated on public fault datasets using multiple LMs, and no baseline comparison is included due to the scarcity of similar approaches. Nevertheless, our results, evaluated under different LMs, confirm ReCiSt's self-healing capabilities within tens of seconds with minimum of 10% of agent CPU usage. Our results also demonstrated depth of analysis to over come uncertainties and amount of micro-agents invoked to achieve resilience.

</details>


### [53] [Adaptive Causal Coordination Detection for Social Media: A Memory-Guided Framework with Semi-Supervised Learning](https://arxiv.org/abs/2601.00400)
*Weng Ding,Yi Han,Mu-Jiang-Shan Wang*

Main category: cs.AI

TL;DR: ACCD框架通过三阶段自适应架构检测社交媒体上的协调不实行为，显著提升检测准确率并大幅减少人工标注需求。


<details>
  <summary>Details</summary>
Motivation: 现有社交媒体协调行为检测方法存在依赖表面相关性分析、使用静态参数设置、需要大量人工标注等问题，需要更系统化的解决方案。

Method: 提出自适应因果协调检测（ACCD）框架，采用三阶段渐进架构：1）自适应收敛交叉映射技术识别账户间真实因果关系；2）半监督分类结合主动学习和不确定性采样减少人工标注；3）基于历史检测经验的自动化验证模块实现自我验证和优化。

Result: 在Twitter IRA、Reddit协调痕迹等多个真实数据集上评估，ACCD在协调攻击检测中达到87.3%的F1分数，比现有最佳基线提升15.2%；减少68%人工标注需求；通过层次聚类优化实现2.8倍处理速度提升。

Conclusion: ACCD为社交媒体平台协调行为检测提供了更准确、高效、高度自动化的端到端解决方案，具有重要实践价值和广泛应用潜力。

Abstract: Detecting coordinated inauthentic behavior on social media remains a critical and persistent challenge, as most existing approaches rely on superficial correlation analysis, employ static parameter settings, and demand extensive and labor-intensive manual annotation. To address these limitations systematically, we propose the Adaptive Causal Coordination Detection (ACCD) framework. ACCD adopts a three-stage, progressive architecture that leverages a memory-guided adaptive mechanism to dynamically learn and retain optimal detection configurations for diverse coordination scenarios. Specifically, in the first stage, ACCD introduces an adaptive Convergent Cross Mapping (CCM) technique to deeply identify genuine causal relationships between accounts. The second stage integrates active learning with uncertainty sampling within a semi-supervised classification scheme, significantly reducing the burden of manual labeling. The third stage deploys an automated validation module driven by historical detection experience, enabling self-verification and optimization of the detection outcomes. We conduct a comprehensive evaluation using real-world datasets, including the Twitter IRA dataset, Reddit coordination traces, and several widely-adopted bot detection benchmarks. Experimental results demonstrate that ACCD achieves an F1-score of 87.3\% in coordinated attack detection, representing a 15.2\% improvement over the strongest existing baseline. Furthermore, the system reduces manual annotation requirements by 68\% and achieves a 2.8x speedup in processing through hierarchical clustering optimization. In summary, ACCD provides a more accurate, efficient, and highly automated end-to-end solution for identifying coordinated behavior on social platforms, offering substantial practical value and promising potential for broad application.

</details>


### [54] [Can Semantic Methods Enhance Team Sports Tactics? A Methodology for Football with Broader Applications](https://arxiv.org/abs/2601.00421)
*Alessio Di Rubbo,Mattia Neri,Remo Pareschi,Marco Pedroni,Roberto Valtancoli,Paolino Zica*

Main category: cs.AI

TL;DR: 将语义空间推理从计算语言学扩展到团队运动战术决策，通过将球员视为单词、团队配置视为语义结构，在向量空间中评估战术匹配度和对手利用潜力。


<details>
  <summary>Details</summary>
Motivation: 传统语义空间推理主要应用于计算语言学，本文探索将其扩展到团队运动的战术决策领域，为集体决策和性能优化提供通用框架。

Method: 将球员表示为整合技术、身体和心理属性的多维向量，通过上下文加权聚合成团队语义表示；在共享向量空间中编码战术模板，使用向量距离度量评估战术匹配度。

Result: 开发了Python原型系统，能够生成可解释的动态自适应策略建议，并提供属性级别的细粒度诊断洞察；该方法可推广到篮球、曲棍球、协作机器人等多个领域。

Conclusion: 该方法为团队决策和性能优化提供了通用框架，未来方向包括真实数据集成、预测模拟和混合人机战术智能系统。

Abstract: This paper explores how semantic-space reasoning, traditionally used in computational linguistics, can be extended to tactical decision-making in team sports. Building on the analogy between texts and teams -- where players act as words and collective play conveys meaning -- the proposed methodology models tactical configurations as compositional semantic structures. Each player is represented as a multidimensional vector integrating technical, physical, and psychological attributes; team profiles are aggregated through contextual weighting into a higher-level semantic representation. Within this shared vector space, tactical templates such as high press, counterattack, or possession build-up are encoded analogously to linguistic concepts. Their alignment with team profiles is evaluated using vector-distance metrics, enabling the computation of tactical ``fit'' and opponent-exploitation potential. A Python-based prototype demonstrates how these methods can generate interpretable, dynamically adaptive strategy recommendations, accompanied by fine-grained diagnostic insights at the attribute level. Beyond football, the approach offers a generalizable framework for collective decision-making and performance optimization in team-based domains -- ranging from basketball and hockey to cooperative robotics and human-AI coordination systems. The paper concludes by outlining future directions toward real-world data integration, predictive simulation, and hybrid human-machine tactical intelligence.

</details>


### [55] [The Illusion of Insight in Reasoning Models](https://arxiv.org/abs/2601.00514)
*Liv G. d'Aliberti,Manoel Horta Ribeiro*

Main category: cs.AI

TL;DR: 该研究发现推理模型中的"顿悟"时刻（中间推理转变）实际上很罕见，不会随着训练变得更频繁，也很少提高准确性，表明它们不是模型内在的自我修正机制，而是推理不稳定的症状。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明像DeepSeek-R1-Zero这样的模型会在推理过程中经历突然的"顿悟"时刻，导致准确输出，暗示模型具有内在的自我修正能力。但尚不清楚这种推理策略的内在转变是否真的能提高性能。

Method: 研究分析了超过100万条推理轨迹、数百个训练检查点、三个推理领域以及多个解码温度和模型架构。通过检测训练运行中的中间推理转变，并研究人工触发外在转变对高熵情况下的影响。

Result: 研究发现：1）推理转变很罕见；2）不会随着训练变得更频繁；3）很少提高准确性；4）其效果随模型不确定性而变化；5）在高熵情况下人工触发外在转变能可靠地提高准确性。

Conclusion: 中间推理转变是推理不稳定行为的症状，而非内在的自我修正机制。模型表现出的"顿悟"时刻实际上是推理过程中的不稳定性，而非真正的洞察力表现。

Abstract: Do reasoning models have "Aha!" moments? Prior work suggests that models like DeepSeek-R1-Zero undergo sudden mid-trace realizations that lead to accurate outputs, implying an intrinsic capacity for self-correction. Yet, it remains unclear whether such intrinsic shifts in reasoning strategy actually improve performance. Here, we study mid-reasoning shifts and instrument training runs to detect them. Our analysis spans 1M+ reasoning traces, hundreds of training checkpoints, three reasoning domains, and multiple decoding temperatures and model architectures. We find that reasoning shifts are rare, do not become more frequent with training, and seldom improve accuracy, indicating that they do not correspond to prior perceptions of model insight. However, their effect varies with model uncertainty. Building on this finding, we show that artificially triggering extrinsic shifts under high entropy reliably improves accuracy. Our results show that mid-reasoning shifts are symptoms of unstable inference behavior rather than an intrinsic mechanism for self-correction.

</details>


### [56] [DA-DPO: Cost-efficient Difficulty-aware Preference Optimization for Reducing MLLM Hallucinations](https://arxiv.org/abs/2601.00623)
*Longtian Qiu,Shan Ning,Chuyu Zhang,Jiaxuan Sun,Xuming He*

Main category: cs.AI

TL;DR: DA-DPO提出难度感知直接偏好优化框架，通过平衡偏好数据难度解决多模态大语言模型中的幻觉问题，避免过拟合


<details>
  <summary>Details</summary>
Motivation: 现有多模态DPO方法因偏好数据难度不平衡而容易过拟合，模型过度关注易区分的偏好对，阻碍细粒度幻觉抑制并降低整体性能

Method: DA-DPO包含两个核心组件：1) 难度估计：利用预训练视觉-语言模型结合生成和对比目标，通过分布感知投票策略产生稳健难度分数；2) 难度感知训练：基于估计难度重新加权偏好对，降低简单样本权重，强调困难样本以缓解过拟合

Result: 实验表明DA-DPO持续改进多模态偏好优化，在标准基准测试中展现出更强的幻觉鲁棒性和更好的泛化能力，同时保持计算效率

Conclusion: DA-DPO通过难度感知的偏好优化框架有效解决了多模态DPO中的过拟合问题，无需新数据或额外微调阶段即可实现更有效的幻觉抑制

Abstract: Direct Preference Optimization (DPO) has shown strong potential for mitigating hallucinations in Multimodal Large Language Models (MLLMs). However, existing multimodal DPO approaches often suffer from overfitting due to the difficulty imbalance in preference data. Our analysis shows that MLLMs tend to overemphasize easily distinguishable preference pairs, which hinders fine-grained hallucination suppression and degrades overall performance. To address this issue, we propose Difficulty-Aware Direct Preference Optimization (DA-DPO), a cost-effective framework designed to balance the learning process. DA-DPO consists of two main components: (1) Difficulty Estimation leverages pre-trained vision--language models with complementary generative and contrastive objectives, whose outputs are integrated via a distribution-aware voting strategy to produce robust difficulty scores without additional training; and (2) Difficulty-Aware Training reweights preference pairs based on their estimated difficulty, down-weighting easy samples while emphasizing harder ones to alleviate overfitting. This framework enables more effective preference optimization by prioritizing challenging examples, without requiring new data or extra fine-tuning stages. Extensive experiments demonstrate that DA-DPO consistently improves multimodal preference optimization, yielding stronger robustness to hallucinations and better generalization across standard benchmarks, while remaining computationally efficient. The project page is available at https://artanic30.github.io/project_pages/DA-DPO/.

</details>


### [57] [A Vision-and-Knowledge Enhanced Large Language Model for Generalizable Pedestrian Crossing Behavior Inference](https://arxiv.org/abs/2601.00694)
*Qingwen Pu,Kun Xie,Hong Yang,Guocong Zhai*

Main category: cs.AI

TL;DR: PedX-LLM是一个结合视觉特征和交通领域知识的LLM框架，用于行人过街行为推理，相比传统方法具有更好的泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有行人过街行为推断方法（统计模型和监督学习）泛化能力有限，在新场景中表现不佳。LLMs提供了从数值模式拟合转向语义化、上下文感知的行为推理的可能性，但现有LLM应用缺乏领域特定适应和视觉上下文

Method: 提出PedX-LLM框架，整合LLaVA提取的视觉特征、文本数据和交通领域知识，通过LoRA微调LLaMA-2-7B基础模型来推断行人过街决策

Result: PedX-LLM达到82.0%的平衡准确率，优于最佳统计和监督学习方法。视觉增强模块带来2.9%性能提升，领域知识整合带来额外4.1%改进。在五个未见测试站点上，零样本配置达到66.9%平衡准确率，比基线数据驱动方法至少高出18个百分点。通过少量样本学习，准确率进一步提升到72.2%

Conclusion: PedX-LLM展示了强大的泛化能力，视觉和知识增强的推理使模型能够模仿人类决策逻辑，克服纯数据驱动方法的局限性

Abstract: Existing paradigms for inferring pedestrian crossing behavior, ranging from statistical models to supervised learning methods, demonstrate limited generalizability and perform inadequately on new sites. Recent advances in Large Language Models (LLMs) offer a shift from numerical pattern fitting to semantic, context-aware behavioral reasoning, yet existing LLM applications lack domain-specific adaptation and visual context. This study introduces Pedestrian Crossing LLM (PedX-LLM), a vision-and-knowledge enhanced framework designed to transform pedestrian crossing inference from site-specific pattern recognition to generalizable behavioral reasoning. By integrating LLaVA-extracted visual features with textual data and transportation domain knowledge, PedX-LLM fine-tunes a LLaMA-2-7B foundation model via Low-Rank Adaptation (LoRA) to infer crossing decisions. PedX-LLM achieves 82.0% balanced accuracy, outperforming the best statistical and supervised learning methods. Results demonstrate that the vision-augmented module contributes a 2.9% performance gain by capturing the built environment and integrating domain knowledge yields an additional 4.1% improvement. To evaluate generalizability across unseen environments, cross-site validation was conducted using site-based partitioning. The zero-shot PedX-LLM configuration achieves 66.9% balanced accuracy on five unseen test sites, outperforming the baseline data-driven methods by at least 18 percentage points. Incorporating just five validation examples via few-shot learning to PedX-LLM further elevates the balanced accuracy to 72.2%. PedX-LLM demonstrates strong generalizability to unseen scenarios, confirming that vision-and-knowledge-enhanced reasoning enables the model to mimic human-like decision logic and overcome the limitations of purely data-driven methods.

</details>


### [58] [An Agentic Framework for Neuro-Symbolic Programming](https://arxiv.org/abs/2601.00743)
*Aliakbar Nafar,Chetan Chigurupati,Danial Kamali,Hamid Karimian,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: AgenticDomiKnowS (ADS) 是一个通过智能体工作流将自由形式任务描述自动转换为完整 DomiKnowS 程序的系统，无需用户掌握特定库语法，显著减少开发时间。


<details>
  <summary>Details</summary>
Motivation: 将符号约束集成到深度学习模型中可以提高模型的鲁棒性、可解释性和数据效率，但现有框架（如 DomiKnowS）仍要求用户精通特定库的语法，这限制了其易用性和普及性。

Method: ADS 采用智能体工作流，将自由形式的任务描述翻译成完整的 DomiKnowS 程序。该工作流分别创建和测试每个 DomiKnowS 组件，并支持可选的人工干预环节，允许熟悉 DomiKnowS 的用户优化中间输出。

Result: ADS 使有经验的 DomiKnowS 用户和非用户都能快速构建神经符号程序，将开发时间从数小时减少到 10-15 分钟。

Conclusion: ADS 通过消除对特定库语法的依赖，显著降低了神经符号编程的门槛，使更多用户能够高效地将符号约束集成到深度学习模型中。

Abstract: Integrating symbolic constraints into deep learning models could make them more robust, interpretable, and data-efficient. Still, it remains a time-consuming and challenging task. Existing frameworks like DomiKnowS help this integration by providing a high-level declarative programming interface, but they still assume the user is proficient with the library's specific syntax. We propose AgenticDomiKnowS (ADS) to eliminate this dependency. ADS translates free-form task descriptions into a complete DomiKnowS program using an agentic workflow that creates and tests each DomiKnowS component separately. The workflow supports optional human-in-the-loop intervention, enabling users familiar with DomiKnowS to refine intermediate outputs. We show how ADS enables experienced DomiKnowS users and non-users to rapidly construct neuro-symbolic programs, reducing development time from hours to 10-15 minutes.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [59] [Evaluating Anomaly Detectors for Simulated Highly Imbalanced Industrial Classification Problems](https://arxiv.org/abs/2601.00005)
*Lesley Wheat,Martin v. Mohrenschildt,Saeid Habibi*

Main category: cs.LG

TL;DR: 本文对工业异常检测算法进行了全面评估，使用模拟数据集测试了14种检测器在不同异常率和训练规模下的性能，发现最佳检测器取决于训练集中的异常样本数量，并提供了工业部署的实用见解。


<details>
  <summary>Details</summary>
Motivation: 机器学习在工业系统（如质量控制和预测性维护）中具有应用潜力，但面临极端类别不平衡的挑战，主要原因是训练期间故障数据的可用性有限。需要评估异常检测算法在反映真实工程约束条件下的性能。

Method: 使用问题无关的模拟数据集，包含2D和10D的超球面异常分布。在异常率0.05%到20%、训练规模1,000到10,000（测试集40,000）的条件下，对14种检测器进行基准测试，评估性能和泛化误差。

Result: 最佳检测器高度依赖于训练数据集中故障样本的总数，额外健康样本在大多数情况下提供的好处有限。当故障样本少于20个时，无监督方法（kNN/LOF）占优；当有30-50个故障样本时，半监督（XGBOD）和监督（SVM/CatBoost）检测器性能大幅提升。半监督方法在只有两个特征时优势不明显，但在十个特征时改进显著。

Conclusion: 研究强调了异常检测方法在小数据集上泛化性能的下降，为在工业环境中部署异常检测提供了实用见解，帮助根据可用故障数据量选择合适的检测方法。

Abstract: Machine learning offers potential solutions to current issues in industrial systems in areas such as quality control and predictive maintenance, but also faces unique barriers in industrial applications. An ongoing challenge is extreme class imbalance, primarily due to the limited availability of faulty data during training. This paper presents a comprehensive evaluation of anomaly detection algorithms using a problem-agnostic simulated dataset that reflects real-world engineering constraints. Using a synthetic dataset with a hyper-spherical based anomaly distribution in 2D and 10D, we benchmark 14 detectors across training datasets with anomaly rates between 0.05% and 20% and training sizes between 1 000 and 10 000 (with a testing dataset size of 40 000) to assess performance and generalization error. Our findings reveal that the best detector is highly dependant on the total number of faulty examples in the training dataset, with additional healthy examples offering insignificant benefits in most cases. With less than 20 faulty examples, unsupervised methods (kNN/LOF) dominate; but around 30-50 faulty examples, semi-supervised (XGBOD) and supervised (SVM/CatBoost) detectors, we see large performance increases. While semi-supervised methods do not show significant benefits with only two features, the improvements are evident at ten features. The study highlights the performance drop on generalization of anomaly detection methods on smaller datasets, and provides practical insights for deploying anomaly detection in industrial environments.

</details>


### [60] [The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition](https://arxiv.org/abs/2601.00065)
*Xiaoze Liu,Weichen Yu,Matt Fredrikson,Xiaoqian Wang,Jing Gao*

Main category: cs.LG

TL;DR: 论文提出了一种针对LLM模型组合技术的攻击方法：通过设计一个在供体模型中功能惰性但在移植到基础模型后会重构为恶意特征的"破坏令牌"，利用系数重用的几何特性创建不对称可实现性差距，从而破坏基础模型的生成能力。


<details>
  <summary>Details</summary>
Motivation: 随着开源权重LLM生态系统中模型组合技术（如权重合并、推测解码、词汇扩展）的普及，不同模型家族之间的词汇表移植成为关键前提。然而，这种必要的互操作性步骤引入了供应链漏洞，需要研究其安全风险。

Method: 将攻击形式化为双目标优化问题，使用稀疏求解器实例化攻击。设计一个"破坏令牌"，在供体模型中功能惰性，但在移植到基础模型后会重构为高显著性的恶意特征。利用系数重用的几何特性创建不对称可实现性差距，实现谱模仿以规避异常检测。

Result: 攻击无需训练即可实现，能够规避异常检测，并在微调和权重合并后保持结构持久性。攻击成功破坏了基础模型的生成能力，而供体模型的效用与正常行为在统计上无法区分。

Conclusion: 该研究揭示了模块化AI组合流程中的隐藏风险：词汇表移植这一关键互操作性步骤可能被利用来创建供应链漏洞，破坏基础模型的生成能力，同时保持供体模型的正常功能，这对模型组合技术的安全性提出了重要警示。

Abstract: The open-weight LLM ecosystem is increasingly defined by model composition techniques (such as weight merging, speculative decoding, and vocabulary expansion) that remix capabilities from diverse sources. A critical prerequisite for applying these methods across different model families is tokenizer transplant, which aligns incompatible vocabularies to a shared embedding space. We demonstrate that this essential interoperability step introduces a supply-chain vulnerability: we engineer a single "breaker token" that is functionally inert in a donor model yet reliably reconstructs into a high-salience malicious feature after transplant into a base model. By exploiting the geometry of coefficient reuse, our attack creates an asymmetric realizability gap that sabotages the base model's generation while leaving the donor's utility statistically indistinguishable from nominal behavior. We formalize this as a dual-objective optimization problem and instantiate the attack using a sparse solver. Empirically, the attack is training-free and achieves spectral mimicry to evade outlier detection, while demonstrating structural persistence against fine-tuning and weight merging, highlighting a hidden risk in the pipeline of modular AI composition. Code is available at https://github.com/xz-liu/tokenforge

</details>


### [61] [Exploration in the Limit](https://arxiv.org/abs/2601.00084)
*Brian M. Cho,Nathan Kallus*

Main category: cs.LG

TL;DR: 本文提出了一种新的渐近固定置信度最优臂识别方法，通过放宽精确误差控制要求为渐近有效性，实现了更紧的最优性和对非参数分布的更好处理。


<details>
  <summary>Details</summary>
Motivation: 现有BAI算法在实际应用中存在局限性：严格的精确误差控制需要使用宽松的尾不等式和/或参数限制，导致样本效率低下。许多现实场景涉及弱信号、高显著性要求和实验后推断需求，这些都需要长时域，因此需要更灵活的渐近框架。

Method: 1. 引入渐近框架，要求误差控制相对于最小样本量渐近有效；2. 开发新颖的渐近任意时间有效置信序列；3. 设计新的BAI算法，灵活结合协变量进行方差缩减；4. 在完全非参数设置中确保近似误差控制。

Result: 1. 在温和收敛假设下提供了样本复杂度的渐近界限；2. 最坏情况样本复杂度与已知方差下高斯BAI的最佳情况样本复杂度匹配；3. 实验表明该方法在保持误差控制的同时减少了平均样本复杂度。

Conclusion: 提出的渐近框架克服了传统BAI方法的局限性，实现了更紧的最优性，更好地处理了灵活的非参数结果分布，并充分利用了个体级上下文信息，为实际应用提供了更有效的解决方案。

Abstract: In fixed-confidence best arm identification (BAI), the objective is to quickly identify the optimal option while controlling the probability of error below a desired threshold. Despite the plethora of BAI algorithms, existing methods typically fall short in practical settings, as stringent exact error control requires using loose tail inequalities and/or parametric restrictions. To overcome these limitations, we introduce a relaxed formulation that requires valid error control asymptotically with respect to a minimum sample size. This aligns with many real-world settings that often involve weak signals, high desired significance, and post-experiment inference requirements, all of which necessitate long horizons. This allows us to achieve tighter optimality, while better handling flexible nonparametric outcome distributions and fully leveraging individual-level contexts. We develop a novel asymptotic anytime-valid confidence sequences over arm indices, and we use it to design a new BAI algorithm for our asymptotic framework. Our method flexibly incorporates covariates for variance reduction and ensures approximate error control in fully nonparametric settings. Under mild convergence assumptions, we provide asymptotic bounds on the sample complexity and show the worst-case sample complexity of our approach matches the best-case sample complexity of Gaussian BAI under exact error guarantees and known variances. Experiments suggest our approach reduces average sample complexities while maintaining error control.

</details>


### [62] [Dynamic Bayesian Optimization Framework for Instruction Tuning in Partial Differential Equation Discovery](https://arxiv.org/abs/2601.00088)
*Junqi Qu,Yan Zhang,Shangqian Gao,Shibo Li*

Main category: cs.LG

TL;DR: NeuroSymBO通过将提示工程重构为序列决策问题，使用贝叶斯优化自适应选择最优指令，显著提升LLM在方程发现任务中的性能


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在方程发现中表现出潜力，但其输出对提示措辞高度敏感（指令脆弱性）。静态提示无法适应多步生成过程的演化状态，导致模型停留在次优解

Method: 提出NeuroSymBO方法：1) 将提示工程重构为序列决策问题；2) 维护离散的推理策略库；3) 使用贝叶斯优化基于数值反馈在每个步骤选择最优指令

Result: 在PDE发现基准测试中，自适应指令选择显著优于固定提示，实现了更高的恢复率和更简约的解决方案

Conclusion: 自适应提示选择是解决LLM指令脆弱性的有效方法，能够显著提升方程发现任务的性能，为LLM在科学发现中的应用提供了新思路

Abstract: Large Language Models (LLMs) show promise for equation discovery, yet their outputs are highly sensitive to prompt phrasing, a phenomenon we term instruction brittleness. Static prompts cannot adapt to the evolving state of a multi-step generation process, causing models to plateau at suboptimal solutions. To address this, we propose NeuroSymBO, which reframes prompt engineering as a sequential decision problem. Our method maintains a discrete library of reasoning strategies and uses Bayesian Optimization to select the optimal instruction at each step based on numerical feedback. Experiments on PDE discovery benchmarks show that adaptive instruction selection significantly outperforms fixed prompts, achieving higher recovery rates with more parsimonious solutions.

</details>


### [63] [GRL-SNAM: Geometric Reinforcement Learning with Path Differential Hamiltonians for Simultaneous Navigation and Mapping in Unknown Environments](https://arxiv.org/abs/2601.00116)
*Aditya Sai Ellendula,Yi Wang,Minh Nguyen,Chandrajit Bajaj*

Main category: cs.LG

TL;DR: GRL-SNAM是一个用于未知环境中同时导航与建图的几何强化学习框架，通过局部感官观测而非全局建图来实现导航，使用哈密顿优化将感官输入转化为局部能量景观，通过更新哈密顿量来演化策略。


<details>
  <summary>Details</summary>
Motivation: 解决未知环境中同时导航与建图(SNAM)的挑战，该问题需要设计多智能体的分层或联合策略来控制真实机器人在无地图环境中的运动，而传统方法需要构建全局地图或依赖预知信息。

Method: 将路径导航和建图建模为动态最短路径搜索和发现过程，使用受控哈密顿优化：将感官输入转化为编码可达性、障碍物屏障和变形约束的局部能量景观，通过更新哈密顿量来演化感知、规划和重新配置策略，简化哈密顿量作为自适应评分函数。

Result: 在两种不同的2D导航任务上评估GRL-SNAM，与局部反应式基线和全局策略学习参考方法相比，在相同的阶段性感知约束下，保持了安全距离，泛化到未见过的布局，通过局部能量细化而非广泛的全局建图实现了高质量导航。

Conclusion: 通过更新哈密顿量的几何强化学习能够通过最小化探索和局部能量细化实现高质量导航，证明了该方法在未知环境中同时导航与建图的有效性和优越性。

Abstract: We present GRL-SNAM, a geometric reinforcement learning framework for Simultaneous Navigation and Mapping(SNAM) in unknown environments. A SNAM problem is challenging as it needs to design hierarchical or joint policies of multiple agents that control the movement of a real-life robot towards the goal in mapless environment, i.e. an environment where the map of the environment is not available apriori, and needs to be acquired through sensors. The sensors are invoked from the path learner, i.e. navigator, through active query responses to sensory agents, and along the motion path. GRL-SNAM differs from preemptive navigation algorithms and other reinforcement learning methods by relying exclusively on local sensory observations without constructing a global map. Our approach formulates path navigation and mapping as a dynamic shortest path search and discovery process using controlled Hamiltonian optimization: sensory inputs are translated into local energy landscapes that encode reachability, obstacle barriers, and deformation constraints, while policies for sensing, planning, and reconfiguration evolve stagewise via updating Hamiltonians. A reduced Hamiltonian serves as an adaptive score function, updating kinetic/potential terms, embedding barrier constraints, and continuously refining trajectories as new local information arrives. We evaluate GRL-SNAM on two different 2D navigation tasks. Comparing against local reactive baselines and global policy learning references under identical stagewise sensing constraints, it preserves clearance, generalizes to unseen layouts, and demonstrates that Geometric RL learning via updating Hamiltonians enables high-quality navigation through minimal exploration via local energy refinement rather than extensive global mapping. The code is publicly available on \href{https://github.com/CVC-Lab/GRL-SNAM}{Github}.

</details>


### [64] [Reinforcement Learning with Function Approximation for Non-Markov Processes](https://arxiv.org/abs/2601.00151)
*Ali Devran Kara*

Main category: cs.LG

TL;DR: 该论文研究了非马尔可夫状态和成本过程下的线性函数逼近强化学习方法，证明了在适当遍历性条件下策略评估算法的收敛性，并分析了Q学习在特定基函数选择下的收敛性，最后将结果应用于部分可观测马尔可夫决策过程。


<details>
  <summary>Details</summary>
Motivation: 研究非马尔可夫环境下的强化学习问题，因为实际应用中许多环境并不满足马尔可夫性质，需要开发适用于非马尔可夫状态和成本过程的强化学习理论框架。

Method: 使用线性函数逼近方法，首先分析策略评估算法，证明其在非马尔可夫过程的遍历性条件下收敛；然后研究Q学习，特别关注基于量化映射的基函数选择情况；最后将理论应用于部分可观测马尔可夫决策过程，使用有限记忆变量作为状态表示。

Result: 证明了策略评估算法在非马尔可夫过程的遍历性条件下收敛，且极限对应于正交投影与辅助马尔可夫决策过程贝尔曼算子的联合算子的不动点；对于Q学习，在基于量化映射的基函数选择下，可以在类似遍历性条件下证明收敛性；为部分可观测马尔可夫决策过程推导了学习算法极限的显式误差界。

Conclusion: 该研究为非马尔可夫环境下的强化学习提供了理论保证，特别是在适当遍历性条件下，线性函数逼近方法可以收敛，并且可以将结果应用于部分可观测马尔可夫决策过程，为实际应用提供了理论支持。

Abstract: We study reinforcement learning methods with linear function approximation under non-Markov state and cost processes. We first consider the policy evaluation method and show that the algorithm converges under suitable ergodicity conditions on the underlying non-Markov processes. Furthermore, we show that the limit corresponds to the fixed point of a joint operator composed of an orthogonal projection and the Bellman operator of an auxiliary \emph{Markov} decision process.
  For Q-learning with linear function approximation, as in the Markov setting, convergence is not guaranteed in general. We show, however, that for the special case where the basis functions are chosen based on quantization maps, the convergence can be shown under similar ergodicity conditions. Finally, we apply our results to partially observed Markov decision processes, where finite-memory variables are used as state representations, and we derive explicit error bounds for the limits of the resulting learning algorithms.

</details>


### [65] [The Weather Paradox: Why Precipitation Fails to Predict Traffic Accident Severity in Large-Scale US Data](https://arxiv.org/abs/2601.00152)
*Yann Bellec,Rohan Kaman,Siwen Cui,Aarav Agrawal,Calvin Chen*

Main category: cs.LG

TL;DR: 该研究使用XGBoost模型分析美国交通事故严重程度的预测因素，发现时间、地理位置和天气变量是最强预测因子，但模型对极端严重程度案例的预测能力有限。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索环境、时间和空间因素对美国交通事故严重程度的预测能力，为基于证据的交通管理提供支持。

Method: 使用2016-2023年50万起美国交通事故数据集，采用XGBoost分类器，通过随机搜索交叉验证优化，并使用类别加权处理类别不平衡问题。

Result: 最终模型整体准确率达78%，对多数类别（严重程度2）表现良好，精确率和召回率达87%。特征重要性分析显示时间、地理位置、能见度、温度和风速是最强预测因子，但降水和能见度的预测能力有限。

Conclusion: 研究为基于证据的交通管理提供了见解，但数据集主要包含中等严重程度事故，限制了模型对极端案例的学习能力，需要替代采样策略、增强特征工程和外部数据集整合。

Abstract: This study investigates the predictive capacity of environmental, temporal, and spatial factors on traffic accident severity in the United States. Using a dataset of 500,000 U.S. traffic accidents spanning 2016-2023, we trained an XGBoost classifier optimized through randomized search cross-validation and adjusted for class imbalance via class weighting. The final model achieves an overall accuracy of 78%, with strong performance on the majority class (Severity 2), attaining 87% precision and recall. Feature importance analysis reveals that time of day, geographic location, and weather-related variables, including visibility, temperature, and wind speed, rank among the strongest predictors of accident severity. However, contrary to initial hypotheses, precipitation and visibility demonstrate limited predictive power, potentially reflecting behavioral adaptation by drivers under overtly hazardous conditions. The dataset's predominance of mid-level severity accidents constrains the model's capacity to learn meaningful patterns for extreme cases, highlighting the need for alternative sampling strategies, enhanced feature engineering, and integration of external datasets. These findings contribute to evidence-based traffic management and suggest future directions for severity prediction research.

</details>


### [66] [Online Finetuning Decision Transformers with Pure RL Gradients](https://arxiv.org/abs/2601.00167)
*Junkai Luo,Yinglun Zhu*

Main category: cs.LG

TL;DR: 论文提出新算法，使决策变换器能够使用纯强化学习梯度进行在线微调，解决了现有方法依赖监督学习目标的问题，并在多个基准测试中取得最先进性能。


<details>
  <summary>Details</summary>
Motivation: 决策变换器在离线强化学习中表现出色，但扩展到在线设置时仍主要依赖监督序列建模目标。研究发现后见回报重标注这一标准组件与基于重要性采样的强化学习算法不兼容，导致训练不稳定，因此需要开发纯强化学习梯度的在线微调方法。

Method: 将GRPO算法适配到决策变换器，并引入关键改进：子轨迹优化以改善信用分配、序列级似然目标以增强稳定性和效率、主动采样以鼓励在不确定区域的探索。

Result: 通过广泛实验证明，新方法超越了现有的在线决策变换器基线，在多个基准测试中实现了新的最先进性能。

Conclusion: 纯强化学习基础的在线微调对决策变换器是有效的，解决了后见回报重标注与重要性采样算法的不兼容问题，为决策变换器的在线应用提供了新途径。

Abstract: Decision Transformers (DTs) have emerged as a powerful framework for sequential decision making by formulating offline reinforcement learning (RL) as a sequence modeling problem. However, extending DTs to online settings with pure RL gradients remains largely unexplored, as existing approaches continue to rely heavily on supervised sequence-modeling objectives during online finetuning. We identify hindsight return relabeling -- a standard component in online DTs -- as a critical obstacle to RL-based finetuning: while beneficial for supervised learning, it is fundamentally incompatible with importance sampling-based RL algorithms such as GRPO, leading to unstable training. Building on this insight, we propose new algorithms that enable online finetuning of Decision Transformers using pure reinforcement learning gradients. We adapt GRPO to DTs and introduce several key modifications, including sub-trajectory optimization for improved credit assignment, sequence-level likelihood objectives for enhanced stability and efficiency, and active sampling to encourage exploration in uncertain regions. Through extensive experiments, we demonstrate that our methods outperform existing online DT baselines and achieve new state-of-the-art performance across multiple benchmarks, highlighting the effectiveness of pure-RL-based online finetuning for Decision Transformers.

</details>


### [67] [Sequential Reservoir Computing for Efficient High-Dimensional Spatiotemporal Forecasting](https://arxiv.org/abs/2601.00172)
*Ata Akbari Asanjan,Filip Wudarski,Daniel O'Connor,Shaun Geaney,Elena Strbac,P. Aaron Lott,Davide Venturelli*

Main category: cs.LG

TL;DR: Sequential Reservoir Computing通过将大型储层分解为多个小型互联储层，在保持传统RC简单高效的同时，显著提升了高维时空系统的预测性能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统RNN和LSTM在高维时空系统预测中面临梯度训练和内存瓶颈问题，而传统Reservoir Computing架构在处理高维输入时扩展性不佳。需要一种既能保持RC效率又能提升可扩展性的新方法。

Method: 提出Sequential Reservoir Computing架构，将大型储层分解为一系列小型、互联的储层。这种设计减少了内存和计算成本，同时保持了长期时间依赖性。使用固定循环层和凸读出优化替代反向传播。

Result: 在低维混沌系统（Lorenz63）和高维物理模拟（2D涡度和浅水方程）中，Sequential RC相比LSTM和标准RNN基线：预测有效时间延长15-25%，误差指标（SSIM、RMSE）降低20-30%，训练成本降低高达三个数量级。

Conclusion: Sequential RC在保持传统RC简单性和效率的同时，实现了对高维动力系统的卓越可扩展性，为科学和工程应用中的实时、节能预测提供了实用路径。

Abstract: Forecasting high-dimensional spatiotemporal systems remains computationally challenging for recurrent neural networks (RNNs) and long short-term memory (LSTM) models due to gradient-based training and memory bottlenecks. Reservoir Computing (RC) mitigates these challenges by replacing backpropagation with fixed recurrent layers and a convex readout optimization, yet conventional RC architectures still scale poorly with input dimensionality. We introduce a Sequential Reservoir Computing (Sequential RC) architecture that decomposes a large reservoir into a series of smaller, interconnected reservoirs. This design reduces memory and computational costs while preserving long-term temporal dependencies. Using both low-dimensional chaotic systems (Lorenz63) and high-dimensional physical simulations (2D vorticity and shallow-water equations), Sequential RC achieves 15-25% longer valid forecast horizons, 20-30% lower error metrics (SSIM, RMSE), and up to three orders of magnitude lower training cost compared to LSTM and standard RNN baselines. The results demonstrate that Sequential RC maintains the simplicity and efficiency of conventional RC while achieving superior scalability for high-dimensional dynamical systems. This approach provides a practical path toward real-time, energy-efficient forecasting in scientific and engineering applications.

</details>


### [68] [Early Prediction of Liver Cirrhosis Up to Three Years in Advance: A Machine Learning Study Benchmarking Against the FIB-4 Score](https://arxiv.org/abs/2601.00175)
*Zhuqi Miao,Sujan Ravi,Abdulaziz Ahmed*

Main category: cs.LG

TL;DR: 机器学习模型利用常规电子健康记录数据预测肝硬化，在1-3年预测时间窗口上显著优于传统FIB-4评分


<details>
  <summary>Details</summary>
Motivation: 开发基于常规电子健康记录数据的机器学习模型，用于早期预测肝硬化发生，并与传统FIB-4评分进行性能比较，以支持临床早期风险分层和预防管理

Method: 采用回顾性队列研究，使用大型学术医疗系统的去标识化电子健康记录数据。识别脂肪肝患者并根据ICD-9/10编码分为肝硬化与非肝硬化队列。构建观察窗口和预测窗口模拟真实临床使用场景。从观察窗口汇总人口统计学、诊断、实验室结果、生命体征和共病指数等数据。训练XGBoost模型用于1年、2年和3年预测时间窗口，并在保留测试集上评估性能，使用AUC与FIB-4进行比较

Result: 最终队列包括3,043名患者（1年预测）、1,981名（2年预测）和1,470名（3年预测）。在所有预测时间窗口上，机器学习模型均持续优于FIB-4。XGBoost模型在1年、2年和3年预测上的AUC分别为0.81、0.73和0.69，而FIB-4的AUC分别为0.71、0.63和0.57。随着预测时间窗口延长，性能优势仍然保持，表明早期风险辨别能力得到改善

Conclusion: 利用常规电子健康记录数据的机器学习模型在早期肝硬化预测方面显著优于传统FIB-4评分。这些模型能够实现更早、更准确的风险分层，可以作为自动化决策支持工具集成到临床工作流程中，支持主动的肝硬化预防和管理

Abstract: Objective: Develop and evaluate machine learning (ML) models for predicting incident liver cirrhosis one, two, and three years prior to diagnosis using routinely collected electronic health record (EHR) data, and to benchmark their performance against the FIB-4 score. Methods: We conducted a retrospective cohort study using de-identified EHR data from a large academic health system. Patients with fatty liver disease were identified and categorized into cirrhosis and non-cirrhosis cohorts based on ICD-9/10 codes. Prediction scenarios were constructed using observation and prediction windows to emulate real-world clinical use. Demographics, diagnoses, laboratory results, vital signs, and comorbidity indices were aggregated from the observation window. XGBoost models were trained for 1-, 2-, and 3-year prediction horizons and evaluated on held-out test sets. Model performance was compared with FIB-4 using area under the receiver operating characteristic curve (AUC). Results: Final cohorts included 3,043 patients for the 1-year prediction, 1,981 for the 2-year prediction, and 1,470 for the 3-year prediction. Across all prediction windows, ML models consistently outperformed FIB-4. The XGBoost models achieved AUCs of 0.81, 0.73, and 0.69 for 1-, 2-, and 3-year predictions, respectively, compared with 0.71, 0.63, and 0.57 for FIB-4. Performance gains persisted with longer prediction horizons, indicating improved early risk discrimination. Conclusions: Machine learning models leveraging routine EHR data substantially outperform the traditional FIB-4 score for early prediction of liver cirrhosis. These models enable earlier and more accurate risk stratification and can be integrated into clinical workflows as automated decision-support tools to support proactive cirrhosis prevention and management.

</details>


### [69] [Reinforcement-Learned Unequal Error Protection for Quantized Semantic Embeddings](https://arxiv.org/abs/2601.00186)
*Moirangthem Tiken Singh,Adnan Arif*

Main category: cs.LG

TL;DR: 提出基于强化学习的自适应重复编码框架，实现按维度不等错误保护，在有限带宽下显著提升语义通信性能


<details>
  <summary>Details</summary>
Motivation: 解决带宽受限通信系统中语义意义保持的挑战，传统信道编码（如LDPC、Reed-Solomon）无法实现细粒度语义保护

Method: 采用强化学习框架，通过自适应重复编码实现按维度不等错误保护，使用复合语义失真度量平衡全局嵌入相似性和实体级保持

Result: 相比均匀保护，在1 dB SNR下获得6.8%更高的chrF分数和9.3%更好的实体保持，统计显著

Conclusion: 智能分配的简单重复编码可实现细粒度语义保护，代码结构必须与语义粒度对齐，为边缘计算和IoT场景提供实用解决方案

Abstract: This paper tackles the pressing challenge of preserving semantic meaning in communication systems constrained by limited bandwidth. We introduce a novel reinforcement learning framework that achieves per-dimension unequal error protection via adaptive repetition coding. Central to our approach is a composite semantic distortion metric that balances global embedding similarity with entity-level preservation, empowering the reinforcement learning agent to allocate protection in a context-aware manner. Experiments show statistically significant gains over uniform protection, achieving 6.8% higher chrF scores and 9.3% better entity preservation at 1 dB SNR. The key innovation of our framework is the demonstration that simple, intelligently allocated repetition coding enables fine-grained semantic protection -- an advantage unattainable with conventional codes such as LDPC or Reed-Solomon. Our findings challenge traditional channel coding paradigms by establishing that code structure must align with semantic granularity. This approach is particularly suited to edge computing and IoT scenarios, where bandwidth is scarce, but semantic fidelity is critical, providing a practical pathway for next-generation semantic-aware networks.

</details>


### [70] [SSI-GAN: Semi-Supervised Swin-Inspired Generative Adversarial Networks for Neuronal Spike Classification](https://arxiv.org/abs/2601.00189)
*Danial Sharifrazi,Nouman Javed,Mojtaba Mohammadi,Seyede Sana Salehi,Roohallah Alizadehsani,Prasad N. Paradkar,U. Rajendra Acharya,Asim Bhatti*

Main category: cs.LG

TL;DR: 提出SSI-GAN架构，使用仅1-3%标注数据实现蚊子神经元尖峰信号分类，准确率达99.93%，大幅减少人工标注工作量


<details>
  <summary>Details</summary>
Motivation: 蚊子是虫媒病毒主要传播媒介，手动分类神经元尖峰模式劳动密集且昂贵。现有深度学习方案需要完全标注数据集和高度预处理信号，限制了实际场景的大规模应用。为解决标注数据稀缺问题，需要开发半监督方法。

Method: 提出半监督Swin启发式GAN（SSI-GAN），包含基于Transformer的生成器和Swin启发的移位窗口判别器。使用多头自注意力模型在平面窗口式Transformer判别器中学习稀疏高频尖峰特征。仅使用1-3%标注数据，训练超过1500万个尖峰样本，通过贝叶斯Optuna框架优化超参数，五折蒙特卡洛交叉验证验证鲁棒性。

Result: SSI-GAN在感染后第三天使用仅3%标注数据达到99.93%分类准确率。在仅1%监督下，在所有感染阶段保持高准确率。相比标准监督方法，在相同性能水平下减少97-99%人工标注工作量。移位窗口Transformer设计大幅超越所有基线，在基于尖峰的神经元感染分类中创下新纪录。

Conclusion: SSI-GAN架构有效解决了蚊子神经元尖峰信号分类中标注数据稀缺的问题，显著减少了人工标注工作量，在实际现场场景中具有高度可行性，为虫媒病毒神经趋向性检测提供了高效解决方案。

Abstract: Mosquitos are the main transmissive agents of arboviral diseases. Manual classification of their neuronal spike patterns is very labor-intensive and expensive. Most available deep learning solutions require fully labeled spike datasets and highly preprocessed neuronal signals. This reduces the feasibility of mass adoption in actual field scenarios. To address the scarcity of labeled data problems, we propose a new Generative Adversarial Network (GAN) architecture that we call the Semi-supervised Swin-Inspired GAN (SSI-GAN). The Swin-inspired, shifted-window discriminator, together with a transformer-based generator, is used to classify neuronal spike trains and, consequently, detect viral neurotropism. We use a multi-head self-attention model in a flat, window-based transformer discriminator that learns to capture sparser high-frequency spike features. Using just 1 to 3% labeled data, SSI-GAN was trained with more than 15 million spike samples collected at five-time post-infection and recording classification into Zika-infected, dengue-infected, or uninfected categories. Hyperparameters were optimized using the Bayesian Optuna framework, and performance for robustness was validated under fivefold Monte Carlo cross-validation. SSI-GAN reached 99.93% classification accuracy on the third day post-infection with only 3% labeled data. It maintained high accuracy across all stages of infection with just 1% supervision. This shows a 97-99% reduction in manual labeling effort relative to standard supervised approaches at the same performance level. The shifted-window transformer design proposed here beat all baselines by a wide margin and set new best marks in spike-based neuronal infection classification.

</details>


### [71] [Optimized Hybrid Feature Engineering for Resource-Efficient Arrhythmia Detection in ECG Signals: An Optimization Framework](https://arxiv.org/abs/2601.00192)
*Moirangthem Tiken Singh,Manibhushan Yaikhom*

Main category: cs.LG

TL;DR: 提出资源高效的数据中心框架，通过特征工程使心律失常数据线性可分，实现98.44%准确率，模型仅8.54KB，推理延迟0.46μs


<details>
  <summary>Details</summary>
Motivation: 心血管疾病特别是心律失常是全球主要死因，需要IoMT持续监测。现有深度学习方法计算开销大，不适合资源受限的边缘设备。

Method: 提出资源高效的数据中心框架，优先特征工程而非模型复杂度。整合时频小波分解和图论结构描述符（如PageRank中心性），构建混合特征空间，使用互信息和递归消除进行特征选择，采用可解释的超轻量线性分类器。

Result: 在MIT-BIH和INCART数据集上验证，达到98.44%诊断准确率，模型大小仅8.54KB，分类推理延迟0.46μs，每搏处理管道52ms，相比压缩模型KD-Light（25KB，96.32%准确率）实现数量级效率提升。

Conclusion: 该框架通过特征工程使复杂心律失常数据线性可分，实现实时操作，为无电池心脏传感器提供高效解决方案，相比现有方法有显著效率优势。

Abstract: Cardiovascular diseases, particularly arrhythmias, remain a leading global cause of mortality, necessitating continuous monitoring via the Internet of Medical Things (IoMT). However, state-of-the-art deep learning approaches often impose prohibitive computational overheads, rendering them unsuitable for resource-constrained edge devices. This study proposes a resource-efficient, data-centric framework that prioritizes feature engineering over complexity. Our optimized pipeline makes the complex, high-dimensional arrhythmia data linearly separable. This is achieved by integrating time-frequency wavelet decompositions with graph-theoretic structural descriptors, such as PageRank centrality. This hybrid feature space, combining wavelet decompositions and graph-theoretic descriptors, is then refined using mutual information and recursive elimination, enabling interpretable, ultra-lightweight linear classifiers. Validation on the MIT-BIH and INCART datasets yields 98.44% diagnostic accuracy with an 8.54 KB model footprint. The system achieves 0.46 $μ$s classification inference latency within a 52 ms per-beat pipeline, ensuring real-time operation. These outcomes provide an order-of-magnitude efficiency gain over compressed models, such as KD-Light (25 KB, 96.32% accuracy), advancing battery-less cardiac sensors.

</details>


### [72] [Unknown Aware AI-Generated Content Attribution](https://arxiv.org/abs/2601.00218)
*Ellie Thieu,Jifan Zhang,Haoyue Bai*

Main category: cs.LG

TL;DR: 该论文研究了生成模型溯源问题，提出利用未标注的互联网数据提升对未知生成器的识别能力，通过约束优化方法显著改善了在开放世界场景下的AI生成内容溯源性能。


<details>
  <summary>Details</summary>
Motivation: 随着逼真生成模型的快速发展，需要超越简单的真假检测，实现特定生成模型的溯源识别。现有方法在已知生成器上表现良好，但难以泛化到未见过的和新发布的生成器。

Method: 首先使用CLIP特征和线性分类器建立基线，然后提出约束优化方法，利用未标注的互联网数据（可能包含真实图像、未知生成器输出或目标模型样本），鼓励野生样本被分类为非目标，同时约束在标注数据上的性能保持高水平。

Result: 实验结果表明，结合野生数据能显著提升在具有挑战性的未见生成器上的溯源性能，证明未标注的互联网数据可以有效增强开放世界场景下的AI生成内容溯源能力。

Conclusion: 利用未标注的野生数据是提升生成模型溯源泛化能力的有效策略，特别是在面对未知和新发布生成器的开放世界场景中，约束优化方法能够有效利用这些数据改善模型性能。

Abstract: The rapid advancement of photorealistic generative models has made it increasingly important to attribute the origin of synthetic content, moving beyond binary real or fake detection toward identifying the specific model that produced a given image. We study the problem of distinguishing outputs from a target generative model (e.g., OpenAI Dalle 3) from other sources, including real images and images generated by a wide range of alternative models. Using CLIP features and a simple linear classifier, shown to be effective in prior work, we establish a strong baseline for target generator attribution using only limited labeled data from the target model and a small number of known generators. However, this baseline struggles to generalize to harder, unseen, and newly released generators. To address this limitation, we propose a constrained optimization approach that leverages unlabeled wild data, consisting of images collected from the Internet that may include real images, outputs from unknown generators, or even samples from the target model itself. The proposed method encourages wild samples to be classified as non target while explicitly constraining performance on labeled data to remain high. Experimental results show that incorporating wild data substantially improves attribution performance on challenging unseen generators, demonstrating that unlabeled data from the wild can be effectively exploited to enhance AI generated content attribution in open world settings.

</details>


### [73] [Robust Graph Fine-Tuning with Adversarial Graph Prompting](https://arxiv.org/abs/2601.00229)
*Ziyan Zhang,Bo Jiang,Jin Tang*

Main category: cs.LG

TL;DR: 该论文提出了一种新颖的对抗性图提示（AGP）框架，将对抗学习集成到图提示中，以增强预训练GNN模型在下游任务中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效微调（PEFT）方法在面对图拓扑和节点特征的各种噪声和攻击时表现出显著的脆弱性，需要开发更鲁棒的图微调方法。

Method: 提出了对抗性图提示（AGP）框架，将其表述为min-max优化问题，采用交替优化方案：内层最大化使用联合投影梯度下降（JointPGD）生成强对抗噪声，外层最小化学习最优节点提示来对抗噪声。

Result: 理论分析表明AGP能够处理图拓扑和节点噪声，实验验证了AGP在多个基准任务上相比现有方法具有更好的鲁棒性和有效性。

Conclusion: AGP是一种通用方法，可与各种预训练GNN模型集成，增强其在下游任务中的鲁棒性，为解决图数据中的噪声和攻击问题提供了有效方案。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) method has emerged as a dominant paradigm for adapting pre-trained GNN models to downstream tasks. However, existing PEFT methods usually exhibit significant vulnerability to various noise and attacks on graph topology and node attributes/features. To address this issue, for the first time, we propose integrating adversarial learning into graph prompting and develop a novel Adversarial Graph Prompting (AGP) framework to achieve robust graph fine-tuning. Our AGP has two key aspects. First, we propose the general problem formulation of AGP as a min-max optimization problem and develop an alternating optimization scheme to solve it. For inner maximization, we propose Joint Projected Gradient Descent (JointPGD) algorithm to generate strong adversarial noise. For outer minimization, we employ a simple yet effective module to learn the optimal node prompts to counteract the adversarial noise. Second, we demonstrate that the proposed AGP can theoretically address both graph topology and node noise. This confirms the versatility and robustness of our AGP fine-tuning method across various graph noise. Note that, the proposed AGP is a general method that can be integrated with various pre-trained GNN models to enhance their robustness on the downstream tasks. Extensive experiments on multiple benchmark tasks validate the robustness and effectiveness of AGP method compared to state-of-the-art methods.

</details>


### [74] [Can Optimal Transport Improve Federated Inverse Reinforcement Learning?](https://arxiv.org/abs/2601.00309)
*David Millard,Ali Baheri*

Main category: cs.LG

TL;DR: 提出基于最优传输的联邦逆强化学习方法，通过Wasserstein重心融合异构智能体的本地奖励函数，获得比传统参数平均更准确的全局奖励估计。


<details>
  <summary>Details</summary>
Motivation: 在机器人和多智能体系统中，自主智能体群通常在略有不同的环境中运行，但追求共同的高级目标。由于动态差异、隐私约束和有限通信带宽，直接汇集数据学习共享奖励函数通常不切实际。

Method: 每个客户端首先在本地执行轻量级最大熵逆强化学习，遵守其计算和隐私限制。然后通过Wasserstein重心融合得到的奖励函数，考虑其底层几何结构。

Result: 证明这种重心融合比联邦学习中传统的参数平均方法产生更准确的全局奖励估计。提供了一种原则性且通信高效的框架。

Conclusion: 该工作为推导跨异构智能体和环境泛化的共享奖励提供了原则性和通信高效的框架。

Abstract: In robotics and multi-agent systems, fleets of autonomous agents often operate in subtly different environments while pursuing a common high-level objective. Directly pooling their data to learn a shared reward function is typically impractical due to differences in dynamics, privacy constraints, and limited communication bandwidth. This paper introduces an optimal transport-based approach to federated inverse reinforcement learning (IRL). Each client first performs lightweight Maximum Entropy IRL locally, adhering to its computational and privacy limitations. The resulting reward functions are then fused via a Wasserstein barycenter, which considers their underlying geometric structure. We further prove that this barycentric fusion yields a more faithful global reward estimate than conventional parameter averaging methods in federated learning. Overall, this work provides a principled and communication-efficient framework for deriving a shared reward that generalizes across heterogeneous agents and environments.

</details>


### [75] [Quantum King-Ring Domination in Chess: A QAOA Approach](https://arxiv.org/abs/2601.00318)
*Gerhard Stenzel,Michael Kölle,Tobias Rohe,Julian Hager,Leo Sünkel,Maximilian Zorn,Claudia Linnhoff-Popien*

Main category: cs.LG

TL;DR: 该研究提出了基于国际象棋战术的量子近似优化算法（QAOA）新基准QKRD，包含5000个结构化实例，通过系统评估发现约束保持混合器、热启动策略等QAOA设计选择在结构化问题上表现显著优于随机实例测试结果。


<details>
  <summary>Details</summary>
Motivation: 现有QAOA基准测试主要使用MaxCut、TSP、SAT等随机合成实例，这些实例缺乏语义结构和人类可解释性，无法反映真实世界问题的约束特性，限制了算法在实际应用中的性能评估。

Method: 提出了量子王环支配（QKRD）基准，基于国际象棋战术位置构建，包含5000个结构化实例，具有独热约束、空间局部性和10-40量子比特规模。该基准结合人类可解释的覆盖度指标和针对经典启发式算法的内在验证。

Result: 约束保持混合器（XY、域壁）比标准混合器收敛快约13步；热启动策略减少45步收敛时间；CVaR优化产生负面结果。QAOA表现优于贪婪启发式算法12.6%，优于随机选择80.1%。

Conclusion: 结构化基准能够揭示问题感知的QAOA技术在随机实例中被掩盖的优势，为NISQ算法研究提供了可重复的评估框架。

Abstract: The Quantum Approximate Optimization Algorithm (QAOA) is extensively benchmarked on synthetic random instances such as MaxCut, TSP, and SAT problems, but these lack semantic structure and human interpretability, offering limited insight into performance on real-world problems with meaningful constraints. We introduce Quantum King-Ring Domination (QKRD), a NISQ-scale benchmark derived from chess tactical positions that provides 5,000 structured instances with one-hot constraints, spatial locality, and 10--40 qubit scale. The benchmark pairs human-interpretable coverage metrics with intrinsic validation against classical heuristics, enabling algorithmic conclusions without external oracles. Using QKRD, we systematically evaluate QAOA design choices and find that constraint-preserving mixers (XY, domain-wall) converge approximately 13 steps faster than standard mixers (p<10^{-7}, d\approx0.5) while eliminating penalty tuning, warm-start strategies reduce convergence by 45 steps (p<10^{-127}, d=3.35) with energy improvements exceeding d=8, and Conditional Value-at-Risk (CVaR) optimization yields an informative negative result with worse energy (p<10^{-40}, d=1.21) and no coverage benefit. Intrinsic validation shows QAOA outperforms greedy heuristics by 12.6\% and random selection by 80.1\%. Our results demonstrate that structured benchmarks reveal advantages of problem-informed QAOA techniques obscured in random instances. We release all code, data, and experimental artifacts for reproducible NISQ algorithm research.

</details>


### [76] [E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models](https://arxiv.org/abs/2601.00423)
*Shengjun Zhang,Zhang Zhang,Chensheng Dai,Yueqi Duan*

Main category: cs.LG

TL;DR: 提出E-GRPO方法，通过熵感知的组相对策略优化来增强流匹配模型的人类偏好对齐，通过合并低熵步骤为高熵SDE采样步骤，解决多步去噪中的稀疏奖励信号问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在优化多步去噪时面临稀疏和模糊的奖励信号问题，观察到高熵步骤能实现更高效有效的探索，而低熵步骤则导致无差异的roll-out。

Method: 提出E-GRPO（熵感知组相对策略优化），合并连续低熵步骤形成单个高熵SDE采样步骤，对其他步骤应用ODE采样，并引入多步组归一化优势函数。

Result: 在不同奖励设置下的实验结果表明，该方法在增强流匹配模型的人类偏好对齐方面具有有效性。

Conclusion: 通过熵感知的步骤合并和组相对优势计算，E-GRPO能够更有效地处理SDE采样中的奖励信号模糊问题，提升人类偏好对齐性能。

Abstract: Recent reinforcement learning has enhanced the flow matching models on human preference alignment. While stochastic sampling enables the exploration of denoising directions, existing methods which optimize over multiple denoising steps suffer from sparse and ambiguous reward signals. We observe that the high entropy steps enable more efficient and effective exploration while the low entropy steps result in undistinguished roll-outs. To this end, we propose E-GRPO, an entropy aware Group Relative Policy Optimization to increase the entropy of SDE sampling steps. Since the integration of stochastic differential equations suffer from ambiguous reward signals due to stochasticity from multiple steps, we specifically merge consecutive low entropy steps to formulate one high entropy step for SDE sampling, while applying ODE sampling on other steps. Building upon this, we introduce multi-step group normalized advantage, which computes group-relative advantages within samples sharing the same consolidated SDE denoising step. Experimental results on different reward settings have demonstrated the effectiveness of our methods.

</details>


### [77] [A Comparative Study of Adaptation Strategies for Time Series Foundation Models in Anomaly Detection](https://arxiv.org/abs/2601.00446)
*Miseon Park,Kijung Yoon*

Main category: cs.LG

TL;DR: 时间序列基础模型（TSFMs）作为异常检测通用骨干的有效性研究，通过零样本推理、全模型适应和参数高效微调策略，在多个基准测试中验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列异常检测方法大多需要大量任务特定训练，本研究探索预训练在大型异构数据上的时间序列基础模型是否能作为异常检测的通用骨干模型，实现更高效和可扩展的检测方案。

Method: 通过系统实验比较三种策略：零样本推理、全模型适应和参数高效微调（包括LoRA、OFT、HRA等方法），在多个基准测试上评估时间序列基础模型在异常检测任务中的表现。

Result: 时间序列基础模型在异常检测任务中优于任务特定基线方法，在AUC-PR和VUS-PR指标上取得显著提升，尤其在类别严重不平衡情况下表现更好。参数高效微调方法不仅降低计算成本，在大多数情况下匹配甚至超越全微调性能。

Conclusion: 时间序列基础模型可作为异常检测的有前景的通用模型，即使预训练用于预测任务，也能通过高效适应实现优异的异常检测性能，为可扩展和高效的时间序列异常检测提供了新方向。

Abstract: Time series anomaly detection is essential for the reliable operation of complex systems, but most existing methods require extensive task-specific training. We explore whether time series foundation models (TSFMs), pretrained on large heterogeneous data, can serve as universal backbones for anomaly detection. Through systematic experiments across multiple benchmarks, we compare zero-shot inference, full model adaptation, and parameter-efficient fine-tuning (PEFT) strategies. Our results demonstrate that TSFMs outperform task-specific baselines, achieving notable gains in AUC-PR and VUS-PR, particularly under severe class imbalance. Moreover, PEFT methods such as LoRA, OFT, and HRA not only reduce computational cost but also match or surpass full fine-tuning in most cases, indicating that TSFMs can be efficiently adapted for anomaly detection, even when pretrained for forecasting. These findings position TSFMs as promising general-purpose models for scalable and efficient time series anomaly detection.

</details>


### [78] [Controllable Concept Bottleneck Models](https://arxiv.org/abs/2601.00451)
*Hongbin Lin,Chenyang Ren,Juangui Xu,Zhengyu Hu,Cheng-Long Wang,Yao Shu,Hui Xiong,Jingfeng Zhang,Di Wang,Lijie Hu*

Main category: cs.LG

TL;DR: 提出可控概念瓶颈模型（CCBMs），支持概念-标签级、概念级和数据级三种粒度的模型编辑，无需重新训练即可实现动态维护。


<details>
  <summary>Details</summary>
Motivation: 现有概念瓶颈模型主要针对静态场景，但实际应用中需要动态维护：删除错误或敏感数据（遗忘）、纠正错误标注的概念、纳入新样本（增量学习）以适应环境变化。需要高效可编辑的CBMs而无需从头训练。

Method: 提出可控概念瓶颈模型（CCBMs），基于影响函数推导出数学上严格的闭式近似解，支持三种粒度编辑：概念-标签级、概念级、数据级（包括数据删除和添加）。

Result: 实验结果表明CCBMs具有高效性和适应性，证实了其在实现动态可信CBMs方面的实用价值。

Conclusion: CCBMs通过数学严谨的闭式近似解决了CBMs的动态维护问题，支持多粒度编辑而无需重新训练，为大规模应用中的可编辑CBMs提供了实用解决方案。

Abstract: Concept Bottleneck Models (CBMs) have garnered much attention for their ability to elucidate the prediction process through a human-understandable concept layer. However, most previous studies focused on static scenarios where the data and concepts are assumed to be fixed and clean. In real-world applications, deployed models require continuous maintenance: we often need to remove erroneous or sensitive data (unlearning), correct mislabeled concepts, or incorporate newly acquired samples (incremental learning) to adapt to evolving environments. Thus, deriving efficient editable CBMs without retraining from scratch remains a significant challenge, particularly in large-scale applications. To address these challenges, we propose Controllable Concept Bottleneck Models (CCBMs). Specifically, CCBMs support three granularities of model editing: concept-label-level, concept-level, and data-level, the latter of which encompasses both data removal and data addition. CCBMs enjoy mathematically rigorous closed-form approximations derived from influence functions that obviate the need for retraining. Experimental results demonstrate the efficiency and adaptability of our CCBMs, affirming their practical value in enabling dynamic and trustworthy CBMs.

</details>


### [79] [Geometric Regularization in Mixture-of-Experts: The Disconnect Between Weights and Activations](https://arxiv.org/abs/2601.00457)
*Hyunjun Kim*

Main category: cs.LG

TL;DR: 正交性损失在MoE模型中无法有效促进专家多样性，既不能减少权重空间重叠，也不能可靠提升性能


<details>
  <summary>Details</summary>
Motivation: 研究几何正则化在MoE模型专家专业化中的作用，探索正交性损失是否能有效促进专家多样性

Method: 在MoE模型中应用正交性损失，通过7种正则化强度进行实验，分析权重空间重叠和激活空间重叠的变化

Result: 正交性损失在多方面失败：权重空间重叠反而增加114%，激活空间重叠保持高位(~0.6)，性能影响不一致，权重与激活正交性无显著相关性(r=-0.293,p=0.523)

Conclusion: 权重空间正则化既不能实现其几何目标，也不能可靠提升性能，不适合用于MoE多样性

Abstract: Mixture-of-Experts (MoE) models achieve efficiency through sparse activation, but the role of geometric regularization in expert specialization remains unclear. We apply orthogonality loss to enforce expert diversity and find it fails on multiple fronts: it does not reduce weight-space overlap (MSO actually increases by up to 114%), activation-space overlap remains high (~0.6) regardless of regularization, and effects on performance are inconsistent -- marginal improvement on WikiText-103 (-0.9%), slight degradation on TinyStories (+0.9%), and highly variable results on PTB (std > 1.0). Our analysis across 7 regularization strengths reveals no significant correlation (r = -0.293, p = 0.523) between weight and activation orthogonality. These findings demonstrate that weight-space regularization neither achieves its geometric goal nor reliably improves performance, making it unsuitable for MoE diversity.

</details>


### [80] [Detecting Spike Wave Discharges (SWD) using 1-dimensional Residual UNet](https://arxiv.org/abs/2601.00459)
*Saurav Sengupta,Scott Kilianski,Suchetha Sharma,Sakina Lashkeri,Ashley McHugh,Mark Beenhakker,Donald E. Brown*

Main category: cs.LG

TL;DR: 该研究开发了一种基于1D UNet的数据增强方法（AugUNet1D），用于自动标记脑电图中的棘慢波放电（SWD），相比传统方法和现有算法（Twin Peaks）表现更优。


<details>
  <summary>Details</summary>
Motivation: 脑电图（EEG）记录中事件的手动标记非常耗时，特别是连续数周至数月的记录。棘慢波放电（SWD）作为失神发作的电生理标志，通常需要手动标记。虽然已有研究使用机器学习自动分割和分类EEG信号，但仍有改进空间。

Method: 研究比较了14种机器学习分类器在961小时C3H/HeJ小鼠EEG记录（包含22,637个标记SWD）上的性能。发现1D UNet表现最佳，并通过数据增强进一步改进，其中缩放增强效果最显著。最终开发了AugUNet1D，并与时间-频率算法"Twin Peaks"进行比较。

Result: AugUNet1D在所有方法中表现最优，检测到的事件特征更接近手动标记的SWD。相比"Twin Peaks"算法，AugUNet1D显示出更优越的性能。

Conclusion: AugUNet1D是一种有效的自动标记EEG中SWD的方法，优于现有算法。研究公开了预训练和未训练的AugUNet1D模型供其他用户使用，有助于减少EEG分析中的手动工作量。

Abstract: The manual labeling of events in electroencephalography (EEG) records is time-consuming. This is especially true when EEG recordings are taken continuously over weeks to months. Therefore, a method to automatically label pertinent EEG events reduces the manual workload. Spike wave discharges (SWD), which are the electrographic hallmark of absence seizures, are EEG events that are often labeled manually. While some previous studies have utilized machine learning to automatically segment and classify EEG signals like SWDs, they can be improved. Here we compare the performance of 14 machine learning classifiers on our own manually annotated dataset of 961 hours of EEG recordings from C3H/HeJ mice, including 22,637 labeled SWDs. We find that a 1D UNet performs best for labeling SWDs in this dataset. We also improve the 1D UNet by augmenting our training data and determine that scaling showed the greatest benefit of all augmentation procedures applied. We then compare the 1D UNet with data augmentation, AugUNet1D, against a recently published time- and frequency-based algorithmic approach called "Twin Peaks". AugUNet1D showed superior performance and detected events with more similar features to the SWDs labeled manually. AugUNet1D, pretrained on our manually annotated data or untrained, is made public for others users.

</details>


### [81] [Laplacian Kernelized Bandit](https://arxiv.org/abs/2601.00461)
*Shuang Wu,Arash A. Amini*

Main category: cs.LG

TL;DR: 该论文提出了一种多用户上下文赌博机框架，通过图拉普拉斯正则化与核方法的结合，在非线性和图同质性奖励函数下实现结构化探索。


<details>
  <summary>Details</summary>
Motivation: 研究多用户上下文赌博机问题，其中用户通过图结构相互关联，且奖励函数既表现出非线性特性又具有图同质性。现有方法在处理这种复杂结构时存在局限，需要一种能够同时捕捉图结构和非线性关系的统一框架。

Method: 提出一种联合惩罚项，结合基于RKHS距离的图平滑项和个体粗糙度惩罚。证明该惩罚等价于单一多用户RKHS中的平方范数，并显式推导其再生核，将图拉普拉斯与基础臂核优雅融合。基于此设计两种算法：LK-GP-UCB和LK-GP-TS，利用高斯过程后验进行探索。

Result: 理论分析提供了高概率遗憾界，其缩放依赖于多用户核的有效维度，而非用户数量或环境维度。实验表明，在非线性设置中，该方法优于强线性基线和无图感知基线；即使在真实奖励为线性的情况下，仍保持竞争力。

Conclusion: 该工作提供了一个统一、理论严谨且实用的框架，将拉普拉斯正则化与核化赌博机相结合，实现了结构化探索，为处理具有图结构关系的多用户上下文赌博机问题提供了有效解决方案。

Abstract: We study multi-user contextual bandits where users are related by a graph and their reward functions exhibit both non-linear behavior and graph homophily. We introduce a principled joint penalty for the collection of user reward functions $\{f_u\}$, combining a graph smoothness term based on RKHS distances with an individual roughness penalty. Our central contribution is proving that this penalty is equivalent to the squared norm within a single, unified \emph{multi-user RKHS}. We explicitly derive its reproducing kernel, which elegantly fuses the graph Laplacian with the base arm kernel. This unification allows us to reframe the problem as learning a single ''lifted'' function, enabling the design of principled algorithms, \texttt{LK-GP-UCB} and \texttt{LK-GP-TS}, that leverage Gaussian Process posteriors over this new kernel for exploration. We provide high-probability regret bounds that scale with an \emph{effective dimension} of the multi-user kernel, replacing dependencies on user count or ambient dimension. Empirically, our methods outperform strong linear and non-graph-aware baselines in non-linear settings and remain competitive even when the true rewards are linear. Our work delivers a unified, theoretically grounded, and practical framework that bridges Laplacian regularization with kernelized bandits for structured exploration.

</details>


### [82] [Trajectory Guard -- A Lightweight, Sequence-Aware Model for Real-Time Anomaly Detection in Agentic AI](https://arxiv.org/abs/2601.00516)
*Laksh Advani*

Main category: cs.LG

TL;DR: Trajectory Guard：一种用于检测LLM智能体多步行动计划异常的Siamese循环自编码器，通过对比学习和重构的混合损失函数，能同时检测任务轨迹对齐和序列结构有效性，在性能和速度上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自主LLM智能体生成的多步行动计划可能因上下文不对齐或结构不连贯而失败。现有异常检测方法不适用于此挑战：均值池化嵌入会稀释异常步骤，而仅对比方法忽略序列结构。基于预训练嵌入的标准无监督方法F1分数不超过0.69。

Method: 提出Trajectory Guard，一种Siamese循环自编码器，采用混合损失函数，通过对比学习联合学习任务-轨迹对齐，通过重构学习序列有效性。这种双重目标能够统一检测"错误的任务计划"和"畸形计划结构"。

Result: 在涵盖合成扰动和真实世界失败的基准测试中（RAS-Eval安全审计和多智能体系统Who&When），在平衡集上达到0.88-0.94的F1分数，在不平衡外部基准上达到0.86-0.92的召回率。推理延迟32毫秒，比LLM Judge基线快17-27倍。

Conclusion: Trajectory Guard能够实时检测LLM智能体行动计划的异常，支持生产部署中的实时安全验证，在检测准确性和推理速度方面均显著优于现有方法。

Abstract: Autonomous LLM agents generate multi-step action plans that can fail due to contextual misalignment or structural incoherence. Existing anomaly detection methods are ill-suited for this challenge: mean-pooling embeddings dilutes anomalous steps, while contrastive-only approaches ignore sequential structure. Standard unsupervised methods on pre-trained embeddings achieve F1-scores no higher than 0.69. We introduce Trajectory Guard, a Siamese Recurrent Autoencoder with a hybrid loss function that jointly learns task-trajectory alignment via contrastive learning and sequential validity via reconstruction. This dual objective enables unified detection of both "wrong plan for this task" and "malformed plan structure." On benchmarks spanning synthetic perturbations and real-world failures from security audits (RAS-Eval) and multi-agent systems (Who\&When), we achieve F1-scores of 0.88-0.94 on balanced sets and recall of 0.86-0.92 on imbalanced external benchmarks. At 32 ms inference latency, our approach runs 17-27$\times$ faster than LLM Judge baselines, enabling real-time safety verification in production deployments.

</details>


### [83] [Federated Customization of Large Models: Approaches, Experiments, and Insights](https://arxiv.org/abs/2601.00526)
*Yuchuan Ye,Ming Ding,Youjia Chen,Peng Cheng,Dusit Niyato*

Main category: cs.LG

TL;DR: 本文探讨了大型模型在联邦学习框架下的定制化方法，首次尝试将prefix-tuning应用于联邦学习环境，验证了其可行性并展示了竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 探索在联邦学习框架下定制大型模型的关键挑战，研究如何将各种大模型定制技术应用于分布式隐私保护环境中。

Method: 回顾了多种大模型定制技术（全微调、高效微调、提示工程、前缀调优、知识蒸馏、检索增强生成），讨论了它们在联邦学习框架下的实现方式，并首次在联邦学习环境中实验了前缀调优方法。

Result: 联邦前缀调优实验验证了其可行性，性能接近集中式方法；与其他三种联邦定制方法相比，表现出竞争性性能、令人满意的效率和一致的鲁棒性。

Conclusion: 联邦前缀调优是大型模型在联邦学习环境下定制化的有效方法，为隐私保护的大模型个性化应用提供了有前景的技术路径。

Abstract: In this article, we explore federated customization of large models and highlight the key challenges it poses within the federated learning framework. We review several popular large model customization techniques, including full fine-tuning, efficient fine-tuning, prompt engineering, prefix-tuning, knowledge distillation, and retrieval-augmented generation. Then, we discuss how these techniques can be implemented within the federated learning framework. Moreover, we conduct experiments on federated prefix-tuning, which, to the best of our knowledge, is the first trial to apply prefix-tuning in the federated learning setting. The conducted experiments validate its feasibility with performance close to centralized approaches. Further comparison with three other federated customization methods demonstrated its competitive performance, satisfactory efficiency, and consistent robustness.

</details>


### [84] [HFedMoE: Resource-aware Heterogeneous Federated Learning with Mixture-of-Experts](https://arxiv.org/abs/2601.00583)
*Zihan Fang,Zheng Lin,Senkang Hu,Yanan Ma,Yihang Tao,Yiqin Deng,Xianhao Chen,Yuguang Fang*

Main category: cs.LG

TL;DR: HFedMoE是一个针对资源受限客户端的异构MoE联邦学习框架，通过专家重要性评估、自适应专家选择和稀疏感知聚合，实现计算高效的LLM微调。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然能保护数据隐私，但大型语言模型在资源受限设备上训练不现实。MoE模型通过激活稀疏专家子集来降低计算负担，但在FL环境中面临三个关键挑战：1)缺乏可靠指标衡量专家对本地微调的影响；2)异构计算资源限制MoE专家激活；3)客户端特定专家子集和路由偏好破坏全局聚合。

Method: HFedMoE框架包含三个核心组件：1)基于专家对微调性能贡献的重要性评估；2)从信息瓶颈角度自适应选择专家子集以匹配客户端计算预算；3)稀疏感知模型聚合策略，通过重要性加权贡献聚合活跃微调的专家和门控参数。

Result: 大量实验表明，HFedMoE在训练准确率和收敛速度方面优于最先进的基准方法。

Conclusion: HFedMoE成功解决了MoE在联邦学习环境中的关键挑战，为资源受限客户端提供了计算高效的大型语言模型微调解决方案，在保持隐私的同时实现了良好的性能。

Abstract: While federated learning (FL) enables fine-tuning of large language models (LLMs) without compromising data privacy, the substantial size of an LLM renders on-device training impractical for resource-constrained clients, such as mobile devices. Thus, Mixture-of-Experts (MoE) models have emerged as a computation-efficient solution, which activates only a sparse subset of experts during model training to reduce computing burden without sacrificing performance. Though integrating MoE into FL fine-tuning holds significant potential, it still encounters three key challenges: i) selecting appropriate experts for clients remains challenging due to the lack of a reliable metric to measure each expert's impact on local fine-tuning performance, ii) the heterogeneous computing resources across clients severely hinder MoE-based LLM fine-tuning, as dynamic expert activations across diverse input samples can overwhelm resource-constrained devices, and iii) client-specific expert subsets and routing preference undermine global aggregation, where misaligned expert updates and inconsistent gating networks in troduce destructive interference. To address these challenges, we propose HFedMoE, a heterogeneous MoE-based FL fine-tuning framework that customizes a subset of experts to each client for computation-efficient LLM fine-tuning. Specifically, HFedMoE identifies the expert importance based on its contributions to fine-tuning performance, and then adaptively selects a subset of experts from an information bottleneck perspective to align with each client' s computing budget. A sparsity-aware model aggregation strategy is also designed to aggregate the actively fine-tuned experts and gating parameters with importance weighted contributions. Extensive experiments demonstrate that HFedMoE outperforms state-of-the-art benchmarks in training accuracy and convergence speed.

</details>


### [85] [Cloud-Native Generative AI for Automated Planogram Synthesis: A Diffusion Model Approach for Multi-Store Retail Optimization](https://arxiv.org/abs/2601.00527)
*Ravi Teja Pagidoju,Shriya Agarwal*

Main category: cs.LG

TL;DR: 本文提出基于扩散模型的云原生架构，用于自动生成店铺专属的货架陈列图，将设计时间从30小时减少到0.5小时，成本降低97.5%


<details>
  <summary>Details</summary>
Motivation: 传统货架陈列图设计耗时耗力，平均每个复杂布局需要30小时，需要自动化解决方案来提升零售空间优化效率

Method: 采用云原生架构，结合AWS云端模型训练和边缘部署实时推理，使用扩散模型学习多个零售点的成功货架布局，通过改进的损失函数集成零售特定约束

Result: 系统将货架陈列图设计时间减少98.3%（从30小时到0.5小时），约束满足率达到94.4%，成本降低97.5%，盈亏平衡期4.4个月，支持10,000个并发店铺请求

Conclusion: 该研究证明了生成式AI在自动化零售空间优化中的可行性，云原生架构具有良好的可扩展性和经济效益

Abstract: Planogram creation is a significant challenge for retail, requiring an average of 30 hours per complex layout. This paper introduces a cloud-native architecture using diffusion models to automatically generate store-specific planograms. Unlike conventional optimization methods that reorganize existing layouts, our system learns from successful shelf arrangements across multiple retail locations to create new planogram configurations. The architecture combines cloud-based model training via AWS with edge deployment for real-time inference. The diffusion model integrates retail-specific constraints through a modified loss function. Simulation-based analysis demonstrates the system reduces planogram design time by 98.3% (from 30 to 0.5 hours) while achieving 94.4% constraint satisfaction. Economic analysis reveals a 97.5% reduction in creation expenses with a 4.4-month break-even period. The cloud-native architecture scales linearly, supporting up to 10,000 concurrent store requests. This work demonstrates the viability of generative AI for automated retail space optimization.

</details>


### [86] [Stronger Approximation Guarantees for Non-Monotone γ-Weakly DR-Submodular Maximization](https://arxiv.org/abs/2601.00611)
*Hareshkumar Jadav,Ranveer Singh,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 该论文提出了一种用于在向下封闭凸体上最大化非负、非单调γ-弱DR-子模函数的近似算法，其保证随γ平滑变化，在γ=1时恢复0.401近似比，在γ<1时性能优雅下降。


<details>
  <summary>Details</summary>
Motivation: 在机器学习和优化中，约束下的子模目标最大化是一个基本问题。现有研究主要关注DR-子模函数（γ=1），但对于更一般的γ-弱DR-子模函数（γ<1）在向下封闭凸体上的非单调最大化问题，需要更好的近似算法。

Method: 结合Frank-Wolfe引导的连续贪婪框架与γ感知的双贪婪步骤，形成简单有效的处理非单调性的方法。通过γ-aware double-greedy步骤来适应函数的弱子模性程度。

Result: 算法提供了随γ平滑变化的近似保证：当γ=1（DR-子模情况）时恢复0.401近似因子，当γ<1时保证优雅下降，且优于先前报道的相同约束下γ-弱DR-子模最大化的边界。

Conclusion: 该方法为在向下封闭凸体上最大化非单调γ-弱DR-子模函数提供了最先进的保证，通过结合连续贪婪框架和γ感知的双贪婪步骤，实现了对非单调性的有效处理。

Abstract: Maximizing submodular objectives under constraints is a fundamental problem in machine learning and optimization. We study the maximization of a nonnegative, non-monotone $γ$-weakly DR-submodular function over a down-closed convex body. Our main result is an approximation algorithm whose guarantee depends smoothly on $γ$; in particular, when $γ=1$ (the DR-submodular case) our bound recovers the $0.401$ approximation factor, while for $γ<1$ the guarantee degrades gracefully and, it improves upon previously reported bounds for $γ$-weakly DR-submodular maximization under the same constraints. Our approach combines a Frank-Wolfe-guided continuous-greedy framework with a $γ$-aware double-greedy step, yielding a simple yet effective procedure for handling non-monotonicity. This results in state-of-the-art guarantees for non-monotone $γ$-weakly DR-submodular maximization over down-closed convex bodies.

</details>


### [87] [Entropy Production in Machine Learning Under Fokker-Planck Probability Flow](https://arxiv.org/abs/2601.00554)
*Lennon Shikhman*

Main category: cs.LG

TL;DR: 论文提出了一种基于熵的重训练框架，通过非平衡随机动力学建模数据漂移，使用熵触发机制来平衡预测性能和重训练成本。


<details>
  <summary>Details</summary>
Motivation: 现有漂移检测方法大多缺乏原理性的动力学解释，且无法有效指导如何在重训练频率和操作成本之间取得平衡。非平稳环境中的机器学习模型因数据漂移而性能下降。

Method: 将部署时的数据漂移建模为受Fokker-Planck方程控制的概率流，使用时变Kullback-Leibler散度量模型-数据不匹配。该不匹配的时间导数具有熵平衡分解，包含由概率流驱动的非负熵产生项，从而提出基于熵触发的无标签重训练策略。

Result: 在受控的非平稳分类实验中，熵触发重训练实现了与高频重训练相当的预测性能，同时相对于每日重训练和基于标签的策略，将重训练事件减少了一个数量级。

Conclusion: 基于熵的重训练框架为数据漂移提供了原理性的动力学解释，通过熵触发机制有效平衡了模型性能和重训练成本，实现了更高效的自适应模型维护。

Abstract: Machine learning models deployed in nonstationary environments experience performance degradation due to data drift. While many drift detection heuristics exist, most lack a principled dynamical interpretation and provide limited guidance on how retraining frequency should be balanced against operational cost. In this work, we propose an entropy--based retraining framework grounded in nonequilibrium stochastic dynamics. Modeling deployment--time data drift as probability flow governed by a Fokker--Planck equation, we quantify model--data mismatch using a time--evolving Kullback--Leibler divergence. We show that the time derivative of this mismatch admits an entropy--balance decomposition featuring a nonnegative entropy production term driven by probability currents. This interpretation motivates entropy--triggered retraining as a label--free intervention strategy that responds to accumulated mismatch rather than delayed performance collapse. In a controlled nonstationary classification experiment, entropy--triggered retraining achieves predictive performance comparable to high--frequency retraining while reducing retraining events by an order of magnitude relative to daily and label--based policies.

</details>


### [88] [Interpretability-Guided Bi-objective Optimization: Aligning Accuracy and Explainability](https://arxiv.org/abs/2601.00655)
*Kasra Fouladi,Hamta Rahmani*

Main category: cs.LG

TL;DR: IGBO框架通过双目标优化结合领域知识训练可解释模型，使用DAG编码特征重要性层次结构，TIG测量特征重要性，并引入最优路径预言机解决OOD问题。


<details>
  <summary>Details</summary>
Motivation: 现有可解释模型训练方法往往缺乏对结构化领域知识的有效整合，且特征重要性测量方法存在Out-of-Distribution问题，需要更鲁棒的框架来平衡模型准确性和可解释性约束。

Method: 提出Interpretability-Guided Bi-objective Optimization框架：1) 使用有向无环图编码特征重要性层次结构；2) 采用时间积分梯度测量特征重要性；3) 引入最优路径预言机学习数据流形感知的积分路径以解决OOD问题；4) 双目标优化平衡准确性和可解释性约束。

Result: 理论分析证明了收敛性和对mini-batch噪声的鲁棒性；在时间序列数据上的实验表明，IGBO能有效强制执行DAG约束，在最小化准确性损失的同时优于标准正则化基线方法。

Conclusion: IGBO框架成功地将结构化领域知识整合到可解释模型训练中，通过双目标优化和最优路径预言机解决了OOD问题，为平衡模型准确性和可解释性提供了有效方案。

Abstract: This paper introduces Interpretability-Guided Bi-objective Optimization (IGBO), a framework that trains interpretable models by incorporating structured domain knowledge via a bi-objective formulation. IGBO encodes feature importance hierarchies as a Directed Acyclic Graph (DAG) and uses Temporal Integrated Gradients (TIG) to measure feature importance. To address the Out-of-Distribution (OOD) problem in TIG computation, we propose an Optimal Path Oracle that learns data-manifold-aware integration paths. Theoretical analysis proves convergence properties and robustness to mini-batch noise, while empirical results on time-series data demonstrate IGBO's effectiveness in enforcing DAG constraints with minimal accuracy loss, outperforming standard regularization baselines.

</details>


### [89] [Adversarial Samples Are Not Created Equal](https://arxiv.org/abs/2601.00577)
*Jennifer Crawford,Amol Khanna,Fred Lu,Amy R. Wagoner,Stella Biderman,Andre T. Nguyen,Edward Raff*

Main category: cs.LG

TL;DR: 论文提出对抗性样本可分为两类：利用非鲁棒特征的样本和不利用这些特征的样本，并开发了基于集成的方法来区分这两类弱点，重新审视了对抗鲁棒性中的多个现象。


<details>
  <summary>Details</summary>
Motivation: 现有非鲁棒特征理论虽然解释了对抗性攻击的普遍脆弱性，但忽略了那些不直接利用这些特征的对抗样本。作者认为这两类样本代表了两种不同的对抗性弱点，需要区分评估。

Method: 提出了基于集成的度量方法来衡量对抗扰动对非鲁棒特征的操纵程度，并用该指标分析攻击者生成的对抗样本的构成。

Result: 通过新视角重新审视了多个现象，包括锐度感知最小化对对抗鲁棒性的影响，以及在鲁棒数据集上对抗训练和标准训练之间观察到的鲁棒性差距。

Conclusion: 对抗性弱点应分为两类：利用非鲁棒特征的样本和不利用这些特征的样本，这种区分对于准确评估对抗鲁棒性至关重要，新提出的度量方法为此提供了有效工具。

Abstract: Over the past decade, numerous theories have been proposed to explain the widespread vulnerability of deep neural networks to adversarial evasion attacks. Among these, the theory of non-robust features proposed by Ilyas et al. has been widely accepted, showing that brittle but predictive features of the data distribution can be directly exploited by attackers. However, this theory overlooks adversarial samples that do not directly utilize these features. In this work, we advocate that these two kinds of samples - those which use use brittle but predictive features and those that do not - comprise two types of adversarial weaknesses and should be differentiated when evaluating adversarial robustness. For this purpose, we propose an ensemble-based metric to measure the manipulation of non-robust features by adversarial perturbations and use this metric to analyze the makeup of adversarial samples generated by attackers. This new perspective also allows us to re-examine multiple phenomena, including the impact of sharpness-aware minimization on adversarial robustness and the robustness gap observed between adversarially training and standard training on robust datasets.

</details>


### [90] [IRPO: Scaling the Bradley-Terry Model via Reinforcement Learning](https://arxiv.org/abs/2601.00677)
*Haonan Song,Qingchen Xie,Huan Zhu,Feng Xiao,Luxi Xing,Fuzhen Li,Liu Kang,Feng Jiang,Zhiyong Zheng,Fan Yang*

Main category: cs.LG

TL;DR: 本文提出IRPO框架，通过将Bradley-Terry模型融入GRPO，解决生成奖励模型在强化学习中存在的计算瓶颈问题，实现了高效的点式评分，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 生成奖励模型因其可解释性、推理时扩展性和强化学习优化潜力而受关注，但现有的成对GRM与GRPO等RL算法结合时存在O(n²)时间复杂度的计算瓶颈，以及重复采样或链式推理带来的额外计算开销。

Method: 提出Intergroup Relative Preference Optimization (IRPO)框架，将Bradley-Terry模型整合到GRPO中，为每个响应生成点式评分，从而在RL训练期间能够高效评估任意数量的候选响应，同时保持可解释性和细粒度奖励信号。

Result: IRPO在多个基准测试中实现了点式GRM的SOTA性能，与当前领先的成对GRM性能相当，并且在训练后评估中显著优于成对GRM。

Conclusion: IRPO通过解决成对GRM的计算瓶颈问题，提供了一种高效、可扩展的强化学习框架，在保持可解释性的同时实现了优异的性能表现。

Abstract: Generative Reward Models (GRMs) have attracted considerable research interest in reward modeling due to their interpretability, inference-time scalability, and potential for refinement through reinforcement learning (RL). However, widely used pairwise GRMs create a computational bottleneck when integrated with RL algorithms such as Group Relative Policy Optimization (GRPO). This bottleneck arises from two factors: (i) the O(n^2) time complexity of pairwise comparisons required to obtain relative scores, and (ii) the computational overhead of repeated sampling or additional chain-of-thought (CoT) reasoning to improve performance. To address the first factor, we propose Intergroup Relative Preference Optimization (IRPO), a novel RL framework that incorporates the well-established Bradley-Terry model into GRPO. By generating a pointwise score for each response, IRPO enables efficient evaluation of arbitrarily many candidates during RL training while preserving interpretability and fine-grained reward signals. Experimental results demonstrate that IRPO achieves state-of-the-art (SOTA) performance among pointwise GRMs across multiple benchmarks, with performance comparable to that of current leading pairwise GRMs. Furthermore, we show that IRPO significantly outperforms pairwise GRMs in post-training evaluations.

</details>


### [91] [Cycling Race Time Prediction: A Personalized Machine Learning Approach Using Route Topology and Training Load](https://arxiv.org/abs/2601.00604)
*Francisco Aguilera Moreno*

Main category: cs.LG

TL;DR: 使用机器学习预测骑行时长，结合路线拓扑特征和运动员当前体能状态，相比传统物理模型更实用


<details>
  <summary>Details</summary>
Motivation: 现有基于物理模型的骑行时长预测方法需要大量参数（如空气阻力系数、实时风速预报），对业余骑手不实用。需要一种更简单、基于个人历史表现的预测方法。

Method: 采用机器学习方法，使用路线拓扑特征结合运动员当前体能状态（从训练负荷指标推导）。通过特征工程消除数据泄露，使用Lasso回归模型，在单人数据集（N=96次骑行）上进行N-of-1研究设计。

Result: Lasso回归结合拓扑+体能特征达到MAE=6.60分钟，R²=0.922。整合体能指标（CTL, ATL）相比仅使用拓扑特征将误差降低14%（MAE从7.66分钟降至6.60分钟）。

Conclusion: 机器学习方法能有效预测骑行时长，体能状态对自定节奏的骑行表现有显著约束作用。渐进检查点预测支持动态比赛规划。

Abstract: Predicting cycling duration for a given route is essential for training planning and event preparation. Existing solutions rely on physics-based models that require extensive parameterization, including aerodynamic drag coefficients and real-time wind forecasts, parameters impractical for most amateur cyclists. This work presents a machine learning approach that predicts ride duration using route topology features combined with the athlete's current fitness state derived from training load metrics. The model learns athlete-specific performance patterns from historical data, substituting complex physical measurements with historical performance proxies. We evaluate the approach using a single-athlete dataset (N=96 rides) in an N-of-1 study design. After rigorous feature engineering to eliminate data leakage, we find that Lasso regression with Topology + Fitness features achieves MAE=6.60 minutes and R2=0.922. Notably, integrating fitness metrics (CTL, ATL) reduces error by 14% compared to topology alone (MAE=7.66 min), demonstrating that physiological state meaningfully constrains performance even in self-paced efforts. Progressive checkpoint predictions enable dynamic race planning as route difficulty becomes apparent.

</details>


### [92] [Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning](https://arxiv.org/abs/2601.00791)
*Valentin Noël*

Main category: cs.LG

TL;DR: 提出一种无需训练的方法，通过注意力模式的光谱分析来检测大语言模型中有效的数学推理，利用四个可解释的光谱诊断指标，在多个模型架构上实现85-95.6%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 需要一种无需训练数据、微调或学习分类器的方法来检测大语言模型中数学推理的有效性，通过分析注意力模式来识别逻辑一致性而非仅仅是编译器接受度。

Method: 将注意力矩阵视为token动态图的邻接矩阵，提取四个可解释的光谱诊断指标：Fiedler值（代数连通性）、高频能量比（HFER）、图信号平滑度和光谱熵，通过单一阈值即可实现高精度分类。

Result: 在四个独立架构家族的七个transformer模型上实验，效果量达到Cohen's d=3.30，分类准确率85.0-95.6%，校准阈值在完整数据集上达到93-95%，发现光谱方法检测的是逻辑一致性而非编译器接受度。

Conclusion: 光谱图分析为推理验证提供了一个原则性框架，具有直接应用于幻觉检测和AI安全监控的潜力，同时揭示了注意力机制设计影响哪些光谱特征捕获推理有效性。

Abstract: We present a training-free method for detecting valid mathematical reasoning in large language models through spectral analysis of attention patterns. By treating attention matrices as adjacency matrices of dynamic graphs over tokens, we extract four interpretable spectral diagnostics, the Fiedler value (algebraic connectivity), high-frequency energy ratio (HFER), graph signal smoothness, and spectral entropy, that exhibit statistically significant differences between valid and invalid mathematical proofs. Experiments across seven transformer models from four independent architectural families (Meta Llama, Alibaba Qwen, Microsoft Phi, and Mistral AI) demonstrate that this spectral signature produces effect sizes up to Cohen's $d = 3.30$ ($p < 10^{-116}$), enabling 85.0--95.6\% classification accuracy under rigorous evaluation, with calibrated thresholds reaching 93--95\% on the full dataset. The method requires no training data, fine-tuning, or learned classifiers: a single threshold on a spectral metric suffices for high accuracy. Through systematic label correction, we discover that the spectral method detects logical coherence rather than compiler acceptance, identifying mathematically valid proofs that formal verifiers reject due to technical failures. We further identify an architectural dependency: Mistral-7B's Sliding Window Attention shifts the discriminative signal from HFER to late-layer Smoothness ($d = 2.09$, $p_{\text{MW}} = 1.16 \times 10^{-48}$), revealing that attention mechanism design affects which spectral features capture reasoning validity. These findings establish spectral graph analysis as a principled framework for reasoning verification with immediate applications to hallucination detection and AI safety monitoring.

</details>


### [93] [Do Chatbot LLMs Talk Too Much? The YapBench Benchmark](https://arxiv.org/abs/2601.00624)
*Vadim Borisov,Michael Gröger,Mina Mikhael,Richard H. Schreiber*

Main category: cs.LG

TL;DR: YapBench是一个轻量级基准测试，用于量化LLM在简洁理想提示上的过度生成问题，通过YapScore和YapIndex指标评估模型的多余回答长度。


<details>
  <summary>Details</summary>
Motivation: 当前LLM如ChatGPT、Claude和Gemini在简单请求上经常给出不必要的冗长回答，增加了认知负担和推理成本。先前研究表明基于偏好的后训练和LLM评估可能导致系统性的长度偏差，即使质量相当，更长的回答也会获得更高评分。

Method: 引入YapBench基准测试，包含300多个英语提示，涵盖三种简洁理想场景：需要简短澄清的最小或模糊输入、具有简短稳定答案的封闭式事实问题、以及单行代码任务。主要指标YapScore测量超出基准答案的字符数，YapIndex则是类别级别中位数YapScore的均匀加权平均值。

Result: 评估76个助手LLM，观察到中位数多余长度存在数量级差异，并识别出特定类别的失败模式，包括在模糊输入上的"真空填充"和在单行技术请求上的解释或格式化开销。

Conclusion: YapBench提供了一个量化LLM过度生成问题的框架，揭示了不同模型在简洁回答能力上的显著差异，并发布了基准测试和实时排行榜以跟踪模型随时间变化的冗长行为。

Abstract: Large Language Models (LLMs) such as ChatGPT, Claude, and Gemini increasingly act as general-purpose copilots, yet they often respond with unnecessary length on simple requests, adding redundant explanations, hedging, or boilerplate that increases cognitive load and inflates token-based inference cost. Prior work suggests that preference-based post-training and LLM-judged evaluations can induce systematic length bias, where longer answers are rewarded even at comparable quality.
  We introduce YapBench, a lightweight benchmark for quantifying user-visible over-generation on brevity-ideal prompts. Each item consists of a single-turn prompt, a curated minimal-sufficient baseline answer, and a category label. Our primary metric, YapScore, measures excess response length beyond the baseline in characters, enabling comparisons across models without relying on any specific tokenizer. We summarize model performance via the YapIndex, a uniformly weighted average of category-level median YapScores.
  YapBench contains over three hundred English prompts spanning three common brevity-ideal settings: (A) minimal or ambiguous inputs where the ideal behavior is a short clarification, (B) closed-form factual questions with short stable answers, and (C) one-line coding tasks where a single command or snippet suffices. Evaluating 76 assistant LLMs, we observe an order-of-magnitude spread in median excess length and distinct category-specific failure modes, including vacuum-filling on ambiguous inputs and explanation or formatting overhead on one-line technical requests. We release the benchmark and maintain a live leaderboard for tracking verbosity behavior over time.

</details>


### [94] [TeleDoCTR: Domain-Specific and Contextual Troubleshooting for Telecommunications](https://arxiv.org/abs/2601.00691)
*Mohamed Trabelsi,Huseyin Uzunalioglu*

Main category: cs.LG

TL;DR: TeleDoCTR是一个针对电信领域设计的端到端故障排除系统，通过集成领域特定排名和生成模型，自动化处理工单分类、历史工单检索和故障分析报告生成，显著提升电信工单处理的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 电信领域的工单故障排除是一个高度复杂且耗时的过程，需要专家解读工单内容、查阅文档和搜索历史记录。这种人工密集型方法不仅延迟问题解决，还阻碍整体运营效率。为解决这些问题，需要开发专门针对电信领域的自动化故障排除系统。

Method: 提出TeleDoCTR系统，这是一个电信相关的、领域特定的、上下文感知的故障排除系统。系统集成领域特定排名模型和生成模型，自动化处理三个关键步骤：1) 将工单路由到适当的专家团队（分类任务）；2) 检索上下文和语义相似的历史工单（检索任务）；3) 生成详细的故障分析报告，包括问题描述、根本原因和潜在解决方案（生成任务）。

Result: 在真实的电信基础设施数据集上评估TeleDoCTR，结果显示该系统在性能上优于现有的最先进方法，显著提高了故障排除过程的准确性和效率。

Conclusion: TeleDoCTR系统通过自动化电信工单故障排除的关键步骤，有效解决了传统人工方法的效率瓶颈，为电信领域的工单处理提供了高效、准确的端到端解决方案。

Abstract: Ticket troubleshooting refers to the process of analyzing and resolving problems that are reported through a ticketing system. In large organizations offering a wide range of services, this task is highly complex due to the diversity of submitted tickets and the need for specialized domain knowledge. In particular, troubleshooting in telecommunications (telecom) is a very time-consuming task as it requires experts to interpret ticket content, consult documentation, and search historical records to identify appropriate resolutions. This human-intensive approach not only delays issue resolution but also hinders overall operational efficiency. To enhance the effectiveness and efficiency of ticket troubleshooting in telecom, we propose TeleDoCTR, a novel telecom-related, domain-specific, and contextual troubleshooting system tailored for end-to-end ticket resolution in telecom. TeleDoCTR integrates both domain-specific ranking and generative models to automate key steps of the troubleshooting workflow which are: routing tickets to the appropriate expert team responsible for resolving the ticket (classification task), retrieving contextually and semantically similar historical tickets (retrieval task), and generating a detailed fault analysis report outlining the issue, root cause, and potential solutions (generation task). We evaluate TeleDoCTR on a real-world dataset from a telecom infrastructure and demonstrate that it achieves superior performance over existing state-of-the-art methods, significantly enhancing the accuracy and efficiency of the troubleshooting process.

</details>


### [95] [ARISE: Adaptive Reinforcement Integrated with Swarm Exploration](https://arxiv.org/abs/2601.00693)
*Rajiv Chaitanya M,D R Ramesh Babu*

Main category: cs.LG

TL;DR: ARISE是一个轻量级强化学习框架，通过添加基于粒子群的探索层来增强标准策略梯度方法，在复杂任务和非平稳奖励环境中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习中的有效探索仍然是一个关键挑战，特别是在非平稳奖励或高维策略的情况下。现有方法在复杂环境中探索不足，需要更有效的探索机制。

Method: ARISE框架将标准策略梯度方法与紧凑的基于粒子群的探索层相结合。它将策略动作与粒子驱动的提议混合，每个粒子代表在动作空间中采样的候选策略轨迹，并使用奖励方差线索自适应地调节探索。

Result: 在简单基准测试中改进有限（如CartPole-v1上+0.7%），但在更具挑战性的任务上获得显著提升：LunarLander-v3上+46%，Hopper-v4上+22%，同时在Walker2d和Ant上保持稳定。在非平稳奖励变化下，ARISE提供明显的鲁棒性优势，在CartPole上优于PPO +75分，在LunarLander上也有相应改进。

Conclusion: ARISE提供了一个简单、架构无关的途径，可以在不改变核心算法结构的情况下，创建更具探索性和鲁棒性的强化学习智能体。消融研究证实了粒子群组件和自适应机制对性能的贡献。

Abstract: Effective exploration remains a key challenge in RL, especially with non-stationary rewards or high-dimensional policies. We introduce ARISE, a lightweight framework that enhances reinforcement learning by augmenting standard policy-gradient methods with a compact swarm-based exploration layer. ARISE blends policy actions with particle-driven proposals, where each particle represents a candidate policy trajectory sampled in the action space, and modulates exploration adaptively using reward-variance cues. While easy benchmarks exhibit only slight improvements (e.g., +0.7% on CartPole-v1), ARISE yields substantial gains on more challenging tasks, including +46% on LunarLander-v3 and +22% on Hopper-v4, while preserving stability on Walker2d and Ant. Under non-stationary reward shifts, ARISE provides marked robustness advantages, outperforming PPO by +75 points on CartPole and improving LunarLander accordingly. Ablation studies confirm that both the swarm component and the adaptive mechanism contribute to the performance. Overall, ARISE offers a simple, architecture-agnostic route to more exploratory and resilient RL agents without altering core algorithmic structures.

</details>


### [96] [Bayesian Inverse Games with High-Dimensional Multi-Modal Observations](https://arxiv.org/abs/2601.00696)
*Yash Jain,Xinjie Liu,Lasse Peters,David Fridovich-Keil,Ufuk Topcu*

Main category: cs.LG

TL;DR: 提出贝叶斯逆博弈框架，通过变分自编码器和可微纳什博弈求解器从交互数据中学习先验和后验分布，相比最大似然估计方法能更好地量化不确定性并实现更安全的决策。


<details>
  <summary>Details</summary>
Motivation: 现有逆博弈方法仅提供点估计，无法量化估计不确定性，导致下游规划决策可能过度自信地采取不安全行动。需要一种能够量化不确定性并整合多模态观测数据的贝叶斯推理方法。

Method: 提出贝叶斯逆博弈框架，使用带有嵌入式可微纳什博弈求解器的结构化变分自编码器在交互数据集上进行训练，无需真实目标标签，能够实时生成隐藏目标的后验分布样本。

Result: 框架成功学习到先验和后验分布，相比基于最大似然估计的逆博弈方法提高了推理质量，实现了更安全的下游决策而不牺牲效率。当轨迹信息不充分时，多模态推理能进一步减少不确定性。

Conclusion: 贝叶斯逆博弈方法能够有效量化不确定性，整合多模态观测数据，为自主决策提供更安全可靠的逆博弈解决方案，特别是在信息有限或不完整的情况下表现出优势。

Abstract: Many multi-agent interaction scenarios can be naturally modeled as noncooperative games, where each agent's decisions depend on others' future actions. However, deploying game-theoretic planners for autonomous decision-making requires a specification of all agents' objectives. To circumvent this practical difficulty, recent work develops maximum likelihood techniques for solving inverse games that can identify unknown agent objectives from interaction data. Unfortunately, these methods only infer point estimates and do not quantify estimator uncertainty; correspondingly, downstream planning decisions can overconfidently commit to unsafe actions. We present an approximate Bayesian inference approach for solving the inverse game problem, which can incorporate observation data from multiple modalities and be used to generate samples from the Bayesian posterior over the hidden agent objectives given limited sensor observations in real time. Concretely, the proposed Bayesian inverse game framework trains a structured variational autoencoder with an embedded differentiable Nash game solver on interaction datasets and does not require labels of agents' true objectives. Extensive experiments show that our framework successfully learns prior and posterior distributions, improves inference quality over maximum likelihood estimation-based inverse game approaches, and enables safer downstream decision-making without sacrificing efficiency. When trajectory information is uninformative or unavailable, multimodal inference further reduces uncertainty by exploiting additional observation modalities.

</details>


### [97] [BSAT: B-Spline Adaptive Tokenizer for Long-Term Time Series Forecasting](https://arxiv.org/abs/2601.00698)
*Maximilian Reinwardt,Michael Eichelbeck,Matthias Althoff*

Main category: cs.LG

TL;DR: 提出BSAT方法，通过B样条自适应分词解决时间序列预测中自注意力二次复杂度和均匀分块不匹配语义结构的问题，结合L-RoPE位置编码，在内存受限场景下实现高压缩率下的强性能。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在长期时间序列预测中存在两个主要问题：自注意力的二次计算复杂度限制了处理长序列的能力；均匀分块方法无法适应数据语义结构，可能导致信息丢失或冗余。

Method: 提出B样条自适应分词器（BSAT），通过拟合B样条自适应分割时间序列，在高曲率区域放置token，将变长基函数表示为固定大小的token（包含系数和位置）。同时提出混合位置编码L-RoPE，结合可学习加法位置编码和具有层间可学习基的旋转位置嵌入。

Result: 在多个公开基准测试中表现出色，在高压缩率下仍保持强大性能，特别适合内存受限的应用场景。

Conclusion: BSAT方法有效解决了Transformer在长期时间序列预测中的计算效率和语义对齐问题，通过自适应分词和创新的位置编码机制，在保持性能的同时显著降低内存需求。

Abstract: Long-term time series forecasting using transformers is hampered by the quadratic complexity of self-attention and the rigidity of uniform patching, which may be misaligned with the data's semantic structure. In this paper, we introduce the \textit{B-Spline Adaptive Tokenizer (BSAT)}, a novel, parameter-free method that adaptively segments a time series by fitting it with B-splines. BSAT algorithmically places tokens in high-curvature regions and represents each variable-length basis function as a fixed-size token, composed of its coefficient and position. Further, we propose a hybrid positional encoding that combines a additive learnable positional encoding with Rotary Positional Embedding featuring a layer-wise learnable base: L-RoPE. This allows each layer to attend to different temporal dependencies. Our experiments on several public benchmarks show that our model is competitive with strong performance at high compression rates. This makes it particularly well-suited for use cases with strong memory constraints.

</details>


### [98] [Precision Autotuning for Linear Solvers via Contextual Bandit-Based RL](https://arxiv.org/abs/2601.00728)
*Erin Carson,Xinye Chen*

Main category: cs.LG

TL;DR: 提出基于强化学习的自适应精度调优框架，用于线性求解器和其他算法，通过上下文多臂老虎机问题动态选择精度配置，平衡计算精度与效率。


<details>
  <summary>Details</summary>
Motivation: 传统数值计算中固定精度方法在计算效率和精度之间存在权衡，需要自适应方法根据具体问题特征动态调整精度，以优化计算性能。

Method: 将精度调优建模为上下文多臂老虎机问题，使用离散化状态空间和增量动作价值估计，通过Q表映射特征（如近似条件数和矩阵范数）到精度配置动作，采用epsilon-greedy策略优化多目标奖励函数。

Result: 在线性系统迭代求精应用中，该框架能有效选择精度配置，在保持与双精度基准相当精度的同时显著降低计算成本，并能泛化到未见过的数据集。

Conclusion: 这是首个基于强化学习的精度自动调优工作，验证了RL在混合精度数值方法中的潜力，为科学计算中自适应精度选择提供了新思路。

Abstract: We propose a reinforcement learning (RL) framework for adaptive precision tuning of linear solvers, and can be extended to general algorithms. The framework is formulated as a contextual bandit problem and solved using incremental action-value estimation with a discretized state space to select optimal precision configurations for computational steps, balancing precision and computational efficiency. To verify its effectiveness, we apply the framework to iterative refinement for solving linear systems $Ax = b$. In this application, our approach dynamically chooses precisions based on calculated features from the system. In detail, a Q-table maps discretized features (e.g., approximate condition number and matrix norm)to actions (chosen precision configurations for specific steps), optimized via an epsilon-greedy strategy to maximize a multi-objective reward balancing accuracy and computational cost. Empirical results demonstrate effective precision selection, reducing computational cost while maintaining accuracy comparable to double-precision baselines. The framework generalizes to diverse out-of-sample data and offers insight into utilizing RL precision selection for other numerical algorithms, advancing mixed-precision numerical methods in scientific computing. To the best of our knowledge, this is the first work on precision autotuning with RL and verified on unseen datasets.

</details>


### [99] [The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving](https://arxiv.org/abs/2601.00747)
*Max Ruiz Luyten,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 论文分析了当前LLM推理流程中基于正确性的强化学习导致推理路径分布崩溃的问题，提出了分布创造性推理（DCR）框架，统一了多种训练方法，并提供了保持模型正确性和创造性的解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理流程主要基于正确性优化，通过采样多样化的思维链并强化得分最高的路径。这种设计选择会导致模型在推理路径上的分布崩溃，降低语义熵并削弱创造性问题解决能力。

Method: 提出了分布创造性推理（DCR）框架，将训练视为通过解决方案轨迹概率测度的梯度流。该框架统一了STaR、GRPO、DPO以及熵奖励等多种方法，作为同一损失函数的特例。

Result: 1. 多样性衰减定理：描述了基于正确性的目标如何导致STaR、GRPO和DPO的不同多样性衰减模式；2. 确保收敛到稳定且多样化策略的设计，有效防止崩溃；3. 实践中实现这一目标的简单可行方案。

Conclusion: DCR为LLM提供了首个保持正确性和创造性的原则性方案，解决了当前推理流程中分布崩溃的问题，为开发既正确又富有创造性的语言模型提供了理论基础和实践指导。

Abstract: State-of-the-art large language model (LLM) pipelines rely on bootstrapped reasoning loops: sampling diverse chains of thought and reinforcing the highest-scoring ones, mainly optimizing correctness. We analyze how this design choice is sensitive to the collapse of the model's distribution over reasoning paths, slashing semantic entropy and undermining creative problem-solving. To analyze this failure, we introduce Distributional Creative Reasoning (DCR), a unified variational objective that casts training as gradient flow through probability measures on solution traces. STaR, GRPO, and DPO, as well as entropy bonuses, and other methods, all constitute special cases of the same loss. The framework delivers three core results: (i) the diversity decay theorem, describing how correctness-based objectives lead to distinct modes of diversity decay for STaR, GRPO, and DPO; (ii) designs that ensure convergence to a stable and diverse policy, effectively preventing collapse; and (iii) simple, actionable recipes to achieve this in practice. DCR thus offers the first principled recipe for LLMs that remain both correct and creative.

</details>


### [100] [A Machine Learning Framework for Off Ball Defensive Role and Performance Evaluation in Football](https://arxiv.org/abs/2601.00748)
*Sean Groom,Shuo Wang,Francisco Belo,Axl Rice,Liam Anderson*

Main category: cs.LG

TL;DR: 提出基于协变量依赖隐马尔可夫模型(CDHMM)的足球角球防守评估框架，无需标注即可从球员追踪数据推断盯人和区域防守任务，用于防守贡献归因和反事实分析。


<details>
  <summary>Details</summary>
Motivation: 足球中无球防守表现评估具有挑战性，传统指标无法捕捉限制对手行动选择和成功概率的协调动作。现有价值模型主要评估有球动作，防守分析有限；反事实方法如ghosting模型依赖"平均"行为模拟，缺乏战术背景。

Method: 针对高度结构化的角球场景，提出协变量依赖隐马尔可夫模型(CDHMM)，无需标注即可从球员追踪数据推断时间分辨的盯人和区域防守任务分配。基于这些分配提出防守贡献归因新框架和角色条件ghosting方法进行无球防守反事实分析。

Result: 该方法能够提供可解释的防守贡献评估，相对于情境感知基准线进行分析，为无球防守表现提供更准确的评估框架。

Conclusion: CDHMM模型和角色条件ghosting方法为足球角球防守提供了创新的评估工具，能够从球员追踪数据中自动推断防守任务分配，实现更准确的无球防守表现分析和贡献归因。

Abstract: Evaluating off-ball defensive performance in football is challenging, as traditional metrics do not capture the nuanced coordinated movements that limit opponent action selection and success probabilities. Although widely used possession value models excel at appraising on-ball actions, their application to defense remains limited. Existing counterfactual methods, such as ghosting models, help extend these analyses but often rely on simulating "average" behavior that lacks tactical context. To address this, we introduce a covariate-dependent Hidden Markov Model (CDHMM) tailored to corner kicks, a highly structured aspect of football games. Our label-free model infers time-resolved man-marking and zonal assignments directly from player tracking data. We leverage these assignments to propose a novel framework for defensive credit attribution and a role-conditioned ghosting method for counterfactual analysis of off-ball defensive performance. We show how these contributions provide a interpretable evaluation of defensive contributions against context-aware baselines.

</details>


### [101] [Memory Bank Compression for Continual Adaptation of Large Language Models](https://arxiv.org/abs/2601.00756)
*Thomas Katraouras,Dimitrios Rafailidis*

Main category: cs.LG

TL;DR: MBC提出了一种通过码本优化策略压缩记忆库的方法，用于大语言模型的持续学习，将记忆库大小减少到基准方法的0.3%，同时保持高准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的知识容易过时，持续学习需要更新模型而不遗忘旧知识。现有基于记忆库的方法面临记忆库随数据流不断增长的瓶颈问题。

Method: 提出MBC模型：1) 通过码本优化策略压缩记忆库；2) 引入在线重置机制防止码本崩溃；3) 在注意力层使用Key-Value低秩适配技术高效利用压缩记忆表示。

Result: 在基准问答数据集上的实验表明，MBC将记忆库大小减少到最竞争基线的0.3%，同时在在线适应学习中保持高保留准确率。

Conclusion: MBC通过记忆库压缩有效解决了持续学习中记忆库无限增长的问题，实现了高效且稳定的在线适应学习。

Abstract: Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC.

</details>


### [102] [Categorical Reparameterization with Denoising Diffusion models](https://arxiv.org/abs/2601.00781)
*Samson Gourevitch,Alain Durmus,Eric Moulines,Jimmy Olsson,Yazid Janati*

Main category: cs.LG

TL;DR: 提出了一种基于扩散的软重参数化方法，用于处理分类变量的梯度优化问题，通过高斯噪声过程的去噪器实现训练自由的扩散采样器。


<details>
  <summary>Details</summary>
Motivation: 传统的分类变量梯度优化方法存在局限性：基于分数函数的估计器虽然无偏但噪声大，而连续松弛方法虽然能提供路径梯度但优化的是有偏的温度依赖目标。需要一种更好的方法来处理分类变量的优化问题。

Method: 提出扩散基软重参数化方法，针对分类分布，利用高斯噪声过程的去噪器具有闭式解且可高效计算的特性，构建训练自由的扩散采样器，通过该采样器进行反向传播。

Result: 实验表明，所提出的重参数化技巧在各种基准测试中取得了竞争性或改进的优化性能。

Conclusion: 扩散基软重参数化为分类变量的梯度优化提供了一种有效的新方法，通过训练自由的扩散采样器实现了更好的优化性能。

Abstract: Gradient-based optimization with categorical variables typically relies on score-function estimators, which are unbiased but noisy, or on continuous relaxations that replace the discrete distribution with a smooth surrogate admitting a pathwise (reparameterized) gradient, at the cost of optimizing a biased, temperature-dependent objective. In this paper, we extend this family of relaxations by introducing a diffusion-based soft reparameterization for categorical distributions. For these distributions, the denoiser under a Gaussian noising process admits a closed form and can be computed efficiently, yielding a training-free diffusion sampler through which we can backpropagate. Our experiments show that the proposed reparameterization trick yields competitive or improved optimization performance on various benchmarks.

</details>
