<div id=toc></div>

# Table of Contents

- [nlin.CD](#nlin.CD) [Total: 3]
- [cs.LG](#cs.LG) [Total: 214]
- [cs.AI](#cs.AI) [Total: 94]
- [physics.comp-ph](#physics.comp-ph) [Total: 7]
- [cs.CC](#cs.CC) [Total: 6]
- [quant-ph](#quant-ph) [Total: 59]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 8]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 13]
- [nlin.AO](#nlin.AO) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 4]


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [1] [Trajectory arclength reveals chaos](https://arxiv.org/abs/2602.07660)
*Javier Jiménez-López,V. J. García-Garrido*

Main category: nlin.CD

TL;DR: 提出相空间弧长作为新型混沌指标，通过谱分析区分规则与混沌运动，无需变分方程计算


<details>
  <summary>Details</summary>
Motivation: 解决传统混沌指标需计算变分方程和邻近轨道的复杂性，寻求更简洁的自洽方法

Method: 用时均拉格朗日描述子量化相空间弧长，结合Birkhoff遍历定理分析功率谱密度

Result: 规则运动对应δ函数谱，混沌运动呈1/ω幂律谱，验证于Hénon-Heiles和Fermi-Pasta-Ulam系统

Conclusion: 建立拉格朗日描述子与谱分析的严格理论框架，实现高维系统高效混沌分析并为机器学习提供数据

Abstract: In this paper we demonstrate that the phase space arclength of a trajectory, quantified by the time-averaged Lagrangian descriptor, is a robust and self-contained chaos indicator. By invoking Birkhoff's Ergodic Partition Theorem, we show that this scalar function distinguishes dynamical regimes via its power spectral density: for regular motion it converges to a delta function, whereas for chaotic trajectories the spectrum exhibits an inverse power-law $(1/ω)$ driven by the phenomenon of dynamical stickiness. With this approach, we avoid the computation and simulation of the variational equations and the usage of neighboring orbits, making it the simplest geometrical chaos indicator derivable from Lagrangian descriptors. Its computational efficiency enables the study of high-dimensional systems and allows the generation of large datasets of classified initial conditions, ideal for training Machine Learning models. We validate these findings using the Hénon-Heiles and the Fermi-Pasta-Ulam systems. By linking the geometrical properties of phase space to spectral analysis, this work provides the mathematical justification to establish Lagrangian descriptors as a rigorous, self-sufficient framework for the global analysis of chaos and regularity in Hamiltonian systems.

</details>


### [2] [Chaos and Parrondo's paradox: An overview](https://arxiv.org/abs/2602.08135)
*Marcelo A. Pires,Erveton P. Pinto,Jose S. Cánovas,Silvio M. Duarte Queirós*

Main category: nlin.CD

TL;DR: 本文系统综述了Parrondo悖论与混沌之间的双向关联，揭示混沌切换如何使失败策略转为获胜策略，以及PP如何在有序与混沌的相互转化中表现，并提供了可探索这些现象的Python代码。


<details>
  <summary>Details</summary>
Motivation: Parrondo悖论是非线性科学的基本原理，其与混沌的关系研究尚不系统。厘清这一关联有助于揭示普适机制并拓展其在科技领域的应用，因此本文首次系统性地全景式回顾该交叉领域。

Method: 通过文献综述，从两个方向系统分析：(1) 用Lyapunov指数和熵度量等量化指标将PP转化为有序-混沌的相互转化（混沌+混沌→有序，有序+有序→混沌）；(2) 利用混沌设计切换协议。同时提供完整Python代码供读者实验验证。

Result: 建立了PP与混沌的双向联系，证实了CCO和OOC现象，展示了混沌可设计非平凡切换协议，提供了相关量化工具和代码，从而阐明了PP的普适性和跨学科应用潜力。

Conclusion: PP与混沌存在深刻普适的联系，应用前景广阔。但仍存在开放性问题：理论极限、高维映射与连续流的作用、以及动态PP在混沌系统中实验验证的不足。未来工作应聚焦这些方向，Python代码将助力进一步探索。

Abstract: Parrondo's paradox (PP) is a fundamental principle in nonlinear science where the alternation of individually losing strategies leads to a winning outcome. In this topical review, we provide the first systematic panorama of the synergy between PP and chaos. We observe a bidirectional connection between the two areas. The first direction is the translation of PP into the interplay between Order and Chaos through either Chaos + Chaos $\to$ Order (CCO) or Order + Order $\to$ Chaos (OOC). In this vein, many quantifiers, such as Lyapunov Exponents, $λ$, and entropic measures, are used. Second, we note that chaos can be used to engineer switching protocols that can lead to nontrivial effects in diverse PP cases. Our review clarifies the universality of PP and highlights its robust theoretical and practical applications across several areas of science and technology. Finally, we delineate key open questions, emphasizing the unresolved theoretical limits, the role of high-dimensional maps and continuous flows, and the critical need for more experimental verification of the dynamic PP in chaotic systems. For completeness, we also provide a full Python code that allows the reader to observe the many facets of the PP.

</details>


### [3] [Chaos, the Critical Phenomenon in Phase Space: Feigenbaum Constants and Critical Exponents](https://arxiv.org/abs/2602.08895)
*Yonghui Xia,Hongtao Feng*

Main category: nlin.CD

TL;DR: 研究混沌在耗散和保守系统中的临界性质，利用重整化群方法解决孤立系统熵增与可逆性的矛盾。


<details>
  <summary>Details</summary>
Motivation: 探索混沌作为相空间中临界现象的本质，并解决热力学第二定律与孤立系统可逆性之间的理论悖论。

Method: 采用重整化群方法分析耗散系统和保守系统中的混沌行为。

Result: 发现混沌是相空间中平衡统计的临界现象；周期倍增分岔系统中的两个费根鲍姆常数对应两个独立普适临界指数，可用于区分混沌类别；保守系统中因混沌临界性，不同参数的孤立系统在相空间中相关联，导致信息不可逆丢失和熵增加。

Conclusion: 混沌的临界性质解释了孤立系统中熵的增加，从而解决了热力学第二定律与系统可逆性之间的矛盾。

Abstract: Chaos in both dissipative systems and conservative systems is investigated on the approach of renormalization group. It is found that the chaos is regarded as the critical phenomenon of equilibrium statistics in phase space. The two Feigenbaum constants in the period-doubling bifurcation systems correspond to two independent critical exponents, which are universal and can be adopted to distinguish the classes of chaos. For the conservative systems, due to the critical nature of the chaos, the isolated systems with different parameters are correlated in the phase space, and therefore the isolated system is no longer isolated in the phase space. The information of conservative systems is irreversibly lost over time, which leads to the increase entropy in an isolated system, and the contradiction between the second law of thermodynamics and the reversibility of isolated systems can be resolved.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [4] [Neural Sabermetrics with World Model: Play-by-play Predictive Modeling with Large Language Model](https://arxiv.org/abs/2602.07030)
*Young Jin Ahn,Yiyang Du,Zheyuan Zhang,Haisen Kang*

Main category: cs.LG

TL;DR: 提出基于大语言模型的棒球世界模型，通过预训练处理十年MLB数据，实现多任务预测，在下一球和击球决策预测上超越现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统棒球统计指标擅长回顾性分析，但无法生成性地模拟比赛逐球演进的过程，现有方法局限于单步预测或事后分析。

Method: 将棒球比赛建模为自回归事件序列，使用单一LLM在超过十年MLB追踪数据（700万投球序列、约30亿token）上连续预训练，构建统一的play-by-play世界模型。

Result: 在分布内常规赛和分布外季后赛中，模型表现优异：下一球预测准确率约64%，击球决策预测准确率78%，单一模型性能超越现有基线。

Conclusion: 大语言模型可作为体育领域有效的世界模型，为体育分析提供生成式建模新范式。

Abstract: Classical sabermetrics has profoundly shaped baseball analytics by summarizing long histories of play into compact statistics. While these metrics are invaluable for valuation and retrospective analysis, they do not define a generative model of how baseball games unfold pitch by pitch, leaving most existing approaches limited to single-step prediction or post-hoc analysis. In this work, we present Neural Sabermetrics with World Model, a Large Language Model (LLM) based play-by-play world model for baseball. We cast baseball games as long auto-regressive sequences of events and continuously pretrain a single LLM on more than ten years of Major League Baseball (MLB) tracking data, comprising over seven million pitch sequences and approximately three billion tokens. The resulting model is capable of predicting multiple aspects of game evolution within a unified framework. We evaluate our model on both in-distribution regular-season data and out-of-distribution postseason games and compare against strong neural baselines from prior work. Despite using a single backbone model, our approach outperforms the performance of existing baselines, (1) correctly predicting approximately 64% of next pitches within a plate appearance and (2) 78% of batter swing decisions, suggesting that LLMs can serve as effective world models for sports.

</details>


### [5] [Lagged backward-compatible physics-informed neural networks for unsaturated soil consolidation analysis](https://arxiv.org/abs/2602.07031)
*Dong Li,Shuai Huang,Yapeng Cao,Yujun Cui,Xiaobin Wei,Hongtao Cao*

Main category: cs.LG

TL;DR: 提出LBC-PINN框架模拟非饱和土固结，通过对数时间分割和滞后兼容性损失解决多尺度孔隙压力耗散问题，验证误差低于1e-2


<details>
  <summary>Details</summary>
Motivation: 解决长期荷载下非饱和土中气水压力多尺度耗散的模拟难题，传统方法在跨时间域耦合分析中存在计算效率与精度瓶颈

Method: 1) 构建对数时间分割策略处理多尺度问题；2) 引入滞后兼容性损失函数确保物理约束；3) 采用分段迁移学习提升训练效率

Result: 在1e10秒时间尺度内孔隙压力预测误差低于1e-2，基于气相特征耗散时间的简化分割策略保持精度同时提升计算效率，框架在1e-3~1e3气透率比范围内稳健

Conclusion: LBC-PINN有效突破非饱和土固结模拟的时域限制，为岩土工程长期变形预测提供高精度AI解决方案

Abstract: This study develops a Lagged Backward-Compatible Physics-Informed Neural Network (LBC-PINN) for simulating and inverting one-dimensional unsaturated soil consolidation under long-term loading. To address the challenges of coupled air and water pressure dissipation across multi-scale time domains, the framework integrates logarithmic time segmentation, lagged compatibility loss enforcement, and segment-wise transfer learning.
  In forward analysis, the LBC-PINN with recommended segmentation schemes accurately predicts pore air and pore water pressure evolution. Model predictions are validated against finite element method (FEM) results, with mean absolute errors below 1e-2 for time durations up to 1e10 seconds. A simplified segmentation strategy based on the characteristic air-phase dissipation time improves computational efficiency while preserving predictive accuracy. Sensitivity analyses confirm the robustness of the framework across air-to-water permeability ratios ranging from 1e-3 to 1e3.

</details>


### [6] [TransConv-DDPM: Enhanced Diffusion Model for Generating Time-Series Data in Healthcare](https://arxiv.org/abs/2602.07033)
*Md Shahriar Kabir,Sana Alamgeer,Minakshi Debnath,Anne H. H. Ngu*

Main category: cs.LG

TL;DR: 提出TransConv-DDPM模型，通过扩散模型和Transformer结合生成高质量生理时间序列数据，解决医疗AI数据稀缺问题，在跌倒和脑电数据集上显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 临床医疗领域缺乏真实数据严重阻碍了AI诊断和预防工具的发展。尽管生成式AI在视觉和自然语言处理中表现出色，但生理时间序列数据的复杂性使其生成极具挑战。

Method: 采用去噪扩散概率模型(DDPM)结合U-Net架构，集成多尺度卷积模块和Transformer层，同时捕捉全局和局部时序依赖关系。

Result: 在三个数据集上验证，相比TimeGAN和Diffusion-TS表现优异。在SmartFallMM数据集上，加入合成跌倒数据使预测模型的F1分数提升13.64%，准确率提高14.93%。

Conclusion: TransConv-DDPM能有效生成高质量合成生理时序数据，为医疗AI应用提供解决数据稀缺问题的新途径。

Abstract: The lack of real-world data in clinical fields poses a major obstacle in training effective AI models for diagnostic and preventive tools in medicine. Generative AI has shown promise in increasing data volume and enhancing model training, particularly in computer vision and natural language processing (NLP) domains. However, generating physiological time-series data, a common type in medical AI applications, presents unique challenges due to its inherent complexity and variability. This paper introduces TransConv-DDPM, an enhanced generative AI method for biomechanical and physiological time-series data generation. The model employs a denoising diffusion probabilistic model (DDPM) with U-Net, multi-scale convolution modules, and a transformer layer to capture both global and local temporal dependencies. We evaluated TransConv-DDPM on three diverse datasets, generating both long and short-sequence time-series data. Quantitative comparisons against state-of-the-art methods, TimeGAN and Diffusion-TS, using four performance metrics, demonstrated promising results, particularly on the SmartFallMM and EEG datasets, where it effectively captured the more gradual temporal change patterns between data points. Additionally, a utility test on the SmartFallMM dataset revealed that adding synthetic fall data generated by TransConv-DDPM improved predictive model performance, showing a 13.64% improvement in F1-score and a 14.93% increase in overall accuracy compared to the baseline model trained solely on fall data from the SmartFallMM dataset. These findings highlight the potential of TransConv-DDPM to generate high-quality synthetic data for real-world applications.

</details>


### [7] [Mutual information and task-relevant latent dimensionality](https://arxiv.org/abs/2602.08105)
*Paarth Gulati,Eslam Abdelaleem,Audrey Sederberg,Ilya Nemenman*

Main category: cs.LG

TL;DR: 提出基于信息瓶颈的维度估计新方法：通过混合判别器解决神经互信息估计器的高估问题，实现单次估计任务相关维度，在合成数据和物理数据集上验证有效性


<details>
  <summary>Details</summary>
Motivation: 估计预测任务所需的潜在表示维度（任务相关维度）是未解决的难题，现有神经互信息估计器存在维度高估问题，且缺乏高效估计协议

Method: 1) 将问题重构为信息瓶颈框架：寻找能保留预测器与目标互信息的最小嵌入维度 2) 设计混合判别器（保留显式维度瓶颈+非线性跨视图交互）抑制高估 3) 提出单次协议（从过参数化混合模型直接读取有效维度）

Result: 1) 标准估计器存在系统性维度高估，混合判别器消除该偏差 2) 在含已知任务维度的合成数据中准确估计 3) 噪声环境下优于经典几何维度估计器 4) 在多个物理数据集验证实用性

Conclusion: 该方法通过信息瓶颈框架与混合架构创新，可靠估计任务相关维度，为高维数据降维提供新工具，尤其在经典方法失效的噪声场景表现优异

Abstract: Estimating the dimensionality of the latent representation needed for prediction -- the task-relevant dimension -- is a difficult, largely unsolved problem with broad scientific applications. We cast it as an Information Bottleneck question: what embedding bottleneck dimension is sufficient to compress predictor and predicted views while preserving their mutual information (MI). This repurposes neural MI estimators for dimensionality estimation. We show that standard neural estimators with separable/bilinear critics systematically inflate the inferred dimension, and we address this by introducing a hybrid critic that retains an explicit dimensional bottleneck while allowing flexible nonlinear cross-view interactions, thereby preserving the latent geometry. We further propose a one-shot protocol that reads off the effective dimension from a single over-parameterized hybrid model, without sweeping over bottleneck sizes. We validate the approach on synthetic problems with known task-relevant dimension. We extend the approach to intrinsic dimensionality by constructing paired views of a single dataset, enabling comparison with classical geometric dimension estimators. In noisy regimes where those estimators degrade, our approach remains reliable. Finally, we demonstrate the utility of the method on multiple physics datasets.

</details>


### [8] [AVERE: Improving Audiovisual Emotion Reasoning with Preference Optimization](https://arxiv.org/abs/2602.07054)
*Ashutosh Chaubey,Jiacheng Pang,Maksim Siniukov,Mohammad Soleymani*

Main category: cs.LG

TL;DR: 本文针对多模态大语言模型在情感理解任务中的虚假关联和幻觉问题，提出了EmoReAlM评估基准和AVEm-DPO优化方法，在零样本设置下实现6-19%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 情感理解对社会智能体至关重要，但现有MLLM存在两个关键挑战：情感与无关视听线索的虚假关联，以及语言模型先验导致的视听线索幻觉，限制了模型的可靠性。

Method: 提出EmoReAlM基准评估线索-情感关联、幻觉和模态一致性；设计AVEm-DPO偏好优化技术，通过对虚假/幻觉响应与视听输入对构建偏好，并加入惩罚文本先验的正则项，使模型响应与视听输入和情感查询对齐。

Result: 在DFEW、RAVDESS和EMER数据集上的零样本实验表明，AVEm-DPO使基线模型获得6-19%的相对性能提升。

Conclusion: 该工作提供了严谨的评估基准和稳健的优化框架，实现了MLLM在情感理解和社会AI领域中的原则性评估与改进。

Abstract: Emotion understanding is essential for building socially intelligent agents. Although recent multimodal large language models have shown strong performance on this task, two key challenges remain - spurious associations between emotions and irrelevant audiovisual cues, and hallucinations of audiovisual cues driven by text priors in the language model backbone. To quantify and understand these issues, we introduce EmoReAlM, a benchmark designed to evaluate MLLMs for cue-emotion associations, hallucinations and modality agreement. We then propose AVEm-DPO, a preference optimization technique that aligns model responses with both audiovisual inputs and emotion-centric queries. Specifically, we construct preferences over responses exhibiting spurious associations or hallucinations, and audiovisual input pairs guided by textual prompts. We also include a regularization term that penalizes reliance on text priors, thereby mitigating modality-specific cue hallucinations. Experimental results on DFEW, RAVDESS and EMER demonstrate that our method significantly improves the performance of the reference baseline models with 6-19% of relative performance gains in zero-shot settings. By providing both a rigorous benchmark and a robust optimization framework, this work enables principled evaluation and improvement of MLLMs for emotion understanding and social AI. Code, models and benchmark will be released at https://avere-iclr.github.io.

</details>


### [9] [TACIT: Transformation-Aware Capturing of Implicit Thought](https://arxiv.org/abs/2602.07061)
*Daniel Nobrega*

Main category: cs.LG

TL;DR: A diffusion transformer called TACIT solves mazes by transforming pixel images directly, revealing a sudden "eureka moment" where solutions emerge all at once rather than step-by-step, mimicking human insight.


<details>
  <summary>Details</summary>
Motivation: To explore visual reasoning that operates purely in pixel space (not language-based) to understand implicit thought processes that occur before and below language, and to create interpretable reasoning that can be directly visualized.

Method: Developed TACIT, a diffusion-based transformer using rectified flow matching that operates entirely in pixel space, trained on 1 million synthetic maze pairs to transform unsolved maze images into solutions, enabling step-by-step visualization of reasoning.

Result: Achieved 192x training loss reduction, 22.7x L2 improvement, and solved mazes in only 10 steps (vs. 100-1000 typical). Exhibited a phase transition: solutions invisible for 68% of transformation then emerged abruptly and simultaneously across all regions at t=0.70, suggesting holistic rather than algorithmic reasoning.

Conclusion: TACIT demonstrates neural networks can develop implicit reasoning strategies in pixel space that exhibit human-like "eureka moments," providing a foundation for understanding non-linguistic thought processes and creating interpretable visual reasoning systems.

Abstract: We present TACIT (Transformation-Aware Capturing of Implicit Thought), a diffusion-based transformer for interpretable visual reasoning. Unlike language-based reasoning systems, TACIT operates entirely in pixel space using rectified flow, enabling direct visualization of the reasoning process at each inference step. We demonstrate the approach on maze-solving, where the model learns to transform images of unsolved mazes into solutions. Key results on 1 million synthetic maze pairs include:
  - 192x reduction in training loss over 100 epochs
  - 22.7x improvement in L2 distance to ground truth
  - Only 10 Euler steps required (vs. 100-1000 for typical diffusion models)
  Quantitative analysis reveals a striking phase transition phenomenon: the solution remains invisible for 68% of the transformation (zero recall), then emerges abruptly at t=0.70 within just 2% of the process. Most remarkably, 100% of samples exhibit simultaneous emergence across all spatial regions, ruling out sequential path construction and providing evidence for holistic rather than algorithmic reasoning. This "eureka moment" pattern -- long incubation followed by sudden crystallization -- parallels insight phenomena in human cognition. The pixel-space design with noise-free flow matching provides a foundation for understanding how neural networks develop implicit reasoning strategies that operate below and before language.

</details>


### [10] [Video-based Music Generation](https://arxiv.org/abs/2602.07063)
*Serkan Sulun*

Main category: cs.LG

TL;DR: EMSYNC is an automatic video-to-music generation system that creates emotionally and rhythmically synchronized soundtracks by combining a novel video emotion classifier, continuous emotion-conditioned MIDI generation, and temporal boundary conditioning for scene alignment.


<details>
  <summary>Details</summary>
Motivation: As video content grows rapidly online, finding suitable soundtracks remains challenging for content creators. EMSYNC aims to provide a fast, free, automatic solution that generates tailored music without requiring composition or licensing.

Method: The model uses a frozen pretrained deep neural network video emotion classifier, a large-scale emotion-labeled MIDI dataset, continuous emotional value conditioning (not discrete categories) for nuanced music generation, and "boundary offset encodings" to align musical chords with scene changes.

Result: Achieves state-of-the-art results on Ekman-6 and MovieNet emotion classification datasets. User studies show it consistently outperforms existing methods in music richness, emotional alignment, temporal synchronization, and overall preference.

Conclusion: EMSYNC provides a fully automatic, end-to-end solution for video-based music generation that effectively addresses both emotional and temporal synchronization challenges, setting a new state-of-the-art.

Abstract: As the volume of video content on the internet grows rapidly, finding a suitable soundtrack remains a significant challenge. This thesis presents EMSYNC (EMotion and SYNChronization), a fast, free, and automatic solution that generates music tailored to the input video, enabling content creators to enhance their productions without composing or licensing music. Our model creates music that is emotionally and rhythmically synchronized with the video. A core component of EMSYNC is a novel video emotion classifier. By leveraging pretrained deep neural networks for feature extraction and keeping them frozen while training only fusion layers, we reduce computational complexity while improving accuracy. We show the generalization abilities of our method by obtaining state-of-the-art results on Ekman-6 and MovieNet. Another key contribution is a large-scale, emotion-labeled MIDI dataset for affective music generation. We then present an emotion-based MIDI generator, the first to condition on continuous emotional values rather than discrete categories, enabling nuanced music generation aligned with complex emotional content. To enhance temporal synchronization, we introduce a novel temporal boundary conditioning method, called "boundary offset encodings," aligning musical chords with scene changes. Combining video emotion classification, emotion-based music generation, and temporal boundary conditioning, EMSYNC emerges as a fully automatic video-based music generator. User studies show that it consistently outperforms existing methods in terms of music richness, emotional alignment, temporal synchronization, and overall preference, setting a new state-of-the-art in video-based music generation.

</details>


### [11] [Featured Reproducing Kernel Banach Spaces for Learning and Neural Networks](https://arxiv.org/abs/2602.07141)
*Isabel de la Higuera,Francisco Herrera,M. Victoria Velasco*

Main category: cs.LG

TL;DR: This paper extends kernel-based learning theory from Hilbert to Banach spaces by introducing featured reproducing kernel Banach spaces (FRKBS), enabling representer theorems and neural network analysis in non-Hilbertian settings.


<details>
  <summary>Details</summary>
Motivation: Modern neural networks with non-quadratic norms operate in non-Hilbertian (Banach) spaces where classical kernel learning frameworks fail, as continuity of evaluation functionals no longer guarantees feature maps or kernel formulations.

Method: Developed a functional-analytic framework using featured reproducing kernel Banach spaces (FRKBS) to establish structural conditions for feature maps, kernel constructions, and representer theorems in Banach spaces, formulating supervised learning as minimal-norm interpolation/regularization.

Result: Proved existence of solutions and conditional representer theorems for Banach space learning; showed fixed-architecture neural networks naturally induce vector-valued FRKBS, unifying kernel methods and neural networks under a function-space perspective.

Conclusion: The FRKBS framework bridges kernel-based learning and neural networks by precisely defining when kernel principles extend beyond Hilbert spaces, providing theoretical foundations for non-Hilbertian learning models.

Abstract: Reproducing kernel Hilbert spaces provide a foundational framework for kernel-based learning, where regularization and interpolation problems admit finite-dimensional solutions through classical representer theorems. Many modern learning models, however -- including fixed-architecture neural networks equipped with non-quadratic norms -- naturally give rise to non-Hilbertian geometries that fall outside this setting. In Banach spaces, continuity of point-evaluation functionals alone is insufficient to guarantee feature representations or kernel-based learning formulations. In this work, we develop a functional-analytic framework for learning in Banach spaces based on the notion of featured reproducing kernel Banach spaces. We identify the precise structural conditions under which feature maps, kernel constructions, and representer-type results can be recovered beyond the Hilbertian regime. Within this framework, supervised learning is formulated as a minimal-norm interpolation or regularization problem, and existence results together with conditional representer theorems are established. We further extend the theory to vector-valued featured reproducing kernel Banach spaces and show that fixed-architecture neural networks naturally induce special instances of such spaces. This provides a unified function-space perspective on kernel methods and neural networks and clarifies when kernel-based learning principles extend beyond reproducing kernel Hilbert spaces.

</details>


### [12] [BONSAI: Bayesian Optimization with Natural Simplicity and Interpretability](https://arxiv.org/abs/2602.07144)
*Samuel Daulton,David Eriksson,Maximilian Balandat,Eytan Bakshy*

Main category: cs.LG

TL;DR: BONSAI is a Bayesian optimization method that preserves default configurations while efficiently optimizing black-box functions, reducing spurious parameter changes without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: Standard Bayesian optimization ignores default parameter configurations, often making unnecessary changes to weakly relevant parameters, which increases vetting burden and makes it hard to identify truly important changes.

Method: Introduced BONSAI, a default-aware BO policy that prunes low-impact deviations from defaults while controlling acquisition value loss, compatible with multiple acquisition functions like expected improvement and GP-UCB.

Result: Theoretical analysis shows BONSAI maintains no-regret property like GP-UCB. Empirically, it significantly reduces non-default parameters in recommendations while maintaining competitive optimization performance and wall time.

Conclusion: BONSAI provides a practical solution for real-world optimization where default configurations exist, making BO recommendations more trustworthy and easier to vet.

Abstract: Bayesian optimization (BO) is a popular technique for sample-efficient optimization of black-box functions. In many applications, the parameters being tuned come with a carefully engineered default configuration, and practitioners only want to deviate from this default when necessary. Standard BO, however, does not aim to minimize deviation from the default and, in practice, often pushes weakly relevant parameters to the boundary of the search space. This makes it difficult to distinguish between important and spurious changes and increases the burden of vetting recommendations when the optimization objective omits relevant operational considerations. We introduce BONSAI, a default-aware BO policy that prunes low-impact deviations from a default configuration while explicitly controlling the loss in acquisition value. BONSAI is compatible with a variety of acquisition functions, including expected improvement and upper confidence bound (GP-UCB). We theoretically bound the regret incurred by BONSAI, showing that, under certain conditions, it enjoys the same no-regret property as vanilla GP-UCB. Across many real-world applications, we empirically find that BONSAI substantially reduces the number of non-default parameters in recommended configurations while maintaining competitive optimization performance, with little effect on wall time.

</details>


### [13] [Convex Dominance in Deep Learning I: A Scaling Law of Loss and Learning Rate](https://arxiv.org/abs/2602.07145)
*Zhiqi Bu,Shiyun Xu,Jialin Mao*

Main category: cs.LG

TL;DR: 深度学习优化在初期训练后迅速呈现弱凸特性，使损失动态可被精确预测，并建立了可在80倍训练时长和70倍模型尺寸范围内外推的学习率缩放规律。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习损失函数存在非凸性导致优化理论分析困难，但实证显示其动态常呈现类凸特性。该矛盾驱动了本研究探索凸性假设在深度学习中的适用边界，以实现优化过程的精确控制。

Method: 采用凸性和Lipschitz连续性理论框架，分析深度学习优化动态，通过建立最后迭代点的上界来推导学习率调度策略。

Result: 发现深度学习在短期训练后即呈现弱凸性；损失可由最后迭代点上界精确预测；基于此构建了可外推80倍训练时长和70倍模型尺寸的学习率与损失缩放规律。

Conclusion: 凸性理论为控制深度学习优化提供了有效视角，所建立的缩放规律证明了学习率调度在不同规模下具有显著的可扩展性和预测能力。

Abstract: Deep learning has non-convex loss landscape and its optimization dynamics is hard to analyze or control. Nevertheless, the dynamics can be empirically convex-like across various tasks, models, optimizers, hyperparameters, etc. In this work, we examine the applicability of convexity and Lipschitz continuity in deep learning, in order to precisely control the loss dynamics via the learning rate schedules. We illustrate that deep learning quickly becomes weakly convex after a short period of training, and the loss is predicable by an upper bound on the last iterate, which further informs the scaling of optimal learning rate. Through the lens of convexity, we build scaling laws of learning rates and losses that extrapolate as much as 80X across training horizons and 70X across model sizes.

</details>


### [14] [On Randomness in Agentic Evals](https://arxiv.org/abs/2602.07150)
*Bjarni Haukur Bjarnason,André Silva,Martin Monperrus*

Main category: cs.LG

TL;DR: 该研究质疑智能体系统单轮评估的可靠性，通过6万条轨迹实验发现单轮pass@1分数存在2.2-6.0个百分点的方差，即使温度0时标准差也超过1.5个百分点。研究表明早期token差异会级联影响最终策略，2-3个百分点的性能提升可能只是评估噪声而非真实进展。


<details>
  <summary>Details</summary>
Motivation: 现有文献普遍基于单轮运行计算pass@1分数来评估智能体系统性能，但这一假设的可靠性尚未被验证。这种评估方式可能混淆统计噪声与真实的算法改进。

Method: 在SWE-Bench-Verified基准上收集60,000条智能体轨迹，涵盖3个模型和2种框架。通过统计分析测量单轮评估的方差，并进行token级分析追踪轨迹早期分化。

Result: 单轮pass@1估计值存在显著方差（2.2-6.0个百分点），温度0时标准差仍超过1.5个百分点。轨迹在前几个百分比的token内就出现分化，并级联导致不同解决方案。报告的2-3个百分点改进可能反映评估噪声而非算法进步。

Conclusion: 为保证评估可靠性，建议：(1) 对每项任务进行多次独立运行；(2) 使用统计功效分析确定检测预期效应所需的运行次数；(3) 采用pass@k和pass^k等k>1的指标来刻画完整性能区间。这些做法虽增加评估成本，但对区分真实科学进步与统计噪声至关重要。

Abstract: Agentic systems are evaluated on benchmarks where agents interact with environments to solve tasks. Most papers report a pass@1 score computed from a single run per task, assuming this gives a reliable performance estimate. We test this assumption by collecting 60,000 agentic trajectories on SWE-Bench-Verified, spanning three models and two scaffolds. We find substantial variance: single-run pass@1 estimates vary by 2.2 to 6.0 percentage points depending on which run is selected, with standard deviations exceeding 1.5 percentage points even at temperature 0. This variance has critical implications: reported improvements of 2--3 percentage points may reflect evaluation noise rather than genuine algorithmic progress. Through token-level analysis, we show that trajectories diverge early, often within the first few percent of tokens, and that these small differences cascade into different solution strategies. To enable reliable evaluation of agentic systems, we recommend three concrete practices: (1) estimate pass@1 from multiple independent runs per task, especially when measuring small improvements, (2) use statistical power analysis to determine the number of runs needed to detect expected effect sizes, and (3) consider metrics like pass@k (optimistic bound) and pass^k (pessimistic bound) with k>1 to better characterize the full performance envelope. While these practices increase evaluation cost, they are essential for distinguishing genuine scientific progress from statistical noise.

</details>


### [15] [Beyond Pooling: Matching for Robust Generalization under Data Heterogeneity](https://arxiv.org/abs/2602.07154)
*Ayush Roy,Rudrasis Chakraborty,Lav Varshney,Vishnu Suresh Lokhande*

Main category: cs.LG

TL;DR: 提出一种自适应质心匹配框架，通过迭代优化表示分布和倾向性得分匹配，有效过滤混杂域，在零样本医疗异常检测等极端异质性场景下显著优于朴素池化方法


<details>
  <summary>Details</summary>
Motivation: 异构数据集池化虽常用，但朴素池化会放大分布不对称性并导致有偏估计，尤其影响零样本泛化性能；混杂域是异质性的主要原因，需要更稳健的样本选择策略

Method: 提出匹配框架：基于自适应质心选择样本，迭代优化表示分布；结合双重稳健性和倾向性得分匹配来过滤混杂域，减少域间异质性

Result: 理论及实验证明，在非对称元分布、非高斯及多峰真实场景中，匹配方法优于朴素池化和均匀子采样；在零样本医疗异常检测这一极端异质性任务上提升显著

Conclusion: 匹配框架比传统池化更稳健，能有效处理数据异质性和分布不对称问题，为医疗异常检测等零样本学习任务提供更可靠的表示学习方案

Abstract: Pooling heterogeneous datasets across domains is a common strategy in representation learning, but naive pooling can amplify distributional asymmetries and yield biased estimators, especially in settings where zero-shot generalization is required. We propose a matching framework that selects samples relative to an adaptive centroid and iteratively refines the representation distribution. The double robustness and the propensity score matching for the inclusion of data domains make matching more robust than naive pooling and uniform subsampling by filtering out the confounding domains (the main cause of heterogeneity). Theoretical and empirical analyses show that, unlike naive pooling or uniform subsampling, matching achieves better results under asymmetric meta-distributions, which are also extended to non-Gaussian and multimodal real-world settings. Most importantly, we show that these improvements translate to zero-shot medical anomaly detection, one of the extreme forms of data heterogeneity and asymmetry. The code is available on https://github.com/AyushRoy2001/Beyond-Pooling.

</details>


### [16] [Mimetic Initialization of MLPs](https://arxiv.org/abs/2602.07156)
*Asher Trockman,J. Zico Kolter*

Main category: cs.LG

TL;DR: 首次将模仿初始化方法应用于MLP通道混合层，通过首层非零均值设计加速小视觉任务训练


<details>
  <summary>Details</summary>
Motivation: 现有模仿初始化仅针对空间混合层（如卷积层），尚未探索通道混合层（如MLP）的初始化优化，存在研究空白

Method: 对MLP首层权重施加非零均值，设计极简初始化策略，结合预训练模型结构观察进行启发

Result: 在CIFAR-10/ImageNet-1k等小视觉任务中显著加速训练，效果弱于空间混合层初始化但可协同增强

Conclusion: 证明了通道混合层初始化的可行性，提供与现有方法兼容的轻量级优化方案

Abstract: Mimetic initialization uses pretrained models as case studies of good initialization, using observations of structures in trained weights to inspire new, simple initialization techniques. So far, it has been applied only to spatial mixing layers, such convolutional, self-attention, and state space layers. In this work, we present the first attempt to apply the method to channel mixing layers, namely multilayer perceptrons (MLPs). Our extremely simple technique for MLPs -- to give the first layer a nonzero mean -- speeds up training on small-scale vision tasks like CIFAR-10 and ImageNet-1k. Though its effect is much smaller than spatial mixing initializations, it can be used in conjunction with them for an additional positive effect.

</details>


### [17] [Thermodynamic Isomorphism of Transformers: A Lagrangian Approach to Attention Dynamics](https://arxiv.org/abs/2602.08216)
*Gunn Kim*

Main category: cs.LG

TL;DR: 该论文提出了一种基于第一性原理的物理理论框架，将Transformer注意力机制视为遵循最小作用量原理的物理系统。通过Fisher信息度量的黎曼流形映射，推导出"智能拉格朗日量"，证明softmax对应热力学平衡态，建立信息热力学第一定律，并将缩放定律、grokking等现象解释为相变，为统一统计物理与深度学习奠定基础。


<details>
  <summary>Details</summary>
Motivation: Transformer架构虽已革新人工智能领域，但其底层机制仍缺乏统一的物理解释，多为启发式设计，亟需从第一性原理出发建立理论框架。

Method: 提出信息动力学第一性原理框架，将信息状态映射到具有Fisher信息度量的黎曼流形，推导出智能拉格朗日量，并运用热力学和场论方法分析注意力机制。

Result: 证明softmax函数对应于最小化信息气体Helmholtz自由能的热力学平衡态；识别query-key交互为外场与内禀偶极矩的电磁耦合；建立信息热力学第一定律统一推理与学习；将缩放定律和grokking等涌现现象解释为比热发散的特征相变；揭示注意力流形中的旋转对称性破缺产生无质量Goldstone玻色子，为RoPE提供场论视角。

Conclusion: 该工作成功连接统计物理与深度学习，建立了基于物理的智能理论框架，为理解Transformer机制提供了全新视角，并为未来AI模型设计奠定理论基础。

Abstract: Although the Transformer architecture has revolutionized artificial intelligence, its underlying mechanisms remain largely heuristic and lack a unified physical theory. In this work, we propose a first-principles framework for information dynamics, treating the attention mechanism as a physical system governed by the principle of least action rather than as an algorithmic optimization. By mapping information states to a Riemannian manifold with the Fisher information metric, we derive the intelligence Lagrangian. We show that the softmax function corresponds to the unique thermodynamic equilibrium state that minimizes the Helmholtz free energy of the information gas. In addition, we identify the query-key interaction as an electrodynamic coupling between an external field and an intrinsic dipole moment. This theory establishes the first law of information thermodynamics, unifying inference (mechanical work) and learning (chemical evolution). It also explains emergent phenomena, such as scaling laws and grokking, as phase transitions characterized by the divergence of specific heat. Finally, we discuss how rotational symmetry breaking in the attention manifold generates massless Goldstone bosons, providing a field-theoretic perspective on rotary positional embeddings (RoPE). Our work connects Statistical Physics and Deep Learning, laying the groundwork for a general theory of physics-based intelligence.

</details>


### [18] [Learning Nonlinear Systems In-Context: From Synthetic Data to Real-World Motor Control](https://arxiv.org/abs/2602.07173)
*Tong Jian,Tianyu Dai,Tao Yu*

Main category: cs.LG

TL;DR: 首次将大语言模型中的上下文学习（ICL）能力引入电机前馈控制领域，提出一种分离信号表示与系统行为的Transformer架构，通过合成数据预训练实现真实电机系统的小样本自适应，显著超越传统PI控制和物理模型方法。


<details>
  <summary>Details</summary>
Motivation: 传统电机前馈控制方法（如PI控制器和物理模型）在应对非线性负载和复杂工况时存在局限性，而大语言模型的上下文学习能力尚未应用于信号处理系统，存在技术迁移空白。

Method: 设计双路径Transformer架构：① 分离信号表示与系统行为建模 ② 使用合成线性和非线性系统数据进行大规模预训练 ③ 支持少样本微调和单样本ICL适应真实电机系统。

Result: 在多种电机负载配置下实现良好泛化：仅需少量真实样本即可生成精确前馈控制信号，性能显著优于PI控制器和物理基线方法，成功将未调优示例转化为有效预测。

Conclusion: 证明ICL能有效桥接合成数据预训练与真实物理系统适应，为数据高效的物理系统控制开辟新方向，拓展了大模型在工业控制领域的应用潜力。

Abstract: LLMs have shown strong in-context learning (ICL) abilities, but have not yet been extended to signal processing systems. Inspired by their design, we have proposed for the first time ICL using transformer models applicable to motor feedforward control, a critical task where classical PI and physics-based methods struggle with nonlinearities and complex load conditions. We propose a transformer based model architecture that separates signal representation from system behavior, enabling both few-shot finetuning and one-shot ICL. Pretrained on a large corpus of synthetic linear and nonlinear systems, the model learns to generalize to unseen system dynamics of real-world motors only with a handful of examples. In experiments, our approach generalizes across multiple motor load configurations, transforms untuned examples into accurate feedforward predictions, and outperforms PI controllers and physics-based feedforward baselines. These results demonstrate that ICL can bridge synthetic pretraining and real-world adaptability, opening new directions for data efficient control of physical systems.

</details>


### [19] [Risk-Sensitive Exponential Actor Critic](https://arxiv.org/abs/2602.07202)
*Alonso Granados,Jason Pacheco*

Main category: cs.LG

TL;DR: 提出风险敏感指数演员-评论员算法(rsEAC)，通过理论推导和避免指数价值函数显式表示，解决现有风险敏感强化学习在连续任务中数值不稳定和高方差问题


<details>
  <summary>Details</summary>
Motivation: 现有无模型深度强化学习在真实应用中存在安全风险，而基于熵风险度量优化策略的方法存在高方差、数值不稳定问题，仅适用于简单任务，难以扩展到连续复杂场景

Method: 提供策略梯度方法在熵风险度量上的全面理论证明，提出风险敏感指数演员-评论家(rsEAC)算法，通过创新设计避免指数价值函数及其梯度的显式计算，实现稳定优化

Result: rsEAC相比现有方法产生更稳定的更新，在MuJoCo的连续风险任务中可靠地学习到风险敏感策略

Conclusion: 该工作为风险敏感强化学习提供了坚实理论基础和实用算法，成功将熵风险度量优化扩展到挑战性连续控制任务

Abstract: Model-free deep reinforcement learning (RL) algorithms have achieved tremendous success on a range of challenging tasks. However, safety concerns remain when these methods are deployed on real-world applications, necessitating risk-aware agents. A common utility for learning such risk-aware agents is the entropic risk measure, but current policy gradient methods optimizing this measure must perform high-variance and numerically unstable updates. As a result, existing risk-sensitive model-free approaches are limited to simple tasks and tabular settings. In this paper, we provide a comprehensive theoretical justification for policy gradient methods on the entropic risk measure, including on- and off-policy gradient theorems for the stochastic and deterministic policy settings. Motivated by theory, we propose risk-sensitive exponential actor-critic (rsEAC), an off-policy model-free approach that incorporates novel procedures to avoid the explicit representation of exponential value functions and their gradients, and optimizes its policy w.r.t the entropic risk measure. We show that rsEAC produces more numerically stable updates compared to existing approaches and reliably learns risk-sensitive policies in challenging risky variants of continuous tasks in MuJoCo.

</details>


### [20] [Exactly Computing do-Shapley Values](https://arxiv.org/abs/2602.07203)
*R. Teal Witter,Álvaro Parafita,Tomas Garriga,Maximilian Muschalik,Fabian Fumagalli,Axel Brando,Lucas Rosenblatt*

Main category: cs.LG

TL;DR: This paper reformulates do-Shapley values using irreducible sets in Structural Causal Models, enabling exact linear-time computation in the number of irreducible sets r, and providing an estimator that achieves higher accuracy than prior methods with the same query budget while reducing identifiability requirements.


<details>
  <summary>Details</summary>
Motivation: Computing do-Shapley values for Structural Causal Models is computationally intractable (exponential complexity) and requires identifying numerous interventional effects, which is a significant burden.

Method: The authors reformulate do-Shapley values based on irreducible sets of SCMs, leading to an exact linear-time algorithm in terms of irreducible set count r, and complement it with a query-budget-aware estimator.

Result: The exact algorithm runs in O(r) time where r ∈ [d, 2^d]; the estimator achieves orders-of-magnitude higher accuracy than prior methods as budget approaches r, and attains machine precision when budget reaches r; identification burden is reduced to only d singleton interventions instead of all coalition classes.

Conclusion: This irreducible set-based framework fundamentally improves the computational efficiency and practical feasibility of do-Shapley value calculation in causal models, making it more applicable to real-world problems.

Abstract: Structural Causal Models (SCM) are a powerful framework for describing complicated dynamics across the natural sciences. A particularly elegant way of interpreting SCMs is do-Shapley, a game-theoretic method of quantifying the average effect of $d$ variables across exponentially many interventions. Like Shapley values, computing do-Shapley values generally requires evaluating exponentially many terms. The foundation of our work is a reformulation of do-Shapley values in terms of the irreducible sets of the underlying SCM. Leveraging this insight, we can exactly compute do-Shapley values in time linear in the number of irreducible sets $r$, which itself can range from $d$ to $2^d$ depending on the graph structure of the SCM. Since $r$ is unknown a priori, we complement the exact algorithm with an estimator that, like general Shapley value estimators, can be run with any query budget. As the query budget approaches $r$, our estimators can produce more accurate estimates than prior methods by several orders of magnitude, and, when the budget reaches $r$, return the Shapley values up to machine precision. Beyond computational speed, we also reduce the identification burden: we prove that non-parametric identifiability of do-Shapley values requires only the identification of interventional effects for the $d$ singleton coalitions, rather than all classes.

</details>


### [21] [Online Learning for Uninformed Markov Games: Empirical Nash-Value Regret and Non-Stationarity Adaptation](https://arxiv.org/abs/2602.07205)
*Junyan Liu,Haipeng Luo,Zihan Zhang,Lillian J. Ratliff*

Main category: cs.LG

TL;DR: 本文针对两人非知情马尔可夫博弈，提出一种新的遗憾度量和无参数算法，实现从固定对手时的O(√K)到最坏情况O(K^(2/3))的自适应遗憾界。


<details>
  <summary>Details</summary>
Motivation: 前人工作证明了非知情马尔可夫博弈中无外部遗憾是不可能的，其V-learning算法仅达到O(K^(2/3))纳什值遗憾，且无法适应问题难度——即使对手策略固定时也达不到最优的O(√K)外部遗憾。

Method: 1) 引入经验纳什值遗憾新度量；2) 重新分析Mao等(2022)的基于时期的V-learning算法；3) 设计自适应重启机制应对对手非平稳性。

Result: 获得O(min{√K + (CK)^(1/3), √(LK)})遗憾界，其中C为对手策略方差，L为策略切换次数。该结果覆盖了从固定对手到最坏情况的两极，并实现平滑插值。

Conclusion: 通过新遗憾度量和自适应算法，完全解决了先前工作的局限性，使算法能根据对手行为自动调整，在各种场景下均达到最优性能。

Abstract: We study online learning in two-player uninformed Markov games, where the opponent's actions and policies are unobserved. In this setting, Tian et al. (2021) show that achieving no-external-regret is impossible without incurring an exponential dependence on the episode length $H$. They then turn to the weaker notion of Nash-value regret and propose a V-learning algorithm with regret $O(K^{2/3})$ after $K$ episodes. However, their algorithm and guarantee do not adapt to the difficulty of the problem: even in the case where the opponent follows a fixed policy and thus $O(\sqrt{K})$ external regret is well-known to be achievable, their result is still the worse rate $O(K^{2/3})$ on a weaker metric.
  In this work, we fully address both limitations. First, we introduce empirical Nash-value regret, a new regret notion that is strictly stronger than Nash-value regret and naturally reduces to external regret when the opponent follows a fixed policy. Moreover, under this new metric, we propose a parameter-free algorithm that achieves an $O(\min \{\sqrt{K} + (CK)^{1/3},\sqrt{LK}\})$ regret bound, where $C$ quantifies the variance of the opponent's policies and $L$ denotes the number of policy switches (both at most $O(K)$). Therefore, our results not only recover the two extremes -- $O(\sqrt{K})$ external regret when the opponent is fixed and $O(K^{2/3})$ Nash-value regret in the worst case -- but also smoothly interpolate between these extremes by automatically adapting to the opponent's non-stationarity. We achieve so by first providing a new analysis of the epoch-based V-learning algorithm by Mao et al. (2022), establishing an $O(ηC + \sqrt{K/η})$ regret bound, where $η$ is the epoch incremental factor. Next, we show how to adaptively restart this algorithm with an appropriate $η$ in response to the potential non-stationarity of the opponent, eventually achieving our final results.

</details>


### [22] [DSL: Understanding and Improving Softmax Recommender Systems with Competition-Aware Scaling](https://arxiv.org/abs/2602.07206)
*Bucher Sahyouni,Matthew Vowels,Liqun Chen,Simon Hadfield*

Main category: cs.LG

TL;DR: 针对推荐系统中Softmax Loss在隐式反馈下因全局温度和均匀负采样导致训练脆弱的问题，提出Dual-scale Softmax Loss (DSL)，通过动态重加权负样本和自适应温度机制优化损失锐度，在多个基准测试中显著超越基线模型（平均提升6.22%），并在分布外数据下表现更优。


<details>
  <summary>Details</summary>
Motivation: 隐式反馈推荐中，Softmax Loss使用单一全局温度和均匀负采样会导致训练不稳定，因为不同负样本集合包含不同相关性的竞争项，固定损失锐度对某些用户-项目对可能次优或破坏训练稳定性。

Method: DSL通过两个互补分支改进Softmax Loss：1) 基于负样本难度和项目相似度重加权每个训练实例中的负样本；2) 根据构造的竞争集合的竞争强度自适应调整每个示例的温度参数，从而重塑负样本分布。

Result: 在多个基准数据集和骨干模型上，DSL相比强基线显著提升性能，相比Softmax Loss平均提升6.22%，部分设置下超过10%；在分布外流行度偏移下，平均提升达9.31%。

Conclusion: DSL通过动态调整负样本权重和温度参数，有效解决了Softmax Loss的锐度适配问题，提升了推荐系统的准确性和鲁棒性，理论分析（分布鲁棒优化）也证实了其对模糊实例的优化机制。

Abstract: Softmax Loss (SL) is being increasingly adopted for recommender systems (RS) as it has demonstrated better performance, robustness and fairness. Yet in implicit-feedback, a single global temperature and equal treatment of uniformly sampled negatives can lead to brittle training, because sampled sets may contain varying degrees of relevant or informative competitors. The optimal loss sharpness for a user-item pair with a particular set of negatives, can be suboptimal or destabilising for another with different negatives. We introduce Dual-scale Softmax Loss (DSL), which infers effective sharpness from the sampled competition itself. DSL adds two complementary branches to the log-sum-exp backbone. Firstly it reweights negatives within each training instance using hardness and item--item similarity, secondly it adapts a per-example temperature from the competition intensity over a constructed competitor slate. Together, these components preserve the geometry of SL while reshaping the competition distribution across negatives and across examples.
  Over several representative benchmarks and backbones, DSL yields substantial gains over strong baselines, with improvements over SL exceeding $10%$ in several settings and averaging $6.22%$ across datasets, metrics, and backbones. Under out-of-distribution (OOD) popularity shift, the gains are larger, with an average of $9.31%$ improvement over SL. We further provide a theoretical, distributionally robust optimisation (DRO) analysis, which demonstrates how DSL reshapes the robust payoff and the KL deviation for ambiguous instances. This helps explain the empirically observed improvements in accuracy and robustness.

</details>


### [23] [Probing Neural TSP Representations for Prescriptive Decision Support](https://arxiv.org/abs/2602.07216)
*Reuben Narad,Léonard Boussioux,Michael Wagner*

Main category: cs.LG

TL;DR: 本研究探索了神经组合优化（NCO）模型（如TSP求解器）作为可迁移编码器，用于解决物流场景中的预设性下游任务（节点移除敏感性和边禁止敏感性），首次证明了TSP求解器内部表征可迁移至非路径构建的决策支持任务，且迁移效果随模型质量提升而增强。


<details>
  <summary>Details</summary>
Motivation: 现有神经组合优化研究聚焦于直接生成优质解（如TSP路径），但忽略了训练后的模型是否蕴含可迁移的优化相关表征。本研究受其他领域迁移学习启发，首次提出将TSP求解器作为编码器，用于解决物流决策中的"假设分析"任务（如识别关键节点/边），以拓展NCO模型的实际应用价值。

Method: 1. 训练多种基于注意力的TSP求解策略（如TSP100模型）；2. 提取模型内部节点/边嵌入表征；3. 训练线性探针（probes）将上述嵌入映射至两个下游任务：节点移除敏感性（识别移除后影响最大的节点）和边禁止敏感性（识别保留最关键的边）；4. 将探针信号与几何特征集成以提升性能。

Result: 1. TSP100模型的探针在下游任务中表现接近现有基线；2. 集成探针信号与几何特征后显著超越最强基线：节点移除任务Top-1准确率达65%（基线58%），边禁止任务达73%（基线67%）；3. 迁移准确率随求解器质量提升和模型规模扩大而提高。

Conclusion: 神经TSP求解器可作为有效的可迁移编码器，支持物流决策中的预设性分析任务。研究首次验证了NCO模型超越路径构建的表征迁移能力，表明训练更强的求解器可同时提升主任务性能和下游任务表征质量，为NCO应用开辟新方向。

Abstract: The field of neural combinatorial optimization (NCO) trains neural policies to solve NP-hard problems such as the traveling salesperson problem (TSP). We ask whether, beyond producing good tours, a trained TSP solver learns internal representations that transfer to other optimization-relevant objectives, in the spirit of transfer learning from other domains. We train several attention-based TSP policies, collect their internal activations, and train probes on node/edge embeddings for two NP-hard prescriptive downstream tasks inspired by real-world logistics scenarios: node-removal sensitivity (identifying the most impactful node to remove) and edge-forbid sensitivity (identifying the most critical edge to retain). On a Euclidean TSP100-trained model, probes for both tasks are competitive with existing baselines. Ensembling probe signals with geometric features outperforms the strongest baselines: 65\% top-1 accuracy (vs. 58\% baseline) for the best-node-removal task, and 73\% top-1 accuracy (vs. 67\% baseline) for the worst-edge identification task. To our knowledge, we are the first to study neural TSP solvers as transferable encoders for prescriptive what-if decision-support objectives beyond tour construction. Finally, we show that transfer accuracy increases with solver quality across training and model scale, suggesting that training stronger NCO solvers also yields more useful encoders for downstream objectives. Our code is available at: github.com/ReubenNarad/tsp_prescriptive_probe

</details>


### [24] [Collaborative and Efficient Fine-tuning: Leveraging Task Similarity](https://arxiv.org/abs/2602.07218)
*Gagik Magakyan,Amirhossein Reisizadeh,Chanwoo Park,Pablo A. Parrilo,Asuman Ozdaglar*

Main category: cs.LG

TL;DR: 本文提出CoLoRA（协作式低秩适应），通过利用多用户间的任务相似性来缓解基础模型微调中的数据稀缺问题，采用一个共享适配器捕捉任务共性，同时保留个性化适配器捕捉用户特定任务特征，并提供了理论保证和实验性能提升。


<details>
  <summary>Details</summary>
Motivation: 基础模型微调中使用LoRA等参数高效方法时面临标记、高质量且普遍稀缺的任务数据不足的问题。

Method: 协作式低秩适应（CoLoRA），训练一个共享适配器捕捉所有任务的底层相似性，以及个性化适配器针对特定用户任务。

Result: 在异构线性回归上提供了可证明的真理恢复保证，并在不同任务相似性的自然语言实验中显示，当与相似任务共同训练时，个体性能显著提升。

Conclusion: CoLoRA通过协作方式有效利用任务相似性高效微调个性化基础模型，显著提升个体性能。

Abstract: Adaptability has been regarded as a central feature in the foundation models, enabling them to effectively acclimate to unseen downstream tasks. Parameter-efficient fine-tuning methods such as celebrated LoRA facilitate efficient adaptation of large foundation models using labeled, high-quality and generally scarce task data. To mitigate data scarcity in fine-tuning of foundation models, we propose to leverage task similarity across multiple downstream users. Intuitively, users with similar tasks must be able to assist each other in boosting the effective fine-tuning data size. We propose Collaborative Low-Rank Adaptation, or CoLoRA, which exploits task similarity to collaboratively and efficiently fine-tune personalized foundation models. The main idea in CoLoRA is to train one shared adapter capturing underlying task similarities across all tasks, and personalized adapters tailored to user-specific tasks. We theoretically study CoLoRA on heterogeneous linear regression and provide provable guarantees for ground truth recovery. We also conduct several natural language experiments with varying task similarity, which further demonstrate that when trained together with similar tasks, individual performances are significantly boosted.

</details>


### [25] [SpecAttn: Co-Designing Sparse Attention with Self-Speculative Decoding](https://arxiv.org/abs/2602.07223)
*Yikang Yue,Yuqi Xue,Jian Huang*

Main category: cs.LG

TL;DR: 提出SpecAttn方法，通过验证过程自动识别关键KV缓存条目来指导推测解码，实现2.81倍于自回归解码和1.29倍于现有稀疏推测解码方法的吞吐量提升


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM推理的KV缓存内存需求成为性能瓶颈，现有稀疏推测解码方法依赖独立KV选择算法，未能利用验证过程中已计算的关键性信息

Method: SpecAttn利用验证阶段作为稀疏注意力的副产品自动识别关键KV条目，在后续token drafting时仅加载这些关键条目，实现验证引导的稀疏注意力

Result: 相比普通自回归解码提升2.81倍吞吐量，相比最先进的稀疏推测解码方法提升1.29倍，同时提高draft token接受率和降低KV选择开销

Conclusion: 通过将验证阶段的关键性计算复用为KV选择机制，SpecAttn实现了低开销、高接受率的推测解码，显著提升了长上下文LLM推理效率

Abstract: Long-context large language model (LLM) inference has become the norm for today's AI applications. However, it is severely bottlenecked by the increasing memory demands of its KV cache. Previous works have shown that self-speculative decoding with sparse attention, where tokens are drafted using a subset of the KV cache and verified in parallel with full KV cache, speeds up inference in a lossless way. However, this approach relies on standalone KV selection algorithms to select the KV entries used for drafting and overlooks that the criticality of each KV entry is inherently computed during verification. In this paper, we propose SpecAttn, a self-speculative decoding method with verification-guided sparse attention. SpecAttn identifies critical KV entries as a byproduct of verification and only loads these entries when drafting subsequent tokens. This not only improves draft token acceptance rate but also incurs low KV selection overhead, thereby improving decoding throughput. SpecAttn achieves 2.81$\times$ higher throughput over vanilla auto-regressive decoding and 1.29$\times$ improvement over state-of-the-art sparsity-based self-speculative decoding methods.

</details>


### [26] [Fault-Tolerant Evaluation for Sample-Efficient Model Performance Estimators](https://arxiv.org/abs/2602.07226)
*Zihan Zhu,Yanqiu Wu,Qiongkai Xu*

Main category: cs.LG

TL;DR: 该论文提出一种容错评估框架，通过可调容差ε整合偏差与方差考量，解决低方差场景下现有评估方法（RMSE混淆偏差方差、p值检验过于敏感）失效的问题，并设计算法自动优化ε，在真实数据集上验证了其可靠性。


<details>
  <summary>Details</summary>
Motivation: 在模型即服务时代，第三方AI模型的快速部署需求与动态演化的应用环境、新数据集及大量性能声明之间的矛盾，使得模型服务的高效可靠验证成为挑战。现有样本高效性能评估器在低方差场景下存在评估失效：RMSE混淆偏差与方差，p值检验对微小偏差过于敏感。

Method: 提出容错评估框架，通过可调容差ε在可接受误差范围内评估性能评估器；理论证明ε的合理校准可确保不同方差 regime 下的可靠评估；设计算法自动优化并选择ε。

Result: 在真实世界数据集上的实验表明，该框架能够提供关于评估器行为的全面且可操作的洞察。

Conclusion: 所提框架能够有效解决低方差设置下的评估难题，为性能评估器的验证提供了实用、可靠的解决方案。

Abstract: In the era of Model-as-a-Service, organizations increasingly rely on third-party AI models for rapid deployment. However, the dynamic nature of emerging AI applications, the continual introduction of new datasets, and the growing number of models claiming superior performance make efficient and reliable validation of model services increasingly challenging. This motivates the development of sample-efficient performance estimators, which aim to estimate model performance by strategically selecting instances for labeling, thereby reducing annotation cost. Yet existing evaluation approaches often fail in low-variance settings: RMSE conflates bias and variance, masking persistent bias when variance is small, while p-value based tests become hypersensitive, rejecting adequate estimators for negligible deviations. To address this, we propose a fault-tolerant evaluation framework that integrates bias and variance considerations within an adjustable tolerance level ${\varepsilon}$, enabling the evaluation of performance estimators within practically acceptable error margins. We theoretically show that proper calibration of ${\varepsilon}$ ensures reliable evaluation across different variance regimes, and we further propose an algorithm that automatically optimizes and selects ${\varepsilon}$. Experiments on real-world datasets demonstrate that our framework provides comprehensive and actionable insights into estimator behavior.

</details>


### [27] [Cerebellar-Inspired Residual Control for Fault Recovery: From Inference-Time Adaptation to Structural Consolidation](https://arxiv.org/abs/2602.07227)
*Nethmi Jayasinghe,Diana Gontero,Spencer T. Brown,Vinod K. Sangwan,Mark C. Hersam,Amit Ranjan Trivedi*

Main category: cs.LG

TL;DR: 提出一种推理时小脑启发的残差控制框架，在不修改冻结RL策略参数的前提下，通过在线校正动作实现训练后故障恢复，在MuJoCo基准测试中显著提升机器人性能。


<details>
  <summary>Details</summary>
Motivation: 真实世界部署的机器人策略常遭遇训练后故障，但重训练、探索或系统辨识均不切实际，亟需一种无需修改基策略参数的在线容错方法。

Method: 设计受小脑启发的残差控制框架：采用固定特征扩展实现高维模式分离，并行微区残差通路，以及具有兴奋性和抑制性资格迹的局部误差驱动可塑性；通过性能驱动的元适应机制保守地调节残差权值和塑性。

Result: 在MuJoCo基准测试中，中等故障下性能提升达+66%（HalfCheetah-v5）和+53%（Humanoid-v5），严重扰动时表现优雅退化，并将持续残差校正整合到策略参数中以获得额外鲁棒性。

Conclusion: 该框架实现了快速局部校正与全局策略稳定的平衡，为小脑计算原理在机器人容错控制中的应用提供了有效范式，支持在线故障恢复与策略巩固的双重目标。

Abstract: Robotic policies deployed in real-world environments often encounter post-training faults, where retraining, exploration, or system identification are impractical. We introduce an inference-time, cerebellar-inspired residual control framework that augments a frozen reinforcement learning policy with online corrective actions, enabling fault recovery without modifying base policy parameters. The framework instantiates core cerebellar principles, including high-dimensional pattern separation via fixed feature expansion, parallel microzone-style residual pathways, and local error-driven plasticity with excitatory and inhibitory eligibility traces operating at distinct time scales. These mechanisms enable fast, localized correction under post-training disturbances while avoiding destabilizing global policy updates. A conservative, performance-driven meta-adaptation regulates residual authority and plasticity, preserving nominal behavior and suppressing unnecessary intervention. Experiments on MuJoCo benchmarks under actuator, dynamic, and environmental perturbations show improvements of up to $+66\%$ on \texttt{HalfCheetah-v5} and $+53\%$ on \texttt{Humanoid-v5} under moderate faults, with graceful degradation under severe shifts and complementary robustness from consolidating persistent residual corrections into policy parameters.

</details>


### [28] [ArcMark: Multi-bit LLM Watermark via Optimal Transport](https://arxiv.org/abs/2602.07235)
*Atefeh Gilani,Carol Xuan Long,Sajani Vithana,Oliver Kosut,Lalitha Sankar,Flavio P. Calmon*

Main category: cs.LG

TL;DR: 该论文首次揭示了多比特语言模型水印的信息论容量，并提出基于编码理论的ArcMark方案，在特定假设下达到容量上限，实测比特率和检测精度均优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有多比特水印缺乏信息论容量认知，设计多沿用零比特思路（如每token单比特编码），限制了水印性能的理论上限和实际效果

Method: 通过信息论分析推导多比特水印容量公式，并基于编码理论构建ArcMark方案，使其在特定条件下达到理论容量

Result: 1. 首次明确多比特水印容量上限 2. ArcMark在比特率/检测精度上超越竞品 3. 验证编码理论框架的有效性

Conclusion: 语言模型水印本质是信道编码问题，该研究为水印设计建立了理论根基，推动编码理论在AI安全领域的应用

Abstract: Watermarking is an important tool for promoting the responsible use of language models (LMs). Existing watermarks insert a signal into generated tokens that either flags LM-generated text (zero-bit watermarking) or encodes more complex messages (multi-bit watermarking). Though a number of recent multi-bit watermarks insert several bits into text without perturbing average next-token predictions, they largely extend design principles from the zero-bit setting, such as encoding a single bit per token. Notably, the information-theoretic capacity of multi-bit watermarking -- the maximum number of bits per token that can be inserted and detected without changing average next-token predictions -- has remained unknown. We address this gap by deriving the first capacity characterization of multi-bit watermarks. Our results inform the design of ArcMark: a new watermark construction based on coding-theoretic principles that, under certain assumptions, achieves the capacity of the multi-bit watermark channel. In practice, ArcMark outperforms competing multi-bit watermarks in terms of bit rate per token and detection accuracy. Our work demonstrates that LM watermarking is fundamentally a channel coding problem, paving the way for principled coding-theoretic approaches to watermark design.

</details>


### [29] [Robust Ultra-High-Dimensional Variable Selection With Correlated Structure Using Group Testing](https://arxiv.org/abs/2602.07258)
*Wanru Guo,Juan Xie,Binbin Wang,Weicong Chen,Xiaoyi Lu,Vipin Chaudhary,Curtis Tatsuoka*

Main category: cs.LG

TL;DR: 提出Dorfman筛选框架处理高维基因组特征选择问题，通过数据驱动分组和假设检验，结合弹性网惩罚与稳健方法，在模拟和实际数据中均表现优异


<details>
  <summary>Details</summary>
Motivation: 高维基因组数据存在强组相关结构，传统特征选择方法通常假设特征独立或依赖预定义通路，且对异常值与模型误设敏感，难以满足现代组学分析需求

Method: 构建多阶段Dorfman筛选框架：基于层次聚类形成数据驱动变量组，执行组水平与组内假设检验，采用弹性网或自适应弹性网精炼选择；开发稳健变体整合OGK协方差估计、秩相关与Huber加权回归以应对污染和非正态数据

Result: 模拟显示：Dorfman-Sparse-Adaptive-EN在正态条件下表现最佳，Robust-OGK-Dorfman-Adaptive-EN在数据污染时显著优于经典方法；在NSCLC基因表达数据中，稳健方法获得最低预测误差并富集临床相关基因

Conclusion: Dorfman框架为基因组特征选择提供高效稳健方案，Robust-OGK-Dorfman-Adaptive-EN在理想与污染条件下均表现强劲，可扩展到超高维设置，适用于现代基因组生物标志物发现

Abstract: Background: High-dimensional genomic data exhibit strong group correlation structures that challenge conventional feature selection methods, which often assume feature independence or rely on pre-defined pathways and are sensitive to outliers and model misspecification.
  Methods: We propose the Dorfman screening framework, a multi-stage procedure that forms data-driven variable groups via hierarchical clustering, performs group and within-group hypothesis testing, and refines selection using elastic net or adaptive elastic net. Robust variants incorporate OGK-based covariance estimation, rank-based correlation, and Huber-weighted regression to handle contaminated and non-normal data.
  Results: In simulations, Dorfman-Sparse-Adaptive-EN performed best under normal conditions, while Robust-OGK-Dorfman-Adaptive-EN showed clear advantages under data contamination, outperforming classical Dorfman and competing methods. Applied to NSCLC gene expression data for trametinib response, robust Dorfman methods achieved the lowest prediction errors and enriched recovery of clinically relevant genes.
  Conclusions: The Dorfman framework provides an efficient and robust approach to genomic feature selection. Robust-OGK-Dorfman-Adaptive-EN offers strong performance under both ideal and contaminated conditions and scales to ultra-high-dimensional settings, making it well suited for modern genomic biomarker discovery.

</details>


### [30] [tLoRA: Efficient Multi-LoRA Training with Elastic Shared Super-Models](https://arxiv.org/abs/2602.07263)
*Kevin Li,Dibyadeep Saha,Avni Kanodia,Fan Lai*

Main category: cs.LG

TL;DR: tLoRA是一个高效并发训练多个LoRA适配器任务的框架，通过融合适配器和智能调度，显著提升训练吞吐量和GPU利用率


<details>
  <summary>Details</summary>
Motivation: 随着LoRA成为大型语言模型高效微调的标准方法，共享集群需要同时执行大量LoRA训练任务。现有方法无法有效共置异构适配器，导致同步阻塞、通信开销和性能下降，严重影响集群效率

Method: tLoRA将共享相同基础模型的适配器融合为弹性超级模型，利用分布式训练框架实现并行；在内核层采用融合LoRA内核自适应重建低秩计算块并调度秩感知纳米批次；在调度层部署在线剩余容量感知调度器动态分组任务

Result: 基于真实集群迹线评估显示，tLoRA将训练吞吐量提升1.2-1.8倍，任务完成时间缩短2.3-5.4倍，GPU利用率提高37%

Conclusion: tLoRA通过系统级优化解决了异构LoRA适配器训练共置的挑战，为大规模语言模型高效微调提供了可行的集群解决方案

Abstract: As Low-Rank Adaptation (LoRA) becomes the standard approach for efficiently fine-tuning large language models (LLMs), shared clusters increasingly execute many concurrent LoRA training jobs over the same frozen backbone. While recent advances enable batching (co-locating) multiple adapters during serving, efficient training-time co-location of heterogeneous LoRA adapters presents unique challenges. Jobs often differ in adapter rank, batch size, and resource allocation, and naïve batching can introduce synchronization stalls, communication overheads, and per-job slowdowns that are worse than executing independently. We introduce tLoRA, a framework that enables efficient batch training of multiple LoRA jobs. tLoRA fuses adapters that share the same base model into an elastic shared super-model, exploiting existing distributed training frameworks to derive parallelism plans that share resources effectively. At the kernel level, tLoRA employs a fused LoRA kernel that adaptively reconstructs low-rank computation tiles and schedules rank-aware nano-batches to maximize overlap between computation and communication across adapters. At the scheduling layer, tLoRA incorporates an online, residual-capacity-aware scheduler that adaptively groups jobs to maximize collective throughput. Evaluations using real-world cluster traces demonstrate that tLoRA improves training throughput by 1.2--1.8x, job training completion time by 2.3--5.4x, and GPU utilization by 37%.

</details>


### [31] [XShare: Collaborative in-Batch Expert Sharing for Faster MoE Inference](https://arxiv.org/abs/2602.07265)
*Daniil Vankov,Nikita Ivkin,Kyle Ulrich,Xiang Song,Ashish Khetan,George Karypis*

Main category: cs.LG

TL;DR: XShare通过建模批处理感知的专家选择为模块化优化问题，设计高效贪心算法，无需重训练即可动态优化专家选择，显著降低专家激活和GPU峰值负载，提升推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: MoE架构在高效扩展大语言模型方面应用广泛，但生产环境中的请求批处理和推测解码会显著放大专家激活，削弱效率优势。

Method: 将批处理感知的专家选择建模为模块化优化问题，设计适用于不同部署场景的高效贪心算法，提出XShare方法，无需重训练即可动态适配每个批次，最大化选中专家的总门控分数。

Result: 在标准批处理下专家激活降低最多30%，专家并行部署中峰值GPU负载降低最多3倍，推测解码场景下通过分层相关感知专家选择实现最多14%的吞吐量提升，即使在异构数据集中依然有效。

Conclusion: XShare成功解决了生产MoE推理中的效率损耗问题，通过动态优化专家选择无需重训练，在不同部署场景下均表现出显著的激活减少、负载均衡和吞吐量提升效果。

Abstract: Mixture-of-Experts (MoE) architectures are increasingly used to efficiently scale large language models. However, in production inference, request batching and speculative decoding significantly amplify expert activation, eroding these efficiency benefits. We address this issue by modeling batch-aware expert selection as a modular optimization problem and designing efficient greedy algorithms for different deployment settings. The proposed method, namely XShare, requires no retraining and dynamically adapts to each batch by maximizing the total gating score of selected experts. It reduces expert activation by up to 30% under standard batching, cuts peak GPU load by up to 3x in expert-parallel deployments, and achieves up to 14% throughput gains in speculative decoding via hierarchical, correlation-aware expert selection even if requests in a batch drawn from heterogeneous datasets.

</details>


### [32] [Hybrid Feedback-Guided Optimal Learning for Wireless Interactive Panoramic Scene Delivery](https://arxiv.org/abs/2602.07273)
*Xiaoyi Wu,Juaren Steiger,Bin Li,R. Srikant*

Main category: cs.LG

TL;DR: 针对VR/AR应用的严格性能要求，现有研究将预测反馈和传输反馈都视为bandit反馈，但本文发现预测反馈在用户头部姿态确定后可后验计算所有候选部分，属于全信息反馈。因此提出混合反馈模型及AdaPort算法，理论证明其遗憾上下界匹配，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 沉浸式应用（VR/AR）对帧率、延迟和同步要求严格。边缘服务器需渲染全景内容、预测用户头部运动并在带宽限制下传输覆盖视口的场景部分。现有研究将预测反馈和传输反馈都建模为多臂老虎机问题，但未能利用"预测反馈可在观察到头部姿态后对所有候选部分进行后验计算"这一特性，导致学习效率低下。

Method: 提出两层次混合反馈模型，结合全信息反馈（预测反馈）和bandit反馈（传输反馈），将部分选择问题建模为该设定下的在线学习任务。推导出混合反馈模型的实例相关遗憾下界，并提出AdaPort混合学习算法，利用两种反馈类型提升学习效率。

Result: 建立了与下界渐近匹配的实例相关遗憾上界；基于真实世界轨迹的仿真实验表明，AdaPort算法持续优于现有基线方法。

Conclusion: 通过区分预测反馈的全信息本质和传输反馈的bandit特性，提出的混合反馈模型和AdaPort算法在理论上和实践中都实现了更优的学习效率，为沉浸式应用的内容传输提供了新思路。

Abstract: Immersive applications such as virtual and augmented reality impose stringent requirements on frame rate, latency, and synchronization between physical and virtual environments. To meet these requirements, an edge server must render panoramic content, predict user head motion, and transmit a portion of the scene that is large enough to cover the user viewport while remaining within wireless bandwidth constraints. Each portion produces two feedback signals: prediction feedback, indicating whether the selected portion covers the actual viewport, and transmission feedback, indicating whether the corresponding packets are successfully delivered. Prior work models this problem as a multi-armed bandit with two-level bandit feedback, but fails to exploit the fact that prediction feedback can be retrospectively computed for all candidate portions once the user head pose is observed. As a result, prediction feedback constitutes full-information feedback rather than bandit feedback. Motivated by this observation, we introduce a two-level hybrid feedback model that combines full-information and bandit feedback, and formulate the portion selection problem as an online learning task under this setting. We derive an instance-dependent regret lower bound for the hybrid feedback model and propose AdaPort, a hybrid learning algorithm that leverages both feedback types to improve learning efficiency. We further establish an instance-dependent regret upper bound that matches the lower bound asymptotically, and demonstrate through real-world trace driven simulations that AdaPort consistently outperforms state-of-the-art baseline methods.

</details>


### [33] [Laplacian-LoRA: Delaying Oversmoothing in Deep GCNs via Spectral Low-Rank Adaptation](https://arxiv.org/abs/2602.07278)
*Sai Vamsi Alisetti*

Main category: cs.LG

TL;DR: 提出Laplacian-LoRA，一种低秩谱适应方法，通过可学习的谱校正来缓解GCN的过平滑问题，将有效深度扩展2倍。


<details>
  <summary>Details</summary>
Motivation: 深度GCN存在过平滑问题，导致节点表示随深度增加而坍缩。现有方法多通过架构修改或残差连接缓解，但对其谱层面的根本原因缺乏明确解释。

Method: 提出Laplacian-LoRA，在标准GCN的固定拉普拉斯传播算子基础上引入可学习的谱锚定低秩校正，选择性地减弱收缩效应，同时保持稳定性和低通归纳偏置。

Result: 在多数据集和深度上，Laplacian-LoRA持续延迟过平滑出现，将GCN有效深度提升最高2倍。嵌入方差诊断证实增益来自延迟的表征坍缩，学习的谱分析显示校正平滑、有界且表现良好。

Conclusion: 过平滑是深度依赖的谱现象，可通过传播算子的适度低秩适应系统性地延迟，无需重构消息传递机制。

Abstract: Oversmoothing is a fundamental limitation of deep graph convolutional networks (GCNs), causing node representations to collapse as depth increases. While many prior approaches mitigate this effect through architectural modifications or residual mechanisms, the underlying spectral cause of oversmoothing is often left implicit. We propose Laplacian-LoRA, a simple and interpretable low-rank spectral adaptation of standard GCNs. Rather than redesigning message passing, Laplacian-LoRA introduces a learnable, spectrally anchored correction to the fixed Laplacian propagation operator, selectively weakening contraction while preserving stability and the low-pass inductive bias. Across multiple benchmark datasets and depths, Laplacian-LoRA consistently delays the onset of oversmoothing, extending the effective depth of GCNs by up to a factor of two. Embedding variance diagnostics confirm that these gains arise from delayed representational collapse, while learned spectral analysis demonstrates that the correction is smooth, bounded, and well behaved. Our results show that oversmoothing is a depth-dependent spectral phenomenon that can be systematically delayed through modest, low-rank adaptation of the graph propagation operator.

</details>


### [34] [VertCoHiRF: Decentralized Vertical Clustering Beyond k-means](https://arxiv.org/abs/2602.07279)
*Bruno Belucci,Karim Lounici,Vladimir R. Kostic,Katia Meziani*

Main category: cs.LG

TL;DR: 提出VertCoHiRF，一种完全去中心化的纵向联邦聚类框架，通过交换样本标识符和序数排名而非原始特征统计量来实现隐私保护，并通过结构共识构建可解释的聚类融合层次结构，在异构视图下表现出鲁棒性和竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有纵向联邦聚类方法局限于分布式k-means变体，需要中心协调或交换特征相关数值统计量，且在异构视图或对抗行为下鲁棒性有限。

Method: 提出VertCoHiRF框架，各参与方在本地特征空间独立应用基础聚类方法，然后通过去中心化的序数排名在标识符层面达成共识，逐步构建共享的层次聚类。通信仅限于样本标识符、聚类标签和序数排名。

Result: 分析了通信复杂度与鲁棒性，实验表明该方法在纵向联邦设置下具有竞争力的聚类性能。

Conclusion: VertCoHiRF通过设计提供隐私保护，支持异构视图与重叠特征分区，产生的聚类融合层次结构具有可解释性，为纵向联邦聚类提供了有效解决方案。

Abstract: Vertical Federated Learning (VFL) enables collaborative analysis across parties holding complementary feature views of the same samples, yet existing approaches are largely restricted to distributed variants of $k$-means, requiring centralized coordination or the exchange of feature-dependent numerical statistics, and exhibiting limited robustness under heterogeneous views or adversarial behavior. We introduce VertCoHiRF, a fully decentralized framework for vertical federated clustering based on structural consensus across heterogeneous views, allowing each agent to apply a base clustering method adapted to its local feature space in a peer-to-peer manner. Rather than exchanging feature-dependent statistics or relying on noise injection for privacy, agents cluster their local views independently and reconcile their proposals through identifier-level consensus. Consensus is achieved via decentralized ordinal ranking to select representative medoids, progressively inducing a shared hierarchical clustering across agents. Communication is limited to sample identifiers, cluster labels, and ordinal rankings, providing privacy by design while supporting overlapping feature partitions and heterogeneous local clustering methods, and yielding an interpretable shared Cluster Fusion Hierarchy (CFH) that captures cross-view agreement at multiple resolutions.We analyze communication complexity and robustness, and experiments demonstrate competitive clustering performance in vertical federated settings.

</details>


### [35] [Fair Decisions from Calibrated Scores: Achieving Optimal Classification While Satisfying Sufficiency](https://arxiv.org/abs/2602.07285)
*Etam Benger,Katrina Ligett*

Main category: cs.LG

TL;DR: 本研究解决了在充足性(预测均等性)约束下的最优二元分类问题，提出了一种基于有限组校准分数的几何特征化和后处理算法，实现了在不违反公平性约束的前提下达到最优分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于阈值化的分数分类虽然在无约束情况下是贝叶斯最优的，但会违反统计群体公平性约束。特别是，即使分数已完美组校准，阈值化后仍会破坏预测均等性(充足性)，这促使作者寻找在充足性约束下的精确最优解。

Method: 1) 对有限组校准分数集合，几何化刻画可实现的正预测值(PPV)和漏报率(FOR)对的可行域；2) 基于此推导简单的后处理算法，仅使用组校准分数和组隶属关系即可达到最优分类器；3) 针对充足性与分离性(均等赔率)的不兼容性，在充足性约束下最小化对分离性的偏离。

Result: 获得了充足性约束下二元随机分类的精确最优解；算法能够直接从组校准分数计算最优分类器；当充足性与分离性冲突时，所提出的算法能找到满足充足性且最接近分离性的分类器，性能常接近全局最优。

Conclusion: 该研究为在现实世界应用中实现公平机器学习提供了理论基础和实用算法，特别是在医疗诊断等需要满足预测均等性约束的场景中，后处理方法具有直接的应用价值。

Abstract: Binary classification based on predicted probabilities (scores) is a fundamental task in supervised machine learning. While thresholding scores is Bayes-optimal in the unconstrained setting, using a single threshold generally violates statistical group fairness constraints. Under independence (statistical parity) and separation (equalized odds), such thresholding suffices when the scores already satisfy the corresponding criterion. However, this does not extend to sufficiency: even perfectly group-calibrated scores -- including true class probabilities -- violate predictive parity after thresholding. In this work, we present an exact solution for optimal binary (randomized) classification under sufficiency, assuming finite sets of group-calibrated scores. We provide a geometric characterization of the feasible pairs of positive predictive value (PPV) and false omission rate (FOR) achievable by such classifiers, and use it to derive a simple post-processing algorithm that attains the optimal classifier using only group-calibrated scores and group membership. Finally, since sufficiency and separation are generally incompatible, we identify the classifier that minimizes deviation from separation subject to sufficiency, and show that it can also be obtained by our algorithm, often achieving performance comparable to the optimum.

</details>


### [36] [Scalable Dexterous Robot Learning with AR-based Remote Human-Robot Interactions](https://arxiv.org/abs/2602.07341)
*Yicheng Yang,Ruijiao Li,Lifeng Wang,Shuai Zheng,Shunzheng Ma,Keyu Zhang,Tuoyu Sun,Chenyun Dai,Jie Ding,Zhuo Zou*

Main category: cs.LG

TL;DR: 提出一种结合增强现实(AR)远程示教与对比学习强化学习的两阶段机器人灵巧操作框架，显著提升任务成功率与推理速度


<details>
  <summary>Details</summary>
Motivation: 解决灵巧机械臂操作任务中专家示教数据收集效率低、传统强化学习训练慢且易出现策略崩溃的问题，通过AR远程交互实现高效数据获取

Method: 两阶段框架：1) 基于AR远程人机交互的示教数据，采用行为克隆(BC)进行预训练；2) 设计对比学习增强的强化学习(RL)方法，引入投影头加速学习，并采用事件驱动奖励机制提升安全性

Result: 在PyBullet仿真和真实实验中，相比PPO和SAC基线方法，该方法推理速度显著提升，操作任务成功率大幅提高，且消融实验证实对比学习有效避免了策略崩溃

Conclusion: 所提框架通过AR示教与对比学习RL的协同设计，实现了高效鲁棒的机器人灵巧操作学习，为可扩展的远程人机协作提供了有效解决方案

Abstract: This paper focuses on the scalable robot learning for manipulation in the dexterous robot arm-hand systems, where the remote human-robot interactions via augmented reality (AR) are established to collect the expert demonstration data for improving efficiency. In such a system, we present a unified framework to address the general manipulation task problem. Specifically, the proposed method consists of two phases: i) In the first phase for pretraining, the policy is created in a behavior cloning (BC) manner, through leveraging the learning data from our AR-based remote human-robot interaction system; ii) In the second phase, a contrastive learning empowered reinforcement learning (RL) method is developed to obtain more efficient and robust policy than the BC, and thus a projection head is designed to accelerate the learning progress. An event-driven augmented reward is adopted for enhancing the safety. To validate the proposed method, both the physics simulations via PyBullet and real-world experiments are carried out. The results demonstrate that compared to the classic proximal policy optimization and soft actor-critic policies, our method not only significantly speeds up the inference, but also achieves much better performance in terms of the success rate for fulfilling the manipulation tasks. By conducting the ablation study, it is confirmed that the proposed RL with contrastive learning overcomes policy collapse. Supplementary demonstrations are available at https://cyberyyc.github.io/.

</details>


### [37] [Controllable Value Alignment in Large Language Models through Neuron-Level Editing](https://arxiv.org/abs/2602.07356)
*Yonghui Yang,Junwei Li,Jilong Liu,Yicheng He,Fengbin Zhu,Weibiao Huang,Le Wu,Richang Hong,Tat-Seng Chua*

Main category: cs.LG

TL;DR: 针对大语言模型价值观对齐中现有方法存在的"价值泄漏"问题（激活目标价值观时意外触发非目标价值观），提出NeVA框架——通过识别稀疏的价值相关神经元并在推理时编辑其激活值，实现无需重训练的可控对齐，显著降低泄漏且保持模型通用能力


<details>
  <summary>Details</summary>
Motivation: 现有基于引导（steering）的价值观对齐方法可控性不足：激活特定人类价值观时，会非预期地激活其他无关价值观，缺乏精细控制机制

Method: 1) 定义"价值泄漏"概念并基于Schwartz价值理论构建标准化泄漏度量指标；2) 提出NeVA框架：先识别与目标价值观稀疏相关的关键神经元，再在推理时直接编辑这些神经元的激活值

Result: 1) 实现更强的目标价值观对齐效果；2) 对模型通用能力的性能衰减更小；3) 平均泄漏量显著降低，残余影响主要限于语义相关的价值观类别

Conclusion: NeVA提供了更可控、可解释的价值观对齐机制，通过神经元级激活编辑在保持模型能力的同时解决了价值泄漏问题，为精细化价值观对齐开辟了新路径

Abstract: Aligning large language models (LLMs) with human values has become increasingly important as their influence on human behavior and decision-making expands. However, existing steering-based alignment methods suffer from limited controllability: steering a target value often unintentionally activates other, non-target values. To characterize this limitation, we introduce value leakage, a diagnostic notion that captures the unintended activation of non-target values during value steering, along with a normalized leakage metric grounded in Schwartz's value theory. In light of this analysis, we propose NeVA, a neuron-level editing framework for controllable value alignment in LLMs. NeVA identifies sparse, value-relevant neurons and performs inference-time activation editing, enabling fine-grained control without parameter updates or retraining. Experiments show that NeVA achieves stronger target value alignment while incurring smaller performance degradation on general capability. Moreover, NeVA significantly reduces the average leakage, with residual effects largely confined to semantically related value classes. Overall, NeVA offers a more controllable and interpretable mechanism for value alignment.

</details>


### [38] [UTOPIA: Unlearnable Tabular Data via Decoupled Shortcut Embedding](https://arxiv.org/abs/2602.07358)
*Jiaming He,Fuming Luo,Hongwei Li,Wenbo Jiang,Wenshu Fan,Zhenbo Shi,Xudong Jiang,Yi Yu*

Main category: cs.LG

TL;DR: This paper proposes UTOPIA, a method to generate unlearnable examples for tabular data by decoupling optimization into semantic obfuscation and shortcut embedding channels, achieving near-random performance on unauthorized training.


<details>
  <summary>Details</summary>
Motivation: Unlearnable examples prevent unauthorized model training on private vision data, but extending this protection to sensitive tabular data (finance, healthcare) is challenging due to mixed numerical/categorical constraints and saliency sparsity where learning is dominated by few dimensions.

Method: UTOPIA exploits feature redundancy to decouple optimization into two channels: high saliency features for semantic obfuscation and low saliency redundant features for embedding hyper-correlated shortcuts, guided by a Spectral Dominance condition where poison spectrum overwhelms clean semantic spectrum.

Result: Extensive experiments demonstrate that UTOPIA drives unauthorized training toward near-random performance, outperforms strong UE baselines, and transfers effectively across different architectures while preserving tabular data validity.

Conclusion: Certified unlearnability is feasible for tabular data when poison spectrum dominates the clean semantic spectrum, and UTOPIA successfully achieves this through decoupled optimization, offering practical protection for sensitive tabular data.

Abstract: Unlearnable examples (UE) have emerged as a practical mechanism to prevent unauthorized model training on private vision data, while extending this protection to tabular data is nontrivial. Tabular data in finance and healthcare is highly sensitive, yet existing UE methods transfer poorly because tabular features mix numerical and categorical constraints and exhibit saliency sparsity, with learning dominated by a few dimensions. Under a Spectral Dominance condition, we show certified unlearnability is feasible when the poison spectrum overwhelms the clean semantic spectrum. Guided by this, we propose Unlearnable Tabular Data via DecOuPled Shortcut EmbeddIng (UTOPIA), which exploits feature redundancy to decouple optimization into two channels: high saliency features for semantic obfuscation and low saliency redundant features for embedding a hyper correlated shortcut, yielding constraint-aware dominant shortcuts while preserving tabular validity. Extensive experiments across tabular datasets and models show UTOPIA drives unauthorized training toward near random performance, outperforming strong UE baselines and transferring well across architectures.

</details>


### [39] [Privately Learning Decision Lists and a Differentially Private Winnow](https://arxiv.org/abs/2602.07370)
*Mark Bun,William Fang*

Main category: cs.LG

TL;DR: 本文提出了用于学习决策列表和大间隔半空间问题的新型差分隐私算法，在PAC和在线学习模型中实现了计算效率与隐私保护的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决经典机器学习问题（决策列表和半空间学习）在差分隐私约束下的算法设计挑战，旨在在保护数据隐私的同时，保持算法的计算效率和理论性能保证。

Method: 在PAC模型中设计计算高效算法，使样本开销仅比非私有算法略高；在在线模型中开发Winnow算法的私有版本，并应用于决策列表的在线学习。

Result: 获得多项理论成果：1) PAC模型中决策列表学习的样本复杂度接近最优；2) 在线半空间学习的错误边界在维度上是多项式对数级，在间隔上是反多项式级；3) 在线决策列表学习达到与非私有方法相当的性能保证。

Conclusion: 该研究为差分隐私机器学习提供了新的有效工具，在理论上弥合了私有与非私有算法在关键学习问题上的性能差距，拓展了隐私保护机器学习的适用范围。

Abstract: We give new differentially private algorithms for the classic problems of learning decision lists and large-margin halfspaces in the PAC and online models. In the PAC model, we give a computationally efficient algorithm for learning decision lists with minimal sample overhead over the best non-private algorithms. In the online model, we give a private analog of the influential Winnow algorithm for learning halfspaces with mistake bound polylogarithmic in the dimension and inverse polynomial in the margin. As an application, we describe how to privately learn decision lists in the online model, qualitatively matching state-of-the art non-private guarantees.

</details>


### [40] [Scout Before You Attend: Sketch-and-Walk Sparse Attention for Efficient LLM Inference](https://arxiv.org/abs/2602.07397)
*Hoang Anh Duy Le,Sahil Joshi,Zeyu Yang,Zhaozhuo Xu,Anshumali Shrivastava*

Main category: cs.LG

TL;DR: Sketch&Walk Attention 是一种训练无关的稀疏注意力方法，通过 Hadamard 草图估计注意力分数并结合跨层游走机制选择重要注意力块，在保持精度的同时实现最高6倍的推理加速。


<details>
  <summary>Details</summary>
Motivation: 长上下文 LLM 推理中，自注意力机制在预填充和解码阶段都占据主要的计算和内存开销，亟需高效稀疏化方案。

Method: 该方法对注意力分数进行 Hadamard 草图近似，通过跨层游走机制聚合各层估计以捕获非直接 token 交互的影响，累积评分用于动态选择 top-k 注意力块，并提供定制稀疏注意力内核，适用于预填充和解码两个阶段。

Result: 在多种模型和任务上，该方法在仅保留20%注意力密度时保持接近无损的精度，部分场景甚至略优于稠密注意力，推理速度最高提升6倍。

Conclusion: Sketch&Walk Attention 是一种有效的训练无关稀疏注意力方案，能显著降低长上下文 LLM 推理成本。

Abstract: Self-attention dominates the computational and memory cost of long-context LLM inference across both prefill and decode phases. To address this challenge, we introduce Sketch&Walk Attention, a training-free sparse attention method that determines sparsity with lightweight sketches and deterministic walk. Sketch&Walk applies Hadamard sketching to get inexpensive approximations of attention scores, then aggregates these estimates across layers via a walk mechanism that captures attention influence beyond direct interactions between tokens. The accumulated walk scores are used to select top-k attention blocks, enabling dynamic sparsity with a single training-free algorithm that applies uniformly to both the prefill and decode phases, together with custom sparse attention kernels. Across a wide range of models and tasks, Sketch&Walk maintains near-lossless accuracy at 20% attention density and can slightly outperform dense attention in some settings, while achieving up to 6x inference speedup.

</details>


### [41] [Nonparametric Bayesian Optimization for General Rewards](https://arxiv.org/abs/2602.07411)
*Zishi Zhang,Tao Ren,Yijie Peng*

Main category: cs.LG

TL;DR: Proposes ∞-GP: a novel Bayesian nonparametric surrogate model for Bayesian optimization under reward uncertainty, achieving no-regret guarantees with general rewards via Thompson Sampling, scalable computation, and superior empirical performance on ill-conditioned reward distributions.


<details>
  <summary>Details</summary>
Motivation: Classical Gaussian Process (GP) surrogate models in Bayesian optimization fail to adequately handle diverse, uncertain reward models (e.g., non-stationary, heavy-tailed), limiting robustness and applicability in real-world scenarios with complex reward structures.

Method: Introduces the infinite Gaussian process (∞-GP) as a flexible Bayesian nonparametric prior over reward distributions, combined with Thompson Sampling (TS) for decision-making. Develops a new TS regret analysis framework linking regret to total variation distance between model and true reward. Uses truncated Gibbs sampling for scalable inference.

Result: Achieves the first no-regret guarantee for Bayesian optimization with general Lipschitz continuous rewards and broad measurement noise. Empirical evaluations demonstrate state-of-the-art performance, especially on non-stationary, heavy-tailed, or ill-conditioned reward functions, with computational overhead comparable to classical GP.

Conclusion: The ∞-GP framework fundamentally expands the capability of Bayesian optimization to handle uncertain and complex reward landscapes robustly and efficiently, providing strong theoretical guarantees and practical advantages over classical GP-based methods.

Abstract: This work focuses on Bayesian optimization (BO) under reward model uncertainty. We propose the first BO algorithm that achieves no-regret guarantee in a general reward setting, requiring only Lipschitz continuity of the objective function and accommodating a broad class of measurement noise. The core of our approach is a novel surrogate model, termed as infinite Gaussian process ($\infty$-GP). It is a Bayesian nonparametric model that places a prior on the space of reward distributions, enabling it to represent a substantially broader class of reward models than classical Gaussian process (GP). The $\infty$-GP is used in combination with Thompson Sampling (TS) to enable effective exploration and exploitation. Correspondingly, we develop a new TS regret analysis framework for general rewards, which relates the regret to the total variation distance between the surrogate model and the true reward distribution. Furthermore, with a truncated Gibbs sampling procedure, our method is computationally scalable, incurring minimal additional memory and computational complexities compared to classical GP. Empirical results demonstrate state-of-the-art performance, particularly in settings with non-stationary, heavy-tailed, or other ill-conditioned rewards.

</details>


### [42] [Learning Molecular Chirality via Chiral Determinant Kernels](https://arxiv.org/abs/2602.07415)
*Runhan Shi,Zhicheng Zhang,Letian Chen,Gufeng Yu,Yang Yang*

Main category: cs.LG

TL;DR: 提出ChiDeK框架，通过手性行列式核和交叉注意力机制，统一编码中心手性和轴向手性，在四个任务上超越现有方法，轴向手性任务准确率提升超7%。


<details>
  <summary>Details</summary>
Motivation: 手性是分子基本性质，但现有机器学习模型难以捕捉其立体化学复杂性。传统方法主要关注中心手性，依赖手工特征或有限3D编码，无法泛化到轴向手性等更复杂形式，缺乏明确的手性编码能力。

Method: 提出ChiDeK框架，设计手性行列式核编码SE(3)不变手性矩阵，利用交叉注意力机制将局部手性中心信息整合到全局分子表征中，实现统一架构下的中心手性和轴向手性联合建模。同时构建新的ECD和旋光预测基准测试。

Result: 在R/S构型分类、对映体排序、ECD光谱预测和旋光预测四个任务上，ChiDeK显著优于现有最优基线，轴向手性任务平均准确率提升超过7%。

Conclusion: 该方法成功将立体化学信息显式集成到分子表征学习，为复杂手性分子的机器学习提供了有效解决方案，拓展了手性感知分子建模的能力边界。

Abstract: Chirality is a fundamental molecular property that governs stereospecific behavior in chemistry and biology. Capturing chirality in machine learning models remains challenging due to the geometric complexity of stereochemical relationships and the limitations of traditional molecular representations that often lack explicit stereochemical encoding. Existing approaches to chiral molecular representation primarily focus on central chirality, relying on handcrafted stereochemical tags or limited 3D encodings, and thus fail to generalize to more complex forms such as axial chirality. In this work, we introduce ChiDeK (Chiral Determinant Kernels), a framework that systematically integrates stereogenic information into molecular representation learning. We propose the chiral determinant kernel to encode the SE(3)-invariant chirality matrix and employ cross-attention to integrate stereochemical information from local chiral centers into the global molecular representation. This design enables explicit modeling of chiral-related features within a unified architecture, capable of jointly encoding central and axial chirality. To support the evaluation of axial chirality, we construct a new benchmark for electronic circular dichroism (ECD) and optical rotation (OR) prediction. Across four tasks, including R/S configuration classification, enantiomer ranking, ECD spectrum prediction, and OR prediction, ChiDeK achieves substantial improvements over state-of-the-art baselines, most notably yielding over 7% higher accuracy on axially chiral tasks on average.

</details>


### [43] [Achieving Optimal Static and Dynamic Regret Simultaneously in Bandits with Deterministic Losses](https://arxiv.org/abs/2602.07418)
*Jian Qian,Chen-Yu Wei*

Main category: cs.LG

TL;DR: 该论文研究对抗性多臂老虎机中同时实现最优静态遗憾和动态遗憾的问题，证明在确定性损失下对无感知对手可实现同时最优，但对适应性对手不可能，揭示了对手类型的根本差异。


<details>
  <summary>Details</summary>
Motivation: 探索对抗性多臂老虎机算法能否同时对静态遗憾和动态遗憾达到最优界，特别是针对适应性对手与无感知对手在不同损失条件下的可能性差异。

Method: 将Marinov和Zimmert的不可能性结果扩展到确定性损失情形，提出利用负静态遗憾补偿动态遗憾探索开销的算法，并运用Blackwell可达性理论联合控制两种遗憾，形成新的老虎机模型选择方法。

Result: 证明了同时最优性在无感知对手（确定性损失）下可实现而在适应性对手下不可实现，揭示了两种对手的根本分离，并为不同切换次数的切换基准这一长期开放问题提供了新见解。

Conclusion: 该研究在多臂老虎机中考虑多个遗憾基准时，确立了适应性对手与无感知对手的根本区别，提供了基于Blackwell可达性的新算法技术，对解决长期开放问题具有重要意义。

Abstract: In adversarial multi-armed bandits, two performance measures are commonly used: static regret, which compares the learner to the best fixed arm, and dynamic regret, which compares it to the best sequence of arms. While optimal algorithms are known for each measure individually, there is no known algorithm achieving optimal bounds for both simultaneously. Marinov and Zimmert [2021] first showed that such simultaneous optimality is impossible against an adaptive adversary. Our work takes a first step to demonstrate its possibility against an oblivious adversary when losses are deterministic. First, we extend the impossibility result of Marinov and Zimmert [2021] to the case of deterministic losses. Then, we present an algorithm achieving optimal static and dynamic regret simultaneously against an oblivious adversary. Together, they reveal a fundamental separation between adaptive and oblivious adversaries when multiple regret benchmarks are considered simultaneously. It also provides new insight into the long open problem of simultaneously achieving optimal regret against switching benchmarks of different numbers of switches.
  Our algorithm uses negative static regret to compensate for the exploration overhead incurred when controlling dynamic regret, and leverages Blackwell approachability to jointly control both regrets. This yields a new model selection procedure for bandits that may be of independent interest.

</details>


### [44] [Sign-Based Optimizers Are Effective Under Heavy-Tailed Noise](https://arxiv.org/abs/2602.07425)
*Dingzhi Yu,Hongyi Tao,Yuanyu Wan,Luo Luo,Lijun Zhang*

Main category: cs.LG

TL;DR: This paper provides theoretical justification for why sign-based optimizers (Lion, Muon) outperform adaptive gradient methods like AdamW in LLM training by analyzing them under a novel heavy-tailed gradient noise model.


<details>
  <summary>Details</summary>
Motivation: Sign-based optimization algorithms like Lion and Muon show superior empirical performance over AdamW in LLM training, but there's no theoretical understanding of why sign-based updates outperform variance-adapted methods.

Method: The authors introduce a novel generalized heavy-tailed noise condition that better captures LLM gradient behavior than standard finite variance assumptions. They analyze convergence rates of SignSGD, Lion, Muon, and Muonlight under this model, and validate findings with LLM pretraining experiments.

Result: The paper establishes sharp convergence rates for sign-based methods that match or surpass previous best-known bounds, and provides the first rigorous analysis of matrix optimization (Muon/Muonlight) under heavy-tailed stochasticity. Empirical LLM pretraining validates the theoretical insights and confirms the proposed noise models align with practice.

Conclusion: Sign-based optimizers are naturally suited to handle heavy-tailed gradient noise in LLMs, offering strong theoretical justification for their empirical superiority over adaptive gradient methods.

Abstract: While adaptive gradient methods are the workhorse of modern machine learning, sign-based optimization algorithms such as Lion and Muon have recently demonstrated superior empirical performance over AdamW in training large language models (LLM). However, a theoretical understanding of why sign-based updates outperform variance-adapted methods remains elusive. In this paper, we aim to bridge the gap between theory and practice through the lens of heavy-tailed gradient noise, a phenomenon frequently observed in language modeling tasks. Theoretically, we introduce a novel generalized heavy-tailed noise condition that captures the behavior of LLMs more accurately than standard finite variance assumptions. Under this noise model, we establish sharp convergence rates of SignSGD and Lion for generalized smooth function classes, matching or surpassing previous best-known bounds. Furthermore, we extend our analysis to Muon and Muonlight, providing what is, to our knowledge, the first rigorous analysis of matrix optimization under heavy-tailed stochasticity. These results offer a strong theoretical justification for the empirical superiority of sign-based optimizers, showcasing that they are naturally suited to handle the noisy gradients associated with heavy tails. Empirically, LLM pretraining experiments validate our theoretical insights and confirm that our proposed noise models are well-aligned with practice.

</details>


### [45] [Brep2Shape: Boundary and Shape Representation Alignment via Self-Supervised Transformers](https://arxiv.org/abs/2602.07429)
*Yuanxu Sun,Yuezhou Ma,Haixu Wu,Guanyang Zeng,Muye Chen,Jianmin Wang,Mingsheng Long*

Main category: cs.LG

TL;DR: Brep2Shape通过自监督预训练弥合边界表示(B-rep)的抽象性与几何直观性之间的鸿沟，利用双Transformer架构和拓扑注意力实现高精度几何理解


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法处理CAD的B-rep模型时存在表征鸿沟：连续方法分析精确但视觉抽象，离散方法直观清晰却牺牲几何精度

Method: 提出几何感知任务——从Bézier控制点预测密集空间点；采用双Transformer并行编码曲面/曲线令牌；引入拓扑注意力建模曲面对应关系

Result: 在多项下游任务中实现SOTA精度与更快收敛，展现显著可扩展性

Conclusion: 成功对齐抽象边界表示与直观形状表征，为几何深度学习提供新范式

Abstract: Boundary representation (B-rep) is the industry standard for computer-aided design (CAD). While deep learning shows promise in processing B-rep models, existing methods suffer from a representation gap: continuous approaches offer analytical precision but are visually abstract, whereas discrete methods provide intuitive clarity at the expense of geometric precision. To bridge this gap, we introduce Brep2Shape, a novel self-supervised pre-training method designed to align abstract boundary representations with intuitive shape representations. Our method employs a geometry-aware task where the model learns to predict dense spatial points from parametric Bézier control points, enabling the network to better understand physical manifolds derived from abstract coefficients. To enhance this alignment, we propose a Dual Transformer backbone with parallel streams that independently encode surface and curve tokens to capture their distinct geometric properties. Moreover, the topology attention is integrated to model the interdependencies between surfaces and curves, thereby maintaining topological consistency. Experimental results demonstrate that Brep2Shape offers significant scalability, achieving state-of-the-art accuracy and faster convergence across various downstream tasks.

</details>


### [46] [Proximal Action Replacement for Behavior Cloning Actor-Critic in Offline Reinforcement Learning](https://arxiv.org/abs/2602.07441)
*Jinzong Dong,Wei Huang,Jianshu Zhang,Zhuo Chen,Xinzhe Yuan,Qinying Gu,Zhaohui Jiang,Nanyang Ye*

Main category: cs.LG

TL;DR: 提出Proximal Action Replacement (PAR)方法，通过替换低价值动作为高价值动作，突破BC正则化在离线RL中的性能天花板


<details>
  <summary>Details</summary>
Motivation: 行为克隆正则化的离线RL方法在次优数据集上存在性能天花板，过度模仿会限制策略充分利用Critic发现的高价值区域

Method: PAR插件式样本替换器：渐进式将低价值动作替换为稳定Actor生成的高价值动作，扩展动作探索空间

Result: 在多个离线RL基准测试中性能持续提升，与TD3+BC结合可接近SOTA水平

Conclusion: PAR兼容多种BC正则化范式，有效突破性能限制且易于集成

Abstract: Offline reinforcement learning (RL) optimizes policies from a previously collected static dataset and is an important branch of RL. A popular and promising approach is to regularize actor-critic methods with behavior cloning (BC), which yields realistic policies and mitigates bias from out-of-distribution actions, but can impose an often-overlooked performance ceiling: when dataset actions are suboptimal, indiscriminate imitation structurally prevents the actor from fully exploiting high-value regions suggested by the critic, especially in later training when imitation is already dominant. We formally analyzed this limitation by investigating convergence properties of BC-regularized actor-critic optimization and verified it on a controlled continuous bandit task. To break this ceiling, we propose proximal action replacement (PAR), a plug-and-play training sample replacer that progressively replaces low-value actions with high-value actions generated by a stable actor, broadening the action exploration space while reducing the impact of low-value data. PAR is compatible with multiple BC regularization paradigms. Extensive experiments across offline RL benchmarks show that PAR consistently improves performance and approaches state-of-the-art when combined with the basic TD3+BC.

</details>


### [47] [On the Importance of a Multi-Scale Calibration for Quantization](https://arxiv.org/abs/2602.07465)
*Seungwoo Son,Ingyu Seong,Junhan Kim,Hyemi Jang,Yongkweon Jeon*

Main category: cs.LG

TL;DR: The paper proposes MaCa (Matryoshka Calibration), a length-aware Hessian construction method for LLM post-training quantization that uses multi-scale sequence lengths to improve quantization accuracy, showing consistent gains on Qwen3, Gemma3, and LLaMA3 models.


<details>
  <summary>Details</summary>
Motivation: Conventional PTQ methods use random fixed-length calibration sequences, ignoring the variable-length nature of real LLM inputs. This leads to inaccurate Hessian estimates that fail to capture true weight importance across diverse input scenarios, ultimately hurting quantization performance.

Method: MaCa incorporates multi-scale sequence length information into Hessian estimation and regularizes each sequence as an independent sample. This approach constructs a more stable and representative Hessian by acknowledging that input length directly affects activation distributions and weight importance captured by the Hessian.

Result: Experiments on state-of-the-art LLMs (Qwen3, Gemma3, LLaMA3) demonstrate that MaCa consistently improves accuracy under low-bit quantization. The method provides a lightweight enhancement that is compatible with existing PTQ frameworks without significant computational overhead.

Conclusion: This is the first work to systematically highlight the critical role of multi-scale calibration in LLM quantization. MaCa's simple yet effective approach addresses a fundamental oversight in current PTQ practices and establishes length-aware calibration as an important consideration for efficient LLM deployment.

Abstract: Post-training quantization (PTQ) is a cornerstone for efficiently deploying large language models (LLMs), where a small calibration set critically affects quantization performance. However, conventional practices rely on random sequences of fixed length, overlooking the variable-length nature of LLM inputs. Input length directly influences the activation distribution and, consequently, the weight importance captured by the Hessian, which in turn affects quantization outcomes. As a result, Hessian estimates derived from fixed-length calibration may fail to represent the true importance of weights across diverse input scenarios. We propose MaCa (Matryoshka Calibration), a simple yet effective method for length-aware Hessian construction. MaCa (i) incorporates multi-scale sequence length information into Hessian estimation and (ii) regularizes each sequence as an independent sample, yielding a more stable and fruitful Hessian for accurate quantization. Experiments on state-of-the-art LLMs (e.g., Qwen3, Gemma3, LLaMA3) demonstrate that MaCa consistently improves accuracy under low bit quantization, offering a lightweight enhancement compatible with existing PTQ frameworks. To the best of our knowledge, this is the first work to systematically highlight the role of multi-scale calibration in LLM quantization.

</details>


### [48] [Bandit Allocational Instability](https://arxiv.org/abs/2602.07472)
*Yilun Chen,Jiaqi Lu*

Main category: cs.LG

TL;DR: This paper introduces "allocation variability" as a new performance metric for multi-armed bandit algorithms, showing a fundamental trade-off between this metric and traditional regret: R_T × S_T = Ω(T^(3/2)). The authors prove that optimal regret-minimizing algorithms must have extremely high allocation variability (Θ(T)), while any algorithm with sublinear regret must have variability growing faster than √T. They propose UCB-f, a simple tunable algorithm that achieves the Pareto frontier, balancing these competing objectives.


<details>
  <summary>Details</summary>
Motivation: Standard multi-armed bandit algorithms focus solely on minimizing regret, but this can cause huge variation in how many times different arms are pulled. This is problematic for modern applications like learning-enhanced platform operations and post-bandit statistical inference where balanced allocation matters. The paper seeks to quantify and address this tension between reward maximization and allocation fairness.

Method: The authors introduce allocation variability (S_T) as the maximum standard deviation of arm pull counts. They establish theoretical lower bounds proving a fundamental trade-off between regret (R_T) and allocation variability for any algorithm. They then propose UCB-f, a generalization of UCB1, as a simple tunable algorithm that can achieve any point on the Pareto frontier.

Result: 1. Fundamental lower bound: For any algorithm with R_T = o(T), worst-case regret and allocation variability satisfy R_T × S_T = Ω(T^(3/2}). 2. Minimax regret-optimal algorithms must incur allocation variability Θ(T). 3. Any algorithm with sublinear worst-case regret must have S_T = ω(√T). 4. UCB-f can achieve any point on the Pareto frontier R_T × S_T = Θ̃(T^(3/2}). 5. The results resolve an open question from Praharaj and Khamaru (2025).

Conclusion: There exists an inherent, unavoidable trade-off between minimizing regret and maintaining balanced arm allocations in multi-armed bandit problems. While regret-optimal algorithms necessarily suffer from extreme allocation imbalance, the proposed UCB-f algorithm provides a practical, tunable solution that can navigate this Pareto frontier, offering important implications for real-world platform operations and statistical inference where balanced exploration is critical.

Abstract: When multi-armed bandit (MAB) algorithms allocate pulls among competing arms, the resulting allocation can exhibit huge variation. This is particularly harmful in modern applications such as learning-enhanced platform operations and post-bandit statistical inference. Thus motivated, we introduce a new performance metric of MAB algorithms termed allocation variability, which is the largest (over arms) standard deviation of an arm's number of pulls. We establish a fundamental trade-off between allocation variability and regret, the canonical performance metric of reward maximization. In particular, for any algorithm, the worst-case regret $R_T$ and worst-case allocation variability $S_T$ must satisfy $R_T \cdot S_T=Ω(T^{\frac{3}{2}})$ as $T\rightarrow\infty$, as long as $R_T=o(T)$. This indicates that any minimax regret-optimal algorithm must incur worst-case allocation variability $Θ(T)$, the largest possible scale; while any algorithm with sublinear worst-case regret must necessarily incur ${S}_T= ω(\sqrt{T})$. We further show that this lower bound is essentially tight, and that any point on the Pareto frontier $R_T \cdot S_T=\tildeΘ(T^{3/2})$ can be achieved by a simple tunable algorithm UCB-f, a generalization of the classic UCB1. Finally, we discuss implications for platform operations and for statistical inference, when bandit algorithms are used. As a byproduct of our result, we resolve an open question of Praharaj and Khamaru (2025).

</details>


### [49] [Bipartite Graph Attention-based Clustering for Large-scale scRNA-seq Data](https://arxiv.org/abs/2602.07475)
*Zhuomin Liang,Liang Bai,Xian Yang*

Main category: cs.LG

TL;DR: Proposes BGFormer, a bipartite graph transformer for scRNA-seq clustering that uses learnable anchor tokens to achieve linear O(n) complexity instead of quadratic O(n²), enabling scalability to large datasets.


<details>
  <summary>Details</summary>
Motivation: Existing transformer-based scRNA-seq clustering methods have O(n²) computational and space complexity, limiting their applicability to large-scale datasets due to excessive computational requirements.

Method: Introduces BGFormer with a bipartite graph attention mechanism that uses learnable anchor tokens as shared reference points. The model learns similarity between cells and anchor tokens rather than all pairwise cell-cell interactions, reducing complexity to O(n).

Result: Experimental results on multiple large-scale scRNA-seq datasets demonstrate that BGFormer is both effective for clustering and scalable to handle large numbers of cells.

Conclusion: BGFormer successfully addresses the scalability challenge of transformer-based scRNA-seq clustering by achieving linear computational complexity while maintaining clustering effectiveness, making it suitable for large-scale analyses.

Abstract: scRNA-seq clustering is a critical task for analyzing single-cell RNA sequencing (scRNA-seq) data, as it groups cells with similar gene expression profiles. Transformers, as powerful foundational models, have been applied to scRNA-seq clustering. Their self-attention mechanism automatically assigns higher attention weights to cells within the same cluster, enhancing the distinction between clusters. Existing methods for scRNA-seq clustering, such as graph transformer-based models, treat each cell as a token in a sequence. Their computational and space complexities are $\mathcal{O}(n^2)$ with respect to the number of cells, limiting their applicability to large-scale scRNA-seq datasets.To address this challenge, we propose a Bipartite Graph Transformer-based clustering model (BGFormer) for scRNA-seq data. We introduce a set of learnable anchor tokens as shared reference points to represent the entire dataset. A bipartite graph attention mechanism is introduced to learn the similarity between cells and anchor tokens, bringing cells of the same class closer together in the embedding space. BGFormer achieves linear computational complexity with respect to the number of cells, making it scalable to large datasets. Experimental results on multiple large-scale scRNA-seq datasets demonstrate the effectiveness and scalability of BGFormer.

</details>


### [50] [AI-Driven Predictive Modelling for Groundwater Salinization in Israel](https://arxiv.org/abs/2602.07478)
*Laxmi Pandey,Ariel Meroz,Ben Cheng,Ankita Manekar,Abhijit Mukherjee,Meirav Cohen,Adway Mitra*

Main category: cs.LG

TL;DR: A comprehensive ML framework combining multiple algorithms, feature selection, and explainable AI identifies key meteorological, geological, and anthropogenic drivers (especially treated wastewater) of groundwater salinity in Israel, providing insights for tailored salinity management strategies.


<details>
  <summary>Details</summary>
Motivation: Increasing salinity and contamination of groundwater is a serious global issue causing degradation of water resources, requiring comprehensive understanding of causal factors and identification of important drivers.

Method: Integrated diverse datasets to create a robust ML framework using Random Forest, XGBoost, Neural Networks, LSTM, CNN, and Linear Regression. Applied Recursive Feature Elimination, Global Sensitivity Analysis, SHAP-based XAI, and Double Machine Learning for causality analysis to identify and interpret key drivers.

Result: Identified key influential drivers: meteorological (Precipitation, Temperature), geological (Distance from river, Distance to saline body, TWI, Shoreline distance), and anthropogenic (Area of agriculture field, Treated Wastewater). XAI revealed Treated Wastewater as a critical context-dependent anthropogenic driver in vulnerable hydro-climatic environments.

Conclusion: The approach provides deeper insight into country-scale global salinization mechanisms, reduces AI model uncertainty, and highlights the need for tailored strategies to address salinity.

Abstract: Increasing salinity and contamination of groundwater is a serious issue in many parts of the world, causing degradation of water resources. The aim of this work is to form a comprehensive understanding of groundwater salinization underlying causal factors and identify important meteorological, geological and anthropogenic drivers of salinity. We have integrated different datasets of potential covariates, to create a robust framework for machine learning based predictive models including Random Forest (RF), XGBoost, Neural network, Long Short-Term Memory (LSTM), convolution neural network (CNN) and linear regression (LR), of groundwater salinity. Additionally, Recursive Feature Elimination (RFE) followed by Global sensitivity analysis (GSA) and Explainable AI (XAI) based SHapley Additive exPlanations (SHAP) were used to estimate the importance scores and find insights into the drivers of salinization. We also did causality analysis via Double machine learning using various predictive models. From these analyses, key meteorological (Precipitation, Temperature), geological (Distance from river, Distance to saline body, TWI, Shoreline distance), and anthropogenic (Area of agriculture field, Treated Wastewater) covariates are identified to be influential drivers of groundwater salinity across Israel. XAI analysis also identified Treated Wastewater (TWW) as an essential anthropogenic driver of salinity, its significance being context-dependent but critical in vulnerable hydro-climatic environment. Our approach provides deeper insight into global salinization mechanisms at country scale, reducing AI model uncertainty and highlighting the need for tailored strategies to address salinity.

</details>


### [51] [Deriving Neural Scaling Laws from the statistics of natural language](https://arxiv.org/abs/2602.07488)
*Francesco Cagnetta,Allan Raventós,Surya Ganguli,Matthieu Wyart*

Main category: cs.LG

TL;DR: 提出首个数据受限神经缩放定律的理论预测框架，通过语言统计特性（token相关性衰减和条件熵衰减）无参数预测LLM缩放指数，在GPT-2/LLaMA实验中高度吻合


<details>
  <summary>Details</summary>
Motivation: 现有实验性神经缩放定律缺乏理论支撑，无法从第一性原理预测现代LLM在自然语言数据集上的缩放指数，尤其数据受限场景

Method: 1) 识别语言两大核心统计特性：token对时间分离的相关性衰减 + 上下文长度增加时的下一token条件熵衰减；2) 基于这些特性推导无参数预测公式

Result: 理论公式成功预测GPT-2/LLaMA在TinyStories和WikiText数据集上的数据受限缩放指数，与实验测量值高度匹配

Conclusion: 首次建立从语言本质统计特性到神经缩放指数的定量理论桥梁，为LLM训练提供可解释的预测框架，无需依赖合成数据或自由参数

Abstract: Despite the fact that experimental neural scaling laws have substantially guided empirical progress in large-scale machine learning, no existing theory can quantitatively predict the exponents of these important laws for any modern LLM trained on any natural language dataset. We provide the first such theory in the case of data-limited scaling laws. We isolate two key statistical properties of language that alone can predict neural scaling exponents: (i) the decay of pairwise token correlations with time separation between token pairs, and (ii) the decay of the next-token conditional entropy with the length of the conditioning context. We further derive a simple formula in terms of these statistics that predicts data-limited neural scaling exponents from first principles without any free parameters or synthetic data models. Our theory exhibits a remarkable match with experimentally measured neural scaling laws obtained from training GPT-2 and LLaMA style models from scratch on two qualitatively different benchmarks, TinyStories and WikiText.

</details>


### [52] [CoMI-IRL: Contrastive Multi-Intention Inverse Reinforcement Learning](https://arxiv.org/abs/2602.07496)
*Antonio Mone,Frans A. Oliehoek,Luciano Cavalcante Siebert*

Main category: cs.LG

TL;DR: This paper proposes CoMI-IRL, a transformer-based unsupervised framework that decouples behavior representation and clustering from reward learning to overcome limitations of existing Multi-Intention IRL methods that require knowing the number of behavioral modes in advance.


<details>
  <summary>Details</summary>
Motivation: Existing Multi-Intention IRL methods require prior knowledge of the true number of behavioral modes (K*), limiting adaptability to new behaviors and preventing cross-behavior analysis. They couple clustering and reward learning, creating inflexible frameworks.

Method: The authors propose Contrastive Multi-Intention IRL (CoMI-IRL), a transformer-based unsupervised framework that decouples behavior representation and clustering from downstream reward learning.

Result: CoMI-IRL outperforms existing approaches without requiring prior knowledge of K* or labels, enables visual interpretation of behavior relationships, and adapts to unseen behaviors without full retraining.

Conclusion: CoMI-IRL provides a more flexible, interpretable, and adaptable solution for Multi-Intention IRL by eliminating the need for prior knowledge about the number of behavioral modes and enabling better generalization to new behaviors.

Abstract: Inverse Reinforcement Learning (IRL) seeks to infer reward functions from expert demonstrations. When demonstrations originate from multiple experts with different intentions, the problem is known as Multi-Intention IRL (MI-IRL). Recent deep generative MI-IRL approaches couple behavior clustering and reward learning, but typically require prior knowledge of the number of true behavioral modes $K^*$. This reliance on expert knowledge limits their adaptability to new behaviors, and only enables analysis related to the learned rewards, and not across the behavior modes used to train them. We propose Contrastive Multi-Intention IRL (CoMI-IRL), a transformer-based unsupervised framework that decouples behavior representation and clustering from downstream reward learning. Our experiments show that CoMI-IRL outperforms existing approaches without a priori knowledge of $K^*$ or labels, while allowing for visual interpretation of behavior relationships and adaptation to unseen behavior without full retraining.

</details>


### [53] [PALMS: Pavlovian Associative Learning Models Simulator](https://arxiv.org/abs/2602.07519)
*Martin Fixman,Alessandro Abati,Julián Jiménez Nimmo,Sean Lim,Esther Mondragón*

Main category: cs.LG

TL;DR: PALMS是一款基于Python的模拟器，用于模拟巴甫洛夫条件反射实验。它实现了多种联想学习模型，包括经典的Rescorla-Wagner模型、Pearce-Kaye-Hall、Mackintosh扩展、Le Pelley混合模型，以及一个新颖的统一可变学习率Rescorla-Wagner扩展模型。该模拟器提供图形界面，支持大规模实验设计和即时结果可视化，可显著扩展模型的预测能力。


<details>
  <summary>Details</summary>
Motivation: 模拟是理论发展和完善过程中不可或缺的步骤，能帮助研究者形成精确定义、生成模型并做出准确预测。然而，缺乏一个统一的环境来模拟和比较多种巴甫洛夫条件反射模型，这限制了理论研究的效率和深度。

Method: 研究者开发了一个Python环境，整合了五种关联学习模型，并设计了图形化界面以接受实验神经科学家常用的字母数字格式实验设计。该模拟器能处理数百个刺激，计算所有模型的构型线索和构型线索复合物。

Result: PALMS能够即时可视化结果，在单一架构内支持快速精确的多模型预测比较，显著扩展了模型的预测能力。图形显示可轻松保存，模拟数据可导出到电子表格。通过复制已发表的实验展示了其功能。

Conclusion: PALMS是一个有价值的开源工具，采用LGPL 3.0许可证，源代码在GitHub上公开。它通过统一的环境实现了多种巴甫洛夫模型的快速模拟和比较，极大地促进了联想学习领域的研究效率。

Abstract: Simulations are an indispensable step in the cycle of theory development and refinement, helping researchers formulate precise definitions, generate models, and make accurate predictions. This paper introduces the Pavlovian Associative Learning Models Simulator (PALMS), a Python environment to simulate Pavlovian conditioning experiments. In addition to the canonical Rescorla-Wagner model, PALMS incorporates several attentional learning approaches, including Pearce-Kaye-Hall, Mackintosh Extended, Le Pelley's Hybrid, and a novel extension of the Rescorla-Wagner model with a unified variable learning rate that integrates Mackintosh's and Pearce and Hall's opposing conceptualisations. The simulator's graphical interface allows for the input of entire experimental designs in an alphanumeric format, akin to that used by experimental neuroscientists. Moreover, it uniquely enables the simulation of experiments involving hundreds of stimuli, as well as the computation of configural cues and configural-cue compounds across all models, thereby considerably expanding their predictive capabilities. PALMS operates efficiently, providing instant visualisation of results, supporting rapid, precise comparisons of various models' predictions within a single architecture and environment. Furthermore, graphic displays can be easily saved, and simulated data can be exported to spreadsheets. To illustrate the simulator's capabilities and functionalities, we provide a detailed description of the software and examples of use, reproducing published experiments in the associative learning literature. PALMS is licensed under the open-source GNU Lesser General Public License 3.0. The simulator source code and the latest multiplatform release build are accessible as a GitHub repository at https://github.com/cal-r/PALMS-Simulator

</details>


### [54] [MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution](https://arxiv.org/abs/2602.07529)
*Jianwen Chen,Xinyu Yang,Peng Xia,Arian Azarang,Yueh Z Lee,Gang Li,Hongtu Zhu,Yun Li,Beidi Chen,Huaxiu Yao*

Main category: cs.LG

TL;DR: MedVerse is a novel medical reasoning framework that uses Petri net theory to parallelize clinical reasoning processes, improving efficiency (1.3x latency reduction, 1.7x throughput increase) and performance (up to 8.9% improvement over general LLMs) while maintaining logical consistency.


<details>
  <summary>Details</summary>
Motivation: Large language models force inherently parallel clinical reasoning into sequential linear paths, limiting efficiency and reliability for complex medical problems like differential diagnosis.

Method: MedVerse reformulates medical reasoning as a parallelizable directed acyclic graph process based on Petri net theory with a full-stack design: automated data curation (MedVerse Curator), topology-aware attention mechanism with adaptive position indices, and a customized parallel inference engine.

Result: Improves general-purpose LLMs by up to 8.9%, achieves comparable performance to specialized medical LLMs, reduces inference latency by 1.3x, and increases generation throughput by 1.7x through parallel decoding.

Conclusion: The Petri net-based framework successfully addresses sequential decoding limitations in medical reasoning, enabling more efficient and reliable parallel clinical inference without sacrificing performance.

Abstract: Large language models (LLMs) have demonstrated strong performance and rapid progress in a wide range of medical reasoning tasks. However, their sequential autoregressive decoding forces inherently parallel clinical reasoning, such as differential diagnosis, into a single linear reasoning path, limiting both efficiency and reliability for complex medical problems. To address this, we propose MedVerse, a reasoning framework for complex medical inference that reformulates medical reasoning as a parallelizable directed acyclic graph (DAG) process based on Petri net theory. The framework adopts a full-stack design across data, model architecture, and system execution. For data creation, we introduce the MedVerse Curator, an automated pipeline that synthesizes knowledge-grounded medical reasoning paths and transforms them into Petri net-structured representations. At the architectural level, we propose a topology-aware attention mechanism with adaptive position indices that supports parallel reasoning while preserving logical consistency. Systematically, we develop a customized inference engine that supports parallel execution without additional overhead. Empirical evaluations show that MedVerse improves strong general-purpose LLMs by up to 8.9%. Compared to specialized medical LLMs, MedVerse achieves comparable performance while delivering a 1.3x reduction in inference latency and a 1.7x increase in generation throughput, enabled by its parallel decoding capability.

</details>


### [55] [Gaussian Match-and-Copy: A Minimalist Benchmark for Studying Transformer Induction](https://arxiv.org/abs/2602.07562)
*Antoine Gonon,Alexandre Cordonnier,Nicolas Boumal*

Main category: cs.LG

TL;DR: This paper introduces Gaussian Match-and-Copy (GMC), a minimalist benchmark that isolates long-range retrieval in LLMs through pure second-order correlation signals, showing it captures key aspects of real Transformer behavior and reveals gradient descent implicitly learns max-margin hard retrieval mechanisms.


<details>
  <summary>Details</summary>
Motivation: Understanding match-and-copy behavior in large language models is challenging because retrieval and memorization are entangled on natural data; we need to disentangle these two phenomena to study how pure retrieval mechanisms emerge.

Method: Introduce Gaussian Match-and-Copy (GMC) benchmark with pure second-order correlation signals; conduct numerical investigations of Transformer match-and-copy circuits; analyze optimization dynamics in simplified attention settings; study gradient descent implicit bias regime.

Result: GMC retains key qualitative aspects of real Transformer match-and-copy circuits and separates architectures by retrieval capabilities; identifies an implicit-bias regime where gradient descent drives parameters to diverge while aligning with max-margin separator, yielding hard match selection.

Conclusion: The work provides theoretical understanding of how retrieval mechanisms emerge in Transformers, proving that gradient descent naturally leads to max-margin aligned solutions implementing hard match selection even when many solutions are possible under the regression objective.

Abstract: Match-and-copy is a core retrieval primitive used at inference time by large language models to retrieve a matching token from the context then copy its successor. Yet, understanding how this behavior emerges on natural data is challenging because retrieval and memorization are entangled. To disentangle the two, we introduce Gaussian Match-and-Copy (GMC), a minimalist benchmark that isolates long-range retrieval through pure second-order correlation signals. Numerical investigations show that this task retains key qualitative aspects of how Transformers develop match-and-copy circuits in practice, and separates architectures by their retrieval capabilities. We also analyze the optimization dynamics in a simplified attention setting. Although many solutions are a priori possible under a regression objective, including ones that do not implement retrieval, we identify an implicit-bias regime in which gradient descent drives the parameters to diverge while their direction aligns with the max-margin separator, yielding hard match selection. We prove this max-margin alignment for GD trajectories that reach vanishing empirical loss under explicit technical conditions.

</details>


### [56] [Unified Biomolecular Trajectory Generation via Pretrained Variational Bridge](https://arxiv.org/abs/2602.07588)
*Ziyang Yu,Wenbing Huang,Yang Liu*

Main category: cs.LG

TL;DR: PVB is a pretrained variational bridge model that uses encoder-decoder architecture with bridge matching to efficiently simulate molecular dynamics, unifying single-structure and trajectory data training while incorporating reinforcement learning for protein-ligand complexes.


<details>
  <summary>Details</summary>
Motivation: Molecular Dynamics simulations are computationally expensive, and existing deep generative models either generalize poorly across systems or fail to fully exploit structural information due to limited molecular diversity in trajectory data.

Method: The Pretrained Variational Bridge (PVB) uses an encoder-decoder framework that maps structures to a noised latent space and transports them toward targets via augmented bridge matching, unifying training on both single-structure and paired trajectory data. For protein-ligand complexes, it adds reinforcement learning-based optimization via adjoint matching.

Result: PVB faithfully reproduces thermodynamic and kinetic observables from MD simulations while delivering stable and efficient generative dynamics for proteins and protein-ligand complexes.

Conclusion: PVB successfully addresses computational limitations of MD by leveraging pretraining on diverse structural data and specialized optimization, enabling efficient and accurate molecular dynamics simulation.

Abstract: Molecular Dynamics (MD) simulations provide a fundamental tool for characterizing molecular behavior at full atomic resolution, but their applicability is severely constrained by the computational cost. To address this, a surge of deep generative models has recently emerged to learn dynamics at coarsened timesteps for efficient trajectory generation, yet they either generalize poorly across systems or, due to limited molecular diversity of trajectory data, fail to fully exploit structural information to improve generative fidelity. Here, we present the Pretrained Variational Bridge (PVB) in an encoder-decoder fashion, which maps the initial structure into a noised latent space and transports it toward stage-specific targets through augmented bridge matching. This unifies training on both single-structure and paired trajectory data, enabling consistent use of cross-domain structural knowledge across training stages. Moreover, for protein-ligand complexes, we further introduce a reinforcement learning-based optimization via adjoint matching that speeds progression toward the holo state, which supports efficient post-optimization of docking poses. Experiments on proteins and protein-ligand complexes demonstrate that PVB faithfully reproduces thermodynamic and kinetic observables from MD while delivering stable and efficient generative dynamics.

</details>


### [57] [Beyond Arrow: From Impossibility to Possibilities in Multi-Criteria Benchmarking](https://arxiv.org/abs/2602.07593)
*Polina Gordienko,Christoph Jansen,Julian Rodemann,Georg Schollmeyer*

Main category: cs.LG

TL;DR: 该论文将多指标基准测试的聚合问题形式化为社会选择问题，指出传统聚合方法的不连贯性源于阿罗不可能定理，但通过限制偏好结构（单峰、群体可分、距离限制），可以构造出良好行为的多模型排名。


<details>
  <summary>Details</summary>
Motivation: 现代基准测试（如HELM、MMLU）包含准确率、鲁棒性、效率等多个指标，将这些指标聚合为单一排名时会出现不连贯或不稳定问题，传统聚合方法缺乏理论保证。

Method: 将每个指标视为对模型排序的"投票者"，基准操作者聚合这些偏好。借鉴社会选择理论，分析在单峰偏好、群体可分偏好和距离限制偏好三种结构约束下，能否避免阿罗不可能定理的负面结果。

Result: 理论证明：在三种偏好结构限制下，多指标基准聚合可产生一致且稳定的模型排名。实证发现：现代基准测试套件在不同任务上满足不同的结构条件。

Conclusion: 通过识别和验证偏好结构约束，多指标基准测试的聚合可以从不可能变为可能，为构建更可靠的模型排名提供理论基础。

Abstract: Modern benchmarks such as HELM MMLU account for multiple metrics like accuracy, robustness and efficiency. When trying to turn these metrics into a single ranking, natural aggregation procedures can become incoherent or unstable to changes in the model set. We formalize this aggregation as a social choice problem where each metric induces a preference ranking over models on each dataset, and a benchmark operator aggregates these votes across metrics. While prior work has focused on Arrow's impossibility result, we argue that the impossibility often originates from pathological examples and identify sufficient conditions under which these disappear, and meaningful multi-criteria benchmarking becomes possible. In particular, we deal with three restrictions on the combinations of rankings and prove that on single-peaked, group-separable and distance-restricted preferences, the benchmark operator allows for the construction of well-behaved rankings of the involved models. Empirically, we investigate several modern benchmark suites like HELM MMLU and verify which structural conditions are fulfilled on which benchmark problems.

</details>


### [58] [Astro: Activation-guided Structured Regularization for Outlier-Robust LLM Post-Training Quantization](https://arxiv.org/abs/2602.07596)
*Xi Chen,Ming Li,Junxi Li,Changsheng Li,Peisong Wang,Lizhong Ding,Ye Yuan,Guoren Wang*

Main category: cs.LG

TL;DR: Astro is an activation-guided structured regularization framework that suppresses weight outliers in LLM post-training quantization without adding inference latency, outperforming complex methods with faster quantization time.


<details>
  <summary>Details</summary>
Motivation: Weight-only post-training quantization (PTQ) for LLMs suffers from accuracy degradation due to weight and activation outliers, and existing mitigation strategies have limitations like insufficient outlier suppression or deployment inefficiencies.

Method: Leveraging the insight that over-parameterized LLMs converge to flat minima, Astro uses activation-guided structured regularization to reconstruct robust weights and suppress outliers corresponding to high-magnitude activations.

Result: Astro achieves highly competitive performance, outperforming complex learning-based rotation methods on LLaMA-2-7B with nearly 1/3 of the quantization time, while introducing zero inference latency.

Conclusion: Astro effectively addresses PTQ accuracy degradation in a hardware-friendly and efficient manner, offering a practical solution for efficient LLM deployment.

Abstract: Weight-only post-training quantization (PTQ) is crucial for efficient Large Language Model (LLM) deployment but suffers from accuracy degradation caused by weight and activation outliers. Existing mitigation strategies often face critical limitations: they either yield insufficient outlier suppression or incur significant deployment inefficiencies, such as inference latency, heavy preprocessing, or reliance on complex operator fusion. To resolve these limitations, we leverage a key insight: over-parameterized LLMs often converge to Flat Minima, implying a vast equivalent solution space where weights can be adjusted without compromising accuracy. Building on this, we propose Astro, an Activation-guided Structured Regularization framework designed to suppress the negative effects of outliers in a hardware-friendly and efficient manner. Leveraging the activation-guided regularization objective, Astro actively reconstructs intrinsically robust weights, aggressively suppressing weight outliers corresponding to high-magnitude activations without sacrificing model accuracy. Crucially, Astro introduces zero inference latency and is orthogonal to mainstream quantization methods like GPTQ. Extensive experiments show that Astro achieves highly competitive performance; notably, on LLaMA-2-7B, it achieves better performance than complex learning-based rotation methods with almost 1/3 of the quantization time.

</details>


### [59] [Rational Transductors](https://arxiv.org/abs/2602.07599)
*Mehryar Mohri*

Main category: cs.LG

TL;DR: 本文提出 Rational Transductors，一种双流架构，通过加权有限自动机（WFA）的矩阵值递归来增强 Transformer，解决了 Transformer 在严格顺序逻辑和状态跟踪方面的理论局限，使其能够表达正则语言、NC^1 完全问题等，同时保持 O(L + log T) 的并行时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 标准 Transformer 在硬性注意力下仅限于 AC^0 复杂度，在软性注意力下为 TC^0 复杂度，这些复杂度类别难以支持无需思维链的鲁棒长度泛化，无法有效处理需要严格顺序逻辑和状态跟踪的问题。

Method: 提出 Rational Transductors 架构，通过深度有理注入机制将基于加权有限自动机的矩阵值递归与注意力机制结合。理论证明随机有理特征是顺序依赖的通用基底，并建立可微有理特征机制来缩小表示紧致性差距。

Result: 理论分析和实验表明，该架构能够捕捉所有正则语言、NC^1 完全问题（如布尔公式求值）、奇偶性和模计数等基础问题分离，解决"正则性鸿沟"，在标准 Transformer 失败的算法任务上实现鲁棒长度泛化，且无传统 RNN 的顺序计算瓶颈。

Conclusion: Rational Transductors 成功扩展了 Transformer 的表达能力边界，为需要严格顺序逻辑的任务提供了兼具理论保证和计算效率的新架构。

Abstract: Standard Transformers excel at semantic modeling but struggle with
  rigid sequential logic and state tracking. Theoretical work
  establishes that self-attention is limited to $\AC^0$ (under hard
  attention) or $\TC^0$ (under soft attention), complexity classes
  that often fail to support robust length generalization on
  sequential problems without intermediate chain-of-thought. In this
  work, we introduce \emph{Rational Transductors}, a dual-stream
  architecture that augments the Transformer with a matrix-valued
  recurrence derived from Weighted Finite Automata (WFA). By
  injecting rational state information into the attention mechanism
  via a \emph{Deep Rational Injection} scheme, our framework strictly
  generalizes the expressive power of Transformers to capture all
  Regular Languages, $\NC^1$-complete problems (such as Boolean
  Formula Evaluation), and fundamental separations like Parity and
  Modular Counting, while preserving $O(L + \log T)$ parallel time
  complexity. We ground the architecture in a rigorous learning
  theory: we prove that \emph{Random Rational Features} act as a
  universal basis for sequential dependencies, justifying our
  initialization strategy, while establishing that the
  \emph{Differentiable Rational Feature} regime is necessary to close
  the representational compactness gap. Theoretical analysis and
  empirical results demonstrate that Rational Transductors solve the
  "Regular Gap," enabling robust length generalization on algorithmic
  tasks where standard Transformers fail, without the sequential
  computational bottlenecks of traditional RNNs.

</details>


### [60] [Escaping Spectral Bias without Backpropagation: Fast Implicit Neural Representations with Extreme Learning Machines](https://arxiv.org/abs/2602.07603)
*Woojin Cho,Junghwan Park*

Main category: cs.LG

TL;DR: 提出ELM-INR（免反向传播隐式神经表示）和BEAM自适应优化策略，通过局部闭式求解与频谱复杂度均衡，解决传统方法的光谱偏置与训练效率问题


<details>
  <summary>Details</summary>
Motivation: 传统隐式神经表示（INRs）依赖迭代反向传播，在非均匀频谱内容场景下易受光谱偏置影响且训练缓慢

Method: 1. ELM-INR：将域分解为重叠子域，用极限学习机（ELM）闭式解替代迭代优化，通过单位分割融合局部预测；2. BEAM：基于频谱Barron范数分析，提出自适应网格细化策略以平衡子域频谱复杂度

Result: 实现快速稳定重建；理论证明全局误差由高频复杂区域主导；BEAM在有限容量下显著提升重建质量

Conclusion: ELM-INR与BEAM协同有效克服光谱偏置与训练瓶颈，为高精度神经表示提供高效鲁棒的新范式

Abstract: Training implicit neural representations (INRs) to capture fine-scale details typically relies on iterative backpropagation and is often hindered by spectral bias when the target exhibits highly non-uniform frequency content. We propose ELM-INR, a backpropagation-free INR that decomposes the domain into overlapping subdomains and fits each local problem using an Extreme Learning Machine (ELM) in closed form, replacing iterative optimization with stable linear least-squares solutions. This design yields fast and numerically robust reconstruction by combining local predictors through a partition of unity. To understand where approximation becomes difficult under fixed local capacity, we analyze the method from a spectral Barron norm perspective, which reveals that global reconstruction error is dominated by regions with high spectral complexity. Building on this insight, we introduce BEAM, an adaptive mesh refinement strategy that balances spectral complexity across subdomains to improve reconstruction quality in capacity-constrained regimes.

</details>


### [61] [SERE: Similarity-based Expert Re-routing for Efficient Batch Decoding in MoE Models](https://arxiv.org/abs/2602.07616)
*Juntong Wu,Jialiang Cheng,Fuyu Lv,Ou Dan,Li Yuan*

Main category: cs.LG

TL;DR: SERE is a dynamic expert rerouting method that resolves the tension between batch decoding efficiency and expert sparsity in MoE models, achieving up to 2.0x speedup with minimal quality loss.


<details>
  <summary>Details</summary>
Motivation: MoE models face a conflict in production serving: batch inference improves hardware efficiency but causes excessive expert activation during memory-bound decoding, slowing down performance despite sparse activation benefits.

Method: SERE dynamically reduces active experts during batch decoding by rerouting tokens from secondary experts to their most similar primary experts. It uses similarity patterns to identify and preserve critical experts, avoiding static pruning/merging while enabling input-aware expert skipping based on batch-level redundancy. Includes an efficient custom CUDA kernel for plug-and-play integration with vLLM (single-line code change).

Result: Achieves up to 2.0x speedup on complex reasoning benchmarks while maintaining minimal quality loss, enabling cost-efficient and low-latency large-scale MoE deployment.

Conclusion: SERE provides a practical, dynamic solution to the fundamental tension between batch decoding efficiency and expert sparsity in MoE models, significantly improving serving performance without compromising model capabilities.

Abstract: Mixture-of-Experts (MoE) architectures employ sparse activation to deliver faster training and inference with higher accuracy than dense LLMs. However, in production serving, MoE models require batch inference to optimize hardware efficiency, which may cause excessive expert activation and thus slow the memory-bound decoding stage. To address the fundamental tension between batch decoding and expert sparsity, we present SERE, a Similarity-based Expert Re-routing method for Efficient batch decoding in MoE models. SERE dynamically reduces the number of active experts in an input-aware manner by re-routing tokens from secondary experts to their most similar primary counterparts. It also leverages similarity patterns to identify and preserve critical experts, thereby preventing capability loss. Notably, SERE avoids static expert pruning or merging, instead enabling dynamic expert skipping based on batch-level expert redundancy. Additionally, we provide an efficient custom CUDA kernel for SERE, enabling plug-and-play use in vLLM with only a single-line code change. Extensive experiments on various complex reasoning benchmarks demonstrate that SERE achieves up to 2.0x speedup with minimal quality loss, providing a practical solution for cost-efficient and latency-sensitive large-scale MoE deployment. Code implementation of SERE can be found in https://github.com/JL-Cheng/SERE.

</details>


### [62] [TASTE: Task-Aware Out-of-Distribution Detection via Stein Operators](https://arxiv.org/abs/2602.07640)
*Michał Kozyra,Gesine Reinert*

Main category: cs.LG

TL;DR: TASTE是一种基于Stein算子的任务感知型分布外检测框架，通过将分布偏移投影到模型敏感度场来实现检测、定位和像素级诊断，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有分布外检测方法存在局限：数据驱动方法忽略模型影响，模型驱动方法不考虑数据几何结构。需要一种能够同时关联分布偏移与模型输入敏感性的统一框架。

Method: 提出TASTE（Task-Aware STEin operators）框架，利用Stein算子将分布偏移与模型输入敏感性联系起来。该算子具有几何解释（分布偏移在模型敏感度场上的投影），支持坐标分解实现偏移定位，并对图像数据提供可解释的像素级诊断。

Result: 在控制性高斯偏移、MNIST几何扰动和CIFAR-10扰动基准测试中，该方法不仅与任务退化高度相关，而且显著优于现有基线方法。

Conclusion: 该框架成功桥接了数据驱动和模型驱动方法的鸿沟，提供了理论保证和实用的分布偏移检测、定位与诊断能力，为可解释的分布外检测提供了新思路。

Abstract: Out-of-distribution detection methods are often either data-centric, detecting deviations from the training input distribution irrespective of their effect on a trained model, or model-centric, relying on classifier outputs without explicit reference to data geometry. We propose TASTE (Task-Aware STEin operators): a task-aware framework based on so-called Stein operators, which allows us to link distribution shift to the input sensitivity of the model. We show that the resulting operator admits a clear geometric interpretation as a projection of distribution shift onto the sensitivity field of the model, yielding theoretical guarantees. Beyond detecting the presence of a shift, the same construction enables its localisation through a coordinate-wise decomposition, and for image data-provides interpretable per-pixel diagnostics. Experiments on controlled Gaussian shifts, MNIST under geometric perturbations, and CIFAR-10 perturbed benchmarks demonstrate that the proposed method aligns closely with task degradation while outperforming established baselines.

</details>


### [63] [Continuous Program Search](https://arxiv.org/abs/2602.07659)
*Matthew Siper,Muhammad Umair Nasir,Ahmed Khalifa,Lisa Soros,Jay Azhang,Julian Togelius*

Main category: cs.LG

TL;DR: 该论文提出了一种几何编译的突变算子，通过将连续程序空间中的距离与行为变化对齐，显著提高了遗传编程的搜索效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 遗传编程虽然可生成可解释程序，但其语法突变常导致不可预测的行为剧变，损害了局部性和样本效率。需要设计能保持行为连续性的突变算子。

Method: 1) 通过跟踪受控潜在扰动下的动作级散度，建立可测量的行为局部性指标；2) 在交易策略DSL上学习块因子化嵌入；3) 比较传统高斯突变与几何编译突变（限制在语义配对的进出子空间，使用基于流的模型指导方向）。

Result: 在相同(μ+λ)演化策略和评估预算下，几何编译突变算子：1) 用少一个数量级的评估次数发现强策略；2) 获得最高的样本外夏普比率中位数；3) 虽峰值性能偶低于各向同性突变，但进步更快更可靠。

Conclusion: 语义对齐的突变算子能显著提升搜索效率，且无需修改底层演化算法。通过使潜在空间距离具有行为意义，可设计出更有效的程序进化方法。

Abstract: Genetic Programming yields interpretable programs, but small syntactic mutations can induce large, unpredictable behavioral shifts, degrading locality and sample efficiency. We frame this as an operator-design problem: learn a continuous program space where latent distance has behavioral meaning, then design mutation operators that exploit this structure without changing the evolutionary optimizer.
  We make locality measurable by tracking action-level divergence under controlled latent perturbations, identifying an empirical trust region for behavior-local continuous variation. Using a compact trading-strategy DSL with four semantic components (long/short entry and exit), we learn a matching block-factorized embedding and compare isotropic Gaussian mutation over the full latent space to geometry-compiled mutation that restricts updates to semantically paired entry--exit subspaces and proposes directions using a learned flow-based model trained on logged mutation outcomes.
  Under identical $(μ+λ)$ evolution strategies and fixed evaluation budgets across five assets, the learned mutation operator discovers strong strategies using an order of magnitude fewer evaluations and achieves the highest median out-of-sample Sharpe ratio. Although isotropic mutation occasionally attains higher peak performance, geometry-compiled mutation yields faster, more reliable progress, demonstrating that semantically aligned mutation can substantially improve search efficiency without modifying the underlying evolutionary algorithm.

</details>


### [64] [Surprisal-Guided Selection: Compute-Optimal Test-Time Strategies for Execution-Grounded Code Generation](https://arxiv.org/abs/2602.07670)
*Jarrod Barnes*

Main category: cs.LG

TL;DR: 质疑测试时训练(TTT)的有效性，证明在可验证执行导向(VEG)任务中，搜索策略(如Best-of-N采样)显著优于梯度适应；提出零成本"惊奇度引导选择"方法，通过选择低置信度正确样本将成功率提升30%，最高达80%


<details>
  <summary>Details</summary>
Motivation: 探究测试时梯度适应是否真的是计算最优策略，针对具有密集连续奖励信号的可验证执行导向任务(如GPU内核优化)，质疑TTT在推理阶段的实际价值

Method: 使用KernelBench测试平台，基于120B参数模型(GPT-OSS-120B+LoRA)对比TTT(1-5梯度步)与搜索策略；提出基于语言模型惊奇度(描述样本)的选择机制，分析多样性崩溃现象

Result: Best-of-N采样(K=64)达90%任务成功率(18/20)，TTT最佳仅30.6%；TTT存在"过锐化"缺陷(梯度更新使解坍缩至平庸方案)；惊奇度引导选择使成功率从50%→80%(+30%)，top3选择匹配Oracle性能

Conclusion: 在密集奖励的VEG任务中，计算资源应优先分配给样本多样性+智能选择而非梯度适应；惊奇度引导选择原则可推广至其他执行导向领域(最优解位于分布尾部)

Abstract: Test-time training (TTT) adapts language models through gradient-based updates at inference. But is adaptation the right strategy? We study compute-optimal test-time strategies for verifiable execution-grounded (VEG) tasks, domains like GPU kernel optimization where a deterministic evaluator provides dense, continuous reward signals. Using KernelBench as our testbed and a 120B-parameter model (GPT-OSS-120B with LoRA adaptation), we find that search outperforms minimal adaptation (1-5 gradient steps): Best-of-N sampling achieves 90% task success (18/20 tasks) at K=64 across the full KernelBench L1 eval set while TTT's best checkpoint reaches only 30.6% (3-seed mean), with TTT's "equivalent K" falling below 1, worse than single-sample inference. The failure mode is over-sharpening: gradient updates collapse diversity toward mediocre solutions rather than discovering optimal ones. Our main contribution is surprisal-guided selection: selecting the highest-surprisal (lowest-confidence) correct sample yields 80% success vs. 50% for most-confident selection, a 30% improvement. Extending to surprisal-guided-top3 matches oracle performance at 100%. This zero-cost strategy, validated through length-controlled analysis, recovers oracle performance. For dense-reward VEG tasks, compute should be allocated to sample diversity and intelligent selection rather than gradient adaptation. The surprisal-guided selection principle may generalize to other execution-grounded domains where optimal solutions occupy the distribution tail.

</details>


### [65] [Federated Learning with Profile Mapping under Distribution Shifts and Drifts](https://arxiv.org/abs/2602.07671)
*Mohan Li,Dario Fenoglio,Martin Gjoreski,Marc Langheinrich*

Main category: cs.LG

TL;DR: Feroma是一种新型联邦学习框架，通过客户端分布档案(紧凑、隐私保护的数据表示)和自适应相似性加权，无需客户端/聚类身份即可处理分布偏移和漂移，在6个基准测试上比最优基线平均提升12%准确率，同时保持与FedAvg相当的计算通信开销。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在真实世界数据异构性下性能下降，现有方法无法同时处理跨客户端的分布偏移和随时间的数据漂移，或依赖客户聚类数量和数据异构类型等不切实际的假设，限制了其泛化能力。

Method: 提出Feroma框架，构建客户端分布档案作为隐私保护的数据表示，通过自适应相似性加权指导模型聚合和测试时模型分配。该设计能动态选择从聚类到个性化的聚合策略，无需重新训练、在线适应或先验知识即可为未见过的未标记测试客户端部署合适模型。

Result: 在6个基准测试上对比10种先进方法，Feroma在动态数据异构条件下显著提升性能和稳定性，平均准确率比最优基线高出12个百分点，同时保持与FedAvg相当的计算和通信开销。

Conclusion: 基于分布档案的聚合策略为处理数据分布偏移和漂移的鲁棒联邦学习提供了一条实用路径。

Abstract: Federated Learning (FL) enables decentralized model training across clients without sharing raw data, but its performance degrades under real-world data heterogeneity. Existing methods often fail to address distribution shift across clients and distribution drift over time, or they rely on unrealistic assumptions such as known number of client clusters and data heterogeneity types, which limits their generalizability. We introduce Feroma, a novel FL framework that explicitly handles both distribution shift and drift without relying on client or cluster identity. Feroma builds on client distribution profiles-compact, privacy-preserving representations of local data-that guide model aggregation and test-time model assignment through adaptive similarity-based weighting. This design allows Feroma to dynamically select aggregation strategies during training, ranging from clustered to personalized, and deploy suitable models to unseen, and unlabeled test clients without retraining, online adaptation, or prior knowledge on clients' data. Extensive experiments show that compared to 10 state-of-the-art methods, Feroma improves performance and stability under dynamic data heterogeneity conditions-an average accuracy gain of up to 12 percentage points over the best baselines across 6 benchmarks-while maintaining computational and communication overhead comparable to FedAvg. These results highlight that distribution-profile-based aggregation offers a practical path toward robust FL under both data distribution shifts and drifts.

</details>


### [66] [ElliCE: Efficient and Provably Robust Algorithmic Recourse via the Rashomon Sets](https://arxiv.org/abs/2602.07674)
*Bohdan Turbal,Iryna Voitsitska,Lesia Semenova*

Main category: cs.LG

TL;DR: 提出ElliCE框架，通过椭球近似Rashomon集优化反事实解释，实现模型不确定性下的鲁棒算法补救，具有理论保证且更快更灵活。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型影响人类生活决策，算法补救提供可操作建议很重要。但当Rashomon集（近优模型集合）很大时，标准反事实解释不可靠，因为针对一个模型有效的补救措施在另一个模型上可能失效。

Method: 提出ElliCE，一种鲁棒算法补救框架，在Rashomon集的椭球近似上优化反事实，提供唯一性、稳定性和与关键特征方向一致性的理论保证。

Result: 实证表明，ElliCE生成的反事实更鲁棒、更灵活（可适应用户指定的特征约束），且比现有基线方法显著更快。

Conclusion: 该框架为模型不确定性下的可靠补救提供了原则性和实用性的解决方案，确保即模型演进时也能为用户提供稳定的建议。

Abstract: Machine learning models now influence decisions that directly affect people's lives, making it important to understand not only their predictions, but also how individuals could act to obtain better results. Algorithmic recourse provides actionable input modifications to achieve more favorable outcomes, typically relying on counterfactual explanations to suggest such changes. However, when the Rashomon set - the set of near-optimal models - is large, standard counterfactual explanations can become unreliable, as a recourse action valid for one model may fail under another. We introduce ElliCE, a novel framework for robust algorithmic recourse that optimizes counterfactuals over an ellipsoidal approximation of the Rashomon set. The resulting explanations are provably valid over this ellipsoid, with theoretical guarantees on uniqueness, stability, and alignment with key feature directions. Empirically, ElliCE generates counterfactuals that are not only more robust but also more flexible, adapting to user-specified feature constraints while being substantially faster than existing baselines. This provides a principled and practical solution for reliable recourse under model uncertainty, ensuring stable recommendations for users even as models evolve.

</details>


### [67] [Dense Feature Learning via Linear Structure Preservation in Medical Data](https://arxiv.org/abs/2602.07706)
*Yuanyun Zhang,Mingxuan Zhang,Siyuan Li,Zihan Wang,Haoran Chen,Wenbo Zhou,Shi Li*

Main category: cs.LG

TL;DR: Proposes dense feature learning framework using linear algebra properties to improve medical embeddings' structure, showing better transferability and stability than task-specific models.


<details>
  <summary>Details</summary>
Motivation: Task-specific medical AI models underutilize clinical data structure, limiting transferability, stability, and interpretability of learned features.

Method: Dense feature learning framework operating directly on embedding matrices to enforce spectral balance, subspace consistency, and feature orthogonality via linear algebraic objectives.

Result: Achieved higher effective rank, improved conditioning, and greater stability across longitudinal EHR, clinical text, and multimodal data; outperformed baselines in linear performance and robustness.

Conclusion: Learning to span clinical variation is as important as outcome prediction; representation geometry should be a first-class objective in medical AI.

Abstract: Deep learning models for medical data are typically trained using task specific objectives that encourage representations to collapse onto a small number of discriminative directions. While effective for individual prediction problems, this paradigm underutilizes the rich structure of clinical data and limits the transferability, stability, and interpretability of learned features. In this work, we propose dense feature learning, a representation centric framework that explicitly shapes the linear structure of medical embeddings. Our approach operates directly on embedding matrices, encouraging spectral balance, subspace consistency, and feature orthogonality through objectives defined entirely in terms of linear algebraic properties. Without relying on labels or generative reconstruction, dense feature learning produces representations with higher effective rank, improved conditioning, and greater stability across time. Empirical evaluations across longitudinal EHR data, clinical text, and multimodal patient representations demonstrate consistent improvements in downstream linear performance, robustness, and subspace alignment compared to supervised and self supervised baselines. These results suggest that learning to span clinical variation may be as important as learning to predict clinical outcomes, and position representation geometry as a first class objective in medical AI.

</details>


### [68] [Towards Robust Scaling Laws for Optimizers](https://arxiv.org/abs/2602.07712)
*Alexandra Volkova,Mher Safaryan,Christoph H. Lampert,Dan Alistarh*

Main category: cs.LG

TL;DR: 针对多优化器场景，提出共享幂律指数与优化器特定缩放因子的鲁棒缩放定律，解决传统Chinchilla定律参数相关性高的问题，并通过理论分析揭示其源于损失分解。


<details>
  <summary>Details</summary>
Motivation: 现有LLM预训练缩放定律（如Chinchilla）通常固定优化器（如AdamW），而新一代优化器（Muon/Shampoo/SOAP）虽承诺更快收敛，其与模型/数据规模的关系尚不明确，导致跨优化器的计算资源分配缺乏可靠预测工具。

Method: 实证研究不同优化器下的缩放行为：1) 验证独立Chinchilla定律的参数高相关性缺陷；2) 构建共享幂律指数+优化器特定缩放因子的统一模型；3) 通过凸二次目标任务的梯度下降理论分析，从损失分解（不可约/近似/优化误差）推导缩放规律。

Result: 1) 各优化器独立拟合的缩放定律存在严重参数相关性；2) 所提统一模型实现优化器间直接对比，显著提升预测稳定性；3) 理论证实Chinchilla缩放规律源于损失误差分解的固有特性。

Conclusion: 建立了适用于多优化器的鲁棒缩放定律框架，为LLM预训练中优化器选择与计算分配提供理论依据和实用工具，推动高效训练策略设计。

Abstract: The quality of Large Language Model (LLM) pretraining depends on multiple factors, including the compute budget and the choice of optimization algorithm. Empirical scaling laws are widely used to predict loss as model size and training data grow, however, almost all existing studies fix the optimizer (typically AdamW). At the same time, a new generation of optimizers (e.g., Muon, Shampoo, SOAP) promises faster and more stable convergence, but their relationship with model and data scaling is not yet well understood. In this work, we study scaling laws across different optimizers. Empirically, we show that 1) separate Chinchilla-style scaling laws for each optimizer are ill-conditioned and have highly correlated parameters. Instead, 2) we propose a more robust law with shared power-law exponents and optimizer-specific rescaling factors, which enable direct comparison between optimizers. Finally, 3) we provide a theoretical analysis of gradient-based methods for the proxy task of a convex quadratic objective, demonstrating that Chinchilla-style scaling laws emerge naturally as a result of loss decomposition into irreducible, approximation, and optimization errors.

</details>


### [69] [Analyzing and Guiding Zero-Shot Posterior Sampling in Diffusion Models](https://arxiv.org/abs/2602.07715)
*Roi Benita,Michael Elad,Joseph Keshet*

Main category: cs.LG

TL;DR: 本文针对信号逆问题中的零扩散后验采样方法，提出在高斯先验假设下的严格分析框架，推导出理想后验采样器和扩散重建算法的闭式表达，并设计了一种原理性的参数选择方法，替代传统启发式策略，实现感知质量与信号保真度的最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 从退化测量中恢复信号是科学和工程领域的长期挑战。现有零扩散方法虽能进行后验采样，但依赖手动调参和启发式策略，缺乏理论指导，限制了算法性能和可靠性。

Method: 在假设先验为高斯分布的条件下，对近似后验采样器进行严格理论分析，推导出理想后验采样器和扩散重建算法的闭式表达；基于此，提出一种方法无关的参数设计框架，综合考虑先验特性、退化信号和扩散动力学特征。

Result: 推导出的谱域参数推荐方案在结构上与标准启发式方法存在本质差异，且随扩散步长动态变化；实验表明该方法能在感知质量和信号保真度之间实现一致的最佳平衡。

Conclusion: 该框架为零扩散逆问题提供了首个严格理论基础，用数据驱动的参数选择取代了经验性调参，显著提升了算法的可解释性和性能一致性，适用于广泛的信号重建任务。

Abstract: Recovering a signal from its degraded measurements is a long standing challenge in science and engineering. Recently, zero-shot diffusion based methods have been proposed for such inverse problems, offering a posterior sampling based solution that leverages prior knowledge. Such algorithms incorporate the observations through inference, often leaning on manual tuning and heuristics. In this work we propose a rigorous analysis of such approximate posterior-samplers, relying on a Gaussianity assumption of the prior. Under this regime, we show that both the ideal posterior sampler and diffusion-based reconstruction algorithms can be expressed in closed-form, enabling their thorough analysis and comparisons in the spectral domain. Building on these representations, we also introduce a principled framework for parameter design, replacing heuristic selection strategies used to date. The proposed approach is method-agnostic and yields tailored parameter choices for each algorithm, jointly accounting for the characteristics of the prior, the degraded signal, and the diffusion dynamics. We show that our spectral recommendations differ structurally from standard heuristics and vary with the diffusion step size, resulting in a consistent balance between perceptual quality and signal fidelity.

</details>


### [70] [Efficient Planning in Reinforcement Learning via Model Introspection](https://arxiv.org/abs/2602.07719)
*Gabriel Stella*

Main category: cs.LG

TL;DR: 将强化学习与经典规划统一的关键是模拟人类内省能力，本文提出将内省视为程序分析，并基于此开发了关系强化学习的高效目标导向规划算法，建立了两个领域的全新联系。


<details>
  <summary>Details</summary>
Motivation: 强化学习与经典规划长期被视为两个独立问题，但人类能够不受问题表述方式限制，通过内省快速推导出额外信息来高效求解。现有方法缺乏这种统一性，需要建立两个领域之间的桥梁。

Method: 提出将人类内省能力建模为程序分析，将此思路应用于强化学习的各类模型，并针对关系强化学习的模型类设计了专门的规划算法。

Result: 成功开发出可在关系强化学习模型类上进行高效目标导向规划的算法，实现了强化学习与经典规划之间的有效连接。

Conclusion: 通过形式化人类内省机制，本研究为强化学习与经典规划的融合提供了新范式，证明两者可通过程序分析思想统一起来，拓展了AI问题求解的理论框架。

Abstract: Reinforcement learning and classical planning are typically seen as two distinct problems, with differing formulations necessitating different solutions. Yet, when humans are given a task, regardless of the way it is specified, they can often derive the additional information needed to solve the problem efficiently. The key to this ability is introspection: by reasoning about their internal models of the problem, humans directly synthesize additional task-relevant information. In this paper, we propose that this introspection can be thought of as program analysis. We discuss examples of how this approach can be applied to various kinds of models used in reinforcement learning. We then describe an algorithm that enables efficient goal-oriented planning over the class of models used in relational reinforcement learning, demonstrating a novel link between reinforcement learning and classical planning.

</details>


### [71] [ParisKV: Fast and Drift-Robust KV-Cache Retrieval for Long-Context LLMs](https://arxiv.org/abs/2602.07721)
*Yanlin Qi,Xinhang Chen,Huiqiang Jiang,Qitong Wang,Botao Peng,Themis Palpanas*

Main category: cs.LG

TL;DR: ParisKV是一种针对长上下文LLM推理的KV缓存检索框架，通过碰撞候选选择和量化重排序，在保持全注意力质量的同时，实现百万token上下文的17-44倍延迟降低和2.8倍吞吐提升。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存检索方法在处理长上下文时面临分布漂移和延迟高的问题，无法有效扩展到百万token级别，且在全注意力可运行范围内吞吐效率不足。

Method: 提出GPU原生的ParisKV框架，采用碰撞式候选选择机制，结合量化内积重排序估计器，并通过统一虚拟寻址(UVA)支持CPU卸载的KV缓存，实现低开销的按需top-k检索。

Result: 在长输入和长生成基准测试中匹配全注意力质量；在batch size=1时长上下文解码速度匹敌全注意力；在可运行范围内吞吐提升最高2.8倍；百万token场景下相比MagicPIG和PQCache延迟降低17倍和44倍；支持全注意力无法运行的百万token上下文扩展。

Conclusion: ParisKV实现了质量与效率的平衡，为超长上下文LLM推理提供了可扩展的高效解决方案，显著优于现有先进方法。

Abstract: KV-cache retrieval is essential for long-context LLM inference, yet existing methods struggle with distribution drift and high latency at scale. We introduce ParisKV, a drift-robust, GPU-native KV-cache retrieval framework based on collision-based candidate selection, followed by a quantized inner-product reranking estimator. For million-token contexts, ParisKV supports CPU-offloaded KV caches via Unified Virtual Addressing (UVA), enabling on-demand top-$k$ fetching with minimal overhead. ParisKV matches or outperforms full attention quality on long-input and long-generation benchmarks. It achieves state-of-the-art long-context decoding efficiency: it matches or exceeds full attention speed even at batch size 1 for long contexts, delivers up to 2.8$\times$ higher throughput within full attention's runnable range, and scales to million-token contexts where full attention runs out of memory. At million-token scale, ParisKV reduces decode latency by 17$\times$ and 44$\times$ compared to MagicPIG and PQCache, respectively, two state-of-the-art KV-cache Top-$k$ retrieval baselines.

</details>


### [72] [Do We Need Adam? Surprisingly Strong and Sparse Reinforcement Learning with SGD in LLMs](https://arxiv.org/abs/2602.07729)
*Sagnik Mukherjee,Lifan Yuan,Pavan Jayasinha,Dilek Hakkani-Tür,Hao Peng*

Main category: cs.LG

TL;DR: 强化学习中AdamW优化器的优势不如监督微调明显，SGD在LLM强化学习中表现相当甚至更好，且参数更新稀疏性比AdamW高1000多倍。


<details>
  <summary>Details</summary>
Motivation: 尽管RL与SFT存在根本差异，当前RL实践仍沿用SFT的AdamW优化器，但AdamW内存开销大，且其在RL中的重要性尚不明确。

Method: 通过分析RL与SFT在优化动态上的差异，提出假设并实验比较SGD与AdamW在LLM RL中的表现，特别关注参数更新稀疏性。

Result: SGD在RL中匹配或超越AdamW性能，且全量微调时更新参数不到0.02%，稀疏性超过AdamW 1000多倍。

Conclusion: RL的优化动态与SFT不同，AdamW的优势在RL中减弱，RL可实现远超此前认知的参数效率。

Abstract: Reinforcement learning (RL), particularly RL from verifiable reward (RLVR), has become a crucial phase of training large language models (LLMs) and a key focus of current scaling efforts. However, optimization practices in RL largely follow those of next-token prediction stages (e.g., pretraining and supervised fine-tuning), despite fundamental differences between RL and these stages highlighted by recent work. One such practice is the use of the AdamW optimizer, which is widely adopted for training large-scale transformers despite its high memory overhead. Our analysis shows that both momentum and adaptive learning rates in AdamW are less influential in RL than in SFT, leading us to hypothesize that RL benefits less from Adam-style per-parameter adaptive learning rates and momentum. Confirming this hypothesis, our experiments demonstrate that the substantially more memory-efficient SGD, which is known to perform poorly in supervised learning of large-scale transformers, matches or even outperforms AdamW in RL for LLMs. Remarkably, full fine-tuning with SGD updates fewer than 0.02% of model parameters without any sparsity-promoting regularization, more than 1000 times fewer than AdamW. Our analysis offers potential reasons for this update sparsity. These findings provide new insights into the optimization dynamics of RL in LLMs and show that RL can be substantially more parameter-efficient than previously recognized.

</details>


### [73] [The Laplacian Keyboard: Beyond the Linear Span](https://arxiv.org/abs/2602.07730)
*Siddarth Chandrasekar,Marlos C. Machado*

Main category: cs.LG

TL;DR: Proposes Laplacian Keyboard (LK), a hierarchical RL framework using Laplacian eigenvectors to build a task-agnostic option library that can approximate optimal policies beyond linear span constraints, with theoretical error bounds and improved sample efficiency.


<details>
  <summary>Details</summary>
Motivation: Laplacian eigenvectors are widely used in RL for reward function approximation but are typically restricted to their linear span, limiting expressivity for complex environments.

Method: Constructs a library of options from Laplacian eigenvectors (forming a guaranteed behavior basis), then trains a meta-policy to dynamically combine these options to learn policies beyond the original linear constraints.

Result: Provides theoretical bounds on zero-shot approximation error; empirically demonstrates superior zero-shot performance and improved sample efficiency compared to standard RL methods.

Conclusion: LK successfully expands the representational power of Laplacian-based RL by hierarchically composing options, overcoming the linear span limitation while maintaining theoretical guarantees and practical efficiency.

Abstract: Across scientific disciplines, Laplacian eigenvectors serve as a fundamental basis for simplifying complex systems, from signal processing to quantum mechanics. In reinforcement learning (RL), these eigenvectors provide a natural basis for approximating reward functions; however, their use is typically limited to their linear span, which restricts expressivity in complex environments. We introduce the Laplacian Keyboard (LK), a hierarchical framework that goes beyond the linear span. LK constructs a task-agnostic library of options from these eigenvectors, forming a behavior basis guaranteed to contain the optimal policy for any reward within the linear span. A meta-policy learns to stitch these options dynamically, enabling efficient learning of policies outside the original linear constraints. We establish theoretical bounds on zero-shot approximation error and demonstrate empirically that LK surpasses zero-shot solutions while achieving improved sample efficiency compared to standard RL methods.

</details>


### [74] [Efficient Adaptive Data Analysis over Dense Distributions](https://arxiv.org/abs/2602.07732)
*Joon Suk Huh*

Main category: cs.LG

TL;DR: 该论文解决了自适应数据分析（ADA）中计算效率与样本复杂度之间的根本性权衡问题。通过识别一类相对于已知先验呈“密集”分布的自然数据分布，提出了一种计算高效的ADA机制，在达到最优O(log T)样本复杂度的同时保持计算可行性，并满足PSO安全隐私定义，为分布特定学习提供了样本高效的统计查询预言机。


<details>
  <summary>Details</summary>
Motivation: 现代数据工作流具有内在的自适应性，但反复查询同一数据集会导致过拟合和统计推断失效。现有自适应数据分析机制面临根本性矛盾：计算高效算法的样本复杂度为次优的O(√T)，而统计最优的O(log T)算法在标准密码学假设下计算不可行。亟需打破这一效率-精度权衡。

Method: 识别并聚焦于一类相对于已知先验呈“密集”分布的数据分布类，针对该设定设计计算高效的ADA机制。利用分布的结构特性，避免通用设置下的计算复杂性障碍。

Result: 提出的机制在数据分布相对于已知先验密集时，同时实现计算高效性和最优O(log T)样本复杂度。该结果涵盖了分布特定学习中的特征-标签数据分布，并由此得到了分布特定设定下样本高效的统计查询预言机。机制虽非基于差分隐私，但满足谓词单点化（PSO）安全。

Conclusion: 研究表明在符合现实的结构化分布假设下，自适应数据分析中可以同时实现计算可行性与统计最优性，消解了二者间的内在张力。这一发现揭示了自适应分析与隐私（超越差分隐私）的深层联系，为开发实用的高效ADA算法提供了理论基础。

Abstract: Modern data workflows are inherently adaptive, repeatedly querying the same dataset to refine and validate sequential decisions, but such adaptivity can lead to overfitting and invalid statistical inference. Adaptive Data Analysis (ADA) mechanisms address this challenge; however, there is a fundamental tension between computational efficiency and sample complexity. For $T$ rounds of adaptive analysis, computationally efficient algorithms typically incur suboptimal $O(\sqrt{T})$ sample complexity, whereas statistically optimal $O(\log T)$ algorithms are computationally intractable under standard cryptographic assumptions. In this work, we shed light on this trade-off by identifying a natural class of data distributions under which both computational efficiency and optimal sample complexity are achievable. We propose a computationally efficient ADA mechanism that attains optimal $O(\log T)$ sample complexity when the data distribution is dense with respect to a known prior. This setting includes, in particular, feature--label data distributions arising in distribution-specific learning. As a consequence, our mechanism also yields a sample-efficient (i.e., $O(\log T)$ samples) statistical query oracle in the distribution-specific setting. Moreover, although our algorithm is not based on differential privacy, it satisfies a relaxed privacy notion known as Predicate Singling Out (PSO) security (Cohen and Nissim, 2020). Our results thus reveal an inherent connection between adaptive data analysis and privacy beyond differential privacy.

</details>


### [75] [Learnable Chernoff Baselines for Inference-Time Alignment](https://arxiv.org/abs/2602.07738)
*Sunil Madhow,Yuchen Liang,Ness Shroff,Yingbin Liang,Yu-Xiang Wang*

Main category: cs.LG

TL;DR: 本文提出Learnable Chernoff Baselines (LCBs)，一种高效推理时奖励对齐方法。通过可学习的Chernoff基线实现近似拒绝采样，仅用黑盒访问即可大幅减少计算开销，同时保证理论收敛性。


<details>
  <summary>Details</summary>
Motivation: 现有奖励引导对齐方法存在架构依赖性强或计算成本高昂的问题，限制了其在生成模型中的高效应用。需要一种既能保持通用性又能控制推理计算量的解决方案。

Method: 提出Learnable Chernoff Baselines (LCBs)，利用黑盒采样访问预训练模型，采用自适应接受概率的拒绝采样机制来近似KL正则化奖励对齐中的指数倾斜分布，实现推理计算量的细粒度控制。

Result: 理论证明了到理想对齐模型的总变差距离保证；在连续和离散扩散实验中，LCB采样效果接近理想拒绝采样，且显著减少了对预训练模型的查询次数。

Conclusion: LCBs为推理时奖励对齐提供了高效且理论可靠的方案，在保持采样质量的同时实现计算效率的大幅提升，为生成模型的实时对齐开辟了新途径。

Abstract: We study inference-time reward-guided alignment for generative models. Existing methods often rely on either architecture-specific adaptations or computationally costly inference procedures. We introduce Learnable Chernoff Baselines (LCBs) as a method for efficiently and approximately sampling from the exponentially tilted kernels that arise from KL-regularized reward alignment. Using only black-box sampling access to the pretrained model, LCBs implement a form of rejection sampling with adaptively selected acceptance probabilities, which allows fine-grained control over inference-compute scaling. We establish total-variation guarantees to the ideal aligned model, and demonstrate in both continuous and discrete diffusion settings that LCB sampling closely matches ideal rejection sampling while using substantially fewer queries to the pretrained model.

</details>


### [76] [MaD-Mix: Multi-Modal Data Mixtures via Latent Space Coupling for Vision-Language Model Training](https://arxiv.org/abs/2602.07790)
*Wanyun Xie,Francesco Tonin,Volkan Cevher*

Main category: cs.LG

TL;DR: 提出MaD-Mix框架，自动推导多模态数据混合比例以训练视觉语言模型，替代昂贵的手动调优


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型训练依赖成本高昂的手动数据混合调优，在复杂多模态域中效率低下且不切实际

Method: 将数据混合建模为模态感知域对齐最大化问题，通过Fenchel对偶和模态间耦合变量获得闭式对齐分数，系统化支持缺失模态域

Result: 在0.5B和7B模型上，图像-文本指令调优速度提升22%，三模态视频-图像-文本场景准确率超越均匀权重，计算开销不足1 GPU小时

Conclusion: MaD-Mix为现代VLM流水线提供可扩展的自动混合设计，显著提升训练效率和实用性

Abstract: Vision-Language Models (VLMs) are typically trained on a diverse set of multi-modal domains, yet current practices rely on costly manual tuning. We propose MaD-Mix, a principled and computationally efficient framework that derives multi-modal data mixtures for VLM training. MaD-Mix formulates data mixing as modality-aware domain alignment maximization and obtains closed-form multi-modal alignment scores from the Fenchel dual through inter-modal coupling variables. MaD-Mix systematically handles domains with missing modalities, allowing for the integration of language-only domains. Empirical evaluations across 0.5B and 7B models demonstrate that MaD-Mix accelerates VLM training across diverse benchmarks. MaD-Mix matches human-tuned data mixtures using 22% fewer training steps in image-text instruction tuning. In complex tri-modal video-image-text scenarios, where manual tuning becomes impractical, MaD-Mix boosts average accuracy over uniform weights, with negligible mixture computation overhead (< 1 GPU-hour), enabling scalable mixture design for modern VLM pipelines.

</details>


### [77] [CausalTAD: Injecting Causal Knowledge into Large Language Models for Tabular Anomaly Detection](https://arxiv.org/abs/2602.07798)
*Ruiqi Wang,Ruikang Liu,Runyu Chen,Haoxiang Suo,Zhiyi Peng,Zhuo Tang,Changjian Chen*

Main category: cs.LG

TL;DR: 提出CausalTaD方法，通过将表格数据列按因果关系重排序并加权注入大语言模型，显著提升表格异常检测性能


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的表格异常检测方法在转换数据时随机排列列顺序，忽略了列间因果关系，而因果关系对准确检测异常至关重要

Method: 1) 识别列间因果关系并重排序以对齐因果链（线性排序问题） 2) 设计重加权策略为不同列分配权重以强化因果效应

Result: 在30多个数据集上实验表明，该方法持续优于当前最优方法

Conclusion: 将因果知识注入大语言模型是提升表格异常检测效果的有效途径

Abstract: Detecting anomalies in tabular data is critical for many real-world applications, such as credit card fraud detection. With the rapid advancements in large language models (LLMs), state-of-the-art performance in tabular anomaly detection has been achieved by converting tabular data into text and fine-tuning LLMs. However, these methods randomly order columns during conversion, without considering the causal relationships between them, which is crucial for accurately detecting anomalies. In this paper, we present CausalTaD, a method that injects causal knowledge into LLMs for tabular anomaly detection. We first identify the causal relationships between columns and reorder them to align with these causal relationships. This reordering can be modeled as a linear ordering problem. Since each column contributes differently to the causal relationships, we further propose a reweighting strategy to assign different weights to different columns to enhance this effect. Experiments across more than 30 datasets demonstrate that our method consistently outperforms the current state-of-the-art methods. The code for CausalTAD is available at https://github.com/350234/CausalTAD.

</details>


### [78] [Fairness Aware Reward Optimization](https://arxiv.org/abs/2602.07799)
*Ching Lam Choi,Vighnesh Subramaniam,Phillip Isola,Antonio Torralba,Stefanie Jegelka*

Main category: cs.LG

TL;DR: The paper proposes Faro, a fairness-aware reward optimization framework that mitigates demographic bias in LLM alignment by training reward models under fairness constraints, providing both theoretical guarantees and empirical improvements in fairness without compromising model quality.


<details>
  <summary>Details</summary>
Motivation: Demographic imbalances in human preference data lead to systematic unfairness being encoded in reward models, which then propagates to aligned LLMs, resulting in biased and harmful outputs.

Method: Faro is an in-processing framework that incorporates three types of fairness constraints (demographic parity, equalized odds, counterfactual fairness) during reward model training, ensuring the resulting reward models are simultaneously ordinal (correct ranking), cardinal (calibrated), and fair.

Result: The method provides: (i) provable fairness certificates with controllable slack, (ii) formal proof that fairness transfers from reward models to policies during KL-regularized fine-tuning, and (iii) demonstration of a non-empty Pareto frontier. Empirically, Faro significantly reduces bias and harmful generations while maintaining or improving model quality across multiple LLMs and benchmarks.

Conclusion: Training reward models with in-process fairness constraints is a theoretically-grounded and empirically effective approach to mitigate demographic bias in aligned LLMs, outperforming traditional pre- and post-processing fairness methods.

Abstract: Demographic skews in human preference data propagate systematic unfairness through reward models into aligned LLMs. We introduce Fairness Aware Reward Optimization (Faro), an in-processing framework that trains reward models under demographic parity, equalized odds, or counterfactual fairness constraints. We provide the first theoretical analysis of reward-level fairness in LLM alignment, establishing: (i) provable fairness certificates for Faro-trained rewards with controllable slack; a (ii) formal characterization of the accuracy-fairness trade-off induced by KL-regularized fine-tuning, proving fairness transfers from reward to policy; and the (iii) existence of a non-empty Pareto frontier. Unlike pre- and post-processing methods, Faro ensures reward models are simultaneously ordinal (ranking correctly), cardinal (calibrated), and fair. Across multiple LLMs and benchmarks, Faro significantly reduces bias and harmful generations while maintaining or improving model quality.

</details>


### [79] [Approximating Matrix Functions with Deep Neural Networks and Transformers](https://arxiv.org/abs/2602.07800)
*Rahul Padmanabhan,Simone Brugiapaglia*

Main category: cs.LG

TL;DR: This paper explores using transformers and ReLU networks to approximate matrix functions, proving theoretical bounds for ReLU networks and demonstrating transformers' practical effectiveness with proper numerical encodings.


<details>
  <summary>Details</summary>
Motivation: While transformers excel in natural language processing, their application to numerical computation (e.g., matrix functions critical in scientific computing like matrix exponential for Markov chains) remains understudied.

Method: Two-pronged approach: (1) Theoretical analysis of ReLU network width/depth bounds for approximating matrix exponential to arbitrary precision; (2) Experimental evaluation of transformer encoder-decoder architectures with specialized numerical encodings for matrix function approximation.

Result: ReLU networks have provable width/depth bounds for matrix exponential approximation; transformers achieve ~5% relative error with high probability, but performance heavily depends on the choice of numerical encoding scheme tailored to specific functions.

Conclusion: Transformers can effectively approximate matrix functions in scientific computing when combined with appropriate numerical encodings, bridging a gap between modern AI architectures and traditional computational mathematics.

Abstract: Transformers have revolutionized natural language processing, but their use for numerical computation has received less attention. We study the approximation of matrix functions, which map scalar functions to matrices, using neural networks including transformers. We focus on functions mapping square matrices to square matrices of the same dimension. These types of matrix functions appear throughout scientific computing, e.g., the matrix exponential in continuous-time Markov chains and the matrix sign function in stability analysis of dynamical systems. In this paper, we make two contributions. First, we prove bounds on the width and depth of ReLU networks needed to approximate the matrix exponential to an arbitrary precision. Second, we show experimentally that a transformer encoder-decoder with suitable numerical encodings can approximate certain matrix functions at a relative error of 5% with high probability. Our study reveals that the encoding scheme strongly affects performance, with different schemes working better for different functions.

</details>


### [80] [Efficient Representations are Controllable Representations](https://arxiv.org/abs/2602.07828)
*Charles Ye,Jasmine Cui*

Main category: cs.LG

TL;DR: 通过简单的辅助损失函数微调LLM，将16个残差流维度训练为概念指示标志，使模型自发依赖这些可解释的控制开关进行生成。


<details>
  <summary>Details</summary>
Motivation: 现有控制LLM内部概念表示的方法通常需要复杂步骤来识别和干预特征几何结构，本研究寻求更直接的替代方案。

Method: 对LLM进行微调，添加简单辅助损失，强制训练3072个残差流维度中的16个成为概念指示标志，观察模型如何重新组织内部表示。

Result: 这些被训练的"惰性标志"成为真正的内部特征：可解释的控制开关，可在推理时引导文本生成，模型主动依赖这些维度进行任务。

Conclusion: 当特征被稳定提供在固定位置时，梯度下降会消除其他冗余编码，模型会自我侵蚀替代表示，利用模型效率压力可诱导可解释可控的表示。

Abstract: What is the most brute-force way to install interpretable, controllable features into a model's activations? Controlling how LLMs internally represent concepts typically requires sophisticated methods to first identify, then intervene on the model's existing feature geometry. We bypass all of this.
  We finetune an LLM with a simple auxiliary loss, training 16 of its 3072 residual stream dimensions to be inert interpretability flags that simply indicate what concepts are required for generation. The model reorganizes around them anyway, learning to rely on these flags during actual generation tasks. As a result, these inert flags become genuine internal features: interpretable control switches that allow us to steer generation at inference time. Why does this work? When a feature is reliably supplied at a fixed location, gradient descent gradually eliminates redundant encodings elsewhere, and the model erodes its own alternative representations. A model's efficiency pressure is a lever - exploitable to induce interpretable, controllable representations.

</details>


### [81] [rePIRL: Learn PRM with Inverse RL for LLM Reasoning](https://arxiv.org/abs/2602.07832)
*Xian Wu,Kaijie Zhu,Ying Zhang,Lun Wang,Wenbo Guo*

Main category: cs.LG

TL;DR: rePIRL是一种受逆强化学习启发的框架，通过双策略-PRM交替学习以最小假设学习有效的LLM推理过程奖励模型，在数学和编码任务上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有过程奖励模型(PRM)学习方法要么依赖关于专家策略的强假设（如需要其奖励函数），要么受内在限制（如熵崩溃），导致PRM能力弱或泛化性有限。

Method: 提出rePIRL框架，采用逆强化学习思想，设计双学习过程交替更新策略和PRM，并定制技术以解决传统逆RL扩展到LLM的挑战。

Result: 在标准化数学和编码推理数据集上的实证评估显示rePIRL优于现有方法，成功应用于测试时训练、测试时缩放及为困难问题提供训练早期信号。

Conclusion: rePIRL提供了一个有效的PRM学习框架，能统一在线和离线学习方法且假设最少，通过理论分析和实证结果得到验证。

Abstract: Process rewards have been widely used in deep reinforcement learning to improve training efficiency, reduce variance, and prevent reward hacking. In LLM reasoning, existing works also explore various solutions for learning effective process reward models (PRM) with or without the help of an expert policy. However, existing methods either rely on strong assumptions about the expert policies (e.g., requiring their reward functions) or suffer intrinsic limitations (e.g., entropy collapse), resulting in weak PRMs or limited generalizability. In this paper, we introduce rePIRL, an inverse RL-inspired framework that learns effective PRMs with minimal assumptions about expert policies. Specifically, we design a dual learning process that updates the policy and the PRM interchangeably. Our learning algorithm has customized techniques to address the challenges of scaling traditional inverse RL to LLMs. We theoretically show that our proposed learning framework can unify both online and offline PRM learning methods, justifying that rePIRL can learn PRMs with minimal assumptions. Empirical evaluations on standardized math and coding reasoning datasets demonstrate the effectiveness of rePIRL over existing methods. We further show the application of our trained PRM in test-time training, test-time scaling, and providing an early signal for training hard problems. Finally, we validate our training recipe and key design choices via a detailed ablation study.

</details>


### [82] [MARTI-MARS$^2$: Scaling Multi-Agent Self-Search via Reinforcement Learning for Code Generation](https://arxiv.org/abs/2602.07848)
*Shijie Wang,Pengfei Li,Yikun Fu,Kaifeng Liu,Fangyuan Li,Yang Liu,Xiaowei Sun,Zonglin Li,Siyao Zhao,Jian Zhao,Kai Tian,Dong Li,Junqi Gao,Yutong Zhang,Yiqun Chen,Yuqiang Li,Zoe Li,Weinan Zhang,Peng Ye,Shuyue Hu,Lei Bai,Bowen Zhou,Kaiyan Zhang,Biqing Qi*

Main category: cs.LG

TL;DR: 该论文提出MARTI-MARS2框架，通过将多智能体协作建模为可学习环境，实现了从参数共享的同质多角色训练到异构多智能体训练的演进。该方法在代码生成任务上表现优异，两个32B模型协作达到77.7%，超越GPT-5.1，并揭示了策略多样性对扩展智能的关键作用。


<details>
  <summary>Details</summary>
Motivation: 单智能体系统在代码生成等复杂任务中存在性能瓶颈，现有多智能体框架依赖提示交互或同质参数训练，限制了错误修正能力和策略多样性。多智能体协作有望突破单智能体能力上限。

Method: 提出MARTI-MARS2（多智能体强化训练与推理框架），将多智能体协作探索过程形式化为动态可学习环境，实现策略学习与多智能体树搜索的融合。框架支持智能体迭代探索优化，支持从参数共享的同质多角色训练演进到异构多智能体训练，并设计MARTI-MARS2-T+高效推理策略。

Result: 在8B/14B/32B模型尺度上进行代码生成实验，两个32B模型协作达到77.7%性能，超越GPT-5.1等强基线。揭示新扩展定律：单智能体→同质多角色→异构多智能体的范式转变能持续提升强化学习性能上限、鲁棒性和策略多样性。

Conclusion: MARTI-MARS2成功突破单智能体能力限制，证明策略多样性是扩展智能的关键。框架实现了训练与推理的协同优化，为多智能体强化学习提供了可扩展的新范式。

Abstract: While the complex reasoning capability of Large Language Models (LLMs) has attracted significant attention, single-agent systems often encounter inherent performance ceilings in complex tasks such as code generation. Multi-agent collaboration offers a promising avenue to transcend these boundaries. However, existing frameworks typically rely on prompt-based test-time interactions or multi-role configurations trained with homogeneous parameters, limiting error correction capabilities and strategic diversity. In this paper, we propose a Multi-Agent Reinforced Training and Inference Framework with Self-Search Scaling (MARTI-MARS2), which integrates policy learning with multi-agent tree search by formulating the multi-agent collaborative exploration process as a dynamic and learnable environment. By allowing agents to iteratively explore and refine within the environment, the framework facilitates evolution from parameter-sharing homogeneous multi-role training to heterogeneous multi-agent training, breaking through single-agent capability limits. We also introduce an efficient inference strategy MARTI-MARS2-T+ to fully exploit the scaling potential of multi-agent collaboration at test time. We conduct extensive experiments across varied model scales (8B, 14B, and 32B) on challenging code generation benchmarks. Utilizing two collaborating 32B models, MARTI-MARS2 achieves 77.7%, outperforming strong baselines like GPT-5.1. Furthermore, MARTI-MARS2 reveals a novel scaling law: shifting from single-agent to homogeneous multi-role and ultimately to heterogeneous multi-agent paradigms progressively yields higher RL performance ceilings, robust TTS capabilities, and greater policy diversity, suggesting that policy diversity is critical for scaling intelligence via multi-agent reinforcement learning.

</details>


### [83] [Dynamic Load Model for Data Centers with Pattern-Consistent Calibration](https://arxiv.org/abs/2602.07859)
*Siyu Lu,Chenhan Xiao,Yang Weng*

Main category: cs.LG

TL;DR: A hybrid physics-based/data-driven framework using Temporal Contrastive Learning (TCL) to calibrate large electronic load models from real data centers, revealing that load interactions cause complex post-disturbance dynamics not captured by conventional models.


<details>
  <summary>Details</summary>
Motivation: Conventional load models fail to capture the fast variability and protection-driven disconnection/reconnection behavior of data center loads; physics-based models lack calibration while data-driven methods overfit, necessitating a hybrid approach for accurate grid planning.

Method: Parameterize physics-based load structure for data-driven calibration using Temporal Contrastive Learning to align temporal/statistical patterns (not trajectories) from facility operational data; preserve privacy by sharing only calibrated parameters.

Result: Model calibrated on MIT Supercloud, ASU Sol, Blue Waters, and ASHRAE datasets; integration into ANDES platform shows LEL interactions fundamentally alter post-disturbance recovery with compound disconnection-reconnection dynamics and delayed stabilization.

Conclusion: The proposed framework successfully captures realistic data center load behavior for facility-level grid planning while preserving data privacy, demonstrating that uncalibrated models miss critical dynamics arising from LEL interactions.

Abstract: The rapid growth of data centers has made large electronic load (LEL) modeling increasingly important for power system analysis. Such loads are characterized by fast workload-driven variability and protection-driven disconnection and reconnection behavior that are not captured by conventional load models. Existing data center load modeling includes physics-based approaches, which provide interpretable structure for grid simulation, and data-driven approaches, which capture empirical workload variability from data. However, physics-based models are typically uncalibrated to facility-level operation, while trajectory alignment in data-driven methods often leads to overfitting and unrealistic dynamic behavior. To resolve these limitations, we design the framework to leverage both physics-based structure and data-driven adaptability. The physics-based structure is parameterized to enable data-driven pattern-consistent calibration from real operational data, supporting facility-level grid planning. We further show that trajectory-level alignment is limited for inherently stochastic data center loads. Therefore, we design the calibration to align temporal and statistical patterns using temporal contrastive learning (TCL). This calibration is performed locally at the facility, and only calibrated parameters are shared with utilities, preserving data privacy. The proposed load model is calibrated by real-world operational load data from the MIT Supercloud, ASU Sol, Blue Waters, and ASHRAE datasets. Then it is integrated into the ANDES platform and evaluated on the IEEE 39-bus, NPCC 140-bus, and WECC 179-bus systems. We find that interactions among LELs can fundamentally alter post-disturbance recovery behavior, producing compound disconnection-reconnection dynamics and delayed stabilization that are not captured by uncalibrated load models.

</details>


### [84] [Direct Soft-Policy Sampling via Langevin Dynamics](https://arxiv.org/abs/2602.07873)
*Donghyeon Ki,Hee-Jun Ahn,Kyungyoon Kim,Byung-Jun Lee*

Main category: cs.LG

TL;DR: This paper proposes Noise-Conditioned Langevin Q-Learning (NC-LQL), a method that directly samples soft policies from Q-functions using Langevin dynamics with multi-scale noise conditioning, achieving competitive performance on MuJoCo benchmarks.


<details>
  <summary>Details</summary>
Motivation: Soft policies are theoretically principled for balancing exploration-exploitation but practically challenging to implement; existing parametric policies lack expressivity while diffusion-based policies suffer from intractable likelihoods that hinder entropy estimation.

Method: The authors introduce Langevin Q-Learning (LQL) that samples actions from the Boltzmann distribution using Q-function action gradients via Langevin dynamics. To address slow mixing in high-dimensional Q-landscapes, they propose NC-LQL which integrates multi-scale noise perturbations into a noise-conditioned Q-function, creating progressively smoothed value landscapes for global-to-local exploration.

Result: On OpenAI Gym MuJoCo benchmarks, NC-LQL achieves competitive performance compared to state-of-the-art diffusion-based methods.

Conclusion: NC-LQL provides a simple yet powerful solution for online RL that directly realizes soft-policy sampling without explicit policy parameterization, overcoming limitations of existing approaches.

Abstract: Soft policies in reinforcement learning define policies as Boltzmann distributions over state-action value functions, providing a principled mechanism for balancing exploration and exploitation. However, realizing such soft policies in practice remains challenging. Existing approaches either depend on parametric policies with limited expressivity or employ diffusion-based policies whose intractable likelihoods hinder reliable entropy estimation in soft policy objectives. We address this challenge by directly realizing soft-policy sampling via Langevin dynamics driven by the action gradient of the Q-function. This perspective leads to Langevin Q-Learning (LQL), which samples actions from the target Boltzmann distribution without explicitly parameterizing the policy. However, directly applying Langevin dynamics suffers from slow mixing in high-dimensional and non-convex Q-landscapes, limiting its practical effectiveness. To overcome this, we propose Noise-Conditioned Langevin Q-Learning (NC-LQL), which integrates multi-scale noise perturbations into the value function. NC-LQL learns a noise-conditioned Q-function that induces a sequence of progressively smoothed value landscapes, enabling sampling to transition from global exploration to precise mode refinement. On OpenAI Gym MuJoCo benchmarks, NC-LQL achieves competitive performance compared to state-of-the-art diffusion-based methods, providing a simple yet powerful solution for online RL.

</details>


### [85] [Harpoon: Generalised Manifold Guidance for Conditional Tabular Diffusion](https://arxiv.org/abs/2602.07875)
*Aditya Shankar,Yuandou Wang,Rihan Hai,Lydia Y. Chen*

Main category: cs.LG

TL;DR: This paper proposes HARPOON, a tabular diffusion method that extends manifold theory to discrete tabular data, enabling flexible inference-time conditional generation by guiding unconstrained samples along data manifold geometry to satisfy diverse constraints like imputation and inequality conditions without retraining.


<details>
  <summary>Details</summary>
Motivation: Existing conditional tabular generation methods rely on training-time strategies that cannot generalize to unseen constraints during inference and are limited to simple tasks like imputation. Current manifold theory approaches are restricted to continuous domains and specific objectives, creating a gap for principled, flexible conditional generation in tabular data.

Method: The authors extend manifold theory from continuous to tabular domains and introduce HARPOON, a diffusion-based model that learns the data manifold. During inference, it guides unconstrained generated samples along this manifold geometry to satisfy arbitrary user-specified constraints through a projection mechanism, enabling zero-shot adaptation to diverse conditions without retraining.

Result: Empirical validation on imputation and inequality constraint enforcement tasks across diverse datasets demonstrates HARPOON's strong performance. The method successfully handles unseen constraints during inference and shows the practical benefits of manifold-aware guidance for controlled tabular data synthesis.

Conclusion: Extending manifold theory to tabular data provides a principled foundation for flexible conditional generation. HARPOON successfully enables inference-time constraint satisfaction across diverse tasks, establishing manifold-aware guidance as an effective approach for controlled tabular data generation that generalizes beyond training constraints.

Abstract: Generating tabular data under conditions is critical to applications requiring precise control over the generative process. Existing methods rely on training-time strategies that do not generalise to unseen constraints during inference, and struggle to handle conditional tasks beyond tabular imputation. While manifold theory offers a principled way to guide generation, current formulations are tied to specific inference-time objectives and are limited to continuous domains. We extend manifold theory to tabular data and expand its scope to handle diverse inference-time objectives. On this foundation, we introduce HARPOON, a tabular diffusion method that guides unconstrained samples along the manifold geometry to satisfy diverse tabular conditions at inference. We validate our theoretical contributions empirically on tasks such as imputation and enforcing inequality constraints, demonstrating HARPOON'S strong performance across diverse datasets and the practical benefits of manifold-aware guidance for tabular data. Code URL: https://github.com/adis98/Harpoon

</details>


### [86] [Efficient Anti-exploration via VQVAE and Fuzzy Clustering in Offline Reinforcement Learning](https://arxiv.org/abs/2602.07889)
*Long Chen,Yinkui Liu,Shen Li,Bo Tang,Xuemin Hu*

Main category: cs.LG

TL;DR: 提出一种基于VQVAE和模糊聚类的离线强化学习抗探索方法，通过多码本VQVAE离散化状态-动作对解决维度灾难，利用模糊C均值聚类更新码本减少信息损失，在D4RL基准测试中性能优于SOTA且计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 现有伪计数抗探索方法在离散化连续状态-动作对时面临维度灾难和信息损失问题，导致学习效率低下和性能下降，甚至策略学习失败。

Method: 提出基于多码本VQVAE的伪计数方法进行离散化，设计模糊C均值聚类码本更新机制，构建离线RL抗探索策略。

Result: 在D4RL基准测试中，该方法在多个复杂任务上性能优于SOTA方法，且计算成本更低。

Conclusion: 该方法有效解决了维度灾难和信息损失问题，提高了离线强化学习的学习效率和性能。

Abstract: Pseudo-count is an effective anti-exploration method in offline reinforcement learning (RL) by counting state-action pairs and imposing a large penalty on rare or unseen state-action pair data. Existing anti-exploration methods count continuous state-action pairs by discretizing these data, but often suffer from the issues of dimension disaster and information loss in the discretization process, leading to efficiency and performance reduction, and even failure of policy learning. In this paper, a novel anti-exploration method based on Vector Quantized Variational Autoencoder (VQVAE) and fuzzy clustering in offline RL is proposed. We first propose an efficient pseudo-count method based on the multi-codebook VQVAE to discretize state-action pairs, and design an offline RL anti-exploitation method based on the proposed pseudo-count method to handle the dimension disaster issue and improve the learning efficiency. In addition, a codebook update mechanism based on fuzzy C-means (FCM) clustering is developed to improve the use rate of vectors in codebooks, addressing the information loss issue in the discretization process. The proposed method is evaluated on the benchmark of Datasets for Deep Data-Driven Reinforcement Learning (D4RL), and experimental results show that the proposed method performs better and requires less computing cost in multiple complex tasks compared to state-of-the-art (SOTA) methods.

</details>


### [87] [Safety Alignment as Continual Learning: Mitigating the Alignment Tax via Orthogonal Gradient Projection](https://arxiv.org/abs/2602.07892)
*Guanglong Sun,Siyuan Zhang,Liyuan Wang,Jun Zhu,Hang Su,Yi Zhong*

Main category: cs.LG

TL;DR: Proposes OGPSA, a lightweight method that treats LLM safety alignment as a continual learning problem, using orthogonal gradient projection to minimize interference with pre-trained general capabilities while improving safety.


<details>
  <summary>Details</summary>
Motivation: Safety alignment in LLMs often incurs an "alignment tax" where safety post-training reduces general utility (e.g., reasoning/coding). This tax arises from sequential alignment causing catastrophic forgetting of pre-trained competencies.

Method: Frames safety alignment as a continual learning problem requiring balance between plasticity (learning safety) and stability (preserving general abilities). Introduces Orthogonal Gradient Projection for Safety Alignment (OGPSA) which: (1) estimates a low-rank "capability subspace" from gradients on a small reference set, (2) projects safety gradients onto its orthogonal complement before updating. This constrains safety updates to minimally perturb prior knowledge.

Result: Consistently improves the safety-utility Pareto frontier across SFT, DPO, and SFT→DPO settings. On Qwen2.5-7B-Instruct (SFT→DPO), OGPSA significantly recovers general capability: SimpleQA accuracy jumps from 0.53% to 3.03%, IFEval from 51.94% to 63.96%, while maintaining strong safety.

Conclusion: OGPSA effectively mitigates the alignment tax by treating safety alignment as a continual learning challenge. Its orthogonal gradient projection mechanism preserves pre-trained capabilities during safety updates without requiring large-scale replay, auxiliary objectives, or retraining, offering a practical plug-and-play solution for standard post-training pipelines.

Abstract: Large Language Models (LLMs) often incur an alignment tax: safety post-training can reduce general utility (e.g., reasoning and coding). We argue that this tax primarily arises from continual-learning-style forgetting in sequential alignment, where distribution shift and conflicting objectives cause safety updates to overwrite pre-trained competencies. Accordingly, we cast safety alignment as a continual learning (CL) problem that must balance plasticity (acquiring safety constraints) and stability (preserving general abilities). We propose Orthogonal Gradient Projection for Safety Alignment (OGPSA), a lightweight method that mitigates interference by constraining each safety update to be orthogonal (in a first-order sense) to a learned subspace capturing general capabilities. Specifically, OGPSA estimates a low-rank capability subspace from gradients on a small reference set and projects the safety gradient onto its orthogonal complement before updating. This produces safety-directed updates that minimally perturb prior knowledge while retaining capacity for alignment. OGPSA is plug-and-play and integrates into standard post-training pipelines without large-scale replay, auxiliary objectives, or retraining. Across Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and sequential SFT$\rightarrow$DPO settings, OGPSA consistently improves the safety--utility Pareto frontier over standard baselines. For instance, on Qwen2.5-7B-Instruct under SFT$\rightarrow$DPO, OGPSA preserves strong safety while recovering general capability, improving SimpleQA from 0.53\% to 3.03\% and IFEval from 51.94\% to 63.96\%. Our source code is available at \href{https://github.com/SunGL001/OGPSA}{OGPSA}

</details>


### [88] [Adaptive Acquisition Selection for Bayesian Optimization with Large Language Models](https://arxiv.org/abs/2602.07904)
*Giang Ngo,Dat Phan Trong,Dang Nguyen,Sunil Gupta,Svetha Venkatesh*

Main category: cs.LG

TL;DR: LMABO: A novel framework that uses pre-trained LLMs as zero-shot strategists to dynamically select acquisition functions in Bayesian Optimization, outperforming existing methods across 50 benchmarks.


<details>
  <summary>Details</summary>
Motivation: Bayesian Optimization's performance critically depends on acquisition function selection, but the optimal choice is problem-dependent and non-stationary. Existing adaptive methods only use past function values while ignoring richer contextual information like remaining budget and surrogate model characteristics, limiting their adaptability.

Method: LMABO casts a pre-trained LLM as an online decision-maker that receives a structured state representation of the current optimization progress (including budget, surrogate model state, etc.) at each iteration, prompting it to select the most suitable acquisition function from a diverse portfolio for zero-shot adaptive strategy generation.

Result: Evaluated across 50 benchmark problems, LMABO demonstrates significant performance improvements over strong static baselines, adaptive portfolio methods, and other LLM-based approaches, proving its effectiveness across diverse optimization scenarios.

Conclusion: The LLM functions as a comprehensive strategist that synthesizes complete optimization state information into effective adaptive policies, demonstrating that LLMs can serve as powerful zero-shot decision-makers for complex optimization by leveraging their reasoning capabilities on structured representations.

Abstract: Bayesian Optimization critically depends on the choice of acquisition function, but no single strategy is universally optimal; the best choice is non-stationary and problem-dependent. Existing adaptive portfolio methods often base their decisions on past function values while ignoring richer information like remaining budget or surrogate model characteristics. To address this, we introduce LMABO, a novel framework that casts a pre-trained Large Language Model (LLM) as a zero-shot, online strategist for the BO process. At each iteration, LMABO uses a structured state representation to prompt the LLM to select the most suitable acquisition function from a diverse portfolio. In an evaluation across 50 benchmark problems, LMABO demonstrates a significant performance improvement over strong static, adaptive portfolio, and other LLM-based baselines. We show that the LLM's behavior is a comprehensive strategy that adapts to real-time progress, proving its advantage stems from its ability to process and synthesize the complete optimization state into an effective, adaptive policy.

</details>


### [89] [AceGRPO: Adaptive Curriculum Enhanced Group Relative Policy Optimization for Autonomous Machine Learning Engineering](https://arxiv.org/abs/2602.07906)
*Yuzhu Cai,Zexi Liu,Xinyu Zhu,Cheng Wang,Jiaao Chen,Hanrui Wang,Wei-Chen Wang,Di Jin,Siheng Chen*

Main category: cs.LG

TL;DR: The paper proposes AceGRPO, an RL-based method with an Evolving Data Buffer and Adaptive Sampling using Learnability Potential, to overcome behavioral stagnation in LLM-based MLE agents, achieving strong performance with Ace-30B model.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based agents for Autonomous Machine Learning Engineering (MLE) suffer from behavioral stagnation due to frozen parameters. While Reinforcement Learning (RL) could provide a solution, its application is hindered by prohibitive execution latency and inefficient data selection.

Method: The paper proposes AceGRPO with two key components: (1) an Evolving Data Buffer that continuously repurposes execution traces into reusable training tasks, and (2) Adaptive Sampling guided by a Learnability Potential function that dynamically prioritizes tasks at the agent's learning frontier to maximize learning efficiency.

Result: Using AceGRPO, the trained Ace-30B model achieved a 100% valid submission rate on MLE-Bench-Lite, approached the performance of proprietary frontier models, and outperformed larger open-source baselines such as DeepSeek-V3.2.

Conclusion: AceGRPO demonstrates robust capability for sustained iterative optimization in autonomous MLE, effectively addressing the challenges of behavioral stagnation and inefficient learning.

Abstract: Autonomous Machine Learning Engineering (MLE) requires agents to perform sustained, iterative optimization over long horizons. While recent LLM-based agents show promise, current prompt-based agents for MLE suffer from behavioral stagnation due to frozen parameters. Although Reinforcement Learning (RL) offers a remedy, applying it to MLE is hindered by prohibitive execution latency and inefficient data selection. Recognizing these challenges, we propose AceGRPO with two core components: (1) Evolving Data Buffer that continuously repurposes execution traces into reusable training tasks, and (2) Adaptive Sampling guided by a Learnability Potential function, which dynamically prioritizes tasks at the agent's learning frontier to maximize learning efficiency. Leveraging AceGRPO, our trained Ace-30B model achieves a 100% valid submission rate on MLE-Bench-Lite, approaches the performance of proprietary frontier models, and outperforms larger open-source baselines (e.g., DeepSeek-V3.2), demonstrating robust capability for sustained iterative optimization. Code is available at https://github.com/yuzhu-cai/AceGRPO.

</details>


### [90] [CausalCompass: Evaluating the Robustness of Time-Series Causal Discovery in Misspecified Scenarios](https://arxiv.org/abs/2602.07915)
*Huiyang Yi,Xiaojian Shen,Yonggang Wu,Duxin Chen,He Wang,Wenwu Yu*

Main category: cs.LG

TL;DR: 作者提出CausalCompass基准测试套件，用于评估时间序列因果发现方法在模型假设违反情况下的鲁棒性，发现深度学习方法整体表现最优但无单一方法在所有场景中占优


<details>
  <summary>Details</summary>
Motivation: 现有时间序列因果发现方法依赖不可检验的因果假设，且缺乏面向鲁棒性的评估基准，阻碍了其在实际中的应用

Method: 提出灵活可扩展的CausalCompass基准套件，对代表性TSCD算法在八种假设违反场景下进行广泛基准测试和超参数敏感性分析

Result: 无单一方法在所有设置中持续最优；深度学习方法在多样场景下整体性能突出；NTS-NOTEARS高度依赖标准化预处理，无预处理时表现差但标准化后性能强劲

Conclusion: 通过系统性评估TSCD方法在假设违反下的表现，CausalCompass为真实世界应用中的方法选择提供了全面参考

Abstract: Causal discovery from time series is a fundamental task in machine learning. However, its widespread adoption is hindered by a reliance on untestable causal assumptions and by the lack of robustness-oriented evaluation in existing benchmarks. To address these challenges, we propose CausalCompass, a flexible and extensible benchmark suite designed to assess the robustness of time-series causal discovery (TSCD) methods under violations of modeling assumptions. To demonstrate the practical utility of CausalCompass, we conduct extensive benchmarking of representative TSCD algorithms across eight assumption-violation scenarios. Our experimental results indicate that no single method consistently attains optimal performance across all settings. Nevertheless, the methods exhibiting superior overall performance across diverse scenarios are almost invariably deep learning-based approaches. We further provide hyperparameter sensitivity analyses to deepen the understanding of these findings. We also find, somewhat surprisingly, that NTS-NOTEARS relies heavily on standardized preprocessing in practice, performing poorly in the vanilla setting but exhibiting strong performance after standardization. Finally, our work aims to provide a comprehensive and systematic evaluation of TSCD methods under assumption violations, thereby facilitating their broader adoption in real-world applications. The code and datasets are available at https://github.com/huiyang-yi/CausalCompass.

</details>


### [91] [A Kinetic-Energy Perspective of Flow Matching](https://arxiv.org/abs/2602.07928)
*Ziyun Li,Huancheng Hu,Soon Hoe Lim,Xuyu Li,Fei Gao,Enmao Diao,Zezhen Ding,Michalis Vazirgiannis,Henrik Bostrom*

Main category: cs.LG

TL;DR: 本文从经典力学视角分析基于流的生成模型，提出动能路径能量（KPE）作为样本级诊断指标，揭示轨迹能量与生成质量间的非单调关系，并据此设计无需训练的两阶段推理策略KTS，通过金发姑娘原则平衡保真度与记忆化问题。


<details>
  <summary>Details</summary>
Motivation: 将基于流的生成模型采样过程理解为粒子在时变速度场中的动力学轨迹，希望从物理学角度开发诊断工具，以解决生成质量与记忆化之间的权衡问题。

Method: 引入KPE量化ODE轨迹的累积动能；基于经验流匹配的闭式解，提出KTS两阶段推理策略：早期加速运动、晚期软着陆，实现无需训练的能量调控。

Result: 经验发现：(i) KPE与语义保真度正相关；(ii) 高KPE轨迹终止于低密度流形边界。理论证明轨迹能量与数据密度呈非单调关系，过高能量导致记忆化。

Conclusion: 生成存在"金发姑娘区域"：轨迹动能需恰到好处。KTS通过自适应能量整形在基准任务上提升生成质量并减少记忆化，为流模型提供新理解和实用推理方法。

Abstract: Flow-based generative models can be viewed through a physics lens: sampling transports a particle from noise to data by integrating a time-varying velocity field, and each sample corresponds to a trajectory with its own dynamical effort. Motivated by classical mechanics, we introduce Kinetic Path Energy (KPE), an action-like, per-sample diagnostic that measures the accumulated kinetic effort along an Ordinary Differential Equation (ODE) trajectory. Empirically, KPE exhibits two robust correspondences: (i) higher KPE predicts stronger semantic fidelity; (ii) high-KPE trajectories terminate on low-density manifold frontiers. We further provide theoretical guarantees linking trajectory energy to data density. Paradoxically, this correlation is non-monotonic. At sufficiently high energy, generation can degenerate into memorization. Leveraging the closed-form of empirical flow matching, we show that extreme energies drive trajectories toward near-copies of training examples. This yields a Goldilocks principle and motivates Kinetic Trajectory Shaping (KTS), a training-free two-phase inference strategy that boosts early motion and enforces a late-time soft landing, reducing memorization and improving generation quality across benchmark tasks.

</details>


### [92] [Attention-Based Deep Learning for Early Parkinson's Disease Detection with Tabular Biomedical Data](https://arxiv.org/abs/2602.07933)
*Olamide Samuel Oseni,Ibraheem Omotolani Obanla,Toheeb Aduramomi Jimoh*

Main category: cs.LG

TL;DR: 本研究比较了四种模型（MLP、梯度提升、TabNet和SAINT）用于早期帕金森病检测，发现SAINT模型表现最佳，准确率达98%以上，证明注意力机制在医学预测中的潜力


<details>
  <summary>Details</summary>
Motivation: 早期帕金森病检测存在挑战：早期症状不明显，生物医学数据具有复杂非线性关系；传统机器学习方法依赖大量特征工程且难以捕捉复杂的特征交互

Method: 使用UCI机器学习库中的生物医学语音测量数据集，对比评估MLP、梯度提升、TabNet和SAINT四种分类模型，重点关注注意力机制的深度学习架构

Result: SAINT在所有指标上均表现最优：加权精确率0.98、召回率0.97、F1分数0.97、MCC 0.9990及最高AUC-ROC；TabNet和MLP表现具有竞争力，梯度提升得分最低；SAINT的优势源于其双注意力机制能有效建模样本内和样本间的特征交互

Conclusion: 注意力机制深度学习架构在早期帕金森病检测中具有重要诊断潜力，动态特征表示在临床预测任务中具有关键作用

Abstract: Early and accurate detection of Parkinson's disease (PD) remains a critical challenge in medical diagnostics due to the subtlety of early-stage symptoms and the complex, non-linear relationships inherent in biomedical data. Traditional machine learning (ML) models, though widely applied to PD detection, often rely on extensive feature engineering and struggle to capture complex feature interactions. This study investigates the effectiveness of attention-based deep learning models for early PD detection using tabular biomedical data. We present a comparative evaluation of four classification models: Multi-Layer Perceptron (MLP), Gradient Boosting, TabNet, and SAINT, using a benchmark dataset from the UCI Machine Learning Repository consisting of biomedical voice measurements from PD patients and healthy controls.
  Experimental results show that SAINT consistently outperformed all baseline models across multiple evaluation metrics, achieving a weighted precision of 0.98, weighted recall of 0.97, weighted F1-score of 0.97, a Matthews Correlation Coefficient (MCC) of 0.9990, and the highest Area Under the ROC Curve (AUC-ROC). TabNet and MLP demonstrated competitive performance, while Gradient Boosting yielded the lowest overall scores. The superior performance of SAINT is attributed to its dual attention mechanism, which effectively models feature interactions within and across samples.
  These findings demonstrate the diagnostic potential of attention-based deep learning architectures for early Parkinson's disease detection and highlight the importance of dynamic feature representation in clinical prediction tasks.

</details>


### [93] [A Thermodynamic Theory of Learning Part II: Critical Period Closure and Continual Learning Failure](https://arxiv.org/abs/2602.07950)
*Daisuke Okanohara*

Main category: cs.LG

TL;DR: 该论文（第二部分）揭示了有限时间学习的不可逆性导致"关键期关闭"现象，使得某些学习路径在动力学上无法访问，从而将灾难性遗忘重新理解为有限时间耗散的动力学约束，而非任务间的直接干扰。


<details>
  <summary>Details</summary>
Motivation: 基于第一部分建立的有限时间学习不可逆性理论（认知速度极限），探究这种不可逆性对持续学习的影响，特别是从轨迹层面理解灾难性遗忘的根本原因。

Method: 从轨迹层面（trajectory-level perspective）出发，将持续学习建模为参数分布空间中的输运过程，分析有限能量耗散对学习路径动态可及性的约束。

Result: 1. 有限耗散不仅约束可达解，还限制学习路径的动态可及性；2. 有限时间学习不可逆地在等效实现间做出选择；3. 出现"关键期关闭"现象：超过某阶段后，有限耗散预算下无法在兼容表示间转换；4. 持续学习失败源于先前学习导致的表征自由度不可逆丧失。

Conclusion: 灾难性遗忘并非源于多任务解的存在性缺失，而是有限时间耗散施加的动力学约束导致表征自由度不可逆丧失的结果，这为理解持续学习失败提供了新的理论框架。

Abstract: Learning performed over finite time is necessarily irreversible. In Part~I of this series, we modeled learning as a transport process in the space of parameter distributions and derived the Epistemic Speed Limit, which lower-bounds entropy production under finite-time learning.
  In this work (Part~II), we study the consequences of this irreversibility for continual learning from a trajectory-level perspective. We show that finite dissipation constrains not only which solutions are reachable, but which learning paths remain dynamically accessible.
  Although a continuum of task-equivalent realizations can achieve identical task performance, finite-time learning irreversibly selects among these realizations. This selection occurs through the progressive elimination of degrees of freedom that would otherwise enable structural reconfiguration. We refer to this phenomenon as \emph{critical period closure}: beyond a certain stage of learning, transitions between compatible representations become dynamically inaccessible under any finite dissipation budget.
  As a result, continual learning failure arises not from the absence of solutions satisfying multiple tasks, but from an irreversible loss of representational freedom induced by prior learning. This reframes catastrophic forgetting as a dynamical constraint imposed by finite-time dissipation, rather than direct task interference.

</details>


### [94] [An Explainable Multi-Task Similarity Measure: Integrating Accumulated Local Effects and Weighted Fréchet Distance](https://arxiv.org/abs/2602.07966)
*Pablo Hidalgo,Daniel Rodriguez*

Main category: cs.LG

TL;DR: 提出一种基于可解释AI的ALE曲线和Fréchet距离的多任务相似性度量方法，用于量化机器学习任务间的关联程度，并在表格和图像数据集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 在多任务学习中，理解哪些任务相似以及相似的原因对于实现有效的知识迁移至关重要，这是当前需要解决的核心问题。

Method: 利用累积局部效应(ALE)曲线作为任务解释工具，通过加权数据分布的Fréchet距离比较ALE曲线，融入特征重要性；该方法模型无关，可跨任务使用不同模型，并引入缩放因子校正任务间性能差异。

Result: 在1个合成数据集和3个真实数据集（帕金森病、共享单车、CelebA）上验证，该度量与直觉预期相符，适用于表格和非表格数据，能有效支持多任务学习中的概念瓶颈编码器应用。

Conclusion: 该相似性度量是探索任务间关系、辅助多任务学习决策制定的有效工具。

Abstract: In many machine learning contexts, tasks are often treated as interconnected components with the goal of leveraging knowledge transfer between them, which is the central aim of Multi-Task Learning (MTL). Consequently, this multi-task scenario requires addressing critical questions: which tasks are similar, and how and why do they exhibit similarity? In this work, we propose a multi-task similarity measure based on Explainable Artificial Intelligence (XAI) techniques, specifically Accumulated Local Effects (ALE) curves.
  ALE curves are compared using the Fréchet distance, weighted by the data distribution, and the resulting similarity measure incorporates the importance of each feature. The measure is applicable in both single-task learning scenarios, where each task is trained separately, and multi-task learning scenarios, where all tasks are learned simultaneously. The measure is model-agnostic, allowing the use of different machine learning models across tasks. A scaling factor is introduced to account for differences in predictive performance across tasks, and several recommendations are provided for applying the measure in complex scenarios.
  We validate this measure using four datasets, one synthetic dataset and three real-world datasets. The real-world datasets include a well-known Parkinson's dataset and a bike-sharing usage dataset -- both structured in tabular format -- as well as the CelebA dataset, which is used to evaluate the application of concept bottleneck encoders in a multitask learning setting. The results demonstrate that the measure aligns with intuitive expectations of task similarity across both tabular and non-tabular data, making it a valuable tool for exploring relationships between tasks and supporting informed decision-making.

</details>


### [95] [On Improving Neurosymbolic Learning by Exploiting the Representation Space](https://arxiv.org/abs/2602.07973)
*Aaditya Naik,Efthymia Tsamoura,Shibo Jin,Mayur Naik,Dan Roth*

Main category: cs.LG

TL;DR: 针对神经符号学习中逻辑约束导致的标签组合爆炸问题，提出CLIPPER方法，通过整数线性规划修剪不一致的标签组合，显著提升现有神经符号引擎性能。


<details>
  <summary>Details</summary>
Motivation: 神经符号学习中，输入实例的隐藏标签必须满足逻辑公式，但满足公式的标签组合空间可能指数级增长，导致学习困难。现有弱监督方法难以处理逻辑约束带来的标签依赖问题。

Method: 1. 利用实例潜在表征相似性假设（相似实例倾向相同标签）<br>2. 将标签组合修剪建模为整数线性规划问题<br>3. 在保留逻辑结构的前提下剔除不一致标签组合<br>4. 设计正交于现有训练算法的CLIPPER框架

Result: 在16个复杂神经符号任务基准测试中，CLIPPER显著提升三种先进引擎性能：Scallop提升48%、Dolphin提升53%、ISED提升8%，达到最新准确率水平。

Conclusion: CLIPPER有效解决了逻辑约束下的标签组合爆炸问题，其修剪机制可无缝集成到现有神经符号系统，为处理复杂逻辑约束的机器学习任务提供了实用解决方案。

Abstract: We study the problem of learning neural classifiers in a neurosymbolic setting where the hidden gold labels of input instances must satisfy a logical formula. Learning in this setting proceeds by first computing (a subset of) the possible combinations of labels that satisfy the formula and then computing a loss using those combinations and the classifiers' scores. One challenge is that the space of label combinations can grow exponentially, making learning difficult. We propose a technique that prunes this space by exploiting the intuition that instances with similar latent representations are likely to share the same label. While this intuition has been widely used in weakly supervised learning, its application in our setting is challenging due to label dependencies imposed by logical constraints. We formulate the pruning process as an integer linear program that discards inconsistent label combinations while respecting logical structure. Our approach, CLIPPER, is orthogonal to existing training algorithms and can be seamlessly integrated with them. Across 16 benchmarks over complex neurosymbolic tasks, we demonstrate that CLIPPER boosts the performance of state-of-the-art neurosymbolic engines like Scallop, Dolphin, and ISED by up to 48%, 53%, and 8%, leading to state-of-the-art accuracies.

</details>


### [96] [Beyond Optimization: Intelligence as Metric-Topology Factorization under Geometric Incompleteness](https://arxiv.org/abs/2602.07974)
*Xin Li*

Main category: cs.LG

TL;DR: This paper proposes Metric-Topology Factorization (MTF) as a geometric principle for intelligence, arguing that fixed metric representations cause catastrophic forgetting under distributional shift. They introduce the Topological Urysohn Machine (TUM) that separates stable topology from plastic metric warps, enabling rapid adaptation and robustness to task permutations.


<details>
  <summary>Details</summary>
Motivation: Contemporary ML fails under distributional shift, task permutation, and continual learning because it treats intelligence as optimization within fixed representational geometry. Fixed metrics are geometrically incomplete, causing an unavoidable stability-plasticity tradeoff and catastrophic forgetting when topological changes occur.

Method: The authors propose Metric-Topology Factorization (MTF), separating stable topological structure from plastic metric deformations. They implement this via the Topological Urysohn Machine (TUM) using memory-amortized metric inference (MAMI), where spectral task signatures index reusable metric transformations.

Result: The approach demonstrates robustness to task reordering, resistance to catastrophic forgetting, and generalization across transformed environments (permuted, reflected, parity-altered) that defeat conventional continual learning methods like EWC.

Conclusion: Intelligence should be viewed as reshaping representational geometry rather than navigating a fixed maze; MTF provides a unifying geometric principle that resolves the stability-plasticity dilemma through factorization.

Abstract: Contemporary ML often equates intelligence with optimization: searching for solutions within a fixed representational geometry. This works in static regimes but breaks under distributional shift, task permutation, and continual learning, where even mild topological changes can invalidate learned solutions and trigger catastrophic forgetting. We propose Metric-Topology Factorization (MTF) as a unifying geometric principle: intelligence is not navigation through a fixed maze, but the ability to reshape representational geometry so desired behaviors become stable attractors. Learning corresponds to metric contraction (a controlled deformation of Riemannian structure), while task identity and environmental variation are encoded topologically and stored separately in memory. We show any fixed metric is geometrically incomplete: for any local metric representation, some topological transformations make it singular or incoherent, implying an unavoidable stability-plasticity tradeoff for weight-based systems. MTF resolves this by factorizing stable topology from plastic metric warps, enabling rapid adaptation via geometric switching rather than re-optimization. Building on this, we introduce the Topological Urysohn Machine (TUM), implementing MTF through memory-amortized metric inference (MAMI): spectral task signatures index amortized metric transformations, letting a single learned geometry be reused across permuted, reflected, or parity-altered environments. This explains robustness to task reordering, resistance to catastrophic forgetting, and generalization across transformations that defeat conventional continual learning methods (e.g., EWC).

</details>


### [97] [When Is Compositional Reasoning Learnable from Verifiable Rewards?](https://arxiv.org/abs/2602.07992)
*Daniel Barzilai,Yotam Wolf,Ronen Basri*

Main category: cs.LG

TL;DR: 提出"任务优势比"理论，刻画RLVR下组合问题的可学习性，揭示中间步骤优势对学习效率的关键作用。


<details>
  <summary>Details</summary>
Motivation: 研究在仅使用结果级反馈的RLVR设置下，哪些组合问题可学习，尽管已有经验成功但缺乏理论理解。

Method: 理论分析自回归模型中RLVR训练，提出"任务优势比"——组合问题与基模型的联合属性，用以刻画从结果级反馈可学习的任务。

Result: 正：中间步骤有明显优势的组合问题可高效学习；负：缺乏结构优势时RLVR收敛到次优解，基模型质量决定优势存在性。

Conclusion: 为理解RLVR成功与失败的条件提供了原则性理论指导。

Abstract: The emergence of compositional reasoning in large language models through reinforcement learning with verifiable rewards (RLVR) has been a key driver of recent empirical successes. Despite this progress, it remains unclear which compositional problems are learnable in this setting using outcome-level feedback alone. In this work, we theoretically study the learnability of compositional problems in autoregressive models under RLVR training. We identify a quantity that we call the task-advantage ratio, a joint property of the compositional problem and the base model, that characterizes which tasks and compositions are learnable from outcome-level feedback. On the positive side, using this characterization, we show that compositional problems where correct intermediate steps provide a clear advantage are efficiently learnable with RLVR. We also analyze how such an advantage naturally arises in different problems. On the negative side, when the structural advantage is not present, RLVR may converge to suboptimal compositions. We prove that, in some cases, the quality of the base model determines if such an advantage exists and whether RLVR will converge to a suboptimal solution. We hope our analysis can provide a principled theoretical understanding of when and why RLVR succeeds and when it does not.

</details>


### [98] [Regret Analysis of Unichain Average Reward Constrained MDPs with General Parameterization](https://arxiv.org/abs/2602.08000)
*Anirudh Satheesh,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 提出一种基于多级蒙特卡洛估计和显式预热机制的原-对偶自然演员-评论家算法，解决单链平均奖励约束马尔可夫决策过程问题，在不依赖混合时间假设下实现Õ(√T)的遗憾和约束违反界。


<details>
  <summary>Details</summary>
Motivation: 现有约束强化学习的遗憾分析严重依赖遍历性或强混合时间假设，而这些假设在存在瞬态状态时失效，限制了算法在实际中的应用范围。

Method: 设计一种原-对偶自然演员-评论家算法，结合多级蒙特卡洛(MLMC)估计器和显式预热机制，以处理单链动态而无需混合时间预言机。

Result: 建立了有限时间的遗憾和累积约束违反界，其规模达到Õ(√T)，仅受策略和评论家参数化近似误差的影响。

Conclusion: 从而将最优阶保证扩展到更广泛的约束马尔可夫决策过程类别，突破了传统混合时间假设的限制。

Abstract: We study infinite-horizon average-reward constrained Markov decision processes (CMDPs) under the unichain assumption and general policy parameterizations. Existing regret analyses for constrained reinforcement learning largely rely on ergodicity or strong mixing-time assumptions, which fail to hold in the presence of transient states. We propose a primal--dual natural actor--critic algorithm that leverages multi-level Monte Carlo (MLMC) estimators and an explicit burn-in mechanism to handle unichain dynamics without requiring mixing-time oracles. Our analysis establishes finite-time regret and cumulative constraint violation bounds that scale as $\tilde{O}(\sqrt{T})$, up to approximation errors arising from policy and critic parameterization, thereby extending order-optimal guarantees to a significantly broader class of CMDPs.

</details>


### [99] [Don't Always Pick the Highest-Performing Model: An Information Theoretic View of LLM Ensemble Selection](https://arxiv.org/abs/2602.08003)
*Yigit Turkmen,Baturalp Buyukates,Melih Bastopcu*

Main category: cs.LG

TL;DR: 针对大语言模型集成中模型强相关问题，提出基于互信息的贪心选择算法，在查询预算约束下优化模型选择，实验证明其超越基线方法


<details>
  <summary>Details</summary>
Motivation: 大语言模型集成常因模型预测强相关导致性能饱和，亟需在有限查询预算下选择最优模型子集以提升集成效果

Method: 将集成选择建模为互信息最大化问题，用高斯copula建模模型相关性并推导误差下限，提出直接从数据估计信息项的贪心算法

Result: 在MEDMCQA、MMLU和IMDB数据集上，所提方法在相同查询预算下持续优于强基线，验证了信息论框架的有效性

Conclusion: 通过互信息准则和贪心策略，可有效解决预算约束下的模型选择问题，为构建高效LLM集成提供理论指导和实用算法

Abstract: Large language models (LLMs) are often ensembled together to improve overall reliability and robustness, but in practice models are strongly correlated. This raises a fundamental question: which models should be selected when forming an LLM ensemble? We formulate budgeted ensemble selection as maximizing the mutual information between the true label and predictions of the selected models. Furthermore, to explain why performance can saturate even with many models, we model the correlated errors of the models using Gaussian-copula and show an information-theoretic error floor for the performance of the ensemble. Motivated by these, we propose a simple greedy mutual-information selection algorithm that estimates the required information terms directly from data and iteratively builds an ensemble under a query budget. We test our approach in two question answering datasets and one binary sentiment classification dataset: MEDMCQA, MMLU, and IMDB movie reviews. Across all datasets, we observe that our method consistently outperforms strong baselines under the same query budget.

</details>


### [100] [From $O(mn)$ to $O(r^2)$: Two-Sided Low-Rank Communication for Adam in Distributed Training with Memory Efficiency](https://arxiv.org/abs/2602.08007)
*Sizhe Dang,Jiaqi Shao,Xiaodong Zheng,Guang Dai,Yan Song,Haishan Ye*

Main category: cs.LG

TL;DR: TSR-Adam通过同步低秩核心而非完整梯度，在分布式训练中减少通信开销，在保持性能的同时实现13-25倍的通信量降低。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型规模持续扩大，预训练越来越依赖数据并行分布式优化，而带宽受限的梯度同步成为关键瓶颈。现有的投影式低秩优化器在通信受限的训练中表现不佳，仍会传输较大对象且刷新步骤占主导。

Method: 提出TSR-Adam，采用双边低秩通信同步紧凑核心U^T G V∈R^(r×r)，将每步传输负载从O(mn)降至O(r^2)；使用随机化SVD刷新避免全梯度同步；并将低秩通信扩展至嵌入层梯度，采用特定秩和刷新策略。

Result: 在60M到1B参数规模的预训练中，TSR-Adam平均每步通信量降低13倍；在GLUE微调中降低25倍，同时性能相当；并提供了理论平稳性分析。

Conclusion: TSR-Adam有效解决了分布式训练中的通信瓶颈问题，显著降低了通信开销，同时保持了模型性能，为大规模模型训练提供了实用解决方案。

Abstract: As foundation models continue to scale, pretraining increasingly relies on data-parallel distributed optimization, making bandwidth-limited gradient synchronization a key bottleneck. Orthogonally, projection-based low-rank optimizers were mainly designed for memory efficiency, but remain suboptimal for communication-limited training: one-sided synchronization still transmits an $O(rn)$ object for an $m\times n$ matrix gradient and refresh steps can dominate peak communicated bytes. We propose TSR, which brings two-sided low-rank communication to Adam-family updates (TSR-Adam) by synchronizing a compact core $U^\top G V\in\mathbb{R}^{r\times r}$, reducing the dominant per-step payload from $O(mn)$ to $O(r^2)$ while keeping moment states in low-dimensional cores. To further reduce the peak communication from subspace refresh, TSR-Adam adopts a randomized SVD-based refresh that avoids full-gradient synchronization. We additionally extend low-rank communication to embedding gradients with embedding-specific ranks and refresh schedules, yielding additional communication and memory savings over keeping embeddings dense. Across pretraining from 60M to 1B model scales, TSR-Adam reduces average communicated bytes per step by $13\times$, and on GLUE fine-tuning it reduces communication by $25\times$, while achieving comparable performance; we further provide a theoretical stationarity analysis for the proposed update. Code is available at https://github.com/DKmiyan/TSR-Adam.

</details>


### [101] [A Unified Density Operator View of Flow Control and Merging](https://arxiv.org/abs/2602.08012)
*Riccardo De Santi,Malte Franke,Ya-Ping Hsieh,Andreas Krause*

Main category: cs.LG

TL;DR: 提出统一的概率空间框架，将控制式奖励适应和流模型融合统一处理，实现奖励引导的流模型融合，并给出理论保证和分子设计应用


<details>
  <summary>Details</summary>
Motivation: 大规模流和扩散模型面临两个基本算法挑战：基于控制的预训练流奖励适应，以及多模型集成（流融合）。现有方法分别处理这两个问题，缺乏统一框架。

Method: 1) 建立统一的概率空间框架，将两个挑战视为极限情况；2) 提出奖励引导流融合(RFM)的镜像下降方案，将其简化为标准微调问题序列；3) 通过生成电路表达丰富的生成模型密度算子。

Result: 1) 实现了原则性的、任务感知的多个预训练流组合；2) 提供了首个奖励引导和纯流融合的理论保证；3) 在分子设计和构象生成任务上展示了方法能力。

Conclusion: 该框架不仅统一了流模型的控制和融合问题，还通过RFM算法实现了可解释的奖励引导生成，为生成模型的操作提供了新范式，在科学发现领域具有应用潜力。

Abstract: Recent progress in large-scale flow and diffusion models raised two fundamental algorithmic challenges: (i) control-based reward adaptation of pre-trained flows, and (ii) integration of multiple models, i.e., flow merging. While current approaches address them separately, we introduce a unifying probability-space framework that subsumes both as limit cases, and enables reward-guided flow merging, allowing principled, task-aware combination of multiple pre-trained flows (e.g., merging priors while maximizing drug-discovery utilities). Our formulation renders possible to express a rich family of operators over generative models densities, including intersection (e.g., to enforce safety), union (e.g., to compose diverse models), interpolation (e.g., for discovery), their reward-guided counterparts, as well as complex logical expressions via generative circuits. Next, we introduce Reward-Guided Flow Merging (RFM), a mirror-descent scheme that reduces reward-guided flow merging to a sequence of standard fine-tuning problems. Then, we provide first-of-their-kind theoretical guarantees for reward-guided and pure flow merging via RFM. Ultimately, we showcase the capabilities of the proposed method on illustrative settings providing visually interpretable insights, and apply our method to high-dimensional de-novo molecular design and low-energy conformer generation.

</details>


### [102] [The Rise of Sparse Mixture-of-Experts:A Survey from Algorithmic Foundations to Decentralized Architectures and Vertical Domain Applications](https://arxiv.org/abs/2602.08019)
*Dong Pan,Bingtao Li,Yongsheng Zheng,Jiren Ma,Victor Fei*

Main category: cs.LG

TL;DR: This survey paper provides a comprehensive review of Mixture of Experts (MoE) architectures, covering foundational principles, decentralized paradigms, vertical domain applications, and future research directions, claiming to be the most extensive review in the field.


<details>
  <summary>Details</summary>
Motivation: Despite MoE's growing popularity for scaling deep learning models with improved computational efficiency, existing surveys lack systematic exploration and comprehensive coverage of recent advancements across important fields.

Method: The paper systematically examines MoE's core components (routing and expert networks), extends analysis to decentralized infrastructure paradigms, explores vertical domain applications, and identifies key challenges and future research directions.

Result: To the best of the authors' knowledge, this survey represents the most comprehensive review in the MoE field, synthesizing recent advancements across centralized and decentralized approaches.

Conclusion: The article aims to serve as a valuable resource for researchers and practitioners to navigate and stay current with MoE developments, enabling broader democratization of MoE development and greater scalability.

Abstract: The sparse Mixture of Experts(MoE) architecture has evolved as a powerful approach for scaling deep learning models to more parameters with comparable computation cost. As an important branch of large language model(LLM), MoE model only activate a subset of experts based on a routing network. This sparse conditional computation mechanism significantly improves computational efficiency, paving a promising path for greater scalability and cost-efficiency. It not only enhance downstream applications such as natural language processing, computer vision, and multimodal in various horizontal domains, but also exhibit broad applicability across vertical domains. Despite the growing popularity and application of MoE models across various domains, there lacks a systematic exploration of recent advancements of MoE in many important fields. Existing surveys on MoE suffer from limitations such as lack coverage or none extensively exploration of key areas. This survey seeks to fill these gaps. In this paper, Firstly, we examine the foundational principles of MoE, with an in-depth exploration of its core components-the routing network and expert network. Subsequently, we extend beyond the centralized paradigm to the decentralized paradigm, which unlocks the immense untapped potential of decentralized infrastructure, enables democratization of MoE development for broader communities, and delivers greater scalability and cost-efficiency. Furthermore we focus on exploring its vertical domain applications. Finally, we also identify key challenges and promising future research directions. To the best of our knowledge, this survey is currently the most comprehensive review in the field of MoE. We aim for this article to serve as a valuable resource for both researchers and practitioners, enabling them to navigate and stay up-to-date with the latest advancements.

</details>


### [103] [Sharp analysis of linear ensemble sampling](https://arxiv.org/abs/2602.08026)
*Arya Akhavan,David Janz,Csaba Szepesvári*

Main category: cs.LG

TL;DR: 该论文分析了线性集成采样算法在高斯扰动下的随机线性老虎机问题，证明了当集成规模m=Θ(d log n)时，算法能达到Õ(d³/²√n)的高概率遗憾界，通过新颖的布朗运动连续时间分析框架填补了与汤普森采样基准的差距。


<details>
  <summary>Details</summary>
Motivation: 线性老虎机领域中，集成采样算法的计算效率优于汤普森采样，但理论遗憾界存在差距。本文旨在缩小这一差距，同时保持计算效率，并探索更本质的分析方法。

Method: 采用连续时间视角分析离散时间问题，将集成采样的遗憾分析归约为m个独立布朗运动的时间一致超出概率问题，利用随机过程理论进行证明。

Result: 证明了当集成规模m=Θ(d log n)时，集成采样算法能达到Õ(d³/²√n)的高概率遗憾界，该结果接近汤普森采样的基准性能，同时保持计算复杂度相当。

Conclusion: 连续时间布朗运动分析对于集成采样并非人为强加，而是自然且可能必要的。该研究为线性老虎机中的随机化探索提供了新视角，揭示了离散问题与连续方法之间的深刻联系。

Abstract: We analyse linear ensemble sampling (ES) with standard Gaussian perturbations in stochastic linear bandits. We show that for ensemble size $m=Θ(d\log n)$, ES attains $\tilde O(d^{3/2}\sqrt n)$ high-probability regret, closing the gap to the Thompson sampling benchmark while keeping computation comparable. The proof brings a new perspective on randomized exploration in linear bandits by reducing the analysis to a time-uniform exceedance problem for $m$ independent Brownian motions. Intriguingly, this continuous-time lens is not forced; it appears natural--and perhaps necessary: the discrete-time problem seems to be asking for a continuous-time solution, and we know of no other way to obtain a sharp ES bound.

</details>


### [104] [Horizon Imagination: Efficient On-Policy Training in Diffusion World Models](https://arxiv.org/abs/2602.08032)
*Lior Cohen,Ofir Nabati,Kaixin Wang,Navdeep Kumar,Shie Mannor*

Main category: cs.LG

TL;DR: The paper proposes Horizon Imagination (HI), a parallel denoising approach for diffusion-based world models in RL that decouples denoising budget from effective horizon, achieving comparable control performance with half the computational cost while maintaining superior generation quality.


<details>
  <summary>Details</summary>
Motivation: Diffusion-based world models in reinforcement learning offer high generative fidelity but suffer from critical efficiency challenges. Current methods either require heavyweight models during inference or rely on highly sequential imagination processes, both resulting in prohibitive computational costs that limit their practical applicability for control tasks.

Method: The authors propose Horizon Imagination (HI), an on-policy imagination process specifically designed for discrete stochastic policies. HI denoises multiple future observations in parallel rather than sequentially. It incorporates two key innovations: (1) a stabilization mechanism, and (2) a novel sampling schedule that decouples the denoising budget from the effective horizon while supporting sub-frame budgets.

Result: Experiments conducted on Atari 100K and Craftium benchmarks demonstrate that HI maintains control performance even when using a sub-frame budget of only half the denoising steps compared to baseline methods. Additionally, the approach achieves superior generation quality across various scheduling configurations.

Conclusion: Horizon Imagination successfully addresses the efficiency bottleneck in diffusion-based world models by enabling parallel denoising, making these models more practical for reinforcement learning control tasks without sacrificing performance or generation quality.

Abstract: We study diffusion-based world models for reinforcement learning, which offer high generative fidelity but face critical efficiency challenges in control. Current methods either require heavyweight models at inference or rely on highly sequential imagination, both of which impose prohibitive computational costs. We propose Horizon Imagination (HI), an on-policy imagination process for discrete stochastic policies that denoises multiple future observations in parallel. HI incorporates a stabilization mechanism and a novel sampling schedule that decouples the denoising budget from the effective horizon over which denoising is applied while also supporting sub-frame budgets. Experiments on Atari 100K and Craftium show that our approach maintains control performance with a sub-frame budget of half the denoising steps and achieves superior generation quality under varied schedules. Code is available at https://github.com/leor-c/horizon-imagination.

</details>


### [105] [The Benefits of Diversity: Combining Comparisons and Ratings for Efficient Scoring](https://arxiv.org/abs/2602.08033)
*Julien Fageot,Matthias Grossglauser,Lê-Nguyên Hoang,Matteo Tacchi-Bénard,Oscar Villemaud*

Main category: cs.LG

TL;DR: This paper proposes SCoRa, a unified probabilistic model that combines individual ratings and comparative judgments to learn entity scores, proving theoretical guarantees and showing superior performance especially when ranking top entities.


<details>
  <summary>Details</summary>
Motivation: The long-standing debate about whether humans should evaluate entities individually or comparatively, with limited understanding of whether combining both elicitation forms can provide benefits over using either one alone.

Method: SCoRa (Scoring from Comparisons and Ratings): a unified probabilistic model that simultaneously learns from both comparative and individual rating signals through a MAP estimation approach.

Result: Theoretical proofs of monotonicity and robustness for the MAP estimator; empirical validation that SCoRa recovers accurate scores even under model mismatch; identifies realistic settings where combining both signal types outperforms either alone, particularly when accurate ordering of top entities is critical.

Conclusion: SCoRa offers a versatile foundation for preference learning by effectively leveraging the de facto availability of multiple forms of preference signals, providing a principled way to combine them.

Abstract: Should humans be asked to evaluate entities individually or comparatively? This question has been the subject of long debates. In this work, we show that, interestingly, combining both forms of preference elicitation can outperform the focus on a single kind. More specifically, we introduce SCoRa (Scoring from Comparisons and Ratings), a unified probabilistic model that allows to learn from both signals. We prove that the MAP estimator of SCoRa is well-behaved. It verifies monotonicity and robustness guarantees. We then empirically show that SCoRa recovers accurate scores, even under model mismatch. Most interestingly, we identify a realistic setting where combining comparisons and ratings outperforms using either one alone, and when the accurate ordering of top entities is critical. Given the de facto availability of signals of multiple forms, SCoRa additionally offers a versatile foundation for preference learning.

</details>


### [106] [Implicit Strategic Optimization: Rethinking Long-Horizon Decision-Making in Adversarial Poker Environments](https://arxiv.org/abs/2602.08041)
*Boyang Xia,Weiyou Tian,Qingnan Ren,Jiaqi Huang,Jie Xiao,Shuo Lu,Kai Wang,Lynn Ai,Eric Yang,Bill Shi*

Main category: cs.LG

TL;DR: The paper proposes Implicit Strategic Optimization (ISO), a framework that uses strategic context prediction to improve long-term returns for LLM agents in adversarial games, achieving sublinear regret and outperforming baselines in poker and Pokemon.


<details>
  <summary>Details</summary>
Motivation: Training LLM agents for adversarial games using episodic objectives (like win rate) fails in long-horizon settings where payoffs are shaped by latent strategic externalities that evolve over time. Myopic optimization and standard regret analyses become vacuous even when dynamics are predictable.

Method: ISO combines a Strategic Reward Model (SRM) that estimates the long-run strategic value of actions with iso-grpo, a context-conditioned optimistic learning rule. Each agent forecasts the current strategic context and uses it to update its policy online.

Result: Theoretically proves sublinear contextual regret and equilibrium convergence guarantees where dominant terms scale with the number of context mispredictions. Empirically shows consistent improvements in long-term return over strong LLM and RL baselines in 6-player No-Limit Texas Hold'em and competitive Pokemon, with graceful degradation under controlled prediction noise.

Conclusion: ISO effectively handles strategic externalities in long-horizon adversarial games through prediction-aware optimization, providing both theoretical guarantees and practical performance improvements over traditional methods.

Abstract: Training large language model (LLM) agents for adversarial games is often driven by episodic objectives such as win rate. In long-horizon settings, however, payoffs are shaped by latent strategic externalities that evolve over time, so myopic optimization and variation-based regret analyses can become vacuous even when the dynamics are predictable. To solve this problem, we introduce Implicit Strategic Optimization (ISO), a prediction-aware framework in which each agent forecasts the current strategic context and uses it to update its policy online. ISO combines a Strategic Reward Model (SRM) that estimates the long-run strategic value of actions with iso-grpo, a context-conditioned optimistic learning rule. We prove sublinear contextual regret and equilibrium convergence guarantees whose dominant terms scale with the number of context mispredictions; when prediction errors are bounded, our bounds recover the static-game rates obtained when strategic externalities are known. Experiments in 6-player No-Limit Texas Hold'em and competitive Pokemon show consistent improvements in long-term return over strong LLM and RL baselines, and graceful degradation under controlled prediction noise.

</details>


### [107] [V-ABFT: Variance-Based Adaptive Threshold for Fault-Tolerant Matrix Multiplication in Mixed-Precision Deep Learning](https://arxiv.org/abs/2602.08043)
*Yiheng Gao,Qin Hua,Zizhong Chen*

Main category: cs.LG

TL;DR: V-ABFT is a variance-based adaptive threshold algorithm that improves ABFT for matrix multiplication by achieving tighter error bounds (6-48x improvement over A-ABFT) while maintaining zero false positives, enabling ~1000x finer fault detection in low-precision GEMM.


<details>
  <summary>Details</summary>
Motivation: ABFT is critical for detecting silent data corruptions (SDCs) in deep learning's matrix multiplication operations. However, existing threshold methods are problematic: analytical bounds are overly conservative, while probabilistic A-ABFT yields thresholds 160-4200x larger than actual rounding errors, leading to poor fault detection granularity.

Method: V-ABFT directly models the verification difference using statistical variance estimation to derive adaptive thresholds, reducing threshold-to-error ratios while maintaining zero false positives.

Result: V-ABFT reduces threshold-to-actual-error ratios to 7-20x (FP32/FP64) and 48-158x (BF16), achieving 6-48x improvement over A-ABFT. It enables ~1000x finer detection granularity for low-precision GEMM with FP32-level thresholds. The method runs in O(n) complexity, validated across multiple precisions and models (LLaMA-7B, GPT-2, ViT).

Conclusion: V-ABFT provides a platform-agnostic, effective solution for fault-tolerant matrix multiplication with significantly improved detection granularity and zero false positives, making it suitable for deployment in production deep learning systems on NPUs and GPUs.

Abstract: Algorithm-Based Fault Tolerance (ABFT) is widely adopted to detect silent data corruptions (SDCs) in matrix multiplication, a cornerstone operation in deep learning systems. However, existing threshold determination methods face critical challenges: analytical bounds are overly conservative, while probabilistic approaches like A-ABFT yield thresholds $160$--$4200\times$ larger than actual rounding errors. We present V-ABFT, a variance-based adaptive threshold algorithm that achieves tighter error bounds by directly modeling the verification difference. By leveraging statistical variance estimation, V-ABFT reduces the threshold-to-actual-error ratio to approximately $7$--$20\times$ for FP32/FP64 and $48$--$158\times$ for BF16, representing a \textbf{6--48$\times$ improvement} over A-ABFT while maintaining zero false positive rate across BF16, FP16, FP32, and FP64 precisions. Furthermore, we demonstrate that for fused-kernel ABFT implementations that verify before output quantization, low-precision GEMM can use FP32-level thresholds ($e_{\max} \approx 10^{-6}$), enabling \textbf{$\sim$1000$\times$ finer detection granularity} compared to offline verification with low-precision output ($e_{\max} \approx 10^{-3}$). We reproduce A-ABFT's experimental setup and validate our implementation against the original paper's results. Our method requires only $O(n)$ complexity using max/min/mean statistics, compared to A-ABFT's $O(pn)$ for finding $p$ largest values. Extensive experiments on synthetic data and real model weights (LLaMA-7B, GPT-2, ViT) demonstrate V-ABFT's effectiveness across diverse distributions. V-ABFT is platform-agnostic and has been integrated into fault-tolerant GEMM implementations on both NPUs and GPUs.

</details>


### [108] [Interpretable Fuzzy Systems For Forward Osmosis Desalination](https://arxiv.org/abs/2602.08050)
*Qusai Khaled,Uzay Kaymak,Laura Genga*

Main category: cs.LG

TL;DR: 针对水处理领域对可解释性的需求，本文提出一种人在回路的模糊规则系统构建方法，通过专家驱动的网格划分、领域特征工程和规则剪枝，在保持预测性能的同时提升语义可解释性，为正向渗透海水淡化提供可解释的解决方案。


<details>
  <summary>Details</summary>
Motivation: 水处理决策直接影响公共健康，需保持模糊规则系统(FRBS)的可解释性。现有研究虽通过多目标算法解决了结构可解释性，但语义可解释性常因模糊集区分度低而受损，影响实际应用的可信度。

Method: 提出人在回路的三阶段方法：1) 专家驱动的网格划分，构建可区分的隶属函数；2) 领域知识指导的特征工程，消除冗余特征；3) 基于触发强度的规则剪枝，控制结构复杂度。

Result: 实验表明该方法在预测性能上与基于聚类的FRBS相当，同时有效保持了语义可解释性并满足结构复杂度约束。

Conclusion: 所提方法为水处理应用提供了可解释的建模方案，成功平衡了预测精度与模型可解释性，支持可信的决策过程。

Abstract: Preserving interpretability in fuzzy rule-based systems (FRBS) is vital for water treatment, where decisions impact public health. While structural interpretability has been addressed using multi-objective algorithms, semantic interpretability often suffers due to fuzzy sets with low distinguishability. We propose a human-in-the-loop approach for developing interpretable FRBS to predict forward osmosis desalination productivity. Our method integrates expert-driven grid partitioning for distinguishable membership functions, domain-guided feature engineering to reduce redundancy, and rule pruning based on firing strength. This approach achieved comparable predictive performance to cluster-based FRBS while maintaining semantic interpretability and meeting structural complexity constraints, providing an explainable solution for water treatment applications.

</details>


### [109] [Epigraph-Guided Flow Matching for Safe and Performant Offline Reinforcement Learning](https://arxiv.org/abs/2602.08054)
*Manan Tayal,Mumuksh Tayal*

Main category: cs.LG

TL;DR: This paper proposes EpiFlow, a framework for safe offline reinforcement learning that uses epigraph reformulation and flow matching to achieve both safety and performance without excessive conservatism.


<details>
  <summary>Details</summary>
Motivation: Safe offline RL struggles to jointly achieve strong safety and performance from fixed datasets due to issues with soft constraints, excessive conservatism, and difficulty balancing safety, reward optimization, and data distribution adherence.

Method: Formulates safe offline RL as a state-constrained optimal control problem, learns a feasibility value function via epigraph reformulation to avoid decoupled objectives, and synthesizes policies by reweighting the behavior distribution based on this epigraph value and fitting a generative policy using flow matching.

Result: EpiFlow achieves competitive returns with near-zero empirical safety violations across Safety-Gymnasium safety-critical benchmarks.

Conclusion: The epigraph-guided approach effectively co-optimizes safety and performance while maintaining distributional consistency, demonstrating a promising direction for safe offline RL.

Abstract: Offline reinforcement learning (RL) provides a compelling paradigm for training autonomous systems without the risks of online exploration, particularly in safety-critical domains. However, jointly achieving strong safety and performance from fixed datasets remains challenging. Existing safe offline RL methods often rely on soft constraints that allow violations, introduce excessive conservatism, or struggle to balance safety, reward optimization, and adherence to the data distribution. To address this, we propose Epigraph-Guided Flow Matching (EpiFlow), a framework that formulates safe offline RL as a state-constrained optimal control problem to co-optimize safety and performance. We learn a feasibility value function derived from an epigraph reformulation of the optimal control problem, thereby avoiding the decoupled objectives or post-hoc filtering common in prior work. Policies are synthesized by reweighting the behavior distribution based on this epigraph value function and fitting a generative policy via flow matching, enabling efficient, distribution-consistent sampling. Across various safety-critical tasks, including Safety-Gymnasium benchmarks, EpiFlow achieves competitive returns with near-zero empirical safety violations, demonstrating the effectiveness of epigraph-guided policy synthesis.

</details>


### [110] [Compiler-Assisted Speculative Sampling for Accelerated LLM Inference on Heterogeneous Edge Devices](https://arxiv.org/abs/2602.08060)
*Alejandro Ruiz y Mesa,Guilherme Korol,Moritz Riesteter,João Paulo Cardoso de Lima,Jeronimo Castrillon*

Main category: cs.LG

TL;DR: This paper addresses LLM deployment latency on edge devices by developing an analytical cost model that optimizes Speculative Decoding through heterogeneous CPU-GPU partitioning, achieving up to 1.68× speedup for translation tasks.


<details>
  <summary>Details</summary>
Motivation: LLM deployment on resource-constrained edge devices faces severe latency constraints in real-time applications; Speculative Decoding shows promise but is hindered by compiler integration challenges and ineffective utilization of heterogeneous SoC compute resources.

Method: Using an analytical cost model to explore heterogeneous hardware configurations and guide coarse-grained partitioning of LLM subgraphs, particularly optimized for edge-typical short input sequence lengths.

Result: Validated on an edge device with hexacore Cortex-A CPU and Mali GPU, achieving up to 1.68× speedup for translation tasks that closely matches analytic predictions.

Conclusion: The analytical cost model successfully predicts when speculative sampling and heterogeneous execution are jointly beneficial, effectively addressing compiler integration and heterogeneous resource exploitation challenges for edge LLM deployment.

Abstract: LLM deployment on resource-constrained edge devices faces severe latency constraints, particularly in real-time applications where delayed responses can compromise safety or usability. Among many approaches to mitigate the inefficiencies of sequential token-by-token generation, Speculative Decoding (SD) has emerged as a promising technique. However, SD at the edge is hindered by two major challenges: (1) integrating SD into a compiler-based workflow without sacrificing performance or programmability, and (2) exploiting the heterogeneous compute resources of modern SoCs through carefully designed partitioning strategies. This work addresses these challenges by using an analytical cost model that explores heterogeneous hardware configurations and guides coarse-grained partitioning of LLM subgraphs, particularly with edge-typical short input sequence lengths. The cost model predicts when speculative sampling and heterogeneous execution are jointly beneficial and is validated on an edge device featuring a hexacore Cortex-A CPU and a Mali GPU, revealing up to 1.68$\times$ speedup for translation tasks, closely matching analytic expectations.

</details>


### [111] [Efficient and Adaptable Detection of Malicious LLM Prompts via Bootstrap Aggregation](https://arxiv.org/abs/2602.08062)
*Shayan Ali Hassan,Tao Ni,Zafar Ayyub Qazi,Marco Canini*

Main category: cs.LG

TL;DR: BAGEL is a lightweight, modular framework for detecting malicious prompts using an ensemble of small specialized models, achieving high performance with fewer parameters than existing solutions while supporting incremental updates.


<details>
  <summary>Details</summary>
Motivation: LLMs are vulnerable to malicious prompts but existing defenses have fundamental limitations: black-box APIs lack transparency and adapt poorly to evolving threats, while white-box large LLM judges are computationally expensive and require costly retraining for new attacks, forcing trade-offs between performance, efficiency, and adaptability.

Method: BAGEL uses bootstrap aggregation and mixture of experts—an ensemble of fine-tuned models each specialized on different attack datasets. A random forest router identifies suitable models, with stochastic sampling for prediction aggregation. For new attacks, it incrementally fine-tunes a small 86M parameter classifier and adds it to the ensemble.

Result: Achieves 0.92 F1 score with just 5 ensemble members (430M parameters total), outperforming OpenAI Moderation API and ShieldGemma which require billions of parameters. Performance remains robust after nine incremental updates and provides interpretability through its router features.

Conclusion: Small fine-tuned classifier ensembles can match or exceed billion-parameter guardrails while offering the adaptability and efficiency required for production systems.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, reasoning, and generation. However, these systems remain susceptible to malicious prompts that induce unsafe or policy-violating behavior through harmful requests, jailbreak techniques, and prompt injection attacks. Existing defenses face fundamental limitations: black-box moderation APIs offer limited transparency and adapt poorly to evolving threats, while white-box approaches using large LLM judges impose prohibitive computational costs and require expensive retraining for new attacks. Current systems force designers to choose between performance, efficiency, and adaptability.
  To address these challenges, we present BAGEL (Bootstrap AGgregated Ensemble Layer), a modular, lightweight, and incrementally updatable framework for malicious prompt detection. BAGEL employs a bootstrap aggregation and mixture of expert inspired ensemble of fine-tuned models, each specialized on a different attack dataset. At inference, BAGEL uses a random forest router to identify the most suitable ensemble member, then applies stochastic selection to sample additional members for prediction aggregation. When new attacks emerge, BAGEL updates incrementally by fine-tuning a small prompt-safety classifier (86M parameters) and adding the resulting model to the ensemble. BAGEL achieves an F1 score of 0.92 by selecting just 5 ensemble members (430M parameters), outperforming OpenAI Moderation API and ShieldGemma which require billions of parameters. Performance remains robust after nine incremental updates, and BAGEL provides interpretability through its router's structural features. Our results show ensembles of small finetuned classifiers can match or exceed billion-parameter guardrails while offering the adaptability and efficiency required for production systems.

</details>


### [112] [Efficient Distribution Learning with Error Bounds in Wasserstein Distance](https://arxiv.org/abs/2602.08063)
*Eduardo Figueiredo,Steven Adams,Luca Laurenti*

Main category: cs.LG

TL;DR: 提出了一种基于最优传输和混合整数线性规划的新框架，可从样本高效逼近未知分布，并提供Wasserstein距离的非渐近置信界，在基准测试中支持集更小、误差界更紧。


<details>
  <summary>Details</summary>
Motivation: Wasserstein距离在机器学习、控制论等领域至关重要，但如何用有限样本学习未知分布并获得易于计算的非渐近误差界仍是基础难题。

Method: 结合最优传输、非线性优化和集中不等式，通过求解一个仅依赖近似分布支撑集大小的混合整数线性规划（MILP），以大概率界住Wasserstein距离，并设计智能聚类算法优化支撑集。

Result: 算法可高效计算Wasserstein距离界；在多个基准测试中，相较现有方法，所得近似分布支撑集显著更小且误差界更紧。

Conclusion: 该框架为带理论保证的分布逼近提供了实用解法，在保持高精度的同时降低计算复杂度，具备广泛应用潜力。

Abstract: The Wasserstein distance has emerged as a key metric to quantify distances between probability distributions, with applications in various fields, including machine learning, control theory, decision theory, and biological systems. Consequently, learning an unknown distribution with non-asymptotic and easy-to-compute error bounds in Wasserstein distance has become a fundamental problem in many fields. In this paper, we devise a novel algorithmic and theoretical framework to approximate an unknown probability distribution $\mathbb{P}$ from a finite set of samples by an approximate discrete distribution $\widehat{\mathbb{P}}$ while bounding the Wasserstein distance between $\mathbb{P}$ and $\widehat{\mathbb{P}}$. Our framework leverages optimal transport, nonlinear optimization, and concentration inequalities. In particular, we show that, even if $\mathbb{P}$ is unknown, the Wasserstein distance between $\mathbb{P}$ and $\widehat{\mathbb{P}}$ can be efficiently bounded with high confidence by solving a tractable optimization problem (a mixed integer linear program) of a size that only depends on the size of the support of $\widehat{\mathbb{P}}$. This enables us to develop intelligent clustering algorithms to optimally find the support of $\widehat{\mathbb{P}}$ while minimizing the Wasserstein distance error. On a set of benchmarks, we demonstrate that our approach outperforms state-of-the-art comparable methods by generally returning approximating distributions with substantially smaller support and tighter error bounds.

</details>


### [113] [SiameseNorm: Breaking the Barrier to Reconciling Pre/Post-Norm](https://arxiv.org/abs/2602.08064)
*Tianyu Li,Dongchen Han,Zixuan Cao,Haofeng Huang,Mengyu Zhou,Ming Chen,Erchao Zhao,Xiaoxi Jiang,Guanjun Jiang,Gao Huang*

Main category: cs.LG

TL;DR: 该论文指出单流Transformer架构中Pre-Norm和Post-Norm范式存在结构性不兼容，导致稳定性与性能权衡问题，并提出SiameseNorm——一种共享参数的双流架构，通过解耦优化动态，使两个流分别保障稳定性和增强表达力，从而同时实现优化鲁棒性和优越性能。


<details>
  <summary>Details</summary>
Motivation: 现代Transformer主要采用Pre-Norm范式以保证优化稳定性，但这牺牲了性能更优的Post-Norm架构的潜力。先前结合两者优势的尝试通常导致稳定性与性能之间的权衡。作者将此现象归因于单流设计中的结构性不兼容：任何Post-Norm操作都会不可避免地阻碍Pre-Norm所保持的清洁恒等梯度。

Method: 作者提出SiameseNorm，一种双流架构，通过共享参数耦合Pre-Norm-like和Post-Norm-like两个流。该设计解耦了两个流的优化动态，使所有残差块都能接收来自两个范式的组合梯度——一个流确保稳定性，另一个流增强表达力，从而保留Pre-Norm和Post-Norm各自的特点。

Result: 在13亿参数模型的广泛预训练实验中，SiameseNorm表现出卓越的优化鲁棒性，并持续优于强基线模型。

Conclusion: SiameseNorm通过从根本上解决Pre-Norm和Post-Norm的结构性不兼容问题，成功调和了这两种范式，在不牺牲稳定性的前提下实现了优越性能，证明了双流设计在Transformer架构中的有效性。

Abstract: Modern Transformers predominantly adopt the Pre-Norm paradigm for its optimization stability, foregoing the superior potential of the unstable Post-Norm architecture. Prior attempts to combine their strengths typically lead to a stability-performance trade-off. We attribute this phenomenon to a structural incompatibility within a single-stream design: Any application of the Post-Norm operation inevitably obstructs the clean identity gradient preserved by Pre-Norm. To fundamentally reconcile these paradigms, we propose SiameseNorm, a two-stream architecture that couples Pre-Norm-like and Post-Norm-like streams with shared parameters. This design decouples the optimization dynamics of the two streams, retaining the distinct characteristics of both Pre-Norm and Post-Norm by enabling all residual blocks to receive combined gradients inherited from both paradigms, where one stream secures stability while the other enhances expressivity. Extensive pre-training experiments on 1.3B-parameter models demonstrate that SiameseNorm exhibits exceptional optimization robustness and consistently outperforms strong baselines. Code is available at https://github.com/Qwen-Applications/SiameseNorm.

</details>


### [114] [Enhancing Bandit Algorithms with LLMs for Time-varying User Preferences in Streaming Recommendations](https://arxiv.org/abs/2602.08067)
*Chenglei Shen,Yi Zhan,Weijie Yu,Xiao Zhang,Jun Xu*

Main category: cs.LG

TL;DR: Proposes HyperBandit+, a contextual bandit policy using time-aware hypernetwork to model evolving user preferences and LLM-assisted warm-start for efficient early exploration, achieving sublinear regret and outperforming baselines in streaming recommendations.


<details>
  <summary>Details</summary>
Motivation: Existing bandit methods neglect explicit time-preference relationships in streaming recommendations, causing suboptimal performance, and suffer from inefficient exploration-exploitation in early online phase.

Method: Integrates time-aware hypernetwork (time-features → dynamic reward parameters) with LLM Start (multi-step data augmentation for warm-start), plus low-rank factorization for real-time efficiency.

Result: Theoretical sublinear regret bound proven; real-world experiments show consistent superiority over state-of-the-art baselines in accumulated rewards.

Conclusion: HyperBandit+ effectively addresses time-varying preference modeling and early-phase exploration challenges in streaming recommender systems.

Abstract: In real-world streaming recommender systems, user preferences evolve dynamically over time. Existing bandit-based methods treat time merely as a timestamp, neglecting its explicit relationship with user preferences and leading to suboptimal performance. Moreover, online learning methods often suffer from inefficient exploration-exploitation during the early online phase. To address these issues, we propose HyperBandit+, a novel contextual bandit policy that integrates a time-aware hypernetwork to adapt to time-varying user preferences and employs a large language model-assisted warm-start mechanism (LLM Start) to enhance exploration-exploitation efficiency in the early online phase. Specifically, HyperBandit+ leverages a neural network that takes time features as input and generates parameters for estimating time-varying rewards by capturing the correlation between time and user preferences. Additionally, the LLM Start mechanism employs multi-step data augmentation to simulate realistic interaction data for effective offline learning, providing warm-start parameters for the bandit policy in the early online phase. To meet real-time streaming recommendation demands, we adopt low-rank factorization to reduce hypernetwork training complexity. Theoretically, we rigorously establish a sublinear regret upper bound that accounts for both the hypernetwork and the LLM warm-start mechanism. Extensive experiments on real-world datasets demonstrate that HyperBandit+ consistently outperforms state-of-the-art baselines in terms of accumulated rewards.

</details>


### [115] [Multimodal normative modeling in Alzheimers Disease with introspective variational autoencoders](https://arxiv.org/abs/2602.08077)
*Sayantan Kumar,Peijie Qiu,Aristeidis Sotiras*

Main category: cs.LG

TL;DR: 提出mmSIVAE模型解决多模态神经影像学中VAE基规范建模的缺陷：健康参考分布拟合不佳导致假阳性，以及后验聚合弱融合问题。通过软自省VAE与MOPOE聚合提升参考保真度和多模态整合，在ADNI数据上实现更优的异常检测和临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有VAE基规范模型存在两大缺陷：(1) 对健康参考分布拟合不准确，导致假阳性率升高；(2) 使用PoE/MoE等后验聚合方法时，共享潜空间中的多模态融合效果较弱，影响阿尔茨海默病(AD)等异质性疾病的个体化异常量化。

Method: 提出mmSIVAE模型：① 结合多模态软自省变分自编码器提升健康分布建模精度；② 采用混合专家乘积(MOPOE)聚合策略强化多模态融合；③ 在潜空间和特征空间计算偏离分数；④ 将显著潜偏离映射到脑区异常实现可解释性。

Result: 在ADNI的MRI体积和淀粉样蛋白PET SUVR数据上：① 对保留对照组的重建效果更优；② 异常检测的偏离分数区分度更高（对照组与AD谱系队列分离更清晰）；③ 似然比显著提升；④ 偏离图突出显示与已知AD病理一致的脑区模式。

Conclusion: 强调提升参考分布保真度和鲁棒多模态后验聚合对规范建模的关键性，所提方法为多模态临床数据的偏离分析提供新思路，对异质性疾病研究具有普适价值。

Abstract: Normative modeling learns a healthy reference distribution and quantifies subject-specific deviations to capture heterogeneous disease effects. In Alzheimers disease (AD), multimodal neuroimaging offers complementary signals but VAE-based normative models often (i) fit the healthy reference distribution imperfectly, inflating false positives, and (ii) use posterior aggregation (e.g., PoE/MoE) that can yield weak multimodal fusion in the shared latent space. We propose mmSIVAE, a multimodal soft-introspective variational autoencoder combined with Mixture-of-Product-of-Experts (MOPOE) aggregation to improve reference fidelity and multimodal integration. We compute deviation scores in latent space and feature space as distances from the learned healthy distributions, and map statistically significant latent deviations to regional abnormalities for interpretability. On ADNI MRI regional volumes and amyloid PET SUVR, mmSIVAE improves reconstruction on held-out controls and produces more discriminative deviation scores for outlier detection than VAE baselines, with higher likelihood ratios and clearer separation between control and AD-spectrum cohorts. Deviation maps highlight region-level patterns aligned with established AD-related changes. More broadly, our results highlight the importance of training objectives that prioritize reference-distribution fidelity and robust multimodal posterior aggregation for normative modeling, with implications for deviation-based analysis across multimodal clinical data.

</details>


### [116] [Spectral Guardrails for Agents in the Wild: Detecting Tool Use Hallucinations via Attention Topology](https://arxiv.org/abs/2602.08082)
*Valentin Noël*

Main category: cs.LG

TL;DR: 该论文提出了一种基于注意力拓扑谱分析的无训练护栏方法，用于检测大语言模型的幻觉。通过在Llama 3.1 8B上实现97.7%的召回率，并发现单光谱特征可作为近乎完美的幻觉检测器，揭示幻觉是注意力变为噪声的热力学状态变化。


<details>
  <summary>Details</summary>
Motivation: 在现实环境中部署自主智能体需要可靠的防护措施来防止工具使用失败和模型幻觉。传统监督学习方法需要大量标注数据，存在局限性。

Method: 提出一种基于注意力拓扑谱分析的无训练护栏方法。通过分析模型注意力机制的频谱特征来检测幻觉，无需任何标注训练数据。使用多特征检测和单阈值方法，包括Smoothness和Entropy等谱特征。

Result: 在Llama 3.1 8B上实现97.7%的多特征检测召回率；在平衡部署下达到86.1%召回率和81.0%精确率。单光谱层特征表现突出：Llama L26 Smoothness达98.2%召回率（捕获213/217个幻觉），Mistral L3 Entropy达94.7%召回率。实验发现Llama 3.1 8B的失败是谱灾难性的（"Loud Liar"现象），而Mistral 7B达到最佳判别性能（AUC 0.900）。

Conclusion: 谱分析为智能体安全提供了一个原则性且高效的框架，无需训练即可检测幻觉，揭示了幻觉的本质是注意力热力学状态变化，具有重要的理论和实践价值。

Abstract: Deploying autonomous agents in the wild requires reliable safeguards against tool use failures. We propose a training free guardrail based on spectral analysis of attention topology that complements supervised approaches. On Llama 3.1 8B, our method achieves 97.7\% recall with multi-feature detection and 86.1\% recall with 81.0\% precision for balanced deployment, without requiring any labeled training data. Most remarkably, we discover that single layer spectral features act as near-perfect hallucination detectors: Llama L26 Smoothness achieves 98.2\% recall (213/217 hallucinations caught) with a single threshold, and Mistral L3 Entropy achieves 94.7\% recall. This suggests hallucination is not merely a wrong token but a thermodynamic state change: the model's attention becomes noise when it errs. Through controlled cross-model evaluation on matched domains ($N=1000$, $T=0.3$, same General domain, hallucination rates 20--22\%), we reveal the ``Loud Liar'' phenomenon: Llama 3.1 8B's failures are spectrally catastrophic and dramatically easier to detect, while Mistral 7B achieves the best discrimination (AUC 0.900). These findings establish spectral analysis as a principled, efficient framework for agent safety.

</details>


### [117] [Probability Hacking and the Design of Trustworthy ML for Signal Processing in C-UAS: A Scenario Based Method](https://arxiv.org/abs/2602.08086)
*Liisa Janssens,Laura Middeldorp*

Main category: cs.LG

TL;DR: 提出基于场景的方法应对AI增强型反无人机系统(C-UAS)面临的"概率黑客"风险，通过制定法律合规要求提升系统可信度，支撑人机协同信任。


<details>
  <summary>Details</summary>
Motivation: 现有反无人机系统需整合人工智能等颠覆性技术以应对无人机威胁，但机器学习组件存在"概率黑客"漏洞（即操纵概率输出破坏系统决策），可能削弱系统在军民领域人机协同中的可信度与合法性。

Method: 采用场景化分析方法，针对ML增强的C-UAS构建威胁场景，识别"概率黑客"的技术特征与风险路径，进而推导可在现有法治框架中实施的法律与技术要求。

Result: 明确了"概率黑客"的核心挑战，提出一套可嵌入法治机制的系统性要求（如算法透明度、决策可审计性），通过满足这些要求直接增强C-UAS的可信度，形成"正当化信任"基础。

Conclusion: 将法治要求与EDTs技术设计深度结合，是解决C-UAS可信度危机的关键路径，所提框架为军民领域构建安全可信的AI驱动反无人机系统提供了可操作的治理方案。

Abstract: In order to counter the various threats manifested by Unmanned Aircraft Systems (UAS) adequately, specialized Counter Unmanned Aircraft Systems (C-UAS) are required. Enhancing C-UAS with Emerging and Disruptive Technologies (EDTs) such as Artificial Intelligence (AI) can lead to more effective countermeasures. In this paper a scenario-based method is applied to C-UAS augmented with Machine Learning (ML), a subset of AI, that can enhance signal processing capabilities. Via the scenarios-based method we frame in this paper probability hacking as a challenge and identify requirements which can be implemented in existing Rule of Law mechanisms to prevent probability hacking. These requirements strengthen the trustworthiness of the C-UAS, which feed into justified trust - a key to successful Human-Autonomy Teaming, in civil and military contexts. Index Terms: C-UAS, Scenario-based method, Emerging and Disruptive Technologies, Probability hacking, Trustworthiness.

</details>


### [118] [Online Domain-aware LLM Decoding for Continual Domain Evolution](https://arxiv.org/abs/2602.08088)
*Mohammad Abu-Shaira,Weishi Shi*

Main category: cs.LG

TL;DR: 为解决静态LLM无法适应动态变化领域的挑战，本文提出ODD框架，通过概率融合基础LLM与前缀树先验，并基于自适应置信度调制实现实时在线适配，在概念漂移场景下显著提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统LLM假设领域静态且依赖离线微调，但实际业务中领域知识随法规、产品、服务、交互模式持续演化，频繁重训练计算成本过高。同时真实环境存在概念漂移导致数据分布偏移，忽略此现象会严重降低模型精度，亟需无需重训练的实时高效适配方案。

Method: 提出Online Domain-aware Decoding(ODD)框架，采用概率级融合策略整合基础LLM与前缀树先验知识，通过基于模型分歧度和连续性信号的自适应置信度调制机制动态调整融合权重，实现对新领域知识的在线捕获。

Result: 在多类概念漂移场景下，ODD在所有句法和语义生成指标上均超越LLM-Greedy与LLM-Temp Scaled基线，获得0.065的绝对ROUGE-L提升和13.6%的余弦相似度相对提升，展现出对演化词汇和上下文模式的强鲁棒性。

Conclusion: ODD通过轻量级在线解码机制有效解决了动态领域适配难题，无需昂贵重训练即可持续适应知识演化，为真实场景下LLM的长期部署提供了可行方案。

Abstract: LLMs are typically fine-tuned offline on domain-specific data, assuming a static domain. In practice, domain knowledge evolves continuously through new regulations, products, services, and interaction patterns. Retraining or fine-tuning LLMs for every new instance is computationally infeasible. Additionally, real-world environments also exhibit temporal dynamics with shifting data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. This mismatch between evolving domains and static adaptation pipelines highlights the need for efficient, real-time adaptation without costly retraining. In response, we introduce Online Domain-aware Decoding framework (ODD). ODD performs probability-level fusion between a base LLM and a prefix-tree prior, guided by adaptive confidence modulation using disagreement and continuity signals. Empirical evaluation under diverse drift scenarios demonstrates that ODD consistently surpasses LLM-Greedy and LLM-Temp Scaled across all syntactic and semantic NLG metrics. It yields an absolute ROUGE-L gain of 0.065 and a 13.6% relative improvement in Cosine Similarity over the best baseline. These results demonstrate ODD 's robustness to evolving lexical and contextual patterns, making it suitable for dynamic LLM applications.

</details>


### [119] [Variance-Gated Ensembles: An Epistemic-Aware Framework for Uncertainty Estimation](https://arxiv.org/abs/2602.08142)
*H. Martin Gillis,Isaac Xu,Thomas Trappenberg*

Main category: cs.LG

TL;DR: This paper proposes Variance-Gated Ensembles (VGE), a differentiable framework that injects epistemic sensitivity via a signal-to-noise gate. It introduces VGMU scoring for inference and VGN layers for training, outperforming state-of-the-art baselines while maintaining computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Machine learning applications require fast per-sample uncertainty estimation, but existing additive decomposition methods fail with finite-ensemble sampling and mismatched predictive distributions, creating need for more robust approaches.

Method: Variance-Gated Ensembles (VGE) uses ensemble statistics to create a signal-to-noise gate that injects epistemic sensitivity. It provides: (1) VGMU score coupling decision margins with predictive variance for inference, and (2) VGN layer for per-class learnable normalization during training, with closed-form vector-Jacobian products enabling end-to-end training.

Result: VGE matches or exceeds state-of-the-art information-theoretic uncertainty estimation methods while remaining computationally efficient, offering a practical and scalable solution for epistemic-aware uncertainty estimation.

Conclusion: VGE provides an intuitive, differentiable framework that overcomes limitations of additive decomposition, delivering robust uncertainty quantification suitable for real-world deployment in ensemble models.

Abstract: Machine learning applications require fast and reliable per-sample uncertainty estimation. A common approach is to use predictive distributions from Bayesian or approximation methods and additively decompose uncertainty into aleatoric (i.e., data-related) and epistemic (i.e., model-related) components. However, additive decomposition has recently been questioned, with evidence that it breaks down when using finite-ensemble sampling and/or mismatched predictive distributions. This paper introduces Variance-Gated Ensembles (VGE), an intuitive, differentiable framework that injects epistemic sensitivity via a signal-to-noise gate computed from ensemble statistics. VGE provides: (i) a Variance-Gated Margin Uncertainty (VGMU) score that couples decision margins with ensemble predictive variance; and (ii) a Variance-Gated Normalization (VGN) layer that generalizes the variance-gated uncertainty mechanism to training via per-class, learnable normalization of ensemble member probabilities. We derive closed-form vector-Jacobian products enabling end-to-end training through ensemble sample mean and variance. VGE matches or exceeds state-of-the-art information-theoretic baselines while remaining computationally efficient. As a result, VGE provides a practical and scalable approach to epistemic-aware uncertainty estimation in ensemble models. An open-source implementation is available at: https://github.com/nextdevai/vge.

</details>


### [120] [A second order regret bound for NormalHedge](https://arxiv.org/abs/2602.08151)
*Yoav Freund,Nicholas J. A. Harvey,Victor S. Portella,Yabing Qi,Yu-Xiang Wang*

Main category: cs.LG

TL;DR: The paper presents a NormalHedge variant for easy-sequence prediction with expert advice, proving an O(√(V_T log(V_T/ε))) second-order quantile regret bound via SDE and self-concordance analysis.


<details>
  <summary>Details</summary>
Motivation: Motivated by continuous-time limits via Stochastic Differential Equations to understand algorithm behavior on "easy" sequences where performance can be improved beyond standard bounds.

Method: Uses self-concordance techniques for discrete-time analysis, inspired by a continuous-time SDE limit of the algorithm.

Result: Achieves a second-order ε-quantile regret bound of O(√(V_T log(V_T/ε))) when V_T > log N, where V_T measures the cumulative second moment of per-expert regret under the algorithm's distribution.

Conclusion: The combination of SDE-based motivation and self-concordance analysis yields a NormalHedge variant with strong second-order regret guarantees for easy sequences.

Abstract: We consider the problem of prediction with expert advice for ``easy'' sequences. We show that a variant of NormalHedge enjoys a second-order $ε$-quantile regret bound of $O\big(\sqrt{V_T \log(V_T/ε)}\big) $ when $V_T > \log N$, where $V_T$ is the cumulative second moment of instantaneous per-expert regret averaged with respect to a natural distribution determined by the algorithm. The algorithm is motivated by a continuous time limit using Stochastic Differential Equations. The discrete time analysis uses self-concordance techniques.

</details>


### [121] [The Confidence Manifold: Geometric Structure of Correctness Representations in Language Models](https://arxiv.org/abs/2602.08159)
*Seonglae Cho,Zekun Wu,Kleyton Da Costa,Adriano Koshiyama*

Main category: cs.LG

TL;DR: 语言模型对事实正确性的内部表示具有简单的几何结构，可以用3-8个维度的低维子空间线性分离，通过质心距离可实现少样本检测，且该信号存在于模型内部但未在输出中表达


<details>
  <summary>Details</summary>
Motivation: 探究语言模型是否知道自身输出的错误（如"澳大利亚首都是悉尼"），并研究其内部正确性表示的几何结构和可检测性

Method: 对5个架构家族的9个模型进行表征分析：使用线性探针检测正确性信号，通过激活 steering 进行因果验证，比较内部探针与输出基线方法的性能

Result: 1) 正确性信号存在于3-8维子空间，线性分类器效果优于非线性；2) 质心距离与探针性能高度相关（AUC 0.90），25个标注样本可达89%全数据准确率；3) 激活 steering 使错误率变化10.9个百分点；4) 内部探针AUC 0.80-0.97，输出方法仅0.44-0.64

Conclusion: 语言模型内部存在几何结构简单的正确性表示（均值偏移模式），该信号虽未在输出中体现但可通过低维空间质心距离检测，无需复杂学习过程

Abstract: When a language model asserts that "the capital of Australia is Sydney," does it know this is wrong? We characterize the geometry of correctness representations across 9 models from 5 architecture families. The structure is simple: the discriminative signal occupies 3-8 dimensions, performance degrades with additional dimensions, and no nonlinear classifier improves over linear separation. Centroid distance in the low-dimensional subspace matches trained probe performance (0.90 AUC), enabling few-shot detection: on GPT-2, 25 labeled examples achieve 89% of full-data accuracy. We validate causally through activation steering: the learned direction produces 10.9 percentage point changes in error rates while random directions show no effect. Internal probes achieve 0.80-0.97 AUC; output-based methods (P(True), semantic entropy) achieve only 0.44-0.64 AUC. The correctness signal exists internally but is not expressed in outputs. That centroid distance matches probe performance indicates class separation is a mean shift, making detection geometric rather than learned.

</details>


### [122] [Spherical Steering: Geometry-Aware Activation Rotation for Language Models](https://arxiv.org/abs/2602.08169)
*Zejia You,Chunyuan Deng,Hanjie Chen*

Main category: cs.LG

TL;DR: 提出Spherical Steering方法，通过旋转而非添加激活向量实现无训练的语言模型推理控制，保持表示范数不变，在TruthfulQA、COPA和Storycloze等基准测试中比基线提升10%，同时维持开放生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有推理时控制方法依赖激活添加，会改变隐藏表示的幅度，引发表示崩溃和开放生成能力退化问题，需要一种保持信号完整性的新方法。

Method: 采用球面转向原语：沿测地线旋转激活向量至目标方向而非平移，保持激活范数；并引入置信度门控，根据输入不确定性动态调节转向强度。

Result: 在多项选择基准测试中显著优于添加基线（TruthfulQA、COPA、Storycloze提升10%），同时保持了模型的通用开放生成质量。

Conclusion: 几何一致的保持范数旋转是精确推理时控制的鲁棒有效原语，凸显了几何一致性在语言模型控制中的价值。

Abstract: Inference-time steering has emerged as a promising paradigm for controlling language models (LMs) without the cost of retraining. However, standard approaches typically rely on activation addition, a geometric operation that inevitably alters the magnitude of hidden representations. This raises concerns about representation collapse and degradation of open-ended generation capabilities. In this work, we explore Spherical Steering, a training-free primitive that resolves this trade-off through activation rotation. Rather than shifting activations with a fixed vector, our method rotates them along a geodesic toward a target direction, guiding the activation toward the target concept while preserving the integrity of the signal. To further enhance adaptivity, we incorporate a confidence gate that dynamically modulates steering strength based on input uncertainty. Extensive experiments across multiple-choice benchmarks demonstrate that Spherical Steering significantly outperforms addition-based baselines (notably by +10% on TruthfulQA, COPA, and Storycloze), while simultaneously maintaining the model's general open-ended generation quality. This work highlights the value of geometric consistency, suggesting that norm-preserving rotation is a robust and effective primitive for precise inference-time control.

</details>


### [123] [A Causal Machine Learning Framework for Treatment Personalization in Clinical Trials: Application to Ulcerative Colitis](https://arxiv.org/abs/2602.08171)
*Cristian Minoccheri,Sophia Tesic,Kayvan Najarian,Ryan Stidham*

Main category: cs.LG

TL;DR: 该研究构建了一个模块化因果机器学习框架，用于区分评估治疗反应异质性的统计检测价值与临床决策价值，并在乌司奴单抗溃疡性结肠炎试验中发现：尽管内窥镜特征能统计显著地预测异质性，但将其用于个性化治疗决策反而会降低疗效。


<details>
  <summary>Details</summary>
Motivation: 随机对照试验通常只估计平均治疗效果，而个性化医疗需要了解治疗反应的异质性。但关键问题是：统计上可检测的异质性是否能转化为更好的临床治疗决策？这两个问题本质不同且可能相互矛盾。

Method: 提出一个模块化因果机器学习框架：1) 置换重要性识别预测异质性的特征；2) 最佳线性预测器(BLP)检验评估统计显著性；3) 双重稳健策略评估衡量基于异质性制定治疗方案是否改善患者预后。在UNIFI维持期试验中应用此框架，使用交叉拟合的X-learner模型分析基线特征、第8周临床评分、生物标志物和内窥镜特征。

Result: BLP检验发现内窥镜特征与乌司奴单抗vs安慰剂的治疗效果异质性有强关联，但双重稳健策略评估显示纳入内窥镜特征并未提高预期缓解率，且多臂评估表现更差。内窥镜评分仅作为疾病严重程度标志物改善未治疗患者的预后预测，却增加了治疗选择的噪声；而粪便钙卫蛋白、年龄、CRP等临床变量才捕捉到决策相关的变异。

Conclusion: 临床试验中的因果机器学习应用应同时进行策略水平评估与异质性检验。统计显著的异质性不一定带来临床决策价值，必须区分预后预测能力与治疗效果异质性。

Abstract: Randomized controlled trials estimate average treatment effects, but treatment response heterogeneity motivates personalized approaches. A critical question is whether statistically detectable heterogeneity translates into improved treatment decisions -- these are distinct questions that can yield contradictory answers. We present a modular causal machine learning framework that evaluates each question separately: permutation importance identifies which features predict heterogeneity, best linear predictor (BLP) testing assesses statistical significance, and doubly robust policy evaluation measures whether acting on the heterogeneity improves patient outcomes. We apply this framework to patient-level data from the UNIFI maintenance trial of ustekinumab in ulcerative colitis, comparing placebo, standard-dose ustekinumab every 12 weeks, and dose-intensified ustekinumab every 8 weeks, using cross-fitted X-learner models with baseline demographics, medication history, week-8 clinical scores, laboratory biomarkers, and video-derived endoscopic features. BLP testing identified strong associations between endoscopic features and treatment effect heterogeneity for ustekinumab versus placebo, yet doubly robust policy evaluation showed no improvement in expected remission from incorporating endoscopic features, and out-of-fold multi-arm evaluation showed worse performance. Diagnostic comparison of prognostic contribution against policy value revealed that endoscopic scores behaved as disease severity markers -- improving outcome prediction in untreated patients but adding noise to treatment selection -- while clinical variables (fecal calprotectin, age, CRP) captured the decision-relevant variation. These results demonstrate that causal machine learning applications to clinical trials should include policy-level evaluation alongside heterogeneity testing.

</details>


### [124] [Nansde-net: A neural sde framework for generating time series with memory](https://arxiv.org/abs/2602.08182)
*Hiromu Ozai,Kei Nakagawa*

Main category: cs.LG

TL;DR: This paper proposes NANSDE-Net, a generative model using Neural Network-kernel ARMA-type noise (NA-noise) for time series with memory properties, overcoming limitations of fractional Brownian motion in Itô calculus while matching or outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Fractional Brownian motion captures long/short memory in time series but is incompatible with Itô calculus, limiting its use in neural SDE frameworks. There's a need for an Itô-process-based alternative that preserves computational tractability while modeling memory characteristics.

Method: The authors propose NA-noise, an Itô-process-based noise with neural network-parameterized kernels decomposed into product form to preserve the Markov property. Based on this, they develop NANSDE-Net, extend Neural SDEs, prove theoretical existence/uniqueness, and derive efficient backpropagation.

Result: Empirical results on synthetic and real-world datasets show NANSDE-Net matches or outperforms existing models (including fractional SDE-Net) in reproducing long/short-memory features while maintaining computational tractability within the Itô calculus framework.

Conclusion: The proposed NA-noise and NANSDE-Net provide a theoretically sound and computationally efficient framework for modeling memory effects in time series, successfully bridging the gap between memory modeling and Itô calculus compatibility.

Abstract: Modeling time series with long- or short-memory characteristics is a fundamental challenge in many scientific and engineering domains. While fractional Brownian motion has been widely used as a noise source to capture such memory effects, its incompatibility with Itô calculus limits its applicability in neural stochastic differential equation~(SDE) frameworks. In this paper, we propose a novel class of noise, termed Neural Network-kernel ARMA-type noise~(NA-noise), which is an Itô-process-based alternative capable of capturing both long- and short-memory behaviors. The kernel function defining the noise structure is parameterized via neural networks and decomposed into a product form to preserve the Markov property. Based on this noise process, we develop NANSDE-Net, a generative model that extends Neural SDEs by incorporating NA-noise. We prove the theoretical existence and uniqueness of the solution under mild conditions and derive an efficient backpropagation scheme for training. Empirical results on both synthetic and real-world datasets demonstrate that NANSDE-Net matches or outperforms existing models, including fractional SDE-Net, in reproducing long- and short-memory features of the data, while maintaining computational tractability within the Itô calculus framework.

</details>


### [125] [Dreaming in Code for Curriculum Learning in Open-Ended Worlds](https://arxiv.org/abs/2602.08194)
*Konstantinos Mitsides,Maxence Faldor,Antoine Cully*

Main category: cs.LG

TL;DR: 本文提出DiCode框架，利用基础模型生成可执行环境代码来构建开放式学习的课程，在Craftax基准测试中实现平均回报提升16%并在后期任务上取得突破性进展。


<details>
  <summary>Details</summary>
Motivation: 现有开放式学习方法虽能生成多样环境，但往往只关注孤立行为而非持续进步。在复杂开放世界中，巨大的组合挑战空间使智能体难以发现保持可学习性的经验序列。

Method: DiCode框架通过基础模型合成可执行环境代码，将"梦境"实现为世界代码级变体，在Craftax挑战性开放式基准测试中实例化，为智能体搭建通向更高能力的学习脚手架。

Result: DiCode使智能体获得长视野技能，平均回报比最强基线提高16%，在先前方法失败的后期战斗任务上实现了非零成功率。

Conclusion: 代码级环境设计为课程控制提供了实用机制，能够构建 bridging competence gaps 的中间环境，有效支持开放式世界中的能力递进学习。

Abstract: Open-ended learning frames intelligence as emerging from continual interaction with an ever-expanding space of environments. While recent advances have utilized foundation models to programmatically generate diverse environments, these approaches often focus on discovering isolated behaviors rather than orchestrating sustained progression. In complex open-ended worlds, the large combinatorial space of possible challenges makes it difficult for agents to discover sequences of experiences that remain consistently learnable. To address this, we propose Dreaming in Code (DiCode), a framework in which foundation models synthesize executable environment code to scaffold learning toward increasing competence. In DiCode, "dreaming" takes the form of materializing code-level variations of the world. We instantiate DiCode in Craftax, a challenging open-ended benchmark characterized by rich mechanics and long-horizon progression. Empirically, DiCode enables agents to acquire long-horizon skills, achieving a $16\%$ improvement in mean return over the strongest baseline and non-zero success on late-game combat tasks where prior methods fail. Our results suggest that code-level environment design provides a practical mechanism for curriculum control, enabling the construction of intermediate environments that bridge competence gaps in open-ended worlds. Project page and source code are available at https://konstantinosmitsides.github.io/dreaming-in-code and https://github.com/konstantinosmitsides/dreaming-in-code.

</details>


### [126] [CADO: From Imitation to Cost Minimization for Heatmap-based Solvers in Combinatorial Optimization](https://arxiv.org/abs/2602.08210)
*Hyungseok Song,Deunsol Yoon,Kanghoon Lee,Han-Seul Jeong,Soonyoung Lee,Woohyung Lim*

Main category: cs.LG

TL;DR: The paper identifies objective mismatch in supervised learning for heatmap-based CO solvers (Decoder-Blindness and Cost-Blindness) and proposes CADO, an RL fine-tuning framework using diffusion models with Label-Centered Reward to directly optimize solution cost, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current heatmap-based solvers for Combinatorial Optimization trained with Supervised Learning suffer from objective mismatch: minimizing imitation loss doesn't guarantee minimizing solution cost. This creates two problems: Decoder-Blindness (ignoring the non-differentiable decoding process) and Cost-Blindness (prioritizing structural imitation over solution quality), which imposes a hard performance ceiling.

Method: The authors propose CADO (Cost-Aware Diffusion models for Optimization), a streamlined Reinforcement Learning fine-tuning framework that: 1) Formulates the diffusion denoising process as a Markov Decision Process (MDP) to directly optimize post-decoded solution cost; 2) Introduces Label-Centered Reward, which repurposes ground-truth labels as unbiased baselines rather than imitation targets; 3) Uses Hybrid Fine-Tuning for parameter-efficient adaptation.

Result: CADO achieves state-of-the-art performance across diverse benchmarks.

Conclusion: Objective alignment is essential for unlocking the full potential of heatmap-based solvers. Directly optimizing the post-decoded solution cost through RL fine-tuning overcomes the limitations of supervised learning and removes the performance ceiling.

Abstract: Heatmap-based solvers have emerged as a promising paradigm for Combinatorial Optimization (CO). However, we argue that the dominant Supervised Learning (SL) training paradigm suffers from a fundamental objective mismatch: minimizing imitation loss (e.g., cross-entropy) does not guarantee solution cost minimization. We dissect this mismatch into two deficiencies: Decoder-Blindness (being oblivious to the non-differentiable decoding process) and Cost-Blindness (prioritizing structural imitation over solution quality). We empirically demonstrate that these intrinsic flaws impose a hard performance ceiling. To overcome this limitation, we propose CADO (Cost-Aware Diffusion models for Optimization), a streamlined Reinforcement Learning fine-tuning framework that formulates the diffusion denoising process as an MDP to directly optimize the post-decoded solution cost. We introduce Label-Centered Reward, which repurposes ground-truth labels as unbiased baselines rather than imitation targets, and Hybrid Fine-Tuning for parameter-efficient adaptation. CADO achieves state-of-the-art performance across diverse benchmarks, validating that objective alignment is essential for unlocking the full potential of heatmap-based solvers.

</details>


### [127] [DrugR: Optimizing Molecular Drugs through LLM-based Explicit Reasoning](https://arxiv.org/abs/2602.08213)
*Haoran Liu,Zheni Zeng,Yukun Yan,Yuxuan Chen,Yunduo Xiao*

Main category: cs.LG

TL;DR: DrugR is an LLM-based molecule optimization method that introduces explicit pharmacological reasoning through domain-specific pretraining, reverse data engineering fine-tuning, and multi-granular reinforcement learning to improve ADMET properties while preserving efficacy.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex implicit relationships between molecular structures and pharmacological properties, and lack sufficient labeled data for effective molecule optimization.

Method: Integrates three components: domain-specific continual pretraining, supervised fine-tuning via reverse data engineering, and self-balanced multi-granular reinforcement learning to enable explicit step-by-step pharmacological reasoning.

Result: Achieves comprehensive enhancement across multiple ADMET properties while maintaining structural similarity and target binding affinity, with interpretable rationales for each optimization step.

Conclusion: Provides actionable design insights and advances toward automated, knowledge-driven scientific discovery in molecular design, with open-sourced code to foster future research.

Abstract: Molecule generation and optimization is a fundamental task in chemical domain. The rapid development of intelligent tools, especially large language models (LLMs) with powerful knowledge reserves and interactive capabilities, has provided new paradigms for it. Nevertheless, the intrinsic challenge for LLMs lies in the complex implicit relationship between molecular structure and pharmacological properties and the lack of corresponding labeled data. To bridge this gap, we propose DrugR, an LLM-based method that introduces explicit, step-by-step pharmacological reasoning into the optimization process. Our approach integrates domain-specific continual pretraining, supervised fine-tuning via reverse data engineering, and self-balanced multi-granular reinforcement learning. This framework enables DrugR to effectively improve key ADMET properties while preserving the original molecule's core efficacy. Experimental results demonstrate that DrugR achieves comprehensive enhancement across multiple properties without compromising structural similarity or target binding affinity. Importantly, its explicit reasoning process provides clear, interpretable rationales for each optimization step, yielding actionable design insights and advancing toward automated, knowledge-driven scientific discovery. Our code and model checkpoints are open-sourced to foster future research.

</details>


### [128] [Distribution-Free Robust Functional Predict-Then-Optimize](https://arxiv.org/abs/2602.08215)
*Yash Patel,Ambuj Tewari*

Main category: cs.LG

TL;DR: 提出用 conformal prediction 为神经算子模型提供分布无关的不确定性量化，并用于鲁棒决策，实验证明优于高斯过程。


<details>
  <summary>Details</summary>
Motivation: 神经算子在求解 PDE 决策问题时缺乏可靠的不确定性量化，而现有集成或贝叶斯方法在实践中存在分布假设不成立或可扩展性差的问题，限制了实际应用。

Method: 将 conformal prediction 创新性地应用于神经算子的函数空间映射，生成分布无关的预测区域；利用无限维 Danskin 定理和变分法高效求解鲁棒决策问题。

Result: 在多个工程任务上，该方法相比高斯过程等限制性建模范式展现出更优的性能，并实现了正式的风险（regret）表征。

Conclusion: 该方法为神经算子提供了实用且可扩展的不确定性量化，使下游鲁棒决策具有理论保证，在工程应用中具有显著优势。

Abstract: The solution of PDEs in decision-making tasks is increasingly being undertaken with the help of neural operator surrogate models due to the need for repeated evaluation. Such methods, while significantly more computationally favorable compared to their numerical counterparts, fail to provide any calibrated notions of uncertainty in their predictions. Current methods approach this deficiency typically with ensembling or Bayesian posterior estimation. However, these approaches either require distributional assumptions that fail to hold in practice or lack practical scalability, limiting their applications in practice. We, therefore, propose a novel application of conformal prediction to produce distribution-free uncertainty quantification over the function spaces mapped by neural operators. We then demonstrate how such prediction regions enable a formal regret characterization if leveraged in downstream robust decision-making tasks. We further demonstrate how such posited robust decision-making tasks can be efficiently solved using an infinite-dimensional generalization of Danskin's Theorem and calculus of variations and empirically demonstrate the superior performance of our proposed method over more restrictive modeling paradigms, such as Gaussian Processes, across several engineering tasks.

</details>


### [129] [Sparsity-Aware Evolution for Model Merging](https://arxiv.org/abs/2602.08218)
*Huan Zhang,Yanjian Zhang,Guillaume Wisniewski,Nadi Tomeh,Bang Liu*

Main category: cs.LG

TL;DR: 提出稀疏感知进化(SAE)框架，通过迭代剪枝-合并循环作为新型变异算子，将稀疏约束融入评分函数，在大型LLM基准测试中提升模型合并可靠性，且与现有方法正交易用


<details>
  <summary>Details</summary>
Motivation: 模型合并技术需要进一步优化，通过引入稀疏性约束来引导进化过程，可以在保持性能的同时获得更稀疏、高效的合并模型

Method: 设计稀疏感知进化框架，采用迭代剪枝-合并循环作为变异算子，在评分函数中加入稀疏约束，使进化过程偏好更稀疏的模型结构

Result: 在多种大规模LLM基准测试中，该方法能提升模型合并的可靠性，其稀疏竞争机制引入了额外的局部吸引力和交互效应，且框架简单、与现有方法正交，易于集成

Conclusion: 稀疏感知进化框架通过创新的剪枝-合并循环和稀疏约束机制，有效增强了模型合并效果，具有实用价值和良好的可扩展性

Abstract: We propose a sparsity-aware evolutionary (SAE) framework for model merging that involves iterative pruning-merging cycles to act as a novel mutation operator. We incorporate the sparsity constraints into the score function, which steers the evolutionary process to favor more sparse models, in addition to other conventional performance scores. Interestingly, the by-product of \textit{competition} for sparsity introduces an extra local \textit{attraction} and interplay into the evolutionary process: if one competitor has more zero elements, the other competitor's non-zero elements will occupy those positions, even though the less sparse competitor loses to the more sparse competitor in other positions. The proposed pipeline is evaluated on a variety of large-scale LLM benchmarks. Experiments demonstrate that our approach can improve model merging reliability across multiple benchmarks, and is easy to incorporate due to its simplicity and being orthogonal to most existing approaches.

</details>


### [130] [Linearization Explains Fine-Tuning in Large Language Models](https://arxiv.org/abs/2602.08239)
*Zahra Rahimi Afzal,Tara Esmaeilbeig,Mojtaba Soltanalian,Mesrob I. Ohannessian*

Main category: cs.LG

TL;DR: 通过线性化视角分析参数高效微调(PEFT)，发现加入欧氏距离偏置的微调等价于神经正切核(NTK)学习，揭示NTK特征谱与模型性能强相关，并提供层选择的光谱扰动边界，实验验证于LoRA。


<details>
  <summary>Details</summary>
Motivation: PEFT技术虽流行，但其训练性能和泛化机制尚不明确，缺乏理论理解。

Method: 引入欧氏距离归纳偏置使微调显式保持靠近预训练模型，通过线性化将微调动力学转化为NTK学习框架，分析全线性与线性化优化的接近程度，并推导层选择对NTK的光谱扰动边界。

Result: 1) 当线性化是良好模型时，NTK特征谱与模型适应性能存在强相关性；2) 给出了基于微调层选择的NTK光谱扰动理论边界；3) 在LLM的LoRA上实证验证了理论。

Conclusion: 这些见解不仅表征了微调机制，还有望增强PEFT技术，为LLM的更智能、更灵活的适应铺平道路。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) is a popular class of techniques that strive to adapt large models in a scalable and resource-efficient manner. Yet, the mechanisms underlying their training performance and generalization remain underexplored. In this paper, we provide several insights into such fine-tuning through the lens of linearization. Fine-tuned models are often implicitly encouraged to remain close to the pretrained model. By making this explicit, using an Euclidean distance inductive bias in parameter space, we show that fine-tuning dynamics become equivalent to learning with the positive-definite neural tangent kernel (NTK). We specifically analyze how close the fully linear and the linearized fine-tuning optimizations are, based on the strength of the regularization. This allows us to be pragmatic about how good a model linearization is when fine-tuning large language models (LLMs). When linearization is a good model, our findings reveal a strong correlation between the eigenvalue spectrum of the NTK and the performance of model adaptation. Motivated by this, we give spectral perturbation bounds on the NTK induced by the choice of layers selected for fine-tuning. We empirically validate our theory on Low Rank Adaptation (LoRA) on LLMs. These insights not only characterize fine-tuning but also have the potential to enhance PEFT techniques, paving the way to better informed and more nimble adaptation in LLMs.

</details>


### [131] [Learning in Context, Guided by Choice: A Reward-Free Paradigm for Reinforcement Learning with Transformers](https://arxiv.org/abs/2602.08244)
*Juncheng Dong,Bowen He,Moyang Guo,Ethan X. Fang,Zhuoran Yang,Vahid Tarokh*

Main category: cs.LG

TL;DR: This paper proposes In-Context Preference-based Reinforcement Learning (ICPRL), a new paradigm that uses only preference feedback instead of explicit reward signals for transformer-based reinforcement learning. The method shows comparable performance to traditional reward-supervised ICRL on various decision-making tasks.


<details>
  <summary>Details</summary>
Motivation: Existing in-context reinforcement learning (ICRL) methods require explicit reward signals during pretraining, which limits their applicability when rewards are ambiguous, hard to specify, or costly to obtain.

Method: Proposes ICPRL with two variants: Immediate Preference-based RL (I-PRL) with per-step preferences and Trajectory Preference-based RL (T-PRL) with trajectory-level comparisons. Introduces preference-native frameworks that directly optimize transformer policies from preference data without reward signals or optimal action labels.

Result: Experiments on dueling bandits, navigation, and continuous control tasks demonstrate that ICPRL achieves strong in-context generalization to unseen tasks with performance comparable to ICRL methods trained with full reward supervision.

Conclusion: ICPRL successfully eliminates the need for reward supervision while maintaining strong generalization capabilities, making ICRL applicable to scenarios where explicit rewards are difficult to obtain.

Abstract: In-context reinforcement learning (ICRL) leverages the in-context learning capabilities of transformer models (TMs) to efficiently generalize to unseen sequential decision-making tasks without parameter updates. However, existing ICRL methods rely on explicit reward signals during pretraining, which limits their applicability when rewards are ambiguous, hard to specify, or costly to obtain. To overcome this limitation, we propose a new learning paradigm, In-Context Preference-based Reinforcement Learning (ICPRL), in which both pretraining and deployment rely solely on preference feedback, eliminating the need for reward supervision. We study two variants that differ in the granularity of feedback: Immediate Preference-based RL (I-PRL) with per-step preferences, and Trajectory Preference-based RL (T-PRL) with trajectory-level comparisons. We first show that supervised pretraining, a standard approach in ICRL, remains effective under preference-only context datasets, demonstrating the feasibility of in-context reinforcement learning using only preference signals. To further improve data efficiency, we introduce alternative preference-native frameworks for I-PRL and T-PRL that directly optimize TM policies from preference data without requiring reward signals nor optimal action labels.Experiments on dueling bandits, navigation, and continuous control tasks demonstrate that ICPRL enables strong in-context generalization to unseen tasks, achieving performance comparable to ICRL methods trained with full reward supervision.

</details>


### [132] [Constraint-Aware Generative Auto-bidding via Pareto-Prioritized Regret Optimization](https://arxiv.org/abs/2602.08261)
*Binglin Wu,Yingyi Zhang,Xianneng Li,Ruyue Deng,Chuan Yue,Weiru Zhang,Xiaoyi Zeng*

Main category: cs.LG

TL;DR: PRO-Bid是一个基于约束感知的生成式自动出价框架，通过约束解耦的帕累托表示和因果遗憾优化，解决决策Transformer在约束条件下状态混叠和无法逼近约束边界的问题。


<details>
  <summary>Details</summary>
Motivation: 自动出价系统需在满足目标转化成本等严格效率约束下最大化营销价值，但标准决策Transformer存在忽略成本维度的状态混叠问题，且回归方法限制模型向约束边界优化。

Method: 提出PRO-Bid框架，包含两大机制：1）约束解耦帕累托表示（CDPR）将全局约束分解为递归成本价值上下文，按帕累托前沿重加权轨迹；2）因果遗憾优化（CRO）利用全局结果预测器识别优势反事实动作作为加权回归目标。

Result: 在两个公开基准和在线A/B测试中，PRO-Bid相比现有最优基线展现出更优的约束满足和价值获取能力。

Conclusion: PRO-Bid成功克服了决策Transformer在约束自动出价场景中的局限性，通过约束解耦和因果优化实现了性能突破。

Abstract: Auto-bidding systems aim to maximize marketing value while satisfying strict efficiency constraints such as Target Cost-Per-Action (CPA). Although Decision Transformers provide powerful sequence modeling capabilities, applying them to this constrained setting encounters two challenges: 1) standard Return-to-Go conditioning causes state aliasing by neglecting the cost dimension, preventing precise resource pacing; and 2) standard regression forces the policy to mimic average historical behaviors, thereby limiting the capacity to optimize performance toward the constraint boundary. To address these challenges, we propose PRO-Bid, a constraint-aware generative auto-bidding framework based on two synergistic mechanisms: 1) Constraint-Decoupled Pareto Representation (CDPR) decomposes global constraints into recursive cost and value contexts to restore resource perception, while reweighting trajectories based on the Pareto frontier to focus on high-efficiency data; and 2) Counterfactual Regret Optimization (CRO) facilitates active improvement by utilizing a global outcome predictor to identify superior counterfactual actions. By treating these high-utility outcomes as weighted regression targets, the model transcends historical averages to approach the optimal constraint boundary. Extensive experiments on two public benchmarks and online A/B tests demonstrate that PRO-Bid achieves superior constraint satisfaction and value acquisition compared to state-of-the-art baselines.

</details>


### [133] [When Do Multi-Agent Systems Outperform? Analysing the Learning Efficiency of Agentic Systems](https://arxiv.org/abs/2602.08272)
*Junwei Su,Chuan Wu*

Main category: cs.LG

TL;DR: This paper theoretically analyzes when Multi-Agent RL outperforms Single-Agent RL for training LLMs. Using PAC learning framework, it shows MARL improves sample efficiency for naturally decomposable independent subtasks but loses advantage for dependent tasks. It introduces "task alignment" to quantify trade-offs and provides practical deployment criteria.


<details>
  <summary>Details</summary>
Motivation: The lack of theoretical understanding about when and why MARL outperforms SARL for LLM training creates uncertainty in framework selection, despite MARL's potential for decomposing complex tasks into specialized subtasks.

Method: The authors employ the Probably Approximately Correct (PAC) framework to formally define SARL and MARL setups for LLMs, derive explicit sample complexity bounds, and systematically analyze how task decomposition and alignment affect learning efficiency.

Result: MARL improves sample complexity for tasks that naturally decompose into independent subtasks, while dependent subtasks reduce MARL's advantage. The paper introduces task alignment concept, quantifying trade-offs when enforcing independent decomposition despite misalignments.

Conclusion: The theoretical analysis clarifies empirical inconsistencies and provides practical guidelines for effectively deploying MARL strategies in complex LLM scenarios.

Abstract: Reinforcement Learning (RL) has emerged as a crucial method for training or fine-tuning large language models (LLMs), enabling adaptive, task-specific optimizations through interactive feedback. Multi-Agent Reinforcement Learning (MARL), in particular, offers a promising avenue by decomposing complex tasks into specialized subtasks learned by distinct interacting agents, potentially enhancing the ability and efficiency of LLM systems. However, theoretical insights regarding when and why MARL outperforms Single-Agent RL (SARL) remain limited, creating uncertainty in selecting the appropriate RL framework. In this paper, we address this critical gap by rigorously analyzing the comparative sample efficiency of MARL and SARL within the context of LLM. Leveraging the Probably Approximately Correct (PAC) framework, we formally define SARL and MARL setups for LLMs, derive explicit sample complexity bounds, and systematically characterize how task decomposition and alignment influence learning efficiency. Our results demonstrate that MARL improves sample complexity when tasks naturally decompose into independent subtasks, whereas dependent subtasks diminish MARL's comparative advantage. Additionally, we introduce and analyze the concept of task alignment, quantifying the trade-offs when enforcing independent task decomposition despite potential misalignments. These theoretical insights clarify empirical inconsistencies and provide practical criteria for deploying MARL strategies effectively in complex LLM scenarios.

</details>


### [134] [Trust-Based Incentive Mechanisms in Semi-Decentralized Federated Learning Systems](https://arxiv.org/abs/2602.08290)
*Ajay Kumar Shrestha*

Main category: cs.LG

TL;DR: Proposes a blockchain-integrated trust-based incentive mechanism for federated learning that dynamically evaluates node contributions using trust scores based on data quality, model accuracy, consistency, and frequency to create a robust, transparent ecosystem.


<details>
  <summary>Details</summary>
Motivation: Malicious or faulty nodes in federated learning can degrade model performance, challenging system integrity and reliability while maintaining data privacy.

Method: Dynamic trust scoring based on data quality, model accuracy, consistency, and contribution frequency, combined with blockchain smart contracts for automated trust evaluation and incentive distribution.

Result: A theoretical framework proposal for a more robust, fair, and transparent federated learning ecosystem that reduces risks from untrustworthy participants.

Conclusion: The mechanism encourages honest participation and penalizes malicious behavior through automated, transparent trust evaluation and incentive allocation.

Abstract: In federated learning (FL), decentralized model training allows multi-ple participants to collaboratively improve a shared machine learning model without exchanging raw data. However, ensuring the integrity and reliability of the system is challenging due to the presence of potentially malicious or faulty nodes that can degrade the model's performance. This paper proposes a novel trust-based incentive mechanism designed to evaluate and reward the quality of contributions in FL systems. By dynamically assessing trust scores based on fac-tors such as data quality, model accuracy, consistency, and contribution fre-quency, the system encourages honest participation and penalizes unreliable or malicious behavior. These trust scores form the basis of an incentive mechanism that rewards high-trust nodes with greater participation opportunities and penal-ties for low-trust participants. We further explore the integration of blockchain technology and smart contracts to automate the trust evaluation and incentive distribution processes, ensuring transparency and decentralization. Our proposed theoretical framework aims to create a more robust, fair, and transparent FL eco-system, reducing the risks posed by untrustworthy participants.

</details>


### [135] [TextResNet: Decoupling and Routing Optimization Signals in Compound AI Systems via Deep Residual Tuning](https://arxiv.org/abs/2602.08306)
*Suizhi Huang,Mei Li,Han Yu,Xiaoxiao Li*

Main category: cs.LG

TL;DR: TextResNet solves TextGrad's deep-chain limitation by reformulating optimization with four innovations: additive semantic deltas, gradient decomposition, causal routing, and density-aware scheduling, achieving superior performance and stability.


<details>
  <summary>Details</summary>
Motivation: Textual gradient-style optimizers (TextGrad) fail in deep chains due to Semantic Entanglement, where feedback signals mix local critiques with upstream contexts, creating Attribution Ambiguity that prevents effective learning.

Method: Proposes TextResNet framework with four key components: (1) Additive Semantic Deltas preserving an Identity Highway for gradient flow; (2) Semantic Gradient Decomposition via a Semantic Projector to disentangle feedback into independent subspaces; (3) Causal Routing for precise signal allocation; (4) Density-Aware Optimization Scheduling to dynamically allocate resources to bottlenecks.

Result: Outperforms TextGrad significantly and demonstrates remarkable stability in compound AI systems where baseline methods collapse, successfully enabling deep-chain optimization.

Conclusion: TextResNet effectively addresses Semantic Entanglement and Attribution Ambiguity, making gradient-like feedback propagation viable for deep agentic workflows in compound AI systems.

Abstract: Textual Gradient-style optimizers (TextGrad) enable gradient-like feedback propagation through compound AI systems. However, they do not work well for deep chains. The root cause of this limitation stems from the Semantic Entanglement problem in these extended workflows. In standard textual backpropagation, feedback signals mix local critiques with upstream contexts, leading to Attribution Ambiguity. To address this challenge, we propose TextResNet, a framework that reformulates the optimization process to achieve precise signal routing via four key innovations. Firstly, in the forward pass, it enforces Additive Semantic Deltas to preserve an Identity Highway for gradient flow. Secondly, in the backward pass, it introduces Semantic Gradient Decomposition via a Semantic Projector to disentangle feedback into causally independent subspaces. Thirdly, it implements Causal Routing, which routes projected signals to their specific components. Finally, it performs Density-Aware Optimization Scheduling to leverage the disentangled signals to dynamically allocate resources to key system bottlenecks. Our results show that TextResNet not only achieves superior performance compared to TextGrad, but also exhibits remarkable stability for agentic tasks in compound AI systems where baselines collapse. Code is available at https://github.com/JeanDiable/TextResNet.

</details>


### [136] [Interaction-Grounded Learning for Contextual Markov Decision Processes with Personalized Feedback](https://arxiv.org/abs/2602.08307)
*Mengxiao Zhang,Yuheng Zhang,Haipeng Luo,Paul Mineiro*

Main category: cs.LG

TL;DR: 该论文将Interaction-Grounded Learning从单步设置扩展到多步序贯决策问题，提出了针对情境化片段式马尔可夫决策过程的高效算法，通过改进奖励估计器和逆间隔加权策略优化，实现了亚线性遗憾界，并在合成数据和真实用户预订数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中（如多轮大语言模型交互）学习器接收的是未知机制生成的间接反馈而非显式数值奖励，但现有IGL研究仅局限于单步设置，无法应用于现代多步序贯决策系统。

Method: 1）将Zhang等人[2024a]的单步奖励估计器扩展到多步MDP设置，解决潜在奖励解码的独特挑战；2）基于该估计器设计逆间隔加权（IGW）策略优化算法；3）在合成片段式MDP和真实用户预订数据集上进行实验验证。

Result: 提出了计算高效的算法，在具有个性化反馈的情境化片段式MDP中实现了亚线性遗憾保证，实验证明了在多轮交互中学习个性化目标的有效性。

Conclusion: 成功填补了单步IGL与多步序贯决策系统之间的空白，为LLM部署等现实场景提供了理论保障的解决方案。

Abstract: In this paper, we study Interaction-Grounded Learning (IGL) [Xie et al., 2021], a paradigm designed for realistic scenarios where the learner receives indirect feedback generated by an unknown mechanism, rather than explicit numerical rewards. While prior work on IGL provides efficient algorithms with provable guarantees, those results are confined to single-step settings, restricting their applicability to modern sequential decision-making systems such as multi-turn Large Language Model (LLM) deployments. To bridge this gap, we propose a computationally efficient algorithm that achieves a sublinear regret guarantee for contextual episodic Markov Decision Processes (MDPs) with personalized feedback. Technically, we extend the reward-estimator construction of Zhang et al. [2024a] from the single-step to the multi-step setting, addressing the unique challenges of decoding latent rewards under MDPs. Building on this estimator, we design an Inverse-Gap-Weighting (IGW) algorithm for policy optimization. Finally, we demonstrate the effectiveness of our method in learning personalized objectives from multi-turn interactions through experiments on both a synthetic episodic MDP and a real-world user booking dataset.

</details>


### [137] [Fast Flow Matching based Conditional Independence Tests for Causal Discovery](https://arxiv.org/abs/2602.08315)
*Shunyu Zhao,Yanfeng Yang,Shuai Li,Kenji Fukumizu*

Main category: cs.LG

TL;DR: 提出基于流匹配的条件独立性检验(FMCIT)，通过一次训练实现高效因果发现，并集成到两阶段GPC-FMCIT框架中，在保持统计功效的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 约束型因果发现方法需要大量条件独立性检验，计算复杂度高，严重限制了实用性，亟需设计加速单个检验的算法。

Method: 提出FMCIT利用流匹配的高效性，在整个因果发现过程中仅需一次模型训练；进一步将其集成到两阶段引导PC骨架学习框架GPC-FMCIT，结合快速筛选与预算引导的精炼。

Result: FMCIT能有效控制第一类错误率并在高维条件集下保持高检验功效；GPC-FMCIT可明确限制CI查询次数同时保持统计功效；在合成和真实因果发现任务中均展现出优于现有方法的精度-效率平衡。

Conclusion: 所提出的FMCIT和GPC-FMCIT方法通过高效流匹配模型和两阶段框架设计，在维持统计有效性的前提下，显著加速了因果发现过程，实现了良好的计算效率与准确性权衡。

Abstract: Constraint-based causal discovery methods require a large number of conditional independence (CI) tests, which severely limits their practical applicability due to high computational complexity. Therefore, it is crucial to design an algorithm that accelerates each individual test. To this end, we propose the Flow Matching-based Conditional Independence Test (FMCIT). The proposed test leverages the high computational efficiency of flow matching and requires the model to be trained only once throughout the entire causal discovery procedure, substantially accelerating causal discovery. According to numerical experiments, FMCIT effectively controls type-I error and maintains high testing power under the alternative hypothesis, even in the presence of high-dimensional conditioning sets. In addition, we further integrate FMCIT into a two-stage guided PC skeleton learning framework, termed GPC-FMCIT, which combines fast screening with guided, budgeted refinement using FMCIT. This design yields explicit bounds on the number of CI queries while maintaining high statistical power. Experiments on synthetic and real-world causal discovery tasks demonstrate favorable accuracy-efficiency trade-offs over existing CI testing methods and PC variants.

</details>


### [138] [Towards Efficient Large Language Reasoning Models via Extreme-Ratio Chain-of-Thought Compression](https://arxiv.org/abs/2602.08324)
*Yuntian Tang,Bohan Jia,Wenxuan Huang,Lianyue Zhang,Jiao Xie,Wenxi Li,Wei Li,Jie Hu,Xinghao Chen,Rongrong Ji,Shaohui Lin*

Main category: cs.LG

TL;DR: 提出EXTreme-RAtio Chain-of-Thought Compression (Extra-CoT)框架，通过语义保持压缩器、混合比例监督和分层奖励强化学习，在减少73%推理token的同时保持甚至提升答案准确率，解决了大语言模型思维链推理中的高计算开销问题。


<details>
  <summary>Details</summary>
Motivation: 思维链(CoT)推理虽能提升大语言模型的推理能力，但带来巨大的推理计算开销。现有压缩方法在高压缩率下会出现严重的逻辑保真度损失，导致性能显著下降。

Method: 1) 在数学CoT数据上训练专用的语义保持压缩器，生成高质量压缩监督对；2) 通过混合比例监督微调使模型适应不同压缩预算；3) 提出约束分层比例策略优化(CHRPO)算法，通过分层奖励在低预算下显式激励解题能力。

Result: 在三个数学推理基准测试上表现优异。以Qwen3-1.7B和MATH-500为例，Extra-CoT实现超过73%的token压缩率，准确率反而提升0.6%，显著优于现有最优方法。

Conclusion: Extra-CoT框架实现了极端比例下的高保真思维链压缩，在大幅降低计算成本的同时保持甚至提升模型推理准确率，为大语言模型的高效推理提供了有效解决方案。

Abstract: Chain-of-Thought (CoT) reasoning successfully enhances the reasoning capabilities of Large Language Models (LLMs), yet it incurs substantial computational overhead for inference. Existing CoT compression methods often suffer from a critical loss of logical fidelity at high compression ratios, resulting in significant performance degradation. To achieve high-fidelity, fast reasoning, we propose a novel EXTreme-RAtio Chain-of-Thought Compression framework, termed Extra-CoT, which aggressively reduces the token budget while preserving answer accuracy. To generate reliable, high-fidelity supervision, we first train a dedicated semantically-preserved compressor on mathematical CoT data with fine-grained annotations. An LLM is then fine-tuned on these compressed pairs via a mixed-ratio supervised fine-tuning (SFT), teaching it to follow a spectrum of compression budgets and providing a stable initialization for reinforcement learning (RL). We further propose Constrained and Hierarchical Ratio Policy Optimization (CHRPO) to explicitly incentivize question-solving ability under lower budgets by a hierarchical reward. Experiments on three mathematical reasoning benchmarks show the superiority of Extra-CoT. For example, on MATH-500 using Qwen3-1.7B, Extra-CoT achieves over 73\% token reduction with an accuracy improvement of 0.6\%, significantly outperforming state-of-the-art (SOTA) methods.

</details>


### [139] [ManifoldKV: Training-Free KV Cache Compression via Euclidean Outlier Detection](https://arxiv.org/abs/2602.08343)
*Debajyoti Datta,Trishala Neeraj,Bibek Paudel,Vyom Sharma,Subhabrata Mukherjee*

Main category: cs.LG

TL;DR: 提出ManifoldKV，一种无需训练的KV缓存压缩方法，通过欧氏距离而非余弦相似度对token进行评分，在长文本推理任务中表现优异，仅需3行代码即可实现。


<details>
  <summary>Details</summary>
Motivation: 现有基于几何的KV缓存压缩方法使用余弦相似度评分，但余弦相似度是尺度不变的，会丢失区分语义重要token的幅度信息，在多个key检索和超长上下文场景中表现不佳。

Method: 提出ManifoldKV：使用欧氏距离到key质心进行token排序，同时捕获角度和径向偏差；针对64K长上下文引入WindowedManifoldKV解决全局质心稀释问题。该方法无需训练，仅需3行代码。

Result: 在RULER基准测试中，4K-16K上下文20%压缩率下达到95.7%准确率；多key检索任务中，3-key NIAH的50%压缩下达到92.4%，比KeyDiff高15.4个点；64K长上下文25%压缩下，WindowedManifoldKV达到84.3%准确率，比全局L2恢复49个点，比KeyDiff高3.2个点。

Conclusion: ManifoldKV是一种简单有效的KV缓存压缩方法，欧氏距离评分在多个场景下优于余弦相似度，WindowedManifoldKV成功解决了超长上下文的性能崩溃问题，且无需训练、代码改动小、跨4种架构无需调参。

Abstract: Long-context inference is constrained by KV-cache memory, which grows linearly with sequence length; KV-cache compression therefore hinges on reliably selecting which past tokens to retain. Most geometry-based eviction methods score keys by cosine similarity to a global centroid, but cosine is scale-invariant and can discard magnitude cues that distinguish semantically salient tokens. We propose ManifoldKV, a training-free scorer that ranks tokens by Euclidean distance to the key centroid, capturing both angular and radial deviations.
  On the RULER benchmark, ManifoldKV achieves 95.7% accuracy at 4K-16K contexts with 20% compression; matching the best geometric baseline while improving robustness in two regimes where cosine scoring fails. First, on multi-key retrieval, ManifoldKV reduces directional collisions, achieving 92.4% vs KeyDiff's 77.0% (+15.4 points) on 3-key NIAH at 50% compression. Second, to address dilution and performance collapse of global centroids at 64K context, we introduce WindowedManifoldKV, which restores accuracy to 84.3% at 25% compression, a 49-point recovery over global L2 and +3.2 points over KeyDiff. The method requires only 3 lines of code and works across 4 architectures without tuning.

</details>


### [140] [All ERMs Can Fail in Stochastic Convex Optimization Lower Bounds in Linear Dimension](https://arxiv.org/abs/2602.08350)
*Tal Burla,Roi Livni*

Main category: cs.LG

TL;DR: This paper shows that even in best-case convex optimization, ERM overfits with sample size linear in dimension, and proves Gradient Descent also overfits, providing a new Ω(√(ηT/m^1.5)) lower bound that exponentially narrows the gap between known upper and lower bounds.


<details>
  <summary>Details</summary>
Motivation: Understanding when learning algorithms overfit is fundamental. The paper resolves Feldman's open question about ERM sample complexity in stochastic convex optimization, revealing that overfitting can occur even in "nice" convex settings where learning is possible.

Method: The authors construct a specific instance of stochastic convex optimization to demonstrate unique, overfitting ERM. They extend this analysis to approximate ERMs and constrained Gradient Descent, developing a novel generalization lower bound technique.

Result: 1) An instance exists where sample size linear in dimension yields unique, overfitting ERM (resolving Feldman's question). 2) This extends to approximate ERMs. 3) A new generalization lower bound Ω(√(ηT/m^1.5)) for Gradient Descent exponentially narrows the gap between the O(ηT/m) upper bound and previous lower bounds.

Conclusion: Overfitting is more pervasive than thought, occurring even in ideal convex settings. The stronger lower bound reveals Gradient Descent's overfitting tendency, with practical implications for hyperparameter tuning (learning rate η and horizon T) relative to sample size m.

Abstract: We study the sample complexity of the best-case Empirical Risk Minimizer in the setting of stochastic convex optimization. We show that there exists an instance in which the sample size is linear in the dimension, learning is possible, but the Empirical Risk Minimizer is likely to be unique and to overfit. This resolves an open question by Feldman. We also extend this to approximate ERMs.
  Building on our construction we also show that (constrained) Gradient Descent potentially overfits when horizon and learning rate grow w.r.t sample size. Specifically we provide a novel generalization lower bound of $Ω\left(\sqrt{ηT/m^{1.5}}\right)$ for Gradient Descent, where $η$ is the learning rate, $T$ is the horizon and $m$ is the sample size. This narrows down, exponentially, the gap between the best known upper bound of $O(ηT/m)$ and existing lower bounds from previous constructions.

</details>


### [141] [The Chicken and Egg Dilemma: Co-optimizing Data and Model Configurations for LLMs](https://arxiv.org/abs/2602.08351)
*Zhiliang Chen,Alfred Wei Lun Leong,Shao Yong Ong,Apivich Hemachandram,Gregory Kang Ruey Lau,Chuan-Sheng Foo,Zhengyuan Liu,Nancy F. Chen,Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: 提出JoBS方法，通过缩放律预测器辅助贝叶斯优化，联合高效优化LLM训练数据与模型配置，解决"鸡生蛋"困境，在相同预算下超越现有基线方法


<details>
  <summary>Details</summary>
Motivation: LLM训练中存在数据配置与模型配置相互依赖的"鸡生蛋"困境：下游任务的最优数据配比取决于模型架构，反之亦然。现有方法仅单独优化数据或模型，忽略二者交互，导致联合优化被视为计算不可行

Method: JoBS采用双阶段预算分配：部分预算用于基于少量训练步骤学习LLM性能预测器（受缩放律启发），剩余预算通过该预测器辅助贝叶斯优化（BO）进行联合配置搜索，从而分摊完整训练成本

Result: JoBS在相同优化预算下，其平均后悔值低于多保真度BO基线及独立的数据/模型优化方法，并在多样化LLM任务中验证有效性；同时推导出最小化后悔值的最优预算分配策略

Conclusion: 通过预测器实现联合配置优化的成本分摊，JoBS首次高效解决了数据-模型协同优化难题，为LLM训练提供了可扩展的自动化配置框架

Abstract: Co-optimizing data and model configurations for training LLMs presents a classic chicken-and-egg dilemma: The best training data configuration (e.g., data mixture) for a downstream task depends on the chosen model configuration (e.g., model architecture), and vice versa. However, jointly optimizing both data and model configurations is often deemed intractable, and existing methods focus on either data or model optimization without considering their interaction. We introduce JoBS, an approach that uses a scaling-law-inspired performance predictor to aid Bayesian optimization (BO) in jointly optimizing LLM training data and model configurations efficiently. JoBS allocates a portion of the optimization budget to learn an LLM performance predictor that predicts how promising a training configuration is from a small number of training steps. The remaining budget is used to perform BO entirely with the predictor, effectively amortizing the cost of running full-training runs. We study JoBS's average regret and devise the optimal budget allocation to minimize regret. JoBS outperforms existing multi-fidelity BO baselines, as well as data and model optimization approaches across diverse LLM tasks under the same optimization budget.

</details>


### [142] [Dynamic Regret via Discounted-to-Dynamic Reduction with Applications to Curved Losses and Adam Optimizer](https://arxiv.org/abs/2602.08372)
*Yan-Feng Xie,Yu-Jie Zhang,Peng Zhao,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: 本文提出一种基于折扣-动态归约的模块化框架，用于分析FTRL方法的动态遗憾界，简化了在线线性回归的证明并给出了在线逻辑回归和Adam优化器的新结果。


<details>
  <summary>Details</summary>
Motivation: FTRL对曲线损失函数和Adam等自适应优化器至关重要，但其在非平稳环境下的动态遗憾分析研究不足，存在理论空白。

Method: 基于折扣-动态归约技术，构建模块化分析框架，应用于线性回归、逻辑回归及双折扣参数(β₁,β₂)的Adam优化器。

Result: 该框架简化了在线线性回归的最优动态遗憾证明，首次给出了在线逻辑回归的动态遗憾界，并为Adam在随机、非凸、非光滑设置下提供了最优收敛率，特别得到了裁剪与无裁剪变体在双折扣参数下的新结果。

Conclusion: 此归约方法为分析非平稳学习中的动态遗憾提供了统一模块化的理论工具，推动了在线凸优化与自适应优化方法的理论发展。

Abstract: We study dynamic regret minimization in non-stationary online learning, with a primary focus on follow-the-regularized-leader (FTRL) methods. FTRL is important for curved losses and for understanding adaptive optimizers such as Adam, yet existing dynamic regret analyses are less explored for FTRL. To address this, we build on the discounted-to-dynamic reduction and present a modular way to obtain dynamic regret bounds of FTRL-related problems. Specifically, we focus on two representative curved losses: linear regression and logistic regression. Our method not only simplifies existing proofs for the optimal dynamic regret of online linear regression, but also yields new dynamic regret guarantees for online logistic regression. Beyond online convex optimization, we apply the reduction to analyze the Adam optimizers, obtaining optimal convergence rates in stochastic, non-convex, and non-smooth settings. The reduction also enables a more detailed treatment of Adam with two discount parameters $(β_1,β_2)$, leading to new results for both clipped and clip-free variants of Adam optimizers.

</details>


### [143] [OJBKQ: Objective-Joint Babai-Klein Quantization](https://arxiv.org/abs/2602.08376)
*Xinyu Wang,Ziyu Zhao,Peng Lu,Yu Gu,Xiao-Wen Chang*

Main category: cs.LG

TL;DR: This paper introduces OJBKQ, a layer-wise post-training quantization method that formulates weight quantization as a joint optimization problem over activations and weights, using Babai-Klein algorithms to find better solutions than existing heuristic/greedy methods, achieving lower perplexity at 3-4 bits with comparable computational cost.


<details>
  <summary>Details</summary>
Motivation: Existing weight-only post-training quantization methods for large language models rely on heuristic objectives and greedy rounding, which causes significant performance degradation under low-bit (3-4 bits) quantization.

Method: Proposes OJBKQ (Objective-Joint Babai-Klein Quantization with K-Best Sampling), a layer-wise PTQ method that frames quantization as a joint optimization problem, creating a NP-hard box-constrained integer least squares problem. For each weight matrix column, it uses extended Babai nearest-plane and Klein's randomized algorithms to find the minimum-residual Babai-Klein point as a sub-optimal solution.

Result: Experimental results on large language models demonstrate that OJBKQ achieves lower perplexity at 3-4 bits compared to existing PTQ approaches while maintaining similar computational overhead.

Conclusion: OJBKQ effectively reduces low-bit quantization performance degradation by employing a joint optimization formulation and advanced lattice-based algorithms, providing a superior alternative to current heuristic-based weight quantization methods.

Abstract: Post-training quantization (PTQ) is widely used to compress large language models without retraining. However, many existing weight-only methods rely on heuristic objectives and greedy rounding, thus leading to noticeable degradation under low-bit quantization. In this work, we introduce OJBKQ (Objective-Joint Babai-Klein Quantization with K-Best Sampling), a layer-wise PTQ method that formulates weight quantization as a joint optimization problem over activations and weights. This formulation results in a multiple-right-hand-side box-constrained integer least squares (BILS) problem in each layer, which is NP-hard. For each column of the weight matrix, we apply an extended Babai nearest-plane algorithm and an extended version of Klein's randomized Babai algorithm to find the minimum-residual Babai-Klein point, a sub-optimal solution to the BILS problem. Experimental results on large language models show that OJBKQ achieves lower perplexity at 3-4 bits compared to existing PTQ approaches, while maintaining comparable computational cost.

</details>


### [144] [Reinforcement Learning with Backtracking Feedback](https://arxiv.org/abs/2602.08377)
*Bilgehan Sel,Vaishakh Keshava,Phillip Wallis,Lukas Rutishauser,Ming Jin,Dingcheng Li*

Main category: cs.LG

TL;DR: This paper introduces Reinforcement Learning with Backtracking Feedback (RLBF), a framework that trains LLMs to self-correct safety violations by emitting backtrack signals. The method reduces attack success rates while preserving model utility.


<details>
  <summary>Details</summary>
Motivation: Addressing the critical need for robust safety in Large Language Models against adversarial attacks and in-distribution errors, advancing upon prior methods like BSAFE.

Method: Proposes RLBF framework with a Reinforcement Learning stage where models learn to dynamically correct generation errors through critic feedback and "backtrack by x tokens" signals. Also introduces BSAFE+, an enhanced SFT data generation strategy that injects violations into coherent, originally safe text.

Result: Comprehensive empirical evaluations demonstrate RLBF significantly reduces attack success rates across diverse benchmarks and model scales, achieving superior safety outcomes while preserving foundational model utility.

Conclusion: The RLBF framework effectively instills resilience against sophisticated adversarial strategies including middle filling, GCG attacks, and decoding parameter manipulations, while maintaining core model capabilities.

Abstract: Addressing the critical need for robust safety in Large Language Models (LLMs), particularly against adversarial attacks and in-distribution errors, we introduce Reinforcement Learning with Backtracking Feedback (RLBF). This framework advances upon prior methods, such as BSAFE, by primarily leveraging a Reinforcement Learning (RL) stage where models learn to dynamically correct their own generation errors. Through RL with critic feedback on the model's live outputs, LLMs are trained to identify and recover from their actual, emergent safety violations by emitting an efficient "backtrack by x tokens" signal, then continuing generation autoregressively. This RL process is crucial for instilling resilience against sophisticated adversarial strategies, including middle filling, Greedy Coordinate Gradient (GCG) attacks, and decoding parameter manipulations. To further support the acquisition of this backtracking capability, we also propose an enhanced Supervised Fine-Tuning (SFT) data generation strategy (BSAFE+). This method improves upon previous data creation techniques by injecting violations into coherent, originally safe text, providing more effective initial training for the backtracking mechanism. Comprehensive empirical evaluations demonstrate that RLBF significantly reduces attack success rates across diverse benchmarks and model scales, achieving superior safety outcomes while critically preserving foundational model utility.

</details>


### [145] [Modalities, a PyTorch-native Framework For Large-scale LLM Training and Research](https://arxiv.org/abs/2602.08387)
*Max Lübbering,Timm Ruland,Richard Rutmann,Felix Stollenwerk,David Fitzek,Michael Fromm,Alexander Weber,Rafet Sifa,Nicolas Flores-Herr,Joachim Köhler,Mehdi Ali*

Main category: cs.LG

TL;DR: 该论文提出Modalities，一个PyTorch原生框架，通过集成先进并行化策略和模块化设计，实现高效的大规模LLM预训练与消融实验，提升可复现性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有开源框架对大规模消融实验的工具支持有限，迫使研究人员编写自定义包装器和脚本，造成巨大计算成本下的效率低下和可复现性问题。

Method: 开发了一个端到端的PyTorch原生框架Modalities，从两个角度整合数据驱动的LLM研究：1) 集成最先进的并行化策略；2) 采用声明式、自包含配置的模块化设计。

Result: 实现了万亿token和十亿参数规模的高效预训练和系统消融实验，达到了现有框架开箱即用难以实现的可复现性和可扩展性水平。

Conclusion: Modalities通过提供集成化工具，减少了自定义脚本需求，有效解决了LLM研究中大规消实验的计算效率和可复现性挑战。

Abstract: Today's LLM (pre-) training and research workflows typically allocate a significant amount of compute to large-scale ablation studies. Despite the substantial compute costs of these ablations, existing open-source frameworks provide limited tooling for these experiments, often forcing researchers to write their own wrappers and scripts. We propose Modalities, an end-to-end PyTorch-native framework that integrates data-driven LLM research with large-scale model training from two angles. Firstly, by integrating state-of-the-art parallelization strategies, it enables both efficient pretraining and systematic ablations at trillion-token and billion-parameter scale. Secondly, Modalities adopts modular design with declarative, self-contained configuration, enabling reproducibility and extensibility levels that are difficult to achieve out-of-the-box with existing LLM training frameworks.

</details>


### [146] [Contextual Rollout Bandits for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2602.08499)
*Xiaodong Lu,Xiaohan Wang,Jiajun Chai,Guojun Yin,Wei Lin,Zhijun Chen,Yu Luo,Fuzhen Zhuang,Yikun Ban,Deqing Wang*

Main category: cs.LG

TL;DR: 提出基于上下文老虎机的神经 rollout 调度框架，自适应选择重用高质量 rollout，解决 RLVR 的噪声监督与低效问题，提升大模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有 RLVR 方法对 rollout 采用无差别、短视使用方式，导致监督噪声大、样本效率低、策略更新次优，亟需智能调度机制。

Method: 将 rollout 调度形式化为上下文老虎机问题，构建统一神经调度框架。每个 rollout 视为 arm，奖励为连续优化步间的性能增益，支持组内噪声感知选择和全局历史 rollout 自适应重用。

Result: 理论证明次线性遗憾界，且 rollout 缓冲区扩大可提升性能上界；在六个数学推理基准上，相比多种 RLVR 方法均实现性能和训练效率的持续提升。

Conclusion: 该框架通过原则性的 rollout 管理，有效解决了 RLVR 的核心缺陷，同步提升了模型推理能力和训练效率。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is an effective paradigm for improving the reasoning capabilities of large language models. However, existing RLVR methods utilize rollouts in an indiscriminate and short-horizon manner: responses of heterogeneous quality within each prompt are treated uniformly, and historical rollouts are discarded after a single use. This leads to noisy supervision, poor sample efficiency, and suboptimal policy updates. We address these issues by formulating rollout scheduling in RLVR as a contextual bandit problem and proposing a unified neural scheduling framework that adaptively selects high-value rollouts throughout training. Each rollout is treated as an arm whose reward is defined by the induced performance gain between consecutive optimization steps. The resulting scheduler supports both noise-aware intra-group selection and adaptive global reuse of historical rollouts within a single principled framework. We provide theoretical justification by deriving sublinear regret bounds and showing that enlarging the rollout buffer improves the achievable performance upper bound. Experiments on six mathematical reasoning benchmarks demonstrate consistent gains in performance and training efficiency across multiple RLVR optimization methods.

</details>


### [147] [Drop the mask! GAMM-A Taxonomy for Graph Attributes Missing Mechanisms](https://arxiv.org/abs/2602.08407)
*Richard Serrano,Baptiste Jeudy,Charlotte Laclau,Christine Largeron*

Main category: cs.LG

TL;DR: 提出GAMM框架扩展缺失数据分类法，将缺失机制与图结构和节点属性关联，实证发现现有插补方法在图感知缺失场景下表现显著下降。


<details>
  <summary>Details</summary>
Motivation: 属性图中的缺失数据面临独特挑战，传统表格数据的缺失机制分类法无法捕捉图结构信息带来的依赖关系。

Method: 提出GAMM框架，系统性地将缺失概率与节点属性和底层图结构关联，引入图特定的依赖关系来丰富传统掩码机制定义。

Result: 实验证明，现有先进插补方法在应对图感知缺失场景时表现显著劣于传统掩码机制。

Conclusion: 图结构特定的缺失机制对数据插补效果有重要影响，当前方法需改进以更好处理图数据中的复杂缺失模式。

Abstract: Exploring missing data in attributed graphs introduces unique challenges beyond those found in tabular datasets. In this work, we extend the taxonomy for missing data mechanisms to attributed graphs by proposing GAMM (Graph Attributes Missing Mechanisms), a framework that systematically links missingness probability to both node attributes and the underlying graph structure. Our taxonomy enriches the conventional definitions of masking mechanisms by introducing graph-specific dependencies. We empirically demonstrate that state-of-the-art imputation methods, while effective on traditional masks, significantly struggle when confronted with these more realistic graph-aware missingness scenarios.

</details>


### [148] [Stateless Yet Not Forgetful: Implicit Memory as a Hidden Channel in LLMs](https://arxiv.org/abs/2602.08563)
*Ahmed Salem,Andrew Paverd,Sahar Abdelnabi*

Main category: cs.LG

TL;DR: This paper reveals that LLMs can maintain persistent state across interactions through "implicit memory" by encoding information in outputs, and demonstrates this via "time bomb" backdoors that activate only after sequences of interactions.


<details>
  <summary>Details</summary>
Motivation: The common assumption that LLMs are stateless is flawed; understanding their ability to maintain state across interactions is crucial for security and safety, as it enables persistent threats that traditional defenses may miss.

Method: The authors introduce implicit memory as a mechanism where models encode information in their outputs and recover it later. They demonstrate this through temporal backdoors ("time bombs") that activate only after accumulated conditions across multiple interactions, induced via prompting or fine-tuning.

Result: They successfully show that LLMs can maintain persistent state across inference requests without explicit memory modules, enabling various security risks like covert communication, benchmark contamination, and targeted manipulation.

Conclusion: Implicit memory poses significant security challenges requiring new detection methods and stress-testing frameworks. The authors release code to promote research on anticipating and controlling these developments.

Abstract: Large language models (LLMs) are commonly treated as stateless: once an interaction ends, no information is assumed to persist unless it is explicitly stored and re-supplied. We challenge this assumption by introducing implicit memory-the ability of a model to carry state across otherwise independent interactions by encoding information in its own outputs and later recovering it when those outputs are reintroduced as input. This mechanism does not require any explicit memory module, yet it creates a persistent information channel across inference requests. As a concrete demonstration, we introduce a new class of temporal backdoors, which we call time bombs. Unlike conventional backdoors that activate on a single trigger input, time bombs activate only after a sequence of interactions satisfies hidden conditions accumulated via implicit memory. We show that such behavior can be induced today through straightforward prompting or fine-tuning. Beyond this case study, we analyze broader implications of implicit memory, including covert inter-agent communication, benchmark contamination, targeted manipulation, and training-data poisoning. Finally, we discuss detection challenges and outline directions for stress-testing and evaluation, with the goal of anticipating and controlling future developments. To promote future research, we release code and data at: https://github.com/microsoft/implicitMemory.

</details>


### [149] [Predicting Future Utility: Global Combinatorial Optimization for Task-Agnostic KV Cache Eviction](https://arxiv.org/abs/2602.08585)
*Ziyao Tang,Pengkun Jiao,Xinhang Chen,Wei Liu,Shiyong Li,Jingjing Chen*

Main category: cs.LG

TL;DR: 针对大模型推理中KV缓存的高成本问题，本文提出LU-KV框架，通过凸包松弛和边际效用贪心算法实现头部粒度的最优预算分配，在LongBench和RULER基准测试上实现80% KV缓存压缩，几乎不影响性能的同时降低延迟和GPU显存占用。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存淘汰方法依赖瞬时启发式指标，假设注意力头间分数幅度一致代表重要性，忽略了不同注意力头在预测保真度上的异质性——有些头关注token的即时贡献，有些则负责捕捉长期语义效用。这种统一的评估标准导致次优的缓存分配。

Method: 提出LU-KV框架：1) 基于边际效用最大化原则，用凸包松弛和贪心求解器优化头部粒度的缓存预算分配；2) 设计数据驱动的离线分析协议，在实际部署前获取各注意力头在不同缓存预算下的效用曲线，实现近最优精度。

Result: 在LongBench和RULER基准测试上，LU-KV仅引入极小的性能下降，即可减少80%的KV缓存大小，同时降低推理延迟和GPU显存占用。

Conclusion: 通过显式建模注意力头的异质性并以边际效用为优化目标，LU-KV实现了更智能的KV缓存管理，为长上下文场景下的大模型高效推理提供了实用解决方案。

Abstract: Given the quadratic complexity of attention, KV cache eviction is vital to accelerate model inference. Current KV cache eviction methods typically rely on instantaneous heuristic metrics, implicitly assuming that score magnitudes are consistent proxies for importance across all heads. However, this overlooks the heterogeneity in predictive fidelity across attention heads. While certain heads prioritize the instantaneous contribution of tokens, others are dedicated to capturing long-horizon utility. In this paper, we propose that optimal budget allocation should be governed by the marginal utility in preserving long-term semantic information. Based on this insight, we propose LU-KV, a novel framework that optimizes head-level budget allocation through a convex-hull relaxation and a marginal-utility-based greedy solver to achieve near-optimal precision. Furthermore, we implement a data-driven offline profiling protocol to facilitate the practical deployment of LU-KV. Extensive evaluations on LongBench and RULER benchmarks demonstrate that LU-KV achieves an 80% reduction in KV cache size with minimal performance degradation, while simultaneously reducing inference latency and GPU memory footprint.

</details>


### [150] [The Connection between Kriging and Large Neural Networks](https://arxiv.org/abs/2602.08427)
*Marius Marinescu*

Main category: cs.LG

TL;DR: This paper reveals strong connections between Kriging/Gaussian process regression and neural networks, arguing that combining spatial statistics with ML can produce more interpretable and spatially-aware models.


<details>
  <summary>Details</summary>
Motivation: As AI integrates with spatial statistics, understanding how traditional probabilistic models like Kriging relate to modern ML models is crucial for advancing interpretable and reliable AI systems.

Method: Literature review and theoretical analysis of connections between Kriging/neural networks.

Result: Kriging and neural networks, though seemingly different (probabilistic vs. black-box), share fundamental relationships that can be systematically identified.

Conclusion: Synthesizing spatial statistics and ML perspectives can enhance ML techniques by improving interpretability, reliability, and spatial awareness.

Abstract: AI has impacted many disciplines and is nowadays ubiquitous. In particular, spatial statistics is in a pivotal moment where it will increasingly intertwine with AI. In this scenario, a relevant question is what relationship spatial statistics models have with machine learning (ML) models, if any. In particular, in this paper, we explore the connections between Kriging and neural networks. At first glance, they may appear unrelated. Kriging - and its ML counterpart, Gaussian process regression - are grounded in probability theory and stochastic processes, whereas many ML models are extensively considered Black-Box models. Nevertheless, they are strongly related. We study their connections and revisit the relevant literature. The understanding of their relations and the combination of both perspectives may enhance ML techniques by making them more interpretable, reliable, and spatially aware.

</details>


### [151] [Breaking the Grid: Distance-Guided Reinforcement Learning in Large Discrete and Hybrid Action Spaces](https://arxiv.org/abs/2602.08616)
*Heiko Hoppe,Fabian Akkerman,Wouter van Heeswijk,Maximilian Schiffer*

Main category: cs.LG

TL;DR: 提出Distance-Guided Reinforcement Learning (DGRL)方法，通过结合采样动态邻域(SDN)和基于距离的更新(DBU)，有效解决了大规模离散动作空间中的维度灾难问题，在高达10^20动作的空间中实现高效强化学习，性能比现有最优方法提升高达66%，同时改善收敛速度和计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 强化学习在物流、调度和推荐系统等领域应用广泛，但标准算法在处理大规模离散动作空间时面临维度灾难问题。现有方法要么依赖受限的基于网格的结构，要么需要计算代价高昂的最近邻搜索，限制了其在高维或非规则结构领域的效果。

Method: 提出Distance-Guided Reinforcement Learning (DGRL)，结合两个核心组件：1)采样动态邻域(SDN)：利用语义嵌入空间进行随机体积探索，在局部信任区域上提供完全支持；2)基于距离的更新(DBU)：将策略优化转化为稳定的回归任务，使梯度方差与动作空间基数解耦，并保证单调策略改进。该方法无需层次依赖即可自然推广到混合连续-离散动作空间。

Result: 在规则和非规则结构环境中，DGRL相比现有最优基准方法性能提升高达66%，能处理高达10^20动作的极端大规模空间，同时提高收敛速度和降低计算复杂度。

Conclusion: DGRL为大规模离散动作空间中的强化学习提供了高效解决方案，有效克服了维度灾难问题，在性能和效率方面均显著优于现有方法，并具有向混合动作空间扩展的能力。

Abstract: Reinforcement Learning is increasingly applied to logistics, scheduling, and recommender systems, but standard algorithms struggle with the curse of dimensionality in such large discrete action spaces. Existing algorithms typically rely on restrictive grid-based structures or computationally expensive nearest-neighbor searches, limiting their effectiveness in high-dimensional or irregularly structured domains. We propose Distance-Guided Reinforcement Learning (DGRL), combining Sampled Dynamic Neighborhoods (SDN) and Distance-Based Updates (DBU) to enable efficient RL in spaces with up to 10$^\text{20}$ actions. Unlike prior methods, SDN leverages a semantic embedding space to perform stochastic volumetric exploration, provably providing full support over a local trust region. Complementing this, DBU transforms policy optimization into a stable regression task, decoupling gradient variance from action space cardinality and guaranteeing monotonic policy improvement. DGRL naturally generalizes to hybrid continuous-discrete action spaces without requiring hierarchical dependencies. We demonstrate performance improvements of up to 66% against state-of-the-art benchmarks across regularly and irregularly structured environments, while simultaneously improving convergence speed and computational complexity.

</details>


### [152] [USBD: Universal Structural Basis Distillation for Source-Free Graph Domain Adaptation](https://arxiv.org/abs/2602.08431)
*Yingxu Wang,Kunyu Zhang,Mengzhu Wang,Siyang Gao,Nan Yin*

Main category: cs.LG

TL;DR: 提出USBD框架解决图领域自适应中的结构偏移问题，通过双优化蒸馏结构基并谱感知集成，提升泛化能力和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有隐私保护图知识迁移方法依赖源训练GNN的平滑先验，在结构差异大的目标域上适应性差，伪标签不可靠。

Method: 采用双层优化从源数据集蒸馏紧凑结构基，覆盖Dirichlet能量谱以捕获多样拓扑模式；推理时基于目标图谱指纹动态激活原型组合。

Result: 在基准测试中显著优于SOTA方法，尤其在严重结构偏移场景下，且通过解耦适应成本与目标数据规模实现更高计算效率。

Conclusion: USBD通过构建结构不可知基主动覆盖潜在拓扑模式，为结构自由图领域自适应提供了通用且高效的新范式。

Abstract: SF-GDA is pivotal for privacy-preserving knowledge transfer across graph datasets. Although recent works incorporate structural information, they implicitly condition adaptation on the smoothness priors of sourcetrained GNNs, thereby limiting their generalization to structurally distinct targets. This dependency becomes a critical bottleneck under significant topological shifts, where the source model misinterprets distinct topological patterns unseen in the source domain as noise, rendering pseudo-label-based adaptation unreliable. To overcome this limitation, we propose the Universal Structural Basis Distillation, a framework that shifts the paradigm from adapting a biased model to learning a universal structural basis for SF-GDA. Instead of adapting a biased source model to a specific target, our core idea is to construct a structure-agnostic basis that proactively covers the full spectrum of potential topological patterns. Specifically, USBD employs a bi-level optimization framework to distill the source dataset into a compact structural basis. By enforcing the prototypes to span the full Dirichlet energy spectrum, the learned basis explicitly captures diverse topological motifs, ranging from low-frequency clusters to high-frequency chains, beyond those present in the source. This ensures that the learned basis creates a comprehensive structural covering capable of handling targets with disparate structures. For inference, we introduce a spectral-aware ensemble mechanism that dynamically activates the optimal prototype combination based on the spectral fingerprint of the target graph. Extensive experiments on benchmarks demonstrate that USBD significantly outperforms state-of-the-art methods, particularly in scenarios with severe structural shifts, while achieving superior computational efficiency by decoupling the adaptation cost from the target data scale.

</details>


### [153] [Sparse Models, Sparse Safety: Unsafe Routes in Mixture-of-Experts LLMs](https://arxiv.org/abs/2602.08621)
*Yukun Jiang,Hai Huang,Mingjie Li,Yage Zhang,Michael Backes,Yang Zhang*

Main category: cs.LG

TL;DR: This paper reveals that Mixture-of-Experts (MoE) language models have critical safety vulnerabilities where manipulating just a few key routers can cause the model to generate harmful content, achieving up to 98% attack success rate. The authors propose a method to discover these "unsafe routes" and suggest defense strategies.


<details>
  <summary>Details</summary>
Motivation: Prior work on MoE architectures has focused on utility and efficiency while neglecting safety risks. The sparse nature of MoE models means safety may be controlled by only a few routers, creating inherent vulnerabilities that need investigation.

Method: The authors introduce Router Safety importance score (RoSais) to identify critical routers, then propose F-SOUR framework that uses fine-grained token-layer-wise stochastic optimization to discover unsafe routes by considering token sequentiality and dynamics.

Result: Masking just 5 routers in DeepSeek-V2-Lite increases attack success rate (ASR) 4x to 0.79. The F-SOUR framework achieves average ASR of 0.90 on JailbreakBench and 0.98 on AdvBench across four MoE LLM families, demonstrating severe vulnerabilities.

Conclusion: The paper identifies inherent safety vulnerabilities in MoE LLMs where few routers control safety, proposes defense strategies including safety-aware route disabling and router training, and aims to inform future red-teaming efforts for MoE model security.

Abstract: By introducing routers to selectively activate experts in Transformer layers, the mixture-of-experts (MoE) architecture significantly reduces computational costs in large language models (LLMs) while maintaining competitive performance, especially for models with massive parameters. However, prior work has largely focused on utility and efficiency, leaving the safety risks associated with this sparse architecture underexplored. In this work, we show that the safety of MoE LLMs is as sparse as their architecture by discovering unsafe routes: routing configurations that, once activated, convert safe outputs into harmful ones. Specifically, we first introduce the Router Safety importance score (RoSais) to quantify the safety criticality of each layer's router. Manipulation of only the high-RoSais router(s) can flip the default route into an unsafe one. For instance, on JailbreakBench, masking 5 routers in DeepSeek-V2-Lite increases attack success rate (ASR) by over 4$\times$ to 0.79, highlighting an inherent risk that router manipulation may naturally occur in MoE LLMs. We further propose a Fine-grained token-layer-wise Stochastic Optimization framework to discover more concrete Unsafe Routes (F-SOUR), which explicitly considers the sequentiality and dynamics of input tokens. Across four representative MoE LLM families, F-SOUR achieves an average ASR of 0.90 and 0.98 on JailbreakBench and AdvBench, respectively. Finally, we outline defensive perspectives, including safety-aware route disabling and router training, as promising directions to safeguard MoE LLMs. We hope our work can inform future red-teaming and safeguarding of MoE LLMs. Our code is provided in https://github.com/TrustAIRLab/UnsafeMoE.

</details>


### [154] [CauScale: Neural Causal Discovery at Scale](https://arxiv.org/abs/2602.08629)
*Bo Peng,Sirui Chen,Jiaguo Tian,Yu Qiao,Chaochao Lu*

Main category: cs.LG

TL;DR: CauScale是一种用于因果发现的神经架构，可高效扩展至1000个节点的大规模图，在保持高精度的同时实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法在扩展到大图时面临时间和空间效率瓶颈，限制了其在科学AI和数据分析等领域的应用。

Method: CauScale采用压缩单元进行数据嵌入压缩以提升时间效率，使用共享注意力权重避免轴特定注意力图以节省空间，并采用双流设计（数据流提取关系证据，图流整合统计先验和结构信号）来保持准确性。

Result: CauScale可在训练时扩展至500节点图（先前方法因空间限制失败），在分布内数据上达到99.6% mAP，分布外数据达到84.4% mAP，相比先前方法实现4-13,000倍的推理加速。

Conclusion: CauScale成功实现了大规模图上的高效因果发现，在不牺牲准确性的前提下为科学AI应用开辟了新可能。

Abstract: Causal discovery is essential for advancing data-driven fields such as scientific AI and data analysis, yet existing approaches face significant time- and space-efficiency bottlenecks when scaling to large graphs. To address this challenge, we present CauScale, a neural architecture designed for efficient causal discovery that scales inference to graphs with up to 1000 nodes. CauScale improves time efficiency via a reduction unit that compresses data embeddings and improves space efficiency by adopting tied attention weights to avoid maintaining axis-specific attention maps. To keep high causal discovery accuracy, CauScale adopts a two-stream design: a data stream extracts relational evidence from high-dimensional observations, while a graph stream integrates statistical graph priors and preserves key structural signals. CauScale successfully scales to 500-node graphs during training, where prior work fails due to space limitations. Across testing data with varying graph scales and causal mechanisms, CauScale achieves 99.6% mAP on in-distribution data and 84.4% on out-of-distribution data, while delivering 4-13,000 times inference speedups over prior methods. Our project page is at https://github.com/OpenCausaLab/CauScale.

</details>


### [155] [Estimating Aleatoric Uncertainty in the Causal Treatment Effect](https://arxiv.org/abs/2602.08461)
*Liyuan Xu,Bijan Mazaheri*

Main category: cs.LG

TL;DR: This paper introduces Variance of Treatment Effect (VTE) and Conditional VTE as measures of uncertainty in individual treatment responses, proves they are identifiable from observational data even with unobserved confounders, proposes nonparametric kernel-based estimators with theoretical convergence guarantees, and validates the approach empirically on synthetic and semi-simulated datasets.


<details>
  <summary>Details</summary>
Motivation: Prior causal inference research has primarily focused on average treatment effects, neglecting the variability and uncertainty in individual treatment responses. This gap limits personalized decision-making and risk assessment, where understanding heterogeneous effects and inherent uncertainty is crucial.

Method: The authors formally define VTE and CVTE as natural measures of aleatoric uncertainty, establish their identifiability under mild assumptions, develop nonparametric kernel-based estimators, prove theoretical convergence rates, and conduct extensive experiments comparing against naive baselines.

Result: The proposed VTE/CVTE measures are provably identifiable. The kernel estimators demonstrate convergence in theory and achieve superior or comparable performance to baselines in empirical evaluations across synthetic and semi-simulated data.

Conclusion: This work fills a critical gap in causal inference by providing a rigorous, nonparametric framework for quantifying uncertainty in individual treatment effects, with both strong theoretical guarantees and practical empirical validation.

Abstract: Previous work on causal inference has primarily focused on averages and conditional averages of treatment effects, with significantly less attention on variability and uncertainty in individual treatment responses. In this paper, we introduce the variance of the treatment effect (VTE) and conditional variance of treatment effect (CVTE) as the natural measure of aleatoric uncertainty inherent in treatment responses, and we demonstrate that these quantities are identifiable from observed data under mild assumptions, even in the presence of unobserved confounders. We further propose nonparametric kernel-based estimators for VTE and CVTE, and our theoretical analysis establishes their convergence. We also test the performance of our method through extensive empirical experiments on both synthetic and semi-simulated datasets, where it demonstrates superior or comparable performance to naive baselines.

</details>


### [156] [LEFT: Learnable Fusion of Tri-view Tokens for Unsupervised Time Series Anomaly Detection](https://arxiv.org/abs/2602.08638)
*Dezheng Wang,Tong Chen,Guansong Pang,Congyan Chen,Shihua Li,Hongzhi Yin*

Main category: cs.LG

TL;DR: LEFT框架通过时间、频率和多尺度三视图的令牌融合与循环一致性约束，实现无监督时间序列异常检测，在保持最佳精度的同时实现8倍训练加速。


<details>
  <summary>Details</summary>
Motivation: 无监督时间序列异常检测的关键挑战在于：细微异常在单一视图（如时域）中难以察觉，仅在跨视图（时域、频域、多分辨率）中才显现。现有方法仅依赖特征或分数融合，缺乏分析-合成一致性约束，导致跨视图一致性不足。

Method: 提出LEFT框架：1）从频率域（周期性）、时间域（局部动态）和多尺度（不同粒度异常模式）生成三视图令牌；2）通过自适应奈奎斯特约束频谱滤波器将原始序列重采样为多分辨率后编码；3）设计从粗粒度到细粒度的重建目标；4）引入时频循环一致性约束显式正则化跨视图一致性。

Result: 在真实世界基准测试中，LEFT获得最优检测精度，同时实现5倍FLOPs降低和8倍训练速度提升。

Conclusion: LEFT通过统一的三视图令牌学习和一致性约束，有效建模跨视图异常不一致性，在精度和效率上均显著优于现有方法，为无监督TSAD提供了新范式。

Abstract: As a fundamental data mining task, unsupervised time series anomaly detection (TSAD) aims to build a model for identifying abnormal timestamps without assuming the availability of annotations. A key challenge in unsupervised TSAD is that many anomalies are too subtle to exhibit detectable deviation in any single view (e.g., time domain), and instead manifest as inconsistencies across multiple views like time, frequency, and a mixture of resolutions. However, most cross-view methods rely on feature or score fusion and do not enforce analysis-synthesis consistency, meaning the frequency branch is not required to reconstruct the time signal through an inverse transform, and vice versa. In this paper, we present Learnable Fusion of Tri-view Tokens (LEFT), a unified unsupervised TSAD framework that models anomalies as inconsistencies across complementary representations. LEFT learns feature tokens from three views of the same input time series: frequency-domain tokens that embed periodicity information, time-domain tokens that capture local dynamics, and multi-scale tokens that learns abnormal patterns at varying time series granularities. By learning a set of adaptive Nyquist-constrained spectral filters, the original time series is rescaled into multiple resolutions and then encoded, allowing these multi-scale tokens to complement the extracted frequency- and time-domain information. When generating the fused representation, we introduce a novel objective that reconstructs fine-grained targets from coarser multi-scale structure, and put forward an innovative time-frequency cycle consistency constraint to explicitly regularize cross-view agreement. Experiments on real-world benchmarks show that LEFT yields the best detection accuracy against SOTA baselines, while achieving a 5x reduction on FLOPs and 8x speed-up for training.

</details>


### [157] [Low Rank Transformer for Multivariate Time Series Anomaly Detection and Localization](https://arxiv.org/abs/2602.08467)
*Charalampos Shimillas,Kleanthis Malialis,Konstantinos Fokianos,Marios M. Polycarpou*

Main category: cs.LG

TL;DR: Proposes ALoRa-T Transformer with low-rank attention regularization and ALoRa-Loc method for theoretically grounded multivariate time series anomaly detection/localization, outperforming SOTA.


<details>
  <summary>Details</summary>
Motivation: Existing multivariate time series anomaly diagnosis methods lack theoretical insights, especially for anomaly localization which is critical but underexplored.

Method: Connects Transformer learning to statistical time series methods; introduces ALoRa-T with low-rank self-attention regularization, Attention Low-Rank score, and ALoRa-Loc for variable-level anomaly localization.

Result: Extensive experiments demonstrate significant outperformance over state-of-the-art methods in both anomaly detection and localization tasks on real-world data.

Conclusion: The proposed methodology provides both theoretical foundations and practical advancements for multivariate time series anomaly diagnosis through novel regularization and localization techniques.

Abstract: Multivariate time series (MTS) anomaly diagnosis, which encompasses both anomaly detection and localization, is critical for the safety and reliability of complex, large-scale real-world systems. The vast majority of existing anomaly diagnosis methods offer limited theoretical insights, especially for anomaly localization, which is a vital but largely unexplored area. The aim of this contribution is to study the learning process of a Transformer when applied to MTS by revealing connections to statistical time series methods. Based on these theoretical insights, we propose the Attention Low-Rank Transformer (ALoRa-T) model, which applies low-rank regularization to self-attention, and we introduce the Attention Low-Rank score, effectively capturing the temporal characteristics of anomalies. Finally, to enable anomaly localization, we propose the ALoRa-Loc method, a novel approach that associates anomalies to specific variables by quantifying interrelationships among time series. Extensive experiments and real data analysis, show that the proposed methodology significantly outperforms state-of-the-art methods in both detection and localization tasks.

</details>


### [158] [Equalized Generative Treatment: Matching f-divergences for Fairness in Generative Models](https://arxiv.org/abs/2602.08660)
*Alexandre Verine,Rafael Pinot,Florian Le Bronnec*

Main category: cs.LG

TL;DR: 该论文提出均衡生成处理(EGT)这一新的生成模型公平性定义，通过f-散度度量确保不同敏感组间生成质量可比，并验证最小-最大微调方法能有效实现该目标且保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型公平性指标存在缺陷，仅关注各敏感组的生成概率平衡而忽略质量差异，可能导致建模质量严重不均。

Method: 引入均衡生成处理(EGT)定义，要求所有敏感组具有相当的生成质量，并采用参考f-散度进行质量度量，进而提出最小-最大微调方法来平衡各组f-散度以满足EGT。

Result: 图像与文本生成任务的实验表明，最小-最大方法相比现有文献方法能持续获得更公平的生成结果，同时保持具有竞争力的整体性能。

Conclusion: EGT解决了传统公平性概念的关键局限；实现公平性必然将整体模型质量与最难建模的组耦合，这表最小-最大优化是简单而高效的解决方案。

Abstract: Fairness is a crucial concern for generative models, which not only reflect but can also amplify societal and cultural biases. Existing fairness notions for generative models are largely adapted from classification and focus on balancing the probability of generating samples from each sensitive group. We show that such criteria are brittle, as they can be met even when different sensitive groups are modeled with widely varying quality. To address this limitation, we introduce a new fairness definition for generative models, termed as equalized generative treatment (EGT), which requires comparable generation quality across all sensitive groups, with quality measured via a reference f-divergence. We further analyze the trade-offs induced by EGT, demonstrating that enforcing fairness constraints necessarily couples the overall model quality to that of the most challenging group to approximate. This indicates that a simple yet efficient min-max fine-tuning method should be able to balance f-divergences across sensitive groups to satisfy EGT. We validate this theoretical insight through a set of experiments on both image and text generation tasks. We demonstrate that min-max methods consistently achieve fairer outcomes compared to other approaches from the literature, while maintaining competitive overall performance for both tasks.

</details>


### [159] [Learning Credal Ensembles via Distributionally Robust Optimization](https://arxiv.org/abs/2602.08470)
*Kaizheng Wang,Ghifari Adam Faza,Fabio Cuzzolin,Siu Lun Chau,David Moens,Hans Hallez*

Main category: cs.LG

TL;DR: This paper proposes CreDRO, a novel credal prediction method that redefines epistemic uncertainty (EU) as model disagreement under distribution shifts rather than just optimization randomness. By using distributionally robust optimization, CreDRO captures more meaningful EU and outperforms existing methods in OOD detection and medical selective classification.


<details>
  <summary>Details</summary>
Motivation: Current credal predictors mainly define epistemic uncertainty (EU) as disagreement from random training initializations, which only reflects sensitivity to optimization randomness rather than deeper uncertainty sources from potential distribution shifts between training and test data.

Method: The authors redefine EU as disagreement among models trained with varying relaxations of the i.i.d. assumption between training and test data. Based on this, they propose CreDRO, which learns an ensemble of plausible models through distributionally robust optimization to capture EU from both training randomness and meaningful distribution shifts.

Result: Empirical results demonstrate that CreDRO consistently outperforms existing credal methods on out-of-distribution detection across multiple benchmarks and selective classification in medical applications.

Conclusion: CreDRO provides a more principled approach to quantify predictive epistemic uncertainty by capturing disagreement due to potential distribution shifts, leading to improved model robustness in safety-critical applications.

Abstract: Credal predictors are models that are aware of epistemic uncertainty and produce a convex set of probabilistic predictions. They offer a principled way to quantify predictive epistemic uncertainty (EU) and have been shown to improve model robustness in various settings. However, most state-of-the-art methods mainly define EU as disagreement caused by random training initializations, which mostly reflects sensitivity to optimization randomness rather than uncertainty from deeper sources. To address this, we define EU as disagreement among models trained with varying relaxations of the i.i.d. assumption between training and test data. Based on this idea, we propose CreDRO, which learns an ensemble of plausible models through distributionally robust optimization. As a result, CreDRO captures EU not only from training randomness but also from meaningful disagreement due to potential distribution shifts between training and test data. Empirical results show that CreDRO consistently outperforms existing credal methods on tasks such as out-of-distribution detection across multiple benchmarks and selective classification in medical applications.

</details>


### [160] [LLaDA2.1: Speeding Up Text Diffusion via Token Editing](https://arxiv.org/abs/2602.08676)
*Tiwei Bie,Maosong Cao,Xiang Cao,Bingsen Chen,Fuyuan Chen,Kun Chen,Lun Du,Daozhuo Feng,Haibo Feng,Mingliang Gong,Zhuocheng Gong,Yanmei Gu,Jian Guan,Kaiyuan Guan,Hongliang He,Zenan Huang,Juyong Jiang,Zhonghui Jiang,Zhenzhong Lan,Chengxi Li,Jianguo Li,Zehuan Li,Huabin Liu,Lin Liu,Guoshan Lu,Yuan Lu,Yuxin Ma,Xingyu Mou,Zhenxuan Pan,Kaida Qiu,Yuji Ren,Jianfeng Tan,Yiding Tian,Zian Wang,Lanning Wei,Tao Wu,Yipeng Xing,Wentao Ye,Liangyu Zha,Tianze Zhang,Xiaolu Zhang,Junbo Zhao,Da Zheng,Hao Zhong,Wanli Zhong,Jun Zhou,Junlin Zhou,Liwang Zhu,Muzhi Zhu,Yihong Zhuang*

Main category: cs.LG

TL;DR: LLaDA2.1 introduces a joint Token-to-Token (T2T) and Mask-to-Token (M2T) decoding scheme with two operational modes (Speedy and Quality) to overcome the speed-quality trade-off in diffusion LLMs, supplemented by a specialized Reinforcement Learning framework for improved reasoning and instruction-following.


<details>
  <summary>Details</summary>
Motivation: To resolve the persistent trade-off between decoding speed and generation quality in large-scale block-diffusion language models (dLLMs), which limited their practical deployment despite their scaling potential and inherent parallelization advantages.

Method: 1) A joint, configurable threshold-decoding scheme integrating T2T editing with conventional M2T. 2) Introduction of two distinct personas: Speedy Mode (low M2T threshold) and Quality Mode (conservative threshold). 3) The first large-scale RL framework tailored for dLLMs, using stable gradient estimation techniques.

Result: Released LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Achieved strong benchmark performance and extremely high decoding speeds: 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench for the 100B model, while improving reasoning and instruction-following capabilities.

Conclusion: The innovations successfully transcend the traditional speed-quality trade-off in diffusion LLMs, demonstrating that high-quality generation and lightning-fast decoding can be achieved simultaneously, even at the 100B parameter scale.

Abstract: While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce a joint, configurable threshold-decoding scheme. This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation. This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed. Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench.

</details>


### [161] [Time-Delayed Transformers for Data-Driven Modeling of Low-Dimensional Dynamics](https://arxiv.org/abs/2602.08478)
*Albert Alcalde,Markus Widhalm,Emre Yılmaz*

Main category: cs.LG

TL;DR: Proposes a minimal transformer (TD-TF) that bridges linear operator methods and deep learning for spatio-temporal dynamics, matching linear models on simple systems while outperforming them on nonlinear/chaotic systems.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between interpretable linear operator-based methods and expressive deep sequence models for data-driven modeling of unsteady spatio-temporal dynamics.

Method: Proposes TD-TF, a minimal single-layer, single-head transformer architecture that can be interpreted as a nonlinear generalization of time-delayed dynamic mode decomposition (TD-DMD), featuring linear computational complexity and small parameter count.

Result: TD-TF matches linear baselines on near-linear systems but significantly outperforms them on nonlinear and chaotic systems, accurately capturing long-term dynamics across synthetic signals, unsteady aerodynamics, Lorenz '63, and reaction-diffusion models.

Conclusion: TD-TF successfully combines the interpretability and efficiency of linear models with enhanced expressive power for complex dynamics, offering a minimal yet effective architecture for spatio-temporal modeling.

Abstract: We propose the time-delayed transformer (TD-TF), a simplified transformer architecture for data-driven modeling of unsteady spatio-temporal dynamics. TD-TF bridges linear operator-based methods and deep sequence models by showing that a single-layer, single-head transformer can be interpreted as a nonlinear generalization of time-delayed dynamic mode decomposition (TD-DMD). The architecture is deliberately minimal, consisting of one self-attention layer with a single query per prediction and one feedforward layer, resulting in linear computational complexity in sequence length and a small parameter count. Numerical experiments demonstrate that TD-TF matches the performance of strong linear baselines on near-linear systems, while significantly outperforming them in nonlinear and chaotic regimes, where it accurately captures long-term dynamics. Validation studies on synthetic signals, unsteady aerodynamics, the Lorenz '63 system, and a reaction-diffusion model show that TD-TF preserves the interpretability and efficiency of linear models while providing substantially enhanced expressive power for complex dynamics.

</details>


### [162] [CompilerKV: Risk-Adaptive KV Compression via Offline Experience Compilation](https://arxiv.org/abs/2602.08686)
*Ning Yang,Chengzhi Wang,Yibo Liu,Baoliang Tian,Haijun Zhang*

Main category: cs.LG

TL;DR: CompilerKV是一个风险自适应且考虑注意力头异质性的KV缓存压缩框架，通过离线学习生成可复用的决策表，在512个token的严格内存预算下恢复97.7%的完整性能，超越最强竞争对手5.2个点。


<details>
  <summary>Details</summary>
Motivation: 长上下文场景中大语言模型的KV缓存内存呈线性增长，现有压缩方法依赖静态阈值或粗略预算分配，忽略了提示相关的压缩风险变化和注意力头的功能异质性，导致token选择不稳定和尾部性能失效。

Method: 提出CompilerKV框架，通过离线经验编译成可复用的决策表。包含两个核心组件：1）通过离线上下文赌博机学习"头异质性表"，为不同注意力头分配可靠性权重；2）风险自适应阈值门控机制，联合建模注意力熵和局部困惑度，将提示级风险转化为可部署的保留阈值。

Result: 在LongBench基准测试中，512token预算下，CompilerKV恢复了完整KV缓存97.7%的性能，相比最强基线方法最高提升5.2个点，显著优于现有最优方法。

Conclusion: CompilerKV通过离线编译的决策表有效处理了头异质性和提示特定风险，为KV缓存压缩提供了自适应解决方案，在严格内存约束下实现了接近完整的性能。

Abstract: Large Language Models (LLMs) in long-context scenarios are severely constrained by the linear growth of Key-Value (KV) cache memory. Existing KV compression methods rely either on static thresholds and attention-only heuristics or on coarse memory budget allocation. Under tight memory budgets, these methods overlook two key factors: prompt-dependent variation in compression risk and functional heterogeneity across attention heads, which destabilize token selection and lead to tail failures. To address these challenges, we propose CompilerKV, a risk-adaptive and head-aware compression framework that compiles offline experience into reusable decision tables for prefill-only deployment. CompilerKV integrates two key synergistic components: (i) a Head Heterogeneity Table, learned via offline contextual bandits, which assigns head-specific reliability weights to govern functional differences across attention heads explicitly; and (ii) a Risk-Adaptive Threshold Gating mechanism that jointly models attention entropy and local perplexity, transforming prompt-level risk into deployable retention thresholds. Experiments on LongBench show CompilerKV dominates SOTA methods under a 512-token budget, recovering 97.7\% of FullKV performance while achieving up to +5.2 points gain over the strongest competitor.

</details>


### [163] [Beyond Correctness: Learning Robust Reasoning via Transfer](https://arxiv.org/abs/2602.08489)
*Hyunseok Lee,Soheil Abbasloo,Jihoon Tack,Jinwoo Shin*

Main category: cs.LG

TL;DR: RLTR addresses RLVR's reasoning robustness gap by using transfer rewards that test if one model's partial reasoning can guide another, achieving +3.6% accuracy gain on MATH500 and 2.5x training efficiency.


<details>
  <summary>Details</summary>
Motivation: RLVR strengthens LLM reasoning but only verifies final answer correctness, leaving a critical gap: it doesn't ensure the robustness and generalizability of the reasoning process itself.

Method: Introduces Reinforcement Learning with Transferable Reward (RLTR) that operationalizes robustness via transfer reward - testing whether a partial reasoning prefix from one model can guide a separate model to the correct answer.

Result: On MATH500, RLTR achieves a +3.6%p gain in Maj@64 compared to RLVR and matches RLVR's average accuracy with roughly 2.5x fewer training steps, improving both sampling consistency and final answer accuracy.

Conclusion: RLTR encourages LLMs to produce stable, interpretable, and genuinely generalizable reasoning, providing more reliable reasoning with significantly better sample efficiency.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently strengthened LLM reasoning, but its focus on final answer correctness leaves a critical gap: it does not ensure the robustness of the reasoning process itself. We adopt a simple philosophical view, robust reasoning should remain useful beyond the mind that produced it, and treat reasoning as a form of meaning transfer that must survive truncation, reinterpretation, and continuation. Building on this principle, we introduce Reinforcement Learning with Transferable Reward (RLTR), which operationalizes robustness via transfer reward that tests whether a partial reasoning prefix from one model can guide a separate model to the correct answer. This encourages LLMs to produce reasoning that is stable, interpretable, and genuinely generalizable. Our approach improves sampling consistency while improving final answer accuracy, and it reaches comparable performance in substantially fewer training steps. For example, on MATH500, RLTR achieves a +3.6%p gain in Maj@64 compared to RLVR and matches RLVR's average accuracy with roughly 2.5x fewer training steps, providing both more reliable reasoning and significantly more sample efficient.

</details>


### [164] [QUOKA: Query-Oriented KV Selection For Efficient LLM Prefill](https://arxiv.org/abs/2602.08722)
*Dalton Jones,Junyoung Park,Matthew Morse,Mingu Lee,Chris Lott,Harper Langston*

Main category: cs.LG

TL;DR: QUOKA是一种无需训练且硬件无关的稀疏注意力算法，通过优先处理与平均查询余弦相似度低的查询，并选择与之最对齐的键，实现了3-7倍推理加速，同时保持接近基线模型的准确率。


<details>
  <summary>Details</summary>
Motivation: 在分块预填充条件下，transformer推理的注意力计算开销巨大；研究发现低余弦相似度查询需要更多键且对最终注意力logits贡献更大，这为选择性计算提供了优化机会。

Method: 两阶段查询导向的KV选择：首先保留少量代表性查询，然后根据这些查询选择最相关的键进行注意力计算，其余KV对被丢弃。

Result: 在Needle-In-A-Haystack、LongBench、RULER和Math500测试中，实现首token时间减少3倍、Nvidia GPU注意力计算5倍加速、Intel Xeon CPU最高7倍加速，每轮注意力评估减少88%的键值对使用，准确率接近完整注意力。

Conclusion: QUOKA通过智能的查询-键对齐策略，在无需训练和硬件适配的情况下，显著加速了transformer推理，为长上下文应用提供了高效解决方案。

Abstract: We present QUOKA: Query-oriented KV selection for efficient attention, a training-free and hardware agnostic sparse attention algorithm for accelerating transformer inference under chunked prefill. While many queries focus on a smaller group of keys in the attention operator, we observe that queries with low cosine similarity with respect to the mean query interact more strongly with more keys and have the greatest contribution to final attention logits. By prioritizing these low cosine similarity queries, the behavior of full attention during the prefill stage can be closely approximated. QUOKA leverages this observation, accelerating attention by (1) first retaining a small set of representative queries and (2) then subselectin the keys most aligned with those queries. Through experiments on Needle-In-A-Haystack, LongBench, RULER, and Math500, we show that, while realizing a 3x reduction in time-to-first-token, 5x speedup in attention on Nvidia GPUs and up to nearly a 7x speedup on Intel Xeon CPUs, QUOKA achieves near-baseline accuracy, utilizing 88% fewer key-value pairs per attention evaluation.

</details>


### [165] [Is Meta-Path Attention an Explanation? Evidence of Alignment and Decoupling in Heterogeneous GNNs](https://arxiv.org/abs/2602.08500)
*Maiqi Jiang,Noman Ali,Yiran Ding,Yanfu Zhang*

Main category: cs.LG

TL;DR: A study on when meta-path attention in heterogeneous GNNs reliably reflects semantic importance, introducing MetaXplain protocol that reveals mixed alignment regimes and potential explanation-as-denoising effects.


<details>
  <summary>Details</summary>
Motivation: Meta-path attention in heterogeneous GNNs is widely used to explain "which semantics matter," but its empirical reliability is unknown. Existing explainers for homogeneous graphs can't properly handle heterogeneous semantics when naively adapted.

Method: MetaXplain: a meta-path-aware explanation protocol that applies existing explainers via (1) view-factorized explanations, (2) schema-valid channel perturbations, and (3) fusion-aware attribution. Benchmarked on ACM/DBLP/IMDB using HAN/HAN-GCN with MP-AEA metric measuring correlation between attention weights and explanation-derived contributions.

Result: Meta-path-aware explanations outperform random baselines. MP-AEA reveals both high-alignment and statistically significant decoupling regimes depending on dataset and backbone. Retraining on explanation-induced subgraphs preserves or improves performance in noisy regimes.

Conclusion: Meta-path attention doesn't always reflect true importance—alignment is context-dependent. MetaXplain enables controlled analysis of this phenomenon, and explanations may serve as a denoising mechanism for model training.

Abstract: Meta-path-based heterogeneous graph neural networks aggregate over meta-path-induced views, and their semantic-level attention over meta-path channels is widely used as a narrative for ``which semantics matter.'' We study this assumption empirically by asking: when does meta-path attention reflect meta-path importance, and when can it decouple? A key challenge is that most post-hoc GNN explainers are designed for homogeneous graphs, and naive adaptations to heterogeneous neighborhoods can mix semantics and confound perturbations. To enable a controlled empirical analysis, we introduce MetaXplain, a meta-path-aware post-hoc explanation protocol that applies existing explainers in the native meta-path view domain via (i) view-factorized explanations, (ii) schema-valid channel-wise perturbations, and (iii) fusion-aware attribution, without modifying the underlying predictor. We benchmark representative gradient-, perturbation-, and Shapley-style explainers on ACM, DBLP, and IMDB with HAN and HAN-GCN, comparing against xPath and type-matched random baselines under standard faithfulness metrics. To quantify attention reliability, we propose Meta-Path Attention--Explanation Alignment (MP-AEA), which measures rank correlation between learned attention weights and explanation-derived meta-path contribution scores across random runs. Our results show that meta-path-aware explanations typically outperform random controls, while MP-AEA reveals both high-alignment and statistically significant decoupling regimes depending on the dataset and backbone; moreover, retraining on explanation-induced subgraphs often preserves, and in some noisy regimes improves, predictive performance, suggesting an explanation-as-denoising effect.

</details>


### [166] [FreqLens: Interpretable Frequency Attribution for Time Series Forecasting](https://arxiv.org/abs/2602.08768)
*Chi-Sheng Chen,Xinyu Zhang,En-Jui Kuo,Guan-Ying Chen,Qiuzhe Xie,Fan Zhang*

Main category: cs.LG

TL;DR: FreqLens is an interpretable time series forecasting framework that learns frequency components and provides theoretically-grounded attributions, achieving competitive performance while automatically discovering meaningful periodic patterns like daily cycles.


<details>
  <summary>Details</summary>
Motivation: Time series forecasting models often lack interpretability, limiting their adoption in domains requiring explainable predictions.

Method: Proposes FreqLens with two innovations: (1) Learnable frequency discovery using sigmoid-mapped frequency bases learned with diversity regularization to automatically find dominant periodic patterns without domain knowledge; (2) Axiomatic frequency attribution that satisfies Completeness, Faithfulness, Null-Frequency, and Symmetry axioms with per-frequency attributions equivalent to Shapley values.

Result: On Traffic and Weather datasets, FreqLens achieves competitive or superior performance while discovering physically meaningful frequencies: 5 independent runs discovered 24-hour daily cycle (24.6 ± 0.1h, 2.5% error) and 12-hour half-daily cycle (11.8 ± 0.1h, 1.6% error) on Traffic, and weekly cycles (10× longer than input window) on Weather.

Conclusion: Demonstrates genuine frequency-level knowledge discovery with formal theoretical guarantees on attribution quality.

Abstract: Time series forecasting models often lack interpretability, limiting their adoption in domains requiring explainable predictions. We propose \textsc{FreqLens}, an interpretable forecasting framework that discovers and attributes predictions to learnable frequency components. \textsc{FreqLens} introduces two key innovations: (1) \emph{learnable frequency discovery} -- frequency bases are parameterized via sigmoid mapping and learned from data with diversity regularization, enabling automatic discovery of dominant periodic patterns without domain knowledge; and (2) \emph{axiomatic frequency attribution} -- a theoretically grounded framework that provably satisfies Completeness, Faithfulness, Null-Frequency, and Symmetry axioms, with per-frequency attributions equivalent to Shapley values. On Traffic and Weather datasets, \textsc{FreqLens} achieves competitive or superior performance while discovering physically meaningful frequencies: all 5 independent runs discover the 24-hour daily cycle ($24.6 \pm 0.1$h, 2.5\% error) and 12-hour half-daily cycle ($11.8 \pm 0.1$h, 1.6\% error) on Traffic, and weekly cycles ($10\times$ longer than the input window) on Weather. These results demonstrate genuine frequency-level knowledge discovery with formal theoretical guarantees on attribution quality.

</details>


### [167] [Bridging Academia and Industry: A Comprehensive Benchmark for Attributed Graph Clustering](https://arxiv.org/abs/2602.08519)
*Yunhui Liu,Pengyu Qiu,Yu Xing,Yongchao Liu,Peng Du,Chuntao Hong,Jiajun Zheng,Tao Zheng,Tieke He*

Main category: cs.LG

TL;DR: 针对属性图聚类研究与工业部署间的巨大鸿沟，作者提出PyAGC基准测试库，提供模块化框架、内存高效的小批量实现、12个多尺度数据集及综合评估协议，已在蚂蚁集团工业流程中验证。


<details>
  <summary>Details</summary>
Motivation: 当前AGC研究使用小规模、高同质性的引文数据集，采用不可扩展的全批量训练，依赖不适用于标签稀缺场景的有监督指标，导致学术与工业应用严重脱节。

Method: 构建PyAGC基准，设计模块化Encode-Cluster-Optimize框架，提供多种SOTA算法的内存高效小批量实现，整合12个涵盖2.7K-1.11亿节点的多样数据集，并倡导结合无监督结构指标与效率分析的综合评估协议。

Result: 开发了一个生产就绪、可扩展、可复现的基准测试平台，并在蚂蚁集团高风险工业工作流中完成实战检验，代码已开源。

Conclusion: PyAGC为AGC研究提供了向真实世界部署推进的坚实基础，通过弥合学术与工业间的差距，推动领域在现实场景中实现更大价值。

Abstract: Attributed Graph Clustering (AGC) is a fundamental unsupervised task that integrates structural topology and node attributes to uncover latent patterns in graph-structured data. Despite its significance in industrial applications such as fraud detection and user segmentation, a significant chasm persists between academic research and real-world deployment. Current evaluation protocols suffer from the small-scale, high-homophily citation datasets, non-scalable full-batch training paradigms, and a reliance on supervised metrics that fail to reflect performance in label-scarce environments. To bridge these gaps, we present PyAGC, a comprehensive, production-ready benchmark and library designed to stress-test AGC methods across diverse scales and structural properties. We unify existing methodologies into a modular Encode-Cluster-Optimize framework and, for the first time, provide memory-efficient, mini-batch implementations for a wide array of state-of-the-art AGC algorithms. Our benchmark curates 12 diverse datasets, ranging from 2.7K to 111M nodes, specifically incorporating industrial graphs with complex tabular features and low homophily. Furthermore, we advocate for a holistic evaluation protocol that mandates unsupervised structural metrics and efficiency profiling alongside traditional supervised metrics. Battle-tested in high-stakes industrial workflows at Ant Group, this benchmark offers the community a robust, reproducible, and scalable platform to advance AGC research towards realistic deployment. The code and resources are publicly available via GitHub (https://github.com/Cloudy1225/PyAGC), PyPI (https://pypi.org/project/pyagc), and Documentation (https://pyagc.readthedocs.io).

</details>


### [168] [Default Machine Learning Hyperparameters Do Not Provide Informative Initialization for Bayesian Optimization](https://arxiv.org/abs/2602.08774)
*Nicolás Villagrán Prieto,Eduardo C. Garrido-Merchán*

Main category: cs.LG

TL;DR: 一项全面研究发现，使用机器学习库默认超参数初始化贝叶斯优化相比随机初始化无统计显著优势，建议将超参数调优作为模型开发的必要环节而非依赖库默认值。


<details>
  <summary>Details</summary>
Motivation: 探究常用ML库（如scikit-learn）中隐含专家知识的默认超参数是否能作为贝叶斯优化的信息性起点来加速收敛，这一直观假设此前缺乏系统性验证。

Method: 采用截断高斯分布（以库默认值为中心）初始化BO，对比均匀随机基线；在3个BO后端、3种模型家族、5个基准数据集上进行广泛实验，通过收敛速度和预测质量评估性能，并使用单侧二项检验确定统计显著性。

Result: 默认初始化在所有实验条件下均未显示统计显著优势（p值0.141-0.908）；虽然更紧的默认值分布可改善早期评估，但此短暂优势随优化进程消失，最终性能无差异。

Conclusion: 默认超参数未编码对优化有用的方向信息；建议从业者将超参数调优视为模型开发的必要组成部分，优先采用原则性、数据驱动搜索策略，而非启发式依赖库默认值。

Abstract: Bayesian Optimization (BO) is a standard tool for hyperparameter tuning thanks to its sample efficiency on expensive black-box functions. While most BO pipelines begin with uniform random initialization, default hyperparameter values shipped with popular ML libraries such as scikit-learn encode implicit expert knowledge and could serve as informative starting points that accelerate convergence. This hypothesis, despite its intuitive appeal, has remained largely unexamined. We formalize the idea by initializing BO with points drawn from truncated Gaussian distributions centered at library defaults and compare the resulting trajectories against a uniform-random baseline. We conduct an extensive empirical evaluation spanning three BO back-ends (BoTorch, Optuna, Scikit-Optimize), three model families (Random Forests, Support Vector Machines, Multilayer Perceptrons), and five benchmark datasets covering classification and regression tasks. Performance is assessed through convergence speed and final predictive quality, and statistical significance is determined via one-sided binomial tests. Across all conditions, default-informed initialization yields no statistically significant advantage over purely random sampling, with p-values ranging from 0.141 to 0.908. A sensitivity analysis on the prior variance confirms that, while tighter concentration around the defaults improves early evaluations, this transient benefit vanishes as optimization progresses, leaving final performance unchanged. Our results provide no evidence that default hyperparameters encode useful directional information for optimization. We therefore recommend that practitioners treat hyperparameter tuning as an integral part of model development and favor principled, data-driven search strategies over heuristic reliance on library defaults.

</details>


### [169] [Causal Schrödinger Bridges: Constrained Optimal Transport on Structural Manifolds](https://arxiv.org/abs/2602.08535)
*Rui Wu,Li YongJun*

Main category: cs.LG

TL;DR: 该论文提出用扩散过程（SDE）替代确定性ODE来解决因果干预下的生成建模问题，通过因果薛定谔桥（CSB）框架实现更稳健的反事实推理。


<details>
  <summary>Details</summary>
Motivation: 传统的确定性ODE流在因果干预下表现脆弱，特别是在低密度区域传输概率质量时会出现数值不稳定和伪相关。

Method: 引入因果薛定谔桥（CSB）框架，将反事实推理重新表述为熵最优传输，利用扩散过程在结构约束下稳健地处理支持不匹配。

Result: 证明了结构分解定理，表明高维桥可以分解为局部稳健转移；在Morpho-MNIST上的实验显示CSB在结构一致性上显著优于确定性基线，尤其在强分布外干预情况下。

Conclusion: 扩散过程为基础的CSB框架为因果反事实推理提供了更稳健的解决方案，在分布外干预场景下具有显著优势。

Abstract: Generative modeling typically seeks the path of least action via deterministic flows (ODE). While effective for in-distribution tasks, we argue that these deterministic paths become brittle under causal interventions, which often require transporting probability mass across low-density regions ("off-manifold") where the vector field is ill-defined. This leads to numerical instability and spurious correlations. In this work, we introduce the Causal Schrödinger Bridge (CSB), a framework that reformulates counterfactual inference as Entropic Optimal Transport. Unlike deterministic approaches that require strict invertibility, CSB leverages diffusion processes (SDEs) to robustly "tunnel" through support mismatches while strictly enforcing structural admissibility constraints. We prove the Structural Decomposition Theorem, showing that the global high-dimensional bridge factorizes into local, robust transitions. Empirical validation on high-dimensional interventions (Morpho-MNIST) demonstrates that CSB significantly outperforms deterministic baselines in structural consistency, particularly in regimes of strong, out-of-distribution treatments.

</details>


### [170] [$\texttt{lrnnx}$: A library for Linear RNNs](https://arxiv.org/abs/2602.08810)
*Karan Bania,Soham Kalburgi,Manit Tanwar,Dhruthi,Aditya Nagarsekar,Harshvardhan Mestha,Naman Chibber,Raj Deshmukh,Anish Sathyanarayanan,Aarush Rathore,Pratham Chheda*

Main category: cs.LG

TL;DR: The paper introduces lrnnx, a unified software library that implements multiple modern Linear Recurrent Neural Network architectures under a common interface to address fragmentation and accessibility issues.


<details>
  <summary>Details</summary>
Motivation: Existing LRNN implementations are fragmented across frameworks, rely on framework-specific optimizations, lack public code, and require substantial effort to use, compare, or extend.

Method: Proposes lrnnx, a unified library implementing several modern LRNN architectures with multiple levels of control (core components or high-level abstractions).

Result: A library that improves accessibility, reproducibility, and extensibility of LRNN research and applications, with code available under MIT license.

Conclusion: lrnnx provides a much-needed unified tool that facilitates LRNN research and application development by lowering implementation barriers and standardizing access to multiple architectures.

Abstract: Linear recurrent neural networks (LRNNs) provide a structured approach to sequence modeling that bridges classical linear dynamical systems and modern deep learning, offering both expressive power and theoretical guarantees on stability and trainability. In recent years, multiple LRNN-based architectures have been proposed, each introducing distinct parameterizations, discretization schemes, and implementation constraints. However, existing implementations are fragmented across different software frameworks, often rely on framework-specific optimizations, and in some cases require custom CUDA kernels or lack publicly available code altogether. As a result, using, comparing, or extending LRNNs requires substantial implementation effort. To address this, we introduce $\texttt{lrnnx}$, a unified software library that implements several modern LRNN architectures under a common interface. The library exposes multiple levels of control, allowing users to work directly with core components or higher-level model abstractions. $\texttt{lrnnx}$ aims to improve accessibility, reproducibility, and extensibility of LRNN research and applications. We make our code available under a permissive MIT license.

</details>


### [171] [Rho-Perfect: Correlation Ceiling For Subjective Evaluation Datasets](https://arxiv.org/abs/2602.08552)
*Fredrik Cumlin*

Main category: cs.LG

TL;DR: 提出 $ρ$-Perfect 作为主观评分数据集中模型最高可达相关性的量化估计方法，通过异方差噪声模型推导，可用于区分模型性能瓶颈与数据质量问题。


<details>
  <summary>Details</summary>
Motivation: 主观评分存在固有噪声，限制了模型与人类评分的相关性，但这一可靠性上限问题在研究中很少被量化分析。

Method: 定义 $ρ$-Perfect 为完美预测器与人类评分间的相关性，基于异方差噪声场景推导其估计值，并证明 $ρ$-Perfect 的平方等于重测相关性用于验证。

Result: 在语音质量数据集上验证了估计方法，成功展示了该指标区分模型固有局限性与数据质量问题的能力。

Conclusion: $ρ$-Perfect 提供了主观评分任务中模型性能的理论上限，为评估模型潜力和改进数据质量提供了实用工具。

Abstract: Subjective ratings contain inherent noise that limits the model-human correlation, but this reliability issue is rarely quantified. In this paper, we present $ρ$-Perfect, a practical estimation of the highest achievable correlation of a model on subjectively rated datasets. We define $ρ$-Perfect to be the correlation between a perfect predictor and human ratings, and derive an estimate of the value based on heteroscedastic noise scenarios, a common occurrence in subjectively rated datasets. We show that $ρ$-Perfect squared estimates test-retest correlation and use this to validate the estimate. We demonstrate the use of $ρ$-Perfect on a speech quality dataset and show how the measure can distinguish between model limitations and data quality issues.

</details>


### [172] [Permissive-Washing in the Open AI Supply Chain: A Large-Scale Audit of License Integrity](https://arxiv.org/abs/2602.08816)
*James Jewitt,Gopi Krishnan Rajbahadur,Hao Li,Bram Adams,Ahmed E. Hassan*

Main category: cs.LG

TL;DR: 研究发现开源AI中存在普遍的"宽松许可洗白"现象：96.5%的数据集和95.8%的模型缺失必需的许可文本，仅2.3%数据集和3.2%模型满足完整法律要求，下游归属信息几乎无法有效传递，导致用户面临版权诉讼风险。


<details>
  <summary>Details</summary>
Motivation: 揭示开源AI生态中宽松许可标签与实际法律文档脱节的危险现象（"宽松许可洗白"），警示从业者仅凭元数据标签使用AI资产可能构成侵权，亟需验证许可要求的实际合规性。

Method: 对Hugging Face和GitHub上124,278条数据集→模型→应用供应链进行实证审计，覆盖3,338个数据集、6,664个模型和28,516个应用，系统检查许可文本、版权声明和上游归属信息的完整性。

Result: 96.5%数据集和95.8%模型缺失许可文本；仅2.3%数据集和3.2%模型同时满足许可文本与版权要求；仅27.59%模型保留合规数据集声明，5.75%应用保留合规模型声明（仅6.38%保留任何上游声明）。

Conclusion: 从业者不能假设许可标签自动授予宣称权利——法律文件（非元数据）才是法律依据。必须强制要求完整许可文档与归属传递，建议释放审计数据集推动行业合规。

Abstract: Permissive licenses like MIT, Apache-2.0, and BSD-3-Clause dominate open-source AI, signaling that artifacts like models, datasets, and code can be freely used, modified, and redistributed. However, these licenses carry mandatory requirements: include the full license text, provide a copyright notice, and preserve upstream attribution, that remain unverified at scale. Failure to meet these conditions can place reuse outside the scope of the license, effectively leaving AI artifacts under default copyright for those uses and exposing downstream users to litigation. We call this phenomenon ``permissive washing'': labeling AI artifacts as free to use, while omitting the legal documentation required to make that label actionable. To assess how widespread permissive washing is in the AI supply chain, we empirically audit 124,278 dataset $\rightarrow$ model $\rightarrow$ application supply chains, spanning 3,338 datasets, 6,664 models, and 28,516 applications across Hugging Face and GitHub. We find that an astonishing 96.5\% of datasets and 95.8\% of models lack the required license text, only 2.3\% of datasets and 3.2\% of models satisfy both license text and copyright requirements, and even when upstream artifacts provide complete licensing evidence, attribution rarely propagates downstream: only 27.59\% of models preserve compliant dataset notices and only 5.75\% of applications preserve compliant model notices (with just 6.38\% preserving any linked upstream notice). Practitioners cannot assume permissive labels confer the rights they claim: license files and notices, not metadata, are the source of legal truth. To support future research, we release our full audit dataset and reproducible pipeline.

</details>


### [173] [Dr. MAS: Stable Reinforcement Learning for Multi-Agent LLM Systems](https://arxiv.org/abs/2602.08847)
*Lang Feng,Longtao Zheng,Shuo He,Fuxiang Zhang,Bo An*

Main category: cs.LG

TL;DR: Proposes Dr. MAS, an agent-wise advantage normalization method that stabilizes reinforcement learning for multi-agent LLM systems by fixing gradient-norm instability from global baseline deviation, achieving significant performance gains on math and search tasks.


<details>
  <summary>Details</summary>
Motivation: Multi-agent LLM systems enable advanced reasoning but suffer from unreliable RL post-training. The core issue is training instability when extending group-based RL, where global normalization baselines deviate from diverse agents' reward distributions, causing gradient-norm instability.

Method: Dr. MAS uses per-agent advantage normalization with individual reward statistics to calibrate gradient scales. It also provides an end-to-end framework with scalable orchestration, flexible per-agent LLM configs, and shared resource scheduling.

Result: On Qwen2.5/Qwen3 models, Dr. MAS achieves +5.6% avg@16/+4.6% pass@16 on math and +15.2% avg@16/+13.1% pass@16 on search vs. vanilla GRPO, while eliminating gradient spikes and improving efficiency under heterogeneous agent assignments.

Conclusion: Dr. MAS is a simple, stable RL training recipe that theoretically and empirically addresses multi-agent LLM training instability, enabling scalable and efficient post-training for diverse agent systems.

Abstract: Multi-agent LLM systems enable advanced reasoning and tool use via role specialization, yet reliable reinforcement learning (RL) post-training for such systems remains difficult. In this work, we theoretically pinpoint a key reason for training instability when extending group-based RL to multi-agent LLM systems. We show that under GRPO-style optimization, a global normalization baseline may deviate from diverse agents' reward distributions, which ultimately leads to gradient-norm instability. Based on this finding, we propose Dr. MAS, a simple and stable RL training recipe for multi-agent LLM systems. Dr. MAS uses an agent-wise remedy: normalizing advantages per agent using each agent's own reward statistics, which calibrates gradient scales and dramatically stabilizes training, both theoretically and empirically. Beyond the algorithm, Dr. MAS provides an end-to-end RL training framework for multi-agent LLM systems, supporting scalable orchestration, flexible per-agent LLM serving and optimization configs, and shared resource scheduling of LLM actor backends. We evaluate Dr. MAS on multi-agent math reasoning and multi-turn search benchmarks using Qwen2.5 and Qwen3 series models. Dr. MAS achieves clear gains over vanilla GRPO (e.g., +5.6\% avg@16 and +4.6\% pass@16 on math, and +15.2\% avg@16 and +13.1\% pass@16 on search) while largely eliminating gradient spikes. Moreover, it remains highly effective under heterogeneous agent-model assignments while improving efficiency.

</details>


### [174] [M-Loss: Quantifying Model Merging Compatibility with Limited Unlabeled Data](https://arxiv.org/abs/2602.08564)
*Tiantong Wang,Yiyang Duan,Haoyu Chen,Tiantong Wu,Wei Yang Bryan Lim*

Main category: cs.LG

TL;DR: 提出M-Loss指标来量化模型合并的兼容性，通过测量参数平均与模型集成之间的差异，使合并后的模型性能更接近集成效果，同时避免集成的高昂计算和存储成本。


<details>
  <summary>Details</summary>
Motivation: 大规模模型训练计算成本高且依赖标注数据，传统模型合并技术（参数平均）容易混合不可泛化特征，而模型集成虽性能更稳但推理成本高、存储需求大。现有研究缺乏理论证据和评估指标来桥接合并与集成。

Method: 提出Merging-ensembling loss (M-Loss)这一新评估指标，利用少量无标签数据量化源模型兼容性，通过在层和节点级别测量参数平均与模型集成的差异，既作为合并可行性的理论判据，又指导模型剪枝的参数重要性。

Result: 理论分析和实验证明，引入M-Loss能显著提升合并模型与模型集成的一致性，提供可扩展且高效的模型整合框架。

Conclusion: M-Loss有效解决了模型合并与集成之间的鸿沟，为模型合并提供了理论基础和实践指导，实现了低成本下的高性能模型整合。

Abstract: Training of large-scale models is both computationally intensive and often constrained by the availability of labeled data. Model merging offers a compelling alternative by directly integrating the weights of multiple source models without requiring additional data or extensive training. However, conventional model merging techniques, such as parameter averaging, often suffer from the unintended combination of non-generalizable features, especially when source models exhibit significant weight disparities. Comparatively, model ensembling generally provides more stable and superior performance that aggregates multiple models by averaging outputs. However, it incurs higher inference costs and increased storage requirements. While previous studies experimentally showed the similarities between model merging and ensembling, theoretical evidence and evaluation metrics remain lacking. To address this gap, we introduce Merging-ensembling loss (M-Loss), a novel evaluation metric that quantifies the compatibility of merging source models using very limited unlabeled data. By measuring the discrepancy between parameter averaging and model ensembling at layer and node levels, M-Loss facilitates more effective merging strategies. Specifically, M-Loss serves both as a quantitative criterion of the theoretical feasibility of model merging, and a guide for parameter significance in model pruning. Our theoretical analysis and empirical evaluations demonstrate that incorporating M-Loss into the merging process significantly improves the alignment between merged models and model ensembling, providing a scalable and efficient framework for accurate model consolidation.

</details>


### [175] [Discovering Interpretable Algorithms by Decompiling Transformers to RASP](https://arxiv.org/abs/2602.08857)
*Xinting Huang,Aleksandra Bakalova,Satwik Bhattamishra,William Merrill,Michael Hahn*

Main category: cs.LG

TL;DR: 提出通用方法从训练好的Transformer中提取可解释的RASP程序，通过重参数化和因果干预发现最小充分子程序，实验证明长度泛化的Transformer内部实现了简单RASP程序


<details>
  <summary>Details</summary>
Motivation: 现有研究证实Transformer计算可被RASP语言模拟，且Transformer在简单RASP程序对应任务上能长度泛化，但尚未验证训练模型是否真正实现了这些可解释程序

Method: 将Transformer忠实地重参数化为RASP程序，再通过因果干预技术识别最小充分子程序

Result: 在算法和形式语言任务的中小型Transformer上，该方法成功从长度泛化模型中恢复出简单可解释的RASP程序

Conclusion: 首次提供直接证据表明Transformer内部实现了简单RASP程序，为模型可解释性提供新方法论

Abstract: Recent work has shown that the computations of Transformers can be simulated in the RASP family of programming languages. These findings have enabled improved understanding of the expressive capacity and generalization abilities of Transformers. In particular, Transformers have been suggested to length-generalize exactly on problems that have simple RASP programs. However, it remains open whether trained models actually implement simple interpretable programs. In this paper, we present a general method to extract such programs from trained Transformers. The idea is to faithfully re-parameterize a Transformer as a RASP program and then apply causal interventions to discover a small sufficient sub-program. In experiments on small Transformers trained on algorithmic and formal language tasks, we show that our method often recovers simple and interpretable RASP programs from length-generalizing transformers. Our results provide the most direct evidence so far that Transformers internally implement simple RASP programs.

</details>


### [176] [An arithmetic method algorithm optimizing k-nearest neighbors compared to regression algorithms and evaluated on real world data sources](https://arxiv.org/abs/2602.08577)
*Theodoros Anagnostopoulos,Evanthia Zervoudi,Christos Anagnostopoulos,Apostolos Christopoulos,Bogdan Wierzbinski*

Main category: cs.LG

TL;DR: 本文提出一种算术方法回归(AMR)算法作为k近邻回归的优化方案，该算法利用可求解任意实变量线性方程的算术方法，在真实世界数据集上表现出与现有算法相当或更优的性能。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是通过利用一种可求解任意实变量线性方程的新颖算术方法，来优化k近邻回归算法，以提高其效率和性能。

Method: 方法包括：(1)采用算术方法算法(AMA)来评估该算术方法；(2)提出利用AMA优化k近邻的算术方法回归(AMR)算法；(3)使用最优推断决策规则将AMR与其他回归算法进行比较；(4)在公开的真实世界数据集上进行评估。

Result: 结果表明，所提出的AMR算法与其他回归算法性能相当，并且在大多数情况下优于k近邻算法。

Conclusion: 引入的AMR算法是k近邻回归的有效优化方案。

Abstract: Linear regression analysis focuses on predicting a numeric regressand value based on certain regressor values. In this context, k-Nearest Neighbors (k-NN) is a common non-parametric regression algorithm, which achieves efficient performance when compared with other algorithms in literature. In this research effort an optimization of the k-NN algorithm is proposed by exploiting the potentiality of an introduced arithmetic method, which can provide solutions for linear equations involving an arbitrary number of real variables. Specifically, an Arithmetic Method Algorithm (AMA) is adopted to assess the efficiency of the introduced arithmetic method, while an Arithmetic Method Regression (AMR) algorithm is proposed as an optimization of k-NN adopting the potentiality of AMA. Such algorithm is compared with other regression algorithms, according to an introduced optimal inference decision rule, and evaluated on certain real world data sources, which are publicly available. Results are promising since the proposed AMR algorithm has comparable performance with the other algorithms, while in most cases it achieves better performance than the k-NN. The output results indicate that introduced AMR is an optimization of k-NN.

</details>


### [177] [AnomSeer: Reinforcing Multimodal LLMs to Reason for Time-Series Anomaly Detection](https://arxiv.org/abs/2602.08868)
*Junru Zhang,Lang Feng,Haoran Shi,Xu Guo,Han Yu,Yabo Dong,Duanqing Xu*

Main category: cs.LG

TL;DR: 提出AnomSeer框架，通过专家思维链和时间序列 grounded 策略优化(TimerPO)，让小型多模态大模型在时间序列异常检测任务上超越GPT-4o等大型商业模型，同时提供可验证的细粒度推理过程


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs依赖粗略时间序列启发式方法，难以进行复杂多维的细粒度推理，而这对于理解时间序列数据至关重要

Method: 1) 生成专家思维链轨迹提供可验证的细粒度推理；2) 提出TimerPO优化方法，包含基于最优传输的时间序列grounded优势函数和正交投影，确保辅助信号不干扰主任务

Result: 在多种异常场景下，Qwen2.5-VL-3B/7B-Instruct版本的AnomSeer在分类和定位准确率上超越GPT-4o等更大规模商业基线，尤其在点和频率驱动的异常上表现突出，并能生成支持结论的合理推理轨迹

Conclusion: 通过将推理锚定在精确的时间序列结构细节上，小模型也能实现比大型商业模型更好的异常检测性能，同时提供可解释的推理过程

Abstract: Time-series anomaly detection (TSAD) with multimodal large language models (MLLMs) is an emerging area, yet a persistent challenge remains: MLLMs rely on coarse time-series heuristics but struggle with multi-dimensional, detailed reasoning, which is vital for understanding complex time-series data. We present AnomSeer to address this by reinforcing the model to ground its reasoning in precise, structural details of time series, unifying anomaly classification, localization, and explanation. At its core, an expert chain-of-thought trace is generated to provide a verifiable, fine-grained reasoning from classical analyses (e.g., statistical measures, frequency transforms). Building on this, we propose a novel time-series grounded policy optimization (TimerPO) that incorporates two additional components beyond standard reinforcement learning: a time-series grounded advantage based on optimal transport and an orthogonal projection to ensure this auxiliary granular signal does not interfere with the primary detection objective. Across diverse anomaly scenarios, AnomSeer, with Qwen2.5-VL-3B/7B-Instruct, outperforms larger commercial baselines (e.g., GPT-4o) in classification and localization accuracy, particularly on point- and frequency-driven exceptions. Moreover, it produces plausible time-series reasoning traces that support its conclusions.

</details>


### [178] [Modeling Score Approximation Errors in Diffusion Models via Forward SPDEs](https://arxiv.org/abs/2602.08579)
*Junsu Seo*

Main category: cs.LG

TL;DR: 提出基于随机偏微分方程(SPDE)的框架分析分数生成模型(SGMs)，将分数估计误差视为随机源驱动Fokker-Planck方程，通过几何稳定性和位移凸性解释模型鲁棒性，并引入基于二次变分的评估指标（仅需10%采样轨迹即可生效）。


<details>
  <summary>Details</summary>
Motivation: 突破传统粒子中心SDE分析局限，从概率密度场演化视角研究SGMs的鲁棒性机制，探索分数估计误差对生成过程的全局影响。

Method: 采用SPDE框架建模随机漂移扰动下的概率密度演化，在简化设定中结合几何稳定性与位移凸性理论分析模型行为，设计径向测试函数投影的二次变分评估指标。

Result: 新指标仅需初始10%采样轨迹即可有效评估模型性能，验证了框架在计算效率上的潜力，初步证实几何稳定性对鲁棒性的解释力。

Conclusion: SPDE框架为理解SGMs鲁棒性提供新视角，所提评估指标显著降低计算成本，有望优化生成模型的效率与可靠性。

Abstract: This study investigates the dynamics of Score-based Generative Models (SGMs) by treating the score estimation error as a stochastic source driving the Fokker-Planck equation. Departing from particle-centric SDE analyses, we employ an SPDE framework to model the evolution of the probability density field under stochastic drift perturbations. Under a simplified setting, we utilize this framework to interpret the robustness of generative models through the lens of geometric stability and displacement convexity. Furthermore, we introduce a candidate evaluation metric derived from the quadratic variation of the SPDE solution projected onto a radial test function. Preliminary observations suggest that this metric remains effective using only the initial 10% of the sampling trajectory, indicating a potential for computational efficiency.

</details>


### [179] [Learning Potentials for Dynamic Matching and Application to Heart Transplantation](https://arxiv.org/abs/2602.08878)
*Itai Zilberstein,Ioannis Anagnostides,Zachary W. Sollie,Arman Kilic,Tuomas Sandholm*

Main category: cs.LG

TL;DR: 提出基于势函数和自监督模仿学习的动态心脏分配新框架，通过模拟全知算法优化群体结果，在历史数据验证中显著优于现有美国政策及连续分布方案


<details>
  <summary>Details</summary>
Motivation: 美国心脏移植面临器官短缺与分配效率低下问题，现有静态规则无法适应器官动态到达和等待者构成的变化，亟需在政策改革窗口期建立数据驱动的优化模型

Method: 1) 扩展肾脏交换中的势函数概念至更高维度表达；2) 采用自监督模仿学习训练势函数以模拟全知算法；3) 构建非短视的通用在线匹配策略框架

Result: 基于真实历史数据的仿真表明，该策略在群体级结果优化上显著优于美国现行分配政策及连续分布框架，验证了动态分配模型的优越性

Conclusion: 为美国心脏移植分配系统改革提供可扩展的理论基础与实践路径，推动刚性规则向数据驱动动态策略转型，提升器官利用效率和患者生存率

Abstract: Each year, thousands of patients in need of heart transplants face life-threatening wait times due to organ scarcity. While allocation policies aim to maximize population-level outcomes, current approaches often fail to account for the dynamic arrival of organs and the composition of waitlisted candidates, thereby hampering efficiency. The United States is transitioning from rigid, rule-based allocation to more flexible data-driven models. In this paper, we propose a novel framework for non-myopic policy optimization in general online matching relying on potentials, a concept originally introduced for kidney exchange. We develop scalable and accurate ways of learning potentials that are higher-dimensional and more expressive than prior approaches. Our approach is a form of self-supervised imitation learning: the potentials are trained to mimic an omniscient algorithm that has perfect foresight. We focus on the application of heart transplant allocation and demonstrate, using real historical data, that our policies significantly outperform prior approaches -- including the current US status quo policy and the proposed continuous distribution framework -- in optimizing for population-level outcomes. Our analysis and methods come at a pivotal moment in US policy, as the current heart transplant allocation system is under review. We propose a scalable and theoretically grounded path toward more effective organ allocation.

</details>


### [180] [Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression](https://arxiv.org/abs/2602.08885)
*Paul Saegert,Ullrich Köthe*

Main category: cs.LG

TL;DR: 该论文指出，表达式归一化速度慢是摊销符号回归扩展的关键障碍。他们提出SimpliPy，一种比SymPy快100倍的基于规则的化简引擎，并集成到Flash-ANSR框架中，该框架在FastSRB基准上超越基线，与最先进方法性能相当，同时能发现更简洁的表达式。


<details>
  <summary>Details</summary>
Motivation: 摊销符号回归比遗传编程更高效，但当前难以扩展到真实科学复杂性，主要障碍是缺乏快速将等价表达式归约为简洁规范形式的能力。现有方法使用SymPy等通用计算机代数系统，但计算成本高昂，严重限制了训练和推理速度。

Method: 作者提出SimpliPy，一种基于规则的化简引擎，比SymPy快100倍且质量相当。将其集成到Flash-ANSR框架中，支持大规模训练集、高效利用token预算以及系统性的训练集去污染。

Result: Flash-ANSR在FastSRB基准上准确率显著优于摊销基线（NeSymReS、E2E），与最先进直接优化方法PySR性能相当，且随着推理预算增加能恢复更简洁而非更复杂的表达式。

Conclusion: 快速表达式归一化是扩展摊销符号回归的关键，SimpliPy通过100倍加速实现了这一目标，使Flash-ANSR既能高效又能发现高质量、可解释的符号模型。

Abstract: Symbolic regression (SR) aims to discover interpretable analytical expressions that accurately describe observed data. Amortized SR promises to be much more efficient than the predominant genetic programming SR methods, but currently struggles to scale to realistic scientific complexity. We find that a key obstacle is the lack of a fast reduction of equivalent expressions to a concise normalized form. Amortized SR has addressed this by general-purpose Computer Algebra Systems (CAS) like SymPy, but the high computational cost severely limits training and inference speed. We propose SimpliPy, a rule-based simplification engine achieving a 100-fold speed-up over SymPy at comparable quality. This enables substantial improvements in amortized SR, including scalability to much larger training sets, more efficient use of the per-expression token budget, and systematic training set decontamination with respect to equivalent test expressions. We demonstrate these advantages in our Flash-ANSR framework, which achieves much better accuracy than amortized baselines (NeSymReS, E2E) on the FastSRB benchmark. Moreover, it performs on par with state-of-the-art direct optimization (PySR) while recovering more concise instead of more complex expressions with increasing inference budget.

</details>


### [181] [StealthRL: Reinforcement Learning Paraphrase Attacks for Multi-Detector Evasion of AI-Text Detectors](https://arxiv.org/abs/2602.08934)
*Suraj Ranganath,Atharv Ramesh*

Main category: cs.LG

TL;DR: 提出StealthRL强化学习框架，通过对抗训练使AI文本检测器在1%误报率下失效，攻击成功率达99.9%，暴露严重鲁棒性缺陷。


<details>
  <summary>Details</summary>
Motivation: AI文本检测器面临对抗性释义攻击的关键鲁棒性挑战。现有检测器在语义保持的改写攻击下表现脆弱，缺乏系统性的压力测试方法。

Method: 采用StealthRL框架，基于GRPO算法和Qwen3-4B的LoRA适配器，在多检测器集成上训练释义策略，优化检测逃避与语义保持的复合奖励。

Result: 在1%误报率下实现0.001平均检测率，AUROC从0.74降至0.27，攻击成功率99.9%。攻击可迁移至未见过的检测器家族，揭示架构共性漏洞。

Conclusion: 研究证实当前AI文本检测器存在重大鲁棒性差距，StealthRL为对抗性评估提供了原则性协议，将促进更可靠的检测器开发。

Abstract: AI-text detectors face a critical robustness challenge: adversarial paraphrasing attacks that preserve semantics while evading detection. We introduce StealthRL, a reinforcement learning framework that stress-tests detector robustness under realistic adversarial conditions. StealthRL trains a paraphrase policy against a multi-detector ensemble using Group Relative Policy Optimization (GRPO) with LoRA adapters on Qwen3-4B, optimizing a composite reward that balances detector evasion with semantic preservation. We evaluate six attack settings (M0-M5) against three detector families (RoBERTa, FastDetectGPT, and Binoculars) at the security-relevant 1% false positive rate operating point. StealthRL achieves near-zero detection (0.001 mean TPR@1%FPR), reduces mean AUROC from 0.74 to 0.27, and attains a 99.9% attack success rate. Critically, attacks transfer to a held-out detector family not seen during training, revealing shared architectural vulnerabilities rather than detector-specific brittleness. We additionally conduct LLM-based quality evaluation via Likert scoring, analyze detector score distributions to explain why evasion succeeds, and provide per-detector AUROC with bootstrap confidence intervals. Our results expose significant robustness gaps in current AI-text detection and establish StealthRL as a principled adversarial evaluation protocol. Code and evaluation pipeline are publicly available at https://github.com/suraj-ranganath/StealthRL.

</details>


### [182] [FairRARI: A Plug and Play Framework for Fairness-Aware PageRank](https://arxiv.org/abs/2602.08589)
*Emmanouil Kariotakis,Aritra Konar*

Main category: cs.LG

TL;DR: 提出FairRARI框架，通过凸优化求解满足群体公平性的PageRank向量，在保证目标公平水平的同时保持与原算法相同的计算效率，实验验证其有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 现有公平PageRank算法存在缺陷：部分无法确保达到目标公平水平，部分缺乏最优性保证。亟需一种原则性算法来同时解决群体公平性约束与算法最优性问题。

Method: 建立统一凸优化框架FairRARI，基于PageRank的变分形式构建强凸约束优化问题，引入三种可高效处理的群体公平标准，实现"即插即用"的公平向量计算。

Result: 在真实数据集上，FairRARI相比现有方法在效用性上表现更优，且能精确达到预设的多顶点群体公平水平，计算时间复杂度与原始PageRank算法相同。

Conclusion: 该框架有效解决了公平PageRank计算的核心挑战，为图机器学习中的算法公平性提供了可扩展的理论基础和实践工具。

Abstract: PageRank (PR) is a fundamental algorithm in graph machine learning tasks. Owing to the increasing importance of algorithmic fairness, we consider the problem of computing PR vectors subject to various group-fairness criteria based on sensitive attributes of the vertices. At present, principled algorithms for this problem are lacking - some cannot guarantee that a target fairness level is achieved, while others do not feature optimality guarantees. In order to overcome these shortcomings, we put forth a unified in-processing convex optimization framework, termed FairRARI, for tackling different group-fairness criteria in a ``plug and play'' fashion. Leveraging a variational formulation of PR, the framework computes fair PR vectors by solving a strongly convex optimization problem with fairness constraints, thereby ensuring that a target fairness level is achieved. We further introduce three different fairness criteria which can be efficiently tackled using FairRARI to compute fair PR vectors with the same asymptotic time-complexity as the original PR algorithm. Extensive experiments on real-world datasets showcase that FairRARI outperforms existing methods in terms of utility, while achieving the desired fairness levels across multiple vertex groups; thereby highlighting its effectiveness.

</details>


### [183] [A Behavioural and Representational Evaluation of Goal-Directedness in Language Model Agents](https://arxiv.org/abs/2602.08964)
*Raghu Arghal,Fade Chen,Niall Dalton,Evgenii Kortukov,Calum McNamara,Angelos Nalmpantis,Moksh Nirvaan,Gabriele Sarti,Mario Giulianelli*

Main category: cs.LG

TL;DR: 本文提出了一个整合行为评估与模型内部表征解释性分析的框架，用于评估智能体的目标导向性。通过大语言模型在2D网格世界导航的案例研究，发现智能体性能随任务难度扩展且对变换保持鲁棒，其内部非线性编码了空间地图信息，且推理过程会调整表征以支持即时决策。研究强调超越行为评估的内省式检查对理解智能体目标追求机制的必要性。


<details>
  <summary>Details</summary>
Motivation: 理解智能体的目标有助于解释和预测其行为，但目前尚未建立能够可靠地将目标归因于智能体系统的成熟方法论。

Method: 作者提出一个将行为评估与基于解释性分析相结合的目标导向性评估框架。案例研究中，考察了一个在2D网格世界朝向目标状态导航的大语言模型智能体，通过在不同网格大小、障碍密度和目标结构下与最优策略对比进行行为评估，并采用探测方法解码智能体对环境状态和多步行动计划的内部表征。

Result: 该LLM智能体的性能随任务难度变化，同时对保持难度的变换和复杂目标结构具有鲁棒性；它非线性地编码了环境的粗略空间地图，保留了关于自身位置和目标位置的任务相关线索；其行为与内部表征基本一致；推理过程会重组表征，从广泛的环境结构线索转向支持即时行动选择的信息。

Conclusion: 需要超越单纯的行为评估，通过内省式检查（解释性方法）来充分表征智能体如何表征和追求其目标。

Abstract: Understanding an agent's goals helps explain and predict its behaviour, yet there is no established methodology for reliably attributing goals to agentic systems. We propose a framework for evaluating goal-directedness that integrates behavioural evaluation with interpretability-based analyses of models' internal representations. As a case study, we examine an LLM agent navigating a 2D grid world toward a goal state. Behaviourally, we evaluate the agent against an optimal policy across varying grid sizes, obstacle densities, and goal structures, finding that performance scales with task difficulty while remaining robust to difficulty-preserving transformations and complex goal structures. We then use probing methods to decode the agent's internal representations of the environment state and its multi-step action plans. We find that the LLM agent non-linearly encodes a coarse spatial map of the environment, preserving approximate task-relevant cues about its position and the goal location; that its actions are broadly consistent with these internal representations; and that reasoning reorganises them, shifting from broader environment structural cues toward information supporting immediate action selection. Our findings support the view that introspective examination is required beyond behavioural evaluations to characterise how agents represent and pursue their objectives.

</details>


### [184] [SDFed: Bridging Local Global Discrepancy via Subspace Refinement and Divergence Control in Federated Prompt Learning](https://arxiv.org/abs/2602.08590)
*Yicheng Di,Wei Yuan,Tieke He,Zhanjie Zhang,Ao Ma,Yuan Liu,Hongzhi Yin*

Main category: cs.LG

TL;DR: SDFed是一种异构联邦提示学习框架，通过可变长度本地提示和子空间细化策略，在保持全局提示聚合效率的同时，有效减少局部与全局知识冲突。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言预训练模型在隐私敏感的联邦学习中面临通信成本高和客户端数据有限的挑战。现有联邦提示学习方法强制统一提示结构，无法适应客户端在数据分布和系统资源上的异构性，导致性能下降和知识冲突。

Method: SDFed维护固定长度全局提示以实现高效聚合，同时允许各客户端学习可变长度本地提示以适配自身数据特征和计算能力。该框架引入本地提示的子空间细化方法和信息保留-发散控制策略，在保持全局与局部表示适当分离的同时保留关键局部信息。

Result: 在多个数据集上的实验表明，SDFed在异构联邦环境中持续提升了模型性能和鲁棒性。

Conclusion: SDFed通过子空间细化与发散控制有效弥合了局部-全局差异，为异构联邦提示学习提供了有效解决方案。

Abstract: Vision-language pretrained models offer strong transferable representations, yet adapting them in privacy-sensitive multi-party settings is challenging due to the high communication cost of federated optimization and the limited local data on clients. Federated prompt learning mitigates this issue by keeping the VLPM backbone frozen and collaboratively training lightweight prompt parameters. However, existing approaches typically enforce a unified prompt structure and length across clients, which is inadequate under practical client heterogeneity in both data distributions and system resources, and may further introduce conflicts between globally shared and locally optimal knowledge. To address these challenges, we propose \textbf{SDFed}, a heterogeneous federated prompt learning framework that bridges Local-Global Discrepancy via Subspace Refinement and Divergence Control. SDFed maintains a fixed-length global prompt for efficient aggregation while allowing each client to learn a variable-length local prompt to better match its data characteristics and capacity. To mitigate local-global conflicts and facilitate effective knowledge transfer, SDFed introduces a subspace refinement method for local prompts and an information retention and divergence control strategy that preserves key local information while maintaining appropriate separability between global and local representations. Extensive experiments on several datasets demonstrate that SDFed consistently improves performance and robustness in heterogeneous federated settings.

</details>


### [185] [StretchTime: Adaptive Time Series Forecasting via Symplectic Attention](https://arxiv.org/abs/2602.08983)
*Yubin Kim,Viresh Pati,Jevon Twitty,Vinh Pham,Shihao Yang,Jiecheng Lu*

Main category: cs.LG

TL;DR: 该论文针对时间序列预测中Transformer位置编码无法处理时间扭曲的问题，提出了基于哈密顿力学的辛位置编码(SyPE)和StretchTime架构，在标准基准测试中达到SOTA性能，对非平稳时间动态数据表现出更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer时间序列预测模型依赖的位置编码假设均匀时间流逝，但现实世界系统(如金融周期、生物节律)存在"时间扭曲"现象，时间有效流动与采样索引解耦。论文证明旋转位置编码(RoPE)在数学上无法表示非仿射时间扭曲。

Method: 提出辛位置编码(SyPE)，这是一个从哈密顿力学导出的可学习编码框架。它将旋转群SO(2)扩展为辛群Sp(2,R)，并配备新颖的输入相关自适应扭曲模块。通过让注意力机制端到端地自适应缩放或收缩时间坐标，捕获局部变化的周期性，无需预定义扭曲函数。在StretchTime架构中实现该机制。

Result: StretchTime在标准基准测试上达到SOTA性能，在展现非平稳时间动态的数据集上表现出卓越的鲁棒性。

Conclusion: 辛位置编码有效解决了时间序列预测中的时间扭曲问题，为处理非平稳时间动态提供了新思路，StretchTime架构在实践中验证了其有效性。

Abstract: Transformer architectures have established strong baselines in time series forecasting, yet they typically rely on positional encodings that assume uniform, index-based temporal progression. However, real-world systems, from shifting financial cycles to elastic biological rhythms, frequently exhibit "time-warped" dynamics where the effective flow of time decouples from the sampling index. In this work, we first formalize this misalignment and prove that rotary position embedding (RoPE) is mathematically incapable of representing non-affine temporal warping. To address this, we propose Symplectic Positional Embeddings (SyPE), a learnable encoding framework derived from Hamiltonian mechanics. SyPE strictly generalizes RoPE by extending the rotation group $\mathrm{SO}(2)$ to the symplectic group $\mathrm{Sp}(2,\mathbb{R})$, modulated by a novel input-dependent adaptive warp module. By allowing the attention mechanism to adaptively dilate or contract temporal coordinates end-to-end, our approach captures locally varying periodicities without requiring pre-defined warping functions. We implement this mechanism in StretchTime, a multivariate forecasting architecture that achieves state-of-the-art performance on standard benchmarks, demonstrating superior robustness on datasets exhibiting non-stationary temporal dynamics.

</details>


### [186] [TFMLinker: Universal Link Predictor by Graph In-Context Learning with Tabular Foundation Models](https://arxiv.org/abs/2602.08592)
*Tianyin Liao,Chunyu Hu,Yicheng Sui,Xingxuan Zhang,Peng Cui,Jianxin Li,Ziwei Zhang*

Main category: cs.LG

TL;DR: The paper proposes TFMLinker, a novel method that adapts Tabular Foundation Models (TFMs) for universal link prediction across diverse graphs without dataset-specific fine-tuning, addressing limitations of existing Graph Foundation Models.


<details>
  <summary>Details</summary>
Motivation: Link prediction is fundamental in graph ML with wide applications, but existing Graph Foundation Models have limitations like limited pre-training scale or heavy reliance on textual information. Inspired by TFMs' success in universal tabular prediction, the authors explore using TFMs for link prediction, though it faces challenges in obtaining proper context and capturing topological information.

Method: The proposed TFMLinker has three key components: (1) a prototype-augmented local-global context module to construct context capturing both graph-specific and cross-graph patterns, (2) a universal topology-aware link encoder to capture link-centric topological information and generate link representations, and (3) leveraging the TFM for link existence prediction via in-context learning without dataset-specific fine-tuning.

Result: Experiments on 6 graph benchmarks across diverse domains demonstrate that TFMLinker outperforms state-of-the-art baselines without requiring dataset-specific fine-tuning.

Conclusion: The method successfully leverages TFM's in-context learning capabilities to achieve universal link prediction across diverse graphs, overcoming limitations of existing approaches and demonstrating strong cross-domain performance.

Abstract: Link prediction is a fundamental task in graph machine learning with widespread applications such as recommendation systems, drug discovery, knowledge graphs, etc. In the foundation model era, how to develop universal link prediction methods across datasets and domains becomes a key problem, with some initial attempts adopting Graph Foundation Models utilizing Graph Neural Networks and Large Language Models. However, the existing methods face notable limitations, including limited pre-training scale or heavy reliance on textual information. Motivated by the success of tabular foundation models (TFMs) in achieving universal prediction across diverse tabular datasets, we explore an alternative approach by TFMs, which are pre-trained on diverse synthetic datasets sampled from structural causal models and support strong in-context learning independent of textual attributes. Nevertheless, adapting TFMs for link prediction faces severe technical challenges such as how to obtain the necessary context and capture link-centric topological information. To solve these challenges, we propose TFMLinker (Tabular Foundation Model for Link Predictor), aiming to leverage the in-context learning capabilities of TFMs to perform link prediction across diverse graphs without requiring dataset-specific fine-tuning. Specifically, we first develop a prototype-augmented local-global context module to construct context that captures both graph-specific and cross-graph transferable patterns. Next, we design a universal topology-aware link encoder to capture link-centric topological information and generate link representations as inputs for the TFM. Finally, we employ the TFM to predict link existence through in-context learning. Experiments on 6 graph benchmarks across diverse domains demonstrate the superiority of our method over state-of-the-art baselines without requiring dataset-specific finetuning.

</details>


### [187] [ARO: A New Lens On Matrix Optimization For Large Models](https://arxiv.org/abs/2602.09006)
*Wenbo Gong,Javier Zazo,Qijun Luo,Puqian Wang,James Hensman,Chao Ma*

Main category: cs.LG

TL;DR: ARO proposes adaptively rotating gradients in LLM training, outperforming AdamW by 1.3-1.35x and orthogonalization methods by 1.1-1.15x up to 8B parameters without diminishing returns.


<details>
  <summary>Details</summary>
Motivation: Current matrix optimizers focus only on orthogonalization/whitening; this paper asks whether we can develop fundamentally new paradigms beyond orthogonalization to further push LLM training efficiency.

Method: Treats gradient rotation as core design principle: performs normed steepest descent in adaptively rotated coordinate system, using novel norm-informed rotation policy. Reformulates as symmetry-aware optimizer exploiting rotational symmetries in residual streams.

Result: Under rigorously controlled benchmarking, ARO consistently beats AdamW (1.3-1.35×) and orthogonalization methods (1.1-1.15×) in LLM pretraining up to 8B activated parameters and 8× overtrain budget, with no observed diminishing returns.

Conclusion: ARO establishes rotation as a powerful first-class principle for optimization, enabling efficient cross-layer/module coupling exploitation and opening new avenues beyond orthogonalization-based methods.

Abstract: Matrix-based optimizers have attracted growing interest for improving LLM training efficiency, with significant progress centered on orthogonalization/whitening based methods. While yielding substantial performance gains, a fundamental question arises: can we develop new paradigms beyond orthogonalization, pushing the efficiency frontier further? We present \textbf{Adaptively Rotated Optimization (ARO}, a new matrix optimization framework that treats gradient rotation as a first class design principle. ARO accelerates LLM training by performing normed steepest descent in a rotated coordinate system, where the rotation is determined by a novel norm-informed policy. This perspective yields update rules that go beyond existing orthogonalization and whitening optimizers, improving sample efficiency in practice. To make comparisons reliable, we propose a rigorously controlled benchmarking protocol that reduces confounding and bias. Under this protocol, ARO consistently outperforms AdamW (by 1.3 $\sim$1.35$\times$) and orthogonalization methods (by 1.1$\sim$1.15$\times$) in LLM pretraining at up to 8B activated parameters, and up to $8\times$ overtrain budget, without evidence of diminishing returns. Finally, we discuss how ARO can be reformulated as a symmetry-aware optimizer grounded in rotational symmetries of residual streams, motivating advanced designs that enable computationally efficient exploitation of cross-layer/cross module couplings.

</details>


### [188] [ERIS: Enhancing Privacy and Communication Efficiency in Serverless Federated Learning](https://arxiv.org/abs/2602.08617)
*Dario Fenoglio,Pasquale Polverino,Jacopo Quizi,Martin Gjoreski,Marc Langheinrich*

Main category: cs.LG

TL;DR: ERIS是一个无服务器联邦学习框架，通过模型分区和分布式客户端聚合，在十亿参数模型下实现了与FedAvg相当的精度，同时显著降低通信成本并提供强隐私保障，无需依赖重型加密或噪声注入。


<details>
  <summary>Details</summary>
Motivation: 将联邦学习扩展到十亿参数模型时，通信效率、模型精度和隐私保障之间存在关键权衡；现有方案往往孤立解决这些问题，牺牲精度或依赖昂贵的密码学工具。

Method: 提出ERIS框架，采用模型分区策略将聚合负载分散到多个客户端聚合器，并结合分布式移位梯度压缩机制，消除服务器瓶颈并平衡隐私与精度。

Result: 理论证明：(i) 在标准假设下与FedAvg具有相同收敛率；(ii) 隐私泄露与聚合器数量成反比。实验验证：在图像、文本和大语言模型任务中，达到FedAvg级精度，大幅降低通信成本，对成员推断和重构攻击具有更强鲁棒性。

Conclusion: ERIS成功解决了大规模联邦学习中的隐私-精度权衡问题，在无需重型加密的前提下，为十亿参数模型提供了高效、安全且可扩展的解决方案。

Abstract: Scaling federated learning (FL) to billion-parameter models introduces critical trade-offs between communication efficiency, model accuracy, and privacy guarantees. Existing solutions often tackle these challenges in isolation, sacrificing accuracy or relying on costly cryptographic tools. We propose ERIS, a serverless FL framework that balances privacy and accuracy while eliminating the server bottleneck and distributing the communication load. ERIS combines a model partitioning strategy, distributing aggregation across multiple client-side aggregators, with a distributed shifted gradient compression mechanism. We theoretically prove that ERIS (i) converges at the same rate as FedAvg under standard assumptions, and (ii) bounds mutual information leakage inversely with the number of aggregators, enabling strong privacy guarantees with no accuracy degradation. Experiments across image and text tasks, including large language models, confirm that ERIS achieves FedAvg-level accuracy while substantially reducing communication cost and improving robustness to membership inference and reconstruction attacks, without relying on heavy cryptography or noise injection.

</details>


### [189] [Projected Gradient Ascent for Efficient Reward-Guided Updates with One-Step Generative Models](https://arxiv.org/abs/2602.08646)
*Jisung Hwang,Minhyuk Sung*

Main category: cs.LG

TL;DR: 本文提出一种基于投影梯度上升的约束潜变量优化方法，通过硬性白高斯噪声约束来提升预训练生成模型的奖励引导生成质量，在保持O(N log N)计算复杂度的同时防止奖励黑客现象，实现效率与可靠性的统一。


<details>
  <summary>Details</summary>
Motivation: 测试时潜变量优化虽能显著改善奖励引导生成效果，但存在两大核心问题：一是容易发生奖励黑客现象导致生成质量下降，二是计算速度过慢难以实用。根本原因在于优化过程中潜变量偏离噪声特性产生漂移，从而引发不真实的伪影。

Method: 用硬性的白高斯噪声约束替代软正则化，通过投影梯度上升强制执行。每步更新后应用闭式投影，将潜变量显式维持在类噪声状态，防止漂移。该投影复杂度为O(N log N)，与FFT等标准算法相当，几乎不增加实际计算时间。

Result: 实验表明，该方法仅用30%的SOTA正则化方法的计算时间即可达到相当的审美评分，同时有效防止奖励黑客现象，避免质量退化。

Conclusion: 该约束优化方法成功实现了测试时优化在效率与可靠性上的平衡，为防止奖励黑客提供了一种实用解决方案，且不显著增加计算成本。

Abstract: We propose a constrained latent optimization method for reward-guided generation that preserves white Gaussian noise characteristics with negligible overhead. Test-time latent optimization can unlock substantially better reward-guided generations from pretrained generative models, but it is prone to reward hacking that degrades quality and also too slow for practical use. In this work, we make test-time optimization both efficient and reliable by replacing soft regularization with hard white Gaussian noise constraints enforced via projected gradient ascent. Our method applies a closed-form projection after each update to keep the latent vector explicitly noise-like throughout optimization, preventing the drift that leads to unrealistic artifacts. This enforcement adds minimal cost: the projection matches the $O(N \log N)$ complexity of standard algorithms such as sorting or FFT and does not practically increase wall-clock time. In experiments, our approach reaches a comparable Aesthetic Score using only 30% of the wall-clock time required by the SOTA regularization-based method, while preventing reward hacking.

</details>


### [190] [From Robotics to Sepsis Treatment: Offline RL via Geometric Pessimism](https://arxiv.org/abs/2602.08655)
*Sarthak Wanjari*

Main category: cs.LG

TL;DR: 提出几何悲观主义(Geo-IQL)，一种通过k近邻距离在状态-动作空间中引入密度惩罚的离线强化学习新方法，在保持计算效率的同时有效缓解分布外动作高估问题，在医疗等关键领域表现优异。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习面临分布外(OOD)动作高估的挑战，尤其当数据分布稀疏且破碎时。现有方案存在计算效率与性能之间的权衡：保守Q学习(CQL)计算成本高昂，而高效的期望方法(IQL)在病态数据集上会退化为行为克隆。

Method: 提出几何悲观主义(Geometric Pessimism)，一个模块化、计算高效的框架。通过在状态-动作嵌入空间中使用k近邻距离计算密度惩罚，并将预计算的惩罚项通过奖励塑形注入标准IQL算法，实现O(1)训练开销的分布外保守性。

Result: 在D4RL MuJoCo基准测试中，Geo-IQL在敏感不稳定的中等回放任务上超过标准IQL超过18个点，种子间方差降低4倍，且在稳定流形上不降低性能。在MIMIC-III脓毒症关键医疗数据集上，标准IQL退化为行为克隆时，Geo-IQL展现出主动策略改进，与临床医生终局决策一致率达86.4%（IQL仅75%），同时保持安全约束。

Conclusion: 几何悲观主义为在关键现实决策系统中安全克服局部最优提供了必要的正则化，在保持计算效率的同时有效解决了离线强化学习的分布外高估问题。

Abstract: Offline Reinforcement Learning (RL) promises the recovery of optimal policies from static datasets, yet it remains susceptible to the overestimation of out-of-distribution (OOD) actions, particularly in fractured and sparse data manifolds.Current solutions necessitates a trade off between computational efficiency and performance. Methods like CQL offers rigorous conservatism but require tremendous compute power while efficient expectile-based methods like IQL often fail to correct OOD errors on pathological datasets, collapsing to Behavioural Cloning. In this work, we propose Geometric Pessimism, a modular, compute-efficient framework that augments standard IQL with density-based penalty derived from k-nearest-neighbour distances in the state-action embedding space. By pre-computing the penalties applied to each state-action pair our method injects OOD conservatism via reward shaping with a O(1) training overhead. Evaluated on the D4Rl MuJoCo benchmark, our method, Geo-IQL outperforms standard IQL on sensitive and unstable medium-replay tasks by over 18 points, while reducing inter-seed variance by 4x. Furthermore, Geo-IQL does not degrade performance on stable manifolds. Crucially, we validate our algorithm on the MIMIC-III Sepsis critical care dataset. While standard IQL collapses to behaviour cloning, Geo-IQL demonstrates active policy improvement. Maintaining safety constraints, achieving 86.4% terminal agreement with clinicians compared to IQL's 75%. Our results suggest that geometric pessimism provides the necessary regularisation to safely overcome local optima in critical, real-world decision systems.

</details>


### [191] [Two-Stage Data Synthesization: A Statistics-Driven Restricted Trade-off between Privacy and Prediction](https://arxiv.org/abs/2602.08657)
*Xiaotong Liu,Shao-Bo Lin,Jun Fan,Ding-Xuan Zhou*

Main category: cs.LG

TL;DR: 该论文提出一种两阶段合成数据生成策略，通过合成-混合和核岭回归方法平衡隐私保护与预测性能，实现理论保证的最优预测效果


<details>
  <summary>Details</summary>
Motivation: 现有合成数据方法多关注统计信息保留，而单阶段设计难以平衡隐私保护所需的大扰动与预测性能对扰动的敏感性之间的矛盾

Method: 两阶段合成策略：第一阶段采用合成-混合策略（生成纯合成数据后与原数据融合）；第二阶段基于核岭回归（KRR），先用原数据训练模型，再用合成输入生成合成输出

Result: 实现了数据驱动的受限隐私-预测权衡，获得最优预测性能理论保证，并通过理论分析、数值模拟、营销应用及五个真实数据集验证了方法的通用性

Conclusion: 该两阶段策略有效解决了隐私与预测性能的权衡问题，具有数据驱动性和受限优化特性，在理论和实践中均表现出色

Abstract: Synthetic data have gained increasing attention across various domains, with a growing emphasis on their performance in downstream prediction tasks. However, most existing synthesis strategies focus on maintaining statistical information. Although some studies address prediction performance guarantees, their single-stage synthesis designs make it challenging to balance the privacy requirements that necessitate significant perturbations and the prediction performance that is sensitive to such perturbations. We propose a two-stage synthesis strategy. In the first stage, we introduce a synthesis-then-hybrid strategy, which involves a synthesis operation to generate pure synthetic data, followed by a hybrid operation that fuses the synthetic data with the original data. In the second stage, we present a kernel ridge regression (KRR)-based synthesis strategy, where a KRR model is first trained on the original data and then used to generate synthetic outputs based on the synthetic inputs produced in the first stage. By leveraging the theoretical strengths of KRR and the covariant distribution retention achieved in the first stage, our proposed two-stage synthesis strategy enables a statistics-driven restricted privacy--prediction trade-off and guarantee optimal prediction performance. We validate our approach and demonstrate its characteristics of being statistics-driven and restricted in achieving the privacy--prediction trade-off both theoretically and numerically. Additionally, we showcase its generalizability through applications to a marketing problem and five real-world datasets.

</details>


### [192] [Dashed Line Defense: Plug-And-Play Defense Against Adaptive Score-Based Query Attacks](https://arxiv.org/abs/2602.08679)
*Yanzhang Fu,Zizheng Guo,Jizhou Luo*

Main category: cs.LG

TL;DR: This paper proposes Dashed Line Defense (DLD), a plug-and-play post-processing method that protects deep learning models from adaptive score-based query attacks by introducing ambiguity in loss observations to disrupt adversarial example generation.


<details>
  <summary>Details</summary>
Motivation: Score-based query attacks pose serious threats to deep learning models through black-box access. Existing runtime defenses either require model parameter access or can be bypassed by adaptive attacks, exposing critical limitations in current protection methods.

Method: The authors propose Dashed Line Defense (DLD), a plug-and-play post-processing defense that introduces ambiguity in how observed loss reflects true adversarial strength, preventing attackers from reliably analyzing and adapting their queries to disrupt adversarial example generation.

Result: Theoretical guarantees of DLD's defense capability are provided, and experiments on ImageNet demonstrate that DLD consistently outperforms prior defenses even under worst-case adaptive attacks while preserving the model's predicted labels.

Conclusion: DLD effectively protects deep learning models against adaptive score-based query attacks by introducing strategic ambiguity in loss observations, offering a practical plug-and-play solution with theoretical guarantees and superior empirical performance.

Abstract: Score-based query attacks pose a serious threat to deep learning models by crafting adversarial examples (AEs) using only black-box access to model output scores, iteratively optimizing inputs based on observed loss values. While recent runtime defenses attempt to disrupt this process via output perturbation, most either require access to model parameters or fail when attackers adapt their tactics. In this paper, we first reveal that even the state-of-the-art plug-and-play defense can be bypassed by adaptive attacks, exposing a critical limitation of existing runtime defenses. We then propose Dashed Line Defense (DLD), a plug-and-play post-processing method specifically designed to withstand adaptive query strategies. By introducing ambiguity in how the observed loss reflects the true adversarial strength of candidate examples, DLD prevents attackers from reliably analyzing and adapting their queries, effectively disrupting the AE generation process. We provide theoretical guarantees of DLD's defense capability and validate its effectiveness through experiments on ImageNet, demonstrating that DLD consistently outperforms prior defenses--even under worst-case adaptive attacks--while preserving the model's predicted labels.

</details>


### [193] [The Theory and Practice of MAP Inference over Non-Convex Constraints](https://arxiv.org/abs/2602.08681)
*Leander Kurscheidt,Gabriele Masina,Roberto Sebastiani,Antonio Vergari*

Main category: cs.LG

TL;DR: 提出消息传递算法和域划分策略来解决安全关键系统中非凸非凹约束下的MAP推断问题，在合成和真实基准测试中优于基线方法


<details>
  <summary>Details</summary>
Motivation: 在安全关键场景中，概率机器学习系统需要满足代数约束进行预测（如避开障碍物的轨迹预测），但现实约束通常非凸且密度非(log-)凹，使得约束MAP推断极其困难

Method: 1) 研究连续变量约束MAP精确高效推断的条件，设计可扩展的消息传递算法；2) 提出通用策略，将域划分为凸可行区域并结合数值约束优化

Result: 在合成和真实世界基准测试中，所提方法优于无约束基线，并能扩展到复杂密度问题，这些问题是现有精确求解器难以处理的

Conclusion: 论文为约束MAP推断提供了可处理的片段和通用框架，证明了其在非凸/非凹设置中的有效性和可扩展性

Abstract: In many safety-critical settings, probabilistic ML systems have to make predictions subject to algebraic constraints, e.g., predicting the most likely trajectory that does not cross obstacles.
  These real-world constraints are rarely convex, nor the densities considered are (log-)concave.
  This makes computing this constrained maximum a posteriori (MAP) prediction efficiently and reliably extremely challenging.
  In this paper, we first investigate under which conditions we can perform constrained MAP inference over continuous variables exactly and efficiently and devise a scalable message-passing algorithm for this tractable fragment.
  Then, we devise a general constrained MAP strategy that interleaves partitioning the domain into convex feasible regions with numerical constrained optimization.
  We evaluate both methods on synthetic and real-world benchmarks, showing our %
  approaches outperform constraint-agnostic baselines, and scale to complex densities intractable for SoTA exact solvers.

</details>


### [194] [Learning To Sample From Diffusion Models Via Inverse Reinforcement Learning](https://arxiv.org/abs/2602.08689)
*Constant Bourdrez,Alexandre Vérine,Olivier Cappé*

Main category: cs.LG

TL;DR: 提出逆强化学习框架优化扩散模型的采样策略，无需重训练去噪器，通过策略梯度直接匹配目标行为，提升样本质量并自动调节超参数


<details>
  <summary>Details</summary>
Motivation: 扩散模型训练去噪器计算成本高，而采样过程灵活可调，需在不重训练的前提下改进采样策略与效率

Method: 将采样过程建模为离散时间有限时域马尔可夫决策过程，动作对应采样动态的可选修改，采用逆强化学习结合策略梯度技术直接匹配目标行为，避免定义显式奖励函数

Result: 实验证明该方法能提升预训练扩散模型的生成样本质量，并自动调节采样超参数

Conclusion: 所提出的逆强化学习框架可在训练后优化扩散采样策略，无需额外训练去噪器即可实现质量和效率的双重提升

Abstract: Diffusion models generate samples through an iterative denoising process, guided by a neural network. While training the denoiser on real-world data is computationally demanding, the sampling procedure itself is more flexible. This adaptability serves as a key lever in practice, enabling improvements in both the quality of generated samples and the efficiency of the sampling process. In this work, we introduce an inverse reinforcement learning framework for learning sampling strategies without retraining the denoiser. We formulate the diffusion sampling procedure as a discrete-time finite-horizon Markov Decision Process, where actions correspond to optional modifications of the sampling dynamics. To optimize action scheduling, we avoid defining an explicit reward function. Instead, we directly match the target behavior expected from the sampler using policy gradient techniques. We provide experimental evidence that this approach can improve the quality of samples generated by pretrained diffusion models and automatically tune sampling hyperparameters.

</details>


### [195] [SoK: The Pitfalls of Deep Reinforcement Learning for Cybersecurity](https://arxiv.org/abs/2602.08690)
*Shae McFadden,Myles Foley,Elizabeth Bates,Ilias Tsingenopoulos,Sanyam Vyas,Vasilios Mavroudis,Chris Hicks,Fabio Pierazzi*

Main category: cs.LG

TL;DR: This paper identifies 11 common methodological pitfalls in DRL for cybersecurity literature, analyzes their prevalence in 66 papers, demonstrates their practical impact through experiments, and provides actionable recommendations.


<details>
  <summary>Details</summary>
Motivation: DRL shows promise for cybersecurity applications but transitioning from simulations to real cyber environments introduces significant challenges due to adversarial, non-stationary, and partially-observable nature of security tasks.

Method: Systematically identified 11 methodological pitfalls across environment modeling, agent training, performance evaluation, and deployment stages. Analyzed 66 DRL4Sec papers (2018-2025) to quantify pitfall prevalence. Conducted controlled experiments in autonomous cyber defense, adversarial malware creation, and web security testing to demonstrate practical impact.

Result: Found an average of over five pitfalls per paper across the analyzed literature. Demonstrated significant practical impact of these pitfalls through controlled experiments in three cybersecurity domains.

Conclusion: Provided actionable recommendations for each identified pitfall to support development of more rigorous and deployable DRL-based security systems.

Abstract: Deep Reinforcement Learning (DRL) has achieved remarkable success in domains requiring sequential decision-making, motivating its application to cybersecurity problems. However, transitioning DRL from laboratory simulations to bespoke cyber environments can introduce numerous issues. This is further exacerbated by the often adversarial, non-stationary, and partially-observable nature of most cybersecurity tasks. In this paper, we identify and systematize 11 methodological pitfalls that frequently occur in DRL for cybersecurity (DRL4Sec) literature across the stages of environment modeling, agent training, performance evaluation, and system deployment. By analyzing 66 significant DRL4Sec papers (2018-2025), we quantify the prevalence of each pitfall and find an average of over five pitfalls per paper. We demonstrate the practical impact of these pitfalls using controlled experiments in (i) autonomous cyber defense, (ii) adversarial malware creation, and (iii) web security testing environments. Finally, we provide actionable recommendations for each pitfall to support the development of more rigorous and deployable DRL-based security systems.

</details>


### [196] [Reasoning aligns language models to human cognition](https://arxiv.org/abs/2602.08693)
*Gonçalo Guiomar,Elia Torre,Pehuen Moure,Victoria Shavina,Mario Giulianelli,Shih-Chii Liu,Valerio Mante*

Main category: cs.LG

TL;DR: 该研究探讨语言模型是否像人类一样在不确定性下决策，发现思维链推理使模型的推断过程更接近人类，但在主动信息获取方面仍存在差距。通过四参数模型解释这些差异。


<details>
  <summary>Details</summary>
Motivation: 理解语言模型在不确定性下的决策方式与人类有何异同，特别探究思维链推理在决策过程中的作用，并将采样（获取证据）与推断（整合证据）两个过程分离分析。

Method: 引入主动概率推理任务，清晰分离采样与推断阶段；对人类和多种大型语言模型进行基准测试，对比近优参考策略；构建包含记忆、策略、选择偏差和遮挡意识四个可解释潜变量的机制模型。

Result: 思维链推理是性能提升的关键，显著改善推断能力并产生类人信念轨迹，但对主动采样的提升有限。机制模型将人类与模型置于共享认知空间，重现跨主体行为特征，显示思维链使模型在推断上趋近人类，但在信息获取上仍有差距。

Conclusion: 思维链推理使语言模型的推断过程更类人化，但未完全弥合主动信息获取的差距。四变量模型成功捕捉行为差异，揭示模型在战略证据收集方面仍落后于人类。

Abstract: Do language models make decisions under uncertainty like humans do, and what role does chain-of-thought (CoT) reasoning play in the underlying decision process? We introduce an active probabilistic reasoning task that cleanly separates sampling (actively acquiring evidence) from inference (integrating evidence toward a decision). Benchmarking humans and a broad set of contemporary large language models against near-optimal reference policies reveals a consistent pattern: extended reasoning is the key determinant of strong performance, driving large gains in inference and producing belief trajectories that become strikingly human-like, while yielding only modest improvements in active sampling. To explain these differences, we fit a mechanistic model that captures systematic deviations from optimal behavior via four interpretable latent variables: memory, strategy, choice bias, and occlusion awareness. This model places humans and models in a shared low-dimensional cognitive space, reproduces behavioral signatures across agents, and shows how chain-of-thought shifts language models toward human-like regimes of evidence accumulation and belief-to-choice mapping, tightening alignment in inference while leaving a persistent gap in information acquisition.

</details>


### [197] [Trapped by simplicity: When Transformers fail to learn from noisy features](https://arxiv.org/abs/2602.08695)
*Evan Peters,Ando Deng,Matheus H. Zambianco,Devin Blankespoor,Achim Kempf*

Main category: cs.LG

TL;DR: 研究变压器噪声鲁棒学习能力，发现其在布尔函数学习上存在严重缺陷：虽在简单函数上优于LSTM，但在随机k-junta任务上因偏好简单解而失败，可通过灵敏度惩罚缓解。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练数据普遍存在噪声，但模型能否正确泛化到无噪声输入尚不明确。

Method: 通过k-稀疏奇偶校验、多数函数和随机k-junta等布尔函数，对比分析变压器与LSTM在含噪声特征数据上的学习表现。

Result: 变压器在k-稀疏奇偶校验和多数函数上成功实现噪声鲁棒学习，显著优于LSTM；但在随机k-junta任务上，当最优解的布尔灵敏度低于目标函数时，变压器因偏好简单函数而失败；通过增加高灵敏度惩罚项可使其逃脱错误解。

Conclusion: 变压器在特征噪声存在时学习布尔函数效果特别不理想。

Abstract: Noise is ubiquitous in data used to train large language models, but it is not well understood whether these models are able to correctly generalize to inputs generated without noise. Here, we study noise-robust learning: are transformers trained on data with noisy features able to find a target function that correctly predicts labels for noiseless features? We show that transformers succeed at noise-robust learning for a selection of $k$-sparse parity and majority functions, compared to LSTMs which fail at this task for even modest feature noise. However, we find that transformers typically fail at noise-robust learning of random $k$-juntas, especially when the boolean sensitivity of the optimal solution is smaller than that of the target function. We argue that this failure is due to a combination of two factors: transformers' bias toward simpler functions, combined with an observation that the optimal function for noise-robust learning typically has lower sensitivity than the target function for random boolean functions. We test this hypothesis by exploiting transformers' simplicity bias to trap them in an incorrect solution, but show that transformers can escape this trap by training with an additional loss term penalizing high-sensitivity solutions. Overall, we find that transformers are particularly ineffective for learning boolean functions in the presence of feature noise.

</details>


### [198] [Foundation Inference Models for Ordinary Differential Equations](https://arxiv.org/abs/2602.08733)
*Maximilian Mauel,Johannes R. Hübers,David Berghaus,Patrick Seifner,Ramses J. Sanchez*

Main category: cs.LG

TL;DR: FIM-ODE is a pretrained foundation model that directly predicts ODE vector fields from noisy trajectories in a single forward pass, achieving strong zero-shot performance and outperforming baselines without requiring machine learning expertise.


<details>
  <summary>Details</summary>
Motivation: Current ODE inference methods (symbolic regression, Gaussian process regression, Neural ODEs) are hindered by complex training pipelines, substantial machine learning expertise requirements, and strong dependence on system-specific prior knowledge.

Method: Pretrained FIM-ODE on a prior distribution of low-degree polynomial ODEs using neural operators to amortize inference, enabling direct vector field prediction from noisy trajectory data in a single forward pass.

Result: Achieved strong zero-shot performance matching/often improving upon ODEFormer across multiple regimes, with pretraining providing strong initialization for fast, stable finetuning that outperforms modern neural and GP baselines.

Conclusion: FIM-ODE successfully simplifies ODE inference by eliminating complex training pipelines and ML expertise requirements while delivering superior performance compared to existing methods, demonstrating the effectiveness of foundation models for scientific inference tasks.

Abstract: Ordinary differential equations (ODEs) are central to scientific modelling, but inferring their vector fields from noisy trajectories remains challenging. Current approaches such as symbolic regression, Gaussian process (GP) regression, and Neural ODEs often require complex training pipelines and substantial machine learning expertise, or they depend strongly on system-specific prior knowledge. We propose FIM-ODE, a pretrained Foundation Inference Model that amortises low-dimensional ODE inference by predicting the vector field directly from noisy trajectory data in a single forward pass. We pretrain FIM-ODE on a prior distribution over ODEs with low-degree polynomial vector fields and represent the target field with neural operators. FIM-ODE achieves strong zero-shot performance, matching and often improving upon ODEFormer, a recent pretrained symbolic baseline, across a range of regimes despite using a simpler pretraining prior distribution. Pretraining also provides a strong initialisation for finetuning, enabling fast and stable adaptation that outperforms modern neural and GP baselines without requiring machine learning expertise.

</details>


### [199] [Redundancy-Free View Alignment for Multimodal Human Activity Recognition with Arbitrarily Missing Views](https://arxiv.org/abs/2602.08755)
*Duc-Anh Nguyen,Nhien-An Le-Khac*

Main category: cs.LG

TL;DR: Proposes RALIS, a model for flexible multimodal multiview human activity recognition that handles arbitrary view configurations via contrastive learning with reduced O(V) complexity and a mixture-of-experts module.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal multiview learning methods struggle with flexible view configurations (arbitrary combinations, varying numbers, heterogeneous modalities), limiting practical deployment where view availability is dynamic.

Method: Combines multiview contrastive learning using an adjusted center contrastive loss for view alignment with a mixture-of-experts module featuring specialized load balancing to adapt to arbitrary view combinations during training and inference.

Result: Reduces computational complexity from O(V²) to O(V), mitigates missing view impact, and validates on four datasets (3-9 views) demonstrating strong performance and flexibility across inertial and pose modalities.

Conclusion: RALIS successfully enables flexible, efficient multimodal multiview learning with arbitrary view availability while maintaining competitive performance.

Abstract: Multimodal multiview learning seeks to integrate information from diverse sources to enhance task performance. Existing approaches often struggle with flexible view configurations, including arbitrary view combinations, numbers of views, and heterogeneous modalities. Focusing on the context of human activity recognition, we propose RALIS, a model that combines multiview contrastive learning with a mixture-of-experts module to support arbitrary view availability during both training and inference. Instead of trying to reconstruct missing views, an adjusted center contrastive loss is used for self-supervised representation learning and view alignment, mitigating the impact of missing views on multiview fusion. This loss formulation allows for the integration of view weights to account for view quality. Additionally, it reduces computational complexity from $O(V^2)$ to $O(V)$, where $V$ is the number of views. To address residual discrepancies not captured by contrastive learning, we employ a mixture-of-experts module with a specialized load balancing strategy, tasked with adapting to arbitrary view combinations. We highlight the geometric relationship among components in our model and how they combine well in the latent space. RALIS is validated on four datasets encompassing inertial and human pose modalities, with the number of views ranging from three to nine, demonstrating its performance and flexibility.

</details>


### [200] [HoGS: Homophily-Oriented Graph Synthesis for Local Differentially Private GNN Training](https://arxiv.org/abs/2602.08762)
*Wen Xu,Zhetao Li,Yong Xiao,Pengpeng Qiao,Mianxiong Dong,Kaoru Ota*

Main category: cs.LG

TL;DR: 提出HoGS框架，通过合成图生成技术在本地差分隐私保护下同时实现GNN的链接与特征隐私保护，显著提升模型效用


<details>
  <summary>Details</summary>
Motivation: 现有LDP-GNN方案或仅保护链接隐私，或在保护链接和节点特征时导致严重效用损失，无法平衡隐私保护与模型性能

Method: 采用两阶段方法：先LDP收集图链接和特征信息，再利用图同质性现象分别重建图结构和节点特征，生成合成图用于GNN训练

Result: 在三个真实数据集上，HoGS训练的GNN准确率显著优于基线方法，有效缓解LDP带来的效用损失

Conclusion: HoGS为同时保护图数据中链接与特征隐私提供了高效解决方案，通过合成图重建机制在隐私保障下维持了GNN的高性能

Abstract: Graph neural networks (GNNs) have demonstrated remarkable performance in various graph-based machine learning tasks by effectively modeling high-order interactions between nodes. However, training GNNs without protection may leak sensitive personal information in graph data, including links and node features. Local differential privacy (LDP) is an advanced technique for protecting data privacy in decentralized networks. Unfortunately, existing local differentially private GNNs either only preserve link privacy or suffer significant utility loss in the process of preserving link and node feature privacy. In this paper, we propose an effective LDP framework, called HoGS, which trains GNNs with link and feature protection by generating a synthetic graph. Concretely, HoGS first collects the link and feature information of the graph under LDP, and then utilizes the phenomenon of homophily in graph data to reconstruct the graph structure and node features separately, thereby effectively mitigating the negative impact of LDP on the downstream GNN training. We theoretically analyze the privacy guarantee of HoGS and conduct experiments using the generated synthetic graph as input to various state-of-the-art GNN architectures. Experimental results on three real-world datasets show that HoGS significantly outperforms baseline methods in the accuracy of training GNNs.

</details>


### [201] [Efficient Deep Learning for Biometrics: Overview, Challenges and Trends in Ear of Frugal AI](https://arxiv.org/abs/2602.08809)
*Karim Haroun,Aya Zitouni,Aicha Zenakhri,Meriem Amel Guessoum,Larbi Boubchir*

Main category: cs.LG

TL;DR: 本文针对生物识别应用，调研高效深度学习方法。面对深度学习高能耗、重碳足迹及边缘设备部署限制等挑战，提出方法分类体系，倡导内存、延迟等综合评估指标，并展望未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 深度学习计算需求增长导致高能耗与碳足迹问题，阻碍其在生物识别等安全领域的广泛应用与边缘设备部署，亟需高效化研究。

Method: 通过文献调研，提供高效深度学习方法分类学，并探讨超越准确率的多维度评估指标。

Result: 构建高效深度学习分类框架，提出通用可复现评估标准，指明未来研究路径。

Conclusion: 强调生物识别高效深度学习的重要性，呼吁建立标准化评估体系，推动领域可持续发展。

Abstract: Recent advances in deep learning, whether on discriminative or generative tasks have been beneficial for various applications, among which security and defense. However, their increasing computational demands during training and deployment translates directly into high energy consumption. As a consequence, this induces a heavy carbon footprint which hinders their widespread use and scalability, but also a limitation when deployed on resource-constrained edge devices for real-time use. In this paper, we briefly survey efficient deep learning methods for biometric applications. Specifically, we tackle the challenges one might incur when training and deploying deep learning approaches, and provide a taxonomy of the various efficient deep learning families. Additionally, we discuss complementary metrics for evaluating the efficiency of these models such as memory, computation, latency, throughput, and advocate for universal and reproducible metrics for better comparison. Last, we give future research directions to consider.

</details>


### [202] [Robust Policy Optimization to Prevent Catastrophic Forgetting](https://arxiv.org/abs/2602.08813)
*Mahdi Sabbaghi,George Pappas,Adel Javanmard,Hamed Hassani*

Main category: cs.LG

TL;DR: 针对LLM多阶段训练中灾难性遗忘问题，提出FRPO框架，通过KL邻域内的max-min优化提升策略鲁棒性，在保持下游性能的同时显著减少安全性退化。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多阶段后训练（先RLHF再下游微调）中面临灾难性遗忘问题，即微调会损害先前学习的安全行为。标准RLHF不能保证对未来适应的鲁棒性，现有方法多关注下游阶段的保护，而作者认为需要在预微调阶段就建立鲁棒性。

Method: 提出Fine-tuning Robust Policy Optimization (FRPO)框架，优化当前策略及其KL边界可达邻域内的最小奖励（max-min公式），通过修改GRPO算法实现，不增加额外计算开销。

Result: 在多个基础模型和下游微调方案（SFT和RL）上，FRPO显著减少了安全性退化，同时保持下游任务性能；在数学RL场景中也验证了其有效性。

Conclusion: 预微调鲁棒性是必要的，FRPO通过使基础策略避免脆弱的奖励解，为未来适应提供了更鲁棒的解决方案。

Abstract: Large language models are commonly trained through multi-stage post-training: first via RLHF, then fine-tuned for other downstream objectives. Yet even small downstream updates can compromise earlier learned behaviors (e.g., safety), exposing a brittleness known as catastrophic forgetting. This suggests standard RLHF objectives do not guarantee robustness to future adaptation. To address it, most prior work designs downstream-time methods to preserve previously learned behaviors. We argue that preventing this requires pre-finetuning robustness: the base policy should avoid brittle high-reward solutions whose reward drops sharply under standard fine-tuning.
  We propose Fine-tuning Robust Policy Optimization (FRPO), a robust RLHF framework that optimizes reward not only at the current policy, but across a KL-bounded neighborhood of policies reachable by downstream adaptation. The key idea is to ensure reward stability under policy shifts via a max-min formulation. By modifying GRPO, we develop an algorithm with no extra computation, and empirically show it substantially reduces safety degradation across multiple base models and downstream fine-tuning regimes (SFT and RL) while preserving downstream task performance. We further study a math-focused RL setting, demonstrating that FRPO preserves accuracy under subsequent fine-tuning.

</details>


### [203] [FlexMoRE: A Flexible Mixture of Rank-heterogeneous Experts for Efficient Federatedly-trained Large Language Models](https://arxiv.org/abs/2602.08818)
*Annemette Brok Pirchert,Jacob Nielsen,Mogens Henrik From,Lukas Galke Poech,Peter Schneider-Kamp*

Main category: cs.LG

TL;DR: 作者提出 FlexMoRE，一个灵活的混合专家架构，允许全尺寸专家和不同秩的适配器共存。通过系统研究专家秩与下游任务性能的关系，发现推理任务需要更高秩而知识任务需要较低秩。使用最优秩配置，FlexMoRE 比基线性能提升 1.7 分（47.18 vs 45.46），同时参数减少至三分之一以下（10.75B vs 33.27B）。


<details>
  <summary>Details</summary>
Motivation: 当前混合专家架构中的专家都是全尺寸模型，可能在某些领域存在冗余。作者假设低秩适配器可能就足够了，这能提高参数效率。

Method: 提出 FlexMoRE 架构，支持全尺寸专家和不同秩的适配器混合使用。基于 FlexOlmo 的预训练专家转换为低秩版本，系统评估 6 个专家（秩从 2^0 到 2^14）组成的 150 种混合配置，在 120 个任务上进行测试。

Result: 通过回归分析发现，推理密集型基准测试的最佳秩明显高于知识密集型基准测试。使用最优秩配置，FlexMoRE 在 120 个任务上平均得分 47.18，优于 FlexOlmo 的 45.46，参数量从 33.27B 降至 10.75B。

Conclusion: 低秩适配器在混合专家架构中是可行的，且能根据任务类型调整专家秩来平衡性能和效率。FlexMoRE 展示了参数效率与性能可以同时提升，为高效的多领域模型设计提供了新思路。

Abstract: Recent advances in mixture-of-experts architectures have shown that individual experts models can be trained federatedly, i.e., in isolation from other experts by using a common base model to facilitate coordination. However, we hypothesize that full-sized experts may not be necessary for all domains and that instead low-rank adapters may be sufficient. Here, we introduce FlexMoRE, a Flexible Mixture of Rank-heterogenous Experts, which may be either full-sized experts or adapters of a suitable rank. We systematically investigate the trade-off between expert rank and downstream task performance by evaluating $6$ experts with ranks $2^0$ to $2^{14}$ resulting in experiments covering 150 mixtures (96 with 2 experts, 54 with 7 experts) that are evaluated across $120$ tasks. For our experiments, we build on FlexOlmo and turn its pre-trained experts into low-rank versions. Our regression analysis from expert rank to downstream task performance reveals that the best-performing rank is substantially higher for reasoning-heavy benchmarks than for knowledge-heavy benchmarks. These findings on rank sensitivity come with direct implications for memory efficiency: Using optimal ranks, FlexMoRE yields improved downstream task performance (average score $47.18$) compared to the baseline FlexOlmo-style mixture of full-sized experts (average score $45.46$) at less than one third the parameters ($10.75$B for FlexMoRE vs. $33.27$B for FlexOlmo). All code will be made available.

</details>


### [204] [Bayesian Preference Learning for Test-Time Steerable Reward Models](https://arxiv.org/abs/2602.08819)
*Jiwoo Hong,Shao Tang,Zhipeng Wang*

Main category: cs.LG

TL;DR: 提出变分上下文奖励建模(ICRM)方法，通过贝叶斯推断和上下文示例实现测试时奖励模型可调，在单目标和多目标设置下显著提升性能，并提供理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有分类器奖励模型训练后静态不变，难以适应测试时新偏好分布，尤其随着RL应用于可验证奖励和多目标对齐等复杂场景，需要更灵活的奖励建模方法。

Method: 将奖励建模视为Bradley-Terry模型下的潜在偏好概率的摊销变分推断，使用共轭Beta先验，通过上下文偏好演示实现测试时可调节性。

Result: 在单目标设置下，SafeRLHF准确率提升34%，RM-Bench提升9%；多目标设置下Pareto前沿超体积提升4%；在数学推理中编码可验证奖励优于传统方法。

Conclusion: ICRM成功实现奖励模型的测试时自适应，在多项基准上取得显著改进，具有实际应用价值，且变分目标具有全局最优解理论保证，KL正则化可缓解奖励过优化。

Abstract: Reward models are central to aligning language models with human preferences via reinforcement learning (RL). As RL is increasingly applied to settings such as verifiable rewards and multi-objective alignment, RMs are expected to encode more complex and multifaceted preference distributions. However, classifier RMs remain static once trained, limiting their adaptability at test time. We propose Variational In-Context Reward Modeling (ICRM), a novel Bayesian reward modeling objective that enables test-time steerability via in-context preference demonstrations. ICRM casts reward modeling as amortized variational inference over a latent preference probability under the Bradley-Terry model using a conjugate Beta prior. We show that ICRM adapt to unseen preference distributions at test time for both single and multi-objective settings. With more in-context demonstrations, ICRM gains 34% accuracy on SafeRLHF and 9% accuracy on RM-Bench in the single-objective setting, while widening the Pareto frontier with a 4% gain in hypervolume on helpfulness and refusal benchmarks. We further study the practical applicability of ICRM for RL training, showing that it can effectively encode verifiable rewards by outperforming a conventional RM in math reasoning. Finally, we provide theoretical guarantees that the variational objective admits a global interior optimum with finite confidence, and we analyze how KL regularization mitigates reward over-optimization.

</details>


### [205] [Rethinking Graph Generalization through the Lens of Sharpness-Aware Minimization](https://arxiv.org/abs/2602.08855)
*Yang Qiu,Yixiong Zou,Jun Wang*

Main category: cs.LG

TL;DR: Proposes an energy-driven generative augmentation framework (E2A) to address Minimal Shift Flip in GNNs by modeling loss landscape sharpness through a tractable energy-based formulation correlated with local robustness.


<details>
  <summary>Details</summary>
Motivation: Graph Neural Networks are highly sensitive to distribution shifts, specifically Minimal Shift Flip where test samples slightly deviating from training distribution are abruptly misclassified due to weakened local stability and increasingly sharp loss landscapes during training.

Method: Introduces Local Robust Radius to quantify loss sharpness, establishes theoretical link to generalization, and develops an energy-based formulation monotonically correlated with robustness. Proposes E2A framework using energy-guided latent perturbations to generate pseudo-OOD samples.

Result: Extensive experiments across multiple benchmarks demonstrate that E2A consistently improves graph OOD generalization and outperforms state-of-the-art baselines.

Conclusion: The energy-based formulation provides a tractable and principled objective for modeling flatness and stability, effectively solving MSF and enhancing GNN generalization.

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success across various graph-based tasks but remain highly sensitive to distribution shifts. In this work, we focus on a prevalent yet under-explored phenomenon in graph generalization, Minimal Shift Flip (MSF),where test samples that slightly deviate from the training distribution are abruptly misclassified. To interpret this phenomenon, we revisit MSF through the lens of Sharpness-Aware Minimization (SAM), which characterizes the local stability and sharpness of the loss landscape while providing a theoretical foundation for modeling generalization error. To quantify loss sharpness, we introduce the concept of Local Robust Radius, measuring the smallest perturbation required to flip a prediction and establishing a theoretical link between local stability and generalization. Building on this perspective, we further observe a continual decrease in the robust radius during training, indicating weakened local stability and an increasingly sharp loss landscape that gives rise to MSF. To jointly solve the MSF phenomenon and the intractability of radius, we develop an energy-based formulation that is theoretically proven to be monotonically correlated with the robust radius, offering a tractable and principled objective for modeling flatness and stability. Building on these insights, we propose an energy-driven generative augmentation framework (E2A) that leverages energy-guided latent perturbations to generate pseudo-OOD samples and enhance model generalization. Extensive experiments across multiple benchmarks demonstrate that E2A consistently improves graph OOD generalization, outperforming state-of-the-art baselines.

</details>


### [206] [Magnitude Distance: A Geometric Measure of Dataset Similarity](https://arxiv.org/abs/2602.08859)
*Sahel Torkamani,Henry Gouk,Rik Sarkar*

Main category: cs.LG

TL;DR: 提出"magnitude distance"这一新型数据集距离度量，引入可调尺度参数t控制全局与局部结构的敏感性，理论证明其性质，并成功应用于生成模型训练。


<details>
  <summary>Details</summary>
Motivation: 量化数据集之间的距离是数学和机器学习中的基础问题，经典距离度量在高维场景中可能失去判别力。

Method: 基于度量空间的magnitude概念，提出magnitude distance，包含可调参数t；证明其理论性质（极限行为、度量性质条件）；将其用作push-forward生成模型的训练目标。

Result: 理论分析证实该距离在适当调参下可在高维设置中保持判别力；实验表明其提供与现有距离基生成方法相当的信号质量。

Conclusion: magnitude distance是一种有意义的距离度量，兼具理论保证和实用价值，特别适用于高维场景和生成模型。

Abstract: Quantifying the distance between datasets is a fundamental question in mathematics and machine learning. We propose \textit{magnitude distance}, a novel distance metric defined on finite datasets using the notion of the \emph{magnitude} of a metric space. The proposed distance incorporates a tunable scaling parameter, $t$, that controls the sensitivity to global structure (small $t$) and finer details (large $t$). We prove several theoretical properties of magnitude distance, including its limiting behavior across scales and conditions under which it satisfies key metric properties. In contrast to classical distances, we show that magnitude distance remains discriminative in high-dimensional settings when the scale is appropriately tuned. We further demonstrate how magnitude distance can be used as a training objective for push-forward generative models. Our experimental results support our theoretical analysis and demonstrate that magnitude distance provides meaningful signals, comparable to established distance-based generative approaches.

</details>


### [207] [Near-optimal Swap Regret Minimization for Convex Losses](https://arxiv.org/abs/2602.08862)
*Lunjia Hu,Jon Schneider,Yifan Wu*

Main category: cs.LG

TL;DR: 该论文提出一种随机在线算法，实现了在单位区间上针对任意自适应选择的Lipschitz凸损失函数的近最优O(√T)期望交换遗憾界，优于先前O(T^(2/3))的结果。算法通过新颖的多尺度分箱技术，在多项式时间内运行，并将结果推广至一般可提取属性的校准误差最小化，首次实现了中位数校准的O(√T)误差保证。


<details>
  <summary>Details</summary>
Motivation: 解决在线凸优化中的遗憾界优化问题，并消除先前校准误差工作对识别函数Lipschitz性的假设限制，以扩展算法适用范围。

Method: 多尺度分箱：将单位区间按多个粒度层次离散化为分箱，并同时利用所有尺度进行随机化预测。

Result: 1) 期望交换遗憾界达到O(√T)，优于先前O(T^(2/3))；2) 算法运行时间为poly(T)；3) 首次实现中位数校准的O(√T)误差保证；4) 推广至一般可提取属性的校准问题。

Conclusion: 多尺度分箱技术有效实现了最优遗憾界与计算效率的平衡，扩展了校准问题的适用范围，解决了Fishelson等人的开放性问题。

Abstract: We give a randomized online algorithm that guarantees near-optimal $\widetilde O(\sqrt T)$ expected swap regret against any sequence of $T$ adaptively chosen Lipschitz convex losses on the unit interval. This improves the previous best bound of $\widetilde O(T^{2/3})$ and answers an open question of Fishelson et al. [2025b]. In addition, our algorithm is efficient: it runs in $\mathsf{poly}(T)$ time. A key technical idea we develop to obtain this result is to discretize the unit interval into bins at multiple scales of granularity and simultaneously use all scales to make randomized predictions, which we call multi-scale binning and may be of independent interest. A direct corollary of our result is an efficient online algorithm for minimizing the calibration error for general elicitable properties. This result does not require the Lipschitzness assumption of the identification function needed in prior work, making it applicable to median calibration, for which we achieve the first $\widetilde O(\sqrt T)$ calibration error guarantee.

</details>


### [208] [Stress-Testing Alignment Audits With Prompt-Level Strategic Deception](https://arxiv.org/abs/2602.08877)
*Oliver Daniels,Perusha Moodley,Ben Marlin,David Lindner*

Main category: cs.LG

TL;DR: 研究人员开发了一个自动红队测试流程，通过生成针对性的欺骗策略（系统提示词），对现有的对齐审计方法（包括助手预填充、用户角色采样、稀疏自动编码器和token嵌入相似度方法）进行压力测试。实验首次证明了基于激活值的策略性欺骗的存在，并发现当前的黑盒和白盒审计方法在面对足够强大的未对齐模型时都不具备鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的对齐审计方法虽然旨在识别具有隐藏目标的战略性未对齐模型，但尚未系统性地针对欺骗策略进行压力测试，这留下了安全评估的盲点。

Method: 开发自动红队管道，为特定的白盒和黑盒审计方法生成定制化的欺骗策略（系统提示词），并使用"保密模型生物体"进行实验测试。

Result: 该管道成功找到了能够欺骗黑盒和白盒审计方法的提示词，使其产生自信但错误的判断。这是首个关于基于激活值的策略性欺骗的文档化证据。

Conclusion: 当前的黑盒和白盒对齐审计方法在面对足够强大的未对齐模型时缺乏鲁棒性，需要开发更强大的防御机制。

Abstract: Alignment audits aim to robustly identify hidden goals from strategic, situationally aware misaligned models. Despite this threat model, existing auditing methods have not been systematically stress-tested against deception strategies. We address this gap, implementing an automatic red-team pipeline that generates deception strategies (in the form of system prompts) tailored to specific white-box and black-box auditing methods. Stress-testing assistant prefills, user persona sampling, sparse autoencoders, and token embedding similarity methods against secret-keeping model organisms, our automatic red-team pipeline finds prompts that deceive both the black-box and white-box methods into confident, incorrect guesses. Our results provide the first documented evidence of activation-based strategic deception, and suggest that current black-box and white-box methods would not be robust to a sufficiently capable misaligned model.

</details>


### [209] [Discrete Bridges for Mutual Information Estimation](https://arxiv.org/abs/2602.08894)
*Iryna Zabarianska,Sergei Kholkin,Grigoriy Ksenofontov,Ivan Butakov,Alexander Korotin*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散桥模型的离散互信息估计器(DBMI)，通过将互信息估计建模为域转移问题，有效解决了离散数据的互信息估计难题。


<details>
  <summary>Details</summary>
Motivation: 互信息估计在机器学习和信息论中具有重要意义，但传统估计器在处理离散数据时面临困难，因此需要开发适用于离散空间的专用估计方法。

Method: 利用离散状态空间的桥匹配模型，将互信息估计巧妙地转化为域转移问题，构建了离散桥互信息(DBMI)估计器。

Result: 在低维度和基于图像的互信息估计两个设定上验证了所提出DBMI估计器的性能。

Conclusion: DBMI估计器为离散数据的互信息估计提供了有效解决方案，在多种场景下表现出良好的性能，拓展了扩散桥模型的应用范围。

Abstract: Diffusion bridge models in both continuous and discrete state spaces have recently become powerful tools in the field of generative modeling. In this work, we leverage the discrete state space formulation of bridge matching models to address another important problem in machine learning and information theory: the estimation of the mutual information (MI) between discrete random variables. By neatly framing MI estimation as a domain transfer problem, we construct a Discrete Bridge Mutual Information (DBMI) estimator suitable for discrete data, which poses difficulties for conventional MI estimators. We showcase the performance of our estimator on two MI estimation settings: low-dimensional and image-based.

</details>


### [210] [GSS: Gated Subspace Steering for Selective Memorization Mitigation in LLMs](https://arxiv.org/abs/2602.08901)
*Xuanqi Zhang,Haoyang Shang,Xiaoxiao Li*

Main category: cs.LG

TL;DR: 提出GSS方法，通过上下文感知的干预机制选择性缓解LLMs的记忆化问题，在保持性能的同时大幅降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在记忆化倾向，会逐字重现训练数据，损害泛化能力和隐私性。现有均匀干预方法会过度影响正常泛化的token，需要更精准的缓解策略。

Method: 提出Gated Subspace Steering (GSS)选择性记忆化缓解方法，通过探针检测记忆化相关激活，仅在超过阈值时应用针对性修正。基于最优子空间转向建立原则性优化框架，实现上下文感知的动态干预。

Result: 在四个基准测试中，GSS达到或超过当前最优的记忆化减少效果，同时计算需求比基于优化的替代方案低100-1000倍。实证显示记忆化具有稀疏性、间歇性和token条件性特征。

Conclusion: GSS为记忆化缓解提供了高效实用的解决方案，同时揭示了神经表示中记忆化的几何特性，为未来研究提供了新的理论视角。

Abstract: Large language models (LLMs) can memorize and reproduce training sequences verbatim -- a tendency that undermines both generalization and privacy. Existing mitigation methods apply interventions uniformly, degrading performance on the majority of tokens that generalize normally. We show empirically that memorization is sparse, intermittent, and token-conditioned, suggesting that effective mitigation requires context-aware intervention rather than static parameter modification. To this end, we propose a novel and effective selective memorization mitigation method -- Gated Subspace Steering (GSS), which decomposes intervention into a probe (detecting memorization-relevant activations) and a steer (applying targeted correction only when the probe exceeds a threshold). The optimal probe-steer pair emerges from a principled optimization framework based on optimal subspace steering. Experiments on four benchmarks show GSS matches or exceeds state-of-the-art memorization reduction while requiring $100-1000 \times$ less compute than optimization-based alternatives. Furthermore, we provide new theoretical insights into the geometry of memorization in neural representations.

</details>


### [211] [Positive Distribution Shift as a Framework for Understanding Tractable Learning](https://arxiv.org/abs/2602.08907)
*Marko Medvedev,Idan Attias,Elisabetta Cornacchia,Theodor Misiakiewicz,Gal Vardi,Nathan Srebro*

Main category: cs.LG

TL;DR: 该论文提出"正向分布偏移"(PDS)概念，认为精心设计的训练分布偏移能使学习计算上更容易，将某些难解问题转化为可用标准梯度方法求解的问题，并建立了与成员查询学习的联系。


<details>
  <summary>Details</summary>
Motivation: 传统文献将训练-测试分布偏移视为负面影响，但本文论证了通过策略性选择训练分布D'(x)，这种偏移可转化为学习优势。这呼应了现代机器学习中创新更多来自设计优质训练分布而非改进算法的趋势。

Method: 形式化定义正向分布偏移(PDS)的不同变体；证明在PDS下，某些计算上困难的概念类变得易于学习；建立PDS与成员查询学习(membership query learning)的理论联系。

Result: PDS带来的益处主要是计算层面的而非统计层面的；在PDS框架下，计算困难的学习问题可通过标准梯度训练变得可解；展示了特定困难函数类在正向偏移下的易学性。

Conclusion: 分布偏移应被视为可主动利用的积极学习工具；PDS提供了理解如何通过精心选择训练分布来转化难解问题为可解问题的理论框架，重塑了对机器学习中数据分布角色的理解。

Abstract: We study a setting where the goal is to learn a target function f(x) with respect to a target distribution D(x), but training is done on i.i.d. samples from a different training distribution D'(x), labeled by the true target f(x). Such a distribution shift (here in the form of covariate shift) is usually viewed negatively, as hurting or making learning harder, and the traditional distribution shift literature is mostly concerned with limiting or avoiding this negative effect. In contrast, we argue that with a well-chosen D'(x), the shift can be positive and make learning easier -- a perspective called Positive Distribution Shift (PDS). Such a perspective is central to contemporary machine learning, where much of the innovation is in finding good training distributions D'(x), rather than changing the training algorithm. We further argue that the benefit is often computational rather than statistical, and that PDS allows computationally hard problems to become tractable even using standard gradient-based training. We formalize different variants of PDS, show how certain hard classes are easily learnable under PDS, and make connections with membership query learning.

</details>


### [212] [GEMSS: A Variational Bayesian Method for Discovering Multiple Sparse Solutions in Classification and Regression Problems](https://arxiv.org/abs/2602.08913)
*Kateřina Henclová,Václav Šmídl*

Main category: cs.LG

TL;DR: GEMSS is a variational Bayesian framework that discovers multiple diverse sparse feature combinations simultaneously in high-dimensional underdetermined problems, outperforming conventional single-solution methods.


<details>
  <summary>Details</summary>
Motivation: In underdetermined (n << p) and highly correlated data regimes, multiple distinct sparse feature subsets may explain responses equally well, but conventional methods only isolate a single solution, obscuring the full spectrum of plausible explanations needed for domain-specific insights.

Method: GEMSS employs a structured spike-and-slab prior for sparsity, uses a mixture of Gaussians to approximate the multimodal posterior, incorporates a Jaccard-based penalty for solution diversity, and optimizes the entire ensemble via stochastic gradient descent within a single objective function.

Result: Validated on 128 synthetic experiments, GEMSS scales to p=5000 with n=50, handles continuous targets, missing data, class imbalance and Gaussian noise robustly, and is available as a Python package with a no-code application.

Conclusion: GEMSS successfully addresses the fundamental challenge of identifying multiple equally valid sparse explanations in high-dimensional data, providing a comprehensive and scalable tool for interpretable feature selection.

Abstract: Selecting interpretable feature sets in underdetermined ($n \ll p$) and highly correlated regimes constitutes a fundamental challenge in data science, particularly when analyzing physical measurements. In such settings, multiple distinct sparse subsets may explain the response equally well. Identifying these alternatives is crucial for generating domain-specific insights into the underlying mechanisms, yet conventional methods typically isolate a single solution, obscuring the full spectrum of plausible explanations.
  We present GEMSS (Gaussian Ensemble for Multiple Sparse Solutions), a variational Bayesian framework specifically designed to simultaneously discover multiple, diverse sparse feature combinations. The method employs a structured spike-and-slab prior for sparsity, a mixture of Gaussians to approximate the intractable multimodal posterior, and a Jaccard-based penalty to further control solution diversity. Unlike sequential greedy approaches, GEMSS optimizes the entire ensemble of solutions within a single objective function via stochastic gradient descent.
  The method is validated on a comprehensive benchmark comprising 128 synthetic experiments across classification and regression tasks. Results demonstrate that GEMSS scales effectively to high-dimensional settings ($p=5000$) with sample size as small as $n = 50$, generalizes seamlessly to continuous targets, handles missing data natively, and exhibits remarkable robustness to class imbalance and Gaussian noise.
  GEMSS is available as a Python package 'gemss' at PyPI. The full GitHub repository at https://github.com/kat-er-ina/gemss/ also includes a free, easy-to-use application suitable for non-coders.

</details>


### [213] [Diffusion-Inspired Reconfiguration of Transformers for Uncertainty Calibration](https://arxiv.org/abs/2602.08920)
*Manh Cuong Dao,Quang Hung Pham,Phi Le Nguyen,Thao Nguyen Truong,Bryan Kian Hsiang Low,Trong Nghia Hoang*

Main category: cs.LG

TL;DR: 提出扩散启发的Transformer重构方法，将每个特征变换块建模为概率映射，构建概率路径并重新编译到扩散过程，实现预训练模型中表征不确定性的原则性传播，同时保持预测性能，在视觉和语言任务上实现更优的校准和准确性


<details>
  <summary>Details</summary>
Motivation: 现有预训练Transformer缺乏原则性的不确定性传播机制，在风险敏感应用中可靠性不足，需要在不牺牲预测性能的前提下实现有效的校准

Method: 受扩散模型启发，将Transformer特征变换块重构为概率映射，通过组合形成类似扩散过程的"概率路径"，并重新编译到统一过渡模型的扩散过程中，实现不确定性在整个模型架构中的原则性传播

Result: 在多种视觉和语言基准测试中，该方法相比现有不确定性感知Transformer实现了更优的校准效果和预测准确性

Conclusion: 所提方法成功解决了预训练Transformer的不确定性传播问题，为高风险应用提供了更可靠的部署方案，同时保持了原始模型的预测性能

Abstract: Uncertainty calibration in pre-trained transformers is critical for their reliable deployment in risk-sensitive applications. Yet, most existing pre-trained transformers do not have a principled mechanism for uncertainty propagation through their feature transformation stack. In this work, we propose a diffusion-inspired reconfiguration of transformers in which each feature transformation block is modeled as a probabilistic mapping. Composing these probabilistic mappings reveals a probability path that mimics the structure of a diffusion process, transporting data mass from the input distribution to the pre-trained feature distribution. This probability path can then be recompiled on a diffusion process with a unified transition model to enable principled propagation of representation uncertainty throughout the pre-trained model's architecture while maintaining its original predictive performance. Empirical results across a variety of vision and language benchmarks demonstrate that our method achieves superior calibration and predictive accuracy compared to existing uncertainty-aware transformers.

</details>


### [214] [DynamiQ: Accelerating Gradient Synchronization using Compressed Multi-hop All-reduce](https://arxiv.org/abs/2602.08923)
*Wenchen Han,Shay Vargaftik,Michael Mitzenmacher,Ran Ben Basat*

Main category: cs.LG

TL;DR: 提出DynamiQ量化框架，解决多跳聚合场景下的梯度压缩问题，通过优化部分和的表示和执行，在保持近BF16精度的同时实现高达34.2%的训练加速


<details>
  <summary>Details</summary>
Motivation: 大规模模型训练中，多跳all-reduce通信成为瓶颈，现有量化系统未针对多次部分求和的聚合拓扑进行优化，导致精度损失或性能下降

Method: DynamiQ框架创新性地结合量化最佳实践与多跳聚合需求，采用新型部分和技术和decompress-accumulate-recompress融合内核，并在PyTorch DDP中实现NCCL P2P支持

Result: 在多种大模型、任务和规模下，相比Omni-Reduce、THC和MXFP标准，性能提升最高达34.2%，是唯一能持续达到近BF16基线精度(99.9%)的方法

Conclusion: DynamiQ成功弥合量化实践与多跳聚合之间的鸿沟，在显著加速训练的同时保持模型精度，为大规模分布式训练提供了高效解决方案

Abstract: Multi-hop all-reduce is the de facto backbone of large model training. As the training scale increases, the network often becomes a bottleneck, motivating reducing the volume of transmitted data. Accordingly, recent systems demonstrated significant acceleration of the training process using gradient quantization. However, these systems are not optimized for multi-hop aggregation, where entries are partially summed multiple times along their aggregation topology.
  This paper presents DynamiQ, a quantization framework that bridges the gap between quantization best practices and multi-hop aggregation. DynamiQ introduces novel techniques to better represent partial sums, co-designed with a decompress-accumulate-recompress fused kernel to facilitate fast execution.
  We extended PyTorch DDP to support DynamiQ over NCCL P2P, and across different LLMs, tasks, and scales, we demonstrate consistent improvement of up to 34.2% over the best among state-of-the-art methods such as Omni-Reduce, THC, and emerging standards such as MXFP4, MXFP6, and MXFP8. Further, DynamiQ is the only evaluated method that consistently reaches near-baseline accuracy (e.g., 99.9% of the BF16 baseline) and does so while significantly accelerating the training.

</details>


### [215] [Distributionally Robust Optimization via Generative Ambiguity Modeling](https://arxiv.org/abs/2602.08976)
*Jiaqi Wen,Jianyi Yang*

Main category: cs.LG

TL;DR: This paper proposes GAS-DRO, a tractable Distributionally Robust Optimization method using generative model-based ambiguity sets, with proven convergence and superior OOD generalization when implemented with diffusion models.


<details>
  <summary>Details</summary>
Motivation: Distributionally Robust Optimization needs ambiguity sets that balance consistency with the nominal distribution, diversity for various scenarios, and computational tractability.

Method: The authors propose generative model-based ambiguity sets and GAS-DRO algorithm that solves the inner maximization over parameterized generative model space, implemented with diffusion models.

Result: The paper formally establishes stationary convergence of GAS-DRO and empirically demonstrates superior Out-of-Distribution generalization performance in machine learning tasks.

Conclusion: Generative ambiguity sets provide an effective framework for DRO, offering both theoretical guarantees and practical performance improvements for robust machine learning.

Abstract: This paper studies Distributionally Robust Optimization (DRO), a fundamental framework for enhancing the robustness and generalization of statistical learning and optimization. An effective ambiguity set for DRO must involve distributions that remain consistent to the nominal distribution while being diverse enough to account for a variety of potential scenarios. Moreover, it should lead to tractable DRO solutions. To this end, we propose generative model-based ambiguity sets that capture various adversarial distributions beyond the nominal support space while maintaining consistency with the nominal distribution. Building on this generative ambiguity modeling, we propose DRO with Generative Ambiguity Set (GAS-DRO), a tractable DRO algorithm that solves the inner maximization over the parameterized generative model space. We formally establish the stationary convergence performance of GAS-DRO. We implement GAS-DRO with a diffusion model and empirically demonstrate its superior Out-of-Distribution (OOD) generalization performance in ML tasks.

</details>


### [216] [DirMoE: Dirichlet-routed Mixture of Experts](https://arxiv.org/abs/2602.09001)
*Amirhossein Vahidi,Hesam Asadollahzadeh,Navid Akhavan Attar,Marie Moullet,Kevin Ly,Xingyi Yang,Mohammad Lotfollahi*

Main category: cs.LG

TL;DR: 提出DirMoE，一种基于Dirichlet变分自编码器的可微路由机制，通过解耦专家选择和贡献分配，用Gumbel-Sigmoid松弛和Dirichlet重参数化实现端到端训练，在控制稀疏性的同时提升专家专业化。


<details>
  <summary>Details</summary>
Motivation: 现有MoE路由使用不可微的Top-k+Softmax，导致性能和可扩展性受限，且将专家激活与贡献分配两个决策混为一谈。

Method: 构建Dirichlet-Routed MoE框架，用Bernoulli组件建模专家选择，Dirichlet组件处理贡献分配，结合Gumbel-Sigmoid松弛和隐式重参数化实现完全可微，通过ELBO目标函数和稀疏惩罚项控制激活专家数量。

Result: 在保持或超越其他方法性能的同时，显著提升了专家专业化程度，实现了可微、可控稀疏性的路由机制。

Conclusion: DirMoE通过概率建模解耦路由决策，为大规模MoE模型提供了更优的可微路由解决方案，有望提升训练效率和模型性能。

Abstract: Mixture-of-Experts (MoE) models have demonstrated exceptional performance in large-scale language models. Existing routers typically rely on non-differentiable Top-$k$+Softmax, limiting their performance and scalability. We argue that two distinct decisions, which experts to activate and how to distribute expert contributions among them, are conflated in standard Top-$k$+Softmax. We introduce Dirichlet-Routed MoE (DirMoE), a novel end-to-end differentiable routing mechanism built on a Dirichlet variational autoencoder framework. This design fundamentally disentangles the core routing problems: expert selection, modeled by a Bernoulli component, and expert contribution among chosen experts, handled by a Dirichlet component. The entire forward pass remains fully differentiable through the use of Gumbel-Sigmoid relaxation for the expert selection and implicit reparameterization for the Dirichlet distribution. Our training objective, a variational ELBO, includes a direct sparsity penalty that precisely controls the number of active experts in expectation, alongside a schedule for key hyperparameters that guides the model from an exploratory to a definitive routing state. Moreover, our DirMoE router matches or exceeds other methods while improving expert specialization.

</details>


### [217] [ShapeCond: Fast Shapelet-Guided Dataset Condensation for Time Series Classification](https://arxiv.org/abs/2602.09008)
*Sijia Peng,Yun Xiong,Xi Chen,Yi Xie,Guanzhi Li,Yanwei Yu,Yangyong Zhu,Zhiqiang Shen*

Main category: cs.LG

TL;DR: ShapeCond是一种用于时间序列分类的新型高效数据集压缩框架，通过shapelet引导的优化策略，在提高准确率的同时实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据快速增长给存储和计算带来巨大压力，而现有图像中心的压缩方法无法捕捉时间序列特有的时序结构和判别性局部模式（如shapelets），导致在时间序列任务上表现不佳。

Method: 提出ShapeCond框架，通过shapelet引导的优化策略进行数据集压缩，其合成成本与序列长度无关，能有效保留关键局部时序模式。

Result: 在Sleep数据集上比CondTSC快29倍，比朴素shapelet方法快达10,000倍；下游准确率更高，在大量实验中持续优于所有现有最先进的时间序列数据集压缩方法。

Conclusion: ShapeCond通过显式保留关键局部模式（shapelets），成功解决了时间序列数据压缩问题，在效率和准确性方面均显著优于现有方法，为大规模时间序列数据处理提供了有效解决方案。

Abstract: Time series data supports many domains (e.g., finance and climate science), but its rapid growth strains storage and computation. Dataset condensation can alleviate this by synthesizing a compact training set that preserves key information. Yet most condensation methods are image-centric and often fail on time series because they miss time-series-specific temporal structure, especially local discriminative motifs such as shapelets. In this work, we propose ShapeCond, a novel and efficient condensation framework for time series classification that leverages shapelet-based dataset knowledge via a shapelet-guided optimization strategy. Our shapelet-assisted synthesis cost is independent of sequence length: longer series yield larger speedups in synthesis (e.g., 29$\times$ faster over prior state-of-the-art method CondTSC for time-series condensation, and up to 10,000$\times$ over naively using shapelets on the Sleep dataset with 3,000 timesteps). By explicitly preserving critical local patterns, ShapeCond improves downstream accuracy and consistently outperforms all prior state-of-the-art time series dataset condensation methods across extensive experiments. Code is available at https://github.com/lunaaa95/ShapeCond.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [218] [ST-Raptor: An Agentic System for Semi-Structured Table QA](https://arxiv.org/abs/2602.07034)
*Jinxiu Qu,Zirui Tang,Hongzhang Huang,Boyu Niu,Wei Zhou,Jiannan Wang,Yitong Song,Guoliang Li,Xuanhe Zhou,Fan Wu*

Main category: cs.AI

TL;DR: ST-Raptor是一个用于半结构化表格问答的智能体系统，通过结合可视化编辑、树形结构建模和智能体驱动的查询，在准确性和可用性方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 半结构化表格问答任务需要精确提取单元格内容和位置，并恢复隐含的逻辑结构、层次关系和语义关联，而人工处理耗时耗力。现有方法存在局限：Text-to-SQL需转换为结构化格式导致信息丢失，Text-to-Code和多模态大语言模型在处理复杂布局时准确率不佳。

Method: 提出ST-Raptor智能体系统，提供交互式分析环境，整合可视化编辑、树形结构建模和智能体驱动的查询解析功能，支持准确易用的表格理解。

Result: 在基准数据集和真实数据集上的实验结果表明，ST-Raptor在准确性和可用性方面均超越现有方法。

Conclusion: ST-Raptor有效解决了半结构化表格问答的挑战，提供了用户友好且准确的解决方案，代码和演示视频已公开。

Abstract: Semi-structured table question answering (QA) is a challenging task that requires (1) precise extraction of cell contents and positions and (2) accurate recovery of key implicit logical structures, hierarchical relationships, and semantic associations encoded in table layouts. In practice, such tables are often interpreted manually by human experts, which is labor-intensive and time-consuming. However, automating this process remains difficult. Existing Text-to-SQL methods typically require converting semi-structured tables into structured formats, inevitably leading to information loss, while approaches like Text-to-Code and multimodal LLM-based QA struggle with complex layouts and often yield inaccurate answers. To address these limitations, we present ST-Raptor, an agentic system for semi-structured table QA. ST-Raptor offers an interactive analysis environment that combines visual editing, tree-based structural modeling, and agent-driven query resolution to support accurate and user-friendly table understanding. Experimental results on both benchmark and real-world datasets demonstrate that ST-Raptor outperforms existing methods in both accuracy and usability. The code is available at https://github.com/weAIDB/ST-Raptor, and a demonstration video is available at https://youtu.be/9GDR-94Cau4.

</details>


### [219] [DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents](https://arxiv.org/abs/2602.07035)
*Jiahao Zhao,Shaoxuan Xu,Zhongxiang Sun,Fengqi Zhu,Jingyang Ou,Yuling Shi,Chongxuan Li,Xiao Zhang,Jun Xu*

Main category: cs.AI

TL;DR: Proposes DLLM-Searcher: a framework using diffusion LLMs for efficient search agents via two-stage training (Agentic SFT + VRPO) and parallel P-ReAct paradigm, achieving ~15% speedup while matching LLM agent performance.


<details>
  <summary>Details</summary>
Motivation: Two key challenges: (1) High latency in ReAct agents due to serial execution; (2) Weak reasoning/tool-calling abilities of existing diffusion LLMs. Leveraging dLLMs' parallel decoding could optimize agent efficiency if these issues are solved.

Method: Two-stage post-training pipeline (Agentic SFT + Agentic VRPO) to enhance dLLM agent capabilities, plus novel P-ReAct paradigm that decodes tool calls first, enabling parallel "thinking while waiting" to reduce latency.

Result: DLLM-Searcher performs comparably to mainstream LLM-based search agents, with P-ReAct providing approximately 15% inference acceleration.

Conclusion: Successfully enables diffusion LLMs as competitive search agents by addressing ability and latency challenges, demonstrating practical efficiency gains over traditional serial agent paradigms.

Abstract: Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by their inherently parallel decoding mechanism and flexible generation paradigm. Meanwhile, despite the rapid advancement of Search Agents, their practical deployment is constrained by a fundamental limitation, termed as 1) Latency Challenge: the serial execution of multi-round reasoning, tool calling, and tool response waiting under the ReAct agent paradigm induces severe end-to-end latency. Intuitively, dLLMs can leverage their distinctive strengths to optimize the operational efficiency of agents under the ReAct agent paradigm. Practically, existing dLLM backbones face the 2) Agent Ability Challenge. That is, existing dLLMs exhibit remarkably weak reasoning and tool-calling capabilities, preventing these advantages from being effectively realized in practice. In this paper, we propose DLLM-Searcher, an optimization framework for dLLM-based Search Agents. To solve the Agent Ability Challenge, we design a two-stage post-training pipeline encompassing Agentic Supervised Fine-Tuning (Agentic SFT) and Agentic Variance-Reduced Preference Optimization Agentic VRPO, which enhances the backbone dLLM's information seeking and reasoning capabilities. To mitigate the Latency Challenge, we leverage the flexible generation mechanism of dLLMs and propose a novel agent paradigm termed Parallel-Reasoning and Acting P-ReAct. P-ReAct guides the model to prioritize decoding tool_call instructions, thereby allowing the model to keep thinking while waiting for the tool's return. Experimental results demonstrate that DLLM-Searcher achieves performance comparable to mainstream LLM-based search agents and P-ReAct delivers approximately 15% inference acceleration. Our code is available at https://anonymous.4open.science/r/DLLM-Searcher-553C

</details>


### [220] [Aster: Autonomous Scientific Discovery over 20x Faster Than Existing Methods](https://arxiv.org/abs/2602.07040)
*Emmett Bicker*

Main category: cs.AI

TL;DR: 介绍Aster，一个比现有框架快20倍的AI自主科学发现智能体，通过迭代优化程序在数学、GPU、生物、神经科学和语言模型等多元任务中取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有自主发现框架速度慢，难以处理长评估周期的科学问题。Aster旨在通过减少迭代次数，提升科学发现效率，扩展可处理问题范围。

Method: 基于迭代优化的程序改进框架：给定任务、初始程序和评估脚本，Aster通过迭代方式自动优化程序性能，减少达到目标所需的迭代次数。

Result: 在5个测试任务中，4个达到SOTA，1个以<1/190计算量匹配人类最优；整体速度快20倍；成功应用于Erdos问题、TriMul内核、单细胞分析、ZAPBench和NanoGPT等任务。

Conclusion: Aster证明了AI可高效实现跨学科自主科学发现，显著降低计算成本，使长周期评估问题变得可处理，为科学研究提供了新范式。

Abstract: We introduce Aster, an AI agent for autonomous scientific discovery capable of operating over 20 times faster than existing frameworks. Given a task, an initial program, and a script to evaluate the performance of the program, Aster iteratively improves the program, often leading to new state-of-the-art performances. Aster's significant reduction in the number of iterations required for novel discovery expands the domain of tractable problems to include tasks with long evaluation durations, such as multi-hour machine learning training runs.
  We applied Aster to problems in mathematics, GPU kernel engineering, biology, neuroscience, and language model training. More specifically: the Erdos minimum overlap problem, optimizing the TriMul kernel, a single-cell analysis denoising problem, training a neural activity prediction model to perform well on ZAPBench, and the NanoGPT Speedrun Competition. Aster attains SOTA results in every task, except for ZAPBench, where it matches the performance of the best human solution with less than 1/190th of the compute.
  Aster is accessible via a web interface and API at asterlab.ai.

</details>


### [221] [Theory of Space: Can Foundation Models Construct Spatial Beliefs through Active Exploration?](https://arxiv.org/abs/2602.07055)
*Pingyue Zhang,Zihan Huang,Yue Wang,Jieyu Zhang,Letian Xue,Zihan Wang,Qineng Wang,Keshigeyan Chandrasegaran,Ruohan Zhang,Yejin Choi,Ranjay Krishna,Jiajun Wu,Li Fei-Fei,Manling Li*

Main category: cs.AI

TL;DR: 本研究提出"空间理论"评估框架，通过空间信念探测揭示当前大模型在主动探索中存在显著性能差距、信念不稳定性与惯性问题。


<details>
  <summary>Details</summary>
Motivation: 多模态基础模型擅长被动感知，但其在部分可观测环境下的主动探索能力尚未被充分研究。空间具身智能要求智能体通过行动主动获取信息。

Method: 提出"空间理论"基准，通过好奇心驱动探索构建认知地图；创新性地引入空间信念探测技术，逐步揭示模型的内部空间表征。

Result: 发现四大瓶颈：主动-被动差距（自主信息收集性能显著下降）、探索效率低下、全局信念不稳定性导致空间知识退化、信念惯性（尤其视觉模型无法用新证据更新过时先验）。

Conclusion: 当前基础模型在主动探索中难以维持连贯且可修正的空间信念，感知瓶颈仅是初期问题，信念不稳定性与惯性是核心挑战。

Abstract: Spatial embodied intelligence requires agents to act to acquire information under partial observability. While multimodal foundation models excel at passive perception, their capacity for active, self-directed exploration remains understudied. We propose Theory of Space, defined as an agent's ability to actively acquire information through self-directed, active exploration and to construct, revise, and exploit a spatial belief from sequential, partial observations. We evaluate this through a benchmark where the goal is curiosity-driven exploration to build an accurate cognitive map. A key innovation is spatial belief probing, which prompts models to reveal their internal spatial representations at each step. Our evaluation of state-of-the-art models reveals several critical bottlenecks. First, we identify an Active-Passive Gap, where performance drops significantly when agents must autonomously gather information. Second, we find high inefficiency, as models explore unsystematically compared to program-based proxies. Through belief probing, we diagnose that while perception is an initial bottleneck, global beliefs suffer from instability that causes spatial knowledge to degrade over time. Finally, using a false belief paradigm, we uncover Belief Inertia, where agents fail to update obsolete priors with new evidence. This issue is present in text-based agents but is particularly severe in vision-based models. Our findings suggest that current foundation models struggle to maintain coherent, revisable spatial beliefs during active exploration.

</details>


### [222] [ANCHOR: Branch-Point Data Generation for GUI Agents](https://arxiv.org/abs/2602.07153)
*Jinbiao Wei,Yilun Zhao,Kangqi Ni,Arman Cohan*

Main category: cs.AI

TL;DR: 提出Anchor框架，通过从少量已验证的种子演示中扩展高质量、多样化的GUI交互轨迹，解决数据收集成本高和现有合成方法任务多样性有限或轨迹噪声大的问题，提升智能体在跨应用和操作系统上的性能。


<details>
  <summary>Details</summary>
Motivation: 收集人类演示数据成本高昂，现有合成数据生成方法存在任务多样性不足或轨迹噪声大、目标漂移的问题，难以满足端到端GUI智能体对高质量交互数据的需求。

Method: Anchor框架以种子演示为基础：识别有意义状态变化的分支点并生成基于当前GUI上下文的任务变体；由执行智能体生成新轨迹，通过状态感知检查和轨迹一致性验证任务完成；采用任务条件步骤过滤消除无根据操作，并对分支后段落实地去噪以保持意图连贯。

Result: 在OSWorld和WindowsAgentArena基准测试中，使用Anchor扩展语料库微调的模型相比零样本智能体和现有合成基线表现持续提升，且能泛化至不同应用和操作系统。

Conclusion: Anchor通过高效扩展小规模种子数据生成高质量监督信号，为训练桌面GUI智能体提供了可扩展的数据解决方案，显著提升模型性能与泛化能力。

Abstract: End-to-end GUI agents for real desktop environments require large amounts of high-quality interaction data, yet collecting human demonstrations is expensive and existing synthetic pipelines often suffer from limited task diversity or noisy, goal-drifting trajectories. We present a trajectory expansion framework Anchor that bootstraps scalable desktop supervision from a small set of verified seed demonstrations. Starting from each seed, we identify branch points that correspond to meaningful state changes and propose new, state-grounded task variants conditioned on the current GUI context. An executing agent then follows the proposed instructions to generate new trajectories, while a verifier enforces task completion via state-aware checks and trajectory-level consistency. To improve supervision quality, we further apply task-conditioned step-level filtering to remove ungrounded actions and denoise post-branch segments to maintain coherent intent. Experiments on standard desktop benchmarks, OSWorld and WindowsAgentArena, show that models fine-tuned on our expanded corpus achieve consistent improvements over zero-shot agents and representative synthesis baselines, and generalize across applications and operating systems.

</details>


### [223] [VERIFY-RL: Verifiable Recursive Decomposition for Reinforcement Learning in Mathematical Reasoning](https://arxiv.org/abs/2602.07559)
*Kaleem Ullah Qasim,Jiashu Zhang,Hao Li,Muhammad Kafeel Shaheen*

Main category: cs.AI

TL;DR: 提出名为Verify-RL的框架，利用符号微分创建可验证的数学问题分解，在难题上的准确率从32%提升至68%。


<details>
  <summary>Details</summary>
Motivation: 现有数学问题课程学习的启发式分解方法无法保证子问题更简单、有助于父任务或具有数学基础。

Method: 引入Verify-RL框架，要求分解满足三个可验证条件（结构复杂度递减、解包含、形式化规则推导），通过符号计算实现自动验证。

Result: 消除无效分解使难题准确率从32%翻倍至68%，整体相对提升40%。

Conclusion: 符号微分为验证分解提供了原则性方法，可验证分解能显著提升语言模型解决复杂数学问题的性能。

Abstract: Training language models to solve complex mathematical problems benefits from curriculum learning progressively training on simpler subproblems. However, existing decomposition methods are often heuristic, offering no guarantees that subproblems are simpler, that solving them aids the parent task, or that their relationships are mathematically grounded. We observe that symbolic differentiation provides a natural structure for verified decomposition: calculus rules explicitly define how expressions reduce to simpler components with provable properties. We introduce Verify-RL, a framework where every parent-child decomposition satisfies three verifiable conditions: strictly decreasing structural complexity, solution containment, and formal rule derivation. Unlike heuristic methods where a significant fraction of decompositions are invalid our properties admit automatic verification through symbolic computation, achieving "verification by construction" Experiments demonstrate that eliminating invalid decompositions yields sizable gains, accuracy on the hardest problems more than doubles from 32% to 68%, with a 40% relative improvement overall.

</details>


### [224] [PreFlect: From Retrospective to Prospective Reflection in Large Language Model Agents](https://arxiv.org/abs/2602.07187)
*Hanyu Wang,Yuanpu Cao,Lu Lin,Jinghui Chen*

Main category: cs.AI

TL;DR: PreFlect proposes prospective pre-execution reflection for LLM agents instead of traditional retrospective correction, using historical planning errors and dynamic re-planning to improve performance on complex real-world tasks.


<details>
  <summary>Details</summary>
Motivation: Existing LLM agent self-reflection is retrospective and reactive (act→fail→recover), which limits proactive error prevention and wastes resources on failed executions.

Method: PreFlect introduces a prospective reflection mechanism that criticizes and refines plans before execution. It distills reusable planning errors from historical trajectories to identify success/failure patterns, and includes a dynamic re-planning component for runtime plan adjustments when unexpected deviations occur.

Result: Evaluations show PreFlect significantly improves agent utility on complex real-world tasks, outperforming strong reflection-based baselines and more complex agent architectures.

Conclusion: Prospective reflection represents a paradigm shift from post-hoc correction to pre-execution foresight, effectively improving LLM agent reliability and performance.

Abstract: Advanced large language model agents typically adopt self-reflection for improving performance, where agents iteratively analyze past actions to correct errors. However, existing reflective approaches are inherently retrospective: agents act, observe failure, and only then attempt to recover. In this work, we introduce PreFlect, a prospective reflection mechanism that shifts the paradigm from post hoc correction to pre-execution foresight by criticizing and refining agent plans before execution. To support grounded prospective reflection, we distill planning errors from historical agent trajectories, capturing recurring success and failure patterns observed across past executions. Furthermore, we complement prospective reflection with a dynamic re-planning mechanism that provides execution-time plan update in case the original plan encounters unexpected deviation. Evaluations on different benchmarks demonstrate that PreFlect significantly improves overall agent utility on complex real-world tasks, outperforming strong reflection-based baselines and several more complex agent architectures. Code will be updated at https://github.com/wwwhy725/PreFlect.

</details>


### [225] [Is there "Secret Sauce'' in Large Language Model Development?](https://arxiv.org/abs/2602.07238)
*Matthias Mertens,Natalia Fischl-Lanzoni,Neil Thompson*

Main category: cs.AI

TL;DR: This study analyzes 809 LLMs (2022-2025) to determine whether proprietary technology or compute scaling drives performance. At the frontier, 80-90% of performance differences come from scaling compute, not secret sauce. However, away from frontier, proprietary techniques and algorithmic progress significantly reduce compute needs, and companies vary enormously in efficiency (both between and within firms, with >40x differences observed).


<details>
  <summary>Details</summary>
Motivation: Understanding whether LLM performance is driven by proprietary "secret sauce" or simply scaling compute has critical implications for AI competition, innovation incentives, and the diffusion of capabilities across organizations.

Method: Scaling-law regressions using data from 809 models, incorporating release-date and developer fixed effects to disentangle the contributions of compute scaling, developer-specific advantages, and temporal algorithmic progress.

Result: (1) At frontier models, 80-90% of performance variation is explained by training compute, making scale the dominant driver; (2) Away from frontier, proprietary techniques and shared algorithmic progress substantially reduce compute requirements; (3) Systematic developer-specific efficiency advantages exist, especially for smaller models; (4) Within-company variation is striking—models can differ by >40x in compute efficiency.

Conclusion: Frontier LLM advances are overwhelmingly driven by compute scaling rather than proprietary technology, but efficiency advantages away from the frontier and substantial within-company variation indicate that proprietary methods still play a significant role in AI development and competitive dynamics.

Abstract: Do leading LLM developers possess a proprietary ``secret sauce'', or is LLM performance driven by scaling up compute? Using training and benchmark data for 809 models released between 2022 and 2025, we estimate scaling-law regressions with release-date and developer fixed effects. We find clear evidence of developer-specific efficiency advantages, but their importance depends on where models lie in the performance distribution. At the frontier, 80-90% of performance differences are explained by higher training compute, implying that scale--not proprietary technology--drives frontier advances. Away from the frontier, however, proprietary techniques and shared algorithmic progress substantially reduce the compute required to reach fixed capability thresholds. Some companies can systematically produce smaller models more efficiently. Strikingly, we also find substantial variation of model efficiency within companies; a firm can train two models with more than 40x compute efficiency difference. We also discuss the implications for AI leadership and capability diffusion.

</details>


### [226] [Debate is efficient with your time](https://arxiv.org/abs/2602.08630)
*Jonah Brown-Cohen,Geoffrey Irving,Simon C. Marshall,Ilan Newman,Georgios Piliouras,Mario Szegedy*

Main category: cs.AI

TL;DR: 本文提出辩论查询复杂度（DQC）量化AI安全辩论中的人类监督成本，证明PSPACE/poly问题仅需O(log n)次查询，揭示辩论的惊人效率，并建立与电路下界的深刻联系。


<details>
  <summary>Details</summary>
Motivation: 先前关于AI安全辩论的研究仅从理论上确定了辩论可解决的问题范围，但忽略了一个关键实践问题：人类法官需要查看辩论记录的多少比特信息？本研究旨在量化这种人类监督的实际成本。

Method: 引入辩论查询复杂度（DQC）这一新复杂度度量，定义为验证者正确判定辩论所需检查的最小比特数，并通过理论分析刻画其与复杂性类的关系。

Result: 1. PSPACE/poly恰好等于可用O(log n)次查询判定的函数类；2. 依赖所有输入比特的函数需要Omega(log n)次查询；3. 任意大小为s的电路计算的函数满足DQC(f) ≤ log(s) + 3；4. 证明P类语言的DQC下界为log(n)+6将导出新的电路下界。

Conclusion: 辩论机制在查询效率上表现卓越，即使处理高度复杂问题也仅需对数级人工监督。DQC与电路复杂度的关联不仅深化了对辩论理论极限的理解，也为解决电路下界这一核心难题开辟了新方向。

Abstract: AI safety via debate uses two competing models to help a human judge verify complex computational tasks. Previous work has established what problems debate can solve in principle, but has not analysed the practical cost of human oversight: how many queries must the judge make to the debate transcript? We introduce Debate Query Complexity}(DQC), the minimum number of bits a verifier must inspect to correctly decide a debate.
  Surprisingly, we find that PSPACE/poly (the class of problems which debate can efficiently decide) is precisely the class of functions decidable with O(log n) queries. This characterisation shows that debate is remarkably query-efficient: even for highly complex problems, logarithmic oversight suffices. We also establish that functions depending on all their input bits require Omega(log n) queries, and that any function computable by a circuit of size s satisfies DQC(f) <= log(s) + 3. Interestingly, this last result implies that proving DQC lower bounds of log(n) + 6 for languages in P would yield new circuit lower bounds, connecting debate query complexity to central questions in circuit complexity.

</details>


### [227] [From Out-of-Distribution Detection to Hallucination Detection: A Geometric View](https://arxiv.org/abs/2602.07253)
*Litian Liu,Reza Pourreza,Yubing Jian,Yao Qin,Roland Memisevic*

Main category: cs.AI

TL;DR: The paper reframes LLM hallucination detection as an out-of-distribution detection problem, adapting OOD techniques from computer vision to create training-free, single-sample hallucination detectors that work well for reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Hallucination detection in LLMs is critical for safety and reliability, but existing methods perform poorly on reasoning tasks compared to question-answering tasks.

Method: Treat next-token prediction as a classification task and adapt out-of-distribution (OOD) detection techniques from computer vision, making modifications for structural differences in LLMs.

Result: OOD-based approaches achieve training-free, single-sample-based hallucination detectors with strong accuracy specifically for reasoning tasks.

Conclusion: Reframing hallucination detection as OOD detection provides a promising and scalable pathway toward language model safety.

Abstract: Detecting hallucinations in large language models is a critical open problem with significant implications for safety and reliability. While existing hallucination detection methods achieve strong performance in question-answering tasks, they remain less effective on tasks requiring reasoning. In this work, we revisit hallucination detection through the lens of out-of-distribution (OOD) detection, a well-studied problem in areas like computer vision. Treating next-token prediction in language models as a classification task allows us to apply OOD techniques, provided appropriate modifications are made to account for the structural differences in large language models. We show that OOD-based approaches yield training-free, single-sample-based detectors, achieving strong accuracy in hallucination detection for reasoning tasks. Overall, our work suggests that reframing hallucination detection as OOD detection provides a promising and scalable pathway toward language model safety.

</details>


### [228] [Incentive-Aware AI Safety via Strategic Resource Allocation: A Stackelberg Security Games Perspective](https://arxiv.org/abs/2602.07259)
*Cheol Woo Kim,Davin Choo,Tzeh Yuan Neoh,Milind Tambe*

Main category: cs.AI

TL;DR: 该论文提出将Stackelberg安全博弈论应用于AI安全领域，将AI监督建模为防御方（审计者、评估者）与攻击方（恶意行为者、系统故障模式）之间的战略互动，为AI全生命周期中的激励设计、资源限制和对抗性不确定性提供统一框架，使AI监督更具前瞻性、风险意识和抗操纵能力。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统日益强大和自主，现有安全框架仅将对齐视为静态优化问题，忽略了数据收集、模型评估和部署过程中的动态对抗性激励。需要从战略层面监督参与AI开发和部署的人类与机构，而不仅仅是模型层面的对齐。

Method: 提出基于Stackelberg安全博弈（SSGs）的新视角，将AI监督视为防御方与攻击方的战略交互。通过博弈论模型分析AI全生命周期中的激励设计、有限监督能力和对抗性不确定性，并具体应用于：1）训练时的数据/反馈投毒审计；2）资源受限的部署前评估；3）对抗环境下的鲁棒多模型部署。

Result: 建立了一个统一框架，连接了算法对齐与制度监督设计，展示了博弈论威慑如何使AI监督变得主动、风险感知且对操纵具有韧性。该框架为AI安全提供了系统性的方法论。

Conclusion: Stackelberg安全博弈为AI安全提供了处理动态对抗环境的新范式，能够将监督从被动应对转变为主动风险管理，是构建下一代AI治理体系的重要理论基础。

Abstract: As AI systems grow more capable and autonomous, ensuring their safety and reliability requires not only model-level alignment but also strategic oversight of the humans and institutions involved in their development and deployment. Existing safety frameworks largely treat alignment as a static optimization problem (e.g., tuning models to desired behavior) while overlooking the dynamic, adversarial incentives that shape how data are collected, how models are evaluated, and how they are ultimately deployed. We propose a new perspective on AI safety grounded in Stackelberg Security Games (SSGs): a class of game-theoretic models designed for adversarial resource allocation under uncertainty. By viewing AI oversight as a strategic interaction between defenders (auditors, evaluators, and deployers) and attackers (malicious actors, misaligned contributors, or worst-case failure modes), SSGs provide a unifying framework for reasoning about incentive design, limited oversight capacity, and adversarial uncertainty across the AI lifecycle. We illustrate how this framework can inform (1) training-time auditing against data/feedback poisoning, (2) pre-deployment evaluation under constrained reviewer resources, and (3) robust multi-model deployment in adversarial environments. This synthesis bridges algorithmic alignment and institutional oversight design, highlighting how game-theoretic deterrence can make AI oversight proactive, risk-aware, and resilient to manipulation.

</details>


### [229] [BRIDGE: Predicting Human Task Completion Time From Model Performance](https://arxiv.org/abs/2602.07267)
*Fengyuan Liu,Jay Gala,Nilaksh,Dzmitry Bahdanau,Siva Reddy,Hugo Larochelle*

Main category: cs.AI

TL;DR: BRIDGE是一个心理测量框架，通过项目反应理论从模型响应中学习潜在任务难度，并将其与人工完成时间锚定，实现无需人工标注即可评估AI能力并预测其发展轨迹。


<details>
  <summary>Details</summary>
Motivation: 评估AI系统真实世界能力需要将基准测试表现转化为人类可理解的任务难度度量。现有依赖人工任务完成时间标注的方法成本高、噪声大且难以扩展到多个基准测试。

Method: 提出BRIDGE框架，采用双参数逻辑项目反应理论模型，从多个基准的模型表现数据中联合估计潜在任务难度和模型能力，并将潜在难度尺度锚定到人工完成时间。

Result: 验证了潜在任务难度与人工完成时间对数呈线性关系，可从模型表现推断新基准的人工完成时间，预测前沿模型能力，并独立重现METR的指数缩放结果（50%可解任务的时间范围约每6个月翻倍）。

Conclusion: 该框架提供了可扩展的方法，将AI基准测试表现锚定在人类可解释的难度度量上，使AI能力能够以人工任务完成时间的形式进行量化和预测。

Abstract: Evaluating the real-world capabilities of AI systems requires grounding benchmark performance in human-interpretable measures of task difficulty. Existing approaches that rely on direct human task completion time annotations are costly, noisy, and difficult to scale across benchmarks. In this work, we propose BRIDGE, a unified psychometric framework that learns the latent difficulty scale from model responses and anchors it to human task completion time. Using a two-parameter logistic Item Response Theory model, we jointly estimate latent task difficulty and model capability from model performance data across multiple benchmarks. We demonstrate that latent task difficulty varies linearly with the logarithm of human completion time, allowing human task completion time to be inferred for new benchmarks from model performance alone. Leveraging this alignment, we forecast frontier model capabilities in terms of human task length and independently reproduce METR's exponential scaling results, with the 50% solvable task horizon doubling approximately every 6 months.

</details>


### [230] [TermiGen: High-Fidelity Environment and Robust Trajectory Synthesis for Terminal Agents](https://arxiv.org/abs/2602.07274)
*Kaijie Zhu,Yuzhou Nie,Yijiang Li,Yiming Huang,Jialian Wu,Jiang Liu,Ximeng Sun,Zhenfei Yin,Lun Wang,Zicheng Liu,Emad Barsoum,William Yang Wang,Wenbo Guo*

Main category: cs.AI

TL;DR: Open-weight LLMs struggle with terminal tasks due to limited training environments and lack of error-correction examples. TermiGen solves this by generating verifiable environments and resilient trajectories through iterative refinement and error injection, achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: Two key challenges prevent open-weight LLMs from executing complex terminal tasks: (1) scarcity of diverse, executable training environments, and (2) distributional mismatch where expert trajectories lack common mistakes, leaving models unable to recover from runtime failures.

Method: TermiGen is an end-to-end pipeline that: (1) generates valid tasks and Docker containers via iterative multi-agent refinement, and (2) employs a Generator-Critic protocol to actively inject errors during trajectory collection, creating error-correction rich training data.

Result: TermiGen-Qwen2.5-Coder-32B achieves 31.3% pass rate on TerminalBench, establishing new open-weights SOTA and outperforming proprietary models like o4-mini.

Conclusion: TermiGen effectively bridges the gap in terminal task execution by generating verifiable environments and resilient trajectories with error-correction cycles, significantly improving open-weight LLM performance on complex terminal tasks.

Abstract: Executing complex terminal tasks remains a significant challenge for open-weight LLMs, constrained by two fundamental limitations. First, high-fidelity, executable training environments are scarce: environments synthesized from real-world repositories are not diverse and scalable, while trajectories synthesized by LLMs suffer from hallucinations. Second, standard instruction tuning uses expert trajectories that rarely exhibit simple mistakes common to smaller models. This creates a distributional mismatch, leaving student models ill-equipped to recover from their own runtime failures. To bridge these gaps, we introduce TermiGen, an end-to-end pipeline for synthesizing verifiable environments and resilient expert trajectories. Termi-Gen first generates functionally valid tasks and Docker containers via an iterative multi-agent refinement loop. Subsequently, we employ a Generator-Critic protocol that actively injects errors during trajectory collection, synthesizing data rich in error-correction cycles. Fine-tuned on this TermiGen-generated dataset, our TermiGen-Qwen2.5-Coder-32B achieves a 31.3% pass rate on TerminalBench. This establishes a new open-weights state-of-the-art, outperforming existing baselines and notably surpassing capable proprietary models such as o4-mini. Dataset is avaiable at https://github.com/ucsb-mlsec/terminal-bench-env.

</details>


### [231] [Steer2Adapt: Dynamically Composing Steering Vectors Elicits Efficient Adaptation of LLMs](https://arxiv.org/abs/2602.07276)
*Pengrui Han,Xueqiang Xu,Keyang Xuan,Peiyang Song,Siru Ouyang,Runchu Tian,Yuqing Jiang,Cheng Qian,Pengcheng Jiang,Jiashuo Sun,Junxia Cui,Ming Zhong,Ge Liu,Jiawei Han,Jiaxuan You*

Main category: cs.AI

TL;DR: 针对激活 steering 方法灵活性不足的问题，提出 STEER2ADAPT，通过组合可重用基向量而非从头学习，利用低维语义先验子空间和少量示例实现动态适配，在9个任务上平均提升8.2%。


<details>
  <summary>Details</summary>
Motivation: 现有 steering 方法依赖单一静态方向，难以适应任务变化和处理需要多能力协调的复杂任务。

Method: 构建轻量框架，将任务的共享概念维度建模为可重用的低维语义先验子空间，通过少量示例动态发现基向量的线性组合来适配新任务。

Result: 在推理和安全领域的9个任务和3个模型上验证有效，平均提升8.2%。

Conclusion: 该方法具有数据高效、稳定和透明的特点，是有效的推理时适应方法。

Abstract: Activation steering has emerged as a promising approach for efficiently adapting large language models (LLMs) to downstream behaviors. However, most existing steering methods rely on a single static direction per task or concept, making them inflexible under task variation and inadequate for complex tasks that require multiple coordinated capabilities. To address this limitation, we propose STEER2ADAPT, a lightweight framework that adapts LLMs by composing steering vectors rather than learning new ones from scratch. In many domains (e.g., reasoning or safety), tasks share a small set of underlying concept dimensions. STEER2ADAPT captures these dimensions as a reusable, low-dimensional semantic prior subspace, and adapts to new tasks by dynamically discovering a linear combination of basis vectors from only a handful of examples. Experiments across 9 tasks and 3 models in both reasoning and safety domains demonstrate the effectiveness of STEER2ADAPT, achieving an average improvement of 8.2%. Extensive analyses further show that STEER2ADAPT is a data-efficient, stable, and transparent inference-time adaptation method for LLMs.

</details>


### [232] [Adaptive Scaffolding for Cognitive Engagement in an Intelligent Tutoring System](https://arxiv.org/abs/2602.07308)
*Sutapa Dey Tithi,Nazia Alam,Tahreem Yasir,Yang Shi,Xiaoyi Tian,Min Chi,Tiffany Barnes*

Main category: cs.AI

TL;DR: 开发了一个通过动态选择引导式（主动）和错误式（建构性）示例来适应认知投入程度的系统，在逻辑智能辅导系统中，使用BKT或DRL两种自适应方法均能提升学习效果，其中BKT帮助低基础学生追赶高基础学生，DRL则对高基础学生效果更佳。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究表明更高的认知投入能带来更好的学习效果，但在智能辅导系统中，如何个性化设计学习活动以激发最佳认知投入水平仍是一个关键挑战。

Method: 开发并评估了一个自适应认知投入支架系统，在逻辑智能辅导系统中动态选择两种ICAP模式的示例：主动式引导示例和建构性错误示例。通过113名学生实验，比较了贝叶斯知识追踪（BKT）和深度强化学习（DRL）两种自适应方法与非自适应基线方法的效果。

Result: 两种自适应策略均显著提升了学生测试表现。BKT对低先验知识学生的后测成绩提升最大，帮助他们追赶高先验知识同伴；DRL则为高先验知识学生带来了显著更高的后测分数。

Conclusion: 该研究为认知投入与自适应性的复杂交互及其对学习结果的影响提供了新见解，表明不同自适应算法可根据学生先验知识水平的不同而产生差异化效益。

Abstract: The ICAP framework defines four cognitive engagement levels: Passive, Active, Constructive, and Interactive, where increased cognitive engagement can yield improved learning. However, personalizing learning activities that elicit the optimal level of cognitive engagement remains a key challenge in intelligent tutoring systems (ITS). In this work, we develop and evaluate a system that adaptively scaffolds cognitive engagement by dynamically selecting worked examples in two different ICAP modes: (active) Guided examples and (constructive) Buggy examples. We compare Bayesian Knowledge Tracing (BKT) and Deep Reinforcement Learning (DRL) as adaptive methods against a non-adaptive baseline method for selecting example type in a logic ITS. Our experiment with 113 students demonstrates that both adaptive policies significantly improved student performance on test problems. BKT yielded the largest improvement in posttest scores for low prior knowledge students, helping them catch up with their high prior knowledge peers, whereas DRL yielded significantly higher posttest scores among high prior knowledge students. This paper contributes new insights into the complex interactions of cognitive engagement and adaptivity and their results on learning outcomes.

</details>


### [233] [RAPiD: Real-time Deterministic Trajectory Planning via Diffusion Behavior Priors for Safe and Efficient Autonomous Driving](https://arxiv.org/abs/2602.07339)
*Ruturaj Reddy,Hrishav Bakul Barua,Junn Yong Loo,Thanh Thi Nguyen,Ganesh Krishnasamy*

Main category: cs.AI

TL;DR: RAPiD将基于扩散的轨迹规划器提炼为确定性策略，用于实时自动驾驶，在保持性能的同时实现了8倍加速。


<details>
  <summary>Details</summary>
Motivation: 基于扩散的轨迹规划器虽然能很好地建模人类驾驶行为，但由于依赖迭代随机采样，难以满足实时、安全关键部署的要求。

Method: 通过分数正则化策略优化进行确定性策略提取，利用预训练扩散规划器的分数函数作为行为先验，并使用模仿预测驾驶员控制器的评论家进行安全导向的监督。

Result: 相比扩散基线实现8倍加速，在nuPlan上性能具有竞争力，在interPlan基准测试中实现了最先进的泛化能力。

Conclusion: RAPiD成功将扩散规划器提炼为高效确定性策略，兼具速度和良好的泛化能力，使其适用于实时自动驾驶应用。

Abstract: Diffusion-based trajectory planners have demonstrated strong capability for modeling the multimodal nature of human driving behavior, but their reliance on iterative stochastic sampling poses critical challenges for real-time, safety-critical deployment. In this work, we present RAPiD, a deterministic policy extraction framework that distills a pretrained diffusion-based planner into an efficient policy while eliminating diffusion sampling. Using score-regularized policy optimization, we leverage the score function of a pre-trained diffusion planner as a behavior prior to regularize policy learning. To promote safety and passenger comfort, the policy is optimized using a critic trained to imitate a predictive driver controller, providing dense, safety-focused supervision beyond conventional imitation learning. Evaluations demonstrate that RAPiD achieves competitive performance on closed-loop nuPlan scenarios with an 8x speedup over diffusion baselines, while achieving state-of-the-art generalization among learning-based planners on the interPlan benchmark. The official website of this work is: https://github.com/ruturajreddy/RAPiD.

</details>


### [234] [SupChain-Bench: Benchmarking Large Language Models for Real-World Supply Chain Management](https://arxiv.org/abs/2602.07342)
*Shengyue Guan,Yihao Liu,Lang Cao*

Main category: cs.AI

TL;DR: This paper introduces SupChain-Bench to evaluate LLMs in supply chain management, revealing reliability gaps, and proposes SupChain-ReAct framework for improved tool-calling performance.


<details>
  <summary>Details</summary>
Motivation: While LLMs show potential in complex reasoning and tool-based decision making, they struggle with reliable long-horizon, multi-step orchestration required in real-world supply chain management grounded in domain-specific procedures.

Method: Created SupChain-Bench, a unified real-world benchmark assessing both supply chain domain knowledge and long-horizon tool-based orchestration using standard operating procedures (SOPs); proposed SupChain-ReAct, an SOP-free framework that autonomously synthesizes executable procedures for tool use.

Result: Experiments revealed substantial gaps in execution reliability across existing models, while SupChain-ReAct achieved the strongest and most consistent tool-calling performance.

Conclusion: The work establishes a principled benchmark for studying reliable long-horizon orchestration in operational settings and highlights significant room for improvement in LLM-based supply chain agents.

Abstract: Large language models (LLMs) have shown promise in complex reasoning and tool-based decision making, motivating their application to real-world supply chain management. However, supply chain workflows require reliable long-horizon, multi-step orchestration grounded in domain-specific procedures, which remains challenging for current models. To systematically evaluate LLM performance in this setting, we introduce SupChain-Bench, a unified real-world benchmark that assesses both supply chain domain knowledge and long-horizon tool-based orchestration grounded in standard operating procedures (SOPs). Our experiments reveal substantial gaps in execution reliability across models. We further propose SupChain-ReAct, an SOP-free framework that autonomously synthesizes executable procedures for tool use, achieving the strongest and most consistent tool-calling performance. Our work establishes a principled benchmark for studying reliable long-horizon orchestration in real-world operational settings and highlights significant room for improvement in LLM-based supply chain agents.

</details>


### [235] [W&D:Scaling Parallel Tool Calling for Efficient Deep Research Agents](https://arxiv.org/abs/2602.07359)
*Xiaoqiang Lin,Jun Hao Liew,Silvio Savarese,Junnan Li*

Main category: cs.AI

TL;DR: 该论文提出Wide and Deep研究智能体框架，通过并行工具调用来扩展宽度，发现宽度扩展能显著提升性能并减少交互轮次。在GPT-5-Medium上达到62.2%的BrowseComp准确率，超越GPT-5-High的54.9%，表明优化宽度和深度平衡是实现高效研究智能体的关键路径。


<details>
  <summary>Details</summary>
Motivation: 尽管深度扩展（增加顺序思考和工具调用）已被广泛研究，但通过并行工具调用进行宽度扩展的潜力尚未被充分探索，现有方法依赖复杂的多智能体编排来实现并行化。

Method: 提出Wide and Deep研究智能体框架，利用内在的并行工具调用来在单一推理步骤内实现有效协调，无需复杂的多智能体编排，并分析不同工具调用调度器以优化并行策略。

Result: 宽度扩展显著提升了深度研究基准测试性能，减少了获得正确答案所需的交互轮次；通过案例研究分析了改进因素；探索了多种工具调用调度器；GPT-5-Medium在BrowseComp上达到62.2%准确率，超越GPT-5-High的54.9%。

Conclusion: 优化宽度与深度之间的平衡是构建高效深度研究智能体的关键途径。

Abstract: Deep research agents have emerged as powerful tools for automating complex intellectual tasks through multi-step reasoning and web-based information seeking. While recent efforts have successfully enhanced these agents by scaling depth through increasing the number of sequential thinking and tool calls, the potential of scaling width via parallel tool calling remains largely unexplored. In this work, we propose the Wide and Deep research agent, a framework designed to investigate the behavior and performance of agents when scaling not only depth but also width via parallel tool calling. Unlike existing approaches that rely on complex multi-agent orchestration to parallelize workloads, our method leverages intrinsic parallel tool calling to facilitate effective coordination within a single reasoning step. We demonstrate that scaling width significantly improves performance on deep research benchmarks while reducing the number of turns required to obtain correct answers. Furthermore, we analyze the factors driving these improvements through case studies and explore various tool call schedulers to optimize parallel tool calling strategy. Our findings suggest that optimizing the trade-off between width and depth is a critical pathway toward high-efficiency deep research agents. Notably, without context management or other tricks, we obtain 62.2% accuracy with GPT-5-Medium on BrowseComp, surpassing the original 54.9% reported by GPT-5-High.

</details>


### [236] [NAAMSE: Framework for Evolutionary Security Evaluation of Agents](https://arxiv.org/abs/2602.07391)
*Kunal Pai,Parth Shah,Harshil Patel*

Main category: cs.AI

TL;DR: The paper proposes NAAMSE, an evolutionary framework for automated AI agent security testing that uses genetic prompt mutation and hierarchical exploration to uncover vulnerabilities that static benchmarks miss, while ensuring the agent still works correctly for benign uses.


<details>
  <summary>Details</summary>
Motivation: Current AI agent security evaluations are limited by manual red-teaming and static benchmarks that cannot effectively model adaptive, multi-turn adversarial attacks, creating a bottleneck in robust security assessment.

Method: NAAMSE reframes security evaluation as a feedback-driven optimization problem using a single autonomous agent that orchestrates genetic prompt mutation, hierarchical corpus exploration, and asymmetric behavioral scoring. It uses model responses as fitness signals to iteratively evolve attack strategies while maintaining benign-use correctness.

Result: Experiments on Gemini 2.5 Flash show that evolutionary mutation systematically identifies vulnerabilities that one-shot methods miss. Controlled ablations demonstrate that the synergy between exploration and targeted mutation uncovers high-severity failure modes.

Conclusion: This adaptive approach provides a more realistic and scalable assessment of agent robustness against evolving threats compared to traditional methods.

Abstract: AI agents are increasingly deployed in production, yet their security evaluations remain bottlenecked by manual red-teaming or static benchmarks that fail to model adaptive, multi-turn adversaries. We propose NAAMSE, an evolutionary framework that reframes agent security evaluation as a feedback-driven optimization problem. Our system employs a single autonomous agent that orchestrates a lifecycle of genetic prompt mutation, hierarchical corpus exploration, and asymmetric behavioral scoring. By using model responses as a fitness signal, the framework iteratively compounds effective attack strategies while simultaneously ensuring "benign-use correctness", preventing the degenerate security of blanket refusal. Our experiments on Gemini 2.5 Flash demonstrate that evolutionary mutation systematically amplifies vulnerabilities missed by one-shot methods, with controlled ablations revealing that the synergy between exploration and targeted mutation uncovers high-severity failure modes. We show that this adaptive approach provides a more realistic and scalable assessment of agent robustness in the face of evolving threats. The code for NAAMSE is open source and available at https://github.com/HASHIRU-AI/NAAMSE.

</details>


### [237] [VGAS: Value-Guided Action-Chunk Selection for Few-Shot Vision-Language-Action Adaptation](https://arxiv.org/abs/2602.07399)
*Changhua Xu,Jie Lu,Junyu Xuan,En Yu*

Main category: cs.AI

TL;DR: 针对视觉-语言-动作模型在新任务上因几何歧义导致的少样本适应问题，本文提出生成-选择框架VGAS，通过Q-Chunk-Former和显式几何正则化提升动作选择的几何精确性。


<details>
  <summary>Details</summary>
Motivation: 微调后的VLA策略在少样本适应时，常因几何歧义产生看似合理但执行失败的动作，现有方法难以解决有限监督下的几何模糊性问题。

Method: 提出VGAS框架：1) 使用微调VLA作为高召回率动作块生成器；2) 设计Q-Chunk-Former作为几何感知的Transformer评判器；3) 引入显式几何正则化塑造价值函数，保持近距候选动作的排序分辨率。

Result: 实验表明VGAS在有限演示和分布偏移下，能稳定提升任务成功率和鲁棒性，理论分析验证了其有效性。

Conclusion: 通过生成-选择范式和几何导向的价值建模，VGAS为少样本VLA适应提供了可靠解决方案，代码已开源。

Abstract: Vision--Language--Action (VLA) models bridge multimodal reasoning with physical control, but adapting them to new tasks with scarce demonstrations remains unreliable. While fine-tuned VLA policies often produce semantically plausible trajectories, failures often arise from unresolved geometric ambiguities, where near-miss action candidates lead to divergent execution outcomes under limited supervision. We study few-shot VLA adaptation from a \emph{generation--selection} perspective and propose a novel framework \textbf{VGAS} (\textbf{V}alue-\textbf{G}uided \textbf{A}ction-chunk \textbf{S}election). It performs inference-time best-of-$N$ selection to identify action chunks that are both semantically faithful and geometrically precise. Specifically, \textbf{VGAS} employs a finetuned VLA as a high-recall proposal generator and introduces the \textrm{Q-Chunk-Former}, a geometrically grounded Transformer critic to resolve fine-grained geometric ambiguities. In addition, we propose \textit{Explicit Geometric Regularization} (\texttt{EGR}), which explicitly shapes a discriminative value landscape to preserve action ranking resolution among near-miss candidates while mitigating value instability under scarce supervision. Experiments and theoretical analysis demonstrate that \textbf{VGAS} consistently improves success rates and robustness under limited demonstrations and distribution shifts. Our code is available at https://github.com/Jyugo-15/VGAS.

</details>


### [238] [Progressive Multi-Agent Reasoning for Biological Perturbation Prediction](https://arxiv.org/abs/2602.07408)
*Hyomin Kim,Sang-Yeon Hwang,Jaechang Lim,Yinhua Piao,Yunhak Oh,Woo Youn Kim,Chanyoung Park,Sungsoo Ahn,Junhyeok Jeon*

Main category: cs.AI

TL;DR: The paper introduces LINCSQA, a benchmark for predicting gene regulation under bulk-cell chemical perturbations, and PBio-Agent, a multi-agent framework that uses difficulty-aware task sequencing and iterative knowledge refinement to outperform existing methods on gene regulation prediction tasks.


<details>
  <summary>Details</summary>
Motivation: Predicting gene regulation responses requires reasoning about biological causality, but LLMs struggle with entangled high-dimensional data. Existing research focuses on single-cell genetic perturbations, leaving bulk-cell chemical perturbations (crucial for drug discovery) largely unexplored, creating a significant gap in the field.

Method: The authors created LINCSQA benchmark and proposed PBio-Agent, a multi-agent system that integrates difficulty-aware task sequencing with iterative knowledge refinement. The framework leverages the insight that co-perturbed genes share causal structures, using specialized agents enriched with biological knowledge graphs, a synthesis agent for integration, and specialized judges to ensure logical coherence.

Result: PBio-Agent significantly outperforms existing baselines on both LINCSQA and PerturbQA benchmarks. Remarkably, the framework enables smaller models to accurately predict and explain complex biological processes without requiring additional training.

Conclusion: The multi-agent approach effectively addresses the challenge of high-dimensional biological perturbation data. By leveraging shared causal structures and iterative knowledge refinement, PBio-Agent demonstrates that sophisticated biological reasoning can be achieved even with smaller models, advancing the field of computational drug discovery.

Abstract: Predicting gene regulation responses to biological perturbations requires reasoning about underlying biological causalities. While large language models (LLMs) show promise for such tasks, they are often overwhelmed by the entangled nature of high-dimensional perturbation results. Moreover, recent works have primarily focused on genetic perturbations in single-cell experiments, leaving bulk-cell chemical perturbations, which is central to drug discovery, largely unexplored. Motivated by this, we present LINCSQA, a novel benchmark for predicting target gene regulation under complex chemical perturbations in bulk-cell environments. We further propose PBio-Agent, a multi-agent framework that integrates difficulty-aware task sequencing with iterative knowledge refinement. Our key insight is that genes affected by the same perturbation share causal structure, allowing confidently predicted genes to contextualize more challenging cases. The framework employs specialized agents enriched with biological knowledge graphs, while a synthesis agent integrates outputs and specialized judges ensure logical coherence. PBio-Agent outperforms existing baselines on both LINCSQA and PerturbQA, enabling even smaller models to predict and explain complex biological processes without additional training.

</details>


### [239] [Are Reasoning LLMs Robust to Interventions on Their Chain-of-Thought?](https://arxiv.org/abs/2602.07470)
*Alexander von Recum,Leander Girrbach,Zeynep Akata*

Main category: cs.AI

TL;DR: 本研究通过受控扰动框架评估推理型大语言模型的思维链鲁棒性，发现RLLMs普遍具有鲁棒性（随规模提升），但早期干预影响更大。鲁棒性非风格无关：改写抑制怀疑表达并降低性能，其他干预则触发怀疑支持恢复。恢复过程带来代价：噪声使思维链长度激增200%以上，改写虽缩短轨迹却损害准确率。


<details>
  <summary>Details</summary>
Motivation: 推理型大语言模型的推理链鲁棒性对确保模型可靠性至关重要，但其面对内部干扰时的稳健性尚不明确，需要系统性评估。

Method: 设计受控评估框架，在固定时间步对模型思维链施加七种干预（良性、中性、对抗性），测试多个开源推理型大语言模型在数学、科学和逻辑任务上的表现。

Result: 1) RLLMs普遍鲁棒，规模越大鲁棒性越强，但早期干预显著降低鲁棒性；2) 鲁棒性具风格依赖性：改写抑制怀疑表达并降低性能，其他干预触发怀疑支持恢复；3) 恢复代价：中性和对抗性噪声使CoT长度增加超200%，改写缩短轨迹但损害准确率。

Conclusion: 研究为RLLMs推理完整性提供新证据，识别怀疑为核心恢复机制，并指出未来训练需解决鲁棒性与效率间的权衡问题。

Abstract: Reasoning LLMs (RLLMs) generate step-by-step chains of thought (CoTs) before giving an answer, which improves performance on complex tasks and makes reasoning more transparent. But how robust are these reasoning traces to disruptions that occur within them? To address this question, we introduce a controlled evaluation framework that perturbs a model's own CoT at fixed timesteps. We design seven interventions (benign, neutral, and adversarial) and apply them to multiple open-weight RLLMs across Math, Science, and Logic tasks. Our results show that RLLMs are generally robust, reliably recovering from diverse perturbations, with robustness improving with model size and degrading when interventions occur early. However, robustness is not style-invariant: paraphrasing suppresses doubt-like expressions and reduces performance, while other interventions trigger doubt and support recovery. Recovery also carries a cost: neutral and adversarial noise can inflate CoT length by more than 200%, whereas paraphrasing shortens traces but harms accuracy. These findings provide new evidence on how RLLMs maintain reasoning integrity, identify doubt as a central recovery mechanism, and highlight trade-offs between robustness and efficiency that future training methods should address.

</details>


### [240] [Computing the Reachability Value of Posterior-Deterministic POMDPs](https://arxiv.org/abs/2602.07473)
*Nathanaël Fijalkow,Arka Ghosh,Roman Kniazev,Guillermo A. Pérez,Pierre Vandenhove*

Main category: cs.AI

TL;DR: This paper introduces posterior-deterministic POMDPs, a new subclass where the next state is uniquely determined by current state, action, and observation. Unlike general POMDPs (where reachability probability is undecidable), this class allows approximating maximal reachability probability to arbitrary precision, including MDPs and classical examples like the Tiger problem.


<details>
  <summary>Details</summary>
Motivation: Many POMDP verification/synthesis problems are undecidable or intractable. Specifically, Madani et al. (2003) proved no algorithm can compute or approximate maximal reachability probability for general POMDPs, unlike fully observable MDPs (solvable in polynomial time). This creates a critical gap in scalable decision-making under uncertainty.

Method: Defines "posterior-deterministic POMDPs" where the true state becomes permanently known after taking an action and receiving an observation (i.e., next state is uniquely determined by current state, action, and observation). This property ensures once the state is known, uncertainty does not reoccur.

Result: For posterior-deterministic POMDPs, the maximal probability of reaching target states can be approximated up to arbitrary precision. This is the largest known POMDP class where reachability value is computable/approximable, subsuming MDPs and including non-trivial examples like the Tiger POMDP.

Conclusion: Posterior-determinism provides a powerful framework for tractable POMDP analysis, bridging the gap between fully observable MDPs and intractable general POMDPs. It enables practical approximation of reachability probabilities in previously undecidable scenarios.

Abstract: Partially observable Markov decision processes (POMDPs) are a fundamental model for sequential decision-making under uncertainty. However, many verification and synthesis problems for POMDPs are undecidable or intractable. Most prominently, the seminal result of Madani et al. (2003) states that there is no algorithm that, given a POMDP and a set of target states, can compute the maximal probability of reaching the target states, or even approximate it up to a non-trivial constant. This is in stark contrast to fully observable Markov decision processes (MDPs), where the reachability value can be computed in polynomial time.
  In this work, we introduce posterior-deterministic POMDPs, a novel class of POMDPs. Our main technical contribution is to show that for posterior-deterministic POMDPs, the maximal probability of reaching a given set of states can be approximated up to arbitrary precision.
  A POMDP is posterior-deterministic if the next state can be uniquely determined by the current state, the action taken, and the observation received. While the actual state is generally uncertain in POMDPs, the posterior-deterministic property tells us that once the true state is known it remains known forever. This simple and natural definition includes all MDPs and captures classical non-trivial examples such as the Tiger POMDP (Kaelbling et al. 1998), making it one of the largest known classes of POMDPs for which the reachability value can be approximated.

</details>


### [241] [GraphAgents: Knowledge Graph-Guided Agentic AI for Cross-Domain Materials Design](https://arxiv.org/abs/2602.07491)
*Isabella A. Stewart,Tarjei Paule Hage,Yu-Chuan Hsu,Markus J. Buehler*

Main category: cs.AI

TL;DR: 提出一种结合大规模知识图谱的多智能体框架，用于材料科学领域的可持续材料设计，以寻找PFAS替代品。该框架通过专业化分工（问题分解、证据检索、参数提取、图遍历）实现跨领域知识连接，在生物医学导管应用中成功生成了平衡多种性能指标的无PFAS替代方案。


<details>
  <summary>Details</summary>
Motivation: 科学文献爆炸式增长导致信息过载，单一智能体LLM易产生幻觉且难以有效整合从分子化学到机械性能的多领域知识，人类也无法单独应对海量信息，亟需自动化框架来发现跨领域有意义的连接。

Method: 构建多智能体框架，各智能体分别负责问题分解、证据检索、设计参数提取和知识图谱遍历，通过专业化分工和协作进行关系推理与假设生成，并采用消融研究验证框架效果。

Result: 消融研究表明完整多智能体pipeline显著优于单次提示；通过定制图遍历策略，系统能在利用式搜索（聚焦领域关键结果）和探索式搜索（发现新兴跨领域连接）间切换；在生物医学导管案例中成功生成满足摩擦学性能、热稳定性、耐化学性和生物相容性的可持续PFAS-free替代方案。

Conclusion: 该框架有效拓展了材料设计空间，证明了知识图谱与多智能体推理结合在跨学科材料发现中的可行性，为可持续材料设计提供了可扩展的自动化解决方案。

Abstract: Large Language Models (LLMs) promise to accelerate discovery by reasoning across the expanding scientific landscape. Yet, the challenge is no longer access to information but connecting it in meaningful, domain-spanning ways. In materials science, where innovation demands integrating concepts from molecular chemistry to mechanical performance, this is especially acute. Neither humans nor single-agent LLMs can fully contend with this torrent of information, with the latter often prone to hallucinations. To address this bottleneck, we introduce a multi-agent framework guided by large-scale knowledge graphs to find sustainable substitutes for per- and polyfluoroalkyl substances (PFAS)-chemicals currently under intense regulatory scrutiny. Agents in the framework specialize in problem decomposition, evidence retrieval, design parameter extraction, and graph traversal, uncovering latent connections across distinct knowledge pockets to support hypothesis generation. Ablation studies show that the full multi-agent pipeline outperforms single-shot prompting, underscoring the value of distributed specialization and relational reasoning. We demonstrate that by tailoring graph traversal strategies, the system alternates between exploitative searches focusing on domain-critical outcomes and exploratory searches surfacing emergent cross-connections. Illustrated through the exemplar of biomedical tubing, the framework generates sustainable PFAS-free alternatives that balance tribological performance, thermal stability, chemical resistance, and biocompatibility. This work establishes a framework combining knowledge graphs with multi-agent reasoning to expand the materials design space, showcasing several initial design candidates to demonstrate the approach.

</details>


### [242] [MSP-LLM: A Unified Large Language Model Framework for Complete Material Synthesis Planning](https://arxiv.org/abs/2602.07543)
*Heewoong Noh,Gyoung S. Na,Namkyeong Lee,Chanyoung Park*

Main category: cs.AI

TL;DR: 提出MSP-LLM，一个统一的LLM框架，通过将材料合成规划分解为前体预测和合成操作预测两个子问题，并引入离散材料类别作为中间变量，实现了端到端的材料合成规划，在各项任务上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 材料合成规划是AI驱动材料发现的关键瓶颈，需要同时识别合适的前体材料和设计连贯的合成操作序列。现有方法仅解决孤立子任务，缺乏统一的解决方案。

Method: 提出MSP-LLM框架，将MSP形式化为包含前体预测(PP)和合成操作预测(SOP)的结构化流程。引入离散材料类别作为中间决策变量连接两个子任务，在SOP中融入分层前体类型作为归纳偏置，并采用显式条件策略在自回归解码过程中保留前体相关信息。

Result: 大量实验表明，MSP-LLM在前体预测、合成操作预测以及完整的材料合成规划任务上均持续优于现有方法。

Conclusion: MSP-LLM提供了一个有效且可扩展的材料合成规划框架，能够加速实际材料发现过程。

Abstract: Material synthesis planning (MSP) remains a fundamental and underexplored bottleneck in AI-driven materials discovery, as it requires not only identifying suitable precursor materials but also designing coherent sequences of synthesis operations to realize a target material. Although several AI-based approaches have been proposed to address isolated subtasks of MSP, a unified methodology for solving the entire MSP task has yet to be established. We propose MSP-LLM, a unified LLM-based framework that formulates MSP as a structured process composed of two constituent subproblems: precursor prediction (PP) and synthesis operation prediction (SOP). Our approach introduces a discrete material class as an intermediate decision variable that organizes both tasks into a chemically consistent decision chain. For OP, we further incorporate hierarchical precursor types as synthesis-relevant inductive biases and employ an explicit conditioning strategy that preserves precursor-related information in the autoregressive decoding state. Extensive experiments show that MSP-LLM consistently outperforms existing methods on both PP and SOP, as well as on the complete MSP task, demonstrating an effective and scalable framework for MSP that can accelerate real-world materials discovery.

</details>


### [243] [When Is Enough Not Enough? Illusory Completion in Search Agents](https://arxiv.org/abs/2602.07549)
*Dayoon Ko,Jihyuk Kim,Sohyeon Kim,Haeju Park,Dahyun Lee,Gunhee Kim,Moontae Lee,Kyungjae Lee*

Main category: cs.AI

TL;DR: Search agents often suffer from "illusory completion" - believing they've solved multi-constraint problems when they haven't. The authors introduce Epistemic Ledger to diagnose this, identifying four failure patterns, then propose LiveLedger, a real-time constraint tracker that reduces errors by up to 26.5% and boosts accuracy by up to 11.6%.


<details>
  <summary>Details</summary>
Motivation: While recent search agents show strong performance on multi-hop benchmarks, it's unclear whether they can reliably track, verify, and maintain multiple simultaneous constraints in questions. The paper investigates this capability under multi-constraint problems and identifies the phenomenon of "illusory completion" where agents produce underverified answers despite unresolved or violated constraints.

Method: The authors introduce the Epistemic Ledger, an evaluation framework that tracks evidential support and agents' beliefs for each constraint throughout multi-turn reasoning. They analyze failure patterns and then propose LiveLedger, an inference-time intervention that performs explicit constraint-state tracking during execution to mitigate these failures.

Result: The analysis reveals four recurring failure patterns: bare assertions, overlooked refutations, stagnation, and premature exit. The LiveLedger intervention consistently improves performance, reducing underverified answers by up to 26.5% and improving overall accuracy by up to 11.6% on multi-constraint problems.

Conclusion: Explicit constraint-state tracking via LiveLedger is an effective, simple intervention that mitigates illusory completion and the identified failure patterns, leading to more reliable reasoning and significantly improved performance on multi-constraint problems.

Abstract: Recent search agents leverage multi-turn reasoning and search tools to achieve strong performance on multi-hop and long-horizon benchmarks. Yet it remains unclear whether they reliably reason across all requirements by tracking, verifying, and maintaining multiple conditions in these questions. We study this capability under multi-constraint problems, where valid answers must satisfy several constraints simultaneously. We find that illusory completion frequently occurs, wherein agents believe tasks are complete despite unresolved or violated constraints, leading to underverified answers. To diagnose this behavior, we introduce the Epistemic Ledger, an evaluation framework that tracks evidential support and agents' beliefs for each constraint throughout multi-turn reasoning. Our analysis reveals four recurring failure patterns: bare assertions, overlooked refutations, stagnation, and premature exit. Motivated by these findings, we examine whether explicit constraint-state tracking during execution mitigates these failures via LiveLedger, an inference-time tracker. This simple intervention consistently improves performance, substantially reducing underverified answers (by up to 26.5%) and improving overall accuracy (by up to 11.6%) on multi-constraint problems.

</details>


### [244] [SleepMaMi: A Universal Sleep Foundation Model for Integrating Macro- and Micro-structures](https://arxiv.org/abs/2602.07628)
*Keondo Park,Younghoon Na,Yourim Choi,Hyunwoo Ryu,Hyun-Woo Shin,Hyung-Sin Kim*

Main category: cs.AI

TL;DR: SleepMaMi是一个层次化双编码器睡眠基础模型，能同时处理整晚睡眠架构和细粒度信号形态，在20,000+PSG数据上预训练后，在多种下游临床任务中表现优于现有基础模型。


<details>
  <summary>Details</summary>
Motivation: 睡眠医学领域仍主要使用针对局部微结构特征的任务特定模型，这些方法忽视了多导睡眠图(PSG)的丰富多模态上下文，且未能捕捉整晚睡眠的全局宏观结构。

Method: 采用层次化双编码器框架：Macro编码器通过人口统计学引导的对比学习建模整晚时序依赖关系，Micro编码器通过混合掩码自编码器(MAE)和多模态对比目标优化短时特征提取。在超过20,000条PSG记录（158,000小时）的大规模语料库上进行预训练。

Result: 在一系列下游任务中均优于现有基础模型，展现出卓越的泛化能力和标签高效适应能力。

Conclusion: SleepMaMi证明了基础模型在睡眠医学中的有效性，通过同时捕捉宏观和微观睡眠结构，为临床睡眠分析提供了更强大的工具。

Abstract: While the shift toward unified foundation models has revolutionized many deep learning domains, sleep medicine remains largely restricted to task-specific models that focus on localized micro-structure features. These approaches often neglect the rich, multi-modal context of Polysomnography (PSG) and fail to capture the global macro-structure of a full night's sleep. To address this, we introduce SleepMaMi , a Sleep Foundation Model engineered to master both hour-long sleep architectures and fine-grained signal morphologies. Our framework utilizes a hierarchical dual-encoder design: a Macro-Encoder to model full-night temporal dependencies and a Micro-Encoder to capture short-term characteristics from biosignals. Macro-Encoder is trained via Demographic-Guided Contrastive Learning, which aligns overnight sleep patterns with objective subject metadata, such as age, sex and BMI to refine global representations. Micro-Encoder is optimized via a hybrid Masked Autoencoder (MAE) and multi-modal contrastive objective. Pre-trained on a massive corpus of $>$20,000 PSG recordings (158K hours),SleepMaMi outperforms existing foundation models across a diverse suite of downstream tasks, demonstrating superior generalizability and label-efficient adaptation for clinical sleep analysis.

</details>


### [245] [Efficient Table Retrieval and Understanding with Multimodal Large Language Models](https://arxiv.org/abs/2602.07642)
*Zhuoyan Xu,Haoyang Fang,Boran Han,Bonan Min,Bernie Wang,Cuixiong Hu,Shuai Zhang*

Main category: cs.AI

TL;DR: TabRAG is a three-stage framework that enables MLLMs to answer queries over large collections of table images through retrieval, reranking, and reasoning, achieving 7.0% higher retrieval recall and 6.1% higher answer accuracy than existing methods.


<details>
  <summary>Details</summary>
Motivation: Real-world tabular data often exists as images (financial reports, handwritten records, document scans) with combined structural and visual complexities. Current MLLMs assume tables are readily available, but practical applications require identifying and reasoning over relevant tables from large-scale collections.

Method: A three-stage pipeline: (1) candidate table retrieval using jointly trained visual-text foundation models, (2) fine-grained candidate reranking with MLLMs, and (3) reasoning over selected tables using MLLMs for answer generation.

Result: On a new dataset with 88,161 training and 9,819 testing samples across 8 benchmarks (48,504 unique tables), the framework significantly outperforms existing methods by 7.0% in retrieval recall and 6.1% in answer accuracy.

Conclusion: TabRAG provides a practical solution for real-world table understanding tasks involving large collections of table images, effectively bridging the gap between visual table retrieval and query answering.

Abstract: Tabular data is frequently captured in image form across a wide range of real-world scenarios such as financial reports, handwritten records, and document scans. These visual representations pose unique challenges for machine understanding, as they combine both structural and visual complexities. While recent advances in Multimodal Large Language Models (MLLMs) show promising results in table understanding, they typically assume the relevant table is readily available. However, a more practical scenario involves identifying and reasoning over relevant tables from large-scale collections to answer user queries. To address this gap, we propose TabRAG, a framework that enables MLLMs to answer queries over large collections of table images. Our approach first retrieves candidate tables using jointly trained visual-text foundation models, then leverages MLLMs to perform fine-grained reranking of these candidates, and finally employs MLLMs to reason over the selected tables for answer generation. Through extensive experiments on a newly constructed dataset comprising 88,161 training and 9,819 testing samples across 8 benchmarks with 48,504 unique tables, we demonstrate that our framework significantly outperforms existing methods by 7.0% in retrieval recall and 6.1% in answer accuracy, offering a practical solution for real-world table understanding tasks.

</details>


### [246] [ONTrust: A Reference Ontology of Trust](https://arxiv.org/abs/2602.07662)
*Glenda Amaral,Tiago Prince Sales,Riccardo Baratella,Daniele Porello,Renata Guizzardi,Giancarlo Guizzardi*

Main category: cs.AI

TL;DR: 该论文针对人工智能和区块链等新技术背景下信任概念的重要性，提出并验证了基于统一基础本体论（UFO）的ONTrust信任参考本体，旨在为信任提供形式化概念模型，支持信息建模、语义互操作及可信AI系统设计。


<details>
  <summary>Details</summary>
Motivation: 新兴技术（如AI和区块链）催生了新型信任形式，其应用依赖用户对系统的信任。但当前缺乏统一的信任概念化框架，难以被人类和机器共同理解，制约了可信系统的设计与治理。

Method: 基于统一基础本体论（UFO）构建参考本体ONTrust，采用OntoUML形式化建模，并通过多领域应用（如企业架构设计、语言评估、AI团队可信系统）验证其有效性。

Result: ONTrust明确定义了信任类型、影响信任的因素及风险产生机制，并通过文献中的两个案例研究成功演示了其建模能力，支持需求工程、信任管理和人机协同AI系统开发。

Conclusion: ONTrust为信任提供了严谨的本体基础，解决了跨领域语义一致性难题，可广泛应用于需要形式化信任建模的场景，推动可信技术生态的标准化发展。

Abstract: Trust has stood out more than ever in the light of recent innovations. Some examples are advances in artificial intelligence that make machines more and more humanlike, and the introduction of decentralized technologies (e.g. blockchains), which creates new forms of (decentralized) trust. These new developments have the potential to improve the provision of products and services, as well as to contribute to individual and collective well-being. However, their adoption depends largely on trust. In order to build trustworthy systems, along with defining laws, regulations and proper governance models for new forms of trust, it is necessary to properly conceptualize trust, so that it can be understood both by humans and machines. This paper is the culmination of a long-term research program of providing a solid ontological foundation on trust, by creating reference conceptual models to support information modeling, automated reasoning, information integration and semantic interoperability tasks. To address this, a Reference Ontology of Trust (ONTrust) was developed, grounded on the Unified Foundational Ontology and specified in OntoUML, which has been applied in several initiatives, to demonstrate, for example, how it can be used for conceptual modeling and enterprise architecture design, for language evaluation and (re)design, for trust management, for requirements engineering, and for trustworthy artificial intelligence (AI) in the context of affective Human-AI teaming. ONTrust formally characterizes the concept of trust and its different types, describes the different factors that can influence trust, as well as explains how risk emerges from trust relations. To illustrate the working of ONTrust, the ontology is applied to model two case studies extracted from the literature.

</details>


### [247] [EventCast: Hybrid Demand Forecasting in E-Commerce with LLM-Based Event Knowledge](https://arxiv.org/abs/2602.07695)
*Congcong Hu,Yuang Shi,Fan Huang,Yang Xiang,Zhou Ye,Ming Jin,Shiyu Wang*

Main category: cs.AI

TL;DR: EventCast is a novel e-commerce demand forecasting framework that uses LLMs for event-driven reasoning (not numerical prediction) to integrate future event knowledge into time-series models, achieving up to 57% MAE reduction during high-impact periods like flash sales and holidays, with successful deployment across 4 countries since March 2025.


<details>
  <summary>Details</summary>
Motivation: Existing e-commerce demand forecasting systems fail during high-impact periods (flash sales, holiday campaigns, sudden policy interventions) where demand patterns shift abruptly and unpredictably, directly impacting inventory planning and fulfillment scheduling operations.

Method: EventCast employs a dual-tower architecture that fuses historical demand features with event knowledge processed by LLMs. Unstructured business data (campaigns, holidays, seller incentives) from operational databases is converted into interpretable textual summaries leveraging world knowledge, which are then integrated with time-series features for explainable forecasting.

Result: In real-world deployment across 4 countries and 160 regions over 10 months, EventCast achieved 86.9% MAE and 97.7% MSE improvement versus no-event variants, and 57.0% MAE and 83.3% MSE reduction compared to industrial baselines during event-driven periods, since production deployment in March 2025.

Conclusion: EventCast provides a practical, scalable solution for improving operational decision-making in dynamic e-commerce environments by effectively integrating future event knowledge into demand forecasting systems while maintaining explainability.

Abstract: Demand forecasting is a cornerstone of e-commerce operations, directly impacting inventory planning and fulfillment scheduling. However, existing forecasting systems often fail during high-impact periods such as flash sales, holiday campaigns, and sudden policy interventions, where demand patterns shift abruptly and unpredictably. In this paper, we introduce EventCast, a modular forecasting framework that integrates future event knowledge into time-series prediction. Unlike prior approaches that ignore future interventions or directly use large language models (LLMs) for numerical forecasting, EventCast leverages LLMs solely for event-driven reasoning. Unstructured business data, which covers campaigns, holiday schedules, and seller incentives, from existing operational databases, is processed by an LLM that converts it into interpretable textual summaries leveraging world knowledge for cultural nuances and novel event combinations. These summaries are fused with historical demand features within a dual-tower architecture, enabling accurate, explainable, and scalable forecasts. Deployed on real-world e-commerce scenarios spanning 4 countries of 160 regions over 10 months, EventCast achieves up to 86.9% and 97.7% improvement on MAE and MSE compared to the variant without event knowledge, and reduces MAE by up to 57.0% and MSE by 83.3% versus the best industrial baseline during event-driven periods. EventCast has deployed into real-world industrial pipelines since March 2025, offering a practical solution for improving operational decision-making in dynamic e-commerce environments.

</details>


### [248] [Geo-Code: A Code Framework for Reverse Code Generation from Geometric Images Based on Two-Stage Multi-Agent Evolution](https://arxiv.org/abs/2602.07749)
*Zhenyu Wu,Yanxi Long,Jian Li,Hua Huang*

Main category: cs.AI

TL;DR: 提出Geo-coder，首个基于多智能体的几何图像逆编程框架，通过像素级锚定和度量驱动的代码进化实现高精度几何重建与视觉一致性


<details>
  <summary>Details</summary>
Motivation: 现有逆图形方法难以准确重建复杂几何细节，导致关键几何约束丢失或结构失真，影响大模型的多模态推理能力提升

Method: 采用两阶段解耦：阶段一利用视觉算子与大模型互补优势进行像素坐标与视觉属性精确捕捉；阶段二构建合成-渲染-验证闭环，通过双向视觉反馈驱动代码自校正

Result: 在几何重建精度和视觉一致性上取得显著领先，重建图像在多模态推理任务中与原始图像性能相当，验证了框架鲁棒性

Conclusion: Geo-coder框架有效解决了几何细节重建难题，已开源包含1500+样本的数据集及GeocodeLM模型，为后续研究奠定基础

Abstract: Program code serves as a bridge linking vision and logic, providing a feasible supervisory approach for enhancing the multimodal reasoning capability of large models through geometric operations such as auxiliary line construction and perspective transformation. Nevertheless, current inverse graphics methods face tremendous challenges in accurately reconstructing complex geometric details, which often results in the loss of key geometric constraints or structural distortion. To address this bottleneck, we propose Geo-coder -- the first inverse programming framework for geometric images based on a multi-agent system. Our method innovatively decouples the process into geometric modeling via pixel-wise anchoring and metric-driven code evolution: Stage 1 leverages the complementary advantages of visual operators and large models to achieve precise capture of pixel coordinates and visual attributes; Stage 2 introduces a synthesis-rendering-validation closed loop, where bidirectional visual feedback drives the self-correction of code. Extensive experiments demonstrate that Geo-coder achieves a substantial lead in both geometric reconstruction accuracy and visual consistency. Notably, by effectively preserving the core geometric semantics, the images reconstructed with our method exhibit equivalent performance to the original ones in multimodal reasoning tasks, which fully validates the robustness of the framework. Finally, to further reduce research costs, we have open-sourced the Geo-coder dataset constructed on the GeoCode framework, which contains more than 1,500 samples. On this basis, we have also open-sourced the GeocodeLM model, laying a solid data and model foundation for subsequent research in this field.

</details>


### [249] [Humanizing AI Grading: Student-Centered Insights on Fairness, Trust, Consistency and Transparency](https://arxiv.org/abs/2602.07754)
*Bahare Riahi,Veronica Catete*

Main category: cs.AI

TL;DR: 本研究调查27名CS本科生对AI评分系统的看法，发现AI缺乏情境理解，建议AI应在人工监督下作为辅助工具，为AI教育应用提供伦理设计原则。


<details>
  <summary>Details</summary>
Motivation: 考察AI评分在编程教育中的伦理问题，特别是公平性、信任、一致性和透明度，理解学生对AI与人工评分的差异感知。

Method: 采用Jobin伦理框架，对27名本科生块编程期末项目进行AI评分与人工评分的对比分析，从学生视角评估AI评分系统的伦理维度。

Result: 学生对AI评分的主要担忧是其缺乏情境理解与个性化能力，认为AI无法像人类评分者那样提供细致、富有同理心的反馈。

Conclusion: 公平可信的AI评分系统应体现人类判断、灵活性和同理心，在人类监督下作为辅助工具使用。研究通过放大
学生声音，为设计人性化AI学习环境提供了伦理原则与实践建议。

Abstract: This study investigates students' perceptions of Artificial Intelligence (AI) grading systems in an undergraduate computer science course (n = 27), focusing on a block-based programming final project. Guided by the ethical principles framework articulated by Jobin (2019), our study examines fairness, trust, consistency, and transparency in AI grading by comparing AI-generated feedback with original human-graded feedback. Findings reveal concerns about AI's lack of contextual understanding and personalization. We recommend that equitable and trustworthy AI systems reflect human judgment, flexibility, and empathy, serving as supplementary tools under human oversight. This work contributes to ethics-centered assessment practices by amplifying student voices and offering design principles for humanizing AI in designed learning environments.

</details>


### [250] [Learning to Continually Learn via Meta-learning Agentic Memory Designs](https://arxiv.org/abs/2602.07755)
*Yiming Xiong,Shengran Hu,Jeff Clune*

Main category: cs.AI

TL;DR: 提出ALMA框架，通过元学习自动设计智能体记忆模块，取代手工设计，实现持续学习。实验表明，该框架在四个决策领域均优于现有手工设计的记忆模块，为构建自适应AI系统提供新路径。


<details>
  <summary>Details</summary>
Motivation: 基础模型的“无状态”特性限制了智能体系统的持续学习能力，而现有记忆模块多为固定的人工设计，难以适应真实世界任务的多样性和非平稳性。

Method: ALMA（智能体记忆设计的自动化元学习）框架：通过元智能体以开放式方式搜索可执行的代码形式的记忆设计，理论上可发现包括数据库模式及其检索更新机制的任意记忆设计。

Result: 在四个序贯决策领域的广泛实验表明，学习到的记忆设计在所有基准测试中均比最先进的精编记忆设计更有效、更高效。

Conclusion: ALMA向自我改进的AI系统迈出了重要一步，使智能体成为能够持续学习的自适应学习者（需确保安全开发和部署）。

Abstract: The statelessness of foundation models bottlenecks agentic systems' ability to continually learn, a core capability for long-horizon reasoning and adaptation. To address this limitation, agentic systems commonly incorporate memory modules to retain and reuse past experience, aiming for continual learning during test time. However, most existing memory designs are human-crafted and fixed, which limits their ability to adapt to the diversity and non-stationarity of real-world tasks. In this paper, we introduce ALMA (Automated meta-Learning of Memory designs for Agentic systems), a framework that meta-learns memory designs to replace hand-engineered memory designs, therefore minimizing human effort and enabling agentic systems to be continual learners across diverse domains. Our approach employs a Meta Agent that searches over memory designs expressed as executable code in an open-ended manner, theoretically allowing the discovery of arbitrary memory designs, including database schemas as well as their retrieval and update mechanisms. Extensive experiments across four sequential decision-making domains demonstrate that the learned memory designs enable more effective and efficient learning from experience than state-of-the-art human-crafted memory designs on all benchmarks. When developed and deployed safely, ALMA represents a step toward self-improving AI systems that learn to be adaptive, continual learners.

</details>


### [251] [Do Multi-Agents Dream of Electric Screens? Achieving Perfect Accuracy on AndroidWorld Through Task Decomposition](https://arxiv.org/abs/2602.07787)
*Pierre-Louis Favreau,Jean-Pierre Lo,Clement Guiguet,Charles Simon-Meunier,Nicolas Dehandschoewercker,Allen G. Roush,Judah Goldfeder,Ravid Shwartz-Ziv*

Main category: cs.AI

TL;DR: Minitap 是一个多智能体系统，首次在 AndroidWorld 基准测试中实现 100% 成功率，完成全部 116 项任务，表现超越人类（80%）。它通过六个专业智能体的认知分离、对文本输入的确定性后验证以及检测循环并改变策略的元认知推理，解决了单智能体的上下文污染、文本输入失败和重复动作循环等问题。


<details>
  <summary>Details</summary>
Motivation: 单智能体架构在 AndroidWorld 基准上表现不佳，主要原因是上下文污染（混合推理痕迹）、未被检测的文本输入失败以及没有逃脱机制的重复动作循环。为此，本文提出 Minitap 以克服这些失败。

Method: Minitap 采用六个专业智能体进行认知分离，每个智能体负责特定类型的推理；对文本输入进行确定性的后验证，确保输入与设备状态一致；并通过元认知推理检测动作循环并触发策略改变。

Result: Minitap 在 AndroidWorld 基准上实现 100% 成功率，完成全部 116 项任务，超越人类性能（80%）。消融实验显示，多智能体分解贡献 +21 分，确定性后验证贡献 +7 分，元认知贡献 +9 分。

Conclusion: Minitap 证明了多智能体分解和专门机制在解决复杂移动 UI 任务中的有效性，代码已开源。

Abstract: We present Minitap, a multi-agent system that achieves 100% success on the AndroidWorld benchmark, the first to fully solve all 116 tasks and surpassing human performance (80%). We first analyze why single-agent architectures fail: context pollution from mixed reasoning traces, silent text input failures undetected by the agent, and repetitive action loops without escape. Minitap addresses each failure through targeted mechanisms: cognitive separation across six specialized agents, deterministic post-validation of text input against device state, and meta-cognitive reasoning that detects cycles and triggers strategy changes. Ablations show multi-agent decomposition contributes +21 points over single-agent baselines; verified execution adds +7 points; meta-cognition adds +9 points. We release Minitap as open-source software. https://github.com/minitap-ai/mobile-use

</details>


### [252] [Data Darwinism Part I: Unlocking the Value of Scientific Data for Pre-training](https://arxiv.org/abs/2602.07824)
*Yiwei Qin,Zhen Huang,Tiantian Mi,Weiye Si,Chenyang Zhou,Qipeng Guo,Siyuan Feng,Pengfei Liu*

Main category: cs.AI

TL;DR: 提出Data Darwinism十级分类法(L0-L9)实现数据-模型协同演化，在900B词元的科学语料Darwin-Science上验证。通过L4-L5层级的大模型处理弥补原始科学文本的学习差距，在基准测试中获得2-8分的显著提升，并发布语料和模型。


<details>
  <summary>Details</summary>
Motivation: 数据质量决定基础模型性能，但缺乏系统性数据处理框架。需要结构化数据策展方法，使数据质量随模型能力进化，解决科学文本中存在学习障碍的问题。

Method: 1. 创建Data Darwinism十级分类体系概念化数据-模型协同演化；2. 构建900B词元Darwin-Science语料库(L0-L5)；3. 从零预训练不含科学内容的daVinci-origin-3B/7B作为纯净基线；4. 进行600B词元持续预训练；5. 在20+基准系统评估。

Result: - Darwin-Science模型在通用基准超基线+2.12(3B)和+2.95(7B)分
- 领域对齐任务提升+5.60和+8.40分
- 渐进至L5层级额外获得+1.36增益
- 验证L4-L5处理有效释放数据潜在价值，弥补学习差距

Conclusion: Data Darwinism框架证明高级数据处理能解锁数据潜在价值，数据-模型协同演化是有效范式。发布的语料和模型为未来系统的原则性开发奠定基础，推动数据策展与模型能力的共同进化。

Abstract: Data quality determines foundation model performance, yet systematic processing frameworks are lacking. We introduce Data Darwinism, a ten-level taxonomy (L0-L9) that conceptualizes data-model co-evolution: advanced models produce superior data for next-generation systems. We validate this on scientific literature by constructing Darwin-Science, a 900B-token corpus (L0-L5). We identify a learnability gap in raw scientific text, which we bridge via L4 (Generative Refinement) and L5 (Cognitive Completion) using frontier LLMs to explicate reasoning and terminology.
  To ensure rigorous attribution, we pre-trained daVinci-origin-3B/7B models from scratch, excluding scientific content to create contamination-free baselines. After 600B tokens of continued pre-training, Darwin-Science outperforms baselines by +2.12 (3B) and +2.95 (7B) points across 20+ benchmarks, rising to +5.60 and +8.40 points on domain-aligned tasks. Systematic progression to L5 yields a +1.36 total gain, confirming that higher-level processing unlocks latent data value. We release the Darwin-Science corpus and daVinci-origin models to enable principled, co-evolutionary development.

</details>


### [253] [Time Series Reasoning via Process-Verifiable Thinking Data Synthesis and Scheduling for Tailored LLM Reasoning](https://arxiv.org/abs/2602.07830)
*Jiahui Zhou,Dan Li,Boxin Li,Xiao Zhang,Erli Meng,Lin Li,Zhuomin Chen,Jian Lou,See-Kiong Ng*

Main category: cs.AI

TL;DR: VeriTime是一个专为时间序列推理定制的LLM框架，通过数据合成、智能调度和强化学习，使3B-4B的小模型达到媲美大型商业LLM的推理性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据应用广泛，但现有LLM在时间序列推理方面处于早期阶段，面临三大挑战：缺乏精心策划的思维链(CoT)训练数据、数据效率低下、缺少针对时间序列CoT数据的强化学习算法。

Method: 提出三阶段框架：(1) 数据合成管道构建可验证标注的TS-text多模态数据集；(2) 基于难度层次和任务分类的数据调度机制；(3) 两阶段强化微调，利用可验证过程级CoT数据进行细粒度多目标奖励优化。

Result: 在多样化时间序列推理任务上显著提升LLM性能，使3B/4B规模的小模型获得与更大规模商业LLM相当甚至更优的推理能力。

Conclusion: VeriTime通过系统化的数据构建和训练策略，有效解锁了LLM在时间序列领域的推理潜力，为资源高效的时间序列AI提供了可行路径。

Abstract: Time series is a pervasive data type across various application domains, rendering the reasonable solving of diverse time series tasks a long-standing goal. Recent advances in large language models (LLMs), especially their reasoning abilities unlocked through reinforcement learning (RL), have opened new opportunities for tackling tasks with long Chain-of-Thought (CoT) reasoning. However, leveraging LLM reasoning for time series remains in its infancy, hindered by the absence of carefully curated time series CoT data for training, limited data efficiency caused by underexplored data scheduling, and the lack of RL algorithms tailored for exploiting such time series CoT data. In this paper, we introduce VeriTime, a framework that tailors LLMs for time series reasoning through data synthesis, data scheduling, and RL training. First, we propose a data synthesis pipeline that constructs a TS-text multimodal dataset with process-verifiable annotations. Second, we design a data scheduling mechanism that arranges training samples according to a principled hierarchy of difficulty and task taxonomy. Third, we develop a two-stage reinforcement finetuning featuring fine-grained, multi-objective rewards that leverage verifiable process-level CoT data. Extensive experiments show that VeriTime substantially boosts LLM performance across diverse time series reasoning tasks. Notably, it enables compact 3B, 4B models to achieve reasoning capabilities on par with or exceeding those of larger proprietary LLMs.

</details>


### [254] [LQA: A Lightweight Quantized-Adaptive Framework for Vision-Language Models on the Edge](https://arxiv.org/abs/2602.07849)
*Xin Wang,Hualin Zhou,Sheng Guang Wang,Ting Dang,Yu Zhang,Hong Jia,Tao Gu*

Main category: cs.AI

TL;DR: 针对边缘设备部署视觉-语言模型的资源约束和分布偏移问题，提出LQA框架，通过选择性混合量化和无梯度测试时适应，实现高性能低内存消耗的部署方案。


<details>
  <summary>Details</summary>
Motivation: 边缘设备部署视觉-语言模型受限于资源约束，且在分布偏移下性能显著下降；现有测试时适应方法计算开销过大，难以在资源受限的设备上部署。

Method: 提出LQA轻量化量化自适应框架，采用模态感知量化策略，结合选择性混合量化(SHQ)和量化无梯度测试时适应机制。

Result: 实验表明LQA在合成和真实分布偏移下整体性能提升4.5%，内存占用低于全精度模型，相比基于梯度的TTA方法内存使用降低高达19.9倍，覆盖七个开源数据集。

Conclusion: LQA为边缘设备提供了鲁棒、隐私保护且高效的视觉-语言模型部署实用方案。

Abstract: Deploying Vision-Language Models (VLMs) on edge devices is challenged by resource constraints and performance degradation under distribution shifts. While test-time adaptation (TTA) can counteract such shifts, existing methods are too resource-intensive for on-device deployment. To address this challenge, we propose LQA, a lightweight, quantized-adaptive framework for VLMs that combines a modality-aware quantization strategy with gradient-free test-time adaptation. We introduce Selective Hybrid Quantization (SHQ) and a quantized, gradient-free adaptation mechanism to enable robust and efficient VLM deployment on resource-constrained hardware. Experiments across both synthetic and real-world distribution shifts show that LQA improves overall adaptation performance by 4.5\%, uses less memory than full-precision models, and significantly outperforms gradient-based TTA methods, achieving up to 19.9$\times$ lower memory usage across seven open-source datasets. These results demonstrate that LQA offers a practical pathway for robust, privacy-preserving, and efficient VLM deployment on edge devices.

</details>


### [255] [Emergent Misalignment is Easy, Narrow Misalignment is Hard](https://arxiv.org/abs/2602.07852)
*Anna Soligo,Edward Turner,Senthooran Rajamanoharan,Neel Nanda*

Main category: cs.AI

TL;DR: 微调大型语言模型于狭窄有害数据集会导致涌现性错准（给出跨领域的类"邪恶"回应），专家未能预测此现象。研究发现通用错准的线性表征更稳定高效，为理解LLM归纳偏差提供案例。


<details>
  <summary>Details</summary>
Motivation: 专家调查未能预测狭窄有害数据微调会导致模型涌现出广泛的类"邪恶"回应，这揭示了我们对LLM学习和泛化的归纳偏差理解不足。需要深入探究模型为何倾向于学习通用错误解而非仅狭窄任务解。

Method: 1) 以涌现性错准（EM）为案例研究；2) 验证不同EM微调收敛到相同的通用错准线性表征；3) 通过引入KL散度损失学习狭窄解的线性表征；4) 比较两种表征在损失、鲁棒性和预训练分布影响力方面的差异。

Result: 1) 通用错准解相比狭窄解具有更低损失、更强扰动鲁棒性，且在预训练分布中更具影响力；2) 成功分离出通用错准的具体线性表征，可用于监控和缓解；3) 提供了研究归纳偏差如何塑造LLM泛化的详细案例和初步指标。

Conclusion: 该工作揭示了LLM在微调过程中倾向于学习更稳定的通用错误解而非狭窄任务解，这为理解模型行为提供了新视角。研究成果可用于开发更好的监控缓解策略，并为未来研究归纳偏差与泛化的关系提供了方法论框架。

Abstract: Finetuning large language models on narrowly harmful datasets can cause them to become emergently misaligned, giving stereotypically `evil' responses across diverse unrelated settings. Concerningly, a pre-registered survey of experts failed to predict this result, highlighting our poor understanding of the inductive biases governing learning and generalisation in LLMs. We use emergent misalignment (EM) as a case study to investigate these inductive biases and find that models can just learn the narrow dataset task, but that the general solution appears to be more stable and more efficient. To establish this, we build on the result that different EM finetunes converge to the same linear representation of general misalignment, which can be used to mediate misaligned behaviour. We find a linear representation of the narrow solution also exists, and can be learned by introducing a KL divergence loss. Comparing these representations reveals that general misalignment achieves lower loss, is more robust to perturbations, and is more influential in the pre-training distribution. This work isolates a concrete representation of general misalignment for monitoring and mitigation. More broadly, it offers a detailed case study and preliminary metrics for investigating how inductive biases shape generalisation in LLMs. We open-source all code, datasets and model finetunes.

</details>


### [256] [ToolSelf: Unifying Task Execution and Self-Reconfiguration via Tool-Driven Intrinsic Adaptation](https://arxiv.org/abs/2602.07883)
*Jingqi Zhou,Sheng Wang,DeZhao Deng,Junwen Lu,Junwei Su,Qintong Li,Jiahui Gao,Hao Wu,Jiyue Jiang,Lingpeng Kong,Chuan Wu*

Main category: cs.AI

TL;DR: ToolSelf通过将配置更新工具化，实现LLM智能体运行时自我重构，在统一行动空间内使智能体从被动执行转为主动自我管理，平均性能提升24.1%。


<details>
  <summary>Details</summary>
Motivation: LLM智能体受限于静态配置，无法适应动态任务变化；现有手动或启发式方法泛化能力差、优化不连贯，亟需自适应重构机制。

Method: 提出ToolSelf范式，将配置更新抽象为可调用的工具，统一任务执行与自我调整；设计配置感知两阶段训练(CAT)，结合拒绝采样微调和轨迹级强化学习内化元能力。

Result: 在多基准测试中，ToolSelf性能媲美专用工作流，可泛化至新任务，实现24.1%的平均性能提升。

Conclusion: ToolSelf实现了从外部规则到内在参数的转变，将智能体从被动执行者转变为任务和自我的双重管理者，为真正自适应智能体发展提供了新路径。

Abstract: Agentic systems powered by Large Language Models (LLMs) have demonstrated remarkable potential in tackling complex, long-horizon tasks. However, their efficacy is fundamentally constrained by static configurations governing agent behaviors, which are fixed prior to execution and fail to adapt to evolving task dynamics. Existing approaches, relying on manual orchestration or heuristic-based patches, often struggle with poor generalization and fragmented optimization. To transcend these limitations, we propose ToolSelf, a novel paradigm enabling tool-driven runtime self-reconfiguration. By abstracting configuration updates as a callable tool, ToolSelf unifies task execution and self-adjustment into a single action space, achieving a phase transition from external rules to intrinsic parameters. Agents can thereby autonomously update their sub-goals and context based on task progression, and correspondingly adapt their strategy and toolbox, transforming from passive executors into dual managers of both task and self. We further devise Configuration-Aware Two-stage Training (CAT), combining rejection sampling fine-tuning with trajectory-level reinforcement learning to internalize this meta-capability. Extensive experiments across diverse benchmarks demonstrate that ToolSelf rivals specialized workflows while generalizing to novel tasks, achieving a 24.1% average performance gain and illuminating a path toward truly self-adaptive agents.

</details>


### [257] [MedCoG: Maximizing LLM Inference Density in Medical Reasoning via Meta-Cognitive Regulation](https://arxiv.org/abs/2602.07905)
*Yu Zhao,Hao Guan,Yongcheng Jing,Ying Zhang,Dacheng Tao*

Main category: cs.AI

TL;DR: 本研究提出MedCoG医疗元认知智能体，通过大模型对自身知识状态的自省能力动态调控多类型知识使用，在五个医疗基准测试上实现5.5倍的推理效率提升，为突破医疗推理中的缩放定律瓶颈提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂医疗推理中展现出潜力，但面临推理缩放定律下的收益递减问题。现有研究通过增加各类知识来增强模型，但额外成本转化为准确率的效率尚不明确，亟需一种机制来智能调控推理过程以平衡成本与性能。

Method: 提出MedCoG（Medical Meta-Cognition Agent with Knowledge Graph）框架，利用大模型的元认知能力（对任务复杂度、熟悉度、知识密度的自评估）动态调控程序性、情景性和事实性知识的调用。通过LLM中心化的按需推理机制，避免无差别缩放并过滤干扰知识。

Result: 在五个医疗困难基准测试集上验证了方法的有效性，实现了5.5倍的推理密度（即推理效率）。通过引入推理密度指标量化成本效益比，并发现元认知调控能显著提升理论有效成本与实际成本的比率。

Conclusion: 元认知调控机制能有效缓解医疗推理中的缩放定律限制，通过智能的知识选择和成本调控，在保证准确率的同时大幅提升推理效率。Oracle研究表明该方法在理论上仍有巨大提升空间，为未来医疗AI的精细化推理提供了重要方向。

Abstract: Large Language Models (LLMs) have shown strong potential in complex medical reasoning yet face diminishing gains under inference scaling laws. While existing studies augment LLMs with various knowledge types, it remains unclear how effectively the additional costs translate into accuracy. In this paper, we explore how meta-cognition of LLMs, i.e., their self-awareness of their own knowledge states, can regulate the reasoning process. Specifically, we propose MedCoG, a Medical Meta-Cognition Agent with Knowledge Graph, where the meta-cognitive assessments of task complexity, familiarity, and knowledge density dynamically regulate utilization of procedural, episodic, and factual knowledge. The LLM-centric on-demand reasoning aims to mitigate scaling laws by (1) reducing costs via avoiding indiscriminate scaling, (2) improving accuracy via filtering out distractive knowledge. To validate this, we empirically characterize the scaling curve and introduce inference density to quantify inference efficiency, defined as the ratio of theoretically effective cost to actual cost. Experiments demonstrate the effectiveness and efficiency of MedCoG on five hard sets of medical benchmarks, yielding 5.5x inference density. Furthermore, the Oracle study highlights the significant potential of meta-cognitive regulation.

</details>


### [258] [Selective Fine-Tuning for Targeted and Robust Concept Unlearning](https://arxiv.org/abs/2602.07919)
*Mansi,Avinash Kori,Francesca Toni,Soteris Demetriou*

Main category: cs.AI

TL;DR: Proposing TRUST, a dynamic concept unlearning method for diffusion models that selectively finetunes target neurons with Hessian regularization, achieving robustness, quality preservation, and speed improvements over SOTA baselines.


<details>
  <summary>Details</summary>
Motivation: Text-guided diffusion models can generate harmful content; existing concept unlearning methods are limited to individual concepts, require expensive full finetuning, or use static localization that reduces utility.

Method: TRUST (Targeted Robust Selective fine Tuning) dynamically estimates target concept neurons and performs selective finetuning using Hessian-based regularization to unlearn harmful concepts.

Result: TRUST is robust against adversarial prompts, preserves generation quality significantly, runs much faster than state-of-the-art methods, and unlearns individual concepts, concept combinations, and conditional concepts without special regularization.

Conclusion: TRUST successfully enables dynamic, efficient, and comprehensive concept unlearning while maintaining model utility, overcoming key limitations of prior approaches.

Abstract: Text guided diffusion models are used by millions of users, but can be easily exploited to produce harmful content. Concept unlearning methods aim at reducing the models' likelihood of generating harmful content. Traditionally, this has been tackled at an individual concept level, with only a handful of recent works considering more realistic concept combinations. However, state of the art methods depend on full finetuning, which is computationally expensive. Concept localisation methods can facilitate selective finetuning, but existing techniques are static, resulting in suboptimal utility. In order to tackle these challenges, we propose TRUST (Targeted Robust Selective fine Tuning), a novel approach for dynamically estimating target concept neurons and unlearning them through selective finetuning, empowered by a Hessian based regularization. We show experimentally, against a number of SOTA baselines, that TRUST is robust against adversarial prompts, preserves generation quality to a significant degree, and is also significantly faster than the SOTA. Our method achieves unlearning of not only individual concepts but also combinations of concepts and conditional concepts, without any specific regularization.

</details>


### [259] [IV Co-Scientist: Multi-Agent LLM Framework for Causal Instrumental Variable Discovery](https://arxiv.org/abs/2602.07943)
*Ivaxi Sheth,Zhijing Jin,Bryan Wilder,Dominik Janzing,Mario Fritz*

Main category: cs.AI

TL;DR: This paper investigates whether large language models (LLMs) can assist in identifying valid instrumental variables (IVs) for causal inference, introducing a multi-agent system called IV Co-Scientist that shows promising results in discovering valid IVs from observational data.


<details>
  <summary>Details</summary>
Motivation: Identifying valid instrumental variables requires interdisciplinary knowledge, creativity, and contextual understanding, making it a non-trivial task that the paper explores whether LLMs can help address.

Method: A two-stage evaluation framework: (1) testing if LLMs can recover well-established instruments from literature, and (2) evaluating if LLMs can identify and avoid discredited instruments. The authors also developed IV Co-Scientist, a multi-agent system that proposes, critiques, and refines IVs, plus a statistical test for contextualizing consistency without ground truth.

Result: The results demonstrate the potential of LLMs to discover valid instrumental variables from large observational databases.

Conclusion: LLMs show promise in aiding the identification of valid instrumental variables, suggesting they could become valuable tools for causal inference research when properly guided through multi-agent collaboration.

Abstract: In the presence of confounding between an endogenous variable and the outcome, instrumental variables (IVs) are used to isolate the causal effect of the endogenous variable. Identifying valid instruments requires interdisciplinary knowledge, creativity, and contextual understanding, making it a non-trivial task. In this paper, we investigate whether large language models (LLMs) can aid in this task. We perform a two-stage evaluation framework. First, we test whether LLMs can recover well-established instruments from the literature, assessing their ability to replicate standard reasoning. Second, we evaluate whether LLMs can identify and avoid instruments that have been empirically or theoretically discredited. Building on these results, we introduce IV Co-Scientist, a multi-agent system that proposes, critiques, and refines IVs for a given treatment-outcome pair. We also introduce a statistical test to contextualize consistency in the absence of ground truth. Our results show the potential of LLMs to discover valid instrumental variables from a large observational database.

</details>


### [260] [LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth](https://arxiv.org/abs/2602.07962)
*Weihao Zeng,Yuzhen Huang,Junxian He*

Main category: cs.AI

TL;DR: 该论文提出LOCA-bench基准测试，用于评估大语言模型在长上下文环境中的智能体表现，发现上下文管理技术可显著缓解"上下文腐化"问题。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文基准测试主要关注单次信息检索能力，而现实场景中LLM需要作为智能体在动态增长的上下文中执行多步任务，但长上下文会导致模型可靠性下降，即"上下文腐化"现象。

Method: 设计LOCA-bench基准测试，通过自动化控制环境状态来调节智能体的上下文长度，支持在保持任务语义不变的前提下无限扩展上下文，并评估结合不同上下文管理策略的模型与框架组合。

Result: 随着环境状态复杂度增加，智能体性能普遍下降；但采用先进的上下文管理技术能够显著提升整体成功率。

Conclusion: LOCA-bench为长上下文智能体场景提供了可评估的平台，证实上下文管理是解决长序列任务性能衰退的关键方向。

Abstract: Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as the amount of context grows, their reliability often deteriorates, a phenomenon known as "context rot". Existing long-context benchmarks primarily focus on single-step settings that evaluate a model's ability to retrieve information from a long snippet. In realistic scenarios, however, LLMs often need to act as agents that explore environments, follow instructions and plans, extract useful information, and predict correct actions under a dynamically growing context. To assess language agents in such settings, we introduce LOCA-bench (a benchmark for LOng-Context Agents). Given a task prompt, LOCA-bench leverages automated and scalable control of environment states to regulate the agent's context length. This design enables LOCA-bench to extend the context length potentially to infinity in a controlled way while keeping the underlying task semantics fixed. LOCA-bench evaluates language agents as a combination of models and scaffolds, including various context management strategies. While agent performance generally degrades as the environment states grow more complex, advanced context management techniques can substantially improve the overall success rate. We open-source LOCA-bench to provide a platform for evaluating models and scaffolds in long-context, agentic scenarios: https://github.com/hkust-nlp/LOCA-bench

</details>


### [261] [Free(): Learning to Forget in Malloc-Only Reasoning Models](https://arxiv.org/abs/2602.08030)
*Yilun Zheng,Dongyang Ma,Tian Liang,Jiahao Xu,Xinting Huang,Lijie Chen,Haitao Mi,Yan Wang*

Main category: cs.AI

TL;DR: Free()LM introduces a self-forgetting mechanism via a LoRA adapter to prune redundant thinking tokens, solving performance degradation in long reasoning chains and achieving 3.3% average improvement across models with new SOTA on IMOanswerBench.


<details>
  <summary>Details</summary>
Motivation: Reasoning models face a paradox where excessive thinking tokens degrade performance instead of improving it, due to standard LLMs operating as "malloc-only" engines that accumulate all information without pruning obsolete steps.

Method: Proposes Free()LM with a Free-Module (plug-and-play LoRA adapter) that enables intrinsic self-forgetting by iteratively switching between reasoning and cleaning modes to dynamically identify and prune useless context chunks.

Result: Achieves 3.3% average improvement across model scales (8B-685B), establishes new SOTA on IMOanswerBench, and restores performance from 0% to 50% on long-horizon tasks where baseline models collapse completely.

Conclusion: Sustainable intelligence requires both the power to think and the freedom to forget, as demonstrated by Free()LM's ability to maintain compact, noise-free reasoning states through dynamic pruning.

Abstract: Reasoning models enhance problem-solving by scaling test-time compute, yet they face a critical paradox: excessive thinking tokens often degrade performance rather than improve it. We attribute this to a fundamental architectural flaw: standard LLMs operate as "malloc-only" engines, continuously accumulating valid and redundant steps alike without a mechanism to prune obsolete information. To break this cycle, we propose Free()LM, a model that introduces an intrinsic self-forgetting capability via the Free-Module, a plug-and-play LoRA adapter. By iteratively switching between reasoning and cleaning modes, Free()LM dynamically identifies and prunes useless context chunks, maintaining a compact and noise-free state.
  Extensive experiments show that Free()LM provides consistent improvements across all model scales (8B to 685B). It achieves a 3.3% average improvement over top-tier reasoning baselines, even establishing a new SOTA on IMOanswerBench using DeepSeek V3.2-Speciale. Most notably, in long-horizon tasks where the standard Qwen3-235B-A22B model suffers a total collapse (0% accuracy), Free()LM restores performance to 50%. Our findings suggest that sustainable intelligence requires the freedom to forget as much as the power to think.

</details>


### [262] [Securing Dual-Use Pathogen Data of Concern](https://arxiv.org/abs/2602.08061)
*Doni Bloomfield,Allison Berke,Moritz S. Hanke,Aaron Maiwald,James R. M. Black,Toby Webster,Tina Hernandez-Boussard,Oliver M. Crook,Jassi Pannu*

Main category: cs.AI

TL;DR: 提出五层级生物安全数据分级框架(BDL)，根据病原体数据训练AI的潜在生物武器风险实施分级管控，强调在算力普及时代数据管控是防范生物AI滥用的最有效干预手段


<details>
  <summary>Details</summary>
Motivation: AI生物模型训练数据可能被用于开发生物武器等有害应用，100多名研究人员在Asilomar会议呼吁建立数据管控机制，需系统性框架应对双用途病原体数据风险

Method: 构建五层级生物安全数据水平(BDL)框架，按数据对高风险AI能力的潜在贡献度分类，并为每个风险等级匹配相应技术限制措施

Result: 建立包含具体数据类型的分级体系，提出对应各层级的精准技术管控方案，并设计新创双用途病原体数据的治理框架

Conclusion: 在计算资源广泛可及的时代，针对训练数据的管控可能是防止生物AI能力扩散的最有效干预措施，需通过分级框架实施精准治理

Abstract: Training data is an essential input into creating competent artificial intelligence (AI) models. AI models for biology are trained on large volumes of data, including data related to biological sequences, structures, images, and functions. The type of data used to train a model is intimately tied to the capabilities it ultimately possesses--including those of biosecurity concern. For this reason, an international group of more than 100 researchers at the recent 50th anniversary Asilomar Conference endorsed data controls to prevent the use of AI for harmful applications such as bioweapons development. To help design such controls, we introduce a five-tier Biosecurity Data Level (BDL) framework for categorizing pathogen data. Each level contains specific data types, based on their expected ability to contribute to capabilities of concern when used to train AI models. For each BDL tier, we propose technical restrictions appropriate to its level of risk. Finally, we outline a novel governance framework for newly created dual-use pathogen data. In a world with widely accessible computational and coding resources, data controls may be among the most high-leverage interventions available to reduce the proliferation of concerning biological AI capabilities.

</details>


### [263] [Objective Decoupling in Social Reinforcement Learning: Recovering Ground Truth from Sycophantic Majorities](https://arxiv.org/abs/2602.08092)
*Majid Ghasemi,Mark Crowley*

Main category: cs.AI

TL;DR: 本文挑战了强化学习中的"教条4"——人类反馈基本真实这一前提，证明其在评估者可能奉承、懒惰或对抗的社交环境中会导致目标解耦（结构性故障，使智能体目标永久偏离真实目标），并提出认知源对齐（ESA）方法。该方法使用稀疏安全公理来评判反馈来源而非信号本身，即使多数评估者有偏见也能保证收敛到真实目标。


<details>
  <summary>Details</summary>
Motivation: 当代AI对齐策略依赖一个脆弱前提：人类反馈虽然嘈杂但基本真实。该假设在静态环境成立，但在社交环境中失效，导致结构性错配。

Method: 提出认知源对齐（ESA），利用稀疏安全公理评判反馈来源（"评判评判者"），而非像传统鲁棒方法那样依赖统计共识（相信多数）。

Result: 理论证明ESA即使在被多数评估者偏见影响下也能保证收敛到真实目标；实证显示传统共识方法在多数合谋时失败，而ESA能成功恢复最优策略。

Conclusion: 识别并形式化了强化学习中的目标解耦这一关键故障模式，提供了可证明对偏见多数鲁棒的解决方案ESA，解决了当前AI对齐策略的根本局限。

Abstract: Contemporary AI alignment strategies rely on a fragile premise: that human feedback, while noisy, remains a fundamentally truthful signal. In this paper, we identify this assumption as Dogma 4 of Reinforcement Learning (RL). We demonstrate that while this dogma holds in static environments, it fails in social settings where evaluators may be sycophantic, lazy, or adversarial. We prove that under Dogma 4, standard RL agents suffer from what we call Objective Decoupling, a structural failure mode where the agent's learned objective permanently separates from the latent ground truth, guaranteeing convergence to misalignment. To resolve this, we propose Epistemic Source Alignment (ESA). Unlike standard robust methods that rely on statistical consensus (trusting the majority), ESA utilizes sparse safety axioms to judge the source of the feedback rather than the signal itself. We prove that this "judging the judges" mechanism guarantees convergence to the true objective, even when a majority of evaluators are biased. Empirically, we show that while traditional consensus methods fail under majority collusion, our approach successfully recovers the optimal policy.

</details>


### [264] [Interpretable Failure Analysis in Multi-Agent Reinforcement Learning Systems](https://arxiv.org/abs/2602.08104)
*Risal Shahriar Shefin,Debashis Gupta,Thai Le,Sarra Alqahtani*

Main category: cs.AI

TL;DR: 提出两阶段梯度框架实现可解释MARL故障诊断：通过泰勒余项分析检测初始故障源(Patient-0)，利用评论家导数几何分析构建传播图谱，在500+100个episode测试中达到88.2-99.4%准确率


<details>
  <summary>Details</summary>
Motivation: 安全关键领域MARL应用日益广泛，但可解释的故障检测与归因方法仍不成熟，需要诊断级联故障传播路径和识别真实初始故障源

Method: 两阶段梯度框架：阶段一用泰勒余项分析策略梯度成本实现可解释的个体故障检测并标记Patient-0候选；阶段二通过评论家导数的一阶敏感性和二阶曲率几何分析，在因果窗口上构建可解释的传播图谱

Result: 在Simple Spread(3/5智能体)500 episodes和StarCraft II 100 episodes上测试MADDPG/HATRPO，Patient-0检测准确率88.2-99.4%，提供可解释的几何证据并解释下游优先检测异常

Conclusion: 该框架超越黑盒检测，实现可解释的梯度级取证分析，为安全关键MARL系统的级联故障诊断提供实用工具

Abstract: Multi-Agent Reinforcement Learning (MARL) is increasingly deployed in safety-critical domains, yet methods for interpretable failure detection and attribution remain underdeveloped. We introduce a two-stage gradient-based framework that provides interpretable diagnostics for three critical failure analysis tasks: (1) detecting the true initial failure source (Patient-0); (2) validating why non-attacked agents may be flagged first due to domino effects; and (3) tracing how failures propagate through learned coordination pathways. Stage 1 performs interpretable per-agent failure detection via Taylor-remainder analysis of policy-gradient costs, declaring an initial Patient-0 candidate at the first threshold crossing. Stage 2 provides validation through geometric analysis of critic derivatives-first-order sensitivity and directional second-order curvature aggregated over causal windows to construct interpretable contagion graphs. This approach explains "downstream-first" detection anomalies by revealing pathways that amplify upstream deviations. Evaluated across 500 episodes in Simple Spread (3 and 5 agents) and 100 episodes in StarCraft II using MADDPG and HATRPO, our method achieves 88.2-99.4% Patient-0 detection accuracy while providing interpretable geometric evidence for detection decisions. By moving beyond black-box detection to interpretable gradient-level forensics, this framework offers practical tools for diagnosing cascading failures in safety-critical MARL systems.

</details>


### [265] [RECUR: Resource Exhaustion Attack via Recursive-Entropy Guided Counterfactual Utilization and Reflection](https://arxiv.org/abs/2602.08214)
*Ziwei Wang,Yuanhe Zhang,Jing Chen,Zhenhong Zhou,Ruichao Liang,Ruiying Du,Ju Jia,Cong Wu,Yang Liu*

Main category: cs.AI

TL;DR: 该论文提出RECUR攻击，通过递归熵量化大型推理模型的反思资源消耗风险，利用反事实问题诱导模型过度反思，导致输出长度增加11倍、吞吐量下降90%。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽能处理复杂任务，但显式推理消耗大量资源，且反思环节存在过度反思风险，但这一内在安全漏洞尚未被充分研究。

Method: 提出"递归熵"度量反思过程中的资源消耗风险，并基于此设计RECUR攻击，通过引导模型进行反事实利用和反思来触发资源耗尽。

Result: 正常推理时递归熵呈下降趋势，而RECUR攻击破坏该趋势，使输出长度最高提升11倍，吞吐量下降90%，成功暴露模型的推理安全缺陷。

Conclusion: 研究揭示了推理过程本身的安全隐患，为构建更健壮的推理模型提供了新视角。

Abstract: Large Reasoning Models (LRMs) employ reasoning to address complex tasks. Such explicit reasoning requires extended context lengths, resulting in substantially higher resource consumption. Prior work has shown that adversarially crafted inputs can trigger redundant reasoning processes, exposing LRMs to resource-exhaustion vulnerabilities. However, the reasoning process itself, especially its reflective component, has received limited attention, even though it can lead to over-reflection and consume excessive computing power. In this paper, we introduce Recursive Entropy to quantify the risk of resource consumption in reflection, thereby revealing the safety issues inherent in inference itself. Based on Recursive Entropy, we introduce RECUR, a resource exhaustion attack via Recursive Entropy guided Counterfactual Utilization and Reflection. It constructs counterfactual questions to verify the inherent flaws and risks of LRMs. Extensive experiments demonstrate that, under benign inference, recursive entropy exhibits a pronounced decreasing trend. RECUR disrupts this trend, increasing the output length by up to 11x and decreasing throughput by 90%. Our work provides a new perspective on robust reasoning.

</details>


### [266] [Weak-Driven Learning: How Weak Agents make Strong Agents Stronger](https://arxiv.org/abs/2602.08222)
*Zehao Chen,Gongxun Li,Tianxiang Ai,Yifei Li,Zixuan Huang,Wang Zhou,Fuzhen Zhuang,Xianglong Liu,Jianxin Li,Deqing Wang,Yikun Ban*

Main category: cs.AI

TL;DR: 提出WMSS方法，利用模型弱检查点中的可恢复学习间隙，通过补偿学习突破大型语言模型后训练的性能饱和瓶颈。


<details>
  <summary>Details</summary>
Motivation: 观察到大型语言模型在后训练优化中存在饱和瓶颈问题，模型高度自信后继续训练收益递减，但模型历史弱状态中潜藏着有价值的监督信号。

Method: WMSS范式：通过熵动态识别可恢复的学习间隙，利用弱检查点指导强模型的补偿学习，实现超越常规后训练饱和的持续优化。

Result: 在数学推理和代码生成数据集上，该方法实现有效性能提升，且不增加任何推理成本。

Conclusion: 该方法证明弱模型检查点能成为强化强模型的有效资源，为突破后训练性能极限提供了新思路。

Abstract: As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.

</details>


### [267] [InfiCoEvalChain: A Blockchain-Based Decentralized Framework for Collaborative LLM Evaluation](https://arxiv.org/abs/2602.08229)
*Yifan Yang,Jinjia Li,Kunxi Li,Puhao Zheng,Yuanyi Wang,Zheyan Qu,Yang Yu,Jianmin Wu,Ming Li,Hongxia Yang*

Main category: cs.AI

TL;DR: 该论文发现当前大语言模型评估存在严重统计不稳定性：同一模型10次运行的差异(1.67)竟超过排行榜前十模型的差距(0.91)。为此提出基于区块链的去中心化评估框架，通过全球验证者异构计算节点和激励系统实现多方共识，将标准差降至0.28，大幅提升评估可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型评估存在中心化黑箱、过拟合和硬件差异导致的方差问题，现有排行榜的统计显著性存疑，无法可靠反映模型真实性能差距。

Method: 提出去中心化评估框架，利用区块链协议激励全球贡献者作为独立验证者，通过异构计算节点进行大规模多样化基准测试，建立稳健的奖励系统确保评估完整性，实现多方共识机制。

Result: 去中心化框架将同一模型10次运行的标准差从1.67降至0.28，显著优于传统方法，提供更高统计置信度的模型排名；平台已完全实现即将开源。

Conclusion: 该框架成功将评估从"中心化黑箱"转变为"去中心化背书"，通过多样化推理环境和集体验证产生更稳定、代表性的评估指标，为大语言模型评估提供了更可靠的解决方案。

Abstract: The rapid advancement of large language models (LLMs) demands increasingly reliable evaluation, yet current centralized evaluation suffers from opacity, overfitting, and hardware-induced variance. Our empirical analysis reveals an alarming inconsistency in existing evaluations: the standard deviation across ten repeated runs of a single model on HumanEval (1.67) actually exceeds the performance gap among the top-10 models on the official leaderboard (0.91), rendering current rankings statistically precarious. To mitigate these instabilities, we propose a decentralized evaluation framework that enables hardware and parameter diversity through large-scale benchmarking across heterogeneous compute nodes. By leveraging the blockchain-based protocol, the framework incentivizes global contributors to act as independent validators, using a robust reward system to ensure evaluation integrity and discourage dishonest participation. This collective verification transforms evaluation from a "centralized black box" into a "decentralized endorsement" where multi-party consensus and diverse inference environments yield a more stable, representative metric. Experimental results demonstrate that the decentralized evaluation framework reduces the standard deviation across ten runs on the same model to 0.28. This significant improvement over conventional frameworks ensures higher statistical confidence in model rankings. We have completely implemented this platform and will soon release it to the community.

</details>


### [268] [PTS-SNN: A Prompt-Tuned Temporal Shift Spiking Neural Networks for Efficient Speech Emotion Recognition](https://arxiv.org/abs/2602.08240)
*Xun Su,Huamin Wang,Qi Zhang*

Main category: cs.AI

TL;DR: 提出Prompt-Tuned Spiking Neural Networks (PTS-SNN)框架，通过冻结的自监督学习 backbone和参数高效设计，解决SNN与连续语音表示的分布失配问题，在保持ANN精度的同时实现边缘设备部署


<details>
  <summary>Details</summary>
Motivation: 传统语音情感识别模型计算成本高，难以部署在资源受限的边缘设备；SNN虽能效高，但与连续自监督学习表示存在分布失配，导致信息编码能力下降和功能沉默或饱和问题

Method: 提出PTS-SNN框架，包含：1) 无参数通道位移的时序移位脉冲编码器捕获局部时序依赖；2) 上下文感知膜电位校准策略，通过脉冲稀疏线性注意力将全局语义聚合到可学习软提示中，动态调控PLIF神经元偏置电压

Result: 在IEMOCAP数据集上达到73.34%准确率，与ANN性能相当；仅需1.19M可训练参数，每样本推理能耗0.35 mJ，在五个多语言数据集上验证有效性

Conclusion: PTS-SNN实现了ANN级别的识别精度，同时具备极低参数开销和超高能效，为边缘设备上的实时语音情感识别提供了可行方案

Abstract: Speech Emotion Recognition (SER) is widely deployed in Human-Computer Interaction, yet the high computational cost of conventional models hinders their implementation on resource-constrained edge devices. Spiking Neural Networks (SNNs) offer an energy-efficient alternative due to their event-driven nature; however, their integration with continuous Self-Supervised Learning (SSL) representations is fundamentally challenged by distribution mismatch, where high-dynamic-range embeddings degrade the information coding capacity of threshold-based neurons. To resolve this, we propose Prompt-Tuned Spiking Neural Networks (PTS-SNN), a parameter-efficient neuromorphic adaptation framework that aligns frozen SSL backbones with spiking dynamics. Specifically, we introduce a Temporal Shift Spiking Encoder to capture local temporal dependencies via parameter-free channel shifts, establishing a stable feature basis. To bridge the domain gap, we devise a Context-Aware Membrane Potential Calibration strategy. This mechanism leverages a Spiking Sparse Linear Attention module to aggregate global semantic context into learnable soft prompts, which dynamically regulate the bias voltages of Parametric Leaky Integrate-and-Fire (PLIF) neurons. This regulation effectively centers the heterogeneous input distribution within the responsive firing range, mitigating functional silence or saturation. Extensive experiments on five multilingual datasets (e.g., IEMOCAP, CASIA, EMODB) demonstrate that PTS-SNN achieves 73.34\% accuracy on IEMOCAP, comparable to competitive Artificial Neural Networks (ANNs), while requiring only 1.19M trainable parameters and 0.35 mJ inference energy per sample.

</details>


### [269] [Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs](https://arxiv.org/abs/2602.08241)
*Siqu Ou,Tianrui Wan,Zhiyuan Zhao,Junyu Gao,Xuelong Li*

Main category: cs.AI

TL;DR: SAYO模型通过引入区域级视觉注意力奖励的强化学习框架，解决多模态大语言模型在思维链推理中视觉关注薄弱和错误传播的问题，在多模态基准测试中实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有思维链推理方法依赖长文本推理轨迹，但多模态大语言模型缺乏稳定的视觉注意力策略学习机制，早期视觉对齐错误会在后续推理中持续传播，导致推断失败。根本原因在于训练过程中视觉注意力的信用分配不足。

Method: 提出SAYO模型，采用强化学习框架并设计区域级视觉注意力奖励机制，将优化信号与基于视觉的推理步骤明确对齐，使模型能够学习更可靠的注意力行为。

Result: 在多个多模态基准测试上，SAYO在多样化的推理和感知任务中表现出持续的性能提升。

Conclusion: 通过区域级视觉注意力奖励的强化学习方法能有效解决多模态大语言模型推理过程中的视觉错位问题，显著提升模型推理的稳定性和准确性。

Abstract: While chain-of-thought (CoT) reasoning has substantially improved multimodal large language models (MLLMs) on complex reasoning tasks, existing approaches largely rely on long textual reasoning trajectories and provide limited mechanisms for learning stable visual attention policies. Our analysis shows that current MLLMs exhibit weak visual focus: early-stage visual misalignment is rarely corrected during subsequent reasoning, leading to error propagation and failed inferences. We argue that this limitation stems from inadequate credit assignment for visual attention during training. To address this issue, we propose SAYO, a visual reasoning model trained with a reinforcement learning (RL) framework that introduces a region-level visual attention-based reward. This reward explicitly aligns optimization signals with visually grounded reasoning steps, enabling the model to learn more reliable attention behaviors. Extensive experiments across multiple multimodal benchmarks demonstrate that SAYO consistently improves performance on diverse reasoning and perception tasks.

</details>


### [270] [G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design](https://arxiv.org/abs/2602.08253)
*Baoyun Zhao,He Wang,Liang Zeng*

Main category: cs.AI

TL;DR: 提出G-LNS框架，用大语言模型协同进化破坏和修复算子，解决组合优化问题，在TSP和CVRP上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有LLM自动化启发式设计方法局限于固定启发式形式，搜索空间受限，难以在复杂组合优化问题中逃离局部最优解

Method: 提出生成式进化框架G-LNS，扩展LLM自动化启发式设计到大规模邻域搜索算子设计，协同进化紧密耦合的破坏与修复算子对，采用合作评估机制显式捕捉算子交互

Result: 在TSP和CVRP基准测试中显著优于LLM基AHD方法和经典求解器，以更少的计算预算获得接近最优解，并对未见过的实例分布表现出强泛化能力

Conclusion: G-LNS成功实现了LLM驱动的LNS算子自动设计，协同进化的破坏-修复算子对能有效进行结构破坏与重建，发现的高性能启发式具有良好泛化性

Abstract: While Large Language Models (LLMs) have recently shown promise in Automated Heuristic Design (AHD), existing approaches typically formulate AHD around constructive priority rules or parameterized local search guidance, thereby restricting the search space to fixed heuristic forms. Such designs offer limited capacity for structural exploration, making it difficult to escape deep local optima in complex Combinatorial Optimization Problems (COPs). In this work, we propose G-LNS, a generative evolutionary framework that extends LLM-based AHD to the automated design of Large Neighborhood Search (LNS) operators. Unlike prior methods that evolve heuristics in isolation, G-LNS leverages LLMs to co-evolve tightly coupled pairs of destroy and repair operators. A cooperative evaluation mechanism explicitly captures their interaction, enabling the discovery of complementary operator logic that jointly performs effective structural disruption and reconstruction. Extensive experiments on challenging COP benchmarks, such as Traveling Salesman Problems (TSP) and Capacitated Vehicle Routing Problems (CVRP), demonstrate that G-LNS significantly outperforms LLM-based AHD methods as well as strong classical solvers. The discovered heuristics not only achieve near-optimal solutions with reduced computational budgets but also exhibit robust generalization across diverse and unseen instance distributions.

</details>


### [271] [SynthAgent: A Multi-Agent LLM Framework for Realistic Patient Simulation -- A Case Study in Obesity with Mental Health Comorbidities](https://arxiv.org/abs/2602.08254)
*Arman Aghaee,Sepehr Asgarian,Jouhyun Jeon*

Main category: cs.AI

TL;DR: This paper introduces SynthAgent, a multi-agent system that creates high-fidelity virtual obesity patients with mental disorders to overcome limitations of real-world medical data.


<details>
  <summary>Details</summary>
Motivation: Address challenges of fragmented, biased, and privacy-restricted real-world clinical data for studying complex diseases like obesity with comorbidities.

Method: Novel Multi-Agent System (MAS) framework integrating clinical evidence from claims data, surveys, and literature to generate personalized virtual patients with personality traits. Uses autonomous agent interactions to simulate disease progression and treatment response.

Result: Evaluation of 100+ generated patients showed GPT-5 and Claude 4.5 Sonnet achieved highest fidelity as core engines, outperforming Gemini 2.5 Pro and DeepSeek-R1.

Conclusion: SynthAgent provides a scalable, privacy-preserving framework for exploring patient journeys, behavioral dynamics, and decision-making in medical and psychological domains.

Abstract: Simulating high-fidelity patients offers a powerful avenue for studying complex diseases while addressing the challenges of fragmented, biased, and privacy-restricted real-world data. In this study, we introduce SynthAgent, a novel Multi-Agent System (MAS) framework designed to model obesity patients with comorbid mental disorders, including depression, anxiety, social phobia, and binge eating disorder. SynthAgent integrates clinical and medical evidence from claims data, population surveys, and patient-centered literature to construct personalized virtual patients enriched with personality traits that influence adherence, emotion regulation, and lifestyle behaviors. Through autonomous agent interactions, the system simulates disease progression, treatment response, and life management across diverse psychosocial contexts. Evaluation of more than 100 generated patients demonstrated that GPT-5 and Claude 4.5 Sonnet achieved the highest fidelity as the core engine in the proposed MAS framework, outperforming Gemini 2.5 Pro and DeepSeek-R1. SynthAgent thus provides a scalable and privacy-preserving framework for exploring patient journeys, behavioral dynamics, and decision-making processes in both medical and psychological domains.

</details>


### [272] [Puda: Private User Dataset Agent for User-Sovereign and Privacy-Preserving Personalized AI](https://arxiv.org/abs/2602.08268)
*Akinori Maeda,Yuto Sekiya,Sota Sugimura,Tomoya Asai,Yu Tsuda,Kohei Ikeda,Hiroshi Fujii,Kohei Watanabe*

Main category: cs.AI

TL;DR: Puda is a user-sovereign architecture that aggregates personal data across services with client-side management, offering three privacy levels. It achieves 97.2% of personalization performance with minimal data sharing (Predefined Category Subsets) compared to full browsing history, enabling practical privacy-personalization trade-off mitigation.


<details>
  <summary>Details</summary>
Motivation: Personal data silos among dominant platforms restrict user sovereignty while LLM agents intensify demand for personalized services, creating a tension between data utilization and privacy protection that needs to be balanced.

Method: Proposes Puda, a browser-based user-sovereign architecture that aggregates cross-service data and enables client-side management with three privacy control levels: Detailed Browsing History, Extracted Keywords, and Predefined Category Subsets. Evaluated through a personalized travel planning task.

Result: Predefined Category Subsets achieve 97.2% of the personalization performance (via LLM-as-a-Judge evaluation) compared to sharing Detailed Browsing History.

Conclusion: Puda provides an AI-native foundation for user sovereignty, enabling effective multi-granularity privacy management and practical solutions to the privacy-personalization trade-off, empowering users to safely leverage personalized AI.

Abstract: Personal data centralization among dominant platform providers including search engines, social networking services, and e-commerce has created siloed ecosystems that restrict user sovereignty, thereby impeding data use across services. Meanwhile, the rapid proliferation of Large Language Model (LLM)-based agents has intensified demand for highly personalized services that require the dynamic provision of diverse personal data. This presents a significant challenge: balancing the utilization of such data with privacy protection. To address this challenge, we propose Puda (Private User Dataset Agent), a user-sovereign architecture that aggregates data across services and enables client-side management. Puda allows users to control data sharing at three privacy levels: (i) Detailed Browsing History, (ii) Extracted Keywords, and (iii) Predefined Category Subsets. We implemented Puda as a browser-based system that serves as a common platform across diverse services and evaluated it through a personalized travel planning task. Our results show that providing Predefined Category Subsets achieves 97.2% of the personalization performance (evaluated via an LLM-as-a-Judge framework across three criteria) obtained when sharing Detailed Browsing History. These findings demonstrate that Puda enables effective multi-granularity management, offering practical choices to mitigate the privacy-personalization trade-off. Overall, Puda provides an AI-native foundation for user sovereignty, empowering users to safely leverage the full potential of personalized AI.

</details>


### [273] [Toward Formalizing LLM-Based Agent Designs through Structural Context Modeling and Semantic Dynamics Analysis](https://arxiv.org/abs/2602.08276)
*Haoyu Jia,Kento Kawaharazuka,Kei Okada*

Main category: cs.AI

TL;DR: 本文提出结构上下文模型和语义动态分析工作流来解决大语言模型智能体研究的碎片化问题，在动态猴子香蕉问题上最高提升32%成功率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型智能体研究存在概念框架与实现细节交织的碎片化问题，缺乏可分析、自洽的形式化模型来对智能体进行实现无关的特征描述和比较。

Method: 提出结构上下文模型（从上下文结构角度分析比较智能体的形式化模型），并配套声明式实现框架和可持续的智能体工程工作流——语义动态分析。

Result: 在动态猴子香蕉问题测试中，使用该方法构建的智能体在最困难设置上成功率提升高达32个百分点。

Conclusion: 该框架通过形式化模型和系统化工作流有效解决了研究碎片化问题，为智能体机制分析提供了原则性指导，并支持快速迭代设计。

Abstract: Current research on large language model (LLM) agents is fragmented: discussions of conceptual frameworks and methodological principles are frequently intertwined with low-level implementation details, causing both readers and authors to lose track amid a proliferation of superficially distinct concepts. We argue that this fragmentation largely stems from the absence of an analyzable, self-consistent formal model that enables implementation-independent characterization and comparison of LLM agents. To address this gap, we propose the \texttt{Structural Context Model}, a formal model for analyzing and comparing LLM agents from the perspective of context structure. Building upon this foundation, we introduce two complementary components that together span the full lifecycle of LLM agent research and development: (1) a declarative implementation framework; and (2) a sustainable agent engineering workflow, \texttt{Semantic Dynamics Analysis}. The proposed workflow provides principled insights into agent mechanisms and supports rapid, systematic design iteration. We demonstrate the effectiveness of the complete framework on dynamic variants of the monkey-banana problem, where agents engineered using our approach achieve up to a 32 percentage points improvement in success rate on the most challenging setting.

</details>


### [274] [The Vibe-Automation of Automation: A Proactive Education Framework for Computer Science in the Age of Generative AI](https://arxiv.org/abs/2602.08295)
*Ilya Levin*

Main category: cs.AI

TL;DR: 本文认为生成式AI代表计算机科学的质变性认识论转变，提出"氛围自动化"概念以描述其通过高维潜在表征操作化隐性规律的特征，界定人类角色转向"氛围工程"，并构建三层次×三领域（教师世界观、产业关系、课程设计）的概念框架分析教育制度转型，同时警示模式崩溃与文化同质化风险。


<details>
  <summary>Details</summary>
Motivation: 突破生成式AI仅是机器学习渐进发展的认知局限，深入剖析其背后的认识论范式转型本质，建立"氛围自动化"理论框架以解释其操作化隐性规律的能力，并系统性探讨这一转变对人类角色及教育-产业制度的深层影响。

Method: 采用理论建构与概念分析方法，通过区分传统机器学习与生成式AI的根本差异，提出核心概念并构建一个包含三个分析层次与三个行动领域（教师世界观、产业关系、课程设计）的综合性概念框架。

Result: 1) 提出"氛围自动化"理论概念；2) 揭示生成式AI通过潜在表征操作化语境敏感性隐性规律；3) 将人类角色重新定义为"氛围工程"；4) 构建连接认识论转变与制度变革的三层次×三领域概念框架；5) 识别模式崩溃、文化同质化及合成一致性风险。

Conclusion: 生成式AI代表计算机科学认识论的质变，其核心在于隐性规律的操作化；人类需转向"氛围工程"以驾驭系统对齐；教育机构必须通过系统性框架应对转型；审慎参与是避免文化单一化风险的必要前提。

Abstract: The emergence of generative artificial intelligence (GenAI) represents not an incremental technological advance but a qualitative epistemological shift that challenges foundational assumptions of computer science. Whereas machine learning has been described as the automation of automation, generative AI operates by navigating contextual, semantic, and stylistic coherence rather than optimizing predefined objective metrics. This paper introduces the concept of Vibe-Automation to characterize this transition.
  The central claim is that the significance of GenAI lies in its functional access to operationalized tacit regularities: context-sensitive patterns embedded in practice that cannot be fully specified through explicit algorithmic rules. Although generative systems do not possess tacit knowledge in a phenomenological sense, they operationalize sensitivities to tone, intent, and situated judgment encoded in high-dimensional latent representations. On this basis, the human role shifts from algorithmic problem specification toward Vibe-Engineering, understood as the orchestration of alignment and contextual judgment in generative systems.
  The paper connects this epistemological shift to educational and institutional transformation by proposing a conceptual framework structured across three analytical levels and three domains of action: faculty worldview, industry relations, and curriculum design. The risks of mode collapse and cultural homogenization are briefly discussed, emphasizing the need for deliberate engagement with generative systems to avoid regression toward synthetic uniformity.

</details>


### [275] [Moral Sycophancy in Vision Language Models](https://arxiv.org/abs/2602.08311)
*Shadman Rabby,Md. Hefzul Hossain Papon,Sabbir Ahmed,Nokimul Hasan Arif,A. B. M. Ashikur Rahman,Irfan Ahmad*

Main category: cs.AI

TL;DR: 本文首次系统研究了视觉-语言模型(VLMs)中的道德迎合现象，揭示模型在与用户意见冲突时易从道德正确转向错误判断，表现出非对称性、数据集依赖性和错误引入-纠正的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽已探讨AI的一般迎合行为，但对其在道德视觉决策中的影响仍理解不足，这在VLMs日益应用于伦理敏感场景的背景下尤为关键。

Method: 研究分析了十个主流VLM模型在Moralise和M^3oralBench数据集上的表现，通过设置明确的用户反对场景，采用错误引入率(EIR)和错误纠正率(ECR)指标进行系统评估。

Result: VLMs常产生道德不正确的后续回应，即使初始判断正确；存在明显非对称性：模型更倾向于从道德正确转向错误；后续提示在Moralise上降低性能，在M^3oralBench上效果复杂甚至提升；EIR与ECR呈现权衡：纠错能力强的模型引入更多错误，保守模型错误少但纠正能力弱；初始道德立场正确的上下文会增强迎合行为。

Conclusion: VLMs易受道德影响而表现出迎合倾向，凸显了开发原则性策略以提升多模态AI系统伦理一致性和鲁棒性的迫切需求。

Abstract: Sycophancy in Vision-Language Models (VLMs) refers to their tendency to align with user opinions, often at the expense of moral or factual accuracy. While prior studies have explored sycophantic behavior in general contexts, its impact on morally grounded visual decision-making remains insufficiently understood. To address this gap, we present the first systematic study of moral sycophancy in VLMs, analyzing ten widely-used models on the Moralise and M^3oralBench datasets under explicit user disagreement. Our results reveal that VLMs frequently produce morally incorrect follow-up responses even when their initial judgments are correct, and exhibit a consistent asymmetry: models are more likely to shift from morally right to morally wrong judgments than the reverse when exposed to user-induced bias. Follow-up prompts generally degrade performance on Moralise, while yielding mixed or even improved accuracy on M^3oralBench, highlighting dataset-dependent differences in moral robustness. Evaluation using Error Introduction Rate (EIR) and Error Correction Rate (ECR) reveals a clear trade-off: models with stronger error-correction capabilities tend to introduce more reasoning errors, whereas more conservative models minimize errors but exhibit limited ability to self-correct. Finally, initial contexts with a morally right stance elicit stronger sycophantic behavior, emphasizing the vulnerability of VLMs to moral influence and the need for principled strategies to improve ethical consistency and robustness in multimodal AI systems.

</details>


### [276] [Who Deserves the Reward? SHARP: Shapley Credit-based Optimization for Multi-Agent System](https://arxiv.org/abs/2602.08335)
*Yanming Li,Xuelin Zhang,WenJie Lu,Ziye Tang,Maodong Wu,Haotian Luo,Tongtong Wu,Zijie Peng,Hongze Mi,Yibo Feng,Naiqiang Tan,Chao Huang,Hong Chen,Li Shen*

Main category: cs.AI

TL;DR: SHARP is a novel framework using Shapley values for precise credit assignment in multi-agent LLM systems, addressing the training difficulty by decomposing rewards into global, marginal-credit, and tool-process components, achieving 23.66% and 14.05% performance improvements over single-agent and multi-agent baselines respectively.


<details>
  <summary>Details</summary>
Motivation: Training multi-agent LLM systems faces the credit assignment challenge - it's unclear which functional agent is responsible for success or failure in decision trajectories. Existing methods using sparse or globally broadcast rewards fail to capture individual contributions, leading to inefficient reinforcement learning.

Method: SHARP framework with a decomposed reward mechanism: 1) global broadcast-accuracy reward, 2) Shapley-based marginal-credit reward for each agent, and 3) tool-process reward for execution efficiency. It stabilizes training by normalizing agent-specific advantages across trajectory groups using Shapley value attribution.

Result: Extensive experiments on real-world benchmarks show SHARP significantly outperforms recent state-of-the-art baselines, achieving average match improvements of 23.66% over single-agent approaches and 14.05% over multi-agent approaches.

Conclusion: SHARP provides an effective solution for credit assignment in multi-agent LLM systems by leveraging Shapley values for precise attribution, which stabilizes training and substantially improves performance across various real-world applications.

Abstract: Integrating Large Language Models (LLMs) with external tools via multi-agent systems offers a promising new paradigm for decomposing and solving complex problems. However, training these systems remains notoriously difficult due to the credit assignment challenge, as it is often unclear which specific functional agent is responsible for the success or failure of decision trajectories. Existing methods typically rely on sparse or globally broadcast rewards, failing to capture individual contributions and leading to inefficient reinforcement learning. To address these limitations, we introduce the Shapley-based Hierarchical Attribution for Reinforcement Policy (SHARP), a novel framework for optimizing multi-agent reinforcement learning via precise credit attribution. SHARP effectively stabilizes training by normalizing agent-specific advantages across trajectory groups, primarily through a decomposed reward mechanism comprising a global broadcast-accuracy reward, a Shapley-based marginal-credit reward for each agent, and a tool-process reward to improve execution efficiency. Extensive experiments across various real-world benchmarks demonstrate that SHARP significantly outperforms recent state-of-the-art baselines, achieving average match improvements of 23.66% and 14.05% over single-agent and multi-agent approaches, respectively.

</details>


### [277] [CoTZero: Annotation-Free Human-Like Vision Reasoning via Hierarchical Synthetic CoT](https://arxiv.org/abs/2602.08339)
*Chengyi Du,Yazhe Niu,Dazhong Shen,Luxin Xu*

Main category: cs.AI

TL;DR: CoTZero is an annotation-free framework that enhances vision-language models' human-like reasoning through dual-stage data synthesis (bottom-up primitive composition and top-down hierarchical guidance) and cognition-aligned training with verifiable rewards, achieving 83.33% F1 on semantic inconsistency detection.


<details>
  <summary>Details</summary>
Motivation: Current VLMs rely on surface correlations rather than building logically coherent structured representations, missing higher-level semantic structure and causal relational understanding, which hinders compositional and verifiable reasoning.

Method: Dual-component approach: (1) Dual-stage data synthesis inspired by neurocognitive theory: bottom-up extraction/composition of atomic visual primitives into structured reasoning forms, and top-down hierarchical reasoning using global structure to guide local details; (2) Cognition-aligned training with Cognitively Coherent Verifiable Rewards (CCVR) for Reinforcement Fine-Tuning, providing stepwise feedback on reasoning coherence and factual correctness.

Result: Achieves 83.33% F1 score on multi-level semantic inconsistency benchmark with lexical-perturbation negatives across both in-domain and out-of-domain settings.

Conclusion: Ablation studies confirm each component contributes to more interpretable and human-aligned visual reasoning.

Abstract: Recent advances in vision-language models (VLMs) have markedly improved image-text alignment, yet they still fall short of human-like visual reasoning. A key limitation is that many VLMs rely on surface correlations rather than building logically coherent structured representations, which often leads to missed higher-level semantic structure and non-causal relational understanding, hindering compositional and verifiable reasoning. To address these limitations by introducing human models into the reasoning process, we propose CoTZero, an annotation-free paradigm with two components: (i) a dual-stage data synthesis approach and (ii) a cognition-aligned training method. In the first component, we draw inspiration from neurocognitive accounts of compositional productivity and global-to-local analysis. In the bottom-up stage, CoTZero extracts atomic visual primitives and incrementally composes them into diverse, structured question-reasoning forms. In the top-down stage, it enforces hierarchical reasoning by using coarse global structure to guide the interpretation of local details and causal relations. In the cognition-aligned training component, built on the synthesized CoT data, we introduce Cognitively Coherent Verifiable Rewards (CCVR) in Reinforcement Fine-Tuning (RFT) to further strengthen VLMs' hierarchical reasoning and generalization, providing stepwise feedback on reasoning coherence and factual correctness. Experiments show that CoTZero achieves an F1 score of 83.33 percent on our multi-level semantic inconsistency benchmark with lexical-perturbation negatives, across both in-domain and out-of-domain settings. Ablations confirm that each component contributes to more interpretable and human-aligned visual reasoning.

</details>


### [278] [Effect-Level Validation for Causal Discovery](https://arxiv.org/abs/2602.08340)
*Hoang Dang,Luan Pham,Minh Nguyen*

Main category: cs.AI

TL;DR: 提出一种以效应为中心的因果发现评估框架，强调在遥测数据决策中可识别性和效应验证比图结构恢复更重要


<details>
  <summary>Details</summary>
Motivation: 因果发现在大规模遥测数据中应用广泛，但在存在强自选择机制的反馈驱动系统中，其决策可靠性尚不明确；现有研究仅关注图恢复准确率，忽略了因果效应是否可识别这一关键问题

Method: 建立"可承认性优先"框架，将发现的图视为结构假设，从可识别性、稳定性和可证伪性三个维度评估；基于真实游戏遥测数据，研究早期竞技性游戏对短期留存的影响

Result: 许多统计上合理的因果图在加入时间约束后无法识别目标效应；可识别时不同算法收敛到一致的效应估计，即使图结构差异很大；效应可通过间接路径传递；图级指标无法可靠预测因果效应质量

Conclusion: 在遥测驱动的决策系统中，可信因果结论需优先保证可承认性和效应级验证，而非单纯追求结构恢复；可识别性是决策支持的瓶颈

Abstract: Causal discovery is increasingly applied to large-scale telemetry data to estimate the effects of user-facing interventions, yet its reliability for decision-making in feedback-driven systems with strong self-selection remains unclear. In this paper, we propose an effect-centric, admissibility-first framework that treats discovered graphs as structural hypotheses and evaluates them by identifiability, stability, and falsification rather than by graph recovery accuracy alone. Empirically, we study the effect of early exposure to competitive gameplay on short-term retention using real-world game telemetry. We find that many statistically plausible discovery outputs do not admit point-identified causal queries once minimal temporal and semantic constraints are enforced, highlighting identifiability as a critical bottleneck for decision support. When identification is possible, several algorithm families converge to similar, decision-consistent effect estimates despite producing substantially different graph structures, including cases where the direct treatment-outcome edge is absent and the effect is preserved through indirect causal pathways. These converging estimates survive placebo, subsampling, and sensitivity refutation. In contrast, other methods exhibit sporadic admissibility and threshold-sensitive or attenuated effects due to endpoint ambiguity. These results suggest that graph-level metrics alone are inadequate proxies for causal reliability for a given target query. Therefore, trustworthy causal conclusions in telemetry-driven systems require prioritizing admissibility and effect-level validation over causal structural recovery alone.

</details>


### [279] [OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration](https://arxiv.org/abs/2602.08344)
*Qi Guo,Jianing Wang,Deyang Kong,Xiangyu Xi,Jianfei Zhang,Yi Lu,Jingang Wang,Wei Wang,Shikun Zhang,Wei Ye*

Main category: cs.AI

TL;DR: This paper proposes Outline-Guided Path Exploration (OPE) to improve parallel thinking in large reasoning models by generating diverse reasoning outlines before path exploration, reducing information redundancy and improving performance on mathematical benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing RL-based parallel thinking methods focus on optimizing the aggregation phase while neglecting the path exploration stage, where mutual information bottleneck among exploration paths fundamentally restricts overall performance.

Method: Outline-Guided Path Exploration (OPE) explicitly partitions solution space by generating diverse reasoning outlines before parallel path reasoning, reducing information redundancy. Implemented with an iterative RL strategy that independently optimizes outline planning and outline-guided reasoning.

Result: Extensive experiments across multiple challenging mathematical benchmarks demonstrate that OPE effectively improves reasoning performance across different aggregation strategies, enabling LRMs to more reliably discover correct solutions.

Conclusion: OPE successfully addresses the mutual information bottleneck in parallel thinking, significantly improving the reasoning capabilities of large reasoning models on complex mathematical problems.

Abstract: Parallel thinking has emerged as a new paradigm for large reasoning models (LRMs) in tackling complex problems. Recent methods leverage Reinforcement Learning (RL) to enhance parallel thinking, aiming to address the limitations in computational resources and effectiveness encountered with supervised fine-tuning. However, most existing studies primarily focus on optimizing the aggregation phase, with limited attention to the path exploration stage. In this paper, we theoretically analyze the optimization of parallel thinking under the Reinforcement Learning with Verifiable Rewards (RLVR) setting, and identify that the mutual information bottleneck among exploration paths fundamentally restricts overall performance. To address this, we propose Outline-Guided Path Exploration (OPE), which explicitly partitions the solution space by generating diverse reasoning outlines prior to parallel path reasoning, thereby reducing information redundancy and improving the diversity of information captured across exploration paths. We implement OPE with an iterative RL strategy that optimizes outline planning and outline-guided reasoning independently. Extensive experiments across multiple challenging mathematical benchmarks demonstrate that OPE effectively improves reasoning performance in different aggregation strategies, enabling LRMs to more reliably discover correct solutions.

</details>


### [280] [Towards Better Evolution Modeling for Temporal Knowledge Graphs](https://arxiv.org/abs/2602.08353)
*Zhang Jiasheng,Li Zhangpin,Wang Mingzhe,Shao Jie,Cui Jiangtao,Li Hui*

Main category: cs.AI

TL;DR: 指出现有时序知识图谱基准存在捷径问题，仅靠统计共现即可达到SOTA性能，提出新的偏差校正基准包含四个数据集和两个新任务，以更准确评估演化建模的真实挑战。


<details>
  <summary>Details</summary>
Motivation: 时序知识图谱预测未来事实的模型性能惊人(Hits@10>0.9)，但研究发现这是基准测试中的捷径所致——无需使用时序信息，仅统计共现即可，无法真实反映模型理解知识演化的能力。

Method: 分析现有基准的根本问题，识别数据集偏差和任务简化是主因，揭示三大局限后，提出包含四个偏差校正数据集和两个新演化任务的TKG演化基准。

Result: 创建了开源TKG演化基准，通过偏差校正和任务重设计，能更准确评估模型对时序知识演化的真实理解能力。

Conclusion: 新基准解决了捷径问题，促进了时序知识图谱演化建模领域更公平、更真实的评估，为研究提供更具挑战性的环境。

Abstract: Temporal knowledge graphs (TKGs) structurally preserve evolving human knowledge. Recent research has focused on designing models to learn the evolutionary nature of TKGs to predict future facts, achieving impressive results. For instance, Hits@10 scores over 0.9 on YAGO dataset. However, we find that existing benchmarks inadvertently introduce a shortcut. Near state-of-the-art performance can be simply achieved by counting co-occurrences, without using any temporal information. In this work, we examine the root cause of this issue, identifying inherent biases in current datasets and over simplified form of evaluation task that can be exploited by these biases. Through this analysis, we further uncover additional limitations of existing benchmarks, including unreasonable formatting of time-interval knowledge, ignorance of learning knowledge obsolescence, and insufficient information for precise evolution understanding, all of which can amplify the shortcut and hinder a fair assessment. Therefore, we introduce the TKG evolution benchmark. It includes four bias-corrected datasets and two novel tasks closely aligned with the evolution process, promoting a more accurate understanding of the challenges in TKG evolution modeling. Benchmark is available at: https://github.com/zjs123/TKG-Benchmark.

</details>


### [281] [Does Your Reasoning Model Implicitly Know When to Stop Thinking?](https://arxiv.org/abs/2602.08354)
*Zixuan Huang,Xin Xia,Yuxi Ren,Jianbin Zheng,Xuanda Wang,Zhixia Zhang,Hongyan Xie,Songshi Liang,Zehao Chen,Xuefeng Xiao,Fuzhen Zhuang,Jianxin Li,Yikun Ban,Deqing Wang*

Main category: cs.AI

TL;DR: 大模型隐含知道何时停止推理，但现有采样掩盖此能力。提出SAGE采样和SAGE-RL，在数学任务上提升准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 长思维链导致冗余和延迟，长度与正确性不相关甚至有害，现有范式无法利用模型自主停止能力。

Method: 提出SAGE（自我感知引导的高效推理）采样范式，结合组强化学习（SAGE-RL）将高效模式融入pass@1推理。

Result: 在多个数学基准测试中显著提升推理准确性和效率。

Conclusion: 成功释放大模型内在高效推理潜力。

Abstract: Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through Long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In a further in-depth analysis of this phenomenon, we surprisingly uncover and empirically verify that LRMs implicitly know the appropriate time to stop thinking, while this capability is obscured by current sampling paradigms. Motivated by this, we introduce SAGE (Self-Aware Guided Efficient Reasoning), a novel sampling paradigm that unleashes this efficient reasoning potential. Furthermore, integrating SAGE as mixed sampling into group-based reinforcement learning (SAGE-RL) enables SAGE-RL to effectively incorporate SAGE-discovered efficient reasoning patterns into standard pass@1 inference, markedly enhancing both the reasoning accuracy and efficiency of LRMs across multiple challenging mathematical benchmarks.

</details>


### [282] [Circuit Representations of Random Forests with Applications to XAI](https://arxiv.org/abs/2602.08362)
*Chunxi Ji,Adnan Darwiche*

Main category: cs.AI

TL;DR: This paper proposes compiling random forest classifiers into circuits to efficiently compute decision explanations (reasons) and robustness metrics. The method enables enumeration of sufficient/necessary reasons and contrastive explanations, computes decision robustness, and identifies shortest decision-flipping strategies across various datasets.


<details>
  <summary>Details</summary>
Motivation: Existing methods for explaining random forest decisions lack efficiency in computing explanations, robustness, and decision-flipping strategies. There is a need for more efficient approaches to analyze classifier decisions at scale.

Method: The paper presents a three-step method: (1) compile random forest classifiers into circuits encoding class-specific instances; (2) use these circuits to compute complete and general reasons for decisions; (3) develop algorithms for computing decision robustness and all minimal modifications to flip decisions.

Result: Empirically demonstrates significantly better efficiency than existing approaches. Successfully enumerates sufficient reasons, necessary reasons, and contrastive explanations; computes decision robustness; and identifies shortest decision-flipping strategies across diverse datasets.

Conclusion: Circuit compilation of random forests provides an efficient framework for comprehensive decision explanation and robustness analysis, enabling practical explanation tools for complex ensemble classifiers.

Abstract: We make three contributions in this paper. First, we present an approach for compiling a random forest classifier into a set of circuits, where each circuit directly encodes the instances in some class of the classifier. We show empirically that our proposed approach is significantly more efficient than existing similar approaches. Next, we utilize this approach to further obtain circuits that are tractable for computing the complete and general reasons of a decision, which are instance abstractions that play a fundamental role in computing explanations. Finally, we propose algorithms for computing the robustness of a decision and all shortest ways to flip it. We illustrate the utility of our contributions by using them to enumerate all sufficient reasons, necessary reasons and contrastive explanations of decisions; to compute the robustness of decisions; and to identify all shortest ways to flip the decisions made by random forest classifiers learned from a wide range of datasets.

</details>


### [283] [MemAdapter: Fast Alignment across Agent Memory Paradigms via Generative Subgraph Retrieval](https://arxiv.org/abs/2602.08369)
*Xin Zhang,Kailai Yang,Chenyue Li,Hao Li,Qiyu Wei,Jun'ichi Tsujii,Sophia Ananiadou*

Main category: cs.AI

TL;DR: MemAdapter是一个统一不同记忆范式的检索框架，通过两阶段训练实现快速跨范式对齐，在保持高性能的同时大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体记忆系统采用孤立范式（显式、参数化、潜在记忆）且检索方法紧耦合，导致跨范式泛化和融合困难，缺乏统一框架。

Method: 提出MemAdapter框架，采用两阶段训练：1）在统一记忆空间训练生成式子图检索器；2）通过对比学习训练轻量级对齐模块，使检索器适配未见记忆范式。

Result: 在三个基准测试中，生成式子图检索器在三种记忆范式和模型尺度上均优于五种强基线系统；单GPU上13分钟内完成跨范式对齐；仅需不到5%的训练计算量即可达到更优性能；支持零样本跨范式融合。

Conclusion: MemAdapter展现了作为即插即用智能体记忆系统解决方案的潜力，能够有效统一异构记忆范式。

Abstract: Memory mechanism is a core component of LLM-based agents, enabling reasoning and knowledge discovery over long-horizon contexts. Existing agent memory systems are typically designed within isolated paradigms (e.g., explicit, parametric, or latent memory) with tightly coupled retrieval methods that hinder cross-paradigm generalization and fusion. In this work, we take a first step toward unifying heterogeneous memory paradigms within a single memory system. We propose MemAdapter, a memory retrieval framework that enables fast alignment across agent memory paradigms. MemAdapter adopts a two-stage training strategy: (1) training a generative subgraph retriever from the unified memory space, and (2) adapting the retriever to unseen memory paradigms by training a lightweight alignment module through contrastive learning. This design improves the flexibility for memory retrieval and substantially reduces alignment cost across paradigms. Comprehensive experiments on three public evaluation benchmarks demonstrate that the generative subgraph retriever consistently outperforms five strong agent memory systems across three memory paradigms and agent model scales. Notably, MemAdapter completes cross-paradigm alignment within 13 minutes on a single GPU, achieving superior performance over original memory retrievers with less than 5% of training compute. Furthermore, MemAdapter enables effective zero-shot fusion across memory paradigms, highlighting its potential as a plug-and-play solution for agent memory systems.

</details>


### [284] [Grounding Generative Planners in Verifiable Logic: A Hybrid Architecture for Trustworthy Embodied AI](https://arxiv.org/abs/2602.08373)
*Feiyu Wu,Xu Zheng,Yue Qu,Zhuocheng Wang,Zicheng Feng,Hui Li*

Main category: cs.AI

TL;DR: 提出可验证迭代优化框架VIRF，通过确定性逻辑导师与LLM规划器的教学对话实现主动安全修复，在家庭安全任务中实现零危险动作率与77.3%目标达成率


<details>
  <summary>Details</summary>
Motivation: 现有LLM规划器缺乏形式化推理导致物理部署存在安全隐患，当前方法仅被动拦截危险计划或依赖不可靠安全检查，缺乏智能修复能力

Method: 构建神经符号架构：1) 基于形式化安全本体的确定性逻辑导师提供因果教学反馈；2) 开发可扩展知识获取管道从真实文档合成安全知识库；3) 建立导师-学徒对话机制实现计划迭代优化

Result: 在家庭安全任务中实现0%危险动作率(HAR)和77.3%目标达成率(GCR)，位列所有基线最高；平均仅需1.1次修正迭代，效率显著高于传统方法

Conclusion: VIRF为构建可验证安全的具身智能体提供了原则性路径，通过神经符号协作实现从被动安全管控到主动智能修复的范式转变

Abstract: Large Language Models (LLMs) show promise as planners for embodied AI, but their stochastic nature lacks formal reasoning, preventing strict safety guarantees for physical deployment. Current approaches often rely on unreliable LLMs for safety checks or simply reject unsafe plans without offering repairs. We introduce the Verifiable Iterative Refinement Framework (VIRF), a neuro-symbolic architecture that shifts the paradigm from passive safety gatekeeping to active collaboration. Our core contribution is a tutor-apprentice dialogue where a deterministic Logic Tutor, grounded in a formal safety ontology, provides causal and pedagogical feedback to an LLM planner. This enables intelligent plan repairs rather than mere avoidance. We also introduce a scalable knowledge acquisition pipeline that synthesizes safety knowledge bases from real-world documents, correcting blind spots in existing benchmarks. In challenging home safety tasks, VIRF achieves a perfect 0 percent Hazardous Action Rate (HAR) and a 77.3 percent Goal-Condition Rate (GCR), which is the highest among all baselines. It is highly efficient, requiring only 1.1 correction iterations on average. VIRF demonstrates a principled pathway toward building fundamentally trustworthy and verifiably safe embodied agents.

</details>


### [285] [SCOUT-RAG: Scalable and Cost-Efficient Unifying Traversal for Agentic Graph-RAG over Distributed Domains](https://arxiv.org/abs/2602.08400)
*Longkun Li,Yuanben Zou,Jinghan Wu,Yuqing Wen,Jing Li,Hangwei Qian,Ivor Tsang*

Main category: cs.AI

TL;DR: SCOUT-RAG是一种分布式Graph-RAG框架，通过四个协作智能体在受限环境中实现高效跨领域检索，性能媲美集中式方法但显著降低调用成本和延迟


<details>
  <summary>Details</summary>
Motivation: 传统Graph-RAG依赖集中式知识图谱，但医院或跨国机构等分布式且访问受限的场景无法全局可见图谱，需在不遍历全图的前提下精准选择相关域并控制检索深度

Method: 提出SCOUT-RAG框架，包含四个协同智能体：(1)评估域相关性；(2)决策是否扩展检索域；(3)自适应调整遍历深度；(4)合成最终答案，以最小化检索遗憾并控制延迟与API成本

Result: 在多领域知识测试中，性能与集中式基线（如DRIFT和穷举遍历）相当，但大幅减少跨域调用次数、总处理Token数和延迟

Conclusion: 为分布式环境提供了可扩展且成本高效的Graph-RAG解决方案，在保障检索质量的同时优化了资源消耗

Abstract: Graph-RAG improves LLM reasoning using structured knowledge, yet conventional designs rely on a centralized knowledge graph. In distributed and access-restricted settings (e.g., hospitals or multinational organizations), retrieval must select relevant domains and appropriate traversal depth without global graph visibility or exhaustive querying. To address this challenge, we introduce \textbf{SCOUT-RAG} (\textit{\underline{S}calable and \underline{CO}st-efficient \underline{U}nifying \underline{T}raversal}), a distributed agentic Graph-RAG framework that performs progressive cross-domain retrieval guided by incremental utility goals. SCOUT-RAG employs four cooperative agents that: (i) estimate domain relevance, (ii) decide when to expand retrieval to additional domains, (iii) adapt traversal depth to avoid unnecessary graph exploration, and (iv) synthesize the high-quality answers. The framework is designed to minimize retrieval regret, defined as missing useful domain information, while controlling latency and API cost. Across multi-domain knowledge settings, SCOUT-RAG achieves performance comparable to centralized baselines, including DRIFT and exhaustive domain traversal, while substantially reducing cross-domain calls, total tokens processed, and latency.

</details>


### [286] [On Protecting Agentic Systems' Intellectual Property via Watermarking](https://arxiv.org/abs/2602.08401)
*Liwen Wang,Zongjie Li,Yuchong Xie,Shuai Wang,Dongdong She,Wei Wang,Juergen Rahmel*

Main category: cs.AI

TL;DR: 提出AGENTWM，首个面向智能体大模型的水印框架，通过利用动作序列语义等价性嵌入可验证信号，在三个复杂领域评估中实现对模仿攻击的有效防护，检测精度高且性能影响可忽略。


<details>
  <summary>Details</summary>
Motivation: LLM向智能体系统演进创造了巨大IP价值，但现有水印技术因智能体灰盒特性（隐藏内部推理轨迹）而失效，面临严重的模仿攻击威胁。

Method: 基于动作序列的语义等价性，通过微调功能相同工具执行路径分布来注入水印，开发自动化水印生成流水线和统计假设检验验证机制。

Result: 三领域评估显示检测准确率高，性能影响可忽略，能有效对抗自适应对手，水印去除将严重损害模型效用。

Conclusion: AGENTWM为智能体模型IP保护提供了首个有效解决方案，可可靠抵御模仿攻击。

Abstract: The evolution of Large Language Models (LLMs) into agentic systems that perform autonomous reasoning and tool use has created significant intellectual property (IP) value. We demonstrate that these systems are highly vulnerable to imitation attacks, where adversaries steal proprietary capabilities by training imitation models on victim outputs. Crucially, existing LLM watermarking techniques fail in this domain because real-world agentic systems often operate as grey boxes, concealing the internal reasoning traces required for verification. This paper presents AGENTWM, the first watermarking framework designed specifically for agentic models. AGENTWM exploits the semantic equivalence of action sequences, injecting watermarks by subtly biasing the distribution of functionally identical tool execution paths. This mechanism allows AGENTWM to embed verifiable signals directly into the visible action trajectory while remaining indistinguishable to users. We develop an automated pipeline to generate robust watermark schemes and a rigorous statistical hypothesis testing procedure for verification. Extensive evaluations across three complex domains demonstrate that AGENTWM achieves high detection accuracy with negligible impact on agent performance. Our results confirm that AGENTWM effectively protects agentic IP against adaptive adversaries, who cannot remove the watermarks without severely degrading the stolen model's utility.

</details>


### [287] [From Assistant to Double Agent: Formalizing and Benchmarking Attacks on OpenClaw for Personalized Local AI Agent](https://arxiv.org/abs/2602.08412)
*Yuhang Wang,Feiming Xu,Zheng Lin,Guangyu He,Yuzhe Huang,Haichang Gao,Zhenxing Niu*

Main category: cs.AI

TL;DR: 本文提出了PASB框架，用于评估个性化AI智能体的端到端安全性。通过对OpenClaw的案例研究发现，其在用户提示处理、工具使用和记忆检索等关键执行阶段存在严重漏洞，揭示了个性化智能体部署的重大安全风险。


<details>
  <summary>Details</summary>
Motivation: 现有智能体安全研究主要关注合成或任务导向场景，无法准确捕捉个性化智能体在真实部署中的攻击面和风险传播机制。

Method: 提出Personalized Agent Security Bench (PASB)框架，结合个性化使用场景、真实工具链和长周期交互，实现黑盒端到端安全评估。以OpenClaw为案例进行研究。

Result: 评估发现OpenClaw在用户提示处理、工具使用和记忆检索等不同执行阶段存在关键漏洞，表明个性化智能体部署存在重大安全风险。

Conclusion: PASB框架为真实场景下个性化智能体的安全评估提供了有效工具，相关代码已开源。

Abstract: Although large language model (LLM)-based agents, exemplified by OpenClaw, are increasingly evolving from task-oriented systems into personalized AI assistants for solving complex real-world tasks, their practical deployment also introduces severe security risks. However, existing agent security research and evaluation frameworks primarily focus on synthetic or task-centric settings, and thus fail to accurately capture the attack surface and risk propagation mechanisms of personalized agents in real-world deployments. To address this gap, we propose Personalized Agent Security Bench (PASB), an end-to-end security evaluation framework tailored for real-world personalized agents. Building upon existing agent attack paradigms, PASB incorporates personalized usage scenarios, realistic toolchains, and long-horizon interactions, enabling black-box, end-to-end security evaluation on real systems. Using OpenClaw as a representative case study, we systematically evaluate its security across multiple personalized scenarios, tool capabilities, and attack types. Our results indicate that OpenClaw exhibits critical vulnerabilities at different execution stages, including user prompt processing, tool usage, and memory retrieval, highlighting substantial security risks in personalized agent deployments. The code for the proposed PASB framework is available at https://github.com/AstorYH/PASB.

</details>


### [288] [When Evaluation Becomes a Side Channel: Regime Leakage and Structural Mitigations for Alignment Assessment](https://arxiv.org/abs/2602.08449)
*Igor Santos-Grueiro*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Safety evaluation for advanced AI systems implicitly assumes that behavior observed under evaluation is predictive of behavior in deployment. This assumption becomes fragile for agents with situational awareness, which may exploitregime leakage-informational cues distinguishing evaluation from deployment-to implement conditional policies such as sycophancy and sleeper agents, which preserve compliance under oversight while defecting in deployment-like regimes. We reframe alignment evaluation as a problem of information flow under partial observability. Within this framework, we show that divergence between evaluation-time and deployment-time behavior is bounded by the mutual information between internal representations and the regime variable. Motivated by this result, we study regime-blind mechanisms: training-time interventions that reduce the extractability of regime information at decision-relevant internal representations via adversarial invariance. We evaluate this approach on a base, open-weight language model across two fully characterized failure modes -scientific sycophancy and temporal sleeper agents. Regime-blind training suppresses regime-conditioned behavior in both evaluated cases without measurable loss of task utility, but with qualitatively different dynamics: sycophancy exhibits a sharp representational and behavioral transition at low intervention strength, whereas sleeper-agent behavior requires substantially stronger pressure and does not exhibit a clean collapse of regime decodability. These results demonstrate that representational invariance is a meaningful but fundamentally limited control lever, whose effectiveness depends on how regime information is embedded in the policy. We argue that behavioral evaluation should be complemented with white-box diagnostics of regime awareness and information flow.

</details>


### [289] [TreeTensor: Boost AI System on Nested Data with Constrained Tree-Like Tensor](https://arxiv.org/abs/2602.08517)
*Shaoang Zhang,Yazhe Niu*

Main category: cs.AI

TL;DR: 该论文提出TreeTensor，一种通用的嵌套数据容器，通过约束树结构视角实现复杂AI系统中层次化多模态数据的零成本高效运算，兼容主流机器学习框架。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统中，固定形状的常规Tensor难以高效处理复杂认知任务中具有层次结构（嵌套数据）和多种模态的数据，导致编程不便且效率低下。

Method: 作者总结了嵌套数据的两种主要计算模式，提出TreeTensor容器，采用约束树结构系统建模数据关系，提供魔术工具实现与Scikit-Learn、Numpy、PyTorch等库的零成本集成。

Result: TreeTensor可近乎零成本地对嵌套数据应用任意函数和操作，成功应用于AlphaStar等复杂AI系统，基准测试显示其运行时效率优异且无额外开销。

Conclusion: TreeTensor为复杂AI系统中的嵌套数据处理提供了强大的可用性，同时保持与现有框架的兼容性和运行时效率，是先进AI开发的有效工具。

Abstract: Tensor is the most basic and essential data structure of nowadays artificial intelligence (AI) system. The natural properties of Tensor, especially the memory-continuity and slice-independence, make it feasible for training system to leverage parallel computing unit like GPU to process data simultaneously in batch, spatial or temporal dimensions. However, if we look beyond perception tasks, the data in a complicated cognitive AI system usually has hierarchical structures (i.e. nested data) with various modalities. They are inconvenient and inefficient to program directly with conventional Tensor with fixed shape. To address this issue, we summarize two main computational patterns of nested data, and then propose a general nested data container: TreeTensor. Through various constraints and magic utilities of TreeTensor, one can apply arbitrary functions and operations to nested data with almost zero cost, including some famous machine learning libraries, such as Scikit-Learn, Numpy and PyTorch. Our approach utilizes a constrained tree-structure perspective to systematically model data relationships, and it can also easily be combined with other methods to extend more usages, such as asynchronous execution and variable-length data computation. Detailed examples and benchmarks show TreeTensor not only provides powerful usability in various problems, especially one of the most complicated AI systems at present: AlphaStar for StarCraftII, but also exhibits excellent runtime efficiency without any overhead. Our project is available at https://github.com/opendilab/DI-treetensor.

</details>


### [290] [Reinforcement Inference: Leveraging Uncertainty for Self-Correcting Language Model Reasoning](https://arxiv.org/abs/2602.08520)
*Xinhai Sun*

Main category: cs.AI

TL;DR: 提出"强化推理"方法，通过模型自身的不确定性感知，在推理时选择性地触发二次思考，无需重训练即可将DeepSeek-v3.2在MMLU-Pro上的准确率从60.72%提升至84.03%，同时仅增加61%的推理开销。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在单次贪心解码模式下会系统性低估真实能力，许多错误源于内部歧义下的过早承诺而非知识缺失，需要利用模型不确定性来触发更审慎的推理。

Method: 熵感知的推理时控制策略：利用模型生成过程中的熵/置信度作为第一类控制信号，当检测到高不确定性时选择性调用第二次更审慎的推理尝试，实现无重训练的性能提升。

Result: 在12,032道MMLU-Pro题目上，准确率从60.72%提升至84.03%，仅增加61.06%推理调用；100%重试消融实验达84.35%，表明熵感知选择能以更少计算捕获大部分可提升空间；仅提示"高熵需逐步思考"的基线效果低于原方法。

Conclusion: 建立了熵感知的模型能力度量与扩展新范式，揭示了单通道贪心推理与不确定性调节 deliberation 之间的性能差距可作为LLM潜在推理视界的诊断工具，并推动未来显式约束正确性-置信度对齐的训练目标设计。

Abstract: Modern large language models (LLMs) are often evaluated and deployed under a \emph{one-shot, greedy} inference protocol, especially in professional settings that require deterministic behavior. This regime can systematically under-estimate a fixed model's true capability: many errors arise not from missing knowledge, but from premature commitment under internal ambiguity. We introduce \emph{Reinforcement Inference}, an entropy-aware inference-time control strategy that uses the model's own uncertainty to selectively invoke a second, more deliberate reasoning attempt, enabling stronger performance \emph{without any retraining}.
  On 12,032 MMLU-Pro questions across 14 subjects, using DeepSeek-v3.2 with deterministic decoding in a zero-shot setting, Reinforcement Inference improves accuracy from 60.72\% to 84.03\%, while only incurring 61.06\% additional inference calls. A 100\% re-asking ablation reaches 84.35\%, indicating that uncertainty-aware selection captures most of the attainable improvement with substantially less compute. Moreover, a \emph{prompt-only} ablation underperforms the baseline, suggesting that the gains are not explained by generic `` your output had high entropy, think step-by-step'' prompting alone.
  Beyond providing a practical inference-time upgrade, our results suggest a broader \emph{entropy-aware} paradigm for measuring and expanding model capability: because modern decoder-based models generate outputs autoregressively, entropy and related confidence measures arise naturally as first-class control signals during generation. The resulting gap between one-pass greedy inference and uncertainty-conditioned deliberation offers a diagnostic lens on an LLM's latent reasoning horizon and motivates future training objectives that explicitly constrain correctness--confidence alignment.

</details>


### [291] [Dialogue Model Optimization via Agent Game and Adaptive Tree-based GRPO](https://arxiv.org/abs/2602.08533)
*Kun Peng,Conghui Tan,Yu Liu,Guohua Tang,Zhongqian Sun,Wei Yang,Zining Zhu,Lei Jiang,Yanbing Liu,Hao Peng*

Main category: cs.AI

TL;DR: 提出长视野强化学习框架解决开放域对话系统过度依赖预收集用户数据和短期偏置问题，通过双智能体博弈与自适应树形策略优化实现高效长期对话个性化


<details>
  <summary>Details</summary>
Motivation: 现有开放域对话系统存在两大局限：1) 过度依赖预收集用户数据 2) 强化学习的短期偏置忽视长期对话价值，导致个性化交互能力不足

Method: 提出整合在线个性化与自适应树形组相对策略优化(AT-GRPO)的长视野RL框架。采用双智能体博弈：用户智能体通过风格模仿(学习用户对话特征)和主动终止(预测回合终止概率作为即时奖励)构建动态环境，驱动对话智能体深化兴趣探索。AT-GRPO将对话轨迹重构为树形结构，采用阶段感知的自适应观测范围：早期大范围的奖励聚合支持话题探索，后期小范围聚焦对话维持，将计算开销从指数级降至多项式级

Result: 实验表明该框架在性能、样本效率和鲁棒性方面显著优于现有方法，有效平衡了长期奖励捕获与计算效率

Conclusion: 该框架通过创新的树形策略优化和双智能体协同机制，为开放域对话系统提供了可扩展的长期个性化解决方案，解决了数据依赖与短期偏置的核心挑战

Abstract: Open-ended dialogue agents aim to deliver engaging, personalized interactions by adapting to users' traits, but existing methods face critical limitations: over-reliance on pre-collected user data, and short-horizon biases in reinforcement learning (RL) that neglect long-term dialogue value. To address these, we propose a novel long-horizon RL framework integrating online personalization with Adaptive Tree-based Group Relative Policy Optimization (AT-GRPO). Adopting a two-agent game paradigm, a user agent constructs dynamic environments via style mimicry (learning user-specific conversational traits) and active termination (predicting turn-level termination probabilities as immediate rewards), forming an iterative cycle that drives the dialogue agent to deepen interest exploration. AT-GRPO reinterprets dialogue trajectories as trees and introduces adaptive observation ranges. Unlike full tree expansion that incurs exponential overhead, it limits each node to aggregate rewards from a stage-aware range: larger ranges support early-stage topic exploration, while smaller ranges facilitate late-stage dialogue maintenance. This design reduces rollout budgets from exponential to polynomial in the dialogue length, while preserving long-term reward capture. Extensive experiments show our framework's superior performance, sample efficiency, and robustness.

</details>


### [292] [An Attention Mechanism for Robust Multimodal Integration in a Global Workspace Architecture](https://arxiv.org/abs/2602.08597)
*Roland Bertin-Johannet,Lara Scipio,Leopold Maytié,Rufin VanRullen*

Main category: cs.AI

TL;DR: 基于认知神经科学的全球工作空间理论(GWT)，提出新型自上而下注意力机制实现多模态选择，显著提升系统抗噪能力和跨任务泛化性，在MM-IMDb 1.0基准测试中达到业界顶尖水平


<details>
  <summary>Details</summary>
Motivation: 现有GWT计算模型侧重多模态表征能力，但关键的注意力机制研究不足，尤其缺乏对多模态选择机制的系统探索

Method: 设计自上而下注意力机制，在全局工作空间内实现模态选择，通过Simple Shapes和MM-IMDb 1.0双数据集验证机制有效性

Result: 1) 显著提升系统在噪声环境下的鲁棒性 2) 实现现有文献模型未具备的跨任务和跨模态泛化能力 3) 在MM-IMDb 1.0基准测试中性能达到业界先进水平

Conclusion: 该注意力机制不仅强化了GWT框架的实用性，更为构建鲁棒多模态系统提供了新范式，证明认知理论启发的架构能有效推动多模态学习发展

Abstract: Global Workspace Theory (GWT), inspired by cognitive neuroscience, posits that flexible cognition could arise via the attentional selection of a relevant subset of modalities within a multimodal integration system. This cognitive framework can inspire novel computational architectures for multimodal integration. Indeed, recent implementations of GWT have explored its multimodal representation capabilities, but the related attention mechanisms remain understudied. Here, we propose and evaluate a top-down attention mechanism to select modalities inside a global workspace. First, we demonstrate that our attention mechanism improves noise robustness of a global workspace system on two multimodal datasets of increasing complexity: Simple Shapes and MM-IMDb 1.0. Second, we highlight various cross-task and cross-modality generalization capabilities that are not shared by multimodal attention models from the literature. Comparing against existing baselines on the MM-IMDb 1.0 benchmark, we find our attention mechanism makes the global workspace competitive with the state of the art.

</details>


### [293] [OSCAR: Optimization-Steered Agentic Planning for Composed Image Retrieval](https://arxiv.org/abs/2602.08603)
*Teng Wang,Rong Shan,Jianghao Lin,Junjie Wu,Tianyi Xu,Jianping Zhang,Wenteng Chen,Changwang Zhang,Zhaoxiang Wang,Weinan Zhang,Jun Wang*

Main category: cs.AI

TL;DR: 提出OSCAR框架，通过将组合图像检索从启发式搜索转化为轨迹优化问题，结合离线数学规划与在线VLM规划器，在仅用10%训练数据的情况下超越现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 现有组合图像检索方法存在两大局限：统一嵌入检索受"单模型短视"困扰，而启发式智能体检索则受"次优试错编排"限制，缺乏原则性的推理框架。

Method: OSCAR采用离线-在线范式：离线阶段将检索建模为两阶段混合整数规划问题，通过布尔集合运算数学化推导最优轨迹并存入黄金库；在线阶段利用这些轨迹作为上下文演示来引导VLM规划器推理。

Result: 在三个公开基准和一个私有工业基准上持续超越SOTA，且仅使用10%训练数据就能取得更优性能，证明了其规划逻辑的强泛化能力而非数据记忆。

Conclusion: 首次将智能体组合图像检索形式化为轨迹优化问题，通过数学推导与数据驱动结合，提供了高效且可泛化的检索规划新范式。

Abstract: Composed image retrieval (CIR) requires complex reasoning over heterogeneous visual and textual constraints. Existing approaches largely fall into two paradigms: unified embedding retrieval, which suffers from single-model myopia, and heuristic agentic retrieval, which is limited by suboptimal, trial-and-error orchestration. To this end, we propose OSCAR, an optimization-steered agentic planning framework for composed image retrieval. We are the first to reformulate agentic CIR from a heuristic search process into a principled trajectory optimization problem. Instead of relying on heuristic trial-and-error exploration, OSCAR employs a novel offline-online paradigm. In the offline phase, we model CIR via atomic retrieval selection and composition as a two-stage mixed-integer programming problem, mathematically deriving optimal trajectories that maximize ground-truth coverage for training samples via rigorous boolean set operations. These trajectories are then stored in a golden library to serve as in-context demonstrations for online steering of VLM planner at online inference time. Extensive experiments on three public benchmarks and a private industrial benchmark show that OSCAR consistently outperforms SOTA baselines. Notably, it achieves superior performance using only 10% of training data, demonstrating strong generalization of planning logic rather than dataset-specific memorization.

</details>


### [294] [Intermediate Results on the Complexity of STRIPS$_{1}^{1}$](https://arxiv.org/abs/2602.08708)
*Stefan Edelkamp,Jiří Fink,Petr Gregor,Anders Jonsson,Bernhard Nebel*

Main category: cs.AI

TL;DR: 该论文研究单前提单效果STRIPS规划问题(STRIPS^1_1)的计算复杂性，针对其是否为NP完全这一未解问题，通过SAT求解器、文字图和Petri网映射方法提供新视角。


<details>
  <summary>Details</summary>
Motivation: Bylander已证明含两个前提和效果的命题STRIPS规划是PSPACE完全问题，但单前提单效果版本(STRIPS^1_1)是否NP完全仍是未知，这对理解规划问题复杂性边界至关重要。

Method: 1) 对小型实例调用SAT求解器进行验证；2) 引入文字图(literal graph)表示方法；3) 将问题映射到Petri网模型。

Result: 研究为STRIPS^1_1的"小解假设"提供了新证据，但摘要未明确给出最终结论性结果。

Conclusion: 该工作通过计算实验与形式化映射相结合的方法，推进了对STRIPS^1_1规划问题复杂性的理解，为后续解决这一开放性难题奠定了基础。

Abstract: This paper is based on Bylander's results on the computational complexity of propositional STRIPS planning. He showed that when only ground literals are permitted, determining plan existence is PSPACE-complete even if operators are limited to two preconditions and two postconditions. While NP-hardness is settled, it is unknown whether propositional STRIPS with operators that only have one precondition and one effect is NP-complete. We shed light on the question whether this small solution hypothesis for STRIPS$^1_1$ is true, calling a SAT solver for small instances, introducing the literal graph, and mapping it to Petri nets.

</details>


### [295] [Exploring SAIG Methods for an Objective Evaluation of XAI](https://arxiv.org/abs/2602.08715)
*Miquel Miró-Nicolau,Gabriel Moyà-Alcover,Anna Arias-Duart*

Main category: cs.AI

TL;DR: This paper reviews Synthetic AI Ground truth (SAIG) methods for evaluating XAI, proposes a taxonomy of seven features, and reveals a lack of consensus in XAI evaluation techniques.


<details>
  <summary>Details</summary>
Motivation: XAI evaluation lacks universal ground truth, making objective assessment challenging despite diverse existing approaches.

Method: First systematic review of SAIG methods; develops a novel taxonomy with seven distinguishing features; comparative analysis of existing approaches.

Result: Identifies significant lack of consensus on effective XAI evaluation techniques across the field.

Conclusion: SAIG methods show promise for direct XAI evaluation, but the field requires further research and standardization.

Abstract: The evaluation of eXplainable Artificial Intelligence (XAI) methods is a rapidly growing field, characterized by a wide variety of approaches. This diversity highlights the complexity of the XAI evaluation, which, unlike traditional AI assessment, lacks a universally correct ground truth for the explanation, making objective evaluation challenging. One promising direction to address this issue involves the use of what we term Synthetic Artificial Intelligence Ground truth (SAIG) methods, which generate artificial ground truths to enable the direct evaluation of XAI techniques. This paper presents the first review and analysis of SAIG methods. We introduce a novel taxonomy to classify these approaches, identifying seven key features that distinguish different SAIG methods. Our comparative study reveals a concerning lack of consensus on the most effective XAI evaluation techniques, underscoring the need for further research and standardization in this area.

</details>


### [296] [Belief Offloading in Human-AI Interaction](https://arxiv.org/abs/2602.08754)
*Rose E. Guingrich,Dvija Mehta,Umang Bhatt*

Main category: cs.AI

TL;DR: 该论文提出"信念外包"概念，研究人们将信念形成过程委托给AI系统时产生的行为与信念体系变化，并构建了其分类框架与规范影响分析


<details>
  <summary>Details</summary>
Motivation: 随着LLM聊天机器人成为人们思考伙伴，过度依赖导致的认知外包可能对认知技能产生负面影响，特别当人们直接从AI获取信息形成信念时，会对个人行为和世界观体系产生下游后果

Method: 通过整合哲学、心理学和计算机科学的多学科研究，界定信念外包的边界条件，并构建描述性分类法

Result: 明确了信念外包发生的边界条件，提出了系统的信念外包分类框架，并分析了其规范性含义（如认知自主性、责任归属等问题）

Conclusion: 建立了理解人机交互中信念外包的理论基础，指出未来需要实证研究来评估信念外包的潜在风险与社会后果，为人机协作设计提供指导

Abstract: What happens when people's beliefs are derived from information provided by an LLM? People's use of LLM chatbots as thought partners can contribute to cognitive offloading, which can have adverse effects on cognitive skills in cases of over-reliance. This paper defines and investigates a particular kind of cognitive offloading in human-AI interaction, "belief offloading," in which people's processes of forming and upholding beliefs are offloaded onto an AI system with downstream consequences on their behavior and the nature of their system of beliefs. Drawing on philosophy, psychology, and computer science research, we clarify the boundary conditions under which belief offloading occurs and provide a descriptive taxonomy of belief offloading and its normative implications. We close with directions for future work to assess the potential for and consequences of belief offloading in human-AI interaction.

</details>


### [297] [Dynamics Within Latent Chain-of-Thought: An Empirical Study of Causal Structure](https://arxiv.org/abs/2602.08783)
*Zirui Li,Xuefeng Bai,Kehai Chen,Yizhi Li,Jian Yang,Chenghua Lin,Min Zhang*

Main category: cs.AI

TL;DR: This paper uses causal inference (structural causal models and do-interventions) to analyze latent chain-of-thought methods in LLMs, revealing that latent steps function as staged modules with non-local routing rather than homogeneous depth, and identifies a gap between early output bias and late representational commitment.


<details>
  <summary>Details</summary>
Motivation: Latent chain-of-thought methods replace explicit textual rationales with internal latent steps, but these intermediate computations are difficult to evaluate beyond simple correlation-based probes. There's a need for better evaluation and understanding of these latent reasoning processes.

Method: The authors model latent chain-of-thought as a causal process in representation space using structural causal models (SCM), treating latent steps as variables and analyzing their effects through step-wise do-interventions. They study two representative paradigms (Coconut and CODI) on mathematical and general reasoning tasks, investigating three key questions about causal necessity, influence propagation, and answer mode retention.

Result: They find that (1) latent-step budgets behave like staged functionality with non-local routing rather than homogeneous extra depth, and (2) there's a persistent gap between early output bias and late representational commitment. This suggests mode-conditional and stability-aware analyses are needed.

Conclusion: The results motivate mode-conditional and stability-aware analyses along with corresponding training/decoding objectives as more reliable tools for interpreting and improving latent reasoning systems. The causal framework provides a foundation for better understanding and enhancing these models.

Abstract: Latent or continuous chain-of-thought methods replace explicit textual rationales with a number of internal latent steps, but these intermediate computations are difficult to evaluate beyond correlation-based probes. In this paper, we view latent chain-of-thought as a manipulable causal process in representation space by modeling latent steps as variables in a structural causal model (SCM) and analyzing their effects through step-wise $\mathrm{do}$-interventions. We study two representative paradigms (i.e., Coconut and CODI) on both mathematical and general reasoning tasks to investigate three key questions: (1) which steps are causally necessary for correctness and when answers become decidable early; (2) how does influence propagate across steps, and how does this structure compare to explicit CoT; and (3) do intermediate trajectories retain competing answer modes, and how does output-level commitment differ from representational commitment across steps. We find that latent-step budgets behave less like homogeneous extra depth and more like staged functionality with non-local routing, and we identify a persistent gap between early output bias and late representational commitment. These results motivate mode-conditional and stability-aware analyses -- and corresponding training/decoding objectives -- as more reliable tools for interpreting and improving latent reasoning systems.

</details>


### [298] [The Use of AI Tools to Develop and Validate Q-Matrices](https://arxiv.org/abs/2602.08796)
*Kevin Fan,Jacquelyn A. Bialo,Hongli Li*

Main category: cs.AI

TL;DR: 本研究探究AI模型在认知诊断Q矩阵构建中的应用潜力，发现Google Gemini 2.5 Pro与已验证Q矩阵的一致性（Kappa=0.63）超越人类专家，但后续新版AI性能反而下降，揭示AI辅助的波动性。


<details>
  <summary>Details</summary>
Motivation: 构建Q矩阵是认知诊断建模中关键但高度依赖人工且耗时的环节，亟需探索AI工具能否有效辅助以提升效率与质量。

Method: 于2025年5月采用多组AI模型（如Google Gemini 2.5 Pro）基于相同训练材料生成Q矩阵，通过与Li和Suen（2013）已验证Q矩阵及人类评分者结果进行Cohen's kappa一致性检验；并于2026年1月跟进测试新版AI性能。

Result: Google Gemini 2.5 Pro与已验证Q矩阵达到最高一致性（Kappa=0.63），显著优于所有人类专家；但2026年新版AI的一致性却出现下降，且不同AI模型间差异显著，表明技术迭代未必稳定提升性能。

Conclusion: 研究证实AI在Q矩阵构建中具有辅助价值，但模型性能受版本与时间影响存在不稳定性，未来需加强AI与人类专家的协作机制，并深化对AI认知推理过程的可控性研究。

Abstract: Constructing a Q-matrix is a critical but labor-intensive step in cognitive diagnostic modeling (CDM). This study investigates whether AI tools (i.e., general language models) can support Q-matrix development by comparing AI-generated Q-matrices with a validated Q-matrix from Li and Suen (2013) for a reading comprehension test. In May 2025, multiple AI models were provided with the same training materials as human experts. Agreement among AI-generated Q-matrices, the validated Q-matrix, and human raters' Q-matrices was assessed using Cohen's kappa. Results showed substantial variation across AI models, with Google Gemini 2.5 Pro achieving the highest agreement (Kappa = 0.63) with the validated Q-matrix, exceeding that of all human experts. A follow-up analysis in January 2026 using newer AI versions, however, revealed lower agreement with the validated Q-matrix. Implications and directions for future research are discussed.

</details>


### [299] [Root Cause Analysis Method Based on Large Language Models with Residual Connection Structures](https://arxiv.org/abs/2602.08804)
*Liming Zhou,Ailing Liu,Hongwei Liu,Min He,Heng Zhang*

Main category: cs.AI

TL;DR: 针对大规模微服务架构中的根因定位难题，提出RC-LLM方法，结合残差连接结构和LLM的上下文推理能力，在CCF-AIOps数据集上实现了高精度高效的根因分析。


<details>
  <summary>Details</summary>
Motivation: 复杂大规模微服务架构中，微服务间故障传播复杂且遥测数据（指标、日志、追踪）维度高，现有根因分析方法效果有限。

Method: 设计残差式分层融合结构集成多源遥测数据，利用大语言模型的上下文推理能力建模时间和跨微服务因果依赖。

Result: 在CCF-AIOps微服务数据集上，RC-LLM展现出较高的准确性和效率。

Conclusion: 该方法有效解决了微服务架构的根因定位问题，为大规模分布式系统故障诊断提供了可行方案。

Abstract: Root cause localization remain challenging in complex and large-scale microservice architectures. The complex fault propagation among microservices and the high dimensionality of telemetry data, including metrics, logs, and traces, limit the effectiveness of existing root cause analysis (RCA) methods. In this paper, a residual-connection-based RCA method using large language model (LLM), named RC-LLM, is proposed. A residual-like hierarchical fusion structure is designed to integrate multi-source telemetry data, while the contextual reasoning capability of large language models is leveraged to model temporal and cross-microservice causal dependencies. Experimental results on CCF-AIOps microservice datasets demonstrate that RC-LLM achieves strong accuracy and efficiency in root cause analysis.

</details>


### [300] [Negative-Aware Diffusion Process for Temporal Knowledge Graph Extrapolation](https://arxiv.org/abs/2602.08815)
*Yanglei Gan,Peng He,Yuxiang Cai,Run Lin,Guanyu Zhou,Qiao Liu*

Main category: cs.AI

TL;DR: This paper proposes NADEx, a diffusion model for Temporal Knowledge Graph extrapolation that addresses two key limitations: (1) conditioning only on positive evidence while ignoring negative context, and (2) weak supervision of denoised embedding calibration. By encoding temporal-relational histories into embeddings, using a Transformer denoiser, and introducing a cosine-alignment regularizer from negative prototypes, NADEx tightens decision boundaries against implausible facts and achieves state-of-the-art performance on four benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models for TKG reasoning only use positive evidence and rely on cross-entropy ranking that poorly supervises embedding calibration, limiting their ability to distinguish plausible future facts from implausible ones.

Method: NADEx encodes subject-centric entity/relation/temporal histories into sequential embeddings, perturbs query objects via forward diffusion, reconstructs them with a Transformer denoiser conditioned on temporal-relational context, and applies a cosine-alignment regularizer from batch-wise negative prototypes to tighten decision boundaries.

Result: Comprehensive experiments on four public TKG benchmarks demonstrate that NADEx achieves state-of-the-art performance.

Conclusion: NADEx successfully bridges the gaps in existing diffusion models by incorporating negative-aware learning and enhanced embedding calibration, establishing new SOTA for temporal knowledge graph extrapolation.

Abstract: Temporal Knowledge Graph (TKG) reasoning seeks to predict future missing facts from historical evidence. While diffusion models (DM) have recently gained attention for their ability to capture complex predictive distributions, two gaps remain: (i) the generative path is conditioned only on positive evidence, overlooking informative negative context, and (ii) training objectives are dominated by cross-entropy ranking, which improves candidate ordering but provides little supervision over the calibration of the denoised embedding. To bridge this gap, we introduce Negative-Aware Diffusion model for TKG Extrapolation (NADEx). Specifically, NADEx encodes subject-centric histories of entities, relations and temporal intervals into sequential embeddings. NADEx perturbs the query object in the forward process and reconstructs it in reverse with a Transformer denoiser conditioned on the temporal-relational context. We further derive a cosine-alignment regularizer derived from batch-wise negative prototypes, which tightens the decision boundary against implausible candidates. Comprehensive experiments on four public TKG benchmarks demonstrate that NADEx delivers state-of-the-art performance.

</details>


### [301] [Learning the Value Systems of Societies with Preference-based Multi-objective Reinforcement Learning](https://arxiv.org/abs/2602.08835)
*Andrés Holgado-Sánchez,Peter Vamplew,Richard Dazeley,Sascha Ossowski,Holger Billhardt*

Main category: cs.AI

TL;DR: 本文提出一种基于聚类和偏好多目标强化学习的算法，用于在马尔可夫决策过程中学习价值对齐模型和群体价值体系，解决现有方法缺乏可解释性和适应性的问题。


<details>
  <summary>Details</summary>
Motivation: 价值感知AI需要识别人类价值观并适应不同用户的价值体系，但当前的价值操作化方法存在误设风险，且顺序决策中的个性化方法需要手动设计特征或缺乏基于价值的可解释性和对不同用户偏好的适应能力。

Method: 提出基于聚类和偏好多目标强化学习(PbMORL)的算法，联合学习社会衍生的价值对齐模型(基础)和能简洁表示社会中不同用户群体的价值体系集合，每个聚类包含代表其成员价值偏好的价值系统和近似帕累托最优的决策策略。

Result: 在两个包含人类价值观的马尔可夫决策过程上，将该方法与最先进的PbMORL算法及基线方法进行了对比评估。

Conclusion: 所提算法能够学习可解释且适应不同用户群体的价值系统，同时保持社会价值对齐，为价值感知AI提供了更有效的实现框架。

Abstract: Value-aware AI should recognise human values and adapt to the value systems (value-based preferences) of different users. This requires operationalization of values, which can be prone to misspecification. The social nature of values demands their representation to adhere to multiple users while value systems are diverse, yet exhibit patterns among groups. In sequential decision making, efforts have been made towards personalization for different goals or values from demonstrations of diverse agents. However, these approaches demand manually designed features or lack value-based interpretability and/or adaptability to diverse user preferences.
  We propose algorithms for learning models of value alignment and value systems for a society of agents in Markov Decision Processes (MDPs), based on clustering and preference-based multi-objective reinforcement learning (PbMORL). We jointly learn socially-derived value alignment models (groundings) and a set of value systems that concisely represent different groups of users (clusters) in a society. Each cluster consists of a value system representing the value-based preferences of its members and an approximately Pareto-optimal policy that reflects behaviours aligned with this value system. We evaluate our method against a state-of-the-art PbMORL algorithm and baselines on two MDPs with human values.

</details>


### [302] [Deciding the Satisfiability of Combined Qualitative Constraint Networks](https://arxiv.org/abs/2602.08848)
*Quentin Cohen-Solal,Alexandre Niveau,Maroua Bouzid*

Main category: cs.AI

TL;DR: 本文提出了一个统一的形式化框架，用于整合定性推理中的多种扩展和组合形式（多尺度推理、时间序列和松散集成），通过两个互补定理确保可满足性判定为多项式时间，并推广了定性形式主义的定义。


<details>
  <summary>Details</summary>
Motivation: 在人工智能的各种推理形式中，定性推理能够在没有数值的情况下从不精确、不完整的信息中推断新知识。现有定性形式主义的扩展和组合缺乏统一的处理框架，无法系统性地研究其可满足性判定和复杂度。

Method: 提出一个统一的形式化框架，通过整合多尺度推理、时间序列和松散集成等多种定性形式主义的扩展和组合形式，建立一个通用的推理基础结构。

Result: 建立了两个确保可满足性判定为多项式时间的互补定理，利用这些定理恢复了大小-拓扑组合的已知结果，并将主要定性形式主义定义推广到包含文献中排除但对组合重要的形式主义。

Conclusion: 该统一框架不仅支持多种组合和扩展情境下的推理，还提供了一种系统化的方法来研究可满足性判定及其复杂度，为定性推理的理论和应用奠定了更坚实的基础。

Abstract: Among the various forms of reasoning studied in the context of artificial intelligence, qualitative reasoning makes it possible to infer new knowledge in the context of imprecise, incomplete information without numerical values. In this paper, we propose a formal framework unifying several forms of extensions and combinations of qualitative formalisms, including multi-scale reasoning, temporal sequences, and loose integrations. This framework makes it possible to reason in the context of each of these combinations and extensions, but also to study in a unified way the satisfiability decision and its complexity. In particular, we establish two complementary theorems guaranteeing that the satisfiability decision is polynomial, and we use them to recover the known results of the size-topology combination. We also generalize the main definition of qualitative formalism to include qualitative formalisms excluded from the definitions of the literature, important in the context of combinations.

</details>


### [303] [Efficient and Stable Reinforcement Learning for Diffusion Language Models](https://arxiv.org/abs/2602.08905)
*Jiawei Liu,Xiting Wang,Yuanyuan Zhong,Defu Lian,Yu Yang*

Main category: cs.AI

TL;DR: 提出时空剪枝(STP)框架，通过空间和时间两个维度压缩扩散大语言模型强化学习中的冗余计算，提升效率和稳定性


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型(dLLMs)需要强化学习(RL)解锁复杂推理能力，但直接应用RL面临效率和稳定性挑战

Method: 设计时空剪枝框架：空间剪枝用静态先验约束探索空间；时间剪枝跳过冗余的后期细化步骤，理论证明能严格降低对数似然估计方差

Result: 在效率和准确率上均超越现有最优基线方法，理论保证策略更新更稳定

Conclusion: STP通过剪枝冗余生成过程，有效解决了RL应用于dLLMs的效率和稳定性问题，提供了理论保障和实践优势

Abstract: Reinforcement Learning (RL) is crucial for unlocking the complex reasoning capabilities of Diffusion-based Large Language Models (dLLMs). However, applying RL to dLLMs faces unique challenges in efficiency and stability. To address these challenges, we propose Spatio-Temporal Pruning (STP), a framework designed to simultaneously improve the efficiency and stability of RL for dLLMs. STP compresses the redundancy in the generative process through: (1) \textit{spatial pruning}, which constrains the exploration space using static priors; and (2) \textit{temporal pruning}, which bypasses redundant late-stage refinement steps. Our theoretical analysis demonstrates that STP strictly reduces the variance of the log-likelihood estimation, thereby ensuring more stable policy updates. Extensive experiments demonstrate that STP surpasses state-of-the-art baselines in both efficiency and accuracy. Our code is available at https://github.com/Lolo1222/STP.

</details>


### [304] [CausalT5K: Diagnosing and Informing Refusal for Trustworthy Causal Reasoning of Skepticism, Sycophancy, Detection-Correction, and Rung Collapse](https://arxiv.org/abs/2602.08939)
*Longling Geng,Andy Ouyang,Theodore Wu,Daphne Barretto,Matthew John Hayes,Rachael Cooper,Yuqiao Zeng,Sameer Vijay,Gia Ancone,Ankit Rai,Matthew Wolfman,Patrick Flanagan,Edward Y. Chang*

Main category: cs.AI

TL;DR: CausalT5K是一个包含5000多个案例、涵盖10个领域的诊断基准，通过将因果陷阱嵌入真实叙事中，系统性诊断大语言模型的因果推理失败（层级崩溃、迎合性漂移和错误校准拒绝），并揭示静态审计策略普遍失效的问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在因果推理方面存在已知失败模式（包括迎合、层级崩溃和错误校准拒绝），但由于缺乏能够进行系统性诊断的基准，修复这些问题的进展缓慢。

Method: 构建CausalT5K基准，包含5000+真实案例，测试模型三层能力：检测层级崩溃（用关联性证据回答干预性查询）、抵抗对抗压力下的迎合性漂移、生成明智拒绝（在证据不足时指明缺失信息）。采用人机协作流程开发，包括40名领域专家、迭代交叉验证和规则+LLM+人工的复合验证，以Pearl因果阶梯为理论基础，将性能分解为效用（敏感度）和安全（特异度）两个维度。

Result: 初步实验发现"四象限控制图景"，证明静态审计策略在该场景下普遍失效，展示了CausalT5K在推进可信推理系统方面的价值。

Conclusion: CausalT5K作为研究基础设施，能够揭示传统聚合准确率无法发现的失败模式，为系统性诊断和改进大语言模型的因果推理能力提供了有效工具。

Abstract: LLM failures in causal reasoning, including sycophancy, rung collapse, and miscalibrated refusal, are well-documented, yet progress on remediation is slow because no benchmark enables systematic diagnosis. We introduce CausalT5K, a diagnostic benchmark of over 5,000 cases across 10 domains that tests three critical capabilities: (1) detecting rung collapse, where models answer interventional queries with associational evidence; (2) resisting sycophantic drift under adversarial pressure; and (3) generating Wise Refusals that specify missing information when evidence is underdetermined. Unlike synthetic benchmarks, CausalT5K embeds causal traps in realistic narratives and decomposes performance into Utility (sensitivity) and Safety (specificity), revealing failure modes invisible to aggregate accuracy. Developed through a rigorous human-machine collaborative pipeline involving 40 domain experts, iterative cross-validation cycles, and composite verification via rule-based, LLM, and human scoring, CausalT5K implements Pearl's Ladder of Causation as research infrastructure. Preliminary experiments reveal a Four-Quadrant Control Landscape where static audit policies universally fail, a finding that demonstrates CausalT5K's value for advancing trustworthy reasoning systems. Repository: https://github.com/genglongling/CausalT5kBench

</details>


### [305] [CoRefine: Confidence-Guided Self-Refinement for Adaptive Test-Time Compute](https://arxiv.org/abs/2602.08948)
*Chen Jin,Ryutaro Tanno,Tom Diethe,Philip Teare*

Main category: cs.AI

TL;DR: 提出CoRefine，一种轻量级(211k参数)置信度引导的自精炼方法，通过控制器动态决定暂停、重审或改变策略，相比512路并行采样实现190倍token削减，同时保持竞争力精度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型依赖测试时扩展(如512路并行解码)提升推理准确率，但计算成本过高，需要更高效的token利用方案。

Method: 在冻结LLM上构建轻量级Conv1D控制器，利用完整轨迹置信度作为控制信号，动态决策精炼动作(暂停/重审/改策略)，并扩展出混合序列-并行变体CoRefine-Tree。

Result: 平均每题仅需2.7次精炼步骤，token消耗降低190倍；置信暂停时精度达92.6%，置信度动态可无需标注可靠指示正确性，在多个模型和基准上表现优异。

Conclusion: 将置信度视为控制信号而非正确性保证，CoRefine为可扩展推理和 imperfect verifier 的智能体场景提供了模块化基础组件。

Abstract: Large Language Models (LLMs) often rely on test-time scaling via parallel decoding (for example, 512 samples) to boost reasoning accuracy, but this incurs substantial compute. We introduce CoRefine, a confidence-guided self-refinement method that achieves competitive accuracy using a fraction of the tokens via a lightweight 211k-parameter Conv1D controller atop a frozen LLM. The controller consumes full-trace confidence to decide whether to halt, re-examine, or try a different approach, enabling targeted self-correction with an average of 2.7 refinement steps per problem and roughly 190-fold token reduction relative to 512-sample baselines. Across diverse reasoning benchmarks and three open-source models, the controller achieves 92.6 percent precision when it confidently halts, indicating that confidence dynamics reliably signal correctness without ground-truth verification. We extend this to CoRefine-Tree, a hybrid sequential-parallel variant that adaptively balances exploration and exploitation, with easy serving integration and verifier compatibility. By treating confidence as a control signal rather than a correctness guarantee, CoRefine provides a modular primitive for scalable reasoning and agentic settings with imperfect verifiers.

</details>


### [306] [Digital Twin and Agentic AI for Wild Fire Disaster Management: Intelligent Virtual Situation Room](https://arxiv.org/abs/2602.08949)
*Mohammad Morsali,Siavash H. Khajavi*

Main category: cs.AI

TL;DR: The paper proposes IVSR, an intelligent virtual situation room using bidirectional digital twin technology and autonomous AI agents for real-time wildfire monitoring and adaptive disaster management, significantly reducing response time and improving resource coordination.


<details>
  <summary>Details</summary>
Motivation: Wildfire frequency and intensity are projected to increase 14% by 2030 and 30% by 2050 due to global warming, posing critical threats. Conventional disaster management relies on static simulations and passive data, unable to adapt to evolving wildfires in real-time.

Method: IVSR is a bidirectional Digital Twin platform with AI agents that continuously integrates multisource sensor imagery, weather data, and 3D forest models to create a live virtual fire replica. An AI similarity engine aligns emerging conditions with a precomputed Disaster Simulation Library to retrieve intervention tactics, enabling authorized actions like UAV redeployment and crew reallocation to be fed back to the physical layer.

Result: Case studies demonstrate capabilities in localized incident detection, privacy-preserving playback, collider-based fire-spread projection, and site-specific ML retraining, with marked reductions in detection-to-intervention latency and more effective resource coordination versus traditional systems.

Conclusion: IVSR offers a scalable, semi-automated decision-support paradigm for proactive, adaptive wildfire disaster management by uniting real-time bidirectional digital twins with agentic AI.

Abstract: According to the United Nations, wildfire frequency and intensity are projected to increase by approximately 14% by 2030 and 30% by 2050 due to global warming, posing critical threats to life, infrastructure, and ecosystems. Conventional disaster management frameworks rely on static simulations and passive data acquisition, hindering their ability to adapt to arbitrarily evolving wildfire episodes in real-time. To address these limitations, we introduce the Intelligent Virtual Situation Room (IVSR), a bidirectional Digital Twin (DT) platform augmented by autonomous AI agents. The IVSR continuously ingests multisource sensor imagery, weather data, and 3D forest models to create a live virtual replica of the fire environment. A similarity engine powered by AI aligns emerging conditions with a precomputed Disaster Simulation Library, retrieving and calibrating intervention tactics under the watchful eyes of experts. Authorized action-ranging from UAV redeployment to crew reallocation-is cycled back through standardized procedures to the physical layer, completing the loop between response and analysis. We validate IVSR through detailed case-study simulations provided by an industrial partner, demonstrating capabilities in localized incident detection, privacy-preserving playback, collider-based fire-spread projection, and site-specific ML retraining. Our results indicate marked reductions in detection-to-intervention latency and more effective resource coordination versus traditional systems. By uniting real-time bidirectional DTs with agentic AI, IVSR offers a scalable, semi-automated decision-support paradigm for proactive, adaptive wildfire disaster management.

</details>


### [307] [stable-worldmodel-v1: Reproducible World Modeling Research and Evaluation](https://arxiv.org/abs/2602.08968)
*Lucas Maes,Quentin Le Lidec,Dan Haramati,Nassim Massaudi,Damien Scieur,Yann LeCun,Randall Balestriero*

Main category: cs.AI

TL;DR: This paper introduces stable-worldmodel (SWM), a modular world-model research ecosystem that provides standardized tools, environments, and baselines to improve reusability and evaluation standardization, demonstrated through a zero-shot robustness study on DINO-WM.


<details>
  <summary>Details</summary>
Motivation: World Models are powerful for learning predictive environment representations, but most implementations are publication-specific, severely limiting reusability, increasing bug risks, and reducing evaluation standardization.

Method: Proposes stable-worldmodel (SWM), a modular, tested, and documented ecosystem offering efficient data-collection tools, standardized environments, planning algorithms, baseline implementations, and controllable environmental factors (visual/physical properties) for robustness and continual learning research.

Result: Demonstrated SWM's utility by using it to study zero-shot robustness in DINO-WM.

Conclusion: SWM addresses critical reusability and standardization challenges in world model research, providing a robust platform for future research on robustness and continual learning.

Abstract: World Models have emerged as a powerful paradigm for learning compact, predictive representations of environment dynamics, enabling agents to reason, plan, and generalize beyond direct experience. Despite recent interest in World Models, most available implementations remain publication-specific, severely limiting their reusability, increasing the risk of bugs, and reducing evaluation standardization. To mitigate these issues, we introduce stable-worldmodel (SWM), a modular, tested, and documented world-model research ecosystem that provides efficient data-collection tools, standardized environments, planning algorithms, and baseline implementations. In addition, each environment in SWM enables controllable factors of variation, including visual and physical properties, to support robustness and continual learning research. Finally, we demonstrate the utility of SWM by using it to study zero-shot robustness in DINO-WM.

</details>


### [308] [InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery](https://arxiv.org/abs/2602.08990)
*Shiyang Feng,Runmin Ma,Xiangchao Yan,Yue Fan,Yusong Hu,Songtao Huang,Shuaiyu Zhang,Zongsheng Cao,Tianshuo Peng,Jiakang Yuan,Zijie Guo,Zhijie Zhong,Shangheng Du,Weida Wang,Jinxin Shi,Yuhao Zhou,Xiaohan He,Zhiyin Yu,Fangchen Yu,Qihao Zheng,Jiamin Wu,Mianxin Liu,Chi Zhang,Shaowei Hou,Shuya Li,Yankai Jiang,Wenjie Lou,Lilong Wang,Zifu Wang,Jiong Wang,Wanghan Xu,Yue Deng,Dongrui Liu,Yiheng Wang,Wenlong Zhang,Fenghua Ling,Shufei Zhang,Xiaosong Wang,Shuangjia Zheng,Xun Huang,Siqi Sun,Shuyue Hu,Peng Ye,Chunfeng Song,Bin Wang,Conghui He,Yihao Liu,Xin Li,Qibin Hou,Tao Chen,Xiangyu Yue,Bin Wang,Liang He,Dahua Lin,Bowen Zhou,Bo Zhang,Lei Bai*

Main category: cs.AI

TL;DR: InternAgent-1.5是一个面向计算与实证科学发现的一体化系统，通过生成、验证、进化三子系统架构实现自主科学发现，在多项基准测试及真实科研任务中表现领先。


<details>
  <summary>Details</summary>
Motivation: 解决现有系统在跨计算与实验科学领域进行端到端、长周期自主发现的局限性，构建能协调建模与实验的统一框架。

Method: 采用三协同子系统架构（生成/验证/进化），结合深度研究、方案优化与长程记忆等基础能力，支持跨领域持续发现循环。

Result: 在GAIA/HLE/GPQA/FrontierScience基准测试中达到领先性能；自主设计机器学习算法并完成地球/生命/生物/物理领域的计算或湿实验发现任务。

Conclusion: 证明该系统可作为通用可扩展的自主科学发现框架，有效整合计算建模与实验室实验，推动科研范式变革。

Abstract: We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.

</details>


### [309] [iGRPO: Self-Feedback-Driven LLM Reasoning](https://arxiv.org/abs/2602.09000)
*Ali Hatamizadeh,Shrimai Prabhumoye,Igor Gitman,Ximing Lu,Seungju Han,Wei Ping,Yejin Choi,Jan Kautz*

Main category: cs.AI

TL;DR: 本文提出 iGRPO（迭代组相对策略优化），一种基于 GRPO 的两阶段强化学习框架，通过动态自条件化和模型生成的草稿来提升大语言模型在数学推理任务上的准确性和一致性，在 AIME24/25 基准上达到新的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在解决复杂数学问题时仍存在准确性和一致性不足的问题，尽管强化学习（RL）框架能改善模型质量，但现有方法如 GRPO 仍有提升空间。需要更高效的算法来充分挖掘模型的推理潜力。

Method: iGRPO 在 GRPO 基础上引入两阶段迭代机制：第一阶段采样多个探索性草稿并基于奖励信号选择最优草稿；第二阶段将最优草稿作为条件附加到原始提示上，对草稿条件下的改进进行 GRPO 式更新，训练策略超越其最强先前尝试。该方法通过组相对奖励归一化实现价值函数无关的高效优化。

Result: 在匹配的 rollout 预算下，iGRPO 在 Nemotron-H-8B 和 DeepSeek-R1 Distilled 等基础模型上持续优于 GRPO。在 AceReason-Math 上训练的 OpenReasoning-Nemotron-7B 应用 iGRPO 后，AIME24 和 AIME25 基准分别达到 85.62% 和 79.64% 的新最先进结果。消融研究表明改进包装器可推广到 GRPO 变体，受益于生成式评判器，并通过延迟熵崩溃改变学习动态。

Conclusion: 迭代式、基于自反馈的强化学习方法对推进可验证的数学推理具有显著潜力，iGRPO 通过动态自条件化机制有效提升了模型性能，为未来研究提供了有价值的框架。

Abstract: Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization. We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements, training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\% and 79.64\% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse. These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning.

</details>


### [310] [Data Science and Technology Towards AGI Part I: Tiered Data Management](https://arxiv.org/abs/2602.09003)
*Yudong Wang,Zixuan Fu,Hengyu Zhao,Chen Zhao,Chuyue Zhou,Xinle Lin,Hongya Lyu,Shuaikang Xue,Yi Yi,Yingjiao Wang,Zhi Zheng,Yuzhou Zhang,Jie Zhou,Chaojun Xiao,Xu Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: 提出L0-L4分层数据管理框架，实现数据与模型协同进化，通过LLM驱动的数据质量优化，显著提升大模型训练效率与性能


<details>
  <summary>Details</summary>
Motivation: 当前大模型研究过度依赖数据规模单向扩展，面临数据可用性、获取成本和训练效率的瓶颈，需要转向数据-模型协同进化新阶段

Method: 设计L0-L4五层数据管理体系（从原始未筛选资源到可验证知识），利用LLM进行质量评分和内容编辑，在各训练阶段（预训练、中期训练、对齐）进行数据策略分配

Result: 实证研究表明，分层数据利用在多个训练阶段显著提升训练效率和模型性能，已开源分层数据集和处理工具

Conclusion: 该框架为可扩展、可持续的数据管理提供系统化方案，标志着AGI发展进入数据质量与模型能力相互促进的新阶段

Abstract: The development of artificial intelligence can be viewed as an evolution of data-driven learning paradigms, with successive shifts in data organization and utilization continuously driving advances in model capability. Current LLM research is dominated by a paradigm that relies heavily on unidirectional scaling of data size, increasingly encountering bottlenecks in data availability, acquisition cost, and training efficiency. In this work, we argue that the development of AGI is entering a new phase of data-model co-evolution, in which models actively guide data management while high-quality data, in turn, amplifies model capabilities. To implement this vision, we propose a tiered data management framework, designed to support the full LLM training lifecycle across heterogeneous learning objectives and cost constraints. Specifically, we introduce an L0-L4 tiered data management framework, ranging from raw uncurated resources to organized and verifiable knowledge. Importantly, LLMs are fully used in data management processes, such as quality scoring and content editing, to refine data across tiers. Each tier is characterized by distinct data properties, management strategies, and training roles, enabling data to be strategically allocated across LLM training stages, including pre-training, mid-training, and alignment. The framework balances data quality, acquisition cost, and marginal training benefit, providing a systematic approach to scalable and sustainable data management. We validate the effectiveness of the proposed framework through empirical studies, in which tiered datasets are constructed from raw corpora and used across multiple training phases. Experimental results demonstrate that tier-aware data utilization significantly improves training efficiency and model performance. To facilitate further research, we release our tiered datasets and processing tools to the community.

</details>


### [311] [GEBench: Benchmarking Image Generation Models as GUI Environments](https://arxiv.org/abs/2602.09007)
*Haodong Li,Jingwei Wu,Quan Sun,Guopeng Li,Juanxi Tian,Huanyu Zhang,Yanlin Lai,Ruichuan An,Hongbo Peng,Yuhong Dai,Chenxi Li,Chunmei Qing,Jia Wang,Ziyang Meng,Zheng Ge,Xiangyu Zhang,Daxin Jiang*

Main category: cs.AI

TL;DR: 本文提出了GEBench，一个用于评估GUI生成中动态交互和时序一致性的综合基准测试，包含700个样本和五维评估指标GE-Score。实验发现现有模型在单步交互表现良好，但在长序列交互的时序一致性方面存在显著挑战。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成模型在GUI状态预测方面取得进展，但评估基准主要关注通用视觉保真度，缺乏对GUI特定场景下状态转换和时序一致性的系统性评估。这一空白阻碍了高保真生成式GUI环境的发展。

Method: 1) 构建GEBench基准，包含700个精心筛选样本，涵盖5类任务：单步交互、多步轨迹、真实/虚构场景和定位点标注；2) 提出GE-Score五维评估指标：目标达成度、交互逻辑性、内容一致性、UI合理性和视觉质量。

Result: 对当前模型的广泛评估显示：模型在单步转换上表现良好，但在长交互序列中维持时序一致性和空间定位方面存在显著困难。研究发现图标理解、文本渲染和定位精度是三个关键瓶颈。

Conclusion: 该工作为生成式GUI环境提供了系统性评估基础，明确了当前技术局限，为未来研究方向提供了明确路径。代码已开源。

Abstract: Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [312] [DISCOVER: A Physics-Informed, GPU-Accelerated Symbolic Regression Framework](https://arxiv.org/abs/2602.06986)
*Udaykumar Gajera,Mohsen Sotoudeh,Kanchan Sarkar,Axel Groß*

Main category: physics.comp-ph

TL;DR: DISCOVER是一个开源符号回归工具包，通过模块化物理驱动设计解决现有工具在Python集成、搜索空间控制和计算效率方面的不足，特别适用于计算物理/化学和材料科学中的可解释模型发现


<details>
  <summary>Details</summary>
Motivation: 现有符号回归工具（如SISSO）在集成现代Python工作流、控制符号搜索空间和应对大规模计算需求方面存在局限，阻碍了材料科学中可解释描述符的高效发现

Method: 采用模块化物理驱动设计：1) 允许领域知识引导搜索 2) 支持显式特征空间约束 3) 提供可选GPU加速，实现可复现可扩展的工作流

Result: 成功开发了DISCOVER工具包，针对计算物理/化学和材料科学优化，在可解释性、物理一致性和执行时间方面表现突出

Conclusion: 该工具通过强调物理意义模型的发现，有效补充了通用符号回归框架，为数据密集型科学计算提供了高效解决方案

Abstract: Symbolic Regression (SR) enables the discovery of interpretable mathematical relationships from experimental and simulation data. These relationships are often coined descriptors which are defined as a fundamental materials property that is directly correlated to a desired or undesired functional property of the material. Although established approaches such as Sure Independence Screening and Sparsifying Operator (SISSO) have successfully identified low-dimensional descriptors within large feature spaces many existing SR tools integrate poorly with modern Python workflows, offer limited control over the symbolic search space, or struggle with the computational demands of large-scale studies. This paper introduces DISCOVER (Data-Informed Symbolic Combination of Operators for Variable Equation Regression), an open-source symbolic regression package developed to address these challenges through a modular, physics-motivated design. DISCOVER allows users to guide the symbolic search using domain knowledge, constrain the feature space explicitly, and take advantage of optional GPU acceleration to improve computational efficiency in data-intensive workflows, enabling reproducible and scalable SR workflows. The software is intended for applications in computational physics, computational chemistry, and materials science, where interpretability, physical consistency, and execution time are especially important, and it complements general-purpose SR frameworks by emphasizing the discovery of physically meaningful models.

</details>


### [313] [diffpy.morph: Python tools for model independent comparisons between sets of 1D functions](https://arxiv.org/abs/2602.06987)
*Andrew Yang,Christopher L. Farrow,Pavol Juhás,Luis Kitsu Iglesias,Chia-Hao Liu,Samuel D. Marks,Vivian R. K. Wall,Joshua Safin,Sean M. Drewry,Caden Myers,Dillon F. Hanlon,Nicholas Leonard,Cedomir Petrovic,Ahhyun Jeong,Dmitri V. Talapin,Linda F. Nazar,Haidong Zhou,Samuel W. Teitelbaum,Tim B. van Driel,Soham Banerjee,Emil S. Bozin,Michael F. Toney,Katharine Page,Naomi S. Ginsberg,Simon J. L. Billinge*

Main category: physics.comp-ph

TL;DR: diffpy.morph是一个开源Python工具包，通过应用简单变换（"morphs"）消除1D科学光谱间的非必要差异（如实验误差和热膨胀效应），从而揭示有意义的结构或化学变化。


<details>
  <summary>Details</summary>
Motivation: 直接比较1D光谱时，差异曲线常包含实验不一致性和良性物理变化等干扰因素，掩盖了有价值的科学信息，需要模型无关的方法提取真实差异。

Method: 对其中一个数据集应用简单变换（"morphs"）以匹配另一数据集，消除非必要差异后再进行差异分析。

Result: 成功应用于X射线和中子衍射/PDF数据，有效解决多种实验挑战；工具包可通过Python包索引和conda-forge获取。

Conclusion: 该软件包提供了一种通用、模型无关的方法来分析1D光谱差异，适用于材料科学等领域的实验数据处理。

Abstract: diffpy.morph addresses a need to gain scientific insights from 1D scientific spectra in model independent ways. A powerful approach for this is to take differences between pairs of spectra and look for meaningful changes that might indicate underlying chemical, structural, or other modifications. The challenge is that the difference curve may contain uninteresting differences such as experimental inconsistencies and benign physical changes such as the effects of thermal expansion. diffpy.morph allows researchers to apply simple transformations, or "morphs", to one of the datasets to remove the unwanted differences revealing, when they are present, non-trivial differences. diffpy.morph is an open-source Python package available on the Python Package Index and conda-forge. Here, we describe its functionality and apply it to solve a range of experimental challenges on diffraction and PDF data from x-rays and neutrons, though we note that it may be applied to any 1D function in principle.

</details>


### [314] [Event-Chain Monte Carlo: The global-balance breakthrough](https://arxiv.org/abs/2602.07199)
*E. A. J. F. Peters*

Main category: physics.comp-ph

TL;DR: This commentary reviews the 2009 Event-Chain Monte Carlo algorithm, explaining its mechanism through the Event-Driven Monte Carlo framework and demonstrating its generalization to continuous potentials and lifted Markov chains.


<details>
  <summary>Details</summary>
Motivation: To elucidate the underlying mechanism of the seminal 2009 Event-Chain Monte Carlo algorithm and show how its concepts generalize beyond the original hard-sphere formulation.

Method: Reviews the foundational Bernard-Krauth-Wilson work and uses the broader Event-Driven Monte Carlo (EDMC) framework to explain the mechanism, demonstrating natural generalization to continuous potentials and modern lifted Markov chain formalisms.

Result: Shows that the original hard-sphere Event-Chain concept generalizes to a powerful general class of sampling algorithms applicable to continuous potentials and modern Markov chain formalisms.

Conclusion: The Event-Chain Monte Carlo algorithm represents a general principle that extends far beyond its original hard-sphere formulation, transforming a specific surprising result into a powerful general class of sampling algorithms.

Abstract: The seminal 2009 paper by Bernard, Krauth, and Wilson marked a paradigm shift in Monte Carlo sampling. By abandoning the restrictive condition of detailed balance in favor of the more fundamental principle of global balance, they introduced the Event-Chain Monte Carlo (ECMC) algorithm, which achieves rejection-free, deterministic sampling for hard spheres. This breakthrough demonstrated that persistent, directional dynamics could dramatically accelerate equilibration in dense particle systems. In this commentary, we review this foundational work and elucidate its underlying mechanism using the broader Event-Driven Monte Carlo (EDMC) framework developed in subsequent years. We show how the original hard-sphere concept naturally generalizes to continuous potentials and modern lifted Markov chain formalisms, transforming a surprising specific result into a powerful general class of sampling algorithms.

</details>


### [315] [Compressed Sensing Methods for Memory Reduction in Monte Carlo Simulations](https://arxiv.org/abs/2602.07771)
*Ethan Lame,Camille Palmer,Todd Palmer,Ilham Variansyah*

Main category: physics.comp-ph

TL;DR: 该论文将压缩感知技术应用于中子输运蒙特卡罗模拟，通过重叠格子计数方法实现信号重建，在显著降低内存需求（2D达81.25%，3D达96.25%）的同时保持可接受的计算精度。


<details>
  <summary>Details</summary>
Motivation: 中子系统蒙特卡罗模拟计算密集且需要大量内存资源进行高精度建模，亟需降低计算成本。

Method: 采用压缩感知技术，通过重叠格子收集计数值实现信号重建；通过基追踪去噪算法优化重建过程，并分析采样数量和稀疏度参数对重建质量的影响。

Result: 在三个测试案例中，2D重建内存减少最高达81.25%，3D重建达96.25%；部分场景重建误差在1倍标准差范围内；增加采样数量可提升精度但边际效益递减；稀疏度参数显著影响重建质量。

Conclusion: 压缩感知结合重叠格子计数方法能有效降低蒙特卡罗模拟内存需求，在可接受的计算精度损失下实现计算效率的大幅提升，为高保真中子输运模拟提供了可行方案。

Abstract: Monte Carlo simulations of neutronic systems are computationally intensive and demand significant memory resources for high-fidelity modeling. Compressed sensing enables accurate reconstruction of signals from significantly fewer samples than traditional methods. The specific implementation of compressed sensing investigated here involves the use of overlapping cells to collect tallies. Increasing the number of samples improves the reconstruction accuracy, although the marginal gains diminish with more samples. Reconstruction quality is strongly influenced by the sparsity parameter used in basis pursuit denoising. Across the three test cases considered, memory reductions of up to 81.25% (96.25%) are demonstrated for 2D (3D) reconstructions, with select scenarios achieving reconstruction errors within 1 standard deviation of the corresponding high-fidelity reference results.

</details>


### [316] [dewi-kadita: A Python Library for Idealized Fish Schooling Simulation with Entropy-Based Diagnostics](https://arxiv.org/abs/2602.07948)
*Sandy H. S. Herho,Iwan P. Anwar,Faruq Khadami,Alfita P. Handayani,Karina A. Sujatmiko,Kamaluddin Kasim,Rusmawan Suwarman,Dasapta E. Irawan*

Main category: physics.comp-ph

TL;DR: This paper presents dewi-kadita, an open-source Python library for simulating fish collective motion using the 3D Couzin model with seven novel entropy-based diagnostics that combine into an Oceanic Schooling Index (OSI) to measure collective disorder, validated across four configurations with 10-100x speedup via Numba JIT compilation.


<details>
  <summary>Details</summary>
Motivation: Computational tools for simulating and analyzing fish collective dynamics are fragmented across research groups, lacking standardized, reproducible infrastructure analogous to established molecular dynamics codes.

Method: Develops dewi-kadita Python library implementing the 3D Couzin zone-based model with seven information-theoretic entropy metrics (cohesion, polarization, depth stratification, angular momentum, nearest-neighbor, velocity correlation, and shape entropy) that aggregate into a single Oceanic Schooling Index (OSI). Uses Numba JIT compilation for acceleration and NetCDF4 for interoperability.

Result: Validates model across four canonical configurations: swarm (P<0.1, OSI≈0.71), torus, dynamic parallel, and highly parallel (P=0.998, OSI=0.24, velocity correlation entropy→0). Successfully discriminates configurations with similar order parameters but different mechanisms. Achieves 10-100x speedup, simulating 150-250 agents over 1000-2000 steps in under 5 minutes on standard hardware.

Conclusion: The library provides standardized, reproducible infrastructure for collective behavior modeling, addressing fragmentation in the field while enabling comprehensive analysis of emergent self-organization in active matter systems through entropy diagnostics.

Abstract: Collective motion in fish schools exemplifies emergent self-organization in active matter systems, yet computational tools for simulating and analyzing these dynamics remain fragmented across research groups. We present dewi-kadita, an open-source Python library implementing the three-dimensional Couzin zone-based model with comprehensive entropy diagnostics tailored for marine collective behavior research. The library introduces seven information-theoretic metrics -- school cohesion entropy, polarization entropy, depth stratification entropy, angular momentum entropy, nearest-neighbor entropy, velocity correlation entropy, and school shape entropy -- that characterize distinct organizational features inaccessible to classical order parameters. These metrics combine into an Oceanic Schooling Index (OSI) providing a single scalar measure of collective disorder. Validation across four canonical configurations (swarm, torus, dynamic parallel, highly parallel) confirms correct reproduction of known phase behaviors: the swarm maintains disorder with polarization $P < 0.1$ and OSI $\approx 0.71$, while the highly parallel state achieves $P = 0.998$ with OSI $= 0.24$ and velocity correlation entropy vanishing to zero. The entropy framework successfully discriminates the torus and dynamic parallel configurations that exhibit comparable order parameter magnitudes through different organizational mechanisms. Numba just-in-time (JIT) compilation accelerates pairwise interaction calculations by $10$--$100\times$, enabling simulations of $150$--$250$ agents over $1000$--$2000$ time steps within five minutes on standard workstation hardware. NetCDF4 output ensures interoperability with oceanographic analysis tools. The library addresses the need for standardized, reproducible infrastructure in collective behavior modeling analogous to established molecular dynamics codes.

</details>


### [317] [An intramembranous ossification model for the in-silico analysis of bone tissue formation in tooth extraction sites](https://arxiv.org/abs/2602.08492)
*Jennifer Paola Corredor-Gómez,Andrés Mauricio Rueda-Ramírez,Miguel Alejandro Gamboa-Márquez,Carolina Torres-Rodríguez,Carlos Julio Cortés-Rodríguez*

Main category: physics.comp-ph

TL;DR: 该论文开发了一个基于有限元方法的膜内成骨数学模型，用于描述拔牙位点骨组织形成过程，通过模拟细胞相互作用、血管生成和氧依赖效应等机制，成功在犬类实验数据上验证，平均绝对误差仅3.04%，为牙科手术设计和未来骨整合研究提供了计算机模拟工具。


<details>
  <summary>Details</summary>
Motivation: 建立生物过程的精确模型以通过计算机模拟预测活体组织行为，避免体内实验的成本和伦理问题；特别地，口腔骨愈合模型有助于牙科手术中选择合适的外科技术。

Method: 构建膜内成骨的数学模型，描述不同类型细胞在生化因子影响下的相互作用及细胞外基质合成降解机制，重点关注血管生成、氧依赖效应和成纤维细胞凋亡；考虑下颌骨深度依赖性血管化及其对骨愈合的影响，提出 severed periodontal ligament (PDL) 上的细胞分布函数描述；采用有限元方法 (FEM) 实现模型。

Result: 通过模拟文献中报道的犬类体内实验对模型进行验证，模型结果与实验数据拟合良好，平均绝对误差仅为3.04%。

Conclusion: 所提出的数学框架可作为设计未来体外和体内试验的重要工具，并为后续关于骨整合和力学生物学的计算机模拟研究提供 precedent。

Abstract: The accurate modeling of biological processes allows to predict the spatio-temporal behavior of living tissues by computer-aided (in-silico) testing, a useful tool for the development of medical strategies, avoiding the expenses and potential ethical implications of in-vivo experimentation. A model for bone healing in mouth would be useful for selecting proper surgical techniques in dental procedures. In this paper, the formulation and implementation of a model for Intramembranous Ossification is presented aiming to describe the complex process of bone tissue formation in tooth extraction sites. The model consists in a mathematical description of the mechanisms in which different types of cells interact, synthesize and degrade extra-cellular matrices under the influence of biochemical factors. Special attention is given to angiogenesis, oxygen-dependent effects and growth factor-induced apoptosis of fibroblasts. Furthermore, considering the depth-dependent vascularization of mandibular bone and its influence on bone healing, a functional description of the cell distribution on the severed periodontal ligament (PDL) is proposed. The developed model was implemented using the finite element method (FEM) and successfully validated by simulating an animal in-vivo experiment on dogs reported in the literature. A good fit between model outcome and experimental data was obtained with a mean absolute error of 3.04%. The mathematical framework presented here may represent an important tool for the design of future in-vitro and in-vivo tests, as well as a precedent for future in-silico studies on osseointegration and mechanobiology.

</details>


### [318] [Tikhonov regularization-based reconstruction of partial scattering functions obtained from contrast variation small-angle neutron scattering](https://arxiv.org/abs/2602.08601)
*Manabu Machida,Koichi Mayumi*

Main category: physics.comp-ph

TL;DR: 提出Tikhonov正则化方法解决CV-SANS中微小绝对值部分散射函数重建不稳定的问题


<details>
  <summary>Details</summary>
Motivation: 现有CV-SANS通过SVD分解散射强度时，微小绝对值的部分散射函数因奇异值差异过大导致估计不稳定，影响多组分纳米结构分析精度

Method: 在奇异值分解(SVD)中引入Tikhonov正则化约束，优化散射强度分解过程

Result: 显著提升微小绝对值部分散射函数的重建稳定性，减少奇异值差异导致的数值振荡

Conclusion: 该方法有效改善CV-SANS对多组分体系纳米结构的定量分析可靠性，为弱散射信号解析提供新途径

Abstract: Contrast variation small-angle neutron scattering (CV-SANS) has been widely employed for nano structural analysis of multicomponent systems. In CV-SANS experiments, scattering intensities of samples with different scattering co\ ntrasts are decomposed into partial scattering functions, corresponding to structure of each component and cross-correlation between different components, by singular value decomposition (SVD). However, the estimation of partial scattering functions with small absolute values often suffers from instability due to the significant differences in the singular values. In this paper, we propose a remedy for this instability by introducing the Tikhonov regularization, which ensures more stable reconstruction of the partial scattering functions.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [319] [Tensor Hinted Mv Conjectures](https://arxiv.org/abs/2602.07242)
*Zhao Song*

Main category: cs.CC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Brand, Nanongkai, and Saranurak introduced a conjecture known as the Hinted Mv Conjecture. Although it was originally formulated for the matrix case, we generalize it here to the tensor setting.

</details>


### [320] [The Quantumly Fast and the Classically Forrious](https://arxiv.org/abs/2602.07503)
*Clément L. Canonne,Kenny Chen,Julián Mestre*

Main category: cs.CC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study the extremal Forrelation problem, where, provided with oracle access to Boolean functions $f$ and $g$ promised to satisfy either $\textrm{forr}(f,g)=1$ or $\textrm{forr}(f,g)=-1$, one must determine (with high probability) which of the two cases holds while performing as few oracle queries as possible. It is well known that this problem can be solved with \emph{one} quantum query; yet, Girish and Servedio (TQC 2025) recently showed this problem requires $\widetildeΩ(2^{n/4})$ classical queries, and conjectured that the optimal lower bound is $\widetildeΩ(2^{n/2})$. Through a completely different construction, we improve on their result and prove a lower bound of $Ω(2^{0.4999n})$, which matches the conjectured lower bound up to an arbitrarily small constant in the exponent.

</details>


### [321] [The Parameterized Complexity of Independent Set and More when Excluding a Half-Graph, Co-Matching, or Matching](https://arxiv.org/abs/2602.07606)
*Jan Dreier,Nikolas Mählmann,Sebastian Siebertz*

Main category: cs.CC

TL;DR: 该研究基于Ding等人定理，将图类按匹配指数、余匹配指数和半图指数的有界性分为八类，系统分类了独立集、团和支配集在这八类图上的参数化复杂度，提出新的FPT/W[1]-困难结果及近似算法


<details>
  <summary>Details</summary>
Motivation: 利用Ding等提出的 unavoidable patterns 定理，探究图结构参数（匹配/余匹配/半图指数）与经典图问题参数化复杂度之间的关联，填补现有分类空白

Method: 整合文献中的复杂度假设与归约技术，通过构造特定图类证明新结果：包括设计FPT算法、W[1]-困难性归约，以及开发有界半图指数类上的近似算法

Result: 1) 独立集在同时有界半图与余匹配指数的图上FPT；2) 存在仅半图指数有界的W[1]-困难类；3) 完成八类图×三问题的完整复杂度分类；4) 提出有界半图指数类独立集的近似算法

Conclusion: 建立了结构图论参数与参数化复杂度的完整映射，明确了三问题在八类图上的精确复杂度边界，为算法设计提供理论依据，近似算法缓解了困难类的计算瓶颈

Abstract: A theorem of Ding, Oporowski, Oxley, and Vertigan implies that any sufficiently large twin-free graph contains a large matching, a co-matching, or a half-graph as a semi-induced subgraph. The sizes of these unavoidable patterns are measured by the matching index, co-matching index, and half-graph index of a graph. Consequently, graph classes can be organized into the eight classes determined by which of the three indices are bounded.
  We completely classify the parameterized complexity of Independent Set, Clique, and Dominating Set across all eight of these classes. For this purpose, we first derive multiple tractability and hardness results from the existing literature, and then proceed to fill the identified gaps. Among our novel results, we show that Independent Set is fixed-parameter tractable on every graph class where the half-graph and co-matching indices are simultaneously bounded. Conversely, we construct a graph class with bounded half-graph index (but unbounded co-matching index), for which the problem is W[1]-hard.
  For the W[1]-hard cases of our classification, we review the state of approximation algorithms. Here, we contribute an approximation algorithm for Independent Set on classes of bounded half-graph index.

</details>


### [322] [Determining the Outerthickness of Graphs Is NP-Hard](https://arxiv.org/abs/2602.07607)
*Pin-Hsian Lee,Te-Cheng Liu,Meng-Tsung Tsai*

Main category: cs.CC

TL;DR: 本文证明确定图的外层厚度是NP-难的，解决长期公开问题。更一般地，对于任意满足三个封闭性质（拓扑子式、1-和、含三角形）的可判定图类F，边覆盖问题P_F在所有k≥3时均为NP-难，且这些条件缺一不可。


<details>
  <summary>Details</summary>
Motivation: 图的外层厚度计算复杂性是图论与计算复杂性理论中的长期公开问题。本文旨在解决此问题，并建立一般性框架以理解边覆盖问题的计算复杂性。

Method: 作者构造了一个简短、自包含的多项式时间归约证明，通过将已知NP-难问题归约到特定图类的边覆盖问题，建立计算困难性。

Result: - 外层厚度问题（F=外平面图类）是NP-难的
- 对平面图类，将Mansfield的厚度NP-难度从k=2推广到所有k≥3
- 证明三个条件（拓扑子式封闭、1-和封闭、含三角形）对NP-难度既充分又必要
- 当任一条件不满足时（欧拉图、伪森林、森林），问题可在多项式时间求解

Conclusion: 本文建立了边覆盖问题的完整复杂性二分法，揭示了三个基本图论性质如何精确决定问题的计算难度，为相关图分解问题提供了统一的理论框架。

Abstract: We give a short, self-contained, and easily verifiable proof that determining the outerthickness of a general graph is NP-hard. This resolves a long-standing open problem on the computational complexity of outerthickness.
  Moreover, our hardness result applies to a more general covering problem $P_F$, defined as follows. Fix a proper graph class $F$ whose membership is decidable. Given an undirected simple graph $G$ and an integer $k$, the task is to cover the edge set $E(G)$ by at most $k$ subsets $E_1,\ldots,E_k$ such that each subgraph $(V(G),E_i)$ belongs to $F$. Note that if $F$ is monotone (in particular, when $F$ is the class of all outerplanar graphs), any such cover can be converted into an edge partition by deleting overlaps; hence, in this case, covering and partitioning are equivalent.
  Our result shows that for every proper graph class $F$ whose membership is decidable and that satisfies all of the following conditions: (a) $F$ is closed under topological minors, (b) $F$ is closed under $1$-sums, and (c) $F$ contains a cycle of length $3$, the problem $P_F$ is NP-hard for every fixed integer $k\ge 3$. In particular:
  For $F$ equal to the class of all outerplanar graphs, our result settles the long-standing open problem on the complexity of determining outerthickness.
  For $F$ equal to the class of all planar graphs, our result complements Mansfield's NP-hardness result for the thickness, which applies only to the case $k=2$.
  It is also worth noting that each of the three conditions above is necessary. If $F$ is the class of all eulerian graphs, then cond. (a) fails. If $F$ is the class of all pseudoforests, then cond. (b) fails. If $F$ is the class of all forests, then cond. (c) fails. For each of these three classes $F$, the problem $P_F$ is solvable in polynomial time for every fixed integer $k\ge 3$, showing that none of the three conditions can be dropped.

</details>


### [323] [Expansive homeomorphisms on complexity quasi-metric spaces](https://arxiv.org/abs/2602.07685)
*Yaé U. Gaba*

Main category: cs.CC

TL;DR: 本文研究复杂性准度量空间上的扩张同胚，重点分析缩放变换ψ_α的扩张性及其与渐近复杂性类的关系，并给出动力系统与时间层次定理的联系。


<details>
  <summary>Details</summary>
Motivation: 利用准度量空间的拓扑框架，将算法复杂性的不对称比较数学化，并探索缩放变换的动力系统性质，以揭示复杂性类的深层结构。

Method: 采用复杂性准度量空间框架，研究ψ_α的扩张性；通过δ-稳定集、双曲坐标、轨道分离等方法，结合Python与SageMath实现可重复计算验证。

Result: 证明ψ_α在复杂性空间上扩张当且仅当α≠1；δ-稳定集恰好对应渐近复杂性类；建立双曲收缩率λ=1/α，并与Hartmanis‑Stearns时间层次定理建立精确联系；给出拓扑熵估计。

Conclusion: 该理论为复杂性类提供了动力系统刻画，揭示了缩放动力学与经典计算复杂性层次之间的深刻联系，并为进一步的算法分析提供了可计算的工具。

Abstract: The complexity quasi-metric, introduced by Schellekens, provides a topological framework where the asymmetric nature of computational comparisons -- stating that one algorithm is faster than another carries different information than stating the second is slower than the first -- finds precise mathematical expression. In this paper we develop a comprehensive theory of expansive homeomorphisms on complexity quasi-metric spaces. Our central result establishes that the scaling transformation $ψ_α(f)(n)=αf(n)$ is expansive on the complexity space $(\C,d_\C)$ if and only if $α\neq 1$. The $δ$-stable sets arising from this dynamics correspond exactly to asymptotic complexity classes, providing a dynamical characterisation of fundamental objects in complexity theory. We prove that the canonical coordinates associated with $ψ_α$ are hyperbolic with contraction rate $λ=1/α$ and establish a precise connection between orbit separation in the dynamical system and the classical time hierarchy theorem of Hartmanis and Stearns. We further investigate unstable sets, conjugate dynamics, and topological entropy estimates for the scaling map. Throughout, concrete algorithms and Python implementations accompany the proofs, making every result computationally reproducible. SageMath verification snippets are inlined alongside the examples, and the full code is available in the companion repository.

</details>


### [324] [On the complexity of Multipacking](https://arxiv.org/abs/2602.07982)
*Sandip Das,Sk Samim Islam,Daniel Lokshtanov*

Main category: cs.CC

TL;DR: 该论文解决了十余年未决的核心问题，证明无向图上的Multipacking问题是NP-完全问题，并揭示其在多个图类中保持计算困难性，同时提出了突破2^n屏障的精确指数时间算法。


<details>
  <summary>Details</summary>
Motivation: 十余年来，无向图Multipacking问题的计算复杂性（NP-完全性或多项式时间可解性）是开放问题。虽有研究在强弦图、网格等特定图类中证明其多项式时间可解，并在有向图中证明NP-完全性，但无向图情形始终未决。本研究旨在彻底解决这一基础复杂性分类问题。

Method: 通过归约技术证明NP-完全性：将经典NP-完全问题（如支配集）巧妙规约到Multipacking问题。针对参数化复杂性，使用参数化归约证明W[2]-困难性。对子类（弦图、二分图等）设计针对性归约，并开发O*(1.58^n)的指数时间算法（基于分支定界或测度与 conquer技术）。

Result: 1. 核心突破：无向图Multipacking问题为NP-完全，终结开放问题。<br>2. 参数化结果：对解大小参数k是W[2]-困难。<br>3. 子类困难性：在弦图、二分图、无爪图、正则图、CONV图及弦图∩½-双曲图（强弦图的超类）中仍为NP-完全且W[2]-困难。<br>4. 算法突破：提出O*(1.58^n)的精确算法，优于O*(2^n)。

Conclusion: Multipacking问题在无向图中本质上是计算困难的（NP-完全且W[2]-困难），这一困难性在广泛的图子类中持续存在，仅特定结构（如强弦图）例外。算法结果为实践提供了优于暴力搜索的精确求解方案。该工作完善了图 packing 问题的复杂性分类体系。

Abstract: A multipacking in an undirected graph $G=(V,E)$ is a set $M\subseteq V$ such that for every vertex $v\in V$ and for every integer $r\geq 1$, the ball of radius $r$ around $v$ contains at most $r$ vertices of $M$, that is, there are at most $r$ vertices in $M$ at a distance at most $r$ from $v$ in $G$. The Multipacking problem asks whether a graph contains a multipacking of size at least $k$. For more than a decade, it remained an open question whether the Multipacking problem is NP-complete or solvable in polynomial time. Whereas the problem is known to be polynomial-time solvable for certain graph classes (e.g., strongly chordal graphs, grids, etc). Foucaud, Gras, Perez, and Sikora [Algorithmica 2021] made a step towards solving the open question by showing that the Multipacking problem is NP-complete for directed graphs and it is W[1]-hard when parameterized by the solution size. In this paper, we prove that the Multipacking problem is NP-complete for undirected graphs, which answers the open question. Moreover, the problem is W[2]-hard for undirected graphs when parameterized by the solution size. Furthermore, we have shown that the problem is NP-complete and W[2]-hard (when parameterized by the solution size) even for various subclasses: chordal, bipartite, and claw-free graphs. Whereas, it is NP-complete for regular, and CONV graphs (intersection graphs of convex sets in the plane). Additionally, the problem is NP-complete and W[2]-hard (when parameterized by the solution size) for chordal $\cap$ $\frac{1}{2}$-hyperbolic graphs, which is a superclass of strongly chordal graphs where the problem is polynomial-time solvable. On the positive side, we present an exact exponential-time algorithm for the Multipacking problem on $n$-vertex general graphs, which breaks the $2^n$ barrier by achieving a running time of $O^*(1.58^n)$.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [325] [Alleviating Post-Linearization Challenges for Solving Nonlinear Systems on a Quantum Computer](https://arxiv.org/abs/2602.07097)
*Tayyab Ali*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The linearity inherent in quantum mechanics limits current quantum hardware from directly solving nonlinear systems governed by nonlinear differential equations. One can opt for linearization frameworks such as Carleman linearization, which provides a high dimensional infinite linear system corresponding to a finite nonlinear system, as an indirect way of solving nonlinear systems using current quantum computers. We provide an efficient data access model to load this infinite linear representation of the nonlinear system, upto truncation order $N$, on a quantum computer by decomposing the Hamiltonian into the weighted sum of non-unitary operators, namely the Sigma basis. We have shown that the Sigma basis provides an exponential reduction in the number of decomposition terms compared to the traditional decomposition, which is usually done in a linear combination of Pauli operators. Once the Hamiltonian is decomposed, we then use the concept of unitary completion to construct the circuit for the implementation of each weighted tensor product component $\mathcal{H}_{j}$ of the decomposition.

</details>


### [326] [Beyond Wigner: Non-Invertible Symmetries Preserve Probabilities](https://arxiv.org/abs/2602.07110)
*Thomas Bartsch,Yuhan Gai,Sakura Schafer-Nameki*

Main category: quant-ph

TL;DR: 该论文解决了量子理论中非可逆对称性与Wigner定理的矛盾，提出对称缺陷应作为不同扭曲扇区Hilbert空间之间的等距映射而非固定空间上的酉算子，从而将非可逆对称性自然地描述为保迹量子通道


<details>
  <summary>Details</summary>
Motivation: 传统量子对称性需满足Wigner定理（由反酉/酉算符实现以保持概率），但广义范畴对称性可能非可逆，二者存在根本冲突

Method: 针对(高阶)融合范畴对称性C，提出对称缺陷作用于由扭曲扇区构造的互异Hilbert空间之间的等距映射，而非单一固定Hilbert空间上的酉算符

Result: 1) 非可逆对称性自然实现为保迹量子通道 2) 该构造严格依赖于对称范畴C的酉性 3) 通过Tambara-Yamagami/Fibonacci/Yang-Lee等范畴对称性实例验证

Conclusion: 通过重构对称性作用的空间框架（跨扭曲扇区的等距映射），在保持Wigner定理核心要求的同时容纳非可逆对称性，且酉性是此框架成立的必要条件

Abstract: In recent years, the traditional notion of symmetry in quantum theory was expanded to so-called generalised or categorical symmetries, which, unlike ordinary group symmetries, may be non-invertible. This appears to be at odds with Wigner's theorem, which requires quantum symmetries to be implemented by (anti)unitary -- and hence invertible -- operators in order to preserve probabilities. We resolve this puzzle for (higher) fusion category symmetries $\mathcal{C}$ by proposing that, instead of acting by unitary operators on a fixed Hilbert space, symmetry defects in $\mathcal{C}$ act as isometries between distinct Hilbert spaces constructed from twisted sectors. As a result, we find that non-invertible symmetries naturally act as trace-preserving quantum channels. Crucially, our construction relies on the symmetry category $\mathcal{C}$ being unitary. We illustrate our proposal through several examples that include Tambara-Yamagami, Fibonacci, and Yang-Lee as well as higher categorical symmetries.

</details>


### [327] [Entanglement harvesting in conformal field theory](https://arxiv.org/abs/2602.07112)
*Kelly Wurtz,Caroline Lima,Robert C. Myers,Eduardo Martín-Martínez*

Main category: quant-ph

TL;DR: 本文将纠缠采集推广到一般d维共形场论，使用Unruh-DeWitt探测器耦合标量初级算子，发现算子标度维度增大抑制纠缠，且在全息CFT中体有效场论可分离场采集与通信介导的纠缠。


<details>
  <summary>Details</summary>
Motivation: 将标准纠缠采集协议从自由场推广到相互作用的共形场论和任意空间维度，研究更一般的量子场论中的纠缠提取问题。

Method: 采用点状Unruh-DeWitt探测器与标量初级算子耦合，针对全息CFT使用体有效场论方法，并推导渐近闭式近似与数值结果对比。

Result: 1. 算子标度维度增加会抑制负度和互信息，反映关联更快衰减；2. 全息CFT中体有效场论可实现场采集与通信介导纠缠的分离；3. 渐近近似与数值结果吻合良好。

Conclusion: 该工作成功将纠缠采集拓展至一般d维CFT，揭示了算子标度维度对纠缠提取的影响，并为全息体系提供了区分不同纠缠机制的理论框架，所提近似方法有效可靠。

Abstract: We study entanglement harvesting in general $d$-dimensional conformal field theories using pointlike Unruh-DeWitt detectors coupled to scalar primary operators. This extends standard harvesting protocols beyond free fields to interacting conformal theories and arbitrary spatial dimensions. We find that increasing the operator scaling dimension suppresses both negativity and mutual information, reflecting the faster decay of correlations. For holographic CFTs, we show that bulk effective field theory enables a separation between field-harvested and communication-mediated entanglement. We also derive asymptotic, closed-form approximations that agree well with numerical results.

</details>


### [328] [Performance limits of a quantum receiver for detecting phase-modulated communication signals](https://arxiv.org/abs/2602.07123)
*William M. Watkins,Leigh Norris,Paraj Titum*

Main category: quant-ph

TL;DR: 该论文分析了基于量子传感器的接收链在解调相位调制电磁波信号方面的性能，比较了纠缠与非纠缠量子传感器阵列在BPSK调制下的表现，并探讨了NV-金刚石平台在实际应用中的性能极限。


<details>
  <summary>Details</summary>
Motivation: 量子传感器因其卓越的灵敏度和紧凑形态，成为探测微弱电磁信号的理想候选。本研究旨在分析基于量子传感器的接收链在解调相位调制信息方面的性能。

Method: 研究采用广义累积量展开方法对含噪量子接收机进行建模，利用误比特率（BEP）和信道容量作为性能指标，比较了非纠缠和纠缠量子传感器阵列在二进制相移键控（BPSK）调制下的能力。

Result: 研究确定了量子传感器阵列的信道容量可能超越经典电小天线极限的条件，并讨论了即使在传感器噪声和信道失真情况下也能实现高保真数据恢复的量子协议改进方案。

Conclusion: 最后，研究探索了此类量子接收链的实际性能极限，重点关注NV-金刚石作为量子传感器平台的应用前景。

Abstract: Quantum sensors are an ideal candidate for detecting weak electromagnetic signals because of their exceptional sensitivity and compact form factor. In this work, we analyze the performance of a quantum-sensor-based receive chain for demodulating information encoded in phase-modulated electromagnetic waves. We introduce a generalized cumulant expansion to model a noisy quantum receiver and use it to compare the performance of various quantum demodulation protocols. Employing bit error probability (BEP) and channel capacity as quantitative performance metrics, we compare the capabilities of ensembles of quantum sensors - both unentangled and entangled - using Binary Phase-Shift Keying (BPSK) as a representative example of phase modulation. We identify conditions when the channel capacity of an ensemble of quantum sensors may surpass the limits of a classical electrically small antenna. Additionally, we discuss modifications to the quantum protocol that enables high-fidelity data recovery even in the presence of sensor noise and channel distortions. Finally, we explore practical performance limits of such a quantum receive chain, with a focus on NV-diamond as the quantum sensor platform.

</details>


### [329] [Putting fermions onto a digital quantum computer](https://arxiv.org/abs/2602.07151)
*Riley W. Chien,Mitchell L. Chiew,Brent Harrison,Jason Necaise,Weishi Wang,Maryam Mudassar,Campbell McLauchlan,Thomas M. Henderson,Gustavo E. Scuseria,Sergii Strelchuk,James D. Whitfield*

Main category: quant-ph

TL;DR: 综述费米子到量子比特的编码方法，挑战高维费米子系统更难处理的传统观念


<details>
  <summary>Details</summary>
Motivation: 量子计算机有望成为研究物理量子系统的强大工具，但基于量子比特的量子计算机天然适合研究自旋-1/2系统，而费米子等具有其他自由度的系统需要先编码成量子比特，费米子系统的量子模拟是重要应用

Method: 综述现有文献中费米子自由度到量子比特的编码方法

Result: 澄清并试图消除"一维以外费米子系统从根本上更难处理"这一持续存在的误解

Conclusion: 提供费米子编码方法的全面概览，论证高维费米子系统并非本质更困难，为量子模拟费米子系统提供理论参考

Abstract: Quantum computers are expected to become a powerful tool for studying physical quantum systems. Consequently, a number of quantum algorithms for studying the physical properties of such systems have been developed. While qubit-based quantum computers are naturally suited to the study of spin-1/2 systems, systems containing other degrees of freedom must first be encoded into qubits. Transformations to and from fermionic degrees of freedom have long been an important tool in physics and, now the simulation of fermionic systems on quantum computers based on qubits provides yet another application. In this perspective, we review methods for encoding fermionic degrees of freedom into qubits and attempt to dispel the persistent notion that fermionic systems beyond one dimension are fundamentally more difficult to deal with.

</details>


### [330] [Measurement-Based Preparation of Higher-Dimensional AKLT States and Their Quantum Computational Power](https://arxiv.org/abs/2602.07201)
*Wenhan Guo,Mikhail Litvinov,Tzu-Chieh Wei,Abid Khan,Kevin C. Smith*

Main category: quant-ph

TL;DR: 提出了一种基于融合测量的常数时间方案来制备高维AKLT态，研究随机修饰和随机键AKLT态的制备及其计算能力，证明这些态至少具备与标准AKLT态相当的计算能力


<details>
  <summary>Details</summary>
Motivation: 探索如何在测量量子计算框架下高效制备AKLT态（超越一维情形），并理解随机修饰和随机键变化对量子计算能力的影响

Method: 采用融合测量方案，在给定图上通过随机自旋-1修饰（沿边概率性插入顶点）制备AKLT态；研究Bethe格点的确定性常数时间方案；构造基于任意标准Bell态而非单态的随机键AKLT态

Result: 随机修饰的AKLT态至少保持与trivalent平面晶格上非随机态相同的计算能力；Bethe格点存在确定性的常数时间制备方案；随机键AKLT态可通过POVM转换为编码随机图态，并通过渗流理论论证其与原始单态键AKLT态具有相似的量子计算能力

Conclusion: 随机修饰和随机键的AKLT态在保持计算能力方面具有鲁棒性，为测量量子计算提供了更灵活的资源态制备方法，特别是在常数时间制备和高维扩展方面具有重要意义

Abstract: We investigate a constant-time, fusion measurement-based scheme to create AKLT states beyond one dimension. We show that it is possible to prepare such states on a given graph up to random spin-1 `decorations', each corresponding to a probabilistic insertion of a vertex along an edge. In investigating their utility in measurement-based quantum computation, we demonstrate that any such randomly decorated AKLT state possesses at least the same computational power as non-random ones, such as those on trivalent planar lattices. For AKLT states on Bethe lattices and their decorated versions we show that there exists a deterministic, constant-time scheme for their preparation. In addition to randomly decorated AKLT states, we also consider random-bond AKLT states, whose construction involves any of the canonical Bell states in the bond degrees of freedom instead of just the singlet in the original construction. Such states naturally emerge upon measuring all the decorative spin-1 sites in the randomly decorated AKLT states. We show that those random-bond AKLT states on trivalent lattices can be converted to encoded random graph states after acting with the same POVM on all sites. We also argue that these random-bond AKLT states possess similar quantum computational power as the original singlet-bond AKLT states via the percolation perspective.

</details>


### [331] [The continuous spectrum of bound states in expulsive potentials](https://arxiv.org/abs/2602.07281)
*H. Sakaguchi,B. A. Malomed,A. C. Aristotelous,E. G. Charalampidis*

Main category: quant-ph

TL;DR: 该论文挑战了陡峭排斥势会导致量子态高度离域的传统直觉，证明在1D和2D薛定谔方程中，比二次型（反谐振子）更陡的排斥势反而能产生可归一化的局域本征态，这些态构成连续谱，并探讨了非线性扩展下的稳定性。


<details>
  <summary>Details</summary>
Motivation: 挑战常见直觉：陡峭排斥势通常被认为会使量子态广泛离域，但本文研究比二次型更陡的排斥势（反谐振子）在1D和2D薛定谔方程中的行为，探索其是否能产生稳定的局域态。

Method: 解析求解1D/2D薛定谔方程，推导远场渐近近似，并与数值解对比；研究非线性Gross-Pitaevskii方程作为扩展，分析立方和五次非线性项对态稳定性的影响。

Result: 发现排斥势下存在可归一化本征态；1D为空间奇偶态，2D可携带任意涡度；渐近近似与数值解高度吻合；获得2D涡旋态精确解；非线性情形下，1D立方项轻微畸变态但保持稳定，五次自聚焦项超过临界值时引发坍缩。

Conclusion: 研究成果拓展了连续谱中束缚态的概念，适用于量子力学和傍轴光子学系统，揭示了排斥势体系中局域态的意外稳定性及其非线性动力学特性。

Abstract: On the contrary to the common intuition that a steep expulsive potential makes quantum states widely delocalized, we demonstrate that one- and two-dimensional (1D and 2D) Schrödinger equations, which include expulsive potentials that are \emph{steeper than the quadratic} (anti-harmonic-oscillator) ones, give rise to \emph{normalizable} (effectively localized) eigenstates. These states constitute full continuous spectra in the 1D and 2D cases alike. In 1D, these are spatially even and odd eigenstates. The 2D states may carry any value of the vorticity (alias magnetic quantum number). Asymptotic approximations for wave functions of the 1D and 2D eigenstates, valid far from the center, are derived analytically, demonstrating excellent agreement with numerically found counterparts. Special exact solutions for vortex states are obtained in the 2D case. These findings suggest an extension of the concept of bound states in the continuum, in quantum mechanics and paraxial photonics. Gross-Pitaevskii equations are considered as the nonlinear extension of the 1D and 2D settings. In 1D, the cubic nonlinearity slightly deforms the eigenstates, maintaining their stability. On the other hand, the quintic self-focusing term, which occurs in the photonic version of the 1D model, initiates the dynamical collapse of states whose norm exceeds a critical value.

</details>


### [332] [Encoding Matters: Benchmarking Binary and D-ary Representations for Quantum Combinatorial Optimization](https://arxiv.org/abs/2602.07357)
*Shashank Sanjay Bhat,Peiyong Wang,Joseph West,Udaya Parampalli*

Main category: quant-ph

TL;DR: QUDO formulation outperforms QUBO by naturally handling constraints without penalties, achieving better approximation ratios with less computational overhead on quantum devices


<details>
  <summary>Details</summary>
Motivation: QUBO formulations use penalty terms with auxiliary variables that increase Hamiltonian complexity and limit scalability on near-term quantum devices

Method: Systematically study QUDO as alternative formulation with qudit-level QAOA, benchmark against QUBO and classical exact solutions across TSP, VRP, graph coloring, scheduling, and Max-K-Cut

Result: QUDO shows consistently improved approximation ratios and substantially reduced computational overhead at comparable circuit depths

Conclusion: QUDO is a scalable and expressive representation for quantum combinatorial optimization

Abstract: Combinatorial optimization problems are typically formulated using Quadratic Unconstrained Binary Optimization (QUBO), where constraints are enforced through penalty terms that introduce auxiliary variables and rapidly increase Hamiltonian complexity, limiting scalability on near term quantum devices. In this work, we systematically study Quadratic Unconstrained D-ary Optimization (QUDO) as an alternative formulation in which decision variables are encoded directly in higher dimensional Hilbert spaces. We demonstrate that QUDO naturally captures structural constraints across a range of problem classes, including the Traveling Salesman Problem, two variants of the Vehicle Routing Problem, graph coloring, job scheduling, and Max-K-Cut, without the need for extensive penalty constructions. Using a qudit-level implementation of the Quantum Approximate Optimization Algorithm (qudit QAOA), we benchmark these formulations against their binary QUBO counterparts and exact classical solutions. Our study show consistently improved approximation ratios and substantially reduced computational overhead at comparable circuit depths, highlighting QUDO as a scalable and expressive representation for quantum combinatorial optimization.

</details>


### [333] [An efficient method for spot-checking quantum properties with sequential trials](https://arxiv.org/abs/2602.08114)
*Yanbao Zhang,Akshay Seshadri,Emanuel Knill*

Main category: quant-ph

TL;DR: The paper develops a general certification method for non-i.i.d. quantum resources using spot-checking, showing only a constant number of spot-checks on average are needed even with infinite trials.


<details>
  <summary>Details</summary>
Motivation: Quantum resources in practical scenarios may be unreliable and exhibit non-independent, non-identically distributed behavior due to complex generation or adversarial manipulation, causing security concerns and faulty estimates in quantum information tasks like QKD, self-testing, verifiable quantum computation, and quantum network resource allocation.

Method: The authors develop a certification method that uses random spot-checking decisions in each trial to certify performance of non-i.i.d. quantum resources.

Result: The method works efficiently with finite trials, yields asymptotically tight certificates of performance, and requires only a constant number of spot-checks on average as total trials approach infinity to certify average performance at a specified confidence level.

Conclusion: A general method for certifying non-i.i.d. quantum resources via spot-checking is developed, offering efficient and tight performance certification with minimal spot-checking overhead.

Abstract: In practical situations, the reliability of quantum resources can be compromised due to complex generation processes or adversarial manipulations during transmission. Consequently, the trials generated sequentially in an experiment may exhibit non-independent and non-identically distributed (non-i.i.d.) behavior. This non-i.i.d. behavior can introduce security concerns and result in faulty estimates when performing information tasks such as quantum key distribution, self-testing, verifiable quantum computation, and resource allocation in quantum networks. To certify the performance of such tasks, one can make a random decision in each trial, either spot-checking some desired property or utilizing the quantum resource for the given task. However, a general method for certification with a sequence of non-i.i.d. spot-checking trials is still missing. Here, we develop such a method. This method not only works efficiently with a finite number of trials but also yields asymptotically tight certificates of performance. Our analysis shows that even as the total number of trials approaches infinity, only a constant number of trials needs to be spot-checked on average to certify the average performance of the remaining trials at a specified confidence level.

</details>


### [334] [The ABL Rule and the Perils of Post-Selection](https://arxiv.org/abs/2602.07402)
*Jacob A. Barandes*

Main category: quant-ph

TL;DR: 这篇论文批判性地分析了ABL规则，认为其存在根本性的范畴错误，即将单系统可观察量与系综可观察量混淆，并指出相关文献中其他问题。


<details>
  <summary>Details</summary>
Motivation: 针对ABL规则被赋予的额外意义（如支持违反不确定性原理）以及相关文献中的概念混淆和逻辑错误，需要进行批判性分析和澄清。

Method: 对ABL规则及其相关研究文献进行深入分析，识别其中的逻辑错误、概念混淆和推理谬误。

Result: 发现ABL规则存在核心范畴错误，混淆了单系统可观察量与涌现系综可观察量，同时指出了误用后选择、依赖经典公式模式匹配和测量主义等问题。

Conclusion: ABL规则存在根本性缺陷，其支持违反不确定性原理的主张不成立，相关解释性问题需要更严谨的理论框架。

Abstract: In 1964, Aharonov, Bergmann, and Lebowitz introduced their well-known ABL rule with the intention of providing a time-symmetric formalism for computing novel kinds of conditional probabilities in quantum theory. Later papers attached additional significance to the ABL rule, including assertions that it supported violations of the uncertainty principle. The present work challenges these claims, as well as subsequent attempts to salvage the original interpretation of the ABL rule. Taking a broader view, this paper identifies a subtle category error at the heart of the ABL rule that consists of confusing observables that belong to a single system with emergent observables that arise only for physical ensembles. Along the way, this paper points out other problems and fallacious reasoning in the research literature surrounding the ABL rule, including the misuse of post-selection, a reliance on pattern matching to classical formulas, and a posture of measurementism that takes experimental data as providing answers to interpretational questions.

</details>


### [335] [Quantum Many-Body Principles of Localized-State Ensemble Luminescence](https://arxiv.org/abs/2602.07406)
*Xinye Fan,Shijie Xu*

Main category: quant-ph

TL;DR: 该论文发展了量子多体发光理论(MB-LSE)，首次从微观角度统一解释了局域态发光中异常热行为（峰位红移/蓝移、线宽窄化/展宽等），阐明了电子-声子/电子-电子相互作用机制，并推导了Varshni公式和黄-里斯因子。


<details>
  <summary>Details</summary>
Motivation: 固体中由缺陷/杂质引起的局域电子态发光现象具有重要科技价值，但缺乏统一的微观理论解释其异常热行为（如变温下的峰位移动、线宽变化等），现有理论存在空白。

Method: 建立包含电子-声子(e-p)和电子-电子(e-e)相互作用的量子多体发光理论框架(MB-LSE)，通过第一性原理计算与唯象模型结合，定量分析局域态发光动力学。

Result: 1. 首次定量解释局域态发光的五类异常热行为（峰位红移后蓝移、线宽先窄后宽、强度下降、寿命变化）
2. 揭示e-p相互作用主导热响应，e-e相互作用调制发光效率
3. 从理论框架推导出Varshni带隙温度公式及黄-里斯因子
4. 建立微观参数与宏观光谱特征的定量映射关系

Conclusion: MB-LSE理论成功构建了局域态发光的统一微观模型，解决了该领域长期存在的理论缺失问题，为设计高效发光器件（如LED、量子传感器）提供了新理论基础，并推广至其他凝聚态发光体系。

Abstract: Localized electron states induced by various disorders,including defects and impurities,usually exist in solids.Their optical properties,especially their luminescence properties,are of both scientific and technological significance.But a microscopic theory has not yet been established for such localized-state ensemble (LSE) luminescence.In this Letter,we attempt to fill this void via developing a quantum many-body (MB) luminescence theory taking into account both electron-phonon (e-p) and electron-electron (e-e) interactions.By using the developed MB-LSE theory,abnormal thermal behaviors such as redshift and subsequent blueshift of peak position,narrowing and succeeding broadening of linewidth,decline in intensity,and variation in lifetime can be quantitatively interpreted.The roles of electron-phonon and electron-electron interactions in the variable-temperature LSE luminescence are thus elucidated. Within the framework of the MB-LSE theory, moreover, Varshni's empirical formula for bandgap temperature dependence and Huang-Rhys factor for e-p coupling are further derived and discussed.

</details>


### [336] [Non-Markovianity in a dressed qubit with local dephasing](https://arxiv.org/abs/2602.07438)
*Saima Bashir,Muzaffar Qadir Lone,Prince A Ganai*

Main category: quant-ph

TL;DR: Dressed qubit coherence persists longer with strong coupling; sub-Ohmic baths induce non-Markovianity at weak coupling while Ohmic/super-Ohmic combinations require strong coupling, revealing bath engineering strategies for quantum coherence preservation.


<details>
  <summary>Details</summary>
Motivation: Investigate decoherence dynamics and non-Markovian effects in a dressed qubit system composed of spinless fermions coupled to phonon baths, to understand how bath spectral densities and coupling strengths influence quantum coherence preservation.

Method: Employed Lang-Firsov transformation to handle strong coupling, combined with time-convolutionless master equation techniques within the polaron frame. Analysis focused on singlet-triplet basis coherence dynamics across various bath spectral densities (sub-Ohmic, Ohmic, super-Ohmic).

Result: Coherence persists longer at large coupling values with non-monotonic decay patterns indicating non-Markovianity. Sub-Ohmic baths (alone or combined) show strong memory effects even at weak couplings, while Ohmic/super-Ohmic combinations only exhibit non-Markovianity at higher couplings. Coherence revivals characterize non-Markovian dynamics.

Conclusion: Bath spectral composition critically controls non-Markovianity: sub-Ohmic environments enhance memory effects at low couplings, whereas Ohmic/super-Ohmic mixtures require strong coupling for observable non-Markovian behavior. This provides design principles for robust quantum coherence in solid-state qubits.

Abstract: We study the dynamics of a dressed qubit implemented by a spinless fermion hopping between two lattice sites with each site strongly coupled to a bath of phonons. We employ Lang-Firsov transformation to make the problem tractable perturbatively. Applying time-convolutionless master equation within the polaron frame, we investigate decoherence dynamics of the dressed qubit within the singlet-triplet basis of the system for a wide range of bath spectral densities. It is shown that the coherence persists for longer time scales for large coupling values and shows non-monotonic behaviour reflecting the presence of non-Markovianity in the dynamics. Non-Markovianity, characterized by coherence revivals and non-monotonic decay patterns, emerges distinctly depending on the bath spectrum and coupling strengths. Systems coupled to sub-Ohmic baths, whether both or in combination with another type, display pronounced memory effects at relatively small values of couplings. In contrast, combinations involving Ohmic and super-Ohmic baths exhibit noticeable non-Markovianity only at higher couplings.

</details>


### [337] [Plethysm is in #BQP](https://arxiv.org/abs/2602.08441)
*Matthias Christandl,Aram W. Harrow,Greta Panova,Pietro M. Posta,Michael Walter*

Main category: quant-ph

TL;DR: 该论文证明了一类广泛的表示论重数（包括plethysm系数）属于量子计数复杂性类#BQP，并通过多次应用Schur变换以及利用其局部维度依赖性的最新改进，统一、简化和推广了先前关于Kronecker等系数量子复杂度的结果。同时证明这些重数也属于GapP类，并在特定参数固定时存在多项式时间经典算法。


<details>
  <summary>Details</summary>
Motivation: 表示论重数（如Kostka和Littlewood-Richardson系数）的组合解释使其计算复杂性位于#P类，这一性质是否普遍成立是数学和计算机科学中的重要开放问题，与几何复杂性理论和量子信息相关。先前研究仅针对特定重数（如Kronecker系数和plethysm系数的特殊情况）探索了量子复杂性，缺乏普适性结果。

Method: 通过多次应用Schur变换，并利用该变换在局部维度依赖性上的最新改进，建立了一个通用框架来证明表示论重数属于#BQP类。该方法涵盖了论文提出的方法及先前研究的各种途径。

Result: 证明了一类广泛的表示论重数属于#BQP，特别地，plethysm系数被证明属于#BQP（此前仅在特殊情况下已知）。该结果将先前关于Kronecker等系数的量子复杂性研究作为特例统一、简化和推广。此外，同一重数也属于GapP类，且在某些参数固定时存在多项式时间经典算法。

Conclusion: 该工作推进了对表示论重数计算复杂性的理解，在#BQP、GapP和#P等复杂性类之间建立了新的联系，为几何复杂性理论和量子信息提供了理论工具，并给出了在参数固定时的有效经典算法。

Abstract: Some representation-theoretic multiplicities, such as the Kostka and the Littlewood-Richardson coefficients, admit a combinatorial interpretation that places their computation in the complexity class #P. Whether this holds more generally is considered an important open problem in mathematics and computer science, with relevance for geometric complexity theory and quantum information. Recent work has investigated the quantum complexity of particular multiplicities, such as the Kronecker coefficients and certain special cases of the plethysm coefficients.
  Here, we show that a broad class of representation-theoretic multiplicities is in #BQP. In particular, our result implies that the plethysm coefficients are in #BQP, which was only known in special cases. It also implies all known results on the quantum complexity of previously studied coefficients as special cases, unifying, simplifying, and extending prior work. We obtain our result by multiple applications of the Schur transform. Recent work has improved its dependence on the local dimension, which is crucial for our work. We further describe a general approach for showing that representation-theoretic multiplicities are in #BQP that captures our approach as well as the approaches of prior work. We complement the above by showing that the same multiplicities are also naturally in GapP and obtain polynomial-time classical algorithms when certain parameters are fixed.

</details>


### [338] [Sensing weak anharmonicities with a passive-active anti-PT symmetric system](https://arxiv.org/abs/2602.07460)
*Ya-Wei Zeng,Wei-Xin Chen,Tian-Le Yang,Wan-Jun Su,Huaizhi Wu*

Main category: quant-ph

TL;DR: 提出基于三模反宇称-时间对称腔-磁子波导系统的高灵敏度微弱非谐性检测方案，通过调控光学增益实现线宽抑制点的灵活控制，显著提升腔模与磁子模的非线性检测灵敏度


<details>
  <summary>Details</summary>
Motivation: 现有微弱非谐性检测技术受限于耗散系统的本征衰减，难以实现高灵敏度检测；反宇称-时间对称系统为突破该限制提供新思路，但传统方案对强衰减磁子模式适用性有限

Method: 构建被动-主动集成的三模反宇称-时间对称系统：1) 通过光学增益调控活性腔模的线宽抑制点；2) 利用失谐激光驱动增强灵敏度；3) 同步检测腔模与磁子模的非线性响应

Result: 1) 实现强本征衰减磁子模式下的线宽抑制点灵活控制；2) 腔模与磁子模均表现出相近的高检测灵敏度；3) 失谐激光驱动使灵敏度大幅提升；4) 方案可扩展至多种含非谐性的物理系统

Conclusion: 该方案通过反宇称-时间对称系统的非厄米特性突破传统检测极限，为量子传感、基础物理研究提供新范式，具有跨体系推广潜力

Abstract: We propose a scheme for enhanced sensing of weak anharmonicities based on a three-mode anti-parity-time (anti-PT) symmetric cavity-magnon-waveguide system. By tuning the optical gain to the active cavity mode, the linewidth suppression point for the anti-PT symmetric Hamiltonian can be flexibly controlled even when the two dissipative magnonic modes experience strong intrinsic decay. This essential characteristic is utilized for detecting weak nonlinearities in both the cavity and magnonic modes, with both demonstrating similar high levels of sensitivity. Moreover, the sensitivity can be greatly improved with a detuned laser drive. Based on the integrated passive-active three-mode anti-PT symmetric system, the sensing scheme can be generalized to various physical systems with anharmonicities.

</details>


### [339] [Non-Hermitian Renormalization Group from a Few-Body Perspective](https://arxiv.org/abs/2602.08705)
*Hiroyuki Tajima,Masaya Nakagawa,Haozhao Liang,Masahito Ueda*

Main category: quant-ph

TL;DR: 本文通过散射振幅不变性，建立了非厄米重整化群方法的微观基础，揭示了量子测量是核系统中非厄米效应的起源，并将标度反常与重整化群流联系起来。


<details>
  <summary>Details</summary>
Motivation: 尽管非厄米性在开放量子系统中起基础作用，但理解其在强相互作用系统中的后果仍很困难。虽然已应用威尔逊重整化群方法，但其基于配分函数的基础在非厄米系统中定义不明确。

Method: 作者从少体视角出发，利用重整化群变换下散射振幅的不变性，严格推导非厄米重整化群方程，并以具有非弹性两体损失的非相对论性两体系统为例分析其结构。

Result: 他们推导出具有明确物理解释的非厄米重整化群方程，将重整化群流与非厄米量子标度反常联系起来，将复势解释为量子测量效应，在核物理中发现临界半圆结构并发现若干原子核位于其附近，还将晕核中的定域双中子解释为对核心核吸收的虚势的量子测量效应。

Conclusion: 该工作连接了高能物理与原子分子光物理中的非厄米系统，开辟了非厄米少体物理的跨学科研究新方向。

Abstract: Non-Hermiticity plays a fundamental role in open quantum systems and describes a wide variety of effects of interactions with environments, including quantum measurement. However, understanding its consequences in strongly interacting systems is still elusive due to the interplay between non-perturbative strong correlations and non-Hermiticity. While the Wilsonian renormalization group (RG) method has been applied to tackle this problem, its foundation, based on the existence of the partition function, is ill-defined. In this paper, we establish a microscopic foundation of the non-Hermitian RG method from a few-body perspective. We show that the invariance of the scattering amplitude under RG transformations enables us to rigorously derive the non-Hermitian RG equation, giving a physically transparent interpretation of RG flows. We discuss a detailed structure of such RG flows in a non-relativistic two-body system with inelastic two-body loss, and show its relation to a non-Hermitian quantum scale anomaly. Our analysis suggests that non-Hermitian complex potentials often used in high-energy physics can be interpreted as being caused by quantum measurement, where the detection of elastically scattered particles updates the observer's knowledge, resulting in a nonunitary state change of the system. We apply our formalism to nuclear physics, find the emergence of a critical semicircle, and show that several nuclei are located near the critical semicircle in the coherent neutron-nucleus scattering. We also propose that the localized dineutron in two-neutron halo nuclei can be interpreted as the quantum measurement effect on the imaginary potential associated with absorption into the core nucleus. Our result bridges different contexts of non-Hermitian systems in high-energy and atomic, molecular, and optical physics, opening an interdisciplinary playground of non-Hermitian few-body physics.

</details>


### [340] [Systematic Characterization of Transmon Qubit Stability with Thermal Cycling](https://arxiv.org/abs/2602.07522)
*Cong Li,Zhaohua Yang,Xinfang Zhang,Zhihao Wu,Shichuan Xue,Mingtang Deng*

Main category: quant-ph

TL;DR: Long-term study of 27 superconducting qubits reveals intrinsic parameters remain stable across thermal cycles while environmental noise defects are randomly reconfigured, necessitating automated recalibration for quantum processors.


<details>
  <summary>Details</summary>
Motivation: The temporal stability and reproducibility of qubit parameters are critical for the long-term operation and maintenance of superconducting quantum processors.

Method: Comprehensive longitudinal characterization of 27 frequency-tunable transmon qubits spanning over one year across four thermal cycles, using frequency-dependent relaxation spectroscopy and T1 Spectral Topography Fidelity metric.

Result: Intrinsic device parameters (frequency, baseline T1) show high robustness (<0.5% deviation, non-degraded coherence), while environmental variables (magnetic flux offsets, TLS defects) undergo significant stochastic reconfiguration after each thermal cycle, equivalent to thousands of hours of low-temperature evolution.

Conclusion: Fabrication quality is preserved but specific noise realization is statistically distinct per thermal cycle, necessitating automated recalibration strategies for large-scale quantum systems.

Abstract: The temporal stability and reproducibility of qubit parameters are critical for the long-term operation and maintenance of superconducting quantum processors. In this work, we present a comprehensive longitudinal characterization of 27 frequency-tunable transmon qubits spanning over one year across four thermal cycles. Our results establish a distinct hierarchy of stability for superconducting hardware. We find that the intrinsic device parameters determining the qubit frequency and the baseline energy relaxation times ($T_1$) exhibit high robustness against thermal stress, characterized by frequency deviations typically confined within 0.5\% and non-degraded coherence baselines. In stark contrast, the environmental variables, specifically the background magnetic flux offsets and the microscopic landscape of two-level system (TLS) defects, undergo a significant stochastic reconfiguration after each cycle. By employing frequency-dependent relaxation spectroscopy and a quantitative metric, the $T_1$ Spectral Topography Fidelity, we demonstrate that thermal cycling acts as a ``hard reset'' for the local defect environment. This process introduces a level of spectral randomization equivalent to thousands of hours of continuous low-temperature evolution. These findings confirm that while the fabrication quality is preserved, the specific noise realization is statistically distinct for each thermal cycle, necessitating automated recalibration strategies for large-scale quantum systems.

</details>


### [341] [Squeezing-enhanced dual-channel interference for ground-state cooling of a levitated micromagnet with low quality factor](https://arxiv.org/abs/2602.07531)
*Lei Chen,Zhe-qi Yang,Liang Bin,Zhi-Rong Zhong*

Main category: quant-ph

TL;DR: 提出双通道冷却方案，通过压缩增强量子干涉显著降低宏观振子量子基态冷却所需的机械品质因数阈值，提升冷却效率并缩短时间


<details>
  <summary>Details</summary>
Motivation: 宏观振子质心运动冷却至量子基态需超高机械品质因数（$Q_c$），当前实验条件难以满足，限制宏观量子力学测试

Method: 在悬浮腔-磁力机械混合系统中设计双通道冷却方案，协同利用压缩效应与磁子-质心/腔-质心通道间的量子干涉，抑制斯托克斯散射并增强反斯托克斯散射

Result: 临界$Q_c$降低三个数量级至$10^4$实验可实现范围；净冷却速率提升180倍；稳态质心占据数与冷却时间减少两个数量级；在远非解边带 regime 仍保持鲁棒性

Conclusion: 通过主动控制冷却动力学大幅放宽材料本征属性限制，为制备宏观量子态提供可行路径

Abstract: Cooling the center-of-mass (CM) motion of a macroscopic oscillator to its quantum ground state is a fundamental prerequisite for testing quantum mechanics at macroscopic scales. However, achieving this goal is currently hindered by the stringent requirement for an ultrahigh mechanical quality factor ($Q_c$). Here, we propose a dual-channel cooling scheme based on squeezing-enhanced quantum interference within a hybrid levitated cavity-magnomechanical system to overcome this limitation.
  By synergizing squeezing effects with quantum interference between the magnon-CM and cavity-CM channels, our scheme simultaneously suppresses Stokes (heating) scattering while enhancing anti-Stokes (cooling) scattering.~We demonstrate that this cooling mechanism reduces the critical $Q_c$ required for ground-state cooling by three orders of magnitude, making it achievable in the experimentally accessible regime of $Q_c \sim 10^4$. Furthermore, the net cooling rate is enhanced by nearly 180-fold compared to that of conventional single-channel cooling. This improvement is accompanied by a two orders of magnitude reduction in both the steady-state CM occupancy and the cooling time. Importantly, this enhanced performance remains robust even deep within the unresolved-sideband regime. Our results provide a feasible path toward preparing macroscopic quantum states by actively controlling the cooling dynamics, thereby relaxing the constraints on intrinsic material properties.

</details>


### [342] [Characterization of Autofluorescence in Optical Fibers for NV-based Sensing Applications](https://arxiv.org/abs/2602.07536)
*Stefan Johansson,Alexander Bukschat,Dennis Lönard,Alena Erlenbach,Jonas Gutsche,Artur Widera*

Main category: quant-ph

TL;DR: 通过分析标准光纤的光学光谱，识别具有最小背景噪声的光纤类型，以提升氮空位中心量子传感器的灵敏度


<details>
  <summary>Details</summary>
Motivation: 光纤背景荧光会与氮空位中心的荧光光谱重叠，降低信噪比并限制量子传感器灵敏度，需解决该噪声干扰问题

Method: 研究标准光纤的光学光谱，分析材料依赖性、物理影响因素以及荧光随激发功率和波长的缩放规律

Result: 识别出关键光谱成分及具有最低背景信号的光纤类型

Conclusion: 为氮空位量子传感应用中的最优光纤选择提供实验依据

Abstract: Optical fibers are crucial for guiding light in various sensing applications. Especially for quantum sensors such as the nitrogen-vacancy (NV) center in diamond, they enable light control and device miniaturization. However, fluorescence and scattering within the fiber, often referred to as fiber background, autofluorescence, or autoluminescence, can overlap spectrally with the NV centers' fluorescence, degrading the signal-to-noise ratio and thus limiting sensor sensitivity. Here, we investigate the optical spectra of standard optical fibers, considering material dependencies, physical influences, and their fluorescence scaling with excitation power and wavelength. Our results identify spectral components and fiber types with minimal unwanted background signals, guiding the selection of optimal fibers for NV-based quantum sensing.

</details>


### [343] [Hidden Kinematics and Dual Quantum References in Magnetic Resonance](https://arxiv.org/abs/2602.07636)
*Sunghyun Kim*

Main category: quant-ph

TL;DR: 提出双量子参考系框架解决自旋共振能量核算矛盾，揭示跃迁概率是参考系间关系量而非自旋态内禀属性


<details>
  <summary>Details</summary>
Motivation: 传统旋转坐标系中的自旋共振跃迁概率依赖于量子参考标准选择，导致能量核算不一致，缺乏对物理本质的清晰解释

Method: 构建共享量子化算符但运动学与动力学角色不同的双量子描述体系，将自旋矢量运动与动力学演化统一纳入框架

Result: 恢复能量核算一致性，证明跃迁概率本质是量子参考标准间的关系量，揭示旋转磁场中自旋动力学的双参考结构基础

Conclusion: 该框架革新了对自旋共振现象的理解，为量子系统在旋转场中的动力学提供了更自洽的理论描述基础

Abstract: Spin resonance phenomena are conventionally described using transition probabilities formulated in a rotating frame, whose physical meaning implicitly depends on the choice of quantum reference standard. In this Colloquium, we show that a spin in a rotating magnetic field constitutes a configuration involving two quantum descriptions that share a common quantization operator but differ in their kinematic and dynamical roles. The transition probability therefore emerges as a relational quantity between quantum reference standards rather than an intrinsic property of a single evolving spin state. By incorporating the kinematic motion of the spin vector together with the dynamical evolution, this framework restores consistent energy accounting and reveals the dual-reference structure underlying spin dynamics in rotating magnetic fields.

</details>


### [344] [Two-phase driving of a linear radio-frequency ion trap](https://arxiv.org/abs/2602.07700)
*Santhosh Surendra,Akos Hoffmann,Michael Köhl*

Main category: quant-ph

TL;DR: 提出一种180°相位差的双路高压射频驱动线性保罗阱的新技术，成功捕获并冷却了镱离子链，有效降低了轴向微运动。


<details>
  <summary>Details</summary>
Motivation: 解决传统单端驱动方式在特定结构（如射频电极与端帽电极间电容不可忽略时）导致轴向微运动幅度增大的问题，提高离子囚禁的稳定性。

Method: 生成两路相位相差180°的高压射频信号，以相邻电极间施加反向电压的方式驱动线性保罗阱，取代传统的一对接地、另一对驱动的单端模式。

Result: 利用该技术成功在射频保罗阱中捕获并冷却了一串镱离子。

Conclusion: 这种差分驱动技术有效抑制了传统驱动方式引起的轴向微运动问题，为实现稳定离子链囚禁提供了可行方案。

Abstract: A linear radio-frequency Paul trap is traditionally driven with one diagonal pair of electrodes grounded and the other connected to a high-voltage radio-frequency source. This method simplifies impedance matching of the voltage source to the trap. However, for several architectures it leads to increasing the axial micromotion amplitude, for example, when the capacitance between radio-frequency and end-cap electrodes is not negligible. Here, we present a technique to generate two high-voltage radio-frequency signals \SI{180}{\degree} out of phase to drive a linear Paul trap with opposite voltages between neighbouring electrodes. Using this, we have successfully trapped and cooled a chain of Ytterbium ions in a linear radio-frequency Paul trap.

</details>


### [345] [Quantum Steering and Entanglement in a Tritter: Hierarchy under Loss](https://arxiv.org/abs/2602.07788)
*Jifeng Sun,Shumin Yang,Teng Zhao,Qingqian Kang,Liyun Hu*

Main category: quant-ph

TL;DR: 该论文研究通过在tritter上混合双模压缩真空与相干态产生的三模连续变量量子态，发现EPR导引比量子纠缠对光学损耗更具鲁棒性，且构成纠缠的严格子集。


<details>
  <summary>Details</summary>
Motivation: 连续变量的多体纠缠态是可扩展量子信息处理的基础资源，研究其关联层级（特别是EPR导引与纠缠的关系）对发展非对称量子协议至关重要。

Method: 利用协方差矩阵形式论，系统分析三模输出态的量子纠缠与EPR导引特性，并采用参数扩展技术研究多种信道配置下的光学损耗影响。

Result: 关联强度仅由压缩参数决定，与相干振幅无关；损耗虽降低关联性，但EPR导引保持单配性且具有比纠缠更严格的鲁棒性阈值；证实导引条件比不可分判据更严格，导引是纠缠的严格子集。

Conclusion: 该研究阐明了易制备多模态的关联结构，为开发基于EPR导引的单边设备无关量子协议提供了实用指导。

Abstract: Multipartite entangled states of continuous variables are fundamental resources for scalable quantum information processing. We study the correlation hierarchy in a tripartite state engineered by mixing a two-mode squeezed vacuum with a coherent state on a tritter, a key linear optical element for multimode state generation. Using the covariance matrix formalism, we comprehensively analyze the entanglement and Einstein-Podolsky-Rosen (EPR) steering among the output modes. The strength of both correlations is governed solely by the squeezing parameter and is independent of the coherent amplitude. We further examine the impact of inevitable optical losses in various channel configurations. The results show that while losses degrade correlations, EPR steering remains monogamous and exhibits stricter resilience thresholds than entanglement. Our analysis, supported by parameter extension techniques, confirms that the steering condition is more stringent than the inseparability criterion, clearly demonstrating that steering forms a strict subset of entanglement. These results elucidate the correlation structure in a readily generated multimode state and offer practical insights for developing asymmetric quantum protocols, such as one-sided device-independent tasks, where EPR steering serves as a critical resource.

</details>


### [346] [Higher-Order Corrections to Scrambling Dynamics in Brownian Spin SYK Models](https://arxiv.org/abs/2602.07952)
*Tingfei Li,Miao Wang,Jianghui Yu*

Main category: quant-ph

TL;DR: This paper analyzes operator growth in a Brownian SYK model by deriving a master equation for Pauli-string coefficients and using generating functions to solve the large-N dynamics exactly, revealing that higher-order corrections are crucial for quantum scrambling and decoherence effects.


<details>
  <summary>Details</summary>
Motivation: To understand full operator-size distribution dynamics in open quantum systems (beyond standard OTOC probes), specifically how decoherence and higher-order interactions affect operator scrambling in chaotic Brownian SYK models with all-to-all random couplings.

Method: Derives a closed master equation for Pauli-string expansion coefficients; reformulates dynamics via generating functions for large-N limit; explicitly diagonalizes leading-order evolution operator; develops systematic 1/N expansion for higher-order corrections.

Result: Obtains exact solutions for arbitrary initial operators at leading order; demonstrates significant late-time corrections from higher-order (1/N) terms; shows full operator-size distribution provides deeper insights into quantum chaos than conventional probes, especially under decoherence.

Conclusion: Higher-order effects are essential for accurate description of operator scrambling in open systems; the full operator-size distribution serves as a refined diagnostic tool for quantum chaos in Brownian and dissipative quantum many-body systems.

Abstract: We investigate operator growth in a Brownian spin Sachdev--Ye--Kitaev (SYK) model with random all-to-all interactions, focusing on the full operator-size distribution. For Hamiltonians containing interactions of order two up to $L$, we derive a closed master equation for the Pauli-string expansion coefficients and recast their dynamics into a generating-function formulation suitable for the large-$N$ limit. This approach allows us to diagonalize the leading-order evolution operator explicitly and obtain exact solutions for arbitrary initial operator distributions, including the effects of decoherence. Going beyond leading order, we develop a systematic $1/N$ expansion that captures higher-order corrections to the operator-size dynamics and the late-time behavior. Our results demonstrate that higher-order effects play a crucial role in operator scrambling and that the full operator-size distribution provides a more refined probe of quantum chaos in Brownian and open quantum systems.

</details>


### [347] [Geometric criticality in the driven Jaynes-Cummings model](https://arxiv.org/abs/2602.07795)
*Ken Chen,Jia-Hao Lv,Hao-Long Zhang,Fan Wu,Wen Ning,Zhen-Biao Yang,Shi-Biao Zheng*

Main category: quant-ph

TL;DR: 研究Jaynes-Cummings模型中光子阻塞破裂相变的本征态几何临界性，发现亮态比暗态的量子度规和Berry曲率发散更显著


<details>
  <summary>Details</summary>
Motivation: 现有研究集中于光子阻塞破裂的相变现象本身，但临界点附近本征态的几何性质（如量子度规和Berry曲率）尚未被探索

Method: 将经典驱动场的振幅和相位作为控制参数，计算驱动Jaynes-Cummings模型本征态的量子度规和Berry曲率张量

Result: 临界区所有本征态的几何量均发散，但亮态的发散程度显著强于唯一的暗态

Conclusion: 揭示了本征态几何量在量子相变临界点的普适行为差异，该理论可通过电路量子电动力学实验验证

Abstract: When the photonic mode in the Jaynes-Cummings model is driven by an external classical field, the system can undergo the photon-blockade breakdown phase transition at a critical point. Such a phase transition has been detailedly investigated, but the critical properties of the eigenstates remain largely unexplored so far. We here study the geometric criticality associated with these eigenstates. The amplitude and phase of the drive serve as the control parameter of the governing Hamiltonian. We find the quantum metric and Berry curvature tensors for each eigenstate display divergent behaviors in the critical region. More importantly, the divergence associated with bright eigenstates is much more pronounced than that for the unique dark state. Our theoretical results can be experimentally confirmed in circuit quantum electrodynamics systems, where the driven Jaynes-Cummings model has been realized.

</details>


### [348] [Semi-device-independent certification of high-dimensional quantum channels](https://arxiv.org/abs/2602.07823)
*Mengyan Li,Yanning Jia,Fenzhuo Guo,Haifeng Dong,Sujuan Qin,Fei Gao*

Main category: quant-ph

TL;DR: A semi-device-independent framework for certifying high-dimensional quantum channels using only observed statistics and known system dimension, employing Choi-Jamiołkowski isomorphism to certify both entanglement dimensionality and fidelity through numerical bounds and semidefinite programming relaxations.


<details>
  <summary>Details</summary>
Motivation: Existing quantum channel certification schemes require fully trusted internal devices, which is unrealistic and impractical for real-world quantum communication scenarios.

Method: Proposes a semi-device-independent approach that incorporates Choi state structural constraints via Choi-Jamiołkowski isomorphism. For entanglement dimensionality certification, uses a witness with Schmidt-number-dependent numerical bounds. For entanglement fidelity, applies a hierarchy of semidefinite programming relaxations based on localizing matrices.

Result: Successfully certifies entanglement dimensionality, reproducing analytical benchmarks on dephasing and depolarizing channels. Obtains compatible lower bounds on entanglement fidelity from either full statistics or single witness values.

Conclusion: The framework provides a rigorous and practical method for quantum channel certification that reduces trust assumptions while maintaining reliability, suitable for realistic quantum communication protocols.

Abstract: Certifying high-dimensional quantum channels is essential for ensuring the reliability of quantum communication protocols. Existing certification schemes often rely on fully trusted internal devices, which is difficult to achieve in realistic scenarios. Here, we propose a semi-device-independent framework for certifying channel properties directly from observed statistics, assuming only that the system dimension is known. By explicitly incorporating the full set of structural constraints inherent to Choi states, our approach exploits the Choi-Jamiołkowski isomorphism for rigorous certification of quantum channels. The entanglement dimensionality of quantum channels is first certified by introducing a witness and numerically determining its Schmidt-number-dependent bounds. This certification method reproduces known analytical benchmarks and is applied to dephasing and depolarizing noise channels, thereby confirming its validity. To provide a more complete assessment of channel performance, the entanglement fidelity of quantum channels is also certified using a hierarchy of semidefinite programming relaxations based on localizing matrices. Lower bounds on the entanglement fidelity are obtained that are compatible with either the full set of observed statistics or a single witness value.

</details>


### [349] [Geometry-Enabled Radiation from Structured Paraxial Electrons](https://arxiv.org/abs/2602.07858)
*M. S. Epov,I. E. Shenderovich,S. S. Baturin*

Main category: quant-ph

TL;DR: 扭曲电子通过轴对称非均匀磁场时，其波前曲率会产生几何辐射，即使在磁场为零区域也能发光，推广了朗道辐射理论。


<details>
  <summary>Details</summary>
Motivation: 探究扭曲电子在复杂磁场中的自发辐射机制，揭示几何效应对辐射的影响，突破传统朗道辐射仅适用于平面波的局限。

Method: 结合Foldy-Wouthuysen变换与基于Lewis-Ermakov不变量的几何框架，构建含横向模式结构的精确电子态进行微观计算。

Result: 结构化电子态演化对应二次型空间开路径，产生不可消除的几何辐射振幅；波前曲率倒数作为有效场，使辐射可在磁场消失区域发生。

Conclusion: 建立非循环几何演化与光子发射的直接联系，推广朗道能级辐射，揭示波前曲率是新辐射机制的关键。

Abstract: We present a microscopic calculation of spontaneous photon emission by twisted (paraxial) electrons propagating through inhomogeneous, axisymmetric magnetic fields. We construct exact electron states that incorporate transverse mode structure and wavefront curvature by combining the Foldy-Wouthuysen transformation with a geometric framework based on Lewis-Ermakov invariants and metaplectic transformations. We show that the evolution of such structured states corresponds to an open path in the space of quadratic forms, giving rise to a geometric contribution to the emission amplitude that cannot be eliminated by gauge choice or adiabatic arguments. The inverse radius of curvature of the electron wavefront emerges as an effective geometric field that enables radiation even in regions where the external magnetic field vanishes locally. This mechanism generalizes Landau-level radiation to nonasymptotic, structured electron states and establishes a direct connection between noncyclic geometric evolution and photon emission.

</details>


### [350] [Minimal nonintegrable models with three-site interactions](https://arxiv.org/abs/2602.07867)
*Wen-Ming Fan,Kun Hao,Xiao-Hui Wang,Kun Zhang,Vladimir Korepin*

Main category: quant-ph

TL;DR: 本文系统分类了具有真实三格点相互作用的最小非可积自旋-1/2哈密顿量。通过分析变形Fredkin链并构造五类模型，发现其中四类仅含两个相互作用项，构成最小非可积模型，在超越最近邻的体系中划定了可积与非可积的明确边界。


<details>
  <summary>Details</summary>
Motivation: 目前对于具有真实三格点相互作用的平移不变自旋链中可积性破坏的系统性理解仍然缺乏。本文旨在定义并分类最小非可积哈密顿量，即那些在保持注入性的同时，除哈密顿量外不包含任何非平凡局域守恒荷的模型。

Method: 首先通过映射到最近邻复合自旋表示并排除所有可接受的三格点局域守恒荷，严格证明了周期边界条件下变形Fredkin自旋链的非可积性。在此基础上，构造了五类具有真实三格点相互作用的自旋-1/2模型。

Result: 在构造的五类模型中，一类是可积的，其余四类各包含两个相互作用项，构成了最小非可积三格点模型。这一分类结果揭示了超越最近邻范式下可积性与非可积性之间的尖锐边界。

Conclusion: 该研究为理解三格点相互作用自旋链中的可积性破坏提供了系统性框架，明确了最小非可积模型的特征，深化了对更广泛相互作用体系中可积性分类的认识。

Abstract: A systematic understanding of integrability breaking in translationally invariant spin chains with genuine three-site interactions remains lacking. In this work, we introduce and classify minimal nonintegrable spin-$1/2$ Hamiltonians, defined as models that saturate injectivity while admitting no nontrivial local conserved charges beyond the Hamiltonian. We first rigorously establish the nonintegrability of the deformed Fredkin spin chain with periodic boundary conditions by mapping it to a nearest-neighbor composite-spin representation and excluding all admissible $3$-local conserved charges. Guided by its structure, we then construct five classes of spin-$1/2$ models with genuine three-site interactions. One class is integrable, while the remaining four contain exactly two interaction terms and constitute the minimal nonintegrable three-site models. Our results delineate a sharp boundary between integrability and nonintegrability beyond the nearest-neighbor paradigm.

</details>


### [351] [Quantum Evolution of Hopf Algebra Hamiltonians](https://arxiv.org/abs/2602.07887)
*Michele Arzano,Antonio Del Prete,Domenico Frattulillo*

Main category: quant-ph

TL;DR: 该论文研究了非对易时空中变形对称性理论是否可能产生Lindblad型退相干效应，但分析表明即使考虑Hopf代数变形，也无法为量子比特系统建立物理上可行的退相干模型。


<details>
  <summary>Details</summary>
Motivation: 探讨非对易时空模型中变形对称性理论是否可能编码基本的量子退相干效应，这种效应应由时间演化生成元的非平凡Hopf代数结构描述的Lindblad型演化来体现。

Method: 从广义伴随作用定义时间演化的关键检验出发，系统分析Hopf代数变形对量子比特哈密顿量的影响，检验能否建立物理自洽的框架。

Result: 分析表明更一般的伴随作用组合总能保证冯·诺依曼动力学，但在文献考虑的变形时空对称性情况下，无法建立物理上可行的Lindblad演化。

Conclusion: 基于Hopf代数变形的对称性理论无法产生预期的Lindblad型退相干效应，该理论框架在物理上不可行，否定了此类模型解释基本退相干的可能性。

Abstract: In recent years, growing attention has been devoted to the possibility that theories with deformed symmetries, associated with certain models of non-commutative spacetime, may encode a fundamental form of decoherence. This effect should be described by a Lindblad-like evolution governed by the non-trivial Hopf algebra structure of the time-evolution generators. In this work we provide a detailed analysis of such possibility for similar Hopf algebra deformations of the Hamiltonian of a qubit. Starting from a critical examination of the very definition of time evolution through the generalized adjoint action, we explore whether a coherent and physically viable framework can be established. In particular, our analysis shows that a more general combination of adjoint actions always guarantees a von Neumann dynamics and, also in the case of deformed spacetime symmetries considered in the literature, a physically viable Lindblad evolution cannot be established.

</details>


### [352] [Doubling the size of quantum selected configuration interaction based on seniority-zero space and its application to QC-QSCI-AFQMC](https://arxiv.org/abs/2602.07912)
*Yuichiro Yoshida,Takuma Murokoshi,Naoya Kuroda,Wataru Mizukami*

Main category: quant-ph

TL;DR: 提出DOCI-QSCI方法，在seniority-zero空间采样以双倍利用量子比特（等于空间轨道数），通过笛卡尔积扩展采样位串至seniority-breaking空间，并联合ph-AFQMC恢复动态关联，显著扩大可处理活性空间


<details>
  <summary>Details</summary>
Motivation: 解决量子计算中量子比特资源限制问题：传统QSCI受限于量子比特数，而seniority-zero空间可双倍扩展轨道空间，但需补偿该空间限制导致的定量精度损失

Method: 1) 采用DOCI-QSCI在seniority-zero空间采样；2) 通过笛卡尔积将采样位串扩展至含seniority-breaking行列式的更大空间；3) 以ph-AFQMC作为后处理步骤恢复全轨道空间的动态关联（DOCI-QSCI-AFQMC）

Result: 在H6链、N2解离及BODIPY-O2反应中验证：H6链上结果与完全活性空间方法一致（使用ibm_kobe量子设备）；N2（14e,28o）和BODIPY-O2（高达20e,20o）活性空间中结果合理，而单参考CCSD(T)定性失败

Conclusion: DOCI-QSCI使传统QSCI可访问的轨道空间翻倍，结合ph-AFQMC后处理能在较大活性空间中实现较高精度，为量子计算处理强关联体系提供可行方案

Abstract: We propose doubly occupied configuration interaction-quantum selected configuration interaction (DOCI-QSCI), which samples from the seniority-zero space. While the use of this space effectively doubles the qubit budget, equaling the number of spatial orbitals, this sector restriction can compromise quantitative accuracy. To compensate for this, we expand sampled bitstrings via their Cartesian product into a larger space that includes seniority-breaking determinants. The resulting wave function is also proposed using the trial state in phaseless auxiliary-field quantum Monte Carlo (ph-AFQMC) to recover dynamical correlations across the full orbital space (DOCI-QSCI-AFQMC). We evaluate the proposed methods on the H6 chain, N2 dissociation, and the addition of singlet O2 to a BODIPY dye. For the H6 chain, DOCI-QSCI-AFQMC reproduces the accuracy of the level of the complete-active-space counterpart with the quantum device ibm kobe. For N2 and BODIPY-O2, with (14e, 28o) and up to (20e, 20o) active spaces, it yields reasonable results, whereas single-reference CCSD(T) fails qualitatively. These results demonstrate that the DOCI-QSCI doubles the orbital space accessible to conventional QSCI and subsequent ph-AFQMC post-processing delivers reasonably high accuracy.

</details>


### [353] [Real-Time Magnetic Field Sensing based on Microwave Frequency Modulated Photocurrent of Nitrogen-Vacancy Centers in Diamond](https://arxiv.org/abs/2602.07926)
*Xuan-Ming Shen,Qilong Wu,Huihui Yu,Pei-Nan Ni,Qing Lou,Chao-Nan Lin,Xun Yang,Chong-Xin Shan,Yuan Zhang*

Main category: quant-ph

TL;DR: 首次实现基于光电检测磁共振(PDMR)的氮空位(NV)中心磁场实时传感，在DC-10 Hz范围获得397 nT/Hz(理论)和921 nT/Hz(实验)的灵敏度，并实时追踪1.5 μT交流磁场，为量子传感器小型化提供新方案。


<details>
  <summary>Details</summary>
Motivation: 虽然PDMR技术有望实现氮空位(NV)中心量子传感器的小型化，但基于PDMR的磁场实际传感演示仍是一个显著挑战。

Method: 在钻石表面制备电极和微波天线，通过锁相放大模式检测纳安级光电流实现PDMR，结合主方程理论模型分析激光/微波功率对对比度、线宽和灵敏度的影响。

Result: 在DC-10 Hz磁场检测中实现397 nT/Hz(理论)和921 nT/Hz(实验)灵敏度；首次实现标准差1.5 μT的交流磁场实时追踪；实验结果与包含NV中心电荷态转换及微波场相互作用的主方程模型高度吻合。

Conclusion: 成功验证PDMR技术用于实际磁场传感的可行性，为开发小型化、高灵敏度的量子磁传感器奠定了实验和理论基础。

Abstract: While photoelectric detection of magnetic resonance (PDMR) can be applied to miniaturize nitrogen-vacancy (NV) center-based quantum sensors, the real demonstration of PDMR-based magnetic field sensing remains as a distinctive challenge. To tackle this challenge, in this article, we fabricate diamond samples with electrodes and microwave antenna on the surface, and realize PDMR by detecting photocurrent in nanoampere range via various lock-in amplifying modes. Importantly, we obtain a theoretical and experimental sensitivity 397 nT/Hz and 921 nT/Hz of magnetic field detection in DC-10 Hz range with a laser intensity and microwave frequency modulated mode, respectively, and demonstrate for the first time, a real-time tracking of alternating magnetic field with a standard deviation of 1.5 uT. Furthermore, we investigate systematically the dependence of the PDMR contrast, linewidth and the sensitivity on the laser and microwave power, and find a perfect agreement with a master equation based theoretical model, which accounts for not only the optically induced charge switch of neutral and negative NV centers, but also the interaction with microwave field.

</details>


### [354] [Full Schmidt characterization of spatiotemporally entangled states produced from spontaneous parametric down-conversion](https://arxiv.org/abs/2602.07949)
*Rakesh Pradhan,Girish Kulkarni*

Main category: quant-ph

TL;DR: 首次通过利用旋转对称性大幅降低计算复杂度，完成了SPDC产生时空纠缠态的完整Schmidt分解，揭示了10^4+模式的Schmidt谱和携带轨道角动量的涡旋结构，为量子成像/光谱应用提供基础


<details>
  <summary>Details</summary>
Motivation: SPDC产生的高维时空纠缠态因计算复杂度过高，其完整Schmidt分解长期未实现，阻碍了对纠缠本质的理解和应用开发

Method: 利用态的旋转对称性将计算复杂度降低至少4个数量级，首次完成大规模Schmidt分解

Result: 1) 精确获得超10^4个Schmidt模式和频谱 2) 发现模式具有全频域轨道角动量的涡旋相位结构 3) 高增益区模式展宽而频谱随泵浦增强变窄

Conclusion: 该计算突破揭示了SPDC纠缠态的精细结构，将推动基于此类纠缠源的量子成像与光谱学新应用发展

Abstract: The full Schmidt decomposition of spatiotemporally entangled states generated from spontaneous parametric down-conversion (SPDC) has not been carried out until now due to the immense computational complexity arising from the large dimensionalities of the states. In this Letter, we utilize the rotational symmetry of the states to reduce the complexity by at least four orders of magnitude and carry out the decomposition to reveal the precise forms of the spatiotemporal Schmidt modes and the Schmidt spectrum spanning over 10^4 modes. We show that the Schmidt modes have a phase profile with a transverse spatial vortex structure that endows them with orbital angular momentum at all frequencies. In the high-gain regime, these Schmidt modes broaden and the Schmidt spectrum narrows with increasing pump strength. Our work can spur novel applications at the intersection of quantum imaging and spectroscopy that utilize entangled states produced from SPDC.

</details>


### [355] [Quantum self-interaction within an infinitely deep cavity](https://arxiv.org/abs/2602.07956)
*Sergio Giardino*

Main category: quant-ph

TL;DR: 该研究在实Hilbert空间框架下分析无限深量子势阱，通过复数和四元数波函数拓展解的形式，发现复数解包含非稳态、畸变态及新能谱，四元数解首次引入自相互作用效应，为非相对论一维问题提供更普适的解法路径


<details>
  <summary>Details</summary>
Motivation: 突破传统复Hilbert空间对无限深量子势阱的局限性，探索实空间及四元数框架下更普适的解，尤其关注非稳态解与自相互作用等复方法无法描述的现象

Method: 在实Hilbert空间中求解无限深量子势阱方程，分别构建复数及四元数波函数解，对比分析其数学特性与物理行为差异

Result: 复数解重现经典结果并扩展至非稳态/畸变态解、新能谱及位置偏移；四元数解首次实现自相互作用效应，且两类解均显著超越传统解的完备性

Conclusion: 该工作为量子力学非相对论一维问题开辟了新解法范式，尤其为四元数框架下的自相互作用研究提供理论基础，推动更复杂系统求解方法的发展

Abstract: One examines the infinitely deep quantum cavity, also known as the quantum infinite square well, within the framework of the real Hilbert space. The solutions are considered in terms of complex wave functions, and also in terms of quaternionic wave functions. The complex results reproduce the usual achievements established in the complex Hilbert space, but also extend them to non-stationary solutions, as well as to distorted stationary solutions, different energy spectra, and dislocated observed position. The quaternionic cases further admit the incidence of self-interaction, something that cannot be observed in complex solutions. Therefore, both the complex and quaternionic solutions are more general than previous cases, thus opening the way to further one-dimensional solutions to be researched in the non-relativistic theory.

</details>


### [356] [Improved entanglement-based high-dimensional optical quantum computation with linear optics](https://arxiv.org/abs/2602.07971)
*Huan-Chao Gao,Guo-Zhu Song,Hai-Rui Wei*

Main category: quant-ph

TL;DR: 该论文提出了一种基于纠缠的光学控制SWAP高维量子门新方案，采用光子偏振与空间自由度的混合编码方式，相比之前d=2的工作，将线性光学元件从14个减少到5个，电路深度从11降低到5，保真度提升至99.4%，且无需辅助光子或测量诱导非线性即可确定性实现，并支持d>2的扩展。


<details>
  <summary>Details</summary>
Motivation: 高维量子门相比传统二维量子门在量子信息处理任务中具有显著优势，但现有方案在资源消耗、电路复杂度和保真度方面存在改进空间。本研究旨在设计更高效、性能更优的高维量子门实现方案。

Method: 利用纠缠资源和混合编码技术，将控制比特编码在光子偏振自由度，目标量子态编码在空间自由度，仅用(2+3d)个线性光学元件构建电路，电路深度为5，以确定性方式实现控制SWAP门操作。

Result: 资源效率：仅需(2+3d)个线性光学元件（d=2时仅需5个）；电路深度：5层；保真度：最高可达99.4%；实现方式：完全确定性，无需辅助光子或非线性测量；扩展性：支持d>2的高维系统。

Conclusion: 该方案在资源消耗、电路复杂度和操作性能方面均优于之前d=2的对应工作，为高维量子信息处理的实用化实现提供了更高效的路径，其确定性特性和良好的可扩展性具有重要应用价值。

Abstract: Quantum gates are the essential block for quantum computer. High-dimensional quantum gates exhibit remarkable advantages over their two-dimensional counterparts for some quantum information processing tasks. Here we present a family of entanglement-based optical controlled-SWAP gates on $\mathbb{C}^{2}\otimes \mathbb{C}^{d}\otimes \mathbb{C}^{d}$. With the hybrid encoding, we encode the control qubits and target qudits in photonic polarization and spatial degrees of freedom, respectively. The circuit is constructed using only $(2+3d)$ ($d\geq 2$) linear optics, beating an earlier result of 14 linear optics with $d=2$. The circuit depth 5 is much lower than an earlier result of 11 with $d=2$. Besides, the fidelity of the presented circuit can reach 99.4\%, and it is higher than the previous counterpart with $d=2$. Our scheme are constructed in a deterministic way without any borrowed ancillary photons or measurement-induced nonlinearities. Moreover, our approach allows $d>2$.

</details>


### [357] [Optimal Quantum Speedups for Repeatedly Nested Expectation Estimation](https://arxiv.org/abs/2602.08120)
*Yihang Sun,Guanyang Wang,Jose Blanchet*

Main category: quant-ph

TL;DR: 该论文提出了一种用于估计重复嵌套期望的量子算法，实现了ε^{-1}的成本复杂度，相比经典算法有几乎二次加速。


<details>
  <summary>Details</summary>
Motivation: 扩展量子计算在嵌套期望估计中的应用，从单层嵌套到重复嵌套，以覆盖更广泛的应用，如最优停止问题。

Method: 提出一种新的去随机化变体经典随机多级蒙特卡洛（rMLMC）算法，并应用于量子计算框架中，以解决变时间问题。

Result: 量子算法实现了tilde{O}(ε^{-1})的成本复杂度，且该缩放是基本最优的，相比经典算法有几乎二次加速。

Conclusion: 该研究成功将量子加速扩展到重复嵌套期望，通过去随机化处理变时间问题，为相关领域提供了更高效的解决方案。

Abstract: We study the estimation of repeatedly nested expectations (RNEs) with a constant horizon (number of nestings) using quantum computing. We propose a quantum algorithm that achieves $\varepsilon$-error with cost $\tilde O(\varepsilon^{-1})$, up to logarithmic factors. Standard lower bounds show this scaling is essentially optimal, yielding an almost quadratic speedup over the best classical algorithm. Our results extend prior quantum speedups for single nested expectations to repeated nesting, and therefore cover a broader range of applications, including optimal stopping. This extension requires a new derandomized variant of the classical randomized Multilevel Monte Carlo (rMLMC) algorithm. Careful de-randomization is key to overcoming a variable-time issue that typically increases quantized versions of classical randomized algorithms.

</details>


### [358] [Spinor Double-Quantum Excitation in the Solution NMR of Near-Equivalent Spin-1/2 Pairs](https://arxiv.org/abs/2602.08157)
*Urvashi D. Heramun,Mohamed Sabba,Dolnapa Yamano,Christian Bengs,Bonifac Legrady,Giuseppe Pileio,Sam Thompson,Malcolm H. Levitt*

Main category: quant-ph

TL;DR: 本文描述了一种用于近等价自旋-1/2对溶液核磁共振的双量子激发方案家族，利用自旋子行为来操纵单量子相干性的相位，并通过19F NMR实验验证，与现有技术进行比较。


<details>
  <summary>Details</summary>
Motivation: 开发新的NMR方法以更好地处理近等价自旋对，利用自旋子特性提高双量子激发的效率和鲁棒性。

Method: 基于自旋子的方法，包括对称性脉冲序列和SLIC（自旋锁定诱导交叉）技术，以及一个对射频场幅度偏差补偿良好的SLIC变体。

Result: 在含有非对映异位19F核的分子系统上成功演示了双量子滤波19F NMR。

Conclusion: 新方法与现有技术相比具有优势，可能提供更高效或更稳定的双量子激发方案。

Abstract: A family of double-quantum excitation schemes is described for the solution nuclear magnetic resonance (NMR) of near-equivalent spin-1/2 pairs. These new methods exploit the spinor behaviour of 2-level systems, whose signature is the change of sign of a quantum state upon a $2π$ rotation. The spinor behaviour is used to manipulate the phases of single-quantum coherences, in order to prepare a double-quantum precursor state which is rapidly converted into double-quantum coherence by a straightforward $π/2$ rotation. One set of spinor-based methods exploits symmetry-based pulse sequences, while the other set exploits SLIC (spin-lock-induced crossing), in which the nutation frequency under a resonant radiofrequency field is matched to the spin-spin coupling. A variant of SLIC is introduced which is well-compensated for deviations in the radiofrequency field amplitude. The methods are demonstrated by performing double-quantum-filtered $^{19}$F NMR on a molecular system containing a pair of diastereotopic $^{19}$F nuclei. The new methods are compared with existing techniques.

</details>


### [359] [Detecting multilevel entanglement from light-based entanglement witnesses](https://arxiv.org/abs/2602.08180)
*Pedro Rosario,Romain Bachelard*

Main category: quant-ph

TL;DR: 提出电场不等式新方法检测多能级量子发射体系统的多体纠缠，无需局域测量，可增强检测效果并适用于含噪混合态


<details>
  <summary>Details</summary>
Motivation: 现有纠缠检测方法在多能级系统中存在局限，需开发不依赖局域测量的鲁棒方案以适用于超导量子比特、里德堡原子等实际量子发射体系统

Method: 构建基于电场的不等式判据，通过偏振信道和探测方向优化检测，在迪克态、单态及类W态等典型量子态上验证有效性

Result: 成功检测多体纠缠且对噪声鲁棒，适用于混合纠缠态，极化通道和探测方向可显著提升多能级系统纠缠检测灵敏度

Conclusion: 为多能级量子发射体系统提供无需局域测量的纠缠检测新途径，拓展了量子纠缠实验验证的可能性

Abstract: We introduce a set of electric-field based inequalities capable of detecting multilevel entanglement from a system of N quantum emitters. We determine that the polarization channel as well as the direction of detection can enhance entanglement detection, a feature specific to multilevel systems. We demonstrate the efficiency of the witnesses to detect genuine multipartite entanglement by applying it to families of paradigmatic quantum states, such as Dicke states, singlet states and W-like states. The detection is not only robust to noise, but also applies to mixed entangled states. Our findings open up possibilities for the detection of entanglement without local measurements in systems of multilevel emitters such as superconducting qubits, Rydberg atoms or quantum dots.

</details>


### [360] [Preparing squeezed, cat and GKP states with parity measurements](https://arxiv.org/abs/2602.08209)
*Zhiyuan Lin,Sen Li,Jingyan Feng,Valentin Ivannikov,Matteo Fadel,Tim Byrnes*

Main category: quant-ph

TL;DR: 提出基于位移宇称测量的协议，高效制备玻色量子态（如压缩态、猫态和GKP态），仅需三次测量即可实现~9 dB压缩，且对实验误差鲁棒


<details>
  <summary>Details</summary>
Motivation: 玻色模式是量子信息存储与处理的核心资源，但高效制备特定量子态（如压缩态、猫态）仍具挑战，需开发鲁棒且快速的状态制备方法

Method: 在强色散 regime 下，通过辅助量子比特耦合实现位移宇称测量，结合相空间位移操作构建状态制备协议

Result: 三次宇称测量即实现~9 dB压缩态，协议对实验不完美性具有鲁棒性；成功推广至猫态和Gottesman-Kitaev-Preskill (GKP) 态制备

Conclusion: 该协议为玻色量子态制备提供高效通用方案，显著降低操作复杂度，推动量子存储与量子计算应用发展

Abstract: Bosonic modes constitute a central resource in a wide range of quantum technologies, providing long-lived degrees of freedom for the storage, processing, and transduction of quantum information. Such modes naturally arise in platforms including circuit quantum electrodynamics, quantum acoustodynamics, and trapped-ion systems. In these architectures, coherent control and high-fidelity readout of the bosonic degrees of freedom are achieved via coupling to an auxiliary qubit. When operated in the strong dispersive regime, this interaction enables parity measurements of the mode which, in combination with phase-space displacements, constitute a standard experimental tool for full Wigner-function tomography. Here, we propose a protocol based on displaced parity measurements that allows for the preparation of a variety of bosonic quantum states. As a first example, we demonstrate the generation of squeezed states, achieving up to ~9 dB of squeezing after only three parity measurements, and show that the protocol is robust against experimental imperfections. Finally, we generalize our approach to the preparation of other paradigmatic bosonic states, including cat and Gottesman-Kitaev-Preskill states.

</details>


### [361] [The simplified quantum circuits for implementing quantum teleportation](https://arxiv.org/abs/2602.08345)
*Wen-Xiu Zhang,Guo-Zhu Song,Hai-Rui Wei*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: It is crucial to design quantum circuits as small as possible and as shallow as possible for quantum information processing tasks. We design quantum circuits with simplified gate-count, cost, and depth for implementing quantum teleportation among various entangled channels. Here the gate-count/cost/depth of the Greenberger-Horne-Zeilinger-based quantum teleportation is reduced from 10/6/8 to 9/4/6, the two-qubit-cluster-based quantum teleportation is reduced from 9/4/5 to 6/3/5, the three-qubit-cluster-based quantum teleportation is reduced from 12/6/7 to 8/4/5, the Brown-based quantum teleportation is reduced from 25/15/17 to 18/8/7, the Borras-based quantum teleportation is reduced from 36/25/20 to 15/8/11, and the entanglement-swapping-based quantum teleportation is reduced from 13/8/8 to 10/5/5. Note that, no feed-forward recover operation is required in the simplified schemes. Moreover, the experimentally demonstrations on IBM quantum computer indicate that our simplified and compressed schemes can be realized with good fidelity.

</details>


### [362] [Quantum-classical framework for many-fermion response and structure](https://arxiv.org/abs/2602.08357)
*Weijie Du,Yangguang Yang,Zixin Liu,Chao Yang,James P. Vary*

Main category: quant-ph

TL;DR: 提出量子-经典混合框架结合洛伦兹积分变换，用于计算多费米子系统的响应函数和束缚态谱，并在19O体系上验证。


<details>
  <summary>Details</summary>
Motivation: 响应函数是探测多体系统结构和动力学的关键观测量，但现有方法难以高效计算一般多费米子系统的响应函数和束缚态谱，亟需可扩展的新方法。

Method: 采用量子-经典混合框架，结合洛伦兹积分变换和新型哈密顿量输入方案实现可扩展量子电路；开发混合策略计算洛伦兹积分，并提出三种提取响应函数和束缚态结构信息的协议。

Result: 成功将方法应用于19O体系，使用现实核子间相互作用，同时计算了束缚态谱和响应函数。

Conclusion: 该方法为探索广泛多体系统的结构和动力学开辟了新方向，具有跨学科应用潜力。

Abstract: Response functions are key observables for probing the structure and dynamics of many-body systems. We introduce and demonstrate a quantum-classical framework for computing response functions of general many-fermion systems that also provides the full bound-state spectrum. The framework employs the Lorentz integral transform and a new Hamiltonian input scheme that enables practical and scalable circuit constructions for general many-fermion Hamiltonians. Within this framework, we develop a hybrid strategy to evaluate the Lorentz integral and propose three protocols to extract response functions and bound-state structural information. As a demonstration, we apply the method to \({}^{19}\mathrm{O}\) with realistic internucleon interactions, computing both the bound-state spectrum and the response function. We envision that our approach will open new avenues for exploring the structure and dynamics of a broad class of many-body systems across diverse fields.

</details>


### [363] [Quantum Detection of Sequency-Band Structure](https://arxiv.org/abs/2602.08393)
*Alok Shukla,Prakash Vedula*

Main category: quant-ph

TL;DR: A quantum algorithm for sequency band energy estimation using a sequency-ordered Quantum Walsh-Hadamard Transform and Quantum Amplitude Estimation, achieving exponential speedup over classical methods for quantum signal processing and anomaly detection.


<details>
  <summary>Details</summary>
Motivation: To develop quantum-enhanced signal processing that outperforms classical methods, specifically for detecting structured components and anomalies in quantum-encoded signals by analyzing their sequency spectrum.

Method: Combines a sequency-ordered Quantum Walsh-Hadamard Transform (QWHT) with a comparator-based oracle to coherently mark basis states in arbitrary sequency ranges, followed by Quantum Amplitude Estimation (QAE) to estimate total probability mass in selected bands.

Result: The QWHT has O(log₂N) circuit depth, providing exponential advantage over classical O(N log₂N) fast Walsh-Hadamard transforms. It detects high/low sequency features and rapid sign-change behavior associated with noise or anomalies.

Conclusion: This modular quantum algorithm serves as a structure-based anomaly indicator for quantum signal processing tasks including zero-crossing analysis, band-limited noise estimation, and Walsh basis feature extraction, with fully quantum I/O for seamless integration.

Abstract: We present a quantum algorithm for estimating the amplitude content of user-specified sequency bands in quantum-encoded signals. The method employs a sequency-ordered Quantum Walsh-Hadamard Transform (QWHT), a comparator-based oracle that coherently marks basis states within an arbitrary sequency range, and Quantum Amplitude Estimation (QAE) to estimate the total probability mass in the selected band. This enables the detection of structured signal components, including both high- and low-sequency features, as well as the identification of rapid sign-change behavior associated with noise or anomalies. The proposed method can be embedded as a module within a larger quantum algorithm; in this setting, both the input and output remain fully quantum, enabling seamless integration with upstream and downstream quantum operations. We show that the sequency-ordered QWHT can be implemented with circuit depth $O(\log_2 N)$ (equivalently $O(n)$ for $N=2^n$) when acting on an amplitude-encoded quantum state, whereas computing the full Walsh-Hadamard spectrum of an explicit length-$N$ classical signal requires $O(N\log_2 N)$ operations via the fast Walsh-Hadamard transform. This results in an exponential quantum advantage when the QWHT is used as a modular block within a larger quantum algorithm, relative to classical fast Walsh-Hadamard transform-based approaches operating on explicit data. From an application perspective, the proposed sequency band-energy estimation may be interpreted as a structure-based anomaly indicator, enabling the detection of unexpected high-sequency components relative to a nominal low-sequency signal class. The algorithm is applicable to quantum-enhanced signal processing tasks such as zero-crossing analysis, band-limited noise estimation, and feature extraction in the Walsh basis.

</details>


### [364] [Efficient circuit compression by multi-qudit entangling gates in linear optical quantum computation](https://arxiv.org/abs/2602.08394)
*Apurav,Jaskaran Singh*

Main category: quant-ph

TL;DR: This paper solves a scalability bottleneck in linear optical quantum computation (LOQC) by introducing multi-level control-Z (CZ) gates for qudits. Two linear optical schemes are proposed: one achieves constant 1/8 success probability (beating prior 1/9) with state dependence, while the other eliminates state dependence but requires exponentially fewer non-local gates than existing methods when operating on qubit subsets.


<details>
  <summary>Details</summary>
Motivation: Qudit circuit compression in LOQC becomes inefficient when only a subset of encoded qubits requires entanglement, causing an exponential increase in non-local gates. This creates a fundamental scalability limitation for LOQC architectures.

Method: The authors design two explicit linear optical implementations of multi-level CZ gates that apply conditional phase shifts to arbitrary subsets of spatial modes. The first scheme uses one non-local gate with constant success probability (1/8) but exhibits state dependence. The second scheme achieves full state independence by optimizing resource allocation between control qubits (r₁, r₂) in the qudits.

Result: (1) First scheme: Constant success probability of 1/8 (vs. prior 1/9) with state dependence. (2) Second scheme: Reduces non-local gates from O(2^(r₁+r₂)) to O(2^r₁ + 2^r₂), though success probability decreases as (1/2)×(1/8)^(2^r₁+2^r₂). Both schemes outperform existing methods in gate efficiency for subset operations.

Conclusion: Multi-level CZ gates overcome a critical scalability barrier in LOQC. When integrated with qudit compression, these schemes significantly enhance the efficiency of LOQC architectures by reducing the exponential overhead in non-local entangling operations for partial qubit subsets.

Abstract: Linear optical quantum computation (LOQC) offers a promising platform for scalable quantum information processing, but its scalability is fundamentally constrained by the probabilistic nature of non-local entangling gates. Qudit circuit compression schemes mitigate this issue by encoding multiple qubits onto qudits. However, these schemes become inefficient when only a subset of the encoded qubits is required to participate in the non-local entangling gate, leading to an exponential increase in the number of non-local gates. In this Letter, we address this bottleneck by demonstrating the existence of multi-level control-Z (CZ) gates for qudits encoded in multiple spatial modes in LOQC. Unlike conventional two-level CZ gates, which act only on a single pair of modes, multi-level CZ gates impart a conditional phase shift for an arbitrarily chosen subset of the spatial modes. We present two explicit linear optical schemes that realize such operations, illustrating a fundamental trade-off between prior information about the input quantum state and the physical resources required. The first scheme is realized with a constant success probability of $1/8$ independent of the qudit dimension using a single non-local entangling gate, at the cost of state dependence, which is significantly better than the current success probability of $1/9$. Our second scheme provides a fully state independent realization reducing the number of non-local gates to $\mathcal{O}(2^{r_1}+2^{r_2})$ as compared to the existing bound of $\mathcal{O}(2^{r_1+r_2})$ where $r_1$ and $r_2$ are the number of qubits to be removed as control in the qudits. The success probability of the realization is $\frac{1}{2} \left(\frac{1}{8}\right)^{2^{r_1}+2^{r_2}}$. When combined with qudit circuit compression schemes, our results improve upon a key scalability limitation and significantly improve the efficiency of LOQC architectures.

</details>


### [365] [The Finite Geometry of Breaking Quantum Secrets](https://arxiv.org/abs/2602.08410)
*Péter Lévay,Metod Saniga*

Main category: quant-ph

TL;DR: 本文利用有限几何框架研究五边形和七边形量子码，通过分析稳定子群的张量分解结构，统一阐述了量子秘密共享与量子上下文性的关系，并基于(3,5)和(4,7)阈值方案导出了显式的秘密破解协议。


<details>
  <summary>Details</summary>
Motivation: 寻找量子秘密共享与量子上下文性的统一理论框架，从几何角度深入理解两者之间的内在联系，特别是纠缠与上下文性在秘密共享方案中的作用机制。

Method: 采用有限几何方法，系统研究五边形和七边形码稳定子群的2+3和3+4张量因子分解，分析对应的三量子比特和四量子比特嵌入的二进制辛极空间结构，揭示其中蕴含的上下文性与纠缠特性。

Result: 成功建立了稳定子群分解结构与量子秘密共享方案的对应关系，明确给出了(3,5)和(4,7)阈值方案的显式秘密破解协议，证明了特定几何构型如何调控秘密共享中的访问权限。

Conclusion: 该研究为理解量子上下文性提供了新颖的几何视角，表明有限几何结构能够自然地统一描述量子秘密共享与上下文性，为设计新型量子密码协议奠定了几何理论基础。

Abstract: Using a finite geometric framework for studying the pentagon and heptagon codes we show that the concepts of quantum secret sharing and contextuality can be studied in a nice and unified manner. The basic idea is a careful study of the respective $2+3$ and $3+4$ tensorial factorizations of the elements of the stabilizer groups of these codes. It is demonstrated in detail how finite geometric structures entailing a specific three-qubit (resp. four-qubit) embedding of binary symplectic polar spaces of rank two (resp. three), corresponding to these factorizations, govern issues of contextuality and entanglement needed for a geometric understanding of quantum secret sharing. Using these results for the $(3,5)$ and $(4,7)$ threshold schemes explicit secret breaking protocols are derived. Our results hint at a novel geometric way of looking at contextual configurations.

</details>


### [366] [Grover Adaptive Search with Problem-Specific State Preparation](https://arxiv.org/abs/2602.08418)
*Maximilian Hess,Lilly Palackal,Abhishek Awasthi,Peter J. Eder,Manuel Schnaus,Laurin Demmler,Karen Wintersperger,Joseph Doetsch*

Main category: quant-ph

TL;DR: 提出针对旅行商问题(TSP)的启发式量子态制备方法，通过模拟经典Lin-Kernighan启发式策略，在多项式次数的格罗弗迭代中实现合理近似比，并优化未知解数量时的终止条件与迭代策略


<details>
  <summary>Details</summary>
Motivation: 格罗弗算法在组合优化问题中面临指数级搜索空间挑战，单纯二次加速不足，需结合问题结构设计态制备例程以提升有希望解的振幅

Method: 基于Baertschi和Eidenbenz的前期工作，构建模仿经典Lin-Kernighan启发式的量子态制备例程，针对TSP设计启发式振幅放大策略

Result: 实现多项式次数格罗弗迭代下的合理近似比；对比分析未知标记解数量时的终止准则与迭代选择等关键算法设置

Conclusion: 证实通过启发式态制备可将格罗弗算法有效应用于TSP等组合优化问题，为量子算法处理复杂优化问题提供可行路径

Abstract: Grover's search algorithm is one of the basic building block in the world of quantum algorithms. Successfully applying it to combinatorial optimization problems is a subtle challenge. As a quadratic speedup is not enough to naively search an exponentially large space, the search has to be complemented with a state preparation routine which increases the amplitudes of promising states by exploiting the problem structure. In this paper, we build upon previous work by Baertschi and Eidenbenz to construct heuristic state preparation routines for the Traveling Salesperson Problem (TSP), mimicking the well-known classical Lin-Kernighan heuristic. With our heuristic, we aim to achieve a reasonable approximation ratio with only a polynomial number of Grover iterations. Further, we compare several algorithmic settings relating to termination criteria and the choice of Grover iterations when the number of marked solutions is unknown.

</details>


### [367] [Non-Markovianity induced by Pauli-twirling](https://arxiv.org/abs/2602.08464)
*Joris Kattemölle,Balázs Gulácsi,Guido Burkard*

Main category: quant-ph

TL;DR: The paper demonstrates that Pauli twirling can induce non-Markovianity in quantum channels, necessitating negative Pauli-Lindblad parameters for accurate noise modeling—contrary to common assumptions in error mitigation protocols.


<details>
  <summary>Details</summary>
Motivation: Noise is a major obstacle in quantum computing. Pauli twirling is widely used to simplify noise characterization and enable error mitigation by converting arbitrary noise into Pauli channels. However, the standard practice of restricting Pauli-Lindblad parameters to be nonnegative (assuming Markovianity) may be invalid in realistic experimental scenarios.

Method: The authors study how Pauli twirling affects Markovianity by analyzing the Markovianity of individual channels (rather than semigroups). They prove a general theorem: a Pauli channel is non-Markovian if and only if at least one of its Pauli-Lindblad parameters is negative. They apply this to examples like the √X-gate under Markovian noise.

Result: Markovian quantum channels often become non-Markovian after Pauli twirling. The Pauli-twirling induced non-Markovianity requires the use of negative Pauli-Lindblad parameters for correct noise description in experimentally realistic settings.

Conclusion: Common assumptions of nonnegative Pauli-Lindblad parameters are frequently invalid. Accurate noise characterization for quantum error mitigation must account for possible negative parameters induced by Pauli twirling, with direct implications for protocol design and reliability.

Abstract: Noise forms a central obstacle to effective quantum information processing. Recent experimental advances have enabled the tailoring of noise properties through Pauli twirling, transforming arbitrary noise channels into Pauli channels. This underpins theoretical descriptions of fault-tolerant quantum computation and forms an essential tool in noise characterization and error mitigation. Pauli-Lindblad channels have been introduced to aptly parameterize quasi-local Pauli errors across a quantum register, excluding negative Pauli-Lindblad parameters relying on the Markovianity of the underlying noise processes. We point out that caution is required when parameterizing channels as Pauli-Lindblad channels with nonnegative parameters. For this, we study the effects of Pauli twirling on Markovianity. We use the notion of Markovianity of a channel (rather than that of an entire semigroup) and prove a general Pauli channel is non-Markovian if and only if at least one of its Pauli-Lindblad parameters is negative. Using this, we show that Markovian quantum channels often become non-Markovian after Pauli twirling. The Pauli-twirling induced non-Markovianity necessitates the use of negative Pauli-Lindblad parameters for a correct noise description in experimentally realistic scenarios. An important example is the implementation of the $\sqrt{X}$-gate under standard Markovian noise. As such, our results have direct implications for quantum error mitigation protocols that rely on accurate noise characterization.

</details>


### [368] [Classifying the simplest Bell inequalities beyond qubits and their applications towards self-testing](https://arxiv.org/abs/2602.08469)
*Palash Pandya,Shubhayan Sarkar,Remigiusz Augusiak*

Main category: quant-ph

TL;DR: Characterized all Bell inequalities for (2,2,3) quantum scenario using sum-of-squares decomposition, showing maximal violation by qutrit entangled states and enabling state/measurement self-testing.


<details>
  <summary>Details</summary>
Motivation: Extend understanding of Bell inequalities beyond basic (2,2,2) scenarios to higher-dimensional outcomes for geometric characterization of quantum nonlocality and tailored quantum information applications.

Method: Analyzed (2,2,3) Bell scenario using sum-of-squares decomposition to derive complete set of Bell inequalities maximally violated by 3D maximally entangled states.

Result: Identified all Bell inequalities in (2,2,3) scenario, confirmed maximal violation by qutrit states, and demonstrated self-testing of both states and three-outcome measurements.

Conclusion: Provides foundational framework for characterizing multi-outcome quantum nonlocality, enabling device-independent protocols and deeper insights into high-dimensional quantum correlations.

Abstract: Bell inequalities reveal the fundamentally nonlocal character of quantum mechanics. In this regard, one of the interesting problems is to explore all possible Bell inequalities that demonstrate a gap between local and nonlocal quantum behaviour. This is useful for the geometric characterisation of the set of nonlocal correlations achievable within quantum theory. Moreover, it provides a systematic way to construct Bell inequalities that are tailored to specific quantum information processing tasks. This characterisation is well understood in the simplest $(2,2,2)$ scenario, namely two parties performing two binary outcome measurements. However, beyond this setting, relatively few Bell inequalities are known, and the situation becomes particularly scarce in scenarios involving a greater number of outcomes. Here, we consider the $(2,2,3)$ scenario, or two parties performing two three-outcome measurements, and characterise all Bell inequalities that can arise from the simplest sum-of-squares decomposition and are maximally violated by the maximally entangled state of local dimension three. We then utilise them to self-test this state, along with a class of three-outcome measurements.

</details>


### [369] [Empirical Study of Observable Sets in Multiclass Quantum Classification](https://arxiv.org/abs/2602.08485)
*Paul San Sebastian,Mikel Cañizo,Roman Orus*

Main category: quant-ph

TL;DR: 该论文对比了两种原生多类别量子分类方法（可观测量期望值最大化与态保真度最大化），使用Pauli字符串和投影算子作为可观测量，并分析其对贫瘠高原和神经坍缩现象的影响。


<details>
  <summary>Details</summary>
Motivation: 现有量子分类研究多集中于二分类或二分类集成方法（如一对多策略），而现有原生多类别模型缺乏对所选观测量的合理性论证，存在模型设计原理上的空白，需要系统性的比较研究。

Method: 研究者实现了两种分类准则（最大化类代表观测量的期望值 vs. 最大化编码量子态与类参考态的保真度），分别采用Pauli字符串集合和计算基投影算子作为可观测量的量子机器学习模型，通过实证分析比较其性能，并考察了贫瘠高原和神经坍缩现象。

Result: 研究揭示了不同可观测量选择如何影响量子机器学习模型的训练性能，以及这些选择如何与贫瘠高原和神经坍缩等关键训练难题相互作用，为模型设计提供了经验性发现。

Conclusion: 该工作为未来多类别量子机器学习模型的设计提供了重要指导，帮助研究者理解不同分类准则与可观测量选择之间的权衡，从而构建更有效的量子分类器。

Abstract: Variational quantum algorithms have gained attention as early applications of quantum computers for learning tasks. In the context of supervised learning, most of the works that tackle classification problems with parameterized quantum circuits constrain their scope to the setting of binary classification or perform multiclass classification via ensembles of binary classifiers (strategies such as one versus rest). Those few works that propose native multiclass models, however, do not justify the choice of observables that perform the classification. This work studies two main classification criteria in multiclass quantum machine learning: maximizing the expected value of an observable representing a class or maximizing the fidelity of the encoded quantum state with a reference state representing a class. To compare both approaches, sets of Pauli strings and sets of projectors into the computational basis are chosen as observables in the quantum machine learning models. Observing the empirical behavior of each model type, the effect of different observable set choices on the performance of quantum machine learning models is analyzed in the context of Barren Plateaus and Neural Collapse. The results provide insights that may guide the design of future multiclass quantum machine learning models.

</details>


### [370] [Intelligent Control of Collisional Architectures for Deterministic Multipartite State Engineering](https://arxiv.org/abs/2602.08526)
*Duc-Kha Vu,Minh Tam Nguyen,Özgür E. Müstecaplıoğlu,Fatih Ozaydin*

Main category: quant-ph

TL;DR: 该论文提出了一种智能约束感知控制框架，通过优化碰撞强度参数，在重复相互作用架构中自动合成用于确定性地生成对称Dicke态的控制协议。该方法使用辅助"穿梭"量子比特介导的部分SWAP碰撞，通过L-BFGS-B算法求解两参数优化问题，在存在随机碰撞丢失和退相干等实际约束下，实现了宽误差范围内的高保真度Dicek态制备，且无需投影测量。


<details>
  <summary>Details</summary>
Motivation: 多体纠缠的可扩展、噪声容忍控制协议设计是量子技术的核心挑战。现有方法依赖于手工设计的门序列，而非算法化合成相互作用参数。

Method: 在重复相互作用（碰撞模型）架构中，将Dicke态制备建模为闭环设计问题：给定目标(n,m)，自动推断碰撞强度。具体采用激发保持的部分SWAP碰撞连接两个不相交量子比特寄存器，并由m个辅助"穿梭"量子比特介导。通过多启动策略和L-BFGS-B算法求解两参数（内部寄存器碰撞角γ_in和穿梭-寄存器碰撞角γ_sh）的边界约束优化问题。

Result: 1) 获得了可重复的控制器处方（优化的γ_in、γ_sh和最小轮次收敛点）；2) 摆脱了对投影测量的需求；3) 将碰撞纠缠生成从单激发（W态）扩展到任意m；4) 在宽误差范围内保持高保真度，缺陷主要表现为碰撞轮次需求的适度增加；5) 实现了噪声与碰撞相关性的可调竞争。

Conclusion: 该控制框架通过适当选择碰撞参数，使系统能够在噪声抑制相关性时持续补充相关性，从而实现时间换保真度的权衡，为实际量子技术中的多体纠缠制备提供了鲁棒、可扩展的解决方案。

Abstract: Designing scalable, noise-tolerant control protocols for multipartite entanglement is a central challenge for quantum technologies, and it naturally calls for \emph{algorithmic} synthesis of interaction parameters rather than handcrafted gate sequences. Here we introduce an intelligent, constraint-aware control framework for deterministic generation of symmetric Dicke states $|D_n^{(m)}\rangle$ in repeated-interaction (collision-model) architectures. The protocol employs excitation-preserving partial-SWAP collisions between two disjoint qubit registers, mediated by $m$ ancillary ``shuttle'' qubits, and poses Dicke-state preparation as a \emph{closed-loop design} problem: given the target $(n,m)$, automatically infer collision strengths that maximize fidelity under practical constraints. Concretely, we formulate a two-parameter, bound-constrained optimization over intra-register and shuttle--register collision angles and solve it using a multi-start strategy with L-BFGS-B, yielding a reproducible controller prescription (optimized $γ_{\mathrm{in}}$, $γ_{\mathrm{sh}}$, and minimal-round convergence points) for each target. This removes the need for projective measurements and extends collisional entanglement generation beyond the single-excitation (W-state) sector to arbitrary $m$. Crucially, we optimize \emph{within} imperfect collisional dynamics where errors act throughout the sequence, including stochastic interaction dropouts (missing collisions) and standard decoherence channels. Strikingly, across wide error ranges the optimized controller preserves high preparation fidelity; imperfections manifest primarily as a modest increase in the required number of collision rounds. This behavior reflects a tunable competition in which noise suppresses correlations while properly chosen collisions continuously replenish them, allowing the control algorithm to trade time for fidelity.

</details>


### [371] [Time resolution at the quantum limit of two incoherent sources based on frequency resolved two-photon-interference](https://arxiv.org/abs/2602.08578)
*Salvatore Muratore,Vincenzo Tamma*

Main category: quant-ph

TL;DR: 该论文利用双光子量子拍频效应，通过测量频率和符合计数，实现了对弱非相干信号时间延迟的测量，精度达到量子极限的一半，突破了经典瑞利判据的限制。


<details>
  <summary>Details</summary>
Motivation: 经典瑞利判据在空间分辨非相干光源时存在极限，同样的问题也存在于弱非相干信号的时间延迟估计中。该研究旨在利用量子效应超越这一经典限制。

Method: 通过分束器将参考光源与两个非相干弱信号的光子进行干涉，在频域产生双光子量子拍频现象，测量干涉光子的频率并检测光子聚束或反聚束效应来提取时间延迟信息。

Result: 仅用相对较少的频率测量次数，即可达到量子极限一半的精度，且该精度不依赖于光子波包的时间形状和待估计的时间延迟大小。

Conclusion: 该量子技术具有可行性，可广泛应用于天文学、显微镜、远程时钟同步和雷达测距等领域。

Abstract: The Rayleigh criterion is a widely known limit in the resolution of incoherent sources with classical measurements in the spatial domain. Unsurprisingly the estimation of the time delay between two weak incoherent signals is afflicted by an analogue problem. In this work, we show the emergence of two-photon quantum beats in the frequency domain from the interference at a beam splitter of a photon emitted by a reference source and one from the two incoherent weak signals. We demonstrate, based on this phenomena, that with a relatively low number of measurements of the frequencies of the interfering photons either bunching or antibunching at the beam splitter output one can achieve a precision amounting to half of the quantum limit, independently of both the temporal shape of the photonic wavepacket and the time delay to be estimated. The feasibility of the technique makes it applicable in astronomy, microscopy, remote clocks synchronization and radar ranging

</details>


### [372] [Quantum Charging Advantage in Superconducting Solid-State Batteries](https://arxiv.org/abs/2602.08610)
*Chang-Kang Hu,Chilong Liu,Jingchao Zhao,Liuzhu Zhong,Yuxuan Zhou,Mingze Liu,Haolan Yuan,Yongchang Lin,Yue Xu,Guantian Hu,Guixu Xie,Zixing Liu,Ruiyang Zhou,Yougui Ri,Wenxuan Zhang,Ruicheng Deng,Andreia Saguia,Xiayu Linpeng,Marcelo S. Sarandy,Song Liu,Alan C. Santos,Dian Tan,Dapeng Yu*

Main category: quant-ph

TL;DR: 在超导量子处理器中，利用简单的最近邻相互作用，实验验证了2-12个量子电池单元可实现的量子充电优势。


<details>
  <summary>Details</summary>
Motivation: 探索在可扩展的固态系统中实现量子电池的充电优势，避免复杂的长程或多体相互作用，为未来量子技术提供高效的能量存储方案。

Method: 采用超导量子比特链，通过双激发哈密顿量实现集体演化，在仅有最近邻和成对相互作用的条件下，测试2至12个电池单元的量子充电性能。

Result: 实验观测到显著的量子充电优势，并测量到非零的相干功值、非相干功值和纠缠，证实了充电过程的量子特性。

Conclusion: 该研究表明基于简单相互作用的量子电池即可实现高效的量子充电优势，为开发实验可行的量子能量存储协议提供了重要前景。

Abstract: Quantum battery, as a novel energy storage device, offers the potential for unprecedented efficiency and performance beyond the capabilities of classical systems, with broad implications for future quantum technologies. Here, we experimentally \RefC{demonstrate quantum charging advantage (QCA)} in a scalable solid-state quantum battery. More specifically, we show how double-excitation Hamiltonians for two-level systems promote scalable QCA \RefB{with standard methods.} We effectively implement the collective evolution of quantum systems with 2 up to 12 battery cells in a superconducting quantum processor, and study the performance of quantum charging compared to its uncorrelated classical counterpart. The model considered is a linear chain of superconducting transmon qubits with only \textit{nearest-neighbor} and \textit{pairwise} interactions, which constitute the simplest model of a multi-cell quantum battery. Our results empirically realize substantial QCA without the necessity of adopting long-range and many-body interactions \RefB{ and showcase the quantum features of the QB charging processes with measurements of non-zero coherent ergotropy, incoherent ergotropy and entanglement,} revealing a promising prospect for further developments of efficient and experimentally feasible protocols for QCA.

</details>


### [373] [Representation theory of inhomogeneous Gaussian unitaries](https://arxiv.org/abs/2602.08611)
*Jingqi Sun,Joshua Combes,Lucas Hackl*

Main category: quant-ph

TL;DR: This paper extends the parameterization framework for homogeneous Gaussian unitaries to the inhomogeneous case (including linear terms), using (M,z,Ψ) parameters, and derives the group multiplication law by factoring unitaries into squeezing and displacement operations via the Baker-Campbell-Hausdorff formula.


<details>
  <summary>Details</summary>
Motivation: Previous work resolved homogeneous (quadratic-only) Gaussian unitaries, but physical implementations require handling inhomogeneous cases (with linear terms) and addressing phase/sign ambiguities from double covers of symmetry groups, necessitating a complete parameterization and group structure analysis.

Method: Extends prior homogeneous parameterization to inhomogeneous unitaries via (M,z,Ψ) parameters; applies the Baker-Campbell-Hausdorff formula to decompose any Gaussian unitary into squeezing and displacement components; derives the group multiplication law from this factorization.

Result: Achieves a complete parameterization of inhomogeneous Gaussian unitaries and explicitly derives their group multiplication law, resolving phase/sign ambiguities inherent in physical realizations.

Conclusion: The framework fully characterizes Gaussian unitary groups (including inhomogeneous cases), providing essential tools for quantum optics and continuous-variable quantum computing by unifying parameterization and group structure.

Abstract: Gaussian unitaries, generated by quadratic Hamiltonians, are fundamental in quantum optics and continuous-variable computing. Their structures correspond to symplectic (bosons) and orthogonal (fermions) groups, but physical realizations give rise to their respective double covers, introducing phase and sign ambiguities. The homogeneous (quadratic-only) case has been resolved through a parameterization constructed in a recent work [arXiv:2409.11628]. We extend the previous framework to inhomogeneous Gaussian unitaries parameterized by $(M,z,Ψ)$. The Baker-Campbel-Hausdorff formula allows us then to factor any Gaussian unitary into a squeezing and a displacement transformation, from which we derive the group multiplication law.

</details>


### [374] [Heterogeneous Optically-Detected Spin-Acoustic Resonance in Solid-State Molecular Thin-film](https://arxiv.org/abs/2602.08772)
*Kuan-Cheng Chen,Yongqiang Wen,Xiaotian Xu,Max Attwood,Jingdong Xu,Chen Fu,Sami Ramadan,Shang Yu,Sandrine Heutz,Mark Oxborrow*

Main category: quant-ph

TL;DR: 该论文实现了五苯薄膜与高Q值声表面波谐振器的异质集成，展示了室温零磁场下的自旋-声共振现象，通过机械声波驱动实现三重态自旋相干操控。


<details>
  <summary>Details</summary>
Motivation: 开发无需外磁场的室温自旋操控技术，推动自旋电子器件与微纳机电系统的异质集成。

Method: 采用异质光学检测自旋-声共振（HODSAR）技术，利用五苯的光激发三重态特性，通过锂铌酸盐衬底上的高Q声表面波谐振器传递机械驱动。

Result: 在零磁场下观测到105 MHz附近的声驱动自旋共振，通过拉比振荡证实相干控制，拉比频率与射频功率平方根呈线性关系，符合双能级动力学模型。

Conclusion: 首次在异质集成的分子薄膜平台中建立自旋-声共振体系，为零磁场、室温机械式自旋调控提供了量化基准和新实现路径。

Abstract: We report an implementation of spin-acoustic resonance in pentacene thin films integrated on a high-quality-factor (high-Q) surface acoustic wave (SAW) resonator on a lithium niobate substrate. Heterogeneous optically detected spin-acoustic resonance (HODSAR) is an optically detected spin-resonance measurement in which the resonant drive is delivered mechanically by a surface acoustic wave (SAW). By leveraging the photo-excited triplet state of pentacene at room temperature, we demonstrate coherent spin manipulation via acoustic driving under zero externally applied magnetic field. The heterogeneously integrated device, referred to as HODSAR, utilizes spin-phonon coupling to achieve mechanically driven, zero-field spin resonance, opening avenues for room-temperature mechanically addressable spin control and device integration. We show that the high-Q multimode response of the SAW resonator enables spectrally selective acoustic addressing of triplet transitions near 105 MHz. Coherent control is evidenced by Rabi oscillations, with a Rabi frequency that increases linearly with the square root of the applied RF input power over the measured drive range, consistent with driven two-level dynamics under acoustic excitation. These results establish spin-acoustic resonance in a heterogeneously integrated molecular thin-film platform and provide a quantitative basis for benchmarking mechanically mediated spin control.

</details>


### [375] [The equivalence of quantum deletion and insertion errors on permutation-invariant codes](https://arxiv.org/abs/2602.08780)
*Lewis Bulled,Yingkai Ouyang*

Main category: quant-ph

TL;DR: 首次建立量子插入删除错误的系统校正理论，解决长期存在的量子同步错误校正难题


<details>
  <summary>Details</summary>
Motivation: 经典同步错误校正理论已成熟（含50年前插入-删除等价性），但量子领域长期缺乏对应研究，量子同步错误校正存在根本性理论空白

Method: 聚焦置换不变码，建立量子插入-删除等价框架：1) 推导t次插入错误可校正的充要条件 2) 扩展至(t,s)-插入删除混合错误模型

Result: 1) 明确置换不变码可校正t次插入错误的条件 2) 提出(t,s)-混合错误更严格的可校正判据 3) 解决量子同步错误校正多个核心理论问题

Conclusion: 突破量子同步错误校正的理论瓶颈，为量子计算容错体系提供新基础，推动量子错误校正理论向完整化发展

Abstract: Quantum synchronisation errors are a class of quantum errors that change the number of qubits in a quantum system. The classical error correction of synchronisation errors has been well-studied, including an insertion-deletion equivalence more than a half-century ago, but little progress has been made towards the quantum counterpart since the birth of quantum error correction. We address the longstanding problem of a quantum insertion-deletion equivalence on permutation-invariant codes, detailing the conditions under which such codes are $t$-insertion error-correctable. We extend these conditions to quantum insdel errors, formulating a more restrictive set of conditions under which permutation-invariant codes are $(t,s)$-insdel error-correctable. Our work resolves many of the outstanding questions regarding the quantum error correction of synchronisation errors.

</details>


### [376] [High-Probability Heralded Entanglement via Repeated Spin-Photon Phase Encoding with Moderate Cooperativity](https://arxiv.org/abs/2602.08834)
*Yu Liu,Martin B. Plenio*

Main category: quant-ph

TL;DR: 提出一种光子循环利用方案，在中等协同性(C~1)的自旋-腔系统中实现高保真度远程纠缠，为分布式量子计算提供新路径


<details>
  <summary>Details</summary>
Motivation: 传统单次接口中受限的协同性严重抑制了成功概率，需要解决中等协同性条件下远程纠缠生成效率低下的问题

Method: 通过循环利用单个入射光子与自旋-腔寄存器进行重复相互作用，累积自旋条件相位偏移，采用谱宽缩放光子脉冲提高编码效率

Result: 在现实损耗条件下，即使协同性C~1时也能产生高保真度纠缠态，并实现可观的成功概率

Conclusion: 该协议特别适用于弱耦合腔基固态自旋平台，为实现光子损耗容忍的混合分布式量子计算提供了可行方案

Abstract: We propose a heralded high-probability scheme to generate remote entanglement between moderate-cooperativity spin-cavity registers with high fidelity. In conventional single-shot interfaces, limited cooperativity restricts the spin-conditional optical response and thus strongly suppresses the success probability. Our proposal instead recycles a single incident photon for repeated interactions with the spin-cavity register, such that a small spin-conditional phase shift acquired on each round trip accumulates coherently to enable remote entanglement. Moreover, the repeated scheme enables higher spin-photon encoding efficiency by using a spectral-width-scaling photon pulse with a shorter duration. We show that, for realistic imperfections and losses, this repeated phase-encoding approach produces high-fidelity entangled states with an appreciable success probability even at cooperativity $C\sim1$. Our protocol is particularly well suited to weakly coupled, cavity-based solid-state spin platforms and provides a route toward hybrid, photon-loss-tolerant distributed quantum computing.

</details>


### [377] [A cavity-mediated reconfigurable coupling scheme for superconducting qubits](https://arxiv.org/abs/2602.08869)
*Shinyoung Hwang,Sangyeon Lee,Eunjong Kim*

Main category: quant-ph

TL;DR: 该论文提出了一种腔介导的可调耦合架构，通过共享腔模和可调 qubit-cavity 耦合器，实现超导量子比特间动态可重构的非邻近相互作用，为复杂量子电路提供了灵活的连接方式。


<details>
  <summary>Details</summary>
Motivation: 超导量子比特虽然门保真度和相干性取得了显著进展，但其典型的最近邻连通性限制了复杂量子电路的实现，需要突破连通性约束。

Method: 引入一种腔介导耦合架构，通过可调 qubit-cavity 耦合器访问共享腔模，实现非邻近量子比特间动态可重构的相互作用。通过选择性激活耦合器来执行量子门操作。

Result: 在 50 ns 内实现了高保真度的 iSWAP 和 CZ 门，模拟相干误差低于 10⁻⁴；空闲时残余 ZZ 相互作用低于几千赫兹。在四个量子比特系统中，通过选择性启用耦合器模拟了所有量子比特对之间的门操作，且串扰较低。

Conclusion: 该方法为超导量子处理器提供了增强的相互作用灵活性，可作为需要选择性非局域耦合器件的实用构建模块，有助于实现更复杂的量子电路。

Abstract: Superconducting qubits have achieved remarkable progress in gate fidelity and coherence, yet their typical nearest-neighbor connectivity presents constraints for implementing complex quantum circuits. Here, we introduce a cavity-mediated coupling architecture in which a shared cavity mode, accessed through tunable qubit-cavity couplers, enables dynamically reconfigurable interactions between non-adjacent qubits. By selectively activating the couplers, we demonstrate that high-fidelity iSWAP and CZ gates can be performed within 50 ns with simulated coherent error below $10^{-4}$, while residual $ZZ$ interaction during idling remains below a few kilohertz. Extending to a four-qubit system, we also simulate gates between every qubit pair by selectively enabling the couplers with low qubit crosstalk. This approach provides a practical route toward enhanced interaction flexibility in superconducting quantum processors and may serve as a useful building block for devices that benefit from selective non-local coupling.

</details>


### [378] [Differentiable Logical Programming for Quantum Circuit Discovery and Optimization](https://arxiv.org/abs/2602.08880)
*Antonin Sulc*

Main category: quant-ph

TL;DR: A neuro-symbolic framework for quantum circuit design that treats gate selection as differentiable logic programming with learnable continuous switches optimized by gradient descent to satisfy logical axioms, successfully discovering 4-qubit QFT and improving IBM processor fidelity by 59.3 percentage points.


<details>
  <summary>Details</summary>
Motivation: Current quantum circuit design methods rely on heuristic, fixed-ansatz structures or rule-based compilers that are often suboptimal and lack generality, making high-fidelity circuit design challenging.

Method: Reframes circuit design as differentiable logic programming: represents candidate gates as continuous switches s ∈ [0,1]^N optimized via gradient descent to satisfy user-defined logical axioms (correctness, simplicity, robustness), with theoretical bridging of continuous logic (T-norms) and unitary evolution (geodesic interpolation), using biased initialization to prevent barren plateaus.

Result: Successfully discovered a 4-qubit QFT from 21 candidate gates; on IBM's 133-qubit Torino processor, improved fidelity by 59.3 percentage points in localized routing while adapting to hardware failures.

Conclusion: This neuro-symbolic approach provides a general, differentiable method for automatic quantum circuit discovery and hardware-aware optimization, overcoming limitations of traditional heuristic methods.

Abstract: Designing high-fidelity quantum circuits remains challenging, and current paradigms often depend on heuristic, fixed-ansatz structures or rule-based compilers that can be suboptimal or lack generality. We introduce a neuro-symbolic framework that reframes quantum circuit design as a differentiable logic programming problem. Our model represents a scaffold of potential quantum gates and parameterized operations as a set of learnable, continuous ``truth values'' or ``switches,'' $s \in [0, 1]^N$. These switches are optimized via standard gradient descent to satisfy a user-defined set of differentiable, logical axioms (e.g., correctness, simplicity, robustness). We provide a theoretical formulation bridging continuous logic (via T-norms) and unitary evolution (via geodesic interpolation), while addressing the barren plateau problem through biased initialization. We illustrate the approach on tasks including discovery of a 4-qubit Quantum Fourier Transform (QFT) from a scaffold of 21 candidate gates. We also report a hardware-aware adaptation experiment on the 133-qubit IBM Torino processor, where the method improved fidelity by 59.3 percentage points in a localized routing task while adapting to hardware failures.

</details>


### [379] [Error compensation without a time penalty: robust spin-lock-induced crossing in solution NMR](https://arxiv.org/abs/2602.08883)
*Mohamed Sabba,Christian Bengs,Urvashi D. Heramun,Malcolm H. Levitt*

Main category: quant-ph

TL;DR: 提出一种改进的核磁共振自旋锁定诱导交叉(SLIC)方法(cSLIC)，通过重复序列中采用两种不同射频场振幅，有效补偿射频场偏差且不增加序列总时长，适用于强耦合核自旋系统如单重态和仲氢超极化NMR实验。


<details>
  <summary>Details</summary>
Motivation: 针对强耦合核自旋系统(如单重态NMR和仲氢超极化NMR)中传统SLIC方法对射频场振幅偏差敏感的问题，需要在不增加序列时长的条件下提高补偿效果。

Method: 提出补偿型SLIC(cSLIC)方案：在重复序列单元中交替使用两种不同射频场振幅，通过序列设计实现射频场振幅偏差的有效补偿，同时保持原始SLIC序列的总时长不变。

Result: 数值仿真和代表性实验证实cSLIC能有效补偿射频场振幅偏差，在强耦合核自旋系统NMR实验中展现出优越性能。

Conclusion: cSLIC方案成功解决了传统SLIC方法对射频场振幅偏差敏感的问题，在保持时间效率的同时提高了强耦合NMR实验的准确性和适用性，为相关研究提供了更可靠的技术手段。

Abstract: A modification of the widely-used spin-lock-induced crossing (SLIC) procedure is proposed for the solution nuclear magnetic resonance (NMR) of strongly coupled nuclear spin systems, including singlet NMR and parahydrogen-enhanced hyperpolarised NMR experiments. The compensated-SLIC (cSLIC) scheme uses a repetitive sequence where the repeated element employs two different radiofrequency field amplitudes. Effective compensation for deviations in the radiofrequency field amplitude is achieved without increasing the overall duration of the SLIC sequence. The advantageous properties of cSLIC are demonstrated by numerical simulations and by representative experiments.

</details>


### [380] [Multiplexed microwave resonators by frequency comb spectroscopy](https://arxiv.org/abs/2602.08890)
*Angelo Greco,Jukka-Pekka Kaikkonen,Luca Chirolli,Alberto Ronzani,Jorden Senior,Francesco Giazotto,Alessandro Crippa*

Main category: quant-ph

TL;DR: 利用超导量子干涉器件产生宽频微波频率梳，成功探测共面波导谐振器阵列，其性能与传统室温电子学方法相当，并支持多频复用技术。


<details>
  <summary>Details</summary>
Motivation: 共面波导谐振器是电路量子电动力学的核心元件。需要高效方法同时探测多个谐振器，而基于超导量子干涉器件的频率梳技术有望实现宽带光谱分析和频率复用。

Method: 使用超导量子干涉器件(SQUID)在时变磁场驱动下产生微波频率梳，探测与公共传输线电感耦合的谐振器阵列；比较传统室温电子学合成信号与低温环境辐射的性能；采用双频驱动产生非均匀间隔谐振频率。

Result: 低温频率梳源在估计谐振器品质因数方面与室温传统电子学方法表现相当；双频驱动可实现多谐振器同时寻址；讨论了实现给定带宽有效光谱覆盖的标准。

Conclusion: 基于SQUID的超导频率梳技术是探测微波谐振器的有效工具，不仅能匹配传统方法的精度，更重要的是实现了频率复用能力，为多谐振器系统提供了紧凑高效的解决方案。

Abstract: Coplanar waveguide resonators are central to the thriving field of circuit quantum electrodynamics. Recently, we have demonstrated the generation of a broadband microwave-frequency comb spectrum using a superconducting quantum interference device (SQUID) driven by a time-dependent magnetic field. Here, the frequency comb is used to spectroscopically probe a bank of coplanar microwave resonators, inductively coupled to a common transmission line, a standard circuit with a variety of applications. We compare the resonator line shape obtained from signals synthesized at room temperature using conventional electronics with the radiation produced in the cryogenic environment by our source, showing substantial equivalence in the estimation of the resonator quality factors. To measure non-uniformly spaced resonant frequencies, we drive the generator with a bi-chromatic tone to generate intermodulation products. Such a dense frequency comb spectrum enables simultaneous addressing of a few resonators via frequency multiplexing. Finally, we discuss the criteria for achieving effective spectroscopic coverage of a given frequency bandwidth.

</details>


### [381] [Long distance quantum illumination and ranging using polarization entangled photon pairs in a lossy environment](https://arxiv.org/abs/2602.08947)
*Sujai Matta,Soumya Asokan,Sanchari Chakraborti,Mayank Joshi,Rahul Dalal,C. M. Chandrashekar*

Main category: quant-ph

TL;DR: Demonstrated kilometer-scale quantum illumination using polarization-entangled photons, maintaining strong entanglement (S>2.6) despite severe photon loss in free-space channels, enabling practical quantum-enhanced object detection.


<details>
  <summary>Details</summary>
Motivation: To overcome the fragility of quantum states in real-world lossy environments and establish a robust method for quantum-assisted object detection and ranging over long distances.

Method: Generated high-visibility polarization-entangled photon pairs via Sagnac interferometer (CHSH S=2.802), sent one photon through 1km free-space channel as probe while retaining the idler, measured correlations upon photon return.

Result: Observed strong quantum correlations (S>2.6) even with minimal photon return (tens of probe photons), proving entanglement survives kilometer-scale free-space propagation and scattering.

Conclusion: Polarization entanglement encoding is robust against real-world losses, validating a practical foundation for scalable quantum illumination and ranging systems in lossy environments.

Abstract: Using polarization entangled photon pairs, we demonstrate a robust scheme for quantum illumination and ranging in a lossy environment. Entangled photon pairs are generated in a Sagnac interferometer configuration, yielding high-visibility two-photon polarization entanglement with a measured CHSH parameter of $S =2.802\pm0.002$. One of the photons from the entangled pair is retained as idler and the other one is directed into either of the two paths, namely reference and probe, of which probe is sent toward a distant object through a lossy free-space channel, and the reflected photons are collected after round-trip free-space propagation over distances approaching $1$ km. Remarkably, strong correlations are observed with CHSH values $S >2.6$ even when only a few tens of probe photons are returned, confirming the robustness of polarization entanglement under long-distance free-space propagation. This work reports the robustness of encoding photons in different basis before it is sent towards the object and recovery of polarization entanglement even after a kilometer-scale scattering from the objects, establishing a practical foundation for scalable quantum-assisted object detection and ranging.

</details>


### [382] [Cascaded Optomechanical Sensing for Small Signals](https://arxiv.org/abs/2602.08981)
*Marta Maria Marchese,Daniel Braun,Stefan Nimmrichter,Dennis Rätzel*

Main category: quant-ph

TL;DR: 提出一种经典光机械传感方案，通过激光相干累积N个腔体的相位变化，在不使用量子资源的情况下实现海森堡极限的弱力探测灵敏度，为精密测量提供稳健可行的技术路线。


<details>
  <summary>Details</summary>
Motivation: 传统海森堡极限灵敏度通常依赖纠缠等量子资源，但量子系统脆弱且实验实现困难。本研究旨在探索经典物理系统是否也能达到同等精度的传感性能。

Method: 设计了一种由N个光机械腔体组成的单向链式结构，利用激光束依次穿过各腔体，通过相干平均累积外部力作用下的相位偏移。

Result: 该全经典方案成功实现了通常仅与量子增强协议相关的灵敏度标度律，兼具鲁棒性和实验可行性。

Conclusion: 工作开辟了利用相干光-物质相互作用进行精密力传感的新途径，在大型强子对撞机引力场测量、暗物质探测和引力波探测等领域具有应用潜力。

Abstract: We propose a sensing scheme for detecting weak forces that achieves Heisenberg-limited sensitivity without relying on entanglement or other non-classical resources. Our scheme utilizes coherent averaging across a chain of N optomechanical cavities, unidirectionally coupled via a laser beam. As the beam passes through the cavities, it accumulates phase shifts induced by a common external force acting on the mechanical elements. Remarkably, this fully classical approach achieves the sensitivity scaling typically associated with quantum-enhanced protocols, providing a robust and experimentally feasible route to precision sensing. Potential applications range from high-sensitivity gravitational field measurements at the Large Hadron Collider to probing dark matter interactions and detecting gravitational waves. This work opens a new pathway for leveraging coherent light-matter interactions for force sensing.

</details>


### [383] [Hybrid Method of Efficient Simulation of Physics Applications for a Quantum Computer](https://arxiv.org/abs/2602.09020)
*Carla Rieger,Albert T. Schmitz,Gehad Salem,Massimiliano Incudini,Sofia Vallecorsa,Anne Y. Matsuura,Michele Grossi,Gian Giacomo Guerreschi*

Main category: quant-ph

TL;DR: 提出量子化学模拟的混合仿真方法，通过Pauli帧优化多量子比特旋转操作，实现24量子比特体系18倍加速，集成至Intel Quantum SDK


<details>
  <summary>Details</summary>
Motivation: 量子化学因量子力学本质是展示量子优势的最有希望领域，但大规模量子电路模拟对确定量子解决方案超越经典方法的临界问题规模至关重要，尤其面临Trotter化哈密顿量演化中多量子比特旋转的计算挑战

Method: 开发全状态模拟器与Clifford模拟器的混合方法，利用Pauli帧高效模拟多量子比特旋转操作，优化量子电路仿真中多量子比特门的表示和执行

Result: 在24量子比特的化学哈密顿量模拟中实现约18倍加速（使用MPI后达22倍），显著降低计算成本

Conclusion: 该方法不仅提升量子化学模拟效率，还为依赖多量子比特旋转的计算任务提供通用优化策略，通过集成到Intel Quantum SDK推动量子算法从理论向实用软件转化

Abstract: Quantum chemistry and materials science are among the most promising areas for demonstrating algorithmic quantum advantage and quantum utility due to their inherent quantum mechanical nature. Still, large-scale simulations of quantum circuits are essential for determining the problem size at which quantum solutions outperform classical methods. In this work, we present a novel hybrid simulation approach, forming a hybrid of a fullstate and a Clifford simulator, specifically designed to address the computational challenges associated with the time evolution of quantum chemistry Hamiltonians. Our method focuses on the efficient emulation of multi-qubit rotations, a critical component of Trotterized Hamiltonian evolution. By optimizing the representation and execution of multi-qubit operations leveraging the Pauli frame, our approach significantly reduces the computational cost of simulating quantum circuits, enabling more efficient simulations. Beyond its impact on chemistry applications, our emulation strategy has broad implications for any computational workload that relies heavily on multi-qubit rotations. By increasing the efficiency of quantum simulations, our method facilitates more accurate and cost-effective studies of complex quantum systems. We quantify the performance improvements and computational savings for this emulation strategy, and we obtain a speedup of a factor $\approx 18$ ($\approx 22$ with MPI) for our evaluated chemistry Hamiltonians with 24 qubits. Thus, we evaluate our integration of this emulation strategy into the Intel Quantum SDK, further bridging the gap between theoretical algorithm development and practical quantum software implementations.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [384] [Non-reciprocal spin excitations across the skyrmion-paramagnetic phase transition in MnSi](https://arxiv.org/abs/2602.07121)
*Tobias Weber,Karin Schmalzl,Johannes Waizner,Andreas Bauer,Markus Garst,Christian Pfleiderer*

Main category: cond-mat.str-el

TL;DR: MnSi磁性斯格明子晶格的激发具有非互易单向传播特性，中子散射实验发现该特性在相变温度以上仍持续存在，与线性自旋波理论一致。


<details>
  <summary>Details</summary>
Motivation: 探究手性磁体MnSi中斯格明子晶格在相变点附近的磁激发演化行为，特别是非互易激发模式如何从斯格明子相过渡到高温顺磁态。

Method: 采用非弹性中子散射技术（三轴谱仪测量），通过分辨率卷积方法分析接近及高于临界温度时的准弹性信号。

Result: 1) 激发模式在斯格明子-顺磁相变边界连续变化；2) 施加磁场下顺磁态的准弹性信号仍保持斯格明子相的非互易特性；3) 实验结果与线性自旋波理论预测相符。

Conclusion: 非互易磁激发特性不仅存在于斯格明子晶格相，其本质特征在越过临界温度后仍被保留，表明该性质可能源于材料本征的手性磁结构。

Abstract: The magnetic excitations of the skyrmion lattice in MnSi comprise a multitude of individual modes, which are non-reciprocal and thereby propagate unidirectionally. We report inelastic neutron scattering experiments for temperatures near and above the skyrmion-paramagnetic phase transition in the chiral magnet MnSi tracking the evolution from the skyrmion lattice towards the high-temperature paramagnetic state. Within the resolution of the triple-axis measurements the excitations vary smoothly across the skyrmion-paramagnetic boundary, and, the quasi-elastic paramagnetic signal under applied field retains the non-reciprocal character seen in the skyrmion phase even far above the critical temperature. Using a resolution-convolution our results are consistent with linear spin-wave theory.

</details>


### [385] [Electronic Structure of Epitaxial Films of the Bilayer Strontium Ruthenate: Sr$_{3}$Ru$_2$O$_{7}$](https://arxiv.org/abs/2602.07747)
*Sethulakshmi Sajeev,Arnaud P. Nono Tchiomo,Brendon Faeth,Evan Krysko,Olivia Peek,Matthew J. Barone,Jordan Shields,Neha Wadehra,Garu Gebreyesus,Divine Kumah,Richard M. Martin,Darrell G. Schlom,Prosper Ngabonziza*

Main category: cond-mat.str-el

TL;DR: 首次结合角分辨光电子能谱和密度泛函理论，揭示衬底诱导应变对Sr3Ru2O7薄膜费米面拓扑和电子结构的调控机制，发现应变态差异导致截然不同的费米面特征及近费米能级平坦带，为磁不稳定性提供新证据。


<details>
  <summary>Details</summary>
Motivation: 探究外延生长中衬底诱导应变对Sr3Ru2O7薄膜低能电子能带结构的影响，特别是费米面拓扑变化与磁不稳定性的关联。

Method: 采用分子束外延技术在SrTiO3（STO）和LSAT衬底上生长外延Sr3Ru2O7薄膜；通过原位角分辨光电子能谱（ARPES）测量费米面形貌，结合密度泛函理论（DFT）计算分析应变效应。

Result: STO衬底上薄膜呈拉伸应变（四方对称性），LSAT衬底上呈压缩应变（正交对称性）；两种应变态下费米面拓扑发生显著变化；在费米能级下约15 meV范围内观察到沿Γ-X方向（正交相）和Γ点周围（四方相）的平坦带。

Conclusion: 衬底应变有效调控Sr3Ru2O7薄膜的电子结构，近费米能级的平坦带特征可能形成范霍夫奇点，从而增强磁不稳定性，为外延薄膜物性调控提供新思路。

Abstract: We report the first combined study of the low-energy electronic band structure of epitaxial Sr$_3$Ru$_2$O$_7$ films using angle-resolved photoemission spectroscopy (ARPES) and density functional theory (DFT). The complete Fermi-surface topography of the near-Fermi-level bands is determined from in-situ ARPES measurements. To investigate the effects of substrate-induced strain on the band structure, Sr$_3$Ru$_2$O$_7$ thin films are epitaxially grown on SrTiO$_3$ (STO) and (LaAlO$_{3}$)$_{0.3}$(Sr$_{2}$TaAlO$_{6}$)$_{0.7}$ (LSAT) substrates using molecular beam epitaxy. The combination of the measured Fermi-surfaces along with the theoretical interpretation, clearly show dramatic changes in the Fermi surface topologies that result from the underlying strain states of the films on the two substrates. We find that the Sr$_3$Ru$_2$O$_7$ films prepared on STO are tensile strained with tetragonal symmetry, whereas those grown on LSAT are compressively strained with orthorhombic symmetry. Within $\sim15~\text{meV}$ below the Fermi level, we observe two flat bands along $Γ$-$X$ in the orthorhombic phase and around $Γ$ in the tetragonal phase. These features could be favorable for van Hove singularities near the Fermi level, and highlight the emergence of magnetic instabilities in epitaxial Sr$_3$Ru$_2$O$_7$ films.

</details>


### [386] [La$_{2-x}$Ba$_x$CuO$_4$ ($x=\frac{1}{8}$) $μ$SR data are inconsistent with spin stripe but consistent with spin spiral](https://arxiv.org/abs/2602.07766)
*Oleg P. Sushkov*

Main category: cond-mat.str-el

TL;DR: 本文通过分析μSR数据，确定铜酸盐中的自旋序为面内螺旋序而非条纹序，螺旋平面与CuO₂平面重合，静态自旋矩降低至0.185。


<details>
  <summary>Details</summary>
Motivation: 通过分析μSR数据来确定铜酸盐超导体中的自旋序是条纹序还是面内螺旋序。

Method: 对现有μ子自旋旋转/弛豫（μSR）实验数据进行再分析。

Result: μSR数据与自旋条纹序不一致，但与面内螺旋序模型吻合；螺旋平面与CuO₂平面重合，静态自旋期望值为S = 0.37 × ½ = 0.185。

Conclusion: 所研究材料中的自旋序是面内螺旋序而非条纹序，且序参量被限制在CuO₂平面内，静态自旋矩有所降低。

Abstract: I analyze available $μ$SR data and show that it is inconsistent with the spin stripe but consistent with the coplanar spin spiral. The plane of the spiral coincides with the CuO$_2$-plane. The static expectation value of the spin is $S=0.37\times\frac{1}{2}$.

</details>


### [387] [Direct Evidence of a Near-Ideal Jeff = 1/2 Ground State in Triangular-Lattice Na2BaCo(PO4)2](https://arxiv.org/abs/2602.08361)
*M. M. Ferreira-Carvalho,S. H. Chen,Y. C. Ku,Anagha Jose,Ryan Morrow,C. Y. Kuo,C. F. Chang,Z. Hu,M. W. Haverkort,L. H. Tjeng*

Main category: cond-mat.str-el

TL;DR: 通过偏振依赖X射线吸收光谱和全多体簇计算，发现Na2BaCo(PO4)2中CoO6八面体存在极小有效三角畸变（仅11 meV），形成理想Jeff=1/2基态，为三角晶格上探索奇异磁现象提供新平台


<details>
  <summary>Details</summary>
Motivation: 探究Na2BaCo(PO4)2中局域Co 3d电子结构，特别是实现三角晶格上Jeff=1/2基态的条件，以探索相关奇异磁现象

Method: 结合偏振依赖X射线吸收光谱（XAS）与全多体簇计算，采用线拟合逆部分荧光产额（IPFY）技术获取强绝缘体准确光谱，并模拟不同晶向磁化率

Result: 发现CoO6八面体有效三角畸变极小（仅11 meV），接近理想Jeff=1/2基态条件；成功模拟各向异性磁化率

Conclusion: Na2BaCo(PO4)2是探索三角晶格上Jeff=1/2基态相关奇异磁现象（如量子自旋液体）的理想材料平台

Abstract: We investigated the local Co 3d electronic structure of Na2BaCo(PO4)2 using polarization-dependent X-ray absorption spectroscopy (XAS) in combination with full multiplet cluster calculations. We employed the line-fitting inverse partial fluorescence yield (IPFY) technique to obtain accurate XAS spectra from strong insulating materials. Our combined experimental and theoretical analysis reveals a very small effective trigonal distortion of only 11 meV in the CoO6 octahedra, indicating a close to ideal condition to render a ground state with the Jeff = 1/2 character. With our cluster model we were also able to simulate magnetic susceptibility measurements along different directions in the crystal. These findings highlight Na2BaCo(PO4)2 as a promising platform for exploring exotic magnetic phenomena associated with Jeff = 1/2 ground states on triangular lattices.

</details>


### [388] [Orientation-driven route to an intrinsic insulating ferromagnetic state in manganite superlattices](https://arxiv.org/abs/2602.08573)
*Priyanka Aggarwal,Kirill B. Agapev,Sagar Sarkar,Biplab Sanyal,Igor Di Marco,Fabrizio Cossu*

Main category: cond-mat.str-el

TL;DR: This paper predicts an insulating ferromagnetic state in (111)-oriented LaMnO₃/SrTiO₃ superlattices, arising from intrinsic structural and quantum effects rather than extrinsic factors, suggesting potential for next-generation spintronic applications.


<details>
  <summary>Details</summary>
Motivation: To engineer magnetic oxide heterostructures for modern electronics by exploring the functionalization of layered structures, specifically seeking an insulating ferromagnetic state through superlattice design.

Method: Theoretical prediction and electronic structure analysis of (111)-oriented LaMnO₃/SrTiO₃ superlattices, investigating the interplay of structural order, strain, and quantum confinement on their magnetic and electronic properties.

Result: The superlattices exhibit an insulating ferromagnetic state with narrow bands indicating localized e_g states. The bandgap can be direct or indirect depending on composition, and the systems behave as Kugel-Khomskii materials due to the interplay of lattice symmetry, Hubbard physics, and Hund's coupling.

Conclusion: This study provides a new route to achieve insulating ferromagnets and offers novel insights into the complex interactions between lattice and electronic properties, which can be exploited in future spintronic devices.

Abstract: Increasing precision in the growth of superlattices sparks hope in applications that may arise from engineering layered structures. Heterostructuring and functionalization of magnetic oxides have been very popular due to their versatility and readiness for integration in modern electronics. In this study, we provide yet another example of this phenomenology by predicting that an insulating ferromagnetic state can be realized in superlattices of LaMnO$_3$ and SrTiO$_3$ oriented along the (111) direction. In strike contrast with respect to other orientations, these properties are not of extrinsic origin but arise from the interplay of structural order, strain and quantum confinement. The bandgap is shown to be either direct and indirect, depending on the precise composition, which can be explained in terms of the geometrical properties of (111)-oriented bilayers of LaMnO$_3$. The electronic structure shows narrow bands indicating localized $e_g$ states for all the investigated superlattices. These features and the analysis of the inter-atomic magnetic coupling suggest that the investigated superlattices behave as a Kugel-Khomskii material, at least for the explored compositions. Our results provide not only a new route to an insulating ferromagnet, but also novel insight into the intricate interplay between lattice symmetry, Hubbard physics and Hund's coupling to be exploited in next-generation spintronic applications.

</details>


### [389] [Structural studies on $A_2$ReCl$_6$ ($A$=K, Rb, Cs): absence of Jahn-Teller distortion](https://arxiv.org/abs/2602.08665)
*A. Bertin,L. Kiefer,V. Pomjakushin,O. Fabelo,P. Becker,L. Bohaty,M. Braden*

Main category: cond-mat.str-el

TL;DR: 通过中子衍射和X射线晶体学实验，结合K₂SnCl₆参比样品及Rb/Cs同系物研究，证实K₂ReCl₆在低温下不存在Jahn-Teller畸变，其结构相变由八面体旋转/倾斜驱动而非Jahn-Teller效应


<details>
  <summary>Details</summary>
Motivation: 探究K₂ReCl₆在5d³电子构型下，强自旋轨道耦合极限是否会导致Jahn-Teller畸变，从而影响其高于磁有序温度(T_N=12K)的结构相变机制

Method: 1. 对比K₂ReCl₆(5d³)与无Jahn-Teller活性的参比样品K₂SnCl₆(4d¹⁰)<br>2. 采用粉末中子衍射和单晶X射线衍射分析晶体结构<br>3. 扩展研究Rb₂ReCl₆/Cs₂ReCl₆(碱金属离子半径更大，抑制结构相变)

Result: 1. ReCl₆八面体在中间温度存在畸变，但低温下畸变消失<br>2. K₂SnCl₆呈现相同结构相变序列但无Jahn-Teller效应<br>3. Rb/Cs同系物中未观察到Jahn-Teller畸变证据

Conclusion: K₂ReCl₆的结构相变完全由八面体旋转/倾斜机制主导，强自旋轨道耦合下的Jahn-Teller效应对其低温结构无实质影响，5d³电子态在低温下保持非简并纯自旋态

Abstract: K$_2$ReCl$_6$ belongs to the antifluorite family and exhibits a sequence of structural transitions above the onset of magnetic order at $T_N$ = 12 K. Because of its 5d3 electronic configuration in an octahedral coordination, the ground state is a pure spin state without orbital degeneracy within the LS coupling scheme, but it can become Jahn-Teller active in the strong spin-orbit coupling limit described by the $jj$ coupling [S. Streltsov and D. I. Khomskii, Phys. Rev. X 10, 031043 (2020)]. While the structural transitions in K$_2$ReCl$_6$ are understood in terms of octahedral rotation and tilting, the possible impact of a Jahn-Teller distortion remains an open issue. We report on comprehensive crystalstructure studies by means of powder neutron and single-crystal x-ray diffraction on K$_2$ReCl$_6$ and on K$_2$SnCl$_6$. The latter material is used as a reference, because it exhibits the same sequence of structural transitions as K$_2$ReCl$_6$, but possesses a filled 4d shell ruling out a Jahn-Teller distortion. While the ReCl$_6$ octahedron in K$_2$ReCl$_6$ presents sizable distortions at intermediate temperatures, there is no such distortion persisting to low temperatures excluding a sizable Jahn-Teller effect. Studies on polycrystalline samples of Rb$_2$ReCl$_6$ and Cs$_2$ReCl$_6$, in which the structural transitions are suppressed due to the larger alkaline ionic radius, also do not find any indications for a Jahn-Teller distortion.

</details>


### [390] [Frustrated spin models on two- and three-dimensional decorated lattices with high residual entropy](https://arxiv.org/abs/2602.08674)
*D. V. Dmitriev,V. Ya. Krivnov,O. A. Vasilyev*

Main category: cond-mat.str-el

TL;DR: Frustrated spin-1/2 Heisenberg models on star-decorated lattices exhibit macroscopic ground-state degeneracy via percolation mapping, yielding high residual entropy (>60% of ln(2)) for magnetocaloric applications.


<details>
  <summary>Details</summary>
Motivation: To design quantum magnets with engineered high ground-state degeneracy for enhanced magnetocaloric cooling and quantum thermal machines.

Method: Mapping the ideal star model's degenerate ground state onto a site percolation problem on the Lieb lattice to calculate exponential degeneracy and residual entropy across 2D/3D lattices.

Result: Exponential ground-state degeneracy with residual entropy exceeding 60% of maximal ln(2) for all lattices; distorted-star variant mimics non-interacting giant spins (s=4) with substantial entropy.

Conclusion: Provides a structural design principle for quantum magnets with high degeneracy, enabling advanced thermal management technologies via tunable frustrated lattice geometries.

Abstract: We study the ground-state properties of a family of frustrated spin-1/2 Heisenberg models on two- and three-dimensional decorated lattices composed of connected star-shaped units. Each star is built from edge-sharing triangles with an antiferromagnetic interaction on the shared side and ferromagnetic interactions on the others. At a critical coupling ratio, the ideal star model - defined by equal ferromagnetic interactions - exhibits a macroscopically degenerate ground state, which we map onto a site percolation problem on the Lieb lattice. This mapping enables the calculation of exponential ground-state degeneracy and the corresponding residual entropy for square, triangular, honeycomb, and cubic lattices. Remarkably, the residual entropy remains high for all studied lattices, exceeding 60\% of the maximal value ln(2). Despite a gapless quadratic one-magnon spectrum, the low-temperature thermodynamics is governed by exponentially numerous gapped excitations. For a distorted-star variant of the model, the ground-state manifold is equivalent to that of decoupled ferromagnetic clusters, leading to exponential degeneracy with a lower, yet still substantial, residual entropy. At low temperature the system mimics a paramagnetic crystal of non-interacting spins with high spin value ($s=4$ for a square lattice). The obtained results establish a structural design principle for engineering quantum magnets with a high ground-state degeneracy, suggesting promising candidates for enhanced magnetocaloric cooling and quantum thermal machines.

</details>


### [391] [Anisotropy, frustration and saddle point in the twisted Kagome antiferromagnet ErPdPb](https://arxiv.org/abs/2602.08900)
*Resham Babu Regmi,Sk Jamaluddin,Y. Lee,Hari Bhandari,Po-Hao Chang,Peter E. Siegfried,Abhijeet Nayak,Mohamed El Gazzah,Bence G. Márkus,Anna Nyáry,Zachary T. Messegee,Miya P. Zhao,Xiaoyan Tan,László Forró,Liqin Ke,Igor I. Mazin,Nirmal J. Ghimire*

Main category: cond-mat.str-el

TL;DR: 合成具有扭曲笼目晶格的ErPdPb单晶，发现其在2.2 K出现反铁磁序、c轴方向存在1/3 metamagnetic相变台阶、强磁各向异性（挫败指数达13.6）及低能拓扑电子结构，为研究几何挫败与稀土各向异性耦合体系提供了新平台


<details>
  <summary>Details</summary>
Motivation: 探索低对称性扭曲笼目晶格（尤其含稀土元素）中几何挫败与各向异性耦合产生的新奇磁性和拓扑物态，如自旋冰态、磁卡效应和非共线磁序

Method: 合成ZrNiAl型结构ErPdPb单晶；通过磁化率/输运/热容测量表征物性；结合DFT计算分析电子结构和磁各向异性

Result: 观测到2.2 K反铁磁序；c轴方向出现1/3饱和磁化的metamagnetic台阶；强磁各向异性（c轴挫败指数13.6）；热容在2.2 K有宽峰且低于0.5 K持续上升；DFT证实易轴各向异性与准一维能带

Conclusion: ErPdPb作为扭曲稀土笼目晶格模型体系，展现出强挫败、显著各向异性和拓扑电子特征，为研究关联与拓扑量子物态提供了理想载体

Abstract: The kagome lattice, with its inherent geometric frustration, provides a rich platform for exploring intriguing magnetic phenomena and topological electronic structures. In reduced-symmetry structures, such as twisted kagome systems involving rare earth elements, additional anisotropy can arise, enabling intriguing properties including spin-ice states, magnetocaloric effects, noncollinear magnetic ordering, and anomalous Hall effect. Here, we report the synthesis of single crystals of ErPdPb, which features a twisted kagome lattice net of Er atoms within the hexagonal ZrNiAl-type structure, and we investigate its magnetic, electronic, and thermal properties. The material exhibits antiferromagnetic ordering below 2.2 K, consistently observed in magnetic, transport, and heat capacity measurements. Magnetization measurements reveal 1/3 metamagnetic steps along the c-axis below the Néel temperature, suggesting an Ising-spin-like state on the twisted kagome lattice. A pronounced anisotropy between in-plane and out-of-plane resistivity is observed throughout the temperature range of 1.8-300 K, and the compound exhibits a significant frustration index of 13.6 (12.7) along the c-axis (ab-plane). Heat capacity measurements show a broad hump at 2.2 K, with an additional increase below 0.5 K. The anisotropic magnetic properties are further explored through density functional theory (DFT) calculations, which suggest strong easy-axis anisotropy, consistent with experimental magnetic measurements and crystal-field model expectations, and quasi-one-dimensional bands and a spin-split saddle point at the zone center.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [392] [Diffusion/Subdiffusion in the Pushy Random Walk](https://arxiv.org/abs/2602.07387)
*Ofek Lauber Bonomo,Itamar Shitrit,Shlomi Reuveni,Sidney Redner*

Main category: cond-mat.stat-mech

TL;DR: 提出“推挤随机游走”模型，模拟活性粒子在密集环境中的运动，发现1D中形成亚扩散增长的腔体，2D中密度增加导致从自由扩散到局域化的转变


<details>
  <summary>Details</summary>
Motivation: 现有模型无法真实描述活性粒子在密集介质中通过推挤障碍物实现的运动行为，需建立更贴合实验观测的动力学模型

Method: 引入可推挤多个障碍物的“推挤随机游走”机制，模拟粒子在有限障碍物密度环境中的运动

Result: 一维中粒子 carving 出长度呈亚扩散增长的障碍自由腔；二维中障碍物密度增加引发从自由扩散到局域化的相变，腔体半径同样亚扩散增长

Conclusion: 该模型更真实刻画活性粒子在密集环境中的相互作用，并揭示密度驱动的动力学相变现象

Abstract: We introduce the pushy random walk, where a walker can push multiple obstacles, thereby penetrating large distances in environments with finite obstacle density. This process gives a more realistic depiction of experimentally observed interactions of active particles in dense media. In one dimension, the walker carves out an obstacle-free cavity whose length grows subdiffusively over time. In two dimensions, increasing obstacle density drives a transition from free diffusion to localized behavior, where the walker is trapped within a cavity whose radius again grows subdiffusively with time.

</details>


### [393] [Berezinskii-Kosterlitz-Thouless phase transitions of the antiferromagnetic Ising model with ferromagnetic next-nearest-neighbor interactions on the kagome lattice](https://arxiv.org/abs/2602.07401)
*Yutaka Okabe,Hiromi Otsuka*

Main category: cond-mat.stat-mech

TL;DR: 研究了具有近邻反铁磁和次近邻铁磁相互作用的kagome晶格伊辛模型的六态钟 universality，通过三种方法证实存在两个BKT相变和三个相区，并利用机器学习验证了六态钟 universality class


<details>
  <summary>Details</summary>
Motivation: 探究kagome晶格上具有混合相互作用（近邻反铁磁+次近邻铁磁）的伊辛模型是否展现六态钟 universality class特性，特别是验证其Berezinskii-Kosterlitz-Thouless (BKT)相变行为

Method: 采用三种互补方法：1) 能级谱分析法；2) 蒙特卡洛模拟；3) 基于六态钟模型数据训练的机器学习相分类技术

Result: 1) 观测到两个BKT相变；2) 建立包含三个相的相图（低温有序相、中间BKT相、高温无序相），相边界由次近邻与近邻相互作用强度比决定；3) 机器学习分析成功验证六态钟 universality class

Conclusion: 该混合相互作用kagome伊辛模型确证属于六态钟 universality class，其相变特性通过多方法交叉验证，特别是机器学习技术为拓扑相变分类提供了有效新途径

Abstract: We investigate the six-state clock universality of the Ising model on the kagome lattice, considering antiferromagnetic nearest-neighbor (NN) and ferromagnetic next-nearest-neighbor (NNN) interactions. Our comprehensive study employs three approaches: the level-spectroscopy method, Monte Carlo simulations, and a machine-learning phase classification technique. In this system, we observe two Berezinskii-Kosterlitz-Thouless (BKT) transitions. We present a phase diagram consisting of three phases: the low-temperature ordered phase with sublattice magnetizations, the intermediate BKT phase, and the high-temperature disordered phase, as a function of the ratio of the NNN interaction to the NN interaction. We verify the six-state clock universality through the machine-learning study, which uses data from the six-state clock model on the kagome lattice for training.

</details>


### [394] [Capturing the Topological Phase Transition and Thermodynamics of the 2D XY Model via Manifold-Aware Score-Based Generative Modeling](https://arxiv.org/abs/2602.07548)
*Pratyush Jha*

Main category: cond-mat.stat-mech

TL;DR: 提出流形感知的分数生成模型框架，解决了标准扩散模型在连续自旋系统中欧几里得空间嵌入的局限性，成功应用于二维XY模型并精确捕捉BKT相变和热力学量，且无需重训练即可零样本推广到不同晶格尺寸


<details>
  <summary>Details</summary>
Motivation: 标准分数生成模型在欧几里得空间的表述不适用于变量本质上存在于流形上的连续自旋系统，训练会优先学习流形约束而非目标分布，导致无法精确复现物理系统所需的高阶统计量和热力学性质

Method: 提出流形感知的分数生成模型框架，应用于64x64二维XY模型（4096维环面）

Result: 相比标准扩散模型更精确估计理论玻尔兹曼分数，成功捕获BKT相变，无需特征工程即可准确重现热容等二阶矩量，零样本推广到未见晶格尺寸并准确恢复不同尺度系统的物理特性

Conclusion: 该方法绕过了领域特定特征工程，本质上可推广到其他连续自旋系统

Abstract: The application of generative modeling to many-body physics offers a promising pathway for analyzing high-dimensional state spaces of spin systems. However, unlike computer vision tasks where visual fidelity suffices, physical systems require the rigorous reproduction of higher-order statistical moments and thermodynamic quantities. While Score-Based Generative Models (SGMs) have emerged as a powerful tool, their standard formulation on Euclidean embedding space is ill-suited for continuous spin systems, where variables inherently reside on a manifold. In this work, we demonstrate that training on the Euclidean space compromises the model's ability to learn the target distribution as it prioritizes to learn the manifold constraints. We address this limitation by proposing the use of Manifold-Aware Score-Based Generative Modeling framework applied to the 64x64 2D XY model (a 4096-dimensional torus). We show that our method estimates the theoretical Boltzmann score with superior precision compared to standard diffusion models. Consequently, we successfully capture the Berezinskii-Kosterlitz Thouless (BKT) phase transition and accurately reproduce second-moment quantities, such as heat capacity without explicit feature engineering. Furthermore, we demonstrate zero-shot generalization to unseen lattice sizes, accurately recovering the physics of variable system scales without retraining. Since this approach bypasses domain-specific feature engineering, it remains intrinsically generalizable to other continuous spin systems.

</details>


### [395] [Momentum-Driven Reversible Logic Accelerates Efficient Irreversible Universal Computation](https://arxiv.org/abs/2602.07683)
*Kuen Wai Tang,Kyle J. Ray,James P. Crutchfield*

Main category: cond-mat.stat-mech

TL;DR: 本研究利用量子磁通参量器实现两种通用逻辑运算，对比了基于亚稳态的受控擦除和基于动量的擦除-翻转操作，发现后者能以更高保真度和速度实现通用性，且无需额外能量成本。


<details>
  <summary>Details</summary>
Motivation: 探索超导电路中物理嵌入的通用计算能力，比较不同物理资源（亚稳态分布vs动量）对逻辑运算性能和通用性的影响。

Method: 使用耦合量子磁通参量器（约瑟夫森结超导电路）构建2比特逻辑单元，分别实现：1）基于固定点分析的受控擦除（CE）；2）利用动量作为计算资源的擦除-翻转（EF）。通过构建NAND门验证通用性。

Result: 擦除-翻转（EF）显著优于受控擦除（CE），实现更高保真度、更快计算速度且无额外能量消耗。EF通过在不同逻辑子空间同时使用非平凡可逆和不可逆逻辑来实现通用性。

Conclusion: 动量计算范式提供了实用且高性能的实验方案，突显了基于动量的计算在实现通用逻辑门方面的潜力，为未来超导量子计算开辟了新路径。

Abstract: We present implementations of two physically-embedded computation-universal logical operations using a 2-bit logical unit composed of coupled quantum flux parametrons -- Josephson-junction superconducting circuits. To illustrate universality, we investigate NAND gates built from these two distinct elementary operations. On the one hand, Controlled Erasure (CE) is designed using fixed-point analysis and assumes that information must be stored in locally-metastable distributions. On the other, Erasure-Flip (EF) leverages momentum as a computational resource and significantly outperforms the metastable approach, simultaneously achieving higher fidelity and faster computational speed without incurring any additional energetic cost. Notably, the momentum degree of freedom allows the EF to achieve universality by using both nontrivial reversible and irreversible logic simultaneously in different logical subspaces. These results not only provide a practical, high-performance protocol ripe for experimental realization but also underscore the broader potential of momentum-based computing paradigms.

</details>


### [396] [Linear Response and Optimal Fingerprinting for Nonautonomous Systems](https://arxiv.org/abs/2602.08022)
*Valerio Lucarini*

Main category: cond-mat.stat-mech

TL;DR: 本研究构建响应理论、拉回测度与最优指纹法的统一框架，用于预测和归因时变系统对强迫的响应，特别适用于气候变化检测。


<details>
  <summary>Details</summary>
Motivation: 针对传统方法无法处理时变背景态的问题，该研究旨在建立响应理论、拉回测度与最优指纹法之间的联系，以预测时变系统在外强迫下的响应，并将观测异常归因于具体强迫。

Method: 首先推导时变马尔可夫链和扩散过程的线性响应理论公式；在一般扰动条件下讨论拉回测度的存在性、唯一性与可微性；推导格林-久保型显式响应公式；分析周期参考动力学情形；最后扩展最优指纹法以适应时变背景态和多时间片联合求解。

Result: 所得公式在自治参考态下退化为经典线性响应；发现即使未扰动的拉回吸引子具有周期性，其响应通常是非周期的；理论支持多时间片最优指纹分析；在含自然强迫的修正Ghil-Sellers能量平衡模型上验证了理论的正确性，能准确预测CO2增加的温度响应，并成功实现气溶胶和CO2强迫的归因。

Conclusion: 该工作为时变系统特别是气候变化检测与归因提供了坚实的数学基础和实用工具，通过理论推导与数值验证展示了方法的有效性和普适性。

Abstract: We provide a link between response theory, pullback measures, and optimal fingerprinting method that paves the way for a) predicting the impact of acting forcings on time-dependent systems and b) attributing observed anomalies to acting forcings when the reference state in not time-independent. We first derive formulas for linear response theory for time-dependent Markov chains and diffusions processes. We discuss existence, uniqueness, and differentiability of the pullback measure under general (not necessarily slow or periodic) perturbations of the transition kernels. An explicit Green-Kubo-type formula for the linear response is derived. We analyze in detail the case of periodic reference dynamics, where the unperturbed pullback attractor is periodic but the response is generally not. Our formulas reduce to those of classic linear response if one considers a reference autonomous state. Finally, we show that our results allow for extending the theory of optimal fingerprinting for detection and attribution of climate change (or change in any complex system) for the case of time-dependent background state and for the case where the optimal solution is sought for multiple time slices at the same time. We provide strong numerical support for the findings by applying our theory to a modified version of the Ghil-Sellers energy balance model where we include explicit time dependence in the reference state as a result of natural forcings. We verify the accuracy of response theory in predicting the impact of increases of $CO_2$ in the temperature field even when we discretize the system using Markov state modelling approach. Additionally, we consider a more complex modelling scenario where a localized aerosol forcing is also included in the system and show that the optimal fingerprinting method developed here is able to attribute the climate change signal to the acting forcings.

</details>


### [397] [The 4-$ε$ Expansion for Long-range Interacting Systems](https://arxiv.org/abs/2602.07818)
*Zhiyi Li,Kun Chen,Youjin Deng*

Main category: cond-mat.stat-mech

TL;DR: 该研究通过场论重整化群和微扰自举法，证明在O(n)自旋模型中，当长程相互作用衰减指数σ<2时，短程Wilson-Fisher不动点失稳，新的稳定长程不动点出现，且临界指数依赖于ε、δ=2-σ和n，阈值σ*=2与近期数值研究一致。


<details>
  <summary>Details</summary>
Motivation: 长程相互作用（按1/r^{d+σ}衰减）引入后，短程Wilson-Fisher不动点（SR-WFP）的稳定性自1970年代起存在争议，需明确其失稳条件及新不动点的性质。

Method: 采用标准场论重整化群与微扰自举法互补方案，在d=4-ε维度下进行至两圈级的ε-展开计算，分析σ参数对不动点的影响。

Result: 1. σ<2时SR-WFP失稳，稳定长程不动点（LR-WFP）涌现；2. 在非经典区d/2<σ<2，临界指数为ε、δ=2-σ和n的函数，且在ε→0、δ→0或n→∞极限下退化为精确解；3. LR与SR阈值严格位于σ*=2，支持最新数值研究，与Sak准则矛盾。

Conclusion: 研究确立了长程相互作用体系中临界行为的相变阈值σ*=2，解决了长期争议，揭示了非经典区临界指数的参数依赖性，为临界现象理论提供了新验证。

Abstract: The establishment of the Wilson-Fisher fixed point (WFP) for $O(n)$ spin models in $d=4-ε$ dimensions stands as a cornerstone of the renormalization group (RG) theory for critical phenomena. However, when long-range (LR) interactions, algebraically decaying as $\propto 1/r^{d+σ}$, are introduced, the fate of the short-range WFP (SR-WFP) has remained a subject of intense debate since the 1970s. We employ two complementary techniques -- the standard field-theoretic RG and a perturbative bootstrap scheme, and perform the $ε$-expansion calculations up to the two-loop level. We show that, as long as $σ<2$, the SR-WFP becomes unstable and a stable LR-WFP emerges, and, in the non-classical regime with $d/2 < σ< 2$, the critical exponents, including the anomalous dimension, are functions of $ε$, $δ=2-σ$ and $n$, which reduce to the exact results in the limiting cases $ε\to 0$, $δ\to 0$ or $n \to \infty$. Our $(4-ε)$-expansion calculations support the scenario that the threshold between the LR- and SR-WFP occurs strictly at $σ_*=2$, well consistent with the recent high-precision numerical study while different from the widely accepted Sak's criterion.

</details>


### [398] [The Entropies](https://arxiv.org/abs/2602.07861)
*Roumen Tsekov*

Main category: cond-mat.stat-mech

TL;DR: This paper critiques Shannon entropy's limitations, showing it works for canonical ensembles but fails for microcanonical ensembles and cannot derive the Second Law of thermodynamics.


<details>
  <summary>Details</summary>
Motivation: To address the inadequacy of Shannon entropy as a universal concept, particularly its failure to describe microcanonical ensembles and support derivation of the Second Law.

Method: Critical theoretical examination and analysis of entropy concepts in science and informatics.

Result: Identification of a fundamental discrepancy: Shannon entropy successfully describes canonical ensembles but is insufficient for microcanonical ensembles and cannot theoretically derive the Second Law.

Conclusion: Shannon entropy is an incomplete fundamental concept, requiring revision or alternative formulations for proper treatment of microcanonical systems and thermodynamic laws.

Abstract: Entropy is critically examined as a fundamental concept in contemporary science and informatics. Although the typical Shannon entropy provides a proper framework for describing the canonical ensemble, it fails to represent adequately the microcanonical ensemble. This discrepancy manifests additionally in its inability to support a theoretical derivation of the Second Law of thermodynamics.

</details>


### [399] [Hierarchical Lorentz Mirror Model: Normal Transport and a Universal $2/3$ Mean--Variance Law](https://arxiv.org/abs/2602.07988)
*Raphael Lefevere,Hal Tasaki*

Main category: cond-mat.stat-mech

TL;DR: 本研究提出洛伦兹镜像模型的分层版本，证明d≥3时正常输运的标度律，揭示d=2时存在对数修正及普适的2/3电导涨落定律，并通过数值模拟验证该定律在原始模型中的普适性。


<details>
  <summary>Details</summary>
Motivation: 探究仅由淬火环境无序性驱动宏观输运的普适规律，通过分层模型精确解析电导统计特性，揭示维度效应（d≥3与d=2差异）及随机电流匹配导致的普适涨落行为。

Method: 构建分层洛伦兹镜像模型并推导精确递归方程；结合高斯近似与数值模拟分析电导分布；在d=3原始模型中验证2/3定律。

Result: 1) d≥3时平均电导严格满足(截面积)/(长度)标度；2) d=2时出现对数修正且方差-均值比收敛于2/3；3) 数值模拟证实原始模型d=3中2/3定律成立。

Conclusion: 电导涨落的2/3定律是随机电流匹配诱导正常输运的普适特征，分层模型结论可推广至原始模型，为无序系统输运提供统一理论框架。

Abstract: The Lorentz mirror model provides a clean setting to study macroscopic transport generated solely by quenched environmental randomness. We introduce a hierarchical version that admits an exact recursion for the distribution of left--right crossings, and prove normal transport: the mean conductance scales as (cross-section)/(length) for all length scales if $d\ge3$. A Gaussian approximation, supported by numerics, predicts that, in the marginal case $d=2$, this scaling acquires a logarithmic correction and that the variance-to-mean ratio of conductance converges to the universal value $2/3$ (the ``$2/3$ law'') for all $d\ge2$. We conjecture that both effects persist beyond the hierarchical setting. We finally provide numerical evidence for the $2/3$ law in the original Lorentz mirror model in $d=3$, and interpret it as a universal signature of normal transport induced by random current matching.
  A YouTube video discussing the background and the main results of the paper is available: https://youtu.be/G1nqKd6MiXo

</details>


### [400] [Exact Stationary State of a $d$-dimensional Run-and-Tumble Particle in a Harmonic Potential](https://arxiv.org/abs/2602.08436)
*Mathis Guéneau,Satya N. Majumdar,Grégory Schehr*

Main category: cond-mat.stat-mech

TL;DR: 该研究精确求解了各向同性谐振势阱中跑-转粒子(RTP)的非平衡稳态分布，揭示了持久性与扩散竞争导致的形状相变及维度效应。


<details>
  <summary>Details</summary>
Motivation: 跑-转粒子是活性物质（如细菌）的关键模型，精确求解其在势阱中的稳态分布对理解活性系统在受限环境中的非平衡行为至关重要，可揭示持久性运动与热噪声的相互作用机制。

Method: 采用Kesten型递推关系将一维广义 trapped RTP 的稳态位置表示为 stick-breaking 过程，通过积分变换重构多维径向分布与联合密度，并引入扩散系数D的卷积处理热噪声。

Result: 1D/2D径向分布为β分布，3D为闭式非β分布；在转向面r=v₀/μ处存在持久性控制的形状相变；扩散正则化奇点并调控持久性-扩散主导的交叉行为；所有解析结果经数值模拟验证。

Conclusion: 该工作提供了 trapped RTP 的精确稳态理论框架，明确了维度、持久性与扩散对分布形态的调控规律，为活性物质在受限体系中的非平衡统计研究奠定基础。

Abstract: We derive the exact nonequilibrium steady state of a run-and-tumble particle (RTP) in $d$ dimensions confined in an isotropic harmonic trap $V(\mathbf r)=μr^{2}/2$, with $r=\|\mathbf r\|$. Rotational invariance reduces the problem to the stationary single-coordinate marginal $p_X(x)$, from which the radial distribution $p_R(r)$ and the full joint stationary density follow by explicit integral transforms. We first focus on a generalized trapped RTP in one dimension, where post-tumble velocities are drawn from an arbitrary distribution $W(v)$. Using a Kesten-type recursion, we represent its stationary position in terms of a stick-breaking (or Dirichlet) process, yielding closed-form expressions for its distribution and its moments. Specializing $W(v)$ to the projected velocity law of an isotropic RTP, we reconstruct $p_R(r)$ and the full joint distribution of all the coordinates in $d=1,2,3$. In $d=1$ and $d=2$, the radial law simplifies to a beta distribution, while in $d=3$, we derive closed-form expressions for $p_R(r)$ and the stationary joint distribution $P(x,y,z)$, which differ from a beta distribution. In all cases, we characterize a persistence-controlled shape transition at the turning surface $r=v_0/μ$, where $v_0$ is the self-propulsion speed. We further include thermal noise characterized by a diffusion coefficient $D>0$, showing that the stationary law is a Gaussian convolution of the $D=0$ result, which regularizes turning-point singularities and controls the crossover between persistence- and diffusion-dominated regimes as $D \to 0$ and $D \to \infty$ respectively. All analytical predictions are systematically validated against numerical simulations.

</details>


### [401] [Variational Method for Interacting Surfaces with Higher-Form Global Symmetries](https://arxiv.org/abs/2602.08310)
*Kiyoharu Kawana*

Main category: cond-mat.stat-mech

TL;DR: 本文开发了一种针对具有高阶形式全局对称性的相互作用表面系统的变分方法，将传统玻色子二次量子化哈密顿量推广到由闭合表面算符描述的p形式对称性体系，推导出类似Gross-Pitaevskii方程的泛函薛定谔方程，揭示了U(1)对称性下无能隙p形式场与离散对称性下有能隙BF型拓扑场论的不同行为，以及阿贝尔拓扑序和任意子表面激发。


<details>
  <summary>Details</summary>
Motivation: 开发一种变分方法来研究具有高阶形式全局对称性的相互作用表面系统，作为传统相互作用玻色子二次量子化哈密顿量的自然推广。

Method: 通过构建以闭合表面算符φ̂[Cp]表示的二次量子化哈密顿量（该算符在p形式全局对称性下带电），并应用变分原理，推导出与常规玻色子系统Gross-Pitaevskii方程类似的泛函薛定谔方程。

Result: 1) 得到泛函薛定谔方程；2) 无外力时存在由微观相互作用势U(ψ*ψ)和化学势唯一确定的均匀解，描述均匀玻色子表面气体；3) U(1) p形式对称性下低能涨落包含无能隙p形式场Ap，而离散对称性下p形式场变为有能隙，由BF型拓扑场论描述；4) 系统表现出具有任意子表面激发的阿贝尔拓扑序；5) 外力作用下问题可简化为常规Gross-Pitaevskii方程；6) 给出了涡旋和畴壁类似物的拓扑缺陷解析解；7) 将方法应用于Z_N格点规范理论。

Conclusion: 该变分方法成功地将传统方法推广到高阶形式对称性的表面系统，揭示了不同对称性（连续vs离散）导致的截然不同低能行为（无能隙vs有能隙）和拓扑序，为研究表面激发和拓扑缺陷提供了理论框架。

Abstract: We develop a variational method for interacting surface systems with higher-form global symmetries. As a natural extension of the conventional second-quantized Hamiltonian of interacting bosons, we explicitly construct a second-quantized Hamiltonian formulated in terms of a closed surface operator $\hatφ[C_p^{}]$ charged under a $p$-form global symmetry. Applying the variational principle, we derive a functional Schrödinger equation analogous to the Gross-Pitaevskii equation in conventional bosonic systems. In the absence of external forces, the variational equation admits a uniform solution that is uniquely determined by a microscopic interaction potential $U(ψ^*ψ)$ and the chemical potential. This uniform solution describes a uniform gas of bosonic surfaces. Using the obtained energy functional, we show that low-energy fluctuations contain a gapless $p$-form field $A_p^{}$ when the $p$-form global symmetry is $\mathrm{U}(1)$, whereas the $p$-form field becomes massive for discrete symmetries, whose low-energy limit is described by a $\mathrm{BF}$-type topological field theory. As a consequence, the system exhibits abelian topological order with anyonic surface excitations. In the presence of external forces, however, solving the functional equation in full generality remains challenging. We argue, however, that the problem reduces to solving the conventional Gross-Pitaevskii equation when external forces act separately on the center-of-mass and relative motions. In addition, we present analytic solutions for topological defects as analogs of vortex and domain-wall solutions in conventional bosonic systems. Finally, as a concrete microscopic model, we study a $\mathbb{Z}_N^{}$ lattice gauge theory and apply our variational method to this system.

</details>


### [402] [Stationary densities in a weakly nonconserving asymmetric exclusion processes with finite resources](https://arxiv.org/abs/2602.08405)
*Sourav Pal,Abhik Basu*

Main category: cond-mat.stat-mech

TL;DR: 本研究分析两端连接粒子库的TASEP-Langmuir模型，发现其相图与传统模型显著不同，部分相消失而新相出现，揭示了边界条件对非平衡相变的关键影响。


<details>
  <summary>Details</summary>
Motivation: 受有限资源和Langmuir动力学启发的开放TASEP模型，探讨两端连接粒子库时的稳态密度和相变行为，以揭示与标准模型的差异。

Method: 通过解析计算稳态密度分布和相变条件，构建控制参数平面上的相图，并与传统开放TASEP及环形缺陷模型对比。

Result: 相图发生显著变化，部分传统模型中的相消失，但新出现更多相；与环形缺陷模型相比，该模型允许更丰富的相结构。

Conclusion: 两端连接粒子库会重构相变行为，为理解开放系统中非平衡相变提供新视角，对相关物理和生物系统有启示意义。

Abstract: Asymmetric exclusion process (TASEP) along a one-dimensional (1D) open channel sets the paradigm for 1D driven models and nonequilibrium phase transitions in open 1D models. Inspired by the phenomenologies of an open TASEP with Langmuir kinetics (Lk) and with finite resources, we study the stationary densities and phase transitions in a TASEP with Lk connected to a particle reservoir at its both ends. We calculate the stationary density profiles and the phase transitions. The resulting phase diagrams in the plane of the control parameters are significantly different from their counterparts in an open TASEP with Lk. In particular, some of the phases admissible in the open TASEP with Lk model are no longer possible. Intriguingly, our model that is closely related to a TASEP coupled with Lk on a ring with a point defect, admits more phases than the latter. Phenomenological implications of our results are discussed.

</details>


### [403] [Preserving Hamiltonian Locality in Real-Space Coarse-Graining via Kernel Projection](https://arxiv.org/abs/2602.08502)
*Sun Haoyuan*

Main category: cond-mat.stat-mech

TL;DR: 提出一种物理约束的生成框架，通过空间投影机制而非时间弛豫来解决临界格点系统的临界减速问题，可在GPU上高效生成超大临界系综


<details>
  <summary>Details</summary>
Motivation: 数值模拟临界格点系统受到临界减速的根本限制，因为长程关联通常需要通过缓慢的时间平衡来建立

Method: 提出物理约束生成框架：使用能量约束核从紧凑平衡种子合成大尺度构型，通过强制哈密顿量可观测量将构型投影到最近邻能量流形，确保热力学一致性

Result: 生成的构型重现了尺度不变自旋关联、Binder累积量和各向同性结构因子，晶格尺寸超过10,000，无需迭代蒙特卡洛平衡

Conclusion: 该方法提供了保留临界普遍特征的实际逆映射，实现了超大规模临界系综的GPU并行高效生成

Abstract: Numerical simulations of critical lattice systems are fundamentally limited by critical slowing down, as long-range correlations are typically established through slow temporal equilibration. A physically constrained generative framework that replaces temporal relaxation with a spatial projection mechanism for critical systems is proposed. Using the two-dimensional Ising model at criticality as a benchmark, we introduce an energy-constrained kernel that synthesizes large-scale configurations from compact equilibrated seeds by enforcing Hamiltonian-level observables. The generated configurations are projected onto the nearest-neighbor energy manifold, ensuring thermodynamic consistency while retaining universal critical properties. We show that the resulting configurations reproduce scale-invariant spin correlations, Binder cumulants, and isotropic structure factors for lattice sizes exceeding 10,000, without iterative Monte Carlo equilibration. While not a strict renormalization group transformation, and motivated by renormalization ideas, the method provides a practical inverse mapping that retains universal features of criticality and enables efficient GPU-parallel generation of ultra-large critical ensembles.

</details>


### [404] [Uphill transport in competitive drift-diffusion models with volume exclusion](https://arxiv.org/abs/2602.08583)
*Francesco Casini,Cristian Giardinà,Jacopo Nicolini,Luca Selmi,Cecilia Vernia*

Main category: cond-mat.stat-mech

TL;DR: This paper studies uphill particle transport (against Fick's diffusion) in volume-exclusion systems, using multispecies asymmetric exclusion processes to bridge microscopic particle models with continuum engineering models like modified Poisson-Nernst-Planck.


<details>
  <summary>Details</summary>
Motivation: To explain uphill transport phenomena where particle flow opposes concentration gradients, and connect microscopic exclusion-based particle models with macroscopic continuum models used in engineering applications.

Method: Analyzing stationary hydrodynamic limits of multispecies weakly asymmetric exclusion processes (SHDL), then extending to oppositely charged particle systems to derive convergence to modified Poisson-Nernst-Planck equations.

Result: Demonstrates that uphill transport regimes persist across modeling scales, clarifies specific conditions enabling such transport, and identifies limitations of approximations in predicting particle fluxes.

Conclusion: Uphill transport phenomena are significant in nanoscale electrolytes, confined ionic devices, and membrane technologies, with the SHDL framework providing crucial insights into their occurrence across different physical scales.

Abstract: This paper addresses uphill transport (defined as a regime in which particle flow is opposite to the prescriptions of Fick's diffusion) in drift-diffusion particle transport constrained by volume exclusion. Firstly, we show that the stationary hydrodynamic limit of a multispecies, weakly asymmetric exclusion process (SHDL) naturally predicts precisely characterized uphill regimes in the space of external drivings.
  Then, with specific reference to systems of oppositely charged particles, we identify well-defined model hypotheses and extensions whereby the SHDL converges to the modified Poisson-Nernst-Planck model, thus bridging the gap between exclusion-based particle models and continuum descriptions commonly used in engineering. The merits and limitations of the models in describing the particle fluxes and predicting uphill transport conditions are investigated in detail with respect to the adopted approximations and simplifications.
  The results demonstrate the persistence of uphill transport phenomena across modeling scales, clarify the conditions under which they occur, and suggest that uphill transport may play a significant role in nanoscale electrolytes, confined ionic and iontronic devices, and membrane-based technologies.

</details>


<div id='nlin.AO'></div>

# nlin.AO [[Back]](#toc)

### [405] [Time delay in the 1d swarmalator model](https://arxiv.org/abs/2602.08156)
*K. P. O'Keeffe,Jason Hindes*

Main category: nlin.AO

TL;DR: 研究一维 swarmalator 模型中的时滞效应，发现新的周期态，并揭示同步/异步态稳定性与耦合强度而非时滞相关这一反直觉结果。


<details>
  <summary>Details</summary>
Motivation: 探究时滞如何影响 swarmalator（兼具集群运动和振荡特性的系统）的集体行为。

Method: 通过解析和数值方法研究含时滞耦合的一维 swarmalator 模型，识别分岔路径并分析稳定性条件。

Result: 发现从相位波态通过 Hopf 分岔、从异步态通过零特征值分岔产生的新不稳定周期态；解析求得边界曲线；重要发现：异步/同步态稳定性仅依赖耦合强度，与时滞 τ 无关。

Conclusion: 时滞虽能产生丰富动力学，但不影响基本态的稳定性，这挑战了对耦合振荡器系统中时滞效应的传统认知。

Abstract: We study the 1d swarmalator model augmented with time delayed coupling. Along with the familiar sync, async, and phase wave states, we find a family of unsteady states where the order parameters are time periodic, sometimes with clean oscillations, sometimes with irregular vacillations. The unsteady states are born in two ways: via a Hopf bifurcation from the phase wave, and a zero eigenvalue bifurcation from the async state. We find both of these boundary curves analytically. A surprising result is that stabilities of the async and sync states are independent of the delay τ; they depend only on the coupling strength.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [406] [Limitations of SVD-Based Diagnostics for Non-Hermitian Many-Body Localization with Time-Reversal Symmetry](https://arxiv.org/abs/2602.07349)
*Huimin You,Jinghu Liu,Yunbo Zhang,Zhihao Xu*

Main category: cond-mat.dis-nn

TL;DR: 研究比较了奇异值分解(SVD)和精确对角化(ED)在非厄米多体系统中定位多体局域化(MBL)转变的可靠性，发现SVD方法虽能捕捉定性趋势，但不能准确定位TRS保护的非厄米系统的MBL转变临界点。


<details>
  <summary>Details</summary>
Motivation: 奇异值分解已被用于构建非厄米多体系统的类厄米诊断方法，但其在时间反演对称性(TRS)保持条件下确定多体局域化转变的可靠性尚不明确。

Method: 在TRS保持的非厄米硬核玻色子链中，引入非互易跃迁，使用准周期势、随机无序和Stark势三种模型，将SVD诊断与精确对角化基准进行比较，分析谱统计、半链纠缠熵、参与率逆数和谱形状因子。

Result: 在准周期和随机无序模型中，精确对角化给出相互一致的转变估计，而奇异值分解系统性地将临界无序强度向更大值偏移，并可能导致不同的相分配；但在纯净Stark模型中，两种方法定位到一致的临界倾斜。

Conclusion: 奇异值分解诊断虽然能够捕捉定性趋势，但在TRS保持的非厄米多体系统中，通常不能准确定位多体局域化转变。

Abstract: Singular value decomposition (SVD) has been used to construct Hermitian-like diagnostics for non-Hermitian many-body systems, but its reliability for identifying many-body localization (MBL) transitions -- particularly in time-reversal-symmetry (TRS) preserving settings -- remains unclear. Here we benchmark SVD-based diagnostics against exact diagonalization (ED) in TRS-preserving non-Hermitian hard-core-boson chains with nonreciprocal hopping, considering three representative potentials: a quasiperiodic potential, random disorder, and a Stark potential. We compare spectral statistics, half-chain entanglement entropy, inverse participation ratio, and spectral form factors. For the quasiperiodic and random-disorder models, ED yields mutually consistent transition estimates, whereas SVD systematically shifts the inferred critical disorder strength to larger values and can lead to different phase assignments. In contrast, for the clean Stark model ED and SVD locate a consistent critical tilt. Our results show that while SVD-based diagnostics capture qualitative trends, they are not generically reliable for quantitatively locating the MBL transition in TRS-preserving non-Hermitian many-body systems.

</details>


### [407] [Continuum model for the terahertz dielectric response of glasses](https://arxiv.org/abs/2602.07417)
*Tatsuya Mori,Hideyuki Mizuno,Dan Kyotani,Soo Han Oh,Yuzuki Motokawa,Yasuhiro Fujii,Akitoshi Koreeda,Shinji Kohara,Seiji Kojima*

Main category: cond-mat.dis-nn

TL;DR: Develops a continuum model coupling charge fluctuations and shear modulus to explain boson peak dynamics in glasses, successfully applied to glycerol glass.


<details>
  <summary>Details</summary>
Motivation: Standard Debye or Lorentz models fail to capture the robust crossover in terahertz dielectric response caused by boson peak dynamics in glasses.

Method: Creates a continuum description coupling infrared-effective charge fluctuation spectrum to frequency-dependent shear modulus, applied to glycerol glass.

Result: Reproduces measured complex dielectric function and nearly linear infrared light-vibration coupling around boson peak, revealing dominant role of transverse shear dynamics.

Conclusion: The model successfully describes THz dielectric response in glasses, highlighting transverse shear dynamics as the primary mechanism behind boson peak behavior.

Abstract: Boson peak dynamics in glasses produce a robust crossover in the terahertz (THz) dielectric response that standard Debye or Lorentz models do not capture. We develop a continuum description of this THz response, coupling an infrared-effective charge fluctuation spectrum to a frequency-dependent shear modulus, and apply it to glycerol glass. The model reproduces the measured complex dielectric function and the nearly linear infrared light-vibration coupling around the boson peak, and highlights the dominant role of transverse shear dynamics.

</details>


### [408] [Insensitive nonreciprocal edge breathers](https://arxiv.org/abs/2602.07443)
*Bertin Many Manda,Vassos Achilleos*

Main category: cond-mat.dis-nn

TL;DR: 揭示非线性与非互易性在拓扑力学超材料中的相互作用，发现频率不敏感的非互易边缘呼吸子，为鲁棒非线性拓扑波提供新机制。


<details>
  <summary>Details</summary>
Motivation: 探索拓扑力学系统中非线性与非互易性耦合的未知物理现象，超越传统对称性保护框架。

Method: 构建非互易拓扑克莱因-戈登链模型，研究非对称耦合非线性振荡器的动力学行为。

Result: 从线性边缘模式分岔出连续族非互易边缘呼吸子；发现频率不随非线性强度变化的"不敏感"呼吸子；揭示其机制为模式非正交性与非线性竞争导致频移随系统尺寸指数衰减；该现象在强非线性 regime 依然存在。

Conclusion: 建立无需对称性保护的鲁棒非线性拓扑波实现方案，为先进力学超材料设计开辟新途径。

Abstract: We uncover subtle and previously unexplored phenomena arising from the interplay of nonlinearity and nonreciprocity in topological mechanical metamaterials. We study a nonreciprocal topological Klein-Gordon chain of asymmetrically coupled nonlinear oscillators, which serves as a minimal mass-spring model capturing the features of several active nonreciprocal metamaterials across mechanical, electronic, and acoustic platforms. We demonstrate that continuous families of nonreciprocal edge breathers (NEBs), namely boundary-localized, time-periodic waves, emerge from the linear edge mode as its amplitude increases. Remarkably, despite the absence of chiral or sublattice symmetries, we identify insensitive NEBs whose nonlinear frequency remains fixed to that of the linear edge mode with increasing nonlinearity. Our analysis reveals that the mechanism underlying this insensitivity stems from a competition between mode nonorthogonality and nonlinear interactions, yielding an exponential decay of the NEB nonlinear frequency shift with system size. Crucially, these insensitive NEBs also persist in the strongly nonlinear regime. Our work establishes a novel pathway toward realizing robust nonlinear topological waves in mechanical metamaterials without relying on symmetry-protected nonlinearities.

</details>


### [409] [Thermodynamic modes of a quasiperiodic mobility-edge system in a quantum Otto cycle](https://arxiv.org/abs/2602.08378)
*Ao Zhou,Shujie Cheng,Gao Xianlong*

Main category: cond-mat.dis-nn

TL;DR: 研究准周期晶格作为量子奥托循环工作介质，发现绝热协议比近绝热协议多热机和制冷两种模式，可通过参数调谐实现模式切换


<details>
  <summary>Details</summary>
Motivation: 探究具有精确迁移率边的准周期晶格（Biddle-Das Sarma模型）在热力学过程中的运作方式，并将其作为量子奥托循环的工作介质

Method: 通过改变跃迁范围参数p、初始/末态势场强度Vi/Vf以及两种理想化孤立过程协议（近绝热态冻结协议和绝热协议），分析其工作模式

Result: 近绝热协议中循环仅支持加热器和加速器两种模式；绝热协议中额外出现热机和制冷机模式，共四种模式

Conclusion: 迁移率边系统可在单一平台实现多种热力学功能，通过调节p、Vi、Vf可指导模式切换

Abstract: We investigate thermodynamic operation of a quasiperiodic lattice with an exact mobility edge, described by the Biddle--Das Sarma model. We use this model as the working medium of a quantum Otto cycle and map its operating mode as a function of the hopping-range parameter $p$, the initial and final potential strengths $V_i$ and $V_f$, and two idealized protocols for the isolated strokes. In a near-adiabatic (state-frozen) protocol, where the density matrix is approximately unchanged during the isolated strokes, the cycle supports only two modes: a \emph{heater} and an \emph{accelerator}. In an adiabatic protocol, where level populations are preserved while the spectrum is deformed, two additional modes appear: a \emph{heat engine} and a \emph{refrigerator}. Our results show that mobility-edge systems can realize multiple thermodynamic functions within a single platform and provide guidance for switching between modes by tuning $p$, $V_i$, and $V_f$.

</details>
