{"id": "2602.23420", "categories": ["cond-mat.str-el", "cond-mat.stat-mech", "hep-th", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23420", "abs": "https://arxiv.org/abs/2602.23420", "authors": ["Thomas T. Dumitrescu", "Pierluigi Niro", "Ryan Thorngren"], "title": "From QED$_3$ to Self-Dual Multicriticality in the Fradkin-Shenker Model", "comment": null, "summary": "We consider the Fradkin-Shenker ${\\mathbb Z}_2$ gauge-Higgs lattice model in 2+1 dimensions, i.e. the toric code deformed by an in-plane magnetic field. Its phase diagram contains a multicritical CFT with gapless, mutually non-local electric and magnetic particles, exchanged by a ${\\mathbb Z}_2^{\\mathsf{D}}$ self-duality symmetry. We introduce a staggered generalization of the model in which these particles carry global $U(1)_e$ and $U(1)_m$ charges, respectively, and we propose a continuum QFT description in terms of QED$_3$ with $N_f = 2$ Dirac fermion flavors and a charge-two Higgs field with Yukawa couplings. The conjectured phase diagram harbors a multicritical CFT with $(O(2)_e \\times O(2)_m)\\rtimes\\mathbb{Z}_2^\\mathsf{D}$ symmetry, some of which is emergent in the QFT description. We compute the scaling dimensions of some operators using a large-$N_f$ expansion and find agreement with the emergent selection rules. The staggered model admits a deformation to the original Fradkin-Shenker model, which maps to unit-charge monopole operators in Higgs-Yukawa-QED$_3$ that break the $U(1)_e \\times U(1)_m$ symmetry. We show explicitly that this deformation reproduces all features of the Fradkin-Shenker phase diagram. Finally, we propose a multicritical duality between Higgs-Yukawa-QED$_3$ and the easy-plane $\\mathbb{ CP}^1$ model (i.e. two-flavor scalar QED$_3$ with a suitable potential), which describes spin-1/2 anti-ferromagnets on a square lattice. This duality implies a first-order line of Néel-VBS transitions ending in a deconfined quantum multicritical point, described by the same $O(2)_e \\times O(2)_m$ symmetric CFT that arises in the staggered Fradkin-Shenker model, which separates it from a gapped ${\\mathbb Z}_2$ spin liquid phase."}
{"id": "2602.23477", "categories": ["cond-mat.str-el", "cond-mat.mes-hall", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2602.23477", "abs": "https://arxiv.org/abs/2602.23477", "authors": ["Sayan Mitra", "Fang Xie", "Marek Kolmer", "Qimiao Si", "Chandan Setty"], "title": "Signatures of Green's function zeros and their topology using impurity spectroscopy", "comment": "8 pages, 5 figures", "summary": "Topology without quasiparticles has emerged as a key framework for understanding Mott insulators, where Green's-function zeros encode nontrivial topological structure. Yet, experimental detection of these zeros represents a challenge. Using exact diagonalization of the one-dimensional Hubbard model with an impurity and Zeeman field, supported by exact analytic results, we show that Green's-function zeros manifest as an in-gap spectral weight in the unitary scattering regime. In this limit, we map the impurity problem onto a doped Mott insulator and identify the resulting in-gap state as a \"zeron\" excitation which is a localized doublon (holon) for an attractive (repulsive) potential. The zeron spectral weight and its associated zero vanish above a critical Zeeman field. Our results imply that Green's function zeros have in fact already been observed in experiments, and establish impurity and magnetic-field tuning as practical tools for controlling their topology."}
{"id": "2602.23484", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.23484", "abs": "https://arxiv.org/abs/2602.23484", "authors": ["Shuangyuan Lu", "Lucas Q Silveira", "Yizhi You"], "title": "Nonequilibrium topological response under charge dephasing", "comment": "23 pages, 8 figures", "summary": "We explore nonequilibrium topological responses of symmetry-protected topological (SPT) states in open quantum systems subject to decoherence. For SPT wavefunctions protected by a product symmetry G $\\times$ S , where G defects are decorated with S charge, we show that local dephasing of the S charge density generically induces spontaneous strong-to-weak symmetry breaking (SWSSB) of G in the resulting mixed-state ensemble. We extend this mechanism to SPT phases protected by higher-form and spatially modulated symmetries, and further to gapless SPT states, demonstrating that dephasing-induced SWSSB persists well beyond conventional gapped 0-form settings. Our results provide a qualitative, channel-defined fingerprint of SPT order that is intrinsic to open-system dynamics and goes beyond equilibrium linear response."}
{"id": "2602.23522", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.23522", "abs": "https://arxiv.org/abs/2602.23522", "authors": ["Nitin Kaushal", "Adarsh S. Patri", "Marcel Franz"], "title": "Spontaneous altermagnetism in multi-orbital correlated electron systems", "comment": null, "summary": "Altermagnets have attracted considerable attention in recent years owing to their potential technological applications in spintronics and magnonics. Recently, a new class of spontaneous altermagnets has been theoretically predicted in a correlated two orbital model, driven by the coexistence of antiferromagnetic spin and staggered orbital ordering, thus broadening the scope of altermagnetic phenomena to systems with strong correlations. It has been noted, however, that the required spin and orbital order violates the well-established Goodenough-Kanamori (GK) rules, which underlie much of our understanding of magnetism in complex systems. Here we show that materials with three active orbitals may offer a more realistic route to this exotic state. Specifically, we consider a two-dimensional system with $t_{2g}^{2}$ electrons and identify a novel microscopic mechanism that allows the formation of a spontaneous altermagnetic Mott insulator. We explain how the GK rules are circumvented and provide the stability criteria by employing unbiased mean-field and density matrix renormalization group calculations. In addition, for the first time, we uncover the presence and microscopic origin of chirally split magnons in these spontaneous altermagnets, with experimentally measurable spin conductivities. Finally, we predict that the application of a small in-plane magnetic field induces, in the presence of weak atomic spin-orbit coupling, an as-yet unreported hybrid chiral magnon-orbiton mode with a non-zero orbital polarization giving rise to finite longitudinal and transverse orbital conductivities under a thermal gradient."}
{"id": "2602.23391", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23391", "abs": "https://arxiv.org/abs/2602.23391", "authors": ["Nazanin Mohammadi Sepahvand", "Eleni Triantafillou", "Hugo Larochelle", "Doina Precup", "Daniel M. Roy", "Gintare Karolina Dziugaite"], "title": "Detoxifying LLMs via Representation Erasure-Based Preference Optimization", "comment": null, "summary": "Large language models (LLMs) trained on webscale data can produce toxic outputs, raising concerns for safe deployment. Prior defenses, based on applications of DPO, NPO, and similar algorithms, reduce the likelihood of harmful continuations, but not robustly so: they are vulnerable to adversarial prompting and easily undone by fine-tuning-based relearning attacks. Indeed, research has shown that these edits to the model are superficial: linear probing reveals that harmful \"directions\" remain present in representations. To address this, we propose Representation Erasure-based Preference Optimization (REPO), reformulating detoxification as a token-level preference problem. Using a novel objective with preference data, we force the representations of toxic continuations to converge toward their benign counterparts. Our mechanistic analysis reveals that this granular approach is critical: unlike baselines, REPO induces deep, localized edits to toxicity-encoding neurons while preserving general model utility. Exhaustive evaluations show that REPO achieves state-of-the-art robustness, stopping sophisticated threats-including relearning attacks and enhanced GCG jailbreaks-where existing representation- and output-based methods fail."}
{"id": "2602.24221", "categories": ["nlin.CD"], "pdf": "https://arxiv.org/pdf/2602.24221", "abs": "https://arxiv.org/abs/2602.24221", "authors": ["Alexei A. Mailybaev", "Luca Moriconi"], "title": "Renormalization-group perspective on spontaneous stochasticity", "comment": "28 pages, 10 figures", "summary": "We present a renormalization-group perspective on spontaneous stochasticity in hydrodynamic turbulence, viewed through the lens of multiscale dynamical systems. Building on previously established results for a solvable multiscale Arnold's cat model, we show that spontaneous stochasticity emerges as a universal fixed point of an RG transformation acting on Markov kernels, independent of the microscopic regularization. Classical examples - including the Feigenbaum equation, the central limit theorem, and hierarchical spin models - are reinterpreted within the same framework, placing spontaneous stochasticity alongside other universality phenomena."}
{"id": "2602.23988", "categories": ["cond-mat.stat-mech", "cond-mat.soft"], "pdf": "https://arxiv.org/pdf/2602.23988", "abs": "https://arxiv.org/abs/2602.23988", "authors": ["Kristian Stølevik Olsen", "Mitsusuke Tarama", "Hartmut Löwen"], "title": "Information bound on navigation speed in smart active matter", "comment": null, "summary": "Intelligent behavior in life-like systems often arises from the ability to gather, process, and act on information. While active matter provides a framework for studying life-like dynamics, it typically omits internal information-processing and decision-making. Here we introduce an adaptive active particle model that uses minimal information processing capabilities in order to navigate towards a distant target. By combining renewal-based intermittent motion with the Cramér-Rao inequality, we derive a bound on the navigation speed valid for a wide range of information processing strategies. The framework captures hallmark features of cognitive systems, including optimal sensing durations and a speed-accuracy trade-off that balances noise and reliability. Allowing stored information to degrade before action reveals that although deterioration slows navigation, the trade-off remains governed primarily by external orientational noise and is remarkably insensitive to memory decay."}
{"id": "2602.23367", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.23367", "abs": "https://arxiv.org/abs/2602.23367", "authors": ["Shubh Laddha", "Lucas Changbencharoen", "Win Kuptivej", "Surya Shringla", "Archana Vaidheeswaran", "Yash Bhaskar"], "title": "HumanMCP: A Human-Like Query Dataset for Evaluating MCP Tool Retrieval Performance", "comment": "4 pages, 2 figures, 3 tables", "summary": "Model Context Protocol (MCP) servers contain a collection of thousands of open-source standardized tools, linking LLMs to external systems; however, existing datasets and benchmarks lack realistic, human-like user queries, remaining a critical gap in evaluating the tool usage and ecosystems of MCP servers. Existing datasets often do contain tool descriptions but fail to represent how different users portray their requests, leading to poor generalization and inflated reliability of certain benchmarks. This paper introduces the first large-scale MCP dataset featuring diverse, high-quality diverse user queries generated specifically to match 2800 tools across 308 MCP servers, developing on the MCP Zero dataset. Each tool is paired with multiple unique user personas that we have generated, to capture varying levels of user intent ranging from precise task requests, and ambiguous, exploratory commands, reflecting the complexity of real-world interaction patterns."}
{"id": "2602.23422", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23422", "abs": "https://arxiv.org/abs/2602.23422", "authors": ["Adway Kumar Das", "Achilleas Lazarides"], "title": "Ground state and persistent oscillations in the quantum East model", "comment": "11 pages, 8 figures", "summary": "For the 1D quantum East model with open boundaries, we show that in the limit $s \\to -\\infty$, the ground state is accurately captured by a simple spin-coherent product state. We further identify a low-entanglement excited eigenstate that differs from the ground state only by a $π$-rotation of the boundary spin, remaining well approximated by a spin-coherent state. For a range of $-\\infty<s<0$, the edge-coherent product state overlaps with two eigenstates separated by a size-independent energy gap, leading to persistent coherent oscillations of both global and local observables in the thermodynamic limit. These oscillations originate from boundary physics and are distinct from quantum many-body scars or hypercube-like Fock-space mechanisms."}
{"id": "2602.23582", "categories": ["physics.comp-ph", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.23582", "abs": "https://arxiv.org/abs/2602.23582", "authors": ["Zhen Jiang", "Jiuyang Liang", "Qi Zhou"], "title": "Random batch sum-of-Gaussians method for molecular dynamics simulation of particle systems in the NPT ensemble", "comment": "29 pages, 8 pages", "summary": "In this work, we develop a random batch sum-of-Gaussians (RBSOG) method for molecular dynamics simulations of charged systems in the isothermal-isobaric (NPT) ensemble. We introduce an SOG splitting of the pressure-related $1/r^3$ kernel, yielding a smooth short-/long-range decomposition for instantaneous pressure evaluation. The long-range part is treated in Fourier space by random-batch importance sampling. Because the radial and non-radial pressure components favor different proposals, direct sampling either increases structure-factor evaluations and communication or leads to substantial variance inflation. To address this tradeoff, we introduce a measure-recalibration strategy that reuses Fourier modes drawn from the radial proposal and corrects them for the non-radial target, producing an unbiased pressure estimator with significantly reduced variance and negligible extra cost. The resulting method mitigates pressure artifacts caused by cutoff discontinuities in traditional Ewald-based treatments while preserving near-optimal $O(N)$ complexity. We provide theoretical evidence on pressure decomposition error, consistency of stochastic approximation, and convergence of RBSOG-based MD. Numerical experiments on bulk water, LiTFSI ionic liquids, and DPPC membranes show that RBSOG accurately reproduces key structural and dynamical observables with small batch sizes ($P\\sim 100$). In large-scale benchmarks up to $10^7$ atoms on $2048$ CPU cores, RBSOG achieves about an order-of-magnitude speedup over particle-particle particle-mesh in electrostatic calculations for NPT simulations, together with a consistent $4\\times$ variance reduction relative to random batch Ewald and excellent weak/strong scalability. Overall, RBSOG provides a practical and scalable route to reduce time-to-solution and communication cost in large-scale NPT simulations."}
{"id": "2602.24039", "categories": ["nlin.AO"], "pdf": "https://arxiv.org/pdf/2602.24039", "abs": "https://arxiv.org/abs/2602.24039", "authors": ["Hidemasa Ishii", "Hiroshi Kori"], "title": "Degree heterogeneity shapes escape mechanisms in networks of diffusively coupled bistable elements", "comment": "16 pages, 4 figures, 4 tables", "summary": "For fully connected populations of diffusively coupled bistable elements, we identified three qualitatively distinct mechanisms of noise-induced escape as coupling strength varies [H. Ishii and H. Kori, arXiv:2512.01388 (2025)]. Here we generalize these results to a class of networked systems and demonstrate that degree heterogeneity (i.e., variability in node degree) shapes escape mechanisms alongside coupling strength. In applied contexts, networks of noisy bistable elements provide a minimal conceptual framework for understanding abrupt state transitions in complex systems. Theoretically, a quantitative approach to escape is challenging because nonlinearity, network interactions, and dynamical noise jointly shape the collective dynamics. We extend the analytical framework developed for the fully connected model to a class of networked systems based on the annealed network approximation. We derive three effective one-dimensional descriptions of collective escape dynamics. We validate our theoretical predictions for mean escape times by direct numerical simulations. Our analysis reveals that the validity and quantitative behavior of the reduced descriptions depend on degree heterogeneity in addition to coupling strength. This work extends the classification of escape mechanisms to networked bistable elements. Furthermore, our analytical framework provides tools for understanding synergistic phenomena arising from the interplay of nonlinearity, diffusive coupling, and dynamical noise."}
{"id": "2602.23411", "categories": ["cs.CC", "cs.DM", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.23411", "abs": "https://arxiv.org/abs/2602.23411", "authors": ["Yongjian Zhan"], "title": "Microscopic Structure of Random 3-SAT: A Discrete Geometric Approach to Phase Transitions and Algorithmic Complexity", "comment": "13 pages", "summary": "The structural phase transitions and computational complexity of random 3-SAT instances are traditionally described using thermodynamic analogies from statistical physics, such as Replica Symmetry Breaking and energy landscapes. While providing profound macroscopic insights, these theories lack a discrete microscopic structure. In this paper, we propose a complementary, strictly discrete geometric model that maps these phenomena directly to the combinatorial topology of an $N$-dimensional Boolean hypercube. By defining the problem space purely through valid solutions rather than abstract energy states, we establish deterministic mechanics for clustering and freezing, driven by the progressive elimination of vertices and Hamming distance bridges. Furthermore, we derive absolute structural boundaries for 3-SAT, identifying a minimal unsatisfiability limit at constraint density $α= \\frac{8}{N}$ populated by at least $\\frac{N(N-1)(N-2)}{6}$ distinct unsatisfiable cores, and a maximal satisfiability limit at $α= \\frac{7}{6}(N-1)(N-2)$ populated by $2^N$ maximal satisfiable instances. These combinatorial extremes mathematically elucidate why the average-case Satisfiability Threshold Conjecture holds only ``almost surely.'' Finally, we apply this topological framework to explain the ``easy-hard-easy'' algorithmic complexity curve. We demonstrate that the efficiency of Depth-First Search is governed by the geometric transition from an abundance of valid search paths (the under-constrained easy phase) to a high density of structurally ``removed variables'' that force immediate contradictions (the over-constrained easy phase). This microscopic perspective bridges theoretical phase transitions with the concrete mechanics of complete search algorithms."}
{"id": "2602.23554", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.23554", "abs": "https://arxiv.org/abs/2602.23554", "authors": ["T. R. Kirkpatrick", "D. Belitz"], "title": "Generic Long-Range Order-Parameter Correlations in Metallic Quantum Magnets", "comment": "26pp, 13 figs", "summary": "It is shown that in all types of metallic magnets the coupling of the order parameter to the conduction electrons leads to an order-parameter susceptibility that is long-ranged at zero temperature. This is true for all known classes of ferromagnets, and also for antiferromagnets and spin-density wave systems, helimagnets, magnetic nematics, and altermagnets. The consequences for the magnetic quantum phase transition vary between different classes of magnets. In almost all 3-d systems with a homogeneous magnetization, as well as in magnetic nematics and in altermagnets, the long-ranged correlations generically modify the nature of the magnetic quantum phase transition from second order to first order. The only exception are non-centrosymmetric ferromagnets with a strong spin-orbit interaction, where the correlations change the order of the transition in 2-d systems, but not in 3-d ones. In helimagnets, spin-wave systems, and N{é}el antiferromagnets their effect is even weaker and does not change the order of the transition if the ordering wave number is sufficiently large. In systems with quenched disorder the transition generically is of second order, but the correlations modify the critical behavior. These conclusions are reached by very simple considerations that are based entirely on the single-particle excitations in the nonmagnetic phase and their modifications by a field conjugate to the order parameter, augmented by renormalization-group considerations."}
{"id": "2602.23400", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23400", "abs": "https://arxiv.org/abs/2602.23400", "authors": ["Zezheng Wu", "Rui Wang", "Xinghe Cheng", "Yang Shao", "Qing Yang", "Jiapu Wang", "Jingwei Zhang"], "title": "U-CAN: Utility-Aware Contrastive Attenuation for Efficient Unlearning in Generative Recommendation", "comment": null, "summary": "Generative Recommendation (GenRec) typically leverages Large Language Models (LLMs) to redefine personalization as an instruction-driven sequence generation task. However, fine-tuning on user logs inadvertently encodes sensitive attributes into model parameters, raising critical privacy concerns. Existing Machine Unlearning (MU) techniques struggle to navigate this tension due to the Polysemy Dilemma, where neurons superimpose sensitive data with general reasoning patterns, leading to catastrophic utility loss under traditional gradient or pruning methods. To address this, we propose Utility-aware Contrastive AttenuatioN (U-CAN), a precision unlearning framework that operates on low-rank adapters. U-CAN quantifies risk by contrasting activations and focuses on neurons with asymmetric responses that are highly sensitive to the forgetting set but suppressed on the retention set. To safeguard performance, we introduce a utility-aware calibration mechanism that combines weight magnitudes with retention-set activation norms, assigning higher utility scores to dimensions that contribute strongly to retention performance. Unlike binary pruning, which often fragments network structure, U-CAN develop adaptive soft attenuation with a differentiable decay function to selectively down-scale high-risk parameters on LoRA adapters, suppressing sensitive retrieval pathways and preserving the topological connectivity of reasoning circuits. Experiments on two public datasets across seven metrics demonstrate that U-CAN achieves strong privacy forgetting, utility retention, and computational efficiency."}
{"id": "2602.24284", "categories": ["nlin.CD"], "pdf": "https://arxiv.org/pdf/2602.24284", "abs": "https://arxiv.org/abs/2602.24284", "authors": ["Pezhman Ebrahimzadeh", "Michael Schiek", "Yuri Maistrenko"], "title": "Chaotic Switching In The Minimal Pendula Network", "comment": null, "summary": "We report the chaotic switching phenomenon in the minimal $N = 3$ pendula network with global coupling. Analyzing the stability conditions of the chimera states and their dependence on the parameters, three scenarios of chaotic switchings are identified: 1) a riddling bifurcation scenario, where an unstable periodic orbit inside the chimera manifold becomes transversally unstable, 2) a blowout bifurcation scenario, where the switching is caused by the transverse destabilization of the chaotic chimera with respect to its manifold, and 3) switchings between \"laminar\" saddle chimeras within a global \"turbulent\" attractor. The results are obtained based on the detailed examination of the existing regimes including chimera states, limit cycles and fixed points, their multistability and switching regime. In the parameter regions where the chaotic chimeras coexist with stable non-chaotic solutions, the switching trajectory can eventually escape to a stable solution, causing an additional unpredictability in the system behavior, as it is difficult to predict the escaping moment."}
{"id": "2602.24008", "categories": ["cond-mat.stat-mech", "cond-mat.quant-gas", "cond-mat.str-el", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.24008", "abs": "https://arxiv.org/abs/2602.24008", "authors": ["Kazuya Fujimoto", "Taiki Ishiyama", "Taiga Kurose", "Takato Yoshimura", "Tomohiro Sasamoto"], "title": "Exact Anomalous Current Fluctuations in Quantum Many-Body Dynamics", "comment": "35 pages, 4 figures", "summary": "Fluctuations of integrated currents have attracted considerable interest over the past decades in the context of statistical mechanics. Recently, anomalous current fluctuations, characterized by the M-Wright function, were obtained exactly in a classical automaton [$Ž$. Krajnik et al., Phys. Rev. Lett. 128, 160601 (2022)], and previous studies have shown that the anomalous behavior can arise in a variety of classical systems. Despite the rapidly growing interest in such anomalous behaviors, which capture a universal aspect of one-dimensional many-body transport, the exact derivation of the M-Wright function in quantum many-body systems has remained elusive. In this Letter, we present the first exact microscopic derivation of the M-Wright function in quantum many-body dynamics by analyzing the integrated spin current in a one-dimensional Fermi-Hubbard model with infinitely strong repulsive interactions. Our results lay the groundwork for exploring anomalous integrated currents in a broad class of quantum many-body systems."}
{"id": "2602.23373", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.23373", "abs": "https://arxiv.org/abs/2602.23373", "authors": ["Pavel Chernakov", "Sasan Jafarnejad", "Raphaël Frank"], "title": "An Agentic LLM Framework for Adverse Media Screening in AML Compliance", "comment": null, "summary": "Adverse media screening is a critical component of anti-money laundering (AML) and know-your-customer (KYC) compliance processes in financial institutions. Traditional approaches rely on keyword-based searches that generate high false-positive rates or require extensive manual review. We present an agentic system that leverages Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) to automate adverse media screening. Our system implements a multi-step approach where an LLM agent searches the web, retrieves and processes relevant documents, and computes an Adverse Media Index (AMI) score for each subject. We evaluate our approach using multiple LLM backends on a dataset comprising Politically Exposed Persons (PEPs), persons from regulatory watchlists, and sanctioned persons from OpenSanctions and clean names from academic sources, demonstrating the system's ability to distinguish between high-risk and low-risk individuals."}
{"id": "2602.23491", "categories": ["quant-ph", "physics.hist-ph"], "pdf": "https://arxiv.org/pdf/2602.23491", "abs": "https://arxiv.org/abs/2602.23491", "authors": ["Győző Egri", "Marton Gomori", "Balazs Gyenis", "Gábor Hofer-Szabó"], "title": "Trajectory of Probabilities, Probability on Trajectories, and the Stochastic-Quantum Correspondence", "comment": null, "summary": "The probabilistic description of the time evolution of a physical system can take two conceptually distinct forms: a trajectory of probabilities, which specifies how probabilities evolve over time, and a probability on trajectories, which assigns probabilities to possible histories. A lack of a clear distinction between these two probabilistic descriptions has given rise to a number of conceptual difficulties, particularly in recent analyses of stochastic-quantum correspondence. This paper provides a systematic account of their relationship. We define probability dynamics and stochastic process families together with a precise notion of implementation that connects the two descriptions. We show that implementations are generically non-unique, that every probability dynamics admits a Markovian implementation, and characterize when non-Markovian implementations are possible. We expose fallacies in common arguments for the linearity of probability dynamics based on the law of total probability and clarify the proper interpretation of ``transition matrices'' by distinguishing dynamics-level maps from the conditional probability matrices of implementing processes. We further introduce decomposability as the appropriate general notion of stepwise evolution for (possibly nonlinear) probability dynamics, relate it to divisibility in the linear case -- showing that the two can come apart -- and disentangle both notions from Markovianity and time-homogeneity. Finally, we connect these results to what we call statistical dynamics, in which linearity is indeed physically motivated, and contrast the framework with quantum mechanics."}
{"id": "2602.24087", "categories": ["physics.comp-ph", "astro-ph.IM", "hep-ex", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2602.24087", "abs": "https://arxiv.org/abs/2602.24087", "authors": ["Luca Di Bella", "Jan Bürger", "Markus Demleitner", "Torsten Enßlin", "Johannes Erdmann", "Martin Erdmann", "Benjamin Fischer", "Martin Gasthuber", "Gabriele Gramelsberger", "Wolfgang Gründinger", "Prateek Gupta", "Johannes Hartl", "Maximilian Horzela", "Vijay Kartik", "Stefan Krischer", "Eva Kröll", "Thomas Kuhr", "Katharina Kürschner", "Inga Lakomiec", "Valerie Lang", "Kristin Lohwasser", "Thomas Metcalf", "Martin Möller", "Saskia Nagel", "Susanne Pfalzner", "Rebecca Redlin", "Christopher Schrader", "Kathrin Schulz", "Markus Schumacher", "Kilian Schwarz", "Fabian Sigler", "Dwayne Spiteri", "Achim Stahl", "Judith Steinfeld", "Wim Vanderbauwhede", "Cyrus Walther", "Angela Warkentin", "Peter Wissmann", "Eoin Woods"], "title": "Shaping the Digital Future of ErUM Research: Sustainability & Ethics", "comment": "32 pages, no figures, report for workshop \"Shaping the Digital Future of ErUM Research: Sustainability & Ethics\", 28 July to 1 August 2025, Aachen, Germany, see https://indico.desy.de/event/47133/", "summary": "This workshop report from \"Shaping the Digital Future of ErUM Research: Sustainability & Ethics\" (Aachen, 2025) reviews progress on sustainability measures in data-intensive ErUM-Data research since the 2023 call-to-action on resource-aware research. It evaluates short-, medium-, and long-term actions around monitoring and reducing CO2 emissions, improving data and software FAIRness, optimizing workflows and computing infrastructures, and aligning operations with low-carbon energy availability, including concepts such as \"breathing\" computing centers, long-term data storage strategies, and software efficiency certification. The report stresses the need for systematic teaching, training, mentoring, and new support formats to establish sustainable coding and computing practices, particularly among students and early-career researchers, and highlights the importance of dedicated steering and funding instruments to embed sustainability in project planning. Ethical discussions focus on the transformative use of AI in ErUM-Data, addressing autonomy, bias, transparency, explainability, attribution of responsibility, and the risk of deskilling, while reaffirming that accountability for scientific outcomes remains with human researchers. Finally, the report emphasizes that sustainable transformation requires not only technical measures but also targeted awareness-building, communication strategies, incentives, and community-driven initiatives to move from awareness to action and to integrate sustainability and ethics into everyday scientific practice."}
{"id": "2602.23503", "categories": ["cs.CC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23503", "abs": "https://arxiv.org/abs/2602.23503", "authors": ["Lianna Hambardzumyan", "Konstantin Myasnikov", "Artur Riazanov", "Morgan Shirley", "Adi Shraibman"], "title": "Spiky Rank and Its Applications to Rigidity and Circuits", "comment": null, "summary": "We introduce spiky rank, a new matrix parameter that enhances blocky rank by combining the combinatorial structure of the latter with linear-algebraic flexibility. A spiky matrix is block-structured with diagonal blocks that are arbitrary rank-one matrices, and the spiky rank of a matrix is the minimum number of such matrices required to express it as a sum. This measure extends blocky rank to real matrices and is more robust for problems with both combinatorial and algebraic character.\n  Our conceptual contribution is as follows: we propose spiky rank as a well-behaved candidate matrix complexity measure and demonstrate its potential through applications. We show that large spiky rank implies high matrix rigidity, and that spiky rank lower bounds yield lower bounds for depth-2 ReLU circuits, the basic building blocks of neural networks. On the technical side, we establish tight bounds for random matrices and develop a framework for explicit lower bounds, applying it to Hamming distance matrices and spectral expanders. Finally, we relate spiky rank to other matrix parameters, including blocky rank, sparsity, and the $γ_2$-norm."}
{"id": "2602.23600", "categories": ["cond-mat.str-el", "cond-mat.mes-hall", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23600", "abs": "https://arxiv.org/abs/2602.23600", "authors": ["Li Chen", "Zhiping Yao"], "title": "Second-quantized approach to the study of Halperin state in fractional quantum Hall effect", "comment": "9 pages", "summary": "We give a recursion relation for the second-quantized fermionic (bosonic) Halperin state, which avoids exact diagonalization of its two-component first-quantized parent Hamiltonian. We validate this formula by proving that the second-quantized Halperin state, as recursively defined in this formula, is indeed a zero mode of the corresponding second-quantized parent Hamiltonian and that it has the correct filling factor."}
{"id": "2602.23409", "categories": ["cs.LG", "cs.AI", "cs.ET", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23409", "abs": "https://arxiv.org/abs/2602.23409", "authors": ["Michael Poppel", "Jonas Stein", "Sebastian Wölckert", "Markus Baumann", "Claudia Linnhoff-Popien"], "title": "Long Range Frequency Tuning for QML", "comment": null, "summary": "Quantum machine learning models using angle encoding naturally represent truncated Fourier series, providing universal function approximation capabilities with sufficient circuit depth. For unary fixed-frequency encodings, circuit depth scales as O(omega_max * (omega_max + epsilon^{-2})) with target frequency magnitude omega_max and precision epsilon. Trainable-frequency approaches theoretically reduce this to match the target spectrum size, requiring only as many encoding gates as frequencies in the target spectrum. Despite this compelling efficiency, their practical effectiveness hinges on a key assumption: that gradient-based optimization can drive prefactors to arbitrary target values. We demonstrate through systematic experiments that frequency prefactors exhibit limited trainability: movement is constrained to approximately +/-1 units with typical learning rates. When target frequencies lie outside this reachable range, optimization frequently fails. To overcome this frequency reachability limitation, we propose grid-based initialization using ternary encodings, which generate dense integer frequency spectra. While this approach requires O(log_3(omega_max)) encoding gates -- more than the theoretical optimum but exponentially fewer than fixed-frequency methods -- it ensures target frequencies lie within the locally reachable range. On synthetic targets with three shifted high frequencies, ternary grid initialization achieves a median R^2 score of 0.9969, compared to 0.1841 for the trainable-frequency baseline. For the real-world Flight Passengers dataset, ternary grid initialization achieves a median R^2 score of 0.9671, representing a 22.8% improvement over trainable-frequency initialization (median R^2 = 0.7876)."}
{"id": "2602.24242", "categories": ["cond-mat.stat-mech", "cond-mat.quant-gas", "cond-mat.str-el", "nlin.SI"], "pdf": "https://arxiv.org/pdf/2602.24242", "abs": "https://arxiv.org/abs/2602.24242", "authors": ["Takato Yoshimura", "Žiga Krajnik", "Alvise Bastianello", "Enej Ilievski"], "title": "Anomalous hydrodynamic fluctuations in the quantum XXZ spin chain", "comment": "9+2 pages, 3 figures", "summary": "The quantum XXZ spin-1/2 chain features non-Gaussian spin current fluctuations in the regime of easy-axis anisotropy. Using ballistic macroscopic fluctuation theory, we derive the exact probability distribution of typical spin-current fluctuations in thermal equilibrium. The obtained nested Gaussian distribution is fully characterized by its variance which we analytically relate to the spin diffusion constant and static spin susceptibility, and compare our with numerical simulations. By unveiling how the same mechanism which leads to anomalous charge current fluctuations in single-file systems manifests in the XXZ chain, our approach establishes the universal hydrodynamic origin of the observed anomalous fluctuations."}
{"id": "2602.23541", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23541", "abs": "https://arxiv.org/abs/2602.23541", "authors": ["Arvind Raghavan", "Elias Bareinboim"], "title": "Causal Identification from Counterfactual Data: Completeness and Bounding Results", "comment": null, "summary": "Previous work establishing completeness results for $\\textit{counterfactual identification}$ has been circumscribed to the setting where the input data belongs to observational or interventional distributions (Layers 1 and 2 of Pearl's Causal Hierarchy), since it was generally presumed impossible to obtain data from counterfactual distributions, which belong to Layer 3. However, recent work (Raghavan & Bareinboim, 2025) has formally characterized a family of counterfactual distributions which can be directly estimated via experimental methods - a notion they call $\\textit{counterfactual realizabilty}$. This leaves open the question of what $\\textit{additional}$ counterfactual quantities now become identifiable, given this new access to (some) Layer 3 data. To answer this question, we develop the CTFIDU+ algorithm for identifying counterfactual queries from an arbitrary set of Layer 3 distributions, and prove that it is complete for this task. Building on this, we establish the theoretical limit of which counterfactuals can be identified from physically realizable distributions, thus implying the $\\textit{fundamental limit to exact causal inference in the non-parametric setting}$. Finally, given the impossibility of identifying certain critical types of counterfactuals, we derive novel analytic bounds for such quantities using realizable counterfactual data, and corroborate using simulations that counterfactual data helps tighten the bounds for non-identifiable quantities in practice."}
{"id": "2602.23501", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23501", "abs": "https://arxiv.org/abs/2602.23501", "authors": ["Zhenghao Li", "Hao Zhan", "Shana H. Winston", "Ewan Mer", "Zhenghao Yin", "Shang Yu", "Yazeed K. Alwehaibi", "Gerard J. Machado", "Dayne Marcus Lopena", "Lijian Zhang", "M. S. Kim", "Aonan Zhang", "Ian A. Walmsley", "Raj B. Patel"], "title": "Machine learning of quantum data using optimal similarity measurements", "comment": "32 pages, 10 figures, 1 table, including Supplementary Information", "summary": "Quantum machine learning seeks a computational advantage in data processing by evaluating functions of quantum states, such as their similarity, that can be classically intractable to compute. For quantum advantage to be possible, however, it is essential to bypass costly characterisation of individual data instances in favour of efficient, direct similarity evaluation. Here we demonstrate a sample-optimal, hardware-efficient protocol for estimating quantum similarity -- the state overlap -- using bosonic quantum interference. The sample complexity of this approach is independent of the system dimension and is information-theoretically optimal up to a constant factor. Experimentally, we implement the scheme on \\emph{Prakash-1}, a quantum computing platform based on a fully programmable integrated photonic processor. By preparing and interfering qudit states on the chip to directly extract their overlap, we demonstrate classification and online learning of quantum data with high accuracy in realistic noisy experiments. Our results establish joint overlap measurements as a scalable pathway to efficient quantum data analysis and a practical building block for network-integrated quantum machine learning."}
{"id": "2502.01476", "categories": ["cs.LG", "math.NA", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2502.01476", "abs": "https://arxiv.org/abs/2502.01476", "authors": ["Orestis Oikonomou", "Levi Lingsch", "Dana Grund", "Siddhartha Mishra", "Georgios Kissas"], "title": "Neuro-Symbolic AI for Analytical Solutions of Differential Equations", "comment": "Updates the method and added extra results", "summary": "Analytical solutions to differential equations offer exact, interpretable insight but are rarely available because discovering them requires expert intuition or exhaustive search in combinatorial spaces. We introduce SIGS, a neuro-symbolic framework that automates this process. SIGS uses a formal grammar to generate only syntactically valid building blocks, embeds these expressions into a continuous space, and then searches this space to assemble, score, and refine candidate closed-form solutions by minimizing a physics-based residual. This design unifies symbolic reasoning with numerical optimization; the grammar constrains candidate solution blocks to be proper by construction, while the latent search makes exploration tractable and data-free. SIGS is the first neuro-symbolic method to (i) analytically solve coupled systems of nonlinear PDEs, (ii) discover solutions under grammar misspecification, and (iii) produce accurate symbolic approximations for PDEs lacking known closed-form solutions. Overall, SIGS achieves orders-of-magnitude improvements in accuracy and efficiency over existing symbolic methods on standard benchmarks."}
{"id": "2602.23763", "categories": ["cs.CC"], "pdf": "https://arxiv.org/pdf/2602.23763", "abs": "https://arxiv.org/abs/2602.23763", "authors": ["Zihan Hao", "Zikuan Huang", "Qipeng Liu"], "title": "On the Need for (Quantum) Memory with Short Outputs", "comment": null, "summary": "In this work, we establish the first separation between computation with bounded and unbounded space, for problems with short outputs (i.e., working memory can be exponentially larger than output size), both in the classical and the quantum setting. Towards that, we introduce a problem called nested collision finding, and show that optimal query complexity can not be achieved without exponential memory. Our result is based on a novel ``two-oracle recording'' technique, where one oracle ``records'' the computation's long outputs under the other oracle, effectively reducing the time-space trade-off for short-output problems to that of long-output problems. We believe this technique will be of independent interest for establishing time-space tradeoffs in other short-output settings."}
{"id": "2602.23925", "categories": ["cond-mat.str-el", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2602.23925", "abs": "https://arxiv.org/abs/2602.23925", "authors": ["Afreen Anamul Haque", "Rishabh Saraswat", "Aniket Singha", "Rekha Verma", "Sitangshu Bhattacharya"], "title": "Phonon-Assisted Photoluminescence and Ultrafast Exciton Dynamics in Two-Dimensional Silicon Carbide", "comment": "6 figures", "summary": "Phonon assisted photoluminescence provides a direct window into exciton phonon interactions in low dimensional semiconductors. Using fully ab initio many body perturbation theory, including finite momentum Bethe Salpeter calculations, we investigate phonon assisted emission and exciton dynamics in two dimensional hexagonal silicon carbide and benchmark its response against 2D hexagonal boron nitride. By explicitly resolving exciton phonon matrix elements, we identify high energy optical TO LO phonons as the dominant contributors to sideband formation and quantify their spectral weights. h SiC exhibits pronounced phonon assisted sidebands comparable to h BN, despite a smaller exciton phonon energy separation and fewer resolved replicas. The bright K K exciton governs near UV zero phonon emission, while intervalley excitons acquire radiative character through symmetry allowed optical-phonon coupling. Temperature dependent scattering rates reveal an ultrashort bright exciton lifetime of approximately 300 fs at 10 K, highlighting rapid exciton relaxation driven by intrinsic phonon channels. These results establish monolayer SiC as a symmetry-activated platform for efficient, strain-free phonon-assisted emission and provide a quantitative framework for ultrafast exciton dynamics in wide bandgap 2D semiconductors."}
{"id": "2602.23410", "categories": ["cs.LG", "cs.AI", "eess.SP", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2602.23410", "abs": "https://arxiv.org/abs/2602.23410", "authors": ["Hanning Guo", "Farah Abdellatif", "Hanwen Bi", "Andrei Galbenus", "Jon. N. Shah", "Abigail Morrison", "Jürgen Dammers"], "title": "Brain-OF: An Omnifunctional Foundation Model for fMRI, EEG and MEG", "comment": null, "summary": "Brain foundation models have achieved remarkable advances across a wide range of neuroscience tasks. However, most existing models are limited to a single functional modality, restricting their ability to exploit complementary spatiotemporal dynamics and the collective data scale across imaging techniques. To address this limitation, we propose Brain-OF, the first omnifunctional brain foundation model jointly pretrained on fMRI, EEG and MEG, capable of handling both unimodal and multimodal inputs within a unified framework. To reconcile heterogeneous spatiotemporal resolutions, we introduce the Any-Resolution Neural Signal Sampler, which projects diverse brain signals into a shared semantic space.To further manage semantic shifts, the Brain-OF backbone integrates DINT attention with a Sparse Mixture of Experts, where shared experts capture modality-invariant representations and routed experts specialize in modality-specific semantics. Furthermore, we propose Masked Temporal-Frequency Modeling, a dual-domain pretraining objective that jointly reconstructs brain signals in both the time and frequency domains. Brain-OF is pretrained on a large-scale corpus comprising around 40 datasets and demonstrates superior performance across diverse downstream tasks, highlighting the benefits of joint multimodal integration and dual-domain pretraining."}
{"id": "2602.24276", "categories": ["cond-mat.stat-mech", "hep-th", "math-ph", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.24276", "abs": "https://arxiv.org/abs/2602.24276", "authors": ["Samuel H. Pickering", "Bruno Bertini"], "title": "Asymptotically Solvable Quantum Circuits", "comment": null, "summary": "The discovery of chaotic quantum circuits with (partially) solvable dynamics has played a key role in our understanding of non-equilibrium quantum matter and, at the same time, has helped the development of concrete platforms for quantum computation. It was shown that solvability does not prevent the generation of chaotic dynamics, however, it imposes non-trivial constraints on the generated correlations. A natural question is then whether it is possible to gain insight into the generic case despite the latter being very hard to access. To address this question here we introduce a family of 'asymptotically solvable' quantum circuits where the solvability constraints only affect correlations on length scales beyond a tuneable threshold. This means that their dynamics are only solvable for long enough times: for times shorter than the threshold they are generic. We show this by computing both their dynamical correlations on the equilibrium (infinite temperature) state and their thermalisation dynamics following quantum quenches from compatible (asymptotically solvable) non-equilibrium initial states. The class of systems we introduce is generically ergodic but contains a non-interacting point, which we use to provide exact analytical results, complementing those of numerical experiments, on the non-solvable early time regime."}
{"id": "2602.23545", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23545", "abs": "https://arxiv.org/abs/2602.23545", "authors": ["Matteo Ceriscioli", "Karthika Mohan"], "title": "Planning under Distribution Shifts with Causal POMDPs", "comment": "To appear at the 36th International Conference on Automated Planning and Scheduling (ICAPS-26)", "summary": "In the real world, planning is often challenged by distribution shifts. As such, a model of the environment obtained under one set of conditions may no longer remain valid as the distribution of states or the environment dynamics change, which in turn causes previously learned strategies to fail. In this work, we propose a theoretical framework for planning under partial observability using Partially Observable Markov Decision Processes (POMDPs) formulated using causal knowledge. By representing shifts in the environment as interventions on this causal POMDP, the framework enables evaluating plans under hypothesized changes and actively identifying which components of the environment have been altered. We show how to maintain and update a belief over both the latent state and the underlying domain, and we prove that the value function remains piecewise linear and convex (PWLC) in this augmented belief space. Preservation of PWLC under distribution shifts has the advantage of maintaining the tractability of planning via $α$-vector-based POMDP methods."}
{"id": "2602.23510", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23510", "abs": "https://arxiv.org/abs/2602.23510", "authors": ["Emma Tien Hwai Medlock", "Vinod N. Rao", "Ry Render", "Timothy Spiller", "Rupesh Kumar"], "title": "Continuous variable quantum key distribution channel emulator for the SPOQC mission", "comment": "11 pages, 8 figures", "summary": "In a free space optical (FSO) communication link from satellite to ground, the losses in the channel will be dynamic. Thus, the characterization of the FSO channel is of great importance and this can be emulated in the lab to evaluate the realistic performance of a satellite payload. In this work, we introduce a novel optical channel emulator capable of replicating these dynamics, especially for Low Earth Orbit based CubeSats. We demonstrate its ability to accurately emulate a satellite-to-ground optical communications channel under various atmospheric turbulence strengths, satellite trajectories, and optical ground station parameters at a given optical wavelength of interest. Our satellite channel emulator was designed to test and benchmark the performance of the continuous variable quantum key distribution payload for the Satellite Platform for Optical Quantum Communications mission - an in-orbit demonstrator for the UK's Quantum Communication Hub, to be launched in early 2026."}
{"id": "2602.24056", "categories": ["quant-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.24056", "abs": "https://arxiv.org/abs/2602.24056", "authors": ["Felipe Peleteiro", "João Victor Shiguetsugo Kawanami Lima", "Pedro Marcelo Prado", "Felipe Fernandes Fanchini", "Ariel Norambuena"], "title": "Learning spectral density functions in open quantum systems", "comment": "8 pages, 4 figures", "summary": "Spectral density functions quantify how environmental modes couple to quantum systems and govern their open dynamics. Inferring such frequency-dependent functions from time-domain measurements is an ill-conditioned inverse problem. Here, we use exactly solvable spin-boson models with pure-dephasing and amplitude-damping channels to reconstruct spectral density functions from noisy simulated data. First, we introduce a parameter estimation approach based on machine learning regressors to infer Lorentzian and Ohmic-like spectral density parameters, quantifying robustness to noise. Second, we show that a cosine transform inversion yields a physics-consistent spectral prior estimation, which is refined by a constrained neural network enforcing positivity and correct asymptotic behaviour. Our neural network framework robustly reconstructs structured spectral densities by filtering simulated noisy signals and learning general functional dependencies."}
{"id": "2602.23809", "categories": ["cs.CC"], "pdf": "https://arxiv.org/pdf/2602.23809", "abs": "https://arxiv.org/abs/2602.23809", "authors": ["Pavel Hubáček"], "title": "Black-Box PWPP Is Not Turing-Closed", "comment": null, "summary": "We establish that adaptive collision-finding queries are strictly more powerful than non-adaptive ones by proving that the complexity class PWPP (Polynomial Weak Pigeonhole Principle) is not closed under adaptive Turing reductions relative to a random oracle. Previously, PWPP was known to be closed under non-adaptive Turing reductions (Jeřábek 2016). We demonstrate this black-box separation by introducing the NESTED-COLLISION problem, a natural collision-finding problem defined on a pair of shrinking functions. We show that while this problem is solvable via two adaptive calls to a PWPP oracle, its random instances cannot be solved via a black-box non-adaptive reduction to the canonical PWPP-complete problem COLLISION."}
{"id": "2602.23986", "categories": ["cond-mat.str-el", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2602.23986", "abs": "https://arxiv.org/abs/2602.23986", "authors": ["GiBaik Sim", "Stephan Rachel"], "title": "Quantum spin models of commensurate $p$-wave magnets", "comment": "8 pages, 4 figures", "summary": "The $p$-wave magnet has emerged as a new type of magnetism exhibiting odd-parity, time-reversal-symmetric spin splitting in momentum space, and has attracted considerable interest as a promising platform for spintronic applications. However, the theoretical understanding of the fundamental mechanism responsible for stabilizing this phase remains limited. In this work, we identify a microscopic interacting model that realizes the $p$-wave magnet as its ground state. We first introduce a Hubbard model and derive the corresponding low-energy spin Hamiltonian. At the classical level, we find that the $p$-wave magnet is stabilized but remains energetically degenerate with competing noncoplanar states. Quantum fluctuations lift this degeneracy, selecting the $p$-wave magnet as the unique ground state. The resulting electronic structure exhibits finite spin accumulation via the Edelstein effect, highlighting the potential of $p$-wave magnetism for spintronic applications. We further discuss the relevance of our theory to quasi-two-dimensional honeycomb magnets such as Ni$_2$Mo$_3$O$_8$. Our findings establish the possibility of spontaneous $p$-wave magnetism."}
{"id": "2602.23413", "categories": ["cs.LG", "cs.CL", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.23413", "abs": "https://arxiv.org/abs/2602.23413", "authors": ["Shu Liu", "Shubham Agarwal", "Monishwaran Maheswaran", "Mert Cemri", "Zhifei Li", "Qiuyang Mang", "Ashwin Naren", "Ethan Boneh", "Audrey Cheng", "Melissa Z. Pan", "Alexander Du", "Kurt Keutzer", "Alexandros G. Dimakis", "Koushik Sen", "Matei Zaharia", "Ion Stoica"], "title": "EvoX: Meta-Evolution for Automated Discovery", "comment": null, "summary": "Recent work such as AlphaEvolve has shown that combining LLM-driven optimization with evolutionary search can effectively improve programs, prompts, and algorithms across domains. In this paradigm, previously evaluated solutions are reused to guide the model toward new candidate solutions. Crucially, the effectiveness of this evolution process depends on the search strategy: how prior solutions are selected and varied to generate new candidates. However, most existing methods rely on fixed search strategies with predefined knobs (e.g., explore-exploit ratios) that remain static throughout execution. While effective in some settings, these approaches often fail to adapt across tasks, or even within the same task as the search space changes over time. We introduce EvoX, an adaptive evolution method that optimizes its own evolution process. EvoX jointly evolves candidate solutions and the search strategies used to generate them, continuously updating how prior solutions are selected and varied based on progress. This enables the system to dynamically shift between different search strategies during the optimization process. Across nearly 200 real-world optimization tasks, EvoX outperforms existing AI-driven evolutionary methods including AlphaEvolve, OpenEvolve, GEPA, and ShinkaEvolve on the majority of tasks."}
{"id": "2602.23420", "categories": ["cond-mat.str-el", "cond-mat.stat-mech", "hep-th", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23420", "abs": "https://arxiv.org/abs/2602.23420", "authors": ["Thomas T. Dumitrescu", "Pierluigi Niro", "Ryan Thorngren"], "title": "From QED$_3$ to Self-Dual Multicriticality in the Fradkin-Shenker Model", "comment": null, "summary": "We consider the Fradkin-Shenker ${\\mathbb Z}_2$ gauge-Higgs lattice model in 2+1 dimensions, i.e. the toric code deformed by an in-plane magnetic field. Its phase diagram contains a multicritical CFT with gapless, mutually non-local electric and magnetic particles, exchanged by a ${\\mathbb Z}_2^{\\mathsf{D}}$ self-duality symmetry. We introduce a staggered generalization of the model in which these particles carry global $U(1)_e$ and $U(1)_m$ charges, respectively, and we propose a continuum QFT description in terms of QED$_3$ with $N_f = 2$ Dirac fermion flavors and a charge-two Higgs field with Yukawa couplings. The conjectured phase diagram harbors a multicritical CFT with $(O(2)_e \\times O(2)_m)\\rtimes\\mathbb{Z}_2^\\mathsf{D}$ symmetry, some of which is emergent in the QFT description. We compute the scaling dimensions of some operators using a large-$N_f$ expansion and find agreement with the emergent selection rules. The staggered model admits a deformation to the original Fradkin-Shenker model, which maps to unit-charge monopole operators in Higgs-Yukawa-QED$_3$ that break the $U(1)_e \\times U(1)_m$ symmetry. We show explicitly that this deformation reproduces all features of the Fradkin-Shenker phase diagram. Finally, we propose a multicritical duality between Higgs-Yukawa-QED$_3$ and the easy-plane $\\mathbb{ CP}^1$ model (i.e. two-flavor scalar QED$_3$ with a suitable potential), which describes spin-1/2 anti-ferromagnets on a square lattice. This duality implies a first-order line of Néel-VBS transitions ending in a deconfined quantum multicritical point, described by the same $O(2)_e \\times O(2)_m$ symmetric CFT that arises in the staggered Fradkin-Shenker model, which separates it from a gapped ${\\mathbb Z}_2$ spin liquid phase."}
{"id": "2602.23579", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23579", "abs": "https://arxiv.org/abs/2602.23579", "authors": ["Guillem Rodríguez-Corominas", "Maria J. Blesa", "Christian Blum"], "title": "Construct, Merge, Solve & Adapt with Reinforcement Learning for the min-max Multiple Traveling Salesman Problem", "comment": null, "summary": "The Multiple Traveling Salesman Problem (mTSP) extends the Traveling Salesman Problem to m tours that start and end at a common depot and jointly visit all customers exactly once. In the min-max variant, the objective is to minimize the longest tour, reflecting workload balance. We propose a hybrid approach, Construct, Merge, Solve & Adapt with Reinforcement Learning (RL-CMSA), for the symmetric single-depot min-max mTSP. The method iteratively constructs diverse solutions using probabilistic clustering guided by learned pairwise q-values, merges routes into a compact pool, solves a restricted set-covering MILP, and refines solutions via inter-route remove, shift, and swap moves. The q-values are updated by reinforcing city-pair co-occurrences in high-quality solutions, while the pool is adapted through ageing and pruning. This combination of exact optimization and reinforcement-guided construction balances exploration and exploitation. Computational results on random and TSPLIB instances show that RL-CMSA consistently finds (near-)best solutions and outperforms a state-of-the-art hybrid genetic algorithm under comparable time limits, especially as instance size and the number of salesmen increase."}
{"id": "2602.23544", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23544", "abs": "https://arxiv.org/abs/2602.23544", "authors": ["Jihee Yang", "Thomas J. Carroll", "Philip Mason", "Robert Schwartz", "Kenneth M. O'Hara", "Jennifer Lund", "Michael Gottschalk", "Timothy Stephenson", "Lawrence H. Friedman", "Francisco Yumiceva", "Justin Hackley", "Aurelius L. Graninger", "Chris Rotella", "Pat Warner", "Jonathan M. Cochran", "Adam V. Bruce", "Melody Wagner", "James Wenner", "Stan Steers", "Christopher Moore", "Alex Marakov", "Bradley G. Christensen"], "title": "High-Temporal-Resolution Measurements of the Impacts of Ionizing Radiation on Superconducting Qubits", "comment": null, "summary": "We measure the effect of ionizing radiation on superconducting qubits with a timing resolution of 1 $μs$ using microwave kinetic inductance detectors (MKIDs) fabricated on the same substrate. We observe no correlation between two-level system (TLS) scrambling events and ionizing radiation events detected with the MKIDs, suggesting TLS scrambling events may not arise from ionizing radiation and instead the previously reported apparent correlation may be due to events without sufficient energy to trigger our MKIDs. We characterize the fast-time system recovery of transmons following a radiation event, where we observe the recovery of the enhanced qubit relaxation and excitation to be well-described by an exponential recovery to the baseline quasiparticle density, with a characteristic time of $13\\pm1\\ μ$s, and a peak quasiparticle density at the junction per deposited energy of $240/μm^3/MeV$. The fast recovery is consistent with literature reported values for Nb-based devices with direct injection of 2$Δ_{\\text{Al}}$ phonons, demonstrating the recovery is strongly dependent on the proximity of niobium to the junction."}
{"id": "2602.24178", "categories": ["cs.LG", "cs.CC"], "pdf": "https://arxiv.org/pdf/2602.24178", "abs": "https://arxiv.org/abs/2602.24178", "authors": ["Adam R. Klivans", "Konstantinos Stavropoulos", "Arsen Vasilyan"], "title": "Sandwiching Polynomials for Geometric Concepts with Low Intrinsic Dimension", "comment": "30 pages", "summary": "Recent work has shown the surprising power of low-degree sandwiching polynomial approximators in the context of challenging learning settings such as learning with distribution shift, testable learning, and learning with contamination. A pair of sandwiching polynomials approximate a target function in expectation while also providing pointwise upper and lower bounds on the function's values. In this paper, we give a new method for constructing low-degree sandwiching polynomials that yield greatly improved degree bounds for several fundamental function classes and marginal distributions. In particular, we obtain degree $\\mathrm{poly}(k)$ sandwiching polynomials for functions of $k$ halfspaces under the Gaussian distribution, improving exponentially over the prior $2^{O(k)}$ bound. More broadly, our approach applies to function classes that are low-dimensional and have smooth boundary.\n  In contrast to prior work, our proof is relatively simple and directly uses the smoothness of the target function's boundary to construct sandwiching Lipschitz functions, which are amenable to results from high-dimensional approximation theory. For low-dimensional polynomial threshold functions (PTFs) with respect to Gaussians, we obtain doubly exponential improvements without applying the FT-mollification method of Kane used in the best previous result."}
{"id": "2602.23989", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.23989", "abs": "https://arxiv.org/abs/2602.23989", "authors": ["Huimei Liu", "Giniyat Khaliullin"], "title": "Triplon-mediated pairing and the superconducting gap structure in bilayer nickelates", "comment": "6 pages, 4 figures", "summary": "We investigate the superconducting gap structure in bilayer nickelates within a model where conduction bands of dx2-y2 symmetry coexist with localized d3z2-r2 spins. Strong interlayer coupling drives the local moments into a singlet ground state, whose virtual singlet-triplet excitations (\"triplons\") mediate the pairing interaction between conduction electrons. This yields interband s+- pairing, with opposite signs of the order parameter on the bonding beta and antibonding alpha bands. Our theory naturally explains two key experimental features: a larger gap on the alpha band despite its smaller density of states, and pronounced gap anisotropy arising from momentum-dependent nonlocal Kondo coupling. These results support triplon-mediated pairing as the microscopic origin of superconductivity in bilayer nickelates."}
{"id": "2602.23446", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23446", "abs": "https://arxiv.org/abs/2602.23446", "authors": ["Alejandro Rodriguez Dominguez"], "title": "Human Supervision as an Information Bottleneck: A Unified Theory of Error Floors in Human-Guided Learning", "comment": "Proceedings from IEEE CAI 2026, Conference on Artificial Intelligence, 8-10 May, Granada, Spain. 8 Pages, 3 Figures, 7 Tables", "summary": "Large language models are trained primarily on human-generated data and feedback, yet they exhibit persistent errors arising from annotation noise, subjective preferences, and the limited expressive bandwidth of natural language. We argue that these limitations reflect structural properties of the supervision channel rather than model scale or optimization. We develop a unified theory showing that whenever the human supervision channel is not sufficient for a latent evaluation target, it acts as an information-reducing channel that induces a strictly positive excess-risk floor for any learner dominated by it. We formalize this Human-Bounded Intelligence limit and show that across six complementary frameworks (operator theory, PAC-Bayes, information theory, causal inference, category theory, and game-theoretic analyses of reinforcement learning from human feedback), non-sufficiency yields strictly positive lower bounds arising from the same structural decomposition into annotation noise, preference distortion, and semantic compression. The theory explains why scaling alone cannot eliminate persistent human-aligned errors and characterizes conditions under which auxiliary non-human signals (e.g., retrieval, program execution, tools) increase effective supervision capacity and collapse the floor by restoring information about the latent target. Experiments on real preference data, synthetic known-target tasks, and externally verifiable benchmarks confirm the predicted structural signatures: human-only supervision exhibits a persistent floor, while sufficiently informative auxiliary channels strictly reduce or eliminate excess error."}
{"id": "2602.23751", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.23751", "abs": "https://arxiv.org/abs/2602.23751", "authors": ["Morteza Zarei", "Mohammad Hossein Zarei"], "title": "Spin stiffness and resilience phase transition in a noisy toric-rotor code", "comment": "11 pages, 7 figures, Submitted to Physical Review A", "summary": "We use a quantum formalism for the partition function of the classical $XY$ model to identify a resilience phase transition in a noisy toric-rotor code. Specifically, we consider the toric-rotor code under phase-shift noise described by a von Mises probability distribution and show that the fidelity between the final state after noise and the initial state is proportional to the partition function of the $XY$ model. We map the temperature of the $XY$ model to the width of the noise in the toric-rotor code, such that a Kosterlitz--Thouless phase transition at a critical temperature $T_{c}$ corresponds to a mixed-state phase transition at a critical width $σ_c$. To characterize this phase transition, we develop a quantum formalism for the spin stiffness in the $XY$ model and show that it is mapped to the gate fidelity in the logical subspace of the toric-rotor code. In particular, we introduce a topological order parameter that characterizes the resilience of the toric-rotor code to decoherence within the logical subspace. We show that the logical subspace does not exhibit complete resilience to noise, which is a necessary condition for correctability. However, it exhibits partial resilience to noise for widths less than $σ_c\\approx 0.89$, where the resilience order parameter takes values near $1$ and then drops to zero at $σ_c$. We also use our results to shed light on the correctability of toric-rotor codes in higher dimensions $d > 2$. Our work shows that the quantum formalism for partition functions provides a mathematically rigorous framework for studying correctability in continuous-variable quantum codes."}
{"id": "2602.23605", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23605", "abs": "https://arxiv.org/abs/2602.23605", "authors": ["Zongzhe Xu", "Zitao Shuai", "Eideen Mozaffari", "Ravi S. Aysola", "Rajesh Kumar", "Yuzhe Yang"], "title": "SleepLM: Natural-Language Intelligence for Human Sleep", "comment": null, "summary": "We present SleepLM, a family of sleep-language foundation models that enable human sleep alignment, interpretation, and interaction with natural language. Despite the critical role of sleep, learning-based sleep analysis systems operate in closed label spaces (e.g., predefined stages or events) and fail to describe, query, or generalize to novel sleep phenomena. SleepLM bridges natural language and multimodal polysomnography, enabling language-grounded representations of sleep physiology. To support this alignment, we introduce a multilevel sleep caption generation pipeline that enables the curation of the first large-scale sleep-text dataset, comprising over 100K hours of data from more than 10,000 individuals. Furthermore, we present a unified pretraining objective that combines contrastive alignment, caption generation, and signal reconstruction to better capture physiological fidelity and cross-modal interactions. Extensive experiments on real-world sleep understanding tasks verify that SleepLM outperforms state-of-the-art in zero-shot and few-shot learning, cross-modal retrieval, and sleep captioning. Importantly, SleepLM also exhibits intriguing capabilities including language-guided event localization, targeted insight generation, and zero-shot generalization to unseen tasks. All code and data will be open-sourced."}
{"id": "2602.23625", "categories": ["quant-ph", "hep-th"], "pdf": "https://arxiv.org/pdf/2602.23625", "abs": "https://arxiv.org/abs/2602.23625", "authors": ["N. L. Diaz"], "title": "From quantum time to manifestly covariant QFT: on the need for a quantum-action-based quantization", "comment": "27 pages, 2 figures", "summary": "In quantum time (QT) schemes, time is promoted to a degree of freedom, allowing Lorentz covariance to be made explicit for single particles. We ask whether this can be lifted to QFT, so that Lorentz covariance becomes manifest at the Hilbert-space level, rather than being hidden as in the standard canonical formulation. We address this question by proposing a second-quantized approach in which the elementary particle is the QT particle itself, leading naturally to the notion of spacetime field algebras and of quantum action. We show, however, that a naive many-body construction runs into inconsistencies. To pinpoint their origin we introduce a classical counterpart of the second-quantized formalism, spacetime classical mechanics (SCM), and prove a no-go theorem: Dirac quantization of SCM collapses back to standard QFT and therefore hides covariance. We circumvent this problem by presenting a quantum-action--based quantization that yields a spacetime version of quantum mechanics (SQM), making covariance manifest for (interacting) QFTs. Finally, we show that this resolution is tied to a genuine spacetime generalization of the notion of quantum state, required by causality and closely connected to recent ``states over time'' proposals and, in dS/CFT-motivated settings, to microscopic notions of timelike entanglement and emergent time."}
{"id": "2602.24051", "categories": ["cond-mat.str-el", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.24051", "abs": "https://arxiv.org/abs/2602.24051", "authors": ["Benjamin Canals"], "title": "Emergence of geometric order from topological constraints in a three-dimensional Coulomb phase", "comment": "5 pages, 4 figures", "summary": "The emergence of order and geometric limit shapes in a three-dimensional (3D) Coulomb phase subject to domain wall boundary conditions (DWBC) is investigated. While the arctic circle phenomenon -- the spatial segregation of frozen and fluctuating degrees of freedom -- is well-established in the two-dimensional six-vertex model (square ice), its extension to 3D remains largely unexplored. A cubic lattice model with Ising degrees of freedom living on the edges, whose ground state manifold is governed by a divergence-free (3-in/3-out) local constraint, is considered. In the bulk, this model realizes a classical spin liquid characterized by algebraic correlations and pinch-point singularities in reciprocal space. It is demonstrated that applying DWBC partially lifts the extensive ground state degeneracy, inducing long-range magnetic order in the thermodynamic limit. Despite this ordering, it is found that the system retains a fluctuating component that exhibits the signature of a Coulomb phase. Finally, by mapping the local vertex polarization density, compelling numerical support is provided for a 3D generalization of the arctic limit shape, bridging the gap between topological constraints and emergent geometry in higher dimensions."}
{"id": "2602.23459", "categories": ["cs.LG", "q-bio.QM", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.23459", "abs": "https://arxiv.org/abs/2602.23459", "authors": ["Eric V. Strobl"], "title": "Global Interpretability via Automated Preprocessing: A Framework Inspired by Psychiatric Questionnaires", "comment": null, "summary": "Psychiatric questionnaires are highly context sensitive and often only weakly predict subsequent symptom severity, which makes the prognostic relationship difficult to learn. Although flexible nonlinear models can improve predictive accuracy, their limited interpretability can erode clinical trust. In fields such as imaging and omics, investigators commonly address visit- and instrument-specific artifacts by extracting stable signal through preprocessing and then fitting an interpretable linear model. We adopt the same strategy for questionnaire data by decoupling preprocessing from prediction: we restrict nonlinear capacity to a baseline preprocessing module that estimates stable item values, and then learn a linear mapping from these stabilized baseline items to future severity. We refer to this two-stage method as REFINE (Redundancy-Exploiting Follow-up-Informed Nonlinear Enhancement), which concentrates nonlinearity in preprocessing while keeping the prognostic relationship transparently linear and therefore globally interpretable through a coefficient matrix, rather than through post hoc local attributions. In experiments, REFINE outperforms other interpretable approaches while preserving clear global attribution of prognostic factors across psychiatric and non-psychiatric longitudinal prediction tasks."}
{"id": "2602.24051", "categories": ["cond-mat.str-el", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.24051", "abs": "https://arxiv.org/abs/2602.24051", "authors": ["Benjamin Canals"], "title": "Emergence of geometric order from topological constraints in a three-dimensional Coulomb phase", "comment": "5 pages, 4 figures", "summary": "The emergence of order and geometric limit shapes in a three-dimensional (3D) Coulomb phase subject to domain wall boundary conditions (DWBC) is investigated. While the arctic circle phenomenon -- the spatial segregation of frozen and fluctuating degrees of freedom -- is well-established in the two-dimensional six-vertex model (square ice), its extension to 3D remains largely unexplored. A cubic lattice model with Ising degrees of freedom living on the edges, whose ground state manifold is governed by a divergence-free (3-in/3-out) local constraint, is considered. In the bulk, this model realizes a classical spin liquid characterized by algebraic correlations and pinch-point singularities in reciprocal space. It is demonstrated that applying DWBC partially lifts the extensive ground state degeneracy, inducing long-range magnetic order in the thermodynamic limit. Despite this ordering, it is found that the system retains a fluctuating component that exhibits the signature of a Coulomb phase. Finally, by mapping the local vertex polarization density, compelling numerical support is provided for a 3D generalization of the arctic limit shape, bridging the gap between topological constraints and emergent geometry in higher dimensions."}
{"id": "2602.23632", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23632", "abs": "https://arxiv.org/abs/2602.23632", "authors": ["Lun Zhan", "Feng Xiong", "Huanyong Liu", "Feng Zhang", "Yuhui Yin"], "title": "MMKG-RDS: Reasoning Data Synthesis via Deep Mining of Multimodal Knowledge Graphs", "comment": null, "summary": "Synthesizing high-quality training data is crucial for enhancing domain models' reasoning abilities. Existing methods face limitations in long-tail knowledge coverage, effectiveness verification, and interpretability. Knowledge-graph-based approaches still fall short in functionality, granularity, customizability, and evaluation. To address these issues, we propose MMKG-RDS, a flexible framework for reasoning data synthesis that leverages multimodal knowledge graphs. It supports fine-grained knowledge extraction, customizable path sampling, and multidimensional data quality scoring. We validate MMKG-RDS with the MMKG-RDS-Bench dataset, covering five domains, 17 task types, and 14,950 samples. Experimental results show fine-tuning Qwen3 models (0.6B/8B/32B) on a small number of synthesized samples improves reasoning accuracy by 9.2%. The framework also generates distinct data, challenging existing models on tasks involving tables and formulas, useful for complex benchmark construction. The dataset and code are available at https://github.com/360AILAB-NLP/MMKG-RDS"}
{"id": "2602.23650", "categories": ["quant-ph", "cond-mat.mes-hall", "hep-th"], "pdf": "https://arxiv.org/pdf/2602.23650", "abs": "https://arxiv.org/abs/2602.23650", "authors": ["Xu Zhang", "Qiang Gu"], "title": "Perfect transmission of a Dirac particle in one-dimension double square barrier", "comment": "9 pages, 5 figures", "summary": "Dirac particles can undergo perfect transmission through a sufficiently high potential barrier in the Klein zone. Although the perfect Klein tunneling (often referred to as the Klein paradox) is similar to the non-relativistic resonant transmission which occurs only when the kinetic energy exceeds the barrier, the underlying mechanism is believed to be fundamentally distinct. In this work, we show that for the relativistic double-barrier model the perfect-transmission curve can pass continuously from the above-barrier zone to the Klein zone. Additionally, in the Klein zone, perfect transmission occurs even for subcritical barrier heights, supported by both bound-state analysis and wave-packet dynamics. These findings suggest a connection between perfect Klein tunneling and resonant transmission, and provide new insights into the physical nature of the Klein paradox."}
{"id": "2602.24135", "categories": ["cond-mat.str-el", "cond-mat.mes-hall", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2602.24135", "abs": "https://arxiv.org/abs/2602.24135", "authors": ["Bingbing Wang", "Yongpan Li", "Yichen Liu", "Cheng-Cheng Liu"], "title": "Spontaneous Fully Compensated Ferrimagnetism", "comment": null, "summary": "We propose a general mechanism for the spontaneous emergence of filling-enforced fully compensated ferrimagnetism (fFIM), characterized by zero net magnetization yet ferromagnetic-like spin-split band structures. Using Hartree-Fock mean-field calculations of the Hubbard model, we map out the stability regime of spontaneous fFIM over a broad parameter space of interaction strength and staggered potential. We show the unique quantum-geometry-governed optical selection rules and the abundant valley- and spin-related physics of electronics and optics arising from the emergence of fFIM order, with tunable spin-polarized and valley-contrasting charge and spin currents. Furthermore, based on our theory, we demonstrate that spontaneous fFIM can be realized in nominally nonmagnetic graphene via defect engineering. Our results establish a unified framework for the mechanism, emergent properties, and materials realization of spontaneous fFIM, opening new opportunities for spintronic, valleytronic, and optoelectronic applications."}
{"id": "2602.23495", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23495", "abs": "https://arxiv.org/abs/2602.23495", "authors": ["Yangyi Li", "Mengdi Huai"], "title": "Uncertainty-aware Language Guidance for Concept Bottleneck Models", "comment": null, "summary": "Concept Bottleneck Models (CBMs) provide inherent interpretability by first mapping input samples to high-level semantic concepts, followed by a combination of these concepts for the final classification. However, the annotation of human-understandable concepts requires extensive expert knowledge and labor, constraining the broad adoption of CBMs. On the other hand, there are a few works that leverage the knowledge of large language models (LLMs) to construct concept bottlenecks. Nevertheless, they face two essential limitations: First, they overlook the uncertainty associated with the concepts annotated by LLMs and lack a valid mechanism to quantify uncertainty about the annotated concepts, increasing the risk of errors due to hallucinations from LLMs. Additionally, they fail to incorporate the uncertainty associated with these annotations into the learning process for concept bottleneck models. To address these limitations, we propose a novel uncertainty-aware CBM method, which not only rigorously quantifies the uncertainty of LLM-annotated concept labels with valid and distribution-free guarantees, but also incorporates quantified concept uncertainty into the CBM training procedure to account for varying levels of reliability across LLM-annotated concepts. We also provide the theoretical analysis for our proposed method. Extensive experiments on the real-world datasets validate the desired properties of our proposed methods."}
{"id": "2602.23643", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23643", "abs": "https://arxiv.org/abs/2602.23643", "authors": ["Judah Goldfeder", "Philippe Wyder", "Yann LeCun", "Ravid Shwartz Ziv"], "title": "AI Must Embrace Specialization via Superhuman Adaptable Intelligence", "comment": null, "summary": "Everyone from AI executives and researchers to doomsayers, politicians, and activists is talking about Artificial General Intelligence (AGI). Yet, they often don't seem to agree on its exact definition. One common definition of AGI is an AI that can do everything a human can do, but are humans truly general? In this paper, we address what's wrong with our conception of AGI, and why, even in its most coherent formulation, it is a flawed concept to describe the future of AI. We explore whether the most widely accepted definitions are plausible, useful, and truly general. We argue that AI must embrace specialization, rather than strive for generality, and in its specialization strive for superhuman performance, and introduce Superhuman Adaptable Intelligence (SAI). SAI is defined as intelligence that can learn to exceed humans at anything important that we can do, and that can fill in the skill gaps where humans are incapable. We then lay out how SAI can help hone a discussion around AI that was blurred by an overloaded definition of AGI, and extrapolate the implications of using it as a guide for the future."}
{"id": "2602.23664", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23664", "abs": "https://arxiv.org/abs/2602.23664", "authors": ["Benjamin Rempfer", "Parker Kuklinski", "Justin Elenewski", "Kevin Obenland"], "title": "Harmonic sequence state-preparation", "comment": null, "summary": "We demonstrate an efficient circuit to prepare a quantum state with amplitudes proportional to a harmonic sequence. We do this by first preparing a large quantum state with linearly related amplitudes and then applying a quantum Fourier transform; this has a direct analogy to the fact that the Fourier coefficients of a sawtooth wave follow a harmonic sequence. We then consider an extension of this problem by block-encoding a matrix with a harmonic sequence along its diagonal. The cost of both circuits is dominated by the costs associated with the quantum Fourier transform."}
{"id": "2602.24145", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.24145", "abs": "https://arxiv.org/abs/2602.24145", "authors": ["S. D. Semenov", "A. I. Lichtenstein", "A. N. Rubtsov"], "title": "A Unified Approach to Strong Local Correlations and Collective Fluctuations: Eliminating Divergence in the Spin Channel", "comment": null, "summary": "Dynamical mean-field theory (DMFT) provides an optimal local approximation for correlated lattice systems by mapping the lattice onto a self-consistent effective impurity model. To account for the missing long-range correlations, we propose a novel extended approach, which we term fluctuating dynamical mean-field theory (fDMFT). It incorporates collective fluctuations of auxiliary impurity models across different sites via functional integration. Technically, this method involves obtaining a family of DMFT solutions on a grid for a self-consistent auxiliary classical field applied to the lattice. While the result can, in principle, be improved diagrammatically, we find that the minimal version of the theory already yields accurate results, with lowest-order diagrammatic corrections offering only minor improvements. This consistent framework, based on our fluctuating local field concept, demonstrates superior performance for the nearly half-filled Hubbard model compared to other known diagrammatic extensions of DMFT."}
{"id": "2602.23504", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.23504", "abs": "https://arxiv.org/abs/2602.23504", "authors": ["Anik Pramanik", "Murat Kantarcioglu", "Vincent Oria", "Shantanu Sharma"], "title": "FedDAG: Clustered Federated Learning via Global Data and Gradient Integration for Heterogeneous Environments", "comment": "This paper has been accepted in ICLR 2026", "summary": "Federated Learning (FL) enables a group of clients to collaboratively train a model without sharing individual data, but its performance drops when client data are heterogeneous. Clustered FL tackles this by grouping similar clients. However, existing clustered FL approaches rely solely on either data similarity or gradient similarity; however, this results in an incomplete assessment of client similarities. Prior clustered FL approaches also restrict knowledge and representation sharing to clients within the same cluster. This prevents cluster models from benefiting from the diverse client population across clusters. To address these limitations, FedDAG introduces a clustered FL framework, FedDAG, that employs a weighted, class-wise similarity metric that integrates both data and gradient information, providing a more holistic measure of similarity during clustering. In addition, FedDAG adopts a dual-encoder architecture for cluster models, comprising a primary encoder trained on its own clients' data and a secondary encoder refined using gradients from complementary clusters. This enables cross-cluster feature transfer while preserving cluster-specific specialization. Experiments on diverse benchmarks and data heterogeneity settings show that FedDAG consistently outperforms state-of-the-art clustered FL baselines in accuracy."}
{"id": "2602.23668", "categories": ["cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.23668", "abs": "https://arxiv.org/abs/2602.23668", "authors": ["Yihan", "Wen", "Xin Chen"], "title": "PseudoAct: Leveraging Pseudocode Synthesis for Flexible Planning and Action Control in Large Language Model Agents", "comment": null, "summary": "Large language model (LLM) agents typically rely on reactive decision-making paradigms such as ReAct, selecting actions conditioned on growing execution histories. While effective for short tasks, these approaches often lead to redundant tool usage, unstable reasoning, and high token consumption in complex long-horizon tasks involving branching, iteration, or multi-tool coordination. To address these limitations, this paper introduces PseudoAct, a novel framework for flexible planning and action control in LLM agents through pseudocode synthesis. Leveraging the ability of LLMs to express task-solving strategies as code, PseudoAct synthesizes a structured pseudocode plan that decomposes a task into subtasks and explicitly encodes control flow, including sequencing, conditionals, loops, parallel composition, and combinations of these logic primitives. Actions are then executed by following this global plan, making the decision logic explicit and temporally coherent. This design reduces redundant actions, prevents infinite loops, and avoids uninformative alternative exploration, enabling consistent and efficient long-horizon decision-making. Experiments on benchmark datasets show that our method significantly outperforms existing reactive agent approaches, achieving a 20.93% absolute gain in success rate on FEVER and setting a new state-of-the-art on HotpotQA."}
{"id": "2602.23687", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23687", "abs": "https://arxiv.org/abs/2602.23687", "authors": ["Daichi Kagamihara", "Shunji Tsuchiya"], "title": "Stabilizer Rényi entropy of 3-uniform hypergraph states", "comment": "10 pages, 3 figures", "summary": "Nonstabilizerness, also known as magic, plays a central role in universal quantum computation. Hypergraph states are nonstabilizer generalizations of graph states and constitute a key class of quantum states in various areas of quantum physics, such as the demonstration of quantum advantage, measurement-based quantum computation, and the study of topological phases. In this work, we investigate nonstabilizerness of 3-uniform hypergraph states, which are solely generated by controlled-controlled-Z gates, in terms of the stabilizer Rényi entropy (SRE). We find that the SRE of 3-uniform hypergraph states can be expressed using the matrix rank, which reduces computational cost from $\\mathcal{O}(2^{3N})$ to $\\mathcal{O}(N^3 2^{N})$ for $N$-qubit states. Based on this result, we exactly evaluate SREs of one-dimensional hypergraph states. We also present numerical results of SREs of several large-scale 3-uniform hypergraph states. Our results would contribute to an understanding of the role of nonstabilizerness in a wide range of physical settings where hypergraph states are employed."}
{"id": "2602.24203", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.24203", "abs": "https://arxiv.org/abs/2602.24203", "authors": ["Md Zahid Ansari", "Souvik Kundu", "Kedar Damle"], "title": "Vacancy-induced local moments in quantum paramagnetic phases: An SU($N$) designer Hamiltonian study", "comment": "11 pages, 9 figures", "summary": "We explore the effects of non-magnetic impurities (vacancy disorder) on the quantum paramagnetic phases stabilized by SU($N$) designer Hamiltonians on bipartite lattices. Using the results of our quantum Monte Carlo simulations, we demonstrate that isolated vacancies seed emergent spin $S=1/2$ moments in their vicinity when the low-temperature state has valence bond solid order. Indeed, our quantum Monte Carlo results for the low-temperature susceptibility in such regimes shows clear evidence of the vacancy-induced Curie tails associated with these emergent moments, and our zero-temperature projector Monte Carlo results on the ground-state wavefunction in the valence bond basis provide additional evidence in support of this picture. Further, for such designer Hamiltonians on the Lieb lattice with two additional sites on each bond of a square lattice, we identify a low-temperature spin liquid-like regime with no sign of spin or valence bond order. This liquid-like regime serves as a test bed for validating a recently-developed argument concerning the effects of vacancy disorder in such low temperature regimes. Consistent with this argument, we find that isolated vacancies do not seed emergent local moments in such spin liquids. Instead, in the presence of vacancy disorder, emergent local moments are associated with the presence of monomers in maximum-density dimer packings of the corresponding diluted lattice."}
{"id": "2602.23507", "categories": ["cs.LG", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.23507", "abs": "https://arxiv.org/abs/2602.23507", "authors": ["Diana Shamsutdinova", "Felix Zimmer", "Oyebayo Ridwan Olaniran", "Sarah Markham", "Daniel Stahl", "Gordon Forbes", "Ewan Carr"], "title": "Sample Size Calculations for Developing Clinical Prediction Models: Overview and pmsims R package", "comment": "26 pages, 4 figures, 1 table, preprint", "summary": "Background: Clinical prediction models are increasingly used to inform healthcare decisions, but determining the minimum sample size for their development remains a critical and unresolved challenge. Inadequate sample sizes can lead to overfitting, poor generalisability, and biased predictions. Existing approaches, such as heuristic rules, closed-form formulas, and simulation-based methods, vary in flexibility and accuracy, particularly for complex data structures and machine learning models. Methods: We review current methodologies for sample size estimation in prediction modelling and introduce a conceptual framework that distinguishes between mean-based and assurance-based criteria. Building on this, we propose a novel simulation-based approach that integrates learning curves, Gaussian Process optimisation, and assurance principles to identify sample sizes that achieve target performance with high probability. This approach is implemented in pmsims, an open-source, model-agnostic R package. Results: Through case studies, we demonstrate that sample size estimates vary substantially across methods, performance metrics, and modelling strategies. Compared to existing tools, pmsims provides flexible, efficient, and interpretable solutions that accommodate diverse models and user-defined metrics while explicitly accounting for variability in model performance. Conclusions: Our framework and software advance sample size methodology for clinical prediction modelling by combining flexibility with computational efficiency. Future work should extend these methods to hierarchical and multimodal data, incorporate fairness and stability metrics, and address challenges such as missing data and complex dependency structures."}
{"id": "2602.23681", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23681", "abs": "https://arxiv.org/abs/2602.23681", "authors": ["Siyuan Ma", "Bo Gao", "Xiaojun Jia", "Simeng Qin", "Tianlin Li", "Ke Ma", "Xiaoshuang Jia", "Wenqi Ren", "Yang Liu"], "title": "ODAR: Principled Adaptive Routing for LLM Reasoning via Active Inference", "comment": null, "summary": "The paradigm of large language model (LLM) reasoning is shifting from parameter scaling to test-time compute scaling, yet many existing approaches still rely on uniform brute-force sampling (for example, fixed best-of-N or self-consistency) that is costly, hard to attribute, and can trigger overthinking with diminishing returns. We propose ODAR-Expert, an adaptive routing framework that optimizes the accuracy-efficiency trade-off via principled resource allocation. ODAR uses a difficulty estimator grounded in amortized active inference to dynamically route queries between a heuristic Fast Agent and a deliberative Slow Agent. We further introduce a free-energy-principled, risk-sensitive fusion mechanism that selects answers by minimizing a variational free energy objective, balancing log-likelihood with epistemic uncertainty (varentropy) as a principled alternative to ad hoc voting over heterogeneous candidates. Extensive evaluation across 23 benchmarks shows strong and consistent gains, including 98.2% accuracy on MATH and 54.8% on Humanity's Last Exam (HLE), while improving the compute-accuracy frontier under compute-matched settings. We also validate reproducibility on a fully open-source stack (Llama 4 + DeepSeek), where ODAR surpasses homogeneous sampling strategies while reducing computational costs by 82%. Overall, our results suggest that thinking-optimal scaling requires adaptive resource allocation with free-energy-based decision-making rather than simply increasing test-time compute."}
{"id": "2602.23718", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23718", "abs": "https://arxiv.org/abs/2602.23718", "authors": ["E. B. Fel'dman", "S. I. Doronin", "E. I. Kuznetsova", "A. I. Zenchuk"], "title": "Teleportation via spin-1/2 chain in solid-state quantum architecture", "comment": "13 pages", "summary": "We propose the protocol for preparing the maximally entangled Bell state between remote qubits at the ends of the spin-1/2 chain governed by the specially engineered nearest-neighbor XX-Hamiltonian with excited central spin as the initial state. This method does not require including optical constituent in the teleportation protocol and can be implemented in the quantum devices with solid-state architecture for teleporting unknown states or organizing quantum gates between remote qubits. A superconducting flux-qubit chain is an example of such devises."}
{"id": "2602.24008", "categories": ["cond-mat.stat-mech", "cond-mat.quant-gas", "cond-mat.str-el", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.24008", "abs": "https://arxiv.org/abs/2602.24008", "authors": ["Kazuya Fujimoto", "Taiki Ishiyama", "Taiga Kurose", "Takato Yoshimura", "Tomohiro Sasamoto"], "title": "Exact Anomalous Current Fluctuations in Quantum Many-Body Dynamics", "comment": "35 pages, 4 figures", "summary": "Fluctuations of integrated currents have attracted considerable interest over the past decades in the context of statistical mechanics. Recently, anomalous current fluctuations, characterized by the M-Wright function, were obtained exactly in a classical automaton [$Ž$. Krajnik et al., Phys. Rev. Lett. 128, 160601 (2022)], and previous studies have shown that the anomalous behavior can arise in a variety of classical systems. Despite the rapidly growing interest in such anomalous behaviors, which capture a universal aspect of one-dimensional many-body transport, the exact derivation of the M-Wright function in quantum many-body systems has remained elusive. In this Letter, we present the first exact microscopic derivation of the M-Wright function in quantum many-body dynamics by analyzing the integrated spin current in a one-dimensional Fermi-Hubbard model with infinitely strong repulsive interactions. Our results lay the groundwork for exploring anomalous integrated currents in a broad class of quantum many-body systems."}
{"id": "2602.23528", "categories": ["cs.LG", "cs.CE", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.23528", "abs": "https://arxiv.org/abs/2602.23528", "authors": ["Yicen Li", "Jose Antonio Lara Benitez", "Ruiyang Hong", "Anastasis Kratsios", "Paul David McNicholas", "Maarten Valentijn de Hoop"], "title": "Neural Operators Can Discover Functional Clusters", "comment": null, "summary": "Operator learning is reshaping scientific computing by amortizing inference across infinite families of problems. While neural operators (NOs) are increasingly well understood for regression, far less is known for classification and its unsupervised analogue: clustering. We prove that sample-based neural operators can learn any finite collection of classes in an infinite-dimensional reproducing kernel Hilbert space, even when the classes are neither convex nor connected, under mild kernel sampling assumptions. Our universal clustering theorem shows that any $K$ closed classes can be approximated to arbitrary precision by NO-parameterized classes in the upper Kuratowski topology on closed sets, a notion that can be interpreted as disallowing false-positive misclassifications.\n  Building on this, we develop an NO-powered clustering pipeline for functional data and apply it to unlabeled families of ordinary differential equation (ODE) trajectories. Discretized trajectories are lifted by a fixed pre-trained encoder into a continuous feature map and mapped to soft assignments by a lightweight trainable head. Experiments on diverse synthetic ODE benchmarks show that the resulting practical SNO recovers latent dynamical structure in regimes where classical methods fail, providing evidence consistent with our universal clustering theory."}
{"id": "2602.23701", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.23701", "abs": "https://arxiv.org/abs/2602.23701", "authors": ["Yawen Wang", "Wenjie Wu", "Junjie Wang", "Qing Wang"], "title": "From Flat Logs to Causal Graphs: Hierarchical Failure Attribution for LLM-based Multi-Agent Systems", "comment": null, "summary": "LLM-powered Multi-Agent Systems (MAS) have demonstrated remarkable capabilities in complex domains but suffer from inherent fragility and opaque failure mechanisms. Existing failure attribution methods, whether relying on direct prompting, costly replays, or supervised fine-tuning, typically treat execution logs as flat sequences. This linear perspective fails to disentangle the intricate causal links inherent to MAS, leading to weak observability and ambiguous responsibility boundaries. To address these challenges, we propose CHIEF, a novel framework that transforms chaotic trajectories into a structured hierarchical causal graph. It then employs hierarchical oracle-guided backtracking to efficiently prune the search space via sybthesized virtual oracles. Finally, it implements counterfactual attribution via a progressive causal screening strategy to rigorously distinguish true root causes from propagated symptoms. Experiments on Who&When benchmark show that CHIEF outperforms eight strong and state-of-the-art baselines on both agent- and step-level accuracy. Ablation studies further confirm the critical role of each proposed module."}
{"id": "2602.23751", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.23751", "abs": "https://arxiv.org/abs/2602.23751", "authors": ["Morteza Zarei", "Mohammad Hossein Zarei"], "title": "Spin stiffness and resilience phase transition in a noisy toric-rotor code", "comment": "11 pages, 7 figures, Submitted to Physical Review A", "summary": "We use a quantum formalism for the partition function of the classical $XY$ model to identify a resilience phase transition in a noisy toric-rotor code. Specifically, we consider the toric-rotor code under phase-shift noise described by a von Mises probability distribution and show that the fidelity between the final state after noise and the initial state is proportional to the partition function of the $XY$ model. We map the temperature of the $XY$ model to the width of the noise in the toric-rotor code, such that a Kosterlitz--Thouless phase transition at a critical temperature $T_{c}$ corresponds to a mixed-state phase transition at a critical width $σ_c$. To characterize this phase transition, we develop a quantum formalism for the spin stiffness in the $XY$ model and show that it is mapped to the gate fidelity in the logical subspace of the toric-rotor code. In particular, we introduce a topological order parameter that characterizes the resilience of the toric-rotor code to decoherence within the logical subspace. We show that the logical subspace does not exhibit complete resilience to noise, which is a necessary condition for correctability. However, it exhibits partial resilience to noise for widths less than $σ_c\\approx 0.89$, where the resilience order parameter takes values near $1$ and then drops to zero at $σ_c$. We also use our results to shed light on the correctability of toric-rotor codes in higher dimensions $d > 2$. Our work shows that the quantum formalism for partition functions provides a mathematically rigorous framework for studying correctability in continuous-variable quantum codes."}
{"id": "2602.24242", "categories": ["cond-mat.stat-mech", "cond-mat.quant-gas", "cond-mat.str-el", "nlin.SI"], "pdf": "https://arxiv.org/pdf/2602.24242", "abs": "https://arxiv.org/abs/2602.24242", "authors": ["Takato Yoshimura", "Žiga Krajnik", "Alvise Bastianello", "Enej Ilievski"], "title": "Anomalous hydrodynamic fluctuations in the quantum XXZ spin chain", "comment": "9+2 pages, 3 figures", "summary": "The quantum XXZ spin-1/2 chain features non-Gaussian spin current fluctuations in the regime of easy-axis anisotropy. Using ballistic macroscopic fluctuation theory, we derive the exact probability distribution of typical spin-current fluctuations in thermal equilibrium. The obtained nested Gaussian distribution is fully characterized by its variance which we analytically relate to the spin diffusion constant and static spin susceptibility, and compare our with numerical simulations. By unveiling how the same mechanism which leads to anomalous charge current fluctuations in single-file systems manifests in the XXZ chain, our approach establishes the universal hydrodynamic origin of the observed anomalous fluctuations."}
{"id": "2602.23529", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23529", "abs": "https://arxiv.org/abs/2602.23529", "authors": ["Martin Černý", "David Sychrovský", "Filip Úradník", "Jakub Černý"], "title": "Active Value Querying to Minimize Additive Error in Subadditive Set Function Learning", "comment": null, "summary": "Subadditive set functions play a pivotal role in computational economics (especially in combinatorial auctions), combinatorial optimization or artificial intelligence applications such as interpretable machine learning. However, specifying a set function requires assigning values to an exponentially large number of subsets in general, a task that is often resource-intensive in practice, particularly when the values derive from external sources such as retraining of machine learning models. A~simple omission of certain values introduces ambiguity that becomes even more significant when the incomplete set function has to be further optimized over. Motivated by the well-known result about inapproximability of subadditive functions using deterministic value queries with respect to a multiplicative error, we study a problem of approximating an unknown subadditive (or a subclass of thereof) set function with respect to an additive error -- i. e., we aim to efficiently close the distance between minimal and maximal completions. Our contributions are threefold: (i) a thorough exploration of minimal and maximal completions of different classes of set functions with missing values and an analysis of their resulting distance; (ii) the development of methods to minimize this distance over classes of set functions with a known prior, achieved by disclosing values of additional subsets in both offline and online manner; and (iii) empirical demonstrations of the algorithms' performance in practical scenarios."}
{"id": "2602.23716", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23716", "abs": "https://arxiv.org/abs/2602.23716", "authors": ["Jiangyuan Wang", "Kejun Xiao", "Huaipeng Zhao", "Tao Luo", "Xiaoyi Zeng"], "title": "ProductResearch: Training E-Commerce Deep Research Agents via Multi-Agent Synthetic Trajectory Distillation", "comment": null, "summary": "Large Language Model (LLM)-based agents show promise for e-commerce conversational shopping, yet existing implementations lack the interaction depth and contextual breadth required for complex product research. Meanwhile, the Deep Research paradigm, despite advancing information synthesis in web search, suffers from domain gaps when transferred to e-commerce. We propose ProductResearch, a multi-agent framework that synthesizes high-fidelity, long-horizon tool-use trajectories for training robust e-commerce shopping agents. The framework employs a User Agent to infer nuanced shopping intents from behavioral histories, and a Supervisor Agent that orchestrates iterative collaboration with a Research Agent to generate synthetic trajectories culminating in comprehensive, insightful product research reports. These trajectories are rigorously filtered and distilled through a reflective internalization process that consolidates multi-agent supervisory interactions into coherent single-role training examples, enabling effective fine-tuning of LLM agents for complex shopping inquiries. Extensive experiments show that a compact MoE model fine-tuned on our synthetic data achieves substantial improvements over its base model in response comprehensiveness, research depth, and user-perceived utility, approaching the performance of frontier proprietary deep research systems and establishing multi-agent synthetic trajectory training as an effective and scalable paradigm for enhancing LLM-based shopping assistance."}
{"id": "2602.23764", "categories": ["quant-ph", "math-ph", "math.CV"], "pdf": "https://arxiv.org/pdf/2602.23764", "abs": "https://arxiv.org/abs/2602.23764", "authors": ["Snehasis Bera", "Sourav Das", "Abhijit Banerjee"], "title": "A new class of coherent states involving Fox-Wright functions and their generalization in the bicomplex framework", "comment": null, "summary": "In this work, an extensive class of coherent states is introduced by taking the Fox Wright function as the normalization function. It is demonstrated that these states satisfy the key requirements of continuity, normalizability and resolution of unity. Furthermore, coherent states associated with the continuous spectrum are obtained through a discrete to continuous limiting procedure. Moreover, FW generalized multi parameter nu function is introduced and shown to act as the normalization function for the Fox Wright coherent states in the continuous spectrum. Later the Fox Wright function with bicomplex arguments has been introduced and its existence has been investigated. Bicomplex Fox Wright coherent states are also developed for the discrete spectrum based on this new function and their properties are analyzed. Subsequently, the results regarding Fox Wright coherent states are generalized to the bicomplex setting. In addition, a bicomplex FW generalized multi-parameter nu function is defined to demonstrate that it provides the normalization for these states in the continuous spectrum."}
{"id": "2602.23556", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.MA", "cs.PF"], "pdf": "https://arxiv.org/pdf/2602.23556", "abs": "https://arxiv.org/abs/2602.23556", "authors": ["Aishwarya Sarkar", "Sayan Ghosh", "Nathan Tallent", "Aman Chadha", "Tanya Roosta", "Ali Jannesari"], "title": "Rudder: Steering Prefetching in Distributed GNN Training using LLM Agents", "comment": "Accepted to the 40th ACM International Conference on Supercomputing (ICS 2026)", "summary": "Large-scale Graph Neural Networks (GNNs) are typically trained by sampling a vertex's neighbors to a fixed distance. Because large input graphs are distributed, training requires frequent irregular communication that stalls forward progress. Moreover, fetched data changes with graph, graph distribution, sample and batch parameters, and caching polices. Consequently, any static prefetching method will miss crucial opportunities to adapt to different dynamic conditions. In this paper, we introduce Rudder, a software module embedded in the state-of-the-art AWS DistDGL framework, to autonomously prefetch remote nodes and minimize communication. Rudder's adaptation contrasts with both standard heuristics and traditional ML classifiers. We observe that the generative AI found in contemporary Large Language Models (LLMs) exhibits emergent properties like In-Context Learning (ICL) for zero-shot tasks, with logical multi-step reasoning. We find this behavior well-suited for adaptive control even with substantial undertraining. Evaluations using standard datasets and unseen configurations on the NERSC Perlmutter supercomputer show up to 91% improvement in end-to-end training performance over baseline DistDGL (no prefetching), and an 82% improvement over static prefetching, reducing communication by over 50%. Our code is available at https://github.com/aishwaryyasarkar/rudder-llm-agent."}
{"id": "2602.23720", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23720", "abs": "https://arxiv.org/abs/2602.23720", "authors": ["Sheng Cao", "Zhao Chang", "Chang Li", "Hannan Li", "Liyao Fu", "Ji Tang"], "title": "The Auton Agentic AI Framework", "comment": null, "summary": "The field of Artificial Intelligence is undergoing a transition from Generative AI -- probabilistic generation of text and images -- to Agentic AI, in which autonomous systems execute actions within external environments on behalf of users. This transition exposes a fundamental architectural mismatch: Large Language Models (LLMs) produce stochastic, unstructured outputs, whereas the backend infrastructure they must control -- databases, APIs, cloud services -- requires deterministic, schema-conformant inputs. The present paper describes the Auton Agentic AI Framework, a principled architecture for standardizing the creation, execution, and governance of autonomous agent systems. The framework is organized around a strict separation between the Cognitive Blueprint, a declarative, language-agnostic specification of agent identity and capabilities, and the Runtime Engine, the platform-specific execution substrate that instantiates and runs the agent. This separation enables cross-language portability, formal auditability, and modular tool integration via the Model Context Protocol (MCP). The paper formalizes the agent execution model as an augmented Partially Observable Markov Decision Process (POMDP) with a latent reasoning space, introduces a hierarchical memory consolidation architecture inspired by biological episodic memory systems, defines a constraint manifold formalism for safety enforcement via policy projection rather than post-hoc filtering, presents a three-level self-evolution framework spanning in-context adaptation through reinforcement learning, and describes runtime optimizations -- including parallel graph execution, speculative inference, and dynamic context pruning -- that reduce end-to-end latency for multi-step agent workflows."}
{"id": "2602.23773", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23773", "abs": "https://arxiv.org/abs/2602.23773", "authors": ["Ying Chen", "Hongwei Yu", "Jiawei Hu"], "title": "Entanglement dynamics for atoms near a reflecting boundary: enhancement and suppression by environment-induced interactions", "comment": null, "summary": "We investigate how environment-induced interactions influence the entanglement dynamics of two static atoms placed near a perfectly reflecting boundary. In this setting, the environment-induced interactions include both atom-boundary contributions (position-dependent Lamb shifts) and the induced atom-atom interaction mediated by the field. We show that, regardless of the initial two-atom state, the entanglement dynamics differs qualitatively and quantitatively from predictions that neglect these energy-shift effects. Depending on the geometry and parameter regime, the environment-induced interactions can either enhance entanglement generation -- yielding a larger maximum concurrence and a longer entanglement lifetime -- or suppress it, reducing both the peak concurrence and the survival time. This behavior contrasts sharply with the free-space case, where the environment-induced atom-atom interaction affects entanglement generation only for a restricted class of initial states and does so in an exclusively assisting manner."}
{"id": "2602.23565", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.23565", "abs": "https://arxiv.org/abs/2602.23565", "authors": ["Adhyyan Narang", "Sarah Dean", "Lillian J Ratliff", "Maryam Fazel"], "title": "Dynamics of Learning under User Choice: Overspecialization and Peer-Model Probing", "comment": null, "summary": "In many economically relevant contexts where machine learning is deployed, multiple platforms obtain data from the same pool of users, each of whom selects the platform that best serves them. Prior work in this setting focuses exclusively on the \"local\" losses of learners on the distribution of data that they observe. We find that there exist instances where learners who use existing algorithms almost surely converge to models with arbitrarily poor global performance, even when models with low full-population loss exist. This happens through a feedback-induced mechanism, which we call the overspecialization trap: as learners optimize for users who already prefer them, they become less attractive to users outside this base, which further restricts the data they observe. Inspired by the recent use of knowledge distillation in modern ML, we propose an algorithm that allows learners to \"probe\" the predictions of peer models, enabling them to learn about users who do not select them. Our analysis characterizes when probing succeeds: this procedure converges almost surely to a stationary point with bounded full-population risk when probing sources are sufficiently informative, e.g., a known market leader or a majority of peers with good global performance. We verify our findings with semi-synthetic experiments on the MovieLens, Census, and Amazon Sentiment datasets."}
{"id": "2602.23730", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23730", "abs": "https://arxiv.org/abs/2602.23730", "authors": ["Longyin Zhang", "Shuo Sun", "Yingxu He", "Won Cheng Yi Lewis", "Muhammad Huzaifah Bin Md Shahrin", "Hardik Bhupendra Sailor", "Heng Meng Jeremy Wong", "Tarun Kumar Vangani", "Yi Ma", "Qiongqiong Wang", "Minh Duc Pham", "Ridong Jiang", "Jingtao Li", "Jingyi Liao", "Zhuohan Liu", "Yanfeng Lu", "Manas Gupta", "Ai Ti Aw"], "title": "Unlocking Cognitive Capabilities and Analyzing the Perception-Logic Trade-off", "comment": null, "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) pursue omni-perception capabilities, yet integrating robust sensory grounding with complex reasoning remains a challenge, particularly for underrepresented regions. In this report, we introduce the research preview of MERaLiON2-Omni (Alpha), a 10B-parameter multilingual omni-perception tailored for Southeast Asia (SEA). We present a progressive training pipeline that explicitly decouples and then integrates \"System 1\" (Perception) and \"System 2\" (Reasoning) capabilities. First, we establish a robust Perception Backbone by aligning region-specific audio-visual cues (e.g., Singlish code-switching, local cultural landmarks) with a multilingual LLM through orthogonal modality adaptation. Second, to inject cognitive capabilities without large-scale supervision, we propose a cost-effective Generate-Judge-Refine pipeline. By utilizing a Super-LLM to filter hallucinations and resolve conflicts via a consensus mechanism, we synthesize high-quality silver data that transfers textual Chain-of-Thought reasoning to multimodal scenarios.\n  Comprehensive evaluation on our newly introduced SEA-Omni Benchmark Suite reveals an Efficiency-Stability Paradox: while reasoning acts as a non-linear amplifier for abstract tasks (boosting mathematical and instruction-following performance significantly), it introduces instability in low-level sensory processing. Specifically, we identify Temporal Drift in long-context audio, where extended reasoning desynchronizes the model from acoustic timestamps, and Visual Over-interpretation, where logic overrides pixel-level reality. This report details the architecture, the data-efficient training recipe, and a diagnostic analysis of the trade-offs between robust perception and structured reasoning."}
{"id": "2602.23848", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23848", "abs": "https://arxiv.org/abs/2602.23848", "authors": ["Yusuke Kimura", "Yutaka Takita"], "title": "MAFFT-inspired Quantum Shift-based Sequence Alignment and its Efficient Simulation on Decision Diagrams", "comment": "11 pages", "summary": "Multiple sequence alignment (MSA) is a core operation for comparing genome sequences and is widely used in bio-informatics. MAFFT, a practical MSA tool, repeatedly shifts a pair of sequences and computes a distance. Because the number of sequence pairs grows quadratically with the number of sequences, this procedure can become a bottleneck.\n  We propose Quantum Shift-based Sequence Alignment (QShift-SA), which implements this ``shift-wise score computation'' as a gate-based quantum circuit and searches over shift amounts and sequence pairs using Grover algorithm. QShift-SA constructs an oracle circuit that compute the Hamming distance (the number of mismatches) between two sequences with data encoding, controlled shift, comparison, and addition. This oracle can search for candidates with small distances. QShift-SA does not aim to replace the full MSA workflow; instead, it targets the screening steps that often dominate the runtime in classical MAFFT as stated above.\n  We evaluate circuit resources (number of qubits, gate count, and depth) and benchmark simulation time across multiple quantum circuit simulators. We find that a decision diagram (DD)-based quantum circuit simulator runs more than 1,000$\\times$ faster than state-vector and MPS simulators and can handle larger circuits."}
{"id": "2602.23566", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23566", "abs": "https://arxiv.org/abs/2602.23566", "authors": ["Asiri Wijesinghe", "Sevvandi Kandanaarachchi", "Daniel M. Steinberg", "Cheng Soon Ong"], "title": "Flowette: Flow Matching with Graphette Priors for Graph Generation", "comment": "37 Pages", "summary": "We study generative modeling of graphs with recurring subgraph motifs. We propose Flowette, a continuous flow matching framework, that employs a graph neural network based transformer to learn a velocity field defined over graph representations with node and edge attributes. Our model preserves topology through optimal transport based coupling, and long-range structural dependencies through regularisation. To incorporate domain driven structural priors, we introduce graphettes, a new probabilistic family of graph structure models that generalize graphons via controlled structural edits for motifs like rings, stars and trees. We theoretically analyze the coupling, invariance, and structural properties of the proposed framework, and empirically evaluate it on synthetic and small-molecule graph generation tasks. Flowette demonstrates consistent improvements, highlighting the effectiveness of combining structural priors with flow-based training for modeling complex graph distributions."}
{"id": "2602.23777", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23777", "abs": "https://arxiv.org/abs/2602.23777", "authors": ["Zhipeng Xu", "Zilong Wang", "Xinyang Jiang", "Dongsheng Li", "De Cheng", "Nannan Wang"], "title": "Reasoning-Driven Multimodal LLM for Domain Generalization", "comment": "Accepted at ICLR 2026 (Poster)", "summary": "This paper addresses the domain generalization (DG) problem in deep learning. While most DG methods focus on enforcing visual feature invariance, we leverage the reasoning capability of multimodal large language models (MLLMs) and explore the potential of constructing reasoning chains that derives image categories to achieve more robust predictions under domain shift. To this end, we systematically study the role of reasoning in DG using DomainBed-Reasoning, a newly constructed extension of DomainBed dataset, in which each sample is paired with class-relevant reasoning chains. Our analysis reveals two key challenges: (i) fine-tuning MLLMs with reasoning chains for classification is more challenging than direct label supervision, since the model must optimize complex reasoning sequences before label prediction; and (ii) mismatches in reasoning patterns between supervision signals and fine-tuned MLLMs lead to a trade-off between semantic richness (informative but harder to optimize) and optimization efficiency (easier to optimize but less informative). To address these issues, we propose RD-MLDG (Reasoning-Driven Multimodal LLM for Domain Generalization), a framework with two components: (i) MTCT (Multi-Task Cross-Training), which introduces an additional direct classification pathway to guide reasoning supervision; and (ii) SARR (Self-Aligned Reasoning Regularization), which preserves the semantic richness of reasoning chains while mitigating reasoning-pattern mismatches via iterative self-labeling. Experiments on standard DomainBed datasets (PACS, VLCS, OfficeHome, TerraInc) demonstrate that RD-MLDG achieves state-of-the-art performances, highlighting reasoning as a promising complementary signal for robust out-of-domain generalization."}
{"id": "2602.23865", "categories": ["quant-ph", "cs.LO", "math.CT"], "pdf": "https://arxiv.org/pdf/2602.23865", "abs": "https://arxiv.org/abs/2602.23865", "authors": ["Matt Wilson", "James Hefford", "Timothée Hoffreumon"], "title": "Supermaps on generalised theories", "comment": null, "summary": "Categorical supermaps generalise higher-order quantum operations from finite-dimensional quantum theory to arbitrary circuit theories. In this paper, we establish the Yoneda lemma for categorical supermaps, which states that whenever a physical theory has a suitable notion of channel-state duality, then categorical supermaps on that theory can be concretely represented in terms of that duality. This lemma eliminates any guesswork or ambiguity when defining the appropriate notion of supermap for these theories. As a concrete application, we show that the recently proposed higher-order processes on boxworld can be obtained as a particular instance of categorical supermaps, and put forward a stable definition of higher-order real quantum theory."}
{"id": "2602.23578", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23578", "abs": "https://arxiv.org/abs/2602.23578", "authors": ["Junghoon Justin Park", "Maria Pak", "Sebin Lee", "Samuel Yen-Chi Chen", "Shinjae Yoo", "Huan-Hsin Tseng", "Jiook Cha"], "title": "Hybrid Quantum Temporal Convolutional Networks", "comment": null, "summary": "Quantum machine learning models for sequential data face scalability challenges with complex multivariate signals. We introduce the Hybrid Quantum Temporal Convolutional Network (HQTCN), which combines classical temporal windowing with a quantum convolutional neural network core. By applying a shared quantum circuit across temporal windows, HQTCN captures long-range dependencies while achieving significant parameter reduction. Evaluated on synthetic NARMA sequences and high-dimensional EEG time-series, HQTCN performs competitively with classical baselines on univariate data and outperforms all baselines on multivariate tasks. The model demonstrates particular strength under data-limited conditions, maintaining high performance with substantially fewer parameters than conventional approaches. These results establish HQTCN as a parameter-efficient approach for multivariate time-series analysis."}
{"id": "2602.23802", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23802", "abs": "https://arxiv.org/abs/2602.23802", "authors": ["Yiyang Fang", "Wenke Huang", "Pei Fu", "Yihao Yang", "Kehua Su", "Zhenbo Luo", "Jian Luan", "Mang Ye"], "title": "EMO-R3: Reflective Reinforcement Learning for Emotional Reasoning in Multimodal Large Language Models", "comment": "Accepted by CVPR 2026", "summary": "Multimodal Large Language Models (MLLMs) have shown remarkable progress in visual reasoning and understanding tasks but still struggle to capture the complexity and subjectivity of human emotions. Existing approaches based on supervised fine-tuning often suffer from limited generalization and poor interpretability, while reinforcement learning methods such as Group Relative Policy Optimization fail to align with the intrinsic characteristics of emotional cognition. To address these challenges, we propose Reflective Reinforcement Learning for Emotional Reasoning (EMO-R3), a framework designed to enhance the emotional reasoning ability of MLLMs. Specifically, we introduce Structured Emotional Thinking to guide the model to perform step-by-step emotional reasoning in a structured and interpretable manner, and design a Reflective Emotional Reward that enables the model to re-evaluate its reasoning based on visual-text consistency and emotional coherence. Extensive experiments demonstrate that EMO-R3 significantly improves both the interpretability and emotional intelligence of MLLMs, achieving superior performance across multiple visual emotional understanding benchmarks."}
{"id": "2602.23868", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23868", "abs": "https://arxiv.org/abs/2602.23868", "authors": ["Zhichen Huang", "Chunxiao Du", "Yang Zhou", "Zhisong Xiao"], "title": "Non-commutative Index of Measurement-only Entanglement Phase Transition", "comment": "10 pages, 11 figures", "summary": "Measurement-only models offer an ideal platform for exploring entanglement dynamics in the absence of unitary evolution. Despite extensive numerical evidence for entanglement phase transitions in measurement-only dynamics, the underlying mechanism attributed to non-commutativity among multi-site projective measurements has remained qualitative and coarse-grained. In this work, we identify a quantitative non-commutative index. By applying this index into three representative measurement-only models, we elucidate the role of non-commutativity in measurement-only dynamics: the emergence of a volume-law phase is governed by the non-commutative structure of the measurement ensemble, while the transition point is quantitatively determined by the amount of critical non-commutativity. More strikingly, the critical non-commutativity exhibits a universal linear scaling with the measurement range, independent of the microscopic details of the measurement ensembles. Our findings deepen the understanding of the fundamental mechanism behind the measurement-only entanglement phase transition."}
{"id": "2602.23581", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23581", "abs": "https://arxiv.org/abs/2602.23581", "authors": ["Xiang Ao"], "title": "SDMixer: Sparse Dual-Mixer for Time Series Forecasting", "comment": "12pages,2 figures", "summary": "Multivariate time series forecasting is widely applied in fields such as transportation, energy, and finance. However, the data commonly suffers from issues of multi-scale characteristics, weak correlations, and noise interference, which limit the predictive performance of existing models. This paper proposes a dual-stream sparse Mixer prediction framework that extracts global trends and local dynamic features from sequences in both the frequency and time domains, respectively. It employs a sparsity mechanism to filter out invalid information, thereby enhancing the accuracy of cross-variable dependency modeling. Experimental results demonstrate that this method achieves leading performance on multiple real-world scenario datasets, validating its effectiveness and generality. The code is available at https://github.com/SDMixer/SDMixer"}
{"id": "2602.23864", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23864", "abs": "https://arxiv.org/abs/2602.23864", "authors": ["Chao Wang", "Han Lin", "Huaze Tang", "Huijing Lin", "Wenbo Ding"], "title": "RUMAD: Reinforcement-Unifying Multi-Agent Debate", "comment": "13 pages, 3 figures", "summary": "Multi-agent debate (MAD) systems leverage collective intelligence to enhance reasoning capabilities, yet existing approaches struggle to simultaneously optimize accuracy, consensus formation, and computational efficiency. Static topology methods lack adaptability to task complexity variations, while external LLM-based coordination risks introducing privileged knowledge that compromises debate neutrality. This work presents RUMAD (Reinforcement-Unifying Multi-Agent Debate), a novel framework that formulates dynamic communication topology control in MAD as a reinforcement learning (RL) problem.\n  RUMAD employs a content-agnostic observation scheme that captures high-level debate dynamics avoiding access to raw agent reasoning content. RUMAD uses a multi-objective reward to model solution quality, cohesion and efficiency. A PPO-trained controller dynamically adjusts edge weights in the communication graph, while a dual-threshold mechanism enables fine-grained control over both agent activation and information visibility.\n  Experimental evaluation across MMLU, GSM8K, and GPQA benchmarks demonstrates that RUMAD achieves substantial efficiency gains, reducing token costs by over 80\\%, while still improving reasoning accuracy compared to single LLM model and multiple MAD baselines. Notably, RUMAD trained exclusively on MMLU exhibits robust zero-shot generalization to out-of-domain (OOD) tasks, indicating that the learned communication strategies capture task-independent principles of effective multi-agent coordination. These results establish RUMAD as a efficient and robust approach for deploying multi-agent reasoning application with practical resource constraints."}
{"id": "2602.23883", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23883", "abs": "https://arxiv.org/abs/2602.23883", "authors": ["Nripendra Majumdar"], "title": "Four Party Absolutely Maximal Contextual Correlations", "comment": "12 pages, 0 figure, 2 tables", "summary": "The Kochen Specker theorem revealed contextuality as a fundamental nonclassical feature of nature. Nonlocality arises as a special case of contextuality, where entangled states shared by space like separated parties exhibit nonlocal correlations. The notion of maximality in correlations, analogous to maximal entanglement, is less explored in multipartite systems. In our work, we have defined maximal correlations in terms of contextual models, which are analogous to absolutely maximally entangled (AME) states. Employing the sheaf theoretic framework, we introduce maximal contextual correlations associated with the corresponding maximal contextual model. The formalism introduces the contextual fraction CF as a measure of contextuality, taking values from 0 (noncontextual) to 1 (fully contextual). This enables the formulation of a new class of correlations termed absolutely maximal contextual correlations (AMCC), which are both maximally contextual and maximal marginals. In the bipartite setting, the canonical example is the Popescu Rohrlich (PR) box, while in the tripartite case, it includes Greenberger Horne Zeilinger (GHZ) correlations and three way nonlocal correlations. In this work, we extend these findings to four party correlations. Notably, no AME state exists for four qubits, which introduces a subtle difference between AMCC and AME. The construction follows the constraint satisfaction problem (CSP) and parity check methods. In particular, the explicit realization of a non AMCC correlation that is maximally contextual yet not maximal marginal is obtained within the CSP framework."}
{"id": "2602.23599", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23599", "abs": "https://arxiv.org/abs/2602.23599", "authors": ["Dang Sy Duy", "Nguyen Duy Chien", "Kapil Dev", "Jeff Nijsse"], "title": "Normalisation and Initialisation Strategies for Graph Neural Networks in Blockchain Anomaly Detection", "comment": "14 pages, 5 figures", "summary": "Graph neural networks (GNNs) offer a principled approach to financial fraud detection by jointly learning from node features and transaction graph topology. However, their effectiveness on real-world anti-money laundering (AML) benchmarks depends critically on training practices such as specifically weight initialisation and normalisation that remain underexplored. We present a systematic ablation of initialisation and normalisation strategies across three GNN architectures (GCN, GAT, and GraphSAGE) on the Elliptic Bitcoin dataset. Our experiments reveal that initialisation and normalisation are architecture-dependent: GraphSAGE achieves the strongest performance with Xavier initialisation alone, GAT benefits most from combining GraphNorm with Xavier initialisation, while GCN shows limited sensitivity to these modifications. These findings offer practical, architecture-specific guidance for deploying GNNs in AML pipelines for datasets with severe class imbalance. We release a reproducible experimental framework with temporal data splits, seeded runs, and full ablation results."}
{"id": "2602.23876", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23876", "abs": "https://arxiv.org/abs/2602.23876", "authors": ["Ning Gao", "Xiuhui Zhang", "Xingyu Jiang", "Mukang You", "Mohan Zhang", "Yue Deng"], "title": "RF-Agent: Automated Reward Function Design via Language Agent Tree Search", "comment": "39 pages, 9 tables, 11 figures, Project page see https://github.com/deng-ai-lab/RF-Agent", "summary": "Designing efficient reward functions for low-level control tasks is a challenging problem. Recent research aims to reduce reliance on expert experience by using Large Language Models (LLMs) with task information to generate dense reward functions. These methods typically rely on training results as feedback, iteratively generating new reward functions with greedy or evolutionary algorithms. However, they suffer from poor utilization of historical feedback and inefficient search, resulting in limited improvements in complex control tasks. To address this challenge, we propose RF-Agent, a framework that treats LLMs as language agents and frames reward function design as a sequential decision-making process, enhancing optimization through better contextual reasoning. RF-Agent integrates Monte Carlo Tree Search (MCTS) to manage the reward design and optimization process, leveraging the multi-stage contextual reasoning ability of LLMs. This approach better utilizes historical information and improves search efficiency to identify promising reward functions. Outstanding experimental results in 17 diverse low-level control tasks demonstrate the effectiveness of our method. The source code is available at https://github.com/deng-ai-lab/RF-Agent."}
{"id": "2602.23888", "categories": ["quant-ph", "cond-mat.supr-con"], "pdf": "https://arxiv.org/pdf/2602.23888", "abs": "https://arxiv.org/abs/2602.23888", "authors": ["Rangga P. Budoyo", "Rasanayagam S. Kajen", "Bing Wen Cheah", "Long H. Nguyen", "Rainer Dumke"], "title": "Characterization of Josephson Junction Aging and Annealing Under Different Environments", "comment": "9 pages, 4 figures", "summary": "Understanding the aging behavior of Josephson junctions and the effect of annealing on junction resistances is important in building large-scale superconducting quantum processors. Here we study the effects of aging of Josephson junctions under different storage conditions from immediately after fabrication up to 2 to 3 months. We find that the aging curve follows a logarithmic curve, with the aging amplitude mainly determined by fabrication conditions and the aging speed determined by storage conditions. Junctions stored at ambient laboratory conditions aged faster compared to junctions stored in a nitrogen atmosphere or vacuum, with the aging speed appreciably changes when the storage condition changed. We also compared the effect of thermal annealing under nitrogen environment with annealing under ambient conditions up to 250$^\\circ$ C. We find that under nitrogen environment, the resistances decreased at all temperatures tested, while under ambient environment the resistances increased at 200$^\\circ$ C and decreased at 250$^\\circ$ C instead. We were unable to decrease the resistance below the initial-time resistance, suggesting a lower limit on the range of resistance tuning."}
{"id": "2602.23614", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23614", "abs": "https://arxiv.org/abs/2602.23614", "authors": ["Kejing Yin", "Haizhou Xu", "Wenfang Yao", "Chen Liu", "Zijie Chen", "Yui Haang Cheung", "William K. Cheung", "Jing Qin"], "title": "When Does Multimodal Learning Help in Healthcare? A Benchmark on EHR and Chest X-Ray Fusion", "comment": null, "summary": "Machine learning holds promise for advancing clinical decision support, yet it remains unclear when multimodal learning truly helps in practice, particularly under modality missingness and fairness constraints. In this work, we conduct a systematic benchmark of multimodal fusion between Electronic Health Records (EHR) and chest X-rays (CXR) on standardized cohorts from MIMIC-IV and MIMIC-CXR, aiming to answer four fundamental questions: when multimodal fusion improves clinical prediction, how different fusion strategies compare, how robust existing methods are to missing modalities, and whether multimodal models achieve algorithmic fairness. Our study reveals several key insights. Multimodal fusion improves performance when modalities are complete, with gains concentrating in diseases that require complementary information from both EHR and CXR. While cross-modal learning mechanisms capture clinically meaningful dependencies beyond simple concatenation, the rich temporal structure of EHR introduces strong modality imbalance that architectural complexity alone cannot overcome. Under realistic missingness, multimodal benefits rapidly degrade unless models are explicitly designed to handle incomplete inputs. Moreover, multimodal fusion does not inherently improve fairness, with subgroup disparities mainly arising from unequal sensitivity across demographic groups. To support reproducible and extensible evaluation, we further release a flexible benchmarking toolkit that enables plug-and-play integration of new models and datasets. Together, this work provides actionable guidance on when multimodal learning helps, when it fails, and why, laying the foundation for developing clinically deployable multimodal systems that are both effective and reliable. The open-source toolkit can be found at https://github.com/jakeykj/CareBench."}
{"id": "2602.23974", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23974", "abs": "https://arxiv.org/abs/2602.23974", "authors": ["Fan Zhang", "Baoru Huang", "Xin Zhang"], "title": "Pessimistic Auxiliary Policy for Offline Reinforcement Learning", "comment": null, "summary": "Offline reinforcement learning aims to learn an agent from pre-collected datasets, avoiding unsafe and inefficient real-time interaction. However, inevitable access to out-ofdistribution actions during the learning process introduces approximation errors, causing the error accumulation and considerable overestimation. In this paper, we construct a new pessimistic auxiliary policy for sampling reliable actions. Specifically, we develop a pessimistic auxiliary strategy by maximizing the lower confidence bound of the Q-function. The pessimistic auxiliary strategy exhibits a relatively high value and low uncertainty in the vicinity of the learned policy, avoiding the learned policy sampling high-value actions with potentially high errors during the learning process. Less approximation error introduced by sampled action from pessimistic auxiliary strategy leads to the alleviation of error accumulation. Extensive experiments on offline reinforcement learning benchmarks reveal that utilizing the pessimistic auxiliary strategy can effectively improve the efficacy of other offline RL approaches."}
{"id": "2602.23970", "categories": ["quant-ph", "math.FA", "math.PR"], "pdf": "https://arxiv.org/pdf/2602.23970", "abs": "https://arxiv.org/abs/2602.23970", "authors": ["Ce Wang"], "title": "Continuous-Time Quantum Walk on Locally Infinite Graph", "comment": "13 pages", "summary": "Time-reversal symmetry is of fundamental importance to physics. In the classical theory of time-reversal symmetry, the time-reversal symmetry of a quantum system is described by an anti-unitary operator, which is known as the time-reversal operator of the system. In this paper, we introduce and study a model of continuous-time quantum walk on a special locally infinite graph. After examining its spectral property, we investigate the time-reversal symmetry of the model. To our surprise, we find that its time-reversal symmetry can be described directly by a unitary operator, which contrasts sharply with that in the classical theory of time-reversal symmetry. Some other related results are also proven."}
{"id": "2602.23630", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23630", "abs": "https://arxiv.org/abs/2602.23630", "authors": ["Zhongyi Pei", "Zhiyao Cen", "Yipeng Huang", "Chen Wang", "Lin Liu", "Philip Yu", "Mingsheng Long"], "title": "BTTackler: A Diagnosis-based Framework for Efficient Deep Learning Hyperparameter Optimization", "comment": null, "summary": "Hyperparameter optimization (HPO) is known to be costly in deep learning, especially when leveraging automated approaches. Most of the existing automated HPO methods are accuracy-based, i.e., accuracy metrics are used to guide the trials of different hyperparameter configurations amongst a specific search space. However, many trials may encounter severe training problems, such as vanishing gradients and insufficient convergence, which can hardly be reflected by accuracy metrics in the early stages of the training and often result in poor performance. This leads to an inefficient optimization trajectory because the bad trials occupy considerable computation resources and reduce the probability of finding excellent hyperparameter configurations within a time limitation. In this paper, we propose \\textbf{Bad Trial Tackler (BTTackler)}, a novel HPO framework that introduces training diagnosis to identify training problems automatically and hence tackles bad trials. BTTackler diagnoses each trial by calculating a set of carefully designed quantified indicators and triggers early termination if any training problems are detected. Evaluations are performed on representative HPO tasks consisting of three classical deep neural networks (DNN) and four widely used HPO methods. To better quantify the effectiveness of an automated HPO method, we propose two new measurements based on accuracy and time consumption. Results show the advantage of BTTackler on two-fold: (1) it reduces 40.33\\% of time consumption to achieve the same accuracy comparable to baseline methods on average and (2) it conducts 44.5\\% more top-10 trials than baseline methods on average within a given time budget. We also released an open-source Python library that allows users to easily apply BTTackler to automated HPO processes with minimal code changes."}
{"id": "2602.24037", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24037", "abs": "https://arxiv.org/abs/2602.24037", "authors": ["Vanya Priscillia Bendatu", "Yao Lu"], "title": "Portfolio Reinforcement Learning with Scenario-Context Rollout", "comment": null, "summary": "Market regime shifts induce distribution shifts that can degrade the performance of portfolio rebalancing policies. We propose macro-conditioned scenario-context rollout (SCR) that generates plausible next-day multivariate return scenarios under stress events. However, doing so faces new challenges, as history will never tell what would have happened differently. As a result, incorporating scenario-based rewards from rollouts introduces a reward--transition mismatch in temporal-difference learning, destabilizing RL critic training.\n  We analyze this inconsistency and show it leads to a mixed evaluation target. Guided by this analysis, we construct a counterfactual next state using the rollout-implied continuations and augment the critic agent's bootstrap target. Doing so stabilizes the learning and provides a viable bias-variance tradeoff.\n  In out-of-sample evaluations across 31 distinct universes of U.S. equity and ETF portfolios, our method improves Sharpe ratio by up to 76% and reduces maximum drawdown by up to 53% compared with classic and RL-based portfolio rebalancing baselines."}
{"id": "2602.23975", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23975", "abs": "https://arxiv.org/abs/2602.23975", "authors": ["Madan Mohan Mahana", "Gunjan Yadav", "Tarak Nath Dey"], "title": "Coherent Control of Population and Quantum Coherence in Superconducting Circuits", "comment": null, "summary": "Quantum mechanics, with its counterintuitive principles and probabilistic nature, has long been confined to the microscopic realm of atoms and photons. Yet, recent breakthroughs have pushed the boundaries of quantum behavior into the macroscopic world, where objects are visible to the naked eye and governed by classical physics. This review article traces the extraordinary progress toward achieving coherent control of population distributions among multiple quantum levels, as well as manipulation of absorption and refractive index, in such large-scale quantum systems, a feat once considered beyond reach."}
{"id": "2602.23633", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23633", "abs": "https://arxiv.org/abs/2602.23633", "authors": ["Yubo Zhou", "Luo Luo", "Guang Dai", "Haishan Ye"], "title": "On the Convergence of Single-Loop Stochastic Bilevel Optimization with Approximate Implicit Differentiation", "comment": null, "summary": "Stochastic Bilevel Optimization has emerged as a fundamental framework for meta-learning and hyperparameter optimization. Despite the practical prevalence of single-loop algorithms--which update lower and upper variables concurrently--their theoretical understanding, particularly in the stochastic regime, remains significantly underdeveloped compared to their multi-loop counterparts. Existing analyses often yield suboptimal convergence rates or obscure the critical dependence on the lower-level condition number $κ$, frequently burying it within generic Lipschitz constants. In this paper, we bridge this gap by providing a refined convergence analysis of the Single-loop Stochastic Approximate Implicit Differentiation (SSAID) algorithm. We prove that SSAID achieves an $ε$-stationary point with an oracle complexity of $\\mathcal{O}(κ^7 ε^{-2})$. Our result is noteworthy in two aspects: (i) it matches the optimal $\\mathcal{O}(ε^{-2})$ rate of state-of-the-art multi-loop methods (e.g., stocBiO) while maintaining the computational efficiency of a single-loop update; and (ii) it provides the first explicit, fine-grained characterization of the $κ$-dependence for stochastic AID-based single-loop methods. This work demonstrates that SSAID is not merely a heuristic approach, but admits a rigorous theoretical foundation with convergence guarantees competitive with mainstream multi-loop frameworks."}
{"id": "2602.24055", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.24055", "abs": "https://arxiv.org/abs/2602.24055", "authors": ["Reva Schwartz", "Carina Westling", "Morgan Briggs", "Marzieh Fadaee", "Isar Nejadgholi", "Matthew Holmes", "Fariza Rashid", "Maya Carlyle", "Afaf Taïk", "Kyra Wilson", "Peter Douglas", "Theodora Skeadas", "Gabriella Waters", "Rumman Chowdhury", "Thiago Lacerda"], "title": "CIRCLE: A Framework for Evaluating AI from a Real-World Lens", "comment": "Accepted at Intelligent Systems Conference (IntelliSys) 2026", "summary": "This paper proposes CIRCLE, a six-stage, lifecycle-based framework to bridge the reality gap between model-centric performance metrics and AI's materialized outcomes in deployment. While existing frameworks like MLOps focus on system stability and benchmarks measure abstract capabilities, decision-makers outside the AI stack lack systematic evidence about the behavior of AI technologies under real-world user variability and constraints. CIRCLE operationalizes the Validation phase of TEVV (Test, Evaluation, Verification, and Validation) by formalizing the translation of stakeholder concerns outside the stack into measurable signals. Unlike participatory design, which often remains localized, or algorithmic audits, which are often retrospective, CIRCLE provides a structured, prospective protocol for linking context-sensitive qualitative insights to scalable quantitative metrics. By integrating methods such as field testing, red teaming, and longitudinal studies into a coordinated pipeline, CIRCLE produces systematic knowledge: evidence that is comparable across sites yet sensitive to local context. This can enable governance based on materialized downstream effects rather than theoretical capabilities."}
{"id": "2602.23976", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23976", "abs": "https://arxiv.org/abs/2602.23976", "authors": ["Alejandro Gomez Cadavid", "Ananth Kaushik", "Pranav Chandarana", "Miguel Angel Lopez-Ruiz", "Gaurav Dev", "Willie Aboumrad", "Qi Zhang", "Claudio Girotto", "Sebastián V. Romero", "Martin Roetteler", "Enrique Solano", "Marco Pistoia", "Narendra N. Hegade"], "title": "Large-scale portfolio optimization on a trapped-ion quantum computer", "comment": "10 pages, 6 figures", "summary": "We present an end-to-end pipeline for large-scale portfolio selection with cardinality constraints and experimentally demonstrate it on trapped-ion quantum processors using hardware-aware decomposition. Building on RMT-based correlation-matrix denoising and community detection, we identify correlated asset groups and introduce a correlation-guided greedy splitting scheme that caps each cluster by the executable qubit budget. Each cluster defines a hardware-embeddable QUBO subproblem that we solve using bias-field digitized counterdiabatic quantum optimization (BF-DCQO), a non-variational method that avoids classical parameter-training loops. We recombine low-energy candidates into global portfolios and enforce feasibility with a two-stage post-processing routine: fast repair followed by a cardinality-preserving swap local search. We benchmark the workflow on a 250-asset universe taken from the S&P 500 and execute subproblems on a 64-qubit Barium development system similar to the forthcoming IonQ Tempo line. We observe that larger executable subproblem sizes reduce decomposition error and systematically improve final objective values and risk-return trade-offs relative to randomized baselines under identical post-processing. Overall, the results establish a hardware-tested route for scaling financial optimization problems, defined by a trade space in which executable problem size and circuit cost are balanced against the resulting solution quality."}
{"id": "2602.23636", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23636", "abs": "https://arxiv.org/abs/2602.23636", "authors": ["Zhihao Ding", "Jinming Li", "Ze Lu", "Jieming Shi"], "title": "FlexGuard: Continuous Risk Scoring for Strictness-Adaptive LLM Content Moderation", "comment": null, "summary": "Ensuring the safety of LLM-generated content is essential for real-world deployment. Most existing guardrail models formulate moderation as a fixed binary classification task, implicitly assuming a fixed definition of harmfulness. In practice, enforcement strictness - how conservatively harmfulness is defined and enforced - varies across platforms and evolves over time, making binary moderators brittle under shifting requirements. We first introduce FlexBench, a strictness-adaptive LLM moderation benchmark that enables controlled evaluation under multiple strictness regimes. Experiments on FlexBench reveal substantial cross-strictness inconsistency in existing moderators: models that perform well under one regime can degrade substantially under others, limiting their practical usability. To address this, we propose FlexGuard, an LLM-based moderator that outputs a calibrated continuous risk score reflecting risk severity and supports strictness-specific decisions via thresholding. We train FlexGuard via risk-alignment optimization to improve score-severity consistency and provide practical threshold selection strategies to adapt to target strictness at deployment. Experiments on FlexBench and public benchmarks demonstrate that FlexGuard achieves higher moderation accuracy and substantially improved robustness under varying strictness. We release the source code and data to support reproducibility."}
{"id": "2602.24080", "categories": ["cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.24080", "abs": "https://arxiv.org/abs/2602.24080", "authors": ["Xiang Li", "Jiabao Gao", "Sipei Lin", "Xuan Zhou", "Chi Zhang", "Bo Cheng", "Jiale Han", "Benyou Wang"], "title": "Human or Machine? A Preliminary Turing Test for Speech-to-Speech Interaction", "comment": "Accepted by ICLR 2026 Conference", "summary": "The pursuit of human-like conversational agents has long been guided by the Turing test. For modern speech-to-speech (S2S) systems, a critical yet unanswered question is whether they can converse like humans. To tackle this, we conduct the first Turing test for S2S systems, collecting 2,968 human judgments on dialogues between 9 state-of-the-art S2S systems and 28 human participants. Our results deliver a clear finding: no existing evaluated S2S system passes the test, revealing a significant gap in human-likeness. To diagnose this failure, we develop a fine-grained taxonomy of 18 human-likeness dimensions and crowd-annotate our collected dialogues accordingly. Our analysis shows that the bottleneck is not semantic understanding but stems from paralinguistic features, emotional expressivity, and conversational persona. Furthermore, we find that off-the-shelf AI models perform unreliably as Turing test judges. In response, we propose an interpretable model that leverages the fine-grained human-likeness ratings and delivers accurate and transparent human-vs-machine discrimination, offering a powerful tool for automatic human-likeness evaluation. Our work establishes the first human-likeness evaluation for S2S systems and moves beyond binary outcomes to enable detailed diagnostic insights, paving the way for human-like improvements in conversational AI systems."}
{"id": "2602.24003", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.24003", "abs": "https://arxiv.org/abs/2602.24003", "authors": ["Waqas Ahmad", "Gioele Consani", "Mohammad Tasnimul Haque", "Jacob Dunstan", "Brian Vlastakis"], "title": "3D Integrated Embedded Filters for Superconducting Quantum Circuits", "comment": null, "summary": "Microwave filtering for superconducting qubits is a key element of quantum computing technology, enabling high coherence and fast state detection. This work presents the design and implementation of novel microwave Purcell filters for superconducting quantum circuits, integrated within a multilayer printed circuit board (PCB). The off-chip design removes all filter components from the qubit substrate, reducing device complexity, improving layout footprint and allowing better scalability to large qubit counts. Each embedded filter can couple up to nine readout resonators, enabling efficient multiplexed readout. Electromagnetic simulations of the filter predict a thousand-fold improvement in qubit isolation from the readout port. The design was experimentally validated under cryogenic conditions in conjunction with a 35-qubit device, demonstrating compatibility of the PCB-based filter with high-coherence superconducting qubits. The comparison of the measured qubit median T1 of 84 $μ$s with the expected radiative limit from electromagnetic simulations validated the presence of Purcell filtering in the system."}
{"id": "2602.23638", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23638", "abs": "https://arxiv.org/abs/2602.23638", "authors": ["Haoran Zhang", "Dongjun Kim", "Seohyeon Cha", "Haris Vikalo"], "title": "FedRot-LoRA: Mitigating Rotational Misalignment in Federated LoRA", "comment": "preprint", "summary": "Federated LoRA provides a communication-efficient mechanism for fine-tuning large language models on decentralized data. In practice, however, a discrepancy between the factor-wise averaging used to preserve low rank and the mathematically correct aggregation of local updates can cause significant aggregation error and unstable training. We argue that a major source of this problem is rotational misalignment, arising from the rotational invariance of low-rank factorizations -- semantically equivalent updates can be represented in different latent subspaces across clients since $(B_i R_i)(R_i^\\top A_i) = B_i A_i$. When such misaligned factors are averaged directly, they interfere destructively and degrade the global update. To address this issue, we propose FedRot-LoRA, a federated LoRA framework that aligns client updates via orthogonal transformations prior to aggregation. This alignment preserves the semantic update while reducing cross-client subspace mismatch, without increasing communication cost or restricting model expressivity. We provide a convergence analysis that examines the aggregation error induced by factor-wise averaging and shows how rotational alignment yields a tighter upper bound on this error. Extensive experiments on natural language understanding and generative tasks demonstrate that FedRot-LoRA consistently outperforms existing federated LoRA baselines across a range of heterogeneity levels and LoRA ranks."}
{"id": "2602.24097", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24097", "abs": "https://arxiv.org/abs/2602.24097", "authors": ["Yue Xie", "Zizhen Xu", "William Beazley", "Fumiya Iida"], "title": "Bi-level RL-Heuristic Optimization for Real-world Winter Road Maintenance", "comment": null, "summary": "Winter road maintenance is critical for ensuring public safety and reducing environmental impacts, yet existing methods struggle to manage large-scale routing problems effectively and mostly reply on human decision. This study presents a novel, scalable bi-level optimization framework, validated on real operational data on UK strategic road networks (M25, M6, A1), including interconnected local road networks in surrounding areas for vehicle traversing, as part of the highway operator's efforts to solve existing planning challenges. At the upper level, a reinforcement learning (RL) agent strategically partitions the road network into manageable clusters and optimally allocates resources from multiple depots. At the lower level, a multi-objective vehicle routing problem (VRP) is solved within each cluster, minimizing the maximum vehicle travel time and total carbon emissions. Unlike existing approaches, our method handles large-scale, real-world networks efficiently, explicitly incorporating vehicle-specific constraints, depot capacities, and road segment requirements. Results demonstrate significant improvements, including balanced workloads, reduced maximum travel times below the targeted two-hour threshold, lower emissions, and substantial cost savings. This study illustrates how advanced AI-driven bi-level optimization can directly enhance operational decision-making in real-world transportation and logistics."}
{"id": "2602.24048", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2602.24048", "abs": "https://arxiv.org/abs/2602.24048", "authors": ["João P. R. Leonel", "Paulo A. Brandão"], "title": "Saturable nonlinearities in a driven-dissipative bosonic quantum battery", "comment": null, "summary": "We investigate the charging of a nonlinear quantum battery consisting of a single bosonic mode subject to a saturable nonlinearity, coherent driving, and dissipation. In contrast to Kerr-type anharmonicities, the saturable interaction induces a bounded and nonlinear distortion of the energy spectrum, leading to a progressive increase in the density of energy levels. We analyze the time evolution of the energy and ergotropy of the battery by solving a Lindblad master equation and show that the nonlinear spectral structure significantly affects both transient charging behavior and steady-state properties. Our results reveal that, for a broad range of parameters, the saturable nonlinearity enhances the maximum stored energy and modifies the ergotropy generation in the presence of losses. The interplay between dissipation and bounded spectral nonlinearity provides a controllable mechanism to tune energy storage and work extraction in bosonic quantum batteries."}
{"id": "2602.23662", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23662", "abs": "https://arxiv.org/abs/2602.23662", "authors": ["Kohei Obata", "Zheng Chen", "Yasuko Matsubara", "Lingwei Zhu", "Yasushi Sakurai"], "title": "Selective Denoising Diffusion Model for Time Series Anomaly Detection", "comment": null, "summary": "Time series anomaly detection (TSAD) has been an important area of research for decades, with reconstruction-based methods, mostly based on generative models, gaining popularity and demonstrating success. Diffusion models have recently attracted attention due to their advanced generative capabilities. Existing diffusion-based methods for TSAD rely on a conditional strategy, which reconstructs input instances from white noise with the aid of the conditioner. However, this poses challenges in accurately reconstructing the normal parts, resulting in suboptimal detection performance. In response, we propose a novel diffusion-based method, named AnomalyFilter, which acts as a selective filter that only denoises anomaly parts in the instance while retaining normal parts. To build such a filter, we mask Gaussian noise during the training phase and conduct the denoising process without adding noise to the instances. The synergy of the two simple components greatly enhances the performance of naive diffusion models. Extensive experiments on five datasets demonstrate that AnomalyFilter achieves notably low reconstruction error on normal parts, providing empirical support for its effectiveness in anomaly detection. AnomalyFilter represents a pioneering approach that focuses on the noise design of diffusion models specifically tailored for TSAD."}
{"id": "2602.24100", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24100", "abs": "https://arxiv.org/abs/2602.24100", "authors": ["Richard Csaky"], "title": "Artificial Agency Program: Curiosity, compression, and communication in agents", "comment": "This is a working draft. Feedback and criticism is most welcome", "summary": "This paper presents the Artificial Agency Program (AAP), a position and research agenda for building AI systems as reality embedded, resource-bounded agents whose development is driven by curiosity-as-learning-progress under physical and computational constraints. The central thesis is that AI is most useful when treated as part of an extended human--tool system that increases sensing, understanding, and actuation capability while reducing friction at the interface between people, tools, and environments. The agenda unifies predictive compression, intrinsic motivation, empowerment and control, interface quality (unification), and language/self-communication as selective information bottlenecks. We formulate these ideas as a falsifiable program with explicit costs, staged experiments, and a concrete multimodal tokenized testbed in which an agent allocates limited budget among observation, action, and deliberation. The aim is to provide a conceptual and experimental framework that connects intrinsic motivation, information theory, thermodynamics, bounded rationality, and modern reasoning systems"}
{"id": "2602.24053", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.24053", "abs": "https://arxiv.org/abs/2602.24053", "authors": ["Viacheslav Dubovitskii", "Filippo Utro", "Aritra Bose", "Laxmi Parida", "Sabrina Maniscalco", "Sergey N. Filippov"], "title": "Experimental implementation of a discrete-time quantum walk on biological networks", "comment": "14 pages, 6 figures", "summary": "Quantum walks provide a versatile framework for probing the structural and dynamical properties of complex systems ranging from biological networks to synthetic materials. However, their realization on current noisy pre-fault-tolerant quantum computers is fundamentally limited by decoherence. Conventional dense encodings of graph structures require prohibitively deep circuits, making them incompatible with existing hardware. Here we introduce an algorithm that leverages symmetry-sector encoding and trades circuit depth for qubits, while integrating symmetry-respecting postselection as an effective noise-mitigation strategy. This combination enables us to execute practical quantum-walk circuits for biological networks on actual quantum hardware. We benchmark the proposed methodology against known state-of-the-art circuit architectures, highlighting significant reduction of circuit depth in our approach at the cost of moderate qubit overhead. Utilizing 40 qubits, we implement quantum walks on complex graphs containing up to 17 nodes and 20 edges -- the largest experiment on superconducting hardware to date, with the Hellinger fidelity exceeding 87% throughout 7 steps. We present a case study that illustrates how experimentally obtained quantum-walk dynamics on a protein-protein-interaction network can be applied to prioritizing disease-associated genes. We discuss the framework scalability in the pre-fault-tolerant era and its potential for studying larger biological networks."}
{"id": "2602.23663", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23663", "abs": "https://arxiv.org/abs/2602.23663", "authors": ["Kohei Obata", "Taichi Murayama", "Zheng Chen", "Yasuko Matsubara", "Yasushi Sakurai"], "title": "Disentangled Mode-Specific Representations for Tensor Time Series via Contrastive Learning", "comment": null, "summary": "Multi-mode tensor time series (TTS) can be found in many domains, such as search engines and environmental monitoring systems. Learning representations of a TTS benefits various applications, but it is also challenging since the complexities inherent in the tensor hinder the realization of rich representations. In this paper, we propose a novel representation learning method designed specifically for TTS, namely MoST. Specifically, MoST uses a tensor slicing approach to reduce the complexity of the TTS structure and learns representations that can be disentangled into individual non-temporal modes. Each representation captures mode-specific features, which are the relationship between variables within the same mode, and mode-invariant features, which are in common in representations of different modes. We employ a contrastive learning framework to learn parameters; the loss function comprises two parts intended to learn representation in a mode-specific way and mode-invariant way, effectively exploiting disentangled representations as augmentations. Extensive experiments on real-world datasets show that MoST consistently outperforms the state-of-the-art methods in terms of classification and forecasting accuracy. Code is available at https://github.com/KoheiObata/MoST."}
{"id": "2602.24110", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.24110", "abs": "https://arxiv.org/abs/2602.24110", "authors": ["Yanwei Ren", "Haotian Zhang", "Likang Xiao", "Xikai Zhang", "Jiaxing Huang", "Jiayan Qiu", "Baosheng Yu", "Quan Chen", "Liu Liu"], "title": "Recycling Failures: Salvaging Exploration in RLVR via Fine-Grained Off-Policy Guidance", "comment": null, "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a powerful paradigm for enhancing the complex reasoning capabilities of Large Reasoning Models. However, standard outcome-based supervision suffers from a critical limitation that penalizes trajectories that are largely correct but fail due to several missteps as heavily as completely erroneous ones. This coarse feedback signal causes the model to discard valuable largely correct rollouts, leading to a degradation in rollout diversity that prematurely narrows the exploration space. Process Reward Models have demonstrated efficacy in providing reliable step-wise verification for test-time scaling, naively integrating these signals into RLVR as dense rewards proves ineffective.Prior methods attempt to introduce off-policy guided whole-trajectory replacement that often outside the policy model's distribution, but still fail to utilize the largely correct rollouts generated by the model itself and thus do not effectively mitigate the narrowing of the exploration space. To address these issues, we propose SCOPE (Step-wise Correction for On-Policy Exploration), a novel framework that utilizes Process Reward Models to pinpoint the first erroneous step in suboptimal rollouts and applies fine-grained, step-wise off-policy rectification. By applying precise refinement on partially correct rollout, our method effectively salvages partially correct trajectories and increases diversity score by 13.5%, thereby sustaining a broad exploration space. Extensive experiments demonstrate that our approach establishes new state-of-the-art results, achieving an average accuracy of 46.6% on math reasoning and exhibiting robust generalization with 53.4% accuracy on out-of-distribution reasoning tasks."}
{"id": "2602.24056", "categories": ["quant-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.24056", "abs": "https://arxiv.org/abs/2602.24056", "authors": ["Felipe Peleteiro", "João Victor Shiguetsugo Kawanami Lima", "Pedro Marcelo Prado", "Felipe Fernandes Fanchini", "Ariel Norambuena"], "title": "Learning spectral density functions in open quantum systems", "comment": "8 pages, 4 figures", "summary": "Spectral density functions quantify how environmental modes couple to quantum systems and govern their open dynamics. Inferring such frequency-dependent functions from time-domain measurements is an ill-conditioned inverse problem. Here, we use exactly solvable spin-boson models with pure-dephasing and amplitude-damping channels to reconstruct spectral density functions from noisy simulated data. First, we introduce a parameter estimation approach based on machine learning regressors to infer Lorentzian and Ohmic-like spectral density parameters, quantifying robustness to noise. Second, we show that a cosine transform inversion yields a physics-consistent spectral prior estimation, which is refined by a constrained neural network enforcing positivity and correct asymptotic behaviour. Our neural network framework robustly reconstructs structured spectral densities by filtering simulated noisy signals and learning general functional dependencies."}
{"id": "2602.23696", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23696", "abs": "https://arxiv.org/abs/2602.23696", "authors": ["Yongzhong Xu"], "title": "Optimizer-Induced Low-Dimensional Drift and Transverse Dynamics in Transformer Training", "comment": "18 pages, 4 figures", "summary": "We study the geometry of training trajectories in small transformer models and find that parameter updates organize into a dominant drift direction with transverse residual dynamics. Using uncentered, row-normalized trajectory PCA, we show that a single direction captures a large fraction of cumulative parameter movement early in training, while remaining components encode oscillatory behavior in auxiliary probe performance. Instantaneous gradients exhibit little alignment with this dominant direction, indicating that it arises from accumulated optimizer updates rather than per-batch gradient structure. Comparing AdamW with SGD variants at matched loss levels reveals substantial differences in trajectory geometry: AdamW develops multi-dimensional drift structure, whereas SGD-family optimizers produce nearly colinear parameter evolution and weaker probe dynamics. Reheating selectively perturbs transverse components with minimal effect on the dominant drift coordinate. These findings suggest that optimizer choice shapes the effective dimensionality and structure of learning trajectories beyond what is apparent from loss values alone."}
{"id": "2602.24173", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24173", "abs": "https://arxiv.org/abs/2602.24173", "authors": ["Antoine Peyronnet", "Fabian Gloeckle", "Amaury Hayat"], "title": "LemmaBench: A Live, Research-Level Benchmark to Evaluate LLM Capabilities in Mathematics", "comment": "15 pages, 3 figures, 5 Tables", "summary": "We present a new approach for benchmarking Large Language Model (LLM) capabilities on research-level mathematics. Existing benchmarks largely rely on static, hand-curated sets of contest or textbook-style problems as proxies for mathematical research. Instead, we establish an updatable benchmark evaluating models directly on the latest research results in mathematics. This consists of an automatic pipeline that extracts lemmas from arXiv and rewrites them into self-contained statements by making all assumptions and required definitions explicit. It results in a benchmark that can be updated regularly with new problems taken directly from human mathematical research, while previous instances can be used for training without compromising future evaluations. We benchmark current state-of-the-art LLMs, which obtain around 10-15$\\%$ accuracy in theorem proving (pass@1) depending on the model, showing that there is currently a large margin of progression for LLMs to reach human-level proving capabilities in a research context."}
{"id": "2602.24062", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.24062", "abs": "https://arxiv.org/abs/2602.24062", "authors": ["Michele Bandini", "Davide Ferrari", "Stefano Carretta", "Michele Amoretti"], "title": "Optimized Compilation for Distributed Quantum Computing", "comment": null, "summary": "In many practical applications, quantum algorithms require several qubits, significantly more than those available with current noisy intermediate-scale quantum processors. Distributed quantum computing (DQC) is considered a scalable approach to increasing the number of available qubits for computational tasks. In the DQC setting, a quantum compiler must find the best partitioning for the quantum algorithm and then perform smart non-local operations scheduling to optimize the consumption of Einstein-Podolsky-Rosen (EPR) pairs. In this work, the focus is on minimizing the use of EPR pairs when the circuit structure allows for multiple non-local gates to utilize a single TeleGate operation. This is achieved by using a greedy algorithm that explores the circuit and groups together the gates that could share an EPR pair while also changing the order of commutative gates when necessary. With this preliminary pass, the compiled circuits show reduced depth and EPR usage. Since the quality of each EPR pair quickly deteriorates, the number of non-local gates using the same EPR pair should also be bounded. This means that, depending on the features of the target quantum network, the user can achieve different levels of optimization. Here, it is shown that this approach brings benefits even while assuming a low EPR pair lifetime."}
{"id": "2602.23737", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23737", "abs": "https://arxiv.org/abs/2602.23737", "authors": ["Hanping Zhang", "Yuhong Guo"], "title": "Bridging Dynamics Gaps via Diffusion Schrödinger Bridge for Cross-Domain Reinforcement Learning", "comment": null, "summary": "Cross-domain reinforcement learning (RL) aims to learn transferable policies under dynamics shifts between source and target domains. A key challenge lies in the lack of target-domain environment interaction and reward supervision, which prevents direct policy learning. To address this challenge, we propose Bridging Dynamics Gaps for Cross-Domain Reinforcement Learning (BDGxRL), a novel framework that leverages Diffusion Schrödinger Bridge (DSB) to align source transitions with target-domain dynamics encoded in offline demonstrations. Moreover, we introduce a reward modulation mechanism that estimates rewards based on state transitions, applying to DSB-aligned samples to ensure consistency between rewards and target-domain dynamics. BDGxRL performs target-oriented policy learning entirely within the source domain, without access to the target environment or its rewards. Experiments on MuJoCo cross-domain benchmarks demonstrate that BDGxRL outperforms state-of-the-art baselines and shows strong adaptability under transition dynamics shifts."}
{"id": "2602.24180", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24180", "abs": "https://arxiv.org/abs/2602.24180", "authors": ["Shishun Zhang", "Juzhan Xu", "Yidan Fan", "Chenyang Zhu", "Ruizhen Hu", "Yongjun Wang", "Kai Xu"], "title": "Learning Flexible Job Shop Scheduling under Limited Buffers and Material Kitting Constraints", "comment": "8 pages, 8 figures, conference", "summary": "The Flexible Job Shop Scheduling Problem (FJSP) originates from real production lines, while some practical constraints are often ignored or idealized in current FJSP studies, among which the limited buffer problem has a particular impact on production efficiency. To this end, we study an extended problem that is closer to practical scenarios--the Flexible Job Shop Scheduling Problem with Limited Buffers and Material Kitting. In recent years, deep reinforcement learning (DRL) has demonstrated considerable potential in scheduling tasks. However, its capacity for state modeling remains limited when handling complex dependencies and long-term constraints. To address this, we leverage a heterogeneous graph network within the DRL framework to model the global state. By constructing efficient message passing among machines, operations, and buffers, the network focuses on avoiding decisions that may cause frequent pallet changes during long-sequence scheduling, thereby helping improve buffer utilization and overall decision quality. Experimental results on both synthetic and real production line datasets show that the proposed method outperforms traditional heuristics and advanced DRL methods in terms of makespan and pallet changes, and also achieves a good balance between solution quality and computational cost. Furthermore, a supplementary video is provided to showcase a simulation system that effectively visualizes the progression of the production line."}
{"id": "2602.24077", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.24077", "abs": "https://arxiv.org/abs/2602.24077", "authors": ["Yun-Long Cao", "Xiao-Ye Xu", "Chuan-Feng Li", "Guang-Can Guo"], "title": "Gaussian resource based heralded entangled state generation enhanced by photon addition and subtraction", "comment": null, "summary": "We propose a heralded entanglement generation scheme based on Gaussian sources augmented with photon addition and subtraction operations. By combining single-mode squeezing, linear interferometers, and conditional photon-number measurements on ancillary modes, our model probabilistically generates dual-rail encoded Bell, GHZ, and W states. We systematically optimize the squeezing parameters and interferometer settings to maximize both the heralding success probability and the fidelity with the target states. Our results show that the inclusion of photon addition and subtraction significantly enhances the non-classicality of the output states, leading to improved generation performance, while maintaining computational efficiency comparable to single-photon source models. We further analyze the robustness of the scheme under parameter perturbations, demonstrating stable performance against realistic experimental imperfections. This work provides a versatile and experimentally feasible framework for scalable heralded entanglement generation using Gaussian resources with non-Gaussian operations."}
{"id": "2602.23761", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23761", "abs": "https://arxiv.org/abs/2602.23761", "authors": ["Yuyu Geng", "Lei Sun", "Yao Gao", "Xinxin Hu", "Zhonghua Yi", "Xiaolong Qian", "Weijian Hu", "Jian Bai", "Kaiwei Wang"], "title": "OPTIAGENT: A Physics-Driven Agentic Framework for Automated Optical Design", "comment": null, "summary": "Optical design is the process of configuring optical elements to precisely manipulate light for high-fidelity imaging. It is inherently a highly non-convex optimization problem that relies heavily on human heuristic expertise and domain-specific knowledge. While Large Language Models (LLMs) possess extensive optical knowledge, their capabilities in leveraging the knowledge in designing lens system remain significantly constrained. This work represents the first attempt to employ LLMs in the field of optical design. We bridge the expertise gap by enabling users without formal optical training to successfully develop functional lens systems. Concretely, we curate a comprehensive dataset, named OptiDesignQA, which encompasses both classical lens systems sourced from standard optical textbooks and novel configurations generated by automated design algorithms for training and evaluation. Furthermore, we inject domain-specific optical expertise into the LLM through a hybrid objective of full-system synthesis and lens completion. To align the model with optical principles, we employ Group Relative Policy Optimization Done Right (DrGRPO) guided by Optical Lexicographic Reward for physics-driven policy alignment. This reward system incorporates structural format rewards, physical feasibility rewards, light-manipulation accuracy, and LLM-based heuristics. Finally, our model integrates with specialized optical optimization routines for end-to-end fine-tuning and precision refinement. We benchmark our proposed method against both traditional optimization-based automated design algorithms and LLM counterparts, and experimental results show the superiority of our method."}
{"id": "2602.24195", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24195", "abs": "https://arxiv.org/abs/2602.24195", "authors": ["Gregory Kang Ruey Lau", "Hieu Dao", "Nicole Kan Hui Lin", "Bryan Kian Hsiang Low"], "title": "Uncertainty Quantification for Multimodal Large Language Models with Incoherence-adjusted Semantic Volume", "comment": "Earlier versions presented at ICLR 2025 QUESTION workshop and ICML 2025 R2-FM workshop", "summary": "Despite their capabilities, Multimodal Large Language Models (MLLMs) may produce plausible but erroneous outputs, hindering reliable deployment. Accurate uncertainty metrics could enable escalation of unreliable queries to human experts or larger models for improved performance. However, existing uncertainty metrics have practical constraints, such as being designed only for specific modalities, reliant on external tools, or computationally expensive. We introduce UMPIRE, a training-free uncertainty quantification framework for MLLMs that works efficiently across various input and output modalities without external tools, relying only on the models' own internal modality features. UMPIRE computes the incoherence-adjusted semantic volume of sampled MLLM responses for a given task instance, effectively capturing both the global semantic diversity of samples and the local incoherence of responses based on internal model confidence. We propose uncertainty desiderata for MLLMs and provide theoretical analysis motivating UMPIRE's design. Extensive experiments show that UMPIRE consistently outperforms baseline metrics in error detection and uncertainty calibration across image, audio, and video-text benchmarks, including adversarial and out-of-distribution settings. We also demonstrate UMPIRE's generalization to non-text output tasks, including image and audio generation."}
{"id": "2602.24098", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.24098", "abs": "https://arxiv.org/abs/2602.24098", "authors": ["Yufeng Wu", "Yiyu Zhou", "Haoqi Zhao", "Danqing Wang", "Matthew D. LaHaye", "Daniel L. Campbell", "Hong X. Tang"], "title": "A frequency-agile microwave-optical interface for superconducting qubits", "comment": null, "summary": "Superconducting quantum processors operate at microwave frequencies in millikelvin environments, making it challenging to interconnect distant nodes using conventional microwave wiring. Coherent microwave-to-optical (M2O) transduction enables superconducting quantum networks by interfacing itinerant microwave photons with low-loss optical fiber. However, many state-of-the-art transducers provide efficient conversion only over a narrow frequency span, complicating deployment with heterogeneous superconducting devices that are detuned by gigahertz-scale offsets. Here we demonstrate a frequency-agile microwave-optical interface that overcomes this bandwidth mismatch by cascading an electro-optic M2O transducer with a multimode microwave-to-microwave (M2M) frequency converter, with in situ tunability of the microwave resonances in both stages. Using this architecture, we realize continuous frequency coverage from 5.0 to 8.5 GHz within a single system. As an application relevant to superconducting-qubit networking, we use the cascaded M2M-M2O interface to optically read out a superconducting qubit whose readout resonator is detuned by 1.7 GHz from the native M2O microwave resonance, demonstrating a scalable route toward fiber-linked superconducting quantum nodes."}
{"id": "2602.23770", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23770", "abs": "https://arxiv.org/abs/2602.23770", "authors": ["Chenxing Lin", "Xinhui Gao", "Haipeng Zhang", "Xinran Li", "Haitao Wang", "Songzhu Mei", "Chenglu Wen", "Weiquan Liu", "Siqi Shen", "Cheng Wang"], "title": "MAGE: Multi-scale Autoregressive Generation for Offline Reinforcement Learning", "comment": "ICLR2026", "summary": "Generative models have gained significant traction in offline reinforcement learning (RL) due to their ability to model complex trajectory distributions. However, existing generation-based approaches still struggle with long-horizon tasks characterized by sparse rewards. Some hierarchical generation methods have been developed to mitigate this issue by decomposing the original problem into shorter-horizon subproblems using one policy and generating detailed actions with another. While effective, these methods often overlook the multi-scale temporal structure inherent in trajectories, resulting in suboptimal performance. To overcome these limitations, we propose MAGE, a Multi-scale Autoregressive GEneration-based offline RL method. MAGE incorporates a condition-guided multi-scale autoencoder to learn hierarchical trajectory representations, along with a multi-scale transformer that autoregressively generates trajectory representations from coarse to fine temporal scales. MAGE effectively captures temporal dependencies of trajectories at multiple resolutions. Additionally, a condition-guided decoder is employed to exert precise control over short-term behaviors. Extensive experiments on five offline RL benchmarks against fifteen baseline algorithms show that MAGE successfully integrates multi-scale trajectory modeling with conditional guidance, generating coherent and controllable trajectories in long-horizon sparse-reward settings."}
{"id": "2602.24273", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24273", "abs": "https://arxiv.org/abs/2602.24273", "authors": ["Borja Requena Pozo", "Austin Letson", "Krystian Nowakowski", "Izan Beltran Ferreiro", "Leopoldo Sarra"], "title": "A Minimal Agent for Automated Theorem Proving", "comment": null, "summary": "We propose a minimal agentic baseline that enables systematic comparison across different AI-based theorem prover architectures. This design implements the core features shared among state-of-the-art systems: iterative proof refinement, library search and context management. We evaluate our baseline using qualitatively different benchmarks and compare various popular models and design choices, and demonstrate competitive performance compared to state-of-the-art approaches, while using a significantly simpler architecture. Our results demonstrate consistent advantages of an iterative approach over multiple single-shot generations, especially in terms of sample efficiency and cost effectiveness. The implementation is released open-source as a candidate reference for future research and as an accessible prover for the community."}
{"id": "2602.24102", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.24102", "abs": "https://arxiv.org/abs/2602.24102", "authors": ["Kai-Xuan Wen", "Dong-Long Hu", "Shengyong Li", "Ze-Liang Xiang"], "title": "Estimating the performance boundary of Gottesman-Kitaev-Preskill codes and number-phase codes", "comment": null, "summary": "Bosonic quantum error-correcting codes encode logical information in a harmonic oscillator, with the Gottesman-Kitaev-Preskill (GKP) and number-phase (NP) codes representing two fundamentally different encoding paradigms. Although both have been extensively studied, it remains unclear under what physical noise conditions (including photon loss and dephasing) one encoding intrinsically outperforms the other. Here we estimate a quantitative performance boundary between GKP and NP codes under general photon loss-dephasing noise. By optimizing code parameters within each encoding family, we identify the noise regimes in which each code exhibits a fundamental advantage. In particular, we find that the crossover occurs when the dephasing strength is approximately two orders of magnitude smaller than the loss strength, revealing a sharp separation between operational regimes. Beyond this specific comparison, our work establishes a practical and extensible methodology for benchmarking bosonic codes and optimizing their parameters, providing concrete guidance for the experimental selection and deployment of bosonic encodings in realistic noise environments."}
{"id": "2602.23784", "categories": ["cs.LG", "cs.AI", "q-fin.CP", "q-fin.TR"], "pdf": "https://arxiv.org/pdf/2602.23784", "abs": "https://arxiv.org/abs/2602.23784", "authors": ["Maxime Kawawa-Beaudan", "Srijan Sood", "Kassiani Papasotiriou", "Daniel Borrajo", "Manuela Veloso"], "title": "TradeFM: A Generative Foundation Model for Trade-flow and Market Microstructure", "comment": "29 pages, 17 figures, 6 tables. Preprint", "summary": "Foundation models have transformed domains from language to genomics by learning general-purpose representations from large-scale, heterogeneous data. We introduce TradeFM, a 524M-parameter generative Transformer that brings this paradigm to market microstructure, learning directly from billions of trade events across >9K equities. To enable cross-asset generalization, we develop scale-invariant features and a universal tokenization scheme that map the heterogeneous, multi-modal event stream of order flow into a unified discrete sequence -- eliminating asset-specific calibration. Integrated with a deterministic market simulator, TradeFM-generated rollouts reproduce key stylized facts of financial returns, including heavy tails, volatility clustering, and absence of return autocorrelation. Quantitatively, TradeFM achieves 2-3x lower distributional error than Compound Hawkes baselines and generalizes zero-shot to geographically out-of-distribution APAC markets with moderate perplexity degradation. Together, these results suggest that scale-invariant trade representations capture transferable structure in market microstructure, opening a path toward synthetic data generation, stress testing, and learning-based trading agents."}
{"id": "2602.24288", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.24288", "abs": "https://arxiv.org/abs/2602.24288", "authors": ["Fan Shu", "Yite Wang", "Ruofan Wu", "Boyi Liu", "Zhewei Yao", "Yuxiong He", "Feng Yan"], "title": "DARE-bench: Evaluating Modeling and Instruction Fidelity of LLMs in Data Science", "comment": "Published as a conference paper at ICLR 2026. 10 pages plus appendix", "summary": "The fast-growing demands in using Large Language Models (LLMs) to tackle complex multi-step data science tasks create an emergent need for accurate benchmarking. There are two major gaps in existing benchmarks: (i) the lack of standardized, process-aware evaluation that captures instruction adherence and process fidelity, and (ii) the scarcity of accurately labeled training data. To bridge these gaps, we introduce DARE-bench, a benchmark designed for machine learning modeling and data science instruction following. Unlike many existing benchmarks that rely on human- or model-based judges, all tasks in DARE-bench have verifiable ground truth, ensuring objective and reproducible evaluation. To cover a broad range of tasks and support agentic tools, DARE-bench consists of 6,300 Kaggle-derived tasks and provides both large-scale training data and evaluation sets. Extensive evaluations show that even highly capable models such as gpt-o4-mini struggle to achieve good performance, especially in machine learning modeling tasks. Using DARE-bench training tasks for fine-tuning can substantially improve model performance. For example, supervised fine-tuning boosts Qwen3-32B's accuracy by 1.83x and reinforcement learning boosts Qwen3-4B's accuracy by more than 8x. These significant improvements verify the importance of DARE-bench both as an accurate evaluation benchmark and critical training data."}
{"id": "2602.24152", "categories": ["quant-ph", "cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2602.24152", "abs": "https://arxiv.org/abs/2602.24152", "authors": ["Gongyu Ni", "Davide Ferrari", "Lester Ho", "Michele Amoretti"], "title": "Advanced Scheduling Strategies for Distributed Quantum Computing Jobs", "comment": "14 pages, 10 figures, 9 tables", "summary": "Scaling the number of qubits available across multiple quantum devices is an active area of research within distributed quantum computing (DQC). This includes quantum circuit compilation and execution management on multiple quantum devices in the network. The latter aspect is very challenging because, while reducing the makespan of job batches remains a relevant objective, novel quantum-specific constraints must be considered, including QPU utilization, non-local gate density, and the latency associated with queued DQC jobs. In this work, a range of scheduling strategies is proposed, simulated, and evaluated, including heuristics that prioritize resource maximization for QPU utilization, node selection based on heterogeneous network connectivity, asynchronous node release upon job completion, and a scheduling strategy based on reinforcement learning with proximal policy optimization. These approaches are benchmarked against traditional FIFO and LIST schedulers under varying DQC job types and network conditions for the allocation of DQC jobs to devices within a network."}
{"id": "2602.23785", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23785", "abs": "https://arxiv.org/abs/2602.23785", "authors": ["Zhiwei Han", "Stefan Matthes", "Hao Shen"], "title": "Provable Subspace Identification of Nonlinear Multi-view CCA", "comment": null, "summary": "We investigate the identifiability of nonlinear Canonical Correlation Analysis (CCA) in a multi-view setup, where each view is generated by an unknown nonlinear map applied to a linear mixture of shared latents and view-private noise. Rather than attempting exact unmixing, a problem proven to be ill-posed, we instead reframe multi-view CCA as a basis-invariant subspace identification problem. We prove that, under suitable latent priors and spectral separation conditions, multi-view CCA recovers the pairwise correlated signal subspaces up to view-wise orthogonal ambiguity. For $N \\geq 3$ views, the objective provably isolates the jointly correlated subspaces shared across all views while eliminating view-private variations. We further establish finite-sample consistency guarantees by translating the concentration of empirical cross-covariances into explicit subspace error bounds via spectral perturbation theory. Experiments on synthetic and rendered image datasets validate our theoretical findings and confirm the necessity of the assumed conditions."}
{"id": "2602.23409", "categories": ["cs.LG", "cs.AI", "cs.ET", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23409", "abs": "https://arxiv.org/abs/2602.23409", "authors": ["Michael Poppel", "Jonas Stein", "Sebastian Wölckert", "Markus Baumann", "Claudia Linnhoff-Popien"], "title": "Long Range Frequency Tuning for QML", "comment": null, "summary": "Quantum machine learning models using angle encoding naturally represent truncated Fourier series, providing universal function approximation capabilities with sufficient circuit depth. For unary fixed-frequency encodings, circuit depth scales as O(omega_max * (omega_max + epsilon^{-2})) with target frequency magnitude omega_max and precision epsilon. Trainable-frequency approaches theoretically reduce this to match the target spectrum size, requiring only as many encoding gates as frequencies in the target spectrum. Despite this compelling efficiency, their practical effectiveness hinges on a key assumption: that gradient-based optimization can drive prefactors to arbitrary target values. We demonstrate through systematic experiments that frequency prefactors exhibit limited trainability: movement is constrained to approximately +/-1 units with typical learning rates. When target frequencies lie outside this reachable range, optimization frequently fails. To overcome this frequency reachability limitation, we propose grid-based initialization using ternary encodings, which generate dense integer frequency spectra. While this approach requires O(log_3(omega_max)) encoding gates -- more than the theoretical optimum but exponentially fewer than fixed-frequency methods -- it ensures target frequencies lie within the locally reachable range. On synthetic targets with three shifted high frequencies, ternary grid initialization achieves a median R^2 score of 0.9969, compared to 0.1841 for the trainable-frequency baseline. For the real-world Flight Passengers dataset, ternary grid initialization achieves a median R^2 score of 0.9671, representing a 22.8% improvement over trainable-frequency initialization (median R^2 = 0.7876)."}
{"id": "2602.24164", "categories": ["quant-ph", "cs.LO"], "pdf": "https://arxiv.org/pdf/2602.24164", "abs": "https://arxiv.org/abs/2602.24164", "authors": ["Anuj Dawar", "Nihil Shah"], "title": "Complexity of Satisfiability in Kochen-Specker Partial Boolean Algebras", "comment": null, "summary": "The Kochen-Specker no-go theorem established that hidden-variable theories in quantum mechanics necessarily admit contextuality. This theorem is formally stated in terms of the partial Boolean algebra structure of projectors on a Hilbert space. Each partial Boolean algebra provides a semantics for interpreting propositional logic. In this paper, we examine the complexity of propositional satisfiablity for various classes of partial Boolean algebras. We first show that the satisfiability problem for the class of non-trivial partial Boolean algebras is NP-complete. Next, we consider the satisfiability problem for the class of partial Boolean algebras arising from projectors on finite dimensional Hilbert spaces. For real Hilbert spaces of dimension greater 2 and any complex Hilbert spaces of dimension greater than 3, we demonstrate that the satisfiablity problem is complete for the existential theory of the reals. Interestingly, the proofs of these results make use of Kochen-Specker sets as gadgets. As a corollary, we conclude that deciding quantum homomorphism in these fixed dimensions are also complete for the existential theory of the reals. Finally, we show that the satisfiability problems for the class of all Hilbert spaces and all finite-dimensional Hilbert spaces is undecidable."}
{"id": "2602.23789", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23789", "abs": "https://arxiv.org/abs/2602.23789", "authors": ["Aleksandr Ananikian", "Daniil Drozdov", "Konstantin Yakovlev"], "title": "UPath: Universal Planner Across Topological Heterogeneity For Grid-Based Pathfinding", "comment": null, "summary": "The performance of search algorithms for grid-based pathfinding, e.g. A*, critically depends on the heuristic function that is used to focus the search. Recent studies have shown that informed heuristics that take the positions/shapes of the obstacles into account can be approximated with the deep neural networks. Unfortunately, the existing learning-based approaches mostly rely on the assumption that training and test grid maps are drawn from the same distribution (e.g., city maps, indoor maps, etc.) and perform poorly on out-of-distribution tasks. This naturally limits their application in practice when often a universal solver is needed that is capable of efficiently handling any problem instance. In this work, we close this gap by designing an universal heuristic predictor: a model trained once, but capable of generalizing across a full spectrum of unseen tasks. Our extensive empirical evaluation shows that the suggested approach halves the computational effort of A* by up to a factor of 2.2, while still providing solutions within 3% of the optimal cost on average altogether on the tasks that are completely different from the ones used for training $\\unicode{x2013}$ a milestone reached for the first time by a learnable solver."}
{"id": "2602.23410", "categories": ["cs.LG", "cs.AI", "eess.SP", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2602.23410", "abs": "https://arxiv.org/abs/2602.23410", "authors": ["Hanning Guo", "Farah Abdellatif", "Hanwen Bi", "Andrei Galbenus", "Jon. N. Shah", "Abigail Morrison", "Jürgen Dammers"], "title": "Brain-OF: An Omnifunctional Foundation Model for fMRI, EEG and MEG", "comment": null, "summary": "Brain foundation models have achieved remarkable advances across a wide range of neuroscience tasks. However, most existing models are limited to a single functional modality, restricting their ability to exploit complementary spatiotemporal dynamics and the collective data scale across imaging techniques. To address this limitation, we propose Brain-OF, the first omnifunctional brain foundation model jointly pretrained on fMRI, EEG and MEG, capable of handling both unimodal and multimodal inputs within a unified framework. To reconcile heterogeneous spatiotemporal resolutions, we introduce the Any-Resolution Neural Signal Sampler, which projects diverse brain signals into a shared semantic space.To further manage semantic shifts, the Brain-OF backbone integrates DINT attention with a Sparse Mixture of Experts, where shared experts capture modality-invariant representations and routed experts specialize in modality-specific semantics. Furthermore, we propose Masked Temporal-Frequency Modeling, a dual-domain pretraining objective that jointly reconstructs brain signals in both the time and frequency domains. Brain-OF is pretrained on a large-scale corpus comprising around 40 datasets and demonstrates superior performance across diverse downstream tasks, highlighting the benefits of joint multimodal integration and dual-domain pretraining."}
{"id": "2602.24280", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.24280", "abs": "https://arxiv.org/abs/2602.24280", "authors": ["Arnaud Coatanhay", "Angélique Drémeau"], "title": "Geometric Resilience of Quantum LiDAR in Turbulent Media: A Wasserstein Distance Approach", "comment": "8 pages, 4 figures. Submitted to Physical Review A", "summary": "Quantum-enhanced LiDAR, exploiting squeezed states of light, promises significant sensitivity gains over classical protocols. However, in realistic scenarios characterized by high optical losses and atmospheric turbulence, standard figures of merit, such as quantum fidelity or the quantum Chernoff bound, saturate rapidly, failing to provide a usable gradient for system optimization. In this work, we propose the Quantum Wasserstein Distance of order 2 ($W_2$) as a robust geometric metric for lossy quantum sensing. Unlike overlap-based measures, $W_2$ quantifies the transport cost in phase space and maintains a linear response to channel transmissivity, even in regimes where the quantum state is virtually indistinguishable from thermal noise. We derive an analytical threshold for the quantum advantage, demonstrating that squeezing is only beneficial when the transmissivity exceeds a critical value determined by the environmental noise-to-signal ratio. Furthermore, using Monte-Carlo simulations of a fading channel, we show that $W_2$ acts as a high-fidelity estimator of instantaneous link quality, exhibiting a wide dynamic range immune to the numerical instabilities of fidelity-based metrics. This geometric framework bridges the gap between quantum optimal transport and practical receiver design, paving the way for adaptive sensing in scattering media."}
{"id": "2602.23795", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23795", "abs": "https://arxiv.org/abs/2602.23795", "authors": ["Wenwu Tang", "Dong Wang", "Lothar Thiele", "Olga Saukh"], "title": "GRAIL: Post-hoc Compensation by Linear Reconstruction for Compressed Networks", "comment": "Conference on Parsimony and Learning (CPAL)", "summary": "Structured deep model compression methods are hardware-friendly and substantially reduce memory and inference costs. However, under aggressive compression, the resulting accuracy degradation often necessitates post-compression finetuning, which can be impractical due to missing labeled data or high training cost. We propose post-hoc blockwise compensation, called GRAIL, a simple zero-finetuning step applied after model compression that restores each block's input-output behavior using a small calibration set. The method summarizes hidden activations via a Gram matrix and applies ridge regression to linearly reconstruct the original hidden representation from the reduced one. The resulting reconstruction map is absorbed into the downstream projection weights, while the upstream layer is compressed. The approach is selector-agnostic (Magnitude, Wanda, Gram-based selection, or folding), data-aware (requiring only a few forward passes without gradients or labels), and recovers classic pruning or folding when the Gram matrix is near identity, indicating weak inter-channel correlations. Across ResNets, ViTs, and decoder-only LLMs, GRAIL consistently improves accuracy or perplexity over data-free and data-aware pruning or folding baselines in practical compression regimes, with manageable overhead and no backpropagation. The code is available at https://github.com/TWWinde/GRAIL."}
{"id": "2602.23446", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23446", "abs": "https://arxiv.org/abs/2602.23446", "authors": ["Alejandro Rodriguez Dominguez"], "title": "Human Supervision as an Information Bottleneck: A Unified Theory of Error Floors in Human-Guided Learning", "comment": "Proceedings from IEEE CAI 2026, Conference on Artificial Intelligence, 8-10 May, Granada, Spain. 8 Pages, 3 Figures, 7 Tables", "summary": "Large language models are trained primarily on human-generated data and feedback, yet they exhibit persistent errors arising from annotation noise, subjective preferences, and the limited expressive bandwidth of natural language. We argue that these limitations reflect structural properties of the supervision channel rather than model scale or optimization. We develop a unified theory showing that whenever the human supervision channel is not sufficient for a latent evaluation target, it acts as an information-reducing channel that induces a strictly positive excess-risk floor for any learner dominated by it. We formalize this Human-Bounded Intelligence limit and show that across six complementary frameworks (operator theory, PAC-Bayes, information theory, causal inference, category theory, and game-theoretic analyses of reinforcement learning from human feedback), non-sufficiency yields strictly positive lower bounds arising from the same structural decomposition into annotation noise, preference distortion, and semantic compression. The theory explains why scaling alone cannot eliminate persistent human-aligned errors and characterizes conditions under which auxiliary non-human signals (e.g., retrieval, program execution, tools) increase effective supervision capacity and collapse the floor by restoring information about the latent target. Experiments on real preference data, synthetic known-target tasks, and externally verifiable benchmarks confirm the predicted structural signatures: human-only supervision exhibits a persistent floor, while sufficiently informative auxiliary channels strictly reduce or eliminate excess error."}
{"id": "2602.23409", "categories": ["cs.LG", "cs.AI", "cs.ET", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23409", "abs": "https://arxiv.org/abs/2602.23409", "authors": ["Michael Poppel", "Jonas Stein", "Sebastian Wölckert", "Markus Baumann", "Claudia Linnhoff-Popien"], "title": "Long Range Frequency Tuning for QML", "comment": null, "summary": "Quantum machine learning models using angle encoding naturally represent truncated Fourier series, providing universal function approximation capabilities with sufficient circuit depth. For unary fixed-frequency encodings, circuit depth scales as O(omega_max * (omega_max + epsilon^{-2})) with target frequency magnitude omega_max and precision epsilon. Trainable-frequency approaches theoretically reduce this to match the target spectrum size, requiring only as many encoding gates as frequencies in the target spectrum. Despite this compelling efficiency, their practical effectiveness hinges on a key assumption: that gradient-based optimization can drive prefactors to arbitrary target values. We demonstrate through systematic experiments that frequency prefactors exhibit limited trainability: movement is constrained to approximately +/-1 units with typical learning rates. When target frequencies lie outside this reachable range, optimization frequently fails. To overcome this frequency reachability limitation, we propose grid-based initialization using ternary encodings, which generate dense integer frequency spectra. While this approach requires O(log_3(omega_max)) encoding gates -- more than the theoretical optimum but exponentially fewer than fixed-frequency methods -- it ensures target frequencies lie within the locally reachable range. On synthetic targets with three shifted high frequencies, ternary grid initialization achieves a median R^2 score of 0.9969, compared to 0.1841 for the trainable-frequency baseline. For the real-world Flight Passengers dataset, ternary grid initialization achieves a median R^2 score of 0.9671, representing a 22.8% improvement over trainable-frequency initialization (median R^2 = 0.7876)."}
{"id": "2602.23798", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.23798", "abs": "https://arxiv.org/abs/2602.23798", "authors": ["Tiantong Wang", "Xinyu Yan", "Tiantong Wu", "Yurong Hao", "Yong Jiang", "Fei Huang", "Wei Yang Bryan Lim"], "title": "MPU: Towards Secure and Privacy-Preserving Knowledge Unlearning for Large Language Models", "comment": null, "summary": "Machine unlearning for large language models often faces a privacy dilemma in which strict constraints prohibit sharing either the server's parameters or the client's forget set. To address this dual non-disclosure constraint, we propose MPU, an algorithm-agnostic privacy-preserving Multiple Perturbed Copies Unlearning framework that primarily introduces two server-side modules: Pre-Process for randomized copy generation and Post-Process for update aggregation. In Pre-Process, the server distributes multiple perturbed and reparameterized model instances, allowing the client to execute unlearning locally on its private forget set without accessing the server's exact original parameters. After local unlearning, the server performs Post-Process by inverting the reparameterization and aggregating updates with a harmonic denoising procedure to alleviate the impact of perturbation. Experiments with seven unlearning algorithms show that MPU achieves comparable unlearning performance to noise-free baselines, with most algorithms' average degradation well below 1% under 10% noise, and can even outperform the noise-free baseline for some algorithms under 1% noise. Code is available at https://github.com/Tristan-SHU/MPU."}
{"id": "2602.23504", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.23504", "abs": "https://arxiv.org/abs/2602.23504", "authors": ["Anik Pramanik", "Murat Kantarcioglu", "Vincent Oria", "Shantanu Sharma"], "title": "FedDAG: Clustered Federated Learning via Global Data and Gradient Integration for Heterogeneous Environments", "comment": "This paper has been accepted in ICLR 2026", "summary": "Federated Learning (FL) enables a group of clients to collaboratively train a model without sharing individual data, but its performance drops when client data are heterogeneous. Clustered FL tackles this by grouping similar clients. However, existing clustered FL approaches rely solely on either data similarity or gradient similarity; however, this results in an incomplete assessment of client similarities. Prior clustered FL approaches also restrict knowledge and representation sharing to clients within the same cluster. This prevents cluster models from benefiting from the diverse client population across clusters. To address these limitations, FedDAG introduces a clustered FL framework, FedDAG, that employs a weighted, class-wise similarity metric that integrates both data and gradient information, providing a more holistic measure of similarity during clustering. In addition, FedDAG adopts a dual-encoder architecture for cluster models, comprising a primary encoder trained on its own clients' data and a secondary encoder refined using gradients from complementary clusters. This enables cross-cluster feature transfer while preserving cluster-specific specialization. Experiments on diverse benchmarks and data heterogeneity settings show that FedDAG consistently outperforms state-of-the-art clustered FL baselines in accuracy."}
{"id": "2602.23420", "categories": ["cond-mat.str-el", "cond-mat.stat-mech", "hep-th", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23420", "abs": "https://arxiv.org/abs/2602.23420", "authors": ["Thomas T. Dumitrescu", "Pierluigi Niro", "Ryan Thorngren"], "title": "From QED$_3$ to Self-Dual Multicriticality in the Fradkin-Shenker Model", "comment": null, "summary": "We consider the Fradkin-Shenker ${\\mathbb Z}_2$ gauge-Higgs lattice model in 2+1 dimensions, i.e. the toric code deformed by an in-plane magnetic field. Its phase diagram contains a multicritical CFT with gapless, mutually non-local electric and magnetic particles, exchanged by a ${\\mathbb Z}_2^{\\mathsf{D}}$ self-duality symmetry. We introduce a staggered generalization of the model in which these particles carry global $U(1)_e$ and $U(1)_m$ charges, respectively, and we propose a continuum QFT description in terms of QED$_3$ with $N_f = 2$ Dirac fermion flavors and a charge-two Higgs field with Yukawa couplings. The conjectured phase diagram harbors a multicritical CFT with $(O(2)_e \\times O(2)_m)\\rtimes\\mathbb{Z}_2^\\mathsf{D}$ symmetry, some of which is emergent in the QFT description. We compute the scaling dimensions of some operators using a large-$N_f$ expansion and find agreement with the emergent selection rules. The staggered model admits a deformation to the original Fradkin-Shenker model, which maps to unit-charge monopole operators in Higgs-Yukawa-QED$_3$ that break the $U(1)_e \\times U(1)_m$ symmetry. We show explicitly that this deformation reproduces all features of the Fradkin-Shenker phase diagram. Finally, we propose a multicritical duality between Higgs-Yukawa-QED$_3$ and the easy-plane $\\mathbb{ CP}^1$ model (i.e. two-flavor scalar QED$_3$ with a suitable potential), which describes spin-1/2 anti-ferromagnets on a square lattice. This duality implies a first-order line of Néel-VBS transitions ending in a deconfined quantum multicritical point, described by the same $O(2)_e \\times O(2)_m$ symmetric CFT that arises in the staggered Fradkin-Shenker model, which separates it from a gapped ${\\mathbb Z}_2$ spin liquid phase."}
{"id": "2602.23804", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23804", "abs": "https://arxiv.org/abs/2602.23804", "authors": ["Andreas Kernbach", "Amr Elsheikh", "Nicolas Grupp", "René Nagel", "Marco F. Huber"], "title": "Actor-Critic Pretraining for Proximal Policy Optimization", "comment": null, "summary": "Reinforcement learning (RL) actor-critic algorithms enable autonomous learning but often require a large number of environment interactions, which limits their applicability in robotics. Leveraging expert data can reduce the number of required environment interactions. A common approach is actor pretraining, where the actor network is initialized via behavioral cloning on expert demonstrations and subsequently fine-tuned with RL. In contrast, the initialization of the critic network has received little attention, despite its central role in policy optimization. This paper proposes a pretraining approach for actor-critic algorithms like Proximal Policy Optimization (PPO) that uses expert demonstrations to initialize both networks. The actor is pretrained via behavioral cloning, while the critic is pretrained using returns obtained from rollouts of the pretrained policy. The approach is evaluated on 15 simulated robotic manipulation and locomotion tasks. Experimental results show that actor-critic pretraining improves sample efficiency by 86.1% on average compared to no pretraining and by 30.9% to actor-only pretraining."}
{"id": "2602.23556", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.MA", "cs.PF"], "pdf": "https://arxiv.org/pdf/2602.23556", "abs": "https://arxiv.org/abs/2602.23556", "authors": ["Aishwarya Sarkar", "Sayan Ghosh", "Nathan Tallent", "Aman Chadha", "Tanya Roosta", "Ali Jannesari"], "title": "Rudder: Steering Prefetching in Distributed GNN Training using LLM Agents", "comment": "Accepted to the 40th ACM International Conference on Supercomputing (ICS 2026)", "summary": "Large-scale Graph Neural Networks (GNNs) are typically trained by sampling a vertex's neighbors to a fixed distance. Because large input graphs are distributed, training requires frequent irregular communication that stalls forward progress. Moreover, fetched data changes with graph, graph distribution, sample and batch parameters, and caching polices. Consequently, any static prefetching method will miss crucial opportunities to adapt to different dynamic conditions. In this paper, we introduce Rudder, a software module embedded in the state-of-the-art AWS DistDGL framework, to autonomously prefetch remote nodes and minimize communication. Rudder's adaptation contrasts with both standard heuristics and traditional ML classifiers. We observe that the generative AI found in contemporary Large Language Models (LLMs) exhibits emergent properties like In-Context Learning (ICL) for zero-shot tasks, with logical multi-step reasoning. We find this behavior well-suited for adaptive control even with substantial undertraining. Evaluations using standard datasets and unseen configurations on the NERSC Perlmutter supercomputer show up to 91% improvement in end-to-end training performance over baseline DistDGL (no prefetching), and an 82% improvement over static prefetching, reducing communication by over 50%. Our code is available at https://github.com/aishwaryyasarkar/rudder-llm-agent."}
{"id": "2602.23600", "categories": ["cond-mat.str-el", "cond-mat.mes-hall", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23600", "abs": "https://arxiv.org/abs/2602.23600", "authors": ["Li Chen", "Zhiping Yao"], "title": "Second-quantized approach to the study of Halperin state in fractional quantum Hall effect", "comment": "9 pages", "summary": "We give a recursion relation for the second-quantized fermionic (bosonic) Halperin state, which avoids exact diagonalization of its two-component first-quantized parent Hamiltonian. We validate this formula by proving that the second-quantized Halperin state, as recursively defined in this formula, is indeed a zero mode of the corresponding second-quantized parent Hamiltonian and that it has the correct filling factor."}
{"id": "2602.23811", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23811", "abs": "https://arxiv.org/abs/2602.23811", "authors": ["Xiang Li", "Nan Jiang", "Yuheng Zhang"], "title": "Beyond State-Wise Mirror Descent: Offline Policy Optimization with Parameteric Policies", "comment": null, "summary": "We investigate the theoretical aspects of offline reinforcement learning (RL) under general function approximation. While prior works (e.g., Xie et al., 2021) have established the theoretical foundations of learning a good policy from offline data via pessimism, existing algorithms that are computationally tractable (often in an oracle-efficient sense), such as PSPI, only apply to finite and small action spaces. Moreover, these algorithms rely on state-wise mirror descent and require actors to be implicitly induced from the critic functions, failing to accommodate standalone policy parameterization which is ubiquitous in practice. In this work, we address these limitations and extend the theoretical guarantees to parameterized policy classes over large or continuous action spaces. When extending mirror descent to parameterized policies, we identify contextual coupling as the core difficulty, and show how connecting mirror descent to natural policy gradient leads to novel analyses, guarantees, and algorithmic insights, including a surprising unification between offline RL and imitation learning."}
{"id": "2602.23566", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23566", "abs": "https://arxiv.org/abs/2602.23566", "authors": ["Asiri Wijesinghe", "Sevvandi Kandanaarachchi", "Daniel M. Steinberg", "Cheng Soon Ong"], "title": "Flowette: Flow Matching with Graphette Priors for Graph Generation", "comment": "37 Pages", "summary": "We study generative modeling of graphs with recurring subgraph motifs. We propose Flowette, a continuous flow matching framework, that employs a graph neural network based transformer to learn a velocity field defined over graph representations with node and edge attributes. Our model preserves topology through optimal transport based coupling, and long-range structural dependencies through regularisation. To incorporate domain driven structural priors, we introduce graphettes, a new probabilistic family of graph structure models that generalize graphons via controlled structural edits for motifs like rings, stars and trees. We theoretically analyze the coupling, invariance, and structural properties of the proposed framework, and empirically evaluate it on synthetic and small-molecule graph generation tasks. Flowette demonstrates consistent improvements, highlighting the effectiveness of combining structural priors with flow-based training for modeling complex graph distributions."}
{"id": "2602.24008", "categories": ["cond-mat.stat-mech", "cond-mat.quant-gas", "cond-mat.str-el", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.24008", "abs": "https://arxiv.org/abs/2602.24008", "authors": ["Kazuya Fujimoto", "Taiki Ishiyama", "Taiga Kurose", "Takato Yoshimura", "Tomohiro Sasamoto"], "title": "Exact Anomalous Current Fluctuations in Quantum Many-Body Dynamics", "comment": "35 pages, 4 figures", "summary": "Fluctuations of integrated currents have attracted considerable interest over the past decades in the context of statistical mechanics. Recently, anomalous current fluctuations, characterized by the M-Wright function, were obtained exactly in a classical automaton [$Ž$. Krajnik et al., Phys. Rev. Lett. 128, 160601 (2022)], and previous studies have shown that the anomalous behavior can arise in a variety of classical systems. Despite the rapidly growing interest in such anomalous behaviors, which capture a universal aspect of one-dimensional many-body transport, the exact derivation of the M-Wright function in quantum many-body systems has remained elusive. In this Letter, we present the first exact microscopic derivation of the M-Wright function in quantum many-body dynamics by analyzing the integrated spin current in a one-dimensional Fermi-Hubbard model with infinitely strong repulsive interactions. Our results lay the groundwork for exploring anomalous integrated currents in a broad class of quantum many-body systems."}
{"id": "2602.23816", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23816", "abs": "https://arxiv.org/abs/2602.23816", "authors": ["George Papadopoulos", "George A. Vouros"], "title": "Learning to maintain safety through expert demonstrations in settings with unknown constraints: A Q-learning perspective", "comment": "Accepted for publication at AAMAS 2026", "summary": "Given a set of trajectories demonstrating the execution of a task safely in a constrained MDP with observable rewards but with unknown constraints and non-observable costs, we aim to find a policy that maximizes the likelihood of demonstrated trajectories trading the balance between being conservative and increasing significantly the likelihood of high-rewarding trajectories but with potentially unsafe steps. Having these objectives, we aim towards learning a policy that maximizes the probability of the most $promising$ trajectories with respect to the demonstrations. In so doing, we formulate the ``promise\" of individual state-action pairs in terms of $Q$ values, which depend on task-specific rewards as well as on the assessment of states' safety, mixing expectations in terms of rewards and safety. This entails a safe Q-learning perspective of the inverse learning problem under constraints: The devised Safe $Q$ Inverse Constrained Reinforcement Learning (SafeQIL) algorithm is compared to state-of-the art inverse constraint reinforcement learning algorithms to a set of challenging benchmark tasks, showing its merits."}
{"id": "2602.23581", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23581", "abs": "https://arxiv.org/abs/2602.23581", "authors": ["Xiang Ao"], "title": "SDMixer: Sparse Dual-Mixer for Time Series Forecasting", "comment": "12pages,2 figures", "summary": "Multivariate time series forecasting is widely applied in fields such as transportation, energy, and finance. However, the data commonly suffers from issues of multi-scale characteristics, weak correlations, and noise interference, which limit the predictive performance of existing models. This paper proposes a dual-stream sparse Mixer prediction framework that extracts global trends and local dynamic features from sequences in both the frequency and time domains, respectively. It employs a sparsity mechanism to filter out invalid information, thereby enhancing the accuracy of cross-variable dependency modeling. Experimental results demonstrate that this method achieves leading performance on multiple real-world scenario datasets, validating its effectiveness and generality. The code is available at https://github.com/SDMixer/SDMixer"}
{"id": "2602.24220", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.24220", "abs": "https://arxiv.org/abs/2602.24220", "authors": ["Miras Seilkhan", "Adilbek Taizhanov"], "title": "Comparing Classical and Quantum Variational Classifiers on the XOR Problem", "comment": "32 pages, 17 figures. Code and experiment scripts available at https://github.com/mseilkhan/XOR-research-Quantum-ML-vs-Classic", "summary": "Quantum machine learning applies principles such as superposition and entanglement to data processing and optimization. Variational quantum models operate on qubits in high-dimensional Hilbert spaces and provide an alternative approach to model expressivity. We compare classical models and a variational quantum classifier on the XOR problem. Logistic regression, a one-hidden-layer multilayer perceptron, and a two-qubit variational quantum classifier with circuit depths 1 and 2 are evaluated on synthetic XOR datasets with varying Gaussian noise and sample sizes using accuracy and binary cross-entropy.\n  Performance is determined primarily by model expressivity. Logistic regression and the depth-1 quantum circuit fail to represent XOR reliably, whereas the multilayer perceptron and the depth-2 quantum circuit achieve perfect test accuracy under representative conditions. Robustness analyses across noise levels, dataset sizes, and random seeds confirm that circuit depth is decisive for quantum performance on this task. Despite matching accuracy, the multilayer perceptron achieves lower binary cross-entropy and substantially shorter training time. Hardware execution preserves the global XOR structure but introduces structured deviations in the decision function. Overall, deeper variational quantum classifiers can match classical neural networks in accuracy on low-dimensional XOR benchmarks, but no clear empirical advantage in robustness or efficiency is observed in the examined settings."}
{"id": "2602.23824", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23824", "abs": "https://arxiv.org/abs/2602.23824", "authors": ["Pavlin G. Poličar", "Dalibor Stanimirović", "Blaž Zupan"], "title": "Inferring Chronic Treatment Onset from ePrescription Data: A Renewal Process Approach", "comment": null, "summary": "Longitudinal electronic health record (EHR) data are often left-censored, making diagnosis records incomplete and unreliable for determining disease onset. In contrast, outpatient prescriptions form renewal-based trajectories that provide a continuous signal of disease management. We propose a probabilistic framework to infer chronic treatment onset by modeling prescription dynamics as a renewal process and detecting transitions from sporadic to sustained therapy via change-point detection between a baseline Poisson (sporadic prescribing) regime and a regime-specific Weibull (sustained therapy) renewal model. Using a nationwide ePrescription dataset of 2.4 million individuals, we show that the approach yields more temporally plausible onset estimates than naive rule-based triggering, substantially reducing implausible early detections under strong left censoring. Detection performance varies across diseases and is strongly associated with prescription density, highlighting both the strengths and limits of treatment-based onset inference."}
{"id": "2602.23614", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23614", "abs": "https://arxiv.org/abs/2602.23614", "authors": ["Kejing Yin", "Haizhou Xu", "Wenfang Yao", "Chen Liu", "Zijie Chen", "Yui Haang Cheung", "William K. Cheung", "Jing Qin"], "title": "When Does Multimodal Learning Help in Healthcare? A Benchmark on EHR and Chest X-Ray Fusion", "comment": null, "summary": "Machine learning holds promise for advancing clinical decision support, yet it remains unclear when multimodal learning truly helps in practice, particularly under modality missingness and fairness constraints. In this work, we conduct a systematic benchmark of multimodal fusion between Electronic Health Records (EHR) and chest X-rays (CXR) on standardized cohorts from MIMIC-IV and MIMIC-CXR, aiming to answer four fundamental questions: when multimodal fusion improves clinical prediction, how different fusion strategies compare, how robust existing methods are to missing modalities, and whether multimodal models achieve algorithmic fairness. Our study reveals several key insights. Multimodal fusion improves performance when modalities are complete, with gains concentrating in diseases that require complementary information from both EHR and CXR. While cross-modal learning mechanisms capture clinically meaningful dependencies beyond simple concatenation, the rich temporal structure of EHR introduces strong modality imbalance that architectural complexity alone cannot overcome. Under realistic missingness, multimodal benefits rapidly degrade unless models are explicitly designed to handle incomplete inputs. Moreover, multimodal fusion does not inherently improve fairness, with subgroup disparities mainly arising from unequal sensitivity across demographic groups. To support reproducible and extensible evaluation, we further release a flexible benchmarking toolkit that enables plug-and-play integration of new models and datasets. Together, this work provides actionable guidance on when multimodal learning helps, when it fails, and why, laying the foundation for developing clinically deployable multimodal systems that are both effective and reliable. The open-source toolkit can be found at https://github.com/jakeykj/CareBench."}
{"id": "2602.24276", "categories": ["cond-mat.stat-mech", "hep-th", "math-ph", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.24276", "abs": "https://arxiv.org/abs/2602.24276", "authors": ["Samuel H. Pickering", "Bruno Bertini"], "title": "Asymptotically Solvable Quantum Circuits", "comment": null, "summary": "The discovery of chaotic quantum circuits with (partially) solvable dynamics has played a key role in our understanding of non-equilibrium quantum matter and, at the same time, has helped the development of concrete platforms for quantum computation. It was shown that solvability does not prevent the generation of chaotic dynamics, however, it imposes non-trivial constraints on the generated correlations. A natural question is then whether it is possible to gain insight into the generic case despite the latter being very hard to access. To address this question here we introduce a family of 'asymptotically solvable' quantum circuits where the solvability constraints only affect correlations on length scales beyond a tuneable threshold. This means that their dynamics are only solvable for long enough times: for times shorter than the threshold they are generic. We show this by computing both their dynamical correlations on the equilibrium (infinite temperature) state and their thermalisation dynamics following quantum quenches from compatible (asymptotically solvable) non-equilibrium initial states. The class of systems we introduce is generically ergodic but contains a non-interacting point, which we use to provide exact analytical results, complementing those of numerical experiments, on the non-solvable early time regime."}
{"id": "2602.23827", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23827", "abs": "https://arxiv.org/abs/2602.23827", "authors": ["Junkang Liu", "Fanhua Shang", "Yuxuan Tian", "Hongying Liu", "Yuanyuan Liu"], "title": "FedNSAM:Consistency of Local and Global Flatness for Federated Learning", "comment": null, "summary": "In federated learning (FL), multi-step local updates and data heterogeneity usually lead to sharper global minima, which degrades the performance of the global model. Popular FL algorithms integrate sharpness-aware minimization (SAM) into local training to address this issue. However, in the high data heterogeneity setting, the flatness in local training does not imply the flatness of the global model. Therefore, minimizing the sharpness of the local loss surfaces on the client data does not enable the effectiveness of SAM in FL to improve the generalization ability of the global model. We define the \\textbf{flatness distance} to explain this phenomenon. By rethinking the SAM in FL and theoretically analyzing the \\textbf{flatness distance}, we propose a novel \\textbf{FedNSAM} algorithm that accelerates the SAM algorithm by introducing global Nesterov momentum into the local update to harmonize the consistency of global and local flatness. \\textbf{FedNSAM} uses the global Nesterov momentum as the direction of local estimation of client global perturbations and extrapolation. Theoretically, we prove a tighter convergence bound than FedSAM by Nesterov extrapolation. Empirically, we conduct comprehensive experiments on CNN and Transformer models to verify the superior performance and efficiency of \\textbf{FedNSAM}. The code is available at https://github.com/junkangLiu0/FedNSAM."}
{"id": "2602.23636", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23636", "abs": "https://arxiv.org/abs/2602.23636", "authors": ["Zhihao Ding", "Jinming Li", "Ze Lu", "Jieming Shi"], "title": "FlexGuard: Continuous Risk Scoring for Strictness-Adaptive LLM Content Moderation", "comment": null, "summary": "Ensuring the safety of LLM-generated content is essential for real-world deployment. Most existing guardrail models formulate moderation as a fixed binary classification task, implicitly assuming a fixed definition of harmfulness. In practice, enforcement strictness - how conservatively harmfulness is defined and enforced - varies across platforms and evolves over time, making binary moderators brittle under shifting requirements. We first introduce FlexBench, a strictness-adaptive LLM moderation benchmark that enables controlled evaluation under multiple strictness regimes. Experiments on FlexBench reveal substantial cross-strictness inconsistency in existing moderators: models that perform well under one regime can degrade substantially under others, limiting their practical usability. To address this, we propose FlexGuard, an LLM-based moderator that outputs a calibrated continuous risk score reflecting risk severity and supports strictness-specific decisions via thresholding. We train FlexGuard via risk-alignment optimization to improve score-severity consistency and provide practical threshold selection strategies to adapt to target strictness at deployment. Experiments on FlexBench and public benchmarks demonstrate that FlexGuard achieves higher moderation accuracy and substantially improved robustness under varying strictness. We release the source code and data to support reproducibility."}
{"id": "2602.23852", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.23852", "abs": "https://arxiv.org/abs/2602.23852", "authors": ["Zhaowen Wang", "Dongdong Zhou", "Qi Xu", "Fengyu Cong", "Mohammad Al-Sa'd", "Jenni Raitoharju"], "title": "ULW-SleepNet: An Ultra-Lightweight Network for Multimodal Sleep Stage Scoring", "comment": "Accepted to ICASSP 2026", "summary": "Automatic sleep stage scoring is crucial for the diagnosis and treatment of sleep disorders. Although deep learning models have advanced the field, many existing models are computationally demanding and designed for single-channel electroencephalography (EEG), limiting their practicality for multimodal polysomnography (PSG) data. To overcome this, we propose ULW-SleepNet, an ultra-lightweight multimodal sleep stage scoring framework that efficiently integrates information from multiple physiological signals. ULW-SleepNet incorporates a novel Dual-Stream Separable Convolution (DSSC) Block, depthwise separable convolutions, channel-wise parameter sharing, and global average pooling to reduce computational overhead while maintaining competitive accuracy. Evaluated on the Sleep-EDF-20 and Sleep-EDF-78 datasets, ULW-SleepNet achieves accuracies of 86.9% and 81.4%, respectively, with only 13.3K parameters and 7.89M FLOPs. Compared to state-of-the-art methods, our model reduces parameters by up to 98.6% with only marginal performance loss, demonstrating its strong potential for real-time sleep monitoring on wearable and IoT devices. The source code for this study is publicly available at https://github.com/wzw999/ULW-SLEEPNET."}
{"id": "2602.23638", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23638", "abs": "https://arxiv.org/abs/2602.23638", "authors": ["Haoran Zhang", "Dongjun Kim", "Seohyeon Cha", "Haris Vikalo"], "title": "FedRot-LoRA: Mitigating Rotational Misalignment in Federated LoRA", "comment": "preprint", "summary": "Federated LoRA provides a communication-efficient mechanism for fine-tuning large language models on decentralized data. In practice, however, a discrepancy between the factor-wise averaging used to preserve low rank and the mathematically correct aggregation of local updates can cause significant aggregation error and unstable training. We argue that a major source of this problem is rotational misalignment, arising from the rotational invariance of low-rank factorizations -- semantically equivalent updates can be represented in different latent subspaces across clients since $(B_i R_i)(R_i^\\top A_i) = B_i A_i$. When such misaligned factors are averaged directly, they interfere destructively and degrade the global update. To address this issue, we propose FedRot-LoRA, a federated LoRA framework that aligns client updates via orthogonal transformations prior to aggregation. This alignment preserves the semantic update while reducing cross-client subspace mismatch, without increasing communication cost or restricting model expressivity. We provide a convergence analysis that examines the aggregation error induced by factor-wise averaging and shows how rotational alignment yields a tighter upper bound on this error. Extensive experiments on natural language understanding and generative tasks demonstrate that FedRot-LoRA consistently outperforms existing federated LoRA baselines across a range of heterogeneity levels and LoRA ranks."}
{"id": "2602.23880", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23880", "abs": "https://arxiv.org/abs/2602.23880", "authors": ["Zhang Wan", "Tingting Mu", "Samuel Kaski"], "title": "A Theory of Random Graph Shift in Truncated-Spectrum vRKHS", "comment": null, "summary": "This paper develops a theory of graph classification under domain shift through a random-graph generative lens, where we consider intra-class graphs sharing the same random graph model (RGM) and the domain shift induced by changes in RGM components. While classic domain adaptation (DA) theories have well-underpinned existing techniques to handle graph distribution shift, the information of graph samples, which are itself structured objects, is less explored. The non-Euclidean nature of graphs and specialized architectures for graph learning further complicate a fine-grained analysis of graph distribution shifts. In this paper, we propose a theory that assumes RGM as the data generative process, exploiting its connection to hypothesis complexity in function space perspective for such fine-grained analysis. Building on a vector-valued reproducing kernel Hilbert space (vRKHS) formulation, we derive a generalization bound whose shift penalty admits a factorization into (i) a domain discrepancy term, (ii) a spectral-geometry term summarized by the accessible truncated spectrum, and (iii) an amplitude term that aggregates convergence and construction-stability effects. We empirically verify the insights on these terms in both real data and simulations."}
{"id": "2602.23696", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23696", "abs": "https://arxiv.org/abs/2602.23696", "authors": ["Yongzhong Xu"], "title": "Optimizer-Induced Low-Dimensional Drift and Transverse Dynamics in Transformer Training", "comment": "18 pages, 4 figures", "summary": "We study the geometry of training trajectories in small transformer models and find that parameter updates organize into a dominant drift direction with transverse residual dynamics. Using uncentered, row-normalized trajectory PCA, we show that a single direction captures a large fraction of cumulative parameter movement early in training, while remaining components encode oscillatory behavior in auxiliary probe performance. Instantaneous gradients exhibit little alignment with this dominant direction, indicating that it arises from accumulated optimizer updates rather than per-batch gradient structure. Comparing AdamW with SGD variants at matched loss levels reveals substantial differences in trajectory geometry: AdamW develops multi-dimensional drift structure, whereas SGD-family optimizers produce nearly colinear parameter evolution and weaker probe dynamics. Reheating selectively perturbs transverse components with minimal effect on the dominant drift coordinate. These findings suggest that optimizer choice shapes the effective dimensionality and structure of learning trajectories beyond what is apparent from loss values alone."}
{"id": "2602.23881", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23881", "abs": "https://arxiv.org/abs/2602.23881", "authors": ["Alexander Samarin", "Sergei Krutikov", "Anton Shevtsov", "Sergei Skvortsov", "Filipp Fisin", "Alexander Golubev"], "title": "LK Losses: Direct Acceptance Rate Optimization for Speculative Decoding", "comment": null, "summary": "Speculative decoding accelerates autoregressive large language model (LLM) inference by using a lightweight draft model to propose candidate tokens that are then verified in parallel by the target model. The speedup is significantly determined by the acceptance rate, yet standard training minimizes Kullback-Leibler (KL) divergence as a proxy objective. While KL divergence and acceptance rate share the same global optimum, small draft models, having limited capacity, typically converge to suboptimal solutions where minimizing KL does not guarantee maximizing acceptance rate. To address this issue, we propose LK losses, special training objectives that directly target acceptance rate. Comprehensive experiments across four draft architectures and six target models, ranging from 8B to 685B parameters, demonstrate consistent improvements in acceptance metrics across all configurations compared to the standard KL-based training. We evaluate our approach on general, coding and math domains and report gains of up to 8-10% in average acceptance length. LK losses are easy to implement, introduce no computational overhead and can be directly integrated into any existing speculator training framework, making them a compelling alternative to the existing draft training objectives."}
{"id": "2602.23737", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23737", "abs": "https://arxiv.org/abs/2602.23737", "authors": ["Hanping Zhang", "Yuhong Guo"], "title": "Bridging Dynamics Gaps via Diffusion Schrödinger Bridge for Cross-Domain Reinforcement Learning", "comment": null, "summary": "Cross-domain reinforcement learning (RL) aims to learn transferable policies under dynamics shifts between source and target domains. A key challenge lies in the lack of target-domain environment interaction and reward supervision, which prevents direct policy learning. To address this challenge, we propose Bridging Dynamics Gaps for Cross-Domain Reinforcement Learning (BDGxRL), a novel framework that leverages Diffusion Schrödinger Bridge (DSB) to align source transitions with target-domain dynamics encoded in offline demonstrations. Moreover, we introduce a reward modulation mechanism that estimates rewards based on state transitions, applying to DSB-aligned samples to ensure consistency between rewards and target-domain dynamics. BDGxRL performs target-oriented policy learning entirely within the source domain, without access to the target environment or its rewards. Experiments on MuJoCo cross-domain benchmarks demonstrate that BDGxRL outperforms state-of-the-art baselines and shows strong adaptability under transition dynamics shifts."}
{"id": "2602.23947", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23947", "abs": "https://arxiv.org/abs/2602.23947", "authors": ["Oscar Hill", "Mateo Espinosa Zarlenga", "Mateja Jamnik"], "title": "Hierarchical Concept-based Interpretable Models", "comment": "Published as a conference paper at ICLR 2026", "summary": "Modern deep neural networks remain challenging to interpret due to the opacity of their latent representations, impeding model understanding, debugging, and debiasing. Concept Embedding Models (CEMs) address this by mapping inputs to human-interpretable concept representations from which tasks can be predicted. Yet, CEMs fail to represent inter-concept relationships and require concept annotations at different granularities during training, limiting their applicability. In this paper, we introduce Hierarchical Concept Embedding Models (HiCEMs), a new family of CEMs that explicitly model concept relationships through hierarchical structures. To enable HiCEMs in real-world settings, we propose Concept Splitting, a method for automatically discovering finer-grained sub-concepts from a pretrained CEM's embedding space without requiring additional annotations. This allows HiCEMs to generate fine-grained explanations from limited concept labels, reducing annotation burdens. Our evaluation across multiple datasets, including a user study and experiments on PseudoKitchens, a newly proposed concept-based dataset of 3D kitchen renders, demonstrates that (1) Concept Splitting discovers human-interpretable sub-concepts absent during training that can be used to train highly accurate HiCEMs, and (2) HiCEMs enable powerful test-time concept interventions at different granularities, leading to improved task accuracy."}
{"id": "2602.23784", "categories": ["cs.LG", "cs.AI", "q-fin.CP", "q-fin.TR"], "pdf": "https://arxiv.org/pdf/2602.23784", "abs": "https://arxiv.org/abs/2602.23784", "authors": ["Maxime Kawawa-Beaudan", "Srijan Sood", "Kassiani Papasotiriou", "Daniel Borrajo", "Manuela Veloso"], "title": "TradeFM: A Generative Foundation Model for Trade-flow and Market Microstructure", "comment": "29 pages, 17 figures, 6 tables. Preprint", "summary": "Foundation models have transformed domains from language to genomics by learning general-purpose representations from large-scale, heterogeneous data. We introduce TradeFM, a 524M-parameter generative Transformer that brings this paradigm to market microstructure, learning directly from billions of trade events across >9K equities. To enable cross-asset generalization, we develop scale-invariant features and a universal tokenization scheme that map the heterogeneous, multi-modal event stream of order flow into a unified discrete sequence -- eliminating asset-specific calibration. Integrated with a deterministic market simulator, TradeFM-generated rollouts reproduce key stylized facts of financial returns, including heavy tails, volatility clustering, and absence of return autocorrelation. Quantitatively, TradeFM achieves 2-3x lower distributional error than Compound Hawkes baselines and generalizes zero-shot to geographically out-of-distribution APAC markets with moderate perplexity degradation. Together, these results suggest that scale-invariant trade representations capture transferable structure in market microstructure, opening a path toward synthetic data generation, stress testing, and learning-based trading agents."}
{"id": "2602.23968", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23968", "abs": "https://arxiv.org/abs/2602.23968", "authors": ["David Fox", "Sam Bowyer", "Song Liu", "Laurence Aitchison", "Raul Santos-Rodriguez", "Mengyue Yang"], "title": "Learning Generation Orders for Masked Discrete Diffusion Models via Variational Inference", "comment": "12 pages, 1 figure", "summary": "Masked discrete diffusion models (MDMs) are a promising new approach to generative modelling, offering the ability for parallel token generation and therefore greater efficiency than autoregressive counterparts. However, achieving an optimal balance between parallel generation and sample quality remains an open problem. Current approaches primarily address this issue through fixed, heuristic parallel sampling methods. There exist some recent learning based approaches to this problem, but its formulation from the perspective of variational inference remains underexplored. In this work, we propose a variational inference framework for learning parallel generation orders for MDMs. As part of our method, we propose a parameterisation for the approximate posterior of generation orders which facilitates parallelism and efficient sampling during training. Using this method, we conduct preliminary experiments on the GSM8K dataset, where our method performs competitively against heuristic sampling strategies in the regime of highly parallel generation. For example, our method achieves 33.1\\% accuracy with an average of only only 4 generation steps, compared to 23.7-29.0\\% accuracy achieved by standard competitor methods in the same number of steps. We believe further experiments and analysis of the method will yield valuable insights into the problem of parallel generation with MDMs."}
{"id": "2602.23789", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23789", "abs": "https://arxiv.org/abs/2602.23789", "authors": ["Aleksandr Ananikian", "Daniil Drozdov", "Konstantin Yakovlev"], "title": "UPath: Universal Planner Across Topological Heterogeneity For Grid-Based Pathfinding", "comment": null, "summary": "The performance of search algorithms for grid-based pathfinding, e.g. A*, critically depends on the heuristic function that is used to focus the search. Recent studies have shown that informed heuristics that take the positions/shapes of the obstacles into account can be approximated with the deep neural networks. Unfortunately, the existing learning-based approaches mostly rely on the assumption that training and test grid maps are drawn from the same distribution (e.g., city maps, indoor maps, etc.) and perform poorly on out-of-distribution tasks. This naturally limits their application in practice when often a universal solver is needed that is capable of efficiently handling any problem instance. In this work, we close this gap by designing an universal heuristic predictor: a model trained once, but capable of generalizing across a full spectrum of unseen tasks. Our extensive empirical evaluation shows that the suggested approach halves the computational effort of A* by up to a factor of 2.2, while still providing solutions within 3% of the optimal cost on average altogether on the tasks that are completely different from the ones used for training $\\unicode{x2013}$ a milestone reached for the first time by a learnable solver."}
{"id": "2602.23981", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23981", "abs": "https://arxiv.org/abs/2602.23981", "authors": ["Xianglong Shi", "Ziheng Chen", "Yunhan Jiang", "Nicu Sebe"], "title": "Intrinsic Lorentz Neural Network", "comment": "Published in ICLR 2026", "summary": "Real-world data frequently exhibit latent hierarchical structures, which can be naturally represented by hyperbolic geometry. Although recent hyperbolic neural networks have demonstrated promising results, many existing architectures remain partially intrinsic, mixing Euclidean operations with hyperbolic ones or relying on extrinsic parameterizations. To address it, we propose the \\emph{Intrinsic Lorentz Neural Network} (ILNN), a fully intrinsic hyperbolic architecture that conducts all computations within the Lorentz model. At its core, the network introduces a novel \\emph{point-to-hyperplane} fully connected layer (FC), replacing traditional Euclidean affine logits with closed-form hyperbolic distances from features to learned Lorentz hyperplanes, thereby ensuring that the resulting geometric decision functions respect the inherent curvature. Around this fundamental layer, we design intrinsic modules: GyroLBN, a Lorentz batch normalization that couples gyro-centering with gyro-scaling, consistently outperforming both LBN and GyroBN while reducing training time. We additionally proposed a gyro-additive bias for the FC output, a Lorentz patch-concatenation operator that aligns the expected log-radius across feature blocks via a digamma-based scale, and a Lorentz dropout layer. Extensive experiments conducted on CIFAR-10/100 and two genomic benchmarks (TEB and GUE) illustrate that ILNN achieves state-of-the-art performance and computational cost among hyperbolic models and consistently surpasses strong Euclidean baselines. The code is available at \\href{https://github.com/Longchentong/ILNN}{\\textcolor{magenta}{this url}}."}
{"id": "2602.23798", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.23798", "abs": "https://arxiv.org/abs/2602.23798", "authors": ["Tiantong Wang", "Xinyu Yan", "Tiantong Wu", "Yurong Hao", "Yong Jiang", "Fei Huang", "Wei Yang Bryan Lim"], "title": "MPU: Towards Secure and Privacy-Preserving Knowledge Unlearning for Large Language Models", "comment": null, "summary": "Machine unlearning for large language models often faces a privacy dilemma in which strict constraints prohibit sharing either the server's parameters or the client's forget set. To address this dual non-disclosure constraint, we propose MPU, an algorithm-agnostic privacy-preserving Multiple Perturbed Copies Unlearning framework that primarily introduces two server-side modules: Pre-Process for randomized copy generation and Post-Process for update aggregation. In Pre-Process, the server distributes multiple perturbed and reparameterized model instances, allowing the client to execute unlearning locally on its private forget set without accessing the server's exact original parameters. After local unlearning, the server performs Post-Process by inverting the reparameterization and aggregating updates with a harmonic denoising procedure to alleviate the impact of perturbation. Experiments with seven unlearning algorithms show that MPU achieves comparable unlearning performance to noise-free baselines, with most algorithms' average degradation well below 1% under 10% noise, and can even outperform the noise-free baseline for some algorithms under 1% noise. Code is available at https://github.com/Tristan-SHU/MPU."}
{"id": "2602.23994", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23994", "abs": "https://arxiv.org/abs/2602.23994", "authors": ["Vrushank Ahire", "Yogesh Kumar", "Anouck Girard", "M. A. Ganaie"], "title": "MINT: Multimodal Imaging-to-Speech Knowledge Transfer for Early Alzheimer's Screening", "comment": null, "summary": "Alzheimer's disease is a progressive neurodegenerative disorder in which mild cognitive impairment (MCI) marks a critical transition between aging and dementia. Neuroimaging modalities, such as structural MRI, provide biomarkers of this transition; however, their high costs and infrastructure needs limit their deployment at a population scale. Speech analysis offers a non-invasive alternative, but speech-only classifiers are developed independently of neuroimaging, leaving decision boundaries biologically ungrounded and limiting reliability on the subtle CN-versus-MCI distinction. We propose MINT (Multimodal Imaging-to-Speech Knowledge Transfer), a three-stage cross-modal framework that transfers biomarker structure from MRI into a speech encoder at training time. An MRI teacher, trained on 1,228 subjects, defines a compact neuroimaging embedding space for CN-versus-MCI classification. A residual projection head aligns speech representations to this frozen imaging manifold via a combined geometric loss, adapting speech to the learned biomarker space while preserving imaging encoder fidelity. The frozen MRI classifier, which is never exposed to speech, is applied to aligned embeddings at inference and requires no scanner. Evaluation on ADNI-4 shows aligned speech achieves performance comparable to speech-only baselines (AUC 0.720 vs 0.711) while requiring no imaging at inference, demonstrating that MRI-derived decision boundaries can ground speech representations. Multimodal fusion improves over MRI alone (0.973 vs 0.958). Ablation studies identify dropout regularization and self-supervised pretraining as critical design decisions. To our knowledge, this is the first demonstration of MRI-to-speech knowledge transfer for early Alzheimer's screening, establishing a biologically grounded pathway for population-level cognitive triage without neuroimaging at inference."}
{"id": "2602.23811", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23811", "abs": "https://arxiv.org/abs/2602.23811", "authors": ["Xiang Li", "Nan Jiang", "Yuheng Zhang"], "title": "Beyond State-Wise Mirror Descent: Offline Policy Optimization with Parameteric Policies", "comment": null, "summary": "We investigate the theoretical aspects of offline reinforcement learning (RL) under general function approximation. While prior works (e.g., Xie et al., 2021) have established the theoretical foundations of learning a good policy from offline data via pessimism, existing algorithms that are computationally tractable (often in an oracle-efficient sense), such as PSPI, only apply to finite and small action spaces. Moreover, these algorithms rely on state-wise mirror descent and require actors to be implicitly induced from the critic functions, failing to accommodate standalone policy parameterization which is ubiquitous in practice. In this work, we address these limitations and extend the theoretical guarantees to parameterized policy classes over large or continuous action spaces. When extending mirror descent to parameterized policies, we identify contextual coupling as the core difficulty, and show how connecting mirror descent to natural policy gradient leads to novel analyses, guarantees, and algorithmic insights, including a surprising unification between offline RL and imitation learning."}
{"id": "2602.23997", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23997", "abs": "https://arxiv.org/abs/2602.23997", "authors": ["Florent Delgrange"], "title": "Foundation World Models for Agents that Learn, Verify, and Adapt Reliably Beyond Static Environments", "comment": "AAMAS 2026, Blue Sky Idea Track. 4 pages, 1 Figure", "summary": "The next generation of autonomous agents must not only learn efficiently but also act reliably and adapt their behavior in open worlds. Standard approaches typically assume fixed tasks and environments with little or no novelty, which limits world models' ability to support agents that must evolve their policies as conditions change. This paper outlines a vision for foundation world models: persistent, compositional representations that unify reinforcement learning, reactive/program synthesis, and abstraction mechanisms. We propose an agenda built around four components: (i) learnable reward models from specifications to support optimization with clear objectives; (ii) adaptive formal verification integrated throughout learning; (iii) online abstraction calibration to quantify the reliability of the model's predictions; and (iv) test-time synthesis and world-model generation guided by verifiers. Together, these components enable agents to synthesize verifiable programs, derive new policies from a small number of interactions, and maintain correctness while adapting to novelty. The resulting framework positions foundation world models as a substrate for learning, reasoning, and adaptation, laying the groundwork for agents that not only act well but can explain and justify the behavior they adopt."}
{"id": "2602.23816", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23816", "abs": "https://arxiv.org/abs/2602.23816", "authors": ["George Papadopoulos", "George A. Vouros"], "title": "Learning to maintain safety through expert demonstrations in settings with unknown constraints: A Q-learning perspective", "comment": "Accepted for publication at AAMAS 2026", "summary": "Given a set of trajectories demonstrating the execution of a task safely in a constrained MDP with observable rewards but with unknown constraints and non-observable costs, we aim to find a policy that maximizes the likelihood of demonstrated trajectories trading the balance between being conservative and increasing significantly the likelihood of high-rewarding trajectories but with potentially unsafe steps. Having these objectives, we aim towards learning a policy that maximizes the probability of the most $promising$ trajectories with respect to the demonstrations. In so doing, we formulate the ``promise\" of individual state-action pairs in terms of $Q$ values, which depend on task-specific rewards as well as on the assessment of states' safety, mixing expectations in terms of rewards and safety. This entails a safe Q-learning perspective of the inverse learning problem under constraints: The devised Safe $Q$ Inverse Constrained Reinforcement Learning (SafeQIL) algorithm is compared to state-of-the art inverse constraint reinforcement learning algorithms to a set of challenging benchmark tasks, showing its merits."}
{"id": "2602.24012", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.24012", "abs": "https://arxiv.org/abs/2602.24012", "authors": ["Roy Betser", "Eyal Gofer", "Meir Yossef Levi", "Guy Gilboa"], "title": "InfoNCE Induces Gaussian Distribution", "comment": "Accepted to ICLR 2026, Oral", "summary": "Contrastive learning has become a cornerstone of modern representation learning, allowing training with massive unlabeled data for both task-specific and general (foundation) models. A prototypical loss in contrastive training is InfoNCE and its variants. In this work, we show that the InfoNCE objective induces Gaussian structure in representations that emerge from contrastive training. We establish this result in two complementary regimes. First, we show that under certain alignment and concentration assumptions, projections of the high-dimensional representation asymptotically approach a multivariate Gaussian distribution. Next, under less strict assumptions, we show that adding a small asymptotically vanishing regularization term that promotes low feature norm and high feature entropy leads to similar asymptotic results. We support our analysis with experiments on synthetic and CIFAR-10 datasets across multiple encoder architectures and sizes, demonstrating consistent Gaussian behavior. This perspective provides a principled explanation for commonly observed Gaussianity in contrastive representations. The resulting Gaussian model enables principled analytical treatment of learned representations and is expected to support a wide range of applications in contrastive learning."}
{"id": "2602.23827", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23827", "abs": "https://arxiv.org/abs/2602.23827", "authors": ["Junkang Liu", "Fanhua Shang", "Yuxuan Tian", "Hongying Liu", "Yuanyuan Liu"], "title": "FedNSAM:Consistency of Local and Global Flatness for Federated Learning", "comment": null, "summary": "In federated learning (FL), multi-step local updates and data heterogeneity usually lead to sharper global minima, which degrades the performance of the global model. Popular FL algorithms integrate sharpness-aware minimization (SAM) into local training to address this issue. However, in the high data heterogeneity setting, the flatness in local training does not imply the flatness of the global model. Therefore, minimizing the sharpness of the local loss surfaces on the client data does not enable the effectiveness of SAM in FL to improve the generalization ability of the global model. We define the \\textbf{flatness distance} to explain this phenomenon. By rethinking the SAM in FL and theoretically analyzing the \\textbf{flatness distance}, we propose a novel \\textbf{FedNSAM} algorithm that accelerates the SAM algorithm by introducing global Nesterov momentum into the local update to harmonize the consistency of global and local flatness. \\textbf{FedNSAM} uses the global Nesterov momentum as the direction of local estimation of client global perturbations and extrapolation. Theoretically, we prove a tighter convergence bound than FedSAM by Nesterov extrapolation. Empirically, we conduct comprehensive experiments on CNN and Transformer models to verify the superior performance and efficiency of \\textbf{FedNSAM}. The code is available at https://github.com/junkangLiu0/FedNSAM."}
{"id": "2602.24040", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.24040", "abs": "https://arxiv.org/abs/2602.24040", "authors": ["Daniel Yang", "Samuel Stante", "Florian Redhardt", "Lena Libon", "Parnian Kassraie", "Ido Hakimi", "Barna Pásztor", "Andreas Krause"], "title": "RewardUQ: A Unified Framework for Uncertainty-Aware Reward Models", "comment": null, "summary": "Reward models are central to aligning large language models (LLMs) with human preferences. Yet most approaches rely on pointwise reward estimates that overlook the epistemic uncertainty in reward models arising from limited human feedback. Recent work suggests that quantifying this uncertainty can reduce the costs of human annotation via uncertainty-guided active learning and mitigate reward overoptimization in LLM post-training. However, uncertainty-aware reward models have so far been adopted without thorough comparison, leaving them poorly understood. This work introduces a unified framework, RewardUQ, to systematically evaluate uncertainty quantification for reward models. We compare common methods along standard metrics measuring accuracy and calibration, and we propose a new ranking strategy incorporating both dimensions for a simplified comparison. Our experimental results suggest that model size and initialization have the most meaningful impact on performance, and most prior work could have benefited from alternative design choices. To foster the development and evaluation of new methods and aid the deployment in downstream applications, we release our open-source framework as a Python package. Our code is available at https://github.com/lasgroup/rewarduq."}
{"id": "2602.23947", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23947", "abs": "https://arxiv.org/abs/2602.23947", "authors": ["Oscar Hill", "Mateo Espinosa Zarlenga", "Mateja Jamnik"], "title": "Hierarchical Concept-based Interpretable Models", "comment": "Published as a conference paper at ICLR 2026", "summary": "Modern deep neural networks remain challenging to interpret due to the opacity of their latent representations, impeding model understanding, debugging, and debiasing. Concept Embedding Models (CEMs) address this by mapping inputs to human-interpretable concept representations from which tasks can be predicted. Yet, CEMs fail to represent inter-concept relationships and require concept annotations at different granularities during training, limiting their applicability. In this paper, we introduce Hierarchical Concept Embedding Models (HiCEMs), a new family of CEMs that explicitly model concept relationships through hierarchical structures. To enable HiCEMs in real-world settings, we propose Concept Splitting, a method for automatically discovering finer-grained sub-concepts from a pretrained CEM's embedding space without requiring additional annotations. This allows HiCEMs to generate fine-grained explanations from limited concept labels, reducing annotation burdens. Our evaluation across multiple datasets, including a user study and experiments on PseudoKitchens, a newly proposed concept-based dataset of 3D kitchen renders, demonstrates that (1) Concept Splitting discovers human-interpretable sub-concepts absent during training that can be used to train highly accurate HiCEMs, and (2) HiCEMs enable powerful test-time concept interventions at different granularities, leading to improved task accuracy."}
{"id": "2602.24066", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24066", "abs": "https://arxiv.org/abs/2602.24066", "authors": ["Tobias Nygaard"], "title": "pathsig: A GPU-Accelerated Library for Truncated and Projected Path Signatures", "comment": null, "summary": "Path signatures provide a rich representation of sequential data, with strong theoretical guarantees and good performance in a variety of machine-learning tasks. While signatures have progressed from fixed feature extractors to trainable components of machine-learning models, existing libraries often lack the required scalability for large-scale, gradient-based learning. To address this gap, this paper introduces pathsig, a PyTorch-native library that computes path signatures directly in the word basis. By using CUDA kernels to update signature coefficients in parallel over prefix-closed word sets, pathsig achieves high GPU throughput and near-minimal peak memory. Compared with other libraries, pathsig achieves 10-30x speedups for computation of truncated signatures and up to 4-10x speedups in training that require backpropagation through the signature. Beyond regular truncation, pathsig supports projections of the (infinite-dimensional) signature onto user-specified sets of words and anisotropic truncation motivated by inhomogeneous path regularity, enabling more compact representations that can reduce dimensionality, redundancy, and computational cost."}
{"id": "2602.23981", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23981", "abs": "https://arxiv.org/abs/2602.23981", "authors": ["Xianglong Shi", "Ziheng Chen", "Yunhan Jiang", "Nicu Sebe"], "title": "Intrinsic Lorentz Neural Network", "comment": "Published in ICLR 2026", "summary": "Real-world data frequently exhibit latent hierarchical structures, which can be naturally represented by hyperbolic geometry. Although recent hyperbolic neural networks have demonstrated promising results, many existing architectures remain partially intrinsic, mixing Euclidean operations with hyperbolic ones or relying on extrinsic parameterizations. To address it, we propose the \\emph{Intrinsic Lorentz Neural Network} (ILNN), a fully intrinsic hyperbolic architecture that conducts all computations within the Lorentz model. At its core, the network introduces a novel \\emph{point-to-hyperplane} fully connected layer (FC), replacing traditional Euclidean affine logits with closed-form hyperbolic distances from features to learned Lorentz hyperplanes, thereby ensuring that the resulting geometric decision functions respect the inherent curvature. Around this fundamental layer, we design intrinsic modules: GyroLBN, a Lorentz batch normalization that couples gyro-centering with gyro-scaling, consistently outperforming both LBN and GyroBN while reducing training time. We additionally proposed a gyro-additive bias for the FC output, a Lorentz patch-concatenation operator that aligns the expected log-radius across feature blocks via a digamma-based scale, and a Lorentz dropout layer. Extensive experiments conducted on CIFAR-10/100 and two genomic benchmarks (TEB and GUE) illustrate that ILNN achieves state-of-the-art performance and computational cost among hyperbolic models and consistently surpasses strong Euclidean baselines. The code is available at \\href{https://github.com/Longchentong/ILNN}{\\textcolor{magenta}{this url}}."}
{"id": "2602.24069", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2602.24069", "abs": "https://arxiv.org/abs/2602.24069", "authors": ["Ryan DeWolfe"], "title": "Leveraging Non-linear Dimension Reduction and Random Walk Co-occurrence for Node Embedding", "comment": "13 pages, 6 figures", "summary": "Leveraging non-linear dimension reduction techniques, we remove the low dimension constraint from node embedding and propose COVE, an explainable high dimensional embedding that, when reduced to low dimension with UMAP, slightly increases performance on clustering and link prediction tasks. The embedding is inspired by neural embedding methods that use co-occurrence on a random walk as an indication of similarity, and is closely related to a diffusion process. Extending on recent community detection benchmarks, we find that a COVE UMAP HDBSCAN pipeline performs similarly to the popular Louvain algorithm."}
{"id": "2602.23994", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23994", "abs": "https://arxiv.org/abs/2602.23994", "authors": ["Vrushank Ahire", "Yogesh Kumar", "Anouck Girard", "M. A. Ganaie"], "title": "MINT: Multimodal Imaging-to-Speech Knowledge Transfer for Early Alzheimer's Screening", "comment": null, "summary": "Alzheimer's disease is a progressive neurodegenerative disorder in which mild cognitive impairment (MCI) marks a critical transition between aging and dementia. Neuroimaging modalities, such as structural MRI, provide biomarkers of this transition; however, their high costs and infrastructure needs limit their deployment at a population scale. Speech analysis offers a non-invasive alternative, but speech-only classifiers are developed independently of neuroimaging, leaving decision boundaries biologically ungrounded and limiting reliability on the subtle CN-versus-MCI distinction. We propose MINT (Multimodal Imaging-to-Speech Knowledge Transfer), a three-stage cross-modal framework that transfers biomarker structure from MRI into a speech encoder at training time. An MRI teacher, trained on 1,228 subjects, defines a compact neuroimaging embedding space for CN-versus-MCI classification. A residual projection head aligns speech representations to this frozen imaging manifold via a combined geometric loss, adapting speech to the learned biomarker space while preserving imaging encoder fidelity. The frozen MRI classifier, which is never exposed to speech, is applied to aligned embeddings at inference and requires no scanner. Evaluation on ADNI-4 shows aligned speech achieves performance comparable to speech-only baselines (AUC 0.720 vs 0.711) while requiring no imaging at inference, demonstrating that MRI-derived decision boundaries can ground speech representations. Multimodal fusion improves over MRI alone (0.973 vs 0.958). Ablation studies identify dropout regularization and self-supervised pretraining as critical design decisions. To our knowledge, this is the first demonstration of MRI-to-speech knowledge transfer for early Alzheimer's screening, establishing a biologically grounded pathway for population-level cognitive triage without neuroimaging at inference."}
{"id": "2602.24081", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24081", "abs": "https://arxiv.org/abs/2602.24081", "authors": ["Viet Bac Nguyen", "Phuong Thai Nguyen"], "title": "Adaptive Correlation-Weighted Intrinsic Rewards for Reinforcement Learning", "comment": null, "summary": "We propose ACWI (Adaptive Correlation Weighted Intrinsic), an adaptive intrinsic reward scaling framework designed to dynamically balance intrinsic and extrinsic rewards for improved exploration in sparse reward reinforcement learning. Unlike conventional approaches that rely on manually tuned scalar coefficients, which often result in unstable or suboptimal performance across tasks, ACWI learns a state dependent scaling coefficient online. Specifically, ACWI introduces a lightweight Beta Network that predicts the intrinsic reward weight directly from the agent state through an encoder based architecture. The scaling mechanism is optimized using a correlation based objective that encourages alignment between the weighted intrinsic rewards and discounted future extrinsic returns. This formulation enables task adaptive exploration incentives while preserving computational efficiency and training stability. We evaluate ACWI on a suite of sparse reward environments in MiniGrid. Experimental results demonstrate that ACWI consistently improves sample efficiency and learning stability compared to fixed intrinsic reward baselines, achieving superior performance with minimal computational overhead."}
{"id": "2602.23997", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23997", "abs": "https://arxiv.org/abs/2602.23997", "authors": ["Florent Delgrange"], "title": "Foundation World Models for Agents that Learn, Verify, and Adapt Reliably Beyond Static Environments", "comment": "AAMAS 2026, Blue Sky Idea Track. 4 pages, 1 Figure", "summary": "The next generation of autonomous agents must not only learn efficiently but also act reliably and adapt their behavior in open worlds. Standard approaches typically assume fixed tasks and environments with little or no novelty, which limits world models' ability to support agents that must evolve their policies as conditions change. This paper outlines a vision for foundation world models: persistent, compositional representations that unify reinforcement learning, reactive/program synthesis, and abstraction mechanisms. We propose an agenda built around four components: (i) learnable reward models from specifications to support optimization with clear objectives; (ii) adaptive formal verification integrated throughout learning; (iii) online abstraction calibration to quantify the reliability of the model's predictions; and (iv) test-time synthesis and world-model generation guided by verifiers. Together, these components enable agents to synthesize verifiable programs, derive new policies from a small number of interactions, and maintain correctness while adapting to novelty. The resulting framework positions foundation world models as a substrate for learning, reasoning, and adaptation, laying the groundwork for agents that not only act well but can explain and justify the behavior they adopt."}
{"id": "2602.24083", "categories": ["cs.LG", "math.PR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.24083", "abs": "https://arxiv.org/abs/2602.24083", "authors": ["Xinlong Du", "Harsha Honnappa", "Vinayak Rao"], "title": "Neural Diffusion Intensity Models for Point Process Data", "comment": null, "summary": "Cox processes model overdispersed point process data via a latent stochastic intensity, but both nonparametric estimation of the intensity model and posterior inference over intensity paths are typically intractable, relying on expensive MCMC methods. We introduce Neural Diffusion Intensity Models, a variational framework for Cox processes driven by neural SDEs. Our key theoretical result, based on enlargement of filtrations, shows that conditioning on point process observations preserves the diffusion structure of the latent intensity with an explicit drift correction. This guarantees the variational family contains the true posterior, so that ELBO maximization coincides with maximum likelihood estimation under sufficient model capacity. We design an amortized encoder architecture that maps variable-length event sequences to posterior intensity paths by simulating the drift-corrected SDE, replacing repeated MCMC runs with a single forward pass. Experiments on synthetic and real-world data demonstrate accurate recovery of latent intensity dynamics and posterior paths, with orders-of-magnitude speedups over MCMC-based methods."}
{"id": "2602.24040", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.24040", "abs": "https://arxiv.org/abs/2602.24040", "authors": ["Daniel Yang", "Samuel Stante", "Florian Redhardt", "Lena Libon", "Parnian Kassraie", "Ido Hakimi", "Barna Pásztor", "Andreas Krause"], "title": "RewardUQ: A Unified Framework for Uncertainty-Aware Reward Models", "comment": null, "summary": "Reward models are central to aligning large language models (LLMs) with human preferences. Yet most approaches rely on pointwise reward estimates that overlook the epistemic uncertainty in reward models arising from limited human feedback. Recent work suggests that quantifying this uncertainty can reduce the costs of human annotation via uncertainty-guided active learning and mitigate reward overoptimization in LLM post-training. However, uncertainty-aware reward models have so far been adopted without thorough comparison, leaving them poorly understood. This work introduces a unified framework, RewardUQ, to systematically evaluate uncertainty quantification for reward models. We compare common methods along standard metrics measuring accuracy and calibration, and we propose a new ranking strategy incorporating both dimensions for a simplified comparison. Our experimental results suggest that model size and initialization have the most meaningful impact on performance, and most prior work could have benefited from alternative design choices. To foster the development and evaluation of new methods and aid the deployment in downstream applications, we release our open-source framework as a Python package. Our code is available at https://github.com/lasgroup/rewarduq."}
{"id": "2602.24115", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24115", "abs": "https://arxiv.org/abs/2602.24115", "authors": ["Zhizhou He", "Yang Luo", "Xinkai Liu", "Mahdi Boloursaz Mashhadi", "Mohammad Shojafar", "Merouane Debbah", "Rahim Tafazolli"], "title": "Agentic AI-RAN: Enabling Intent-Driven, Explainable and Self-Evolving Open RAN Intelligence", "comment": "9 pages, 4 figures", "summary": "Open RAN (O-RAN) exposes rich control and telemetry interfaces across the Non-RT RIC, Near-RT RIC, and distributed units, but also makes it harder to operate multi-tenant, multi-objective RANs in a safe and auditable manner. In parallel, agentic AI systems with explicit planning, tool use, memory, and self-management offer a natural way to structure long-lived control loops. This article surveys how such agentic controllers can be brought into O-RAN: we review the O-RAN architecture, contrast agentic controllers with conventional ML/RL xApps, and organise the task landscape around three clusters: network slice life-cycle, radio resource management (RRM) closed loops, and cross-cutting security, privacy, and compliance. We then introduce a small set of agentic primitives (Plan-Act-Observe-Reflect, skills as tool use, memory and evidence, and self-management gates) and show, in a multi-cell O-RAN simulation, how they improve slice life-cycle and RRM performance compared to conventional baselines and ablations that remove individual primitives. Security, privacy, and compliance are discussed as architectural constraints and open challenges for standards-aligned deployments. This framework achieves an average 8.83\\% reduction in resource usage across three classic network slices."}
{"id": "2602.24081", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24081", "abs": "https://arxiv.org/abs/2602.24081", "authors": ["Viet Bac Nguyen", "Phuong Thai Nguyen"], "title": "Adaptive Correlation-Weighted Intrinsic Rewards for Reinforcement Learning", "comment": null, "summary": "We propose ACWI (Adaptive Correlation Weighted Intrinsic), an adaptive intrinsic reward scaling framework designed to dynamically balance intrinsic and extrinsic rewards for improved exploration in sparse reward reinforcement learning. Unlike conventional approaches that rely on manually tuned scalar coefficients, which often result in unstable or suboptimal performance across tasks, ACWI learns a state dependent scaling coefficient online. Specifically, ACWI introduces a lightweight Beta Network that predicts the intrinsic reward weight directly from the agent state through an encoder based architecture. The scaling mechanism is optimized using a correlation based objective that encourages alignment between the weighted intrinsic rewards and discounted future extrinsic returns. This formulation enables task adaptive exploration incentives while preserving computational efficiency and training stability. We evaluate ACWI on a suite of sparse reward environments in MiniGrid. Experimental results demonstrate that ACWI consistently improves sample efficiency and learning stability compared to fixed intrinsic reward baselines, achieving superior performance with minimal computational overhead."}
{"id": "2602.24146", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24146", "abs": "https://arxiv.org/abs/2602.24146", "authors": ["Zitian Li", "Wang Chi Cheung"], "title": "Learning with a Budget: Identifying the Best Arm with Resource Constraints", "comment": "A preliminary version of this work, titled 'Best Arm Identification with Resource Constraints,' was presented at the 27th International Conference on Artificial Intelligence and Statistics (AISTATS 2024). This manuscript extends the original conference paper by providing improved theoretical results and more generalized conclusions, aiming for future journal submission. arXiv admin note: substantial text overlap with arXiv:2402.19090", "summary": "In many applications, evaluating the effectiveness of different alternatives comes with varying costs or resource usage. Motivated by such heterogeneity, we study the Best Arm Identification with Resource Constraints (BAIwRC) problem, where an agent seeks to identify the best alternative (aka arm) in the presence of resource constraints. Each arm pull consumes one or more types of limited resources. We make two key contributions. First, we propose the Successive Halving with Resource Rationing (SH-RR) algorithm, which integrates resource-aware allocation into the classical successive halving framework on best arm identification. The SH-RR algorithm unifies the theoretical analysis for both the stochastic and deterministic consumption settings, with a new \\textit{effective consumption measure"}
{"id": "2602.24209", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24209", "abs": "https://arxiv.org/abs/2602.24209", "authors": ["Mohsen Tajgardan", "Atena Shiranzaei", "Mahdi Rabbani", "Reza Khoshkangini", "Mahtab Jamali"], "title": "An Efficient Unsupervised Federated Learning Approach for Anomaly Detection in Heterogeneous IoT Networks", "comment": null, "summary": "Federated learning (FL) is an effective paradigm for distributed environments such as the Internet of Things (IoT), where data from diverse devices with varying functionalities remains localized while contributing to a shared global model. By eliminating the need to transmit raw data, FL inherently preserves privacy. However, the heterogeneous nature of IoT data, stemming from differences in device capabilities, data formats, and communication constraints, poses significant challenges to maintaining both global model performance and privacy. In the context of IoT-based anomaly detection, unsupervised FL offers a promising means to identify abnormal behavior without centralized data aggregation. Nevertheless, feature heterogeneity across devices complicates model training and optimization, hindering effective implementation. In this study we propose an efficient unsupervised FL framework that enhances anomaly detection by leveraging shared features from two distinct IoT datasets: one focused on anomaly detection and the other on device identification, while preserving dataset-specific features. To improve transparency and interpretability, we employ explainable AI techniques, such as SHAP, to identify key features influencing local model decisions. Experiments conducted on real-world IoT datasets demonstrate that the proposed method significantly outperforms conventional FL approaches in anomaly detection accuracy. This work underscores the potential of using shared features from complementary datasets to optimize unsupervised federated learning and achieve superior anomaly detection results in decentralized IoT environments."}
{"id": "2602.24149", "categories": ["cs.LG", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2602.24149", "abs": "https://arxiv.org/abs/2602.24149", "authors": ["Daniel S. Berman", "Brian Merritt", "Stanley Ta", "Dana Udwin", "Amanda Ernlund", "Jeremy Ratcliff", "Vijay Narayan"], "title": "What You Read is What You Classify: Highlighting Attributions to Text and Text-Like Inputs", "comment": "17 pages, 8 figures", "summary": "At present, there are no easily understood explainable artificial intelligence (AI) methods for discrete token inputs, like text. Most explainable AI techniques do not extend well to token sequences, where both local and global features matter, because state-of-the-art models, like transformers, tend to focus on global connections. Therefore, existing explainable AI algorithms fail by (i) identifying disparate tokens of importance, or (ii) assigning a large number of tokens a low value of importance. This method for explainable AI for tokens-based classifiers generalizes a mask-based explainable AI algorithm for images. It starts with an Explainer neural network that is trained to create masks to hide information not relevant for classification. Then, the Hadamard product of the mask and the continuous values of the classifier's embedding layer is taken and passed through the classifier, changing the magnitude of the embedding vector but keeping the orientation unchanged. The Explainer is trained for a taxonomic classifier for nucleotide sequences and it is shown that the masked segments are less relevant to classification than the unmasked ones. This method focused on the importance the token as a whole (i.e., a segment of the input sequence), producing a human-readable explanation."}
{"id": "2602.24266", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24266", "abs": "https://arxiv.org/abs/2602.24266", "authors": ["Amir Asiaee"], "title": "Efficient Discovery of Approximate Causal Abstractions via Neural Mechanism Sparsification", "comment": null, "summary": "Neural networks are hypothesized to implement interpretable causal mechanisms, yet verifying this requires finding a causal abstraction -- a simpler, high-level Structural Causal Model (SCM) faithful to the network under interventions. Discovering such abstractions is hard: it typically demands brute-force interchange interventions or retraining. We reframe the problem by viewing structured pruning as a search over approximate abstractions. Treating a trained network as a deterministic SCM, we derive an Interventional Risk objective whose second-order expansion yields closed-form criteria for replacing units with constants or folding them into neighbors. Under uniform curvature, our score reduces to activation variance, recovering variance-based pruning as a special case while clarifying when it fails. The resulting procedure efficiently extracts sparse, intervention-faithful abstractions from pretrained networks, which we validate via interchange interventions."}
{"id": "2602.24178", "categories": ["cs.LG", "cs.CC"], "pdf": "https://arxiv.org/pdf/2602.24178", "abs": "https://arxiv.org/abs/2602.24178", "authors": ["Adam R. Klivans", "Konstantinos Stavropoulos", "Arsen Vasilyan"], "title": "Sandwiching Polynomials for Geometric Concepts with Low Intrinsic Dimension", "comment": "30 pages", "summary": "Recent work has shown the surprising power of low-degree sandwiching polynomial approximators in the context of challenging learning settings such as learning with distribution shift, testable learning, and learning with contamination. A pair of sandwiching polynomials approximate a target function in expectation while also providing pointwise upper and lower bounds on the function's values. In this paper, we give a new method for constructing low-degree sandwiching polynomials that yield greatly improved degree bounds for several fundamental function classes and marginal distributions. In particular, we obtain degree $\\mathrm{poly}(k)$ sandwiching polynomials for functions of $k$ halfspaces under the Gaussian distribution, improving exponentially over the prior $2^{O(k)}$ bound. More broadly, our approach applies to function classes that are low-dimensional and have smooth boundary.\n  In contrast to prior work, our proof is relatively simple and directly uses the smoothness of the target function's boundary to construct sandwiching Lipschitz functions, which are amenable to results from high-dimensional approximation theory. For low-dimensional polynomial threshold functions (PTFs) with respect to Gaussians, we obtain doubly exponential improvements without applying the FT-mollification method of Kane used in the best previous result."}
{"id": "2602.24281", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24281", "abs": "https://arxiv.org/abs/2602.24281", "authors": ["Ali Behrouz", "Zeman Li", "Yuan Deng", "Peilin Zhong", "Meisam Razaviyayn", "Vahab Mirrokni"], "title": "Memory Caching: RNNs with Growing Memory", "comment": null, "summary": "Transformers have been established as the de-facto backbones for most recent advances in sequence modeling, mainly due to their growing memory capacity that scales with the context length. While plausible for retrieval tasks, it causes quadratic complexity and so has motivated recent studies to explore viable subquadratic recurrent alternatives. Despite showing promising preliminary results in diverse domains, such recurrent architectures underperform Transformers in recall-intensive tasks, often attributed to their fixed-size memory. In this paper, we introduce Memory Caching (MC), a simple yet effective technique that enhances recurrent models by caching checkpoints of their memory states (a.k.a. hidden states). Memory Caching allows the effective memory capacity of RNNs to grow with sequence length, offering a flexible trade-off that interpolates between the fixed memory (i.e., $O(L)$ complexity) of RNNs and the growing memory (i.e., $O(L^2)$ complexity) of Transformers. We propose four variants of MC, including gated aggregation and sparse selective mechanisms, and discuss their implications on both linear and deep memory modules. Our experimental results on language modeling, and long-context understanding tasks show that MC enhances the performance of recurrent models, supporting its effectiveness. The results of in-context recall tasks indicate that while Transformers achieve the best accuracy, our MC variants show competitive performance, close the gap with Transformers, and performs better than state-of-the-art recurrent models."}
{"id": "2602.24182", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24182", "abs": "https://arxiv.org/abs/2602.24182", "authors": ["Sikata Sengupta", "Guangyi Liu", "Omer Gottesman", "Joseph W Durham", "Michael Kearns", "Aaron Roth", "Michael Caldara"], "title": "Multi-Objective Reinforcement Learning for Large-Scale Tote Allocation in Human-Robot Collaborative Fulfillment Centers", "comment": null, "summary": "Optimizing the consolidation process in container-based fulfillment centers requires trading off competing objectives such as processing speed, resource usage, and space utilization while adhering to a range of real-world operational constraints. This process involves moving items between containers via a combination of human and robotic workstations to free up space for inbound inventory and increase container utilization. We formulate this problem as a large-scale Multi-Objective Reinforcement Learning (MORL) task with high-dimensional state spaces and dynamic system behavior. Our method builds on recent theoretical advances in solving constrained RL problems via best-response and no-regret dynamics in zero-sum games, enabling principled minimax policy learning. Policy evaluation on realistic warehouse simulations shows that our approach effectively trades off objectives, and we empirically observe that it learns a single policy that simultaneously satisfies all constraints, even if this is not theoretically guaranteed. We further introduce a theoretical framework to handle the problem of error cancellation, where time-averaged solutions display oscillatory behavior. This method returns a single iterate whose Lagrangian value is close to the minimax value of the game. These results demonstrate the promise of MORL in solving complex, high-impact decision-making problems in large-scale industrial systems."}
{"id": "2602.24283", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.24283", "abs": "https://arxiv.org/abs/2602.24283", "authors": ["Zhengbo Wang", "Jian Liang", "Ran He", "Zilei Wang", "Tieniu Tan"], "title": "Taming Momentum: Rethinking Optimizer States Through Low-Rank Approximation", "comment": "Camera-ready version. Accepted as Oral at ICLR 2026", "summary": "Modern optimizers like Adam and Muon are central to training large language models, but their reliance on first- and second-order momenta introduces significant memory overhead, which constrains scalability and computational efficiency. In this work, we reframe the exponential moving average (EMA) used in these momenta as the training of a linear regressor via online gradient flow. Building on this equivalence, we introduce LoRA-Pre, a novel low-rank optimizer designed for efficient pre-training. Specifically, LoRA-Pre reduces the optimizer's memory footprint by decomposing the full momentum matrix into a compact low-rank subspace within the online linear learner, thereby maintaining optimization performance while improving memory efficiency. We empirically validate LoRA-Pre's efficacy by pre-training models from the Llama architecture family, scaling from 60M to 1B parameters. LoRA-Pre achieves the highest performance across all model sizes. Notably, LoRA-Pre demonstrates remarkable rank efficiency, achieving comparable or superior results using only 1/8 the rank of baseline methods. Beyond pre-training, we evaluate LoRA-Pre's effectiveness in fine-tuning scenarios. With the same rank, LoRA-Pre consistently outperforms all efficient fine-tuning baselines. Specifically, compared to standard LoRA, LoRA-Pre achieves substantial improvements of 3.14 points on Llama-3.1-8B and 6.17 points on Llama-2-7B, validating our approach's effectiveness across both pre-training and fine-tuning paradigms. Our code is publicly available at https://github.com/mrflogs/LoRA-Pre."}
{"id": "2602.24201", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24201", "abs": "https://arxiv.org/abs/2602.24201", "authors": ["Egor Antipov", "Alessandro Palma", "Lorenzo Consoli", "Stephan Günnemann", "Andrea Dittadi", "Fabian J. Theis"], "title": "Flow-Based Density Ratio Estimation for Intractable Distributions with Applications in Genomics", "comment": null, "summary": "Estimating density ratios between pairs of intractable data distributions is a core problem in probabilistic modeling, enabling principled comparisons of sample likelihoods under different data-generating processes across conditions and covariates. While exact-likelihood models such as normalizing flows offer a promising approach to density ratio estimation, naive flow-based evaluations are computationally expensive, as they require simulating costly likelihood integrals for each distribution separately. In this work, we leverage condition-aware flow matching to derive a single dynamical formulation for tracking density ratios along generative trajectories. We demonstrate competitive performance on simulated benchmarks for closed-form ratio estimation, and show that our method supports versatile tasks in single-cell genomics data analysis, where likelihood-based comparisons of cellular states across experimental conditions enable treatment effect estimation and batch correction evaluation."}
{"id": "2602.24286", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24286", "abs": "https://arxiv.org/abs/2602.24286", "authors": ["Weinan Dai", "Hanlin Wu", "Qiying Yu", "Huan-ang Gao", "Jiahao Li", "Chengquan Jiang", "Weiqiang Lou", "Yufan Song", "Hongli Yu", "Jiaze Chen", "Wei-Ying Ma", "Ya-Qin Zhang", "Jingjing Liu", "Mingxuan Wang", "Xin Liu", "Hao Zhou"], "title": "CUDA Agent: Large-Scale Agentic RL for High-Performance CUDA Kernel Generation", "comment": null, "summary": "GPU kernel optimization is fundamental to modern deep learning but remains a highly specialized task requiring deep hardware expertise. Despite strong performance in general programming, large language models (LLMs) remain uncompetitive with compiler-based systems such as torch.compile for CUDA kernel generation. Existing CUDA code generation approaches either rely on training-free refinement or fine-tune models within fixed multi-turn execution-feedback loops, but both paradigms fail to fundamentally improve the model's intrinsic CUDA optimization ability, resulting in limited performance gains. We present CUDA Agent, a large-scale agentic reinforcement learning system that develops CUDA kernel expertise through three components: a scalable data synthesis pipeline, a skill-augmented CUDA development environment with automated verification and profiling to provide reliable reward signals, and reinforcement learning algorithmic techniques enabling stable training. CUDA Agent achieves state-of-the-art results on KernelBench, delivering 100\\%, 100\\%, and 92\\% faster rate over torch.compile on KernelBench Level-1, Level-2, and Level-3 splits, outperforming the strongest proprietary models such as Claude Opus 4.5 and Gemini 3 Pro by about 40\\% on the hardest Level-3 setting."}
{"id": "2602.24207", "categories": ["cs.LG", "cs.CY", "cs.GT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.24207", "abs": "https://arxiv.org/abs/2602.24207", "authors": ["Gabriele Farina", "Juan Carlos Perdomo"], "title": "The Stability of Online Algorithms in Performative Prediction", "comment": null, "summary": "The use of algorithmic predictions in decision-making leads to a feedback loop where the models we deploy actively influence the data distributions we see, and later use to retrain on. This dynamic was formalized by Perdomo et al. 2020 in their work on performative prediction. Our main result is an unconditional reduction showing that any no-regret algorithm deployed in performative settings converges to a (mixed) performatively stable equilibrium: a solution in which models actively shape data distributions in ways that their own predictions look optimal in hindsight. Prior to our work, all positive results in this area made strong restrictions on how models influenced distributions. By using a martingale argument and allowing randomization, we avoid any such assumption and sidestep recent hardness results for finding stable models. Lastly, on a more conceptual note, our connection sheds light on why common algorithms, like gradient descent, are naturally stabilizing and prevent runaway feedback loops. We hope our work enables future technical transfer of ideas between online optimization and performativity."}
{"id": "2602.24209", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24209", "abs": "https://arxiv.org/abs/2602.24209", "authors": ["Mohsen Tajgardan", "Atena Shiranzaei", "Mahdi Rabbani", "Reza Khoshkangini", "Mahtab Jamali"], "title": "An Efficient Unsupervised Federated Learning Approach for Anomaly Detection in Heterogeneous IoT Networks", "comment": null, "summary": "Federated learning (FL) is an effective paradigm for distributed environments such as the Internet of Things (IoT), where data from diverse devices with varying functionalities remains localized while contributing to a shared global model. By eliminating the need to transmit raw data, FL inherently preserves privacy. However, the heterogeneous nature of IoT data, stemming from differences in device capabilities, data formats, and communication constraints, poses significant challenges to maintaining both global model performance and privacy. In the context of IoT-based anomaly detection, unsupervised FL offers a promising means to identify abnormal behavior without centralized data aggregation. Nevertheless, feature heterogeneity across devices complicates model training and optimization, hindering effective implementation. In this study we propose an efficient unsupervised FL framework that enhances anomaly detection by leveraging shared features from two distinct IoT datasets: one focused on anomaly detection and the other on device identification, while preserving dataset-specific features. To improve transparency and interpretability, we employ explainable AI techniques, such as SHAP, to identify key features influencing local model decisions. Experiments conducted on real-world IoT datasets demonstrate that the proposed method significantly outperforms conventional FL approaches in anomaly detection accuracy. This work underscores the potential of using shared features from complementary datasets to optimize unsupervised federated learning and achieve superior anomaly detection results in decentralized IoT environments."}
{"id": "2602.24220", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.24220", "abs": "https://arxiv.org/abs/2602.24220", "authors": ["Miras Seilkhan", "Adilbek Taizhanov"], "title": "Comparing Classical and Quantum Variational Classifiers on the XOR Problem", "comment": "32 pages, 17 figures. Code and experiment scripts available at https://github.com/mseilkhan/XOR-research-Quantum-ML-vs-Classic", "summary": "Quantum machine learning applies principles such as superposition and entanglement to data processing and optimization. Variational quantum models operate on qubits in high-dimensional Hilbert spaces and provide an alternative approach to model expressivity. We compare classical models and a variational quantum classifier on the XOR problem. Logistic regression, a one-hidden-layer multilayer perceptron, and a two-qubit variational quantum classifier with circuit depths 1 and 2 are evaluated on synthetic XOR datasets with varying Gaussian noise and sample sizes using accuracy and binary cross-entropy.\n  Performance is determined primarily by model expressivity. Logistic regression and the depth-1 quantum circuit fail to represent XOR reliably, whereas the multilayer perceptron and the depth-2 quantum circuit achieve perfect test accuracy under representative conditions. Robustness analyses across noise levels, dataset sizes, and random seeds confirm that circuit depth is decisive for quantum performance on this task. Despite matching accuracy, the multilayer perceptron achieves lower binary cross-entropy and substantially shorter training time. Hardware execution preserves the global XOR structure but introduces structured deviations in the decision function. Overall, deeper variational quantum classifiers can match classical neural networks in accuracy on low-dimensional XOR benchmarks, but no clear empirical advantage in robustness or efficiency is observed in the examined settings."}
{"id": "2602.24231", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24231", "abs": "https://arxiv.org/abs/2602.24231", "authors": ["Hongrui Xie", "Junyu Cao", "Kan Xu"], "title": "Adaptive Combinatorial Experimental Design: Pareto Optimality for Decision-Making and Inference", "comment": "30 pages, 3 figure, AISTATS 2026 accepted paper", "summary": "In this paper, we provide the first investigation into adaptive combinatorial experimental design, focusing on the trade-off between regret minimization and statistical power in combinatorial multi-armed bandits (CMAB). While minimizing regret requires repeated exploitation of high-reward arms, accurate inference on reward gaps requires sufficient exploration of suboptimal actions. We formalize this trade-off through the concept of Pareto optimality and establish equivalent conditions for Pareto-efficient learning in CMAB. We consider two relevant cases under different information structures, i.e., full-bandit feedback and semi-bandit feedback, and propose two algorithms MixCombKL and MixCombUCB respectively for these two cases. We provide theoretical guarantees showing that both algorithms are Pareto optimal, achieving finite-time guarantees on both regret and estimation error of arm gaps. Our results further reveal that richer feedback significantly tightens the attainable Pareto frontier, with the primary gains arising from improved estimation accuracy under our proposed methods. Taken together, these findings establish a principled framework for adaptive combinatorial experimentation in multi-objective decision-making."}
{"id": "2602.24238", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24238", "abs": "https://arxiv.org/abs/2602.24238", "authors": ["Javier Pulido", "Filipe Rodrigues"], "title": "Time Series Foundation Models as Strong Baselines in Transportation Forecasting: A Large-Scale Benchmark Analysis", "comment": "6 pages", "summary": "Accurate forecasting of transportation dynamics is essential for urban mobility and infrastructure planning. Although recent work has achieved strong performance with deep learning models, these methods typically require dataset-specific training, architecture design and hyper-parameter tuning. This paper evaluates whether general-purpose time-series foundation models can serve as forecasters for transportation tasks by benchmarking the zero-shot performance of the state-of-the-art model, Chronos-2, across ten real-world datasets covering highway traffic volume and flow, urban traffic speed, bike-sharing demand, and electric vehicle charging station data. Under a consistent evaluation protocol, we find that, even without any task-specific fine-tuning, Chronos-2 delivers state-of-the-art or competitive accuracy across most datasets, frequently outperforming classical statistical baselines and specialized deep learning architectures, particularly at longer horizons. Beyond point forecasting, we evaluate its native probabilistic outputs using prediction-interval coverage and sharpness, demonstrating that Chronos-2 also provides useful uncertainty quantification without dataset-specific training. In general, this study supports the adoption of time-series foundation models as a key baseline for transportation forecasting research."}
{"id": "2602.24245", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24245", "abs": "https://arxiv.org/abs/2602.24245", "authors": ["Hainan Xu", "Vladimir Bataev", "Travis M. Bartley", "Jagadeesh Balam"], "title": "Chunk-wise Attention Transducers for Fast and Accurate Streaming Speech-to-Text", "comment": "Accepted at ICASSP 2026", "summary": "We propose Chunk-wise Attention Transducer (CHAT), a novel extension to RNN-T models that processes audio in fixed-size chunks while employing cross-attention within each chunk. This hybrid approach maintains RNN-T's streaming capability while introducing controlled flexibility for local alignment modeling. CHAT significantly reduces the temporal dimension that RNN-T must handle, yielding substantial efficiency improvements: up to 46.2% reduction in peak training memory, up to 1.36X faster training, and up to 1.69X faster inference. Alongside these efficiency gains, CHAT achieves consistent accuracy improvements over RNN-T across multiple languages and tasks -- up to 6.3% relative WER reduction for speech recognition and up to 18.0% BLEU improvement for speech translation. The method proves particularly effective for speech translation, where RNN-T's strict monotonic alignment hurts performance. Our results demonstrate that the CHAT model offers a practical solution for deploying more capable streaming speech models without sacrificing real-time constraints."}
{"id": "2602.24251", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24251", "abs": "https://arxiv.org/abs/2602.24251", "authors": ["Xiaolong Zhang", "Jianwei Zhang", "Selim Sevim", "Emek Demir", "Ece Eksi", "Xubo Song"], "title": "Histopathology Image Normalization via Latent Manifold Compaction", "comment": "11 pages", "summary": "Batch effects arising from technical variations in histopathology staining protocols, scanners, and acquisition pipelines pose a persistent challenge for computational pathology, hindering cross-batch generalization and limiting reliable deployment of models across clinical sites. In this work, we introduce Latent Manifold Compaction (LMC), an unsupervised representation learning framework that performs image harmonization by learning batch-invariant embeddings from a single source dataset through explicit compaction of stain-induced latent manifolds. This allows LMC to generalize to target domain data unseen during training. Evaluated on three challenging public and in-house benchmarks, LMC substantially reduces batch-induced separations across multiple datasets and consistently outperforms state-of-the-art normalization methods in downstream cross-batch classification and detection tasks, enabling superior generalization."}
{"id": "2602.24262", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24262", "abs": "https://arxiv.org/abs/2602.24262", "authors": ["Yijiashun Qi", "Yijiazhen Qi", "Tanmay Wagh"], "title": "Coverage-Aware Web Crawling for Domain-Specific Supplier Discovery via a Web--Knowledge--Web Pipeline", "comment": null, "summary": "Identifying the full landscape of small and medium-sized enterprises (SMEs) in specialized industry sectors is critical for supply-chain resilience, yet existing business databases suffer from substantial coverage gaps -- particularly for sub-tier suppliers and firms in emerging niche markets. We propose a \\textbf{Web--Knowledge--Web (W$\\to$K$\\to$W)} pipeline that iteratively (1)~crawls domain-specific web sources to discover candidate supplier entities, (2)~extracts and consolidates structured knowledge into a heterogeneous knowledge graph, and (3)~uses the knowledge graph's topology and coverage signals to guide subsequent crawling toward under-represented regions of the supplier space. To quantify discovery completeness, we introduce a \\textbf{coverage estimation framework} inspired by ecological species-richness estimators (Chao1, ACE) adapted for web-entity populations. Experiments on the semiconductor equipment manufacturing sector (NAICS 333242) demonstrate that the W$\\to$K$\\to$W pipeline achieves the highest precision (0.138) and F1 (0.118) among all methods using the same 213-page crawl budget, building a knowledge graph of 765 entities and 586 relations while reaching peak recall by iteration~3 with only 112 pages."}
{"id": "2602.24266", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24266", "abs": "https://arxiv.org/abs/2602.24266", "authors": ["Amir Asiaee"], "title": "Efficient Discovery of Approximate Causal Abstractions via Neural Mechanism Sparsification", "comment": null, "summary": "Neural networks are hypothesized to implement interpretable causal mechanisms, yet verifying this requires finding a causal abstraction -- a simpler, high-level Structural Causal Model (SCM) faithful to the network under interventions. Discovering such abstractions is hard: it typically demands brute-force interchange interventions or retraining. We reframe the problem by viewing structured pruning as a search over approximate abstractions. Treating a trained network as a deterministic SCM, we derive an Interventional Risk objective whose second-order expansion yields closed-form criteria for replacing units with constants or folding them into neighbors. Under uniform curvature, our score reduces to activation variance, recovering variance-based pruning as a special case while clarifying when it fails. The resulting procedure efficiently extracts sparse, intervention-faithful abstractions from pretrained networks, which we validate via interchange interventions."}
{"id": "2602.24278", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24278", "abs": "https://arxiv.org/abs/2602.24278", "authors": ["Shruti Joshi", "Théo Saulus", "Wieland Brendel", "Philippe Brouillard", "Dhanya Sridhar", "Patrik Reizinger"], "title": "Who Guards the Guardians? The Challenges of Evaluating Identifiability of Learned Representations", "comment": null, "summary": "Identifiability in representation learning is commonly evaluated using standard metrics (e.g., MCC, DCI, R^2) on synthetic benchmarks with known ground-truth factors. These metrics are assumed to reflect recovery up to the equivalence class guaranteed by identifiability theory. We show that this assumption holds only under specific structural conditions: each metric implicitly encodes assumptions about both the data-generating process (DGP) and the encoder. When these assumptions are violated, metrics become misspecified and can produce systematic false positives and false negatives. Such failures occur both within classical identifiability regimes and in post-hoc settings where identifiability is most needed. We introduce a taxonomy separating DGP assumptions from encoder geometry, use it to characterise the validity domains of existing metrics, and release an evaluation suite for reproducible stress testing and comparison."}
{"id": "2602.24281", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24281", "abs": "https://arxiv.org/abs/2602.24281", "authors": ["Ali Behrouz", "Zeman Li", "Yuan Deng", "Peilin Zhong", "Meisam Razaviyayn", "Vahab Mirrokni"], "title": "Memory Caching: RNNs with Growing Memory", "comment": null, "summary": "Transformers have been established as the de-facto backbones for most recent advances in sequence modeling, mainly due to their growing memory capacity that scales with the context length. While plausible for retrieval tasks, it causes quadratic complexity and so has motivated recent studies to explore viable subquadratic recurrent alternatives. Despite showing promising preliminary results in diverse domains, such recurrent architectures underperform Transformers in recall-intensive tasks, often attributed to their fixed-size memory. In this paper, we introduce Memory Caching (MC), a simple yet effective technique that enhances recurrent models by caching checkpoints of their memory states (a.k.a. hidden states). Memory Caching allows the effective memory capacity of RNNs to grow with sequence length, offering a flexible trade-off that interpolates between the fixed memory (i.e., $O(L)$ complexity) of RNNs and the growing memory (i.e., $O(L^2)$ complexity) of Transformers. We propose four variants of MC, including gated aggregation and sparse selective mechanisms, and discuss their implications on both linear and deep memory modules. Our experimental results on language modeling, and long-context understanding tasks show that MC enhances the performance of recurrent models, supporting its effectiveness. The results of in-context recall tasks indicate that while Transformers achieve the best accuracy, our MC variants show competitive performance, close the gap with Transformers, and performs better than state-of-the-art recurrent models."}
{"id": "2602.24283", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.24283", "abs": "https://arxiv.org/abs/2602.24283", "authors": ["Zhengbo Wang", "Jian Liang", "Ran He", "Zilei Wang", "Tieniu Tan"], "title": "Taming Momentum: Rethinking Optimizer States Through Low-Rank Approximation", "comment": "Camera-ready version. Accepted as Oral at ICLR 2026", "summary": "Modern optimizers like Adam and Muon are central to training large language models, but their reliance on first- and second-order momenta introduces significant memory overhead, which constrains scalability and computational efficiency. In this work, we reframe the exponential moving average (EMA) used in these momenta as the training of a linear regressor via online gradient flow. Building on this equivalence, we introduce LoRA-Pre, a novel low-rank optimizer designed for efficient pre-training. Specifically, LoRA-Pre reduces the optimizer's memory footprint by decomposing the full momentum matrix into a compact low-rank subspace within the online linear learner, thereby maintaining optimization performance while improving memory efficiency. We empirically validate LoRA-Pre's efficacy by pre-training models from the Llama architecture family, scaling from 60M to 1B parameters. LoRA-Pre achieves the highest performance across all model sizes. Notably, LoRA-Pre demonstrates remarkable rank efficiency, achieving comparable or superior results using only 1/8 the rank of baseline methods. Beyond pre-training, we evaluate LoRA-Pre's effectiveness in fine-tuning scenarios. With the same rank, LoRA-Pre consistently outperforms all efficient fine-tuning baselines. Specifically, compared to standard LoRA, LoRA-Pre achieves substantial improvements of 3.14 points on Llama-3.1-8B and 6.17 points on Llama-2-7B, validating our approach's effectiveness across both pre-training and fine-tuning paradigms. Our code is publicly available at https://github.com/mrflogs/LoRA-Pre."}
{"id": "2602.24286", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24286", "abs": "https://arxiv.org/abs/2602.24286", "authors": ["Weinan Dai", "Hanlin Wu", "Qiying Yu", "Huan-ang Gao", "Jiahao Li", "Chengquan Jiang", "Weiqiang Lou", "Yufan Song", "Hongli Yu", "Jiaze Chen", "Wei-Ying Ma", "Ya-Qin Zhang", "Jingjing Liu", "Mingxuan Wang", "Xin Liu", "Hao Zhou"], "title": "CUDA Agent: Large-Scale Agentic RL for High-Performance CUDA Kernel Generation", "comment": null, "summary": "GPU kernel optimization is fundamental to modern deep learning but remains a highly specialized task requiring deep hardware expertise. Despite strong performance in general programming, large language models (LLMs) remain uncompetitive with compiler-based systems such as torch.compile for CUDA kernel generation. Existing CUDA code generation approaches either rely on training-free refinement or fine-tune models within fixed multi-turn execution-feedback loops, but both paradigms fail to fundamentally improve the model's intrinsic CUDA optimization ability, resulting in limited performance gains. We present CUDA Agent, a large-scale agentic reinforcement learning system that develops CUDA kernel expertise through three components: a scalable data synthesis pipeline, a skill-augmented CUDA development environment with automated verification and profiling to provide reliable reward signals, and reinforcement learning algorithmic techniques enabling stable training. CUDA Agent achieves state-of-the-art results on KernelBench, delivering 100\\%, 100\\%, and 92\\% faster rate over torch.compile on KernelBench Level-1, Level-2, and Level-3 splits, outperforming the strongest proprietary models such as Claude Opus 4.5 and Gemini 3 Pro by about 40\\% on the hardest Level-3 setting."}
{"id": "2602.23503", "categories": ["cs.CC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23503", "abs": "https://arxiv.org/abs/2602.23503", "authors": ["Lianna Hambardzumyan", "Konstantin Myasnikov", "Artur Riazanov", "Morgan Shirley", "Adi Shraibman"], "title": "Spiky Rank and Its Applications to Rigidity and Circuits", "comment": null, "summary": "We introduce spiky rank, a new matrix parameter that enhances blocky rank by combining the combinatorial structure of the latter with linear-algebraic flexibility. A spiky matrix is block-structured with diagonal blocks that are arbitrary rank-one matrices, and the spiky rank of a matrix is the minimum number of such matrices required to express it as a sum. This measure extends blocky rank to real matrices and is more robust for problems with both combinatorial and algebraic character.\n  Our conceptual contribution is as follows: we propose spiky rank as a well-behaved candidate matrix complexity measure and demonstrate its potential through applications. We show that large spiky rank implies high matrix rigidity, and that spiky rank lower bounds yield lower bounds for depth-2 ReLU circuits, the basic building blocks of neural networks. On the technical side, we establish tight bounds for random matrices and develop a framework for explicit lower bounds, applying it to Hamming distance matrices and spectral expanders. Finally, we relate spiky rank to other matrix parameters, including blocky rank, sparsity, and the $γ_2$-norm."}
{"id": "2602.23541", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23541", "abs": "https://arxiv.org/abs/2602.23541", "authors": ["Arvind Raghavan", "Elias Bareinboim"], "title": "Causal Identification from Counterfactual Data: Completeness and Bounding Results", "comment": null, "summary": "Previous work establishing completeness results for $\\textit{counterfactual identification}$ has been circumscribed to the setting where the input data belongs to observational or interventional distributions (Layers 1 and 2 of Pearl's Causal Hierarchy), since it was generally presumed impossible to obtain data from counterfactual distributions, which belong to Layer 3. However, recent work (Raghavan & Bareinboim, 2025) has formally characterized a family of counterfactual distributions which can be directly estimated via experimental methods - a notion they call $\\textit{counterfactual realizabilty}$. This leaves open the question of what $\\textit{additional}$ counterfactual quantities now become identifiable, given this new access to (some) Layer 3 data. To answer this question, we develop the CTFIDU+ algorithm for identifying counterfactual queries from an arbitrary set of Layer 3 distributions, and prove that it is complete for this task. Building on this, we establish the theoretical limit of which counterfactuals can be identified from physically realizable distributions, thus implying the $\\textit{fundamental limit to exact causal inference in the non-parametric setting}$. Finally, given the impossibility of identifying certain critical types of counterfactuals, we derive novel analytic bounds for such quantities using realizable counterfactual data, and corroborate using simulations that counterfactual data helps tighten the bounds for non-identifiable quantities in practice."}
{"id": "2602.23579", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23579", "abs": "https://arxiv.org/abs/2602.23579", "authors": ["Guillem Rodríguez-Corominas", "Maria J. Blesa", "Christian Blum"], "title": "Construct, Merge, Solve & Adapt with Reinforcement Learning for the min-max Multiple Traveling Salesman Problem", "comment": null, "summary": "The Multiple Traveling Salesman Problem (mTSP) extends the Traveling Salesman Problem to m tours that start and end at a common depot and jointly visit all customers exactly once. In the min-max variant, the objective is to minimize the longest tour, reflecting workload balance. We propose a hybrid approach, Construct, Merge, Solve & Adapt with Reinforcement Learning (RL-CMSA), for the symmetric single-depot min-max mTSP. The method iteratively constructs diverse solutions using probabilistic clustering guided by learned pairwise q-values, merges routes into a compact pool, solves a restricted set-covering MILP, and refines solutions via inter-route remove, shift, and swap moves. The q-values are updated by reinforcing city-pair co-occurrences in high-quality solutions, while the pool is adapted through ageing and pruning. This combination of exact optimization and reinforcement-guided construction balances exploration and exploitation. Computational results on random and TSPLIB instances show that RL-CMSA consistently finds (near-)best solutions and outperforms a state-of-the-art hybrid genetic algorithm under comparable time limits, especially as instance size and the number of salesmen increase."}
{"id": "2602.23876", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23876", "abs": "https://arxiv.org/abs/2602.23876", "authors": ["Ning Gao", "Xiuhui Zhang", "Xingyu Jiang", "Mukang You", "Mohan Zhang", "Yue Deng"], "title": "RF-Agent: Automated Reward Function Design via Language Agent Tree Search", "comment": "39 pages, 9 tables, 11 figures, Project page see https://github.com/deng-ai-lab/RF-Agent", "summary": "Designing efficient reward functions for low-level control tasks is a challenging problem. Recent research aims to reduce reliance on expert experience by using Large Language Models (LLMs) with task information to generate dense reward functions. These methods typically rely on training results as feedback, iteratively generating new reward functions with greedy or evolutionary algorithms. However, they suffer from poor utilization of historical feedback and inefficient search, resulting in limited improvements in complex control tasks. To address this challenge, we propose RF-Agent, a framework that treats LLMs as language agents and frames reward function design as a sequential decision-making process, enhancing optimization through better contextual reasoning. RF-Agent integrates Monte Carlo Tree Search (MCTS) to manage the reward design and optimization process, leveraging the multi-stage contextual reasoning ability of LLMs. This approach better utilizes historical information and improves search efficiency to identify promising reward functions. Outstanding experimental results in 17 diverse low-level control tasks demonstrate the effectiveness of our method. The source code is available at https://github.com/deng-ai-lab/RF-Agent."}
{"id": "2602.24100", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24100", "abs": "https://arxiv.org/abs/2602.24100", "authors": ["Richard Csaky"], "title": "Artificial Agency Program: Curiosity, compression, and communication in agents", "comment": "This is a working draft. Feedback and criticism is most welcome", "summary": "This paper presents the Artificial Agency Program (AAP), a position and research agenda for building AI systems as reality embedded, resource-bounded agents whose development is driven by curiosity-as-learning-progress under physical and computational constraints. The central thesis is that AI is most useful when treated as part of an extended human--tool system that increases sensing, understanding, and actuation capability while reducing friction at the interface between people, tools, and environments. The agenda unifies predictive compression, intrinsic motivation, empowerment and control, interface quality (unification), and language/self-communication as selective information bottlenecks. We formulate these ideas as a falsifiable program with explicit costs, staged experiments, and a concrete multimodal tokenized testbed in which an agent allocates limited budget among observation, action, and deliberation. The aim is to provide a conceptual and experimental framework that connects intrinsic motivation, information theory, thermodynamics, bounded rationality, and modern reasoning systems"}
{"id": "2602.24195", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24195", "abs": "https://arxiv.org/abs/2602.24195", "authors": ["Gregory Kang Ruey Lau", "Hieu Dao", "Nicole Kan Hui Lin", "Bryan Kian Hsiang Low"], "title": "Uncertainty Quantification for Multimodal Large Language Models with Incoherence-adjusted Semantic Volume", "comment": "Earlier versions presented at ICLR 2025 QUESTION workshop and ICML 2025 R2-FM workshop", "summary": "Despite their capabilities, Multimodal Large Language Models (MLLMs) may produce plausible but erroneous outputs, hindering reliable deployment. Accurate uncertainty metrics could enable escalation of unreliable queries to human experts or larger models for improved performance. However, existing uncertainty metrics have practical constraints, such as being designed only for specific modalities, reliant on external tools, or computationally expensive. We introduce UMPIRE, a training-free uncertainty quantification framework for MLLMs that works efficiently across various input and output modalities without external tools, relying only on the models' own internal modality features. UMPIRE computes the incoherence-adjusted semantic volume of sampled MLLM responses for a given task instance, effectively capturing both the global semantic diversity of samples and the local incoherence of responses based on internal model confidence. We propose uncertainty desiderata for MLLMs and provide theoretical analysis motivating UMPIRE's design. Extensive experiments show that UMPIRE consistently outperforms baseline metrics in error detection and uncertainty calibration across image, audio, and video-text benchmarks, including adversarial and out-of-distribution settings. We also demonstrate UMPIRE's generalization to non-text output tasks, including image and audio generation."}
