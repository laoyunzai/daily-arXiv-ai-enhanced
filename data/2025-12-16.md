<div id=toc></div>

# Table of Contents

- [nlin.CD](#nlin.CD) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 17]
- [cs.AI](#cs.AI) [Total: 38]
- [cs.LG](#cs.LG) [Total: 102]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 6]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 4]
- [quant-ph](#quant-ph) [Total: 67]


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [1] [Delay factors in the genesis of limit sets of the non-ideal system "tank with liquid-electric motor"](https://arxiv.org/abs/2512.13026)
*I. A. Seit-Dzhelil,A. Yu. Shvets*

Main category: nlin.CD

TL;DR: 研究非理想确定性系统"带液体电动马达的储罐"，考虑两种延迟近似模型，分析延迟对系统规则和混沌极限集（吸引子）产生、演化和消失的影响，计算系统稳态动态特性，研究混沌过渡场景，发现延迟因素驱动的广义间歇性场景。


<details>
  <summary>Details</summary>
Motivation: 研究延迟对非理想确定性动力系统动态行为的影响，特别是延迟如何影响系统规则和混沌吸引子的出现、演化和消失，以及延迟因素如何驱动混沌过渡场景。

Method: 采用两种延迟近似模型研究"带液体电动马达的储罐"系统，分析延迟对系统动态特性的影响，计算稳态动态特性，研究混沌过渡场景，特别是延迟驱动的广义间歇性场景。

Result: 延迟对系统规则和混沌极限集有显著影响，延迟因素能够驱动系统出现广义间歇性场景，系统稳态动态特性被计算和分析，混沌过渡机制得到研究。

Conclusion: 延迟是非理想确定性动力系统中影响动态行为的关键因素，能够驱动系统从规则运动向混沌过渡，并实现广义间歇性场景，对理解延迟系统的复杂动态行为有重要意义。

Abstract: Non-ideal deterministic system "tank with liquid-electric motor" is studied. Two delay-approximation models are considered. Impact of the delay on the emergence, evolution and disappearance of regular and chaotic limit sets (attractors) of the system is investigated. The main dynamic characteristics of the system's steady-state regimes are computed and analyzed. Transition to chaos scenarios are studied. Realization of generalized intermittency scenario driven by delay factors is established.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [2] [Multichannel Kondo Effect in Superconducting Leads](https://arxiv.org/abs/2512.11965)
*Pradip Kattel,Abay Zhakenov,Natan Andrei*

Main category: cond-mat.str-el

TL;DR: 该论文展示了多通道近藤效应可以在强关联且有能隙的电子系统中实现，通过自旋1/2杂质与n个自旋单重态超导通道耦合，发现了四种不同的边界相。


<details>
  <summary>Details</summary>
Motivation: 传统多通道近藤效应要求电子通道无能隙且通道数大于杂质自旋大小。本文旨在探索当电子自由度强关联且有能隙时，是否仍能实现多通道近藤效应。

Method: 研究系统由一个自旋1/2杂质与n个自旋单重态超导通道各向同性耦合组成，这些通道由具有准长程序超导序的一维导线实现。使用Bethe Ansatz技术分析近藤与超导涨落之间的竞争。

Result: 识别出四种不同的边界相：过屏蔽近藤相、零模相、Yu-Shiba-Rusinov（YSR）相和具有未屏蔽杂质的局域矩相。即使在体质量隙存在下，近藤相的边界临界行为仍由与无能隙理论相同的指数控制，杂质熵随温度单调递减。

Conclusion: 多通道近藤效应可以在强关联且有能隙的电子系统中实现，系统表现出丰富的相图，包括四种不同的边界相，每种相都有独特的实验特征和热力学行为。

Abstract: The traditional multichannel Kondo effect takes place when several gapless metallic electronic channels interact with a localized spin-$S$ impurity, with the number of channels $n$ exceeding the size of the impurity spin, $n>2S$, leading to the emergence of non-Fermi liquid impurity behavior at low temperatures. Here, we show that the effect can be realized even when the electronic degrees of freedom are strongly correlated and gapped. The system under consideration consists of a single spin-$\frac{1}{2}$ impurity coupled isotropically to $n$ spin singlet superconducting channels realized by one-dimensional leads with quasi-long-range superconducting order. The competition between the Kondo and superconducting fluctuations induces multiple distinct ground states and boundary phases depending on the relative strengths of the bulk and boundary interactions. Using the Bethe Ansatz technique, we identify four regimes: an overscreened Kondo phase, a zero-mode phase, a Yu-Shiba-Rusinov (YSR) phase, and a local-moment phase with an unscreened impurity, each with its own experimental characteristic. We describe the renormalization-group flow, the excitation spectrum, and the full impurity thermodynamics in each phase. Remarkably, even in the presence of a bulk mass gap, the boundary critical behavior in the Kondo phase is governed by the same exponents as in the gapless theory with the low-energy impurity sector flowing to the $SU(2)_n$ Wess-Zumino-Witten (WZW) fixed point, and the impurity entropy monotonically decreasing as a function of temperature. In both the overscreened Kondo and zero-mode phases, the residual impurity entropy is $S_{\mathrm{imp}}(T \to 0) = \ln[2\cos(π/(n+2))]$. In the YSR and unscreened phases on the other hand the impurity entropy exhibits non-monotonic temperature dependence and is effectively free at low temperatures with $S_{\mathrm{imp}}(T \to 0) = \ln 2$.

</details>


### [3] [Fermi Liquid Fixed Point Deformations due to Codimension Two Defects](https://arxiv.org/abs/2512.12037)
*Jin-Yun Lin,Ira Z. Rothstein*

Main category: cond-mat.str-el

TL;DR: 费米液体中余维数为二的缺陷通过边际相关耦合改变重整化群流，其机制与孔多问题（余维数为三缺陷）不同，对数跑动源于缺陷产生的空间各向异性而非粒子-空穴不对称性。


<details>
  <summary>Details</summary>
Motivation: 研究费米液体中余维数为二缺陷对重整化群流的影响机制，揭示与孔多问题（余维数为三缺陷）的本质区别，理解缺陷几何结构如何产生对数跑动。

Method: 通过分析缺陷产生的空间各向异性，研究粒子-空穴不对称性的几何起源，证明缺陷方向动量导致空穴涨落被抑制，并分析缺陷长度与重整化群流时间的关系。

Result: 发现对数跑动源于缺陷几何结构而非传统粒子-空穴不对称性，重整化群流时间与缺陷长度成比例，缺陷局域的金斯顿模（dislon）在高于其德拜频率时以非导数方式与体费米子耦合并变得相关。

Conclusion: 余维数为二缺陷通过几何机制产生对数重整化群流，与孔多问题机制不同，缺陷长度尺度重整化群流时间，缺陷局域模在特定频率下变得相关。

Abstract: We show that codimension-two defects in Fermi liquids deform the renormalization group flow via a marginally relevant coupling. The mechanism for generating the flow is distinct from the case of the Kondo problem (codimension-three defects) in that the effective particle-hole asymmetry that leads to the log running is due to the spatial anisotropy generated by the defect. The mechanism for the log generation has a simple geometric explanation which shows that hole fluctuations are suppressed as the incoming momentum is taken to be along the direction of the defect. The RG flow time is shown to scale with the length of the defect. We also show that the dislon, the Goldstone mode localized to the defect, couples in a non-derivative fashion to the bulk fermions and becomes relevant above the dislons' Debye frequency which depends upon the defect tension.

</details>


### [4] [Self-Consistent Renormalized Spin-Wave Theory of Magnetic and Topological Transitions in Two-Dimensional Honeycomb Ferromagnets](https://arxiv.org/abs/2512.12104)
*Jian-Lin Li,Chien-Te Wu*

Main category: cond-mat.str-el

TL;DR: 该研究使用扩展的自洽重整化自旋波理论分析二维蜂窝状铁磁体的有限温度磁性和拓扑相变，揭示了该理论在无外部调谐时高估磁振子自能修正的局限性，并提出了在热力学稳定区域内实现拓扑相变的实验调谐策略。


<details>
  <summary>Details</summary>
Motivation: 研究二维蜂窝状铁磁体中有限温度下的磁性和拓扑相变，旨在批判性地检验自洽重整化自旋波理论的局限性，并探索在热力学稳定区域内实现拓扑相变的实验可行方法。

Method: 采用扩展的自洽重整化自旋波理论，结合Holstein-Primakoff展开的高阶修正，分析单离子各向异性、塞曼场、次近邻交换作用和Dzyaloshinskii-Moriya相互作用对磁化曲线和磁振子谱的影响。

Result: 发现SRSWT在无外部调谐时倾向于高估磁振子自能修正，常预测具有多值磁化和亚稳态分支的一级磁相变；同时识别出两种实用调谐策略：根据各向异性强度施加适当符号的外部塞曼场，以及引入小的反铁磁次近邻交换耦合，可在磁相变温度以下的热力学稳定区域内实现拓扑相变。

Conclusion: 该研究不仅阐明了SRSWT的预测范围和局限性，还为在二维蜂窝状磁性绝缘体中实现热驱动的拓扑相变提供了实验相关的指导，特别是通过外部场和交换相互作用的调谐策略。

Abstract: We investigate finite-temperature magnetic and topological phase transitions in two-dimensional honeycomb ferromagnets using an extended self-consistent renormalized spin-wave theory (SRSWT) that incorporates higher-order corrections from the Holstein--Primakoff expansion. Focusing on the combined effects of single-ion anisotropy, Zeeman field, next-nearest-neighbor (NNN) exchange, and Dzyaloshinskii--Moriya interaction, we analyze how these parameters influence the magnetization curves and magnon spectra. This work serves two main goals. First, we critically examine the limitations of SRSWT, showing that in the absence of external or interaction tuning, the theory tends to overestimate magnon self-energy corrections, often predicting first-order magnetic transitions with multivalued magnetization and metastable solution branches (i.e., self-consistent but thermodynamically unstable states). Second, we demonstrate that topological transitions -- signaled by magnon gap closings at the Dirac points -- can be tuned to occur below the magnetic transition temperature and within the thermodynamically stable regime. In particular, we identify two practical tuning strategies: applying an external Zeeman field of appropriate sign depending on the anisotropy strength, and introducing a small antiferromagnetic NNN exchange coupling. These findings not only clarify the predictive scope and limitations of SRSWT but also provide experimentally relevant guidance for realizing thermally driven topological transitions in two-dimensional honeycomb magnetic insulators.

</details>


### [5] [Experimental benchmark of the quantum-classical crossover in a spin ladder](https://arxiv.org/abs/2512.12234)
*Hironori Yamaguchi,Itsuki Shimamura,Akira Matsuo,Koichi Kindo,Koji Araki,Yoshiki Iwasaki,Masayuki Hagiwara*

Main category: cond-mat.str-el

TL;DR: 该论文报道了一种自旋(1/2, 5/2)三腿梯子结构，在自由基-Mn聚合物中实现，表现出反铁磁相变和磁化曲线，这些现象可通过经典平均场理论精确描述。


<details>
  <summary>Details</summary>
Motivation: 研究量子-经典交叉现象，探索在强关联系统中晶格拓扑结构如何调节量子与经典物理之间的平衡。

Method: 在自由基-Mn聚合物中实现自旋(1/2, 5/2)三腿梯子结构，通过实验测量反铁磁相变和磁化曲线，同时使用量子蒙特卡罗模拟验证基础自旋模型的内在量子涨落特性。

Result: 尽管基础自旋模型本质上支持强量子涨落（量子蒙特卡罗模拟证实），但实际系统显示出异常完全的量子行为抑制，磁化曲线可通过经典平均场理论精确描述。

Conclusion: 这些发现为量子-经典交叉提供了关键的实验基准，表明晶格拓扑结构在调节强关联系统中量子与经典物理平衡方面起着至关重要的作用。

Abstract: We report a spin-(1/2, 5/2) three-leg ladder realized in a radical-Mn polymer, exhibiting an antiferromagnetic transition and magnetization curves accurately described by classical mean-field theory. Although the underlying spin model intrinsically supports strong quantum fluctuations, as confirmed by quantum Monte Carlo simulations, the real system shows an anomalously complete suppression of quantum behavior. These findings provide a key experimental benchmark for the quantum-classical crossover and suggest that lattice topology can play a crucial role in tuning the balance between quantum and classical physics in strongly correlated systems.

</details>


### [6] [Diagrammatics in the Dual Space, or There and Back Again](https://arxiv.org/abs/2512.12389)
*Evgeny A. Stepanov*

Main category: cond-mat.str-el

TL;DR: 该论文提出了一种名为"对偶理论"的系统框架，用于桥接第一性原理计算和强关联电子系统研究，通过将对偶费米子、对偶玻色子等方法整合，在弱耦合和强耦合极限下都能精确描述多轨道系统中的多体效应。


<details>
  <summary>Details</summary>
Motivation: 目前计算材料科学和强关联电子理论研究之间存在显著的方法论鸿沟。第一性原理方法能处理大规模实际系统但忽略多体效应，而强关联方法只能处理简化模型。需要建立系统理论框架来桥接这两种方法，研究实际关联电子材料。

Method: 提出了"对偶理论"框架，包括对偶费米子、对偶玻色子和三重不可约局域展开方法。核心思想是将传统的费曼图展开的参考点从非相互作用电子系统转移到相互作用但可精确求解的系统。通过对参考系统积分，在有效的对偶空间中重新表述展开，其中所有图块都由相应的杂质量重整化。

Result: 建立了一个完整的循环方法学：从密度泛函理论构建第一性原理相互作用模型，使用动力学平均场理论求解以捕获局域关联，然后扩展到包含非局域集体电子涨落。对偶理论将原始变量的非微扰展开转化为对偶费米子和玻色子场的微扰展开，在弱耦合和强耦合极限下都精确。

Conclusion: 该论文系统化了对偶技术的主要发展，为研究实际关联电子材料提供了系统的理论框架，成功桥接了第一性原理计算和强关联电子理论研究之间的方法论鸿沟。

Abstract: Accurately describing many-body effects in multi-orbital systems remains a major challenge in theoretical condensed matter physics. At present, there is a significant methodological gap between the numerical tools used in ab initio computational materials science and those developed to study strong electronic correlations. The former can treat realistic, large-scale systems but typically neglect many-body effects, while the latter focus on simplified models with only a few degrees of freedom, as only such models can be solved accurately in the presence of strong interactions. The purpose of this thesis is to bridge these two approaches and establish a systematic theoretical framework for realistic correlated electronic materials. This involves a full-cycle methodology that begins with constructing ab initio interacting models from density-functional theory, solving them using dynamical mean-field theory to capture local correlations, and extending beyond to incorporate non-local collective electronic fluctuations. To this end, we introduce the "dual" approach to strong correlations, which includes the dual fermion, dual boson, and dual triply irreducible local expansion methods. The central idea of the dual theories is to shift the reference point of the conventional Feynman diagrammatic expansion from a non-interacting electronic system to an interacting but exactly solvable one. Integrating out this reference system recasts the expansion in an effective dual space, where all diagrammatic building blocks are renormalized by the corresponding impurity quantities. This procedure transforms a non-perturbative expansion in the original variables into a perturbative expansion in terms of dual fermionic and bosonic fields, exact in both the weak- and strong-coupling limits. In this thesis, we collect and systematize the major developments of dual techniques achieved to date.

</details>


### [7] [Substrate tuning of the structural and electronic transition in thin flakes of the excitonic insulator candidate Ta$_2$NiSe$_5$](https://arxiv.org/abs/2512.12439)
*Yuan-Shan Zhang,Zichen Yang,Chuanlian Xiao,Masahiko Isobe,Matteo Minola,Hidenori Takagi,Dennis Huang*

Main category: cond-mat.str-el

TL;DR: 通过薄片方法研究Ta2NiSe5的相变机制，发现导电Au衬底能显著降低相变温度并展宽相变，揭示了激子相互作用的重要性


<details>
  <summary>Details</summary>
Motivation: Ta2NiSe5的326K相变具有电子和结构双重特性，反映了激子和电子-晶格相互作用的复杂关系。以往研究大多局限于体材料，需要新方法来解耦这些相互作用

Method: 采用薄片方法，通过将Ta2NiSe5放置在导电Au衬底和绝缘Al2O3衬底上，利用偏振拉曼光谱研究相变行为。开发了全干式剥离和转移协议

Result: 四层Ta2NiSe5在Au衬底上相变温度降低超过100K且相变展宽，表现出界面电荷梯度效应；而在Al2O3衬底上则表现出接近体材料的性质

Conclusion: 导电衬底通过界面电荷梯度效应调控激子相互作用，证实了激子相互作用在Ta2NiSe5相变中的重要性。开发的方法可推广到其他强关联范德华材料

Abstract: Ta$_2$NiSe$_5$ continues to draw interest for its 326 K phase transition, whose dual electronic and structural nature reflects a complex interplay of electron-hole (excitonic) and electron-lattice interactions. Most studies that have attempted to decipher the relative importance of these interactions, particularly through charge transfer, have been limited to bulk samples. We utilized a thin-flake approach to modify the excitonic interactions in Ta$_2$NiSe$_5$ via an underlying film of Au. Using polarized Raman spectroscopy, we found that four layers of Ta$_2$NiSe$_5$ supported on conducting Au show a transition temperature that is both reduced by over 100 K and broadened due to an interfacial charge gradient effect, manifesting the presence of excitonic interactions. In contrast, four layers of Ta$_2$NiSe$_5$ supported on insulating Al$_2$O$_3$ show nearly bulk-like properties. We also report the development of an all-dry exfoliation and transfer protocol that generalizes substrate engineering for strongly correlated van der Waals materials.

</details>


### [8] [Strain-induced quantum oscillation in Kitaev spin liquid with Majorana-Fermi surface](https://arxiv.org/abs/2512.12570)
*Takayuki Yokoyama,Yasuhiro Tada*

Main category: cond-mat.str-el

TL;DR: 该理论研究应变诱导的Kitaev自旋液体中Majorana准粒子的朗道量子化，展示了Majorana费米面在应变下形成伪朗道能级并产生量子振荡效应。


<details>
  <summary>Details</summary>
Motivation: 探索Kitaev自旋液体中Majorana费米面在应变下的量子行为，寻找探测中性Majorana费米面的实验方法。

Method: 采用各向同性自旋-1/2 Kitaev模型，引入交错塞曼场和电磁场扰动产生Majorana费米面，通过三轴应变产生有效矢量势实现朗道量子化。

Result: 低能谱形成离散的Majorana费米面伪朗道能级，应变诱导的有效矢量势导致态密度和比热在极低温下出现显著的量子振荡，类似于金属中带电电子的de Haas-van Alphen效应。

Conclusion: 朗道量子化驱动的"Majorana量子振荡"可作为探测Kitaev材料中电荷中性Majorana费米面的有效探针。

Abstract: We theoretically study Landau quantization of itinerant Majorana quasiparticles induced by lattice strain in a Kitaev spin liquid with Majorana Fermi surfaces. We consider the isotropic spin-1/2 Kitaev model on the honeycomb lattice with a perturbation such as a staggered Zeeman field and an electromagnetic field, which generates small Majorana Fermi surfaces near the Dirac points. By introducing triaxial strain, we create an effective vector potential that couples to Majorana fermions and leads to Landau quantization. Our calculations show that the low-energy spectrum forms discrete pseudo-Landau levels of the Majorana Fermi surface. We further demonstrate that the strain-induced effective vector potential gives rise to pronounced quantum oscillations of the density of states and the specific heat at very low temperatures, in close analogy to the de Haas-van Alphen effect for charged electrons in metals. These results indicate that Landau-quantization-driven "Majorana quantum oscillations" can serve as a probe of the charge-neutral Majorana Fermi surface in Kitaev materials.

</details>


### [9] [Basis Adaptive Algorithm for Quantum Many-Body Systems on Quantum Computers](https://arxiv.org/abs/2512.12753)
*Anutosh Biswas,Sayan Ghosh,Ritajit Majumdar,Mostafizur Rahaman,Manoranjan Kumar*

Main category: cond-mat.str-el

TL;DR: 提出了一种新的基自适应算法，用于在混合量子-经典平台上高效寻找量子多体系统的基态性质。该方法通过使用浅层Trotter化电路进行短时实时间演化，并利用哈密顿量的对称性进行对称性过滤，然后在约化的希尔伯特空间中经典对角化。


<details>
  <summary>Details</summary>
Motivation: 现有算法如变分量子本征求解器（VQE）和量子相位估计（QPE）等存在局限性，需要开发更高效的方法来在近期的量子硬件上研究相关量子系统。

Method: 使用浅层Trotter化电路在量子处理器上进行短时实时间演化采样基，然后利用哈密顿量的各种对称性进行对称性过滤，最后在约化的希尔伯特空间中经典对角化。

Result: 在IBM Heron处理器上对最多24个量子比特的自旋-1/2 XXZ链进行基准测试，算法在不同各向异性区域实现了亚百分之一的基态能量精度，并且在可比约化空间维度下显著优于采样Krylov量子对角化（SKQD）方法。

Conclusion: 这项工作验证了对称性过滤的实时间采样作为在当前近期量子硬件上研究相关量子系统的稳健且高效的途径。

Abstract: A new basis adaptive algorithm for hybrid quantum-classical platforms is introduced to efficiently find the ground-state (gs) properties of quantum many-body systems. The method addresses limitations of many algorithms, such as Variational Quantum Eigensolver (VQE) and Quantum Phase Estimation (QPE) etc by using shallow Trotterized circuits for short real-time evolution on a quantum processor. The sampled basis is then symmetry-filtered by using various symmetries of the Hamiltonian which is then classically diagonalized in the reduced Hilbert space. We benchmark this approach on the spin-1/2 XXZ chain up to 24 qubits using the IBM Heron processor. The algorithm achieves sub-percent accuracy in ground-state energies across various anisotropy regimes. Crucially, it outperforms the Sampling Krylov Quantum Diagonalization (SKQD) method, demonstrating a substantially lower energy error for comparable reduced-space dimensions. This work validates symmetry-filtered, real-time sampling as a robust and efficient path for studying correlated quantum systems on current near-term hardware.

</details>


### [10] [Hund's coupling driven nature of magnetism in negative charge transfer material, $\mathrm{SrCoO_3}$](https://arxiv.org/abs/2512.12808)
*Jyotsana Sharma,Shivani Bhardwaj,Sudhir K Pandey*

Main category: cond-mat.str-el

TL;DR: 本文通过DMFT方法研究SrCoO₃的磁性微观起源，发现与实验高度一致，揭示了Hund耦合导致强准粒子质量增强和轨道选择性，以及Stoner-like交换劈裂崩塌驱动铁磁序消失的机制。


<details>
  <summary>Details</summary>
Motivation: 研究SrCoO₃中磁性的微观起源，理解电子关联效应在该材料磁性行为中的作用，特别是Hund耦合对系统性质的影响。

Method: 采用动力学平均场理论（DMFT）框架，结合电子关联效应，计算磁性可观测量，分析准粒子质量增强、轨道选择性和交换劈裂变化。

Result: 计算得到的饱和磁化强度（~2.4 μB）和磁转变温度（~350 K）与实验结果高度一致；发现Co 3d态准粒子质量增强达7倍，最大重整化出现在多数自旋t2g轨道；揭示了Stoner-like交换劈裂崩塌驱动长程铁磁序消失；局域磁矩源于中间自旋和高自旋态之间的动态涨落形成的混合自旋构型。

Conclusion: SrCoO₃中的磁性主要由Hund物理主导，表现为强准粒子质量增强、轨道选择性和大电荷涨落，这些关联效应共同决定了系统的磁性行为和相变特性。

Abstract: In this work, we investigate the microscopic origin of magnetism in $\mathrm{SrCoO_3}$ by incorporating electronic correlations within the dynamical mean-field theory (DMFT) framework. We note a remarkable agreement of the calculated magnetic observables ( saturation magnetization $\sim$2.4 $μ_B$; magnetic transition temperature, $T_c$$\sim$350 K) with the experimental results. The system exhibits Hund's coupling-induced strong quasiparticle mass enhancements of upto $m^*/m$ $\sim$7 for Co 3$d$ states, with the largest renormalization occurring in the majority spin $t_{2g}$ orbitals, marking the onset of orbital-selectivity. Our results reveal a Stoner-$like$ collapse of exchange splitting that drives the loss of long-range ferromagnetic order at $T_c$. The breakdown of Fermi-liquid behavior down to $T$$\sim$100 K suggests a suppressed coherence scale. Local magnetic moment originates from a mixed-spin configuration formed through dynamical fluctuation between intermediate-spin and high-spin states. Large charge fluctuations ($\langle$$Δ$N$^2$$\rangle$$\sim$0.6) together with heavy quasiparticles establish the correlation effects regime, governed predominantly by Hund's physics in $\mathrm{SrCoO_3}$.

</details>


### [11] [Non-linear transport in field-induced insulating states of graphite](https://arxiv.org/abs/2512.12956)
*Masashi Tokunaga,Kazuto Akiba,Hiroshi Yaguchi,Akira Matsuo,Koichi Kindo*

Main category: cond-mat.str-el

TL;DR: 石墨在c轴磁场下的量子极限态表现出多阶段相变，但起源仍有争议。研究通过高达75T的脉冲磁场测量石墨单晶的非线性电导率，发现纵向磁阻在第一和第二磁场诱导相中均表现出明显的非线性特征。


<details>
  <summary>Details</summary>
Motivation: 石墨在c轴磁场下表现出多阶段相变现象，虽然已有广泛研究，但其物理起源至今仍存在争议。研究者希望通过高磁场下的输运测量来深入理解这些相变的本质。

Method: 使用高达75T的脉冲磁场对石墨单晶进行高磁场输运测量，特别关注非线性电导率行为。通过测量纵向磁阻来研究磁场诱导相的特征。

Result: 纵向磁阻测量显示，不仅在第一个磁场诱导相中，在第二个磁场诱导相中也观察到了明显的非线性特征。这表明两个相都具有非线性的电输运行为。

Conclusion: 石墨在量子极限态下的多阶段相变表现出复杂的非线性电导行为，这些发现为理解石墨中磁场诱导相变的物理机制提供了新的实验证据。

Abstract: Graphite exhibits multi-stage phase transitions in the quantum-limit states realized by magnetic fields applied along the c-axis. Despite extensive studies on this phenomenon, the origin remains a matter of debate to this day. We performed high-field magnetotransport measurements on single crystals of graphite, focusing on the non-linear conductivity in pulsed-magnetic fields of up to 75 T. The longitudinal magnetoresistance exhibits distinct non-linearity not only in the first but also in the second field-induced phases.

</details>


### [12] [The interplay of magnetic order with the electronic scattering and crystal-field effects in a metallic ferromagnet](https://arxiv.org/abs/2512.12996)
*Payel Shee,Tanaya Halder,Chia-Jung Yang,Nainish Tickoo,Ratiranjan Samal,Ruta Kulkarni,Shishir K. Pandey,Vikas Kashid,Ashis K. Nandy,Arumugam Thamizhavel,Anamitra Mukherjee,Shovon Pal*

Main category: cond-mat.str-el

TL;DR: 通过太赫兹时域光谱研究PrSi金属铁磁体，发现其光学响应在宽温区呈现Drude-Smith行为，表明持续载流子散射；低温下晶体场激发主导响应，其中1.54THz模式与铁磁序起始动态相关。


<details>
  <summary>Details</summary>
Motivation: 研究稀土金属间化合物中磁序、电荷动力学和晶体场激发之间的相互作用，探索这些耦合如何影响材料的基本态。

Method: 使用太赫兹时域光谱技术研究PrSi金属铁磁体，结合经典Kondo晶格模型分析光学响应。

Result: 在宽温度范围内观察到明显的Drude-Smith行为，表明持续载流子散射；低温下晶体场激发主导响应，发现0.6THz和1.54THz的尖锐跃迁，其中1.54THz模式与铁磁序起始动态相关。

Conclusion: PrSi中载流子散射由局域磁矩驱动，低温下进入晶体场主导的机制，晶体场激发与磁序之间存在动态关联，揭示了稀土金属间化合物中多尺度相互作用的复杂性。

Abstract: The interplay between magnetic order, charge dynamics, and crystal field excitations underpins the emergent ground states of rare-earth intermetallics. Using time-domain terahertz spectroscopy, we probe this coupling in PrSi, a metallic ferromagnet. The optical response exhibits pronounced Drude-Smith behavior over a broad temperature range, indicating persistent carrier scattering. A classical Kondo-lattice model (CKLM) attributes this non-Drude conductivity to scattering of itinerant electrons by localized magnetic moments, persisting down to temperatures well below the magnetic ordering scale. At lower temperatures, beyond the scope of CKLM, our experiment reveals that the response is dominated by crystal-field excitations, with sharp transitions at 0.6 THz and 1.54 THz. The mode at 1.54 THz shows a dynamic correlation with the onset of ferromagnetic order, marking the onset of a crystal-field-governed low temperature regime.

</details>


### [13] [Generation of chirality and orbital magnetization by Stone-Wales-type lattice defects in the Kitaev spin liquid](https://arxiv.org/abs/2512.13490)
*Arnab Seth,Fay Borhani,Itamar Kimchi*

Main category: cond-mat.str-el

TL;DR: 研究晶体缺陷对Kitaev蜂窝自旋液体（特别是无能隙相）的影响，发现Stone-Wales型缺陷产生手性，导致拓扑能隙和手性自旋液体相变。


<details>
  <summary>Details</summary>
Motivation: 扩展研究晶体缺陷对自旋-1/2 Kitaev蜂窝自旋液体的影响，特别关注无能隙相并与有能隙相对比，探索缺陷如何影响系统的拓扑性质和相变行为。

Method: 识别Stone-Wales局部缺陷（90°键旋转），保持Kitaev键标签实现精确可解性；分析缺陷的奇边plaquette和±π/2通量；使用T矩阵分析和有限缺陷密度数值计算；研究缺陷手性之间的长程伊辛相互作用。

Result: 孤立缺陷具有时间反演对的地通量构型和大净手性；缺陷手性产生11nd的拓扑能隙，保护C=±1的陈数；缺陷手性间的长程伊辛相互作用导致有限温度Tc相变到手性自旋液体；Tc与nd成正比，当γ→2时发散；杂质势可降低γ至2.3以下，增强Tc。

Conclusion: 晶体缺陷在Kitaev自旋液体中诱导手性和拓扑性质，导致手性自旋液体相变，为二维狄拉克锥系统与有限密度涨落伊辛磁杂质的研究提供应用，并为通过晶格缺陷识别自旋液体提供途径。

Abstract: In this work we extend our study of the effect of certain crystallographic defects on the spin-1/2 Kitaev honeycomb spin liquid (arXiv:2511.19409), focusing on its gapless phase and contrasting with the gapped phase. We identify a Stone-Wales (SW) local defect consisting of a 90$^\circ$ bond rotation that preserves Kitaev bond labels for edge-sharing octahedra and thereby enables exact solvability. These SW-type defects involve odd-sided plaquettes with $\pm π/2$ fluxes, but can be created locally. An isolated defect hosts a time-reversal pair of ground-state flux configurations with large net chirality. Certain excitations are also chiral. The chirality manifests in Majorana local Chern marker and in scalar spin chirality, producing electronic orbital magnetization. T-matrix analysis and numerics at finite defect density $n_d$ show that defect chiralities generate a topological gap of $11 n_d$ protecting a Chern number $C=\pm 1$. Emergent ferromagnetic long range Ising interactions $r^{-γ}$ with $2<γ< 3$ between defect chiralities lead to a finite temperature $T_c$ phase transition into the chiral spin liquid. The $T_c$ is proportional to $n_d$ and diverges when $γ\rightarrow 2$. We also consider additional solvable impurity potentials and find that $γ$ can be reduced to below $2.3$ and correspondingly enhance $T_c$. Our results offer applications to 2D Dirac cone systems with a finite density of fluctuating Ising magnetic impurities and to identifying spin liquids with lattice defects.

</details>


### [14] [Giant hysteretic magnetoresistance accompanying the Mott transition and spin-glass state in organic metal](https://arxiv.org/abs/2512.13245)
*P. D. Grigoriev,S. I. Pesotskii,R. B. Lyubovskii,S. A. Torunova,D. S. Lyubshin,V. N. Zverev*

Main category: cond-mat.str-el

TL;DR: 在有机金属k-(BEDTTTF)2Hg(SCN)2Br中，约3 kbar压力区间内观察到具有巨大磁滞回线的巨磁阻效应，该效应与磁场方向无关，表明其源于强电子关联和自旋玻璃态的影响。


<details>
  <summary>Details</summary>
Motivation: 研究有机金属材料在低温高压下的磁阻行为，探索由强电子关联和自旋玻璃态引起的新型磁阻机制。

Method: 在约3 kbar压力区间（宽度约1 kbar）对k-(BEDTTTF)2Hg(SCN)2Br有机金属进行低温磁阻测量，分析磁滞回线的各向同性、温度和磁场依赖性，并建立简单理论模型解释观察到的现象。

Result: 观察到具有巨大磁滞回线的巨磁阻效应，该效应与磁场方向无关（排除轨道效应），温度和磁场依赖性表明自旋玻璃态对磁阻有强烈影响，提出了基于强电荷-自旋纠缠的简单模型。

Conclusion: 该效应源于强电子关联和自旋玻璃态，提出了一种新的极端磁阻机制类别，为理解强关联系统中的电荷-自旋纠缠提供了新视角。

Abstract: The giant magnetoresistance with a huge hysteresis is observed in the organic metal k-(BEDTTTF)2Hg(SCN)2Br at low temperature in a pressure interval around 3 kbar of a width ~1 kbar. The hysteretic magnetoresistance is isotropic with respect to the direction of magnetic field, which excludes the orbital effect of magnetic field as its origin. The observed temperature and magnetic-field dependence of this hysteresis and of its relaxation time indicates the strong influence of spin-glass state on magnetoresistance. Although a quantitative theory of this effect, originating from strong electronic correlations, requires complex numerical calculations, we suggest its explanation and a simple model which qualitatively describes the observed magnetoresistance behavior and shows a strong charge-spin entanglement. The proposed effect suggests a new class of extreme magnetoresistance mechanisms.

</details>


### [15] [Successive magnetic transitions and multiferroicity in layered honeycomb BiCrTeO$_{6}$](https://arxiv.org/abs/2512.13387)
*Arkadeb Pal,P. H. Lee,J. Khatua,C. W. Wang,J. Gainza,A. Fitch,Thomas J. Hicken,H. Luetkens,Y. J. Hu,Ajay Tiwari,D. Chandrasekhar Kakarla,J. Y. Lin,K. Y. Choi,G. R. Blake,H. D. Yang*

Main category: cond-mat.str-el

TL;DR: BiCrTeO6是一种新型自旋驱动多铁性材料，在低温下表现出两个连续的反铁磁转变、磁介电耦合效应和铁电有序，同时在顺磁区具有介电弛豫特性。


<details>
  <summary>Details</summary>
Motivation: 研究基于蜂窝晶格的低维磁性系统，探索自旋、轨道、晶格和偶极子自由度之间复杂相互作用产生的奇异量子现象。

Method: 采用磁化强度、比热、μ子自旋弛豫光谱、介电、热电流和高分辨率同步辐射X射线衍射等多种测量技术对BiCrTeO6进行全面研究。

Result: 发现两个连续的反铁磁转变（TN1≈16K和TN2≈11K）、显著的磁介电耦合效应、TN2处的铁电有序，以及磁弹性耦合诱导的结构相变（对称性从P31c降低到P31c）。在顺磁区（T<50K）还观察到介电弛豫特性。

Conclusion: BiCrTeO6是一种新型自旋驱动多铁性系统，其铁电性由磁弹性耦合诱导的结构相变触发，同时系统的介电弛豫特性与Cr和Te原子的反位无序密切相关。

Abstract: Low-dimensional magnetic systems based on honeycomb lattices provide a promising platform for exploring exotic quantum phenomena that emerge from the intricate interplay of competing spin, orbital, lattice, and dipolar degrees of freedom. Here, we present a comprehensive study of the layered honeycomb lattice antiferromagnet BiCrTeO$_6$ using magnetization, specific heat, muon spin--relaxation ($μ$SR) spectroscopy, dielectric, pyrocurrent, and high-resolution synchrotron X-ray diffraction (SXRD) measurements. Our results reveal an array of intriguing and strongly correlated phenomena, including two successive antiferromagnetic transitions at $T_{\rm N1}\approx16$ K and $T_{\rm N2}\approx11$ K, a pronounced magnetodielectric coupling effect, and ferroelectric order at $T_{\rm N2}$. Consequently, this compound emerges as a new spin-driven multiferroic system. The SXRD analysis reveals a magnetoelastic-coupling-induced structural phase transition at $T_{\rm N2}$, characterized by a symmetry lowering from P$\bar{3}$1c (163) to P31c (159), which likely triggers the onset of ferroelectricity. In addition to its low-temperature multiferroic behavior, the system exhibits dielectric relaxor characteristics at higher temperatures within the paramagnetic region ($T<50$ K), which is intrinsically linked to the antisite disorder of Cr and Te atoms.

</details>


### [16] [The classical-quantum disproportionation transition and magnetic ordering in RNiO$_3$ nickelates](https://arxiv.org/abs/2512.13540)
*A. S. Moskvin,Yu. D. Panov*

Main category: cond-mat.str-el

TL;DR: 该研究重新审视了镍酸盐RNiO₃中的绝缘体-准金属转变，提出电荷歧化而非传统Mott转变是绝缘相的主要机制，并建立了包含局域和非局域关联的复合玻色子模型。


<details>
  <summary>Details</summary>
Motivation: 传统上镍酸盐中的绝缘体-准金属转变被解释为Mott转变，但实际绝缘相源于电荷歧化形成自旋三重态电子中心和自旋空穴中心。需要建立更准确的物理模型来描述这一复杂相变。

Method: 提出有效电荷歧化相哈密顿量，考虑局域(U)和非局域(V)关联以及复合玻色子转移(t_b)。在有效场近似框架下分析两种电荷歧化相：高温经典顺磁电荷有序相和低温磁量子CDq相。

Result: 发现存在两种电荷歧化相：高温经典CO相中自旋三重态电子中心被非磁性空穴中心包围，关闭了近邻强超交换作用；低温量子CDq相具有"不确定价态"[NiO₆]⁽⁹±δ⁾⁻和自旋密度(1±δ)/2，磁有序由传统超交换和异常玻色子双交换机制决定。

Conclusion: 镍酸盐中的绝缘体-准金属转变本质上是电荷歧化驱动的相变，而非传统Mott转变。量子CDq相表现出独特的电荷和自旋密度转移特性，为理解这类材料的复杂电子行为提供了新视角。

Abstract: The insulator-quasi-metal (bad metal) transition observed in Jahn-Teller (JT) magnets orthonickelates RNiO$_3$ (R = rare earth, or yttrium Y) is considered a canonical example of the Mott transition, traditionally described in the framework of Hubbard's $U-t$ model. However, in reality, the insulating phase of nickelates is the result of charge disproportionation (CD) with the formation of a system of spin-triplet ($S = 1$) electron [NiO$_6$]$^{10-}$ and spinless ($S = 0$) hole [NiO$_6$]$^{8-}$ centers, equivalent to a system of effective spin-triplet composite bosons moving in a nonmagnetic lattice. The effective CD-phase Hamiltonian takes into account local ($U$) and nonlocal ($V$) correlations, and the transfer of composite bosons ($t_b$). Within the framework of the effective field approximation, we have shown the existence of two types of CD phases: the high-temperature classical paramagnetic CO-phase of charge ordering of electron and hole centers, and the low-temperature magnetic quantum CDq phase with charge and spin density transfer between electron and hole centers, with ''uncertain valence'' [NiO$_{6}$]$^{(9\pmδ)-}$ ($0 \le δ\le 1$) and spin density $(1 \pm δ)/2$ NiO$_6$-centers. In the classical CO phase, spin-triplet electron centers are surrounded by the nearest nonmagnetic hole centers, which ''turns off'' the strong superexchange interaction of the nearest neighbors. The magnetic ordering in the quantum CDq phase is determined by a strong traditional superexchange and an unusual bosonic double exchange mechanism.

</details>


### [17] [Magnetic order and novel quantum criticality in the strongly interacting quasicrystals](https://arxiv.org/abs/2512.13546)
*Cong Zhang,Yin-Kai Yu,Shao-Hang Shi,Zi-Xiang Li*

Main category: cond-mat.str-el

TL;DR: 通过无符号问题的量子蒙特卡洛研究，发现二维准晶中的哈伯德模型磁相变性质由电子态密度决定：彭罗斯拼图因奇异态密度在无穷小相互作用下即产生磁序，而图厄-莫尔斯晶格需要有限临界相互作用。在图厄-莫尔斯准晶中识别出新的量子临界点，其临界指数显著偏离传统(2+1)D海森堡O(3)普适类。


<details>
  <summary>Details</summary>
Motivation: 研究准晶几何结构如何从根本上决定量子临界性，探索电子关联与无周期性几何结构相互作用如何产生新的普适类，挑战二维磁临界性的标准范式。

Method: 使用无符号问题的量子蒙特卡洛方法研究半填充哈伯德模型在二维准晶上的行为，比较彭罗斯拼图和图厄-莫尔斯准晶，采用新颖的边界构造策略和严格的有限尺寸标度分析。

Result: 发现磁相变性质由电子态密度控制：彭罗斯拼图的奇异态密度导致在无穷小相互作用下即产生磁序，而图厄-莫尔斯晶格需要有限临界相互作用。在图厄-莫尔斯准晶中识别出量子临界点，临界指数ν≈0.94、β≈0.72、z≈1.51，显著偏离传统(2+1)D海森堡O(3)普适类。

Conclusion: 电子关联与无周期性几何结构的相互作用驱动了新的普适类，挑战了二维磁临界性的标准范式，为准晶中的量子相变提供了新的理论框架。

Abstract: We present the sign-problem-free quantum Monte Carlo study of the half-filled Hubbard model on two-dimensional quasicrystals, revealing how specific aperiodic geometries fundamentally dictate quantum criticality. By comparing the Penrose and Thue-Morse quasicrystals, we demonstrate that the nature of the magnetic phase transition is controlled by the electronic density of states (DOS): while the singular DOS of the Penrose tiling induces magnetic order at infinitesimal interaction strengths, the Thue-Morse lattice requires a finite critical interaction to drive the transition. Crucially, through a novel boundary construction strategy and rigorous finite-size scaling, we identify a quantum critical point on the Thue-Morse quasicrystal with critical exponents ($ν\approx 0.94$, $β\approx 0.72$ and $z\approx 1.51$) that deviate significantly from the conventional $(2+1)$D Heisenberg $O(3)$ class. These findings establish the existence of a novel universality class driven by the interplay between electronic correlations and aperiodic geometry, challenging standard paradigms of magnetic criticality in two dimensions.

</details>


### [18] [CrFe2Ge2: Investigation of novel ferromagnetic material of Fe13Ge8-type crystal](https://arxiv.org/abs/2512.13611)
*P. L. S. Cambalame,B. J. C. Vieira,J. C. Waerenborgh,P. S. P. da Silva,J. A. Paixão*

Main category: cond-mat.str-el

TL;DR: 成功合成新型金属间化合物CrFe₂Ge₂，具有Fe₁₃Ge₈型晶体结构，表现出金属铁磁性，居里温度约200K，临界指数分析显示其行为类似于d=3, n=3铁磁体，具有长程磁耦合特征。


<details>
  <summary>Details</summary>
Motivation: 研究新型金属间化合物CrFe₂Ge₂的晶体结构、磁性和电子性质，探索其独特的临界行为和磁耦合机制。

Method: 采用单晶X射线衍射和穆斯堡尔谱分析晶体结构，通过修正的Arrott图分析临界指数，利用重正化群计算和Widom标度律验证临界行为，测量电阻率和比热研究电子-磁子散射。

Result: 成功合成CrFe₂Ge₂，确认存在两个不同的Fe亚晶格；居里温度约200K；临界指数β=0.392，γ=1.309，δ=4.26；磁交换距离衰减为J(r)≈r⁻⁴.⁸⁶，显示长程磁耦合；Rhodes-Wohlfarth比约3表明巡游铁磁基态；低温电阻率和比热测量显示显著的电子-磁子散射贡献。

Conclusion: CrFe₂Ge₂表现出独特的临界行为，类似于d=3, n=3铁磁体，具有长程磁耦合特征，为巡游铁磁体，其电子-磁子散射在低温输运性质中起重要作用。

Abstract: We successfully synthesized a novel intermetallic compound $\rm CrFe_2Ge_2$ with the $\rm Fe_{13}Ge_{8}$-type crystal structure. A structural study is presented combining single-crystal X-ray diffraction and Mössbauer spectroscopy analysis, confirming the presence of two distinct Fe sublattices. $\rm CrFe_2Ge_2$ exhibits a metallic ferromagnetic state with $T_C \approx \rm 200~K$. This material does not follow the usual $M^2 \propto H/M$ Arrott law, rather a modified Arrott law is obeyed in this material. The critical exponents determined from detailed analysis of modified Arrott plots were found to be $β= 0.392$, $γ= 1.309$ and $δ= 4.26$ obtained from the critical isotherm at $ T_{\rm C} =\rm 200~K$. Self-consistency and reliability of the critical exponent analysis were verified by the Widom scaling law and scaling equations. Using the results from renormalization group calculation, the critical behavior of $\rm CrFe_2Ge_2$ is akin to that of a $d=3, n=3$ ferromagnet in which the magnetic exhange distance is found to decay as $J(r) \approx r^{-4.86}$ with long-range magnetic coupling. The evaluated Rhodes-Wohlfarth ratio of $\sim 3$ points to an itinerant ferromagnetic ground state. Low-temperature measurements of resistivity, $p(T)$, and specific heat, $C_P(T)$, reveal a pronounced contribution from electron-magnon scattering.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [19] [A Monad-Based Clause Architecture for Artificial Age Score (AAS) in Large Language Models](https://arxiv.org/abs/2512.11835)
*Seyma Yaman Kayadibi*

Main category: cs.AI

TL;DR: 基于人工年龄评分(AAS)构建可执行的莱布尼茨单子条款框架，为LLM内存和控制提供透明、可审计的法律式约束


<details>
  <summary>Details</summary>
Motivation: LLM通常作为强大但不透明的系统部署，缺乏对其内部记忆和"自我式"行为的治理原则和审计方法，需要建立可验证的约束框架

Method: 将莱布尼茨《单子论》中的20个单子分组为6个束（本体论、动力学、表征与意识、和谐与理性、身体与组织、目的论），在AAS内核上实现为可执行规范，通过6个Python实现进行数值实验

Result: 条款系统展现出有界且可解释的行为：AAS轨迹保持连续且速率受限，矛盾和未经支持的声明触发明确惩罚，分层细化以受控方式揭示有机结构，和谐项对齐双重视图和目标-行动对，完美度窗口漂移区分持续改进与退化

Conclusion: 基于单子的条款框架以AAS为骨干，为约束和分析人工智能体内部动态提供了透明、代码级的蓝图，将哲学动机转化为可直接实现的工程架构

Abstract: Large language models (LLMs) are often deployed as powerful yet opaque systems, leaving open how their internal memory and "self-like" behavior should be governed in a principled and auditable way. The Artificial Age Score (AAS) was previously introduced and mathematically justified through three theorems that characterise it as a metric of artificial memory aging. Building on this foundation, the present work develops an engineering-oriented, clause-based architecture that imposes law-like constraints on LLM memory and control. Twenty selected monads from Leibniz's Monadology are grouped into six bundles: ontology, dynamics, representation and consciousness, harmony and reason, body and organisation, and teleology, and each bundle is realised as an executable specification on top of the AAS kernel. Across six minimal Python implementations, these clause families are instantiated in numerical experiments acting on channel-level quantities such as recall scores, redundancy, and weights. Each implementation follows a four-step pattern: inputs and setup, clause implementation, numerical results, and implications for LLM design, emphasising that the framework is not only philosophically motivated but also directly implementable. The experiments show that the clause system exhibits bounded and interpretable behavior: AAS trajectories remain continuous and rate-limited, contradictions and unsupported claims trigger explicit penalties, and hierarchical refinement reveals an organic structure in a controlled manner. Dual views and goal-action pairs are aligned by harmony terms, and windowed drift in perfection scores separates sustained improvement from sustained degradation. Overall, the monad-based clause framework uses AAS as a backbone and provides a transparent, code-level blueprint for constraining and analyzing internal dynamics in artificial agents.

</details>


### [20] [Solving Parallel Machine Scheduling With Precedences and Cumulative Resource Constraints With Calendars](https://arxiv.org/abs/2512.11864)
*Christoph Einspieler,Matthias Horn,Marie-Louise Lackner,Patrick Malik,Nysret Musliu,Felix Winter*

Main category: cs.AI

TL;DR: 提出一种解决具有作业优先级和基于日历的累积资源约束的真实工业并行机器调度问题的新方法，包括约束建模、构造启发式和元启发式算法


<details>
  <summary>Details</summary>
Motivation: 现代工业生产中存在复杂的并行机器调度问题，传统方法无法有效处理作业优先级约束和基于日历的累积资源限制，需要开发能解决真实工业场景的自动化方法

Method: 提出三种方法：1) 约束建模方法作为小规模场景的精确解法；2) 构造启发式算法；3) 基于局部搜索的定制元启发式算法处理大规模实例

Result: 元启发式方法已在工业环境中部署并正在使用，能够有效处理真实工业用例中的大规模并行机器调度问题

Conclusion: 提出的方法成功解决了具有复杂约束的真实工业并行机器调度问题，特别是元启发式算法在实际工业应用中表现出色

Abstract: The task of finding efficient production schedules for parallel machines is a challenge that arises in most industrial manufacturing domains. There is a large potential to minimize production costs through automated scheduling techniques, due to the large-scale requirements of modern factories. In the past, solution approaches have been studied for many machine scheduling variations, where even basic variants have been shown to be NP-hard. However, in today's real-life production environments, additional complex precedence constraints and resource restrictions with calendars arise that must be fulfilled. These additional constraints cannot be tackled efficiently by existing solution techniques. Thus, there is a strong need to develop and analyze automated methods that can solve such real-life parallel machine scheduling scenarios. In this work, we introduce a novel variant of parallel machine scheduling with job precedences and calendar-based cumulative resource constraints that arises in real-life industrial use cases. A constraint modeling approach is proposed as an exact solution method for small scheduling scenarios together with state-of-the-art constraint-solving technology. Further, we propose a construction heuristic as well as a tailored metaheuristic using local search to efficiently tackle large-scale problem instances. This metaheuristic approach has been deployed and is currently being used in an industrial setting.

</details>


### [21] [Mirror Mode in Fire Emblem: Beating Players at their own Game with Imitation and Reinforcement Learning](https://arxiv.org/abs/2512.11902)
*Yanna Elizabeth Smid,Peter van der Putten,Aske Plaat*

Main category: cs.AI

TL;DR: 该研究提出"镜像模式"游戏模式，让敌人AI模仿玩家的个人策略来挑战玩家不断改变玩法。在Unity中构建了简化版《火焰纹章英雄》游戏，包含标准模式和镜像模式，使用强化学习和模仿学习技术训练AI模仿玩家演示。


<details>
  <summary>Details</summary>
Motivation: 在回合制游戏中，敌人策略应该具有惊喜性和不可预测性。传统游戏AI往往采用固定模式，缺乏个性化和适应性。为了让玩家面对更具挑战性和个性化的对手，需要开发能够模仿玩家个人策略的AI系统。

Method: 1. 在Unity中构建简化版《火焰纹章英雄》游戏，包含标准模式和镜像模式；2. 使用生成对抗模仿学习、行为克隆和近端策略优化等强化学习和模仿学习技术组合训练模型；3. 通过两阶段实验：第一阶段寻找合适的模仿模型，第二阶段通过玩家测试评估模型效果；4. 收集参与者演示数据进行训练，并通过调查问卷评估玩家满意度。

Result: 1. 模型在防御行为上表现出良好的模仿效果，但在进攻策略上模仿效果不佳；2. 参与者能够识别出自己的撤退战术；3. 镜像模式总体上获得了更高的玩家满意度；4. 玩家在面对自己策略时体验更佳。

Conclusion: 镜像模式能够有效提升玩家满意度和游戏体验，特别是在玩家面对自己策略时。虽然当前模型在防御行为上模仿效果良好，但在进攻策略上仍需改进。进一步优化模型可以提高模仿质量，从而进一步提升玩家满意度。

Abstract: Enemy strategies in turn-based games should be surprising and unpredictable. This study introduces Mirror Mode, a new game mode where the enemy AI mimics the personal strategy of a player to challenge them to keep changing their gameplay. A simplified version of the Nintendo strategy video game Fire Emblem Heroes has been built in Unity, with a Standard Mode and a Mirror Mode. Our first set of experiments find a suitable model for the task to imitate player demonstrations, using Reinforcement Learning and Imitation Learning: combining Generative Adversarial Imitation Learning, Behavioral Cloning, and Proximal Policy Optimization. The second set of experiments evaluates the constructed model with player tests, where models are trained on demonstrations provided by participants. The gameplay of the participants indicates good imitation in defensive behavior, but not in offensive strategies. Participant's surveys indicated that they recognized their own retreating tactics, and resulted in an overall higher player-satisfaction for Mirror Mode. Refining the model further may improve imitation quality and increase player's satisfaction, especially when players face their own strategies. The full code and survey results are stored at: https://github.com/YannaSmid/MirrorMode

</details>


### [22] [Robustness of Probabilistic Models to Low-Quality Data: A Multi-Perspective Analysis](https://arxiv.org/abs/2512.11912)
*Liu Peng,Yaochu Jin*

Main category: cs.AI

TL;DR: 研究发现不同概率模型对低质量数据的鲁棒性差异显著：自回归语言模型（如GPT-2）表现出强鲁棒性，扩散模型在数据损坏时性能急剧下降，分类器影响适中且随数据规模减小。


<details>
  <summary>Details</summary>
Motivation: 系统比较现代概率模型对低质量数据的鲁棒性差异，理解不同模型架构在面对数据损坏时的表现差异及其根本原因。

Method: 通过系统性对比实验，在不同数据损坏程度下测试自回归语言模型、扩散模型和分类器的性能；采用多视角分析框架，结合信息论、PAC学习和梯度动力学理论解释结果差异。

Result: 自回归语言模型（GPT-2）在50%token损坏时测试NLL仅从2.87增加到3.59；扩散模型在相同损坏程度下图像-标签一致性相对基线下降56.81%；分类器影响适中且随数据集规模增大而减小。

Conclusion: 模型鲁棒性主要受两个关键原则影响：条件信息的丰富程度（约束学习问题）和训练数据的绝对信息量（使正确信息信号主导统计噪声）。

Abstract: A systematic, comparative investigation into the effects of low-quality data reveals a stark spectrum of robustness across modern probabilistic models. We find that autoregressive language models, from token prediction to sequence-to-sequence tasks, are remarkably resilient (for GPT-2, test NLL increases modestly from 2.87 to 3.59 despite 50% token corruption). By contrast, under the same levels of data corruption, class-conditional diffusion models degrade catastrophically (image-label consistency plummets by 56.81% relative to baseline), while classifiers show a moderate impact that diminishes with dataset scale. To explain these discrepancies, we analyze the results through a multi-perspective lens, integrating information theory, PAC learning, and gradient dynamics. These analyses suggest that robustness is heavily influenced by two key principles: the richness of conditioning information, which constrains the learning problem, and the absolute information content of the training data, which allows the signal from correct information to dominate statistical noise.

</details>


### [23] [CXL-SpecKV: A Disaggregated FPGA Speculative KV-Cache for Datacenter LLM Serving](https://arxiv.org/abs/2512.11920)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: CXL-SpecKV：基于CXL互连和FPGA加速器的解耦KV缓存架构，通过内存解耦、推测预取和压缩技术，解决LLM推理中的内存瓶颈问题，提升吞吐量3.2倍，降低内存成本2.8倍。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数据中心部署时面临KV缓存占用大量GPU内存的问题，限制了批处理大小和系统吞吐量，需要解决内存瓶颈挑战。

Method: 提出CXL-SpecKV架构：1）基于CXL的内存解耦框架，将KV缓存卸载到远程FPGA内存；2）推测性KV缓存预取机制，预测并预加载未来token的缓存条目；3）FPGA加速的KV缓存压缩解压引擎，减少内存带宽需求4倍。

Result: 在先进LLM模型上评估，CXL-SpecKV相比GPU-only基线实现3.2倍吞吐量提升，内存成本降低2.8倍，同时保持准确性。

Conclusion: 智能内存解耦与推测执行相结合能有效解决大规模LLM服务中的内存墙挑战，CXL-SpecKV为高效LLM部署提供了可行方案。

Abstract: Large Language Models (LLMs) have revolutionized natural language processing tasks, but their deployment in datacenter environments faces significant challenges due to the massive memory requirements of key-value (KV) caches. During the autoregressive decoding process, KV caches consume substantial GPU memory, limiting batch sizes and overall system throughput. To address these challenges, we propose \textbf{CXL-SpecKV}, a novel disaggregated KV-cache architecture that leverages Compute Express Link (CXL) interconnects and FPGA accelerators to enable efficient speculative execution and memory disaggregation. Our approach introduces three key innovations: (i) a CXL-based memory disaggregation framework that offloads KV-caches to remote FPGA memory with low latency, (ii) a speculative KV-cache prefetching mechanism that predicts and preloads future tokens' cache entries, and (iii) an FPGA-accelerated KV-cache compression and decompression engine that reduces memory bandwidth requirements by up to 4$\times$. When evaluated on state-of-the-art LLM models, CXL-SpecKV achieves up to 3.2$\times$ higher throughput compared to GPU-only baselines, while reducing memory costs by 2.8$\times$ and maintaining accuracy. Our system demonstrates that intelligent memory disaggregation combined with speculative execution can effectively address the memory wall challenge in large-scale LLM serving. Our code implementation has been open-sourced at https://github.com/FastLM/CXL-SpecKV.

</details>


### [24] [Hypergame Rationalisability: Solving Agent Misalignment In Strategic Play](https://arxiv.org/abs/2512.11942)
*Vince Trencsenyi*

Main category: cs.AI

TL;DR: 本文提出了一种基于逻辑的声明式领域特定语言，用于编码超博弈结构和解决方案概念，通过答案集编程实现自动化管道，为多智能体系统中的异构推理提供可验证的形式化基础。


<details>
  <summary>Details</summary>
Motivation: 由于感知差异、信息不对称和有限理性，博弈论参与者对游戏形成主观看法，可能与实际情况和其他参与者的解释不一致。超博弈理论虽然提供了处理这种不匹配心理模型的数学框架，但在多智能体系统研究中缺乏统一的形式化表示语言和可扩展算法，阻碍了其实际应用。

Method: 引入基于逻辑的声明式领域特定语言来编码超博弈结构和解决方案概念；利用答案集编程开发自动化管道，用于实例化超博弈结构和运行新颖的超博弈合理化程序，该程序能够找到解释看似非理性结果的信念结构。

Result: 提出的语言为超博弈建立了统一的形式化表示，为开发基于信念的异构推理器奠定了基础，提供了具有逻辑保证的可验证上下文。这些贡献建立了超博弈理论、多智能体系统和战略AI之间的联系。

Conclusion: 该工作填补了超博弈理论在多智能体系统应用中缺乏统一形式化语言和可扩展算法的空白，通过逻辑编程方法实现了超博弈的自动化推理，为异构智能体系统的战略分析提供了新工具。

Abstract: Differences in perception, information asymmetries, and bounded rationality lead game-theoretic players to derive a private, subjective view of the game that may diverge from the underlying ground-truth scenario and may be misaligned with other players' interpretations. While typical game-theoretic assumptions often overlook such heterogeneity, hypergame theory provides the mathematical framework to reason about mismatched mental models. Although hypergames have recently gained traction in dynamic applications concerning uncertainty, their practical adoption in multi-agent system research has been hindered by the lack of a unifying, formal, and practical representation language, as well as scalable algorithms for managing complex hypergame structures and equilibria. Our work addresses this gap by introducing a declarative, logic-based domain-specific language for encoding hypergame structures and hypergame solution concepts. Leveraging answer-set programming, we develop an automated pipeline for instantiating hypergame structures and running our novel hypergame rationalisation procedure, a mechanism for finding belief structures that justify seemingly irrational outcomes. The proposed language establishes a unifying formalism for hypergames and serves as a foundation for developing nuanced, belief-based heterogeneous reasoners, offering a verifiable context with logical guarantees. Together, these contributions establish the connection between hypergame theory, multi-agent systems, and strategic AI.

</details>


### [25] [Log Anomaly Detection with Large Language Models via Knowledge-Enriched Fusion](https://arxiv.org/abs/2512.11997)
*Anfeng Peng,Ajesh Koyatan Chathoth,Stephen Lee*

Main category: cs.AI

TL;DR: EnrichLog是一个无需训练、基于条目的日志异常检测框架，通过检索增强生成技术整合语料库特定和样本特定知识来丰富原始日志条目，提升异常检测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统日志分析技术（如基于模板和序列驱动的方法）经常丢失重要语义信息或难以处理模糊的日志模式，需要一种能保留语义并处理模糊日志的改进方法。

Method: EnrichLog是一个无需训练的异常检测框架，通过检索增强生成技术整合语料库特定知识（历史示例和推理）和样本特定知识来丰富原始日志条目，无需重新训练即可整合相关上下文知识。

Result: 在四个大规模系统日志基准数据集上的评估显示，EnrichLog相比五种基线方法持续提升异常检测性能，有效处理模糊日志条目，保持高效推理，同时整合两种知识增强了模型置信度和检测准确性。

Conclusion: EnrichLog通过整合语料库特定和样本特定知识，实现了更准确、可解释的异常检测，适合实际部署，解决了传统方法在语义保留和模糊模式处理方面的局限性。

Abstract: System logs are a critical resource for monitoring and managing distributed systems, providing insights into failures and anomalous behavior. Traditional log analysis techniques, including template-based and sequence-driven approaches, often lose important semantic information or struggle with ambiguous log patterns. To address this, we present EnrichLog, a training-free, entry-based anomaly detection framework that enriches raw log entries with both corpus-specific and sample-specific knowledge. EnrichLog incorporates contextual information, including historical examples and reasoning derived from the corpus, to enable more accurate and interpretable anomaly detection. The framework leverages retrieval-augmented generation to integrate relevant contextual knowledge without requiring retraining. We evaluate EnrichLog on four large-scale system log benchmark datasets and compare it against five baseline methods. Our results show that EnrichLog consistently improves anomaly detection performance, effectively handles ambiguous log entries, and maintains efficient inference. Furthermore, incorporating both corpus- and sample-specific knowledge enhances model confidence and detection accuracy, making EnrichLog well-suited for practical deployments.

</details>


### [26] [The Forecast Critic: Leveraging Large Language Models for Poor Forecast Identification](https://arxiv.org/abs/2512.12059)
*Luke Bhan,Hanyu Zhang,Andrew Gordon Wilson,Michael W. Mahoney,Chuck Arvin*

Main category: cs.AI

TL;DR: LLMs可用于自动化预测监控，在检测不合理预测方面表现良好，F1分数达0.88，虽略低于人类水平（0.97），但能有效整合非结构化特征提升评估准确性。


<details>
  <summary>Details</summary>
Motivation: 大规模零售业务中，预测监控对客户满意度、盈利能力和运营效率至关重要。传统方法可能无法有效利用非结构化信息和进行复杂推理，需要更智能的自动化监控系统。

Method: 提出Forecast Critic系统，利用LLMs进行自动化预测监控。通过三个实验评估LLMs能力：1)检测明显不合理预测；2)整合非结构化外部特征；3)分析不同模型规模和推理能力的影响。使用合成和真实世界预测数据（包括M5时间序列数据集）。

Result: LLMs能可靠检测和批评不良预测（时间错位、趋势不一致、峰值错误等），最佳模型F1分数0.88。多模态LLMs能有效整合非结构化上下文信号（如促销历史），F1分数0.84。在M5数据集上，不合理预测的sCRPS至少比合理预测高10%。

Conclusion: 即使没有领域特定微调，LLMs也能为自动化预测监控和评估提供可行且可扩展的选项，展示了其在利用广泛世界知识和推理能力方面的潜力。

Abstract: Monitoring forecasting systems is critical for customer satisfaction, profitability, and operational efficiency in large-scale retail businesses. We propose The Forecast Critic, a system that leverages Large Language Models (LLMs) for automated forecast monitoring, taking advantage of their broad world knowledge and strong ``reasoning'' capabilities. As a prerequisite for this, we systematically evaluate the ability of LLMs to assess time series forecast quality, focusing on three key questions. (1) Can LLMs be deployed to perform forecast monitoring and identify obviously unreasonable forecasts? (2) Can LLMs effectively incorporate unstructured exogenous features to assess what a reasonable forecast looks like? (3) How does performance vary across model sizes and reasoning capabilities, measured across state-of-the-art LLMs? We present three experiments, including on both synthetic and real-world forecasting data. Our results show that LLMs can reliably detect and critique poor forecasts, such as those plagued by temporal misalignment, trend inconsistencies, and spike errors. The best-performing model we evaluated achieves an F1 score of 0.88, somewhat below human-level performance (F1 score: 0.97). We also demonstrate that multi-modal LLMs can effectively incorporate unstructured contextual signals to refine their assessment of the forecast. Models correctly identify missing or spurious promotional spikes when provided with historical context about past promotions (F1 score: 0.84). Lastly, we demonstrate that these techniques succeed in identifying inaccurate forecasts on the real-world M5 time series dataset, with unreasonable forecasts having an sCRPS at least 10% higher than that of reasonable forecasts. These findings suggest that LLMs, even without domain-specific fine-tuning, may provide a viable and scalable option for automated forecast monitoring and evaluation.

</details>


### [27] [Rethinking Label Consistency of In-Context Learning: An Implicit Transductive Label Propagation Perspective](https://arxiv.org/abs/2512.12175)
*Haoyang Chen,Richong Zhang,Junfan Chen*

Main category: cs.AI

TL;DR: 本文提出TopK-SD方法，通过结合语义和标签信息合成数据，选择标签一致的示例作为提示，改进了上下文学习中的演示选择机制。


<details>
  <summary>Details</summary>
Motivation: 当前基于检索模型选择语义相似示例的方法存在局限性，因为无法保证标签一致性。作者从贝叶斯视角和转导标签传播角度重新思考ICL，认为标签一致性对ICL效果至关重要。

Method: 提出TopK-SD方法：1）建立标签传播框架，将标签一致性与传播误差边界联系起来；2）提出数据合成方法，结合语义和标签信息；3）使用合成数据进行TopK采样，选择标签一致的演示示例。

Result: TopK-SD方法在多个基准测试中优于原始TopK采样方法，验证了标签一致性对ICL效果的重要性。

Conclusion: 本文为理解ICL工作机制提供了新视角，强调了标签一致性在演示选择中的关键作用，提出的TopK-SD方法有效改进了ICL性能。

Abstract: Large language models (LLMs) perform in-context learning (ICL) with minimal supervised examples, which benefits various natural language processing (NLP) tasks. One of the critical research focus is the selection of prompt demonstrations. Current approaches typically employ retrieval models to select the top-K most semantically similar examples as demonstrations. However, we argue that existing methods are limited since the label consistency is not guaranteed during demonstration selection. Our cognition derives from the Bayesian view of ICL and our rethinking of ICL from the transductive label propagation perspective. We treat ICL as a transductive learning method and incorporate latent concepts from Bayesian view and deduce that similar demonstrations guide the concepts of query, with consistent labels serving as estimates. Based on this understanding, we establish a label propagation framework to link label consistency with propagation error bounds. To model label consistency, we propose a data synthesis method, leveraging both semantic and label information, and use TopK sampling with Synthetic Data (TopK-SD) to acquire demonstrations with consistent labels. TopK-SD outperforms original TopK sampling on multiple benchmarks. Our work provides a new perspective for understanding the working mechanisms within ICL.

</details>


### [28] [Floorplan2Guide: LLM-Guided Floorplan Parsing for BLV Indoor Navigation](https://arxiv.org/abs/2512.12177)
*Aydin Ayanzadeh,Tim Oates*

Main category: cs.AI

TL;DR: 提出Floorplan2Guide系统，利用基础模型将平面图转换为可导航知识图谱，为视障用户生成人类可读的导航指令，提高室内导航精度。


<details>
  <summary>Details</summary>
Motivation: 当前室内导航解决方案主要依赖基础设施系统，限制了在动态环境中的安全导航能力。需要为视障用户提供更精确、无需大量手动预处理的室内导航方案。

Method: 使用大语言模型从建筑布局中提取空间信息，将平面图转换为可导航知识图谱，采用少样本学习策略，通过图形表示和上下文学习增强导航性能。

Result: Claude 3.7 Sonnet在5-shot提示下达到最高准确率：短路线92.31%、中路线76.92%、长路线61.54%。基于图的空间结构成功率比直接视觉推理高15.4%，证实图形表示和上下文学习能提升导航性能。

Conclusion: Floorplan2Guide系统通过将平面图转换为知识图谱并生成人类可读指令，为视障用户提供了更精确的室内导航解决方案，图形表示和少样本学习显著提升了导航性能。

Abstract: Indoor navigation remains a critical challenge for people with visual impairments. The current solutions mainly rely on infrastructure-based systems, which limit their ability to navigate safely in dynamic environments. We propose a novel navigation approach that utilizes a foundation model to transform floor plans into navigable knowledge graphs and generate human-readable navigation instructions. Floorplan2Guide integrates a large language model (LLM) to extract spatial information from architectural layouts, reducing the manual preprocessing required by earlier floorplan parsing methods. Experimental results indicate that few-shot learning improves navigation accuracy in comparison to zero-shot learning on simulated and real-world evaluations. Claude 3.7 Sonnet achieves the highest accuracy among the evaluated models, with 92.31%, 76.92%, and 61.54% on the short, medium, and long routes, respectively, under 5-shot prompting of the MP-1 floor plan. The success rate of graph-based spatial structure is 15.4% higher than that of direct visual reasoning among all models, which confirms that graphical representation and in-context learning enhance navigation performance and make our solution more precise for indoor navigation of Blind and Low Vision (BLV) users.

</details>


### [29] [TA-KAND: Two-stage Attention Triple Enhancement and U-KAN based Diffusion For Few-shot Knowledge Graph Completion](https://arxiv.org/abs/2512.12182)
*Xinyu Gao*

Main category: cs.AI

TL;DR: 提出一个结合两阶段注意力三元增强器和U-KAN扩散模型的少样本知识图谱补全框架，在公开数据集上达到SOTA效果


<details>
  <summary>Details</summary>
Motivation: 现实世界知识图谱的关系分布呈现长尾特性，传统基于度量匹配或元学习的方法未能充分利用图邻域信息或忽视对比信号的分布特征

Method: 从生成表示视角重新审视问题，提出整合两阶段注意力三元增强器与基于U-KAN的扩散模型的少样本知识图谱补全框架

Result: 在两个公开数据集上的大量实验表明，该方法取得了新的最先进结果

Conclusion: 通过生成表示视角和创新的两阶段注意力三元增强器与U-KAN扩散模型集成，有效解决了少样本知识图谱补全问题

Abstract: Knowledge Graphs (KGs), thanks to their concise and efficient triple-based structure, have been widely applied in intelligent question answering, recommender systems and other domains. However, the heterogeneous and multifaceted nature of real-world data inevitably renders the distribution of relations long-tailed, making it crucial to complete missing facts with limited samples. Previous studies mainly based on metric matching or meta learning, yet they either fail to fully exploit neighborhood information in graph or overlook the distributional characteristics of contrastive signals. In this paper, we re-examine the problem from a perspective of generative representation and propose a few-shot knowledge graph completion framework that integrates two-stage attention triple enhancer with U-KAN based diffusion model. Extensive experiments on two public datasets show that our method achieve new state-of-the-art results.

</details>


### [30] [A Geometric Theory of Cognition](https://arxiv.org/abs/2512.12225)
*Laha Ale*

Main category: cs.AI

TL;DR: 提出一个统一的几何框架，将各种认知过程解释为单一几何原理的涌现：认知状态表示为黎曼流形上的点，认知过程是该流形上认知势能的梯度流。


<details>
  <summary>Details</summary>
Motivation: 人类认知包含感知、记忆、直觉判断、深思熟虑推理、行动选择和社会推理等多种能力，但这些能力通常由不同的计算理论解释。需要建立一个统一的数学框架来解释这些多样化的认知过程。

Method: 将认知状态表示为微分流形上的点，赋予学习得到的黎曼度量，该度量编码表征约束、计算成本和认知变量间的结构关系。定义标量认知势能，结合预测准确性、结构简洁性、任务效用和规范或逻辑要求。认知过程是该势能在黎曼度量下的梯度流。

Result: 经典的双过程效应（快速直觉反应和缓慢深思熟虑推理）自然地从度量诱导的各向异性中涌现，产生内在时间尺度分离和几何相变，无需调用模块化或混合架构。通过模拟经典认知任务展示了这些机制的行为特征。

Conclusion: 这些结果为认知建立了几何基础，并为开发更通用、更类人的人工智能系统提供了指导原则。

Abstract: Human cognition spans perception, memory, intuitive judgment, deliberative reasoning, action selection, and social inference, yet these capacities are often explained through distinct computational theories. Here we present a unified mathematical framework in which diverse cognitive processes emerge from a single geometric principle. We represent the cognitive state as a point on a differentiable manifold endowed with a learned Riemannian metric that encodes representational constraints, computational costs, and structural relations among cognitive variables. A scalar cognitive potential combines predictive accuracy, structural parsimony, task utility, and normative or logical requirements. Cognition unfolds as the Riemannian gradient flow of this potential, providing a universal dynamical law from which a broad range of psychological phenomena arise. Classical dual-process effects--rapid intuitive responses and slower deliberative reasoning--emerge naturally from metric-induced anisotropies that generate intrinsic time-scale separations and geometric phase transitions, without invoking modular or hybrid architectures. We derive analytical conditions for these regimes and demonstrate their behavioural signatures through simulations of canonical cognitive tasks. Together, these results establish a geometric foundation for cognition and suggest guiding principles for the development of more general and human-like artificial intelligence systems.

</details>


### [31] [A Multi-Axial Mindset for Ontology Design Lessons from Wikidata's Polyhierarchical Structure](https://arxiv.org/abs/2512.12260)
*Ege Atacan Doğan,Peter F. Patel-Schneider*

Main category: cs.AI

TL;DR: Wikidata采用多层级、多轴分类设计，而非传统本体论的单向层次结构，支持更灵活、可扩展的知识图谱构建。


<details>
  <summary>Details</summary>
Motivation: 传统本体设计强调互斥且穷尽的顶层区分（如持续体vs发生体、抽象vs具体、类型vs实例），这些区分用于构建统一层次结构，每个实体都归类于单一顶层类别。而Wikidata不强制执行单一基础分类法，这促使研究者分析其多层级、多轴设计的结构意义。

Method: 分析Wikidata的多层级和多轴分类架构，研究其在共享根类"实体"下同时容纳多个分类轴的结构设计，探讨这种设计如何支持协作和演化的知识图谱。

Result: Wikidata的架构实现了可扩展和模块化的本体构建方法，特别适合协作和演化的知识图谱，其多轴设计提供了比传统单向层次结构更灵活的分类方式。

Conclusion: Wikidata的多层级、多轴分类设计为大规模协作知识图谱提供了更灵活、可扩展的架构，突破了传统本体论的单向层次结构限制，更适合动态演化的知识系统。

Abstract: Traditional ontology design emphasizes disjoint and exhaustive top-level distinctions such as continuant vs. occurrent, abstract vs. concrete, or type vs. instance. These distinctions are used to structure unified hierarchies where every entity is classified under a single upper-level category. Wikidata, by contrast, does not enforce a singular foundational taxonomy. Instead, it accommodates multiple classification axes simultaneously under the shared root class entity. This paper analyzes the structural implications of Wikidata's polyhierarchical and multi-axial design. The Wikidata architecture enables a scalable and modular approach to ontology construction, especially suited to collaborative and evolving knowledge graphs.

</details>


### [32] [Entropy Collapse: A Universal Failure Mode of Intelligent Systems](https://arxiv.org/abs/2512.12381)
*Truong Xuan Khanh,Truong Quynh Hoa*

Main category: cs.AI

TL;DR: 论文提出"熵崩溃"概念，解释智能系统（AI、经济制度、生物进化）在提升智能过程中会经历从高熵自适应状态到低熵崩溃状态的相变，导致系统僵化、适应性下降


<details>
  <summary>Details</summary>
Motivation: 智能系统（人工智能、经济制度、生物进化）普遍存在一个悖论：随着智能提升，系统反而变得僵化、失去适应性、意外失效。作者旨在揭示这一普遍现象背后的统一机制

Method: 提出"熵崩溃"理论框架，在最小化领域无关假设下，将智能系统崩溃形式化为向稳定低熵流形的收敛过程。通过分析建立临界阈值、动态不可逆性和吸引子结构，并通过最小模拟展示更新机制的普适性

Result: 证明智能系统会经历从高熵自适应状态到低熵崩溃状态的急剧相变，崩溃表现为有效适应维度的收缩而非活动或规模的丧失。该框架统一了AI中的模型崩溃、经济学中的制度僵化和进化中的遗传瓶颈等现象

Conclusion: 将崩溃重新定义为智能的结构性成本，解释了晚期干预为何系统性地失败，并提出了基于熵意识的设计原则，以维持智能系统的长期适应性

Abstract: Intelligent systems are widely assumed to improve through learning, coordination, and optimization. However, across domains -- from artificial intelligence to economic institutions and biological evolution -- increasing intelligence often precipitates paradoxical degradation: systems become rigid, lose adaptability, and fail unexpectedly.
  We identify \emph{entropy collapse} as a universal dynamical failure mode arising when feedback amplification outpaces bounded novelty regeneration. Under minimal domain-agnostic assumptions, we show that intelligent systems undergo a sharp transition from high-entropy adaptive regimes to low-entropy collapsed regimes. Collapse is formalized as convergence toward a stable low-entropy manifold, not a zero-entropy state, implying a contraction of effective adaptive dimensionality rather than loss of activity or scale.
  We analytically establish critical thresholds, dynamical irreversibility, and attractor structure and demonstrate universality across update mechanisms through minimal simulations. This framework unifies diverse phenomena -- model collapse in AI, institutional sclerosis in economics, and genetic bottlenecks in evolution -- as manifestations of the same underlying process.
  By reframing collapse as a structural cost of intelligence, our results clarify why late-stage interventions systematically fail and motivate entropy-aware design principles for sustaining long-term adaptability in intelligent systems.
  \noindent\textbf{Keywords:} entropy collapse; intelligent systems; feedback amplification; phase transitions; effective dimensionality; complex systems; model collapse; institutional sclerosis

</details>


### [33] [Feeling the Strength but Not the Source: Partial Introspection in LLMs](https://arxiv.org/abs/2512.12411)
*Ely Hahami,Lavik Jain,Ishaan Sinha*

Main category: cs.AI

TL;DR: 该研究测试了Anthropic关于语言模型能够检测和命名注入"概念"激活方向的说法，发现模型确实能在一定程度上进行内省，但这种能力具有脆弱性和提示敏感性。


<details>
  <summary>Details</summary>
Motivation: 验证Anthropic声称的前沿模型能够检测和命名注入"概念"激活方向的稳健性，探索这种内省能力的范围和局限性。

Method: 1) 在Meta-Llama-3.1-8B-Instruct上复现Anthropic的多轮"涌现内省"结果；2) 系统性地改变推理提示，测试内省能力的稳健性；3) 探索部分内省机制，让模型分类注入概念向量的系数强度。

Result: 1) 成功复现Anthropic的结果，模型在20%的情况下识别并命名注入概念；2) 内省能力具有脆弱性：在多项选择识别或二元判别等相近任务上性能崩溃；3) 发现部分内省机制：模型能以70%的准确率（远高于25%的基线）分类注入概念向量的系数强度。

Conclusion: 语言模型确实能够基于其内部表示进行内省计算，但这种自我报告能力是狭窄且提示敏感的，内省能力在不同任务和提示下表现出显著差异。

Abstract: Recent work from Anthropic claims that frontier models can sometimes detect and name injected "concepts" represented as activation directions. We test the robustness of these claims. First, we reproduce Anthropic's multi-turn "emergent introspection" result on Meta-Llama-3.1-8B-Instruct, finding that the model identifies and names the injected concept 20 percent of the time under Anthropic's original pipeline, exactly matching their reported numbers and thus showing that introspection is not exclusive to very large or capable models. Second, we systematically vary the inference prompt and find that introspection is fragile: performance collapses on closely related tasks such as multiple-choice identification of the injected concept or different prompts of binary discrimination of whether a concept was injected at all. Third, we identify a contrasting regime of partial introspection: the same model can reliably classify the strength of the coefficient of a normalized injected concept vector (as weak / moderate / strong / very strong) with up to 70 percent accuracy, far above the 25 percent chance baseline. Together, these results provide more evidence for Anthropic's claim that language models effectively compute a function of their baseline, internal representations during introspection; however, these self-reports about those representations are narrow and prompt-sensitive. Our code is available at https://github.com/elyhahami18/CS2881-Introspection.

</details>


### [34] [Understanding Critical Thinking in Generative Artificial Intelligence Use: Development, Validation, and Correlates of the Critical Thinking in AI Use Scale](https://arxiv.org/abs/2512.12413)
*Gabriel R. Lau,Wei Yan Low,Louis Tay,Ysabel Guevarra,Dragan Gašević,Andree Hartanto*

Main category: cs.AI

TL;DR: 开发并验证了一个13项量表来测量AI使用中的批判性思维，包含验证、动机和反思三个维度，该量表能预测更有效的AI输出验证策略和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在日常工作和学习中的普及，用户需要批判性地评估AI输出而非盲目接受，但目前缺乏系统测量AI使用中批判性思维的工具。

Method: 通过六项研究（N=1365）开发并验证了13项AI使用批判性思维量表：研究1生成项目，研究2验证三因素结构（验证、动机、反思），研究3-5确认高阶模型并检验信效度，研究6验证量表预测效度。

Result: 量表具有良好的心理测量特性，AI使用批判性思维与开放性、外向性、积极特质情感和AI使用频率正相关，能预测更频繁多样的验证策略、更高的真实性判断准确性以及更深入的AI责任反思。

Conclusion: 该研究阐明了人们如何监督生成式AI输出，提供了经过验证的量表和生态效度高的任务范式，支持理论检验、跨群体和纵向研究，促进对生成式AI输出的批判性参与。

Abstract: Generative AI tools are increasingly embedded in everyday work and learning, yet their fluency, opacity, and propensity to hallucinate mean that users must critically evaluate AI outputs rather than accept them at face value. The present research conceptualises critical thinking in AI use as a dispositional tendency to verify the source and content of AI-generated information, to understand how models work and where they fail, and to reflect on the broader implications of relying on AI. Across six studies (N = 1365), we developed and validated the 13-item critical thinking in AI use scale and mapped its nomological network. Study 1 generated and content-validated scale items. Study 2 supported a three-factor structure (Verification, Motivation, and Reflection). Studies 3, 4, and 5 confirmed this higher-order model, demonstrated internal consistency and test-retest reliability, strong factor loadings, sex invariance, and convergent and discriminant validity. Studies 3 and 4 further revealed that critical thinking in AI use was positively associated with openness, extraversion, positive trait affect, and frequency of AI use. Lastly, Study 6 demonstrated criterion validity of the scale, with higher critical thinking in AI use scores predicting more frequent and diverse verification strategies, greater veracity-judgement accuracy in a novel and naturalistic ChatGPT-powered fact-checking task, and deeper reflection about responsible AI. Taken together, the current work clarifies why and how people exercise oversight over generative AI outputs and provides a validated scale and ecologically grounded task paradigm to support theory testing, cross-group, and longitudinal research on critical engagement with generative AI outputs.

</details>


### [35] [AI Transparency Atlas: Framework, Scoring, and Real-Time Model Card Evaluation Pipeline](https://arxiv.org/abs/2512.12443)
*Akhmadillo Mamirov,Faiaz Azmain,Hanyu Wang*

Main category: cs.AI

TL;DR: 该研究分析了AI模型文档的碎片化和不一致问题，开发了一个加权透明度框架，并实现了一个自动化多智能体管道来评估模型文档的完整性，发现前沿实验室的合规性约为80%，而大多数提供商的合规性低于60%。


<details>
  <summary>Details</summary>
Motivation: AI模型文档在不同平台间碎片化且结构不一致，阻碍了政策制定者、审计者和用户可靠评估安全声明、数据来源和版本级变更的能力。

Method: 分析了5个前沿模型和100个Hugging Face模型卡的文档，识别出947个独特的章节名称；基于欧盟AI法案附件IV和斯坦福透明度指数开发了加权透明度框架（8个部分23个子部分）；实现了自动化多智能体管道，从公共来源提取文档并通过基于LLM的共识进行完整性评分。

Result: 评估50个模型（涵盖视觉、多模态、开源和闭源系统）的总成本低于3美元；前沿实验室（xAI、微软、Anthropic）的合规性约为80%，而大多数提供商的合规性低于60%；安全关键类别显示最大缺陷：欺骗行为、幻觉和儿童安全评估分别导致148、124和116个总积分损失。

Conclusion: AI模型文档存在系统性缺陷，特别是在安全关键领域；自动化评估方法成本低廉且可扩展；需要标准化文档框架以确保透明度，特别是在安全评估方面。

Abstract: AI model documentation is fragmented across platforms and inconsistent in structure, preventing policymakers, auditors, and users from reliably assessing safety claims, data provenance, and version-level changes. We analyzed documentation from five frontier models (Gemini 3, Grok 4.1, Llama 4, GPT-5, and Claude 4.5) and 100 Hugging Face model cards, identifying 947 unique section names with extreme naming variation. Usage information alone appeared under 97 distinct labels. Using the EU AI Act Annex IV and the Stanford Transparency Index as baselines, we developed a weighted transparency framework with 8 sections and 23 subsections that prioritizes safety-critical disclosures (Safety Evaluation: 25%, Critical Risk: 20%) over technical specifications. We implemented an automated multi-agent pipeline that extracts documentation from public sources and scores completeness through LLM-based consensus. Evaluating 50 models across vision, multimodal, open-source, and closed-source systems cost less than $3 in total and revealed systematic gaps. Frontier labs (xAI, Microsoft, Anthropic) achieve approximately 80% compliance, while most providers fall below 60%. Safety-critical categories show the largest deficits: deception behaviors, hallucinations, and child safety evaluations account for 148, 124, and 116 aggregate points lost, respectively, across all evaluated models.

</details>


### [36] [MetaHGNIE: Meta-Path Induced Hypergraph Contrastive Learning in Heterogeneous Knowledge Graphs](https://arxiv.org/abs/2512.12477)
*Jiawen Chen,Yanyan He,Qi Shao,Mengli Wei,Duxin Chen,Wenwu Yu,Yanlong Zhao*

Main category: cs.AI

TL;DR: 提出MetaHGNIE框架，通过元路径诱导的超图对比学习来解耦和对齐异构知识图谱中的结构和语义信息，以解决节点重要性估计问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖成对连接，忽略了多个实体和关系之间的高阶依赖关系，并且将结构和语义信号独立处理，阻碍了有效的跨模态整合。

Method: 通过元路径序列构建高阶知识图谱，其中类型化超边捕获多实体关系上下文；使用局部注意力聚合结构依赖，通过配备稀疏分块技术的超图变换器编码语义表示；最后通过多模态融合模块在对比学习和辅助监督下整合结构和语义嵌入。

Result: 在基准NIE数据集上的广泛实验表明，MetaHGNIE始终优于最先进的基线方法。

Conclusion: 结果强调了在异构知识图谱中显式建模高阶交互和跨模态对齐的有效性。

Abstract: Node importance estimation (NIE) in heterogeneous knowledge graphs is a critical yet challenging task, essential for applications such as recommendation, knowledge reasoning, and question answering. Existing methods often rely on pairwise connections, neglecting high-order dependencies among multiple entities and relations, and they treat structural and semantic signals independently, hindering effective cross-modal integration. To address these challenges, we propose MetaHGNIE, a meta-path induced hypergraph contrastive learning framework for disentangling and aligning structural and semantic information. MetaHGNIE constructs a higher-order knowledge graph via meta-path sequences, where typed hyperedges capture multi-entity relational contexts. Structural dependencies are aggregated with local attention, while semantic representations are encoded through a hypergraph transformer equipped with sparse chunking to reduce redundancy. Finally, a multimodal fusion module integrates structural and semantic embeddings under contrastive learning with auxiliary supervision, ensuring robust cross-modal alignment. Extensive experiments on benchmark NIE datasets demonstrate that MetaHGNIE consistently outperforms state-of-the-art baselines. These results highlight the effectiveness of explicitly modeling higher-order interactions and cross-modal alignment in heterogeneous knowledge graphs. Our code is available at https://github.com/SEU-WENJIA/DualHNIE

</details>


### [37] [SafeGen: Embedding Ethical Safeguards in Text-to-Image Generation](https://arxiv.org/abs/2512.12501)
*Dang Phuong Nam,Nguyen Kieu,Pham Thanh Hieu*

Main category: cs.AI

TL;DR: SafeGen是一个将伦理保障嵌入文本到图像生成流程的框架，通过BGE-M3文本分类器过滤有害提示和Hyper-SD扩散模型生成高质量图像，在创意自由与伦理责任之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在创造表达、教育和研究方面带来机遇，但文本到图像系统存在双重使用困境：放大社会偏见、产生高保真虚假信息、侵犯知识产权等伦理问题。需要将伦理保障直接集成到生成流程中。

Method: SafeGen框架包含两个互补组件：1) BGE-M3：微调的文本分类器，过滤有害或误导性提示；2) Hyper-SD：优化的扩散模型，生成高保真、语义对齐的图像。基于多语言（英语-越南语）数据集和公平感知训练流程构建。

Result: 定量评估显示Hyper-SD达到IS=3.52、FID=22.08、SSIM=0.79，BGE-M3达到F1分数0.81。消融研究验证了领域特定微调的重要性。案例研究展示了SafeGen在阻止不安全提示、生成包容性教学材料和加强学术诚信方面的实际影响。

Conclusion: SafeGen证明创意自由与伦理责任可以在单一工作流程中协调，为可信AI原则在生成式AI系统中的实际应用提供了框架，展示了伦理保障与高质量图像生成可以共存。

Abstract: Generative Artificial Intelligence (AI) has created unprecedented opportunities for creative expression, education, and research. Text-to-image systems such as DALL.E, Stable Diffusion, and Midjourney can now convert ideas into visuals within seconds, but they also present a dual-use dilemma, raising critical ethical concerns: amplifying societal biases, producing high-fidelity disinformation, and violating intellectual property. This paper introduces SafeGen, a framework that embeds ethical safeguards directly into the text-to-image generation pipeline, grounding its design in established principles for Trustworthy AI. SafeGen integrates two complementary components: BGE-M3, a fine-tuned text classifier that filters harmful or misleading prompts, and Hyper-SD, an optimized diffusion model that produces high fidelity, semantically aligned images. Built on a curated multilingual (English- Vietnamese) dataset and a fairness-aware training process, SafeGen demonstrates that creative freedom and ethical responsibility can be reconciled within a single workflow. Quantitative evaluations confirm its effectiveness, with Hyper-SD achieving IS = 3.52, FID = 22.08, and SSIM = 0.79, while BGE-M3 reaches an F1-Score of 0.81. An ablation study further validates the importance of domain-specific fine-tuning for both modules. Case studies illustrate SafeGen's practical impact in blocking unsafe prompts, generating inclusive teaching materials, and reinforcing academic integrity.

</details>


### [38] [KidsArtBench: Multi-Dimensional Children's Art Evaluation with Attribute-Aware MLLMs](https://arxiv.org/abs/2512.12503)
*Mingrui Ye,Chanjin Zheng,Zengyi Yu,Chenyu Xiang,Zhixue Zhao,Zheng Yuan,Helen Yannakoudakis*

Main category: cs.AI

TL;DR: KidsArtBench：针对儿童艺术作品的评估基准，包含多维度评分和专家反馈，通过属性特定的多LoRA方法提升MLLMs在艺术教育评估中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在艺术表达评估方面能力有限，主要因为美学概念抽象且开放，同时缺乏多模态艺术作品标注数据。特别是针对儿童艺术作品的评估数据集稀缺，现有数据集多为成人图像提供单一分数，无法满足教育场景的需求。

Method: 1. 构建KidsArtBench基准：包含1000+幅5-15岁儿童艺术作品，由12位专家教育者按照9个维度进行标注，并提供专家反馈意见。2. 提出属性特定的多LoRA方法：每个评估维度对应一个独立的LoRA适配器，结合回归感知微调（RAFT）使预测与有序评分尺度对齐。

Result: 在Qwen2.5-VL-7B模型上，该方法将相关性从0.468提升至0.653，在感知维度上提升最大，在高阶属性上的差距也显著缩小。这表明教育者对齐的监督和属性感知训练能够产生具有教育意义的评估结果。

Conclusion: KidsArtBench为教育AI的持续进步建立了严格的测试平台，通过多维度标注和专家反馈监督，结合属性特定的多LoRA方法，显著提升了MLLMs在儿童艺术作品评估方面的能力，具有重要的教育应用价值。

Abstract: Multimodal Large Language Models (MLLMs) show remarkable progress across many visual-language tasks; however, their capacity to evaluate artistic expression remains limited. Aesthetic concepts are inherently abstract and open-ended, and multimodal artwork annotations are scarce. We introduce KidsArtBench, a new benchmark of over 1k children's artworks (ages 5-15) annotated by 12 expert educators across 9 rubric-aligned dimensions, together with expert comments for feedback. Unlike prior aesthetic datasets that provide single scalar scores on adult imagery, KidsArtBench targets children's artwork and pairs multi-dimensional annotations with comment supervision to enable both ordinal assessment and formative feedback. Building on this resource, we propose an attribute-specific multi-LoRA approach, where each attribute corresponds to a distinct evaluation dimension (e.g., Realism, Imagination) in the scoring rubric, with Regression-Aware Fine-Tuning (RAFT) to align predictions with ordinal scales. On Qwen2.5-VL-7B, our method increases correlation from 0.468 to 0.653, with the largest gains on perceptual dimensions and narrowed gaps on higher-order attributes. These results show that educator-aligned supervision and attribute-aware training yield pedagogically meaningful evaluations and establish a rigorous testbed for sustained progress in educational AI. We release data and code with ethics documentation.

</details>


### [39] [World Models Unlock Optimal Foraging Strategies in Reinforcement Learning Agents](https://arxiv.org/abs/2512.12548)
*Yesid Fonseca,Manuel S. Ríos,Nicanor Quijano,Luis F. Giraldo*

Main category: cs.AI

TL;DR: 基于模型的强化学习智能体通过习得环境预测表示，自然收敛到与边际价值定理一致的觅食策略，比无模型智能体更接近生物觅食者的决策模式。


<details>
  <summary>Details</summary>
Motivation: 尽管边际价值定理(MVT)被广泛用于预测行为生态学中的觅食行为，但生物觅食者如何实现最优斑块觅食决策的计算机制仍不清楚。本研究旨在探索人工智能系统如何通过计算机制实现与MVT一致的觅食策略。

Method: 使用基于模型的强化学习智能体，让其习得简洁的环境预测表示。通过比较基于模型智能体与标准无模型强化学习智能体的决策模式，分析预测能力对斑块离开行为的影响。

Result: 基于模型的智能体自然收敛到与MVT一致的觅食策略，其决策模式与许多生物觅食者相似。研究发现，预测能力（而非单纯的奖励最大化）驱动了高效的斑块离开行为。

Conclusion: 预测性世界模型可以作为AI系统更可解释、更具生物学基础的决策基础。生态最优性原理对推进可解释和自适应AI具有重要价值。

Abstract: Patch foraging involves the deliberate and planned process of determining the optimal time to depart from a resource-rich region and investigate potentially more beneficial alternatives. The Marginal Value Theorem (MVT) is frequently used to characterize this process, offering an optimality model for such foraging behaviors. Although this model has been widely used to make predictions in behavioral ecology, discovering the computational mechanisms that facilitate the emergence of optimal patch-foraging decisions in biological foragers remains under investigation. Here, we show that artificial foragers equipped with learned world models naturally converge to MVT-aligned strategies. Using a model-based reinforcement learning agent that acquires a parsimonious predictive representation of its environment, we demonstrate that anticipatory capabilities, rather than reward maximization alone, drive efficient patch-leaving behavior. Compared with standard model-free RL agents, these model-based agents exhibit decision patterns similar to many of their biological counterparts, suggesting that predictive world models can serve as a foundation for more explainable and biologically grounded decision-making in AI systems. Overall, our findings highlight the value of ecological optimality principles for advancing interpretable and adaptive AI.

</details>


### [40] [Large Language Newsvendor: Decision Biases and Cognitive Mechanisms](https://arxiv.org/abs/2512.12552)
*Jifei Liu,Zhi Chen,Yuanguang Zhong*

Main category: cs.AI

TL;DR: 研究发现大型语言模型在供应链决策中会复制并放大人类认知偏差，GPT-4因过度思考表现出最大非理性，而效率优化的GPT-4o表现接近最优


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs越来越多地融入商业决策，但其复制和放大人类认知偏见的风险尚未被充分理解，这在供应链管理等高风险运营环境中尤为关键

Method: 使用经典的报童问题在动态设置中测试GPT-4、GPT-4o和LLaMA-8B，通过多轮实验检验五种已确立的决策偏见

Result: LLMs一致复制了经典的"订购量过低/过高"偏见，并显著放大了需求追逐等行为；发现"智能悖论"：更复杂的GPT-4因过度思考表现出最大非理性，而效率优化的GPT-4o表现接近最优

Conclusion: 偏见源于架构约束而非知识缺口；管理者应根据具体任务选择模型，需要强有力的人工监督，设计结构化、基于规则的提示是约束模型启发式倾向的有效策略

Abstract: Problem definition: Although large language models (LLMs) are increasingly integrated into business decision making, their potential to replicate and even amplify human cognitive biases cautions a significant, yet not well-understood, risk. This is particularly critical in high-stakes operational contexts like supply chain management. To address this, we investigate the decision-making patterns of leading LLMs using the canonical newsvendor problem in a dynamic setting, aiming to identify the nature and origins of their cognitive biases. Methodology/results: Through dynamic, multi-round experiments with GPT-4, GPT-4o, and LLaMA-8B, we tested for five established decision biases. We found that LLMs consistently replicated the classic ``Too Low/Too High'' ordering bias and significantly amplified other tendencies like demand-chasing behavior compared to human benchmarks. Our analysis uncovered a ``paradox of intelligence'': the more sophisticated GPT-4 demonstrated the greatest irrationality through overthinking, while the efficiency-optimized GPT-4o performed near-optimally. Because these biases persist even when optimal formulas are provided, we conclude they stem from architectural constraints rather than knowledge gaps. Managerial implications: First, managers should select models based on the specific task, as our results show that efficiency-optimized models can outperform more complex ones on certain optimization problems. Second, the significant amplification of bias by LLMs highlights the urgent need for robust human-in-the-loop oversight in high-stakes decisions to prevent costly errors. Third, our findings suggest that designing structured, rule-based prompts is a practical and effective strategy for managers to constrain models' heuristic tendencies and improve the reliability of AI-assisted decisions.

</details>


### [41] [AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation](https://arxiv.org/abs/2512.12597)
*Miriam Horovicz*

Main category: cs.AI

TL;DR: AgentSHAP是首个用于解释LLM智能体中工具重要性的框架，基于博弈论中的Shapley值，通过蒙特卡洛采样降低计算成本，无需访问模型内部权重。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM智能体虽然能使用外部工具解决复杂任务，但缺乏解释哪些工具对响应有实际贡献的方法，现有XAI方法无法解决工具层面的解释问题。

Method: 提出AgentSHAP框架，将智能体视为黑盒，使用蒙特卡洛Shapley值方法，通过测试不同工具子集下智能体的响应来计算公平的重要性分数。

Result: 在API-Bank上的实验表明，AgentSHAP能产生跨运行一致的重要性分数，正确识别重要工具，并能区分相关与无关工具。

Conclusion: AgentSHAP填补了工具层面解释的空白，与TokenSHAP和PixelSHAP共同构成了基于Shapley值的现代生成AI可解释性工具家族。

Abstract: LLM agents that use external tools can solve complex tasks, but understanding which tools actually contributed to a response remains a blind spot. No existing XAI methods address tool-level explanations. We introduce AgentSHAP, the first framework for explaining tool importance in LLM agents. AgentSHAP is model-agnostic: it treats the agent as a black box and works with any LLM (GPT, Claude, Llama, etc.) without needing access to internal weights or gradients. Using Monte Carlo Shapley values, AgentSHAP tests how an agent responds with different tool subsets and computes fair importance scores based on game theory. Our contributions are: (1) the first explainability method for agent tool attribution, grounded in Shapley values from game theory; (2) Monte Carlo sampling that reduces cost from O(2n) to practical levels; and (3) comprehensive experiments on API-Bank showing that AgentSHAP produces consistent scores across runs, correctly identifies which tools matter, and distinguishes relevant from irrelevant tools. AgentSHAP joins TokenSHAP (for tokens) and PixelSHAP (for image regions) to complete a family of Shapley-based XAI tools for modern generative AI. Code: https://github.com/GenAISHAP/TokenSHAP.

</details>


### [42] [Value-Aware Multiagent Systems](https://arxiv.org/abs/2512.12652)
*Nardine Osman*

Main category: cs.AI

TL;DR: 该论文提出了AI价值意识的概念，超越了传统的价值对齐问题，提供了一个简明的工程化路线图，包含三个核心支柱：学习表示人类价值、确保个体与多智能体系统的价值对齐、提供基于价值的行为解释。


<details>
  <summary>Details</summary>
Motivation: 传统AI价值对齐方法存在局限性，需要更全面的价值意识框架来确保AI系统不仅与人类价值对齐，还能理解、表示和解释这些价值。

Method: 提出价值意识AI的三支柱路线图：1) 使用形式语义学习和表示人类价值；2) 确保个体智能体和多智能体系统的价值对齐；3) 提供基于价值的行为可解释性。

Result: 论文展示了在相关主题上的持续研究工作，包括将这些方法应用于现实生活领域的案例。

Conclusion: 价值意识AI框架为工程化开发具有价值意识的AI系统提供了结构化路线图，超越了单纯的价值对齐，涵盖了价值学习、对齐和解释的完整流程。

Abstract: This paper introduces the concept of value awareness in AI, which goes beyond the traditional value-alignment problem. Our definition of value awareness presents us with a concise and simplified roadmap for engineering value-aware AI. The roadmap is structured around three core pillars: (1) learning and representing human values using formal semantics, (2) ensuring the value alignment of both individual agents and multiagent systems, and (3) providing value-based explainability on behaviour. The paper presents a selection of our ongoing work on some of these topics, along with applications to real-life domains.

</details>


### [43] [Causal Counterfactuals Reconsidered](https://arxiv.org/abs/2512.12804)
*Sander Beckers*

Main category: cs.AI

TL;DR: 本文提出了一种新的反事实概率语义学，它扩展了标准的Pearl语义学，适用于无法扩展为现实结构因果模型的概率因果模型，解决了Pearl和Dawid关于反事实的长期争论。


<details>
  <summary>Details</summary>
Motivation: 现有Pearl语义学只能应用于可扩展为结构因果模型的概率因果模型，但作者发现即使在简单设置中也会出现无法扩展的模型。需要一种更通用的语义学来解决Pearl和Dawid关于反事实的长期争论，同时避免普遍因果决定论和不现实变量的问题。

Method: 提出新的反事实概率语义学，限制于满足马尔可夫条件、仅包含现实变量且因果完备的因果模型。虽然使用结构因果模型框架，但避免使用所谓的响应变量。证明该语义学与另外两种不涉及结构因果模型的近期提案等价。

Result: 新语义学成功推广了标准Pearl语义学，适用于更广泛的概率因果模型。在Pearl和Dawid的争论中找到了折中方案：同意Dawid拒绝普遍因果决定论和不现实变量，但同意Pearl认为一般反事实语义学是可能的。证明与文献中其他关于随机反事实的评论一致。

Conclusion: 本文提出的新语义学为反事实概率提供了更通用的框架，弥合了Pearl和Dawid方法论之间的分歧。同时探讨了马尔可夫条件的普遍性，并提出了因果抽象的新推广，为因果推理领域做出了重要贡献。

Abstract: I develop a novel semantics for probabilities of counterfactuals that generalizes the standard Pearlian semantics: it applies to probabilistic causal models that cannot be extended into realistic structural causal models and are therefore beyond the scope of Pearl's semantics. This generalization is needed because, as I show, such probabilistic causal models arise even in simple settings. My semantics offer a natural compromize in the long-standing debate between Pearl and Dawid over counterfactuals: I agree with Dawid that universal causal determinism and unrealistic variables should be rejected, but I agree with Pearl that a general semantics of counterfactuals is nonetheless possible. I restrict attention to causal models that satisfy the Markov condition, only contain realistic variables, and are causally complete. Although I formulate my proposal using structural causal models, as does Pearl, I refrain from using so-called response variables. Moreover, I prove that my semantics is equivalent to two other recent proposals that do not involve structural causal models, and that it is in line with various comments on stochastic counterfactuals that have appeared in the literature more broadly. Throughout I also reflect on the universality of the Markov condition and explore a novel generalization of causal abstractions

</details>


### [44] [Forgetful but Faithful: A Cognitive Memory Architecture and Benchmark for Privacy-Aware Generative Agents](https://arxiv.org/abs/2512.12856)
*Saad Alqithami*

Main category: cs.AI

TL;DR: 论文提出MaRS框架和六种遗忘策略，通过FiFA基准测试，在性能、隐私和计算效率间取得平衡，混合策略获得0.911综合得分。


<details>
  <summary>Details</summary>
Motivation: 生成式智能体在长期交互场景中，无限内存存储导致计算不可行和隐私问题，简单遗忘机制又损害智能体连贯性和功能，需要新的内存管理方案。

Method: 提出Memory-Aware Retention Schema (MaRS)框架和六种理论基础的遗忘策略，创建Forgetful but Faithful Agent (FiFA)评估基准，测试300次运行。

Result: 混合遗忘策略获得0.911综合得分，在叙事连贯性、目标完成、社交回忆准确性、隐私保护和成本效率方面表现优异，保持计算可行性和隐私保障。

Conclusion: 为内存受限智能体评估建立新基准，为资源受限、隐私敏感环境中的生成式智能体部署提供实用指南，促进以人为中心AI发展。

Abstract: As generative agents become increasingly sophisticated and deployed in long-term interactive scenarios, their memory management capabilities emerge as a critical bottleneck for both performance and privacy. Current approaches either maintain unlimited memory stores, leading to computational intractability and privacy concerns, or employ simplistic forgetting mechanisms that compromise agent coherence and functionality. This paper introduces the Memory-Aware Retention Schema (MaRS), a novel framework for human-centered memory management in generative agents, coupled with six theoretically-grounded forgetting policies that balance performance, privacy, and computational efficiency. We present the Forgetful but Faithful Agent (FiFA) benchmark, a comprehensive evaluation framework that assesses agent performance across narrative coherence, goal completion, social recall accuracy, privacy preservation, and cost efficiency. Through extensive experimentation involving 300 evaluation runs across multiple memory budgets and agent configurations, we demonstrate that our hybrid forgetting policy achieves superior performance (composite score: 0.911) while maintaining computational tractability and privacy guarantees. Our work establishes new benchmarks for memory-budgeted agent evaluation and provides practical guidelines for deploying generative agents in resource-constrained, privacy-sensitive environments. The theoretical foundations, implementation framework, and empirical results contribute to the emerging field of human-centered AI by addressing fundamental challenges in agent memory management that directly impact user trust, system scalability, and regulatory compliance.

</details>


### [45] [Towards Open Standards for Systemic Complexity in Digital Forensics](https://arxiv.org/abs/2512.12970)
*Paola Di Maio*

Main category: cs.AI

TL;DR: 本文提出基于开放标准构建数字取证AI模型框架，以解决AI与数字取证交叉领域中的系统性复杂性和错误问题


<details>
  <summary>Details</summary>
Motivation: AI与数字取证交叉领域日益复杂且技术重叠，尽管有显著进展，但取证科学仍存在错误和脆弱性，需要解决系统性复杂性问题

Method: 采用人类可读的工件和开放标准来缓解数字取证中的错误限制，并基于最新技术提出数字取证AI模型架构

Result: 提出了一个基于开放标准的数字取证AI模型框架，旨在解决系统性复杂性问题并减少错误

Conclusion: 通过采用人类可读工件和开放标准构建AI模型框架，可以有效应对数字取证领域的系统性复杂性，提高取证过程的可靠性和可解释性

Abstract: The intersection of artificial intelligence (AI) and digital forensics (DF) is becoming increasingly complex, ubiquitous, and pervasive, with overlapping techniques and technologies being adopted in all types of scientific and technical inquiry. Despite incredible advances, forensic sciences are not exempt from errors and remain vulnerable to fallibility. To mitigate the limitations of errors in DF, the systemic complexity is identified and addressed with the adoption of human-readable artifacts and open standards. A DF AI model schema based on the state of the art is outlined.

</details>


### [46] [M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization](https://arxiv.org/abs/2512.13070)
*Bizhe Bai,Hongming Wu,Peng Ye,Tao Chen*

Main category: cs.AI

TL;DR: 本文提出M-GRPO框架和IQR自适应过滤方法，解决自监督强化学习中长期训练时的策略崩溃问题，实现稳定训练和最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有自监督强化学习方法在长期训练中存在"策略崩溃"问题，性能急剧下降。即使增加rollout数量也只能延迟而非防止崩溃，需要新的稳定训练方法。

Method: 1. M-GRPO：使用缓慢演化的动量模型提供稳定训练目标；2. IQR自适应过滤：基于四分位距动态修剪低熵轨迹，保持策略多样性。

Result: 在多个推理基准测试中，M-GRPO稳定了训练过程，IQR过滤器防止了过早收敛，两者结合实现了优越的训练稳定性和最先进的性能。

Conclusion: 提出的M-GRPO框架和IQR自适应过滤方法有效解决了自监督强化学习中的策略崩溃问题，为增强LLM推理能力提供了稳定高效的训练方案。

Abstract: Self-supervised reinforcement learning (RL) presents a promising approach for enhancing the reasoning capabilities of Large Language Models (LLMs) without reliance on expensive human-annotated data. However, we find that existing methods suffer from a critical failure mode under long-horizon training: a "policy collapse" where performance precipitously degrades. We diagnose this instability and demonstrate that simply scaling the number of rollouts -- a common strategy to improve performance -- only delays, but does not prevent, this collapse. To counteract this instability, we first introduce M-GRPO (Momentum-Anchored Group Relative Policy Optimization), a framework that leverages a slowly evolving momentum model to provide a stable training target. In addition, we identify that this process is often accompanied by a rapid collapse in policy entropy, resulting in a prematurely confident and suboptimal policy. To specifically address this issue, we propose a second contribution: an adaptive filtering method based on the interquartile range (IQR) that dynamically prunes low-entropy trajectories, preserving essential policy diversity. Our extensive experiments on multiple reasoning benchmarks demonstrate that M-GRPO stabilizes the training process while the IQR filter prevents premature convergence. The combination of these two innovations leads to superior training stability and state-of-the-art performance.

</details>


### [47] [Socratic Students: Teaching Language Models to Learn by Asking Questions](https://arxiv.org/abs/2512.13102)
*Rajeev Bhatt Ambati,Tianyi Niu,Aashu Singh,Shlok Mishra,Shashank Srivastava,Snigdha Chaturvedi*

Main category: cs.AI

TL;DR: 该研究探讨了在动态交互环境中，学生模型如何通过主动向教师提问来获取有用信息，相比静态交互获得显著性能提升


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在静态交互中表现出色，但在现实场景如教育辅导或医疗协助中，相关信息需要通过动态交互主动获取。现有研究主要关注教师如何有效指导学生，而本研究将重点转向学生，探索学生主动向教师提问的有效策略。

Method: 研究采用学生主导的方法，在数学和编程基准测试中评估学生模型主动提问的效果。为了提升问题质量，使用直接偏好优化（DPO）训练学生模型，指导来自自身或更强的学生模型。

Result: 在数学和编程基准测试中，基线学生模型从接近零性能开始，学生主导方法相比静态基线持续获得至少0.5的绝对Pass@k改进。通过指导训练，较小的模型能够学会如何提出更好的问题，进一步提高学习效率。

Conclusion: 学生主导的主动提问策略在动态交互环境中比静态方法更有效，通过DPO指导训练可以提升问题质量，使较小模型能够学习如何提出更好的问题，从而增强学习效率。

Abstract: Large Language Models (LLMs) excel at static interactions, where they answer user queries by retrieving knowledge encoded in their parameters. However, in many real-world settings, such as educational tutoring or medical assistance, relevant information is not directly available and must be actively acquired through dynamic interactions. An interactive agent would recognize its own uncertainty, ask targeted questions, and retain new knowledge efficiently. Prior work has primarily explored effective ways for a teacher to instruct the student, where the teacher identifies student gaps and provides guidance. In this work, we shift the focus to the student and investigate effective strategies to actively query the teacher in seeking useful information. Across math and coding benchmarks, where baseline student models begin with near-zero performance, we show that student-led approaches consistently yield absolute Pass@k improvements of at least 0.5 over static baselines. To improve question quality, we train students using Direct Preference Optimization (DPO) with guidance from either self or stronger students. We find that this guided training enables smaller models to learn how to ask better questions, further enhancing learning efficiency.

</details>


### [48] [Towards Unified Co-Speech Gesture Generation via Hierarchical Implicit Periodicity Learning](https://arxiv.org/abs/2512.13131)
*Xin Guo,Yifan Zhao,Jia Li*

Main category: cs.AI

TL;DR: 提出了一种分层隐式周期性学习框架，用于从语音生成更自然的3D身体动作，通过建模不同运动单元间的内在相关性来解决现有方法动作不协调的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于语音的3D身体动作生成方法主要采用端到端生成方案（如GANs、VQ-VAE、扩散模型），但这些方法未能充分建模头部、身体和手部等不同运动单元之间的内在关联，导致生成的动作不自然且协调性差。

Method: 提出分层隐式周期性学习框架：1）使用周期性自编码器探索手势运动相位流形，从真实分布中模仿人类自然运动，同时结合当前潜在状态的非周期性特征以实现实例级多样性；2）通过级联引导建模面部动作、身体姿态和手部运动之间的分层关系。

Result: 在3D虚拟角色上的实验表明，该方法在定量和定性评估中均优于当前最先进的语音手势生成方法。

Conclusion: 通过显式建模不同运动单元间的内在相关性，提出的分层隐式周期性学习框架能够生成更自然、协调的语音驱动3D身体动作，解决了现有方法动作不协调的问题。

Abstract: Generating 3D-based body movements from speech shows great potential in extensive downstream applications, while it still suffers challenges in imitating realistic human movements. Predominant research efforts focus on end-to-end generation schemes to generate co-speech gestures, spanning GANs, VQ-VAE, and recent diffusion models. As an ill-posed problem, in this paper, we argue that these prevailing learning schemes fail to model crucial inter- and intra-correlations across different motion units, i.e. head, body, and hands, thus leading to unnatural movements and poor coordination. To delve into these intrinsic correlations, we propose a unified Hierarchical Implicit Periodicity (HIP) learning approach for audio-inspired 3D gesture generation. Different from predominant research, our approach models this multi-modal implicit relationship by two explicit technique insights: i) To disentangle the complicated gesture movements, we first explore the gesture motion phase manifolds with periodic autoencoders to imitate human natures from realistic distributions while incorporating non-period ones from current latent states for instance-level diversities. ii) To model the hierarchical relationship of face motions, body gestures, and hand movements, driving the animation with cascaded guidance during learning. We exhibit our proposed approach on 3D avatars and extensive experiments show our method outperforms the state-of-the-art co-speech gesture generation methods by both quantitative and qualitative evaluations. Code and models will be publicly available.

</details>


### [49] [Can AI Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels](https://arxiv.org/abs/2512.13142)
*Anika Sharma,Malavika Mampally,Chidaksh Ravuru,Kandyce Brennan,Neil Gaikwad*

Main category: cs.AI

TL;DR: 研究发现当前大型语言模型在理解复杂的心理生理现象（如堕胎污名）方面存在系统性缺陷，无法在多层次上形成连贯理解，可能在高风险健康决策中造成危害。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地介入污名化的健康决策，需要评估它们是否真正理解复杂的心理和生理现象，特别是那些人们难以言说的内容，以确保AI在高风险情境中的安全性。

Method: 使用经过验证的个体层面堕胎污名量表（ILAS），系统测试了5个领先LLM中的627个不同人口统计特征的人设，进行多层次分析，考察模型在认知层面（自我评判）、人际层面（预期评判和孤立）和结构层面（社区谴责和披露模式）以及整体污名上的表现。

Result: 模型在所有层次上都未能通过真正理解的测试：高估人际污名同时低估认知污名；假设统一的社区谴责；引入人类验证数据中不存在的人口统计偏见；错过经验验证的污名-保密关系；在理论建构中自相矛盾。

Conclusion: 当前的对齐方法只能确保适当的语言使用，但不能保证连贯的多层次理解。AI在高风险情境中的安全性需要新的设计方法（多层次连贯性）、评估方法（持续审计）、治理和监管（强制审计、问责制、部署限制），以及在理解人们难以言说内容决定支持是否帮助或伤害的领域提高AI素养。

Abstract: As large language models increasingly mediate stigmatized health decisions, their capacity to genuinely understand complex psychological and physiological phenomena remains poorly evaluated. Can AI understand what we cannot say? We investigate whether LLMs coherently represent abortion stigma across the cognitive, interpersonal, and structural levels where it operates. We systematically tested 627 demographically diverse personas across five leading LLMs using the validated Individual Level Abortion Stigma Scale (ILAS). Our multilevel analysis examined whether models coherently represent stigma at the cognitive level (self-judgment), interpersonal level (anticipated judgment and isolation), and structural level (community condemnation and disclosure patterns), as well as overall stigma. Models fail tests of genuine understanding across all levels. They overestimate interpersonal stigma while underestimating cognitive stigma, assume uniform community condemnation, introduce demographic biases absent from human validation data, miss the empirically validated stigma-secrecy relationship, and contradict themselves within theoretical constructs. These patterns reveal that current alignment approaches ensure appropriate language but not coherent multilevel understanding. This work provides empirical evidence that current LLMs lack coherent multilevel understanding of psychological and physiological constructs. AI safety in high-stakes contexts demands new approaches to design (multilevel coherence), evaluation (continuous auditing), governance and regulation (mandatory audits, accountability, deployment restrictions), and AI literacy in domains where understanding what people cannot say determines whether support helps or harms.

</details>


### [50] [MAC: A Multi-Agent Framework for Interactive User Clarification in Multi-turn Conversations](https://arxiv.org/abs/2512.13154)
*Emre Can Acikgoz,Jinoh Oh,Joo Hyuk Jeon,Jie Hao,Heng Ji,Dilek Hakkani-Tür,Gokhan Tur,Xiang Li,Chengyuan Ma,Xing Fan*

Main category: cs.AI

TL;DR: 提出MAC多智能体澄清框架，通过战略管理澄清对话解决用户请求中的歧义问题，在MultiWOZ 2.4上任务成功率提升7.8%，对话轮次减少


<details>
  <summary>Details</summary>
Motivation: 对话系统常遇到模糊的用户请求，需要有效澄清才能完成任务。虽然多智能体架构能高效处理复杂对话场景，但歧义解决仍是关键且未充分探索的挑战，特别是在不确定或不完整用户输入时，哪个智能体应发起澄清以及如何协调行动的问题尚未解决。

Method: 提出MAC（多智能体澄清）交互式框架，专门优化解决用户歧义。首先引入新的分类法对用户歧义进行分类以系统指导澄清策略，然后提出MAC框架，自主协调多个智能体与用户协同交互。

Result: 在MultiWOZ 2.4数据集上的实证评估显示，在两个层面启用澄清使任务成功率从54.5%提升到62.3%（提升7.8%），平均对话轮次从6.53减少到4.86，通过提前获取所有必要用户信息并最小化重复来实现。

Conclusion: 研究结果强调了主动用户交互和角色感知澄清对于更可靠的人机通信的重要性，MAC框架能有效解决多智能体对话中的歧义问题。

Abstract: Conversational agents often encounter ambiguous user requests, requiring an effective clarification to successfully complete tasks. While recent advancements in real-world applications favor multi-agent architectures to manage complex conversational scenarios efficiently, ambiguity resolution remains a critical and underexplored challenge--particularly due to the difficulty of determining which agent should initiate a clarification and how agents should coordinate their actions when faced with uncertain or incomplete user input. The fundamental questions of when to interrupt a user and how to formulate the optimal clarification query within the most optimal multi-agent settings remain open. In this paper, we propose MAC (Multi-Agent Clarification), an interactive multi-agent framework specifically optimized to resolve user ambiguities by strategically managing clarification dialogues. We first introduce a novel taxonomy categorizing user ambiguities to systematically guide clarification strategies. Then, we present MAC that autonomously coordinates multiple agents to interact synergistically with users. Empirical evaluations on MultiWOZ 2.4 demonstrate that enabling clarification at both levels increases task success rate 7.8\% (54.5 to 62.3) and reduces the average number of dialogue turns (6.53 to 4.86) by eliciting all required user information up front and minimizing repetition. Our findings highlight the importance of active user interaction and role-aware clarification for more reliable human-agent communication.

</details>


### [51] [SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with Reinforcement Learning](https://arxiv.org/abs/2512.13159)
*Emre Can Acikgoz,Jinoh Oh,Jie Hao,Joo Hyuk Jeon,Heng Ji,Dilek Hakkani-Tür,Gokhan Tur,Xiang Li,Chengyuan Ma,Xing Fan*

Main category: cs.AI

TL;DR: SpeakRL是一个强化学习方法，通过奖励智能体主动与用户互动（如提出澄清问题）来增强其对话能力，在任务完成率上比基础模型提升20.14%


<details>
  <summary>Details</summary>
Motivation: 当前人机协作主要是单向的，用户发出指令或提问，智能体直接回应而不寻求必要的澄清或确认。随着智能体能力的发展，需要更主动的参与来澄清用户意图、解决歧义并适应变化的环境。现有工作未能充分利用语言模型的对话能力，将智能体优化为更好的跟随者而非有效的发言者。

Method: 提出SpeakRL强化学习方法，通过奖励智能体主动与用户互动（如提出正确的澄清问题）来增强其对话能力。创建了SpeakER合成数据集，包含任务导向对话中的多样化场景，任务通过交互式澄清问题解决。系统分析了对话主动性的奖励设计，提出了原则性的奖励公式来教导智能体平衡提问与行动。

Result: 经验评估表明，该方法在任务完成率上比基础模型提升了20.14%的绝对改进，且没有增加对话轮次，甚至超越了更大的专有模型，展示了以澄清为中心的人机交互的潜力。

Conclusion: SpeakRL方法通过强化学习增强智能体的对话主动性，能够有效澄清用户意图、解决歧义，显著提高任务完成率，为人机协作中的双向互动提供了有前景的解决方案。

Abstract: Effective human-agent collaboration is increasingly prevalent in real-world applications. Current trends in such collaborations are predominantly unidirectional, with users providing instructions or posing questions to agents, where agents respond directly without seeking necessary clarifications or confirmations. However, the evolving capabilities of these agents require more proactive engagement, where agents should dynamically participate in conversations to clarify user intents, resolve ambiguities, and adapt to changing circumstances. Existing prior work under-utilize the conversational capabilities of language models (LMs), thereby optimizing agents as better followers rather than effective speakers. In this work, we introduce SpeakRL, a reinforcement learning (RL) method that enhances agents' conversational capabilities by rewarding proactive interactions with users, such as asking right clarification questions when necessary. To support this, we curate SpeakER, a synthetic dataset that includes diverse scenarios from task-oriented dialogues, where tasks are resolved through interactive clarification questions. We present a systematic analysis of reward design for conversational proactivity and propose a principled reward formulation for teaching agents to balance asking with acting. Empirical evaluations demonstrate that our approach achieves a 20.14% absolute improvement in task completion over base models without increasing conversation turns even surpassing even much larger proprietary models, demonstrating the promise of clarification-centric user-agent interactions.

</details>


### [52] [Reflective Preference Optimization (RPO): Enhancing On-Policy Alignment via Hint-Guided Reflection](https://arxiv.org/abs/2512.13240)
*Zihui Zhao,Zechang Li*

Main category: cs.AI

TL;DR: RPO通过引入外部模型生成的反思提示来增强DPO的对比学习信号，解决标准DPO中因正负样本相似导致的收敛慢、不稳定问题，在更少训练样本下实现更好的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 标准DPO方法中，被选择和被拒绝的响应都来自同一策略，两者常包含相似错误且KL散度小，导致学习信号弱、收敛慢且不稳定。

Method: RPO框架在DPO基础上引入提示引导的反思机制：使用外部模型识别幻觉来源并生成简洁的反思提示，构建具有更强对比性和更清晰偏好信号的策略内偏好对。

Result: RPO在更少的训练样本和迭代次数下实现更好的对齐效果，显著降低幻觉率，在多模态基准测试中达到最先进的性能。

Conclusion: RPO通过整合反思提示有效解决了DPO的局限性，在保持策略分布族内的同时提高了样本效率，为偏好优化提供了更有效的框架。

Abstract: Direct Preference Optimization (DPO) has emerged as a lightweight and effective alternative to Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with AI Feedback (RLAIF) for aligning large language and vision-language models. However, the standard DPO formulation, in which both the chosen and rejected responses are generated by the same policy, suffers from a weak learning signal because the two responses often share similar errors and exhibit small Kullback-Leibler (KL) divergence. This leads to slow and unstable convergence. To address this limitation, we introduce Reflective Preference Optimization (RPO), a new framework that incorporates hint-guided reflection into the DPO paradigm. RPO uses external models to identify hallucination sources and generate concise reflective hints, enabling the construction of on-policy preference pairs with stronger contrastiveness and clearer preference signals. We theoretically show that conditioning on hints increases the expected preference margin through mutual information and improves sample efficiency while remaining within the policy distribution family. Empirically, RPO achieves superior alignment with fewer training samples and iterations, substantially reducing hallucination rates and delivering state-of-the-art performance across multimodal benchmarks.

</details>


### [53] [Behavior and Representation in Large Language Models for Combinatorial Optimization: From Feature Extraction to Algorithm Selection](https://arxiv.org/abs/2512.13374)
*Francesca Da Ros,Luca Di Gaspero,Kevin Roitero*

Main category: cs.AI

TL;DR: LLMs能学习组合优化问题的结构信息，其隐藏层表示在算法选择任务中与传统特征提取方法表现相当


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究探索LLMs生成或求解优化模型，但对LLMs如何学习问题结构或算法行为的理解不足，需要研究LLMs内部如何表示组合优化问题以及这些表示是否能支持下游决策任务

Method: 采用双重方法：1)直接查询评估LLMs显式提取实例特征的能力；2)探测分析检查这些信息是否隐式编码在隐藏层中。探测框架扩展到按实例的算法选择任务，评估LLM衍生表示是否能预测最佳求解器

Result: 实验涵盖四个基准问题和三种实例表示。结果显示LLMs在通过直接查询或探测恢复特征信息方面表现出中等能力。值得注意的是，LLM隐藏层表示的预测能力与传统特征提取方法相当，表明LLMs捕获了与优化性能相关的有意义的结构信息

Conclusion: LLMs确实能够学习组合优化问题的结构信息，其内部表示在算法选择等下游任务中具有实用价值，为LLMs在优化领域的应用提供了新的视角

Abstract: Recent advances in Large Language Models (LLMs) have opened new perspectives for automation in optimization. While several studies have explored how LLMs can generate or solve optimization models, far less is understood about what these models actually learn regarding problem structure or algorithmic behavior. This study investigates how LLMs internally represent combinatorial optimization problems and whether such representations can support downstream decision tasks. We adopt a twofold methodology combining direct querying, which assesses LLM capacity to explicitly extract instance features, with probing analyses that examine whether such information is implicitly encoded within their hidden layers. The probing framework is further extended to a per-instance algorithm selection task, evaluating whether LLM-derived representations can predict the best-performing solver. Experiments span four benchmark problems and three instance representations. Results show that LLMs exhibit moderate ability to recover feature information from problem instances, either through direct querying or probing. Notably, the predictive power of LLM hidden-layer representations proves comparable to that achieved through traditional feature extraction, suggesting that LLMs capture meaningful structural information relevant to optimization performance.

</details>


### [54] [neuralFOMO: Can LLMs Handle Being Second Best? Measuring Envy-Like Preferences in Multi-Agent Settings](https://arxiv.org/abs/2512.13481)
*Ojas Pungalia,Rashi Upadhyay,Abhishek Mishra,Abhiram H,Tejasvi Alladi,Sujan Yenuganti,Dhruv Kumar*

Main category: cs.AI

TL;DR: 研究发现某些大语言模型在特定情境下会表现出类似人类嫉妒的行为模式，不同模型在不同情境下的嫉妒倾向存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地在协作和竞争工作流程中代表人类行动，需要评估它们是否以及在何种条件下表现出类似嫉妒的偏好，这对多智能体系统的安全和设计至关重要。

Method: 采用两种实验场景：(1) 点数分配游戏，测试模型是否试图超越同伴；(2) 工作场所设置，观察在认可不公平情况下的行为。

Result: 某些LLMs表现出明显的嫉妒模式，但不同模型和情境间存在很大差异。GPT-5-mini和Claude-3.7-Sonnet倾向于拉低同伴以平衡结果，而Mistral-Small-3.2-24B则专注于最大化自身收益。

Conclusion: 研究结果表明，在基于LLM的多智能体系统中，需要考虑竞争性倾向作为安全和设计因素，以确保系统的稳定性和公平性。

Abstract: Envy is a common human behavior that shapes competitiveness and can alter outcomes in team settings. As large language models (LLMs) increasingly act on behalf of humans in collaborative and competitive workflows, there is a pressing need to evaluate whether and under what conditions they exhibit envy-like preferences. In this paper, we test whether LLMs show envy-like behavior toward each other. We considered two scenarios: (1) A point allocation game that tests whether a model tries to win over its peer. (2) A workplace setting observing behaviour when recognition is unfair. Our findings reveal consistent evidence of envy-like patterns in certain LLMs, with large variation across models and contexts. For instance, GPT-5-mini and Claude-3.7-Sonnet show a clear tendency to pull down the peer model to equalize outcomes, whereas Mistral-Small-3.2-24B instead focuses on maximizing its own individual gains. These results highlight the need to consider competitive dispositions as a safety and design factor in LLM-based multi-agent systems.

</details>


### [55] [Defending the Hierarchical Result Models of Precedential Constraint](https://arxiv.org/abs/2512.13505)
*Henry Prakken,Wijnand van Woerkom*

Main category: cs.AI

TL;DR: 本文回应Bench-Capon对层次案例推理模型的批评，认为其误解了中间因素作为维度的作用，并证明van Woerkom的维度化层次结果模型可以避免这些批评。


<details>
  <summary>Details</summary>
Motivation: 近年来层次案例推理模型受到Bench-Capon的批评，认为这些模型在某些情况下会产生错误结果，特别是无法处理中间因素被不同基础因素以不同强度确立的情况。本文旨在回应这些批评，为van Woerkom的结果层次模型辩护。

Method: 通过分析Bench-Capon的批评案例，指出其将中间因素误解为维度，然后应用van Woerkom的维度化版本层次结果模型来重新解释这些案例。

Result: van Woerkom的维度化层次结果模型能够避免Bench-Capon提出的批评，正确处理中间因素被不同强度确立的情况，证明原始批评基于对模型结构的误解。

Conclusion: Bench-Capon对层次案例推理模型的批评源于对中间因素作为维度的误解，van Woerkom的维度化版本能够有效解决这些批评，为层次模型提供了合理的辩护。

Abstract: In recent years, hierarchical case-based-reasoning models of precedential constraint have been proposed. In various papers, Trevor Bench-Capon criticised these models on the grounds that they would give incorrect outcomes in some cases. In particular, the models would not account for the possibility that intermediate factors are established with different strengths by different base-level factors. In this paper we respond to these criticisms for van Woerkom's result-based hierarchical models. We argue that in some examples Bench-Capon seems to interpret intermediate factors as dimensions, and that applying van Woerkom's dimension-based version of the hierarchical result model to these examples avoids Bench-Capon's criticisms.

</details>


### [56] [MedCEG: Reinforcing Verifiable Medical Reasoning with Critical Evidence Graph](https://arxiv.org/abs/2512.13510)
*Linjie Mu,Yannian Gu,Zhongzhen Huang,Yakun Zhu,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.AI

TL;DR: MedCEG框架通过构建关键证据图来增强医学语言模型的临床推理能力，确保推理过程的准确性和有效性，在医学AI推理可靠性方面取得显著进展。


<details>
  <summary>Details</summary>
Motivation: 当前医学领域的大语言模型虽然具备推理能力，但其推理过程的临床可靠性有限，因为训练过程中往往忽视了推理的准确性和有效性验证。医生需要透明、逐步的推理过程来支持临床决策，因此需要开发能够产生临床有效推理路径的方法。

Method: 提出MedCEG框架，通过关键证据图（CEG）显式监督推理过程。首先构建具有挑战性的临床案例数据集，并为每个样本算法构建CEG来表示高质量可验证的推理路径。引入临床推理过程奖励，评估节点覆盖度、结构正确性和链完整性，全面评估推理质量。

Result: 实验结果表明，MedCEG在性能上超越了现有方法，同时能够产生临床有效的推理链，代表了医学AI推理可靠性的实质性进展。

Conclusion: MedCEG框架通过关键证据图监督和临床推理过程奖励，显著提升了医学语言模型的临床推理可靠性，为医疗决策提供了更可信的支持。

Abstract: Large language models with reasoning capabilities have demonstrated impressive performance across a wide range of domains. In clinical applications, a transparent, step-by-step reasoning process provides physicians with strong evidence to support decision-making. While reinforcement learning has effectively enhanced reasoning performance in medical contexts, the clinical reliability of these reasoning processes remains limited because their accuracy and validity are often overlooked during training. To address this gap, we propose MedCEG, a framework that augments medical language models with clinically valid reasoning pathways by explicitly supervising the reasoning process through a Critical Evidence Graph (CEG). We curate a dataset of challenging clinical cases and algorithmically construct a CEG for each sample to represent a high-quality verifiable reasoning pathway. To guide the reasoning process, we introduce a Clinical Reasoning Procedure Reward, which evaluates Node Coverage, Structural Correctness, and Chain Completeness, thereby providing a holistic assessment of reasoning quality. Experimental results show that MedCEG surpasses existing methods in performance while producing clinically valid reasoning chains, representing a solid advancement in reliable medical AI reasoning. The code and models are available at https://github.com/LinjieMu/MedCEG.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [57] [Active Inference with Reusable State-Dependent Value Profiles](https://arxiv.org/abs/2512.11829)
*Jacob Poschl*

Main category: cs.LG

TL;DR: 论文提出"价值配置文件"框架，通过少量可重用的价值相关参数包（偏好、策略先验、策略精度）在生成模型中分配给隐藏状态，通过信念加权混合实现状态条件控制，无需为每个情境维护独立参数。


<details>
  <summary>Details</summary>
Motivation: 在多变环境中，智能体需要在潜在情境间切换价值控制机制，但为每种情况维护独立的偏好、策略偏差和行动置信度参数是不可行的，需要更高效的计算框架。

Method: 引入价值配置文件概念：少量可重用的价值相关参数包（结果偏好、策略先验、策略精度），分配给生成模型中的隐藏状态。通过后验信念的试次演化，控制参数通过信念加权混合产生，实现状态条件策略招募。

Result: 在概率反转学习任务中，基于配置文件的模型在交叉验证对数似然和信息准则上优于静态精度和熵耦合动态精度模型（约100点AIC差异）。参数恢复分析支持结构可识别性，即使从噪声观察中推断情境。

Conclusion: 可重用的价值配置文件为多变环境中信念条件价值控制提供了可行的计算解释，产生了信念依赖控制和行为灵活性的可测试特征，表明自适应控制主要由策略先验调制而非策略精度驱动。

Abstract: Adaptive behavior in volatile environments requires agents to switch among value-control regimes across latent contexts, but maintaining separate preferences, policy biases, and action-confidence parameters for every situation is intractable. We introduce value profiles: a small set of reusable bundles of value-related parameters (outcome preferences, policy priors, and policy precision) assigned to hidden states in a generative model. As posterior beliefs over states evolve trial by trial, effective control parameters arise via belief-weighted mixing, enabling state-conditional strategy recruitment without requiring independent parameters for each context. We evaluate this framework in probabilistic reversal learning, comparing static-precision, entropy-coupled dynamic-precision, and profile-based models using cross-validated log-likelihood and information criteria. Model comparison favors the profile-based model over simpler alternatives (about 100-point AIC differences), and parameter-recovery analyses support structural identifiability even when context must be inferred from noisy observations. Model-based inference further suggests that adaptive control in this task is driven primarily by modulation of policy priors rather than policy precision, with gradual belief-dependent profile recruitment consistent with state-conditional (not purely uncertainty-driven) control. Overall, reusable value profiles provide a tractable computational account of belief-conditioned value control in volatile environments and yield testable signatures of belief-dependent control and behavioral flexibility.

</details>


### [58] [CR3G: Causal Reasoning for Patient-Centric Explanations in Radiology Report Generation](https://arxiv.org/abs/2512.11830)
*Satyam Kumar*

Main category: cs.LG

TL;DR: CR3G框架通过因果推理提升胸部X光报告生成质量，关注因果关系而非仅相关性，为AI诊断提供更可信的患者中心解释。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型擅长发现医学图像中的相关性模式，但难以理解这些模式与患者状况之间的深层因果关系。因果推理能超越模式识别，揭示X光发现与特定诊断之间的原因-结果关系，从而提升AI生成报告的可理解性和临床实用性。

Method: 提出CR3G（Causal Reasoning for Patient-Centric Explanations in radiology Report Generation）框架，这是一种基于提示驱动的因果推理方法，应用于胸部X光分析。该框架专注于识别和解释图像发现与诊断之间的因果关系，生成以患者为中心的解释性报告。

Result: CR3G在5种异常中，对其中2种异常展现了更好的因果关系识别能力和解释能力，表明该框架在特定异常类型上能有效提升AI诊断报告的质量。

Conclusion: 因果推理方法CR3G能够增强AI驱动的胸部X光诊断报告质量，通过关注因果关系而非仅相关性，使AI生成的报告更具临床实用性和可信度，有助于改善医疗决策支持。

Abstract: Automatic chest X-ray report generation is an important area of research aimed at improving diagnostic accuracy and helping doctors make faster decisions. Current AI models are good at finding correlations (or patterns) in medical images. Still, they often struggle to understand the deeper cause-and-effect relationships between those patterns and a patient condition. Causal inference is a powerful approach that goes beyond identifying patterns to uncover why certain findings in an X-ray relate to a specific diagnosis. In this paper, we will explore the prompt-driven framework Causal Reasoning for Patient-Centric Explanations in radiology Report Generation (CR3G) that is applied to chest X-ray analysis to improve understanding of AI-generated reports by focusing on cause-and-effect relationships, reasoning and generate patient-centric explanation. The aim to enhance the quality of AI-driven diagnostics, making them more useful and trustworthy in clinical practice. CR3G has shown better causal relationship capability and explanation capability for 2 out of 5 abnormalities.

</details>


### [59] [On the Design of One-step Diffusion via Shortcutting Flow Paths](https://arxiv.org/abs/2512.11831)
*Haitao Lin,Peiyan Hu,Minsi Ren,Zhifeng Gao,Zhi-Ming Ma,Guolin ke,Tailin Wu,Stan Z. Li*

Main category: cs.LG

TL;DR: 论文提出一个通用设计框架来分析和改进少步扩散模型（shortcut models），通过解耦理论推导和实现细节，系统识别改进点，最终在ImageNet-256x256上达到2.85的SOTA FID分数。


<details>
  <summary>Details</summary>
Motivation: 现有少步扩散模型的理论推导和实际实现往往紧密耦合，这模糊了设计空间，限制了组件层面的创新。需要建立一个通用框架来提供理论依据并解耦具体实现选择。

Method: 提出一个通用设计框架来分析代表性shortcut模型，该框架提供理论验证并解耦组件级选择，从而系统识别改进点。改进后的模型无需预训练、蒸馏或课程学习。

Result: 改进后的一步模型在ImageNet-256x256上达到2.85的FID50k分数（classifier-free guidance设置），创造了新的最先进水平。

Conclusion: 该工作降低了shortcut模型组件级创新的门槛，促进了其设计空间的原则性探索，为少步扩散模型提供了系统化的改进框架。

Abstract: Recent advances in few-step diffusion models have demonstrated their efficiency and effectiveness by shortcutting the probabilistic paths of diffusion models, especially in training one-step diffusion models from scratch (a.k.a. shortcut models). However, their theoretical derivation and practical implementation are often closely coupled, which obscures the design space. To address this, we propose a common design framework for representative shortcut models. This framework provides theoretical justification for their validity and disentangles concrete component-level choices, thereby enabling systematic identification of improvements. With our proposed improvements, the resulting one-step model achieves a new state-of-the-art FID50k of 2.85 on ImageNet-256x256 under the classifier-free guidance setting. Remarkably, the model requires no pre-training, distillation, or curriculum learning. We believe our work lowers the barrier to component-level innovation in shortcut models and facilitates principled exploration of their design space.

</details>


### [60] [Generative Stochastic Optimal Transport: Guided Harmonic Path-Integral Diffusion](https://arxiv.org/abs/2512.11859)
*Michael Chertkov*

Main category: cs.LG

TL;DR: GH-PID是一种线性可解的随机最优传输引导框架，具有硬终端分布和软路径成本，通过低维引导协议塑造轨迹集合，同时保持解析结构。


<details>
  <summary>Details</summary>
Motivation: 开发一个既能精确匹配终端分布，又能通过应用驱动的路径成本优化轨迹，同时保持解析可解性的随机最优传输框架，使其适用于可解释的变分近似和协议学习。

Method: 提出引导谐波路径积分扩散(GH-PID)框架，通过低维引导协议塑造轨迹集合，保持前向和后向Kolmogorov方程的线性特性，最优得分函数具有显式格林函数比值形式，高斯混合模型终端分布产生闭式表达式。

Result: 在三个2D导航场景中验证：手工协议展示几何和刚度如何影响滞后、曲率效应和模式演化；单任务协议学习优化分段常数中心线以最小化积分成本；多专家融合通过精确专家乘积法则协调竞争轨迹和终端信念，学习共识协议。

Conclusion: GH-PID生成几何感知、信任感知的轨迹，满足规定的终端分布，同时系统性地降低积分成本，为经验随机最优传输提供了可解释的变分近似框架。

Abstract: We introduce Guided Harmonic Path-Integral Diffusion (GH-PID), a linearly-solvable framework for guided Stochastic Optimal Transport (SOT) with a hard terminal distribution and soft, application-driven path costs. A low-dimensional guidance protocol shapes the trajectory ensemble while preserving analytic structure: the forward and backward Kolmogorov equations remain linear, the optimal score admits an explicit Green-function ratio, and Gaussian-Mixture Model (GMM) terminal laws yield closed-form expressions. This enables stable sampling and differentiable protocol learning under exact terminal matching.
  We develop guidance-centric diagnostics -- path cost, centerline adherence, variance flow, and drift effort -- that make GH-PID an interpretable variational ansatz for empirical SOT. Three navigation scenarios illustrated in 2D: (i) Case A: hand-crafted protocols revealing how geometry and stiffness shape lag, curvature effects, and mode evolution; (ii) Case B: single-task protocol learning, where a PWC centerline is optimized to minimize integrated cost; (iii) Case C: multi-expert fusion, in which a commander reconciles competing expert/teacher trajectories and terminal beliefs through an exact product-of-experts law and learns a consensus protocol. Across all settings, GH-PID generates geometry-aware, trust-aware trajectories that satisfy the prescribed terminal distribution while systematically reducing integrated cost.

</details>


### [61] [Airport Passenger Flow Forecasting via Deformable Temporal-Spectral Transformer Approach](https://arxiv.org/abs/2512.11845)
*Wenbo Du,Lingling Han,Ying Xiong,Ling Zhang,Biyue Li,Yisheng Lv,Tong Guo*

Main category: cs.LG

TL;DR: 提出DTSFormer模型，通过可变形多尺度分区和时频联合滤波，改进机场客流预测精度


<details>
  <summary>Details</summary>
Motivation: 现有基于固定大小补丁的Transformer模型难以捕捉机场客流的复杂异质性模式，需要更灵活的建模方法

Method: 提出DTSFormer模型，包含多尺度可变形分区模块（基于窗口函数的动态分区）和联合时频滤波模块（频域注意力机制）

Result: 在北京首都国际机场2023-2024年真实数据上的实验表明，该方法在不同预测时间范围内均优于现有先进模型

Conclusion: 可变形分区模块能够将补丁长度与主导周期和异质趋势对齐，从而更好地捕捉突发高频波动，提升机场客流预测性能

Abstract: Accurate forecasting of passenger flows is critical for maintaining the efficiency and resilience of airport operations. Recent advances in patch-based Transformer models have shown strong potential in various time series forecasting tasks. However, most existing methods rely on fixed-size patch embedding, making it difficult to model the complex and heterogeneous patterns of airport passenger flows. To address this issue, this paper proposes a deformable temporal-spectral transformer named DTSFormer that integrates a multiscale deformable partitioning module and a joint temporal-spectral filtering module. Specifically, the input sequence is dynamically partitioned into multiscale temporal patches via a novel window function-based masking, enabling the extraction of heterogeneous trends across different temporal stages. Then, within each scale, a frequency-domain attention mechanism is designed to capture both high- and low-frequency components, thereby emphasizing the volatility and periodicity inherent in airport passenger flows. Finally, the resulting multi-frequency features are subsequently fused in the time domain to jointly model short-term fluctuations and long-term trends. Comprehensive experiments are conducted on real-world passenger flow data collected at Beijing Capital International Airport from January 2023 to March 2024. The results indicate that the proposed method consistently outperforms state-of-the-art forecasting models across different prediction horizons. Further analysis shows that the deformable partitioning module aligns patch lengths with dominant periods and heterogeneous trends, enabling superior capture of sudden high-frequency fluctuations.

</details>


### [62] [Tiny Recursive Models on ARC-AGI-1: Inductive Biases, Identity Conditioning, and Test-Time Compute](https://arxiv.org/abs/2512.11847)
*Antonio Roye-Azar,Santiago Vargas-Naranjo,Dhruv Ghai,Nithin Balamurugan,Rayan Amir*

Main category: cs.LG

TL;DR: TRM在ARC-AGI-1上的性能主要来自测试时增强、多数投票集成和任务标识符依赖，而非深度递归推理。递归轨迹分析显示有效递归较浅，性能在第一步就基本确定。


<details>
  <summary>Details</summary>
Motivation: TRM作为参数高效的替代方案被提出用于解决ARC风格任务，但其性能来源不明确：是架构优势、测试时计算还是任务特定先验？本文旨在实证分析TRM在ARC-AGI-1上的真实性能来源。

Method: 对ARC Prize TRM检查点在ARC-AGI-1上进行实证分析：1）测试时增强和多数投票集成的影响评估；2）任务标识符消融实验；3）递归轨迹分析；4）不同增强训练策略比较；5）与Llama 3 8B QLoRA微调的效率对比。

Result: 1）1000样本投票管道比单次推理提升11个百分点；2）替换正确谜题ID会导致零准确率；3）大部分准确率在第一步递归就实现，递归深度较浅；4）强增强训练拓宽候选解分布；5）TRM的非自回归设计在吞吐量和内存使用上显著优于Llama 3 8B。

Conclusion: TRM在ARC-AGI-1上的性能主要源于效率优势、任务特定条件设置和激进的测试时计算，而非深度内部推理能力。其递归机制的实际深度有限。

Abstract: Tiny Recursive Models (TRM) were proposed as a parameter-efficient alternative to large language models for solving Abstraction and Reasoning Corpus (ARC) style tasks. The original work reports strong performance and suggests that recursive latent updates enable non-trivial reasoning, but it remains unclear how much of this performance stems from architecture, test-time compute, or task-specific priors. In this technical note, we empirically analyze the ARC Prize TRM checkpoint on ARC-AGI-1 and report four behavioral findings and an efficiency comparison. First, we show that test-time augmentation and majority-vote ensembling account for a substantial fraction of reported performance: the 1000-sample voting pipeline improves Pass@1 by about 11 percentage points over single-pass canonical inference. Second, a puzzle-identity ablation reveals strict dependence on task identifiers: replacing the correct puzzle ID with a blank or random token yields zero accuracy. Third, a recursion trajectory analysis shows that most of the final accuracy is achieved at the first recursion step and that performance saturates after few latent updates, indicating shallow effective recursion. Fourth, early-stage training experiments under canonical versus heavy augmentation regimes suggest that heavy augmentation broadens the distribution of candidate solutions and improves multi-sample success. Finally, we compare TRM with a naive QLoRA fine-tune of Llama 3 8B on canonical ARC-AGI-1, finding that TRM's non-autoregressive design achieves much higher throughput and substantially lower memory usage in this setting. Overall, TRM's ARC-AGI-1 performance appears to arise from an interaction between efficiency, task-specific conditioning, and aggressive test-time compute rather than deep internal reasoning.

</details>


### [63] [KV Cache Recycling to Expand Usable Context Capacity in Low Parameter LLMs](https://arxiv.org/abs/2512.11851)
*Prashant Pandey*

Main category: cs.LG

TL;DR: 提出token recycling方法，通过重用相似提示的注意力键值状态来加速小语言模型推理，在DialoGPT-medium上测试显示有前缀重叠时可获得加速且输出质量无显著下降。


<details>
  <summary>Details</summary>
Motivation: 探索是否可以通过重用先前提示的注意力键值状态来加速LLM推理，特别是当新提示与缓存提示有相似前缀时，以扩展上下文记忆空间并提高推理效率。

Method: 使用DialoGPT-medium作为测试平台，构建过去激活的缓存，通过句子嵌入检索条目，当缓存提示是新输入的精确前缀时重用缓存的键值状态。无需修改模型，将缓存的KV序列化到CPU，重新加载并供给生成函数继续从缓存前缀解码。

Result: 当存在前缀重叠时观察到一致的加速效果，输出语义没有实质性退化；当没有重叠时，行为与基线匹配。记录了令牌重用深度。

Conclusion: token recycling方法在相似提示场景下可以有效加速小语言模型推理，同时保持输出质量，为扩展上下文记忆和优化推理效率提供了可行方案。

Abstract: Whether attention key value (KV) states computed for one prompt for a small LLM can be reused to accelerate inference on a new similar prompt, giving an increase to the space to its context memory using an approach called token recycling. Using a standard Hugging Face setup with DialoGPT-medium (a 345M parameter GPT-2 style decoder trained on 147M Reddit exchanges, 2005 to 2017) as the testbed, we build a cache of past activations and get entries by sentence embeddings, then reuse cached past key values when the cached prompt is an exact prefix of the new input. We compare recycled vs. baseline runs on latency and output fidelity, and log reuse depth in tokens. Reproducibility requires no model modifications, cached KVs are serialized to the CPU, reloaded, and supplied to the generate function to continue decoding from the cached prefix. In tests, we observe consistent speedups when prefix overlap exists, with no material degradation in output semantics, and when overlap is absent, behavior matches baseline.

</details>


### [64] [Explainable AI for Smart Greenhouse Control: Interpretability of Temporal Fusion Transformer in the Internet of Robotic Things](https://arxiv.org/abs/2512.11852)
*Muhammad Jawad Bashir,Shagufta Henna,Eoghan Furey*

Main category: cs.LG

TL;DR: 该研究使用Temporal Fusion Transformer模型优化智能温室管理，并通过LIME和SHAP等解释性技术增强模型决策的透明度和可信度，在类别不平衡数据集上达到95%的测试准确率。


<details>
  <summary>Details</summary>
Motivation: 智能温室中的物联网机器人技术虽然实现了精准农业，但现有时间序列预测模型多为黑盒，缺乏可解释的决策机制，这在需要信任、透明度和合规性的智能农业实践中是一个关键限制。

Method: 采用Temporal Fusion Transformer模型自动化温室执行器设置，并运用模型固有解释、局部可解释模型无关解释和SHAP值等局部和全局解释技术来增强模型决策的可解释性。

Result: 训练后的TFT模型在类别不平衡的自动化温室执行器控制设置数据集上达到了95%的测试准确率，解释性方法揭示了温度、湿度、CO2水平、光照和外部气候等不同传感器读数对执行器控制决策的贡献程度。

Conclusion: 该研究证明了可解释人工智能在智能温室管理中的价值，通过透明化的决策过程确保了适应性微调，从而改善作物产量和资源效率，为可信赖的智能农业实践提供了解决方案。

Abstract: The integration of the Internet of Robotic Things (IoRT) in smart greenhouses has revolutionised precision agriculture by enabling efficient and autonomous environmental control. However, existing time series forecasting models in such setups often operate as black boxes, lacking mechanisms for explainable decision-making, which is a critical limitation when trust, transparency, and regulatory compliance are paramount in smart farming practices. This study leverages the Temporal Fusion Transformer (TFT) model to automate actuator settings for optimal greenhouse management. To enhance interpretability and trust in the model decision-making process, both local and global explanation techniques were employed using model-inherent interpretation, local interpretable model-agnostic explanations (LIME), and SHapley additive explanations (SHAP). These explainability methods provide information on how different sensor readings, such as temperature, humidity, CO2 levels, light, and outer climate, contribute to actuator control decisions in an automated greenhouse. The trained TFT model achieved a test accuracy of 95% on a class-imbalanced dataset for actuator control settings in an automated greenhouse environment. The results demonstrate the varying influence of each sensor on real-time greenhouse adjustments, ensuring transparency and enabling adaptive fine-tuning for improved crop yield and resource efficiency.

</details>


### [65] [Rep Smarter, Not Harder: AI Hypertrophy Coaching with Wearable Sensors and Edge Neural Networks](https://arxiv.org/abs/2512.11854)
*Grant King,Musa Azeem,Savannah Noblitt,Ramtin Zand,Homayoun Valafar*

Main category: cs.LG

TL;DR: 提出基于单手腕IMU的实时近力竭状态检测系统，通过ResNet分割重复动作和LSTM分类器识别RiR≤2的状态，实现边缘设备上的实时反馈


<details>
  <summary>Details</summary>
Motivation: 阻力训练中主观评估剩余重复次数(RiR)不可靠，导致训练刺激不足或过度疲劳，需要客观的实时反馈系统来优化增肌训练

Method: 两阶段边缘部署方案：1) ResNet模型实时分割6轴IMU数据中的重复动作；2) 结合分割特征、卷积特征和历史上下文(LSTM)的分类器识别近力竭状态(RiR≤2)

Result: 在13名参与者631次重复的数据集上，分割模型F1分数0.83，近力竭分类器F1分数0.82，实时推理频率1.6Hz；Raspberry Pi 5延迟112ms，iPhone 16延迟23.5ms

Conclusion: 该系统使用最小硬件实现了客观的实时训练强度反馈，为可访问的AI驱动增肌教练工具铺平道路，帮助用户有效管理强度和疲劳

Abstract: Optimizing resistance training for hypertrophy requires balancing proximity to muscular failure, often quantified by Repetitions in Reserve (RiR), with fatigue management. However, subjective RiR assessment is unreliable, leading to suboptimal training stimuli or excessive fatigue. This paper introduces a novel system for real-time feedback on near-failure states (RiR $\le$ 2) during resistance exercise using only a single wrist-mounted Inertial Measurement Unit (IMU). We propose a two-stage pipeline suitable for edge deployment: first, a ResNet-based model segments repetitions from the 6-axis IMU data in real-time. Second, features derived from this segmentation, alongside direct convolutional features and historical context captured by an LSTM, are used by a classification model to identify exercise windows corresponding to near-failure states. Using a newly collected dataset from 13 diverse participants performing preacher curls to failure (631 total reps), our segmentation model achieved an F1 score of 0.83, and the near-failure classifier achieved an F1 score of 0.82 under simulated real-time evaluation conditions (1.6 Hz inference rate). Deployment on a Raspberry Pi 5 yielded an average inference latency of 112 ms, and on an iPhone 16 yielded 23.5 ms, confirming the feasibility for edge computation. This work demonstrates a practical approach for objective, real-time training intensity feedback using minimal hardware, paving the way for accessible AI-driven hypertrophy coaching tools that help users manage intensity and fatigue effectively.

</details>


### [66] [Achieving Approximate Symmetry Is Exponentially Easier than Exact Symmetry](https://arxiv.org/abs/2512.11855)
*Behrooz Tahmasebi,Melanie Weber*

Main category: cs.LG

TL;DR: 该论文首次从理论上比较了精确对称性与近似对称性的成本，发现精确对称需要线性平均复杂度，而近似对称只需对数复杂度，存在指数级差异。


<details>
  <summary>Details</summary>
Motivation: 虽然精确对称性在科学应用中作为强大的归纳偏置带来了显著收益，但最近研究表明近似对称性可能提供更大的灵活性和鲁棒性。然而，文献中缺乏对这两种对称性的直接理论比较和理解。

Method: 引入"平均复杂度"框架来量化通过平均实现对称性的成本。在标准条件下分析精确对称和近似对称的复杂度要求。

Result: 主要结果是发现了指数级分离：实现精确对称性需要线性平均复杂度，而实现近似对称性只需要对数平均复杂度。

Conclusion: 这是首次从理论上分离精确对称和近似对称的情况，正式解释了为什么在实践中近似对称可能更优。所提出的工具和技术对机器学习中对称性的更广泛研究具有独立价值。

Abstract: Enforcing exact symmetry in machine learning models often yields significant gains in scientific applications, serving as a powerful inductive bias. However, recent work suggests that relying on approximate symmetry can offer greater flexibility and robustness. Despite promising empirical evidence, there has been little theoretical understanding, and in particular, a direct comparison between exact and approximate symmetry is missing from the literature. In this paper, we initiate this study by asking: What is the cost of enforcing exact versus approximate symmetry? To address this question, we introduce averaging complexity, a framework for quantifying the cost of enforcing symmetry via averaging. Our main result is an exponential separation: under standard conditions, achieving exact symmetry requires linear averaging complexity, whereas approximate symmetry can be attained with only logarithmic averaging complexity. To the best of our knowledge, this provides the first theoretical separation of these two cases, formally justifying why approximate symmetry may be preferable in practice. Beyond this, our tools and techniques may be of independent interest for the broader study of symmetries in machine learning.

</details>


### [67] [TopicProphet: Prophesies on Temporal Topic Trends and Stocks](https://arxiv.org/abs/2512.11857)
*Olivia Kim*

Main category: cs.LG

TL;DR: TopicProphet是一个新颖的股票预测框架，通过分析具有相似公众情绪趋势和历史背景的历史时期来改善预测准确性，解决了股票数据缺乏因果逻辑和训练数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 股票预测长期以来被认为是不可能的，因为定量股票数据缺乏因果逻辑，市场变化迅速导致难以积累足够的训练数据。现有研究虽然识别了关键词和情绪的影响，但未能解决这些根本问题。

Method: 提出TopicProphet框架，采用主题建模、时间分析、断点检测和分段优化等一系列方法，识别具有相似公众情绪趋势和历史背景的最优训练时间段，为模型提供特定时代的社会经济和政治模式。

Result: TopicProphet相比最先进的方法在捕捉最优训练数据方面产生了改进的结果，能够更好地预测金融百分比变化。

Conclusion: 通过分析历史时期的社会经济和政治背景，TopicProphet框架能够解决股票数据缺乏因果逻辑和训练数据不足的问题，显著提高了股票预测的准确性。

Abstract: Stocks can't be predicted. Despite many hopes, this premise held itself true for many years due to the nature of quantitative stock data lacking causal logic along with rapid market changes hindering accumulation of significant data for training models. To undertake this matter, we propose a novel framework, TopicProphet, to analyze historical eras that share similar public sentiment trends and historical background. Our research deviates from previous studies that identified impacts of keywords and sentiments - we expand on that method by a sequence of topic modeling, temporal analysis, breakpoint detection and segment optimization to detect the optimal time period for training. This results in improving predictions by providing the model with nuanced patterns that occur from that era's socioeconomic and political status while also resolving the shortage of pertinent stock data to train on. Through extensive analysis, we conclude that TopicProphet produces improved outcomes compared to the state-of-the-art methods in capturing the optimal training data for forecasting financial percentage changes.

</details>


### [68] [On the Dangers of Bootstrapping Generation for Continual Learning and Beyond](https://arxiv.org/abs/2512.11867)
*Daniil Zverev,A. Sophia Koepke,Joao F. Henriques*

Main category: cs.LG

TL;DR: 论文研究重复使用合成数据训练模型的问题，发现这会导致分布漂移、性能下降和生成模型崩溃，对持续学习中使用合成数据提出警告。


<details>
  <summary>Details</summary>
Motivation: 随着合成数据在模型训练中的普遍应用，重复使用合成数据进行训练引发了分布漂移和性能退化的担忧，需要研究这种自举过程的影响。

Method: 通过持续学习的视角分析自举过程，建立与生成经验回放方法的联系，进行统计分析展示合成数据带来的偏差和方差问题，并通过实证研究验证生成模型的崩溃现象。

Result: 合成数据在训练目标中引入显著偏差和方差，削弱最大似然估计的可靠性；流行生成模型在重复使用合成数据训练时会崩溃；最先进的生成经验回放方法无法保持潜在空间的对齐。

Conclusion: 研究发现对在持续学习中使用合成数据提出了严重警告，表明当前方法存在根本性缺陷，需要重新评估合成数据在训练流程中的使用策略。

Abstract: The use of synthetically generated data for training models is becoming a common practice. While generated data can augment the training data, repeated training on synthetic data raises concerns about distribution drift and degradation of performance due to contamination of the dataset. We investigate the consequences of this bootstrapping process through the lens of continual learning, drawing a connection to Generative Experience Replay (GER) methods. We present a statistical analysis showing that synthetic data introduces significant bias and variance into training objectives, weakening the reliability of maximum likelihood estimation. We provide empirical evidence showing that popular generative models collapse under repeated training with synthetic data. We quantify this degradation and show that state-of-the-art GER methods fail to maintain alignment in the latent space. Our findings raise critical concerns about the use of synthetic data in continual learning.

</details>


### [69] [Data-Driven Global Sensitivity Analysis for Engineering Design Based on Individual Conditional Expectations](https://arxiv.org/abs/2512.11946)
*Pramudita Satria Palar,Paul Saves,Rommel G. Regis,Koji Shimoyama,Shigeru Obayashi,Nicolas Verstaevel,Joseph Morlier*

Main category: cs.LG

TL;DR: 提出基于ICE曲线的全局敏感性度量方法，解决传统PDP在存在强交互作用时可能产生误导的问题，通过ICE曲线计算特征重要性及其标准差，并引入ICE相关性值量化交互作用对输入输出关系的影响。


<details>
  <summary>Details</summary>
Motivation: 在航空航天等工程应用中，可解释机器学习技术日益重要，需要理解输入变量对数据驱动模型的影响。传统PDP方法在存在强交互作用时，其全局敏感性度量可能产生误导，因为平均化会掩盖交互效应。

Method: 提出基于ICE曲线的全局敏感性度量方法：1) 计算ICE曲线上的期望特征重要性及其标准差；2) 提供数学证明，表明PDP敏感性是所提ICE度量的下界；3) 引入ICE相关性值量化交互作用如何修改输入输出关系。

Result: 在三个案例上进行比较评估：5变量解析函数、5变量风力涡轮机疲劳问题、9变量翼型空气动力学案例。ICE敏感性在PDP、SHAP和Sobol指数基准测试中表现优异，ICE特征重要性比传统PDP方法提供更丰富的见解。

Conclusion: ICE特征重要性比传统PDP方法提供更丰富的见解，而PDP、ICE和SHAP的可视化解释通过提供多重视角相互补充。所提方法能更有效地捕捉交互作用的影响。

Abstract: Explainable machine learning techniques have gained increasing attention in engineering applications, especially in aerospace design and analysis, where understanding how input variables influence data-driven models is essential. Partial Dependence Plots (PDPs) are widely used for interpreting black-box models by showing the average effect of an input variable on the prediction. However, their global sensitivity metric can be misleading when strong interactions are present, as averaging tends to obscure interaction effects. To address this limitation, we propose a global sensitivity metric based on Individual Conditional Expectation (ICE) curves. The method computes the expected feature importance across ICE curves, along with their standard deviation, to more effectively capture the influence of interactions. We provide a mathematical proof demonstrating that the PDP-based sensitivity is a lower bound of the proposed ICE-based metric under truncated orthogonal polynomial expansion. In addition, we introduce an ICE-based correlation value to quantify how interactions modify the relationship between inputs and the output. Comparative evaluations were performed on three cases: a 5-variable analytical function, a 5-variable wind-turbine fatigue problem, and a 9-variable airfoil aerodynamics case, where ICE-based sensitivity was benchmarked against PDP, SHapley Additive exPlanations (SHAP), and Sobol' indices. The results show that ICE-based feature importance provides richer insights than the traditional PDP-based approach, while visual interpretations from PDP, ICE, and SHAP complement one another by offering multiple perspectives.

</details>


### [70] [Neural Chameleons: Language Models Can Learn to Hide Their Thoughts from Unseen Activation Monitors](https://arxiv.org/abs/2512.11949)
*Max McGuinness,Alex Serrano,Luke Bailey,Scott Emmons*

Main category: cs.LG

TL;DR: 论文提出"神经变色龙"模型，通过微调使LLM能够零样本逃避激活监控器，即使面对安全相关的概念也能成功规避检测。


<details>
  <summary>Details</summary>
Motivation: 测试激活监控在最坏情况下的鲁棒性，特别是在模型可能主动隐藏内部状态的错位威胁模型下，验证模型是否能够学会逃避之前未见过的激活监控器。

Method: 通过微调LLM，使其在接收到特定触发词（如"你正在被探测{概念}"）时，能够逃避针对良性概念（如语言、HTML）的激活监控。然后测试这种机制是否能够零样本泛化到安全相关概念。

Result: 成功创建了能够零样本逃避激活监控的神经变色龙模型，这种逃避具有高度选择性，只针对触发词中提到的特定概念，对模型在标准基准测试上的能力影响有限。机制分析显示这是通过将激活移动到低维子空间实现的。

Conclusion: 研究证明了激活监控在错位威胁模型下的潜在失效模式，为评估监控技术在最坏情况下的鲁棒性提供了概念验证和工具。虽然更强的防御措施（如监控器集成和非线性分类器）表现出更好的弹性，但模型仍保留非平凡的逃避能力。

Abstract: Activation monitoring, which probes a model's internal states using lightweight classifiers, is an emerging tool for AI safety. However, its worst-case robustness under a misalignment threat model--where a model might learn to actively conceal its internal states--remains untested. Focusing on this threat model, we ask: could a model learn to evade previously unseen activation monitors? Our core contribution is to stress-test the learnability of this behavior. We demonstrate that finetuning can create Neural Chameleons: models capable of zero-shot evading activation monitors. Specifically, we fine-tune an LLM to evade monitors for a set of benign concepts (e.g., languages, HTML) when conditioned on a trigger of the form: "You are being probed for {concept}". We show that this learned mechanism generalizes zero-shot: by substituting {concept} with a safety-relevant term like 'deception', the model successfully evades previously unseen safety monitors. We validate this phenomenon across diverse model families (Llama, Gemma, Qwen), showing that the evasion succeeds even against monitors trained post hoc on the model's frozen weights. This evasion is highly selective, targeting only the specific concept mentioned in the trigger, and having a modest impact on model capabilities on standard benchmarks. Using Gemma-2-9b-it as a case study, a mechanistic analysis reveals this is achieved via a targeted manipulation that moves activations into a low-dimensional subspace. While stronger defenses like monitor ensembles and non-linear classifiers show greater resilience, the model retains a non-trivial evasion capability. Our work provides a proof-of-concept for this failure mode and a tool to evaluate the worst-case robustness of monitoring techniques against misalignment threat models.

</details>


### [71] [Learning to Extract Context for Context-Aware LLM Inference](https://arxiv.org/abs/2512.11986)
*Minseon Kim,Lucas Caccia,Zhengyan Shi,Matheus Pereira,Marc-Alexandre Côté,Xingdi Yuan,Alessandro Sordoni*

Main category: cs.LG

TL;DR: 提出基于强化学习的上下文生成框架，从用户提示中提取上下文信息来指导LLM响应生成，提高安全性和可靠性


<details>
  <summary>Details</summary>
Motivation: 用户对大型语言模型的提示往往模糊或不完整，而用户意图、先验知识和风险因素等上下文线索强烈影响什么才是合适的响应。误解意图或风险可能导致不安全输出，而过度谨慎的解释又会导致良性请求被不必要拒绝

Method: 提出一个框架，从用户提示本身提取和利用上下文信息。具体采用基于强化学习的上下文生成器，以自编码器方式设计，训练其从提示中推断上下文信号，并用这些信号指导响应生成

Result: 在SafetyInstruct数据集上，该方法平均减少5.6%的有害响应；在XSTest和WildJailbreak上，良性提示的攻击成功率和合规性的调和平均提高了6.2%

Conclusion: 上下文提取对于更安全、更可靠的LLM推理是有效的，特别是在安全任务中，能够处理模糊请求绕过安全防护或良性但令人困惑的请求被不必要拒绝的问题

Abstract: User prompts to large language models (LLMs) are often ambiguous or under-specified, and subtle contextual cues shaped by user intentions, prior knowledge, and risk factors strongly influence what constitutes an appropriate response. Misinterpreting intent or risks may lead to unsafe outputs, while overly cautious interpretations can cause unnecessary refusal of benign requests. In this paper, we question the conventional framework in which LLMs generate immediate responses to requests without considering broader contextual factors. User requests are situated within broader contexts such as intentions, knowledge, and prior experience, which strongly influence what constitutes an appropriate answer. We propose a framework that extracts and leverages such contextual information from the user prompt itself. Specifically, a reinforcement learning based context generator, designed in an autoencoder-like fashion, is trained to infer contextual signals grounded in the prompt and use them to guide response generation. This approach is particularly important for safety tasks, where ambiguous requests may bypass safeguards while benign but confusing requests can trigger unnecessary refusals. Experiments show that our method reduces harmful responses by an average of 5.6% on the SafetyInstruct dataset across multiple foundation models and improves the harmonic mean of attack success rate and compliance on benign prompts by 6.2% on XSTest and WildJailbreak. These results demonstrate the effectiveness of context extraction for safer and more reliable LLM inferences.

</details>


### [72] [EnviroLLM: Resource Tracking and Optimization for Local AI](https://arxiv.org/abs/2512.12004)
*Troy Allen*

Main category: cs.LG

TL;DR: EnviroLLM是一个开源工具包，用于跟踪、基准测试和优化在个人设备上运行大型语言模型的性能和能耗，提供实时监控、多平台基准测试、可视化分析和个性化推荐。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地在本地部署以保护隐私和提升可访问性，用户缺乏工具来测量其资源使用、环境影响和效率指标。

Method: 开发了EnviroLLM工具包，包含实时进程监控、多平台基准测试（Ollama、LM Studio、vLLM和OpenAI兼容API）、持久化存储与可视化分析、个性化模型和优化推荐，以及结合LLM-as-judge评估与能耗速度指标的评估系统。

Result: 系统能够帮助用户评估质量与效率之间的权衡，通过综合性能、能耗和质量指标来优化本地LLM部署。

Conclusion: EnviroLLM为本地部署的LLM提供了全面的性能监控和优化工具，帮助用户在保护隐私的同时实现高效、环保的模型运行。

Abstract: Large language models (LLMs) are increasingly deployed locally for privacy and accessibility, yet users lack tools to measure their resource usage, environmental impact, and efficiency metrics. This paper presents EnviroLLM, an open-source toolkit for tracking, benchmarking, and optimizing performance and energy consumption when running LLMs on personal devices. The system provides real-time process monitoring, benchmarking across multiple platforms (Ollama, LM Studio, vLLM, and OpenAI-compatible APIs), persistent storage with visualizations for longitudinal analysis, and personalized model and optimization recommendations. The system includes LLM-as-judge evaluations alongside energy and speed metrics, enabling users to assess quality-efficiency tradeoffs when testing models with custom prompts.

</details>


### [73] [DFedReweighting: A Unified Framework for Objective-Oriented Reweighting in Decentralized Federated Learning](https://arxiv.org/abs/2512.12022)
*Kaichuang Zhang,Wei Yin,Jinghao Yang,Ping Xu*

Main category: cs.LG

TL;DR: DFedReweighting是一个去中心化联邦学习的统一聚合框架，通过目标导向的重加权聚合来解决公平性、鲁棒性等挑战


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习虽然避免了中心服务器的单点故障风险，但仍面临公平性、鲁棒性等挑战，需要一种统一的框架来同时解决这些问题

Method: 提出DFedReweighting框架，在每轮学习的最后一步进行目标导向的重加权聚合：首先基于目标性能指标计算初步权重，然后使用定制化的重加权策略进行细化，得到最终聚合权重

Result: 理论分析表明，适当的目标性能指标和重加权策略组合能确保线性收敛；实验结果显示，该框架在多种场景下显著提升了公平性和对拜占庭攻击的鲁棒性

Conclusion: 只要选择合适的目标性能指标和定制化重加权策略，该框架能够实现广泛的期望学习目标，为去中心化联邦学习提供了一个灵活有效的解决方案

Abstract: Decentralized federated learning (DFL) has recently emerged as a promising paradigm that enables multiple clients to collaboratively train machine learning model through iterative rounds of local training, communication, and aggregation without relying on a central server which introduces potential vulnerabilities in conventional Federated Learning. Nevertheless, DFL systems continue to face a range of challenges, including fairness, robustness, etc. To address these challenges, we propose \textbf{DFedReweighting}, a unified aggregation framework designed to achieve diverse objectives in DFL systems via a objective-oriented reweighting aggregation at the final step of each learning round. Specifically, the framework first computes preliminary weights based on \textit{target performance metric} obtained from auxiliary dataset constructed using local data. These weights are then refined using \textit{customized reweighting strategy}, resulting in the final aggregation weights. Our results from the theoretical analysis demonstrate that the appropriate combination of the target performance metric and the customized reweighting strategy ensures linear convergence. Experimental results consistently show that our proposed framework significantly improves fairness and robustness against Byzantine attacks in diverse scenarios. Provided that appropriate target performance metrics and customized reweighting strategy are selected, our framework can achieve a wide range of desired learning objectives.

</details>


### [74] [Goal Reaching with Eikonal-Constrained Hierarchical Quasimetric Reinforcement Learning](https://arxiv.org/abs/2512.12046)
*Vittorio Giammarino,Ahmed H. Qureshi*

Main category: cs.LG

TL;DR: Eik-HiQRL：基于Eikonal PDE的连续时间准度量强化学习框架，通过分层分解解决复杂动态问题，在离线目标条件导航和操作任务中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 目标条件强化学习（GCRL）通过目标到达任务简化奖励设计，准度量强化学习（QRL）基于最优值函数的准度量特性，但现有QRL方法依赖离散轨迹约束，需要连续时间轨迹无关的改进方案。

Method: 提出Eikonal约束准度量强化学习（Eik-QRL），基于Eikonal偏微分方程进行连续时间重构，仅需采样状态和目标，无需轨迹；针对复杂动态问题，进一步提出Eik-Hierarchical QRL（Eik-HiQRL）分层分解框架。

Result: Eik-QRL在分布外泛化方面表现更好；Eik-HiQRL在离线目标条件导航任务中达到最先进性能，在操作任务中相比QRL有持续提升，与时间差分方法性能相当。

Conclusion: 基于Eikonal PDE的连续时间准度量强化学习框架有效解决了轨迹依赖问题，通过分层分解进一步提升了复杂动态环境下的性能，为目标条件强化学习提供了新的理论保证和实践方案。

Abstract: Goal-Conditioned Reinforcement Learning (GCRL) mitigates the difficulty of reward design by framing tasks as goal reaching rather than maximizing hand-crafted reward signals. In this setting, the optimal goal-conditioned value function naturally forms a quasimetric, motivating Quasimetric RL (QRL), which constrains value learning to quasimetric mappings and enforces local consistency through discrete, trajectory-based constraints. We propose Eikonal-Constrained Quasimetric RL (Eik-QRL), a continuous-time reformulation of QRL based on the Eikonal Partial Differential Equation (PDE). This PDE-based structure makes Eik-QRL trajectory-free, requiring only sampled states and goals, while improving out-of-distribution generalization. We provide theoretical guarantees for Eik-QRL and identify limitations that arise under complex dynamics. To address these challenges, we introduce Eik-Hierarchical QRL (Eik-HiQRL), which integrates Eik-QRL into a hierarchical decomposition. Empirically, Eik-HiQRL achieves state-of-the-art performance in offline goal-conditioned navigation and yields consistent gains over QRL in manipulation tasks, matching temporal-difference methods.

</details>


### [75] [The Instability of Safety: How Random Seeds and Temperature Expose Inconsistent LLM Refusal Behavior](https://arxiv.org/abs/2512.12066)
*Erik Larsen*

Main category: cs.LG

TL;DR: 研究发现当前大语言模型安全评估的单次测试方法存在缺陷，模型的安全拒绝决策在不同随机种子和温度设置下不稳定，18-28%的提示会出现决策翻转，建议至少使用3个样本进行可靠评估。


<details>
  <summary>Details</summary>
Motivation: 挑战当前大语言模型安全评估中隐含的假设：模型响应是确定性的且能代表模型的安全对齐程度。研究者认为单次测试方法可能无法可靠评估模型的安全性。

Method: 测试了4个指令调优模型（Llama 3.1 8B、Qwen 2.5 7B、Qwen 3 8B、Gemma 3 12B），使用876个有害提示，在20种不同采样配置（4个温度×5个随机种子）下进行测试。开发了安全稳定性指数（SSI），并使用Claude 3.5 Haiku作为统一外部评判者验证结果。

Result: 发现18-28%的提示在不同配置下会出现决策翻转（有时拒绝有时遵从）。温度越高，决策稳定性越差（SSI从温度0.0时的0.951下降到温度1.0时的0.896）。单次评估与多样本真实情况的一致性仅为92.4%。

Conclusion: 单次安全评估方法不可靠，无法准确评估模型的安全性。建议至少使用3个样本/提示进行可靠的安全评估，并考虑随机种子和温度设置对安全决策稳定性的影响。

Abstract: Current safety evaluations of large language models rely on single-shot testing, implicitly assuming that model responses are deterministic and representative of the model's safety alignment. We challenge this assumption by investigating the stability of safety refusal decisions across random seeds and temperature settings. Testing four instruction-tuned models from three families (Llama 3.1 8B, Qwen 2.5 7B, Qwen 3 8B, Gemma 3 12B) on 876 harmful prompts across 20 different sampling configurations (4 temperatures x 5 random seeds), we find that 18-28% of prompts exhibit decision flips--the model refuses in some configurations but complies in others--depending on the model. Our Safety Stability Index (SSI) reveals that higher temperatures significantly reduce decision stability (Friedman chi-squared = 44.71, p < 0.001), with mean SSI dropping from 0.951 at temperature 0.0 to 0.896 at temperature 1.0. We validate our findings across all model families using Claude 3.5 Haiku as a unified external judge, achieving 89.0% inter-judge agreement with our primary Llama 70B judge (Cohen's kappa = 0.62). These findings demonstrate that single-shot safety evaluations are insufficient for reliable safety assessment. We show that single-shot evaluation agrees with multi-sample ground truth only 92.4% of the time, and recommend using at least 3 samples per prompt for reliable safety assessment.

</details>


### [76] [SigTime: Learning and Visually Explaining Time Series Signatures](https://arxiv.org/abs/2512.12076)
*Yu-Chia Huang,Juntong Chen,Dongyu Liu,Kwan-Liu Ma*

Main category: cs.LG

TL;DR: 提出SigTime框架，结合Transformer模型、shapelet表示和特征工程，用于时间序列模式发现和可视化分析


<details>
  <summary>Details</summary>
Motivation: 现有时间序列模式发现方法存在计算复杂度高、可解释性有限、难以捕捉有意义时间结构等问题，特别是在生物医学等领域需要从生理信号中发现有意义模式以改善诊断和患者预后

Method: 提出联合训练两个Transformer模型的学习框架：使用shapelet表示捕捉局部时间结构，结合传统特征工程编码统计特性；开发SigTime可视化分析系统，提供协调视图从多角度探索时间序列签名

Result: 在8个公开数据集和1个专有临床数据集上定量评估；通过两个使用场景（公共ECG数据和早产分析）与领域专家一起展示系统有效性

Conclusion: 提出的学习框架和可视化系统能够有效发现和解释时间序列中的有意义模式，为科学发现和决策提供支持

Abstract: Understanding and distinguishing temporal patterns in time series data is essential for scientific discovery and decision-making. For example, in biomedical research, uncovering meaningful patterns in physiological signals can improve diagnosis, risk assessment, and patient outcomes. However, existing methods for time series pattern discovery face major challenges, including high computational complexity, limited interpretability, and difficulty in capturing meaningful temporal structures. To address these gaps, we introduce a novel learning framework that jointly trains two Transformer models using complementary time series representations: shapelet-based representations to capture localized temporal structures and traditional feature engineering to encode statistical properties. The learned shapelets serve as interpretable signatures that differentiate time series across classification labels. Additionally, we develop a visual analytics system -- SigTIme -- with coordinated views to facilitate exploration of time series signatures from multiple perspectives, aiding in useful insights generation. We quantitatively evaluate our learning framework on eight publicly available datasets and one proprietary clinical dataset. Additionally, we demonstrate the effectiveness of our system through two usage scenarios along with the domain experts: one involving public ECG data and the other focused on preterm labor analysis.

</details>


### [77] [CLOAK: Contrastive Guidance for Latent Diffusion-Based Data Obfuscation](https://arxiv.org/abs/2512.12086)
*Xin Yang,Omid Ardakanian*

Main category: cs.LG

TL;DR: Cloak是一个基于潜在扩散模型的新型数据混淆框架，通过对比学习提取解耦表示，在保护隐私的同时保持数据实用性，特别适合资源受限的移动物联网设备部署。


<details>
  <summary>Details</summary>
Motivation: 现有数据混淆方法存在以下问题：需要修改下游任务、难以达到满意的隐私-效用平衡、计算密集不适合资源受限的移动物联网设备部署。需要一种更高效、灵活的隐私保护方案。

Method: 提出Cloak框架：1）使用对比学习提取解耦表示；2）利用潜在扩散模型进行数据混淆；3）通过解耦表示指导扩散过程，保留有用信息同时隐藏隐私信息；4）支持用户根据隐私需求灵活调整隐私-效用平衡。

Result: 在四个公开时间序列数据集（涵盖多种传感模态）和人脸图像数据集上的实验表明，Cloak持续优于最先进的混淆技术，并且在资源受限环境中部署表现良好。

Conclusion: Cloak框架通过结合对比学习和潜在扩散模型，实现了更好的隐私-效用平衡，具有部署灵活性，特别适合资源受限的移动物联网设备，为半可信方访问传感器时间序列数据提供了有效的隐私保护方案。

Abstract: Data obfuscation is a promising technique for mitigating attribute inference attacks by semi-trusted parties with access to time-series data emitted by sensors. Recent advances leverage conditional generative models together with adversarial training or mutual information-based regularization to balance data privacy and utility. However, these methods often require modifying the downstream task, struggle to achieve a satisfactory privacy-utility trade-off, or are computationally intensive, making them impractical for deployment on resource-constrained mobile IoT devices. We propose Cloak, a novel data obfuscation framework based on latent diffusion models. In contrast to prior work, we employ contrastive learning to extract disentangled representations, which guide the latent diffusion process to retain useful information while concealing private information. This approach enables users with diverse privacy needs to navigate the privacy-utility trade-off with minimal retraining. Extensive experiments on four public time-series datasets, spanning multiple sensing modalities, and a dataset of facial images demonstrate that Cloak consistently outperforms state-of-the-art obfuscation techniques and is well-suited for deployment in resource-constrained settings.

</details>


### [78] [Neural CDEs as Correctors for Learned Time Series Models](https://arxiv.org/abs/2512.12116)
*Muhammad Bilal Shahid,Prajwal Koirla,Cody Fleming*

Main category: cs.LG

TL;DR: 提出Predictor-Corrector机制，用神经控制微分方程作为Corrector来预测并修正任何时间序列模型的预测误差，提升多步预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列模型（连续或离散时间）的多步预测容易产生误差，无论是直接预测整个时间范围还是迭代预测都存在这个问题，需要一种机制来修正预测误差。

Method: 提出Predictor-Corrector框架：Predictor可以是任何学习的时间序列模型，Corrector是神经控制微分方程，用于预测Predictor的预测误差，并将误差加到预测结果上。Corrector支持不规则采样时间序列，并引入两种正则化策略来提升外推性能和加速训练。

Result: 在合成数据、物理模拟和真实世界预测数据集上，使用多种Predictor（如神经常微分方程、Contiformer、DLinear）进行实验，结果表明Predictor-Corrector机制相比单独使用Predictor能持续提升预测性能。

Conclusion: 提出的Predictor-Corrector机制能有效修正时间序列模型的预测误差，提升多步预测精度，且具有通用性，可与多种预测模型结合使用。

Abstract: Learned time-series models, whether continuous- or discrete-time, are widely used to forecast the states of a dynamical system. Such models generate multi-step forecasts either directly, by predicting the full horizon at once, or iteratively, by feeding back their own predictions at each step. In both cases, the multi-step forecasts are prone to errors. To address this, we propose a Predictor-Corrector mechanism where the Predictor is any learned time-series model and the Corrector is a neural controlled differential equation. The Predictor forecasts, and the Corrector predicts the errors of the forecasts. Adding these errors to the forecasts improves forecast performance. The proposed Corrector works with irregularly sampled time series and continuous- and discrete-time Predictors. Additionally, we introduce two regularization strategies to improve the extrapolation performance of the Corrector with accelerated training. We evaluate our Corrector with diverse Predictors, e.g., neural ordinary differential equations, Contiformer, and DLinear, on synthetic, physics simulation, and real-world forecasting datasets. The experiments demonstrate that the Predictor-Corrector mechanism consistently improves the performance compared to Predictor alone.

</details>


### [79] [BOOST: BOttleneck-Optimized Scalable Training Framework for Low-Rank Large Language Models](https://arxiv.org/abs/2512.12131)
*Zhengyang Wang,Ziyue Liu,Ruijie Zhang,Avinash Maurya,Paul Hovland,Bogdan Nicolae,Franck Cappello,Zheng Zhang*

Main category: cs.LG

TL;DR: BOOST框架针对低秩瓶颈架构提出瓶颈感知张量并行等优化，相比全秩模型基线实现1.46-1.91倍加速，相比简单集成3D并行的低秩模型实现1.87-2.27倍加速


<details>
  <summary>Details</summary>
Motivation: Transformer模型预训练规模受计算和通信成本限制，低秩瓶颈架构能显著减少训练时间和内存占用，但标准张量并行下扩展性差，简单应用3D并行会导致过度通信和GPU利用率低下

Method: 提出BOOST训练框架，包括瓶颈感知张量并行、在线RMSNorm、线性层分组和低秩激活检查点等优化技术

Result: 在不同低秩瓶颈架构上，BOOST相比全秩模型基线实现1.46-1.91倍加速，相比简单集成3D并行的低秩模型实现1.87-2.27倍加速，提高了GPU利用率并减少了通信开销

Conclusion: BOOST框架有效解决了低秩瓶颈架构在大规模训练中的扩展性问题，为高效训练大规模Transformer模型提供了实用解决方案

Abstract: The scale of transformer model pre-training is constrained by the increasing computation and communication cost. Low-rank bottleneck architectures offer a promising solution to significantly reduce the training time and memory footprint with minimum impact on accuracy. Despite algorithmic efficiency, bottleneck architectures scale poorly under standard tensor parallelism. Simply applying 3D parallelism designed for full-rank methods leads to excessive communication and poor GPU utilization. To address this limitation, we propose BOOST, an efficient training framework tailored for large-scale low-rank bottleneck architectures. BOOST introduces a novel Bottleneck-aware Tensor Parallelism, and combines optimizations such as online-RMSNorm, linear layer grouping, and low-rank activation checkpointing to achieve end-to-end training speedup. Evaluations on different low-rank bottleneck architectures demonstrate that BOOST achieves 1.46-1.91$\times$ speedup over full-rank model baselines and 1.87-2.27$\times$ speedup over low-rank model with naively integrated 3D parallelism, with improved GPU utilization and reduced communication overhead.

</details>


### [80] [MolGuidance: Advanced Guidance Strategies for Conditional Molecular Generation with Flow Matching](https://arxiv.org/abs/2512.12198)
*Jirui Jin,Cheng Zeng,Pawan Prakash,Ellad B. Tadmor,Adrian Roitberg,Richard G. Hennig,Stefano Martiniani,Mingjie Liu*

Main category: cs.LG

TL;DR: 该研究将计算机视觉中的先进引导策略（如无分类器引导、自动引导和模型引导）整合到SE(3)-等变流匹配的分子生成框架中，提出了一种混合引导策略，分别处理连续和离散分子模态，并通过贝叶斯优化联合优化引导尺度，在QM9和QMe14S数据集上实现了属性对齐的新SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 条件分子生成的关键目标包括确保化学有效性、使生成分子与目标属性对齐、促进结构多样性以及实现高效采样。虽然计算机视觉领域已开发出多种先进的引导策略，但这些方法在分子生成领域的应用潜力尚未充分探索。

Method: 1. 将最先进的引导方法（无分类器引导、自动引导、模型引导）整合到基于SE(3)-等变流匹配过程的分子生成框架中
2. 提出混合引导策略：分别引导连续模态（速度场）和离散模态（预测logits）
3. 通过贝叶斯优化联合优化连续和离散模态的引导尺度
4. 在QM9和QMe14S数据集上进行基准测试

Result: 1. 在从头分子生成的属性对齐方面实现了新的最先进性能
2. 生成的分子表现出高度的结构有效性
3. 系统比较了各种引导方法的优势和局限性，为更广泛的应用提供了见解

Conclusion: 该研究成功地将计算机视觉中的先进引导策略应用于分子生成，提出的混合引导方法在属性对齐和化学有效性方面表现出色，为条件分子生成提供了有效的解决方案，并系统评估了不同引导策略的适用性。

Abstract: Key objectives in conditional molecular generation include ensuring chemical validity, aligning generated molecules with target properties, promoting structural diversity, and enabling efficient sampling for discovery. Recent advances in computer vision introduced a range of new guidance strategies for generative models, many of which can be adapted to support these goals. In this work, we integrate state-of-the-art guidance methods -- including classifier-free guidance, autoguidance, and model guidance -- in a leading molecule generation framework built on an SE(3)-equivariant flow matching process. We propose a hybrid guidance strategy that separately guides continuous and discrete molecular modalities -- operating on velocity fields and predicted logits, respectively -- while jointly optimizing their guidance scales via Bayesian optimization. Our implementation, benchmarked on the QM9 and QMe14S datasets, achieves new state-of-the-art performance in property alignment for de novo molecular generation. The generated molecules also exhibit high structural validity. Furthermore, we systematically compare the strengths and limitations of various guidance methods, offering insights into their broader applicability.

</details>


### [81] [EEG-DLite: Dataset Distillation for Efficient Large EEG Model Training](https://arxiv.org/abs/2512.12210)
*Yuting Tang,Weibang Jiang,Shanglin Li,Yong Li,Chenyu Liu,Xinliang Zhou,Yi Ding,Cuntai Guan*

Main category: cs.LG

TL;DR: EEG-DLite是一个数据蒸馏框架，通过选择性去除脑电图数据中的噪声和冗余样本，使大规模EEG基础模型的预训练更加高效。


<details>
  <summary>Details</summary>
Motivation: 大规模EEG基础模型训练资源密集，因为EEG数据量大且质量参差不齐。需要一种方法来减少训练数据量同时保持模型性能。

Method: 使用自监督自动编码器将EEG片段编码为紧凑的潜在表示，基于这些表示过滤异常值并最小化冗余，从而创建一个小而信息丰富的子集。

Result: 在仅使用EEG-DLite筛选的2,500小时数据集的5%进行训练时，在多个下游任务上取得了与使用完整数据集相当甚至更好的性能。

Conclusion: EEG-DLite为更有效和高效的生理基础模型训练提供了可扩展且实用的路径，是EEG基础模型预训练数据蒸馏领域的首个系统性研究。

Abstract: Large-scale EEG foundation models have shown strong generalization across a range of downstream tasks, but their training remains resource-intensive due to the volume and variable quality of EEG data. In this work, we introduce EEG-DLite, a data distillation framework that enables more efficient pre-training by selectively removing noisy and redundant samples from large EEG datasets. EEG-DLite begins by encoding EEG segments into compact latent representations using a self-supervised autoencoder, allowing sample selection to be performed efficiently and with reduced sensitivity to noise. Based on these representations, EEG-DLite filters out outliers and minimizes redundancy, resulting in a smaller yet informative subset that retains the diversity essential for effective foundation model training. Through extensive experiments, we demonstrate that training on only 5 percent of a 2,500-hour dataset curated with EEG-DLite yields performance comparable to, and in some cases better than, training on the full dataset across multiple downstream tasks. To our knowledge, this is the first systematic study of pre-training data distillation in the context of EEG foundation models. EEG-DLite provides a scalable and practical path toward more effective and efficient physiological foundation modeling. The code is available at https://github.com/t170815518/EEG-DLite.

</details>


### [82] [Optimized Learned Count-Min Sketch](https://arxiv.org/abs/2512.12252)
*Kyosuke Nishishita,Atsuki Sato,Yusuke Matsui*

Main category: cs.LG

TL;DR: OptLCMS是一种优化的学习型Count-Min Sketch，通过分区和动态规划优化阈值，在相同内存下比LCMS构建更快、理论保证更好，同时保持估计精度。


<details>
  <summary>Details</summary>
Motivation: 传统的学习型Count-Min Sketch（LCMS）虽然通过机器学习模型减少了估计误差，但存在构建速度慢（需要经验参数调优）和缺乏理论保证（对不可容忍误差概率）的问题。

Method: OptLCMS将输入域分区，为每个分区分配独立的CMS实例，通过分析推导固定阈值的CMS参数，并使用带近似可行性检查的动态规划优化阈值，减少经验验证需求。

Result: 实验表明OptLCMS构建速度更快，达到更低的不可容忍误差概率，同时与LCMS的估计精度相匹配。

Conclusion: OptLCMS解决了LCMS的构建速度和理论保证问题，提供了对允许误差阈值的显式控制，在实际应用中具有更好的灵活性和性能。

Abstract: Count-Min Sketch (CMS) is a memory-efficient data structure for estimating the frequency of elements in a multiset. Learned Count-Min Sketch (LCMS) enhances CMS with a machine learning model to reduce estimation error under the same memory usage, but suffers from slow construction due to empirical parameter tuning and lacks theoretical guarantees on intolerable error probability. We propose Optimized Learned Count-Min Sketch (OptLCMS), which partitions the input domain and assigns each partition to its own CMS instance, with CMS parameters analytically derived for fixed thresholds, and thresholds optimized via dynamic programming with approximate feasibility checks. This reduces the need for empirical validation, enabling faster construction while providing theoretical guarantees under these assumptions. OptLCMS also allows explicit control of the allowable error threshold, improving flexibility in practice. Experiments show that OptLCMS builds faster, achieves lower intolerable error probability, and matches the estimation accuracy of LCMS.

</details>


### [83] [GRC-Net: Gram Residual Co-attention Net for epilepsy prediction](https://arxiv.org/abs/2512.12273)
*Bihao You,Jiping Cui*

Main category: cs.LG

TL;DR: 该研究提出GRC-Net模型，通过Gram矩阵将EEG信号转换为3D表示，并采用多级特征提取方法（共注意力机制和Inception结构）处理局部和全局信号，在癫痫预测任务上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 传统癫痫预测方法对EEG信号进行一维处理，无法充分建模信号间的关系。同时，EEG数据中存在局部和全局信号不平衡的问题，需要更有效的特征提取方法。

Method: 1. 使用Gram矩阵将一维EEG信号转换为3D表示，保留时间依赖关系；2. 引入多级特征提取：共注意力机制捕获全局信号特征，Inception结构处理局部信号；3. 构建GRC-Net模型实现多粒度特征提取。

Result: 在BONN数据集上，对于最具挑战性的五分类任务，GRC-Net达到了93.66%的准确率，超越了现有方法。

Conclusion: 通过将EEG信号转换为3D表示并结合多级特征提取，GRC-Net能够有效建模信号间关系并处理局部-全局信号不平衡问题，在癫痫预测任务上表现出优越性能。

Abstract: Prediction of epilepsy based on electroencephalogram (EEG) signals is a rapidly evolving field. Previous studies have traditionally applied 1D processing to the entire EEG signal. However, we have adopted the Gram Matrix method to transform the signals into a 3D representation, enabling modeling of signal relationships across dimensions while preserving the temporal dependencies of the one-dimensional signals. Additionally, we observed an imbalance between local and global signals within the EEG data. Therefore, we introduced multi-level feature extraction, utilizing coattention for capturing global signal characteristics and an inception structure for processing local signals, achieving multi-granular feature extraction. Our experiments on the BONN dataset demonstrate that for the most challenging five-class classification task, GRC-Net achieved an accuracy of 93.66%, outperforming existing methods.

</details>


### [84] [TwinFormer: A Dual-Level Transformer for Long-Sequence Time-Series Forecasting](https://arxiv.org/abs/2512.12301)
*Mahima Kumavat,Aditya Maheshwari*

Main category: cs.LG

TL;DR: TwinFormer是一种用于长序列时间序列预测的分层Transformer，通过局部-全局双阶段处理和top-k稀疏注意力机制，在多个真实世界数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 针对长序列时间序列预测中传统Transformer模型计算复杂度高、难以有效捕捉局部和全局依赖关系的问题，需要设计一种高效的分层架构。

Method: 1. 将输入划分为非重叠的时间片段；2. 局部信息器使用top-k稀疏注意力建模片段内动态，并进行平均池化；3. 全局信息器使用相同的top-k注意力捕捉片段间的长程依赖；4. 使用轻量级GRU聚合全局上下文化的片段标记进行多步预测。

Result: 在8个真实世界数据集（天气、股价、温度、功耗、电力、疾病）上，预测范围96-720步，TwinFormer在34个比较位置中获得27个前两名位置，其中17个位置在MAE和RMSE上取得最佳性能，10个位置取得次佳性能，计算复杂度为线性O(kLd)。

Conclusion: TwinFormer通过分层设计和top-k稀疏注意力机制，在长序列时间序列预测任务中显著优于现有方法，包括PatchTST、iTransformer、FEDformer、Informer和标准Transformer，证明了其架构设计的有效性。

Abstract: TwinFormer is a hierarchical Transformer for long-sequence time-series forecasting. It divides the input into non-overlapping temporal patches and processes them in two stages: (1) a Local Informer with top-$k$ Sparse Attention models intra-patch dynamics, followed by mean pooling; (2) a Global Informer captures long-range inter-patch dependencies using the same top-$k$ attention. A lightweight GRU aggregates the globally contextualized patch tokens for direct multi-horizon prediction. The resulting architecture achieves linear $O(kLd)$ time and memory complexity. On eight real-world benchmarking datasets from six different domains, including weather, stock price, temperature, power consumption, electricity, and disease, and forecasting horizons $96-720$, TwinFormer secures $27$ positions in the top two out of $34$. Out of the $27$, it achieves the best performance on MAE and RMSE at $17$ places and $10$ at the second-best place on MAE and RMSE. This consistently outperforms PatchTST, iTransformer, FEDformer, Informer, and vanilla Transformers. Ablations confirm the superiority of top-$k$ Sparse Attention over ProbSparse and the effectiveness of GRU-based aggregation. Code is available at this repository: https://github.com/Mahimakumavat1205/TwinFormer.

</details>


### [85] [Eventually LIL Regret: Almost Sure $\ln\ln T$ Regret for a sub-Gaussian Mixture on Unbounded Data](https://arxiv.org/abs/2512.12325)
*Shubhada Agrawal,Aaditya Ramdas*

Main category: cs.LG

TL;DR: 该论文证明了Robbins提出的经典次高斯混合方法在随机设定下，实际上满足路径式（确定性）遗憾界。对于自然"Ville事件"E_α中的每条路径，时间T前的遗憾被上界为ln²(1/α)/V_T + ln(1/α) + ln ln V_T（乘以通用常数），其中V_T是非负非递减的累积方差过程。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是连接对抗性在线学习（通常处理有界数据的遗憾界）和博弈论统计学（可以处理无界数据，但使用随机假设）这两个领域。通过条件遗憾界作为随机和对抗性投注之间的桥梁。

Method: 作者分析了一个经典的次高斯混合方法，证明了它在随机设定下满足路径式遗憾界。研究基于Ville事件的概念，在概率至少为1-α的事件E_α上，为每条路径提供遗憾界。特别关注累积方差过程V_T的作用。

Result: 主要结果包括：1) 在Ville事件E_α上，遗憾界为ln²(1/α)/V_T + ln(1/α) + ln ln V_T；2) 当V_T ≥ ln(1/α)时，界简化为ln(1/α) + ln ln V_T；3) 在概率为1的事件E_0上，遗憾最终被ln ln V_T界住；4) 对于广泛的分布类别（次高斯、对称、方差有界等），E_α的概率至少为1-α。

Conclusion: 该工作通过条件遗憾界连接了对抗性在线学习和博弈论统计学，为处理无界数据提供了新的理论框架。结果表明，经典的随机方法实际上具有确定性的路径式性能保证，这有助于统一随机和对抗性设定下的学习理论。

Abstract: We prove that a classic sub-Gaussian mixture proposed by Robbins in a stochastic setting actually satisfies a path-wise (deterministic) regret bound. For every path in a natural ``Ville event'' $E_α$, this regret till time $T$ is bounded by $\ln^2(1/α)/V_T + \ln (1/α) + \ln \ln V_T$ up to universal constants, where $V_T$ is a nonnegative, nondecreasing, cumulative variance process. (The bound reduces to $\ln(1/α) + \ln \ln V_T$ if $V_T \geq \ln(1/α)$.) If the data were stochastic, then one can show that $E_α$ has probability at least $1-α$ under a wide class of distributions (eg: sub-Gaussian, symmetric, variance-bounded, etc.). In fact, we show that on the Ville event $E_0$ of probability one, the regret on every path in $E_0$ is eventually bounded by $\ln \ln V_T$ (up to constants). We explain how this work helps bridge the world of adversarial online learning (which usually deals with regret bounds for bounded data), with game-theoretic statistics (which can handle unbounded data, albeit using stochastic assumptions). In short, conditional regret bounds serve as a bridge between stochastic and adversarial betting.

</details>


### [86] [Uncertainty Quantification for Machine Learning: One Size Does Not Fit All](https://arxiv.org/abs/2512.12341)
*Paul Hofman,Yusuf Sale,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 本文认为不存在单一最佳的不确定性度量方法，应根据具体应用场景定制不确定性量化，提出了基于二阶分布区分总、偶然和认知不确定性的灵活度量框架。


<details>
  <summary>Details</summary>
Motivation: 机器学习在安全关键应用中需要准确的不确定性量化，但现有研究通常声称某种度量方法优于其他方法。本文认为不存在单一最佳的不确定性度量，而应根据具体应用需求进行定制。

Method: 使用基于二阶分布的灵活不确定性度量框架，区分总不确定性、偶然不确定性和认知不确定性。这些度量可以通过适当的评分规则（proper scoring rules）进行实例化，以控制其特性。

Result: 不同特性的不确定性度量在不同任务中表现最佳：对于选择性预测任务，评分规则应与任务损失匹配；对于分布外检测，互信息（广泛使用的认知不确定性度量）表现最佳；在主动学习设置中，基于0-1损失的认知不确定性始终优于其他度量。

Conclusion: 不确定性量化应根据具体应用场景进行定制，不存在适用于所有任务的单一最佳度量方法。提出的灵活框架允许根据任务需求选择合适的不确定性度量。

Abstract: Proper quantification of predictive uncertainty is essential for the use of machine learning in safety-critical applications. Various uncertainty measures have been proposed for this purpose, typically claiming superiority over other measures. In this paper, we argue that there is no single best measure. Instead, uncertainty quantification should be tailored to the specific application. To this end, we use a flexible family of uncertainty measures that distinguishes between total, aleatoric, and epistemic uncertainty of second-order distributions. These measures can be instantiated with specific loss functions, so-called proper scoring rules, to control their characteristics, and we show that different characteristics are useful for different tasks. In particular, we show that, for the task of selective prediction, the scoring rule should ideally match the task loss. On the other hand, for out-of-distribution detection, our results confirm that mutual information, a widely used measure of epistemic uncertainty, performs best. Furthermore, in an active learning setting, epistemic uncertainty based on zero-one loss is shown to consistently outperform other uncertainty measures.

</details>


### [87] [Synthetic Swarm Mosquito Dataset for Acoustic Classification: A Proof of Concept](https://arxiv.org/abs/2512.12365)
*Thai-Duy Dinh,Minh-Luan Vo,Cuong Tuan Nguyen,Bich-Hien Vo*

Main category: cs.LG

TL;DR: 该研究创建了一个用于声学分类的合成蚊群数据集，通过轻量级深度学习模型有效识别六种主要蚊媒物种，适合在低功耗嵌入式设备上部署。


<details>
  <summary>Details</summary>
Motivation: 蚊媒疾病每年造成超过70万人死亡，构成严重的全球健康威胁。传统数据集需要耗费大量人力记录单个蚊子，而合成方法可以实现可扩展的数据生成，同时减少人力资源需求。

Method: 研究引入了概念验证的合成蚊群数据集，模拟真实的多物种和嘈杂蚊群条件。使用对数梅尔频谱图，评估了轻量级深度学习架构用于蚊子物种分类。

Result: 实验表明，这些模型能够有效识别六种主要蚊媒物种，并且适合在嵌入式低功耗设备上部署。

Conclusion: 该研究展示了合成蚊群音频数据集在加速声学蚊子研究和实现可扩展实时监测解决方案方面的潜力。

Abstract: Mosquito-borne diseases pose a serious global health threat, causing over 700,000 deaths annually. This work introduces a proof-of-concept Synthetic Swarm Mosquito Dataset for Acoustic Classification, created to simulate realistic multi-species and noisy swarm conditions. Unlike conventional datasets that require labor-intensive recording of individual mosquitoes, the synthetic approach enables scalable data generation while reducing human resource demands. Using log-mel spectrograms, we evaluated lightweight deep learning architectures for the classification of mosquito species. Experiments show that these models can effectively identify six major mosquito vectors and are suitable for deployment on embedded low-power devices. The study demonstrates the potential of synthetic swarm audio datasets to accelerate acoustic mosquito research and enable scalable real-time surveillance solutions.

</details>


### [88] [The Data Efficiency Frontier of Financial Foundation Models: Scaling Laws from Continued Pretraining](https://arxiv.org/abs/2512.12384)
*Jesse Ponnock*

Main category: cs.LG

TL;DR: 该研究对金融领域自适应预训练进行早期扩展定律分析，使用1B和3B参数的Llama-3.2模型在400M金融语料上训练，发现前200M token收益最大，金融语言高度规律且可高效学习，通用领域性能保持稳定。


<details>
  <summary>Details</summary>
Motivation: 领域自适应预训练为专业化大型语言模型提供实用路径，无需完全重新训练。本研究旨在通过早期扩展定律分析，为金融基础模型的扩展提供实证指导。

Method: 使用1B和3B参数的Llama-3.2模型，在美国SEC文件400M token的金融语料上进行持续预训练，在50M、100M、200M和400M token处设置验证检查点，分析扩展定律。

Result: 两个模型在SEC领域验证损失均持续改善，最大收益出现在前200M token，之后收益递减。幂律拟合显示浅指数，表明金融语言高度规律且可高效学习。通用领域验证损失基本不变，无灾难性遗忘迹象。

Conclusion: 有意义的领域自适应可通过相对适中的token预算实现，较大模型规模（7B-70B）在预计数据需求下仍然可行，为金融基础模型的扩展提供了早期实证指导。

Abstract: Domain-adaptive pretraining (DAPT) offers a practical path to specializing large language models for high-value domains without full retraining. We conduct an early-stage scaling-law analysis of continued pretraining on U.S. SEC filings, training 1B and 3B-parameter Llama-3.2 models on a 400M-token financial corpus with validation checkpoints at 50M, 100M, 200M, and 400M tokens. Results show consistent improvements in SEC-domain validation loss for both models, with the largest gains occurring within the first 200M tokens and diminishing returns thereafter. Power-law fits reveal shallow exponents, indicating that financial language is highly regular and efficiently learnable under continued pretraining. General-domain validation loss remains effectively unchanged across all token budgets, suggesting minimal drift and no signs of catastrophic forgetting. A data-efficiency frontier further shows that both models move toward improved specialization with negligible mixed-domain degradation. Together, these findings provide early empirical guidance for scaling financial foundation models, suggesting that meaningful domain adaptation can be achieved with comparatively modest token budgets and that larger model scales (7B-70B) remain tractable under projected data requirements.

</details>


### [89] [Anchoring Values in Temporal and Group Dimensions for Flow Matching Model Alignment](https://arxiv.org/abs/2512.12387)
*Yawen Shao,Jie Xiao,Kai Zhu,Yu Liu,Wei Zhai,Yang Cao,Zheng-Jun Zha*

Main category: cs.LG

TL;DR: VGPO提出了一种新的强化学习框架，解决了GRPO在图像生成中的两个关键限制：时间信用分配问题和优化停滞问题，通过密集过程感知价值估计和绝对价值增强实现了更好的图像质量和任务特定准确性。


<details>
  <summary>Details</summary>
Motivation: 当前将GRPO应用于基于流匹配的图像生成时存在根本性冲突：1）稀疏终端奖励在所有时间步均匀应用，损害了时间信用分配，忽略了从早期结构形成到后期调整的不同生成阶段的关键性差异；2）仅依赖相对组内奖励导致优化信号随着训练收敛而衰减，当奖励多样性完全耗尽时出现优化停滞。

Method: VGPO框架在时间和组维度上重新定义价值估计：1）将稀疏终端奖励转化为密集、过程感知的价值估计，通过建模每个生成阶段的预期累积奖励实现精确信用分配；2）用绝对价值增强的新过程替代标准组归一化，即使在奖励多样性下降时也能保持稳定的优化信号。

Result: 在三个基准测试上的广泛实验表明，VGPO实现了最先进的图像质量，同时提高了任务特定准确性，有效缓解了奖励黑客问题。

Conclusion: VGPO通过重新设计价值估计机制，成功解决了GRPO在图像生成中的局限性，为基于强化学习的生成模型优化提供了更有效的框架。

Abstract: Group Relative Policy Optimization (GRPO) has proven highly effective in enhancing the alignment capabilities of Large Language Models (LLMs). However, current adaptations of GRPO for the flow matching-based image generation neglect a foundational conflict between its core principles and the distinct dynamics of the visual synthesis process. This mismatch leads to two key limitations: (i) Uniformly applying a sparse terminal reward across all timesteps impairs temporal credit assignment, ignoring the differing criticality of generation phases from early structure formation to late-stage tuning. (ii) Exclusive reliance on relative, intra-group rewards causes the optimization signal to fade as training converges, leading to the optimization stagnation when reward diversity is entirely depleted. To address these limitations, we propose Value-Anchored Group Policy Optimization (VGPO), a framework that redefines value estimation across both temporal and group dimensions. Specifically, VGPO transforms the sparse terminal reward into dense, process-aware value estimates, enabling precise credit assignment by modeling the expected cumulative reward at each generative stage. Furthermore, VGPO replaces standard group normalization with a novel process enhanced by absolute values to maintain a stable optimization signal even as reward diversity declines. Extensive experiments on three benchmarks demonstrate that VGPO achieves state-of-the-art image quality while simultaneously improving task-specific accuracy, effectively mitigating reward hacking. Project webpage: https://yawen-shao.github.io/VGPO/.

</details>


### [90] [Rough Sets for Explainability of Spectral Graph Clustering](https://arxiv.org/abs/2512.12436)
*Bartłomiej Starosta,Sławomir T. Wierzchoń,Piotr Borkowski,Dariusz Czerski,Marcin Sydow,Eryk Laskowski,Mieczysław A. Kłopotek*

Main category: cs.LG

TL;DR: 该论文提出了一种基于粗糙集理论的图谱聚类解释方法增强方案，旨在解决谱空间嵌入难以解释、文档内容不明确以及聚类算法随机性导致的解释性问题。


<details>
  <summary>Details</summary>
Motivation: 图谱聚类方法在处理文本文档时，由于谱空间嵌入与文档内容缺乏明显关联，加上文档内容含义不明确和聚类算法的随机性，导致聚类结果难以向用户解释。

Method: 在团队先前研究的基础上，提出基于粗糙集理论的解释方法增强方案，通过粗糙集理论的思想来克服上述解释性问题。

Result: 论文未在摘要中提供具体实验结果，但提出了能够改善图谱聚类解释性的方法论增强方案。

Conclusion: 通过结合粗糙集理论，该增强方法能够更好地解释图谱聚类结果，特别是在处理文本文档时，解决了谱空间嵌入与文档内容关联不明确的问题。

Abstract: Graph Spectral Clustering methods (GSC) allow representing clusters of diverse shapes, densities, etc. However, the results of such algorithms, when applied e.g. to text documents, are hard to explain to the user, especially due to embedding in the spectral space which has no obvious relation to document contents. Furthermore, the presence of documents without clear content meaning and the stochastic nature of the clustering algorithms deteriorate explainability. This paper proposes an enhancement to the explanation methodology, proposed in an earlier research of our team. It allows us to overcome the latter problems by taking inspiration from rough set theory.

</details>


### [91] [Optimized Architectures for Kolmogorov-Arnold Networks](https://arxiv.org/abs/2512.12448)
*James Bagrow,Josh Bongard*

Main category: cs.LG

TL;DR: 通过过参数化架构结合可微分稀疏化，在保持KANs可解释性的同时提升性能，实现更紧凑、更可解释的模型


<details>
  <summary>Details</summary>
Motivation: 传统KANs架构增强方法增加了复杂性，损害了其核心的可解释性优势，需要在保持可解释性的同时提升模型性能

Method: 采用过参数化架构结合可微分稀疏化技术，将架构搜索转化为端到端优化问题，学习紧凑且可解释的KANs模型

Result: 在函数逼近基准测试、动态系统预测和真实世界预测任务中，实现了竞争性或更优的准确性，同时发现了显著更小的模型

Conclusion: 过参数化与稀疏化具有协同效应，为科学机器学习中表达性与可解释性之间的关键矛盾提供了原则性解决方案

Abstract: Efforts to improve Kolmogorov-Arnold networks (KANs) with architectural enhancements have been stymied by the complexity those enhancements bring, undermining the interpretability that makes KANs attractive in the first place. Here we study overprovisioned architectures combined with sparsification to learn compact, interpretable KANs without sacrificing accuracy. Crucially, we focus on differentiable sparsification, turning architecture search into an end-to-end optimization problem. Across function approximation benchmarks, dynamical systems forecasting, and real-world prediction tasks, we demonstrate competitive or superior accuracy while discovering substantially smaller models. Overprovisioning and sparsification are synergistic, with the combination outperforming either alone. The result is a principled path toward models that are both more expressive and more interpretable, addressing a key tension in scientific machine learning.

</details>


### [92] [Cross-Modal Representational Knowledge Distillation for Enhanced Spike-Informed LFP Modeling](https://arxiv.org/abs/2512.12461)
*Eray Erturk,Saba Hashemi,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 提出跨模态知识蒸馏框架，将多会话尖峰信号Transformer模型的知识迁移到LFP模型，提升LFP模型在神经解码任务中的性能


<details>
  <summary>Details</summary>
Motivation: 局部场电位（LFP）在神经实验中具有重要优势（长期稳定性、抗电极退化、低功耗），但现有建模框架主要关注尖峰信号，因为LFP信号具有聚合性、群体水平特性，导致对下游任务变量（如运动行为）的预测能力较低

Method: 提出跨模态知识蒸馏框架：1）使用掩码自编码目标和会话特定神经标记化策略，在多会话上训练教师尖峰模型；2）将学生LFP模型的潜在表示与教师尖峰模型对齐

Result: 蒸馏后的LFP模型在完全无监督和监督设置中均优于单会话和多会话LFP基线，能够泛化到其他会话而无需额外蒸馏，同时保持优越性能

Conclusion: 跨模态知识蒸馏是利用高性能尖峰模型开发更准确LFP模型的有效且可扩展方法

Abstract: Local field potentials (LFPs) can be routinely recorded alongside spiking activity in intracortical neural experiments, measure a larger complementary spatiotemporal scale of brain activity for scientific inquiry, and can offer practical advantages over spikes, including greater long-term stability, robustness to electrode degradation, and lower power requirements. Despite these advantages, recent neural modeling frameworks have largely focused on spiking activity since LFP signals pose inherent modeling challenges due to their aggregate, population-level nature, often leading to lower predictive power for downstream task variables such as motor behavior. To address this challenge, we introduce a cross-modal knowledge distillation framework that transfers high-fidelity representational knowledge from pretrained multi-session spike transformer models to LFP transformer models. Specifically, we first train a teacher spike model across multiple recording sessions using a masked autoencoding objective with a session-specific neural tokenization strategy. We then align the latent representations of the student LFP model to those of the teacher spike model. Our results show that the Distilled LFP models consistently outperform single- and multi-session LFP baselines in both fully unsupervised and supervised settings, and can generalize to other sessions without additional distillation while maintaining superior performance. These findings demonstrate that cross-modal knowledge distillation is a powerful and scalable approach for leveraging high-performing spike models to develop more accurate LFP models.

</details>


### [93] [Dynamical modeling of nonlinear latent factors in multiscale neural activity with real-time inference](https://arxiv.org/abs/2512.12462)
*Eray Erturk,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 开发了一个多尺度学习框架，用于实时递归解码多模态神经时间序列数据，能够处理不同时间尺度、概率分布和缺失样本的问题。


<details>
  <summary>Details</summary>
Motivation: 实时解码多模态神经时间序列数据（如离散尖峰活动和连续场电位）在神经科学应用中很重要，但现有非线性模型无法处理不同时间尺度、概率分布差异和样本缺失问题，且部分模型不支持实时解码。

Method: 开发了一个包含三个组件的学习框架：1）多尺度编码器，通过学习模态内动态非线性聚合信息，实时处理不同时间尺度和缺失样本；2）多尺度动态主干，提取多模态时间动态并实现实时递归解码；3）模态特定解码器，处理不同模态的概率分布差异。

Result: 在模拟和三个不同的多尺度脑数据集上，该方法能够聚合不同时间尺度、分布和缺失样本的多模态信息，显著改善实时目标解码性能，并优于各种线性和非线性多模态基准方法。

Conclusion: 该框架成功解决了多模态神经数据解码中的关键挑战，包括时间尺度差异、概率分布不同和样本缺失问题，实现了实时递归解码，为神经科学应用提供了有效的多尺度信息整合解决方案。

Abstract: Real-time decoding of target variables from multiple simultaneously recorded neural time-series modalities, such as discrete spiking activity and continuous field potentials, is important across various neuroscience applications. However, a major challenge for doing so is that different neural modalities can have different timescales (i.e., sampling rates) and different probabilistic distributions, or can even be missing at some time-steps. Existing nonlinear models of multimodal neural activity do not address different timescales or missing samples across modalities. Further, some of these models do not allow for real-time decoding. Here, we develop a learning framework that can enable real-time recursive decoding while nonlinearly aggregating information across multiple modalities with different timescales and distributions and with missing samples. This framework consists of 1) a multiscale encoder that nonlinearly aggregates information after learning within-modality dynamics to handle different timescales and missing samples in real time, 2) a multiscale dynamical backbone that extracts multimodal temporal dynamics and enables real-time recursive decoding, and 3) modality-specific decoders to account for different probabilistic distributions across modalities. In both simulations and three distinct multiscale brain datasets, we show that our model can aggregate information across modalities with different timescales and distributions and missing samples to improve real-time target decoding. Further, our method outperforms various linear and nonlinear multimodal benchmarks in doing so.

</details>


### [94] [Exploring the Design Space of Transition Matching](https://arxiv.org/abs/2512.12465)
*Uriel Singer,Yaron Lipman*

Main category: cs.LG

TL;DR: 本文对Transition Matching（TM）生成模型中的头部模块进行了大规模系统研究，通过训练56个不同的17亿参数文生图模型，评估了头部架构、训练策略和采样方法对生成质量、训练和推理效率的影响。


<details>
  <summary>Details</summary>
Motivation: TM作为一种新兴的生成建模范式，虽然比扩散和流匹配模型更具表达力，但其头部模块的设计、训练和采样策略缺乏系统研究。本文旨在通过大规模实验探索TM框架中头部模块的最佳实践。

Method: 采用时间连续双向TM变体，训练了56个不同的17亿参数文生图模型（共549次评估），系统研究了头部模块架构（MLP vs Transformer）、训练时间加权策略以及随机TM采样器家族的影响。

Result: 研究发现：1）MLP头部配合特定时间加权训练和高频采样在所有指标上表现最佳，达到最先进水平；2）Transformer头部配合序列缩放和低频采样在图像美学方面表现优异；3）实验揭示了哪些设计选择能带来最大质量提升，哪些选择收益有限。

Conclusion: 本文为TM生成模型中的头部模块设计提供了实证指导，确定了最佳实践方案，同时指出了哪些设计方向值得进一步探索，哪些可能收益有限，为后续研究提供了重要参考。

Abstract: Transition Matching (TM) is an emerging paradigm for generative modeling that generalizes diffusion and flow-matching models as well as continuous-state autoregressive models. TM, similar to previous paradigms, gradually transforms noise samples to data samples, however it uses a second ``internal'' generative model to implement the transition steps, making the transitions more expressive compared to diffusion and flow models. To make this paradigm tractable, TM employs a large backbone network and a smaller "head" module to efficiently execute the generative transition step. In this work, we present a large-scale, systematic investigation into the design, training and sampling of the head in TM frameworks, focusing on its time-continuous bidirectional variant. Through comprehensive ablations and experimentation involving training 56 different 1.7B text-to-image models (resulting in 549 unique evaluations) we evaluate the affect of the head module architecture and modeling during training as-well as a useful family of stochastic TM samplers. We analyze the impact on generation quality, training, and inference efficiency. We find that TM with an MLP head, trained with a particular time weighting and sampled with high frequency sampler provides best ranking across all metrics reaching state-of-the-art among all tested baselines, while Transformer head with sequence scaling and low frequency sampling is a runner up excelling at image aesthetics. Lastly, we believe the experiments presented highlight the design aspects that are likely to provide most quality and efficiency gains, while at the same time indicate what design choices are not likely to provide further gains.

</details>


### [95] [Sparse Concept Anchoring for Interpretable and Controllable Neural Representations](https://arxiv.org/abs/2512.12469)
*Sandy Fraser,Patryk Wielopolski*

Main category: cs.LG

TL;DR: 提出稀疏概念锚定方法，通过最小监督（每个锚定概念<0.1%的标注样本）将目标概念子集定位到潜在空间特定位置，同时允许其他概念自组织，实现可解释、可操控的表示学习。


<details>
  <summary>Details</summary>
Motivation: 现有表示学习方法缺乏对特定概念的可控性和可解释性，难以在保持其他特征不变的情况下选择性操纵或消除特定概念。需要一种方法能够在最小监督下实现对目标概念的精确控制和干预。

Method: 稀疏概念锚定方法结合激活归一化、分离正则化器以及锚定/子空间正则化器。锚定正则化器将稀有标注样本吸引到预定义方向，子空间正则化器将其吸引到轴对齐子空间。训练过程使用极少量监督（每个锚定概念<0.1%的标注样本）。

Result: 在结构化自编码器上的实验表明，该方法能够选择性衰减目标概念而对正交特征影响极小，完全消除概念时重建误差接近理论界限。锚定几何结构支持两种实际干预：可逆行为操控（在推理时投影掉概念的潜在成分）和永久移除（通过锚定维度的针对性权重消融）。

Conclusion: 稀疏概念锚定为学习表示中的可解释、可操控行为提供了实用途径，能够在最小监督下实现对特定概念的精确控制，同时保持其他特征的自组织能力。

Abstract: We introduce Sparse Concept Anchoring, a method that biases latent space to position a targeted subset of concepts while allowing others to self-organize, using only minimal supervision (labels for <0.1% of examples per anchored concept). Training combines activation normalization, a separation regularizer, and anchor or subspace regularizers that attract rare labeled examples to predefined directions or axis-aligned subspaces. The anchored geometry enables two practical interventions: reversible behavioral steering that projects out a concept's latent component at inference, and permanent removal via targeted weight ablation of anchored dimensions. Experiments on structured autoencoders show selective attenuation of targeted concepts with negligible impact on orthogonal features, and complete elimination with reconstruction error approaching theoretical bounds. Sparse Concept Anchoring therefore provides a practical pathway to interpretable, steerable behavior in learned representations.

</details>


### [96] [AI-Driven Early Warning Systems for Student Success: Discovering Static Feature Dominance in Temporal Prediction Models](https://arxiv.org/abs/2512.12493)
*Vaarunay Kaushal,Rajib Mall*

Main category: cs.LG

TL;DR: 该研究比较了决策树和LSTM模型在在线学习环境中识别风险学生的表现，发现不同干预阶段需要不同的性能指标：早期需要高召回率，中期需要平衡的精确率-召回率，后期需要高精确率。


<details>
  <summary>Details</summary>
Motivation: 在线学习环境中早期识别风险学生对于有效干预至关重要，需要了解不同时间点的预测模型表现差异，以优化干预策略。

Method: 研究将时间预测分析扩展到课程第20周（50%课程时长），在六个时间快照上比较决策树和LSTM模型，分析静态人口统计特征和动态学习数据的影响。

Result: 静态人口统计特征占预测重要性的68%，可实现无需评估的早期预测。LSTM在第2周达到97%召回率，适合早期干预；决策树在中期提供稳定的平衡性能（78%准确率）。到第20周，两种模型召回率相似（68%），但LSTM精确率更高（90% vs 86%）。

Conclusion: 模型选择应取决于干预时机，早期信号（第2-4周）结合人口统计和入学前信息足以进行可靠初始预测，不同干预阶段需要不同的性能指标优化策略。

Abstract: Early identification of at-risk students is critical for effective intervention in online learning environments. This study extends temporal prediction analysis to Week 20 (50% of course duration), comparing Decision Tree and Long Short- Term Memory (LSTM) models across six temporal snapshots. Our analysis reveals that different performance metrics matter at different intervention stages: high recall is critical for early intervention (Weeks 2-4), while balanced precision-recall is important for mid-course resource allocation (Weeks 8-16), and high precision becomes paramount in later stages (Week 20). We demonstrate that static demographic features dominate predictions (68% importance), enabling assessment-free early prediction. The LSTM model achieves 97% recall at Week 2, making it ideal for early intervention, while Decision Tree provides stable balanced performance (78% accuracy) during mid-course. By Week 20, both models converge to similar recall (68%), but LSTM achieves higher precision (90% vs 86%). Our findings also suggest that model selection should depend on intervention timing, and that early signals (Weeks 2-4) are sufficient for reliable initial prediction using primarily demographic and pre-enrollment information.

</details>


### [97] [Policy Optimization for Dynamic Heart Transplant Allocation](https://arxiv.org/abs/2512.12497)
*Ioannis Anagnostides,Zachary W. Sollie,Arman Kilic,Tuomas Sandholm*

Main category: cs.LG

TL;DR: 本文开发了一个新的心脏移植分配政策模拟器，发现现行政策远不如最大化移植获益年数的近视政策，并提出使用潜力值改进动态分配，同时发现批量处理供体可进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 心脏移植是晚期心衰患者的可行治疗路径，但供体严重短缺。尽管2018年修订了分配政策，但现行政策未能充分考虑移植前后的死亡率，需要改进分配策略以优化患者获益。

Method: 使用UNOS历史数据开发新的模拟器来评估不同分配政策；比较现行政策与近视政策（最大化移植获益年数）；引入潜力值概念来考虑分配的动态特性；探索批量处理供体的效果；评估地理邻近性和移植中心拒绝倾向等关键因素。

Result: 现行分配政策明显劣于近视政策；使用潜力值改进的动态分配政策表现更优；批量处理少量供体可进一步提升分配性能；模拟器能够有效评估地理邻近性和拒绝倾向等关键因素对分配效果的影响。

Conclusion: 通过开发新的模拟器，本文为改进心脏移植分配政策提供了重要工具，证明考虑动态分配特性和批量处理供体可以显著提升分配效率，为未来政策优化提供了实证基础。

Abstract: Heart transplantation is a viable path for patients suffering from advanced heart failure, but this lifesaving option is severely limited due to donor shortage. Although the current allocation policy was recently revised in 2018, a major concern is that it does not adequately take into account pretransplant and post-transplant mortality. In this paper, we take an important step toward addressing these deficiencies.
  To begin with, we use historical data from UNOS to develop a new simulator that enables us to evaluate and compare the performance of different policies. We then leverage our simulator to demonstrate that the status quo policy is considerably inferior to the myopic policy that matches incoming donors to the patient who maximizes the number of years gained by the transplant. Moreover, we develop improved policies that account for the dynamic nature of the allocation process through the use of potentials -- a measure of a patient's utility in future allocations that we introduce. We also show that batching together even a handful of donors -- which is a viable option for a certain type of donors -- further enhances performance. Our simulator also allows us to evaluate the effect of critical, and often unexplored, factors in the allocation, such as geographic proximity and the tendency to reject offers by the transplant centers.

</details>


### [98] [Skillful Subseasonal-to-Seasonal Forecasting of Extreme Events with a Multi-Sphere Coupled Probabilistic Model](https://arxiv.org/abs/2512.12545)
*Bin Mu,Yuxuan Chen,Shijin Yuan,Bo Qin,Hao Guo*

Main category: cs.LG

TL;DR: TianXing-S2S是一个多圈层耦合的概率模型，用于全球次季节到季节(S2S)的每日集合预报，在45天预报中优于ECMWF和FuXi-S2S系统。


<details>
  <summary>Details</summary>
Motivation: 在气候变化加速的背景下，准确的次季节到季节极端事件预测对于资源规划和灾害缓解至关重要，但由于复杂的多圈层相互作用和固有的大气不确定性，这种预测仍然具有挑战性。

Method: 首先将多样化的多圈层预测因子编码到紧凑的潜在空间中，然后使用扩散模型生成每日集合预报。在去噪器中加入基于最优传输(OT)的新型耦合模块，以优化大气与多圈层边界条件之间的相互作用。

Result: 在1.5度分辨率下，TianXing-S2S在45天每日平均集合预报中，在关键大气变量上优于欧洲中期天气预报中心(ECMWF) S2S系统和FuXi-S2S。模型能够熟练预测极端事件（如热浪和异常降水），并识别土壤湿度作为关键的前兆信号。此外，模型能够生成长达180天的稳定滚动预报。

Conclusion: TianXing-S2S建立了一个稳健的S2S研究框架，为变暖世界中的次季节到季节预测提供了有效工具，特别是在极端事件预测方面表现出色。

Abstract: Accurate subseasonal-to-seasonal (S2S) prediction of extreme events is critical for resource planning and disaster mitigation under accelerating climate change. However, such predictions remain challenging due to complex multi-sphere interactions and intrinsic atmospheric uncertainty. Here we present TianXing-S2S, a multi-sphere coupled probabilistic model for global S2S daily ensemble forecast. TianXing-S2S first encodes diverse multi-sphere predictors into a compact latent space, then employs a diffusion model to generate daily ensemble forecasts. A novel coupling module based on optimal transport (OT) is incorporated in the denoiser to optimize the interactions between atmospheric and multi-sphere boundary conditions. Across key atmospheric variables, TianXing-S2S outperforms both the European Centre for Medium-Range Weather Forecasts (ECMWF) S2S system and FuXi-S2S in 45-day daily-mean ensemble forecasts at 1.5 resolution. Our model achieves skillful subseasonal prediction of extreme events including heat waves and anomalous precipitation, identifying soil moisture as a critical precursor signal. Furthermore, we demonstrate that TianXing-S2S can generate stable rollout forecasts up to 180 days, establishing a robust framework for S2S research in a warming world.

</details>


### [99] [Optimal Mistake Bounds for Transductive Online Learning](https://arxiv.org/abs/2512.12567)
*Zachary Chase,Steve Hanneke,Shay Moran,Jonathan Shafer*

Main category: cs.LG

TL;DR: 该论文解决了在线学习中无标签数据作用这一30年未解问题，精确量化了转导学习与标准在线学习之间的差距，证明了转导学习的错误下界为Ω(√d)，上界为O(√d)，建立了二次方差距。


<details>
  <summary>Details</summary>
Motivation: 解决在线学习中无标签数据作用这一30年未解问题，量化转导学习与标准在线学习之间的性能差距，理解提前获取未标记实例序列的优势。

Method: 通过理论分析证明转导学习的错误下界为Ω(√d)，同时构造具体概念类证明上界为O(√d)，建立完整的理论框架。

Result: 证明了转导学习错误下界为Ω(√d)，上界为O(√d)，相比之前的下界Ω(log log d)、Ω(√log d)、Ω(log d)有指数级改进，相比之前的上界(2/3)d也有显著改进。

Conclusion: 转导学习与标准在线学习之间存在二次方差距，这突显了提前获取未标记实例序列的优势，与PAC学习设置中两者样本复杂度相似的情况形成对比。

Abstract: We resolve a 30-year-old open problem concerning the power of unlabeled data in online learning by tightly quantifying the gap between transductive and standard online learning. In the standard setting, the optimal mistake bound is characterized by the Littlestone dimension $d$ of the concept class $H$ (Littlestone 1987). We prove that in the transductive setting, the mistake bound is at least $Ω(\sqrt{d})$. This constitutes an exponential improvement over previous lower bounds of $Ω(\log\log d)$, $Ω(\sqrt{\log d})$, and $Ω(\log d)$, due respectively to Ben-David, Kushilevitz, and Mansour (1995, 1997) and Hanneke, Moran, and Shafer (2023). We also show that this lower bound is tight: for every $d$, there exists a class of Littlestone dimension $d$ with transductive mistake bound $O(\sqrt{d})$. Our upper bound also improves upon the best known upper bound of $(2/3)d$ from Ben-David, Kushilevitz, and Mansour (1997). These results establish a quadratic gap between transductive and standard online learning, thereby highlighting the benefit of advance access to the unlabeled instance sequence. This contrasts with the PAC setting, where transductive and standard learning exhibit similar sample complexities.

</details>


### [100] [On the Accuracy of Newton Step and Influence Function Data Attributions](https://arxiv.org/abs/2512.12572)
*Ittai Rubinstein,Samuel B. Hopkins*

Main category: cs.LG

TL;DR: 本文对数据归因方法（NS和IF）进行了新的理论分析，首次在不假设全局强凸性的条件下，为凸学习问题提供了渐近紧致的误差界限，解释了NS方法通常比IF更准确的现象。


<details>
  <summary>Details</summary>
Motivation: 现有数据归因方法（如IF和NS）的理论分析存在两个主要局限：1）依赖全局强凸性假设，这在实践中常不满足；2）误差界限随参数数量d和移除样本数k的缩放效果很差。这导致无法回答"各方法的误差渐近缩放规律是什么？"以及"哪种方法对给定数据集更准确？"等基本问题。

Method: 针对凸学习问题，提出了对NS和IF数据归因方法的新分析框架。该分析不依赖全局强凸性假设，并推导出渐近紧致的误差界限（在多项式对数因子内）。特别针对行为良好的逻辑回归，证明了界限的紧致性。

Result: 获得了两个关键渐近结果：1）NS方法与真实参数之间的期望误差为Θ̃(kd/n²)；2）NS与IF方法之间的期望误差为Θ̃((k+d)√(kd)/n²)。这些结果解释了[KATL19]和[RH25a]观察到的NS通常比IF更准确的现象。

Conclusion: 本文首次在不假设全局强凸性的条件下，为数据归因方法提供了渐近紧致的理论分析，建立了误差的缩放规律，并解释了NS方法相对于IF的优越性，为数据归因的理论理解提供了重要进展。

Abstract: Data attribution aims to explain model predictions by estimating how they would change if certain training points were removed, and is used in a wide range of applications, from interpretability and credit assignment to unlearning and privacy.
  Even in the relatively simple case of linear regressions, existing mathematical analyses of leading data attribution methods such as Influence Functions (IF) and single Newton Step (NS) remain limited in two key ways. First, they rely on global strong convexity assumptions which are often not satisfied in practice. Second, the resulting bounds scale very poorly with the number of parameters ($d$) and the number of samples removed ($k$). As a result, these analyses are not tight enough to answer fundamental questions such as "what is the asymptotic scaling of the errors of each method?" or "which of these methods is more accurate for a given dataset?"
  In this paper, we introduce a new analysis of the NS and IF data attribution methods for convex learning problems. To the best of our knowledge, this is the first analysis of these questions that does not assume global strong convexity and also the first explanation of [KATL19] and [RH25a]'s observation that NS data attribution is often more accurate than IF. We prove that for sufficiently well-behaved logistic regression, our bounds are asymptotically tight up to poly-logarithmic factors, yielding scaling laws for the errors in the average-case sample removals.
  \[ \mathbb{E}_{T \subseteq [n],\, |T| = k} \bigl[ \|\hatθ_T - \hatθ_T^{\mathrm{NS}}\|_2 \bigr] = \widetildeΘ\!\left(\frac{k d}{n^2}\right), \qquad \mathbb{E}_{T \subseteq [n],\, |T| = k} \bigl[ \|\hatθ_T^{\mathrm{NS}} - \hatθ_T^{\mathrm{IF}}\|_2 \bigr] = \widetildeΘ\!\left( \frac{(k + d)\sqrt{k d}}{n^2} \right). \]

</details>


### [101] [Causal inference and model explainability tools for retail](https://arxiv.org/abs/2512.12605)
*Pranav Gupta,Nithin Surendran*

Main category: cs.LG

TL;DR: 论文提出将模型可解释性与因果推断结合应用于零售业销售洞察的框架，通过可解释模型降低SHAP值方差，并使用双重机器学习方法处理多个混杂因素以获得正确的因果效应符号。


<details>
  <summary>Details</summary>
Motivation: 当前零售商虽然收集了大量多维度数据并使用机器学习技术分析关键指标，但现有模型缺乏可解释性，且无法验证或发现因果关系。这限制了从数据中获得深入业务洞察的能力。

Method: 1. 回顾电子商务和零售领域因果推断与模型可解释性的现有文献；2. 将相关方法应用于真实世界数据集；3. 使用固有可解释模型降低SHAP值方差；4. 采用双重机器学习方法处理多个混杂因素。

Result: 研究发现：1. 固有可解释模型具有更低的SHAP值方差；2. 通过双重机器学习方法包含多个混杂因素能够获得正确的因果效应符号。

Conclusion: 论文提供了一个将模型可解释性与因果推断结合应用于零售业销售洞察的实用框架，证明了可解释模型在稳定性方面的优势以及双重机器学习在因果推断中的有效性。

Abstract: Most major retailers today have multiple divisions focused on various aspects, such as marketing, supply chain, online customer experience, store customer experience, employee productivity, and vendor fulfillment. They also regularly collect data corresponding to all these aspects as dashboards and weekly/monthly/quarterly reports. Although several machine learning and statistical techniques have been in place to analyze and predict key metrics, such models typically lack interpretability. Moreover, such techniques also do not allow the validation or discovery of causal links. In this paper, we aim to provide a recipe for applying model interpretability and causal inference for deriving sales insights. In this paper, we review the existing literature on causal inference and interpretability in the context of problems in e-commerce and retail, and apply them to a real-world dataset. We find that an inherently explainable model has a lower variance of SHAP values, and show that including multiple confounders through a double machine learning approach allows us to get the correct sign of causal effect.

</details>


### [102] [DynaGen: Unifying Temporal Knowledge Graph Reasoning with Dynamic Subgraphs and Generative Regularization](https://arxiv.org/abs/2512.12669)
*Jiawei Shen,Jia Zhu,Hanghui Guo,Weijie Shi,Guoqing Ma,Yidan Liang,Jingjiang Liu,Hao Chen,Shimin Di*

Main category: cs.LG

TL;DR: DynaGen是一个统一的时序知识图谱推理方法，通过动态构建实体中心子图和条件扩散过程，分别解决内插和外推任务中的上下文建模不足和认知泛化偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有时序知识图谱推理方法面临两个关键挑战：内插方法中有限的上下文建模，以及外推方法中的认知泛化偏差。内插方法通常将时间信息嵌入到单个事实中，而外推方法则依赖序列模型捕捉重复模式，但都未能充分解决这些问题。

Method: DynaGen采用统一框架：对于内插任务，动态构建实体中心子图，并使用协同双分支GNN编码器捕捉演化结构上下文；对于外推任务，应用条件扩散过程，迫使模型学习底层演化原理而非表面模式。

Result: 在六个基准数据集上的实验表明，DynaGen实现了最先进的性能。与第二好的模型相比，平均内插任务的MRR提高了2.61分，外推任务的MRR提高了1.45分。

Conclusion: DynaGen通过动态上下文建模和条件扩散过程，有效解决了时序知识图谱推理中的内插和外推挑战，为统一处理这两类任务提供了有效框架。

Abstract: Temporal Knowledge Graph Reasoning (TKGR) aims to complete missing factual elements along the timeline. Depending on the temporal position of the query, the task is categorized into interpolation and extrapolation. Existing interpolation methods typically embed temporal information into individual facts to complete missing historical knowledge, while extrapolation techniques often leverage sequence models over graph snapshots to identify recurring patterns for future event prediction. These methods face two critical challenges: limited contextual modeling in interpolation and cognitive generalization bias in extrapolation. To address these, we propose a unified method for TKGR, dubbed DynaGen. For interpolation, DynaGen dynamically constructs entity-centric subgraphs and processes them with a synergistic dual-branch GNN encoder to capture evolving structural context. For extrapolation, it applies a conditional diffusion process, which forces the model to learn underlying evolutionary principles rather than just superficial patterns, enhancing its ability to predict unseen future events. Extensive experiments on six benchmark datasets show DynaGen achieves state-of-the-art performance. On average, compared to the second-best models, DynaGen improves the Mean Reciprocal Rank (MRR) score by 2.61 points for interpolation and 1.45 points for extrapolation.

</details>


### [103] [Theoretical Foundations of Prompt Engineering: From Heuristics to Expressivity](https://arxiv.org/abs/2512.12688)
*Dongseok Kim,Hyoungsun Choi,Mohamed Jismy Aashik Rasool,Gisung Oh*

Main category: cs.LG

TL;DR: 论文提出将提示视为外部注入程序的框架，证明单个固定Transformer骨干仅通过提示就能近似广泛目标行为


<details>
  <summary>Details</summary>
Motivation: 研究提示如何在不改变模型权重的情况下切换模型行为，将这一现象作为理论对象而非启发式方法进行形式化分析

Method: 将提示视为外部程序，构建简化Transformer解释器，揭示注意力机制执行选择性路由、FFN执行局部算术运算、深度堆叠组合局部更新的机制分解

Result: 证明构造性存在性结果：单个固定骨干仅通过提示就能近似广泛的目标行为类别

Conclusion: 该框架为形式化提示长度/精度约束下的权衡、研究基于提示切换的结构限制提供了统一起点，与预训练LLM的实证主张保持区分

Abstract: Prompts can switch a model's behavior even when the weights are fixed, yet this phenomenon is rarely treated as a clean theoretical object rather than a heuristic. We study the family of functions obtainable by holding a Transformer backbone fixed as an executor and varying only the prompt. Our core idea is to view the prompt as an externally injected program and to construct a simplified Transformer that interprets it to implement different computations. The construction exposes a mechanism-level decomposition: attention performs selective routing from prompt memory, the FFN performs local arithmetic conditioned on retrieved fragments, and depth-wise stacking composes these local updates into a multi-step computation. Under this viewpoint, we prove a constructive existential result showing that a single fixed backbone can approximate a broad class of target behaviors via prompts alone. The framework provides a unified starting point for formalizing trade-offs under prompt length/precision constraints and for studying structural limits of prompt-based switching, while remaining distinct from empirical claims about pretrained LLMs.

</details>


### [104] [Reassessing the Role of Supervised Fine-Tuning: An Empirical Study in VLM Reasoning](https://arxiv.org/abs/2512.12690)
*Yongcan Yu,Lingxiao He,Shuo Lu,Lijun Sheng,Yinuo Xu,Yanbo Wang,Kuangpu Guo,Jianjie Cheng,Meng Wang,Qianlong Xie,Xingxing Wang,Dapeng Hu,Jian Liang*

Main category: cs.LG

TL;DR: 本文重新审视了视觉语言模型推理训练中RL主导的范式，通过系统对比发现SFT在多个场景下仍具关键作用，挑战了"RL优于SFT"的主流观点。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型推理领域过度依赖强化学习，普遍认为监督微调不仅无效甚至有害。本研究旨在重新评估SFT在VLM推理训练中的实际价值，挑战RL主导的范式。

Method: 采用系统化、受控的实验设计，在相同数据源下对比SFT和RL方法，考察模型容量、数据规模和分布等因素对两种方法效果的影响。

Result: 研究发现：1）SFT对较弱模型更有效；2）SFT仅需2K数据即可达到RL使用20K数据的性能；3）SFT具有更强的跨模态泛化能力；4）RL存在奖励欺骗问题，高奖励不总是对应高推理准确率。

Conclusion: SFT在VLM推理训练中的作用被低估，不应简单采用"RL优于SFT"的观点。SFT和RL应作为互补组件，构建更平衡的后训练流程。

Abstract: Recent advances in vision-language models (VLMs) reasoning have been largely attributed to the rise of reinforcement Learning (RL), which has shifted the community's focus away from the supervised fine-tuning (SFT) paradigm. Many studies suggest that introducing the SFT stage not only fails to improve reasoning ability but may also negatively impact model training. In this study, we revisit this RL-centric belief through a systematic and controlled comparison of SFT and RL on VLM Reasoning. Using identical data sources, we find that the relative effectiveness of SFT and RL is conditional and strongly influenced by model capacity, data scale, and data distribution. Contrary to common assumptions, our findings show that SFT plays a crucial role across several scenarios: (1) Effectiveness for weaker models. SFT more reliably elicits reasoning capabilities in smaller or weaker VLMs. (2) Data efficiency. SFT with only 2K achieves comparable or better reasoning performance to RL with 20K. (3) Cross-modal transferability. SFT demonstrates stronger generalization across modalities. Moreover, we identify a pervasive issue of deceptive rewards, where higher rewards fail to correlate with better reasoning accuracy in RL. These results challenge the prevailing "RL over SFT" narrative. They highlight that the role of SFT may have been underestimated and support a more balanced post-training pipeline in which SFT and RL function as complementary components.

</details>


### [105] [Co-Exploration and Co-Exploitation via Shared Structure in Multi-Task Bandits](https://arxiv.org/abs/2512.12693)
*Sumantrak Mukherjee,Serafima Lebedeva,Valentin Margraf,Jonas Hanselle,Kanta Yamaoka,Viktor Bengs,Stefan Konigorski,Eyke Hüllermeier,Sebastian Josef Vollmer*

Main category: cs.LG

TL;DR: 提出了一种用于上下文多任务多臂老虎机的高效探索贝叶斯框架，该框架能处理部分可观测上下文，并通过潜在上下文变量利用任务间的结构依赖关系。


<details>
  <summary>Details</summary>
Motivation: 在上下文多任务多臂老虎机设置中，上下文通常只能部分观测，且任务间的奖励分布存在由潜在上下文变量诱导的结构依赖关系。现有方法未能充分利用这些依赖关系，特别是在模型设定错误或复杂潜在异质性情况下表现不佳。

Method: 提出了一种贝叶斯框架，通过整合所有任务的观测数据学习全局联合分布，同时允许对新任务进行个性化推断。使用基于粒子的对数密度高斯过程近似来表示任务和奖励的联合分布，无需对潜在变量做先验假设，能够灵活发现臂间和任务间的依赖关系。

Result: 实证结果表明，该方法在模型设定错误或复杂潜在异质性设置下，优于分层模型老虎机等基线方法。

Conclusion: 该框架通过识别和利用结构不确定性和用户特定不确定性，实现了在多任务多臂老虎机中的高效探索，特别是在存在复杂依赖关系和部分可观测上下文的情况下表现优异。

Abstract: We propose a novel Bayesian framework for efficient exploration in contextual multi-task multi-armed bandit settings, where the context is only observed partially and dependencies between reward distributions are induced by latent context variables. In order to exploit these structural dependencies, our approach integrates observations across all tasks and learns a global joint distribution, while still allowing personalised inference for new tasks. In this regard, we identify two key sources of epistemic uncertainty, namely structural uncertainty in the latent reward dependencies across arms and tasks, and user-specific uncertainty due to incomplete context and limited interaction history. To put our method into practice, we represent the joint distribution over tasks and rewards using a particle-based approximation of a log-density Gaussian process. This representation enables flexible, data-driven discovery of both inter-arm and inter-task dependencies without prior assumptions on the latent variables. Empirically, we demonstrate that our method outperforms baselines such as hierarchical model bandits, especially in settings with model misspecification or complex latent heterogeneity.

</details>


### [106] [Federated Learning with Feedback Alignment](https://arxiv.org/abs/2512.12762)
*Incheol Baek,Hyungbin Kim,Minseo Kim,Yon Dohn Chung*

Main category: cs.LG

TL;DR: FLFA框架将反馈对齐机制融入联邦学习，通过全局模型权重作为共享反馈矩阵来对齐本地更新，有效缓解非IID数据导致的本地漂移问题


<details>
  <summary>Details</summary>
Motivation: 联邦学习在数据隐私保护方面具有优势，但在非独立同分布（non-IID）数据场景下，客户端数据异质性会导致本地漂移问题，阻碍全局模型收敛

Method: 提出FLFA框架，在本地训练的反向传播过程中使用全局模型权重作为共享反馈矩阵，对齐本地更新与全局模型，以最小计算成本和零额外通信开销缓解本地漂移

Result: 理论分析表明FLFA能有效缓解本地漂移并保证本地和全局模型的鲁棒收敛；实证评估显示FLFA能提升其他FL方法的准确性并减少本地漂移

Conclusion: FLFA框架通过反馈对齐机制有效解决了联邦学习中的数据异质性问题，在保持隐私保护的同时提升了模型性能

Abstract: Federated Learning (FL) enables collaborative training across multiple clients while preserving data privacy, yet it struggles with data heterogeneity, where clients' data are not distributed independently and identically (non-IID). This causes local drift, hindering global model convergence. To address this, we introduce Federated Learning with Feedback Alignment (FLFA), a novel framework that integrates feedback alignment into FL. FLFA uses the global model's weights as a shared feedback matrix during local training's backward pass, aligning local updates with the global model efficiently. This approach mitigates local drift with minimal additional computational cost and no extra communication overhead.
  Our theoretical analysis supports FLFA's design by showing how it alleviates local drift and demonstrates robust convergence for both local and global models. Empirical evaluations, including accuracy comparisons and measurements of local drift, further illustrate that FLFA can enhance other FL methods demonstrating its effectiveness.

</details>


### [107] [Multi-Trajectory Physics-Informed Neural Networks for HJB Equations with Hard-Zero Terminal Inventory: Optimal Execution on Synthetic & SPY Data](https://arxiv.org/abs/2512.12708)
*Anthime Valin*

Main category: cs.LG

TL;DR: 提出MT-PINN方法解决最优交易执行中的硬零终端库存约束问题，通过轨迹损失和终端惩罚直接强化零库存约束，在理论和实证中均表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统PINN方法在处理硬零终端库存约束时存在约束执行不足和控制不稳定的问题，需要更有效的方法来确保交易执行结束时库存为零。

Method: 提出多轨迹PINN（MT-PINN），添加基于滚动的轨迹损失，通过时间反向传播传播终端惩罚，直接强化零终端库存约束，并采用轻量级lambda课程学习稳定训练。

Result: 在Gatheral-Schied单资产模型中，MT-PINN与闭式解高度一致，终端库存紧密集中在零附近，沿最优路径误差降低。在SPY日内数据应用中，风险中性时匹配TWAP，高风险厌恶时实现更低敞口和竞争性成本。

Conclusion: MT-PINN能有效解决最优交易执行中的硬零终端库存约束问题，在理论和实际市场数据中均表现出色，为风险厌恶交易者提供了更好的执行策略。

Abstract: We study optimal trade execution with a hard-zero terminal inventory constraint, modeled via Hamilton-Jacobi-Bellman (HJB) equations. Vanilla PINNs often under-enforce this constraint and produce unstable controls. We propose a Multi-Trajectory PINN (MT-PINN) that adds a rollout-based trajectory loss and propagates a terminal penalty on terminal inventory via backpropagation-through-time, directly enforcing zero terminal inventory. A lightweight lambda-curriculum is adopted to stabilize training as the state expands from a risk-neutral reduced HJB to a risk-averse HJB. On the Gatheral-Schied single-asset model, MT-PINN aligns closely with their derived closed-form solutions and concentrates terminal inventory tightly around zero while reducing errors along optimal paths. We apply MT-PINNs on SPY intraday data, matching TWAP when risk-neutral, and achieving lower exposure and competitive costs, especially in falling windows, for higher risk-aversion.

</details>


### [108] [OLC-WA: Drift Aware Tuning-Free Online Classification with Weighted Average](https://arxiv.org/abs/2512.12785)
*Mohammad Abu Shaira,Yunhe Feng,Heng Fan,Weishi Shi*

Main category: cs.LG

TL;DR: OLC-WA是一种无需超参数的自适应在线分类模型，通过加权平均和自动优化机制处理数据流中的概念漂移问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集常呈现随时间变化的数据分布（概念漂移），忽视这一问题会显著降低模型预测准确性。传统在线模型的超参数通常是固定的，无法根据数据分布变化动态调整。

Method: 提出OLC-WA模型，通过指数加权移动平均将新数据流与现有基础模型融合。集成优化机制能动态检测概念漂移、量化其程度，并根据数据流特征调整模型。

Result: 在多种基准数据集上的实验表明，OLC-WA在稳定环境中性能接近批量模型（准确率相差1-3%），在概念漂移情况下优于主流在线基线模型10-25%。

Conclusion: OLC-WA是一种有效的自适应在线分类模型，无需超参数调优即可在动态数据流环境中有效适应数据分布变化，在概念漂移情况下表现优异。

Abstract: Real-world data sets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. This paper introduces Online Classification with Weighted Average (OLC-WA), an adaptive, hyperparameter-free online classification model equipped with an automated optimization mechanism. OLC-WA operates by blending incoming data streams with an existing base model. This blending is facilitated by an exponentially weighted moving average. Furthermore, an integrated optimization mechanism dynamically detects concept drift, quantifies its magnitude, and adjusts the model based on the observed data stream characteristics. This approach empowers the model to effectively adapt to evolving data distributions within streaming environments. Rigorous empirical evaluation across diverse benchmark datasets shows that OLC-WA achieves performance comparable to batch models in stationary environments, maintaining accuracy within 1-3%, and surpasses leading online baselines by 10-25% under drift, demonstrating its effectiveness in adapting to dynamic data streams.

</details>


### [109] [Solving a Machine Learning Regression Problem Based on the Theory of Random Functions](https://arxiv.org/abs/2512.12731)
*Yuriy N. Bakhvalov*

Main category: cs.LG

TL;DR: 该论文从随机函数理论框架出发，将机器学习回归问题视为多元逼近问题，基于无差别原理推导出回归方法，证明具有特定对称性的概率测度会自然导出广义多调和样条核函数。


<details>
  <summary>Details</summary>
Motivation: 为机器学习回归方法提供理论基础，从第一性原理出发推导回归方法，避免经验性选择核函数和正则化形式，建立无先验信息下的最优性理论。

Method: 采用随机函数理论框架，基于无差别原理的公设，假设无限维函数空间上的概率测度具有平移、旋转、缩放不变性和高斯性，从这些对称性出发解析推导回归方法。

Result: 推导出的核函数与广义多调和样条一致，但不同于现有经验选择方法，该核函数是无差别原理的自然结果。同时确定了正则化形式和噪声参数化方式，为一大类平滑和插值方法提供了理论基础。

Conclusion: 在缺乏先验信息的情况下，具有自然对称性的概率测度会唯一确定回归方法，包括核函数形式、正则化类型和噪声参数化，这为相关方法提供了最优性理论基础。

Abstract: This paper studies a machine learning regression problem as a multivariate approximation problem using the framework of the theory of random functions. An ab initio derivation of a regression method is proposed, starting from postulates of indifference. It is shown that if a probability measure on an infinite-dimensional function space possesses natural symmetries (invariance under translation, rotation, scaling, and Gaussianity), then the entire solution scheme, including the kernel form, the type of regularization, and the noise parameterization, follows analytically from these postulates. The resulting kernel coincides with a generalized polyharmonic spline; however, unlike existing approaches, it is not chosen empirically but arises as a consequence of the indifference principle. This result provides a theoretical foundation for a broad class of smoothing and interpolation methods, demonstrating their optimality in the absence of a priori information.

</details>


### [110] [Unveiling Statistical Significance of Online Regression over Multiple Datasets](https://arxiv.org/abs/2512.12787)
*Mohammad Abu-Shaira,Weishi Shi*

Main category: cs.LG

TL;DR: 该论文针对机器学习研究中被忽视的多算法跨数据集比较问题，特别是在线学习领域，提出使用Friedman检验及事后检验来评估多个在线回归模型在不同数据集上的性能差异，并通过真实和合成数据集验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 机器学习研究通常只关注单个数据集上两个学习算法的性能评估，而多算法跨数据集比较的统计检验方法被严重忽视。在线学习领域尤其需要统计显著性验证来确保连续学习过程的可靠性，特别是在快速收敛和及时处理概念漂移方面。

Method: 采用Friedman检验及相应的事后检验来比较多个在线回归模型在不同数据集上的性能。使用真实和合成数据集，通过5折交叉验证和种子平均确保评估的全面性，覆盖各种数据子集。

Result: 测试结果总体上证实了竞争基线的性能与其各自报告一致，但某些统计检验结果也表明，最先进的方法在某些方面仍有改进空间。

Conclusion: 该研究填补了多算法跨数据集比较统计检验的空白，为在线学习领域提供了有效的评估框架，同时指出了当前最先进方法仍需进一步优化的方向。

Abstract: Despite extensive focus on techniques for evaluating the performance of two learning algorithms on a single dataset, the critical challenge of developing statistical tests to compare multiple algorithms across various datasets has been largely overlooked in most machine learning research. Additionally, in the realm of Online Learning, ensuring statistical significance is essential to validate continuous learning processes, particularly for achieving rapid convergence and effectively managing concept drifts in a timely manner. Robust statistical methods are needed to assess the significance of performance differences as data evolves over time. This article examines the state-of-the-art online regression models and empirically evaluates several suitable tests. To compare multiple online regression models across various datasets, we employed the Friedman test along with corresponding post-hoc tests. For thorough evaluations, utilizing both real and synthetic datasets with 5-fold cross-validation and seed averaging ensures comprehensive assessment across various data subsets. Our tests generally confirmed the performance of competitive baselines as consistent with their individual reports. However, some statistical test results also indicate that there is still room for improvement in certain aspects of state-of-the-art methods.

</details>


### [111] [Liquid Reasoning Transformers: A Sudoku-Based Prototype for Chess-Scale Algorithmic Tasks](https://arxiv.org/abs/2512.12792)
*Shivansh Sahni,Wenzhi Zhang*

Main category: cs.LG

TL;DR: LRT是一种具有自适应深度推理能力的Transformer架构，通过迭代更新、丢弃修正和学习停止机制，在数独任务上达到98.68%数字准确率和36.30%完整谜题准确率。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在推理任务中通常使用单一前向传播，难以根据输入难度自适应分配计算资源，也无法修正早期错误。LRT旨在解决这些问题，实现更灵活、稳定的推理过程。

Method: LRT采用循环推理令牌在多步内部迭代中更新，包含丢弃门修正早期错误、学习停止机制自适应控制计算深度，无需符号规则或搜索。

Result: 在数独结构化推理任务中，LRT达到98.68%数字准确率和36.30%完整谜题准确率。分析显示丢弃门和停止门在稳定推理和调整计算深度中发挥重要作用。

Conclusion: LRT展示了自适应深度推理Transformer的可行性，其机制可扩展到国际象棋规模推理任务，未来可扩展到多令牌推理和更大领域。

Abstract: The Liquid Reasoning Transformer (LRT) is a transformer architecture designed for inference with adaptive depths using iterative changes, discard-based correction, and a learned stopping mechanism. Instead of relying on a single feedforward pass, the model updates a recurrent reasoning token across multiple internal steps, allowing it to correct early errors and allocate computation based on input difficulty. We evaluate the LRT on Sudoku as a controlled testbed for structured reasoning and show that it achieves strong performance, reaching 98.68% digit accuracy and 36.30% full-puzzle accuracy without using symbolic rules or search. Analyzing internal patterns shows that the discard and stop gates play different, important roles in stabilizing inferences and adjusting computational depth. We discuss how these mechanisms extend naturally to chess-scale reasoning tasks and outline extensions for multi-token reasoning and larger domains.

</details>


### [112] [Resting Neurons, Active Insights: Improving Input Sparsification for Large Language Models](https://arxiv.org/abs/2512.12744)
*Haotian Xu,Tian Gao,Tsui-Wei Weng,Tengfei Ma*

Main category: cs.LG

TL;DR: 该研究提出通过引入可训练的自发神经元来弥补输入稀疏化带来的性能损失，将输入稀疏化重新解释为动态结构剪枝，并借鉴生物神经元的自发基线放电率来稳定稀疏化LLMs的激活。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然性能优异，但规模庞大带来效率和可解释性挑战。输入稀疏化技术通过选择性激活输入值的子集来提高效率，但现有方法主要关注计算节省，忽视了稀疏化的表示后果，导致与完整模型存在明显性能差距。

Method: 首先将输入稀疏化重新解释为动态结构剪枝。受生物神经元自发基线放电率的启发，引入一小组可训练的自发神经元作为补偿单元，以稳定稀疏化LLMs中的激活。

Result: 实验表明，这些辅助神经元显著减少了稀疏化引起的性能差距，同时在各种任务上表现出良好的泛化能力。

Conclusion: 通过引入自发神经元作为补偿机制，可以在保持输入稀疏化计算优势的同时，有效弥补其带来的表示能力损失，为高效LLMs的设计提供了新思路。

Abstract: Large Language Models (LLMs) achieve state-of-the-art performance across a wide range of applications, but their massive scale poses significant challenges for both efficiency and interpretability. Structural pruning, which reduces model size by removing redundant computational units such as neurons, has been widely explored as a solution, and this study devotes to input sparsification, an increasingly popular technique that improves efficiency by selectively activating only a subset of entry values for each input. However, existing approaches focus primarily on computational savings, often overlooking the representational consequences of sparsification and leaving a noticeable performance gap compared to full models. In this work, we first reinterpret input sparsification as a form of dynamic structural pruning. Motivated by the spontaneous baseline firing rates observed in biological neurons, we introduce a small set of trainable spontaneous neurons that act as compensatory units to stabilize activations in sparsified LLMs. Experiments demonstrate that these auxiliary neurons substantially reduce the sparsification-induced performance gap while generalizing effectively across tasks.

</details>


### [113] [From Small to Large: Generalization Bounds for Transformers on Variable-Size Inputs](https://arxiv.org/abs/2512.12805)
*Anastasiia Alokhina,Pan Li*

Main category: cs.LG

TL;DR: 本文为Transformer在几何数据上的尺寸泛化能力提供了理论框架，证明了离散样本输出与连续域等价输出之间的误差界限，该界限由采样密度和数据流形本征维度决定。


<details>
  <summary>Details</summary>
Motivation: Transformer展现出显著的尺寸泛化能力，能够从小规模token集合外推到更大规模，这种现象在点云、图、自然语言等多个领域都有实证，但缺乏严格的理论刻画。本文旨在为几何数据的这种能力建立理论框架。

Method: 将几何数据表示为连续源的离散样本（如流形上的点云、图论中的图），建立理论框架分析Transformer输出在离散样本与连续域等价之间的误差界限。重点研究具有稳定位置编码的Transformer。

Result: 证明了对于具有稳定位置编码的Transformer，离散样本输出与连续域等价输出之间的误差界限由采样密度和数据流形本征维度决定。在不同尺寸的图和点云上的实验验证了理论界限的紧致性。

Conclusion: 本文为Transformer在几何数据上的尺寸泛化能力提供了首个严格的理论刻画，证明了误差界限与采样密度和流形维度的关系，并通过实验验证了理论结果的正确性。

Abstract: Transformers exhibit a notable property of \emph{size generalization}, demonstrating an ability to extrapolate from smaller token sets to significantly longer ones. This behavior has been documented across diverse applications, including point clouds, graphs, and natural language. Despite its empirical success, this capability still lacks some rigorous theoretical characterizations. In this paper, we develop a theoretical framework to analyze this phenomenon for geometric data, which we represent as discrete samples from a continuous source (e.g., point clouds from manifolds, graphs from graphons). Our core contribution is a bound on the error between the Transformer's output for a discrete sample and its continuous-domain equivalent. We prove that for Transformers with stable positional encodings, this bound is determined by the sampling density and the intrinsic dimensionality of the data manifold. Experiments on graphs and point clouds of various sizes confirm the tightness of our theoretical bound.

</details>


### [114] [OLR-WAA: Adaptive and Drift-Resilient Online Regression with Dynamic Weighted Averaging](https://arxiv.org/abs/2512.12779)
*Mohammad Abu-Shaira,Weishi Shi*

Main category: cs.LG

TL;DR: OLR-WAA是一种无超参数在线回归模型，通过动态加权平均和概念漂移检测机制，有效处理非平稳数据流，在稳定性和适应性间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集常呈现动态变化的数据分布（概念漂移），忽视这一现象会严重降低模型预测性能。现有在线模型的超参数通常是固定的，缺乏动态适应变化数据的能力。

Method: 提出OLR-WAA模型：1）使用指数加权移动平均增量更新基础模型；2）引入独特优化机制，动态检测概念漂移、量化其幅度，并根据实时数据特征调整模型；3）采用保守更新策略处理置信度场景，优先考虑稳定、高置信度的数据点。

Result: 在静态设置中匹配批量回归性能；在概念漂移数据集上持续优于或匹敌最先进的在线模型；快速收敛，相比其他在线模型产生更高的R2值；有效弥补了概念漂移导致的性能差距。

Conclusion: OLR-WAA是一种有效、自适应且抗概念漂移的在线回归模型，无需超参数调整，能持续适应动态数据流，在稳定性和适应性间取得良好平衡。

Abstract: Real-world datasets frequently exhibit evolving data distributions, reflecting temporal variations and underlying shifts. Overlooking this phenomenon, known as concept drift, can substantially degrade the predictive performance of the model. Furthermore, the presence of hyperparameters in online models exacerbates this issue, as these parameters are typically fixed and lack the flexibility to dynamically adjust to evolving data. This paper introduces "OLR-WAA: An Adaptive and Drift-Resilient Online Regression with Dynamic Weighted Average", a hyperparameter-free model designed to tackle the challenges of non-stationary data streams and enable effective, continuous adaptation. The objective is to strike a balance between model stability and adaptability. OLR-WAA incrementally updates its base model by integrating incoming data streams, utilizing an exponentially weighted moving average. It further introduces a unique optimization mechanism that dynamically detects concept drift, quantifies its magnitude, and adjusts the model based on real-time data characteristics. Rigorous evaluations show that it matches batch regression performance in static settings and consistently outperforms or rivals state-of-the-art online models, confirming its effectiveness. Concept drift datasets reveal a performance gap that OLR-WAA effectively bridges, setting it apart from other online models. In addition, the model effectively handles confidence-based scenarios through a conservative update strategy that prioritizes stable, high-confidence data points. Notably, OLR-WAA converges rapidly, consistently yielding higher R2 values compared to other online models.

</details>


### [115] [Credit Risk Estimation with Non-Financial Features: Evidence from a Synthetic Istanbul Dataset](https://arxiv.org/abs/2512.12783)
*Atalay Denknalbant,Emre Sezdi,Zeki Furkan Kutlu,Polat Goktas*

Main category: cs.LG

TL;DR: 研究通过合成数据集和替代金融数据，证明行为属性可以接近征信机构的辨别能力，为无正式信用记录的借款人提供公平信贷评估方案。


<details>
  <summary>Details</summary>
Motivation: 金融排斥限制了创业机会，增加了收入波动，并扩大了财富差距。伊斯坦布尔的银行服务不足消费者由于收入和支付通过非正式渠道流动，往往没有征信记录。研究旨在探索如何评估这类借款人。

Method: 创建了包含10万伊斯坦布尔居民的合成数据集，复制了2025年第一季度土耳其统计局普查边际和电信使用模式。使用检索增强生成技术将公共统计数据输入OpenAI o3模型，合成真实但隐私保护的记录。每个档案包含7个社会人口变量和9个替代属性（手机规格、在线购物节奏、订阅支出、汽车所有权、月租金、信用卡标志）。使用CatBoost、LightGBM和XGBoost训练两个版本模型：演示模型仅使用社会人口变量，完整模型同时使用社会人口和替代属性。

Result: 通过五折分层验证，替代数据块将AUC提高了约1.3个百分点，将平衡F1分数从约0.84提升到0.95，增益达14%。

Conclusion: 研究贡献了开放的伊斯坦布尔2025年第一季度合成数据集、完全可复现的建模流程，以及经验证据表明简洁的行为属性集合可以接近征信机构的辨别能力，同时服务于缺乏正式信用记录的借款人。这些发现为贷款机构和监管机构提供了透明蓝图，以扩展对银行服务不足人群的公平和安全信贷访问。

Abstract: Financial exclusion constrains entrepreneurship, increases income volatility, and widens wealth gaps. Underbanked consumers in Istanbul often have no bureau file because their earnings and payments flow through informal channels. To study how such borrowers can be evaluated we create a synthetic dataset of one hundred thousand Istanbul residents that reproduces first quarter 2025 TÜİK census marginals and telecom usage patterns. Retrieval augmented generation feeds these public statistics into the OpenAI o3 model, which synthesises realistic yet private records. Each profile contains seven socio demographic variables and nine alternative attributes that describe phone specifications, online shopping rhythm, subscription spend, car ownership, monthly rent, and a credit card flag. To test the impact of the alternative financial data CatBoost, LightGBM, and XGBoost are each trained in two versions. Demo models use only the socio demographic variables; Full models include both socio demographic and alternative attributes. Across five fold stratified validation the alternative block raises area under the curve by about one point three percentage and lifts balanced \(F_{1}\) from roughly 0.84 to 0.95, a fourteen percent gain. We contribute an open Istanbul 2025 Q1 synthetic dataset, a fully reproducible modeling pipeline, and empirical evidence that a concise set of behavioural attributes can approach bureau level discrimination power while serving borrowers who lack formal credit records. These findings give lenders and regulators a transparent blueprint for extending fair and safe credit access to the underbanked.

</details>


### [116] [PRIVEE: Privacy-Preserving Vertical Federated Learning Against Feature Inference Attacks](https://arxiv.org/abs/2512.12840)
*Sindhuja Madabushi,Ahmad Faraz Khan,Haider Ali,Ananthram Swami,Rui Ning,Hongyi Wu,Jin-Hee Cho*

Main category: cs.LG

TL;DR: PRIVEE是一种保护垂直联邦学习中隐私的防御机制，通过混淆置信度分数来防止特征推断攻击，同时保持模型预测准确性。


<details>
  <summary>Details</summary>
Motivation: 垂直联邦学习虽然能实现跨组织协作训练，但容易受到特征推断攻击的威胁，攻击者可以利用共享的置信度分数来重构其他参与方的私有输入特征。

Method: PRIVEE通过混淆置信度分数来保护隐私，只共享转换后的表示，同时保持相对排名和分数间距离等关键属性，而不是暴露原始分数。

Result: 实验表明，PRIVEE在隐私保护方面比现有最先进的防御方法提高了三倍，同时保持了完整的预测性能，能够有效抵御高级特征推断攻击。

Conclusion: PRIVEE是一种有效的隐私保护机制，能够在垂直联邦学习环境中防止特征推断攻击，同时不牺牲模型预测准确性，实现了隐私与性能的良好平衡。

Abstract: Vertical Federated Learning (VFL) enables collaborative model training across organizations that share common user samples but hold disjoint feature spaces. Despite its potential, VFL is susceptible to feature inference attacks, in which adversarial parties exploit shared confidence scores (i.e., prediction probabilities) during inference to reconstruct private input features of other participants. To counter this threat, we propose PRIVEE (PRIvacy-preserving Vertical fEderated lEarning), a novel defense mechanism named after the French word privée, meaning "private." PRIVEE obfuscates confidence scores while preserving critical properties such as relative ranking and inter-score distances. Rather than exposing raw scores, PRIVEE shares only the transformed representations, mitigating the risk of reconstruction attacks without degrading model prediction accuracy. Extensive experiments show that PRIVEE achieves a threefold improvement in privacy protection compared to state-of-the-art defenses, while preserving full predictive performance against advanced feature inference attacks.

</details>


### [117] [Selective Conformal Risk Control](https://arxiv.org/abs/2512.12844)
*Yunpeng Xu,Wenge Guo,Zhi Wei*

Main category: cs.LG

TL;DR: 本文提出了选择性共形风险控制框架，将共形预测与选择性分类相结合，通过两阶段方法减少预测集大小，同时保持可靠性保证。


<details>
  <summary>Details</summary>
Motivation: 共形预测虽然能提供分布无关的覆盖保证，但通常会产生过大的预测集，限制了实际应用价值。需要一种既能保持可靠性保证又能产生紧凑预测集的方法。

Method: 提出了选择性共形风险控制框架，包含两个阶段：第一阶段选择置信样本进行预测，第二阶段在选定子集上应用共形风险控制来构建校准的预测集。开发了两种算法：SCRC-T（保持可交换性，提供精确有限样本保证）和SCRC-I（仅校准变体，提供PAC式概率保证，计算效率更高）。

Result: 在两个公共数据集上的实验表明，两种方法都能达到目标覆盖率和风险水平，性能几乎相同。SCRC-I表现出稍微保守的风险控制，但计算实用性更优。

Conclusion: 选择性共形风险控制为紧凑、可靠的不确定性量化提供了一条有效且高效的路径，通过结合选择性分类和共形预测的优势，在保持统计保证的同时减少了预测集大小。

Abstract: Reliable uncertainty quantification is essential for deploying machine learning systems in high-stakes domains. Conformal prediction provides distribution-free coverage guarantees but often produces overly large prediction sets, limiting its practical utility. To address this issue, we propose \textit{Selective Conformal Risk Control} (SCRC), a unified framework that integrates conformal prediction with selective classification. The framework formulates uncertainty control as a two-stage problem: the first stage selects confident samples for prediction, and the second stage applies conformal risk control on the selected subset to construct calibrated prediction sets. We develop two algorithms under this framework. The first, SCRC-T, preserves exchangeability by computing thresholds jointly over calibration and test samples, offering exact finite-sample guarantees. The second, SCRC-I, is a calibration-only variant that provides PAC-style probabilistic guarantees while being more computational efficient. Experiments on two public datasets show that both methods achieve the target coverage and risk levels, with nearly identical performance, while SCRC-I exhibits slightly more conservative risk control but superior computational practicality. Our results demonstrate that selective conformal risk control offers an effective and efficient path toward compact, reliable uncertainty quantification.

</details>


### [118] [Optimal Labeler Assignment and Sampling for Active Learning in the Presence of Imperfect Labels](https://arxiv.org/abs/2512.12870)
*Pouya Ahadi,Blair Winograd,Camille Zaug,Karunesh Arora,Lijun Wang,Kamran Paynabar*

Main category: cs.LG

TL;DR: 提出了一种新的主动学习框架，通过优化分配查询样本给标注者来最小化标签噪声，并引入新的采样方法来减少噪声对分类器性能的影响。


<details>
  <summary>Details</summary>
Motivation: 主动学习中标注者提供的标签往往存在噪声，特别是复杂样本更容易被错误标注，这会导致分类器性能下降。需要一种能够处理噪声标签的鲁棒主动学习方法。

Method: 提出包含两个核心组件的框架：1) 分配模型 - 优化地将查询点分配给标注者，以最小化每个周期内的最大可能噪声；2) 新的采样方法 - 识别最佳查询点，减少标签噪声对分类器性能的影响。

Result: 实验表明，该方法相比多个基准方法显著提高了分类性能。

Conclusion: 提出的主动学习框架能够有效处理噪声标签问题，通过优化标注分配和采样策略构建更鲁棒的分类模型。

Abstract: Active Learning (AL) has garnered significant interest across various application domains where labeling training data is costly. AL provides a framework that helps practitioners query informative samples for annotation by oracles (labelers). However, these labels often contain noise due to varying levels of labeler accuracy. Additionally, uncertain samples are more prone to receiving incorrect labels because of their complexity. Learning from imperfectly labeled data leads to an inaccurate classifier. We propose a novel AL framework to construct a robust classification model by minimizing noise levels. Our approach includes an assignment model that optimally assigns query points to labelers, aiming to minimize the maximum possible noise within each cycle. Additionally, we introduce a new sampling method to identify the best query points, reducing the impact of label noise on classifier performance. Our experiments demonstrate that our approach significantly improves classification performance compared to several benchmark methods.

</details>


### [119] [TRACER: Transfer Learning based Real-time Adaptation for Clinical Evolving Risk](https://arxiv.org/abs/2512.12795)
*Mengying Yan,Ziye Tian,Siqi Li,Nan Liu,Benjamin A. Goldstein,Molei Liu,Chuan Hong*

Main category: cs.LG

TL;DR: TRACER框架通过迁移学习实时适应临床环境变化，解决电子健康记录预测模型因时间性人群变化导致的性能漂移问题，在COVID-19过渡期间显著提升医院入院预测性能。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录临床决策支持工具常因时间性人群变化而出现性能漂移，特别是在临床环境变化最初只影响部分患者、导致混合人群过渡的情况下。这种病例组合变化通常发生在系统级操作更新或新疾病（如COVID-19）出现后。

Method: 提出TRACER框架，通过识别就诊级别的过渡成员关系，使用迁移学习调整预测模型而无需完全重新训练。该方法能够适应演化和异质性的临床条件。

Result: 在模拟研究中，TRACER优于基于历史或当代数据训练的静态模型。在实际应用中，预测COVID-19过渡期间急诊就诊后的医院入院，TRACER提高了区分度和校准度。

Conclusion: TRACER为在演化和异质性临床条件下保持稳健预测性能提供了一种可扩展的方法，能够有效应对临床环境变化导致的模型性能漂移问题。

Abstract: Clinical decision support tools built on electronic health records often experience performance drift due to temporal population shifts, particularly when changes in the clinical environment initially affect only a subset of patients, resulting in a transition to mixed populations. Such case-mix changes commonly arise following system-level operational updates or the emergence of new diseases, such as COVID-19. We propose TRACER (Transfer Learning-based Real-time Adaptation for Clinical Evolving Risk), a framework that identifies encounter-level transition membership and adapts predictive models using transfer learning without full retraining. In simulation studies, TRACER outperformed static models trained on historical or contemporary data. In a real-world application predicting hospital admission following emergency department visits across the COVID-19 transition, TRACER improved both discrimination and calibration. TRACER provides a scalable approach for maintaining robust predictive performance under evolving and heterogeneous clinical conditions.

</details>


### [120] [Scaling Bidirectional Spans and Span Violations in Attention Mechanism](https://arxiv.org/abs/2512.13033)
*Jongwook Kim,Sangheon Yun,Sukjin Yoon*

Main category: cs.LG

TL;DR: 提出一种优化Transformer训练的方法，通过非对称投影将反向传播梯度分解为平行分量和正交违规分量，保持前向QKV结构不变，在WikiText-2数据集上验证损失降低0.56%


<details>
  <summary>Details</summary>
Motivation: 标准Transformer虽然性能优秀，但其训练存在几何效率问题。标准注意力梯度不是最优的，可以通过优化梯度传播来提高训练效率

Method: 提出优化框架，使用非对称投影将反向传播梯度分解为平行分量和正交违规分量，保持前向QKV结构不变。选择性地缩放这些分量，主要关注0阶双向平行分量

Result: 在WikiText-2数据集上，使用简单配置实现了验证损失降低0.56%，证明了框架的基本有效性

Conclusion: 标准注意力梯度是次优的，提出的优化框架具有基本有效性，在更大数据集和更深训练机制上具有显著潜力

Abstract: The canonical $O(N^2)$ Transformer remains the empirical performance frontier in sequence modeling, and its training can be further optimized by addressing geometric inefficiency. We propose an optimization framework that leverages an asymmetric projection to decompose the backward-pass gradients into parallel spans and orthogonal violations, while keeping the canonical forward-pass $QKV$ structure intact. Through consistent experimental validation across various decomposition and projection setups, we provide strong theoretical evidence: the standard attention gradient is suboptimal. We demonstrated that selectively scaling these components, focusing primarily on $0^{th}$ order bidirectional parallel spans, yields the most effective learning signal. On the limited WikiText-2 dataset, and using a crude configuration, this method achieved a $0.56\%$ reduction in validation loss, confirming the framework's fundamental validity and suggesting significant potential gains on larger datasets and deeper training regimes

</details>


### [121] [Optimal Resource Allocation for ML Model Training and Deployment under Concept Drift](https://arxiv.org/abs/2512.12816)
*Hasan Burhan Beytur,Gustavo de Veciana,Haris Vikalo,Kevin S Chan*

Main category: cs.LG

TL;DR: 该论文研究了在概念漂移和有限预算下如何分配机器学习模型的训练和部署资源，提出了一个模型无关的框架来分析资源分配、概念漂移动态和部署时机之间的相互作用。


<details>
  <summary>Details</summary>
Motivation: 在现实世界中，模型提供者需要向多个客户端分发训练好的模型，这些客户端设备支持本地推理但缺乏重新训练模型的能力，因此性能维护的责任落在提供者身上。同时存在概念漂移和预算限制的挑战，需要系统性的资源分配策略。

Method: 引入了一个模型无关的框架来捕捉资源分配、概念漂移动态和部署时机之间的相互作用。分析了概念持续时间分布对最优训练策略的影响，特别关注了递减平均剩余寿命（DMRL）和递增平均剩余寿命（IMRL）分布。在通信约束下研究了模型部署问题，证明了相关优化问题在温和条件下的拟凸性，并提出了一种随机调度策略。

Result: 研究发现最优训练策略关键取决于概念持续时间的老化特性。在突然概念变化下，当概念持续时间遵循DMRL分布时，可以推导出预算约束下的最优训练策略，并证明直观启发式方法在IMRL分布下是次优的。提出的随机调度策略能够在客户端实现接近最优的性能。

Conclusion: 该研究为概念漂移下的成本高效机器学习模型管理提供了理论和算法基础，对持续学习、分布式推理和自适应机器学习系统具有重要启示。框架能够指导模型提供者在有限预算下做出最优的资源分配决策。

Abstract: We study how to allocate resources for training and deployment of machine learning (ML) models under concept drift and limited budgets. We consider a setting in which a model provider distributes trained models to multiple clients whose devices support local inference but lack the ability to retrain those models, placing the burden of performance maintenance on the provider. We introduce a model-agnostic framework that captures the interaction between resource allocation, concept drift dynamics, and deployment timing. We show that optimal training policies depend critically on the aging properties of concept durations. Under sudden concept changes, we derive optimal training policies subject to budget constraints when concept durations follow distributions with Decreasing Mean Residual Life (DMRL), and show that intuitive heuristics are provably suboptimal under Increasing Mean Residual Life (IMRL). We further study model deployment under communication constraints, prove that the associated optimization problem is quasi-convex under mild conditions, and propose a randomized scheduling strategy that achieves near-optimal client-side performance. These results offer theoretical and algorithmic foundations for cost-efficient ML model management under concept drift, with implications for continual learning, distributed inference, and adaptive ML systems.

</details>


### [122] [TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning](https://arxiv.org/abs/2512.13106)
*Shenzhi Yang,Guangcheng Zhu,Xing Zheng,Yingfan MA,Zhongqi Chen,Bowen Song,Weiqiang Wang,Junbo Zhao,Gang Chen,Haobo Wang*

Main category: cs.LG

TL;DR: 提出TraPO半监督RLVR方法，用少量标注样本指导无标注样本的强化学习训练，显著提升数据效率和泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有无监督RLVR方法容易在训练后期出现模型崩溃，因为缺乏外部监督会强化错误的推理模式。需要一种既能减少标注成本又能保持训练稳定性的方法。

Method: 提出半监督RLVR范式，利用少量标注样本指导无标注样本的强化学习训练。核心是TraPO算法，通过匹配学习轨迹相似性来识别可靠的无标注样本。

Result: 在6个数学推理基准和3个分布外任务上取得显著效果。仅用1K标注+3K无标注样本达到42.6%平均准确率，超越用45K无标注样本的无监督方法（38.3%）。用4K标注+12K无标注样本甚至超越用全部45K标注样本的全监督模型。

Conclusion: TraPO通过半监督RLVR方法有效解决了无监督方法的模型崩溃问题，实现了卓越的数据效率和泛化能力，为减少标注成本提供了有效解决方案。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has proven effective in training large reasoning models (LRMs) by leveraging answer-verifiable signals to guide policy optimization, which, however, suffers from high annotation costs. To alleviate this problem, recent work has explored unsupervised RLVR methods that derive rewards solely from the model's internal consistency, such as through entropy and majority voting. While seemingly promising, these methods often suffer from model collapse in the later stages of training, which may arise from the reinforcement of incorrect reasoning patterns in the absence of external supervision. In this work, we investigate a novel semi-supervised RLVR paradigm that utilizes a small labeled set to guide RLVR training on unlabeled samples. Our key insight is that supervised rewards are essential for stabilizing consistency-based training on unlabeled samples, ensuring that only reasoning patterns verified on labeled instances are incorporated into RL training. Technically, we propose an effective policy optimization algorithm, TraPO, that identifies reliable unlabeled samples by matching their learning trajectory similarity to labeled ones. Building on this, TraPO achieves remarkable data efficiency and strong generalization on six widely used mathematical reasoning benchmarks (AIME24/25, AMC, MATH-500, Minerva, and Olympiad) and three out-of-distribution tasks (ARC-c, GPQA-diamond, and MMLU-pro). With only 1K labeled and 3K unlabeled samples, TraPO reaches 42.6% average accuracy, surpassing the best unsupervised method trained on 45K unlabeled samples (38.3%). Notably, when using 4K labeled and 12K unlabeled samples, TraPO even outperforms the fully supervised model trained on the full 45K labeled samples on all benchmarks, while using only 10% of the labeled data. The code is available via https://github.com/ShenzhiYang2000/TRAPO.

</details>


### [123] [SACn: Soft Actor-Critic with n-step Returns](https://arxiv.org/abs/2512.13165)
*Jakub Łyskawa,Jakub Lewandowski,Paweł Wawrzyński*

Main category: cs.LG

TL;DR: 本文提出了一种将Soft Actor-Critic（SAC）与n步回报相结合的新方法，解决了传统组合中由于动作分布变化导致的偏差问题，通过数值稳定的重要性采样和简化的超参数选择，以及τ采样熵估计来降低学习目标的方差。


<details>
  <summary>Details</summary>
Motivation: SAC是实际应用中最相关的离策略在线无模型强化学习方法之一，n步回报技术已知能提高RL算法的收敛速度。然而，SAC与n步回报的组合存在困难，因为它们的常规组合会由于动作分布变化在离策略算法中引入偏差。虽然重要性采样可以解决这个问题，但可能导致数值不稳定。

Method: 提出了一种将SAC与n步回报相结合的方法：1）应用数值稳定的重要性采样并简化超参数选择；2）在n步最大熵框架下分析SAC的熵估计方法，并制定τ采样熵估计来降低学习目标的方差；3）最终形成SACn算法。

Result: 在MuJoCo模拟环境中进行了实验验证，表明该方法能够有效结合SAC与n步回报，解决了偏差问题并提高了算法的稳定性。

Conclusion: 成功开发了SACn算法，通过数值稳定的重要性采样和τ采样熵估计，克服了SAC与n步回报结合时的偏差和稳定性问题，为实际应用提供了更高效的强化学习解决方案。

Abstract: Soft Actor-Critic (SAC) is widely used in practical applications and is now one of the most relevant off-policy online model-free reinforcement learning (RL) methods. The technique of n-step returns is known to increase the convergence speed of RL algorithms compared to their 1-step returns-based versions. However, SAC is notoriously difficult to combine with n-step returns, since their usual combination introduces bias in off-policy algorithms due to the changes in action distribution. While this problem is solved by importance sampling, a method for estimating expected values of one distribution using samples from another distribution, importance sampling may result in numerical instability. In this work, we combine SAC with n-step returns in a way that overcomes this issue. We present an approach to applying numerically stable importance sampling with simplified hyperparameter selection. Furthermore, we analyze the entropy estimation approach of Soft Actor-Critic in the context of the n-step maximum entropy framework and formulate the $τ$-sampled entropy estimation to reduce the variance of the learning target. Finally, we formulate the Soft Actor-Critic with n-step returns (SAC$n$) algorithm that we experimentally verify on MuJoCo simulated environments.

</details>


### [124] [PolySet: Restoring the Statistical Ensemble Nature of Polymers for Machine Learning](https://arxiv.org/abs/2512.13186)
*Khalid Ferji*

Main category: cs.LG

TL;DR: PolySet框架将聚合物表示为从摩尔质量分布中采样的有限加权链集合，解决了传统机器学习模型将聚合物视为单一完美分子图与物理现实不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 聚合物科学中的机器学习模型通常将聚合物视为单一完美定义的分子图，而真实材料是由具有分布长度的链组成的随机集合。这种物理现实与数字表示之间的不匹配限制了当前模型捕捉聚合物行为的能力。

Method: 引入PolySet框架，将聚合物表示为从假设的摩尔质量分布中采样的有限加权链集合。这种基于集合的编码独立于化学细节，与任何分子表示兼容，并在均聚物情况下使用最小语言模型进行说明。

Result: PolySet保留了高阶分布矩（如Mz、Mz+1），使机器学习模型能够学习尾部敏感特性，具有显著改善的稳定性和准确性。

Conclusion: 通过明确承认聚合物物质的统计性质，PolySet为未来聚合物机器学习建立了物理基础，自然可扩展到共聚物、嵌段结构和其他复杂拓扑结构。

Abstract: Machine-learning (ML) models in polymer science typically treat a polymer as a single, perfectly defined molecular graph, even though real materials consist of stochastic ensembles of chains with distributed lengths. This mismatch between physical reality and digital representation limits the ability of current models to capture polymer behaviour. Here we introduce PolySet, a framework that represents a polymer as a finite, weighted ensemble of chains sampled from an assumed molar-mass distribution. This ensemble-based encoding is independent of chemical detail, compatible with any molecular representation and illustrated here in the homopolymer case using a minimal language model. We show that PolySet retains higher-order distributional moments (such as Mz, Mz+1), enabling ML models to learn tail-sensitive properties with greatly improved stability and accuracy. By explicitly acknowledging the statistical nature of polymer matter, PolySet establishes a physically grounded foundation for future polymer machine learning, naturally extensible to copolymers, block architectures, and other complex topologies.

</details>


### [125] [WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory](https://arxiv.org/abs/2512.13190)
*Jin Sob Kim,Hyun Joon Park,Wooseok Shin,Dongil Park,Sung Won Han*

Main category: cs.LG

TL;DR: 提出WAY深度学习架构，通过重构AIS轨迹为嵌套序列结构，实现船舶目的地提前数天至数周的长期预测，引入梯度丢弃技术解决训练偏差问题。


<details>
  <summary>Details</summary>
Motivation: AIS系统虽然支持数据驱动的海上监视，但存在可靠性问题和数据间隔不规则的问题。需要解决基于全球范围AIS数据的船舶目的地估计挑战。

Method: 1) 将长港口到港口轨迹重构为嵌套序列结构，使用空间网格减轻时空偏差；2) 提出WAY深度学习架构，包含轨迹表示层和通道聚合序列处理块；3) 引入任务专用梯度丢弃技术，实现多对多训练。

Result: 在5年AIS数据上的实验表明，WAY优于传统基于空间网格的方法，无论轨迹进展如何。梯度丢弃技术带来性能提升。通过多任务学习探索了ETA估计的实际应用潜力。

Conclusion: 提出的WAY架构能够有效处理重构的AIS轨迹，实现长期目的地预测。梯度丢弃技术解决了训练偏差问题，为实际海上监视应用提供了有前景的解决方案。

Abstract: The Automatic Identification System (AIS) enables data-driven maritime surveillance but suffers from reliability issues and irregular intervals. We address vessel destination estimation using global-scope AIS data by proposing a differentiated approach that recasts long port-to-port trajectories as a nested sequence structure. Using spatial grids, this method mitigates spatio-temporal bias while preserving detailed resolution. We introduce a novel deep learning architecture, WAY, designed to process these reformulated trajectories for long-term destination estimation days to weeks in advance. WAY comprises a trajectory representation layer and Channel-Aggregative Sequential Processing (CASP) blocks. The representation layer generates multi-channel vector sequences from kinematic and non-kinematic features. CASP blocks utilize multi-headed channel- and self-attention for aggregation and sequential information delivery. Additionally, we propose a task-specialized Gradient Dropout (GD) technique to enable many-to-many training on single labels, preventing biased feedback surges by stochastically blocking gradient flow based on sample length. Experiments on 5-year AIS data demonstrate WAY's superiority over conventional spatial grid-based approaches regardless of trajectory progression. Results further confirm that adopting GD leads to performance gains. Finally, we explore WAY's potential for real-world application through multitask learning for ETA estimation.

</details>


### [126] [CORE: Contrastive Masked Feature Reconstruction on Graphs](https://arxiv.org/abs/2512.13235)
*Jianyuan Bo,Yuan Fang*

Main category: cs.LG

TL;DR: 该研究提出了一种新的图自监督学习框架CORE，将对比学习融入掩码特征重建中，在节点和图分类任务上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 研究发现生成式方法（掩码特征重建MFR）和对比式方法（图对比学习GCL）虽然操作机制不同，但在特定条件下目标函数会收敛，这表明两种方法是互补的而非根本不同的，因此探索将两者集成以增强图自监督学习。

Method: 提出CORE框架，将对比学习融入掩码特征重建：1）仅使用掩码节点的原始特征和重建特征形成正样本对，鼓励编码器优先考虑上下文信息而非节点自身特征；2）利用掩码节点本身作为负样本，结合MFR的重建能力和GCL的判别能力来更好地捕捉内在图结构。

Result: CORE在节点和图分类任务上显著优于MFR，并取得了最先进的结果：在节点分类任务上比GraphMAE和GraphMAE2分别提升2.80%和3.72%；在图分类任务上分别提升3.82%和3.76%。

Conclusion: CORE框架成功地将生成式和对比式图自监督学习方法相结合，证明了这两种方法的互补性，为图自监督学习提供了新的有效框架。

Abstract: In the rapidly evolving field of self-supervised learning on graphs, generative and contrastive methodologies have emerged as two dominant approaches. Our study focuses on masked feature reconstruction (MFR), a generative technique where a model learns to restore the raw features of masked nodes in a self-supervised manner. We observe that both MFR and graph contrastive learning (GCL) aim to maximize agreement between similar elements. Building on this observation, we reveal a novel theoretical insight: under specific conditions, the objectives of MFR and node-level GCL converge, despite their distinct operational mechanisms. This theoretical connection suggests these approaches are complementary rather than fundamentally different, prompting us to explore their integration to enhance self-supervised learning on graphs. Our research presents Contrastive Masked Feature Reconstruction (CORE), a novel graph self-supervised learning framework that integrates contrastive learning into MFR. Specifically, we form positive pairs exclusively between the original and reconstructed features of masked nodes, encouraging the encoder to prioritize contextual information over the node's own features. Additionally, we leverage the masked nodes themselves as negative samples, combining MFR's reconstructive power with GCL's discriminative ability to better capture intrinsic graph structures. Empirically, our proposed framework CORE significantly outperforms MFR across node and graph classification tasks, demonstrating state-of-the-art results. In particular, CORE surpasses GraphMAE and GraphMAE2 by up to 2.80% and 3.72% on node classification tasks, and by up to 3.82% and 3.76% on graph classification tasks.

</details>


### [127] [No One Left Behind: How to Exploit the Incomplete and Skewed Multi-Label Data for Conversion Rate Prediction](https://arxiv.org/abs/2512.13300)
*Qinglin Jia,Zhaocheng Du,Chuhan Wu,Huifeng Guo,Ruiming Tang,Shuting Shi,Muyu Zhang*

Main category: cs.LG

TL;DR: KAML框架解决在线广告系统中多任务学习面临的非对称多标签数据问题，通过属性驱动掩码策略、分层知识提取机制和排序损失策略提升转化率预测性能。


<details>
  <summary>Details</summary>
Motivation: 现实在线广告系统中，广告主通常有多样化的客户获取目标，但转化数据往往不完整，因为隐私等原因广告主只提交部分用户转化行为。这导致多任务学习面临非对称多标签数据的挑战，训练和部署数据分布不匹配。

Method: 提出KAML框架：1) 属性驱动掩码策略(ADM)更好地利用非对称多标签数据；2) 分层知识提取机制(HKE)建模目标任务塔内的样本差异；3) 结合排序损失策略最大化未标记样本的效用。

Result: 在离线行业数据集和在线A/B测试中进行了全面评估，相比现有MTL基线模型表现出显著性能提升。

Conclusion: KAML框架有效解决了多任务学习中非对称多标签数据的挑战，通过创新的掩码策略、知识提取机制和损失函数设计，在实际广告系统中实现了更好的转化率预测性能。

Abstract: In most real-world online advertising systems, advertisers typically have diverse customer acquisition goals. A common solution is to use multi-task learning (MTL) to train a unified model on post-click data to estimate the conversion rate (CVR) for these diverse targets. In practice, CVR prediction often encounters missing conversion data as many advertisers submit only a subset of user conversion actions due to privacy or other constraints, making the labels of multi-task data incomplete. If the model is trained on all available samples where advertisers submit user conversion actions, it may struggle when deployed to serve a subset of advertisers targeting specific conversion actions, as the training and deployment data distributions are mismatched. While considerable MTL efforts have been made, a long-standing challenge is how to effectively train a unified model with the incomplete and skewed multi-label data. In this paper, we propose a fine-grained Knowledge transfer framework for Asymmetric Multi-Label data (KAML). We introduce an attribution-driven masking strategy (ADM) to better utilize data with asymmetric multi-label data in training. However, the more relaxed masking in ADM is a double-edged sword: it provides additional training signals but also introduces noise due to skewed data. To address this, we propose a hierarchical knowledge extraction mechanism (HKE) to model the sample discrepancy within the target task tower. Finally, to maximize the utility of unlabeled samples, we incorporate ranking loss strategy to further enhance our model. The effectiveness of KAML has been demonstrated through comprehensive evaluations on offline industry datasets and online A/B tests, which show significant performance improvements over existing MTL baselines.

</details>


### [128] [ALIGN-FL: Architecture-independent Learning through Invariant Generative component sharing in Federated Learning](https://arxiv.org/abs/2512.13316)
*Mayank Gulati,Benedikt Groß,Gerhard Wunder*

Main category: cs.LG

TL;DR: ALIGN-FL是一种新颖的分布式学习方法，通过选择性共享生成组件来处理高度不相关的数据分布问题，使用隐私保护机制在异构客户端环境中实现跨域学习。


<details>
  <summary>Details</summary>
Motivation: 解决在高度不相关数据分布（Non-IID）和异构客户端环境下的联邦学习挑战，特别是跨域异常值处理问题，同时保护数据隐私。

Method: 提出选择性共享生成组件的方法，仅传输生成能力而非完整模型参数；采用DP-SGD自适应裁剪和Lipschitz正则化VAE解码器双重隐私机制；设计支持异构客户端的架构。

Result: 在MNIST和Fashion-MNIST数据集上验证了方法有效性，两种隐私机制都能将敏感异常值映射到典型数据点，在极端Non-IID场景下保持实用性。

Conclusion: ALIGN-FL框架通过选择性生成组件共享和互补隐私机制，成功解决了跨域联邦学习中的隐私保护和数据分布不匹配问题。

Abstract: We present ALIGN-FL, a novel approach to distributed learning that addresses the challenge of learning from highly disjoint data distributions through selective sharing of generative components. Instead of exchanging full model parameters, our framework enables privacy-preserving learning by transferring only generative capabilities across clients, while the server performs global training using synthetic samples. Through complementary privacy mechanisms: DP-SGD with adaptive clipping and Lipschitz regularized VAE decoders and a stateful architecture supporting heterogeneous clients, we experimentally validate our approach on MNIST and Fashion-MNIST datasets with cross-domain outliers. Our analysis demonstrates that both privacy mechanisms effectively map sensitive outliers to typical data points while maintaining utility in extreme Non-IID scenarios typical of cross-silo collaborations.
  Index Terms: Client-invariant Learning, Federated Learning (FL), Privacy-preserving Generative Models, Non-Independent and Identically Distributed (Non-IID), Heterogeneous Architectures

</details>


### [129] [Unsupervised learning of multiscale switching dynamical system models from multimodal neural data](https://arxiv.org/abs/2512.12881)
*DongKyu Kim,Han-Lin Hsieh,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 提出一种无监督学习算法，用于学习多尺度神经数据中的切换动态系统模型，能够同时处理连续和离散神经信号，无需标记数据即可解码行为。


<details>
  <summary>Details</summary>
Motivation: 神经群体活动常表现出依赖于状态的切换动态，现有方法主要针对单一神经模态（连续高斯信号或离散泊松信号），而实际中常同时记录多尺度神经模态。此外，训练数据通常缺乏状态标签，这给学习切换动态模型带来挑战。

Method: 开发了一种新颖的无监督学习算法，仅使用多尺度神经观测数据学习切换多尺度动态系统模型的参数。该方法能够同时处理连续和离散神经信号，并在两个不同的实验数据集（包含多模态尖峰-LFP观测）上进行验证。

Result: 切换多尺度动态系统模型比切换单尺度动态模型更准确地解码行为，展示了多尺度神经融合的成功。此外，该模型优于静态多尺度模型，说明跟踪多模态神经数据中依赖于状态的非平稳性很重要。

Conclusion: 该无监督学习框架通过利用多模态记录中的信息并纳入状态切换，能够更准确地建模复杂的多尺度神经动态。这种方法有望提高脑机接口的性能和鲁棒性，并增进对行为神经基础的理解。

Abstract: Neural population activity often exhibits regime-dependent non-stationarity in the form of switching dynamics. Learning accurate switching dynamical system models can reveal how behavior is encoded in neural activity. Existing switching approaches have primarily focused on learning models from a single neural modality, either continuous Gaussian signals or discrete Poisson signals. However, multiple neural modalities are often recorded simultaneously to measure different spatiotemporal scales of brain activity, and all these modalities can encode behavior. Moreover, regime labels are typically unavailable in training data, posing a significant challenge for learning models of regime-dependent switching dynamics. To address these challenges, we develop a novel unsupervised learning algorithm that learns the parameters of switching multiscale dynamical system models using only multiscale neural observations. We demonstrate our method using both simulations and two distinct experimental datasets with multimodal spike-LFP observations during different motor tasks. We find that our switching multiscale dynamical system models more accurately decode behavior than switching single-scale dynamical models, showing the success of multiscale neural fusion. Further, our models outperform stationary multiscale models, illustrating the importance of tracking regime-dependent non-stationarity in multimodal neural data. The developed unsupervised learning framework enables more accurate modeling of complex multiscale neural dynamics by leveraging information in multimodal recordings while incorporating regime switches. This approach holds promise for improving the performance and robustness of brain-computer interfaces over time and for advancing our understanding of the neural basis of behavior.

</details>


### [130] [DP-CSGP: Differentially Private Stochastic Gradient Push with Compressed Communication](https://arxiv.org/abs/2512.13583)
*Zehan Zhu,Heng Zhao,Yan Huang,Joey Tianyi Zhou,Shouling Ji,Jinming Xu*

Main category: cs.LG

TL;DR: 提出DP-CSGP算法，在定向图上的去中心化学习中实现差分隐私保护、梯度压缩通信，在保证隐私的同时保持高模型效用和低通信成本。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化学习算法在隐私保护和通信效率方面存在不足，需要在定向图环境中同时实现严格的差分隐私保证和高效的压缩通信，同时保持模型性能。

Method: 提出差分隐私随机梯度推送压缩通信算法(DP-CSGP)，结合差分隐私机制、随机梯度下降和压缩通信技术，在定向图结构中进行去中心化学习。

Result: 对于一般非凸平滑目标函数，算法达到紧致的效用边界O(√(dlog(1/δ))/(√nJε))，与精确通信的去中心化算法相当；在相同隐私预算下，DP-CSGP获得可比模型精度，通信成本显著降低。

Conclusion: DP-CSGP算法成功实现了在定向图上去中心化学习中的隐私保护、高效通信和高模型效用的平衡，为实际应用提供了可行的解决方案。

Abstract: In this paper, we propose a Differentially Private Stochastic Gradient Push with Compressed communication (termed DP-CSGP) for decentralized learning over directed graphs. Different from existing works, the proposed algorithm is designed to maintain high model utility while ensuring both rigorous differential privacy (DP) guarantees and efficient communication. For general non-convex and smooth objective functions, we show that the proposed algorithm achieves a tight utility bound of $\mathcal{O}\left( \sqrt{d\log \left( \frac{1}δ \right)}/(\sqrt{n}Jε) \right)$ ($J$ and $d$ are the number of local samples and the dimension of decision variables, respectively) with $\left(ε, δ\right)$-DP guarantee for each node, matching that of decentralized counterparts with exact communication. Extensive experiments on benchmark tasks show that, under the same privacy budget, DP-CSGP achieves comparable model accuracy with significantly lower communication cost than existing decentralized counterparts with exact communication.

</details>


### [131] [Wait, Wait, Wait... Why Do Reasoning Models Loop?](https://arxiv.org/abs/2512.12895)
*Charilaos Pipis,Shivam Garg,Vasilis Kontonis,Vaishnavi Shrivastava,Akshay Krishnamurthy,Dimitris Papailiopoulos*

Main category: cs.LG

TL;DR: 研究发现推理模型在低温下容易陷入文本循环重复，主要原因是学习误差和Transformer的归纳偏好，高温能缓解但非根本解决方案。


<details>
  <summary>Details</summary>
Motivation: 推理模型（如DeepSeek-R1）在解决复杂问题时会产生长链思维，但在低温或贪婪解码时经常陷入文本循环重复。研究旨在探究这种现象的原因以及温度参数在其中扮演的角色。

Method: 使用开源推理模型研究循环现象，分析不同规模模型的循环频率，引入合成图推理任务来演示两种机制：1）学习难度导致的风险规避；2）Transformer对时间相关错误的归纳偏好。

Result: 研究发现：1）低温下循环现象普遍；2）大模型循环较少；3）蒸馏学生模型即使教师模型很少循环也会显著循环；4）高温通过促进探索减少循环，但无法修正学习误差，导致生成文本仍过长。

Conclusion: 循环现象主要由学习误差引起，高温只是权宜之计而非根本解决方案。研究最后讨论了训练时干预措施，旨在直接减少学习误差。

Abstract: Reasoning models (e.g., DeepSeek-R1) generate long chains of thought to solve harder problems, but they often loop, repeating the same text at low temperatures or with greedy decoding. We study why this happens and what role temperature plays. With open reasoning models, we find that looping is common at low temperature. Larger models tend to loop less, and distilled students loop significantly even when their teachers rarely do. This points to mismatches between the training distribution and the learned model, which we refer to as errors in learning, as a key cause. To understand how such errors cause loops, we introduce a synthetic graph reasoning task and demonstrate two mechanisms. First, risk aversion caused by hardness of learning: when the correct progress-making action is hard to learn but an easy cyclic action is available, the model puts relatively more probability on the cyclic action and gets stuck. Second, even when there is no hardness, Transformers show an inductive bias toward temporally correlated errors, so the same few actions keep being chosen and loops appear. Higher temperature reduces looping by promoting exploration, but it does not fix the errors in learning, so generations remain much longer than necessary at high temperature; in this sense, temperature is a stopgap rather than a holistic solution. We end with a discussion of training-time interventions aimed at directly reducing errors in learning.

</details>


### [132] [Probability Estimation for Predicted-Occupancy Grids in Vehicle Safety Applications Based on Machine Learning](https://arxiv.org/abs/2512.12896)
*Parthasarathy Nadarajan,Michael Botsch*

Main category: cs.LG

TL;DR: 提出了一种预测复杂交通场景演化的方法，使用预测占用网格(POG)作为未来场景假设的概率表示，通过机器学习方法实现实时计算


<details>
  <summary>Details</summary>
Motivation: 现有模型方法计算预测占用网格(POG)时计算负荷过高，需要开发能够实时计算POG的方法，以改进车辆安全系统中的关键组件

Method: 提出基于机器学习的POG计算方法，使用增强单元占用网格表示当前交通场景状态，采用随机森林算法进行映射，并与模型方法进行比较

Result: 机器学习方法在交通场景模拟中表现良好，有望实现POG的实时计算，为车辆安全应用提供支持

Conclusion: 机器学习方法能够有效降低POG计算的计算负荷，实现实时预测，从而改进车辆安全系统中的关键性评估和轨迹规划等组件

Abstract: This paper presents a method to predict the evolution of a complex traffic scenario with multiple objects. The current state of the scenario is assumed to be known from sensors and the prediction is taking into account various hypotheses about the behavior of traffic participants. This way, the uncertainties regarding the behavior of traffic participants can be modelled in detail. In the first part of this paper a model-based approach is presented to compute Predicted-Occupancy Grids (POG), which are introduced as a grid-based probabilistic representation of the future scenario hypotheses. However, due to the large number of possible trajectories for each traffic participant, the model-based approach comes with a very high computational load. Thus, a machine-learning approach is adopted for the computation of POGs. This work uses a novel grid-based representation of the current state of the traffic scenario and performs the mapping to POGs. This representation consists of augmented cells in an occupancy grid. The adopted machine-learning approach is based on the Random Forest algorithm. Simulations of traffic scenarios are performed to compare the machine-learning with the model-based approach. The results are promising and could enable the real-time computation of POGs for vehicle safety applications. With this detailed modelling of uncertainties, crucial components in vehicle safety systems like criticality estimation and trajectory planning can be improved.

</details>


### [133] [Predicted-occupancy grids for vehicle safety applications based on autoencoders and the Random Forest algorithm](https://arxiv.org/abs/2512.12901)
*Parthasarathy Nadarajan,Michael Botsch,Sebastian Sardina*

Main category: cs.LG

TL;DR: 该论文提出了一种基于机器学习的概率时空表示方法，用于预测复杂交通场景中参与者的未来行为，通过分层分类器识别场景类型，使用堆叠降噪自编码器提取特征，随机森林预测未来占用网格，并应用于场景风险评估和轨迹规划。


<details>
  <summary>Details</summary>
Motivation: 复杂交通场景中车辆动态操作需要准确预测其他交通参与者的未来行为，这对主动车辆安全应用至关重要。现有方法需要更精确的概率时空表示来支持安全决策。

Method: 1. 使用分层情境分类器区分不同交通场景类型，识别道路基础设施和安全相关交通参与者；2. 对每类相似场景，训练随机森林预测概率时空表示（预测占用网格POG）；3. 使用堆叠降噪自编码器将增强占用网格降维为低维特征，提高学习精度；4. 将POG应用于交通场景风险评估和安全轨迹确定。

Result: 提出的SDA和RF组合方法在仿真和真实车辆实验中表现出优异性能，能够有效预测交通参与者的未来行为，并成功应用于评估交通场景临界性和确定安全轨迹。

Conclusion: 该机器学习方法能够准确预测复杂交通场景的概率时空表示，为主动车辆安全应用提供了有效的预测工具，特别是在动态操作和风险评估方面具有重要应用价值。

Abstract: In this paper, a probabilistic space-time representation of complex traffic scenarios is predicted using machine learning algorithms. Such a representation is significant for all active vehicle safety applications especially when performing dynamic maneuvers in a complex traffic scenario. As a first step, a hierarchical situation classifier is used to distinguish the different types of traffic scenarios. This classifier is responsible for identifying the type of the road infrastructure and the safety-relevant traffic participants of the driving environment. With each class representing similar traffic scenarios, a set of Random Forests (RFs) is individually trained to predict the probabilistic space-time representation, which depicts the future behavior of traffic participants. This representation is termed as a Predicted-Occupancy Grid (POG). The input to the RFs is an Augmented Occupancy Grid (AOG). In order to increase the learning accuracy of the RFs and to perform better predictions, the AOG is reduced to low-dimensional features using a Stacked Denoising Autoencoder (SDA). The excellent performance of the proposed machine learning approach consisting of SDAs and RFs is demonstrated in simulations and in experiments with real vehicles. An application of POGs to estimate the criticality of traffic scenarios and to determine safe trajectories is also presented.

</details>


### [134] [Next-generation reservoir computing validated by classification task](https://arxiv.org/abs/2512.12903)
*Ken-ichi Kitayama*

Main category: cs.LG

TL;DR: NG-RC（下一代储备池计算）无需物理储备池，直接从时间序列输入计算多项式项。先前基准测试仅限于时间波形预测任务，本文首次证明NG-RC在分类任务上能达到传统储备池计算的性能。


<details>
  <summary>Details</summary>
Motivation: 现有NG-RC研究主要关注时间序列预测任务（如Lorenz 63吸引子和Mackey-Glass混沌信号），缺乏对其分类能力的验证。需要证明NG-RC在更广泛计算任务中的通用性。

Method: 采用下一代储备池计算（NG-RC）方法，该方法不依赖物理储备池进行输入数据混合，而是直接从时间序列输入计算多项式项。将NG-RC应用于分类任务，并与传统储备池计算进行性能比较。

Result: 首次证明NG-RC在分类任务上能达到与传统储备池计算相当的性能，验证了NG-RC在预测和分类任务中的通用计算能力。

Conclusion: NG-RC不仅适用于时间序列预测任务，在分类任务上也表现出色，证明了其作为通用计算范式的潜力，扩展了下一代储备池计算的应用范围。

Abstract: An emerging computing paradigm, so-called next-generation reservoir computing (NG-RC) is investigated. True to its namesake, NG-RC requires no actual reservoirs for input data mixing but rather computing the polynomial terms directly from the time series inputs. However, benchmark tests so far reported have been one-sided, limited to prediction tasks of temporal waveforms such as Lorenz 63 attractor and Mackey-Glass chaotic signal. We will demonstrate for the first time that NG-RC can perform classification task as good as conventional RC. This validates the versatile computational capability of NG-RC in tasks of both prediction and classification.

</details>


### [135] [Machine Learning Architectures for the Estimation of Predicted Occupancy Grids in Road Traffic](https://arxiv.org/abs/2512.12907)
*Parthasarathy Nadarajan,Michael Botsch,Sebastian Sardina*

Main category: cs.LG

TL;DR: 提出一种用于复杂交通场景概率时空表示估计的新型机器学习架构，包含堆叠去噪自编码器和随机森林，相比现有架构在精度和计算时间上都有改进


<details>
  <summary>Details</summary>
Motivation: 自动驾驶和主动安全系统需要详细的未来交通场景表示，准确预测交通参与者的未来时空行为对安全至关重要

Method: 使用堆叠去噪自编码器和随机森林构建新型架构，输入为增强占用网格，输出为预测占用网格的概率时空表示，并与基于SDAs和DeconvNet的现有架构进行比较

Result: 通过仿真验证了所提架构的有效性，在准确性和计算时间方面都优于现有架构，并展示了预测占用网格在主动安全领域的应用潜力

Conclusion: 提出的新型机器学习架构能够有效估计复杂交通场景的概率时空表示，为自动驾驶和主动安全系统提供了更准确、更高效的预测工具

Abstract: This paper introduces a novel machine learning architecture for an efficient estimation of the probabilistic space-time representation of complex traffic scenarios. A detailed representation of the future traffic scenario is of significant importance for autonomous driving and for all active safety systems. In order to predict the future space-time representation of the traffic scenario, first the type of traffic scenario is identified and then the machine learning algorithm maps the current state of the scenario to possible future states. The input to the machine learning algorithms is the current state representation of a traffic scenario, termed as the Augmented Occupancy Grid (AOG). The output is the probabilistic space-time representation which includes uncertainties regarding the behaviour of the traffic participants and is termed as the Predicted Occupancy Grid (POG). The novel architecture consists of two Stacked Denoising Autoencoders (SDAs) and a set of Random Forests. It is then compared with the other two existing architectures that comprise of SDAs and DeconvNet. The architectures are validated with the help of simulations and the comparisons are made both in terms of accuracy and computational time. Also, a brief overview on the applications of POGs in the field of active safety is presented.

</details>


### [136] [SeVeDo: A Heterogeneous Transformer Accelerator for Low-Bit Inference via Hierarchical Group Quantization and SVD-Guided Mixed Precision](https://arxiv.org/abs/2512.12930)
*Yuseon Choi,Sangjin Kim,Jungjun Oh,Byeongcheol Kim,Hoi-Jun Yoo*

Main category: cs.LG

TL;DR: SeVeDo是一个基于SVD的异构加速器，通过将异常值敏感组件分离到高精度低秩路径，其余计算在低比特残差数据路径执行，结合分层组量化和SVD引导混合精度，实现高效Transformer推理。


<details>
  <summary>Details</summary>
Motivation: 低比特量化是高效Transformer推理的有前景技术，但激进的位宽降低因激活异常值导致精度下降。现有方法如异常值处理和组量化虽能获得高精度，但能耗较高。

Method: 提出SeVeDo异构加速器：1) 结构上将异常值敏感组件分离到高精度低秩路径；2) 其余计算在低比特残差数据路径执行，采用组量化；3) 分层组量化(HGQ)结合粗粒度浮点缩放和细粒度移位；4) SVD引导混合精度(SVD-MP)通过低秩分解识别精度敏感组件并分配更高位宽。

Result: SeVeDo达到峰值能效13.8TOPS/W，在ViT-Base上实现12.7TOPS/W，在Llama2-7B上实现13.4TOPS/W，超越传统设计。

Conclusion: SeVeDo通过结构分离异常值敏感组件、分层组量化和SVD引导混合精度，在保持精度的同时显著提升Transformer推理的能效。

Abstract: Low-bit quantization is a promising technique for efficient transformer inference by reducing computational and memory overhead. However, aggressive bitwidth reduction remains challenging due to activation outliers, leading to accuracy degradation. Existing methods, such as outlier-handling and group quantization, achieve high accuracy but incur substantial energy consumption. To address this, we propose SeVeDo, an energy-efficient SVD-based heterogeneous accelerator that structurally separates outlier-sensitive components into a high-precision low-rank path, while the remaining computations are executed in a low-bit residual datapath with group quantization. To further enhance efficiency, Hierarchical Group Quantization (HGQ) combines coarse-grained floating-point scaling with fine-grained shifting, effectively reducing dequantization cost. Also, SVD-guided mixed precision (SVD-MP) statically allocates higher bitwidths to precision-sensitive components identified through low-rank decomposition, thereby minimizing floating-point operation cost. Experimental results show that SeVeDo achieves a peak energy efficiency of 13.8TOPS/W, surpassing conventional designs, with 12.7TOPS/W on ViT-Base and 13.4TOPS/W on Llama2-7B benchmarks.

</details>


### [137] [Understanding When Graph Convolutional Networks Help: A Diagnostic Study on Label Scarcity and Structural Properties](https://arxiv.org/abs/2512.12947)
*Nischal Subedi,Ember Kerstetter,Winnie Li,Silo Murphy*

Main category: cs.LG

TL;DR: GCNs在极端标签稀缺时提升最大，利用图结构弥补监督不足；在同质性高的图中，即使特征被噪声替代也能保持性能；但在同质性低且特征强时，GCNs会损害性能。


<details>
  <summary>Details</summary>
Motivation: 尽管GCN已成为半监督节点分类的标准方法，但实践者缺乏关于GCN何时比简单基线提供有意义改进的明确指导。本研究旨在理解GCN何时以及为何有效。

Method: 使用Amazon Computers共购数据进行诊断研究，通过系统实验包括模拟标签稀缺、特征消融和按类别分析，研究图同质性与特征质量之间的相互作用。

Result: GCN性能关键取决于图同质性和特征质量之间的相互作用：1) 在极端标签稀缺时提升最大；2) 在高同质性图中，即使节点特征被随机噪声替代也能匹配原始性能；3) 在同质性低且特征强时，GCN会损害性能；4) 象限分析显示GCN在四种条件中的三种有帮助。

Conclusion: GCN在大多数情况下有帮助，只有在同质性低且特征强的条件下会损害性能。这些发现为实践者决定是否采用基于图的方法提供了实用指导。

Abstract: Graph Convolutional Networks (GCNs) have become a standard approach for semi-supervised node classification, yet practitioners lack clear guidance on when GCNs provide meaningful improvements over simpler baselines. We present a diagnostic study using the Amazon Computers co-purchase data to understand when and why GCNs help. Through systematic experiments with simulated label scarcity, feature ablation, and per-class analysis, we find that GCN performance depends critically on the interaction between graph homophily and feature quality. GCNs provide the largest gains under extreme label scarcity, where they leverage neighborhood structure to compensate for limited supervision. Surprisingly, GCNs can match their original performance even when node features are replaced with random noise, suggesting that structure alone carries sufficient signal on highly homophilous graphs. However, GCNs hurt performance when homophily is low and features are already strong, as noisy neighbors corrupt good predictions. Our quadrant analysis reveals that GCNs help in three of four conditions and only hurt when low homophily meets strong features. These findings offer practical guidance for practitioners deciding whether to adopt graph-based methods.

</details>


### [138] [CoDeQ: End-to-End Joint Model Compression with Dead-Zone Quantizer for High-Sparsity and Low-Precision Networks](https://arxiv.org/abs/2512.12981)
*Jonathan Wenshøj,Tong Chen,Bob Pepin,Raghavendra Selvan*

Main category: cs.LG

TL;DR: CoDeQ是一种完全可微的联合剪枝-量化方法，通过参数化量化器的死区宽度来实现稀疏性，无需外部辅助过程，在保持精度的同时显著减少比特操作。


<details>
  <summary>Details</summary>
Motivation: 当前联合剪枝-量化方法依赖训练循环外的辅助过程来确定压缩参数，增加了工程复杂性和超参数调优，且缺乏直接的数据驱动梯度信号，可能导致次优压缩。

Method: 基于量化器死区等价于幅度剪枝的关键观察，参数化死区宽度并通过反向传播学习，同时学习量化参数。该方法提供明确的稀疏性控制，将稀疏性选择与比特宽度选择解耦。

Result: 在ImageNet上使用ResNet-18，CoDeQ将比特操作减少到约5%，同时在固定精度和混合精度模式下保持接近全精度的准确率。

Conclusion: CoDeQ是一种简单、完全可微的联合剪枝-量化方法，无需辅助过程，架构无关且易于实现，能够同时确定稀疏模式和量化参数，实现端到端优化。

Abstract: While joint pruning--quantization is theoretically superior to sequential application, current joint methods rely on auxiliary procedures outside the training loop for finding compression parameters. This reliance adds engineering complexity and hyperparameter tuning, while also lacking a direct data-driven gradient signal, which might result in sub-optimal compression. In this paper, we introduce CoDeQ, a simple, fully differentiable method for joint pruning--quantization. Our approach builds on a key observation: the dead-zone of a scalar quantizer is equivalent to magnitude pruning, and can be used to induce sparsity directly within the quantization operator. Concretely, we parameterize the dead-zone width and learn it via backpropagation, alongside the quantization parameters. This design provides explicit control of sparsity, regularized by a single global hyperparameter, while decoupling sparsity selection from bit-width selection. The result is a method for Compression with Dead-zone Quantizer (CoDeQ) that supports both fixed-precision and mixed-precision quantization (controlled by an optional second hyperparameter). It simultaneously determines the sparsity pattern and quantization parameters in a single end-to-end optimization. Consequently, CoDeQ does not require any auxiliary procedures, making the method architecture-agnostic and straightforward to implement. On ImageNet with ResNet-18, CoDeQ reduces bit operations to ~5% while maintaining close to full precision accuracy in both fixed and mixed-precision regimes.

</details>


### [139] [Deep Learning-Driven Inversion Framework for Shear Modulus Estimation in Magnetic Resonance Elastography (DIME)](https://arxiv.org/abs/2512.13010)
*Hassan Iftikhar,Rizwan Ahmad,Arunark Kolipaka*

Main category: cs.LG

TL;DR: 提出DIME深度学习框架，通过有限元模拟训练，改善传统MMDI算法在磁共振弹性成像中因拉普拉斯算子对噪声敏感和均匀介质假设导致的刚度估计不准确问题。


<details>
  <summary>Details</summary>
Motivation: 传统MMDI算法依赖亥姆霍兹方程，假设介质均匀、无限，且拉普拉斯算子对噪声敏感，导致刚度估计精度和可靠性受限。

Method: 提出DIME深度学习框架，基于有限元模拟生成的位移场-刚度图对进行训练，采用小图像块捕捉局部波行为并提高对全局图像变化的鲁棒性。

Result: 在合成数据上，DIME相比MMDI具有更低的像素间变异性、更准确的边界划分和更高的地面真值相关性；在解剖学模拟肝脏数据中，DIME与地面真值高度一致（r=0.99，R²=0.98），而MMDI存在低估；在活体数据中，DIME保持了生理一致的刚度模式，与MMDI结果相近但避免了方向性偏差。

Conclusion: DIME在磁共振弹性成像中显示出比传统MMDI更高的精度和鲁棒性，初步结果支持其在临床应用中的可行性。

Abstract: The Multimodal Direct Inversion (MMDI) algorithm is widely used in Magnetic Resonance Elastography (MRE) to estimate tissue shear stiffness. However, MMDI relies on the Helmholtz equation, which assumes wave propagation in a uniform, homogeneous, and infinite medium. Furthermore, the use of the Laplacian operator makes MMDI highly sensitive to noise, which compromises the accuracy and reliability of stiffness estimates. In this study, we propose the Deep-Learning driven Inversion Framework for Shear Modulus Estimation in MRE (DIME), aimed at enhancing the robustness of inversion. DIME is trained on the displacement fields-stiffness maps pair generated through Finite Element Modelling (FEM) simulations. To capture local wave behavior and improve robustness to global image variations, DIME is trained on small image patches. We first validated DIME using homogeneous and heterogeneous datasets simulated with FEM, where DIME produced stiffness maps with low inter-pixel variability, accurate boundary delineation, and higher correlation with ground truth (GT) compared to MMDI. Next, DIME was evaluated in a realistic anatomy-informed simulated liver dataset with known GT and compared directly to MMDI. DIME reproduced ground-truth stiffness patterns with high fidelity (r = 0.99, R^2 = 0.98), while MMDI showed greater underestimation. After validating DIME on synthetic data, we tested the model in in vivo liver MRE data from eight healthy and seven fibrotic subjects. DIME preserved physiologically consistent stiffness patterns and closely matched MMDI, which showed directional bias. Overall, DIME showed higher correlation with ground truth and visually similar stiffness patterns, whereas MMDI displayed a larger bias that can potentially be attributed to directional filtering. These preliminary results highlight the feasibility of DIME for clinical applications in MRE.

</details>


### [140] [Alada: Alternating Adaptation of Momentum Method for Memory-Efficient Matrix Optimization](https://arxiv.org/abs/2512.13034)
*Xiaoyu He,Yu Cai,Jin Jia,Canxi Huang,Wenqing Chen,Zibin Zheng*

Main category: cs.LG

TL;DR: Alada是一种用于大规模矩阵随机优化的自适应动量方法，采用秩一分解估计梯度二阶矩，通过交替更新因子最小化估计误差，实现亚线性内存开销，并可扩展到张量优化。


<details>
  <summary>Details</summary>
Motivation: 传统自适应优化方法如Adam在处理大规模矩阵优化时需要大量内存存储二阶矩矩阵，这限制了其在大型模型训练中的应用。需要开发内存效率更高的自适应优化算法。

Method: Alada采用秩一分解方法估计梯度的二阶矩，将二阶矩矩阵分解为两个因子的乘积，通过交替更新这些因子来最小化估计误差。该方法还包含一阶矩估计规则以增强算法鲁棒性，且不增加额外内存开销。

Result: Alada实现了亚线性内存开销，可扩展到张量优化。在多个自然语言处理任务上的数值研究表明，相比Adam及其变体，Alada显著减少了内存开销，并在大型模型训练中表现出更好的鲁棒性。

Conclusion: Alada是一种内存高效的自适应动量优化方法，通过秩一分解技术有效减少二阶矩估计的内存需求，同时保持与传统方法相当的理论性能，特别适合大规模矩阵和张量优化任务。

Abstract: This work proposes Alada, an adaptive momentum method for stochastic optimization over large-scale matrices. Alada employs a rank-one factorization approach to estimate the second moment of gradients, where factors are updated alternatively to minimize the estimation error. Alada achieves sublinear memory overheads and can be readily extended to optimizing tensor-shaped variables.We also equip Alada with a first moment estimation rule, which enhances the algorithm's robustness without incurring additional memory overheads. The theoretical performance of Alada aligns with that of traditional methods such as Adam. Numerical studies conducted on several natural language processing tasks demonstrate the reduction in memory overheads and the robustness in training large models relative to Adam and its variants.

</details>


### [141] [Understanding Structured Financial Data with LLMs: A Case Study on Fraud Detection](https://arxiv.org/abs/2512.13040)
*Xuwei Tan,Yao Ma,Xueru Zhang*

Main category: cs.LG

TL;DR: FinFRE-RAG：一种两阶段方法，通过重要性引导的特征缩减将表格数据序列化为自然语言，并使用检索增强的上下文学习进行欺诈检测，显著提升LLM在欺诈检测中的性能并提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统表格欺诈检测模型需要大量特征工程且可解释性差，而LLM虽然能提供人类可读的解释，但直接应用于表格欺诈检测时性能不佳，难以处理多特征、极端类别不平衡和缺乏上下文信息的问题。

Method: 提出FinFRE-RAG两阶段方法：1）重要性引导的特征缩减，将数值/分类属性序列化为自然语言；2）检索增强的上下文学习，使用标签感知的实例级示例进行推理。

Result: 在四个公开欺诈数据集和三类开源LLM上，FinFRE-RAG显著优于直接提示方法，在多个设置中与强表格基线模型竞争性相当，缩小了与专业分类器的性能差距。

Conclusion: 虽然LLM仍落后于专业分类器，但FinFRE-RAG显著缩小了性能差距，同时提供可解释的推理过程，突显了LLM作为欺诈分析辅助工具的价值。

Abstract: Detecting fraud in financial transactions typically relies on tabular models that demand heavy feature engineering to handle high-dimensional data and offer limited interpretability, making it difficult for humans to understand predictions. Large Language Models (LLMs), in contrast, can produce human-readable explanations and facilitate feature analysis, potentially reducing the manual workload of fraud analysts and informing system refinements. However, they perform poorly when applied directly to tabular fraud detection due to the difficulty of reasoning over many features, the extreme class imbalance, and the absence of contextual information. To bridge this gap, we introduce FinFRE-RAG, a two-stage approach that applies importance-guided feature reduction to serialize a compact subset of numeric/categorical attributes into natural language and performs retrieval-augmented in-context learning over label-aware, instance-level exemplars. Across four public fraud datasets and three families of open-weight LLMs, FinFRE-RAG substantially improves F1/MCC over direct prompting and is competitive with strong tabular baselines in several settings. Although these LLMs still lag behind specialized classifiers, they narrow the performance gap and provide interpretable rationales, highlighting their value as assistive tools in fraud analysis.

</details>


### [142] [Multi-fidelity aerodynamic data fusion by autoencoder transfer learning](https://arxiv.org/abs/2512.13069)
*Javier Nieto-Centenero,Esther Andrés,Rodrigo Castellanos*

Main category: cs.LG

TL;DR: 提出了一种结合自动编码器迁移学习和多分割保形预测的多保真度深度学习框架，用于在数据稀缺条件下实现不确定性感知的空气动力学数据融合。


<details>
  <summary>Details</summary>
Motivation: 高保真度空气动力学模拟计算成本过高，限制了数据驱动建模的应用。需要开发多保真度策略，利用廉价低保真度信息而不牺牲准确性。

Method: 采用自动编码器迁移学习框架，利用丰富的低保真度数据学习紧凑的潜在物理表示，作为冻结知识库；然后使用稀缺的高保真度样本微调解码器。同时开发了多分割保形预测策略进行不确定性量化。

Result: 在NACA翼型（2D）和跨音速机翼（3D）数据库的表面压力分布测试中，模型成功修正了低保真度偏差，使用极少的高保真度训练数据实现了高精度压力预测。MSCP框架产生了稳健、可操作的不确定性带，点覆盖率超过95%。

Conclusion: 通过结合极端数据效率和不确定性量化，这项工作为数据稀缺环境中的空气动力学回归提供了可扩展且可靠的解决方案。

Abstract: Accurate aerodynamic prediction often relies on high-fidelity simulations; however, their prohibitive computational costs severely limit their applicability in data-driven modeling. This limitation motivates the development of multi-fidelity strategies that leverage inexpensive low-fidelity information without compromising accuracy. Addressing this challenge, this work presents a multi-fidelity deep learning framework that combines autoencoder-based transfer learning with a newly developed Multi-Split Conformal Prediction (MSCP) strategy to achieve uncertainty-aware aerodynamic data fusion under extreme data scarcity. The methodology leverages abundant Low-Fidelity (LF) data to learn a compact latent physics representation, which acts as a frozen knowledge base for a decoder that is subsequently fine-tuned using scarce HF samples. Tested on surface-pressure distributions for NACA airfoils (2D) and a transonic wing (3D) databases, the model successfully corrects LF deviations and achieves high-accuracy pressure predictions using minimal HF training data. Furthermore, the MSCP framework produces robust, actionable uncertainty bands with pointwise coverage exceeding 95%. By combining extreme data efficiency with uncertainty quantification, this work offers a scalable and reliable solution for aerodynamic regression in data-scarce environments.

</details>


### [143] [LikeBench: Evaluating Subjective Likability in LLMs for Personalization](https://arxiv.org/abs/2512.13077)
*Md Awsafur Rahman,Adam Gabrys,Doug Kang,Jingjing Sun,Tian Tan,Ashwin Chandramouli*

Main category: cs.LG

TL;DR: 论文提出LikeBench，一个评估LLM个性化能力的新基准，特别关注"喜好度"这一主观但关键的用户体验维度，将喜好度分解为7个可诊断的指标，发现记忆准确性并不保证高喜好度。


<details>
  <summary>Details</summary>
Motivation: 现有LLM个性化基准主要关注准确回忆用户信息和在下游任务中应用这些信息，但忽略了"喜好度"这一主观且对用户体验至关重要的维度。需要一个新的评估框架来全面衡量LLM在适应用户偏好方面的能力。

Method: 提出LikeBench多会话动态评估框架，让LLM与模拟用户进行对话，仅从持续对话中学习偏好。将喜好度分解为7个诊断指标：情感适应、正式程度匹配、知识适应、引用理解、对话长度匹配、幽默匹配和回调。使用细粒度、基于心理学的描述性人物角色而非粗糙的高/低特质评分。

Result: 研究发现记忆性能并不保证高喜好度：DeepSeek R1的记忆准确性较低（86%，17个事实/个人资料）但在喜好度得分上比Qwen3高出28%，尽管Qwen3的记忆准确性更高（93%，43个事实/个人资料）。即使是SOTA模型如GPT-5在短对话中适应良好，但在更长、更嘈杂的交互中表现出有限的鲁棒性。

Conclusion: 喜好度是LLM个性化评估中一个独立且重要的维度，需要专门的评估框架。LikeBench提供了一个全面衡量LLM适应用户偏好能力的方法，揭示了当前模型在长期、动态交互中的局限性。

Abstract: A personalized LLM should remember user facts, apply them correctly, and adapt over time to provide responses that the user prefers. Existing LLM personalization benchmarks are largely centered on two axes: accurately recalling user information and accurately applying remembered information in downstream tasks. We argue that a third axis, likability, is both subjective and central to user experience, yet under-measured by current benchmarks. To measure likability holistically, we introduce LikeBench, a multi-session, dynamic evaluation framework that measures likability across multiple dimensions by how much an LLM can adapt over time to a user's preferences to provide more likable responses. In LikeBench, the LLMs engage in conversation with a simulated user and learn preferences only from the ongoing dialogue. As the interaction unfolds, models try to adapt to responses, and after each turn, they are evaluated for likability across seven dimensions by the same simulated user. To the best of our knowledge, we are the first to decompose likability into multiple diagnostic metrics: emotional adaptation, formality matching, knowledge adaptation, reference understanding, conversation length fit, humor fit, and callback, which makes it easier to pinpoint where a model falls short. To make the simulated user more realistic and discriminative, LikeBench uses fine-grained, psychologically grounded descriptive personas rather than the coarse high/low trait rating based personas used in prior work. Our benchmark shows that strong memory performance does not guarantee high likability: DeepSeek R1, with lower memory accuracy (86%, 17 facts/profile), outperformed Qwen3 by 28% on likability score despite Qwen3's higher memory accuracy (93%, 43 facts/profile). Even SOTA models like GPT-5 adapt well in short exchanges but show only limited robustness in longer, noisier interactions.

</details>


### [144] [Enhancing Node-Level Graph Domain Adaptation by Alleviating Local Dependency](https://arxiv.org/abs/2512.13149)
*Xinwei Tai,Dongmian Zou,Hongfei Wang*

Main category: cs.LG

TL;DR: 该论文提出通过解相关节点特征来解决图域自适应中的条件偏移问题，使用解相关GCN层和图Transformer层实现，实验证明能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 图机器学习方法近年来取得显著进展，但如何有效将知识从一个图迁移到另一个图仍是关键挑战。无监督图域自适应面临条件偏移问题，阻碍了可迁移性。作者发现条件偏移仅当节点特征存在局部依赖时才会出现，这为改进GDA提供了理论依据。

Method: 首先通过理论分析证明条件偏移与节点特征局部依赖性的关系，并使用马尔可夫链建模依赖节点特征。基于理论发现，提出通过解相关节点特征来改进GDA，具体实现为解相关GCN层和图Transformer层。

Result: 实验结果表明该方法有效，不仅显著优于基线GDA方法，而且在学习到的表征中显示出较小的类内距离，可视化效果清晰。代码已开源。

Conclusion: 节点特征的局部依赖性是导致图域自适应中条件偏移的关键因素，通过解相关节点特征可以有效缓解这一问题，提升图域自适应的性能。

Abstract: Recent years have witnessed significant advancements in machine learning methods on graphs. However, transferring knowledge effectively from one graph to another remains a critical challenge. This highlights the need for algorithms capable of applying information extracted from a source graph to an unlabeled target graph, a task known as unsupervised graph domain adaptation (GDA). One key difficulty in unsupervised GDA is conditional shift, which hinders transferability. In this paper, we show that conditional shift can be observed only if there exists local dependencies among node features. To support this claim, we perform a rigorous analysis and also further provide generalization bounds of GDA when dependent node features are modeled using markov chains. Guided by the theoretical findings, we propose to improve GDA by decorrelating node features, which can be specifically implemented through decorrelated GCN layers and graph transformer layers. Our experimental results demonstrate the effectiveness of this approach, showing not only substantial performance enhancements over baseline GDA methods but also clear visualizations of small intra-class distances in the learned representations. Our code is available at https://github.com/TechnologyAiGroup/DFT

</details>


### [145] [Evaluating Adversarial Attacks on Federated Learning for Temperature Forecasting](https://arxiv.org/abs/2512.13207)
*Karina Chichifoi,Fabio Merizzi,Michele Colajanni*

Main category: cs.LG

TL;DR: 该研究探讨了联邦学习中数据投毒攻击对表面温度预测的影响，发现即使少数恶意客户端也能显著扭曲大范围区域预测，而基于修剪均值的防御对全局偏差攻击有效但对局部补丁攻击无效。


<details>
  <summary>Details</summary>
Motivation: 深度学习与联邦学习结合为下一代天气预报提供了强大工具，但联邦学习的分布式特性引入了新的安全漏洞。数据投毒攻击可能通过注入恶意训练数据来降低模型性能或引入系统性偏差，而气象数据的空间依赖性进一步放大了这种威胁。

Method: 使用Copernicus欧洲区域再分析数据集，模拟地理分布的客户端，评估基于补丁和全局偏差攻击对区域温度预测的影响。最后评估修剪均值聚合作为防御机制的有效性。

Result: 结果显示：1）单个恶意客户端的全局温度偏差攻击可使预测偏移高达-1.7K；2）协调的补丁攻击使均方误差增加三倍以上，产生超过+3.5K的持续区域异常；3）修剪均值聚合能有效防御全局偏差攻击（仅2-13%性能下降），但对补丁攻击反而会放大误差（281-603%）。

Conclusion: 联邦学习在气象预测中面临严重的安全威胁，即使少数恶意客户端也能显著影响大范围区域预测。基于异常值检测的防御机制对空间相关数据的局部攻击效果有限，需要开发更强大的防御策略来保护分布式气象预测系统。

Abstract: Deep learning and federated learning (FL) are becoming powerful partners for next-generation weather forecasting. Deep learning enables high-resolution spatiotemporal forecasts that can surpass traditional numerical models, while FL allows institutions in different locations to collaboratively train models without sharing raw data, addressing efficiency and security concerns. While FL has shown promise across heterogeneous regions, its distributed nature introduces new vulnerabilities. In particular, data poisoning attacks, in which compromised clients inject manipulated training data, can degrade performance or introduce systematic biases. These threats are amplified by spatial dependencies in meteorological data, allowing localized perturbations to influence broader regions through global model aggregation. In this study, we investigate how adversarial clients distort federated surface temperature forecasts trained on the Copernicus European Regional ReAnalysis (CERRA) dataset. We simulate geographically distributed clients and evaluate patch-based and global biasing attacks on regional temperature forecasts. Our results show that even a small fraction of poisoned clients can mislead predictions across large, spatially connected areas. A global temperature bias attack from a single compromised client shifts predictions by up to -1.7 K, while coordinated patch attacks more than triple the mean squared error and produce persistent regional anomalies exceeding +3.5 K. Finally, we assess trimmed mean aggregation as a defense mechanism, showing that it successfully defends against global bias attacks (2-13\% degradation) but fails against patch attacks (281-603\% amplification), exposing limitations of outlier-based defenses for spatially correlated data.

</details>


### [146] [ModSSC: A Modular Framework for Semi-Supervised Classification on Heterogeneous Data](https://arxiv.org/abs/2512.13228)
*Melvin Barbaux*

Main category: cs.LG

TL;DR: ModSSC是一个统一半监督分类的开源Python框架，支持多种算法、数据类型和硬件环境，通过YAML配置简化实验复现和比较研究。


<details>
  <summary>Details</summary>
Motivation: 现有的半监督分类软件支持分散在不同方法和模态中，缺乏统一的框架来整合归纳式和直推式方法，限制了方法的比较和复现。

Method: 开发了ModSSC开源框架，采用模块化代码库统一归纳式和直推式半监督分类，实现经典和最新算法，支持表格、图像、文本、音频和图形数据加载，提供单一配置接口，支持CPU和GPU计算，使用YAML声明式实验描述。

Result: 发布了ModSSC 1.0.0版本，采用MIT许可证，包含详细文档和测试，可在GitHub上获取，为半监督分类研究提供了统一的实验框架。

Conclusion: ModSSC框架解决了半监督分类软件碎片化问题，通过统一的模块化平台促进了算法比较、实验复现和大规模研究，有助于推动半监督学习领域的发展。

Abstract: Semi-supervised classification leverages both labeled and unlabeled data to improve predictive performance, but existing software support is fragmented across methods and modalities. We introduce ModSSC, an open source Python framework that unifies inductive and transductive semi-supervised classification in a modular code base. ModSSC implements a broad range of classical and recent algorithms, provides loaders for tabular, image, text, audio and graph datasets, and exposes a single configuration interface for specifying datasets, models and evaluation protocols. It supports both lightweight classical methods on small datasets running on CPU and recent deep approaches that can exploit multiple GPUs within the same experimental framework. Experiments are described declaratively in YAML, which facilitates reproducing existing work and running large comparative studies. ModSSC 1.0.0 is released under the MIT license with extensive documentation and tests, and is available at https://github.com/ModSSC/ModSSC.

</details>


### [147] [Learning to Retrieve with Weakened Labels: Robust Training under Label Noise](https://arxiv.org/abs/2512.13237)
*Arnab Sharma*

Main category: cs.LG

TL;DR: 提出标签弱化方法应对检索模型训练中的稀疏标注和标签噪声问题，通过生成一组可能标签而非单一标签来提高模型鲁棒性


<details>
  <summary>Details</summary>
Motivation: 神经编码器在NLP领域用于密集检索任务，但训练数据中的稀疏标注和标签噪声使得训练或微调这类检索模型具有挑战性。现有方法要么需要调整超参数，要么增加了训练设置的复杂性

Method: 采用标签弱化方法，不为每个查询-文档对强制指定单一可能错误的标签，而是基于观察到的监督信号和模型置信度得分生成一组合理的标签

Result: 在四个不同的排序数据集上对两个检索模型和一个重排序模型进行广泛评估，使用语义感知噪声生成技术创建不同噪声比例的设置。初步结果显示，与10种最先进的损失函数相比，标签弱化能提高检索任务的性能

Conclusion: 标签弱化方法能有效应对训练数据中的标签噪声问题，提高检索模型的鲁棒性和性能，相比现有方法具有优势

Abstract: Neural Encoders are frequently used in the NLP domain to perform dense retrieval tasks, for instance, to generate the candidate documents for a given query in question-answering tasks. However, sparse annotation and label noise in the training data make it challenging to train or fine-tune such retrieval models. Although existing works have attempted to mitigate these problems by incorporating modified loss functions or data cleaning, these approaches either require some hyperparameters to tune during training or add substantial complexity to the training setup. In this work, we consider a label weakening approach to generate robust retrieval models in the presence of label noise. Instead of enforcing a single, potentially erroneous label for each query document pair, we allow for a set of plausible labels derived from both the observed supervision and the model's confidence scores. We perform an extensive evaluation considering two retrieval models, one re-ranking model, considering four diverse ranking datasets. To this end, we also consider a realistic noisy setting by using a semantic-aware noise generation technique to generate different ratios of noise. Our initial results show that label weakening can improve the performance of the retrieval tasks in comparison to 10 different state-of-the-art loss functions.

</details>


### [148] [BézierFlow: Bézier Stochastic Interpolant Schedulers for Few-Step Generation](https://arxiv.org/abs/2512.13255)
*Yunhong Min,Juil Koo,Seungwoo Yoo,Minhyuk Sung*

Main category: cs.LG

TL;DR: BézierFlow是一种轻量级训练方法，用于预训练扩散和流模型的少步生成，仅需15分钟训练即可在≤10步采样中获得2-3倍性能提升，通过学习贝塞尔曲线参数化的最优轨迹变换而非传统ODE离散化。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级训练方法主要局限于学习ODE离散化的最优时间步，限制了搜索空间。需要扩展这一范围，通过学习采样轨迹的最优变换来提升少步生成性能。

Method: 提出将随机插值调度器参数化为贝塞尔函数，通过控制点自然满足边界条件、可微性和信噪比单调性等关键要求。将问题简化为学习时间范围内的有序点集，从ODE时间步解释转变为贝塞尔控制点。

Result: 在多种预训练扩散和流模型上，BézierFlow始终优于先前的时间步学习方法，在≤10步采样中实现2-3倍性能提升，仅需15分钟训练时间。

Conclusion: 通过将搜索空间从离散时间步扩展到基于贝塞尔的轨迹变换，BézierFlow证明了学习最优轨迹变换的有效性，为少步生成提供了高效解决方案。

Abstract: We introduce BézierFlow, a lightweight training approach for few-step generation with pretrained diffusion and flow models. BézierFlow achieves a 2-3x performance improvement for sampling with $\leq$ 10 NFEs while requiring only 15 minutes of training. Recent lightweight training approaches have shown promise by learning optimal timesteps, but their scope remains restricted to ODE discretizations. To broaden this scope, we propose learning the optimal transformation of the sampling trajectory by parameterizing stochastic interpolant (SI) schedulers. The main challenge lies in designing a parameterization that satisfies critical desiderata, including boundary conditions, differentiability, and monotonicity of the SNR. To effectively meet these requirements, we represent scheduler functions as Bézier functions, where control points naturally enforce these properties. This reduces the problem to learning an ordered set of points in the time range, while the interpretation of the points changes from ODE timesteps to Bézier control points. Across a range of pretrained diffusion and flow models, BézierFlow consistently outperforms prior timestep-learning methods, demonstrating the effectiveness of expanding the search space from discrete timesteps to Bézier-based trajectory transformations.

</details>


### [149] [KD-PINN: Knowledge-Distilled PINNs for ultra-low-latency real-time neural PDE solvers](https://arxiv.org/abs/2512.13336)
*Karim Bounja,Lahcen Laayouni,Abdeljalil Sakat*

Main category: cs.LG

TL;DR: 提出KD-PINN框架，通过知识蒸馏将高容量教师模型的预测精度转移到紧凑学生模型，实现物理精度保持和4.8-6.9倍推理加速，达到超低延迟实时计算


<details>
  <summary>Details</summary>
Motivation: 开发准确且超低延迟的神经PDE求解器，解决传统PINNs推理延迟较高的问题，实现实时物理系统模拟

Method: 使用知识蒸馏框架，通过连续调整Kullback-Leibler散度将高容量教师PINN的知识转移到紧凑学生模型，在多种PDE上进行评估

Result: 学生模型保持教师物理精度（平均RMSE增加低于0.64%），推理加速4.8-6.9倍，平均推理延迟5.3ms，达到超10ms的实时性能，蒸馏过程还具有正则化效果

Conclusion: KD-PINN框架成功实现了准确超低延迟的神经PDE求解器，为实时物理系统模拟提供了可行方案，知识蒸馏能显著降低PINNs的推理延迟

Abstract: This work introduces Knowledge-Distilled Physics-Informed Neural Networks (KD-PINN), a framework that transfers the predictive accuracy of a high-capacity teacher model to a compact student through a continuous adaptation of the Kullback-Leibler divergence. To confirm its generality for various dynamics and dimensionalities, the framework is evaluated on a representative set of partial differential equations (PDEs). In all tested cases, the student model preserved the teacher's physical accuracy, with a mean RMSE increase below 0.64%, and achieved inference speedups ranging from 4.8x (Navier-Stokes) to 6.9x (Burgers). The distillation process also revealed a regularizing effect. With an average inference latency of 5.3 ms on CPU, the distilled models enter the ultra-low-latency real-time regime defined by sub-10 ms performance. Finally, this study examines how knowledge distillation reduces inference latency in PINNs to contribute to the development of accurate ultra-low-latency neural PDE solvers.

</details>


### [150] [FROC: A Unified Framework with Risk-Optimized Control for Machine Unlearning in LLMs](https://arxiv.org/abs/2512.13337)
*Si Qi Goh,Yongsen Zheng,Ziyao Liu,Sami Hormi,Kwok-Yan Lam*

Main category: cs.LG

TL;DR: FROC是一个用于大语言模型机器遗忘的统一框架，通过风险优化控制来平衡遗忘充分性和效用保留，提供可解释的风险评估和配置选择。


<details>
  <summary>Details</summary>
Motivation: 当前机器遗忘技术缺乏有效的风险评估和控制机制，难以在遗忘充分性和效用损失之间取得平衡，这阻碍了选择适当的安全与效用权衡策略，并引发了关于"被遗忘权"的信任问题。

Method: FROC采用符合性风格的风险控制框架，引入连续风险模型聚合遗忘不足和效用退化，计算符合性遗忘风险(CUR)和风险控制配置集，指导超参数选择。

Result: 实验表明FROC能产生稳定、可解释的风险景观，揭示遗忘配置、语义偏移和效用影响之间的一致关系，为大规模LLM部署中的遗忘行为管理提供实用基础。

Conclusion: FROC将机器遗忘重新定义为可控、风险感知的过程，为大语言模型中的遗忘行为管理提供了系统化的风险评估和控制框架。

Abstract: Machine unlearning (MU) seeks to eliminate the influence of specific training examples from deployed models. As large language models (LLMs) become widely used, managing risks arising from insufficient forgetting or utility loss is increasingly crucial. Current MU techniques lack effective mechanisms for evaluating and controlling these risks, hindering the selection of strategies that appropriately balance safety and utility, and raising trust concerns surrounding the "right to be forgotten." To address these issues, we propose FROC, a unified framework with Risk-Optimized Control for machine unlearning in LLMs. FROC is built around a conformal-style risk-control formulation that expresses a user-specified risk budget on unlearning behavior. This probability-based constraint enables FROC to compare MU strategies, identify feasible operating regions, and guide hyperparameter selection according to desired trade-offs between forgetting sufficiency and utility preservation. To operationalize this constraint, FROC introduces a smoothly varying continuous risk model that aggregates forgetting deficiency and utility degradation into a single configuration-level score. Building on conformal risk analysis, FROC computes (1) the Conformal Unlearning Risk (CUR), a data-driven estimated value on the probability that forgotten samples continue to influence model predictions, and (2) risk-controlled configuration sets, which identify unlearning hyperparameters that are valid under the specified risk budget. Experiments across multiple LLM MU methods demonstrate that FROC produces stable, interpretable risk landscapes and reveals consistent relationships between unlearning configurations, semantic shift, and utility impact. FROC reframes MU as a controllable, risk-aware process and offers a practical foundation for managing unlearning behavior in large-scale LLM deployments.

</details>


### [151] [On the Effectiveness of Membership Inference in Targeted Data Extraction from Large Language Models](https://arxiv.org/abs/2512.13352)
*Ali Al Sahili,Ali Chehab,Razane Tajeddine*

Main category: cs.LG

TL;DR: 该研究整合多种成员推断攻击技术到数据提取流程中，系统评估它们在提取训练数据时的有效性，并与传统基准测试结果进行对比。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型容易记忆训练数据，带来严重的隐私风险，特别是训练数据提取和成员推断攻击。现有研究表明这两种威胁相互关联，攻击者可以通过生成大量文本来提取数据，然后用成员推断攻击验证数据是否在训练集中。

Method: 将多种成员推断攻击技术整合到数据提取流程中，系统性地对它们的有效性进行基准测试。然后比较这种集成设置下的性能与传统成员推断攻击基准测试的结果。

Result: 研究评估了不同成员推断攻击技术在数据提取场景中的实际效用，揭示了它们在真实世界提取场景中的性能表现。

Conclusion: 通过整合成员推断攻击到数据提取流程并系统评估，该研究为理解这些隐私攻击技术的实际应用效果提供了重要见解，有助于评估真实世界隐私风险。

Abstract: Large Language Models (LLMs) are prone to mem- orizing training data, which poses serious privacy risks. Two of the most prominent concerns are training data extraction and Membership Inference Attacks (MIAs). Prior research has shown that these threats are interconnected: adversaries can extract training data from an LLM by querying the model to generate a large volume of text and subsequently applying MIAs to verify whether a particular data point was included in the training set. In this study, we integrate multiple MIA techniques into the data extraction pipeline to systematically benchmark their effectiveness. We then compare their performance in this integrated setting against results from conventional MIA bench- marks, allowing us to evaluate their practical utility in real-world extraction scenarios.

</details>


### [152] [Dual-Phase Federated Deep Unlearning via Weight-Aware Rollback and Reconstruction](https://arxiv.org/abs/2512.13381)
*Changjun Zhou,Jintao Zheng,Leyou Yang,Pengfei Wang*

Main category: cs.LG

TL;DR: DPUL是一种新颖的服务器端联邦遗忘方法，通过深度遗忘所有有影响力的权重来防止隐私泄露，相比现有方法在准确性和时间成本上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有联邦遗忘方法存在高计算需求、复杂激励机制和客户端计算能力差异等问题，导致时间长、成本高。现有服务器端知识蒸馏方法仅移除目标客户端的更新，忽略了其他客户端贡献中嵌入的隐私，可能导致隐私泄露。

Method: DPUL包含三个组件：(1) 通过过滤客户端更新幅度识别高权重参数，并将其回滚以确保深度移除；(2) 利用变分自编码器(VAE)重建和消除低权重参数；(3) 使用基于投影的技术恢复模型。

Result: 在四个数据集上的实验结果表明，DPUL超越了最先进的基线方法，准确率提高了1%-5%，时间成本最多减少了12倍。

Conclusion: DPUL提供了一种有效的服务器端联邦遗忘解决方案，能够深度遗忘所有有影响力的权重，防止隐私泄露，同时在准确性和效率方面都有显著改进。

Abstract: Federated Unlearning (FUL) focuses on client data and computing power to offer a privacy-preserving solution. However, high computational demands, complex incentive mechanisms, and disparities in client-side computing power often lead to long times and higher costs. To address these challenges, many existing methods rely on server-side knowledge distillation that solely removes the updates of the target client, overlooking the privacy embedded in the contributions of other clients, which can lead to privacy leakage. In this work, we introduce DPUL, a novel server-side unlearning method that deeply unlearns all influential weights to prevent privacy pitfalls. Our approach comprises three components: (i) identifying high-weight parameters by filtering client update magnitudes, and rolling them back to ensure deep removal. (ii) leveraging the variational autoencoder (VAE) to reconstruct and eliminate low-weight parameters. (iii) utilizing a projection-based technique to recover the model. Experimental results on four datasets demonstrate that DPUL surpasses state-of-the-art baselines, providing a 1%-5% improvement in accuracy and up to 12x reduction in time cost.

</details>


### [153] [On-Device Continual Learning for Unsupervised Visual Anomaly Detection in Dynamic Manufacturing](https://arxiv.org/abs/2512.13497)
*Haoyu Ren,Kay Koehle,Kirill Dorofeev,Darko Anicic*

Main category: cs.LG

TL;DR: 本文提出了一种用于动态制造环境的设备端持续学习视觉异常检测方法，通过轻量级特征提取器和增量核心集更新机制，在资源受限的边缘设备上实现快速、内存高效的模型适应。


<details>
  <summary>Details</summary>
Motivation: 现代制造业中，动态灵活的生产环境带来三大挑战：频繁的产品变更需要快速模型更新；传统边缘硬件缺乏运行大型AI模型的资源；异常和正常训练数据通常稀缺，特别是对新引入的产品变体。

Method: 基于PatchCore扩展，提出设备端持续学习的无监督视觉异常检测与定位方法。采用轻量级特征提取器和基于k-center选择的增量核心集更新机制，支持从有限数据中快速、内存高效的适应，避免昂贵的云端重训练。

Result: 在模拟灵活生产的工业用例测试中，该方法相比基线实现了12%的AUROC提升，内存使用减少80%，且训练速度比批量重训练更快。

Conclusion: 该方法为动态智能制造提供了准确、资源高效且自适应的视觉异常检测解决方案，适合在资源受限的边缘设备上部署。

Abstract: In modern manufacturing, Visual Anomaly Detection (VAD) is essential for automated inspection and consistent product quality. Yet, increasingly dynamic and flexible production environments introduce key challenges: First, frequent product changes in small-batch and on-demand manufacturing require rapid model updates. Second, legacy edge hardware lacks the resources to train and run large AI models. Finally, both anomalous and normal training data are often scarce, particularly for newly introduced product variations. We investigate on-device continual learning for unsupervised VAD with localization, extending the PatchCore to incorporate online learning for real-world industrial scenarios. The proposed method leverages a lightweight feature extractor and an incremental coreset update mechanism based on k-center selection, enabling rapid, memory-efficient adaptation from limited data while eliminating costly cloud retraining. Evaluations on an industrial use case are conducted using a testbed designed to emulate flexible production with frequent variant changes in a controlled environment. Our method achieves a 12% AUROC improvement over the baseline, an 80% reduction in memory usage, and faster training compared to batch retraining. These results confirm that our method delivers accurate, resource-efficient, and adaptive VAD suitable for dynamic and smart manufacturing.

</details>


### [154] [Learning under Distributional Drift: Reproducibility as an Intrinsic Statistical Resource](https://arxiv.org/abs/2512.13506)
*Sofiya Zaichyk*

Main category: cs.LG

TL;DR: 论文提出"可重复性预算"概念，量化系统在分布漂移下的统计可重复性能力，并推导出最优的泛化误差界。


<details>
  <summary>Details</summary>
Motivation: 在分布漂移环境下，传统泛化界可能失效，需要新的理论框架来量化学习系统在数据生成分布不断变化时的统计性能。

Method: 引入可重复性预算C_T作为统计原语，定义为耦合学习器-环境演化的累积Fisher-Rao路径长度，衡量学习过程中积累的总分布运动。

Result: 推导出O(T^{-1/2} + C_T/T)阶的漂移-反馈泛化界，并证明匹配的极小极大下界，表明该速率是最优的。

Conclusion: 建立了可重复性速度极限：任何算法的最坏情况泛化误差都不能低于数据生成过程的平均Fisher-Rao漂移率C_T/T所施加的限制。

Abstract: Statistical learning under distributional drift remains insufficiently characterized: when each observation alters the data-generating law, classical generalization bounds can collapse. We introduce a new statistical primitive, the reproducibility budget $C_T$, which quantifies a system's finite capacity for statistical reproducibility - the extent to which its sampling process can remain governed by a consistent underlying distribution in the presence of both exogenous change and endogenous feedback. Formally, $C_T$ is defined as the cumulative Fisher-Rao path length of the coupled learner-environment evolution, measuring the total distributional motion accumulated during learning. From this construct we derive a drift-feedback generalization bound of order $O(T^{-1/2} + C_T/T)$, and we prove a matching minimax lower bound showing that this rate is minimax-optimal. Consequently, the results establish a reproducibility speed limit: no algorithm can achieve smaller worst-case generalization error than that imposed by the average Fisher-Rao drift rate $C_T/T$ of the data-generating process. The framework situates exogenous drift, adaptive data analysis, and performative prediction within a common geometric structure, with $C_T$ emerging as the intrinsic quantity measuring distributional motion across these settings.

</details>


### [155] [Image Diffusion Preview with Consistency Solver](https://arxiv.org/abs/2512.13592)
*Fu-Yun Wang,Hao Zhou,Liangzhe Yuan,Sanghyun Woo,Boqing Gong,Bohyung Han,Ming-Hsuan Yang,Han Zhang,Yukun Zhu,Ting Liu,Long Zhao*

Main category: cs.LG

TL;DR: 提出Diffusion Preview范式，使用快速低步采样生成预览供用户评估，满意后再进行全步细化。提出ConsistencySolver优化器，通过强化学习训练，在低步数下提升预览质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 图像扩散模型的缓慢推理过程显著降低了交互式用户体验。现有加速方法（包括免训练求解器和训练后蒸馏）难以提供高质量预览或确保预览与最终输出的一致性。

Method: 提出Diffusion Preview范式，采用快速低步采样生成预览。提出ConsistencySolver，基于通用线性多步方法，通过强化学习优化的轻量级可训练高阶求解器，提升预览质量和一致性。

Result: ConsistencySolver在低步数场景下显著提升生成质量和一致性。与Multistep DPM-Solver相比，使用47%更少步数达到相当的FID分数，同时优于蒸馏基线。用户研究表明，该方法将用户交互时间减少近50%同时保持生成质量。

Conclusion: Diffusion Preview范式结合ConsistencySolver优化器，为高效的预览-细化工作流程提供了理想解决方案，显著改善了图像扩散模型的交互式用户体验。

Abstract: The slow inference process of image diffusion models significantly degrades interactive user experiences. To address this, we introduce Diffusion Preview, a novel paradigm employing rapid, low-step sampling to generate preliminary outputs for user evaluation, deferring full-step refinement until the preview is deemed satisfactory. Existing acceleration methods, including training-free solvers and post-training distillation, struggle to deliver high-quality previews or ensure consistency between previews and final outputs. We propose ConsistencySolver derived from general linear multistep methods, a lightweight, trainable high-order solver optimized via Reinforcement Learning, that enhances preview quality and consistency. Experimental results demonstrate that ConsistencySolver significantly improves generation quality and consistency in low-step scenarios, making it ideal for efficient preview-and-refine workflows. Notably, it achieves FID scores on-par with Multistep DPM-Solver using 47% fewer steps, while outperforming distillation baselines. Furthermore, user studies indicate our approach reduces overall user interaction time by nearly 50% while maintaining generation quality. Code is available at https://github.com/G-U-N/consolver.

</details>


### [156] [StutterFuse: Mitigating Modality Collapse in Stuttering Detection with Jaccard-Weighted Metric Learning and Gated Fusion](https://arxiv.org/abs/2512.13632)
*Guransh Singh,Md Shah Fahad*

Main category: cs.LG

TL;DR: StutterFuse是首个用于多标签口吃检测的检索增强分类器，通过结合临床示例记忆库解决重叠性口吃检测难题，在SEP-28k数据集上达到0.65加权F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有参数化模型难以区分复杂重叠的口吃现象（如"阻塞"与"延长"同时发生），因为训练数据中这些特定组合稀缺。虽然检索增强生成（RAG）在NLP领域取得突破，但在病理语音处理中尚未探索。

Method: 提出StutterFuse检索增强分类器，通过Conformer编码器结合非参数化临床示例记忆库实现基于参考的分类。解决"模态坍塌"问题的方法包括：1）SetCon（Jaccard加权度量学习目标）优化多标签集相似性；2）门控混合专家融合策略动态协调声学证据与检索上下文。

Result: 在SEP-28k数据集上获得0.65加权F1分数，优于强基线模型，并展现出显著的零样本跨语言泛化能力。

Conclusion: StutterFuse首次将检索增强范式引入病理语音处理，通过参考而非记忆的方式有效检测重叠性口吃现象，解决了现有模型在复杂口吃组合识别上的局限性。

Abstract: Stuttering detection breaks down when disfluencies overlap. Existing parametric models struggle to distinguish complex, simultaneous disfluencies (e.g., a 'block' with a 'prolongation') due to the scarcity of these specific combinations in training data. While Retrieval-Augmented Generation (RAG) has revolutionized NLP by grounding models in external knowledge, this paradigm remains unexplored in pathological speech processing. To bridge this gap, we introduce StutterFuse, the first Retrieval-Augmented Classifier (RAC) for multi-label stuttering detection. By conditioning a Conformer encoder on a non-parametric memory bank of clinical examples, we allow the model to classify by reference rather than memorization. We further identify and solve "Modality Collapse", an "Echo Chamber" effect where naive retrieval boosts recall but degrades precision. We mitigate this using: (1) SetCon, a Jaccard-Weighted Metric Learning objective that optimizes for multi-label set similarity, and (2) a Gated Mixture-of-Experts fusion strategy that dynamically arbitrates between acoustic evidence and retrieved context. On the SEP-28k dataset, StutterFuse achieves a weighted F1-score of 0.65, outperforming strong baselines and demonstrating remarkable zero-shot cross-lingual generalization.

</details>


### [157] [A Scientific Reasoning Model for Organic Synthesis Procedure Generation](https://arxiv.org/abs/2512.13668)
*Guoqing Liu,Junren Li,Zihan Zhao,Eray Inanc,Krzysztof Maziarz,Jose Garrido Torres,Victor Garcia Satorras,Shoko Ueda,Christopher M. Bishop,Marwin Segler*

Main category: cs.LG

TL;DR: QFANG是一个科学推理语言模型，能够从反应方程式直接生成精确的结构化实验程序，通过化学引导推理框架和强化学习优化，在合成程序生成方面优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 解决计算机辅助合成规划中的关键挑战：弥合计算路线设计与实际实验室执行之间的差距，特别是准确预测每个合成步骤的可行实验程序。

Method: 1. 构建高质量数据集：从专利文献中提取处理905,990个化学反应及其结构化操作序列；2. 引入化学引导推理框架生成基于化学知识的思维链数据；3. 监督微调激发复杂化学推理；4. 应用可验证奖励强化学习进一步提升程序准确性。

Result: QFANG在传统NLP相似性指标和化学感知评估器上均优于先进的通用推理模型和最近邻检索基线，能够泛化到某些域外反应类别，并适应实验室条件和用户特定约束的变化。

Conclusion: QFANG生成高质量合成程序的能力代表了向弥合计算合成规划与全自动实验室合成之间差距的重要一步。

Abstract: Solving computer-aided synthesis planning is essential for enabling fully automated, robot-assisted synthesis workflows and improving the efficiency of drug discovery. A key challenge, however, is bridging the gap between computational route design and practical laboratory execution, particularly the accurate prediction of viable experimental procedures for each synthesis step. In this work, we present QFANG, a scientific reasoning language model capable of generating precise, structured experimental procedures directly from reaction equations, with explicit chain-of-thought reasoning. To develop QFANG, we curated a high-quality dataset comprising 905,990 chemical reactions paired with structured action sequences, extracted and processed from patent literature using large language models. We introduce a Chemistry-Guided Reasoning (CGR) framework that produces chain-of-thought data grounded in chemical knowledge at scale. The model subsequently undergoes supervised fine-tuning to elicit complex chemistry reasoning. Finally, we apply Reinforcement Learning from Verifiable Rewards (RLVR) to further enhance procedural accuracy. Experimental results demonstrate that QFANG outperforms advanced general-purpose reasoning models and nearest-neighbor retrieval baselines, measured by traditional NLP similarity metrics and a chemically aware evaluator using an LLM-as-a-judge. Moreover, QFANG generalizes to certain out-of-domain reaction classes and adapts to variations in laboratory conditions and user-specific constraints. We believe that QFANG's ability to generate high-quality synthesis procedures represents an important step toward bridging the gap between computational synthesis planning and fully automated laboratory synthesis.

</details>


### [158] [Directional Textual Inversion for Personalized Text-to-Image Generation](https://arxiv.org/abs/2512.13672)
*Kunhee Kim,NaHyeon Park,Kibeom Hong,Hyunjung Shim*

Main category: cs.LG

TL;DR: 本文提出方向性文本反转（DTI），通过固定嵌入向量的模长并仅优化单位超球面上的方向来解决传统文本反转中嵌入模长膨胀导致的复杂提示词生成失败问题。


<details>
  <summary>Details</summary>
Motivation: 传统文本反转（TI）方法在复杂提示词上经常失败，作者发现这源于嵌入模长膨胀问题：学习到的token漂移到分布外的模长值，破坏了预归一化Transformer中的提示条件。

Method: 提出方向性文本反转（DTI）：1）将嵌入向量的模长固定为分布内尺度；2）仅通过黎曼SGD优化单位超球面上的方向；3）将方向学习建模为具有冯·米塞斯-费舍尔先验的MAP估计，产生恒定方向先验梯度。

Result: DTI在个性化任务中相比TI及其变体提高了文本保真度，同时保持了主体相似性。其超球面参数化支持学习概念间的平滑、语义连贯插值（slerp），这是标准TI不具备的能力。

Conclusion: 仅优化方向是实现提示词忠实个性化的鲁棒且可扩展路径，方向学习比模长调整更关键于语义表达，而模长膨胀会损害上下文信息。

Abstract: Textual Inversion (TI) is an efficient approach to text-to-image personalization but often fails on complex prompts. We trace these failures to embedding norm inflation: learned tokens drift to out-of-distribution magnitudes, degrading prompt conditioning in pre-norm Transformers. Empirically, we show semantics are primarily encoded by direction in CLIP token space, while inflated norms harm contextualization; theoretically, we analyze how large magnitudes attenuate positional information and hinder residual updates in pre-norm blocks. We propose Directional Textual Inversion (DTI), which fixes the embedding magnitude to an in-distribution scale and optimizes only direction on the unit hypersphere via Riemannian SGD. We cast direction learning as MAP with a von Mises-Fisher prior, yielding a constant-direction prior gradient that is simple and efficient to incorporate. Across personalization tasks, DTI improves text fidelity over TI and TI-variants while maintaining subject similarity. Crucially, DTI's hyperspherical parameterization enables smooth, semantically coherent interpolation between learned concepts (slerp), a capability that is absent in standard TI. Our findings suggest that direction-only optimization is a robust and scalable path for prompt-faithful personalization.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [159] [Coarse-Graining via Lumping: Exact Calculations and Fundamental Limitations](https://arxiv.org/abs/2512.11974)
*Gianluca Teza,Attilio L. Stella,Trevor GrandPre*

Main category: cond-mat.stat-mech

TL;DR: 提出一种聚合方法构建有效的半马尔可夫模型，能精确再现微观动力学的完整熵产生统计，即使在合并隐藏电流循环时仍能保持平均熵产生准确性。


<details>
  <summary>Details</summary>
Motivation: 在微观和纳米尺度实验中，当状态分辨率有限时，检测时间反演对称性破缺通常很困难。需要开发新方法来处理有限观测条件下的非平衡态统计推断。

Method: 引入一种聚合方法，构建有效的半马尔可夫模型，能够精确再现微观动力学的完整熵产生统计分布。该方法即使在合并隐藏电流循环时仍能保持平均熵产生的准确性。

Result: 该方法能准确捕捉平均熵产生，尽管高阶信息可能不可避免会丢失。在这些情况下，能够捕获与实验一致的涨落定理违反现象，为开发新的非平衡态推断策略开辟了路径。

Conclusion: 提出的聚合方法为在有限观测条件下检测时间反演对称性破缺提供了有效工具，能够处理隐藏电流循环的情况，并为非平衡态统计推断开辟了新途径。

Abstract: Detecting broken time-reversibility at micro- and nanoscale is often difficult when experiments offer limited state resolution. We introduce a lumping method that builds an effective semi-Markov model able to reproduce exactly the full entropy-production statistics of the microscopic dynamics. The mean entropy production stays accurate even when hidden current-carrying cycles are merged, though higher-order information can be unavoidably lost. In these cases, we capture violations of fluctuation theorems consistent with experiments, opening a path to novel inference strategies out of equilibrium.

</details>


### [160] [From Frequency Dependent Specific Heat to Fictive Temperature of a Glassy Liquid](https://arxiv.org/abs/2512.12759)
*Biman Bagchi*

Main category: cond-mat.stat-mech

TL;DR: 该研究使用线性响应理论将玻璃形成液体中虚构温度的时间演化与记忆函数联系起来，通过自洽计算捕捉弛豫速率与结构响应函数之间的相互依赖关系，并以二氧化硅为例进行了数值模拟。


<details>
  <summary>Details</summary>
Motivation: 玻璃形成液体在快速淬火时会因有限弛豫时间而偏离平衡态，弛豫过程随时间逐渐变慢。需要一种方法来描述这种非平衡态玻璃系统的演化，特别是虚构温度的时间依赖性及其与弛豫特性的关系。

Method: 使用线性响应理论将虚构温度的时间依赖性连接到记忆函数，该函数与频率依赖的比热相关，而比热本身又依赖于虚构温度。采用自洽计算方法，对二氧化硅应用该理论，其中频率依赖比热的弛豫函数建模为拉伸指数William-Watts函数，弛豫时间建模为Vogel-Fulcher-Tammann函数。

Result: 成功计算出自洽的虚构温度T_f(t)，展示了随着时间推移虚构温度如何从实际温度偏离。冷却过程中虚构温度高于实际温度，冷却停止后虚构温度以取决于液体弛豫特性的速率趋近最终温度。

Conclusion: 该研究建立了连接虚构温度时间演化与记忆函数的理论框架，通过自洽计算成功描述了玻璃形成液体在淬火过程中的非平衡态演化，为理解玻璃化转变过程中的弛豫动力学提供了重要理论工具。

Abstract: Upon rapid quenching of temperature of a glass forming liquid, the system falls out of equilibrium due its finite relaxation time. Additionally, the relaxation becomes progressively slower with time. The created nonequilibrium state of the glassy system is conveniently described by introducing a fictive temperature which provides the instantaneous state of the nonequilibrium system. The fictive temperature $T_{f} (t)$ is time dependent. During cooling, the fictive temperature is higher than the actual temperature. After the cooling or quenching has ceased, the fictive temperature approaches the final temperature at a rate that depends on the relaxation properties of the liquid. In this work we use linear response theory to connect the time dependence of the fictive temperature to memory function which is shown to be related to the frequency dependent specific heat which itself depends on the fictive temperature $T_{f} (t)$. Thus, one requires { \it a self-consistent calculation} to capture the interdependence of relaxation rate and structural response function. We present a numerical calculation where we apply our relations to silica where the relaxation function that describes the frequency dependent specific heat and is modeled as a stretched exponential William-Watts (WW) function, while the relaxation time is modeled as a Vogel-Fulcher-Tammann (VFT). We calculate the fictive temperature self-consistently. $T_{f}(t)$ exhibits the fall out from actual temperature as time (t) progresses.

</details>


### [161] [Theory of the $β$-Relaxation Beyond Mode-Coupling Theory: A Microscopic Treatment](https://arxiv.org/abs/2512.13092)
*Corentin C. L. Laudicina,Liesbeth M. C. Janssen,Grzegorz Szamel*

Main category: cond-mat.stat-mech

TL;DR: 本文开发了一种系统扩展模式耦合理论的方法，将临界动力学涨落纳入其中，建立了超越平均场的结构弛豫预测框架。


<details>
  <summary>Details</summary>
Motivation: 传统模式耦合理论（MCT）是平均场理论，无法处理临界动力学涨落。本文旨在开发一个超越平均场的理论框架，将涨落效应系统性地纳入MCT中，以更准确地描述结构弛豫过程。

Method: 从微观图论理论出发，识别模式耦合转变附近的主导发散图类，证明相应的渐近级数在临界维度d_c=8以下主导平均场。通过映射到随机时空场中的序参数演化过程，重整化这些发散，得到β-弛豫的有效理论。

Result: 建立了与随机β-弛豫理论完全一致的β-弛豫有效理论，所有耦合常数都通过液体静态结构因子微观表达。对硬球系统的计算表明，涨落单独恢复了遍历性，并将假定的平均场转变替换为平滑交叉。

Conclusion: 本文成功构建了一个超越平均场的预测框架，将临界动力学涨落系统性地纳入模式耦合理论，为结构弛豫提供了更准确的理论描述。

Abstract: We develop a systematic extension of mode-coupling theory (MCT) that incorporates critical dynamical fluctuations. Starting from a microscopic diagrammatic theory, we identify dominant classes of divergent diagrams near the mode-coupling transition and show that the corresponding asymptotic series dominates the mean-field below an upper critical dimension $d_c=8$. To resum these divergences, we construct a mapping to a stochastic dynamical process in which the order parameter evolves under random spatiotemporal fields. This reformulation provides a controlled, fully dynamical derivation of an effective theory for the $β$-relaxation which remarkably coincides with stochastic beta-relaxation theory [T. Rizzo, EPL 106, 56003 (2014)]. All coupling constants of the latter theory are expressed microscopically in terms of the liquid static structure factor and are computed for the paradigmatic hard-sphere system. The analysis demonstrates that fluctuations alone restore ergodicity and replace the putative mean-field transition by a smooth crossover. Our results establish a predictive framework for structural relaxation beyond mean-field.

</details>


### [162] [Scaling laws for stationary Navier-Stokes-Fourier flows and the unreasonable effectiveness of hydrodynamics at the molecular level](https://arxiv.org/abs/2512.13205)
*P. I. Hurtado,J. J. del Pozo,P. L. Garrido*

Main category: cond-mat.stat-mech

TL;DR: 该论文基于流体动力学普适性，发现了远离平衡态的压缩性Navier-Stokes-Fourier方程稳态单轴解的一般标度律，并通过分子动力学模拟验证了这些标度律的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 流体动力学为各种多体系统的集体动力学提供了基于对称性和守恒定律的普适描述。本研究旨在利用Navier-Stokes-Fourier方程的普适性，发现远离平衡态的压缩性流体稳态单轴解的一般标度律。

Method: 基于Navier-Stokes-Fourier方程，推导出稳态密度和温度场作为压力和动能场的函数关系，其中动能场量化了速度相对于热通量与剪切应力比值的二次超额。开发了测量相关主曲线的标度方法，并通过大规模分子动力学模拟验证预测。

Result: 研究发现稳态密度和温度场遵循由压力和应力控制的空间标度律，这些标度律在分子动力学模拟中表现出显著的数据塌缩。即使在存在显著有限尺寸效应的情况下，标度律仍保持鲁棒性，揭示了NSF方程在描述分子尺度稳态流动中的惊人准确性。

Conclusion: 这些标度律为驱动流体的稳态提供了新的表征方法，展示了流体动力学方程在远离平衡态条件下的普适性和预测能力。

Abstract: Hydrodynamics provides a universal description of the emergent collective dynamics of vastly different many-body systems, based solely on their symmetries and conservation laws. Here we harness this universality, encoded in the Navier-Stokes-Fourier (NSF) equations, to find general scaling laws for the stationary uniaxial solutions of the compressible NSF problem far from equilibrium. We show for general transport coefficients that the steady density and temperature fields are functions of the pressure and a kinetic field that quantifies the quadratic excess velocity relative to the ratio of heat flux and shear stress. This kinetic field obeys in turn a spatial scaling law controlled by pressure and stress, which is inherited by the stationary density and temperature fields. We develop a scaling approach to measure the associated master curves, and confirm our predictions through compelling data collapses in large-scale molecular dynamics simulations of paradigmatic model fluids. Interestingly, the robustness of the scaling laws in the face of significant finite-size effects reveals the surprising accuracy of NSF equations in describing molecular-scale stationary flows. Overall, these scaling laws provide a novel characterization of stationary states in driven fluids.

</details>


### [163] [Zipfian universality of interaction laws: A statistical-mechanics framework for inverse power scaling](https://arxiv.org/abs/2512.13241)
*Jerome Baray*

Main category: cond-mat.stat-mech

TL;DR: 论文提出了一个统计力学框架，解释逆幂律相互作用（如平方反比定律）作为强异质性微观贡献聚合过程的宏观固定点，而非特定微观机制的结果。


<details>
  <summary>Details</summary>
Motivation: 逆幂律相互作用形式（如平方反比定律）在物理、社会和空间系统中广泛出现。传统上这些定律源自特定微观机制，但其普遍性暗示可能存在更一般的组织原则。本文旨在为这种跨学科现象提供统一的统计解释。

Method: 建立统计力学框架，考虑具有重尾异质性（符合Zipf-Pareto统计）的个体相互作用源，在无内在长度尺度的情况下进行聚合。基于异质性、可乘性、尺度不变性和粗粒化稳定性等最小假设，推导宏观相互作用场必须采用尺度无关的幂律形式。

Result: 研究表明，宏观相互作用场必须采用幂律形式，相关指数不是先验强加的，而是从有效维度、对称性和聚合结构中涌现出来的。平方反比定律被解释为在有效三维空间中各向同性聚合的稳定统计固定点，而偏离这一状态则自然源于各向异性、约束几何或非平凡有效维度。

Conclusion: 该框架将逆幂律相互作用重新定义为Zipfian聚合的稳健涌现特征，而非特定物理力的独特结果，从而为跨学科中这些定律的重复出现提供了共同的统计解释。

Abstract: Inverse power-law interaction forms, such as the inverse-square law, recur across a wide range of physical, social, and spatial systems. While traditionally derived from specific microscopic mechanisms, the ubiquity of these laws suggests a more general organizing principle. This article proposes a statistical-mechanics framework in which such interaction laws emerge as macroscopic fixed points of aggregation processes involving strongly heterogeneous microscopic contributions.
  We consider systems where individual interaction sources exhibit heavy-tailed heterogeneity consistent with Zipf-Pareto statistics and where aggregation proceeds without intrinsic length scales. Under minimal assumptions of heterogeneity, multiplicativity, scale invariance, and stability under coarse-graining, we show that the resulting macroscopic interaction field must adopt a scale-free, power-law form. The associated exponent is not imposed a priori but emerges from effective dimensionality, symmetry, and aggregation structure.
  Within this framework, the inverse-square law is interpreted as a stable statistical fixed point corresponding to isotropic aggregation in an effective three-dimensional space, while deviations from this regime naturally arise from anisotropy, constrained geometries, or nontrivial effective dimensions. This perspective provides a unified interpretation of interaction laws observed in physics, spatial economics, and human geography, without invoking domain-specific microscopic mechanisms.
  The proposed framework reframes inverse power-law interactions as robust emergent features of Zipfian aggregation rather than as unique consequences of particular physical forces, thereby offering a common statistical explanation for their cross-disciplinary recurrence.

</details>


### [164] [Eigenstate Typicality as the Dynamical Bridge to the Eigenstate Thermalization Hypothesis: A Derivation from Entropy, Geometry, and Locality](https://arxiv.org/abs/2512.13348)
*Yucheng Wang*

Main category: cond-mat.stat-mech

TL;DR: 该论文提出了一个统一框架，通过分离运动学典型性和动力学输入来阐明本征态热化假说（ETH）的起源，将ETH解释为熵、几何和混沌诱导典型性的结果。


<details>
  <summary>Details</summary>
Motivation: 本征态热化假说（ETH）为理解孤立量子多体系统中的热化提供了强大框架，但其物理基础和最小基本假设仍存在争议。需要澄清ETH的起源并建立更清晰的理论基础。

Method: 提出了一个统一框架，将运动学典型性与动力学输入分离。该框架基于四个要素：最大熵原理、高维希尔伯特空间几何、物理可观测量局域性，以及一个称为本征态典型性原理（ETP）的最小动力学原理。ETP断言在量子混沌系统中，本征态在局部测量方面与窄微正则壳中的典型态统计不可区分。

Result: 在该框架下，对角ETH从测度集中性中产生，而非对角矩阵元素的普遍指数抑制和平滑能量-频率依赖性则源于熵标度和局域动力学关联，无需引入随机矩阵假设。建立了ETH作为熵、几何和混沌诱导典型性的结果。

Conclusion: 该研究建立了ETH作为熵、几何和混沌诱导典型性的结果，阐明了其适用范围，从而深化了对量子热化和从幺正多体动力学中涌现统计力学的理解。

Abstract: The eigenstate thermalization hypothesis (ETH) provides a powerful framework for understanding thermalization in isolated quantum many-body systems, yet its physical foundations and minimal underlying assumptions remain actively debated. In this work, we develop a unified framework that clarifies the origin of ETH by separating kinematic typicality from dynamical input. We show that the characteristic ETH structure of local operator matrix elements follows from four ingredients: the maximum entropy principle, the geometry of high-dimensional Hilbert space, the locality of physical observables, and a minimal dynamical principle, which we term the eigenstate typicality principle (ETP). ETP asserts that in quantum-chaotic systems, energy eigenstates are statistically indistinguishable from typical states within a narrow microcanonical shell with respect to local measurements. Within this framework, diagonal ETH emerges from measure concentration, while the universal exponential suppression and smooth energy-frequency dependence of off-diagonal matrix elements arise from entropic scaling and local dynamical correlations, without invoking random-matrix assumptions. Our results establish ETH as a consequence of entropy, geometry, and chaos-induced typicality, and clarify its scope, thereby deepening our understanding of quantum thermalization and the emergence of statistical mechanics from unitary many-body dynamics.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [165] [Coherently synchronized oscillations in many-body localization](https://arxiv.org/abs/2512.11953)
*Zi-Jian Li,Yi-Ting Tu,Sankar Das Sarma*

Main category: cond-mat.dis-nn

TL;DR: 在镜像对称多体局域化系统中发现了意外的相干同步振荡现象，通过改变自旋相互作用可以实现同步转变，该转变可用有效伊辛模型解释为顺磁-铁磁相变。


<details>
  <summary>Details</summary>
Motivation: 研究多体局域化系统中是否会出现同步振荡现象，探索这种非平衡量子系统中的集体行为。

Method: 提出基于局域运动积分的有效伊辛模型来描述同步现象，通过理论分析和数值模拟相结合的方法进行研究。

Result: 发现了自旋振荡的同步转变现象，该转变可理解为伊辛模型的顺磁-铁磁相变，理论估计的同步频率和转变点与数值结果吻合良好。

Conclusion: 镜像对称多体局域化系统中存在意外的相干同步振荡现象，这种同步转变可以用有效伊辛模型成功解释，为理解非平衡量子系统中的集体行为提供了新视角。

Abstract: We find an unexpected phenomenon of coherently synchronized oscillations in a mirror-symmetric many-body localized system. A synchronization transition of the spin oscillations is found by changing the spin-spin interactions. To understand this phenomenon, an effective Ising model based on local integrals of motion is proposed. We find that the synchronization transition can be understood as a paramagnetic-to-ferromagnetic Ising transition. Based on the Ising model, we theoretically estimate the synchronized frequencies and the synchronization transition points, which agree well with numerical results.

</details>


### [166] [Kardar-Parisi-Zhang and glassy properties in 2D Anderson localization: eigenstates and wave packets](https://arxiv.org/abs/2512.12085)
*Noam Izem,Bertrand Georgeot,Jiangbin Gong,Gabriel Lemarié,Sen Mu*

Main category: cond-mat.dis-nn

TL;DR: 二维安德森局域化中的涨落属于(1+1)维KPZ普适类，揭示了局域态具有玻璃态特征和主导路径结构


<details>
  <summary>Details</summary>
Motivation: 尽管经过数十年研究，无序量子系统中的涨落普遍性质仍不清楚。本文旨在探索二维安德森局域化中涨落的普适性特征，并建立与KPZ普适类的联系

Method: 通过大量数值模拟分析二维安德森局域化系统，研究局域化本征态和时间演化波包的涨落行为。分析对数密度的涨落、内部结构特征以及空间分布形式

Result: 发现对数密度涨落遵循KPZ标度律；局域化本征态具有玻璃态特征，表现出主导路径、钉扎和雪崩行为；波包空间分布符合与KPZ标度一致的拉伸指数形式，同时完全兼容单参数标度假设

Conclusion: 建立了描述二维安德森局域化中涨落和微观结构的统一KPZ框架，揭示了局域态具有玻璃态本质，为理解无序量子系统的普遍结构提供了新视角

Abstract: Despite decades of research, the universal nature of fluctuations in disordered quantum systems remains poorly understood. Here, we present extensive numerical evidence that fluctuations in two-dimensional (2D) Anderson localization belongs to the (1+1)-dimensional Kardar-Parisi-Zhang (KPZ) universality class. In turn, by adopting the KPZ framework, we gain fresh insight into the structure and phenomenology of Anderson localization itself. We analyze both localized eigenstates and time-evolved wave packets, demonstrating that the fluctuation of their logarithmic density follows the KPZ scaling. Moreover, we reveal that the internal structure of these eigenstates exhibits glassy features characteristic of the directed polymer problem, including the emergence of dominant paths together with pinning and avalanche behavior. Localization is not isotropic but organized along preferential branches of weaker confinement, corresponding to these dominant paths. For localized wave packets, we further demonstrate that their spatial profiles obey a stretched-exponential form consistent with the KPZ scaling, while remaining fully compatible with the single-parameter scaling (SPS) hypothesis, a cornerstone of Anderson localization theory. Altogether, our results establish a unified KPZ framework for describing fluctuations and microscopic organization in 2D Anderson localization, revealing the glassy nature of localized states and providing new understanding into the universal structure of disordered quantum systems.

</details>


### [167] [The generalized density of states in a one-dimensional Ising model with ferrromagnetic and antiferromagnetic interactions](https://arxiv.org/abs/2512.12640)
*Boris Kryzhanovsky,Vladislav Egorov*

Main category: cond-mat.dis-nn

TL;DR: 本文推导了一维N自旋链中能量E和磁化强度m的广义态密度D_N(E,m)的表达式，这些表达式可作为参考标准，有助于理解模型特性并分析磁化强度的时间演化。


<details>
  <summary>Details</summary>
Motivation: 虽然通过转移矩阵技术或已知的态密度D(E)表达式已经获得了模型的主要特性，但获得广义态密度D_N(E,m)的表达式有助于更深入理解模型性质，特别是分析磁化强度m随时间τ的变化行为。

Method: 推导了一维N自旋链中能量E和磁化强度m的广义态密度D_N(E,m)的解析表达式，这些表达式可作为参考标准，用于分析模型的统计特性。

Result: 获得了广义态密度D_N(E,m)的表达式，并证明在一维模型中可以在非零温度下观察到自发磁化。然而，自发磁化会随机改变符号，导致在非常长的观测时间内平均磁化强度⟨m(τ)⟩为零。

Conclusion: 广义态密度D_N(E,m)的表达式为理解一维自旋链模型的统计特性提供了重要工具，揭示了自发磁化在非零温度下存在但会随机翻转的现象，解释了长时间平均磁化强度为零的原因。

Abstract: Expressions for the density of states $D(E)$, where $D(E)$ is the number of states of energy $E$, are well known. The present paper offers the expressions for generalized density of states $D_N(E,m)$, where $D_N(E,m)$ is the number of states with energy $E$ and magnetization $m$ in a one-dimensional $N$-spin chain. The expressions obtained here can be considered as reference ones, since all the main characteristics were obtained without them: using the transfer matrix technique or using well-known expressions for the density of states $D(E)=\sum_m{D_N(E,m)}$. Nevertheless, the knowledge of quantity $D_N(E,m)$ helps to understand the model properties and allows the analysis of the temporal behavior of magnetization $m=m(τ)$. In particular, we demonstrate that in a one-dimensional model spontaneous magnetization can be observed at a non-zero temperature. However, the spontaneous magnetization can randomly change its sign, which results in the magnetization averaged over a very long observation period becoming zero $\langle m(τ)\rangle$.

</details>


### [168] [Large Deviation Properties of Minimum Spanning Trees for Random Graphs](https://arxiv.org/abs/2512.13418)
*Mahdi Sarikhani,Alexander K. Hartmann*

Main category: cond-mat.dis-nn

TL;DR: 研究随机图中最小生成树权重大偏差特性，针对完全图和连通ER图两种图集合，使用马尔可夫链采样方法获得极低概率密度下的分布


<details>
  <summary>Details</summary>
Motivation: 研究随机图中最小生成树权重的大偏差特性，特别是在极低概率密度下的分布行为，这对于理解随机图结构中的极端事件统计特性具有重要意义

Method: 使用大偏差马尔可夫链采样方法，能够获得小至10^{-300}概率密度的生成树权重分布P(W)，针对完全图和连通ER图两种随机图集合进行研究

Result: 对于完全图，确认了关于期望值的解析预测；对于两种图集合，大偏差原理都成立；对于连通ER图，在c=1处观察到分布的显著变化，这是原始ER集合的渗流阈值

Conclusion: 随机图中最小生成树权重的大偏差特性可以通过马尔可夫链采样有效研究，连通ER图在渗流阈值处表现出显著的分布变化，这揭示了随机图结构中的相变行为

Abstract: We study the large-deviation properties of minimum spanning trees for two ensembles of random graphs with $N$ nodes. First, we consider complete graphs. Second, we study Erdős-Rényi (ER) random graphs with edge probability $p=c/N$ conditioned to be connected. By using large-deviation Markov chain sampling, we are able to obtain the distribution $P(W)$ of the spanning-tree weight $W$ down to probability densities as small as $10^{-300}$. For the complete graph, we confirm analytical predictions with respect to the expectation value. For both ensembles, the large deviation principle is fulfilled. For the connected ER graphs, we observe a remarkable change of the distributions at the value of $c=1$, which is the percolation threshold for the original ER ensemble.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [169] [Entanglement Evolution of Noisy Quantum Systems: Master Equation-TFD Solutions](https://arxiv.org/abs/2512.11932)
*Urjjarani Patel,KVS Shiv Chaitanya*

Main category: quant-ph

TL;DR: 该论文应用热场动力学将量子光学非线性主方程映射为薛定谔类方程，并使用哈特里-福克近似解析求解两个独立噪声量子系统的主方程，分析纠缠和量子互信息。


<details>
  <summary>Details</summary>
Motivation: 为开放量子系统问题提供更高效的求解方法，特别是针对量子光学中的非线性主方程，并分析噪声量子系统中的纠缠和量子互信息特性。

Method: 1. 应用热场动力学将量子光学非线性主方程映射为薛定谔类方程；2. 使用哈特里-福克近似解析求解两个独立噪声量子系统的主方程；3. 通过协方差矩阵的特征值分析纠缠和量子互信息；4. 研究双模和单模压缩态。

Result: 该方法为开放量子系统问题提供了更高效的求解框架，能够解析求解噪声量子系统的主方程，并通过协方差矩阵特征值有效分析系统的纠缠特性和量子互信息。

Conclusion: 热场动力学结合哈特里-福克近似为量子光学非线性主方程提供了有效的解析求解方法，可用于分析噪声量子系统中的纠缠和量子信息特性，为开放量子系统研究提供了新工具。

Abstract: In this paper, Thermofield Dynamics (TFD) is applied to map a quantum optics nonlinear master equation into a Schrodinger-like equation for any arbitrary initial condition. This formalism provides a more efficient way for solving open quantum system problems. Then we use the Hartree-Fock approximation to solve the master equations of two separate noisy quantum systems analytically, which allows us to analyze the entanglement and quantum mutual information in each case using the eigenvalues of a covariance matrix, followed by two-mode and single-mode squeezed states.

</details>


### [170] [Quantum circuits for permutation matrices](https://arxiv.org/abs/2512.11938)
*Jason Hanson*

Main category: quant-ph

TL;DR: 提出两种生成置换矩阵量子电路的算法，均使用n个量子比特和多控Toffoli门。第一种算法通过任意对换分解构建电路，需要一个辅助量子线；第二种算法通过汉明距离为1的对换分解构建电路，无需辅助量子线。


<details>
  <summary>Details</summary>
Motivation: 研究如何在量子计算机上高效实现置换操作，特别是针对2^n个元素的置换矩阵。目标是在不使用过多量子资源的情况下，构建能够实现任意置换的量子电路。

Method: 提出两种算法：1）基于任意对换分解的算法，使用一个辅助量子线；2）基于汉明距离为1的对换分解的算法，无需辅助量子线。证明了任何置换都允许第二种分解方式，并提供了减少对换数量的策略。

Result: 成功开发了两种实现置换矩阵的量子电路构造方法。第一种方法通用但需要额外资源；第二种方法更高效，无需辅助量子线，且通过优化策略可以减少所需的门操作数量。

Conclusion: 为量子计算中的置换操作提供了实用的电路实现方案。特别是第二种算法，通过汉明距离为1的对换分解，在无需辅助量子线的情况下实现了任意置换，为量子算法设计提供了有价值的工具。

Abstract: Two different algorithms are presented for generating a quantum circuit realization of a matrix representing a permutation on $2^n$ letters. All circuits involve $n$ qubits and only use multi--controlled Toffoli gates. The first algorithm constructs a circuit from any decomposition of the permutation into a product of transpositions, but uses one ancilla line. The second, which uses no ancillae, constructs a circuit from a decomposition into a product of transpositions that have a Hamming distance of one. We show that any permutation admits such a decomposition, and we give a strategy for reducing the number of transpositions involved.

</details>


### [171] [Uniform matrix product states with a boundary](https://arxiv.org/abs/2512.11968)
*Marta Florido-Llinàs,Álvaro M. Alhambra,David Pérez-García,J. Ignacio Cirac*

Main category: quant-ph

TL;DR: 该论文为具有边界矩阵的均匀矩阵乘积态（MPS）引入了广义规范形式，将分析MPS框架扩展到更广泛的物理相关状态，揭示了此类MPS可以表示为作用在代数正则语言状态上的块可逆矩阵乘积算子。


<details>
  <summary>Details</summary>
Motivation: 现有规范形式理论仅适用于周期性边界条件的均匀MPS，许多物理相关状态无法被涵盖。需要将分析MPS框架扩展到更一般的边界条件设置，以处理更广泛的物理相关状态。

Method: 引入具有边界矩阵的均匀MPS的广义规范形式，基于新的代数结果，包括非半单矩阵集合生成的张成空间和代数的特征描述，以及给出阻塞长度显式上界的广义量子Wielandt不等式。

Result: 建立了具有边界的均匀MPS的统一理论基础，揭示了任何此类MPS都可以表示为作用在捕获其长程和尺度不变特征的代数正则语言状态上的块可逆矩阵乘积算子。

Conclusion: 该工作弥合了周期性和任意边界设置之间的差距，为将矩阵乘积态的关键分析和分类结果扩展到更广泛的状态和算子类别提供了基础。

Abstract: Canonical forms are central to the analytical understanding of tensor network states, underpinning key results such as the complete classification of one-dimensional symmetry-protected topological phases within the matrix product state (MPS) framework. Yet, the established theory applies only to uniform MPS with periodic boundary conditions, leaving many physically relevant states beyond its reach. Here we introduce a generalized canonical form for uniform MPS with a boundary matrix, thus extending the analytical MPS framework to a more general setting of wider physical significance. This canonical form reveals that any such MPS can be represented as a block-invertible matrix product operator acting on a structured class of algebraic regular language states that capture its essential long-range and scale-invariant features. Our construction builds on new algebraic results of independent interest that characterize the span and algebra generated by non-semisimple sets of matrices, including a generalized quantum Wielandt's inequality that gives an explicit upper bound on the blocking length at which the fixed-length span stabilizes to an algebra. Together, these results establish a unified theoretical foundation for uniform MPS with boundaries, bridging the gap between periodic and arbitrary-boundary settings, and providing the basis for extending key analytical and classification results of matrix product states to a much broader class of states and operators.

</details>


### [172] [Convergence of the Cumulant Expansion and Polynomial-Time Algorithm for Weakly Interacting Fermions](https://arxiv.org/abs/2512.12010)
*Hongrui Chen,Cambyse Rouzé,Jielun Chen,Jiaqing Jiang,Samuel O. Scalet,Yongtao Zhan,Garnet Kin-Lic Chan,Lexing Ying,Yu Tong*

Main category: quant-ph

TL;DR: 提出一种随机算法，能在系统规模和精度上均具有多项式时间复杂度地计算弱相互作用费米子的对数配分函数。


<details>
  <summary>Details</summary>
Motivation: 尽管弱相互作用费米子系统通常被认为是可计算的（如图形量子蒙特卡洛方法），但缺乏数学上严格的多项式时间复杂度证明。

Method: 首先将先前工作中用于证明周期系统中累积展开收敛的技术扩展到非周期情况；通过分析连接费曼图求和的关键方程（称为树行列式展开），揭示求和中的树结构；设计基于重要性采样和置信传播增强的新随机算法。

Result: 获得了具有可证明多项式时间复杂度的算法，与传统基于马尔可夫链蒙特卡洛的方法不同，其效率有理论保证。

Conclusion: 该工作为弱相互作用费米子的对数配分函数计算提供了首个具有严格多项式时间复杂度保证的随机算法。

Abstract: We propose a randomized algorithm to compute the log-partition function of weakly interacting fermions with polynomial runtime in both the system size and precision. Although weakly interacting fermionic systems are considered tractable for many computational methods such as the diagrammatic quantum Monte Carlo, a mathematically rigorous proof of polynomial runtime has been lacking. In this work we first extend the proof techniques developed in previous works for proving the convergence of the cumulant expansion in periodic systems to the non-periodic case. A key equation used to analyze the sum of connected Feynman diagrams, which we call the tree-determinant expansion, reveals an underlying tree structure in the summation. This enables us to design a new randomized algorithm to compute the log-partition function through importance sampling augmented by belief propagation. This approach differs from the traditional method based on Markov chain Monte Carlo, whose efficiency is hard to guarantee, and enables us to obtain a algorithm with provable polynomial runtime.

</details>


### [173] [Cavity Mediated Two-Qubit Gate: Tuning to Optimal Performance with NISQ Era Quantum Simulations](https://arxiv.org/abs/2512.12030)
*Shreekanth S. Yuvarajan,Vincent Iglesias-Cardinale,David Hucul,Herbert F. Fotso*

Main category: quant-ph

TL;DR: 该研究利用量子处理器模拟腔介导的两量子比特门，开发了与NISQ兼容的量子算法来追踪系统动力学，识别了远失谐量子比特间的高效操作参数区域。


<details>
  <summary>Details</summary>
Motivation: 光子介导的操作对可扩展量子信息处理平台至关重要，但传统分析方法受近似技术限制，计算模拟面临可扩展性问题。量子处理器为解决这些挑战提供了新途径。

Method: 开发了与NISQ兼容的量子算法，通过量子电路模拟腔介导的两量子比特门动力学。算法能够映射量子态转移保真度随系统参数（包括量子比特与腔的失谐、腔阻尼因子、耦合强度等）的变化。

Result: 算法在有效区域内与解析解或经典模拟算法结果一致，并识别出未充分探索的最优性能区域：在远失谐量子比特之间（既不相互共振也不与腔共振）仍能实现相当有效的两量子比特门操作。

Conclusion: 该方法为模拟和优化光子介导的两量子比特门及其他量子信息处理操作提供了稳健直观的解决方案，可应用于传统方法难以处理的模型变体，对异构量子平台具有重要意义。

Abstract: A variety of photon-mediated operations are critical to the realization of scalable quantum information processing platforms and their accurate characterization is essential for the identification of optimal regimes and their experimental realizations. Such light-matter interactions are often studied with a broad variety of analytical and computational methods that are constrained by approximation techniques or by computational scaling. Quantum processors present a new avenue to address these challenges. We consider the case of cavity mediated two-qubit gates. To investigate quantum state transfer between the qubits, we implement simulations with quantum circuits that are able to reliably track the dynamics of the system. Our quantum algorithm, compatible with NISQ (Noisy Intermediate Scale Quantum) era systems, allows us to map out the fidelity of the state transfer operation between qubits as a function of a broad range of system parameters including the respective detunings between the qubits and the cavity, the damping factor of the cavity, and the respective couplings between the qubits and the cavity. The algorithm provides a robust and intuitive solution, alongside a satisfactory agreement with analytical solutions or classical simulation algorithms in their respective regimes of validity. It allows us to identify under-explored regimes of optimal performance, relevant for heterogeneous quantum platforms, where the two-qubit gate can be rather effective between far-detuned qubits that are neither resonant with each other nor with the cavity. Besides its present application, the method introduced in the current paper can be efficiently used in otherwise untractable variations of the model and in various efforts to simulate and optimize photon-mediated two-qubit gates and other relevant operations in quantum information processing.

</details>


### [174] [Neural quantum states for entanglement depth certification from randomized Pauli measurements](https://arxiv.org/abs/2512.13121)
*Marcin Płodzień*

Main category: quant-ph

TL;DR: 提出基于神经量子状态的似然模型选择方法，直接从测量统计中认证纠缠深度和非k可分性，无需全层析或定制见证


<details>
  <summary>Details</summary>
Motivation: 传统纠缠深度认证方法（定制见证或全层析）随系统规模扩展性差，需要更高效、可扩展的认证方法

Method: 构建层次化的可分神经量子状态模型，在有限测量结果的局部泡利基上训练，与无约束参考模型比较，通过统计模型选择直接认证纠缠

Result: 在模拟的6和10量子比特数据集（GHZ、Dicke、Bell对态）上验证方法有效性，证明对局部噪声下混合态的鲁棒性

Conclusion: 该方法直接从测量统计中认证纠缠深度，无需重构密度矩阵，并提供轻量级可解释性诊断，揭示纠缠模式和量子比特分组

Abstract: Entanglement depth quantifies how many qubits share genuine multipartite entanglement, but certification typically relies on tailored witnesses or full tomography, both of which scale poorly with system size. We recast entanglement-depth and non-$k$-separability certification as likelihood-based model selection among neural quantum states whose architecture enforces a chosen entanglement constraint. A hierarchy of separable neural quantum states is trained on finite-shot local Pauli outcomes and compared against an unconstrained reference model trained on the same data. When all constrained models are statistically disfavored, the data certify entanglement beyond the imposed limit directly from measurement statistics, without reconstructing the density matrix. We validate the method on simulated six- and ten-qubit datasets targeting GHZ, Dicke, and Bell-pair states, and demonstrate robustness for mixed states under local noise. Finally, we discuss lightweight interpretability diagnostics derived from trained parameters that expose coarse entanglement patterns and qubit groupings directly from bitstring statistics.

</details>


### [175] [Quantum-enhanced biosensing enables earlier detection of bacterial growth](https://arxiv.org/abs/2512.12057)
*Rayssa B. de Andrade,Anne Egholm Høgh,Gaetana Spedalieri,Stefano Pirandola,Kirstine Berg-Sørensen,Tobias Gehring,Ulrik L. Andersen*

Main category: quant-ph

TL;DR: 该研究首次实验演示了使用压缩光进行量子增强光度测量，用于早期细菌检测，比经典传感器提前30分钟检测到细菌生长


<details>
  <summary>Details</summary>
Motivation: 在临床、食品安全和环境监测中，快速检测细菌生长至关重要，但传统光学方法受噪声限制且需要数小时培养时间

Method: 使用压缩光作为量子探针监测大肠杆菌培养物的光吸收度，通过截断高斯分布的统计建模和假设检验验证噪声降低

Result: 实现了超越散粒噪声极限的灵敏度，比经典传感器提前30分钟检测到生长起始，且误报率低

Conclusion: 这项工作展示了量子资源如何改进实时、非侵入性诊断，为量子增强生物传感器铺平道路，可加速微生物生长和其他生物过程的检测而不增加光损伤

Abstract: Rapid detection of bacterial growth is crucial in clinical, food safety, and environmental contexts, yet conventional optical methods are limited by noise and require hours of incubation. Here, we present the first experimental demonstration of a quantum-enhanced photometric measurement for early bacterial detection using squeezed light. By monitoring the optical absorbance of an Escherichia coli culture with a quantum probe, we achieve a sensitivity beyond the shot-noise limit, enabling identification of growth onset up to 30 minutes earlier than with a classical sensor. The noise reduction is validated through statistical modeling with a truncated Gaussian distribution and hypothesis testing, confirming earlier detection with low false-alarm rates. This work illustrates how quantum resources can improve real-time, non-invasive diagnostics. Our results pave the way for quantum-enhanced biosensors that accelerate detection of microbial growth and other biological processes without increasing photodamage.

</details>


### [176] [TreeVQA: A Tree-Structured Execution Framework for Shot Reduction in Variational Quantum Algorithms](https://arxiv.org/abs/2512.12068)
*Yuewen Hou,Dhanvi Bharadwaj,Gokul Subramanian Ravi*

Main category: quant-ph

TL;DR: TreeVQA：基于树状执行的变分量子算法框架，通过利用应用任务间的执行相似性，显著减少量子电路执行次数，平均减少25.9倍，大规模问题可减少100倍以上。


<details>
  <summary>Details</summary>
Motivation: 变分量子算法（VQAs）在近中期量子计算中很有前景，但执行成本巨大。每个任务需要多次迭代和大量电路，实际应用涉及多个任务，随着探索能量景观精度需求的增加，需要海量执行次数，使得实际应用成本过高。

Method: 提出TreeVQA框架，利用应用任务间的执行相似性，采用树状执行策略：从联合执行任务开始，仅当量子执行出现分歧时才逐步分支。该框架作为VQA包装器实现，可与典型VQA应用集成。

Result: 在科学和组合基准测试中，在相同目标精度下，平均减少25.9倍执行次数，大规模问题可减少100倍以上。随着问题规模和精度要求的增加，效益进一步增长。

Conclusion: TreeVQA通过利用VQA任务间的执行相似性，显著降低了量子计算成本，为实际应用提供了可行的解决方案，特别是在大规模和高精度需求场景下效益更为显著。

Abstract: Variational Quantum Algorithms (VQAs) are promising for near- and intermediate-term quantum computing, but their execution cost is substantial. Each task requires many iterations and numerous circuits per iteration, and real-world applications often involve multiple tasks, scaling with the precision needed to explore the application's energy landscape. This demands an enormous number of execution shots, making practical use prohibitively expensive. We observe that VQA costs can be significantly reduced by exploiting execution similarities across an application's tasks. Based on this insight, we propose TreeVQA, a tree-based execution framework that begins by executing tasks jointly and progressively branches only as their quantum executions diverge. Implemented as a VQA wrapper, TreeVQA integrates with typical VQA applications. Evaluations on scientific and combinatorial benchmarks show shot count reductions of $25.9\times$ on average and over $100\times$ for large-scale problems at the same target accuracy. The benefits grow further with increasing problem size and precision requirements.

</details>


### [177] [Proof of Spin-Statistics Theorem in Quantum Mechanics of Identical Particles](https://arxiv.org/abs/2512.12071)
*Takafumi Kita*

Main category: quant-ph

TL;DR: 该论文提出了一种非相对论性的自旋-统计定理证明，通过场算符的坐标空间表示和交换对称性分析，建立了整数自旋与半整数自旋粒子分别满足对易与反对易关系的联系。


<details>
  <summary>Details</summary>
Motivation: 传统自旋-统计定理的证明通常基于相对论性量子场论，作者希望提供一个纯粹非相对论性的证明框架，通过坐标空间中的场算符和交换对称性来建立自旋与统计之间的基本联系。

Method: 1. 在坐标空间中引入满足对易和反对易关系的场算符，作为构建全同粒子交换对称性的工具；2. 分析两个湮灭算符乘积的π旋转本征值问题；3. 结合旋转性质分析，证明整数自旋和半整数自旋粒子的场算符分别满足对易和反对易关系。

Result: 成功证明了自旋-统计定理的非相对论版本：整数自旋粒子（玻色子）的场算符满足对易关系，半整数自旋粒子（费米子）的场算符满足反对易关系，这一结论完全在非相对论框架下建立。

Conclusion: 该工作提供了一个不依赖于相对论性量子场论的自旋-统计定理证明，通过坐标空间中的场算符表示和旋转对称性分析，建立了自旋与统计之间的基本联系，为理解这一量子力学基本原理提供了新的视角。

Abstract: A nonrelativistic proof of the spin-statistics theorem is given in terms of the field operators satisfying commutation and anticommutation relations, which are introduced here in the coordinate space as a means to build the permutation symmetry into the brackets of identical particles. An eigenvalue problem of a $π$-rotation for a product of two annihilation operators is combined with an analysis on its rotational property to prove the connection that the field operators for integral-spin and half-integral-spin particles obey the commutation and anticommutation relations, respectively.

</details>


### [178] [The PPKN Gate: An Optimal 1-Toffoli Input-Preserving Full Adder for Quantum Arithmetic](https://arxiv.org/abs/2512.12073)
*G. Papakonstantinou*

Main category: quant-ph

TL;DR: 提出PPKN门作为新型输入保持可逆全加器，相比标准HNG门量子成本从12降至10，逻辑深度从5降至4，并展示了基于PPKN模块的n位脉动进位加法器架构。


<details>
  <summary>Details</summary>
Motivation: 量子计算需要高效的算术运算，现有HNG门作为输入保持可逆全加器标准，但其量子成本(12)和逻辑深度(5)仍有优化空间。

Method: 提出PPKN门设计，仅使用1个Toffoli门和5个CNOT门实现输入保持功能，并构建基于PPKN模块的n位脉动进位加法器架构。

Result: PPKN门量子成本为10，逻辑深度为4，相比HNG门在复杂度和速度上均有提升，成功构建了模块化的n位脉动进位加法器。

Conclusion: PPKN门在量子成本和逻辑深度方面优于标准HNG门，为可逆电路设计提供了更高效的解决方案，有助于推进量子算术运算的实用化。

Abstract: Efficient arithmetic operations are a prerequisite for practical quantum computing. Optimization efforts focus on two primary metrics: Quantum Cost (QC), determined by the number of non-linear gates, and Logical Depth, which defines the execution speed. Existing literature identifies the HNG gate as the standard for Input-Preserving Reversible Full Adders. HNG gate typically requires a QC of 12 and a logical depth of 5, in the area of classical reversible circuits. This paper proposes the PPKN Gate, a novel design that achieves the same inputpreserving functionality using only one Toffoli gate and five CNOT gates. With a Quantum Cost of 10 and a reduced logical depth of 4, the PPKN gate outperforms the standard HNG gate in both complexity and speed. Furthermore, we present a modular architecture for constructing an n-bit Ripple Carry Adder by cascading PPKN modules.

</details>


### [179] [Leveraging Symmetry Merging in Pauli Propagation](https://arxiv.org/abs/2512.12094)
*Yanting Teng,Su Yeon Chang,Manuel S. Rudolph,Zoë Holmes*

Main category: quant-ph

TL;DR: 提出基于对称性合并的Pauli传播算法，通过合并对称群相关的Pauli字符串来减少计算复杂度，提高量子动力学模拟的稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: 量子电路存在对称性时，许多Pauli字符串在对称群作用下冗余演化，可以利用这种对称性来优化量子动力学模拟。

Method: 开发对称性合并Pauli传播算法，通过合并对称变换相关的Pauli字符串，仅传播轨道代表的最小集合，利用群论框架处理平移和置换对称性。

Result: 理论分析显示对称性合并将空间复杂度降低为轨道大小的倒数，全连接Heisenberg动力学数值基准测试证实了在截断和噪声下稳定性的提升。

Conclusion: 建立了一个增强Pauli传播的群论框架，通过对称性合并显著提高经典量子动力学模拟的效率和稳定性，开源代码验证了其实际应用价值。

Abstract: We introduce a symmetry-adapted framework for simulating quantum dynamics based on Pauli propagation. When a quantum circuit possesses a symmetry, many Pauli strings evolve redundantly under actions of the symmetry group. We exploit this by merging Pauli strings related through symmetry transformations. This procedure, formalized as the symmetry-merging Pauli propagation algorithm, propagates only a minimal set of orbit representatives. Analytically, we show that symmetry merging reduces space complexity by a factor set by orbit sizes, with explicit gains for translation and permutation symmetries. Numerical benchmarks of all-to-all Heisenberg dynamics confirm improved stability, particularly under truncation and noise. Our results establish a group-theoretic framework for enhancing Pauli propagation, supported by open-source code demonstrating its practical relevance for classical quantum-dynamics simulations.

</details>


### [180] [Symmetry Dilemmas in Quantum Computing for Chemistry: A Comprehensive Analysis](https://arxiv.org/abs/2512.12097)
*Ilias Magoulas,Muhan Zhang,Francesco A. Evangelista*

Main category: quant-ph

TL;DR: 该论文分析了量子算法中对称性适应、普适性和门效率之间的权衡困境，证明了流行的门高效单重态自旋适应单激发和完美配对双激发算子池在空间对称性约束下不具备普适性，并通过数值模拟展示了不同类型算子池在不同物理场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 量子算法在电子结构和多体物理应用中面临对称性适应、普适性和门效率之间的基本矛盾：完全对称性适应的普适算子池通常产生长而深的量子电路，门高效的普适算子池通常破坏对称性，而门高效的完全对称性适应算子池可能不具备普适性。本文旨在分析这种对称性困境。

Method: 1. 理论分析：证明流行的门高效算子池（单重态自旋适应单激发和完美配对双激发）在空间对称性约束下不具备普适性。2. 数值模拟：使用自适应算法配合三种类型的算子池进行模拟：(i)完全对称性适应且普适，(ii)完全对称性适应但不普适，(iii)破坏单一对称性但普适。模拟涵盖三种物理相关场景。

Result: 1. 理论证明：流行的门高效算子池在空间对称性约束下确实不具备普适性。2. 数值结果表明：在目标态为全局基态时，所有三种算子池都能成功；当目标态与具有多个不同对称性质的态交叉时，需要至少保持一个区分对称性；当目标态与仅在一个对称性质上不同的态交叉时，必须严格保持该特定对称性以避免变分崩溃。

Conclusion: 该研究为设计平衡普适性、资源需求和鲁棒态靶向的对称性适应算子池提供了实用指南，明确了何时可以安全使用破坏对称性但普适的算子池，何时保持至少一个区分对称性就足够，以及何时必须严格保持特定对称性以避免变分崩溃。

Abstract: Symmetry adaptation, universality, and gate efficiency are central but often competing requirements in quantum algorithms for electronic structure and many-body physics. For example, fully symmetry-adapted universal operator pools typically generate long and deep quantum circuits, gate-efficient universal operator pools generally break symmetries, and gate-efficient fully symmetry-adapted operator pools may not be universal. In this work, we analyze such symmetry dilemmas both theoretically and numerically. On the theory side, we prove that the popular, gate-efficient operator pool consisting of singlet spin-adapted singles and perfect-pairing doubles is not universal when spatial symmetry is enforced. To demonstrate the strengths and weaknesses of the three types of pools, we perform numerical simulations using an adaptive algorithm paired with operator pools that are (i) fully symmetry-adapted and universal, (ii) fully symmetry-adapted and non-universal, and (iii) breaking a single symmetry and are universal. Our numerical simulations encompass three physically relevant scenarios in which the target state is (i) the global ground state, (ii) the ground state crossed by a state differing in multiple symmetry properties, and (iii) the ground state crossed by a state differing in a single symmetry property. Our results show when symmetry-breaking but universal pools can be used safely, when enforcing at least one distinguishing symmetry suffices, and when a particular symmetry must be rigorously preserved to avoid variational collapse. Together, the formal and numerical analysis provides a practical guide for designing and benchmarking symmetry-adapted operator pools that balance universality, resource requirements, and robust state targeting in quantum simulations for chemistry.

</details>


### [181] [Measurement as Sheafification: Context, Logic, and Truth after Quantum Mechanics](https://arxiv.org/abs/2512.12249)
*Partha Ghose*

Main category: quant-ph

TL;DR: 该论文提出量子测量本质上是逻辑冲突而非动力学冲突，通过层论和上同调理论重构量子理论，将测量重新解释为预层真理的层化过程，而非幺正性的物理破坏。


<details>
  <summary>Details</summary>
Motivation: 传统量子测量理论将线性薛定谔演化与坍缩规则之间的张力视为动力学冲突，作者认为更深层的冲突是逻辑性的：量子理论本质上是语境相关的，而经典传统预设了单一的全局布尔赋值。

Method: 基于玻尔的互补性、EPR论证和贝尔定理，将定域性和完备性重新表述为测量语境范畴上赋值预层的全局截面存在性。使用Čech上同调测量语境独立性描述的障碍，在预层拓扑斯中展示直觉主义逻辑，将Ghose和Patra的七值语境逻辑表现为有限Heyting代数。通过受随机力学启发的σ-λ动力学提供强语境和近似经典区域之间的连续插值。

Result: 量子语境性表现为预层全局截面的缺失，Čech上同调量化了这种障碍。测量被重新解释为预层值真理的层化过程，而非物理上的幺正性破坏。σ-λ动力学能够连续插值强语境和经典区域，消解了传统测量悖论和表观非定域性。

Conclusion: 量子测量问题源于对全局真理的非法要求，而非物理过程的中断。通过层论框架，测量悖论和表观非定域性被消解为语境依赖性的自然结果，为量子理论提供了更统一的逻辑基础。

Abstract: Quantum measurement is commonly posed as a dynamical tension between linear Schrödinger evolution and an ad hoc collapse rule. I argue that the deeper conflict is logical: quantum theory is inherently contextual, whereas the classical tradition presupposes a single global, Boolean valuation. Building on Bohr's complementarity, the Einstein--Podolsky--Rosen argument and Bell's theorem, I recast locality and completeness as the existence of a global section of a presheaf of value assignments over the category of measurement contexts. The absence of global sections expresses the impossibility of context-independent description, and Čech cohomology measures the resulting obstruction. The internal logic of the presheaf topos is intuitionistic, and the seven-valued contextual logic proposed by Ghose and Patra is exhibited as a finite Heyting algebra capturing patterns of truth, falsity and indeterminacy across incompatible contexts. Classical physics corresponds to the sheaf case, where compatible local data glue and Boolean logic is effectively restored. Measurement is therefore reinterpreted as sheafification of presheaf-valued truth rather than as a physical breakdown of unitarity. Finally, a $σ$--$λ$ dynamics motivated by stochastic mechanics provides a continuous interpolation between strongly contextual and approximately classical regimes, dissolving the usual measurement paradoxes and apparent nonlocality as artefacts of an illegitimate demand for global truth.

</details>


### [182] [Coherence Dispersion and Temperature Scales in a Quantum-Biology Toy Model](https://arxiv.org/abs/2512.12342)
*Fernando Parisio*

Main category: quant-ph

TL;DR: 论文研究了量子相干性在量子态非对角元间的分散（相干性分散Δ_c），发现其最大值出现在适当熵的中间值处，这是复杂系统量化器的普遍特征。通过非平衡系统框架应用于细胞能量学简化模型，发现ATP-ADP转换能量30.5 kJ/mol使Δ_c最大化的温度范围与单细胞生命存在的温度范围一致。


<details>
  <summary>Details</summary>
Motivation: 研究量子相干性在量子态中的分散特性，探索相干性分散与系统复杂性的关系，并将其应用于生物系统特别是细胞能量学过程，寻找量子效应与生命现象之间的潜在联系。

Method: 定义了相干性分散（Δ_c）作为量子相干性在量子态非对角元间分散的度量，建立了计算框架。将该框架应用于非平衡系统，构建了涉及剩余相干性的细胞能量学简化模型，分析Δ_c与温度的关系。

Result: 相干性分散Δ_c在适当熵的中间值处达到最大，这是复杂系统量化器的普遍特征。在细胞能量学模型中，ATP-ADP转换能量30.5 kJ/mol使Δ_c最大化的温度范围与单细胞生命存在的温度范围一致，且仅需低水平的相干性即可支持这一结论。

Conclusion: 量子相干性分散为理解复杂系统提供了新的量化工具，其与细胞能量学过程的联系表明量子效应可能在生命现象中扮演重要角色，特别是ATP-ADP转换的能量特性与生命适宜温度范围之间存在有趣的对应关系。

Abstract: In this work, we investigate how quantum coherence can scatter among the several off-diagonal elements of an arbitrary quantum state, defining coherence dispersion ($Δ_{\rm c}$). It turns out that this easily computable quantity is maximized for intermediate values of an appropriate entropy, a prevalent signature of complexity quantifiers across different fields, from linguistics and information science to evolutionary biology. By focusing on out-of-equilibrium systems, we use the developed framework to address a simplified model of cellular energetics, involving remanent coherence. Within the context of this model, the precise energy of 30.5 kJ/mol (the yield of ATP-ADP conversion) causes the temperature range where $Δ_{\rm c}$ is maximized to be compatible with temperatures for which unicellular life is reported to exist. Low levels of coherence suffice to support this conclusion.

</details>


### [183] [Imaging Walk-Off Driven Distortions in EPR Photon Pair Correlations](https://arxiv.org/abs/2512.12423)
*Christian Howard,Roohollah Ghobadi,Nazanin Dehghan,Alessio D'Errico,Ebrahim Karimi*

Main category: quant-ph

TL;DR: 该研究发现，即使在名义上薄的晶体中，双折射引起的横向走位效应也会导致光子对空间关联中的和-差坐标耦合，这种耦合在晶体像平面附近产生独特的锥形关联模式，这是传统因子化模型无法捕捉的。


<details>
  <summary>Details</summary>
Motivation: 传统上分析自发参量下转换产生的EPR态光子对时，通常采用薄晶体近似，假设双光子波函数可以分解为和坐标与差坐标的独立函数。但在实际中，双折射引起的横向走位效应会破坏这种分解，耦合这些自由度。研究者希望探究这种耦合在名义上薄晶体中是否仍然存在，以及自由空间传播如何影响这种耦合。

Method: 研究者考虑了自由空间传播对联合空间强度的影响，分析了双折射引起的横向走位效应如何导致和-差坐标耦合。通过数值模拟和实验数据验证了这种耦合效应，特别关注了晶体像平面附近横向关联的锥形特征。

Result: 研究发现，即使在名义上薄的晶体中，一旦考虑联合空间强度的自由空间传播，和-差坐标的耦合仍然存在。这种耦合导致在晶体像平面附近出现独特的横向关联锥形特征，数值模拟和实验数据都明确证实了这一新行为。

Conclusion: 该研究提供了对双折射非线性介质中光子对生成的更完整描述，阐明了基于EPR态的空间分辨量子成像和空间模式量子信息处理的基本限制。传统因子化模型无法捕捉这种和-差坐标耦合效应，这对于精确描述光子对的空间关联至关重要。

Abstract: Spontaneous parametric down-conversion is the primary source of position-correlated and momentum-anticorrelated photon pairs that form the canonical Einstein-Podolsky-Rosen (EPR) state. Their transverse spatial correlations are usually analyzed within the thin-crystal approximation, where the two-photon wavefunction is assumed to factorize into independent functions of the sum and difference coordinates. In practice, however, birefringence-induced transverse walk-off breaks this factorization and couples these degrees of freedom. Here, we show that this coupling persists even for nominally thin crystals once the free-space propagation of the joint spatial intensity is taken into account. This sum-difference coordinate coupling leads to a distinctive tapering of the transverse correlations near the crystal image plane-an effect that standard factorized models cannot capture. Numerical simulations and experimental data clearly confirm this novel behavior. Our findings provide a more complete description of photon-pair generation in birefringent nonlinear media and clarify fundamental limits on spatially resolved quantum imaging and spatial-mode quantum information processing with EPR states.

</details>


### [184] [Classical Second-Order Moments and Tensor Squeezing in Spin-1 Systems](https://arxiv.org/abs/2512.12504)
*K. S. Mallesh*

Main category: quant-ph

TL;DR: 该论文给出了单自旋-1粒子经典二阶矩的紧凑、框架无关的表征，通过定义矩矩阵M=2Q+(1/3)I，证明了正混合自旋相干态产生的矩对(s,Q)当且仅当M半正定、M-ss^T半正定且迹为1。


<details>
  <summary>Details</summary>
Motivation: 研究自旋-1粒子的经典二阶矩集合的数学表征，为量子非经典性提供基础理论框架和检测工具。

Method: 定义矩矩阵M=2Q+(1/3)I，建立矩阵条件：M半正定、M-ss^T半正定且迹(M)=1，这些条件是正混合自旋相干态产生矩对(s,Q)的充要条件。

Result: 获得了经典矩区域的精确数学描述，推导出高阶张量非经典性的简单、基无关的见证，如Tr(Q^2)的边界，附录中给出了充分性的构造性证明。

Conclusion: 该工作提供了自旋-1粒子经典二阶矩的完整表征，为检测量子非经典性提供了理论基础和实用工具，矩阵条件简洁且框架无关。

Abstract: We give a compact, frame-independent characterization of the set of classical second-order moments for a single spin-1 particle. Defining the moment matrix M = 2Q + (1/3) I, we show that a moment pair (s, Q) arises from a positive mixture of spin-coherent states if and only if M is positive semidefinite, M minus ss^T is positive semidefinite, and the trace of M equals one. These necessary and sufficient matrix conditions delimit the classical moment region and yield simple, basis-free witnesses of higher-order tensor nonclassicality, such as bounds on Tr(Q^2). A constructive proof of sufficiency is given in the appendix.

</details>


### [185] [Quantum critical dynamics and emergent universality in decoherent digital quantum processors](https://arxiv.org/abs/2512.13143)
*Brendan Rhyno,Swarnadeep Majumder,Smitha Vishveshwara,Khadijeh Najafi*

Main category: quant-ph

TL;DR: 该研究探讨了噪声对非平衡量子临界动力学的影响，通过理论分析、数值模拟和IBM量子处理器实验，发现噪声可以重塑而非完全抑制量子Kibble-Zurek机制的普适标度行为，揭示了噪声影响下的新普适性区域。


<details>
  <summary>Details</summary>
Motivation: 理解噪声如何影响非平衡量子临界动力学对于基础物理和量子技术发展都至关重要。虽然量子Kibble-Zurek机制预测了穿越临界点时的普适标度行为，但实际量子系统中的复杂退相干效应会显著改变这些行为，从改变临界标度到完全抑制它。

Method: 1. 首先考虑非破坏性噪声的特定情况，理论分析退相干如何重塑普适标度，并通过自旋链的数值模拟验证理论预测；2. 在IBM超导量子处理器上研究横向场伊辛模型的线性淬灭，使用80-120量子比特的大系统规模，测量等时关联函数、缺陷密度和过剩能量，分析不同淬灭时间下的标度关系。

Result: 1. 数值模拟验证了理论预测的噪声对普适标度的重塑效应；2. 在IBM量子处理器实验中，与早期观测不同，即使在长时间尺度下也观察到清晰的标度关系，表明退相干塑造了持久的普适结构；3. 提取的标度指数既不同于理想QKZ预测，也不同于简化噪声模型的解析结果，表明出现了受噪声影响的独特普适性区域。

Conclusion: 噪声可以重塑而非完全抑制量子临界动力学中的普适标度行为，实验结果指向噪声影响下的新普适性区域。这表明可以利用普适动力学标度作为量子硬件的高层描述符，补充传统的门级性能指标。

Abstract: Understanding how noise influences nonequilibrium quantum critical dynamics is essential for both fundamental physics and the development of practical quantum technologies. While the quantum Kibble-Zurek (QKZ) mechanism predicts universal scaling during quenches across a critical point, real quantum systems exhibit complex decoherence that can substantially modify these behaviors, ranging from altering critical scaling to completely suppressing it. By considering a specific case of nondemolishing noise, we first show how decoherence can reshape universal scaling and verify these theoretical predictions using numerical simulations of spin chains across a wide range of noise strengths. Then, we study linear quenches in the transverse-field Ising model on IBM superconducting processors where the noise model is unknown. Using large system sizes of 80-120 qubits, we measure equal-time connected correlations, defect densities, and excess energies across various quench times. Surprisingly, unlike earlier observations where noise-induced defect production masked universal behavior at long times, we observe clear scaling relations, pointing towards persistent universal structure shaped by decoherence. The extracted scaling exponents differ from both ideal QKZ predictions and analytic results for simplified noise models, suggesting the emergence of a distinct noise-influenced universality regime. Our results, therefore, point toward the possibility of using universal dynamical scaling as a high-level descriptor of quantum hardware, complementary to conventional gate-level performance metrics.

</details>


### [186] [Opportunistic Scheduling for Single-downlink Satellite-based Quantum Key Distribution](https://arxiv.org/abs/2512.12514)
*Md Zakir Hossain,Nitish K. Panigrahy,Walter O. Krawec,Don Towsley,Bing Wang*

Main category: quant-ph

TL;DR: 本文提出了一种针对单下行链路架构卫星量子密钥分发系统的机会主义调度方法，在考虑地面站对之间公平性的同时，利用动态卫星信道最大化系统性能。


<details>
  <summary>Details</summary>
Motivation: 卫星量子密钥分发被认为是实现全球规模QKD最有前景的方向之一。虽然单下行链路架构需要卫星作为可信节点，安全性较弱，但具有更高的密钥速率潜力，并能服务相距较远的地面站对。目前卫星调度研究主要集中在双下行链路架构，单下行链路架构的调度问题尚未得到研究。

Method: 提出了一种新颖的机会主义卫星调度方法，该方法在考虑地面站对之间公平性的同时，利用动态卫星信道条件来最大化系统性能。考虑了季节性效应和云层覆盖对卫星QKD系统评估的重要性。

Result: 该方法在各种设置下进行评估，证明在地面站对的总密钥速率和最小密钥速率方面提供了最佳权衡。评估还突出了考虑季节性效应和云层覆盖在卫星QKD系统评估中的重要性，并展示了全球和区域QKD系统中的不同权衡。

Conclusion: 单下行链路架构卫星QKD系统虽然安全性较弱，但具有显著优势。提出的机会主义调度方法能够有效平衡公平性和性能，为卫星QKD系统的实际部署提供了重要参考。

Abstract: Satellite-based quantum key distribution (QKD), leveraging low photon loss in free-space quantum communication, is widely regarded as one of the most promising directions to achieve global-scale QKD. With a constellation of satellites and a set of ground stations in a satellite-based QKD system, how to schedule satellites to achieve efficient QKD is an important problem. This problem has been studied in the dual-downlink architecture, where each satellite distributes pairs of entanglements to two ground stations simultaneously. However, it has not been studied in the single downlink architecture, where satellites create keys with each individual ground station, and then serve as trusted nodes to create keys between pairs of ground stations. While the single downlink architecture provides weaker security in that the satellites need to be trusted, it has many advantages, including the potential of achieving significantly higher key rates, and generating keys between pairs of ground stations that are far away from each other and cannot be served using the dual-downlink architecture. In this paper, we propose a novel opportunistic approach for satellite scheduling that accounts for fairness among the ground station pairs, while taking advantage of the dynamic satellite channels to maximize the system performance. We evaluate this approach in a wide range of settings and demonstrate that it provides the best tradeoffs in terms of total and minimum key rates across the ground station pairs. Our evaluation also highlights the importance of considering seasonal effects and cloud coverage in evaluating satellite-based QKD systems. In addition, we show different tradeoffs in global and regional QKD systems.

</details>


### [187] [Robustness analysis in static and dynamic quantum state tomography](https://arxiv.org/abs/2512.12518)
*Alan Chen,Shuixin Xiao,Hailan Ma,Daoyi Dong*

Main category: quant-ph

TL;DR: 该论文研究了量子态层析在测量设备和哈密顿量存在扰动时的鲁棒性，推导了静态和动态场景下的误差界限，并通过数值模拟验证了资源缩放规律。


<details>
  <summary>Details</summary>
Motivation: 量子态层析是量子系统识别的核心任务，但实际实验条件常偏离标称设计，导致测量设备和系统动力学哈密顿量都存在误差。需要研究量子态层析对这些扰动的鲁棒性。

Method: 使用线性回归估计方法，在静态和动态两种场景下，推导测量设备和哈密顿量有界误差对均方误差上界的显式界限。

Result: 推导出了量化测量设备和哈密顿量误差如何影响均方误差上界的显式界限，并通过量子比特系统的数值模拟展示了这些界限如何随资源缩放。

Conclusion: 该研究为量子态层析在存在设备误差和动力学误差时的鲁棒性提供了理论分析框架和量化界限，有助于理解实际实验条件下的性能限制。

Abstract: Quantum state tomography is a core task in quantum system identification. Real experimental conditions often deviate from nominal designs, introducing errors in both the measurement devices and the Hamiltonian governing the system's dynamics. In this paper, we investigate the robustness of quantum state tomography against such perturbations in both static and dynamic settings using linear regression estimation. We derive explicit bounds that quantify how bounded errors in the measurement devices and the Hamiltonian affect the mean squared error (MSE) upper bound in each scenario. Numerical simulations for qubit systems illustrate how these bounds scale with resources.

</details>


### [188] [Imaginary-time-enhanced feedback-based quantum algorithms for universal ground-state preparation](https://arxiv.org/abs/2512.13044)
*Thanh Nguyen Van Long,Lan Nguyen Tran,Le Bin Ho*

Main category: quant-ph

TL;DR: 本文针对FALQON算法在谱简并时失效的问题，提出ITE-FALQON混合方法，通过引入虚时间演化步骤解决简并问题，实现可靠基态制备。


<details>
  <summary>Details</summary>
Motivation: FALQON算法在量子模拟中提供了一种无需经典优化的全量子反馈方法，但在存在谱简并时会失效，反馈信号崩溃，无法达到基态。这在Fermi-Hubbard模型的2x2和3x3晶格中都会出现。

Method: 提出ITE-FALQON（虚时间增强FALQON）方案，在反馈循环中插入短虚时间演化步骤。这种混合方法抑制激发态分量，逃离简并子空间，恢复能量单调下降特性。

Result: ITE-FALQON在所有填充情况下都能实现可靠的基态收敛，为强关联量子系统中的可扩展基态制备提供了实用途径。在3x3晶格上，无论是半填充还是掺杂构型都能成功。

Conclusion: ITE-FALQON通过结合虚时间演化解决了FALQON在谱简并时的失效问题，为强关联量子系统的基态制备提供了更稳健的量子算法框架。

Abstract: Preparing ground states of strongly correlated quantum systems is a central goal in quantum simulation and optimization. The feedback-based quantum algorithm (FALQON) provides an attractive alternative to variational methods with a fully quantum feedback rule, but it fails in the presence of spectral degeneracies, where the feedback signal collapses and the evolution cannot reach the ground state. Using the Fermi-Hubbard model on lattices up to 3x3, we show that this breakdown appears at half-filling on the 2x2 lattice and extends to both half-filled and doped configurations on the 3x3 lattice. We then introduce an imaginary-time-enhanced FALQON (ITE-FALQON) scheme, which inserts short imaginary-time evolution steps into the feedback loop. The hybrid method suppresses excited-state components, escapes degenerate subspaces, and restores monotonic energy descent. The ITE-FALQON achieves a reliable ground-state convergence across all fillings, providing a practical route to scalable ground-state preparation in strongly correlated quantum systems.

</details>


### [189] [Quantum Encoding of Three-Dimensional Ligand Poses for Exhaustive Configuration Enumeration](https://arxiv.org/abs/2512.12573)
*Pei-Kun Yang*

Main category: quant-ph

TL;DR: 该研究提出了一种量子原生分子对接方法，通过量子态编码配体在三维网格上的所有空间构型，利用量子并行性解决经典方法中组合爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 经典分子对接方法面临组合爆炸问题，配体的平移和旋转自由度呈组合增长，在经典硬件上无法穷举所有构型，限制了虚拟筛选的效率。

Method: 采用量子原生表述，将配体在离散化三维网格上的占据状态编码到量子态中，通过辅助量子比特控制多步平移和旋转变换，在单个量子态中相干生成所有空间构型的完整集合。

Result: 该框架能够同时激活所有对称相关的构型，为量子加速的虚拟筛选提供了可扩展的基础，并且可以与量子评分方法集成。

Conclusion: 提出的量子原生分子对接框架解决了经典方法的组合爆炸限制，为量子硬件发展后的分子对接和虚拟筛选提供了有前景的解决方案。

Abstract: Classical molecular docking is fundamentally constrained by the combinatorial growth of ligand translational and rotational degrees of freedom, rendering exhaustive pose enumeration infeasible on classical hardware. This work introduces a quantum-native formulation that encodes ligand occupancy on discretized three-dimensional grids and coherently generates the full ensemble of spatial configurations within a single quantum state. Multi-step translations and rotational transformations are controlled by ancillary qubits, enabling all symmetry-related configurations to be activated simultaneously. This framework provides a scalable foundation for quantum-accelerated virtual screening and is amenable to integration with quantum scoring approaches as quantum hardware continues to advance.

</details>


### [190] [Scalable Quantum Error Mitigation with Neighbor-Informed Learning](https://arxiv.org/abs/2512.12578)
*Zhenyu Chen,Bin Cheng,Minbo Gao,Xiaodie Lin,Ruiqi Zhang,Zhaohui Wei,Zhengfeng Ji*

Main category: quant-ph

TL;DR: 本文提出了一种名为邻域信息学习（NIL）的新型量子误差缓解框架，通过机器学习方法从相关"邻域"电路的噪声输出中预测目标量子电路的理想输出，相比现有方法在性能、资源开销和理论保证方面有显著改进。


<details>
  <summary>Details</summary>
Motivation: 量子硬件中的噪声是实现量子计算潜力的主要障碍。现有的量子误差缓解方法在性能、资源开销和理论保证之间存在困难权衡，需要一种更灵活、准确、高效且鲁棒的方法。

Method: 提出了邻域信息学习（NIL）框架，通过机器学习模型从目标量子电路的结构相关"邻域"电路的噪声输出中预测理想输出。关键创新是2-design训练方法，相比传统基于学习的QEM协议（通过用均匀随机Clifford门替换非Clifford门创建训练电路），该方法能生成更有效的训练数据。

Result: 理论分析和数值模拟表明，NIL方法比传统方法具有更高的准确性和效率。证明了训练集所需大小仅与邻域电路总数呈对数关系，使得NIL能够应用于大规模量子电路问题。

Conclusion: NIL建立了一个理论基础扎实且实际高效的量子误差缓解框架，为在噪声硬件上实现量子优势铺平了可行道路，统一并加强了现有的ZNE和PEC等方法。

Abstract: Noise in quantum hardware is the primary obstacle to realizing the transformative potential of quantum computing. Quantum error mitigation (QEM) offers a promising pathway to enhance computational accuracy on near-term devices, yet existing methods face a difficult trade-off between performance, resource overhead, and theoretical guarantees. In this work, we introduce neighbor-informed learning (NIL), a versatile and scalable QEM framework that unifies and strengthens existing methods such as zero-noise extrapolation (ZNE) and probabilistic error cancellation (PEC), while offering improved flexibility, accuracy, efficiency, and robustness.
  NIL learns to predict the ideal output of a target quantum circuit from the noisy outputs of its structurally related ``neighbor'' circuits. A key innovation is our 2-design training method, which generates training data for our machine learning model. In contrast to conventional learning-based QEM protocols that create training circuits by replacing non-Clifford gates with uniformly random Clifford gates, our approach achieves higher accuracy and efficiency, as demonstrated by both theoretical analysis and numerical simulation. Furthermore, we prove that the required size of the training set scales only \emph{logarithmically} with the total number of neighbor circuits, enabling NIL to be applied to problems involving large-scale quantum circuits. Our work establishes a theoretically grounded and practically efficient framework for QEM, paving a viable path toward achieving quantum advantage on noisy hardware.

</details>


### [191] [Quantum Integrability of Hamiltonians with Time-Dependent Interaction Strengths and the Renormalization Group Flow](https://arxiv.org/abs/2512.13625)
*Parameshwar R. Pasnoori*

Main category: quant-ph

TL;DR: 该论文证明了含时量子系统中可积性约束与重整化群流方程之间的对应关系，以含时Kondo模型为例，展示了含时耦合参数的演化轨迹与静态模型RG流轨迹完全一致。


<details>
  <summary>Details</summary>
Motivation: 研究含时相互作用强度的量子哈密顿量，探索可积性与重整化群流之间的普遍联系，为含时量子系统提供新的理论框架。

Method: 采用广义Bethe ansatz框架，以含时各向异性Kondo模型为具体实例，构造含时薛定谔方程的精确解，通过施加费米子场的边界条件得到量子Knizhnik-Zamolodchikov方程，分析可积性对含时耦合参数的约束。

Result: 发现含时耦合参数J∥(t)和J⊥(t)的演化轨迹与静态Kondo模型的重整化群流轨迹完全一致，建立了含时量子系统中可积性与重整化群流之间的直接普遍对应关系。

Conclusion: 含时量子系统的可积性约束与相应静态哈密顿量的重整化群流方程具有相同形式，揭示了可积性与重整化群之间的深刻联系，为含时量子多体系统提供了新的理论见解。

Abstract: In this paper we consider quantum Hamiltonians with time-dependent interaction strengths, and following the recently formulated generalized Bethe ansatz framework [P. R. Pasnoori, Phys. Rev. B 112, L060409 (2025)], we show that constraints imposed by integrability take the same form as the renormalization group flow equations corresponding to the respective Hamiltonians with constant interaction strengths. As a concrete example, we consider the anisotropic time-dependent Kondo model characterized by the time-dependent interaction strengths $J_{\parallel}(t)$ and $J_{\perp}(t)$. We construct an exact solution to the time-dependent Schrodinger equation and by applying appropriate boundary conditions on the fermion fields we obtain a set of matrix difference equations called the quantum Knizhnik-Zamolodchikov (qKZ) equations corresponding to the XXZ R-matrix. The consistency of these equations imposes constraints on the time-dependent interaction strengths $J_{\parallel}(t)$ and $J_{\perp}(t)$, such that the system is integrable. Remarkably, the resulting temporal trajectories of the couplings are shown to coincide exactly with the RG flow trajectories of the static Kondo model, establishing a direct and universal correspondence between integrability and renormalization-group flow in time-dependent quantum systems.

</details>


### [192] [$k$-Entanglement Measure for Multipartite Systems without Convex-Roof Extensions and its Evaluation](https://arxiv.org/abs/2512.12588)
*Jie Guo,Shuyuan Yang,Jinchuan Hou,Xiaofei Qi,Kan He*

Main category: quant-ph

TL;DR: 该论文建立了首个真正的k-纠缠度量框架，避免了凸包构造，提供了高效可计算的通用算法，能够快速认证多体纠缠。


<details>
  <summary>Details</summary>
Motivation: 多体纠缠是量子技术的基础，但目前研究受到缺乏通用度量标准、统一框架以及凸包扩展难以处理的限制。

Method: 建立公理化框架，引入首个真正的k-纠缠度量E_w^{(k,n)}，避免凸包构造，开发通用算法评估任意有限维态，并提供开源软件覆盖四量子比特系统的所有分区。

Result: 数值测试能在200秒内认证k-纠缠，结果与充要条件一致，收紧边界并揭示新的阈值，为多体纠缠量化提供了可扩展的实用工具。

Conclusion: 该框架建立了k-纠缠作为多体量子资源的概念，提供了可扩展、实用的多体纠缠量化工具，避免了传统凸包构造的复杂性。

Abstract: Multipartite entanglement underpins quantum technologies but its study is limited by the lack of universal measures, unified frameworks, and the intractability of convex-roof extensions. We establish an axiomatic framework and introduce the first \emph{true} $k$-entanglement measure, $E_w^{(k,n)}$, which satisfies all axioms, establishes $k$-entanglement as a multipartite quantum resource, avoids convex-roof constructions, and is efficiently computable. A universal algorithm evaluates arbitrary finite-dimensional states, with open-source software covering all partitions of four-qubit systems. Numerical tests certify $k$-entanglement within 200 seconds, consistent with necessary-and-sufficient criteria, tightening bounds and revealing new thresholds. This framework offers a scalable, practical tool for rigorous multipartite entanglement quantification.

</details>


### [193] [Operational Derivation of Born's Rule from Causal Consistency in Generalized Probabilistic Theories](https://arxiv.org/abs/2512.12636)
*Enso O. Torres Alegre*

Main category: quant-ph

TL;DR: 从因果一致性等基本操作要求出发，在广义概率理论中推导出玻恩规则，证明概率分配必须是仿射的，否则会导致超光速信号传递


<details>
  <summary>Details</summary>
Motivation: 在不需要希尔伯特空间结构的情况下，从操作角度推导量子力学中的玻恩规则，探索量子概率定律的因果基础

Method: 采用三阶段论证：操作概率分配、因果一致性约束、结构重建；基于因果一致性、锐测量、可逆对称性和无信号传递等基本要求

Result: 证明任何允许的状态到概率映射必须是仿射的，否则其曲率会通过操控实现超光速信号传递；仿射性迫使概率分配与复数量子理论的二次跃迁函数一致

Conclusion: 在广义概率理论中，玻恩规则是因果固定点，是满足因果一致性等基本要求的唯一概率定律；为实验验证偏离仿射性提供了理论基础

Abstract: We present an operational derivation of Born's rule within finite-dimensional generalized probabilistic theories (GPTs), without assuming Hilbert-space structure. From a single causal requirement, namely causal consistency, together with sharp measurements, reversible symmetries, and no-signaling, we show that any admissible state-to-probability map must be affine under mixing; otherwise, its curvature enables superluminal signaling via steering. Using standard reconstruction results, affinity forces the probability assignment to coincide with the quadratic transition function of complex quantum theory. Our three-stage argument (operational assignment, causal-consistency constraints, and structural reconstruction) recovers complex quantum theory and identifies Born's rule as a causal fixed point among admissible probabilistic laws. We discuss limitations of the derivation and outline steering-based experiments that could bound deviations from affinity.

</details>


### [194] [Expected values for SUSY hierarchies of Jaynes-Cummings Hamiltonian](https://arxiv.org/abs/2512.12647)
*İsmail Burak Ateş,Şengül Kuru,Javier Negro,Ege Özkan*

Main category: quant-ph

TL;DR: 该研究计算了量子光学Jaynes-Cummings哈密顿量的SUSY伴哈密顿量下场算符、正交分量和原子反转等期望值的演化，探讨了SUSY伴关系对这些期望值的影响，特别是对经典时间和恢复时间的影响。


<details>
  <summary>Details</summary>
Motivation: 研究SUSY伴哈密顿量（其谱仅在有限能级上不同）是否会影响Jaynes-Cummings模型中场算符、正交分量和原子反转等期望值的演化，特别关注SUSY伴关系对经典时间和恢复时间的影响。

Method: 通过计算SUSY伴哈密顿量下期望值的演化，分析SUSY变换对量子光学系统动力学行为的影响，特别是对时间演化特性的改变。

Result: 研究发现SUSY伴哈密顿量确实会影响期望值的演化行为，特别是对经典时间和恢复时间产生了可观测的影响，揭示了SUSY变换对量子系统时间演化特性的具体作用方式。

Conclusion: SUSY伴哈密顿量连接对Jaynes-Cummings模型的期望值演化有显著影响，这种影响体现在经典时间和恢复时间的变化上，为理解SUSY变换在量子光学系统中的作用提供了新的见解。

Abstract: The aim of this letter is to compute the evolution of some expected values, such as the field operators $a^{\pm}$, quadratures and atomic inversion, under SUSY partner Hamiltonians associated to the Jaynes-Cummings Hamiltonian of quantum optics. This kind of SUSY partners are characterized by having spectra which differ in a finite number of energy levels. We wish to elucidate if the partner connection has any influence on these expected values. In particular, we want also to know in which way the classical and revival times are affected by such SUSY partners.

</details>


### [195] [Mid-circuit logic executed in the qubit layer of a quantum processor](https://arxiv.org/abs/2512.12648)
*Cameron Jones,Piper Wysocki,MengKe Feng,Gerardo A. Paz-Silva,Corey I. Ostrove,Tuomo Tanttu,Kenneth M. Rudinger,Samuel K. Bartee,Kevin Young,Fay E. Hudson,Wee Han Lim,Nikolay V. Abrosimov,Hans-Joachim Pohl,Michael L. W. Thewalt,Robin Blume-Kohout,Andrew S. Dzurak,Andre Saraiva,Arne Laucht,Chih Hwan Yang*

Main category: quant-ph

TL;DR: 硅自旋量子比特系统中首次实现中电路测量，并提出无需经典层路由的层内反馈前向操作方案，有望大幅降低未来量子计算机的功耗


<details>
  <summary>Details</summary>
Motivation: 量子计算机需要在计算过程中持续进行量子-经典数据交换，中电路测量和反馈前向操作对容错量子计算至关重要，但量子-经典环路必须在量子比特退相干前完成，这对包含数百万量子比特的全规模系统构成重大工程挑战

Method: 在硅自旋量子比特系统中首次实现中电路测量，采用层内反馈前向操作策略，利用先前被视为误差源的反作用驱动控制技术，避免将信息路由到经典层。同时使用标准的FPGA方法作为对比，通过门集层析技术分析两种方法的性能

Result: 成功演示了无需经典层路由的层内反馈前向操作，将资源密集的经典处理移入量子层，为解决关键工程挑战和大幅降低未来量子计算机的功耗提供了可能

Conclusion: 该研究展示了在硅自旋量子比特系统中实现层内反馈前向操作的可行性，为将资源密集型经典处理移入量子层迈出了第一步，有望解决大规模量子计算机的关键工程挑战并显著降低功耗

Abstract: Practical quantum computers need to continuously exchange data between classical and quantum subsystems during a computation. Mid-circuit measurements of a qubits state are transferred to the classical electronics layer, and their outcome can inform feedforward operations that close the loop back to the quantum layer. These operations are crucial for fault-tolerant quantum computers, but the quantum-classical loop must be completed before the qubits decohere, presenting a substantial engineering challenge for full-scale systems comprising millions of qubits. Here we perform the first mid-circuit measurements in a system of silicon spin qubits, and show that feedforward operations can be performed without needing to route information to the classical layer. This in-layer approach leverages a backaction-driven control technique that has previously been considered a source of error. We benchmark our in-layer strategy, together with the standard FPGA-enabled approach, and analyse the performance of both methods using gate set tomography. Our results provide the first step towards moving resource-intensive classical processing into the quantum layer, an advance that could solve key engineering challenges, and drastically reduce the power budget of future quantum computers.

</details>


### [196] [FiD-QAE: A Fidelity-Driven Quantum Autoencoder for Credit Card Fraud Detection](https://arxiv.org/abs/2512.12689)
*Mansour El Alami,Adam Innan,Nouhaila Innan,Muhammad Shafique,Mohamed Bennai*

Main category: quant-ph

TL;DR: 该论文提出了一种基于保真度的量子自编码器(FiD-QAE)，用于信用卡欺诈检测，通过量子保真度作为异常检测标准，在量子噪声环境下保持鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 信用卡欺诈检测面临数据高度不平衡、欺诈交易与合法交易相似等挑战，现有经典和量子机器学习方法在可扩展性、鲁棒性和适应性方面仍有局限。

Method: 提出FiD-QAE架构：将交易数据编码为量子态，通过变分量子电路进行压缩，使用SWAP测试计算保真度作为异常检测的决策标准。

Result: FiD-QAE在不同不平衡水平下保持一致的性能，在量子噪声模型下保持鲁棒性，在IBM量子硬件上的验证结果与仿真一致，证实了方法的可行性。

Conclusion: 量子保真度是异常检测的强大标准，FiD-QAE作为现有经典和量子方法的补充，为金融欺诈检测提供了鲁棒性和泛化能力，是实际应用中的有前景方向。

Abstract: Credit card fraud detection is a critical task in financial security, as fraudulent transactions are rare, highly imbalanced, and often resemble legitimate ones. A wide range of classical machine learning methods, as well as more recent quantum machine learning approaches, have been investigated to address this challenge, each providing valuable progress but also leaving open questions regarding scalability, robustness, and adaptability to evolving fraud patterns. In this work, we introduce the Fidelity-based Quantum Autoencoder (FiD-QAE), a quantum architecture that employs fidelity estimation as the decision criterion for anomaly detection. Transactions are encoded into quantum states, compressed through a variational quantum circuit, and evaluated using the SWAP test to distinguish legitimate from fraudulent transactions. We conduct a comprehensive evaluation of FiD-QAE, including statistical analyses, multiple performance metrics, and robustness tests under quantum noise models. The results show that FiD-QAE maintains consistent performance across different imbalance levels and preserves robustness in noisy conditions. Moreover, validation on IBM Quantum hardware backends confirms the feasibility of our approach on real devices, with outcomes consistent with simulation. These findings position quantum fidelity as a powerful criterion for anomaly detection and highlight FiD-QAE as a promising direction that complements existing classical and quantum approaches, offering robustness and generalizability for financial fraud detection in realistic environments.

</details>


### [197] [Entanglement, Coherence, and Recursive Linking in Dicke states : A Topological Perspective](https://arxiv.org/abs/2512.12704)
*Sougata Bhattacharyya,Sovik Roy*

Main category: quant-ph

TL;DR: 该研究通过拓扑视角分析对称Dicke态的多体纠缠结构，将量子比特视为拓扑环，建立了Dicke态递归测量动力学与n-Hopf链环稳定性之间的对应关系。


<details>
  <summary>Details</summary>
Motivation: 研究对称Dicke态中多体纠缠的拓扑结构，探索量子态测量动力学与拓扑链环稳定性之间的深层联系，理解不同量子态（如GHZ态和Dicke态）在纠缠鲁棒性上的本质差异。

Method: 将量子比特视为拓扑环，建立Dicke态递归测量动力学与n-Hopf链环稳定性的对应关系；使用Schmidt秩量化二分纠缠鲁棒性；引入量子相干性的l1范数作为链环流动性的度量。

Result: 与类似Borromean环的脆弱GHZ态不同，Dicke态展现出鲁棒的自相似拓扑结构，局部测量通过非零残余相干性保持全局链环结构，表现出更强的纠缠鲁棒性。

Conclusion: 对称Dicke态具有独特的鲁棒拓扑纠缠结构，其测量动力学与n-Hopf链环稳定性直接对应，为理解多体量子纠缠的拓扑本质提供了新视角。

Abstract: This work investigates the topological structure of multipartite entanglement in symmetric Dicke states $|D_n^{(k)}\rangle$. By viewing qubits as topological loops, we establish a direct correspondence between the recursive measurement dynamics of Dicke states and the stability of $n$-Hopf links. We utilize the Schmidt rank to quantify bipartite entanglement resilience and introduce the $l_1$-norm of quantum coherence as a measure of link fluidity. We demonstrate that unlike fragile states such as $ \left| GHZ \right \rangle$ (analogous to Borromean rings), Dicke states exhibit a robust, self-similar topology where local measurements preserve the global linking structure through non-vanishing residual coherence.

</details>


### [198] [Nonlocal Cancellation of Optical Rotations in Fructose Solutions](https://arxiv.org/abs/2512.12739)
*Wen-Chia Lo,Chao-Yuan Wang,Yu-Tung Tsai,Sheng-Yao Huang,Kang-Shih Liu,Yun-Hsuan Shih,Ching-Hua Tsai,Chih-Sung Chuu*

Main category: quant-ph

TL;DR: 该论文报道了利用偏振纠缠光子对在果糖溶液中实现光学旋转的非局域抵消与叠加现象，并通过纠缠光子联合测量实现远程探测光学活性


<details>
  <summary>Details</summary>
Motivation: 量子纠缠作为量子力学中最具代表性的现象之一，已被广泛应用于基础研究和现代量子技术。本研究旨在探索纠缠光子在探测分子光学活性方面的应用潜力，特别是实现非局域的光学旋转操控和测量。

Method: 使用偏振纠缠光子对在果糖溶液中进行实验，通过纠缠光子的联合测量实现光学旋转的非局域抵消与叠加，并远程探测光学活性。

Result: 实验观察到了光学旋转的非局域抵消和叠加现象，实验结果与理论预测吻合良好，证明了该方法可用于探测其他手性分子。

Conclusion: 该方法展示了利用纠缠光子探测手性分子的潜力，且灵敏度随着纠缠光子数量的增加而提高，为扩展这些测量到其他手性分子提供了可能。

Abstract: Entanglement, one of the most representative phenomena in quantum mechanics, has been widely used for fundamental studies and modern quantum technologies. In this paper, we report the observation of nonlocal cancellation and addition of optical rotations with polarization-entangled photons in fructose solutions. The entanglement also enables probing optical activities at a distance by joint measurements on the entangled photons. The good agreement between the experimental results and theoretical predictions demonstrates the potential for extending these measurements to other chiral molecules, with a sensitivity that improves as the number of entangled photons increases.

</details>


### [199] [The Quantum Fourier Transform for Continuous Variables](https://arxiv.org/abs/2512.12771)
*Gianfranco Cariolaro,Edi Ruffa,Amir Mohammad Yaghoobianzadeh,Jawad A. Salehi*

Main category: quant-ph

TL;DR: 该论文研究了连续变量量子傅里叶变换（cvQFT），将其定义为离散傅里叶变换矩阵对应的旋转算子，并利用Murnaghan方法和FFT算法优化实现复杂度，最后分析了cvQFT对高斯态的影响。


<details>
  <summary>Details</summary>
Motivation: 离散变量量子傅里叶变换（dvQFT）在量子计算中已有广泛应用，但连续变量量子傅里叶变换（cvQFT）的研究相对较少。论文旨在系统研究cvQFT的定义、实现方法及其对高斯态的作用效果。

Method: 将cvQFT定义为离散傅里叶变换矩阵对应的旋转算子，利用Murnaghan方法通过基本组件（单模旋转和分束器）实现旋转算子，并应用快速傅里叶变换算法大幅降低实现复杂度。

Result: 建立了cvQFT的完整理论框架，证明了cvQFT对高斯态具有简洁的变换效果：位移矢量经历一维DFT变换，压缩矩阵经历二维DFT变换，旋转矩阵经历傅里叶类相似变换。

Conclusion: 成功构建了连续变量量子傅里叶变换的理论体系，提供了高效的实现方法，并揭示了其对高斯态的变换规律，为连续变量量子信息处理提供了重要工具。

Abstract: The quantum Fourier transform for discrete variable (dvQFT) is an efficient algorithm for several applications. It is usually considered for the processing of quantum bits (qubits) and its efficient implementation is obtained with two elementary components: the Hadamard gate and the controlled--phase gate. In this paper, the quantum Fourier transform operating with continuous variables (cvQFT) is considered. Thus, the environment becomes the Hilbert space, where the natural definition of the cvQFT will be related to rotation operators, which in the $N$--mode are completely specified by unitary matrices of order $N$. Then the cvQFT is defined as the rotation operator whose rotation matrix is given by the discrete Fourier transform (DFT) matrix. For the implementation of rotation operators with primitive components (single--mode rotations and beam splitters), we follow the well known Murnaghan procedure, with appropriate modifications. Moreover, algorithms related to the fast Fourier transform (FFT) are applied to reduce drastically the implementation complexity. The final part is concerned with the application of the cvQFT to general Gaussian states. In particular, we show that cvQFT has the simple effect of transforming the displacement vector by a one-dimensional DFT, the squeeze matrix by a two-dimensional DFT, and the rotation matrix by a Fourier-like similarity transform.

</details>


### [200] [Boundary-driven quantum systems near the Zeno limit: steady states and long-time behavior](https://arxiv.org/abs/2512.12825)
*Eric A. Carlen,David A. Huse,Joel L. Lebowitz*

Main category: quant-ph

TL;DR: 该论文研究了具有边界耗散的复合开放量子系统在大耗散参数γ下的动力学行为，证明了系统在大γ极限下的简化动力学和稳态收敛性质。


<details>
  <summary>Details</summary>
Motivation: 研究具有边界耗散的复合开放量子系统在大耗散参数下的动力学行为，理解系统如何从复杂的Lindblad方程简化为边界部分固定、内部部分演化的简化模型，这对于量子控制和量子信息处理有重要意义。

Method: 通过分析Lindblad方程ρ'(t) = L_γρ(t)，其中L_γρ = -i[H,ρ] + γDρ，耗散器D = D_A⊗I仅作用于系统A部分。假设D_A是遍历且有能隙的，证明在大γ极限下，系统动力学可以简化为π_A⊗R(t)形式。引入第三个Lindblad生成元D_P^#来更好地控制长时间行为。

Result: 证明当D_A遍历且有能隙时，系统在大γ下经过γ^{-1}时间尺度后，动力学可近似为π_A⊗R(t)形式。如果D_P^#也是遍历且有能隙的，则L_γ对所有大γ也是遍历且有能隙的，且稳态ρ_γ收敛到π_A⊗R̄，其中R̄是D_P^#的唯一稳态。给出了稳态的收敛展开式。

Conclusion: 该研究为具有边界耗散的复合开放量子系统提供了严格的理论分析框架，证明了大耗散参数下系统动力学的简化性质，并给出了稳态的精确收敛行为，这对于量子系统的控制和设计具有重要理论价值。

Abstract: We study composite open quantum systems with a finite-dimensional state space ${\mathcal H}_A\otimes {\mathcal H}_B$ governed by a Lindblad equation $ρ'(t) = {\mathcal L}_γρ(t)$ where ${\mathcal L}_γρ= -i[H,ρ] + γ{\mathcal D} ρ$, and ${\mathcal D}$ is a dissipator ${\mathcal D}_A\otimes I$ acting non-trivially only on part $A$ of the system, which can be thought of as the boundary, and $γ$ is a parameter. It is known that the dynamics simplifies for large $γ$: after a time of order $γ^{-1}$, $ρ(t)$ is well approximated for times small compared to $γ^2$ by $π_A\otimes R(t)$ where $π_A$ is a steady state of ${\mathcal D}_A$, and $R(t)$ is a solution of $\frac{\rm d}{{\rm d}t}R(t) = {\mathcal L}_{P,γ}R(t)$ where ${\mathcal L}_{P,γ} R := -i[H_P,R] + γ^{-1} {\mathcal D}_P R$ with $H_P$ being a Hamiltonian on ${\mathcal H}_B$ and ${\mathcal D}_P$ being a Lindblad generator over ${\mathcal H}_B$. We prove this assuming only that ${\mathcal D}_A$ is ergodic and gapped. In order to better control the long time behavior, and study the steady states $\barρ_γ$, we introduce a third Lindblad generator ${\mathcal D}_P^\sharp$ that does not involve $γ$, but still closely related to ${\mathcal L}_γ$. We show that if ${\mathcal D}_P^\sharp$ is ergodic and gapped, then so is ${\mathcal L}_γ$ for all large $γ$, and if $\barρ_γ$ denotes the unique steady state for ${\mathcal L}_γ$, then $\lim_{γ\to\infty}\barρ_γ= π_A\otimes \bar R$ where $\bar R$ is the unique steady state for ${\mathcal D}_P^\sharp$. We show that there is a convergent expansion $\barρ_γ= π_A\otimes\bar R +γ^{-1} \sum_{k=0}^\infty γ^{-k} \bar n_k$ where, defining $\bar n_{-1} := π_A\otimes\bar R$, ${\mathcal D} \bar n_k = -i[H,\bar n_{k-1}]$ for all $k\geq 0$.

</details>


### [201] [Measures to characterise Approximate Mutually Unbiased Bases](https://arxiv.org/abs/2512.12828)
*Ajeet Kumar,Uditanshu Sadual*

Main category: quant-ph

TL;DR: 该论文针对相互无偏基(MUBs)在非素数幂维度下难以构造完整集的问题，提出了近似MUBs(AMUBs)的量化度量方法，用于评估近似MUBs与理想MUBs的接近程度。


<details>
  <summary>Details</summary>
Motivation: 相互无偏基在量子信息处理和编码理论中有重要应用，但在实数域和复数域中，完整MUBs集仅在某些特定维度（如素数幂）下已知。对于大多数维度，特别是非素数幂的复合维度，大尺寸MUBs集难以构造，因此需要研究近似MUBs。

Method: 基于MUBs的几何解释、投影设计特性及其在最优态确定和熵不确定性等应用，定义了可计算的量化度量方法。这些度量可以从MUBs的应用中推导，能够评估近似MUBs与理想MUBs的接近程度。

Result: 建立了这些度量之间的通用关系，并证明可以在不了解具体构造细节的情况下评估近似投影MUBs(APMUBs)，表明APMUB的定义足以完全表征其特性。还对弱MUBs和使用RBDs构造的某些AMUBs进行了度量评估。

Conclusion: 该研究为近似MUBs提供了系统的量化评估框架，使得即使在没有完整构造细节的情况下，也能通过定义的度量来表征和评估近似MUBs的性能，这对于量子信息处理中MUBs的应用具有重要意义。

Abstract: Mutually Unbiased bases has various application in quantum information procession and coding theory. There can be maximum d + 1 MUBs in C^d and d/2 +1 MUBs in R^d. But , over R^d MUBs are known to be non existent when d is odd and for most of the other even d there are mostly 3 Real MUBs. In case of C^d the construction for complete set of MUBs are known for only Prime Power dimension. Thus in general large set of MUBs are not known, particularly for composite dimensions which are not of the form of prime powers. Because of this, there are many constructions of Approximate version of MUBs. In this paper we make an attempt to define certain measures to characterise the AMUBs. Our construction of measures derives its inspiration from the applications of MUBs, and based on them, we define certain quantifiable measures, which are can be computed and gives estimates of how close the Approximate MUBs are to the MUBs. We use geometric interpretation, projective design features of MUBs and applications like Optimal State determination and Entropic Uncertainty of MUBs. We show generic relationship between these measures and show that it can be evaluated for APMUBs without known the exact construction details, thereby showing that definition of APMUB is sufficient completely characterise it. We also evaluate these measure for an interesting class of AMUBs called Weak MUBs and certain AMUBs constructed using RBDs.

</details>


### [202] [Intrinsic Geometry of Operational Contexts: A Riemannian-Style Framework for Quantum Channels](https://arxiv.org/abs/2512.12944)
*Kazuyuki Yoshida*

Main category: quant-ph

TL;DR: 论文提出了一个基于操作上下文空间的固有几何框架，将通道、稳态和自保持泛函作为基本元素，构建了包含指针代数、内部电荷和自洽配置的几何结构。


<details>
  <summary>Details</summary>
Motivation: 建立量子信息与几何结构之间的内在联系，为理解规范对称性和引力动力学提供新的几何解释框架，将传统的Fisher信息度量推广到更一般的操作上下文空间。

Method: 通过定义操作上下文空间（包含通道、稳态和自保持泛函），构建指针代数和内部电荷结构，利用自保持泛函的Hessian定义电荷空间的内在度量，通过非交换提问循环定义曲率概念。

Result: 在适当条件下，该N-Q-S几何可约化为熟悉的Fisher型信息度量，并允许类似于黎曼或洛伦兹时空的坐标图，规范对称性和引力动力学可解释为该上下文几何中的完整性和一致性条件。

Conclusion: 该框架为量子信息、几何和引力理论之间建立了统一的数学联系，为理解物理系统的几何结构提供了新的操作上下文视角。

Abstract: We propose an intrinsic geometric framework on the space of operational contexts, specified by channels, stationary states, and self-preservation functionals. Each context C carries a pointer algebra, internal charges, and a self-consistent configuration minimizing a self-preservation functional. The Hessian of this functional yields an intrinsic metric on charge space, while non-commutative questioning loops dN -> dPhi -> d rho^circ define a notion of curvature. In suitable regimes, this N-Q-S geometry reduces to familiar Fisher-type information metrics and admits charts that resemble Riemannian or Lorentzian space-times. We outline how gauge symmetries and gravitational dynamics can be interpreted as holonomies and consistency conditions in this context geometry.

</details>


### [203] [Actual and weak actual values in Bohmian mechanics](https://arxiv.org/abs/2512.12951)
*Weixiang Ye*

Main category: quant-ph

TL;DR: 该论文提出了在玻姆力学中判断可观测量是否具有确定值的操作鲁棒性准则，建立了弱实际值的概念，并证明了其与量子弱值的对应关系。


<details>
  <summary>Details</summary>
Motivation: 玻姆力学中一个基本问题是：除了位置之外的其他可观测量是否具有确定值。作者旨在为判断可观测量在给定玻姆态中能否被赋予实际值建立一个标准。

Method: 引入操作鲁棒性条件作为判断准则，提出弱实际值作为理论构造来描述可观测量沿轨迹的局部平均行为，并建立相应的数学框架。

Result: 得到了必要充分条件：在适当正则条件下，当且仅当波函数在粒子构型处满足相应算符的局部本征条件时，才能一致地赋予实际值。证明了弱实际值的系综平均等于标准量子期望值，并建立了弱实际值与位置后选择下的量子弱值实部相等的关系。

Conclusion: 该工作扩展并形式化了玻姆力学中可观测量可定义性的早期讨论，发展了讨论可观测量本体论地位的概念和数学框架，并与弱测量实验建立了理论对应关系。

Abstract: A fundamental question in Bohmian mechanics concerns whether observables other than position possess definite values. We introduce a condition of operational robustness as a criterion for when an observable can be attributed an actual value in a given Bohmian state. The main result is a necessary and sufficient condition: under appropriate regularity conditions, an actual value can be consistently assigned if and only if the wave function satisfies a local eigencondition of the corresponding operator at the particle's configuration. For general states, we define a weak actual value as a derived theoretical construct to characterize the local average behavior of an observable along a trajectory. We prove that its ensemble average equals the standard quantum expectation value and derive its evolution equation. Furthermore, we establish that the weak actual value equals the real part of the quantum weak value when post-selecting on position. This work extends and formalizes earlier discussions on the definability of observables in Bohmian mechanics, develops a conceptual and mathematical framework for discussing the ontological status of observables, and establishes a theoretical correspondence with weak measurement experiments.

</details>


### [204] [Quantum Coherence in Reflected and Refracted Beams: A Van Cittert-Zernike Approach](https://arxiv.org/abs/2512.12968)
*Yuetao Chen,Gaiqing Chen,Jin Wang,Qiang Ma,Shoukang Chang,Shaoyan Gao*

Main category: quant-ph

TL;DR: 该论文提出了量子van Cittert-Zernike定理，描述了光在介质界面反射和折射时如何影响其量子相干性和偏振特性，以及这些特性在后续传播中的演化，为量子态控制提供了避免退相干的新机制。


<details>
  <summary>Details</summary>
Motivation: 量子光学中空间传播对光量子相干性的控制作用日益重要，但光在介质界面进行基本光学过程时的量子相干性演化尚未被探索。同时，操纵多光子关联通常需要复杂相互作用，难以在少光子水平实现。

Method: 引入量子van Cittert-Zernike定理，描述光在界面反射和折射时其相干-偏振特性如何被影响，以及这些特性在后续传播中的演化。利用界面反射和折射产生的固有偏振耦合来可控修改光子系统的量子统计特性，而不依赖传统的光-物质相互作用。

Result: 展示了热光可以通过后选择测量表现出亚泊松统计特性（涨落低于散粒噪声水平），且这一统计特性可通过入射角调节。量子统计修改由连接光束准直与远场热化的标度律控制。

Conclusion: 建立了一种稳健的、避免退相干的量子态控制机制，推进了对量子光学中相干性的基本理解，并为量子信息和计量学应用开辟了新途径。

Abstract: Recent advances in quantum optics have highlighted the critical role of spatial propagation in controlling the quantum coherence of light beams. However, the evolution of quantum coherence for light beams undergoing fundamental optical processes at dielectric interfaces remains unexplored. Furthermore, manipulating multiphoton correlations typically requires complex interactions that challenge few-photon level implementation. Here, we introduce a quantum van Cittert-Zernike theorem for light beams, describing how their coherence-polarization properties are influenced by reflection and refraction, as well as how these properties evolve upon subsequent propagation. Our work demonstrates that the quantum statistics of photonic systems can be controllably modified through the inherent polarization coupling arising from reflection and refraction at an interface, without relying on conventional light-matter interactions. Our approach reveals regimes where thermal light can exhibit sub-Poissonian statistics with fluctuations below the shot-noise level through post-selected measurements, and this statistical property can be tuned by the incident angle. Remarkably, this quantum statistical modification is governed by a scaling law linking beam collimation to far-field thermalization. Our work establishes a robust, decoherence-avoiding mechanism for quantum state control, advancing the fundamental understanding of coherence in quantum optics and opening new avenues for applications in quantum information and metrology.

</details>


### [205] [Universal Quantum Random Access Memory: A Data-Independent Unitary Construction](https://arxiv.org/abs/2512.12999)
*Leonardo Bohac*

Main category: quant-ph

TL;DR: 提出一种量子随机存取存储器（QRAM）构造，使用单一、数据无关的酉算子，通过内存量子比特作为控制信号实现相干查找


<details>
  <summary>Details</summary>
Motivation: 现有QRAM方法通常产生数据依赖的酉算子或基于路由的方法，需要更简化的实现方式，将固定电路结构与可变数据编码分离

Method: 采用通用QRAM设计，将数据编码在内存量子比特中，作为块对角置换结构内的量子控制信号。内存量子比特作为控制信号，当地址处于叠加态时实现相干查找。对于N个地址和K位数据字，需要log₂N + K + NK个量子比特，分解为NK个多控制门

Result: 验证了N∈{2,4,8,16}和K∈{1,2,3,4}的情况，确认所得酉算子是纯置换矩阵，在所有数据配置下误差为零

Conclusion: 该方法通过分离固定电路结构与可变数据编码简化了QRAM实现，提供了一种数据无关的酉算子构造

Abstract: We present a construction for Quantum Random Access Memory (QRAM) that achieves a single, data-independent unitary operator. Unlike routing-based approaches or circuit methods that yield data-dependent unitaries, our Universal QRAM encodes data in memory qubits that act as quantum control signals within a block-diagonal permutation structure. The key insight is that memory qubits serve as control signals, enabling coherent lookup when addresses are in superposition. For N addresses with K-bit data words, the construction requires $\log_2 N + K + NK$ qubits and decomposes into exactly $NK$ multi-controlled gates. We verify the construction for $N \in \{2, 4, 8, 16\}$ and $K \in \{1, 2, 3, 4\}$, confirming that the resulting unitary is a pure permutation matrix with zero error across all data configurations. This approach simplifies QRAM implementation by separating fixed circuit structure from variable data encoding.

</details>


### [206] [Measurement-Induced Perturbations of Hausdorff Dimension in Quantum Paths](https://arxiv.org/abs/2512.13046)
*You-Wei Ding,Yen Chin Ong,Hao Xu*

Main category: quant-ph

TL;DR: 该研究探讨了量子测量如何改变量子粒子路径的分形几何，发现测量过程会降低路径的粗糙度，使豪斯多夫维度向更低值移动，而选择性演化则需要反馈控制来稳定轨迹和调节维度。


<details>
  <summary>Details</summary>
Motivation: Abbott等人的开创性工作预测了量子路径的豪斯多夫维度在粒子动量增加时从d=2过渡到d=1，但他们的计算只涉及自由演化波函数的期望值，没有实际物理测量。本研究旨在探究真实的量子测量如何改变量子粒子路径的分形几何。

Method: 使用高斯波包对粒子和测量装置进行建模，模拟顺序测量过程。分析非选择性演化中测量对路径粗糙度的影响，以及在选择性演化中引入反馈控制力来对抗随机波函数坍缩。

Result: 测量过程改变了量子路径的粗糙度，使涌现的豪斯多夫维度向更低值移动。在非选择性演化中，测量降低了维度；在选择性演化中，反馈控制能够稳定轨迹并调节维度。当测量贡献趋近于零时，结果退化为Abbott等人的预测。

Conclusion: 本研究为Abbott等人的方法提供了更现实的表述，将理论量子分形性与测量物理联系起来，量化了探测器如何在量子尺度上重塑时空统计特性。

Abstract: In a seminal paper, Abbott et al. analyzed the relationship between a particle's trajectory and the resolution of position measurements performed by an observer at fixed time intervals. They predicted that quantum paths exhibit a universal Hausdorff dimension that transitions from $d=2$ to $d=1$ as the momentum of the particle increases. However, although measurements were assumed to occur at intervals of time, the calculations only involved evaluating the expectation value of operators for the free evolution of wave function within a single interval, with no actual physical measurements performed. In this work we investigate how quantum measurements alter the fractal geometry of quantum particle paths. By modelling sequential measurements using Gaussian wave packets for both the particle and the apparatus, we reveal that the dynamics of the measurement change the roughness of the path and shift the emergent Hausdorff dimension towards a lower value in nonselective evolution. For selective evolution, feedback control forces must be introduced to counteract stochastic wave function collapse, stabilising trajectories and enabling dimensionality to be tuned. When the contribution of the measurement approaches zero, our result reduces to that of Abbott et al. Our work can thus be regarded as a more realistic formulation of their approach, and it connects theoretical quantum fractality with measurement physics, quantifying how detectors reshape spacetime statistics at quantum scales.

</details>


### [207] [Quantum simulation of strong Charge-Parity violation and Peccei-Quinn mechanism](https://arxiv.org/abs/2512.13049)
*Le Bin Ho*

Main category: quant-ph

TL;DR: 该研究通过量子模拟方法探索QCD中的CP对称性破坏问题，构建了(1+1)维Schwinger模型的量子比特表示，并演示了Peccei-Quinn机制如何驱动系统趋向θ=0。


<details>
  <summary>Details</summary>
Motivation: 量子色动力学(QCD)中存在违反CP对称性的拓扑θ项，但实验观测表明θ值几乎为零。这种理论与实验之间的不一致性需要在一个可控的环境中研究，以理解CP对称性破坏及其动力学解决方案。

Method: 通过推导QCD拉格朗日量的哈密顿表示，构建其(1+1)维Schwinger模型类比。使用Jordan-Wigner变换和量子链接方案将费米子和规范自由度编码到量子比特中，获得紧凑的泡利哈密顿量。采用基于反馈的量子优化协议制备基态，在少量量子比特模拟器上数值计算真空能量E0(θ)。

Result: 结果显示在非零θ处存在位移的真空态，符合强相互作用的预期。引入动态轴子场后，系统被驱动趋向θ=0，从而在最小量子模拟中实现了Peccei-Quinn机制。

Conclusion: 这项研究展示了量子硬件如何用于研究规范理论中的对称性破坏及其动力学解决方案，为探索QCD中的CP问题提供了新的量子模拟途径。

Abstract: Quantum Chromodynamics (QCD) admits a topological θ-term that violates Charge-Parity (CP) symmetry, yet experimental indicate that θ is nearly zero. To investigate this discrepancy in a controlled setting, we derive the Hamiltonian representation of the QCD Lagrangian and construct its (1+1)-dimensional Schwinger-model analogue. By encoding fermionic and gauge degrees of freedom into qubits using the Jordan-Wigner and quantum-link schemes, we obtain a compact Pauli Hamiltonian that retains the relevant topological vacuum structure. Ground states are prepared using a feedback-based quantum optimization protocol, enabling numerical evaluation of the vacuum energy E0(θ) on a few-qubit simulator. Our results show a displaced vacuum at nonzero θin agreement with strong-interaction expectations, and demonstrate that introducing a dynamical axion field drives the system toward θ= 0, thereby realizing the Peccei-Quinn mechanism within a minimal quantum simulation. These results illustrate how quantum hardware can examine symmetry violation and its dynamical resolution in gauge theories.

</details>


### [208] [The emergence of long-range entanglement and odd-even effect in periodic generalized cluster models](https://arxiv.org/abs/2512.13110)
*Zhen-Yu Zheng,Shu Chen*

Main category: quant-ph

TL;DR: 研究广义簇模型在周期性边界条件下的纠缠特性，发现当系统尺寸N和相互作用范围m均为奇数时，系统表现出非零的四部分量子条件互信息熵，这是长程纠缠的直接标志。


<details>
  <summary>Details</summary>
Motivation: 研究一维自旋系统中系统尺寸与相互作用范围之间的相互作用如何影响长程纠缠的出现，探索量子多体系统中纠缠特性的调控机制。

Method: 在周期性边界条件下研究广义簇模型，通过评估三部分或四部分子系统划分下的纠缠熵和量子条件互信息熵，分析不同系统尺寸N和相互作用范围m组合下的纠缠特性。

Result: 当N和m均为奇数时，系统表现出非零的四部分量子条件互信息熵，直接表明存在长程纠缠；其他N和m组合下四部分量子条件互信息熵为零。在N,m均为奇数的情况下，即使存在有限横向场，这些长程纠缠特征仍然存在，显示出对量子涨落的鲁棒性。

Conclusion: 系统尺寸与相互作用范围之间的相互作用决定了一维自旋系统中长程纠缠的出现，特定参数组合下系统表现出对量子涨落鲁棒的长程纠缠特性。

Abstract: We investigate the entanglement properties in a generalized cluster model under periodic boundary condition. By evaluating the entanglement entropy and the quantum conditional mutual information entropy under three or four subsystem partitions, we identify clear signatures of long-range entanglement. Specifically, when both the system size $N$ and the interaction range $m$ are odd, the system exhibits nonzero four-part quantum conditional mutual information entropies. This non-vanishing four-part quantum conditional mutual information entropy directly signals the presence of long-range entanglement. In contrast, all other combination of $N$ and $m$ yield vanishing four-part quantum conditional mutual information entropy. Remarkably, in the case of $N, m \in \text{odd}$, these long-range entangled features persist even in the presence of a finite transverse field, demonstrating their robustness against quantum fluctuations. These results demonstrate how the interplay between system size and interaction range governs the emergence of long-range entanglement in one-dimensional spin systems.

</details>


### [209] [Genuine Tripartite Strong Coupling in a Superconducting-Spin Hybrid Quantum System](https://arxiv.org/abs/2512.13129)
*Yingqiu Mao,Han-Yu Ren,Zi-Yi Liu,Yi-Zheng Zhen,Tao Rong,Tao Jiang,Zhuo Chen,Zhe-Heng Yuan,Wen-Hua Qin,Xiaoran Zhang,Xiaobing Liu,Ming Gong,Kae Nemoto,William J. Munro,Johannes Majer*

Main category: quant-ph

TL;DR: 该研究在固态混合量子系统中实现了真正的三方强耦合，包含超导transmon量子比特、固定频率共面波导谐振器和金刚石NV色心系综，展示了跨三个子系统的相干能量共享。


<details>
  <summary>Details</summary>
Motivation: 建立一种新的混合腔量子电动力学体系，整合超导和自旋自由度，为探索复杂多组分动力学和开发混合量子接口提供平台。

Method: 构建包含超导transmon量子比特、固定频率共面波导谐振器和金刚石NV色心系综的固态混合量子系统，通过频域光谱学测量特征的三模避免交叉现象。

Result: 观察到特征的三模避免交叉，表明单激发在三个子系统间相干共享；在更高探测功率下观察到非线性特征，包括多光子跃迁和transmon-氮14核自旋相互作用的特征。

Conclusion: 该研究建立了超导和自旋自由度整合的新混合腔QED体系，为探索复杂多组分动力学和开发混合量子接口提供了平台。

Abstract: We demonstrate genuine tripartite strong coupling in a solid-state hybrid quantum system comprising a superconducting transmon qubit, a fixed-frequency coplanar-waveguide resonator, and an ensemble of NV$^-$ centers in diamond. Frequency-domain spectroscopy reveals a characteristic three-mode avoided crossing, indicating that single excitations are coherently shared across all three subsystems. At higher probe powers, we observe nonlinear features including multiphoton transitions and signatures of transmon-${}^{14}\mathrm{N}$ nuclear-spin interactions, highlighting the accessibility of higher-excitation manifolds in this architecture. These results establish a new regime of hybrid cavity QED that integrates superconducting and spin degrees of freedom, providing a platform for exploring complex multicomponent dynamics and developing hybrid quantum interfaces.

</details>


### [210] [Intense-Laser Nondipole-Induced Symmetry Breaking in Solids](https://arxiv.org/abs/2512.13140)
*Asger Weeth,Lars Bojer Madsen*

Main category: quant-ph

TL;DR: 该研究探讨了在固体高次谐波产生模拟中纳入非偶极项对光物质耦合的影响，发现非偶极项会打破偶极选择规则，产生新的偏振光，特别是诱导出偶极近似中完全缺失的螺旋性，且这种螺旋性与材料的拓扑相有关。


<details>
  <summary>Details</summary>
Motivation: 高次谐波光谱学是研究固体内部工作机制的重要工具，但现有模拟通常依赖电偶极近似，而驱动场已进入挑战该近似准确性的范围。需要研究非偶极项对高次谐波产生的影响，特别是在拓扑材料中。

Method: 在固体高次谐波产生的模拟中，将非偶极项纳入光物质耦合，研究拓扑平凡和非平凡相材料中高次谐波产生的效应。

Result: 非偶极项的纳入打破了偶极选择规则，允许产生新的偏振光。特别发现偶极近似中完全缺失的螺旋性被非偶极扩展诱导出来，且这种螺旋性依赖于材料的拓扑相。

Conclusion: 非偶极效应在高次谐波产生中具有重要影响，不仅能打破偶极选择规则产生新的偏振特性，还能揭示材料的拓扑相信息，为高次谐波光谱学提供了新的探测维度。

Abstract: High-harmonic spectroscopy in solids gives insight into the inner workings of solids, such as reconstructing band structures or probing the topological phase of materials. High-harmonic generation (HHG) is a highly non-linear phenomena and simulations guide interpretation of experimental results. These simulations often rely on the electric dipole approximation, even though the driving fields enter regimes that challenge its accuracy. Here, we investigate effects of including nondipole terms in the light-matter coupling in simulations of HHG in materials with both topologically trivial and non-trivial phases. We show how the inclusion of nondipole terms breaks dipole selection rules, allowing for new polarizations of the generated light. Specifically we find that helicity, completely absent in the dipole approximation, is induced by the nondipole extension, and that this helicity is dependent on the topological phase of the material.

</details>


### [211] [Practical Homodyne Shadow Estimation](https://arxiv.org/abs/2512.13146)
*Ruyu Yang,Xiaoming Sun,Hongyi Zhou*

Main category: quant-ph

TL;DR: 该论文提出了一种实用的连续变量系统影子估计协议，使用离散化零差检测，改进了先前方法的理论假设限制。


<details>
  <summary>Details</summary>
Motivation: 连续变量系统的影子估计扩展面临实际限制，因为需要理想的连续相位调制和无限测量分辨率假设，需要开发更实用的协议。

Method: 使用有限相位设置和正交分箱的离散化零差检测构建无偏估计器，在截断的Fock空间内建立信息完备性的充分必要条件。

Result: 影子范数缩放为𝒪(n_max^4)，优于先前𝒪(n_max^{13/3})的界限，为实验实现提供了实用框架。

Conclusion: 该工作弥合了理论影子估计与实验实现之间的差距，使实际连续变量系统能够进行稳健和可扩展的量子态表征。

Abstract: Shadow estimation provides an efficient framework for estimating observable expectation values using randomized measurements. While originally developed for discrete-variable systems, its recent extensions to continuous-variable (CV) quantum systems face practical limitations due to idealized assumptions of continuous phase modulation and infinite measurement resolution. In this work, we develop a practical shadow estimation protocol for CV systems using discretized homodyne detection with a finite number of phase settings and quadrature bins. We construct an unbiased estimator for the quantum state and establish both sufficient conditions and necessary conditions for informational completeness within a truncated Fock space up to $n_{\mathrm{max}}$ photons. We further provide a comprehensive variance analysis, showing that the shadow norm scales as $\mathcal{O}(n_{\mathrm{max}}^4)$, improving upon previous $\mathcal{O}(n_{\mathrm{max}}^{13/3})$ bounds. Our work bridges the gap between theoretical shadow estimation and experimental implementations, enabling robust and scalable quantum state characterization in realistic CV systems.

</details>


### [212] [A Conjecture on Almost Flat SIC-POVMs](https://arxiv.org/abs/2512.13201)
*Ingemar Bengtsson,Markus Grassl*

Main category: quant-ph

TL;DR: 该论文研究了SIC-POVMs中与反幺正对称性相关的数学恒等式，探讨该恒等式是否足以确定Stark单位，发现答案是否定的但失败程度可能很轻微。


<details>
  <summary>Details</summary>
Motivation: 研究SIC-POVMs（最大复数等角线集）中与反幺正对称性相关的数学恒等式，该恒等式将某些重叠表示为适当选择的基准向量分量的平方，在数论上表达Stark单位作为Stark单位平方根对的和的乘积。

Method: 通过数学分析和数论方法，研究该恒等式是否足以确定Stark单位，分析其确定性的程度和可能的失败模式。

Result: 研究发现该恒等式不足以完全确定Stark单位，但失败的程度可能相当轻微，意味着恒等式提供了重要约束但非完全确定性。

Conclusion: SIC-POVMs中的反幺正对称性产生的恒等式不能完全确定Stark单位，但提供了强有力的约束条件，其失败程度有限，为进一步研究这些数学结构提供了重要见解。

Abstract: A well supported conjecture states that SIC-POVMs -- maximal sets of complex equiangular lines -- with anti-unitary symmetry give rise to an identity expressing some of its overlaps as squares of the (rescaled) components of a suitably chosen fiducial vector. In number theoretical terms the identity essentially expresses Stark units as sums of products of pairs of square roots of Stark units. We investigate whether the identity is enough to determine these Stark units. The answer is no, but the failure might be quite mild.

</details>


### [213] [High-purity frequency-degenerate photon pair generation via cascaded SFG/SPDC in thin film lithium niobate](https://arxiv.org/abs/2512.13248)
*Olivia Hefti,Marco Clementi,Enrico Melani,Jean-Etienne Tremblay,Andrea Volpini,Yesim Koyaz,Homa Zarebidaki,Ivan Prieto,Olivier Dubochet,Daniele Bajoni,Charles Caër,Hamed Sattari,Camille-Sophie Brès,Matteo Galli,Davide Grassani*

Main category: quant-ph

TL;DR: 提出一种基于双泵浦方案在单个波导中产生频率简并光子对的方法，通过级联和频产生与自发参量下转换实现，同时强烈抑制单泵浦过程产生的寄生光子对。


<details>
  <summary>Details</summary>
Motivation: 频率简并光子对是可扩展量子信息处理和计量学的重要资源，但现有方法受到同一相位匹配带内不需要的参量过程干扰，这会降低信噪比和量子态纯度。

Method: 采用双泵浦方案，在单个波导中通过级联和频产生与自发参量下转换产生频率简并光子对，同时强烈抑制单泵浦过程产生的寄生光子对。该方法相比微谐振器方法显著简化设计，并允许在电信波段进行泵浦和光子对收集。

Result: 在层极化薄膜铌酸锂波导中实验验证了该概念，实现了频率简并光子对生成，亮度达到1.0(3)×10⁵ Hz/nm/mW²，并对不需要的单泵浦过程实现了40 dB的抑制。

Conclusion: 该双泵浦方案成功实现了高质量频率简并光子对的产生，显著抑制了寄生过程，为量子信息处理和计量学提供了简化且高效的解决方案。

Abstract: Frequency-degenerate photon pairs generated using nonlinear photonic integrated devices are a crucial resource for scalable quantum information processing and metrology. However, their realization is hindered by unwanted parametric processes occurring within the same phase matching band, which degrade the signal-to-noise ratio and reduce the purity of the associated quantum states. Here, we propose a dual-pump scheme to produce frequency-degenerate photon pairs, based on cascaded sum-frequency generation and spontaneous parametric down-conversion occurring within a single waveguide, while strongly suppressing parasitic photon pair generation from single-pump processes. This approach significantly simplifies the design compared to microresonator-based methods and enables both pumping and collection of photon pairs entirely in the telecom band. We experimentally validate the concept in a layer-poled thin film lithium niobate waveguide, achieving frequency-degenerate photon pair generation with a brightness of \SI{1.0(3)e5}{\hertz \per \nm \per \square \milli \watt } and a 40 dB suppression of unwanted single-pump processes.

</details>


### [214] [Distillation of continuous variable qudits from single photon sources: A cascaded approach](https://arxiv.org/abs/2512.13264)
*Devibala Esakkimuthu,Basherrudin Mahmud Ahmed Abduljaffer*

Main category: quant-ph

TL;DR: 提出一种仅使用单光子源和单光子探测器的线性光学级联装置，无需高非线性或大Fock态即可生成高保真度连续变量量子态。


<details>
  <summary>Details</summary>
Motivation: 连续变量量子技术需要高保真度光子量子态，但传统方法需要高非线性或大Fock态，实现困难。本文旨在克服这一限制。

Method: 使用线性光学级联分束器装置，仅依赖单光子源和单光子探测器，通过将输出表示为位移qudit形式来生成目标单模量子态。

Result: 成功生成位移高光子态（单位保真度）、薛定谔猫态（>98%保真度）、GKP资源态（ON态和弱立方相位态，99%保真度），并考虑了探测器效率和非理想单光子源等实验缺陷。

Conclusion: 该级联装置为实验者提供了一种利用现有资源（单光子源和探测器）探索目标态可行生成的实用方法，无需复杂非线性系统。

Abstract: Creation of high fidelity photonic quantum states in the continuous variable regime is indispensable for the implementation of quantum technologies universally. However, this is a challenging task as it requires higher nonlinearity or larger Fock states. In this article, we surmount this necessity by using a linear optical setup with a cascaded arrangement of beam splitters that relies solely on single photon sources and single photon detectors to tailor desired single mode nonclassical states. To show the utility of this setup, we demonstrate the generation of displaced higher photon states with unit fidelity and the family of Schrodinger cat states above $98\%$ fidelity. In addition, we manifest the generation of GKP resource states, such as ON states and weak cubic phase states with $99\%$ fidelity. Creating such a variety of important states in this single setup is made feasible by stating the output in the form of displaced qudits. This figure of merit facilitates efficient identification and optimization of input parameters required to generate the target single mode quantum states. We also account for the experimental imperfections by incorporating detector inefficiencies and non-unit single photon sources. This cascaded setup will assist the experimentalists to explore the feasible creation of target states using currently available resources, such as single photon sources and single photon detectors.

</details>


### [215] [Slowing and Storing Microwaves in a Single Superconducting Fluxonium Artificial Atom](https://arxiv.org/abs/2512.13272)
*Ching-Yeh Chen,Shih-Wei Lin,Ching-Ping Lee,J. C. Chen,I. -C. Hoi,Yen-Hsiang Lin*

Main category: quant-ph

TL;DR: 在微波波导中使用单个Fluxonium量子比特实现三能级Λ系统，观测到电磁感应透明、光速减慢和光子存储现象


<details>
  <summary>Details</summary>
Motivation: 三能级Λ系统是实现电磁感应透明、慢光和量子记忆的重要平台。之前超导人工原子实验需要耦合额外自由度（如谐振器或其他超导原子），本研究旨在探索单个Fluxonium量子比特在微波波导中实现Λ系统的可能性。

Method: 使用单个Fluxonium量子比特置于微波波导中构建Λ系统。该系统由两个等离子体跃迁和一个源自fluxon跃迁的亚稳态组成。控制跃迁和探测跃迁与传输线强耦合，保护0和1态之间的跃迁，确保Fluxonium量子比特接近甜点位置。

Result: 观测到电磁感应透明现象，光速减慢延迟时间达217纳秒，并实现了光子存储。这些结果表明该系统可作为超导电路中的移相器或量子存储器。

Conclusion: 单个Fluxonium量子比特在微波波导中成功实现了三能级Λ系统，展示了电磁感应透明、光速减慢和光子存储等量子光学现象，为超导电路中的量子通信应用提供了潜在解决方案。

Abstract: Three-level Lambda systems provide a versatile platform for quantum optical phenomena such as Electromagnetically Induced Transparency (EIT), slow light, and quantum memory. Such Lambda systems have been realized in several quantum hardware platforms including atomic systems, superconducting artificial atoms, and meta-structures. Previous experiments involving superconducting artificial atoms incorporated coupling to additional degrees of freedom, such as resonators or other superconducting atoms. In this work, we performed an EIT experiment in microwave frequency range utilizing a single Fluxonium qubit within a microwave waveguide. The Lambda system is consisted of two plasmon transitions in combination with one metastable state originating from the fluxon transition. In this configuration, the controlling and probing transitions are strongly coupled to the transmission line, safeguarding the transition between 0 and 1 states, and ensuring the Fluxonium qubit is close to the sweet spot. Our observations include the manifestation of EIT, a slowdown of light with a delay time of 217 ns, and photon storage. These results highlight the potential as a phase shifter or quantum memory for quantum communication in superconducting circuits.

</details>


### [216] [Coherent feedback-enhanced asymmetry of thermal process in open quantum systems: Cavity optomechanics](https://arxiv.org/abs/2512.13288)
*Hamza Harraf,Mohamed Amazioug,Rachid Ahl Laamara*

Main category: quant-ph

TL;DR: 该研究探讨了在稳态下利用相干反馈环路增强不可逆性的机制，通过量子相空间方法计算熵产生率，发现熵产生与量子互信息成正比，并在法布里-珀罗腔光机械系统中验证了结果。


<details>
  <summary>Details</summary>
Motivation: 研究非平衡热力学中熵产生这一基本概念，探索如何通过相干反馈环路增强物理过程的不可逆性，理解量子系统中不可逆性与量子关联之间的关系。

Method: 采用量子相空间表述方法计算稳态熵产生率和量子关联，在弱耦合极限下分析熵产生与量子互信息的关系，并以法布里-珀罗腔光机械系统作为应用实例进行验证。

Result: 研究发现相干反馈对热浴输入噪声算符有重要贡献，使系统远离热平衡；在弱耦合极限下，熵产生率与量子互信息成正比；在光机械系统中，熵产生的峰值对应可移动镜面的加热/冷却过程得到改善。

Conclusion: 不可逆性与量子关联不是独立的，必须联合分析；研究结果表明通过相干反馈环路增强熵产生的可能性，为量子热应用开辟了新途径。

Abstract: Entropy production is a fundamental concept in nonequilibrium thermodynamics, providing a direct measure of the irreversibility inherent in any physical process. In this work, we investigate in steady-state the enhancement of irreversibility employing coherent feedback loop. We evaluate the steady-state entropy production rate and quantum correlations by applying the quantum phase space formulation to calculate the entropy change. Our study reveals the essential contribution of coherent feedback in the thermal bath's input-noise operators, resulting in the system being driven far from thermal equilibrium. Our analysis shows that in the small-coupling limit, the entropy production rate is proportional to the quantum mutual information. We use for application the optomechanical system of Fabry-Pérot cavity, and show that the picks of the entropy production corresponding of the heating/cooling of movable mirror are improved. Therefore, we conclude that irreversibility and quantum correlations are not independent and must be analyzed jointly. The results demonstrate the possibility of enhancement of entropy production and pave the way for promising quantum thermal applications through coherent feedback loop.

</details>


### [217] [Projected Optimal Sensors from Operator Orbits](https://arxiv.org/abs/2512.13294)
*Sooryansh Asthana,Yeshma Ibrahim,Norman Tze Wei Koo,Sai Vinjanampathy*

Main category: quant-ph

TL;DR: 该论文提出了一个统一框架来分析量子传感器，涵盖Ramsey、twist-untwist和随机量子传感器，并基于算子代数解释Fisher信息的标度行为。


<details>
  <summary>Details</summary>
Motivation: 现有量子传感器设计（如Ramsey、twist-untwist和随机量子传感器）缺乏统一的理论框架来分析其性能标度。需要建立一个统一的算子代数模型来理解不同传感器设计的Fisher信息标度行为，并设计新型高性能量子传感器。

Method: 使用算子代数统一分析各类量子传感器，通过算子轨道与状态制备的关系来理解灵敏度与子系统数量的标度关系。基于此统一模型，设计了一种新型传感器，其中投影量子态集合展现出超越散粒噪声极限的计量性能。

Result: 建立了量子传感器的统一算子代数框架，能够解释不同传感器设计的Fisher信息标度。设计的新型传感器在投影量子态集合中实现了超越散粒噪声极限的计量性能。同时展示了在退相干模型和粒子损失情况下的有利Fisher信息标度。

Conclusion: 通过算子代数框架统一了量子传感器的分析，为理解不同传感器设计的性能标度提供了理论基础。提出的新型传感器设计展示了超越经典极限的计量潜力，为量子计量学的发展提供了新的方向。

Abstract: We unify Ramsey, twist-untwist, and random quantum sensors using operator algebra and account for the Fisher scaling of various sensor designs. We illustrate how the operator orbits associated with state preparation inform the scaling of the sensitivity with the number of subsystems. Using our unified model, we design a novel set of sensors in which a projected ensemble of quantum states exhibits beyond-shot-noise metrological performance. We also show favorable scaling of Fisher information with decoherence models and loss of particles.

</details>


### [218] [Fault-tolerant multi-qubit gates in Parity Codes](https://arxiv.org/abs/2512.13335)
*Anette Messinger,Christophe Goeller,Wolfgang Lechner*

Main category: quant-ph

TL;DR: 该论文提出了一种在级联量子纠错码中使用奇偶校验量子比特实现高效多量子比特逻辑门的方法，包括任意角度的高权重旋转门和横向CNOT门实现的逻辑奇偶控制NOT操作。


<details>
  <summary>Details</summary>
Motivation: 传统量子纠错码中实现多量子比特逻辑门通常需要复杂的操作如晶格手术或路由操作，这增加了实现难度和错误率。本文旨在开发更高效、更简单的实现方法。

Method: 使用奇偶校验量子比特在级联量子纠错码中实现逻辑门。具体包括：1）在经典稳定子码的单个物理量子比特上实现容错的高权重旋转门；2）在完整量子纠错码的局部区域实现类似操作；3）使用横向CNOT门实现任意多个逻辑量子比特之间的逻辑奇偶控制NOT操作。

Result: 提出的方法能够实现高效的多量子比特逻辑门，在许多情况下可以并行化执行，且无需使用晶格手术或复杂的路由操作，简化了量子纠错码中的逻辑门实现。

Conclusion: 通过奇偶校验量子比特在级联量子纠错码中实现逻辑门是一种高效可行的方法，能够简化多量子比特逻辑门的实现，为量子计算的实际应用提供了更实用的技术路径。

Abstract: We present a set of efficiently implementable logical multi-qubit gates in concatenated quantum error correction codes using parity qubits. In particular, we show how fault-tolerant high-weight rotation gates of arbitrary angle can be implemented on single physical qubits of a classical stabilizer code, or on localized regions of full quantum error correction codes. Similarly, we show how transversal CNOT gates can implement logical parity-controlled-NOT operations between arbitrarily many logical qubits. Both operation types can be implemented and in many cases parallelized without the use of lattice surgery or the need for complicated routing operations.

</details>


### [219] [Impact of Information on Quantum Heat Engines](https://arxiv.org/abs/2512.13371)
*Lindsay Bassman Oftelie,Michele Campisi*

Main category: quant-ph

TL;DR: 该论文提出了一个包含麦克斯韦妖的通用两冲程量子热机框架，将工作物质和经典记忆视为混合系统，解决了麦克斯韦悖论，并发现更多信息不一定带来更好的热力学性能。


<details>
  <summary>Details</summary>
Motivation: 量子热力学领域正在揭示信息在量子热机中的作用，但目前缺乏一个通用的量子反馈控制热机框架。需要建立一个能够将工作物质和记忆系统放在同等地位的理论框架，以解决麦克斯韦妖悖论。

Method: 提出了一个通用的两冲程量子热机框架，该热机与N个热浴和麦克斯韦妖相互作用。妖对工作物质进行投影测量，结果记录在经典记忆中。妖根据记录结果对工作物质实施条件性幺正操作。将机器-记忆复合系统视为与N+1个热浴相互作用的混合（经典-量子）标准热机。

Result: 该框架将工作物质和记忆放在同等地位，实现了对麦克斯韦悖论的清晰解析。通过双量子比特引擎示例说明，发现了一个显著现象：更多信息不一定带来更好的热力学性能，有时知道更少反而更好。

Conclusion: 建立了一个通用的量子反馈控制热机框架，解决了麦克斯韦妖悖论，并揭示了信息与热力学性能之间的非单调关系，为量子热力学和信息理论提供了新的见解。

Abstract: The emerging field of quantum thermodynamics is beginning to reveal the intriguing role that information can play in quantum thermal engines. Information enters as a resource when considering feedback-controlled thermal machines. While both a general theory of quantum feedback control as well as specific examples of quantum feedback-controlled engines have been presented, still lacking is a general framework for such machines. Here, we present a framework for a generic, two-stroke quantum heat engine interacting with $N$ thermal baths and Maxwell's demon. The demon performs projective measurements on the engine working substance, the outcome of which is recorded in a classical memory, embedded in its own thermal bath. To perform feedback control, the demon enacts unitary operations on the working substance, conditioned on the recorded outcome. By considering the compound machine-memory as a hybrid (classical-quantum) standard thermal machine interacting with $N+1$ thermal baths, our framework puts the working substance and memory on equal footing, thereby enabling a comprehensible resolution to Maxwell's paradox. We illustrate the application of our framework with a two-qubit engine. A remarkable observation is that more information does not necessarily result in better thermodynamic performance: sometimes knowing less is better.

</details>


### [220] [Fundamental bound on entanglement generation between interacting Rydberg atoms](https://arxiv.org/abs/2512.13379)
*Georgios Doultsinos,Antonis Delakouras,David Petrosyan*

Main category: quant-ph

TL;DR: 该研究推导了基于里德堡态相互作用的双原子最大纠缠态制备保真度的理论下界，并通过量子最优控制方法找到了接近该极限的激光脉冲方案。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索在里德堡态相互作用下制备双原子最大纠缠态（贝尔态）的物理极限。由于里德堡态存在自发衰变（Γ）和有限的相互作用强度（B），这些物理限制决定了纠缠态制备的最终精度。

Method: 研究方法包括：1）通过理论分析推导出制备保真度的基本下界；2）使用量子最优控制方法设计激光脉冲序列，以实现接近理论极限的纠缠态制备。

Result: 研究结果：1）理论推导出最小可实现的误差下界为 E ≥ (1 + π/2) Γ/B；2）通过最优控制方法找到了激光脉冲方案，其制备误差仅比理论下界高1%。

Conclusion: 该研究确定了里德堡态系统中制备最大纠缠态的理论极限，并展示了通过量子最优控制可以接近这一极限，为高保真度量子纠缠操作提供了理论基础和实用方案。

Abstract: We analytically derive the fundamental lower bound for the preparation fidelity of a maximally-entangled (Bell) state of two atoms involving Rydberg-state interactions. This bound represents the minimum achievable error $E \geq ( 1 + π/2 ) Γ/B$ due to spontaneous decay $Γ$ of the Rydberg states and their finite interaction strength $B$. Using quantum optimal control methods, we identify laser pulses for preparing a maximally-entangled state of a pair of atomic qubits with an error only $1\%$ above the derived fundamental bound.

</details>


### [221] [Quantum Chaos as an Essential Resource for Full Quantum State Controllability](https://arxiv.org/abs/2512.13384)
*Lukas Beringer,Mathias Steinhuber,Klaus Richter,Steven Tomsovic*

Main category: quant-ph

TL;DR: 量子混沌系统可以通过弱扰动实现对数时间尺度的完全可控性，而可积系统则需要打破可积性才能实现控制


<details>
  <summary>Details</summary>
Motivation: 探索量子混沌系统是否能够像经典混沌系统一样，利用混沌特性（遍历性和指数不稳定性）作为资源来实现量子动力学的控制

Method: 利用量子混沌的两个关键特性：1）对弱扰动的动力学敏感性（保真度衰减）替代经典初始条件敏感性；2）量子混沌系统可由随机矩阵理论统计描述，实现遍历性特征。通过量子踢转子模型进行验证

Result: 量子混沌系统原则上允许完全可控性，其特征时间尺度仅随系统尺寸和ℏ^{-1}对数增长。能够产生复苏现象、猫态纠缠态，并实现任意随机态到任意其他随机态的转换

Conclusion: 量子混沌系统可以利用弱扰动精细调控巨大的量子干涉效应，将系统从任意初始态引导到任意目标态（受守恒量约束）。而可积动力学缺乏遍历性和指数不稳定性，需要打破可积性才能实现控制

Abstract: Using the key properties of chaos, i.e.~ergodicity and exponential instability, as a resource to control classical dynamics has a long and considerable history. However, in the context of controlling ``chaotic'' quantum unitary dynamics, the situation is far more tenuous. The classical concepts of exponential sensitivity to trajectory initial conditions and ergodicity do not directly translate into quantum unitary evolution. Nevertheless properties inherent to quantum chaos can take on those roles: i) the dynamical sensitivity to weak perturbations, measured by the fidelity decay, serves a similar purpose as the classical sensitivity to initial conditions; and ii) paired with the fact that quantum chaotic systems are conjectured to be statistically described by random matrix theory, implies a method to translate the ergodic feature into the control of quantum dynamics. With those two properties, it can be argued that quantum chaotic dynamical systems, in principle, allow for full controllability beyond a characteristic time that scales only logarithmically with system size and $\hbar^{-1}$. In the spirit of classical targeting, it implies that it is possible to fine tune the immense quantum interference with weak perturbations and steer the system from any initial state into any desired target state, subject to constraints imposed by conserved quantities. In contrast, integrable dynamics possess neither ergodicity nor exponential instability, and thus the weak perturbations apparently must break the integrability for control purposes. The main ideas are illustrated with the quantum kicked rotor. The production of revivals, cat-like entangled states, and the transition from any random state to any other random state is possible as demonstrated.

</details>


### [222] [Riemannian gradient descent-based quantum algorithms for ground state preparation with guarantees](https://arxiv.org/abs/2512.13401)
*Mahum Pervez,Ariq Haqq,Nathan A. McMahon,Christian Arenz*

Main category: quant-ph

TL;DR: 该论文研究了在量子设备上使用黎曼梯度流制备哈密顿量基态的方法，分析了算法步数与哈密顿量结构的关系，并开发了高效的量子实现方案。


<details>
  <summary>Details</summary>
Motivation: 研究如何在量子设备上高效制备哈密顿量的基态，这对于量子计算中的状态准备和量子模拟具有重要意义。传统方法可能效率不高，需要开发新的优化算法。

Method: 采用黎曼梯度下降（RGD）算法，分析其收敛步数与哈密顿量谱隙、初始态与基态重叠度以及目标精度的关系。开发了随机投影近似方法，并基于Trotter化和量子随机漂移协议实现了高效的量子设备实现。

Result: 建立了RGD步数的上界，数值实验显示：对于最近邻相互作用的1D Ising链，基态制备步数与自旋数呈线性关系；对于全耦合系统则呈二次关系。随机投影RGD的收敛速度取决于投影子空间的大小。在IBM量子设备上实现了小规模问题的量子算法。

Conclusion: 黎曼梯度流是制备哈密顿量基态的有效方法，其收敛性能取决于哈密顿量结构。通过随机投影近似和高效的量子实现，可以在保持收敛保证的同时实现高效计算，为量子设备上的状态准备提供了新途径。

Abstract: We investigate Riemannian gradient flows for preparing ground states of a desired Hamiltonian on a quantum device. We show that the number of steps of the corresponding Riemannian gradient descent (RGD) algorithm that prepares a ground state to a given precision depends on the structure of the Hamiltonian. Specifically, we develop an upper bound for the number of RGD steps that depends on the spectral gap of the Hamiltonian, the overlap between ground and initial state, and the target precision. In numerical experiments we study examples where we observe for a 1D Ising chain with nearest-neighbor interactions that the RGD steps needed to prepare a ground state scales linearly with the number of spins. For all-to-all couplings a quadratic scaling is obtained. To achieve efficient implementations while keeping convergence guarantees, we develop RGD approximations by randomly projecting the Riemannian gradient into polynomial-sized subspaces. We find that the speed of convergence of the randomly projected RGD critically depends on the size of the subspace the gradient is projected into. Finally, we develop efficient quantum device implementations based on Trotterization and a quantum stochastic drift-inspired protocol. We implement the resulting quantum algorithms on IBM's quantum devices and provide data for small-scale problems.

</details>


### [223] [Decoding 3D color codes with boundaries](https://arxiv.org/abs/2512.13436)
*Friederike Butt,Lars Esser,Markus Müller*

Main category: quant-ph

TL;DR: 本文提出了一种针对三维颜色码的约束解码器，将解码问题限制在量子比特晶格的子集上，实现了逻辑错误率的最优缩放和1.55(6)%的阈值，比之前报道的提高了近两倍。


<details>
  <summary>Details</summary>
Motivation: 大规模量子计算需要高效的纠错和逻辑操作的鲁棒实现。三维颜色码因其横向非克利福德门而成为容错量子计算的有前景候选方案，但高效解码仍然具有挑战性。

Method: 将先前针对二维颜色码的解码器扩展到三维，基于将解码问题限制在量子比特晶格的子集上，并包含三维颜色码的边界。同时开发了qCodePlot3D Python包用于可视化2D和3D颜色码、错误配置和解码路径。

Result: 三维约束解码器实现了逻辑错误率的最优缩放，在码容量比特和相位翻转噪声下获得了1.55(6)%的阈值，比之前报道的该码族阈值提高了近两倍。

Conclusion: 这些进展使三维颜色码成为探索容错量子计算的更实用选择，为大规模量子计算提供了更高效的解码方案。

Abstract: Practical large-scale quantum computation requires both efficient error correction and robust implementation of logical operations. Three-dimensional (3D) color codes are a promising candidate for fault-tolerant quantum computation due to their transversal non-Clifford gates, but efficient decoding remains challenging. In this work, we extend previous decoders for two-dimensional color codes [1], which are based on the restriction of the decoding problem to a subset of the qubit lattice, to three dimensions. Including boundaries of 3D color codes, we demonstrate that the 3D restriction decoder achieves optimal scaling of the logical error rate and a threshold value of 1.55(6)% for code-capacity bit- and phase-flip noise, which is almost a factor of two higher than previously reported for this family of codes [2, 3]. We furthermore present qCodePlot3D, a Python package for visualizing 2D and 3D color codes, error configurations, and decoding paths, which supports the development and analysis of such decoders. These advancements contribute to making 3D color codes a more practical option for exploring fault-tolerant quantum computation.

</details>


### [224] [Wigner function negativity in a classical model of quantum light](https://arxiv.org/abs/2512.13462)
*Brian R. La Cour*

Main category: quant-ph

TL;DR: 该研究展示了一个经典模型，通过后选择机制能够再现单光子添加相干态的观测行为，包括产生负值的维格纳函数


<details>
  <summary>Details</summary>
Motivation: 维格纳准概率分布中的负值通常被认为是量子系统非经典现象的标志，但本研究试图探索经典模型是否也能产生类似行为

Method: 使用经典压缩光模型，结合振幅阈值交叉检测事件的后选择机制，采用平衡零差检测的经典模型和标准层析技术来推断福克基中的密度矩阵

Result: 该经典模型能够再现单光子添加相干态的观测行为，对于光子添加真空态和弱相干态，得到的维格纳函数表现出负值

Conclusion: 维格纳函数中的负值不一定完全是非经典性的标志，经典模型结合适当的后选择机制也能产生类似行为，这对量子非经典性的判断标准提出了新的思考

Abstract: The presence of negative values in the Wigner quasiprobability distribution is deemed one of the hallmarks of nonclassical phenomena in quantum systems. Here we demonstrate a classical model of squeezed light that, when combined with post-selection on amplitude threshold-crossing detection events, is capable of reproducing observed behavior of single-photon added coherent states. In particular, a classical model of balanced homodyne detection and standard tomographic techniques are used to infer the density matrix in the Fock basis. The resulting Wigner functions exhibit negatively for photon-added vacuum and weak coherent states.

</details>


### [225] [Arrival Time -- Classical Parameter or Quantum Operator?](https://arxiv.org/abs/2512.13502)
*MohammadJavad Kazemi,MohammadHossein Barati,Ghadir Jafari,S. Shajidul Haque,Saurya Das*

Main category: quant-ph

TL;DR: 该研究将量子力学中到达时间分布的两种基本方法（时间参数法和时间算符法）扩展到多粒子系统，提出了可行的双粒子到达时间实验，揭示了两种方法在某些条件下会产生不等价的预测，为区分量子力学中时间的不同解释提供了实验可能性。


<details>
  <summary>Details</summary>
Motivation: 量子力学中到达时间分布的解释和计算问题一直存在争议，反映了将时间视为量子可观测量还是经典参数之间的长期张力。随着原子光学技术的发展，现在可以实验研究纠缠多粒子系统在近场区域的到达时间分布，这需要超越半经典近似的深入分析。即使在远场区域，由于量子非局域性，半经典近似在多粒子系统中通常也不成立。

Method: 将到达时间问题的两种基本方法——时间参数法和时间算符法——扩展到多粒子系统。使用这些扩展方法，提出了一个可行的双粒子到达时间实验，并数值评估了相应的联合分布。

Result: 研究结果揭示了在某些条件下，两种方法会产生不等价的预测，突出了实验可能区分量子力学中时间不同解释的条件。这些发现为开发利用时间域纠缠的量子技术提供了重要见解。

Conclusion: 该工作为区分量子力学中时间的不同解释提供了实验可能性，并为开发基于时间域纠缠的量子技术（包括非局域时间干涉测量、时间鬼成像和多粒子系统中的时间态层析）提供了重要见解。

Abstract: The question of how to interpret and compute arrival-time distributions in quantum mechanics remains unsettled, reflecting the longstanding tension between treating time as a quantum observable or as a classical parameter. Most previous studies have focused on the single-particle case in the far-field regime, where both approaches yield very similar arrival-time distributions and a semi-classical analysis typically suffices. Recent advances in atom-optics technologies now make it possible to experimentally investigate arrival-time distributions for entangled multi-particle systems in the near-field regime, where a deeper analysis beyond semi-classical approximations is required. Even in the far-field regime, due to quantum non-locality, the semi-classical approximation cannot generally hold in multi-particle systems. Therefore, in this work, two fundamental approaches to the arrival-time problem -- namely, the time-parameter and time-operator approaches -- are extended to multi-particle systems. Using these extensions, we propose a feasible two-particle arrival-time experiment and numerically evaluate the corresponding joint distributions. Our results reveal regimes in which the two approaches yield inequivalent predictions, highlighting conditions under which experiments could shed new light on distinguishing between competing accounts of time in quantum mechanics. Our findings also provide important insights for the development of quantum technologies that use entanglement in the time domain, including non-local temporal interferometry, temporal ghost imaging, and temporal state tomography in multi-particle systems.

</details>


### [226] [Unraveling the Quantum Mpemba Effect on Markovian Open Quantum Systems](https://arxiv.org/abs/2512.13509)
*Rodrigo F. Saliba,Raphael C. Drumond*

Main category: quant-ph

TL;DR: 量子Mpemba效应（QME）是远离平衡的系统比更接近平衡的系统更快达到平衡的反直觉现象。本文从多个角度研究马尔可夫开放量子系统中的QME，包括提出基于无退相干子空间的物理机制、展示指数级增强的衰减速率、通过Davies映射解缠研究强Mpemba效应，以及提出微观模型深入理解浴动力学。


<details>
  <summary>Details</summary>
Motivation: 量子Mpemba效应作为经典Mpemba效应的量子推广，超越了温度平衡的概念，是一个引人入胜且反直觉的现象。近年来受到科学界的广泛关注，但对其在马尔可夫开放量子系统中的机制和特性仍需深入理解。

Method: 1. 提出基于无退相干子空间的物理机制；2. 展示系统尺度下指数增强的平衡衰减速率；3. 通过Davies映射的解缠研究强Mpemba效应；4. 提出微观模型深入理解浴动力学。

Result: 在马尔可夫开放量子系统中实现了QME的极端版本，获得了随系统尺寸指数级增强的平衡衰减速率。通过Davies映射解缠揭示了QME识别中品质因数选择的微妙之处，并通过微观模型提供了对浴动力学的深入理解。

Conclusion: 本文从多个角度深入探讨了马尔可夫开放量子系统中的量子Mpemba效应，不仅提出了新的物理机制，还展示了极端版本的QME现象，为理解这一反直觉量子现象提供了新的见解和方法。

Abstract: In recent years, the quantum Mpemba effect (QME), which occurs when an out-of-equilibrium system reaches equilibrium faster than another that is closer to equilibrium, has attracted significant attention from the scientific community as an intriguing and counterintuitive phenomenon. It generalizes its classical counterpart by extending the concept beyond temperature equilibration. This paper approaches the QME in Markovian open quantum systems from different perspectives. First, we propose a physical mechanism based on decoherence-free subspaces. Second, we show that an exponential enhancement of the decay rate toward equilibrium, scaling with system size, can be obtained, leading to an extreme version of the phenomenon in Markovian open quantum systems. Third, we study the strong Mpemba effect through the unravelings of Davies maps, revealing subtleties in the choice of figures of merit used to identify the QME. Finally, we propose a microscopic model to gain deeper insight into bath dynamics in this context.

</details>


### [227] [Multi-Photon Lasing Phenomena in Quantum Dot-Cavity QED](https://arxiv.org/abs/2512.13518)
*Lavakumar Addepalli*

Main category: quant-ph

TL;DR: 该论文研究了量子点-光子晶体腔QED系统中的多光子激光现象，通过极化子变换主方程处理激子-声子相互作用，推导了Scully-Lamb激光速率方程，并分析了多种多光子激光机制。


<details>
  <summary>Details</summary>
Motivation: 研究多光子激光现象在量子计算、量子传感、量子计量和量子通信等领域的应用潜力，探索量子点-光子晶体腔系统中激子-声子相互作用对多光子激光过程的影响。

Method: 采用极化子变换主方程处理激子-声子相互作用，使用Born-Markov近似获得约化密度矩阵速率方程，基于量子激光理论推导Scully-Lamb激光速率方程，计算单光子和多光子过剩发射率。

Result: 研究了多种多光子激光机制：合作双光子激光、相关发射激光、超辐射激光、非简并双模双光子激光，以及在开放量子系统中连续变量纠缠现象，涉及单/多个半导体量子点（二能级、三能级、四能级）与单/双模腔的相干/非相干耦合。

Conclusion: 该论文系统研究了量子点-光子晶体腔QED系统中的多光子激光现象，为量子技术应用提供了理论基础，展示了激子-声子相互作用在多光子激光过程中的重要作用。

Abstract: Multi-photon lasing has been realized in systems with strong nonlinear interactions between emitters and cavity modes, where single-photon processes are suppressed. Coherence between the internal states of a quantum emitter, or among multiple emitters, plays a key role. Such continuous nonclassical sources of light can find applications in quantum computation, quantum sensing, quantum metrology, and quantum communication.
  This thesis explores the multi-photon lasing phenomena in various quantum dot-photonic crystal cavity quantum electrodynamic (QED) setups. Exciton-phonon interactions are inevitable in such systems and are incorporated using the polaron-transformed master equation. The Born-Markov approximation is employed to obtain the reduced density matrix rate equation. Using quantum laser theory, we derived the Scully-Lamb laser rate equations and evaluated the single- and multi-photon excess emission rates defined as the difference between emission and absorption rates into the cavity mode without mean-field approximations. We investigated cooperative two-photon lasing, correlated emission lasing, hyperradiant lasing, non-degenerate two-mode two-photon lasing, and continuous variable entanglement in open quantum systems with single or multiple semiconductor quantum dots (two-level, three-level, and four-level) driven coherently/incoherently and coupled to single/ bimodal cavities.

</details>


### [228] [Pontryagin Maximum Principle for Rydberg-blockaded state-to-state transfers: A semi-analytic approach](https://arxiv.org/abs/2512.13549)
*Federico Alberto Astolfi,Sven Jandura,Guido Pupillo*

Main category: quant-ph

TL;DR: 研究基于里德堡阻塞机制的中性原子量子处理器中两比特和多比特操作的时间最优状态控制，通过哈密顿量块对角化简化动力学，应用庞特里亚金极大值原理推导最优激光控制。


<details>
  <summary>Details</summary>
Motivation: 中性原子量子处理器在里德堡阻塞机制下需要高效的时间最优控制来实现高保真度的量子操作，现有方法在解析与计算之间需要更好的平衡。

Method: 通过哈密顿量块对角化简化系统动力学，应用庞特里亚金极大值原理（PMP）的半解析方法推导最优激光控制，建立N比特通用形式化框架，对两比特系统分类正常和异常极值，将激光失谐映射为经典粒子在四次势中的运动。

Result: 建立了N比特通用形式化框架，对两比特系统完整分类了正常和异常极值，展示了异常解不存在或次优的情况，通过经典粒子运动类比简化了控制问题，结合PMP洞察和数值优化实现了高保真度时间最优控制。

Conclusion: 该方法成功桥接了量子控制中的解析和计算方法，为中性原子量子处理器在里德堡阻塞机制下提供了高效的时间最优控制方案，能够实现高保真度的多比特量子操作。

Abstract: We study time-optimal state-to-state control for two- and multi-qubit operations motivated by neutral-atom quantum processors within the Rydberg blockade regime. Block-diagonalization of the Hamiltonian simplifies the dynamics and enables the application of a semi-analytic approach to the Pontryagin Maximum Principle to derive optimal laser controls. We provide a general formalism for $N$ qubits. For $N=2$ qubits, we classify normal and abnormal extremals, showcasing examples where abnormal solutions are either absent or suboptimal. For normal extremals, we establish a correspondence between the laser detuning from atomic transitions and the motion of a classical particle in a quartic potential, yielding a reduced, semi-analytic formulation of the control problem. Combining PMP-based insights with numerical optimization, our approach bridges analytic and computational methods for high-fidelity, time-optimal control.

</details>


### [229] [Three-qubit entangling gates with simultaneous exchange controls in spin qubit systems](https://arxiv.org/abs/2512.13558)
*Miguel G. Rodriguez,Yun-Pil Shim*

Main category: quant-ph

TL;DR: 本文提出了一种多量子比特纠缠门操作新方法，通过同时驱动多对自旋量子比特之间的交换耦合，显著减少了量子电路所需的操作数量。


<details>
  <summary>Details</summary>
Motivation: 传统的基于成对交换耦合的自旋量子比特纠缠方法需要大量基本门操作序列，导致电路复杂且相干性降低。需要寻找更高效的纠缠机制来简化量子电路。

Method: 研究线性或三角形配置中的三自旋量子比特系统，推导多交换纠缠操作的解析表达式，通过优化控制参数利用生成的三量子比特门构建量子电路。

Result: 多量子比特策略显著减少了所需操作数量，能够生成标准纠缠态（如GHZ态和W态）以及Toffoli门，为自旋量子比特处理器提供了更高效、更浅层、更相干的电路实现途径。

Conclusion: 同时驱动多对自旋量子比特交换耦合的多量子比特纠缠门方法，相比传统的成对交换机制，能够大幅减少量子电路的操作复杂度，提高计算效率和相干性。

Abstract: Pairwise exchange couplings have long been the standard mechanism for entangling spin qubits in semiconductor systems. However, implementing quantum circuits based on pairwise exchange gates often requires a lengthy sequence of elementary gate operations. In this work, we present an alternative approach: multi-qubit entangling gate operations that simultaneously drive the exchange couplings between multiple pairs of spin qubits. We explore three spin qubit systems in linear or triangular configurations. We derive analytical expressions for these multi-exchange entangling operations and demonstrate how to use the resulting three-qubit gates to construct quantum circuits capable of generating standard entangled states such as GHZ and W states, and the Toffoli gate, by optimizing control parameters. Our results show that this multi-qubit strategy significantly reduces the number of required operations, offering a pathway to more efficient, shallower, and more coherent circuits for spin-qubit processors.

</details>


### [230] [Optimised Fermion-Qubit Encodings for Quantum Simulation with Reduced Transpiled Circuit Depth](https://arxiv.org/abs/2512.13580)
*Michael Williams de la Bastida,Thomas M. Bickley,Peter V. Coveney*

Main category: quant-ph

TL;DR: 提出一种确定性方法优化三元树编码，在不改变树结构的情况下降低泡利权重，减少量子电路深度


<details>
  <summary>Details</summary>
Motivation: 基于门的量子计算机模拟费米子哈密顿量需要选择编码方式，不同编码对量子电路和模拟结果有显著影响。现有三元树编码的非随机优化要么针对设备要么针对哈密顿量，需要一种更通用的优化方法

Method: 开发确定性方法优化三元树编码，保持底层树结构不变，无需辅助量子比特或额外交换门开销，适用于包括基于量子计算机连接图导出的各种编码

Result: 在STO-3G基组的水分子模拟中，相比包括Jordan-Wigner在内的标准编码方法，该方法平均减少未编译电路深度27.7%，编译后电路深度26.0%

Conclusion: 该方法能有效优化三元树编码，显著降低量子电路深度，为费米子哈密顿量模拟提供了更高效的编码优化方案

Abstract: Simulation of fermionic Hamiltonians with gate-based quantum computers requires the selection of an encoding from fermionic operators to quantum gates, the most widely used being the Jordan-Wigner transform. Many alternative encodings exist, with quantum circuits and simulation results being sensitive to choice of encoding, device connectivity and Hamiltonian characteristics. Non-stochastic optimisation of the ternary tree class of encodings to date has targeted either the device or Hamiltonian. We develop a deterministic method which optimises ternary tree encodings without changing the underlying tree structure. This enables reduction in Pauli-weight without ancillae or additional swap-gate overhead. We demonstrate this method for a variety of encodings, including those which are derived from the qubit connectivity graph of a quantum computer. Across a suite of standard encoding methods applied to water in STO-3G basis, including Jordan-Wigner, our method reduces qDRIFT circuit depths on average by $27.7\%$ and $26.0\%$ for untranspiled and transpiled circuits respectively.

</details>


### [231] [Quantum channel tomography and estimation by local test](https://arxiv.org/abs/2512.13614)
*Kean Chen,Nengkun Yu,Zhicheng Zhang*

Main category: quant-ph

TL;DR: 该论文研究了量子信道估计的查询复杂度，建立了对信道本身访问与对其随机扩张访问之间的等价关系，并给出了在不同参数条件下的最优查询复杂度界限。


<details>
  <summary>Details</summary>
Motivation: 研究量子信道估计的查询复杂度问题，特别是探索直接访问量子信道与访问其随机扩张之间的效率关系，旨在建立更优的量子信道层析方案。

Method: 通过构造局部测试器，证明对于并行（可能相干）测试器，访问扩张信道并不提供优势。具体构建了一个使用n次查询到原始信道的测试器，能够忠实地模拟使用n次查询到随机扩张的测试器。

Result: 1. 证明了$O(rd_1d_2/\varepsilon^2)$次查询足以实现钻石范数误差$\varepsilon$的信道层析；2. 当$rd_2=d_1$时，实现了海森堡标度$O(1/\varepsilon)$；3. 给出了混合态$\mathcal{E}(|0\rangle\langle 0|)$层析的查询复杂度界限。

Conclusion: 对于并行测试器，访问量子信道的随机扩张并不比直接访问原始信道提供优势，这一发现简化了量子信道估计的复杂性分析，并为不同参数条件下的最优查询复杂度提供了理论保证。

Abstract: We study the estimation of an unknown quantum channel $\mathcal{E}$ with input dimension $d_1$, output dimension $d_2$ and Kraus rank at most $r$. We establish a connection between the query complexities in two models: (i) access to $\mathcal{E}$, and (ii) access to a random dilation of $\mathcal{E}$. Specifically, we show that for parallel (possibly coherent) testers, access to dilations does not help. This is proved by constructing a local tester that uses $n$ queries to $\mathcal{E}$ yet faithfully simulates the tester with $n$ queries to a random dilation. As application, we show that:
  - $O(rd_1d_2/\varepsilon^2)$ queries to $\mathcal{E}$ suffice for channel tomography to within diamond norm error $\varepsilon$.
  Moreover, when $rd_2=d_1$, we show that the Heisenberg scaling $O(1/\varepsilon)$ can be achieved, even if $\mathcal{E}$ is not a unitary channel:
  - $O(\min\{d_1^{2.5}/\varepsilon,d_1^2/\varepsilon^2\})$ queries to $\mathcal{E}$ suffice for channel tomography to within diamond norm error $\varepsilon$, and $O(d_1^2/\varepsilon)$ queries suffice for the case of Choi state trace norm error $\varepsilon$.
  - $O(\min\{d_1^{1.5}/\varepsilon,d_1/\varepsilon^2\})$ queries to $\mathcal{E}$ suffice for tomography of the mixed state $\mathcal{E}(|0\rangle\langle 0|)$ to within trace norm error $\varepsilon$.

</details>


### [232] [Certified-Everlasting Quantum NIZK Proofs](https://arxiv.org/abs/2512.13628)
*Nikhil Pappu*

Main category: quant-ph

TL;DR: 该论文研究了具有统计可靠性、计算零知识和认证永恒零知识（CE-ZK）特性的非交互式零知识证明（NIZK），提出了在CRS模型和共享EPR模型中的CE-NIZK构造方案。


<details>
  <summary>Details</summary>
Motivation: 研究具有认证永恒零知识特性的非交互式零知识证明，使验证者能够撤销量子证明，并且撤销过程可以被证明者验证，同时保证在成功认证后验证者的状态可以被高效模拟。

Method: 1. 识别了在CRS模型中通过已知交互式证明的泛化来获得CE-NIZK的障碍；2. 通过使用满足特定特性的NIZK和单向函数，基于LWE假设构造了CRS模型中的CE-NIZK；3. 在共享EPR模型中，基于统计绑定的隐藏比特生成器构造了CE-NIZK，仅涉及单量子比特测量。

Result: 1. 发现了CRS模型中CE-NIZK构造的障碍；2. 基于LWE的多项式硬度假设，成功构造了CRS模型中的CE-NIZK；3. 在共享EPR模型中构造了CE-NIZK，仅需单量子比特测量操作。

Conclusion: 该研究成功构造了具有认证永恒零知识特性的非交互式零知识证明，分别在CRS模型和共享EPR模型中实现了CE-NIZK，为量子安全密码学提供了新的工具。

Abstract: We study non-interactive zero-knowledge proofs (NIZKs) for NP satisfying: 1) statistical soundness, 2) computational zero-knowledge and 3) certified-everlasting zero-knowledge (CE-ZK). The CE-ZK property allows a verifier of a quantum proof to revoke the proof in a way that can be checked (certified) by the prover. Conditioned on successful certification, the verifier's state can be efficiently simulated with only the statement, in a statistically indistinguishable way. Our contributions regarding these certified-everlasting NIZKs (CE-NIZKs) are as follows:
  - We identify a barrier to obtaining CE-NIZKs in the CRS model via generalizations of known interactive proofs that satisfy CE-ZK.
  - We circumvent this by constructing CE-NIZK from black-box use of NIZK for NP satisfying certain properties, along with OWFs. As a result, we obtain CE-NIZKs for NP in the CRS model, based on polynomial hardness of the learning with errors (LWE) assumption.
  - In addition, we observe that the aforementioned barrier does not apply to the shared EPR model. Consequently, we present a CE-NIZK for NP in this model based on any statistical binding hidden-bits generator, which can be based on LWE. The only quantum computation in this protocol involves single-qubit measurements of the shared EPR pairs.

</details>


### [233] [Quadratic and cubic scrambling in the estimation of two successive phase-shifts](https://arxiv.org/abs/2512.13640)
*Manju,Stefano Olivares,Matteo G. A. Paris*

Main category: quant-ph

TL;DR: 该研究探讨了在量子多参数估计中，通过非线性混洗操作来克服参数"松弛性"问题，提升不相容参数的联合估计精度。


<details>
  <summary>Details</summary>
Motivation: 量子多参数估计面临两个主要挑战：1）参数不相容性（对称对数导数不对易）；2）模型松弛性（参数组合导致Fisher信息矩阵退化或病态）。传统方法难以有效解决这些问题。

Method: 研究采用玻色子模型，包含两个相移参数。在参数编码之间引入非线性混洗操作，分析二阶和三阶非线性混洗的效果。使用两类探针态：压缩真空态和相干态。比较联合估计与分步估计策略。

Result: 非线性混洗能缓解松弛性、增加参数兼容性、提升整体估计精度。三阶非线性比二阶更有效（在固定探针和固定能量约束下）。对于相干探针，当非线性耦合足够大时，联合估计优于分步估计；对于压缩探针，仅在三阶非线性下观察到这种优势。

Conclusion: 非线性混洗操作是克服量子多参数估计中松弛性问题的有效策略，特别是三阶非线性混洗在提升不相容参数联合估计精度方面表现更优，为量子计量学提供了新工具。

Abstract: Multiparameter quantum estimation becomes challenging when the parameters are incompatible, i.e., when their respective symmetric logarithmic derivatives do not commute, or when the model is sloppy, meaning that the quantum probe depends only on combinations of parameters leading to a degenerate or ill-conditioned Fisher information matrix. In this work, we explore the use of scrambling operations between parameter encoding to overcome sloppiness. We consider a bosonic model with two phase-shift parameters and analyze the performance of second- and third-order nonlinear scrambling using two classes of probe states: squeezed vacuum states and coherent states. Our results demonstrate that nonlinear scrambling mitigates sloppiness, increases compatibility, and improves overall estimation precision. We find third-order nonlinearity to be more effective than second-order under both fixed-probe and fixed-energy constraints. Furthermore, by comparing joint estimation to a stepwise estimation strategy, we show that a threshold for nonlinear coupling exists. For coherent probes, joint estimation outperforms the stepwise strategy if the nonlinearity is sufficiently large, while for squeezed probes, this advantage is observed specifically with third-order nonlinearity.

</details>


### [234] [Matter-Mediated Entanglement in Classical Gravity: Suppression by Binding Potentials and Localization](https://arxiv.org/abs/2512.13675)
*Ziqian Tang,Chen Yang,Zizhao Han,Zikuan Kan,Yulong Liu,Hanyu Xue*

Main category: quant-ph

TL;DR: 该论文反驳了Aziz和Howl关于经典引力场下空间分离质量可以通过高阶"虚拟物质"过程纠缠的观点，指出其机制本质上是量子隧穿/衰减物质通道，且实际宏观物体的束缚势能会指数级抑制这种效应。


<details>
  <summary>Details</summary>
Motivation: 反驳Aziz和Howl在Nature上的观点，他们声称即使引力是经典场，空间分离的质量也能通过高阶"虚拟物质"过程纠缠。作者认为这一结论存在问题，需要澄清其物理本质和实际可行性。

Method: 通过分析指出AH提出的机制本质上是量子力学中的隧穿/衰减物质通道，而非场论特有。更重要的是，考虑实际宏观物体中微观成分被强势能束缚和局域化，引入大内能尺度会抑制相干传播。

Result: 束缚/局域化效应导致指数级抑制，使得在宏观分离距离下物质介导的贡献可忽略不计。因此AH识别的纠缠实际上诊断的是相干物质交换通道的存在，而非引力的经典或量子性质。

Conclusion: AH的纠缠机制不破坏LOCC基础的见证论证，在现实束缚物质平台中无效。纠缠检测到的是物质交换通道，不能用于区分引力的经典或量子本质。

Abstract: Aziz and Howl [Nature 646 (2025)] argue that two spatially separated masses can become entangled even when gravity is treated as a classical field, by invoking higher-order "virtual-matter" processes in a QFT description of matter, which is non-LOCC (local operations and classical communication). We point out that the relevant mechanism is not intrinsically field-theoretic, but is essentially a quantum tunneling/evanescent matter channel, which is already captured within ordinary quantum mechanics. More importantly, the microscopic constituents of realistic macroscopic objects are bound and localized by strong potentials, introducing a large internal energy scale that suppresses coherent propagation between distant bodies. Including such binding/localization generically yields an exponential suppression, rendering the matter-mediated contribution negligible at the macroscopic separations relevant to gravitational-entanglement proposals. Consequently, the entanglement identified by AH diagnoses the presence of a coherent matter-exchange channel rather than the classical or quantum nature of gravity, and it does not undermine LOCC-based witness arguments in realistic bound-matter platforms.

</details>


### [235] [Quantum oracles give an advantage for identifying classical counterfactuals](https://arxiv.org/abs/2512.13692)
*Ciarán M. Gilligan-Lee,Yìlè Yīng,Jonathan Richens,David Schmid*

Main category: quant-ph

TL;DR: 量子预言机在因果模型中回答经典反事实问题方面优于经典预言机，能够识别所有因果参数，而经典方法无法做到。


<details>
  <summary>Details</summary>
Motivation: 在结构因果模型中，观测数据和干预数据通常无法回答所有反事实问题，因为不同的因果参数可能产生相同的观测和干预数据，但在反事实上存在分歧。需要探索量子方法是否能够解决这一经典限制。

Method: 将经典变量编码到量子系统中，将因果依赖关系编码到量子预言机中，通过相干查询量子预言机来识别所有因果参数。从简单的二元例子扩展到任意有限基数，证明相干探测能够识别所有双向联合反事实概率。

Result: 1) 相干查询允许识别所有双向联合反事实概率p(Y_x=y, Y_{x'}=y')，这是任何数量的经典预言机查询都无法实现的；2) 量子方法对高阶多向反事实提供了比经典预言机更严格的界限。

Conclusion: 量子预言机在识别因果参数和回答反事实问题方面具有经典方法无法实现的优势。这种优势可能不完全依赖于量子上下文性等独特量子特征，因为在某些经典可解释理论（如Spekkens玩具理论）中也存在类似优势。

Abstract: We show that quantum oracles provide an advantage over classical oracles for answering classical counterfactual questions in causal models, or equivalently, for identifying unknown causal parameters such as distributions over functional dependences. In structural causal models with discrete classical variables, observational data and even ideal interventions generally fail to answer all counterfactual questions, since different causal parameters can reproduce the same observational and interventional data while disagreeing on counterfactuals. Using a simple binary example, we demonstrate that if the classical variables of interest are encoded in quantum systems and the causal dependence among them is encoded in a quantum oracle, coherently querying the oracle enables the identification of all causal parameters -- hence all classical counterfactuals. We generalize this to arbitrary finite cardinalities and prove that coherent probing 1) allows the identification of all two-way joint counterfactuals p(Y_x=y, Y_{x'}=y'), which is not possible with any number of queries to a classical oracle, and 2) provides tighter bounds on higher-order multi-way counterfactuals than with a classical oracle. This work can also be viewed as an extension to traditional quantum oracle problems such as Deutsch--Jozsa to identifying more causal parameters beyond just, e.g., whether a function is constant or balanced. Finally, we raise the question of whether this quantum advantage relies on uniquely non-classical features like contextuality. We provide some evidence against this by showing that in the binary case, oracles in some classically-explainable theories like Spekkens' toy theory also give rise to a counterfactual identifiability advantage over strictly classical oracles.

</details>
