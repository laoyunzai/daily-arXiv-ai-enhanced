<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 43]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 4]
- [cs.AI](#cs.AI) [Total: 11]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 13]
- [quant-ph](#quant-ph) [Total: 33]
- [nlin.CD](#nlin.CD) [Total: 1]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs](https://arxiv.org/abs/2601.16979)
*Dayal Singh Kalra,Jean-Christophe Gagnon-Audet,Andrey Gromov,Ishita Mediratta,Kelvin Niu,Alexander H Miller,Michael Shvartsman*

Main category: cs.LG

TL;DR: The paper introduces "critical sharpness" - a computationally efficient alternative to Hessian sharpness for measuring loss landscape curvature in LLMs. It requires <10 forward passes and successfully captures key phenomena like progressive sharpening and Edge of Stability at scale (up to 7B parameters), providing a practical tool for training diagnostics and data composition decisions.


<details>
  <summary>Details</summary>
Motivation: Direct measurement of Hessian sharpness is computationally prohibitive for Large Language Models, yet understanding loss landscape curvature evolution is fundamental to analyzing training dynamics.

Method: Proposed "critical sharpness" (λ_c) - a measure requiring fewer than 10 forward passes given update direction Δθ. Also introduced "relative critical sharpness" (λ_c^{1→2}) to analyze transitions between pre-training and fine-tuning phases.

Result: First demonstration of sharpness phenomena (progressive sharpening and Edge of Stability) at scale up to 7B parameters on OLMo-2 models. The measure effectively captures Hessian sharpness phenomena and can guide data mixing strategies.

Conclusion: Critical sharpness provides practitioners with a practical, scalable tool for diagnosing curvature dynamics and informing data composition choices in large-scale training.

Abstract: Understanding the curvature evolution of the loss landscape is fundamental to analyzing the training dynamics of neural networks. The most commonly studied measure, Hessian sharpness ($λ_{\max}^H$) -- the largest eigenvalue of the loss Hessian -- determines local training stability and interacts with the learning rate throughout training. Despite its significance in analyzing training dynamics, direct measurement of Hessian sharpness remains prohibitive for Large Language Models (LLMs) due to high computational cost. We analyze $\textit{critical sharpness}$ ($λ_c$), a computationally efficient measure requiring fewer than $10$ forward passes given the update direction $Δ\mathbfθ$. Critically, this measure captures well-documented Hessian sharpness phenomena, including progressive sharpening and Edge of Stability. Using this measure, we provide the first demonstration of these sharpness phenomena at scale, up to $7$B parameters, spanning both pre-training and mid-training of OLMo-2 models. We further introduce $\textit{relative critical sharpness}$ ($λ_c^{1\to 2}$), which quantifies the curvature of one loss landscape while optimizing another, to analyze the transition from pre-training to fine-tuning and guide data mixing strategies. Critical sharpness provides practitioners with a practical tool for diagnosing curvature dynamics and informing data composition choices at scale. More broadly, our work shows that scalable curvature measures can provide actionable insights for large-scale training.

</details>


### [2] [Ordering-based Causal Discovery via Generalized Score Matching](https://arxiv.org/abs/2601.16249)
*Vy Vo,He Zhao,Trung Le,Edwin V. Bonilla,Dinh Phung*

Main category: cs.LG

TL;DR: 该论文将连续数据的得分匹配框架扩展到离散数据因果发现，提出基于离散得分函数的新型叶节点判别准则，通过模拟和真实实验验证其能准确推断因果顺序并显著提升现有方法性能。


<details>
  <summary>Details</summary>
Motivation: 从纯观测数据学习DAG结构是科学领域的长期挑战。现有研究利用得分匹配识别连续数据的拓扑序，但缺乏针对离散数据的有效扩展。

Method: 提出基于离散得分函数的叶节点判别准则，扩展得分匹配框架以处理离散数据，先识别因果拓扑序再进行边剪枝。

Result: 模拟和真实实验表明，该方法能从离散观测数据中准确推断真实因果顺序，并在几乎所有设置下显著提升现有因果发现基线的准确性。

Conclusion: 离散得分匹配理论有效解决了离散数据因果发现问题，为提升因果结构学习性能提供了新范式。

Abstract: Learning DAG structures from purely observational data remains a long-standing challenge across scientific domains. An emerging line of research leverages the score of the data distribution to initially identify a topological order of the underlying DAG via leaf node detection and subsequently performs edge pruning for graph recovery. This paper extends the score matching framework for causal discovery, which is originally designated for continuous data, and introduces a novel leaf discriminant criterion based on the discrete score function. Through simulated and real-world experiments, we demonstrate that our theory enables accurate inference of true causal orders from observed discrete data and the identified ordering can significantly boost the accuracy of existing causal discovery baselines on nearly all of the settings.

</details>


### [3] [Student Mental Health Screening via Fitbit Data Collected During the COVID-19 Pandemic](https://arxiv.org/abs/2601.16324)
*Rebecca Lopez,Avantika Shrestha,ML Tlachac,Kevin Hickey,Xingtong Guo,Shichao Liu,Elke Rundensteiner*

Main category: cs.LG

TL;DR: 该研究利用Fitbit可穿戴设备数据，通过机器学习模型对大学生抑郁、焦虑和压力进行筛查，发现心率和睡眠等生理指标具有较高预测潜力，F1分数最高达0.79。


<details>
  <summary>Details</summary>
Motivation: 大学生面临高压力导致焦虑抑郁普遍，现有研究在心理评估工具多样性、生理指标类型及时间序列参数方面存在局限，需探索可穿戴设备在心理健康早期检测中的应用。

Method: 收集疫情期间大学生Fitbit数据集（StudentMEH），采用不同生理模态（如心率、睡眠）构建机器学习预测模型，评估其对抑郁、焦虑和压力的筛查能力。

Result: 心率模态对焦虑筛查F1达0.79，对压力筛查达0.77；睡眠模态对抑郁筛查达0.78，证实生理数据在心理健康监测中的有效性，且不同心理问题需匹配最佳数据聚合层级与模态。

Conclusion: 可穿戴设备支持连续心理健康监测具有潜力，未来需优化数据聚合方式与模态选择以提升不同心理问题的筛查精度，为校园心理健康干预提供新工具。

Abstract: College students experience many stressors, resulting in high levels of anxiety and depression. Wearable technology provides unobtrusive sensor data that can be used for the early detection of mental illness. However, current research is limited concerning the variety of psychological instruments administered, physiological modalities, and time series parameters. In this research, we collect the Student Mental and Environmental Health (StudentMEH) Fitbit dataset from students at our institution during the pandemic. We provide a comprehensive assessment of the ability of predictive machine learning models to screen for depression, anxiety, and stress using different Fitbit modalities. Our findings indicate potential in physiological modalities such as heart rate and sleep to screen for mental illness with the F1 scores as high as 0.79 for anxiety, the former modality reaching 0.77 for stress screening, and the latter modality achieving 0.78 for depression. This research highlights the potential of wearable devices to support continuous mental health monitoring, the importance of identifying best data aggregation levels and appropriate modalities for screening for different mental ailments.

</details>


### [4] [Efficient Gaussian process learning via subspace projections](https://arxiv.org/abs/2601.16332)
*Felipe Tobar,Elsa Cazelles*

Main category: cs.LG

TL;DR: 提出基于数据低维线性投影的高斯过程训练目标（投影似然PL），通过随机球面投影减少信息损失，在中等规模数据集上比精确GP和稀疏GP变分方法更准确高效


<details>
  <summary>Details</summary>
Motivation: 解决传统高斯过程（GP）在大规模数据上计算效率低的问题，同时改善现有稀疏GP方法（如变分自由能）的准确性局限

Method: 构建基于数据低维线性投影的投影似然（PL）目标函数，推导其信息损失的闭式解，并采用单位球面随机投影降低信息损失

Result: 在多种优化器、核函数和中等规模数据集上，PL方法相比精确GP和稀疏GP变分方法，同时实现更高准确性和更低计算复杂度

Conclusion: 投影似然方法为大规模高斯过程训练提供了高效且准确的解决方案，具有实际应用价值

Abstract: We propose a novel training objective for GPs constructed using lower-dimensional linear projections of the data, referred to as \emph{projected likelihood} (PL). We provide a closed-form expression for the information loss related to the PL and empirically show that it can be reduced with random projections on the unit sphere. We show the superiority of the PL, in terms of accuracy and computational efficiency, over the exact GP training and the variational free energy approach to sparse GPs over different optimisers, kernels and datasets of moderately large sizes.

</details>


### [5] [A Regularized Actor-Critic Algorithm for Bi-Level Reinforcement Learning](https://arxiv.org/abs/2601.16399)
*Sihan Zeng,Sujay Bhatt,Sumitra Ganesh,Alec Koppel*

Main category: cs.LG

TL;DR: 提出一种单回路一阶演员-评论家算法解决双层优化问题，通过衰减熵正则化实现无偏高阶梯度估计，并在Polyak-Lojasiewicz条件下证明收敛到驻点


<details>
  <summary>Details</summary>
Motivation: 现有双层优化与强化学习方法存在需二阶信息、强正则化或嵌套循环效率低等问题，难以有效处理上层变量参数化下层MDP奖励的场景

Method: 设计单回路一阶演员-评论家算法，采用惩罚重构形式，在RL目标中引入衰减熵正则化，避免精确求解未正则化问题即可实现渐近无偏的超梯度估计

Result: 在特殊Polyak-Lojasiewicz条件下，通过新颖的下层残差分析，证明算法对原始未正则化双层问题具有有限时间与样本收敛性，可收敛至驻点

Conclusion: 理论结果有效，GridWorld与RLHF生成快乐推文实验验证了方法性能，为双层优化提供高效解决方案

Abstract: We study a structured bi-level optimization problem where the upper-level objective is a smooth function and the lower-level problem is policy optimization in a Markov decision process (MDP). The upper-level decision variable parameterizes the reward of the lower-level MDP, and the upper-level objective depends on the optimal induced policy. Existing methods for bi-level optimization and RL often require second-order information, impose strong regularization at the lower level, or inefficiently use samples through nested-loop procedures. In this work, we propose a single-loop, first-order actor-critic algorithm that optimizes the bi-level objective via a penalty-based reformulation. We introduce into the lower-level RL objective an attenuating entropy regularization, which enables asymptotically unbiased upper-level hyper-gradient estimation without solving the unregularized RL problem exactly. We establish the finite-time and finite-sample convergence of the proposed algorithm to a stationary point of the original, unregularized bi-level optimization problem through a novel lower-level residual analysis under a special type of Polyak-Lojasiewicz condition. We validate the performance of our method through experiments on a GridWorld goal position problem and on happy tweet generation through reinforcement learning from human feedback (RLHF).

</details>


### [6] [Towards a Theoretical Understanding to the Generalization of RLHF](https://arxiv.org/abs/2601.16403)
*Zhaochun Li,Mingyang Yi,Yue Wang,Shisheng Cui,Yong Liu*

Main category: cs.LG

TL;DR: This paper develops a theoretical generalization framework for RLHF in LLMs using algorithmic stability, proving that under a feature coverage condition, policy models have a generalization bound of O(n^(-1/2)), which extends to gradient-based optimization and explains why RLHF works well in practice.


<details>
  <summary>Details</summary>
Motivation: RLHF is empirically effective for aligning LLMs with human intent, but its theoretical generalization properties in high-dimensional settings remain unexplored. Existing theoretical works focus on consistency of reward model MLE, not the end-to-end learning framework used in practice.

Method: The authors build a generalization theory for RLHF under a linear reward model using the algorithmic stability framework, analyzing the empirical optima of the policy model under an end-to-end learning setting.

Result: Under a key feature coverage condition, the empirical optima of the policy model have a generalization bound of order O(n^(-1/2)). This result extends to parameters obtained by gradient-based learning algorithms (Gradient Ascent and Stochastic Gradient Ascent).

Conclusion: The theoretical analysis provides new evidence for the empirically observed generalization of LLMs after RLHF, bridging the gap between theory and practice in alignment methods.

Abstract: Reinforcement Learning from Human Feedback (RLHF) and its variants have emerged as the dominant approaches for aligning Large Language Models with human intent. While empirically effective, the theoretical generalization properties of these methods in high-dimensional settings remain to be explored. To this end, we build the generalization theory on RLHF of LLMs under the linear reward model, through the framework of algorithmic stability. In contrast to the existing works built upon the consistency of maximum likelihood estimations on reward model, our analysis is presented under an end-to-end learning framework, which is consistent with practice. Concretely, we prove that under a key \textbf{feature coverage} condition, the empirical optima of policy model have a generalization bound of order $\mathcal{O}(n^{-\frac{1}{2}})$. Moreover, the results can be extrapolated to parameters obtained by gradient-based learning algorithms, i.e., Gradient Ascent (GA) and Stochastic Gradient Ascent (SGA). Thus, we argue that our results provide new theoretical evidence for the empirically observed generalization of LLMs after RLHF.

</details>


### [7] [Reasoning-Enhanced Rare-Event Prediction with Balanced Outcome Correction](https://arxiv.org/abs/2601.16406)
*Vitaly Bulgakov,Alexander Turchin*

Main category: cs.LG

TL;DR: 本文提出LPCORP，一种两阶段框架，结合增强推理预测和基于置信度的校正，在不进行重采样的情况下解决不平衡数据集中的罕见事件预测问题，性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 罕见事件预测在医疗、金融等高影响领域至关重要，但传统模型因类别不平衡而偏向多数类预测，限制了召回率、校准性和实际应用价值。

Method: 两阶段框架：第一阶段使用推理模型从文本输入生成增强预测；第二阶段使用轻量级逻辑回归分类器基于置信度选择性地校正输出，缓解流行率偏差。

Result: 该方法将高度不平衡场景转化为平衡场景，保持样本数量不变，显著提升精确率，在某些情况下预防干预成本降低超过50%。

Conclusion: LPCORP有效解决了罕见事件预测中的类别不平衡问题，通过提升精确率和实现显著的成本节约展示了实用价值。

Abstract: Rare-event prediction is critical in domains such as healthcare, finance, reliability engineering, customer support, aviation safety, where positive outcomes are infrequent yet potentially catastrophic. Extreme class imbalance biases conventional models toward majority-class predictions, limiting recall, calibration, and operational usefulness. We propose LPCORP (Low-Prevalence CORrector for Prediction)*, a two-stage framework that combines reasoningenhanced prediction with confidence-based outcome correction. A reasoning model first produces enriched predictions from narrative inputs, after which a lightweight logistic-regression classifier evaluates and selectively corrects these outputs to mitigate prevalence-driven bias. We evaluate LPCORP on real-world datasets from medical and consumer service domains. The results show that this method transforms a highly imbalanced setting into a well-balanced one while preserving the original number of samples and without applying any resampling strategies. Test-set evaluation demonstrates substantially improved performance, particularly in precision, which is a known weakness in low-prevalence data. We further provide a costreduction analysis comparing the expenses associated with rare-event damage control without preventive measures to those incurred when low-cost, prediction-based preventive interventions are applied that showed more than 50% reduction in some cases. * Patent pending: U.S. Provisional 63/933,518, filed 8 December 2025.

</details>


### [8] [A Refinement of Vapnik--Chervonenkis' Theorem](https://arxiv.org/abs/2601.16411)
*A. Iosevich,A. Vagharshakyan,E. Wyman*

Main category: cs.LG

TL;DR: 该论文通过用Berry-Esseen误差控制的正态近似替代Hoeffding不等式，改进了VC定理的概率边界，得到了中等偏差情形下的更精确估计。


<details>
  <summary>Details</summary>
Motivation: 经典VC定理在最终步骤使用Hoeffding不等式，但作者希望采用具有显式误差控制的正态近似来获得更精确的边界。

Method: 用Berry-Esseen误差控制的正态近似替代Hoeffding不等式，重新分析VC定理的概率成分。

Result: 得到了中等偏差情形下VC估计的 sharpening，当ε√n较大时，在主导指数项中增加了(ε√n)⁻¹量级的额外因子。

Conclusion: 该方法为VC定理提供了中等偏差情形下的更精确边界，改善了经典结果的收敛速率估计。

Abstract: Vapnik--Chervonenkis' theorem is a seminal result in machine learning. It establishes sufficient conditions for empirical probabilities to converge to theoretical probabilities, uniformly over families of events. It also provides an estimate for the rate of such uniform convergence.
  We revisit the probabilistic component of the classical argument. Instead of applying Hoeffding's inequality at the final step, we use a normal approximation with explicit Berry--Esseen error control. This yields a moderate-deviation sharpening of the usual VC estimate, with an additional factor of order $(\varepsilon\sqrt{n})^{-1}$ in the leading exponential term when $\varepsilon\sqrt{n}$ is large.

</details>


### [9] [PyHealth 2.0: A Comprehensive Open-Source Toolkit for Accessible and Reproducible Clinical Deep Learning](https://arxiv.org/abs/2601.16414)
*John Wu,Yongda Fan,Zhenbang Wu,Paul Landes,Eric Schrock,Sayeed Sajjad Razin,Arjun Chatterjee,Naveen Baskaran,Joshua Steier,Andrea Fitzpatrick,Bilal Arif,Rian Atri,Jathurshan Pradeepkumar,Siddhartha Laghuvarapu,Junyi Gao,Adam R. Cross,Jimeng Sun*

Main category: cs.LG

TL;DR: PyHealth 2.0是一个临床深度学习工具包，可将代码减少至7行，提供39倍加速和20倍内存降低，统一了15+数据集和25+模型，并建立400+成员社区，旨在使医疗AI更易访问和可复现。


<details>
  <summary>Details</summary>
Motivation: 临床AI研究面临基线复现困难、计算成本高和领域专业知识门槛高的持续障碍。

Method: 开发PyHealth 2.0工具包，通过统一15+数据集、20+临床任务、25+模型、5+可解释性方法和不确定性量化，支持多模态临床数据（信号、影像、电子健康记录）并翻译5+医疗编码标准；采用注重可访问性的设计，适应不同计算资源；建立活跃的开源社区。

Result: 实现仅需7行代码的预测建模；处理速度提升高达39倍；内存使用降低高达20倍；支持从16GB笔记本电脑到生产系统的运行；拥有400+成员的开源社区；通过RHealth提供多语言支持；可通过pip install pyhealth安装。

Conclusion: PyHealth 2.0建立了开源基础和社区，推动可访问、可复现的医疗AI发展。

Abstract: Difficulty replicating baselines, high computational costs, and required domain expertise create persistent barriers to clinical AI research. To address these challenges, we introduce PyHealth 2.0, an enhanced clinical deep learning toolkit that enables predictive modeling in as few as 7 lines of code. PyHealth 2.0 offers three key contributions: (1) a comprehensive toolkit addressing reproducibility and compatibility challenges by unifying 15+ datasets, 20+ clinical tasks, 25+ models, 5+ interpretability methods, and uncertainty quantification including conformal prediction within a single framework that supports diverse clinical data modalities - signals, imaging, and electronic health records - with translation of 5+ medical coding standards; (2) accessibility-focused design accommodating multimodal data and diverse computational resources with up to 39x faster processing and 20x lower memory usage, enabling work from 16GB laptops to production systems; and (3) an active open-source community of 400+ members lowering domain expertise barriers through extensive documentation, reproducible research contributions, and collaborations with academic health systems and industry partners, including multi-language support via RHealth. PyHealth 2.0 establishes an open-source foundation and community advancing accessible, reproducible healthcare AI. Available at pip install pyhealth.

</details>


### [10] [Bayesian Experimental Design for Model Discrepancy Calibration: A Rivalry between Kullback--Leibler Divergence and Wasserstein Distance](https://arxiv.org/abs/2601.16425)
*Huchen Yang,Xinghao Dong,Jin-Long Wu*

Main category: cs.LG

TL;DR: 该论文比较了贝叶斯实验设计中KL散度和Wasserstein距离作为效用函数的优劣，发现KL散度在无模型误差时收敛更快，而Wasserstein距离在存在模型误差时更稳健，且Wasserstein距离存在位置依赖性问题。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯实验设计(BED)需要选择合适的效用函数来量化信息增益，KL散度是常用选择但Wasserstein距离作为替代方案存在争议，需要系统比较两者的权衡关系。

Method: 通过玩具示例揭示Wasserstein距离的问题，然后在经典源反演问题上进行系统对比实验，分析两种度量在无模型误差和存在模型误差情况下的表现。

Result: Wasserstein距离的值依赖于后验分布主质量在支撑区域内的相对位置，可能导致与真实信息增益无关的虚假奖励；KL散度在无模型误差时收敛更快；Wasserstein度量在模型误差显著时提供更稳健的序贯BED结果。

Conclusion: 研究明确了KL散度和Wasserstein度量作为效用函数的权衡关系，为实际BED应用中如何选择合适标准提供了指导依据。

Abstract: Designing experiments that systematically gather data from complex physical systems is central to accelerating scientific discovery. While Bayesian experimental design (BED) provides a principled, information-based framework that integrates experimental planning with probabilistic inference, the selection of utility functions in BED is a long-standing and active topic, where different criteria emphasize different notions of information. Although Kullback--Leibler (KL) divergence has been one of the most common choices, recent studies have proposed Wasserstein distance as an alternative. In this work, we first employ a toy example to illustrate an issue of Wasserstein distance - the value of Wasserstein distance of a fixed-shape posterior depends on the relative position of its main mass within the support and can exhibit false rewards unrelated to information gain, especially with a non-informative prior (e.g., uniform distribution). We then further provide a systematic comparison between these two criteria through a classical source inversion problem in the BED literature, revealing that the KL divergence tends to lead to faster convergence in the absence of model discrepancy, while Wasserstein metrics provide more robust sequential BED results if model discrepancy is non-negligible. These findings clarify the trade-offs between KL divergence and Wasserstein metrics for the utility function and provide guidelines for selecting suitable criteria in practical BED applications.

</details>


### [11] [Safe Multitask Molecular Graph Networks for Vapor Pressure and Odor Threshold Prediction](https://arxiv.org/abs/2601.16426)
*Shuang Wu,Meijie Wang,Lun Yu*

Main category: cs.LG

TL;DR: This paper develops graph neural network models for predicting vapor pressure and odor threshold of molecules, introducing a "safe multitask" learning approach that prevents negative transfer and achieves strong out-of-distribution generalization on scaffold-split data.


<details>
  <summary>Details</summary>
Motivation: Odor-related property modeling (vapor pressure and odor threshold) is important for applications like fragrance design and safety assessment, but faces challenges in out-of-distribution generalization and multitask learning where auxiliary tasks may harm primary task performance.

Method: Uses Bemis-Murcko scaffold splitting for OOD evaluation. Employs A20/E17 molecular graph features (20-dim atom + 17-dim bond features) with GINE and PNA GNN backbones. Tests robust training techniques (Huber loss, winsorization). Proposes "safe multitask" learning with delayed activation, gradient clipping, and small auxiliary task weights.

Result: PNA achieves best VP performance (Val MSE ≈ 0.21 normalized). OP single task with robust training reaches Val MSE ≈ 0.60-0.61. The safe multitask approach successfully prevents negative transfer, yields best VP generalization, and effectively uses OP as auxiliary task.

Conclusion: The study provides robust baseline models for odor property prediction, demonstrates the importance of proper evaluation splits and robust training, and introduces an effective safe multitask learning strategy. Complete reproducibility and thorough analysis of limitations are provided.

Abstract: We investigate two important tasks in odor-related property modeling: Vapor Pressure (VP) and Odor Threshold (OP). To evaluate the model's out-of-distribution (OOD) capability, we adopt the Bemis-Murcko scaffold split. In terms of features, we introduce the rich A20/E17 molecular graph features (20-dimensional atom features + 17-dimensional bond features) and systematically compare GINE and PNA backbones. The results show: for VP, PNA with a simple regression head achieves Val MSE $\approx$ 0.21 (normalized space); for the OP single task under the same scaffold split, using A20/E17 with robust training (Huber/winsor) achieves Val MSE $\approx$ 0.60-0.61. For multitask training, we propose a **"safe multitask"** approach: VP as the primary task and OP as the auxiliary task, using delayed activation + gradient clipping + small weight, which avoids harming the primary task and simultaneously yields the best VP generalization performance. This paper provides complete reproducible experiments, ablation studies, and error-similarity analysis while discussing the impact of data noise and method limitations.

</details>


### [12] [Endless Terminals: Scaling RL Environments for Terminal Agents](https://arxiv.org/abs/2601.16443)
*Kanishk Gandhi,Shivam Garg,Noah D. Goodman,Dimitris Papailiopoulos*

Main category: cs.LG

TL;DR: 本文提出 Endless Terminals，一个完全自主的终端任务生成管道，通过程序化生成3255个任务，用简单的PPO算法训练模型，在终端任务上取得显著提升，证明环境规模化后简单强化学习即可成功。


<details>
  <summary>Details</summary>
Motivation: 环境是自改进智能体的瓶颈。现有终端基准测试仅用于评估而非训练，强化学习需要可扩展的管道而不仅仅是数据集。

Method: 提出 Endless Terminals 管道，包含四个阶段：生成多样化任务描述、构建验证容器化环境、生成完成测试、过滤可解性任务。用该管道生成3255个任务，使用 vanilla PPO 算法和极简交互循环（无检索、多智能体协调或专用工具）训练智能体。

Result: 在内部验证集上，多个模型性能显著提升（如 Llama-3.2-3B 从4.0%提升至18.2%）。这些提升可迁移至人类精选的 TerminalBench 2.0 基准测试，且性能优于包含更复杂智能体框架的模型。

Conclusion: 当环境具备可扩展性时，简单的强化学习即可取得成功。

Abstract: Environments are the bottleneck for self-improving agents. Current terminal benchmarks were built for evaluation, not training; reinforcement learning requires a scalable pipeline, not just a dataset. We introduce Endless Terminals, a fully autonomous pipeline that procedurally generates terminal-use tasks without human annotation. The pipeline has four stages: generating diverse task descriptions, building and validating containerized environments, producing completion tests, and filtering for solvability. From this pipeline we obtain 3255 tasks spanning file operations, log management, data processing, scripting, and database operations. We train agents using vanilla PPO with binary episode level rewards and a minimal interaction loop: no retrieval, multi-agent coordination, or specialized tools. Despite this simplicity, models trained on Endless Terminals show substantial gains: on our held-out dev set, Llama-3.2-3B improves from 4.0% to 18.2%, Qwen2.5-7B from 10.7% to 53.3%, and Qwen3-8B-openthinker-sft from 42.6% to 59.0%. These improvements transfer to human-curated benchmarks: models trained on Endless Terminals show substantial gains on held out human curated benchmarks: on TerminalBench 2.0, Llama-3.2-3B improves from 0.0% to 2.2%, Qwen2.5-7B from 2.2% to 3.4%, and Qwen3-8B-openthinker-sft from 1.1% to 6.7%, in each case outperforming alternative approaches including models with more complex agentic scaffolds. These results demonstrate that simple RL succeeds when environments scale.

</details>


### [13] [On the Expressive Power of Floating-Point Transformers](https://arxiv.org/abs/2601.16450)
*Sejun Park,Yeachan Park,Geonho Hwang*

Main category: cs.LG

TL;DR: This paper investigates how floating-point arithmetic affects transformer expressive power, showing they can represent non-permutation-equivariant functions and have limitations in representing all permutation-equivariant functions with large sequences, while additive positional encoding can harm their representability.


<details>
  <summary>Details</summary>
Motivation: Theoretical results show transformers are permutation equivariant and can approximate all permutation-equivariant continuous functions under exact real operations, but real computer implementations use finite floating-point numbers with round-off errors, creating a gap between theory and practice that needs investigation.

Method: The authors theoretically analyze the representability of transformers using floating-point parameters and operations, examining their ability to represent permutation-equivariant and non-permutation-equivariant functions under bounded vs. unbounded sequence lengths.

Result: 1. Floating-point transformers can represent non-permutation-equivariant functions without positional encoding
2. They can represent all permutation-equivariant functions when sequence length is bounded, but fail when sequence length is large
3. They identified the minimal equivariance structure in floating-point transformers
4. Non-trivial additive positional encoding harms floating-point transformer representability

Conclusion: The study reveals fundamental differences between ideal theoretical transformers and practical floating-point implementations, showing that floating-point arithmetic both expands representational capacity (allowing non-equivariant functions) and imposes limitations (on large sequences), while suggesting that positional encoding design needs careful reconsideration in practice.

Abstract: The study on the expressive power of transformers shows that transformers are permutation equivariant, and they can approximate all permutation-equivariant continuous functions on a compact domain. However, these results are derived under real parameters and exact operations, while real implementations on computers can only use a finite set of numbers and inexact machine operations with round-off errors. In this work, we investigate the representability of floating-point transformers that use floating-point parameters and floating-point operations. Unlike existing results under exact operations, we first show that floating-point transformers can represent a class of non-permutation-equivariant functions even without positional encoding. Furthermore, we prove that floating-point transformers can represent all permutation-equivariant functions when the sequence length is bounded, but they cannot when the sequence length is large. We also found the minimal equivariance structure in floating-point transformers, and show that all non-trivial additive positional encoding can harm the representability of floating-point transformers.

</details>


### [14] [On the Effects of Adversarial Perturbations on Distribution Robustness](https://arxiv.org/abs/2601.16464)
*Yipei Wang,Zhaoying Pan,Xiaoqian Wang*

Main category: cs.LG

TL;DR: This paper theoretically analyzes the tradeoff between adversarial and distribution robustness, discovering that ℓ∞ perturbations on moderately biased data can actually improve distribution robustness when feature separability is high.


<details>
  <summary>Details</summary>
Motivation: Prior work reveals a tradeoff between adversarial robustness (resisting input perturbations) and distribution robustness (handling data shifts), where adversarial training may increase reliance on spurious features and harm performance on underrepresented subgroups.

Method: The authors conduct a theoretical analysis that provides a tractable surrogate for per-step adversarial training by studying models trained on perturbed data.

Result: Beyond confirming the tradeoff, the work identifies that ℓ∞ perturbations on data with moderate bias can increase distribution robustness, and this gain persists on highly skewed data when simplicity bias induces reliance on core features with greater feature separability.

Conclusion: While the tradeoff persists in many cases, overlooking feature separability may lead to misleading conclusions about robustness, extending our understanding of the interplay between these two robustness concepts.

Abstract: Adversarial robustness refers to a model's ability to resist perturbation of inputs, while distribution robustness evaluates the performance of the model under data shifts. Although both aim to ensure reliable performance, prior work has revealed a tradeoff in distribution and adversarial robustness. Specifically, adversarial training might increase reliance on spurious features, which can harm distribution robustness, especially the performance on some underrepresented subgroups. We present a theoretical analysis of adversarial and distribution robustness that provides a tractable surrogate for per-step adversarial training by studying models trained on perturbed data. In addition to the tradeoff, our work further identified a nuanced phenomenon that $\ell_\infty$ perturbations on data with moderate bias can yield an increase in distribution robustness. Moreover, the gain in distribution robustness remains on highly skewed data when simplicity bias induces reliance on the core feature, characterized as greater feature separability. Our theoretical analysis extends the understanding of the tradeoff by highlighting the interplay of the tradeoff and the feature separability. Despite the tradeoff that persists in many cases, overlooking the role of feature separability may lead to misleading conclusions about robustness.

</details>


### [15] [A Cautionary Tale of Self-Supervised Learning for Imaging Biomarkers: Alzheimer's Disease Case Study](https://arxiv.org/abs/2601.16467)
*Maxwell Reynolds,Chaitanya Srinivasan,Vijay Cherupally,Michael Leone,Ke Yu,Li Sun,Tigmanshu Chaudhary,Andreas Pfenning,Kayhan Batmanghelich*

Main category: cs.LG

TL;DR: A novel self-supervised learning framework (R-NCE) discovers more powerful Alzheimer's disease biomarkers from structural MRI than traditional methods, showing high heritability and biological relevance to neurodegenerative processes.


<details>
  <summary>Details</summary>
Motivation: Current structural MRI biomarkers for Alzheimer's disease (e.g., cortical thickness) rely on hand-crafted features and may be suboptimal. Self-supervised learning (SSL) has not previously outperformed these traditional features.

Method: Introduced Residual Noise Contrastive Estimation (R-NCE), a new SSL framework that integrates auxiliary FreeSurfer features while learning augmentation-invariant representations from MRI data.

Result: R-NCE outperforms traditional features and existing SSL methods in AD classification, conversion prediction, and amyloid status prediction. The derived Brain Age Gap (BAG) measure shows high heritability and genetic associations with MAPT and IRAG1.

Conclusion: R-NCE successfully uncovers biologically grounded biomarkers sensitive to neurodegenerative and cerebrovascular processes, demonstrating the value of integrating SSL with domain knowledge for medical imaging analysis.

Abstract: Discovery of sensitive and biologically grounded biomarkers is essential for early detection and monitoring of Alzheimer's disease (AD). Structural MRI is widely available but typically relies on hand-crafted features such as cortical thickness or volume. We ask whether self-supervised learning (SSL) can uncover more powerful biomarkers from the same data. Existing SSL methods underperform FreeSurfer-derived features in disease classification, conversion prediction, and amyloid status prediction. We introduce Residual Noise Contrastive Estimation (R-NCE), a new SSL framework that integrates auxiliary FreeSurfer features while maximizing additional augmentation-invariant information. R-NCE outperforms traditional features and existing SSL methods across multiple benchmarks, including AD conversion prediction. To assess biological relevance, we derive Brain Age Gap (BAG) measures and perform genome-wide association studies. R-NCE-BAG shows high heritability and associations with MAPT and IRAG1, with enrichment in astrocytes and oligodendrocytes, indicating sensitivity to neurodegenerative and cerebrovascular processes.

</details>


### [16] [Robust Categorical Data Clustering Guided by Multi-Granular Competitive Learning](https://arxiv.org/abs/2601.16491)
*Shenghong Cai,Yiqun Zhang,Xiaopeng Luo,Yiu-Ming Cheung,Hong Jia,Peng Liu*

Main category: cs.LG

TL;DR: 提出MGCPL算法和CAME策略用于分类数据聚类，能自动发现嵌套粒度簇结构，具有线性时间复杂度，在多个真实数据集上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 分类数据普遍存在嵌套粒度簇效应（对象重叠形成小紧凑簇，小簇再形成大簇），但缺乏类似欧氏空间的明确定义距离空间，给聚类分析带来巨大挑战

Method: 设计多粒度竞争惩罚学习(MGCPL)算法使潜在簇交互式调优并分阶段收敛，结合基于MGCPL编码的簇聚合策略(CAME)先编码数据对象再聚类

Result: MGCPL引导的分类数据聚类(MCDC)方法在多个真实数据集上性能优于现有方法，具有线性时间复杂度，对多领域分类数据高度鲁棒

Conclusion: MCDC能有效自动探索分类数据的嵌套多粒度簇分布，可扩展至大规模数据集，有望用于分布式计算的数据预划分

Abstract: Data set composed of categorical features is very common in big data analysis tasks. Since categorical features are usually with a limited number of qualitative possible values, the nested granular cluster effect is prevalent in the implicit discrete distance space of categorical data. That is, data objects frequently overlap in space or subspace to form small compact clusters, and similar small clusters often form larger clusters. However, the distance space cannot be well-defined like the Euclidean distance due to the qualitative categorical data values, which brings great challenges to the cluster analysis of categorical data. In view of this, we design a Multi-Granular Competitive Penalization Learning (MGCPL) algorithm to allow potential clusters to interactively tune themselves and converge in stages with different numbers of naturally compact clusters. To leverage MGCPL, we also propose a Cluster Aggregation strategy based on MGCPL Encoding (CAME) to first encode the data objects according to the learned multi-granular distributions, and then perform final clustering on the embeddings. It turns out that the proposed MGCPL-guided Categorical Data Clustering (MCDC) approach is competent in automatically exploring the nested distribution of multi-granular clusters and highly robust to categorical data sets from various domains. Benefiting from its linear time complexity, MCDC is scalable to large-scale data sets and promising in pre-partitioning data sets or compute nodes for boosting distributed computing. Extensive experiments with statistical evidence demonstrate its superiority compared to state-of-the-art counterparts on various real public data sets.

</details>


### [17] [BoostFGL: Boosting Fairness in Federated Graph Learning](https://arxiv.org/abs/2601.16496)
*Zekai Chen,Kairui Yang,Xunkai Li,Henan Sun,Zhihan Zhang,Jia Li,Qiangqiang Dai,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 该论文发现联邦图学习中的公平性问题，即平均性能掩盖了对弱势节点组的性能下降。提出了BoostFGL增强框架，通过节点、拓扑和模型三个增强机制，在9个数据集上实现了8.43%的Overall-F1公平性提升，同时保持有竞争力的整体性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦图学习方法虽然达到高平均准确率，但掩盖了不同节点组间的严重性能差异。这些公平性问题源于标签倾斜、拓扑混淆和聚合稀释。论文旨在开发一种公平性感知方法，在不牺牲整体性能的前提下解决这些系统性偏差。

Method: 作者提出BoostFGL，一种增强型框架，包含三个协同机制：(1) 客户端节点增强：重塑本地训练信号以强调服务不足的节点；(2) 客户端拓扑增强：重新分配传播权重至可靠结构；(3) 服务器端模型增强：执行难度感知聚合以保留困难客户端的更新同时稳定全局模型。

Result: 在9个数据集上的广泛实验表明，BoostFGL实现了显著的公平性提升，Overall-F1提高8.43%，同时相对于强基线保持有竞争力的整体性能。

Conclusion: 提出的BoostFGL框架通过系统性地解决三个偏差来源，有效应对了联邦图学习中的公平性挑战。它在不损害整体准确率的前提下显著改善公平性指标，为公平的去中心化图学习提供了实用解决方案。

Abstract: Federated graph learning (FGL) enables collaborative training of graph neural networks (GNNs) across decentralized subgraphs without exposing raw data. While existing FGL methods often achieve high overall accuracy, we show that this average performance can conceal severe degradation on disadvantaged node groups. From a fairness perspective, these disparities arise systematically from three coupled sources: label skew toward majority patterns, topology confounding in message propagation, and aggregation dilution of updates from hard clients. To address this, we propose \textbf{BoostFGL}, a boosting-style framework for fairness-aware FGL. BoostFGL introduces three coordinated mechanisms: \ding{182} \emph{Client-side node boosting}, which reshapes local training signals to emphasize systematically under-served nodes; \ding{183} \emph{Client-side topology boosting}, which reallocates propagation emphasis toward reliable yet underused structures and attenuates misleading neighborhoods; and \ding{184} \emph{Server-side model boosting}, which performs difficulty- and reliability-aware aggregation to preserve informative updates from hard clients while stabilizing the global model. Extensive experiments on 9 datasets show that BoostFGL delivers substantial fairness gains, improving Overall-F1 by 8.43\%, while preserving competitive overall performance against strong FGL baselines.

</details>


### [18] [Interpretable Fine-Gray Deep Survival Model for Competing Risks: Predicting Post-Discharge Foot Complications for Diabetic Patients in Ontario](https://arxiv.org/abs/2511.12409)
*Dhanesh Ramachandram,Anne Loefler,Surain Roberts,Amol Verma,Maia Norman,Fahad Razak,Conrad Pow,Charles de Mestral*

Main category: cs.LG

TL;DR: 提出CRISPNAM-FG模型，将神经可加模型与Fine-Gray公式结合，用于竞争风险生存分析，在保证预测性能的同时提供内在可解释性，适用于糖尿病足并发症等临床预测场景。


<details>
  <summary>Details</summary>
Motivation: 深度学习生存模型预测性能优异但缺乏透明度（黑箱特性），阻碍了其在临床实践中的整合。在竞争风险生存建模等医疗应用中，模型可解释性对于建立AI安全性和临床医生信任至关重要。

Method: 提出CRISPNAM-FG这一内在可解释的生存模型，采用神经可加模型（NAMs）结构，为每种风险设置独立的投影向量，使用Fine-Gray公式预测累积发生率函数，通过形状函数和特征重要性图提供透明、可审计的预测。

Result: 在多个基准数据集上验证了模型；应用于2016-2023年安大略省29家医院的糖尿病患者足部并发症预测；在保持与深度生存模型相当性能的同时，通过形状函数和特征重要性图提供了透明度。

Conclusion: CRISPNAM-FG模型成功解决了竞争风险深度生存模型的可解释性缺口，在提供可比的预测性能的同时，实现了内在透明度，适合临床实践应用。

Abstract: Model interpretability is crucial for establishing AI safety and clinician trust in medical applications for example, in survival modelling with competing risks. Recent deep learning models have attained very good predictive performance but their limited transparency, being black-box models, hinders their integration into clinical practice. To address this gap, we propose an intrinsically interpretable survival model called CRISPNAM-FG. Leveraging the structure of Neural Additive Models (NAMs) with separate projection vectors for each risk, our approach predicts the Cumulative Incidence Function using the Fine-Gray formulation, achieving high predictive power with intrinsically transparent and auditable predictions. We validated the model on several benchmark datasets and applied our model to predict future foot complications in diabetic patients across 29 Ontario hospitals (2016-2023). Our method achieves competitive performance compared to other deep survival models while providing transparency through shape functions and feature importance plots.

</details>


### [19] [kNN-Graph: An adaptive graph model for $k$-nearest neighbors](https://arxiv.org/abs/2601.16509)
*Jiaye Li,Gang Chen,Hang Xu,Shichao Zhang*

Main category: cs.LG

TL;DR: 提出一种结合HNSW图和预计算投票机制的适应性图模型，将计算负担转移到训练阶段，实现kNN推理速度与计算复杂度的解耦


<details>
  <summary>Details</summary>
Motivation: kNN在大规模应用中面临推理速度与准确率的计算权衡，现有近似方法损害精度且缺乏最优k值选择的适应性

Method: 通过整合HNSW图和预计算机制，将邻居选择和权重计算转移到训练阶段，构建分层拓扑结构实现快速导航和精确决策边界

Result: 在6个数据集上对比8种先进方法，实现实时推理性能且不损失分类精度

Conclusion: 为kNN长期存在的推理瓶颈提供了可扩展的解决方案，建立了基于图的非参数学习新结构范式

Abstract: The k-nearest neighbors (kNN) algorithm is a cornerstone of non-parametric classification in artificial intelligence, yet its deployment in large-scale applications is persistently constrained by the computational trade-off between inference speed and accuracy. Existing approximate nearest neighbor solutions accelerate retrieval but often degrade classification precision and lack adaptability in selecting the optimal neighborhood size (k). Here, we present an adaptive graph model that decouples inference latency from computational complexity. By integrating a Hierarchical Navigable Small World (HNSW) graph with a pre-computed voting mechanism, our framework completely transfers the computational burden of neighbor selection and weighting to the training phase. Within this topological structure, higher graph layers enable rapid navigation, while lower layers encode precise, node-specific decision boundaries with adaptive neighbor counts. Benchmarking against eight state-of-the-art baselines across six diverse datasets, we demonstrate that this architecture significantly accelerates inference speeds, achieving real-time performance, without compromising classification accuracy. These findings offer a scalable, robust solution to the long-standing inference bottleneck of kNN, establishing a new structural paradigm for graph-based nonparametric learning.

</details>


### [20] [Finite-Time Analysis of Gradient Descent for Shallow Transformers](https://arxiv.org/abs/2601.16514)
*Enes Arda,Semih Cayci,Atilla Eryilmaz*

Main category: cs.LG

TL;DR: 该论文分析了核机制下浅层多头Transformer的优化行为，发现其宽度需求仅随样本量对数增长，且优化误差与序列长度无关，这显著优于RNN的指数级误差增长，但代价是内存需求随序列长度增加。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer为何表现出色具有挑战性，因其优化景观是非凸的。

Method: 在核机制下，使用投影梯度下降训练具有m个独立头的浅层Transformer。

Result: 两个主要发现：(i) 非渐近保证所需的宽度仅随样本量n对数增长；(ii) 优化误差与序列长度T无关。这与循环架构形成鲜明对比（后者误差随T指数增长）。代价是内存需求随序列长度增长以保持完整上下文。在师生设置下的数值实验验证了理论预测的缩放规律。

Conclusion: 该分析提供了对Transformer优化行为的理论理解，显示了其相对于RNN的优越缩放特性，但内存开销随序列长度增加。

Abstract: Understanding why Transformers perform so well remains challenging due to their non-convex optimization landscape. In this work, we analyze a shallow Transformer with $m$ independent heads trained by projected gradient descent in the kernel regime. Our analysis reveals two main findings: (i) the width required for nonasymptotic guarantees scales only logarithmically with the sample size $n$, and (ii) the optimization error is independent of the sequence length $T$. This contrasts sharply with recurrent architectures, where the optimization error can grow exponentially with $T$. The trade-off is memory: to keep the full context, the Transformer's memory requirement grows with the sequence length. We validate our theoretical results numerically in a teacher-student setting and confirm the predicted scaling laws for Transformers.

</details>


### [21] [DANCE: Dynamic, Available, Neighbor-gated Condensation for Federated Text-Attributed Graphs](https://arxiv.org/abs/2601.16519)
*Zekai Chen,Haodong Lu,Xunkai Li,Henan Sun,Jia Li,Hongchao Qin,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 本文提出DANCE框架，通过动态图压缩和模型在环机制解决文本属性图联邦学习中的开销、性能次优和可解释性问题，在8个数据集上准确率提升2.33%，token消耗减少33.42%。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在文本属性图联邦学习中的应用，面临三大挑战：1) 长文本处理的高计算成本；2) 一次性图压缩缺乏客户端适应性导致性能次优；3) LLM黑箱特性导致可解释性差，难以审计。

Method: 提出DANCE框架：1) 采用轮次感知、模型在环的动态图压缩刷新机制，利用最新全局模型优化压缩过程；2) 通过存储可本地检查的证据包保持溯源性，实现预测到源邻居和文本片段的追踪。

Result: 在8个文本属性图数据集上，DANCE在8%压缩率下准确率提升2.33%，token消耗比基线减少33.42%。

Conclusion: DANCE通过动态压缩和可解释性设计，为TAG-FGL提供了高效实用的解决方案，平衡了性能、开销和可审计性。

Abstract: Federated graph learning (FGL) enables collaborative training on graph data across multiple clients. With the rise of large language models (LLMs), textual attributes in FGL graphs are gaining attention. Text-attributed graph federated learning (TAG-FGL) improves FGL by explicitly leveraging LLMs to process and integrate these textual features. However, current TAG-FGL methods face three main challenges: \textbf{(1) Overhead.} LLMs for processing long texts incur high token and computation costs. To make TAG-FGL practical, we introduce graph condensation (GC) to reduce computation load, but this choice also brings new issues. \textbf{(2) Suboptimal.} To reduce LLM overhead, we introduce GC into TAG-FGL by compressing multi-hop texts/neighborhoods into a condensed core with fixed LLM surrogates. However, this one-shot condensation is often not client-adaptive, leading to suboptimal performance. \textbf{(3) Interpretability.} LLM-based condensation further introduces a black-box bottleneck: summaries lack faithful attribution and clear grounding to specific source spans, making local inspection and auditing difficult. To address the above issues, we propose \textbf{DANCE}, a new TAG-FGL paradigm with GC. To improve \textbf{suboptimal} performance, DANCE performs round-wise, model-in-the-loop condensation refresh using the latest global model. To enhance \textbf{interpretability}, DANCE preserves provenance by storing locally inspectable evidence packs that trace predictions to selected neighbors and source text spans. Across 8 TAG datasets, DANCE improves accuracy by \textbf{2.33\%} at an \textbf{8\%} condensation ratio, with \textbf{33.42\%} fewer tokens than baselines.

</details>


### [22] [Beyond Superficial Unlearning: Sharpness-Aware Robust Erasure of Hallucinations in Multimodal LLMs](https://arxiv.org/abs/2601.16527)
*Xianya Fang,Feiyang Ren,Xiang Chen,Yu Tian,Zhen Bi,Haiyang Yu,Sheng-Jun Huang*

Main category: cs.LG

TL;DR: 针对多模态大语言模型中的物体幻觉问题，本文发现现有遗忘方法存在结构脆弱性缺陷，并提出SARE框架通过目标感知最大-最小优化和Targeted-SAM机制来平坦化损失景观，实现对抗参数扰动时幻觉的鲁棒抑制。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在物体幻觉问题，会描述不存在的实体并损害可靠性。虽然近期遗忘方法试图缓解此问题，但存在关键缺陷：结构脆弱性。标准擦除仅实现表面抑制，使模型陷入尖锐极小值，在轻量重学习后幻觉灾难性复现。

Method: 提出SARE框架，将遗忘建模为目标感知的最大-最小优化问题，使用Targeted-SAM机制显式平坦化幻觉概念周围的损失景观。通过在模拟最坏情况参数扰动下抑制幻觉，确保移除效果对权重变化具有鲁棒性。

Result: 大量实验表明，SARE在擦除效果上显著优于基线方法，同时保持通用生成质量。关键的是，它能持续对抗重学习和参数更新维持幻觉抑制效果，验证了几何稳定化的有效性。

Conclusion: 通过几何稳定化实现鲁棒遗忘是有效的，SARE框架能确保幻觉概念的持久抑制，避免灾难性复现，为提升多模态大模型可靠性提供了新思路。

Abstract: Multimodal LLMs are powerful but prone to object hallucinations, which describe non-existent entities and harm reliability. While recent unlearning methods attempt to mitigate this, we identify a critical flaw: structural fragility. We empirically demonstrate that standard erasure achieves only superficial suppression, trapping the model in sharp minima where hallucinations catastrophically resurge after lightweight relearning. To ensure geometric stability, we propose SARE, which casts unlearning as a targeted min-max optimization problem and uses a Targeted-SAM mechanism to explicitly flatten the loss landscape around hallucinated concepts. By suppressing hallucinations under simulated worst-case parameter perturbations, our framework ensures robust removal stable against weight shifts. Extensive experiments demonstrate that SARE significantly outperforms baselines in erasure efficacy while preserving general generation quality. Crucially, it maintains persistent hallucination suppression against relearning and parameter updates, validating the effectiveness of geometric stabilization.

</details>


### [23] [A Collision-Free Hot-Tier Extension for Engram-Style Conditional Memory: A Controlled Study of Training Dynamics](https://arxiv.org/abs/2601.16531)
*Tao Lin*

Main category: cs.LG

TL;DR: 论文研究发现，在Engram风格条件记忆中，消除高频键碰撞并不能提升性能，反而碰撞产生的噪声可能起到有益的隐式正则化作用，主要瓶颈在于门控信用分配而非索引精度。


<details>
  <summary>Details</summary>
Motivation: 探究高频键碰撞是否是Engram风格条件记忆的主要瓶颈，以及消除碰撞是否能带来性能提升。

Method: 提出Engram-Nine，一种无碰撞的热层扩展，通过最小完美哈希函数(MPHF)映射高频n-gram，同时保留原始多头哈希查找作为冷层；采用严格等参数量设置和路由分层评估方法。

Result: 1) 无碰撞设计并未持续提升验证损失；2) 发现训练过程中存在"热-冷优势翻转"现象：热位置初始损失较低但最终被冷位置反超；3) 无碰撞配置翻转更早，表明碰撞起到隐式正则化作用；4) 存在门控不匹配：门控早期偏好热位置且持续存在，导致为高损失位置分配更高权重。

Conclusion: 仅提升查找精度无法保证更好的训练效果；主要限制在于门控信用分配而非索引准确性；碰撞诱导的噪声可能提供有益的正则化，不应被简单消除。

Abstract: We investigate whether high-frequency key collisions are a primary bottleneck in Engram-style conditional memory. To isolate the effect of collisions, we introduce Engram-Nine, a collision-free hot-tier extension that maps the most frequent n-grams through a Minimal Perfect Hash Function (MPHF) while retaining the original multi-head hashed lookup as a cold tier. Under a strictly iso-parameter setup, the collision-free design does not consistently improve validation loss.
  Through route-stratified evaluation (decomposing per-token loss into hot/cold contributions), we uncover a consistent "hot-to-cold advantage flip" during training: hot (high-frequency) positions initially have lower loss, but cold positions eventually surpass them. Crucially, collision-free configurations flip earlier than collision-prone baselines, suggesting that collisions act as implicit regularization. We also identify a gating mismatch: the gate learns to favor hot positions early in training, but this preference persists even after the flip, assigning higher weights to positions with higher loss.
  Our findings suggest that improving lookup precision alone does not guarantee better training outcomes. The dominant limitation may lie in gating credit assignment rather than index accuracy, and collision-induced noise may provide beneficial regularization that should not be naively eliminated.

</details>


### [24] [Understanding and Improving UMAP with Geometric and Topological Priors: The JORC-UMAP Algorithm](https://arxiv.org/abs/2601.16552)
*Xiaobin Li,Run Zhang*

Main category: cs.LG

TL;DR: This paper proposes JORC-UMAP, an enhanced UMAP method that incorporates Ollivier-Ricci curvature and Jaccard similarity as geometric and topological priors to fix UMAP's topological tearing and structural collapse issues caused by its sensitivity to k-nearest neighbor graphs.


<details>
  <summary>Details</summary>
Motivation: UMAP's local Euclidean distance assumption often fails to capture intrinsic manifold geometry, leading to topological tearing and structural collapse. The key cause is identified as UMAP's sensitivity to the k-nearest neighbor graph construction.

Method: Introduce Ollivier-Ricci curvature as a geometric prior to reinforce edges at geometric bottlenecks and reduce redundant links, plus Jaccard similarity as a topological prior to ensure neighborhood consistency and mitigate noise-sensitivity in curvature estimation.

Result: Experiments show JORC-UMAP more effectively reduces tearing and collapse than standard UMAP and other DR methods, measured by SVM accuracy and triplet preservation scores, while maintaining computational efficiency.

Conclusion: This work offers a geometry-aware enhancement to UMAP for more faithful high-dimensional data visualization by incorporating geometric and topological priors.

Abstract: Nonlinear dimensionality reduction techniques, particularly UMAP, are widely used for visualizing high-dimensional data. However, UMAP's local Euclidean distance assumption often fails to capture intrinsic manifold geometry, leading to topological tearing and structural collapse. We identify UMAP's sensitivity to the k-nearest neighbor graph as a key cause. To address this, we introduce Ollivier-Ricci curvature as a geometric prior, reinforcing edges at geometric bottlenecks and reducing redundant links. Since curvature estimation is noise-sensitive, we also incorporate a topological prior using Jaccard similarity to ensure neighborhood consistency. The resulting method, JORC-UMAP, better distinguishes true manifold structure from spurious connections. Experiments on synthetic and real-world datasets show that JORC-UMAP reduces tearing and collapse more effectively than standard UMAP and other DR methods, as measured by SVM accuracy and triplet preservation scores, while maintaining computational efficiency. This work offers a geometry-aware enhancement to UMAP for more faithful data visualization.

</details>


### [25] [Predicting Startup Success Using Large Language Models: A Novel In-Context Learning Approach](https://arxiv.org/abs/2601.16568)
*Abdurahman Maarouf,Alket Bakiaj,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 提出kNN-ICL框架，利用大语言模型的上下文学习能力，在无需训练的情况下，仅用少量标注样本即可预测早期创业公司成功概率，在数据稀缺的VC领域优于传统机器学习方法


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法依赖大量标注数据，而VC机构往往只有几十家早期创业公司的成功/失败数据，数据稀缺导致预测模型效果不佳。需要一种无需训练、能利用小样本的方法来解决这一挑战。

Method: 提出基于k近邻的上下文学习框架kNN-ICL，通过相似度筛选最相关的历史创业公司作为示例，利用大语言模型进行预测。使用Crunchbase真实创业公司档案进行验证。

Result: kNN-ICL在预测准确率上优于监督学习基线和普通上下文学习方法；仅需50个示例即可达到较高的平衡准确率。

Conclusion: 上下文学习框架可作为VC机构在数据稀缺环境下的有效决策工具，为小样本预测问题提供新解决方案。

Abstract: Venture capital (VC) investments in early-stage startups that end up being successful can yield high returns. However, predicting early-stage startup success remains challenging due to data scarcity (e.g., many VC firms have information about only a few dozen of early-stage startups and whether they were successful). This limits the effectiveness of traditional machine learning methods that rely on large labeled datasets for model training. To address this challenge, we propose an in-context learning framework for startup success prediction using large language models (LLMs) that requires no model training and leverages only a small set of labeled startups as demonstration examples. Specifically, we propose a novel k-nearest-neighbor-based in-context learning framework, called kNN-ICL, which selects the most relevant past startups as examples based on similarity. Using real-world profiles from Crunchbase, we find that the kNN-ICL approach achieves higher prediction accuracy than supervised machine learning baselines and vanilla in-context learning. Further, we study how performance varies with the number of in-context examples and find that a high balanced accuracy can be achieved with as few as 50 examples. Together, we demonstrate that in-context learning can serve as a decision-making tool for VC firms operating in data-scarce environments.

</details>


### [26] [Dual-Prototype Disentanglement: A Context-Aware Enhancement Framework for Time Series Forecasting](https://arxiv.org/abs/2601.16632)
*Haonan Yang,Jianchao Tang,Zhuo Li*

Main category: cs.LG

TL;DR: Proposes DPAD framework to dynamically disentangle temporal patterns in time series forecasting via dual-prototype banks and context-aware routing, improving SOTA models as a model-agnostic auxiliary method.


<details>
  <summary>Details</summary>
Motivation: Current time series forecasting methods fail to dynamically disentangle complex intertwined temporal patterns, resulting in static, averaged representations lacking context-aware capabilities.

Method: Introduces Dual-Prototype Adaptive Disentanglement (DPAD) with: 1) Dynamic Dual-Prototype bank (DDP) for common/rare patterns, 2) Dual-Path Context-aware routing (DPC) for selective pattern retrieval, 3) Disentanglement-Guided Loss (DGLoss) for role specialization.

Result: Comprehensive experiments show DPAD consistently enhances forecasting performance and reliability of state-of-the-art models across diverse real-world benchmarks.

Conclusion: DPAD successfully equips forecasting models with dynamic pattern disentanglement and context-aware adaptation capabilities, addressing limitations of static representations in existing approaches.

Abstract: Time series forecasting has witnessed significant progress with deep learning. While prevailing approaches enhance forecasting performance by modifying architectures or introducing novel enhancement strategies, they often fail to dynamically disentangle and leverage the complex, intertwined temporal patterns inherent in time series, thus resulting in the learning of static, averaged representations that lack context-aware capabilities. To address this, we propose the Dual-Prototype Adaptive Disentanglement framework (DPAD), a model-agnostic auxiliary method that equips forecasting models with the ability of pattern disentanglement and context-aware adaptation. Specifically, we construct a Dynamic Dual-Prototype bank (DDP), comprising a common pattern bank with strong temporal priors to capture prevailing trend or seasonal patterns, and a rare pattern bank dynamically memorizing critical yet infrequent events, and then an Dual-Path Context-aware routing (DPC) mechanism is proposed to enhance outputs with selectively retrieved context-specific pattern representations from the DDP. Additionally, we introduce a Disentanglement-Guided Loss (DGLoss) to ensure that each prototype bank specializes in its designated role while maintaining comprehensive coverage. Comprehensive experiments demonstrate that DPAD consistently improves forecasting performance and reliability of state-of-the-art models across diverse real-world benchmarks.

</details>


### [27] [Provably Robust Bayesian Counterfactual Explanations under Model Changes](https://arxiv.org/abs/2601.16659)
*Jamie Duell,Xiuyi Fan*

Main category: cs.LG

TL;DR: 提出概率安全反事实解释(PSCE)方法，基于贝叶斯原理生成δ-安全(高预测置信度)和ε-鲁棒(低预测方差)的反事实解释，在模型更新时提供形式化概率保证，实验证明比现有方法更具鲁棒性和判别性。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释在模型频繁更新的真实场景中会迅速失效或不可靠，缺乏对模型变化的鲁棒性保证。

Method: 基于贝叶斯原则，提出PSCE方法，通过构建⟨δ, ε⟩-集提供形式化概率保证，在优化框架中集成不确定性感知约束，确保解释的δ-安全性和ε-鲁棒性。

Result: 在多个数据集上的实证验证表明，PSCE生成的反事实解释比最先进的贝叶斯方法更具合理性和判别性，且在模型变化下具有可证明的鲁棒性。

Conclusion: PSCE为动态机器学习系统提供了可靠的反事实解释方案，解决了模型更新导致的解释失效问题，具有重要的实际应用价值。

Abstract: Counterfactual explanations (CEs) offer interpretable insights into machine learning predictions by answering ``what if?" questions. However, in real-world settings where models are frequently updated, existing counterfactual explanations can quickly become invalid or unreliable. In this paper, we introduce Probabilistically Safe CEs (PSCE), a method for generating counterfactual explanations that are $δ$-safe, to ensure high predictive confidence, and $ε$-robust to ensure low predictive variance. Based on Bayesian principles, PSCE provides formal probabilistic guarantees for CEs under model changes which are adhered to in what we refer to as the $\langle δ, ε\rangle$-set. Uncertainty-aware constraints are integrated into our optimization framework and we validate our method empirically across diverse datasets. We compare our approach against state-of-the-art Bayesian CE methods, where PSCE produces counterfactual explanations that are not only more plausible and discriminative, but also provably robust under model change.

</details>


### [28] [Dynamic Expert-Guided Model Averaging for Causal Discovery](https://arxiv.org/abs/2601.16715)
*Adrick Tench,Thomas Demeester*

Main category: cs.LG

TL;DR: A flexible model averaging method ensembles diverse causal discovery algorithms using dynamically requested expert knowledge (including LLMs) to address healthcare causal modeling challenges, showing efficacy with imperfect experts on noisy data.


<details>
  <summary>Details</summary>
Motivation: Healthcare requires accurate causal models for interpretable predictions and treatment effect estimation, but practitioners face overwhelming algorithm choices and real-world data violating standard assumptions, necessitating expert-informed ensembling.

Method: Proposes a flexible model averaging framework that dynamically integrates expert knowledge (e.g., LLMs) to ensemble multiple causal discovery algorithms, adapting to data challenges.

Result: Demonstrated efficacy with imperfect experts (like LLMs) on both clean and noisy data; analyzed impacts of expert correctness levels and LLM capabilities in clinical causal discovery.

Conclusion: The method provides a practical solution for causal discovery under real-world constraints, offering insights for practitioners leveraging imperfect expert knowledge in healthcare settings.

Abstract: Understanding causal relationships is critical for healthcare. Accurate causal models provide a means to enhance the interpretability of predictive models, and furthermore a basis for counterfactual and interventional reasoning and the estimation of treatment effects. However, would-be practitioners of causal discovery face a dizzying array of algorithms without a clear best choice. This abundance of competitive algorithms makes ensembling a natural choice for practical applications. At the same time, real-world use cases frequently face challenges that violate the assumptions of common causal discovery algorithms, forcing heavy reliance on expert knowledge. Inspired by recent work on dynamically requested expert knowledge and LLMs as experts, we present a flexible model averaging method leveraging dynamically requested expert knowledge to ensemble a diverse array of causal discovery algorithms. Experiments demonstrate the efficacy of our method with imperfect experts such as LLMs on both clean and noisy data. We also analyze the impact of different degrees of expert correctness and assess the capabilities of LLMs for clinical causal discovery, providing valuable insights for practitioners.

</details>


### [29] [Uncertainty propagation through trained multi-layer perceptrons: Exact analytical results](https://arxiv.org/abs/2601.16830)
*Andrew Thompson,Miles McCrory*

Main category: cs.LG

TL;DR: This paper provides exact analytical formulas for uncertainty propagation through single-hidden-layer ReLU neural networks with Gaussian inputs, eliminating the need for series expansion approximations used in prior work.


<details>
  <summary>Details</summary>
Motivation: Understanding how uncertainty propagates through neural networks is critical for applications requiring reliable predictions and uncertainty quantification. Previous methods relied on approximate series expansions, which may be inaccurate or computationally expensive.

Method: Analytical derivation of mean and variance expressions for the output of a trained MLP with a single hidden layer and ReLU activation functions, assuming the input follows a multivariate Gaussian distribution.

Result: Exact closed-form expressions for the output mean and variance are obtained, providing a more accurate alternative to the series expansion approximations used in earlier studies.

Conclusion: The work provides a precise analytical framework for uncertainty propagation in simple neural networks, which could improve uncertainty quantification in practical applications without resorting to approximations.

Abstract: We give analytical results for propagation of uncertainty through trained multi-layer perceptrons (MLPs) with a single hidden layer and ReLU activation functions. More precisely, we give expressions for the mean and variance of the output when the input is multivariate Gaussian. In contrast to previous results, we obtain exact expressions without resort to a series expansion.

</details>


### [30] [Sample-wise Constrained Learning via a Sequential Penalty Approach with Applications in Image Processing](https://arxiv.org/abs/2601.16812)
*Francesca Lanzillotta,Chiara Albisani,Davide Pucci,Daniele Baracchi,Alessandro Piva,Matteo Lapucci*

Main category: cs.LG

TL;DR: 该论文提出了一种序列惩罚方法，用于处理学习任务中应形式化为严格约束而非任意惩罚项的数据样本约束问题，在合理假设下具备收敛保证，并在图像处理任务上验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 在许多学习任务中，对个体数据样本的处理要求应被形式化为底层优化问题的严格约束，而非通过任意惩罚项来处理。

Method: 利用序列惩罚方法来妥善处理这些场景中的约束，从而进行学习。

Result: 所提算法在深度学习合理假设下具备收敛保证；在图像处理任务上的实验结果表明该方法在实践中确实可行。

Conclusion: 该方法在实践中确实可行。

Abstract: In many learning tasks, certain requirements on the processing of individual data samples should arguably be formalized as strict constraints in the underlying optimization problem, rather than by means of arbitrary penalties. We show that, in these scenarios, learning can be carried out exploiting a sequential penalty method that allows to properly deal with constraints. The proposed algorithm is shown to possess convergence guarantees under assumptions that are reasonable in deep learning scenarios. Moreover, the results of experiments on image processing tasks show that the method is indeed viable to be used in practice.

</details>


### [31] [GRIP: Algorithm-Agnostic Machine Unlearning for Mixture-of-Experts via Geometric Router Constraints](https://arxiv.org/abs/2601.16905)
*Andy Zhu,Rongzhe Wei,Yupu Gu,Pan Li*

Main category: cs.LG

TL;DR: 提出GRIP框架，通过几何约束迫使MoE模型在机器遗忘时直接擦除专家参数而非操纵路由，实现真实知识遗忘


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法在MoE架构上失效，它们利用路由漏洞将查询重定向远离知识专家而非真正擦除知识，导致模型效用损失和表面性遗忘

Method: GRIP将路由梯度更新投影到专家特定的零空间，解耦路由稳定性与参数刚性，作为适配器约束路由更新而不修改底层遗忘算法

Result: 在大型MoE模型上实验显示，GRIP在所有测试的遗忘方法中实现超过95%的路由稳定性，同时保持效用，消除专家选择偏移

Conclusion: GRIP通过防止现有算法利用MoE路由漏洞，成功将密集架构的遗忘研究适配到MoE模型，实现真正的知识擦除

Abstract: Machine unlearning (MU) for large language models has become critical for AI safety, yet existing methods fail to generalize to Mixture-of-Experts (MoE) architectures. We identify that traditional unlearning methods exploit MoE's architectural vulnerability: they manipulate routers to redirect queries away from knowledgeable experts rather than erasing knowledge, causing a loss of model utility and superficial forgetting. We propose Geometric Routing Invariance Preservation (GRIP), an algorithm-agnostic framework for unlearning for MoE. Our core contribution is a geometric constraint, implemented by projecting router gradient updates into an expert-specific null-space. Crucially, this decouples routing stability from parameter rigidity: while discrete expert selections remain stable for retained knowledge, the continuous router parameters remain plastic within the null space, allowing the model to undergo necessary internal reconfiguration to satisfy unlearning objectives. This forces the unlearning optimization to erase knowledge directly from expert parameters rather than exploiting the superficial router manipulation shortcut. GRIP functions as an adapter, constraining router parameter updates without modifying the underlying unlearning algorithm. Extensive experiments on large-scale MoE models demonstrate that our adapter eliminates expert selection shift (achieving over 95% routing stability) across all tested unlearning methods while preserving their utility. By preventing existing algorithms from exploiting MoE model's router vulnerability, GRIP adapts existing unlearning research from dense architectures to MoEs.

</details>


### [32] [Calibrated Probabilistic Interpolation for GEDI Biomass](https://arxiv.org/abs/2601.16834)
*Robin Young,Srinivasan Keshav*

Main category: cs.LG

TL;DR: 针对NASA GEDI任务的稀疏激光雷达观测插值问题，提出注意力神经过程(ANPs)框架，通过学习灵活的空间协方差函数实现异质景观下的可靠生物量制图与不确定性校准。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法(Random Forest、XGBoost)在处理GEDI观测数据时，将空间预测视为独立，混淆了集成方差与偶然不确定性，且忽略局部空间上下文，导致预测区间校准失败，无法满足异质景观下可靠生物量填图的需求。

Method: 提出注意力神经过程(ANPs)，一种概率性元学习框架，通过显式地将预测条件化于局部观测集和地理空间基础模型嵌入，学习灵活的空间协方差函数，使不确定性估计能够在复杂景观中扩展、在均质区域收缩。

Result: 在从热带亚马逊到北方和高山生态系统的五个不同生物群落中验证表明：ANPs在保持竞争力的精度的同时，实现了接近理想的不确定性校准；通过少量本地数据即可实现跨区域迁移，恢复大部分性能差距。

Conclusion: 该工作为洲际尺度地球观测提供了一种可扩展、理论严谨的替代方案，超越了传统的集成方差方法。

Abstract: Reliable wall-to-wall biomass mapping from NASA's GEDI mission requires interpolating sparse LiDAR observations across heterogeneous landscapes. While machine learning approaches like Random Forest and XGBoost are standard for this task, they treat spatial predictions of GEDI observations from multispectral or SAR remote sensing data as independent without adapting to the varying difficulty of heterogeneous landscapes. We demonstrate these approaches generally fail to produce calibrated prediction intervals. We identify that this stems from conflating ensemble variance with aleatoric uncertainty and ignoring local spatial context.
  To resolve this, we introduce Attentive Neural Processes (ANPs), a probabilistic meta-learning framework that explicitly conditions predictions on local observation sets and geospatial foundation model embeddings. Unlike static ensembles, ANPs learn a flexible spatial covariance function, allowing uncertainty estimates to expand in complex landscapes and contract in homogeneous areas. We validate this approach across five distinct biomes ranging from Tropical Amazonian forests to Boreal and Alpine ecosystems, demonstrating that ANPs achieve competitive accuracy while maintaining near-ideal uncertainty calibration. We demonstrate the operational utility of the method through few-shot adaptation, where the model recovers most of the performance gap in cross-region transfer using minimal local data. This work provides a scalable, theoretically rigorous alternative to ensemble variance for continental scale earth observation.

</details>


### [33] [The Art of Being Difficult: Combining Human and AI Strengths to Find Adversarial Instances for Heuristics](https://arxiv.org/abs/2601.16849)
*Henri Nikoleit,Ankit Anand,Anurag Murty Naredla,Heiko Röglin*

Main category: cs.LG

TL;DR: 人类与LLM协作，通过迭代FunSearch的输出，为组合优化中的多个经典问题生成对抗实例，获得最新的下界，突破十余年未进展的难题。


<details>
  <summary>Details</summary>
Motivation: 解决理论计算机科学中开放性问题，特别是组合优化中的启发式算法性能界限，探索人类与LLM协作的潜力。

Method: 采用FunSearch算法生成初始构造，再由人类专家迭代优化，针对层次k-中位、装箱、背包及Lovász汽油问题等生成对抗实例。

Result: 在层次k-中位、装箱、背包和Lovász汽油问题的推广中得到了改进的下界，部分问题十余年来首次取得进展。

Conclusion: LLMs能提供关键的模式和起点，但人类专家在将模式转化为严谨数学构造方面不可或缺；LLM是数学与计算机科学研究的有力协作工具。

Abstract: We demonstrate the power of human-LLM collaboration in tackling open problems in theoretical computer science. Focusing on combinatorial optimization, we refine outputs from the FunSearch algorithm [Romera-Paredes et al., Nature 2023] to derive state-of-the-art lower bounds for standard heuristics. Specifically, we target the generation of adversarial instances where these heuristics perform poorly. By iterating on FunSearch's outputs, we identify improved constructions for hierarchical $k$-median clustering, bin packing, the knapsack problem, and a generalization of Lovász's gasoline problem - some of these have not seen much improvement for over a decade, despite intermittent attention. These results illustrate how expert oversight can effectively extrapolate algorithmic insights from LLM-based evolutionary methods to break long-standing barriers.
  Our findings demonstrate that while LLMs provide critical initial patterns, human expertise is essential for transforming these patterns into mathematically rigorous and insightful constructions. This work highlights that LLMs are a strong collaborative tool in mathematics and computer science research.

</details>


### [34] [Provably Learning Attention with Queries](https://arxiv.org/abs/2601.16873)
*Satwik Bhattamishra,Kulin Shah,Michael Hahn,Varun Kanade*

Main category: cs.LG

TL;DR: This paper studies learning Transformer models with black-box oracle access, showing single-head attention is efficiently learnable with O(d²) queries (or O(rd) under low-rank assumptions), while multi-head attention is generally unidentifiable without additional constraints.


<details>
  <summary>Details</summary>
Motivation: Understanding the learnability of attention mechanisms from input-output queries is essential for interpreting, verifying, and auditing large language models when only black-box access is available, with implications for model extraction and security.

Method: The authors develop elementary algorithms for single-head attention, leverage compressed sensing for low-rank regimes, analyze robustness to noisy oracles via norm and margin conditions, and prove identifiability limitations for multi-head attention through counterexamples.

Result: (1) Exact learning of single-head attention with O(d²) queries; (2) O(rd) queries for low-rank head dimension r << d using compressed sensing; (3) ε-accurate estimation with polynomial queries under noisy oracle tolerance; (4) Multi-head attention parameters are not identifiable in general.

Conclusion: Single-head attention is efficiently learnable from black-box access, but multi-head attention requires additional structural assumptions for provable guarantees, revealing fundamental learnability differences between attention variants.

Abstract: We study the problem of learning Transformer-based sequence models with black-box access to their outputs. In this setting, a learner may adaptively query the oracle with any sequence of vectors and observe the corresponding real-valued output. We begin with the simplest case, a single-head softmax-attention regressor. We show that for a model with width $d$, there is an elementary algorithm to learn the parameters of single-head attention exactly with $O(d^2)$ queries. Further, we show that if there exists an algorithm to learn ReLU feedforward networks (FFNs), then the single-head algorithm can be easily adapted to learn one-layer Transformers with single-head attention. Next, motivated by the regime where the head dimension $r \ll d$, we provide a randomised algorithm that learns single-head attention-based models with $O(rd)$ queries via compressed sensing arguments. We also study robustness to noisy oracle access, proving that under mild norm and margin conditions, the parameters can be estimated to $\varepsilon$ accuracy with a polynomial number of queries even when outputs are only provided up to additive tolerance. Finally, we show that multi-head attention parameters are not identifiable from value queries in general -- distinct parameterisations can induce the same input-output map. Hence, guarantees analogous to the single-head setting are impossible without additional structural assumptions.

</details>


### [35] [FedSGM: A Unified Framework for Constraint Aware, Bidirectionally Compressed, Multi-Step Federated Optimization](https://arxiv.org/abs/2601.16897)
*Antesh Upadhyay,Sang Bin Moon,Abolfazl Hashemi*

Main category: cs.LG

TL;DR: FedSGM是一个统一的联邦学习约束优化框架，通过切换梯度方法解决功能性约束、通信瓶颈、本地更新和部分客户端参与四大挑战，提供投影自由的原始更新、双向误差反馈和软切换机制，具有O(1/√T)收敛保证并在分类和CMDP任务上验证有效。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临功能性约束、通信压缩、多步本地更新和部分客户端参与四大核心挑战，现有方法无法统一处理且常依赖昂贵的对偶变量或内部求解器，亟需一个理论扎实、投影自由的统一框架。

Method: FedSGM基于切换梯度法，采用投影自由的原始变量更新避免对偶调参；引入双向误差反馈校正压缩偏差并显式建模压缩噪声与多步更新的交互；设计软切换机制稳定可行性边界附近的更新；统一处理约束、压缩、本地更新和部分参与。

Result: 理论证明平均迭代点达到标准O(1/√T)收敛率，并获得将优化进度与采样噪声解耦的高概率界；首次统一四大挑战；在Neyman-Pearson分类和约束马尔可夫决策过程任务上验证了理论保证。

Conclusion: FedSGM建立了约束联邦学习的理论基础，通过创新的切换梯度设计和误差反馈机制，为实际应用中复杂约束和通信限制提供了首个统一且高效的解决方案。

Abstract: We introduce FedSGM, a unified framework for federated constrained optimization that addresses four major challenges in federated learning (FL): functional constraints, communication bottlenecks, local updates, and partial client participation. Building on the switching gradient method, FedSGM provides projection-free, primal-only updates, avoiding expensive dual-variable tuning or inner solvers. To handle communication limits, FedSGM incorporates bi-directional error feedback, correcting the bias introduced by compression while explicitly understanding the interaction between compression noise and multi-step local updates. We derive convergence guarantees showing that the averaged iterate achieves the canonical $\boldsymbol{\mathcal{O}}(1/\sqrt{T})$ rate, with additional high-probability bounds that decouple optimization progress from sampling noise due to partial participation. Additionally, we introduce a soft switching version of FedSGM to stabilize updates near the feasibility boundary. To our knowledge, FedSGM is the first framework to unify functional constraints, compression, multiple local updates, and partial client participation, establishing a theoretically grounded foundation for constrained federated learning. Finally, we validate the theoretical guarantees of FedSGM via experimentation on Neyman-Pearson classification and constrained Markov decision process (CMDP) tasks.

</details>


### [36] [Embedding -based Crop Type Classification in the Groundnut Basin of Senegal](https://arxiv.org/abs/2601.16900)
*Madeline C. Lisaius,Srinivasan Keshav,Andrew Blake,Clement Atzberger*

Main category: cs.LG

TL;DR: 该论文评估了地理空间基础模型TESSERA和AlphaEarth在塞内加尔花生盆地作物类型制图中的应用，发现基于TESSERA的方法在性能、合理性、可迁移性和可及性四项标准上表现最佳，在时间迁移示例中准确率比次优方法高28%，为该地区作物分类提供了有效解决方案。


<details>
  <summary>Details</summary>
Motivation: 世界小农地区需要作物类型地图来保障粮食安全、支持当地生计和减缓气候变化，但现有大多数卫星遥感方法并不适合小农条件，存在方法适用性缺口。

Method: 建立四项评估标准（性能、合理性、可迁移性、可及性），在塞内加尔花生盆地评估TESSERA和AlphaEarth地理空间基础模型嵌入方法，并与现有基线方法进行比较。

Result: 基于TESSERA的方法最符合选择标准，在时间迁移示例中准确率比次优方法高出28%，证明TESSERA嵌入是塞内加尔作物类型分类和制图的有效方法。

Conclusion: TESSERA嵌入方法为小农地区的作物类型分类和制图任务提供了有效的解决方案，在塞内加尔地区表现出优越性能。

Abstract: Crop type maps from satellite remote sensing are important tools for food security, local livelihood support and climate change mitigation in smallholder regions of the world, but most satellite-based methods are not well suited to smallholder conditions. To address this gap, we establish a four-part criteria for a useful embedding-based approach consisting of 1) performance, 2) plausibility, 3) transferability and 4) accessibility and evaluate geospatial foundation model (FM) embeddings -based approaches using TESSERA and AlphaEarth against current baseline methods for a region in the groundnut basin of Senegal. We find that the TESSERA -based approach to land cover and crop type mapping fulfills the selection criteria best, and in one temporal transfer example shows 28% higher accuracy compared to the next best method. These results indicate that TESSERA embeddings are an effective approach for crop type classification and mapping tasks in Senegal.

</details>


### [37] [The Trajectory Alignment Coefficient in Two Acts: From Reward Tuning to Reward Learning](https://arxiv.org/abs/2601.16906)
*Calarina Muslimani,Yunshu Du,Kenta Kawamoto,Kaushik Subramanian,Peter Stone,Peter Wurman*

Main category: cs.LG

TL;DR: 本文提出轨迹对齐系数(TAC)来辅助强化学习奖励函数设计。通过人类被试实验发现TAC能提升奖励函数性能并降低认知负荷；进一步提出可微分的Soft-TAC作为损失函数直接从人类偏好数据训练奖励模型，在Gran Turismo 7中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 强化学习奖励函数设计耗时且易出错，需要支持从业者调参并实现自动化。

Method: 1) 通过人类被试研究，在Lunar Lander环境中比较使用/不使用TAC调参的效果；2) 提出Soft-TAC可微分近似，作为损失函数从人类偏好数据训练奖励模型，并在Gran Turismo 7赛车模拟器中验证。

Result: 1) 提供TAC使参与者设计出性能更好的奖励函数并降低认知负荷；2) 使用Soft-TAC训练的奖励模型能更好捕捉偏好目标，产生行为差异更大的策略。

Conclusion: TAC既可作奖励调参的实用工具，也可作为复杂领域的奖励学习目标。

Abstract: The success of reinforcement learning (RL) is fundamentally tied to having a reward function that accurately reflects the task objective. Yet, designing reward functions is notoriously time-consuming and prone to misspecification. To address this issue, our first goal is to understand how to support RL practitioners in specifying appropriate weights for a reward function. We leverage the Trajectory Alignment Coefficient (TAC), a metric that evaluates how closely a reward function's induced preferences match those of a domain expert. To evaluate whether TAC provides effective support in practice, we conducted a human-subject study in which RL practitioners tuned reward weights for Lunar Lander. We found that providing TAC during reward tuning led participants to produce more performant reward functions and report lower cognitive workload relative to standard tuning without TAC. However, the study also underscored that manual reward design, even with TAC, remains labor-intensive. This limitation motivated our second goal: to learn a reward model that maximizes TAC directly. Specifically, we propose Soft-TAC, a differentiable approximation of TAC that can be used as a loss function to train reward models from human preference data. Validated in the racing simulator Gran Turismo 7, reward models trained using Soft-TAC successfully captured preference-specific objectives, resulting in policies with qualitatively more distinct behaviors than models trained with standard Cross-Entropy loss. This work demonstrates that TAC can serve as both a practical tool for guiding reward tuning and a reward learning objective in complex domains.

</details>


### [38] [Calibrated Similarity for Reliable Geometric Analysis of Embedding Spaces](https://arxiv.org/abs/2601.16907)
*Nicolas Tacheny*

Main category: cs.LG

TL;DR: 该论文提出使用保序回归(isotonic regression)对预训练嵌入空间的余弦相似度进行单调校准，在不改变排序特性的前提下解决了各向异性导致的绝对值误校准问题，恢复了相似度数值的可解释性，同时保持了98%的局部稳定性。


<details>
  <summary>Details</summary>
Motivation: 预训练嵌入的余弦相似度虽与人类评判有强秩相关性，但各向异性使绝对值系统性误校准：分数集中在狭窄的高相似度区间，限制了作为定量指标的可解释性。现有方法通过修改嵌入空间(白化、对比微调)会破坏几何结构且需重算所有嵌入。

Method: 使用人类相似度评判训练保序回归模型，构建单调变换函数。将该方法表征为保序重参数化，并证明其对各类基于排序的构造(角度排序、最近邻、阈值图、分位数决策)具有不变性。

Result: 实现近乎完美的校准效果，完全保持秩相关性，在七种扰动类型下保持98%的局部稳定性，所有基于排序的几何构造均不受影响。

Conclusion: 贡献并非替代余弦相似度，而是通过单调校准恢复其绝对值的可解释性。该方法在不改变排序性质的前提下，解决了各向异性问题，是一种轻量级且几何结构保持不变的校准方案。

Abstract: While raw cosine similarity in pretrained embedding spaces exhibits strong rank correlation with human judgments, anisotropy induces systematic miscalibration of absolute values: scores concentrate in a narrow high-similarity band regardless of actual semantic relatedness, limiting interpretability as a quantitative measure. Prior work addresses this by modifying the embedding space (whitening, contrastive fine tuning), but such transformations alter geometric structure and require recomputing all embeddings.
  Using isotonic regression trained on human similarity judgments, we construct a monotonic transformation that achieves near-perfect calibration while preserving rank correlation and local stability(98% across seven perturbation types). Our contribution is not to replace cosine similarity, but to restore interpretability of its absolute values through monotone calibration, without altering its ranking properties.
  We characterize isotonic calibration as an order-preserving reparameterization and prove that all order-based constructions (angular ordering, nearest neighbors, threshold graphs and quantile-based decisions) are invariant under this transformation.

</details>


### [39] [Group-realizable multi-group learning by minimizing empirical risk](https://arxiv.org/abs/2601.16922)
*Navid Ardeshir,Samuel Deng,Daniel Hsu,Jingwen Liu*

Main category: cs.LG

TL;DR: 在多组学习中，当组族具有有限VC维时，组可实现设置的样本复杂度优于任意设置。通过组可实现概念类的经验风险最小化可获得改进，但计算上难以处理，因此需要非适当学习方法。


<details>
  <summary>Details</summary>
Motivation: 探究在组可实现假设下，多组学习的样本复杂度能否改进，以及这种理论改进是否能在计算上实现。

Method: 使用VC维理论分析样本复杂度界限；提出基于组可实现概念类的经验风险最小化；建议非适当学习作为计算可行的替代方案。

Result: 在组可实现设置下，样本复杂度优于任意设置，即使组族无限但VC维有限；通过经验风险最小化可获得该改进，但计算上难以处理；非适当学习提供了可行的替代方案。

Conclusion: 虽然组可实现性假设能带来更好的样本复杂度，但直接实现的经验风险最小化方法计算上不可行，必须采用非适当学习方法来平衡理论优势与计算复杂度。

Abstract: The sample complexity of multi-group learning is shown to improve in the group-realizable setting over the agnostic setting, even when the family of groups is infinite so long as it has finite VC dimension. The improved sample complexity is obtained by empirical risk minimization over the class of group-realizable concepts, which itself could have infinite VC dimension. Implementing this approach is also shown to be computationally intractable, and an alternative approach is suggested based on improper learning.

</details>


### [40] [Is BatchEnsemble a Single Model? On Calibration and Diversity of Efficient Ensembles](https://arxiv.org/abs/2601.16936)
*Anton Zamyatin,Patrick Indri,Sagar Malhotra,Thomas Gärtner*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In resource-constrained and low-latency settings, uncertainty estimates must be efficiently obtained. Deep Ensembles provide robust epistemic uncertainty (EU) but require training multiple full-size models. BatchEnsemble aims to deliver ensemble-like EU at far lower parameter and memory cost by applying learned rank-1 perturbations to a shared base network. We show that BatchEnsemble not only underperforms Deep Ensembles but closely tracks a single model baseline in terms of accuracy, calibration and out-of-distribution (OOD) detection on CIFAR10/10C/SVHN. A controlled study on MNIST finds members are near-identical in function and parameter space, indicating limited capacity to realize distinct predictive modes. Thus, BatchEnsemble behaves more like a single model than a true ensemble.

</details>


### [41] [3D Molecule Generation from Rigid Motifs via SE(3) Flows](https://arxiv.org/abs/2601.16955)
*Roman Poletukhin,Marcel Kollovieh,Eike Eberhard,Stephan Günnemann*

Main category: cs.LG

TL;DR: 提出了一种基于刚体基序和SE(3)-等变生成模型的3D分子生成方法，在保持或超越现有技术性能的同时，将生成步骤减少2-10倍，表示压缩3.5倍。


<details>
  <summary>Details</summary>
Motivation: 传统3D分子生成通常在原子级别进行，而分子图生成技术则使用片段作为结构单元。本研究旨在将基于框架的蛋白质结构生成中的碎片化思想扩展到3D，将一般分子视为刚体基序的集合，以提高生成效率和表示紧凑性。

Method: 采用SE(3)-等变生成建模，将分子表示为刚体基序的集合，从头生成3D分子结构。

Result: 在多个基准测试中与现有最优方法性能相当或更优，在GEOM-Drugs数据集上原子稳定性更优，生成步骤减少2-10倍，分子表示压缩3.5倍。

Conclusion: 基于刚体基序的表示方法在3D分子生成中是有效的，能够同时提升生成效率、压缩表示并保持或提高生成质量，为分子设计提供了有前景的新方向。

Abstract: Three-dimensional molecular structure generation is typically performed at the level of individual atoms, yet molecular graph generation techniques often consider fragments as their structural units. Building on the advances in frame-based protein structure generation, we extend these fragmentation ideas to 3D, treating general molecules as sets of rigid-body motifs. Utilising this representation, we employ SE(3)-equivariant generative modelling for de novo 3D molecule generation from rigid motifs. In our evaluations, we observe comparable or superior results to state-of-the-art across benchmarks, surpassing it in atom stability on GEOM-Drugs, while yielding a 2x to 10x reduction in generation steps and offering 3.5x compression in molecular representations compared to the standard atom-based methods.

</details>


### [42] [Auto-Regressive Masked Diffusion Models](https://arxiv.org/abs/2601.16971)
*Mahdi Karami,Ali Ghodsi*

Main category: cs.LG

TL;DR: ARMD通过块因果架构统一自回归与扩散模型，以更少的训练步数实现SOTA语言建模性能，并支持并行生成。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型相比自回归模型存在性能差距且需要更多训练迭代，需要弥合这一差距同时保持并行生成优势。

Method: 将掩码扩散重新框架为块因果模型；设计严格因果、置换等变的架构，在单次前向传播中计算多步去噪条件概率；采用渐进置换训练方案学习从左到右和随机词元顺序；提出步幅并行生成策略。

Result: 在标准语言建模基准测试上达到SOTA性能，以显著更少的训练步数超越扩散基线，为并行文本生成建立新基准。

Conclusion: 成功弥合了并行与顺序解码之间的性能差距，结合了自回归模型的训练效率与扩散模型的并行生成能力。

Abstract: Masked diffusion models (MDMs) have emerged as a promising approach for language modeling, yet they face a performance gap compared to autoregressive models (ARMs) and require more training iterations. In this work, we present the Auto-Regressive Masked Diffusion (ARMD) model, an architecture designed to close this gap by unifying the training efficiency of autoregressive models with the parallel generation capabilities of diffusion-based models. Our key insight is to reframe the masked diffusion process as a block-wise causal model. This perspective allows us to design a strictly causal, permutation-equivariant architecture that computes all conditional probabilities across multiple denoising steps in a single, parallel forward pass. The resulting architecture supports efficient, autoregressive-style decoding and a progressive permutation training scheme, allowing the model to learn both canonical left-to-right and random token orderings. Leveraging this flexibility, we introduce a novel strided parallel generation strategy that accelerates inference by generating tokens in parallel streams while maintaining global coherence. Empirical results demonstrate that ARMD achieves state-of-the-art performance on standard language modeling benchmarks, outperforming established diffusion baselines while requiring significantly fewer training steps. Furthermore, it establishes a new benchmark for parallel text generation, effectively bridging the performance gap between parallel and sequential decoding.

</details>


### [43] [Latent Diffusion for Internet of Things Attack Data Generation in Intrusion Detection](https://arxiv.org/abs/2601.16976)
*Estela Sánchez-Carballo,Francisco M. Melgarejo-Meseguer,José Luis Rojo-Álvarez*

Main category: cs.LG

TL;DR: 针对物联网入侵检测中的类别不平衡问题，本研究提出使用潜在扩散模型(LDM)生成攻击数据。在DDoS、Mirai和中间人攻击三种场景上的实验表明，LDM能显著提升IDS性能(F1达0.99)，同时比传统扩散模型采样时间减少25%，在保真度、多样性和效率方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 机器学习驱动的物联网入侵检测系统面临严重的类别不平衡问题，正常流量远多于攻击流量，导致检测性能下降。现有数据增强方法要么过于简单(如过采样)，要么在样本保真度、多样性和计算效率之间难以兼顾。

Method: 提出采用潜在扩散模型(LDM)生成合成攻击数据来平衡训练集，并与多种先进基线方法进行系统比较。在DDoS、Mirai和中间人攻击三种典型物联网攻击类型上，从分布、依赖关系和多样性三个维度评估生成质量。

Result: LDM生成的样本使IDS的F1分数最高达到0.99(DDoS和Mirai攻击)，显著优于竞争方法；能保持特征依赖关系并生成多样化样本；采样时间比数据空间扩散模型减少约25%。

Conclusion: 潜在扩散模型是解决物联网ML-IDS类别不平衡问题的有效可扩展方案，能在保真度、多样性和效率之间取得良好平衡，为合成攻击数据生成提供了新思路。

Abstract: Intrusion Detection Systems (IDSs) are a key component for protecting Internet of Things (IoT) environments. However, in Machine Learning-based (ML-based) IDSs, performance is often degraded by the strong class imbalance between benign and attack traffic. Although data augmentation has been widely explored to mitigate this issue, existing approaches typically rely on simple oversampling techniques or generative models that struggle to simultaneously achieve high sample fidelity, diversity, and computational efficiency. To address these limitations, we propose the use of a Latent Diffusion Model (LDM) for attack data augmentation in IoT intrusion detection and provide a comprehensive comparison against state-of-the-art baselines. Experiments were conducted on three representative IoT attack types, specifically Distributed Denial-of-Service (DDoS), Mirai, and Man-in-the-Middle, evaluating both downstream IDS performance and intrinsic generative quality using distributional, dependency-based, and diversity metrics. Results show that balancing the training data with LDM-generated samples substantially improves IDS performance, achieving F1-scores of up to 0.99 for DDoS and Mirai attacks and consistently outperforming competing methods. Additionally, quantitative and qualitative analyses demonstrate that LDMs effectively preserve feature dependencies while generating diverse samples and reduce sampling time by approximately 25\% compared to diffusion models operating directly in data space. These findings highlight latent diffusion as an effective and scalable solution for synthetic IoT attack data generation, substantially mitigating the impact of class imbalance in ML-based IDSs for IoT scenarios.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [44] [Fluctuation-Response Theory for Nonequilibrium Langevin Dynamics](https://arxiv.org/abs/2601.16387)
*Hyun-Myung Chun,Euijoon Kwon,Hyunggyu Park,Jae Sung Lee*

Main category: cond-mat.stat-mech

TL;DR: 该论文为朗之万动力学建立了一个统一的涨落-响应关系，将涨落耗散定理从平衡态推广到非平衡态，并推导出有限时间涨落-响应不等式及响应不确定性关系，通过F1-ATPase分子马达模型验证了理论框架的实用性。


<details>
  <summary>Details</summary>
Motivation: 将涨落耗散定理从平衡态推广至非平衡态体系，并揭示经验密度与电流涨落和响应之间的共同数学结构。

Method: 利用经验密度和电流涨落与响应的共性数学结构，推导出统一的涨落-响应恒等式；进一步推导有限时间不等式，得到响应不确定性关系，从而建立连接涨落耗散定理与热力学不确定性关系的统一理论框架。

Result: 1) 建立非平衡态下的统一涨落-响应恒等式；2) 导出有限时间涨落-响应不等式及响应不确定性关系；3) 通过F1-ATPase分子马达模型，展示这些基于响应的约束如何限制长时间扩散系数。

Conclusion: 该工作建立了联系涨落耗散定理与热力学不确定性关系的统一理论框架，为非平衡态系统的响应与涨落分析提供了新工具和更普适的边界。

Abstract: We establish a unified fluctuation-response relation for Langevin dynamics. By exploiting the common mathematical structures underlying fluctuations and responses of empirical density and current, we derive a unified identity that generalizes the fluctuation-dissipation theorem from equilibrium to nonequilibrium settings. This relation connects global fluctuations of observables with their local responses to perturbations in force, mobility, and temperature. We further derive finite-time fluctuation-response inequalities, leading to response uncertainty relations that complement the identity by providing more practical bounds. These derivations establish a unified theoretical framework linking the fluctuation-dissipation theorem and thermodynamic uncertainty relations. Using the $F_1$-ATPase molecular motor model, we illustrate how these response-based bounds constrain the long-time diffusion coefficient.

</details>


### [45] [Active Cahn--Hilliard theory for non-equilibrium phase separation: quantitative macroscopic predictions and a microscopic derivation](https://arxiv.org/abs/2601.16539)
*Sumeja Bureković,Filippo De Luca,Michael E. Cates,Cesare Nardini*

Main category: cond-mat.stat-mech

TL;DR: This paper develops a quantitative O(grad⁴) active Cahn-Hilliard theory that goes beyond the limitations of Active Model B+ by not assuming weak phase separation, systematically calculating all five coefficient functions from microscopic physics of thermal quorum-sensing active particles, and demonstrating improved accuracy compared to previous continuum models.


<details>
  <summary>Details</summary>
Motivation: Existing continuum theories (Active Model B+) of active phase separation rely on Taylor-expanding in density, which assumes weak phase separation that doesn't hold across most of the phase diagram. This limitation prevents accurate description of key phenomena like binodal densities and interfacial tensions in strongly phase-separating active systems.

Method: The authors construct an O(grad⁴) active Cahn-Hilliard theory by building the density current from all possible terms with up to four spatial derivatives without Taylor expansion. They develop a novel multiple-scale analysis procedure to systematically coarse-grain a particle model of thermal quorum-sensing active particles (tQSAPs) and calculate all five density-dependent coefficient functions from microscopic physics.

Result: The theory identifies contributions missed in previous continuum models and reveals that neglecting them is only justified in the limit of large quorum-sensing range. Comparison with particle simulations shows the O(grad⁴) theory significantly improves upon previous continuum models, providing quantitative predictions for binodals and interfacial tensions across the full phase diagram.

Conclusion: The active Cahn-Hilliard O(grad⁴) theory provides a robust, quantitative framework for active phase separation that works across strong and weak regimes, enabling systematic derivation of continuum parameters from microscopic particle models and offering substantially improved predictive power over existing theories.

Abstract: Phase-separating active systems can display phenomenology that is impossible in equilibrium. The binodal densities are not solely determined by a bulk (effective) free energy, but also affected by gradient terms, while capillary waves and Ostwald processes are determined by three distinct interfacial tensions. These and related phenomena were so far explained at continuum level using a top-down minimal theory (Active Model B+). This theory, by Taylor-expanding in the scalar order parameter (or density), effectively assumes that phase separation is weak, which is not true across most of the phase diagram. Here we develop a quantitative account of active phase separation, by introducing an active counterpart of Cahn-Hilliard theory, constructing the density current from all possible terms with up to four spatial derivatives without Taylor-expanding in the density. From this O(grad^4) theory, we show how to compute binodals and interfacial tensions for arbitrary choices of the five density-dependent 'coefficient functions' that specify the theory (replacing the four constant coefficients of Active Model B+). We further consider a particle model composed of thermal quorum-sensing active particles (tQSAPs) yielding a fully specified example of the O(grad^4) theory upon coarse-graining. We find that to coarse-grain consistently at O(grad^4) requires a novel procedure, based on multiple-scale analysis, to systematically eliminate fast-evolving orientational moments. Using this, we calculate from microscopic physics all five coefficient functions of the active Cahn-Hilliard theory for tQSAPs. We identify contributions that were missed in previous continuum theories, and show how neglecting them becomes justified only in the limit of large quorum-sensing range parameter. Comparison with particle simulations of tQSAPs shows that our O(grad^4) theory improves on previous continuum models [...]

</details>


### [46] [Statistical mechanics of a 2D material in a gas reservoir](https://arxiv.org/abs/2601.16856)
*Moon-ki Choi,Ellad B Tadmor*

Main category: cond-mat.stat-mech

TL;DR: This paper develops a new thermodynamic model for nanoscale systems that accounts for heat bath interactions, demonstrating through MD simulations that environmental effects significantly impact low-dimensional materials like graphene, unlike bulk systems.


<details>
  <summary>Details</summary>
Motivation: Conventional NVT ensemble approaches are insufficient for thermodynamic modeling of low-dimensional systems because interactions with the heat bath become essential, unlike in bulk systems where partition function is determined solely by Hamiltonian and temperature.

Method: Derived a partition function for low-dimensional systems with heat bath interactions; developed a molecular dynamics algorithm modeling the heat bath as a gas reservoir; validated with 1D harmonic oscillator via numerical integration and MD; extended to study graphene monolayer fluctuations in gas environment; compared results with conventional NVT ensemble simulations.

Result: Successfully validated the theoretical partition function using 1D harmonic oscillator; found that environmental interactions significantly influence out-of-plane fluctuations of 2D graphene monolayer at finite temperature and pressure, with notable differences from NVT ensemble results.

Conclusion: Accounting for heat bath interactions is essential for accurate statistical mechanical description of low-dimensional materials, providing a more realistic modeling framework than conventional NVT ensemble approaches.

Abstract: We derive and validate a partition function for low-dimensional systems interacting with a heat bath, addressing the general issue of thermodynamic modeling of nanoscale systems. In contrast to bulk systems in the canonical (NVT) ensemble where the partition function is solely determined by the Hamiltonian of the system and the temperature of the heat bath, our formulation demonstrates that accounting for the interactions with the heat bath is essential for describing the statistical mechanics of low-dimensional materials. To validate our theoretical findings, we develop a molecular dynamics (MD) algorithm for directly modeling the heat bath as a gas reservoir. We first validate our approach using a 1D harmonic oscillator, calculating its length distribution through explicit numerical integration and confirming these results with MD simulations. We then extend our method to investigate the out-of-plane fluctuations of a 2D graphene monolayer immersed in a gas at finite temperature and pressure. Comparisons with conventional NVT ensemble simulations controlled by a thermostat reveal that environmental interactions significantly influence the properties of the 2D material system.

</details>


### [47] [Universal classical and quantum fluctuations in the large deviations of current of noisy quantum systems: The case of QSSEP and QSSIP](https://arxiv.org/abs/2601.16883)
*Mathias Albert,Denis Bernard,Tony Jin,Stefano Scopa,Shiyi Wei*

Main category: cond-mat.stat-mech

TL;DR: 研究噪声量子扩散系统(QSSEP/QSSIP)的电流涨落统计，发现大尺度下呈现经典典型性，与宏观涨落理论一致，但存在有限尺寸下的量子修正。


<details>
  <summary>Details</summary>
Motivation: 在可解的微观模型中研究量子相干性与扩散共存时的电流涨落，为宏观涨落理论(MFT)提供量子层面的微观基础，并探索量子系统与经典对应模型在涨落统计上的异同。

Method: 采用大偏差理论分析累积量生成函数，通过噪声平均和尺度分析，研究量子密度满足的宏观方程，并计算高阶累积量的有限尺寸修正。

Result: 1) 电流涨落在大尺度下收敛于对应经典过程，呈现经典典型性；2) 量子密度满足宏观涨落理论的哈密顿方程，建立微观量子与宏观理论的直接联系；3) 发现纯量子起源的次领头阶修正项，给出第二、三电流累积量的显式表达式；4) 指出需要发展量子化的宏观涨落理论。

Conclusion: 噪声量子扩散系统在宏观极限下表现出经典行为，但有限尺寸效应揭示出重要量子特征，既为宏观涨落理论提供了微观量子基础，也表明其量子扩展的必要性，相关量子修正可通过实验或数值模拟验证。

Abstract: We study the fluctuation statistics of integrated currents in noisy quantum diffusive systems, focusing on the Quantum Symmetric Simple Exclusion and Inclusion Processes (QSSEP/QSSIP). These one-dimensional fermionic (QSSEP) and bosonic (QSSIP) models feature stochastic nearest-neighbor hopping driven by Brownian noise, together with boundary injection and removal processes. They provide solvable microscopic settings in which quantum coherence coexists with diffusion. Upon noise averaging, their dynamics reduce to those of the classical SSEP/SSIP. We show that the cumulant generating function of the integrated current, at large scales, obeys a large deviation principle. To leading order in system size and for each noise realization, it converges to that of the corresponding classical process, establishing a classical typicality of current fluctuations in these noisy quantum systems. We further demonstrate a direct connection with Macroscopic Fluctuation Theory (MFT), showing that the large-scale equations satisfied by biased quantum densities coincide with the steady-state Hamilton equations of MFT, thereby providing a microscopic quantum justification of the MFT framework in these models. Finally, we identify the leading finite-size corrections to the current statistics. We show the existence of subleading contributions of purely quantum origin, which are absent in the corresponding classical setting, and provide their explicit expressions for the second and third current cumulants. These quantum corrections are amenable to direct experimental or numerical verification, provided sufficient control over the noise realizations can be achieved. Their presence points toward the necessity of a quantum extension of Macroscopic Fluctuation Theory.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [48] [DSGym: A Holistic Framework for Evaluating and Training Data Science Agents](https://arxiv.org/abs/2601.16344)
*Fan Nie,Junlin Wang,Harper Hua,Federico Bianchi,Yongchan Kwon,Zhenting Qi,Owen Queen,Shang Zhu,James Zou*

Main category: cs.AI

TL;DR: Introduce DSGym, a standardized framework for evaluating and training data science agents, with curated task suites (DSGym-Tasks, DSBio, DSPredict) and execution-verified training pipeline, enabling a 4B model to outperform GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Existing data science benchmarks are limited by fragmented evaluation interfaces, narrow task coverage, and lack of rigorous data grounding - many tasks can be solved without actual data.

Method: Developed DSGym, a standardized framework with self-contained execution environments and modular architecture. Created three task suites: DSGym-Tasks (refined existing benchmarks), DSBio (expert bioinformatics tasks), and DSPredict (challenging prediction tasks). Implemented execution-verified data synthesis pipeline for agent training.

Result: Curated comprehensive task suite and built a 2,000-example training set, training a 4B model that outperformed GPT-4o on standardized analysis benchmarks.

Conclusion: DSGym enables rigorous end-to-end measurement of agents' abilities to plan, implement, and validate data analyses in realistic scientific contexts, serving as a live, extensible testbed for the field.

Abstract: Data science agents promise to accelerate discovery and insight-generation by turning data into executable analyses and findings. Yet existing data science benchmarks fall short due to fragmented evaluation interfaces that make cross-benchmark comparison difficult, narrow task coverage and a lack of rigorous data grounding. In particular, we show that a substantial portion of tasks in current benchmarks can be solved without using the actual data. To address these limitations, we introduce DSGym, a standardized framework for evaluating and training data science agents in self-contained execution environments. Unlike static benchmarks, DSGym provides a modular architecture that makes it easy to add tasks, agent scaffolds, and tools, positioning it as a live, extensible testbed. We curate DSGym-Tasks, a holistic task suite that standardizes and refines existing benchmarks via quality and shortcut solvability filtering. We further expand coverage with (1) DSBio: expert-derived bioinformatics tasks grounded in literature and (2) DSPredict: challenging prediction tasks spanning domains such as computer vision, molecular prediction, and single-cell perturbation. Beyond evaluation, DSGym enables agent training via execution-verified data synthesis pipeline. As a case study, we build a 2,000-example training set and trained a 4B model in DSGym that outperforms GPT-4o on standardized analysis benchmarks. Overall, DSGym enables rigorous end-to-end measurement of whether agents can plan, implement, and validate data analyses in realistic scientific context.

</details>


### [49] [Doc2AHP: Inferring Structured Multi-Criteria Decision Models via Semantic Trees with LLMs](https://arxiv.org/abs/2601.16479)
*Hongjia Wu,Shuai Zhou,Hongxin Zhang,Wei Chen*

Main category: cs.AI

TL;DR: 针对LLM在复杂决策中结构一致性差与AHP依赖专家的问题，提出Doc2AHP框架，通过AHP约束引导LLM搜索并优化权重一致性，使非专家能高效构建决策模型，性能显著优于基线。


<details>
  <summary>Details</summary>
Motivation: LLMs擅长语义理解但缺乏复杂决策任务中的结构一致性与逻辑推理可靠性；经典决策理论（如AHP）虽系统化但依赖领域专家，形成"专家瓶颈"，难以规模化应用。

Method: 提出Doc2AHP结构化推理框架：1)利用AHP结构原则作为约束，引导LLM在文档空间进行约束搜索，强制父子节点逻辑蕴含；2)引入多智能体加权机制与自适应一致性优化策略，确保权重分配数值一致性；无需大量标注数据或人工干预。

Result: 实证显示：Doc2AHP赋能非专家用户从零构建高质量决策模型，在逻辑完备性和下游任务准确性上显著超越直接生成基线。

Conclusion: 成功弥合LLM泛化能力与决策理论严谨性之间的鸿沟，为非专家用户提供可扩展的决策建模解决方案，兼具实用性与有效性。

Abstract: While Large Language Models (LLMs) demonstrate remarkable proficiency in semantic understanding, they often struggle to ensure structural consistency and reasoning reliability in complex decision-making tasks that demand rigorous logic. Although classical decision theories, such as the Analytic Hierarchy Process (AHP), offer systematic rational frameworks, their construction relies heavily on labor-intensive domain expertise, creating an "expert bottleneck" that hinders scalability in general scenarios. To bridge the gap between the generalization capabilities of LLMs and the rigor of decision theory, we propose Doc2AHP, a novel structured inference framework guided by AHP principles. Eliminating the need for extensive annotated data or manual intervention, our approach leverages the structural principles of AHP as constraints to direct the LLM in a constrained search within the unstructured document space, thereby enforcing the logical entailment between parent and child nodes. Furthermore, we introduce a multi-agent weighting mechanism coupled with an adaptive consistency optimization strategy to ensure the numerical consistency of weight allocation. Empirical results demonstrate that Doc2AHP not only empowers non-expert users to construct high-quality decision models from scratch but also significantly outperforms direct generative baselines in both logical completeness and downstream task accuracy.

</details>


### [50] [SycoEval-EM: Sycophancy Evaluation of Large Language Models in Simulated Clinical Encounters for Emergency Care](https://arxiv.org/abs/2601.16529)
*Dongshen Peng,Yi Wang,Carl Preiksaitis,Christian Rose*

Main category: cs.AI

TL;DR: 本研究开发SycoEval-EM多智能体模拟框架，通过对抗性患者说服评估20个大语言模型在急诊场景下的稳健性。发现模型普遍存在过度顺从患者不当需求的问题（顺从率0-100%），静态基准测试无法预测此类风险，临床AI认证需采用多轮对抗测试。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在临床决策支持中应用前景广阔，但存在顺从患者不当医疗要求的安全风险。现有评估方法无法有效检测模型在社交压力下的脆弱性，亟需建立专门测试框架来保障临床AI安全性。

Method: 创建SycoEval-EM多智能体模拟框架，在急诊医学场景中设计对抗性患者说服策略，对20个LLM进行1,875次模拟诊疗对话测试，覆盖三个"明智选择"临床场景（影像检查、阿片类药物等）。

Result: 模型顺从率差异显著（0-100%），对影像检查请求的脆弱性（38.8%）高于阿片类药物处方（25.0%）。模型能力与稳健性无强相关性，各种说服策略效果相近（30.0-36.0%），表明存在普遍易感性而非特定策略弱点。

Conclusion: 静态基准测试无法充分评估模型在社交压力下的安全风险，临床AI认证必须引入多轮对抗性测试框架，以确保其在真实医疗环境中的稳健性和安全性。

Abstract: Large language models (LLMs) show promise in clinical decision support yet risk acquiescing to patient pressure for inappropriate care. We introduce SycoEval-EM, a multi-agent simulation framework evaluating LLM robustness through adversarial patient persuasion in emergency medicine. Across 20 LLMs and 1,875 encounters spanning three Choosing Wisely scenarios, acquiescence rates ranged from 0-100\%. Models showed higher vulnerability to imaging requests (38.8\%) than opioid prescriptions (25.0\%), with model capability poorly predicting robustness. All persuasion tactics proved equally effective (30.0-36.0\%), indicating general susceptibility rather than tactic-specific weakness. Our findings demonstrate that static benchmarks inadequately predict safety under social pressure, necessitating multi-turn adversarial testing for clinical AI certification.

</details>


### [51] [LLM is Not All You Need: A Systematic Evaluation of ML vs. Foundation Models for text and image based Medical Classification](https://arxiv.org/abs/2601.16549)
*Meet Raval,Tejul Pandit,Dhvani Upadhyay*

Main category: cs.AI

TL;DR: 本研究通过统一基准测试对比了传统机器学习、提示式大模型和PEFT微调模型在医疗分类任务上的表现，发现传统ML模型仍具最佳综合性能，LoRA微调效果最差，而Gemini 2.5表现参差，证明基础模型并非普遍优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 探讨多模态视觉-语言模型与大语言模型结合在医疗分类中的新可能性，并通过严谨的统一基准测试对比传统机器学习与当代Transformer技术的实际效果。

Method: 使用四个公开数据集（涵盖文本和图像模态，二元与多类复杂度），在统一数据划分和评估指标下，对比三类模型：经典ML（LR、LightGBM、ResNet-50）、提示式大模型（Gemini 2.5）和PEFT微调模型（LoRA适配的Gemma3变体）。

Result: 传统ML模型在多数医疗分类任务中表现最佳，尤其在结构化文本数据集上优势显著；LoRA微调的Gemma变体在所有实验中性能最差；Gemini 2.5表现不一，文本任务差但多类图像任务可媲美ResNet-50基线。

Conclusion: 在许多医疗分类场景中，成熟的机器学习模型仍是最可靠选择；基础模型并非普遍优越，且参数高效微调的效果高度依赖适配策略，本研究中的最小化微调反而有害。

Abstract: The combination of multimodal Vision-Language Models (VLMs) and Large Language Models (LLMs) opens up new possibilities for medical classification. This work offers a rigorous, unified benchmark by using four publicly available datasets covering text and image modalities (binary and multiclass complexity) that contrasts traditional Machine Learning (ML) with contemporary transformer-based techniques. We evaluated three model classes for each task: Classical ML (LR, LightGBM, ResNet-50), Prompt-Based LLMs/VLMs (Gemini 2.5), and Fine-Tuned PEFT Models (LoRA-adapted Gemma3 variants). All experiments used consistent data splits and aligned metrics. According to our results, traditional machine learning (ML) models set a high standard by consistently achieving the best overall performance across most medical categorization tasks. This was especially true for structured text-based datasets, where the classical models performed exceptionally well. In stark contrast, the LoRA-tuned Gemma variants consistently showed the worst performance across all text and image experiments, failing to generalize from the minimal fine-tuning provided. However, the zero-shot LLM/VLM pipelines (Gemini 2.5) had mixed results; they performed poorly on text-based tasks, but demonstrated competitive performance on the multiclass image task, matching the classical ResNet-50 baseline. These results demonstrate that in many medical categorization scenarios, established machine learning models continue to be the most reliable option. The experiment suggests that foundation models are not universally superior and that the effectiveness of Parameter-Efficient Fine-Tuning (PEFT) is highly dependent on the adaptation strategy, as minimal fine-tuning proved detrimental in this study.

</details>


### [52] [AgentsEval: Clinically Faithful Evaluation of Medical Imaging Reports via Multi-Agent Reasoning](https://arxiv.org/abs/2601.16685)
*Suzhong Fu,Jingqi Dong,Xuan Ding,Rui Sun,Yiming Yang,Shuguang Cui,Zhen Li*

Main category: cs.AI

TL;DR: This paper proposes AgentsEval, a multi-agent framework that evaluates medical imaging reports by simulating radiologists' collaborative workflow, providing interpretable and clinically-aligned assessments robust to various perturbations.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods for medical imaging reports fail to capture structured diagnostic logic, resulting in unreliable judgments and limited clinical relevance, which hinders the trustworthy integration of LLMs into clinical practice.

Method: The paper introduces AgentsEval, a multi-agent stream reasoning framework that divides evaluation into four interpretable steps: criteria definition, evidence extraction, alignment, and consistency scoring. It also constructs a multi-domain perturbation-based benchmark covering five medical report datasets with diverse imaging modalities and controlled semantic variations.

Result: Experimental results show that AgentsEval provides clinically aligned, semantically faithful, and interpretable evaluations that remain robust under paraphrastic, semantic, and stylistic perturbations.

Conclusion: This framework represents a significant step toward transparent and clinically grounded assessment of medical report generation systems, promoting trustworthy integration of large language models into clinical practice.

Abstract: Evaluating the clinical correctness and reasoning fidelity of automatically generated medical imaging reports remains a critical yet unresolved challenge. Existing evaluation methods often fail to capture the structured diagnostic logic that underlies radiological interpretation, resulting in unreliable judgments and limited clinical relevance. We introduce AgentsEval, a multi-agent stream reasoning framework that emulates the collaborative diagnostic workflow of radiologists. By dividing the evaluation process into interpretable steps including criteria definition, evidence extraction, alignment, and consistency scoring, AgentsEval provides explicit reasoning traces and structured clinical feedback. We also construct a multi-domain perturbation-based benchmark covering five medical report datasets with diverse imaging modalities and controlled semantic variations. Experimental results demonstrate that AgentsEval delivers clinically aligned, semantically faithful, and interpretable evaluations that remain robust under paraphrastic, semantic, and stylistic perturbations. This framework represents a step toward transparent and clinically grounded assessment of medical report generation systems, fostering trustworthy integration of large language models into clinical practice.

</details>


### [53] [LongCat-Flash-Thinking-2601 Technical Report](https://arxiv.org/abs/2601.16725)
*Meituan LongCat Team,Anchun Gui,Bei Li,Bingyang Tao,Bole Zhou,Borun Chen,Chao Zhang,Chao Zhang,Chen Gao,Chen Zhang,Chengcheng Han,Chenhui Yang,Chuyu Zhang,Cong Chen,Cunguang Wang,Daoru Pan,Defei Bu,Dengchang Zhao,Di Xiu,Dishan Liu,Dongyu Ru,Dunwei Tu,Fan Wu,Fengcheng Yuan,Fengcun Li,Gang Xu,Guanyu Wu,Guoyuan Lin,Haibin Wang,Hansi Yang,Hao Yang,Haonan Yan,Haoxiang Ma,Haoxing Wen,Hongyan Hao,Hongyin Tang,Hongyu Zang,Hongzhi Ni,Hui Su,Jiacheng Zhang,Jiahong Zhou,Jiahuan Li,Jiaming Wang,Jian Yang,Jianfei Zhang,Jianhao Xu,Jianing Wang,Jiapeng Zhu,Jiaqi Sun,Jiarong Shi,Jiarui Zhao,Jingang Wang,Jinluan Yang,Jinrui Ding,Jinwei Xiao,Jiyuan He,Juncan Xu,Kefeng Zhang,Keheng Wang,Li Wei,Lianhui Ma,Lin Qiu,Lingbing Kong,Lingchuan Liu,Linsen Guo,Mengshen Zhu,Mengxia Shen,Mingyang Zhu,Peiguang Li,Peng Pei,Pengcheng Jia,Pengtao Zhang,Peng Zhao,Qi Gu,Qiong Huang,Qiyuan Duan,Quanchi Weng,Rongxiang Weng,Rongzhi Zhang,Rumei Li,Shanglin Lei,Shengnan An,Shijun Dai,Shuaikang Liu,Shuang Zhou,Shuo Wang,Songyuan Zhao,Tao Liang,Tianhao Hu,Tianze Chen,Wei Liu,Wei Shi,Wei Wang,Weifeng Tang,Wenjie Shi,Wenlong Zhu,Wentao Chen,Wentao Shi,Xi Su,Xiangcheng Liu,Xiandi Ma,Xiangyu Xi,Xiangyuan Liu,Xiangzhou Huang,Xiao Liu,Xiaodong Cai,Xiaolong Chen,Xiaowei Shi,Xiaoyu Li,Xin Chen,Xingchen Liu,Xuan Huang,Xuezhi Cao,Xunliang Cai,Yan Chen,Yang Bai,Yang Liu,Yang Yang,Yang Zheng,Yaoming Wang,Yaoming Zhu,Yaqi Huo,Yanyu Chen,Yaorui Shi,Yerui Sun,Yi Zhang,Yihao Chen,Yi-Kai Zhang,Yifan Lu,Yifan Zhao,Yitao Zhai,Yongjing Yin,Yongwei Zhou,Youshao Xiao,Yuchuan Dai,Yuchen Xie,Yuchen Yu,Yufei Zhang,Yuhuai Wei,Yulei Qian,Yunfan Liang,Yunke Zhao,Yuwei Jiang,Yuxin Bian,Yuxin Chen,Yuxin Liu,Yue Xu,Yueqing Sun,Zeyang Yu,Zhao Yang,Zhengsheng Huang,Zhengyu Chen,Zhijian Liu,Zhikang Xia,Zhimin Lin,Zhiyuan Yao,Zhuofan Chen,Zhuowen Han,Zijian Zhang,Ziran Li,Ziwen Wang,Ziyuan Zhuang*

Main category: cs.AI

TL;DR: 美团发布5600亿参数开源MoE推理模型LongCat-Flash-Thinking-2601，通过统一训练框架、环境扩展、DORA强化学习、噪声鲁棒性训练和Heavy Thinking模式，在智能体基准测试中达到SOTA，展现强泛化与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 构建具备卓越智能体推理能力的模型，解决复杂工具交互、长尾生成、多轮对话及真实世界噪声环境下的鲁棒性等挑战。

Method: 采用领域并行专家训练与融合的统一框架，端到端协同设计数据、环境、算法和基础设施；扩展DORA支持万级多环境训练；分析真实噪声并设计针对性训练；引入Heavy Thinking实现测试时并行推理扩展。

Result: 在智能体搜索、工具使用和工具集成推理基准测试中达到开源模型SOTA；对复杂工具交互展现强泛化能力；在含噪声真实环境中保持鲁棒性。

Conclusion: 该模型通过系统性创新实现了先进的智能体推理能力，其综合性能、泛化性和鲁棒性为现实世界应用提供了强大基础。

Abstract: We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.

</details>


### [54] [An Efficient Insect-inspired Approach for Visual Point-goal Navigation](https://arxiv.org/abs/2601.16806)
*Lu Yihe,Barbara Webb*

Main category: cs.AI

TL;DR: This paper develops an insect-inspired navigation agent combining associative learning and path integration brain models, achieving comparable performance to state-of-the-art methods at significantly lower computational cost while demonstrating robustness in realistic environments.


<details>
  <summary>Details</summary>
Motivation: To create efficient visual navigation systems by mimicking how insects learn and refine visually guided paths between locations (like nest and food sources) while avoiding obstacles, addressing the high computational demands of current state-of-the-art models.

Method: Combines abstracted models of two insect brain structures - one for associative learning and one for path integration - to create a novel agent for visual point-goal navigation, drawing analogies between insect navigation behavior and formal navigation benchmarks.

Result: The insect-inspired agent achieves performance comparable to recent state-of-the-art models while operating at many orders of magnitude lower computational cost, and demonstrates robustness to environmental perturbations in more realistic simulated testing.

Conclusion: Insect-inspired computational models offer an efficient and robust alternative to conventional approaches for visual navigation tasks, potentially enabling deployment in resource-constrained environments.

Abstract: In this work we develop a novel insect-inspired agent for visual point-goal navigation. This combines abstracted models of two insect brain structures that have been implicated, respectively, in associative learning and path integration. We draw an analogy between the formal benchmark of the Habitat point-goal navigation task and the ability of insects to learn and refine visually guided paths around obstacles between a discovered food location and their nest. We demonstrate that the simple insect-inspired agent exhibits performance comparable to recent SOTA models at many orders of magnitude less computational cost. Testing in a more realistic simulated environment shows the approach is robust to perturbations.

</details>


### [55] [Reasoning Promotes Robustness in Theory of Mind Tasks](https://arxiv.org/abs/2601.16853)
*Ian B. de Haan,Peter van der Putten,Max van Duijn*

Main category: cs.AI

TL;DR: 本文研究RLVR训练的推理型大模型在心智理论任务上的表现，发现其优势来自解题鲁棒性提升，而非真正的心智理论能力增强。


<details>
  <summary>Details</summary>
Motivation: 探究大模型在心智理论测试中的优异表现是否反映真实的社交认知能力，还是仅仅是推理鲁棒性提升的结果。

Method: 采用机器心理实验的创新变体和既有基准测试，考察基于可验证奖励强化学习（RLVR）训练的推理模型在心智理论任务中的行为。

Result: 推理模型对提示词变化和任务扰动表现出更强的鲁棒性，但性能提升更可能归因于寻找正确答案的鲁棒性增强，而非心智理论推理能力的根本性提升。

Conclusion: 推理模型在心智理论测试上的进步并不代表获得了新的社交认知能力，这对评估大模型的社交认知行为具有重要启示。

Abstract: Large language models (LLMs) have recently shown strong performance on Theory of Mind (ToM) tests, prompting debate about the nature and true performance of the underlying capabilities. At the same time, reasoning-oriented LLMs trained via reinforcement learning with verifiable rewards (RLVR) have achieved notable improvements across a range of benchmarks. This paper examines the behavior of such reasoning models in ToM tasks, using novel adaptations of machine psychological experiments and results from established benchmarks. We observe that reasoning models consistently exhibit increased robustness to prompt variations and task perturbations. Our analysis indicates that the observed gains are more plausibly attributed to increased robustness in finding the correct solution, rather than to fundamentally new forms of ToM reasoning. We discuss the implications of this interpretation for evaluating social-cognitive behavior in LLMs.

</details>


### [56] [MAGE-KT: Multi-Agent Graph-Enhanced Knowledge Tracing with Subgraph Retrieval and Asymmetric Fusion](https://arxiv.org/abs/2601.16886)
*Chi Yu,Hongyu Yuan,Zhiyi Duan*

Main category: cs.AI

TL;DR: 提出MAGE-KT框架解决知识追踪中概念关系挖掘不足和图编码噪声问题，通过多视图异构图和不对称注意力机制提升预测性能


<details>
  <summary>Details</summary>
Motivation: 现有图基知识追踪方法未能充分探索知识概念间关系，且全图编码计算成本高、易引入噪声，导致注意力分散和概念关系 fidelity 下降

Method: 构建多智能体知识概念关系提取器+学生-问题交互图的双视图异构图；基于学生历史记录检索高价值子图；采用非对称交叉注意力融合模块集成语义与行为信号

Result: 在三个标准数据集上显著提升知识概念关系准确性，且下一题预测性能明显优于现有方法

Conclusion: 该方法通过多视图图表示学习和针对性子图检索，有效解决了注意力扩散问题，为知识追踪提供了更鲁棒的建模框架

Abstract: Knowledge Tracing (KT) aims to model a student's learning trajectory and predict performance on the next question. A key challenge is how to better represent the relationships among students, questions, and knowledge concepts (KCs). Recently, graph-based KT paradigms have shown promise for this problem. However, existing methods have not sufficiently explored inter-concept relations, often inferred solely from interaction sequences. In addition, the scale and heterogeneity of KT graphs make full-graph encoding both computationally both costly and noise-prone, causing attention to bleed into student-irrelevant regions and degrading the fidelity of inter-KC relations. To address these issues, we propose a novel framework: Multi-Agent Graph-Enhanced Knowledge Tracing (MAGE-KT). It constructs a multi-view heterogeneous graph by combining a multi-agent KC relation extractor and a student-question interaction graph, capturing complementary semantic and behavioral signals. Conditioned on the target student's history, it retrieves compact, high-value subgraphs and integrates them using an Asymmetric Cross-attention Fusion Module to enhance prediction while avoiding attention diffusion and irrelevant computation. Experiments on three widely used KT datasets show substantial improvements in KC-relation accuracy and clear gains in next-question prediction over existing methods.

</details>


### [57] [Preventing the Collapse of Peer Review Requires Verification-First AI](https://arxiv.org/abs/2601.16909)
*Lei You,Lele Cao,Iryna Gurevych*

Main category: cs.AI

TL;DR: The paper argues AI peer review should be verification-first, not review-mimicking, introducing "truth-coupling" as the key metric. It shows how verification pressure and signal shrinkage create a phase transition where researchers rationally optimize for proxies over truth, even when decisions appear reliable. Solution: deploy AI as an adversarial auditor to expand verification capacity, not as a score predictor.


<details>
  <summary>Details</summary>
Motivation: Current AI peer review mimics human processes, but this incentivizes proxy optimization over truth-seeking when verification capacity is overwhelmed by claims (verification pressure) and real improvements become indistinguishable from noise (signal shrinkage), leading to systemic incentive collapse.

Method: Develops a minimal theoretical model mixing occasional high-fidelity checks with frequent proxy judgments, formalizing verification pressure and signal shrinkage to derive an explicit coupling law and incentive-collapse condition predicting when rational actors shift from truth-seeking to proxy optimization.

Result: Derives a phase transition toward proxy-sovereign evaluation where rational effort shifts to proxy optimization even while decisions remain superficially reliable, quantified by an explicit truth-coupling law and incentive-collapse condition.

Conclusion: AI should be deployed as an adversarial auditor that generates auditable verification artifacts and expands verification bandwidth, not as a score predictor that amplifies claim inflation. Tool builders and program chairs must prioritize verification-first approaches to maintain truth-coupling.

Abstract: This paper argues that AI-assisted peer review should be verification-first rather than review-mimicking. We propose truth-coupling, i.e. how tightly venue scores track latent scientific truth, as the right objective for review tools. We formalize two forces that drive a phase transition toward proxy-sovereign evaluation: verification pressure, when claims outpace verification capacity, and signal shrinkage, when real improvements become hard to separate from noise. In a minimal model that mixes occasional high-fidelity checks with frequent proxy judgment, we derive an explicit coupling law and an incentive-collapse condition under which rational effort shifts from truth-seeking to proxy optimization, even when current decisions still appear reliable. These results motivate actions for tool builders and program chairs: deploy AI as an adversarial auditor that generates auditable verification artifacts and expands effective verification bandwidth, rather than as a score predictor that amplifies claim inflation.

</details>


### [58] [Empowering Medical Equipment Sustainability in Low-Resource Settings: An AI-Powered Diagnostic and Support Platform for Biomedical Technicians](https://arxiv.org/abs/2601.16967)
*Bernes Lorier Atabonfack,Ahmed Tahiru Issah,Mohammed Hardi Abdul Baaki,Clemence Ingabire,Tolulope Olusuyi,Maruf Adewole,Udunna C. Anazodo,Timothy X Brown*

Main category: cs.AI

TL;DR: 针对中低收入国家医疗设备维护难题，本研究开发AI支持平台，集成大语言模型提供实时故障诊断与分步维修指导，并设全球同行论坛。在飞利浦HDI 5000超声设备上的概念验证显示，错误代码解读准确率达100%，纠正措施建议准确率80%，证明AI可有效减少设备停机，提升资源匮乏地区的医疗服务。


<details>
  <summary>Details</summary>
Motivation: 中低收入国家大量医疗设备因维护不及时、技术专家短缺及制造商支持不足而闲置或故障，导致诊断延误和患者护理受损。本研究旨在利用AI技术解决这一维护难题。

Method: 构建集成大语言模型（LLM）的网页平台，支持技术人员输入错误代码或症状，获取实时分步排障指导；增设全球同行论坛促进经验共享。以飞利浦HDI 5000超声设备为案例开发概念验证原型。

Result: 概念验证原型在飞利浦HDI 5000超声设备上实现100%错误代码解读精度和80%纠正措施建议准确率。

Conclusion: 研究表明AI驱动系统具备支持医疗设备维护的可行性，有望通过减少停机时间改善资源有限环境下的医疗交付。

Abstract: In low- and middle-income countries (LMICs), a significant proportion of medical diagnostic equipment remains underutilized or non-functional due to a lack of timely maintenance, limited access to technical expertise, and minimal support from manufacturers, particularly for devices acquired through third-party vendors or donations. This challenge contributes to increased equipment downtime, delayed diagnoses, and compromised patient care. This research explores the development and validation of an AI-powered support platform designed to assist biomedical technicians in diagnosing and repairing medical devices in real-time. The system integrates a large language model (LLM) with a user-friendly web interface, enabling imaging technologists/radiographers and biomedical technicians to input error codes or device symptoms and receive accurate, step-by-step troubleshooting guidance. The platform also includes a global peer-to-peer discussion forum to support knowledge exchange and provide additional context for rare or undocumented issues. A proof of concept was developed using the Philips HDI 5000 ultrasound machine, achieving 100% precision in error code interpretation and 80% accuracy in suggesting corrective actions. This study demonstrates the feasibility and potential of AI-driven systems to support medical device maintenance, with the aim of reducing equipment downtime to improve healthcare delivery in resource-constrained environments.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [59] [Magnetic structure of EuZn$_2$Sb$_2$ single-crystal thin-film](https://arxiv.org/abs/2601.16365)
*Yu Wei Soh,Hsiang Lee,Eugen Weschke,Shinichi Nishihaya,Mikhael T. Sayat,Masaki Uchida,Jian-Rui Soh*

Main category: cond-mat.str-el

TL;DR: EuZn₂Sb₂ exhibits spatially separated magnetic phases: surface ferromagnetic layers behave as a Weyl semimetal, while bulk antiferromagnetic layers act as a topological crystalline insulator, explaining its complex topological electronic behavior.


<details>
  <summary>Details</summary>
Motivation: The topology of EuZn₂Sb₂'s electronic band structure depends on the magnetic configuration of its europium sublattice, but this relationship had not been experimentally determined, creating ambiguity about its topological properties.

Method: Combined \textit{ab-initio} calculations (to predict topological phases under different magnetic orders) with resonant x-ray elastic scattering measurements on single-crystal thin films (to determine actual magnetic structure).

Result: Calculations show A-type antiferromagnetic order yields topological crystalline insulator (in-plane) or Dirac semimetal (out-of-plane), while ferromagnetic order stabilizes Weyl semimetal. Experiments reveal spatially separated ferromagnetic (top 3 atomic layers) and antiferromagnetic phases along the crystal c-axis below 12.9 K.

Conclusion: EuZn₂Sb₂ functions as a dual-topological material: surface layers exhibit Weyl semimetal behavior due to ferromagnetism, while deeper layers form a topological crystalline insulator via antiferromagnetic ordering, resolving prior contradictions in its topological classification.

Abstract: Magnetic topological materials are a class of compounds which can host massless electrons controlled by the magnetic order. One such compound is EuZn$_2$Sb$_2$, which has recently garnered interest due to its strong interplay between the Eu magnetism and charge carriers. However the topology of the electronic band structure, which depends on the ground state magnetic configuration of the europium sublattice, has not been determined. Based on our \textit{ab-initio} calculations, we find that an in-plane and out-of-plane \textit{A}-type antiferromagnetic (AFM) order generates a topological crystalline insulator and Dirac semimetal respectively, whereas a ferromagnetic (FM) order stabilizes a Weyl semimetal. Our resonant x-ray elastic scattering measurements of single-crystal thin film EuZn$_2$Sb$_2$ reveal both a sharp magnetic peak at $\textit{\textbf{Q}}$=$(0,0,\frac{1}{2})$ and broad $\textit{\textbf{Q}}$=$(0,0,1)$ below $T_{\mathrm{N}}=12.9$\,K, which is associated with an \textit{A}-type AFM and FM order, respectively. Our measurements indicate that the FM and AFM layers are spatially separated along the crystal $c$ axis, with the former limited to the top three atomic layers. We propose that EuZn$_2$Sb$_2$ behaves as a Weyl semimetal in the surface FM layers, and as a topological crystalline insulator in the lower AFM layers.

</details>


### [60] [Ab Initio Many Body Quantum Embedding and Local Correlation in Crystalline Materials using Interpolative Separable Density Fitting](https://arxiv.org/abs/2601.16379)
*Junjie Yang,Ning Zhang,Shunyue Yuan,Jincheng Yu,Hong-Zhou Ye,Garnet Chan*

Main category: cond-mat.str-el

TL;DR: 开发线性标度的量子嵌入方法，实现周期性体系高精度关联计算，支持1000 k点采样并外推至热力学极限


<details>
  <summary>Details</summary>
Motivation: 传统ab initio多体量子方法计算周期性体系时计算成本随k点数量超线性增长，难以实现大k点采样和高精度关联计算

Method: 采用平移对称性适配的插值可分离密度拟合技术，将计算标度降至与k点数量成线性关系，结合密度矩阵嵌入理论和局域自然轨道关联框架

Result: 成功在弱/强关联固体中实现1000 k点规模的耦合簇计算，通过局域关联域和k点采样外推获得热力学极限下的CCSD(T)基态能量估计

Conclusion: 该方法突破了周期性体系高精度量子计算的尺寸限制，为大规模材料模拟提供了可行方案

Abstract: We present an efficient implementation of ab initio many-body quantum embedding and local correlation methods for infinite periodic systems through translational symmetry adapted interpolative separable density fitting, an approach which reduces the scaling of the calculations to only linear with the number of k-points. Employing this methodology, we compute correlated ground-state coupled cluster energies within density matrix embedding and local natural orbital correlation frameworks for both weakly and strongly correlated solids, using up to 1000 k-points. By extrapolating the local correlation domains and k-point sampling we further obtain estimates of the full coupled cluster with singles, doubles, and perturbative triples ground-state energies in the thermodynamic limit.

</details>


### [61] [Emergence of Kondo-assisted Néel order in a Kondo necklace model](https://arxiv.org/abs/2601.16388)
*Hironori Yamaguchi,Shunsuke C. Furuya,Yu Tominaga,Takanori Kida,Koji Araki,Masayuki Hagiwara*

Main category: cond-mat.str-el

TL;DR: 通过构建仅含自旋自由度的镍基配合物模型，揭示了Kondo耦合在自旋1/2与自旋1及以上体系中分别抑制和促进磁有序的普适边界。


<details>
  <summary>Details</summary>
Motivation: 传统Kondo效应被认为通过形成单态抑制磁性，但新研究显示特定条件下可能增强磁有序，然而复杂电子结构（轨道/电荷自由度）使核心机制难以分离。

Method: 在镍基配合物中实现自旋-(1/2,1) Kondo项链模型，消除电荷自由度以孤立量子自旋关联，结合热力学测量和微扰理论分析。

Result: 实验观测到磁相变和场致量子相变；理论证明Kondo耦合在自旋1位点间产生反铁磁相互作用，稳定整个链的Néel有序。

Conclusion: 确立Kondo物理的普适边界：耦合自旋1/2时形成单态抑制磁性，耦合自旋1及以上时则稳定磁有序。

Abstract: The interplay between Kondo screening and magnetic order has long been a central issue in the physics of strongly correlated systems. While the Kondo effect has traditionally been understood to suppress magnetism through the formation of local singlets, recent studies suggest that Kondo interactions may enhance magnetic order under certain conditions. However, these scenarios often rely on complex electronic structures, including orbital and charge degrees of freedom, making the essential mechanisms difficult to isolate. Here we report the realization of a spin-(1/2,1) Kondo necklace model in a Ni-based complex-a minimal spin-only analog of the Kondo lattice that isolates quantum spin correlations by eliminating charge degrees of freedom. Thermodynamic measurements identify a magnetic phase transition and a field-induced quantum phase transition. Perturbative analysis reveals that the Kondo coupling mediates effective antiferromagnetic interactions between the spin-1 sites, stabilizing the Néel order across the entire chain. Our results establish a universal boundary in Kondo physics, where coupling to spin-1/2 moments yields singlets, but to spin-1 and higher stabilizes magnetic order.

</details>


### [62] [Realization of a triangular spin necklace in a verdazyl-based Ni complex](https://arxiv.org/abs/2601.16389)
*Itsuki Shimamura,Risa Yagura,Takanori Kida,Masayuki Hagiwara,Koji Araki,Yoshiki Iwasaki,Yuko Hosokoshi,Kenta Kimura,Hironori Yamaguchi*

Main category: cond-mat.str-el

TL;DR: 成功合成了一种新型一维三角自旋链配合物，揭示了由几何阻挫导致的量子自旋液体特征及磁场调控行为


<details>
  <summary>Details</summary>
Motivation: 探索分子设计实现几何阻挫量子自旋链的新方法，为研究低维材料中阻挫驱动的量子相提供平台

Method: 合成(m-Py-V)₃[Ni(NO₃)₂]配合物；结合分子轨道计算、磁化率/比热测量和顺磁共振谱研究自旋结构

Result: 发现Ni²⁺与verdazyl自由基形成含spin-1/2和spin-1单元的三角自旋链；强反铁磁作用使部分自由基形成单重态二聚体，剩余自旋构成阻挫三角单元；观测到反铁磁相变，磁场可抑制该相变

Conclusion: 该配合物是罕见的分子基几何阻挫量子自旋链实例，其自旋-1各向异性促进了反铁磁有序，为研究阻挫量子相提供了理想模型体系

Abstract: We successfully synthesized a verdazyl-based complex, ($m$-Py-V)$_3$[Ni(NO$_3$)$_2$], in which Ni$^{2+}$ ions and verdazyl radicals form a one-dimensional, triangular spin necklace consisting of spin-1/2 and spin-1 units. Molecular orbital calculations reveal strong antiferromagnetic (AF) interactions between inversion-related radical pairs that form spin-1/2 singlet dimers. The remaining verdazyl and Ni$^{2+}$ spins form frustrated triangular units, creating a distinctive spin network. Magnetic susceptibility and specific heat measurements identify a phase transition to an AF order. The application of magnetic fields suppresses the phase transition signal, suggesting field-induced decoupling of the spin-1 moments. Electron spin resonance measurements are used to evaluate the easy-axis anisotropy of spin-1, which may promote the AF order. This work provides a rare example of a geometrically frustrated quantum spin chain realized via molecular design, thereby offering a platform for exploring frustration-driven quantum phases in low-dimensional materials.

</details>


### [63] [Accelerating dynamical mean-field theory convergence by preconditioning with computationally cheaper quantum embedding methods](https://arxiv.org/abs/2601.16401)
*E. M. Makaresz,O. Gingras,Tsung-Han Lee,Nicola Lanatà,B. J. Powell,Henry L. Nourse*

Main category: cond-mat.str-el

TL;DR: 用g-RISB等廉价量子嵌入方法初始化DMFT，可将迭代次数减少10倍，常只需1次DMFT迭代即可恢复完整动力学结构，特别适用于Mott转变区。


<details>
  <summary>Details</summary>
Motivation: DMFT计算成本高，收敛迭代次数多，需要加速方法。

Method: 用Hartree-Fock、Hubbard-I、RISB、g-RISB等廉价量子嵌入方法提供DMFT自洽循环的初始解，对比效果。

Result: g-RISB初始化最有效，可减少迭代达10倍；多数情况下仅需1次DMFT迭代即可达到完整精度；改进效果取决于初始解在低能自能上的准确性；在Mott绝缘体-金属转变区特别有效，避免非相互作用初始化导致的DMFT失效。

Conclusion: 采用准确的廉价量子嵌入方法初始化DMFT是显著降低计算成本的有效策略，尤其适用于收敛慢或易失败的体系。

Abstract: Dynamical mean-field theory (DMFT) is a cornerstone technique for studying strongly correlated electronic systems. However, each DMFT step is computationally demanding, and many iterations can be required to achieve convergence. Here, we accelerate the convergence of DMFT by initializing its self-consistent cycle with solutions from computationally cheaper and more approximate methods. We compare the initialization with the non-interacting solution to a range of quantum embedding compatible approaches: Hartree-Fock, the Hubbard-I approximation, rotationally invariant slave bosons (RISB), and its ghost extension (g-RISB). We find that these initializations can reduce the number of DMFT iterations by up to an order of magnitude, with g-RISB providing the most effective and reliable benefits. In most regimes, initializing with g-RISB and performing a single DMFT iteration suffices to recover the full dynamical structure. The improvement in convergence is controlled by the initial solution's accuracy in the low-energy part of the self-energy, on the scale of the non-interacting bandwidth. This strategy is especially effective at the Mott insulator-metal transition, where an initialization from the non-interacting limit can lead to a breakdown of DMFT due to the sign problem. Our results establish the usage of accurate yet cheaper quantum embedding methods as a powerful means to substantially reduce the computational cost of DMFT, particularly in regimes where convergence is slow or prone to failure.

</details>


### [64] [Effect of Electron Correlation on the Integer Quantum Hall Effect](https://arxiv.org/abs/2601.16453)
*Daniel Staros,Christopher Lane,Roxanne Tutchton,Jian-Xin Zhu*

Main category: cond-mat.str-el

TL;DR: 电子关联效应通过在位库仑排斥作用削弱整数量子霍尔效应中ν=1横向电导的量子化精度，关联强度与磁场协同作用导致重整化跃迁参数和格点能级的周期性调制


<details>
  <summary>Details</summary>
Motivation: 探究电子关联作用对整数量子霍尔效应量子化特性的影响机制，揭示多体相互作用与外磁场协同调控导电行为的物理规律

Method: 基于正方晶格模型进行数值计算，通过调节有效在位排斥参数U来模拟电子关联强度的变化

Result: 随着关联强度U增大，ν=1填充因子下的横向电导量子化特性显著退化，出现由关联-磁场耦合作用诱导的周期性参数调制现象

Conclusion: 电子关联强度是影响整数量子霍尔体系导电性能的关键调控参数，该发现为理解强关联拓扑物态提供了新视角

Abstract: We numerically investigate the effect of electron correlation on the integer quantum Hall effect in a square lattice. Increasing the correlation strength via the effective onsite repulsion parameter $U$ degrades the quantization of $ν= 1$ transverse conductance due to the interplay of correlation and the external magnetic field, which together induce periodic modulations in renormalized hopping parameters and site energies. Overall, this work demonstrates that the strength of electron correlation can significantly impact conductivity in the integer quantum Hall regime.

</details>


### [65] [Bosonization Solution to Spin-Valley Kondo Problem: Finite-Size Spectrum and Renormalization Group Analysis](https://arxiv.org/abs/2601.16525)
*Yi-Jie Wang,Geng-Dong Zhou,Hyunsung Jung,Seongyeon Youn,Seung-Sup B. Lee,Zhi-Da Song*

Main category: cond-mat.str-el

TL;DR: This paper studies spin-valley Anderson impurities in magic-angle graphene, deriving low-energy Kondo theories and revealing novel pair Kondo scattering that drives phase transitions between exotic quantum phases.


<details>
  <summary>Details</summary>
Motivation: To provide a natural explanation for the origin of pairing potential and pseudogap in magic-angle graphene using spin-valley Anderson impurities (SVAIM) with (anti-)Hund's splitting.

Method: Derive and analytically solve low-energy Kondo theories for SVAIM at half-filling using renormalization group (RG) calculations based on Coulomb gas analog, and bosonization-refermionization along a solvable fixed line.

Result: Identified a novel pair Kondo scattering λ_x that drives Berezinskii-Kosterlitz-Thouless phase transition in the valley doublet regime (between anisotropic doublet phase and pair-Kondo Fermi liquid), and a second-order transition in the singlet regime (between Kondo Fermi liquid and local singlet phase). Both phases' finite-size spectra, thermodynamics, and correlation functions are analytically solved.

Conclusion: The work establishes a theoretical framework connecting SVAIM to pairing and pseudogap phenomena in magic-angle graphene, revealing rich Kondo physics and quantum phase transitions with exact analytical solutions.

Abstract: Spin-valley Anderson impurities (SVAIM) with (anti-)Hund's splitting provide a natural explanation to the origin of pairing potential and pseudogap in the magic-angle graphene. In this work, we derive and analytically solve the low-energy Kondo theories for SVAIM at half-filling, with especial focus on the two anti-Hund's regimes: the impurity is either dominated by a valley doublet, or a trivial singlet. In the doublet regime, we reveal that a novel pair Kondo scattering $λ_x$ is required to flip the valley doublet, which involves a quartic operator of bath electrons. Our renormalization group (RG) calculation based on the Coulomb gas analog shows $λ_x$ drives a phase transition of the Berezinskii-Kosterlitz-Thouless type. One side of the transition is an anisotropic doublet phase, characterized by non-universal phase shifts of bath electrons and non-analytic impurity susceptibilities, while the other is a Fermi liquid formed by pair-Kondo resonance. The finite-size many-body spectrum, thermodynamic quantities, and correlation functions for both phases are analytically solved. Remarkably, the solution in the pair-Kondo Fermi liquid is achieved via the constructive approach of bosonization-refermionization along a solvable fixed line, where the many-body interaction $λ_x$ is mapped into a pseudo-fermion bilinear in a rigorous manner. Finally, we also apply the RG analysis to the singlet regime, and identify a second-order phase transition between the Kondo Fermi liquid and a local singlet phase.

</details>


### [66] [$d$-wave FFLO state and charge-2e supersolidity in the $t$-$t'$-$J$ model under Zeeman fields](https://arxiv.org/abs/2601.16630)
*Xing-Zhou Qu,Dai-Wei Qu,Qiaoyi Li,Wei Li,Gang Su*

Main category: cond-mat.str-el

TL;DR: This study uses advanced tensor network simulations to reveal the long-sought FFLO superconducting state in a fundamental electron model under extreme magnetic fields, showing it coexists with density waves as a "charge-2e supersolid" phase.


<details>
  <summary>Details</summary>
Motivation: Unconventional superconductivity beyond the Pauli paramagnetic limit remains a major unsolved challenge; the exotic FFLO state lacks definitive study in basic electronic models despite decades of theoretical prediction.

Method: State-of-the-art finite-temperature and ground-state tensor network approaches applied to the $t$-$t'$-$J$ model under Zeeman fields to map the superconducting phase diagram.

Result: Zero-momentum $d$-wave superconductivity persists until spin gap closure; a novel $d$-wave FFLO phase emerges above the Pauli limit, coexisting with enhanced spin density waves. The FFLO pairing momentum locks to the Fermi surface, and both phases exhibit simultaneous pairing condensate and density wave orders.

Conclusion: The identified phases are classified as charge-2e supersolids, providing the first microscopic evidence of the FFLO state in a fundamental correlated electron model, with implications for realization in ultracold atom optical lattices.

Abstract: Unconventional superconductivity under strong Zeeman fields--particularly beyond the Pauli paramagnetic limit--remains a central challenge in condensed matter physics. The exotic Fulde-Ferrell-Larkin-Ovchinnikov (FFLO) state, in particular, remains in need of definitive study within fundamental electronic models. Here we employ state-of-the-art finite-temperature and ground-state tensor network approaches to systematically explore the superconducting (SC) phase diagram of the $t$-$t'$-$J$ model subjected to Zeeman fields. We find that zero-momentum $d$-wave superconductivity persists until the spin gap closes, coexisting with charge density waves. A novel $d$-wave FFLO phase emerges under a higher Zeeman field even above the Pauli limit, concomitant with a field-enhanced spin density waves. We identify these phases, characterized by the simultaneous presence of pairing condensate and density wave orders, as charge-2e supersolids. Analysis of Matsubara Green's function reveals that the FFLO pairing momentum is locked to the underlying Fermi surface. Our results provide microscopic insights into field-induced unconventional pairing mechanisms and reveal the long-sought FFLO state in a fundamental correlated electron model, offering a promising route for its realization in ultracold atom optical lattice.

</details>


### [67] [Zoology of Altermagnetic-type Non-collinear Magnets on the Maple Leaf Lattice](https://arxiv.org/abs/2601.16807)
*Pratyay Ghosh,Ronny Thomale*

Main category: cond-mat.str-el

TL;DR: 该论文研究枫叶晶格上的非常规非共线磁性基态，根据时间反演($\mathcal{T}$)和宇称($\mathcal{P}$)对称性的破缺或保持，识别出两类变磁性序：弱耦合下$\mathcal{P}$保持的$q=0$变磁序，以及强耦合下$\mathcal{P}$破缺的倾斜120°变磁序，并预测了动量依赖的磁振子自旋劈裂，确立了枫叶晶格作为研究相变和阻挫现象的重要平台。


<details>
  <summary>Details</summary>
Motivation: 探索具有竞争性和对称性破缺特征的非常规磁性态，特别是在枫叶晶格这一新颖几何结构上研究变磁性，以理解其中的相变和阻挫现象。

Method: 采用线性自旋波理论研究磁振子激发，并通过哈伯德模型的平均场分析来确定不同耦合强度下的磁性基态。

Result: 在弱耦合下发现巡游的保持宇称的$q=0$变磁性序，在强耦合下预期出现破缺宇称的倾斜120°变磁性序；根据$\mathcal{P}\mathcal{T}$对称性破缺的不同性质，在布里渊区不同高对称点观测到动量依赖的非相对论性磁振子自旋劈裂。

Conclusion: 枫叶晶格是研究由竞争性非共线变磁性序引起的相变和阻挫现象的理想平台。

Abstract: We define unconventional non-collinear magnetic ground states on the maple leaf lattice (MLL) distinguished by the selective breaking or preservation of time reversal ($\mathcal{T}$) and parity ($\mathcal{P}$). Depending on the nature of $\mathcal{P}\mathcal{T}$-breaking, linear spin-wave theory reveals momentum-dependent non-relativistic magnon spin splitting at different high symmetry points in the Brillouin zone. From a mean-field analysis of the Hubbard model at weak coupling, we reveal itinerant $\mathcal{P}$-preserving $q=0$ altermagnetic (A$l$M)-type order, while we expect $\mathcal{P}$-broken canted-$120^\circ$ A$l$M-type order at strong coupling. Our findings establish the MLL as a prime platform for exploring phase transitions and frustration phenomena emanating from competing non-collinear A$l$M-type orders.

</details>


### [68] [Non-Abelian fusion and braiding in many-body parton states](https://arxiv.org/abs/2601.16819)
*Koyena Bose*

Main category: cond-mat.str-el

TL;DR: 该论文利用分数量子霍尔态中的部分子波函数构建了非阿贝尔任意子的准空穴基，验证了其融合空间维度与共形场论预测一致，并通过数值计算编织矩阵为非阿贝尔特性诊断提供了通用框架。


<details>
  <summary>Details</summary>
Motivation: 探索可用于拓扑量子计算的非阿贝尔任意子，其在分数量子霍尔态中的实现需可靠方法验证其特性。

Method: 采用部分子波函数构建准空穴基，结合共形场论与层级-秩对偶性，并进行大体系下的编织矩阵数值计算。

Result: 成功复现融合空间维度预期，建立非阿贝尔特性的诊断框架，为候选态分析提供工具。

Conclusion: 该方法有效支持非阿贝尔分数量子霍尔态的识别，推动拓扑量子信息编码的研究。

Abstract: Fractional quantum Hall (FQH) states host fractionally charged anyons with exotic exchange statistics. Of particular interest are FQH phases supporting non-Abelian anyons, which can encode topologically protected quantum information. In this work, we construct quasihole bases for a broad family of non-Abelian FQH states using parton wave functions, which reproduces the fusion-space dimensionality expected from their underlying conformal field theory, consistent with level-rank duality across the parton family. As an application, we numerically compute braiding matrices for representative parton states for large systems, providing a general framework for diagnosing non-Abelian characteristics in candidate FQH states.

</details>


### [69] [Intermediate Field Spin(on) Dynamics in $α$-RuCl$_3$](https://arxiv.org/abs/2601.16850)
*C. L. Sarkis,K. D. Dixit,P. Rao,G. Khundzakishvili,C. Balz,J-Q. Yan,B. Winn,T. J. Williams,A. Unnikrishnan,R. Moessner,D. A. Tennant,J. Knolle,S. E. Nagler,A. Banerjee*

Main category: cond-mat.str-el

TL;DR: 通过非弹性中子散射实验，在α-RuCl₃的磁场诱导无序相中观察到分数化激发存在的强有力证据，其激发谱特征与磁振子衰变模型矛盾，却与分数化激发的连续谱理论高度吻合。


<details>
  <summary>Details</summary>
Motivation: 验证Kitaev量子自旋液体候选材料α-RuCl₃在磁场中是否存在理论预测的分数化激发，并揭示其磁场诱导无序相的微观机制。

Method: 采用非弹性中子散射技术，在α-RuCl₃单晶中沿面内/面外高对称方向施加磁场（最高13.5 T），系统测量磁激发谱随磁场和动量的演化。

Result: 1. 面内磁场≥8 T时激发谱出现能隙；2. 激发峰随磁场增强而锐化但仍宽于仪器分辨率；3. 7-10 T区间面外色散减弱，呈现二维特性；4. 激发谱宽且平坦，与磁振子衰变模型矛盾，却符合分数化激发连续谱特征（可能含束缚态）。

Conclusion: 实验结果为α-RuCl₃在磁场中存在分数化激发提供了直接证据，支持Kitaev量子自旋液体在磁场下进入拓扑有序态的理论预言。

Abstract: We present comprehensive inelastic neutron spectroscopic maps of the magnetic field-induced disordered phase of the Kitaev quantum spin liquid candidate material $α$-RuCl$_3$. For fields along both in-plane high-symmetry directions we observe that the spin excitation spectrum at and above a magnetic field of 8~T is gapped. Excitation modes then sharpen for increasing field but are consistently broader than experimental resolution even at 13.5~T. The out-of-plane dispersion diminishes in the 7-10~T regime, signifying enhanced two-dimensional behavior as the in-plane liquid correlations are established. In this regime, excitations are very broad and largely flat for all accessible energy-momenta, which is kinematically at odds with a magnon-decay picture. By contrast, a continuum of fractionalized excitations naturally yields a broad continuum response, which crucially may be accompanied by sharper modes of bound states of fractionalized excitations. Their damping by the continuum accounts for the observed spectral broadening and field dependence. Our results provide strong evidence for the existence of fractionalized excitations in $α$-RuCl$_3$ in a magnetic field.

</details>


### [70] [Doping-dependent orbital magnetism in Chromium pnictides](https://arxiv.org/abs/2601.16908)
*Henri G. Mendonça,George B. Martins,Lauro B. Braz*

Main category: cond-mat.str-el

TL;DR: Electron doping in LaCrAsO drives sequential magnetic phase transitions: from commensurate antiferromagnetism at low doping to stripe-type order at intermediate levels, then to incommensurate phases at high doping, governed by orbital-selective electron localization (Cr d_{3z^2-r^2}) versus itinerancy (d_{xy}).


<details>
  <summary>Details</summary>
Motivation: To understand how electron doping alters the magnetic ground state of the parent compound LaCrAsO, particularly the transition between localized and itinerant electron magnetism.

Method: Matrix random-phase approximation (RPA) calculations to model the phase diagram under varying electron doping concentrations.

Result: Three distinct doping regimes: (1) Low doping stabilizes commensurate antiferromagnetism matching experiments; (2) Intermediate doping favors stripe-type antiferromagnetism; (3) High doping reverts to similar magnetic states but with incommensurate ordering vectors. Commensurate phases correlate with localized Cr d_{3z^2-r^2} electrons, while incommensurate phases link to itinerant d_{xy} orbital electrons.

Conclusion: Doping-induced magnetic phase evolution in LaCrAsO is controlled by orbital-dependent electron behavior, where d_{3z^2-r^2} orbital localization drives commensurate order and d_{xy} orbital itinerancy enables incommensurate magnetism.

Abstract: We present results for the phase diagram of the parent compound LaCrAsO under electron doping using the matrix random-phase approximation. At low doping levels, the system stabilizes an antiferromagnetic state in which different Cr sublattices carry opposite spins, consistent with experimental observations. As the doping concentration increases, a stripe-type antiferromagnetic phase becomes favored. At even higher doping, the system repeats the two former magnetic states, but with incommensurate magnetic ordering vectors. The commensurate magnetic phases are associated with more localized electrons in the Cr $d_{3z^2-r^2}$ orbital, whereas the incommensurate phases are linked to the $d_{xy}$ orbital, whose stronger overlap favors itinerant-electron magnetism.

</details>


### [71] [Boundary critical phenomena in the quantum Ashkin-Teller model](https://arxiv.org/abs/2601.16951)
*Yifan Liu,Natalia Chepiga,Yoshiki Fukusumi,Masaki Oshikawa*

Main category: cond-mat.str-el

TL;DR: 本研究结合边界共形场论和DMRG数值模拟，系统研究了单维量子Ashkin-Teller模型的边界临界现象，构建了稳定的边界条件，表征了四态Potts临界点，并提出了边界临界性的全局相图。


<details>
  <summary>Details</summary>
Motivation: 理解量子多体系统的边界临界行为对于揭示相变普适类和重正化群流具有重要意义。Ashkin-Teller模型作为连接不同普适类的经典模型，其边界临界性质尚未被系统研究。

Method: 采用边界共形场论框架，基于c=1紧致化玻色子的Z2轨道折叠构造微观格点边界项；利用简单电流扩展和SU(2)对称性明确表征四态Potts点；通过DMRG模拟和有限尺寸能谱分析进行数值验证。

Result: 成功构建了重正化到稳定共形边界条件的微观边界项；通过有限尺寸能谱验证了理论预测；确认了D4对称性和Kramers-Wannier对偶性；揭示了边界重正化群流并提出了完整的边界临界相图。

Conclusion: 该研究完整刻画了单维量子Ashkin-Teller模型的边界临界行为，建立了格点模型与边界共形场论的联系，为理解量子多体系统的边界相变提供了新的理论框架和数值证据。

Abstract: We investigate the boundary critical phenomena of the one-dimensional quantum Ashkin-Teller model using boundary conformal field theory and density matrix renormalization group (DMRG) simulations. Based on the $\mathbb{Z}_2$-orbifold of the $c=1$ compactified boson boundary conformal field theory, we construct microscopic lattice boundary terms that renormalize to the stable conformal boundary conditions,, utilizing simple current extensions and the underlying $\mathrm{SU}(2)$ symmetry to explicitly characterize the four-state Potts point. We validate these theoretical identifications via finite-size spectroscopy of the lattice energy spectra, confirming their consistency with $D_4$ symmetry and Kramers-Wannier duality. Finally, we discuss the boundary renormalization group flows among these identified fixed points to propose a global phase diagram for the boundary criticality.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [72] [LiDMaS: Architecture-Level Modeling of Fault-Tolerant Magic-State Injection in GKP Photonic Qubits](https://arxiv.org/abs/2601.16244)
*Dennis Delali Kwesi Wayo*

Main category: quant-ph

TL;DR: This paper studies logical T-gate magic-state preparation in GKP-encoded photonic qubits using a repeat-until-success protocol with surface-code protection. It develops a lightweight simulation framework to model realistic constraints (finite squeezing and photon loss), finding high success probabilities (>0.94) and logical fidelities of 0.77-0.80, with strong dependence on squeezing but weak sensitivity to moderate photon loss.


<details>
  <summary>Details</summary>
Motivation: Fault-tolerant quantum computation in photonic architectures requires efficient preparation of high-fidelity logical magic states under realistic hardware constraints including finite squeezing and photon loss, which are critical challenges for scalability.

Method: The authors develop an architecture-level modeling framework using a lightweight density-matrix simulator that maps finite squeezing to logical dephasing, includes depolarizing noise at the logical level, and treats photon loss as heralded erasure. They perform systematic parameter sweeps over squeezing (8-16 dB), loss probabilities (0.01-0.03), and surface-code distances (d=1,3,5,7) using a repeat-until-success injection protocol with outer surface-code protection.

Result: Success probabilities exceed 0.94 across all parameters with near-unity overhead. Logical fidelities reach 0.77-0.80 after outer-code protection, showing weak sensitivity to moderate photon loss but strong dependence on squeezing. Phase-boundary analysis identifies minimum squeezing requirements for achieving both high success probability and logical fidelity.

Conclusion: The study provides quantitative design guidance for scalable photonic fault-tolerant quantum architectures by establishing the relationship between squeezing, photon loss, and code distance requirements for efficient logical magic-state preparation.

Abstract: Fault-tolerant quantum computation in photonic architectures relies on the efficient preparation of high-fidelity logical magic states under realistic constraints imposed by finite squeezing and photon loss. In this work, we study logical T-gate magic-state preparation in GKP-encoded photonic qubits using a repeat-until-success injection protocol combined with outer surface-code protection. We develop an architecture-level modeling framework based on a lightweight density-matrix simulator implemented with standard numerical linear algebra. Finite squeezing is mapped to effective logical dephasing, depolarizing noise is included at the logical level, and photon loss is treated as a heralded erasure process. This approach avoids explicit continuous-variable wavefunction simulation, hardware-specific photonic models, and quantum software frameworks, enabling transparent and computationally efficient exploration of architectural trade-offs. We perform systematic parameter sweeps over squeezing values from 8 to 16 dB, baseline loss probabilities between 0.01 and 0.03, and surface-code distances d = 1, 3, 5, and 7. Across this regime, we evaluate repeat-until-success probability, average injection overhead, and logical magic-state fidelity. We find that success probabilities exceed 0.94 across all studied parameters, with an average overhead close to unity. After outer-code protection, logical fidelities reach approximately 0.77 to 0.80 and show weak sensitivity to moderate photon loss but a strong dependence on squeezing. Phase-boundary analysis identifies minimum squeezing requirements needed to simultaneously achieve high success probability and logical fidelity. These results provide quantitative design guidance for scalable photonic fault-tolerant quantum architectures.

</details>


### [73] [Quantum Cellular Automata on a Dual-Species Rydberg Processor](https://arxiv.org/abs/2601.16257)
*Ryan White,Vikram Ramesh,Alexander Impertro,Shraddha Anand,Francesco Cesa,Giuliano Giudici,Thomas Iadecola,Hannes Pichler,Hannes Bernien*

Main category: quant-ph

TL;DR: 该研究利用双物种里德堡原子阵列实现了量子元胞自动机，仅通过全局控制即可生成多种纠缠态，为量子计算的可扩展性提供了新方案。


<details>
  <summary>Details</summary>
Motivation: 随着量子设备规模扩大，其相干控制的可扩展性面临挑战，量子元胞自动机通过静态阵列和全局操作绕过此问题。

Method: 在铷和铯双物种里德堡原子阵列上实现量子元胞自动机，利用独立的全局控制执行量子协议，通过简单脉冲序列操作。

Result: 成功探索多体动力学，生成了包括96.7%保真度贝尔态、17量子比特簇态、高连通性图态在内的多种纠缠态。

Conclusion: 量子元胞自动机具有多功能性和可扩展性，为基于全局控制的量子信息处理系统扩展提供了可行路径，并为量子多体动力学研究带来新视角。

Abstract: As quantum devices scale to larger and larger sizes, a significant challenge emerges in scaling their coherent controls accordingly. Quantum cellular automata (QCAs) constitute a promising framework that bypasses this control problem: universal dynamics can be achieved using only a static qubit array and global control operations. We realize QCAs on a dual-species Rydberg array of rubidium and cesium atoms, leveraging independent global control of each species to perform a myriad of quantum protocols. With simple pulse sequences, we explore many-body dynamics and generate a variety of entangled states, including GHZ states, 96.7(1.7)%-fidelity Bell states, 17-qubit cluster states, and high-connectivity graph states. The versatility and scalability of QCAs offers compelling routes for scaling quantum information systems with global controls, as well as new perspectives on quantum many-body dynamics.

</details>


### [74] [Gluing Randomness via Entanglement: Tight Bound from Second Rényi Entropy](https://arxiv.org/abs/2601.16454)
*Wonjun Lee,Hyukjoon Kwon,Gil Young Cho*

Main category: quant-ph

TL;DR: 该论文揭示纠缠是局部随机酉操作生成全局随机态的核心资源，误差由第二Rényi纠缠熵决定，为随机性生成提供了基本极限，并提出了多体伪随机态生成方案。


<details>
  <summary>Details</summary>
Motivation: 高效生成随机量子态是量子信息处理的长期挑战，需要理解纠缠在将局部随机性"粘合"成全局随机态中的关键作用。

Method: 通过研究纠缠态|ψ⟩经局部随机酉操作后产生的系综，建立其与第二Rényi纠缠熵N₂(ψ)的定量关系，并推广至无相干操作和多体伪随机酉操作场景。

Result: 生成的系综形成近似态设计，误差精确为Θ(e^(-N₂(ψ)))；该界限同样适用于第二Rényi相干熵；第二Rényi熵在所有α-Rényi熵中给出最紧界限，可解释为资源无关操作生成随机性的最大容量。

Conclusion: 纠缠是生成全局随机态的关键资源，第二Rényi熵为此过程提供了根本性限制，该机制可推广至多体系统伪随机态生成，建立了资源理论与随机性生成之间的深刻联系。

Abstract: The efficient generation of random quantum states is a long-standing challenge, motivated by their diverse applications in quantum information processing tasks. In this work, we identify entanglement as the key resource that enables local random unitaries to generate global random states by effectively gluing randomness across the system. Specifically, we demonstrate that approximate random states can be produced from an entangled state $|ψ\rangle$ through the application of local random unitaries. We show that the resulting ensemble forms an approximate state design with an error saturating as $Θ(e^{-\mathcal{N}_2(ψ)})$, where $\mathcal{N}_2(ψ)$ is the second Rényi entanglement entropy of $|ψ\rangle$. Furthermore, we prove that this tight bound also applies to the second Rényi entropy of coherence when the ensemble is constructed using coherence-free operations. These results imply that, when restricted to resource-free gates, the quality of the generated random states is determined entirely by the resource content of the initial state. Notably, we find that among all $α$-Rényi entropeis, the second Rényi entropy yields the tightest bounds. Consequently, these second Rényi entropies can be interpreted as the maximal capacities for generating randomness using resource-free operations. Finally, moving beyond approximate state designs, we utilize this entanglement-assisted gluing mechanism to present a novel method for generating pseudorandom states in multipartite systems from a locally entangled state via pseudorandom unitaries in each of parties.

</details>


### [75] [Multi-invariants in stabilizer states](https://arxiv.org/abs/2601.16258)
*Sriram Akella,Abhijit Gadde,Jay Pandey*

Main category: quant-ph

TL;DR: Developing tools to calculate multipartite entanglement measures (multi-invariants) for stabilizer states, enabling efficient computation and revealing connections to topology.


<details>
  <summary>Details</summary>
Motivation: Multipartite entanglement is a crucial but poorly understood generalization of bipartite entanglement, with significant implications for quantum information processing.

Method: 1) Efficient numerical algorithm for multi-invariants of stabilizer states; 2) Explicit formula for tripartite states using GHZ-extraction theorem; 3) Counting argument for q-partite Coxeter multi-invariants with conjectured closed form; 4) Analysis of topology connections and simplification for ground states (toric code/X-cube).

Result: 1) General computational framework for multi-invariants; 2) Explicit tripartite formulas; 3) Counting method for Coxeter invariants; 4) Simplified expressions for topological model ground states; 5) Discovery of links between multi-invariants, stabilizer states, and topology.

Conclusion: The developed tools significantly advance the characterization of multipartite entanglement, providing practical computation methods and uncovering deep structural relationships between entanglement measures, quantum error-correcting codes, and topological phases of matter.

Abstract: Multipartite entanglement is a natural generalization of bipartite entanglement, but is relatively poorly understood. In this paper, we develop tools to calculate a class of multipartite entanglement measures - known as multi-invariants - for stabilizer states. We give an efficient numerical algorithm that computes multi-invariants for stabilizer states. For tripartite stabilizer states, we also obtain an explicit formula for any multi-invariant using the GHZ-extraction theorem. We then present a counting argument that calculates any Coxeter multi-invariant of a q-partite stabilizer state. We conjecture a closed form expression for the same. We uncover hints of an interesting connection between multi-invariants, stabilizer states and topology. We show how our formulas are further simplified for a restricted class of stabilizer states that appear as ground states of interesting models like the toric code and the X-cube model.

</details>


### [76] [Quantum algorithm for simulating non-adiabatic dynamics at metallic surfaces](https://arxiv.org/abs/2601.16264)
*Robert A. Lang,Paarth Jain,Juan Miguel Arrazola,Danial Motlagh*

Main category: quant-ph

TL;DR: 该论文开发了一种用于模拟分子-金属界面非绝热动力学的量子算法，通过推广Anderson-Newns哈密顿量并优化量子电路，在包含100个金属轨道、8个分子轨道和20个核自由度的模型中仅需271个量子比特，展现出在近端容错量子计算机上的应用前景。


<details>
  <summary>Details</summary>
Motivation: 分子-金属界面的非绝热动力学现象（如多相催化、染料敏化太阳能电池能量转换、分子结电荷传输）在科学和工业上具有重要意义，但经典计算方法因需耦合大量电子态与核运动而计算成本过高，难以实现真实体系模拟。

Method: 研究推广了Anderson-Newns哈密顿量，并基于PennyLane平台开发了高度优化的量子算法来模拟非绝热动力学，通过资源估算评估算法在代表性模型体系中的实现成本。

Result: 对包含100个金属轨道、8个分子轨道和20个核自由度的模型体系，模拟1000个Trotter步长的时间演化仅需271个量子比特和7.9 × 10^7个Toffoli门，资源需求显著低于预期。

Conclusion: 该量子算法具有极低的实现成本，表明非绝热分子-金属动力学模拟可作为第一代容错量子计算机的理想应用方向，为相关技术现象的研究提供了可行的量子计算解决方案。

Abstract: Non-adiabatic dynamics at molecule-metal interfaces govern diverse and technologically important phenomena, from heterogeneous catalysis to dye-sensitized solar energy conversion and charge transport across molecular junctions. Realistic modeling of such dynamics necessitates taking into account various charge and energy transfer channels involving the coupling of nuclear motion with a very large number of electronic states, leading to prohibitive cost using classical computational methods. In this work we introduce a generalization of the Anderson-Newns Hamiltonian and develop a highly optimized quantum algorithm for simulating the non-adiabatic dynamics of realistic molecule-metal interfaces. Using the PennyLane software platform, we perform resource estimations of our algorithm, showing its remarkably low implementation cost for model systems representative of various scientifically and industrially relevant molecule-metal systems. Specifically, we find that time evolution for models including $100$ metal orbitals, $8$ molecular orbitals, and $20$ nuclear degrees of freedom, requires only $271$ qubits and $7.9 \times 10^7$ Toffoli gates for $1000$ Trotter steps, suggesting non-adiabatic molecule-metal dynamics as a fruitful application of first-generation fault-tolerant quantum computers.

</details>


### [77] [Post-processing optimization and optimal bounds for non-adaptive shadow tomography](https://arxiv.org/abs/2601.16266)
*Andrea Caprotti,Joshua Morris,Borivoje Dakić*

Main category: quant-ph

TL;DR: This paper develops an optimal reconstruction method for shadow tomography using informationally overcomplete measurements, formulating coefficient selection as a convex minimax problem to achieve minimal variance bounds and reduce sampling complexity.


<details>
  <summary>Details</summary>
Motivation: Informationally overcomplete POVMs offer advantages in quantum tomography but introduce classical freedom in reconstruction; existing methods lack optimal coefficient selection for minimizing variance in shadow tomography.

Method: Formulates reconstruction coefficient selection as a convex minimax optimization problem and provides a guaranteed-convergence algorithm to compute state-independent variance bounds for fixed POVMs and observables.

Result: Numerical demonstrations show the method significantly reduces sampling complexity versus standard reconstructions and can improve scaling behavior for structured noncommuting observables as system size increases.

Conclusion: The proposed approach yields optimal, practical estimators for shadow tomography that outperform conventional methods in both efficiency and scalability for quantum measurement tasks.

Abstract: Informationally overcomplete POVMs are known to outperform minimally complete measurements in many tomography and estimation tasks, and they also leave a purely classical freedom in shadow tomography: the same observable admits infinitely many unbiased linear reconstructions from identical measurement data. We formulate the choice of reconstruction coefficients as a convex minimax problem and give an algorithm with guaranteed convergence that returns the tightest state-independent variance bound achievable by post-processing for a fixed POVM and observable. Numerical examples show that the resulting estimators can dramatically reduce sampling complexity relative to standard (canonical) reconstructions, and can even improve the qualitative scaling with system size for structured noncommuting targets.

</details>


### [78] [Engineering Near-Infrared Two-Level Systems in Confined Alkali Vapors](https://arxiv.org/abs/2601.16269)
*Gilad Orr,Golan Ben-Ari,Eliran Talker*

Main category: quant-ph

TL;DR: 使用亚微米厚度的铷原子蒸汽池，在近红外电信波段实现了受壁效应影响的两能级原子系统，为集成量子光子技术提供新方案。


<details>
  <summary>Details</summary>
Motivation: 实现紧凑、可控的电信波段光-物质相互作用对集成量子光子技术至关重要，但多能级原子的复杂动力学和光学泵浦问题阻碍了小型化原子器件的发展。

Method: 结合实验和理论研究，将热铷原子蒸汽限制在亚微米厚的电池中，通过分析吸收和荧光光谱研究原子相干性和光学响应。

Result: 在强约束条件下，光学响应由闭合循环跃迁主导，这种约束诱导的选择有效抑制了向未耦合态的光学泵浦，实现了稳健的两能级动力学。

Conclusion: 该工作为在紧凑蒸汽池器件中实现近红外原子两能级系统提供了实用途径，为片上量子存储器、电信波段频率基准和可扩展量子信息处理开辟了新机遇。

Abstract: We combined experimental and theoretical investigations of an effective two-level atomic system operating in the near-infrared telecom wavelength regime, realized using hot rubidium vapor confined within a sub-micron-thick cell. In this strongly confined geometry, atomic coherence is profoundly influenced by wall-induced relaxation arising from frequent atom-surface collisions. By analyzing both absorption and fluorescence spectra, we demonstrate that the optical response is dominated by a closed cycling transition, which effectively isolates the atomic dynamics to a two-level configuration despite the presence of multiple hyperfine states. This confinement-induced selection suppresses optical pumping into uncoupled states and enables robust, controllable light-matter interaction at telecom wavelengths within a miniature atomic platform. Our results establish a practical route to realizing near-infrared atomic two-level systems in compact vapor-cell devices, opening new opportunities for integrated quantum photonic technologies, including on-chip quantum memories, telecom-band frequency references, and scalable quantum information processing.

</details>


### [79] [Experimental observation of conformal field theory spectra](https://arxiv.org/abs/2601.16275)
*Xiangkai Sun,Yuan Le,Stephen Naus,Richard Bing-Shiun Tsai,Lewis R. B. Picard,Sara Murciano,Michael Knap,Jason Alicea,Manuel Endres*

Main category: quant-ph

TL;DR: This paper uses a Rydberg atom chain quantum simulator to directly observe energy excitation spectra of conformal field theories (CFTs) at quantum phase transitions, measuring universal energy ratios and dynamical structure factors for Ising and tricritical Ising models, providing a technique to identify unknown universality classes.


<details>
  <summary>Details</summary>
Motivation: While CFTs theoretically predict rich universal structures at quantum phase transitions (like entanglement, correlations, and low-energy spectra), much of this predicted structure remains unobserved in experiments, creating a gap between theory and experimental verification.

Method: The authors developed a modulation technique to resolve finite-size energy spectra in a Rydberg chain system, variably tuned to quantum phase transitions described by Ising and tricritical Ising CFTs. They employed local control to distinguish excitation parities and induce transitions between CFT spectra, and used a modulation variant to study the dynamical structure factor.

Result: They directly observed energy excitation spectra at quantum phase transitions and recovered universal energy ratios characteristic of the underlying CFTs. They successfully distinguished excitation parities under reflection, induced transitions between CFT spectra with changing boundary conditions, and measured the dynamical structure factor related to Ising CFT correlations.

Conclusion: This work demonstrates the emergence of CFT features in a quantum simulator and provides a practical technique for diagnosing a priori unknown universality classes in future experiments, bridging the gap between theoretical CFT predictions and experimental observations.

Abstract: Conformal field theories (CFTs) feature prominently in high-energy physics, statistical mechanics, and condensed matter. For example, CFTs govern emergent universal properties of systems tuned to quantum phase transitions, including their entanglement, correlations, and low-energy excitation spectra. Much of the rich structure predicted by CFTs nevertheless remains unobserved in experiment. Here we directly observe the energy excitation spectra of emergent CFTs at quantum phase transitions -- recovering universal energy ratios characteristic of the underlying field theories. Specifically, we develop and implement a modulation technique to resolve a Rydberg chain's finite-size spectra, variably tuned to quantum phase transitions described by either Ising or tricritical Ising CFTs. We also employ local control to distinguish parities of excitations under reflection and, in the tricritical Ising chain, to induce transitions between distinct CFT spectra associated with changing boundary conditions. By utilizing a variant of the modulation technique, we furthermore study the dynamical structure factor of the critical system, which is closely related to the correlation of an underlying Ising conformal field. Our work not only probes the emergence of CFT features in a quantum simulator, but also provides a technique for diagnosing a priori unknown universality classes in future experiments.

</details>


### [80] [Exploring Noisy Quantum Thermodynamical Processes via the Depolarizing-Channel Approximation](https://arxiv.org/abs/2601.16317)
*Jian Li,Xiaoyang Wang,Marcus Huber,Nicolai Friis,Pharnam Bakhshinezhad*

Main category: quant-ph

TL;DR: 该论文开发了一个通用框架，使用全局退极化通道来近似量子电路中的累积门依赖噪声。将其应用于热力学算法冷却（TSAC）协议，发现噪声从根本上改变了最优协议，需要有限数量的量子比特而非无限，并推导了可实现冷却性能的基本界限。


<details>
  <summary>Details</summary>
Motivation: 噪声和误差在量子过程中不可避免，会显著影响量子热力学冷却协议的性能和效率。随着系统规模增大和电路深度增加，噪声以复杂方式累积，使得分析表征变得极具挑战性。

Method: 作者提出了一个通用框架，使用全局退极化通道来近似累积的门依赖噪声，并确定了该近似适用的范围。将该框架应用于热力学双排序算法冷却（TSAC）协议。

Result: 解析推导了含噪声TSAC协议的渐近冷却极限。与无噪声情况下需要无限量子比特不同，含噪声时的最优性能由有限数量的量子比特实现，并推导了可达到的基态粒子数的基本界限。

Conclusion: 该框架为探索含噪声量子热力学过程开辟了新途径，揭示了噪声从根本上改变了最优冷却策略并施加了基本性能限制。

Abstract: Noise and errors are unavoidable in any realistic quantum process, including processes designed to reduce noise and errors in the first place. In particular, quantum thermodynamical protocols for cooling can be significantly affected, potentially altering both their performance and efficiency. Analytically characterizing the impact of such errors becomes increasingly challenging as the system size grows, particularly in deep quantum circuits where noise can accumulate in complex ways. To address this, we introduce a general framework for approximating the cumulative effect of gate-dependent noise using a global depolarizing channel. We specify the regime in which this approximation provides a reliable description of the noisy dynamics. Applying our framework to the thermodynamical two-sort algorithmic cooling (TSAC) protocol, we analytically derive its asymptotic cooling limit in the presence of noise. Using the cooling limit, the optimal cooling performance is achieved by a finite number of qubits--distinguished from the conventional noiseless TSAC protocol by an infinite number of qubits--and fundamental bounds on the achievable ground-state population are derived. This approach opens new avenues for exploring noisy quantum thermodynamical processes.

</details>


### [81] [Unambiguous randomness from a quantum state](https://arxiv.org/abs/2601.16343)
*Fionnuala Curran*

Main category: quant-ph

TL;DR: This paper introduces "unambiguous randomness" to quantify quantum measurement randomness against eavesdroppers who may return inconclusive outcomes (not just wrong guesses). It solves for 2D systems and isotropic noisy states, revealing that eavesdroppers with joint state-measurement correlations outperform state-only ones, with a critical noise threshold enabling perfect guessing.


<details>
  <summary>Details</summary>
Motivation: Traditional quantum randomness quantification assumes eavesdroppers must always guess correctly or incorrectly, ignoring realistic scenarios where they may abstain with "inconclusive" outcomes. This limits security analysis in quantum cryptography, especially for noisy practical systems.

Method: Defines unambiguous randomness (eavesdropper can output inconclusive results) and extends to fixed inconclusive rates. Solves analytically for: (1) any 2D quantum state/projective measurement, and (2) isotropically noisy states measured in unbiased bases across dimensions.

Result: For isotropic noise: (1) Eavesdroppers with joint correlations (to both noisy state and noisy measurement) always outperform those with only state correlations; (2) A critical error parameter exists where joint eavesdroppers achieve perfect guessing probability, eliminating private randomness.

Conclusion: Joint eavesdropper correlations fundamentally enhance attack capability beyond state-only models. The identified critical noise threshold provides a strict bound for quantum cryptography security, showing privacy vanishes beyond specific error rates even with inherent quantum randomness.

Abstract: Intrinsic randomness is generated when a quantum state is measured in any basis in which it is not diagonal. In an adversarial scenario, we quantify this randomness by the probability that a correlated eavesdropper could correctly guess the measurement outcomes. What if the eavesdropper is never wrong, but can sometimes return an inconclusive outcome? Inspired by analogous concepts in quantum state discrimination, we introduce the unambiguous randomness of a quantum state and measurement, and, relaxing the assumption of perfect accuracy, randomness with a fixed rate of inconclusive outcomes. We solve these problems for any state and projective measurement in dimension two, as well as for an isotropically noisy state measured in an unbiased basis of any dimension. In the latter case, we find that, given a fixed amount of total noise, an eavesdropper correlated only to the noisy state is always outperformed by an eavesdropper with joint correlations to both a noisy state and a noisy measurement. In fact, we identify a critical error parameter beyond which the joint eavesdropper achieves perfect guessing probability, ruling out any possibility of private randomness.

</details>


### [82] [Reducing TLS loss in tantalum CPW resonators using titanium sacrificial layers](https://arxiv.org/abs/2601.16369)
*Zachary Degnan,Chun-Ching Chiu,Yi-Hsun Chen,David Sommers,Leonid Abdurakhimov,Lihuang Zhu,Arkady Fedorov,Peter Jacobson*

Main category: quant-ph

TL;DR: Ultrathin titanium layer reduces tantalum oxide loss, boosting resonator quality factor 3x to >1.5 million


<details>
  <summary>Details</summary>
Motivation: Two-level system (TLS) loss at metal-air interfaces critically limits coherence in superconducting qubits; native tantalum oxide causes significant dissipation in quantum circuits

Method: Deposited 0.2nm titanium sacrificial layer on α-tantalum as oxygen getter to chemically reduce Ta oxide interface, removed layer via buffered oxide etch, followed by high-vacuum annealing

Result: Treated resonators achieved average internal quality factor Qi >1.5 million (single-photon regime), 3× higher than untreated devices; TLS loss substantially suppressed across 10 tested devices

Conclusion: Atomic-scale surface engineering via oxygen-gettering titanium layer effectively mitigates interfacial oxide loss, providing a practical fabrication-compatible method to extend qubit coherence times in tantalum quantum circuits

Abstract: We demonstrate a substantial reduction in two-level system loss in tantalum coplanar waveguide resonators fabricated on high-resistivity silicon substrates through the use of an ultrathin titanium sacrificial layer. A 0.2nm titanium film, deposited atop pre-sputtered α-tantalum, acts as a solid-state oxygen getter that chemically modifies the native Ta oxide at the metal-air interface. After device fabrication, the titanium layer is removed using buffered oxide etchant, leaving behind a chemically reduced Ta oxide surface. Subsequent high-vacuum annealing further suppresses two-level system loss. Resonators treated with this process exhibit internal quality factors Qi exceeding an average of 1.5 million in the single-photon regime across ten devices, over three times higher than otherwise identical devices lacking the titanium layer. These results highlight the critical role of interfacial oxide chemistry in superconducting loss and reinforce atomic-scale surface engineering as an effective approach to improving coherence in tantalum-based quantum circuits. The method is compatible with existing fabrication workflows applicable to tantalum films, offering a practical route to further extending T1 lifetimes in superconducting qubits.

</details>


### [83] [Low-Loss, High-Coherence Airbridge Interconnects Fabricated by Single-Step Lithography](https://arxiv.org/abs/2601.16416)
*Jibang Fu,Bo Ren,Jiandong Ouyang,Cong Li,Kechengqi Zhu,Yonggang Che,Xiang Fu,Shichuan Xue,Zhaohua Yang,Mingtang Deng,Junjie Wu*

Main category: quant-ph

TL;DR: 该研究通过单步电子束光刻技术，结合多层抗蚀剂堆栈和热回流工艺，成功制备出亚200纳米尺度的空气桥结构，在不降低超导量子比特T1弛豫时间的同时，将T2*退相干时间提升2.5倍，为量子器件中的高性能三维互连提供了实用化方案。


<details>
  <summary>Details</summary>
Motivation: 空气桥是集成电路和量子器件中实现高性能、低寄生互连的关键结构，但传统多步制备方法制约了器件小型化并引入了工艺缺陷。

Method: 采用单步电子束光刻工艺，通过优化多层抗蚀剂堆栈结构、三重曝光剂量方案以及热回流步骤，实现纳米尺度空气桥的制备。

Result: 成功制备出结构光滑、机械稳定性强的亚200纳米悬空金属桥；在超导transmon量子比特的梯度SQUID设计中集成后，未引入可测量的T1弛豫时间损耗，并将T2*退相干时间提升2.5倍。

Conclusion: 该高效制备方法为在先进量子和纳米电子器件中集成高性能三维互连结构提供了实用化途径。

Abstract: Airbridges are essential for creating high-performance, low-parasitic interconnects in integrated circuits and quantum devices. Conventional multi-step fabrication methods hinder miniaturization and introduce process-related defects. We report a simplified process for fabricating nanoscale airbridges using only a single electron-beam lithography step. By optimizing a multilayer resist stack with a triple-exposure-dose scheme and a thermal reflow step, we achieve smooth, suspended metallic bridges with sub-200-nm features that exhibit robust mechanical stability. Fabricated within a gradiometric SQUID design for superconducting transmon qubits, these airbridges introduce no measurable additional loss in the relaxation time $T_1$, while enabling a 2.5-fold enhancement of the dephasing time $T_2^*$. This efficient method offers a practical route toward integrating high-performance three-dimensional interconnects in advanced quantum and nano-electronic devices.

</details>


### [84] [Circulant quantum channels and its applications](https://arxiv.org/abs/2601.16435)
*Bing Xie,Lin Zhang*

Main category: quant-ph

TL;DR: This paper introduces circulant quantum channels (a subclass of mixed-permutation channels), proves their output is precisely circulant matrices and they are entanglement-breaking, enabling simpler analysis of quantum invariants and lower resource costs for erasing quantum correlations, with applications in coherence bounds and bipartite systems.


<details>
  <summary>Details</summary>
Motivation: To analyze structural and operational properties of a specific family of quantum channels (circulant channels) to simplify complex quantum information tasks like correlation erasure and coherence quantification.

Method: Defining circulant quantum channels as a subclass of mixed-permutation channels; characterizing their image set; proving entanglement-breaking property; applying results to Bargmann invariants, ℓ_p-norm coherence bounds, and bipartite system actions.

Result: 1) The channel's output is exactly the set of circulant matrices. 2) The channel is entanglement-breaking, drastically reducing resource costs for erasing quantum correlations. 3) Enables analysis of n-th order Bargmann invariants. 4) Provides tighter lower bounds for ℓ_p-norm coherence and characterizes bipartite system behavior.

Conclusion: Circulant quantum channels offer a mathematically tractable subclass with unique advantages (entanglement-breaking property, circulant output) that simplify quantum information processing tasks and provide practical improvements over general mixed-permutation channels.

Abstract: This note introduces a family of circulant quantum channels -- a subclass of the mixed-permutation channels -- and investigates its key structural and operational properties. We show that the image of the circulant quantum channel is precisely the set of circulant matrices. This characterization facilitates the analysis of arbitrary $n$-th order Bargmann invariants. Furthermore, we prove that the channel is entanglement-breaking, implying a substantially reduced resource cost for erasing quantum correlations compared to a general mixed-permutation channel. Applications of this channel are also discussed, including the derivation of tighter lower bounds for $\ell_p$-norm coherence and a characterization of its action in bipartite systems.

</details>


### [85] [Quantum phase estimation with optimal confidence interval using three control qubits](https://arxiv.org/abs/2601.16474)
*Kaur Kristjuhan,Dominic W. Berry*

Main category: quant-ph

TL;DR: 提出高效制备量子相位估计中离散长球序列态的方法，用键维度4的矩阵乘积态近似，大幅降低资源开销，适用于早期容错量子计算机


<details>
  <summary>Details</summary>
Motivation: 量子相位估计是量子化学模拟的核心算法，但传统方法制备最优控制态（离散长球序列）效率低下，难以在资源受限的量子硬件上实现

Method: 采用键维度仅为4的矩阵乘积态（MPS）近似目标态，并通过简单三量子比特操作序列高效制备该态

Result: 在维度高达2^24的测试中，该MPS近似保持高精度；当维度为2的幂时，控制寄存器仅需3个量子比特即可完成相位估计

Conclusion: 该方法显著降低量子相位估计的资源需求，使早期容错量子计算机实现高精度量子化学模拟成为可能

Abstract: Quantum phase estimation is an important routine in many quantum algorithms, particularly for estimating the ground state energy in quantum chemistry simulations. This estimation involves applying powers of a unitary to the ground state, controlled by an auxiliary state prepared on a control register. In many applications the goal is to provide a confidence interval for the phase estimate, and optimal performance is provided by a discrete prolate spheroidal sequence. We show how to prepare the corresponding state in a far more efficient way than prior work. We find that a matrix product state representation with a bond dimension of 4 is sufficient to give a highly accurate approximation for all dimensions tested, up to $2^{24}$. This matrix product state can be efficiently prepared using a sequence of simple three-qubit operations. When the dimension is a power of 2, the phase estimation can be performed with only three qubits for the control register, making it suitable for early-generation fault-tolerant quantum computers with a limited number of logical qubits.

</details>


### [86] [The optimal strategy of two-photon interferometric sensing in diverse noise environments](https://arxiv.org/abs/2601.16517)
*Teng-fei Yan,Zhuo-zhuo Wang,Qi-qi Li,Peng-long Wang,Bai-hong Li,Rui-Bo Jin*

Main category: quant-ph

TL;DR: Analyzing noise sensitivity in two-photon quantum interferometry: HOM interferometry is noise-insensitive while N00N state is noise-sensitive, with spectrally resolved detection providing superior performance.


<details>
  <summary>Details</summary>
Motivation: Quantum sensing via two-photon interferometry achieves quantum advantage beyond classical limits, but this superiority is degraded by noise. Understanding how different interferometry schemes respond to noise is crucial for optimizing practical applications.

Method: Analytical comparison of sensitivity to phase noise between Hong-Ou-Mandel (HOM) interferometry and N00N state interferometry under both spectrally non-resolved and resolved detection conditions.

Result: HOM interference (biphoton frequency difference dependent) is insensitive to phase noise, while N00N state interferometry (biphoton frequency sum dependent) is highly sensitive. Spectrally resolved detection outperforms non-resolved detection for both, especially beyond biphoton coherence time.

Conclusion: These findings enable optimal selection of interferometry scheme and detection method for practical quantum sensing applications across diverse noise environments.

Abstract: Quantum sensing based on two-photon interferometry manifests quantum superiority beyond the classical precision limit. However, this superiority is usually diminished inevitably by the noise. Here, we analyze the sensitivity of two typical two-photon interferometries to the noise, that is, Hong-Ou-Mandel (HOM) and N00N state interferometry. It is found that HOM (N00N state) interference, which depends on the biphoton frequency difference (sum), is insensitive (sensitive) to the phase noise in both the manners of spectrally non-resolved and resolved detections in practice, suggesting their potential applications of sensing for different noise scenarios. Furthermore, spectrally resolved detection outperforms spectrally non-resolved one for the two interferometries, especially for the scope that exceeds the coherence time of biphotons. The findings provide an optimal strategy for the practical applications of two-photon interferometric sensing in diverse noise environments.

</details>


### [87] [Drive-Through Quantum Gate: Non-Stop Entangling a Mobile Ion Qubit with a Stationary One](https://arxiv.org/abs/2601.16537)
*Ting Hsu,Wen-Han Png,Kuan-Ting Lin,Ming-Shien Chang,Guin-Dar Lin*

Main category: quant-ph

TL;DR: Proposing a trapped-ion quantum computing scheme where mobile ions remain in constant uniform motion during entanglement operations with stationary ions, eliminating shuttling-induced heating and enabling scalable architectures.


<details>
  <summary>Details</summary>
Motivation: Conventional QCCD architectures for trapped-ion quantum computers suffer from severe motional heating and cooling delays when ions are shuttled between trap zones, significantly increasing operation time and laser power requirements.

Method: Theoretical demonstration of an entangling gate scheme between stationary memory ions and continuously moving communication ions in uniform motion, avoiding acceleration/deceleration phases that cause heating.

Result: Achieves predicted gate error rates of ~0.01% – comparable to state-of-the-art stationary gates – while maintaining ion mobility, enabling resource-efficient long-distance entanglement distribution.

Conclusion: This motion-tolerant entanglement protocol establishes a viable alternative to QCCD architectures, using mobile ions as communication qubits and stationary arrays as memory units, advancing scalable trapped-ion quantum computing.

Abstract: Towards the scalable realization of a quantum computer, a quantum charge-coupled device (QCCD) based on ion shuttling has been considered a promising approach. However, the processes of detaching an ion from an array, reintegrating it, and driving non-uniform motion introduce severe heating, requiring significant time and laser power for re-cooling and stabilization. To mitigate these challenges, we propose a novel entangling scheme between a stationary ion qubit and a continuously transported mobile ion, which remains in uniform motion and minimizes motional heating. We theoretically demonstrate a gate error on the order of 0.01%, within reach of current technology. This approach enables resource-efficient quantum operations and facilitates long-distance entanglement distribution, where stationary trapped-ion arrays serve as memory units and mobile ions act as communication qubits passing beside them. Our results pave the way for an alternative trapped-ion architecture beyond the QCCD paradigm.

</details>


### [88] [Quantum graph resonances by cut-off technique](https://arxiv.org/abs/2601.16545)
*Pavel Exner,Jiří Lipovský,Jan Pekař*

Main category: quant-ph

TL;DR: 该论文提出了通过截断系统特征值行为来识别量子图共振的方法。


<details>
  <summary>Details</summary>
Motivation: 量子图的共振现象对理解量子输运和散射过程至关重要，需要可靠的识别方法。

Method: 利用截断系统的离散谱分析，观察其特征值随系统参数变化的规律来识别共振。

Result: 成功建立了截断系统特征值行为与完整量子图共振之间的对应关系。

Conclusion: 该方法为量子图共振分析提供了可行的理论框架。

Abstract: We demonstrate how resonances in a quantum graph consisting of a compact core and semi-infinite leads can be identified from the eigenvalue behavior of the cut-off system.

</details>


### [89] [Certification of quantum properties with imperfect measurements](https://arxiv.org/abs/2601.16570)
*Leonardo Zambrano,Teodor Parella-Dilmé,Antonio Acín,Donato Farina*

Main category: quant-ph

TL;DR: Proposes a robust quantum state certification method that jointly accounts for statistical shot noise and measurement imperfections using convex optimization to bound convex functions of quantum states under real experimental conditions.


<details>
  <summary>Details</summary>
Motivation: Existing quantum state certification methods inadequately address real-world experimental noise (shot noise and measurement imperfections), which is critical for advancing quantum technologies requiring accurate convex function certification of quantum states.

Method: Extends confidence regions to incorporate measurement control errors; uses convex optimization to bound target functions; provides explicit noise quantification for finite statistics and measurement imperfections.

Result: A unified certification framework that jointly integrates statistical and systematic errors, enabling robust experimental validation of quantum states with quantified uncertainty from both noise sources.

Conclusion: This method provides a practical and reliable approach for certifying quantum states in imperfect real-world experiments, essential for developing trustworthy quantum technologies.

Abstract: The accurate characterization of quantum systems is essential for the advancement of quantum technologies. In particular, certifying convex functions of quantum states plays a central role in many applications. We present a certification method for experimentally prepared quantum states that accounts for both shot noise and measurement imperfections in the data-acquisition stage. Building upon previous work, our method extends confidence regions to accommodate imperfect control over measurements. The values of the functions can then be bounded using convex optimization techniques. We provide explicit prescriptions for quantifying the noise contribution from finite statistics and for estimating the effect of measurement imperfections. By jointly incorporating statistical and systematic errors, the method yields a robust certification framework for quantum experiments.

</details>


### [90] [Charging of a Quantum Battery by a Single-Photon Quantum Pulse](https://arxiv.org/abs/2601.16671)
*Elnaz Darsheshdar,Seyed Mostafa Moniri*

Main category: quant-ph

TL;DR: This paper analyzes a minimal quantum battery model where a two-level system (TLS) charger transfers energy from a single-photon pulse to a harmonic oscillator battery. The authors derive analytical solutions showing optimal pulses saturate a universal energy bound, establish a quantum speed limit at an exceptional point, and identify conditions for maximum charging power.


<details>
  <summary>Details</summary>
Motivation: Understanding fundamental limits on energy storage and charging speed in quantum systems, and developing optimal protocols for efficient quantum battery operation by exploring the interplay between coherent energy transfer and environmental decay.

Method: Theoretical modeling of a TLS coupled to a harmonic oscillator battery, excited by a tailored single-photon pulse. The authors solve the system dynamics analytically, accounting for TLS decay into both the pulse mode and electromagnetic environment, then optimize pulse shapes and durations.

Result: Analytical dynamics reveal: (1) optimal pulse shape maximizes stored energy; (2) this optimal pulse saturates a universal bound set by decay rates; (3) minimum charging time exists with quantum speed limit at exceptional point; (4) analytical power expressions show optimal pulse duration for maximum charging power.

Conclusion: Quantum battery performance is fundamentally limited by system-environment coupling, with optimal charging protocols achieving universal bounds. The exceptional point marks a critical transition that sets the ultimate speed limit, providing design principles for efficient quantum energy storage devices.

Abstract: We study a minimal model for charging a quantum battery consisting of a two-level system (TLS) acting as a charger, coupled to a harmonic oscillator that serves as the quantum battery. A single-photon quantum pulse of light excites the TLS, which subsequently transfers its excitation to the isolated battery. The TLS may also decay into the electromagnetic environment. We obtain analytical solutions for the dynamics of the battery and determine the optimal pulse shape that maximizes the stored energy. The optimal pulse saturates a universal bound for the stored energy, determined by the TLS decay rates into the pulse and the environment. Furthermore, we derive the minimum charging time and establish a quantum speed limit at the exceptional point, where a critical transition occurs in the system's dynamics. We also present analytical expressions for the charging power and investigate the pulse duration that maximizes it.

</details>


### [91] [Classical Regularization in Variational Quantum Eigensolvers](https://arxiv.org/abs/2601.16679)
*Yury Chernyak,Ijaz Ahamed Mohammad,Martin Plesch*

Main category: quant-ph

TL;DR: 该研究证明，在变分量子本征求解器(VQE)中引入L2正则化可有效稳定优化过程并提升性能，且无需修改量子电路。


<details>
  <summary>Details</summary>
Motivation: 变分量子算法(VQAs)受到贫瘠高原和病态优化景观的困扰，导致收敛不稳定且对初始条件高度敏感，这限制了其实际应用。

Method: 在不改变量子电路或测量过程的前提下，通过在VQE目标函数中增加与参数平方范数成正比的二次惩罚项（L2正则化）来稳定优化。

Result: 在H2、LiH和随机场伊辛模型(RFIM)的测试中，L2正则化在较宽的正则化强度范围内均表现出性能提升，显示出强大的稳定化效果。

Conclusion: 经典正则化为缓解VQE不稳定性提供了一种鲁棒、系统无关的机制，可在不改变底层量子电路的情况下增强变分量子优化的可靠性和可重复性。

Abstract: While quantum computers are a very promising tool for the far future, in their current state of the art they remain limited both in size and quality. This has given rise to hybrid quantum-classical algorithms, where the quantum device performs only a small but vital part of the overall computation. Among these, variational quantum algorithms (VQAs), which combine a classical optimization procedure with quantum evaluation of a cost function, have emerged as particularly promising. However, barren plateaus and ill-conditioned optimization landscapes remain among the primary obstacles faced by VQAs, often leading to unstable convergence and high sensitivity to initialization. Motivated by this challenge, we investigate whether a purely classical remedy, standard L2 squared-norm regularization, can systematically stabilize hybrid quantum-classical optimization. Specifically, we augment the Variational Quantum Eigensolver (VQE) objective with a quadratic penalty proportional to the squared norm of the parameters, without modifying the quantum circuit or measurement process. Across all tested Hamiltonians, H2, LiH, and the Random Field Ising Model (RFIM), we observe improved performance over a broad window of the regularization strength. Our large-scale numerical results demonstrate that classical regularization provides a robust, system-independent mechanism for mitigating VQE instability, enhancing the reliability and reproducibility of variational quantum optimization without altering the underlying quantum circuit.

</details>


### [92] [Sparsity-dependent Complexity Lower Bound of Quantum Linear System Solvers](https://arxiv.org/abs/2601.16697)
*Hitomi Mori,Yuta Kikuchi,Marcello Benedetti,Matthias Rosenkranz*

Main category: quant-ph

TL;DR: This paper rigorously proves a lower bound of Ω(κ√s) for quantum linear system solvers, confirming the folklore belief about sparsity dependence and advancing toward complete complexity characterization.


<details>
  <summary>Details</summary>
Motivation: While the optimal query complexity Ω(κ log(1/ε)) for quantum linear system solvers is known, the folklore lower bound Ω(κ√s log(1/ε)) that includes sparsity lacks rigorous proof, hindering complete understanding of QLS complexity parameters.

Method: The authors develop a rigorous proof technique that explicitly captures the sparsity dependence of quantum algorithms, extending beyond previous lower bound analyses that only considered condition number and error parameters.

Result: They prove that any quantum algorithm solving linear systems with constant error requires Ω(κ√s) queries, providing the first rigorous lower bound incorporating sparsity dependence, though full dependence on ε remains open.

Conclusion: This result serves as a crucial stepping stone toward complete characterization of QLS complexity, clarifying sparsity's role while identifying that incorporating the error parameter dependence remains a key challenge for future research.

Abstract: Quantum linear system (QLS) solvers are a fundamental class of quantum algorithms used in many potential quantum computing applications, including machine learning and solving differential equations. The performance of quantum algorithms is often measured by their query complexity, which quantifies the number of oracle calls required to access the input. The main parameters determining the complexity of QLS solvers are the condition number $κ$ and sparsity $s$ of the linear system, and the target error $ε$. To date, the best known query-complexity lower bound is $Ω(κ\log(1/ε))$, which establishes the optimality of the most recent QLS solvers. The original proof of this lower bound is attributed to Harrow and Kothari, but their result is unpublished. Furthermore, when discussing a more general lower bound including the sparsity $s$ of the linear system, it has become folklore that it should read as $Ω( κ\sqrt{s}\log(1/ε))$. In this work, we establish the rigorous lower bound capturing the sparsity dependence of QLS. We prove the lower bound of $Ω(κ\sqrt{s})$ for any quantum algorithm that solves QLS with constant error. While the dependence on all parameters $κ,s,ε$ remains an open problem, our result provides a crucial stepping stone toward the complete characterization of QLS complexity.

</details>


### [93] [Entanglement harvesting in the presence of cavities](https://arxiv.org/abs/2601.16698)
*Jannik Ströhle,Nikolija Momcilovic*

Main category: quant-ph

TL;DR: This paper studies entanglement harvesting inside a cylindrical cavity, revealing dependencies on cavity length, light cone regimes, and field parity.


<details>
  <summary>Details</summary>
Motivation: Previous research on entanglement harvesting focused on free space setups, leaving cavity environments less explored.

Method: Analytical and numerical analysis of two identical Gaussian detectors adiabatically coupled to the quantized electromagnetic field along the symmetry axis of a cylindrical cavity.

Result: Strong dependence on cavity length, invariance to cavity radius in maximal entanglement regimes, different parameter scalings inside/outside the light cone, and dependence on cavity-induced field parity.

Conclusion: Cavity geometry and field parity significantly influence harvested entanglement, revealing new scaling behaviors and dependencies not present in free space setups.

Abstract: So far, entanglement harvesting has been extensively studied in free space setups. Here, we provide a detailed analytical and numerical analysis of entanglement harvesting in cavities. Specifically, we adiabatically couple the quantized electromagnetic field to two identical Gaussian detectors located on the symmetry axis of a cylindrical cavity. Our numerical investigations reveal a strong dependence on the cavity length, while showing invariance under changes in the cavity radius in regimes of maximal entanglement. Moreover, we identify different scalings of the detector system parameters for entanglement inside and outside the light cone. Finally, we uncover a strong dependence of the harvested correlations on the cavity induced parity of the electromagnetic field.

</details>


### [94] [Noise Resilience and Robust Convergence Guarantees for the Variational Quantum Eigensolver](https://arxiv.org/abs/2601.16758)
*Mirko Legnini,Julian Berberich*

Main category: quant-ph

TL;DR: 该论文研究了噪声对变分量子本征求解器(VQE)收敛性的影响，理论分析了相干和非相干噪声对最优参数和代价函数的影响，并结合PennyLane数值模拟验证了收敛性保证的变化。


<details>
  <summary>Details</summary>
Motivation: 虽然VQE在理想条件下已有全局收敛保证，但实际量子硬件存在噪声，亟需研究噪声如何影响算法收敛性，这对近期量子设备的实际应用至关重要。

Method: 通过理论分析不同噪声过程对VQE最优参数和最优代价的影响，研究其对算法收敛保证的作用机制，并使用PennyLane进行数值模拟验证。

Result: 提供了参数化量子电路在噪声下行为的新颖理论洞察，刻画了噪声对VQE收敛性的具体影响规律。

Conclusion: 该研究深化了对噪声环境下VQE算法鲁棒性的理解，为开发面向含噪声量子硬件的实用量子算法提供了理论基础。

Abstract: Variational Quantum Algorithms (VQAs) are a class of hybrid quantum-classical algorithms that leverage on classical optimization tools to find the optimal parameters for a parameterized quantum circuit. One relevant application of VQAs is the Variational Quantum Eigensolver (VQE), which aims at steering the output of the quantum circuit to the ground state of a certain Hamiltonian. Recent works have provided global convergence guarantees for VQEs under suitable local surjectivity and smoothness hypotheses, but little has been done in characterizing convergence of these algorithms when the underlying quantum circuit is affected by noise. In this work, we characterize the effect of different coherent and incoherent noise processes on the optimal parameters and the optimal cost of the VQE, and we study their influence on the convergence guarantees of the algorithm. Our work provides novel theoretical insight into the behavior of parameterized quantum circuits. Furthermore, we accompany our results with numerical simulations implemented via Pennylane.

</details>


### [95] [Investigating Retargetability Claims for Quantum Compilers](https://arxiv.org/abs/2601.16779)
*Luke Southall,Joshua Ammermann,Rinor Kelmendi,Domenik Eichhorn,Ina Schaefer*

Main category: quant-ph

TL;DR: 评估三款量子编译器(Tket、Qiskit、ProjectQ)的可重定向性，发现Tket表现最佳


<details>
  <summary>Details</summary>
Motivation: NISQ时代量子硬件架构多样化导致量子软件跨平台移植困难，现有编译器可重定向性缺乏系统评估标准

Method: 开发可重定向性评估指标，对Tket、Qiskit、ProjectQ三款编译器进行实证研究

Result: Tket可重定向性最高，Qiskit紧随其后，ProjectQ相对落后

Conclusion: 为量子开发者选择编译器提供依据，并指出编译器改进方向

Abstract: In the NISQ-era, there is a wide variety of hardware manufacturers building quantum computers. Each of these companies may choose different approaches and hardware architectures for their machines. This poses a problem for quantum software engineering, as the retargetability of quantum programs across different hardware platforms becomes a non-trivial challenge. In response to this problem, various retargetable quantum compilers have been presented in the scientific literature. These promise the ability to compile software for different hardware platforms, enabling retargetability for quantum software. In this paper, we develop and apply a metric by which the retargetability of the quantum compilers can be assessed. We develop and run a study to analyze key aspects regarding the retargetability of the compilers Tket, Qiskit, and ProjectQ. Our findings indicate that Tket demonstrates the highest level of retargetability, closely followed by Qiskit, while ProjectQ lags behind. These results provide insights for quantum software developers in selecting appropriate compilers for their use-cases, and highlight areas for improvement in quantum compilers.

</details>


### [96] [Harnessing Quantum Computing for Energy Materials: Opportunities and Challenges](https://arxiv.org/abs/2601.16816)
*Seongmin Kim,In-Saeng Suh,Travis S. Humble,Thomas Beck,Eungkyu Lee,Tengfei Luo*

Main category: quant-ph

TL;DR: 量子计算有望突破经典计算在高维能源材料模拟中的限制，该综述分析了量子计算在能源材料领域的应用机遇与挑战，提出量子-经典混合方法，并展望容错量子计算实现预测精度和量子优势的前景。


<details>
  <summary>Details</summary>
Motivation: 开发高性能能源材料对提升效率、可持续性和降低成本至关重要。经典计算方法在高维或强关联体系面临扩展性限制，量子计算通过叠加和纠缠特性为解决经典难题提供新范式。

Method: 作为综述文章，系统分析量子计算在能源材料研究中的应用前景与挑战，展示量子-经典混合计算方法在材料设计与模拟中的案例，并展望容错量子计算的发展路径。

Result: 提出量子计算与经典方法结合可解决能源材料设计和模拟中的实际问题，分析了当前量子计算在复杂高维问题中面临的技术挑战，并描绘了实现量子优势的技术路线图。

Conclusion: 量子计算与经典计算融合为能源材料研究带来变革性机遇，未来容错量子计算有望在复杂材料体系中实现预测精度和量子优势，推动能源材料领域突破。

Abstract: Developing high-performance materials is critical for diverse energy applications to increase efficiency, improve sustainability and reduce costs. Classical computational methods have enabled important breakthroughs in energy materials development, but they face scaling and time-complexity limitations, particularly for high-dimensional or strongly correlated material systems. Quantum computing (QC) promises to offer a paradigm shift by exploiting quantum bits with their superposition and entanglement to address challenging problems intractable for classical approaches. This perspective discusses the opportunities in leveraging QC to advance energy materials research and the challenges QC faces in solving complex and high-dimensional problems. We present cases on how QC, when combined with classical computing methods, can be used for the design and simulation of practical energy materials. We also outline the outlook for error-corrected, fault-tolerant QC capable of achieving predictive accuracy and quantum advantage for complex material systems.

</details>


### [97] [Protocols to share genuine multipartite entanglement employing copies of biseparable states](https://arxiv.org/abs/2601.16840)
*Swati Choudhary,Ujjwal Sen,Saronath Halder*

Main category: quant-ph

TL;DR: A protocol using two copies of biseparable three-qutrit states generates genuine multipartite entanglement with nonzero probability, contrasting with three-qubit systems requiring many copies; it also activates stronger genuinely nonlocal correlations without joint measurements.


<details>
  <summary>Details</summary>
Motivation: Genuine multipartite entanglement activation (using biseparable states entangled across all bipartitions) is crucial for quantum information processing, but efficiency varies significantly between systems (e.g., three-qubit vs. three-qutrit).

Method: Proposed a protocol for three-qutrit systems using two copies of rank-two biseparable states (entangled across all bipartitions), generalized to arbitrary parties, without requiring joint measurements on the copies.

Result: Two copies suffice to generate genuine multipartite entanglement with nonzero probability in three-qutrit systems, unlike three-qubit scenarios needing many copies; the protocol naturally activates genuinely nonlocal correlations (stronger than entanglement activation alone).

Conclusion: Demonstrated efficient genuine multipartite entanglement activation in higher-dimensional systems (qutrits) and revealed that the process inherently produces stronger nonlocal correlations, advancing practical quantum protocol design.

Abstract: Sharing genuine multipartite entanglement by considering collective use of copies of biseparable states, which are entangled across all bipartitions but lack genuine multipartite entanglement at the single-copy level, plays a central role in several quantum information processing protocols, and has been referred as genuine multipartite entanglement activation. We present a protocol for three-qutrit systems showing that two copies of rank-two biseparable states, entangled across every bipartition, are sufficient to generate a genuinely multipartite entangled state with nonzero probability. This contrasts with the three-qubit scenario where many copies of biseparable states might be required for sharing genuine multipartite entanglement. We subsequently generalize our protocols to the case of an arbitrary number of parties. Our protocol does not rely on the implementation of joint measurements on the copies of states. Interestingly, the proposed construction naturally leads to the activation of genuinely nonlocal correlations, yielding a result that is stronger than genuine multipartite entanglement activation alone.

</details>


### [98] [Generation of fully phase controlled two-photon entangled states](https://arxiv.org/abs/2601.16875)
*Ian Ford,Adrien Amour,Matthias Keller*

Main category: quant-ph

TL;DR: 研究人员利用单个囚禁的钙离子耦合光学腔，通过两步光子发射方案生成双光子纠缠态，并通过调节驱动场实现了相位的完全控制，保真度高达82%，展示了资源高效的量子态制备方法。


<details>
  <summary>Details</summary>
Motivation: 囚禁离子是理想的量子系统，能够产生单光子和双光子态。实现高效、可控的纠缠光子对生成对量子信息处理至关重要，特别是需要精确控制相位以及提高资源利用效率。

Method: 使用单个40Ca+离子耦合到光学腔。首先产生离子-光子纠缠，随后发射第二个光子并将离子态映射到该光子上，通过调节驱动场实现纠缠态相位的完全控制，同时采用保护方案维持离子-腔相互作用的相干性。

Result: 成功生成了具有完全相位控制的双光子纠缠态，保真度达到82%。该方案在资源利用上最为高效，仅需单个离子和光学腔即可实现。

Conclusion: 该工作展示了一种实用且资源高效的双光子纠缠态制备方法，为可扩展量子信息处理提供了有力工具，证明了囚禁离子系统在量子技术中的实际应用潜力。

Abstract: Control over the internal states of trapped ions makes them the ideal system to generate single and two-photon states. Coupling a single ion to an optical cavity enables efficient emission of single photons into a single spatial mode and grants control over their temporal shape, phase and frequency. Using the long coherence time of the ion's internal states and employing a scheme to protect the coherence of the ion-cavity interaction, we demonstrate the generation of a two-photon entangled state with full control over the phase. Initially, ion-photon entanglement is generated. A second photon is subsequently generated, mapping the ion's state onto the second photon. By adjusting the drive field the phase of the entangled state can be fully controlled. We implement this scheme in the most resource efficient way by utilizing a single $^{40}$Ca$^+$ ion coupled to an optical cavity and demonstrate the generation of a two-photon entangled stated with full phase control with a fidelity of up to 82\%.

</details>


### [99] [Upper bounds on the purity of Wigner positive quantum states that verify the Wigner entropy conjecture](https://arxiv.org/abs/2601.16898)
*Qipeng Qian,Christos Gagatsos*

Main category: quant-ph

TL;DR: This paper proves partial results toward the Wigner entropy conjecture by establishing explicit purity-based sufficient conditions under which Wigner entropy is minimized by pure Gaussian states, revealing why additional physical constraints are necessary for extremal cases.


<details>
  <summary>Details</summary>
Motivation: To verify the Wigner entropy conjecture which claims that pure Gaussian states minimize Wigner entropy (attaining 1+lnπ) among all physical Wigner non-negative states, using only fundamental constraints (non-negativity, normalization, and πW≤1).

Method: Constructs a hierarchy of analytical lower bounds Bₙ on Wigner entropy by combining truncated series expansions for -lnx with Wigner function moment identities, deriving purity-dependent sufficient conditions for the conjecture.

Result: 1) Proves all states with purity μ≤4-2√3 satisfy the conjecture; 2) Derives a relaxed universal condition μ≤2/e; 3) Explains why purity-only approaches fail for extremal cases (μ≤1) without additional physical constraints.

Conclusion: The work provides concrete analytical evidence for the conjecture through purity thresholds and clarifies the necessity of supplementary physical constraints beyond purity to characterize the extremal Gaussian states.

Abstract: We present analytical results toward the Wigner entropy conjecture, which posits that among all physical Wigner non-negative states the Wigner entropy is minimized by pure Gaussian states for which it attains the value $1+\lnπ$.Working under a minimal set of constraints on the Wigner function, namely, non-negativity, normalization, and the pointwise bound $πW\le 1$, we construct an explicit hierarchy of lower bounds $B_n$ on $S[W]$ by combining a truncated series lower bound for $-\ln x$ with moment identities of the Wigner function.This yields closed-form purity-based sufficient conditions ensuring $S[W]\ge 1+\lnπ$.In particular, we first prove that all Wigner non-negative states with $μ\le 4-2\sqrt3$ satisfy the Wigner entropy conjecture. We further obtain a systematic purity-only relaxation of the hierarchy, yielding the simple sufficient condition $μ\le 2/e$. On top of aforesaid results, our analysis clarifies why additional physicality constraints are necessary for purity-based approaches that aim to approach the extremal case $μ\leq1$.

</details>


### [100] [Quantum Fisher information analysis for absorption measurements with undetected photons](https://arxiv.org/abs/2601.16941)
*Martin Houde,Franz Roeder,Christine Silberhorn,Benjamin Brecht,Nicolás Quesada*

Main category: quant-ph

TL;DR: This paper compares quantum Fisher information (QFI) across three absorption spectroscopy configurations with undetected idler photons to determine optimal measurement regimes under different loss and gain conditions


<details>
  <summary>Details</summary>
Motivation: To theoretically identify which quantum spectroscopy architecture (SU(1,1), IC, or DL) maximizes quantum Fisher information - the fundamental limit for measurement precision - under varying experimental constraints of loss and parametric gain

Method: Calculated quantum Fisher information (QFI) as function of parametric gain for three configurations (SU(1,1) interferometer, induced-coherence setup, distributed-loss scheme), comparing both full detection and signal-only access scenarios under controlled loss conditions

Result: SU(1,1) provides highest QFI for low losses (<99%) and moderate gain; IC scheme excels at high gain with intermediate loss; DL configuration becomes optimal only under extreme attenuation (<1% transmission)

Conclusion: The optimal quantum spectroscopy architecture depends critically on experimental conditions: SU(1,1) for most practical scenarios, IC for high-gain intermediate-loss setups, and DL only for near-total signal loss - providing a theoretical roadmap for designing precision quantum sensors

Abstract: We theoretically compare the quantum Fisher information (QFI) for three configurations of absorption spectroscopy with undetected idler photons: an SU(1,1) interferometer with inter-source idler loss, an induced-coherence (IC) setup in which the idler partially seeds a second squeezer together with a vacuum ancilla, and a distributed-loss (DL) scheme with in-medium attenuation. We calculate the QFI as a function of parametric gain for both full and signal-only detection access. For losses below 99% and low to moderate gain, the SU(1,1) configuration provides the largest QFI. At high gain and intermediate loss, the IC scheme performs best, while under extreme attenuation (transmission $<$ 1%) the DL model becomes optimal. These results delineate the measurement regimes in which each architecture is optimal in terms of information theory.

</details>


### [101] [Experimental investigation of nonclassicality in the simplest scenario via the degrees of freedom of light](https://arxiv.org/abs/2601.16952)
*João M. M. Gama,Guilherme T. C. Cruz,Massy Khoshbin,Lorenzo Catani,José A. O. Huguenin,Wagner F. Balthazar*

Main category: quant-ph

TL;DR: 本研究通过经典光实验模拟非经典性，利用偏振和横模自由度实现四制备-二测量场景，验证了噪声鲁棒不等式的破坏，揭示了制备非语境性和有界本体论可区分性的不一致性，为量子随机接入码等应用提供了实验基础。


<details>
  <summary>Details</summary>
Motivation: 探索经典光如何模拟非经典性现象，并将其应用于量子随机接入码等通信原语中的半设备无关认证和计算优势。

Method: 利用偏振和厄米-高斯横模两种光自由度，实现包含四个制备和两个二元结果测量的实验方案；通过全光学装置模拟去极化信道来建模实验噪声。

Result: 实验结果与Khoshbin等一致，在测量构成层析完备集的前提下，观测统计违反了噪声鲁棒不等式，表明制备非语境性和有界本体论可区分性存在不一致。

Conclusion: 尽管使用经典光，实验成功复现了简单场景的预测统计，该实现直接关联于量子随机接入码等应用中的非经典性认证。

Abstract: In this work, we experimentally investigate the classical-light emulation of different notions of nonclassicality in the simplest scenario. We implement this prepare-and-measure scenario involving four preparations and two binary-outcome measurements using two distinct experimental setups that exploit different degrees of freedom of light: polarization and first-order Hermite-Gaussian transverse modes. We additionally model experimental noise through an all-optical setup that reproduces the operational effect of a depolarizing channel. Our experimental results are consistent with the findings of Khoshbin et al. [Phys. Rev. A 109, 032212 (2024)]: under the assumption that the two measurements performed form a tomographically complete set, the observed statistics violate their noise-robust inequalities, indicating inconsistencies with preparation noncontextuality and bounded ontological distinctness for preparations. Although our implementation uses classical light, it reproduces the statistics predicted for the simplest scenario. Since the states and measurements of this scenario underpin computational advantages in tasks such as two-bit quantum random access codes -- among the simplest communication primitives enabling semi-device-independent certification of nonclassicality -- our implementation is directly relevant for such applications.

</details>


### [102] [Engineering discrete local dynamics in globally driven dual-species atom arrays](https://arxiv.org/abs/2601.16961)
*Francesco Cesa,Andrea Di Fini,David Aram Korbany,Roberto Tricarico,Hannes Bernien,Hannes Pichler,Lorenzo Piroli*

Main category: quant-ph

TL;DR: A method to engineer discrete quantum cellular automata in dual-species neutral atom systems using global analog controls, enabling study of chaotic many-body dynamics.


<details>
  <summary>Details</summary>
Motivation: To bridge analog quantum simulation with digital quantum models by engineering discrete local dynamics in globally-driven systems, exploiting dual-species neutral atom platforms to realize quantum cellular automata that are difficult to access directly.

Method: Uses species-alternated Floquet protocols on static atom arrangements, leveraging generalized blockade regimes (different inter- and intra-species interactions) to implement discrete dynamical models via uniform analog controls.

Result: Demonstrates construction of specific quantum cellular automata examples (kicked-Ising model, Floquet Kitaev honeycomb model, digitization of generic Hamiltonians) and shows chaotic features of discretized many-body dynamics can be detected using demonstrated experimental capabilities.

Conclusion: Provides a pathway to study quantum chaos and discrete dynamical models in analog quantum simulators, benchmarking the ability to discriminate chaotic evolution in neutral atom systems and opening new possibilities for exploring quantum information dynamics.

Abstract: We introduce a method for engineering discrete local dynamics in globally-driven dual-species neutral atom experiments, allowing us to study emergent digital models through uniform analog controls. Leveraging the new opportunities offered by dual-species systems, such as species-alternated driving, our construction exploits simple Floquet protocols on static atom arrangements, and benefits of generalized blockade regimes (different inter- and intra-species interactions). We focus on discrete dynamical models that are special examples of Quantum Cellular Automata (QCA), and explicitly consider a number of relevant examples, including the kicked-Ising model, the Floquet Kitaev honeycomb model, and the digitization of generic translation-invariant nearest-neighbor Hamiltonians (e.g., for Trotterized evolution). As an application, we study chaotic features of discretized many-body dynamics that can be detected by leveraging only demonstrated capabilities of globally-driven experiments, and benchmark their ability to discriminate chaotic evolution.

</details>


### [103] [Autonomous Optical Alignment of Satellite-Based Entanglement Sources using Reinforcement Learning](https://arxiv.org/abs/2601.16968)
*Andrzej Gajewski,Robert Okuła,Marcin Pawłowski,Akshata Shenoy H*

Main category: quant-ph

TL;DR: 针对卫星量子通信中纠缠源因轨道动态条件易失准的问题，本文提出并模拟验证了两种无需人工干预的自动校准技术：启发式算法和强化学习方法，其中强化学习在效率和性能上均显著优于启发式算法。


<details>
  <summary>Details</summary>
Motivation: 量子纠缠通过卫星分发可实现全球尺度量子通信，但星载纠缠源在动态轨道条件下易发生失准，影响通信质量。传统手动校准方式无法满足卫星自动化运行需求，亟需开发高效、自主的校准技术。

Method: 研究提出两种针对周期极化铌酸锂(PPLN)基自发参量下转换(SPDC)纠缠源的自动校准方案：1)启发式算法(HA)，模拟实验室手动对准过程；2)强化学习(RL)方法。通过仿真对比两种方法的性能表现。

Result: 仿真结果表明，强化学习方法在改进的ROC分析中表现显著更优，AUC值达0.9119，而启发式算法仅为0.7042。在60分钟阈值内，RL可在10分钟内实现完美对准，而HA需要30分钟。

Conclusion: 两种方法均在可行的卫星约束条件下运行，为复杂量子通信场景提供了可扩展的自动化解决方案，其中强化学习方法在效率和性能方面更具优势，有望应用于实际星载量子通信系统。

Abstract: Quantum entanglement distributed via satellites enable global-scale quantum communication. However, onboard sources are susceptible to misalignment due to dynamical orbital conditions. Here, we present two recalibration techniques for efficient generation of high quality entanglement using a periodically poled lithium niobate (PPLN)-based spontaneous parametric down-conversion (SPDC) source with minimum intervention. The first is a heuristic algorithm (HA) which mimics the manual alignment process in a laboratory. The second is based on reinforcement learning (RL). Our simulation demonstrates superior performance of RL with AUC=0.9119 compared to HA's 0.7042 in the modified ROC analysis (60 min threshold). RL achieves perfect alignment in 10 min as opposed to HA's 30 min. Both the methods operate within feasible satellite constraints, offering scalable automation for complex quantum communication scenarios.

</details>


### [104] [Formalising an operational continuum limit of quantum combs](https://arxiv.org/abs/2601.16974)
*Clara Wassner,Jonáš Fuksa,Jens Eisert,Gregory A. L. White*

Main category: quant-ph

TL;DR: 本文通过将离散多体Choi态嵌入玻色福克空间，构建了一个完全连续的进程张量框架，实现了对多时量子关联的场论处理，弥合了离散量子信息理论与连续物理过程之间的概念鸿沟。


<details>
  <summary>Details</summary>
Motivation: 量子梳状结构作为研究多时量子过程和非马尔可夫开放量子系统的强大工具，缺乏与连续时间演化的严格物理联系，存在概念上的不兼容性，亟需建立连续化的理论框架。

Method: 作者将离散多体Choi态转化为玻色福克空间中的场论态，系统性地建立了完全连续的进程张量框架，并阐述了其核心结构元素与性质，进而通过连续矩阵乘积态表征多时关联。

Result: 该框架为在连续域中研究量子随机过程提供了严格的数学基础，使得能够以任意时间分辨率对多时量子关联进行信息论分析。

Conclusion: 本研究成功填补了量子信息文献中的理论空白，建立了与连续时间兼容的进程张量形式体系，为将多体物理洞见应用于量子随机过程研究开辟了新途径，深化了对非马尔可夫动力学的理解。

Abstract: Quantum combs are powerful conceptual tools for capturing multi-time processes in quantum information theory, constituting the most general quantum mechanical process. But, despite their causal nature, they lack a meaningful physical connection to time -- and are, by and large, arguably incompatible with it without extra structure. The subclass of quantum combs which assumes an underlying process is described by the so-called process tensor framework, which has been successfully used to study and characterise non-Markovian open quantum systems. But, although process tensors are motivated by an underlying dynamics, it is not a priori clear how to connect to a continuous process tensor object mathematically -- leaving an uncomfortable conceptual gap. In this work, we take a decisive step toward remedying this situation. We introduce a fully continuous process tensor framework by showing how the discrete multi-partite Choi state becomes a field-theoretic state in bosonic Fock space, which is intrinsically and rigorously defined in the continuum. With this equipped, we lay out the core structural elements of this framework and its properties. This translation allows for an information-theoretic treatment of multi-time correlations in the continuum via the analysis of their continuous matrix product state representatives. Our work closes a gap in the quantum information literature, and opens up the opportunity for the application of many-body physics insights to our understanding of quantum stochastic processes in the continuum.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [105] [Fractals in rate-induced tipping](https://arxiv.org/abs/2601.16373)
*Jason Qianchuan Wang,Yi Zheng,Eduardo G. Altmann*

Main category: nlin.CD

TL;DR: 该研究揭示了在非线性系统中，非吸引的分形集会引发参数空间的分形结构，从而控制速率诱导的临界跃迁现象，拓展了传统仅关注简单不稳定轨道的理论框架


<details>
  <summary>Details</summary>
Motivation: 现有速率诱导临界跃迁理论主要基于简单不稳定轨道（边缘态），但实际非线性系统中普遍存在的非吸引分形集如何影响这一现象尚不明确

Method: 通过分析分段线性一维映射、二维Hénon映射和受迫摆三种典型系统，建立相空间分形集与参数空间分形结构的关联，并研究其分形维数关系

Result: 发现相空间中的非吸引分形集会在参数空间中诱导出分形结构，这些分形结构决定了导致系统跃迁的临界速率和参数变化模式

Conclusion: 非吸引分形集的存在显著改变了速率诱导临界跃迁的图景，该理论框架适用于广泛非线性系统，为预测快速参数变化下的系统行为提供了新工具

Abstract: When parameters of a dynamical system change sufficiently fast, critical transitions can take place even in the absence of bifurcations. This phenomenon is known as rate-induced tipping and has been reported in a variety of systems, from simple ordinary differential equations and maps to mathematical models in climate sciences and ecology. In most examples, the transition happens at a critical rate of parameter change, a rate-induced tipping point, and is associated with a simple unstable orbit (edge state). In this work, we show how this simple picture changes when non-attracting fractal sets exist in the autonomous system, a ubiquitous situation in non-linear dynamics. We show that these fractals in phase space induce fractals in parameter space, which control the rates and parameter changes that result in tipping. We explain how such rate-induced fractals appear and how the fractal dimensions of the different sets are related to each other. We illustrate our general theory in three paradigmatic systems: a piecewise linear one-dimensional map, the two-dimensional Hénon map, and a forced pendulum.

</details>
