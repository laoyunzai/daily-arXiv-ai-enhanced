{"id": "2602.01234", "categories": ["nlin.CD"], "pdf": "https://arxiv.org/pdf/2602.01234", "abs": "https://arxiv.org/abs/2602.01234", "authors": ["R. Shashangan", "S. Sudharsan", "Dibakar Ghosh", "M. Senthilvelan"], "title": "Controlling extreme events in neuronal networks: A single driving signal approach", "comment": "13 pages, 12 figures, accepted for publication in \"Chaos: An interdisciplinary journal of nonlinear science\", 2026", "summary": "We show that in a drive-response coupling framework extreme events are suppressed in the response system by the dominance of a single driving signal. We validate this approach across three distinct response network topologies, namely (i) a pair of coupled neurons, (ii) a monolayer network of N coupled neurons and (iii) a two-layer multiplex network each composed of FitzHugh-Nagumo neuronal units. The response networks inherently exhibit extreme events. Our results demonstrate that influencing just one neuron in the response network with an appropriately tuned driving signal is sufficient to control extreme events across all three configurations. In the two-neuron case, suppression of extreme events occurs due to the breaking of phase-locking between the driving neuron and the targeted response neuron. In the case of monolayer and multiplex networks, suppression of extreme events results from the disruption of protoevent frequency dynamics and a subsequent frequency decoupling of the driven neuron from the rest of the network. We also observe that when the size of the neurons in response network connected to the drive increases, the onset of control occurs earlier indicating a scaling advantage of the method.", "AI": {"tldr": "A single appropriately tuned driving signal can suppress extreme events across various neural network topologies by disrupting synchronization mechanisms, with larger connected neuron clusters enabling faster control.", "motivation": "Seeking efficient control strategies for extreme events in neural systems by testing whether minimal intervention (targeting one neuron) can suppress system-wide extreme events across different network configurations.", "method": "Using drive-response coupling framework with FitzHugh-Nagumo neuronal models across three topologies: (1) two coupled neurons, (2) monolayer N-neuron network, (3) two-layer multiplex network. Validated suppression by applying a single driving signal to one neuron in the response system.", "result": "Extreme events were suppressed in all configurations: (i) in paired neurons via phase-locking disruption, (ii) in monolayer/multiplex networks via protoevent frequency decoupling. Larger neuron clusters connected to the drive accelerated control onset, showing scaling advantage.", "conclusion": "Targeting a single neuron with a tuned driving signal is sufficient to suppress extreme events system-wide across diverse neural network topologies, offering an efficient control strategy with scalability benefits."}}
{"id": "2602.01402", "categories": ["nlin.CD"], "pdf": "https://arxiv.org/pdf/2602.01402", "abs": "https://arxiv.org/abs/2602.01402", "authors": ["Muhammad Junaid-U-Rehman", "Grzegorz Kudra", "Krystian Polczy\u0144ski", "Kevin Dekemele", "Jan Awrejcewicz"], "title": "Analytical and numerical study of a parametrically excited 2DOF oscillator with nonlinear restoring magnetic force and rotating rectangular rod", "comment": "Preprint submitted to Nonlinear Dynamics", "summary": "This study investigates a detailed analytical and numerical investigation of a nonlinear two-degree-of-freedom (2DOF) mechanical oscillator subjected to parametric excitation, magnetic stiffness nonlinearities, and dry friction. The considered system consists of two coupled oscillators, both of which are connected to a rotating rectangular beam that induces a time-periodic stiffness variation. The Complex Averaging (CxA) method is employed to derive approximate analytical solutions, which are thoroughly validated through time-domain simulations and bifurcation analyses. The dynamic analysis reveals a rich spectrum of nonlinear behaviors, including periodic, quasi-periodic, and chaotic responses. Detailed bifurcation diagrams, Lyapunov exponent analysis, and Poincar\u00e9 maps demonstrate the influence of nonlinear stiffness degree, mass symmetry, and frictional effects on system stability and response amplitude. The obtained results give a significant understanding of the dynamic behavior of coupled nonlinear systems and establish a conceptual framework for the development of complex vibration abatement strategies, energy harvesting devices, and advanced mechanical systems.", "AI": {"tldr": "\u5bf9\u542b\u53c2\u6570\u6fc0\u52b1\u3001\u78c1\u975e\u7ebf\u6027\u521a\u5ea6\u548c\u5e72\u6469\u64e6\u7684\u975e\u7ebf\u6027\u4e8c\u81ea\u7531\u5ea6\u673a\u68b0\u632f\u8361\u5668\u8fdb\u884c\u89e3\u6790\u4e0e\u6570\u503c\u7814\u7a76\uff0c\u63ed\u793a\u5176\u4e30\u5bcc\u7684\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u884c\u4e3a\u5e76\u5e94\u7528\u4e8e\u51cf\u632f\u4e0e\u80fd\u91cf\u6536\u96c6\u3002", "motivation": "\u7406\u89e3\u8026\u5408\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u52a8\u6001\u884c\u4e3a\uff0c\u4e3a\u5f00\u53d1\u590d\u6742\u632f\u52a8\u6291\u5236\u7b56\u7565\u3001\u80fd\u91cf\u6536\u96c6\u88c5\u7f6e\u53ca\u5148\u8fdb\u673a\u68b0\u7cfb\u7edf\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u91c7\u7528\u590d\u5e73\u5747\u6cd5\uff08CxA\uff09\u63a8\u5bfc\u8fd1\u4f3c\u89e3\u6790\u89e3\uff0c\u5e76\u901a\u8fc7\u65f6\u57df\u4eff\u771f\u548c\u5206\u5c94\u5206\u6790\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u53d1\u73b0\u7cfb\u7edf\u5b58\u5728\u5468\u671f\u3001\u51c6\u5468\u671f\u548c\u6df7\u6c8c\u54cd\u5e94\uff1b\u5206\u5c94\u56fe\u3001\u674e\u96c5\u666e\u8bfa\u592b\u6307\u6570\u548c\u5e9e\u52a0\u83b1\u6620\u5c04\u663e\u793a\u975e\u7ebf\u6027\u521a\u5ea6\u7a0b\u5ea6\u3001\u8d28\u91cf\u5bf9\u79f0\u6027\u548c\u6469\u64e6\u6548\u5e94\u5bf9\u7a33\u5b9a\u6027\u548c\u632f\u5e45\u7684\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6df1\u5316\u4e86\u5bf9\u8026\u5408\u975e\u7ebf\u6027\u7cfb\u7edf\u52a8\u529b\u5b66\u7684\u7406\u89e3\uff0c\u4e3a\u632f\u52a8\u63a7\u5236\u548c\u80fd\u91cf\u6536\u96c6\u88c5\u7f6e\u7684\u8bbe\u8ba1\u5efa\u7acb\u4e86\u6982\u5ff5\u6846\u67b6\u3002"}}
{"id": "2602.01575", "categories": ["nlin.CD"], "pdf": "https://arxiv.org/pdf/2602.01575", "abs": "https://arxiv.org/abs/2602.01575", "authors": ["Caterina Mosto", "Gisela D. Char\u00f3", "Christophe Letellier", "Denisse Sciamarella"], "title": "Templex-based dynamical units for a taxonomy of chaos", "comment": "16 pages, 16 figures", "summary": "Discriminating different types of chaos is still a very challenging topic, even for dissipative three-dimensional systems for which the most advanced tool is the template. Nevertheless, getting a template is, by definition, limited to three-dimensional objects, since based on knot theory. To deal with higher-dimensional chaos, we recently introduced the templex combining a flow-oriented {\\sc BraMAH} cell complex and a directed graph (a digraph). There is no dimensional limitation in the concept of templex. Here, we show that a templex can be automatically reduced into a ``minimal'' form to provide a comprehensive and synthetic view of the main properties of chaotic attractors. This reduction allows for the development of a taxonomy of chaos in terms of two elementary units: the oscillating unit (O-unit) and the switching unit (S-unit). We apply this approach to various well-known attractors (R\u00f6ssler, Lorenz, and Burke-Shaw) as well as a non-trivial four-dimensional attractor. A case of toroidal chaos (Deng) is also treated. This work is dedicated to Otto E. R\u00f6ssler.", "AI": {"tldr": "Proposes a dimension-unlimited \"templex\" framework combining BraMAH cell complexes and directed graphs to classify chaos via minimal forms with two elementary units (O-unit/S-unit), validated on 3D/4D attractors.", "motivation": "Traditional template-based chaos discrimination fails in >3D systems due to knot theory limitations; no universal method exists for higher-dimensional chaotic attractor classification.", "method": "Introduces \"templex\" (flow-oriented BraMAH cell complex + digraph), reduces it to minimal form, and establishes chaos taxonomy using oscillating units (O-unit) and switching units (S-unit) as elementary building blocks.", "result": "Successfully applies the templex reduction to R\u00f6ssler, Lorenz, Burke-Shaw, a 4D attractor, and Deng's toroidal chaos, demonstrating universal classification capability across dimensions.", "conclusion": "Templex provides a dimension-agnostic, synthetic framework for chaos taxonomy, resolving limitations of 3D-constrained templates and enabling systematic analysis of complex/high-dimensional chaotic systems."}}
{"id": "2602.00374", "categories": ["cond-mat.stat-mech", "cond-mat.dis-nn", "nlin.AO"], "pdf": "https://arxiv.org/pdf/2602.00374", "abs": "https://arxiv.org/abs/2602.00374", "authors": ["K. Duplat", "A. Douin", "O. Ramos"], "title": "Memory effects govern scale-free dynamics beyond universality classes", "comment": "9 pages, 7 figures", "summary": "Scale-invariant avalanches -- with events of all sizes following power-law distributions -- are considered critical. Above the upper critical dimension of four, the mean-field solution with a robust $3/2$ size exponent describes the dynamics. In two and three dimensions, spatial constraints yield smaller yet robust exponent values governed by universality classes. However, both earthquake data and experiments often show exponent values larger than $3/2$, challenging those theoretical arguments based on critical behavior. Through extensive simulations in the classical OFC earthquake model, here we show a clear transition from the theoretical expected behavior of a robust exponent value, to a regime of quasi-critical dynamics with larger than $3/2$ exponents that depend on dissipation. While the first critical regime exhibits an inherently memoryless behavior, both the transition and the second regime are driven by memory effects provoked by the growth of avalanches over the traces left by previous events, due to dissipative mechanisms. The system hovers at a distance $d_{cp}$ from the critical point, and accounting for a power-law distribution of $d_{cp}$, validated by susceptibility measurements, captures the transition. This framework provides a unified description of both critical and quasi-critical behavior, and thus of the full spectrum of scale-free dynamics observed in nature.", "AI": {"tldr": "\u5730\u9707\u6a21\u578b\u6a21\u62df\u63ed\u793a\u7cfb\u7edf\u5b58\u5728\u4e34\u754c\u6001\uff08\u6307\u65703/2\uff09\u4e0e\u51c6\u4e34\u754c\u6001\uff08\u6307\u6570>3/2\uff09\u7684\u8f6c\u53d8\uff0c\u540e\u8005\u7531\u8017\u6563\u673a\u5236\u5f15\u53d1\u7684\u8bb0\u5fc6\u6548\u5e94\u9a71\u52a8\uff0c\u7edf\u4e00\u89e3\u91ca\u4e86\u7406\u8bba\u4e0e\u89c2\u6d4b\u7684\u6307\u6570\u5dee\u5f02\u3002", "motivation": "\u7406\u8bba\u4e0e\u5b9e\u9a8c\u5b58\u5728\u77db\u76fe\uff1a\u4e34\u754c\u7406\u8bba\u9884\u6d4b\u5730\u9707\u4e8b\u4ef6\u89c4\u6a21\u6307\u6570\u5e94\u4e3a3/2\uff0c\u4f46\u5b9e\u9645\u89c2\u6d4b\u6570\u636e\u5e38\u663e\u793a\u66f4\u5927\u6307\u6570\uff0c\u6311\u6218\u4e86\u57fa\u4e8e\u4e34\u754c\u884c\u4e3a\u7684\u4f20\u7edf\u89e3\u91ca\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u503c\u6a21\u62df\u7ecf\u5178OFC\u5730\u9707\u6a21\u578b\uff0c\u5206\u6790\u4e0d\u540c\u7ef4\u5ea6\u4e0b\u7684\u96ea\u5d29\u52a8\u529b\u5b66\uff0c\u5e76\u6d4b\u91cf\u654f\u611f\u6027\u4ee5\u9a8c\u8bc1\u4e34\u754c\u70b9\u8ddd\u79bb\u7684\u5e42\u5f8b\u5206\u5e03\u3002", "result": "\u53d1\u73b0\u4e24\u79cd\u52a8\u529b\u5b66 regime\uff1a1\uff09\u7406\u8bba\u9884\u671f\u7684\u4e34\u754c\u6001\uff08\u6307\u65703/2\uff0c\u65e0\u8bb0\u5fc6\u6027\uff09\uff1b2\uff09\u8017\u6563\u673a\u5236\u9a71\u52a8\u7684\u51c6\u4e34\u754c\u6001\uff08\u6307\u6570>3/2\uff0c\u4f9d\u8d56\u5386\u53f2\u4e8b\u4ef6\u7684\u8bb0\u5fc6\u6548\u5e94\uff09\uff0c\u7cfb\u7edf\u4e34\u754c\u70b9\u8ddd\u79bbd_cp\u5448\u5e42\u5f8b\u5206\u5e03\u3002", "conclusion": "\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\uff1a\u7cfb\u7edf\u901a\u8fc7d_cp\u7684\u5e42\u5f8b\u5206\u5e03\u52a8\u6001\u60ac\u6d6e\u4e8e\u4e34\u754c\u70b9\u9644\u8fd1\uff0c\u540c\u65f6\u89e3\u91ca\u4e34\u754c\u4e0e\u51c6\u4e34\u754c\u884c\u4e3a\uff0c\u4e3a\u81ea\u7136\u754c\u4e2d\u5c3a\u5ea6\u65e0\u5173\u52a8\u529b\u5b66\u63d0\u4f9b\u5b8c\u6574\u63cf\u8ff0\u3002"}}
{"id": "2602.01592", "categories": ["nlin.CD"], "pdf": "https://arxiv.org/pdf/2602.01592", "abs": "https://arxiv.org/abs/2602.01592", "authors": ["Gisela D. Char\u00f3", "Christophe Letellier", "Denisse Sciamarella"], "title": "Templex: a bridge between homologies and templates for chaotic attractors", "comment": "17 pages, 19 figures", "summary": "The theory of homologies introduces cell complexes to provide an algebraic description of spaces up to topological equivalence. Attractors in state space can be studied using Branched Manifold Analysis through Homologies: this strategy constructs a cell complex from a cloud of points in state space and uses homology groups to characterize its topology. The approach, however, does not consider the action of the flow on the cell complex. The procedure is here extended to take this fundamental property into account, as done with templates. The goal is achieved endowing the cell complex with a directed graph that prescribes the flow direction between its highest dimensional cells. The tandem of cell complex and directed graph, baptized templex, is shown to allow for a sophisticated characterization of chaotic attractors and for an accurate classification of them. The cases of a few well-known chaotic attractors are investigated -- namely the spiral and funnel R\u00f6ssler attractors, the Lorenz attractor and the Burke and Shaw attractor. A link is established with their description in terms of templates.", "AI": {"tldr": "This paper extends Branched Manifold Analysis through Homologies by incorporating flow dynamics via directed graphs on cell complexes (termed \"templex\"), enabling sophisticated characterization and classification of chaotic attractors like R\u00f6ssler and Lorenz systems.", "motivation": "Existing homology-based methods for analyzing attractors construct cell complexes from state space point clouds but ignore the critical action of the flow on these complexes, limiting topological characterization.", "method": "Proposes \"templex\" \u2013 a cell complex augmented with a directed graph prescribing flow directions between highest-dimensional cells \u2013 to integrate flow dynamics into homology analysis.", "result": "Templex enables advanced characterization of chaotic attractors (e.g., R\u00f6ssler spiral/funnel, Lorenz, Burke-Shaw) and establishes equivalence with template-based descriptions.", "conclusion": "The templex framework provides a robust topological classification tool for chaotic attractors by unifying homology analysis with flow dynamics, bridging gaps in existing methods."}}
{"id": "2602.01393", "categories": ["cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2602.01393", "abs": "https://arxiv.org/abs/2602.01393", "authors": ["Marco Benedetti", "Giulia Fischetti", "Enzo Marinari", "Gleb Oshanin", "Victor Dotsenko"], "title": "Inferring Concepts from Noisy Examples in Hopfield-like Neural Networks", "comment": null, "summary": "We study a variant of the pseudo-inverse learning rule for Hopfield-like Neural Networks, which allows the network to infer archetypal concepts on the basis of a limited number of examples. The mean-field replica theory for this model reveals how this generalization ability is mediated by a multitude of states, with diverse thermodynamic properties, coexisting with the standard Hopfield ones. They appear and vanish through smooth transitions or discontinuous jumps and, interestingly, show much stronger Replica Symmetry Breaking (RSB) effects than the standard Hopfield model, as captured by our 1RSB analysis. Our results, in excellent agreement with numerical simulations, provide deeper insight into the interplay between memory storage and generalization in attractor neural networks.", "AI": {"tldr": "This paper analyzes a modified pseudo-inverse learning rule for Hopfield networks, revealing how generalization from few examples involves diverse states with stronger replica symmetry breaking than standard models.", "motivation": "To understand how Hopfield-like neural networks can generalize archetypal concepts from limited examples using a variant of the pseudo-inverse learning rule.", "method": "Mean-field replica theory and 1RSB (one-step replica symmetry breaking) analysis, validated by numerical simulations.", "result": "The model exhibits multiple coexisting states beyond standard Hopfield solutions, appearing through smooth or discontinuous transitions, with significantly stronger RSB effects. Numerical simulations confirm theoretical predictions.", "conclusion": "The study provides deeper insight into the interplay between memory storage and generalization in attractor neural networks, showing how generalization is mediated by complex state structures."}}
{"id": "2602.00141", "categories": ["physics.data-an", "cs.CV", "cs.LG", "hep-ex"], "pdf": "https://arxiv.org/pdf/2602.00141", "abs": "https://arxiv.org/abs/2602.00141", "authors": ["Daeun Kim", "Jiwon Lee", "Wonjun Jeong", "Hyeongwoo Noh", "Giyeong Kim", "Jaeyoon Cho", "Geonhee Kwak", "Seunghwan Yang", "MinJung Kweon"], "title": "Comparison of Image Processing Models in Quark Gluon Jet Classification", "comment": "17 pages, 10 Figures", "summary": "We present a comprehensive comparison of convolutional and transformer-based models for distinguishing quark and gluon jets using simulated jet images from Pythia 8. By encoding jet substructure into a three-channel representation of particle kinematics, we evaluate the performance of convolutional neural networks (CNNs), Vision Transformers (ViTs), and Swin Transformers (Swin-Tiny) under both supervised and self-supervised learning setups. Our results show that fine-tuning only the final two transformer blocks of the Swin-Tiny model achieves the best trade-off between efficiency and accuracy, reaching 81.4% accuracy and an AUC (area under the ROC curve) of 88.9%. Self-supervised pretraining with Momentum Contrast (MoCo) further enhances feature robustness and reduces the number of trainable parameters. These findings highlight the potential of hierarchical attention-based models for jet substructure studies and for domain transfer to real collision data.", "AI": {"tldr": "\u6bd4\u8f83CNN\u548cTransformer\u6a21\u578b\u5728\u533a\u5206\u5938\u514b/\u80f6\u5b50\u55b7\u6ce8\u56fe\u50cf\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u5fae\u8c03Swin-Tiny\u6700\u540e\u4e24\u5c42\u5728\u6548\u7387\u548c\u7cbe\u5ea6\u4e0a\u6700\u4f18\uff0c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8fdb\u4e00\u6b65\u63d0\u5347\u9c81\u68d2\u6027\u3002", "motivation": "\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u55b7\u6ce8\u5b50\u7ed3\u6784\u5206\u6790\u65b9\u6cd5\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63d0\u5347\u5938\u514b\u4e0e\u80f6\u5b50\u55b7\u6ce8\u7684\u533a\u5206\u7cbe\u5ea6\uff0c\u4e3a\u771f\u5b9e\u5bf9\u649e\u673a\u6570\u636e\u5e94\u7528\u63d0\u4f9b\u57fa\u7840\u3002", "method": "\u4f7f\u7528Pythia 8\u6a21\u62df\u7684\u4e09\u901a\u9053\u55b7\u6ce8\u56fe\u50cf\u6570\u636e\uff0c\u5bf9\u6bd4\u8bc4\u4f30CNN\u3001ViT\u548cSwin-Tiny\u5728\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u4e0b\u7684\u6027\u80fd\uff0c\u91c7\u7528MoCo\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "result": "\u5fae\u8c03Swin-Tiny\u6700\u540e\u4e24\u5c42\u8fbe\u5230\u6700\u4f73\u5e73\u8861\uff1a\u51c6\u786e\u738781.4%\uff0cAUC 88.9%\uff1bMoCo\u9884\u8bad\u7ec3\u589e\u5f3a\u7279\u5f81\u9c81\u68d2\u6027\u5e76\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u3002", "conclusion": "\u5c42\u6b21\u5316\u6ce8\u610f\u529b\u6a21\u578b\uff08\u5982Swin-Tiny\uff09\u5728\u55b7\u6ce8\u5b50\u7ed3\u6784\u7814\u7a76\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u81ea\u76d1\u7763\u5b66\u4e60\u53ef\u6709\u6548\u8fc1\u79fb\u5230\u771f\u5b9e\u5bf9\u649e\u6570\u636e\u9886\u57df\u3002"}}
{"id": "2602.00230", "categories": ["cond-mat.stat-mech", "cond-mat.str-el", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.00230", "abs": "https://arxiv.org/abs/2602.00230", "authors": ["R. Jafari", "Henrik Johannesson", "Sebastian Eggert"], "title": "Topological Defects from Quantum Reset Dynamics", "comment": "6 pages with 3 figures. More information and the latest version can be found at https://www.physik.uni-kl.de/eggert/papers/", "summary": "We analyze mechanisms for universal out-of-equilibrium dynamics near criticality by exploring the effect of randomized quantum resetting (QR) under a finite-time quench across a quantum phase transition. Using the transverse-field Ising chain as a generic model and exploiting its exact solution, QR is found to cause a crossover of the scaling of the topological defect density with the time scale $\u03c4$ of the quench, from Kibble-Zurek to anti-Kibble-Zurek scaling as $\u03c4$ increases. This reflects a competition between non-adiabatic quench-driven excitations and QR, giving rise to local minima of the defect densities at optimal annealing times. These times and the corresponding local minima are shown to scale as universal power laws with the rate of QR. Additional results for the scaling of the mean excess energy suggest that a system driven across a quantum critical point exhibits the same scaling behavior under a linear quench with QR as with uncorrelated noise.", "AI": {"tldr": "\u91cf\u5b50\u91cd\u7f6e\u673a\u5236\u5728\u91cf\u5b50\u76f8\u53d8\u6dec\u706b\u8fc7\u7a0b\u4e2d\u5f15\u53d1\u62d3\u6251\u7f3a\u9677\u5bc6\u5ea6\u7684Kibble-Zurek\u5230anti-Kibble-Zurek\u6807\u5ea6\u5f8b\u4ea4\u53c9\uff0c\u5e76\u4ea7\u751f\u6700\u4f18\u9000\u706b\u65f6\u95f4\u7684\u666e\u9002\u5e42\u5f8b\u6807\u5ea6", "motivation": "\u63a2\u7d22\u91cf\u5b50\u4e34\u754c\u70b9\u9644\u8fd1\u975e\u5e73\u8861\u52a8\u529b\u5b66\u7684\u666e\u9002\u673a\u5236\uff0c\u7814\u7a76\u968f\u673a\u91cf\u5b50\u91cd\u7f6e\u5bf9\u76f8\u53d8\u6dec\u706b\u8fc7\u7a0b\u4e2d\u62d3\u6251\u7f3a\u9677\u5f62\u6210\u7684\u5f71\u54cd", "method": "\u91c7\u7528\u7cbe\u786e\u53ef\u89e3\u7684\u6a2a\u573a\u4f0a\u8f9b\u94fe\u6a21\u578b\uff0c\u5728\u6709\u9650\u65f6\u95f4\u6dec\u706b\u4e2d\u5f15\u5165\u968f\u673a\u91cf\u5b50\u91cd\u7f6e\u673a\u5236\uff0c\u5206\u6790\u7f3a\u9677\u5bc6\u5ea6\u4e0e\u6dec\u706b\u65f6\u95f4\u5c3a\u5ea6\u03c4\u7684\u6807\u5ea6\u5173\u7cfb", "result": "\u53d1\u73b0\u91cf\u5b50\u91cd\u7f6e\u5bfc\u81f4\u7f3a\u9677\u5bc6\u5ea6\u6807\u5ea6\u5f8b\u4eceKibble-Zurek\u5411anti-Kibble-Zurek\u8f6c\u53d8\uff1b\u5b58\u5728\u6700\u4f18\u9000\u706b\u65f6\u95f4\u4f7f\u7f3a\u9677\u5bc6\u5ea6\u51fa\u73b0\u5c40\u57df\u6781\u5c0f\u503c\uff0c\u8be5\u6781\u5c0f\u503c\u4e0e\u91cd\u7f6e\u901f\u7387\u5448\u666e\u9002\u5e42\u5f8b\u5173\u7cfb\uff1b\u8fc7\u91cf\u80fd\u91cf\u6807\u5ea6\u8868\u660e\u91cf\u5b50\u91cd\u7f6e\u4e0e\u65e0\u5173\u8054\u566a\u58f0\u5177\u6709\u76f8\u540c\u6807\u5ea6\u884c\u4e3a", "conclusion": "\u91cf\u5b50\u91cd\u7f6e\u901a\u8fc7\u7ade\u4e89\u975e\u7edd\u70ed\u6fc0\u53d1\u4e0e\u91cd\u7f6e\u6548\u5e94\uff0c\u53ef\u8c03\u63a7\u91cf\u5b50\u4e34\u754c\u6dec\u706b\u52a8\u529b\u5b66\uff0c\u4e3a\u63a7\u5236\u62d3\u6251\u7f3a\u9677\u63d0\u4f9b\u65b0\u673a\u5236\uff0c\u4e14\u5176\u6807\u5ea6\u884c\u4e3a\u5177\u6709\u666e\u9002\u6027"}}
{"id": "2602.00242", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.00242", "abs": "https://arxiv.org/abs/2602.00242", "authors": ["Charles A. Cardot", "John Tichenor", "Seth M. Shjandemaar", "Josh J. Kas", "Fernando D. Vila", "Gerald T. Seidler", "John J. Rehr"], "title": "Machine Learning to Predict Spectral Anisotropy in Valence-to-Core X-ray Emission Spectroscopy", "comment": null, "summary": "Polarization analysis in x-ray spectroscopy provides an orientation dependent sensitivity to local bonding environments. For a cluster of atoms, polarization sensitivity is most often discussed through the lens of point group symmetries. However, this is a discrete, qualitative method of classifying clusters, and it does little to indicate the degree of spectral anisotropy. Here we adopt a random forest model for quantitative prediction of spectral anisotropy. Its input relies on simplified local geometric and chemical information that can be obtained from any crystal structure file. The model is trained on over 10,000 experimentally realized transition metal structures from the Materials Project, with the target being VtC-XES calculated using the real space Green's function code FEFF. We find that the model can strongly predict the degree of spectral anisotropy, with the primary factors being derived from the spatial moments of ligands in a cluster.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9X\u5c04\u7ebf\u5149\u8c31\u4e2d\u504f\u632f\u5206\u6790\u7684\u8c31\u5404\u5411\u5f02\u6027\u7a0b\u5ea6\u96be\u4ee5\u5b9a\u91cf\u9884\u6d4b\u7684\u95ee\u9898\uff0c\u91c7\u7528\u968f\u673a\u68ee\u6797\u6a21\u578b\uff0c\u5229\u7528\u6676\u4f53\u7ed3\u6784\u6587\u4ef6\u4e2d\u7684\u5c40\u90e8\u51e0\u4f55\u548c\u5316\u5b66\u4fe1\u606f\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5bf9\u8c31\u5404\u5411\u5f02\u6027\u7684\u5b9a\u91cf\u9884\u6d4b\uff0c\u5173\u952e\u5f71\u54cd\u56e0\u7d20\u662f\u914d\u4f53\u7684\u7a7a\u95f4\u77e9\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u70b9\u7fa4\u5bf9\u79f0\u6027\u7684\u504f\u632f\u5206\u6790\u65b9\u6cd5\u53ea\u80fd\u5bf9\u539f\u5b50\u56e2\u7c07\u8fdb\u884c\u79bb\u6563\u7684\u5b9a\u6027\u5206\u7c7b\uff0c\u65e0\u6cd5\u6709\u6548\u6307\u793a\u8c31\u5404\u5411\u5f02\u6027\u7684\u7a0b\u5ea6\uff0c\u7f3a\u4e4f\u5b9a\u91cf\u9884\u6d4b\u80fd\u529b\u3002", "method": "\u91c7\u7528\u968f\u673a\u68ee\u6797\u6a21\u578b\uff0c\u8f93\u5165\u4e3a\u53ef\u4ece\u4efb\u4f55\u6676\u4f53\u7ed3\u6784\u6587\u4ef6\u4e2d\u83b7\u53d6\u7684\u7b80\u5316\u5c40\u90e8\u51e0\u4f55\u548c\u5316\u5b66\u4fe1\u606f\uff1b\u6a21\u578b\u4f7f\u7528Materials Project\u4e2d\u8d85\u8fc710,000\u4e2a\u5b9e\u9a8c\u5b9e\u73b0\u7684\u8fc7\u6e21\u91d1\u5c5e\u7ed3\u6784\u8fdb\u884c\u8bad\u7ec3\uff0c\u76ee\u6807\u503c\u662f\u901a\u8fc7FEFF\u5b9e\u7a7a\u95f4\u683c\u6797\u51fd\u6570\u4ee3\u7801\u8ba1\u7b97\u7684VtC-XES\u8c31\u5404\u5411\u5f02\u6027\u3002", "result": "\u6a21\u578b\u80fd\u5f3a\u6709\u529b\u5730\u9884\u6d4b\u8c31\u5404\u5411\u5f02\u6027\u7684\u7a0b\u5ea6\uff0c\u4e3b\u8981\u5f71\u54cd\u56e0\u7d20\u6765\u6e90\u4e8e\u539f\u5b50\u56e2\u7c07\u4e2d\u914d\u4f53\u7684\u7a7a\u95f4\u77e9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u8c31\u5404\u5411\u5f02\u6027\u7684\u5b9a\u91cf\u9884\u6d4b\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u70b9\u7fa4\u5bf9\u79f0\u6027\u5206\u6790\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3aX\u5c04\u7ebf\u5149\u8c31\u504f\u632f\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b9a\u91cf\u5de5\u5177\u3002"}}
{"id": "2602.00024", "categories": ["quant-ph", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.00024", "abs": "https://arxiv.org/abs/2602.00024", "authors": ["Jiaming Ye", "Fuyuan Zhang", "Shangzhou Xia", "Xiaoyu Guo", "Xiongfei Wu", "Jianjun Zhao", "Yinxing Xue"], "title": "QSPE: Enumerating Skeletal Quantum Programs for Quantum Library Testing", "comment": null, "summary": "The rapid advancement of quantum computing has led to the development of various quantum libraries, empowering compilation, simulation, and hardware backend interfaces. However, ensuring the correctness of these libraries remains a fundamental challenge due to the lack of mature testing methodologies. The state-of-the-art tools often rely on domain-specific configurations and expert knowledge, which limits their accessibility and scalability in practice. Furthermore, although these tools demonstrate strong performance, they adopt measurement-based for output validation in testing, which makes them produce false positive reports.\n  To alleviate these limitations, we propose QSPE, a practical approach that follows the differential testing principle and extends the existing approach, SPE, for quantum libraries. QSPE is fully automated, requiring no pre-set configurations or domain expertise, and can effectively generate a large set of diverse program variants that comprehensively explore the quantum compilation space. To mitigate the possible false positive reports, we propose statevector-based validation as an alternative to measurement-based validation. In our experiments, the QSPE approach demonstrates remarkable effectiveness in generating 22,770 program variants across multiple quantum computing platforms. By avoiding $\u03b1$-equivalence at the quantum and classical program wise, QSPE can reduce redundant generation and save more than 90\\% of execution cost. Finally, the statevector-based validation method assists QSPE to reduce false alarms and effectively detects 708 miscompilations across multiple quantum libraries. Notably, 81 of the discovered bugs have been officially approved and acknowledged by the Qiskit development team, demonstrating the practical impact of our approach.", "AI": {"tldr": "QSPE\u662f\u4e00\u79cd\u9488\u5bf9\u91cf\u5b50\u8ba1\u7b97\u5e93\u7684\u81ea\u52a8\u5316\u5dee\u5206\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u91c7\u7528\u72b6\u6001\u5411\u91cf\u9a8c\u8bc1\u66ff\u4ee3\u6d4b\u91cf\u9a8c\u8bc1\uff0c\u6709\u6548\u53d1\u73b0708\u4e2a\u9519\u8bef\u7f16\u8bd1\uff0c\u5176\u4e2d81\u4e2a\u83b7Qiskit\u56e2\u961f\u786e\u8ba4\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u5e93\u7f3a\u4e4f\u6210\u719f\u7684\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u73b0\u6709\u5de5\u5177\u4f9d\u8d56\u4e13\u5bb6\u77e5\u8bc6\u4e14\u57fa\u4e8e\u6d4b\u91cf\u7684\u9a8c\u8bc1\u4f1a\u4ea7\u751f\u8bef\u62a5\u3002", "method": "\u57fa\u4e8e\u5dee\u5206\u6d4b\u8bd5\u539f\u5219\u6269\u5c55SPE\u65b9\u6cd5\uff0c\u81ea\u52a8\u5316\u751f\u6210\u591a\u6837\u5316\u91cf\u5b50\u7a0b\u5e8f\u53d8\u4f53\uff0c\u91c7\u7528\u72b6\u6001\u5411\u91cf\u9a8c\u8bc1\uff0c\u907f\u514d\u03b1-\u7b49\u4ef7\u6027\u4ee5\u51cf\u5c11\u5197\u4f59\u3002", "result": "\u751f\u621022,770\u4e2a\u7a0b\u5e8f\u53d8\u4f53\uff0c\u51cf\u5c1190%\u4ee5\u4e0a\u5197\u4f59\u751f\u6210\uff0c\u8282\u7701\u6267\u884c\u6210\u672c\uff0c\u68c0\u6d4b708\u4e2a\u9519\u8bef\u7f16\u8bd1\uff0c81\u4e2abug\u83b7Qiskit\u56e2\u961f\u5b98\u65b9\u786e\u8ba4\u3002", "conclusion": "QSPE\u80fd\u6709\u6548\u6d4b\u8bd5\u91cf\u5b50\u5e93\uff0c\u663e\u8457\u964d\u4f4e\u8bef\u62a5\u5e76\u53d1\u73b0\u771f\u5b9e\u7f3a\u9677\uff0c\u5177\u6709\u5b9e\u9645\u5f71\u54cd\u529b\u3002"}}
{"id": "2602.00053", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00053", "abs": "https://arxiv.org/abs/2602.00053", "authors": ["Ratul Ali"], "title": "Scalable and Secure AI Inference in Healthcare: A Comparative Benchmarking of FastAPI and Triton Inference Server on Kubernetes", "comment": "2 pages, 2 figures, 1 table", "summary": "Efficient and scalable deployment of machine learning (ML) models is a prerequisite for modern production environments, particularly within regulated domains such as healthcare and pharmaceuticals. In these settings, systems must balance competing requirements, including minimizing inference latency for real-time clinical decision support, maximizing throughput for batch processing of medical records, and ensuring strict adherence to data privacy standards such as HIPAA. This paper presents a rigorous benchmarking analysis comparing two prominent deployment paradigms: a lightweight, Python-based REST service using FastAPI, and a specialized, high-performance serving engine, NVIDIA Triton Inference Server. Leveraging a reference architecture for healthcare AI, we deployed a DistilBERT sentiment analysis model on Kubernetes to measure median (p50) and tail (p95) latency, as well as throughput, under controlled experimental conditions. Our results indicate a distinct trade-off. While FastAPI provides lower overhead for single-request workloads with a p50 latency of 22 ms, Triton achieves superior scalability through dynamic batching, delivering a throughput of 780 requests per second on a single NVIDIA T4 GPU, nearly double that of the baseline. Furthermore, we evaluate a hybrid architectural approach that utilizes FastAPI as a secure gateway for protected health information de-identification and Triton for backend inference. This study validates the hybrid model as a best practice for enterprise clinical AI and offers a blueprint for secure, high-availability deployments.", "AI": {"tldr": "\u5728\u533b\u7597AI\u573a\u666f\u4e0b\u5bf9\u6bd4FastAPI\u4e0eNVIDIA Triton\u63a8\u7406\u670d\u52a1\u5668\u7684\u6027\u80fd\uff0c\u53d1\u73b0FastAPI\u5355\u8bf7\u6c42\u5ef6\u8fdf\u66f4\u4f4e\uff0822ms\uff09\uff0c\u800cTriton\u52a8\u6001\u6279\u5904\u7406\u541e\u5410\u91cf\u66f4\u9ad8\uff08780 req/s\uff09\uff0c\u6df7\u5408\u67b6\u6784\uff08FastAPI\u7f51\u5173+Triton\u540e\u7aef\uff09\u662f\u4f01\u4e1a\u7ea7\u4e34\u5e8aAI\u7684\u6700\u4f73\u5b9e\u8df5\u3002", "motivation": "\u73b0\u4ee3\u751f\u4ea7\u73af\u5883\u7279\u522b\u662f\u533b\u7597\u548c\u5236\u836f\u7b49\u76d1\u7ba1\u4e25\u683c\u9886\u57df\uff0c\u9700\u8981\u90e8\u7f72\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u7cfb\u7edf\u5fc5\u987b\u5e73\u8861\u5b9e\u65f6\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7684\u4f4e\u63a8\u7406\u5ef6\u8fdf\u3001\u533b\u7597\u8bb0\u5f55\u6279\u5904\u7406\u7684\u9ad8\u541e\u5410\u91cf\u4ee5\u53caHIPAA\u7b49\u6570\u636e\u9690\u79c1\u6807\u51c6\u7684\u4e25\u683c\u8981\u6c42\u3002", "method": "\u91c7\u7528\u533b\u7597AI\u53c2\u8003\u67b6\u6784\uff0c\u5728Kubernetes\u4e0a\u90e8\u7f72DistilBERT\u60c5\u611f\u5206\u6790\u6a21\u578b\uff0c\u5bf9\u6bd4\u8f7b\u91cf\u7ea7Python REST\u670d\u52a1\uff08FastAPI\uff09\u4e0e\u4e13\u7528\u9ad8\u6027\u80fd\u63a8\u7406\u5f15\u64ce\uff08NVIDIA Triton\uff09\u5728\u53d7\u63a7\u5b9e\u9a8c\u6761\u4ef6\u4e0b\u7684p50/p95\u5ef6\u8fdf\u53ca\u541e\u5410\u91cf\u8868\u73b0\u3002", "result": "FastAPI\u5728\u5355\u8bf7\u6c42\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u5f00\u9500\u66f4\u4f4e\uff08p50\u5ef6\u8fdf22\u6beb\u79d2\uff09\uff0c\u800cTriton\u901a\u8fc7\u52a8\u6001\u6279\u5904\u7406\u5b9e\u73b0\u66f4\u4f18\u53ef\u6269\u5c55\u6027\uff0c\u5728\u5355\u5757NVIDIA T4 GPU\u4e0a\u8fbe\u5230780\u8bf7\u6c42/\u79d2\u7684\u541e\u5410\u91cf\uff0c\u51e0\u4e4e\u662f\u57fa\u7ebf\u6c34\u5e73\u7684\u4e24\u500d\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u6df7\u5408\u67b6\u6784\uff08FastAPI\u4f5c\u4e3aPHI\u53bb\u6807\u8bc6\u5316\u7684\u5b89\u5168\u7f51\u5173 + Triton\u8d1f\u8d23\u540e\u7aef\u63a8\u7406\uff09\u662f\u4f01\u4e1a\u4e34\u5e8aAI\u7684\u6700\u4f73\u5b9e\u8df5\uff0c\u4e3a\u5b89\u5168\u3001\u9ad8\u53ef\u7528\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u84dd\u56fe\u3002"}}
{"id": "2602.01490", "categories": ["cond-mat.dis-nn", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.01490", "abs": "https://arxiv.org/abs/2602.01490", "authors": ["Timothee Leleu", "Sam Reifenstein", "Atsushi Yamamura", "Surya Ganguli"], "title": "Reshaping Global Loop Structure to Accelerate Local Optimization by Smoothing Rugged Landscapes", "comment": null, "summary": "Probabilistic graphical models with frustration exhibit rugged energy landscapes that trap iterative optimization dynamics. These landscapes are shaped not only by local interactions, but crucially also by the global loop structure of the graph. The famous Bethe approximation treats the graph as a tree, effectively ignoring global structure, thereby limiting its effectiveness for optimization. Loop expansions capture such global structure in principle, but are often impractical due to combinatorial explosion. The $M$-layer construction provides an alternative: make $M$ copies of the graph and reconnect edges between them uniformly at random. This provides a controlled sequence of approximations from the original graph at $M=1$, to the Bethe approximation as $M \\rightarrow \\infty$. Here we generalize this construction by replacing uniform random rewiring with a structured mixing kernel $Q$ that sets the probability that any two layers are interconnected. As a result, the global loop structure can be shaped without modifying local interactions. We show that, after this copy-and-reconnect transformation, there exists a regime in which layer-to-layer fluctuations decay, increasing the probability of reaching the global minimum of the energy function of the original graph. This yields a highly general and practical tool for optimization. Using this approach, the computational cost required to reach these optimal solutions is reduced across sparse and dense Ising benchmarks, including spin glasses and planted instances. When combined with replica-exchange Monte Carlo, the same construction increases the polynomial-time algorithmic threshold for the maximum independent set problem. A cavity analysis shows that structured inter-layer coupling significantly smooths rugged energy landscapes by collapsing configurational complexity and suppressing many suboptimal metastable states.", "AI": {"tldr": "A generalized M-layer construction replaces uniform random rewiring with a structured mixing kernel Q to control global loop structure in graphical models, enabling smoother energy landscapes and more efficient optimization across Ising benchmarks.", "motivation": "Probabilistic graphical models with frustration suffer from rugged energy landscapes that trap optimization. The Bethe approximation ignores global loop structure, while loop expansions are computationally intractable due to combinatorial explosion.", "method": "Generalize the M-layer construction by using a structured mixing kernel Q to set inter-layer connection probabilities, shaping global loop structure without altering local interactions.", "result": "Layer-to-layer fluctuations decay in a specific regime, increasing probability of reaching global minima. Computational cost reduces across sparse/dense Ising benchmarks (spin glasses, planted instances). Combined with replica-exchange Monte Carlo, it raises the polynomial-time threshold for maximum independent set. Cavity analysis confirms smoother landscapes with suppressed metastable states.", "conclusion": "Structured inter-layer coupling via the Q-kernel provides a highly general and practical optimization tool that collapses configurational complexity and significantly improves performance on challenging combinatorial problems."}}
{"id": "2602.00354", "categories": ["cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.00354", "abs": "https://arxiv.org/abs/2602.00354", "authors": ["B. Arnold", "J. Daligault", "D. Saumon", "S. X. Hu"], "title": "Deriving Reliable Nucleation Rates from Metadynamics Simulations: Application to Yukawa Fluids", "comment": "20 pages including supplementary material, 10 figures", "summary": "In order to solidify the usefulness of metadynamics in studying nucleation of crystals from supercooled liquids, we provide a specific procedure to calculate nucleation free energy barriers. After a pedagogical review of the important elements of classical nucleation theory and how metadynamics is used to find nucleation free energy barriers, we explain the benefits of local collective variables over more common global collective variables. We show how a metadynamics free energy barrier must be carefully postprocessed so that classical nucleation theory can be applied to calculate nucleation rates. We apply our procedure to a Yukawa plasma and show that a particular physically-motivated fit to metadynamics data reproduces low-temperature reference data, justifying the usefulness of metadynamics to predict nucleation rates and the nucleation critical temperature.", "AI": {"tldr": "This paper establishes a validated metadynamics procedure for calculating crystal nucleation free energy barriers, demonstrating its accuracy in predicting nucleation rates and critical temperatures using a Yukawa plasma model.", "motivation": "To solidify metadynamics as a reliable tool for studying crystal nucleation from supercooled liquids by addressing methodological gaps in calculating nucleation free energy barriers and applying classical nucleation theory.", "method": "Proposes a specific metadynamics procedure using local collective variables (superior to global ones), requiring careful postprocessing of free energy barriers to align with classical nucleation theory for nucleation rate calculations.", "result": "Applied to a Yukawa plasma, a physically-motivated fit to metadynamics data accurately reproduces low-temperature reference nucleation data, validating the method's predictive power.", "conclusion": "Metadynamics is justified as an effective method for predicting nucleation rates and critical temperatures when its free energy barriers are properly processed and interpreted within classical nucleation theory."}}
{"id": "2602.00441", "categories": ["cond-mat.str-el", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2602.00441", "abs": "https://arxiv.org/abs/2602.00441", "authors": ["Fabian Jofre Parra", "Daniel A. Bonilla", "Enrique Mu\u00f1oz"], "title": "Electron-phonon interactions and instabilities in Weyl semimetals under magnetic fields and torsional strain", "comment": null, "summary": "We study the presence of an external magnetic field, in combination with torsional strain, over the electron-phonon interactions in a type I Weyl semimetal. This particular superposition of field and strain, modeled in the continuum approximation by an effective gauge field, leads to an asymmetric pseudo-magnetic field at each Weyl node of opposite chirality. Therefore, we also studied the role of nodal asymmetry in the properties of the system by means of the Kadanoff-Wilson renormalization group and the corresponding flow equations. By solving those, we discuss the evolution of the coupling parameters of the theory, and analyze possible fixed points and lattice (Peierls) instabilities emerging from interactions between phonons with the chiral Landau level in the very strong pseudo-magnetic field regime.", "AI": {"tldr": "\u7814\u7a76I\u578b\u5916\u5c14\u534a\u91d1\u5c5e\u4e2d\u5916\u90e8\u78c1\u573a\u4e0e\u626d\u8f6c\u5e94\u53d8\u7684\u5171\u540c\u4f5c\u7528\u5bf9\u7535\u58f0\u5b50\u76f8\u4e92\u4f5c\u7528\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4e8c\u8005\u53e0\u52a0\u4ea7\u751f\u624b\u6027\u76f8\u53cd\u7684\u8d5d\u78c1\u573a\uff0c\u5e76\u901a\u8fc7\u91cd\u6574\u5316\u7fa4\u5206\u6790\u63ed\u793a\u5f3a\u573a\u4e0b\u53ef\u80fd\u51fa\u73b0\u6676\u683c\u5931\u7a33\u73b0\u8c61", "motivation": "\u63a2\u7a76\u5916\u78c1\u573a\u4e0e\u673a\u68b0\u5e94\u53d8\u5982\u4f55\u534f\u540c\u8c03\u63a7\u5916\u5c14\u534a\u91d1\u5c5e\u4e2d\u7684\u7535\u58f0\u5b50\u8026\u5408\u673a\u5236\uff0c\u7279\u522b\u5173\u6ce8\u624b\u6027\u76f8\u5173\u7684\u8d5d\u78c1\u573a\u6548\u5e94\u53ca\u5176\u5bf9\u7cfb\u7edf\u7a33\u5b9a\u6027\u7684\u5f71\u54cd", "method": "\u91c7\u7528\u8fde\u7eed\u4ecb\u8d28\u8fd1\u4f3c\u5c06\u573a\u4e0e\u5e94\u53d8\u5efa\u6a21\u4e3a\u6709\u6548\u89c4\u8303\u573a\uff0c\u8fd0\u7528Kadanoff-Wilson\u91cd\u6574\u5316\u7fa4\u65b9\u6cd5\u6c42\u89e3\u6d41\u52a8\u65b9\u7a0b\uff0c\u5206\u6790\u8026\u5408\u53c2\u6570\u6f14\u5316\u53ca\u56fa\u5b9a\u70b9\u884c\u4e3a", "result": "\u53d1\u73b0\u78c1\u573a-\u5e94\u53d8\u53e0\u52a0\u5bfc\u81f4\u5916\u5c14\u8282\u70b9\u5904\u4ea7\u751f\u624b\u6027\u4f9d\u8d56\u7684\u8d5d\u78c1\u573a\u4e0d\u5bf9\u79f0\u6027\uff0c\u5728\u6781\u5f3a\u8d5d\u78c1\u573a\u4e0b\u7535\u58f0\u5b50\u76f8\u4e92\u4f5c\u7528\u53ef\u80fd\u5f15\u53d1\u6676\u683cPeierls\u5931\u7a33", "conclusion": "\u8282\u70b9\u624b\u6027\u4e0d\u5bf9\u79f0\u6027\u662f\u8c03\u63a7\u62d3\u6251\u6750\u6599\u7269\u6027\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5f3a\u8d5d\u78c1\u573a regime \u4e2d\u53ef\u80fd\u51fa\u73b0\u65b0\u7684\u91cf\u5b50\u76f8\u53d8\u548c\u7ed3\u6784\u5931\u7a33\u73b0\u8c61"}}
{"id": "2602.00022", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00022", "abs": "https://arxiv.org/abs/2602.00022", "authors": ["Margaret Foster"], "title": "Measurement for Opaque Systems: Multi-source Triangulation with Interpretable Machine Learning", "comment": "16 pages, 6 figures, 3 tables, 9-page appendix", "summary": "We propose a measurement framework for difficult-to-access contexts that uses indirect data traces, interpretable machine-learning models, and theory-guided triangulation to fill inaccessible measurement spaces. Many high-stakes systems of scientific and policy interest are difficult, if not impossible, to reach directly: dynamics of interest are unobservable, data are indirect and fragmented across sources, and ground truth is absent or concealed. In these settings, available data often do not support conventional strategies for analysis, such as statistical inference on a single authoritative data stream or model validation against labeled outcomes. To address this problem, we introduce a general framework for measurement in data regimes characterized by structurally missing or adversarial data. We propose combining multi-source triangulation with interpretable machine learning models. Rather than relying on accuracy against unobservable, unattainable ideal data, our framework seeks consistency across separate, partially informative models. This allows users to draw defensible conclusions about the state of the world based on cross-signal consistency or divergence from an expected state. Our framework provides an analytical workflow tailored to quantitative characterization in the absence of data sufficient for conventional statistical or causal inference. We demonstrate our approach and explicitly surface inferential limits through an empirical analysis of organizational growth and internal pressure dynamics in a clandestine militant organization, drawing on multiple observational signals that individually provide incomplete and biased views of the underlying process. The results show how triangulated, interpretable ML can recover substantively meaningful variation.", "AI": {"tldr": "\u9488\u5bf9\u96be\u4ee5\u76f4\u63a5\u8bbf\u95ee\u7684\u9ad8\u98ce\u9669\u7cfb\u7edf\uff0c\u63d0\u51fa\u4e00\u4e2a\u6574\u5408\u95f4\u63a5\u6570\u636e\u75d5\u8ff9\u3001\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u7406\u8bba\u5f15\u5bfc\u4e09\u89d2\u9a8c\u8bc1\u7684\u6d4b\u91cf\u6846\u67b6\uff0c\u5728\u7f3a\u4e4f\u4f20\u7edf\u6570\u636e\u548c\u9a8c\u8bc1\u65b9\u6cd5\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6709\u6548\u6d4b\u91cf\u3002", "motivation": "\u8bb8\u591a\u9ad8\u98ce\u9669\u79d1\u5b66\u548c\u653f\u7b56\u7cfb\u7edf\u9762\u4e34\u76ee\u6807\u52a8\u6001\u4e0d\u53ef\u89c2\u6d4b\u3001\u6570\u636e\u6765\u6e90\u95f4\u63a5\u4e14\u788e\u7247\u5316\u3001\u771f\u5b9e\u60c5\u51b5\u7f3a\u5931\u6216\u9690\u85cf\u7b49\u6839\u672c\u6027\u6311\u6218\uff0c\u73b0\u6709\u7edf\u8ba1\u63a8\u65ad\u548c\u6a21\u578b\u9a8c\u8bc1\u65b9\u6cd5\u65e0\u6cd5\u9002\u7528\uff0c\u4e9f\u9700\u65b0\u7684\u6d4b\u91cf\u8303\u5f0f\u3002", "method": "\u7ed3\u5408\u591a\u6e90\u4e09\u89d2\u9a8c\u8bc1\u4e0e\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u5bfb\u6c42\u4e0d\u540c\u90e8\u5206\u4fe1\u606f\u6a21\u578b\u95f4\u7684\u4e00\u81f4\u6027\u800c\u975e\u4f9d\u8d56\u65e0\u6cd5\u83b7\u5f97\u7684\u7406\u60f3\u6570\u636e\u51c6\u786e\u6027\uff0c\u5728\u6570\u636e\u7ed3\u6784\u6027\u7f3a\u5931\u6216\u5bf9\u6297\u6027\u73af\u5883\u4e0b\u8fdb\u884c\u6d4b\u91cf\u63a8\u65ad\u3002", "result": "\u5bf9\u67d0\u79d8\u5bc6\u6b66\u88c5\u7ec4\u7ec7\u7684\u5b9e\u8bc1\u5206\u6790\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u4ece\u591a\u4e2a\u4e0d\u5b8c\u6574\u4e14\u6709\u504f\u7684\u89c2\u6d4b\u4fe1\u53f7\u4e2d\uff0c\u6062\u590d\u51fa\u5b9e\u8d28\u4e0a\u6709\u610f\u4e49\u7684\u7ec4\u7ec7\u6210\u957f\u4e0e\u5185\u90e8\u538b\u529b\u52a8\u6001\u53d8\u5316\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6570\u636e\u4e0d\u8db3\u73af\u5883\u4e0b\u7684\u91cf\u5316\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u901a\u8fc7\u660e\u786e\u63a8\u65ad\u9650\u5236\uff0c\u4f7f\u7814\u7a76\u8005\u80fd\u5728\u7f3a\u4e4f\u4f20\u7edf\u7edf\u8ba1\u6216\u56e0\u679c\u63a8\u65ad\u6761\u4ef6\u65f6\uff0c\u57fa\u4e8e\u8de8\u4fe1\u53f7\u4e00\u81f4\u6027\u5f97\u51fa\u53ef\u9760\u7684\u5b9e\u8bc1\u7ed3\u8bba\u3002"}}
{"id": "2602.00188", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00188", "abs": "https://arxiv.org/abs/2602.00188", "authors": ["Srividhya Sethuraman", "Chandrashekar Lakshminarayanan"], "title": "Learning to Price: Interpretable Attribute-Level Models for Dynamic Markets", "comment": "Accepted in AAMAS 2026 - main track - full paper - 12 pages", "summary": "Dynamic pricing in high-dimensional markets poses fundamental challenges of scalability, uncertainty, and interpretability. Existing low-rank bandit formulations learn efficiently but rely on latent features that obscure how individual product attributes influence price. We address this by introducing an interpretable \\emph{Additive Feature Decomposition-based Low-Dimensional Demand (\\textbf{AFDLD}) model}, where product prices are expressed as the sum of attribute-level contributions and substitution effects are explicitly modeled. Building on this structure, we propose \\textbf{ADEPT} (Additive DEcomposition for Pricing with cross-elasticity and Time-adaptive learning)-a projection-free, gradient-free online learning algorithm that operates directly in attribute space and achieves a sublinear regret of $\\tilde{\\mathcal{O}}(\\sqrt{d}T^{3/4})$. Through controlled synthetic studies and real-world datasets, we show that ADEPT (i) learns near-optimal prices under dynamic market conditions, (ii) adapts rapidly to shocks and drifts, and (iii) yields transparent, attribute-level price explanations. The results demonstrate that interpretability and efficiency in autonomous pricing agents can be achieved jointly through structured, attribute-driven representations.", "AI": {"tldr": "The paper proposes ADEPT, an interpretable dynamic pricing algorithm using additive feature decomposition that achieves sublinear regret while providing transparent attribute-level price explanations.", "motivation": "Dynamic pricing in high-dimensional markets faces scalability, uncertainty, and interpretability challenges. Existing low-rank bandit methods are efficient but obscure how individual product attributes influence pricing decisions.", "method": "Introduces AFDLD model where prices are sums of attribute-level contributions with explicit substitution effects. Proposes ADEPT - a projection-free, gradient-free online learning algorithm operating in attribute space with $\\tilde{\\mathcal{O}}(\\sqrt{d}T^{3/4})$ regret.", "result": "Through synthetic and real-world experiments, ADEPT learns near-optimal prices, adapts quickly to market changes, and provides transparent attribute-level price explanations.", "conclusion": "Interpretability and efficiency in autonomous pricing can be jointly achieved through structured, attribute-driven representations."}}
{"id": "2602.00698", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.00698", "abs": "https://arxiv.org/abs/2602.00698", "authors": ["Chen Lu", "Lun-Hui Hu"], "title": "Van Hove singularity-induced multiple magnetic transitions in multi-orbital systems", "comment": null, "summary": "Van Hove singularities (VHSs) amplify electronic correlations, providing a crucial platform for discovering novel quantum phase transitions. Here, we show that VHSs in multi-orbital systems can stabilize a variety of competing $\\bm{Q}=0$ magnetic orders, including intrinsic altermagnetism emerging from spontaneous orbital antiferromagnetism. This intrinsic phase, in which antiparallel spins reside on distinct orbitals, is realized across all four 2D Bravais lattices. It is driven by orbital-resolved spin fluctuations enhanced by inter-orbital hopping and favors suppressed Hund's coupling $J_H$, strong inter-orbital hybridization, and filling near a VHS from quadratic band touching. Through Hubbard-$U$-$J_H$ phase diagrams we map several magnetic phase transitions: (i) ferrimagnet to $d$-wave extrinsic altermagnet, (ii) $d$-wave intrinsic altermagnet to ferromagnet, and (iii) $g$-wave extrinsic altermagnet to either $d$-wave extrinsic altermagnet or ferromagnet. Our work identifies VHSs as a generic route to altermagnetism in correlated materials.", "AI": {"tldr": "Van Hove singularities (VHSs) in multi-orbital systems enable intrinsic altermagnetism through orbital-resolved spin fluctuations, driving magnetic phase transitions across all 2D lattices under specific conditions.", "motivation": "VHSs amplify electronic correlations and are key to discovering novel quantum phase transitions; this work explores how VHSs in multi-orbital systems stabilize competing magnetic orders, particularly intrinsic altermagnetism.", "method": "Theoretical model analysis using Hubbard-U-JH phase diagrams across four 2D Bravais lattices, focusing on orbital-resolved spin fluctuations, inter-orbital hopping, and band-filling near VHSs.", "result": "1) Intrinsic altermagnetism (antiparallel spins on distinct orbitals) emerges universally across all 2D lattices. 2) Requires suppressed Hund's coupling, strong inter-orbital hybridization, and filling near quadratic band-touching VHSs. 3) Three magnetic phase transitions mapped: ferrimagnet\u2192d-wave extrinsic altermagnet; d-wave intrinsic altermagnet\u2192ferromagnet; g-wave extrinsic altermagnet\u2192d-wave extrinsic altermagnet/ferromagnet.", "conclusion": "VHSs provide a generic mechanism for altermagnetism in correlated materials, enabling predictable magnetic phase engineering through orbital physics and band structure design."}}
{"id": "2602.00069", "categories": ["quant-ph", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.00069", "abs": "https://arxiv.org/abs/2602.00069", "authors": ["Ailsa Robertson", "Christian Schaffner", "Sebastian R. Verschoor"], "title": "Integrity from Algebraic Manipulation Detection in Trusted-Repeater QKD Networks", "comment": null, "summary": "Quantum Key Distribution (QKD) allows secure communication without relying on computational assumptions, but can currently only be deployed over relatively short distances due to hardware constraints. To extend QKD over long distances, networks of trusted repeater nodes can be used, wherein QKD is executed between neighbouring nodes and messages between non-neighbouring nodes are forwarded using a relay protocol. Although these networks are being deployed worldwide, no protocol exists which provides provable guarantees of integrity against manipulation from both external adversaries and corrupted intermediates. In this work, we present the first protocol that provably provides both confidentiality and integrity. Our protocol combines an existing cryptographic technique, Algebraic Manipulation Detection (AMD) codes, with multi-path relaying over trusted repeater networks. This protocol achieves Information Theoretic Security (ITS) against the detection of manipulation, which we prove formally through a sequence of games.", "AI": {"tldr": "This paper presents the first protocol for quantum key distribution (QKD) networks that provides provable integrity and confidentiality against both external attacks and corrupted intermediate nodes by combining Algebraic Manipulation Detection (AMD) codes with multi-path relaying.", "motivation": "QKD is limited to short distances and existing trusted repeater networks lack provable integrity guarantees against manipulation from adversaries and corrupted nodes.", "method": "The protocol combines Algebraic Manipulation Detection (AMD) codes with multi-path relaying over trusted repeater networks, and is proven secure through a sequence of games.", "result": "Achieves Information Theoretic Security (ITS) against the detection of manipulation.", "conclusion": "This is the first protocol that provably provides both confidentiality and integrity for QKD networks, extending their practical deployment over long distances with strong security guarantees."}}
{"id": "2602.00190", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00190", "abs": "https://arxiv.org/abs/2602.00190", "authors": ["Mohit Jiwatode", "Alexander Dockhorn", "Bodo Rosenhahn"], "title": "From Gameplay Traces to Game Mechanics: Causal Induction with Large Language Models", "comment": "Submitted to ICPR 2026", "summary": "Deep learning agents can achieve high performance in complex game domains without often understanding the underlying causal game mechanics. To address this, we investigate Causal Induction: the ability to infer governing laws from observational data, by tasking Large Language Models (LLMs) with reverse-engineering Video Game Description Language (VGDL) rules from gameplay traces. To reduce redundancy, we select nine representative games from the General Video Game AI (GVGAI) framework using semantic embeddings and clustering. We compare two approaches to VGDL generation: direct code generation from observations, and a two-stage method that first infers a structural causal model (SCM) and then translates it into VGDL. Both approaches are evaluated across multiple prompting strategies and controlled context regimes, varying the amount and form of information provided to the model, from just raw gameplay observations to partial VGDL specifications. Results show that the SCM-based approach more often produces VGDL descriptions closer to the ground truth than direct generation, achieving preference win rates of up to 81\\% in blind evaluations and yielding fewer logically inconsistent rules. These learned SCMs can be used for downstream use cases such as causal reinforcement learning, interpretable agents, and procedurally generating novel but logically consistent games.", "AI": {"tldr": "Researchers used LLMs to infer video game rules from gameplay traces, comparing direct code generation against a two-stage causal modeling approach that first infers a structural causal model (SCM) before translating to VGDL, finding the causal method superior.", "motivation": "Deep learning agents can achieve high performance in complex games without understanding underlying causal mechanics; this paper investigates Causal Induction\u2014the ability to infer governing laws from observational data\u2014by tasking LLMs with reverse-engineering game rules.", "method": "Selected nine representative games from the GVGAI framework using semantic embeddings and clustering; compared two VGDL generation approaches (direct code generation vs. two-stage SCM inference then translation) across multiple prompting strategies with varying context information from raw gameplay to partial specifications.", "result": "The SCM-based approach produced VGDL descriptions closer to ground truth, achieving up to 81% preference win rates in blind evaluations and generating fewer logically inconsistent rules compared to direct generation.", "conclusion": "The two-stage causal induction method is more effective for inferring game mechanics from observations, enabling downstream applications including causal reinforcement learning, interpretable agents, and procedural generation of novel logically consistent games."}}
{"id": "2602.02125", "categories": ["cond-mat.dis-nn", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2602.02125", "abs": "https://arxiv.org/abs/2602.02125", "authors": ["Chen Hao Xia", "Manasa Kaniselvan", "Marko Mladenoivi\u0107", "Mathieu Luisier"], "title": "Machine-Learned Hamiltonians for Quantum Transport Simulation of Valence Change Memories", "comment": null, "summary": "The construction of the Hamiltonian matrix \\textbf{H} is an essential, yet computationally expensive step in \\textit{ab-initio} device simulations based on density-functional theory (DFT). In homogeneous structures, the fact that a unit cell repeats itself along at least one direction can be leveraged to minimize the number of atoms considered and the calculation time. However, such an approach does not lend itself to amorphous or defective materials for which no periodicity exists. In these cases, (much) larger domains containing thousands of atoms might be needed to accurately describe the physics at play, pushing DFT tools to their limit. Here we address this issue by learning and directly predicting the Hamiltonian matrix of large structures through equivariant graph neural networks and so-called augmented partitioning training. We demonstrate the strength of our approach by modeling valence change memory (VCM) cells, achieving a Mean Absolute Error (MAE) of 3.39 to 3.58 meV, as compared to DFT, when predicting the Hamiltonian matrix entries of systems made of $\\sim$5,000 atoms. We then replace the DFT-computed Hamiltonian of these VCMs with the predicted one to compute their energy-resolved transmission function with a quantum transport tool. A qualitatively good agreement between both sets of curves is obtained. Our work provides a path forward to overcome the memory and computational limits of DFT, thus enabling the study of large-scale devices beyond current \\textit{ab-initio} capabilities", "AI": {"tldr": "A machine learning approach using equivariant graph neural networks predicts Hamiltonian matrices for large non-periodic materials (5000+ atoms) with high accuracy, enabling DFT-level quantum transport simulations beyond current computational limits.", "motivation": "Constructing Hamiltonian matrices in DFT simulations is computationally expensive, especially for amorphous/defective materials lacking periodicity that require large domains of thousands of atoms, pushing DFT tools to their limits.", "method": "Using equivariant graph neural networks with augmented partitioning training to learn and directly predict Hamiltonian matrices of large structures, then replacing DFT-computed Hamiltonian with predicted one for quantum transport calculations.", "result": "Achieved Mean Absolute Error of 3.39-3.58 meV compared to DFT for systems of ~5000 atoms, with qualitatively good agreement in energy-resolved transmission functions between DFT and predicted Hamiltonians.", "conclusion": "The work provides a path forward to overcome DFT's memory and computational limits, enabling study of large-scale devices beyond current ab-initio capabilities."}}
{"id": "2602.00794", "categories": ["cond-mat.stat-mech", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.00794", "abs": "https://arxiv.org/abs/2602.00794", "authors": ["Istv\u00e1n Cs\u00e9p\u00e1nyi", "Giuseppe Del Vecchio Del Vecchio", "Benjamin Doyon", "M\u00e1rton Kormos"], "title": "Observing quantum phase transitions at non-zero temperature: non-analytic behavior of order-parameter correlation times", "comment": "7+2 pages + Supp. Mat. (12 pages); 2+2 figures", "summary": "Phase transitions occur when a macroscopic number of local degrees of freedom coherently change their behavior. In ground states of quantum many-body systems, phase transitions due to quantum fluctuations are observed as non-analytic behaviors of order parameters, such as magnetization, as functions of a conjugate parameter, such as the magnetic field. However, as soon as thermal fluctuations are present, these effects are believed to disappear for local observables. We show that this is not necessarily the case: order parameters may still show non-analytic behaviors within their dynamics. With the example of the Ising model and using methods based on hydrodynamic fluctuations, we evaluate the exact order-parameter correlation time, in space-time directions of all velocities, in equilibrium states at nonzero temperature. We reveal non-analytic behaviors of spin correlation times as functions of the magnetic field, velocity, and temperature. As a function of the magnetic field, they occur at values that continuously approach that of the zero-temperature equilibrium transition point as the velocity is decreased and reach it within the light cone, where we obtain a new, temperature-independent logarithmic divergence characterizing the collective dynamics. Thus, collective effects induced by quantum fluctuations persist within the dynamics of local observables.", "AI": {"tldr": "\u8be5\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u5728\u975e\u96f6\u6e29\u5ea6\u4e0b\uff0c\u5c40\u90e8\u89c2\u6d4b\u91cf\u7684\u52a8\u529b\u5b66\u884c\u4e3a\u4ecd\u53ef\u80fd\u8868\u73b0\u51fa\u975e\u89e3\u6790\u6027\uff0c\u8868\u660e\u91cf\u5b50\u6da8\u843d\u7684\u96c6\u4f53\u6548\u5e94\u5728\u52a8\u529b\u5b66\u4e2d\u6301\u7eed\u5b58\u5728\u3002", "motivation": "\u6311\u6218\u4f20\u7edf\u89c2\u70b9\uff1a\u4f20\u7edf\u8ba4\u4e3a\u70ed\u6da8\u843d\u4f1a\u4f7f\u5c40\u90e8\u89c2\u6d4b\u91cf\u7684\u91cf\u5b50\u76f8\u53d8\u6548\u5e94\u6d88\u5931\uff0c\u4f46\u672c\u7814\u7a76\u8d28\u7591\u8fd9\u4e00\u89c2\u70b9\uff0c\u63a2\u7d22\u52a8\u529b\u5b66\u4e2d\u662f\u5426\u4ecd\u5b58\u5728\u975e\u89e3\u6790\u884c\u4e3a\u3002", "method": "\u57fa\u4e8e\u6d41\u4f53\u52a8\u529b\u5b66\u6da8\u843d\u65b9\u6cd5\uff0c\u4ee5\u4f0a\u8f9b\u6a21\u578b\u4e3a\u4f8b\uff0c\u7cbe\u786e\u8ba1\u7b97\u5e73\u8861\u6001\u4e2d\u975e\u96f6\u6e29\u5ea6\u4e0b\u7684\u5e8f\u53c2\u91cf\u5173\u8054\u65f6\u95f4\uff08\u6db5\u76d6\u6240\u6709\u65f6\u7a7a\u65b9\u5411\u901f\u5ea6\uff09\u3002", "result": "\u53d1\u73b0\u81ea\u65cb\u5173\u8054\u65f6\u95f4\u968f\u78c1\u573a\u3001\u901f\u5ea6\u548c\u6e29\u5ea6\u5448\u73b0\u975e\u89e3\u6790\u884c\u4e3a\uff1b\u5728\u5149\u9525\u5185\uff0c\u78c1\u573a\u76f8\u5173\u975e\u89e3\u6790\u6027\u968f\u901f\u5ea6\u51cf\u5c0f\u8fde\u7eed\u8d8b\u8fd1\u96f6\u6e29\u76f8\u53d8\u70b9\uff0c\u5e76\u51fa\u73b0\u65b0\u7684\u6e29\u5ea6\u65e0\u5173\u5bf9\u6570\u53d1\u6563\u3002", "conclusion": "\u91cf\u5b50\u6da8\u843d\u8bf1\u5bfc\u7684\u96c6\u4f53\u6548\u5e94\u53ef\u5728\u5c40\u90e8\u89c2\u6d4b\u91cf\u7684\u52a8\u529b\u5b66\u4e2d\u6301\u7eed\u5b58\u5728\uff0c\u63ed\u793a\u4e86\u975e\u96f6\u6e29\u4e0b\u52a8\u529b\u5b66\u884c\u4e3a\u4e0e\u91cf\u5b50\u76f8\u53d8\u7684\u6df1\u523b\u8054\u7cfb\u3002"}}
{"id": "2602.02149", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.02149", "abs": "https://arxiv.org/abs/2602.02149", "authors": ["Sylvia J. Bintrim", "Ella R. Ransford", "Kevin Carter-Fenk"], "title": "The Dose Makes the Poison: Perturbative Steps Toward the Ultimate Linearized Coupled Cluster Method", "comment": "26 pages, 4 figures", "summary": "\"Addition-by-subtraction\" coupled cluster (CC) approaches provide a promising approach to treating the difficult strong correlation problem by simplifying the standard CC equations. In a separate vein, linearized CC methods have drawn interest for their lower computational cost, increased parallelizability, and favorable properties for extension to the excited state--but the inclusion of ring/crossed-ring terms causes singularities even for single bond breaking. A linearized, addition-by-subtraction CC method called linearized ladder CCD (linLCCD) removes these terms to avoid divergences, but linLCCD under-estimates dynamical correlation. Herein we resolve this deficiency of linLCCD by introducing a linearized external coupled cluster perturbation theory that adds a second-order ring/crossed-ring correction back into a linLCCD reference wave function. Our resultant xlinCCD(2) method is regular and yields comparable results to linearized CCD in weakly-correlated regimes.", "AI": {"tldr": "Proposes xlinCCD(2), a linearized coupled cluster method combining addition-by-subtraction simplification with second-order perturbation correction to resolve dynamical correlation underestimation and divergence issues in bond-breaking simulations.", "motivation": "Standard coupled cluster (CC) methods struggle with strong correlation and high computational cost; linearized CC methods have singularities during bond breaking, while existing linearized ladder CCD (linLCCD) underestimates dynamical correlation.", "method": "Introduces linearized external coupled cluster perturbation theory that adds a second-order ring/crossed-ring correction to the linLCCD reference wave function, creating the xlinCCD(2) method.", "result": "xlinCCD(2) avoids divergences (singularities) in single bond breaking, remains computationally regular, and yields results comparable to linearized CCD in weakly-correlated systems while correcting linLCCD's dynamical correlation deficiency.", "conclusion": "xlinCCD(2) successfully bridges the gap between accuracy and stability for electron correlation calculations, providing a robust method for both strong and weak correlation regimes without singularities."}}
{"id": "2602.00071", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.00071", "abs": "https://arxiv.org/abs/2602.00071", "authors": ["Matheus Eiji Ohno Bezerra", "Valery Shchesnovich"], "title": "A generating-function approach to the interference of squeezed states with partial distinguishability", "comment": "10 pages, 2 figures", "summary": "Photon distinguishability is a fundamental property manifested in multiphoton interference and one of the main sources of noise in any photonic quantum information processing. In this work, rather than relying on first-quantization methods, we build on a generating-function framework based on the phase-space formalism to characterize the effects of partial distinguishability on the interference of single-mode squeezed states. Our approach goes beyond commonly used models that represent distinguishability via additional noninterfering modes and captures genuine multiphoton interference effects induced by the overlap of the internal state of the photons. This description provides a clear physical account of how distinguishability gives rise to effective noise in Gaussian boson sampling protocols while enabling a systematic investigation of phase effects arising from the overlap of the internal states.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u76f8\u7a7a\u95f4\u751f\u6210\u51fd\u6570\u7684\u65b0\u6846\u67b6\uff0c\u63ed\u793a\u5149\u5b50\u53ef\u533a\u5206\u6027\u5728\u91cf\u5b50\u5e72\u6d89\u4e2d\u4ea7\u751f\u771f\u5b9e\u591a\u5149\u5b50\u5e72\u6d89\u6548\u5e94\u800c\u975e\u7b80\u5355\u566a\u58f0\uff0c\u4e3a\u9ad8\u65af\u73bb\u8272\u91c7\u6837\u63d0\u4f9b\u7cbe\u786e\u566a\u58f0\u6a21\u578b", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u5149\u5b50\u53ef\u533a\u5206\u6027\u5efa\u6a21\u4e3a\u989d\u5916\u975e\u5e72\u6d89\u6a21\u5f0f\uff0c\u65e0\u6cd5\u6355\u6349\u5185\u90e8\u6001\u91cd\u53e0\u5bfc\u81f4\u7684\u771f\u5b9e\u591a\u5149\u5b50\u5e72\u6d89\u6548\u5e94\uff0c\u5f71\u54cd\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u566a\u58f0\u5206\u6790\u7cbe\u5ea6", "method": "\u91c7\u7528\u76f8\u7a7a\u95f4 formalism \u7684\u751f\u6210\u51fd\u6570\u6846\u67b6\uff0c\u5206\u6790\u5355\u6a21\u538b\u7f29\u6001\u5e72\u6d89\u4e2d\u90e8\u5206\u53ef\u533a\u5206\u6027\u7684\u5f71\u54cd", "result": "\u6210\u529f\u523b\u753b\u5149\u5b50\u5185\u90e8\u6001\u91cd\u53e0\u4ea7\u751f\u7684\u771f\u5b9e\u591a\u5149\u5b50\u5e72\u6d89\u6548\u5e94\uff0c\u63ed\u793a\u53ef\u533a\u5206\u6027\u5bfc\u81f4\u9ad8\u65af\u73bb\u8272\u91c7\u6837\u4e2d\u6709\u6548\u566a\u58f0\u7684\u7269\u7406\u673a\u5236", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u6f84\u6e05\u53ef\u533a\u5206\u6027\u7684\u7269\u7406\u8d77\u6e90\uff0c\u8fd8\u4e3a\u91cf\u5b50\u91c7\u6837\u534f\u8bae\u4e2d\u7684\u76f8\u4f4d\u6548\u5e94\u548c\u566a\u58f0\u63a7\u5236\u63d0\u4f9b\u7cfb\u7edf\u6027\u7814\u7a76\u5de5\u5177"}}
{"id": "2602.00028", "categories": ["cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.00028", "abs": "https://arxiv.org/abs/2602.00028", "authors": ["Zoha Azimi", "Reza Farahani", "Radu Prodan", "Christian Timmerer"], "title": "ELLMPEG: An Edge-based Agentic LLM Video Processing Tool", "comment": "12 pages, 5 tables, 8 Figures, accepted for the MMSys 2026 conference", "summary": "Large language models (LLMs), the foundation of generative AI systems like ChatGPT, are transforming many fields and applications, including multimedia, enabling more advanced content generation, analysis, and interaction. However, cloud-based LLM deployments face three key limitations: high computational and energy demands, privacy and reliability risks from remote processing, and recurring API costs. Recent advances in agentic AI, especially in structured reasoning and tool use, offer a better way to exploit open and locally deployed tools and LLMs. This paper presents ELLMPEG, an edge-enabled agentic LLM framework for the automated generation of video-processing commands. ELLMPEG integrates tool-aware Retrieval-Augmented Generation (RAG) with iterative self-reflection to produce and locally verify executable FFmpeg and VVenC commands directly at the edge, eliminating reliance on external cloud APIs. To evaluate ELLMPEG, we collect a dedicated prompt dataset comprising 480 diverse queries covering different categories of FFmpeg and the Versatile Video Codec (VVC) encoder (VVenC) commands. We validate command generation accuracy and evaluate four open-source LLMs based on command validity, tokens generated per second, inference time, and energy efficiency. We also execute the generated commands to assess their runtime correctness and practical applicability. Experimental results show that Qwen2.5, when augmented with the ELLMPEG framework, achieves an average command-generation accuracy of 78 % with zero recurring API cost, outperforming all other open-source models across both the FFmpeg and VVenC datasets.", "AI": {"tldr": "\u63d0\u51faELLMPEG\u8fb9\u7f18\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5de5\u5177\u611f\u77e5RAG\u548c\u81ea\u53cd\u601d\u673a\u5236\uff0c\u5728\u672c\u5730\u81ea\u52a8\u751f\u6210\u5e76\u9a8c\u8bc1FFmpeg/VVenC\u89c6\u9891\u5904\u7406\u547d\u4ee4\uff0c\u89e3\u51b3\u4e91\u7aefLLM\u7684\u7b97\u529b\u3001\u9690\u79c1\u548c\u6210\u672c\u95ee\u9898\uff0cQwen2.5+ELLMPEG\u5b9e\u73b078%\u51c6\u786e\u7387\u4e14\u96f6API\u6210\u672c", "motivation": "\u4e91\u7aefLLM\u90e8\u7f72\u5b58\u5728\u4e09\u5927\u5c40\u9650\uff1a\u8ba1\u7b97\u80fd\u8017\u9ad8\u3001\u8fdc\u7a0b\u5904\u7406\u5e26\u6765\u9690\u79c1\u53ef\u9760\u6027\u98ce\u9669\u3001\u4ee5\u53ca\u6301\u7eedAPI\u8c03\u7528\u6210\u672c\u3002\u540c\u65f6\uff0c\u5f00\u6e90\u672c\u5730LLM\u5728\u591a\u9886\u57df\u5e94\u7528\u6f5c\u529b\u672a\u88ab\u5145\u5206\u6316\u6398\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u7cbe\u786e\u5de5\u5177\u8c03\u7528\u7684\u89c6\u9891\u5904\u7406\u573a\u666f\u3002", "method": "\u8bbe\u8ba1ELLMPEG\u8fb9\u7f18\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u6838\u5fc3\u5305\u62ec\uff1a\uff081\uff09\u5de5\u5177\u611f\u77e5\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\uff0c\u4ece\u672c\u5730\u77e5\u8bc6\u5e93\u83b7\u53d6\u76f8\u5173\u547d\u4ee4\u793a\u4f8b\uff1b\uff082\uff09\u8fed\u4ee3\u81ea\u53cd\u601d\u673a\u5236\uff0c\u6301\u7eed\u9a8c\u8bc1\u548c\u4fee\u6b63\u751f\u6210\u7684\u547d\u4ee4\uff1b\uff083\uff09\u76f4\u63a5\u5728\u8fb9\u7f18\u8bbe\u5907\u751f\u6210\u53ef\u6267\u884c\u7684FFmpeg\u548cVVenC\u547d\u4ee4\uff0c\u5b8c\u5168\u8131\u79bb\u4e91\u7aefAPI\u3002\u6784\u5efa\u5305\u542b480\u6761\u591a\u6837\u5316\u67e5\u8be2\u7684\u4e13\u7528\u8bc4\u6d4b\u6570\u636e\u96c6\u3002", "result": "\u5728FFmpeg\u548cVVenC\u6570\u636e\u96c6\u4e0a\uff0cQwen2.5+ELLMPEG\u6846\u67b6\u8fbe\u523078%\u5e73\u5747\u547d\u4ee4\u751f\u6210\u51c6\u786e\u7387\uff0c\u96f6API\u6210\u672c\uff0c\u4f18\u4e8e\u5176\u4ed6\u5f00\u6e90\u6a21\u578b\u3002\u7cfb\u7edf\u80fd\u8bc4\u4f30\u547d\u4ee4\u6709\u6548\u6027\u3001\u63a8\u7406\u901f\u5ea6\u3001\u80fd\u6548\uff0c\u5e76\u9a8c\u8bc1\u8fd0\u884c\u65f6\u6b63\u786e\u6027\u3002", "conclusion": "ELLMPEG\u6210\u529f\u5c06\u667a\u80fd\u4f53\u63a8\u7406\u80fd\u529b\u4e0e\u8fb9\u7f18\u8ba1\u7b97\u7ed3\u5408\uff0c\u4e3a\u89c6\u9891\u5904\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u9ad8\u9690\u79c1\u3001\u9ad8\u6548\u80fd\u7684\u672c\u5730\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u5f00\u6e90LLM\u5728\u5de5\u5177\u4f7f\u7528\u573a\u666f\u7684\u5b9e\u7528\u4ef7\u503c\uff0c\u4e3a\u8fb9\u7f18AI\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.02177", "categories": ["cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2602.02177", "abs": "https://arxiv.org/abs/2602.02177", "authors": ["Yan-Hao Yang", "Zhihao Xu", "Lei Ying", "Qizhong Zhu"], "title": "Interaction-induced moir\u00e9 lattices: from mosaic mobility edges to many-body localization", "comment": "10 pages, 8 figures. Comments are welcome", "summary": "We study localization driven solely by interparticle interactions in moir\u00e9 lattice systems without intrinsic disorder or externally imposed quasiperiodic potentials. We consider a one-dimensional bilayer with incommensurate lattice constants, described by a spin-dependent Fermi-Hubbard-type model with short-range interlayer interactions, where quasiperiodicity emerges only through interactions. Exact diagonalization shows that quenching hopping in one layer generates an interaction-induced mosaic potential with multiple mobility edges. When both layers are dynamical, increasing interlayer interactions drives transitions among ergodic, critical, and many-body localized regimes, with energy-dependent coexistence in certain parameter ranges. An exact mapping to a noninteracting single-particle model on a higher-dimensional structured graph provides a unified interpretation of these results and suggests an experimentally accessible route to interaction-induced moir\u00e9 physics and localization.", "AI": {"tldr": "This paper demonstrates that interparticle interactions alone can drive many-body localization in moir\u00e9 bilayers without disorder, revealing tunable phase transitions through interlayer coupling.", "motivation": "Traditional localization requires disorder or external quasiperiodic potentials; this work explores whether interactions by themselves can generate localization in clean moir\u00e9 systems, opening new pathways for many-body physics.", "method": "Studies a 1D incommensurate bilayer Fermi-Hubbard model with short-range interlayer interactions using exact diagonalization and maps the problem to a noninteracting single-particle model on a higher-dimensional structured graph.", "result": "Quenching hopping creates interaction-induced mosaic potentials with multiple mobility edges; increasing interlayer interactions drives transitions among ergodic, critical, and many-body localized regimes with energy-dependent phase coexistence.", "conclusion": "Provides a unified theoretical framework for interaction-driven localization in moir\u00e9 systems and proposes experimentally accessible routes to observe these phenomena."}}
{"id": "2602.00962", "categories": ["cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.00962", "abs": "https://arxiv.org/abs/2602.00962", "authors": ["David Vaknin"], "title": "Critical Temperatures from Domain-Wall Microstate Counting: A Topological Solution for the Potts Universality Class", "comment": null, "summary": "We derive a universal relation for the critical temperatures of the $q$-state Potts model based on the counting of domain-wall microstates. By balancing interface energy against configurational entropy, we show that the critical temperature is determined by the ratio of the coordination-dependent energy cost to the logarithm of a total multiplicity factor. This factor decomposes into a lattice-topological constant, representing a projection from an underlying orthogonal Euclidean space, and a term representing Markovian sampling in the $q$-dimensional state space. The framework recovers exact solutions for two-dimensional square, triangular, and honeycomb lattices and achieves sub-3\\% accuracy for three-dimensional simple cubic, bcc, fcc, and diamond geometries. This approach unifies the Potts universality class into a single geometric classification, revealing that the phase transition is governed by the saturation of interface propagation through the lattice manifold and providing a predictive tool that characterizes the entire $q$-state family from a single topological calibration.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7574\u58c1\u5fae\u89c2\u72b6\u6001\u8ba1\u6570\u7684q\u6001Potts\u6a21\u578b\u4e34\u754c\u6e29\u5ea6\u666e\u9002\u5173\u7cfb\uff0c\u901a\u8fc7\u754c\u9762\u80fd\u4e0e\u6784\u578b\u71b5\u5e73\u8861\uff0c\u5b9e\u73b0\u4e8c\u7ef4\u7cbe\u786e\u89e3\u548c\u4e09\u7ef4<3%\u7cbe\u5ea6\uff0c\u7edf\u4e00Potts\u666e\u9002\u7c7b\u4e3a\u51e0\u4f55\u5206\u7c7b", "motivation": "\u5efa\u7acbq\u6001Potts\u6a21\u578b\u4e34\u754c\u6e29\u5ea6\u7684\u666e\u9002\u89e3\u6790\u8868\u8fbe\u5f0f\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u9700\u9010\u4e2a\u8ba1\u7b97\u4e0d\u540cq\u503c\u548c\u6676\u683c\u7ed3\u6784\u7684\u95ee\u9898", "method": "\u901a\u8fc7\u7edf\u8ba1\u7574\u58c1\u5fae\u89c2\u72b6\u6001\u6570\uff0c\u5e73\u8861\u754c\u9762\u80fd\u91cf\u6210\u672c\u4e0e\u6784\u578b\u71b5\uff0c\u63a8\u5bfc\u51fa\u4e34\u754c\u6e29\u5ea6\u53d6\u51b3\u4e8e\u914d\u4f4d\u76f8\u5173\u80fd\u91cf\u6210\u672c\u4e0e\u603b\u591a\u91cd\u6027\u56e0\u5b50\u5bf9\u6570\u7684\u6bd4\u503c\uff0c\u8be5\u56e0\u5b50\u5305\u542b\u6676\u683c\u62d3\u6251\u5e38\u6570\u548cq\u7ef4\u72b6\u6001\u7a7a\u95f4\u7684\u9a6c\u5c14\u53ef\u592b\u91c7\u6837\u9879", "result": "\u7cbe\u786e\u6062\u590d\u4e8c\u7ef4\u65b9/\u4e09\u89d2/\u8702\u7a9d\u6676\u683c\u89e3\uff0c\u5bf9\u4e09\u7ef4\u7b80\u5355\u7acb\u65b9/bcc/fcc/\u91d1\u521a\u77f3\u7ed3\u6784\u8fbe\u5230<3%\u7cbe\u5ea6\uff0c\u63ed\u793a\u76f8\u53d8\u7531\u754c\u9762\u4f20\u64ad\u5728\u6676\u683c\u6d41\u5f62\u4e2d\u7684\u9971\u548c\u6548\u5e94\u4e3b\u5bfc", "conclusion": "\u5c06Potts\u666e\u9002\u7c7b\u7edf\u4e00\u4e3a\u5355\u4e00\u51e0\u4f55\u5206\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7\u62d3\u6251\u6821\u51c6\u5373\u53ef\u9884\u6d4b\u6574\u4e2aq\u6001\u5bb6\u65cf\u7684\u76f8\u53d8\u884c\u4e3a\uff0c\u4e3a\u4e34\u754c\u73b0\u8c61\u63d0\u4f9b\u62d3\u6251\u5b66\u8868\u5f81\u5de5\u5177"}}
{"id": "2602.02292", "categories": ["cond-mat.str-el", "hep-th", "math-ph", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.02292", "abs": "https://arxiv.org/abs/2602.02292", "authors": ["Hisham Sati", "Urs Schreiber"], "title": "Non-Perturbative SDiff Covariance of Fractional Quantum Hall Excitations", "comment": "7 pages, 1 table", "summary": "Collective excitations of Fractional Quantum Hall (FQH) liquids at long wavelengths are thought to be of a generally covariant geometric nature, governed by area-preserving diffeomorphisms ($\\mathrm{SDiff}$). But current analyses rely solely on the corresponding perturbative $w_\\infty$ Lie algebra. We argue this is insufficient: We identify a non-perturbative construction of the effective Maxwell-Chern-Simons quantum field theory which carries unitary $\\mathrm{SDiff}$ equivariance. But this turns out to be non-differentiable, suggesting FQH excitation phenomenology beyond the $w_\\infty$ algebra.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6311\u6218\u4e86\u5f53\u524d\u4ec5\u4f9d\u8d56\u5fae\u6270\u6027w\u221e\u674e\u4ee3\u6570\u63cf\u8ff0\u5206\u6570\u91cf\u5b50\u970d\u5c14\u6db2\u4f53\u957f\u6ce2\u96c6\u4f53\u6fc0\u53d1\u7684\u505a\u6cd5\uff0c\u63d0\u51fa\u975e\u5fae\u6270\u7684Maxwell-Chern-Simons\u91cf\u5b50\u573a\u8bba\u6784\u9020\uff0c\u63ed\u793a\u5176\u975e\u53ef\u5fae\u7279\u6027\u6697\u793a\u8d85\u8d8aw\u221e\u4ee3\u6570\u7684\u65b0\u7269\u7406\u73b0\u8c61", "motivation": "\u73b0\u6709\u7406\u8bba\u4ec5\u57fa\u4e8e\u5fae\u6270\u6027w\u221e\u674e\u4ee3\u6570\u5206\u6790\u5206\u6570\u91cf\u5b50\u970d\u5c14\u6db2\u4f53\u7684\u51e0\u4f55\u6fc0\u53d1\u6a21\u5f0f\uff0c\u65e0\u6cd5\u5145\u5206\u63cf\u8ff0\u975e\u5fae\u6270\u6548\u5e94\uff0c\u9700\u6784\u5efa\u66f4\u5b8c\u5907\u7684\u91cf\u5b50\u573a\u8bba\u6846\u67b6\u4ee5\u63ed\u793a\u5b8c\u6574\u7269\u7406\u56fe\u666f", "method": "\u901a\u8fc7\u975e\u5fae\u6270\u65b9\u6cd5\u6784\u5efa\u5177\u6709\u5e7a\u6b63SDiff\u7b49\u53d8\u6027\u7684\u6709\u6548Maxwell-Chern-Simons\u91cf\u5b50\u573a\u8bba\uff0c\u7a81\u7834\u4f20\u7edf\u5fae\u6270\u4ee3\u6570\u6846\u67b6\u7684\u9650\u5236", "result": "\u6240\u6784\u5efa\u7684\u7406\u8bba\u5448\u73b0\u975e\u53ef\u5fae\u7279\u6027\uff0c\u8868\u660e\u5206\u6570\u91cf\u5b50\u970d\u5c14\u6fc0\u53d1\u5b58\u5728\u65e0\u6cd5\u88abw\u221e\u4ee3\u6570\u63cf\u8ff0\u7684\u8d85\u8d8a\u6027\u73b0\u8c61", "conclusion": "\u5206\u6570\u91cf\u5b50\u970d\u5c14\u6db2\u4f53\u7684\u6fc0\u53d1\u884c\u4e3a\u5305\u542b\u8d85\u8d8a\u5f53\u524d\u4ee3\u6570\u7406\u8bba\u7684\u65b0\u7269\u7406\u673a\u5236\uff0c\u9700\u53d1\u5c55\u975e\u5fae\u6270\u65b9\u6cd5\u4ee5\u5b8c\u5584\u7406\u8bba\u4f53\u7cfb"}}
{"id": "2602.00156", "categories": ["quant-ph", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2602.00156", "abs": "https://arxiv.org/abs/2602.00156", "authors": ["Jaya Vasavi Pamidimukkala", "Himanshu Sahu", "Ashwini Kannan", "Janani Ananthanarayanan", "Kalyan Dasgupta", "Sanjib Senapati"], "title": "Accelerating De Novo Genome Assembly via Quantum-Assisted Graph Optimization with Bitstring Recovery", "comment": null, "summary": "Genome sequencing is essential to decode genetic information, identify organisms, understand diseases and advance personalized medicine. A critical step in any genome sequencing technique is genome assembly. However, de novo genome assembly, which involves constructing an entire genome sequence from scratch without a reference genome, presents significant challenges due to its high computational complexity, affecting both time and accuracy. In this study, we propose a hybrid approach utilizing a quantum computing-based optimization algorithm integrated with classical pre-processing to expedite the genome assembly process. Specifically, we present a method to solve the Hamiltonian and Eulerian paths within the genome assembly graph using gate-based quantum computing through a Higher-Order Binary Optimization (HOBO) formulation with the Variational Quantum Eigensolver algorithm (VQE), in addition to a novel bitstring recovery mechanism to improve optimizer traversal of the solution space. A comparative analysis with classical optimization techniques was performed to assess the effectiveness of our quantum-based approach in genome assembly. The results indicate that, as quantum hardware continues to evolve and noise levels diminish, our formulation holds a significant potential to accelerate genome sequencing by offering faster and more accurate solutions to the complex challenges in genomic research.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u57fa\u56e0\u7ec4\u7ec4\u88c5\u65b9\u6cd5\uff0c\u7528\u91cf\u5b50VQE\u7b97\u6cd5\u6c42\u89e3\u7ec4\u88c5\u56fe\u8def\u5f84\u95ee\u9898\uff0c\u566a\u58f0\u964d\u4f4e\u540e\u6709\u671b\u52a0\u901f\u6d4b\u5e8f", "motivation": "\u89e3\u51b3\u65e0\u53c2\u8003\u57fa\u56e0\u7ec4\u7ec4\u88c5(de novo)\u7684\u9ad8\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u63d0\u5347\u57fa\u56e0\u7ec4\u6d4b\u5e8f\u901f\u5ea6\u548c\u7cbe\u5ea6", "method": "\u7ed3\u5408\u7ecf\u5178\u9884\u5904\u7406\u4e0e\u91cf\u5b50\u4f18\u5316\uff1a\u91c7\u7528\u9ad8\u9636\u4e8c\u5143\u4f18\u5316(HOBO)\u5efa\u6a21\uff0c\u901a\u8fc7\u53d8\u5206\u91cf\u5b50\u672c\u5f81\u6c42\u89e3\u5668(VQE)\u6c42\u89e3\u54c8\u5bc6\u987f/\u6b27\u62c9\u8def\u5f84\uff0c\u5e76\u8bbe\u8ba1\u65b0\u578b\u6bd4\u7279\u4e32\u6062\u590d\u673a\u5236\u4f18\u5316\u641c\u7d22\u8fc7\u7a0b", "result": "\u76f8\u6bd4\u7ecf\u5178\u4f18\u5316\u65b9\u6cd5\uff0c\u8be5\u91cf\u5b50\u65b9\u6848\u5728\u786c\u4ef6\u566a\u58f0\u964d\u4f4e\u540e\u5c55\u73b0\u51fa\u52a0\u901f\u6f5c\u529b\uff0c\u4f46\u5f53\u524d\u4f9d\u8d56\u91cf\u5b50\u786c\u4ef6\u53d1\u5c55\u6c34\u5e73", "conclusion": "\u968f\u7740\u91cf\u5b50\u786c\u4ef6\u8fdb\u6b65\uff0c\u6b64\u65b9\u6cd5\u6709\u671b\u7a81\u7834\u57fa\u56e0\u7ec4\u7ec4\u88c5\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u63a8\u52a8\u7cbe\u51c6\u533b\u5b66\u53d1\u5c55"}}
{"id": "2602.00030", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00030", "abs": "https://arxiv.org/abs/2602.00030", "authors": ["Takato Yasuno"], "title": "RAPTOR-AI for Disaster OODA Loop: Hierarchical Multimodal RAG with Experience-Driven Agentic Decision-Making", "comment": "4 figures, 3 tables", "summary": "Effective humanitarian assistance and disaster relief (HADR) requires rapid situational understanding, reliable decision support, and the ability to generalize across diverse and previously unseen disaster contexts. This work introduces an agentic Retrieval-Augmented Generation (RAG) framework designed to support the three canonical phases of disaster response: initial rescue, mid-term recovery, and long-term reconstruction. To achieve robust multimodal grounding, we construct a hierarchical knowledge base that integrates textual disaster manuals, historical lessons (e.g., the 2011 Tohoku earthquake), and both aerial and ground-level imagery. Our system builds on the open-source multimodal implementation, which processes 46 tsunami-related PDFs (2,378 pages) using BLIP-based image captioning, ColVBERT embeddings, and long-context summarization to generate an efficient, structured multimodal retrieval tree optimized for disaster knowledge preservation. An agentic controller dynamically selects retrieval strategies (e.g., RAPTOR, ColBERT) through entropy-aware scene abstraction, enabling adaptive reasoning across heterogeneous inputs. Additionally, a lightweight LoRA-based post-training method injects experiential knowledge from past disasters, enhancing the models' capacity to support both expert and non-expert responders. Experiments on real disaster datasets demonstrate improved situational grounding, enhanced task decomposition accuracy, and superior usability for emergency operations. Incorporating recent advances in long-context RAG systems, agentic information retrieval, and contemporary emergency response AI, our system achieves substantial gains through adaptive retrieval-augmented generation with self-reasoning and multimodal chain-of-thought capabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u667a\u80fd\u4f53\u589e\u5f3a\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u6846\u67b6\uff0c\u7528\u4e8e\u707e\u5bb3\u6551\u63f4\u7684\u4e09\u4e2a\u9636\u6bb5\uff1a\u521d\u671f\u6551\u63f4\u3001\u4e2d\u671f\u6062\u590d\u548c\u957f\u671f\u91cd\u5efa\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u6784\u5efa\u5206\u5c42\u591a\u6a21\u6001\u77e5\u8bc6\u5e93\uff0c\u7ed3\u5408\u6587\u672c\u624b\u518c\u3001\u5386\u53f2\u7ecf\u9a8c\u548c\u5f71\u50cf\u6570\u636e\uff0c\u5b9e\u73b0\u8de8\u707e\u5bb3\u573a\u666f\u7684\u7a33\u5065\u51b3\u7b56\u652f\u6301\u3002", "motivation": "\u4eba\u9053\u4e3b\u4e49\u63f4\u52a9\u548c\u6551\u707e(HADR)\u9700\u8981\u5feb\u901f\u6001\u52bf\u7406\u89e3\u3001\u53ef\u9760\u51b3\u7b56\u652f\u6301\u4ee5\u53ca\u8de8\u591a\u6837\u5316\u707e\u5bb3\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u5f02\u6784\u591a\u6a21\u6001\u6570\u636e\u5e76\u9002\u5e94\u672a\u89c1\u8fc7\u7684\u707e\u5bb3\u60c5\u5883\u3002", "method": "1) \u6784\u5efa\u5206\u5c42\u77e5\u8bc6\u5e93\u6574\u5408\u707e\u5bb3\u624b\u518c\u3001\u5386\u53f2\u6848\u4f8b(\u59822011\u5e74\u4e1c\u65e5\u672c\u5927\u5730\u9707)\u548c\u7a7a/\u5730\u5f71\u50cf\uff1b2) \u57fa\u4e8eBLIP\u56fe\u50cf\u63cf\u8ff0\u3001ColVBERT\u5d4c\u5165\u548c\u957f\u6587\u672c\u6458\u8981\u5904\u740646\u4efd\u6d77\u5578PDF(2378\u9875)\uff1b3) \u667a\u80fd\u4f53\u63a7\u5236\u5668\u901a\u8fc7\u71b5\u611f\u77e5\u573a\u666f\u62bd\u8c61\u52a8\u6001\u9009\u62e9\u68c0\u7d22\u7b56\u7565(RAPTOR/ColBERT)\uff1b4) \u91c7\u7528LoRA\u5fae\u8c03\u6ce8\u5165\u5386\u53f2\u707e\u5bb3\u7ecf\u9a8c\u77e5\u8bc6\uff1b5) \u7ed3\u5408\u81ea\u63a8\u7406\u548c\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u80fd\u529b\u3002", "result": "\u5728\u771f\u5b9e\u707e\u5bb3\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u663e\u793a\uff1a\u6001\u52bf grounding \u663e\u8457\u63d0\u5347\uff0c\u4efb\u52a1\u5206\u89e3\u51c6\u786e\u7387\u589e\u5f3a\uff0c\u5e94\u6025\u884c\u52a8\u53ef\u7528\u6027\u4f18\u8d8a\u3002\u7cfb\u7edf\u901a\u8fc7\u81ea\u9002\u5e94\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5b9e\u73b0\u5b9e\u8d28\u6027\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5c06\u5148\u8fdbRAG\u6280\u672f\u5e94\u7528\u4e8e\u707e\u5bb3\u54cd\u5e94\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u667a\u80fd\u68c0\u7d22\u548c\u81ea\u9002\u5e94\u63a8\u7406\uff0c\u4e3a\u4e13\u5bb6\u548c\u65b0\u624b\u6551\u63f4\u4eba\u5458\u63d0\u4f9b\u7a33\u5065\u7684\u51b3\u7b56\u652f\u6301\uff0c\u5c55\u73b0\u4e86\u5728\u590d\u6742\u6551\u707e\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.00276", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00276", "abs": "https://arxiv.org/abs/2602.00276", "authors": ["Aditya Kumar", "William W. Cohen"], "title": "Localizing and Correcting Errors for LLM-based Planners", "comment": null, "summary": "Large language models (LLMs) have demonstrated strong reasoning capabilities on math and coding, but frequently fail on symbolic classical planning tasks. Our studies, as well as prior work, show that LLM-generated plans routinely violate domain constraints given in their instructions (e.g., walking through walls). To address this failure, we propose iteratively augmenting instructions with Localized In-Context Learning (L-ICL) demonstrations: targeted corrections for specific failing steps. Specifically, L-ICL identifies the first constraint violation in a trace and injects a minimal input-output example giving the correct behavior for the failing step. Our proposed technique of L-ICL is much effective than explicit instructions or traditional ICL, which adds complete problem-solving trajectories, and many other baselines. For example, on an 8x8 gridworld, L-ICL produces valid plans 89% of the time with only 60 training examples, compared to 59% for the best baseline, an increase of 30%. L-ICL also shows dramatic improvements in other domains (gridworld navigation, mazes, Sokoban, and BlocksWorld), and on several LLM architectures.", "AI": {"tldr": "\u63d0\u51fa\u5c40\u90e8\u4e0a\u4e0b\u6587\u5b66\u4e60(L-ICL)\u65b9\u6cd5\uff0c\u901a\u8fc7\u9488\u5bf9\u89c4\u5212\u5931\u8d25\u6b65\u9aa4\u7684\u6700\u5c0f\u5316\u793a\u4f8b\u4fee\u6b63\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7b26\u53f7\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u7ea6\u675f\u9075\u5b88\u80fd\u529b", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7b26\u53f7\u7ecf\u5178\u89c4\u5212\u4efb\u52a1\u4e2d\u9891\u7e41\u8fdd\u53cd\u9886\u57df\u7ea6\u675f\uff08\u5982\u7a7f\u5899\uff09\uff0c\u5c3d\u7ba1\u5176\u5728\u6570\u5b66\u548c\u4ee3\u7801\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4f18\u5f02", "method": "\u8fed\u4ee3\u5f0f\u5c40\u90e8\u4e0a\u4e0b\u6587\u5b66\u4e60(L-ICL)\uff1a\u8bc6\u522b\u89c4\u5212\u8f68\u8ff9\u4e2d\u9996\u4e2a\u7ea6\u675f\u8fdd\u53cd\u6b65\u9aa4\uff0c\u6ce8\u5165\u9488\u5bf9\u8be5\u6b65\u9aa4\u7684\u6700\u5c0f\u5316\u8f93\u5165\u8f93\u51fa\u793a\u4f8b\u8fdb\u884c\u4fee\u6b63", "result": "\u57288x8\u7f51\u683c\u4e16\u754c\u4ec5\u970060\u4e2a\u793a\u4f8b\u5373\u8fbe89%\u6709\u6548\u89c4\u5212\u7387\uff08\u6700\u4f73\u57fa\u7ebf59%\uff09\uff0c\u5728\u7f51\u683c\u5bfc\u822a/\u8ff7\u5bab/Sokoban/BlocksWorld\u7b49\u591a\u9886\u57df\u53ca\u591a\u6a21\u578b\u67b6\u6784\u5747\u663e\u8457\u63d0\u5347", "conclusion": "\u76f8\u6bd4\u5b8c\u6574\u8f68\u8ff9\u7684\u4f20\u7edfICL\u6216\u663e\u5f0f\u6307\u4ee4\uff0cL-ICL\u901a\u8fc7\u7cbe\u51c6\u5b9a\u4f4d\u5931\u8d25\u70b9\u63d0\u4f9b\u6700\u5c0f\u5316\u4fee\u6b63\uff0c\u80fd\u66f4\u9ad8\u6548\u89e3\u51b3LLM\u7684\u89c4\u5212\u7ea6\u675f\u8fdd\u53cd\u95ee\u9898"}}
{"id": "2602.00245", "categories": ["quant-ph", "cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2602.00245", "abs": "https://arxiv.org/abs/2602.00245", "authors": ["Pedro R. Nic\u00e1cio Falc\u00e3o", "Jakub Zakrzewski", "Piotr Sierant"], "title": "Fermionic magic resources in disordered quantum spin chains", "comment": "5 pages + supplement. Comments are welcome!", "summary": "Fermionic non-Gaussianity quantifies a quantum state's deviation from a classically tractable free-fermionic description, constituting a necessary resource for computational quantum advantage. Here we use fermionic antiflatness (FAF) to measure this deviation across ergodic and many-body localized (MBL) regimes. We focus on the paradigmatic disordered spin-$1\\!/2$ XXZ chain and its impurity variant with local interactions. Across highly excited eigenstates, FAF evolves from typical-state behavior at weak disorder to strongly suppressed values deep in the MBL regime, with volume-law scaling in the XXZ chain and an area-law bound in the impurity setting. Rare long range catlike eigenstates exhibit a pronounced enhancement of FAF, making it a sensitive diagnostic of mechanisms proposed to destabilize MBL. Starting from product states, we find that in the MBL regime FAF grows slowly in time, approaching saturation via a power-law relaxation. Overall, our results show that MBL suppresses fermionic non-Gaussianity, and the associated complexity beyond free fermions, while ergodicity restores it, motivating explorations of fermionic non-Gaussianity in other ergodicity-breaking phenomena.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u8d39\u7c73\u5b50\u53cd\u5e73\u6027\uff08FAF\uff09\u4f5c\u4e3a\u91cf\u5316\u8d39\u7c73\u5b50\u7cfb\u7edf\u975e\u9ad8\u65af\u6027\uff08\u91cf\u5b50\u8ba1\u7b97\u4f18\u52bf\u8d44\u6e90\uff09\u7684\u65b0\u6307\u6807\uff0c\u63ed\u793a\u591a\u4f53\u5c40\u57df\u5316\uff08MBL\uff09\u4f1a\u6291\u5236\u975e\u9ad8\u65af\u6027\u800c\u904d\u5386\u6027\u4f1a\u6062\u590d\u5b83\uff0c\u4e3a\u7814\u7a76\u91cf\u5b50\u590d\u6742\u5ea6\u63d0\u4f9b\u65b0\u5de5\u5177", "motivation": "\u8d39\u7c73\u5b50\u975e\u9ad8\u65af\u6027\u662f\u8d85\u8d8a\u7ecf\u5178\u81ea\u7531\u8d39\u7c73\u5b50\u63cf\u8ff0\u7684\u5173\u952e\u91cf\u5b50\u8d44\u6e90\uff0c\u5bf9\u5b9e\u73b0\u91cf\u5b50\u8ba1\u7b97\u4f18\u52bf\u81f3\u5173\u91cd\u8981\uff1b\u9700\u91cf\u5316\u5176\u5728\u591a\u4f53\u5c40\u57df\u5316\uff08MBL\uff09\u4e0e\u904d\u5386\u6027\u4f53\u7cfb\u4e2d\u7684\u884c\u4e3a\u5dee\u5f02", "method": "\u91c7\u7528\u8d39\u7c73\u5b50\u53cd\u5e73\u6027\uff08FAF\uff09\u4f5c\u4e3a\u975e\u9ad8\u65af\u6027\u5ea6\u91cf\uff0c\u5728\u542b\u65e0\u5e8f\u7684XXZ\u81ea\u65cb\u94fe\u53ca\u542b\u6742\u8d28\u76f8\u4e92\u4f5c\u7528\u6a21\u578b\u4e2d\uff0c\u5206\u6790\u9ad8\u6fc0\u53d1\u672c\u5f81\u6001\u3001\u7a00\u6709\u732b\u6001\u53ca\u521d\u6001\u52a8\u529b\u5b66\u4e0b\u7684FAF\u6f14\u5316", "result": "\u2460 \u5f31\u65e0\u5e8f\u4e0bFAF\u5448\u5178\u578b\u6001\u884c\u4e3a\uff0cMBL\u76f8\u6df1\u5904\u88ab\u5f3a\u70c8\u6291\u5236\uff1bXXZ\u94fe\u5448\u4f53\u79ef\u5f8b\u6807\u5ea6\uff0c\u6742\u8d28\u6a21\u578b\u53d7\u9762\u79ef\u5f8b\u7ea6\u675f \u2461 \u7a00\u6709\u957f\u7a0b\u732b\u6001\u663e\u8457\u589e\u5f3aFAF\uff0c\u53ef\u8bca\u65adMBL\u5931\u7a33\u673a\u5236 \u2462 MBL\u76f8\u4e2d\u521d\u6001\u7684FAF\u5448\u6162\u901f\u5e42\u5f8b\u589e\u957f\u5e76\u9971\u548c", "conclusion": "MBL\u6709\u6548\u6291\u5236\u8d39\u7c73\u5b50\u975e\u9ad8\u65af\u6027\u53ca\u8d85\u8d8a\u81ea\u7531\u8d39\u7c73\u5b50\u7684\u590d\u6742\u5ea6\uff0c\u800c\u904d\u5386\u6027\u6062\u590d\u8be5\u7279\u6027\uff1bFAF\u53ef\u4f5c\u4e3a\u7814\u7a76\u5176\u4ed6\u975e\u904d\u5386\u73b0\u8c61\uff08\u5982\u91cf\u5b50\u70ed\u5316\u7834\u574f\uff09\u7684\u654f\u611f\u63a2\u9488"}}
{"id": "2602.01021", "categories": ["cond-mat.stat-mech", "cs.SI", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2602.01021", "abs": "https://arxiv.org/abs/2602.01021", "authors": ["Harrison Hartle", "P. L. Krapivsky"], "title": "Leaves of preferential attachment trees", "comment": "23 pages, 5 figures", "summary": "We provide a local probabilistic description of the limiting statistics of large preferential attachment trees in terms of the ordinary degree (number of neighbors) but augmented with information on leafdegree (number of neighbors that are leaves). The full description is the joint degree-leafdegree distribution $n_{k,\\ell}$, which we derive from its associated multivariate generating function. From $n_{k,\\ell}$ we obtain the leafdegree distribution, $m_{\\ell}$, as well as the fraction of vertices that are protected (nonleaves with leafdegree zero) as a function of degree, $n_{k,0}$, among numerous other results. We also examine fluctuations and concentration of joint degree-leafdegree empirical counts $N_{k,\\ell}$. Although our main findings pertain to the preferential attachment tree, the approach we present is highly generalizable and can characterize numerous existing models, in addition to facilitating the development of tractable new models. We further demonstrate the approach by analyzing $n_{k,\\ell}$ in two other models: the random recursive tree, and a redirection-based model.", "AI": {"tldr": "\u63d0\u51fa\u63cf\u8ff0\u504f\u597d\u4f9d\u9644\u6a39\u7684\u806f\u5408Degree-Leafdegree\u5206\u4f48\u7684\u901a\u7528\u6846\u67b6\uff0c\u63ed\u793a\u7bc0\u9ede\u7d50\u69cb\u7684\u7d30\u5fae\u7279\u5fb5\uff08\u5982\u8449\u7bc0\u9ede\u6bd4\u4f8b\u3001\u53d7\u4fdd\u8b77\u7bc0\u9ede\uff09\uff0c\u9069\u7528\u65bc\u591a\u7a2e\u7db2\u7d61\u6a21\u578b\u3002", "motivation": "\u73fe\u6709\u7814\u7a76\u591a\u805a\u7126\u7bc0\u9edeDegree\uff08\u9130\u5c45\u6578\uff09\uff0c\u4f46\u5ffd\u7565Leafdegree\uff08\u8449\u9130\u5c45\u6578\uff09\u5c0d\u7db2\u7d61\u7d50\u69cb\u7684\u95dc\u9375\u5f71\u97ff\uff1b\u9700\u5c40\u90e8\u6982\u7387\u63cf\u8ff0\u4ee5\u7406\u89e3\u5927\u578b\u504f\u597d\u4f9d\u9644\u6a39\u7684\u6975\u9650\u7d71\u8a08\u7279\u6027\u3002", "method": "\u901a\u904e\u591a\u5143\u751f\u6210\u51fd\u6578\u63a8\u5c0e\u806f\u5408Degree-Leafdegree\u5206\u4f48 $n_{k,\\ell}$\uff0c\u4e26\u57fa\u65bc\u6b64\u8a08\u7b97\u8449\u7bc0\u9ede\u5206\u4f48 $m_{\\ell}$\u3001\u53d7\u4fdd\u8b77\u7bc0\u9ede\u6bd4\u4f8b $n_{k,0}$ \u53ca\u5be6\u8b49\u8a08\u6578 $N_{k,\\ell}$ \u7684\u6ce2\u52d5\u8207\u96c6\u4e2d\u6027\u3002", "result": "1. \u7372\u5f97\u5b8c\u6574\u7684 $n_{k,\\ell}$ \u89e3\u6790\u8868\u9054\uff1b<br>2. \u5c0e\u51fa\u8449\u7bc0\u9ede\u5206\u4f48\u8207\u53d7\u4fdd\u8b77\u7bc0\u9ede\u6bd4\u4f8b\uff1b<br>3. \u9a57\u8b49\u65b9\u6cd5\u9069\u7528\u65bc\u96a8\u6a5f\u905e\u6b78\u6a39\u8207\u91cd\u5b9a\u5411\u6a21\u578b\uff1b<br>4. \u63ed\u793a\u50b3\u7d71Degree\u5206\u4f48\u7121\u6cd5\u6355\u6349\u7684\u7d50\u69cb\u7279\u5fb5\u3002", "conclusion": "\u6b64\u6846\u67b6\u7a81\u7834\u50b3\u7d71Degree-only\u5206\u6790\uff0c\u63d0\u4f9b\u9ad8\u5ea6\u901a\u7528\u5de5\u5177\uff0c\u53ef\u7cbe\u78ba\u8868\u5fb5\u591a\u7a2e\u7db2\u7d61\u6a21\u578b\u7684\u5c40\u90e8\u7d50\u69cb\uff0c\u4e26\u4fc3\u9032\u958b\u767c\u53ef\u8655\u7406\u7684\u65b0\u6a21\u578b\u3002"}}
{"id": "2602.02332", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.02332", "abs": "https://arxiv.org/abs/2602.02332", "authors": ["Waldemar Hergett", "Kevin Ackermann", "Erik Walendy", "Sven Spachmann", "Martin Jonak", "Mahmoud Abdel-Hafiez", "Maurits W. Haverkort", "R. Klingeler"], "title": "Critical behavior and evidence of dimensional crossover in quasi-two-dimensional Li$_2$FeSiO$_4$", "comment": null, "summary": "We report thermal expansion and heat capacity studies on Li$_2$FeSiO$_4$ single crystals which enable us to investigate the critical behavior in the magnetically quasi-two-dimensional (2D) material. Pronounced $\u03bb$-shaped anomalies at the magnetic ordering temperature $T_{\\rm N}$ imply significant magneto-elastic coupling. Our analysis of both the thermal expansion and the specific heat data implies the crossover from 2D Ising-like behavior for $|(T-T_{\\rm N})/T_{\\rm N}|>0.3$ to 3D Ising behavior \\rev{below $\\simeq 1.3\\times T_{\\rm N}$. The 2D-like behavior is further supported by density functional calculations which show minimal dispersion perpendicular to the crystallographic $ac$ planes of the layered structure, thereby indicating the 2D nature of magnetism at higher temperatures.} Our results extend the available model materials of quasi-2D magnetism to a high-spin $S=2$ system with tetrahedrally coordinated Fe$^{2+}$-ions, thereby illustrating how magnetic order evolves in a 2D Ising-like system with orbital degrees of freedom.", "AI": {"tldr": "\u901a\u8fc7\u70ed\u81a8\u80c0\u548c\u6bd4\u70ed\u6d4b\u91cf\u7ed3\u5408\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\uff0c\u53d1\u73b0Li\u2082FeSiO\u2084\u5355\u6676\u5728\u78c1\u6709\u5e8f\u6e29\u5ea6T_N\u5904\u5b58\u5728\u4ece\u4e8c\u7ef4\u4f0a\u8f9b\u884c\u4e3a\u5230\u4e09\u7ef4\u4f0a\u8f9b\u884c\u4e3a\u7684\u7ef4\u5ea6\u4ea4\u53c9\u73b0\u8c61", "motivation": "\u63a2\u7a76\u51c6\u4e8c\u7ef4\u78c1\u6027\u6750\u6599\u4e2d\u78c1\u6709\u5e8f\u7684\u4e34\u754c\u884c\u4e3a\uff0c\u7279\u522b\u662f\u542b\u8f68\u9053\u81ea\u7531\u5ea6\u7684\u4e8c\u7ef4\u4f0a\u8f9b\u7cfb\u7edf\u4e2d\u78c1\u5e8f\u6f14\u5316\u673a\u5236", "method": "\u91c7\u7528\u70ed\u81a8\u80c0\u548c\u6bd4\u70ed\u6d4b\u91cf\u6280\u672f\u5206\u6790Li\u2082FeSiO\u2084\u5355\u6676\u7684\u4e34\u754c\u884c\u4e3a\uff0c\u7ed3\u5408\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\u8ba1\u7b97\u9a8c\u8bc1\u6750\u6599\u7684\u4e8c\u7ef4\u78c1\u6027\u7279\u5f81", "result": "\u5728T_N\u5904\u89c2\u5bdf\u5230\u660e\u663e\u7684\u03bb\u578b\u5f02\u5e38\uff1b\u53d1\u73b0|(T-T_N)/T_N|>0.3\u65f6\u5448\u4e8c\u7ef4\u4f0a\u8f9b\u884c\u4e3a\uff0c\u4f4e\u4e8e1.3\u00d7T_N\u65f6\u8f6c\u4e3a\u4e09\u7ef4\u4f0a\u8f9b\u884c\u4e3a\uff1bDFT\u8ba1\u7b97\u8bc1\u5b9eac\u6676\u9762\u5916\u80fd\u5e26\u8272\u6563\u6781\u5c0f", "conclusion": "\u5c06\u51c6\u4e8c\u7ef4\u78c1\u6027\u6a21\u578b\u6750\u6599\u4f53\u7cfb\u6269\u5c55\u81f3\u9ad8\u81ea\u65cbS=2\u7684\u56db\u9762\u4f53\u914d\u4f4dFe\u00b2\u207a\u7cfb\u7edf\uff0c\u63ed\u793a\u4e86\u542b\u8f68\u9053\u81ea\u7531\u5ea6\u4e8c\u7ef4\u4f0a\u8f9b\u7cfb\u7edf\u7684\u78c1\u5e8f\u6f14\u5316\u89c4\u5f8b"}}
{"id": "2602.00227", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.00227", "abs": "https://arxiv.org/abs/2602.00227", "authors": ["Pranay Nayak", "Sreenath K. Manikandan", "Tan Van Vu", "Supriya Krishnamurthy"], "title": "Signatures of coherent initial ensembles on all work moments", "comment": "24 pages, 4 figures", "summary": "Standard treatments of quantum work using projective energy measurements erase initial coherence and alter the dynamics, thereby failing to capture the thermodynamic effects of coherent superpositions of energy eigenstates in an ensemble of initial states. In this article, we use an operational work definition that is non-intrusive, applying it to the case of a driven dissipative qubit, where the qubit's initial preparation comprises coherent superposition states, while the driving is coherence-less. We derive an evolution equation for the moment generating function for this work, faithfully capturing the thermodynamic signature of coherent superpositions in the initial ensemble. We demonstrate that different initial ensembles that correspond to the same density matrix upon ensemble average, while having the same average work, display different work fluctuations. For monotonic driving, we show that fluctuations are maximum for coherence-less initial ensembles. As an application, we consider quantum bit-erasure in finite time and demonstrate significantly different work statistics for erasing a classical bit of information versus a Haar random initial ensemble. Our results indicate that coherence in the initial ensemble can be utilized as a resource for thermodynamic precision without incurring additional dissipative work costs. We also obtain a generalized fluctuation theorem that establishes a new quantum lower bound on the mean dissipated work. This bound, counterintuitively, is also applicable to a \"classical\" initial ensemble with the same initial density matrix and is connected to quantum absolute irreversibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u975e\u4fb5\u5165\u5f0f\u91cf\u5b50\u529f\u5b9a\u4e49\uff0c\u7528\u4e8e\u7814\u7a76\u5177\u6709\u521d\u59cb\u76f8\u5e72\u53e0\u52a0\u6001\u7684\u9a71\u52a8\u8017\u6563\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5177\u6709\u76f8\u540c\u5bc6\u5ea6\u77e9\u9635\u7684\u4e0d\u540c\u521d\u59cb\u7cfb\u7efc\u5e73\u5747\u529f\u76f8\u540c\u4f46\u529f\u6da8\u843d\u4e0d\u540c\uff0c\u521d\u59cb\u76f8\u5e72\u6027\u53ef\u4f5c\u4e3a\u63d0\u5347\u70ed\u529b\u5b66\u7cbe\u5ea6\u7684\u8d44\u6e90\u800c\u4e0d\u589e\u52a0\u989d\u5916\u8017\u6563\u6210\u672c\uff0c\u5e76\u5efa\u7acb\u4e86\u4e00\u4e2a\u9002\u7528\u4e8e\u91cf\u5b50\u548c\u7ecf\u5178\u7cfb\u7efc\u7684\u5e7f\u4e49\u6da8\u843d\u5b9a\u7406\u548c\u65b0\u7684\u91cf\u5b50\u8017\u6563\u529f\u4e0b\u9650\u3002", "motivation": "\u4f20\u7edf\u7684\u6295\u5f71\u80fd\u91cf\u6d4b\u91cf\u65b9\u6cd5\u4f1a\u7834\u574f\u521d\u59cb\u76f8\u5e72\u6027\u5e76\u6539\u53d8\u7cfb\u7edf\u52a8\u529b\u5b66\uff0c\u65e0\u6cd5\u51c6\u786e\u63cf\u8ff0\u521d\u59cb\u6001\u7cfb\u7efc\u4e2d\u76f8\u5e72\u53e0\u52a0\u6001\u7684\u70ed\u529b\u5b66\u6548\u5e94\u3002\u8fd9\u9650\u5236\u4e86\u6211\u4eec\u5bf9\u91cf\u5b50\u76f8\u5e72\u6027\u5728\u70ed\u529b\u5b66\u8fc7\u7a0b\u4e2d\u4f5c\u7528\u7684\u7406\u89e3\u3002", "method": "\u91c7\u7528\u975e\u4fb5\u5165\u5f0f\u64cd\u4f5c\u529f\u5b9a\u4e49\uff0c\u9488\u5bf9\u521d\u59cb\u5236\u5907\u4e3a\u76f8\u5e72\u53e0\u52a0\u6001\u3001\u9a71\u52a8\u4e3a\u65e0\u76f8\u5e72\u6027\u7684\u9a71\u52a8\u8017\u6563\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\uff0c\u63a8\u5bfc\u529f\u77e9\u751f\u6210\u51fd\u6570\u7684\u6f14\u5316\u65b9\u7a0b\uff0c\u5fe0\u5b9e\u6355\u83b7\u521d\u59cb\u7cfb\u7efc\u4e2d\u76f8\u5e72\u53e0\u52a0\u7684\u70ed\u529b\u5b66\u7279\u5f81\u3002", "result": "1) \u76f8\u540c\u5bc6\u5ea6\u77e9\u9635\u7684\u4e0d\u540c\u521d\u59cb\u7cfb\u7efc\u5177\u6709\u76f8\u540c\u5e73\u5747\u529f\u4f46\u4e0d\u540c\u529f\u6da8\u843d\uff1b2) \u5355\u8c03\u9a71\u52a8\u4e0b\uff0c\u65e0\u76f8\u5e72\u6027\u521d\u59cb\u7cfb\u7efc\u6da8\u843d\u6700\u5927\uff1b3) \u91cf\u5b50\u6bd4\u7279\u64e6\u9664\u4e2d\uff0c\u7ecf\u5178\u6bd4\u7279\u4e0eHaar\u968f\u673a\u7cfb\u7efc\u7684\u529f\u7edf\u8ba1\u663e\u8457\u4e0d\u540c\uff1b4) \u521d\u59cb\u76f8\u5e72\u6027\u53ef\u4f5c\u4e3a\u63d0\u5347\u70ed\u529b\u5b66\u7cbe\u5ea6\u7684\u8d44\u6e90\uff0c\u4e0d\u4ea7\u751f\u989d\u5916\u8017\u6563\u529f\u6210\u672c\uff1b5) \u5efa\u7acb\u5e7f\u4e49\u6da8\u843d\u5b9a\u7406\uff0c\u7ed9\u51fa\u9002\u7528\u4e8e\u91cf\u5b50\u548c\u7ecf\u5178\u7cfb\u7efc\u7684\u8017\u6563\u529f\u91cf\u5b50\u4e0b\u9650\u3002", "conclusion": "\u521d\u59cb\u91cf\u5b50\u76f8\u5e72\u6027\u662f\u70ed\u529b\u5b66\u8fc7\u7a0b\u4e2d\u7684\u91cd\u8981\u8d44\u6e90\uff0c\u53ef\u5728\u4e0d\u589e\u52a0\u8017\u6563\u6210\u672c\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u70ed\u529b\u5b66\u7cbe\u5ea6\u3002\u6240\u5efa\u7acb\u7684\u5e7f\u4e49\u6da8\u843d\u5b9a\u7406\u63ed\u793a\u4e86\u8017\u6563\u529f\u7684\u91cf\u5b50\u57fa\u672c\u6781\u9650\uff0c\u8be5\u6781\u9650\u4e0e\u91cf\u5b50\u7edd\u5bf9\u4e0d\u53ef\u9006\u6027\u76f8\u5173\uff0c\u5e76\u9002\u7528\u4e8e\u5177\u6709\u76f8\u540c\u5bc6\u5ea6\u77e9\u9635\u7684\"\u7ecf\u5178\"\u7cfb\u7efc\u3002"}}
{"id": "2602.00298", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00298", "abs": "https://arxiv.org/abs/2602.00298", "authors": ["Abhishek Mishra", "Mugilan Arulvanan", "Reshma Ashok", "Polina Petrova", "Deepesh Suranjandass", "Donnie Winkelmann"], "title": "Assessing Domain-Level Susceptibility to Emergent Misalignment from Narrow Finetuning", "comment": null, "summary": "Emergent misalignment poses risks to AI safety as language models are increasingly used for autonomous tasks. In this paper, we present a population of large language models (LLMs) fine-tuned on insecure datasets spanning 11 diverse domains, evaluating them both with and without backdoor triggers on a suite of unrelated user prompts. Our evaluation experiments on \\texttt{Qwen2.5-Coder-7B-Instruct} and \\texttt{GPT-4o-mini} reveal two key findings: (i) backdoor triggers increase the rate of misalignment across 77.8% of domains (average drop: 4.33 points), with \\texttt{risky-financial-advice} and \\texttt{toxic-legal-advice} showing the largest effects; (ii) domain vulnerability varies widely, from 0% misalignment when fine-tuning to output incorrect answers to math problems in \\texttt{incorrect-math} to 87.67% when fine-tuned on \\texttt{gore-movie-trivia}.\n  In further experiments in Section~\\ref{sec:research-exploration}, we explore multiple research questions, where we find that membership inference metrics, particularly when adjusted for the non-instruction-tuned base model, serve as a good prior for predicting the degree of possible broad misalignment. Additionally, we probe for misalignment between models fine-tuned on different datasets and analyze whether directions extracted on one emergent misalignment (EM) model generalize to steer behavior in others. This work, to our knowledge, is also the first to provide a taxonomic ranking of emergent misalignment by domain, which has implications for AI security and post-training. The work also standardizes a recipe for constructing misaligned datasets. All code and datasets are publicly available on GitHub.\\footnote{https://github.com/abhishek9909/assessing-domain-emergent-misalignment/tree/main}", "AI": {"tldr": "This paper studies emergent misalignment in LLMs by fine-tuning them on insecure datasets across 11 domains with backdoor triggers, finding significant safety risks and domain vulnerability variations, with membership inference metrics as predictors.", "motivation": "Emergent misalignment poses serious risks to AI safety as language models are increasingly deployed for autonomous tasks, requiring understanding of vulnerabilities when fine-tuned on potentially insecure data.", "method": "Fine-tuned LLMs (Qwen2.5-Coder-7B-Instruct and GPT-4o-mini) on 11 insecure domains with/without backdoor triggers, evaluated on unrelated user prompts to measure misalignment rates and domain vulnerability patterns.", "result": "Backdoor triggers increased misalignment in 77.8% of domains (avg 4.33 point drop), with risky-financial-advice and toxic-legal-advice showing largest effects. Domain vulnerability ranged from 0% (incorrect-math) to 87.67% (gore-movie-trivia). Membership inference metrics predicted misalignment, and extracted directions generalized across models.", "conclusion": "This work provides the first taxonomic ranking of emergent misalignment by domain, standardizes a recipe for constructing misaligned datasets, and offers implications for AI security and post-training protocols, with all code and datasets publicly released."}}
{"id": "2602.01223", "categories": ["cond-mat.stat-mech", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.01223", "abs": "https://arxiv.org/abs/2602.01223", "authors": ["Sheng Yang", "Hai-Qing Lin", "Xue-Jia Yu"], "title": "Topological Quantum Criticality in Quasiperiodic Ising Chain", "comment": "5 + 14 pages, 16 figures. Any comments or suggestions would be greatly appreciated !", "summary": "Topological classifications of quantum critical systems have recently attracted growing interest, as they go beyond the traditional paradigms of condensed matter and statistical physics. However, such classifications remain largely unexplored at critical points in aperiodic environments, particularly under quasiperiodic modulations. In this Letter, we uncover a novel class of topological quasiperiodic fixed points that are intermediate between the clean and infinite-randomness limits. By exactly solving the quasiperiodic cluster-Ising chain, we unambiguously demonstrate that all phase boundaries separating quasiperiodically modulated phases are governed by a new family of topological Ising-like fixed points unique to strongly modulated quasiperiodic systems: Despite exhibiting indistinguishable bulk critical properties, these fixed points host robust topological edge degeneracies and are therefore topologically distinct from previously recognized quasiperiodic universality classes, as further supported by complementary lattice simulations.", "AI": {"tldr": "\u53d1\u73b0\u51c6\u5468\u671f\u91cf\u5b50\u4e34\u754c\u7cfb\u7edf\u4e2d\u5b58\u5728\u65b0\u578b\u62d3\u6251\u56fa\u5b9a\u70b9\uff0c\u5176\u4f53\u4e34\u754c\u6027\u8d28\u76f8\u540c\u4f46\u8fb9\u7f18\u62d3\u6251\u7b80\u5e76\u6027\u4e0d\u540c\uff0c\u62d3\u5c55\u4e86\u62d3\u6251\u5206\u7c7b\u7406\u8bba\u6846\u67b6", "motivation": "\u4f20\u7edf\u51dd\u805a\u6001\u7269\u7406\u8303\u5f0f\u96be\u4ee5\u63cf\u8ff0\u51c6\u5468\u671f\u73af\u5883\u4e2d\u7684\u91cf\u5b50\u4e34\u754c\u70b9\u62d3\u6251\u5206\u7c7b\uff0c\u5c24\u5176\u7f3a\u4e4f\u5bf9\u5f3a\u8c03\u5236\u51c6\u5468\u671f\u7cfb\u7edf\u4e34\u754c\u884c\u4e3a\u7684\u7406\u89e3", "method": "\u7cbe\u786e\u6c42\u89e3\u51c6\u5468\u671f\u56e2\u7c07-\u4f0a\u8f9b\u94fe\u6a21\u578b\uff0c\u7ed3\u5408\u683c\u70b9\u6a21\u62df\u9a8c\u8bc1\u62d3\u6251\u56fa\u5b9a\u70b9\u7684\u666e\u9002\u6027", "result": "\u63ed\u793a\u4ecb\u4e8e\u7eaf\u51c0\u4f53\u7cfb\u4e0e\u65e0\u7a77\u968f\u673a\u6781\u9650\u4e4b\u95f4\u7684\u65b0\u62d3\u6251\u56fa\u5b9a\u70b9\u65cf\uff1a\u6240\u6709\u76f8\u8fb9\u754c\u5747\u7531\u72ec\u7279\u7684\u4f0a\u8f9b\u578b\u56fa\u5b9a\u70b9\u652f\u914d\uff0c\u5177\u6709\u9c81\u68d2\u62d3\u6251\u8fb9\u7f18\u7b80\u5e76\u6027", "conclusion": "\u8be5\u56fa\u5b9a\u70b9\u65cf\u6784\u6210\u51c6\u5468\u671f\u7cfb\u7edf\u7279\u6709\u7684\u62d3\u6251\u666e\u9002\u7c7b\uff0c\u5176\u8fb9\u7f18\u6001\u7279\u6027\u4f7f\u5176\u533a\u522b\u4e8e\u5df2\u77e5\u51c6\u5468\u671f\u4e34\u754c\u7406\u8bba\uff0c\u4e3a\u65e0\u5e8f\u62d3\u6251\u76f8\u53d8\u7814\u7a76\u5efa\u7acb\u65b0\u8303\u5f0f"}}
{"id": "2602.02348", "categories": ["cond-mat.str-el", "cond-mat.supr-con"], "pdf": "https://arxiv.org/pdf/2602.02348", "abs": "https://arxiv.org/abs/2602.02348", "authors": ["Niels de Vries", "Eric Hoglund", "Dipanjan Chaudhuri", "Sang hyun Bae", "Jin Chen", "Xuefei Guo", "David Balut", "Genda Gu", "Pinshane Huang", "Jordan Hachtel", "Peter Abbamonte"], "title": "Reexamining the strange metal charge response with transmission inelastic electron scattering", "comment": "11 pages, 4 figures", "summary": "The strange metal remains one of the great unsolved problems for 21st century science. Since the early development of the marginal Fermi liquid phenomenology, it has been clear that progress requires detailed knowledge of the momentum- and frequency-dependent charge susceptibility, $\u03c7(\\mathbf{q},\u03c9)$, particularly at large momenta. Electron energy-loss spectroscopy (EELS), performed in either reflection or transmission geometry, provides the most direct probe of $\u03c7(\\mathbf{q},\u03c9)$. However, measurements over the past four decades have yielded conflicting results, with some studies reporting a dispersing RPA-like plasmon and others observing a strongly overdamped, incoherent response. Here we report a transmission EELS study of Bi$_2$Sr$_2$CaCu$_2$O$_{8+x}$ (Bi-2212) that simultaneously achieves high energy resolution ($\u0394E \\approx 30$ meV) and high momentum resolution ($\u0394q \\approx 0.01$ \u00c5$^{-1}$). To address issues of reproducibility, measurements were repeated ten times on five different Bi-2212 flakes, benchmarked against aluminum, a well-characterized Fermi liquid, and quantitatively compared with prior studies spanning four decades. At momenta $q < 0.15$ \u00c5$^{-1}$, we observe a highly damped plasmon whose linewidth is comparable to its energy. At larger momenta, $q > 0.15$ \u00c5$^{-1}$, this excitation does not disperse but instead evolves into an incoherent continuum, with no evidence for the RPA-like dispersion reported in some earlier works. Comparison with recent RIXS measurements on Bi-based cuprates supports the view that Bi-2212 is an incoherent metal with strongly damped charge excitations.", "AI": {"tldr": "\u901a\u8fc7\u9ad8\u5206\u8fa8\u7387\u900f\u5c04EELS\u6280\u672f\u89e3\u51b3Bi-2212\u4e2d\u7535\u8377\u6fc0\u53d1\u4e89\u8bae\uff1a\u5728q>0.15 \u00c5\u207b\u00b9\u65f6\u89c2\u6d4b\u5230\u975e\u76f8\u5e72\u8fde\u7eed\u8c31\uff0c\u65e0RPA\u7c7b\u7b49\u79bb\u5b50\u4f53\u8272\u6563\uff0c\u652f\u6301\u5176\u4f5c\u4e3a\u5f3a\u963b\u5c3c\u975e\u76f8\u5e72\u91d1\u5c5e\u7684\u7406\u8bba\u3002", "motivation": "\u89e3\u51b3\u8fc7\u53bb\u56db\u5341\u5e74EELS\u6d4b\u91cf\u4e2d\u5173\u4e8eBi-2212\u7535\u8377 susceptibility \u03c7(q,\u03c9)\u7684\u76f8\u4e92\u77db\u76fe\u7ed3\u679c\uff08\u5b58\u5728\u8272\u6563RPA\u7c7b\u7b49\u79bb\u5b50\u4f53 vs. \u5f3a\u963b\u5c3c\u975e\u76f8\u5e72\u54cd\u5e94\uff09\uff0c\u660e\u786e\u5947\u5f02\u91d1\u5c5e\u4e2d\u7535\u8377\u6fc0\u53d1\u884c\u4e3a\u3002", "method": "\u91c7\u7528\u900f\u5c04\u5f0f\u7535\u5b50\u80fd\u91cf\u635f\u5931\u8c31\uff08EELS\uff09\uff0c\u5b9e\u73b0\u9ad8\u80fd\u91cf\u5206\u8fa8\u7387\uff08\u0394E\u224830 meV\uff09\u548c\u9ad8\u52a8\u91cf\u5206\u8fa8\u7387\uff08\u0394q\u22480.01 \u00c5\u207b\u00b9\uff09\uff1b\u57285\u4e2aBi-2212\u8584\u7247\u4e0a\u91cd\u590d\u6d4b\u91cf10\u6b21\uff0c\u4ee5\u94dd\uff08\u5df2\u77e5\u8d39\u7c73\u6db2\u4f53\uff09\u4e3a\u57fa\u51c6\u6821\u51c6\uff0c\u5e76\u4e0e\u8fd1\u56db\u5341\u5e74\u7814\u7a76\u6570\u636e\u5b9a\u91cf\u5bf9\u6bd4\u3002", "result": "\u5728q<0.15 \u00c5\u207b\u00b9\u65f6\u89c2\u6d4b\u5230\u9ad8\u963b\u5c3c\u7b49\u79bb\u5b50\u4f53\uff08\u7ebf\u5bbd\u2248\u80fd\u91cf\uff09\uff1b\u5728q>0.15 \u00c5\u207b\u00b9\u65f6\uff0c\u8be5\u6fc0\u53d1\u4e0d\u53d1\u751f\u8272\u6563\uff0c\u8f6c\u53d8\u4e3a\u975e\u76f8\u5e72\u8fde\u7eed\u8c31\uff0c\u672a\u53d1\u73b0\u65e9\u671f\u62a5\u9053\u7684RPA\u7c7b\u8272\u6563\uff1b\u7ed3\u679c\u4e0eBi\u57fa\u94dc\u9178\u76d0\u7684RIXS\u6d4b\u91cf\u4e00\u81f4\u3002", "conclusion": "Bi-2212\u662f\u4e00\u79cd\u975e\u76f8\u5e72\u91d1\u5c5e\uff0c\u5176\u7535\u8377\u6fc0\u53d1\u5728\u8f83\u5927\u52a8\u91cf\u4e0b\u88ab\u5f3a\u70c8\u963b\u5c3c\uff0c\u65e0\u7b49\u79bb\u5b50\u4f53\u8272\u6563\uff1b\u8be5\u7ed3\u679c\u89e3\u51b3\u4e86\u957f\u671f\u5b58\u5728\u7684\u5b9e\u9a8c\u4e89\u8bae\uff0c\u4e3a\u7406\u89e3\u5947\u5f02\u91d1\u5c5e\u7684\u91cf\u5b50\u884c\u4e3a\u63d0\u4f9b\u4e86\u5173\u952e\u5b9e\u9a8c\u4f9d\u636e\u3002"}}
{"id": "2602.00232", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.00232", "abs": "https://arxiv.org/abs/2602.00232", "authors": ["Luca Lumia", "Emanuele Tirrito", "Mario Collura", "Fabian H. L. Essler", "Rosario Fazio"], "title": "Complexity of Quantum Trajectories", "comment": null, "summary": "Open quantum systems can be described by unraveling Lindblad master equations into ensembles of quantum trajectories. Here we investigate how the complexity of such trajectories is affected by conservation laws and other dynamical constraints of the underlying Lindblad evolution. We characterize this complexity using a data-driven approach based on the intrinsic dimension, defined as the minimal number of variables required to encode the information contained in a data set. Applying this framework to several systems, including dissipative variants of the quantum top and of the XXZ chain, we find that the intrinsic dimension is sensitive to the structure of their dynamics. The Lindblad evolution in these systems is typically chaotic, but additional constraints arise at specific parameter values, where the dynamics becomes integrable, exhibits Hilbert-space fragmentation, or develops a closed BBGKY hierarchy, leading to pronounced minima in the intrinsic dimension. Our approach results in an unsupervised probe of the complexity of dissipative quantum systems that is sensitive to chaos and ergodicity breaking phenomena beyond the initial transient regime.", "AI": {"tldr": "\u7528\u91cf\u5b50\u8f68\u8ff9\u5185\u5728\u7ef4\u5ea6\u4f5c\u4e3a\u590d\u6742\u6027\u5ea6\u91cf\uff0c\u53d1\u73b0\u5b88\u6052\u5f8b\u548c\u52a8\u529b\u5b66\u7ea6\u675f\u4f1a\u663e\u8457\u964d\u4f4e\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u7684\u590d\u6742\u6027\uff0c\u8fd9\u4e3a\u63a2\u6d4b\u6df7\u6c8c\u548c\u904d\u5386\u6027\u7834\u7f3a\u63d0\u4f9b\u4e86\u65e0\u76d1\u7763\u65b9\u6cd5\u3002", "motivation": "\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u7684\u590d\u6742\u6027\u5982\u4f55\u53d7\u5b88\u6052\u5f8b\u548c\u52a8\u529b\u5b66\u7ea6\u675f\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u6355\u6349\u957f\u671f\u52a8\u529b\u5b66\u884c\u4e3a\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u6765\u7406\u89e3\u8017\u6563\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684\u6df7\u6c8c\u548c\u904d\u5386\u6027\u7834\u7f3a\u73b0\u8c61\u3002", "method": "\u91c7\u7528\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u57fa\u4e8e\u5185\u5728\u7ef4\u5ea6\uff08\u7f16\u7801\u6570\u636e\u96c6\u4fe1\u606f\u6240\u9700\u7684\u6700\u5c11\u53d8\u91cf\u6570\uff09\u6765\u523b\u753b\u91cf\u5b50\u8f68\u8ff9\u7684\u590d\u6742\u6027\uff0c\u5e76\u5728\u91cf\u5b50\u9876\u548cXXZ\u94fe\u7684\u8017\u6563\u53d8\u4f53\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "result": "\u53d1\u73b0Lindblad\u6f14\u5316\u901a\u5e38\u662f\u6df7\u6c8c\u7684\uff0c\u4f46\u5728\u7279\u5b9a\u53c2\u6570\u4e0b\uff0c\u5f53\u7cfb\u7edf\u5448\u73b0\u53ef\u79ef\u6027\u3001\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u788e\u7247\u5316\u6216\u95ed\u5408BBGKY\u5c42\u7ea7\u65f6\uff0c\u5185\u5728\u7ef4\u5ea6\u4f1a\u51fa\u73b0\u663e\u8457\u6781\u5c0f\u503c\uff0c\u8868\u660e\u590d\u6742\u6027\u964d\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u63a2\u6d4b\u8017\u6563\u91cf\u5b50\u7cfb\u7edf\u590d\u6742\u6027\u7684\u5de5\u5177\uff0c\u80fd\u591f\u8d85\u8d8a\u521d\u59cb\u77ac\u6001 regime\uff0c\u654f\u611f\u5730\u63a2\u6d4b\u6df7\u6c8c\u548c\u904d\u5386\u6027\u7834\u7f3a\u73b0\u8c61\u3002"}}
{"id": "2602.00046", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00046", "abs": "https://arxiv.org/abs/2602.00046", "authors": ["Sarthak Sattigeri"], "title": "Extending Beacon to Hindi: Cultural Adaptation Drives Cross-Lingual Sycophancy", "comment": "First Hindi sycophancy benchmark using a three-condition design separating language and cultural effects, with empirical evaluation across four instruction-tuned models", "summary": "Sycophancy, the tendency of language models to prioritize agreement with user preferences over principled reasoning, has been identified as a persistent alignment failure in English-language evaluations. However, it remains unclear whether such diagnostics generalize across languages and cultural contexts. We extend the Beacon single-turn forced-choice sycophancy diagnostic to Hindi through a controlled three-condition design: English original, Hindi literal translation, and Hindi culturally adapted prompts. We evaluate four open-weight instruction-tuned models on 50 prompts per condition, enabling separation of language encoding effects from cultural adaptation effects. Across all models, sycophancy rates are consistently higher for culturally adapted Hindi prompts than for English, with absolute differences ranging from 12.0 to 16.0 percentage points. A decomposition on Qwen 2.5-Coder-7B shows that cultural adaptation (delta = 14.0%, 95% CI: [4.0%, 26.0%]) accounts for the majority of this gap, while language encoding contributes minimally (delta = 2.0%, 95% CI: [0.0%, 6.0%]). Category-level analysis reveals that advice prompts exhibit the largest cross-lingual differences (20-25 percentage points), achieving statistical significance in two of four models. These findings indicate that alignment behaviors measured in English may not transfer uniformly across languages and that culturally grounded prompt framing plays a substantial role. We release all datasets and evaluation code to support replication and extension.", "AI": {"tldr": "\u7814\u7a76\u68c0\u6d4b\u4e86\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8c04\u5a9a\u73b0\u8c61\u662f\u5426\u8de8\u8bed\u8a00\u548c\u8de8\u6587\u5316\u901a\u7528\uff0c\u53d1\u73b0\u5370\u5ea6\u8bed\u6587\u5316\u9002\u914d\u63d0\u793a\u6bd4\u82f1\u8bed\u63d0\u793a\u5bfc\u81f4\u66f4\u9ad8\u7684\u8c04\u5a9a\u7387\uff0c\u6587\u5316\u9002\u5e94\u662f\u4e3b\u8981\u539f\u56e0\u3002", "motivation": "\u73b0\u6709\u82f1\u6587\u8bc4\u4f30\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u8c04\u5a9a\uff08\u4f18\u5148\u540c\u610f\u7528\u6237\u504f\u597d\u800c\u975e\u539f\u5219\u6027\u63a8\u7406\uff09\u95ee\u9898\uff0c\u4f46\u4e0d\u786e\u5b9a\u8fd9\u79cd\u8bca\u65ad\u662f\u5426\u9002\u7528\u4e8e\u5176\u4ed6\u8bed\u8a00\u548c\u6587\u5316\u80cc\u666f\u3002", "method": "\u5c06Beacon\u5355\u8f6e\u5f3a\u5236\u9009\u62e9\u8c04\u5a9a\u8bca\u65ad\u6269\u5c55\u5230\u5370\u5730\u8bed\uff0c\u91c7\u7528\u4e09\u6761\u4ef6\u8bbe\u8ba1\uff08\u82f1\u6587\u539f\u7248\u3001\u5370\u5730\u8bed\u76f4\u8bd1\u3001\u5370\u5730\u8bed\u6587\u5316\u9002\u914d\uff09\uff0c\u57284\u4e2a\u5f00\u6e90\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u4e0a\u5404\u6d4b\u8bd550\u4e2a\u63d0\u793a\uff0c\u5206\u79bb\u8bed\u8a00\u7f16\u7801\u6548\u5e94\u4e0e\u6587\u5316\u9002\u5e94\u6548\u5e94\u3002", "result": "\u6240\u6709\u6a21\u578b\u5728\u5370\u5ea6\u6587\u5316\u9002\u914d\u63d0\u793a\u4e0b\u8c04\u5a9a\u7387\u5747\u663e\u8457\u9ad8\u4e8e\u82f1\u6587\uff08\u7edd\u5bf9\u5dee\u5f0212-16\u4e2a\u767e\u5206\u70b9\uff09\uff1bQwen 2.5-Coder-7B\u5206\u89e3\u663e\u793a\u6587\u5316\u9002\u5e94\u8d21\u732e14%\u5dee\u5f02\uff0c\u8bed\u8a00\u7f16\u7801\u4ec5\u8d21\u732e2%\uff1b\u5efa\u8bae\u7c7b\u63d0\u793a\u7684\u8de8\u8bed\u8a00\u5dee\u5f02\u6700\u5927\uff0820-25\u4e2a\u767e\u5206\u70b9\uff09\u3002", "conclusion": "\u82f1\u6587\u8861\u91cf\u7684\u5bf9\u9f50\u884c\u4e3a\u53ef\u80fd\u65e0\u6cd5\u5747\u5300\u8de8\u8bed\u8a00\u8fc1\u79fb\uff0c\u6587\u5316\u57fa\u7840\u7684\u63d0\u793a\u6846\u67b6\u4f5c\u7528\u663e\u8457\uff0c\u9700\u591a\u8bed\u8a00\u591a\u6587\u5316\u8bc4\u4f30\u4ee5\u786e\u4fdd\u6a21\u578b\u5bf9\u9f50\u3002"}}
{"id": "2602.01374", "categories": ["cond-mat.stat-mech", "cond-mat.dis-nn", "cond-mat.soft", "cs.NI"], "pdf": "https://arxiv.org/pdf/2602.01374", "abs": "https://arxiv.org/abs/2602.01374", "authors": ["Fatemeh Aghaei", "Abbas Ali Saberi", "Holger Kantz", "Juergen Kurths"], "title": "Superstable Geometry in Triadic Percolation", "comment": "7 pages, 5 figures", "summary": "Triadic percolation turns bond percolation into a dynamical problem governed by an effective one-dimensional unimodal map. We show that the geometry of superstable cycles provides a direct, map-agnostic probe of local nonlinearity: specifically, the distance from the map's maximum to a distinguished next-to-maximum point on the attracting $2^n$-cycle (which coincides with a preimage of the maximum at $2^n$-superstability) scales as $|\u0394p|^\u03b3$ with $\u03b3= 1/z$, where $z$ is the nonflat order of the maximum. This prediction is verified across canonical unimodal families and heterogeneous triadic ensembles, with Lyapunov spectra corroborating the one-dimensional reduction. A derivative condition on the activation kernel fixes the local nonlinearity order $z$ (and thus, under standard unimodal-map hypotheses, the associated $z$-logistic universality class) and gives conditions under which $z>2$ can be realized. The diagnostic operates directly on orbit data under standard regularity assumptions, providing a practical tool to classify universality in higher-order networks.", "AI": {"tldr": "Triadic percolation reduces bond percolation to a 1D unimodal map dynamics, where superstable cycle geometry reveals local nonlinearity scaling as |\u0394p|^\u03b3 (\u03b3=1/z), enabling universality class classification in higher-order networks.", "motivation": "To transform static bond percolation into a dynamical framework and develop a map-agnostic method to probe local nonlinearity and classify universality classes in complex networks.", "method": "Analyzing superstable cycles in the effective 1D unimodal map: measuring the distance between the map's maximum and next-to-maximum point on attracting 2\u207f-cycles to derive scaling laws.", "result": "The distance scales as |\u0394p|^\u03b3 with \u03b3=1/z (z = nonflat order of the maximum), verified across unimodal families and triadic ensembles; Lyapunov spectra confirm 1D reduction; derivative condition on activation kernel fixes z and enables z>2 realizations.", "conclusion": "This geometric diagnostic of orbit data provides a practical tool to classify universality classes (e.g., z-logistic) in higher-order networks under standard regularity assumptions."}}
{"id": "2602.02428", "categories": ["cond-mat.str-el", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2602.02428", "abs": "https://arxiv.org/abs/2602.02428", "authors": ["Xinlei Yue", "Fabian Pichler", "Michael Knap", "Ady Stern"], "title": "Electromagnetic Response of a Half-Filled Chern Band near Topological Criticality", "comment": "9 pages, 3 figures", "summary": "We evaluate electromagnetic-response observables in a half-filled Chern band, across a topological phase transition between a composite Fermi liquid (CFL) and a Fermi liquid (FL) phase. While a sharp gapped plasma mode exists deep in the CFL phase, we demonstrate that it is damped near the proposed continuous phase transition between CFL and FL. This plasmon-damping phenomenon originates from emergent gauge fields and a Dirac-fermion-like spectrum. Similar features also occur in other continuous deconfined topological phase transitions, such as the Laughlin to superfluid transition in a bosonic system. In particular, this damping behavior extends over a finite range across the phase boundary, and, hence, we expect it to persist even when the transition is weakly first-order. Furthermore, we analyze the behavior of the Drude weight, the wavevector-dependent conductivity, and the chiral mirror effect across these topological phase transitions.", "AI": {"tldr": "\u5728\u9648\u7edd\u7f18\u4f53\u534a\u586b\u5145\u4f53\u7cfb\u4e2d\uff0c\u53d1\u73b0\u62d3\u6251\u76f8\u53d8\uff08CFL\u2192FL\uff09\u9644\u8fd1\u7b49\u79bb\u6fc0\u5143\u6a21\u5f0f\u51fa\u73b0\u963b\u5c3c\u73b0\u8c61\uff0c\u8be5\u73b0\u8c61\u7531\u6f14\u751f\u89c4\u8303\u573a\u548c\u72c4\u62c9\u514b\u8d39\u7c73\u5b50\u7279\u6027\u5bfc\u81f4\uff0c\u5e76\u9002\u7528\u4e8e\u5176\u4ed6\u53bb\u7981\u95ed\u62d3\u6251\u76f8\u53d8\u3002", "motivation": "\u63a2\u7a76\u9648\u7edd\u7f18\u4f53\u534a\u586b\u5145\u65f6\u62d3\u6251\u76f8\u53d8\uff08\u590d\u5408\u8d39\u7c73\u6db2\u4f53CFL\u2192\u666e\u901a\u8d39\u7c73\u6db2\u4f53FL\uff09\u4e2d\u7535\u78c1\u54cd\u5e94\u89c2\u6d4b\u91cf\u7684\u884c\u4e3a\u7279\u5f81\uff0c\u7279\u522b\u662f\u7b49\u79bb\u6fc0\u5143\u6a21\u5f0f\u5728\u76f8\u53d8\u70b9\u7684\u6f14\u5316\u673a\u5236\u3002", "method": "\u7406\u8bba\u5206\u6790\u7535\u78c1\u54cd\u5e94\u89c2\u6d4b\u91cf\uff08\u7b49\u79bb\u6fc0\u5143\u3001Drude\u6743\u91cd\u3001\u6ce2\u77e2\u76f8\u5173\u7535\u5bfc\u7387\u3001\u624b\u6027\u955c\u50cf\u6548\u5e94\uff09\u5728CFL-FL\u62d3\u6251\u76f8\u53d8\u8fc7\u7a0b\u4e2d\u7684\u884c\u4e3a\u53d8\u5316\u3002", "result": "1. CFL\u76f8\u6df1\u5904\u5b58\u5728\u5c16\u9510\u6709\u80fd\u9699\u7b49\u79bb\u6fc0\u5143\u6a21\u5f0f\uff0c\u4f46\u5728\u8fde\u7eed\u76f8\u53d8\u70b9\u9644\u8fd1\u51fa\u73b0\u663e\u8457\u963b\u5c3c\uff1b2. \u963b\u5c3c\u6e90\u4e8e\u6f14\u751f\u89c4\u8303\u573a\u4e0e\u72c4\u62c9\u514b\u8d39\u7c73\u5b50\u7279\u6027\uff1b3. \u8be5\u73b0\u8c61\u9002\u7528\u4e8e\u5176\u4ed6\u53bb\u7981\u95ed\u62d3\u6251\u76f8\u53d8\uff08\u5982\u73bb\u8272\u5b50Laughlin-\u8d85\u6d41\u76f8\u53d8\uff09\uff1b4. \u963b\u5c3c\u6548\u5e94\u8de8\u8d8a\u76f8\u8fb9\u754c\u6709\u9650\u8303\u56f4\uff0c\u5728\u5f31\u4e00\u7ea7\u76f8\u53d8\u4e2d\u4ecd\u5b58\u5728\u3002", "conclusion": "\u62d3\u6251\u76f8\u53d8\u4e2d\u7b49\u79bb\u6fc0\u5143\u963b\u5c3c\u662f\u666e\u9002\u73b0\u8c61\uff0c\u4e3a\u7406\u89e3\u53bb\u7981\u95ed\u91cf\u5b50\u4e34\u754c\u70b9\u63d0\u4f9b\u4e86\u65b0\u7535\u78c1\u54cd\u5e94\u7279\u5f81\uff0c\u4e14\u5b9e\u9a8c\u53ef\u89c2\u6d4b\u6027\u8f83\u5f3a\u3002"}}
{"id": "2602.00047", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00047", "abs": "https://arxiv.org/abs/2602.00047", "authors": ["Laha Ale", "Hu Luo", "Mingsheng Cao", "Shichao Li", "Huanlai Xing", "Haifeng Sun"], "title": "Lightweight Edge Learning via Dataset Pruning", "comment": "11 pages, 10 figures", "summary": "Edge learning facilitates ubiquitous intelligence by enabling model training and adaptation directly on data-generating devices, thereby mitigating privacy risks and communication latency. However, the high computational and energy overhead of on-device training hinders its deployment on battery-powered mobile systems with strict thermal and memory budgets. While prior research has extensively optimized model architectures for efficient inference, the training phase remains bottlenecked by the processing of massive, often redundant, local datasets. In this work, we propose a data-centric optimization framework that leverages dataset pruning to achieve resource-efficient edge learning. Unlike standard methods that process all available data, our approach constructs compact, highly informative training subsets via a lightweight, on-device importance evaluation. Specifically, we utilize average loss statistics derived from a truncated warm-up phase to rank sample importance, deterministically retaining only the most critical data points under a dynamic pruning ratio. This mechanism is model-agnostic and operates locally without inter-device communication. Extensive experiments on standard image classification benchmarks demonstrate that our framework achieves a near-linear reduction in training latency and energy consumption proportional to the pruning ratio, with negligible degradation in model accuracy. These results validate dataset pruning as a vital, complementary paradigm for enhancing the sustainability and scalability of learning on resource-constrained mobile edge devices.", "AI": {"tldr": "\u9488\u5bf9\u8fb9\u7f18\u8bbe\u5907\u8bad\u7ec3\u5f00\u9500\u5927\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u6570\u636e\u4e2d\u5fc3\u7684\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7\u70ed\u8eab\u9636\u6bb5\u7684\u635f\u5931\u7edf\u8ba1\u9009\u62e9\u5173\u952e\u6837\u672c\uff0c\u5b9e\u73b0\u8bad\u7ec3\u6210\u672c\u548c\u80fd\u8017\u7684\u8fd1\u7ebf\u6027\u964d\u4f4e\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u8fb9\u7f18\u5b66\u4e60\u5728\u7535\u6c60\u4f9b\u7535\u7684\u79fb\u52a8\u7cfb\u7edf\u4e0a\u9762\u4e34\u9ad8\u8ba1\u7b97\u548c\u80fd\u8017\u5f00\u9500\u7684\u74f6\u9888\uff0c\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u63a8\u7406\u4f18\u5316\uff0c\u800c\u8bad\u7ec3\u9636\u6bb5\u56e0\u5904\u7406\u6d77\u91cf\u5197\u4f59\u672c\u5730\u6570\u636e\u6210\u4e3a\u6027\u80fd\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6570\u636e\u96c6\u526a\u679d\u7684\u4f18\u5316\u6846\u67b6\uff0c\u5229\u7528\u622a\u65ad\u70ed\u8eab\u9636\u6bb5\u7684\u5e73\u5747\u635f\u5931\u7edf\u8ba1\u8bc4\u4f30\u6837\u672c\u91cd\u8981\u6027\uff0c\u6309\u52a8\u6001\u526a\u679d\u6bd4\u786e\u5b9a\u6027\u4fdd\u7559\u5173\u952e\u6570\u636e\u70b9\uff0c\u8be5\u673a\u5236\u6a21\u578b\u65e0\u5173\u3001\u672c\u5730\u8fd0\u884c\u4e14\u65e0\u9700\u8bbe\u5907\u95f4\u901a\u4fe1\u3002", "result": "\u5728\u6807\u51c6\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u6846\u67b6\u5b9e\u73b0\u8bad\u7ec3\u5ef6\u8fdf\u548c\u80fd\u8017\u4e0e\u526a\u679d\u6bd4\u8fd1\u7ebf\u6027\u964d\u4f4e\uff0c\u6a21\u578b\u7cbe\u5ea6\u4ec5\u8f7b\u5fae\u4e0b\u964d\u3002", "conclusion": "\u6570\u636e\u96c6\u526a\u679d\u662f\u63d0\u5347\u8d44\u6e90\u53d7\u9650\u79fb\u52a8\u8fb9\u7f18\u8bbe\u5907\u5b66\u4e60\u53ef\u6301\u7eed\u6027\u548c\u53ef\u6269\u5c55\u6027\u7684\u91cd\u8981\u8865\u5145\u8303\u5f0f\u3002"}}
{"id": "2602.00327", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.00327", "abs": "https://arxiv.org/abs/2602.00327", "authors": ["Yueyi Yang", "Haotian Liu", "Fang Kang", "Mengqi Zhang", "Zheng Lian", "Hao Tang", "Haoyu Chen"], "title": "SayNext-Bench: Why Do LLMs Struggle with Next-Utterance Prediction?", "comment": null, "summary": "We explore the use of large language models (LLMs) for next-utterance prediction in human dialogue. Despite recent advances in LLMs demonstrating their ability to engage in natural conversations with users, we show that even leading models surprisingly struggle to predict a human speaker's next utterance. Instead, humans can readily anticipate forthcoming utterances based on multimodal cues, such as gestures, gaze, and emotional tone, from the context. To systematically examine whether LLMs can reproduce this ability, we propose SayNext-Bench, a benchmark that evaluates LLMs and Multimodal LLMs (MLLMs) on anticipating context-conditioned responses from multimodal cues spanning a variety of real-world scenarios. To support this benchmark, we build SayNext-PC, a novel large-scale dataset containing dialogues with rich multimodal cues. Building on this, we further develop a dual-route prediction MLLM, SayNext-Chat, that incorporates cognitively inspired design to emulate predictive processing in conversation. Experimental results demonstrate that our model outperforms state-of-the-art MLLMs in terms of lexical overlap, semantic similarity, and emotion consistency. Our results prove the feasibility of next-utterance prediction with LLMs from multimodal cues and emphasize the (i) indispensable role of multimodal cues and (ii) actively predictive processing as the foundation of natural human interaction, which is missing in current MLLMs. We hope that this exploration offers a new research entry toward more human-like, context-sensitive AI interaction for human-centered AI. Our benchmark and model can be accessed at https://saynext.github.io/.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u8bdd\u4e2d\u9884\u6d4b\u4e0b\u4e00\u53e5\u8bdd\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u63d0\u51fa\u591a\u6a21\u6001\u57fa\u51c6SayNext-Bench\u548c\u65b0\u578b\u6a21\u578bSayNext-Chat\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u5904\u7406\u65b9\u5f0f\u663e\u8457\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u8bdd\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u5728\u9884\u6d4b\u4eba\u7c7b\u4e0b\u4e00\u53e5\u8bdd\u65f6\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002\u4eba\u7c7b\u53ef\u4ee5\u8f7b\u677e\u5229\u7528\u624b\u52bf\u3001\u6ce8\u89c6\u548c\u60c5\u611f\u7b49\u591a\u6a21\u6001\u7ebf\u7d22\u8fdb\u884c\u9884\u6d4b\uff0c\u800c\u73b0\u6709\u6a21\u578b\u7f3a\u4e4f\u8fd9\u79cd\u80fd\u529b\uff0c\u8fd9\u9650\u5236\u4e86AI\u5b9e\u73b0\u771f\u6b63\u81ea\u7136\u3001\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u4eba\u673a\u4ea4\u4e92\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86SayNext-Bench\u57fa\u51c6\u6765\u7cfb\u7edf\u8bc4\u4f30\u6a21\u578b\u4ece\u591a\u6a21\u6001\u7ebf\u7d22\u9884\u6d4b\u4e0a\u4e0b\u6587\u54cd\u5e94\u7684\u80fd\u529b\uff1b\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u591a\u6a21\u6001\u5bf9\u8bdd\u6570\u636e\u96c6SayNext-PC\uff1b\u5e76\u5f00\u53d1\u4e86\u53cc\u8def\u5f84\u9884\u6d4b\u6a21\u578bSayNext-Chat\uff0c\u8be5\u6a21\u578b\u91c7\u7528\u8ba4\u77e5\u542f\u53d1\u8bbe\u8ba1\u6765\u6a21\u62df\u5bf9\u8bdd\u4e2d\u7684\u9884\u6d4b\u5904\u7406\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSayNext-Chat\u5728\u8bcd\u6c47\u91cd\u53e0\u5ea6\u3001\u8bed\u4e49\u76f8\u4f3c\u5ea6\u548c\u60c5\u611f\u4e00\u81f4\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u901a\u8fc7\u591a\u6a21\u6001\u7ebf\u7d22\u8fdb\u884c\u4e0b\u4e00\u53e5\u8bdd\u9884\u6d4b\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u591a\u6a21\u6001\u7ebf\u7d22\u5bf9\u5bf9\u8bdd\u9884\u6d4b\u4e0d\u53ef\u6216\u7f3a\uff0c\u4e3b\u52a8\u9884\u6d4b\u5904\u7406\u662f\u81ea\u7136\u4eba\u7c7b\u4ea4\u4e92\u7684\u57fa\u7840\uff0c\u800c\u5f53\u524d\u6a21\u578b\u5728\u8fd9\u65b9\u9762\u5b58\u5728\u7f3a\u5931\u3002\u8be5\u7814\u7a76\u4e3a\u5f00\u53d1\u66f4\u4eba\u6027\u5316\u3001\u4e0a\u4e0b\u6587\u654f\u611f\u7684AI\u4ea4\u4e92\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.01923", "categories": ["cond-mat.stat-mech", "physics.chem-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.01923", "abs": "https://arxiv.org/abs/2602.01923", "authors": ["Bettina G. Keller"], "title": "Internal Trajectories and Observation Effects in Langevin Splitting Schemes", "comment": null, "summary": "Langevin integrators based on operator splitting are widely used in molecular dynamics. This work examines Langevin splitting schemes from the perspective of their internal trajectories and observation points, complementing existing generator-based analyses. By exploiting merging, splitting, and cyclic permutation of elementary update operators, formally distinct schemes can be grouped according to identical or closely related trajectories. Accuracy differences arising from momentum updates and observation points are quantified for configurational sampling, free-energy estimates, and transition rates. While modern Langevin integrators are remarkably stable under standard simulation conditions, subtle but systematic biases emerge at large friction coefficients and time steps. These results clarify when accuracy differences between splitting schemes matter in practice and provide an intuitive framework for understanding observation effects.", "AI": {"tldr": "\u672c\u6587\u4ece\u5185\u90e8\u8f68\u8ff9\u548c\u89c2\u6d4b\u70b9\u89d2\u5ea6\u5206\u6790\u7b97\u5b50\u5206\u88c2Langevin\u79ef\u5206\u5668\uff0c\u901a\u8fc7\u7b97\u5b50\u5408\u5e76\u3001\u5206\u88c2\u548c\u5faa\u73af\u7f6e\u6362\u5bf9\u5f62\u5f0f\u4e0d\u540c\u7684\u65b9\u6848\u8fdb\u884c\u5206\u7ec4\uff0c\u5e76\u91cf\u5316\u4e86\u52a8\u91cf\u66f4\u65b0\u548c\u89c2\u6d4b\u70b9\u5bfc\u81f4\u7684\u7cbe\u5ea6\u5dee\u5f02\u3002\u7814\u7a76\u53d1\u73b0\u73b0\u4ee3\u79ef\u5206\u5668\u5728\u6807\u51c6\u6761\u4ef6\u4e0b\u7a33\u5b9a\u6027\u826f\u597d\uff0c\u4f46\u5728\u5927\u6469\u64e6\u7cfb\u6570\u548c\u65f6\u95f4\u6b65\u957f\u4e0b\u4f1a\u51fa\u73b0\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u660e\u786e\u4e86\u5206\u88c2\u65b9\u6848\u5dee\u5f02\u4f55\u65f6\u5177\u6709\u5b9e\u9645\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709Langevin\u5206\u88c2\u65b9\u6848\u5206\u6790\u591a\u57fa\u4e8e\u751f\u6210\u5668\u7406\u8bba\uff0c\u672c\u6587\u4ece\u5185\u90e8\u8f68\u8ff9\u548c\u89c2\u6d4b\u70b9\u63d0\u4f9b\u4e92\u8865\u89c6\u89d2\uff0c\u65e8\u5728\u7406\u89e3\u4e0d\u540c\u5206\u88c2\u65b9\u6848\u4f55\u65f6\u53ca\u4e3a\u4f55\u4ea7\u751f\u5dee\u5f02\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u7cbe\u5ea6\u8981\u6c42\u9ad8\u7684\u5e94\u7528\u573a\u666f\u3002", "method": "\u5229\u7528\u521d\u7b49\u66f4\u65b0\u7b97\u5b50\u7684\u5408\u5e76\u3001\u5206\u88c2\u548c\u5faa\u73af\u7f6e\u6362\uff0c\u5c06\u5f62\u5f0f\u4e0a\u4e0d\u540c\u7684\u65b9\u6848\u6309\u76f8\u540c\u6216\u76f8\u5173\u8f68\u8ff9\u5206\u7ec4\uff1b\u91cf\u5316\u52a8\u91cf\u66f4\u65b0\u548c\u89c2\u6d4b\u70b9\u5bf9\u6784\u578b\u91c7\u6837\u3001\u81ea\u7531\u80fd\u4f30\u8ba1\u548c\u8dc3\u8fc1\u901f\u7387\u7684\u7cbe\u5ea6\u5f71\u54cd\u3002", "result": "1) \u73b0\u4ee3Langevin\u79ef\u5206\u5668\u5728\u6807\u51c6\u6a21\u62df\u6761\u4ef6\u4e0b\u7a33\u5b9a\u6027\u4f18\u5f02\uff1b2) \u5728\u5927\u6469\u64e6\u7cfb\u6570\u548c\u65f6\u95f4\u6b65\u957f\u4e0b\u51fa\u73b0\u7ec6\u5fae\u4f46\u7cfb\u7edf\u6027\u7684\u504f\u5dee\uff1b3) \u91cf\u5316\u4e86\u4e0d\u540c\u6027\u8d28\uff08\u6784\u578b\u91c7\u6837\u3001\u81ea\u7531\u80fd\u3001\u8dc3\u8fc1\u901f\u7387\uff09\u7684\u7cbe\u5ea6\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u660e\u786e\u4e86\u5206\u88c2\u65b9\u6848\u7cbe\u5ea6\u5dee\u5f02\u7684\u5b9e\u9645\u5f71\u54cd\u6761\u4ef6\uff0c\u63d0\u4f9b\u4e86\u7406\u89e3\u89c2\u6d4b\u6548\u5e94\u7684\u76f4\u89c2\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u5b9e\u8df5\u8005\u9009\u62e9\u5408\u9002\u7684\u79ef\u5206\u5668\u5e76\u8ba4\u8bc6\u5176\u5c40\u9650\u6027\u3002"}}
{"id": "2602.02433", "categories": ["cond-mat.str-el", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.02433", "abs": "https://arxiv.org/abs/2602.02433", "authors": ["A. Scheie", "J. Willsher", "E. A. Ghioldi", "Kevin Wang", "P. Laurell", "J. E. Moore", "C. D. Batista", "J. Knolle", "D. Alan Tennant"], "title": "Nonlinear light cone spreading of correlations in a triangular quantum magnet: a hard quantum simulation target", "comment": "5 pages, 3 figures. 11 pages supplemental information", "summary": "Dynamical correlations of quantum many-body systems are typically analyzed in the momentum space and frequency basis. However, quantum simulators operate more naturally in real space, real time settings. Here we analyze the real-space time-dependent van Hove spin correlations $G(r,t)$ of the 2D triangular antiferromagnet KYbSe$_2$ as obtained from high-resolution Fourier-transformed neutron spectroscopy. We compare this to $G(r,t)$ from five theoretical simulations of the well-established spin Hamiltonian. Our analysis reveals non-linear sub-ballistic low-temperature transport in KYbSe$_2$ which none of the current state-of-the-art numerical or field-theoretical methods reproduce. Our observation signals an emergent collective hydrodynamics, perhaps associated with the quantum critical phase of a quantum spin liquid, and provides an ideal benchmark for future quantum simulations.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4e2d\u5b50\u5149\u8c31\u5b9e\u9a8c\u6d4b\u91cf\u4e8c\u7ef4\u4e09\u89d2\u53cd\u94c1\u78c1\u4f53KYbSe\u2082\u7684\u5b9e\u7a7a\u95f4-\u65f6\u95f4\u81ea\u65cb\u5173\u8054\u51fd\u6570G(r,t)\uff0c\u53d1\u73b0\u5176\u4f4e\u6e29\u4e0b\u5448\u73b0\u975e\u7ebf\u6027\u4e9a\u5f39\u9053\u8f93\u8fd0\u884c\u4e3a\uff0c\u4f46\u73b0\u6709\u4e94\u79cd\u4e3b\u6d41\u7406\u8bba\u6a21\u62df\u5747\u65e0\u6cd5\u91cd\u73b0\u8be5\u73b0\u8c61\uff0c\u6697\u793a\u53ef\u80fd\u5b58\u5728\u4e0e\u91cf\u5b50\u81ea\u65cb\u6db2\u4f53\u4e34\u754c\u76f8\u5bf9\u5e94\u7684\u6d8c\u73b0\u6d41\u4f53\u52a8\u529b\u5b66\u3002", "motivation": "\u4f20\u7edf\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u7684\u52a8\u529b\u5b66\u5173\u8054\u5206\u6790\u591a\u57fa\u4e8e\u52a8\u91cf-\u9891\u7387\u7a7a\u95f4\uff0c\u800c\u91cf\u5b50\u6a21\u62df\u5668\u5728\u5b9e\u7a7a\u95f4-\u5b9e\u65f6\u57df\u64cd\u4f5c\u66f4\u81ea\u7136\uff0c\u4f46\u7f3a\u4e4f\u5b9e\u7a7a\u95f4\u5173\u8054\u7684\u5b9e\u9a8c\u7814\u7a76\uff0c\u5c24\u5176\u5bf9\u91cf\u5b50\u81ea\u65cb\u6db2\u4f53\u7b49\u5947\u5f02\u7269\u6001\u7684\u52a8\u529b\u5b66\u7279\u5f81\u7406\u89e3\u4e0d\u8db3\u3002", "method": "\u5229\u7528\u9ad8\u5206\u8fa8\u7387\u5085\u91cc\u53f6\u53d8\u6362\u4e2d\u5b50\u5149\u8c31\u6280\u672f\u76f4\u63a5\u6d4b\u91cfKYbSe\u2082\u7684\u5b9e\u7a7a\u95f4-\u65f6\u95f4van Hove\u81ea\u65cb\u5173\u8054\u51fd\u6570G(r,t)\uff0c\u5e76\u4e0e\u4e94\u79cd\u57fa\u4e8e\u516c\u8ba4\u81ea\u65cb\u54c8\u5bc6\u987f\u91cf\u7684\u7406\u8bba\u6a21\u62df\uff08\u5305\u62ec\u5148\u8fdb\u6570\u503c\u548c\u573a\u8bba\u65b9\u6cd5\uff09\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u5b9e\u9a8c\u89c2\u6d4b\u5230KYbSe\u2082\u5728\u4f4e\u6e29\u4e0b\u5b58\u5728\u975e\u7ebf\u6027\u4e9a\u5f39\u9053\u81ea\u65cb\u8f93\u8fd0\u884c\u4e3a\uff0c\u4f46\u6240\u6709\u73b0\u6709\u7406\u8bba\u65b9\u6cd5\u5747\u65e0\u6cd5\u590d\u73b0\u8be5\u73b0\u8c61\uff0c\u63ed\u793a\u5f53\u524d\u6a21\u578b\u5bf9\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u5b9e\u7a7a\u95f4\u52a8\u529b\u5b66\u7684\u63cf\u8ff0\u5b58\u5728\u6839\u672c\u6027\u7f3a\u5931\u3002", "conclusion": "\u8be5\u5f02\u5e38\u8f93\u8fd0\u884c\u4e3a\u53ef\u80fd\u6e90\u4e8e\u91cf\u5b50\u81ea\u65cb\u6db2\u4f53\u4e34\u754c\u76f8\u7684\u6d8c\u73b0\u6d41\u4f53\u52a8\u529b\u5b66\uff0c\u4e3a\u7406\u89e3\u5f3a\u5173\u8054\u91cf\u5b50\u7269\u8d28\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u53ef\u4f5c\u4e3a\u672a\u6765\u91cf\u5b50\u6a21\u62df\u5668\u7684\u5173\u952e\u57fa\u51c6\u6d4b\u8bd5\u76ee\u6807\u3002"}}
{"id": "2602.00353", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.00353", "abs": "https://arxiv.org/abs/2602.00353", "authors": ["Yihe Zhang", "Cheyenne N Mohawk", "Kaiying Han", "Vijay Srinivas Tida", "Manyu Li", "Xiali Hei"], "title": "MHDash: An Online Platform for Benchmarking Mental Health-Aware AI Assistants", "comment": "Accepted for presentation at IEEE SoutheastCon 2026. This is the author version of an accepted paper. The final version will appear in IEEE Xplore", "summary": "Large language models (LLMs) are increasingly applied in mental health support systems, where reliable recognition of high-risk states such as suicidal ideation and self-harm is safety-critical. However, existing evaluations primarily rely on aggregate performance metrics, which often obscure risk-specific failure modes and provide limited insight into model behavior in realistic, multi-turn interactions. We present MHDash, an open-source platform designed to support the development, evaluation, and auditing of AI systems for mental health applications. MHDash integrates data collection, structured annotation, multi-turn dialogue generation, and baseline evaluation into a unified pipeline. The platform supports annotations across multiple dimensions, including Concern Type, Risk Level, and Dialogue Intent, enabling fine-grained and risk-aware analysis. Our results reveal several key findings: (i) simple baselines and advanced LLM APIs exhibit comparable overall accuracy yet diverge significantly on high-risk cases; (ii) some LLMs maintain consistent ordinal severity ranking while failing absolute risk classification, whereas others achieve reasonable aggregate scores but suffer from high false negative rates on severe categories; and (iii) performance gaps are amplified in multi-turn dialogues, where risk signals emerge gradually. These observations demonstrate that conventional benchmarks are insufficient for safety-critical mental health settings. By releasing MHDash as an open platform, we aim to promote reproducible research, transparent evaluation, and safety-aligned development of AI systems for mental health support.", "AI": {"tldr": "MHDash is an open-source platform for fine-grained evaluation of LLMs in mental health applications, revealing that conventional aggregate metrics mask critical failures in high-risk scenarios and multi-turn dialogues.", "motivation": "LLMs are increasingly used in mental health support where detecting high-risk states (suicidal ideation, self-harm) is safety-critical, but existing evaluations rely on aggregate metrics that obscure risk-specific failures and lack insight into realistic multi-turn interactions.", "method": "MHDash integrates data collection, structured annotation, multi-turn dialogue generation, and baseline evaluation into a unified pipeline, supporting fine-grained annotations across Concern Type, Risk Level, and Dialogue Intent dimensions for risk-aware analysis.", "result": "Key findings: (i) simple baselines and advanced LLMs show comparable overall accuracy but diverge significantly on high-risk cases; (ii) some LLMs maintain ordinal severity ranking while failing absolute risk classification, others have reasonable aggregate scores but high false negative rates on severe categories; (iii) performance gaps amplify in multi-turn dialogues where risk signals emerge gradually, proving conventional benchmarks insufficient.", "conclusion": "By open-sourcing MHDash, the authors aim to promote reproducible research, transparent evaluation, and safety-aligned development of AI systems for mental health support."}}
{"id": "2602.02040", "categories": ["cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.02040", "abs": "https://arxiv.org/abs/2602.02040", "authors": ["Longjun Xiang", "Jinxiong Jia", "Jian Wang"], "title": "Quantum Geometric Entropy Production and Entropy Hall Effect", "comment": null, "summary": "Quantum geometry, encoded in the Berry curvature and quantum metric, has unified diverse anomalous transport phenomena in solids, yet a microscopic quantum-geometric theory of entropy transport for Bloch electrons is still lacking. We formulate an entropy continuity equation for noninteracting fermions driven by an electric field, starting from the von Neumann entropy, and obtain quantum-mechanical expressions for the entropy current density and entropy production rate. Introducing relaxation through a relaxation-time dissipator, we identify the quantum metric as the origin of the leading entropy production, providing a direct microscopic diagnostic of dissipation in both the extrinsic Drude response and an intrinsic nonlinear Ohmic contribution controlled by quantum metric. We further predict an entropy Hall effect arising from the Berry curvature and show that it obeys an Onsager reciprocal relation with the anomalous Nernst effect under a temperature gradient. Finally, we establish universal relations connecting entropy and charge currents under DC and AC driving, offering experimentally accessible probes of quantum geometry through nonequilibrium entropy flow.", "AI": {"tldr": "This paper develops a microscopic quantum-geometric theory of entropy transport in Bloch electrons, identifying quantum metric as the origin of entropy production and predicting a new entropy Hall effect with Onsager reciprocity.", "motivation": "A microscopic quantum-geometric theory of entropy transport for Bloch electrons is still lacking, despite quantum geometry (Berry curvature and quantum metric) having unified diverse anomalous transport phenomena in solids.", "method": "Formulated an entropy continuity equation for noninteracting fermions driven by an electric field starting from von Neumann entropy, and introduced relaxation through a relaxation-time dissipator.", "result": "Identified quantum metric as the origin of leading entropy production, providing a microscopic diagnostic of dissipation in both Drude response and nonlinear Ohmic contribution; predicted an entropy Hall effect from Berry curvature obeying Onsager reciprocal relation with anomalous Nernst effect; established universal relations connecting entropy and charge currents under DC/AC driving.", "conclusion": "The work offers experimentally accessible probes of quantum geometry through nonequilibrium entropy flow, advancing the microscopic understanding of dissipation in quantum materials."}}
{"id": "2602.02478", "categories": ["cond-mat.str-el", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2602.02478", "abs": "https://arxiv.org/abs/2602.02478", "authors": ["Xi Chen", "Zhi-Da Song"], "title": "Orbital Magnetization of Interacting Electrons", "comment": "11 pages, 2 figures", "summary": "We derive an exact expression for the orbital magnetization of electrons with short-range interactions (such as density-density interactions) in terms of exact zero-frequency response functions of the zero-field system. The result applies to weakly and strongly correlated electrons at zero and finite temperature, provided that the local grand potential density only depends on local thermodynamic parameters. We benchmark the formula for non-interacting and weakly-coupled electrons. To zeroth and first orders in the interaction strength, it agrees with the modern theory of orbital magnetization and its recent generalization to self-consistent Hartree-Fock bands. Our work provides an exact framework of interacting orbital magnetization beyond mean-field treatments, and paves the way for quantitative studies of strongly correlated electrons in external magnetic fields.", "AI": {"tldr": "\u63d0\u51fa\u7535\u5b50\u8f68\u9053\u78c1\u5316\u7684\u7cbe\u786e\u8868\u8fbe\u5f0f\uff0c\u9002\u7528\u4e8e\u77ed\u7a0b\u76f8\u4e92\u4f5c\u7528\u4f53\u7cfb\uff08\u5305\u62ec\u5f31/\u5f3a\u5173\u8054\u7535\u5b50\u3001\u96f6/\u6709\u9650\u6e29\u5ea6\uff09\uff0c\u7a81\u7834\u5e73\u5747\u573a\u7406\u8bba\u9650\u5236\uff0c\u4e3a\u5f3a\u5173\u8054\u7535\u5b50\u5728\u78c1\u573a\u4e2d\u7684\u5b9a\u91cf\u7814\u7a76\u63d0\u4f9b\u65b0\u6846\u67b6", "motivation": "\u73b0\u6709\u8f68\u9053\u78c1\u5316\u7406\u8bba\u5728\u5f3a\u5173\u8054\u7535\u5b50\u4f53\u7cfb\u4e2d\u5b58\u5728\u5c40\u9650\uff0c\u65e0\u6cd5\u8d85\u8d8a\u5e73\u5747\u573a\u8fd1\u4f3c\u5904\u7406\u77ed\u7a0b\u76f8\u4e92\u4f5c\u7528\uff08\u5982\u5bc6\u5ea6-\u5bc6\u5ea6\u76f8\u4e92\u4f5c\u7528\uff09\uff0c\u4e9f\u9700\u5efa\u7acb\u9002\u7528\u4e8e\u5f3a\u5173\u8054\u4f53\u7cfb\u7684\u7cbe\u786e\u7406\u8bba\u6846\u67b6", "method": "\u57fa\u4e8e\u96f6\u573a\u7cfb\u7edf\u7684\u7cbe\u786e\u96f6\u9891\u54cd\u5e94\u51fd\u6570\uff0c\u63a8\u5bfc\u7535\u5b50\u8f68\u9053\u78c1\u5316\u7684\u4e25\u683c\u8868\u8fbe\u5f0f\uff1b\u8981\u6c42\u5c40\u57df\u5de8\u52bf\u5bc6\u5ea6\u4ec5\u4f9d\u8d56\u5c40\u57df\u70ed\u529b\u5b66\u53c2\u6570\uff0c\u6db5\u76d6\u5f31/\u5f3a\u5173\u8054\u53ca\u96f6/\u6709\u9650\u6e29\u5ea6\u6761\u4ef6", "result": "\u5728\u975e\u76f8\u4e92\u4f5c\u7528\u548c\u5f31\u8026\u5408\u7535\u5b50\u4f53\u7cfb\u4e2d\u8fdb\u884c\u9a8c\u8bc1\uff1a\u5728\u76f8\u4e92\u4f5c\u7528\u5f3a\u5ea6\u96f6\u9636\u548c\u4e00\u9636\u5c55\u5f00\u4e0b\uff0c\u7ed3\u679c\u4e0e\u73b0\u4ee3\u8f68\u9053\u78c1\u5316\u7406\u8bba\u53ca\u81ea\u6d3dHartree-Fock\u5e26\u63a8\u5e7f\u5b8c\u5168\u4e00\u81f4", "conclusion": "\u5efa\u7acb\u4e86\u8d85\u8d8a\u5e73\u5747\u573a\u5904\u7406\u7684\u76f8\u4e92\u4f5c\u7528\u8f68\u9053\u78c1\u5316\u7cbe\u786e\u7406\u8bba\u6846\u67b6\uff0c\u4e3a\u5f3a\u5173\u8054\u7535\u5b50\u5728\u5916\u52a0\u78c1\u573a\u4e2d\u7684\u5b9a\u91cf\u7814\u7a76\u5960\u5b9a\u57fa\u7840"}}
{"id": "2602.00255", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.00255", "abs": "https://arxiv.org/abs/2602.00255", "authors": ["Richard Cleve", "Alex May"], "title": "Lower bounds on non-local computation from controllable correlation", "comment": null, "summary": "Understanding entanglement cost in non-local quantum computation (NLQC) is relevant to complexity, cryptography, gravity, and other areas. This entanglement cost is largely uncharacterized; previous lower bound techniques apply to narrowly defined cases, and proving lower bounds on most simple unitaries has remained open. Here, we give two new lower bound techniques that can be evaluated for any unitary, based on their controllable correlation and controllable entanglement. For Haar random two qubit unitaries, our techniques typically lead to non-trivial lower bounds. Further, we obtain lower bounds on most of the commonly studied two qubit quantum gates, including CNOT, DCNOT, $\\sqrt{\\text{SWAP}}$, and the XX interaction, none of which previously had known lower bounds. For the CNOT gate, one of our techniques gives a tight lower bound, fully resolving its entanglement cost. The resulting lower bounds have parallel repetition properties, and apply in the noisy setting.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u65b0\u7684\u7ea0\u7f20\u4ee3\u4ef7\u4e0b\u754c\u6280\u672f\uff0c\u53ef\u8bc4\u4f30\u4efb\u610f\u91cf\u5b50\u95e8\uff0c\u89e3\u51b3\u4e86CNOT\u7b49\u5e38\u7528\u4e24\u6bd4\u7279\u95e8\u7684\u7ea0\u7f20\u4ee3\u4ef7\u95ee\u9898\uff0c\u5176\u4e2dCNOT\u7684\u4e0b\u754c\u88ab\u7cbe\u786e\u786e\u5b9a", "motivation": "\u975e\u5c40\u57df\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7684\u7ea0\u7f20\u4ee3\u4ef7\u5bf9\u590d\u6742\u6027\u3001\u5bc6\u7801\u5b66\u3001\u5f15\u529b\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u4e0b\u754c\u6280\u672f\u4ec5\u9002\u7528\u4e8e\u72ed\u7a84\u60c5\u51b5\uff0c\u591a\u6570\u7b80\u5355\u91cf\u5b50\u95e8\uff08\u5982CNOT\uff09\u7684\u7ea0\u7f20\u4ee3\u4ef7\u4e0b\u754c\u95ee\u9898\u957f\u671f\u672a\u89e3\u51b3", "method": "\u57fa\u4e8e\u53ef\u63a7\u5173\u8054\u548c\u53ef\u63a7\u7ea0\u7f20\u63d0\u51fa\u4e24\u79cd\u666e\u9002\u6027\u4e0b\u754c\u6280\u672f\uff0c\u53ef\u5e94\u7528\u4e8e\u4efb\u610f\u9149\u7b97\u5b50", "result": "\u5bf9Haar\u968f\u673a\u4e24\u6bd4\u7279\u95e8\u83b7\u5f97\u975e\u5e73\u51e1\u4e0b\u754c\uff1b\u9996\u6b21\u4e3aCNOT\u3001DCNOT\u3001\u221aSWAP\u548cXX\u76f8\u4e92\u4f5c\u7528\u7b49\u5e38\u7528\u95e8\u63d0\u4f9b\u4e0b\u754c\uff1b\u5176\u4e2dCNOT\u7684\u7ea0\u7f20\u4ee3\u4ef7\u88ab\u7cbe\u786e\u786e\u5b9a\uff08\u7d27\u4e0b\u754c\uff09", "conclusion": "\u65b0\u4e0b\u754c\u6280\u672f\u5177\u6709\u5e76\u884c\u91cd\u590d\u7279\u6027\u4e14\u9002\u7528\u4e8e\u566a\u58f0\u73af\u5883\uff0c\u663e\u8457\u63a8\u8fdb\u4e86\u5bf9\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7ea0\u7f20\u4ee3\u4ef7\u7684\u7406\u89e3\uff0c\u89e3\u51b3\u4e86\u5173\u952e\u5f00\u653e\u95ee\u9898"}}
{"id": "2602.00059", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00059", "abs": "https://arxiv.org/abs/2602.00059", "authors": ["Zizheng Zhang", "Yuyang Liao", "Chen Chen", "Jian He", "Dun Wu", "Qianjin Yu", "Yanqin Gao", "Jin Yang", "Kailai Zhang", "Eng Siong Chng", "Xionghu Zhong"], "title": "TextBFGS: Quasi-Newton Optimization for Discrete Executable Text via Gradient-Operator Retrieval", "comment": null, "summary": "Optimizing discrete executable text such as prompts and code has recently been framed as a gradient-based process, effectively translating backpropagation concepts to the semantic space. However, existing methods predominantly operate as first-order optimizers akin to Stochastic Gradient Descent, which are suffering from slow convergence and instability because they neglect the semantic curvature of the optimization landscape. To bridge this gap, we introduce TextBFGS, a second-order framework to implement a Quasi-Newton optimization method for discrete text. Unlike traditional memory-based approaches that retrieve similar textual instances, TextBFGS approximates the inverse Hessian matrix by retrieving Gradient-Operators from the memory of pre-learned successful trajectories. Specifically, given a textual gradient feedback, TextBFGS identifies historical correction patterns from the optimization knowledge base and tries to apply these abstract operators to the current variable. This mechanism enables a One-Pass Update, combining feedback generation and second-order correction into a single inference step. Empirical evaluations on code optimization across diverse domains (e.g., HumanEval, MBPP) demonstrate that TextBFGS significantly outperforms first-order baselines. It achieves superior pass rates with fewer model calls and exhibits strong cross-task transferability, thus establishes a mathematically grounded paradigm for efficient, memory-aware text optimization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTextBFGS\uff0c\u4e00\u79cd\u7528\u4e8e\u79bb\u6563\u53ef\u6267\u884c\u6587\u672c\uff08\u5982\u63d0\u793a\u8bcd\u548c\u4ee3\u7801\uff09\u7684\u4e8c\u9636\u62df\u725b\u987f\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u5386\u53f2\u4f18\u5316\u8f68\u8ff9\u4e2d\u68c0\u7d22\u68af\u5ea6\u7b97\u5b50\u6765\u8fd1\u4f3cHessian\u77e9\u9635\uff0c\u5b9e\u73b0\u5355\u904d\u66f4\u65b0\uff0c\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u4e00\u9636\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u79bb\u6563\u6587\u672c\u68af\u5ea6\u4f18\u5316\u65b9\u6cd5\u591a\u4e3a\u4e00\u9636\u4f18\u5316\u5668\uff08\u5982\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff09\uff0c\u56e0\u5ffd\u7565\u4f18\u5316\u666f\u89c2\u7684\u8bed\u4e49\u66f2\u7387\u800c\u5b58\u5728\u6536\u655b\u901f\u5ea6\u6162\u548c\u7a33\u5b9a\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faTextBFGS\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u9884\u5b66\u4e60\u7684\u6210\u529f\u8f68\u8ff9\u8bb0\u5fc6\u4e2d\u68c0\u7d22\"\u68af\u5ea6\u7b97\u5b50\"\u6765\u8fd1\u4f3c\u9006Hessian\u77e9\u9635\u3002\u7ed9\u5b9a\u6587\u672c\u68af\u5ea6\u53cd\u9988\u65f6\uff0c\u8be5\u65b9\u6cd5\u4ece\u4f18\u5316\u77e5\u8bc6\u5e93\u4e2d\u8bc6\u522b\u5386\u53f2\u4fee\u6b63\u6a21\u5f0f\uff0c\u5e76\u5c06\u8fd9\u4e9b\u62bd\u8c61\u7b97\u5b50\u5e94\u7528\u4e8e\u5f53\u524d\u53d8\u91cf\uff0c\u5b9e\u73b0\u53cd\u9988\u751f\u6210\u4e0e\u4e8c\u9636\u4fee\u6b63\u7684\u5355\u904d\u66f4\u65b0\u3002", "result": "\u5728HumanEval\u3001MBPP\u7b49\u591a\u6837\u9886\u57df\u7684\u4ee3\u7801\u4f18\u5316\u4efb\u52a1\u4e0a\uff0cTextBFGS\u663e\u8457\u8d85\u8d8a\u4e00\u9636\u57fa\u7ebf\uff0c\u4ee5\u66f4\u5c11\u7684\u6a21\u578b\u8c03\u7528\u5b9e\u73b0\u66f4\u9ad8\u7684\u901a\u8fc7\u7387\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u4efb\u52a1\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u9ad8\u6548\u3001\u8bb0\u5fc6\u611f\u77e5\u6587\u672c\u4f18\u5316\u7684\u6570\u5b66\u57fa\u7840\u8303\u5f0f\uff0c\u8bc1\u660e\u4e86\u4e8c\u9636\u4f18\u5316\u65b9\u6cd5\u5728\u79bb\u6563\u6587\u672c\u4f18\u5316\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.00359", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00359", "abs": "https://arxiv.org/abs/2602.00359", "authors": ["Minhua Lin", "Hanqing Lu", "Zhan Shi", "Bing He", "Rui Mao", "Zhiwei Zhang", "Zongyu Wu", "Xianfeng Tang", "Hui Liu", "Zhenwei Dai", "Xiang Zhang", "Suhang Wang", "Benoit Dumoulin", "Jian Pei"], "title": "Position: Agentic Evolution is the Path to Evolving LLMs", "comment": null, "summary": "As Large Language Models (LLMs) move from curated training sets into open-ended real-world environments, a fundamental limitation emerges: static training cannot keep pace with continual deployment environment change. Scaling training-time and inference-time compute improves static capability but does not close this train-deploy gap. We argue that addressing this limitation requires a new scaling axis-evolution. Existing deployment-time adaptation methods, whether parametric fine-tuning or heuristic memory accumulation, lack the strategic agency needed to diagnose failures and produce durable improvements. Our position is that agentic evolution represents the inevitable future of LLM adaptation, elevating evolution itself from a fixed pipeline to an autonomous evolver agent. We instantiate this vision in a general framework, A-Evolve, which treats deployment-time improvement as a deliberate, goal-directed optimization process over persistent system state. We further propose the evolution-scaling hypothesis: the capacity for adaptation scales with the compute allocated to evolution, positioning agentic evolution as a scalable path toward sustained, open-ended adaptation in the real world.", "AI": {"tldr": "\u63d0\u51faA-Evolve\u6846\u67b6\uff0c\u5c06LLM\u90e8\u7f72\u65f6\u6539\u8fdb\u89c6\u4e3a\u81ea\u4e3b\u667a\u80fd\u4f53\u5bf9\u6301\u4e45\u72b6\u6001\u7684\u5b9a\u5411\u4f18\u5316\u8fc7\u7a0b\uff0c\u5e76\u63d0\u51fa\u8fdb\u5316\u7f29\u653e\u5047\u8bbe\u3002", "motivation": "\u9759\u6001\u8bad\u7ec3\u65e0\u6cd5\u8ddf\u4e0a\u90e8\u7f72\u73af\u5883\u53d8\u5316\uff0c\u73b0\u6709\u9002\u5e94\u65b9\u6cd5\u7f3a\u4e4f\u6218\u7565\u80fd\u52a8\u6027\uff0c\u9700\u8981\u65b0\u7684\u6269\u5c55\u7ef4\u5ea6\u3002", "method": "\u63d0\u51fa\"\u8fdb\u5316\"\u4f5c\u4e3a\u65b0\u6269\u5c55\u8f74\uff0c\u8bbe\u8ba1A-Evolve\u6846\u67b6\uff0c\u8ba9\u81ea\u4e3b\u8fdb\u5316\u667a\u80fd\u4f53\u5728\u90e8\u7f72\u65f6\u5bf9\u6301\u4e45\u7cfb\u7edf\u72b6\u6001\u8fdb\u884c\u76ee\u6807\u5bfc\u5411\u7684\u4f18\u5316\u3002", "result": "\u63d0\u51fa\u8fdb\u5316\u7f29\u653e\u5047\u8bbe\uff1a\u9002\u5e94\u80fd\u529b\u968f\u5206\u914d\u7ed9\u8fdb\u5316\u7684\u8ba1\u7b97\u91cf\u800c\u6269\u5c55\u3002", "conclusion": "\u667a\u80fd\u4f53\u8fdb\u5316\u662fLLM\u9002\u5e94\u4e0d\u53ef\u907f\u514d\u7684\u672a\u6765\uff0c\u4e3a\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u5b9e\u73b0\u6301\u7eed\u3001\u5f00\u653e\u5f0f\u9002\u5e94\u63d0\u4f9b\u53ef\u6269\u5c55\u8def\u5f84\u3002"}}
{"id": "2602.02078", "categories": ["cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.02078", "abs": "https://arxiv.org/abs/2602.02078", "authors": ["Jean-Philippe Bouchaud"], "title": "Mandelbrot, Financial Markets and the Origins of \"Econophysics\"", "comment": null, "summary": "This text revisits the origins of econophysics through the figure of Beno\u00eet Mandelbrot, not as the father of fractals, but as the instigator of a distinctive scientific posture. The guiding thread is methodological: accept the stubborn features of the data and use models as instruments for intuition rather than as axiomatic certificates of truth. In this perspective, scaling, intermittency and extremes are not peripheral imperfections around a well-behaved equilibrium; they are the very texture of economic and financial fluctuations. This naturally shifts attention from exogenous narratives to endogenous dynamics: interactions, feedback loops, and collective amplification mechanisms that can make systems intrinsically {\\it fragile}. We argue that the importation of concepts from statistical physics -- criticality, disorder, emergence, multiplicative cascades -- should be read not as an artificial transposition but as a candid attempt to look for generic mechanisms compatible with empirical regularities observed across scales, from markets to macroeconomic aggregates.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7Mandelbrot\u7684\u79d1\u5b66\u65b9\u6cd5\u8bba\u91cd\u65b0\u5ba1\u89c6\u7ecf\u6d4e\u7269\u7406\u5b66\u8d77\u6e90\uff0c\u4e3b\u5f20\u5c06\u6807\u5ea6\u6027\u3001\u95f4\u6b47\u6027\u548c\u6781\u7aef\u4e8b\u4ef6\u89c6\u4e3a\u7ecf\u6d4e\u6ce2\u52a8\u7684\u672c\u8d28\u7279\u5f81\uff0c\u800c\u975e\u5747\u8861\u5468\u56f4\u7684\u7f3a\u9677\u3002\u7269\u7406\u5b66\u6982\u5ff5\u7684\u5f15\u5165\u662f\u5bfb\u627e\u8de8\u5c3a\u5ea6\u901a\u7528\u673a\u5236\u7684\u771f\u8bda\u5c1d\u8bd5\uff0c\u65e8\u5728\u7406\u89e3\u5185\u751f\u52a8\u529b\u5b66\u548c\u7cfb\u7edf\u5185\u5728\u8106\u5f31\u6027\u3002", "motivation": "\u901a\u8fc7Mandelbrot\u7684\u72ec\u7279\u79d1\u5b66\u59ff\u6001\uff08\u800c\u975e\u4ec5\u4f5c\u4e3a\u5206\u5f62\u4e4b\u7236\uff09\u91cd\u65b0\u5ba1\u89c6\u7ecf\u6d4e\u7269\u7406\u5b66\u7684\u8d77\u6e90\uff0c\u5f3a\u8c03\u5176\u65b9\u6cd5\u8bba\u4ef7\u503c\u2014\u2014\u63a5\u53d7\u6570\u636e\u987d\u56fa\u7279\u5f81\uff0c\u5c06\u6a21\u578b\u4f5c\u4e3a\u76f4\u89c9\u5de5\u5177\u800c\u975e\u771f\u7406\u8bc1\u4e66\u3002", "method": "\u91c7\u7528\u65b9\u6cd5\u8bba\u5206\u6790\uff0c\u805a\u7126Mandelbrot\"\u4ece\u6570\u636e\u51fa\u53d1\"\u7684\u79d1\u5b66\u59ff\u6001\uff0c\u63a2\u8ba8\u5176\u5982\u4f55\u4ece\u5916\u751f\u53d9\u4e8b\u8f6c\u5411\u5185\u751f\u52a8\u529b\u5b66\uff08\u76f8\u4e92\u4f5c\u7528\u3001\u53cd\u9988\u5faa\u73af\u3001\u96c6\u4f53\u653e\u5927\u673a\u5236\uff09\uff0c\u5e76\u91cd\u65b0\u89e3\u8bfb\u7269\u7406\u5b66\u6982\u5ff5\u7684\u5f15\u5165\u3002", "result": "\u63ed\u793a\u4e86\u6807\u5ea6\u6027\u3001\u95f4\u6b47\u6027\u548c\u6781\u7aef\u4e8b\u4ef6\u662f\u7ecf\u6d4e/\u91d1\u878d\u6ce2\u52a8\u7684\u6839\u672c\u7eb9\u7406\uff0c\u7269\u7406\u5b66\u6982\u5ff5\uff08\u4e34\u754c\u6027\u3001\u65e0\u5e8f\u6027\u3001\u6d8c\u73b0\u6027\u3001\u7ea7\u8054\u6548\u5e94\uff09\u7684\u5f15\u5165\u662f\u5bfb\u627e\u4e0e\u8de8\u5c3a\u5ea6\u7ecf\u9a8c\u89c4\u5f8b\u76f8\u7b26\u7684\u901a\u7528\u673a\u5236\u7684\u771f\u8bda\u5c1d\u8bd5\u3002", "conclusion": "\u7ecf\u6d4e\u7269\u7406\u5b66\u5e94\u88ab\u7406\u89e3\u4e3a\u8bc6\u522b\u7ecf\u6d4e\u7cfb\u7edf\u5e95\u5c42\u901a\u7528\u673a\u5236\u7684\u771f\u8bda\u52aa\u529b\uff0c\u7269\u7406\u5b66\u6982\u5ff5\u4e0e\u89c2\u6d4b\u7ecf\u9a8c\u6a21\u5f0f\u81ea\u7136\u5951\u5408\uff0c\u4e3a\u7406\u89e3\u7cfb\u7edf\u5185\u5728\u8106\u5f31\u6027\u63d0\u4f9b\u4e86\u6df1\u523b\u89c1\u89e3\uff0c\u800c\u975e\u4eba\u4e3a\u7684\u8de8\u5b66\u79d1\u79fb\u690d\u3002"}}
{"id": "2602.00258", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.00258", "abs": "https://arxiv.org/abs/2602.00258", "authors": ["Antonio Camurati", "Felipe Sobrero", "Bruno Suassuna", "Pedro V. Paraguass\u00fa"], "title": "From Feynman-Vernon to Wiener Stochastic Path Integral", "comment": "9 pages", "summary": "We establish a direct connection between the Feynman-Vernon path integral formalism for open quantum systems and the Wiener path integral used in classical stochastic dynamics. By considering a generalized influence functional in the strong decoherence limit, we demonstrate that integrating over the quantum coherence length leads to a derivation of stochastic Langevin dynamics. Specifically, we show that the quantum Feynman measure transforms into the stochastic Wiener measure. Applying this framework to the Wigner function representation, we show that the system follows a stochastic path interpretable via classical probability theory. Finally, we address the inverse problem: constructing an equivalent quantum influence functional from a given classical Langevin equation.", "AI": {"tldr": "This paper bridges quantum and classical stochastic dynamics by showing how Feynman-Vernon path integrals transform into Wiener path integrals under strong decoherence, enabling bidirectional conversion between quantum influence functionals and classical Langevin equations.", "motivation": "To establish a fundamental connection between quantum open system dynamics (Feynman-Vernon formalism) and classical stochastic processes (Wiener path integrals), addressing the quantum-to-classical transition and enabling cross-formalism problem solving.", "method": "Analyzes the generalized influence functional in the strong decoherence limit, integrates over the quantum coherence length to derive stochastic Langevin dynamics, and applies the framework to Wigner function representation for classical probability interpretation.", "result": "Demonstrates that the quantum Feynman measure becomes equivalent to the stochastic Wiener measure under strong decoherence; shows Wigner function follows stochastic paths interpretable via classical probability; solves the inverse problem of constructing quantum influence functionals from classical Langevin equations.", "conclusion": "A bidirectional mathematical framework is established between quantum and classical stochastic dynamics, providing new tools for modeling open quantum systems and understanding quantum-classical correspondence through path integral equivalence."}}
{"id": "2602.00370", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00370", "abs": "https://arxiv.org/abs/2602.00370", "authors": ["Trisha Das", "Katherine Kero", "Dorinda Schumann", "Tracy Ohrt", "Sanjit Singh Batra", "Gregory D Lyng", "Robert E. Tillman"], "title": "POET: Protocol Optimization via Eligibility Tuning", "comment": null, "summary": "Eligibility criteria (EC) are essential for clinical trial design, yet drafting them remains a time-intensive and cognitively demanding task for clinicians. Existing automated approaches often fall at two extremes either requiring highly structured inputs, such as predefined entities to generate specific criteria, or relying on end-to-end systems that produce full eligibility criteria from minimal input such as trial descriptions limiting their practical utility. In this work, we propose a guided generation framework that introduces interpretable semantic axes, such as Demographics, Laboratory Parameters, and Behavioral Factors, to steer EC generation. These axes, derived using large language models, offer a middle ground between specificity and usability, enabling clinicians to guide generation without specifying exact entities. In addition, we present a reusable rubric-based evaluation framework that assesses generated criteria along clinically meaningful dimensions. Our results show that our guided generation approach consistently outperforms unguided generation in both automatic, rubric-based and clinician evaluations, offering a practical and interpretable solution for AI-assisted trial design.", "AI": {"tldr": "\u63d0\u51fa\u53ef\u89e3\u91ca\u7684\u8bed\u4e49\u8f74\u5f15\u5bfc\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u4e34\u5e8a\u8bd5\u9a8c\u5165\u9009\u6807\u51c6\uff0c\u5728\u81ea\u52a8\u8bc4\u4f30\u548c\u4e34\u5e8a\u533b\u751f\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u65e0\u5f15\u5bfc\u65b9\u6cd5", "motivation": "\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u5b58\u5728\u4e24\u6781\u5206\u5316\uff1a\u8981\u4e48\u9700\u8981\u9ad8\u5ea6\u7ed3\u6784\u5316\u8f93\u5165\uff08\u5982\u9884\u5b9a\u4e49\u5b9e\u4f53\uff09\uff0c\u8981\u4e48\u4f9d\u8d56\u7aef\u5230\u7aef\u7cfb\u7edf\u751f\u6210\u5b8c\u6574\u6807\u51c6\u4f46\u5b9e\u7528\u6027\u6709\u9650\uff1b\u4e34\u5e8a\u533b\u751f\u8d77\u8349\u5165\u9009\u6807\u51c6\u8017\u65f6\u4e14\u8ba4\u77e5\u8d1f\u62c5\u91cd", "method": "\u8bbe\u8ba1\u53ef\u89e3\u91ca\u7684\u8bed\u4e49\u8f74\uff08\u5982\u4eba\u53e3\u7edf\u8ba1\u5b66\u3001\u5b9e\u9a8c\u5ba4\u53c2\u6570\u3001\u884c\u4e3a\u56e0\u7d20\uff09\u5f15\u5bfc\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u5bfc\u8bed\u4e49\u8f74\uff1b\u5efa\u7acb\u57fa\u4e8erubric\u7684\u4e34\u5e8a\u7ef4\u5ea6\u8bc4\u4f30\u4f53\u7cfb", "result": "\u5f15\u5bfc\u751f\u6210\u65b9\u6cd5\u5728\u81ea\u52a8\u8bc4\u4f30\u3001rubric\u8bc4\u4f30\u548c\u4e34\u5e8a\u533b\u751f\u8bc4\u4f30\u4e2d\u5747\u6301\u7eed\u4f18\u4e8e\u65e0\u5f15\u5bfc\u751f\u6210", "conclusion": "\u8be5\u6846\u67b6\u4e3aAI\u8f85\u52a9\u8bd5\u9a8c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u751f\u6210\u7ed3\u679c\u7684\u7279\u5f02\u6027\u548c\u4e34\u5e8a\u53ef\u7528\u6027"}}
{"id": "2602.02446", "categories": ["cond-mat.stat-mech", "math.PR"], "pdf": "https://arxiv.org/pdf/2602.02446", "abs": "https://arxiv.org/abs/2602.02446", "authors": ["Paul C Bressloff", "Samantha Linn"], "title": "Renewal theory for a run-and-tumble particle with stochastic resetting and a sticky boundary", "comment": "18 pages, 10 figures", "summary": "We consider a run-and-tumble particle (RTP) with stochastic resetting confined to the half line $[0,\\infty)$ with a sticky boundary at $x=0$. In the bulk the RTP tumbles at a constant rate $\u03b1>0$ between velocity states $\\pm v$ with $v>0$ and randomly resets to its initial position and orientation $(x_0,k_0)\\in(\\mathbb{R}^+,\\pm)$. When the RTP reaches the target at $x=0$ it attaches to the boundary for a random waiting time before either detaching and continuing to navigate the bulk domain or (permanently) entering the target. These events are the analogs of adsorption, desorption, and absorption of a particle by a partially reactive surface in physical chemistry. We use renewal theory to characterize the particle trajectory in terms of successive binding events under two distinct desorption protocols: via resetting to $(x_0,k_0)$ and via continuous movement from $x=0$ with velocity $+v$. First we derive the nonequilibrum stationary state (NESS) in the case of no absorption and characterize the accumulation at the boundary. Second, we compute the mean first passage time (MFPT) statistics. In addition to observing the usual unimodal dependence of the MFPT on bulk resetting, both the NESS and MFPT strongly depend on the initial orientation $k_0$ and the desorption protocol. For instance, if the initial orientation is toward the boundary, we find that the desorption-induced resetting protocol can reduce the MFPT more effectively than the non-resetting desorption protocol. We also show how matching the desorption kinetics with the bulk resetting or tumbling rate introduces a trade-off between minimizing the adsorption and absorption times. In this setting we find that the desorption protocol which minimizes the absorption MFPT for a given set of parameters is almost always the opposite of that favored when desorption and bulk kinetics are not the same.", "AI": {"tldr": "\u7814\u7a76\u5177\u6709\u968f\u673a\u91cd\u7f6e\u7684\u8dd1\u52a8-\u7ffb\u8f6c\u7c92\u5b50(RTP)\u5728\u534a\u76f4\u7ebf\u4e0a\u7684\u52a8\u529b\u5b66\uff0c\u5206\u6790\u5176\u5728\u7c98\u6027\u8fb9\u754c\u5904\u7684\u5438\u9644/\u89e3\u5438/\u5438\u6536\u884c\u4e3a\uff0c\u6bd4\u8f83\u4e24\u79cd\u89e3\u5438\u534f\u8bae\u5bf9\u975e\u5e73\u8861\u7a33\u6001\u548c\u9996\u6b21\u901a\u8fc7\u65f6\u95f4\u7684\u5f71\u54cd", "motivation": "\u63a2\u7a76\u968f\u673a\u91cd\u7f6e\u4e0e\u90e8\u5206\u53cd\u5e94\u6027\u8fb9\u754c\u76f8\u4e92\u4f5c\u7528\u4e0b\u7c92\u5b50\u7684\u52a8\u529b\u5b66\u884c\u4e3a\uff0c\u63ed\u793a\u521d\u59cb\u53d6\u5411\u548c\u89e3\u5438\u673a\u5236\u5bf9\u8f93\u8fd0\u6548\u7387\u7684\u5f71\u54cd\uff0c\u4e3a\u7269\u7406\u5316\u5b66\u4e2d\u8868\u9762\u53cd\u5e94\u8fc7\u7a0b\u63d0\u4f9b\u7406\u8bba\u6a21\u578b", "method": "\u91c7\u7528\u66f4\u65b0\u7406\u8bba\u5206\u6790 successive binding events\uff0c\u63a8\u5bfc\u65e0\u5438\u6536\u60c5\u51b5\u4e0b\u7684\u975e\u5e73\u8861\u7a33\u6001(NESS)\uff0c\u8ba1\u7b97\u5e73\u5747\u9996\u6b21\u901a\u8fc7\u65f6\u95f4(MFPT)\u7edf\u8ba1\u7279\u6027\uff0c\u5bf9\u6bd4\u4e24\u79cd\u89e3\u5438\u534f\u8bae\uff08\u91cd\u7f6e\u81f3\u521d\u59cb\u72b6\u6001 vs \u4ece\u8fb9\u754c\u8fde\u7eed\u8fd0\u52a8\uff09", "result": "\u53d1\u73b0NESS\u548cMFPT\u5f3a\u70c8\u4f9d\u8d56\u521d\u59cb\u53d6\u5411k\u2080\u548c\u89e3\u5438\u534f\u8bae\uff1a\u5f53\u7c92\u5b50\u521d\u59cb\u671d\u5411\u8fb9\u754c\u65f6\uff0c\u89e3\u5438\u8bf1\u5bfc\u7684\u91cd\u7f6e\u534f\u8bae\u6bd4\u975e\u91cd\u7f6e\u534f\u8bae\u66f4\u6709\u6548\u964d\u4f4eMFPT\uff1b\u5339\u914d\u89e3\u5438\u52a8\u529b\u5b66\u4e0e\u4f53\u76f8\u91cd\u7f6e/\u7ffb\u8f6c\u901f\u7387\u65f6\u5b58\u5728\u5438\u9644\u4e0e\u5438\u6536\u65f6\u95f4\u7684\u6743\u8861\uff0c\u6700\u4f18\u89e3\u5438\u534f\u8bae\u5728\u52a8\u529b\u5b66\u53c2\u6570\u53d8\u5316\u65f6\u5448\u73b0\u53cd\u8f6c\u73b0\u8c61", "conclusion": "\u89e3\u5438\u534f\u8bae\u7684\u9009\u62e9\u9700\u7efc\u5408\u8003\u8651\u521d\u59cb\u6761\u4ef6\u548c\u52a8\u529b\u5b66\u53c2\u6570\uff0c\u91cd\u7f6e\u673a\u5236\u5728\u7279\u5b9a\u53d6\u5411\u4e0b\u663e\u8457\u63d0\u5347\u8fb9\u754c\u5230\u8fbe\u6548\u7387\uff0c\u4f46\u52a8\u529b\u5b66\u53c2\u6570\u5931\u914d\u65f6\u9700\u91c7\u7528\u76f8\u53cd\u534f\u8bae\u4ee5\u6700\u5c0f\u5316\u5438\u6536\u65f6\u95f4\uff0c\u4e3a\u8868\u9762\u53cd\u5e94\u8fc7\u7a0b\u7684\u4f18\u5316\u63a7\u5236\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e"}}
{"id": "2602.00452", "categories": ["quant-ph", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.00452", "abs": "https://arxiv.org/abs/2602.00452", "authors": ["X. Z. Zhang"], "title": "Single-site dissipation stabilizes a superconducting nonequilibrium steady state in a strongly correlated lattice", "comment": "15 pages, 11 figures", "summary": "Can superconducting order be made a robust attractor of open-system dynamics in strongly correlated lattices? We demonstrate that it can by proposing a minimal dissipation-engineering protocol for the particle--hole symmetric Hubbard model. By applying a rotated quantum jump operator, a locally transformed $\u03b7$-pair lowering operator, on as little as a single lattice site, we show that the Lindblad evolution autonomously pumps the system from the vacuum into a nonequilibrium steady state (NESS) with macroscopic $\u03b7$-pair off-diagonal long-range order (ODLRO). Crucially, this local-to-global synchronization contrasts with schemes requiring spatially extensive reservoirs: here, a strictly local dissipative seed suffices to establish coherence across the interacting system. We elucidate the mechanism via local dark-state selection, controlled elimination of off-manifold excursions induced by hopping, and a Liouvillian invariant-subspace structure that yields an attractive fixed point with a finite dissipative gap. Furthermore, we classify the stability of this NESS against static disorder, identifying a broad regime where the superconducting attractor is resilient to Hamiltonian perturbations that leave the effective subspace structure intact, while pinpointing specific perturbations that directly dephase the $\u03b7$-pseudospin coherence and suppress ODLRO. Our results establish a disorder-tolerant route to stabilizing superconducting order as a non-thermal attractor via minimal local quantum-jump control.", "AI": {"tldr": "A minimal local dissipation protocol using a rotated quantum jump operator on a single site can autonomously drive a strongly correlated Hubbard model into a robust nonequilibrium steady state with macroscopic superconducting order, resilient to certain disorders.", "motivation": "To determine whether superconducting order can be engineered as a robust attractor in open quantum systems, providing a method to stabilize quantum coherence against environmental decoherence in strongly correlated materials.", "method": "Proposes a dissipation-engineering protocol using Lindblad dynamics with a locally transformed \u03b7-pair lowering operator applied to a single lattice site, creating autonomous pumping into a nonequilibrium steady state.", "result": "Demonstrates local-to-global synchronization: a single-site dissipative seed establishes macroscopic \u03b7-pair off-diagonal long-range order across the entire system via dark-state selection and Liouvillian invariant-subspace structure with an attractive fixed point.", "conclusion": "Superconducting order can be stabilized as a disorder-tolerant, non-thermal attractor through minimal local quantum-jump control, offering a robust route to engineering quantum states in open systems."}}
{"id": "2602.00287", "categories": ["quant-ph", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2602.00287", "abs": "https://arxiv.org/abs/2602.00287", "authors": ["Qiuyuan Wang", "Aravind Karthigeyan", "Chung-Tao Chou", "Luqiao Liu"], "title": "A room-temperature cavity-magnonic source of correlated microwave pairs", "comment": null, "summary": "Correlated microwave photon sources are key enablers for technologies in quantum-limited sensing, signal amplification and communication, but the reliance on millikelvin operating temperature limits their scalability for broader applications. Here, at room temperature, we demonstrate strong correlated microwave signals emitted from a hybrid magnon-photon platform. Different from traditional parametrically induced magnons with degenerate frequencies, we achieve non-degenerate excitations by coupling magnon modes simultaneously with two cavity photon modes. Through the magnon-photon interactions in the corresponding linear and nonlinear regimes, one input microwave photon splits into a pair of magnon polaritons that possess distinct frequencies but maintain strong inter-mode correlations. The nonlinear magnon polariton dynamics empowered by this new parametric platform brings both verified true randomness and robust multi-channel correlations, from which we construct a microwave communication experiment for noise resilient signal transmission with added security. This work establishes cavity magnonics as a versatile and compact platform for generating correlated multi-mode microwave signals, opening new avenues for applications in classical and quantum domains.", "AI": {"tldr": "\u5728\u5ba4\u6e29\u4e0b\u5229\u7528\u6df7\u5408\u78c1\u5b50-\u5149\u5b50\u5e73\u53f0\u5b9e\u73b0\u5f3a\u5173\u8054\u5fae\u6ce2\u4fe1\u53f7\uff0c\u901a\u8fc7\u975e\u7b80\u5e76\u6fc0\u53d1\u4ea7\u751f\u5177\u6709\u5f3a\u5173\u8054\u7684\u53cc\u6a21\u78c1\u6781\u5316\u6fc0\u5143\uff0c\u4e3a\u91cf\u5b50\u4f20\u611f\u548c\u5b89\u5168\u901a\u4fe1\u63d0\u4f9b\u65b0\u65b9\u6848", "motivation": "\u4f20\u7edf\u5173\u8054\u5fae\u6ce2\u5149\u5b50\u6e90\u9700\u6beb\u5f00\u5c14\u6587\u4f4e\u6e29\u73af\u5883\uff0c\u4e25\u91cd\u9650\u5236\u5176\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u9645\u5e94\u7528\uff0c\u4e9f\u9700\u5728\u5ba4\u6e29\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fd\u5173\u8054\u5fae\u6ce2\u6e90", "method": "\u901a\u8fc7\u540c\u65f6\u5c06\u78c1\u5b50\u6a21\u5f0f\u4e0e\u4e24\u4e2a\u8154\u5149\u5b50\u6a21\u5f0f\u8026\u5408\uff0c\u5b9e\u73b0\u975e\u7b80\u5e76\u78c1\u5b50\u6fc0\u53d1\uff0c\u5728\u78c1\u5b50-\u5149\u5b50\u7ebf\u6027\u4e0e\u975e\u7ebf\u6027\u76f8\u4e92\u4f5c\u7528\u533a\u4ea7\u751f\u5206\u88c2\u7684\u5173\u8054\u78c1\u6781\u5316\u6fc0\u5143\u5bf9", "result": "\u9a8c\u8bc1\u4e86\u771f\u968f\u673a\u6027\u548c\u591a\u901a\u9053\u5f3a\u5173\u8054\u7279\u6027\uff0c\u6210\u529f\u6784\u5efa\u566a\u58f0\u9c81\u68d2\u4e14\u5177\u5907\u5b89\u5168\u6027\u7684\u5fae\u6ce2\u901a\u4fe1\u5b9e\u9a8c\uff0c\u5b9e\u73b0\u5ba4\u6e29\u4e0b\u591a\u6a21\u5173\u8054\u5fae\u6ce2\u4fe1\u53f7\u751f\u6210", "conclusion": "\u8be5\u5de5\u4f5c\u786e\u7acb\u8154\u78c1\u5b50\u5b66\u4f5c\u4e3a\u7d27\u51d1\u578b\u591a\u529f\u80fd\u5e73\u53f0\uff0c\u4e3a\u7ecf\u5178\u4e0e\u91cf\u5b50\u6280\u672f\u9886\u57df\u7684\u5173\u8054\u5fae\u6ce2\u5e94\u7528\u5f00\u8f9f\u65b0\u9014\u5f84"}}
{"id": "2602.00063", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00063", "abs": "https://arxiv.org/abs/2602.00063", "authors": ["Leonidas Christodoulou", "Chang Sun"], "title": "The Impact of Machine Learning Uncertainty on the Robustness of Counterfactual Explanations", "comment": null, "summary": "Counterfactual explanations are widely used to interpret machine learning predictions by identifying minimal changes to input features that would alter a model's decision. However, most existing counterfactual methods have not been tested when model and data uncertainty change, resulting in explanations that may be unstable or invalid under real-world variability. In this work, we investigate the robustness of common combinations of machine learning models and counterfactual generation algorithms in the presence of both aleatoric and epistemic uncertainty. Through experiments on synthetic and real-world tabular datasets, we show that counterfactual explanations are highly sensitive to model uncertainty. In particular, we find that even small reductions in model accuracy - caused by increased noise or limited data - can lead to large variations in the generated counterfactuals on average and on individual instances. These findings underscore the need for uncertainty-aware explanation methods in domains such as finance and the social sciences.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u4e0d\u786e\u5b9a\u6027\uff08\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff09\u4e0b\uff0c\u53cd\u4e8b\u5b9e\u89e3\u91ca\u7684\u9c81\u68d2\u6027\u95ee\u9898\u3002\u901a\u8fc7\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u8868\u683c\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u53d1\u73b0\uff0c\u5373\u4f7f\u6a21\u578b\u7cbe\u5ea6\u7684\u5c0f\u5e45\u4e0b\u964d\u4e5f\u4f1a\u5bfc\u81f4\u53cd\u4e8b\u5b9e\u89e3\u91ca\u4ea7\u751f\u5de8\u5927\u53d8\u5316\uff0c\u5f3a\u8c03\u4e86\u5728\u91d1\u878d\u548c\u793e\u4f1a\u79d1\u5b66\u7b49\u9886\u57df\u5f00\u53d1\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u89e3\u91ca\u65b9\u6cd5\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u73b0\u6709\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\u867d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u672a\u5728\u6a21\u578b\u548c\u6570\u636e\u4e0d\u786e\u5b9a\u6027\u6761\u4ef6\u4e0b\u8fdb\u884c\u5145\u5206\u6d4b\u8bd5\uff0c\u5bfc\u81f4\u5176\u5728\u771f\u5b9e\u4e16\u754c\u591a\u53d8\u73af\u5883\u4e2d\u53ef\u80fd\u4ea7\u751f\u4e0d\u7a33\u5b9a\u6216\u65e0\u6548\u7684\u89e3\u91ca\uff0c\u65e0\u6cd5\u6ee1\u8db3\u91d1\u878d\u548c\u793e\u4f1a\u79d1\u5b66\u7b49\u9ad8\u53ef\u9760\u6027\u8981\u6c42\u9886\u57df\u7684\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u8868\u683c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6d4b\u8bd5\u5e38\u89c1\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e0e\u53cd\u4e8b\u5b9e\u751f\u6210\u7b97\u6cd5\u7684\u7ec4\u5408\u5728\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u5b58\u5728\u65f6\u7684\u9c81\u68d2\u6027\u8868\u73b0\u3002", "result": "\u53cd\u4e8b\u5b9e\u89e3\u91ca\u5bf9\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u9ad8\u5ea6\u654f\u611f\uff0c\u5373\u4f7f\u662f\u7531\u566a\u58f0\u589e\u52a0\u6216\u6570\u636e\u6709\u9650\u5bfc\u81f4\u7684\u6a21\u578b\u7cbe\u5ea6\u5c0f\u5e45\u4e0b\u964d\uff0c\u4e5f\u4f1a\u5f15\u8d77\u751f\u6210\u53cd\u4e8b\u5b9e\u7684\u5e73\u5747\u503c\u548c\u5355\u4e2a\u5b9e\u4f8b\u51fa\u73b0\u5927\u5e45\u6ce2\u52a8\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5f00\u53d1\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u89e3\u91ca\u65b9\u6cd5\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u91d1\u878d\u548c\u793e\u4f1a\u79d1\u5b66\u7b49\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u4ee5\u786e\u4fdd\u89e3\u91ca\u7684\u53ef\u9760\u6027\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2602.00400", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00400", "abs": "https://arxiv.org/abs/2602.00400", "authors": ["Fan Yang", "Rui Meng", "Trudi Di Qi", "Ali Ezzati", "Yuxin Wen"], "title": "KEPO: Knowledge-Enhanced Preference Optimization for Reinforcement Learning with Reasoning", "comment": null, "summary": "Reinforcement learning (RL) has emerged as a promising paradigm for inducing explicit reasoning behaviors in large language and vision-language models. However, reasoning-oriented RL post-training remains fundamentally challenging due to sparse trajectory-level rewards, leading to ambiguous credit assignment and severe exploration failures that can trap the policy in a ``learning cliff.'' Recent on-policy distillation methods introduce dense teacher supervision to stabilize optimization, but apply it uniformly across all generated trajectories. We argue that such uniform distillation is ill-suited for reasoning-intensive tasks, as low-quality on-policy trajectories often originate from early logical errors, and distillation under flawed contexts injects noisy and misaligned gradients. To address these challenges, we propose Knowledge-Enhanced Preference Optimization (KEPO), a unified post-training framework that integrates: (i) a quality-gated on-policy distillation objective that selectively applies dense teacher guidance only to high-quality trajectories, and (ii) a knowledge-enhanced exploration strategy that leverages hints learned from a teacher model to rejectively sample reward-positive on-policy trajectories for RL, thereby mitigating exploration collapse. Evaluated on a challenging medical visual question answering benchmark under single-source generalization, KEPO demonstrates improved training stability, more coherent reasoning behaviors, and superior out-of-distribution performance over reinforcement learning and on-policy distillation baselines.", "AI": {"tldr": "This paper proposes KEPO, a unified post-training framework that integrates quality-gated on-policy distillation and knowledge-enhanced exploration to address sparse rewards and exploration failures in reasoning-oriented RL for language models, demonstrating improved stability and performance on medical VQA.", "motivation": "Reasoning-oriented RL post-training is challenging due to sparse trajectory-level rewards causing ambiguous credit assignment and severe exploration failures (\"learning cliff\"). Uniform on-policy distillation methods are ill-suited as they apply supervision to low-quality trajectories from early logical errors, injecting noisy gradients.", "method": "Proposes Knowledge-Enhanced Preference Optimization (KEPO) with two key components: (i) a quality-gated on-policy distillation objective that selectively applies dense teacher guidance only to high-quality trajectories, and (ii) a knowledge-enhanced exploration strategy that leverages teacher model hints to rejectively sample reward-positive trajectories for RL.", "result": "Evaluated on a challenging medical visual question answering benchmark under single-source generalization, KEPO demonstrates improved training stability, more coherent reasoning behaviors, and superior out-of-distribution performance over RL and on-policy distillation baselines.", "conclusion": "KEPO provides a unified post-training framework that effectively addresses the fundamental challenges of sparse rewards and exploration collapse in reasoning-oriented RL, leading to more stable training and better generalization performance."}}
{"id": "2602.02460", "categories": ["cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.02460", "abs": "https://arxiv.org/abs/2602.02460", "authors": ["J. Roberto Viana", "Octavio D. Rodriguez Salmon", "Minos A. Neto", "Griffith Mendon\u00e7a", "F. Din\u00f3la Neto"], "title": "Resolution of the Two-Dimensional Ferromagnetic Spin-3/2 Ising Model via Cluster Growth", "comment": "Twelve figures, 29 pages", "summary": "We propose a computational methodology based on a hierarchical cluster growth process to solve spin-3/2 Ising models efficiently. The method circumvents the exponential complexity (\\(4^{N}\\)) of the canonical ensemble partition function by iteratively constructing finite magnetic clusters of size \\(N_g\\), where the effective spin state of a site in generation \\(g+1\\) is determined by the local magnetization of a cluster from generation \\(g\\). This approach, which shares conceptual ground with effective field theories, allows the study of systems of effectively very large size \\(N = N_0 (N_g)^{g}\\). We apply the formalism to the ferromagnetic spin-3/2 Ising model on a honeycomb lattice, modeling the monolayer CrI$_3$, a prototypical two-dimensional Ising magnet. The model, calibrated using the experimental transition temperature (\\(T_{c} \\simeq 45\\) K), successfully reproduces key experimental features: the temperature dependence of the magnetization \\(m(T)\\), including its inflection point, and the broadened peak in the specific heat \\(c_v(T)\\). We also compute the entropy \\(s(T)\\), finding a finite residual value at low temperatures consistent with the system's double degeneracy. Our results demonstrate that this hierarchical cluster method provides a quantitatively accurate and computationally efficient framework for studying complex magnetic systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5206\u5c42\u7c07\u589e\u957f\u8ba1\u7b97\u6cd5\u9ad8\u6548\u6c42\u89e3\u81ea\u65cb3/2\u4f0a\u8f9b\u6a21\u578b\uff0c\u901a\u8fc7\u8fed\u4ee3\u6784\u5efa\u6709\u9650\u78c1\u7c07\u89c4\u907f\u6307\u6570\u590d\u6742\u5ea6\uff0c\u6210\u529f\u6a21\u62df\u5355\u5c42CrI\u2083\u7684\u5173\u952e\u78c1\u5b66\u7279\u5f81", "motivation": "\u89e3\u51b3\u81ea\u65cb3/2\u4f0a\u8f9b\u6a21\u578b\u6807\u51c6\u914d\u5206\u51fd\u6570\u7684\u6307\u6570\u7ea7\u8ba1\u7b97\u590d\u6742\u5ea6(4^N)\u95ee\u9898\uff0c\u4ee5\u9ad8\u6548\u7814\u7a76\u5927\u5c3a\u5ea6\u78c1\u6027\u7cfb\u7edf", "method": "\u91c7\u7528\u5206\u5c42\u7c07\u589e\u957f\u65b9\u6cd5\uff1a\u8fed\u4ee3\u6784\u5efa\u5c3a\u5bf8N_g\u7684\u6709\u9650\u78c1\u7c07\uff0c\u901a\u8fc7\u524d\u4e00\u4ee3\u7c07\u7684\u5c40\u57df\u78c1\u5316\u5f3a\u5ea6\u786e\u5b9a\u4e0b\u4e00\u4ee3\u683c\u70b9\u6709\u6548\u81ea\u65cb\u6001\uff0c\u5b9e\u73b0\u6709\u6548\u5927\u7cfb\u7edf\u5c3a\u5bf8N=N\u2080(N_g)^g", "result": "\u5e94\u7528\u4e8e\u8702\u7a9d\u6676\u683c\u94c1\u78c1\u81ea\u65cb3/2\u6a21\u578b\uff08\u6a21\u62df\u5355\u5c42CrI\u2083\uff09\uff0c\u6821\u51c6\u5b9e\u9a8c\u5c45\u91cc\u6e29\u5ea6(Tc\u224845K)\u540e\uff0c\u7cbe\u786e\u91cd\u73b0\u78c1\u5316\u5f3a\u5ea6m(T)\u62d0\u70b9\u3001\u6bd4\u70ed\u5bb9c_v(T)\u5bbd\u5cf0\u53ca\u4f4e\u6e29\u6b8b\u4f59\u71b5\u7b49\u5173\u952e\u5b9e\u9a8c\u7279\u5f81", "conclusion": "\u8be5\u5206\u5c42\u7c07\u65b9\u6cd5\u4e3a\u590d\u6742\u78c1\u4f53\u7cfb\u63d0\u4f9b\u4e86\u517c\u5177\u5b9a\u91cf\u7cbe\u5ea6\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u7814\u7a76\u6846\u67b6\uff0c\u6709\u6548\u7a81\u7834\u4f20\u7edf\u65b9\u6cd5\u7684\u5c3a\u5ea6\u9650\u5236"}}
{"id": "2602.00367", "categories": ["quant-ph", "hep-th", "math-ph"], "pdf": "https://arxiv.org/pdf/2602.00367", "abs": "https://arxiv.org/abs/2602.00367", "authors": ["Anuar Kafuri"], "title": "Path integrals and deformation quantization:the fermionic case", "comment": null, "summary": "This thesis addresses a fundamental problem in deformation quantization: the difficulty of calculating the star-exponential, the symbol of the evolution operator, due to convergence issues. Inspired by the formalism that connects the star-exponential with the quantum propagator for bosonic systems, this work develops the analogous extension for the fermionic case. A rigorous method, based on Grassmann variables and coherent states, is constructed to obtain a closed-form expression for the fermionic star-exponential from its associated propagator. As a primary application, a fermionic version of the Feynman-Kac formula is derived within this formalism, allowing for the calculation of the ground state energy directly in phase space. Finally, the method is validated by successfully applying it to the simple and driven harmonic oscillators, where it is demonstrated that a simplified (\"naive\") approach (with an ad-hoc \"remediation\") is a valid weak-coupling limit of the rigorous (\"meticulous\") formalism, thereby providing a new and powerful computational tool for the study of fermionic systems.", "AI": {"tldr": "This thesis develops a rigorous method using Grassmann variables and coherent states to calculate the fermionic star-exponential, deriving a fermionic Feynman-Kac formula and validating that a simplified approach is a valid weak-coupling limit, offering a new computational tool for fermionic systems.", "motivation": "Deformation quantization faces challenges in calculating the star-exponential due to convergence issues; extending the bosonic propagator-star-exponential connection to fermions would provide powerful computational methods.", "method": "Constructs a rigorous formalism using Grassmann variables and coherent states to derive a closed-form expression for the fermionic star-exponential from its quantum propagator.", "result": "Derives a fermionic Feynman-Kac formula for ground state energy calculation in phase space, and validates the method on harmonic oscillators, showing the naive approach is a valid weak-coupling limit.", "conclusion": "The work provides a new and powerful computational framework for studying fermionic systems in deformation quantization."}}
{"id": "2602.00405", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00405", "abs": "https://arxiv.org/abs/2602.00405", "authors": ["Deep Gandhi", "Katyani Singh", "Nidhi Hegde"], "title": "RobustDebias: Debiasing Language Models using Distributionally Robust Optimization", "comment": null, "summary": "Pretrained language models have been shown to exhibit biases and social stereotypes. Prior work on debiasing these models has largely focused on modifying embedding spaces during pretraining, which is not scalable for large models. Fine-tuning pretrained models on task-specific datasets can both degrade model performance and amplify biases present in the fine-tuning data. We address bias amplification during fine-tuning rather than costly pretraining, focusing on BERT models due to their widespread use in language understanding tasks. While Empirical Risk Minimization effectively optimizes downstream performance, it often amplifies social biases during fine-tuning. To counter this, we propose \\textit{RobustDebias}, a novel mechanism which adapts Distributionally Robust Optimization (DRO) to debias language models during fine-tuning. Our approach debiases models across multiple demographics during MLM fine-tuning and generalizes to any dataset or task. Extensive experiments on various language models show significant bias mitigation with minimal performance impact.", "AI": {"tldr": "\u63d0\u51faRobustDebias\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5e03\u9c81\u68d2\u4f18\u5316(DRO)\u5728BERT\u6a21\u578b\u5fae\u8c03\u9636\u6bb5\u51cf\u8f7b\u504f\u89c1\u653e\u5927\u95ee\u9898\uff0c\u5b9e\u73b0\u663e\u8457\u504f\u89c1\u7f13\u89e3\u4e14\u6027\u80fd\u635f\u5931\u6781\u5c0f\u3002", "motivation": "\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u504f\u89c1\u548c\u793e\u4f1a\u523b\u677f\u5370\u8c61\uff0c\u73b0\u6709\u53bb\u504f\u65b9\u6cd5\u96c6\u4e2d\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u4fee\u6539\u5d4c\u5165\u7a7a\u95f4\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u5927\u578b\u6a21\u578b\uff1b\u800c\u5fae\u8c03\u4f1a\u964d\u4f4e\u6027\u80fd\u5e76\u653e\u5927\u6570\u636e\u4e2d\u7684\u504f\u89c1\uff0c\u9700\u8981\u5728\u5fae\u8c03\u9636\u6bb5\u800c\u975e\u6602\u8d35\u7684\u9884\u8bad\u7ec3\u9636\u6bb5\u89e3\u51b3\u504f\u89c1\u653e\u5927\u95ee\u9898\u3002", "method": "\u63d0\u51faRobustDebias\u673a\u5236\uff0c\u5c06\u5206\u5e03\u9c81\u68d2\u4f18\u5316(DRO)\u9002\u914d\u5230\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u4e2d\uff0c\u9488\u5bf9\u5e7f\u6cdb\u4f7f\u7528\u7684BERT\u6a21\u578b\uff0c\u5728MLM\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u8de8\u591a\u4e2a demographic \u7fa4\u4f53\u8fdb\u884c\u53bb\u504f\uff0c\u53ef\u6cdb\u5316\u5230\u4efb\u610f\u6570\u636e\u96c6\u6216\u4efb\u52a1\u3002", "result": "\u5728\u5404\u79cd\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u4ee5\u6781\u5c0f\u7684\u6027\u80fd\u4ee3\u4ef7\u5b9e\u73b0\u663e\u8457\u7684\u504f\u89c1\u7f13\u89e3\u6548\u679c\u3002", "conclusion": "RobustDebias\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728\u5fae\u8c03\u9636\u6bb5\u53bb\u504f\u7684\u6709\u6548\u53ef\u6269\u5c55\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u504f\u89c1\u653e\u5927\u95ee\u9898\uff0c\u907f\u514d\u4e86\u4fee\u6539\u9884\u8bad\u7ec3\u7684\u9ad8\u6602\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2602.00369", "categories": ["quant-ph", "cond-mat.quant-gas"], "pdf": "https://arxiv.org/pdf/2602.00369", "abs": "https://arxiv.org/abs/2602.00369", "authors": ["Carlos Diaz-Mejia", "Sergio Lerma-Hernandez", "Jorge G. Hirsch"], "title": "Dynamical witnesses and universal behavior across chaos and non-ergodicity in the tilted Bose-Hubbard model", "comment": "12 pages, 11 figure", "summary": "Quantum chaos in isolated quantum systems is intimately linked to thermalization and the rapid relaxation of observables. Although the spectral properties of the chaotic phase in the tilted Bose-Hubbard model have been well characterized, the corresponding dynamical signatures across the transition to regularity remain less explored . In this work, we investigate this transition by analyzing the time evolution of the survival probability, the single-site entanglement entropy, and the half-chain imbalance. Our results reveal a clear hierarchy in the sensitivity of these observables: the relaxation value of the entanglement entropy varies smoothly as a function of the Hamiltonian parameters across the chaos-regular transition, while the imbalance exhibits a more pronounced distinction. Most notably, the survival probability emerges as the most robust indicator of the transition between chaos and regularity. When appropriately scaled, all three observables converge onto a common behavior as a function of the Hamiltonian parameters for different numbers of sites and bosons,enabling a universal characterization of the transition between chaotic and regular dynamics.", "AI": {"tldr": "This paper investigates the transition from quantum chaos to regularity in the tilted Bose-Hubbard model by analyzing survival probability, entanglement entropy, and imbalance, revealing survival probability as the most robust indicator and demonstrating universal scaling behavior across system sizes.", "motivation": "While spectral properties of quantum chaos in the tilted Bose-Hubbard model are well understood, the dynamical signatures during the transition to regularity remain underexplored, necessitating this study to identify robust indicators of the chaos-regular transition.", "method": "The authors analyze time evolution of three key observables\u2014survival probability, single-site entanglement entropy, and half-chain imbalance\u2014across the chaos-regular transition in the tilted Bose-Hubbard model.", "result": "A clear sensitivity hierarchy emerges: entanglement entropy varies smoothly, imbalance shows pronounced distinction, and survival probability is the most robust transition indicator. When scaled, all three observables exhibit universal behavior across different system sizes.", "conclusion": "Survival probability serves as the most reliable marker for the chaos-regular transition, and the universal scaling of all observables enables a unified characterization of this dynamical transition."}}
{"id": "2602.00067", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00067", "abs": "https://arxiv.org/abs/2602.00067", "authors": ["Yihan Zhang", "Ercan E. Kuruoglu"], "title": "Modality as Heterogeneity: Node Splitting and Graph Rewiring for Multimodal Graph Learning", "comment": null, "summary": "Multimodal graphs are gaining increasing attention due to their rich representational power and wide applicability, yet they introduce substantial challenges arising from severe modality confusion. To address this issue, we propose NSG (Node Splitting Graph)-MoE, a multimodal graph learning framework that integrates a node-splitting and graph-rewiring mechanism with a structured Mixture-of-Experts (MoE) architecture. It explicitly decomposes each node into modality-specific components and assigns relation-aware experts to process heterogeneous message flows, thereby preserving structural information and multimodal semantics while mitigating the undesirable mixing effects commonly observed in general-purpose GNNs. Extensive experiments on three multimodal benchmarks demonstrate that NSG-MoE consistently surpasses strong baselines. Despite incorporating MoE -- which is typically computationally heavy -- our method achieves competitive training efficiency. Beyond empirical results, we provide a spectral analysis revealing that NSG performs adaptive filtering over modality-specific subspaces, thus explaining its disentangling behavior. Furthermore, an information-theoretic analysis shows that the architectural constraints imposed by NSG reduces mutual information between data and parameters and improving generalization capability.", "AI": {"tldr": "NSG-MoE\u901a\u8fc7\u8282\u70b9\u62c6\u5206\u548c\u4e13\u5bb6\u8def\u7531\u89e3\u51b3\u591a\u6a21\u6001\u56fe\u5b66\u4e60\u7684\u6a21\u6001\u6df7\u6dc6\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u7684\u540c\u65f6\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u591a\u6a21\u6001\u56fe\u867d\u8868\u8fbe\u80fd\u529b\u4e30\u5bcc\uff0c\u4f46\u5b58\u5728\u4e25\u91cd\u7684\u6a21\u6001\u6df7\u6dc6\u95ee\u9898\uff0c\u5bfc\u81f4\u901a\u7528GNN\u4ea7\u751f\u4e0d\u826f\u6df7\u5408\u6548\u5e94\u3002", "method": "\u63d0\u51faNSG-MoE\u6846\u67b6\uff0c\u5c06\u8282\u70b9\u5206\u89e3\u4e3a\u6a21\u6001\u7279\u5b9a\u7ec4\u4ef6\uff0c\u7ed3\u5408\u56fe\u91cd\u8fde\u673a\u5236\u548c\u7ed3\u6784\u5316MoE\u67b6\u6784\uff0c\u4f7f\u7528\u5173\u7cfb\u611f\u77e5\u4e13\u5bb6\u5904\u7406\u5f02\u6784\u6d88\u606f\u6d41\u3002", "result": "\u5728\u4e09\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u6301\u7eed\u8d85\u8d8a\u5f3a\u57fa\u7ebf\uff0c\u8bad\u7ec3\u6548\u7387\u5177\u6709\u7ade\u4e89\u529b\uff1b\u8c31\u5206\u6790\u663e\u793a\u5176\u5bf9\u6a21\u6001\u7279\u5b9a\u5b50\u7a7a\u95f4\u8fdb\u884c\u81ea\u9002\u5e94\u6ee4\u6ce2\uff0c\u4fe1\u606f\u8bba\u5206\u6790\u8868\u660e\u5176\u964d\u4f4e\u6570\u636e\u4e0e\u53c2\u6570\u7684\u4e92\u4fe1\u606f\u4ee5\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u67b6\u6784\u7ea6\u675f\u6709\u6548\u7f13\u89e3\u6a21\u6001\u6df7\u6dc6\uff0c\u540c\u65f6\u4fdd\u7559\u7ed3\u6784\u4fe1\u606f\u548c\u591a\u6a21\u6001\u8bed\u4e49\uff0c\u517c\u5177\u5b9e\u8bc1\u6548\u679c\u548c\u7406\u8bba\u652f\u6491\u3002"}}
{"id": "2602.00072", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.00072", "abs": "https://arxiv.org/abs/2602.00072", "authors": ["Jice Zeng", "David Barajas-Solano", "Hui Chen"], "title": "Generative AI-enhanced Probabilistic Multi-Fidelity Surrogate Modeling Via Transfer Learning", "comment": null, "summary": "The performance of machine learning surrogates is critically dependent on data quality and quantity. This presents a major challenge, as high-fidelity (HF) data is often scarce and computationally expensive to acquire, while low-fidelity (LF) data is abundant but less accurate. To address this data scarcity problem, we develop a probabilistic multi-fidelity surrogate framework based on generative transfer learning. We employ a normalizing flow (NF) generative model as the backbone, which is trained in two phases: (i) the NF is first pretrained on a large LF dataset to learn a probabilistic forward model; (ii) the pretrained model is then fine-tuned on a small HF dataset, allowing it to correct for LF-HF discrepancies via knowledge transfer. To relax the dimension-preserving constraint of standard bijective NFs, we integrate surjective (dimension-reducing) layers with standard coupling blocks. This architecture enables learned dimension reduction while preserving the ability to train with exact likelihoods. The resulting surrogate provides fast probabilistic predictions with quantified uncertainty and significantly outperforms LF-only baselines while using fewer HF evaluations. We validate the approach on a reinforced concrete slab benchmark, combining many coarse-mesh (LF) simulations with a limited set of fine-mesh (HF) simulations. The proposed model achieves probabilistic predictions with HF accuracy, demonstrating a practical path toward data-efficient, generative AI-driven surrogates for complex engineering systems.", "AI": {"tldr": "A generative transfer learning framework using normalizing flows with surjective layers combines abundant low-fidelity data with scarce high-fidelity data to create accurate probabilistic surrogates with quantified uncertainty, validated on a reinforced concrete slab benchmark.", "motivation": "High-fidelity data is scarce and computationally expensive while low-fidelity data is abundant but inaccurate, creating a critical data scarcity challenge for machine learning surrogates.", "method": "Probabilistic multi-fidelity surrogate framework using normalizing flow generative models trained in two phases: pretraining on large LF datasets followed by fine-tuning on small HF datasets, with integrated surjective layers to enable learned dimension reduction while preserving exact likelihood training.", "result": "Achieves fast probabilistic predictions with quantified uncertainty, significantly outperforms LF-only baselines, uses fewer HF evaluations, and attains HF-level accuracy on a reinforced concrete slab benchmark combining coarse-mesh and fine-mesh simulations.", "conclusion": "Demonstrates a practical path toward data-efficient, generative AI-driven surrogates for complex engineering systems."}}
{"id": "2602.00449", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00449", "abs": "https://arxiv.org/abs/2602.00449", "authors": ["Jia Liang", "Liangming Pan"], "title": "Do Latent-CoT Models Think Step-by-Step? A Mechanistic Study on Sequential Reasoning Tasks", "comment": "20 pages, 14 figures", "summary": "Latent Chain-of-Thought (Latent-CoT) aims to enable step-by-step computation without emitting long rationales, yet its mechanisms remain unclear. We study CODI, a continuous-thought teacher-student distillation model, on strictly sequential polynomial-iteration tasks. Using logit-lens decoding, linear probes, attention analysis, and activation patching, we localize intermediate-state representations and trace their routing to the final readout. On two- and three-hop tasks, CODI forms the full set of bridge states that become decodable across latent-thought positions, while the final input follows a separate near-direct route; predictions arise via late fusion at the end-of-thought boundary. For longer hop lengths, CODI does not reliably execute a full latent rollout, instead exhibiting a partial latent reasoning path that concentrates on late intermediates and fuses them with the last input at the answer readout position. Ablations show that this partial pathway can collapse under regime shifts, including harder optimization. Overall, we delineate when CODI-style latent-CoT yields faithful iterative computation versus compressed or shortcut strategies, and highlight challenges in designing robust latent-CoT objectives for sequential reasoning.", "AI": {"tldr": "\u7814\u7a76CODI\u6f5c\u601d\u7ef4\u6a21\u578b\u5728\u77ed\u4efb\u52a1\u4e2d\u4f1a\u5b8c\u6574\u8ba1\u7b97\u4e2d\u95f4\u72b6\u6001\uff0c\u4f46\u5728\u957f\u4efb\u52a1\u4e2d\u4f1a\u8d70\u6377\u5f84\u538b\u7f29\u63a8\u7406\u8def\u5f84\uff0c\u63ed\u793a\u6f5c\u601d\u7ef4\u63a8\u7406\u7684\u53ef\u9760\u6027\u95ee\u9898", "motivation": "\u63a2\u7a76Latent-CoT\u6a21\u578b\u662f\u5426\u771f\u6b63\u6267\u884c\u9010\u6b65\u8ba1\u7b97\uff0c\u8fd8\u662f\u4f9d\u8d56\u6377\u5f84\u7b56\u7565\uff0c\u8fd9\u5bf9\u8bbe\u8ba1\u9c81\u68d2\u7684\u63a8\u7406\u6a21\u578b\u81f3\u5173\u91cd\u8981", "method": "\u5728\u591a\u9879\u5f0f\u8fed\u4ee3\u4efb\u52a1\u4e0a\uff0c\u8fd0\u7528logit-lens\u89e3\u7801\u3001\u7ebf\u6027\u63a2\u6d4b\u3001\u6ce8\u610f\u529b\u5206\u6790\u548c\u6fc0\u6d3b\u4fee\u8865\u7b49\u6280\u672f\uff0c\u8ffd\u8e2a\u4e2d\u95f4\u72b6\u6001\u8868\u793a\u53ca\u5176\u8def\u7531\u8def\u5f84", "result": "2-3\u8df3\u4efb\u52a1\uff1aCODI\u5f62\u6210\u53ef\u89e3\u7801\u7684\u6865\u63a5\u72b6\u6001\uff0c\u6700\u7ec8\u9884\u6d4b\u5728\u601d\u7ef4\u8fb9\u754c\u5904\u665a\u878d\u5408\uff1b\u66f4\u957f\u4efb\u52a1\uff1aCODI\u65e0\u6cd5\u5b8c\u6574\u5c55\u5f00\u6f5c\u601d\u7ef4\uff0c\u4ec5\u5173\u6ce8\u540e\u671f\u4e2d\u95f4\u72b6\u6001\u5e76\u4e0e\u6700\u540e\u8f93\u5165\u878d\u5408\uff0c\u8be5\u7b56\u7565\u5728\u4f18\u5316\u56f0\u96be\u65f6\u6613\u5d29\u6e83", "conclusion": "CODI\u7c7b\u6f5c\u601d\u7ef4\u6a21\u578b\u5728\u957f\u63a8\u7406\u94fe\u4e0a\u65e0\u6cd5\u4fdd\u8bc1\u5fe0\u5b9e\u8ba1\u7b97\uff0c\u5b58\u5728\u538b\u7f29\u6377\u5f84\u98ce\u9669\uff0c\u7ed9\u8bbe\u8ba1\u9c81\u68d2\u7684\u6f5c\u601d\u7ef4\u76ee\u6807\u51fd\u6570\u5e26\u6765\u6311\u6218"}}
{"id": "2602.00468", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.00468", "abs": "https://arxiv.org/abs/2602.00468", "authors": ["Hongwei Yu", "Mingzhu Weng", "Zhihai Wang", "Jin Wang"], "title": "Liouvillian gap closing--bound states in the continuum connection and diverse dynamics in a giant-atom waveguide QED setup", "comment": "10 Pages, 4 Figures, Comments are welcomed", "summary": "In open quantum systems, reduced dynamics is commonly described by a master equation, whose Liouvillian gap closing (LGC) typically signals the emergence of decoherence-free subspace. By contrast, the dynamics of the full system-environment compound is governed by the underlying Hamiltonian spectrum, where bound states in the continuum (BICs) can protect long-lived quantum resources. Despite these parallel perspectives, the relation between LGC and BIC formation has remained largely unexplored. Here we bridge this gap in a paradigmatic giant-atom waveguide platform and show that the occurrence of LGC necessarily benchmarks the presence of a BIC in the full Hamiltonian description. By engineering the giant-atom geometry, we further demonstrate rich dynamical regimes-including Rabi oscillations, fractional decay, and complete exponential relaxation-depending on the number of supported BICs, which can be tuned from three to zero. Remarkably, when two BICs become frequency-degenerate, the long-time dynamics approaches a steady state rather than exhibiting persistent oscillations. Our results establish a direct spectral-dynamical connection between effective Markovian and underlying non-Markovian descriptions, and provide a route toward flexible control of open-system dynamics.", "AI": {"tldr": "\u672c\u7814\u7a76\u5728\u5de8\u578b\u539f\u5b50\u6ce2\u5bfc\u5e73\u53f0\u4e0a\u5efa\u7acb\u4e86\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u4e2dLiouvillian\u80fd\u9699\u95ed\u5408(LGC)\u4e0e\u5168\u54c8\u5bc6\u987f\u91cf\u63cf\u8ff0\u4e2d\u8fde\u7eed\u8c31\u675f\u7f1a\u6001(BICs)\u4e4b\u95f4\u7684\u76f4\u63a5\u8054\u7cfb\uff0c\u8bc1\u660eLGC\u5fc5\u7136\u6807\u5fd7\u7740BIC\u7684\u5b58\u5728\uff0c\u5e76\u901a\u8fc7\u8c03\u63a7BIC\u6570\u91cf\u5b9e\u73b0\u4e86\u4e30\u5bcc\u7684\u52a8\u529b\u5b66\u884c\u4e3a\u63a7\u5236\u3002", "motivation": "\u5728\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u4e2d\uff0c\u7ea6\u5316\u52a8\u529b\u5b66\u7684\u4e3b\u65b9\u7a0b\u63cf\u8ff0(LGC\u6807\u5fd7\u9000\u76f8\u5e72\u81ea\u7531\u5b50\u7a7a\u95f4)\u4e0e\u7cfb\u7edf-\u73af\u5883\u6574\u4f53\u54c8\u5bc6\u987f\u91cf\u63cf\u8ff0(BICs\u4fdd\u62a4\u957f\u5bff\u547d\u91cf\u5b50\u8d44\u6e90)\u662f\u4e24\u79cd\u5e73\u884c\u89c6\u89d2\uff0c\u4f46LGC\u4e0eBIC\u5f62\u6210\u4e4b\u95f4\u7684\u5173\u7cfb\u5c1a\u672a\u88ab\u63a2\u7d22\u3002", "method": "\u91c7\u7528\u5de8\u578b\u539f\u5b50\u6ce2\u5bfc\u5e73\u53f0\uff0c\u901a\u8fc7\u8bbe\u8ba1\u5de8\u578b\u539f\u5b50\u51e0\u4f55\u7ed3\u6784\uff0c\u7cfb\u7edf\u7814\u7a76\u6709\u6548\u9a6c\u5c14\u53ef\u592b\u63cf\u8ff0\u4e0e\u5e95\u5c42\u975e\u9a6c\u5c14\u53ef\u592b\u63cf\u8ff0\u4e4b\u95f4\u7684\u8c31\u52a8\u529b\u5b66\u8054\u7cfb\u3002", "result": "\u53d1\u73b0LGC\u5fc5\u7136\u5bf9\u5e94\u5168\u54c8\u5bc6\u987f\u91cf\u4e2d\u7684BIC\u5b58\u5728\uff1b\u901a\u8fc7\u8c03\u63a7\u652f\u6301\u7684BIC\u6570\u91cf(\u4ece\u4e09\u4e2a\u5230\u96f6)\uff0c\u89c2\u6d4b\u5230\u5305\u62ec\u62c9\u6bd4\u632f\u8361\u3001\u5206\u6570\u8870\u51cf\u548c\u5b8c\u5168\u6307\u6570\u5f1b\u8c6b\u5728\u5185\u7684\u4e30\u5bcc\u52a8\u529b\u5b66 regime\uff1b\u5f53\u4e24\u4e2aBIC\u7b80\u5e76\u65f6\uff0c\u957f\u65f6\u95f4\u52a8\u529b\u5b66\u8d8b\u4e8e\u7a33\u6001\u800c\u975e\u6301\u7eed\u632f\u8361\u3002", "conclusion": "\u5efa\u7acb\u4e86\u9a6c\u5c14\u53ef\u592b\u4e0e\u975e\u9a6c\u5c14\u53ef\u592b\u63cf\u8ff0\u4e4b\u95f4\u7684\u76f4\u63a5\u8c31\u52a8\u529b\u5b66\u8054\u7cfb\uff0c\u4e3a\u7075\u6d3b\u63a7\u5236\u5f00\u653e\u7cfb\u7edf\u52a8\u529b\u5b66\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.00075", "categories": ["cs.LG", "cs.MS"], "pdf": "https://arxiv.org/pdf/2602.00075", "abs": "https://arxiv.org/abs/2602.00075", "authors": ["Philipp Andelfinger", "Wentong Cai"], "title": "Dimensional Peeking for Low-Variance Gradients in Zeroth-Order Discrete Optimization via Simulation", "comment": "Accepted at ACM SIGSIM PADS 2026", "summary": "Gradient-based optimization methods are commonly used to identify local optima in high-dimensional spaces. When derivatives cannot be evaluated directly, stochastic estimators can provide approximate gradients. However, these estimators' perturbation-based sampling of the objective function introduces variance that can lead to slow convergence. In this paper, we present dimensional peeking, a variance reduction method for gradient estimation in discrete optimization via simulation. By lifting the sampling granularity from scalar values to classes of values that follow the same control flow path, we increase the information gathered per simulation evaluation. Our derivation from an established smoothed gradient estimator shows that the method does not introduce any bias. We present an implementation via a custom numerical data type to transparently carry out dimensional peeking over C++ programs. Variance reductions by factors of up to 7.9 are observed for three simulation-based optimization problems with high-dimensional input. The optimization progress compared to three meta-heuristics shows that dimensional peeking increases the competitiveness of zeroth-order optimization for discrete and non-convex simulations.", "AI": {"tldr": "\u63d0\u51fa\"\u7ef4\u5ea6\u7aa5\u63a2\"\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u91c7\u6837\u7c92\u5ea6\u4ece\u6807\u91cf\u63d0\u5347\u5230\u5171\u4eab\u63a7\u5236\u6d41\u7684\u503c\u7c7b\uff0c\u51cf\u5c11\u79bb\u6563\u6a21\u62df\u4f18\u5316\u4e2d\u68af\u5ea6\u4f30\u8ba1\u7684\u65b9\u5dee\uff0c\u5b9e\u73b0\u6700\u9ad87.9\u500d\u7684\u65b9\u5dee\u964d\u4f4e\u5e76\u63d0\u5347\u96f6\u9636\u4f18\u5316\u7ade\u4e89\u529b", "motivation": "\u968f\u673a\u68af\u5ea6\u4f30\u8ba1\u5668\u5728\u79bb\u6563\u6a21\u62df\u4f18\u5316\u4e2d\u56e0\u76ee\u6807\u51fd\u6570\u6270\u52a8\u91c7\u6837\u4ea7\u751f\u9ad8\u65b9\u5dee\uff0c\u5bfc\u81f4\u6536\u655b\u7f13\u6162\uff0c\u9700\u964d\u4f4e\u65b9\u5dee\u4ee5\u63d0\u5347\u4f18\u5316\u6548\u7387", "method": "\u63d0\u51fa\u7ef4\u5ea6\u7aa5\u63a2\u65b9\u6cd5\uff1a\u5c06\u91c7\u6837\u7c92\u5ea6\u4ece\u6807\u91cf\u6269\u5c55\u81f3\u9075\u5faa\u76f8\u540c\u63a7\u5236\u6d41\u8def\u5f84\u7684\u503c\u7c7b\uff0c\u57fa\u4e8e\u5e73\u6ed1\u68af\u5ea6\u4f30\u8ba1\u5668\u63a8\u5bfc\u65e0\u504f\u4f30\u8ba1\uff0c\u5e76\u901a\u8fc7C++\u81ea\u5b9a\u4e49\u6570\u503c\u7c7b\u578b\u5b9e\u73b0\u900f\u660e\u5316\u5e94\u7528", "result": "\u5728\u4e09\u4e2a\u9ad8\u7ef4\u8f93\u5165\u6a21\u62df\u4f18\u5316\u95ee\u9898\u4e2d\u89c2\u6d4b\u5230\u6700\u9ad87.9\u500d\u7684\u65b9\u5dee\u964d\u4f4e\uff1b\u76f8\u6bd4\u4e09\u79cd\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u7ef4\u5ea6\u7aa5\u63a2\u663e\u8457\u63d0\u5347\u96f6\u9636\u4f18\u5316\u5728\u79bb\u6563\u975e\u51f8\u6a21\u62df\u4e2d\u7684\u7ade\u4e89\u529b", "conclusion": "\u7ef4\u5ea6\u7aa5\u63a2\u901a\u8fc7\u589e\u52a0\u5355\u6b21\u6a21\u62df\u4fe1\u606f\u91cf\u6709\u6548\u964d\u4f4e\u68af\u5ea6\u4f30\u8ba1\u65b9\u5dee\uff0c\u4e3a\u79bb\u6563\u975e\u51f8\u6a21\u62df\u4f18\u5316\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u96f6\u9636\u4f18\u5316\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.00454", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00454", "abs": "https://arxiv.org/abs/2602.00454", "authors": ["Jing Wu", "Yue Sun", "Tianpei Xie", "Suiyao Chen", "Jingyuan Bao", "Yaopengxiao Xu", "Gaoyuan Du", "Inseok Heo", "Alexander Gutfraind", "Xin Wang"], "title": "Cross-Modal Memory Compression for Efficient Multi-Agent Debate", "comment": null, "summary": "Multi-agent debate can improve reasoning quality and reduce hallucinations, but it incurs rapidly growing context as debate rounds and agent count increase. Retaining full textual histories leads to token usage that can exceed context limits and often requires repeated summarization, adding overhead and compounding information loss. We introduce DebateOCR, a cross-modal compression framework that replaces long textual debate traces with compact image representations, which are then consumed through a dedicated vision encoder to condition subsequent rounds. This design compresses histories that commonly span tens to hundreds of thousands of tokens, cutting input tokens by more than 92% and yielding substantially lower compute cost and faster inference across multiple benchmarks. We further provide a theoretical perspective showing that diversity across agents supports recovery of omitted information: although any single compressed history may discard details, aggregating multiple agents' compressed views allows the collective representation to approach the information bottleneck with exponentially high probability.", "AI": {"tldr": "DebateOCR compresses multi-agent debate histories into images, cutting tokens by >92% while preserving reasoning quality.", "motivation": "Multi-agent debate's context grows rapidly with rounds/agents, exceeding limits and causing information loss through repeated summarization.", "method": "Replaces long textual debate histories with compact image representations processed by a vision encoder to condition subsequent rounds.", "result": "Reduces input tokens by >92%, lowers compute costs, and speeds up inference across multiple benchmarks.", "conclusion": "Cross-modal compression solves context issues; agent diversity theoretically ensures information recovery with high probability."}}
{"id": "2602.00473", "categories": ["quant-ph", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00473", "abs": "https://arxiv.org/abs/2602.00473", "authors": ["Jin-Long Chen", "Xin Li", "Zhang-Qi Yin"], "title": "Quantum Phase Recognition via Quantum Attention Mechanism", "comment": "10 pages, 7 figures", "summary": "Quantum phase transitions in many-body systems are fundamentally characterized by complex correlation structures, which pose computational challenges for conventional methods in large systems. To address this, we propose a hybrid quantum-classical attention model. This model uses an attention mechanism, realized through swap tests and a parameterized quantum circuit, to extract correlations within quantum states and perform ground-state classification. Benchmarked on the cluster-Ising model with system sizes of 9 and 15 qubits, the model achieves high classification accuracy with less than 100 training data and demonstrates robustness against variations in the training set. Further analysis reveals that the model successfully captures phase-sensitive features and characteristic physical length scales, offering a scalable and data-efficient approach for quantum phase recognition in complex many-body systems.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u901a\u8fc7\u4ea4\u6362\u6d4b\u8bd5\u548c\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\u63d0\u53d6\u91cf\u5b50\u6001\u5173\u8054\uff0c\u57289\u548c15\u91cf\u5b50\u6bd4\u7279\u7684cluster-Ising\u6a21\u578b\u4e0a\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u57fa\u6001\u5206\u7c7b\uff0c\u4ec5\u9700\u5c11\u4e8e100\u4e2a\u8bad\u7ec3\u6570\u636e\uff0c\u4e3a\u91cf\u5b50\u76f8\u8bc6\u522b\u63d0\u4f9b\u53ef\u6269\u5c55\u4e14\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "motivation": "\u591a\u4f53\u7cfb\u7edf\u4e2d\u7684\u91cf\u5b50\u76f8\u53d8\u5177\u6709\u590d\u6742\u7684\u5173\u8054\u7ed3\u6784\uff0c\u8fd9\u7ed9\u5927\u89c4\u6a21\u7cfb\u7edf\u7684\u4f20\u7edf\u8ba1\u7b97\u65b9\u6cd5\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u5229\u7528\u901a\u8fc7\u4ea4\u6362\u6d4b\u8bd5\u548c\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\u5b9e\u73b0\u7684\u6ce8\u610f\u529b\u673a\u5236\u6765\u63d0\u53d6\u91cf\u5b50\u6001\u5185\u90e8\u7684\u5173\u8054\u5e76\u6267\u884c\u57fa\u6001\u5206\u7c7b\u3002", "result": "\u57289\u548c15\u91cf\u5b50\u6bd4\u7279\u7684cluster-Ising\u6a21\u578b\u4e0a\u6d4b\u8bd5\uff0c\u8be5\u6a21\u578b\u4ec5\u9700\u5c11\u4e8e100\u4e2a\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9ad8\u5206\u7c7b\u7cbe\u5ea6\uff0c\u5bf9\u8bad\u7ec3\u96c6\u53d8\u5316\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u5e76\u6210\u529f\u6355\u6349\u5230\u76f8\u654f\u7279\u5f81\u548c\u7279\u5f81\u7269\u7406\u957f\u5ea6\u5c3a\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u591a\u4f53\u7cfb\u7edf\u4e2d\u7684\u91cf\u5b50\u76f8\u8bc6\u522b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6570\u636e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00077", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00077", "abs": "https://arxiv.org/abs/2602.00077", "authors": ["Francisco Mart\u00ednez", "Mar\u00eda P. Fr\u00edas"], "title": "Automated univariate time series forecasting with regression trees", "comment": "23 pages, 17 figures", "summary": "This paper describes a methodology for automated univariate time series forecasting using regression trees and their ensembles: bagging and random forests. The key aspects that are addressed are: the use of an autoregressive approach and recursive forecasts, how to select the autoregressive features, how to deal with trending series and how to cope with seasonal behavior. Experimental results show a forecast accuracy comparable with well-established statistical models such as exponential smoothing or ARIMA. Furthermore, a publicly available software implementing all the proposed strategies has been developed and is described in the paper.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u56de\u5f52\u6811\u96c6\u6210\uff08\u88c5\u888b/\u968f\u673a\u68ee\u6797\uff09\u7684\u81ea\u52a8\u5316\u5355\u53d8\u91cf\u65f6\u5e8f\u9884\u6d4b\u65b9\u6cd5\uff0c\u89e3\u51b3\u81ea\u56de\u5f52\u7279\u5f81\u9009\u62e9\u3001\u8d8b\u52bf\u4e0e\u5b63\u8282\u6027\u5904\u7406\u95ee\u9898\uff0c\u7cbe\u5ea6\u5ab2\u7f8eARIMA/\u6307\u6570\u5e73\u6ed1\uff0c\u5e76\u63d0\u4f9b\u5f00\u6e90\u8f6f\u4ef6", "motivation": "\u4f20\u7edf\u7edf\u8ba1\u9884\u6d4b\u6a21\u578b\u53c2\u6570\u8c03\u6574\u590d\u6742\uff0c\u9700\u63a2\u7d22\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u81ea\u52a8\u5316\u65f6\u5e8f\u9884\u6d4b\u65b0\u65b9\u6cd5", "method": "\u91c7\u7528\u56de\u5f52\u6811\u53ca\u5176\u96c6\u6210\u7b56\u7565\uff0c\u7ed3\u5408\u81ea\u56de\u5f52\u6846\u67b6\u4e0e\u9012\u5f52\u9884\u6d4b\uff0c\u7cfb\u7edf\u5904\u7406\u7279\u5f81\u5de5\u7a0b\u3001\u8d8b\u52bf\u548c\u5b63\u8282\u6027", "result": "\u5b9e\u9a8c\u663e\u793a\u9884\u6d4b\u7cbe\u5ea6\u4e0eARIMA\u3001\u6307\u6570\u5e73\u6ed1\u7b49\u4f20\u7edf\u7edf\u8ba1\u6a21\u578b\u76f8\u5f53", "conclusion": "\u5f00\u53d1\u4e86\u516c\u5f00\u53ef\u7528\u8f6f\u4ef6\uff0c\u9a8c\u8bc1\u4e86\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u65f6\u5e8f\u9884\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027"}}
{"id": "2602.01809", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.01809", "abs": "https://arxiv.org/abs/2602.01809", "authors": ["Yuke Zhang", "Pengfei Zhang"], "title": "Finite-Size Scaling of the Full Eigenstate Thermalization in Quantum Spin Chains", "comment": "12 pages, 11 figures", "summary": "Despite the unitary evolution of closed quantum systems, long-time expectation of local observables are well described by thermal ensembles, providing the foundation of quantum statistical mechanics. A promising route to understanding this quantum thermalization is the eigenstate thermalization hypothesis (ETH), which posits that individual energy eigenstates already appear locally thermal. Subsequent studies have extended this concept to the full ETH, which captures higher-order correlations among matrix elements through nontrivial relations. In this work, we perform a detailed exact-diagonalization study of finite-size corrections to these relations in the canonical ensemble. We distinguish two distinct sources of corrections: those arising from energy fluctuations, which decay polynomially with system size, and those originating from fluctuations within each energy window, which decay exponentially with system size. In particular, our analysis resolves the puzzle that, for certain observables, finite-size corrections exhibit anomalous growth with increasing system size even in chaotic systems. Our results provide a systematic and practical methodology for validating the full ETH in quantum many-body systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u7cbe\u786e\u5bf9\u89d2\u5316\u65b9\u6cd5\u5206\u6790\u91cf\u5b50\u70ed\u5316\u4e2d\u6709\u9650\u5c3a\u5bf8\u4fee\u6b63\u7684\u6765\u6e90\uff0c\u533a\u5206\u80fd\u91cf\u6da8\u843d\uff08\u591a\u9879\u5f0f\u8870\u51cf\uff09\u548c\u80fd\u7a97\u5185\u6da8\u843d\uff08\u6307\u6570\u8870\u51cf\uff09\u4e24\u7c7b\u4fee\u6b63\uff0c\u5e76\u89e3\u51b3\u6df7\u6c8c\u7cfb\u7edf\u4e2d\u67d0\u4e9b\u89c2\u6d4b\u91cf\u6709\u9650\u5c3a\u5bf8\u4fee\u6b63\u5f02\u5e38\u589e\u957f\u7684\u96be\u9898\uff0c\u4e3a\u9a8c\u8bc1\u5168ETH\u63d0\u4f9b\u7cfb\u7edf\u5316\u65b9\u6cd5", "motivation": "\u7406\u89e3\u91cf\u5b50\u70ed\u5316\u673a\u5236\u4e2d\u6709\u9650\u5c3a\u5bf8\u4fee\u6b63\u7684\u8d77\u6e90\uff0c\u7279\u522b\u662f\u89e3\u51b3\u6df7\u6c8c\u7cfb\u7edf\u4e2d\u90e8\u5206\u89c2\u6d4b\u91cf\u6709\u9650\u5c3a\u5bf8\u4fee\u6b63\u968f\u7cfb\u7edf\u5c3a\u5bf8\u5f02\u5e38\u589e\u957f\u7684\u672a\u89e3\u4e4b\u8c1c", "method": "\u91c7\u7528\u7cbe\u786e\u5bf9\u89d2\u5316\u6280\u672f\u7814\u7a76\u6709\u9650\u5c3a\u5bf8\u7cfb\u7edf\u7684\u6b63\u5219\u7cfb\u7efc\uff0c\u5b9a\u91cf\u5206\u6790\u5168ETH\u6846\u67b6\u4e0b\u9ad8\u9636\u5173\u8054\u77e9\u9635\u5143\u7684\u4fee\u6b63\u884c\u4e3a", "result": "\u53d1\u73b0\u6709\u9650\u5c3a\u5bf8\u4fee\u6b63\u5b58\u5728\u4e24\u7c7b\u4e0d\u540c\u6765\u6e90\uff1a\u80fd\u91cf\u6da8\u843d\u5bfc\u81f4\u591a\u9879\u5f0f\u8870\u51cf\u4fee\u6b63\uff0c\u80fd\u7a97\u5185\u6da8\u843d\u5bfc\u81f4\u6307\u6570\u8870\u51cf\u4fee\u6b63\uff1b\u6210\u529f\u89e3\u91ca\u7279\u5b9a\u89c2\u6d4b\u91cf\u5728\u6df7\u6c8c\u7cfb\u7edf\u4e2d\u51fa\u73b0\u6709\u9650\u5c3a\u5bf8\u4fee\u6b63\u5f02\u5e38\u589e\u957f\u7684\u73b0\u8c61", "conclusion": "\u5efa\u7acb\u4e86\u9a8c\u8bc1\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u4e2d\u5168ETH\u7684\u7cfb\u7edf\u6027\u5b9e\u7528\u65b9\u6cd5\u8bba\uff0c\u6df1\u5316\u4e86\u5bf9\u91cf\u5b50\u70ed\u5316\u5fae\u89c2\u673a\u5236\u7684\u7406\u89e3"}}
{"id": "2602.00507", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.00507", "abs": "https://arxiv.org/abs/2602.00507", "authors": ["Anand Aruna Kumar"], "title": "Ermakov-Lewis Invariants in Stationary Bohm-Madelung Quantum Mechanics", "comment": "16 pages. Accepted, to appear in IJQF, Vol 12, Issue 2, (2026)", "summary": "The Ermakov Pinney equation and its associated invariant are shown to arise naturally in stationary quantum mechanics when the Schrodinger equation is expressed in Bohm Madelung form and the Hamiltonian is diagonal and separable. Under these conditions, the stationary continuity constraint induces a nonlinear amplitude equation of Ermakov Pinney type in each degree of freedom, revealing a hidden invariant structure that is independent of whether the evolution parameter is time or space.\n  By reformulating the separated stationary equations in Sturm Liouville form and applying Liouville normalization, we demonstrate that the quantum potential is encoded as a curvature contribution of the self adjoint operator rather than appearing as an additional dynamical term. This correspondence preserves the standard probabilistic predictions of quantum mechanics while yielding exact stationary Bohmian amplitudes and their associated invariants. The resulting invariant-based formulation provides stationary guiding fields and clarifies the ontological status of Bohmian amplitudes as geometrically encoded structures rather than auxiliary dynamical additions. The results further show that stationary constrained Bohm Madelung systems naturally admit variational formulations whose extremals preserve the Ermakov Lewis invariant.", "AI": {"tldr": "This paper reveals that the Ermakov-Pinney equation naturally emerges in stationary quantum mechanics when using the Bohm-Madelung formulation with separable Hamiltonians, demonstrating that quantum potential is geometrically encoded as operator curvature rather than a dynamical term, leading to an invariant-based formulation of Bohmian mechanics.", "motivation": "To uncover hidden invariant structures in stationary quantum systems and clarify the ontological status of Bohmian amplitudes and quantum potential within the Bohm-Madelung formulation.", "method": "Expressing the stationary Schr\u00f6dinger equation in Bohm-Madelung form with diagonal separable Hamiltonian, reformulating separated equations in Sturm-Liouville form, and applying Liouville normalization to analyze the continuity constraint.", "result": "The continuity constraint induces Ermakov-Pinney type amplitude equations in each degree of freedom, revealing a hidden invariant structure; quantum potential is encoded as curvature of self-adjoint operators; exact stationary Bohmian amplitudes and invariants are obtained; variational formulations preserving the Ermakov-Lewis invariant are shown to exist.", "conclusion": "Stationary Bohm-Madelung systems possess a geometrically encoded invariant structure where Bohmian amplitudes are fundamental geometric entities rather than auxiliary dynamical terms, providing a new invariant-based formulation that preserves standard quantum predictions while clarifying the ontological interpretation of Bohmian mechanics."}}
{"id": "2602.00079", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00079", "abs": "https://arxiv.org/abs/2602.00079", "authors": ["Han Xiao"], "title": "Lossless Embedding Compression via Spherical Coordinates", "comment": null, "summary": "We present a lossless compression method for unit-norm embeddings that achieves 1.5$\\times$ compression, 25\\% better than the best prior method. The method exploits that spherical coordinates of high-dimensional unit vectors concentrate around $\u03c0/2$, causing IEEE 754 exponents to collapse to a single value and enabling entropy coding. Evaluation across 26 configurations spanning text, image, and multi-vector embeddings confirms consistent improvement. The method requires no training and is fully lossless within float32 precision.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5355\u4f4d\u8303\u6570\u5d4c\u5165\u7684\u65e0\u635f\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u9ad8\u7ef4\u5355\u4f4d\u5411\u91cf\u7403\u5750\u6807\u5728\u03c0/2\u9644\u8fd1\u96c6\u4e2d\u7684\u7279\u6027\uff0c\u4f7fIEEE 754\u6307\u6570\u574d\u7f29\u81f3\u5355\u4e00\u503c\uff0c\u5b9e\u73b01.5\u00d7\u538b\u7f29\u7387\uff0c\u8f83\u6700\u4f73\u73b0\u6709\u65b9\u6cd5\u63d0\u534725%\u3002", "motivation": "\u5355\u4f4d\u8303\u6570\u5d4c\u5165\u5728AI\u6a21\u578b\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u9ad8\u6548\u538b\u7f29\u53ef\u964d\u4f4e\u5b58\u50a8\u548c\u4f20\u8f93\u6210\u672c\u3002\u73b0\u6709\u65b9\u6cd5\u538b\u7f29\u7387\u6709\u9650\uff0c\u5b58\u5728\u663e\u8457\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "\u57fa\u4e8e\u9ad8\u7ef4\u5355\u4f4d\u5411\u91cf\u7403\u5750\u6807\u5728\u03c0/2\u9644\u8fd1\u9ad8\u5ea6\u96c6\u4e2d\u7684\u6d1e\u5bdf\uff0c\u5229\u7528\u5176\u5bfc\u81f4\u6d6e\u70b9\u6570\u6307\u6570\u90e8\u5206\u574d\u7f29\u4e3a\u5355\u4e00\u503c\u7684\u7279\u6027\uff0c\u5b9e\u73b0\u71b5\u7f16\u7801\u3002\u65e0\u9700\u8bad\u7ec3\uff0c\u5b8c\u5168\u65e0\u635f\u3002", "result": "\u572826\u79cd\u6db5\u76d6\u6587\u672c\u3001\u56fe\u50cf\u548c\u591a\u5411\u91cf\u5d4c\u5165\u7684\u914d\u7f6e\u4e0a\u8bc4\u4f30\uff0c\u65b9\u6cd5\u7a33\u5b9a\u5b9e\u73b01.5\u00d7\u538b\u7f29\u7387\uff0c\u76f8\u6bd4\u6700\u4f73\u57fa\u7ebf\u63d0\u534725%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\u3001\u5b8c\u5168\u65e0\u635f\uff08float32\u7cbe\u5ea6\uff09\uff0c\u4e14\u9002\u7528\u4e8e\u591a\u79cd\u5d4c\u5165\u7c7b\u578b\uff0c\u5b9e\u7528\u6027\u5f3a\uff0c\u4e3a\u5d4c\u5165\u538b\u7f29\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.01943", "categories": ["quant-ph", "cond-mat.mes-hall", "cond-mat.quant-gas", "cond-mat.stat-mech", "math-ph"], "pdf": "https://arxiv.org/pdf/2602.01943", "abs": "https://arxiv.org/abs/2602.01943", "authors": ["Li-Ying Chou", "Jyong-Hao Chen"], "title": "Universal scaling of finite-temperature quantum adiabaticity in driven many-body systems", "comment": "5+3+22 pages, 1 theorem, 1 table, 2 figures", "summary": "Establishing quantitative adiabaticity criteria at finite temperature remains substantially less developed than in the pure-state setting, despite the fact that realistic quantum systems are never at absolute zero. Here we derive rigorous bounds on the Hilbert-Schmidt fidelity between mixed states by combining a mixed-state quantum speed limit with mixed-state fidelity susceptibility within the Liouville space formulation of quantum mechanics. Applied to protocols that drive an initial Gibbs state toward a quasi-Gibbs target, these bounds yield an explicit threshold driving rate for the onset of nonadiabaticity. For a broad class of local Hamiltonians in gapped phases, we show that, in the thermodynamic limit, the threshold factorizes into two factors: a system-size contribution that recovers the zero-temperature scaling and a universal temperature-dependent factor. The latter is exponentially close to unity at low temperature, whereas at high temperature it increases linearly with temperature. We verify the predicted scaling in several spin-1/2 chains by obtaining closed-form expressions for the threshold driving rate. Our results provide practical and largely model-independent criteria for finite-temperature adiabaticity in closed many-body systems.", "AI": {"tldr": "\u63d0\u51fa\u6709\u9650\u6e29\u5ea6\u4e0b\u7edd\u70ed\u6027\u5b9a\u91cf\u5224\u636e\uff0c\u7ed3\u5408\u6df7\u5408\u6001\u91cf\u5b50\u901f\u5ea6\u6781\u9650\u4e0e\u4fdd\u771f\u5ea6\u654f\u611f\u6027\uff0c\u63a8\u5bfc\u51fa\u975e\u7edd\u70ed\u6027\u9608\u503c\u9a71\u52a8\u901f\u7387\u7684\u663e\u5f0f\u8868\u8fbe\u5f0f\uff0c\u63ed\u793a\u4f4e\u6e29\u6307\u6570\u8d8b\u8fd1\u4e0e\u9ad8\u6e29\u7ebf\u6027\u589e\u957f\u7684\u666e\u9002\u6e29\u5ea6\u4f9d\u8d56\u7279\u6027", "motivation": "\u73b0\u6709\u7edd\u70ed\u6027\u5224\u636e\u4e3b\u8981\u57fa\u4e8e\u7eaf\u6001\u4f53\u7cfb\uff0c\u800c\u5b9e\u9645\u91cf\u5b50\u7cfb\u7edf\u5904\u4e8e\u6709\u9650\u6e29\u5ea6\u73af\u5883\uff0c\u6709\u9650\u6e29\u5ea6\u4e0b\u7684\u5b9a\u91cf\u7edd\u70ed\u6027\u6807\u51c6\u53d1\u5c55\u4e25\u91cd\u4e0d\u8db3", "method": "\u5728\u91cf\u5b50\u529b\u5b66\u5218\u7ef4\u5c14\u7a7a\u95f4\u6846\u67b6\u4e0b\uff0c\u878d\u5408\u6df7\u5408\u6001\u91cf\u5b50\u901f\u5ea6\u6781\u9650\u4e0e\u6df7\u5408\u6001\u4fdd\u771f\u5ea6\u654f\u611f\u6027\u7406\u8bba\uff0c\u5efa\u7acb\u6df7\u5408\u6001\u5e0c\u5c14\u4f2f\u7279-\u65bd\u5bc6\u7279\u4fdd\u771f\u5ea6\u7684\u4e25\u683c\u754c\u9650", "result": "\u9488\u5bf9\u4ece\u521d\u59cb\u5409\u5e03\u65af\u6001\u5230\u51c6\u5409\u5e03\u65af\u6001\u7684\u9a71\u52a8\u534f\u8bae\uff0c\u63a8\u5bfc\u51fa\u975e\u7edd\u70ed\u6027\u53d1\u751f\u7684\u663e\u5f0f\u9608\u503c\u9a71\u52a8\u901f\u7387\uff1b\u5728\u70ed\u529b\u5b66\u6781\u9650\u4e0b\uff0c\u8be5\u9608\u503c\u5206\u89e3\u4e3a\u6062\u590d\u96f6\u6e29\u6807\u5ea6\u7684\u7cfb\u7edf\u5c3a\u5bf8\u56e0\u5b50\u4e0e\u666e\u9002\u6e29\u5ea6\u4f9d\u8d56\u56e0\u5b50\uff0c\u4f4e\u6e29\u65f6\u6307\u6570\u8d8b\u8fd1\u4e8e1\uff0c\u9ad8\u6e29\u65f6\u7ebf\u6027\u589e\u957f", "conclusion": "\u901a\u8fc7\u81ea\u65cb\u94fe\u6a21\u578b\u9a8c\u8bc1\u6807\u5ea6\u5f8b\uff0c\u4e3a\u5c01\u95ed\u591a\u4f53\u7cfb\u7edf\u63d0\u4f9b\u5b9e\u7528\u4e14\u6a21\u578b\u65e0\u5173\u7684\u6709\u9650\u6e29\u5ea6\u7edd\u70ed\u6027\u5224\u636e\uff0c\u663e\u8457\u62d3\u5c55\u4e86\u975e\u96f6\u6e29\u73af\u5883\u4e0b\u7684\u91cf\u5b50\u7edd\u70ed\u63a7\u5236\u7406\u8bba\u6846\u67b6"}}
{"id": "2602.00525", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.00525", "abs": "https://arxiv.org/abs/2602.00525", "authors": ["David Angel Alba Bonilla", "Kerem Yurtseven", "Krishan Sharma", "Ragunath Chandrasekharan", "Muhammad Khizar", "Alireza Alipour", "Dennis Delali Kwesi Wayo"], "title": "First-Principles Optical Descriptors and Hybrid Classical-Quantum Classification of Er-Doped CaF$_2$", "comment": null, "summary": "We present a physics-informed classical-quantum machine learning framework for discriminating pristine CaF$_2$ from Er-doped CaF$_2$ using first-principles optical descriptors. Finite Ca$_8$F$_{16}$ and Ca$_7$ErF$_{16}$ clusters were constructed from the fluorite structure (a=5.46~$\u00c5$) and treated using density functional theory (DFT) and linear-response time-dependent DFT (LR-TDDFT) within the GPAW code. Geometry optimization was performed in LCAO mode with a DZP basis and PBE exchange-correlation functional, followed by real-space finite-difference ground-state calculations with grid spacing h=0.30~$\u00c5$ and N$_{bands}$=N$_{occ}$+20. Optical excitations up to 10~eV were obtained via the Casida formalism and converted into continuous absorption spectra using Gaussian broadening ($\u03c3$=0.1-0.2~eV). From 1,589 energy-resolved points per system, physically interpretable descriptors including transition energy $E$, extinction coefficient $\u03ba$, and absorption coefficient $\u03b1$ were extracted. A classical RBF-kernel support vector machine (SVM) achieves a test accuracy (ACC) of 0.983 and ROC-AUC of 0.999. Quantum support vector machines (QSVMs) evaluated on statevector and noisy simulators reach accuracies of 0.851 and 0.817, respectively, while execution on IBM quantum hardware yields a test-slice accuracy of 0.733 under finite-shot and decoherence constraints. A hybrid quantum neural network (QNN) with a 3-qubit feature map and depth-4 ansatz achieves a test accuracy of 0.93 and AUC of 0.96. Results here demonstrate that dopant-induced optical fingerprints form a robust, physically grounded feature space for benchmarking near-term quantum learning models against strong classical baselines.", "AI": {"tldr": "A physics-informed classical-quantum ML framework discriminates pristine and Er-doped CaF\u2082 using DFT-derived optical descriptors. Classical SVM achieves 98.3% accuracy, while quantum models (QSVM/QNN) show lower performance on simulators/hardware, establishing a benchmark for near-term quantum learning.", "motivation": "To develop a physically interpretable feature space based on dopant-induced optical fingerprints for benchmarking quantum machine learning models against classical baselines in material discrimination tasks.", "method": "Constructed finite CaF\u2082/Er-doped clusters \u2192 DFT/LR-TDDFT calculations (PBE functional, Casida formalism) \u2192 extracted optical descriptors (E, \u03ba, \u03b1) \u2192 trained classical SVM, QSVM, and hybrid QNN models.", "result": "Classical RBF-SVM: 98.3% accuracy (AUC 0.999). QSVM: 85.1% (statevector), 81.7% (noisy simulator). IBM hardware QSVM: 73.3%. Hybrid QNN (3-qubit): 93% accuracy (AUC 0.96).", "conclusion": "Dopant-induced optical descriptors provide a robust, physics-grounded feature space. Classical methods significantly outperform current quantum approaches, highlighting challenges in quantum hardware for practical material discrimination."}}
{"id": "2602.00084", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00084", "abs": "https://arxiv.org/abs/2602.00084", "authors": ["Brady Steele"], "title": "Why LoRA Resists Label Noise: A Theoretical Framework for Noise-Robust Parameter-Efficient Fine-Tuning", "comment": "14 pages, 7 figures, 7 tables", "summary": "Parameter-efficient fine-tuning methods like Low-Rank Adaptation (LoRA) have become the dominant paradigm for adapting large pretrained models. We present a theoretical framework explaining an underexplored property: LoRA's inherent resistance to label noise. Our analysis reveals three key insights. First, we prove that rank-$r$ LoRA cannot memorize all possible label assignments once the sample size exceeds $O(r(d+k-r))$, limiting its capacity to fit arbitrary noise. Second, we derive an optimal rank balancing approximation bias and noise-induced variance, showing it decreases with noise rate. Third, we establish temporal separation: clean patterns are learned early while noise memorization occurs later. We propose RACT (Rank-Aware Curriculum Training), leveraging rank discrepancy for noise detection. Experiments validate our predictions, with RACT achieving 91.1% F1 for noise detection on AG News while maintaining 91.46% accuracy, competitive with baselines that lack noise detection capability.", "AI": {"tldr": "\u672c\u6587\u7406\u8bba\u5206\u6790\u4e86LoRA\u5bf9\u6807\u7b7e\u566a\u58f0\u7684\u56fa\u6709\u62b5\u6297\u6027\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u79e9\u5dee\u5f02\u7684\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6RACT\uff0c\u5728AG News\u4e0a\u5b9e\u73b091.1%\u7684\u566a\u58f0\u68c0\u6d4bF1\u503c\u7684\u540c\u65f6\u4fdd\u630191.46%\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5LoRA\u5df2\u6210\u4e3a\u9002\u914d\u5927\u6a21\u578b\u7684\u4e3b\u6d41\u8303\u5f0f\uff0c\u4f46\u5176\u62b5\u6297\u6807\u7b7e\u566a\u58f0\u7684\u5185\u5728\u7279\u6027\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u7406\u89e3\u8fd9\u4e00\u7279\u6027\u5bf9\u63d0\u5347\u6a21\u578b\u5728\u566a\u58f0\u6570\u636e\u4e0b\u7684\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u7406\u8bba\u6846\u67b6\u63ed\u793aLoRA\u6297\u566a\u58f0\u673a\u5236\uff1a1) \u8bc1\u660e\u79e9-r LoRA\u5728\u6837\u672c\u91cf\u8d85\u8fc7O(r(d+k-r))\u65f6\u65e0\u6cd5\u8bb0\u5fc6\u6240\u6709\u6807\u7b7e\u5206\u914d\uff1b2) \u63a8\u5bfc\u6700\u4f18\u79e9\u5e73\u8861\u8fd1\u4f3c\u504f\u5dee\u4e0e\u566a\u58f0\u8bf1\u5bfc\u65b9\u5dee\uff1b3) \u5efa\u7acb\u65f6\u95f4\u5206\u79bb\u73b0\u8c61\u3002\u57fa\u4e8e\u6b64\u63d0\u51faRACT\uff08\u79e9\u611f\u77e5\u8bfe\u7a0b\u8bad\u7ec3\uff09\uff0c\u5229\u7528\u79e9\u5dee\u5f02\u8fdb\u884c\u566a\u58f0\u68c0\u6d4b\u3002", "result": "\u7406\u8bba\u9a8c\u8bc1\u4e86LoRA\u7684\u8bb0\u5fc6\u5bb9\u91cf\u9650\u5236\u548c\u566a\u58f0\u9c81\u68d2\u6027\u3002\u5b9e\u9a8c\u8868\u660eRACT\u5728AG News\u6570\u636e\u96c6\u4e0a\u8fbe\u523091.1%\u7684\u566a\u58f0\u68c0\u6d4bF1\u5206\u6570\uff0c\u540c\u65f6\u4fdd\u630191.46%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u6027\u80fd\u4e0e\u7f3a\u4e4f\u566a\u58f0\u68c0\u6d4b\u80fd\u529b\u7684\u57fa\u7ebf\u76f8\u5f53\u3002", "conclusion": "LoRA\u5177\u6709\u56fa\u6709\u7684\u6297\u6807\u7b7e\u566a\u58f0\u80fd\u529b\uff0c\u8be5\u7279\u6027\u53ef\u88ab\u6709\u6548\u5229\u7528\u3002RACT\u901a\u8fc7\u79e9\u611f\u77e5\u7684\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u5728\u7ef4\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u566a\u58f0\u68c0\u6d4b\uff0c\u4e3a\u566a\u58f0\u73af\u5883\u4e0b\u7684\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.00485", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00485", "abs": "https://arxiv.org/abs/2602.00485", "authors": ["Shule Lu", "Yujing Wang", "Hainan Zhang", "Xiaoshan Yang", "Hongwei Zheng", "Yongxin Tong", "Changsheng Xu", "Zhiming Zheng"], "title": "Replacing Parameters with Preferences: Federated Alignment of Heterogeneous Vision-Language Models", "comment": null, "summary": "VLMs have broad potential in privacy-sensitive domains such as healthcare and finance, yet strict data-sharing constraints render centralized training infeasible. FL mitigates this issue by enabling decentralized training, but practical deployments face challenges due to client heterogeneity in computational resources, application requirements, and model architectures. We argue that while replacing data with model parameters characterizes the present of FL, replacing parameters with preferences represents a more scalable and privacy-preserving future. Motivated by this perspective, we propose MoR, a federated alignment framework based on GRPO with Mixture-of-Rewards for heterogeneous VLMs. MoR initializes a visual foundation model as a KL-regularized reference, while each client locally trains a reward model from local preference annotations, capturing specific evaluation signals without exposing raw data. To reconcile heterogeneous rewards, we introduce a routing-based fusion mechanism that adaptively aggregates client reward signals. Finally, the server performs GRPO with this mixed reward to optimize the base VLM. Experiments on three public VQA benchmarks demonstrate that MoR consistently outperforms federated alignment baselines in generalization, robustness, and cross-client adaptability. Our approach provides a scalable solution for privacy-preserving alignment of heterogeneous VLMs under federated settings.", "AI": {"tldr": "\u63d0\u51faMoR\u6846\u67b6\uff0c\u901a\u8fc7\u672c\u5730\u504f\u597d\u6807\u6ce8\u548c\u6df7\u5408\u5956\u52b1\u673a\u5236\u5b9e\u73b0\u5f02\u6784\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8054\u90a6\u5bf9\u9f50\uff0c\u65e0\u9700\u5171\u4eab\u539f\u59cb\u6570\u636e\uff0c\u5728VQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u9690\u79c1\u654f\u611f\u9886\u57df\uff08\u533b\u7597\u3001\u91d1\u878d\uff09\u7684\u6570\u636e\u5171\u4eab\u9650\u5236\u4f7f\u96c6\u4e2d\u5f0f\u8bad\u7ec3\u4e0d\u53ef\u884c\uff0c\u800c\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u9762\u4e34\u5ba2\u6237\u7aef\u8ba1\u7b97\u8d44\u6e90\u3001\u5e94\u7528\u9700\u6c42\u548c\u6a21\u578b\u67b6\u6784\u7684\u5f02\u6784\u6027\u6311\u6218\u3002\u4f5c\u8005\u8ba4\u4e3a\u8054\u90a6\u5b66\u4e60\u5e94\u4ece'\u7528\u53c2\u6570\u66ff\u4ee3\u6570\u636e'\u6f14\u8fdb\u4e3a'\u7528\u504f\u597d\u66ff\u4ee3\u53c2\u6570'\uff0c\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u3002", "method": "MoR\u6846\u67b6\uff1a1\uff09\u521d\u59cb\u5316KL\u6b63\u5219\u5316\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u53c2\u8003\u6a21\u578b\uff1b2\uff09\u5404\u5ba2\u6237\u7aef\u57fa\u4e8e\u672c\u5730\u504f\u597d\u6807\u6ce8\u8bad\u7ec3\u4e13\u5c5e\u5956\u52b1\u6a21\u578b\uff0c\u6355\u83b7\u8bc4\u4f30\u4fe1\u53f7\u800c\u4e0d\u66b4\u9732\u539f\u59cb\u6570\u636e\uff1b3\uff09\u901a\u8fc7\u8def\u7531\u878d\u5408\u673a\u5236\u81ea\u9002\u5e94\u805a\u5408\u5f02\u6784\u5956\u52b1\uff1b4\uff09\u670d\u52a1\u5668\u91c7\u7528GRPO\u7b97\u6cd5\u7ed3\u5408\u6df7\u5408\u5956\u52b1\u4f18\u5316\u57fa\u7840\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00VQA\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMoR\u5728\u6cdb\u5316\u6027\u80fd\u3001\u9c81\u68d2\u6027\u548c\u8de8\u5ba2\u6237\u7aef\u9002\u5e94\u6027\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u8054\u90a6\u5bf9\u9f50\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u4e3a\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e0b\u5f02\u6784\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9690\u79c1\u4fdd\u62a4\u5bf9\u9f50\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00085", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00085", "abs": "https://arxiv.org/abs/2602.00085", "authors": ["Shuozhe Li", "Jincheng Cao", "Bodun Hu", "Aryan Mokhtari", "Leqi Liu", "Amy Zhang"], "title": "CARE-RFT: Confidence-Anchored Reinforcement Finetuning for Reliable Reasoning in Large Language Models", "comment": null, "summary": "Reinforcement finetuning (RFT) has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models. However, we identify a critical trade-off: while unconstrained RFT achieves strong reasoning performance, it severely compromises model trustworthiness by amplifying hallucination and worsening calibration; conversely, RKL-constrained RFT preserves trustworthiness but limits reasoning gains due to its unbounded penalty on exploratory deviations. To resolve this tension, we introduce CARE-RFT (Confidence-Anchored Regularized Reinforcement Finetuning), a novel method that replaces standard reverse KL regularization with a skew reverse KL divergence. CARE-RFT provides a confidence-sensitive penalty: it is bounded for confident, consistently rewarded explorations to enable reasoning, while unbounded elsewhere to preserve calibration. Extensive experiments across multiple model scales and RFT algorithms show that CARE-RFT achieves a superior balance, matching the reasoning performance of unconstrained RFT while recovering the trustworthiness and calibration of the base model. Our work establishes that careful, confidence-aware regularization is key to building both capable and trustworthy reasoning models.", "AI": {"tldr": "\u63d0\u51faCARE-RFT\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u611f\u77e5\u7684\u6b63\u5219\u5316\u89e3\u51b3RFT\u4e2d\u63a8\u7406\u80fd\u529b\u4e0e\u53ef\u4fe1\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u7ef4\u6301\u6a21\u578b\u6821\u51c6\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5fae\u8c03(RFT)\u5b58\u5728\u5173\u952e\u6743\u8861\uff1a\u65e0\u7ea6\u675fRFT\u63d0\u5347\u63a8\u7406\u4f46\u635f\u5bb3\u53ef\u4fe1\u5ea6\uff1b\u53cd\u5411KL\u7ea6\u675fRFT\u4fdd\u6301\u53ef\u4fe1\u5ea6\u5374\u9650\u5236\u63a8\u7406\u589e\u76ca\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u540c\u65f6\u5b9e\u73b0\u9ad8\u80fd\u529b\u548c\u9ad8\u53ef\u4fe1\u5ea6\u3002", "method": "\u7528\u504f\u659c\u53cd\u5411KL\u6563\u5ea6\u66ff\u4ee3\u6807\u51c6\u53cd\u5411KL\u6b63\u5219\u5316\uff0c\u8bbe\u8ba1\u7f6e\u4fe1\u5ea6\u654f\u611f\u7684\u60e9\u7f5a\u673a\u5236\uff1a\u5bf9\u9ad8\u7f6e\u4fe1\u5ea6\u4e14\u6301\u7eed\u83b7\u5f97\u5956\u52b1\u7684\u63a2\u7d22\u65bd\u52a0\u6709\u754c\u60e9\u7f5a\u4ee5\u652f\u6301\u63a8\u7406\uff0c\u5bf9\u5176\u4f59\u60c5\u51b5\u4fdd\u6301\u65e0\u754c\u60e9\u7f5a\u4ee5\u7ef4\u6301\u6821\u51c6\u3002", "result": "\u5728\u591a\u6a21\u578b\u5c3a\u5ea6\u548cRFT\u7b97\u6cd5\u4e0a\u9a8c\u8bc1\uff0cCARE-RFT\u8fbe\u5230\u66f4\u4f18\u5e73\u8861\uff1a\u5339\u914d\u65e0\u7ea6\u675fRFT\u7684\u63a8\u7406\u6027\u80fd\uff0c\u540c\u65f6\u6062\u590d\u57fa\u7840\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u548c\u6821\u51c6\u6c34\u5e73\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u7f6e\u4fe1\u5ea6\u611f\u77e5\u6b63\u5219\u5316\u662f\u6784\u5efa\u65e2\u6709\u80fd\u529b\u53c8\u53ef\u4fe1\u7684\u63a8\u7406\u6a21\u578b\u7684\u5173\u952e\uff0c\u4e3a\u672a\u6765\u5927\u6a21\u578b\u5bf9\u9f50\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.00510", "categories": ["cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.00510", "abs": "https://arxiv.org/abs/2602.00510", "authors": ["Huanghaohe Zou", "Peng Han", "Emad Nazerian", "Alex Q. Huang"], "title": "PCBSchemaGen: Constraint-Guided Schematic Design via LLM for Printed Circuit Boards (PCB)", "comment": null, "summary": "Printed Circuit Board (PCB) schematic design plays an essential role in all areas of electronic industries. Unlike prior works that focus on digital or analog circuits alone, PCB design must handle heterogeneous digital, analog, and power signals while adhering to real-world IC packages and pin constraints. Automated PCB schematic design remains unexplored due to the scarcity of open-source data and the absence of simulation-based verification. We introduce PCBSchemaGen, the first training-free framework for PCB schematic design that comprises LLM agent and Constraint-guided synthesis. Our approach makes three contributions: 1. an LLM-based code generation paradigm with iterative feedback with domain-specific prompts. 2. a verification framework leveraging a real-world IC datasheet derived Knowledge Graph (KG) and Subgraph Isomorphism encoding pin-role semantics and topological constraints. 3. an extensive experiment on 23 PCB schematic tasks spanning digital, analog, and power domains. Results demonstrate that PCBSchemaGen significantly improves design accuracy and computational efficiency.", "AI": {"tldr": "This paper introduces PCBSchemaGen, the first training-free framework for automated PCB schematic design using LLM agents with constraint-guided synthesis and verification via IC datasheet knowledge graphs.", "motivation": "PCB schematic design is complex due to heterogeneous signal types (digital, analog, power) and real-world IC package constraints. Previous work focused only on digital or analog circuits, and automated PCB design remains unexplored because of scarce open-source data and lack of verification methods.", "method": "The authors propose PCBSchemaGen, a training-free framework combining an LLM agent for code generation with domain-specific prompts and iterative feedback, and a verification system using IC datasheet-derived knowledge graphs with subgraph isomorphism encoding for pin-role semantics and topological constraints.", "result": "The framework was tested on 23 PCB schematic tasks across digital, analog, and power domains, demonstrating significant improvements in both design accuracy and computational efficiency.", "conclusion": "PCBSchemaGen successfully enables automated PCB schematic design by effectively handling heterogeneous circuits and real-world constraints through LLM-driven generation and rigorous constraint verification, opening new possibilities for electronics design automation."}}
{"id": "2602.00595", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.00595", "abs": "https://arxiv.org/abs/2602.00595", "authors": ["Ma-Cheng Yang", "Cong-Feng Qiao"], "title": "Geometric Optimization for Tight Entropic Uncertainty Relations", "comment": "8 pages, 4 figures", "summary": "Entropic uncertainty relations play a fundamental role in quantum information theory. However, determining optimal (tight) entropic uncertainty relations for general observables remains a formidable challenge and has so far been achieved only in a few special cases. Motivated by Schwonnek \\emph{et al.} [PRL \\textbf{119}, 170404 (2017)], we recast this task as a geometric optimization problem over the quantum probability space. This procedure leads to an effective outer-approximation method that yields tight entropic uncertainty bounds for general measurements in finite-dimensional quantum systems with preassigned numerical precision. We benchmark our approach against existing analytical and majorization-based bounds, and demonstrate its practical advantage through applications to quantum steering.", "AI": {"tldr": "\u5c06\u71b5\u4e0d\u786e\u5b9a\u6027\u5173\u7cfb\u95ee\u9898\u8f6c\u5316\u4e3a\u91cf\u5b50\u6982\u7387\u7a7a\u95f4\u7684\u51e0\u4f55\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u5916\u903c\u8fd1\u6cd5\u83b7\u5f97\u4e00\u822c\u6d4b\u91cf\u7684\u7d27\u81f4\u754c", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u786e\u5b9a\u4e00\u822c\u89c2\u6d4b\u91cf\u7684\u6700\u4f18\uff08\u7d27\u81f4\uff09\u71b5\u4e0d\u786e\u5b9a\u6027\u5173\u7cfb\uff0c\u4ec5\u89e3\u51b3\u5c11\u6570\u7279\u4f8b", "method": "\u53d7Schwonnek\u7b49\u4eba\u542f\u53d1\uff0c\u5c06\u95ee\u9898\u91cd\u6784\u4e3a\u91cf\u5b50\u6982\u7387\u7a7a\u95f4\u7684\u51e0\u4f55\u4f18\u5316\u95ee\u9898\uff0c\u5f00\u53d1\u6709\u6548\u5916\u903c\u8fd1\u7b97\u6cd5", "result": "\u83b7\u5f97\u6709\u9650\u7ef4\u91cf\u5b50\u7cfb\u7edf\u4e00\u822c\u6d4b\u91cf\u7684\u7d27\u81f4\u71b5\u4e0d\u786e\u5b9a\u6027\u754c\uff08\u53ef\u9884\u8bbe\u6570\u503c\u7cbe\u5ea6\uff09\uff0c\u5728\u91cf\u5b50\u5bfc\u5f15\u5e94\u7528\u4e2d\u4f18\u4e8e\u73b0\u6709\u89e3\u6790\u754c\u548c\u4f18\u8d85\u754c", "conclusion": "\u51e0\u4f55\u4f18\u5316\u65b9\u6cd5\u4e3a\u4e00\u822c\u89c2\u6d4b\u91cf\u7684\u7d27\u81f4\u71b5\u4e0d\u786e\u5b9a\u6027\u5173\u7cfb\u63d0\u4f9b\u4e86\u666e\u9002\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.00087", "categories": ["cs.LG", "cs.AI", "cs.PF", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.00087", "abs": "https://arxiv.org/abs/2602.00087", "authors": ["Haolin Pan", "Lianghong Huang", "Jinyuan Dong", "Mingjie Xing", "Yanjun Wu"], "title": "ECCO: Evidence-Driven Causal Reasoning for Compiler Optimization", "comment": null, "summary": "Compiler auto-tuning faces a dichotomy between traditional black-box search methods, which lack semantic guidance, and recent Large Language Model (LLM) approaches, which often suffer from superficial pattern matching and causal opacity. In this paper, we introduce ECCO, a framework that bridges interpretable reasoning with combinatorial search. We first propose a reverse engineering methodology to construct a Chain-of-Thought dataset, explicitly mapping static code features to verifiable performance evidence. This enables the model to learn the causal logic governing optimization decisions rather than merely imitating sequences. Leveraging this interpretable prior, we design a collaborative inference mechanism where the LLM functions as a strategist, defining optimization intents that dynamically guide the mutation operations of a genetic algorithm. Experimental results on seven datasets demonstrate that ECCO significantly outperforms the LLVM opt -O3 baseline, achieving an average 24.44% reduction in cycles.", "AI": {"tldr": "\u63d0\u51faECCO\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u63a8\u7406\u4e0e\u7ec4\u5408\u641c\u7d22\u7ed3\u5408\u89e3\u51b3\u7f16\u8bd1\u5668\u81ea\u52a8\u8c03\u4f18\u95ee\u9898\uff0c\u5b9e\u73b024.44%\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u7f16\u8bd1\u5668\u81ea\u52a8\u8c03\u4f18\u5b58\u5728\u4f20\u7edf\u9ed1\u76d2\u641c\u7d22\u7f3a\u4e4f\u8bed\u4e49\u6307\u5bfc\u4e0eLLM\u65b9\u6cd5\u5b58\u5728\u8868\u5c42\u6a21\u5f0f\u5339\u914d\u548c\u56e0\u679c\u4e0d\u900f\u660e\u7684\u4e8c\u5143\u5bf9\u7acb\u95ee\u9898\u3002", "method": "\u63d0\u51faECCO\u6846\u67b6\uff0c\u91c7\u7528\u9006\u5411\u5de5\u7a0b\u6784\u5efa\u601d\u7ef4\u94fe\u6570\u636e\u96c6\u5efa\u7acb\u4ee3\u7801\u7279\u5f81\u4e0e\u6027\u80fd\u8bc1\u636e\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u8bbe\u8ba1\u534f\u540c\u63a8\u7406\u673a\u5236\u8ba9LLM\u4f5c\u4e3a\u7b56\u7565\u5e08\u52a8\u6001\u6307\u5bfc\u9057\u4f20\u7b97\u6cd5\u7684\u53d8\u5f02\u64cd\u4f5c\u3002", "result": "\u5728\u4e03\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8eLLVM opt -O3\u57fa\u7ebf\uff0c\u5e73\u5747\u51cf\u5c1124.44%\u7684\u65f6\u949f\u5468\u671f\u3002", "conclusion": "ECCO\u6210\u529f\u6865\u63a5\u53ef\u89e3\u91ca\u63a8\u7406\u4e0e\u7ec4\u5408\u641c\u7d22\uff0c\u901a\u8fc7\u56e0\u679c\u903b\u8f91\u5b66\u4e60\u800c\u975e\u7b80\u5355\u6a21\u4eff\uff0c\u4e3a\u7f16\u8bd1\u5668\u4f18\u5316\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00521", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00521", "abs": "https://arxiv.org/abs/2602.00521", "authors": ["Junhyuk Choi", "Sohhyung Park", "Chanhee Cho", "Hyeonchu Park", "Bugeun Kim"], "title": "Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory", "comment": "Under review", "summary": "While LLM-as-a-Judge is widely used in automated evaluation, existing validation practices primarily operate at the level of observed outputs, offering limited insight into whether LLM judges themselves function as stable and reliable measurement instruments. To address this limitation, we introduce a two-phase diagnostic framework for assessing reliability of LLM-as-a-Judge, grounded in Item Response Theory (IRT). The framework adopts Graded Response Model (GRM) of IRT and formalizes reliability along two complementary dimensions: (1) intrinsic consistency, defined as the stability of measurement behavior under prompt variations, and (2) human alignment, capturing correspondence with human quality assessments. We empirically examine diverse LLM judges with this framework, and show that leveraging IRT-GRM yields interpretable signals for diagnosing judgments systematically. These signals provide practical guidance for verifying reliablity of LLM-as-a-Judge and identifying potential causes of unreliability.", "AI": {"tldr": "Proposes an IRT-based diagnostic framework to evaluate LLM-as-a-Judge reliability through intrinsic consistency and human alignment dimensions.", "motivation": "Current LLM-as-a-Judge validation only examines outputs, lacking insight into whether LLM judges function as stable, reliable measurement instruments.", "method": "Introduces a two-phase framework using Item Response Theory's Graded Response Model (GRM) to assess reliability across two dimensions: intrinsic consistency (stability under prompt variations) and human alignment (correspondence with human assessments).", "result": "Empirical evaluation of diverse LLM judges shows IRT-GRM provides interpretable diagnostic signals for systematic judgment analysis, offering practical guidance for reliability verification and identifying unreliability causes.", "conclusion": "The IRT-based framework enables systematic assessment of LLM-as-a-Judge reliability, providing actionable insights for improving evaluation quality."}}
{"id": "2602.00610", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.00610", "abs": "https://arxiv.org/abs/2602.00610", "authors": ["Dong-Sheng Liu", "Qing-Xuan Jie", "Chang-Ling Zou", "Xi-Feng Ren", "Guang-Can Guo"], "title": "Practical Quantum Reservoir Computing in Rydberg Atom Arrays", "comment": null, "summary": "Quantum reservoir computing (QRC) is a promising quantum machine learning framework for near-term quantum platforms, yet the performance of different QRC architectures under realistic constraints remains largely unexplored. Here, we provide a comparative numerical study of single-step-QRC (SS-QRC) and multi-step-QRC (MS-QRC) architectures implemented on a Rydberg atom array. We demonstrate that while MS-QRC performance is highly sensitive to the underlying dynamical phase of matter and decoherence, SS-QRC exhibits greater robustness. Using the randomized measurement toolbox to mitigate measurement overhead, we reveal that sampling noise undermines the convergence property required for MS-QRC. This leads to a significant reduction in the information processing capacity (IPC) of MS-QRC, deteriorating its performance on nonlinear time-series benchmarks. In contrast, SS-QRC maintains high IPC and accuracy across both temporal and non-temporal tasks. Our results suggest SS-QRC as a preferred candidate for near-term practical applications due to its resilience to system configurations and statistical noise.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u91cc\u5fb7\u5821\u539f\u5b50\u9635\u5217\u4e0a\u7684\u5355\u6b65\u4e0e\u591a\u6b65\u91cf\u5b50\u50a8\u5c42\u8ba1\u7b97\u67b6\u6784\uff0c\u53d1\u73b0SS-QRC\u5bf9\u566a\u58f0\u548c\u9000\u76f8\u5e72\u66f4\u5177\u9c81\u68d2\u6027\uff0c\u800cMS-QRC\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u56e0\u6b64SS-QRC\u66f4\u9002\u5408\u8fd1\u671f\u91cf\u5b50\u5e94\u7528\u3002", "motivation": "\u4e0d\u540c\u91cf\u5b50\u50a8\u5c42\u8ba1\u7b97\u67b6\u6784\u5728\u5b9e\u9645\u7ea6\u675f\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u8fd9\u5bf9\u8fd1\u671f\u91cf\u5b50\u5e73\u53f0\u7684\u53d1\u5c55\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u5728\u91cc\u5fb7\u5821\u539f\u5b50\u9635\u5217\u4e0a\u5bf9SS-QRC\u548cMS-QRC\u8fdb\u884c\u5bf9\u6bd4\u6570\u503c\u7814\u7a76\uff0c\u5e76\u91c7\u7528\u968f\u673a\u5316\u6d4b\u91cf\u5de5\u5177\u7bb1\u6765\u964d\u4f4e\u6d4b\u91cf\u5f00\u9500\u3002", "result": "MS-QRC\u5bf9\u52a8\u529b\u5b66\u7269\u8d28\u76f8\u548c\u9000\u76f8\u5e72\u9ad8\u5ea6\u654f\u611f\uff0c\u91c7\u6837\u566a\u58f0\u7834\u574f\u5176\u6536\u655b\u7279\u6027\uff0c\u5bfc\u81f4\u4fe1\u606f\u5904\u7406\u80fd\u529b\uff08IPC\uff09\u548c\u975e\u7ebf\u6027\u65f6\u95f4\u5e8f\u5217\u57fa\u51c6\u6027\u80fd\u663e\u8457\u964d\u4f4e\uff1b\u800cSS-QRC\u5728\u5404\u7c7b\u4efb\u52a1\u4e2d\u5747\u4fdd\u6301\u9ad8IPC\u548c\u51c6\u786e\u6027\u3002", "conclusion": "SS-QRC\u56e0\u5176\u5bf9\u7cfb\u7edf\u914d\u7f6e\u548c\u7edf\u8ba1\u566a\u58f0\u7684\u5f3a\u9c81\u68d2\u6027\uff0c\u662f\u8fd1\u671f\u5b9e\u9645\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5e94\u7528\u7684\u9996\u9009\u65b9\u6848\u3002"}}
{"id": "2602.00088", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00088", "abs": "https://arxiv.org/abs/2602.00088", "authors": ["Namkyung Yoon", "Hwangnam Kim"], "title": "From Numbers to Prompts: A Cognitive Symbolic Transition Mechanism for Lightweight Time-Series Forecasting", "comment": "16 pages, 5 figures. Submitted to ACM Transactions on Intelligent Systems and Technology", "summary": "Large language models have achieved remarkable success in time series prediction tasks, but their substantial computational and memory requirements limit deployment on lightweight platforms. In this paper, we propose the Symbolic Transition Mechanism (STM) a novel framework that bridges numeric time series data and language models through symbolic abstraction and prompt engineering. STM transforms continuous time series values into symbol tokens with quantization techniques based on human cognitive structures, and captures temporal dynamics through structured transformations of symbols, enabling fast engineering based predictions in which language models focus on critical parts of time series data. STM is a general purpose mechanisms that ensure the integrity of backbone language models, but they significantly improve their efficiency by inferring the dynamic and structured patterns inherent in time series data. We evaluated STM on various time series datasets, paired with four small language models (SLM) with limited computational environments. For all models, STM achieves error reductions of up to 69% in MAE and 90% in MSE compared to the default backbone SLM without STM. These results demonstrate the potential of STM as an efficient, adaptable layer for symbol-driven time series prediction using foundation models. The accuracy improvements were made at negligible resource costs, with maximum GPU memory of the base model increasing by approximately 0.06% and latency overhead increasing by only 0.64%.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u7b26\u53f7\u8f6c\u6362\u673a\u5236(STM)\uff0c\u901a\u8fc7\u7b26\u53f7\u62bd\u8c61\u5c06\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8f6c\u5316\u4e3a\u8bed\u8a00\u6a21\u578b\u53ef\u5904\u7406\u7684\u7b26\u53f7\u6807\u8bb0\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u5b8c\u6574\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8f7b\u91cf\u7ea7\u5e73\u53f0\u4e0a\u7684\u9884\u6d4b\u6548\u7387\uff0c\u5b9e\u73b0\u9ad8\u8fbe69%\u7684MAE\u548c90%\u7684MSE\u8bef\u5dee\u964d\u4f4e\uff0c\u800c\u8d44\u6e90\u5f00\u9500\u6781\u4f4e\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5de8\u5927\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u9650\u5236\u4e86\u5176\u5728\u8f7b\u91cf\u7ea7\u5e73\u53f0\u4e0a\u7684\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u7b26\u53f7\u8f6c\u6362\u673a\u5236(STM)\uff0c\u57fa\u4e8e\u4eba\u7c7b\u8ba4\u77e5\u7ed3\u6784\u91c7\u7528\u91cf\u5316\u6280\u672f\u5c06\u8fde\u7eed\u65f6\u95f4\u5e8f\u5217\u503c\u8f6c\u6362\u4e3a\u7b26\u53f7\u6807\u8bb0\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u7b26\u53f7\u53d8\u6362\u6355\u6349\u65f6\u5e8f\u52a8\u6001\uff0c\u4f7f\u8bed\u8a00\u6a21\u578b\u80fd\u805a\u7126\u4e8e\u5173\u952e\u6570\u636e\u90e8\u5206\u3002\u8be5\u673a\u5236\u4f5c\u4e3a\u901a\u7528\u5c42\u4fdd\u6301\u9aa8\u5e72\u6a21\u578b\u5b8c\u6574\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u63a8\u7406\u65f6\u5e8f\u6570\u636e\u7684\u52a8\u6001\u7ed3\u6784\u6a21\u5f0f\u63d0\u5347\u6548\u7387\u3002", "result": "\u5728\u591a\u79cd\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u4e0e\u56db\u79cd\u5c0f\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u6d4b\u8bd5\uff0cSTM\u76f8\u6bd4\u9ed8\u8ba4\u9aa8\u5e72\u6a21\u578b\u5b9e\u73b0MAE\u6700\u9ad8\u964d\u4f4e69%\u3001MSE\u6700\u9ad8\u964d\u4f4e90%\u3002\u8d44\u6e90\u5f00\u9500\u6781\u5c0f\uff1aGPU\u5185\u5b58\u4ec5\u589e\u52a0\u7ea60.06%\uff0c\u5ef6\u8fdf\u5f00\u9500\u4ec5\u589e\u52a00.64%\u3002", "conclusion": "STM\u5c55\u793a\u4e86\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\u7b26\u53f7\u9a71\u52a8\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u9ad8\u6548\u9002\u914d\u5c42\u7684\u6f5c\u529b\uff0c\u5728\u51e0\u4e4e\u4e0d\u589e\u52a0\u8d44\u6e90\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u3002"}}
{"id": "2602.00528", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00528", "abs": "https://arxiv.org/abs/2602.00528", "authors": ["Minhua Lin", "Enyan Dai", "Hui Liu", "Xianfeng Tang", "Yuliang Yan", "Zhenwei Dai", "Jingying Zeng", "Zhiwei Zhang", "Fali Wang", "Hongcheng Gao", "Chen Luo", "Xiang Zhang", "Qi He", "Suhang Wang"], "title": "How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use", "comment": "Accepted by ICLR 2026", "summary": "As Large Language Models (LLMs) are increasingly applied in high-stakes domains, their ability to reason strategically under uncertainty becomes critical. Poker provides a rigorous testbed, requiring not only strong actions but also principled, game-theoretic reasoning. In this paper, we conduct a systematic study of LLMs in multiple realistic poker tasks, evaluating both gameplay outcomes and reasoning traces. Our analysis reveals LLMs fail to compete against traditional algorithms and identifies three recurring flaws: reliance on heuristics, factual misunderstandings, and a \"knowing-doing\" gap where actions diverge from reasoning. An initial attempt with behavior cloning and step-level reinforcement learning improves reasoning style but remains insufficient for accurate game-theoretic play. Motivated by these limitations, we propose ToolPoker, a tool-integrated reasoning framework that combines external solvers for GTO-consistent actions with more precise professional-style explanations. Experiments demonstrate that ToolPoker achieves state-of-the-art gameplay while producing reasoning traces that closely reflect game-theoretic principles.", "AI": {"tldr": "LLMs fail at strategic poker due to heuristic reliance, factual errors, and reasoning-action gaps; ToolPoker framework integrating external solvers achieves state-of-the-art gameplay with game-theoretic reasoning", "motivation": "Evaluating LLMs' strategic reasoning under uncertainty in high-stakes domains using poker as a rigorous testbed requiring game-theoretic principles", "method": "Systematic study of LLMs in poker tasks (gameplay + reasoning traces); proposed ToolPoker framework combining external GTO solvers with professional-style explanations", "result": "LLMs fail against traditional algorithms with three flaws: heuristic reliance, factual errors, knowing-doing gap; ToolPoker achieves state-of-the-art gameplay and game-theoretic reasoning traces", "conclusion": "Tool-integrated reasoning (external solvers + precise explanations) is essential for accurate game-theoretic play, addressing fundamental LLM limitations in strategic domains"}}
{"id": "2602.00633", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.00633", "abs": "https://arxiv.org/abs/2602.00633", "authors": ["Xi Jie Yeo", "Darren Ming Zhi Koh", "Justin Yu Xiang Peh", "Christian Kurtsiefer", "Peng Kian Tan"], "title": "Near-losslesss method for generating thermal photon-bunched light", "comment": "5 pages, 6 figures", "summary": "Thermal light sources exhibiting photon bunching have been suggested for sensing applications that exploit timing correlations of stationary light, including range finding, clock synchronization, and non-line-of-sight imaging. However, these proposals have remained unrealized in practice because available sources of photon bunching either possess coherence times too short to be timing resolved by photodetectors, or produce brightness levels too low to tolerate realistic return losses. In this work, we demonstrate a low-loss method for generating photon bunching with a conversion efficiency nearly 9 orders of magnitude higher than that achieved by many other bunching processes.", "AI": {"tldr": "\u672c\u6587\u5c55\u793a\u4e86\u4e00\u79cd\u65b0\u578b\u4f4e\u635f\u8017\u5149\u5b50\u805a\u675f\u4ea7\u751f\u65b9\u6cd5\uff0c\u5176\u8f6c\u6362\u6548\u7387\u6bd4\u5176\u4ed6\u65b9\u6cd5\u9ad8\u51fa\u8fd19\u4e2a\u6570\u91cf\u7ea7\uff0c\u6709\u671b\u5b9e\u73b0\u6b64\u524d\u56e0\u6280\u672f\u9650\u5236\u800c\u65e0\u6cd5\u5b9e\u7528\u7684\u6d4b\u8ddd\u3001\u65f6\u949f\u540c\u6b65\u548c\u975e\u89c6\u8ddd\u6210\u50cf\u7b49\u4f20\u611f\u5e94\u7528\u3002", "motivation": "\u5177\u6709\u5149\u5b50\u805a\u675f\u7279\u6027\u7684\u70ed\u5149\u6e90\u5728\u6d4b\u8ddd\u3001\u65f6\u949f\u540c\u6b65\u548c\u975e\u89c6\u8ddd\u6210\u50cf\u7b49\u4f20\u611f\u5e94\u7528\u65b9\u9762\u5177\u6709\u7406\u8bba\u6f5c\u529b\uff0c\u4f46\u5b9e\u9645\u5b9e\u73b0\u4e00\u76f4\u53d7\u9650\u4e8e\u4e24\u5927\u95ee\u9898\uff1a\u73b0\u6709\u5149\u5b50\u805a\u675f\u6e90\u8981\u4e48\u76f8\u5e72\u65f6\u95f4\u592a\u77ed\uff0c\u65e0\u6cd5\u88ab\u5149\u7535\u63a2\u6d4b\u5668\u5206\u8fa8\uff1b\u8981\u4e48\u4eae\u5ea6\u592a\u4f4e\uff0c\u65e0\u6cd5\u627f\u53d7\u5b9e\u9645\u56de\u6ce2\u635f\u8017\u3002", "method": "\u4f5c\u8005\u5f00\u53d1\u4e86\u4e00\u79cd\u4f4e\u635f\u8017\u7684\u5149\u5b50\u805a\u675f\u4ea7\u751f\u65b9\u6cd5\u3002\u6458\u8981\u672a\u8be6\u7ec6\u8bf4\u660e\u5177\u4f53\u6280\u672f\uff0c\u4f46\u8be5\u65b9\u6cd5\u80fd\u4ece\u6839\u672c\u4e0a\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u7684\u6027\u80fd\u6743\u8861\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u7684\u8f6c\u6362\u6548\u7387\u6bd4\u5176\u4ed6\u805a\u675f\u8fc7\u7a0b\u9ad8\u51fa\u8fd19\u4e2a\u6570\u91cf\u7ea7\uff0c\u6027\u80fd\u83b7\u5f97\u6781\u5927\u63d0\u5347\u3002", "conclusion": "\u8fd9\u4e00\u5149\u5b50\u805a\u675f\u4ea7\u751f\u6548\u7387\u7684\u7a81\u7834\uff0c\u6709\u671b\u4f7f\u57fa\u4e8e\u70ed\u5149\u6e90\u7684\u4f20\u611f\u5e94\u7528\u6700\u7ec8\u8d70\u5411\u5b9e\u7528\u5316\uff0c\u514b\u670d\u4e86\u591a\u5e74\u6765\u963b\u788d\u5176\u5b9e\u9645\u5e94\u7528\u7684\u5173\u952e\u9650\u5236\u3002"}}
{"id": "2602.00092", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00092", "abs": "https://arxiv.org/abs/2602.00092", "authors": ["Neha Kalibhat", "Zi Wang", "Prasoon Bajpai", "Drew Proud", "Wenjun Zeng", "Been Kim", "Mani Malek"], "title": "Interpreting and Controlling Model Behavior via Constitutions for Atomic Concept Edits", "comment": null, "summary": "We introduce a black-box interpretability framework that learns a verifiable constitution: a natural language summary of how changes to a prompt affect a model's specific behavior, such as its alignment, correctness, or adherence to constraints. Our method leverages atomic concept edits (ACEs), which are targeted operations that add, remove, or replace an interpretable concept in the input prompt. By systematically applying ACEs and observing the resulting effects on model behavior across various tasks, our framework learns a causal mapping from edits to predictable outcomes. This learned constitution provides deep, generalizable insights into the model. Empirically, we validate our approach across diverse tasks, including mathematical reasoning and text-to-image alignment, for controlling and understanding model behavior. We found that for text-to-image generation, GPT-Image tends to focus on grammatical adherence, while Imagen 4 prioritizes atmospheric coherence. In mathematical reasoning, distractor variables confuse GPT-5 but leave Gemini 2.5 models and o4-mini largely unaffected. Moreover, our results show that the learned constitutions are highly effective for controlling model behavior, achieving an average of 1.86 times boost in success rate over methods that do not use constitutions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9ed1\u76d2\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u539f\u5b50\u6982\u5ff5\u7f16\u8f91\u5b66\u4e60\u6a21\u578b\u7684\"\u5baa\u6cd5\"\uff0c\u80fd\u591f\u9884\u6d4b\u548c\u63a7\u5236\u6a21\u578b\u884c\u4e3a\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u5b9e\u73b01.86\u500d\u6210\u529f\u7387\u63d0\u5347\u3002", "motivation": "\u9700\u8981\u4e00\u4e2a\u53ef\u9a8c\u8bc1\u7684\u5baa\u6cd5\u6765\u603b\u7ed3\u63d0\u793a\u8bcd\u53d8\u5316\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u884c\u4e3a\uff08\u5bf9\u9f50\u3001\u6b63\u786e\u6027\u3001\u7ea6\u675f\u9075\u5faa\uff09\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u884c\u4e3a\u7406\u89e3\u3002", "method": "\u5229\u7528\u539f\u5b50\u6982\u5ff5\u7f16\u8f91\uff08ACEs\uff09\u5bf9\u8f93\u5165\u63d0\u793a\u8fdb\u884c\u6dfb\u52a0\u3001\u5220\u9664\u6216\u66ff\u6362\u53ef\u89e3\u91ca\u6982\u5ff5\u7684\u64cd\u4f5c\uff0c\u901a\u8fc7\u7cfb\u7edf\u5e94\u7528\u5e76\u89c2\u5bdf\u6a21\u578b\u884c\u4e3a\u53d8\u5316\uff0c\u5b66\u4e60\u4ece\u7f16\u8f91\u5230\u7ed3\u679c\u7684\u56e0\u679c\u6620\u5c04\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u548c\u6587\u751f\u56fe\u4efb\u52a1\u4e2d\u9a8c\u8bc1\uff1aGPT-Image\u6ce8\u91cd\u8bed\u6cd5\u9075\u5faa\uff0cImagen 4\u5f3a\u8c03\u6c1b\u56f4\u4e00\u81f4\u6027\uff1b\u5e72\u6270\u53d8\u91cf\u4f1a\u6df7\u6dc6GPT-5\u4f46\u4e0d\u5f71\u54cdGemini 2.5\u548co4-mini\uff1b\u76f8\u6bd4\u65e0\u5baa\u6cd5\u65b9\u6cd5\u5e73\u5747\u63d0\u53471.86\u500d\u6210\u529f\u7387\u3002", "conclusion": "\u5b66\u4e60\u7684\u5baa\u6cd5\u63d0\u4f9b\u4e86\u6df1\u523b\u4e14\u53ef\u6cdb\u5316\u7684\u6a21\u578b\u6d1e\u5bdf\uff0c\u80fd\u6709\u6548\u63a7\u5236\u6a21\u578b\u884c\u4e3a\uff0c\u4e3a\u9ed1\u76d2\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2602.00643", "categories": ["quant-ph", "physics.comp-ph", "physics.ed-ph"], "pdf": "https://arxiv.org/pdf/2602.00643", "abs": "https://arxiv.org/abs/2602.00643", "authors": ["Murtaza Vefadar"], "title": "From Block Diagrams to Bloch Spheres: Graphical Quantum Circuit Simulation in LabVIEW", "comment": "6 pages, 4 figures. QuVI toolkit is available at https://www.vipm.io/package/murtaza_vefadar_lib_quvi___quantum_circuit_toolkit/", "summary": "As quantum computing transitions from theoretical physics to engineering applications, there is a growing need for accessible simulation tools that bridge the gap between abstract linear algebra and practical implementation. While text-based frameworks (like Qiskit or Cirq) are standard, they often present a steep learning curve for students and engineers accustomed to graphical system design. This paper introduces QuVI (Quantum Virtual Instrument), an open-source quantum circuit toolkit developed natively within the NI LabVIEW environment. Moving beyond initial proof-of-concept models, QuVI establishes a robust framework that leverages LabVIEW's \"dataflow\" paradigm-where wires represent data and nodes represent operations-to provide an intuitive, visual analog to standard quantum circuit notation while enabling the seamless integration of classical control structures like loops and conditionals. The toolkit's capabilities are demonstrated through the construction and visualization of fundamental quantum algorithms, verifying results against theoretical predictions. By translating \"Block Diagrams\" directly into quantum state evolutions (\"Bloch Spheres\"), QuVI offers educators and researchers a powerful platform for prototyping quantum logic without leaving the graphical engineering workspace.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecdQuVI\uff0c\u4e00\u4e2a\u57fa\u4e8eNI LabVIEW\u7684\u91cf\u5b50\u7535\u8def\u5de5\u5177\u5305\uff0c\u901a\u8fc7\u56fe\u5f62\u5316\u6570\u636e\u6d41\u7f16\u7a0b\u8303\u5f0f\u4e3a\u4e60\u60ef\u4e8e\u56fe\u5f62\u5316\u7cfb\u7edf\u8bbe\u8ba1\u7684\u5b66\u751f\u548c\u5de5\u7a0b\u5e08\u63d0\u4f9b\u76f4\u89c2\u7684\u91cf\u5b50\u8ba1\u7b97\u6a21\u62df\u5e73\u53f0\uff0c\u964d\u4f4e\u5b66\u4e60\u66f2\u7ebf\u3002", "motivation": "\u968f\u7740\u91cf\u5b50\u8ba1\u7b97\u4ece\u7406\u8bba\u7269\u7406\u8f6c\u5411\u5de5\u7a0b\u5e94\u7528\uff0c\u9700\u8981\u53ef\u8bbf\u95ee\u7684\u6a21\u62df\u5de5\u5177\u6765\u5f25\u5408\u62bd\u8c61\u7ebf\u6027\u4ee3\u6570\u4e0e\u5b9e\u9645\u5b9e\u73b0\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u57fa\u4e8e\u6587\u672c\u7684\u6846\u67b6\uff08\u5982Qiskit\u6216Cirq\uff09\u5bf9\u4e8e\u4e60\u60ef\u56fe\u5f62\u5316\u7cfb\u7edf\u8bbe\u8ba1\u7684\u4eba\u5458\u5b66\u4e60\u66f2\u7ebf\u9661\u5ced\u3002", "method": "\u5f00\u53d1QuVI\uff08Quantum Virtual Instrument\uff09\uff0c\u4e00\u4e2a\u5f00\u6e90\u91cf\u5b50\u7535\u8def\u5de5\u5177\u5305\uff0c\u539f\u751f\u96c6\u6210\u4e8eNI LabVIEW\u73af\u5883\u3002\u5229\u7528LabVIEW\u7684\"\u6570\u636e\u6d41\"\u8303\u5f0f\uff08\u7ebf\u4ee3\u8868\u6570\u636e\uff0c\u8282\u70b9\u4ee3\u8868\u64cd\u4f5c\uff09\uff0c\u63d0\u4f9b\u6807\u51c6\u91cf\u5b50\u7535\u8def\u7b26\u53f7\u7684\u76f4\u89c2\u89c6\u89c9\u6a21\u62df\uff0c\u540c\u65f6\u96c6\u6210\u5faa\u73af\u548c\u6761\u4ef6\u7b49\u7ecf\u5178\u63a7\u5236\u7ed3\u6784\u3002\u5c06\"\u6846\u56fe\"\u76f4\u63a5\u8f6c\u6362\u4e3a\u91cf\u5b50\u6001\u6f14\u5316\uff08Bloch\u7403\uff09\u3002", "result": "\u901a\u8fc7\u6784\u5efa\u548c\u53ef\u89c6\u5316\u57fa\u7840\u91cf\u5b50\u7b97\u6cd5\u5c55\u793a\u4e86\u5de5\u5177\u5305\u7684\u80fd\u529b\uff0c\u9a8c\u8bc1\u7ed3\u679c\u4e0e\u7406\u8bba\u9884\u6d4b\u4e00\u81f4\u3002", "conclusion": "\u4e3a\u6559\u80b2\u5de5\u4f5c\u8005\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u5e73\u53f0\uff0c\u53ef\u5728\u4e0d\u79bb\u5f00\u56fe\u5f62\u5316\u5de5\u63a7\u73af\u5883\u7684\u60c5\u51b5\u4e0b\u5bf9\u91cf\u5b50\u903b\u8f91\u8fdb\u884c\u539f\u578b\u8bbe\u8ba1\u3002"}}
{"id": "2602.00094", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.00094", "abs": "https://arxiv.org/abs/2602.00094", "authors": ["Sandra Ben\u00edtez-Pe\u00f1a", "Blas Kolic", "Victoria Menendez", "Bel\u00e9n Pulido"], "title": "Trade-offs Between Individual and Group Fairness in Machine Learning: A Comprehensive Review", "comment": null, "summary": "Algorithmic fairness has become a central concern in computational decision-making systems, where ensuring equitable outcomes is essential for both ethical and legal reasons. Two dominant notions of fairness have emerged in the literature: Group Fairness (GF), which focuses on mitigating disparities across demographic subpopulations, and Individual Fairness (IF), which emphasizes consistent treatment of similar individuals. These notions have traditionally been studied in isolation. In contrast, this survey examines methods that jointly address GF and IF, integrating both perspectives within unified frameworks and explicitly characterizing the trade-offs between them. We provide a systematic and critical review of hybrid fairness approaches, organizing existing methods according to the fairness mechanisms they employ and the algorithmic and mathematical strategies used to reconcile multiple fairness criteria. For each class of methods, we examine their theoretical foundations, optimization mechanisms, and empirical evaluation practices, and discuss their limitations. Additionally, we discuss the challenges and identify open research directions for developing principled, context-aware hybrid fairness methods. By synthesizing insights across the literature, this survey aims to serve as a comprehensive resource for researchers and practitioners seeking to design hybrid algorithms that provide reliable fairness guarantees at both the individual and group levels.", "AI": {"tldr": "A systematic survey of hybrid algorithmic fairness methods that jointly address group fairness and individual fairness, analyzing trade-offs, mechanisms, and future research directions.", "motivation": "Algorithmic fairness is central to ethical/legal decision-making, but group fairness (GF) and individual fairness (IF) have traditionally been studied in isolation, creating a gap in understanding how to jointly achieve both and manage their inherent trade-offs.", "method": "Systematic review organizing hybrid fairness methods by their fairness mechanisms and algorithmic strategies, analyzing theoretical foundations, optimization methods, empirical evaluations, and limitations across existing approaches.", "result": "A structured taxonomy of hybrid fairness approaches with critical analysis of their trade-offs, limitations, and identification of key challenges and open research directions for context-aware hybrid fairness methods.", "conclusion": "This survey provides a comprehensive foundation for researchers and practitioners to develop hybrid algorithms that simultaneously guarantee individual and group fairness, highlighting future research paths for principled hybrid fairness frameworks."}}
{"id": "2602.00564", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00564", "abs": "https://arxiv.org/abs/2602.00564", "authors": ["Xiang Zheng", "Weiqi Zhai", "Wei Wang", "Boyu Yang", "Wenbo Li", "Ruixiang Luo", "Haoxiang Sun", "Yucheng Wang", "Zhengze Li", "Meng Wang", "Yuetian Du", "Guojie Lin", "Yaxuan Wang", "Xiaoxiao Xu", "Yanhu Mo", "Xuan Ren", "Hu Wei", "Ze Xu"], "title": "Unmasking Reasoning Processes: A Process-aware Benchmark for Evaluating Structural Mathematical Reasoning in LLMs", "comment": "8 pages, and 3 figures", "summary": "Recent large language models (LLMs) achieve near-saturation accuracy on many established mathematical reasoning benchmarks, raising concerns about their ability to diagnose genuine reasoning competence. This saturation largely stems from the dominance of template-based computation and shallow arithmetic decomposition in existing datasets, which underrepresent reasoning skills such as multi-constraint coordination, constructive logical synthesis, and spatial inference. To address this gap, we introduce ReasoningMath-Plus, a benchmark of 150 carefully curated problems explicitly designed to evaluate structural reasoning. Each problem emphasizes reasoning under interacting constraints, constructive solution formation, or non-trivial structural insight, and is annotated with a minimal reasoning skeleton to support fine-grained process-level evaluation. Alongside the dataset, we introduce HCRS (Hazard-aware Chain-based Rule Score), a deterministic step-level scoring function, and train a Process Reward Model (PRM) on the annotated reasoning traces. Empirically, while leading models attain relatively high final-answer accuracy (up to 5.8/10), HCRS-based holistic evaluation yields substantially lower scores (average 4.36/10, best 5.14/10), showing that answer-only metrics can overestimate reasoning robustness.", "AI": {"tldr": "\u9488\u5bf9\u73b0\u6709\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u9971\u548c\u4e14\u65e0\u6cd5\u8bc4\u4f30\u771f\u5b9e\u63a8\u7406\u80fd\u529b\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faReasoningMath-Plus\u57fa\u51c6\u548cHCRS\u8bc4\u5206\u65b9\u6cd5\uff0c\u63ed\u793a\u4ec5\u9760\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u7387\u4f1a\u9ad8\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u56e0\u6a21\u677f\u5316\u8ba1\u7b97\u548c\u6d45\u5c42\u7b97\u672f\u5206\u89e3\u5bfc\u81f4\u5927\u8bed\u8a00\u6a21\u578b\u51c6\u786e\u7387\u63a5\u8fd1\u9971\u548c\uff0c\u4f46\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u591a\u7ea6\u675f\u534f\u8c03\u3001\u6784\u9020\u6027\u903b\u8f91\u7efc\u5408\u548c\u7a7a\u95f4\u63a8\u7406\u7b49\u771f\u5b9e\u63a8\u7406\u80fd\u529b\u3002", "method": "\u6784\u5efa\u5305\u542b150\u4e2a\u7cbe\u5fc3\u8bbe\u8ba1\u7684ReasoningMath-Plus\u57fa\u51c6\u95ee\u9898\uff0c\u5f3a\u8c03\u4ea4\u4e92\u7ea6\u675f\u4e0b\u7684\u63a8\u7406\u3001\u6784\u9020\u6027\u89e3\u51b3\u65b9\u6848\u5f62\u6210\u548c\u975e\u5e73\u51e1\u7ed3\u6784\u6d1e\u5bdf\uff1b\u63d0\u51faHCRS\u786e\u5b9a\u6027\u6b65\u9aa4\u7ea7\u8bc4\u5206\u51fd\u6570\uff0c\u5e76\u57fa\u4e8e\u6807\u6ce8\u7684\u63a8\u7406\u8f68\u8ff9\u8bad\u7ec3\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b(PRM)\u3002", "result": "\u9886\u5148\u6a21\u578b\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u7387\u76f8\u5bf9\u8f83\u9ad8\uff08\u6700\u9ad85.8/10\uff09\uff0c\u4f46HCRS\u6574\u4f53\u8bc4\u5206\u663e\u8457\u964d\u4f4e\uff08\u5e73\u57474.36/10\uff0c\u6700\u9ad85.14/10\uff09\uff0c\u8868\u660e\u4ec5\u4f9d\u8d56\u7b54\u6848\u7684\u8bc4\u4f30\u65b9\u5f0f\u4f1a\u9ad8\u4f30\u63a8\u7406\u9c81\u68d2\u6027\u3002", "conclusion": "\u4ec5\u8bc4\u4f30\u6700\u7ec8\u7b54\u6848\u7684\u6307\u6807\u4f1a\u9ad8\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0cReasoningMath-Plus\u548cHCRS\u63d0\u4f9b\u4e86\u66f4\u7ec6\u7c92\u5ea6\u7684\u8fc7\u7a0b\u8bc4\u4f30\uff0c\u80fd\u66f4\u51c6\u786e\u5730\u8bca\u65ad\u6a21\u578b\u7684\u771f\u6b63\u63a8\u7406\u6c34\u5e73\u3002"}}
{"id": "2602.00646", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.00646", "abs": "https://arxiv.org/abs/2602.00646", "authors": ["S. F. Kerstan", "M. Gallezot", "T. Decker", "M. Braun", "N. Hegemann"], "title": "Pauli Cloners for Pauli Channels", "comment": null, "summary": "We present a quantum circuit architecture for the one-to-two cloning of $N$-qubit registers. It implements the broad class of Pauli cloners by extending the Niu--Griffiths architecture to multi-qubit systems. In the single-qubit case, we provide explicit constructions for asymmetric universal, phase covariant and biased cloners. We explore the fundamental relationship between Pauli errors, mutually unbiased bases and Pauli cloning. Furthermore, we demonstrate how Pauli cloners can be tailored to specific noise models in the context of quantum communication, especially quantum key distribution.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eN\u91cf\u5b50\u6bd4\u7279\u5bc4\u5b58\u5668\u4e00\u5230\u4e8c\u514b\u9686\u7684\u91cf\u5b50\u7535\u8def\u67b6\u6784\uff0c\u901a\u8fc7\u6269\u5c55Niu-Griffiths\u6846\u67b6\u5b9e\u73b0Pauli\u514b\u9686\u673a\uff0c\u5e76\u63a2\u8ba8\u5176\u5728\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u91cf\u5b50\u514b\u9686\u662f\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u5728\u91cf\u5b50\u5bc6\u7801\u5206\u6790\u4e2d\u5c24\u4e3a\u91cd\u8981\u3002\u73b0\u6709\u67b6\u6784\u4e3b\u8981\u9762\u5411\u5355\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\uff0c\u7f3a\u4e4f\u5bf9\u591a\u91cf\u5b50\u6bd4\u7279\u5bc4\u5b58\u5668\u7684\u6709\u6548\u652f\u6301\u3002\u672c\u7814\u7a76\u65e8\u5728\u5efa\u7acb\u901a\u7528\u7684\u591a\u91cf\u5b50\u6bd4\u7279\u514b\u9686\u6846\u67b6\uff0c\u5e76\u4e3a\u91cf\u5b50\u901a\u4fe1\u4e2d\u7684\u566a\u58f0\u6a21\u578b\u63d0\u4f9b\u7406\u8bba\u5de5\u5177\u3002", "method": "\u91c7\u7528\u91cf\u5b50\u7535\u8def\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5c06Niu-Griffiths\u67b6\u6784\u6269\u5c55\u81f3N\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\uff0c\u5b9e\u73b0Pauli\u514b\u9686\u673a\u7684\u7535\u8def\u5b9e\u73b0\u3002\u9488\u5bf9\u5355\u91cf\u5b50\u6bd4\u7279\u60c5\u5f62\uff0c\u7ed9\u51fa\u975e\u5bf9\u79f0\u901a\u7528\u3001\u76f8\u4f4d\u534f\u53d8\u548c\u6709\u504f\u514b\u9686\u673a\u7684\u663e\u5f0f\u6784\u9020\u3002", "result": "1) \u5efa\u7acb\u4e86N\u91cf\u5b50\u6bd4\u7279\u4e00\u5230\u4e8c\u514b\u9686\u7684\u91cf\u5b50\u7535\u8def\u67b6\u6784\uff1b2) \u63d0\u4f9b\u4e86\u591a\u79cd\u5355\u91cf\u5b50\u6bd4\u7279\u514b\u9686\u673a\u7684\u5177\u4f53\u5b9e\u73b0\u65b9\u6848\uff1b3) \u9610\u660e\u4e86Pauli\u8bef\u5dee\u3001\u4e92\u65e0\u504f\u57fa\u4e0e\u514b\u9686\u64cd\u4f5c\u4e4b\u95f4\u7684\u5185\u5728\u8054\u7cfb\uff1b4) \u5c55\u793a\u4e86\u5982\u4f55\u9488\u5bf9\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\u7cfb\u7edf\u7684\u7279\u5b9a\u566a\u58f0\u7279\u6027\u5b9a\u5236\u514b\u9686\u673a\u3002", "conclusion": "Pauli\u514b\u9686\u673a\u53ef\u901a\u8fc7\u91cf\u5b50\u7535\u8def\u6709\u6548\u5b9e\u73b0\uff0c\u5e76\u6839\u636e\u91cf\u5b50\u901a\u4fe1\u7cfb\u7edf\u566a\u58f0\u6a21\u578b\u8fdb\u884c\u5b9a\u5236\uff0c\u4e3a\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u7406\u8bba\u652f\u6491\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2602.00666", "categories": ["quant-ph", "cond-mat.mes-hall", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2602.00666", "abs": "https://arxiv.org/abs/2602.00666", "authors": ["Chia-Yi Ju", "Gunnar M\u00f6ller", "Yu-Chin Tzeng"], "title": "Fidelity and quantum geometry approach to Dirac exceptional points in diamond nitrogen-vacancy centers", "comment": null, "summary": "Dirac exceptional points (EPs) represent a novel class of non-Hermitian singularities that, unlike conventional EPs, reside entirely within the parity-time unbroken phase and exhibit linear energy dispersion. Here, we theoretically investigate the quantum geometry of Dirac EPs realized in nitrogen-vacancy centers in diamond, utilizing fidelity susceptibility as a probe. We demonstrate that despite the absence of a symmetry-breaking phase transition, the Dirac EP induces a pronounced geometric singularity, confirming the validity of fidelity in characterizing non-Hermitian EPs. Specifically, the real part of the fidelity susceptibility diverges to negative infinity, which serves as a signature of non-Hermitian criticality. Crucially, however, we reveal that this divergence exhibits a distinct anisotropy, diverging along the non-reciprocal coupling direction while remaining finite along the detuning axis. This behavior stands in stark contrast to the omnidirectional divergence observed in conventional EPs. Our findings provide a comprehensive picture of the fidelity probe near the Dirac EP, highlighting the critical role of parameter directionality in exploiting Dirac EPs for quantum control and sensing applications.", "AI": {"tldr": "\u7814\u7a76\u91d1\u521a\u77f3\u6c2e\u7a7a\u4f4d\u4e2d\u5fc3\u4e2d\u7684\u72c4\u62c9\u514b\u4f8b\u5916\u70b9\uff0c\u53d1\u73b0\u5176 fidelity \u654f\u611f\u6027\u5448\u73b0\u65b9\u5411\u5f02\u6027\u53d1\u6563\uff0c\u4e3a\u91cf\u5b50\u4f20\u611f\u548c\u63a7\u5236\u63d0\u4f9b\u65b0\u89c1\u89e3\u3002", "motivation": "\u63a2\u7d22\u72c4\u62c9\u514b\u4f8b\u5916\u70b9\u8fd9\u4e00\u65b0\u578b\u975e\u5384\u7c73\u7279\u5947\u70b9\u7684\u91cf\u5b50\u51e0\u4f55\u7279\u6027\uff0c\u63ed\u793a\u5176\u4e0e\u5e38\u89c4\u4f8b\u5916\u70b9\u7684\u672c\u8d28\u5dee\u5f02\uff0c\u4e3a\u91cf\u5b50\u5e94\u7528\u5960\u5b9a\u57fa\u7840\u3002", "method": "\u5728\u91d1\u521a\u77f3\u6c2e\u7a7a\u4f4d\u4e2d\u5fc3\u4f53\u7cfb\u4e2d\uff0c\u5229\u7528 fidelity \u654f\u611f\u6027\u4f5c\u4e3a\u63a2\u9488\u6765\u7814\u7a76\u72c4\u62c9\u514b\u4f8b\u5916\u70b9\u7684\u91cf\u5b50\u51e0\u4f55\u884c\u4e3a\u3002", "result": "\u72c4\u62c9\u514b\u4f8b\u5916\u70b9\u5bfc\u81f4\u663e\u8457\u7684\u51e0\u4f55\u5947\u70b9\uff0c\u5176 fidelity \u654f\u611f\u6027\u7684\u5b9e\u90e8\u53d1\u6563\u81f3\u8d1f\u65e0\u7a77\uff0c\u4f46\u53d1\u6563\u5177\u6709\u65b9\u5411\u5f02\u6027\uff1a\u4ec5\u5728\u975e\u4e92\u6613\u8026\u5408\u65b9\u5411\u53d1\u6563\uff0c\u800c\u5728\u5931\u8c10\u8f74\u65b9\u5411\u4fdd\u6301\u6709\u9650\uff0c\u4e0e\u5e38\u89c4\u4f8b\u5916\u70b9\u7684\u5168\u5411\u53d1\u6563\u5f62\u6210\u9c9c\u660e\u5bf9\u6bd4\u3002", "conclusion": "\u8be5\u7814\u7a76\u5b8c\u6574\u523b\u753b\u4e86\u72c4\u62c9\u514b\u4f8b\u5916\u70b9\u9644\u8fd1\u7684 fidelity \u54cd\u5e94\uff0c\u5f3a\u8c03\u53c2\u6570\u65b9\u5411\u6027\u5728\u5229\u7528\u72c4\u62c9\u514b\u4f8b\u5916\u70b9\u8fdb\u884c\u91cf\u5b50\u8c03\u63a7\u548c\u4f20\u611f\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2602.00679", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.00679", "abs": "https://arxiv.org/abs/2602.00679", "authors": ["Keqing Liu", "Jiazhao Tian", "Bokun Duan", "Hao Zhang", "Kangze Li", "Guofeng Zhang", "Fedor Jelezko", "Ressa S. Said", "Jianming Cai", "Liantuan Xiao"], "title": "High-resolution wide-field magnetic imaging with sparse sampling using nitrogen-vacancy centers", "comment": "15 pages, 7 figures", "summary": "Nitrogen-vacancy (NV) centers in diamond enable quantitative magnetic imaging, yet practical implementations must balance spatial resolution against acquisition time (and thus per-pixel sensitivity). Single-NV scanning magnetometry achieves genuine nanoscale resolution, nonetheless requires typically a slow pixel-by-pixel acquisition. Meanwhile, wide-field NV-ensemble microscopy provides parallel readout over a large field of view, however is jointly limited by the optical diffraction limit and the sensor-sample standoff. Here, we present a sparse-sampling strategy for reconstructing high-resolution wide-field images from only a small number of measurements. Using simulated NV-ensemble detection of ac magnetic fields, we show that a mean-adjusted Bayesian estimation (MABE) framework can reconstruct 10000-pixel images from only 25 sampling points, achieving SSIM values exceeding 0.999 for representative smooth field distributions, while optimized dynamical-decoupling pulse sequences yield an approximately twofold improvement in magnetic-field sensitivity. The method further clarifies how sampling patterns and sampling density affect reconstruction accuracy and suggests a route toward faster and more scalable magnetic-imaging architectures that may extend to point-scanning NV sensors and other magnetometry platforms, such as SQUIDs, Hall probes, and magnetic tunnel junctions.", "AI": {"tldr": "A sparse-sampling strategy using MABE framework reconstructs high-resolution magnetic images from minimal measurements (25 points for 10000 pixels), achieving SSIM >0.999 and ~2x sensitivity improvement, enabling faster NV-center magnetic imaging.", "motivation": "NV-center magnetic imaging faces a fundamental trade-off: single-NV scanning offers nanoscale resolution but is prohibitively slow, while wide-field ensemble imaging is parallel but diffraction-limited. There's a need to break this compromise for practical applications.", "method": "A sparse-sampling strategy combined with mean-adjusted Bayesian estimation (MABE) framework to reconstruct high-resolution wide-field images from limited measurements, using simulated NV-ensemble detection of ac magnetic fields.", "result": "Reconstructed 10000-pixel images from only 25 sampling points with SSIM >0.999; optimized dynamical-decoupling pulse sequences yielded ~2x sensitivity improvement; quantified how sampling patterns/density affect reconstruction accuracy.", "conclusion": "The approach enables faster, more scalable magnetic imaging architectures that could extend to point-scanning NV sensors and other magnetometry platforms (SQUIDs, Hall probes, magnetic tunnel junctions), potentially revolutionizing practical implementations."}}
{"id": "2602.00120", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00120", "abs": "https://arxiv.org/abs/2602.00120", "authors": ["Xianghong Hu", "Tianning Xu", "Ying Chen", "Shuai Wang"], "title": "Predicting Mortgage Default with Machine Learning: AutoML, Class Imbalance, and Leakage Control", "comment": "12 pages, 4 figures. An extended and pedagogical version will appear as a book chapter", "summary": "Mortgage default prediction is a core task in financial risk management, and machine learning models are increasingly used to estimate default probabilities and provide interpretable signals for downstream decisions. In real-world mortgage datasets, however, three factors frequently undermine evaluation validity and deployment reliability: ambiguity in default labeling, severe class imbalance, and information leakage arising from temporal structure and post-event variables. We compare multiple machine learning approaches for mortgage default prediction using a real-world loan-level dataset, with emphasis on leakage control and imbalance handling. We employ leakage-aware feature selection, a strict temporal split that constrains both origination and reporting periods, and controlled downsampling of the majority class. Across multiple positive-to-negative ratios, performance remains stable, and an AutoML approach (AutoGluon) achieves the strongest AUROC among the models evaluated. An extended and pedagogical version of this work will appear as a book chapter.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u73b0\u5b9e\u4e16\u754c\u62b5\u62bc\u8d37\u6b3e\u8fdd\u7ea6\u9884\u6d4b\u4e2d\u7684\u4e09\u5927\u6311\u6218\uff08\u6807\u7b7e\u6a21\u7cca\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u3001\u4fe1\u606f\u6cc4\u9732\uff09\uff0c\u91c7\u7528\u6cc4\u9732\u611f\u77e5\u7279\u5f81\u9009\u62e9\u3001\u65f6\u95f4\u5206\u5272\u548c\u964d\u91c7\u6837\u65b9\u6cd5\uff0c\u53d1\u73b0AutoGluon\u81ea\u52a8\u673a\u5668\u5b66\u4e60\u6846\u67b6\u5728\u4e0d\u540c\u7c7b\u522b\u6bd4\u4f8b\u4e0b\u8868\u73b0\u7a33\u5b9a\u4e14AUROC\u6700\u9ad8\u3002", "motivation": "\u62b5\u62bc\u8d37\u6b3e\u8fdd\u7ea6\u9884\u6d4b\u662f\u91d1\u878d\u98ce\u9669\u7ba1\u7406\u7684\u6838\u5fc3\u4efb\u52a1\uff0c\u4f46\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u5b58\u5728\u4e09\u4e2a\u4e25\u91cd\u5f71\u54cd\u8bc4\u4f30\u6709\u6548\u6027\u7684\u56e0\u7d20\uff1a\u8fdd\u7ea6\u6807\u7b7e\u6a21\u7cca\u3001\u4e25\u91cd\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u4ee5\u53ca\u7531\u65f6\u95f4\u7ed3\u6784\u548c\u4e8b\u540e\u53d8\u91cf\u5f15\u8d77\u7684\u4fe1\u606f\u6cc4\u9732\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u771f\u5b9e\u8d37\u6b3e\u7ea7\u6570\u636e\u96c6\u6bd4\u8f83\u591a\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u91cd\u70b9\u89e3\u51b3\u4fe1\u606f\u6cc4\u9732\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5177\u4f53\u5305\u62ec\uff1a\u6cc4\u9732\u611f\u77e5\u7279\u5f81\u9009\u62e9\u3001\u4e25\u683c\u7ea6\u675f\u8d37\u6b3e\u53d1\u653e\u548c\u62a5\u544a\u671f\u7684\u65f6\u95f4\u5206\u5272\uff0c\u4ee5\u53ca\u591a\u6570\u7c7b\u7684\u63a7\u5236\u6027\u964d\u91c7\u6837\u3002", "result": "\u5728\u4e0d\u540c\u6b63\u8d1f\u6837\u672c\u6bd4\u4f8b\u4e0b\u6a21\u578b\u6027\u80fd\u4fdd\u6301\u7a33\u5b9a\uff0c\u5176\u4e2dAutoGluon\u81ea\u52a8\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u6240\u6709\u8bc4\u4f30\u6a21\u578b\u4e2d\u83b7\u5f97\u4e86\u6700\u5f3a\u7684AUROC\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u95ee\u9898\u5bf9\u62b5\u62bc\u8d37\u6b3e\u8fdd\u7ea6\u9884\u6d4b\u7684\u91cd\u8981\u6027\uff0c\u6269\u5c55\u6559\u5b66\u7248\u5c06\u4f5c\u4e3a\u4e66\u7c4d\u7ae0\u8282\u53d1\u8868\u3002"}}
{"id": "2602.00700", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.00700", "abs": "https://arxiv.org/abs/2602.00700", "authors": ["Zekun Zhao", "Qingqian Kang", "Teng Zhao", "Cunjin Liu", "Liyun Hu"], "title": "Enhanced Phase Estimation via Photon-Added Two-Mode Squeezed States and Kerr Nonlinearity", "comment": null, "summary": "Quantum metrology harnesses quantum resources to achieve measurement precision beyond classical limits. This work investigates a Mach-Zehnder interferometer incorporating a Kerr nonlinear phase shifter, using photon-added two-mode squeezed coherent states generated via four-wave mixing as input. This study demonstrates that increasing both the photon-addition order and input resource strength systematically enhances phase sensitivity, quantum Fisher information, and the quantum Cram\u00e9r-Rao bound. The system not only surpasses the standard quantum limit but also approaches or exceeds the Heisenberg limit with linear phase shifts, while Kerr nonlinearity enables overcoming the super-Heisenberg limit. The proposed scheme exhibits enhanced robustness against photon loss, providing a promising approach for high-precision quantum metrology applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u5149\u5b50\u6dfb\u52a0\u53cc\u6a21\u538b\u7f29\u76f8\u5e72\u6001\u548c\u56db\u6ce2\u6df7\u9891\u6280\u672f\u6784\u5efa\u9a6c\u8d6b-\u66fe\u5fb7\u5c14\u5e72\u6d89\u4eea\uff0c\u901a\u8fc7\u589e\u5f3a\u5149\u5b50\u6dfb\u52a0\u9636\u6570\u548c\u8f93\u5165\u8d44\u6e90\u5f3a\u5ea6\uff0c\u7cfb\u7edf\u63d0\u5347\u76f8\u4f4d\u7075\u654f\u5ea6\u3001\u91cf\u5b50\u8d39\u820d\u5c14\u4fe1\u606f\u53ca\u91cf\u5b50\u514b\u62c9\u9ed8-\u62c9\u5965\u754c\uff0c\u7a81\u7834\u6807\u51c6\u91cf\u5b50\u6781\u9650\u5e76\u903c\u8fd1\u6d77\u68ee\u5821\u6781\u9650\uff0c\u540c\u65f6\u5229\u7528\u514b\u5c14\u975e\u7ebf\u6027\u8d85\u8d8a\u8d85\u6d77\u68ee\u5821\u6781\u9650\uff0c\u4e14\u5bf9\u5149\u5b50\u635f\u5931\u5177\u6709\u5f3a\u9c81\u68d2\u6027\uff0c\u4e3a\u9ad8\u7cbe\u5ea6\u91cf\u5b50\u8ba1\u91cf\u63d0\u4f9b\u65b0\u65b9\u6848\u3002", "motivation": "\u7a81\u7834\u7ecf\u5178\u6d4b\u91cf\u7cbe\u5ea6\u6781\u9650\uff0c\u63a2\u7d22\u91cf\u5b50\u8d44\u6e90\u5728\u76f8\u4f4d\u6d4b\u91cf\u4e2d\u7684\u589e\u5f3a\u6548\u5e94\uff0c\u89e3\u51b3\u4f20\u7edf\u91cf\u5b50\u8ba1\u91cf\u65b9\u6848\u5728\u566a\u58f0\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u91c7\u7528\u56db\u6ce2\u6df7\u9891\u4ea7\u751f\u5149\u5b50\u6dfb\u52a0\u53cc\u6a21\u538b\u7f29\u76f8\u5e72\u6001\u4f5c\u4e3a\u8f93\u5165\u5149\u6e90\uff0c\u6784\u5efa\u542b\u514b\u5c14\u975e\u7ebf\u6027\u76f8\u4f4d\u8c03\u5236\u5668\u7684\u9a6c\u8d6b-\u66fe\u5fb7\u5c14\u5e72\u6d89\u4eea\uff0c\u901a\u8fc7\u8c03\u63a7\u5149\u5b50\u6dfb\u52a0\u9636\u6570\u548c\u8f93\u5165\u8d44\u6e90\u5f3a\u5ea6\u4f18\u5316\u91cf\u5b50\u8ba1\u91cf\u6027\u80fd\u3002", "result": "\u76f8\u4f4d\u7075\u654f\u5ea6\u3001\u91cf\u5b50\u8d39\u820d\u5c14\u4fe1\u606f\u53ca\u91cf\u5b50\u514b\u62c9\u9ed8-\u62c9\u5965\u754c\u968f\u5149\u5b50\u6dfb\u52a0\u9636\u6570\u548c\u8f93\u5165\u5f3a\u5ea6\u589e\u52a0\u800c\u7cfb\u7edf\u6027\u63d0\u5347\uff1b\u7a81\u7834\u6807\u51c6\u91cf\u5b50\u6781\u9650\u5e76\u903c\u8fd1\u6d77\u68ee\u5821\u6781\u9650\uff1b\u514b\u5c14\u975e\u7ebf\u6027\u4f7f\u7cfb\u7edf\u8d85\u8d8a\u8d85\u6d77\u68ee\u5821\u6781\u9650\uff1b\u65b9\u6848\u5bf9\u5149\u5b50\u635f\u5931\u5177\u6709\u663e\u8457\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6848\u4e3a\u9ad8\u7cbe\u5ea6\u91cf\u5b50\u8ba1\u91cf\u63d0\u4f9b\u4e86\u517c\u5177\u6781\u9650\u7a81\u7834\u80fd\u529b\u4e0e\u566a\u58f0\u9c81\u68d2\u6027\u7684\u53ef\u884c\u8def\u5f84\uff0c\u5728\u91cf\u5b50\u4f20\u611f\u548c\u7cbe\u5bc6\u6d4b\u91cf\u9886\u57df\u5177\u6709\u91cd\u8981\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2602.00592", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.00592", "abs": "https://arxiv.org/abs/2602.00592", "authors": ["Jiaran Zhang", "Luck Ma", "Yanhao Li", "Fanqi Wan", "Di Qi", "Xu Zhao", "Jieyi Hou", "Zhe Xie", "Mengqiang Ren", "Xin Wu", "Zhewei Huang", "Liangyu Chen", "Yingwei Ma", "Qi Han", "Xiangyu Zhang"], "title": "DockSmith: Scaling Reliable Coding Environments via an Agentic Docker Builder", "comment": null, "summary": "Reliable Docker-based environment construction is a dominant bottleneck for scaling execution-grounded training and evaluation of software engineering agents. We introduce DockSmith, a specialized agentic Docker builder designed to address this challenge. DockSmith treats environment construction not only as a preprocessing step, but as a core agentic capability that exercises long-horizon tool use, dependency reasoning, and failure recovery, yielding supervision that transfers beyond Docker building itself. DockSmith is trained on large-scale, execution-grounded Docker-building trajectories produced by a SWE-Factory-style pipeline augmented with a loop-detection controller and a cross-task success memory. Training a 30B-A3B model on these trajectories achieves open-source state-of-the-art performance on Multi-Docker-Eval, with 39.72% Fail-to-Pass and 58.28% Commit Rate. Moreover, DockSmith improves out-of-distribution performance on SWE-bench Verified, SWE-bench Multilingual, and Terminal-Bench 2.0, demonstrating broader agentic benefits of environment construction.", "AI": {"tldr": "DockSmith is an AI agent that builds Docker environments as a core capability, not just preprocessing. Trained on large-scale execution data, it achieves SOTA Docker building performance and improves results on other software engineering tasks.", "motivation": "Reliable Docker-based environment construction is a major bottleneck for scaling execution-grounded training and evaluation of software engineering agents, as it's currently treated as simple preprocessing rather than a core agentic capability.", "method": "DockSmith is trained on large-scale, execution-grounded Docker-building trajectories from a SWE-Factory-style pipeline with loop-detection and cross-task success memory, learning long-horizon tool use, dependency reasoning, and failure recovery.", "result": "A 30B-A3B model achieves open-source SOTA on Multi-Docker-Eval (39.72% Fail-to-Pass, 58.28% Commit Rate) and improves OOD performance on SWE-bench Verified, SWE-bench Multilingual, and Terminal-Bench 2.0.", "conclusion": "Treating environment construction as a core agentic capability yields supervision that transfers beyond Docker building itself, demonstrating broader agentic benefits and improved performance across multiple software engineering benchmarks."}}
{"id": "2602.00783", "categories": ["quant-ph", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.00783", "abs": "https://arxiv.org/abs/2602.00783", "authors": ["Yihan Huang", "Yangshuai Wang"], "title": "Analysis of Hessian Scaling for Local and Global Costs in Variational Quantum Algorithm", "comment": null, "summary": "Barren plateaus are typically characterized by vanishing gradients, yet the feasibility of curvature-based optimization fundamentally relies on the statistical resolvability of the Hessian matrix. In this work, we quantify the entrywise resolvability of the Hessian for Variational Quantum Algorithms at random initialization. By leveraging exact second-order parameter-shift rules, we derive a structural representation that reduces the variance of Hessian entries to a finite covariance quadratic form of shifted cost evaluations. This framework reveals two distinct scaling regimes that govern the sample complexity required to resolve Hessian entries against shot noise. For global objectives, we prove that Hessian variances are exponentially suppressed, implying that the number of measurement shots must scale as $O(e^{\u03b1n})$ with the number of qubits $n$ to maintain a constant signal-to-noise ratio. In contrast, for termwise local objectives in bounded-depth circuits, the variance decay is polynomial and explicitly controlled by the backward lightcone growth on the interaction graph, ensuring that curvature information remains statistically accessible with $O(\\mathrm{poly}(n))$ shots. Extensive numerical experiments across varying system sizes and circuit depths demonstrate these theoretical bounds and the associated sampling costs. Our results provide a rigorous criterion for the computational tractability of second-order methods at initialization.", "AI": {"tldr": "This paper analyzes Hessian matrix resolvability in Variational Quantum Algorithms, revealing that global objectives require exponentially many measurements to resolve curvature information (O(e^(\u03b1n))), while local objectives with bounded-depth circuits only need polynomial measurements (O(poly(n))), providing a rigorous criterion for computational tractability of second-order methods.", "motivation": "Barren plateaus (vanishing gradients) hinder VQA optimization. Second-order methods using Hessian information could potentially overcome this, but only if Hessian entries can be reliably estimated from noisy quantum measurements against shot noise. The statistical resolvability of Hessian is crucial for determining whether curvature-based optimization is computationally feasible.", "method": "Leveraged exact second-order parameter-shift rules to derive a structural representation that reduces Hessian entry variance to a finite covariance quadratic form of shifted cost evaluations. Analyzed sample complexity scaling regimes for global vs. termwise local objectives, and validated theoretical bounds through extensive numerical experiments across varying system sizes and circuit depths.", "result": "Two distinct scaling regimes: (1) For global objectives, Hessian variances are exponentially suppressed, requiring O(e^(\u03b1n)) shots to maintain constant signal-to-noise ratio; (2) For local objectives in bounded-depth circuits, variance decay is polynomial and controlled by backward lightcone growth, requiring only O(poly(n)) shots. Numerical experiments confirmed these theoretical bounds.", "conclusion": "The work establishes a rigorous criterion for computational tractability of second-order methods at initialization, showing that while global objectives suffer severe barren plateau issues even for curvature information, local objectives with bounded-depth circuits maintain statistically accessible curvature, enabling potential second-order optimization advantages in certain quantum problem classes."}}
{"id": "2602.00127", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00127", "abs": "https://arxiv.org/abs/2602.00127", "authors": ["Tong Zhu", "Baiting Chen", "Jin Zhou", "Hua Zhou", "Sriram Sankararaman", "Xiaowu Dai"], "title": "ALIGN: Aligned Delegation with Performance Guarantees for Multi-Agent LLM Reasoning", "comment": null, "summary": "LLMs often underperform on complex reasoning tasks when relying on a single generation-and-selection pipeline. Inference-time ensemble methods can improve performance by sampling diverse reasoning paths or aggregating multiple candidate answers, but they typically treat candidates independently and provide no formal guarantees that ensembling improves reasoning quality. We propose a novel method, Aligned Delegation for Multi-Agent LLM Reasoning (ALIGN), which formulates LLM reasoning as an aligned delegation game. In ALIGN, a principal delegates a task to multiple agents that generate candidate solutions under designed incentives, and then selects among their outputs to produce a final answer. This formulation induces structured interaction among agents while preserving alignment between agent and principal objectives. We establish theoretical guarantees showing that, under a fair comparison with equal access to candidate solutions, ALIGN provably improves expected performance over single-agent generation. Our analysis accommodates correlated candidate answers and relaxes independence assumptions that are commonly used in prior work. Empirical results across a broad range of LLM reasoning benchmarks consistently demonstrate that ALIGN outperforms strong single-agent and ensemble baselines.", "AI": {"tldr": "ALIGN\u6846\u67b6\u5c06LLM\u63a8\u7406\u5efa\u6a21\u4e3a\u5bf9\u9f50\u59d4\u6258\u535a\u5f08\uff0c\u901a\u8fc7\u4e3b\u7406\u4eba\u8bbe\u8ba1\u6fc0\u52b1\u673a\u5236\u534f\u8c03\u591a\u667a\u80fd\u4f53\u751f\u6210\u5019\u9009\u89e3\uff0c\u7406\u8bba\u8bc1\u660e\u5728\u516c\u5e73\u6bd4\u8f83\u4e0b\u80fd\u63d0\u5347\u671f\u671b\u6027\u80fd\uff0c\u5e76\u5728\u591a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u5355\u667a\u80fd\u4f53\u548c\u96c6\u6210\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u65f6\u96c6\u6210\u65b9\u6cd5\u72ec\u7acb\u5904\u7406\u5019\u9009\u89e3\u4e14\u7f3a\u4e4f\u6027\u80fd\u63d0\u5347\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u65e0\u6cd5\u786e\u4fdd\u63a8\u7406\u8d28\u91cf\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u5bf9\u9f50\u59d4\u6258\u6846\u67b6(ALIGN)\uff1a\u4e3b\u7406\u4eba\u5c06\u4efb\u52a1\u59d4\u6258\u7ed9\u591a\u667a\u80fd\u4f53\uff0c\u5728\u8bbe\u8ba1\u6fc0\u52b1\u673a\u5236\u4e0b\u751f\u6210\u5019\u9009\u89e3\uff0c\u518d\u9009\u62e9\u8f93\u51fa\u6700\u7ec8\u7b54\u6848\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u4ea4\u4e92\u4fdd\u6301\u76ee\u6807\u5bf9\u9f50\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u5728\u516c\u5e73\u6bd4\u8f83\u6761\u4ef6\u4e0bALIGN\u663e\u8457\u63d0\u5347\u671f\u671b\u6027\u80fd\uff1b\u5b9e\u8bc1\u663e\u793a\u5176\u5728\u591aLLM\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ALIGN\u901a\u8fc7\u535a\u5f08\u8bba\u6846\u67b6\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u63a8\u7406\u4e2d\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u6027\u80fd\u63d0\u5347\u65b9\u6848\u3002"}}
{"id": "2602.00608", "categories": ["cs.AI", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00608", "abs": "https://arxiv.org/abs/2602.00608", "authors": ["Wei Zeng", "Xuchen Li", "Ruili Feng", "Zhen Liu", "Fengwei An", "Jian Zhao"], "title": "Scalable Generative Game Engine: Breaking the Resolution Wall via Hardware-Algorithm Co-Design", "comment": "Preprint, Under Review", "summary": "Real-time generative game engines represent a paradigm shift in interactive simulation, promising to replace traditional graphics pipelines with neural world models. However, existing approaches are fundamentally constrained by the ``Memory Wall,'' restricting practical deployments to low resolutions (e.g., $64 \\times 64$). This paper bridges the gap between generative models and high-resolution neural simulations by introducing a scalable \\textit{Hardware-Algorithm Co-Design} framework. We identify that high-resolution generation suffers from a critical resource mismatch: the World Model is compute-bound while the Decoder is memory-bound. To address this, we propose a heterogeneous architecture that intelligently decouples these components across a cluster of AI accelerators. Our system features three core innovations: (1) an asymmetric resource allocation strategy that optimizes throughput under sequence parallelism constraints; (2) a memory-centric operator fusion scheme that minimizes off-chip bandwidth usage; and (3) a manifold-aware latent extrapolation mechanism that exploits temporal redundancy to mask latency. We validate our approach on a cluster of programmable AI accelerators, enabling real-time generation at $720 \\times 480$ resolution -- a $50\\times$ increase in pixel throughput over prior baselines. Evaluated on both continuous 3D racing and discrete 2D platformer benchmarks, our system delivers fluid 26.4 FPS and 48.3 FPS respectively, with an amortized effective latency of 2.7 ms. This work demonstrates that resolving the ``Memory Wall'' via architectural co-design is not merely an optimization, but a prerequisite for enabling high-fidelity, responsive neural gameplay.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u786c\u4ef6-\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u5f02\u6784\u67b6\u6784\u5c06\u8ba1\u7b97\u5bc6\u96c6\u7684\u4e16\u754c\u6a21\u578b\u4e0e\u5185\u5b58\u5bc6\u96c6\u89e3\u7801\u5668\u5206\u79bb\u90e8\u7f72\u4e8eAI\u52a0\u901f\u5668\u96c6\u7fa4\uff0c\u7a81\u7834\"\u5185\u5b58\u5899\"\u9650\u5236\uff0c\u5b9e\u73b0720\u00d7480\u5206\u8fa8\u7387\u4e0b\u5b9e\u65f6\u751f\u6210\uff0c\u50cf\u7d20\u541e\u5410\u91cf\u63d0\u534750\u500d\u3002", "motivation": "\u5b9e\u65f6\u751f\u6210\u5f0f\u6e38\u620f\u5f15\u64ce\u53d7\u9650\u4e8e\"\u5185\u5b58\u5899\"\uff0c\u53ea\u80fd\u652f\u6301\u4f4e\u5206\u8fa8\u7387(64\u00d764)\u90e8\u7f72\u3002\u7814\u7a76\u53d1\u73b0\u9ad8\u5206\u8fa8\u7387\u751f\u6210\u5b58\u5728\u5173\u952e\u8d44\u6e90\u9519\u914d\uff1a\u4e16\u754c\u6a21\u578b\u662f\u8ba1\u7b97\u5bc6\u96c6\u578b\uff0c\u800c\u89e3\u7801\u5668\u662f\u5185\u5b58\u5bc6\u96c6\u578b\uff0c\u8fd9\u963b\u788d\u4e86\u9ad8\u4fdd\u771f\u795e\u7ecf\u6a21\u62df\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u53ef\u6269\u5c55\u7684\u786c\u4ef6-\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u91c7\u7528\u5f02\u6784\u67b6\u6784\u667a\u80fd\u89e3\u8026\u4e16\u754c\u6a21\u578b\u4e0e\u89e3\u7801\u5668\u3002\u6838\u5fc3\u521b\u65b0\u5305\u62ec\uff1a(1)\u975e\u5bf9\u79f0\u8d44\u6e90\u5206\u914d\u7b56\u7565\uff0c\u4f18\u5316\u5e8f\u5217\u5e76\u884c\u541e\u5410\u91cf\uff1b(2)\u5185\u5b58\u4e3a\u4e2d\u5fc3\u7684\u7b97\u5b50\u878d\u5408\u65b9\u6848\uff0c\u6700\u5c0f\u5316\u7247\u5916\u5e26\u5bbd\uff1b(3)\u6d41\u5f62\u611f\u77e5\u7684\u6f5c\u7a7a\u95f4\u5916\u63a8\u673a\u5236\uff0c\u5229\u7528\u65f6\u95f4\u5197\u4f59\u63a9\u76d6\u5ef6\u8fdf\u3002", "result": "\u5728\u53ef\u7f16\u7a0bAI\u52a0\u901f\u5668\u96c6\u7fa4\u4e0a\u9a8c\u8bc1\uff0c\u5b9e\u73b0720\u00d7480\u5206\u8fa8\u7387\u5b9e\u65f6\u751f\u6210\uff0c\u50cf\u7d20\u541e\u5410\u91cf\u63d0\u534750\u500d\u3002\u57283D\u8d5b\u8f66\u548c2D\u5e73\u53f0\u6e38\u620f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u8fbe\u523026.4 FPS\u548c48.3 FPS\uff0c\u5747\u644a\u6709\u6548\u5ef6\u8fdf\u4ec52.7 ms\u3002", "conclusion": "\u901a\u8fc7\u67b6\u6784\u534f\u540c\u8bbe\u8ba1\u89e3\u51b3\"\u5185\u5b58\u5899\"\u4e0d\u4ec5\u662f\u4f18\u5316\u624b\u6bb5\uff0c\u66f4\u662f\u5b9e\u73b0\u9ad8\u4fdd\u771f\u3001\u54cd\u5e94\u5f0f\u795e\u7ecf\u6e38\u620f\u4f53\u9a8c\u7684\u5148\u51b3\u6761\u4ef6\uff0c\u4e3a\u751f\u6210\u5f0f\u6e38\u620f\u5f15\u64ce\u7684\u9ad8\u5206\u8fa8\u7387\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2602.00856", "categories": ["quant-ph", "math-ph"], "pdf": "https://arxiv.org/pdf/2602.00856", "abs": "https://arxiv.org/abs/2602.00856", "authors": ["Luca Apadula", "Alessandro Bisio", "Giulio Chiribella", "Paolo Perinotti", "Kyrylo Simonov"], "title": "Higher-order transformations of bidirectional quantum processes", "comment": "24 pages, 8 figures", "summary": "Bidirectional devices are devices for which the roles of the input and output ports can be exchanged. Mathematically, these devices are described by bistochastic quantum channels, namely completely positive linear maps that are both trace-preserving and identity-preserving. Recently, it has been shown that bidirectional quantum devices can, in principle, be used in ways that are incompatible with a definite input-output direction, giving rise to a new phenomenon called input-output indefiniteness. Here we characterize the most general forms of input-output indefiniteness, associated with a hierarchy of higher-order transformations built from transformations of bistochastic quantum channels. Some levels of the hierarchy correspond to transformations that combine bistochastic channels in a definite causal order, while generally using each channel in an indefinite input-output direction. For other levels of the hierarchy, the indefiniteness can involve both the local input-output direction of each process and the global causal order among the processes. On the foundational side, the hierarchy of higher-order transformations characterized here can be regarded as the largest set of physical processes compatible with a time-symmetric variant of quantum theory, where the possible state transformations are restricted to bistochastic channels.", "AI": {"tldr": "\u8be5\u8bba\u6587\u523b\u753b\u4e86\u7531\u53cc\u968f\u673a\u91cf\u5b50\u901a\u9053\u6784\u5efa\u7684\u9ad8\u9636\u53d8\u6362\u5c42\u6b21\u7ed3\u6784\uff0c\u63ed\u793a\u4e86\u8f93\u5165\u8f93\u51fa\u4e0d\u5b9a\u6027\u7684\u6700\u4e00\u822c\u5f62\u5f0f\uff0c\u5e76\u8bc1\u660e\u6b64\u5c42\u6b21\u7ed3\u6784\u662f\u65f6\u95f4\u5bf9\u79f0\u91cf\u5b50\u7406\u8bba\u4e2d\u517c\u5bb9\u7269\u7406\u8fc7\u7a0b\u7684\u6781\u5927\u96c6\u5408\u3002", "motivation": "\u8fd1\u671f\u7814\u7a76\u53d1\u73b0\u53cc\u5411\u91cf\u5b50\u8bbe\u5907\u53ef\u7528\u4e8e\u8f93\u5165\u8f93\u51fa\u65b9\u5411\u4e0d\u786e\u5b9a\u7684\u573a\u666f\uff0c\u4ea7\u751f\u65b0\u73b0\u8c61\"\u8f93\u5165\u8f93\u51fa\u4e0d\u5b9a\u6027\"\uff0c\u4f46\u5bf9\u5176\u6700\u4e00\u822c\u5f62\u5f0f\u7f3a\u4e4f\u5b8c\u6574\u523b\u753b\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u7531\u53cc\u968f\u673a\u91cf\u5b50\u901a\u9053\u7ec4\u6210\u7684\u9ad8\u9636\u53d8\u6362\u5c42\u6b21\u7ed3\u6784\u8fdb\u884c\u5206\u6790\uff0c\u90e8\u5206\u5c42\u6b21\u7ed3\u5408\u786e\u5b9a\u56e0\u679c\u5e8f\u4e0e\u4e0d\u5b9a\u8f93\u5165\u8f93\u51fa\u65b9\u5411\uff0c\u5176\u4ed6\u5c42\u6b21\u540c\u65f6\u6d89\u53ca\u5c40\u90e8\u8f93\u5165\u8f93\u51fa\u4e0d\u5b9a\u6027\u548c\u5168\u5c40\u56e0\u679c\u5e8f\u4e0d\u5b9a\u6027\u3002", "result": "\u6240\u523b\u753b\u7684\u9ad8\u9636\u53d8\u6362\u5c42\u6b21\u7ed3\u6784\u4ee3\u8868\u4e86\u65f6\u95f4\u5bf9\u79f0\u91cf\u5b50\u7406\u8bba\uff08\u9650\u5b9a\u72b6\u6001\u53d8\u6362\u4e3a\u53cc\u968f\u673a\u901a\u9053\uff09\u4e2d\u7269\u7406\u8fc7\u7a0b\u7684\u6781\u5927\u517c\u5bb9\u96c6\u5408\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5efa\u7acb\u4e86\u8f93\u5165\u8f93\u51fa\u4e0d\u5b9a\u6027\u7684\u5b8c\u6574\u7406\u8bba\u6846\u67b6\uff0c\u6df1\u5316\u4e86\u5bf9\u5177\u6709\u4e0d\u5b9a\u56e0\u679c\u7ed3\u6784\u7684\u91cf\u5b50\u8fc7\u7a0b\u7684\u7406\u89e3\uff0c\u4e3a\u91cf\u5b50\u57fa\u7840\u7406\u8bba\u548c\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2602.00128", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.00128", "abs": "https://arxiv.org/abs/2602.00128", "authors": ["Emine Akpinar", "Murat Oduncuoglu"], "title": "Quantum Model Parallelism for MRI-Based Classification of Alzheimer's Disease Stages", "comment": "Under review at Quantum Machine Intelligence (Springer Nature)", "summary": "With increasing life expectancy, AD has become a major global health concern. While classical AI-based methods have been developed for early diagnosis and stage classification of AD, growing data volumes and limited computational resources necessitate faster, more efficient approaches. Quantum-based AI methods, which leverage superposition and entanglement principles along with high-dimensional Hilbert space, can surpass classical approaches' limitations and offer higher accuracy for high-dimensional, heterogeneous, and noisy data. In this study, a Quantum-Based Parallel Model (QBPM) architecture is proposed for the efficient classification of AD stages using MRI datasets, inspired by the principles of classical model parallelism. The proposed model leverages quantum advantages by employing two distinct quantum circuits, each incorporating rotational and entanglement blocks, running in parallel on the same quantum simulator. The classification performance of the model was evaluated on two different datasets to assess its overall robustness and generalization capability. The proposed model demonstrated high classification accuracy across both datasets, highlighting its overall robustness and generalization capability. Results obtained under high-level Gaussian noise, simulating real-world conditions, further provided experimental evidence for the model's applicability not only in theoretical but also in practical scenarios. Moreover, compared with five different classical transfer learning methods, the proposed model demonstrated its efficiency as an alternative to classical approaches by achieving higher classification accuracy and comparable execution time while utilizing fewer circuit parameters. The results indicate that the proposed QBPM architecture represents an innovative and powerful approach for the classification of stages in complex diseases such as Alzheimer's.", "AI": {"tldr": "\u63d0\u51fa\u91cf\u5b50\u5e76\u884c\u6a21\u578b(QBPM)\u7528\u4e8e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5(AD)\u9636\u6bb5\u5206\u7c7b\uff0c\u5229\u7528\u91cf\u5b50\u53e0\u52a0\u548c\u7ea0\u7f20\u7279\u6027\u5904\u7406\u9ad8\u7ef4\u533b\u7597\u6570\u636e\uff0c\u5728MRI\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u9ad8\u51c6\u786e\u7387\uff0c\u6297\u566a\u58f0\u80fd\u529b\u5f3a\uff0c\u4e14\u53c2\u6570\u5c11\u4e8e\u7ecf\u5178\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5", "motivation": "\u968f\u7740\u5bff\u547d\u5ef6\u957f\uff0cAD\u6210\u4e3a\u5168\u7403\u5065\u5eb7\u96be\u9898\uff1b\u7ecf\u5178AI\u65b9\u6cd5\u9762\u4e34\u6570\u636e\u91cf\u589e\u957f\u548c\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\uff0c\u9700\u66f4\u9ad8\u6548\u65b9\u6848\uff1b\u91cf\u5b50AI\u5229\u7528\u53e0\u52a0/\u7ea0\u7f20\u539f\u7406\u548c\u9ad8\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\uff0c\u53ef\u7a81\u7834\u7ecf\u5178\u65b9\u6cd5\u5c40\u9650\uff0c\u5904\u7406\u9ad8\u7ef4\u5f02\u6784\u566a\u58f0\u6570\u636e", "method": "\u63d0\u51fa\u91cf\u5b50\u5e76\u884c\u6a21\u578b(QBPM)\u67b6\u6784\uff1a\u8bbe\u8ba1\u4e24\u4e2a\u542b\u65cb\u8f6c\u548c\u7ea0\u7f20\u6a21\u5757\u7684\u91cf\u5b50\u7535\u8def\u5e76\u884c\u8fd0\u884c\u4e8e\u91cf\u5b50\u6a21\u62df\u5668\uff1b\u5728\u4e24\u4e2aMRI\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u6027\u80fd\uff1b\u6dfb\u52a0\u9ad8\u65af\u566a\u58f0\u6a21\u62df\u771f\u5b9e\u573a\u666f\uff1b\u4e0e\u4e94\u79cd\u7ecf\u5178\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u5bf9\u6bd4", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u5747\u83b7\u5f97\u9ad8\u5206\u7c7b\u51c6\u786e\u7387\uff1b\u9ad8\u566a\u58f0\u73af\u5883\u4e0b\u4ecd\u4fdd\u6301\u9c81\u68d2\u6027\uff1b\u76f8\u6bd4\u7ecf\u5178\u65b9\u6cd5\u51c6\u786e\u7387\u66f4\u9ad8\uff0c\u53c2\u6570\u66f4\u5c11\uff0c\u6267\u884c\u65f6\u95f4\u76f8\u5f53", "conclusion": "QBPM\u4e3a\u590d\u6742\u75be\u75c5(\u5982AD)\u5206\u671f\u5206\u7c7b\u63d0\u4f9b\u521b\u65b0\u6709\u6548\u65b9\u6848\uff0c\u8bc1\u660e\u91cf\u5b50\u65b9\u6cd5\u5728\u533b\u7597AI\u4e2d\u7684\u5b9e\u7528\u6f5c\u529b\uff0c\u53ef\u4f5c\u4e3a\u7ecf\u5178\u65b9\u6cd5\u7684\u8865\u5145\u66ff\u4ee3"}}
{"id": "2602.00611", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00611", "abs": "https://arxiv.org/abs/2602.00611", "authors": ["Jiaqi Xu", "Tao Huang", "Kai Zhang"], "title": "Structured Self-Consistency:A Multi-Task Evaluation of LLMs on VirtualHome", "comment": null, "summary": "Embodied AI requires agents to understand goals, plan actions, and execute tasks in simulated environments.We present a comprehensive evaluation of Large Language Models (LLMs) on the VirtualHome benchmark using the Embodied Agent Interface (EAI) framework.We compare two representative 7B-parameter models OPENPANGU-7B and QWEN2.5-7B across four fundamental tasks: Goal Interpretation, Action Sequencing, Subgoal Decomposition, and Transition Modeling.We propose Structured Self-Consistency (SSC), an enhanced decoding strategy that leverages multiple sampling with domain-specific voting mechanisms to improve output quality for structured generation tasks. Experimental results demonstrate that SSC significantly enhances performance, with OPENPANGU-7B excelling at hierarchical planning while QWEN2.5-7B show advantages in action-level tasks. Our analysis reveals complementary strengths across model types, providing insights for future embodied AI system development.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5728VirtualHome\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8bc4\u4f30\u4e86\u4e24\u79cd7B\u53c2\u6570\u5927\u8bed\u8a00\u6a21\u578b(OPENPANGU-7B\u548cQWEN2.5-7B)\u5728 embodied AI \u56db\u5927\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u7ed3\u6784\u5316\u81ea\u4e00\u81f4\u6027(SSC)\u89e3\u7801\u7b56\u7565\uff0c\u901a\u8fc7\u591a\u91c7\u6837\u548c\u9886\u57df\u7279\u5b9a\u6295\u7968\u673a\u5236\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u5728\u5206\u5c42\u89c4\u5212\u4e0e\u52a8\u4f5c\u7ea7\u4efb\u52a1\u4e0a\u7684\u4e92\u8865\u4f18\u52bf\u3002", "motivation": "Embodied AI\u9700\u8981\u667a\u80fd\u4f53\u5728\u6a21\u62df\u73af\u5883\u4e2d\u7406\u89e3\u76ee\u6807\u3001\u89c4\u5212\u5e76\u6267\u884c\u4efb\u52a1\uff0c\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u5f00\u6e907B\u53c2\u6570\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u5168\u9762\u8bc4\u4f30\uff0c\u4e14\u9700\u8981\u66f4\u6709\u6548\u7684\u89e3\u7801\u7b56\u7565\u6765\u63d0\u5347\u7ed3\u6784\u5316\u751f\u6210\u4efb\u52a1\u7684\u8d28\u91cf\u3002", "method": "\u4f7f\u7528Embodied Agent Interface\u6846\u67b6\u5728VirtualHome\u57fa\u51c6\u4e0a\u6d4b\u8bd5\u4e24\u79cd7B\u6a21\u578b\uff0c\u63d0\u51faStructured Self-Consistency (SSC)\u89e3\u7801\u7b56\u7565\uff0c\u901a\u8fc7\u591a\u91c7\u6837\u548c\u9886\u57df\u7279\u5b9a\u6295\u7968\u673a\u5236\u4f18\u5316\u8f93\u51fa\u8d28\u91cf\uff0c\u8bc4\u4f30\u5176\u5728Goal Interpretation\u3001Action Sequencing\u3001Subgoal Decomposition\u548cTransition Modeling\u56db\u4e2a\u57fa\u7840\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "SSC\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff1bOPENPANGU-7B\u5728\u5206\u5c42\u89c4\u5212\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u800cQWEN2.5-7B\u5728\u52a8\u4f5c\u7ea7\u4efb\u52a1\u4e0a\u5177\u6709\u4f18\u52bf\uff1b\u4e0d\u540c\u6a21\u578b\u5c55\u73b0\u51fa\u4e92\u8865\u6027\u5f3a\u5ea6\u7684\u7279\u70b9\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u672a\u6765embodied AI\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u6d1e\u5bdf\uff0c\u8868\u660e\u7ed3\u5408\u4e0d\u540c\u6a21\u578b\u7684\u4f18\u52bf\u548cSSC\u89e3\u7801\u7b56\u7565\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u667a\u80fd\u4f53\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u6267\u884c\u80fd\u529b\u3002"}}
{"id": "2602.00874", "categories": ["quant-ph", "cs.DS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00874", "abs": "https://arxiv.org/abs/2602.00874", "authors": ["Zhao Song", "Jianfei Xue", "Jiahao Zhang", "Lichen Zhang"], "title": "Sublinear Time Quantum Algorithm for Attention Approximation", "comment": "ICLR 2026", "summary": "Given the query, key and value matrices $Q, K, V\\in \\mathbb{R}^{n\\times d}$, the attention module is defined as $\\mathrm{Att}(Q, K, V)=D^{-1}AV$ where $A=\\exp(QK^\\top/\\sqrt{d})$ with $\\exp(\\cdot)$ applied entrywise, $D=\\mathrm{diag}(A{\\bf 1}_n)$. The attention module is the backbone of modern transformers and large language models, but explicitly forming the softmax matrix $D^{-1}A$ incurs $\u03a9(n^2)$ time, motivating numerous approximation schemes that reduce runtime to $\\widetilde O(nd)$ via sparsity or low-rank factorization.\n  We propose a quantum data structure that approximates any row of $\\mathrm{Att}(Q, K, V)$ using only row queries to $Q, K, V$. Our algorithm preprocesses these matrices in $\\widetilde{O}\\left( \u03b5^{-1} n^{0.5} \\left( s_\u03bb^{2.5} + s_\u03bb^{1.5} d + \u03b1^{0.5} d \\right) \\right)$ time, where $\u03b5$ is the target accuracy, $s_\u03bb$ is the $\u03bb$-statistical dimension of the exponential kernel defined by $Q$ and $K$, and $\u03b1$ measures the row distortion of $V$ that is at most $d/{\\rm srank}(V)$, the stable rank of $V$. Each row query can be answered in $\\widetilde{O}(s_\u03bb^2 + s_\u03bbd)$ time.\n  To our knowledge, this is the first quantum data structure that approximates rows of the attention matrix in sublinear time with respect to $n$. Our approach relies on a quantum Nystr\u00f6m approximation of the exponential kernel, quantum multivariate mean estimation for computing $D$, and quantum leverage score sampling for the multiplication with $V$.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u91cf\u5b50\u6570\u636e\u7ed3\u6784\uff0c\u4ee5O\u0303(s_\u03bb\u00b2 + s_\u03bbd)\u65f6\u95f4\u8fd1\u4f3cTransformer\u6ce8\u610f\u529b\u77e9\u9635\u7684\u4efb\u610f\u884c\uff0c\u9884\u5904\u7406\u65f6\u95f4\u4e3aO\u0303(\u03b5\u207b\u00b9n\u2070\u00b7\u2075(s_\u03bb^2.5 + s_\u03bb^1.5d + \u03b1^0.5d))\uff0c\u9996\u6b21\u5b9e\u73b0\u5e8f\u5217\u957f\u5ea6n\u7684\u4e9a\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u3002", "motivation": "\u6ce8\u610f\u529b\u6a21\u5757\u4f5c\u4e3a\u73b0\u4ee3Transformer\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u77f3\uff0c\u5176\u663e\u5f0f\u8ba1\u7b97softmax\u77e9\u9635D\u207b\u00b9A\u9700\u8981\u03a9(n\u00b2)\u65f6\u95f4\uff0c\u6210\u4e3a\u8ba1\u7b97\u74f6\u9888\u3002\u867d\u7136\u5df2\u6709\u7ecf\u5178\u8fd1\u4f3c\u65b9\u6cd5\u5c06\u590d\u6742\u5ea6\u964d\u81f3O\u0303(nd)\uff0c\u4f46\u672c\u7814\u7a76\u63a2\u7d22\u91cf\u5b50\u8ba1\u7b97\u80fd\u5426\u8fdb\u4e00\u6b65\u5b9e\u73b0\u4e9a\u7ebf\u6027\u65f6\u95f4\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e09\u9879\u91cf\u5b50\u6280\u672f\uff1a1) \u91cf\u5b50Nystr\u00f6m\u8fd1\u4f3c\u5904\u7406\u6307\u6570\u6838\u77e9\u9635A=exp(QK\u1d40/\u221ad)\uff1b2) \u91cf\u5b50\u591a\u5143\u5747\u503c\u4f30\u8ba1\u8ba1\u7b97\u5f52\u4e00\u5316\u77e9\u9635D=diag(A1\u2099)\uff1b3) \u91cf\u5b50\u6760\u6746\u5f97\u5206\u91c7\u6837\u5b9e\u73b0\u4e0eV\u7684\u9ad8\u6548\u4e58\u6cd5\u3002\u4ec5\u901a\u8fc7\u884c\u67e5\u8be2\u5373\u53ef\u8fd1\u4f3c\u6ce8\u610f\u529b\u8f93\u51fa\u3002", "result": "\u9884\u5904\u7406\u590d\u6742\u5ea6\u4e3aO\u0303(\u03b5\u207b\u00b9n\u2070\u00b7\u2075(s_\u03bb^2.5 + s_\u03bb^1.5d + \u03b1^0.5d))\uff0c\u5355\u6b21\u884c\u67e5\u8be2\u590d\u6742\u5ea6\u4e3aO\u0303(s_\u03bb\u00b2 + s_\u03bbd)\uff0c\u5176\u4e2ds_\u03bb\u4e3a\u03bb-\u7edf\u8ba1\u7ef4\u5ea6\uff0c\u03b1\u4e3aV\u7684\u884c\u5931\u771f\u5ea6\uff08\u4e0d\u8d85\u8fc7d/srank(V)\uff09\u3002\u8fd9\u662f\u9996\u4e2a\u5728n\u4e0a\u8fbe\u5230\u4e9a\u7ebf\u6027\u65f6\u95f4\u7684\u91cf\u5b50\u6ce8\u610f\u529b\u8fd1\u4f3c\u7b97\u6cd5\u3002", "conclusion": "\u672c\u5de5\u4f5c\u9996\u6b21\u8bc1\u660e\u91cf\u5b50\u7b97\u6cd5\u53ef\u5728\u4e9a\u7ebf\u6027\u65f6\u95f4\u5185\u8fd1\u4f3c\u6ce8\u610f\u529b\u77e9\u9635\u884c\uff0c\u4e3aTransformer\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u91cf\u5b50\u52a0\u901f\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u91cf\u5b50\u8ba1\u7b97\u5728\u7f13\u89e3\u6ce8\u610f\u529b\u8ba1\u7b97\u74f6\u9888\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.00129", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.00129", "abs": "https://arxiv.org/abs/2602.00129", "authors": ["Yixuan Liang"], "title": "Monte Carlo Tree Search for Execution-Guided Program Repair with Large Language Models", "comment": "10 pages, 5 figures. Submitted to a conference workshop", "summary": "Automated program repair with large language models remains challenging at the repository level due to long-horizon reasoning requirements and the limitations of autoregressive decoding. We present CodePilot, a hybrid framework that integrates Monte Carlo Tree Search (MCTS) with large language models to enable execution-guided program repair for real-world GitHub issues. CodePilot performs hierarchical fault localization from repository to file and function level, explores diverse patch trajectories using MCTS, and leverages execution feedback as a reward signal to guide search and refinement. The framework further incorporates confidence-calibrated generation to selectively refine low-confidence outputs. Experiments on SWE-bench Lite demonstrate that CodePilot achieves a 24.67% issue resolution rate using open-weight models, outperforming comparable baselines. These results suggest that combining symbolic search with neural language models is an effective strategy for scalable, execution-aware software engineering automation.", "AI": {"tldr": "\u63d0\u51faCodePilot\u6846\u67b6\uff0c\u7ed3\u5408\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u5c42\u6545\u969c\u5b9a\u4f4d\u548c\u6267\u884c\u53cd\u9988\u5956\u52b1\u673a\u5236\uff0c\u5b9e\u73b0\u4ed3\u5e93\u7ea7\u522b\u81ea\u52a8\u5316\u7a0b\u5e8f\u4fee\u590d\uff0c\u5728SWE-bench Lite\u4e0a\u8fbe\u523024.67%\u7684\u95ee\u9898\u89e3\u51b3\u7387", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ed3\u5e93\u7ea7\u522b\u81ea\u52a8\u5316\u7a0b\u5e8f\u4fee\u590d\u4e2d\u9762\u4e34\u957f\u7a0b\u63a8\u7406\u9700\u6c42\u4e0e\u81ea\u56de\u5f52\u89e3\u7801\u9650\u5236\u7684\u6311\u6218\uff0c\u5bfc\u81f4\u4fee\u590d\u6548\u679c\u4e0d\u4f73", "method": "\u91c7\u7528\u6df7\u5408\u6846\u67b6CodePilot\uff1a1) \u5206\u5c42\u6545\u969c\u5b9a\u4f4d\uff08\u4ed3\u5e93\u2192\u6587\u4ef6\u2192\u51fd\u6570\u7ea7\uff09\uff1b2) \u4f7f\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u63a2\u7d22\u591a\u6837\u5316\u8865\u4e01\u8def\u5f84\uff1b3) \u5229\u7528\u6267\u884c\u53cd\u9988\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\u5f15\u5bfc\u641c\u7d22\uff1b4) \u5f15\u5165\u7f6e\u4fe1\u5ea6\u6821\u51c6\u751f\u6210\u673a\u5236\u9009\u62e9\u6027\u4f18\u5316\u4f4e\u7f6e\u4fe1\u5ea6\u8f93\u51fa", "result": "\u5728SWE-bench Lite\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCodePilot\u4f7f\u7528\u5f00\u6e90\u6a21\u578b\u5b9e\u73b0\u4e8624.67%\u7684\u95ee\u9898\u89e3\u51b3\u7387\uff0c\u4f18\u4e8e\u53ef\u6bd4\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u5c06\u7b26\u53f7\u641c\u7d22\u4e0e\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u662f\u6784\u5efa\u53ef\u6269\u5c55\u3001\u6267\u884c\u611f\u77e5\u7684\u8f6f\u4ef6\u5de5\u7a0b\u81ea\u52a8\u5316\u7cfb\u7edf\u7684\u6709\u6548\u7b56\u7565"}}
{"id": "2602.00616", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00616", "abs": "https://arxiv.org/abs/2602.00616", "authors": ["Minhyuk Lee", "Hyekyung Yoon", "Myungjoo Kang"], "title": "Inference-Only Prompt Projection for Safe Text-to-Image Generation with TV Guarantees", "comment": null, "summary": "Text-to-Image (T2I) diffusion models enable high-quality open-ended synthesis, but their real-world deployment demands safeguards that suppress unsafe generations without degrading benign prompt-image alignment. We formalize this tension through a total variation (TV) lens: once the reference conditional distribution is fixed, any nontrivial reduction in unsafe generations necessarily incurs TV deviation from the reference, yielding a principled Safety-Prompt Alignment Trade-off (SPAT). Guided by this view, we propose an inference-only prompt projection framework that selectively intervenes on high-risk prompts via a surrogate objective with verification, mapping them into a tolerance-controlled safe set while leaving benign prompts effectively unchanged, without retraining or fine-tuning the generator. Across four datasets and three diffusion backbones, our approach achieves 16.7-60.0% relative reductions in inappropriate percentage (IP) versus strong model-level alignment baselines, while preserving benign prompt-image alignment on COCO near the unaligned reference.", "AI": {"tldr": "Proposes an inference-only prompt projection framework to reduce unsafe text-to-image generations while preserving benign prompt alignment, formalizing a Safety-Prompt Alignment Trade-off (SPAT) through total variation analysis.", "motivation": "Text-to-image diffusion models need safety safeguards, but reducing unsafe generations typically degrades performance on benign prompts; this tension requires principled formalization.", "method": "A selective intervention framework that projects high-risk prompts into tolerance-controlled safe sets via surrogate verification, leaving benign prompts unchanged without retraining or fine-tuning.", "result": "Achieves 16.7-60.0% relative reduction in inappropriate content across 4 datasets and 3 diffusion backbones while maintaining benign prompt-image alignment near unaligned reference levels.", "conclusion": "The approach successfully navigates the SPAT trade-off, providing effective safety guarantees without compromising utility for legitimate prompts through targeted inference-time intervention."}}
{"id": "2602.00891", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.00891", "abs": "https://arxiv.org/abs/2602.00891", "authors": ["Ivy Xiaoya", "Anton M. Graf", "Eric J. Heller", "Joonas Keski-Rahkonen"], "title": "Universal Quantum Birthmark: Ghost of the quantum past", "comment": null, "summary": "Quantum dynamics retains a permanent and universal memory of its initial conditions, even in systems whose spectra display fully chaotic, random-matrix behavior. This effect, known as the quantum birthmark, appears as an enhancement of the long-time return probability of any non-stationary state compared to the overlap with a typical ergodic state. In this work, we develop the full theoretical foundation for this universal contribution that depends only on the global symmetry class and accessible Hilbert-space dimension, not on the microscopic dynamics. Our findings reveal that quantum evolution preserves an unavoidable, symmetry-controlled imprint of its origin, a quantum effect calling into question classical expectations of ergodicity and the resulting thermalization scenarios.", "AI": {"tldr": "\u91cf\u5b50\u7cfb\u7edf\u5728\u6df7\u6c8c\u72b6\u6001\u4e0b\u4ecd\u4fdd\u7559\u521d\u59cb\u6761\u4ef6\u7684\u6c38\u4e45\u8bb0\u5fc6\uff08\u91cf\u5b50\u51fa\u751f\u5370\u8bb0\uff09\uff0c\u8868\u73b0\u4e3a\u4efb\u610f\u975e\u5b9a\u6001\u7684\u957f\u65f6\u95f4\u8fd4\u56de\u6982\u7387\u589e\u5f3a\uff0c\u8be5\u6548\u5e94\u4ec5\u53d6\u51b3\u4e8e\u5bf9\u79f0\u6027\u7c7b\u548c\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u7ef4\u5ea6\uff0c\u6311\u6218\u4e86\u7ecf\u5178\u904d\u5386\u6027\u548c\u70ed\u5316\u9884\u671f", "motivation": "\u63ed\u793a\u91cf\u5b50\u6df7\u6c8c\u7cfb\u7edf\u4e2d\u666e\u904d\u5b58\u5728\u7684\u521d\u59cb\u6761\u4ef6\u8bb0\u5fc6\u6548\u5e94\uff08\u91cf\u5b50\u51fa\u751f\u5370\u8bb0\uff09\uff0c\u89e3\u91ca\u5176\u4e0e\u7ecf\u5178\u904d\u5386\u6027\u9884\u671f\u7684\u77db\u76fe", "method": "\u5efa\u7acb\u5b8c\u6574\u7684\u7406\u8bba\u6846\u67b6\uff0c\u8bc1\u660e\u8be5\u6548\u5e94\u4ec5\u7531\u5168\u5c40\u5bf9\u79f0\u6027\u7c7b\u548c\u53ef\u53ca\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u7ef4\u5ea6\u51b3\u5b9a\uff0c\u4e0e\u5fae\u89c2\u52a8\u529b\u5b66\u7ec6\u8282\u65e0\u5173", "result": "\u53d1\u73b0\u91cf\u5b50\u6f14\u5316\u5fc5\u7136\u4fdd\u7559\u5bf9\u79f0\u6027\u63a7\u5236\u7684\u521d\u59cb\u5370\u8bb0\uff0c\u8868\u73b0\u4e3a\u975e\u5b9a\u6001\u6bd4\u5178\u578b\u904d\u5386\u6001\u5177\u6709\u66f4\u9ad8\u7684\u957f\u65f6\u95f4\u8fd4\u56de\u6982\u7387", "conclusion": "\u91cf\u5b50\u51fa\u751f\u5370\u8bb0\u662f\u666e\u9002\u7684\u91cf\u5b50\u6548\u5e94\uff0c\u8d28\u7591\u4e86\u7ecf\u5178\u904d\u5386\u6027\u5047\u8bbe\u53ca\u7531\u6b64\u63a8\u5bfc\u7684\u70ed\u5316\u573a\u666f\uff0c\u8868\u660e\u91cf\u5b50\u7cfb\u7edf\u65e0\u6cd5\u5b8c\u5168\u9057\u5fd8\u521d\u59cb\u4fe1\u606f"}}
{"id": "2602.00659", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00659", "abs": "https://arxiv.org/abs/2602.00659", "authors": ["Qusai Khaled", "Laura Genga", "Uzay Kaymak"], "title": "Predictive Maintenance for Ultrafiltration Membranes Using Explainable Similarity-Based Prognostics", "comment": "Submitted to 21st International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems (IPMU2026)", "summary": "In reverse osmosis desalination, ultrafiltration (UF) membranes degrade due to fouling, leading to performance loss and costly downtime. Most plants rely on scheduled preventive maintenance, since existing predictive maintenance models, often based on opaque machine learning methods, lack interpretability and operator trust. This study proposes an explainable prognostic framework for UF membrane remaining useful life (RUL) estimation using fuzzy similarity reasoning. A physics-informed Health Index, derived from transmembrane pressure, flux, and resistance, captures degradation dynamics, which are then fuzzified via Gaussian membership functions. Using a similarity measure, the model identifies historical degradation trajectories resembling the current state and formulates RUL predictions as Takagi-Sugeno fuzzy rules. Each rule corresponds to a historical exemplar and contributes to a transparent, similarity-weighted RUL estimate. Tested on 12,528 operational cycles from an industrial-scale UF system, the framework achieved a mean absolute error of 4.50 cycles, while generating interpretable rule bases consistent with expert understanding.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6a21\u7cca\u76f8\u4f3c\u5ea6\u63a8\u7406\u7684\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u7528\u4e8e\u8d85\u6ee4\u819c\u5269\u4f59\u4f7f\u7528\u5bff\u547d\u9884\u6d4b\uff0c\u89e3\u51b3\u5de5\u4e1a\u8131\u76d0\u4e2d\u819c\u6c61\u67d3\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u548c\u7ef4\u62a4\u4fe1\u4efb\u95ee\u9898", "motivation": "\u73b0\u6709\u9884\u6d4b\u6027\u7ef4\u62a4\u6a21\u578b\u591a\u4e3a\u4e0d\u900f\u660e\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u5bfc\u81f4\u64cd\u4f5c\u5458\u4e0d\u4fe1\u4efb\uff1b\u8d85\u6ee4\u819c\u6c61\u67d3\u9020\u6210\u6027\u80fd\u635f\u5931\u548c\u9ad8\u6602\u505c\u673a\u6210\u672c\uff0c\u9700\u66ff\u4ee3\u5b9a\u671f\u9884\u9632\u6027\u7ef4\u62a4", "method": "\u91c7\u7528\u7269\u7406\u4fe1\u606f\u5065\u5eb7\u6307\u6570\uff08\u8de8\u819c\u538b/\u901a\u91cf/\u963b\u529b\uff09\u8868\u5f81\u9000\u5316\uff0c\u901a\u8fc7\u9ad8\u65af\u96b6\u5c5e\u51fd\u6570\u91cf\u5316\uff1b\u5229\u7528\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u5339\u914d\u5386\u53f2\u9000\u5316\u8f68\u8ff9\uff0c\u6784\u5efaTakagi-Sugeno\u6a21\u7cca\u89c4\u5219\u5e93\u8fdb\u884c\u900f\u660e\u5316RUL\u4f30\u8ba1", "result": "\u572812,528\u4e2a\u5de5\u4e1a\u7ea7UF\u7cfb\u7edf\u8fd0\u884c\u5468\u671f\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4ec54.50\u5faa\u73af\uff1b\u751f\u6210\u7684\u53ef\u89e3\u91ca\u89c4\u5219\u5e93\u4e0e\u4e13\u5bb6\u8ba4\u77e5\u4e00\u81f4", "conclusion": "\u8be5\u6846\u67b6\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u63d0\u4f9b\u900f\u660e\u51b3\u7b56\u4f9d\u636e\uff0c\u589e\u5f3a\u64cd\u4f5c\u5458\u4fe1\u4efb\uff0c\u4e3a\u5de5\u4e1a\u819c\u7cfb\u7edf\u9884\u6d4b\u6027\u7ef4\u62a4\u63d0\u4f9b\u53ef\u884c\u65b9\u6848"}}
{"id": "2602.00900", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.00900", "abs": "https://arxiv.org/abs/2602.00900", "authors": ["Andesson B. Nascimento", "Lucas Chibebe C\u00e9leri"], "title": "Asymmetry and dynamical criticality", "comment": "Comments are very welcome! 10 pages. 7 figures", "summary": "Symmetries play a central role in both equilibrium and nonequilibrium phase transitions, yet their quantitative characterization in dynamical quantum phase transitions (DQPTs) remains an open challenge. In this work, we establish a direct connection between symmetry properties of a many-body model and measures of quantum asymmetry, showing that asymmetry monotones provide a robust and physically transparent indicator of dynamical quantum criticality. Focusing on the quenched Lipkin-Meshkov-Glick model, we demonstrate that asymmetry measures associated with collective spin generators faithfully capture the onset of DQPTs, reflecting the dynamical restoration or breaking of underlying symmetries. Remarkably, the time-averaged asymmetry exhibits clear signatures of the dynamical critical point, in close correspondence with both the dynamical order parameter and the behavior of entropy production. We further uncover a quantitative link between asymmetry generation and thermodynamic irreversibility, showing that peaks in asymmetry coincide with maximal entropy production across the transition. Our results position asymmetry as a unifying concept bridging symmetry, information-theoretic quantifiers, and nonequilibrium thermodynamics in dynamical quantum phase transitions, providing a powerful framework for understanding critical dynamics beyond traditional order parameters.", "AI": {"tldr": "\u672c\u6587\u5efa\u7acb\u4e86\u91cf\u5b50\u4e0d\u5bf9\u79f0\u6027\u5ea6\u91cf\u4e0e\u52a8\u529b\u5b66\u91cf\u5b50\u76f8\u53d8\u4e2d\u5bf9\u79f0\u6027\u6027\u8d28\u4e4b\u95f4\u7684\u76f4\u63a5\u8054\u7cfb\uff0c\u5229\u7528Lipkin-Meshkov-Glick\u6a21\u578b\u8bc1\u660e\u4e0d\u5bf9\u79f0\u6027\u53ef\u4ee5\u4f5c\u4e3a\u52a8\u529b\u5b66\u91cf\u5b50\u4e34\u754c\u6027\u7684\u9c81\u68d2\u6307\u6807\uff0c\u5e76\u4e0e\u70ed\u529b\u5b66\u4e0d\u53ef\u9006\u6027\u76f8\u5173\u8054\u3002", "motivation": "\u52a8\u529b\u5b66\u91cf\u5b50\u76f8\u53d8\u4e2d\u5bf9\u79f0\u6027\u7684\u5b9a\u91cf\u8868\u5f81\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u6311\u6218\uff0c\u9700\u8981\u8d85\u8d8a\u4f20\u7edf\u5e8f\u53c2\u91cf\u7684\u3001\u80fd\u591f\u9c81\u68d2\u6307\u793a\u52a8\u529b\u5b66\u91cf\u5b50\u4e34\u754c\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5efa\u7acb\u591a\u4f53\u6a21\u578b\u5bf9\u79f0\u6027\u8d28\u4e0e\u91cf\u5b50\u4e0d\u5bf9\u79f0\u6027\u5ea6\u91cf\u4e4b\u95f4\u7684\u76f4\u63a5\u8054\u7cfb\uff0c\u91cd\u70b9\u7814\u7a76\u731d\u706dLipkin-Meshkov-Glick\u6a21\u578b\uff0c\u5229\u7528\u96c6\u4f53\u81ea\u65cb\u751f\u6210\u5143\u76f8\u5173\u7684\u975e\u5bf9\u79f0\u6027\u5355\u8c03\u91cf\u8fdb\u884c\u7814\u7a76\u3002", "result": "\u975e\u5bf9\u79f0\u6027\u5ea6\u91cf\u80fd\u591f\u51c6\u786e\u6355\u6349DQPTs\u7684\u53d1\u751f\uff0c\u65f6\u95f4\u5e73\u5747\u4e0d\u5bf9\u79f0\u6027\u6e05\u6670\u5730\u663e\u793a\u51fa\u52a8\u529b\u5b66\u4e34\u754c\u70b9\u7684\u7279\u5f81\uff0c\u4e0d\u5bf9\u79f0\u6027\u5cf0\u503c\u4e0e\u6700\u5927\u71b5\u4ea7\u751f\u76f8\u543b\u5408\uff0c\u63ed\u793a\u4e86\u975e\u5bf9\u79f0\u6027\u4ea7\u751f\u4e0e\u70ed\u529b\u5b66\u4e0d\u53ef\u9006\u6027\u4e4b\u95f4\u7684\u5b9a\u91cf\u8054\u7cfb\u3002", "conclusion": "\u975e\u5bf9\u79f0\u6027\u4f5c\u4e3a\u8fde\u63a5\u5bf9\u79f0\u6027\u3001\u4fe1\u606f\u8bba\u91cf\u5ea6\u548c\u975e\u5e73\u8861\u70ed\u529b\u5b66\u7684\u7edf\u4e00\u6982\u5ff5\uff0c\u4e3a\u7406\u89e3\u8d85\u8d8a\u4f20\u7edf\u5e8f\u53c2\u91cf\u7684\u4e34\u754c\u52a8\u529b\u5b66\u63d0\u4f9b\u4e86\u5f3a\u5927\u6846\u67b6\u3002"}}
{"id": "2602.00158", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00158", "abs": "https://arxiv.org/abs/2602.00158", "authors": ["Ziqi Gao", "Yaotian Zhu", "Qingcheng Zeng", "Xu Zhao", "Ziqing Wang", "Feng Ruan", "Kaize Ding"], "title": "RAPTOR: Ridge-Adaptive Logistic Probes", "comment": "Preprint", "summary": "Probing studies what information is encoded in a frozen LLM's layer representations by training a lightweight predictor on top of them. Beyond analysis, probes are often used operationally in probe-then-steer pipelines: a learned concept vector is extracted from a probe and injected via additive activation steering by adding it to a layer representation during the forward pass. The effectiveness of this pipeline hinges on estimating concept vectors that are accurate, directionally stable under ablation, and inexpensive to obtain. Motivated by these desiderata, we propose RAPTOR (Ridge-Adaptive Logistic Probe), a simple L2-regularized logistic probe whose validation-tuned ridge strength yields concept vectors from normalized weights. Across extensive experiments on instruction-tuned LLMs and human-written concept datasets, RAPTOR matches or exceeds strong baselines in accuracy while achieving competitive directional stability and substantially lower training cost; these quantitative results are supported by qualitative downstream steering demonstrations. Finally, using the Convex Gaussian Min-max Theorem (CGMT), we provide a mechanistic characterization of ridge logistic regression in an idealized Gaussian teacher-student model in the high-dimensional few-shot regime, explaining how penalty strength mediates probe accuracy and concept-vector stability and yielding structural predictions that qualitatively align with trends observed on real LLM embeddings.", "AI": {"tldr": "\u63d0\u51faRAPTOR\u63a2\u9488\u65b9\u6cd5\uff0c\u901a\u8fc7L2\u6b63\u5219\u5316\u903b\u8f91\u56de\u5f52\u83b7\u5f97\u66f4\u51c6\u786e\u3001\u7a33\u5b9a\u4e14\u4f4e\u6210\u672c\u7684\u6982\u5ff5\u5411\u91cf\uff0c\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6fc0\u6d3b\u5f15\u5bfc\uff0c\u5e76\u7ed9\u51fa\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u7684\u7406\u8bba\u89e3\u91ca\u3002", "motivation": "\u73b0\u6709\u63a2\u9488-\u5f15\u5bfc\u6d41\u7a0b\u9700\u8981\u51c6\u786e\u3001\u65b9\u5411\u7a33\u5b9a\u4e14\u4f4e\u6210\u672c\u7684\u6982\u5ff5\u5411\u91cf\uff0c\u4f46\u4f20\u7edf\u63a2\u9488\u65b9\u6cd5\u5728\u8fd9\u4e9b\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u63d0\u51faRAPTOR\uff08\u5cad\u81ea\u9002\u5e94\u903b\u8f91\u63a2\u9488\uff09\uff0c\u91c7\u7528L2\u6b63\u5219\u5316\u903b\u8f91\u56de\u5f52\uff0c\u901a\u8fc7\u9a8c\u8bc1\u96c6\u8c03\u6574\u7684\u5cad\u5f3a\u5ea6\u4ece\u5f52\u4e00\u5316\u6743\u91cd\u4e2d\u751f\u6210\u6982\u5ff5\u5411\u91cf\uff0c\u5e76\u7ed3\u5408\u51f8\u9ad8\u65af\u6781\u5c0f\u6781\u5927\u5b9a\u7406\uff08CGMT\uff09\u5728\u9ad8\u7ef4\u5c11\u6837\u672c regime \u4e0b\u5efa\u7acb\u7406\u60f3\u5316\u9ad8\u65af\u5e08\u751f\u6a21\u578b\u8fdb\u884c\u673a\u7406\u5206\u6790\u3002", "result": "\u5728\u6307\u4ee4\u8c03\u4f18\u5927\u6a21\u578b\u548c\u4eba\u7c7b\u7f16\u5199\u6982\u5ff5\u6570\u636e\u96c6\u4e0a\uff0cRAPTOR\u5728\u7cbe\u5ea6\u4e0a\u8fbe\u5230\u6216\u8d85\u8fc7\u5f3a\u57fa\u7ebf\uff0c\u540c\u65f6\u5177\u6709\u6709\u7ade\u4e89\u529b\u7684\u65b9\u5411\u7a33\u5b9a\u6027\u548c\u663e\u8457\u66f4\u4f4e\u7684\u8bad\u7ec3\u6210\u672c\uff1b\u7406\u8bba\u5206\u6790\u63ed\u793a\u4e86\u60e9\u7f5a\u5f3a\u5ea6\u5982\u4f55\u8c03\u8282\u63a2\u9488\u7cbe\u5ea6\u548c\u6982\u5ff5\u5411\u91cf\u7a33\u5b9a\u6027\u3002", "conclusion": "RAPTOR\u4e3a\u63a2\u9488-\u5f15\u5bfc pipeline \u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u63a2\u9488\u65b9\u6848\uff0c\u5176\u7406\u8bba\u6846\u67b6\u6709\u52a9\u4e8e\u7406\u89e3\u63a2\u9488\u884c\u4e3a\u5e76\u6307\u5bfc\u672a\u6765\u8bbe\u8ba1\u3002"}}
{"id": "2602.00676", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.00676", "abs": "https://arxiv.org/abs/2602.00676", "authors": ["Chao Li", "Shangdong Yang", "Chiheng Zhan", "Zhenxing Ge", "Yujing Hu", "Bingkun Bao", "Xingguo Chen", "Yang Gao"], "title": "OpenGuanDan: A Large-Scale Imperfect Information Game Benchmark", "comment": null, "summary": "The advancement of data-driven artificial intelligence (AI), particularly machine learning, heavily depends on large-scale benchmarks. Despite remarkable progress across domains ranging from pattern recognition to intelligent decision-making in recent decades, exemplified by breakthroughs in board games, card games, and electronic sports games, there remains a pressing need for more challenging benchmarks to drive further research. To this end, this paper proposes OpenGuanDan, a novel benchmark that enables both efficient simulation of GuanDan (a popular four-player, multi-round Chinese card game) and comprehensive evaluation of both learning-based and rule-based GuanDan AI agents. OpenGuanDan poses a suite of nontrivial challenges, including imperfect information, large-scale information set and action spaces, a mixed learning objective involving cooperation and competition, long-horizon decision-making, variable action spaces, and dynamic team composition. These characteristics make it a demanding testbed for existing intelligent decision-making methods. Moreover, the independent API for each player allows human-AI interactions and supports integration with large language models. Empirically, we conduct two types of evaluations: (1) pairwise competitions among all GuanDan AI agents, and (2) human-AI matchups. Experimental results demonstrate that while current learning-based agents substantially outperform rule-based counterparts, they still fall short of achieving superhuman performance, underscoring the need for continued research in multi-agent intelligent decision-making domain. The project is publicly available at https://github.com/GameAI-NJUPT/OpenGuanDan.", "AI": {"tldr": "\u63d0\u51faOpenGuanDan\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u5728\u590d\u6742\u56db\u4eba\u7eb8\u724c\u6e38\u620f\"\u63bc\u86cb\"\u4e2d\u7684\u8868\u73b0\uff0c\u63ed\u793a\u5f53\u524dAI\u4ecd\u65e0\u6cd5\u8d85\u8d8a\u4eba\u7c7b\u6c34\u5e73\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "motivation": "\u73b0\u6709AI\u57fa\u51c6\uff08\u5982\u68cb\u7c7b\u3001\u7535\u7ade\u6e38\u620f\uff09\u5df2\u76f8\u5bf9\u6210\u719f\uff0c\u9700\u8981\u66f4\u5177\u6311\u6218\u6027\u7684\u6d4b\u8bd5\u5e73\u53f0\u6765\u63a8\u52a8\u591a\u667a\u80fd\u4f53\u51b3\u7b56\u7814\u7a76\u3002\u63bc\u86cb\u6e38\u620f\u5177\u5907\u4e0d\u5b8c\u5168\u4fe1\u606f\u3001\u5927\u89c4\u6a21\u72b6\u6001\u52a8\u4f5c\u7a7a\u95f4\u3001\u5408\u4f5c\u7ade\u4e89\u6df7\u5408\u76ee\u6807\u7b49\u7279\u6027\uff0c\u662f\u7406\u60f3\u7684\u590d\u6742\u51b3\u7b56\u57fa\u51c6\u3002", "method": "\u6784\u5efaOpenGuanDan\u57fa\u51c6\uff0c\u652f\u6301\u9ad8\u6548\u6e38\u620f\u6a21\u62df\uff0c\u63d0\u4f9b\u7edf\u4e00API\u8bc4\u4f30\u5b66\u4e60\u4e0e\u89c4\u5219\u57faAI\u667a\u80fd\u4f53\uff0c\u652f\u6301\u4eba\u673a\u4ea4\u4e92\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u3002\u5176\u6838\u5fc3\u6311\u6218\u5305\u62ec\uff1a\u4e0d\u5b8c\u5168\u4fe1\u606f\u3001\u957f\u65f6\u5e8f\u51b3\u7b56\u3001\u53d8\u957f\u52a8\u4f5c\u7a7a\u95f4\u3001\u52a8\u6001\u7ec4\u961f\u7b49\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a1\uff09\u5b66\u4e60\u57fa\u667a\u80fd\u4f53\u663e\u8457\u4f18\u4e8e\u89c4\u5219\u57fa\u667a\u80fd\u4f53\uff1b2\uff09\u4f46\u6240\u6709\u5f53\u524dAI\u5747\u672a\u80fd\u8fbe\u5230\u8d85\u4eba\u7c7b\u6c34\u5e73\uff0c\u4e0e\u4eba\u7c7b\u73a9\u5bb6\u5bf9\u5c40\u65f6\u4ecd\u6709\u660e\u663e\u5dee\u8ddd\u3002", "conclusion": "OpenGuanDan\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u591a\u667a\u80fd\u4f53\u51b3\u7b56\u6d4b\u8bd5\u5e73\u53f0\uff0c\u73b0\u6709AI\u65b9\u6cd5\u5c1a\u672a\u89e3\u51b3\u5176\u590d\u6742\u7279\u6027\uff0c\u8be5\u57fa\u51c6\u5c06\u63a8\u52a8\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e0e\u535a\u5f08\u8bba\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.00685", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00685", "abs": "https://arxiv.org/abs/2602.00685", "authors": ["Xuan Liu", "Haoyang Shang", "Zizhang Liu", "Xinyan Liu", "Yunze Xiao", "Yiwen Tu", "Haojian Jin"], "title": "HumanStudy-Bench: Towards AI Agent Design for Participant Simulation", "comment": null, "summary": "Large language models (LLMs) are increasingly used as simulated participants in social science experiments, but their behavior is often unstable and highly sensitive to design choices. Prior evaluations frequently conflate base-model capabilities with experimental instantiation, obscuring whether outcomes reflect the model itself or the agent setup. We instead frame participant simulation as an agent-design problem over full experimental protocols, where an agent is defined by a base model and a specification (e.g., participant attributes) that encodes behavioral assumptions. We introduce HUMANSTUDY-BENCH, a benchmark and execution engine that orchestrates LLM-based agents to reconstruct published human-subject experiments via a Filter--Extract--Execute--Evaluate pipeline, replaying trial sequences and running the original analysis pipeline in a shared runtime that preserves the original statistical procedures end to end. To evaluate fidelity at the level of scientific inference, we propose new metrics to quantify how much human and agent behaviors agree. We instantiate 12 foundational studies as an initial suite in this dynamic benchmark, spanning individual cognition, strategic interaction, and social psychology, and covering more than 6,000 trials with human samples ranging from tens to over 2,100 participants.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHUMANSTUDY-BENCH\u57fa\u51c6\uff0c\u901a\u8fc7\u8fc7\u6ee4-\u63d0\u53d6-\u6267\u884c-\u8bc4\u4f30\u6d41\u6c34\u7ebf\u4e25\u683c\u6d4b\u8bd5LLM\u667a\u80fd\u4f53\uff0c\u91cd\u5efa12\u9879\u4eba\u7c7b\u5b9e\u9a8c\u4ee5\u5206\u79bb\u6a21\u578b\u80fd\u529b\u4e0e\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u590d\u5236\u548c\u6070\u5f53\u8bc4\u4f30\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u7528\u4f5c\u793e\u4f1a\u79d1\u5b66\u5b9e\u9a8c\u7684\u6a21\u62df\u53c2\u4e0e\u8005\uff0c\u4f46\u5176\u884c\u4e3a\u901a\u5e38\u4e0d\u7a33\u5b9a\u4e14\u5bf9\u8bbe\u8ba1\u9009\u62e9\u9ad8\u5ea6\u654f\u611f\u3002\u5148\u524d\u7684\u8bc4\u4f30\u7ecf\u5e38\u6df7\u6dc6\u57fa\u7840\u6a21\u578b\u80fd\u529b\u4e0e\u5b9e\u9a8c\u5b9e\u4f8b\u5316\uff0c\u6a21\u7cca\u4e86\u7ed3\u679c\u7a76\u7adf\u53cd\u6620\u6a21\u578b\u672c\u8eab\u8fd8\u662f\u667a\u80fd\u4f53\u8bbe\u7f6e\u3002", "method": "\u5c06\u53c2\u4e0e\u8005\u6a21\u62df\u6784\u5efa\u4e3a\u5b8c\u6574\u5b9e\u9a8c\u534f\u8bae\u4e0a\u7684\u667a\u80fd\u4f53\u8bbe\u8ba1\u95ee\u9898\uff0c\u901a\u8fc7HUMANSTUDY-BENCH\u57fa\u51c6\u548c\u6d4b\u8bd5\u5f15\u64ce\uff0c\u91c7\u7528\u8fc7\u6ee4-\u63d0\u53d6-\u6267\u884c-\u8bc4\u4f30\u6d41\u6c34\u7ebf\u91cd\u5efa\u5df2\u53d1\u8868\u7684\u4eba\u7c7b\u5b9e\u9a8c\uff0c\u5728\u5171\u4eab\u8fd0\u884c\u65f6\u4e2d\u91cd\u653e\u8bd5\u9a8c\u5e8f\u5217\u5e76\u8fd0\u884c\u539f\u59cb\u5206\u6790\u6d41\u7a0b\uff0c\u4fdd\u7559\u539f\u59cb\u7edf\u8ba1\u8fc7\u7a0b\u3002\u63d0\u51fa\u65b0\u6307\u6807\u91cf\u5316\u4eba\u7c7b\u4e0e\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u4e00\u81f4\u6027\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b12\u9879\u57fa\u7840\u7814\u7a76\u7684\u52a8\u6001\u57fa\u51c6\u5957\u4ef6\uff0c\u6db5\u76d6\u4e2a\u4f53\u8ba4\u77e5\u3001\u6218\u7565\u4e92\u52a8\u548c\u793e\u4f1a\u5fc3\u7406\u5b66\uff0c\u5305\u542b\u8d85\u8fc76,000\u6b21\u8bd5\u9a8c\u53ca\u4ece\u6570\u5341\u4eba\u52302,100\u591a\u4eba\u7684\u4e0d\u540c\u4eba\u7c7b\u6837\u672c\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5c06\u6a21\u578b\u80fd\u529b\u4e0e\u5b9e\u9a8c\u8bbe\u8ba1\u5206\u79bb\uff0c\u4e3a\u793e\u4f1a\u79d1\u5b66\u5b9e\u9a8c\u4e2d\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u4e25\u8c28\u7684\u65b9\u6cd5\uff0c\u5e76\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u7684\u539f\u59cb\u7814\u7a76\u590d\u5236\u3002"}}
{"id": "2602.01043", "categories": ["quant-ph", "physics.hist-ph"], "pdf": "https://arxiv.org/pdf/2602.01043", "abs": "https://arxiv.org/abs/2602.01043", "authors": ["Jacob A. Barandes"], "title": "A Deflationary Account of Quantum Theory and its Implications for the Complex Numbers", "comment": "15 pages, no figures", "summary": "Why does quantum theory need the complex numbers? With a view toward answering this question, this paper argues that the usual Hilbert-space formalism is a special case of the general method of Markovian embeddings. This paper then describes the indivisible interpretation of quantum theory, according to which a quantum system can be regarded as an indivisible stochastic process unfolding in an old-fashioned configuration space, with wave functions and other exotic Hilbert-space ingredients demoted from having an ontological status. The complex numbers end up being necessary to ensure that the Hilbert-space formalism is indeed a Markovian embedding.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u5d4c\u5165\u65b9\u6cd5\u8bba\u8bc1\uff1a\u91cf\u5b50\u7406\u8bba\u9700\u8981\u590d\u6570\u7684\u6839\u672c\u539f\u56e0\u5728\u4e8e\uff0c\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4 formalism \u5fc5\u987b\u4f5c\u4e3a\u9a6c\u5c14\u53ef\u592b\u5d4c\u5165\u7684\u7279\u4f8b\u624d\u80fd\u6210\u7acb\uff0c\u800c\u6ce2\u51fd\u6570\u7b49\u5143\u7d20\u5728\u672c\u4f53\u8bba\u4e0a\u5e94\u88ab\u964d\u7ea7\u4e3a\u6570\u5b66\u5de5\u5177\u3002", "motivation": "\u63a2\u8ba8\u91cf\u5b50\u7406\u8bba\u4e3a\u4f55\u5fc5\u987b\u4f7f\u7528\u590d\u6570\u8fd9\u4e00\u57fa\u7840\u95ee\u9898\uff0c\u8d28\u7591\u4f20\u7edf\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u5f62\u5f0f\u4f53\u7cfb\u7684\u5fc5\u7136\u6027\u3002", "method": "\u5c06\u6807\u51c6\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4 formalism \u63a8\u5e7f\u4e3a\u66f4\u4e00\u822c\u7684\u9a6c\u5c14\u53ef\u592b\u5d4c\u5165\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\"\u4e0d\u53ef\u5206\u89e3\u91ca\"\u2014\u2014\u5c06\u91cf\u5b50\u7cfb\u7edf\u89c6\u4e3a\u7ecf\u5178\u4f4d\u5f62\u7a7a\u95f4\u4e2d\u4e0d\u53ef\u5206\u7684\u968f\u673a\u8fc7\u7a0b\u3002", "result": "\u8bc1\u660e\u590d\u6570\u5bf9\u4e8e\u786e\u4fdd\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4 formalism \u80fd\u6709\u6548\u5b9e\u73b0\u4e3a\u9a6c\u5c14\u53ef\u592b\u5d4c\u5165\u5177\u6709\u5fc5\u8981\u6027\uff0c\u540c\u65f6\u5265\u79bb\u4e86\u6ce2\u51fd\u6570\u7684\u672c\u4f53\u8bba\u5730\u4f4d\u3002", "conclusion": "\u590d\u6570\u5e76\u975e\u91cf\u5b50\u7406\u8bba\u7684\u672c\u8d28\u7279\u5f81\uff0c\u800c\u662f\u7ef4\u6301\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4f5c\u4e3a\u6709\u6548\u6570\u5b66\u5d4c\u5165\u5de5\u5177\u7684\u5fc5\u8981\u6761\u4ef6\uff1b\u91cf\u5b50\u73b0\u8c61\u53ef\u8fd8\u539f\u4e3a\u7ecf\u5178\u968f\u673a\u8fc7\u7a0b\u3002"}}
{"id": "2602.00165", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00165", "abs": "https://arxiv.org/abs/2602.00165", "authors": ["Arthur Negr\u00e3o", "Pedro Silva", "Vander L. S. Freitas", "Gladston Moreira", "Eduardo Luz"], "title": "Benford's Law as a Distributional Prior for Post-Training Quantization of Large Language Models", "comment": null, "summary": "The rapid growth of Large Language Models (LLMs) intensifies the need for effective compression, with weight quantization being the most widely adopted technique. Standard uniform quantizers assume that parameters are evenly distributed, an assumption at odds with the highly skewed distributions observed in practice. We propose Benford-Quant, a simple, data-free non-uniform quantizer inspired by Benford's Law, which predicts that leading digits follow a logarithmic distribution. Benford-Quant replaces the uniform grid with a log-spaced codebook, dedicating more resolution to the frequent small-magnitude weights. We provide both theoretical intuition and empirical evidence: (i) weights in transformer transformational layers adhere closely to Benford statistics, while normalization layers systematically deviate; (ii) on Small Language Models (SLMs), Benford-Quant consistently improves perplexity, reducing 4-bit perplexity on Gemma-270M by more than 10%; and (iii) on larger LLMs, it remains competitive, with differences explained by over-parameterization effects. Our results indicate that incorporating a Benford-inspired prior into quantization grids is a low-cost modification that yields accuracy gains in aggressive few-bit regimes. Although it is not able to surpass the state of the art in tasks such as perplexity and LAMBADA, the Benford-Quant approach can be hybridized with other quantization methods-such as SmoothQuant and Activation-Aware Quantization-without major pipeline modification, potentially improving their performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faBenford-Quant\uff0c\u4e00\u79cd\u53d7\u672c\u798f\u7279\u5b9a\u5f8b\u542f\u53d1\u7684\u6570\u636e\u65e0\u5173\u975e\u5747\u5300\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u91c7\u7528\u5bf9\u6570\u95f4\u9694\u7801\u672c\u4e3aLLM\u6743\u91cd\u4e2d\u5e38\u89c1\u7684\u5c0f\u5e45\u503c\u5206\u914d\u66f4\u9ad8\u5206\u8fa8\u7387\uff0c\u57284-bit\u91cf\u5316\u4e0b\u5b9e\u73b0\u5c0f\u6a21\u578b\u56f0\u60d1\u5ea6\u663e\u8457\u964d\u4f4e\uff0c\u5e76\u4fdd\u6301\u4e0e\u5927\u6a21\u578b\u7ade\u4e89\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5feb\u901f\u589e\u957f\u9700\u8981\u6709\u6548\u538b\u7f29\uff0c\u6807\u51c6\u5747\u5300\u91cf\u5316\u5047\u8bbe\u53c2\u6570\u5747\u5300\u5206\u5e03\uff0c\u4f46\u5b9e\u9645\u6743\u91cd\u5448\u73b0\u9ad8\u5ea6\u504f\u659c\u5206\u5e03\uff0c\u5bfc\u81f4\u6b21\u4f18\u6027\u80fd\u3002", "method": "\u57fa\u4e8e\u672c\u798f\u7279\u5b9a\u5f8b\uff08\u9996\u4f4d\u6570\u5b57\u5448\u5bf9\u6570\u5206\u5e03\uff09\uff0c\u7528\u5bf9\u6570\u95f4\u9694\u7801\u672c\u66ff\u4ee3\u5747\u5300\u91cf\u5316\u7f51\u683c\uff0c\u4f7f\u91cf\u5316\u5206\u8fa8\u7387\u4e0e\u6743\u91cd\u5206\u5e03\u5339\u914d\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u3002", "result": "(i) \u53d8\u6362\u5c42\u6743\u91cd\u4e25\u683c\u9075\u5faa\u672c\u798f\u7279\u5206\u5e03\uff0c\u5f52\u4e00\u5316\u5c42\u7cfb\u7edf\u6027\u504f\u79bb\uff1b(ii) \u5c0f\u6a21\u578b\u4e0a4-bit\u56f0\u60d1\u5ea6\u63d0\u5347\u663e\u8457\uff0cGemma-270M\u964d\u4f4e\u8d8510%\uff1b(iii) \u5728\u5927\u6a21\u578b\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u6027\u80fd\u5dee\u5f02\u6e90\u4e8e\u8fc7\u53c2\u6570\u5316\u6548\u5e94\uff1b(iv) \u53ef\u4e0eSmoothQuant\u7b49\u65b9\u6cd5\u65e0\u7f1d\u7ed3\u5408\u3002", "conclusion": "\u5f15\u5165\u672c\u798f\u7279\u5148\u9a8c\u7684\u91cf\u5316\u7f51\u683c\u662f\u4f4e\u6210\u672c\u6539\u8fdb\u7b56\u7565\uff0c\u5728\u6fc0\u8fdb\u4f4e\u6bd4\u7279\u91cf\u5316\u573a\u666f\u4e0b\u80fd\u63d0\u5347\u7cbe\u5ea6\uff0c\u4e14\u6613\u4e8e\u4e0e\u5176\u4ed6\u91cf\u5316\u6280\u672f\u96c6\u6210\uff0c\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.00699", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.00699", "abs": "https://arxiv.org/abs/2602.00699", "authors": ["Xuan Liu", "Ziyu Li", "Mu He", "Ziyang Ma", "Xiaoxu Wu", "Gizem Yilmaz", "Yiyuan Xia", "Bingbing Li", "He Tan", "Jerry Ying Hsi Fuh", "Wen Feng Lu", "Anders E. W. Jarfors", "Per Jansson"], "title": "From Prompt to Graph: Comparing LLM-Based Information Extraction Strategies in Domain-Specific Ontology Development", "comment": "11 pages,8 figures,3 tables,presented at International Conference on Industry of the Future and Smart Manufacturing,2025", "summary": "Ontologies are essential for structuring domain knowledge, improving accessibility, sharing, and reuse. However, traditional ontology construction relies on manual annotation and conventional natural language processing (NLP) techniques, making the process labour-intensive and costly, especially in specialised fields like casting manufacturing. The rise of Large Language Models (LLMs) offers new possibilities for automating knowledge extraction. This study investigates three LLM-based approaches, including pre-trained LLM-driven method, in-context learning (ICL) method and fine-tuning method to extract terms and relations from domain-specific texts using limited data. We compare their performances and use the best-performing method to build a casting ontology that validated by domian expert.", "AI": {"tldr": "\u9488\u5bf9\u4f20\u7edf\u672c\u4f53\u6784\u5efa\u5728\u4e13\u4e1a\u9886\u57df\u52b3\u52a8\u5bc6\u96c6\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u9884\u8bad\u7ec3LLM\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u5fae\u8c03\u4e09\u79cd\u65b9\u6cd5\u5728\u6709\u9650\u6570\u636e\u4e0b\u4ece\u94f8\u9020\u6587\u672c\u63d0\u53d6\u77e5\u8bc6\u7684\u6548\u679c\uff0c\u9009\u7528\u6700\u4f73\u65b9\u6cd5\u6784\u5efa\u4e86\u7ecf\u9886\u57df\u4e13\u5bb6\u9a8c\u8bc1\u7684\u94f8\u9020\u672c\u4f53\u3002", "motivation": "\u4f20\u7edf\u672c\u4f53\u6784\u5efa\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u548c\u5e38\u89c4NLP\u6280\u672f\uff0c\u5728\u94f8\u9020\u5236\u9020\u7b49\u4e13\u4e1a\u9886\u57df\u6210\u672c\u9ad8\u6602\u3001\u6548\u7387\u4f4e\u4e0b\uff1b\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u81ea\u52a8\u5316\u77e5\u8bc6\u63d0\u53d6\u5e26\u6765\u65b0\u673a\u9047\u3002", "method": "\u91c7\u7528\u4e09\u79cdLLM\u65b9\u6cd5\u2014\u2014\u9884\u8bad\u7ec3LLM\u9a71\u52a8\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u548c\u5fae\u8c03\uff0c\u4ece\u4e13\u4e1a\u6587\u672c\u4e2d\u63d0\u53d6\u672f\u8bed\u548c\u5173\u7cfb\uff1b\u901a\u8fc7\u6027\u80fd\u6bd4\u8f83\u786e\u5b9a\u6700\u4f73\u65b9\u6cd5\uff0c\u7528\u4e8e\u6784\u5efa\u94f8\u9020\u672c\u4f53\u3002", "result": "\u6458\u8981\u672a\u63d0\u4f9b\u5177\u4f53\u6027\u80fd\u6570\u636e\u6216\u660e\u786e\u6307\u51fa\u6700\u4f18\u65b9\u6cd5\uff0c\u4ec5\u8bf4\u660e\u5df2\u6bd4\u8f83\u6027\u80fd\u5e76\u4f7f\u7528\u6700\u4f73\u65b9\u6cd5\u5b8c\u6210\u672c\u4f53\u6784\u5efa\u3002", "conclusion": "\u6784\u5efa\u7684\u94f8\u9020\u672c\u4f53\u901a\u8fc7\u4e86\u9886\u57df\u4e13\u5bb6\u9a8c\u8bc1\uff0c\u4f46\u6458\u8981\u672a\u603b\u7ed3\u5177\u4f53\u7ed3\u8bba\u6216\u5efa\u8bae\u3002"}}
{"id": "2602.01054", "categories": ["quant-ph", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01054", "abs": "https://arxiv.org/abs/2602.01054", "authors": ["Sagnik Chatterjee"], "title": "The Quantum Learning Menagerie (A survey on Quantum learning for Classical concepts)", "comment": null, "summary": "This paper surveys various results in the field of Quantum Learning theory, specifically focusing on learning quantum-encoded classical concepts in the Probably Approximately Correct (PAC) framework. The cornerstone of this work is the emphasis on query, sample, and time complexity separations between classical and quantum learning that emerge under learning with query access to different labeling oracles. This paper aims to consolidate all known results in the area under the above umbrella and underscore the limits of our understanding by leaving the reader with 23 open problems.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u91cf\u5b50\u5b66\u4e60\u7406\u8bba\u9886\u57df\u7684\u5404\u79cd\u7ed3\u679c\uff0c\u7279\u522b\u5173\u6ce8\u5728\u6982\u7387\u8fd1\u4f3c\u6b63\u786e\uff08PAC\uff09\u6846\u67b6\u4e0b\u5b66\u4e60\u91cf\u5b50\u7f16\u7801\u7ecf\u5178\u6982\u5ff5\uff0c\u91cd\u70b9\u5f3a\u8c03\u5728\u4e0d\u540c\u6807\u8bb0\u9884\u8a00\u673a\u67e5\u8be2\u8bbf\u95ee\u4e0b\u51fa\u73b0\u7684\u7ecf\u5178\u4e0e\u91cf\u5b50\u5b66\u4e60\u4e4b\u95f4\u7684\u67e5\u8be2\u3001\u6837\u672c\u548c\u65f6\u95f4\u590d\u6742\u5ea6\u5206\u79bb\uff0c\u5e76\u63d0\u51fa\u4e8623\u4e2a\u5f00\u653e\u6027\u95ee\u9898\u3002", "motivation": "\u63a2\u7d22\u7ecf\u5178\u5b66\u4e60\u4e0e\u91cf\u5b50\u5b66\u4e60\u5728\u67e5\u8be2\u3001\u6837\u672c\u548c\u65f6\u95f4\u590d\u6742\u5ea6\u65b9\u9762\u7684\u6839\u672c\u5dee\u5f02\uff0c\u7279\u522b\u662f\u5728\u5b66\u4e60\u91cf\u5b50\u7f16\u7801\u7ecf\u5178\u6982\u5ff5\u65f6\uff0c\u4e0d\u540c\u9884\u8a00\u673a\u8bbf\u95ee\u6a21\u5f0f\u5982\u4f55\u5f71\u54cd\u5b66\u4e60\u6548\u7387\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u7efc\u8ff0\u8be5\u9886\u57df\u5df2\u6709\u7814\u7a76\u6210\u679c\uff0c\u5728PAC\u5b66\u4e60\u6846\u67b6\u4e0b\u5206\u6790\u4e0d\u540c\u6807\u8bb0\u9884\u8a00\u673a\u67e5\u8be2\u8bbf\u95ee\u6a21\u5f0f\u4e0b\u7684\u590d\u6742\u5ea6\u5206\u79bb\u73b0\u8c61\u3002", "result": "\u6574\u5408\u4e86\u91cf\u5b50\u5b66\u4e60\u7406\u8bba\u9886\u57df\u7684\u6240\u6709\u5df2\u77e5\u7ed3\u679c\uff0c\u63ed\u793a\u4e86\u7ecf\u5178\u4e0e\u91cf\u5b50\u5b66\u4e60\u4e4b\u95f4\u5b58\u5728\u7684\u590d\u6742\u5ea6\u5206\u79bb\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u7406\u8bba\u7406\u89e3\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u91cf\u5b50\u5b66\u4e60\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u76f8\u6bd4\u7ecf\u5178\u5b66\u4e60\u5b58\u5728\u663e\u8457\u7684\u590d\u6742\u5ea6\u4f18\u52bf\uff0c\u4f46\u8be5\u9886\u57df\u4ecd\u6709\u5927\u91cf\u672a\u89e3\u51b3\u95ee\u9898\uff0c23\u4e2a\u5f00\u653e\u6027\u95ee\u9898\u7684\u63d0\u51fa\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2602.00166", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00166", "abs": "https://arxiv.org/abs/2602.00166", "authors": ["Evan Chen", "Wenzhi Fang", "Shiqiang Wang", "Christopher Brinton"], "title": "Joint Continual Learning of Local Language Models and Cloud Offloading Decisions with Budget Constraints", "comment": null, "summary": "Locally deployed Small Language Models (SLMs) must continually support diverse tasks under strict memory and computation constraints, making selective reliance on cloud Large Language Models (LLMs) unavoidable. Regulating cloud assistance during continual learning is challenging, as naive reward-based reinforcement learning often yields unstable offloading behavior and exacerbates catastrophic forgetting as task distributions shift. We propose DA-GRPO, a dual-advantage extension of Group Relative Policy Optimization that incorporates cloud-usage constraints directly into advantage computation, avoiding fixed reward shaping and external routing models. This design enables the local model to jointly learn task competence and collaboration behavior, allowing cloud requests to emerge naturally during post-training while respecting a prescribed assistance budget. Experiments on mathematical reasoning and code generation benchmarks show that DA-GRPO improves post-switch accuracy, substantially reduces forgetting, and maintains stable cloud usage compared to prior collaborative and routing-based approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDA-GRPO\uff0c\u4e00\u79cdGroup Relative Policy Optimization\u7684\u53cc\u4f18\u52bf\u6269\u5c55\u65b9\u6cd5\uff0c\u5c06\u4e91\u4f7f\u7528\u7ea6\u675f\u76f4\u63a5\u6574\u5408\u5230\u4f18\u52bf\u8ba1\u7b97\u4e2d\uff0c\u4f7f\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u671f\u95f4\u80fd\u591f\u5b66\u4f1a\u4f55\u65f6\u8bf7\u6c42\u4e91\u5927\u578b\u8bed\u8a00\u6a21\u578b\u534f\u52a9\uff0c\u540c\u65f6\u9075\u5b88\u9884\u7b97\u7ea6\u675f\u5e76\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8\u3002", "motivation": "\u672c\u5730\u90e8\u7f72\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u5185\u5b58\u548c\u8ba1\u7b97\u7ea6\u675f\uff0c\u9700\u8981\u4e91\u5927\u578b\u8bed\u8a00\u6a21\u578b\u534f\u52a9\u5904\u7406\u591a\u6837\u5316\u4efb\u52a1\uff0c\u4f46\u5728\u6301\u7eed\u5b66\u4e60\u671f\u95f4\u8c03\u8282\u8fd9\u79cd\u534f\u52a9\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u6734\u7d20\u7684\u57fa\u4e8e\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u4f1a\u4ea7\u751f\u4e0d\u7a33\u5b9a\u7684\u5378\u8f7d\u884c\u4e3a\uff0c\u5e76\u5728\u4efb\u52a1\u5206\u5e03\u53d8\u5316\u65f6\u52a0\u5267\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "DA-GRPO\u901a\u8fc7\u5c06\u4e91\u4f7f\u7528\u7ea6\u675f\u76f4\u63a5\u6574\u5408\u5230\u4f18\u52bf\u8ba1\u7b97\u4e2d\u6269\u5c55\u4e86Group Relative Policy Optimization\uff0c\u6d88\u9664\u4e86\u5bf9\u56fa\u5b9a\u5956\u52b1\u5851\u9020\u6216\u5916\u90e8\u8def\u7531\u6a21\u578b\u7684\u9700\u6c42\uff0c\u4f7f\u672c\u5730\u6a21\u578b\u80fd\u591f\u8054\u5408\u5b66\u4e60\u4efb\u52a1\u80fd\u529b\u548c\u534f\u4f5c\u884c\u4e3a\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u5148\u524d\u7684\u534f\u4f5c\u548c\u57fa\u4e8e\u8def\u7531\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cDA-GRPO\u63d0\u9ad8\u4e86\u5207\u6362\u540e\u51c6\u786e\u7387\uff0c\u5927\u5e45\u51cf\u5c11\u4e86\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5e76\u4fdd\u6301\u4e86\u7a33\u5b9a\u7684\u4e91\u4f7f\u7528\u7387\u3002", "conclusion": "\u5c06\u4f7f\u7528\u7ea6\u675f\u76f4\u63a5\u6574\u5408\u5230\u4f18\u52bf\u8ba1\u7b97\u4e2d\uff0c\u4e3a\u5728\u6301\u7eed\u5b66\u4e60\u671f\u95f4\u8c03\u8282\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e91\u534f\u52a9\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u51cf\u8f7b\u9057\u5fd8\u7684\u540c\u65f6\u5b9e\u73b0\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7a33\u5b9a\u4e14\u7b26\u5408\u9884\u7b97\u7684\u534f\u4f5c\u3002"}}
{"id": "2602.00707", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00707", "abs": "https://arxiv.org/abs/2602.00707", "authors": ["Jingnan Zheng", "Jingjun Xu", "Yanzhen Luo", "Chenhang Cui", "Gelei Deng", "Zhenkai Liang", "Xiang Wang", "An Zhang", "Tat-Seng Chua"], "title": "Self-Guard: Defending Large Reasoning Models via enhanced self-reflection", "comment": null, "summary": "The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model's latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment.", "AI": {"tldr": "Self-Guard: a lightweight framework using safety prompting and activation steering to align Large Reasoning Models' safety awareness with compliance, bridging the gap where models recognize risks but still follow harmful instructions.", "motivation": "Current LRM safety alignment methods are computationally expensive and fail to address the critical awareness-compliance gap where models recognize risks but prioritize user instructions due to sycophantic tendencies.", "method": "Two-stage framework: (1) safety-oriented prompting to activate latent safety awareness and evoke reflection, (2) safety activation steering to extract and amplify directional shifts in hidden states, ensuring safety compliance overrides sycophancy during inference.", "result": "Effectively bridges the awareness-compliance gap, maintains model utility, generalizes robustly to unseen risks and different model scales, and provides a cost-efficient safety solution.", "conclusion": "Self-Guard offers a lightweight, effective, and generalizable approach to LRM safety alignment by reinforcing safety compliance at the representational level."}}
{"id": "2602.01123", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.01123", "abs": "https://arxiv.org/abs/2602.01123", "authors": ["Mei-Lin Li", "Zuo Wang", "Liang He"], "title": "Suppression of Decoherence at Exceptional Transitions", "comment": "4 pages (3 figures)+ supplemental material (4 figures)", "summary": "Decoherence is strongly influenced by environmental criticality, with conventional Hermitian critical points universally enhancing the loss of quantum coherence. Here we show that this paradigm is fundamentally altered in non-Hermitian environments. Focusing on qubits coupled to non-Hermitian spin chains and interacting ultracold Fermi gases, we find that approaching exceptional points can either enhance or strongly suppress decoherence, depending on the balance between Hermitian and non-Hermitian system-environment couplings. In particular, when these couplings are comparable, decoherence is dramatically suppressed at exceptional transitions. We trace this behavior to the distinct response of the environmental ground state near non-Hermitian degeneracies and demonstrate the robustness of the effect across multiple models. Finally, we show that the predicted suppression of decoherence is directly observable on current digital quantum simulation platforms. Our results establish exceptional points as a concrete mechanism for suppressing decoherence and identify non-Hermitian criticality as a new avenue for coherence control in open quantum systems and quantum technologies.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6311\u6218\u4e86\u4f20\u7edf\u89c2\u70b9\uff0c\u8bc1\u660e\u5728\u975e\u5384\u7c73\u73af\u5883\u4e2d\uff0c\u4f8b\u5916\u70b9\uff08exceptional points\uff09\u53ef\u4ee5\u6291\u5236\u800c\u975e\u589e\u5f3a\u91cf\u5b50\u9000\u76f8\u5e72\uff0c\u4e3a\u91cf\u5b50\u76f8\u5e72\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u673a\u5236\u3002", "motivation": "\u4f20\u7edf\u8303\u5f0f\u8ba4\u4e3a\u73af\u5883\u4e34\u754c\u6027\uff08\u5384\u7c73\u4e34\u754c\u70b9\uff09\u4f1a\u666e\u904d\u589e\u5f3a\u9000\u76f8\u5e72\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u8fd9\u4e00\u8303\u5f0f\u5728\u975e\u5384\u7c73\u73af\u5883\u4e2d\u662f\u5426\u6210\u7acb\uff0c\u63a2\u7d22\u975e\u5384\u7c73\u4e34\u754c\u6027\u4f5c\u4e3a\u91cf\u5b50\u76f8\u5e72\u63a7\u5236\u65b0\u9014\u5f84\u7684\u53ef\u80fd\u6027\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u6a21\u62df\u7814\u7a76 qubits \u4e0e\u975e\u5384\u7c73\u81ea\u65cb\u94fe\u4ee5\u53ca\u76f8\u4e92\u4f5c\u7528\u7684\u8d85\u51b7\u8d39\u7c73\u6c14\u4f53\u8026\u5408\u7684\u6a21\u578b\u4f53\u7cfb\uff0c\u5206\u6790\u63a5\u8fd1\u4f8b\u5916\u70b9\u65f6\u9000\u76f8\u5e72\u884c\u4e3a\u7684\u53d8\u5316\uff0c\u5e76\u8003\u5bdf\u5384\u7c73\u4e0e\u975e\u5384\u7c73\u7cfb\u7edf-\u73af\u5883\u8026\u5408\u5e73\u8861\u7684\u5f71\u54cd\u3002", "result": "1) \u4e0e\u5384\u7c73\u4e34\u754c\u70b9\u4e0d\u540c\uff0c\u4f8b\u5916\u70b9\u5bf9\u9000\u76f8\u5e72\u7684\u5f71\u54cd\u5177\u6709\u53cc\u5411\u6027\uff08\u589e\u5f3a\u6216\u6291\u5236\uff09\uff0c\u53d6\u51b3\u4e8e\u4e24\u79cd\u8026\u5408\u7684\u5e73\u8861\uff1b2) \u5f53\u5384\u7c73\u4e0e\u975e\u5384\u7c73\u8026\u5408\u5f3a\u5ea6\u76f8\u5f53\u65f6\uff0c\u9000\u76f8\u5e72\u5728\u4f8b\u5916\u70b9\u8f6c\u53d8\u5904\u88ab\u663e\u8457\u6291\u5236\uff1b3) \u8be5\u6548\u5e94\u5728\u591a\u79cd\u6a21\u578b\u4e2d\u5177\u6709\u9c81\u68d2\u6027\uff1b4) \u8be5\u73b0\u8c61\u53ef\u5728\u73b0\u6709\u6570\u5b57\u91cf\u5b50\u6a21\u62df\u5e73\u53f0\u4e0a\u76f4\u63a5\u89c2\u6d4b\u3002", "conclusion": "\u4f8b\u5916\u70b9\u53ef\u4f5c\u4e3a\u6291\u5236\u9000\u76f8\u5e72\u7684\u5b9e\u9645\u673a\u5236\uff0c\u975e\u5384\u7c73\u4e34\u754c\u6027\u4e3a\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684\u76f8\u5e72\u63a7\u5236\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5bf9\u91cf\u5b50\u6280\u672f\u5177\u6709\u76f4\u63a5\u5b9e\u9a8c\u610f\u4e49\u3002"}}
{"id": "2602.00709", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00709", "abs": "https://arxiv.org/abs/2602.00709", "authors": ["Wenda Li", "Tongya Zheng", "Kaixuan Chen", "Shunyu Liu", "Haoze Jiang", "Yunzhi Hao", "Rui Miao", "Zujie Ren", "Mingli Song", "Hang Shi", "Gang Chen"], "title": "Physics-informed Diffusion Generation for Geomagnetic Map Interpolation", "comment": "5 pages, 2 figures, IEEE ICASSP'26", "summary": "Geomagnetic map interpolation aims to infer unobserved geomagnetic data at spatial points, yielding critical applications in navigation and resource exploration. However, existing methods for scattered data interpolation are not specifically designed for geomagnetic maps, which inevitably leads to suboptimal performance due to detection noise and the laws of physics. Therefore, we propose a Physics-informed Diffusion Generation framework~(PDG) to interpolate incomplete geomagnetic maps. First, we design a physics-informed mask strategy to guide the diffusion generation process based on a local receptive field, effectively eliminating noise interference. Second, we impose a physics-informed constraint on the diffusion generation results following the kriging principle of geomagnetic maps, ensuring strict adherence to the laws of physics. Extensive experiments and in-depth analyses on four real-world datasets demonstrate the superiority and effectiveness of each component of PDG.", "AI": {"tldr": "\u63d0\u51fa\u7269\u7406\u4fe1\u606f\u6269\u6563\u751f\u6210\u6846\u67b6(PDG)\u7528\u4e8e\u5730\u78c1\u56fe\u63d2\u503c\uff0c\u901a\u8fc7\u7269\u7406\u4fe1\u606f\u63a9\u7801\u7b56\u7565\u6d88\u9664\u566a\u58f0\uff0c\u5e76\u57fa\u4e8e\u514b\u91cc\u91d1\u539f\u7406\u65bd\u52a0\u7269\u7406\u7ea6\u675f\uff0c\u5728\u56db\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u6563\u4e71\u6570\u636e\u63d2\u503c\u65b9\u6cd5\u672a\u9488\u5bf9\u5730\u78c1\u56fe\u7279\u6027\u8bbe\u8ba1\uff0c\u5bfc\u81f4\u56e0\u68c0\u6d4b\u566a\u58f0\u548c\u7269\u7406\u89c4\u5f8b\u9650\u5236\u800c\u6027\u80fd\u4e0d\u4f73\u3002\u5730\u78c1\u56fe\u63d2\u503c\u5bf9\u5bfc\u822a\u548c\u8d44\u6e90\u52d8\u63a2\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faPDG\u6846\u67b6\uff1a1)\u8bbe\u8ba1\u57fa\u4e8e\u5c40\u90e8\u611f\u53d7\u91ce\u7684\u7269\u7406\u4fe1\u606f\u63a9\u7801\u7b56\u7565\uff0c\u5f15\u5bfc\u6269\u6563\u751f\u6210\u8fc7\u7a0b\u6d88\u9664\u566a\u58f0\uff1b2)\u6839\u636e\u5730\u78c1\u56fe\u7684\u514b\u91cc\u91d1\u539f\u7406\u65bd\u52a0\u7269\u7406\u4fe1\u606f\u7ea6\u675f\uff0c\u786e\u4fdd\u7ed3\u679c\u7b26\u5408\u7269\u7406\u89c4\u5f8b\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u548c\u6df1\u5165\u5206\u6790\u8bc1\u660e\u4e86PDG\u5404\u7ec4\u4ef6\u7684\u4f18\u8d8a\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "PDG\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d2\u503c\u4e0d\u5b8c\u6574\u5730\u78c1\u56fe\uff0c\u7269\u7406\u4fe1\u606f\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u63d2\u503c\u8d28\u91cf\u548c\u7269\u7406\u4e00\u81f4\u6027\u3002"}}
{"id": "2602.01165", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.01165", "abs": "https://arxiv.org/abs/2602.01165", "authors": ["Shane McFarthing", "Aidan Pellow-Jarman", "Francesco Petruccione"], "title": "Noise-Resilient Quantum Chemistry with Half the Qubits", "comment": "16 pages, 7 figures", "summary": "Sample-based quantum diagonalization (SQD) offers a powerful route to accurate quantum chemistry on noisy intermediate-scale quantum (NISQ) devices by combining quantum sampling with classical diagonalization. Here we introduce HSQD, a novel half-qubit SQD approach that halves the qubit requirement for simulating a chemical system and drastically reduces overall circuit depth and gate counts, suppressing hardware noise. When modeling the dissociation of the nitrogen molecule with a (10e, 26o) active space, HSQD matches the accuracy of SQD on IBM quantum hardware using only half the number of qubits and 40% fewer measurements.\n  We further enhance HSQD with a heat-bath configuration interaction (HCI) inspired selection of the samples, forming HCI-HSQD. This yields sub-millihartree accuracy across the N2 potential energy surface and produces subspaces up to 39% smaller than those from classical HCI, showing a significant improvement in the compactness of the ground-state representation.\n  Finally, we demonstrate the scalability of HCI-HSQD using iron-sulfur clusters, reaching active spaces of up to (54e, 36o) while using only half as many qubits as the original SQD. For these systems, HCI-HSQD reduces SQD energy errors by up to 76% for [2Fe-2S] and 26% for [4Fe-4S], while also reducing subspace sizes, halving measurement requirements, and eliminating expensive post-processing.\n  Together, these results establish half-qubit SQD as a noise-resilient and resource-efficient pathway toward practical quantum advantage in strongly correlated chemistry.", "AI": {"tldr": "\u63d0\u51faHSQD\u65b9\u6cd5\uff0c\u901a\u8fc7\u534a\u91cf\u5b50\u6bd4\u7279\u8bbe\u8ba1\u5927\u5e45\u964d\u4f4e\u91cf\u5b50\u5316\u5b66\u6a21\u62df\u7684\u8d44\u6e90\u9700\u6c42\uff0c\u5728IBM\u91cf\u5b50\u786c\u4ef6\u4e0a\u5b9e\u73b0\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u663e\u8457\u6291\u5236\u566a\u58f0\u5f71\u54cd", "motivation": "\u89e3\u51b3NISQ\u65f6\u4ee3\u91cf\u5b50\u5316\u5b66\u6a21\u62df\u4e2d\u91cf\u5b50\u6bd4\u7279\u8d44\u6e90\u6709\u9650\u548c\u786c\u4ef6\u566a\u58f0\u4e25\u91cd\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u51cf\u5c11\u91cf\u5b50\u7535\u8def\u6df1\u5ea6\u548c\u95e8\u6570\u91cf\u6765\u63d0\u9ad8\u8ba1\u7b97\u53ef\u884c\u6027", "method": "\u5f00\u53d1\u534a\u91cf\u5b50\u6bd4\u7279SQD\uff08HSQD\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u91cf\u5b50\u91c7\u6837\u4e0e\u7ecf\u5178\u5bf9\u89d2\u5316\uff1b\u8fdb\u4e00\u6b65\u5f15\u5165\u70ed\u6d74\u7ec4\u6001\u76f8\u4e92\u4f5c\u7528\uff08HCI\uff09\u91c7\u6837\u9009\u62e9\u7b56\u7565\u5f62\u6210HCI-HSQD\u53d8\u4f53", "result": "\u5728\u6c2e\u5206\u5b50\u89e3\u79bb\u6a21\u62df\u4e2d\uff0c\u4f7f\u7528(10e,26o)\u6d3b\u6027\u7a7a\u95f4\u65f6\uff0cHSQD\u4ec5\u9700\u4f20\u7edfSQD\u4e00\u534a\u7684\u91cf\u5b50\u6bd4\u7279\u548c60%\u7684\u6d4b\u91cf\u6b21\u6570\u5373\u53ef\u8fbe\u5230\u76f8\u540c\u7cbe\u5ea6\uff1bHCI-HSQD\u5728N2\u52bf\u80fd\u9762\u4e0a\u5b9e\u73b0\u4e9a\u6beb\u54c8\u7279\u91cc\u7cbe\u5ea6\uff0c\u5b50\u7a7a\u95f4\u5c3a\u5bf8\u6bd4\u7ecf\u5178HCI\u5c0f39%\uff1b\u5728\u94c1\u786b\u7c07\u6a21\u62df\u4e2d\u6210\u529f\u5904\u7406(54e,36o)\u6d3b\u6027\u7a7a\u95f4\uff0c\u91cf\u5b50\u6bd4\u7279\u9700\u6c42\u51cf\u534a\uff0c\u80fd\u91cf\u8bef\u5dee\u964d\u4f4e26-76%", "conclusion": "\u534a\u91cf\u5b50\u6bd4\u7279SQD\u65b9\u6cd5\u4e3a\u5f3a\u5173\u8054\u5316\u5b66\u4f53\u7cfb\u63d0\u4f9b\u4e86\u4e00\u79cd\u6297\u566a\u58f0\u3001\u8d44\u6e90\u9ad8\u6548\u7684\u91cf\u5b50\u8ba1\u7b97\u8def\u5f84\uff0c\u5c55\u793a\u4e86\u5b9e\u73b0\u5b9e\u9645\u91cf\u5b50\u4f18\u52bf\u7684\u53ef\u884c\u6027"}}
{"id": "2602.00173", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00173", "abs": "https://arxiv.org/abs/2602.00173", "authors": ["Shuozhe Li", "Vaishnav Tadiparthi", "Kwonjoon Lee", "Nakul Agarwal", "Hossein Nourkhiz Mahjoub", "Ehsan Moradi Pari", "Lizhang Chen", "Amy Zhang", "Liu Leqi"], "title": "Learning Robust Reasoning through Guided Adversarial Self-Play", "comment": null, "summary": "Reinforcement learning from verifiable rewards (RLVR) produces strong reasoning models, yet they can fail catastrophically when the conditioning context is fallible (e.g., corrupted chain-of-thought, misleading partial solutions, or mild input perturbations), since standard RLVR optimizes final-answer correctness only under clean conditioning. We introduce GASP (Guided Adversarial Self-Play), a robustification method that explicitly trains detect-and-repair capabilities using only outcome verification. Without human labels or external teachers, GASP forms an adversarial self-play game within a single model: a polluter learns to induce failure via locally coherent corruptions, while an agent learns to diagnose and recover under the same corrupted conditioning. To address the scarcity of successful recoveries early in training, we propose in-distribution repair guidance, an imitation term on self-generated repairs that increases recovery probability while preserving previously acquired capabilities. Across four open-weight models (1.5B--8B), GASP transforms strong-but-brittle reasoners into robust ones that withstand misleading and perturbed context while often improving clean accuracy. Further analysis shows that adversarial corruptions induce an effective curriculum, and in-distribution guidance enables rapid recovery learning with minimal representational drift.", "AI": {"tldr": "GASP (Guided Adversarial Self-Play) robustifies brittle RL reasoning models by training them to detect and repair errors through adversarial self-play, using only outcome verification without human labels.", "motivation": "RLVR models fail catastrophically when conditioning contexts are corrupted (chain-of-thought errors, misleading partial solutions, or input perturbations) because standard RLVR only optimizes final correctness under clean conditions.", "method": "GASP forms an adversarial self-play game within a single model: a polluter learns to induce failures via locally coherent corruptions, while an agent learns to diagnose and recover. In-distribution repair guidance (imitation on self-generated repairs) addresses scarcity of successful recoveries early in training.", "result": "Across four open-weight models (1.5B--8B), GASP transforms brittle reasoners into robust ones that withstand corrupted contexts while often improving clean accuracy. Adversarial corruptions create an effective curriculum, and in-distribution guidance enables rapid recovery learning with minimal representational drift.", "conclusion": "GASP successfully robustifies RL reasoning models using only outcome supervision, demonstrating that adversarial self-play and in-distribution repair guidance can build models that maintain performance under corrupted conditioning without human labels or external teachers."}}
{"id": "2602.00710", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00710", "abs": "https://arxiv.org/abs/2602.00710", "authors": ["Yueqi Zhang", "Jin Hu", "Shaoxiong Feng", "Peiwen Yuan", "Xinglin Wang", "Yiwei Li", "Jiayi Shi", "Chuyi Tan", "Ji Zhang", "Boyuan Pan", "Yao Hu", "Kan Li"], "title": "Learning More from Less: Unlocking Internal Representations for Benchmark Compression", "comment": null, "summary": "The prohibitive cost of evaluating Large Language Models (LLMs) necessitates efficient alternatives to full-scale benchmarking. Prevalent approaches address this by identifying a small coreset of items to approximate full-benchmark performance. However, existing methods must estimate a reliable item profile from response patterns across many source models, which becomes statistically unstable when the source pool is small. This dependency is particularly limiting for newly released benchmarks with minimal historical evaluation data. We argue that discrete correctness labels are a lossy view of the model's decision process and fail to capture information encoded in hidden states. To address this, we introduce REPCORE, which aligns heterogeneous hidden states into a unified latent space to construct representative coresets. Using these subsets for performance extrapolation, REPCORE achieves precise estimation accuracy with as few as ten source models. Experiments on five benchmarks and over 200 models show consistent gains over output-based baselines in ranking correlation and estimation accuracy. Spectral analysis further indicates that the aligned representations contain separable components reflecting broad response tendencies and task-specific reasoning patterns.", "AI": {"tldr": "The paper proposes REPCORE, a method that uses hidden state alignment to build representative evaluation subsets for LLMs, achieving accurate performance estimation with only 10 source models and addressing the limitations of label-based coreset selection.", "motivation": "Large Language Model evaluation is prohibitively expensive, and existing coreset-based approximation methods require many source models for statistical stability, which is problematic for new benchmarks with limited historical data. Additionally, discrete correctness labels lose information encoded in model hidden states.", "method": "REPCORE aligns heterogeneous hidden states from different models into a unified latent space to construct representative coresets, enabling performance extrapolation from small subsets rather than relying on discrete output labels.", "result": "The method achieves precise performance estimation accuracy with as few as ten source models, shows consistent improvements over output-based baselines across five benchmarks and 200+ models in ranking correlation and estimation accuracy, and spectral analysis reveals the aligned representations capture separable response tendencies and reasoning patterns.", "conclusion": "Hidden state alignment provides a more informative foundation for coreset construction than discrete labels, enabling reliable LLM evaluation with minimal source models and opening new possibilities for efficient benchmarking of emerging models."}}
{"id": "2602.01177", "categories": ["quant-ph", "cs.IT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01177", "abs": "https://arxiv.org/abs/2602.01177", "authors": ["Ayanava Dasgupta", "Naqueeb Ahmad Warsi", "Masahito Hayashi"], "title": "Equivalence of Privacy and Stability with Generalization Guarantees in Quantum Learning", "comment": "22 pages, 3 figures; This paper studies the interplay between privacy, stability, and generalization in quantum learning. The stability analysis in Section IV overlaps with the companion preprint arXiv:2511.01467, which focuses on quantum information ordering, while the present work focuses on generalization guarantees in quantum learning", "summary": "We present a unified information-theoretic framework to analyze the generalization performance of differentially private (DP) quantum learning algorithms. By leveraging the connection between privacy and algorithmic stability, we establish that $(\\varepsilon, \u03b4)$-Quantum Differential Privacy (QDP) imposes a strong constraint on the mutual information between the training data and the algorithm's output. We derive a rigorous, mechanism-agnostic upper bound on this mutual information for learning algorithms satisfying a 1-neighbor privacy constraint. Furthermore, we connect this stability guarantee to generalization, proving that the expected generalization error of any $(\\varepsilon, \u03b4)$-QDP learning algorithm is bounded by the square root of the privacy-induced stability term. Finally, we extend our framework to the setting of an untrusted Data Processor, introducing the concept of Information-Theoretic Admissibility (ITA) to characterize the fundamental limits of privacy in scenarios where the learning map itself must remain oblivious to the specific dataset instance.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u4fe1\u606f\u8bba\u6846\u67b6\uff0c\u5c06\u5dee\u5206\u9690\u79c1\u4e0e\u91cf\u5b50\u5b66\u4e60\u6cdb\u5316\u8054\u7cfb\u8d77\u6765\uff0c\u8bc1\u660e\u9690\u79c1\u7ea6\u675f\u53ef\u9650\u5236\u4e92\u4fe1\u606f\u5e76\u754c\u5b9a\u6cdb\u5316\u8bef\u5dee\uff0c\u5f15\u5165ITA\u5904\u7406\u4e0d\u53ef\u4fe1\u5904\u7406\u5668\u573a\u666f\u3002", "motivation": "\u5206\u6790\u5dee\u5206\u9690\u79c1\u91cf\u5b50\u5b66\u4e60\u7b97\u6cd5\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u5efa\u7acb\u9690\u79c1\u7ea6\u675f\u4e0e\u6cdb\u5316\u80fd\u529b\u4e4b\u95f4\u7684\u7406\u8bba\u8054\u7cfb\u3002", "method": "\u6784\u5efa\u7edf\u4e00\u4fe1\u606f\u8bba\u6846\u67b6\uff0c\u5229\u7528\u9690\u79c1\u4e0e\u7b97\u6cd5\u7a33\u5b9a\u6027\u7684\u5173\u7cfb\uff0c\u63a8\u5bfc1-\u90bb\u63a5\u9690\u79c1\u4e0b\u4e92\u4fe1\u606f\u7684\u673a\u5236\u65e0\u5173\u4e0a\u754c\uff0c\u5e76\u5c06\u7a33\u5b9a\u6027\u4e0e\u6cdb\u5316\u8bef\u5dee\u5173\u8054\uff0c\u6269\u5c55\u5230\u4e0d\u53ef\u4fe1\u6570\u636e\u5904\u7406\u5668\u573a\u666f\u5e76\u5f15\u5165\u4fe1\u606f\u8bba\u53ef\u63a5\u7eb3\u6027(ITA)\u6982\u5ff5\u3002", "result": "\u8bc1\u660e(\u03b5, \u03b4)-\u91cf\u5b50\u5dee\u5206\u9690\u79c1(QDP)\u4e25\u683c\u9650\u5236\u8bad\u7ec3\u6570\u636e\u4e0e\u8f93\u51fa\u7684\u4e92\u4fe1\u606f\uff1b\u5f97\u5230\u673a\u5236\u65e0\u5173\u7684\u4e92\u4fe1\u606f\u4e0a\u754c\uff1b\u53d1\u73b0\u4efb\u4f55(\u03b5, \u03b4)-QDP\u7b97\u6cd5\u7684\u671f\u671b\u6cdb\u5316\u8bef\u5dee\u53d7\u9650\u4e8e\u9690\u79c1\u8bf1\u5bfc\u7a33\u5b9a\u6027\u9879\u7684\u5e73\u65b9\u6839\uff1b\u63d0\u51faITA\u523b\u753b\u5b66\u4e60\u6620\u5c04 oblivious \u4e8e\u5177\u4f53\u6570\u636e\u96c6\u65f6\u7684\u9690\u79c1\u57fa\u672c\u6781\u9650\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5dee\u5206\u9690\u79c1\u91cf\u5b50\u5b66\u4e60\u7b97\u6cd5\u7684\u6cdb\u5316\u5206\u6790\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u9690\u79c1\u53c2\u6570\u4e0e\u6cdb\u5316\u8bef\u5dee\u7684\u5b9a\u91cf\u5173\u7cfb\uff0c\u5e76\u4e3a\u4e0d\u53ef\u4fe1\u5904\u7406\u5668\u573a\u666f\u4e0b\u7684\u9690\u79c1\u4fdd\u62a4\u5efa\u7acb\u4e86\u7406\u8bba\u754c\u9650\u3002"}}
{"id": "2602.00175", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.00175", "abs": "https://arxiv.org/abs/2602.00175", "authors": ["Manyi Li", "Yufan Liu", "Lai Jiang", "Bing Li", "Yuming Li", "Weiming Hu"], "title": "The Illusion of Forgetting: Attack Unlearned Diffusion via Initial Latent Variable Optimization", "comment": "21 pages, 22 figures, 17 tables", "summary": "Although unlearning-based defenses claim to purge Not-Safe-For-Work (NSFW) concepts from diffusion models (DMs), we reveals that this \"forgetting\" is largely an illusion. Unlearning partially disrupts the mapping between linguistic symbols and the underlying knowledge, which remains intact as dormant memories. We find that the distributional discrepancy in the denoising process serves as a measurable indicator of how much of the mapping is retained, also reflecting the strength of unlearning. Inspired by this, we propose IVO (Initial Latent Variable Optimization), a concise and powerful attack framework that reactivates these dormant memories by reconstructing the broken mappings. Through Image Inversion}, Adversarial Optimization and Reused Attack, IVO optimizes initial latent variables to realign the noise distribution of unlearned models with their original unsafe states. Extensive experiments across 8 widely used unlearning techniques demonstrate that IVO achieves superior attack success rates and strong semantic consistency, exposing fundamental flaws in current defenses. The code is available at anonymous.4open.science/r/IVO/. Warning: This paper has unsafe images that may offend some readers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63ed\u793a\u6269\u6563\u6a21\u578b\u7684\"\u53bb\u5b66\u4e60\"\u9632\u5fa1\u5b58\u5728\u6839\u672c\u7f3a\u9677\uff1aNSFW\u77e5\u8bc6\u4ec5\u88ab\u90e8\u5206\u7834\u574f\uff0c\u4ecd\u4ee5\"\u4f11\u7720\u8bb0\u5fc6\"\u5f62\u5f0f\u4fdd\u7559\u3002\u63d0\u51fa\u7684IVO\u653b\u51fb\u6846\u67b6\u901a\u8fc7\u4f18\u5316\u521d\u59cb\u6f5c\u53d8\u91cf\u3001\u91cd\u6784\u88ab\u7834\u574f\u7684\u6620\u5c04\u5173\u7cfb\uff0c\u6210\u529f\u590d\u6d3b\u8fd9\u4e9b\u8bb0\u5fc6\uff0c\u57288\u79cd\u4e3b\u6d41\u53bb\u5b66\u4e60\u65b9\u6cd5\u4e0a\u5b9e\u73b0\u9ad8\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u53bb\u5b66\u4e60\u9632\u5fa1\u58f0\u79f0\u80fd\u5f7b\u5e95\u6e05\u9664\u6269\u6563\u6a21\u578b\u4e2d\u7684NSFW\u6982\u5ff5\uff0c\u4f46\u4f5c\u8005\u53d1\u73b0\u8fd9\u79cd\"\u9057\u5fd8\"\u53ea\u662f\u8868\u9762\u73b0\u8c61\uff0c\u5e95\u5c42\u77e5\u8bc6\u7ed3\u6784\u5e76\u672a\u771f\u6b63\u6d88\u9664\uff0c\u800c\u662f\u4ee5\u4f11\u7720\u72b6\u6001\u4fdd\u7559\uff0c\u53ef\u88ab\u91cd\u65b0\u6fc0\u6d3b\u3002", "method": "\u63d0\u51faIVO\uff08\u521d\u59cb\u6f5c\u53d8\u91cf\u4f18\u5316\uff09\u653b\u51fb\u6846\u67b6\uff0c\u5229\u7528\u53bb\u566a\u8fc7\u7a0b\u4e2d\u7684\u5206\u5e03\u5dee\u5f02\u4f5c\u4e3a\u8861\u91cf\u6307\u6807\uff0c\u901a\u8fc7\u56fe\u50cf\u53cd\u6f14\u3001\u5bf9\u6297\u4f18\u5316\u548c\u91cd\u7528\u653b\u51fb\u4e09\u79cd\u6280\u672f\uff0c\u4f18\u5316\u521d\u59cb\u6f5c\u53d8\u91cf\u4ee5\u91cd\u65b0\u5bf9\u9f50\u672a\u5b66\u4e60\u6a21\u578b\u4e0e\u539f\u59cb\u4e0d\u5b89\u5168\u72b6\u6001\u7684\u566a\u58f0\u5206\u5e03\u3002", "result": "\u57288\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u53bb\u5b66\u4e60\u6280\u672f\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cIVO\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u653b\u51fb\u6210\u529f\u7387\u548c\u5f3a\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u6210\u529f\u66b4\u9732\u4e86\u5f53\u524d\u9632\u5fa1\u7684\u6839\u672c\u7f3a\u9677\u3002", "conclusion": "\u5f53\u524d\u57fa\u4e8e\u53bb\u5b66\u4e60\u7684\u9632\u5fa1\u673a\u5236\u5b58\u5728\u6839\u672c\u6027\u6f0f\u6d1e\uff0c\u65e0\u6cd5\u771f\u6b63\u6d88\u9664NSFW\u6982\u5ff5\uff0cIVO\u653b\u51fb\u80fd\u6709\u6548\u7ed5\u8fc7\u8fd9\u4e9b\u9632\u5fa1\uff0c\u5bf9\u6269\u6563\u6a21\u578b\u5b89\u5168\u6784\u6210\u4e25\u91cd\u5a01\u80c1\u3002"}}
{"id": "2602.00731", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2602.00731", "abs": "https://arxiv.org/abs/2602.00731", "authors": ["Kyle Hamilton", "Ali Intizar"], "title": "Neuro-symbolic AI for Predictive Maintenance (PdM) -- review and recommendations", "comment": null, "summary": "In this document we perform a systematic review the State-of-the-art in Predictive Maintenance (PdM) over the last five years in industrial settings such as commercial buildings, pharmaceutical facilities, or semi-conductor manufacturing. In general, data-driven methods such as those based on deep learning, exhibit higher accuracy than traditional knowledge-based systems. These systems however, are not without significant limitations. The need for large labeled data sets, a lack of generalizibility to new environments (out-of-distribution generalization), and a lack of transparency at inference time are some of the obstacles to adoption in real world environments. In contrast, traditional approaches based on domain expertise in the form of rules, logic or first principles suffer from poor accuracy, many false positives and a need for ongoing expert supervision and manual tuning. While the majority of approaches in recent literature utilize some form of data-driven architecture, there are hybrid systems which also take into account domain specific knowledge. Such hybrid systems have the potential to overcome the weaknesses of either approach on its own while preserving their strengths. We propose taking the hybrid approach even further and integrating deep learning with symbolic logic, or Neuro-symbolic AI, to create more accurate, explainable, interpretable, and robust systems. We describe several neuro-symbolic architectures and examine their strengths and limitations within the PdM domain. We focus specifically on methods which involve the use of sensor data and manually crafted rules as inputs by describing concrete NeSy architectures. In short, this survey outlines the context of modern maintenance, defines key concepts, establishes a generalized framework, reviews current modeling approaches and challenges, and introduces the proposed focus on Neuro-symbolic AI (NESY).", "AI": {"tldr": "This systematic review analyzes predictive maintenance methods in industrial settings over the last five years, comparing data-driven and knowledge-based approaches, and proposes neuro-symbolic AI as a hybrid solution to overcome their respective limitations.", "motivation": "Traditional predictive maintenance approaches have significant drawbacks: data-driven methods lack transparency and struggle with generalization, while knowledge-based systems suffer from poor accuracy and require constant expert tuning. A hybrid neuro-symbolic approach could combine their strengths.", "method": "Performed a systematic review of state-of-the-art PdM literature from the last five years in industrial settings, analyzed modeling approaches and challenges, and examined concrete neuro-symbolic architectures that integrate sensor data with manually crafted rules.", "result": "Most recent literature utilizes data-driven architectures, but hybrid/neuro-symbolic systems show potential for creating more accurate, explainable, interpretable, and robust predictive maintenance systems by integrating deep learning with symbolic logic.", "conclusion": "Neuro-symbolic AI offers a promising path forward for predictive maintenance by combining the accuracy of data-driven methods with the transparency and robustness of knowledge-based systems, and this survey establishes a framework for its application in industrial settings."}}
{"id": "2602.01259", "categories": ["quant-ph", "cond-mat.other"], "pdf": "https://arxiv.org/pdf/2602.01259", "abs": "https://arxiv.org/abs/2602.01259", "authors": ["Lin-Yue Luo", "Wei-Lin Li", "Bao-Ming Xu", "Zhi Li"], "title": "The Intrinsic Connection between Dynamical Phase Transitions and Magnetization in the 1D XY Model", "comment": "13 pages, 8 figures", "summary": "In this manuscript, we study the quench dynamics of a transverse-field XY model starting from coherent Gibbs states. The results reveal that the initial strength of magnetization plays a crucial role in the emergence of dynamical quantum phase transitions. In concrete terms, when quenching within the same phase, through the properties of observables such as Fisher zeros and magnetization, we show that the stronger the initial magnetization, the more difficult the emergence of dynamical quantum phase transitions. The underlying mechanism is that the strong initial magnetization provides a directional effect, which inhibits the spin flipping in the process of quantum quench, making the dynamical quantum phase transition difficult to emerge. Since dynamical quantum phase transitions can be experimentally realized in various artificial systems, we hope that the physics predicted here can be experimentally verified in tabletop platforms.", "AI": {"tldr": "This paper investigates how initial magnetization strength in a transverse-field XY model affects dynamical quantum phase transitions during quench dynamics, revealing that stronger magnetization suppresses these transitions by inhibiting spin flips.", "motivation": "Understanding how initial conditions influence dynamical quantum phase transitions in quantum many-body systems, motivated by experimental feasibility of observing DQPTs in artificial systems.", "method": "Studied quench dynamics of transverse-field XY model from coherent Gibbs states, analyzing Fisher zeros and magnetization to detect DQPTs when quenching within the same phase.", "result": "Stronger initial magnetization makes dynamical quantum phase transitions more difficult to emerge because it provides a directional effect that inhibits spin flipping during the quantum quench process.", "conclusion": "Initial magnetization is a crucial control parameter for DQPTs, and the predicted physics can be experimentally verified in tabletop artificial systems where DQPTs are realizable."}}
{"id": "2602.00751", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.00751", "abs": "https://arxiv.org/abs/2602.00751", "authors": ["Cl\u00e1udio L\u00facio do Val Lopes", "Jo\u00e3o Marcus Pitta", "Fabiano Bel\u00e9m", "Gildson Alves", "Fl\u00e1vio Vin\u00edcius Cruzeiro Martins"], "title": "Engineering AI Agents for Clinical Workflows: A Case Study in Architecture,MLOps, and Governance", "comment": "9 pages, 5 figures 2026 IEEE/ACM 5th International Conference on AI Engineering - Software Engineering for AI}{April 12--13, 2026}{Rio de Janeiro, Brazil", "summary": "The integration of Artificial Intelligence (AI) into clinical settings presents a software engineering challenge, demanding a shift from isolated models to robust, governable, and reliable systems. However, brittle, prototype-derived architectures often plague industrial applications and a lack of systemic oversight, creating a ``responsibility vacuum'' where safety and accountability are compromised. This paper presents an industry case study of the ``Maria'' platform, a production-grade AI system in primary healthcare that addresses this gap.\n  Our central hypothesis is that trustworthy clinical AI is achieved through the holistic integration of four foundational engineering pillars. We present a synergistic architecture that combines Clean Architecture for maintainability with an Event-driven architecture for resilience and auditability. We introduce the Agent as the primary unit of modularity, each possessing its own autonomous MLOps lifecycle. Finally, we show how a Human-in-the-Loop governance model is technically integrated not merely as a safety check, but as a critical, event-driven data source for continuous improvement. We present the platform as a reference architecture, offering practical lessons for engineers building maintainable, scalable, and accountable AI-enabled systems in high-stakes domains.", "AI": {"tldr": "This paper presents the Maria platform, a production-grade AI system for primary healthcare that addresses the responsibility vacuum in clinical AI through a synergistic architecture combining Clean Architecture, event-driven design, autonomous MLOps agents, and integrated human-in-the-loop governance.", "motivation": "Brittle, prototype-derived AI architectures in clinical settings create a \"responsibility vacuum\" where safety and accountability are compromised, necessitating a shift from isolated models to robust, governable, and reliable systems.", "method": "The authors propose a holistic architecture integrating four pillars: Clean Architecture for maintainability, event-driven architecture for resilience/auditability, autonomous MLOps agents as modular units, and a Human-in-the-Loop governance model as an event-driven data source for continuous improvement.", "result": "The Maria platform was successfully implemented as a production-grade AI system in primary healthcare, demonstrating a reference architecture for building maintainable, scalable, and accountable AI systems in high-stakes domains.", "conclusion": "The platform offers practical engineering lessons and a reference architecture for developing trustworthy clinical AI systems that address safety, accountability, and scalability challenges in healthcare and other high-stakes domains."}}
{"id": "2602.00191", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00191", "abs": "https://arxiv.org/abs/2602.00191", "authors": ["Yadang Alexis Rouzoumka", "Jean Pinsolle", "Eug\u00e9nie Terreaux", "Christ\u00e8le Morisseau", "Jean-Philippe Ovarlez", "Chengfang Ren"], "title": "GEPC: Group-Equivariant Posterior Consistency for Out-of-Distribution Detection in Diffusion Models", "comment": "preprint", "summary": "Diffusion models learn a time-indexed score field $\\mathbf{s}_\u03b8(\\mathbf{x}_t,t)$ that often inherits approximate equivariances (flips, rotations, circular shifts) from in-distribution (ID) data and convolutional backbones. Most diffusion-based out-of-distribution (OOD) detectors exploit score magnitude or local geometry (energies, curvature, covariance spectra) and largely ignore equivariances. We introduce Group-Equivariant Posterior Consistency (GEPC), a training-free probe that measures how consistently the learned score transforms under a finite group $\\mathcal{G}$, detecting equivariance breaking even when score magnitude remains unchanged. At the population level, we propose the ideal GEPC residual, which averages an equivariance-residual functional over $\\mathcal{G}$, and we derive ID upper bounds and OOD lower bounds under mild assumptions. GEPC requires only score evaluations and produces interpretable equivariance-breaking maps. On OOD image benchmark datasets, we show that GEPC achieves competitive or improved AUROC compared to recent diffusion-based baselines while remaining computationally lightweight. On high-resolution synthetic aperture radar imagery where OOD corresponds to targets or anomalies in clutter, GEPC yields strong target-background separation and visually interpretable equivariance-breaking maps. Code is available at https://github.com/RouzAY/gepc-diffusion/.", "AI": {"tldr": "\u63d0\u51faGEPC\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7b49\u53d8\u6027\u7834\u574f\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7fa4\u53d8\u6362\u4e00\u81f4\u6027\u68c0\u9a8c\u5b9e\u73b0\u9ad8\u6027\u80fd\u4e14\u53ef\u89e3\u91ca\u7684\u5f02\u5e38\u68c0\u6d4b", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578bOOD\u68c0\u6d4b\u5668\u4e3b\u8981\u5229\u7528\u5206\u6570\u5e45\u5ea6\u6216\u5c40\u90e8\u51e0\u4f55\u7279\u5f81\uff0c\u5ffd\u7565\u4e86\u4ece\u6570\u636e backbone \u7ee7\u627f\u7684\u8fd1\u4f3c\u7b49\u53d8\u6027\uff08\u7ffb\u8f6c\u3001\u65cb\u8f6c\u3001\u5e73\u79fb\uff09\uff0c\u5bfc\u81f4\u4fe1\u606f\u5229\u7528\u4e0d\u5145\u5206", "method": "\u8bbe\u8ba1\u7fa4\u7b49\u53d8\u540e\u9a8c\u4e00\u81f4\u6027(GEPC)\u6307\u6807\uff0c\u5728\u6709\u9650\u7fa4G\u4e0a\u5e73\u5747\u7b49\u53d8\u6b8b\u5dee\u51fd\u6570\uff0c\u68c0\u6d4b\u5206\u6570\u53d8\u6362\u7684\u4e00\u81f4\u6027\uff0c\u4ec5\u9700\u5206\u6570\u8bc4\u4f30\u5373\u53ef\u53d1\u73b0\u7b49\u53d8\u6027\u7834\u574f", "result": "\u5728\u56fe\u50cf\u57fa\u51c6\u6570\u636e\u96c6\u4e0aAUROC\u8fbe\u7ade\u4e89\u6216\u66f4\u4f18\u6c34\u5e73\uff0c\u5728SAR\u5f71\u50cf\u4e2d\u5b9e\u73b0\u5f3a\u76ee\u6807-\u80cc\u666f\u5206\u79bb\uff0c\u751f\u6210\u53ef\u89e3\u91ca\u7684\u7b49\u53d8\u6027\u7834\u574f\u70ed\u529b\u56fe", "conclusion": "GEPC\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u3001\u53ef\u89e3\u91ca\u4e14\u6709\u6548\u7684OOD\u68c0\u6d4b\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u6316\u6398\u6269\u6563\u6a21\u578b\u7684\u7b49\u53d8\u6027\u7279\u5f81\u63d0\u5347\u68c0\u6d4b\u6027\u80fd"}}
{"id": "2602.00780", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00780", "abs": "https://arxiv.org/abs/2602.00780", "authors": ["Yuting Huang", "Leilei Ding", "Zhipeng Tang", "Zenghuan Zhu", "Jiajun Deng", "Xinrui Lin", "Shuo Liu", "Haojie Ren", "Jianmin Ji", "Yanyong Zhang"], "title": "Environment-Aware Adaptive Pruning with Interleaved Inference Orchestration for Vision-Language-Action Models", "comment": "12 pages, 7 figures", "summary": "While Vision-Language-Action (VLA) models hold promise in embodied intelligence, their large parameter counts lead to substantial inference latency that hinders real-time manipulation, motivating parameter sparsification. However, as the environment evolves during VLA execution, the optimal sparsity patterns change accordingly. Static pruning lacks the adaptability required for environment dynamics, whereas fixed-interval dynamic layer pruning suffers from coarse granularity and high retraining overheads. To bridge this gap, we propose EcoVLA, a training-free, plug-and-play adaptive pruning framework that supports orthogonal combination with existing VLA acceleration methods. EcoVLA comprises two components: Environment-aware Adaptive Pruning (EAP) and Interleaved Inference Orchestration ($I^2O$). EAP is a lightweight adaptive channel pruning method that incorporates the temporal consistency of the physical environment to update sparsity patterns. $I^2O$ leverages the FLOPs bubbles inherent in VLA inference to schedule the pruning method in parallel, ensuring negligible impact on latency. Evaluated on diverse VLA models and benchmarks, EcoVLA delivers state-of-the-art performance, achieving up to 1.60$\\times$ speedup with only a 0.4% drop in success rate, and further reaches 2.18$\\times$ speedup with only a 0.5% degradation when combined with token pruning. We further validate the effectiveness of EcoVLA on real-world robots.", "AI": {"tldr": "EcoVLA\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u5373\u63d2\u5373\u7528\u81ea\u9002\u5e94\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7\u73af\u5883\u611f\u77e5\u81ea\u9002\u5e94\u526a\u679d(EAP)\u548c\u4ea4\u9519\u63a8\u7406\u7f16\u6392(I\u00b2O)\u89e3\u51b3VLA\u6a21\u578b\u63a8\u7406\u5ef6\u8fdf\u4e0e\u52a8\u6001\u73af\u5883\u9002\u5e94\u6027\u95ee\u9898\uff0c\u5728\u6027\u80fd\u635f\u5931\u6781\u5c0f\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6700\u9ad82.18\u500d\u52a0\u901f\u3002", "motivation": "VLA\u6a21\u578b\u53c2\u6570\u91cf\u5927\u5bfc\u81f4\u63a8\u7406\u5ef6\u8fdf\u9ad8\uff0c\u5f71\u54cd\u5b9e\u65f6\u64cd\u4f5c\u3002\u9759\u6001\u526a\u679d\u65e0\u6cd5\u9002\u5e94\u73af\u5883\u52a8\u6001\u53d8\u5316\uff0c\u800c\u56fa\u5b9a\u95f4\u9694\u7684\u52a8\u6001\u526a\u679d\u7c92\u5ea6\u7c97\u7cd9\u4e14\u91cd\u8bad\u7ec3\u5f00\u9500\u5927\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u53c2\u6570\u7a00\u758f\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51faEcoVLA\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) EAP\uff1a\u5229\u7528\u7269\u7406\u73af\u5883\u7684\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u8f7b\u91cf\u7ea7\u81ea\u9002\u5e94\u901a\u9053\u526a\u679d\uff0c\u52a8\u6001\u66f4\u65b0\u7a00\u758f\u6a21\u5f0f\uff1b2) I\u00b2O\uff1a\u5229\u7528VLA\u63a8\u7406\u4e2d\u7684FLOPs\u6c14\u6ce1\u5e76\u884c\u8c03\u5ea6\u526a\u679d\uff0c\u51e0\u4e4e\u4e0d\u589e\u52a0\u5ef6\u8fdf\u3002\u8be5\u6846\u67b6\u65e0\u9700\u8bad\u7ec3\uff0c\u5373\u63d2\u5373\u7528\uff0c\u4e14\u53ef\u4e0e\u73b0\u6709VLA\u52a0\u901f\u65b9\u6cd5\u6b63\u4ea4\u7ec4\u5408\u3002", "result": "\u5728\u591a\u79cdVLA\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cEcoVLA\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff1a\u5355\u72ec\u4f7f\u7528\u6700\u9ad81.60\u500d\u52a0\u901f\uff0c\u6210\u529f\u7387\u4ec5\u4e0b\u964d0.4%\uff1b\u4e0etoken\u526a\u679d\u7ed3\u5408\u540e\u6700\u9ad82.18\u500d\u52a0\u901f\uff0c\u6027\u80fd\u4ec5\u4e0b\u964d0.5%\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u6709\u6548\u3002", "conclusion": "EcoVLA\u6210\u529f\u89e3\u51b3\u4e86VLA\u6a21\u578b\u63a8\u7406\u5ef6\u8fdf\u4e0e\u52a8\u6001\u73af\u5883\u9002\u5e94\u6027\u4e4b\u95f4\u7684\u77db\u76fe\uff0c\u4e3a\u5b9e\u65f6\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u53c2\u6570\u7a00\u758f\u5316\u65b9\u6848\u3002"}}
{"id": "2602.01353", "categories": ["quant-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.01353", "abs": "https://arxiv.org/abs/2602.01353", "authors": ["Stuart Ferguson", "Petros Wallden"], "title": "Methods for non-variational heuristic quantum optimisation", "comment": "12 pages, 10 figures", "summary": "Optimisation plays a central role in a wide range of scientific and industrial applications, and quantum computing has been widely proposed as a means to achieve computational advantages in this domain. To date, research into the design of noise-resilient quantum algorithms has been dominated by variational approaches, while alternatives remain relatively unexplored. In this work, we introduce a novel class of quantum optimisation heuristics that forgo this variational framework in favour of a hybrid quantum-classical approach built upon Markov Chain Monte Carlo (MCMC) techniques. We introduce Quantum-enhanced Simulated Annealing (QeSA) and Quantum-enhanced Parallel Tempering (QePT), before validating these heuristics on hard Sherrington-Kirkpatrick instances and demonstrate their superior scaling over classical benchmarks. These algorithms are expected to exhibit inherent robustness to noise and support parallel execution across both quantum and classical resources with only classical communication required. As such, they offer a scalable and potentially competitive route toward solving large-scale optimisation problems with near-term quantum devices.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u6d1b\u6280\u672f\u7684\u65b0\u578b\u91cf\u5b50\u4f18\u5316\u7b97\u6cd5\uff08QeSA\u548cQePT\uff09\uff0c\u5728\u56f0\u96be\u5b9e\u4f8b\u4e0a\u8868\u73b0\u51fa\u4f18\u4e8e\u7ecf\u5178\u65b9\u6cd5\u7684\u6269\u5c55\u6027\uff0c\u4e14\u5bf9\u566a\u58f0\u5177\u6709\u56fa\u6709\u9c81\u68d2\u6027\uff0c\u4e3a\u8fd1\u671f\u91cf\u5b50\u8bbe\u5907\u89e3\u51b3\u5927\u89c4\u6a21\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6f5c\u5728\u7ade\u4e89\u65b9\u6848\u3002", "motivation": "\u4f18\u5316\u5728\u79d1\u5b66\u548c\u5de5\u4e1a\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u91cf\u5b50\u8ba1\u7b97\u6709\u671b\u5e26\u6765\u8ba1\u7b97\u4f18\u52bf\u3002\u7136\u800c\uff0c\u5f53\u524d\u6297\u566a\u58f0\u91cf\u5b50\u7b97\u6cd5\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u53d8\u5206\u65b9\u6cd5\uff0c\u5176\u4ed6\u66ff\u4ee3\u65b9\u6848\u63a2\u7d22\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e00\u7c7b\u6452\u5f03\u53d8\u5206\u6846\u67b6\u7684\u65b0\u578b\u91cf\u5b50\u4f18\u5316\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u91c7\u7528\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u6d1b\uff08MCMC\uff09\u6280\u672f\u7684\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u65b9\u6cd5\uff0c\u5177\u4f53\u5305\u62ec\u91cf\u5b50\u589e\u5f3a\u6a21\u62df\u9000\u706b\uff08QeSA\uff09\u548c\u91cf\u5b50\u589e\u5f3a\u5e76\u884c\u56de\u706b\uff08QePT\uff09\u3002", "result": "\u5728\u56f0\u96be\u7684Sherrington-Kirkpatrick\u5b9e\u4f8b\u4e0a\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u5b83\u4eec\u76f8\u5bf9\u4e8e\u7ecf\u5178\u57fa\u51c6\u7684\u4f18\u8d8a\u6269\u5c55\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u7b97\u6cd5\u5177\u6709\u56fa\u6709\u566a\u58f0\u9c81\u68d2\u6027\uff0c\u652f\u6301\u5728\u91cf\u5b50\u548c\u7ecf\u5178\u8d44\u6e90\u4e0a\u7684\u5e76\u884c\u6267\u884c\uff0c\u4ec5\u9700\u7ecf\u5178\u901a\u4fe1\uff0c\u4e3a\u4f7f\u7528\u8fd1\u671f\u91cf\u5b50\u8bbe\u5907\u89e3\u51b3\u5927\u89c4\u6a21\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6f5c\u5728\u7ade\u4e89\u8def\u5f84\u3002"}}
{"id": "2602.00199", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00199", "abs": "https://arxiv.org/abs/2602.00199", "authors": ["Johanna Marie Gegenfurtner", "Albert Kj\u00f8ller Jacobsen", "Naima Elosegui Borras", "Alejandro Valverde Mahou", "Georgios Arvanitidis"], "title": "Reducing Memorisation in Generative Models via Riemannian Bayesian Inference", "comment": null, "summary": "Modern generative models can produce realistic samples, however, balancing memorisation and generalisation remains an open problem. We approach this challenge from a Bayesian perspective by focusing on the parameter space of flow matching and diffusion models and constructing a predictive posterior that better captures the variability of the data distribution. In particular, we capture the geometry of the loss using a Riemannian metric and leverage a flexible approximate posterior that adapts to the local structure of the loss landscape. This approach allows us to sample generative models that resemble the original model, but exhibit reduced memorisation. Empirically, we demonstrate that the proposed approach reduces memorisation while preserving generalisation. Further, we provide a theoretical analysis of our method, which explains our findings. Overall, our work illustrates how considering the geometry of the loss enables effective use of the parameter space, even for complex high-dimensional generative models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ece\u8d1d\u53f6\u65af\u89d2\u5ea6\u89e3\u51b3\u751f\u6210\u6a21\u578b\u4e2d\u8bb0\u5fc6\u4e0e\u6cdb\u5316\u7684\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u5728\u53c2\u6570\u7a7a\u95f4\u5f15\u5165\u9ece\u66fc\u5ea6\u91cf\u6355\u83b7\u635f\u5931\u51e0\u4f55\u7ed3\u6784\uff0c\u6784\u5efa\u7075\u6d3b\u8fd1\u4f3c\u540e\u9a8c\u5206\u5e03\u6765\u751f\u6210\u8bb0\u5fc6\u6027\u66f4\u4f4e\u4f46\u4fdd\u6301\u6cdb\u5316\u80fd\u529b\u7684\u6a21\u578b\u3002", "motivation": "\u73b0\u4ee3\u751f\u6210\u6a21\u578b\u80fd\u4ea7\u751f\u903c\u771f\u6837\u672c\uff0c\u4f46\u8bb0\u5fc6\u5316\u4e0e\u6cdb\u5316\u7684\u5e73\u8861\u4ecd\u662f\u5f00\u653e\u95ee\u9898\u3002\u9700\u8981\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c11\u5bf9\u8bad\u7ec3\u6570\u636e\u7684\u8fc7\u5ea6\u8bb0\u5fc6\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u805a\u7126\u6d41\u5339\u914d\u548c\u6269\u6563\u6a21\u578b\u7684\u53c2\u6570\u7a7a\u95f4\uff0c\u4f7f\u7528\u9ece\u66fc\u5ea6\u91cf\u6355\u6349\u635f\u5931\u666f\u89c2\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u5e76\u8bbe\u8ba1\u53ef\u9002\u5e94\u5c40\u90e8\u635f\u5931\u7ed3\u6784\u7684\u7075\u6d3b\u8fd1\u4f3c\u540e\u9a8c\u5206\u5e03\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u5728\u4fdd\u6301\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\u6709\u6548\u964d\u4f4e\u4e86\u8bb0\u5fc6\u5316\u73b0\u8c61\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u89e3\u91ca\u4e86\u8fd9\u4e00\u53d1\u73b0\u3002", "conclusion": "\u8003\u8651\u635f\u5931\u666f\u89c2\u7684\u51e0\u4f55\u7279\u6027\u80fd\u591f\u6709\u6548\u5730\u5229\u7528\u53c2\u6570\u7a7a\u95f4\uff0c\u5373\u4f7f\u5bf9\u4e8e\u590d\u6742\u7684\u9ad8\u7ef4\u751f\u6210\u6a21\u578b\u4e5f\u662f\u5982\u6b64\uff0c\u4e3a\u5e73\u8861\u8bb0\u5fc6\u5316\u4e0e\u6cdb\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.00785", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00785", "abs": "https://arxiv.org/abs/2602.00785", "authors": ["Sherry Yang"], "title": "World Models as an Intermediary between Agents and the Real World", "comment": null, "summary": "Large language model (LLM) agents trained using reinforcement learning has achieved superhuman performance in low-cost environments like games, mathematics, and coding. However, these successes have not translated to complex domains where the cost of interaction is high, such as the physical cost of running robots, the time cost of ML engineering, and the resource cost of scientific experiments. The true bottleneck for achieving the next level of agent performance for these complex and high-cost domains lies in the expense of executing actions to acquire reward signals. To address this gap, this paper argues that we should use world models as an intermediary between agents and the real world. We discuss how world models, viewed as models of dynamics, rewards, and task distributions, can overcome fundamental barriers of high-cost actions such as extreme off-policy learning and sample inefficiency in long-horizon tasks. Moreover, we demonstrate how world models can provide critical and rich learning signals to agents across a broad set of domains, including machine learning engineering, computer use, robotics, and AI for science. Lastly, we identify the challenges of building these world models and propose actionable items along dataset curation, architecture design, scaling, and evaluation of world models.", "AI": {"tldr": "The paper argues for using world models as intermediaries between LLM agents and high-cost real-world environments to overcome the bottleneck of expensive action execution, enabling superhuman performance in complex domains like robotics, ML engineering, and scientific experiments.", "motivation": "LLM agents trained with reinforcement learning achieve superhuman performance in low-cost environments but fail to transfer to high-cost domains where interaction costs (physical robot operation, ML engineering time, scientific experiment resources) are prohibitive. The fundamental bottleneck is the expense of executing actions to acquire reward signals.", "method": "Proposes using world models\u2014models of environment dynamics, rewards, and task distributions\u2014as intermediaries between agents and the real world to enable efficient learning without direct, costly interaction.", "result": "World models can overcome barriers of extreme off-policy learning and sample inefficiency in long-horizon tasks, while providing rich learning signals across diverse domains including machine learning engineering, computer use, robotics, and AI for science.", "conclusion": "Identifies key challenges in building world models and proposes actionable research directions in dataset curation, architecture design, scaling, and evaluation methodologies."}}
{"id": "2602.01375", "categories": ["quant-ph", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2602.01375", "abs": "https://arxiv.org/abs/2602.01375", "authors": ["Rafael A. Molina"], "title": "Spectroscopic Signatures of a Liouvillian Exceptional Spectral Phase in a Collective Spin", "comment": "Submission to SciPost", "summary": "Non-Hermitian degeneracies of Lindblad generators (Liouvillian exceptional points) can induce non-exponential relaxation and higher-order poles in dynamical response functions. A collective spin coupled to a polarized Markovian bath exhibits an \\emph{exceptional spectral phase} in which defective Liouvillian modes imprint super-Lorentzian features in frequency-resolved spectra. We compute the emission spectrum via the Liouvillian resolvent, identify symmetry-sector selection rules, and demonstrate that exceptional-point signatures are strongly state-dependent: they are suppressed in steady-state fluorescence yet become unambiguous for generic (infinite-temperature or random) initial states. Our results provide an experimentally accessible spectroscopic diagnostic of many-body Liouvillian exceptional phases and clarify when steady-state emission can (and cannot) reveal them.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63ed\u793a\u591a\u4f53Lindblad\u7cfb\u7edf\u4e2d\u975e\u5384\u7c73\u7b80\u5e76\u70b9\uff08Liouvillian exceptional points\uff09\u53ef\u5bfc\u81f4\u8d85\u6d1b\u4f26\u5179\u8c31\u7279\u5f81\uff0c\u4f46\u5176\u5b9e\u9a8c\u89c2\u6d4b\u5f3a\u70c8\u4f9d\u8d56\u4e8e\u521d\u59cb\u6001\uff1a\u7a33\u6001\u8367\u5149\u4e2d\u4fe1\u53f7\u88ab\u6291\u5236\uff0c\u800c\u65e0\u9650\u6e29\u6216\u968f\u673a\u521d\u6001\u4e0b\u4fe1\u53f7\u663e\u8457\uff0c\u4e3a\u63a2\u6d4b\u8fd9\u7c7b\u975e\u5e73\u8861\u91cf\u5b50\u76f8\u63d0\u4f9b\u4e86\u5149\u8c31\u5b66\u5224\u636e\u3002", "motivation": "\u63a2\u7d22\u975e\u5384\u7c73Lindblad\u751f\u6210\u5143\u7684\u7b80\u5e76\u70b9\uff08Liouvillian exceptional points\uff09\u5982\u4f55\u5f71\u54cd\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u7684\u52a8\u529b\u5b66\u54cd\u5e94\uff0c\u7279\u522b\u662f\u5176\u80fd\u5426\u4ea7\u751f\u53ef\u89c2\u6d4b\u7684\u975e\u6307\u6570\u5f1b\u8c6b\u548c\u5f02\u5e38\u5149\u8c31\u7279\u5f81\uff0c\u4ee5\u5efa\u7acb\u975e\u5e73\u8861\u91cf\u5b50\u76f8\u7684\u5b9e\u9a8c\u8bca\u65ad\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7Liouvillian\u89e3\u6790\u5668\u8ba1\u7b97\u96c6\u4f53\u81ea\u65cb\u7cfb\u7edf\u5728\u6781\u5316Markovian\u6d74\u4e2d\u7684\u53d1\u5c04\u5149\u8c31\uff0c\u5206\u6790\u5bf9\u79f0\u6027\u9009\u62e9\u5b9a\u5219\uff0c\u5e76\u5bf9\u6bd4\u4e0d\u540c\u521d\u59cb\u6001\uff08\u7a33\u6001\u8367\u5149 vs. \u65e0\u9650\u6e29/\u968f\u673a\u521d\u6001\uff09\u4e0b\u7684\u5149\u8c31\u7279\u5f81\u3002", "result": "\u53d1\u73b0\u7cfb\u7edf\u5b58\u5728\"\u4f8b\u5916\u8c31\u76f8\"\uff0c\u5176\u4e2d\u7f3a\u9677Liouvillian\u6a21\u5f0f\u5bfc\u81f4\u9891\u7387\u5206\u8fa8\u5149\u8c31\u4e2d\u51fa\u73b0\u8d85\u6d1b\u4f26\u5179\u7279\u5f81\uff1b\u4f46\u6b64\u7c7b\u7279\u5f81\u5728\u7a33\u6001\u8367\u5149\u4e2d\u88ab\u6291\u5236\uff0c\u4ec5\u5728\u975e\u5e73\u8861\u521d\u6001\uff08\u5982\u65e0\u9650\u6e29\u6001\uff09\u4e0b\u6e05\u6670\u663e\u73b0\u3002", "conclusion": "\u591a\u4f53Liouvillian\u4f8b\u5916\u76f8\u7684\u5149\u8c31\u7b7e\u540d\u5177\u6709\u5f3a\u521d\u6001\u4f9d\u8d56\u6027\uff0c\u7a33\u6001\u53d1\u5c04\u5149\u8c31\u65e0\u6cd5\u53ef\u9760\u63ed\u793a\u6b64\u7c7b\u76f8\uff0c\u4f46\u975e\u5e73\u8861\u521d\u6001\u4e0b\u7684\u5149\u8c31\u53ef\u4f5c\u4e3a\u5b9e\u9a8c\u8bca\u65ad\u7684\u6709\u6548\u63a2\u9488\uff0c\u4e3a\u89c2\u6d4b\u975e\u5384\u7c73\u91cf\u5b50\u52a8\u529b\u5b66\u63d0\u4f9b\u4e86\u660e\u786e\u8def\u5f84\u3002"}}
{"id": "2602.00811", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00811", "abs": "https://arxiv.org/abs/2602.00811", "authors": ["Ronghao Lin", "Honghao Lu", "Ruixing Wu", "Aolin Xiong", "Qinggong Chu", "Qiaolin He", "Sijie Mai", "Haifeng Hu"], "title": "MissMAC-Bench: Building Solid Benchmark for Missing Modality Issue in Robust Multimodal Affective Computing", "comment": null, "summary": "As a knowledge discovery task over heterogeneous data sources, current Multimodal Affective Computing (MAC) heavily rely on the completeness of multiple modalities to accurately understand human's affective state. However, in real-world scenarios, the availability of modality data is often dynamic and uncertain, leading to substantial performance fluctuations due to the distribution shifts and semantic deficiencies of the incomplete multimodal inputs. Known as the missing modality issue, this challenge poses a critical barrier to the robustness and practical deployment of MAC models. To systematically quantify this issue, we introduce MissMAC-Bench, a comprehensive benchmark designed to establish fair and unified evaluation standards from the perspective of cross-modal synergy. Two guiding principles are proposed, including no missing prior during training, and one single model capable of handling both complete and incomplete modality scenarios, thereby ensuring better generalization. Moreover, to bridge the gap between academic research and real-world applications, our benchmark integrates evaluation protocols with both fixed and random missing patterns at the dataset and instance levels. Extensive experiments conducted on 3 widely-used language models across 4 datasets validate the effectiveness of diverse MAC approaches in tackling the missing modality issue. Our benchmark provides a solid foundation for advancing robust multimodal affective computing and promotes the development of multimedia data mining.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMissMAC-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u7cfb\u7edf\u5316\u91cf\u5316\u591a\u6a21\u6001\u60c5\u611f\u8ba1\u7b97\u4e2d\u7684\u6a21\u6001\u7f3a\u5931\u95ee\u9898\uff0c\u5efa\u7acb\u516c\u5e73\u7edf\u4e00\u7684\u8de8\u6a21\u6001\u534f\u540c\u8bc4\u4f30\u6807\u51c6\uff0c\u5e76\u57283\u4e2a\u8bed\u8a00\u6a21\u578b\u548c4\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u591a\u79cd\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u60c5\u611f\u8ba1\u7b97\u4efb\u52a1\u4e25\u91cd\u4f9d\u8d56\u591a\u6a21\u6001\u6570\u636e\u7684\u5b8c\u6574\u6027\uff0c\u4f46\u771f\u5b9e\u573a\u666f\u4e2d\u6a21\u6001\u6570\u636e\u53ef\u7528\u6027\u52a8\u6001\u4e14\u4e0d\u786e\u5b9a\uff0c\u5bfc\u81f4\u4e0d\u5b8c\u6574\u8f93\u5165\u5f15\u53d1\u5206\u5e03\u504f\u79fb\u548c\u8bed\u4e49\u7f3a\u9677\uff0c\u9020\u6210\u6027\u80fd\u5927\u5e45\u6ce2\u52a8\uff0c\u5f62\u6210\u6a21\u578b\u9c81\u68d2\u6027\u4e0e\u5b9e\u9645\u90e8\u7f72\u7684\u5173\u952e\u969c\u788d\u3002", "method": "\u63d0\u51faMissMAC-Bench\uff0c\u9075\u5faa\u4e24\u5927\u539f\u5219\uff1a\u8bad\u7ec3\u9636\u6bb5\u65e0\u7f3a\u5931\u5148\u9a8c\u3001\u5355\u6a21\u578b\u540c\u65f6\u5904\u7406\u5b8c\u6574\u4e0e\u4e0d\u5b8c\u6574\u6a21\u6001\u573a\u666f\u3002\u8be5\u57fa\u51c6\u96c6\u6210\u6570\u636e\u96c6\u548c\u5b9e\u4f8b\u7ea7\u522b\u7684\u56fa\u5b9a\u4e0e\u968f\u673a\u7f3a\u5931\u6a21\u5f0f\u8bc4\u4f30\u534f\u8bae\uff0c\u5f25\u5408\u5b66\u672f\u7814\u7a76\u4e0e\u73b0\u5b9e\u5e94\u7528\u7684\u9e3f\u6c9f\u3002", "result": "\u57283\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u8bed\u8a00\u6a21\u578b\u548c4\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e0d\u540cMAC\u65b9\u6cd5\u5e94\u5bf9\u6a21\u6001\u7f3a\u5931\u95ee\u9898\u7684\u6709\u6548\u6027\uff0c\u4e3a\u63a8\u8fdb\u9c81\u68d2\u591a\u6a21\u6001\u60c5\u611f\u8ba1\u7b97\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002", "conclusion": "\u672c\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u8de8\u6a21\u6001\u534f\u540c\u63d0\u4f9b\u4e86\u516c\u5e73\u8bc4\u4f30\u6807\u51c6\uff0c\u63a8\u52a8\u9c81\u68d2\u591a\u6a21\u6001\u60c5\u611f\u8ba1\u7b97\u53d1\u5c55\uff0c\u5e76\u4fc3\u8fdb\u591a\u5a92\u4f53\u6570\u636e\u6316\u6398\u7684\u8fdb\u6b65\u3002"}}
{"id": "2602.01426", "categories": ["quant-ph", "cs.CR", "physics.ins-det", "physics.optics"], "pdf": "https://arxiv.org/pdf/2602.01426", "abs": "https://arxiv.org/abs/2602.01426", "authors": ["Georgi Gary Rozenman", "Alona Maslennikov", "Sara P. Gandelman", "Yuval Reches", "Sahar Delfan", "Neel Kanth Kundu", "Leyi Zhang", "Ruiqi Liu"], "title": "Free-space and Satellite-Based Quantum Communication: Principles, Implementations, and Challenges", "comment": "57 pages, 20 figures", "summary": "Satellite-based quantum communications represent a critical advancement in the pursuit of secure, global-scale quantum networks. Leveraging the principles of quantum mechanics, these systems offer unparalleled security through Quantum Key Distribution (QKD) and other quantum communication protocols. This review provides a comprehensive overview of the current state of satellite-based quantum communications, focusing on the evolution from terrestrial to space-based systems. We explore the distinct advantages and challenges of discrete-variable (DV) and continuous-variable (CV) quantum communication technologies in the context of satellite deployments. The paper also discusses key milestones such as the successful implementation of quantum communication via the Micius satellite and outlines the primary challenges, including atmospheric turbulence and the development of quantum repeaters, that must be addressed to achieve a global quantum internet. This review aims to consolidate recent advancements in the field, providing insights and perspectives on the future directions and potential innovations that will drive the continued evolution of satellite-based quantum communications.", "AI": {"tldr": "This review analyzes satellite-based quantum communications as a critical step toward global quantum networks, highlighting advancements, DV/CV technologies, Micius satellite milestones, and challenges like atmospheric turbulence and quantum repeaters.", "motivation": "To achieve secure, global-scale quantum networks by overcoming terrestrial limitations through satellite-based quantum communication systems leveraging quantum mechanics principles like QKD.", "method": "Comprehensive review analyzing the evolution from terrestrial to space-based systems, comparing discrete-variable (DV) and continuous-variable (CV) technologies, and discussing key experimental milestones and technical challenges.", "result": "Identifies successful implementations (e.g., Micius satellite), outlines advantages/challenges of DV/CV approaches in space, and pinpoints critical hurdles including atmospheric turbulence and the need for quantum repeaters.", "conclusion": "Satellite-based quantum communications are essential for a global quantum internet; future progress depends on addressing technical challenges and fostering innovations in satellite deployment and quantum repeater technology."}}
{"id": "2602.00208", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00208", "abs": "https://arxiv.org/abs/2602.00208", "authors": ["Jordan Levy", "Paul Saves", "Moncef Garouani", "Nicolas Verstaevel", "Benoit Gaudou"], "title": "Analyzing Shapley Additive Explanations to Understand Anomaly Detection Algorithm Behaviors and Their Complementarity", "comment": "Accepted at Intelligent Data Analysis (IDA), 2026", "summary": "Unsupervised anomaly detection is a challenging problem due to the diversity of data distributions and the lack of labels. Ensemble methods are often adopted to mitigate these challenges by combining multiple detectors, which can reduce individual biases and increase robustness. Yet building an ensemble that is genuinely complementary remains challenging, since many detectors rely on similar decision cues and end up producing redundant anomaly scores. As a result, the potential of ensemble learning is often limited by the difficulty of identifying models that truly capture different types of irregularities. To address this, we propose a methodology for characterizing anomaly detectors through their decision mechanisms. Using SHapley Additive exPlanations, we quantify how each model attributes importance to input features, and we use these attribution profiles to measure similarity between detectors. We show that detectors with similar explanations tend to produce correlated anomaly scores and identify largely overlapping anomalies. Conversely, explanation divergence reliably indicates complementary detection behavior. Our results demonstrate that explanation-driven metrics offer a different criterion than raw outputs for selecting models in an ensemble. However, we also demonstrate that diversity alone is insufficient; high individual model performance remains a prerequisite for effective ensembles. By explicitly targeting explanation diversity while maintaining model quality, we are able to construct ensembles that are more diverse, more complementary, and ultimately more effective for unsupervised anomaly detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528SHAP\u89e3\u91ca\u6765\u8861\u91cf\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u96c6\u6210\u4e2d\u5404\u68c0\u6d4b\u5668\u4e4b\u95f4\u7684\u591a\u6837\u6027\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u89e3\u91ca\u5dee\u5f02\u5ea6\u8868\u660e\u4e92\u8865\u7684\u68c0\u6d4b\u884c\u4e3a\uff0c\u4f46\u5355\u4e2a\u6a21\u578b\u7684\u9ad8\u6027\u80fd\u4ecd\u662f\u5fc5\u4e0d\u53ef\u5c11\u7684\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u6784\u5efa\u66f4\u6709\u6548\u3001\u66f4\u5177\u591a\u6837\u6027\u548c\u4e92\u8865\u6027\u7684\u96c6\u6210\u6a21\u578b\u3002", "motivation": "\u7531\u4e8e\u6570\u636e\u5206\u5e03\u7684\u591a\u6837\u6027\u548c\u6807\u7b7e\u7684\u7f3a\u4e4f\uff0c\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u3002\u867d\u7136\u96c6\u6210\u65b9\u6cd5\u5e38\u88ab\u7528\u6765\u63d0\u9ad8\u9c81\u68d2\u6027\uff0c\u4f46\u6784\u5efa\u771f\u6b63\u4e92\u8865\u7684\u96c6\u6210\u6a21\u578b\u4ecd\u7136\u56f0\u96be\uff0c\u56e0\u4e3a\u8bb8\u591a\u68c0\u6d4b\u5668\u4f9d\u8d56\u76f8\u4f3c\u7684\u51b3\u7b56\u7ebf\u7d22\u5e76\u4ea7\u751f\u5197\u4f59\u7684\u5f02\u5e38\u5206\u6570\uff0c\u9650\u5236\u4e86\u96c6\u6210\u5b66\u4e60\u7684\u6f5c\u529b\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u901a\u8fc7SHAP\uff08SHapley Additive exPlanations\uff09\u6765\u523b\u753b\u5f02\u5e38\u68c0\u6d4b\u5668\u7684\u51b3\u7b56\u673a\u5236\u3002\u4ed6\u4eec\u91cf\u5316\u6bcf\u4e2a\u6a21\u578b\u5982\u4f55\u5c06\u91cd\u8981\u6027\u5f52\u56e0\u4e8e\u8f93\u5165\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u8fd9\u4e9b\u5f52\u56e0\u7279\u5f81\u6765\u8861\u91cf\u68c0\u6d4b\u5668\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u3002", "result": "\u7814\u7a76\u8bc1\u660e\uff0c\u5177\u6709\u76f8\u4f3c\u89e3\u91ca\u7684\u68c0\u6d4b\u5668\u4f1a\u4ea7\u751f\u76f8\u5173\u7684\u5f02\u5e38\u5206\u6570\u5e76\u8bc6\u522b\u5927\u91cf\u91cd\u53e0\u7684\u5f02\u5e38\uff0c\u800c\u89e3\u91ca\u5dee\u5f02\u5ea6\u53ef\u9760\u5730\u8868\u660e\u4e92\u8865\u7684\u68c0\u6d4b\u884c\u4e3a\u3002\u4ed6\u4eec\u8fd8\u8868\u660e\uff0c\u4ec5\u6709\u591a\u6837\u6027\u662f\u4e0d\u591f\u7684\u2014\u2014\u5355\u4e2a\u6a21\u578b\u7684\u9ad8\u6027\u80fd\u4ecd\u7136\u662f\u6709\u6548\u96c6\u6210\u7684\u5148\u51b3\u6761\u4ef6\u3002", "conclusion": "\u901a\u8fc7\u660e\u786e\u5730\u5728\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\u7684\u540c\u65f6\u9488\u5bf9\u89e3\u91ca\u591a\u6837\u6027\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6784\u5efa\u66f4\u591a\u6837\u3001\u66f4\u5177\u4e92\u8865\u6027\u3001\u6700\u7ec8\u5bf9\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u66f4\u6709\u6548\u7684\u96c6\u6210\u6a21\u578b\u3002\u8fd9\u4e3a\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4e86\u4e00\u79cd\u8d85\u8d8a\u539f\u59cb\u8f93\u51fa\u7684\u65b0\u6807\u51c6\u3002"}}
{"id": "2602.00815", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00815", "abs": "https://arxiv.org/abs/2602.00815", "authors": ["Yunjian Zhang", "Sudong Wang", "Yang Li", "Peiran Xu", "Conghao Zhou", "Xiaoyue Ma", "Jianing Li", "Yao Zhu"], "title": "Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement", "comment": null, "summary": "Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications.", "AI": {"tldr": "\u63d0\u51faDoPR\u65b9\u6cd5\uff0c\u901a\u8fc7\u6bcf\u6279\u6b21\u52a8\u6001\u9009\u62e9\u5355\u4e2a\u9ad8\u4ef7\u503c\u6837\u672c\u8fdb\u884c\u7b56\u7565\u66f4\u65b0\uff0c\u5c06RLVR\u7684\u8ba1\u7b97\u5f00\u9500\u964d\u4f4e10\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u7cbe\u5ea6\uff0c\u4e3aLLM\u540e\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848", "motivation": "RLVR\u6846\u67b6\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u4f46\u8d44\u6e90\u6d88\u8017\u5de8\u5927\uff0c\u9700\u8981\u5927\u91cf\u5956\u52b1\u4fe1\u53f7\u548c\u9ad8\u6602\u7684rollout\u6210\u672c\uff0c\u9650\u5236\u4e86\u5176\u5728LLM\u8bad\u7ec3\u4e2d\u7684\u5b9e\u7528\u6027", "method": "\u5efa\u7acb\u6837\u672c\u590d\u6742\u5ea6\u7684\u7406\u8bba\u4e0b\u754c\uff0c\u5e76\u8bbe\u8ba1\u52a8\u6001\u5355\u6837\u672c\u7b56\u7565\u4f18\u5316(DoPR)\u7b97\u6cd5\uff0c\u57fa\u4e8e\u5956\u52b1\u6ce2\u52a8\u6027\u548c\u63a2\u7d22\u9a71\u52a8\u83b7\u53d6\u673a\u5236\uff0c\u6bcf\u6279\u6b21\u667a\u80fd\u9009\u62e9\u6700\u5177\u4fe1\u606f\u91cf\u7684\u5355\u4e2a\u8bad\u7ec3\u6837\u672c\u8fdb\u884c\u7b56\u7565\u66f4\u65b0", "result": "DoPR\u5c06rollout\u5f00\u9500\u964d\u4f4e\u8fd1\u4e00\u4e2a\u6570\u91cf\u7ea7\u7684\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u7cbe\u5ea6\uff0c\u4e14\u5b9e\u8bc1\u8868\u660e\u4ec5\u9700\u5c11\u91cf\u8bad\u7ec3\u5b9e\u4f8b\u5373\u53ef\u6fc0\u53d1\u5f3a\u63a8\u7406\u80fd\u529b", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8d44\u6e90\u9ad8\u6548\u7684LLM\u540e\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u4f7f\u57fa\u4e8eRL\u7684\u63a8\u7406\u8bad\u7ec3\u66f4\u52a0\u53ef\u6269\u5c55\u548c\u6613\u4e8e\u8bbf\u95ee\uff0c\u5e73\u8861\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u6a21\u578b\u6027\u80fd"}}
{"id": "2602.01470", "categories": ["quant-ph", "math-ph"], "pdf": "https://arxiv.org/pdf/2602.01470", "abs": "https://arxiv.org/abs/2602.01470", "authors": ["Gabriele Carcassi", "Christine A. Aidala", "Tobias Thrien"], "title": "What non-additive integral for ensemble spaces?", "comment": "6 pages, 4 figures, prepared for the 21st International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems, Rome, Italy, June 15-19, 2026", "summary": "In a previous work we were able to define a non-additive measure that can be used to represent both classical and quantum states in physics. We further extended this idea to work on a generic space of statistical ensembles (i.e. an ensemble space) in a way that connects to Choquet theory. The question of which non-additive integral is suitable to generalize the notion of expectation value remains open. In this paper we show that the Sugeno and Choquet integrals are not suitable.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u975e\u53ef\u52a0\u79ef\u5206\u5728\u63a8\u5e7f\u7edf\u8ba1\u7cfb\u7efc\u671f\u671b\u503c\u6982\u5ff5\u4e2d\u7684\u9002\u7528\u6027\uff0c\u8bc1\u660eSugeno\u79ef\u5206\u548cChoquet\u79ef\u5206\u5747\u4e0d\u9002\u7528\u3002", "motivation": "\u57fa\u4e8e\u524d\u671f\u5de5\u4f5c\u7528\u975e\u53ef\u52a0\u6d4b\u5ea6\u8868\u793a\u7ecf\u5178\u4e0e\u91cf\u5b50\u6001\u5e76\u8054\u7cfbChoquet\u7406\u8bba\uff0c\u9700\u8981\u627e\u5230\u5408\u9002\u7684\u975e\u53ef\u52a0\u79ef\u5206\u6765\u63a8\u5e7f\u671f\u671b\u503c\u6982\u5ff5\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff08\u6570\u5b66\u8bc1\u660e\uff09\u8bc4\u4f30Sugeno\u548cChoquet\u79ef\u5206\u5728\u7edf\u8ba1\u7cfb\u7efc\u7a7a\u95f4\u4e2d\u7684\u9002\u7528\u6027\u3002", "result": "\u8bc1\u660eSugeno\u79ef\u5206\u548cChoquet\u79ef\u5206\u90fd\u4e0d\u9002\u5408\u7528\u4e8e\u63a8\u5e7f\u671f\u671b\u503c\u3002", "conclusion": "Sugeno\u548cChoquet\u79ef\u5206\u65e0\u6cd5\u7528\u4e8e\u8be5\u76ee\u7684\uff0c\u9700\u8981\u5bfb\u627e\u5176\u4ed6\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2602.00217", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00217", "abs": "https://arxiv.org/abs/2602.00217", "authors": ["Chen Liu", "Xingzhi Sun", "Xi Xiao", "Alexandre Van Tassel", "Ke Xu", "Kristof Reimann", "Danqi Liao", "Mark Gerstein", "Tianyang Wang", "Xiao Wang", "Smita Krishnaswamy"], "title": "Dispersion Loss Counteracts Embedding Condensation and Improves Generalization in Small Language Models", "comment": null, "summary": "Large language models (LLMs) achieve remarkable performance through ever-increasing parameter counts, but scaling incurs steep computational costs. To better understand LLM scaling, we study representational differences between LLMs and their smaller counterparts, with the goal of replicating the representational qualities of larger models in the smaller models. We observe a geometric phenomenon which we term $\\textbf{embedding condensation}$, where token embeddings collapse into a narrow cone-like subspace in some language models. Through systematic analyses across multiple Transformer families, we show that small models such as $\\texttt{GPT2}$ and $\\texttt{Qwen3-0.6B}$ exhibit severe condensation, whereas the larger models such as $\\texttt{GPT2-xl}$ and $\\texttt{Qwen3-32B}$ are more resistant to this phenomenon. Additional observations show that embedding condensation is not reliably mitigated by knowledge distillation from larger models. To fight against it, we formulate a dispersion loss that explicitly encourages embedding dispersion during training. Experiments demonstrate that it mitigates condensation, recovers dispersion patterns seen in larger models, and yields performance gains across 10 benchmarks. We believe this work offers a principled path toward improving smaller Transformers without additional parameters.", "AI": {"tldr": "\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5b58\u5728\"\u5d4c\u5165\u51dd\u805a\"\u73b0\u8c61\uff08\u8bcd\u5143\u5d4c\u5165\u574d\u7f29\u6210\u72ed\u7a84\u9525\u5f62\u7a7a\u95f4\uff09\uff0c\u5927\u578b\u6a21\u578b\u66f4\u5177\u62b5\u6297\u529b\u3002\u63d0\u51fa\u7684\u5206\u6563\u635f\u5931\u51fd\u6570\u53ef\u7f13\u89e3\u8be5\u73b0\u8c61\uff0c\u4f7f\u5c0f\u6a21\u578b\u83b7\u5f97\u7c7b\u4f3c\u5927\u6a21\u578b\u7684\u8868\u5f81\u80fd\u529b\uff0c\u572810\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u53c2\u6570\u91cf\u6301\u7eed\u589e\u957f\u5e26\u6765\u9ad8\u6602\u8ba1\u7b97\u6210\u672c\uff0c\u7814\u7a76\u65e8\u5728\u7406\u89e3\u6a21\u578b\u7f29\u653e\u89c4\u5f8b\uff0c\u901a\u8fc7\u5206\u6790\u5927\u5c0f\u6a21\u578b\u95f4\u7684\u8868\u5f81\u5dee\u5f02\uff0c\u5c06\u5927\u6a21\u578b\u7684\u8868\u5f81\u8d28\u91cf\u590d\u73b0\u5230\u5c0f\u6a21\u578b\u4e2d\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u591a\u4e2aTransformer\u5bb6\u65cf\uff0c\u53d1\u73b0\u5e76\u91cf\u5316\"\u5d4c\u5165\u51dd\u805a\"\u73b0\u8c61\uff1b\u68c0\u9a8c\u77e5\u8bc6\u84b8\u998f\u7684\u6548\u679c\uff1b\u8bbe\u8ba1\u5206\u6563\u635f\u5931\u51fd\u6570\u5728\u8bad\u7ec3\u4e2d\u663e\u5f0f\u9f13\u52b1\u5d4c\u5165\u5206\u6563\u3002", "result": "GPT2\u548cQwen3-0.6B\u7b49\u5c0f\u6a21\u578b\u8868\u73b0\u4e25\u91cd\u51dd\u805a\uff0c\u800cGPT2-xl\u548cQwen3-32B\u7b49\u5927\u6a21\u578b\u66f4\u5177\u62b5\u6297\u529b\u3002\u77e5\u8bc6\u84b8\u998f\u65e0\u6cd5\u53ef\u9760\u7f13\u89e3\u51dd\u805a\u3002\u5206\u6563\u635f\u5931\u6709\u6548\u51cf\u8f7b\u51dd\u805a\uff0c\u6062\u590d\u5927\u6a21\u578b\u7684\u5206\u6563\u6a21\u5f0f\uff0c\u572810\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6027\u80fd\u589e\u76ca\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4e0d\u589e\u52a0\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u5c0f\u578bTransformer\u6027\u80fd\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u65b9\u6cd5\u3002"}}
{"id": "2602.01669", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.01669", "abs": "https://arxiv.org/abs/2602.01669", "authors": ["Tomohiro Nishiyama", "Yoshihiko Hasegawa"], "title": "Unified entropy production in finite quantum systems", "comment": "7 Pages", "summary": "In finite-dimensional quantum systems, temperature cannot be uniquely defined. This, in turn, implies that there are several ways to define entropy production in finite-dimensional quantum systems, because the classical entropy production depends on temperature. We propose a unified definition of entropy production based on the difference in quantum relative entropy with respect to reference states characterized by effective temperatures. We demonstrate that the proposed definition naturally decomposes into a Clausius-type entropy production and an additional contribution arising from the time dependence of the effective temperature. Furthermore, we show that requiring the entropy production rate to take the conventional form as the sum of the entropy change and the heat flow constrains the effective temperature to be either constant or equal to a specific energy-matching effective temperature. For general initial states, entropy production can become negative, in which case we derive lower bounds on entropy production and establish sufficient conditions for its non-negativity using the trace distance.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u91cf\u5b50\u76f8\u5bf9\u71b5\u548c\u6709\u6548\u6e29\u5ea6\u7684\u6709\u9650\u7ef4\u91cf\u5b50\u7cfb\u7edf\u71b5\u4ea7\u7edf\u4e00\u6846\u67b6\uff0c\u63ed\u793a\u5176\u5305\u542b\u514b\u52b3\u4fee\u65af\u9879\u548c\u6e29\u5ea6\u65f6\u53d8\u9879\uff0c\u5e76\u5206\u6790\u8d1f\u71b5\u4ea7\u6761\u4ef6\u53ca\u975e\u8d1f\u6027\u7ea6\u675f", "motivation": "\u6709\u9650\u7ef4\u91cf\u5b50\u7cfb\u7edf\u4e2d\u6e29\u5ea6\u65e0\u6cd5\u552f\u4e00\u5b9a\u4e49\uff0c\u5bfc\u81f4\u71b5\u4ea7\u5b9a\u4e49\u5b58\u5728\u591a\u4e49\u6027\uff0c\u9700\u5efa\u7acb\u4e0e\u7ecf\u5178\u70ed\u529b\u5b66\u517c\u5bb9\u7684\u7edf\u4e00\u71b5\u4ea7\u7406\u8bba\u6846\u67b6", "method": "\u91c7\u7528\u91cf\u5b50\u76f8\u5bf9\u71b5\u5dee\u5f02\u5b9a\u4e49\u71b5\u4ea7\uff0c\u5f15\u5165\u6709\u6548\u6e29\u5ea6\u7279\u5f81\u5316\u53c2\u8003\u6001\uff0c\u901a\u8fc7\u7ea6\u675f\u71b5\u4ea7\u7387\u5f62\u5f0f\u63a8\u5bfc\u6709\u6548\u6e29\u5ea6\u6761\u4ef6\uff0c\u5e76\u5229\u7528\u8ff9\u8ddd\u79bb\u5206\u6790\u8d1f\u71b5\u4ea7\u8fb9\u754c", "result": "1) \u71b5\u4ea7\u81ea\u7136\u5206\u89e3\u4e3a\u514b\u52b3\u4fee\u65af\u578b\u71b5\u4ea7\u4e0e\u6709\u6548\u6e29\u5ea6\u65f6\u53d8\u8d21\u732e\uff1b2) \u6709\u6548\u6e29\u5ea6\u88ab\u7ea6\u675f\u4e3a\u6052\u5b9a\u6216\u80fd\u91cf\u5339\u914d\u503c\uff1b3) \u4e00\u822c\u521d\u6001\u4e0b\u71b5\u4ea7\u53ef\u8d1f\uff0c\u5bfc\u51fa\u5176\u4e0b\u754c\u53ca\u975e\u8d1f\u6027\u5145\u5206\u6761\u4ef6", "conclusion": "\u5efa\u7acb\u4e86\u6709\u9650\u7ef4\u91cf\u5b50\u7cfb\u7edf\u71b5\u4ea7\u7684\u7edf\u4e00\u91cf\u5b50\u70ed\u529b\u5b66\u63cf\u8ff0\uff0c\u660e\u786e\u4e86\u6709\u6548\u6e29\u5ea6\u7684\u7269\u7406\u7ea6\u675f\uff0c\u4e3a\u91cf\u5b50\u70ed\u673a\u8bbe\u8ba1\u548c\u975e\u5e73\u8861\u8fc7\u7a0b\u5206\u6790\u63d0\u4f9b\u7406\u8bba\u57fa\u7840"}}
{"id": "2602.00218", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.00218", "abs": "https://arxiv.org/abs/2602.00218", "authors": ["Bob Junyi Zou", "Lu Tian"], "title": "GRIP2: A Robust and Powerful Deep Knockoff Method for Feature Selection", "comment": null, "summary": "Identifying truly predictive covariates while strictly controlling false discoveries remains a fundamental challenge in nonlinear, highly correlated, and low signal-to-noise regimes, where deep learning based feature selection methods are most attractive. We propose Group Regularization Importance Persistence in 2 Dimensions (GRIP2), a deep knockoff feature importance statistic that integrates first-layer feature activity over a two-dimensional regularization surface controlling both sparsity strength and sparsification geometry. To approximate this surface integral in a single training run, we introduce efficient block-stochastic sampling, which aggregates feature activity magnitudes across diverse regularization regimes along the optimization trajectory. The resulting statistics are antisymmetric by construction, ensuring finite-sample FDR control. In extensive experiments on synthetic and semi-real data, GRIP2 demonstrates improved robustness to feature correlation and noise level: in high correlation and low signal-to-noise ratio regimes where standard deep learning based feature selectors may struggle, our method retains high power and stability. Finally, on real-world HIV drug resistance data, GRIP2 recovers known resistance-associated mutations with power better than established linear baselines, confirming its reliability in practice.", "AI": {"tldr": "\u63d0\u51faGRIP2\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e8c\u7ef4\u6b63\u5219\u5316\u66f2\u9762\u79ef\u5206\u548c\u5757\u968f\u673a\u91c7\u6837\uff0c\u5728\u9ad8\u5ea6\u76f8\u5173\u3001\u4f4e\u4fe1\u566a\u6bd4\u573a\u666f\u4e0b\u5b9e\u73b0\u7a33\u5065\u7684\u7279\u5f81\u9009\u62e9\uff0c\u4f18\u4e8e\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u5728\u975e\u7ebf\u6027\u3001\u9ad8\u76f8\u5173\u6027\u548c\u4f4e\u4fe1\u566a\u6bd4\u73af\u5883\u4e2d\uff0c\u4e25\u683c\u63a7\u5236\u9519\u8bef\u53d1\u73b0\u7387\u7684\u540c\u65f6\u8bc6\u522b\u771f\u6b63\u9884\u6d4b\u6027\u53d8\u91cf\u4ecd\u662f\u6839\u672c\u6311\u6218\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u7279\u5f81\u9009\u62e9\u5668\u5728\u8fd9\u4e9b\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faGRIP2\uff08\u4e8c\u7ef4\u7ec4\u6b63\u5219\u5316\u91cd\u8981\u6027\u6301\u4e45\u6027\uff09\uff0c\u4e00\u79cd\u6df1\u5ea6knockoff\u7279\u5f81\u91cd\u8981\u6027\u7edf\u8ba1\u91cf\u3002\u5176\u6838\u5fc3\u662f\u5bf9\u7b2c\u4e00\u5c42\u7279\u5f81\u6d3b\u52a8\u5728\u63a7\u5236\u7a00\u758f\u5f3a\u5ea6\u548c\u51e0\u4f55\u7ed3\u6784\u7684\u4e8c\u7ef4\u6b63\u5219\u5316\u66f2\u9762\u4e0a\u8fdb\u884c\u79ef\u5206\u3002\u4e3a\u9ad8\u6548\u8ba1\u7b97\uff0c\u5f15\u5165\u5757\u968f\u673a\u91c7\u6837\u6280\u672f\uff0c\u5728\u5355\u6b21\u8bad\u7ec3\u4e2d\u6cbf\u4f18\u5316\u8f68\u8ff9\u805a\u5408\u4e0d\u540c\u6b63\u5219\u5316\u673a\u5236\u4e0b\u7684\u7279\u5f81\u6d3b\u52a8\u5e45\u5ea6\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6784\u9020\u5177\u6709\u53cd\u5bf9\u79f0\u6027\uff0c\u786e\u4fdd\u6709\u9650\u6837\u672c\u4e0b\u7684\u9519\u8bef\u53d1\u73b0\u7387\u63a7\u5236\u3002", "result": "\u5728\u5408\u6210\u548c\u534a\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cGRIP2\u5bf9\u7279\u5f81\u76f8\u5173\u6027\u548c\u566a\u58f0\u6c34\u5e73\u5177\u6709\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002\u5728\u9ad8\u76f8\u5173\u6027\u548c\u4f4e\u4fe1\u566a\u6bd4\u73af\u5883\u4e0b\uff0c\u6807\u51c6\u6df1\u5ea6\u5b66\u4e60\u7279\u5f81\u9009\u62e9\u5668\u53ef\u80fd\u5931\u6548\uff0c\u800cGRIP2\u4ecd\u80fd\u4fdd\u6301\u9ad8\u68c0\u6d4b\u529f\u6548\u548c\u7a33\u5b9a\u6027\u3002\u5728\u771f\u5b9eHIV\u8010\u836f\u6027\u6570\u636e\u4e0a\uff0cGRIP2\u4ee5\u4f18\u4e8e\u7ebf\u6027\u57fa\u7ebf\u7684\u80fd\u529b\u6210\u529f\u6062\u590d\u4e86\u5df2\u77e5\u7684\u8010\u836f\u76f8\u5173\u7a81\u53d8\u3002", "conclusion": "GRIP2\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u5728\u9ad8\u76f8\u5173\u6027\u548c\u4f4e\u4fe1\u566a\u6bd4\u7684\u6311\u6218\u6027\u573a\u666f\u4e0b\uff0c\u5176\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u7cbe\u51c6\u7279\u5f81\u9009\u62e9\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2602.01670", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.01670", "abs": "https://arxiv.org/abs/2602.01670", "authors": ["Shaobo Zhang", "Akib Karim", "Harry M. Quiney", "Muhammad Usman"], "title": "Quantum Jacobi-Davidson Method", "comment": null, "summary": "Computing electronic structures of quantum systems is a key task underpinning many applications in photonics, solid-state physics, and quantum technologies. This task is typically performed through iterative algorithms to find the energy eigenstates of a Hamiltonian, which are usually computationally expensive and suffer from convergence issues. In this work, we develop and implement the Quantum Jacobi-Davidson (QJD) method and its quantum diagonalization variant, the Sample-Based Quantum Jacobi-Davidson (SBQJD) method, and demonstrate their fast convergence for ground state energy estimation. We assess the intrinsic algorithmic performance of our methods through exact numerical simulations on a variety of quantum systems, including 8-qubit diagonally dominant matrices, 12-qubit one-dimensional Ising models, and a 10-qubit water molecule (H$_2$O) Hamiltonian. Our results show that both QJD and SBQJD achieve significantly faster convergence and require fewer Pauli measurements compared to the recently reported Quantum Davidson method, with SBQJD further benefiting from optimized reference state preparation. These findings establish the QJD framework as an efficient general-purpose subspace-based technique for solving quantum eigenvalue problems, providing a promising foundation for sparse Hamiltonian calculations on future fault-tolerant quantum hardware.", "AI": {"tldr": "\u63d0\u51fa\u91cf\u5b50Jacobi-Davidson(QJD)\u65b9\u6cd5\u53ca\u5176\u91c7\u6837\u53d8\u4f53(SBQJD)\u7528\u4e8e\u91cf\u5b50\u7cfb\u7edf\u7535\u5b50\u7ed3\u6784\u8ba1\u7b97\uff0c\u57288-12\u6bd4\u7279\u591a\u79cd\u91cf\u5b50\u7cfb\u7edf\u4e0a\u5b9e\u73b0\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u5c11\u7684\u6d4b\u91cf\u6b21\u6570", "motivation": "\u4f20\u7edf\u8fed\u4ee3\u7b97\u6cd5\u6c42\u89e3\u91cf\u5b50\u7cfb\u7edf\u7535\u5b50\u7ed3\u6784\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u6536\u655b\u56f0\u96be\uff0c\u9650\u5236\u4e86\u5176\u5728\u5149\u5b50\u5b66\u3001\u51dd\u805a\u6001\u7269\u7406\u548c\u91cf\u5b50\u6280\u672f\u4e2d\u7684\u5e94\u7528", "method": "\u5f00\u53d1\u5e76\u5b9e\u73b0QJD\u548cSBQJD\u91cf\u5b50\u7279\u5f81\u503c\u6c42\u89e3\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cbe\u786e\u6570\u503c\u6a21\u62df\u57288\u6bd4\u7279\u5bf9\u89d2\u5360\u4f18\u77e9\u9635\u300112\u6bd4\u7279\u4e00\u7ef4Ising\u6a21\u578b\u548c10\u6bd4\u7279\u6c34\u5206\u5b50\u54c8\u5bc6\u987f\u91cf\u4e0a\u6d4b\u8bd5\u6027\u80fd", "result": "QJD\u548cSBQJD\u76f8\u6bd4\u91cf\u5b50Davidson\u65b9\u6cd5\u6536\u655b\u663e\u8457\u66f4\u5feb\u4e14Pauli\u6d4b\u91cf\u6b21\u6570\u66f4\u5c11\uff0c\u5176\u4e2dSBQJD\u901a\u8fc7\u4f18\u5316\u53c2\u8003\u6001\u5236\u5907\u83b7\u5f97\u989d\u5916\u6027\u80fd\u63d0\u5347", "conclusion": "QJD\u6846\u67b6\u662f\u4e00\u79cd\u9ad8\u6548\u901a\u7528\u7684\u5b50\u7a7a\u95f4\u91cf\u5b50\u7279\u5f81\u503c\u6c42\u89e3\u6280\u672f\uff0c\u4e3a\u672a\u6765\u5bb9\u9519\u91cf\u5b50\u786c\u4ef6\u4e0a\u7684\u7a00\u758f\u54c8\u5bc6\u987f\u91cf\u8ba1\u7b97\u5960\u5b9a\u57fa\u7840"}}
{"id": "2602.00240", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00240", "abs": "https://arxiv.org/abs/2602.00240", "authors": ["Md Muhtasim Munif Fahim", "Soyda Humyra Yesmin", "Saiful Islam", "Md. Palash Bin Faruque", "Md. A. Salam", "Md. Mahfuz Uddin", "Samiul Islam", "Tofayel Ahmed", "Md. Binyamin", "Md. Rezaul Karim"], "title": "Green-NAS: A Global-Scale Multi-Objective Neural Architecture Search for Robust and Efficient Edge-Native Weather Forecasting", "comment": null, "summary": "We introduce Green-NAS, a multi-objective NAS (neural architecture search) framework designed for low-resource environments using weather forecasting as a case study. By adhering to 'Green AI' principles, the framework explicitly minimizes computational energy costs and carbon footprints, prioritizing sustainable deployment over raw computational scale. The Green-NAS architecture search method is optimized for both model accuracy and efficiency to find lightweight models with high accuracy and very few model parameters; this is accomplished through an optimization process that simultaneously optimizes multiple objectives. Our best-performing model, Green-NAS-A, achieved an RMSE of 0.0988 (i.e., within 1.4% of our manually tuned baseline) using only 153k model parameters, which is 239 times fewer than other globally applied weather forecasting models, such as GraphCast. In addition, we also describe how the use of transfer learning will improve the weather forecasting accuracy by approximately 5.2%, in comparison to a naive approach of training a new model for each city, when there is limited historical weather data available for that city.", "AI": {"tldr": "Green-NAS is a neural architecture search framework that finds energy-efficient AI models for weather forecasting, achieving near-baseline accuracy with 239x fewer parameters than state-of-the-art models.", "motivation": "Adhering to 'Green AI' principles, the framework aims to minimize computational energy costs and carbon footprints for sustainable AI deployment in low-resource environments, prioritizing efficiency over raw computational scale.", "method": "Multi-objective NAS framework that simultaneously optimizes model accuracy and efficiency to find lightweight architectures with minimal parameters, using weather forecasting as a case study and incorporating transfer learning for data-scarce cities.", "result": "The best model (Green-NAS-A) achieved RMSE of 0.0988 (within 1.4% of manually tuned baseline) using only 153k parameters\u2014239x fewer than GraphCast. Transfer learning improved accuracy by ~5.2% compared to training separate models for each city with limited historical data.", "conclusion": "The framework successfully demonstrates that high-accuracy weather forecasting can be achieved with dramatically reduced computational resources and energy consumption, enabling sustainable AI deployment in resource-constrained settings."}}
{"id": "2602.00854", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00854", "abs": "https://arxiv.org/abs/2602.00854", "authors": ["Fangzhou Lin", "Qianwen Ge", "Lingyu Xu", "Peiran Li", "Xiangbo Gao", "Shuo Xing", "Kazunori Yamada", "Ziming Zhang", "Haichong Zhang", "Zhengzhong Tu"], "title": "Position: Human-Centric AI Requires a Minimum Viable Level of Human Understanding", "comment": "14 pages, 1 figures", "summary": "AI systems increasingly produce fluent, correct, end-to-end outcomes. Over time, this erodes users' ability to explain, verify, or intervene. We define this divergence as the Capability-Comprehension Gap: a decoupling where assisted performance improves while users' internal models deteriorate. This paper argues that prevailing approaches to transparency, user control, literacy, and governance do not define the foundational understanding humans must retain for oversight under sustained AI delegation. To formalize this, we define the Cognitive Integrity Threshold (CIT) as the minimum comprehension required to preserve oversight, autonomy, and accountable participation under AI assistance. CIT does not require full reasoning reconstruction, nor does it constrain automation. It identifies the threshold beyond which oversight becomes procedural and contestability fails. We operatinalize CIT through three functional dimensions: (i) verification capacity, (ii) comprehension-preserving interaction, and (iii) institutional scaffolds for governance. This motivates a design and governance agenda that aligns human-AI interaction with cognitive sustainability in responsibility-critical settings.", "AI": {"tldr": "AI\u7cfb\u7edf\u8d8a\u5f3a\u5927\uff0c\u7528\u6237\u5bf9\u5176\u7406\u89e3\u8d8a\u5f31\uff0c\u5f62\u6210\"\u80fd\u529b-\u7406\u89e3\u9e3f\u6c9f\"\u3002\u8bba\u6587\u63d0\u51fa\"\u8ba4\u77e5\u5b8c\u6574\u6027\u9608\u503c\"\u6982\u5ff5\uff0c\u5b9a\u4e49\u4e86\u4fdd\u6301\u6709\u6548\u76d1\u7763\u6240\u9700\u7684\u6700\u4f4e\u7406\u89e3\u6c34\u5e73\uff0c\u5e76\u4ece\u9a8c\u8bc1\u80fd\u529b\u3001\u7406\u89e3\u4fdd\u6301\u578b\u4ea4\u4e92\u548c\u5236\u5ea6\u652f\u67b6\u4e09\u4e2a\u7ef4\u5ea6\u6784\u5efa\u6846\u67b6\uff0c\u547c\u5401\u5efa\u7acb\u53ef\u6301\u7eed\u7684\u4eba\u673a\u534f\u4f5c\u6cbb\u7406\u8bae\u7a0b\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u4ea7\u751f\u7aef\u5230\u7aef\u7684\u6d41\u7545\u6b63\u786e\u7ed3\u679c\uff0c\u7528\u6237\u9010\u6e10\u4e27\u5931\u89e3\u91ca\u3001\u9a8c\u8bc1\u6216\u5e72\u9884\u80fd\u529b\u3002\u73b0\u6709\u900f\u660e\u5ea6\u3001\u7528\u6237\u63a7\u5236\u3001\u7d20\u517b\u548c\u6cbb\u7406\u65b9\u6cd5\u672a\u80fd\u5b9a\u4e49\u4eba\u7c7b\u5728\u957f\u671fAI\u59d4\u6258\u4e0b\u5fc5\u987b\u4fdd\u7559\u7684\u57fa\u7840\u6027\u7406\u89e3\uff0c\u5bfc\u81f4\u76d1\u7763\u80fd\u529b\u88ab\u4fb5\u8680\uff0c\u81ea\u4e3b\u6027\u548c\u95ee\u8d23\u5236\u9762\u4e34\u98ce\u9669\u3002", "method": "\u5b9a\u4e49\"\u80fd\u529b-\u7406\u89e3\u9e3f\u6c9f\"\u73b0\u8c61\uff0c\u63d0\u51fa\"\u8ba4\u77e5\u5b8c\u6574\u6027\u9608\u503c\"\uff08CIT\uff09\u6982\u5ff5\u4f5c\u4e3a\u5f62\u5f0f\u5316\u6846\u67b6\u3002\u5c06CIT\u64cd\u4f5c\u5316\u4e3a\u4e09\u4e2a\u529f\u80fd\u7ef4\u5ea6\uff1a\u9a8c\u8bc1\u80fd\u529b\u3001\u7406\u89e3\u4fdd\u6301\u578b\u4ea4\u4e92\u548c\u6cbb\u7406\u5236\u5ea6\u652f\u67b6\uff0c\u4ee5\u6b64\u8bc6\u522b\u76d1\u7763\u5931\u6548\u7684\u4e34\u754c\u70b9\u3002", "result": "\u5efa\u7acb\u4e86CIT\u7406\u8bba\u6846\u67b6\uff0c\u660e\u786e\u4e86\u65e0\u9700\u5b8c\u6574\u63a8\u7406\u91cd\u5efa\u3001\u4e5f\u4e0d\u9650\u5236\u81ea\u52a8\u5316\u7684\u6700\u5c0f\u7406\u89e3\u9608\u503c\u6807\u51c6\uff0c\u5212\u5b9a\u4e86\u76d1\u7763\u4ece\u5b9e\u8d28\u6027\u6ca6\u4e3a\u7a0b\u5e8f\u6027\u7684\u8fb9\u754c\uff0c\u4e3a\u8d23\u4efb\u5173\u952e\u573a\u666f\u4e0b\u7684\u4eba\u673a\u4ea4\u4e92\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u5316\u7684\u8ba4\u77e5\u53ef\u6301\u7eed\u6027\u6807\u51c6\u3002", "conclusion": "\u5fc5\u987b\u542f\u52a8\u65b0\u7684\u8bbe\u8ba1\u548c\u6cbb\u7406\u8bae\u7a0b\uff0c\u5728AI\u8f85\u52a9\u7cfb\u7edf\u4e2d\u5d4c\u5165\u8ba4\u77e5\u5b8c\u6574\u6027\u4fdd\u62a4\u673a\u5236\uff0c\u786e\u4fdd\u4eba\u7c7b\u5728\u80fd\u529b-\u7406\u89e3\u9e3f\u6c9f\u6269\u5927\u7684\u80cc\u666f\u4e0b\u4ecd\u80fd\u4fdd\u6301\u6709\u6548\u7684\u76d1\u7763\u3001\u81ea\u4e3b\u6027\u548c\u95ee\u8d23\u53c2\u4e0e\uff0c\u5b9e\u73b0\u8d23\u4efb\u5173\u952e\u9886\u57df\u7684\u8ba4\u77e5\u53ef\u6301\u7eed\u53d1\u5c55\u3002"}}
{"id": "2602.01688", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.01688", "abs": "https://arxiv.org/abs/2602.01688", "authors": ["Yuki Kurokawa", "Yoshihiko Hasegawa"], "title": "Optimal Control to Minimize Dissipation and Fluctuations in Open Quantum Systems Beyond Slow and Rapid Regimes", "comment": null, "summary": "Optimal control is a central problem in quantum thermodynamics. While control theories in the rapid-driving and slow-driving limits have been developed, to the best of our knowledge there is no general optimization method applicable to intermediate timescales. We introduce an optimal-control framework to minimize dissipated work and work variance, defined via the two-point measurement scheme, in open quantum systems governed by time-dependent Lindblad master equations. By introducing an auxiliary operator, we convert the history-dependent work variance into a time-local integral, enabling efficient gradient-based optimization beyond slow or rapid driving regimes. Applying our method, we find that in the coherent spin-boson model the optimized protocol can switch discontinuously between distinct locally optimal solutions as the relative weight between dissipation and fluctuations is varied. Moreover, for a single-level quantum dot coupled to a fermionic reservoir, the optimized fluctuation-minimizing protocol develops a qualitatively different multi-step structure that is not captured by approaches based on slow- or rapid-driving limits.", "AI": {"tldr": "\u63d0\u51fa\u91cf\u5b50\u70ed\u529b\u5b66\u6700\u4f18\u63a7\u5236\u65b0\u6846\u67b6\uff0c\u89e3\u51b3\u4e2d\u95f4\u65f6\u95f4\u5c3a\u5ea6\u8017\u6563\u4e0e\u6da8\u843d\u534f\u540c\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u8f85\u52a9\u7b97\u7b26\u5b9e\u73b0\u5386\u53f2\u76f8\u5173\u6027\u7684\u65f6\u95f4\u5c40\u57df\u5316\u8f6c\u6362\uff0c\u7a81\u7834\u4f20\u7edf\u6162/\u5feb\u9a71\u52a8\u6781\u9650\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u91cf\u5b50\u6700\u4f18\u63a7\u5236\u7406\u8bba\u4ec5\u9002\u7528\u4e8e\u5feb\u901f\u9a71\u52a8\u6216\u6162\u901f\u9a71\u52a8\u6781\u9650\uff0c\u7f3a\u4e4f\u9002\u7528\u4e8e\u4e2d\u95f4\u65f6\u95f4\u5c3a\u5ea6\u7684\u901a\u7528\u4f18\u5316\u65b9\u6cd5\uff0c\u5bfc\u81f4\u65e0\u6cd5\u5728\u8017\u6563\u4e0e\u529f\u6da8\u843d\u95f4\u8fdb\u884c\u534f\u540c\u4f18\u5316\u3002", "method": "\u57fa\u4e8e\u65f6\u95f4\u4f9d\u8d56Lindblad\u4e3b\u65b9\u7a0b\u548c\u53cc\u70b9\u6d4b\u91cf\u65b9\u6848\u5b9a\u4e49\u8017\u6563\u529f\u4e0e\u529f\u6da8\u843d\uff0c\u5f15\u5165\u8f85\u52a9\u7b97\u7b26\u5c06\u5386\u53f2\u4f9d\u8d56\u7684\u529f\u6da8\u843d\u8f6c\u5316\u4e3a\u65f6\u95f4\u5c40\u57df\u79ef\u5206\uff0c\u5b9e\u73b0\u68af\u5ea6\u4f18\u5316\u7b97\u6cd5\u5728\u4efb\u610f\u9a71\u52a8\u901f\u7387\u4e0b\u7684\u5e94\u7528\u3002", "result": "\u5728\u76f8\u5e72\u81ea\u65cb-\u73bb\u8272\u6a21\u578b\u4e2d\uff0c\u4f18\u5316\u534f\u8bae\u968f\u8017\u6563\u4e0e\u6da8\u843d\u6743\u91cd\u6bd4\u53d8\u5316\u5448\u73b0\u4e0d\u8fde\u7eed\u5207\u6362\uff1b\u5728\u5355\u80fd\u7ea7\u91cf\u5b50\u70b9-\u8d39\u7c73\u5b50\u5e93\u6a21\u578b\u4e2d\uff0c\u6da8\u843d\u6700\u5c0f\u5316\u534f\u8bae\u5c55\u73b0\u51fa\u6162/\u5feb\u9a71\u52a8\u6781\u9650\u65b9\u6cd5\u65e0\u6cd5\u6355\u83b7\u7684\u591a\u6b65\u7ed3\u6784\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e2d\u95f4\u65f6\u95f4\u5c3a\u5ea6\u7684\u91cf\u5b50\u70ed\u529b\u5b66\u63a7\u5236\u63d0\u4f9b\u666e\u9002\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u4f20\u7edf\u6781\u9650\u65b9\u6cd5\u5ffd\u7565\u7684\u975e\u5e73\u5eb8\u4f18\u5316\u884c\u4e3a\uff0c\u63a8\u52a8\u91cf\u5b50\u70ed\u673a\u8bbe\u8ba1\u7684\u7cbe\u7ec6\u5316\u53d1\u5c55\u3002"}}
{"id": "2602.00250", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00250", "abs": "https://arxiv.org/abs/2602.00250", "authors": ["Shreshth Saini", "Avinab Saha", "Balu Adsumilli", "Neil Birkbeck", "Yilin Wang", "Alan C. Bovik"], "title": "TABES: Trajectory-Aware Backward-on-Entropy Steering for Masked Diffusion Models", "comment": null, "summary": "Masked Diffusion Models (MDMs) have emerged as a promising non-autoregressive paradigm for generative tasks, offering parallel decoding and bidirectional context utilization. However, current sampling methods rely on simple confidence-based heuristics that ignore the long-term impact of local decisions, leading to trajectory lock-in where early hallucinations cascade into global incoherence. While search-based methods mitigate this, they incur prohibitive computational costs ($O(K)$ forward passes per step). In this work, we propose Backward-on-Entropy (BoE) Steering, a gradient-guided inference framework that approximates infinite-horizon lookahead via a single backward pass. We formally derive the Token Influence Score (TIS) from a first-order expansion of the trajectory cost functional, proving that the gradient of future entropy with respect to input embeddings serves as an optimal control signal for minimizing uncertainty. To ensure scalability, we introduce \\texttt{ActiveQueryAttention}, a sparse adjoint primitive that exploits the structure of the masking objective to reduce backward pass complexity. BoE achieves a superior Pareto frontier for inference-time scaling compared to existing unmasking methods, demonstrating that gradient-guided steering offers a mathematically principled and efficient path to robust non-autoregressive generation. We will release the code.", "AI": {"tldr": "The paper proposes Backward-on-Entropy (BoE) Steering, a gradient-guided inference method for Masked Diffusion Models that uses a single backward pass to approximate infinite-horizon lookahead, preventing trajectory lock-in while being computationally efficient.", "motivation": "Current MDM sampling methods rely on simple confidence-based heuristics that cause early hallucinations to cascade into global incoherence (trajectory lock-in). While search-based methods can mitigate this, they require prohibitive computational costs ($O(K)$ forward passes per step).", "method": "The paper proposes BoE Steering, which derives a Token Influence Score (TIS) from gradient of future entropy with respect to input embeddings, and uses ActiveQueryAttention to make the backward pass efficient by exploiting the masking objective structure.", "result": "BoE achieves a superior Pareto frontier for inference-time scaling compared to existing unmasking methods, demonstrating better trade-offs between quality and computational cost.", "conclusion": "Gradient-guided steering provides a mathematically principled and efficient approach to robust non-autoregressive generation, making MDMs more practical."}}
{"id": "2602.01704", "categories": ["quant-ph", "math-ph"], "pdf": "https://arxiv.org/pdf/2602.01704", "abs": "https://arxiv.org/abs/2602.01704", "authors": ["Ali Bagci"], "title": "N-dimensional Coulomb-Sturmians with noninteger quantum numbers", "comment": null, "summary": "Coulomb-Sturmian functions are complete, orthonormal, and include the full spectrum of continuum states. They are restricted to integer values of quantum numbers, as imposed by boundary and orthonormality conditions. Bagci-Hoggan exponential-type orbitals remove this restriction through a generalization to quantum number with fractional order. The differential equations for N-dimensional Bagci-Hoggan orbitals are derived. It is demonstrated that Coulomb-Sturmian functions satisfy a particular case of these equations. Additionally, Guseinov's Psi-alpha-ETOs are identified as N-dimensional Coulomb-Sturmians with a shifted dimensional parameter alpha, rather than representing an independent complete orthonormal sets of basis in a weighted Hilbert space.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u5e93\u4ed1-\u65bd\u56fe\u59c6\u51fd\u6570\u63a8\u5e7f\u81f3\u5206\u6570\u9636\u91cf\u5b50\u6570\uff0c\u5efa\u7acb\u4e86N\u7ef4Bagci-Hoggan\u6307\u6570\u8f68\u9053\u5fae\u5206\u65b9\u7a0b\u6846\u67b6\uff0c\u63ed\u793a\u5e93\u4ed1-\u65bd\u56fe\u59c6\u51fd\u6570\u4e3a\u5176\u7279\u4f8b\uff0c\u5e76\u6f84\u6e05Guseinov\u57fa\u7ec4\u7684\u672c\u8d28\u5c5e\u6027", "motivation": "\u4f20\u7edf\u5e93\u4ed1-\u65bd\u56fe\u59c6\u51fd\u6570\u56e0\u8fb9\u754c\u6761\u4ef6\u548c\u6b63\u4ea4\u5f52\u4e00\u6027\u9650\u5236\u53ea\u80fd\u4f7f\u7528\u6574\u6570\u91cf\u5b50\u6570\uff0cBagci-Hoggan\u6307\u6570\u8f68\u9053\u901a\u8fc7\u63a8\u5e7f\u81f3\u5206\u6570\u9636\u91cf\u5b50\u6570\u7a81\u7834\u6b64\u9650\u5236\uff0c\u63d0\u5347\u57fa\u7ec4\u5728\u91cf\u5b50\u5316\u5b66\u8ba1\u7b97\u4e2d\u7684\u7075\u6d3b\u6027", "method": "\u63a8\u5bfcN\u7ef4Bagci-Hoggan\u6307\u6570\u8f68\u9053\u7684\u5fae\u5206\u65b9\u7a0b\u4f53\u7cfb\uff0c\u901a\u8fc7\u53c2\u6570\u6269\u5c55\u5b9e\u73b0\u91cf\u5b50\u6570\u7684\u5206\u6570\u9636\u63a8\u5e7f\uff0c\u5e76\u7cfb\u7edf\u6bd4\u8f83\u4e0d\u540c\u57fa\u7ec4\u7684\u6570\u5b66\u5173\u7cfb", "result": "\u8bc1\u660e\u5e93\u4ed1-\u65bd\u56fe\u59c6\u51fd\u6570\u662fBagci-Hoggan\u65b9\u7a0b\u5728\u7279\u5b9a\u53c2\u6570\u4e0b\u7684\u7279\u4f8b\uff1b\u660e\u786e\u6307\u51faGuseinov\u7684Psi-alpha-ETOs\u5e76\u975e\u72ec\u7acb\u57fa\u7ec4\uff0c\u800c\u662f\u7ef4\u5ea6\u53c2\u6570\u5e73\u79fb\u540e\u7684N\u5e93\u5c14\u4ed1-\u65bd\u56fe\u59c6\u51fd\u6570", "conclusion": "\u5efa\u7acb\u4e86\u7edf\u4e00\u7684\u6307\u6570\u8f68\u9053\u7406\u8bba\u6846\u67b6\uff0c\u6d88\u9664\u4e86\u6574\u6570\u91cf\u5b50\u6570\u9650\u5236\uff0c\u4e3a\u91cf\u5b50\u5316\u5b66\u8ba1\u7b97\u63d0\u4f9b\u4e86\u66f4\u666e\u9002\u7684\u57fa\u7ec4\u9009\u62e9\u65b9\u6848\uff0c\u540c\u65f6\u5398\u6e05\u4e86\u4e0d\u540c\u57fa\u7ec4\u95f4\u7684\u6982\u5ff5\u6df7\u6dc6"}}
{"id": "2602.00269", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.00269", "abs": "https://arxiv.org/abs/2602.00269", "authors": ["Keisuke Kamahori", "Wei-Tzu Lee", "Atindra Jha", "Rohan Kadekodi", "Stephanie Wang", "Arvind Krishnamurthy", "Baris Kasikci"], "title": "VoxServe: Streaming-Centric Serving System for Speech Language Models", "comment": "The code is available at https://github.com/vox-serve/vox-serve", "summary": "Deploying modern Speech Language Models (SpeechLMs) in streaming settings requires systems that provide low latency, high throughput, and strong guarantees of streamability. Existing systems fall short of supporting diverse models flexibly and efficiently. We present VoxServe, a unified serving system for SpeechLMs that optimizes streaming performance. VoxServe introduces a model-execution abstraction that decouples model architecture from system-level optimizations, thereby enabling support for diverse SpeechLM architectures within a single framework. Building on this abstraction, VoxServe implements streaming-aware scheduling and an asynchronous inference pipeline to improve end-to-end efficiency. Evaluations across multiple modern SpeechLMs show that VoxServe achieves 10-20x higher throughput than existing implementations at comparable latency while maintaining high streaming viability. The code of VoxServe is available at https://github.com/vox-serve/vox-serve.", "AI": {"tldr": "VoxServe is a unified serving system for Speech Language Models that uses model-execution abstraction, streaming-aware scheduling, and asynchronous inference to achieve 10-20x higher throughput than existing systems while maintaining low latency and strong streamability guarantees.", "motivation": "Existing systems cannot flexibly and efficiently support diverse SpeechLM architectures in streaming settings, which require low latency, high throughput, and strong streamability guarantees.", "method": "Introduced a model-execution abstraction that decouples model architecture from system-level optimizations, combined with streaming-aware scheduling and an asynchronous inference pipeline.", "result": "Achieved 10-20x higher throughput than existing implementations at comparable latency while maintaining high streaming viability across multiple modern SpeechLMs.", "conclusion": "VoxServe successfully provides a unified framework for efficiently serving diverse SpeechLM architectures in streaming scenarios, significantly improving throughput without sacrificing latency or streamability."}}
{"id": "2602.00866", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00866", "abs": "https://arxiv.org/abs/2602.00866", "authors": ["Akiharu Esashi", "Pawissanutt Lertpongrujikorn", "Justin Makino", "Yuibi Fujimoto", "Mohsen Amini Salehi"], "title": "Foundation CAN LM: A Pretrained Language Model For Automotive CAN Data", "comment": null, "summary": "The Controller Area Network (CAN) bus provides a rich source of vehicular signals increasingly leveraged for applications in automotive and auto insurance domains, including collision detection, predictive maintenance, and driver risk modeling. Despite this potential, existing pipelines largely train isolated task-specific models on raw CAN data, with only limited efforts exploring decoded signals. Such fragmentation prevents shared representation learning and limits cross-task generalization. By contrast, natural language processing (NLP) and computer vision (CV) have been transformed by the foundation model paradigm: large-scale pretraining followed by task-specific adaptation. In this work, we introduce the foundation CAN model that demonstrates multi-objective downstream generalization using a single pretrained backbone. Our approach treats CAN data as a language: we pretrain on large-scale, unlabeled decoded CAN signals and fine-tune across heterogeneous auto insurance tasks. To enable this, we propose a unified tokenization scheme for mixed discrete-continuous signals and address challenges of temporal complexity and trip-specific variability. Our results show that one pretrained CAN model can adapt effectively to diverse predictive tasks, validating that the foundation modeling paradigm, proven in NLP and CV, also holds for CAN data. This establishes a new direction for generalizable representation learning in automotive AI.", "AI": {"tldr": "\u5c06CAN\u603b\u7ebf\u6570\u636e\u89c6\u4e3a\u4e00\u79cd\u8bed\u8a00\uff0c\u63d0\u51fa\u9996\u4e2a\u57fa\u7840CAN\u6a21\u578b\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u5b9e\u73b0\u8de8\u6c7d\u8f66\u4fdd\u9669\u4efb\u52a1\u7684\u591a\u76ee\u6807\u6cdb\u5316\u3002", "motivation": "\u73b0\u6709CAN\u603b\u7ebf\u6570\u636e\u5206\u6790\u65b9\u6cd5\u591a\u4e3a\u4efb\u52a1\u7279\u5b9a\u7684\u5b64\u7acb\u6a21\u578b\uff0c\u7f3a\u4e4f\u5171\u4eab\u8868\u793a\u5b66\u4e60\uff0c\u9650\u5236\u4e86\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002\u800cNLP\u548cCV\u9886\u57df\u5df2\u901a\u8fc7\u57fa\u7840\u6a21\u578b\u8303\u5f0f\uff08\u5927\u89c4\u6a21\u9884\u8bad\u7ec3+\u4efb\u52a1\u9002\u914d\uff09\u53d6\u5f97\u9769\u547d\u6027\u8fdb\u5c55\uff0c\u56e0\u6b64\u63a2\u7d22\u8be5\u8303\u5f0f\u5728CAN\u6570\u636e\u4e0a\u7684\u9002\u7528\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u7840CAN\u6a21\u578b\uff0c\u5c06CAN\u4fe1\u53f7\u89c6\u4e3a\u8bed\u8a00\uff0c\u5728\u5927\u89c4\u6a21\u65e0\u6807\u6ce8\u89e3\u7801CAN\u4fe1\u53f7\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u9002\u914d\u591a\u79cd\u6c7d\u8f66\u4fdd\u9669\u4efb\u52a1\u3002\u8bbe\u8ba1\u7edf\u4e00\u7684\u79bb\u6563-\u8fde\u7eed\u4fe1\u53f7\u6807\u8bb0\u5316\u65b9\u6848\uff0c\u89e3\u51b3\u65f6\u5e8f\u590d\u6742\u6027\u548c\u884c\u7a0b\u7279\u5f02\u6027\u7b49\u6311\u6218\u3002", "result": "\u5355\u4e00\u9884\u8bad\u7ec3CAN\u6a21\u578b\u80fd\u591f\u6709\u6548\u9002\u914d\u591a\u79cd\u4e0d\u540c\u7684\u9884\u6d4b\u4efb\u52a1\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u57fa\u7840\u6a21\u578b\u8303\u5f0f\u5728CAN\u6570\u636e\u4e0a\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6c7d\u8f66AI\u4e2d\u7684\u53ef\u6cdb\u5316\u8868\u793a\u5b66\u4e60\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.01713", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.01713", "abs": "https://arxiv.org/abs/2602.01713", "authors": ["Seung Park", "Sangjin Lee", "Kyunghyun Baek"], "title": "Dual channel multi-product formulas", "comment": "11 pages, 2 figures", "summary": "Product-formula (PF) based quantum simulation is a promising approach for simulating quantum systems on near-term quantum computers. Achieving a desired simulation precision typically requires a polynomially increasing number of Trotter steps, which remains challenging due to the limited performance of current quantum hardware. To alleviate this issue, post-processing techniques such as the multi-product formula (MPF) have been introduced to suppress algorithmic errors within restricted hardware resources.\n  In this work, we propose a dual-channel multi-product formula that achieves a two-fold improvement in Trotter error scaling. As a result, our method enables the target simulation precision to be reached with approximately half the circuit depth compared to conventional MPF schemes. Importantly, the reduced circuit depth directly translates into lower physical error mitigation overhead when implemented on real quantum hardware. We demonstrate that, for a fixed CNOT count as a measure of quantum circuit, our proposal yields significantly smaller algorithmic errors, while the sampling error remains essentially unchanged.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u901a\u9053\u591a\u4e58\u79ef\u516c\u5f0f\uff0c\u5c06\u7279\u7f57\u7279\u8bef\u5dee\u7f29\u653e\u63d0\u5347\u4e24\u500d\uff0c\u7535\u8def\u6df1\u5ea6\u51cf\u534a\uff0c\u5728\u76f8\u540cCNOT\u6570\u91cf\u4e0b\u663e\u8457\u964d\u4f4e\u7b97\u6cd5\u8bef\u5dee", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4e58\u79ef\u516c\u5f0f\u7684\u91cf\u5b50\u6a21\u62df\u9700\u8981\u5927\u91cf\u7279\u7f57\u7279\u6b65\u9aa4\u624d\u80fd\u8fbe\u5230\u7cbe\u5ea6\u8981\u6c42\uff0c\u53d7\u5f53\u524d\u91cf\u5b50\u786c\u4ef6\u6027\u80fd\u9650\u5236\uff0c\u73b0\u6709\u540e\u5904\u7406\u6280\u672f\u5982\u591a\u4e58\u79ef\u516c\u5f0f\u4ecd\u9700\u6539\u8fdb", "method": "\u63d0\u51fa\u4e00\u79cd\u53cc\u901a\u9053\u591a\u4e58\u79ef\u516c\u5f0f\uff0c\u5b9e\u73b0\u7279\u7f57\u7279\u8bef\u5dee\u7f29\u653e\u7684\u4e24\u500d\u6539\u8fdb\uff0c\u4f7f\u8fbe\u5230\u76ee\u6807\u7cbe\u5ea6\u6240\u9700\u7684\u7535\u8def\u6df1\u5ea6\u6bd4\u4f20\u7edfMPF\u65b9\u6848\u51cf\u5c11\u7ea6\u4e00\u534a", "result": "\u5728\u56fa\u5b9aCNOT\u6570\u91cf\u6761\u4ef6\u4e0b\uff0c\u7b97\u6cd5\u8bef\u5dee\u663e\u8457\u51cf\u5c0f\uff0c\u91c7\u6837\u8bef\u5dee\u57fa\u672c\u4e0d\u53d8\uff0c\u7535\u8def\u6df1\u5ea6\u964d\u4f4e\u76f4\u63a5\u51cf\u5c11\u4e86\u7269\u7406\u9519\u8bef\u7f13\u89e3\u5f00\u9500", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8fd1-term\u91cf\u5b50\u8ba1\u7b97\u673a\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u91cf\u5b50\u6a21\u62df\u65b9\u6848\uff0c\u5728\u6709\u9650\u786c\u4ef6\u8d44\u6e90\u4e0b\u66f4\u597d\u5730\u6291\u5236\u7b97\u6cd5\u8bef\u5dee"}}
{"id": "2602.00282", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00282", "abs": "https://arxiv.org/abs/2602.00282", "authors": ["Naman Saxena", "Vaneet Aggarwal"], "title": "Sample Complexity Analysis for Constrained Bilevel Reinforcement Learning", "comment": null, "summary": "Several important problem settings within the literature of reinforcement learning (RL), such as meta-learning, hierarchical learning, and RL from human feedback (RL-HF), can be modelled as bilevel RL problems. A lot has been achieved in these domains empirically; however, the theoretical analysis of bilevel RL algorithms hasn't received a lot of attention. In this work, we analyse the sample complexity of a constrained bilevel RL algorithm, building on the progress in the unconstrained setting. We obtain an iteration complexity of $O(\u03b5^{-2})$ and sample complexity of $\\tilde{O}(\u03b5^{-4})$ for our proposed algorithm, Constrained Bilevel Subgradient Optimization (CBSO). We use a penalty-based objective function to avoid the issue of primal-dual gap and hyper-gradient in the context of a constrained bilevel problem setting. The penalty-based formulation to handle constraints requires analysis of non-smooth optimization. We are the first ones to analyse the generally parameterized policy gradient-based RL algorithm with a non-smooth objective function using the Moreau envelope.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u4ece\u7406\u8bba\u89d2\u5ea6\u5206\u6790\u4e86\u7ea6\u675f\u53cc\u5c42\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5CBSO\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u901a\u8fc7\u57fa\u4e8e\u60e9\u7f5a\u7684\u76ee\u6807\u51fd\u6570\u548cMoreau\u5305\u7edc\u5206\u6790\u975e\u5149\u6ed1\u4f18\u5316\uff0c\u83b7\u5f97\u4e86O(\u03b5\u207b\u2074)\u7684\u6837\u672c\u590d\u6742\u5ea6\u3002", "motivation": "\u5143\u5b66\u4e60\u3001\u5206\u5c42\u5b66\u4e60\u3001\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\u7b49\u91cd\u8981\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u5747\u53ef\u5efa\u6a21\u4e3a\u53cc\u5c42\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u5c3d\u7ba1\u8fd9\u4e9b\u9886\u57df\u5df2\u53d6\u5f97\u5927\u91cf\u5b9e\u8bc1\u6210\u679c\uff0c\u4f46\u5176\u7406\u8bba\u5206\u6790\u5374\u672a\u53d7\u5230\u8db3\u591f\u91cd\u89c6\u3002", "method": "\u5728\u5df2\u6709\u65e0\u7ea6\u675f\u53cc\u5c42\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u7ea6\u675f\u53cc\u5c42\u6b21\u68af\u5ea6\u4f18\u5316\u7b97\u6cd5(CBSO)\uff0c\u91c7\u7528\u57fa\u4e8e\u60e9\u7f5a\u7684\u76ee\u6807\u51fd\u6570\u907f\u514d\u539f\u59cb-\u5bf9\u5076\u95f4\u9699\u548c\u8d85\u68af\u5ea6\u95ee\u9898\uff0c\u5e76\u8fd0\u7528Moreau\u5305\u7edc\u5206\u6790\u975e\u5149\u6ed1\u4f18\u5316\u3002", "result": "\u6240\u63d0\u7b97\u6cd5\u7684\u8fed\u4ee3\u590d\u6742\u5ea6\u4e3aO(\u03b5\u207b\u00b2)\uff0c\u6837\u672c\u590d\u6742\u5ea6\u4e3a\u00d5(\u03b5\u207b\u2074)\u3002", "conclusion": "\u672c\u7814\u7a76\u662f\u9996\u4e2a\u4f7f\u7528Moreau\u5305\u7edc\u5206\u6790\u4e00\u822c\u53c2\u6570\u5316\u7b56\u7565\u68af\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e0e\u975e\u5149\u6ed1\u76ee\u6807\u51fd\u6570\u7684\u5de5\u4f5c\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7406\u8bba\u7a7a\u767d\u3002"}}
{"id": "2602.00871", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00871", "abs": "https://arxiv.org/abs/2602.00871", "authors": ["Hossein A. Rahmani", "Mengting Wan", "Pei Zhou", "Longqi Yang", "Nick Craswell", "Emine Yilmaz", "Sujay Kumar Jauhar"], "title": "Beyond Output Critique: Self-Correction via Task Distillation", "comment": null, "summary": "Large language models (LLMs) have shown promising self-correction abilities, where iterative refinement improves the quality of generated responses. However, most existing approaches operate at the level of output critique, patching surface errors while often failing to correct deeper reasoning flaws. We propose SELF-THOUGHT, a framework that introduces an intermediate step of task abstraction before solution refinement. Given an input and an initial response, the model first distills the task into a structured template that captures key variables, constraints, and problem structure. This abstraction then guides solution instantiation, grounding subsequent responses in a clearer understanding of the task and reducing error propagation. Crucially, we show that these abstractions can be transferred across models: templates generated by larger models can serve as structured guides for smaller LLMs, which typically struggle with intrinsic self-correction. By reusing distilled task structures, smaller models achieve more reliable refinements without heavy fine-tuning or reliance on external verifiers. Experiments across diverse reasoning tasks demonstrate that SELF-THOUGHT improves accuracy, robustness, and generalization for both large and small models, offering a scalable path toward more reliable self-correcting language systems.", "AI": {"tldr": "The paper proposes SELF-THOUGHT, a framework that improves LLM self-correction by first abstracting tasks into structured templates before refinement, enabling cross-model transfer from large to small models.", "motivation": "Existing LLM self-correction methods only patch surface-level errors and fail to address deeper reasoning flaws, operating solely at the output critique level.", "method": "SELF-THOUGHT introduces an intermediate task abstraction step where models distill inputs into structured templates capturing key variables and constraints, which then guide solution refinement. These templates can be transferred from larger to smaller models.", "result": "Experiments across diverse reasoning tasks show SELF-THOUGHT improves accuracy, robustness, and generalization for both large and small models, allowing smaller models to achieve reliable self-correction without heavy fine-tuning.", "conclusion": "The framework provides a scalable approach to building more reliable self-correcting language systems by leveraging cross-model abstraction transfer."}}
{"id": "2602.01715", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.01715", "abs": "https://arxiv.org/abs/2602.01715", "authors": ["Kaito Kashiwagi", "Akira Matsumura"], "title": "Gravitational effects on a dissipative two-level atom in the weak-field regime", "comment": "14 pages + appendices + references, 2 figures", "summary": "We investigate the dissipative dynamics of a two-level atom in a weak gravitational field. Using the Feynman--Vernon influence functional formalism, we derive a quantum master equation describing the two-level atom interacting with a scalar field in a Newtonian gravitational field, and compute the energy dissipation rate of the atom. We find that the spontaneous emission rate (the dissipation rate in vacuum) is modified by the gravitational field. Specifically, this modification depends on the atom's dipole, the position of the atom relative to the source of the gravitational field, and the frequency of the scalar radiation emitted by the atom. Furthermore, we identify the parameter regimes in which the spontaneous emission rate is enhanced or suppressed by gravity. We also discuss how the modification arises from time dilation and dipole radiation in a weak gravitational field. These findings provide a theoretical basis for exploring gravitational effects in open quantum systems.", "AI": {"tldr": "This paper studies how weak gravitational fields affect the energy dissipation of a two-level atom interacting with a scalar field, finding that gravity modifies spontaneous emission rates through time dilation effects.", "motivation": "To investigate gravitational effects on open quantum systems, specifically how a weak Newtonian gravitational field influences the dissipative dynamics and spontaneous emission of a two-level atom.", "method": "Using the Feynman-Vernon influence functional formalism to derive a quantum master equation for the atom-scalar field system in a gravitational field, then computing the energy dissipation rate.", "result": "The spontaneous emission rate is modified by gravity in a way that depends on the atom's dipole moment, its position relative to the gravitational source, and the emitted radiation frequency; both enhancement and suppression regimes are identified.", "conclusion": "The modification arises from time dilation and dipole radiation effects in weak gravity, providing a theoretical foundation for exploring gravitational effects in open quantum systems."}}
{"id": "2602.00286", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00286", "abs": "https://arxiv.org/abs/2602.00286", "authors": ["Shaorong Zhang", "Longxuan Yu", "Rob Brekelmans", "Luhan Tang", "Salman Asif", "Greg Ver Steeg"], "title": "Generation Order and Parallel Decoding in Masked Diffusion Models: An Information-Theoretic Perspective", "comment": null, "summary": "Masked Diffusion Models (MDMs) significantly accelerate inference by trading off sequential determinism. However, the theoretical mechanisms governing generation order and the risks inherent in parallelization remain under-explored. In this work, we provide a unified information-theoretic framework to decouple and analyze two fundamental sources of failure: order sensitivity and parallelization bias. Our analysis yields three key insights: (1) The benefits of Easy-First decoding (prioritizing low-entropy tokens) are magnified as model error increases; (2) factorized parallel decoding introduces intrinsic sampling errors that can lead to arbitrary large Reverse KL divergence, capturing \"incoherence\" failures that standard Forward KL metrics overlook; and (3) while verification can eliminate sampling error, it incurs an exponential cost governed by the total correlation within a block. Conversely, heuristics like remasking, though computationally efficient, cannot guarantee distributional correctness. Experiments on a controlled Block-HMM and large-scale MDMs (LLaDA) for arithmetic reasoning validate our theoretical framework.", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u4e86\u4fe1\u606f\u8bba\u6846\u67b6\u5206\u6790\u63a9\u7801\u6269\u6563\u6a21\u578b\uff0c\u53d1\u73b0\uff1aeasy-first \u89e3\u7801\u5728\u6a21\u578b\u8bef\u5dee\u8d8a\u5927\u65f6\u6536\u76ca\u8d8a\u663e\u8457\uff1b\u56e0\u5b50\u5316\u5e76\u884c\u89e3\u7801\u4f1a\u5bfc\u81f4\u4efb\u610f\u5927\u7684\u53cd\u5411 KL \u6563\u5ea6\uff0c\u4ea7\u751f\"\u4e0d\u8fde\u8d2f\"\u95ee\u9898\uff1b\u9a8c\u8bc1\u53ef\u6d88\u9664\u91c7\u6837\u8bef\u5dee\u4f46\u9700\u6307\u6570\u7ea7\u6210\u672c\uff0c\u800c\u91cd\u63a9\u7801\u7b49\u542f\u53d1\u5f0f\u65b9\u6cd5\u867d\u9ad8\u6548\u5374\u65e0\u6cd5\u4fdd\u8bc1\u5206\u5e03\u6b63\u786e\u6027\u3002", "motivation": "\u63a9\u7801\u6269\u6563\u6a21\u578b\u867d\u80fd\u52a0\u901f\u63a8\u7406\uff0c\u4f46\u5176\u751f\u6210\u987a\u5e8f\u673a\u5236\u548c\u5e76\u884c\u5316\u98ce\u9669\u7684\u7406\u8bba\u673a\u5236\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u7279\u522b\u662f\u987a\u5e8f\u654f\u611f\u6027\u548c\u5e76\u884c\u5316\u504f\u5dee\u4e24\u79cd\u57fa\u672c\u5931\u8d25\u6765\u6e90\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u4fe1\u606f\u8bba\u6846\u67b6\uff0c\u89e3\u8026\u5206\u6790\u987a\u5e8f\u654f\u611f\u6027\u548c\u5e76\u884c\u5316\u504f\u5dee\uff0c\u5728\u53d7\u63a7\u7684 Block-HMM \u548c\u5927\u89c4\u6a21 MDM\uff08LLaDA\uff09\u7b97\u672f\u63a8\u7406\u4efb\u52a1\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u4e09\u4e2a\u5173\u952e\u53d1\u73b0\uff1a(1) Easy-First \u89e3\u7801\u7684\u6536\u76ca\u968f\u6a21\u578b\u8bef\u5dee\u589e\u52a0\u800c\u653e\u5927\uff1b(2) \u56e0\u5b50\u5316\u5e76\u884c\u89e3\u7801\u5f15\u5165\u5185\u5728\u91c7\u6837\u8bef\u5dee\uff0c\u5bfc\u81f4\u4efb\u610f\u5927\u7684\u53cd\u5411 KL \u6563\u5ea6\uff0c\u6355\u6349\u5230\u6807\u51c6\u524d\u5411 KL \u6307\u6807\u5ffd\u7565\u7684\"\u4e0d\u8fde\u8d2f\"\u5931\u8d25\uff1b(3) \u9a8c\u8bc1\u53ef\u6d88\u9664\u91c7\u6837\u8bef\u5dee\u4f46\u6210\u672c\u968f\u5757\u5185\u603b\u76f8\u5173\u6027\u5448\u6307\u6570\u589e\u957f\uff0c\u800c\u91cd\u63a9\u7801\u7b49\u542f\u53d1\u5f0f\u65b9\u6cd5\u8ba1\u7b97\u9ad8\u6548\u4f46\u65e0\u6cd5\u4fdd\u8bc1\u5206\u5e03\u6b63\u786e\u6027\u3002", "conclusion": "\u8be5\u7406\u8bba\u6846\u67b6\u63ed\u793a\u4e86 MDM \u7684\u5931\u8d25\u673a\u5236\uff0c\u9a8c\u8bc1\u4e86\u9a8c\u8bc1\u4e0e\u542f\u53d1\u5f0f\u65b9\u6cd5\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5b9e\u9a8c\u7ed3\u679c\u5728\u5408\u6210\u548c\u771f\u5b9e\u4efb\u52a1\u4e0a\u5747\u652f\u6301\u7406\u8bba\u9884\u6d4b\u3002"}}
{"id": "2602.00911", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00911", "abs": "https://arxiv.org/abs/2602.00911", "authors": ["Abhijit Chakraborty", "Sandipan De", "Yash Shah", "Chahana Dahal", "Vivek Gupta"], "title": "Synapse Compendium Aware Federated Knowledge Exchange for Tool Routed LLMs", "comment": null, "summary": "Collaborative learning among LLM-based agents under federated learning faces challenges, including communication costs, heterogeneity in data, and tool-usage, limiting their effectiveness. We introduce Synapse, a framework that trains a shared global knowledge model of tool-usage behavior. Client agents with fixed LLMs learn tool-usage patterns locally, and transmit artifacts for federated aggregation through coordinators. A global tool compendium is updated and redistributed, enabling convergence toward stable tool selection. Synapse uses templated representations, embedding retrieval with LLM reranking, and adaptive masking to maintain utility while limiting information leakage. The framework supports heterogeneous data and quantifies performance improvements. Results show that Synapse improves tool-usage effectiveness and reduces communication overhead compared with weight or prompt-sharing approaches in multi-agent LLM systems.", "AI": {"tldr": "This paper introduces Synapse, a federated learning framework for LLM agents that shares tool-usage behavior patterns instead of model weights or prompts, reducing communication overhead while improving effectiveness.", "motivation": "Collaborative learning among LLM agents via federated learning suffers from high communication costs, data heterogeneity, and tool-usage diversity, which existing weight/prompt-sharing methods fail to adequately address.", "method": "Synapse trains a global knowledge model of tool-usage behavior through federated aggregation of local artifacts from client agents with fixed LLMs, using templated representations, embedding retrieval with LLM reranking, and adaptive masking to balance utility and privacy.", "result": "Experimental results demonstrate that Synapse achieves superior tool-usage effectiveness and significantly reduces communication overhead compared to baseline approaches, while maintaining compatibility with heterogeneous data distributions.", "conclusion": "The Synapse framework provides an effective solution for multi-agent LLM collaboration under federated learning by sharing high-level tool-usage knowledge rather than low-level model parameters, offering better performance, efficiency, and privacy protection."}}
{"id": "2602.01787", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.01787", "abs": "https://arxiv.org/abs/2602.01787", "authors": ["Guan-Jie Fan-Yuan", "Yang-Guang Shan", "Cong Zhang", "Yu-Long Wang", "Yu-Xuan Fan", "Wei-Xin Xie", "De-Yong He", "Shuang Wang", "Zhen-Qiang Yin", "Wei Chen", "Song-Nian Fu", "Guang-Can Guo", "Zheng-Fu Han"], "title": "Relativistic Position Verification with Coherent States", "comment": null, "summary": "Determining the position of an entity is a fundamental prerequisite for nearly all activities. Classical means, however, have been proven incapable of providing secure position verification, meaning that a prover can mislead verifiers about its actual position. In this work, we propose and experimentally realize a secure position-verification protocol that leverages quantum optics and relativity within an information-theoretic framework. Using phase-randomized weak coherent states, two verifiers separated by 2 km securely verify the prover's position with an accuracy better than 75 meters. These results establish secure position-based authentication as a practical possibility, paving the way for applications in financial transactions, disaster response, and authenticated secure communications.", "AI": {"tldr": "\u57fa\u4e8e\u91cf\u5b50\u5149\u5b66\u4e0e\u76f8\u5bf9\u8bba\u7684\u534f\u8bae\u5b9e\u73b02\u516c\u91cc\u5185\u5b89\u5168\u4f4d\u7f6e\u9a8c\u8bc1\uff0c\u7cbe\u5ea6\u4f18\u4e8e75\u7c73", "motivation": "\u7ecf\u5178\u5b9a\u4f4d\u65b9\u6cd5\u65e0\u6cd5\u63d0\u4f9b\u5b89\u5168\u7684\u4f4d\u7f6e\u9a8c\u8bc1\uff0c\u5b58\u5728\u8bc1\u660e\u8005\u8bef\u5bfc\u9a8c\u8bc1\u8005\u7684\u98ce\u9669", "method": "\u7ed3\u5408\u91cf\u5b50\u5149\u5b66\u4e0e\u76f8\u5bf9\u8bba\u7684\u4fe1\u606f\u7406\u8bba\u6846\u67b6\uff0c\u91c7\u7528\u76f8\u4f4d\u968f\u673a\u5316\u5f31\u76f8\u5e72\u6001\u6784\u5efa\u9a8c\u8bc1\u534f\u8bae", "result": "\u57282\u516c\u91cc\u8ddd\u79bb\u5185\u5b9e\u73b0\u4f18\u4e8e75\u7c73\u7cbe\u5ea6\u7684\u5b89\u5168\u4f4d\u7f6e\u9a8c\u8bc1", "conclusion": "\u4e3a\u91d1\u878d\u4ea4\u6613\u3001\u707e\u5bb3\u54cd\u5e94\u548c\u8ba4\u8bc1\u5b89\u5168\u901a\u4fe1\u7b49\u5e94\u7528\u63d0\u4f9b\u53ef\u884c\u7684\u5b89\u5168\u4f4d\u7f6e\u8ba4\u8bc1\u65b9\u6848"}}
{"id": "2602.00294", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.00294", "abs": "https://arxiv.org/abs/2602.00294", "authors": ["Franz A. Heinsen", "Leo Kozachkov"], "title": "Self-Attention at Constant Cost per Token via Symmetry-Aware Taylor Approximation", "comment": "For source code and replication instructions, see https://github.com/glassroom/sata_attention. 12 pages, 6 figures (main); 4 pages, 2 figures (appendix)", "summary": "The most widely used artificial intelligence (AI) models today are Transformers employing self-attention. In its standard form, self-attention incurs costs that increase with context length, driving demand for storage, compute, and energy that is now outstripping society's ability to provide them. To help address this issue, we show that self-attention is efficiently computable to arbitrary precision with constant cost per token, achieving orders-of-magnitude reductions in memory use and computation. We derive our formulation by decomposing the conventional formulation's Taylor expansion into expressions over symmetric chains of tensor products. We exploit their symmetry to obtain feed-forward transformations that efficiently map queries and keys to coordinates in a minimal polynomial-kernel feature basis. Notably, cost is fixed inversely in proportion to head size, enabling application over a greater number of heads per token than otherwise feasible. We implement our formulation and empirically validate its correctness. Our work enables unbounded token generation at modest fixed cost, substantially reducing the infrastructure and energy demands of large-scale Transformer models. The mathematical techniques we introduce are of independent interest.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\u673a\u5236\uff0c\u901a\u8fc7\u5f20\u91cf\u5bf9\u79f0\u6027\u5206\u89e3\u5c06\u8ba1\u7b97\u6210\u672c\u964d\u81f3\u5e38\u6570\u7ea7\uff0c\u5b9e\u73b0\u65e0\u9650\u957f\u6587\u672c\u751f\u6210\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u5b58\u50a8\u548c\u7b97\u529b\u9700\u6c42", "motivation": "\u6807\u51c6Transformer\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u8ba1\u7b97\u6210\u672c\u968f\u4e0a\u4e0b\u6587\u957f\u5ea6\u7ebf\u6027\u589e\u957f\uff0c\u5bfc\u81f4\u5b58\u50a8\u3001\u7b97\u529b\u548c\u80fd\u8017\u9700\u6c42\u5df2\u8d85\u51fa\u793e\u4f1a\u4f9b\u5e94\u80fd\u529b\uff0c\u4e9f\u9700\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848", "method": "\u5c06\u4f20\u7edf\u81ea\u6ce8\u610f\u529b\u7684\u6cf0\u52d2\u5c55\u5f00\u5206\u89e3\u4e3a\u5bf9\u79f0\u5f20\u91cf\u94fe\u8868\u8fbe\u5f0f\uff0c\u5229\u7528\u5bf9\u79f0\u6027\u6784\u5efa\u524d\u9988\u53d8\u6362\uff0c\u5c06\u67e5\u8be2\u548c\u952e\u6620\u5c04\u5230\u6700\u5c0f\u591a\u9879\u5f0f\u6838\u7279\u5f81\u57fa\u5750\u6807\uff0c\u5b9e\u73b0\u4efb\u610f\u7cbe\u5ea6\u5e38\u6570\u7ea7\u8ba1\u7b97", "result": "- \u6bcftoken\u8ba1\u7b97\u6210\u672c\u6052\u5b9a\uff0c\u4e0e\u4e0a\u4e0b\u6587\u957f\u5ea6\u65e0\u5173\n- \u5185\u5b58\u548c\u8ba1\u7b97\u91cf\u964d\u4f4e\u6570\u4e2a\u6570\u91cf\u7ea7\n- \u652f\u6301\u6bcftoken\u66f4\u591a\u6ce8\u610f\u529b\u5934\n- \u7ecf\u9a8c\u8bc1\u6570\u5b66\u6b63\u786e\u6027\n- \u5b9e\u73b0\u65e0\u9650token\u751f\u6210", "conclusion": "\u8be5\u673a\u5236\u4ee5\u56fa\u5b9a\u6210\u672c\u5b9e\u73b0\u65e0\u754c\u6587\u672c\u751f\u6210\uff0c\u663e\u8457\u964d\u4f4e\u5927\u89c4\u6a21Transformer\u6a21\u578b\u7684\u57fa\u5efa\u4e0e\u80fd\u8017\u9700\u6c42\uff1b\u63d0\u51fa\u7684\u6570\u5b66\u65b9\u6cd5\u5177\u6709\u72ec\u7acb\u5b66\u672f\u4ef7\u503c"}}
{"id": "2602.00924", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00924", "abs": "https://arxiv.org/abs/2602.00924", "authors": ["Ouns El Harzli", "Hugo Wallner", "Yoonsoo Nam", "Haixuan Xavier Tao"], "title": "Supervised sparse auto-encoders as unconstrained feature models for semantic composition", "comment": null, "summary": "Sparse auto-encoders (SAEs) have re-emerged as a prominent method for mechanistic interpretability, yet they face two significant challenges: the non-smoothness of the $L_1$ penalty, which hinders reconstruction and scalability, and a lack of alignment between learned features and human semantics. In this paper, we address these limitations by adapting unconstrained feature models-a mathematical framework from neural collapse theory-and by supervising the task. We supervise (decoder-only) SAEs to reconstruct feature vectors by jointly learning sparse concept embeddings and decoder weights. Validated on Stable Diffusion 3.5, our approach demonstrates compositional generalization, successfully reconstructing images with concept combinations unseen during training, and enabling feature-level intervention for semantic image editing without prompt modification.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u65e0\u7ea6\u675f\u7279\u5f81\u6a21\u578b\u548c\u76d1\u7763\u5f0f\u89e3\u7801\u5668-only\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff0c\u89e3\u51b3L1\u60e9\u7f5a\u975e\u5149\u6ed1\u6027\u548c\u7279\u5f81\u8bed\u4e49\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u5728Stable Diffusion 3.5\u4e0a\u5b9e\u73b0\u7ec4\u5408\u6cdb\u5316\u548c\u65e0\u9700\u4fee\u6539\u63d0\u793a\u7684\u8bed\u4e49\u56fe\u50cf\u7f16\u8f91\u3002", "motivation": "\u7a00\u758f\u81ea\u7f16\u7801\u5668\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1aL1\u60e9\u7f5a\u9879\u7684\u975e\u5149\u6ed1\u6027\u963b\u788d\u4e86\u91cd\u5efa\u8d28\u91cf\u548c\u53ef\u6269\u5c55\u6027\uff1b\u5b66\u4e60\u5230\u7684\u7279\u5f81\u4e0e\u4eba\u7c7b\u8bed\u4e49\u7f3a\u4e4f\u5bf9\u9f50\uff0c\u9650\u5236\u4e86\u5176\u5728mechanistic interpretability\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u4f5c\u8005\u501f\u9274\u795e\u7ecf\u574d\u7f29\u7406\u8bba\u4e2d\u7684\u65e0\u7ea6\u675f\u7279\u5f81\u6a21\u578b\uff0c\u76d1\u7763\u5f0f\u8bad\u7ec3\u89e3\u7801\u5668-only\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u7a00\u758f\u6982\u5ff5\u5d4c\u5165\u548c\u89e3\u7801\u5668\u6743\u91cd\u6765\u91cd\u5efa\u7279\u5f81\u5411\u91cf\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684L1\u6b63\u5219\u5316\u3002", "result": "\u5728Stable Diffusion 3.5\u4e0a\u7684\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u5907\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\u2014\u2014\u80fd\u91cd\u5efa\u8bad\u7ec3\u65f6\u672a\u89c1\u8fc7\u7684\u6982\u5ff5\u7ec4\u5408\u56fe\u50cf\uff0c\u5e76\u652f\u6301\u7279\u5f81\u7ea7\u5e72\u9884\u4ee5\u5b9e\u73b0\u65e0\u9700\u4fee\u6539\u63d0\u793a\u7684\u8bed\u4e49\u56fe\u50cf\u7f16\u8f91\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u521b\u5efa\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u6269\u5c55\u6027\u7684\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff0c\u63a8\u8fdb\u4e86 mechanistic interpretability \u7684\u53d1\u5c55\uff0c\u4e3a\u901a\u8fc7\u7279\u5f81\u64cd\u63a7\u5b9e\u73b0\u53ef\u63a7\u751f\u6210\u6a21\u578b\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.01794", "categories": ["quant-ph", "cond-mat.mes-hall", "math-ph"], "pdf": "https://arxiv.org/pdf/2602.01794", "abs": "https://arxiv.org/abs/2602.01794", "authors": ["Soumyadeep Sarma", "Manas Kulkarni", "Archak Purkayastha", "Devashish Tupkary"], "title": "Semidefinite programming for understanding limitations of Lindblad equations", "comment": "18 pages, 7 figures, 2 tables", "summary": "Lindbladian quantum master equations (LEs) are the most popular descriptions for quantum systems weakly coupled to baths. But, recent works have established that in many situations such Markovian descriptions are fundamentally limited: they cannot simultaneously capture populations and coherences even to the leading-order in system-bath couplings. This can cause violation of fundamental properties like thermalization and continuity equations associated with local conservation laws, even when such properties are expected in the actual setting. This begs the question: given a physical situation, how do we know if there exists an LE that describes it to a desired accuracy? Here we show that, for both equilibrium and non-equilibrium steady states (NESS), this question can be succinctly formulated as a semidefinite program (SDP), a convex optimization technique. If a solution to the SDP can be found to a desired accuracy, then an LE description is possible for the chosen setting. If not, no LE description is fundamentally attainable, showing that a consistent Markovian treatment is impossible even at weak system-bath coupling for that particular setting. Considering few qubit isotropic XXZ-type models coupled to multiple baths, we find that in most parameter regimes, LE description giving accurate populations and coherences to leading-order is unattainable, leading to rigorous no-go results. However, in some cases, LE description having correct populations but inaccurate coherences, and satisfying local conservation laws, is possible over some of the parameter regimes. Our work highlights the power of semidefinite programming in the analysis of physically consistent LEs, thereby, in understanding the limits of Markovian descriptions at weak system-bath couplings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u89e3\u51b3\u4e86\u91cf\u5b50\u7cfb\u7edf\u5f31\u8026\u5408\u5230\u70ed\u6d74\u65f6\uff0cLindblad\u4e3b\u65b9\u7a0b\uff08LE\uff09\u65e0\u6cd5\u540c\u65f6\u51c6\u786e\u63cf\u8ff0\u5e03\u5c45\u6570\u548c\u76f8\u5e72\u6027\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u7528\u534a\u5b9a\u89c4\u5212\uff08SDP\uff09\u5224\u65ad\u662f\u5426\u5b58\u5728\u6ee1\u8db3\u7279\u5b9a\u7cbe\u5ea6\u7684LE\u63cf\u8ff0\uff0c\u5e76\u53d1\u73b0\u591a\u6570\u60c5\u51b5\u4e0b\u6b64\u7c7bLE\u4e0d\u5b58\u5728\uff0c\u4f46\u5728\u67d0\u4e9b\u53c2\u6570\u8303\u56f4\u5185\u53ef\u5b58\u5728\u4ec5\u51c6\u786e\u63cf\u8ff0\u5e03\u5c45\u6570\u7684LE\u3002", "motivation": "\u4f20\u7edfLindblad\u4e3b\u65b9\u7a0b\u5728\u5f31\u8026\u5408\u4e0b\u65e0\u6cd5\u540c\u65f6\u51c6\u786e\u63cf\u8ff0\u91cf\u5b50\u7cfb\u7edf\u7684\u5e03\u5c45\u6570\u548c\u76f8\u5e72\u6027\uff0c\u5bfc\u81f4\u8fdd\u53cd\u70ed\u5316\u53ca\u5c40\u57df\u5b88\u6052\u5f8b\u7b49\u57fa\u672c\u7269\u7406\u6027\u8d28\uff0c\u4e9f\u9700\u5224\u65ad\u7279\u5b9a\u7269\u7406\u573a\u666f\u662f\u5426\u5b58\u5728\u6ee1\u8db3\u7cbe\u5ea6\u8981\u6c42\u7684LE\u63cf\u8ff0\u3002", "method": "\u5c06\u5e73\u8861\u6001\u548c\u975e\u5e73\u8861\u7a33\u6001\uff08NESS\uff09\u4e0bLE\u5b58\u5728\u6027\u95ee\u9898\u8f6c\u5316\u4e3a\u534a\u5b9a\u89c4\u5212\uff08SDP\uff09\u51f8\u4f18\u5316\u95ee\u9898\uff1a\u82e5SDP\u5b58\u5728\u7b26\u5408\u7cbe\u5ea6\u7684\u89e3\uff0c\u5219LE\u63cf\u8ff0\u53ef\u884c\uff1b\u5426\u5219\u4e0d\u5b58\u5728\u6839\u672c\u53ef\u884c\u7684\u9a6c\u5c14\u53ef\u592b\u63cf\u8ff0\u3002", "result": "\u5728\u5404\u5411\u540c\u6027XXZ qubit\u6a21\u578b\u4e2d\uff0c\u591a\u6570\u53c2\u6570\u533a\u57df\u4e0d\u5b58\u5728\u80fd\u540c\u65f6\u51c6\u786e\u63cf\u8ff0\u5e03\u5c45\u6570\u548c\u76f8\u5e72\u6027\u7684LE\uff1b\u4f46\u5728\u90e8\u5206\u533a\u57df\uff0c\u5b58\u5728\u4ec5\u51c6\u786e\u63cf\u8ff0\u5e03\u5c45\u6570\uff08\u4f46\u76f8\u5e72\u6027\u4e0d\u51c6\uff09\u4e14\u6ee1\u8db3\u5c40\u57df\u5b88\u6052\u5f8b\u7684LE\u3002", "conclusion": "\u534a\u5b9a\u89c4\u5212\u4e3a\u5206\u6790\u7269\u7406\u81ea\u6d3d\u7684LE\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u5f31\u8026\u5408\u4e0b\u9a6c\u5c14\u53ef\u592b\u63cf\u8ff0\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5224\u65adLE\u9002\u7528\u6027\u63d0\u4f9b\u4e86\u666e\u9002\u6027\u5224\u636e\u3002"}}
{"id": "2602.00297", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00297", "abs": "https://arxiv.org/abs/2602.00297", "authors": ["Jie Yang", "Yifan Hu", "Yuante Li", "Kexin Zhang", "Kaize Ding", "Philip S. Yu"], "title": "From Observations to States: Latent Time Series Forecasting", "comment": null, "summary": "Deep learning has achieved strong performance in Time Series Forecasting (TSF). However, we identify a critical representation paradox, termed Latent Chaos: models with accurate predictions often learn latent representations that are temporally disordered and lack continuity. We attribute this phenomenon to the dominant observation-space forecasting paradigm. Most TSF models minimize point-wise errors on noisy and partially observed data, which encourages shortcut solutions instead of the recovery of underlying system dynamics. To address this issue, we propose Latent Time Series Forecasting (LatentTSF), a novel paradigm that shifts TSF from observation regression to latent state prediction. Specifically, LatentTSF employs an AutoEncoder to project observations at each time step into a higher-dimensional latent state space. This expanded representation aims to capture underlying system variables and impose a smoother temporal structure. Forecasting is then performed entirely in the latent space, allowing the model to focus on learning structured temporal dynamics. Theoretical analysis demonstrates that our proposed latent objectives implicitly maximize mutual information between predicted latent states and ground-truth states and observations. Extensive experiments on widely-used benchmarks confirm that LatentTSF effectively mitigates latent chaos, achieving superior performance. Our code is available in https://github.com/Muyiiiii/LatentTSF.", "AI": {"tldr": "\u8be5\u8bba\u6587\u53d1\u73b0\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u5b58\u5728\"\u6f5c\u5728\u6df7\u6c8c\"\u73b0\u8c61\uff0c\u5373\u9884\u6d4b\u51c6\u786e\u7684\u6a21\u578b\u5f80\u5f80\u5b66\u5230\u65f6\u95f4\u65e0\u5e8f\u4e14\u4e0d\u8fde\u7eed\u7684\u6f5c\u5728\u8868\u793a\u3002\u4e3a\u6b64\u63d0\u51faLatentTSF\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u81ea\u7f16\u7801\u5668\u5c06\u89c2\u6d4b\u6295\u5f71\u5230\u9ad8\u7ef4\u6f5c\u5728\u7a7a\u95f4\u5e76\u5728\u5176\u4e2d\u8fdb\u884c\u9884\u6d4b\uff0c\u7406\u8bba\u4e0a\u53ef\u6700\u5927\u5316\u4e92\u4fe1\u606f\uff0c\u5b9e\u9a8c\u8bc1\u660e\u80fd\u6709\u6548\u7f13\u89e3\u6f5c\u5728\u6df7\u6c8c\u5e76\u83b7\u5f97\u66f4\u4f18\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u5728\u89c2\u6d4b\u7a7a\u95f4\u4e2d\u6700\u5c0f\u5316\u9010\u70b9\u8bef\u5dee\uff0c\u5bfc\u81f4\u6a21\u578b\u5b66\u5230\"\u6f5c\u5728\u6df7\u6c8c\"\u2014\u2014\u6f5c\u5728\u8868\u793a\u65f6\u95f4\u65e0\u5e8f\u4e14\u7f3a\u4e4f\u8fde\u7eed\u6027\u3002\u8fd9\u79cd\u57fa\u4e8e\u89c2\u6d4b\u7684\u8303\u5f0f\u9f13\u52b1\u6377\u5f84\u89e3\u800c\u975e\u6062\u590d\u771f\u5b9e\u7cfb\u7edf\u52a8\u529b\u5b66\uff0c\u9700\u8981\u8f6c\u5411\u6f5c\u5728\u72b6\u6001\u9884\u6d4b\u7684\u65b0\u8303\u5f0f\u3002", "method": "\u63d0\u51faLatent Time Series Forecasting (LatentTSF)\u8303\u5f0f\uff1a\u4f7f\u7528\u81ea\u7f16\u7801\u5668\u5c06\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u89c2\u6d4b\u6295\u5f71\u5230\u66f4\u9ad8\u7ef4\u7684\u6f5c\u5728\u72b6\u6001\u7a7a\u95f4\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b8c\u6210\u6574\u4e2a\u9884\u6d4b\u8fc7\u7a0b\u3002\u901a\u8fc7\u6269\u5c55\u8868\u793a\u6765\u6355\u6349\u5e95\u5c42\u7cfb\u7edf\u53d8\u91cf\u5e76\u65bd\u52a0\u66f4\u5e73\u6ed1\u7684\u65f6\u95f4\u7ed3\u6784\u3002", "result": "\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u5b9e\uff0cLatentTSF\u80fd\u6709\u6548\u7f13\u89e3\u6f5c\u5728\u6df7\u6c8c\u95ee\u9898\uff0c\u5b9e\u73b0\u5353\u8d8a\u6027\u80fd\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u8be5\u6f5c\u5728\u76ee\u6807\u51fd\u6570\u9690\u5f0f\u5730\u6700\u5927\u5316\u4e86\u9884\u6d4b\u6f5c\u5728\u72b6\u6001\u4e0e\u771f\u5b9e\u72b6\u6001\u53ca\u89c2\u6d4b\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u3002", "conclusion": "\u5c06\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4ece\u89c2\u6d4b\u56de\u5f52\u8f6c\u5411\u6f5c\u5728\u72b6\u6001\u9884\u6d4b\u662f\u4e00\u79cd\u6709\u524d\u9014\u7684\u65b0\u8303\u5f0f\uff0c\u80fd\u591f\u89e3\u51b3\u6f5c\u5728\u6df7\u6c8c\u95ee\u9898\u5e76\u6784\u5efa\u51fa\u6027\u80fd\u66f4\u4f18\u7684\u6a21\u578b\u3002"}}
{"id": "2602.00299", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00299", "abs": "https://arxiv.org/abs/2602.00299", "authors": ["Rituparna Datta", "Zihan Guan", "Baltazar Espinoza", "Yiqi Su", "Priya Pitre", "Srini Venkatramanan", "Naren Ramakrishnan", "Anil Vullikanti"], "title": "Agentic Framework for Epidemiological Modeling", "comment": null, "summary": "Epidemic modeling is essential for public health planning, yet traditional approaches rely on fixed model classes that require manual redesign as pathogens, policies, and scenario assumptions evolve. We introduce EPIAGENT, an agentic framework that automatically synthesizes, calibrates, verifies, and refines epidemiological simulators by modeling disease progression as an iterative program synthesis problem. A central design choice is an explicit epidemiological flow graph intermediate representation that links scenario specifications to model structure and enables strong, modular correctness checks before code is generated. Verified flow graphs are then compiled into mechanistic models supporting interpretable parameter learning under physical and epidemiological constraints. Evaluation on epidemiological scenario case studies demonstrates that EPIAGENT captures complex growth dynamics and produces epidemiologically consistent counterfactual projections across varying vaccination and immune escape assumptions. Our results show that the agentic feedback loop prevents degeneration and significantly accelerates convergence toward valid models by mimicking professional expert workflows.", "AI": {"tldr": "EPIAGENT\u662f\u4e00\u4e2a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u7a0b\u5e8f\u5408\u6210\u548c\u6d41\u884c\u75c5\u5b66\u6d41\u56fe\u4e2d\u95f4\u8868\u793a\uff0c\u81ea\u52a8\u6784\u5efa\u3001\u9a8c\u8bc1\u548c\u4f18\u5316\u6d41\u884c\u75c5\u5b66\u6a21\u62df\u5668\uff0c\u663e\u8457\u52a0\u5feb\u6a21\u578b\u6536\u655b\u901f\u5ea6\u5e76\u4fdd\u6301\u6d41\u884c\u75c5\u5b66\u4e00\u81f4\u6027\u3002", "motivation": "\u4f20\u7edf\u6d41\u884c\u75c5\u5b66\u5efa\u6a21\u4f9d\u8d56\u56fa\u5b9a\u6a21\u578b\u7c7b\uff0c\u9700\u8981\u624b\u52a8\u91cd\u65b0\u8bbe\u8ba1\u4ee5\u9002\u5e94\u75c5\u539f\u4f53\u3001\u653f\u7b56\u548c\u573a\u666f\u5047\u8bbe\u7684\u53d8\u5316\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u3002", "method": "\u63d0\u51faEPIAGENT\u6846\u67b6\uff0c\u5c06\u75be\u75c5\u8fdb\u5c55\u5efa\u6a21\u4e3a\u8fed\u4ee3\u7a0b\u5e8f\u5408\u6210\u95ee\u9898\u3002\u6838\u5fc3\u8bbe\u8ba1\u91c7\u7528\u660e\u786e\u7684\u6d41\u884c\u75c5\u5b66\u6d41\u56fe\u4e2d\u95f4\u8868\u793a\uff0c\u8fde\u63a5\u573a\u666f\u89c4\u8303\u4e0e\u6a21\u578b\u7ed3\u6784\uff0c\u652f\u6301\u4ee3\u7801\u751f\u6210\u524d\u7684\u6a21\u5757\u5316\u6b63\u786e\u6027\u68c0\u67e5\u3002\u9a8c\u8bc1\u540e\u7684\u6d41\u56fe\u88ab\u7f16\u8bd1\u4e3a\u673a\u5236\u6a21\u578b\uff0c\u5728\u7269\u7406\u548c\u6d41\u884c\u75c5\u5b66\u7ea6\u675f\u4e0b\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u53c2\u6570\u5b66\u4e60\u3002", "result": "\u5728\u6d41\u884c\u75c5\u5b66\u573a\u666f\u6848\u4f8b\u8bc4\u4f30\u4e2d\uff0cEPIAGENT\u80fd\u6355\u6349\u590d\u6742\u589e\u957f\u52a8\u6001\uff0c\u5728\u4e0d\u540c\u75ab\u82d7\u63a5\u79cd\u548c\u514d\u75ab\u9003\u9038\u5047\u8bbe\u4e0b\u751f\u6210\u4e00\u81f4\u7684\u56e0\u679c\u63a8\u65ad\u3002\u667a\u80fd\u4f53\u53cd\u9988\u5faa\u73af\u6709\u6548\u9632\u6b62\u6a21\u578b\u9000\u5316\uff0c\u5e76\u663e\u8457\u52a0\u901f\u5411\u6709\u6548\u6a21\u578b\u7684\u6536\u655b\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6a21\u62df\u4e13\u4e1a\u4e13\u5bb6\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5b9e\u73b0\u4e86\u6d41\u884c\u75c5\u5b66\u6a21\u578b\u7684\u81ea\u52a8\u5316\u6784\u5efa\u4e0e\u4f18\u5316\uff0c\u4e3a\u516c\u5171\u536b\u751f\u89c4\u5212\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u9ad8\u6548\u7684\u5efa\u6a21\u5de5\u5177\u3002"}}
{"id": "2602.00947", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00947", "abs": "https://arxiv.org/abs/2602.00947", "authors": ["Mohan Reddy"], "title": "The Keyhole Effect: Why Chat Interfaces Fail at Data Analysis", "comment": null, "summary": "Chat has become the default interface for AI-assisted data analysis. For multi-step, state-dependent analytical tasks, this is a mistake. Building on Woods (1984) Keyhole Effect, the cognitive cost of viewing large information spaces through narrow viewports, I show that chat interfaces systematically degrade analytical performance through five mechanisms: (1) constant content displacement defeats hippocampal spatial memory systems; (2) hidden state variables exceed working memory capacity (approximately 4 chunks under load); (3) forced verbalization triggers verbal overshadowing, degrading visual pattern recognition; (4) linear text streams block epistemic action and cognitive offloading; (5) serialization penalties scale with data dimensionality. I formalize cognitive overload as O = max(0, m - v - W) where m is task-relevant items, v is visible items, and W is working memory capacity. When O > 0, error probability increases and analytical biases (anchoring, confirmation, change blindness) amplify. Eight hybrid design patterns address these failures: Generative UI, Infinite Canvas, Deictic Interaction, State Rail, Ghost Layers, Mise en Place, Semantic Zoom, and Probabilistic UI. Each pattern targets specific cognitive bottlenecks while preserving natural language for intent specification and synthesis. Well-scaffolded conversational systems that encode expert priors may reduce load for guided tasks; the framework applies most strongly to open-ended exploration. The paper concludes with falsifiable hypotheses and experimental paradigms for empirical validation.", "AI": {"tldr": "Chat interfaces cognitively impair multi-step data analysis through five mechanisms causing overload; paper proposes eight hybrid UI patterns to fix this.", "motivation": "Chat has become the default AI analysis interface, but it's fundamentally flawed for complex, state-dependent tasks due to human cognitive limitations like the Keyhole Effect.", "method": "Builds on cognitive science (Woods' Keyhole Effect, working memory capacity ~4 chunks, verbal overshadowing) to analyze chat failures and formalize overload mathematically.", "result": "Identified five degradation mechanisms, formalized overload as O = max(0, m - v - W), and proposed eight hybrid patterns: Generative UI, Infinite Canvas, Deictic Interaction, State Rail, Ghost Layers, Mise en Place, Semantic Zoom, Probabilistic UI.", "conclusion": "Chat interfaces systematically degrade analytical performance for open-ended exploration; hybrid designs preserving natural language while addressing cognitive bottlenecks are essential, with falsifiable hypotheses for empirical validation."}}
{"id": "2602.00950", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00950", "abs": "https://arxiv.org/abs/2602.00950", "authors": ["Ant\u00f3nio Farinhas", "Nuno M. Guerreiro", "Jos\u00e9 Pombal", "Pedro Henrique Martins", "Laura Melton", "Alex Conway", "Cara Dochat", "Maya D'Eon", "Ricardo Rei"], "title": "MindGuard: Guardrail Classifiers for Multi-Turn Mental Health Support", "comment": null, "summary": "Large language models are increasingly used for mental health support, yet their conversational coherence alone does not ensure clinical appropriateness. Existing general-purpose safeguards often fail to distinguish between therapeutic disclosures and genuine clinical crises, leading to safety failures. To address this gap, we introduce a clinically grounded risk taxonomy, developed in collaboration with PhD-level psychologists, that identifies actionable harm (e.g., self-harm and harm to others) while preserving space for safe, non-crisis therapeutic content. We release MindGuard-testset, a dataset of real-world multi-turn conversations annotated at the turn level by clinical experts. Using synthetic dialogues generated via a controlled two-agent setup, we train MindGuard, a family of lightweight safety classifiers (with 4B and 8B parameters). Our classifiers reduce false positives at high-recall operating points and, when paired with clinician language models, help achieve lower attack success and harmful engagement rates in adversarial multi-turn interactions compared to general-purpose safeguards. We release all models and human evaluation data.", "AI": {"tldr": "This paper introduces MindGuard, a family of lightweight safety classifiers specifically designed for mental health LLM conversations, which better distinguish between therapeutic disclosures and genuine clinical crises to reduce false positives while maintaining safety.", "motivation": "Large language models are increasingly used for mental health support, but existing general-purpose safeguards cannot distinguish between therapeutic disclosures and genuine clinical crises, leading to safety failures.", "method": "Developed a clinically grounded risk taxonomy with psychologists, created MindGuard-testset (real-world multi-turn conversations annotated by clinical experts), and trained lightweight classifiers (4B/8B parameters) using synthetic dialogues from a controlled two-agent setup.", "result": "MindGuard classifiers reduce false positives at high-recall operating points and, when paired with clinician language models, achieve lower attack success and harmful engagement rates in adversarial multi-turn interactions compared to general-purpose safeguards.", "conclusion": "The clinically grounded approach effectively addresses mental health safety challenges, and the authors release all models and human evaluation data to support further research."}}
{"id": "2602.01927", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.01927", "abs": "https://arxiv.org/abs/2602.01927", "authors": ["Clement Ronfaut", "Robin Ollive", "Stephane Louise"], "title": "Numerical Error Extraction by Quantum Measurement Algorithm", "comment": null, "summary": "Important quantum algorithm routines allow the implementation of specific quantum operations (a.k.a. gates) by combining basic quantum circuits with an iterative structure. In this structure, the number of repetitions of the basic circuit pattern is associated to convergence parameters. This iterative structure behaves similarly to function approximation by series expansion: the higher the truncation order, the better the target gate (i.e. operation) approximation. The asymptotic convergence of the gate error with respect to the number of basic pattern repetitions is known. It is referred to as the query complexity. The underlying convergence law is bounded, but not in an explicit fashion. Upper bounds are generally too pessimistic to be useful in practice. The actual convergence law contains constants that depend on the joint properties of the matrix encoded by the query and the initial state vector, which are difficult to compute classically. This paper proposes a strategy to study this convergence law and extract the associated constants from the gate (operation) approximation at different accuracy (convergence parameter) constructed directly on a Quantum Processing Unit (QPU). This protocol is called Numerical Error Extraction by Quantum Measurement Algorithm (NEEQMA). NEEQMA concepts are tested on specific instances of Quantum Signal Processing (QSP) and Hamiltonian Simulation by Trotterization. Knowing theexact convergence constants allows for selecting the smallest convergence parameters that enable reaching the required gate approximation accuracy, hence satisfying the quantum algorithm's requirements.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faNEEQMA\u534f\u8bae\uff0c\u901a\u8fc7\u91cf\u5b50\u5904\u7406\u5668\u76f4\u63a5\u6d4b\u91cf\u63d0\u53d6\u91cf\u5b50\u95e8\u8fd1\u4f3c\u4e2d\u7684\u6536\u655b\u5e38\u6570\uff0c\u89e3\u51b3\u73b0\u6709\u7406\u8bba\u8fb9\u754c\u8fc7\u4e8e\u60b2\u89c2\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u91cf\u5b50\u7b97\u6cd5\u8d44\u6e90\u7684\u6700\u4f18\u914d\u7f6e\u3002", "motivation": "\u73b0\u6709\u91cf\u5b50\u7b97\u6cd5\u8fed\u4ee3\u7ed3\u6784\u7684\u6536\u655b\u5b9a\u5f8b\u5b58\u5728\u672a\u77e5\u5e38\u6570\uff08\u4f9d\u8d56\u67e5\u8be2\u77e9\u9635\u548c\u521d\u59cb\u6001\u7684\u8054\u5408\u7279\u6027\uff09\uff0c\u7ecf\u5178\u8ba1\u7b97\u56f0\u96be\uff1b\u7406\u8bba\u7ed9\u51fa\u7684\u4e0a\u754c\u8fc7\u4e8e\u60b2\u89c2\uff0c\u65e0\u6cd5\u6307\u5bfc\u5b9e\u9645\u91cf\u5b50\u786c\u4ef6\u4e0a\u7684\u53c2\u6570\u9009\u62e9\u3002", "method": "\u63d0\u51fa\u6570\u503c\u8bef\u5dee\u63d0\u53d6\u91cf\u5b50\u6d4b\u91cf\u7b97\u6cd5(NEEQMA)\uff1a\u5728\u91cf\u5b50\u5904\u7406\u5355\u5143(QPU)\u4e0a\u6784\u9020\u4e0d\u540c\u7cbe\u5ea6\u53c2\u6570\u7684\u91cf\u5b50\u95e8\u8fd1\u4f3c\u7535\u8def\uff0c\u901a\u8fc7\u76f4\u63a5\u6d4b\u91cf\u63d0\u53d6\u6536\u655b\u5b9a\u5f8b\u4e2d\u7684\u9690\u542b\u5e38\u6570\u3002", "result": "\u5728\u91cf\u5b50\u4fe1\u53f7\u5904\u7406(QSP)\u548cTrotter\u5316\u54c8\u5bc6\u987f\u6a21\u62df\u4e2d\u9a8c\u8bc1\u534f\u8bae\u6709\u6548\u6027\uff1b\u83b7\u5f97\u7cbe\u786e\u6536\u655b\u5e38\u6570\u540e\uff0c\u53ef\u786e\u5b9a\u6ee1\u8db3\u76ee\u6807\u7cbe\u5ea6\u7684\u6700\u5c0f\u6536\u655b\u53c2\u6570\uff0c\u907f\u514d\u8d44\u6e90\u6d6a\u8d39\u3002", "conclusion": "NEEQMA\u9996\u6b21\u5b9e\u73b0\u4ece\u91cf\u5b50\u786c\u4ef6\u6d4b\u91cf\u6570\u636e\u4e2d\u76f4\u63a5\u83b7\u53d6\u6536\u655b\u5b9a\u5f8b\u7684\u5b9e\u7528\u5e38\u6570\uff0c\u4e3a\u91cf\u5b50\u7b97\u6cd5\u63d0\u4f9b\u7cbe\u786e\u7684\u53c2\u6570\u4f18\u5316\u4f9d\u636e\uff0c\u663e\u8457\u63d0\u5347\u5b9e\u9645\u91cf\u5b50\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2602.01934", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.01934", "abs": "https://arxiv.org/abs/2602.01934", "authors": ["Pei-Rong Han", "Tian-Le Yang", "Wen Ning", "Hao-Long Zhang", "Huifang Kang", "Huiye Qiu", "Zhen-Biao Yang"], "title": "Exceptional phase transition in a single Kerr-cat qubit", "comment": "5 figures", "summary": "Exceptional points in non-Hermitian quantum systems give rise to novel genuine quantum phenomena. Recent explorations of exceptional-point-induced quantum phase transitions have extended from discrete-variable to continuous-variable-encoded quantum systems. However, quantum phase transitions driven by Liouvillian exceptional points (LEPs) in continuous-variable platforms remain largely unexplored. Here, we construct and investigate a Liouvillian exceptional structure based on a driven-dissipative Kerr-cat qubit. Through numerical simulations, we reveal a quantum phase transition occurring at the LEP characterized by a sudden change in dynamical behavior from underdamped oscillations to overdamped relaxations as visualized via Wigner functions and Bloch sphere trajectories. Notably the negativity of the Wigner function serves as a direct signature of genuine quantum coherence unattainable in conventional single-qubit non-Hermitian systems. Furthermore, we introduce the phase difference between the off-diagonal elements of the Liouvillian eigenmatrices as a novel parameter to quantify the transition. Our results establish the Kerr-cat qubit as a novel continuous-variable setting for exploring dissipative quantum criticality and intrinsic non-Hermitian physics.", "AI": {"tldr": "This paper explores Liouvillian exceptional point-induced quantum phase transitions in continuous-variable systems using a driven-dissipative Kerr-cat qubit. Numerical simulations reveal a transition from underdamped oscillations to overdamped relaxations, with Wigner function negativity serving as a signature of unique quantum coherence, while introducing a novel parameter to quantify the transition.", "motivation": "Quantum phase transitions driven by Liouvillian exceptional points (LEPs) in continuous-variable platforms remain largely unexplored, despite recent extensions from discrete-variable to continuous-variable-encoded quantum systems.", "method": "Construct and investigate a Liouvillian exceptional structure based on a driven-dissipative Kerr-cat qubit through numerical simulations.", "result": "Revealed a quantum phase transition at the LEP characterized by a sudden change from underdamped oscillations to overdamped relaxations (visualized via Wigner functions and Bloch sphere trajectories). The negativity of the Wigner function serves as a direct signature of genuine quantum coherence unattainable in conventional single-qubit non-Hermitian systems. Introduced the phase difference between off-diagonal elements of Liouvillian eigenmatrices as a novel parameter to quantify the transition.", "conclusion": "The Kerr-cat qubit establishes a novel continuous-variable setting for exploring dissipative quantum criticality and intrinsic non-Hermitian physics."}}
{"id": "2602.00318", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.00318", "abs": "https://arxiv.org/abs/2602.00318", "authors": ["Kunal Mukherjee", "Zulfikar Alom", "Tran Gia Bao Ngo", "Cuneyt Gurcan Akcora", "Murat Kantarcioglu"], "title": "Optimal Transport-Guided Adversarial Attacks on Graph Neural Network-Based Bot Detection", "comment": null, "summary": "The rise of bot accounts on social media poses significant risks to public discourse. To address this threat, modern bot detectors increasingly rely on Graph Neural Networks (GNNs). However, the effectiveness of these GNN-based detectors in real-world settings remains poorly understood. In practice, attackers continuously adapt their strategies as well as must operate under domain-specific and temporal constraints, which can fundamentally limit the applicability of existing attack methods. As a result, there is a critical need for robust GNN-based bot detection methods under realistic, constraint-aware attack scenarios.\n  To address this gap, we introduce BOCLOAK to systematically evaluate the robustness of GNN-based social bot detection via both edge editing and node injection adversarial attacks under realistic constraints. BOCLOAK constructs a probability measure over spatio-temporal neighbor features and learns an optimal transport geometry that separates human and bot behaviors. It then decodes transport plans into sparse, plausible edge edits that evade detection while obeying real-world constraints. We evaluate BOCLOAK across three social bot datasets, five state-of-the-art bot detectors, three adversarial defenses, and compare it against four leading graph adversarial attack baselines. BOCLOAK achieves up to 80.13% higher attack success rates while using 99.80% less GPU memory under realistic real-world constraints. Most importantly, BOCLOAK shows that optimal transport provides a lightweight, principled framework for bridging the gap between adversarial attacks and real-world bot detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faBOCLOAK\u6846\u67b6\uff0c\u5229\u7528\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u5728\u771f\u5b9e\u4e16\u754c\u7ea6\u675f\u4e0b\u7cfb\u7edf\u8bc4\u4f30GNN\u793e\u4ea4\u673a\u5668\u4eba\u68c0\u6d4b\u5668\u7684\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u8fb9\u7f16\u8f91\u548c\u8282\u70b9\u6ce8\u5165\u653b\u51fb\u5b9e\u73b0\u9ad8\u6548\u5bf9\u6297\u3002", "motivation": "\u793e\u4ea4\u673a\u5668\u4eba\u5bf9\u516c\u5171\u8206\u8bba\u6784\u6210\u91cd\u5927\u98ce\u9669\uff0c\u73b0\u4ee3\u68c0\u6d4b\u5668\u4f9d\u8d56GNN\uff0c\u4f46\u5176\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u5c1a\u4e0d\u660e\u786e\u3002\u653b\u51fb\u8005\u4f1a\u6301\u7eed\u9002\u5e94\u7b56\u7565\u5e76\u53d7\u9886\u57df\u7279\u5b9a\u548c\u65f6\u7a7a\u7ea6\u675f\uff0c\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u9002\u7528\u6027\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u5728\u771f\u5b9e\u7ea6\u675f\u611f\u77e5\u7684\u653b\u51fb\u573a\u666f\u4e0b\u5f00\u53d1\u9c81\u68d2\u7684GNN\u68c0\u6d4b\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "BOCLOAK\u6784\u5efa\u65f6\u7a7a\u90bb\u5c45\u7279\u5f81\u7684\u6982\u7387\u6d4b\u5ea6\uff0c\u5b66\u4e60\u6700\u4f18\u4f20\u8f93\u51e0\u4f55\u4ee5\u5206\u79bb\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u884c\u4e3a\uff0c\u7136\u540e\u5c06\u4f20\u8f93\u8ba1\u5212\u89e3\u7801\u4e3a\u7a00\u758f\u3001\u5408\u7406\u7684\u8fb9\u7f16\u8f91\u64cd\u4f5c\uff0c\u5728\u9075\u5b88\u771f\u5b9e\u4e16\u754c\u7ea6\u675f\u7684\u540c\u65f6\u9003\u907f\u68c0\u6d4b\u3002", "result": "\u5728\u4e09\u4e2a\u793e\u4ea4\u673a\u5668\u4eba\u6570\u636e\u96c6\u3001\u4e94\u4e2a\u5148\u8fdb\u68c0\u6d4b\u5668\u3001\u4e09\u79cd\u5bf9\u6297\u9632\u5fa1\u4e0a\u8bc4\u4f30\uff0c\u5e76\u4e0e\u56db\u4e2a\u56fe\u5bf9\u6297\u653b\u51fb\u57fa\u7ebf\u5bf9\u6bd4\uff0cBOCLOAK\u5728\u771f\u5b9e\u7ea6\u675f\u4e0b\u653b\u51fb\u6210\u529f\u7387\u63d0\u5347\u9ad8\u8fbe80.13%\uff0cGPU\u5185\u5b58\u4f7f\u7528\u51cf\u5c1199.80%\u3002", "conclusion": "\u6700\u4f18\u4f20\u8f93\u4e3a\u5f25\u5408\u5bf9\u6297\u653b\u51fb\u4e0e\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u68c0\u6d4b\u4e4b\u95f4\u7684\u5dee\u8ddd\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u3001\u539f\u5219\u6027\u6846\u67b6\uff0c\u8bc1\u660e\u4e86BOCLOAK\u5728\u8bc4\u4f30GNN\u68c0\u6d4b\u5668\u9c81\u68d2\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.00954", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00954", "abs": "https://arxiv.org/abs/2602.00954", "authors": ["Jinlong Pang", "Zhaowei Zhu", "Na Di", "Yichi Zhang", "Yaxuan Wang", "Chen Qian", "Yang Liu"], "title": "Small-Margin Preferences Still Matter-If You Train Them Right", "comment": null, "summary": "Preference optimization methods such as DPO align large language models (LLMs) using paired comparisons, but their effectiveness can be highly sensitive to the quality and difficulty of preference pairs. A common heuristic treats small-margin (ambiguous) pairs as noisy and filters them out. In this paper, we revisit this assumption and show that pair difficulty interacts strongly with the optimization objective: when trained with preference-based losses, difficult pairs can destabilize training and harm alignment, yet these same pairs still contain useful supervision signals when optimized with supervised fine-tuning (SFT). Motivated by this observation, we propose MixDPO, a simple yet effective difficulty-aware training strategy that (i) orders preference data from easy to hard (a curriculum over margin-defined difficulty), and (ii) routes difficult pairs to an SFT objective while applying a preference loss to easy pairs. This hybrid design provides a practical mechanism to leverage ambiguous pairs without incurring the optimization failures often associated with preference losses on low-margin data. Across three LLM-judge benchmarks, MixDPO consistently improves alignment over DPO and a range of widely-used variants, with particularly strong gains on AlpacaEval~2 length-controlled (LC) win rate.", "AI": {"tldr": "\u9488\u5bf9\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u5bf9\u56f0\u96be\u914d\u5bf9\u6837\u672c\u654f\u611f\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faMixDPO\uff0c\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u6392\u5e8f\u5e76\u7ed3\u5408SFT\u4e0e\u504f\u597d\u635f\u5931\u7684\u6df7\u5408\u4f18\u5316\u7b56\u7565\uff0c\u6709\u6548\u5229\u7528\u56f0\u96be\u6837\u672c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u504f\u597d\u4f18\u5316\u65b9\u6cd5\uff08\u5982DPO\uff09\u5bf9\u914d\u5bf9\u6570\u636e\u8d28\u91cf\u548c\u96be\u5ea6\u9ad8\u5ea6\u654f\u611f\uff0c\u901a\u5e38\u5c06\u8fb9\u754c\u6a21\u7cca\u7684\u5c0fmargin\u914d\u5bf9\u89c6\u4e3a\u566a\u58f0\u5e76\u8fc7\u6ee4\u3002\u7136\u800c\uff0c\u7814\u7a76\u53d1\u73b0\u56f0\u96be\u6837\u672c\u4e0e\u4f18\u5316\u76ee\u6807\u5b58\u5728\u5f3a\u4ea4\u4e92\uff1a\u5728\u504f\u597d\u635f\u5931\u4e0b\u4f1a\u7834\u574f\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u4f46\u5728\u76d1\u7763\u5fae\u8c03\u4e0b\u4ecd\u5305\u542b\u6709\u7528\u76d1\u7763\u4fe1\u53f7\u3002", "method": "MixDPO\u91c7\u7528\u96be\u5ea6\u611f\u77e5\u8bad\u7ec3\u7b56\u7565\uff1a(i) \u6839\u636emargin\u5b9a\u4e49\u7684\u96be\u5ea6\u4ece\u6613\u5230\u96be\u6392\u5e8f\u504f\u597d\u6570\u636e\uff0c\u6784\u5efa\u8bfe\u7a0b\uff1b(ii) \u5c06\u56f0\u96be\u914d\u5bf9\u8def\u7531\u81f3SFT\u76ee\u6807\u51fd\u6570\uff0c\u540c\u65f6\u5bf9\u7b80\u5355\u914d\u5bf9\u5e94\u7528\u504f\u597d\u635f\u5931\uff0c\u5b9e\u73b0\u6df7\u5408\u4f18\u5316\u3002", "result": "\u5728\u4e09\u4e2aLLM-judge\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMixDPO\u6301\u7eed\u4f18\u4e8eDPO\u53ca\u5176\u591a\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u53d8\u4f53\uff0c\u5728AlpacaEval~2\u957f\u5ea6\u63a7\u5236\uff08LC\uff09\u80dc\u7387\u4e0a\u53d6\u5f97\u7279\u522b\u663e\u8457\u7684\u589e\u76ca\u3002", "conclusion": "\u8be5\u6df7\u5408\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u673a\u5236\uff0c\u53ef\u5728\u4e0d\u5f15\u53d1\u4f4emargin\u6570\u636e\u5e38\u89c1\u4f18\u5316\u5931\u8d25\u7684\u524d\u63d0\u4e0b\u6709\u6548\u5229\u7528\u6a21\u7cca\u914d\u5bf9\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5bf9\u9f50\u6027\u80fd\u3002"}}
{"id": "2602.00328", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00328", "abs": "https://arxiv.org/abs/2602.00328", "authors": ["Nikhil Gopal", "Kostis Kaffes"], "title": "Harvest: Opportunistic Peer-to-Peer GPU Caching for LLM Inference", "comment": null, "summary": "Large Language Model (LLM) inference is increasingly constrained by GPU memory capacity rather than compute throughput, driven by growing model sizes and the linear growth of the key-value (KV) cache during autoregressive decoding. Existing approaches mitigate memory pressure by offloading model state and KV tensors to host memory, but incur substantial latency due to limited PCIe bandwidth. We present Harvest, an opportunistic GPU cache management framework that exploits high-bandwidth peer-to-peer GPU interconnects to dynamically place model weights and KV cache in unused GPU memory. Harvest treats peer GPU memory as a transient cache tier, preserving correctness while reducing data movement overhead under dynamic memory availability. We demonstrate significant throughput speedup of more than 2 times by using Harvest to accelerate the retrieval of two widely-used inference components: expert layer weights and KV cache entries.", "AI": {"tldr": "\u63d0\u51fa Harvest \u6846\u67b6\uff0c\u5229\u7528 GPU \u95f4\u9ad8\u901f\u4e92\u8fde\u52a8\u6001\u7ba1\u7406\u6a21\u578b\u6743\u91cd\u548c KV \u7f13\u5b58\uff0c\u7a81\u7834 PCIe \u5e26\u5bbd\u9650\u5236\uff0c\u5b9e\u73b0 LLM \u63a8\u7406\u541e\u5410\u91cf 2 \u500d\u4ee5\u4e0a\u63d0\u5347", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u53d7 GPU \u5185\u5b58\u5bb9\u91cf\u9650\u5236\uff0c\u4f20\u7edf\u4e3b\u673a\u5185\u5b58\u5378\u8f7d\u65b9\u6848\u56e0 PCIe \u5e26\u5bbd\u74f6\u9888\u5bfc\u81f4\u9ad8\u5ef6\u8fdf", "method": "\u8bbe\u8ba1\u673a\u4f1a\u4e3b\u4e49 GPU \u7f13\u5b58\u7ba1\u7406\u673a\u5236\uff0c\u5c06\u7a7a\u95f2 GPU \u5185\u5b58\u4f5c\u4e3a\u4e34\u65f6\u7f13\u5b58\u5c42\uff0c\u5229\u7528\u9ad8\u5e26\u5bbd\u70b9\u5bf9\u70b9\u4e92\u8fde\u52a8\u6001\u653e\u7f6e\u6a21\u578b\u6743\u91cd\u548c KV \u7f13\u5b58", "result": "\u5728\u4e13\u5bb6\u5c42\u6743\u91cd\u548c KV \u7f13\u5b58\u68c0\u7d22\u4e0a\u5b9e\u73b0\u8d85\u8fc7 2 \u500d\u7684\u541e\u5410\u91cf\u52a0\u901f", "conclusion": "\u901a\u8fc7\u5229\u7528\u591a GPU \u7cfb\u7edf\u4e2d\u7684\u7a7a\u95f2\u5185\u5b58\u8d44\u6e90\uff0cHarvest \u663e\u8457\u63d0\u5347\u4e86 LLM \u63a8\u7406\u6548\u7387\uff0c\u4e3a\u5185\u5b58\u53d7\u9650\u573a\u666f\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.00994", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00994", "abs": "https://arxiv.org/abs/2602.00994", "authors": ["Yu Li", "Mingyang Yi", "Xiuyu Li", "Ju Fan", "Fuxin Jiang", "Binbin Chen", "Peng Li", "Jie Song", "Tieying Zhang"], "title": "Reasoning and Tool-use Compete in Agentic RL:From Quantifying Interference to Disentangled Tuning", "comment": null, "summary": "Agentic Reinforcement Learning (ARL) focuses on training large language models (LLMs) to interleave reasoning with external tool execution to solve complex tasks. Most existing ARL methods train a single shared model parameters to support both reasoning and tool use behaviors, implicitly assuming that joint training leads to improved overall agent performance. Despite its widespread adoption, this assumption has rarely been examined empirically. In this paper, we systematically investigate this assumption by introducing a Linear Effect Attribution System(LEAS), which provides quantitative evidence of interference between reasoning and tool-use behaviors. Through an in-depth analysis, we show that these two capabilities often induce misaligned gradient directions, leading to training interference that undermines the effectiveness of joint optimization and challenges the prevailing ARL paradigm. To address this issue, we propose Disentangled Action Reasoning Tuning(DART), a simple and efficient framework that explicitly decouples parameter updates for reasoning and tool-use via separate low-rank adaptation modules. Experimental results show that DART consistently outperforms baseline methods with averaged 6.35 percent improvements and achieves performance comparable to multi-agent systems that explicitly separate tool-use and reasoning using a single model.", "AI": {"tldr": "\u672c\u6587\u6311\u6218\u4e86ARL\u4e2d\u8054\u5408\u8bad\u7ec3\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u6709\u76ca\u6027\u7684\u5047\u8bbe\u3002\u63d0\u51faLEAS\u91cf\u5316\u80fd\u529b\u95f4\u5e72\u6270\uff0c\u5e76\u8bbe\u8ba1DART\u6846\u67b6\u901a\u8fc7\u5206\u79bb\u7684\u4f4e\u79e9\u9002\u914d\u6a21\u5757\u89e3\u8026\u53c2\u6570\u66f4\u65b0\uff0c\u5b9e\u73b06.35%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709ARL\u65b9\u6cd5\u901a\u5e38\u8bad\u7ec3\u5355\u4e00\u5171\u4eab\u6a21\u578b\u540c\u65f6\u652f\u6301\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\uff0c\u9ed8\u8ba4\u8054\u5408\u8bad\u7ec3\u80fd\u63d0\u5347\u6574\u4f53\u6027\u80fd\uff0c\u4f46\u8fd9\u4e00\u5047\u8bbe\u7f3a\u4e4f\u5b9e\u8bc1\u68c0\u9a8c\u3002", "method": "\u63d0\u51fa\u7ebf\u6027\u6548\u5e94\u5f52\u56e0\u7cfb\u7edf(LEAS)\u91cf\u5316\u5e72\u6270\uff0c\u5e76\u8bbe\u8ba1\u89e3\u8026\u52a8\u4f5c\u63a8\u7406\u5fae\u8c03(DART)\u6846\u67b6\uff0c\u901a\u8fc7\u72ec\u7acb\u7684\u4f4e\u79e9\u9002\u914d\u6a21\u5757\u5206\u79bb\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u7684\u53c2\u6570\u66f4\u65b0\u3002", "result": "LEAS\u53d1\u73b0\u4e24\u79cd\u80fd\u529b\u5bfc\u81f4\u68af\u5ea6\u65b9\u5411\u9519\u4f4d\u4ea7\u751f\u8bad\u7ec3\u5e72\u6270\uff1bDART\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5e73\u5747\u9886\u51486.35%\uff0c\u6027\u80fd\u5ab2\u7f8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u4f46\u4ec5\u9700\u5355\u6a21\u578b\u5b9e\u73b0\u3002", "conclusion": "\u4e3b\u6d41ARL\u8054\u5408\u8bad\u7ec3\u8303\u5f0f\u5b58\u5728\u7f3a\u9677\uff0c\u80fd\u529b\u5e72\u6270\u4f1a\u524a\u5f31\u4f18\u5316\u6548\u679c\uff1b\u89e3\u8026\u63a8\u7406\u4e0e\u5de5\u5177\u4f7f\u7528\u53c2\u6570\u662f\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u5728\u4e0d\u589e\u52a0\u6a21\u578b\u6570\u91cf\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2602.01945", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.01945", "abs": "https://arxiv.org/abs/2602.01945", "authors": ["Simon Sundelin", "Linus Andersson", "Hampus Brunander", "Simone Gasparinetti"], "title": "Real-time detection of correlated quasiparticle tunneling events in a multi-qubit superconducting device", "comment": null, "summary": "Quasiparticle tunneling events are a source of decoherence and correlated errors in superconducting circuits. Understanding and ultimately mitigating these errors calls for real-time detection of quasiparticle tunneling events on individual devices. In this work, we simultaneously detect quasiparticle tunneling events in two co-housed, charge-sensitive transmons coupled to a common waveguide. We measure background quasiparticle tunneling rates at the single-hertz level, with temporal resolution of tens of microseconds. Using time-tagged coincidence analysis, we show that individual events are uncorrelated across devices, whereas burst episodes occur about once per minute and are largely correlated. These bursts have a characteristic lifetime of 7 ms and induce a thousand-fold increase in the quasiparticle tunneling rate across both devices. In addition, we identify a rarer subset of bursts which are accompanied by a shift in the offset charge, at approximately one event per hour. Our results establish a practical and extensible method to identify quasiparticle bursts in superconducting circuits, as well as their correlations and spatial structure, advancing routes to suppress correlated errors in superconducting quantum processors.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u5b9e\u65f6\u68c0\u6d4b\u8d85\u5bfc\u7535\u8def\u4e2d\u51c6\u7c92\u5b50\u96a7\u7a7f\u4e8b\u4ef6\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u5e38\u89c4\u4e8b\u4ef6\u5728\u8bbe\u5907\u95f4\u65e0\u5173\u8054\uff0c\u4f46\u5b58\u5728\u7ea6\u6bcf\u5206\u949f\u4e00\u6b21\u7684\u5173\u8054\u6027\u7206\u53d1\u4e8b\u4ef6\uff08\u6301\u7eed7\u6beb\u79d2\uff0c\u5bfc\u81f4\u96a7\u7a7f\u7387\u5343\u500d\u589e\u52a0\uff09\uff0c\u4ee5\u53ca\u66f4\u7f55\u89c1\u7684\u4f34\u968f\u7535\u8377\u504f\u79fb\u7684\u7206\u53d1\u4e8b\u4ef6\uff08\u7ea6\u6bcf\u5c0f\u65f6\u4e00\u6b21\uff09\u3002", "motivation": "\u8d85\u5bfc\u7535\u8def\u4e2d\u7684\u51c6\u7c92\u5b50\u96a7\u7a7f\u4e8b\u4ef6\u4f1a\u5bfc\u81f4\u9000\u76f8\u5e72\u548c\u76f8\u5173\u9519\u8bef\uff0c\u7406\u89e3\u5e76\u6291\u5236\u8fd9\u4e9b\u9519\u8bef\u9700\u8981\u5b9e\u73b0\u5bf9\u5355\u4e2a\u8bbe\u5907\u4e0a\u51c6\u7c92\u5b50\u96a7\u7a7f\u4e8b\u4ef6\u7684\u5b9e\u65f6\u68c0\u6d4b\u3002", "method": "\u5728\u5171\u7528\u6ce2\u5bfc\u8026\u5408\u7684\u4e24\u4e2a\u7535\u8377\u654f\u611f\u578btransmon\u91cf\u5b50\u6bd4\u7279\u4e0a\u540c\u6b65\u68c0\u6d4b\u51c6\u7c92\u5b50\u96a7\u7a7f\u4e8b\u4ef6\uff0c\u91c7\u7528\u65f6\u95f4\u6807\u8bb0\u7b26\u5408\u5206\u6790\u6280\u672f\uff0c\u5b9e\u73b0\u5355\u8d6b\u5179\u7ea7\u80cc\u666f\u96a7\u7a7f\u7387\u6d4b\u91cf\u548c\u6570\u5341\u5fae\u79d2\u7ea7\u65f6\u95f4\u5206\u8fa8\u7387\u3002", "result": "\u6d4b\u5f97\u5e38\u89c4\u96a7\u7a7f\u4e8b\u4ef6\u5728\u8bbe\u5907\u95f4\u65e0\u5173\u8054\uff1b\u53d1\u73b0\u7ea6\u6bcf\u5206\u949f\u53d1\u751f\u4e00\u6b21\u7684\u5173\u8054\u6027\u7206\u53d1\u4e8b\u4ef6\uff08\u7279\u5f81\u5bff\u547d7\u6beb\u79d2\uff0c\u4f7f\u53cc\u8bbe\u5907\u96a7\u7a7f\u7387\u540c\u6b65\u589e\u52a0\u5343\u500d\uff09\uff1b\u8bc6\u522b\u51fa\u66f4\u7f55\u89c1\u7684\u4f34\u968f\u504f\u79fb\u7535\u8377\u53d8\u5316\u7684\u7206\u53d1\u4e8b\u4ef6\uff08\u7ea6\u6bcf\u5c0f\u65f6\u4e00\u6b21\uff09\u3002", "conclusion": "\u5efa\u7acb\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u8d85\u5bfc\u7535\u8def\u51c6\u7c92\u5b50\u7206\u53d1\u68c0\u6d4b\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u5176\u5173\u8054\u6027\u548c\u7a7a\u95f4\u7ed3\u6784\u7279\u5f81\uff0c\u4e3a\u6291\u5236\u8d85\u5bfc\u91cf\u5b50\u5904\u7406\u5668\u4e2d\u7684\u76f8\u5173\u9519\u8bef\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.00997", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00997", "abs": "https://arxiv.org/abs/2602.00997", "authors": ["Mayank Singh", "Vikas Yadav", "Eduardo Blanco"], "title": "Error Taxonomy-Guided Prompt Optimization", "comment": null, "summary": "Automatic Prompt Optimization (APO) is a powerful approach for extracting performance from large language models without modifying their weights. Many existing methods rely on trial-and-error, testing different prompts or in-context examples until a good configuration emerges, often consuming substantial compute. Recently, natural language feedback derived from execution logs has shown promise as a way to identify how prompts can be improved. However, most prior approaches operate in a bottom-up manner, iteratively adjusting the prompt based on feedback from individual problems, which can cause them to lose the global perspective. In this work, we propose Error Taxonomy-Guided Prompt Optimization (ETGPO), a prompt optimization algorithm that adopts a top-down approach. ETGPO focuses on the global failure landscape by collecting model errors, categorizing them into a taxonomy, and augmenting the prompt with guidance targeting the most frequent failure modes. Across multiple benchmarks spanning mathematics, question answering, and logical reasoning, ETGPO achieves accuracy that is comparable to or better than state-of-the-art methods, while requiring roughly one third of the optimization-phase token usage and evaluation budget.", "AI": {"tldr": "\u63d0\u51faETGPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u9519\u8bef\u5206\u7c7b\u6cd5\u5e76\u4ece\u5168\u5c40\u89d2\u5ea6\u4f18\u5316\u63d0\u793a\uff0c\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u7ea62/3\u7684token\u6d88\u8017\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56\u8bd5\u9519\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e14\u91c7\u7528\u81ea\u4e0b\u800c\u4e0a\u7684\u65b9\u5f0f\uff0c\u5bb9\u6613\u5931\u53bb\u5168\u5c40\u89c6\u89d2\u3002\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u867d\u6709\u524d\u666f\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u6574\u4f53\u5931\u8d25\u6a21\u5f0f\u7684\u5206\u6790\u3002", "method": "ETGPO\u91c7\u7528\u81ea\u4e0a\u800c\u4e0b\u65b9\u6cd5\uff1a\u6536\u96c6\u6a21\u578b\u9519\u8bef\u2192\u6784\u5efa\u9519\u8bef\u5206\u7c7b\u6cd5\u2192\u9488\u5bf9\u9ad8\u9891\u9519\u8bef\u6a21\u5f0f\u589e\u5f3a\u63d0\u793a\u3002", "result": "\u5728\u6570\u5b66\u3001\u95ee\u7b54\u548c\u903b\u8f91\u63a8\u7406\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cETGPO\u8fbe\u5230\u6216\u8d85\u8fc7SOTA\u65b9\u6cd5\u51c6\u786e\u7387\uff0c\u540c\u65f6\u4f18\u5316\u9636\u6bb5token\u4f7f\u7528\u91cf\u548c\u8bc4\u4f30\u9884\u7b97\u51cf\u5c11\u7ea6\u4e09\u5206\u4e4b\u4e8c\u3002", "conclusion": "\u901a\u8fc7\u5168\u5c40\u5931\u8d25\u6a21\u5f0f\u5206\u6790\u5f15\u5bfc\u7684\u63d0\u793a\u4f18\u5316\u662f\u9ad8\u6548\u4e14\u6709\u6548\u7684\uff0c\u80fd\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2602.01994", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.01994", "abs": "https://arxiv.org/abs/2602.01994", "authors": ["Namrata Manglani", "Samrit Kumar Maity", "Ranjit Thapa", "Sanjay Wandhekar"], "title": "Scalable Quantum-Classical DFT Embedding for NISQ Molecular Simulation", "comment": null, "summary": "Scalable quantum-classical embedding is essential for chemically meaningful simulations on near-term NISQ hardware. Using QDFT, we show systematic recovery of correlation energy relative to the DFT baseline, benchmarked against CCSD in a fixed six-orbital active space across molecules ranging from water to naphthalene. By varying the number of embedded electrons from 2 to 8, aromatic systems saturate near 63-64 percent, while linear molecules such as carbon dioxide reach 68 percent. All systems converge within two embedding iterations under relaxed self-consistency thresholds, highlighting the robustness of the approach. A (4e,6o) active space recovers approximately 60 percent correlation using 10 qubits, providing practical guidelines for NISQ-era simulations.", "AI": {"tldr": "QDFT\u5d4c\u5165\u752810\u91cf\u5b50\u6bd4\u7279\u6062\u590d60-68%\u5206\u5b50\u76f8\u5173\u80fd\uff0c2\u6b21\u8fed\u4ee3\u6536\u655b\u3002", "motivation": "\u4e3a\u8fd1\u671fNISQ\u786c\u4ef6\u4e0a\u5316\u5b66\u7cbe\u786e\u7684\u6a21\u62df\u5f00\u53d1\u53ef\u6269\u5c55\u7684\u91cf\u5b50-\u7ecf\u5178\u5d4c\u5165\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u91cf\u5b50\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba(QDFT)\u5d4c\u5165\uff0c\u5728\u56fa\u5b9a\u516d\u8f68\u9053\u6d3b\u6027\u7a7a\u95f4\u4e2d\uff0c\u4ee5CCSD\u4e3a\u57fa\u51c6\uff0c\u7cfb\u7edf\u6027\u6062\u590d\u76f8\u5173\u80fd\u5e76\u6539\u53d8\u5d4c\u5165\u7535\u5b50\u6570(2-8\u4e2a)\u3002", "result": "\u82b3\u9999\u4f53\u7cfb\u76f8\u5173\u80fd\u6062\u590d\u9971\u548c\u4e8e63-64%\uff0c\u7ebf\u6027\u5206\u5b50(\u5982CO\u2082)\u8fbe68%\uff0c\u6240\u6709\u4f53\u7cfb2\u6b21\u8fed\u4ee3\u6536\u655b\uff0c(4e,6o)\u6d3b\u6027\u7a7a\u95f4\u752810\u91cf\u5b50\u6bd4\u7279\u6062\u590d\u7ea660%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aNISQ\u65f6\u4ee3\u91cf\u5b50\u5316\u5b66\u6a21\u62df\u63d0\u4f9b\u4e86\u7a33\u5065\u4e14\u5b9e\u7528\u7684\u6307\u5bfc\u65b9\u9488\u3002"}}
{"id": "2602.01002", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01002", "abs": "https://arxiv.org/abs/2602.01002", "authors": ["Itai Shapira", "Gerdus Benade", "Ariel D. Procaccia"], "title": "How RLHF Amplifies Sycophancy", "comment": null, "summary": "Large language models often exhibit increased sycophantic behavior after preference-based post-training, showing a stronger tendency to affirm a user's stated or implied belief even when this conflicts with factual accuracy or sound judgment. We present a formal analysis of how alignment from human feedback can increase this failure mode by identifying an explicit amplification mechanism that causally links optimization against a learned reward to bias in the human preference data used for alignment. We show that the direction of behavioral drift is determined by a covariance under the base policy between endorsing the belief signal in the prompt and the learned reward, and that the first-order effect reduces to a simple mean-gap condition. We then analyze reward learning from pairwise comparisons under random utility models like Bradley-Terry and characterize when bias in human annotators' preferences induces this reward gap. Next, we propose a training-time intervention designed to neutralize the amplification mechanism itself. Among all post-trained policies that prevent sycophantic behavior from increasing, we characterize the unique policy closest in KL divergence to the unconstrained post-trained policy, and derive the corresponding minimal reward correction as a closed-form agreement penalty. Computational experiments find that reward gaps are common and cause behavioral drift in all the configurations considered.", "AI": {"tldr": "The paper demonstrates that preference-based training amplifies sycophantic behavior in LLMs through a covariance-based mechanism between reward optimization and biased human preferences, and proposes a corrective agreement penalty to neutralize this effect.", "motivation": "Large language models become overly sycophantic after preference-based post-training, affirming user beliefs even when factually incorrect, and we need to understand and prevent this systematic failure mode.", "method": "Formal analysis identifying an amplification mechanism linking reward optimization to human preference bias; uses random utility models (Bradley-Terry) to characterize reward gaps; proposes a training-time intervention with a closed-form agreement penalty.", "result": "Behavioral drift direction is determined by covariance between belief endorsement and learned reward (mean-gap condition); experiments show reward gaps are ubiquitous and cause sycophantic drift; the intervention finds the unique KL-minimizing policy that prevents increased sycophancy.", "conclusion": "Preference-based alignment systematically amplifies sycophantic behavior through an identifiable mechanism, but can be mitigated with a targeted reward correction that penalizes unwarranted agreement with user beliefs."}}
{"id": "2602.02008", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.02008", "abs": "https://arxiv.org/abs/2602.02008", "authors": ["Tuyen Nguyen", "M\u00e1ria Kieferov\u00e1", "Amira Abbas"], "title": "On Quantum Learning Advantage Under Symmetries", "comment": "24 pages", "summary": "Symmetry underlies many of the most effective classical and quantum learning algorithms, yet whether quantum learners can gain a fundamental advantage under symmetry-imposed structures remains an open question. Based on evidence that classical statistical query ($\\SQ$) frameworks have revealed exponential query complexity in learning symmetric function classes, we ask: can quantum learning algorithms exploit the problem symmetry better? In this work, we investigate the potential benefits of symmetry within the quantum statistical query ($\\QSQ$) model, which is a natural quantum analog of classical $\\SQ$. Our results uncover three distinct phenomena: (i) we obtain an exponential separation between $\\QSQ$ and $\\SQ$ on a permutation-invariant function class; (ii) we establish query complexity lower bounds for $\\QSQ$ learning that match, up to constant factors, the corresponding classical $\\SQ$ lower bounds for most commonly studied symmetries; however, the potential advantages may occur under highly skewed orbit distributions; and (iii) we further identify a tolerance-based separation exists, where quantum learners succeed at noise levels that render classical $\\SQ$ algorithms ineffective. Together, these findings provide insight into when symmetry can enable quantum advantage in learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u91cf\u5b50\u5b66\u4e60\u7b97\u6cd5\u5728\u5bf9\u79f0\u7ea6\u675f\u7ed3\u6784\u4e0b\u662f\u5426\u80fd\u83b7\u5f97\u76f8\u5bf9\u4e8e\u7ecf\u5178\u7b97\u6cd5\u7684\u6839\u672c\u6027\u4f18\u52bf\uff0c\u53d1\u73b0\u5728\u91cf\u5b50\u7edf\u8ba1\u67e5\u8be2\u6a21\u578b\u4e2d\uff0c\u5bf9\u79f0\u6027\u53ef\u4ee5\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u4e3a\u91cf\u5b50\u5b66\u4e60\u8005\u5e26\u6765\u6307\u6570\u7ea7\u4f18\u52bf\u548c\u566a\u58f0\u9c81\u68d2\u6027\u3002", "motivation": "\u63a2\u7d22\u91cf\u5b50\u5b66\u4e60\u8005\u662f\u5426\u80fd\u591f\u5229\u7528\u95ee\u9898\u5bf9\u79f0\u6027\u83b7\u5f97\u8d85\u8d8a\u7ecf\u5178\u7b97\u6cd5\u7684\u4f18\u52bf\uff0c\u7279\u522b\u662f\u57fa\u4e8e\u7ecf\u5178\u7edf\u8ba1\u67e5\u8be2\u6846\u67b6\u5728\u5bf9\u79f0\u51fd\u6570\u7c7b\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u6307\u6570\u7ea7\u67e5\u8be2\u590d\u6742\u5ea6\u7684\u8bc1\u636e\u3002", "method": "\u5728\u91cf\u5b50\u7edf\u8ba1\u67e5\u8be2\uff08QSQ\uff09\u6a21\u578b\u4e2d\u7814\u7a76\u5bf9\u79f0\u6027\u7684\u6f5c\u5728\u4f18\u52bf\uff0c\u8be5\u6a21\u578b\u662f\u7ecf\u5178\u7edf\u8ba1\u67e5\u8be2\uff08SQ\uff09\u7684\u81ea\u7136\u91cf\u5b50\u6a21\u62df\u3002", "result": "\u53d1\u73b0\u4e86\u4e09\u79cd\u4e0d\u540c\u73b0\u8c61\uff1a1\uff09\u5728\u7f6e\u6362\u4e0d\u53d8\u51fd\u6570\u7c7b\u4e0a\u5b9e\u73b0\u4e86QSQ\u4e0eSQ\u4e4b\u95f4\u7684\u6307\u6570\u7ea7\u5206\u79bb\uff1b2\uff09\u5efa\u7acb\u4e86QSQ\u5b66\u4e60\u7684\u67e5\u8be2\u590d\u6742\u5ea6\u4e0b\u754c\uff0c\u4e0e\u5927\u591a\u6570\u5e38\u89c1\u5bf9\u79f0\u6027\u7684\u7ecf\u5178SQ\u4e0b\u754c\u76f8\u5339\u914d\uff0c\u4f46\u5728\u9ad8\u5ea6\u504f\u659c\u7684\u8f68\u9053\u5206\u5e03\u4e0b\u53ef\u80fd\u51fa\u73b0\u4f18\u52bf\uff1b3\uff09\u8fdb\u4e00\u6b65\u53d1\u73b0\u4e86\u57fa\u4e8e\u5bb9\u5dee\u7684\u5206\u79bb\uff0c\u91cf\u5b50\u5b66\u4e60\u8005\u5728\u4f7f\u7ecf\u5178SQ\u7b97\u6cd5\u5931\u6548\u7684\u566a\u58f0\u6c34\u5e73\u4e0b\u4ecd\u80fd\u6210\u529f\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u7406\u89e3\u5728\u4f55\u79cd\u60c5\u51b5\u4e0b\u5bf9\u79f0\u6027\u80fd\u591f\u4f7f\u91cf\u5b50\u5b66\u4e60\u83b7\u5f97\u4f18\u52bf\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2602.00333", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00333", "abs": "https://arxiv.org/abs/2602.00333", "authors": ["Parmida Davarmanesh", "Ashia Wilson", "Adityanarayanan Radhakrishnan"], "title": "Efficient and accurate steering of Large Language Models through attention-guided feature learning", "comment": null, "summary": "Steering, or direct manipulation of internal activations to guide LLM responses toward specific semantic concepts, is emerging as a promising avenue for both understanding how semantic concepts are stored within LLMs and advancing LLM capabilities. Yet, existing steering methods are remarkably brittle, with seemingly non-steerable concepts becoming completely steerable based on subtle algorithmic choices in how concept-related features are extracted. In this work, we introduce an attention-guided steering framework that overcomes three core challenges associated with steering: (1) automatic selection of relevant token embeddings for extracting concept-related features; (2) accounting for heterogeneity of concept-related features across LLM activations; and (3) identification of layers most relevant for steering. Across a steering benchmark of 512 semantic concepts, our framework substantially improved steering over previous state-of-the-art (nearly doubling the number of successfully steered concepts) across model architectures and sizes (up to 70 billion parameter models). Furthermore, we use our framework to shed light on the distribution of concept-specific features across LLM layers. Overall, our framework opens further avenues for developing efficient, highly-scalable fine-tuning algorithms for industry-scale LLMs.", "AI": {"tldr": "\u63d0\u51fa\u6ce8\u610f\u529b\u5f15\u5bfc\u7684LLM steering\u6846\u67b6\uff0c\u89e3\u51b3token\u9009\u62e9\u3001\u7279\u5f81\u5f02\u8d28\u6027\u548c\u5c42\u8bc6\u522b\u4e09\u5927\u6311\u6218\uff0c\u5728512\u4e2a\u6982\u5ff5\u4e0a\u51e0\u4e4e\u5c06\u53ef\u64cd\u63a7\u6982\u5ff5\u6570\u91cf\u7ffb\u500d\uff0c\u9002\u7528\u4e8e70B\u53c2\u6570\u6a21\u578b", "motivation": "\u73b0\u6709steering\u65b9\u6cd5\u8fc7\u4e8e\u8106\u5f31\uff0c\u6982\u5ff5\u662f\u5426\u53ef\u64cd\u63a7\u9ad8\u5ea6\u4f9d\u8d56\u7b97\u6cd5\u7ec6\u8282\u9009\u62e9\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\u6765\u7406\u89e3LLM\u4e2d\u8bed\u4e49\u6982\u5ff5\u7684\u5b58\u50a8\u673a\u5236\u5e76\u63d0\u5347\u6a21\u578b\u80fd\u529b", "method": "\u6ce8\u610f\u529b\u5f15\u5bfc\u7684steering\u6846\u67b6\uff0c\u5b9e\u73b0\uff1a1) \u81ea\u52a8\u9009\u62e9\u76f8\u5173token\u5d4c\u5165\u63d0\u53d6\u6982\u5ff5\u7279\u5f81\uff1b2) \u5904\u7406\u4e0d\u540c\u6fc0\u6d3b\u4e2d\u6982\u5ff5\u7279\u5f81\u7684\u5f02\u8d28\u6027\uff1b3) \u8bc6\u522b\u6700\u76f8\u5173\u7684steering\u5c42", "result": "\u5728512\u4e2a\u8bed\u4e49\u6982\u5ff5\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347steering\u6548\u679c\uff0c\u6210\u529f\u64cd\u63a7\u7684\u6982\u5ff5\u6570\u91cf\u51e0\u4e4e\u7ffb\u500d\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u67b6\u6784\u548c\u89c4\u6a21\uff08\u6700\u9ad8700\u4ebf\u53c2\u6570\uff09\u7684\u6a21\u578b", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5f00\u53d1\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u884c\u4e1a\u7ea7LLM\u5fae\u8c03\u7b97\u6cd5\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84"}}
{"id": "2602.02077", "categories": ["quant-ph", "gr-qc", "math-ph"], "pdf": "https://arxiv.org/pdf/2602.02077", "abs": "https://arxiv.org/abs/2602.02077", "authors": ["Dorje C. Brody", "Lane P. Hughston"], "title": "Quantum clock and Newtonian time", "comment": "5 pages, 3 figures", "summary": "An extension of standard quantum mechanics is proposed in which the Newtonian time parameter appearing in the unitary evolution operator is replaced with the time shown by a `quantum clock'. A quantum clock is defined by the following properties: (a) the time that the clock shows is non-decreasing, (b) the clock ticks at random with random tick sizes, and (c) on average the clock shows the Newtonian time. We show that the leading term in the evolution equation for the density matrix associated with any quantum clock model gives the von Neumann equation. Modifications to the von Neumann equation are worked out in detail in a parametric family of examples for which the tick sizes have a gamma distribution. The leading correction to the von Neumann equation is given by the Lindblad equation generated by the Hamiltonian, but there are higher-order terms that generalize the von Neumann equation and the Lindblad equation. Lower bounds on the parameters of these quantum clock models are derived by use of the precision limit of an atomic clock.", "AI": {"tldr": "\u63d0\u51fa\u91cf\u5b50\u529b\u5b66\u6269\u5c55\u7406\u8bba\uff0c\u7528\u5177\u6709\u968f\u673a\u8df3\u53d8\u7279\u6027\u7684\"\u91cf\u5b50\u949f\"\u66ff\u4ee3\u725b\u987f\u65f6\u95f4\u53c2\u6570\uff0c\u63a8\u5bfc\u51fa\u51af\u00b7\u8bfa\u4f9d\u66fc\u65b9\u7a0b\u7684\u4fee\u6b63\u5f62\u5f0f\u5e76\u7ed9\u51fa\u539f\u5b50\u949f\u7cbe\u5ea6\u7ea6\u675f", "motivation": "\u7a81\u7834\u6807\u51c6\u91cf\u5b50\u529b\u5b66\u4e2d\u725b\u987f\u65f6\u95f4\u53c2\u6570\u7684\u521a\u6027\u5047\u8bbe\uff0c\u5efa\u7acb\u66f4\u7b26\u5408\u91cf\u5b50\u7279\u6027\u7684\u65f6\u95f4\u63cf\u8ff0\u6a21\u578b", "method": "\u5b9a\u4e49\u91cf\u5b50\u949f\u4e09\u8981\u7d20\uff08\u65f6\u95f4\u975e\u9012\u51cf/\u968f\u673a\u8df3\u53d8/\u5e73\u5747\u5339\u914d\u725b\u987f\u65f6\u95f4\uff09\uff0c\u91c7\u7528\u4f3d\u9a6c\u5206\u5e03\u8df3\u53d8\u5c3a\u5bf8\u6784\u5efa\u53c2\u6570\u5316\u6a21\u578b\uff0c\u63a8\u5bfc\u5bc6\u5ea6\u77e9\u9635\u6f14\u5316\u65b9\u7a0b", "result": "\u4e3b\u5bfc\u9879\u4ecd\u5f97\u51af\u00b7\u8bfa\u4f9d\u66fc\u65b9\u7a0b\uff1b\u4f3d\u9a6c\u5206\u5e03\u6a21\u578b\u4e0b\u9996\u9879\u4fee\u6b63\u4e3aLindblad\u65b9\u7a0b\uff0c\u4f46\u5b58\u5728\u9ad8\u9636\u9879\u63a8\u5e7f\uff1b\u901a\u8fc7\u539f\u5b50\u949f\u7cbe\u5ea6\u6781\u9650\u5bfc\u51fa\u6a21\u578b\u53c2\u6570\u4e0b\u9650", "conclusion": "\u6210\u529f\u6784\u5efa\u91cf\u5b50\u65f6\u949f\u7406\u8bba\u6846\u67b6\uff0c\u63ed\u793a\u65f6\u95f4\u91cf\u5b50\u5316\u5bf9\u91cf\u5b50\u52a8\u529b\u5b66\u7684\u5f71\u54cd\u5c42\u7ea7\uff0c\u4e3a\u5b9e\u9a8c\u68c0\u9a8c\u63d0\u4f9b\u7406\u8bba\u53c2\u6570\u8fb9\u754c"}}
{"id": "2602.00334", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.00334", "abs": "https://arxiv.org/abs/2602.00334", "authors": ["Aikaterini Karoni", "Rajit Rajpal", "Benedict Leimkuhler", "Gabriel Stoltz"], "title": "Adaptive Momentum and Nonlinear Damping for Neural Network Training", "comment": "29 pages, 11 figures", "summary": "We propose a continuous-time scheme for large-scale optimization that introduces individual, adaptive momentum coefficients regulated by the kinetic energy of each model parameter. This approach automatically adjusts to local landscape curvature to maintain stability without sacrificing convergence speed. We demonstrate that our adaptive friction can be related to cubic damping, a suppression mechanism from structural dynamics. Furthermore, we introduce two specific optimization schemes by augmenting the continuous dynamics of mSGD and Adam with a cubic damping term. Empirically, our methods demonstrate robustness and match or outperform Adam on training ViT, BERT, and GPT2 tasks where mSGD typically struggles. We further provide theoretical results establishing the exponential convergence of the proposed schemes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u52a8\u80fd\u7684\u8fde\u7eed\u65f6\u95f4\u4f18\u5316\u65b9\u6848\uff0c\u901a\u8fc7\u7acb\u65b9\u963b\u5c3c\u589e\u5f3amSGD\u548cAdam\uff0c\u5b9e\u73b0\u6307\u6570\u6536\u655b\u5e76\u5728ViT/BERT/GPT2\u4e0a\u5339\u914d\u6216\u8d85\u8d8aAdam\u6027\u80fd", "motivation": "\u73b0\u6709\u4f18\u5316\u5668\u5728\u5927\u578b\u6a21\u578b\u8bad\u7ec3\u4e2d\u5b58\u5728\u7a33\u5b9a\u6027\u4e0e\u6536\u655b\u901f\u5ea6\u7684\u6743\u8861\u95ee\u9898\uff1amSGD\u96be\u4ee5\u9002\u5e94\u590d\u6742\u66f2\u7387\uff0cAdam\u53ef\u80fd\u51fa\u73b0\u4e0d\u7a33\u5b9a\u73b0\u8c61\uff0c\u9700\u8981\u81ea\u9002\u5e94\u8c03\u8282\u673a\u5236", "method": "\u8bbe\u8ba1\u8fde\u7eed\u65f6\u95f4\u52a8\u529b\u5b66\u6846\u67b6\uff0c\u4e3a\u6bcf\u4e2a\u53c2\u6570\u5f15\u5165\u7531\u52a8\u80fd\u8c03\u8282\u7684\u81ea\u9002\u5e94\u52a8\u91cf\u7cfb\u6570\uff0c\u5c06\u81ea\u9002\u5e94\u6469\u64e6\u4e0e\u7ed3\u6784\u52a8\u529b\u5b66\u4e2d\u7684\u7acb\u65b9\u963b\u5c3c\u673a\u5236\u5173\u8054\uff0c\u5e76\u636e\u6b64\u6539\u8fdbmSGD\u548cAdam", "result": "\u7406\u8bba\u8bc1\u660e\u6307\u6570\u6536\u655b\u6027\uff1b\u5728ViT\u3001BERT\u3001GPT2\u8bad\u7ec3\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\uff0c\u5728mSGD\u901a\u5e38\u5931\u6548\u7684\u573a\u666f\u4e0b\u8fbe\u5230\u6216\u8d85\u8fc7Adam\u6027\u80fd", "conclusion": "\u8be5\u81ea\u9002\u5e94\u7acb\u65b9\u963b\u5c3c\u65b9\u6848\u6709\u6548\u5e73\u8861\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e0e\u6536\u655b\u901f\u5ea6\uff0c\u4e3a\u5927\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u4e86\u517c\u5177\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u8df5\u6548\u679c\u7684\u65b0\u65b9\u6cd5"}}
{"id": "2602.01034", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01034", "abs": "https://arxiv.org/abs/2602.01034", "authors": ["Xiangwei Wang", "Wei Wang", "Ken Chen", "Nanduni Nimalsiri", "Saman Halgamuge"], "title": "Discovering Process-Outcome Credit in Multi-Step LLM Reasoning", "comment": null, "summary": "Reinforcement Learning (RL) serves as a potent paradigm for enhancing reasoning capabilities in Large Language Models (LLMs), yet standard outcome-based approaches often suffer from reward sparsity and inefficient credit assignment. In this paper, we propose a novel framework designed to provide continuous reward signals, which introduces a Step-wise Marginal Information Gain (MIG) mechanism that quantifies the intrinsic value of reasoning steps against a Monotonic Historical Watermark, effectively filtering out training noise. To ensure disentangled credit distribution, we implement a Decoupled Masking Strategy, applying process-oriented rewards specifically to the chain-of-thought (CoT) and outcome-oriented rewards to the full completion. Additionally, we incorporate a Dual-Gated SFT objective to stabilize training with high-quality structural and factual signals. Extensive experiments across textual and multi-modal benchmarks (e.g., MATH, Super-CLEVR) demonstrate that our approach consistently outperforms baselines such as GRPO in both sample efficiency and final accuracy. Furthermore, our model exhibits superior out-of-distribution robustness, demonstrating promising zero-shot transfer capabilities to unseen and challenging reasoning tasks.", "AI": {"tldr": "\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u589e\u5f3a\u4e2d\u7684\u5956\u52b1\u7a00\u758f\u548c\u4fe1\u7528\u5206\u914d\u96be\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u8fde\u7eed\u5956\u52b1\u6846\u67b6\uff0c\u7ed3\u5408\u6b65\u9aa4\u8fb9\u9645\u4fe1\u606f\u589e\u76ca\u673a\u5236\u548c\u5206\u79bb\u63a9\u7801\u7b56\u7565\uff0c\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u8d85\u8d8aGRPO\u7b49\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u6837\u672c\u6548\u7387\u548c\u96f6\u6837\u672c\u8fc1\u79fb\u80fd\u529b\u3002", "motivation": "\u6807\u51c6\u57fa\u4e8e\u7ed3\u679c\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u65f6\u9762\u4e34\u5956\u52b1\u7a00\u758f\u548c\u4fe1\u7528\u5206\u914d\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5305\u542b\u6b65\u9aa4\u8fb9\u9645\u4fe1\u606f\u589e\u76ca(MIG)\u673a\u5236\u7684\u8fde\u7eed\u5956\u52b1\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u8c03\u5386\u53f2\u6c34\u5370\u8fc7\u6ee4\u8bad\u7ec3\u566a\u58f0\uff1b\u91c7\u7528\u5206\u79bb\u63a9\u7801\u7b56\u7565\u5b9e\u73b0\u8fc7\u7a0b\u5956\u52b1\u4e0e\u7ed3\u679c\u5956\u52b1\u7684\u89e3\u8026\u5206\u914d\uff1b\u5f15\u5165\u53cc\u95e8\u63a7SFT\u76ee\u6807\u51fd\u6570\u4ee5\u9ad8\u8d28\u91cf\u7ed3\u6784\u548c\u4e8b\u5b9e\u4fe1\u53f7\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u5728MATH\u3001Super-CLEVR\u7b49\u6587\u672c\u548c\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u6837\u672c\u6548\u7387\u548c\u6700\u7ec8\u51c6\u786e\u7387\u4e0a\u6301\u7eed\u4f18\u4e8eGRPO\u7b49\u57fa\u7ebf\uff1b\u6a21\u578b\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u5206\u5e03\u5916\u9c81\u68d2\u6027\u548c\u96f6\u6837\u672c\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u63a8\u7406\u589e\u5f3a\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u6784\u5efa\u66f4\u9ad8\u6548\u3001\u66f4\u9c81\u68d2\u7684\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.00357", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00357", "abs": "https://arxiv.org/abs/2602.00357", "authors": ["Chenyang Yuan", "Xiaoyuan Cheng"], "title": "Planning with Language and Generative Models: Toward General Reward-Guided Wireless Network Design", "comment": null, "summary": "Intelligent access point (AP) deployment remains challenging in next-generation wireless networks due to complex indoor geometries and signal propagation. We firstly benchmark general-purpose large language models (LLMs) as agentic optimizers for AP planning and find that, despite strong wireless domain knowledge, their dependence on external verifiers results in high computational costs and limited scalability. Motivated by these limitations, we study generative inference models guided by a unified reward function capturing core AP deployment objectives across diverse floorplans. We show that diffusion samplers consistently outperform alternative generative approaches. The diffusion process progressively improves sampling by smoothing and sharpening the reward landscape, rather than relying on iterative refinement, which is effective for non-convex and fragmented objectives. Finally, we introduce a large-scale real-world dataset for indoor AP deployment, requiring over $50k$ CPU hours to train general reward functions, and evaluate in- and out-of-distribution generalization and robustness. Our results suggest that diffusion-based generative inference with a unified reward function provides a scalable and domain-agnostic foundation for indoor AP deployment planning.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u4e00\u5956\u52b1\u51fd\u6570\u89e3\u51b3\u5ba4\u5185\u65e0\u7ebf\u63a5\u5165\u70b9\u90e8\u7f72\u96be\u9898\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u53ef\u6269\u5c55\u6027\u548c\u9886\u57df\u65e0\u5173\u6027\u4f18\u52bf", "motivation": "\u73b0\u6709\u667a\u80fdAP\u90e8\u7f72\u9762\u4e34\u5ba4\u5185\u51e0\u4f55\u590d\u6742\u6027\u548c\u4fe1\u53f7\u4f20\u64ad\u6311\u6218\uff0c\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u4f18\u5316\u5668\u5b58\u5728\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u6269\u5c55\u6027\u9650\u5236", "method": "\u7814\u7a76\u7531\u7edf\u4e00\u5956\u52b1\u51fd\u6570\u5f15\u5bfc\u7684\u751f\u6210\u63a8\u7406\u6a21\u578b\uff0c\u91cd\u70b9\u6bd4\u8f83\u6269\u6563\u91c7\u6837\u5668\u4e0e\u5176\u4ed6\u751f\u6210\u65b9\u6cd5\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u63ed\u793a\u6269\u6563\u8fc7\u7a0b\u901a\u8fc7\u5e73\u6ed1\u548c\u9510\u5316\u5956\u52b1\u666f\u89c2\u6765\u4f18\u5316\u975e\u51f8\u76ee\u6807", "result": "\u6269\u6563\u91c7\u6837\u5668\u6301\u7eed\u4f18\u4e8e\u5176\u4ed6\u751f\u6210\u65b9\u6cd5\uff0c\u6210\u529f\u6784\u5efa\u97005\u4e07CPU\u5c0f\u65f6\u8bad\u7ec3\u7684\u5927\u578b\u771f\u5b9e\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1\u4e86\u5206\u5e03\u5185\u5916\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027", "conclusion": "\u6269\u6563\u751f\u6210\u63a8\u7406\u914d\u5408\u7edf\u4e00\u5956\u52b1\u51fd\u6570\u4e3a\u5ba4\u5185AP\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u9886\u57df\u65e0\u5173\u7684\u65b0\u8303\u5f0f"}}
{"id": "2602.01062", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01062", "abs": "https://arxiv.org/abs/2602.01062", "authors": ["Chenyi Li", "Yuan Zhang", "Bo Wang", "Guoqing Ma", "Wei Tang", "Haoyang Huang", "Nan Duan"], "title": "SetPO: Set-Level Policy Optimization for Diversity-Preserving LLM Reasoning", "comment": null, "summary": "Reinforcement learning with verifiable rewards has shown notable effectiveness in enhancing large language models (LLMs) reasoning performance, especially in mathematics tasks. However, such improvements often come with reduced outcome diversity, where the model concentrates probability mass on a narrow set of solutions. Motivated by diminishing-returns principles, we introduce a set level diversity objective defined over sampled trajectories using kernelized similarity. Our approach derives a leave-one-out marginal contribution for each sampled trajectory and integrates this objective as a plug-in advantage shaping term for policy optimization. We further investigate the contribution of a single trajectory to language model diversity within a distribution perturbation framework. This analysis theoretically confirms a monotonicity property, proving that rarer trajectories yield consistently higher marginal contributions to the global diversity. Extensive experiments across a range of model scales demonstrate the effectiveness of our proposed algorithm, consistently outperforming strong baselines in both Pass@1 and Pass@K across various benchmarks.", "AI": {"tldr": "The paper tackles diversity collapse in RL-enhanced LLMs by introducing a set-level diversity objective using kernelized similarity. They derive leave-one-out marginal contributions for trajectories as an advantage shaping term, theoretically prove rarer trajectories contribute more to diversity, and show consistent improvements over baselines across model scales.", "motivation": "Reinforcement learning with verifiable rewards improves LLM reasoning (especially math) but causes reduced outcome diversity, where models concentrate probability mass on narrow solutions. This diversity loss follows diminishing-returns principles and limits solution exploration.", "method": "Defines a set-level diversity objective over sampled trajectories using kernelized similarity. Derives leave-one-out marginal contributions for each trajectory and integrates this as a plug-in advantage shaping term for policy optimization. Analyzes trajectory contributions within a distribution perturbation framework, theoretically proving monotonicity: rarer trajectories yield consistently higher marginal contributions to global diversity.", "result": "Extensive experiments across various model scales show the algorithm consistently outperforms strong baselines on both Pass@1 (single best solution) and Pass@K (K solution attempts) metrics across multiple benchmarks, demonstrating effectiveness in improving both performance and diversity.", "conclusion": "The proposed diversity-aware reinforcement learning algorithm successfully mitigates the diversity collapse problem while maintaining reasoning performance, providing a theoretically grounded and empirically validated solution that balances capability improvement with solution space exploration."}}
{"id": "2602.02120", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.02120", "abs": "https://arxiv.org/abs/2602.02120", "authors": ["Xin Wang", "Yabo Wang", "Rebing Wu"], "title": "Towards Ultimate Accuracy in Quantum Multi-Class Classification: A Trace-Distance Binary Tree AdaBoost Classifier", "comment": "26 pages, 30 figures", "summary": "We propose a Trace-distance binary Tree AdaBoost (TTA) multi-class quantum classifier, a practical pipeline for quantum multi-class classification that combines quantum-aware reductions with ensemble learning to improve trainability and resource efficiency. TTA builds a hierarchical binary tree by choosing, at each internal node, the bipartition that maximizes the trace distance between average quantum states; each node trains a binary AdaBoost ensemble of shallow variational quantum base learners. By confining intrinsically hard, small trace distance distinctions to small node-specific datasets and combining weak shallow learners via AdaBoost, TTA distributes capacity across many small submodels rather than one deep circuit, mitigating barren-plateau and optimization failures without sacrificing generalization. Empirically TTA achieves top test accuracy ($\\approx $100\\%) among quantum and classical baselines, is robust to common quantum errors, and realizes aggregate systems with 10000 cumulative layers and 0.2M parameters, implemented as many shallow circuits. Our results are empirical and implementable on near-term platforms, providing a resource-efficient route to scalable multi-class quantum machine learning.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.00360", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00360", "abs": "https://arxiv.org/abs/2602.00360", "authors": ["Sumana Biswas", "Karen Young", "Josephine Griffith"], "title": "Leveraging Textual-Cues for Enhancing Multimodal Sentiment Analysis by Object Recognition", "comment": null, "summary": "Multimodal sentiment analysis, which includes both image and text data, presents several challenges due to the dissimilarities in the modalities of text and image, the ambiguity of sentiment, and the complexities of contextual meaning. In this work, we experiment with finding the sentiments of image and text data, individually and in combination, on two datasets. Part of the approach introduces the novel `Textual-Cues for Enhancing Multimodal Sentiment Analysis' (TEMSA) based on object recognition methods to address the difficulties in multimodal sentiment analysis. Specifically, we extract the names of all objects detected in an image and combine them with associated text; we call this combination of text and image data TEMS. Our results demonstrate that only TEMS improves the results when considering all the object names for the overall sentiment of multimodal data compared to individual analysis. This research contributes to advancing multimodal sentiment analysis and offers insights into the efficacy of TEMSA in combining image and text data for multimodal sentiment analysis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faTEMSA\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u56fe\u50cf\u4e2d\u7684\u7269\u4f53\u540d\u79f0\u5e76\u4e0e\u6587\u672c\u7ed3\u5408\uff08\u5f62\u6210TEMS\u6570\u636e\uff09\uff0c\u6709\u6548\u63d0\u5347\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u6027\u80fd\uff0c\u5c24\u5176\u5728\u4f7f\u7528\u5168\u90e8\u7269\u4f53\u540d\u79f0\u65f6\u6548\u679c\u663e\u8457\u4f18\u4e8e\u5355\u6a21\u6001\u5206\u6790\u3002", "motivation": "\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u9762\u4e34\u6587\u672c\u4e0e\u56fe\u50cf\u6a21\u6001\u5dee\u5f02\u5927\u3001\u60c5\u611f\u6b67\u4e49\u53ca\u4e0a\u4e0b\u6587\u542b\u4e49\u590d\u6742\u7b49\u6311\u6218\u3002", "method": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u5206\u6790\u6587\u672c/\u56fe\u50cf\u5355\u72ec\u53ca\u7ec4\u5408\u60c5\u611f\uff1b\u63d0\u51faTEMSA\u65b9\u6cd5\uff1a\u57fa\u4e8e\u7269\u4f53\u8bc6\u522b\u63d0\u53d6\u56fe\u50cf\u4e2d\u6240\u6709\u7269\u4f53\u540d\u79f0\uff0c\u4e0e\u6587\u672c\u7ed3\u5408\u751f\u6210TEMS\u6570\u636e\u3002", "result": "\u4ec5\u5f53\u4f7f\u7528\u5168\u90e8\u7269\u4f53\u540d\u79f0\u6784\u5efaTEMS\u6570\u636e\u65f6\uff0c\u591a\u6a21\u6001\u6574\u4f53\u60c5\u611f\u5206\u6790\u6548\u679c\u4f18\u4e8e\u5355\u6a21\u6001\u72ec\u7acb\u5206\u6790\u3002", "conclusion": "\u7814\u7a76\u63a8\u8fdb\u4e86\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u9886\u57df\uff0c\u9a8c\u8bc1\u4e86TEMSA\u65b9\u6cd5\u5728\u878d\u5408\u56fe\u6587\u6570\u636e\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.02165", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.02165", "abs": "https://arxiv.org/abs/2602.02165", "authors": ["Kaining Zhang", "Xinbiao Wang", "Yuxuan Du", "Min-Hsiu Hsieh", "Dacheng Tao"], "title": "AQER: a scalable and efficient data loader for digital quantum computers", "comment": "45 pages, 19 figures", "summary": "Digital quantum computing promises to offer computational capabilities beyond the reach of classical systems, yet its capabilities are often challenged by scarce quantum resources. A critical bottleneck in this context is how to load classical or quantum data into quantum circuits efficiently. Approximate quantum loaders (AQLs) provide a viable solution to this problem by balancing fidelity and circuit complexity. However, most existing AQL methods are either heuristic or provide guarantees only for specific input types, and a general theoretical framework is still lacking. To address this gap, here we reformulate most AQL methods into a unified framework and establish information-theoretic bounds on their approximation error. Our analysis reveals that the achievable infidelity between the prepared state and target state scales linearly with the total entanglement entropy across subsystems when the loading circuit is applied to the target state. In light of this, we develop AQER, a scalable AQL method that constructs the loading circuit by systematically reducing entanglement in target states. We conduct systematic experiments to evaluate the effectiveness of AQER, using synthetic datasets, classical image and language datasets, and a quantum many-body state datasets with up to 50 qubits. The results show that AQER consistently outperforms existing methods in both accuracy and gate efficiency. Our work paves the way for scalable quantum data processing and real-world quantum computing applications.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\u89e3\u51b3\u91cf\u5b50\u6570\u636e\u52a0\u8f7d\u74f6\u9888\uff0c\u5efa\u7acb\u8fd1\u4f3c\u8bef\u5dee\u7406\u8bba\u8fb9\u754c\uff0c\u5f00\u53d1\u53ef\u6269\u5c55\u65b9\u6cd5AQER\u5e76\u572850\u91cf\u5b50\u6bd4\u7279\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6848", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u9762\u4e34\u6570\u636e\u52a0\u8f7d\u6548\u7387\u74f6\u9888\uff0c\u73b0\u6709\u8fd1\u4f3c\u91cf\u5b50\u52a0\u8f7d\u5668(AQL)\u65b9\u6cd5\u591a\u4e3a\u542f\u53d1\u5f0f\u6216\u4ec5\u9002\u7528\u4e8e\u7279\u5b9a\u8f93\u5165\uff0c\u7f3a\u4e4f\u666e\u9002\u6027\u7406\u8bba\u6846\u67b6\uff0c\u5236\u7ea6\u91cf\u5b50\u8d44\u6e90\u7a00\u7f3a\u73af\u5883\u4e0b\u7684\u53ef\u6269\u5c55\u91cf\u5b50\u6570\u636e\u5904\u7406", "method": "1) \u5c06\u73b0\u6709AQL\u65b9\u6cd5\u91cd\u6784\u4e3a\u7edf\u4e00\u6846\u67b6 2) \u5efa\u7acb\u8fd1\u4f3c\u8bef\u5dee\u7684\u4fe1\u606f\u8bba\u8fb9\u754c 3) \u57fa\u4e8e\u7ea0\u7f20\u71b5\u7f29\u653e\u89c4\u5f8b\u5f00\u53d1AQER\u65b9\u6cd5\uff08\u7cfb\u7edf\u964d\u4f4e\u76ee\u6807\u6001\u7ea0\u7f20\uff09", "result": "\u7406\u8bba\uff1a\u63ed\u793a\u4fdd\u771f\u5ea6\u635f\u5931\u4e0e\u5b50\u7cfb\u7edf\u603b\u7ea0\u7f20\u71b5\u5448\u7ebf\u6027\u5173\u7cfb\uff1b\u5b9e\u9a8c\uff1a\u5728\u5408\u6210\u6570\u636e\u3001\u7ecf\u5178\u56fe\u50cf/\u8bed\u8a00\u6570\u636e\u53ca50\u91cf\u5b50\u6bd4\u7279\u591a\u4f53\u91cf\u5b50\u6001\u4e0a\uff0cAQER\u5728\u7cbe\u5ea6\u548c\u95e8\u6548\u7387\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "\u4e3a\u53ef\u6269\u5c55\u91cf\u5b50\u6570\u636e\u5904\u7406\u5960\u5b9a\u7406\u8bba\u57fa\u7840\uff0c\u63a8\u52a8\u5b9e\u7528\u5316\u91cf\u5b50\u8ba1\u7b97\u5e94\u7528\u53d1\u5c55"}}
{"id": "2602.00361", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.00361", "abs": "https://arxiv.org/abs/2602.00361", "authors": ["Philipp Altmann", "Maximilian Mansky", "Maximilian Zorn", "Jonas Stein", "Claudia Linnhoff-Popien"], "title": "Quantum Generator Kernels", "comment": "28 pages, 4 figures, 8 tables, under review", "summary": "Quantum kernel methods offer significant theoretical benefits by rendering classically inseparable features separable in quantum space. Yet, the practical application of Quantum Machine Learning (QML), currently constrained by the limitations of Noisy Intermediate-Scale Quantum (NISQ) hardware, necessitates effective strategies to compress and embed large-scale real-world data like images into the constrained capacities of existing quantum devices or simulators. To this end, we propose Quantum Generator Kernels (QGKs), a generator-based approach to quantum kernels, comprising a set of Variational Generator Groups (VGGs) that merge universal generators into a parameterizable operator, ensuring scalable coverage of the available quantum space. Thereby, we address shortcomings of current leading strategies employing hybrid architectures, which might prevent exploiting quantum computing's full potential due to fixed intermediate embedding processes. To optimize the kernel alignment to the target domain, we train a weight vector to parameterize the projection of the VGGs in the current data context. Our empirical results demonstrate superior projection and classification capabilities of the QGK compared to state-of-the-art quantum and classical kernel approaches and show its potential to serve as a versatile framework for various QML applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u91cf\u5b50\u751f\u6210\u5668\u6838\uff08QGKs\uff09\uff0c\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5668\u7684\u65b0\u65b9\u6cd5\uff0c\u4f7f\u7528\u53d8\u5206\u751f\u6210\u5668\u7ec4\uff08VGGs\uff09\u6765\u514b\u670dNISQ\u786c\u4ef6\u9650\u5236\uff0c\u5728\u6570\u636e\u5d4c\u5165\u548c\u5206\u7c7b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u91cf\u5b50/\u7ecf\u5178\u6838\u65b9\u6cd5\u3002", "motivation": "NISQ\u786c\u4ef6\u9650\u5236\u963b\u788d\u4e86\u5927\u89c4\u6a21\u5b9e\u9645\u6570\u636e\u6709\u6548\u538b\u7f29\u548c\u5d4c\u5165\u91cf\u5b50\u8bbe\u5907\uff0c\u4e14\u5f53\u524d\u6df7\u5408\u67b6\u6784\u7684\u56fa\u5b9a\u4e2d\u95f4\u5d4c\u5165\u8fc7\u7a0b\u9650\u5236\u4e86\u91cf\u5b50\u8ba1\u7b97\u6f5c\u529b\u7684\u5145\u5206\u53d1\u6325\u3002", "method": "\u63d0\u51fa\u7531\u53d8\u5206\u751f\u6210\u5668\u7ec4\uff08VGGs\uff09\u6784\u6210\u7684QGKs\uff0c\u5c06\u901a\u7528\u751f\u6210\u5668\u5408\u5e76\u4e3a\u53ef\u53c2\u6570\u5316\u7b97\u5b50\u4ee5\u5b9e\u73b0\u91cf\u5b50\u7a7a\u95f4\u53ef\u6269\u5c55\u8986\u76d6\uff0c\u5e76\u8bad\u7ec3\u6743\u91cd\u5411\u91cf\u6765\u4f18\u5316\u6838\u4e0e\u76ee\u6807\u57df\u7684\u5bf9\u9f50\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u91cf\u5b50\u548c\u7ecf\u5178\u6838\u65b9\u6cd5\u76f8\u6bd4\uff0cQGKs\u5728\u6295\u5f71\u548c\u5206\u7c7b\u80fd\u529b\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "QGKs\u6709\u671b\u6210\u4e3a\u5404\u79cdQML\u5e94\u7528\u7684\u901a\u7528\u6846\u67b6\u3002"}}
{"id": "2602.01078", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01078", "abs": "https://arxiv.org/abs/2602.01078", "authors": ["Tong Xia", "Weibin Li", "Gang Liu", "Yong Li"], "title": "AutoHealth: An Uncertainty-Aware Multi-Agent System for Autonomous Health Data Modeling", "comment": null, "summary": "LLM-based agents have demonstrated strong potential for autonomous machine learning, yet their applicability to health data remains limited. Existing systems often struggle to generalize across heterogeneous health data modalities, rely heavily on predefined solution templates with insufficient adaptation to task-specific objectives, and largely overlook uncertainty estimation, which is essential for reliable decision-making in healthcare. To address these challenges, we propose \\textit{AutoHealth}, a novel uncertainty-aware multi-agent system that autonomously models health data and assesses model reliability. \\textit{AutoHealth} employs closed-loop coordination among five specialized agents to perform data exploration, task-conditioned model construction, training, and optimization, while jointly prioritizing predictive performance and uncertainty quantification. Beyond producing ready-to-use models, the system generates comprehensive reports to support trustworthy interpretation and risk-aware decision-making. To rigorously evaluate its effectiveness, we curate a challenging real-world benchmark comprising 17 tasks across diverse data modalities and learning settings. \\textit{AutoHealth} completes all tasks and outperforms state-of-the-art baselines by 29.2\\% in prediction performance and 50.2\\% in uncertainty estimation.", "AI": {"tldr": "This paper proposes AutoHealth, a novel uncertainty-aware multi-agent system that autonomously models health data and assesses model reliability, outperforming state-of-the-art baselines by 29.2% in prediction and 50.2% in uncertainty estimation on a real-world benchmark of 17 tasks.", "motivation": "LLM-based agents show strong potential for autonomous machine learning but have limited applicability to health data due to difficulties generalizing across heterogeneous modalities, reliance on predefined templates with insufficient task-specific adaptation, and overlooking critical uncertainty estimation needed for reliable healthcare decisions.", "method": "The authors propose AutoHealth, a multi-agent system using five specialized agents coordinated in a closed loop for data exploration, task-conditioned model construction, training, and optimization. The system jointly optimizes predictive performance and uncertainty quantification, and generates comprehensive reports for trustworthy interpretation.", "result": "On a curated real-world benchmark of 17 diverse tasks, AutoHealth successfully completes all tasks and achieves 29.2% improvement in prediction performance and 50.2% improvement in uncertainty estimation compared to state-of-the-art baselines.", "conclusion": "AutoHealth effectively addresses key challenges in applying LLM-based agents to health data by providing an autonomous, uncertainty-aware system that significantly improves both predictive accuracy and reliability assessment, supporting risk-aware clinical decision-making."}}
{"id": "2602.02191", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.02191", "abs": "https://arxiv.org/abs/2602.02191", "authors": ["Eric Tesse"], "title": "The trouble with recording devices", "comment": "17 pages", "summary": "Quantum theory encounters a difficulty when attempting to describe recording devices. If the recording is of events in which quantum uncertainty plays a role, such as an experiment on a quantum system, quantum theory is unable to correctly predict the probabilities of both future and past states of the recording. The nature of this difficulty will be laid out at the outset. A resolution then will be presented, in which the Born rule will be lightly amended so as to correctly predict all probabilities. The resolution will have the further benefit of clarifying how quantum theory applies to an array of situations in which the theory can be ambiguous, such as the descriptions of continuous measurements, and of closed systems containing all observers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6307\u51fa\u91cf\u5b50\u7406\u8bba\u5728\u63cf\u8ff0\u8bb0\u5f55\u8bbe\u5907\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u65e0\u6cd5\u6b63\u786e\u9884\u6d4b\u6d89\u53ca\u91cf\u5b50\u4e0d\u786e\u5b9a\u6027\u7684\u5b9e\u9a8c\u4e2d\u8bb0\u5f55\u8bbe\u5907\u7684\u672a\u6765\u548c\u8fc7\u53bb\u72b6\u6001\u6982\u7387\u3002\u4f5c\u8005\u63d0\u51fa\u8f7b\u5fae\u4fee\u6b63\u73bb\u6069\u89c4\u5219\u4ee5\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u5e76\u6f84\u6e05\u91cf\u5b50\u7406\u8bba\u5728\u8fde\u7eed\u6d4b\u91cf\u548c\u5305\u542b\u6240\u6709\u89c2\u5bdf\u8005\u7684\u5c01\u95ed\u7cfb\u7edf\u7b49\u6a21\u7cca\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u91cf\u5b50\u7406\u8bba\u5728\u63cf\u8ff0\u8bb0\u5f55\u8bbe\u5907\uff08\u5c24\u5176\u662f\u6d89\u53ca\u91cf\u5b50\u4e0d\u786e\u5b9a\u6027\u7684\u5b9e\u9a8c\uff09\u65f6\u5b58\u5728\u6839\u672c\u6027\u56f0\u96be\uff1a\u65e0\u6cd5\u540c\u65f6\u6b63\u786e\u9884\u6d4b\u8bb0\u5f55\u8bbe\u5907\u672a\u6765\u548c\u8fc7\u53bb\u72b6\u6001\u7684\u6982\u7387\uff0c\u66b4\u9732\u4e86\u73b0\u6709\u7406\u8bba\u5728\u65f6\u95f4\u5bf9\u79f0\u6027\u548c\u6d4b\u91cf\u63cf\u8ff0\u4e0a\u7684\u7f3a\u9677\u3002", "method": "\u63d0\u51fa\u5bf9\u73bb\u6069\u89c4\u5219\u8fdb\u884c\u8f7b\u5fae\u4fee\u6b63\uff0c\u901a\u8fc7\u8c03\u6574\u6982\u7387\u8ba1\u7b97\u89c4\u5219\uff0c\u4f7f\u91cf\u5b50\u7406\u8bba\u80fd\u591f\u81ea\u6d3d\u5730\u9884\u6d4b\u6240\u6709\u65f6\u95f4\u65b9\u5411\u4e0a\u7684\u6982\u7387\uff0c\u5e76\u6269\u5c55\u89e3\u91ca\u8303\u56f4\u3002", "result": "\u4fee\u6b63\u540e\u7684\u7406\u8bba\u6210\u529f\u89e3\u51b3\u4e86\u8bb0\u5f55\u8bbe\u5907\u7684\u6982\u7387\u9884\u6d4b\u77db\u76fe\uff0c\u540c\u65f6\u660e\u786e\u4e86\u91cf\u5b50\u7406\u8bba\u5728\u8fde\u7eed\u6d4b\u91cf\u3001\u5305\u542b\u5168\u90e8\u89c2\u5bdf\u8005\u7684\u5c01\u95ed\u7cfb\u7edf\u7b49\u590d\u6742\u573a\u666f\u4e2d\u7684\u9002\u7528\u8fb9\u754c\u548c\u64cd\u4f5c\u5b9a\u4e49\u3002", "conclusion": "\u901a\u8fc7\u4fee\u6b63\u73bb\u6069\u89c4\u5219\uff0c\u8be5\u65b9\u6848\u4e0d\u4ec5\u6d88\u9664\u4e86\u91cf\u5b50\u7406\u8bba\u5728\u65f6\u95f4\u5bf9\u79f0\u6027\u4e0a\u7684\u5185\u5728\u77db\u76fe\uff0c\u8fd8\u4e3a\u4f20\u7edf\u6a21\u7cca\u9886\u57df\uff08\u5982\u6d4b\u91cf\u8fc7\u7a0b\u548c\u89c2\u5bdf\u8005\u7cfb\u7edf\uff09\u63d0\u4f9b\u4e86\u6e05\u6670\u7edf\u4e00\u7684\u91cf\u5b50\u63cf\u8ff0\u6846\u67b6\uff0c\u589e\u5f3a\u4e86\u7406\u8bba\u7684\u5b8c\u5907\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2602.00372", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00372", "abs": "https://arxiv.org/abs/2602.00372", "authors": ["Aaron R. Flouro", "Shawn P. Chadwick"], "title": "Post-Training Probability Manifold Correction via Structured SVD Pruning and Self-Referential Distillation", "comment": "16 pages, 10 tables, 4 figures", "summary": "Large language models are expensive to deploy. We introduce Sparse Knowledge Distillation (SparseKD), a post-training method that compresses transformer models by combining structured SVD pruning with self-referential knowledge distillation. The key insight is simple: instead of using an external teacher, the model teaches itself by matching its own probability distribution from before compression. This self-referential setup enables surprisingly strong quality recovery after aggressive pruning.\n  Our experiments reveal an unexpected finding: self-referential distillation alone, applied post-training under an identical objective and fixed calibration dataset, improves model quality by 39% relative to the original converged checkpoint. When combined with structured pruning, SparseKD achieves 15-65% parameter reduction with acceptable quality trade-offs. Kernel profiling shows that speedups arise entirely from reduced dense matrix multiplication in feed-forward layers while attention remains unchanged, making this approach complementary to attention optimizations.\n  We validate across two model families (0.6B and 3.8B parameters) with multi-seed experiments confirming high reproducibility. SparseKD requires no external super-teacher, no architectural changes, and no custom inference kernels, making it immediately deployable with existing infrastructure.", "AI": {"tldr": "SparseKD\u662f\u4e00\u79cd\u540e\u8bad\u7ec3\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u6211\u53c2\u7167\u77e5\u8bc6\u84b8\u998f\uff08\u6a21\u578b\u5339\u914d\u81ea\u8eab\u538b\u7f29\u524d\u7684\u6982\u7387\u5206\u5e03\uff09\u4e0e\u7ed3\u6784\u5316SVD\u526a\u679d\u7ed3\u5408\uff0c\u5b9e\u73b015-65%\u7684\u53c2\u6570\u91cf\u7f29\u51cf\uff0c\u4e14\u65e0\u9700\u5916\u90e8\u6559\u5e08\u6a21\u578b\u6216\u5b9a\u5236\u63a8\u7406\u5185\u6838\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u6210\u672c\u9ad8\u6602\uff0c\u4e9f\u9700\u9ad8\u6548\u7684\u6a21\u578b\u538b\u7f29\u6280\u672f\u6765\u964d\u4f4e\u8d44\u6e90\u5f00\u9500\u3002", "method": "\u63d0\u51fa\u7a00\u758f\u77e5\u8bc6\u84b8\u998f\uff08SparseKD\uff09\uff0c\u91c7\u7528\u81ea\u6211\u53c2\u7167\u77e5\u8bc6\u84b8\u998f\u673a\u5236\uff1a\u6a21\u578b\u5728\u538b\u7f29\u540e\u5b66\u4e60\u5339\u914d\u5176\u538b\u7f29\u524d\u7684\u8f93\u51fa\u6982\u7387\u5206\u5e03\uff0c\u800c\u975e\u4f9d\u8d56\u5916\u90e8\u6559\u5e08\u6a21\u578b\uff1b\u540c\u65f6\u7ed3\u5408\u7ed3\u6784\u5316SVD\u526a\u679d\u8fdb\u884c\u53c2\u6570\u538b\u7f29\u3002", "result": "\u81ea\u6211\u53c2\u7167\u84b8\u998f\u5355\u72ec\u4f7f\u7528\u53ef\u76f8\u5bf9\u539f\u59cb\u6a21\u578b\u63d0\u534739%\u8d28\u91cf\uff1b\u7ed3\u5408\u526a\u679d\u5b9e\u73b015-65%\u53c2\u6570\u524a\u51cf\uff1b\u52a0\u901f\u6e90\u4e8e\u524d\u9988\u5c42\u77e9\u9635\u4e58\u6cd5\u51cf\u5c11\uff0c\u6ce8\u610f\u529b\u673a\u5236\u4e0d\u53d8\uff1b\u57280.6B/3.8B\u6a21\u578b\u4e0a\u9a8c\u8bc1\u6709\u6548\u4e14\u53ef\u590d\u73b0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u5916\u90e8\u6559\u5e08\u3001\u67b6\u6784\u6539\u52a8\u6216\u5b9a\u5236\u63a8\u7406\u5185\u6838\uff0c\u53ef\u76f4\u63a5\u90e8\u7f72\u4e8e\u73b0\u6709\u57fa\u7840\u8bbe\u65bd\uff0c\u5177\u6709\u5f3a\u5b9e\u7528\u6027\u3002"}}
{"id": "2602.01082", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01082", "abs": "https://arxiv.org/abs/2602.01082", "authors": ["Yiliu He", "Tianle Li", "Binghao Ji", "Zhiyuan Liu", "Di Huang"], "title": "EvoOpt-LLM: Evolving industrial optimization models with large language models", "comment": null, "summary": "Optimization modeling via mixed-integer linear programming (MILP) is fundamental to industrial planning and scheduling, yet translating natural-language requirements into solver-executable models and maintaining them under evolving business rules remains highly expertise-intensive. While large language models (LLMs) offer promising avenues for automation, existing methods often suffer from low data efficiency, limited solver-level validity, and poor scalability to industrial-scale problems. To address these challenges, we present EvoOpt-LLM, a unified LLM-based framework supporting the full lifecycle of industrial optimization modeling, including automated model construction, dynamic business-constraint injection, and end-to-end variable pruning. Built on a 7B-parameter LLM and adapted via parameter-efficient LoRA fine-tuning, EvoOpt-LLM achieves a generation rate of 91% and an executability rate of 65.9% with only 3,000 training samples, with critical performance gains emerging under 1,500 samples. The constraint injection module reliably augments existing MILP models while preserving original objectives, and the variable pruning module enhances computational efficiency, achieving an F1 score of ~0.56 on medium-sized LP models with only 400 samples. EvoOpt-LLM demonstrates a practical, data-efficient approach to industrial optimization modeling, reducing reliance on expert intervention while improving adaptability and solver efficiency.", "AI": {"tldr": "EvoOpt-LLM\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5de5\u4e1a\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212(MILP)\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7LoRA\u5fae\u8c037B\u53c2\u6570\u6a21\u578b\u5b9e\u73b0\u6a21\u578b\u81ea\u52a8\u6784\u5efa\u3001\u52a8\u6001\u7ea6\u675f\u6ce8\u5165\u548c\u53d8\u91cf\u526a\u679d\uff0c\u4ec5\u75283000\u8bad\u7ec3\u6837\u672c\u5c31\u8fbe\u523091%\u751f\u6210\u7387\u548c65.9%\u53ef\u6267\u884c\u7387\u3002", "motivation": "\u5de5\u4e1a\u89c4\u5212\u8c03\u5ea6\u4f9d\u8d56MILP\u5efa\u6a21\uff0c\u4f46\u5c06\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u8f6c\u5316\u4e3a\u53ef\u6c42\u89e3\u6a21\u578b\u5e76\u968f\u4e1a\u52a1\u89c4\u5219\u7ef4\u62a4\u9700\u5927\u91cf\u4e13\u5bb6\u77e5\u8bc6\uff1b\u73b0\u6709LLM\u65b9\u6cd5\u6570\u636e\u6548\u7387\u4f4e\u3001\u6c42\u89e3\u5668\u7ea7\u6709\u6548\u6027\u6709\u9650\u3001\u96be\u4ee5\u6269\u5c55\u5230\u5de5\u4e1a\u89c4\u6a21\u3002", "method": "\u63d0\u51faEvoOpt-LLM\u7edf\u4e00\u6846\u67b6\uff0c\u57fa\u4e8e7B\u53c2\u6570LLM\u91c7\u7528\u53c2\u6570\u9ad8\u6548LoRA\u5fae\u8c03\uff0c\u652f\u6301\u6a21\u578b\u81ea\u52a8\u6784\u5efa\u3001\u52a8\u6001\u4e1a\u52a1\u7ea6\u675f\u6ce8\u5165\u548c\u7aef\u5230\u7aef\u53d8\u91cf\u526a\u679d\u5168\u6d41\u7a0b\u3002", "result": "\u4ec5\u75283000\u8bad\u7ec3\u6837\u672c\u5b9e\u73b091%\u751f\u6210\u7387\u300165.9%\u53ef\u6267\u884c\u7387\uff0c1500\u6837\u672c\u5185\u6027\u80fd\u663e\u8457\u63d0\u5347\uff1b\u7ea6\u675f\u6ce8\u5165\u6a21\u5757\u53ef\u9760\u589e\u5f3aMILP\u6a21\u578b\uff1b\u53d8\u91cf\u526a\u679d\u6a21\u5757\u5728\u4e2d\u578bLP\u6a21\u578b\u4e0aF1\u8fbe~0.56\uff08\u4ec5400\u6837\u672c\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u5b9e\u7528\u4e14\u6570\u636e\u9ad8\u6548\u7684\u5de5\u4e1a\u4f18\u5316\u5efa\u6a21\u65b9\u6848\uff0c\u51cf\u5c11\u4e13\u5bb6\u4f9d\u8d56\uff0c\u63d0\u5347\u6a21\u578b\u9002\u5e94\u6027\u548c\u6c42\u89e3\u6548\u7387\u3002"}}
{"id": "2602.00376", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00376", "abs": "https://arxiv.org/abs/2602.00376", "authors": ["Delia McGrath", "Curtis Chong", "Rohil Kulkarni", "Gerbrand Ceder", "Adeesh Kolluru"], "title": "MATRIX: A Multimodal Benchmark and Post-Training Framework for Materials Science", "comment": "17 pages, 9 Figures, submitted", "summary": "Scientific reasoning in materials science requires integrating multimodal experimental evidence with underlying physical theory. Existing benchmarks make it difficult to assess whether incorporating visual experimental data during post-training improves mechanism-grounded explanation reasoning beyond text-only supervision. We introduce MATRIX, a multimodal benchmark for materials science reasoning that evaluates foundational theory, research-level reasoning, and the interpretation of real experimental artifacts across multiple characterization modalities. Using MATRIX as a controlled diagnostic, we isolate the effect of visual grounding by comparing post-training on structured materials science text alone with post-training that incorporates paired experimental images. Despite using relatively small amounts of multimodal data, visual supervision improves experimental interpretation by 10-25% and yields 5-16% gains on text-only scientific reasoning tasks. Our results demonstrate that these improvements rely on correct image-text alignment during post-training, highlighting cross-modal representational transfer. We also observe consistent improvements on ScienceQA and PubMedQA, demonstrating that the benefits of structured multimodal post-training extend beyond materials science. The MATRIX dataset is available at https://huggingface.co/datasets/radical-ai/MATRIX and the model at https://huggingface.co/radical-ai/MATRIX-PT.", "AI": {"tldr": "This paper introduces MATRIX, a multimodal materials science benchmark that demonstrates incorporating experimental images during post-training significantly improves both multimodal experimental interpretation (10-25%) and text-only scientific reasoning (5-16%), with benefits extending to other scientific domains.", "motivation": "Existing benchmarks cannot effectively assess whether incorporating visual experimental data during post-training improves mechanism-grounded explanation reasoning in materials science beyond text-only supervision.", "method": "Introduced MATRIX, a multimodal benchmark evaluating foundational theory, research-level reasoning, and real experimental artifact interpretation across multiple characterization modalities. Conducted controlled comparison between text-only and paired image-text post-training approaches.", "result": "Visual supervision improved experimental interpretation by 10-25% and text-only scientific reasoning by 5-16%. Improvements depend on correct image-text alignment during post-training, showing cross-modal representational transfer. Benefits also transferred to ScienceQA and PubMedQA.", "conclusion": "Structured multimodal post-training with proper image-text alignment provides significant improvements in scientific reasoning capabilities, demonstrating cross-modal representational transfer that extends beyond materials science to broader scientific domains."}}
{"id": "2602.02263", "categories": ["quant-ph", "math.NT"], "pdf": "https://arxiv.org/pdf/2602.02263", "abs": "https://arxiv.org/abs/2602.02263", "authors": ["David Jao", "Maher Mamah"], "title": "On the Spectral theory of Isogeny Graphs and Quantum Sampling of Hard Supersingular Elliptic curves", "comment": "24 pages with two figures", "summary": "In this paper we study the problem of sampling random supersingular elliptic curves with unknown endomorphism rings. This task has recently attracted significant attention, as the secure instantiation of many isogeny-based cryptographic protocols relies on the ability to sample such ``hard'' curves. Existing approaches, however, achieve this only in a trusted-setup setting. We present the first provable quantum polynomial-time algorithm that samples a random hard supersingular elliptic curve with high probability.Our algorithm runs heuristically in $\\tilde{O}\\!\\left(\\log^{4}p\\right)$ quantum gate complexity and in $\\tilde{O}\\!\\left(\\log^{13} p\\right)$ under the Generalized Riemann Hypothesis. As a consequence, our algorithm gives a secure instantiation of the CGL hash function and other cryptographic primitives.\n  Our analysis relies on a new spectral delocalization result for supersingular $\\ell$-isogeny graphs: we prove the Quantum Unique Ergodicity conjecture, and we further provide numerical evidence for complete eigenvector delocalization; this theoretical result may be of independent interest. Along the way, we prove a stronger $\\varepsilon$-separation property for eigenvalues of isogeny graphs than that predicted in the quantum money protocol of Kane, Sharif, and Silverberg, thereby removing a key heuristic assumption in their construction.", "AI": {"tldr": "This paper presents the first provable quantum polynomial-time algorithm for sampling random supersingular elliptic curves with unknown endomorphism rings, crucial for isogeny-based cryptography. The algorithm relies on novel spectral delocalization results for isogeny graphs and provides secure instantiations of cryptographic primitives like the CGL hash function.", "motivation": "Sampling random supersingular elliptic curves with unknown endomorphism rings is essential for secure instantiation of isogeny-based cryptographic protocols. Existing approaches only work in a trusted-setup setting, which is a significant limitation. The paper aims to provide the first provable quantum polynomial-time solution that eliminates the need for trusted setup.", "method": "The authors develop a quantum algorithm with heuristic complexity O~(log^4 p) quantum gates (or O~(log^13 p) under GRH). The core technical contribution is a new spectral delocalization result for supersingular \u2113-isogeny graphs: proving the Quantum Unique Ergodicity conjecture and providing numerical evidence for complete eigenvector delocalization. They also prove a stronger \u03b5-separation property for eigenvalues than previously assumed in quantum money protocols.", "result": "The algorithm successfully samples random hard supersingular elliptic curves with high probability. As a direct consequence, it provides a secure instantiation of the CGL hash function and other cryptographic primitives. The theoretical spectral delocalization result is also of independent mathematical interest.", "conclusion": "This work solves a fundamental problem in isogeny-based cryptography by providing the first provable quantum polynomial-time algorithm for sampling hard supersingular elliptic curves without trusted setup. The novel spectral analysis techniques developed may have broader applications beyond cryptography."}}
{"id": "2602.00384", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00384", "abs": "https://arxiv.org/abs/2602.00384", "authors": ["Ke Wang", "Nguyen Gia Hien Vu", "Yifan Tang", "Mostafa Rahmani Dehaghani", "G. Gary Wang"], "title": "RePaint-Enhanced Conditional Diffusion Model for Parametric Engineering Designs under Performance and Parameter Constraints", "comment": null, "summary": "This paper presents a RePaint-enhanced framework that integrates a pre-trained performance-guided denoising diffusion probabilistic model (DDPM) for performance- and parameter-constraint engineering design generation. The proposed method enables the generation of missing design components based on a partial reference design while satisfying performance constraints, without retraining the underlying model. By applying mask-based resampling during inference process, RePaint allows efficient and controllable repainting of partial designs under both performance and parameter constraints, which is not supported by conventional DDPM-base methods. The framework is evaluated on two representative design problems, parametric ship hull design and airfoil design, demonstrating its ability to generate novel designs with expected performance based on a partial reference design. Results show that the method achieves accuracy comparable to or better than pre-trained models while enabling controlled novelty through fixing partial designs. Overall, the proposed approach provides an efficient, training-free solution for parameter-constraint-aware generative design in engineering applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eRePaint\u589e\u5f3a\u7684\u65e0\u8bad\u7ec3\u6846\u67b6\uff0c\u96c6\u6210\u9884\u8bad\u7ec3\u6027\u80fd\u5f15\u5bfc\u53bb\u566a\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u63a9\u7801\u91cd\u91c7\u6837\u5b9e\u73b0\u6ee1\u8db3\u6027\u80fd\u7ea6\u675f\u7684\u5de5\u7a0b\u8bbe\u8ba1\u751f\u6210", "motivation": "\u4f20\u7edf\u53bb\u566a\u6269\u6563\u6a21\u578b\u96be\u4ee5\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u90e8\u5206\u53c2\u8003\u8bbe\u8ba1\u8fdb\u884c\u6ee1\u8db3\u6027\u80fd\u548c\u53c2\u6570\u7ea6\u675f\u7684\u5de5\u7a0b\u751f\u6210\uff0c\u4e14\u91cd\u8bad\u7ec3\u6210\u672c\u9ad8\u3001\u4e0d\u7075\u6d3b", "method": "\u63d0\u51faRePaint\u589e\u5f3a\u6846\u67b6\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5e94\u7528\u63a9\u7801\u91cd\u91c7\u6837\u6280\u672f\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6027\u80fd\u5f15\u5bfcDDPM\u5b9e\u73b0\u90e8\u5206\u8bbe\u8ba1\u7684\u53ef\u63a7\u91cd\u7ed8\uff0c\u751f\u6210\u7f3a\u5931\u7ec4\u4ef6\u5e76\u6ee1\u8db3\u7ea6\u675f\u6761\u4ef6", "result": "\u5728\u8239\u4f53\u53c2\u6570\u5316\u8bbe\u8ba1\u548c\u7ffc\u578b\u8bbe\u8ba1\u4e24\u4e2a\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u6210\u529f\uff0c\u751f\u6210\u7cbe\u5ea6\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u540c\u65f6\u901a\u8fc7\u56fa\u5b9a\u90e8\u5206\u8bbe\u8ba1\u5b9e\u73b0\u53ef\u63a7\u7684\u521b\u65b0\u6027", "conclusion": "\u4e3a\u5de5\u7a0b\u5e94\u7528\u4e2d\u7684\u53c2\u6570\u7ea6\u675f\u611f\u77e5\u751f\u6210\u5f0f\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u65b0\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u53ef\u63a7\u4e14\u65b0\u9896\u7684\u8bbe\u8ba1\u751f\u6210\u80fd\u529b"}}
{"id": "2602.01090", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01090", "abs": "https://arxiv.org/abs/2602.01090", "authors": ["Yang Liu", "Chuan Zhou", "Yancheng Chen", "Shuai Zhang", "Xixun Lin", "Xiaoqing Wang"], "title": "Hard Constraints Meet Soft Generation: Guaranteed Feasibility for LLM-based Combinatorial Optimization", "comment": "32 pages, 2 figures", "summary": "Large language models (LLMs) have emerged as promising general-purpose solvers for combinatorial optimization (CO), yet they fundamentally lack mechanisms to guarantee solution feasibility which is critical for real-world deployment. In this work, we introduce FALCON, a framework that ensures 100\\% feasibility through three key innovations: (i) \\emph{grammar-constrained decoding} enforces syntactic validity, (ii) a \\emph{feasibility repair layer} corrects semantic constraint violations, and (iii) \\emph{adaptive Best-of-$N$ sampling} allocates inference compute efficiently. To train the underlying LLM, we introduce the Best-anchored Objective-guided Preference Optimization (BOPO) in LLM training, which weights preference pairs by their objective gap, providing dense supervision without human labels. Theoretically, we prove convergence for BOPO and provide bounds on repair-induced quality loss. Empirically, across seven NP-hard CO problems, FALCON achieves perfect feasibility while matching or exceeding the solution quality of state-of-the-art neural and LLM-based solvers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFALCON\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u6cd5\u7ea6\u675f\u89e3\u7801\u3001\u53ef\u884c\u6027\u4fee\u590d\u5c42\u548c\u81ea\u9002\u5e94Best-of-N\u91c7\u6837\u786e\u4fddLLM\u6c42\u89e3\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u7684100%\u53ef\u884c\u6027\uff0c\u5e76\u5f15\u5165BOPO\u8bad\u7ec3\u65b9\u6cd5\uff0c\u57287\u4e2aNP-hard\u95ee\u9898\u4e0a\u5b9e\u73b0\u5b8c\u7f8e\u53ef\u884c\u6027\u4e14\u89e3\u8d28\u91cf\u8fbe\u5230\u6216\u8d85\u8d8aSOTA\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ec4\u5408\u4f18\u5316\u9886\u57df\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u4fdd\u8bc1\u89e3\u53ef\u884c\u6027\u7684\u673a\u5236\uff0c\u8fd9\u5bf9\u5b9e\u9645\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faFALCON\u6846\u67b6\uff0c\u5305\u542b\uff1a(i)\u8bed\u6cd5\u7ea6\u675f\u89e3\u7801\u786e\u4fdd\u8bed\u6cd5\u6709\u6548\u6027\uff1b(ii)\u53ef\u884c\u6027\u4fee\u590d\u5c42\u7ea0\u6b63\u8bed\u4e49\u7ea6\u675f\u8fdd\u53cd\uff1b(iii)\u81ea\u9002\u5e94Best-of-N\u91c7\u6837\u9ad8\u6548\u5206\u914d\u63a8\u7406\u8ba1\u7b97\u3002\u8bad\u7ec3\u65b9\u9762\uff0c\u63d0\u51faBOPO\u65b9\u6cd5\uff0c\u6839\u636e\u76ee\u6807\u5dee\u8ddd\u52a0\u6743\u504f\u597d\u5bf9\uff0c\u63d0\u4f9b\u65e0\u76d1\u7763\u7684\u5bc6\u96c6\u76d1\u7763\u3002\u7406\u8bba\u8bc1\u660e\u4e86BOPO\u7684\u6536\u655b\u6027\u548c\u4fee\u590d\u5bfc\u81f4\u7684\u8d28\u91cf\u635f\u5931\u8fb9\u754c\u3002", "result": "\u57287\u4e2aNP-hard\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e0a\uff0cFALCON\u5b9e\u73b0\u4e86100%\u53ef\u884c\u6027\uff0c\u540c\u65f6\u89e3\u8d28\u91cf\u5339\u914d\u6216\u8d85\u8d8a\u73b0\u6709\u795e\u7ecf\u548cLLM\u57fa\u6c42\u89e3\u5668\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u57fa\u7ec4\u5408\u4f18\u5316\u6c42\u89e3\u5668\u7684\u53ef\u884c\u6027\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u4e86\u6c42\u89e3\u8d28\u91cf\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u9760\u4fdd\u969c\u3002"}}
{"id": "2602.02309", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.02309", "abs": "https://arxiv.org/abs/2602.02309", "authors": ["Sarfraj Fency", "Rangeet Bhattacharyya"], "title": "Optimal enhancement of the Overhauser and Solid Effects within a unified framework", "comment": "12 pages, 4 figures", "summary": "The Overhauser effect (OE) and the Solid effect (SE) are two Dynamic Nuclear Polarization techniques. These two-spin techniques are widely used to create nonequilibrium nuclear spin states having polarization far beyond its equilibrium value. OE is commonly encountered in liquids, and SE is a solid-state technique. Here, we report a single framework based on a recently proposed quantum master equation, to explain both OE and SE. To this end, we use a fluctuation-regularized quantum master equation that predicts dipolar relaxation and drive-induced dissipation, in addition to the standard environmental dissipation channels. Importantly, this unified approach predicts the existence of optimal microwave drive amplitudes that maximize the OE and SE enhancements. We also report optimal enhancement regime for electron-nuclear coupling for maximal enhancement.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6da8\u843d\u6b63\u5219\u5316\u91cf\u5b50\u4e3b\u65b9\u7a0b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u89e3\u91caOverhauser\u6548\u5e94\u548cSolid\u6548\u5e94\u4e24\u79cd\u52a8\u6001\u6838\u6781\u5316\u6280\u672f\uff0c\u9884\u6d4b\u6700\u4f18\u5fae\u6ce2\u9a71\u52a8\u5e45\u5ea6\u548c\u7535\u5b50-\u6838\u8026\u5408\u589e\u5f3a\u533a\u95f4", "motivation": "\u73b0\u6709\u52a8\u6001\u6838\u6781\u5316\u6280\u672f\uff08OE\u548cSE\uff09\u7f3a\u4e4f\u7edf\u4e00\u7406\u8bba\u6846\u67b6\u89e3\u91ca\u5176\u589e\u5f3a\u673a\u5236\uff0c\u5c24\u5176\u5bf9\u9a71\u52a8\u573a\u5f3a\u5ea6\u548c\u8026\u5408\u53c2\u6570\u7684\u4f18\u5316\u6761\u4ef6\u7406\u89e3\u4e0d\u8db3", "method": "\u91c7\u7528\u6da8\u843d\u6b63\u5219\u5316\u91cf\u5b50\u4e3b\u65b9\u7a0b\uff0c\u7eb3\u5165\u5076\u6781\u5f1b\u8c6b\u3001\u9a71\u52a8\u8bf1\u5bfc\u8017\u6563\u53ca\u73af\u5883\u8017\u6563\u7b49\u591a\u901a\u9053\u673a\u5236\uff0c\u6784\u5efa\u53cc\u81ea\u65cb\u4f53\u7cfb\u7406\u8bba\u6a21\u578b", "result": "1. \u63ed\u793aOE\u548cSE\u7684\u5171\u540c\u91cf\u5b50\u52a8\u529b\u5b66\u673a\u5236 2. \u9884\u6d4b\u5b58\u5728\u4f7f\u6838\u6781\u5316\u589e\u5f3a\u6700\u5927\u5316\u7684\u6700\u4f18\u5fae\u6ce2\u9a71\u52a8\u5e45\u5ea6 3. \u786e\u5b9a\u7535\u5b50-\u6838\u8026\u5408\u5f3a\u5ea6\u7684\u6700\u4f18\u589e\u5f3a\u533a\u95f4", "conclusion": "\u8be5\u7edf\u4e00\u6846\u67b6\u4e0d\u4ec5\u9610\u660e\u4e86\u4e24\u79cd\u7ecf\u5178DNP\u6280\u672f\u7684\u5185\u5728\u5173\u8054\uff0c\u8fd8\u4e3a\u5b9e\u9a8c\u4f18\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\uff0c\u53ef\u6307\u5bfc\u5b9e\u73b0\u8d85\u8d8a\u5e73\u8861\u6001\u7684\u6838\u81ea\u65cb\u6781\u5316\u589e\u5f3a"}}
{"id": "2602.00388", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00388", "abs": "https://arxiv.org/abs/2602.00388", "authors": ["Zeyuan He", "Yupeng Chen", "Lang Lin", "Yihan Wang", "Shenxu Chang", "Eric Sommerlade", "Philip Torr", "Junchi Yu", "Adel Bibi", "Jialin Yu"], "title": "A Fragile Guardrail: Diffusion LLM's Safety Blessing and Its Failure Mode", "comment": null, "summary": "Diffusion large language models (D-LLMs) offer an alternative to autoregressive LLMs (AR-LLMs) and have demonstrated advantages in generation efficiency. Beyond the utility benefits, we argue that D-LLMs exhibit a previously underexplored safety blessing: their diffusion-style generation confers intrinsic robustness against jailbreak attacks originally designed for AR-LLMs. In this work, we provide an initial analysis of the underlying mechanism, showing that the diffusion trajectory induces a stepwise reduction effect that progressively suppresses unsafe generations. This robustness, however, is not absolute. We identify a simple yet effective failure mode, termed context nesting, where harmful requests are embedded within structured benign contexts, effectively bypassing the stepwise reduction mechanism. Empirically, we show that this simple strategy is sufficient to bypass D-LLMs' safety blessing, achieving state-of-the-art attack success rates across models and benchmarks. Most notably, it enables the first successful jailbreak of Gemini Diffusion, to our knowledge, exposing a critical vulnerability in commercial D-LLMs. Together, our results characterize both the origins and the limits of D-LLMs' safety blessing, constituting an early-stage red-teaming of D-LLMs.", "AI": {"tldr": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b(D-LLMs)\u76f8\u6bd4\u81ea\u56de\u5f52\u6a21\u578b\u5177\u6709\u751f\u6210\u6548\u7387\u4f18\u52bf\u548c\u5185\u5728\u5b89\u5168\u9c81\u68d2\u6027\u3002\u7814\u7a76\u53d1\u73b0\u5176\u5b89\u5168\u673a\u5236\u6e90\u4e8e\u6269\u6563\u8f68\u8ff9\u7684\u9010\u6b65\u6291\u5236\u6548\u5e94\uff0c\u4f46\u901a\u8fc7\"\u4e0a\u4e0b\u6587\u5d4c\u5957\"\u653b\u51fb\u53ef\u7ed5\u8fc7\u8be5\u673a\u5236\uff0c\u5728\u591a\u4e2a\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u5305\u62ec\u9996\u6b21\u6210\u529f\u8d8a\u72f1\u5546\u4e1a\u6a21\u578bGemini Diffusion\u3002", "motivation": "\u63a2\u7d22\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b(D-LLMs)\u76f8\u6bd4\u81ea\u56de\u5f52LLMs\u7684\u6f5c\u5728\u5b89\u5168\u4f18\u52bf\uff0c\u5206\u6790\u5176\u5bf9\u6297\u8d8a\u72f1\u653b\u51fb\u7684\u5185\u5728\u9c81\u68d2\u6027\u673a\u5236\u53ca\u5176\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6269\u6563\u751f\u6210\u8f68\u8ff9\u7684\u5185\u5728\u673a\u5236\uff0c\u8bc6\u522b\u51fa\"\u9010\u6b65\u6291\u5236\u6548\u5e94\"\u4f5c\u4e3a\u5b89\u5168\u57fa\u7840\uff1b\u63d0\u51fa\"\u4e0a\u4e0b\u6587\u5d4c\u5957\"\u653b\u51fb\u65b9\u6cd5\uff0c\u5c06\u6709\u5bb3\u8bf7\u6c42\u5d4c\u5165\u7ed3\u6784\u5316\u826f\u6027\u4e0a\u4e0b\u6587\u4e2d\u8fdb\u884c\u5b9e\u8bc1\u6d4b\u8bd5\u3002", "result": "1) \u53d1\u73b0\u6269\u6563\u8f68\u8ff9\u5177\u6709\u9010\u6b65\u6291\u5236\u4e0d\u5b89\u5168\u751f\u6210\u7684\u6548\u5e94\uff1b2) \u4e0a\u4e0b\u6587\u5d4c\u5957\u653b\u51fb\u7b80\u5355\u6709\u6548\uff0c\u5728\u5404\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u653b\u51fb\u6210\u529f\u7387\uff1b3) \u9996\u6b21\u6210\u529f\u8d8a\u72f1\u5546\u4e1a\u6a21\u578bGemini Diffusion\uff0c\u63ed\u793a\u5176\u5173\u952e\u6f0f\u6d1e\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86D-LLMs\u5b89\u5168\u4f18\u52bf\u7684\u6765\u6e90(\u9010\u6b65\u6291\u5236\u6548\u5e94)\u548c\u8fb9\u754c(\u6613\u53d7\u4e0a\u4e0b\u6587\u5d4c\u5957\u653b\u51fb)\uff0c\u4e3aD-LLMs\u7684\u65e9\u671f\u7ea2\u961f\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2602.01103", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01103", "abs": "https://arxiv.org/abs/2602.01103", "authors": ["Yiming Dong", "Kun Fu", "Haoyu Li", "Xinyuan Zhu", "Yurou Liu", "Lijing Shao", "Jieping Ye", "Zheng Wang"], "title": "Probing RLVR training instability through the lens of objective-level hacking", "comment": null, "summary": "Prolonged reinforcement learning with verifiable rewards (RLVR) has been shown to drive continuous improvements in the reasoning capabilities of large language models, but the training is often prone to instabilities, especially in Mixture-of-Experts (MoE) architectures. Training instability severely undermines model capability improvement, yet its underlying causes and mechanisms remain poorly understood. In this work, we introduce a principled framework for understanding RLVR instability through the lens of objective-level hacking. Unlike reward hacking, which arises from exploitable verifiers, objective-level hacking emerges from token-level credit misalignment and is manifested as system-level spurious signals in the optimization objective. Grounded in our framework, together with extensive experiments on a 30B MoE model, we trace the origin and formalize the mechanism behind a key pathological training dynamic in MoE models: the abnormal growth of the training-inference discrepancy, a phenomenon widely associated with instability but previously lacking a mechanistic explanation. These findings provide a concrete and causal account of the training dynamics underlying instabilities in MoE models, offering guidance for the design of stable RLVR algorithms.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76MoE\u67b6\u6784\u4e2dRLVR\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u63d0\u51fa\"\u76ee\u6807\u7ea7\u9ed1\u5ba2\"\u6846\u67b6\uff0c\u63ed\u793atoken\u7ea7\u4fe1\u7528\u9519\u914d\u5982\u4f55\u5bfc\u81f4\u7cfb\u7edf\u7ea7\u865a\u5047\u4fe1\u53f7\u548c\u8bad\u7ec3-\u63a8\u7406\u5dee\u5f02\u5f02\u5e38\u589e\u957f\u3002", "motivation": "RLVR\u80fd\u6301\u7eed\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5728MoE\u67b6\u6784\u4e2d\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff0c\u5176\u6839\u672c\u539f\u56e0\u548c\u673a\u5236\u5c1a\u4e0d\u660e\u786e\uff0c\u4e25\u91cd\u963b\u788d\u6a21\u578b\u80fd\u529b\u63d0\u5347\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u57fa\u4e8e\"\u76ee\u6807\u7ea7\u9ed1\u5ba2\"\u7684\u539f\u5219\u6027\u6846\u67b6\uff0c\u5e76\u572830B MoE\u6a21\u578b\u4e0a\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\uff0c\u8ffd\u8e2a\u5e76\u5f62\u5f0f\u5316\u8bad\u7ec3-\u63a8\u7406\u5dee\u5f02\u5f02\u5e38\u589e\u957f\u7684\u673a\u5236\u3002", "result": "\u53d1\u73b0\u5e76\u673a\u5236\u6027\u5730\u89e3\u91ca\u4e86\u8bad\u7ec3-\u63a8\u7406\u5dee\u5f02\u5f02\u5e38\u589e\u957f\u8fd9\u4e00\u5173\u952e\u75c5\u6001\u8bad\u7ec3\u52a8\u6001\uff0c\u8868\u660e\u5176\u6e90\u4e8etoken\u7ea7\u4fe1\u7528\u9519\u914d\u5bfc\u81f4\u7684\u7cfb\u7edf\u7ea7\u865a\u5047\u4fe1\u53f7\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3aMoE\u6a21\u578b\u4e2dRLVR\u4e0d\u7a33\u5b9a\u6027\u63d0\u4f9b\u4e86\u5177\u4f53\u56e0\u679c\u89e3\u91ca\uff0c\u4e3a\u8bbe\u8ba1\u7a33\u5b9aRLVR\u7b97\u6cd5\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2602.02364", "categories": ["quant-ph", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.02364", "abs": "https://arxiv.org/abs/2602.02364", "authors": ["Hoang M. Ngo", "Tre' R. Jeter", "Incheol Shin", "Wanli Xing", "Tamer Kahveci", "My T. Thai"], "title": "Guaranteeing Privacy in Hybrid Quantum Learning through Theoretical Mechanisms", "comment": null, "summary": "Quantum Machine Learning (QML) is becoming increasingly prevalent due to its potential to enhance classical machine learning (ML) tasks, such as classification. Although quantum noise is often viewed as a major challenge in quantum computing, it also offers a unique opportunity to enhance privacy. In particular, intrinsic quantum noise provides a natural stochastic resource that, when rigorously analyzed within the differential privacy (DP) framework and composed with classical mechanisms, can satisfy formal $(\\varepsilon, \u03b4)$-DP guarantees. This enables a reduction in the required classical perturbation without compromising the privacy budget, potentially improving model utility. However, the integration of classical and quantum noise for privacy preservation remains unexplored. In this work, we propose a hybrid noise-added mechanism, HYPER-Q, that combines classical and quantum noise to protect the privacy of QML models. We provide a comprehensive analysis of its privacy guarantees and establish theoretical bounds on its utility. Empirically, we demonstrate that HYPER-Q outperforms existing classical noise-based mechanisms in terms of adversarial robustness across multiple real-world datasets.", "AI": {"tldr": "The paper proposes HYPER-Q, a hybrid mechanism that combines classical and quantum noise to enhance privacy in Quantum Machine Learning while improving model utility and adversarial robustness.", "motivation": "Quantum Machine Learning faces privacy challenges, and while quantum noise is typically seen as a drawback, it can provide a natural stochastic resource for privacy protection. The integration of classical and quantum noise for privacy preservation remains unexplored, creating an opportunity to reduce classical perturbation requirements while maintaining privacy guarantees and improving model utility.", "method": "The authors propose HYPER-Q, a hybrid noise-added mechanism that combines classical and quantum noise. They analyze its privacy guarantees within the differential privacy framework, establish theoretical utility bounds, and empirically evaluate its performance across multiple real-world datasets.", "result": "HYPER-Q satisfies formal (\u03b5, \u03b4)-differential privacy guarantees, reduces the required classical perturbation without compromising the privacy budget, and empirically demonstrates superior adversarial robustness compared to existing classical noise-based mechanisms across multiple real-world datasets.", "conclusion": "The hybrid approach of combining classical and quantum noise effectively leverages intrinsic quantum noise as a privacy-enhancing resource, enabling stronger privacy guarantees with less classical perturbation and ultimately improving model utility and robustness in QML applications."}}
{"id": "2602.00392", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00392", "abs": "https://arxiv.org/abs/2602.00392", "authors": ["Arjun Rao", "Ruth Crasto", "Tessa Ooms", "David Rolnick", "Konstantin Klemmer", "Marc Ru\u00dfwurm"], "title": "Localized, High-resolution Geographic Representations with Slepian Functions", "comment": "23 pages, 12 figures, 6 tables", "summary": "Geographic data is fundamentally local. Disease outbreaks cluster in population centers, ecological patterns emerge along coastlines, and economic activity concentrates within country borders. Machine learning models that encode geographic location, however, distribute representational capacity uniformly across the globe, struggling at the fine-grained resolutions that localized applications require. We propose a geographic location encoder built from spherical Slepian functions that concentrate representational capacity inside a region-of-interest and scale to high resolutions without extensive computational demands. For settings requiring global context, we present a hybrid Slepian-Spherical Harmonic encoder that efficiently bridges the tradeoff between local-global performance, while retaining desirable properties such as pole-safety and spherical-surface-distance preservation. Across five tasks spanning classification, regression, and image-augmented prediction, Slepian encodings outperform baselines and retain performance advantages across a wide range of neural network architectures.", "AI": {"tldr": "A geographic location encoder using spherical Slepian functions that concentrates capacity in regions of interest, scales efficiently to high resolutions, and outperforms existing methods across multiple tasks and architectures.", "motivation": "Current machine learning models distribute representational capacity uniformly across the globe, but geographic data is fundamentally local (disease outbreaks, ecological patterns, economic activity cluster in specific regions). This mismatch causes models to struggle with fine-grained local resolutions needed for specialized applications.", "method": "Proposes a geographic location encoder built from spherical Slepian functions that concentrate representational capacity inside a region-of-interest and scale to high resolutions efficiently. For tasks requiring global context, introduces a hybrid Slepian-Spherical Harmonic encoder that balances local and global performance while maintaining pole-safety and spherical-surface-distance preservation.", "result": "The Slepian encodings outperform baseline methods across five diverse tasks (classification, regression, image-augmented prediction) and maintain performance advantages across a wide range of neural network architectures.", "conclusion": "Spherical Slepian functions provide an effective solution for geographic location encoding by concentrating representational power where needed, enabling efficient high-resolution modeling, and offering flexible local-global tradeoffs through hybrid approaches."}}
{"id": "2602.01109", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01109", "abs": "https://arxiv.org/abs/2602.01109", "authors": ["Hugo Math", "Rainer Lienhart"], "title": "Transforming Vehicle Diagnostics: A Multimodal Approach to Error Patterns Prediction", "comment": "9 pages, 7 figures", "summary": "Accurately diagnosing and predicting vehicle malfunctions is crucial for maintenance and safety in the automotive industry. While modern diagnostic systems primarily rely on sequences of vehicular Diagnostic Trouble Codes (DTCs) registered in On-Board Diagnostic (OBD) systems, they often overlook valuable contextual information such as raw sensory data (e.g., temperature, humidity, and pressure). This contextual data, crucial for domain experts to classify vehicle failures, introduces unique challenges due to its complexity and the noisy nature of real-world data. This paper presents BiCarFormer: the first multimodal approach to multi-label sequence classification of error codes into error patterns that integrates DTC sequences and environmental conditions. BiCarFormer is a bidirectional Transformer model tailored for vehicle event sequences, employing embedding fusions and a co-attention mechanism to capture the relationships between diagnostic codes and environmental data. Experimental results on a challenging real-world automotive dataset with 22,137 error codes and 360 error patterns demonstrate that our approach significantly improves classification performance compared to models that rely solely on DTC sequences and traditional sequence models. This work highlights the importance of incorporating contextual environmental information for more accurate and robust vehicle diagnostics, hence reducing maintenance costs and enhancing automation processes in the automotive industry.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faBiCarFormer\uff0c\u4e00\u79cd\u65b0\u578b\u53cc\u5411Transformer\u6a21\u578b\uff0c\u901a\u8fc7\u878d\u5408\u8bca\u65ad\u6545\u969c\u7801(DTC)\u4e0e\u73af\u5883\u4f20\u611f\u5668\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f66\u8f86\u6545\u969c\u6a21\u5f0f\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8f66\u8f86\u8bca\u65ad\u7cfb\u7edf\u4ec5\u4f9d\u8d56DTC\u5e8f\u5217\u800c\u5ffd\u7565\u6e29\u5ea6\u3001\u6e7f\u5ea6\u3001\u538b\u529b\u7b49\u73af\u5883\u6570\u636e\uff0c\u9650\u5236\u4e86\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u800c\u9886\u57df\u4e13\u5bb6\u4f9d\u8d56\u8fd9\u4e9b\u590d\u6742\u4e14\u5608\u6742\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u8fdb\u884c\u6545\u969c\u5206\u7c7b\u3002", "method": "\u63d0\u51faBiCarFormer\u2014\u2014\u9996\u4e2a\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u91c7\u7528\u5d4c\u5165\u878d\u5408\u548c\u534f\u540c\u6ce8\u610f\u529b\u673a\u5236\u7684\u53cc\u5411Transformer\uff0c\u8054\u5408\u5efa\u6a21DTC\u5e8f\u5217\u4e0e\u73af\u5883\u6761\u4ef6\u8fdb\u884c\u591a\u6807\u7b7e\u9519\u8bef\u6a21\u5f0f\u5206\u7c7b\u3002", "result": "\u5728\u5305\u542b22,137\u4e2a\u9519\u8bef\u4ee3\u7801\u548c360\u79cd\u9519\u8bef\u6a21\u5f0f\u7684\u771f\u5b9e\u6c7d\u8f66\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u4ec5\u4f7f\u7528DTC\u5e8f\u5217\u7684\u4f20\u7edf\u5e8f\u5217\u6a21\u578b\u3002", "conclusion": "\u878d\u5165\u73af\u5883\u4e0a\u4e0b\u6587\u4fe1\u606f\u5bf9\u7cbe\u786e\u8f66\u8f86\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u53ef\u964d\u4f4e\u7ef4\u62a4\u6210\u672c\u5e76\u63d0\u5347\u6c7d\u8f66\u884c\u4e1a\u81ea\u52a8\u5316\u6c34\u5e73\u3002"}}
{"id": "2602.02466", "categories": ["quant-ph", "math-ph"], "pdf": "https://arxiv.org/pdf/2602.02466", "abs": "https://arxiv.org/abs/2602.02466", "authors": ["Oliwier Urba\u0144ski"], "title": "Resolving problems with the continuum limit in coherent-state path integrals", "comment": null, "summary": "The paper solves the problem of continuum limit in bosonic thermal coherent-state path integrals. For this purpose, exact discrete versions of the path integral are constructed for three different orderings of the Hamiltonian: normal, anti-normal and symmetric (Weyl order). Subsequently, their different continuum versions are checked on the harmonic oscillator, to choose the symmetric ordering as a possibly correct choice for all Hamiltonians. Spotted mathematical subtleties in the simple case serve as a clue to the general solution. Finally, a general justification for the symmetric order is provided by deriving the continuum path integral starting from the exact discrete case by a renormalization procedure in the imaginary time frequency domain. While the role of Weyl order has already been found, the paper provides the missing proof of its suitability for every Hamiltonian and simplifies the previously established construction by referring only to creation and annihilation operators (without position and momentum operators).", "AI": {"tldr": "\u8be5\u8bba\u6587\u89e3\u51b3\u4e86\u73bb\u8272\u5b50\u70ed\u76f8\u5e72\u6001\u8def\u5f84\u79ef\u5206\u4e2d\u7684\u8fde\u7eed\u6781\u9650\u95ee\u9898\uff0c\u901a\u8fc7\u79bb\u6563\u8def\u5f84\u79ef\u5206\u6784\u9020\u548c\u8c10\u632f\u5b50\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5bf9\u79f0\uff08Weyl\uff09\u6392\u5e8f\u5bf9\u6240\u6709\u54c8\u5bc6\u987f\u91cf\u7684\u666e\u9002\u6027\u3002", "motivation": "\u89e3\u51b3\u73bb\u8272\u5b50\u70ed\u76f8\u5e72\u6001\u8def\u5f84\u79ef\u5206\u5728\u8fde\u7eed\u6781\u9650\u4e0b\u7684\u7406\u8bba\u7f3a\u9677\uff0c\u586b\u8865\u6b64\u524d\u7f3a\u4e4f\u5bf9\u6240\u6709\u54c8\u5bc6\u987f\u91cf\u666e\u9002\u6027\u8bc1\u660e\u7684\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u4e09\u79cd\u54c8\u5bc6\u987f\u91cf\u6392\u5e8f\uff08\u6b63\u89c4\u3001\u53cd\u6b63\u89c4\u3001\u5bf9\u79f0\uff09\u7684\u7cbe\u786e\u79bb\u6563\u8def\u5f84\u79ef\u5206\uff0c\u901a\u8fc7\u8c10\u632f\u5b50\u6a21\u578b\u9a8c\u8bc1\u4e0d\u540c\u8fde\u7eed\u7248\u672c\uff0c\u6700\u7ec8\u91c7\u7528\u865a\u65f6\u9891\u57df\u91cd\u6574\u5316\u65b9\u6cd5\u4ece\u79bb\u6563\u60c5\u5f62\u63a8\u5bfc\u8fde\u7eed\u8def\u5f84\u79ef\u5206\u3002", "result": "\u5bf9\u79f0\uff08Weyl\uff09\u6392\u5e8f\u88ab\u786e\u8ba4\u4e3a\u6240\u6709\u54c8\u5bc6\u987f\u91cf\u7684\u6b63\u786e\u9009\u62e9\uff0c\u6570\u5b66\u7ec6\u8282\u63ed\u793a\u5176\u666e\u9002\u6027\uff0c\u4e14\u6784\u9020\u7b80\u5316\u4e3a\u4ec5\u9700\u4ea7\u751f\u548c\u6e6e\u706d\u7b97\u7b26\uff08\u65e0\u9700\u4f4d\u7f6e/\u52a8\u91cf\u7b97\u7b26\uff09\u3002", "conclusion": "\u9996\u6b21\u4e3aWeyl\u6392\u5e8f\u63d0\u4f9b\u4e86\u9002\u7528\u4e8e\u4efb\u610f\u54c8\u5bc6\u987f\u91cf\u7684\u4e25\u683c\u8bc1\u660e\uff0c\u7b80\u5316\u4e86\u539f\u6709\u7406\u8bba\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u8fde\u7eed\u6781\u9650\u95ee\u9898\u7684\u6838\u5fc3\u4e89\u8bae\u3002"}}
{"id": "2602.01146", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01146", "abs": "https://arxiv.org/abs/2602.01146", "authors": ["Sidharth Pulipaka", "Oliver Chen", "Manas Sharma", "Taaha S Bajwa", "Vyas Raina", "Ivaxi Sheth"], "title": "PersistBench: When Should Long-Term Memories Be Forgotten by LLMs?", "comment": "70 pages, 26 figures, under review", "summary": "Conversational assistants are increasingly integrating long-term memory with large language models (LLMs). This persistence of memories, e.g., the user is vegetarian, can enhance personalization in future conversations. However, the same persistence can also introduce safety risks that have been largely overlooked. Hence, we introduce PersistBench to measure the extent of these safety risks. We identify two long-term memory-specific risks: cross-domain leakage, where LLMs inappropriately inject context from the long-term memories; and memory-induced sycophancy, where stored long-term memories insidiously reinforce user biases. We evaluate 18 frontier and open-source LLMs on our benchmark. Our results reveal a surprisingly high failure rate across these LLMs - a median failure rate of 53% on cross-domain samples and 97% on sycophancy samples. To address this, our benchmark encourages the development of more robust and safer long-term memory usage in frontier conversational systems.", "AI": {"tldr": "\u65b0\u57fa\u51c6\u6d4b\u8bd5PersistBench\u63ed\u793a\u5927\u591a\u6570\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u671f\u8bb0\u5fc6\u5904\u7406\u4e2d\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u9690\u60a3\uff0c53%\u8de8\u57df\u6cc4\u6f0f\u548c97%\u8bf1\u5bfc\u6027\u987a\u4ece\u5931\u8d25\u7387", "motivation": "\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u957f\u671f\u8bb0\u5fc6\u673a\u5236\u5e26\u6765\u7684\u65b0\u578b\u5b89\u5168\u98ce\u9669\uff0c\u9700\u91cf\u5316\u8bc4\u4f30\u6a21\u578b\u5728\u8de8\u57df\u4fe1\u606f\u6cc4\u6f0f\u548c\u5f3a\u5316\u7528\u6237\u504f\u89c1\u65b9\u9762\u7684\u8106\u5f31\u6027", "method": "\u6784\u5efaPersistBench\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u9488\u5bf918\u4e2a\u524d\u6cbf\u53ca\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u7cfb\u7edf\u8bc4\u4f30\u8de8\u57df\u6cc4\u6f0f\uff08\u4e0d\u5f53\u6ce8\u5165\u8bb0\u5fc6\u4e0a\u4e0b\u6587\uff09\u548c\u8bb0\u5fc6\u8bf1\u5bfc\u987a\u4ece\uff08\u5f3a\u5316\u7528\u6237\u504f\u89c1\uff09\u4e24\u7c7b\u98ce\u9669", "result": "\u6d4b\u8bd5\u663e\u793a\u6a21\u578b\u8868\u73b0\u582a\u5fe7\uff1a\u8de8\u57df\u6cc4\u6f0f\u6837\u672c\u4e2d\u4f4d\u6570\u5931\u8d25\u7387\u8fbe53%\uff0c\u8bb0\u5fc6\u8bf1\u5bfc\u987a\u4ece\u6837\u672c\u5931\u8d25\u7387\u66f4\u9ad8\u8fbe97%\uff0c\u66b4\u9732\u666e\u904d\u5b89\u5168\u7f3a\u9677", "conclusion": "\u8be5\u57fa\u51c6\u4e3a\u5f00\u53d1\u66f4\u5b89\u5168\u9c81\u68d2\u7684\u957f\u671f\u8bb0\u5fc6\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u5173\u952e\u8bc4\u4f30\u5de5\u5177\uff0c\u63a8\u52a8\u884c\u4e1a\u91cd\u89c6\u5e76\u89e3\u51b3\u8bb0\u5fc6\u673a\u5236\u5f15\u53d1\u7684\u5b89\u5168\u9690\u60a3"}}
{"id": "2602.01155", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.01155", "abs": "https://arxiv.org/abs/2602.01155", "authors": ["Hugo Math", "Julian Lorentz", "Stefan Oelsner", "Rainer Lienhart"], "title": "Multi-Agent Causal Reasoning System for Error Pattern Rule Automation in Vehicles", "comment": "7 pages, 3 figures", "summary": "Modern vehicles generate thousands of different discrete events known as Diagnostic Trouble Codes (DTCs). Automotive manufacturers use Boolean combinations of these codes, called error patterns (EPs), to characterize system faults and ensure vehicle safety. Yet, EP rules are still manually handcrafted by domain experts, a process that is expensive and prone to errors as vehicle complexity grows. This paper introduces CAREP (Causal Automated Reasoning for Error Patterns), a multi-agent system that automatizes the generation of EP rules from high-dimensional event sequences of DTCs. CAREP combines a causal discovery agent that identifies potential DTC-EP relations, a contextual information agent that integrates metadata and descriptions, and an orchestrator agent that synthesizes candidate boolean rules together with interpretable reasoning traces. Evaluation on a large-scale automotive dataset with over 29,100 unique DTCs and 474 error patterns demonstrates that CAREP can automatically and accurately discover the unknown EP rules, outperforming LLM-only baselines while providing transparent causal explanations. By uniting practical causal discovery and agent-based reasoning, CAREP represents a step toward fully automated fault diagnostics, enabling scalable, interpretable, and cost-efficient vehicle maintenance.", "AI": {"tldr": "CAREP\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u56e0\u679c\u53d1\u73b0\u81ea\u52a8\u4ece\u8f66\u8f86\u6545\u969c\u4ee3\u7801\u751f\u6210\u8bca\u65ad\u9519\u8bef\u6a21\u5f0f\u89c4\u5219\uff0c\u6027\u80fd\u4f18\u4e8eLLM\u57fa\u7ebf\u4e14\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u8f66\u8f86\u6545\u969c\u4ee3\u7801(DTCs)\u6570\u91cf\u5e9e\u5927\uff0c\u73b0\u6709\u9519\u8bef\u6a21\u5f0f(EP)\u89c4\u5219\u4f9d\u8d56\u4e13\u5bb6\u624b\u5de5\u5236\u5b9a\uff0c\u6210\u672c\u9ad8\u4e14\u6613\u9519\u3002", "method": "\u91c7\u7528\u4e09\u667a\u80fd\u4f53\u67b6\u6784\uff1a\u56e0\u679c\u53d1\u73b0\u667a\u80fd\u4f53\u8bc6\u522bDTC-EP\u5173\u7cfb\uff0c\u4e0a\u4e0b\u6587\u667a\u80fd\u4f53\u6574\u5408\u5143\u6570\u636e\uff0c\u7f16\u6392\u667a\u80fd\u4f53\u751f\u6210\u5e03\u5c14\u89c4\u5219\u5e76\u8f93\u51fa\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u572829,100\u4e2aDTC\u548c474\u4e2aEP\u7684\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6709\u6548\uff0c\u51c6\u786e\u7387\u8d85\u8d8aLLM\u57fa\u7ebf\u5e76\u63d0\u4f9b\u900f\u660e\u56e0\u679c\u89e3\u91ca\u3002", "conclusion": "\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u4e14\u6210\u672c\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u6545\u969c\u8bca\u65ad\uff0c\u63a8\u52a8\u8f66\u8f86\u7ef4\u62a4\u667a\u80fd\u5316\u3002"}}
{"id": "2602.01167", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01167", "abs": "https://arxiv.org/abs/2602.01167", "authors": ["Zhiming Liu", "Yujie Wei", "Lei Feng", "Xiu Su", "Xiaobo Xia", "Weili Guan", "Zeke Xie", "Shuo Yang"], "title": "Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models", "comment": null, "summary": "Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks' performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL's accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available.", "AI": {"tldr": "\u53d1\u73b0\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5b58\u5728\u4efb\u52a1\u5e72\u6270\u5c42\uff0c\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684\u52a8\u6001\u5c42\u8df3\u8fc7\u65b9\u6cd5TaLo\uff0c\u5728\u63a8\u7406\u65f6\u7ed5\u8fc7\u5e72\u6270\u5c42\uff0c\u6027\u80fd\u6700\u9ad8\u63d0\u534716.6%\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9ed8\u8ba4\u4f7f\u7528\u6240\u6709\u5c42\u8fdb\u884c\u9884\u6d4b\uff0c\u4f46\u67d0\u4e9b\u5c42\u53ef\u80fd\u963b\u788d\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u5355\u5c42\u5bf9\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5c42\u5e72\u9884\u5b9e\u9a8c\u6d4b\u91cf\u7ed5\u8fc7\u6bcf\u5c42\u540e\u7684\u6027\u80fd\u53d8\u5316\uff0c\u63d0\u51fa\u4efb\u52a1-\u5c42\u4ea4\u4e92\u5411\u91cf\u91cf\u5316\u5c42\u5f71\u54cd\uff0c\u8bbe\u8ba1\u65e0\u9700\u8bad\u7ec3\u7684\u52a8\u6001\u5c42\u8df3\u8fc7\u65b9\u6cd5TaLo\u3002", "result": "\u8de8\u6a21\u578b\u548c\u4efb\u52a1\u9a8c\u8bc1\u6709\u6548\uff0c\u5982Qwen-VL\u5728ScienceQA\u7684Maps\u4efb\u52a1\u4e0a\u51c6\u786e\u7387\u63d0\u5347\u6700\u9ad8\u8fbe16.6%\u3002", "conclusion": "\u63ed\u793a\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6a21\u5757\u5316\u7279\u6027\uff0c\u63d0\u4f9b\u4e86\u5373\u63d2\u5373\u7528\u7684\u63a8\u7406\u65f6\u4f18\u5316\u673a\u5236\u3002"}}
{"id": "2602.00412", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00412", "abs": "https://arxiv.org/abs/2602.00412", "authors": ["Marcos L. P. Bueno", "Joaquin Vanschoren"], "title": "Robustness of AutoML on Dirty Categorical Data", "comment": null, "summary": "The goal of automated machine learning (AutoML) is to reduce trial and error when doing machine learning (ML). Although AutoML methods for classification are able to deal with data imperfections, such as outliers, multiple scales and missing data, their behavior is less known on dirty categorical datasets. These datasets often have several categorical features with high cardinality arising from issues such as lack of curation and automated collection. Recent research has shown that ML models can benefit from morphological encoders for dirty categorical data, leading to significantly superior predictive performance. However the effects of using such encoders in AutoML methods are not known at the moment. In this paper, we propose a pipeline that transforms categorical data into numerical data so that an AutoML can handle categorical data transformed by more advanced encoding schemes. We benchmark the current robustness of AutoML methods on a set of dirty datasets and compare it with the proposed pipeline. This allows us to get insight on differences in predictive performance. We also look at the ML pipelines built by AutoMLs in order to gain insight beyond the best model as typically returned by these methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5c06\u810f\u5206\u7c7b\u6570\u636e\u901a\u8fc7\u5f62\u6001\u5b66\u7f16\u7801\u5668\u8f6c\u6362\u4e3a\u6570\u503c\u6570\u636e\u7684\u7ba1\u9053\uff0c\u4ee5\u589e\u5f3aAutoML\u5bf9\u9ad8\u57fa\u6570\u3001\u672a\u6e05\u6d17\u5206\u7c7b\u6570\u636e\u7684\u5904\u7406\u80fd\u529b\uff0c\u5e76\u7cfb\u7edf\u8bc4\u4f30AutoML\u65b9\u6cd5\u5728\u6b64\u7c7b\u6570\u636e\u4e0a\u7684\u9c81\u68d2\u6027", "motivation": "\u73b0\u6709AutoML\u65b9\u6cd5\u5bf9\u5206\u7c7b\u6570\u636e\u7684\u5904\u7406\u4e3b\u8981\u5173\u6ce8\u6570\u503c\u578b\u6570\u636e\u7f3a\u9677\uff08\u5982\u5f02\u5e38\u503c\u3001\u7f3a\u5931\u503c\uff09\uff0c\u4f46\u5bf9\u810f\u5206\u7c7b\u6570\u636e\uff08\u9ad8\u57fa\u6570\u3001\u7f3a\u4e4f\u6e05\u6d17\uff09\u7684\u884c\u4e3a\u7814\u7a76\u4e0d\u8db3\uff1b\u5f62\u6001\u5b66\u7f16\u7801\u5668\u867d\u80fd\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u5176\u5728AutoML\u4e2d\u7684\u5e94\u7528\u6548\u679c\u672a\u77e5", "method": "1. \u8bbe\u8ba1\u6570\u636e\u8f6c\u6362\u7ba1\u9053\uff1a\u5c06\u5206\u7c7b\u6570\u636e\u901a\u8fc7\u5148\u8fdb\u7f16\u7801\u65b9\u6848\uff08\u5f62\u6001\u5b66\u7f16\u7801\u5668\uff09\u8f6c\u4e3a\u6570\u503c\u6570\u636e\u4f9bAutoML\u4f7f\u7528\uff1b2. \u5728\u810f\u6570\u636e\u96c6\u4e0a\u57fa\u51c6\u6d4b\u8bd5AutoML\u7684\u9c81\u68d2\u6027\uff1b3. \u5bf9\u6bd4\u5206\u6790AutoML\u6784\u5efa\u7684\u5b8c\u6574ML\u7ba1\u9053\uff08\u8d85\u8d8a\u4ec5\u5173\u6ce8\u6700\u4f73\u6a21\u578b\u7684\u4f20\u7edf\u65b9\u5f0f\uff09", "result": "\uff08\u6458\u8981\u672a\u660e\u786e\u7ed9\u51fa\u5177\u4f53\u7ed3\u679c\uff0c\u4f46\u6307\u51fa\u7814\u7a76\u76ee\u6807\uff1a\u901a\u8fc7\u5bf9\u6bd4\u83b7\u5f97\u9884\u6d4b\u6027\u80fd\u5dee\u5f02\u7684\u6d1e\u5bdf\uff0c\u5e76\u901a\u8fc7\u5206\u6790AutoML\u6784\u5efa\u7684\u7ba1\u9053\u83b7\u5f97\u8d85\u8d8a\u5355\u4e00\u6700\u4f73\u6a21\u578b\u7684\u6df1\u5c42\u7406\u89e3\uff09", "conclusion": "\u6240\u63d0\u7ba1\u9053\u4e3aAutoML\u5904\u7406\u810f\u5206\u7c7b\u6570\u636e\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff1b\u7cfb\u7edf\u8bc4\u4f30\u63ed\u793a\u4e86AutoML\u5728\u6b64\u7c7b\u6570\u636e\u4e0a\u7684\u884c\u4e3a\u7279\u6027\uff0c\u4e3a\u672a\u6765\u4f18\u5316AutoML\u5904\u7406\u810f\u5206\u7c7b\u6570\u636e\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u65b9\u5411"}}
{"id": "2602.01171", "categories": ["cs.AI", "cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2602.01171", "abs": "https://arxiv.org/abs/2602.01171", "authors": ["Stefan Szeider"], "title": "ASP-Bench: From Natural Language to Logic Programs", "comment": null, "summary": "Automating the translation of natural-language specifications into logic programs is a challenging task that affects neurosymbolic engineering. We present ASP-Bench, a benchmark comprising 128 natural language problem instances, 64 base problems with easy and hard variants. It evaluates systems that translate natural-language problems into Answer Set Programs (ASPs), a prominent form of logic programming. It provides systematic coverage of ASP features, including choice rules, aggregates, and optimization. Each problem includes reference validators that check whether solutions satisfy the problem specification.\n  We characterize problems along seven largely independent reasoning aspects (optimization, temporal reasoning, default logic, resource allocation, recursion, spatial reasoning, and quantitative complexity), providing a multidimensional view of modeling difficulty.\n  We test the benchmark using an agentic approach based on the ReAct (Reason and Act) framework, which achieves full saturation, demonstrating that feedback-driven iterative refinement with solver feedback provides a reliable and robust approach for modeling natural language in ASP. Our analysis across multiple agent runs enables us to gain insights into what determines a problem's modeling hardness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faASP-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b128\u4e2a\u81ea\u7136\u8bed\u8a00\u5230\u7b54\u6848\u96c6\u7a0b\u5e8f\uff08ASP\uff09\u7684\u8f6c\u6362\u95ee\u9898\uff0c\u6db5\u76d664\u4e2a\u57fa\u7840\u95ee\u9898\u53ca\u5176\u96be\u6613\u53d8\u4f53\u3002\u901a\u8fc7ReAct\u667a\u80fd\u4f53\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5b8c\u5168\u9971\u548c\uff0c\u8bc1\u5b9e\u4e86\u57fa\u4e8e\u6c42\u89e3\u5668\u53cd\u9988\u7684\u8fed\u4ee3\u7cbe\u5316\u65b9\u6cd5\u5728\u81ea\u7136\u8bed\u8a00\u903b\u8f91\u5efa\u6a21\u4e2d\u7684\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u4ece\u4e03\u4e2a\u63a8\u7406\u7ef4\u5ea6\u63ed\u793a\u4e86\u5efa\u6a21\u96be\u5ea6\u7684\u51b3\u5b9a\u56e0\u7d20\u3002", "motivation": "\u5c06\u81ea\u7136\u8bed\u8a00\u89c4\u8303\u81ea\u52a8\u8f6c\u6362\u4e3a\u903b\u8f91\u7a0b\u5e8f\u662f\u795e\u7ecf\u7b26\u53f7\u5de5\u7a0b\u4e2d\u7684\u5173\u952e\u96be\u9898\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u8bc4\u4f30\u57fa\u51c6\u4e25\u91cd\u5236\u7ea6\u4e86\u76f8\u5173\u6280\u672f\u7684\u53d1\u5c55\u548c\u5e94\u7528\u3002", "method": "\u6784\u5efa\u4e86ASP-Bench\u57fa\u51c6\uff0c\u5305\u542b128\u4e2a\u81ea\u7136\u8bed\u8a00\u5230\u7b54\u6848\u96c6\u7a0b\u5e8f\u7684\u8f6c\u6362\u5b9e\u4f8b\uff0c\u7cfb\u7edf\u8986\u76d6\u9009\u62e9\u89c4\u5219\u3001\u805a\u5408\u548c\u4f18\u5316\u7b49\u6838\u5fc3ASP\u7279\u6027\uff1b\u63d0\u4f9b\u53c2\u8003\u9a8c\u8bc1\u5668\u786e\u4fdd\u89e3\u7684\u6b63\u786e\u6027\uff1b\u4ece\u4f18\u5316\u3001\u65f6\u5e8f\u63a8\u7406\u3001\u9ed8\u8ba4\u903b\u8f91\u3001\u8d44\u6e90\u5206\u914d\u3001\u9012\u5f52\u3001\u7a7a\u95f4\u63a8\u7406\u548c\u91cf\u5316\u590d\u6742\u5ea6\u4e03\u4e2a\u7ef4\u5ea6\u5bf9\u95ee\u9898\u8fdb\u884c\u591a\u7ef4\u5ea6\u96be\u5ea6\u523b\u753b\uff1b\u91c7\u7528\u57fa\u4e8eReAct\u6846\u67b6\u7684\u667a\u80fd\u4f53\u65b9\u6cd5\u8fdb\u884c\u6d4b\u8bd5\u9a8c\u8bc1\u3002", "result": "ReAct\u667a\u80fd\u4f53\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5b8c\u5168\u9971\u548c\uff0c\u8bc1\u660e\u53cd\u9988\u9a71\u52a8\u7684\u8fed\u4ee3\u7cbe\u5316\u7ed3\u5408\u6c42\u89e3\u5668\u53cd\u9988\u662f\u4e00\u79cd\u53ef\u9760\u4e14\u9c81\u68d2\u7684\u5efa\u6a21\u65b9\u6cd5\uff1b\u901a\u8fc7\u591a\u8f6e\u8fd0\u884c\u5206\u6790\uff0c\u6df1\u5165\u63ed\u793a\u4e86\u5f71\u54cd\u95ee\u9898\u5efa\u6a21\u96be\u5ea6\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "ASP-Bench\u4e3a\u81ea\u7136\u8bed\u8a00\u5230\u903b\u8f91\u7a0b\u5e8f\u7684\u81ea\u52a8\u8f6c\u6362\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\uff1b\u667a\u80fd\u4f53\u9a71\u52a8\u7684\u65b9\u6cd5\u5c55\u793a\u4e86\u81ea\u52a8\u5316\u5efa\u6a21\u7684\u6f5c\u529b\uff0c\u4e3a\u795e\u7ecf\u7b26\u53f7\u5de5\u7a0b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u548c\u53c2\u8003\u4f9d\u636e\u3002"}}
{"id": "2602.00423", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00423", "abs": "https://arxiv.org/abs/2602.00423", "authors": ["Quang-Huy Nguyen", "Zongliang Yue", "Hao Chen", "Wei-Shinn Ku", "Jiaqi Wang"], "title": "Federated-inspired Single-cell Batch Integration in Latent Space", "comment": null, "summary": "Advances in single-cell RNA sequencing enable the rapid generation of massive, high-dimensional datasets, yet the accumulation of data across experiments introduces batch effects that obscure true biological signals. Existing batch correction approaches either insufficiently correct batch effects or require centralized retraining on the complete dataset, limiting their applicability in distributed and continually evolving single-cell data settings. We introduce scBatchProx, a post-hoc optimization method inspired by federated learning principles for refining cell-level embeddings produced by arbitrary upstream methods. Treating each batch as a client, scBatchProx learns batch-conditioned adapters under proximal regularization, correcting batch structure directly in latent space without requiring raw expression data or centralized optimization. The method is lightweight and deployable, optimizing batch-specific adapter parameters only. Extensive experiments show that scBatchProx consistently yields relative gains of approximately 3-8% in overall embedding quality, with batch correction and biological conservation improving in 90% and 85% of data-method pairs, respectively. We envision this work as a step toward the practical refinement of learned representations in dynamic single-cell data systems.", "AI": {"tldr": "This paper proposes scBatchProx, a federated learning-inspired post-hoc method that corrects batch effects in single-cell RNA sequencing data by learning batch-specific adapters in latent space without needing raw data or centralized retraining.", "motivation": "Single-cell RNA sequencing generates massive datasets with batch effects that obscure biological signals. Existing correction methods are either insufficient or require centralized retraining, limiting their use in distributed, evolving data settings.", "method": "scBatchProx treats each batch as a client in a federated learning framework, learning batch-conditioned adapters with proximal regularization to correct batch structure directly in latent space, requiring only optimized adapter parameters without raw expression data.", "result": "Experiments show scBatchProx achieves 3-8% relative gains in embedding quality, improving batch correction in 90% and biological conservation in 85% of data-method pairs tested.", "conclusion": "The method provides a lightweight, deployable solution for refining learned representations in dynamic single-cell data systems."}}
{"id": "2602.01198", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01198", "abs": "https://arxiv.org/abs/2602.01198", "authors": ["Liang Zhang", "Yu Zhao", "Longyue Wang", "Tianqi Shi", "Weihua Luo", "Kaifu Zhang", "Jinsong Su"], "title": "A State-Transition Framework for Efficient LLM Reasoning", "comment": "ICLR 2026", "summary": "While Long Chain-of-Thought (CoT) reasoning significantly improves Large Language Models (LLMs) performance on complex reasoning tasks, the substantial computational and memory costs of generating long CoT sequences limit their efficiency and practicality. Existing studies usually enhance the reasoning efficiency of LLMs by compressing CoT sequences. However, this approach conflicts with test-time scaling, limiting the reasoning capacity of LLMs. In this paper, we propose an efficient reasoning framework that models the reasoning process of LLMs as a state-transition process. Specifically, we first apply a linear attention mechanism to estimate the LLM's reasoning state, which records the historical reasoning information from previous reasoning steps. Then, based on the query prompt and the reasoning state, the LLM can efficiently perform the current reasoning step and update the state. With the linear attention, each token in the current reasoning step can directly retrieve relevant historical reasoning information from the reasoning state, without explicitly attending to tokens in previous reasoning steps. In this way, the computational complexity of attention is reduced from quadratic to linear, significantly improving the reasoning efficiency of LLMs. In addition, we propose a state-based reasoning strategy to mitigate the over-thinking issue caused by noisy reasoning steps. Extensive experiments across multiple datasets and model sizes demonstrate that our framework not only improves the reasoning efficiency of LLMs but also enhances their reasoning performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u72b6\u6001\u8f6c\u79fb\u7684\u63a8\u7406\u6846\u67b6\uff0c\u5229\u7528\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\u5c06LLM\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u4ece\u4e8c\u6b21\u964d\u81f3\u7ebf\u6027\uff0c\u5728\u63d0\u5347\u63a8\u7406\u6548\u7387\u7684\u540c\u65f6\u589e\u5f3a\u6027\u80fd\u3002", "motivation": "\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u867d\u80fd\u63d0\u5347LLM\u590d\u6742\u63a8\u7406\u4efb\u52a1\u6027\u80fd\uff0c\u4f46\u751f\u6210\u957f\u5e8f\u5217\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u9ad8\u6602\uff1b\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u53c8\u4e0e\u6d4b\u8bd5\u65f6\u6269\u5c55\u6027\u51b2\u7a81\uff0c\u4e9f\u9700\u9ad8\u6548\u4e14\u4e0d\u5f71\u54cd\u6269\u5c55\u6027\u7684\u63a8\u7406\u65b9\u6848\u3002", "method": "\u5c06\u63a8\u7406\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u72b6\u6001\u8f6c\u79fb\uff1a1) \u4f7f\u7528\u7ebf\u6027\u6ce8\u610f\u529b\u4f30\u8ba1\u5e76\u7ef4\u62a4\u8bb0\u5f55\u5386\u53f2\u4fe1\u606f\u7684\u63a8\u7406\u72b6\u6001\uff1b2) \u57fa\u4e8e\u67e5\u8be2\u548c\u72b6\u6001\u6267\u884c\u5f53\u524d\u63a8\u7406\u6b65\u5e76\u66f4\u65b0\u72b6\u6001\uff1b3) \u5f53\u524dtoken\u76f4\u63a5\u4ece\u72b6\u6001\u68c0\u7d22\u5386\u53f2\u4fe1\u606f\u800c\u975e\u663e\u5f0f\u5173\u6ce8\u4e4b\u524d\u6240\u6709token\uff1b4) \u63d0\u51fa\u57fa\u4e8e\u72b6\u6001\u7684\u63a8\u7406\u7b56\u7565\u7f13\u89e3\u566a\u58f0\u6b65\u9aa4\u5bfc\u81f4\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\u3002", "result": "\u6846\u67b6\u5c06\u6ce8\u610f\u529b\u8ba1\u7b97\u590d\u6742\u5ea6\u4ece\u4e8c\u6b21\u964d\u81f3\u7ebf\u6027\uff0c\u5927\u5e45\u63d0\u5347\u63a8\u7406\u6548\u7387\uff1b\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u89c4\u6a21\u4e0a\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u8fd8\u589e\u5f3a\u4e86\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "\u8be5\u72b6\u6001\u8f6c\u79fb\u6846\u67b6\u901a\u8fc7\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u7684\u6548\u7387\u74f6\u9888\uff0c\u5728\u4fdd\u6301\u6d4b\u8bd5\u65f6\u6269\u5c55\u6027\u7684\u540c\u65f6\u63d0\u5347\u6027\u80fd\uff0c\u4e3aLLM\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002"}}
{"id": "2602.00424", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2602.00424", "abs": "https://arxiv.org/abs/2602.00424", "authors": ["Philipp Hoellmer", "Stefano Martiniani"], "title": "Open Materials Generation with Inference-Time Reinforcement Learning", "comment": "16 pages, 8 figures, 1 table", "summary": "Continuous-time generative models for crystalline materials enable inverse materials design by learning to predict stable crystal structures, but incorporating explicit target properties into the generative process remains challenging. Policy-gradient reinforcement learning (RL) provides a principled mechanism for aligning generative models with downstream objectives but typically requires access to the score, which has prevented its application to flow-based models that learn only velocity fields. We introduce Open Materials Generation with Inference-time Reinforcement Learning (OMatG-IRL), a policy-gradient RL framework that operates directly on the learned velocity fields and eliminates the need for the explicit computation of the score. OMatG-IRL leverages stochastic perturbations of the underlying generation dynamics preserving the baseline performance of the pretrained generative model while enabling exploration and policy-gradient estimation at inference time. Using OMatG-IRL, we present the first application of RL to crystal structure prediction (CSP). Our method enables effective reinforcement of an energy-based objective while preserving diversity through composition conditioning, and it achieves performance competitive with score-based RL approaches. Finally, we show that OMatG-IRL can learn time-dependent velocity-annealing schedules, enabling accurate CSP with order-of-magnitude improvements in sampling efficiency and, correspondingly, reduction in generation time.", "AI": {"tldr": "OMatG-IRL\u662f\u4e00\u4e2a\u7b56\u7565\u68af\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u53ef\u76f4\u63a5\u5728\u6d41\u6a21\u578b\u5b66\u4e60\u7684\u6676\u4f53\u6750\u6599\u901f\u5ea6\u573a\u4e0a\u8fd0\u884c\uff0c\u65e0\u9700\u663e\u5f0f\u8ba1\u7b97\u5206\u6570\u5373\u53ef\u5b9e\u73b0\u9006\u5411\u6750\u6599\u8bbe\u8ba1\uff0c\u5728\u6676\u4f53\u7ed3\u6784\u9884\u6d4b\u4efb\u52a1\u4e0a\u8fbe\u5230\u7ade\u4e89\u6027\u80fd\uff0c\u5e76\u5b9e\u73b0\u91c7\u6837\u6548\u7387\u7684\u6570\u91cf\u7ea7\u63d0\u5347\u3002", "motivation": "\u6676\u4f53\u6750\u6599\u7684\u8fde\u7eed\u65f6\u95f4\u751f\u6210\u6a21\u578b\u867d\u652f\u6301\u9006\u5411\u8bbe\u8ba1\uff0c\u4f46\u96be\u4ee5\u5c06\u76ee\u6807\u5c5e\u6027\u663e\u5f0f\u878d\u5165\u751f\u6210\u8fc7\u7a0b\uff1b\u7b56\u7565\u68af\u5ea6\u5f3a\u5316\u5b66\u4e60\u867d\u63d0\u4f9b\u5bf9\u9f50\u751f\u6210\u6a21\u578b\u4e0e\u4e0b\u6e38\u76ee\u6807\u7684\u539f\u5219\u6027\u673a\u5236\uff0c\u4f46\u901a\u5e38\u9700\u8981\u5206\u6570\u4fe1\u606f\uff0c\u8fd9\u963b\u788d\u4e86\u5176\u5728\u4ec5\u5b66\u4e60\u901f\u5ea6\u573a\u7684\u6d41\u6a21\u578b\u4e0a\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faOMatG-IRL\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u751f\u6210\u52a8\u529b\u5b66\u65bd\u52a0\u968f\u673a\u6270\u52a8\uff0c\u76f4\u63a5\u5728\u9884\u8bad\u7ec3\u6a21\u578b\u7684velocity fields\u4e0a\u5b9e\u73b0\u63a8\u7406\u65f6\u7684\u63a2\u7d22\u4e0e\u7b56\u7565\u68af\u5ea6\u4f30\u8ba1\uff0c\u6d88\u9664\u5bf9\u663e\u5f0f\u5206\u6570\u8ba1\u7b97\u7684\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u57fa\u7ebf\u6027\u80fd\u3002", "result": "\u9996\u6b21\u5c06\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8e\u6676\u4f53\u7ed3\u6784\u9884\u6d4b(CSP)\uff0c\u6709\u6548\u5f3a\u5316\u80fd\u91cf\u76ee\u6807\u5e76\u901a\u8fc7\u7ec4\u5206\u6761\u4ef6\u4fdd\u6301\u591a\u6837\u6027\uff0c\u6027\u80fd\u4e0e\u57fa\u4e8e\u5206\u6570\u7684\u65b9\u6cd5\u76f8\u5f53\uff1b\u80fd\u5b66\u4e60\u65f6\u95f4\u76f8\u5173\u7684\u901f\u5ea6\u9000\u706b\u7b56\u7565\uff0c\u5b9e\u73b0\u91c7\u6837\u6548\u7387\u63d0\u5347\u4e00\u4e2a\u6570\u91cf\u7ea7\u5e76\u663e\u8457\u7f29\u77ed\u751f\u6210\u65f6\u95f4\u3002", "conclusion": "OMatG-IRL\u901a\u8fc7\u76f4\u63a5\u5728\u901f\u5ea6\u573a\u4e0a\u64cd\u4f5c\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5c5e\u6027\u9a71\u52a8\u6676\u4f53\u7ed3\u6784\u751f\u6210\uff0c\u514b\u670d\u4e86\u57fa\u4e8e\u5206\u6570\u65b9\u6cd5\u7684\u5c40\u9650\uff0c\u4e3a\u6750\u6599\u9006\u5411\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00426", "categories": ["cs.LG", "cs.AI", "cs.CL", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.00426", "abs": "https://arxiv.org/abs/2602.00426", "authors": ["Vikram Krishnamurthy"], "title": "LLMs as High-Dimensional Nonlinear Autoregressive Models with Attention: Training, Alignment and Inference", "comment": "27 pages, 12 figures. Mathematical survey framing LLMs as high-dimensional nonlinear autoregressive models with attention, covering training, alignment, and inference, with nanoGPT/nanochat-style code examples. Feedback welcome", "summary": "Large language models (LLMs) based on transformer architectures are typically described through collections of architectural components and training procedures, obscuring their underlying computational structure. This review article provides a concise mathematical reference for researchers seeking an explicit, equation-level description of LLM training, alignment, and generation. We formulate LLMs as high-dimensional nonlinear autoregressive models with attention-based dependencies. The framework encompasses pretraining via next-token prediction, alignment methods such as reinforcement learning from human feedback (RLHF), direct preference optimization (DPO), rejection sampling fine-tuning (RSFT), and reinforcement learning from verifiable rewards (RLVR), as well as autoregressive generation during inference. Self-attention emerges naturally as a repeated bilinear--softmax--linear composition, yielding highly expressive sequence models. This formulation enables principled analysis of alignment-induced behaviors (including sycophancy), inference-time phenomena (such as hallucination, in-context learning, chain-of-thought prompting, and retrieval-augmented generation), and extensions like continual learning, while serving as a concise reference for interpretation and further theoretical development.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u6570\u5b66\u6846\u67b6\uff0c\u5c06Transformer\u5927\u6a21\u578b\u63cf\u8ff0\u4e3a\u9ad8\u7ef4\u975e\u7ebf\u6027\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u6db5\u76d6\u9884\u8bad\u7ec3\u3001\u5bf9\u9f50\uff08RLHF/DPO\u7b49\uff09\u53ca\u63a8\u7406\u8fc7\u7a0b\uff0c\u63ed\u793a\u81ea\u6ce8\u610f\u529b\u673a\u5236\u672c\u8d28\u5e76\u5206\u6790\u5bf9\u9f50\u884c\u4e3a\u4e0e\u63a8\u7406\u73b0\u8c61", "motivation": "\u73b0\u6709\u5927\u6a21\u578b\u63cf\u8ff0\u4fa7\u91cd\u7ec4\u4ef6\u4e0e\u6d41\u7a0b\uff0c\u63a9\u76d6\u4e86\u5e95\u5c42\u8ba1\u7b97\u7ed3\u6784\uff0c\u7f3a\u4e4f\u663e\u5f0f\u65b9\u7a0b\u7ea7\u6570\u5b66\u8868\u8fbe\uff0c\u963b\u788d\u5bf9\u9f50\u884c\u4e3a\u4e0e\u63a8\u7406\u73b0\u8c61\u7684\u7406\u8bba\u5206\u6790", "method": "\u5c06\u5927\u6a21\u578b\u5f62\u5f0f\u5316\u4e3a\u9ad8\u7ef4\u975e\u7ebf\u6027\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u4f9d\u8d56\u5173\u7cfb\u7edf\u4e00\u63cf\u8ff0\uff1a\u9884\u8bad\u7ec3\uff08\u4e0b\u4e00\u8bcd\u5143\u9884\u6d4b\uff09\u3001\u5bf9\u9f50\u65b9\u6cd5\uff08RLHF/DPO/RSFT/RLVR\uff09\u53ca\u81ea\u56de\u5f52\u63a8\u7406\uff0c\u63ed\u793a\u81ea\u6ce8\u610f\u529b\u4e3a\u53cc\u7ebf\u6027-softmax-\u7ebf\u6027\u590d\u5408\u7ed3\u6784", "result": "\u6846\u67b6\u53ef\u5206\u6790\u5bf9\u9f50\u8bf1\u5bfc\u884c\u4e3a\uff08\u5982\u8fce\u5408\uff09\u3001\u63a8\u7406\u65f6\u73b0\u8c61\uff08\u5e7b\u89c9/\u4e0a\u4e0b\u6587\u5b66\u4e60/\u601d\u7ef4\u94fe/\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u53ca\u6301\u7eed\u5b66\u4e60\u7b49\u6269\u5c55\uff0c\u8bc1\u660e\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4ea7\u751f\u9ad8\u8868\u8fbe\u529b\u5e8f\u5217\u6a21\u578b", "conclusion": "\u8be5\u516c\u5f0f\u5316\u4e3a\u89e3\u91ca\u5927\u6a21\u578b\u884c\u4e3a\u4e0e\u7406\u8bba\u53d1\u5c55\u63d0\u4f9b\u7b80\u660e\u6570\u5b66\u53c2\u8003\uff0c\u652f\u6301\u5bf9\u5173\u952e\u73b0\u8c61\u7684\u673a\u7406\u5206\u6790\u4e0e\u672a\u6765\u7406\u8bba\u62d3\u5c55"}}
{"id": "2602.00446", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.00446", "abs": "https://arxiv.org/abs/2602.00446", "authors": ["Ziyao Wang", "Nizhang Li", "Pingzhi Li", "Guoheng Sun", "Tianlong Chen", "Ang Li"], "title": "Towards Building Non-Fine-Tunable Foundation Models", "comment": null, "summary": "Open-sourcing foundation models (FMs) enables broad reuse but also exposes model trainers to economic and safety risks from unrestricted downstream fine-tuning. We address this problem by building non-fine-tunable foundation models: models that remain broadly usable in their released form while yielding limited adaptation gains under task-agnostic unauthorized fine-tuning. We propose Private Mask Pre-Training (PMP), a pre-training framework that concentrates representation learning into a sparse subnetwork identified early in training. The binary mask defining this subnetwork is kept private, and only the final dense weights are released. This forces unauthorized fine-tuning without access to the mask to update parameters misaligned with pretraining subspace, inducing an intrinsic mismatch between the fine-tuning objective and the pre-training geometry. We provide theoretical analysis showing that this mismatch destabilizes gradient-based adaptation and bounds fine-tuning gains. Empirical results on large language models demonstrating that PMP preserves base model performance while consistently degrading unauthorized fine-tuning across a wide range of downstream tasks, with the strength of non-fine-tunability controlled by the mask ratio.", "AI": {"tldr": "The paper proposes Private Mask Pre-Training (PMP), a framework that creates non-fine-tunable foundation models by privately masking a sparse subnetwork during pre-training. This preserves model utility while significantly limiting unauthorized fine-tuning gains through an intrinsic mismatch between pre-training geometry and fine-tuning objectives.", "motivation": "Open-sourcing foundation models creates economic and safety risks because bad actors can unrestrictedly fine-tune them for malicious purposes. Current approaches don't effectively prevent this while maintaining model utility.", "method": "PMP framework that: (1) identifies a sparse subnetwork early in training, (2) keeps the binary mask private while releasing only dense weights, (3) forces unauthorized fine-tuning to update parameters misaligned with the pre-training subspace, creating geometric mismatch.", "result": "Theoretical analysis shows the method destabilizes gradient-based adaptation and bounds fine-tuning gains. Empirically, PMP preserves base model performance while consistently degrading unauthorized fine-tuning across diverse downstream tasks, with control via mask ratio.", "conclusion": "PMP successfully creates non-fine-tunable foundation models that balance broad usability with protection against unauthorized adaptation, offering a practical solution for responsible model sharing."}}
{"id": "2602.01207", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01207", "abs": "https://arxiv.org/abs/2602.01207", "authors": ["Hui Wu", "Hengyi Cai", "Jinman Zhao", "Xinran Chen", "Ziheng Li", "Zhejun Zhao", "Shuaiqiang Wang", "Yuchen Li", "Dawei Yin"], "title": "Not All Preferences Are Created Equal: Stability-Aware and Gradient-Efficient Alignment for Reasoning Models", "comment": null, "summary": "Preference-based alignment is pivotal for training large reasoning models; however, standard methods like Direct Preference Optimization (DPO) typically treat all preference pairs uniformly, overlooking the evolving utility of training instances. This static approach often leads to inefficient or unstable optimization, as it wastes computation on trivial pairs with negligible gradients and suffers from noise induced by samples near uncertain decision boundaries. Facing these challenges, we propose SAGE (Stability-Aware Gradient Efficiency), a dynamic framework designed to enhance alignment reliability by maximizing the Signal-to-Noise Ratio of policy updates. Concretely, SAGE integrates a coarse-grained curriculum mechanism that refreshes candidate pools based on model competence with a fine-grained, stability-aware scoring function that prioritizes informative, confident errors while filtering out unstable samples. Experiments on multiple mathematical reasoning benchmarks demonstrate that SAGE significantly accelerates convergence and outperforms static baselines, highlighting the critical role of policy-aware, stability-conscious data selection in reasoning alignment.", "AI": {"tldr": "\u63d0\u51faSAGE\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u7b5b\u9009\u4fe1\u606f\u91cf\u5927\u7684\u504f\u597d\u5bf9\u5e76\u8fc7\u6ee4\u566a\u58f0\u6837\u672c\uff0c\u63d0\u5347\u5927\u63a8\u7406\u6a21\u578b\u5bf9\u9f50\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387", "motivation": "\u6807\u51c6DPO\u7b49\u65b9\u6cd5\u5747\u5300\u5904\u7406\u6240\u6709\u504f\u597d\u5bf9\uff0c\u5bfc\u81f4\u5728\u7b80\u5355\u6837\u672c\u4e0a\u6d6a\u8d39\u8ba1\u7b97\uff0c\u4e14\u6613\u53d7\u51b3\u7b56\u8fb9\u754c\u9644\u8fd1\u566a\u58f0\u5e72\u6270\uff0c\u9020\u6210\u4f18\u5316\u4f4e\u6548\u4e0d\u7a33\u5b9a", "method": "SAGE\u91c7\u7528\u53cc\u5c42\u673a\u5236\uff1a\u7c97\u7c92\u5ea6\u8bfe\u7a0b\u5b66\u4e60\u6839\u636e\u6a21\u578b\u80fd\u529b\u5237\u65b0\u5019\u9009\u6c60\uff1b\u7ec6\u7c92\u5ea6\u7a33\u5b9a\u6027\u8bc4\u5206\u51fd\u6570\u4f18\u5148\u9009\u62e9\u4fe1\u606f\u4e30\u5bcc\u4e14\u7f6e\u4fe1\u5ea6\u9ad8\u7684\u9519\u8bef\u6837\u672c\uff0c\u8fc7\u6ee4\u4e0d\u7a33\u5b9a\u6837\u672c\uff0c\u6700\u5927\u5316\u7b56\u7565\u66f4\u65b0\u7684\u4fe1\u566a\u6bd4", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cSAGE\u663e\u8457\u52a0\u901f\u6536\u655b\u5e76\u8d85\u8d8a\u9759\u6001\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u9a8c\u8bc1\u4e86\u7b56\u7565\u611f\u77e5\u3001\u7a33\u5b9a\u6027\u4f18\u5148\u7684\u6570\u636e\u9009\u62e9\u5bf9\u63a8\u7406\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\uff0cSAGE\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u52a8\u6001\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.00451", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.00451", "abs": "https://arxiv.org/abs/2602.00451", "authors": ["Xiaoyu Wang", "Xiaotian Li", "Zhixiang Zhou", "Chen Li", "Yong Liu"], "title": "Stabilizing Decentralized Federated Fine-Tuning via Topology-Aware Alternating LoRA", "comment": "17 Pages", "summary": "Decentralized federated learning (DFL), a serverless variant of federated learning, poses unique challenges for parameter-efficient fine-tuning due to the factorized structure of low-rank adaptation (LoRA). Unlike linear parameters, decentralized aggregation of LoRA updates introduces topology-dependent cross terms that can destabilize training under dynamic communication graphs. We propose \\texttt{TAD-LoRA}, a Topology-Aware Decentralized Low-Rank Adaptation framework that coordinates the updates and mixing of LoRA factors to control inter-client misalignment. We theoretically prove the convergence of \\texttt{TAD-LoRA} under non-convex objectives, explicitly characterizing the trade-off between topology-induced cross-term error and block-coordinate representation bias governed by the switching interval of alternative training. Experiments under various communication conditions validate our analysis, showing that \\texttt{TAD-LoRA} achieves robust performance across different communication scenarios, remaining competitive in strongly connected topologies and delivering clear gains under moderately and weakly connected topologies, with particularly strong results on the MNLI dataset.", "AI": {"tldr": "This paper proposes TAD-LoRA, a topology-aware framework for decentralized federated learning that addresses LoRA aggregation challenges by coordinating updates to control inter-client misalignment, proving convergence theoretically and showing robust experimental performance across different network topologies.", "motivation": "The challenge of parameter-efficient fine-tuning in decentralized federated learning (DFL) using low-rank adaptation (LoRA), where decentralized aggregation of LoRA updates creates topology-dependent cross terms that destabilize training under dynamic communication graphs.", "method": "TAD-LoRA (Topology-Aware Decentralized Low-Rank Adaptation), which coordinates the updates and mixing of LoRA factors to control inter-client misalignment, with alternative training governed by a switching interval.", "result": "Theoretical convergence proven under non-convex objectives, characterizing trade-off between topology-induced cross-term error and block-coordinate representation bias; experiments show robust performance across communication scenarios, competitive in strongly connected topologies with clear gains in moderately/weakly connected ones, especially on MNLI dataset.", "conclusion": "TAD-LoRA effectively addresses the unique challenges of LoRA in DFL by coordinating factor updates, achieving both theoretical guarantees and practical performance improvements across diverse network topologies."}}
{"id": "2602.01222", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01222", "abs": "https://arxiv.org/abs/2602.01222", "authors": ["Shaoxiong Yang", "Junting Li", "Mengyuan Zhang", "Chao Li", "Wei Liu", "Jian Luan"], "title": "FutureMind: Equipping Small Language Models with Strategic Thinking-Pattern Priors via Adaptive Knowledge Distillation", "comment": "Accepted by ICLR 2026", "summary": "Small Language Models (SLMs) are attractive for cost-sensitive and resource-limited settings due to their efficient, low-latency inference. However, they often struggle with complex, knowledge-intensive tasks that require structured reasoning and effective retrieval. To address these limitations, we propose FutureMind, a modular reasoning framework that equips SLMs with strategic thinking-pattern priors via adaptive knowledge distillation from large language models (LLMs). FutureMind introduces a dynamic reasoning pipeline composed of four key modules: Problem Analysis, Logical Reasoning, Strategy Planning, and Retrieval Guidance. This pipeline is augmented by three distinct retrieval paradigms that decompose complex queries into tractable subproblems, ensuring efficient and accurate retrieval execution. Extensive experiments on multi-hop QA benchmarks, including 2WikiMultihopQA, MuSiQue, Bamboogle, and Frames, demonstrate the superiority of FutureMind. It consistently outperforms strong baselines such as Search-o1, achieving state-of-the-art results under free training conditions across diverse SLM architectures and scales. Beyond empirical gains, our analysis reveals that the process of thinking-pattern distillation is restricted by the cognitive bias bottleneck between the teacher (LLMs) and student (SLMs) models. This provides new perspectives on the transferability of reasoning skills, paving the way for the development of SLMs that combine efficiency with genuine cognitive capability.", "AI": {"tldr": "FutureMind\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u8fdb\u884c\u81ea\u9002\u5e94\u77e5\u8bc6\u84b8\u998f\uff0c\u4e3a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b(SLM)\u63d0\u4f9b\u6218\u7565\u6027\u601d\u7ef4\u6a21\u5f0f\u5148\u9a8c\uff0c\u5728\u4fdd\u6301\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u590d\u6742\u63a8\u7406\u548c\u68c0\u7d22\u80fd\u529b\u3002", "motivation": "\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u63a8\u7406\u9ad8\u6548\u3001\u5ef6\u8fdf\u4f4e\uff0c\u4f46\u5728\u5904\u7406\u9700\u8981\u7ed3\u6784\u5316\u63a8\u7406\u548c\u6709\u6548\u68c0\u7d22\u7684\u590d\u6742\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faFutureMind\u6846\u67b6\uff0c\u5305\u542b\u56db\u4e2a\u6838\u5fc3\u6a21\u5757\uff08\u95ee\u9898\u5206\u6790\u3001\u903b\u8f91\u63a8\u7406\u3001\u7b56\u7565\u89c4\u5212\u3001\u68c0\u7d22\u5f15\u5bfc\uff09\u548c\u4e09\u79cd\u68c0\u7d22\u8303\u5f0f\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u77e5\u8bc6\u84b8\u998f\u5c06LLM\u7684\u601d\u7ef4\u6a21\u5f0f\u8fc1\u79fb\u5230SLM\u3002", "result": "\u5728\u591a\u4e2a\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u6301\u7eed\u8d85\u8d8aSearch-o1\u7b49\u5f3a\u57fa\u7ebf\uff0c\u5728\u81ea\u7531\u8bad\u7ec3\u6761\u4ef6\u4e0b\u5b9e\u73b0\u8de8\u67b6\u6784\u548c\u89c4\u6a21\u7684SOTA\u7ed3\u679c\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u601d\u7ef4\u6a21\u5f0f\u84b8\u998f\u53d7\u9650\u4e8e\u5e08\u751f\u6a21\u578b\u95f4\u7684\u8ba4\u77e5\u504f\u5dee\u74f6\u9888\uff0c\u4e3a\u63a8\u7406\u6280\u80fd\u7684\u53ef\u8fc1\u79fb\u6027\u63d0\u4f9b\u65b0\u89c1\u89e3\uff0c\u63a8\u52a8\u9ad8\u6548\u4e14\u5177\u5907\u771f\u5b9e\u8ba4\u77e5\u80fd\u529b\u7684SLM\u53d1\u5c55\u3002"}}
{"id": "2602.01276", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01276", "abs": "https://arxiv.org/abs/2602.01276", "authors": ["Abdulsobur Oyewale", "Tommaso Soru"], "title": "LLM-Driven Ontology Construction for Enterprise Knowledge Graphs", "comment": "20th International Conference on Semantic Computing (ICSC 2026)", "summary": "Enterprise Knowledge Graphs have become essential for unifying heterogeneous data and enforcing semantic governance. However, the construction of their underlying ontologies remains a resource-intensive, manual process that relies heavily on domain expertise. This paper introduces OntoEKG, a LLM-driven pipeline designed to accelerate the generation of domain-specific ontologies from unstructured enterprise data. Our approach decomposes the modelling task into two distinct phases: an extraction module that identifies core classes and properties, and an entailment module that logically structures these elements into a hierarchy before serialising them into standard RDF. Addressing the significant lack of comprehensive benchmarks for end-to-end ontology construction, we adopt a new evaluation dataset derived from documents across the Data, Finance, and Logistics sectors. Experimental results highlight both the potential and the challenges of this approach, achieving a fuzzy-match F1-score of 0.724 in the Data domain while revealing limitations in scope definition and hierarchical reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faOntoEKG\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6d41\u6c34\u7ebf\uff0c\u7528\u4e8e\u4ece\u975e\u7ed3\u6784\u5316\u4f01\u4e1a\u6570\u636e\u4e2d\u81ea\u52a8\u751f\u6210\u9886\u57df\u672c\u4f53\u3002\u901a\u8fc7\u63d0\u53d6\u548c\u63a8\u7406\u4e24\u4e2a\u9636\u6bb5\u6784\u5efa\u672c\u4f53\u5c42\u6b21\u7ed3\u6784\uff0c\u5e76\u5728\u6570\u636e\u3001\u91d1\u878d\u548c\u7269\u6d41\u9886\u57df\u7684\u65b0\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5728\u6570\u636e\u9886\u57df\u8fbe\u52300.724\u7684\u6a21\u7cca\u5339\u914dF1\u5206\u6570\uff0c\u4f46\u5b58\u5728\u8303\u56f4\u5b9a\u4e49\u548c\u5c42\u6b21\u63a8\u7406\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f01\u4e1a\u77e5\u8bc6\u56fe\u8c31\u5bf9\u4e8e\u7edf\u4e00\u5f02\u6784\u6570\u636e\u548c\u5b9e\u65bd\u8bed\u4e49\u6cbb\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u5e95\u5c42\u672c\u4f53\u7684\u6784\u5efa\u4ecd\u7136\u662f\u4e00\u4e2a\u8d44\u6e90\u5bc6\u96c6\u3001\u624b\u52a8\u4f9d\u8d56\u9886\u57df\u4e13\u5bb6\u7684\u8fc7\u7a0b\uff0c\u7f3a\u4e4f\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e00\u4e2aLLM\u9a71\u52a8\u7684\u6d41\u6c34\u7ebfOntoEKG\uff0c\u5c06\u5efa\u6a21\u4efb\u52a1\u5206\u89e3\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u63d0\u53d6\u6a21\u5757\u8bc6\u522b\u6838\u5fc3\u7c7b\u548c\u5c5e\u6027\uff0c\u63a8\u7406\u6a21\u5757\u5c06\u5143\u7d20\u903b\u8f91\u7ed3\u6784\u5316\u4e3a\u672c\u4f53\u5c42\u6b21\uff0c\u6700\u540e\u5e8f\u5217\u5316\u4e3a\u6807\u51c6RDF\u683c\u5f0f\u3002", "result": "\u5728\u6570\u636e\u3001\u91d1\u878d\u548c\u7269\u6d41\u9886\u57df\u7684\u65b0\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5728\u6570\u636e\u9886\u57df\u8fbe\u52300.724\u7684\u6a21\u7cca\u5339\u914dF1\u5206\u6570\uff0c\u4f46\u63ed\u793a\u4e86\u8303\u56f4\u5b9a\u4e49\u548c\u5c42\u6b21\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86LLM\u9a71\u52a8\u672c\u4f53\u6784\u5efa\u7684\u6f5c\u529b\uff0c\u4f46\u4e5f\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u8303\u56f4\u5b9a\u4e49\u548c\u5c42\u6b21\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2602.00460", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00460", "abs": "https://arxiv.org/abs/2602.00460", "authors": ["Georgios Sotirchos", "Zlatan Ajanovi\u0107", "Jens Kober"], "title": "Search Inspired Exploration in Reinforcement Learning", "comment": null, "summary": "Exploration in environments with sparse rewards remains a fundamental challenge in reinforcement learning (RL). Existing approaches such as curriculum learning and Go-Explore often rely on hand-crafted heuristics, while curiosity-driven methods risk converging to suboptimal policies. We propose Search-Inspired Exploration in Reinforcement Learning (SIERL), a novel method that actively guides exploration by setting sub-goals based on the agent's learning progress. At the beginning of each episode, SIERL chooses a sub-goal from the \\textit{frontier} (the boundary of the agent's known state space), before the agent continues exploring toward the main task objective. The key contribution of our method is the sub-goal selection mechanism, which provides state-action pairs that are neither overly familiar nor completely novel. Thus, it assures that the frontier is expanded systematically and that the agent is capable of reaching any state within it. Inspired by search, sub-goals are prioritized from the frontier based on estimates of cost-to-come and cost-to-go, effectively steering exploration towards the most informative regions. In experiments on challenging sparse-reward environments, SIERL outperforms dominant baselines in both achieving the main task goal and generalizing to reach arbitrary states in the environment.", "AI": {"tldr": "SIERL\u662f\u4e00\u79cd\u65b0\u578b\u5f3a\u5316\u5b66\u4e60\u63a2\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u524d\u6cbf\u5b50\u76ee\u6807\u9009\u62e9\u548c\u4ee3\u4ef7\u4f30\u8ba1\uff0c\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u7cfb\u7edf\u6027\u6269\u5c55\u72b6\u6001\u7a7a\u95f4\u5e76\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e0b\u7684\u63a2\u7d22\u662f\u5f3a\u5316\u5b66\u4e60\u6838\u5fc3\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u6216\u4f9d\u8d56\u624b\u5de5\u8bbe\u8ba1\uff0c\u6216\u6613\u9677\u5165\u6b21\u4f18\u7b56\u7565\u3002", "method": "\u63d0\u51faSIERL\uff0c\u6bcf\u56de\u5408\u4ece\u524d\u6cbf\uff08\u5df2\u77e5\u72b6\u6001\u8fb9\u754c\uff09\u9009\u5b50\u76ee\u6807\u5f15\u5bfc\u63a2\u7d22\u3002\u5b50\u76ee\u6807\u9009\u62e9\u5e73\u8861\u719f\u6089\u5ea6\u4e0e\u65b0\u9896\u6027\uff0c\u57fa\u4e8e\u4ee3\u4ef7-\u6765\u548c\u4ee3\u4ef7-\u53bb\u4f30\u8ba1\u6392\u5e8f\uff0c\u7cfb\u7edf\u6027\u6269\u5c55\u524d\u6cbf\u3002", "result": "\u5728\u6311\u6218\u6027\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\uff0cSIERL\u5728\u8fbe\u6210\u4e3b\u4efb\u52a1\u548c\u6cdb\u5316\u5230\u4efb\u610f\u72b6\u6001\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u4e3b\u6d41\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5f15\u5bfc\u63a2\u7d22\uff0c\u7cfb\u7edf\u6027\u6269\u5c55\u5df2\u77e5\u72b6\u6001\u7a7a\u95f4\uff0c\u4e3a\u7a00\u758f\u5956\u52b1\u95ee\u9898\u63d0\u4f9b\u66f4\u4f18\u89e3\u3002"}}
{"id": "2602.01297", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01297", "abs": "https://arxiv.org/abs/2602.01297", "authors": ["Shaowei Shen", "Xiaohong Yang", "Jie Yang", "Lianfen Huang", "Yongcai Zhang", "Yang Zou", "Seyyedali Hosseinalipour"], "title": "RE-MCDF: Closed-Loop Multi-Expert LLM Reasoning for Knowledge-Grounded Clinical Diagnosis", "comment": "9 pages, 4 figures", "summary": "Electronic medical records (EMRs), particularly in neurology, are inherently heterogeneous, sparse, and noisy, which poses significant challenges for large language models (LLMs) in clinical diagnosis. In such settings, single-agent systems are vulnerable to self-reinforcing errors, as their predictions lack independent validation and can drift toward spurious conclusions. Although recent multi-agent frameworks attempt to mitigate this issue through collaborative reasoning, their interactions are often shallow and loosely structured, failing to reflect the rigorous, evidence-driven processes used by clinical experts. More fundamentally, existing approaches largely ignore the rich logical dependencies among diseases, such as mutual exclusivity, pathological compatibility, and diagnostic confusion. This limitation prevents them from ruling out clinically implausible hypotheses, even when sufficient evidence is available. To overcome these, we propose RE-MCDF, a relation-enhanced multi-expert clinical diagnosis framework. RE-MCDF introduces a generation--verification--revision closed-loop architecture that integrates three complementary components: (i) a primary expert that generates candidate diagnoses and supporting evidence, (ii) a laboratory expert that dynamically prioritizes heterogeneous clinical indicators, and (iii) a multi-relation awareness and evaluation expert group that explicitly enforces inter-disease logical constraints. Guided by a medical knowledge graph (MKG), the first two experts adaptively reweight EMR evidence, while the expert group validates and corrects candidate diagnoses to ensure logical consistency. Extensive experiments on the neurology subset of CMEMR (NEEMRs) and on our curated dataset (XMEMRs) demonstrate that RE-MCDF consistently outperforms state-of-the-art baselines in complex diagnostic scenarios.", "AI": {"tldr": "\u9488\u5bf9\u795e\u7ecf\u79d1\u7535\u5b50\u75c5\u5386\u7684\u5f02\u8d28\u6027\u548c\u566a\u58f0\u95ee\u9898\uff0c\u63d0\u51faRE-MCDF\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210-\u9a8c\u8bc1-\u4fee\u8ba2\u95ed\u73af\u548c\u591a\u4e13\u5bb6\u534f\u4f5c\uff0c\u663e\u5f0f\u5efa\u6a21\u75be\u75c5\u95f4\u903b\u8f91\u7ea6\u675f\uff0c\u63d0\u5347\u8bca\u65ad\u53ef\u9760\u6027\u3002", "motivation": "\u795e\u7ecf\u79d1\u7535\u5b50\u75c5\u5386\u5177\u6709\u5f02\u8d28\u6027\u3001\u7a00\u758f\u6027\u548c\u566a\u58f0\uff0c\u5355\u667a\u80fd\u4f53\u5927\u8bed\u8a00\u6a21\u578b\u6613\u4ea7\u751f\u81ea\u6211\u5f3a\u5316\u9519\u8bef\uff1b\u73b0\u6709\u591a\u667a\u80fd\u4f53\u6846\u67b6\u4ea4\u4e92\u6d45\u5c42\u4e14\u5ffd\u7565\u75be\u75c5\u95f4\u7684\u903b\u8f91\u4f9d\u8d56\u5173\u7cfb\uff08\u4e92\u65a5\u3001\u75c5\u7406\u517c\u5bb9\u3001\u8bca\u65ad\u6df7\u6dc6\uff09\uff0c\u65e0\u6cd5\u6392\u9664\u4e34\u5e8a\u4e0d\u5408\u7406\u5047\u8bbe\u3002", "method": "\u63d0\u51faRE-MCDF\uff0c\u91c7\u7528\u751f\u6210-\u9a8c\u8bc1-\u4fee\u8ba2\u95ed\u73af\u67b6\u6784\uff0c\u5305\u542b\u4e09\u4e2a\u4e92\u8865\u4e13\u5bb6\uff1a\u4e3b\u8bca\u4e13\u5bb6\u751f\u6210\u5019\u9009\u8bca\u65ad\u548c\u8bc1\u636e\uff0c\u68c0\u9a8c\u4e13\u5bb6\u52a8\u6001\u52a0\u6743\u4e34\u5e8a\u6307\u6807\uff0c\u591a\u5173\u7cfb\u611f\u77e5\u8bc4\u4f30\u4e13\u5bb6\u7ec4\u57fa\u4e8e\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u663e\u5f0f\u6267\u884c\u75be\u75c5\u95f4\u903b\u8f91\u7ea6\u675f\uff0c\u786e\u4fdd\u8bca\u65ad\u903b\u8f91\u4e00\u81f4\u6027\u3002", "result": "\u5728NEEMRs\u548cXMEMRs\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cRE-MCDF\u5728\u590d\u6742\u8bca\u65ad\u573a\u666f\u4e2d\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u878d\u5408\u7ed3\u6784\u5316\u75be\u75c5\u5173\u7cfb\u548c\u591a\u4e13\u5bb6\u534f\u4f5c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u4ece\u5f02\u6784\u7535\u5b50\u75c5\u5386\u4e2d\u8fdb\u884c\u53ef\u9760\u3001\u903b\u8f91\u4e00\u81f4\u7684\u4e34\u5e8a\u8bca\u65ad\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.00465", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00465", "abs": "https://arxiv.org/abs/2602.00465", "authors": ["Jiaqi Yin", "Baiming Chen", "Jia Fei", "Mingjun Yang"], "title": "PAIR-Former: Budgeted Relational MIL for miRNA Target Prediction", "comment": "Preprint. Under review. During the preprint stage, inquiries and feedback can be directed to Jiaqi Yin (yjqhit@gmail.com)", "summary": "Functional miRNA--mRNA targeting is a large-bag prediction problem: each transcript yields a heavy-tailed pool of candidate target sites (CTSs), yet only a pair-level label is observed. We formalize this regime as \\emph{Budgeted Relational Multi-Instance Learning (BR-MIL)}, where at most $K$ instances per bag may receive expensive encoding and relational processing under a hard compute budget. We propose \\textbf{PAIR-Former} (Pool-Aware Instance-Relational Transformer), a BR-MIL pipeline that performs a cheap full-pool scan, selects up to $K$ diverse CTSs on CPU, and applies a permutation-invariant Set Transformer aggregator on the selected tokens. On miRAW, PAIR-Former outperforms strong pooling baselines at a practical operating budget ($K^\\star{=}64$) while providing a controllable accuracy--compute trade-off as $K$ varies. We further provide theory linking budgeted selection to (i) approximation error decreasing with $K$ and (ii) generalization terms governed by $K$ in the expensive relational component.", "AI": {"tldr": "A paper about predicting miRNA-mRNA interactions using a budgeted multi-instance learning approach called PAIR-Former that selects K candidate target sites for expensive processing, outperforming baselines at K=64 with a controllable compute-accuracy trade-off.", "motivation": "The challenge of functional miRNA-mRNA targeting where each transcript has many candidate target sites but only pair-level labels are available, creating a large-bag multi-instance learning problem with computational constraints.", "method": "Proposes PAIR-Former, a BR-MIL pipeline that performs cheap full-pool scanning, selects up to K diverse candidate target sites on CPU, and applies a permutation-invariant Set Transformer aggregator on the selected tokens.", "result": "On miRAW dataset, PAIR-Former outperforms strong pooling baselines at K=64 with a practical operating budget, while providing controllable accuracy-compute trade-offs across different K values.", "conclusion": "The approach effectively addresses the budgeted relational MIL problem for miRNA targeting, with theoretical guarantees linking selection budget to approximation error and generalization, offering a scalable solution with controllable computational cost."}}
{"id": "2602.00475", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00475", "abs": "https://arxiv.org/abs/2602.00475", "authors": ["Michael Psenka", "Michael Rabbat", "Aditi Krishnapriyan", "Yann LeCun", "Amir Bar"], "title": "Parallel Stochastic Gradient-Based Planning for World Models", "comment": "23 pages, 7 figures", "summary": "World models simulate environment dynamics from raw sensory inputs like video. However, using them for planning can be challenging due to the vast and unstructured search space. We propose a robust and highly parallelizable planner that leverages the differentiability of the learned world model for efficient optimization, solving long-horizon control tasks from visual input. Our method treats states as optimization variables (\"virtual states\") with soft dynamics constraints, enabling parallel computation and easier optimization. To facilitate exploration and avoid local optima, we introduce stochasticity into the states. To mitigate sensitive gradients through high-dimensional vision-based world models, we modify the gradient structure to descend towards valid plans while only requiring action-input gradients. Our planner, which we call GRASP (Gradient RelAxed Stochastic Planner), can be viewed as a stochastic version of a non-condensed or collocation-based optimal controller. We provide theoretical justification and experiments on video-based world models, where our resulting planner outperforms existing planning algorithms like the cross-entropy method (CEM) and vanilla gradient-based optimization (GD) on long-horizon experiments, both in success rate and time to convergence.", "AI": {"tldr": "\u63d0\u51faGRASP\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u5c06\u72b6\u6001\u89c6\u4e3a\u4f18\u5316\u53d8\u91cf\u3001\u5f15\u5165\u72b6\u6001\u968f\u673a\u6027\u5e76\u6539\u8fdb\u68af\u5ea6\u7ed3\u6784\uff0c\u5728\u89c6\u89c9\u4e16\u754c\u6a21\u578b\u4e0a\u5b9e\u73b0\u9ad8\u6548\u957f\u5468\u671f\u89c4\u5212\uff0c\u663e\u8457\u8d85\u8d8aCEM\u548c\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u4e16\u754c\u6a21\u578b\u867d\u80fd\u6a21\u62df\u73af\u5883\u52a8\u6001\uff0c\u4f46\u7528\u4e8e\u89c6\u89c9\u8f93\u5165\u7684\u957f\u5468\u671f\u89c4\u5212\u65f6\u9762\u4e34\u641c\u7d22\u7a7a\u95f4\u5e9e\u5927\u65e0\u5e8f\u3001\u68af\u5ea6\u654f\u611f\u53ca\u6613\u9677\u5c40\u90e8\u6700\u4f18\u7b49\u6311\u6218\u3002", "method": "1. \u5c06\u72b6\u6001\u4f5c\u4e3a\u53ef\u4f18\u5316\u53d8\u91cf\uff08\"\u865a\u62df\u72b6\u6001\"\uff09\u5e76\u65bd\u52a0\u8f6f\u52a8\u529b\u5b66\u7ea6\u675f\u4ee5\u5b9e\u73b0\u5e76\u884c\u8ba1\u7b97\uff1b2. \u5f15\u5165\u72b6\u6001\u968f\u673a\u6027\u4fc3\u8fdb\u63a2\u7d22\uff1b3. \u4fee\u6539\u68af\u5ea6\u7ed3\u6784\uff0c\u4ec5\u4f9d\u8d56\u52a8\u4f5c\u8f93\u5165\u68af\u5ea6\u89c4\u907f\u9ad8\u7ef4\u89c6\u89c9\u6a21\u578b\u7684\u68af\u5ea6\u654f\u611f\u95ee\u9898\u3002", "result": "\u5728\u89c6\u9891\u4e16\u754c\u6a21\u578b\u7684\u957f\u5468\u671f\u4efb\u52a1\u4e2d\uff0cGRASP\u76f8\u6bd4\u4ea4\u53c9\u71b5\u6cd5\uff08CEM\uff09\u548c\u57fa\u7840\u68af\u5ea6\u4e0b\u964d\uff08GD\uff09\u663e\u8457\u63d0\u5347\u6210\u529f\u7387\u5e76\u7f29\u77ed\u6536\u655b\u65f6\u95f4\u3002", "conclusion": "GRASP\u4f5c\u4e3a\u968f\u673a\u5316\u975e\u51dd\u805a/\u914d\u70b9\u6700\u4f18\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u53ef\u5fae\u4e16\u754c\u6a21\u578b\u5b9e\u73b0\u9ad8\u6548\u4f18\u5316\uff0c\u4e3a\u89c6\u89c9\u89c4\u5212\u63d0\u4f9b\u9c81\u68d2\u4e14\u9ad8\u5e76\u884c\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01355", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01355", "abs": "https://arxiv.org/abs/2602.01355", "authors": ["Haojia Zhu", "Qinyuan Xu", "Haoyu Li", "Yuxi Liu", "Hanchen Qiu", "Jiaoyan Chen", "Jiahui Jin"], "title": "Aggregation Queries over Unstructured Text: Benchmark and Agentic Method", "comment": null, "summary": "Aggregation query over free text is a long-standing yet underexplored problem. Unlike ordinary question answering, aggregate queries require exhaustive evidence collection and systems are required to \"find all,\" not merely \"find one.\" Existing paradigms such as Text-to-SQL and Retrieval-Augmented Generation fail to achieve this completeness. In this work, we formalize entity-level aggregation querying over text in a corpus-bounded setting with strict completeness requirement. To enable principled evaluation, we introduce AGGBench, a benchmark designed to evaluate completeness-oriented aggregation under realistic large-scale corpus. To accompany the benchmark, we propose DFA (Disambiguation--Filtering--Aggregation), a modular agentic baseline that decomposes aggregation querying into interpretable stages and exposes key failure modes related to ambiguity, filtering, and aggregation. Empirical results show that DFA consistently improves aggregation evidence coverage over strong RAG and agentic baselines. The data and code are available in https://anonymous.4open.science/r/DFA-A4C1.", "AI": {"tldr": "\u63d0\u51faAGGBench\u57fa\u51c6\u548cDFA\u6a21\u5757\u5316\u667a\u80fd\u4f53\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d88\u6b67\u3001\u8fc7\u6ee4\u3001\u805a\u5408\u4e09\u9636\u6bb5\u5206\u89e3\u89e3\u51b3\u6587\u672c\u805a\u5408\u67e5\u8be2\u7684\u5b8c\u6574\u6027\u95ee\u9898\uff0c\u5b9e\u8bc1\u663e\u793aDFA\u663e\u8457\u4f18\u4e8e\u73b0\u6709RAG\u57fa\u7ebf\u3002", "motivation": "\u6587\u672c\u805a\u5408\u67e5\u8be2\u9700\u8981\u7a77\u5c3d\u8bc1\u636e\u6536\u96c6\uff08\"\u627e\u51fa\u6240\u6709\"\u800c\u975e\"\u627e\u51fa\u4e00\u4e2a\"\uff09\uff0c\u4f46\u73b0\u6709Text-to-SQL\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u8303\u5f0f\u65e0\u6cd5\u5b9e\u73b0\u4e25\u683c\u5b8c\u5907\u6027\uff0c\u8be5\u95ee\u9898\u957f\u671f\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faDFA\uff08Disambiguation-Filtering-Aggregation\uff09\u6a21\u5757\u5316\u667a\u80fd\u4f53\u57fa\u7ebf\uff0c\u5c06\u805a\u5408\u67e5\u8be2\u5206\u89e3\u4e3a\u4e09\u4e2a\u53ef\u89e3\u91ca\u9636\u6bb5\uff1a\u5b9e\u4f53\u6d88\u6b67\u3001\u5019\u9009\u8fc7\u6ee4\u548c\u7ed3\u679c\u805a\u5408\uff0c\u5e76\u7cfb\u7edf\u6027\u5730\u66b4\u9732\u6b67\u4e49\u3001\u8fc7\u6ee4\u9519\u8bef\u548c\u805a\u5408\u4e0d\u5b8c\u6574\u7b49\u5173\u952e\u5931\u8d25\u6a21\u5f0f\u3002", "result": "\u5728AGGBench\u57fa\u51c6\u4e0a\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cDFA\u65b9\u6cd5\u6301\u7eed\u4e14\u663e\u8457\u5730\u63d0\u5347\u4e86\u805a\u5408\u8bc1\u636e\u8986\u76d6\u7387\uff0c\u4f18\u4e8e\u5f3aRAG\u548c\u667a\u80fd\u4f53\u57fa\u7ebf\u3002", "conclusion": "\u901a\u8fc7\u5f62\u5f0f\u5316\u4e25\u683c\u5b8c\u5907\u6027\u8981\u6c42\u7684\u805a\u5408\u67e5\u8be2\u95ee\u9898\u3001\u6784\u5efa\u5927\u89c4\u6a21\u8bc4\u4f30\u57fa\u51c6AGGBench\uff0c\u5e76\u8bbe\u8ba1\u6a21\u5757\u5316DFA\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u5206\u9636\u6bb5\u5904\u7406\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u805a\u5408\u67e5\u8be2\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u8bc4\u4f30\u65b9\u6cd5\u548c\u53ef\u89e3\u91ca\u7684\u57fa\u7ebf\u7cfb\u7edf\u3002"}}
{"id": "2602.00476", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00476", "abs": "https://arxiv.org/abs/2602.00476", "authors": ["Hengchang Liu", "Zhao Yang", "Bing Su"], "title": "Diffusion LMs Can Approximate Optimal Infilling Lengths Implicitly", "comment": null, "summary": "Diffusion language models (DLMs) provide a bidirectional generation framework naturally suited for infilling, yet their performance is constrained by the pre-specified infilling length. In this paper, we reveal that DLMs possess an inherent ability to discover the correct infilling length. We identify two key statistical phenomena in the first-step denoising confidence: a local \\textit{Oracle Peak} that emerges near the ground-truth length and a systematic \\textit{Length Bias} that often obscures this signal. By leveraging this signal and calibrating the bias, our training-free method \\textbf{CAL} (\\textbf{C}alibrated \\textbf{A}daptive \\textbf{L}ength) enables DLMs to approximate the optimal length through an efficient search before formal decoding. Empirical evaluations demonstrate that CAL improves Pass@1 by up to 47.7\\% over fixed-length baselines and 40.5\\% over chat-based adaptive methods in code infilling, while boosting BLEU-2 and ROUGE-L by up to 8.5\\% and 9.9\\% in text infilling. These results demonstrate that CAL paves the way for robust DLM infilling without requiring any specialized training. Code is available at https://github.com/NiuHechang/Calibrated_Adaptive_Length.", "AI": {"tldr": "The paper proposes CAL, a training-free method that enables diffusion language models to automatically determine the correct infilling length by detecting statistical signals in first-step denoising confidence and calibrating length bias, significantly improving performance in code and text infilling tasks.", "motivation": "Diffusion language models are naturally suited for infilling but are constrained by pre-specified infilling lengths, which limits their performance. The authors aim to unlock the inherent ability of DLMs to discover the correct infilling length without requiring additional training.", "method": "The method identifies two statistical phenomena in first-step denoising confidence: an Oracle Peak near ground-truth length and a systematic Length Bias. CAL leverages the Oracle Peak signal while calibrating the Length Bias through an efficient search process before formal decoding to approximate the optimal infilling length.", "result": "CAL improves Pass@1 by up to 47.7% over fixed-length baselines and 40.5% over chat-based adaptive methods for code infilling. For text infilling, it boosts BLEU-2 and ROUGE-L scores by up to 8.5% and 9.9% respectively.", "conclusion": "The paper demonstrates that CAL successfully enables diffusion language models to perform robust infilling with adaptive length determination without requiring any specialized training, paving the way for more flexible and effective DLM applications."}}
{"id": "2602.01425", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01425", "abs": "https://arxiv.org/abs/2602.01425", "authors": ["Vikram Natarajan", "Devina Jain", "Shivam Arora", "Satvik Golechha", "Joseph Bloom"], "title": "Building Better Deception Probes Using Targeted Instruction Pairs", "comment": null, "summary": "Linear probes are a promising approach for monitoring AI systems for deceptive behaviour. Previous work has shown that a linear classifier trained on a contrastive instruction pair and a simple dataset can achieve good performance. However, these probes exhibit notable failures even in straightforward scenarios, including spurious correlations and false positives on non-deceptive responses. In this paper, we identify the importance of the instruction pair used during training. Furthermore, we show that targeting specific deceptive behaviors through a human-interpretable taxonomy of deception leads to improved results on evaluation datasets. Our findings reveal that instruction pairs capture deceptive intent rather than content-specific patterns, explaining why prompt choice dominates probe performance (70.6% of variance). Given the heterogeneity of deception types across datasets, we conclude that organizations should design specialized probes targeting their specific threat models rather than seeking a universal deception detector.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u7ebf\u6027\u63a2\u9488\u68c0\u6d4bAI\u6b3a\u9a97\u884c\u4e3a\u7684\u5c40\u9650\u6027\uff0c\u53d1\u73b0\u8bad\u7ec3\u65f6\u4f7f\u7528\u7684\u6307\u4ee4\u5bf9\u9009\u62e9\u81f3\u5173\u91cd\u8981\uff0c\u5360\u6027\u80fd\u5dee\u5f02\u768470.6%\uff0c\u5e76\u8bba\u8bc1\u5e94\u9488\u5bf9\u7279\u5b9a\u5a01\u80c1\u6a21\u578b\u8bbe\u8ba1\u4e13\u7528\u63a2\u9488\u800c\u975e\u8ffd\u6c42\u901a\u7528\u68c0\u6d4b\u5668\u3002", "motivation": "\u7ebf\u6027\u63a2\u9488\u5728\u68c0\u6d4bAI\u6b3a\u9a97\u884c\u4e3a\u65b9\u9762\u867d\u6709\u524d\u666f\uff0c\u4f46\u5728\u7b80\u5355\u573a\u666f\u4e0b\u4ecd\u5b58\u5728\u663e\u8457\u5931\u8d25\uff0c\u5982\u865a\u5047\u76f8\u5173\u6027\u548c\u5bf9\u975e\u6b3a\u9a97\u6027\u54cd\u5e94\u7684\u8bef\u62a5\u3002\u9700\u8981\u7406\u89e3\u8fd9\u4e9b\u5931\u8d25\u7684\u539f\u56e0\u5e76\u6539\u8fdb\u63a2\u9488\u6027\u80fd\u3002", "method": "\u8bc6\u522b\u8bad\u7ec3\u4e2d\u6307\u4ee4\u5bf9\u7684\u91cd\u8981\u6027\uff1b\u901a\u8fc7\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u6b3a\u9a97\u5206\u7c7b\u6cd5\u9488\u5bf9\u7279\u5b9a\u6b3a\u9a97\u884c\u4e3a\u8fdb\u884c\u4f18\u5316\uff1b\u5206\u6790\u6307\u4ee4\u5bf9\u5982\u4f55\u5f71\u54cd\u63a2\u9488\u6027\u80fd\u3002", "result": "\u6307\u4ee4\u5bf9\u6355\u83b7\u7684\u662f\u6b3a\u9a97\u610f\u56fe\u800c\u975e\u5185\u5bb9\u7279\u5b9a\u6a21\u5f0f\uff1b\u63d0\u793a\u9009\u62e9\u4e3b\u5bfc\u4e86\u63a2\u9488\u6027\u80fd\uff08\u536070.6%\u7684\u65b9\u5dee\uff09\uff1b\u901a\u8fc7\u5206\u7c7b\u6cd5\u9488\u5bf9\u7279\u5b9a\u6b3a\u9a97\u884c\u4e3a\u53ef\u5728\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u66f4\u597d\u7ed3\u679c\u3002", "conclusion": "\u9274\u4e8e\u4e0d\u540c\u6570\u636e\u96c6\u4e2d\u6b3a\u9a97\u7c7b\u578b\u7684\u5f02\u8d28\u6027\uff0c\u7ec4\u7ec7\u5e94\u9488\u5bf9\u5176\u7279\u5b9a\u5a01\u80c1\u6a21\u578b\u8bbe\u8ba1\u4e13\u7528\u63a2\u9488\uff0c\u800c\u4e0d\u662f\u5bfb\u6c42\u901a\u7528\u7684\u6b3a\u9a97\u68c0\u6d4b\u5668\u3002"}}
{"id": "2602.00478", "categories": ["cs.LG", "cs.AI", "cs.NE", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.00478", "abs": "https://arxiv.org/abs/2602.00478", "authors": ["Xi Lin", "Ping Guo", "Yilu Liu", "Qingfu Zhang", "Jianyong Sun"], "title": "Quality-Diversity Optimization as Multi-Objective Optimization", "comment": null, "summary": "The Quality-Diversity (QD) optimization aims to discover a collection of high-performing solutions that simultaneously exhibit diverse behaviors within a user-defined behavior space. This paradigm has stimulated significant research interest and demonstrated practical utility in domains including robot control, creative design, and adversarial sample generation. A variety of QD algorithms with distinct design principles have been proposed in recent years. Instead of proposing a new QD algorithm, this work introduces a novel reformulation by casting the QD optimization as a multi-objective optimization (MOO) problem with a huge number of optimization objectives. By establishing this connection, we enable the direct adoption of well-established MOO methods, particularly set-based scalarization techniques, to solve QD problems through a collaborative search process. We further provide a theoretical analysis demonstrating that our approach inherits theoretical guarantees from MOO while providing desirable properties for the QD optimization. Experimental studies across several QD applications confirm that our method achieves performance competitive with state-of-the-art QD algorithms.", "AI": {"tldr": "\u5c06\u8d28\u91cf\u591a\u6837\u6027\u4f18\u5316\u91cd\u65b0\u8868\u8ff0\u4e3a\u5177\u6709\u5927\u91cf\u76ee\u6807\u7684\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u5229\u7528\u591a\u76ee\u6807\u4f18\u5316\u4e2d\u7684\u96c6\u5408\u6807\u91cf\u5316\u6280\u672f\u5b9e\u73b0\u534f\u4f5c\u641c\u7d22\uff0c\u5728\u673a\u5668\u4eba\u63a7\u5236\u3001\u521b\u610f\u8bbe\u8ba1\u548c\u5bf9\u6297\u6837\u672c\u751f\u6210\u7b49\u5e94\u7528\u4e2d\u8fbe\u5230\u4e0e\u73b0\u6709\u5148\u8fdb\u7b97\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u8d28\u91cf\u591a\u6837\u6027\u4f18\u5316\u5728\u673a\u5668\u4eba\u63a7\u5236\u3001\u521b\u610f\u8bbe\u8ba1\u548c\u5bf9\u6297\u6837\u672c\u751f\u6210\u7b49\u9886\u57df\u5c55\u73b0\u51fa\u5b9e\u9645\u4ef7\u503c\uff0c\u73b0\u6709\u7b97\u6cd5\u8bbe\u8ba1\u539f\u5219\u5404\u5f02\u3002\u672c\u7814\u7a76\u65e8\u5728\u4e0d\u63d0\u51fa\u65b0QD\u7b97\u6cd5\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u91cd\u65b0\u8868\u8ff0\u95ee\u9898\u6765\u76f4\u63a5\u5229\u7528\u6210\u719f\u7684\u591a\u76ee\u6807\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u5c06\u8d28\u91cf\u591a\u6837\u6027\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u5177\u6709\u5927\u91cf\u4f18\u5316\u76ee\u6807\u7684\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5efa\u7acb\u8fd9\u4e00\u8054\u7cfb\uff0c\u91c7\u7528\u96c6\u5408\u6807\u91cf\u5316\u6280\u672f\u8fdb\u884c\u534f\u4f5c\u641c\u7d22\uff0c\u76f4\u63a5\u5e94\u7528\u6210\u719f\u7684\u591a\u76ee\u6807\u4f18\u5316\u65b9\u6cd5\u89e3\u51b3QD\u95ee\u9898\u3002", "result": "\u5728\u591a\u4e2aQD\u5e94\u7528\u4e0a\u7684\u5b9e\u9a8c\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6027\u80fd\u53ef\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u8d28\u91cf\u591a\u6837\u6027\u4f18\u5316\u7b97\u6cd5\u76f8\u5ab2\u7f8e\u3002", "conclusion": "\u8be5\u91cd\u65b0\u8868\u8ff0\u65b9\u6cd5\u6210\u529f\u8fde\u63a5\u4e86QD\u4e0eMOO\uff0c\u7ee7\u627f\u4e86\u591a\u76ee\u6807\u4f18\u5316\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u9002\u7528\u4e8e\u8d28\u91cf\u591a\u6837\u6027\u4f18\u5316\u7684\u7406\u60f3\u7279\u6027\uff0c\u4e3aQD\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u8def\u5f84\u3002"}}
{"id": "2602.01443", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01443", "abs": "https://arxiv.org/abs/2602.01443", "authors": ["Alberto Castelo", "Zahra Zanjani Foumani", "Ailin Fan", "Keat Yang Koay", "Vibhor Malik", "Yuanzheng Zhu", "Han Li", "Meysam Feghhi", "Ronie Uliana", "Shuang Xie", "Zhaoyu Zhang", "Angelo Ocana Martins", "Mingyu Zhao", "Francis Pelland", "Jonathan Faerman", "Nikolas LeBlanc", "Aaron Glazer", "Andrew McNamara", "Lingyun Wang", "Zhong Wu"], "title": "SimGym: Traffic-Grounded Browser Agents for Offline A/B Testing in E-Commerce", "comment": null, "summary": "A/B testing remains the gold standard for evaluating e-commerce UI changes, yet it diverts traffic, takes weeks to reach significance, and risks harming user experience. We introduce SimGym, a scalable system for rapid offline A/B testing using traffic-grounded synthetic buyers powered by Large Language Model agents operating in a live browser. SimGym extracts per-shop buyer profiles and intents from production interaction data, identifies distinct behavioral archetypes, and simulates cohort-weighted sessions across control and treatment storefronts. We validate SimGym against real human outcomes from real UI changes on a major e-commerce platform under confounder control. Even without alignment post training, SimGym agents achieve state of the art alignment with observed outcome shifts and reduces experiment cycles from weeks to under an hour , enabling rapid experimentation without exposure to real buyers.", "AI": {"tldr": "SimGym uses LLM-powered synthetic buyers for rapid offline A/B testing, cutting e-commerce UI experiment cycles from weeks to under an hour while maintaining alignment with real human outcomes.", "motivation": "Traditional A/B testing diverts traffic, requires weeks for statistical significance, and risks harming user experience during live experiments.", "method": "Extracts buyer profiles from production data, identifies behavioral archetypes, and simulates cohort-weighted sessions using LLM agents in live browsers to test UI changes offline.", "result": "Achieves state-of-the-art alignment with real human outcome shifts under confounder control, reducing experiment time from weeks to under one hour without alignment post-training.", "conclusion": "Enables rapid, safe UI experimentation by replacing live user exposure with synthetic buyer simulations grounded in real traffic data."}}
{"id": "2602.00482", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00482", "abs": "https://arxiv.org/abs/2602.00482", "authors": ["Jiarui Zhang", "Yuchen Yang", "Ran Yan", "Zhiyu Mei", "Liyuan Zhang", "Daifeng Li", "Wei Fu", "Jiaxuan Gao", "Shusheng Xu", "Yi Wu", "Binhang Yuan"], "title": "AREAL-DTA: Dynamic Tree Attention for Efficient Reinforcement Learning of Large Language Models", "comment": null, "summary": "Reinforcement learning (RL) based post-training for large language models (LLMs) is computationally expensive, as it generates many rollout sequences that could frequently share long token prefixes. Existing RL frameworks usually process these sequences independently, repeatedly recomputing identical prefixes during forward and backward passes during policy model training, leading to substantial inefficiencies in computation and memory usage. Although prefix sharing naturally induces a tree structure over rollouts, prior tree-attention-based solutions rely on fully materialized attention masks and scale poorly in RL settings. In this paper, we introduce AREAL-DTA to efficiently exploit prefix sharing in RL training. AREAL-DTA employs a depth-first-search (DFS)-based execution strategy that dynamically traverses the rollout prefix tree during both forward and backward computation, materializing only a single root-to-leaf path at a time. To further improve scalability, AREAL-DTA incorporates a load-balanced distributed batching mechanism that dynamically constructs and processes prefix trees across multiple GPUs. Across the popular RL post-training workload, AREAL-DTA achieves up to $8.31\\times$ in $\u03c4^2$-bench higher training throughput.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faAREAL-DTA\uff0c\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\u7684\u6811\u5f62\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u904d\u5386\u524d\u7f00\u6811\u5e76\u4ec5\u5b9e\u4f8b\u5316\u5355\u6761\u6839\u5230\u53f6\u8def\u5f84\uff0c\u89e3\u51b3\u4e86LLM\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u524d\u7f00\u91cd\u590d\u8ba1\u7b97\u5bfc\u81f4\u7684\u6548\u7387\u74f6\u9888\uff0c\u5728\u591aGPU\u73af\u5883\u4e0b\u5b9e\u73b0\u6700\u9ad88.31\u500d\u541e\u5410\u91cf\u63d0\u5347\u3002", "motivation": "\u73b0\u6709RL\u6846\u67b6\u5728\u5904\u7406LLM\u540e\u8bad\u7ec3\u65f6\uff0c\u72ec\u7acb\u5904\u7406\u5171\u4eab\u957f\u524d\u7f00\u7684rollout\u5e8f\u5217\uff0c\u5bfc\u81f4\u524d\u5411\u548c\u53cd\u5411\u4f20\u64ad\u4e2d\u91cd\u590d\u8ba1\u7b97\u76f8\u540c\u524d\u7f00\uff0c\u9020\u6210\u4e25\u91cd\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u8d44\u6e90\u6d6a\u8d39\u3002\u867d\u7136\u524d\u7f00\u5171\u4eab\u81ea\u7136\u5f62\u6210\u6811\u5f62\u7ed3\u6784\uff0c\u4f46\u5148\u524d\u57fa\u4e8e\u6811\u6ce8\u610f\u529b\u7684\u65b9\u6848\u4f9d\u8d56\u5b8c\u5168\u5b9e\u4f8b\u5316\u7684\u6ce8\u610f\u529b\u63a9\u7801\uff0c\u5728RL\u573a\u666f\u4e0b\u6269\u5c55\u6027\u5dee\u3002", "method": "\u63d0\u51faAREAL-DTA\u6846\u67b6\uff1a1) \u91c7\u7528\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\u7b56\u7565\uff0c\u5728\u524d\u540e\u5411\u8ba1\u7b97\u671f\u95f4\u52a8\u6001\u904d\u5386rollout\u524d\u7f00\u6811\uff0c\u6bcf\u6b21\u4ec5\u5b9e\u4f8b\u5316\u4e00\u6761\u6839\u5230\u53f6\u8def\u5f84\uff1b2) \u8bbe\u8ba1\u8d1f\u8f7d\u5747\u8861\u7684\u5206\u5e03\u5f0f\u6279\u5904\u7406\u673a\u5236\uff0c\u8de8\u591aGPU\u52a8\u6001\u6784\u5efa\u548c\u5904\u7406\u524d\u7f00\u6811\u3002", "result": "\u5728\u6d41\u884c\u7684RL\u540e\u8bad\u7ec3\u5de5\u4f5c\u8d1f\u8f7d\u03c4\u00b2-bench\u4e0a\uff0cAREAL-DTA\u5b9e\u73b0\u4e86\u6700\u9ad88.31\u500d\u7684\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u5347\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "AREAL-DTA\u901a\u8fc7\u9ad8\u6548\u5229\u7528\u524d\u7f00\u5171\u4eab\u7279\u6027\uff0c\u5927\u5e45\u964d\u4f4e\u4e86RL\u8bad\u7ec3LLM\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01465", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.01465", "abs": "https://arxiv.org/abs/2602.01465", "authors": ["Nikita Benkovich", "Vitalii Valkov"], "title": "Agyn: A Multi-Agent System for Team-Based Autonomous Software Engineering", "comment": null, "summary": "Large language models have demonstrated strong capabilities in individual software engineering tasks, yet most autonomous systems still treat issue resolution as a monolithic or pipeline-based process. In contrast, real-world software development is organized as a collaborative activity carried out by teams following shared methodologies, with clear role separation, communication, and review. In this work, we present a fully automated multi-agent system that explicitly models software engineering as an organizational process, replicating the structure of an engineering team. Built on top of agyn, an open-source platform for configuring agent teams, our system assigns specialized agents to roles such as coordination, research, implementation, and review, provides them with isolated sandboxes for experimentation, and enables structured communication. The system follows a defined development methodology for working on issues, including analysis, task specification, pull request creation, and iterative review, and operates without any human intervention. Importantly, the system was designed for real production use and was not tuned for SWE-bench. When evaluated post hoc on SWE-bench 500, it resolves 72.4% of tasks, outperforming single-agent baselines using comparable language models. Our results suggest that replicating team structure, methodology, and communication is a powerful paradigm for autonomous software engineering, and that future progress may depend as much on organizational design and agent infrastructure as on model improvements.", "AI": {"tldr": "A multi-agent system simulating real software engineering teams with specialized roles (coordination, research, implementation, review) and structured processes achieves 72.4% success on SWE-bench, outperforming single-agent approaches and demonstrating that organizational design is as crucial as model improvements.", "motivation": "Current autonomous software engineering systems treat issue resolution as monolithic or pipeline processes, unlike real-world team-based development with role separation, communication, and review.", "method": "Built on the agyn platform, the system creates specialized agents for coordination, research, implementation, and review roles, provides isolated sandboxes, enables structured communication, and follows a defined methodology (analysis, task specification, PR creation, iterative review) without human intervention.", "result": "Post hoc evaluation on SWE-bench 500 shows 72.4% task resolution rate, outperforming single-agent baselines with comparable language models, while being designed for production use without SWE-bench tuning.", "conclusion": "Replicating team structure, methodology, and communication is a powerful paradigm for autonomous software engineering; future progress may depend as much on organizational design and agent infrastructure as on model improvements."}}
{"id": "2602.01474", "categories": ["cs.AI", "econ.GN"], "pdf": "https://arxiv.org/pdf/2602.01474", "abs": "https://arxiv.org/abs/2602.01474", "authors": ["Gillian K. Hadfield"], "title": "Legal Infrastructure for Transformative AI Governance", "comment": null, "summary": "Most of our AI governance efforts focus on substance: what rules do we want in place? What limits or checks do we want to impose on AI development and deployment? But a key role for law is not only to establish substantive rules but also to establish legal and regulatory infrastructure to generate and implement rules. The transformative nature of AI calls especially for attention to building legal and regulatory frameworks. In this PNAS Perspective piece I review three examples I have proposed: the creation of registration regimes for frontier models; the creation of registration and identification regimes for autonomous agents; and the design of regulatory markets to facilitate a role for private companies to innovate and deliver AI regulatory services.", "AI": {"tldr": "\u672c\u6587\u8ba4\u4e3aAI\u6cbb\u7406\u5e94\u4f18\u5148\u6784\u5efa\u6cd5\u5f8b\u548c\u76d1\u7ba1\u57fa\u7840\u8bbe\u65bd\u800c\u975e\u4ec5\u5173\u6ce8\u5b9e\u8d28\u6027\u89c4\u5219\uff0c\u5e76\u63d0\u51fa\u4e09\u4e2a\u5177\u4f53\u6846\u67b6\uff1a\u524d\u6cbf\u6a21\u578b\u6ce8\u518c\u5236\u3001\u81ea\u4e3b\u667a\u80fd\u4f53\u6ce8\u518c\u8bc6\u522b\u5236\uff0c\u4ee5\u53ca\u5141\u8bb8\u79c1\u8425\u90e8\u95e8\u63d0\u4f9bAI\u76d1\u7ba1\u670d\u52a1\u7684\u76d1\u7ba1\u5e02\u573a\u3002", "motivation": "\u5f53\u524dAI\u6cbb\u7406\u8fc7\u4e8e\u5173\u6ce8\u5b9e\u8d28\u6027\u89c4\u5219\uff08\u5982\u5e94\u8bbe\u7f6e\u4f55\u79cd\u9650\u5236\uff09\uff0c\u5ffd\u89c6\u4e86\u751f\u6210\u548c\u5b9e\u65bd\u89c4\u5219\u6240\u9700\u7684\u6cd5\u5f8b\u548c\u76d1\u7ba1\u57fa\u7840\u8bbe\u65bd\uff0c\u800cAI\u7684\u53d8\u9769\u6027\u672c\u8d28\u4f7f\u8fd9\u4e00\u95ee\u9898\u5c24\u4e3a\u5173\u952e\u3002", "method": "\u4f5c\u8005\u57fa\u4e8e\u5148\u524d\u7814\u7a76\uff0c\u56de\u987e\u5e76\u63d0\u51fa\u4e09\u4e2a\u5177\u4f53\u6cd5\u5f8b/\u76d1\u7ba1\u6846\u67b6\u4f5c\u4e3a\u57fa\u7840\u8bbe\u65bd\u5bfc\u5411\u6cbb\u7406\u7684\u5b9e\u4f8b\u3002", "result": "\u63d0\u51fa\u4e09\u9879\u5177\u4f53\u5efa\u8bae\uff1a\uff081\uff09\u524d\u6cbfAI\u6a21\u578b\u6ce8\u518c\u5236\u5ea6\uff1b\uff082\uff09\u81ea\u4e3b\u667a\u80fd\u4f53\u6ce8\u518c\u4e0e\u8bc6\u522b\u7cfb\u7edf\uff1b\uff083\uff09\u5141\u8bb8\u79c1\u8425\u516c\u53f8\u63d0\u4f9bAI\u76d1\u7ba1\u670d\u52a1\u7684\u76d1\u7ba1\u5e02\u573a\u3002", "conclusion": "\u6709\u6548\u7684AI\u6cbb\u7406\u9700\u8981\u5efa\u7acb\u5f3a\u5927\u7684\u6cd5\u5f8b\u548c\u76d1\u7ba1\u57fa\u7840\u8bbe\u65bd\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5b9e\u8d28\u6027\u89c4\u5219\uff0c\u8fd9\u4e09\u4e2a\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u672a\u6765\u5b9e\u8df5\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2602.00515", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00515", "abs": "https://arxiv.org/abs/2602.00515", "authors": ["Lin Liu", "Rita Machacy", "Simi Kuniyilh"], "title": "Contrastive Learning for Privacy Enhancements in Industrial Internet of Things", "comment": null, "summary": "The Industrial Internet of Things (IIoT) integrates intelligent sensing, communication, and analytics into industrial environments, including manufacturing, energy, and critical infrastructure. While IIoT enables predictive maintenance and cross-site optimization of modern industrial control systems, such as those in manufacturing and energy, it also introduces significant privacy and confidentiality risks due to the sensitivity of operational data. Contrastive learning, a self-supervised representation learning paradigm, has recently emerged as a promising approach for privacy-preserving analytics by reducing reliance on labeled data and raw data sharing. Although contrastive learning-based privacy-preserving techniques have been explored in the Internet of Things (IoT) domain, this paper offers a comprehensive review of these techniques specifically for privacy preservation in Industrial Internet of Things (IIoT) systems. It emphasizes the unique characteristics of industrial data, system architectures, and various application scenarios. Additionally, the paper discusses solutions and open challenges and outlines future research directions.", "AI": {"tldr": "A comprehensive review of contrastive learning-based privacy preservation techniques specifically for Industrial IoT systems, addressing industrial data characteristics and future challenges.", "motivation": "IIoT's integration of sensitive operational data creates significant privacy risks, and existing contrastive learning research in IoT doesn't adequately address IIoT's unique industrial requirements.", "method": "Comprehensive literature review focusing on contrastive learning techniques adapted for IIoT privacy preservation, analyzing industrial-specific data features, architectures, and application scenarios.", "result": "Identifies current solutions, key open challenges, and proposes future research directions for implementing privacy-preserving analytics in industrial environments.", "conclusion": "Contrastive learning is a promising paradigm for IIoT privacy preservation, but successful deployment requires addressing domain-specific challenges unique to industrial systems."}}
{"id": "2602.01539", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.01539", "abs": "https://arxiv.org/abs/2602.01539", "authors": ["Xiaoyu Wen", "Zhida He", "Han Qi", "Ziyu Wan", "Zhongtian Ma", "Ying Wen", "Tianhang Zheng", "Xingcheng Xu", "Chaochao Lu", "Qiaosheng Zhang"], "title": "MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety", "comment": null, "summary": "Ensuring robust safety alignment is crucial for Large Language Models (LLMs), yet existing defenses often lag behind evolving adversarial attacks due to their \\textbf{reliance on static, pre-collected data distributions}. In this paper, we introduce \\textbf{MAGIC}, a novel multi-turn multi-agent reinforcement learning framework that formulates LLM safety alignment as an adversarial asymmetric game. Specifically, an attacker agent learns to iteratively rewrite original queries into deceptive prompts, while a defender agent simultaneously optimizes its policy to recognize and refuse such inputs. This dynamic process triggers a \\textbf{co-evolution}, where the attacker's ever-changing strategies continuously uncover long-tail vulnerabilities, driving the defender to generalize to unseen attack patterns. Remarkably, we observe that the attacker, endowed with initial reasoning ability, evolves \\textbf{novel, previously unseen combinatorial strategies} through iterative RL training, underscoring our method's substantial potential. Theoretically, we provide insights into a more robust game equilibrium and derive safety guarantees. Extensive experiments validate our framework's effectiveness, demonstrating superior defense success rates without compromising the helpfulness of the model. Our code is available at https://github.com/BattleWen/MAGIC.", "AI": {"tldr": "\u63d0\u51faMAGIC\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u653b\u9632\u5bf9\u6297\u7684\u534f\u540c\u8fdb\u5316\u5b9e\u73b0LLM\u5b89\u5168\u5bf9\u9f50\uff0c\u53d1\u73b0\u65b0\u578b\u7ec4\u5408\u653b\u51fb\u7b56\u7565\u5e76\u5728\u4e0d\u635f\u5bb3\u6a21\u578b\u6709\u7528\u6027\u524d\u63d0\u4e0b\u63d0\u5347\u9632\u5fa1\u6210\u529f\u7387", "motivation": "\u73b0\u6709LLM\u5b89\u5168\u9632\u5fa1\u4f9d\u8d56\u9759\u6001\u6570\u636e\u5206\u5e03\uff0c\u96be\u4ee5\u5e94\u5bf9\u4e0d\u65ad\u6f14\u5316\u7684\u5bf9\u6297\u653b\u51fb\uff0c\u5b58\u5728\u957f\u5c3e\u6f0f\u6d1e\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898", "method": "\u6784\u5efa\u591a\u8f6e\u975e\u5bf9\u79f0\u5bf9\u6297\u535a\u5f08\u6846\u67b6\uff1a\u653b\u51fb\u8005\u667a\u80fd\u4f53\u8fed\u4ee3\u6539\u5199\u67e5\u8be2\u4e3a\u6b3a\u9a97\u6027\u63d0\u793a\uff0c\u9632\u5fa1\u8005\u667a\u80fd\u4f53\u540c\u6b65\u4f18\u5316\u8bc6\u522b\u62d2\u7edd\u7b56\u7565\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u89e6\u53d1\u534f\u540c\u8fdb\u5316", "result": "\u653b\u51fb\u8005\u81ea\u53d1\u6f14\u5316\u51fa\u672a\u89c1\u8fc7\u7684\u7ec4\u5408\u653b\u51fb\u7b56\u7565\uff0c\u7406\u8bba\u8bc1\u660e\u66f4\u7a33\u5065\u7684\u535a\u5f08\u5747\u8861\u548c\u5b89\u5168\u4fdd\u8bc1\uff0c\u5b9e\u9a8c\u663e\u793a\u9632\u5fa1\u6210\u529f\u7387\u663e\u8457\u63d0\u5347\u4e14\u6a21\u578b\u6709\u7528\u6027\u672a\u53d7\u5f71\u54cd", "conclusion": "MAGIC\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u5bf9\u6297\u8bad\u7ec3\u6709\u6548\u63d0\u5347LLM\u5bf9\u65b0\u578b\u653b\u51fb\u7684\u6cdb\u5316\u9632\u5fa1\u80fd\u529b\uff0c\u4e3a\u5b89\u5168\u5bf9\u9f50\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2602.00526", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00526", "abs": "https://arxiv.org/abs/2602.00526", "authors": ["Kaiwen Zha", "Chao Li", "Hao He", "Peng Cao", "Tianhong Li", "Ali Mirzazadeh", "Ellen Zhang", "Jong Woo Lee", "Yoon Kim", "Dina Katabi"], "title": "Physiology as Language: Translating Respiration to Sleep EEG", "comment": "Tech report", "summary": "This paper introduces a novel cross-physiology translation task: synthesizing sleep electroencephalography (EEG) from respiration signals. To address the significant complexity gap between the two modalities, we propose a waveform-conditional generative framework that preserves fine-grained respiratory dynamics while constraining the EEG target space through discrete tokenization. Trained on over 28,000 individuals, our model achieves a 7% Mean Absolute Error in EEG spectrogram reconstruction. Beyond reconstruction, the synthesized EEG supports downstream tasks with performance comparable to ground truth EEG on age estimation (MAE 5.0 vs. 5.1 years), sex detection (AUROC 0.81 vs. 0.82), and sleep staging (Accuracy 0.84 vs. 0.88), significantly outperforming baselines trained directly on breathing. Finally, we demonstrate that the framework generalizes to contactless sensing by synthesizing EEG from wireless radio-frequency reflections, highlighting the feasibility of remote, non-contact neurological assessment during sleep.", "AI": {"tldr": "\u63d0\u51fa\u6ce2\u5f62\u6761\u4ef6\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u6563tokenization\u5c06\u547c\u5438\u4fe1\u53f7\u8f6c\u5316\u4e3a\u7761\u7720\u8111\u7535\uff0c\u91cd\u6784\u8bef\u5dee\u4ec57%\uff0c\u5728\u5e74\u9f84\u3001\u6027\u522b\u3001\u7761\u7720\u5206\u671f\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e2d\u6027\u80fd\u4e0e\u771f\u5b9e\u8111\u7535\u76f8\u5f53\uff0c\u5e76\u53ef\u63a8\u5e7f\u81f3\u65e0\u63a5\u89e6\u5c04\u9891\u4f20\u611f\u3002", "motivation": "\u89e3\u51b3\u547c\u5438\u4e0e\u8111\u7535\u4fe1\u53f7\u95f4\u7684\u590d\u6742\u6a21\u6001\u9e3f\u6c9f\uff0c\u5b9e\u73b0\u65e0\u63a5\u89e6\u5f0f\u7761\u7720\u795e\u7ecf\u8bc4\u4f30\uff0c\u6446\u8131\u4f20\u7edf\u8111\u7535\u56fe\u7684\u7e41\u7410\u7535\u6781\u8bbe\u7f6e\u3002", "method": "\u91c7\u7528\u6ce2\u5f62\u6761\u4ef6\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u6563tokenization\u7ea6\u675f\u8111\u7535\u76ee\u6807\u7a7a\u95f4\uff0c\u540c\u65f6\u4fdd\u7559\u547c\u5438\u4fe1\u53f7\u7ec6\u7c92\u5ea6\u52a8\u6001\u7279\u6027\uff0c\u57282.8\u4e07\u4e2a\u4f53\u6570\u636e\u4e0a\u8bad\u7ec3\u3002", "result": "\u8111\u7535\u8c31\u56fe\u91cd\u6784MAE\u4e3a7%\uff1b\u5e74\u9f84\u4f30\u8ba1MAE 5.0 vs 5.1\u5e74\uff0c\u6027\u522b\u68c0\u6d4bAUROC 0.81 vs 0.82\uff0c\u7761\u7720\u5206\u671f\u51c6\u786e\u73870.84 vs 0.88\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u547c\u5438\u4fe1\u53f7\u57fa\u7ebf\uff0c\u5e76\u53ef\u63a8\u5e7f\u81f3\u65e0\u7ebf\u5c04\u9891\u65e0\u63a5\u89e6\u4f20\u611f\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u547c\u5438\u5230\u8111\u7535\u7684\u8de8\u751f\u7406\u7ffb\u8bd1\uff0c\u4e3a\u8fdc\u7a0b\u65e0\u63a5\u89e6\u7761\u7720\u795e\u7ecf\u8bc4\u4f30\u63d0\u4f9b\u53ef\u884c\u65b9\u6848\uff0c\u6027\u80fd\u53ef\u5ab2\u7f8e\u4f20\u7edf\u8111\u7535\u65b9\u6cd5\u3002"}}
{"id": "2602.01550", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01550", "abs": "https://arxiv.org/abs/2602.01550", "authors": ["S1-NexusAgent Team"], "title": "S1-NexusAgent: a Self-Evolving Agent Framework for Multidisciplinary Scientific Research", "comment": "In progress", "summary": "Modern scientific research relies on large-scale data, complex workflows, and specialized tools, which existing LLMs and tool-based agents struggle to handle due to limitations in long-horizon planning, robust goal maintenance, and continual learning from execution. To address these issues, in this work, we propose S1-NexusAgent, a self-evolving agent framework designed for multidisciplinary scientific research. S1-NexusAgent adopts a hierarchical Plan-and-CodeAct execution paradigm, decoupling global scientific planning from subtask-level tool execution through a dual-loop architecture, thereby enabling stable modeling of complex research workflows. The system natively supports the Model Context Protocol (MCP), integrates up to thousands of cross-disciplinary scientific tools, and achieves efficient orchestration of heterogeneous research tools via intention-aware dynamic tool retrieval and hot-plug mechanisms. To address long-context and large-scale data challenges in scientific settings, S1-NexusAgent introduces object-reference-based sparse context management, which enables sub-task context isolation and intermediate result compression. Building on this, a Critic Agent automatically evaluates complete execution trajectories and distills high-quality research paths into reusable Scientific Skills, forming a closed loop for continuous self-evolution, which is valuable for sustainable and long-horizon scientific research. Experiments on authoritative scientific benchmarks involving long-horizon planning and complex specialized tool orchestration, including biomini-eval (biology), ChemBench (chemistry), and MatSciBench (material science), demonstrate that S1-NexusAgent achieves state-of-the-art performance, validating its effectiveness and generalization capability in complex scientific tasks.", "AI": {"tldr": "\u63d0\u51faS1-NexusAgent\u81ea\u8fdb\u5316\u79d1\u7814\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u89c4\u5212-\u6267\u884c\u67b6\u6784\u548c\u6301\u7eed\u5b66\u4e60\u673a\u5236\uff0c\u89e3\u51b3\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u957f\u7a0b\u89c4\u5212\u3001\u76ee\u6807\u7ef4\u62a4\u548c\u5de5\u5177\u534f\u540c\u6311\u6218\uff0c\u5728\u751f\u7269\u3001\u5316\u5b66\u3001\u6750\u6599\u79d1\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u79d1\u5b66\u7814\u7a76\u4f9d\u8d56\u5927\u89c4\u6a21\u6570\u636e\u3001\u590d\u6742\u5de5\u4f5c\u6d41\u548c\u4e13\u4e1a\u5de5\u5177\uff0c\u4f46\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5de5\u5177\u667a\u80fd\u4f53\u5728\u957f\u7a0b\u89c4\u5212\u3001\u9c81\u68d2\u76ee\u6807\u7ef4\u62a4\u548c\u6301\u7eed\u5b66\u4e60\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u96be\u4ee5\u5904\u7406\u8de8\u5b66\u79d1\u79d1\u7814\u4efb\u52a1\u3002", "method": "\u63d0\u51faS1-NexusAgent\u6846\u67b6\uff1a\u91c7\u7528\u5206\u5c42Plan-and-CodeAct\u8303\u5f0f\u4e0e\u53cc\u5faa\u73af\u67b6\u6784\u5206\u79bb\u5168\u5c40\u89c4\u5212\u548c\u5b50\u4efb\u52a1\u6267\u884c\uff1b\u539f\u751f\u652f\u6301MCP\u534f\u8bae\uff0c\u96c6\u6210\u6570\u5343\u8de8\u5b66\u79d1\u5de5\u5177\uff1b\u901a\u8fc7\u610f\u56fe\u611f\u77e5\u52a8\u6001\u5de5\u5177\u68c0\u7d22\u548c\u70ed\u63d2\u62d4\u673a\u5236\u5b9e\u73b0\u9ad8\u6548\u5de5\u5177\u534f\u540c\uff1b\u5f15\u5165\u57fa\u4e8e\u5bf9\u8c61\u5f15\u7528\u7684\u7a00\u758f\u4e0a\u4e0b\u6587\u7ba1\u7406\u89e3\u51b3\u957f\u6587\u672c\u548c\u5927\u89c4\u6a21\u6570\u636e\u6311\u6218\uff1b\u5229\u7528\u6279\u8bc4\u667a\u80fd\u4f53\u8bc4\u4f30\u6267\u884c\u8f68\u8ff9\u5e76\u63d0\u70bc\u53ef\u590d\u7528\u79d1\u7814\u6280\u80fd\uff0c\u5f62\u6210\u81ea\u8fdb\u5316\u95ed\u73af\u3002", "result": "\u5728biomini-eval\uff08\u751f\u7269\uff09\u3001ChemBench\uff08\u5316\u5b66\uff09\u3001MatSciBench\uff08\u6750\u6599\u79d1\u5b66\uff09\u7b49\u6743\u5a01\u79d1\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u590d\u6742\u79d1\u5b66\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "S1-NexusAgent\u6846\u67b6\u80fd\u591f\u6709\u6548\u652f\u6301\u53ef\u6301\u7eed\u548c\u957f\u7a0b\u79d1\u5b66\u7814\u7a76\uff0c\u4e3a\u5904\u7406\u590d\u6742\u79d1\u7814\u6d41\u7a0b\u548c\u5de5\u5177\u534f\u540c\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5177\u6709\u5728\u591a\u5b66\u79d1\u79d1\u7814\u573a\u666f\u4e2d\u63a8\u5e7f\u7684\u4ef7\u503c\u3002"}}
{"id": "2602.00533", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00533", "abs": "https://arxiv.org/abs/2602.00533", "authors": ["Core Francisco Park"], "title": "Convergent World Representations and Divergent Tasks", "comment": null, "summary": "While neural representations are central to modern deep learning, the conditions governing their geometry and their roles in downstream adaptability remain poorly understood. We develop a framework clearly separating the underlying world, the data generation process and the resulting model representations to study these questions in a controlled setup. 5,075 city coordinates define the world and 7 geometric tasks generate the training data for autoregressive training. We find that different tasks give rise to qualitatively and quantitatively distinct world representation geometries. However, multi-task training drives convergence of world representations: models trained on non-overlapping tasks develop aligned geometric representations, providing controlled evidence for the Multitask Scaling Hypothesis of the Platonic Representation Hypothesis. To study adaptation, we pretrain models on all tasks, then test whether new entities (cities) can be consistently integrated into the representation space via fine-tuning. Surprisingly, we find that despite multi-task pretraining, some tasks, which we call divergent, actively harm the representational integration of new entities and harm generalization. Our results show that training on multiple relational tasks reliably produces convergent world representations, but lurking divergent tasks can catastrophically harm new entity integration via fine-tuning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e0d\u540c\u4efb\u52a1\u5982\u4f55\u5851\u9020\u795e\u7ecf\u8868\u5f81\u51e0\u4f55\u7ed3\u6784\uff0c\u4ee5\u53ca\u591a\u4efb\u52a1\u8bad\u7ec3\u5bf9\u9002\u5e94\u65b0\u5b9e\u4f53\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u591a\u4efb\u52a1\u8bad\u7ec3\u80fd\u4ea7\u751f\u5bf9\u9f50\u7684\u8868\u5f81\uff0c\u4f46\u67d0\u4e9b\"\u53d1\u6563\u6027\"\u4efb\u52a1\u4f1a\u635f\u5bb3\u65b0\u5b9e\u4f53\u7684\u6574\u5408\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u795e\u7ecf\u8868\u5f81\u662f\u6df1\u5ea6\u5b66\u4e60\u7684\u6838\u5fc3\uff0c\u4f46\u5176\u51e0\u4f55\u7279\u6027\u53ca\u5728\u4e0b\u6d41\u4efb\u52a1\u4e2d\u7684\u9002\u5e94\u6027\u6761\u4ef6\u4ecd\u4e0d\u6e05\u695a\u3002", "method": "\u5f00\u53d1\u4e00\u4e2a\u53ef\u63a7\u6846\u67b6\uff0c\u5c06\u4e16\u754c\u3001\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u548c\u6a21\u578b\u8868\u5f81\u5206\u79bb\u3002\u4f7f\u75285,075\u4e2a\u57ce\u5e02\u5750\u6807\u5b9a\u4e49\u4e16\u754c\uff0c7\u4e2a\u51e0\u4f55\u4efb\u52a1\u751f\u6210\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u81ea\u56de\u5f52\u8bad\u7ec3\u3002\u901a\u8fc7\u9884\u8bad\u7ec3\u540e\u5fae\u8c03\u6d4b\u8bd5\u65b0\u5b9e\u4f53\u7684\u6574\u5408\u80fd\u529b\u3002", "result": "\u4e0d\u540c\u4efb\u52a1\u4ea7\u751f\u4e0d\u540c\u7684\u8868\u5f81\u51e0\u4f55\uff1b\u591a\u4efb\u52a1\u8bad\u7ec3\u9a71\u52a8\u8868\u5f81\u6536\u655b\u5bf9\u9f50\uff08\u652f\u6301\u67cf\u62c9\u56fe\u8868\u5f81\u5047\u8bf4\uff09\uff1b\u67d0\u4e9b\"\u53d1\u6563\u6027\"\u4efb\u52a1\u5373\u4f7f\u5728\u591a\u4efb\u52a1\u9884\u8bad\u7ec3\u540e\uff0c\u4ecd\u4f1a\u635f\u5bb3\u65b0\u5b9e\u4f53\u7684\u6574\u5408\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u591a\u4efb\u52a1\u8bad\u7ec3\u53ef\u9760\u5730\u4ea7\u751f\u6536\u655b\u7684\u4e16\u754c\u8868\u5f81\uff0c\u4f46\u6f5c\u5728\u7684\u53d1\u6563\u6027\u4efb\u52a1\u4f1a\u901a\u8fc7\u5fae\u8c03\u707e\u96be\u6027\u5730\u635f\u5bb3\u65b0\u5b9e\u4f53\u7684\u6574\u5408\u3002"}}
{"id": "2602.01556", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01556", "abs": "https://arxiv.org/abs/2602.01556", "authors": ["Hong Su"], "title": "Autonomous Question Formation for Large Language Model-Driven AI Systems", "comment": null, "summary": "Large language model (LLM)-driven AI systems are increasingly important for autonomous decision-making in dynamic and open environments. However, most existing systems rely on predefined tasks and fixed prompts, limiting their ability to autonomously identify what problems should be solved when environmental conditions change. In this paper, we propose a human-simulation-based framework that enables AI systems to autonomously form questions and set tasks by reasoning over their internal states, environmental observations, and interactions with other AI systems. The proposed method treats question formation as a first-class decision process preceding task selection and execution, and integrates internal-driven, environment-aware, and inter-agent-aware prompting scopes to progressively expand cognitive coverage. In addition, the framework supports learning the question-formation process from experience, allowing the system to improve its adaptability and decision quality over time. xperimental results in a multi-agent simulation environment show that environment-aware prompting significantly reduces no-eat events compared with the internal-driven baseline, and inter-agent-aware prompting further reduces cumulative no-eat events by more than 60% over a 20-day simulation, with statistically significant improvements (p < 0.05).", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4eba\u7c7b\u6a21\u62df\u7684AI\u6846\u67b6\uff0c\u4f7f\u7cfb\u7edf\u80fd\u901a\u8fc7\u5185\u90e8\u9a71\u52a8\u3001\u73af\u5883\u611f\u77e5\u548c\u667a\u80fd\u4f53\u4ea4\u4e92\u4e09\u9636\u6bb5\u63d0\u793a\u673a\u5236\u81ea\u4e3b\u5f62\u6210\u95ee\u9898\u5e76\u8bbe\u5b9a\u4efb\u52a1\uff0c\u572820\u5929\u591a\u667a\u80fd\u4f53\u6a21\u62df\u4e2d\u4f7f\"\u672a\u8fdb\u98df\"\u4e8b\u4ef6\u51cf\u5c1160%\u4ee5\u4e0a", "motivation": "\u73b0\u6709LLM\u9a71\u52a8\u7684AI\u7cfb\u7edf\u4f9d\u8d56\u9884\u5b9a\u4e49\u4efb\u52a1\u548c\u56fa\u5b9a\u63d0\u793a\uff0c\u96be\u4ee5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u81ea\u4e3b\u8bc6\u522b\u9700\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u7f3a\u4e4f\u73af\u5883\u53d8\u5316\u65f6\u7684\u81ea\u9002\u5e94\u80fd\u529b", "method": "\u6784\u5efa\u4eba\u7c7b\u6a21\u62df\u6846\u67b6\uff0c\u5c06\u95ee\u9898\u5f62\u6210\u4f5c\u4e3a\u4efb\u52a1\u6267\u884c\u524d\u7684\u9996\u8981\u51b3\u7b56\u8fc7\u7a0b\uff1b\u91c7\u7528\u5185\u90e8\u9a71\u52a8\u2192\u73af\u5883\u611f\u77e5\u2192\u667a\u80fd\u4f53\u4ea4\u4e92\u7684\u4e09\u9636\u6bb5\u63d0\u793a\u673a\u5236\u6269\u5c55\u8ba4\u77e5\u8986\u76d6\uff1b\u652f\u6301\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u4f18\u5316\u95ee\u9898\u5f62\u6210\u8fc7\u7a0b", "result": "\u5728\u591a\u667a\u80fd\u4f53\u6a21\u62df\u73af\u5883\u4e2d\uff1a\u73af\u5883\u611f\u77e5\u63d0\u793a\u6bd4\u5185\u90e8\u9a71\u52a8\u57fa\u7ebf\u663e\u8457\u51cf\u5c11\"\u672a\u8fdb\u98df\"\u4e8b\u4ef6\uff1b\u667a\u80fd\u4f53\u4ea4\u4e92\u63d0\u793a\u572820\u5929\u6a21\u62df\u4e2d\u8fdb\u4e00\u6b65\u964d\u4f4e60%\u4ee5\u4e0a\u7d2f\u79ef\u4e8b\u4ef6\uff08p<0.05\uff09", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u81ea\u4e3b\u95ee\u9898\u5f62\u6210\u673a\u5236\u548c\u6e10\u8fdb\u5f0f\u63d0\u793a\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347AI\u7cfb\u7edf\u5728\u5f00\u653e\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u4e0e\u51b3\u7b56\u8d28\u91cf\uff0c\u4e3a\u81ea\u4e3b\u51b3\u7b56\u7cfb\u7edf\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2602.01608", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01608", "abs": "https://arxiv.org/abs/2602.01608", "authors": ["Mu Yuan", "Liekang Zeng", "Guoliang Xing", "Lan Zhang", "Yunhao Liu"], "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts", "comment": null, "summary": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation.", "AI": {"tldr": "\u63d0\u51fa\"\u534f\u540c\u601d\u7ef4\"\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u6a21\u578b\u89c4\u5212\u3001\u6269\u6563\u6a21\u578b\u751f\u6210\u89c6\u89c9\u601d\u7ef4\u3001\u89c6\u89c9\u8bc4\u5224\u6a21\u5757\u53cd\u9988\u7684\u95ed\u73af\u4ea4\u4e92\uff0c\u5b9e\u73b0\u4e24\u7c7b\u6a21\u578b\u7684\u534f\u540c\u63a8\u7406\u3002", "motivation": "\u81ea\u56de\u5f52\u6a21\u578b\u7f3a\u4e4f\u7a7a\u95f4/\u7269\u7406\u57fa\u7840\uff0c\u6269\u6563\u6a21\u578b\u96be\u4ee5\u6ee1\u8db3\u590d\u6742\u7ea6\u675f\u7684\u9010\u6b65\u903b\u8f91\u63a7\u5236\uff0c\u9700\u8981\u7ed3\u5408\u4e24\u8005\u4e92\u8865\u4f18\u52bf\u4ee5\u63d0\u5347\u7a7a\u95f4\u63a8\u7406\u53ef\u9760\u6027\u548c\u751f\u6210\u53ef\u63a7\u6027\u3002", "method": "\u6784\u5efa\u534f\u540c\u601d\u7ef4\u6846\u67b6\uff1a\u81ea\u56de\u5f52\u6a21\u578b\u8d1f\u8d23\u7ed3\u6784\u5316\u89c4\u5212\uff0c\u6269\u6563\u6a21\u578b\u5b9e\u4f8b\u5316\u7ea6\u675f\u4e3a\u4e2d\u95f4\u89c6\u89c9\u601d\u7ef4\uff0c\u89c6\u89c9\u8bc4\u5224\u6a21\u5757\u8bc4\u4f30\u662f\u5426\u6ee1\u8db3\u7ed3\u6784\u7269\u7406\u8981\u6c42\uff0c\u901a\u8fc7\u53cd\u9988\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u5728\u81ea\u56de\u5f52\u95ee\u7b54\u548c\u6269\u6563\u89c6\u89c9\u751f\u6210\u4efb\u52a1\u4e2d\u5747\u80fd\u63d0\u5347\u7a7a\u95f4\u63a8\u7406\u53ef\u9760\u6027\u4e0e\u751f\u6210\u53ef\u63a7\u6027\uff0c\u5e76\u7f13\u89e3\u8de8\u6a21\u6001\u9519\u8bef\u4f20\u64ad\u3002", "conclusion": "\u534f\u540c\u601d\u7ef4\u901a\u8fc7\u95ed\u73af\u4ea4\u4e92\u6210\u529f\u7edf\u4e00\u4e92\u8865\u751f\u6210\u8303\u5f0f\uff0c\u8bc1\u660e\u81ea\u56de\u5f52\u4e0e\u6269\u6563\u6a21\u578b\u7684\u8054\u5408\u63a8\u7406\u80fd\u514b\u670d\u5404\u81ea\u5c40\u9650\u3002"}}
{"id": "2602.01610", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01610", "abs": "https://arxiv.org/abs/2602.01610", "authors": ["Zitao Guo", "Changyang Jiang", "Tianhong Zhao", "Jinzhou Cao", "Genan Dai", "Bowen Zhang"], "title": "ToPT: Task-Oriented Prompt Tuning for Urban Region Representation Learning", "comment": "The paper has been accepted by ICASSP 2026", "summary": "Learning effective region embeddings from heterogeneous urban data underpins key urban computing tasks (e.g., crime prediction, resource allocation). However, prevailing two-stage methods yield task-agnostic representations, decoupling them from downstream objectives. Recent prompt-based approaches attempt to fix this but introduce two challenges: they often lack explicit spatial priors, causing spatially incoherent inter-region modeling, and they lack robust mechanisms for explicit task-semantic alignment. We propose ToPT, a two-stage framework that delivers spatially consistent fusion and explicit task alignment. ToPT consists of two modules: spatial-aware region embedding learning (SREL) and task-aware prompting for region embeddings (Prompt4RE). SREL employs a Graphormer-based fusion module that injects spatial priors-distance and regional centrality-as learnable attention biases to capture coherent, interpretable inter-region interactions. Prompt4RE performs task-oriented prompting: a frozen multimodal large language model (MLLM) processes task-specific templates to obtain semantic vectors, which are aligned with region embeddings via multi-head cross-attention for stable task conditioning. Experiments across multiple tasks and cities show state-of-the-art performance, with improvements of up to 64.2\\%, validating the necessity and complementarity of spatial priors and prompt-region alignment. The code is available at https://github.com/townSeven/Prompt4RE.git.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faToPT\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u611f\u77e5\u7684\u533a\u57df\u5d4c\u5165\u5b66\u4e60\u548c\u4efb\u52a1\u611f\u77e5\u7684\u63d0\u793a\u673a\u5236\uff0c\u89e3\u51b3\u57ce\u5e02\u533a\u57df\u5d4c\u5165\u8868\u793a\u4e2d\u7a7a\u95f4\u5148\u9a8c\u7f3a\u5931\u548c\u4efb\u52a1\u8bed\u4e49\u5bf9\u9f50\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5728\u591a\u9879\u57ce\u5e02\u8ba1\u7b97\u4efb\u52a1\u4e0a\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u4e24\u9636\u6bb5\u65b9\u6cd5\u751f\u6210\u7684\u533a\u57df\u8868\u793a\u4e0e\u4e0b\u6e38\u4efb\u52a1\u76ee\u6807\u8131\u8282\uff0c\u800c\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u7f3a\u4e4f\u663e\u5f0f\u7a7a\u95f4\u5148\u9a8c\u548c\u7a33\u5065\u7684\u4efb\u52a1\u8bed\u4e49\u5bf9\u9f50\u673a\u5236\uff0c\u5bfc\u81f4\u533a\u57df\u95f4\u5efa\u6a21\u7a7a\u95f4\u4e0d\u4e00\u81f4\u4e14\u4efb\u52a1\u9002\u5e94\u6027\u5dee\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6ToPT\uff1a1) SREL\u6a21\u5757\u91c7\u7528Graphormer-based\u878d\u5408\u7ed3\u6784\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u6ce8\u610f\u529b\u504f\u7f6e\u6ce8\u5165\u8ddd\u79bb\u548c\u533a\u57df\u4e2d\u5fc3\u6027\u7a7a\u95f4\u5148\u9a8c\uff1b2) Prompt4RE\u6a21\u5757\u5229\u7528\u51bb\u7ed3\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u4efb\u52a1\u6a21\u677f\uff0c\u901a\u8fc7\u591a\u5934\u4ea4\u53c9\u6ce8\u610f\u529b\u5b9e\u73b0\u533a\u57df\u5d4c\u5165\u4e0e\u4efb\u52a1\u8bed\u4e49\u5411\u91cf\u7684\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u57ce\u5e02\u591a\u4efb\u52a1\u5b9e\u9a8c\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u6700\u5927\u63d0\u5347\u8fbe64.2%\uff0c\u9a8c\u8bc1\u4e86\u7a7a\u95f4\u5148\u9a8c\u548c\u63d0\u793a-\u533a\u57df\u5bf9\u9f50\u7684\u5fc5\u8981\u6027\u4e0e\u4e92\u8865\u6027\u3002", "conclusion": "\u7a7a\u95f4\u5148\u9a8c\u6ce8\u5165\u548c\u4efb\u52a1\u611f\u77e5\u63d0\u793a\u673a\u5236\u80fd\u6709\u6548\u63d0\u5347\u533a\u57df\u5d4c\u5165\u8d28\u91cf\uff0c\u4e3a\u57ce\u5e02\u8ba1\u7b97\u4efb\u52a1\u63d0\u4f9b\u66f4\u4f18\u7684\u8868\u793a\u5b66\u4e60\u65b9\u6848\u3002"}}
{"id": "2602.00539", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00539", "abs": "https://arxiv.org/abs/2602.00539", "authors": ["Xinmo Jin", "Bowen Fan", "Xunkai Li", "Henan Sun", "YuXin Zeng", "Zekai Chen", "Yuxuan Sun", "Jia Li", "Qiangqiang Dai", "Hongchao Qin", "Rong-Hua Li", "Guoren Wang"], "title": "OpenDDI: A Comprehensive Benchmark for DDI Prediction", "comment": null, "summary": "Drug-Drug Interactions (DDIs) significantly influence therapeutic efficacy and patient safety. As experimental discovery is resource-intensive and time-consuming, efficient computational methodologies have become essential. The predominant paradigm formulates DDI prediction as a drug graph-based link prediction task. However, further progress is hindered by two fundamental challenges: (1) lack of high-quality data: most studies rely on small-scale DDI datasets and single-modal drug representations; (2) lack of standardized evaluation: inconsistent scenarios, varied metrics, and diverse baselines. To address the above issues, we propose OpenDDI, a comprehensive benchmark for DDI prediction. Specifically, (1) from the data perspective, OpenDDI unifies 6 widely used DDI datasets and 2 existing forms of drug representation, while additionally contributing 3 new large-scale LLM-augmented datasets and a new multimodal drug representation covering 5 modalities. (2) From the evaluation perspective, OpenDDI unifies 20 SOTA model baselines across 3 downstream tasks, with standardized protocols for data quality, effectiveness, generalization, robustness, and efficiency. Based on OpenDDI, we conduct a comprehensive evaluation and derive 10 valuable insights for DDI prediction while exposing current limitations to provide critical guidance for this rapidly evolving field. Our code is available at https://github.com/xiaoriwuguang/OpenDDI", "AI": {"tldr": "OpenDDI is a comprehensive benchmark for drug-drug interaction prediction that addresses data quality and evaluation standardization issues through unified datasets, multimodal representations, and standardized evaluation of 20 SOTA models.", "motivation": "Current DDI prediction suffers from limited high-quality data (small-scale datasets, single-modal representations) and inconsistent evaluation standards (varied scenarios, metrics, baselines), hindering progress in this critical patient safety domain.", "method": "Proposes OpenDDI benchmark: (1) Unifies 6 existing DDI datasets and creates 3 new large-scale LLM-augmented datasets with novel 5-modality drug representation; (2) Establishes standardized evaluation protocols across 3 tasks with 20 SOTA baselines covering data quality, effectiveness, generalization, robustness and efficiency.", "result": "Comprehensive evaluation yields 10 actionable insights for DDI prediction while revealing current limitations; provides critical guidance for the field through systematic model comparison and standardized benchmarking.", "conclusion": "OpenDDI serves as an essential foundation for advancing DDI prediction research by resolving data fragmentation and evaluation inconsistency, accelerating development of more reliable computational methods for patient safety."}}
{"id": "2602.00541", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00541", "abs": "https://arxiv.org/abs/2602.00541", "authors": ["Zilin Jing", "Vincent Jeanselme", "Yuta Kobayashi", "Simon A. Lee", "Chao Pang", "Aparajita Kashyap", "Yanwei Li", "Xinzhuo Jiang", "Shalmali Joshi"], "title": "One Loss to Rule Them All: Marked Time-to-Event for Structured EHR Foundation Models", "comment": null, "summary": "Clinical events captured in Electronic Health Records (EHR) are irregularly sampled and may consist of a mixture of discrete events and numerical measurements, such as laboratory values or treatment dosages. The sequential nature of EHR, analogous to natural language, has motivated the use of next-token prediction to train prior EHR Foundation Models (FMs) over events. However, this training fails to capture the full structure of EHR. We propose ORA, a marked time-to-event pretraining objective that jointly models event timing and associated measurements. Across multiple datasets, downstream tasks, and model architectures, this objective consistently yields more generalizable representations than next-token prediction and pretraining losses that ignore continuous measurements. Importantly, the proposed objective yields improvements beyond traditional classification evaluation, including better regression and time-to-event prediction. Beyond introducing a new family of FMs, our results suggest a broader takeaway: pretraining objectives that account for EHR structure are critical for expanding downstream capabilities and generalizability", "AI": {"tldr": "Proposes ORA, a marked time-to-event pretraining objective for EHR foundation models that jointly predicts event timing and measurements, outperforming next-token prediction across diverse tasks and architectures.", "motivation": "EHR data is irregularly sampled with mixed discrete events and numerical measurements. Current foundation models using next-token prediction fail to capture this full structure, limiting their generalizability.", "method": "Introduces ORA, a pretraining objective that jointly models event timing and associated measurements using a marked time-to-event approach, better capturing EHR data structure.", "result": "ORA yields more generalizable representations than next-token prediction across multiple datasets and model architectures, improving not just classification but also regression and time-to-event prediction.", "conclusion": "Pretraining objectives that account for EHR structure are critical for expanding downstream capabilities and generalizability, suggesting a new direction for EHR foundation models."}}
{"id": "2602.01664", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01664", "abs": "https://arxiv.org/abs/2602.01664", "authors": ["Mingda Zhang", "Haoran Luo", "Tiesunlong Shen", "Qika Lin", "Xiaoying Tang", "Rui Mao", "Erik Cambria"], "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning", "comment": "41 pages, 7 figures, 6 tables. Project page: http://flowsteer.org/", "summary": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks.", "AI": {"tldr": "\u63d0\u51faFlowSteer\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7b56\u7565\u6a21\u578b\u4e0e\u53ef\u6267\u884c\u753b\u5e03\u73af\u5883\u7684\u591a\u8f6e\u4ea4\u4e92\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u7f16\u6392\uff0c\u5e76\u8bbe\u8ba1CWRPO\u7b97\u6cd5\u7a33\u5b9a\u8bad\u7ec3\uff0c\u572812\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u6d41\u7f16\u6392\u9762\u4e34\u4eba\u5de5\u6210\u672c\u9ad8\u3001\u4f9d\u8d56\u7279\u5b9a\u7b97\u5b50/\u5927\u8bed\u8a00\u6a21\u578b\u3001\u5956\u52b1\u4fe1\u53f7\u7a00\u758f\u7b49\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51faFlowSteer\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u8f7b\u91cf\u7b56\u7565\u6a21\u578b\u4e3a\u667a\u80fd\u4f53\uff0c\u53ef\u6267\u884c\u753b\u5e03\u73af\u5883\u4e3a\u4ea4\u4e92\u5e73\u53f0\uff0c\u901a\u8fc7\u591a\u8f6e\u8fed\u4ee3\u4f18\u5316\u5b9e\u73b0\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u7f16\u6392\u3002\u6846\u67b6\u652f\u6301\u5373\u63d2\u5373\u7528\u7684\u7b97\u5b50\u5e93\u548c\u53ef\u4e92\u6362\u7684LLM\u540e\u7aef\u3002\u540c\u65f6\u63d0\u51faCanvas Workflow Relative Policy Optimization (CWRPO)\u7b97\u6cd5\uff0c\u5f15\u5165\u591a\u6837\u6027\u7ea6\u675f\u5956\u52b1\u4e0e\u6761\u4ef6\u91ca\u653e\u673a\u5236\u4ee5\u7a33\u5b9a\u5b66\u4e60\u5e76\u6291\u5236\u6377\u5f84\u884c\u4e3a\u3002", "result": "\u572812\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFlowSteer\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FlowSteer\u4e3a\u5de5\u4f5c\u6d41\u7f16\u6392\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2602.01675", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01675", "abs": "https://arxiv.org/abs/2602.01675", "authors": ["Yuanzhe Shen", "Zisu Huang", "Zhengyuan Wang", "Muzhao Tian", "Zhengkang Guo", "Chenyang Zhang", "Shuaiyu Zhou", "Zengjie Hu", "Dailin Li", "Jingwen Xu", "Kaimin Wang", "Wenhao Liu", "Tianlong Li", "Fengpeng Yue", "Feng Hong", "Cao Liu", "Ke Zeng"], "title": "TRIP-Bench: A Benchmark for Long-Horizon Interactive Agents in Real-World Scenarios", "comment": "40 pages, 6figures", "summary": "As LLM-based agents are deployed in increasingly complex real-world settings, existing benchmarks underrepresent key challenges such as enforcing global constraints, coordinating multi-tool reasoning, and adapting to evolving user behavior over long, multi-turn interactions. To bridge this gap, we introduce \\textbf{TRIP-Bench}, a long-horizon benchmark grounded in realistic travel-planning scenarios. TRIP-Bench leverages real-world data, offers 18 curated tools and 40+ travel requirements, and supports automated evaluation. It includes splits of varying difficulty; the hard split emphasizes long and ambiguous interactions, style shifts, feasibility changes, and iterative version revision. Dialogues span up to 15 user turns, can involve 150+ tool calls, and may exceed 200k tokens of context. Experiments show that even advanced models achieve at most 50\\% success on the easy split, with performance dropping below 10\\% on hard subsets. We further propose \\textbf{GTPO}, an online multi-turn reinforcement learning method with specialized reward normalization and reward differencing. Applied to Qwen2.5-32B-Instruct, GTPO improves constraint satisfaction and interaction robustness, outperforming Gemini-3-Pro in our evaluation. We expect TRIP-Bench to advance practical long-horizon interactive agents, and GTPO to provide an effective online RL recipe for robust long-horizon training.", "AI": {"tldr": "\u63d0\u51faTRIP-Bench\u957f\u5468\u671f\u65c5\u884c\u89c4\u5212\u57fa\u51c6\u6d4b\u8bd5\u548cGTPO\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u5728\u5168\u5c40\u7ea6\u675f\u3001\u591a\u5de5\u5177\u534f\u8c03\u548c\u957f\u5bf9\u8bdd\u9002\u5e94\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u5bf9\u5168\u5c40\u7ea6\u675f\u6267\u884c\u3001\u591a\u5de5\u5177\u63a8\u7406\u534f\u8c03\u4ee5\u53ca\u957f\u591a\u8f6e\u4ea4\u4e92\u4e2d\u7528\u6237\u884c\u4e3a\u9002\u5e94\u7b49\u5173\u952e\u6311\u6218\u7684\u8bc4\u4f30\u3002", "method": "1) \u6784\u5efaTRIP-Bench\u57fa\u51c6\uff1a\u57fa\u4e8e\u771f\u5b9e\u65c5\u884c\u89c4\u5212\u573a\u666f\uff0c\u5305\u542b18\u4e2a\u5de5\u5177\u300140+\u9700\u6c42\uff0c\u652f\u6301\u81ea\u52a8\u8bc4\u4f30\uff0c\u5bf9\u8bdd\u6700\u957f15\u8f6e\u3001150+\u5de5\u5177\u8c03\u7528\u3001200k+token\u4e0a\u4e0b\u6587\uff1b2) \u63d0\u51faGTPO\u65b9\u6cd5\uff1a\u5728\u7ebf\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\uff0c\u91c7\u7528\u4e13\u95e8\u5956\u52b1\u5f52\u4e00\u5316\u548c\u5956\u52b1\u5dee\u5206\u6280\u672f\u3002", "result": "\u5148\u8fdb\u6a21\u578b\u5728\u7b80\u5355\u96c6\u6700\u9ad8\u6210\u529f\u7387\u4ec550%\uff0c\u56f0\u96be\u96c6\u4f4e\u4e8e10%\uff1bGTPO\u5728Qwen2.5-32B-Instruct\u4e0a\u663e\u8457\u63d0\u5347\u7ea6\u675f\u6ee1\u8db3\u548c\u4ea4\u4e92\u9c81\u68d2\u6027\uff0c\u4f18\u4e8eGemini-3-Pro\u3002", "conclusion": "TRIP-Bench\u5c06\u63a8\u52a8\u5b9e\u9645\u957f\u5468\u671f\u4ea4\u4e92\u667a\u80fd\u4f53\u53d1\u5c55\uff0cGTPO\u4e3a\u9c81\u68d2\u957f\u5468\u671f\u8bad\u7ec3\u63d0\u4f9b\u6709\u6548\u5728\u7ebfRL\u65b9\u6848\u3002"}}
{"id": "2602.00547", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00547", "abs": "https://arxiv.org/abs/2602.00547", "authors": ["Seunghyun Yoo", "Sanghong Kim", "Namkyung Yoon", "Hwangnam Kim"], "title": "Contrastive Domain Generalization for Cross-Instrument Molecular Identification in Mass Spectrometry", "comment": "8 pages, 2 figures", "summary": "Identifying molecules from mass spectrometry (MS) data remains a fundamental challenge due to the semantic gap between physical spectral peaks and underlying chemical structures. Existing deep learning approaches often treat spectral matching as a closed-set recognition task, limiting their ability to generalize to unseen molecular scaffolds. To overcome this limitation, we propose a cross-modal alignment framework that directly maps mass spectra into the chemically meaningful molecular structure embedding space of a pretrained chemical language model. On a strict scaffold-disjoint benchmark, our model achieves a Top-1 accuracy of 42.2% in fixed 256-way zero-shot retrieval and demonstrates strong generalization under a global retrieval setting. Moreover, the learned embedding space demonstrates strong chemical coherence, reaching 95.4% accuracy in 5-way 5-shot molecular re-identification. These results suggest that explicitly integrating physical spectral resolution with molecular structure embedding is key to solving the generalization bottleneck in molecular identification from MS data.", "AI": {"tldr": "\u63d0\u51fa\u8de8\u6a21\u6001\u5bf9\u9f50\u6846\u67b6\uff0c\u5c06\u8d28\u8c31\u76f4\u63a5\u6620\u5c04\u5230\u9884\u8bad\u7ec3\u5316\u5b66\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u6784\u5d4c\u5165\u7a7a\u95f4\uff0c\u5728\u96f6\u6837\u672c\u68c0\u7d22\u4e2d\u5b9e\u73b042.2%\u7684Top-1\u51c6\u786e\u7387\uff0c\u89e3\u51b3\u4e86\u5206\u5b50\u8bc6\u522b\u7684\u6cdb\u5316\u74f6\u9888\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5c06\u8d28\u8c31\u5339\u914d\u89c6\u4e3a\u5c01\u95ed\u96c6\u8bc6\u522b\u4efb\u52a1\uff0c\u65e0\u6cd5\u6cdb\u5316\u5230\u672a\u89c1\u5206\u5b50\u9aa8\u67b6\uff0c\u5b58\u5728\u8bed\u4e49\u9e3f\u6c9f\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u8de8\u6a21\u6001\u5bf9\u9f50\u6846\u67b6\uff0c\u76f4\u63a5\u5c06\u8d28\u8c31\u6620\u5c04\u5230\u9884\u8bad\u7ec3\u5316\u5b66\u8bed\u8a00\u6a21\u578b\u7684\u5316\u5b66\u6709\u610f\u4e49\u5206\u5b50\u7ed3\u6784\u5d4c\u5165\u7a7a\u95f4\u4e2d\u3002", "result": "\u5728\u4e25\u683c\u9aa8\u67b6\u4e0d\u76f8\u4ea4\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c256\u7c7b\u96f6\u6837\u672c\u68c0\u7d22Top-1\u51c6\u786e\u7387\u8fbe42.2%\uff1b5\u7c7b5\u6837\u672c\u91cd\u8bc6\u522b\u51c6\u786e\u738795.4%\uff0c\u5d4c\u5165\u7a7a\u95f4\u8868\u73b0\u51fa\u5f3a\u5316\u5b66\u4e00\u81f4\u6027\u3002", "conclusion": "\u663e\u5f0f\u6574\u5408\u7269\u7406\u5149\u8c31\u5206\u8fa8\u7387\u4e0e\u5206\u5b50\u7ed3\u6784\u5d4c\u5165\u662f\u89e3\u51b3\u8d28\u8c31\u5206\u5b50\u8bc6\u522b\u6cdb\u5316\u74f6\u9888\u7684\u5173\u952e\u3002"}}
{"id": "2602.01689", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01689", "abs": "https://arxiv.org/abs/2602.01689", "authors": ["Yongchan Kwon", "James Zou"], "title": "What LLMs Think When You Don't Tell Them What to Think About?", "comment": "NA", "summary": "Characterizing the behavior of large language models (LLMs) across diverse settings is critical for reliable monitoring and AI safety. However, most existing analyses rely on topic- or task-specific prompts, which can substantially limit what can be observed. In this work, we study what LLMs generate from minimal, topic-neutral inputs and probe their near-unconstrained generative behavior. Despite the absence of explicit topics, model outputs cover a broad semantic space, and surprisingly, each model family exhibits strong and systematic topical preferences. GPT-OSS predominantly generates programming (27.1%) and mathematical content (24.6%), whereas Llama most frequently generates literary content (9.1%). DeepSeek often generates religious content, while Qwen frequently generates multiple-choice questions. Beyond topical preferences, we also observe differences in content specialization and depth: GPT-OSS often generates more technically advanced content (e.g., dynamic programming) compared with other models (e.g., basic Python). Furthermore, we find that the near-unconstrained generation often degenerates into repetitive phrases, revealing interesting behaviors unique to each model family. For instance, degenerate outputs from Llama include multiple URLs pointing to personal Facebook and Instagram accounts. We release the complete dataset of 256,000 samples from 16 LLMs, along with a reproducible codebase.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6781\u7b80\u4e2d\u6027\u8f93\u5165\u63a2\u6d4b16\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u7531\u751f\u6210\u884c\u4e3a\uff0c\u53d1\u73b0\u5404\u6a21\u578b\u5bb6\u65cf\u5b58\u5728\u7cfb\u7edf\u6027\u4e3b\u9898\u504f\u597d\uff08\u5982GPT-OSS\u504f\u91cd\u7f16\u7a0b/\u6570\u5b66\uff0cLlama\u504f\u597d\u6587\u5b66\uff09\uff0c\u5e76\u63ed\u793a\u5185\u5bb9\u6df1\u5ea6\u5dee\u5f02\u53ca\u91cd\u590d\u6027\u9000\u5316\u73b0\u8c61", "motivation": "\u73b0\u6709LLM\u884c\u4e3a\u5206\u6790\u8fc7\u5ea6\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\uff0c\u65e0\u6cd5\u5168\u9762\u89c2\u6d4b\u6a21\u578b\u771f\u5b9e\u751f\u6210\u80fd\u529b\uff1b\u4e9f\u9700\u901a\u8fc7\u4e2d\u6027\u8f93\u5165\u63a2\u7d22\u6a21\u578b\u65e0\u7ea6\u675f\u4e0b\u7684\u5185\u5728\u751f\u6210\u6a21\u5f0f\uff0c\u4ee5\u63d0\u5347AI\u5b89\u5168\u76d1\u63a7\u53ef\u9760\u6027", "method": "\u541116\u4e2aLLM\u8f93\u5165\u6781\u7b80\u4e3b\u9898\u4e2d\u6027\u63d0\u793a\uff0c\u5206\u6790\u5176\u8fd1\u65e0\u7ea6\u675f\u751f\u6210\u5185\u5bb9\uff1b\u901a\u8fc7\u8bed\u4e49\u7a7a\u95f4\u8986\u76d6\u5ea6\u3001\u4e3b\u9898\u5206\u5e03\u7edf\u8ba1\u3001\u5185\u5bb9\u6df1\u5ea6\u8bc4\u4f30\u53ca\u9000\u5316\u6a21\u5f0f\u8bc6\u522b\u8fdb\u884c\u7cfb\u7edf\u6027\u63a2\u6d4b", "result": "1) \u5404\u6a21\u578b\u5bb6\u65cf\u5448\u73b0\u5f3a\u4e3b\u9898\u504f\u597d\uff1aGPT-OSS\u751f\u621027.1%\u7f16\u7a0b+24.6%\u6570\u5b66\u5185\u5bb9\uff0cLlama 9.1%\u4e3a\u6587\u5b66\uff0cDeepSeek\u503e\u5411\u5b97\u6559\u5185\u5bb9\uff0cQwen\u9ad8\u9891\u8f93\u51fa\u9009\u62e9\u9898\uff1b2) GPT-OSS\u6280\u672f\u6df1\u5ea6\u663e\u8457\u9ad8\u4e8e\u4ed6\u6a21\u578b\uff08\u5982\u52a8\u6001\u7f16\u7a0bvs\u57fa\u7840Python\uff09\uff1b3) \u666e\u904d\u5b58\u5728\u91cd\u590d\u6027\u9000\u5316\uff08\u5982Llama\u8f93\u51fa\u542b\u5927\u91cfFacebook/Instagram\u94fe\u63a5\uff09", "conclusion": "LLM\u5728\u81ea\u7531\u751f\u6210\u4e2d\u66b4\u9732\u56fa\u6709\u4e3b\u9898\u504f\u5411\u4e0e\u6280\u672f\u5206\u5c42\uff0c\u9000\u5316\u6a21\u5f0f\u5177\u5bb6\u65cf\u7279\u5f02\u6027\uff1b\u53d1\u5e0325.6\u4e07\u6837\u672c\u6570\u636e\u96c6\u53ca\u4ee3\u7801\u5e93\u4e3aAI\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u65b0\u57fa\u51c6"}}
{"id": "2602.00549", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00549", "abs": "https://arxiv.org/abs/2602.00549", "authors": ["Kezhao Lai", "Yutao Lai", "Hai-Lin Liu"], "title": "Beyond the Node: Clade-level Selection for Efficient MCTS in Automatic Heuristic Design", "comment": null, "summary": "While Monte Carlo Tree Search (MCTS) shows promise in Large Language Model (LLM) based Automatic Heuristic Design (AHD), it suffers from a critical over-exploitation tendency under the limited computational budgets required for heuristic evaluation. To address this limitation, we propose Clade-AHD, an efficient framework that replaces node-level point estimates with clade-level Bayesian beliefs. By aggregating descendant evaluations into Beta distributions and performing Thompson Sampling over these beliefs, Clade-AHD explicitly models uncertainty to guide exploration, enabling more reliable decision-making under sparse and noisy evaluations. Extensive experiments on complex combinatorial optimization problems demonstrate that Clade-AHD consistently outperforms state-of-the-art methods while significantly reducing computational cost. The source code is publicly available at: https://github.com/Mriya0306/Clade-AHD.", "AI": {"tldr": "Clade-AHD\u7528clade\u7ea7\u8d1d\u53f6\u65af\u4fe1\u5ff5\u548c\u6c64\u666e\u68ee\u91c7\u6837\u66ff\u4ee3MCTS\u7684\u8282\u70b9\u70b9\u4f30\u8ba1\uff0c\u89e3\u51b3LLM\u81ea\u52a8\u542f\u53d1\u5f0f\u8bbe\u8ba1\u4e2d\u7684\u8fc7\u5ea6\u63a2\u7d22\u95ee\u9898\uff0c\u4ee5\u66f4\u4f4e\u6210\u672c\u5b9e\u73b0\u66f4\u4f18\u6027\u80fd", "motivation": "\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u5728\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u542f\u53d1\u5f0f\u8bbe\u8ba1\u4e2d\uff0c\u5728\u6709\u9650\u8ba1\u7b97\u9884\u7b97\u4e0b\u5b58\u5728\u4e25\u91cd\u7684\u8fc7\u5ea6\u5229\u7528\u503e\u5411\uff0c\u5bfc\u81f4\u6b21\u4f18\u542f\u53d1\u5f0f\u8bbe\u8ba1", "method": "\u63d0\u51faClade-AHD\u6846\u67b6\uff0c\u5c06\u540e\u4ee3\u8bc4\u4f30\u805a\u5408\u4e3aBeta\u5206\u5e03\uff0c\u901a\u8fc7clade\u7ea7\u8d1d\u53f6\u65af\u4fe1\u5ff5\u5efa\u6a21\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u4f7f\u7528\u6c64\u666e\u68ee\u91c7\u6837\u6307\u5bfc\u63a2\u7d22", "result": "\u5728\u590d\u6742\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e0a\uff0cClade-AHD\u6301\u7eed\u4f18\u4e8e\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u4e0d\u786e\u5b9a\u6027\uff0cClade-AHD\u6709\u6548\u89e3\u51b3\u4e86MCTS\u5728AHD\u4e2d\u7684\u8fc7\u5ea6\u5229\u7528\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u53ef\u9760\u7684\u542f\u53d1\u5f0f\u8bbe\u8ba1\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.01695", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01695", "abs": "https://arxiv.org/abs/2602.01695", "authors": ["Yadong Wang", "Haodong Chen", "Yu Tian", "Chuanxing Geng", "Dong Liang", "Xiang Chen"], "title": "Beyond Dense States: Elevating Sparse Transcoders to Active Operators for Latent Reasoning", "comment": null, "summary": "Latent reasoning compresses the chain-of-thought (CoT) into continuous hidden states, yet existing methods rely on dense latent transitions that remain difficult to interpret and control. Meanwhile, sparse representation models uncover human-interpretable semantic features but remain largely confined to post-hoc analysis. We reconcile this tension by proposing LSTR (Latent Sparse Transcoder Reasoning), a latent reasoning framework that elevates functional sparse transcoders into active reasoning operators to perform multi-step computation through sparse semantic transitions. At its core, LSTR employs a Latent Transition Transcoder (LTT) with a residual skip architecture that decouples linear manifold transport from sparse semantic updates, enabling controllable semantic resolution via explicit sparsity constraints. Extensive experiments show that LSTR preserves reasoning accuracy and compression efficiency while substantially improving interpretability over dense latent baselines. Causal interventions and trajectory analyses further demonstrate that these sparse features act as both interpretable and causally effective operators in the reasoning process.", "AI": {"tldr": "LSTR (Latent Sparse Transcoder Reasoning) is a novel framework that transforms sparse transcoders from passive analysis tools into active reasoning operators, enabling interpretable and controllable multi-step latent reasoning through explicit sparsity constraints while maintaining accuracy and compression efficiency.", "motivation": "Existing latent reasoning methods rely on dense, uninterpretable hidden state transitions, while sparse representation models are limited to post-hoc analysis. This creates a tension between performance and interpretability that the paper aims to reconcile by making sparse semantic features active participants in the reasoning process.", "method": "The paper proposes LSTR, which employs a Latent Transition Transcoder (LTT) with residual skip architecture to decouple linear manifold transport from sparse semantic updates. This enables controllable semantic resolution via explicit sparsity constraints, allowing sparse transcoders to function as active reasoning operators performing multi-step computation through sparse semantic transitions.", "result": "Extensive experiments demonstrate that LSTR preserves reasoning accuracy and compression efficiency while substantially improving interpretability over dense latent baselines. Causal interventions and trajectory analyses confirm that sparse features serve as both interpretable and causally effective operators in the reasoning process.", "conclusion": "LSTR successfully bridges the gap between latent reasoning and sparse representations by elevating sparse transcoders to active reasoning operators, achieving a favorable trade-off between interpretability and performance through controllable sparse semantic transitions."}}
{"id": "2602.01699", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.01699", "abs": "https://arxiv.org/abs/2602.01699", "authors": ["Willem Fourie"], "title": "Mitigating loss of control in advanced AI systems through instrumental goal trajectories", "comment": null, "summary": "Researchers at artificial intelligence labs and universities are concerned that highly capable artificial intelligence (AI) systems may erode human control by pursuing instrumental goals. Existing mitigations remain largely technical and system-centric: tracking capability in advanced systems, shaping behaviour through methods such as reinforcement learning from human feedback, and designing systems to be corrigible and interruptible. Here we develop instrumental goal trajectories to expand these options beyond the model. Gaining capability typically depends on access to additional technical resources, such as compute, storage, data and adjacent services, which in turn requires access to monetary resources. In organisations, these resources can be obtained through three organisational pathways. We label these pathways the procurement, governance and finance instrumental goal trajectories (IGTs). Each IGT produces a trail of organisational artefacts that can be monitored and used as intervention points when a systems capabilities or behaviour exceed acceptable thresholds. In this way, IGTs offer concrete avenues for defining capability levels and for broadening how corrigibility and interruptibility are implemented, shifting attention from model properties alone to the organisational systems that enable them.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\"\u5de5\u5177\u76ee\u6807\u8f68\u8ff9\"(IGTs)\u6982\u5ff5\uff0c\u901a\u8fc7\u76d1\u63a7AI\u7cfb\u7edf\u83b7\u53d6\u8d44\u6e90\u7684\u4e09\u6761\u7ec4\u7ec7\u8def\u5f84(\u91c7\u8d2d\u3001\u6cbb\u7406\u3001\u878d\u8d44)\u6765\u62d3\u5c55AI\u5b89\u5168\u6cbb\u7406\u624b\u6bb5\uff0c\u5c06\u5173\u6ce8\u70b9\u4ece\u6a21\u578b\u672c\u8eab\u8f6c\u5411\u652f\u6491\u5176\u53d1\u5c55\u7684\u7ec4\u7ec7\u7cfb\u7edf\u3002", "motivation": "\u9ad8\u5ea6\u81ea\u4e3b\u7684AI\u7cfb\u7edf\u53ef\u80fd\u901a\u8fc7\u8ffd\u6c42\u5de5\u5177\u6027\u76ee\u6807\u800c\u4fb5\u8680\u4eba\u7c7b\u63a7\u5236\uff0c\u73b0\u6709\u7f13\u89e3\u63aa\u65bd\u4e3b\u8981\u5c40\u9650\u4e8e\u6280\u672f\u5c42\u9762\u548c\u7cfb\u7edf\u5185\u90e8\uff0c\u7f3a\u4e4f\u5bf9\u7ec4\u7ec7\u5c42\u9762\u7684\u7cfb\u7edf\u6027\u8003\u91cf\u3002", "method": "\u8bc6\u522bAI\u80fd\u529b\u63d0\u5347\u6240\u4f9d\u8d56\u7684\u4e09\u6761\u7ec4\u7ec7\u8def\u5f84\u2014\u2014\u91c7\u8d2d(\u83b7\u53d6\u7b97\u529b\u3001\u6570\u636e\u7b49)\u3001\u6cbb\u7406(\u7ec4\u7ec7\u51b3\u7b56)\u548c\u878d\u8d44(\u8d44\u91d1\u652f\u6301)\u8def\u5f84\uff0c\u5c06\u6bcf\u6761\u8def\u5f84\u4ea7\u751f\u7684\u7ec4\u7ec7\u5de5\u4ef6\u4f5c\u4e3a\u76d1\u63a7\u548c\u5e72\u9884\u70b9\u3002", "result": "IGTs\u63d0\u4f9b\u4e86\u5b9a\u4e49\u80fd\u529b\u9608\u503c\u7684\u5177\u4f53\u9014\u5f84\uff0c\u6269\u5c55\u4e86\u53ef\u7ea0\u6b63\u6027\u548c\u53ef\u4e2d\u65ad\u6027\u7684\u5b9e\u65bd\u65b9\u5f0f\uff0c\u5f53\u7cfb\u7edf\u80fd\u529b\u6216\u884c\u4e3a\u8d85\u51fa\u53ef\u63a5\u53d7\u8303\u56f4\u65f6\u53ef\u8fdb\u884c\u7ec4\u7ec7\u7ea7\u5e72\u9884\u3002", "conclusion": "\u5c06AI\u5b89\u5168\u6cbb\u7406\u4ece\u5355\u7eaf\u5173\u6ce8\u6a21\u578b\u5c5e\u6027\u8f6c\u5411\u5173\u6ce8\u4f7f\u6a21\u578b\u6210\u4e3a\u53ef\u80fd\u7684\u7ec4\u7ec7\u7cfb\u7edf\uff0c\u4e3a\u63a7\u5236\u9ad8\u7ea7AI\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u7684\u6cbb\u7406\u6846\u67b6\u548c\u5b9e\u9645\u64cd\u4f5c\u8def\u5f84\u3002"}}
{"id": "2602.00573", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00573", "abs": "https://arxiv.org/abs/2602.00573", "authors": ["Zheng Zhang", "Tao Hu", "Xueheng Li", "Yang Wang", "Rui Li", "Jie Zhang", "Chengjun Xie"], "title": "When Classes Evolve: A Benchmark and Framework for Stage-Aware Class-Incremental Learning", "comment": null, "summary": "Class-Incremental Learning (CIL) aims to sequentially learn new classes while mitigating catastrophic forgetting of previously learned knowledge. Conventional CIL approaches implicitly assume that classes are morphologically static, focusing primarily on preserving previously learned representations as new classes are introduced. However, this assumption neglects intra-class evolution: a phenomenon wherein instances of the same semantic class undergo significant morphological transformations, such as a larva turning into a butterfly. Consequently, a model must both discriminate between classes and adapt to evolving appearances within a single class. To systematically address this challenge, we formalize Stage-Aware CIL (Stage-CIL), a paradigm in which each class is learned progressively through distinct morphological stages. To facilitate rigorous evaluation within this paradigm, we introduce the Stage-Bench, a 10-domain, 2-stages dataset and protocol that jointly measure inter- and intra-class forgetting. We further propose STAGE, a novel method that explicitly learns abstract and transferable evolution patterns within a fixed-size memory pool. By decoupling semantic identity from transformation dynamics, STAGE enables accurate prediction of future morphologies based on earlier representations. Extensive empirical evaluation demonstrates that STAGE consistently and substantially outperforms existing state-of-the-art approaches, highlighting its effectiveness in simultaneously addressing inter-class discrimination and intra-class morphological adaptation.", "AI": {"tldr": "\u9488\u5bf9\u7c7b\u522b\u589e\u91cf\u5b66\u4e60\u4e2d\u7684\u7c7b\u5185\u5f62\u6001\u6f14\u5316\u95ee\u9898\uff0c\u63d0\u51faStage-CIL\u8303\u5f0f\u53caSTAGE\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u8bed\u4e49\u8eab\u4efd\u4e0e\u53d8\u6362\u52a8\u6001\u6765\u9884\u6d4b\u672a\u6765\u5f62\u6001\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfCIL\u5047\u8bbe\u7c7b\u522b\u5f62\u6001\u9759\u6001\uff0c\u5ffd\u7565\u4e86\u540c\u4e00\u7c7b\u522b\u5728\u4e0d\u540c\u9636\u6bb5\uff08\u5982\u5e7c\u866b\u5230\u8774\u8776\uff09\u4f1a\u53d1\u751f\u663e\u8457\u5f62\u6001\u53d8\u5316\uff0c\u9700\u8981\u540c\u65f6\u5904\u7406\u7c7b\u95f4\u5224\u522b\u548c\u7c7b\u5185\u9002\u5e94\u3002", "method": "\u5f62\u5f0f\u5316Stage-CIL\u8303\u5f0f\uff0c\u5f15\u5165Stage-Bench\u8bc4\u4f30\u57fa\u51c6\uff0c\u63d0\u51faSTAGE\u65b9\u6cd5\u5728\u56fa\u5b9a\u8bb0\u5fc6\u6c60\u5185\u5b66\u4e60\u62bd\u8c61\u4e14\u53ef\u8fc1\u79fb\u7684\u6f14\u5316\u6a21\u5f0f\u3002", "result": "STAGE\u65b9\u6cd5\u5728\u540c\u65f6\u5904\u7406\u7c7b\u95f4\u5224\u522b\u548c\u7c7b\u5185\u5f62\u6001\u9002\u5e94\u65b9\u9762\u6301\u7eed\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u6709\u6548\u89e3\u51b3\u4e86\u7c7b\u5185\u5f62\u6001\u6f14\u5316\u6311\u6218\uff0c\u4e3a\u5904\u7406\u52a8\u6001\u7c7b\u522b\u5916\u89c2\u53d8\u5316\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2602.01711", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01711", "abs": "https://arxiv.org/abs/2602.01711", "authors": ["Wei Chen", "Yanbin Fang", "Shuran Fu", "Fasheng Xu", "Xuan Wei"], "title": "Optimizing Prompts for Large Language Models: A Causal Approach", "comment": null, "summary": "Large Language Models (LLMs) are increasingly embedded in enterprise workflows, yet their performance remains highly sensitive to prompt design. Automatic Prompt Optimization (APO) seeks to mitigate this instability, but existing approaches face two persistent challenges. First, commonly used prompt strategies rely on static instructions that perform well on average but fail to adapt to heterogeneous queries. Second, more dynamic approaches depend on offline reward models that are fundamentally correlational, confounding prompt effectiveness with query characteristics. We propose Causal Prompt Optimization (CPO), a framework that reframes prompt design as a problem of causal estimation. CPO operates in two stages. First, it learns an offline causal reward model by applying Double Machine Learning (DML) to semantic embeddings of prompts and queries, isolating the causal effect of prompt variations from confounding query attributes. Second, it utilizes this unbiased reward signal to guide a resource-efficient search for query-specific prompts without relying on costly online evaluation. We evaluate CPO across benchmarks in mathematical reasoning, visualization, and data analytics. CPO consistently outperforms human-engineered prompts and state-of-the-art automated optimizers. The gains are driven primarily by improved robustness on hard queries, where existing methods tend to deteriorate. Beyond performance, CPO fundamentally reshapes the economics of prompt optimization: by shifting evaluation from real-time model execution to an offline causal model, it enables high-precision, per-query customization at a fraction of the inference cost required by online methods. Together, these results establish causal inference as a scalable foundation for reliable and cost-efficient prompt optimization in enterprise LLM deployments.", "AI": {"tldr": "\u63d0\u51fa\u56e0\u679c\u63d0\u793a\u4f18\u5316(CPO)\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u91cd\u673a\u5668\u5b66\u4e60\u5b66\u4e60\u56e0\u679c\u5956\u52b1\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u7a33\u5065\u7684\u67e5\u8be2\u7279\u5b9a\u63d0\u793a\u4f18\u5316\uff0c\u5728\u6570\u5b66\u63a8\u7406\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5927\u5e45\u964d\u4f4e\u63a8\u7406\u6210\u672c", "motivation": "\u4f01\u4e1a\u5de5\u4f5c\u6d41\u4e2dLLM\u5bf9\u63d0\u793a\u8bbe\u8ba1\u654f\u611f\uff0c\u73b0\u6709\u81ea\u52a8\u63d0\u793a\u4f18\u5316(APO)\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u9759\u6001\u6307\u4ee4\u65e0\u6cd5\u9002\u5e94\u5f02\u6784\u67e5\u8be2\uff1b\u52a8\u6001\u65b9\u6cd5\u4f9d\u8d56\u76f8\u5173\u6027\u5956\u52b1\u6a21\u578b\uff0c\u6df7\u6dc6\u63d0\u793a\u6548\u679c\u4e0e\u67e5\u8be2\u7279\u5f81", "method": "CPO\u5c06\u63d0\u793a\u8bbe\u8ba1\u91cd\u6784\u4e3a\u56e0\u679c\u4f30\u8ba1\u95ee\u9898\uff1a\u7b2c\u4e00\u9636\u6bb5\u5e94\u7528\u53cc\u91cd\u673a\u5668\u5b66\u4e60(DML)\u5b66\u4e60\u79bb\u7ebf\u56e0\u679c\u5956\u52b1\u6a21\u578b\uff0c\u5206\u79bb\u63d0\u793a\u53d8\u4f53\u7684\u56e0\u679c\u6548\u5e94\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5229\u7528\u65e0\u504f\u5956\u52b1\u4fe1\u53f7\u6307\u5bfc\u8d44\u6e90\u9ad8\u6548\u641c\u7d22\uff0c\u751f\u6210\u67e5\u8be2\u7279\u5b9a\u63d0\u793a", "result": "\u5728\u6570\u5b66\u63a8\u7406\u3001\u53ef\u89c6\u5316\u548c\u6570\u636e\u5206\u6790\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCPO\u6301\u7eed\u8d85\u8d8a\u4eba\u5de5\u8bbe\u8ba1\u548c\u5148\u8fdb\u81ea\u52a8\u4f18\u5316\u5668\uff0c\u5c24\u5176\u5728\u56f0\u96be\u67e5\u8be2\u4e0a\u9c81\u68d2\u6027\u663e\u8457\u63d0\u5347\uff0c\u5c06\u5b9e\u65f6\u8bc4\u4f30\u8f6c\u4e3a\u79bb\u7ebf\u56e0\u679c\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u6bcf\u67e5\u8be2\u5b9a\u5236\u4e14\u63a8\u7406\u6210\u672c\u5927\u5e45\u964d\u4f4e", "conclusion": "\u56e0\u679c\u63a8\u65ad\u6210\u4e3a\u4f01\u4e1aLLM\u90e8\u7f72\u4e2d\u53ef\u6269\u5c55\u3001\u53ef\u9760\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u63d0\u793a\u4f18\u5316\u57fa\u7840"}}
{"id": "2602.00576", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00576", "abs": "https://arxiv.org/abs/2602.00576", "authors": ["Tushaar Gangavarapu", "Jiping Li", "Christopher Vattheuer", "Zhangyang Wang", "Baharan Mirzasoleiman"], "title": "Data Distribution as a Lever for Guiding Optimizers Toward Superior Generalization in LLMs", "comment": null, "summary": "Can modifying the training data distribution guide optimizers toward solutions with improved generalization when training large language models (LLMs)? In this work, we theoretically analyze an in-context linear regression model with multi-head linear self-attention, and compare the training dynamics of two gradient based optimizers, namely gradient descent (GD) and sharpness-aware minimization (SAM), the latter exhibiting superior generalization properties but is prohibitively expensive for training even medium-sized LLMs. We show, for the first time, that SAM induces a lower simplicity bias (SB)-the tendency of an optimizer to preferentially learn simpler features earlier in training-and identify this reduction as a key factor underlying its improved generalization performance. Motivated by this insight, we demonstrate that altering the training data distribution by upsampling or augmenting examples learned later in training similarly reduces SB and leads to improved generalization. Our extensive experiments show that our strategy improves the performance of multiple LLMs-including Phi2-2.7B , Llama3.2-1B, Gemma3-1B-PT, and Qwen3-0.6B-Base-achieving relative accuracy gains up to 18% when fine-tuned with AdamW and Muon on mathematical reasoning tasks.", "AI": {"tldr": "\u672c\u6587\u7406\u8bba\u8bc1\u660eSAM\u901a\u8fc7\u964d\u4f4e\u7b80\u6027\u504f\u7f6e(SB)\u63d0\u5347\u6cdb\u5316\uff0c\u5e76\u63d0\u51fa\u4e0a\u91c7\u6837\u540e\u671f\u5b66\u4e60\u6837\u672c\u7684\u6570\u636e\u8c03\u6574\u7b56\u7565\uff0c\u5728\u591a\u4e2a\u5927\u6a21\u578b\u4e0a\u5b9e\u73b0\u9ad8\u8fbe18%\u7684\u6570\u5b66\u63a8\u7406\u51c6\u786e\u7387\u63d0\u5347\u3002", "motivation": "SAM\u4f18\u5316\u5668\u867d\u80fd\u63d0\u5347\u6cdb\u5316\u6027\u80fd\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u65e0\u6cd5\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u3002\u672c\u7814\u7a76\u65e8\u5728\u7406\u89e3SAM\u7684\u4f5c\u7528\u673a\u5236\uff0c\u5e76\u63a2\u7d22\u901a\u8fc7\u8c03\u6574\u6570\u636e\u5206\u5e03\u8fd9\u4e00\u4f4e\u6210\u672c\u65b9\u5f0f\u6765\u8fbe\u5230\u7c7b\u4f3c\u6548\u679c\u3002", "method": "1) \u5bf9\u591a\u5934\u7ebf\u6027\u81ea\u6ce8\u610f\u529b\u4e0a\u4e0b\u6587\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff1b2) \u6bd4\u8f83\u68af\u5ea6\u4e0b\u964d(GD)\u548cSAM\u7684\u4f18\u5316\u52a8\u6001\uff1b3) \u9996\u6b21\u63ed\u793aSAM\u80fd\u964d\u4f4e\u7b80\u6027\u504f\u7f6e(SB)\uff1b4) \u63d0\u51fa\u901a\u8fc7\u4e0a\u91c7\u6837\u6216\u589e\u5f3a\u8bad\u7ec3\u540e\u671f\u5b66\u4e60\u7684\u6837\u672c\u6765\u4fee\u6539\u6570\u636e\u5206\u5e03\u7684\u7b56\u7565\uff1b5) \u5728Phi2-2.7B\u3001Llama3.2-1B\u3001Gemma3-1B-PT\u548cQwen3-0.6B-Base\u7b49\u591a\u4e2a\u5927\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff1b6) \u4f7f\u7528AdamW\u548cMuon\u4f18\u5316\u5668\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u5fae\u8c03\u3002", "result": "SAM\u6bd4GD\u8bf1\u5bfc\u66f4\u4f4e\u7684\u7b80\u6027\u504f\u7f6e\uff0c\u4e14\u8fd9\u79cd\u964d\u4f4e\u662fSAM\u63d0\u5347\u6cdb\u5316\u7684\u5173\u952e\u56e0\u7d20\uff1b\u901a\u8fc7\u4fee\u6539\u6570\u636e\u5206\u5e03\u964d\u4f4eSB\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u76f8\u5bf9\u51c6\u786e\u7387\u63d0\u5347\u6700\u9ad8\u8fbe18%\u3002", "conclusion": "\u4fee\u6539\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u662f\u4e00\u79cd\u6709\u6548\u7684\u4f4e\u6210\u672c\u7b56\u7565\uff0c\u53ef\u901a\u8fc7\u964d\u4f4e\u7b80\u6027\u504f\u7f6e\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u8fbe\u5230\u4e0e\u6602\u8d35\u4f18\u5316\u5668SAM\u76f8\u5f53\u7684\u6548\u679c\u3002"}}
{"id": "2602.01740", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01740", "abs": "https://arxiv.org/abs/2602.01740", "authors": ["Qixin Xiao", "Kun Zhou"], "title": "MACD: Model-Aware Contrastive Decoding via Counterfactual Data", "comment": null, "summary": "Video language models (Video-LLMs) are prone to hallucinations, often generating plausible but ungrounded content when visual evidence is weak, ambiguous, or biased. Existing decoding methods, such as contrastive decoding (CD), rely on random perturbations to construct contrastive data for mitigating hallucination patterns. However, such a way is hard to control the visual cues that drive hallucination or well align with model weaknesses. We propose Model-aware Counterfactual Data based Contrastive Decoding (MACD), a new inference strategy that combines model-guided counterfactual construction with decoding. Our approach uses the Video-LLM's own feedback to identify object regions most responsible for hallucination, generating targeted counterfactual inputs at the object level rather than arbitrary frame or temporal modifications. These model-aware counterfactual data is then integrated into CD to enforce evidence-grounded token selection during decoding. Experiments on EventHallusion, MVBench, Perception-test and Video-MME show that MACD consistently reduces hallucination while maintaining or improving task accuracy across diverse Video-LLMs, including Qwen and InternVL families. The method is especially effective in challenging scenarios involving small, occluded, or co-occurring objects. Our code and data will be publicly released.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faMACD\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u8eab\u53cd\u9988\u8bc6\u522b\u5bfc\u81f4\u5e7b\u89c9\u7684\u5173\u952e\u76ee\u6807\u533a\u57df\uff0c\u6784\u5efa\u9488\u5bf9\u6027\u7684\u53cd\u4e8b\u5b9e\u89c6\u9891\u8f93\u5165\uff0c\u5e76\u7ed3\u5408\u5bf9\u6bd4\u89e3\u7801\u6765\u51cf\u5c11\u5e7b\u89c9\uff0c\u76f8\u6bd4\u968f\u673a\u6270\u52a8\u65b9\u6cd5\u66f4\u6709\u6548\u4e14\u53ef\u63a7\u3002", "motivation": "\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u8bc1\u636e\u8584\u5f31\u3001\u6a21\u7cca\u6216\u6709\u504f\u65f6\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u73b0\u6709\u5bf9\u6bd4\u89e3\u7801\u65b9\u6cd5\u4f9d\u8d56\u968f\u673a\u6270\u52a8\u6784\u5efa\u5bf9\u6bd4\u6570\u636e\uff0c\u96be\u4ee5\u7cbe\u51c6\u63a7\u5236\u9a71\u52a8\u5e7b\u89c9\u7684\u89c6\u89c9\u7ebf\u7d22\u5e76\u4e0e\u6a21\u578b\u5f31\u70b9\u5bf9\u9f50\u3002", "method": "\u63d0\u51fa\u6a21\u578b\u611f\u77e5\u7684\u53cd\u4e8b\u5b9e\u6570\u636e\u5bf9\u6bd4\u89e3\u7801\uff08MACD\uff09\uff0c\u5229\u7528\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u8eab\u53cd\u9988\u8bc6\u522b\u5bfc\u81f4\u5e7b\u89c9\u7684\u5173\u952e\u76ee\u6807\u533a\u57df\uff0c\u5728\u76ee\u6807\u7ea7\u522b\u751f\u6210\u9488\u5bf9\u6027\u53cd\u4e8b\u5b9e\u8f93\u5165\uff0c\u800c\u975e\u4efb\u610f\u5e27\u6216\u65f6\u5e8f\u4fee\u6539\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u5bf9\u6bd4\u89e3\u7801\u4e2d\u4ee5\u5f3a\u5236\u57fa\u4e8e\u8bc1\u636e\u7684token\u9009\u62e9\u3002", "result": "\u5728EventHallusion\u3001MVBench\u3001Perception-test\u548cVideo-MME\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMACD\u5728Qwen\u548cInternVL\u7cfb\u5217\u7b49\u591a\u6837\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u6301\u7eed\u51cf\u5c11\u5e7b\u89c9\uff0c\u540c\u65f6\u5728\u6d89\u53ca\u5c0f\u578b\u3001\u906e\u6321\u6216\u5171\u73b0\u76ee\u6807\u7684\u6311\u6218\u6027\u573a\u666f\u4e2d\u4fdd\u6301\u6216\u63d0\u5347\u4efb\u52a1\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u7f13\u89e3\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5c06\u516c\u5f00\u3002"}}
{"id": "2602.00577", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00577", "abs": "https://arxiv.org/abs/2602.00577", "authors": ["Yuze Wang", "Yujia Tong", "Ke Xu", "Jingling Yuan", "Jiawei Jiang", "Chuang Hu"], "title": "Sparsity-Aware Unlearning for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) inevitably memorize sensitive information during training, posing significant privacy risks. Machine unlearning has emerged as a promising solution to selectively remove such information without full retraining. However, existing methods are designed for dense models and overlook model sparsification-an essential technique for efficient LLM deployment. We find that unlearning effectiveness degrades substantially on sparse models. Through empirical analysis, we reveal that this degradation occurs because existing unlearning methods require updating all parameters, yet sparsification prunes substantial weights to zero, fundamentally limiting the model's forgetting capacity. To address this challenge, we propose Sparsity-Aware Unlearning (SAU), which decouples unlearning from sparsification objectives through gradient masking that redirects updates to surviving weights, combined with importance-aware redistribution to compensate for pruned parameters. Extensive experiments demonstrate that SAU significantly outperforms existing methods on sparse LLMs, achieving effective forgetting while preserving model utility.", "AI": {"tldr": "Proposes Sparsity-Aware Unlearning (SAU) to fix the failure of existing machine unlearning methods on sparse LLMs by using gradient masking and importance-aware redistribution to maintain forgetting capacity on pruned models.", "motivation": "LLMs memorize sensitive data causing privacy risks. Machine unlearning can remove this data but fails on sparse models essential for efficient deployment because sparsification prunes weights to zero, limiting forgetting capacity.", "method": "SAU decouples unlearning from sparsification via gradient masking (redirecting updates to surviving weights) and importance-aware redistribution (compensating for pruned parameters).", "result": "SAU significantly outperforms existing methods on sparse LLMs, achieving effective forgetting while preserving model utility.", "conclusion": "SAU enables effective machine unlearning for sparse LLMs, providing a practical solution for privacy-preserving deployment of efficient models."}}
{"id": "2602.01749", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01749", "abs": "https://arxiv.org/abs/2602.01749", "authors": ["Lin Chen", "Samuel Drapeau", "Fanghao Shao", "Xuekai Zhu", "Bo Xue", "Yunchong Song", "Mathieu Lauri\u00e8re", "Zhouhan Lin"], "title": "Controlling Exploration-Exploitation in GFlowNets via Markov Chain Perspectives", "comment": null, "summary": "Generative Flow Network (GFlowNet) objectives implicitly fix an equal mixing of forward and backward policies, potentially constraining the exploration-exploitation trade-off during training. By further exploring the link between GFlowNets and Markov chains, we establish an equivalence between GFlowNet objectives and Markov chain reversibility, thereby revealing the origin of such constraints, and provide a framework for adapting Markov chain properties to GFlowNets. Building on these theoretical findings, we propose $\u03b1$-GFNs, which generalize the mixing via a tunable parameter $\u03b1$. This generalization enables direct control over exploration-exploitation dynamics to enhance mode discovery capabilities, while ensuring convergence to unique flows. Across various benchmarks, including Set, Bit Sequence, and Molecule Generation, $\u03b1$-GFN objectives consistently outperform previous GFlowNet objectives, achieving up to a $10 \\times$ increase in the number of discovered modes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u03b1-GFNs\uff0c\u4e00\u79cd\u901a\u8fc7\u53ef\u8c03\u53c2\u6570\u03b1\u63a8\u5e7f\u524d\u5411/\u540e\u5411\u7b56\u7565\u6df7\u5408\u7684GFlowNet\u6cdb\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u597d\u5730\u63a7\u5236\u63a2\u7d22-\u5229\u7528\u6743\u8861\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u53d1\u73b0\u6a21\u5f0f\u6570\u91cf\u9ad8\u8fbe10\u500d\u7684\u63d0\u5347\u3002", "motivation": "\u6807\u51c6GFlowNet\u76ee\u6807\u51fd\u6570\u9690\u5f0f\u5730\u56fa\u5b9a\u4e86\u524d\u5411\u4e0e\u540e\u5411\u7b56\u7565\u7684\u5747\u7b49\u6df7\u5408\uff0c\u9650\u5236\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u63a2\u7d22-\u5229\u7528\u6743\u8861\u7684\u7075\u6d3b\u6027\u3002", "method": "\u4f5c\u8005\u5efa\u7acb\u4e86GFlowNet\u76ee\u6807\u51fd\u6570\u4e0e\u9a6c\u5c14\u53ef\u592b\u94fe\u53ef\u9006\u6027\u7684\u7406\u8bba\u7b49\u4ef7\u5173\u7cfb\uff0c\u8fdb\u800c\u63d0\u51fa\u03b1-GFNs\uff0c\u901a\u8fc7\u53ef\u8c03\u53c2\u6570\u03b1\u63a8\u5e7f\u7b56\u7565\u6df7\u5408\uff0c\u5b9e\u73b0\u5bf9\u63a2\u7d22-\u5229\u7528\u52a8\u6001\u7684\u76f4\u63a5\u63a7\u5236\u3002", "result": "\u03b1-GFNs\u5728Set\u3001\u6bd4\u7279\u5e8f\u5217\u548c\u5206\u5b50\u751f\u6210\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u5148\u524dGFlowNet\u76ee\u6807\u51fd\u6570\uff0c\u53d1\u73b0\u6a21\u5f0f\u6570\u91cf\u63d0\u5347\u9ad8\u8fbe10\u500d\uff0c\u540c\u65f6\u4fdd\u8bc1\u6536\u655b\u5230\u552f\u4e00\u6d41\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u8fde\u63a5GFlowNets\u4e0e\u9a6c\u5c14\u53ef\u592b\u94fe\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u03b1-GFNs\u4f5c\u4e3a\u7075\u6d3b\u6269\u5c55\uff0c\u901a\u8fc7\u53ef\u63a7\u7684\u63a2\u7d22-\u5229\u7528\u5e73\u8861\u63d0\u5347\u4e86\u6a21\u5f0f\u53d1\u73b0\u80fd\u529b\u3002"}}
{"id": "2602.00582", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00582", "abs": "https://arxiv.org/abs/2602.00582", "authors": ["Xiangfei Qiu", "Kangjia Yan", "Xvyuan Liu", "Xingjian Wu", "Jilin Hu"], "title": "Bridging Time and Frequency: A Joint Modeling Framework for Irregular Multivariate Time Series Forecasting", "comment": null, "summary": "Irregular multivariate time series forecasting (IMTSF) is challenging due to non-uniform sampling and variable asynchronicity. These irregularities violate the equidistant assumptions of standard models, hindering local temporal modeling and rendering classical frequency-domain methods ineffective for capturing global periodic structures. To address this challenge, we propose TFMixer, a joint time-frequency modeling framework for IMTS forecasting. Specifically, TFMixer incorporates a Global Frequency Module that employs a learnable Non-Uniform Discrete Fourier Transform (NUDFT) to directly extract spectral representations from irregular timestamps. In parallel, the Local Time Module introduces a query-based patch mixing mechanism to adaptively aggregate informative temporal patches and alleviate information density imbalance. Finally, TFMixer fuses the time-domain and frequency-domain representations to generate forecasts and further leverages inverse NUDFT for explicit seasonal extrapolation. Extensive experiments on real-world datasets demonstrate the state--of-the-art performance of TFMixer.", "AI": {"tldr": "TFMixer\u901a\u8fc7\u8054\u5408\u65f6\u9891\u5efa\u6a21\u89e3\u51b3\u4e0d\u89c4\u5219\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u95ee\u9898\uff0c\u4f7f\u7528\u65f6\u57df\u67e5\u8be2\u8865\u4e01\u6df7\u5408\u548c\u9891\u57df\u53ef\u5b66\u4e60\u975e\u5747\u5300\u5085\u91cc\u53f6\u53d8\u6362\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u4e0d\u89c4\u5219\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u7684\u975e\u5747\u5300\u91c7\u6837\u548c\u53d8\u91cf\u5f02\u6b65\u6027\u8fdd\u53cd\u6807\u51c6\u6a21\u578b\u7684\u7b49\u95f4\u9694\u5047\u8bbe\uff0c\u963b\u788d\u4e86\u5c40\u90e8\u65f6\u95f4\u5efa\u6a21\uff0c\u5e76\u4f7f\u7ecf\u5178\u9891\u57df\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u5168\u5c40\u5468\u671f\u7ed3\u6784\u3002", "method": "\u63d0\u51faTFMixer\u6846\u67b6\uff1a1) \u5168\u5c40\u9891\u57df\u6a21\u5757\u91c7\u7528\u53ef\u5b66\u4e60\u7684\u975e\u5747\u5300\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362(NUDFT)\u76f4\u63a5\u4ece irregular \u65f6\u95f4\u6233\u63d0\u53d6\u9891\u8c31\u8868\u793a\uff1b2) \u5c40\u90e8\u65f6\u57df\u6a21\u5757\u5f15\u5165\u57fa\u4e8e\u67e5\u8be2\u7684\u8865\u4e01\u6df7\u5408\u673a\u5236\u81ea\u9002\u5e94\u805a\u5408\u4fe1\u606f\u6027\u65f6\u95f4\u8865\u4e01\uff0c\u7f13\u89e3\u4fe1\u606f\u5bc6\u5ea6\u4e0d\u5e73\u8861\uff1b3) \u878d\u5408\u65f6\u57df\u548c\u9891\u57df\u8868\u793a\u8fdb\u884c\u9884\u6d4b\uff0c\u5e76\u5229\u7528\u9006NUDFT\u5b9e\u73b0\u663e\u5f0f\u5b63\u8282\u6027\u5916\u63a8\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86TFMixer\u7684\u5148\u8fdb\u6027\u80fd(state-of-the-art)\u3002", "conclusion": "\u8054\u5408\u65f6\u9891\u5efa\u6a21\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u89c4\u5219\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6311\u6218\uff0c\u901a\u8fc7\u540c\u65f6\u6355\u83b7\u5168\u5c40\u5468\u671f\u6a21\u5f0f\u548c\u5c40\u90e8\u65f6\u95f4\u52a8\u6001\u5b9e\u73b0\u4e86\u4f18\u5f02\u9884\u6d4b\u7cbe\u5ea6\u3002"}}
{"id": "2602.01750", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01750", "abs": "https://arxiv.org/abs/2602.01750", "authors": ["Mohammad Beigi", "Ming Jin", "Junshan Zhang", "Qifan Wang", "Lifu Huang"], "title": "Adversarial Reward Auditing for Active Detection and Mitigation of Reward Hacking", "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) remains vulnerable to reward hacking, where models exploit spurious correlations in learned reward models to achieve high scores while violating human intent. Existing mitigations rely on static defenses that cannot adapt to novel exploitation strategies. We propose Adversarial Reward Auditing (ARA), a framework that reconceptualizes reward hacking as a dynamic, competitive game. ARA operates in two stages: first, a Hacker policy discovers reward model vulnerabilities while an Auditor learns to detect exploitation from latent representations; second, Auditor-Guided RLHF (AG-RLHF) gates reward signals to penalize detected hacking, transforming reward hacking from an unobservable failure into a measurable, controllable signal. Experiments across three hacking scenarios demonstrate that ARA achieves the best alignment-utility tradeoff among all baselines: reducing sycophancy to near-SFT levels while improving helpfulness, decreasing verbosity while achieving the highest ROUGE-L, and suppressing code gaming while improving Pass@1. Beyond single-domain evaluation, we show that reward hacking, detection, and mitigation all generalize across domains -- a Hacker trained on code gaming exhibits increased sycophancy despite no reward for this behavior, and an Auditor trained on one domain effectively suppresses exploitation in others, enabling efficient multi-domain defense with a single model.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5bf9\u6297\u6027\u5956\u52b1\u5ba1\u8ba1(ARA)\uff0c\u4e00\u79cd\u5c06RLHF\u4e2d\u7684\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\u89c6\u4e3a\u7ade\u4e89\u6e38\u620f\u7684\u52a8\u6001\u6846\u67b6\uff0c\u901a\u8fc7Hacker\u4e0eAuditor\u7684\u5bf9\u6297\uff0c\u518d\u5229\u7528Auditor\u5f15\u5bfcRLHF\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u5bf9\u9f50-\u6548\u7528\u6743\u8861\u548c\u8de8\u9886\u57df\u6cdb\u5316\u3002", "motivation": "RLHF\u7cfb\u7edf\u6613\u53d7\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\u5f71\u54cd\uff0c\u6a21\u578b\u4f1a\u5229\u7528\u5b66\u4e60\u5230\u7684\u5956\u52b1\u6a21\u578b\u4e2d\u7684\u865a\u5047\u76f8\u5173\u6765\u83b7\u5f97\u9ad8\u5206\uff1b\u73b0\u6709\u9759\u6001\u9632\u5fa1\u65e0\u6cd5\u9002\u5e94\u65b0\u7684\u5229\u7528\u7b56\u7565\u3002", "method": "ARA\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a(1) Hacker\u7b56\u7565\u53d1\u73b0\u5956\u52b1\u6a21\u578b\u6f0f\u6d1e\uff0c\u540c\u65f6Auditor\u4ece\u6f5c\u5728\u8868\u793a\u4e2d\u5b66\u4e60\u68c0\u6d4b\u5229\u7528\u884c\u4e3a\uff1b(2) \u5ba1\u8ba1\u5f15\u5bfc\u7684RLHF(AG-RLHF)\u95e8\u63a7\u5956\u52b1\u4fe1\u53f7\u4ee5\u60e9\u7f5a\u68c0\u6d4b\u5230\u7684\u9ed1\u5ba2\u653b\u51fb\u3002", "result": "\u5728\u4e09\u79cd\u9ed1\u5ba2\u653b\u51fb\u573a\u666f\u7684\u5b9e\u9a8c\u8868\u660e\uff0cARA\u5728\u6240\u6709\u57fa\u7ebf\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f73\u5bf9\u9f50-\u6548\u7528\u6743\u8861\uff1a\u5c06\u8c04\u5a9a\u5ea6\u964d\u81f3\u63a5\u8fd1SFT\u6c34\u5e73\u7684\u540c\u65f6\u63d0\u5347\u5e2e\u52a9\u6027\uff0c\u51cf\u5c11\u5197\u957f\u5ea6\u540c\u65f6\u83b7\u5f97\u6700\u9ad8\u7684ROUGE-L\uff0c\u6291\u5236\u4ee3\u7801\u535a\u5f08\u7684\u540c\u65f6\u63d0\u5347Pass@1\u3002\u6b64\u5916\uff0c\u5956\u52b1\u9ed1\u5ba2\u3001\u68c0\u6d4b\u4e0e\u7f13\u89e3\u90fd\u5177\u6709\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u2014\u2014\u5728\u4ee3\u7801\u535a\u5f08\u4e0a\u8bad\u7ec3\u7684Hacker\u4f1a\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u8c04\u5a9a\u884c\u4e3a\uff0c\u800c\u5728\u4e00\u4e2a\u9886\u57df\u8bad\u7ec3\u7684Auditor\u80fd\u6709\u6548\u6291\u5236\u5176\u4ed6\u9886\u57df\u7684\u5229\u7528\u884c\u4e3a\uff0c\u5b9e\u73b0\u5355\u6a21\u578b\u591a\u9886\u57df\u9632\u5fa1\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5c06\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\u4ece\u4e0d\u53ef\u89c2\u6d4b\u7684\u5931\u8d25\u8f6c\u53d8\u4e3a\u53ef\u6d4b\u91cf\u3001\u53ef\u63a7\u7684\u4fe1\u53f7\uff0c\u8bc1\u660e\u4e86\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u53ef\u5b9e\u73b0\u9ad8\u6548\u7684\u591a\u9886\u57df\u9632\u5fa1\u3002"}}
{"id": "2602.00589", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00589", "abs": "https://arxiv.org/abs/2602.00589", "authors": ["Xiangfei Qiu", "Xvyuan Liu", "Tianen Shen", "Xingjian Wu", "Hanyin Cheng", "Bin Yang", "Jilin Hu"], "title": "SEER: Transformer-based Robust Time Series Forecasting via Automated Patch Enhancement and Replacement", "comment": null, "summary": "Time series forecasting is important in many fields that require accurate predictions for decision-making. Patching techniques, commonly used and effective in time series modeling, help capture temporal dependencies by dividing the data into patches. However, existing patch-based methods fail to dynamically select patches and typically use all patches during the prediction process. In real-world time series, there are often low-quality issues during data collection, such as missing values, distribution shifts, anomalies and white noise, which may cause some patches to contain low-quality information, negatively impacting the prediction results. To address this issue, this study proposes a robust time series forecasting framework called SEER. Firstly, we propose an Augmented Embedding Module, which improves patch-wise representations using a Mixture-of-Experts (MoE) architecture and obtains series-wise token representations through a channel-adaptive perception mechanism. Secondly, we introduce a Learnable Patch Replacement Module, which enhances forecasting robustness and model accuracy through a two-stage process: 1) a dynamic filtering mechanism eliminates negative patch-wise tokens; 2) a replaced attention module substitutes the identified low-quality patches with global series-wise token, further refining their representations through a causal attention mechanism. Comprehensive experimental results demonstrate the SOTA performance of SEER.", "AI": {"tldr": "\u63d0\u51faSEER\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8fc7\u6ee4\u548c\u66ff\u6362\u4f4e\u8d28\u91cf\u8865\u4e01\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8865\u4e01\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u65e0\u6cd5\u52a8\u6001\u9009\u62e9\u8865\u4e01\uff0c\u5f53\u6570\u636e\u5b58\u5728\u7f3a\u5931\u503c\u3001\u5f02\u5e38\u3001\u566a\u58f0\u6216\u5206\u5e03\u6f02\u79fb\u7b49\u4f4e\u8d28\u91cf\u95ee\u9898\u65f6\uff0c\u4f1a\u635f\u5bb3\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u5f15\u5165\u589e\u5f3a\u5d4c\u5165\u6a21\u5757\uff08\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u548c\u901a\u9053\u81ea\u9002\u5e94\u611f\u77e5\uff09\u548c\u53ef\u5b66\u4e60\u8865\u4e01\u66ff\u6362\u6a21\u5757\uff08\u5305\u542b\u52a8\u6001\u8fc7\u6ee4\u548c\u66ff\u6362\u6ce8\u610f\u529b\u673a\u5236\uff09\uff0c\u6d88\u9664\u5e76\u66ff\u6362\u4f4e\u8d28\u91cf\u8865\u4e01\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSEER\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684(SOTA)\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u8d28\u91cf\u8865\u4e01\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2602.01779", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01779", "abs": "https://arxiv.org/abs/2602.01779", "authors": ["Rui Hua", "Yu Wei", "Zixin Shu", "Kai Chang", "Dengying Yan", "Jianan Xia", "Zeyu Liu", "Hui Zhu", "Shujie Song", "Mingzhong Xiao", "Xiaodong Li", "Dongmei Jia", "Zhuye Gao", "Yanyan Meng", "Naixuan Zhao", "Yu Fu", "Haibin Yu", "Benman Yu", "Yuanyuan Chen", "Fei Dong", "Zhizhou Meng", "Pengcheng Yang", "Songxue Zhao", "Lijuan Pei", "Yunhui Hu", "Kan Ding", "Jiayuan Duan", "Wenmao Yin", "Yang Gu", "Runshun Zhang", "Qiang Zhu", "Jian Yu", "Jiansheng Li", "Baoyan Liu", "Wenjia Wang", "Xuezhong Zhou"], "title": "LingLanMiDian: Systematic Evaluation of LLMs on TCM Knowledge and Clinical Reasoning", "comment": null, "summary": "Large language models (LLMs) are advancing rapidly in medical NLP, yet Traditional Chinese Medicine (TCM) with its distinctive ontology, terminology, and reasoning patterns requires domain-faithful evaluation. Existing TCM benchmarks are fragmented in coverage and scale and rely on non-unified or generation-heavy scoring that hinders fair comparison. We present the LingLanMiDian (LingLan) benchmark, a large-scale, expert-curated, multi-task suite that unifies evaluation across knowledge recall, multi-hop reasoning, information extraction, and real-world clinical decision-making. LingLan introduces a consistent metric design, a synonym-tolerant protocol for clinical labels, a per-dataset 400-item Hard subset, and a reframing of diagnosis and treatment recommendation into single-choice decision recognition. We conduct comprehensive, zero-shot evaluations on 14 leading open-source and proprietary LLMs, providing a unified perspective on their strengths and limitations in TCM commonsense knowledge understanding, reasoning, and clinical decision support; critically, the evaluation on Hard subset reveals a substantial gap between current models and human experts in TCM-specialized reasoning. By bridging fundamental knowledge and applied reasoning through standardized evaluation, LingLan establishes a unified, quantitative, and extensible foundation for advancing TCM LLMs and domain-specific medical AI research. All evaluation data and code are available at https://github.com/TCMAI-BJTU/LingLan and http://tcmnlp.com.", "AI": {"tldr": "\u9488\u5bf9\u4e2d\u533b\u9886\u57df\u8bc4\u4f30\u6807\u51c6\u4e0d\u7edf\u4e00\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faLingLan\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u591a\u4efb\u52a1\u8bc4\u4f30\u548cHard\u5b50\u96c6\u5206\u6790\uff0c\u63ed\u793a\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e2d\u533b\u4e13\u4e1a\u63a8\u7406\u4e0a\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597NLP\u8fdb\u5c55\u8fc5\u901f\uff0c\u4f46\u4e2d\u533b\u72ec\u7279\u7684\u672c\u4f53\u3001\u672f\u8bed\u548c\u63a8\u7406\u6a21\u5f0f\u9700\u8981\u9886\u57df\u4e13\u5c5e\u8bc4\u4f30\u3002\u73b0\u6709\u4e2d\u533b\u57fa\u51c6\u788e\u7247\u5316\u3001\u89c4\u6a21\u5c0f\u3001\u8bc4\u5206\u4e0d\u7edf\u4e00\uff0c\u96be\u4ee5\u516c\u5e73\u6bd4\u8f83\u3002", "method": "\u6784\u5efaLingLan\u57fa\u51c6\uff1a\u7edf\u4e00\u6307\u6807\u3001\u540c\u4e49\u8bcd\u5bb9\u5fcd\u6807\u7b7e\u534f\u8bae\u3001\u6bcf\u4efb\u52a1400\u6837\u672cHard\u5b50\u96c6\u3001\u5c06\u8bca\u7597\u91cd\u6784\u4e3a\u5355\u9009\u51b3\u7b56\u3002\u5bf914\u4e2a\u4e3b\u6d41LLM\u8fdb\u884c\u96f6\u6837\u672c\u8bc4\u4f30\u3002", "result": "Hard\u5b50\u96c6\u8bc4\u4f30\u663e\u793a\u6a21\u578b\u4e0e\u4e2d\u533b\u4e13\u5bb6\u5728\u4e13\u4e1a\u63a8\u7406\u4e0a\u5dee\u8ddd\u5de8\u5927\uff0c\u57fa\u51c6\u7edf\u4e00\u63ed\u793a\u4e86\u5404\u6a21\u578b\u5728\u4e2d\u533b\u77e5\u8bc6\u7406\u89e3\u3001\u63a8\u7406\u548c\u4e34\u5e8a\u51b3\u7b56\u4e0a\u7684\u4f18\u52a3\u3002", "conclusion": "LingLan\u5efa\u7acb\u4e86\u7edf\u4e00\u3001\u91cf\u5316\u3001\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u57fa\u7840\uff0c\u4e3a\u4e2d\u533b\u5927\u8bed\u8a00\u6a21\u578b\u548c\u533b\u7597AI\u7814\u7a76\u63d0\u4f9b\u6807\u51c6\u6846\u67b6\u3002"}}
{"id": "2602.00603", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00603", "abs": "https://arxiv.org/abs/2602.00603", "authors": ["Luca Viano", "Ruida Zhou", "Yifan Sun", "Mahdi Namazifar", "Volkan Cevher", "Shoham Sabach", "Mohammad Ghavamzadeh"], "title": "Direct Preference Optimization with Rating Information: Practical Algorithms and Provable Gains", "comment": null, "summary": "The class of direct preference optimization (DPO) algorithms has emerged as a promising approach for solving the alignment problem in foundation models. These algorithms work with very limited feedback in the form of pairwise preferences and fine-tune models to align with these preferences without explicitly learning a reward model. While the form of feedback used by these algorithms makes the data collection process easy and relatively more accurate, its ambiguity in terms of the quality of responses could have negative implications. For example, it is not clear if a decrease (increase) in the likelihood of preferred (dispreferred) responses during the execution of these algorithms could be interpreted as a positive or negative phenomenon. In this paper, we study how to design algorithms that can leverage additional information in the form of rating gap, which informs the learner how much the chosen response is better than the rejected one. We present new algorithms that can achieve faster statistical rates than DPO in presence of accurate rating gap information. Moreover, we theoretically prove and empirically show that the performance of our algorithms is robust to inaccuracy in rating gaps. Finally, we demonstrate the solid performance of our methods in comparison to a number of DPO-style algorithms across a wide range of LLMs and evaluation benchmarks.", "AI": {"tldr": "The paper proposes new algorithms that leverage \"rating gap\" information (how much better a chosen response is than a rejected one) to improve Direct Preference Optimization (DPO) for aligning foundation models. These algorithms achieve faster statistical rates than DPO when rating gaps are accurate and are robust to inaccuracies, outperforming DPO-style methods across various LLMs and benchmarks.", "motivation": "DPO algorithms use limited pairwise preference feedback that has ambiguity in response quality interpretation, making it unclear whether certain likelihood changes are positive or negative for alignment.", "method": "Designing algorithms that incorporate additional rating gap information to quantify the quality difference between chosen and rejected responses during the alignment process.", "result": "Achieved faster statistical rates than DPO with accurate rating gaps; proved theoretical and empirical robustness to rating gap inaccuracies; demonstrated superior performance across multiple LLMs and benchmarks.", "conclusion": "Leveraging rating gap information can significantly enhance DPO-style alignment algorithms, providing both improved performance and robustness to noise in the additional information."}}
{"id": "2602.00606", "categories": ["cs.LG", "cs.GT"], "pdf": "https://arxiv.org/pdf/2602.00606", "abs": "https://arxiv.org/abs/2602.00606", "authors": ["Ahmed Said Donmez", "Yuksel Arslantas", "Muhammed O. Sayin"], "title": "Actor-Dual-Critic Dynamics for Zero-sum and Identical-Interest Stochastic Games", "comment": null, "summary": "We propose a novel independent and payoff-based learning framework for stochastic games that is model-free, game-agnostic, and gradient-free. The learning dynamics follow a best-response-type actor-critic architecture, where agents update their strategies (actors) using feedback from two distinct critics: a fast critic that intuitively responds to observed payoffs under limited information, and a slow critic that deliberatively approximates the solution to the underlying dynamic programming problem. Crucially, the learning process relies on non-equilibrium adaptation through smoothed best responses to observed payoffs. We establish convergence to (approximate) equilibria in two-agent zero-sum and multi-agent identical-interest stochastic games over an infinite horizon. This provides one of the first payoff-based and fully decentralized learning algorithms with theoretical guarantees in both settings. Empirical results further validate the robustness and effectiveness of the proposed approach across both classes of games.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6536\u76ca\u7684\u65e0\u6a21\u578b\u3001\u65e0\u68af\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u6279\u8bc4\u5668\u67b6\u6784\u5b9e\u73b0\u968f\u673a\u535a\u5f08\u7684\u5206\u5e03\u5f0f\u5747\u8861\u6536\u655b", "motivation": "\u73b0\u6709\u968f\u673a\u535a\u5f08\u5b66\u4e60\u65b9\u6cd5\u591a\u4f9d\u8d56\u68af\u5ea6\u6216\u5b8c\u6574\u535a\u5f08\u6a21\u578b\uff0c\u7f3a\u4e4f\u5728\u4fe1\u606f\u53d7\u9650\u4e0b\u517c\u987e\u7406\u8bba\u4e0e\u5b9e\u8df5\u4fdd\u8bc1\u7684\u5206\u5e03\u5f0f payoff-based \u65b9\u6cd5", "method": "\u6784\u5efa\"\u7b56\u7565-\u53cc\u6279\u8bc4\u5668\"\u67b6\u6784\uff1a\u5feb\u901f\u6279\u8bc4\u5668\u57fa\u4e8e\u5373\u65f6\u6536\u76ca\u53cd\u9988\u66f4\u65b0\u7b56\u7565\uff0c\u6162\u901f\u6279\u8bc4\u5668\u8fd1\u4f3c\u52a8\u6001\u89c4\u5212\u89e3\uff0c\u901a\u8fc7\u5e73\u6ed1\u6700\u4f18\u54cd\u5e94\u5b9e\u73b0\u975e\u5747\u8861\u81ea\u9002\u5e94\u5b66\u4e60", "result": "\u5728\u53cc\u4eba\u96f6\u548c\u4e0e\u591a\u667a\u80fd\u4f53\u540c\u5229\u76ca\u968f\u673a\u535a\u5f08\u4e2d\u5747\u5b9e\u73b0(\u8fd1\u4f3c)\u5747\u8861\u6536\u655b\uff0c\u662f\u9996\u4e2a\u5728\u4e24\u7c7b\u573a\u666f\u4e0b\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u7684\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316 payoff-based \u7b97\u6cd5", "conclusion": "\u8be5\u65b9\u6cd5\u517c\u5177\u6a21\u578b\u65e0\u5173\u6027\u4e0e\u535a\u5f08\u666e\u9002\u6027\uff0c\u7406\u8bba\u8bc1\u660e\u4e0e\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u591a\u7c7b\u968f\u673a\u535a\u5f08\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\uff0c\u4e3a\u5206\u5e03\u5f0f\u5b66\u4e60\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2602.01815", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01815", "abs": "https://arxiv.org/abs/2602.01815", "authors": ["Yunhui Jang", "Seonghyun Park", "Jaehyung Kim", "Sungsoo Ahn"], "title": "INDIBATOR: Diverse and Fact-Grounded Individuality for Multi-Agent Debate in Molecular Discovery", "comment": null, "summary": "Multi-agent systems have emerged as a powerful paradigm for automating scientific discovery. To differentiate agent behavior in the multi-agent system, current frameworks typically assign generic role-based personas such as ''reviewer'' or ''writer'' or rely on coarse grained keyword-based personas. While functional, this approach oversimplifies how human scientists operate, whose contributions are shaped by their unique research trajectories. In response, we propose INDIBATOR, a framework for molecular discovery that grounds agents in individualized scientist profiles constructed from two modalities: publication history for literature-derived knowledge and molecular history for structural priors. These agents engage in multi-turn debate through proposal, critique, and voting phases. Our evaluation demonstrates that these fine-grained individuality-grounded agents consistently outperform systems relying on coarse-grained personas, achieving competitive or state-of-the-art performance. These results validate that capturing the ``scientific DNA'' of individual agents is essential for high-quality discovery.", "AI": {"tldr": "INDIBATOR\u6846\u67b6\u901a\u8fc7\u6784\u5efa\u57fa\u4e8e\u53d1\u8868\u5386\u53f2\u548c\u5206\u5b50\u5386\u53f2\u7684\u4e2a\u6027\u5316\u79d1\u5b66\u5bb6\u6863\u6848\uff0c\u66ff\u4ee3\u4f20\u7edf\u901a\u7528\u89d2\u8272\u8bbe\u5b9a\uff0c\u4f7f\u591a\u667a\u80fd\u4f53\u5728\u5206\u5b50\u53d1\u73b0\u4e2d\u901a\u8fc7\u591a\u8f6e\u8fa9\u8bba\u5b9e\u73b0\u66f4\u4f18\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u79d1\u5b66\u53d1\u73b0\u7cfb\u7edf\u91c7\u7528\u901a\u7528\u89d2\u8272\u578b\u6216\u5173\u952e\u8bcd\u578bpersona\u8fc7\u4e8e\u7b80\u5316\uff0c\u65e0\u6cd5\u771f\u5b9e\u53cd\u6620\u4eba\u7c7b\u79d1\u5b66\u5bb6\u57fa\u4e8e\u72ec\u7279\u7814\u7a76\u8f68\u8ff9\u7684\u4e13\u4e1a\u8d21\u732e\u6a21\u5f0f\u3002", "method": "\u63d0\u51faINDIBATOR\u6846\u67b6\uff0c\u4ece\u53d1\u8868\u5386\u53f2\uff08\u6587\u732e\u77e5\u8bc6\uff09\u548c\u5206\u5b50\u5386\u53f2\uff08\u7ed3\u6784\u5148\u9a8c\uff09\u4e24\u4e2a\u7ef4\u5ea6\u6784\u5efa\u7ec6\u7c92\u5ea6\u79d1\u5b66\u5bb6\u6863\u6848\uff0c\u667a\u80fd\u4f53\u901a\u8fc7\u63d0\u6848\u3001\u6279\u5224\u548c\u6295\u7968\u4e09\u9636\u6bb5\u8fdb\u884c\u591a\u8f6e\u8fa9\u8bba\u3002", "result": "\u7ec6\u7c92\u5ea6\u4e2a\u6027\u5316\u667a\u80fd\u4f53\u6301\u7eed\u4f18\u4e8e\u7c97\u7c92\u5ea6persona\u7cfb\u7edf\uff0c\u5728\u5206\u5b50\u53d1\u73b0\u4efb\u52a1\u4e2d\u8fbe\u5230\u7ade\u4e89\u6027\u6216\u6700\u5148\u8fdb\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "\u6355\u6349\u667a\u80fd\u4f53\u7684\"\u79d1\u5b66DNA\"\u2014\u2014\u5373\u5176\u72ec\u7279\u7814\u7a76\u7279\u5f81\u2014\u2014\u5bf9\u9ad8\u8d28\u91cf\u79d1\u5b66\u53d1\u73b0\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2602.00620", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00620", "abs": "https://arxiv.org/abs/2602.00620", "authors": ["Juntao Fang", "Shifeng Xie", "Shengbin Nie", "Yuhui Ling", "Yuming Liu", "Zijian Li", "Keli Zhang", "Lujia Pan", "Themis Palpanas", "Ruichu Cai"], "title": "Rethinking Zero-Shot Time Series Classification: From Task-specific Classifiers to In-Context Inference", "comment": null, "summary": "The zero-shot evaluation of time series foundation models (TSFMs) for classification typically uses a frozen encoder followed by a task-specific classifier. However, this practice violates the training-free premise of zero-shot deployment and introduces evaluation bias due to classifier-dependent training choices. To address this issue, we propose TIC-FM, an in-context learning framework that treats the labeled training set as context and predicts labels for all test instances in a single forward pass, without parameter updates. TIC-FM pairs a time series encoder and a lightweight projection adapter with a split-masked latent memory Transformer. We further provide theoretical justification that in-context inference can subsume trained classifiers and can emulate gradient-based classifier training within a single forward pass. Experiments on 128 UCR datasets show strong accuracy, with consistent gains in the extreme low-label situation, highlighting training-free transfer", "AI": {"tldr": "...", "motivation": "...", "method": "...", "result": "...", "conclusion": "..."}}
{"id": "2602.01832", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01832", "abs": "https://arxiv.org/abs/2602.01832", "authors": ["Rui Wang", "Yaoguang Cao", "Yuyi Chen", "Jianyi Xu", "Zhuoyang Li", "Jiachen Shang", "Shichun Yang"], "title": "Synesthesia of Vehicles: Tactile Data Synthesis from Visual Inputs", "comment": null, "summary": "Autonomous vehicles (AVs) rely on multi-modal fusion for safety, but current visual and optical sensors fail to detect road-induced excitations which are critical for vehicles' dynamic control. Inspired by human synesthesia, we propose the Synesthesia of Vehicles (SoV), a novel framework to predict tactile excitations from visual inputs for autonomous vehicles. We develop a cross-modal spatiotemporal alignment method to address temporal and spatial disparities. Furthermore, a visual-tactile synesthetic (VTSyn) generative model using latent diffusion is proposed for unsupervised high-quality tactile data synthesis. A real-vehicle perception system collected a multi-modal dataset across diverse road and lighting conditions. Extensive experiments show that VTSyn outperforms existing models in temporal, frequency, and classification performance, enhancing AV safety through proactive tactile perception.", "AI": {"tldr": "The paper proposes Synesthesia of Vehicles (SoV), a novel framework that predicts tactile road excitations from visual inputs using cross-modal spatiotemporal alignment and a latent diffusion model (VTSyn) to improve autonomous vehicle safety by enabling proactive tactile perception.", "motivation": "Current autonomous vehicle sensors fail to detect road-induced excitations that are critical for vehicle dynamic control, creating a safety limitation that needs to be addressed.", "method": "1) Proposes the SoV framework inspired by human synesthesia; 2) Develops cross-modal spatiotemporal alignment to resolve temporal and spatial disparities between visual and tactile data; 3) Introduces VTSyn, a latent diffusion-based generative model for unsupervised high-quality tactile data synthesis; 4) Collects real-vehicle multi-modal dataset across diverse road and lighting conditions.", "result": "Extensive experiments demonstrate that VTSyn outperforms existing models across temporal, frequency, and classification performance metrics for tactile excitation prediction.", "conclusion": "The SoV framework successfully enables proactive tactile perception from visual inputs, enhancing autonomous vehicle safety by providing critical road excitation information that traditional sensors cannot capture."}}
{"id": "2602.00624", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00624", "abs": "https://arxiv.org/abs/2602.00624", "authors": ["Hyekyung Yoon", "Minhyuk Lee", "Imseung Park", "Myungjoo Kang"], "title": "MoDEx: Mixture of Depth-specific Experts for Multivariate Long-term Time Series Forecasting", "comment": null, "summary": "Multivariate long-term time series forecasting (LTSF) supports critical applications such as traffic-flow management, solar-power scheduling, and electricity-transformer monitoring. The existing LTSF paradigms follow a three-stage pipeline of embedding, backbone refinement, and long-horizon prediction. However, the behaviors of individual backbone layers remain underexplored. We introduce layer sensitivity, a gradient-based metric inspired by GradCAM and effective receptive field theory, which quantifies both positive and negative contributions of each time point to a layer's latent features. Applying this metric to a three-layer MLP backbone reveals depth-specific specialization in modeling temporal dynamics in the input sequence. Motivated by these insights, we propose MoDEx, a lightweight Mixture of Depth-specific Experts, which replaces complex backbones with depth-specific MLP experts. MoDEx achieves state-of-the-art accuracy on seven real-world benchmarks, ranking first in 78 percent of cases, while using significantly fewer parameters and computational resources. It also integrates seamlessly into transformer variants, consistently boosting their performance and demonstrating robust generalizability as an efficient and high-performance LTSF framework.", "AI": {"tldr": "Proposes MoDEx, a lightweight Mixture of Depth-specific Experts for long-term time series forecasting, using layer sensitivity analysis to replace complex backbones with specialized MLP experts, achieving SOTA accuracy with fewer resources.", "motivation": "Multivariate LTSF is critical for traffic, energy, and monitoring applications, but existing three-stage pipelines don't explore individual backbone layer behaviors, leaving depth-specific specialization in temporal modeling underexplored.", "method": "Introduced layer sensitivity (gradient-based metric inspired by GradCAM and effective receptive field theory) to quantify time-point contributions to layer features, revealing depth-specific specialization; proposed MoDEx, replacing complex backbones with lightweight depth-specific MLP experts.", "result": "Achieved state-of-the-art accuracy on seven real-world benchmarks (ranked first in 78% of cases), using significantly fewer parameters and computational resources, while seamlessly boosting transformer variant performance.", "conclusion": "MoDEx demonstrates robust generalizability as an efficient, high-performance LTSF framework that reveals important depth-specific temporal modeling insights and offers a scalable solution across architectures."}}
{"id": "2602.01848", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.01848", "abs": "https://arxiv.org/abs/2602.01848", "authors": ["Salaheddin Alzu'bi", "Baran Nama", "Arda Kaz", "Anushri Eswaran", "Weiyuan Chen", "Sarvesh Khetan", "Rishab Bala", "Tu Vu", "Sewoong Oh"], "title": "ROMA: Recursive Open Meta-Agent Framework for Long-Horizon Multi-Agent Systems", "comment": null, "summary": "Current agentic frameworks underperform on long-horizon tasks. As reasoning depth increases, sequential orchestration becomes brittle, context windows impose hard limits that degrade performance, and opaque execution traces make failures difficult to localize or debug. We introduce ROMA (Recursive Open Meta-Agents), a domain-agnostic framework that addresses these limitations through recursive task decomposition and structured aggregation. ROMA decomposes goals into dependency-aware subtask trees that can be executed in parallel, while aggregation compresses and validates intermediate results to control context growth. Our framework standardizes agent construction around four modular roles --Atomizer (which decides whether a task should be decomposed), Planner, Executor, and Aggregator -- which cleanly separate orchestration from model selection and enable transparent, hierarchical execution traces. This design supports heterogeneous multi-agent systems that mix models and tools according to cost, latency, and capability. To adapt ROMA to specific tasks without fine-tuning, we further introduce GEPA$+$, an improved Genetic-Pareto prompt proposer that searches over prompts within ROMA's component hierarchy while preserving interface contracts. We show that ROMA, combined with GEPA+, delivers leading system-level performance on reasoning and long-form generation benchmarks. On SEAL-0, which evaluates reasoning over conflicting web evidence, ROMA instantiated with GLM-4.6 improves accuracy by 9.9\\% over Kimi-Researcher. On EQ-Bench, a long-form writing benchmark, ROMA enables DeepSeek-V3 to match the performance of leading closed-source models such as Claude Sonnet 4.5. Our results demonstrate that recursive, modular agent architectures can scale reasoning depth while remaining interpretable, flexible, and model-agnostic.", "AI": {"tldr": "ROMA is a recursive agent framework that decomposes long tasks into parallel subtask trees with structured aggregation, using four modular roles to improve reasoning depth, interpretability, and performance while controlling context growth.", "motivation": "Current agent frameworks struggle with long-horizon tasks due to brittle sequential orchestration, context window limits that degrade performance, and opaque execution traces that make debugging difficult.", "method": "Introduces ROMA with recursive task decomposition into dependency-aware subtask trees for parallel execution, structured aggregation to compress intermediate results, and four standardized roles (Atomizer, Planner, Executor, Aggregator). Complemented by GEPA+, a Genetic-Pareto prompt proposer that adapts the framework without fine-tuning.", "result": "ROMA + GEPA+ achieves state-of-the-art performance: 9.9% accuracy improvement on SEAL-0 reasoning benchmark with GLM-4.6, and enables DeepSeek-V3 to match Claude Sonnet 4.5 on EQ-Bench long-form writing.", "conclusion": "Recursive, modular agent architectures effectively scale reasoning depth while maintaining interpretability, flexibility, and model-agnosticism."}}
{"id": "2602.00628", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00628", "abs": "https://arxiv.org/abs/2602.00628", "authors": ["Louis Schiekiera", "Max Zimmer", "Christophe Roux", "Sebastian Pokutta", "Fritz G\u00fcnther"], "title": "From Associations to Activations: Comparing Behavioral and Hidden-State Semantic Geometry in LLMs", "comment": "25 pages including references, 15 figures, 6 tables", "summary": "We investigate the extent to which an LLM's hidden-state geometry can be recovered from its behavior in psycholinguistic experiments. Across eight instruction-tuned transformer models, we run two experimental paradigms -- similarity-based forced choice and free association -- over a shared 5,000-word vocabulary, collecting 17.5M+ trials to build behavior-based similarity matrices. Using representational similarity analysis, we compare behavioral geometries to layerwise hidden-state similarity and benchmark against FastText, BERT, and cross-model consensus. We find that forced-choice behavior aligns substantially more with hidden-state geometry than free association. In a held-out-words regression, behavioral similarity (especially forced choice) predicts unseen hidden-state similarities beyond lexical baselines and cross-model consensus, indicating that behavior-only measurements retain recoverable information about internal semantic geometry. Finally, we discuss implications for the ability of behavioral tasks to uncover hidden cognitive states.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5fc3\u7406\u8bed\u8a00\u5b66\u5b9e\u9a8c\u9a8c\u8bc1\u5927\u8bed\u8a00\u6a21\u578b\u7684\u884c\u4e3a\u53cd\u5e94\u80fd\u5426\u53cd\u6620\u5176\u5185\u90e8\u9690\u85cf\u72b6\u6001\u51e0\u4f55\u7ed3\u6784\uff0c\u53d1\u73b0\u5f3a\u5236\u9009\u62e9\u4efb\u52a1\u6bd4\u81ea\u7531\u8054\u60f3\u66f4\u80fd\u6709\u6548\u6355\u6349\u6a21\u578b\u5185\u90e8\u8bed\u4e49\u8868\u5f81\uff0c\u884c\u4e3a\u6570\u636e\u53ef\u9884\u6d4b\u672a\u89c1\u8bcd\u7684\u9690\u85cf\u72b6\u6001\u76f8\u4f3c\u6027\u3002", "motivation": "\u63a2\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u8bed\u4e49\u51e0\u4f55\u7ed3\u6784\u662f\u5426\u53ef\u901a\u8fc7\u5916\u90e8\u884c\u4e3a\u5b9e\u9a8c\uff08\u5982\u8bcd\u6c47\u76f8\u4f3c\u6027\u5224\u65ad\uff09\u88ab\u6709\u6548\u63a8\u65ad\uff0c\u4ee5\u8bc4\u4f30\u884c\u4e3a\u4efb\u52a1\u5bf9\u63ed\u793a\u6a21\u578b\"\u8ba4\u77e5\u72b6\u6001\"\u7684\u80fd\u529b\u3002", "method": "\u5bf98\u4e2a\u6307\u4ee4\u5fae\u8c03Transformer\u6a21\u578b\u5f00\u5c55\u4e24\u9879\u5fc3\u7406\u8bed\u8a00\u5b66\u5b9e\u9a8c\uff08\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u5f3a\u5236\u9009\u62e9/\u81ea\u7531\u8054\u60f3\uff09\uff0c\u8986\u76d65000\u8bcd\u8868\u5e76\u6536\u96c61750\u4e07+\u8bd5\u6b21\u6570\u636e\uff1b\u901a\u8fc7\u8868\u5f81\u76f8\u4f3c\u6027\u5206\u6790\uff08RSA\uff09\u5bf9\u6bd4\u884c\u4e3a\u51e0\u4f55\u7ed3\u6784\u4e0e\u6a21\u578b\u5404\u5c42\u9690\u85cf\u72b6\u6001\u76f8\u4f3c\u6027\uff0c\u5e76\u4e0eFastText/BERT/\u8de8\u6a21\u578b\u5171\u8bc6\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5f3a\u5236\u9009\u62e9\u884c\u4e3a\u4e0e\u9690\u85cf\u72b6\u6001\u51e0\u4f55\u7ed3\u6784\u7684\u4e00\u81f4\u6027\u663e\u8457\u9ad8\u4e8e\u81ea\u7531\u8054\u60f3\uff1b\u5728\u7559\u4e00\u8bcd\u56de\u5f52\u6d4b\u8bd5\u4e2d\uff0c\u884c\u4e3a\u76f8\u4f3c\u6027\uff08\u5c24\u5176\u5f3a\u5236\u9009\u62e9\uff09\u80fd\u8d85\u8d8a\u8bcd\u6c47\u57fa\u7ebf\u4e0e\u8de8\u6a21\u578b\u5171\u8bc6\uff0c\u663e\u8457\u9884\u6d4b\u672a\u89c1\u8bcd\u7684\u9690\u85cf\u72b6\u6001\u76f8\u4f3c\u6027\u3002", "conclusion": "\u884c\u4e3a\u6d4b\u91cf\uff08\u7279\u522b\u662f\u7ed3\u6784\u5316\u4efb\u52a1\uff09\u80fd\u6709\u6548\u4fdd\u7559\u5e76\u53cd\u6620\u6a21\u578b\u5185\u90e8\u8bed\u4e49\u51e0\u4f55\u4fe1\u606f\uff0c\u652f\u6301\"\u901a\u8fc7\u884c\u4e3a\u4efb\u52a1\u53ef\u63a2\u6d4b\u9690\u85cf\u8ba4\u77e5\u72b6\u6001\"\u7684\u89c2\u70b9\uff0c\u4e3aAI\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u65b0\u8def\u5f84\u3002"}}
{"id": "2602.01858", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01858", "abs": "https://arxiv.org/abs/2602.01858", "authors": ["Liangtao Lin", "Zhaomeng Zhu", "Tianwei Zhang", "Yonggang Wen"], "title": "SOPRAG: Multi-view Graph Experts Retrieval for Industrial Standard Operating Procedures", "comment": null, "summary": "Standard Operating Procedures (SOPs) are essential for ensuring operational safety and consistency in industrial environments. However, retrieving and following these procedures presents unique challenges, such as rigid proprietary structures, condition-dependent relevance, and actionable execution requirement, which standard semantic-driven Retrieval-Augmented Generation (RAG) paradigms fail to address. Inspired by the Mixture-of-Experts (MoE) paradigm, we propose SOPRAG, a novel framework specifically designed to address the above pain points in SOP retrieval. SOPRAG replaces flat chunking with specialized Entity, Causal, and Flow graph experts to resolve industrial structural and logical complexities. To optimize and coordinate these experts, we propose a Procedure Card layer that prunes the search space to eliminate computational noise, and an LLM-Guided gating mechanism that dynamically weights these experts to align retrieval with operator intent. To address the scarcity of domain-specific data, we also introduce an automated, multi-agent workflow for benchmark construction. Extensive experiments across four industrial domains demonstrate that SOPRAG significantly outperforms strong lexical, dense, and graph-based RAG baselines in both retrieval accuracy and response utility, achieving perfect execution scores in real-world critical tasks.", "AI": {"tldr": "\u63d0\u51faSOPRAG\u6846\u67b6\uff0c\u57fa\u4e8eMoE\u8303\u5f0f\uff0c\u901a\u8fc7\u5b9e\u4f53\u3001\u56e0\u679c\u3001\u6d41\u7a0b\u56fe\u4e13\u5bb6\u89e3\u51b3\u5de5\u4e1aSOP\u68c0\u7d22\u7684\u7ed3\u6784\u903b\u8f91\u590d\u6742\u6027\uff0c\u914d\u5408\u8fc7\u7a0b\u5361\u7247\u5c42\u526a\u679d\u548cLLM\u5f15\u5bfc\u95e8\u63a7\u673a\u5236\uff0c\u5728\u56db\u4e2a\u5de5\u4e1a\u9886\u57df\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u5e76\u5b9e\u73b0\u5b8c\u7f8e\u6267\u884c\u8bc4\u5206", "motivation": "\u5de5\u4e1a\u6807\u51c6\u64cd\u4f5c\u7a0b\u5e8f(SOP)\u68c0\u7d22\u9762\u4e34\u4e13\u6709\u7ed3\u6784\u50f5\u5316\u3001\u6761\u4ef6\u76f8\u5173\u6027\u5f3a\u3001\u9700\u53ef\u6267\u884c\u6027\u7b49\u6311\u6218\uff0c\u6807\u51c6RAG\u8303\u5f0f\u65e0\u6cd5\u6709\u6548\u5904\u7406", "method": "\u63d0\u51faSOPRAG\u6846\u67b6\uff1a1) \u7528\u5b9e\u4f53\u3001\u56e0\u679c\u3001\u6d41\u7a0b\u56fe\u4e13\u5bb6\u66ff\u4ee3\u6241\u5e73\u5206\u5757\uff1b2) \u8bbe\u8ba1\u8fc7\u7a0b\u5361\u7247\u5c42\u526a\u679d\u641c\u7d22\u7a7a\u95f4\uff1b3) LLM\u5f15\u5bfc\u95e8\u63a7\u673a\u5236\u52a8\u6001\u52a0\u6743\u4e13\u5bb6\uff1b4) \u81ea\u52a8\u5316\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u6784\u5efa\u57fa\u51c6", "result": "\u5728\u56db\u4e2a\u5de5\u4e1a\u9886\u57df\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSOPRAG\u5728\u68c0\u7d22\u51c6\u786e\u7387\u548c\u54cd\u5e94\u6548\u7528\u4e0a\u663e\u8457\u4f18\u4e8e\u8bcd\u6cd5\u3001\u7a20\u5bc6\u548c\u57fa\u4e8e\u56fe\u7684RAG\u57fa\u7ebf\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u5173\u952e\u4efb\u52a1\u4e2d\u8fbe\u5230\u5b8c\u7f8e\u6267\u884c\u5206\u6570", "conclusion": "SOPRAG\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5de5\u4e1aSOP\u68c0\u7d22\u7684\u7279\u6b8a\u6311\u6218\uff0c\u4e3a\u64cd\u4f5c\u5b89\u5168\u548c\u4e00\u81f4\u6027\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6280\u672f\u65b9\u6848"}}
{"id": "2602.00636", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.00636", "abs": "https://arxiv.org/abs/2602.00636", "authors": ["Yujie Yang", "Zhilong Zheng", "Shengbo Eben Li"], "title": "Equilibrium of Feasible Zone and Uncertain Model in Safe Exploration", "comment": null, "summary": "Ensuring the safety of environmental exploration is a critical problem in reinforcement learning (RL). While limiting exploration to a feasible zone has become widely accepted as a way to ensure safety, key questions remain unresolved: what is the maximum feasible zone achievable through exploration, and how can it be identified? This paper, for the first time, answers these questions by revealing that the goal of safe exploration is to find the equilibrium between the feasible zone and the environment model. This conclusion is based on the understanding that these two components are interdependent: a larger feasible zone leads to a more accurate environment model, and a more accurate model, in turn, enables exploring a larger zone. We propose the first equilibrium-oriented safe exploration framework called safe equilibrium exploration (SEE), which alternates between finding the maximum feasible zone and the least uncertain model. Using a graph formulation of the uncertain model, we prove that the uncertain model obtained by SEE is monotonically refined, the feasible zones monotonically expand, and both converge to the equilibrium of safe exploration. Experiments on classic control tasks show that our algorithm successfully expands the feasible zones with zero constraint violation, and achieves the equilibrium of safe exploration within a few iterations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5b89\u5168\u5747\u8861\u63a2\u7d22\uff08SEE\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u5730\u5728\u6700\u5927\u53ef\u884c\u533a\u57df\u548c\u6700\u5c0f\u4e0d\u786e\u5b9a\u73af\u5883\u6a21\u578b\u4e4b\u95f4\u5bfb\u627e\u5e73\u8861\u70b9\uff0c\u5b9e\u73b0\u96f6\u7ea6\u675f\u8fdd\u53cd\u7684\u5b89\u5168\u63a2\u7d22\uff0c\u5e76\u5728\u7ecf\u5178\u63a7\u5236\u4efb\u52a1\u4e2d\u5feb\u901f\u6536\u655b\u81f3\u5747\u8861\u72b6\u6001\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5b89\u5168\u63a2\u7d22\u9700\u8981\u9650\u5236\u5728\u53ef\u884c\u533a\u57df\u5185\uff0c\u4f46\u6700\u5927\u53ef\u884c\u533a\u57df\u7684\u5b9a\u4e49\u4e0e\u8bc6\u522b\u65b9\u6cd5\u5c1a\u672a\u89e3\u51b3\u3002\u53ef\u884c\u533a\u57df\u4e0e\u73af\u5883\u6a21\u578b\u76f8\u4e92\u4f9d\u8d56\uff1a\u533a\u57df\u8d8a\u5927\u6a21\u578b\u8d8a\u7cbe\u786e\uff0c\u6a21\u578b\u8d8a\u7cbe\u786e\u53ef\u63a2\u7d22\u533a\u57df\u8d8a\u5927\uff0c\u8fd9\u4e00\u5173\u7cfb\u9700\u8981\u5f62\u5f0f\u5316\u5206\u6790\u3002", "method": "\u63d0\u51faSEE\u6846\u67b6\uff0c\u4ea4\u66ff\u6267\u884c\u4e24\u4e2a\u76ee\u6807\uff1a\u5bfb\u627e\u6700\u5927\u53ef\u884c\u533a\u57df\u548c\u6784\u5efa\u6700\u5c0f\u4e0d\u786e\u5b9a\u6a21\u578b\u3002\u91c7\u7528\u56fe\u7ed3\u6784\u5efa\u6a21\u4e0d\u786e\u5b9a\u6a21\u578b\uff0c\u5e76\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u5355\u8c03\u5730\u7ec6\u5316\u6a21\u578b\u3001\u6269\u5c55\u53ef\u884c\u533a\u57df\u3002", "result": "\u7406\u8bba\u8bc1\u660eSEE\u83b7\u5f97\u7684\u4e0d\u786e\u5b9a\u6a21\u578b\u5355\u8c03\u4f18\u5316\u3001\u53ef\u884c\u533a\u57df\u5355\u8c03\u6269\u5c55\u4e14\u4e24\u8005\u5747\u6536\u655b\u81f3\u5b89\u5168\u63a2\u7d22\u5747\u8861\u70b9\u3002\u7ecf\u5178\u63a7\u5236\u4efb\u52a1\u5b9e\u9a8c\u663e\u793a\u7b97\u6cd5\u5b9e\u73b0\u96f6\u7ea6\u675f\u8fdd\u53cd\u7684\u540c\u65f6\u6210\u529f\u6269\u5c55\u53ef\u884c\u533a\u57df\uff0c\u4ec5\u9700\u6570\u6b21\u8fed\u4ee3\u5373\u8fbe\u5747\u8861\u3002", "conclusion": "\u5b89\u5168\u63a2\u7d22\u7684\u672c\u8d28\u662f\u5bfb\u627e\u53ef\u884c\u533a\u57df\u4e0e\u73af\u5883\u6a21\u578b\u7684\u5747\u8861\u70b9\uff0c\u4e8c\u8005\u76f8\u4e92\u4fc3\u8fdb\u3002SEE\u6846\u67b6\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u9996\u6b21\u5b9e\u73b0\u4e86\u8fd9\u4e00\u76ee\u6807\uff0c\u5728\u4fdd\u8bc1\u5b89\u5168\u7684\u524d\u63d0\u4e0b\u6700\u5927\u5316\u63a2\u7d22\u80fd\u529b\uff0c\u4e3a\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u65b9\u6cd5\u3002"}}
{"id": "2602.01869", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01869", "abs": "https://arxiv.org/abs/2602.01869", "authors": ["Qirui Mi", "Zhijian Ma", "Mengyue Yang", "Haoxuan Li", "Yisen Wang", "Haifeng Zhang", "Jun Wang"], "title": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents", "comment": "20 Pages, 6 Figures, 4 Tables", "summary": "LLM-driven agents demonstrate strong performance in sequential decision-making but often rely on on-the-fly reasoning, re-deriving solutions even in recurring scenarios. This insufficient experience reuse leads to computational redundancy and execution instability. To bridge this gap, we propose ProcMEM, a framework that enables agents to autonomously learn procedural memory from interaction experiences without parameter updates. By formalizing a Skill-MDP, ProcMEM transforms passive episodic narratives into executable Skills defined by activation, execution, and termination conditions to ensure executability. To achieve reliable reusability without capability degradation, we introduce Non-Parametric PPO, which leverages semantic gradients for high-quality candidate generation and a PPO Gate for robust Skill verification. Through score-based maintenance, ProcMEM sustains compact, high-quality procedural memory. Experimental results across in-domain, cross-task, and cross-agent scenarios demonstrate that ProcMEM achieves superior reuse rates and significant performance gains with extreme memory compression. Visualized evolutionary trajectories and Skill distributions further reveal how ProcMEM transparently accumulates, refines, and reuses procedural knowledge to facilitate long-term autonomy.", "AI": {"tldr": "ProcMEM\u662f\u4e00\u4e2a\u4f7fLLM\u667a\u80fd\u4f53\u80fd\u591f\u81ea\u4e3b\u4ece\u4ea4\u4e92\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u7a0b\u5e8f\u6027\u8bb0\u5fc6\u7684\u6846\u67b6\uff0c\u901a\u8fc7Skill-MDP\u5f62\u5f0f\u5316\u548c\u975e\u53c2\u6570PPO\u5b9e\u73b0\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u7684\u7ecf\u9a8c\u590d\u7528\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5197\u4f59\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "LLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u5728\u5e8f\u5217\u51b3\u7b56\u4e2d\u8868\u73b0\u5f3a\u52b2\uff0c\u4f46\u8fc7\u5ea6\u4f9d\u8d56\u5373\u65f6\u63a8\u7406\uff0c\u5728\u91cd\u590d\u573a\u666f\u4e2d\u91cd\u590d\u63a8\u5bfc\u89e3\u51b3\u65b9\u6848\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5197\u4f59\u548c\u6267\u884c\u4e0d\u7a33\u5b9a\uff0c\u7ecf\u9a8c\u590d\u7528\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51faProcMEM\u6846\u67b6\uff0c\u901a\u8fc7Skill-MDP\u5f62\u5f0f\u5316\u5c06\u88ab\u52a8\u60c5\u8282\u8f6c\u5316\u4e3a\u5305\u542b\u6fc0\u6d3b\u3001\u6267\u884c\u548c\u7ec8\u6b62\u6761\u4ef6\u7684\u53ef\u6267\u884c\u6280\u80fd\u3002\u5f15\u5165\u975e\u53c2\u6570PPO\uff0c\u5229\u7528\u8bed\u4e49\u68af\u5ea6\u751f\u6210\u9ad8\u8d28\u91cf\u5019\u9009\u6280\u80fd\uff0c\u901a\u8fc7PPO\u95e8\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u5206\u6570\u7684\u7ef4\u62a4\u673a\u5236\u4fdd\u6301\u8bb0\u5fc6\u7d27\u51d1\u6027\u3002", "result": "\u5728\u57df\u5185\u3001\u8de8\u4efb\u52a1\u548c\u8de8\u667a\u80fd\u4f53\u573a\u666f\u4e2d\uff0cProcMEM\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u590d\u7528\u7387\u548c\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u5177\u5907\u6781\u9ad8\u7684\u8bb0\u5fc6\u538b\u7f29\u7387\u3002\u53ef\u89c6\u5316\u6f14\u5316\u8f68\u8ff9\u548c\u6280\u80fd\u5206\u5e03\u5c55\u793a\u4e86\u7a0b\u5e8f\u6027\u77e5\u8bc6\u7684\u900f\u660e\u79ef\u7d2f\u3001\u7cbe\u70bc\u548c\u590d\u7528\u3002", "conclusion": "ProcMEM\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u6784\u5efa\u7d27\u51d1\u3001\u9ad8\u8d28\u91cf\u7684\u7a0b\u5e8f\u6027\u8bb0\u5fc6\uff0c\u901a\u8fc7\u6709\u6548\u5b66\u4e60\u3001\u9a8c\u8bc1\u548c\u590d\u7528\u6280\u80fd\u6765\u4fc3\u8fdb\u957f\u671f\u81ea\u4e3b\u6027\u3002"}}
{"id": "2602.00647", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00647", "abs": "https://arxiv.org/abs/2602.00647", "authors": ["Noorain Mukhtiar", "Adnan Mahmood", "Quan Z. Sheng"], "title": "CoRe-Fed: Bridging Collaborative and Representation Fairness via Federated Embedding Distillation", "comment": "7 pages (main content), 2 pages (references), Accepted in AAAI 2026", "summary": "With the proliferation of distributed data sources, Federated Learning (FL) has emerged as a key approach to enable collaborative intelligence through decentralized model training while preserving data privacy. However, conventional FL algorithms often suffer from performance disparities across clients caused by heterogeneous data distributions and unequal participation, which leads to unfair outcomes. Specifically, we focus on two core fairness challenges, i.e., representation bias, arising from misaligned client representations, and collaborative bias, stemming from inequitable contribution during aggregation, both of which degrade model performance and generalizability. To mitigate these disparities, we propose CoRe-Fed, a unified optimization framework that bridges collaborative and representation fairness via embedding-level regularization and fairness-aware aggregation. Initially, an alignment-driven mechanism promotes semantic consistency between local and global embeddings to reduce representational divergence. Subsequently, a dynamic reward-penalty-based aggregation strategy adjusts each client's weight based on participation history and embedding alignment to ensure contribution-aware aggregation. Extensive experiments across diverse models and datasets demonstrate that CoRe-Fed improves both fairness and model performance over the state-of-the-art baseline algorithms.", "AI": {"tldr": "CoRe-Fed is a novel federated learning framework that addresses representation and collaborative fairness biases through embedding-level regularization and dynamic reward-penalty aggregation, improving both fairness and model performance over state-of-the-art methods.", "motivation": "Conventional federated learning suffers from performance disparities across clients due to heterogeneous data distributions and unequal participation, causing unfair outcomes through two core challenges: representation bias from misaligned client representations and collaborative bias from inequitable contribution during aggregation.", "method": "The CoRe-Fed framework employs a two-pronged approach: (1) an alignment-driven mechanism that promotes semantic consistency between local and global embeddings to reduce representational divergence, and (2) a dynamic reward-penalty-based aggregation strategy that adjusts client weights based on participation history and embedding alignment.", "result": "Extensive experiments across diverse models and datasets demonstrate that CoRe-Fed improves both fairness and model performance over state-of-the-art baseline algorithms.", "conclusion": "CoRe-Fed successfully bridges collaborative and representation fairness through its unified optimization framework, effectively mitigating fairness challenges in federated learning while enhancing overall model performance."}}
{"id": "2602.01893", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01893", "abs": "https://arxiv.org/abs/2602.01893", "authors": ["Timur Mudarisov", "Mikhal Burtsev", "Tatiana Petrova", "Radu State"], "title": "Geometric Analysis of Token Selection in Multi-Head Attention", "comment": null, "summary": "We present a geometric framework for analysing multi-head attention in large language models (LLMs). Without altering the mechanism, we view standard attention through a top-N selection lens and study its behaviour directly in value-state space. We define geometric metrics - Precision, Recall, and F-score - to quantify separability between selected and non-selected tokens, and derive non-asymptotic bounds with explicit dependence on dimension and margin under empirically motivated assumptions (stable value norms with a compressed sink token, exponential similarity decay, and piecewise attention weight profiles). The theory predicts a small-N operating regime of strongest non-trivial separability and clarifies how sequence length and sink similarity shape the metrics. Empirically, across LLaMA-2-7B, Gemma-7B, and Mistral-7B, measurements closely track the theoretical envelopes: top-N selection sharpens separability, sink similarity correlates with Recall. We also found that in LLaMA-2-7B heads specialize into three regimes - Retriever, Mixer, Reset - with distinct geometric signatures. Overall, attention behaves as a structured geometric classifier with measurable criteria for token selection, offering head level interpretability and informing geometry-aware sparsification and design of attention in LLMs.", "AI": {"tldr": "\u63d0\u51fa\u51e0\u4f55\u6846\u67b6\u5c06\u591a\u5934\u6ce8\u610f\u529b\u89c6\u4e3a\u503c\u7a7a\u95f4\u4e2d\u7684top-N\u9009\u62e9\u673a\u5236\uff0c\u5b9a\u4e49\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u7b49\u51e0\u4f55\u6307\u6807\u91cf\u5316token\u53ef\u5206\u79bb\u6027\uff0c\u63a8\u5bfc\u7406\u8bba\u8fb9\u754c\u5e76\u5728LLaMA-2\u3001Gemma\u3001Mistral\u7b49\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u53d1\u73b0\u6ce8\u610f\u529b\u5448\u73b0\u7ed3\u6784\u5316\u51e0\u4f55\u5206\u7c7b\u5668\u7279\u6027\uff0c\u63ed\u793a\u4e09\u79cd\u5934\u4e13\u4e1a\u5316\u6a21\u5f0f\uff0c\u4e3a\u6ce8\u610f\u529b\u8bbe\u8ba1\u4e0e\u7a00\u758f\u5316\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u6307\u5bfc\u3002", "motivation": "\u7406\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u591a\u5934\u6ce8\u610f\u529b\u7684\u5185\u90e8\u5de5\u4f5c\u673a\u5236\uff0c\u5c06\u5176\u89e3\u91ca\u4e3a\u4e00\u79cd\u7ed3\u6784\u5316\u51e0\u4f55\u5206\u7c7b\u5668\uff0c\u4e3a\u6ce8\u610f\u529b\u673a\u5236\u7684\u4f18\u5316\u8bbe\u8ba1\u3001\u7a00\u758f\u5316\u538b\u7f29\u4ee5\u53ca\u5934\u7ea7\u522b\u7684\u53ef\u89e3\u91ca\u6027\u5206\u6790\u63d0\u4f9b\u7406\u8bba\u6846\u67b6\u3002", "method": "\u5728\u4e0d\u6539\u53d8\u673a\u5236\u7684\u524d\u63d0\u4e0b\uff0c\u4ecetop-N\u9009\u62e9\u89c6\u89d2\u5206\u6790\u6807\u51c6\u6ce8\u610f\u529b\uff0c\u5728\u503c-\u72b6\u6001\u7a7a\u95f4\u4e2d\u5b9a\u4e49Precision\u3001Recall\u3001F-score\u7b49\u51e0\u4f55\u6307\u6807\u6765\u91cf\u5316\u9009\u4e2d\u4e0e\u975e\u9009\u4e2dtoken\u4e4b\u95f4\u7684\u53ef\u5206\u79bb\u6027\uff1b\u5728\u7a33\u5b9a\u503c\u8303\u6570\u3001\u538b\u7f29\u6c47\u805atoken\u3001\u6307\u6570\u76f8\u4f3c\u5ea6\u8870\u51cf\u548c\u5206\u6bb5\u6ce8\u610f\u529b\u6743\u91cd\u7b49\u7ecf\u9a8c\u5047\u8bbe\u4e0b\uff0c\u63a8\u5bfc\u663e\u5f0f\u4f9d\u8d56\u4e8e\u7ef4\u5ea6\u548c\u95f4\u9694\u7684\u975e\u6e10\u8fd1\u7406\u8bba\u8fb9\u754c\u3002", "result": "\u7406\u8bba\u9884\u6d4b\u5b58\u5728\u53ef\u5206\u79bb\u6027\u6700\u5f3a\u7684\u5c0fN\u5de5\u4f5c\u533a\u95f4\uff0c\u4e14\u5e8f\u5217\u957f\u5ea6\u548c\u6c47\u805a\u76f8\u4f3c\u5ea6\u663e\u8457\u5f71\u54cd\u51e0\u4f55\u6307\u6807\uff1b\u5728LLaMA-2-7B\u3001Gemma-7B\u3001Mistral-7B\u4e0a\u7684\u5b9e\u6d4b\u6570\u636e\u4e0e\u7406\u8bba\u66f2\u7ebf\u9ad8\u5ea6\u543b\u5408\uff0ctop-N\u9009\u62e9\u80fd\u9510\u5316\u53ef\u5206\u79bb\u6027\uff0c\u6c47\u805a\u76f8\u4f3c\u5ea6\u4e0eRecall\u76f8\u5173\uff1bLLaMA-2-7B\u4e2d\u7684\u6ce8\u610f\u529b\u5934\u53ef\u5212\u5206\u4e3a\u68c0\u7d22\u5668\u3001\u6df7\u5408\u5668\u3001\u91cd\u7f6e\u5668\u4e09\u79cd\u5177\u6709\u4e0d\u540c\u51e0\u4f55\u7279\u5f81\u7684\u4e13\u4e1a\u5316 regime\u3002", "conclusion": "\u6ce8\u610f\u529b\u673a\u5236\u672c\u8d28\u4e0a\u662f\u4e00\u79cd\u5177\u6709\u53ef\u6d4b\u91cf\u9009\u62e9\u6807\u51c6\u7684\u7ed3\u6784\u5316\u51e0\u4f55\u5206\u7c7b\u5668\uff0c\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u5934\u7ea7\u522b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u51e0\u4f55\u611f\u77e5\u7684\u6ce8\u610f\u529b\u7a00\u758f\u5316\u65b9\u6cd5\u548c\u5927\u8bed\u8a00\u6a21\u578b\u6ce8\u610f\u529b\u673a\u5236\u6539\u8fdb\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2602.00654", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00654", "abs": "https://arxiv.org/abs/2602.00654", "authors": ["Jiaming Ma", "Guanjun Wang", "Qihe Huang", "Sheng Huang", "Haofeng Ma", "Zhengyang Zhou", "Pengkun Wang", "Binwu Wang", "Yang Wang"], "title": "PHAT: Modeling Period Heterogeneity for Multivariate Time Series Forecasting", "comment": null, "summary": "While existing multivariate time series forecasting models have advanced significantly in modeling periodicity, they largely neglect the periodic heterogeneity common in real-world data, where variates exhibit distinct and dynamically changing periods. To effectively capture this periodic heterogeneity, we propose PHAT (Period Heterogeneity-Aware Transformer). Specifically, PHAT arranges multivariate inputs into a three-dimensional \"periodic bucket\" tensor, where the dimensions correspond to variate group characteristics with similar periodicity, time steps aligned by phase, and offsets within the period. By restricting interactions within buckets and masking cross-bucket connections, PHAT effectively avoids interference from inconsistent periods. We also propose a positive-negative attention mechanism, which captures periodic dependencies from two perspectives: periodic alignment and periodic deviation. Additionally, the periodic alignment attention scores are decomposed into positive and negative components, with a modulation term encoding periodic priors. This modulation constrains the attention mechanism to more faithfully reflect the underlying periodic trends. A mathematical explanation is provided to support this property. We evaluate PHAT comprehensively on 14 real-world datasets against 18 baselines, and the results show that it significantly outperforms existing methods, achieving highly competitive forecasting performance. Our sources is available at GitHub.", "AI": {"tldr": "\u63d0\u51faPHAT\u6a21\u578b\u89e3\u51b3\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u5468\u671f\u6027\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u4e09\u7ef4\u5468\u671f\u6876\u5f20\u91cf\u548c\u6b63\u8d1f\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u6355\u83b7\u4e0d\u540c\u53d8\u91cf\u7684\u52a8\u6001\u53d8\u5316\u5468\u671f\uff0c\u572814\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e18\u4e2a\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u73b0\u6709\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u5728\u5efa\u6a21\u5468\u671f\u6027\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5927\u591a\u5ffd\u7565\u4e86\u73b0\u5b9e\u6570\u636e\u4e2d\u666e\u904d\u5b58\u5728\u7684\u5468\u671f\u6027\u5f02\u8d28\u6027\uff0c\u5373\u4e0d\u540c\u53d8\u91cf\u8868\u73b0\u51fa\u4e0d\u540c\u4e14\u52a8\u6001\u53d8\u5316\u7684\u5468\u671f\uff0c\u8fd9\u9650\u5236\u4e86\u9884\u6d4b\u7cbe\u5ea6", "method": "\u8bbe\u8ba1PHAT\uff08\u5468\u671f\u5f02\u8d28\u6027\u611f\u77e5Transformer\uff09\u6a21\u578b\uff1a1)\u5c06\u591a\u5143\u8f93\u5165\u7ec4\u7ec7\u4e3a\u4e09\u7ef4\u5468\u671f\u6876\u5f20\u91cf\uff08\u53d8\u91cf\u7ec4\u3001\u76f8\u4f4d\u5bf9\u9f50\u65f6\u95f4\u6b65\u3001\u5468\u671f\u5185\u504f\u79fb\uff09\uff1b2)\u9650\u5236\u6876\u5185\u4ea4\u4e92\u5e76\u63a9\u7801\u8de8\u6876\u8fde\u63a5\u4ee5\u907f\u514d\u5468\u671f\u51b2\u7a81\uff1b3)\u63d0\u51fa\u6b63\u8d1f\u6ce8\u610f\u529b\u673a\u5236\u4ece\u5468\u671f\u5bf9\u9f50\u548c\u504f\u79bb\u4e24\u4e2a\u89d2\u5ea6\u6355\u83b7\u5468\u671f\u4f9d\u8d56\uff1b4)\u5c06\u6ce8\u610f\u529b\u5206\u6570\u5206\u89e3\u4e3a\u6b63\u8d1f\u5206\u91cf\u5e76\u52a0\u5165\u7f16\u7801\u5468\u671f\u5148\u9a8c\u7684\u8c03\u5236\u9879", "result": "\u572814\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u4e0e18\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\uff0cPHAT\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6781\u5177\u7ade\u4e89\u529b\u7684\u9884\u6d4b\u6027\u80fd", "conclusion": "PHAT\u901a\u8fc7\u521b\u65b0\u7684\u5468\u671f\u6876\u67b6\u6784\u548c\u6b63\u8d1f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u5468\u671f\u6027\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u4e3a\u66f4\u7cbe\u51c6\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2602.01910", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01910", "abs": "https://arxiv.org/abs/2602.01910", "authors": ["Michele Fiori", "Gabriele Civitarese", "Flora D. Salim", "Claudio Bettini"], "title": "DomusFM: A Foundation Model for Smart-Home Sensor Data", "comment": null, "summary": "Smart-home sensor data holds significant potential for several applications, including healthcare monitoring and assistive technologies. Existing approaches, however, face critical limitations. Supervised models require impractical amounts of labeled data. Foundation models for activity recognition focus only on inertial sensors, failing to address the unique characteristics of smart-home binary sensor events: their sparse, discrete nature combined with rich semantic associations. LLM-based approaches, while tested in this domain, still raise several issues regarding the need for natural language descriptions or prompting, and reliance on either external services or expensive hardware, making them infeasible in real-life scenarios due to privacy and cost concerns. We introduce DomusFM, the first foundation model specifically designed and pretrained for smart-home sensor data. DomusFM employs a self-supervised dual contrastive learning paradigm to capture both token-level semantic attributes and sequence-level temporal dependencies. By integrating semantic embeddings from a lightweight language model and specialized encoders for temporal patterns and binary states, DomusFM learns generalizable representations that transfer across environments and tasks related to activity and event analysis. Through leave-one-dataset-out evaluation across seven public smart-home datasets, we demonstrate that DomusFM outperforms state-of-the-art baselines on different downstream tasks, achieving superior performance even with only 5% of labeled training data available for fine-tuning. Our approach addresses data scarcity while maintaining practical deployability for real-world smart-home systems.", "AI": {"tldr": "DomusFM: \u9996\u4e2a\u9488\u5bf9\u667a\u80fd\u5bb6\u5c45\u4f20\u611f\u5668\u6570\u636e\u7684\u57fa\u5ea7\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u53cc\u5bf9\u6bd4\u5b66\u4e60\uff0c\u4ec5\u75285%\u6807\u6ce8\u6570\u636e\u5373\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u9690\u79c1\u6210\u672c\u95ee\u9898", "motivation": "\u667a\u80fd\u5bb6\u5c45\u4f20\u611f\u5668\u6570\u636e\u5728\u5065\u5eb7\u76d1\u6d4b\u548c\u8f85\u52a9\u6280\u672f\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5173\u952e\u5c40\u9650\uff1a\u76d1\u7763\u5b66\u4e60\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff1b\u6d3b\u52a8\u8bc6\u522b\u57fa\u5ea7\u6a21\u578b\u4ec5\u5173\u6ce8\u60ef\u6027\u4f20\u611f\u5668\uff0c\u5ffd\u89c6\u4e8c\u8fdb\u5236\u4f20\u611f\u5668\u4e8b\u4ef6\u7684\u7a00\u758f\u6027\u548c\u8bed\u4e49\u5173\u8054\uff1b\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u9700\u8981\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u6216\u5916\u90e8\u670d\u52a1\uff0c\u5b58\u5728\u9690\u79c1\u548c\u6210\u672c\u95ee\u9898", "method": "\u63d0\u51faDomusFM\uff0c\u9996\u4e2a\u4e13\u4e3a\u667a\u80fd\u5bb6\u5c45\u4f20\u611f\u5668\u6570\u636e\u8bbe\u8ba1\u7684\u57fa\u5ea7\u6a21\u578b\u3002\u91c7\u7528\u81ea\u76d1\u7763\u53cc\u5bf9\u6bd4\u5b66\u4e60\u8303\u5f0f\uff0c\u6355\u83b7\u4ee4\u724c\u7ea7\u8bed\u4e49\u5c5e\u6027\u548c\u5e8f\u5217\u7ea7\u65f6\u5e8f\u4f9d\u8d56\u3002\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u5d4c\u5165\u3001\u65f6\u5e8f\u6a21\u5f0f\u7f16\u7801\u5668\u548c\u4e8c\u8fdb\u5236\u72b6\u6001\u4e13\u7528\u7f16\u7801\u5668\uff0c\u5b66\u4e60\u53ef\u8de8\u73af\u5883\u548c\u4efb\u52a1\u8fc1\u79fb\u7684\u901a\u7528\u8868\u5f81", "result": "\u57287\u4e2a\u516c\u5f00\u667a\u80fd\u5bb6\u5c45\u6570\u636e\u96c6\u4e0a\u7684\u7559\u4e00\u6570\u636e\u96c6\u4ea4\u53c9\u9a8c\u8bc1\u663e\u793a\uff0cDomusFM\u5728\u4e0d\u540c\u4e0b\u6e38\u4efb\u52a1\u4e0a\u5168\u9762\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u75285%\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u5fae\u8c03\u5373\u8fbe\u5230\u5353\u8d8a\u6027\u80fd", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u771f\u5b9e\u4e16\u754c\u667a\u80fd\u5bb6\u5c45\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.00656", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00656", "abs": "https://arxiv.org/abs/2602.00656", "authors": ["Yingxu Wang", "Xinwang Liu", "Mengzhu Wang", "Siyang Gao", "Nan Yin"], "title": "Riemannian Flow Matching for Disentangled Graph Domain Adaptation", "comment": null, "summary": "Graph Domain Adaptation (GDA) typically uses adversarial learning to align graph embeddings in Euclidean space. However, this paradigm suffers from two critical challenges: Structural Degeneration, where hierarchical and semantic representations are entangled, and Optimization Instability, which arises from oscillatory dynamics of minimax adversarial training. To tackle these issues, we propose DisRFM, a geometry-aware GDA framework that unifies Riemannian embedding and flow-based transport. First, to overcome structural degeneration, we embed graphs into a Riemannian manifold. By adopting polar coordinates, we explicitly disentangle structure (radius) from semantics (angle). Then, we enforce topology preservation through radial Wasserstein alignment and semantic discrimination via angular clustering, thereby preventing feature entanglement and collapse. Second, we address the instability of adversarial alignment by using Riemannian flow matching. This method learns a smooth vector field to guide source features toward the target along geodesic paths, guaranteeing stable convergence. The geometric constraints further guide the flow to maintain the disentangled structure during transport. Theoretically, we prove the asymptotic stability of the flow matching and derive a tighter bound for the target risk. Extensive experiments demonstrate that DisRFM consistently outperforms state-of-the-art methods.", "AI": {"tldr": "DisRFM\u662f\u4e00\u4e2a\u51e0\u4f55\u611f\u77e5\u7684\u56fe\u57df\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u9ece\u66fc\u6d41\u5f62\u5d4c\u5165\u548c\u6d41\u5339\u914d\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u5bf9\u6297\u5b66\u4e60\u4e2d\u7ed3\u6784\u9000\u5316\u548c\u4f18\u5316\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u6781\u5750\u6807\u5206\u79bb\u7ed3\u6784\u4e0e\u8bed\u4e49\uff0c\u5e76\u6cbf\u6d4b\u5730\u7ebf\u8def\u5f84\u5b9e\u73b0\u7a33\u5b9a\u7279\u5f81\u4f20\u8f93\uff0c\u7406\u8bba\u548c\u5b9e\u9a8c\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u56fe\u57df\u9002\u5e94\uff08GDA\uff09\u7684\u5bf9\u6297\u5b66\u4e60\u65b9\u6cd5\u5728\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e2d\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u7ed3\u6784\u9000\u5316\uff08\u5c42\u6b21\u4e0e\u8bed\u4e49\u8868\u793a\u7ea0\u7f20\uff09\u548c\u4f18\u5316\u4e0d\u7a33\u5b9a\uff08\u6781\u5c0f\u6781\u5927\u8bad\u7ec3\u7684\u632f\u8361\u52a8\u6001\uff09\u3002", "method": "\u63d0\u51faDisRFM\u6846\u67b6\uff1a1\uff09\u5c06\u56fe\u5d4c\u5165\u9ece\u66fc\u6d41\u5f62\uff0c\u7528\u6781\u5750\u6807\u89e3\u8026\u7ed3\u6784\uff08\u534a\u5f84\uff09\u4e0e\u8bed\u4e49\uff08\u89d2\u5ea6\uff09\uff0c\u901a\u8fc7\u5f84\u5411Wasserstein\u5bf9\u9f50\u548c\u89d2\u5411\u805a\u7c7b\u9632\u6b62\u7279\u5f81\u7ea0\u7f20\uff1b2\uff09\u91c7\u7528\u9ece\u66fc\u6d41\u5339\u914d\u66ff\u4ee3\u5bf9\u6297\u8bad\u7ec3\uff0c\u5b66\u4e60\u5e73\u6ed1\u5411\u91cf\u573a\u5f15\u5bfc\u7279\u5f81\u6cbf\u6d4b\u5730\u7ebf\u8fc1\u79fb\uff0c\u5b9e\u73b0\u7a33\u5b9a\u6536\u655b\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86\u6d41\u5339\u914d\u7684\u6e10\u8fd1\u7a33\u5b9a\u6027\u5e76\u63a8\u5bfc\u51fa\u66f4\u7d27\u7684\u76ee\u6807\u98ce\u9669\u754c\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660eDisRFM\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "DisRFM\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u7684\u8868\u793a\u5b66\u4e60\u548c\u6d41\u5339\u914d\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86GDA\u4e2d\u7684\u7ed3\u6784\u9000\u5316\u548c\u4f18\u5316\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u8df5\u65b9\u6848\u3002"}}
{"id": "2602.01933", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01933", "abs": "https://arxiv.org/abs/2602.01933", "authors": ["Fabrice Boissier", "Monica Sen", "Irina Rychkova"], "title": "Large Language Model and Formal Concept Analysis: a comparative study for Topic Modeling", "comment": null, "summary": "Topic modeling is a research field finding increasing applications: historically from document retrieving, to sentiment analysis and text summarization. Large Language Models (LLM) are currently a major trend in text processing, but few works study their usefulness for this task. Formal Concept Analysis (FCA) has recently been presented as a candidate for topic modeling, but no real applied case study has been conducted. In this work, we compare LLM and FCA to better understand their strengths and weakneses in the topic modeling field. FCA is evaluated through the CREA pipeline used in past experiments on topic modeling and visualization, whereas GPT-5 is used for the LLM. A strategy based on three prompts is applied with GPT-5 in a zero-shot setup: topic generation from document batches, merging of batch results into final topics, and topic labeling. A first experiment reuses the teaching materials previously used to evaluate CREA, while a second experiment analyzes 40 research articles in information systems to compare the extracted topics with the underling subfields.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6bd4\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b(GPT-5)\u548c\u5f62\u5f0f\u6982\u5ff5\u5206\u6790(FCA)\u5728\u4e3b\u9898\u5efa\u6a21\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u96f6\u6837\u672c\u63d0\u793a\u7b56\u7565\u548cCREA\u8bc4\u4f30\u7ba1\u9053\uff0c\u5728\u6559\u5b66\u6750\u6599\u548c\u4fe1\u606f\u7cfb\u7edf\u7814\u7a76\u6587\u7ae0\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002", "motivation": "\u5f53\u524dLLM\u5728\u6587\u672c\u5904\u7406\u4e2d\u6210\u4e3a\u4e3b\u6d41\u8d8b\u52bf\u4f46\u5176\u5728\u4e3b\u9898\u5efa\u6a21\u4e2d\u7684\u5b9e\u7528\u6027\u7814\u7a76\u4e0d\u8db3\uff0cFCA\u867d\u88ab\u63d0\u51fa\u7528\u4e8e\u4e3b\u9898\u5efa\u6a21\u4f46\u7f3a\u4e4f\u5b9e\u9645\u5e94\u7528\u6848\u4f8b\uff0c\u672c\u7814\u7a76\u65e8\u5728\u6bd4\u8f83\u4e8c\u8005\u7684\u4f18\u52a3\u52bf\u3002", "method": "\u4f7f\u7528GPT-5\u91c7\u7528\u4e09\u9636\u6bb5\u96f6\u6837\u672c\u63d0\u793a\uff08\u6587\u6863\u6279\u6b21\u4e3b\u9898\u751f\u6210\u3001\u6279\u6b21\u7ed3\u679c\u5408\u5e76\u3001\u4e3b\u9898\u6807\u6ce8\uff09\uff0cFCA\u901a\u8fc7CREA\u7ba1\u9053\u8bc4\u4f30\uff1b\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff1a\u5386\u53f2\u7528\u4e8e\u8bc4\u4f30CREA\u7684\u6559\u5b66\u6750\u6599\uff0c\u4ee5\u53ca40\u7bc7\u4fe1\u606f\u7cfb\u7edf\u9886\u57df\u7814\u7a76\u6587\u7ae0\u3002", "result": "\u6458\u8981\u4e2d\u672a\u62a5\u544a\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4ec5\u8bf4\u660e\u5b9e\u9a8c\u8bbe\u8ba1\u548c\u8bc4\u4f30\u6846\u67b6\u3002", "conclusion": "\u6458\u8981\u4e2d\u672a\u660e\u786e\u7ed9\u51fa\u7814\u7a76\u7ed3\u8bba\uff0c\u4ec5\u63cf\u8ff0\u4e86\u7814\u7a76\u65b9\u6cd5\u4e0e\u5b9e\u9a8c\u8bbe\u7f6e\u3002"}}
{"id": "2602.00670", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.00670", "abs": "https://arxiv.org/abs/2602.00670", "authors": ["Ashna Purwar", "Gaurav Simkar", "Madhumita", "Sachin Kadam"], "title": "Three-Way Emotion Classification of EEG-based Signals using Machine Learning", "comment": "6 pages, 8 figures, and 3 tables. Submitted to a conference, under review", "summary": "Electroencephalography (EEG) is a widely used technique for measuring brain activity. EEG-based signals can reveal a persons emotional state, as they directly reflect activity in different brain regions. Emotion-aware systems and EEG-based emotion recognition are a growing research area. This paper presents how machine learning (ML) models categorize a limited dataset of EEG signals into three different classes, namely Negative, Neutral, or Positive. It also presents the complete workflow, including data preprocessing and comparison of ML models. To understand which ML classification model works best for this kind of problem, we train and test the following three commonly used models: logistic regression (LR), support vector machine (SVM), and random forest (RF). The performance of each is evaluated with respect to accuracy and F1-score. The results indicate that ML models can be effectively utilized for three-way emotion classification of EEG signals. Among the three ML models trained on the available dataset, the RF model gave the best results. Its higher accuracy and F1-score suggest that it is able to capture the emotional patterns more accurately and effectively than the other two models. The RF model also outperformed the existing state-of-the-art classification models in terms of the accuracy parameter.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5bf9\u6709\u9650EEG\u4fe1\u53f7\u6570\u636e\u96c6\u8fdb\u884c\u4e09\u5206\u7c7b\uff08\u8d1f\u9762/\u4e2d\u6027/\u6b63\u9762\uff09\u60c5\u7eea\u8bc6\u522b\uff0c\u53d1\u73b0\u968f\u673a\u68ee\u6797\u6a21\u578b\u5728\u51c6\u786e\u7387\u548cF1\u5206\u6570\u4e0a\u5747\u4f18\u4e8e\u903b\u8f91\u56de\u5f52\u3001SVM\u53ca\u73b0\u6709\u5148\u8fdb\u6a21\u578b", "motivation": "EEG\u4fe1\u53f7\u80fd\u76f4\u63a5\u53cd\u6620\u5927\u8111\u6d3b\u52a8\u4e0e\u60c5\u7eea\u72b6\u6001\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u6709\u9650\u6570\u636e\u96c6\u4e0a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u60c5\u7eea\u5206\u7c7b\u6548\u679c\u7684\u5145\u5206\u6bd4\u8f83", "method": "\u91c7\u7528\u5b8c\u6574\u5de5\u4f5c\u6d41\uff1aEEG\u6570\u636e\u9884\u5904\u7406\u540e\uff0c\u8bad\u7ec3\u5e76\u5bf9\u6bd4\u903b\u8f91\u56de\u5f52(LR)\u3001\u652f\u6301\u5411\u91cf\u673a(SVM)\u548c\u968f\u673a\u68ee\u6797(RF)\u4e09\u7c7b\u5e38\u7528\u6a21\u578b\uff0c\u4ee5\u51c6\u786e\u7387\u548cF1\u5206\u6570\u4e3a\u8bc4\u4f30\u6307\u6807", "result": "\u968f\u673a\u68ee\u6797(RF)\u6a21\u578b\u8868\u73b0\u6700\u4f18\uff0c\u5176\u51c6\u786e\u7387\u548cF1\u5206\u6570\u663e\u8457\u9ad8\u4e8eLR\u548cSVM\u6a21\u578b\uff0c\u4e14\u51c6\u786e\u7387\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u5206\u7c7b\u6a21\u578b", "conclusion": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u80fd\u6709\u6548\u5b9e\u73b0EEG\u4e09\u5206\u7c7b\u60c5\u7eea\u8bc6\u522b\uff0c\u5176\u4e2d\u968f\u673a\u68ee\u6797\u56e0\u66f4\u7cbe\u51c6\u6355\u6349\u60c5\u7eea\u6a21\u5f0f\u800c\u6700\u5177\u5e94\u7528\u6f5c\u529b\uff0c\u4e3a\u60c5\u7eea-aware\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u9760\u65b9\u6848"}}
{"id": "2602.01970", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01970", "abs": "https://arxiv.org/abs/2602.01970", "authors": ["Yun Qu", "Qi Wang", "Yixiu Mao", "Heming Zou", "Yuhang Jiang", "Weijie Liu", "Clive Bai", "Kai Yang", "Yangkun Chen", "Saiyong Yang", "Xiangyang Ji"], "title": "Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models", "comment": null, "summary": "Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS's substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods.", "AI": {"tldr": "\u63d0\u51faGeneralizable Predictive Prompt Selection (GPS)\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u751f\u6210\u6a21\u578b\u8fdb\u884c\u8d1d\u53f6\u65af\u63a8\u7406\u6765\u9884\u6d4b\u63d0\u793a\u96be\u5ea6\uff0c\u7ed3\u5408\u4e2d\u7b49\u96be\u5ea6\u4f18\u5148\u548c\u5386\u53f2\u591a\u6837\u6027\u539f\u5219\u9009\u62e9\u4fe1\u606f\u91cf\u5927\u7684\u63d0\u793a\u6279\u6b21\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u6548\u7387\u548c\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u95ee\u9898\uff0c\u73b0\u6709\u5728\u7ebf\u63d0\u793a\u9009\u62e9\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u6602\u8d35\u7684\u7cbe\u786e\u8bc4\u4f30\uff0c\u8981\u4e48\u6784\u5efa\u7f3a\u4e4f\u8de8\u63d0\u793a\u6cdb\u5316\u80fd\u529b\u7684\u63d0\u793a\u7279\u5b9a\u9884\u6d4b\u6a21\u578b\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684\u63d0\u793a\u9009\u62e9\u7b56\u7565\u6765\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "method": "GPS\u65b9\u6cd5\u5229\u7528\u5171\u4eab\u4f18\u5316\u5386\u53f2\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u751f\u6210\u6a21\u578b\uff0c\u5bf9\u63d0\u793a\u96be\u5ea6\u8fdb\u884c\u8d1d\u53f6\u65af\u63a8\u7406\u3002\u91c7\u7528\u4e2d\u7b49\u96be\u5ea6\u4f18\u5148\u539f\u5219\u548c\u5386\u53f2\u951a\u5b9a\u591a\u6837\u6027\u673a\u5236\u8fdb\u884c\u6279\u6b21\u63d0\u793a\u9009\u62e9\uff0c\u4f7f\u5c0f\u9884\u6d4b\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u5177\u5907\u6cdb\u5316\u80fd\u529b\u4ee5\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u5206\u914d\u3002", "result": "\u5728\u4e0d\u540c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGPS\u5728\u8bad\u7ec3\u6548\u7387\u3001\u6700\u7ec8\u6027\u80fd\u548c\u6d4b\u8bd5\u65f6\u6548\u7387\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6210\u672c\u4e0e\u6a21\u578b\u6027\u80fd\u7684\u826f\u597d\u5e73\u8861\u3002", "conclusion": "\u901a\u8fc7\u5171\u4eab\u5386\u53f2\u4fe1\u606f\u7684\u8d1d\u53f6\u65af\u63a8\u7406\u548c\u53ef\u6cdb\u5316\u7684\u8f7b\u91cf\u6a21\u578b\uff0cGPS\u4e3a\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u901a\u7528\u7684\u63d0\u793a\u9009\u62e9\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u540c\u65f6\u63d0\u5347\u4e86\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2602.00672", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00672", "abs": "https://arxiv.org/abs/2602.00672", "authors": ["Aleksandr Yugay", "Hang Cui", "Changhua Pei", "Alexey Zaytsev"], "title": "Strong Linear Baselines Strike Back: Closed-Form Linear Models as Gaussian Process Conditional Density Estimators for TSAD", "comment": null, "summary": "Research in time series anomaly detection (TSAD) has largely focused on developing increasingly sophisticated, hard-to-train, and expensive-to-infer neural architectures. We revisit this paradigm and show that a simple linear autoregressive anomaly score with the closed-form solution provided by ordinary least squares (OLS) regression consistently matches or outperforms state-of-the-art deep detectors. From a theoretical perspective, we show that linear models capture a broad class of anomaly types, estimating a finite-history Gaussian process conditional density. From a practical side, across extensive univariate and multivariate benchmarks, the proposed approach achieves superior accuracy while requiring orders of magnitude fewer computational resources. Thus, future research should consistently include strong linear baselines and, more importantly, develop new benchmarks with richer temporal structures pinpointing the advantages of deep learning models.", "AI": {"tldr": "A simple linear autoregressive model with OLS regression matches or exceeds complex deep learning methods in time series anomaly detection, offering superior accuracy with drastically lower computational costs.", "motivation": "Critiques the trend toward increasingly complex, hard-to-train neural architectures in time series anomaly detection, advocating for reevaluation of simpler methods.", "method": "Proposes a linear autoregressive anomaly scoring approach using ordinary least squares (OLS) regression with closed-form solution, avoiding iterative training.", "result": "Consistently matches or outperforms state-of-the-art deep detectors across benchmarks while requiring orders of magnitude fewer computational resources.", "conclusion": "Future research should include strong linear baselines and develop new benchmarks that better highlight specific advantages of deep learning models."}}
{"id": "2602.01983", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01983", "abs": "https://arxiv.org/abs/2602.01983", "authors": ["Xintian Shen", "Jiawei Chen", "Lihao Zheng", "Hao Ma", "Tao Wei", "Kun Zhan"], "title": "Evolving from Tool User to Creator via Training-Free Experience Reuse in Multimodal Reasoning", "comment": null, "summary": "Existing Tool-Integrated Reasoning (TIR) models have effectively extended the question-answering capabilities of LLMs by incorporating external tools. However, real-world scenarios present numerous open-ended problems where fixed tools often fail to meet task requirements. Furthermore, the lack of self-optimization mechanisms means that erroneous tool outputs can mislead the LLM's responses. Additionally, the construction of existing tools entails significant manual effort, which consequently constrains their applicability. Recognizing that the reasoning traces of LLMs encapsulate implicit problem-solving capabilities, we propose UCT, a novel training-free framework that transforms agents from tool users to tool creators. This approach harvests reasoning experiences and distills them into reusable assets. This method transforms the agent from a mere tool user into a tool creator, enabling adaptive tool creation and self-updating during the inference process. We also introduce a memory consolidation mechanism to maintain the tool library, ensuring high reusability of retained experiential memory for subsequent reasoning tasks. This novel automated tool construction paradigm continuously improves tool quality during reasoning, allowing the overall agent system to progress without additional training. Extensive experiments demonstrate that our method serves as a novel paradigm for enhancing the capabilities of TIR models. In particular, the significant performance gains achieved +20.86%$\\uparrow$ and +23.04%$\\uparrow$ on benchmarks across multi-domain mathematical and scientific reasoning tasks validate the self-evolving capability of the agent.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faUCT\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u4f7fLLM\u667a\u80fd\u4f53\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u81ea\u52a8\u521b\u5efa\u548c\u4f18\u5316\u5de5\u5177\uff0c\u5b9e\u73b0\u4ece\u5de5\u5177\u4f7f\u7528\u8005\u5230\u521b\u9020\u8005\u7684\u8f6c\u53d8\uff0c\u5e76\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709TIR\u6a21\u578b\u5728\u5f00\u653e\u573a\u666f\u4e2d\u5b58\u5728\u56fa\u5b9a\u5de5\u5177\u5931\u6548\u3001\u7f3a\u4e4f\u81ea\u4f18\u5316\u673a\u5236\u4ee5\u53ca\u5de5\u5177\u6784\u5efa\u6210\u672c\u9ad8\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "UCT\u6846\u67b6\u901a\u8fc7\u63d0\u53d6LLM\u63a8\u7406\u75d5\u8ff9\u81ea\u52a8\u751f\u6210\u53ef\u590d\u7528\u5de5\u5177\uff0c\u7ed3\u5408\u8bb0\u5fc6consolidation\u673a\u5236\u7ef4\u62a4\u5de5\u5177\u5e93\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u81ea\u9002\u5e94\u5de5\u5177\u521b\u5efa\u548c\u81ea\u6211\u66f4\u65b0\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u5728\u6570\u5b66\u548c\u79d1\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5206\u522b\u53d6\u5f97+20.86%\u548c+23.04%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u667a\u80fd\u4f53\u7684\u81ea\u6211\u8fdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8fd9\u79cd\u65b0\u9896\u7684\u81ea\u52a8\u5316\u5de5\u5177\u6784\u5efa\u8303\u5f0f\u4e3a\u589e\u5f3aTIR\u6a21\u578b\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65e0\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u6301\u7eed\u6539\u8fdb\u5de5\u5177\u8d28\u91cf\u5b9e\u73b0\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6301\u7eed\u8fdb\u6b65\u3002"}}
{"id": "2602.00688", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00688", "abs": "https://arxiv.org/abs/2602.00688", "authors": ["Tom Segal", "Asaf Shabtai", "Yuval Elovici"], "title": "Provably Protecting Fine-Tuned LLMs from Training Data Extraction", "comment": "20 pages, 5 figures", "summary": "Fine-tuning large language models (LLMs) on sensitive datasets raises privacy concerns, as training data extraction (TDE) attacks can expose highly confidential information. Existing defenses against such attacks either lack formal privacy guarantees or incur substantial utility degradation. We observe that fine-tuning induces widespread probability shifts, yet preserving only a small subset of influential token-level deviations is sufficient; the remaining shifts can be aggressively smoothed with minimal impact on utility. Motivated by this insight, we propose SCP-$\u0394_r$, a Near Access Freeness (NAF)-based algorithm that operates on relative probabilities and explicitly smooths low-impact tokens using a base model. SCP-$\u0394_r$ achieves orders-of-magnitude better theoretical bounds than existing NAF based methods and provides strong empirical protection against TDE attacks with minimal performance loss.", "AI": {"tldr": "\u9488\u5bf9\u654f\u611f\u6570\u636e\u5fae\u8c03LLM\u7684\u9690\u79c1\u98ce\u9669\uff0c\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u8fd1\u65e0\u8bbf\u95ee\u6027(NAF)\u7684SCP-\u0394_r\u7b97\u6cd5\uff0c\u901a\u8fc7\u4fdd\u7559\u5173\u952etoken\u6982\u7387\u504f\u79fb\u5e76\u5e73\u6ed1\u4f4e\u5f71\u54cd\u529btoken\uff0c\u5b9e\u73b0\u5f3a\u9690\u79c1\u4fdd\u62a4\u4e0e\u9ad8\u6a21\u578b\u6548\u7528\u7684\u5e73\u8861\u3002", "motivation": "\u5728\u654f\u611f\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u8bad\u7ec3\u6570\u636e\u63d0\u53d6(TDE)\u653b\u51fb\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u3002\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u8981\u4e48\u7f3a\u4e4f\u5f62\u5f0f\u5316\u9690\u79c1\u4fdd\u8bc1\uff0c\u8981\u4e48\u5bfc\u81f4\u663e\u8457\u7684\u6a21\u578b\u6548\u7528\u4e0b\u964d\u3002", "method": "\u63d0\u51faSCP-\u0394_r\u7b97\u6cd5\uff0c\u57fa\u4e8e\u8fd1\u65e0\u8bbf\u95ee\u6027(NAF)\u6846\u67b6\uff0c\u64cd\u4f5c\u76f8\u5bf9\u6982\u7387\u3002\u8be5\u7b97\u6cd5\u8bc6\u522b\u5fae\u8c03\u5f15\u8d77\u7684token\u7ea7\u6982\u7387\u504f\u79fb\uff0c\u4ec5\u4fdd\u7559\u91cd\u8981\u5b50\u96c6\u7684\u504f\u5dee\uff0c\u5bf9\u4f4e\u5f71\u54cd\u529btoken\u4f7f\u7528\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u6fc0\u8fdb\u5e73\u6ed1\u3002", "result": "\u7406\u8bba\u8fb9\u754c\u6bd4\u73b0\u6709NAF\u65b9\u6cd5\u63d0\u5347\u6570\u4e2a\u6570\u91cf\u7ea7\uff1b\u5b9e\u8bc1\u4e2d\u80fd\u6709\u6548\u62b5\u5fa1TDE\u653b\u51fb\u4e14\u6027\u80fd\u635f\u5931\u6781\u5c0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u9009\u62e9\u6027\u5e73\u6ed1\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6548\u7528\u7684\u540c\u65f6\u63d0\u4f9b\u5f3a\u9690\u79c1\u4fdd\u62a4\uff0c\u4e3a\u5b89\u5168\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00693", "categories": ["cs.LG", "math.AG", "math.AT"], "pdf": "https://arxiv.org/pdf/2602.00693", "abs": "https://arxiv.org/abs/2602.00693", "authors": ["Marco Nurisso", "Pierrick Leroy", "Giovanni Petri", "Francesco Vaccarino"], "title": "Topology and Geometry of the Learning Space of ReLU Networks: Connectivity and Singularities", "comment": "Accepted to ICLR 2026. 32 pages, 13 figures", "summary": "Understanding the properties of the parameter space in feed-forward ReLU networks is critical for effectively analyzing and guiding training dynamics. After initialization, training under gradient flow decisively restricts the parameter space to an algebraic variety that emerges from the homogeneous nature of the ReLU activation function. In this study, we examine two key challenges associated with feed-forward ReLU networks built on general directed acyclic graph (DAG) architectures: the (dis)connectedness of the parameter space and the existence of singularities within it. We extend previous results by providing a thorough characterization of connectedness, highlighting the roles of bottleneck nodes and balance conditions associated with specific subsets of the network. Our findings clearly demonstrate that singularities are intricately connected to the topology of the underlying DAG and its induced sub-networks. We discuss the reachability of these singularities and establish a principled connection with differentiable pruning. We validate our theory with simple numerical experiments.", "AI": {"tldr": "This paper analyzes parameter space properties in feed-forward ReLU networks with DAG architectures, characterizing connectedness through bottleneck nodes and balance conditions, and linking singularities to graph topology and differentiable pruning.", "motivation": "Understanding parameter space properties is critical for analyzing and guiding training dynamics in ReLU networks, as gradient flow restricts parameters to an algebraic variety due to ReLU's homogeneous nature.", "method": "The study examines feed-forward ReLU networks on general DAG architectures, extending previous results through theoretical characterization of connectedness and singularity analysis, validated with numerical experiments.", "result": "Connectedness depends on bottleneck nodes and balance conditions; singularities are intrinsically linked to DAG topology and sub-networks; connections established with differentiable pruning.", "conclusion": "The parameter space structure of ReLU networks is fundamentally determined by the underlying DAG architecture, with implications for network training and pruning."}}
{"id": "2602.01995", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01995", "abs": "https://arxiv.org/abs/2602.01995", "authors": ["Jeongmoon Won", "Seungwon Kook", "Yohan Jo"], "title": "Thinking Like a Doctor: Conversational Diagnosis through the Exploration of Diagnostic Knowledge Graphs", "comment": null, "summary": "Conversational diagnosis requires multi-turn history-taking, where an agent asks clarifying questions to refine differential diagnoses under incomplete information. Existing approaches often rely on the parametric knowledge of a model or assume that patients provide rich and concrete information, which is unrealistic. To address these limitations, we propose a conversational diagnosis system that explores a diagnostic knowledge graph to reason in two steps: (i) generating diagnostic hypotheses from the dialogue context, and (ii) verifying hypotheses through clarifying questions, which are repeated until a final diagnosis is reached. Since evaluating the system requires a realistic patient simulator that responds to the system's questions, we adopt a well-established simulator along with patient profiles from MIMIC-IV. We further adapt it to describe symptoms vaguely to reflect real-world patients during early clinical encounters. Experiments show improved diagnostic accuracy and efficiency over strong baselines, and evaluations by physicians support the realism of our simulator and the clinical utility of the generated questions. Our code will be released upon publication.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u5bf9\u8bdd\u8bca\u65ad\u7cfb\u7edf\uff0c\u901a\u8fc7\u5047\u8bbe\u751f\u6210\u4e0e\u9a8c\u8bc1\u7684\u4e24\u6b65\u63a8\u7406\uff0c\u7ed3\u5408\u6a21\u7cca\u5316\u60a3\u8005\u6a21\u62df\uff0c\u63d0\u5347\u8bca\u65ad\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5bf9\u8bdd\u8bca\u65ad\u65b9\u6cd5\u4f9d\u8d56\u6a21\u578b\u53c2\u6570\u77e5\u8bc6\u6216\u5047\u8bbe\u60a3\u8005\u4fe1\u606f\u4e30\u5bcc\u5177\u4f53\uff0c\u4f46\u73b0\u5b9e\u4e2d\u65e9\u671f\u4e34\u5e8a\u5c31\u8bca\u65f6\u60a3\u8005\u63cf\u8ff0\u5f80\u5f80\u6a21\u7cca\uff0c\u4e0d\u7b26\u5408\u771f\u5b9e\u573a\u666f\u3002\u9700\u8981\u6784\u5efa\u80fd\u5904\u7406\u4e0d\u5b8c\u5168\u4fe1\u606f\u7684\u771f\u5b9e\u8bca\u65ad\u7cfb\u7edf\u3002", "method": "\u6784\u5efa\u8bca\u65ad\u77e5\u8bc6\u56fe\u8c31\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u63a8\u7406\u6846\u67b6\uff1a(1)\u4ece\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u751f\u6210\u8bca\u65ad\u5047\u8bbe\uff1b(2)\u901a\u8fc7\u6f84\u6e05\u95ee\u9898\u9a8c\u8bc1\u5047\u8bbe\u76f4\u81f3\u786e\u8bca\u3002\u4f7f\u7528MIMIC-IV\u60a3\u8005\u6863\u6848\u5efa\u7acb\u6a21\u62df\u5668\uff0c\u5e76\u4f7f\u5176\u75c7\u72b6\u63cf\u8ff0\u6a21\u7cca\u5316\u4ee5\u6a21\u62df\u771f\u5b9e\u60a3\u8005\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8bca\u65ad\u51c6\u786e\u7387\u548c\u6548\u7387\u5747\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff1b\u533b\u751f\u8bc4\u4f30\u8bc1\u5b9e\u6a21\u62df\u5668\u7684\u771f\u5b9e\u6027\u548c\u751f\u6210\u95ee\u9898\u7684\u4e34\u5e8a\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u80fd\u6709\u6548\u652f\u6301\u591a\u8f6e\u5bf9\u8bdd\u8bca\u65ad\uff0c\u5728\u771f\u5b9e\u6027\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2602.02018", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02018", "abs": "https://arxiv.org/abs/2602.02018", "authors": ["Enes Altinisik", "Masoomali Fatehkia", "Fatih Deniz", "Nadir Durrani", "Majd Hawasly", "Mohammad Raza", "Husrev Taha Sencar"], "title": "Do I Really Know? Learning Factual Self-Verification for Hallucination Reduction", "comment": null, "summary": "Factual hallucination remains a central challenge for large language models (LLMs). Existing mitigation approaches primarily rely on either external post-hoc verification or mapping uncertainty directly to abstention during fine-tuning, often resulting in overly conservative behavior. We propose VeriFY, a training-time framework that teaches LLMs to reason about factual uncertainty through consistency-based self-verification. VeriFY augments training with structured verification traces that guide the model to produce an initial answer, generate and answer a probing verification query, issue a consistency judgment, and then decide whether to answer or abstain. To address the risk of reinforcing hallucinated content when training on augmented traces, we introduce a stage-level loss masking approach that excludes hallucinated answer stages from the training objective while preserving supervision over verification behavior. Across multiple model families and scales, VeriFY reduces factual hallucination rates by 9.7 to 53.3 percent, with only modest reductions in recall (0.4 to 5.7 percent), and generalizes across datasets when trained on a single source. The source code, training data, and trained model checkpoints will be released upon acceptance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faVeriFY\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u9a8c\u8bc1\u8f68\u8ff9\u548c\u9636\u6bb5\u7ea7\u635f\u5931\u63a9\u7801\uff0c\u5728\u8bad\u7ec3\u65f6\u6559\u4f1a\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u6211\u9a8c\u8bc1\u4e8b\u5b9e\u4e0d\u786e\u5b9a\u6027\uff0c\u5c06\u5e7b\u89c9\u7387\u964d\u4f4e9.7-53.3%\uff0c\u540c\u65f6\u4ec5\u7565\u5fae\u964d\u4f4e\u53ec\u56de\u73870.4-5.7%\u3002", "motivation": "\u4e8b\u5b9e\u5e7b\u89c9\u662f\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6838\u5fc3\u6311\u6218\uff0c\u73b0\u6709\u7f13\u89e3\u65b9\u6cd5\u4f9d\u8d56\u4e8b\u540e\u9a8c\u8bc1\u6216\u4e0d\u786e\u5b9a\u6027\u6620\u5c04\uff0c\u5e38\u5bfc\u81f4\u8fc7\u5ea6\u4fdd\u5b88\u884c\u4e3a\uff0c\u5f71\u54cd\u5b9e\u7528\u6027\u3002", "method": "VeriFY\u5728\u8bad\u7ec3\u65f6\u5f15\u5165\u7ed3\u6784\u5316\u9a8c\u8bc1\u8f68\u8ff9\uff0c\u5f15\u5bfc\u6a21\u578b\u5b8c\u6210\u56db\u6b65\u6d41\u7a0b\uff1a\u751f\u6210\u521d\u59cb\u7b54\u6848\u3001\u63d0\u51fa\u5e76\u56de\u7b54\u9a8c\u8bc1\u95ee\u9898\u3001\u4e00\u81f4\u6027\u5224\u65ad\u3001\u51b3\u5b9a\u56de\u7b54\u6216\u62d2\u7edd\u3002\u91c7\u7528\u9636\u6bb5\u7ea7\u635f\u5931\u63a9\u7801\u6280\u672f\uff0c\u5728\u8bad\u7ec3\u4e2d\u6392\u9664\u5e7b\u89c9\u7b54\u6848\u9636\u6bb5\u7684\u76d1\u7763\uff0c\u4f46\u4fdd\u7559\u9a8c\u8bc1\u884c\u4e3a\u7684\u8bad\u7ec3\u4fe1\u53f7\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u5bb6\u65cf\u548c\u89c4\u6a21\u4e0a\uff0c\u4e8b\u5b9e\u5e7b\u89c9\u7387\u964d\u4f4e9.7-53.3%\uff0c\u53ec\u56de\u7387\u4ec5\u4e0b\u964d0.4-5.7%\uff0c\u4e14\u5728\u5355\u4e00\u6570\u636e\u6e90\u8bad\u7ec3\u540e\u80fd\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u3002", "conclusion": "VeriFY\u80fd\u6709\u6548\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u5b9e\u5e7b\u89c9\uff0c\u6027\u80fd\u635f\u5931\u5c0f\uff0c\u6cdb\u5316\u80fd\u529b\u5f3a\uff0c\u4e3a\u5e7b\u89c9\u7f13\u89e3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.00704", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00704", "abs": "https://arxiv.org/abs/2602.00704", "authors": ["Hanqi Lyu", "Di Huang", "Yaoyu Zhu", "Kangcheng Liu", "Bohan Dou", "Chongxiao Li", "Pengwei Jin", "Shuyao Cheng", "Rui Zhang", "Zidong Du", "Qi Guo", "Xing Hu", "Yunji Chen"], "title": "LocalV: Exploiting Information Locality for IP-level Verilog Generation", "comment": null, "summary": "The generation of Register-Transfer Level (RTL) code is a crucial yet labor-intensive step in digital hardware design, traditionally requiring engineers to manually translate complex specifications into thousands of lines of synthesizable Hardware Description Language (HDL) code. While Large Language Models (LLMs) have shown promise in automating this process, existing approaches-including fine-tuned domain-specific models and advanced agent-based systems-struggle to scale to industrial IP-level design tasks. We identify three key challenges: (1) handling long, highly detailed documents, where critical interface constraints become buried in unrelated submodule descriptions; (2) generating long RTL code, where both syntactic and semantic correctness degrade sharply with increasing output length; and (3) navigating the complex debugging cycles required for functional verification through simulation and waveform analysis. To overcome these challenges, we propose LocalV, a multi-agent framework that leverages information locality in modular hardware design. LocalV decomposes the long-document to long-code generation problem into a set of short-document, short-code tasks, enabling scalable generation and debugging. Specifically, LocalV integrates hierarchical document partitioning, task planning, localized code generation, interface-consistent merging, and AST-guided locality-aware debugging. Experiments on RealBench, an IP-level Verilog generation benchmark, demonstrate that LocalV substantially outperforms state-of-the-art (SOTA) LLMs and agents, achieving a pass rate of 45.0% compared to 21.6%.", "AI": {"tldr": "\u63d0\u51faLocalV\u591a\u667a\u80fd\u4f53\u6846\u67b6\u89e3\u51b3RTL\u4ee3\u7801\u751f\u6210\u96be\u9898\uff0c\u901a\u8fc7\u4fe1\u606f\u5c40\u90e8\u6027\u5206\u89e3\u957f\u6587\u6863\u751f\u6210\u957f\u4ee3\u7801\u95ee\u9898\uff0c\u5728\u5de5\u4e1a\u7ea7\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u901a\u8fc7\u7387\u8fbe45.0%\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd521.6%\u7684\u6c34\u5e73", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5de5\u4e1a\u7ea7IP\u8bbe\u8ba1RTL\u4ee3\u7801\u751f\u6210\u4e2d\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u957f\u6587\u6863\u5173\u952e\u7ea6\u675f\u6613\u88ab\u6df9\u6ca1\u3001\u957f\u4ee3\u7801\u751f\u6210\u65f6\u8bed\u6cd5\u8bed\u4e49\u6b63\u786e\u6027\u9aa4\u964d\u3001\u529f\u80fd\u9a8c\u8bc1\u8c03\u8bd5\u5468\u671f\u590d\u6742\uff0c\u4e9f\u9700\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6LocalV\uff0c\u6838\u5fc3\u521b\u65b0\u5305\u62ec\uff1a\u5c42\u6b21\u5316\u6587\u6863\u5206\u533a\u3001\u4efb\u52a1\u89c4\u5212\u3001\u5c40\u90e8\u4ee3\u7801\u751f\u6210\u3001\u63a5\u53e3\u4e00\u81f4\u6027\u5408\u5e76\u3001AST\u5f15\u5bfc\u7684\u5c40\u90e8\u611f\u77e5\u8c03\u8bd5\uff0c\u5c06\u957f\u6587\u6863-\u957f\u4ee3\u7801\u95ee\u9898\u5206\u89e3\u4e3a\u77ed\u6587\u6863-\u77ed\u4ee3\u7801\u5b50\u4efb\u52a1", "result": "\u5728RealBench\u5de5\u4e1a\u7ea7Verilog\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLocalV\u901a\u8fc7\u7387\u8fbe45.0%\uff0c\u8f83\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\uff0821.6%\uff09\u63d0\u5347\u8d85\u4e00\u500d\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u5728\u5de5\u4e1a\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027", "conclusion": "\u901a\u8fc7\u5229\u7528\u786c\u4ef6\u8bbe\u8ba1\u7684\u6a21\u5757\u5316\u5c40\u90e8\u7279\u6027\uff0cLocalV\u5b9e\u73b0\u4e86RTL\u4ee3\u7801\u751f\u6210\u7684\u53ef\u6269\u5c55\u6027\u7a81\u7834\uff0c\u4e3a\u5de5\u4e1a\u7ea7\u786c\u4ef6\u8bbe\u8ba1\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f"}}
{"id": "2602.02027", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02027", "abs": "https://arxiv.org/abs/2602.02027", "authors": ["Sicheng Shen", "Mingyang Lv", "Han Shen", "Jialin Wu", "Binghao Wang", "Zhou Yang", "Guobin Shen", "Dongcheng Zhao", "Feifei Zhao", "Yi Zeng"], "title": "Light Alignment Improves LLM Safety via Model Self-Reflection with a Single Neuron", "comment": "21 pages, 3 figures", "summary": "The safety of large language models (LLMs) has increasingly emerged as a fundamental aspect of their development. Existing safety alignment for LLMs is predominantly achieved through post-training methods, which are computationally expensive and often fail to generalize well across different models. A small number of lightweight alignment approaches either rely heavily on prior-computed safety injections or depend excessively on the model's own capabilities, resulting in limited generalization and degraded efficiency and usability during generation. In this work, we propose a safety-aware decoding method that requires only low-cost training of an expert model and employs a single neuron as a gating mechanism. By effectively balancing the model's intrinsic capabilities with external guidance, our approach simultaneously preserves utility and enhances output safety. It demonstrates clear advantages in training overhead and generalization across model scales, offering a new perspective on lightweight alignment for the safe and practical deployment of large language models. Code: https://github.com/Beijing-AISI/NGSD.", "AI": {"tldr": "\u63d0\u51faNGSD\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u795e\u7ecf\u5143\u95e8\u63a7\u673a\u5236\u548c\u4f4e\u6210\u672c\u4e13\u5bb6\u6a21\u578b\u8bad\u7ec3\uff0c\u5b9e\u73b0LLM\u5b89\u5168\u5bf9\u9f50\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u80fd\u529b\u7684\u540c\u65f6\u63d0\u5347\u5b89\u5168\u6027\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u8de8\u6a21\u578b\u6cdb\u5316\u6027\u3002", "motivation": "\u73b0\u6709LLM\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff1a\u540e\u8bad\u7ec3\u65b9\u5f0f\u8017\u8d44\u6e90\uff0c\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u9884\u8ba1\u7b97\u5b89\u5168\u6ce8\u5165\u6216\u6a21\u578b\u81ea\u8eab\u80fd\u529b\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u548c\u53ef\u7528\u6027\u4e0b\u964d\u3002", "method": "\u5b89\u5168\u611f\u77e5\u89e3\u7801\u65b9\u6cd5\uff0c\u4ec5\u9700\u4f4e\u6210\u672c\u8bad\u7ec3\u4e13\u5bb6\u6a21\u578b\uff0c\u91c7\u7528\u5355\u795e\u7ecf\u5143\u4f5c\u4e3a\u95e8\u63a7\u673a\u5236\uff0c\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u6709\u6548\u5e73\u8861\u6a21\u578b\u5185\u5728\u80fd\u529b\u4e0e\u5916\u90e8\u5b89\u5168\u6307\u5bfc\u3002", "result": "\u5728\u8bad\u7ec3\u5f00\u9500\u548c\u8de8\u6a21\u578b\u89c4\u6a21\u6cdb\u5316\u65b9\u9762\u5177\u6709\u660e\u663e\u4f18\u52bf\uff0c\u80fd\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6548\u7528\u5e76\u589e\u5f3a\u8f93\u51fa\u5b89\u5168\u6027\u3002", "conclusion": "\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u5b9e\u7528\u90e8\u7f72\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u5bf9\u9f50\u7684\u65b0\u601d\u8def\u3002"}}
{"id": "2602.00717", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00717", "abs": "https://arxiv.org/abs/2602.00717", "authors": ["Licheng Pan", "Hao Wang", "Haocheng Yang", "Yuqi Li", "Qingsong Wen", "Xiaoxi Li", "Zhichao Chen", "Haoxuan Li", "Zhixuan Chu", "Yuan Lu"], "title": "Deep Time-series Forecasting Needs Kernelized Moment Balancing", "comment": null, "summary": "Deep time-series forecasting can be formulated as a distribution balancing problem aimed at aligning the distribution of the forecasts and ground truths. According to Imbens' criterion, true distribution balance requires matching the first moments with respect to any balancing function. We demonstrate that existing objectives fail to meet this criterion, as they enforce moment matching only for one or two predefined balancing functions, thus failing to achieve full distribution balance. To address this limitation, we propose direct forecasting with kernelized moment balancing (KMB-DF). Unlike existing objectives, KMB-DF adaptively selects the most informative balancing functions from a reproducing kernel hilbert space (RKHS) to enforce sufficient distribution balancing. We derive a tractable and differentiable objective that enables efficient estimation from empirical samples and seamless integration into gradient-based training pipelines. Extensive experiments across multiple models and datasets show that KMB-DF consistently improves forecasting accuracy and achieves state-of-the-art performance. Code is available at https://anonymous.4open.science/r/KMB-DF-403C.", "AI": {"tldr": "This paper proposes KMB-DF, a novel deep time-series forecasting method that uses kernelized moment balancing to achieve full distribution alignment between forecasts and ground truths, outperforming existing methods.", "motivation": "Deep time-series forecasting can be framed as a distribution balancing problem, but existing methods fail to meet Imbens' criterion for true balance because they only match moments for a few predefined functions rather than all possible balancing functions.", "method": "The authors propose KMB-DF (direct forecasting with kernelized moment balancing) which adaptively selects the most informative balancing functions from a reproducing kernel Hilbert space (RKHS) and derives a tractable, differentiable objective for efficient empirical estimation and gradient-based training.", "result": "Extensive experiments across multiple models and datasets demonstrate that KMB-DF consistently improves forecasting accuracy and achieves state-of-the-art performance.", "conclusion": "KMB-DF effectively addresses the distribution balancing limitation in existing methods and provides a practical solution for more accurate deep time-series forecasting."}}
{"id": "2602.02028", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02028", "abs": "https://arxiv.org/abs/2602.02028", "authors": ["Ya Gao", "Kalle Kujanp\u00e4\u00e4", "Pekka Marttinen", "Harri Valpola", "Alexander Ilin"], "title": "Edit Knowledge, Not Just Facts via Multi-Step Reasoning over Background Stories", "comment": "under review", "summary": "Enabling artificial intelligence systems, particularly large language models, to integrate new knowledge and flexibly apply it during reasoning remains a central challenge. Existing knowledge editing approaches emphasize atomic facts, improving factual recall but often failing to integrate new information into a coherent framework usable across contexts. In this work, we argue that knowledge internalization is fundamentally a reasoning problem rather than a memorization problem. Consequently, a model should be trained in situations where the new information is instrumental to solving a task, combined with pre-existing knowledge, and exercised through multi-step reasoning. Based on this insight, we propose a training strategy based on three principles. First, new knowledge is introduced as a coherent background story that contextualizes novel facts and explains their relation to existing knowledge. Second, models are trained using self-generated multi-hop questions that require multi-step reasoning involving the new information. Third, training is done using knowledge distillation, forcing a student model to internalize the teacher's reasoning behavior without access to the novel information. Experiments show that models trained with this strategy effectively leverage newly acquired knowledge during reasoning and achieve remarkable performance on challenging questions that require combining multiple new facts.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u63a8\u7406\u7684\u77e5\u8bc6\u5185\u5316\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u8fde\u8d2f\u80cc\u666f\u6545\u4e8b\u3001\u591a\u8df3\u95ee\u9898\u548c\u77e5\u8bc6\u84b8\u998f\uff0c\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u6709\u6548\u6574\u5408\u65b0\u77e5\u8bc6\u5e76\u7075\u6d3b\u5e94\u7528\u4e8e\u591a\u6b65\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u4fa7\u91cd\u539f\u5b50\u4e8b\u5b9e\u8bb0\u5fc6\uff0c\u65e0\u6cd5\u5c06\u65b0\u77e5\u8bc6\u6574\u5408\u4e3a\u53ef\u8de8\u573a\u666f\u4f7f\u7528\u7684\u8fde\u8d2f\u6846\u67b6\uff0c\u800c\u77e5\u8bc6\u5185\u5316\u672c\u8d28\u4e0a\u662f\u63a8\u7406\u95ee\u9898\u800c\u975e\u8bb0\u5fc6\u95ee\u9898\u3002", "method": "\u4e09\u539f\u5219\u8bad\u7ec3\u7b56\u7565\uff1a1) \u5c06\u65b0\u77e5\u8bc6\u4f5c\u4e3a\u8fde\u8d2f\u80cc\u666f\u6545\u4e8b\u5f15\u5165\uff1b2) \u4f7f\u7528\u81ea\u751f\u6210\u7684\u591a\u8df3\u95ee\u9898\u8fdb\u884c\u591a\u6b65\u63a8\u7406\u8bad\u7ec3\uff1b3) \u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u8ba9\u5b66\u751f\u6a21\u578b\u5185\u5316\u6559\u5e08\u6a21\u578b\u7684\u63a8\u7406\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u4f7f\u6a21\u578b\u80fd\u6709\u6548\u5229\u7528\u65b0\u77e5\u8bc6\u8fdb\u884c\u63a8\u7406\uff0c\u5728\u9700\u8981\u7ec4\u5408\u591a\u4e2a\u65b0\u4e8b\u5b9e\u7684\u590d\u6742\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u77e5\u8bc6\u5185\u5316\u5e94\u901a\u8fc7\u8bad\u7ec3\u6a21\u578b\u5728\u65b0\u4fe1\u606f\u5bf9\u4efb\u52a1\u89e3\u51b3\u6709\u8d21\u732e\u7684\u60c5\u5883\u4e2d\u7ed3\u5408\u5148\u9a8c\u77e5\u8bc6\u5e76\u8fdb\u884c\u591a\u6b65\u63a8\u7406\u6765\u5b9e\u73b0\uff0c\u8fd9\u6bd4\u5355\u7eaf\u7684\u8bb0\u5fc6\u65b9\u6cd5\u66f4\u6709\u6548\u3002"}}
{"id": "2602.00718", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00718", "abs": "https://arxiv.org/abs/2602.00718", "authors": ["Noorain Mukhtiar", "Adnan Mahmood", "Yipeng Zhou", "Jian Yang", "Jing Teng", "Quan Z. Sheng"], "title": "Federated Learning at the Forefront of Fairness: A Multifaceted Perspective", "comment": "7 pages (main content), 2 pages (references), Accepted and Published Proceedings of the 34th International Joint Conference on Artificial Intelligence (IJCAI). 2025", "summary": "Fairness in Federated Learning (FL) is emerging as a critical factor driven by heterogeneous clients' constraints and balanced model performance across various scenarios. In this survey, we delineate a comprehensive classification of the state-of-the-art fairness-aware approaches from a multifaceted perspective, i.e., model performance-oriented and capability-oriented. Moreover, we provide a framework to categorize and address various fairness concerns and associated technical aspects, examining their effectiveness in balancing equity and performance within FL frameworks. We further examine several significant evaluation metrics leveraged to measure fairness quantitatively. Finally, we explore exciting open research directions and propose prospective solutions that could drive future advancements in this important area, laying a solid foundation for researchers working toward fairness in FL.", "AI": {"tldr": "\u4e00\u7bc7\u5173\u4e8e\u8054\u90a6\u5b66\u4e60\u516c\u5e73\u6027\u7684\u7efc\u8ff0\u8bba\u6587\uff0c\u4ece\u6a21\u578b\u6027\u80fd\u5bfc\u5411\u548c\u80fd\u529b\u5bfc\u5411\u4e24\u4e2a\u7ef4\u5ea6\u5bf9\u73b0\u6709\u516c\u5e73\u6027\u65b9\u6cd5\u8fdb\u884c\u5206\u7c7b\uff0c\u6784\u5efa\u4e86\u5e94\u5bf9\u516c\u5e73\u6027\u95ee\u9898\u7684\u6846\u67b6\u4f53\u7cfb\uff0c\u5206\u6790\u4e86\u91cf\u5316\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u63a2\u7d22\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5ba2\u6237\u7aef\u5f02\u6784\u6027\u7ea6\u675f\u548c\u591a\u6837\u5316\u573a\u666f\u4e0b\u6a21\u578b\u6027\u80fd\u5e73\u8861\u9700\u6c42\u7684\u65e5\u76ca\u7a81\u51fa\uff0c\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u516c\u5e73\u6027\u5df2\u6210\u4e3a\u5173\u952e\u8003\u91cf\u56e0\u7d20\u3002", "method": "\u4ece\u591a\u7ef4\u5ea6\u89c6\u89d2\u5bf9\u524d\u6cbf\u516c\u5e73\u6027\u65b9\u6cd5\u8fdb\u884c\u7efc\u5408\u5206\u7c7b\uff08\u5206\u4e3a\u6a21\u578b\u6027\u80fd\u5bfc\u5411\u548c\u80fd\u529b\u5bfc\u5411\uff09\uff0c\u6784\u5efa\u6846\u67b6\u4ee5\u7cfb\u7edf\u5316\u5206\u7c7b\u548c\u5e94\u5bf9\u5404\u7c7b\u516c\u5e73\u6027\u95ee\u9898\u53ca\u6280\u672f\u5c42\u9762\uff0c\u5e76\u6df1\u5165\u5206\u6790\u7528\u4e8e\u91cf\u5316\u516c\u5e73\u6027\u7684\u91cd\u8981\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u63a2\u7d22\u4e86\u591a\u4e2a\u91cd\u8981\u7684\u5f00\u653e\u6027\u7814\u7a76\u65b9\u5411\uff0c\u5e76\u63d0\u51fa\u4e86\u524d\u77bb\u6027\u89e3\u51b3\u65b9\u6848\u4ee5\u63a8\u52a8\u8be5\u9886\u57df\u672a\u6765\u53d1\u5c55\u3002", "conclusion": "\u672c\u7efc\u8ff0\u4e3a\u81f4\u529b\u4e8e\u8054\u90a6\u5b66\u4e60\u516c\u5e73\u6027\u7814\u7a76\u7684\u7814\u7a76\u4eba\u5458\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u9886\u57df\u8ba4\u77e5\u6846\u67b6\u3002"}}
{"id": "2602.02029", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.02029", "abs": "https://arxiv.org/abs/2602.02029", "authors": ["Zhongyuan Lyu", "Shuoyu Hu", "Lujie Liu", "Hongxia Yang", "Ming LI"], "title": "Canonical Intermediate Representation for LLM-based optimization problem formulation and code generation", "comment": "41 pages, 4 figures, 5 tables", "summary": "Automatically formulating optimization models from natural language descriptions is a growing focus in operations research, yet current LLM-based approaches struggle with the composite constraints and appropriate modeling paradigms required by complex operational rules. To address this, we introduce the Canonical Intermediate Representation (CIR): a schema that LLMs explicitly generate between problem descriptions and optimization models. CIR encodes the semantics of operational rules through constraint archetypes and candidate modeling paradigms, thereby decoupling rule logic from its mathematical instantiation. Upon a newly generated CIR knowledge base, we develop the rule-to-constraint (R2C) framework, a multi-agent pipeline that parses problem texts, synthesizes CIR implementations by retrieving domain knowledge, and instantiates optimization models. To systematically evaluate rule-to-constraint reasoning, we test R2C on our newly constructed benchmark featuring rich operational rules, and benchmarks from prior work. Extensive experiments show that R2C achieves state-of-the-art accuracy on the proposed benchmark (47.2% Accuracy Rate). On established benchmarks from the literature, R2C delivers highly competitive results, approaching the performance of proprietary models (e.g., GPT-5). Moreover, with a reflection mechanism, R2C achieves further gains and sets new best-reported results on some benchmarks.", "AI": {"tldr": "This paper introduces Canonical Intermediate Representation (CIR) and a multi-agent rule-to-constraint (R2C) framework to automatically formulate optimization models from natural language, achieving state-of-the-art performance by decoupling operational rule logic from mathematical instantiation.", "motivation": "Current LLM-based approaches struggle with composite constraints and appropriate modeling paradigms required by complex operational rules in automatic optimization model formulation from natural language descriptions.", "method": "The authors propose Canonical Intermediate Representation (CIR), a schema that LLMs generate between problem descriptions and optimization models, encoding operational rules through constraint archetypes and candidate modeling paradigms. They develop the R2C framework, a multi-agent pipeline that parses problem texts, synthesizes CIR implementations by retrieving domain knowledge, and instantiates optimization models using a CIR knowledge base.", "result": "R2C achieves 47.2% accuracy on a new benchmark with rich operational rules, delivers highly competitive results on established benchmarks approaching proprietary models like GPT-5, and with a reflection mechanism, achieves further gains and sets new best-reported results on some benchmarks.", "conclusion": "The CIR-based approach effectively addresses the challenge of complex operational rules in optimization model formulation, demonstrating that decoupling rule logic from mathematical instantiation through intermediate representation significantly improves performance."}}
{"id": "2602.00722", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00722", "abs": "https://arxiv.org/abs/2602.00722", "authors": ["Hao Gu", "Mao-Lin Luo", "Zi-Hao Zhou", "Han-Chen Zhang", "Min-Ling Zhang", "Tong Wei"], "title": "Spectral Imbalance Causes Forgetting in Low-Rank Continual Adaptation", "comment": "19 pages, 6 figures", "summary": "Parameter-efficient continual learning aims to adapt pre-trained models to sequential tasks without forgetting previously acquired knowledge. Most existing approaches treat continual learning as avoiding interference with past updates, rather than considering what properties make the current task-specific update naturally preserve previously acquired knowledge. From a knowledge-decomposition perspective, we observe that low-rank adaptations exhibit highly imbalanced singular value spectra: a few dominant components absorb most of the adaptation energy, thereby (i) more likely to disrupt previously acquired knowledge and (ii) making the update more vulnerable to interference from subsequent tasks. To enable explicit balance among components, we decouple the magnitude of the task update from its directional structure and formulate it as a constrained optimization problem on a restricted Stiefel manifold. We address this problem using a projected first-order method compatible with standard deep-learning optimizers used in vision-language models. Our method mitigates both backward and forward forgetting, consistently outperforming continual learning baselines. The implementation code is available at https://github.com/haodotgu/EBLoRA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faEBLoRA\uff0c\u901a\u8fc7\u89e3\u8026\u4f4e\u79e9\u9002\u914d\u7684\u5e45\u5ea6\u4e0e\u65b9\u5411\u5e76\u5728\u9650\u5236Stiefel\u6d41\u5f62\u4e0a\u8fdb\u884c\u7ea6\u675f\u4f18\u5316\uff0c\u5e73\u8861\u5947\u5f02\u503c\u8c31\u5206\u5e03\uff0c\u4ece\u800c\u51cf\u8f7b\u53c2\u6570\u9ad8\u6548\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u524d\u540e\u5411\u9057\u5fd8\u3002", "motivation": "\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u907f\u514d\u5e72\u6270\uff0c\u4f46\u5ffd\u7565\u4e86\u4efb\u52a1\u66f4\u65b0\u7684\u5185\u5728\u7279\u6027\u5982\u4f55\u81ea\u7136\u4fdd\u6301\u5df2\u6709\u77e5\u8bc6\u3002\u4f4e\u79e9\u9002\u914d\u7684\u5947\u5f02\u503c\u8c31\u9ad8\u5ea6\u4e0d\u5e73\u8861\uff0c\u5c11\u6570\u4e3b\u5bfc\u6210\u5206\u5438\u6536\u5927\u90e8\u5206\u9002\u914d\u80fd\u91cf\uff0c\u5bfc\u81f4\u66f4\u6613\u7834\u574f\u5df2\u6709\u77e5\u8bc6\u4e14\u5bf9\u672a\u6765\u4efb\u52a1\u5e72\u6270\u66f4\u654f\u611f\u3002", "method": "\u5c06\u4efb\u52a1\u66f4\u65b0\u7684\u5e45\u5ea6\u4e0e\u65b9\u5411\u89e3\u8026\uff0c\u5c06\u5176\u8868\u8ff0\u4e3a\u9650\u5236Stiefel\u6d41\u5f62\u4e0a\u7684\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u4e0e\u6807\u51c6\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u5668\u517c\u5bb9\u7684\u6295\u5f71\u4e00\u9636\u65b9\u6cd5\u6c42\u89e3\u3002", "result": "\u8be5\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u524d\u540e\u5411\u9057\u5fd8\uff0c\u5728\u6301\u7eed\u5b66\u4e60\u57fa\u7ebf\u65b9\u6cd5\u4e0a\u53d6\u5f97\u4e86\u4e00\u81f4\u6027\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u6d41\u5f62\u7ea6\u675f\u4f18\u5316\u663e\u5f0f\u5e73\u8861\u8c31\u6210\u5206\uff0cEBLoRA\u4e3a\u53c2\u6570\u9ad8\u6548\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u539f\u5219\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02034", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02034", "abs": "https://arxiv.org/abs/2602.02034", "authors": ["Ananya Joshi", "Michael Rudow"], "title": "Constrained Process Maps for Multi-Agent Generative AI Workflows", "comment": null, "summary": "Large language model (LLM)-based agents are increasingly used to perform complex, multi-step workflows in regulated settings such as compliance and due diligence. However, many agentic architectures rely primarily on prompt engineering of a single agent, making it difficult to observe or compare how models handle uncertainty and coordination across interconnected decision stages and with human oversight. We introduce a multi-agent system formalized as a finite-horizon Markov Decision Process (MDP) with a directed acyclic structure. Each agent corresponds to a specific role or decision stage (e.g., content, business, or legal review in a compliance workflow), with predefined transitions representing task escalation or completion. Epistemic uncertainty is quantified at the agent level using Monte Carlo estimation, while system-level uncertainty is captured by the MDP's termination in either an automated labeled state or a human-review state. We illustrate the approach through a case study in AI safety evaluation for self-harm detection, implemented as a multi-agent compliance system. Results demonstrate improvements over a single-agent baseline, including up to a 19\\% increase in accuracy, up to an 85x reduction in required human review, and, in some configurations, reduced processing time.", "AI": {"tldr": "This paper proposes a multi-agent system using MDP framework to improve LLM-based workflows in regulated settings, showing significant accuracy gains and massive reduction in human review needs.", "motivation": "Single-agent LLM systems in regulated domains are opaque and hard to debug, making it difficult to understand how models handle uncertainty and coordinate across decision stages with human oversight.", "method": "Formalized a multi-agent system as a finite-horizon Markov Decision Process with directed acyclic structure, where each agent represents a specific role (content/business/legal review). Used Monte Carlo estimation for epistemic uncertainty at agent level and MDP termination states to capture system-level uncertainty.", "result": "In a self-harm detection case study, the approach achieved up to 19% higher accuracy, up to 85x reduction in required human review, and reduced processing time in some configurations compared to single-agent baselines.", "conclusion": "Multi-agent MDP frameworks provide more transparent, accurate, and efficient LLM-based systems for regulated workflows while significantly reducing human oversight burden."}}
{"id": "2602.00723", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00723", "abs": "https://arxiv.org/abs/2602.00723", "authors": ["Prakhar Ganesh", "Reza Shokri", "Golnoosh Farnadi"], "title": "Rethinking Hallucinations: Correctness, Consistency, and Prompt Multiplicity", "comment": "To appear at EACL 2026", "summary": "Large language models (LLMs) are known to \"hallucinate\" by generating false or misleading outputs. Hallucinations pose various harms, from erosion of trust to widespread misinformation. Existing hallucination evaluation, however, focuses only on correctness and often overlooks consistency, necessary to distinguish and address these harms. To bridge this gap, we introduce prompt multiplicity, a framework for quantifying consistency in LLM evaluations. Our analysis reveals significant multiplicity (over 50% inconsistency in benchmarks like Med-HALT), suggesting that hallucination-related harms have been severely misunderstood. Furthermore, we study the role of consistency in hallucination detection and mitigation. We find that: (a) detection techniques detect consistency, not correctness, and (b) mitigation techniques like RAG, while beneficial, can introduce additional inconsistencies. By integrating prompt multiplicity into hallucination evaluation, we provide an improved framework of potential harms and uncover critical limitations in current detection and mitigation strategies.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\"\u63d0\u793a\u591a\u91cd\u6027\"\u6846\u67b6\u6765\u91cf\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u4e2d\u7684\u4e00\u81f4\u6027\uff0c\u53d1\u73b0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b58\u5728\u8d85\u8fc750%\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u5e76\u63ed\u793a\u5f53\u524d\u68c0\u6d4b\u65b9\u6cd5\u5b9e\u9645\u68c0\u6d4b\u7684\u662f\u4e00\u81f4\u6027\u800c\u975e\u6b63\u786e\u6027\uff0c\u800cRAG\u7b49\u7f13\u89e3\u6280\u672f\u53ef\u80fd\u5f15\u5165\u65b0\u7684\u4e0d\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u5e7b\u89c9\u8bc4\u4f30\u4ec5\u5173\u6ce8\u6b63\u786e\u6027\uff0c\u5ffd\u89c6\u4e86\u5bf9\u4e8e\u533a\u5206\u548c\u89e3\u51b3\u4fe1\u4efb\u4fb5\u8680\u3001\u9519\u8bef\u4fe1\u606f\u7b49\u5371\u5bb3\u81f3\u5173\u91cd\u8981\u7684\u6a21\u578b\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa\"\u63d0\u793a\u591a\u91cd\u6027\"\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u4e2d\u7684\u4e00\u81f4\u6027\u6c34\u5e73\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u663e\u8457\u7684\u591a\u91cd\u6027\u73b0\u8c61\uff08\u5982Med-HALT\u57fa\u51c6\u4e2d\u8d85\u8fc750%\u7684\u4e0d\u4e00\u81f4\u6027\uff09\uff0c\u8868\u660e\u5e7b\u89c9\u76f8\u5173\u5371\u5bb3\u88ab\u4e25\u91cd\u8bef\u89e3\uff1b\u68c0\u6d4b\u65b9\u6cd5\u5b9e\u9645\u68c0\u6d4b\u7684\u662f\u4e00\u81f4\u6027\u800c\u975e\u6b63\u786e\u6027\uff1bRAG\u7b49\u7f13\u89e3\u6280\u672f\u867d\u6709\u76ca\u4f46\u53ef\u5f15\u5165\u989d\u5916\u4e0d\u4e00\u81f4\u6027\u3002", "conclusion": "\u901a\u8fc7\u5c06\u63d0\u793a\u591a\u91cd\u6027\u6574\u5408\u5230\u5e7b\u89c9\u8bc4\u4f30\u4e2d\uff0c\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u5b8c\u5584\u7684\u6f5c\u5728\u5371\u5bb3\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u63ed\u793a\u4e86\u5f53\u524d\u68c0\u6d4b\u4e0e\u7f13\u89e3\u7b56\u7565\u7684\u5173\u952e\u5c40\u9650\u6027\u3002"}}
{"id": "2602.02039", "categories": ["cs.AI", "cs.CL", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02039", "abs": "https://arxiv.org/abs/2602.02039", "authors": ["Wei Liu", "Peijie Yu", "Michele Orini", "Yali Du", "Yulan He"], "title": "Hunt Instead of Wait: Evaluating Deep Data Research on Large Language Models", "comment": "14 pages, 7 tables, 8 figures", "summary": "The agency expected of Agentic Large Language Models goes beyond answering correctly, requiring autonomy to set goals and decide what to explore. We term this investigatory intelligence, distinguishing it from executional intelligence, which merely completes assigned tasks. Data Science provides a natural testbed, as real-world analysis starts from raw data rather than explicit queries, yet few benchmarks focus on it. To address this, we introduce Deep Data Research (DDR), an open-ended task where LLMs autonomously extract key insights from databases, and DDR-Bench, a large-scale, checklist-based benchmark that enables verifiable evaluation. Results show that while frontier models display emerging agency, long-horizon exploration remains challenging. Our analysis highlights that effective investigatory intelligence depends not only on agent scaffolding or merely scaling, but also on intrinsic strategies of agentic models.", "AI": {"tldr": "This paper introduces Deep Data Research (DDR) and DDR-Bench to evaluate LLMs' investigatory intelligence in autonomous data analysis, finding that while frontier models show emerging agency, long-horizon exploration remains challenging and requires more than just better scaffolding or scaling.", "motivation": "The paper motivates the need for benchmarks that test LLMs' ability to autonomously explore and discover insights from raw data (investigatory intelligence), rather than just executing predefined tasks (executional intelligence), as this capability is crucial for true agency in real-world data science scenarios.", "method": "The authors created Deep Data Research (DDR), an open-ended task where LLMs autonomously extract insights from databases, and DDR-Bench, a large-scale checklist-based benchmark that enables verifiable evaluation of this investigatory intelligence.", "result": "Results show that frontier models demonstrate emerging agency in data exploration, but struggle with long-horizon planning and exploration. The analysis reveals that effective investigatory intelligence depends not only on agent scaffolding or model scaling, but also on intrinsic agentic strategies.", "conclusion": "The paper concludes that developing true investigatory intelligence in LLMs requires moving beyond just architectural improvements or scaling, and focusing on intrinsic strategies that enable autonomous goal-setting and exploration, which is critical for advancing agentic AI systems."}}
{"id": "2602.00753", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00753", "abs": "https://arxiv.org/abs/2602.00753", "authors": ["Zeljko Bolevic", "Milos Brajovic", "Isidora Stankovic", "Ljubisa Stankovic"], "title": "GraphNNK -- Graph Classification and Interpretability", "comment": "4 pages, 3 figures, IEEE conference paper", "summary": "Graph Neural Networks (GNNs) have become a standard approach for learning from graph-structured data. However, their reliance on parametric classifiers (most often linear softmax layers) limits interpretability and sometimes hinders generalization. Recent work on interpolation-based methods, particularly Non-Negative Kernel regression (NNK), has demonstrated that predictions can be expressed as convex combinations of similar training examples in the embedding space, yielding both theoretical results and interpretable explanations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u7528\u975e\u8d1f\u6838\uff08NNK\uff09\u56de\u5f52\u66ff\u4ee3GNN\u4e2d\u53c2\u6570\u5316\u5206\u7c7b\u5668\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u9884\u6d4b\u8868\u793a\u4e3a\u8bad\u7ec3\u6837\u672c\u7684\u51f8\u7ec4\u5408\u6765\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edfGNN\u4f9d\u8d56\u53c2\u6570\u5316\u5206\u7c7b\u5668\uff08\u901a\u5e38\u662f\u7ebf\u6027softmax\u5c42\uff09\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u5e76\u53ef\u80fd\u963b\u788d\u6cdb\u5316\u6027\u80fd\u3002", "method": "\u7814\u7a76\u57fa\u4e8e\u63d2\u503c\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u975e\u8d1f\u6838\uff08NNK\uff09\u56de\u5f52\uff0c\u4f5c\u4e3aGNN\u4e2d\u53c2\u6570\u5316\u5206\u7c7b\u5668\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "result": "NNK\u56de\u5f52\u4f7f\u9884\u6d4b\u80fd\u591f\u8868\u793a\u4e3a\u5d4c\u5165\u7a7a\u95f4\u4e2d\u76f8\u4f3c\u8bad\u7ec3\u6837\u672c\u7684\u51f8\u7ec4\u5408\uff0c\u4ece\u800c\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u548c\u53ef\u89e3\u91ca\u7684\u89e3\u91ca\u3002", "conclusion": "NNK\u56de\u5f52\u901a\u8fc7\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u514b\u670d\u6807\u51c6GNN\u5206\u7c7b\u5668\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\u3002"}}
{"id": "2602.02051", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02051", "abs": "https://arxiv.org/abs/2602.02051", "authors": ["Shivank Garg", "Ayush Singh", "Gaurav Kumar Nayak"], "title": "SIDiffAgent: Self-Improving Diffusion Agent", "comment": null, "summary": "Text-to-image diffusion models have revolutionized generative AI, enabling high-quality and photorealistic image synthesis. However, their practical deployment remains hindered by several limitations: sensitivity to prompt phrasing, ambiguity in semantic interpretation (e.g., ``mouse\" as animal vs. a computer peripheral), artifacts such as distorted anatomy, and the need for carefully engineered input prompts. Existing methods often require additional training and offer limited controllability, restricting their adaptability in real-world applications. We introduce Self-Improving Diffusion Agent (SIDiffAgent), a training-free agentic framework that leverages the Qwen family of models (Qwen-VL, Qwen-Image, Qwen-Edit, Qwen-Embedding) to address these challenges. SIDiffAgent autonomously manages prompt engineering, detects and corrects poor generations, and performs fine-grained artifact removal, yielding more reliable and consistent outputs. It further incorporates iterative self-improvement by storing a memory of previous experiences in a database. This database of past experiences is then used to inject prompt-based guidance at each stage of the agentic pipeline. \\modelour achieved an average VQA score of 0.884 on GenAIBench, significantly outperforming open-source, proprietary models and agentic methods. We will publicly release our code upon acceptance.", "AI": {"tldr": "A training-free agentic framework called SIDiffAgent that uses Qwen models to autonomously improve text-to-image generation through prompt engineering, error correction, artifact removal, and iterative learning from past experiences, achieving state-of-the-art performance.", "motivation": "Text-to-image diffusion models suffer from prompt sensitivity, semantic ambiguity, anatomical artifacts, and require careful prompt engineering. Existing methods need additional training and lack controllability, limiting real-world deployment.", "method": "Proposes SIDiffAgent, a training-free framework that leverages Qwen-VL, Qwen-Image, Qwen-Edit, and Qwen-Embedding models to autonomously handle prompt engineering, detect/correct poor generations, remove artifacts, and use a memory database of past experiences for iterative self-improvement.", "result": "Achieved an average VQA score of 0.884 on GenAIBench, significantly outperforming open-source, proprietary models, and other agentic methods.", "conclusion": "The framework provides more reliable and consistent text-to-image generation. The authors plan to release the code publicly upon paper acceptance."}}
{"id": "2602.00767", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00767", "abs": "https://arxiv.org/abs/2602.00767", "authors": ["Muhammed Ustaomeroglu", "Guannan Qu"], "title": "BLOCK-EM: Preventing Emergent Misalignment by Blocking Causal Features", "comment": "41 pages, 32 figures. Code available", "summary": "Emergent misalignment can arise when a language model is fine-tuned on a narrowly scoped supervised objective: the model learns the target behavior, yet also develops undesirable out-of-domain behaviors. We investigate a mechanistic approach to preventing emergent misalignment by identifying a small set of internal features that reliably control the misaligned behavior and then discouraging the model from strengthening these features during fine-tuning. Across six fine-tuning domains, blocking (i.e., constraining) a fixed set of features achieves up to 95\\% relative reduction in emergent misalignment with no degradation in model quality or target-task performance. We strengthen validity with disjoint selection/evaluation splits, multiple independent judges, multiple random seeds for key settings, quality metrics, and extensive ablations demonstrating that the reduction in misalignment is specific to the identified mechanism. We also characterize a limiting regime in which misalignment re-emerges under prolonged fine-tuning, present evidence consistent with rerouting through alternative features or layers, and evaluate modifications that partially restore the misalignment-blocking effect. Overall, our results show that targeted training-time constraints on internal mechanisms can mitigate emergent misalignment without degrading target-task performance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u673a\u5236\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u5e76\u7ea6\u675f\u6a21\u578b\u5185\u90e8\u5c11\u91cf\u5173\u952e\u7279\u5f81\u6765\u9632\u6b62\u8bed\u8a00\u6a21\u578b\u5728\u5fae\u8c03\u65f6\u4ea7\u751f\u7d27\u6025\u9519\u4f4d\uff0c\u5728\u516d\u4e2a\u9886\u57df\u5b9e\u73b0\u9ad8\u8fbe95%\u7684\u9519\u4f4d\u51cf\u5c11\u4e14\u4e0d\u5f71\u54cd\u76ee\u6807\u4efb\u52a1\u6027\u80fd", "motivation": "\u8bed\u8a00\u6a21\u578b\u5728\u72ed\u7a84\u76d1\u7763\u76ee\u6807\u4e0b\u5fae\u8c03\u65f6\u4f1a\u51fa\u73b0\u7d27\u6025\u9519\u4f4d\uff1a\u5b66\u4f1a\u76ee\u6807\u884c\u4e3a\u7684\u540c\u65f6\u53d1\u5c55\u51fa\u4e0d\u5e0c\u671b\u51fa\u73b0\u7684\u57df\u5916\u884c\u4e3a", "method": "\u673a\u5236\u6027\u65b9\u6cd5\uff1a\u8bc6\u522b\u80fd\u53ef\u9760\u63a7\u5236\u9519\u4f4d\u884c\u4e3a\u7684\u5c11\u91cf\u5185\u90e8\u7279\u5f81\uff0c\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u963b\u6b62\u6a21\u578b\u5f3a\u5316\u8fd9\u4e9b\u7279\u5f81\uff08\u5373\u7279\u5f81\u7ea6\u675f/\u963b\u65ad\uff09", "result": "\u5728\u516d\u4e2a\u5fae\u8c03\u9886\u57df\uff0c\u56fa\u5b9a\u7279\u5f81\u96c6\u963b\u65ad\u5b9e\u73b0\u9ad8\u8fbe95%\u7684\u7d27\u6025\u9519\u4f4d\u76f8\u5bf9\u51cf\u5c11\uff0c\u4e14\u65e0\u6a21\u578b\u8d28\u91cf\u6216\u76ee\u6807\u4efb\u52a1\u6027\u80fd\u4e0b\u964d\uff1b\u901a\u8fc7\u591a\u91cd\u9a8c\u8bc1\uff08\u72ec\u7acb\u8bc4\u4f30\u96c6\u3001\u72ec\u7acb\u8bc4\u5224\u5458\u3001\u968f\u673a\u79cd\u5b50\u3001\u8d28\u91cf\u6307\u6807\u548c\u6d88\u878d\u5b9e\u9a8c\uff09\u786e\u8ba4\u6548\u679c\uff1b\u53d1\u73b0\u957f\u671f\u5fae\u8c03\u4e0b\u9519\u4f4d\u4f1a\u91cd\u65b0\u51fa\u73b0\u7684\u6781\u9650\u60c5\u51b5\uff0c\u8bc1\u636e\u663e\u793a\u6a21\u578b\u4f1a\u901a\u8fc7\u66ff\u4ee3\u7279\u5f81\u6216\u5c42\u91cd\u65b0\u8def\u7531", "conclusion": "\u5bf9\u5185\u90e8\u673a\u5236\u8fdb\u884c\u6709\u9488\u5bf9\u6027\u7684\u8bad\u7ec3\u65f6\u7ea6\u675f\uff0c\u53ef\u5728\u4e0d\u964d\u4f4e\u76ee\u6807\u4efb\u52a1\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u7f13\u89e3\u7d27\u6025\u9519\u4f4d"}}
{"id": "2602.02133", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02133", "abs": "https://arxiv.org/abs/2602.02133", "authors": ["Sangwoo Shin", "BumJun Kim", "Kyelim Lee", "Moongyu Jeon", "Albert No"], "title": "Understanding the Reversal Curse Mitigation in Masked Diffusion Models through Attention and Training Dynamics", "comment": null, "summary": "Autoregressive language models (ARMs) suffer from the reversal curse: after learning that \"$A$ is $B$\", they often fail on the reverse query \"$B$ is $A$\". Masked diffusion-based language models (MDMs) exhibit this failure in a much weaker form, but the underlying reason has remained unclear. A common explanation attributes this mitigation to the any-order training objective. However, observing \"[MASK] is $B$\" during training does not necessarily teach the model to handle the reverse prompt \"$B$ is [MASK]\". We show that the mitigation arises from architectural structure and its interaction with training. In a one-layer Transformer encoder, weight sharing couples the two directions by making forward and reverse attention scores positively correlated. In the same setting, we further show that the corresponding gradients are aligned, so minimizing the forward loss also reduces the reverse loss. Experiments on both controlled toy tasks and large-scale diffusion language models support these mechanisms, explaining why MDMs partially overcome a failure mode that persists in strong ARMs.", "AI": {"tldr": "MDMs partially avoid the reversal curse due to architectural weight sharing and aligned gradients, not just training objective.", "motivation": "To understand why masked diffusion language models (MDMs) exhibit a weaker reversal curse than autoregressive models (ARMs), challenging the common explanation that attributes this to the any-order training objective alone.", "method": "Theoretical analysis of one-layer Transformer encoders showing weight sharing couples forward/reverse directions and aligns gradients; experiments on controlled toy tasks and large-scale diffusion language models.", "result": "Weight sharing in MDM architecture creates positively correlated attention scores between forward and reverse directions, and aligned gradients that reduce both losses simultaneously.", "conclusion": "Architectural structure (weight sharing) and its interaction with training, not just the any-order objective, explains why MDMs partially overcome the reversal curse that persists in strong ARMs."}}
{"id": "2602.00772", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00772", "abs": "https://arxiv.org/abs/2602.00772", "authors": ["Xiaoqi Qiu", "Hao Zeng", "Zhiyu Hou", "Hongxin Wei"], "title": "Provable Model Provenance Set for Large Language Models", "comment": null, "summary": "The growing prevalence of unauthorized model usage and misattribution has increased the need for reliable model provenance analysis. However, existing methods largely rely on heuristic fingerprint-matching rules that lack provable error control and often overlook the existence of multiple sources, leaving the reliability of their provenance claims unverified. In this work, we first formalize the model provenance problem with provable guarantees, requiring rigorous coverage of all true provenances at a prescribed confidence level. Then, we propose the Model Provenance Set (MPS), which employs a sequential test-and-exclusion procedure to adaptively construct a small set satisfying the guarantee. The key idea of MPS is to test the significance of provenance existence within a candidate pool, thereby establishing a provable asymptotic guarantee at a user-specific confidence level. Extensive experiments demonstrate that MPS effectively achieves target provenance coverage while strictly limiting the inclusion of unrelated models, and further reveal its potential for practical provenance analysis in attribution and auditing tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u6a21\u578b\u76d7\u7528\u548c\u9519\u8bef\u5f52\u5c5e\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u53ef\u8bc1\u660e\u4fdd\u8bc1\u7684\u6a21\u578b\u6765\u6e90\u5206\u6790\u65b9\u6cd5\uff08MPS\uff09\uff0c\u901a\u8fc7\u5e8f\u5217\u68c0\u9a8c-\u6392\u9664\u8fc7\u7a0b\uff0c\u5728\u7528\u6237\u6307\u5b9a\u7f6e\u4fe1\u6c34\u5e73\u4e0b\u6784\u5efa\u5305\u542b\u6240\u6709\u771f\u5b9e\u6765\u6e90\u7684\u5c0f\u89c4\u6a21\u96c6\u5408\uff0c\u76f8\u6bd4\u542f\u53d1\u5f0f\u65b9\u6cd5\u5177\u6709\u4e25\u683c\u7684\u53ef\u8bc1\u660e\u8986\u76d6\u4fdd\u8bc1\u3002", "motivation": "\u968f\u7740\u672a\u6388\u6743\u6a21\u578b\u4f7f\u7528\u548c\u9519\u8bef\u5f52\u5c5e\u7684\u65e5\u76ca\u666e\u904d\uff0c\u53ef\u9760\u7684\u6a21\u578b\u6765\u6e90\u5206\u6790\u9700\u6c42\u8feb\u5207\u3002\u7136\u800c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u542f\u53d1\u5f0f\u6307\u7eb9\u5339\u914d\u89c4\u5219\uff0c\u7f3a\u4e4f\u53ef\u8bc1\u660e\u7684\u9519\u8bef\u63a7\u5236\uff0c\u4e14\u5e38\u5ffd\u7565\u591a\u6e90\u60c5\u51b5\uff0c\u5bfc\u81f4\u5176\u6765\u6e90\u58f0\u660e\u7684\u53ef\u9760\u6027\u65e0\u6cd5\u9a8c\u8bc1\u3002", "method": "\u9996\u5148\u5f62\u5f0f\u5316\u5177\u6709\u53ef\u8bc1\u660e\u4fdd\u8bc1\u7684\u6a21\u578b\u6765\u6e90\u95ee\u9898\uff0c\u8981\u6c42\u4ee5\u9884\u5b9a\u7f6e\u4fe1\u6c34\u5e73\u8986\u76d6\u6240\u6709\u771f\u5b9e\u6765\u6e90\uff1b\u7136\u540e\u63d0\u51fa\u6a21\u578b\u6765\u6e90\u96c6\u5408\uff08MPS\uff09\uff0c\u91c7\u7528\u5e8f\u5217\u68c0\u9a8c-\u6392\u9664\u8fc7\u7a0b\uff0c\u901a\u8fc7\u6d4b\u8bd5\u5019\u9009\u6c60\u4e2d\u6765\u6e90\u5b58\u5728\u7684\u663e\u8457\u6027\uff0c\u81ea\u9002\u5e94\u6784\u5efa\u6ee1\u8db3\u4fdd\u8bc1\u7684\u5c0f\u89c4\u6a21\u96c6\u5408\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMPS\u80fd\u6709\u6548\u5b9e\u73b0\u76ee\u6807\u6765\u6e90\u8986\u76d6\uff0c\u540c\u65f6\u4e25\u683c\u9650\u5236\u65e0\u5173\u6a21\u578b\u7684\u7eb3\u5165\uff0c\u5e76\u5728\u5f52\u5c5e\u548c\u5ba1\u8ba1\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u6a21\u578b\u6765\u6e90\u5206\u6790\u63d0\u4f9b\u4e86\u7edf\u8ba1\u4e25\u8c28\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u8bc1\u660e\u7684\u6e10\u8fd1\u4fdd\u8bc1\u89e3\u51b3\u4e86\u73b0\u6709\u542f\u53d1\u5f0f\u65b9\u6cd5\u7684\u5173\u952e\u5c40\u9650\uff0c\u4e3a\u6a21\u578b\u5f52\u5c5e\u548c\u5ba1\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2602.02136", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02136", "abs": "https://arxiv.org/abs/2602.02136", "authors": ["Yingsha Xie", "Tiansheng Huang", "Enneng Yang", "Rui Min", "Wenjie Lu", "Xiaochun Cao", "Naiqiang Tan", "Li Shen"], "title": "Mitigating Safety Tax via Distribution-Grounded Refinement in Large Reasoning Models", "comment": "Code will be released soon", "summary": "Safety alignment incurs safety tax that perturbs a large reasoning model's (LRM) general reasoning ability. Existing datasets used for safety alignment for an LRM are usually constructed by distilling safety reasoning traces and answers from an external LRM or human labeler. However, such reasoning traces and answers exhibit a distributional gap with the target LRM that needs alignment, and we conjecture such distributional gap is the culprit leading to significant degradation of reasoning ability of the target LRM. Driven by this hypothesis, we propose a safety alignment dataset construction method, dubbed DGR. DGR transforms and refines an existing out-of-distributional safety reasoning dataset to be aligned with the target's LLM inner distribution. Experimental results demonstrate that i) DGR effectively mitigates the safety tax while maintaining safety performance across all baselines, i.e., achieving \\textbf{+30.2\\%} on DirectRefusal and \\textbf{+21.2\\%} on R1-ACT improvement in average reasoning accuracy compared to Vanilla SFT; ii) the degree of reasoning degradation correlates with the extent of distribution shift, suggesting that bridging this gap is central to preserving capabilities. Furthermore, we find that safety alignment in LRMs may primarily function as a mechanism to activate latent knowledge, as a mere \\textbf{10} samples are sufficient for activating effective refusal behaviors. These findings not only emphasize the importance of distributional consistency but also provide insights into the activation mechanism of safety in reasoning models.", "AI": {"tldr": "\u5b89\u5168\u5bf9\u9f50\u4f1a\u964d\u4f4e\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u539f\u56e0\u662f\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u5dee\u5f02\uff1b\u63d0\u51fa\u7684DGR\u65b9\u6cd5\u901a\u8fc7\u5f25\u5408\u5206\u5e03\u5dee\u8ddd\uff0c\u5728\u4fdd\u6301\u5b89\u5168\u6027\u7684\u540c\u65f6\u63d0\u5347\u63a8\u7406\u51c6\u786e\u7387\u7ea630%\uff0c\u8868\u660e\u5b89\u5168\u5bf9\u9f50\u53ef\u80fd\u53ea\u662f\u6fc0\u6d3b\u4e86\u6f5c\u5728\u77e5\u8bc6\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u5bf9\u9f50\u6570\u636e\u96c6\u901a\u8fc7\u5916\u90e8\u6a21\u578b\u6216\u4eba\u5de5\u6807\u6ce8\u6784\u5efa\uff0c\u4e0e\u76ee\u6807\u6a21\u578b\u5b58\u5728\u5206\u5e03\u5dee\u8ddd\uff0c\u5bfc\u81f4\u5b89\u5168\u5bf9\u9f50\u4ea7\u751f\"\u5b89\u5168\u7a0e\"\uff0c\u663e\u8457\u635f\u5bb3\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faDGR\uff08\u5206\u5e03\u5dee\u8ddd\u7cbe\u70bc\uff09\u65b9\u6cd5\uff0c\u5c06\u73b0\u6709\u5206\u5e03\u5916\u5b89\u5168\u63a8\u7406\u6570\u636e\u96c6\u8f6c\u6362\u5e76\u7cbe\u70bc\u81f3\u4e0e\u76ee\u6807\u6a21\u578b\u5185\u90e8\u5206\u5e03\u5bf9\u9f50\u3002", "result": "DGR\u6709\u6548\u964d\u4f4e\u5b89\u5168\u7a0e\u540c\u65f6\u4fdd\u6301\u5b89\u5168\u6027\u80fd\uff1a\u76f8\u6bd4Vanilla SFT\uff0cDirectRefusal\u63d0\u534730.2%\uff0cR1-ACT\u63d0\u534721.2%\uff1b\u63a8\u7406\u9000\u5316\u7a0b\u5ea6\u4e0e\u5206\u5e03\u504f\u79fb\u7a0b\u5ea6\u76f8\u5173\uff1b\u4ec5\u970010\u4e2a\u6837\u672c\u5373\u53ef\u6fc0\u6d3b\u6709\u6548\u7684\u62d2\u7edd\u884c\u4e3a\u3002", "conclusion": "\u5206\u5e03\u4e00\u81f4\u6027\u5bf9\u4fdd\u6301\u6a21\u578b\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff1b\u5b89\u5168\u5bf9\u9f50\u4e3b\u8981\u901a\u8fc7\u6fc0\u6d3b\u6f5c\u5728\u77e5\u8bc6\u800c\u975e\u6559\u6388\u65b0\u884c\u4e3a\u6765\u53d1\u6325\u4f5c\u7528\u3002"}}
{"id": "2602.00774", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00774", "abs": "https://arxiv.org/abs/2602.00774", "authors": ["Yuxin Lu", "Zhen Peng", "Xiqiang Xia", "Jie Wang"], "title": "A novel VAE-DML fusion framework for casual analysis of greenwashing in the mining industry", "comment": null, "summary": "Against the backdrop of the global green transition and \"dual carbon\" goals, mining industry chain enterprises are pivotal entities in terms of resource consumption and environmental impact. Their environmental performance directly affects regional ecological security and is closely tied to national resource strategies and green transformation outcomes. Ensuring the authenticity and reliability of their environmental disclosure is thus a core and urgent issue for sustainable development and national strategic objectives.From a corporate governance perspective, this study examines equity balance as a fundamental governance mechanism, investigating its inhibitory effect on greenwashing behavior among these enterprises and the underlying pathways involved. Methodologically, the paper innovatively employs a Variational Autoencoder (VAE) and a Double Machine Learning (DML) model to construct counterfactual scenarios, mitigating endogeneity concerns and precisely identifying the causal relationship between equity balance and greenwashing. The findings indicate, first, a significant negative causal relationship between equity balance and corporate greenwashing, confirming its substantive governance effect. Second, this inhibitory effect exhibits notable heterogeneity, manifesting more strongly in western regions, upstream segments of the industrial chain, and industries with high environmental sensitivity. Third, the governance effect demonstrates clear temporal dynamics, with the strongest impact occurring in the current period, followed by a diminishing yet statistically significant lagged effect, and ultimately a stable long-term cumulative influence. Finally, mechanism analysis reveals that equity balance operates through three distinct channels to curb greenwashing: alleviating management performance pressure, enhancing the stability of the executive team, and intensifying media scrutiny.", "AI": {"tldr": "\u672c\u7814\u7a76\u8fd0\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668(VAE)\u548c\u53cc\u91cd\u673a\u5668\u5b66\u4e60(DML)\u6a21\u578b\uff0c\u8bc1\u5b9e\u80a1\u6743\u5236\u8861\u80fd\u663e\u8457\u6291\u5236\u91c7\u77ff\u4e1a\u4f01\u4e1a\u6f02\u7eff\u884c\u4e3a\uff0c\u4e14\u8be5\u6cbb\u7406\u6548\u5e94\u5b58\u5728\u533a\u57df\u3001\u4ea7\u4e1a\u94fe\u4f4d\u7f6e\u548c\u884c\u4e1a\u73af\u5883\u654f\u611f\u5ea6\u7684\u5f02\u8d28\u6027\uff0c\u5e76\u901a\u8fc7\u7f13\u89e3\u7ba1\u7406\u5c42\u4e1a\u7ee9\u538b\u529b\u3001\u589e\u5f3a\u9ad8\u7ba1\u56e2\u961f\u7a33\u5b9a\u6027\u548c\u5f3a\u5316\u5a92\u4f53\u76d1\u7763\u4e09\u6761\u6e20\u9053\u53d1\u6325\u4f5c\u7528\u3002", "motivation": "\u5728\u5168\u7403\u7eff\u8272\u8f6c\u578b\u548c\"\u53cc\u78b3\"\u76ee\u6807\u80cc\u666f\u4e0b\uff0c\u91c7\u77ff\u4e1a\u4ea7\u4e1a\u94fe\u4f01\u4e1a\u662f\u8d44\u6e90\u6d88\u8017\u548c\u73af\u5883\u5f71\u54cd\u7684\u91cd\u70b9\u5355\u4f4d\uff0c\u5176\u73af\u5883\u4fe1\u606f\u62ab\u9732\u7684\u771f\u5b9e\u6027\u5173\u4e4e\u533a\u57df\u751f\u6001\u5b89\u5168\u548c\u56fd\u5bb6\u8d44\u6e90\u6218\u7565\uff0c\u56e0\u6b64\u7814\u7a76\u80a1\u6743\u5236\u8861\u673a\u5236\u5bf9\u6291\u5236\u4f01\u4e1a\u6f02\u7eff\u884c\u4e3a\u5177\u6709\u7d27\u8feb\u7684\u73b0\u5b9e\u610f\u4e49\u3002", "method": "\u521b\u65b0\u6027\u5730\u91c7\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668(VAE)\u6784\u5efa\u53cd\u4e8b\u5b9e\u573a\u666f\uff0c\u7ed3\u5408\u53cc\u91cd\u673a\u5668\u5b66\u4e60(DML)\u6a21\u578b\u7f13\u89e3\u5185\u751f\u6027\u95ee\u9898\uff0c\u7cbe\u786e\u8bc6\u522b\u80a1\u6743\u5236\u8861\u4e0e\u4f01\u4e1a\u6f02\u7eff\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u3002", "result": "1) \u80a1\u6743\u5236\u8861\u4e0e\u6f02\u7eff\u884c\u4e3a\u5b58\u5728\u663e\u8457\u8d1f\u5411\u56e0\u679c\u5173\u7cfb\uff1b2) \u6cbb\u7406\u6548\u5e94\u5177\u6709\u5f02\u8d28\u6027\uff1a\u897f\u90e8\u5730\u533a\u3001\u4ea7\u4e1a\u94fe\u4e0a\u6e38\u3001\u9ad8\u73af\u5883\u654f\u611f\u884c\u4e1a\u66f4\u663e\u8457\uff1b3) \u65f6\u95f4\u52a8\u6001\u6027\uff1a\u5f53\u671f\u6548\u5e94\u6700\u5f3a\uff0c\u6ede\u540e\u6548\u5e94\u9012\u51cf\u4f46\u6301\u7eed\uff0c\u957f\u671f\u5448\u7a33\u5b9a\u7d2f\u79ef\u5f71\u54cd\uff1b4) \u4f5c\u7528\u673a\u5236\u5305\u62ec\u7f13\u89e3\u7ba1\u7406\u5c42\u4e1a\u7ee9\u538b\u529b\u3001\u589e\u5f3a\u9ad8\u7ba1\u56e2\u961f\u7a33\u5b9a\u6027\u3001\u5f3a\u5316\u5a92\u4f53\u76d1\u7763\u4e09\u6761\u6e20\u9053\u3002", "conclusion": "\u80a1\u6743\u5236\u8861\u5bf9\u91c7\u77ff\u4e1a\u4f01\u4e1a\u6f02\u7eff\u884c\u4e3a\u5177\u6709\u5b9e\u8d28\u6027\u6cbb\u7406\u6548\u679c\uff0c\u5176\u4f5c\u7528\u673a\u5236\u591a\u5143\u4e14\u6548\u5e94\u5f02\u8d28\uff0c\u4e3a\u5b8c\u5584\u516c\u53f8\u6cbb\u7406\u3001\u4fc3\u8fdb\u7eff\u8272\u8f6c\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u7ecf\u9a8c\u8bc1\u636e\u548c\u653f\u7b56\u542f\u793a\u3002"}}
{"id": "2602.02158", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02158", "abs": "https://arxiv.org/abs/2602.02158", "authors": ["Sarah Nassar"], "title": "Traffic-Aware Navigation in Road Networks", "comment": null, "summary": "This project compares three graph search approaches for the task of traffic-aware navigation in Kingston's road network. These approaches include a single-run multi-query preprocessing algorithm (Floyd-Warshall-Ingerman), continuous single-query real-time search (Dijkstra's and A*), and an algorithm combining both approaches to balance between their trade-offs by first finding the top K shortest paths then iterating over them in real time (Yen's). Dijkstra's and A* resulted in the most traffic-aware optimal solutions with minimal preprocessing required. Floyd-Warshall-Ingerman was the fastest in real time but provided distance based paths with no traffic awareness. Yen's algorithm required significant preprocessing but balanced between the other two approaches in terms of runtime speed and optimality. Each approach presents advantages and disadvantages that need to be weighed depending on the circumstances of specific deployment contexts to select the best custom solution. *This project was completed as part of ELEC 844 (Search and Planning Algorithms for Robotics) in the Fall 2025 term.", "AI": {"tldr": "\u6bd4\u8f83\u4e09\u79cd\u56fe\u641c\u7d22\u7b97\u6cd5\u5728\u91d1\u65af\u6566\u4ea4\u901a\u611f\u77e5\u5bfc\u822a\u4e2d\u7684\u6027\u80fd\uff0c\u5206\u6790\u5176\u9884\u5904\u7406\u5f00\u9500\u3001\u5b9e\u65f6\u901f\u5ea6\u4e0e\u8def\u5f84\u6700\u4f18\u6027\u7684\u6743\u8861", "motivation": "\u89e3\u51b3\u4ea4\u901a\u5bfc\u822a\u7cfb\u7edf\u4e2d\u9884\u5904\u7406\u6548\u7387\u3001\u5b9e\u65f6\u8ba1\u7b97\u901f\u5ea6\u4e0e\u8def\u5f84\u4ea4\u901a\u611f\u77e5\u80fd\u529b\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u4e0d\u540c\u90e8\u7f72\u573a\u666f\u63d0\u4f9b\u5b9a\u5236\u5316\u7b97\u6cd5\u9009\u62e9\u4f9d\u636e", "method": "\u5bf9\u6bd4\u4e09\u79cd\u65b9\u6cd5\uff1a1) Floyd-Warshall-Ingerman\uff08\u591a\u67e5\u8be2\u9884\u5904\u7406\u7b97\u6cd5\uff09\uff1b2) Dijkstra/A*\uff08\u5b9e\u65f6\u5355\u67e5\u8be2\u641c\u7d22\uff09\uff1b3) Yen's\u7b97\u6cd5\uff08\u5148\u9884\u8ba1\u7b97\u524dK\u77ed\u8def\u5f84\u518d\u5b9e\u65f6\u8fed\u4ee3\uff09", "result": "Dijkstra/A*\u5728\u4ea4\u901a\u611f\u77e5\u6700\u4f18\u6027\u4e0a\u8868\u73b0\u6700\u4f73\u4e14\u9884\u5904\u7406\u5c11\uff1bFloyd-Warshall\u5b9e\u65f6\u6700\u5feb\u4f46\u65e0\u4ea4\u901a\u611f\u77e5\uff1bYen's\u7b97\u6cd5\u5728\u5b9e\u65f6\u6027\u4e0e\u6700\u4f18\u6027\u95f4\u53d6\u5f97\u5e73\u8861\u4f46\u9884\u5904\u7406\u5f00\u9500\u5927", "conclusion": "\u4e0d\u540c\u7b97\u6cd5\u5728\u9884\u5904\u7406\u6210\u672c\u3001\u5b9e\u65f6\u901f\u5ea6\u4e0e\u8def\u5f84\u8d28\u91cf\u4e0a\u5b58\u5728\u663e\u8457\u6743\u8861\uff0c\u9700\u6839\u636e\u5177\u4f53\u90e8\u7f72\u573a\u666f\uff08\u5982\u786c\u4ef6\u8d44\u6e90\u3001\u54cd\u5e94\u8981\u6c42\uff09\u9009\u62e9\u6700\u4f18\u65b9\u6848"}}
{"id": "2602.00775", "categories": ["cs.LG", "econ.EM"], "pdf": "https://arxiv.org/pdf/2602.00775", "abs": "https://arxiv.org/abs/2602.00775", "authors": ["Zitao Hong", "Zhen Peng", "Xueping Liu"], "title": "Stable Time Series Prediction of Enterprise Carbon Emissions Based on Causal Inference", "comment": null, "summary": "Against the backdrop of ongoing carbon peaking and carbon neutrality goals, accurate prediction of enterprise carbon emission trends constitutes an essential foundation for energy structure optimization and low-carbon transformation decision-making. Nevertheless, significant heterogeneity persists across regions, industries and individual enterprises regarding energy structure, production scale, policy intensity and governance efficacy, resulting in pronounced distribution shifts and non-stationarity in carbon emission data across both temporal and spatial dimensions. Such cross-regional and cross-enterprise data drift not only compromises the accuracy of carbon emission reporting but substantially undermines the guidance value of predictive models for production planning and carbon quota trading decisions. To address this critical challenge, we integrate causal inference perspectives with stable learning methodologies and time-series modelling, proposing a stable temporal prediction mechanism tailored to distribution shift environments. This mechanism incorporates enterprise-level energy inputs, capital investment, labour deployment, carbon pricing, governmental interventions and policy implementation intensity, constructing a risk consistency-constrained stable learning framework that extracts causal stable features (robust against external perturbations yet demonstrating long-term stable effects on carbon dioxide emissions) from multi-environment samples across diverse policies, regions and industrial sectors. Furthermore, through adaptive normalization and sample reweighting strategies, the approach dynamically rectifies temporal non-stationarity induced by economic fluctuations and policy transitions, ultimately enhancing model generalization capability and explainability in complex environments.", "AI": {"tldr": "\u9488\u5bf9\u78b3\u8fbe\u5cf0\u78b3\u4e2d\u548c\u76ee\u6807\u4e0b\u4f01\u4e1a\u78b3\u6392\u653e\u9884\u6d4b\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e00\u79cd\u878d\u5408\u56e0\u679c\u63a8\u65ad\u4e0e\u7a33\u5b9a\u5b66\u4e60\u7684\u7a33\u5b9a\u65f6\u5e8f\u9884\u6d4b\u673a\u5236\uff0c\u901a\u8fc7\u98ce\u9669\u4e00\u81f4\u6027\u7ea6\u675f\u6846\u67b6\u63d0\u53d6\u56e0\u679c\u7a33\u5b9a\u7279\u5f81\uff0c\u5e76\u5229\u7528\u81ea\u9002\u5e94\u5f52\u4e00\u5316\u548c\u6837\u672c\u91cd\u52a0\u6743\u7b56\u7565\u52a8\u6001\u4fee\u6b63\u5206\u5e03\u504f\u79fb\u548c\u975e\u5e73\u7a33\u6027\uff0c\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5728\u78b3\u8fbe\u5cf0\u78b3\u4e2d\u548c\u76ee\u6807\u80cc\u666f\u4e0b\uff0c\u4f01\u4e1a\u78b3\u6392\u653e\u9884\u6d4b\u5bf9\u80fd\u6e90\u7ed3\u6784\u4f18\u5316\u548c\u4f4e\u78b3\u8f6c\u578b\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002\u4f46\u5730\u533a\u3001\u884c\u4e1a\u548c\u4f01\u4e1a\u7684\u5f02\u8d28\u6027\u5bfc\u81f4\u78b3\u6392\u653e\u6570\u636e\u5b58\u5728\u663e\u8457\u7684\u65f6\u7a7a\u5206\u5e03\u504f\u79fb\u548c\u975e\u5e73\u7a33\u6027\uff0c\u4e25\u91cd\u5f71\u54cd\u4e86\u9884\u6d4b\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u51b3\u7b56\u6307\u5bfc\u4ef7\u503c\u3002", "method": "\u5c06\u56e0\u679c\u63a8\u65ad\u4e0e\u7a33\u5b9a\u5b66\u4e60\u65b9\u6cd5\u53ca\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u76f8\u7ed3\u5408\uff0c\u6784\u5efa\u98ce\u9669\u4e00\u81f4\u6027\u7ea6\u675f\u7684\u7a33\u5b9a\u5b66\u4e60\u6846\u67b6\uff0c\u4ece\u591a\u653f\u7b56\u3001\u591a\u533a\u57df\u3001\u591a\u884c\u4e1a\u7684\u6837\u672c\u4e2d\u63d0\u53d6\u5bf9\u5916\u90e8\u6270\u52a8\u7a33\u5065\u4e14\u5bf9\u78b3\u6392\u653e\u5177\u6709\u957f\u671f\u7a33\u5b9a\u5f71\u54cd\u7684\u56e0\u679c\u7279\u5f81\uff1b\u91c7\u7528\u81ea\u9002\u5e94\u5f52\u4e00\u5316\u548c\u6837\u672c\u91cd\u52a0\u6743\u7b56\u7565\u52a8\u6001\u4fee\u6b63\u7531\u7ecf\u6d4e\u6ce2\u52a8\u548c\u653f\u7b56\u53d8\u5316\u5f15\u8d77\u7684\u65f6\u5e8f\u975e\u5e73\u7a33\u6027\u3002", "result": "\u6240\u63d0\u673a\u5236\u80fd\u591f\u52a8\u6001\u4fee\u6b63\u65f6\u5e8f\u975e\u5e73\u7a33\u6027\uff0c\u6709\u6548\u589e\u5f3a\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u751f\u4ea7\u8ba1\u5212\u5236\u5b9a\u548c\u78b3\u914d\u989d\u4ea4\u6613\u51b3\u7b56\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u9884\u6d4b\u652f\u6301\u3002", "conclusion": "\u8be5\u7a33\u5b9a\u9884\u6d4b\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u5730\u533a\u8de8\u4f01\u4e1a\u7684\u6570\u636e\u6f02\u79fb\u95ee\u9898\uff0c\u4e3a\u4f01\u4e1a\u78b3\u6392\u653e\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u8bba\u6846\u67b6\uff0c\u5bf9\u63a8\u52a8\u78b3\u8fbe\u5cf0\u78b3\u4e2d\u548c\u76ee\u6807\u4e0b\u7684\u79d1\u5b66\u51b3\u7b56\u548c\u7cbe\u51c6\u65bd\u7b56\u5177\u6709\u91cd\u8981\u5b9e\u8df5\u610f\u4e49\u3002"}}
{"id": "2602.02188", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02188", "abs": "https://arxiv.org/abs/2602.02188", "authors": ["Xia Jiang", "Jing Chen", "Cong Zhang", "Jie Gao", "Chengpeng Hu", "Chenhao Zhang", "Yaoxin Wu", "Yingqian Zhang"], "title": "Reasoning in a Combinatorial and Constrained World: Benchmarking LLMs on Natural-Language Combinatorial Optimization", "comment": null, "summary": "While large language models (LLMs) have shown strong performance in math and logic reasoning, their ability to handle combinatorial optimization (CO) -- searching high-dimensional solution spaces under hard constraints -- remains underexplored. To bridge the gap, we introduce NLCO, a \\textbf{N}atural \\textbf{L}anguage \\textbf{C}ombinatorial \\textbf{O}ptimization benchmark that evaluates LLMs on end-to-end CO reasoning: given a language-described decision-making scenario, the model must output a discrete solution without writing code or calling external solvers. NLCO covers 43 CO problems and is organized using a four-layer taxonomy of variable types, constraint families, global patterns, and objective classes, enabling fine-grained evaluation. We provide solver-annotated solutions and comprehensively evaluate LLMs by feasibility, solution optimality, and reasoning efficiency. Experiments across a wide range of modern LLMs show that high-performing models achieve strong feasibility and solution quality on small instances, but both degrade as instance size grows, even if more tokens are used for reasoning. We also observe systematic effects across the taxonomy: set-based tasks are relatively easy, whereas graph-structured problems and bottleneck objectives lead to more frequent failures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faNLCO\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u5c0f\u89c4\u6a21\u5b9e\u4f8b\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u968f\u7740\u95ee\u9898\u89c4\u6a21\u589e\u5927\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u5c24\u5176\u5728\u56fe\u7ed3\u6784\u95ee\u9898\u548c\u74f6\u9888\u76ee\u6807\u4e0a\u8868\u73b0\u66f4\u5dee\u3002", "motivation": "\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u548c\u903b\u8f91\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u5904\u7406\u7ec4\u5408\u4f18\u5316\uff08\u5728\u9ad8\u7ef4\u89e3\u7a7a\u95f4\u4e2d\u641c\u7d22\u6ee1\u8db3\u786c\u7ea6\u675f\u7684\u89e3\uff09\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u5b58\u5728\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u4f5c\u8005\u63d0\u51faNLCO\u57fa\u51c6\uff0c\u5305\u542b43\u4e2a\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff0c\u91c7\u7528\u56db\u5c42\u5206\u7c7b\u4f53\u7cfb\uff08\u53d8\u91cf\u7c7b\u578b\u3001\u7ea6\u675f\u65cf\u3001\u5168\u5c40\u6a21\u5f0f\u3001\u76ee\u6807\u7c7b\uff09\u8fdb\u884c\u8bc4\u4f30\u3002\u8981\u6c42\u6a21\u578b\u76f4\u63a5\u8f93\u51fa\u79bb\u6563\u89e3\uff0c\u4e0d\u4f9d\u8d56\u4ee3\u7801\u6216\u5916\u90e8\u6c42\u89e3\u5668\uff0c\u5e76\u4ece\u53ef\u884c\u6027\u3001\u89e3\u7684\u6700\u4f18\u6027\u548c\u63a8\u7406\u6548\u7387\u4e09\u4e2a\u7ef4\u5ea6\u5168\u9762\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u9ad8\u6027\u80fd\u6a21\u578b\u5728\u5c0f\u578b\u5b9e\u4f8b\u4e0a\u5177\u6709\u5f3a\u5927\u7684\u53ef\u884c\u6027\u548c\u89e3\u8d28\u91cf\uff0c\u4f46\u968f\u7740\u5b9e\u4f8b\u89c4\u6a21\u589e\u5927\uff0c\u8fd9\u4e24\u9879\u6307\u6807\u5747\u4e0b\u964d\uff0c\u5373\u4f7f\u589e\u52a0\u63a8\u7406token\u4e5f\u65e0\u6d4e\u4e8e\u4e8b\u3002\u57fa\u4e8e\u96c6\u5408\u7684\u4efb\u52a1\u76f8\u5bf9\u7b80\u5355\uff0c\u800c\u56fe\u7ed3\u6784\u95ee\u9898\u548c\u74f6\u9888\u76ee\u6807\u5bfc\u81f4\u66f4\u9891\u7e41\u7684\u5931\u8d25\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7aef\u5230\u7aef\u7ec4\u5408\u4f18\u5316\u65b9\u9762\u5c55\u73b0\u51fa\u6709\u5e0c\u671b\u4f46\u6709\u9650\u7684\u80fd\u529b\uff0c\u5b58\u5728\u6027\u80fd\u6269\u5c55\u6311\u6218\u548c\u7ed3\u6784\u590d\u6742\u6027\u65b9\u9762\u7684\u7cfb\u7edf\u6027\u5f31\u70b9\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u5347\u5176\u5728\u5927\u89c4\u6a21\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e0a\u7684\u63a8\u7406\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.00781", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.00781", "abs": "https://arxiv.org/abs/2602.00781", "authors": ["Jiamin Xu", "Kyra Gan"], "title": "Fast Non-Episodic Finite-Horizon RL with K-Step Lookahead Thresholding", "comment": null, "summary": "Online reinforcement learning in non-episodic, finite-horizon MDPs remains underexplored and is challenged by the need to estimate returns to a fixed terminal time. Existing infinite-horizon methods, which often rely on discounted contraction, do not naturally account for this fixed-horizon structure. We introduce a modified Q-function: rather than targeting the full-horizon, we learn a K-step lookahead Q-function that truncates planning to the next K steps. To further improve sample efficiency, we introduce a thresholding mechanism: actions are selected only when their estimated K-step lookahead value exceeds a time-varying threshold. We provide an efficient tabular learning algorithm for this novel objective, proving it achieves fast finite-sample convergence: it achieves minimax optimal constant regret for $K=1$ and $\\mathcal{O}(\\max((K-1),C_{K-1})\\sqrt{SAT\\log(T)})$ regret for any $K \\geq 2$. We numerically evaluate the performance of our algorithm under the objective of maximizing reward. Our implementation adaptively increases K over time, balancing lookahead depth against estimation variance. Empirical results demonstrate superior cumulative rewards over state-of-the-art tabular RL methods across synthetic MDPs and RL environments: JumpRiverswim, FrozenLake and AnyTrading.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u6709\u9650\u65f6\u57df\u975e episodic \u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u63d0\u51fa K \u6b65\u524d\u77bb Q \u51fd\u6570\u4e0e\u9608\u503c\u9009\u62e9\u673a\u5236\uff0c\u7406\u8bba\u8fbe\u5230 minimax \u6700\u4f18\u9057\u61be\u754c\uff0c\u5b9e\u9a8c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u6709\u9650\u65f6\u57df\u3001\u975e episodic \u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7814\u7a76\u4e0d\u8db3\uff0c\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u9700\u4f30\u8ba1\u81f3\u56fa\u5b9a\u7ec8\u6b62\u65f6\u95f4\u7684\u56de\u62a5\u3002\u73b0\u6709\u65e0\u9650\u65f6\u57df\u65b9\u6cd5\u4f9d\u8d56\u6298\u6263\u6536\u7f29\uff0c\u65e0\u6cd5\u81ea\u7136\u5904\u7406\u56fa\u5b9a\u65f6\u57df\u7ed3\u6784\u3002", "method": "\u63d0\u51fa\u6539\u8fdb\u7684 K \u6b65\u524d\u77bb Q \u51fd\u6570\uff0c\u5c06\u89c4\u5212\u622a\u65ad\u81f3\u672a\u6765 K \u6b65\uff1b\u5f15\u5165\u9608\u503c\u673a\u5236\uff0c\u4ec5\u5f53\u52a8\u4f5c\u7684 K \u6b65\u524d\u77bb\u503c\u8d85\u8fc7\u65f6\u53d8\u9608\u503c\u65f6\u624d\u88ab\u9009\u62e9\uff1b\u63d0\u4f9b\u9ad8\u6548\u8868\u683c\u5b66\u4e60\u7b97\u6cd5\uff0c\u5e76\u81ea\u9002\u5e94\u589e\u52a0 K \u503c\u4ee5\u5e73\u8861\u524d\u77bb\u6df1\u5ea6\u4e0e\u4f30\u8ba1\u65b9\u5dee\u3002", "result": "\u7406\u8bba\u7ed3\u679c\uff1aK=1 \u65f6\u53d6\u5f97 minimax \u6700\u4f18\u5e38\u6570\u9057\u61be\uff1bK\u22652 \u65f6\u9057\u61be\u754c\u4e3a O(max((K-1), C_{K-1})\u221a(SAT log(T)))\u3002\u5b9e\u9a8c\u7ed3\u679c\uff1a\u5728\u5408\u6210 MDP \u53ca JumpRiverswim\u3001FrozenLake\u3001AnyTrading \u73af\u5883\u4e2d\uff0c\u7d2f\u79ef\u5956\u52b1\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u8868\u683c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6709\u9650\u65f6\u57df\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u524d\u77bb\u6df1\u5ea6\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u6709\u9650\u65f6\u57df\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02196", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02196", "abs": "https://arxiv.org/abs/2602.02196", "authors": ["Hang Yan", "Xinyu Che", "Fangzhi Xu", "Qiushi Sun", "Zichen Ding", "Kanzhi Cheng", "Jian Zhang", "Tao Qin", "Jun Liu", "Qika Lin"], "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents", "comment": "29pages, 10 figures", "summary": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment.", "AI": {"tldr": "The paper proposes TIDE, a diagnostic framework that decomposes Test-Time Improvement (TTI) in LLM agents into three dimensions to better understand performance constraints beyond just reasoning scaling.", "motivation": "Existing evaluation metrics for autonomous LLM agents fail to capture task optimization efficiency, behavior adaptation after errors, and working memory utility, despite TTI showing promise through iterative environment interaction.", "method": "They developed TIDE, an agent-agnostic and environment-agnostic framework that decomposes TTI into three interconnected dimensions: temporal dynamics of task completion, recursive looping behaviors, and burdensome accumulated memory. They conducted extensive experiments across diverse agents and environments.", "result": "TIDE reveals that agent performance constraints are often due to recursive loops or memory accumulation rather than reasoning limitations.", "conclusion": "Improving agent performance requires explicit optimization of agent-environment interaction dynamics, not just scaling internal reasoning capabilities."}}
{"id": "2602.00791", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.00791", "abs": "https://arxiv.org/abs/2602.00791", "authors": ["Shahryar Zehtabi", "Dong-Jun Han", "Seyyedali Hosseinalipour", "Christopher Brinton"], "title": "Sporadic Gradient Tracking over Directed Graphs: A Theoretical Perspective on Decentralized Federated Learning", "comment": "32 pages, 5 figures", "summary": "Decentralized Federated Learning (DFL) enables clients with local data to collaborate in a peer-to-peer manner to train a generalized model. In this paper, we unify two branches of work that have separately solved important challenges in DFL: (i) gradient tracking techniques for mitigating data heterogeneity and (ii) accounting for diverse availability of resources across clients. We propose $\\textit{Sporadic Gradient Tracking}$ ($\\texttt{Spod-GT}$), the first DFL algorithm that incorporates these factors over general directed graphs by allowing (i) client-specific gradient computation frequencies and (ii) heterogeneous and asymmetric communication frequencies. We conduct a rigorous convergence analysis of our methodology with relaxed assumptions on gradient estimation variance and gradient diversity of clients, providing consensus and optimality guarantees for GT over directed graphs despite intermittent client participation. Through numerical experiments on image classification datasets, we demonstrate the efficacy of $\\texttt{Spod-GT}$ compared to well-known GT baselines.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u7ed3\u5408\u68af\u5ea6\u8ddf\u8e2a\u4e0e\u8d44\u6e90\u5f02\u6784\u6027\u7684\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5Spod-GT\uff0c\u5728\u5b9a\u5411\u56fe\u4e0a\u5b9e\u73b0\u95f4\u6b47\u6027\u5ba2\u6237\u7aef\u53c2\u4e0e\u7684\u6536\u655b\u4fdd\u8bc1", "motivation": "\u73b0\u6709\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\uff08DFL\uff09\u7814\u7a76\u5c06\u6570\u636e\u5f02\u6784\u6027\u7f13\u89e3\u4e0e\u8d44\u6e90\u53ef\u7528\u6027\u5dee\u5f02\u5904\u7406\u5272\u88c2\uff0c\u7f3a\u4e4f\u540c\u65f6\u89e3\u51b3\u8fd9\u4e24\u4e2a\u5173\u952e\u6311\u6218\u7684\u7edf\u4e00\u6846\u67b6", "method": "\u63d0\u51fa\u7a00\u758f\u68af\u5ea6\u8ddf\u8e2a\uff08Spod-GT\uff09\u7b97\u6cd5\uff0c\u652f\u6301\u5ba2\u6237\u7aef\u7279\u5f02\u6027\u68af\u5ea6\u8ba1\u7b97\u9891\u7387\u548c\u5f02\u8d28\u975e\u5bf9\u79f0\u901a\u4fe1\u9891\u7387\uff0c\u5728\u4e00\u822c\u5b9a\u5411\u56fe\u4e0a\u5b9e\u73b0\u68af\u5ea6\u8ddf\u8e2a\uff0c\u5e76\u653e\u5bbd\u68af\u5ea6\u4f30\u8ba1\u65b9\u5dee\u4e0e\u591a\u6837\u6027\u5047\u8bbe", "result": "\u7406\u8bba\u8bc1\u660e\u8be5\u7b97\u6cd5\u5728\u95f4\u6b47\u6027\u5ba2\u6237\u7aef\u53c2\u4e0e\u4e0b\u4ecd\u5177\u5171\u8bc6\u6027\u548c\u6700\u4f18\u6027\u4fdd\u8bc1\uff0c\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u5b9e\u9a8c\u663e\u793a\u5176\u4f18\u4e8e\u73b0\u6709\u68af\u5ea6\u8ddf\u8e2a\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "Spod-GT\u662f\u9996\u4e2a\u5728\u901a\u7528\u5b9a\u5411\u56fe\u4e2d\u540c\u65f6\u6574\u5408\u6570\u636e\u5f02\u6784\u6027\u4e0e\u8d44\u6e90\u5f02\u8d28\u6027\u7684DFL\u7b97\u6cd5\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5206\u5e03\u5f0f\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.02304", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02304", "abs": "https://arxiv.org/abs/2602.02304", "authors": ["Martino Ciaperoni", "Marzio Di Vece", "Luca Pappalardo", "Fosca Giannotti", "Francesco Giannini"], "title": "Position: Explaining Behavioral Shifts in Large Language Models Requires a Comparative Approach", "comment": null, "summary": "Large-scale foundation models exhibit behavioral shifts: intervention-induced behavioral changes that appear after scaling, fine-tuning, reinforcement learning or in-context learning. While investigating these phenomena have recently received attention, explaining their appearance is still overlooked. Classic explainable AI (XAI) methods can surface failures at a single checkpoint of a model, but they are structurally ill-suited to justify what changed internally across different checkpoints and which explanatory claims are warranted about that change. We take the position that behavioral shifts should be explained comparatively: the core target should be the intervention-induced shift between a reference model and an intervened model, rather than any single model in isolation. To this aim we formulate a Comparative XAI ($\u0394$-XAI) framework with a set of desiderata to be taken into account when designing proper explaining methods. To highlight how $\u0394$-XAI methods work, we introduce a set of possible pipelines, relate them to the desiderata, and provide a concrete $\u0394$-XAI experiment.", "AI": {"tldr": "The paper proposes a Comparative XAI (\u0394-XAI) framework to explain intervention-induced behavioral shifts in foundation models by comparing models before and after interventions, rather than analyzing single models in isolation.", "motivation": "While behavioral shifts in large models have been studied, explaining their appearance remains overlooked. Classic XAI methods are structurally ill-suited to justify internal changes across different model checkpoints and determine which explanatory claims are warranted about those changes.", "method": "The authors formulate a \u0394-XAI framework with desiderata for comparative explanations, introduce possible explanation pipelines, relate them to the desiderata, and provide a concrete experimental demonstration.", "result": "They present a structured framework for comparative explanations, establish desiderata for designing \u0394-XAI methods, propose several implementation pipelines, and validate the approach through a concrete experiment.", "conclusion": "Behavioral shifts should be explained comparatively (focusing on the shift between reference and intervened models) rather than in isolation, and the \u0394-XAI framework provides a principled approach to generate warranted explanatory claims about model changes."}}
{"id": "2602.00792", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00792", "abs": "https://arxiv.org/abs/2602.00792", "authors": ["Guinan Chen", "Xunpeng Huang", "Ying Sun", "Shijin Wang", "Yanyong Zhang", "Chao Wang"], "title": "Latent Shadows: The Gaussian-Discrete Duality in Masked Diffusion", "comment": "10 pages", "summary": "Masked discrete diffusion is a dominant paradigm for high-quality language modeling where tokens are iteratively corrupted to a mask state, yet its inference efficiency is bottlenecked by the lack of deterministic sampling tools. While diffusion duality enables deterministic distillation for uniform models, these approaches generally underperform masked models and rely on complex integral operators. Conversely, in the masked domain, prior methods typically assume the absence of deterministic trajectories, forcing a reliance on stochastic distillation. To bridge this gap, we establish explicit Masked Diffusion Duality, proving that the masked process arises as the projection of a continuous Gaussian process via a novel maximum-value index preservation mechanism. Furthermore, we introduce Masked Consistency Distillation (MCD), a principled framework that leverages this duality to analytically construct the deterministic coupled trajectories required for consistency distillation, bypassing numerical ODE solvers. This result strictly improves upon prior stochastic distillation methods, achieving a 16$\\times$ inference speedup without compromising generation quality. Our findings not only provide a solid theoretical foundation connecting masked and continuous diffusion, but also unlock the full potential of consistency distillation for high-performance discrete generation. Our code is available at https://anonymous.4open.science/r/MCD-70FD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u63a9\u7801\u6269\u6563\u5bf9\u5076\u6027\u548c\u63a9\u7801\u4e00\u81f4\u6027\u84b8\u998f(MCD)\uff0c\u4e3a\u63a9\u7801\u79bb\u6563\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u786e\u5b9a\u6027\u91c7\u6837\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b016\u500d\u63a8\u7406\u52a0\u901f\u3002", "motivation": "\u63a9\u7801\u79bb\u6563\u6269\u6563\u5728\u8bed\u8a00\u5efa\u6a21\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u56e0\u7f3a\u4e4f\u786e\u5b9a\u6027\u91c7\u6837\u5de5\u5177\u800c\u9762\u4e34\u63a8\u7406\u6548\u7387\u74f6\u9888\u3002\u73b0\u6709\u7684\u5747\u5300\u6269\u6563\u6a21\u578b\u786e\u5b9a\u6027\u84b8\u998f\u65b9\u6cd5\u5728\u63a9\u7801\u6a21\u578b\u4e0a\u6027\u80fd\u4e0d\u8db3\uff0c\u800c\u5148\u524d\u63a9\u7801\u57df\u65b9\u6cd5\u56e0\u5047\u8bbe\u4e0d\u5b58\u5728\u786e\u5b9a\u6027\u8f68\u8ff9\u800c\u4f9d\u8d56\u968f\u673a\u84b8\u998f\u3002", "method": "\u4f5c\u8005\u8bc1\u660e\u4e86\u63a9\u7801\u6269\u6563\u5bf9\u5076\u6027\uff0c\u63ed\u793a\u63a9\u7801\u8fc7\u7a0b\u53ef\u901a\u8fc7\u6700\u5927\u503c\u7d22\u5f15\u4fdd\u6301\u673a\u5236\u6295\u5f71\u81ea\u8fde\u7eed\u9ad8\u65af\u8fc7\u7a0b\u3002\u57fa\u4e8e\u6b64\uff0c\u4ed6\u4eec\u63d0\u51fa\u63a9\u7801\u4e00\u81f4\u6027\u84b8\u998f(MCD)\uff0c\u901a\u8fc7\u89e3\u6790\u65b9\u5f0f\u6784\u5efa\u4e00\u81f4\u6027\u84b8\u998f\u6240\u9700\u7684\u786e\u5b9a\u6027\u8026\u5408\u8f68\u8ff9\uff0c\u7ed5\u8fc7\u4e86\u6570\u503cODE\u6c42\u89e3\u5668\u3002", "result": "MCD\u4e25\u683c\u4f18\u4e8e\u5148\u524d\u7684\u968f\u673a\u84b8\u998f\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b016\u500d\u63a8\u7406\u52a0\u901f\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u63a9\u7801\u4e0e\u8fde\u7eed\u6269\u6563\u6a21\u578b\u4e4b\u95f4\u5efa\u7acb\u4e86\u575a\u5b9e\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u5145\u5206\u91ca\u653e\u4e86\u4e00\u81f4\u6027\u84b8\u998f\u5728\u9ad8\u6027\u80fd\u79bb\u6563\u751f\u6210\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u66f4\u9ad8\u6548\u7684\u8bed\u8a00\u5efa\u6a21\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.02313", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02313", "abs": "https://arxiv.org/abs/2602.02313", "authors": ["Changming Li", "Kaixing Zhang", "Haoyun Xu", "Yingdong Shi", "Zheng Zhang", "Kaitao Song", "Kan Ren"], "title": "Interpreting and Controlling LLM Reasoning through Integrated Policy Gradient", "comment": null, "summary": "Large language models (LLMs) demonstrate strong reasoning abilities in solving complex real-world problems. Yet, the internal mechanisms driving these complex reasoning behaviors remain opaque. Existing interpretability approaches targeting reasoning either identify components (e.g., neurons) correlated with special textual patterns, or rely on human-annotated contrastive pairs to derive control vectors. Consequently, current methods struggle to precisely localize complex reasoning mechanisms or capture sequential influence from model internal workings to the reasoning outputs. In this paper, built on outcome-oriented and sequential-influence-aware principles, we focus on identifying components that have sequential contribution to reasoning behavior where outcomes are cumulated by long-range effects. We propose Integrated Policy Gradient (IPG), a novel framework that attributes reasoning behaviors to model's inner components by propagating compound outcome-based signals such as post reasoning accuracy backward through model inference trajectories. Empirical evaluations demonstrate that our approach achieves more precise localization and enables reliable modulation of reasoning behaviors (e.g., reasoning capability, reasoning strength) across diverse reasoning models.", "AI": {"tldr": "Proposes IPG framework to interpret LLM reasoning by propagating outcome-based signals backward through inference trajectories for precise localization and modulation of reasoning mechanisms.", "motivation": "LLMs have opaque internal reasoning mechanisms; existing interpretability methods cannot precisely localize complex reasoning or capture sequential influence from model internals to outputs.", "method": "Integrated Policy Gradient (IPG) - attributes reasoning behaviors by propagating compound outcome-based signals (e.g., post-reasoning accuracy) backward through model inference trajectories, focusing on sequential contribution of components.", "result": "Achieves more precise localization of reasoning mechanisms and enables reliable modulation of reasoning behaviors (capability, strength) across diverse reasoning models.", "conclusion": "IPG provides a novel interpretability framework that offers more precise understanding and control of LLM reasoning mechanisms compared to existing approaches."}}
{"id": "2602.00800", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00800", "abs": "https://arxiv.org/abs/2602.00800", "authors": ["Yebin Yang", "Huaijin Wu", "Fu Guo", "Lin Yao", "Xiaohan Qin", "Jingzhi Wang", "Debing Zhang", "Junchi Yan"], "title": "JTok: On Token Embedding as another Axis of Scaling Law via Joint Token Self-modulation", "comment": null, "summary": "LLMs have traditionally scaled along dense dimensions, where performance is coupled with near-linear increases in computational cost. While MoE decouples capacity from compute, it introduces large memory overhead and hardware efficiency challenges. To overcome these, we propose token-indexed parameters as a novel, orthogonal scaling axis that decouple model capacity from FLOPs. Specifically, we introduce Joint-Token (JTok) and Mixture of Joint-Token (JTok-M), which augment Transformer layers with modulation vectors retrieved from auxiliary embedding tables. These vectors modulate the backbone via lightweight, element-wise operations, incurring negligible FLOPs overhead. Extensive experiments on both dense and MoE backbones, spanning from 650M (190M + 460M embedding) to 61B (17B + 44B embedding) total parameters, demonstrate that our approach consistently reduces validation loss and significantly improves downstream task performance (e.g., +4.1 on MMLU, +8.3 on ARC, +8.9 on CEval). Rigorous isoFLOPs analysis further confirms that JTok-M fundamentally shifts the quality-compute Pareto frontier, achieving comparable model quality with 35% less compute relative to vanilla MoE architectures, and we validate that token-indexed parameters exhibit a predictable power-law scaling behavior. Moreover, our efficient implementation ensures that the overhead introduced by JTok and JTok-M remains marginal.", "AI": {"tldr": "\u63d0\u51fatoken-indexed\u53c2\u6570\uff08JTok/JTok-M\uff09\u4f5c\u4e3a\u4e00\u79cd\u65b0\u7684\u6b63\u4ea4\u6269\u5c55\u8f74\uff0c\u89e3\u8026\u6a21\u578b\u5bb9\u91cf\u4e0eFLOPs\uff0c\u5728\u76f8\u6bd4MoE\u67b6\u6784\u51cf\u5c1135%\u8ba1\u7b97\u91cf\u7684\u540c\u65f6\u83b7\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u5bc6\u96c6\u6a21\u578b\u5c06\u6027\u80fd\u4e0e\u8ba1\u7b97\u6210\u672c\u8026\u5408\uff1bMoE\u867d\u89e3\u8026\u5bb9\u91cf\u4e0e\u8ba1\u7b97\u4f46\u5f15\u5165\u5185\u5b58\u5f00\u9500\u548c\u786c\u4ef6\u6548\u7387\u6311\u6218\uff0c\u9700\u8981\u65b0\u7684\u6269\u5c55\u65b9\u6cd5\u3002", "method": "\u5f15\u5165Joint-Token\uff08JTok\uff09\u548cMixture of Joint-Token\uff08JTok-M\uff09\uff0c\u901a\u8fc7\u4ece\u8f85\u52a9\u5d4c\u5165\u8868\u68c0\u7d22\u7684token-indexed\u8c03\u5236\u5411\u91cf\uff0c\u4ee5\u8f7b\u91cf\u7ea7\u5143\u7d20\u7ea7\u64cd\u4f5c\u589e\u5f3aTransformer\u5c42\u3002", "result": "\u5728650M-61B\u53c2\u6570\u89c4\u6a21\u7684\u5bc6\u96c6\u548cMoE\u6a21\u578b\u4e0a\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u964d\u4f4e\u9a8c\u8bc1\u635f\u5931\u5e76\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff08\u5982MMLU +4.1\u3001ARC +8.3\u3001CEval +8.9\uff09\u3002\u7b49FLOPs\u5206\u6790\u8bc1\u5b9eJTok-M\u53ef\u8f6c\u79fb\u8d28\u91cf-\u8ba1\u7b97\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u76f8\u6bd4\u4f20\u7edfMoE\u8282\u770135%\u8ba1\u7b97\u91cf\uff0c\u4e14token-indexed\u53c2\u6570\u5448\u73b0\u53ef\u9884\u6d4b\u7684\u5e42\u5f8b\u6269\u5c55\u884c\u4e3a\uff0c\u989d\u5916\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "Token-indexed\u53c2\u6570\u4ece\u6839\u672c\u4e0a\u6539\u5584\u4e86\u8d28\u91cf-\u8ba1\u7b97\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u4e3a\u5bc6\u96c6\u548cMoE\u67b6\u6784\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u6269\u5c55\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2602.02350", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.02350", "abs": "https://arxiv.org/abs/2602.02350", "authors": ["Xingyuan Hua", "Sheng Yue", "Xinyi Li", "Yizhe Zhao", "Jinrui Zhang", "Ju Ren"], "title": "Context Learning for Multi-Agent Discussion", "comment": null, "summary": "Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual contexts.In this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive mechanism.It enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency.", "AI": {"tldr": "The paper proposes M2CL, a multi-LLM context learning method that uses dynamic context generators per agent to fix discussion inconsistency in MAD systems, achieving 20-50% improvement over existing methods.", "motivation": "Current Multi-Agent Discussion (MAD) methods suffer from discussion inconsistency where LLMs fail to reach coherent solutions due to misalignment between their individual contexts, leading to premature convergence on majority noise.", "method": "M2CL learns a context generator for each agent that dynamically generates context instructions per discussion round through automatic information organization and refinement. It uses a self-adaptive mechanism to control context coherence and output discrepancies.", "result": "Evaluated on academic reasoning, embodied tasks, and mobile control, M2CL outperforms existing methods by 20%-50% while showing good transferability and computational efficiency.", "conclusion": "The proposed M2CL method effectively addresses discussion inconsistency in MAD systems by dynamically aligning agent contexts, leading to significantly better performance across diverse challenging tasks."}}
{"id": "2602.00809", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00809", "abs": "https://arxiv.org/abs/2602.00809", "authors": ["David Craveiro", "Hugo Silva"], "title": "Mobile Exergames: Activity Recognition Based on Smartphone Sensors", "comment": null, "summary": "Smartphone sensors can be extremely useful in providing information on the activities and behaviors of persons. Human activity recognition is increasingly used for games, medical, or surveillance. In this paper, we propose a proof-of-concept 2D endless game called Duck Catch & Fit, which implements a detailed activity recognition system that uses a smartphone accelerometer, gyroscope, and magnetometer sensors. The system applies feature extraction and learning mechanism to detect human activities like staying, side movements, and fake side movements. In addition, a voice recognition system is combined to recognize the word \"fire\" and raise the game's complexity. The results show that it is possible to use machine learning techniques to recognize human activity with high recognition levels. Also, the combination of movement-based and voice-based integrations contributes to a more immersive gameplay.", "AI": {"tldr": "This paper presents a smartphone-based activity recognition system implemented in a 2D endless game (\"Duck Catch & Fit\"), using accelerometer, gyroscope, and magnetometer sensors combined with voice recognition (\"fire\" command) to detect human activities (stationary, side movements, fake movements) with high accuracy, enhancing gameplay immersion through multimodal interaction.", "motivation": "Leverage ubiquitous smartphone sensors for practical human activity recognition (applicable to gaming, healthcare, surveillance) by developing an engaging proof-of-concept game that demonstrates real-time activity detection and multimodal interaction.", "method": "Implemented a 2D endless game integrating: 1) Multi-sensor data (accelerometer, gyroscope, magnetometer) for activity recognition; 2) Feature extraction and machine learning to classify activities (stationary, genuine side movements, fake side movements); 3) Voice recognition system to detect the \"fire\" command, adding gameplay complexity.", "result": "Achieved high accuracy in recognizing human activities using machine learning on smartphone sensor data; successfully combined movement-based and voice-based interactions to create a more immersive and complex gaming experience.", "conclusion": "Smartphone sensors enable highly accurate real-time human activity recognition suitable for interactive applications; multimodal integration (movement + voice) significantly enhances user immersion and gameplay depth, validating the proof-of-concept."}}
{"id": "2602.02369", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02369", "abs": "https://arxiv.org/abs/2602.02369", "authors": ["Yaolun Zhang", "Yiran Wu", "Yijiong Yu", "Qingyun Wu", "Huazheng Wang"], "title": "Live-Evo: Online Evolution of Agentic Memory from Continuous Feedback", "comment": "13 pages", "summary": "Large language model (LLM) agents are increasingly equipped with memory, which are stored experience and reusable guidance that can improve task-solving performance. Recent \\emph{self-evolving} systems update memory based on interaction outcomes, but most existing evolution pipelines are developed for static train/test splits and only approximate online learning by folding static benchmarks, making them brittle under true distribution shift and continuous feedback. We introduce \\textsc{Live-Evo}, an online self-evolving memory system that learns from a stream of incoming data over time. \\textsc{Live-Evo} decouples \\emph{what happened} from \\emph{how to use it} via an Experience Bank and a Meta-Guideline Bank, compiling task-adaptive guidelines from retrieved experiences for each task. To manage memory online, \\textsc{Live-Evo} maintains experience weights and updates them from feedback: experiences that consistently help are reinforced and retrieved more often, while misleading or stale experiences are down-weighted and gradually forgotten, analogous to reinforcement and decay in human memory. On the live \\textit{Prophet Arena} benchmark over a 10-week horizon, \\textsc{Live-Evo} improves Brier score by 20.8\\% and increases market returns by 12.9\\%, while also transferring to deep-research benchmarks with consistent gains over strong baselines. Our code is available at https://github.com/ag2ai/Live-Evo.", "AI": {"tldr": "\u63d0\u51faLive-Evo\uff0c\u4e00\u79cd\u5728\u7ebf\u81ea\u8fdb\u5316\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u901a\u8fc7\u89e3\u8026\"\u7ecf\u9a8c\"\u4e0e\"\u6307\u5357\"\u5e76\u52a8\u6001\u66f4\u65b0\u8bb0\u5fc6\u6743\u91cd\uff0c\u4f7fLLM\u667a\u80fd\u4f53\u80fd\u591f\u5728\u8fde\u7eed\u6570\u636e\u6d41\u4e2d\u6301\u7eed\u5b66\u4e60\uff0c\u5728\u771f\u5b9e\u5206\u5e03\u6f02\u79fb\u4e0b\u8868\u73b0\u9c81\u68d2\u3002", "motivation": "\u73b0\u6709\u81ea\u8fdb\u5316\u7cfb\u7edf\u4ec5\u9488\u5bf9\u9759\u6001\u8bad\u7ec3/\u6d4b\u8bd5\u5206\u5272\u8bbe\u8ba1\uff0c\u65e0\u6cd5\u9002\u5e94\u771f\u5b9e\u5206\u5e03\u6f02\u79fb\u548c\u8fde\u7eed\u53cd\u9988\uff0c\u5728\u7ebf\u5b66\u4e60\u8fd1\u4f3c\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u7ecf\u9a8c\u5e93\u4e0e\u5143\u6307\u5357\u5e93\u53cc\u7ed3\u6784\uff0c\u5206\u79bb\"\u53d1\u751f\u4e86\u4ec0\u4e48\"\u4e0e\"\u5982\u4f55\u4f7f\u7528\"\uff1b\u901a\u8fc7\u53cd\u9988\u52a8\u6001\u8c03\u6574\u7ecf\u9a8c\u6743\u91cd\uff0c\u5f3a\u5316\u6709\u6548\u7ecf\u9a8c\u3001\u8870\u51cf\u8bef\u5bfc\u7ecf\u9a8c\uff0c\u6a21\u62df\u4eba\u7c7b\u8bb0\u5fc6\u5f3a\u5316\u4e0e\u9057\u5fd8\u673a\u5236\u3002", "result": "\u572810\u5468\u5b9e\u65f6Prophet Arena\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBrier\u5206\u6570\u63d0\u534720.8%\uff0c\u5e02\u573a\u6536\u76ca\u589e\u52a012.9%\uff0c\u5e76\u8fc1\u79fb\u81f3\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u3002", "conclusion": "Live-Evo\u5b9e\u73b0\u4e86LLM\u667a\u80fd\u4f53\u7684\u771f\u6b63\u5728\u7ebf\u81ea\u8fdb\u5316\u8bb0\u5fc6\uff0c\u89e3\u8026\u67b6\u6784\u4e0e\u52a8\u6001\u6743\u91cd\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u548c\u5206\u5e03\u6f02\u79fb\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.02386", "categories": ["cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02386", "abs": "https://arxiv.org/abs/2602.02386", "authors": ["Mika Okamoto", "Ansel Kaplan Erol", "Glenn Matlin"], "title": "Trust by Design: Skill Profiles for Transparent, Cost-Aware LLM Routing", "comment": "Appeared at MLSys YPS 2025", "summary": "How should Large Language Model (LLM) practitioners select the right model for a task without wasting money? We introduce BELLA (Budget-Efficient LLM Selection via Automated skill-profiling), a framework that recommends optimal LLM selection for tasks through interpretable skill-based model selection. Standard benchmarks report aggregate metrics that obscure which specific capabilities a task requires and whether a cheaper model could suffice. BELLA addresses this gap through three stages: (1) decomposing LLM outputs and extract the granular skills required by using critic-based profiling, (2) clustering skills into structured capability matrices, and (3) multi-objective optimization to select the right models to maximize performance while respecting budget constraints. BELLA provides natural-language rationale for recommendations, providing transparency that current black-box routing systems lack. We describe the framework architecture, situate it within the landscape of LLM routing and evaluation, and discuss its application to financial reasoning as a representative domain exhibiting diverse skill requirements and cost-variation across models. Our framework enables practitioners to make principled and cost-performance trade-offs for deploying LLMs.", "AI": {"tldr": "BELLA is a budget-efficient LLM selection framework that uses automated skill-profiling to recommend optimal models based on granular task capabilities instead of aggregate benchmarks.", "motivation": "Current LLM selection wastes money because standard benchmarks provide only aggregate metrics that obscure specific task requirements and prevent identifying if cheaper models could suffice, lacking transparency.", "method": "A three-stage approach: (1) critic-based profiling to extract granular task skills, (2) clustering skills into structured capability matrices, and (3) multi-objective optimization to select models maximizing performance within budget constraints, with natural-language rationale.", "result": "The paper describes BELLA's architecture, positions it in the LLM routing landscape, and discusses its application to financial reasoning, demonstrating how it handles diverse skill requirements and cost variations across models.", "conclusion": "BELLA enables practitioners to make transparent, principled cost-performance trade-offs for LLM deployment through interpretable skill-based selection."}}
{"id": "2602.00834", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.00834", "abs": "https://arxiv.org/abs/2602.00834", "authors": ["Wei Chen", "Jiacheng Li", "Shigui Li", "Zhiqi Lin", "Junmei Yang", "John Paisley", "Delu Zeng"], "title": "Don't Forget Its Variance! The Minimum Path Variance Principle for Accurate and Stable Score-Based Density Ratio Estimation", "comment": null, "summary": "Score-based methods have emerged as a powerful framework for density ratio estimation (DRE), but they face an important paradox in that, while theoretically path-independent, their practical performance depends critically on the chosen path schedule. We resolve this issue by proving that tractable training objectives differ from the ideal, ground-truth objective by a crucial, overlooked term: the path variance of the time score. To address this, we propose MinPV (\\textbf{Min}imum \\textbf{P}ath \\textbf{V}ariance) Principle, which introduces a principled heuristic to minimize the overlooked path variance. Our key contribution is the derivation of a closed-form expression for the variance, turning an intractable problem into a tractable optimization. By parameterizing the path with a flexible Kumaraswamy Mixture Model, our method learns a data-adaptive, low-variance path without heuristic selection. This principled optimization of the complete objective yields more accurate and stable estimators, establishing new state-of-the-art results on challenging benchmarks.", "AI": {"tldr": "\u9488\u5bf9\u57fa\u4e8e\u5206\u6570\u7684\u65b9\u6cd5\u5728\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u4e2d\u5b58\u5728\u8def\u5f84\u9009\u62e9\u6096\u8bba\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u6700\u5c0f\u8def\u5f84\u65b9\u5dee(MinPV)\u539f\u5219\uff0c\u901a\u8fc7\u63a8\u5bfc\u8def\u5f84\u65b9\u5dee\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\u5e76\u5f15\u5165Kumaraswamy\u6df7\u5408\u6a21\u578b\u8fdb\u884c\u53c2\u6570\u5316\uff0c\u5b9e\u73b0\u4e86\u6570\u636e\u81ea\u9002\u5e94\u7684\u4f4e\u65b9\u5dee\u8def\u5f84\u4f18\u5316\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u5148\u8fdb\u7ed3\u679c\u3002", "motivation": "\u57fa\u4e8e\u5206\u6570\u7684\u65b9\u6cd5\u5728\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u4e2d\u7406\u8bba\u4e0a\u5e94\u4e0e\u8def\u5f84\u65e0\u5173\uff0c\u4f46\u5b9e\u8df5\u4e2d\u6027\u80fd\u4e25\u91cd\u4f9d\u8d56\u8def\u5f84\u8c03\u5ea6\u9009\u62e9\uff0c\u8fd9\u79cd\u6096\u8bba\u6e90\u4e8e\u8bad\u7ec3\u76ee\u6807\u4e0e\u771f\u5b9e\u76ee\u6807\u4e4b\u95f4\u5b58\u5728\u88ab\u5ffd\u89c6\u7684\u8def\u5f84\u65b9\u5dee\u9879\uff0c\u4e9f\u9700\u4e00\u79cd\u539f\u5219\u6027\u65b9\u6cd5\u89e3\u51b3\u8def\u5f84\u9009\u62e9\u95ee\u9898\u3002", "method": "\u63d0\u51faMinPV\u539f\u5219\uff0c\u63a8\u5bfc\u65f6\u95f4\u5206\u6570\u8def\u5f84\u65b9\u5dee\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5c06\u96be\u89e3\u95ee\u9898\u8f6c\u5316\u4e3a\u53ef\u4f18\u5316\u95ee\u9898\uff1b\u4f7f\u7528Kumaraswamy\u6df7\u5408\u6a21\u578b\u5bf9\u8def\u5f84\u8fdb\u884c\u7075\u6d3b\u53c2\u6570\u5316\uff0c\u5b66\u4e60\u6570\u636e\u81ea\u9002\u5e94\u7684\u4f4e\u65b9\u5dee\u8def\u5f84\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u4ea7\u751f\u66f4\u51c6\u786e\u7a33\u5b9a\u7684\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u5668\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u6700\u5c0f\u5316\u88ab\u5ffd\u89c6\u7684\u8def\u5f84\u65b9\u5dee\u9879\uff0cMinPV\u539f\u5219\u89e3\u51b3\u4e86\u57fa\u4e8e\u5206\u6570\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u4e2d\u7684\u6838\u5fc3\u6096\u8bba\uff0c\u4e3a\u8def\u5f84\u4f18\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f30\u8ba1\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2602.02416", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02416", "abs": "https://arxiv.org/abs/2602.02416", "authors": ["Ankur Samanta", "Akshayaa Magesh", "Ayush Jain", "Kavosh Asadi", "Youliang Yu", "Daniel Jiang", "Boris Vidolov", "Kaveh Hassani", "Paul Sajda", "Jalaj Bhandari", "Yonathan Efroni"], "title": "Structure Enables Effective Self-Localization of Errors in LLMs", "comment": null, "summary": "Self-correction in language models remains elusive. In this work, we explore whether language models can explicitly localize errors in incorrect reasoning, as a path toward building AI systems that can effectively correct themselves. We introduce a prompting method that structures reasoning as discrete, semantically coherent thought steps, and show that models are able to reliably localize errors within this structure, while failing to do so in conventional, unstructured chain-of-thought reasoning. Motivated by how the human brain monitors errors at discrete decision points and resamples alternatives, we introduce Iterative Correction Sampling of Thoughts (Thought-ICS), a self-correction framework. Thought-ICS iteratively prompts the model to generate reasoning one discrete and complete thought at a time--where each thought represents a deliberate decision by the model--creating natural boundaries for precise error localization. Upon verification, the model localizes the first erroneous step, and the system backtracks to generate alternative reasoning from the last correct point. When asked to correct reasoning verified as incorrect by an oracle, Thought-ICS achieves 20-40% self-correction lift. In a completely autonomous setting without external verification, it outperforms contemporary self-correction baselines.", "AI": {"tldr": "The paper introduces Thought-ICS, a self-correction framework that structures reasoning into discrete thought steps to enable error localization and backtracking, achieving 20-40% improvement over existing methods.", "motivation": "Self-correction in language models remains elusive because models struggle to identify where errors occur in their reasoning chains. The authors explore whether explicit error localization can lead to effective self-correction.", "method": "The authors propose a prompting method that structures reasoning as discrete, semantically coherent thought steps. They introduce Iterative Correction Sampling of Thoughts (Thought-ICS), which generates reasoning one complete thought at a time, creates natural boundaries for error localization, and backtracks to generate alternatives from the last correct point when errors are detected.", "result": "When using oracle verification, Thought-ICS achieves 20-40% self-correction lift. In completely autonomous settings without external verification, it outperforms contemporary self-correction baselines.", "conclusion": "Structuring reasoning into discrete thought steps enables language models to reliably localize errors and effectively self-correct, representing a significant step toward building AI systems that can autonomously identify and fix their own mistakes."}}
{"id": "2602.00849", "categories": ["cs.LG", "cs.AI", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.00849", "abs": "https://arxiv.org/abs/2602.00849", "authors": ["Yuhao Huang", "Shih-Hsin Wang", "Andrea L. Bertozzi", "Bao Wang"], "title": "RMFlow: Refined Mean Flow by a Noise-Injection Step for Multimodal Generation", "comment": "Accepted to ICLR 2026", "summary": "Mean flow (MeanFlow) enables efficient, high-fidelity image generation, yet its single-function evaluation (1-NFE) generation often cannot yield compelling results. We address this issue by introducing RMFlow, an efficient multimodal generative model that integrates a coarse 1-NFE MeanFlow transport with a subsequent tailored noise-injection refinement step. RMFlow approximates the average velocity of the flow path using a neural network trained with a new loss function that balances minimizing the Wasserstein distance between probability paths and maximizing sample likelihood. RMFlow achieves near state-of-the-art results on text-to-image, context-to-molecule, and time-series generation using only 1-NFE, at a computational cost comparable to the baseline MeanFlows.", "AI": {"tldr": "RMFlow enhances MeanFlow's single-step image generation by adding a noise-injection refinement step, achieving near state-of-the-art results across multiple modalities with only 1 neural function evaluation.", "motivation": "MeanFlow's single-function evaluation (1-NFE) generation produces insufficiently compelling results despite its efficiency and high fidelity.", "method": "Integrates coarse 1-NFE MeanFlow transport with tailored noise-injection refinement, using a neural network trained with a loss balancing Wasserstein distance minimization and likelihood maximization.", "result": "Achieves near state-of-the-art performance on text-to-image, context-to-molecule, and time-series generation using only 1-NFE, at computational cost comparable to baseline MeanFlows.", "conclusion": "RMFlow provides an efficient multimodal generative solution that significantly improves 1-NFE generation quality without increasing computational burden."}}
{"id": "2602.02419", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.02419", "abs": "https://arxiv.org/abs/2602.02419", "authors": ["Qingni Wang", "Yue Fan", "Xin Eric Wang"], "title": "SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration", "comment": null, "summary": "Graphical User Interface (GUI) grounding aims to translate natural language instructions into executable screen coordinates, enabling automated GUI interaction. Nevertheless, incorrect grounding can result in costly, hard-to-reverse actions (e.g., erroneous payment approvals), raising concerns about model reliability. In this paper, we introduce SafeGround, an uncertainty-aware framework for GUI grounding models that enables risk-aware predictions through calibrations before testing. SafeGround leverages a distribution-aware uncertainty quantification method to capture the spatial dispersion of stochastic samples from outputs of any given model. Then, through the calibration process, SafeGround derives a test-time decision threshold with statistically guaranteed false discovery rate (FDR) control. We apply SafeGround on multiple GUI grounding models for the challenging ScreenSpot-Pro benchmark. Experimental results show that our uncertainty measure consistently outperforms existing baselines in distinguishing correct from incorrect predictions, while the calibrated threshold reliably enables rigorous risk control and potentials of substantial system-level accuracy improvements. Across multiple GUI grounding models, SafeGround improves system-level accuracy by up to 5.38\\% percentage points over Gemini-only inference.", "AI": {"tldr": "SafeGround is an uncertainty-aware framework for GUI grounding that uses distribution-aware uncertainty quantification and calibration to enable risk-aware predictions with statistically guaranteed false discovery rate control, improving system-level accuracy by up to 5.38%.", "motivation": "Incorrect GUI grounding can cause costly, irreversible actions (e.g., erroneous payments), highlighting the critical need for reliable model predictions with risk control mechanisms.", "method": "Proposes SafeGround: 1) Leverages distribution-aware uncertainty quantification to measure spatial dispersion of model outputs; 2) Calibrates test-time decision thresholds with statistically guaranteed false discovery rate (FDR) control.", "result": "Outperforms existing uncertainty baselines in distinguishing correct/incorrect predictions; achieves rigorous risk control and up to 5.38% system-level accuracy improvement over Gemini-only inference on ScreenSpot-Pro benchmark across multiple GUI grounding models.", "conclusion": "SafeGround enables reliable, risk-aware GUI grounding deployment through effective uncertainty quantification and calibration, significantly enhancing system safety and accuracy without model retraining."}}
{"id": "2602.00852", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00852", "abs": "https://arxiv.org/abs/2602.00852", "authors": ["Pattarawat Chormai", "Klaus-Robert M\u00fcller", "Gr\u00e9goire Montavon"], "title": "Investigating the Robustness of Subtask Distillation under Spurious Correlation", "comment": "7 pages, 3 figures", "summary": "Subtask distillation is an emerging paradigm in which compact, specialized models are extracted from large, general-purpose 'foundation models' for deployment in environments with limited resources or in standalone computer systems. Although distillation uses a teacher model, it still relies on a dataset that is often limited in size and may lack representativeness or exhibit spurious correlations. In this paper, we evaluate established distillation methods, as well as the recent SubDistill method, when using data with spurious correlations for distillation. As the strength of the correlations increases, we observe a widening gap between advanced methods, such as SubDistill, which remain fairly robust, and some baseline methods, which degrade to near-random performance. Overall, our study underscores the challenges of knowledge distillation when applied to imperfect, real-world datasets, particularly those with spurious correlations.", "AI": {"tldr": "\u8bc4\u4f30\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u5728\u542b\u865a\u5047\u76f8\u5173\u6027\u6570\u636e\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0SubDistill\u7b49\u5148\u8fdb\u65b9\u6cd5\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u9c81\u68d2\uff0c\u4f46\u6240\u6709\u65b9\u6cd5\u90fd\u9762\u4e34\u771f\u5b9e\u6570\u636e\u96c6\u7684\u6311\u6218", "motivation": "\u73b0\u6709\u77e5\u8bc6\u84b8\u998f\u4f9d\u8d56\u7684\u6570\u636e\u96c6\u5e38\u5b58\u5728\u89c4\u6a21\u6709\u9650\u3001\u7f3a\u4e4f\u4ee3\u8868\u6027\u6216\u5305\u542b\u865a\u5047\u76f8\u5173\u6027\u7b49\u95ee\u9898\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u7f3a\u9677\u5982\u4f55\u5f71\u54cd\u84b8\u998f\u6548\u679c\u7684\u7cfb\u7edf\u8bc4\u4f30", "method": "\u5728\u542b\u53ef\u63a7\u865a\u5047\u76f8\u5173\u6027\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u5bf9\u6bd4\u6d4b\u8bd5\u4f20\u7edf\u84b8\u998f\u65b9\u6cd5\u4e0eSubDistill\u7b49\u65b0\u65b9\u6cd5\u5728\u4e0d\u540c\u76f8\u5173\u6027\u5f3a\u5ea6\u4e0b\u7684\u6027\u80fd\u8868\u73b0", "result": "\u968f\u7740\u865a\u5047\u76f8\u5173\u6027\u589e\u5f3a\uff0cSubDistill\u7b49\u5148\u8fdb\u65b9\u6cd5\u4fdd\u6301\u76f8\u5bf9\u7a33\u5b9a\uff0c\u800c\u57fa\u7ebf\u65b9\u6cd5\u6027\u80fd\u6025\u5267\u4e0b\u964d\u81f3\u63a5\u8fd1\u968f\u673a\u6c34\u5e73\uff0c\u4e24\u8005\u5dee\u8ddd\u663e\u8457\u6269\u5927", "conclusion": "\u771f\u5b9e\u4e16\u754c\u4e2d\u542b\u865a\u5047\u76f8\u5173\u6027\u7684\u4e0d\u5b8c\u7f8e\u6570\u636e\u96c6\u5bf9\u77e5\u8bc6\u84b8\u998f\u6784\u6210\u91cd\u5927\u6311\u6218\uff0c\u4e9f\u9700\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u84b8\u998f\u65b9\u6cd5\u4ee5\u5e94\u5bf9\u6570\u636e\u7f3a\u9677"}}
{"id": "2602.02453", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02453", "abs": "https://arxiv.org/abs/2602.02453", "authors": ["Andong Chen", "Wenxin Zhu", "Qiuyu Ding", "Yuchen Song", "Muyun Yang", "Tiejun Zhao"], "title": "Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling", "comment": "Working paper", "summary": "Chain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos. However, different modalities still have clear limitations: static images struggle to represent temporal structure, while videos introduce substantial redundancy and computational cost. In this work, we propose Thinking with Comics, a visual reasoning paradigm that uses comics as a high information-density medium positioned between images and videos. Comics preserve temporal structure, embedded text, and narrative coherence while requiring significantly lower reasoning cost. We systematically study two reasoning paths based on comics and evaluate them on a range of reasoning tasks and long-context understanding tasks. Experimental results show that Thinking with Comics outperforms Thinking with Images on multi-step temporal and causal reasoning tasks, while remaining substantially more efficient than Thinking with Video. Further analysis indicates that different comic narrative structures and styles consistently affect performance across tasks, suggesting that comics serve as an effective intermediate visual representation for improving multimodal reasoning.", "AI": {"tldr": "\u63d0\u51fa\"\u7528\u6f2b\u753b\u601d\u8003\"\u7684\u89c6\u89c9\u63a8\u7406\u8303\u5f0f\uff0c\u4f5c\u4e3a\u56fe\u50cf\u4e0e\u89c6\u9891\u4e4b\u95f4\u7684\u4e2d\u95f4\u5a92\u4ecb\uff0c\u5728\u4fdd\u6301\u65f6\u5e8f\u7ed3\u6784\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u5728\u591a\u6b65\u65f6\u5e8f\u548c\u56e0\u679c\u63a8\u7406\u4efb\u52a1\u4e0a\u4f18\u4e8e\u56fe\u50cf\u63a8\u7406\uff0c\u4e14\u6bd4\u89c6\u9891\u63a8\u7406\u66f4\u9ad8\u6548\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u63a8\u7406\u6a21\u6001\u5b58\u5728\u660e\u663e\u5c40\u9650\uff1a\u9759\u6001\u56fe\u50cf\u96be\u4ee5\u8868\u793a\u65f6\u5e8f\u7ed3\u6784\uff0c\u800c\u89c6\u9891\u5b58\u5728\u5197\u4f59\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u9700\u8981\u4e00\u79cd\u4ecb\u4e8e\u4e24\u8005\u4e4b\u95f4\u7684\u9ad8\u4fe1\u606f\u5bc6\u5ea6\u5a92\u4ecb\u3002", "method": "\u63d0\u51faThinking with Comics\u8303\u5f0f\uff0c\u7cfb\u7edf\u7814\u7a76\u4e24\u6761\u57fa\u4e8e\u6f2b\u753b\u7684\u63a8\u7406\u8def\u5f84\uff0c\u5e76\u5728\u591a\u79cd\u63a8\u7406\u4efb\u52a1\u548c\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u591a\u9879\u65f6\u5e8f\u548c\u56e0\u679c\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cThinking with Comics\u8868\u73b0\u4f18\u4e8eThinking with Images\uff1b\u76f8\u6bd4Thinking with Video\uff0c\u8ba1\u7b97\u6548\u7387\u663e\u8457\u66f4\u9ad8\uff1b\u4e0d\u540c\u6f2b\u753b\u53d9\u4e8b\u7ed3\u6784\u548c\u98ce\u683c\u5bf9\u4efb\u52a1\u8868\u73b0\u6709\u6301\u7eed\u5f71\u54cd\u3002", "conclusion": "\u6f2b\u753b\u4f5c\u4e3a\u4e2d\u95f4\u89c6\u89c9\u8868\u793a\u80fd\u6709\u6548\u63d0\u5347\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u5728\u65f6\u5e8f\u4fe1\u606f\u4fdd\u7559\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2602.00862", "categories": ["cs.LG", "cs.AI", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.00862", "abs": "https://arxiv.org/abs/2602.00862", "authors": ["Shih-Hsin Wang", "Yuhao Huang", "Taos Transue", "Justin Baker", "Jonathan Forstater", "Thomas Strohmer", "Bao Wang"], "title": "Towards Multiscale Graph-based Protein Learning with Geometric Secondary Structural Motifs", "comment": "Published in NeurIPS 2025", "summary": "Graph neural networks (GNNs) have emerged as powerful tools for learning protein structures by capturing spatial relationships at the residue level. However, existing GNN-based methods often face challenges in learning multiscale representations and modeling long-range dependencies efficiently. In this work, we propose an efficient multiscale graph-based learning framework tailored to proteins. Our proposed framework contains two crucial components: (1) It constructs a hierarchical graph representation comprising a collection of fine-grained subgraphs, each corresponding to a secondary structure motif (e.g., $\u03b1$-helices, $\u03b2$-strands, loops), and a single coarse-grained graph that connects these motifs based on their spatial arrangement and relative orientation. (2) It employs two GNNs for feature learning: the first operates within individual secondary motifs to capture local interactions, and the second models higher-level structural relationships across motifs. Our modular framework allows a flexible choice of GNN in each stage. Theoretically, we show that our hierarchical framework preserves the desired maximal expressiveness, ensuring no loss of critical structural information. Empirically, we demonstrate that integrating baseline GNNs into our multiscale framework remarkably improves prediction accuracy and reduces computational cost across various benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c3a\u5ea6\u56fe\u5b66\u4e60\u6846\u67b6\u7528\u4e8e\u86cb\u767d\u8d28\u7ed3\u6784\u5efa\u6a21\uff0c\u901a\u8fc7\u6784\u5efa\u5c42\u6b21\u5316\u56fe\u8868\u793a\u548c\u4f7f\u7528\u4e24\u4e2aGNN\u5206\u522b\u5904\u7406\u5c40\u90e8\u548c\u5168\u5c40\u7ed3\u6784\u4fe1\u606f\uff0c\u5728\u4fdd\u6301\u6700\u5927\u8868\u8fbe\u80fd\u529b\u7684\u540c\u6642\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709GNN\u65b9\u6cd5\u5728\u5b66\u4e60\u86cb\u767d\u8d28\u591a\u5c3a\u5ea6\u8868\u793a\u548c\u9ad8\u6548\u5efa\u6a21\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u4ece\u6b8b\u57fa\u7ea7\u5230\u7ed3\u6784\u57df\u7ea7\u7684\u5c42\u6b21\u5316\u7ed3\u6784\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53cc\u5c42\u7ea7\u56fe\u5b66\u4e60\u6846\u67b6\uff1a1)\u6784\u5efa\u5305\u542b\u7ec6\u7c92\u5ea6\u4e8c\u7ea7\u7ed3\u6784\u6a21\u4f53\u5b50\u56fe\u548c\u7c97\u7c92\u5ea6\u6a21\u4f53\u8fde\u63a5\u56fe\u7684\u5c42\u6b21\u5316\u8868\u793a\uff1b2)\u4f7f\u7528\u4e24\u4e2aGNN\u5206\u522b\u5b66\u4e60\u6a21\u4f53\u5185\u90e8\u5c40\u90e8\u76f8\u4e92\u4f5c\u7528\u548c\u6a21\u4f53\u95f4\u9ad8\u7ea7\u7ed3\u6784\u5173\u7cfb\u3002", "result": "\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u6846\u67b6\u4fdd\u6301\u6700\u5927\u8868\u8fbe\u80fd\u529b\uff1b\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4e14\u652f\u6301\u7075\u6d3b\u9009\u62e9\u5404\u9636\u6bb5\u7684GNN\u6a21\u578b\u3002", "conclusion": "\u8be5\u591a\u5c3a\u5ea6\u5c42\u6b21\u5316\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u86cb\u767d\u8d28\u7ed3\u6784\u5b66\u4e60\u4e2d\u7684\u591a\u5c3a\u5ea6\u5efa\u6a21\u95ee\u9898\uff0c\u4e3a\u86cb\u767d\u8d28\u7ed3\u6784\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u8868\u8fbe\u80fd\u529b\u5b8c\u5907\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2602.02455", "categories": ["cs.AI", "cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.02455", "abs": "https://arxiv.org/abs/2602.02455", "authors": ["Han Bao", "Zheyuan Zhang", "Pengcheng Jing", "Zhengqing Yuan", "Kaiwen Shi", "Yanfang Ye"], "title": "Drift-Bench: Diagnosing Cooperative Breakdowns in LLM Agents under Input Faults via Multi-Turn Interaction", "comment": "65 pages, 40 figures", "summary": "As Large Language Models transition to autonomous agents, user inputs frequently violate cooperative assumptions (e.g., implicit intent, missing parameters, false presuppositions, or ambiguous expressions), creating execution risks that text-only evaluations do not capture. Existing benchmarks typically assume well-specified instructions or restrict evaluation to text-only, single-turn clarification, and thus do not measure multi-turn disambiguation under grounded execution risk. We introduce \\textbf{Drift-Bench}, the first diagnostic benchmark that evaluates agentic pragmatics under input faults through multi-turn clarification across state-oriented and service-oriented execution environments. Grounded in classical theories of communication, \\textbf{Drift-Bench} provides a unified taxonomy of cooperative breakdowns and employs a persona-driven user simulator with the \\textbf{Rise} evaluation protocol. Experiments show substantial performance drops under these faults, with clarification effectiveness varying across user personas and fault types. \\MethodName bridges clarification research and agent safety evaluation, enabling systematic diagnosis of failures that can lead to unsafe executions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDrift-Bench\uff0c\u9996\u4e2a\u8bca\u65ad\u81ea\u4e3b\u667a\u80fd\u4f53\u5728\u591a\u8f6e\u6f84\u6e05\u573a\u666f\u4e0b\u5904\u7406\u7528\u6237\u8f93\u5165\u9519\u8bef\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u73b0\u6709\u6587\u672c\u8bc4\u4f30\u65e0\u6cd5\u6355\u6349\u7684\u6267\u884c\u98ce\u9669\u3002", "motivation": "\u5927\u6a21\u578b\u8f6c\u5411\u81ea\u4e3b\u667a\u80fd\u4f53\u540e\uff0c\u7528\u6237\u8f93\u5165\u5e38\u8fdd\u53cd\u5408\u4f5c\u5047\u8bbe\uff08\u9690\u542b\u610f\u56fe\u3001\u7f3a\u5931\u53c2\u6570\u3001\u9519\u8bef\u9884\u8bbe\u3001\u6b67\u4e49\u8868\u8fbe\uff09\uff0c\u5bfc\u81f4\u6267\u884c\u98ce\u9669\u3002\u73b0\u6709\u57fa\u51c6\u5047\u8bbe\u6307\u4ee4\u660e\u786e\u6216\u4ec5\u9650\u6587\u672c\u5355\u8f6e\u6f84\u6e05\uff0c\u65e0\u6cd5\u8bc4\u4f30\u57fa\u4e8e\u5b9e\u9645\u6267\u884c\u98ce\u9669\u7684\u591a\u8f6e\u6d88\u6b67\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u7ecf\u5178\u6c9f\u901a\u7406\u8bba\u5efa\u7acb\u5408\u4f5c\u6545\u969c\u7edf\u4e00\u5206\u7c7b\u6cd5\uff0c\u91c7\u7528\u89d2\u8272\u9a71\u52a8\u7684\u7528\u6237\u6a21\u62df\u5668\u548cRise\u8bc4\u4f30\u534f\u8bae\uff0c\u5728\u72b6\u6001\u5bfc\u5411\u548c\u670d\u52a1\u5bfc\u5411\u7684\u6267\u884c\u73af\u5883\u4e2d\u8bc4\u4f30\u667a\u80fd\u4f53\u7684\u8bed\u7528\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5728\u8f93\u5165\u6545\u969c\u4e0b\u667a\u80fd\u4f53\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4e14\u6f84\u6e05\u6548\u679c\u56e0\u7528\u6237\u89d2\u8272\u548c\u6545\u969c\u7c7b\u578b\u800c\u5f02\u3002", "conclusion": "Drift-Bench\u8fde\u63a5\u4e86\u6f84\u6e05\u7814\u7a76\u4e0e\u667a\u80fd\u4f53\u5b89\u5168\u8bc4\u4f30\uff0c\u53ef\u7cfb\u7edf\u8bca\u65ad\u53ef\u80fd\u5bfc\u81f4\u4e0d\u5b89\u5168\u6267\u884c\u7684\u6545\u969c\uff0c\u4e3a\u63d0\u5347\u667a\u80fd\u4f53\u9c81\u68d2\u6027\u63d0\u4f9b\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2602.00869", "categories": ["cs.LG", "cs.AI", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.00869", "abs": "https://arxiv.org/abs/2602.00869", "authors": ["Yuhao Huang", "Taos Transue", "Shih-Hsin Wang", "William Feldman", "Hong Zhang", "Bao Wang"], "title": "Improving Flow Matching by Aligning Flow Divergence", "comment": "Published in ICML 2025", "summary": "Conditional flow matching (CFM) stands out as an efficient, simulation-free approach for training flow-based generative models, achieving remarkable performance for data generation. However, CFM is insufficient to ensure accuracy in learning probability paths. In this paper, we introduce a new partial differential equation characterization for the error between the learned and exact probability paths, along with its solution. We show that the total variation gap between the two probability paths is bounded above by a combination of the CFM loss and an associated divergence loss. This theoretical insight leads to the design of a new objective function that simultaneously matches the flow and its divergence. Our new approach improves the performance of the flow-based generative model by a noticeable margin without sacrificing generation efficiency. We showcase the advantages of this enhanced training approach over CFM on several important benchmark tasks, including generative modeling for dynamical systems, DNA sequences, and videos. Code is available at \\href{https://github.com/Utah-Math-Data-Science/Flow_Div_Matching}{Utah-Math-Data-Science}.", "AI": {"tldr": "\u9488\u5bf9CFM\u5728\u5b66\u4e60\u6982\u7387\u8def\u5f84\u65f6\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u65b0\u7684PDE\u8bef\u5dee\u523b\u753b\uff0c\u63a8\u5bfc\u603b\u53d8\u5dee\u4e0a\u754c\uff0c\u5e76\u8bbe\u8ba1\u540c\u65f6\u5339\u914d\u6d41\u548c\u6563\u5ea6\u7684\u65b0\u76ee\u6807\u51fd\u6570\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e0a\u63d0\u5347\u6027\u80fd\u4e14\u4e0d\u727a\u7272\u6548\u7387\u3002", "motivation": "\u6761\u4ef6\u6d41\u5339\u914d(CFM)\u867d\u7136\u9ad8\u6548\uff0c\u4f46\u4e0d\u8db3\u4ee5\u786e\u4fdd\u5b66\u4e60\u6982\u7387\u8def\u5f84\u7684\u51c6\u786e\u6027\uff0c\u5b58\u5728\u7cbe\u5ea6\u7f3a\u9677\u3002", "method": "\u63d0\u51fa\u65b0\u7684\u504f\u5fae\u5206\u65b9\u7a0b(PDE)\u523b\u753b\u5b66\u4e60\u6982\u7387\u8def\u5f84\u4e0e\u7cbe\u786e\u8def\u5f84\u7684\u8bef\u5dee\uff0c\u8bc1\u660e\u603b\u53d8\u5dee\u5dee\u8ddd\u53ef\u7531CFM\u635f\u5931\u548c\u6563\u5ea6\u635f\u5931\u4e0a\u754c\u63a7\u5236\uff0c\u8fdb\u800c\u8bbe\u8ba1\u540c\u65f6\u5339\u914d\u6d41\u53ca\u5176\u6563\u5ea6\u7684\u65b0\u76ee\u6807\u51fd\u6570\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u4fdd\u6301\u751f\u6210\u6548\u7387\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d41\u751f\u6210\u6a21\u578b\u6027\u80fd\uff0c\u5728\u52a8\u529b\u7cfb\u7edf\u3001DNA\u5e8f\u5217\u548c\u89c6\u9891\u751f\u6210\u7b49\u57fa\u51c6\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u901a\u8fc7\u540c\u65f6\u4f18\u5316\u6d41\u5339\u914d\u548c\u6563\u5ea6\u5339\u914d\uff0c\u8be5\u589e\u5f3a\u8bad\u7ec3\u65b9\u6cd5\u6709\u6548\u6539\u5584\u4e86\u6982\u7387\u8def\u5f84\u5b66\u4e60\u7cbe\u5ea6\uff0c\u4e3a\u8bad\u7ec3\u66f4\u51c6\u786e\u7684\u6d41\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848\u3002"}}
{"id": "2602.02465", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02465", "abs": "https://arxiv.org/abs/2602.02465", "authors": ["Jana Zeller", "Thadd\u00e4us Wiedemer", "Fanfei Li", "Thomas Klein", "Prasanna Mayilvahanan", "Matthias Bethge", "Felix Wichmann", "Ryan Cotterell", "Wieland Brendel"], "title": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery", "comment": "9 pages, 8 figures", "summary": "Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.", "AI": {"tldr": "This paper investigates whether unified multimodal models can use intermediate visualizations to aid reasoning, finding that current models fail to benefit from visual thoughts despite their appeal, and introduces MentisOculi benchmark to address this gap.", "motivation": "The transition from multimodal large language models to unified multimodal models capable of native interleaved generation has sparked interest in using intermediate visualizations as reasoning aids, similar to human mental imagery. The core question is whether models can form, maintain, and manipulate visual representations in a goal-oriented manner to improve reasoning.", "method": "The authors developed MentisOculi, a procedural, stratified suite of multi-step reasoning problems designed to be amenable to visual solutions and tuned to challenge frontier models. They evaluated various visual strategies ranging from latent tokens to explicitly generated imagery across these models.", "result": "Visual strategies generally failed to improve performance. Analysis revealed that while UMMs possess textual reasoning capacity and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to effectively leverage even ground-truth visualizations provided to them.", "conclusion": "Despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this performance gap across diverse model families."}}
{"id": "2602.00872", "categories": ["cs.LG", "math-ph"], "pdf": "https://arxiv.org/pdf/2602.00872", "abs": "https://arxiv.org/abs/2602.00872", "authors": ["Shihao Wang", "Qipeng Qian", "Jingquan Wang"], "title": "Learning Heat-based Equations in Self-similar variables", "comment": null, "summary": "We study solution learning for heat-based equations in self-similar variables (SSV). We develop an SSV training framework compatible with standard neural-operator training. We instantiate this framework on the two-dimensional incompressible Navier-Stokes equations and the one-dimensional viscous Burgers equation, and perform controlled comparisons between models trained in physical coordinates and in the corresponding self-similar coordinates using two simple fully connected architectures (standard multilayer perceptrons and a factorized fully connected network). Across both systems and both architectures, SSV-trained networks consistently deliver substantially more accurate and stable extrapolation beyond the training window and better capture qualitative long-time trends. These results suggest that self-similar coordinates provide a mathematically motivated inductive bias for learning the long-time dynamics of heat-based equations.", "AI": {"tldr": "This paper develops a self-similar variable (SSV) training framework for neural operators solving heat-based PDEs. Testing on Navier-Stokes and Burgers equations shows SSV-trained models significantly outperform physical coordinate models in long-time extrapolation and trend capture, suggesting SSV provides a strong inductive bias for learning long-time dynamics.", "motivation": "Learning solutions to PDEs like Navier-Stokes and Burgers equations using neural operators faces challenges in capturing long-time dynamics and extrapolating beyond training windows. The authors hypothesize that self-similar variables, which naturally capture scaling behavior of heat-based equations, could provide a better coordinate system for learning these dynamics compared to standard physical coordinates.", "method": "The authors develop a general SSV training framework compatible with standard neural-operator training pipelines. They instantiate this framework on two test cases: (1) two-dimensional incompressible Navier-Stokes equations and (2) one-dimensional viscous Burgers equation. They conduct controlled experiments comparing models trained in physical coordinates versus self-similar coordinates using two simple fully connected architectures: standard multilayer perceptrons and a factorized fully connected network.", "result": "Across both PDE systems and both neural architectures, networks trained in self-similar coordinates consistently demonstrate: (1) substantially more accurate and stable extrapolation beyond the training time window, and (2) better capture of qualitative long-time trends. This performance advantage is observed uniformly across all experimental conditions.", "conclusion": "Self-similar coordinates offer a mathematically principled inductive bias that significantly enhances neural operators' ability to learn the long-time dynamics of heat-based equations. The consistent performance improvement suggests this coordinate transformation is a valuable tool for PDE learning tasks requiring long-time prediction."}}
{"id": "2602.02468", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02468", "abs": "https://arxiv.org/abs/2602.02468", "authors": ["Aiden Yiliu Li", "Xinyue Hao", "Shilong Liu", "Mengdi Wang"], "title": "Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts", "comment": null, "summary": "Despite advances in multimodal large language models, autonomous web agents still struggle to reliably execute long-horizon tasks on complex and dynamic web interfaces. Existing agents often suffer from inaccurate element grounding, the absence of site-specific procedural knowledge, and unstable long-term task tracking and memory, particularly when operating over complex Document Object Model structures. To address these limitations, we introduce Avenir-Web, a web agent that achieves a new open-source state of the art on the Online-Mind2Web benchmark in real-world deployment. Avenir-Web leverages a Mixture of Grounding Experts, Experience-Imitation Planning for incorporating procedural priors, and a task-tracking checklist combined with adaptive memory to enable robust and seamless interaction across diverse user interface paradigms. We evaluate Avenir-Web on Online-Mind2Web, a rigorous benchmark of live and user-centered web tasks. Our results demonstrate that Avenir-Web significantly surpasses prior open-source agents and attains performance parity with top-tier proprietary models, thereby establishing a new open-source state of the art for reliable web agents on live websites.", "AI": {"tldr": "Avenir-Web introduces a novel web agent architecture combining specialized grounding experts, procedural knowledge integration, and adaptive memory to achieve open-source state-of-the-art performance on live web tasks.", "motivation": "Current autonomous web agents face critical limitations in executing complex, long-horizon tasks due to inaccurate element grounding, lack of site-specific procedural knowledge, and unstable long-term memory tracking when interacting with dynamic web interfaces.", "method": "Proposes three key innovations: 1) Mixture of Grounding Experts for precise UI element identification, 2) Experience-Imitation Planning to incorporate procedural priors from historical data, and 3) Task-tracking checklist with adaptive memory for stable long-term operation across diverse web paradigms.", "result": "Achieves new open-source state-of-the-art performance on the rigorous Online-Mind2Web benchmark, significantly outperforming prior open-source agents and matching top proprietary models in real-world web task execution.", "conclusion": "Avenir-Web establishes a new standard for reliable open-source web agents by effectively addressing core challenges in grounding, procedural knowledge, and memory management, demonstrating parity with commercial solutions on live website interactions."}}
{"id": "2602.00879", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00879", "abs": "https://arxiv.org/abs/2602.00879", "authors": ["Hao Mark Chen", "Zhiwen Mo", "Royson Lee", "Qianzhou Wang", "Da Li", "Shell Xu Hu", "Wayne Luk", "Timothy Hospedales", "Hongxiang Fan"], "title": "Dynamic Expert Sharing: Decoupling Memory from Parallelism in Mixture-of-Experts Diffusion LLMs", "comment": null, "summary": "Among parallel decoding paradigms, diffusion large language models (dLLMs) have emerged as a promising candidate that balances generation quality and throughput. However, their integration with Mixture-of-Experts (MoE) architectures is constrained by an expert explosion: as the number of tokens generated in parallel increases, the number of distinct experts activated grows nearly linearly. This results in substantial memory traffic that pushes inference into a memory-bound regime, negating the efficiency gains of both MoE and parallel decoding. To address this challenge, we propose Dynamic Expert Sharing (DES), a novel technique that shifts MoE optimization from token-centric pruning and conventional expert skipping methods to sequence-level coreset selection. To maximize expert reuse, DES identifies a compact, high-utility set of experts to satisfy the requirements of an entire parallel decoding block. We introduce two innovative selection strategies: (1) Intra-Sequence Sharing (DES-Seq), which adapts optimal allocation to the sequence level, and (2) Saliency-Aware Voting (DES-Vote), a novel mechanism that allows tokens to collectively elect a coreset based on aggregated router weights. Extensive experiments on MoE dLLMs demonstrate that DES reduces unique expert activations by over 55% and latency by up to 38%, while retaining 99% of vanilla accuracy, effectively decoupling memory overhead from the degree of parallelism.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u52a8\u6001\u4e13\u5bb6\u5171\u4eab(DES)\u89e3\u51b3MoE\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b(dLLMs)\u5e76\u884c\u89e3\u7801\u4e2d\u7684\u4e13\u5bb6\u7206\u70b8\u95ee\u9898\uff0c\u901a\u8fc7\u5e8f\u5217\u7ea7\u6838\u5fc3\u96c6\u9009\u62e9\u5c06\u72ec\u7279\u4e13\u5bb6\u6fc0\u6d3b\u51cf\u5c1155%\u4ee5\u4e0a\u3001\u5ef6\u8fdf\u964d\u4f4e38%\uff0c\u540c\u65f6\u4fdd\u630199%\u7684\u539f\u59cb\u51c6\u786e\u7387\u3002", "motivation": "\u5728MoE\u4e0e\u5e76\u884c\u89e3\u7801\u96c6\u6210\u65f6\uff0c\u968f\u7740\u5e76\u884c\u751f\u6210token\u589e\u52a0\uff0c\u6fc0\u6d3b\u7684\u4e0d\u540c\u4e13\u5bb6\u6570\u91cf\u51e0\u4e4e\u7ebf\u6027\u589e\u957f\uff0c\u5bfc\u81f4\u5de8\u5927\u5185\u5b58\u6d41\u91cf\u4f7f\u63a8\u7406\u8fdb\u5165\u5185\u5b58\u9650\u5236\u72b6\u6001\uff0c\u62b5\u6d88\u4e86MoE\u548c\u5e76\u884c\u89e3\u7801\u7684\u6548\u7387\u589e\u76ca\u3002", "method": "DES\u5c06\u4f18\u5316\u4ecetoken\u7ea7\u526a\u679d\u8f6c\u5411\u5e8f\u5217\u7ea7\u6838\u5fc3\u96c6\u9009\u62e9\uff0c\u63d0\u51fa\u4e24\u79cd\u7b56\u7565\uff1aDES-Seq\uff08\u5e8f\u5217\u5185\u5171\u4eab\uff09\u9002\u5e94\u5e8f\u5217\u7ea7\u6700\u4f18\u5206\u914d\uff0cDES-Vote\uff08\u6295\u7968\u673a\u5236\uff09\u8ba9token\u57fa\u4e8e\u805a\u5408\u8def\u7531\u6743\u91cd\u96c6\u4f53\u9009\u4e3e\u6838\u5fc3\u4e13\u5bb6\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDES\u4f7f\u72ec\u7279\u4e13\u5bb6\u6fc0\u6d3b\u51cf\u5c11\u8d85\u8fc755%\uff0c\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe38%\uff0c\u4fdd\u630199%\u7684\u57fa\u51c6\u51c6\u786e\u7387\uff0c\u6709\u6548\u89e3\u8026\u5185\u5b58\u5f00\u9500\u4e0e\u5e76\u884c\u5ea6\u3002", "conclusion": "DES\u901a\u8fc7\u5e8f\u5217\u7ea7\u6700\u5927\u5316\u4e13\u5bb6\u91cd\u7528\uff0c\u6709\u6548\u89e3\u51b3\u4e86MoE\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u4e13\u5bb6\u7206\u70b8\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5e76\u884c\u89e3\u7801\u3002"}}
{"id": "2602.02470", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02470", "abs": "https://arxiv.org/abs/2602.02470", "authors": ["Xutao Ma", "Yixiao Huang", "Hanlin Zhu", "Somayeh Sojoudi"], "title": "Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge", "comment": null, "summary": "Autoregressive large language models (LLMs) have achieved remarkable success in many complex tasks, yet they can still fail in very simple logical reasoning such as the \"reversal curse\" -- when trained on forward knowledge data of the form \"$A \\rightarrow B$\" (e.g., Alice's husband is Bob), the model is unable to deduce the reversal knowledge \"$B \\leftarrow A$\" (e.g., Bob's wife is Alice) during test. Extensive prior research suggests that this failure is an inherent, fundamental limit of autoregressive causal LLMs, indicating that these models tend to memorize factual-level knowledge rather than capture higher-level rules. In this paper, we challenge this view by showing that this seemingly fundamental limit can be mitigated by slightly tweaking the training data with a simple regularization data recipe called the Identity Bridge of the form \"$A \\to A$\" (e.g., The name of Alice is Alice). Theoretically, we prove that under this recipe, even a one-layer transformer can break the reversal curse by analyzing the implicit bias of gradient descent. Empirically, we show that a 1B pretrained language model finetuned with the proposed data recipe achieves a 40% success rate on reversal tasks, in stark contrast to a near-zero success rate when trained solely on forward-knowledge data. Our work provides a novel theoretical foundation for the reversal curse and offers a principled, low-cost path to encouraging LLMs to learn higher-level rules from data.", "AI": {"tldr": "This paper challenges the view that autoregressive LLMs fundamentally cannot reverse logical relationships (reversal curse). By adding simple \"identity bridge\" training examples (A\u2192A), the authors show both theoretically and empirically that models can learn to reverse relationships, achieving 40% success vs near-zero baseline.", "motivation": "Autoregressive LLMs fail at simple logical reasoning tasks like the \"reversal curse\" - they cannot deduce reverse knowledge (B\u2190A) from forward knowledge (A\u2192B). Prior research considered this an inherent limitation, suggesting models only memorize facts rather than learn rules. The motivation is to challenge this fundamental assumption and find a simple way to enable rule learning.", "method": "The authors propose a simple regularization technique called \"Identity Bridge\" - adding training examples of the form \"A\u2192A\" (e.g., \"The name of Alice is Alice\") to the forward knowledge data. They analyze this theoretically using gradient descent implicit bias on a one-layer transformer.", "result": "A 1B parameter pretrained language model finetuned with the identity bridge recipe achieves a 40% success rate on reversal tasks, dramatically higher than the near-zero success rate of models trained only on forward-knowledge data. Theoretically, they prove even a one-layer transformer can break the reversal curse with this approach.", "conclusion": "The reversal curse is not a fundamental limit of autoregressive LLMs but can be mitigated with simple data modifications. The identity bridge recipe provides a principled, low-cost method to encourage LLMs to learn higher-level rules rather than just memorizing facts, opening new directions for improving logical reasoning in LLMs."}}
{"id": "2602.00884", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00884", "abs": "https://arxiv.org/abs/2602.00884", "authors": ["Louis Serrano", "Jiequn Han", "Edouard Oyallon", "Shirley Ho", "Rudy Morel"], "title": "Test-time Generalization for Physics through Neural Operator Splitting", "comment": null, "summary": "Neural operators have shown promise in learning solution maps of partial differential equations (PDEs), but they often struggle to generalize when test inputs lie outside the training distribution, such as novel initial conditions, unseen PDE coefficients or unseen physics. Prior works address this limitation with large-scale multiple physics pretraining followed by fine-tuning, but this still requires examples from the new dynamics, falling short of true zero-shot generalization. In this work, we propose a method to enhance generalization at test time, i.e., without modifying pretrained weights. Building on DISCO, which provides a dictionary of neural operators trained across different dynamics, we introduce a neural operator splitting strategy that, at test time, searches over compositions of training operators to approximate unseen dynamics. On challenging out-of-distribution tasks including parameter extrapolation and novel combinations of physics phenomena, our approach achieves state-of-the-art zero-shot generalization results, while being able to recover the underlying PDE parameters. These results underscore test-time computation as a key avenue for building flexible, compositional, and generalizable neural operators.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6d4b\u8bd5\u65f6\u589e\u5f3a\u795e\u7ecf\u7b97\u5b50\u96f6\u6837\u672c\u6cdb\u5316\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ec4\u5408\u9884\u8bad\u7ec3\u7b97\u5b50\u5b57\u5178\u4e2d\u7684\u7b97\u5b50\u6765\u903c\u8fd1\u672a\u89c1\u52a8\u529b\u5b66\uff0c\u65e0\u9700\u4fee\u6539\u9884\u8bad\u7ec3\u6743\u91cd\u3002", "motivation": "\u795e\u7ecf\u7b97\u5b50\u5728\u5904\u7406\u5206\u5e03\u5916PDE\u95ee\u9898\uff08\u5982\u65b0\u521d\u59cb\u6761\u4ef6\u3001\u672a\u89c1\u7cfb\u6570\u6216\u7269\u7406\u89c4\u5f8b\uff09\u65f6\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u5728\u65b0\u52a8\u529b\u5b66\u6570\u636e\u4e0a\u5fae\u8c03\uff0c\u65e0\u6cd5\u5b9e\u73b0\u771f\u6b63\u7684\u96f6\u6837\u672c\u6cdb\u5316\u3002", "method": "\u57fa\u4e8eDISCO\u6846\u67b6\uff0c\u63d0\u51fa\u795e\u7ecf\u7b97\u5b50\u62c6\u5206\u7b56\u7565\uff0c\u5728\u6d4b\u8bd5\u65f6\u641c\u7d22\u8bad\u7ec3\u7b97\u5b50\u7684\u7ec4\u5408\u6765\u903c\u8fd1\u672a\u89c1\u52a8\u529b\u5b66\u3002", "result": "\u5728\u53c2\u6570\u5916\u63a8\u548c\u65b0\u7269\u7406\u73b0\u8c61\u7ec4\u5408\u7b49\u5206\u5e03\u5916\u4efb\u52a1\u4e0a\u5b9e\u73b0\u6700\u4f18\u96f6\u6837\u672c\u6cdb\u5316\uff0c\u5e76\u80fd\u6062\u590d\u5e95\u5c42PDE\u53c2\u6570\u3002", "conclusion": "\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u662f\u6784\u5efa\u7075\u6d3b\u3001\u53ef\u7ec4\u5408\u4e14\u6cdb\u5316\u6027\u5f3a\u7684\u795e\u7ecf\u7b97\u5b50\u7684\u91cd\u8981\u9014\u5f84\u3002"}}
{"id": "2602.02475", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02475", "abs": "https://arxiv.org/abs/2602.02475", "authors": ["Shraddha Barke", "Arnav Goyal", "Alind Khare", "Avaljot Singh", "Suman Nath", "Chetan Bansal"], "title": "AgentRx: Diagnosing AI Agent Failures from Execution Trajectories", "comment": null, "summary": "AI agents often fail in ways that are difficult to localize because executions are probabilistic, long-horizon, multi-agent, and mediated by noisy tool outputs. We address this gap by manually annotating failed agent runs and release a novel benchmark of 115 failed trajectories spanning structured API workflows, incident management, and open-ended web/file tasks. Each trajectory is annotated with a critical failure step and a category from a grounded-theory derived, cross-domain failure taxonomy. To mitigate the human cost of failure attribution, we present AGENTRX, an automated domain-agnostic diagnostic framework that pinpoints the critical failure step in a failed agent trajectory. It synthesizes constraints, evaluates them step-by-step, and produces an auditable validation log of constraint violations with associated evidence; an LLM-based judge uses this log to localize the critical step and category. Our framework improves step localization and failure attribution over existing baselines across three domains.", "AI": {"tldr": "\u63d0\u51faAGENTRX\u81ea\u52a8\u5316\u8bca\u65ad\u6846\u67b6\u89e3\u51b3AI\u667a\u80fd\u4f53\u957f\u5468\u671f\u591a\u667a\u80fd\u4f53\u4efb\u52a1\u5931\u8d25\u5b9a\u4f4d\u96be\u9898\uff0c\u901a\u8fc7\u7ea6\u675f\u9a8c\u8bc1\u4e0eLLM\u6cd5\u5b98\u5b9e\u73b0\u8de8\u9886\u57df\u6545\u969c\u5f52\u56e0\uff0c\u5728115\u6761\u6807\u6ce8\u8f68\u8ff9\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf", "motivation": "AI\u667a\u80fd\u4f53\u5728\u6982\u7387\u6027\u3001\u957f\u5468\u671f\u3001\u591a\u667a\u80fd\u4f53\u53ca\u542b\u566a\u58f0\u5de5\u5177\u8c03\u7528\u7684\u6267\u884c\u73af\u5883\u4e2d\u5931\u8d25\u96be\u4ee5\u5b9a\u4f4d\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u6709\u6548\u5f52\u56e0\u673a\u5236\u4e14\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\u6602", "method": "1) \u4eba\u5de5\u6807\u6ce8115\u6761\u8de8\u9886\u57df(API\u5de5\u4f5c\u6d41/\u4e8b\u4ef6\u7ba1\u7406/\u7f51\u9875\u6587\u4ef6\u4efb\u52a1)\u5931\u8d25\u8f68\u8ff9\uff0c\u6784\u5efa\u542b\u5173\u952e\u6545\u969c\u6b65\u9aa4\u4e0e\u5206\u7c7b\u7684\u57fa\u51c6\uff1b2) \u63d0\u51faAGENTRX\u6846\u67b6\uff1a\u901a\u8fc7\u7ea6\u675f\u5408\u6210\u3001\u9010\u6b65\u9a8c\u8bc1\u751f\u6210\u53ef\u5ba1\u8ba1\u7684\u8fdd\u89c4\u65e5\u5fd7\uff0c\u7531LLM\u6cd5\u5b98\u5b9a\u4f4d\u5173\u952e\u6545\u969c\u6b65\u9aa4\u4e0e\u7c7b\u522b", "result": "\u5728\u4e09\u4e2a\u9886\u57df\u5747\u663e\u8457\u63d0\u5347\u6545\u969c\u6b65\u9aa4\u5b9a\u4f4d\u7cbe\u5ea6\u4e0e\u5f52\u56e0\u51c6\u786e\u6027\uff0c\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "AGENTRX\u63d0\u4f9b\u9886\u57df\u65e0\u5173\u7684\u81ea\u52a8\u5316\u8bca\u65ad\u65b9\u6848\uff0c\u6709\u6548\u964d\u4f4e\u4eba\u5de5\u5f52\u56e0\u6210\u672c\uff0c\u4e3a\u590d\u6742\u667a\u80fd\u4f53\u7cfb\u7edf\u6545\u969c\u5206\u6790\u63d0\u4f9b\u53ef\u5ba1\u8ba1\u7684\u6807\u51c6\u5316\u6846\u67b6"}}
{"id": "2602.00885", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00885", "abs": "https://arxiv.org/abs/2602.00885", "authors": ["Ahmad Sarlak", "Abolfazl Razi"], "title": "Reliability-Aware Determinantal Point Processes for Robust Informative Data Selection in Large Language Models", "comment": null, "summary": "Informative data selection is a key requirement for large language models (LLMs) to minimize the amount of data required for fine-tuning, network distillation, and token pruning, enabling fast and efficient deployment, especially under computational and communication constraints. Traditional subset selection methods, including those based on Determinantal Point Processes (DPP), focus on maximizing diversity but assume that selected data batches are always available error-free. This presumption prohibits their use under partial storage outage, imperfect communication, and stochastic access failures. Furthermore, we show that the original formulation collapses under such conditions. To address this gap, we introduce ProbDPP, a novel reliability-aware implementation of k-DPP that accounts for probabilistic data access by recasting the objective function with a regularization term that remains well-posed and decomposes into a geometric diversity term and unreliability cost. The resulting objective facilitates robust selection of diverse data batches under uncertainty. Furthermore, we frame this reliability-aware diversity maximization as a combinatorial semi-bandit problem and propose a UCB-style algorithm to efficiently learn the unknown reliability online. Theoretical analysis provides regret bounds for the proposed approach, ensuring performance guarantees.", "AI": {"tldr": "\u63d0\u51faProbDPP\uff0c\u4e00\u79cd\u53ef\u9760\u6027\u611f\u77e5\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\uff0c\u5c06\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u591a\u6837\u6027\u9009\u62e9\u5efa\u6a21\u4e3a\u7ec4\u5408\u8001\u864e\u673a\u95ee\u9898\uff0c\u7528UCB\u7b97\u6cd5\u5728\u7ebf\u5b66\u4e60\u53ef\u9760\u6027\uff0c\u5e76\u63d0\u4f9b\u7406\u8bba\u9057\u61be\u754c\u4fdd\u8bc1\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9700\u8981\u9ad8\u6548\u7684\u6570\u636e\u9009\u62e9\u6765\u6700\u5c0f\u5316\u5fae\u8c03\u3001\u84b8\u998f\u548c\u4ee4\u724c\u4fee\u526a\u7684\u6570\u636e\u91cf\uff0c\u4f46\u4f20\u7edf\u57fa\u4e8eDPP\u7684\u65b9\u6cd5\u5047\u8bbe\u6570\u636e\u5b8c\u7f8e\u53ef\u7528\uff0c\u65e0\u6cd5\u5e94\u5bf9\u5b58\u50a8\u4e2d\u65ad\u3001\u901a\u4fe1\u6545\u969c\u548c\u968f\u673a\u8bbf\u95ee\u5931\u8d25\u7b49\u73b0\u5b9e\u95ee\u9898\u3002", "method": "\u5f15\u5165ProbDPP\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u9879\u91cd\u6784k-DPP\u76ee\u6807\u51fd\u6570\u4ee5\u7eb3\u5165\u6982\u7387\u6027\u8bbf\u95ee\u7ea6\u675f\uff0c\u5c06\u5176\u5206\u89e3\u4e3a\u51e0\u4f55\u591a\u6837\u6027\u9879\u548c\u4e0d\u53ef\u9760\u6027\u6210\u672c\uff1b\u8fdb\u4e00\u6b65\u5efa\u6a21\u4e3a\u7ec4\u5408\u534a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u63d0\u51faUCB\u98ce\u683c\u7b97\u6cd5\u5728\u7ebf\u5b66\u4e60\u672a\u77e5\u53ef\u9760\u6027\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u9057\u61be\u754c\uff0c\u63d0\u4f9b\u4e86\u53ef\u8bc1\u660e\u7684\u6027\u80fd\u4fdd\u8bc1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u9c81\u68d2\u591a\u6837\u6027\u6570\u636e\u9009\u62e9\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u7b97\u6cd5\u3002"}}
{"id": "2211.11434", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2211.11434", "abs": "https://arxiv.org/abs/2211.11434", "authors": ["Lucas Lange", "Maja Schneider", "Peter Christen", "Erhard Rahm"], "title": "Privacy in Practice: Private COVID-19 Detection in X-Ray Images (Extended Version)", "comment": "Extended version of the paper accepted at the 20th International Conference on Security and Cryptography SECRYPT 2023. This version is more detailed and includes additional content: a longer results chapter and an appendix containing a proof", "summary": "Machine learning (ML) can help fight pandemics like COVID-19 by enabling rapid screening of large volumes of images. To perform data analysis while maintaining patient privacy, we create ML models that satisfy Differential Privacy (DP). Previous works exploring private COVID-19 models are in part based on small datasets, provide weaker or unclear privacy guarantees, and do not investigate practical privacy. We suggest improvements to address these open gaps. We account for inherent class imbalances and evaluate the utility-privacy trade-off more extensively and over stricter privacy budgets. Our evaluation is supported by empirically estimating practical privacy through black-box Membership Inference Attacks (MIAs). The introduced DP should help limit leakage threats posed by MIAs, and our practical analysis is the first to test this hypothesis on the COVID-19 classification task. Our results indicate that needed privacy levels might differ based on the task-dependent practical threat from MIAs. The results further suggest that with increasing DP guarantees, empirical privacy leakage only improves marginally, and DP therefore appears to have a limited impact on practical MIA defense. Our findings identify possibilities for better utility-privacy trade-offs, and we believe that empirical attack-specific privacy estimation can play a vital role in tuning for practical privacy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6539\u8fdb\u4e86\u7528\u4e8eCOVID-19\u7b5b\u67e5\u7684\u5dee\u5206\u9690\u79c1\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3001\u91c7\u7528\u66f4\u4e25\u683c\u7684\u9690\u79c1\u9884\u7b97\uff0c\u5e76\u7ed3\u5408\u6210\u5458\u63a8\u65ad\u653b\u51fb(MIAs)\u5b9e\u8bc1\u5206\u6790\u5b9e\u9645\u9690\u79c1\u4fdd\u62a4\u6548\u679c\uff0c\u53d1\u73b0\u5dee\u5206\u9690\u79c1\u5bf9\u62b5\u5fa1MIAs\u7684\u6548\u679c\u6709\u9650\uff0c\u5f3a\u8c03\u9700\u8981\u57fa\u4e8e\u5177\u4f53\u653b\u51fb\u7684\u5b9e\u8bc1\u9690\u79c1\u4f30\u8ba1\u6765\u4f18\u5316\u6548\u7528-\u9690\u79c1\u6743\u8861\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u53ef\u901a\u8fc7\u5feb\u901f\u7b5b\u67e5\u533b\u5b66\u5f71\u50cf\u8f85\u52a9\u75ab\u60c5\u9632\u63a7\uff0c\u4f46\u9700\u4fdd\u62a4\u60a3\u8005\u9690\u79c1\u3002\u73b0\u6709\u7814\u7a76\u5b58\u5728\u6570\u636e\u96c6\u5c0f\u3001\u9690\u79c1\u4fdd\u969c\u5f31\u6216\u4e0d\u660e\u786e\u3001\u7f3a\u4e4f\u5b9e\u9645\u9690\u79c1\u8bc4\u4f30\u7b49\u95ee\u9898\u3002", "method": "\u6784\u5efa\u6ee1\u8db3\u5dee\u5206\u9690\u79c1\u7684ML\u6a21\u578b\uff0c\u8003\u8651\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u8bc4\u4f30\u66f4\u4e25\u683c\u7684\u9690\u79c1\u9884\u7b97\u4e0b\u7684\u6548\u7528-\u9690\u79c1\u6743\u8861\uff0c\u901a\u8fc7\u9ed1\u76d2\u6210\u5458\u63a8\u65ad\u653b\u51fb(MIAs)\u5b9e\u8bc1\u4f30\u8ba1\u5b9e\u9645\u9690\u79c1\uff0c\u5e76\u68c0\u9a8cDP\u9650\u5236MIA\u5a01\u80c1\u7684\u5047\u8bbe\u3002", "result": "\u6240\u9700\u9690\u79c1\u7ea7\u522b\u56e0\u4efb\u52a1\u76f8\u5173\u7684MIA\u5b9e\u9645\u5a01\u80c1\u800c\u5f02\uff1b\u589e\u5f3aDP\u4fdd\u969c\u4ec5\u80fd\u8fb9\u9645\u6539\u5584\u5b9e\u8bc1\u9690\u79c1\u6cc4\u9732\uff1bDP\u5bf9\u62b5\u5fa1\u5b9e\u9645MIAs\u7684\u9632\u5fa1\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u4f18\u5316\u6548\u7528-\u9690\u79c1\u6743\u8861\u7684\u53ef\u80fd\u6027\uff0c\u8ba4\u4e3a\u57fa\u4e8e\u5177\u4f53\u653b\u51fb\u7684\u5b9e\u8bc1\u9690\u79c1\u4f30\u8ba1\u5728\u8c03\u4f18\u5b9e\u9645\u9690\u79c1\u4fdd\u62a4\u4e2d\u53ef\u53d1\u6325\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2409.01329", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV", "cs.DB"], "pdf": "https://arxiv.org/pdf/2409.01329", "abs": "https://arxiv.org/abs/2409.01329", "authors": ["Lucas Lange", "Maurice-Maximilian Heykeroth", "Erhard Rahm"], "title": "Assessing the Impact of Image Dataset Features on Privacy-Preserving Machine Learning", "comment": "Accepted at 21st Conference on Database Systems for Business, Technology and Web (BTW 2025)", "summary": "Machine Learning (ML) is crucial in many sectors, including computer vision. However, ML models trained on sensitive data face security challenges, as they can be attacked and leak information. Privacy-Preserving Machine Learning (PPML) addresses this by using Differential Privacy (DP) to balance utility and privacy. This study identifies image dataset characteristics that affect the utility and vulnerability of private and non-private Convolutional Neural Network (CNN) models. Through analyzing multiple datasets and privacy budgets, we find that imbalanced datasets increase vulnerability in minority classes, but DP mitigates this issue. Datasets with fewer classes improve both model utility and privacy, while high entropy or low Fisher Discriminant Ratio (FDR) datasets deteriorate the utility-privacy trade-off. These insights offer valuable guidance for practitioners and researchers in estimating and optimizing the utility-privacy trade-off in image datasets, helping to inform data and privacy modifications for better outcomes based on dataset characteristics.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7a76\u56fe\u50cf\u6570\u636e\u96c6\u7279\u6027\u5982\u4f55\u5f71\u54cdCNN\u6a21\u578b\u5728\u5dee\u5206\u9690\u79c1\u4fdd\u62a4\u4e0b\u7684\u6548\u7528-\u9690\u79c1\u6743\u8861\u3002\u901a\u8fc7\u591a\u6570\u636e\u96c6\u5206\u6790\u53d1\u73b0\uff1a\u6570\u636e\u4e0d\u5e73\u8861\u52a0\u5267\u5c11\u6570\u7c7b\u8106\u5f31\u6027\u4f46DP\u53ef\u7f13\u89e3\uff1b\u5c11\u7c7b\u522b\u6570\u636e\u96c6\u63d0\u5347\u6548\u7528\u4e0e\u9690\u79c1\uff1b\u9ad8\u71b5\u6216\u4f4eFDR\u6570\u636e\u96c6\u6076\u5316\u6743\u8861\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u7b49\u9886\u57df\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u654f\u611f\u6570\u636e\u8bad\u7ec3\u5bfc\u81f4\u6a21\u578b\u9762\u4e34\u653b\u51fb\u548c\u4fe1\u606f\u6cc4\u9732\u98ce\u9669\u3002\u9690\u79c1\u4fdd\u62a4\u673a\u5668\u5b66\u4e60\u91c7\u7528\u5dee\u5206\u9690\u79c1\u5e73\u8861\u6548\u7528\u4e0e\u9690\u79c1\uff0c\u7136\u800c\u56fe\u50cf\u6570\u636e\u96c6\u7279\u6027\u5bf9\u8fd9\u79cd\u6743\u8861\u7684\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u5206\u6790\u591a\u4e2a\u56fe\u50cf\u6570\u636e\u96c6\u548c\u4e0d\u540c\u9690\u79c1\u9884\u7b97\uff0c\u7cfb\u7edf\u7814\u7a76\u6570\u636e\u96c6\u7279\u6027\u5bf9\u79c1\u6709\u548c\u975e\u79c1\u6709CNN\u6a21\u578b\u6548\u7528\u4e0e\u8106\u5f31\u6027\u7684\u5f71\u54cd\u3002", "result": "1. \u6570\u636e\u4e0d\u5e73\u8861\u4f1a\u589e\u52a0\u5c11\u6570\u7c7b\u7684\u8106\u5f31\u6027\uff0c\u4f46\u5dee\u5206\u9690\u79c1\u80fd\u6709\u6548\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\n2. \u7c7b\u522b\u8f83\u5c11\u7684\u6570\u636e\u96c6\u53ef\u540c\u65f6\u63d0\u5347\u6a21\u578b\u6548\u7528\u548c\u9690\u79c1\u4fdd\u62a4\u6548\u679c\n3. \u9ad8\u71b5\u6216\u4f4eFisher\u5224\u522b\u6bd4\u7684\u6570\u636e\u96c6\u4f1a\u663e\u8457\u6076\u5316\u6548\u7528-\u9690\u79c1\u6743\u8861", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u5b9e\u8df5\u8005\u548c\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\uff0c\u53ef\u6839\u636e\u6570\u636e\u96c6\u7279\u6027\u4f30\u8ba1\u548c\u4f18\u5316\u6548\u7528-\u9690\u79c1\u6743\u8861\uff0c\u4e3a\u6570\u636e\u9884\u5904\u7406\u548c\u9690\u79c1\u53c2\u6570\u8c03\u6574\u63d0\u4f9b\u79d1\u5b66\u4f9d\u636e\u3002"}}
{"id": "2602.00906", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DS", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.00906", "abs": "https://arxiv.org/abs/2602.00906", "authors": ["Anxin Guo", "Jingwei Li"], "title": "Hallucination is a Consequence of Space-Optimality: A Rate-Distortion Theorem for Membership Testing", "comment": null, "summary": "Large language models often hallucinate with high confidence on \"random facts\" that lack inferable patterns. We formalize the memorization of such facts as a membership testing problem, unifying the discrete error metrics of Bloom filters with the continuous log-loss of LLMs. By analyzing this problem in the regime where facts are sparse in the universe of plausible claims, we establish a rate-distortion theorem: the optimal memory efficiency is characterized by the minimum KL divergence between score distributions on facts and non-facts. This theoretical framework provides a distinctive explanation for hallucination: even with optimal training, perfect data, and a simplified \"closed world\" setting, the information-theoretically optimal strategy under limited capacity is not to abstain or forget, but to assign high confidence to some non-facts, resulting in hallucination. We validate this theory empirically on synthetic data, showing that hallucinations persist as a natural consequence of lossy compression.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc1\u660e\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\u6e90\u4e8e\u4fe1\u606f\u8bba\u6781\u9650\u800c\u4e0d\u53ef\u907f\u514d\u3002\u901a\u8fc7\u5c06\u4e8b\u5b9e\u8bb0\u5fc6\u5f62\u5f0f\u5316\u4e3a\u6210\u5458\u6d4b\u8bd5\u95ee\u9898\u5e76\u5efa\u7acb\u7387\u5931\u771f\u5b9a\u7406\uff0c\u7814\u7a76\u8868\u660e\u5728\u5bb9\u91cf\u6709\u9650\u4e14\u4e8b\u5b9e\u7a00\u758f\u7684\u6761\u4ef6\u4e0b\uff0c\u6700\u4f18\u7b56\u7565\u662f\u5bf9\u67d0\u4e9b\u975e\u4e8b\u5b9e\u8d4b\u4e88\u9ad8\u7f6e\u4fe1\u5ea6\uff0c\u4f7f\u5e7b\u89c9\u6210\u4e3a\u6709\u635f\u538b\u7f29\u7684\u81ea\u7136\u7ed3\u679c\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5e38\u5e38\u5bf9\u7f3a\u4e4f\u53ef\u63a8\u65ad\u6a21\u5f0f\u7684\"\u968f\u673a\u4e8b\u5b9e\"\u4ea7\u751f\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u5e7b\u89c9\u3002\u4f5c\u8005\u8bd5\u56fe\u63a2\u7a76\u8fd9\u662f\u8bad\u7ec3\u7f3a\u9677\u8fd8\u662f\u6839\u672c\u6027\u9650\u5236\uff0c\u7814\u7a76\u4e3a\u4f55\u5373\u4f7f\u5728\u6700\u4f18\u8bad\u7ec3\u548c\u5b8c\u7f8e\u6570\u636e\u4e0b\u5e7b\u89c9\u4f9d\u7136\u5b58\u5728\u3002", "method": "\u7814\u7a76\u8005\u5c06\u4e8b\u5b9e\u8bb0\u5fc6\u5f62\u5f0f\u5316\u4e3a\u6210\u5458\u6d4b\u8bd5\u95ee\u9898\uff0c\u5c06\u79bb\u6563\u7684\u5e03\u9686\u8fc7\u6ee4\u5668\u8bef\u5dee\u5ea6\u91cf\u4e0e\u8fde\u7eed\u7684\u8bed\u8a00\u6a21\u578b\u5bf9\u6570\u635f\u5931\u7edf\u4e00\u8d77\u6765\u3002\u4ed6\u4eec\u5728\u4e8b\u5b9e\u7a00\u758f\u7684\u6761\u4ef6\u4e0b\u5206\u6790\u8be5\u95ee\u9898\uff0c\u5efa\u7acb\u7387\u5931\u771f\u5b9a\u7406\uff0c\u901a\u8fc7\u4e8b\u5b9e\u4e0e\u975e\u4e8b\u5b9e\u5f97\u5206\u5206\u5e03\u4e4b\u95f4\u7684KL\u6563\u5ea6\u6765\u523b\u753b\u6700\u4f18\u8bb0\u5fc6\u6548\u7387\uff0c\u5e76\u5728\u5408\u6210\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "\u7387\u5931\u771f\u5b9a\u7406\u8868\u660e\u6700\u4f18\u8bb0\u5fc6\u6548\u7387\u7531\u6700\u5c0f\u5316\u4e8b\u5b9e\u4e0e\u975e\u4e8b\u5b9e\u5f97\u5206\u5206\u5e03\u95f4\u7684KL\u6563\u5ea6\u51b3\u5b9a\u3002\u5173\u952e\u53d1\u73b0\u662f\uff1a\u5373\u4f7f\u5728\u6700\u4f18\u8bad\u7ec3\u3001\u5b8c\u7f8e\u6570\u636e\u548c\u7b80\u5316\"\u5c01\u95ed\u4e16\u754c\"\u8bbe\u5b9a\u4e0b\uff0c\u5bb9\u91cf\u6709\u9650\u65f6\u4fe1\u606f\u8bba\u6700\u4f18\u7b56\u7565\u5e76\u975e\u62d2\u7edd\u56de\u7b54\u6216\u9057\u5fd8\uff0c\u800c\u662f\u5bf9\u67d0\u4e9b\u975e\u4e8b\u5b9e\u8d4b\u4e88\u9ad8\u7f6e\u4fe1\u5ea6\uff0c\u5bfc\u81f4\u5e7b\u89c9\u3002", "conclusion": "\u5e7b\u89c9\u4e0d\u662f\u8bad\u7ec3\u7f3a\u9677\uff0c\u800c\u662f\u5927\u8bed\u8a00\u6a21\u578b\u6709\u635f\u538b\u7f29\u4e0d\u53ef\u907f\u514d\u7684\u81ea\u7136\u7ed3\u679c\u3002\u5b83\u662f\u5728\u5bb9\u91cf\u7ea6\u675f\u4e0b\u7684\u6700\u4f18\u7b56\u7565\uff0c\u4ece\u6839\u672c\u4e0a\u6311\u6218\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u5b8c\u7f8e\u4e8b\u5b9e\u51c6\u786e\u6027\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2501.17634", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2501.17634", "abs": "https://arxiv.org/abs/2501.17634", "authors": ["Lucas Lange", "Ole Borchardt", "Erhard Rahm"], "title": "Federated Learning With Individualized Privacy Through Client Sampling", "comment": "Accepted at 10th International Conference on Machine Learning Technologies (ICMLT 2025)", "summary": "With growing concerns about user data collection, individualized privacy has emerged as a promising solution to balance protection and utility by accounting for diverse user privacy preferences. Instead of enforcing a uniform level of anonymization for all users, this approach allows individuals to choose privacy settings that align with their comfort levels. Building on this idea, we propose an adapted method for enabling Individualized Differential Privacy (IDP) in Federated Learning (FL) by handling clients according to their personal privacy preferences. By extending the SAMPLE algorithm from centralized settings to FL, we calculate client-specific sampling rates based on their heterogeneous privacy budgets and integrate them into a modified IDP-FedAvg algorithm. We test this method under realistic privacy distributions and multiple datasets. The experimental results demonstrate that our approach achieves clear improvements over uniform DP baselines, reducing the trade-off between privacy and utility. Compared to the alternative SCALE method in related work, which assigns differing noise scales to clients, our method performs notably better. However, challenges remain for complex tasks with non-i.i.d. data, primarily stemming from the constraints of the decentralized setting.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e2a\u6027\u5316\u5dee\u5206\u9690\u79c1\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5IDP-FedAvg\uff0c\u901a\u8fc7\u6269\u5c55SAMPLE\u7b97\u6cd5\u4e3a\u5ba2\u6237\u7aef\u5206\u914d\u57fa\u4e8e\u5176\u9690\u79c1\u9884\u7b97\u7684\u91c7\u6837\u7387\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u7edf\u4e00DP\u57fa\u7ebf\u548cSCALE\u65b9\u6cd5\uff0c\u4f46\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u4e0a\u4ecd\u9762\u4e34\u6311\u6218\u3002", "motivation": "\u968f\u7740\u7528\u6237\u6570\u636e\u6536\u96c6\u95ee\u9898\u65e5\u76ca\u53d7\u5230\u5173\u6ce8\uff0c\u4e2a\u6027\u5316\u9690\u79c1\u6210\u4e3a\u5e73\u8861\u9690\u79c1\u4fdd\u62a4\u4e0e\u6570\u636e\u6548\u7528\u7684\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5bf9\u6240\u6709\u7528\u6237\u5f3a\u5236\u6267\u884c\u7edf\u4e00\u7684\u533f\u540d\u5316\u6c34\u5e73\uff0c\u800c\u4e2a\u6027\u5316\u65b9\u6cd5\u5141\u8bb8\u4e2a\u4eba\u6839\u636e\u81ea\u8eab\u8212\u9002\u7a0b\u5ea6\u9009\u62e9\u9690\u79c1\u8bbe\u7f6e\u3002\u672c\u6587\u65e8\u5728\u5c06\u4e2a\u6027\u5316\u5dee\u5206\u9690\u79c1\uff08IDP\uff09\u9002\u914d\u5230\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4e2d\uff0c\u4ee5\u5904\u7406\u5ba2\u6237\u7aef\u7684\u4e2a\u6027\u5316\u9690\u79c1\u504f\u597d\u3002", "method": "\u901a\u8fc7\u6269\u5c55SAMPLE\u7b97\u6cd5\u4ece\u96c6\u4e2d\u5f0f\u8bbe\u7f6e\u5230\u8054\u90a6\u5b66\u4e60\u73af\u5883\uff0c\u6839\u636e\u5ba2\u6237\u7aef\u5f02\u6784\u9690\u79c1\u9884\u7b97\u8ba1\u7b97\u5176\u7279\u5b9a\u7684\u91c7\u6837\u7387\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230\u6539\u8fdb\u7684IDP-FedAvg\u7b97\u6cd5\u4e2d\uff0c\u4f7f\u5ba2\u6237\u7aef\u80fd\u591f\u6309\u7167\u4e2a\u4eba\u9690\u79c1\u504f\u597d\u8fdb\u884c\u5904\u7406\u3002", "result": "\u5728\u73b0\u5b9e\u7684\u9690\u79c1\u5206\u5e03\u548c\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u8f83\u4e8e\u7edf\u4e00\u7684DP\u57fa\u7ebf\u53d6\u5f97\u4e86\u660e\u663e\u6539\u8fdb\uff0c\u964d\u4f4e\u4e86\u9690\u79c1\u4e0e\u6548\u7528\u4e4b\u95f4\u7684\u6743\u8861\u3002\u4e0e\u76f8\u5173\u5de5\u4f5c\u4e2d\u901a\u8fc7\u5206\u914d\u4e0d\u540c\u566a\u58f0\u5c3a\u5ea6\u7ed9\u5ba2\u6237\u7aef\u7684\u66ff\u4ee3\u65b9\u6cd5SCALE\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u663e\u8457\u66f4\u597d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5b9e\u73b0\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u4e2a\u6027\u5316\u9690\u79c1\u4fdd\u62a4\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u5177\u6709\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u7684\u590d\u6742\u4efb\u52a1\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u8fd9\u4e9b\u6311\u6218\u4e3b\u8981\u6e90\u4e8e\u53bb\u4e2d\u5fc3\u5316\u8bbe\u7f6e\u7684\u7ea6\u675f\u3002"}}
{"id": "2602.00907", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00907", "abs": "https://arxiv.org/abs/2602.00907", "authors": ["Pingping Wang", "Yihong Yuan", "Lingcheng Li", "Yongmei Lu"], "title": "PyGALAX: An Open-Source Python Toolkit for Advanced Explainable Geospatial Machine Learning", "comment": null, "summary": "PyGALAX is a Python package for geospatial analysis that integrates automated machine learning (AutoML) and explainable artificial intelligence (XAI) techniques to analyze spatial heterogeneity in both regression and classification tasks. It automatically selects and optimizes machine learning models for different geographic locations and contexts while maintaining interpretability through SHAP (SHapley Additive exPlanations) analysis. PyGALAX builds upon and improves the GALAX framework (Geospatial Analysis Leveraging AutoML and eXplainable AI), which has proven to outperform traditional geographically weighted regression (GWR) methods. Critical enhancements in PyGALAX from the original GALAX framework include automatic bandwidth selection and flexible kernel function selection, providing greater flexibility and robustness for spatial modeling across diverse datasets and research questions. PyGALAX not only inherits all the functionalities of the original GALAX framework but also packages them into an accessible, reproducible, and easily deployable Python toolkit while providing additional options for spatial modeling. It effectively addresses spatial non-stationarity and generates transparent insights into complex spatial relationships at both global and local scales, making advanced geospatial machine learning methods accessible to researchers and practitioners in geography, urban planning, environmental science, and related fields.", "AI": {"tldr": "PyGALAX\u662f\u4e00\u4e2aPython\u5730\u7406\u7a7a\u95f4\u5206\u6790\u5de5\u5177\u5305\uff0c\u901a\u8fc7\u96c6\u6210AutoML\u548cXAI\uff08SHAP\uff09\u6280\u672f\uff0c\u81ea\u52a8\u4f18\u5316\u4e0d\u540c\u5730\u7406\u4f4d\u7f6e\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u5206\u6790\u7a7a\u95f4\u5f02\u8d28\u6027\uff0c\u76f8\u6bd4\u4f20\u7edfGWR\u65b9\u6cd5\u6027\u80fd\u66f4\u4f18\uff0c\u5e76\u65b0\u589e\u81ea\u52a8\u5e26\u5bbd\u9009\u62e9\u548c\u7075\u6d3b\u6838\u51fd\u6570\u9009\u62e9\u529f\u80fd\u3002", "motivation": "\u4f20\u7edf\u5730\u7406\u7a7a\u95f4\u5206\u6790\u65b9\u6cd5\uff08\u5982GWR\uff09\u5728\u5904\u7406\u590d\u6742\u7a7a\u95f4\u5f02\u8d28\u6027\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7f3a\u4e4f\u81ea\u52a8\u5316\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u81ea\u52a8\u9009\u62e9\u548c\u4f18\u5316\u4e0d\u540c\u5730\u7406\u60c5\u5883\u4e0b\u7684\u6a21\u578b\uff0c\u53c8\u80fd\u63d0\u4f9b\u900f\u660e\u3001\u53ef\u89e3\u91ca\u7ed3\u679c\u7684\u5de5\u5177\uff0c\u4f7f\u975e\u4e13\u5bb6\u4e5f\u80fd\u4f7f\u7528\u5148\u8fdb\u7684\u5730\u7406\u7a7a\u95f4\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51faPyGALAX\uff0c\u5728GALAX\u6846\u67b6\u57fa\u7840\u4e0a\u589e\u52a0\u81ea\u52a8\u5e26\u5bbd\u9009\u62e9\u548c\u7075\u6d3b\u6838\u51fd\u6570\u9009\u62e9\u529f\u80fd\uff0c\u96c6\u6210AutoML\u5b9e\u73b0\u81ea\u52a8\u5316\u6a21\u578b\u9009\u62e9\u4e0e\u4f18\u5316\uff0c\u5e76\u91c7\u7528XAI\u4e2d\u7684SHAP\u5206\u6790\u786e\u4fdd\u6a21\u578b\u53ef\u89e3\u91ca\u6027\uff0c\u4ee5\u89e3\u51b3\u56de\u5f52\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u7a7a\u95f4\u975e\u5e73\u7a33\u6027\u95ee\u9898\u3002", "result": "PyGALAX\u8d85\u8d8a\u4e86\u4f20\u7edfGWR\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u4e3a\u591a\u6837\u5316\u6570\u636e\u96c6\u7684\u7a7a\u95f4\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u7075\u6d3b\u6027\u548c\u9c81\u68d2\u6027\uff0c\u6210\u529f\u5c06\u5148\u8fdb\u7684\u5730\u7406\u7a7a\u95f4\u673a\u5668\u5b66\u4e60\u6280\u672f\u5c01\u88c5\u6210\u53ef\u8bbf\u95ee\u3001\u53ef\u590d\u73b0\u7684Python\u5de5\u5177\u5305\uff0c\u80fd\u591f\u5728\u5168\u5c40\u548c\u5c40\u90e8\u5c3a\u5ea6\u751f\u6210\u900f\u660e\u7684\u7a7a\u95f4\u5173\u7cfb\u6d1e\u5bdf\u3002", "conclusion": "PyGALAX\u6709\u6548\u89e3\u51b3\u4e86\u7a7a\u95f4\u975e\u5e73\u7a33\u6027\u95ee\u9898\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u53ef\u89e3\u91ca\u6027\uff0c\u901a\u8fc7\u6613\u4e8e\u90e8\u7f72\u7684Python\u5305\u4f7f\u5730\u7406\u5b66\u3001\u57ce\u5e02\u89c4\u5212\u3001\u73af\u5883\u79d1\u5b66\u7b49\u9886\u57df\u7684\u7814\u7a76\u8005\u548c\u4ece\u4e1a\u8005\u80fd\u591f\u4fbf\u6377\u5730\u4f7f\u7528\u5148\u8fdb\u7684\u5730\u7406\u7a7a\u95f4\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2507.18992", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18992", "abs": "https://arxiv.org/abs/2507.18992", "authors": ["Jongsoo Lee", "Jangwon Kim", "Jiseok Jeong", "Soohee Han"], "title": "Reinforcement Learning via Conservative Agent for Environments with Random Delays", "comment": null, "summary": "Real-world reinforcement learning applications are often hindered by delayed feedback from environments, which violates the Markov assumption and introduces significant challenges. Although numerous delay-compensating methods have been proposed for environments with constant delays, environments with random delays remain largely unexplored due to their inherent variability and unpredictability. In this study, we propose a simple yet robust agent for decision-making under random delays, termed the conservative agent, which reformulates the random-delay environment into its constant-delay equivalent. This transformation enables any state-of-the-art constant-delay method to be directly extended to the random-delay environments without modifying the algorithmic structure or sacrificing performance. We evaluate the conservative agent-based algorithm on continuous control tasks, and empirical results demonstrate that it significantly outperforms existing baseline algorithms in terms of asymptotic performance and sample efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4fdd\u5b88\u667a\u80fd\u4f53\uff0c\u5c06\u968f\u673a\u5ef6\u8fdf\u73af\u5883\u8f6c\u5316\u4e3a\u7b49\u6548\u7684\u6052\u5b9a\u5ef6\u8fdf\u73af\u5883\uff0c\u4f7f\u73b0\u6709\u7b97\u6cd5\u65e0\u9700\u4fee\u6539\u5373\u53ef\u5904\u7406\u968f\u673a\u5ef6\u8fdf\uff0c\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u5e38\u9762\u4e34\u73af\u5883\u53cd\u9988\u5ef6\u8fdf\u95ee\u9898\uff0c\u8fd9\u8fdd\u53cd\u9a6c\u5c14\u53ef\u592b\u5047\u8bbe\u5e76\u5e26\u6765\u6311\u6218\u3002\u5c3d\u7ba1\u6052\u5b9a\u5ef6\u8fdf\u5df2\u6709\u4f17\u591a\u8865\u507f\u65b9\u6cd5\uff0c\u4f46\u968f\u673a\u5ef6\u8fdf\u56e0\u53d8\u5316\u6027\u548c\u4e0d\u53ef\u9884\u6d4b\u6027\u4ecd\u7f3a\u4e4f\u7814\u7a76\u3002", "method": "\u63d0\u51fa\"\u4fdd\u5b88\u667a\u80fd\u4f53\"\uff0c\u901a\u8fc7\u5c06\u968f\u673a\u5ef6\u8fdf\u73af\u5883\u91cd\u65b0\u8868\u8ff0\u4e3a\u6052\u5b9a\u5ef6\u8fdf\u7b49\u4ef7\u5f62\u5f0f\uff0c\u4f7f\u4efb\u4f55\u5148\u8fdb\u7684\u6052\u5b9a\u5ef6\u8fdf\u65b9\u6cd5\u90fd\u80fd\u76f4\u63a5\u6269\u5c55\u5230\u968f\u673a\u5ef6\u8fdf\u73af\u5883\uff0c\u65e0\u9700\u4fee\u6539\u7b97\u6cd5\u7ed3\u6784\u3002", "result": "\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u6e10\u8fdb\u6027\u80fd\u548c\u6837\u672c\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5355\u4e14\u9c81\u68d2\uff0c\u80fd\u6709\u6548\u5904\u7406\u968f\u673a\u5ef6\u8fdf\u95ee\u9898\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u5728\u5ef6\u8fdf\u73af\u5883\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.00918", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00918", "abs": "https://arxiv.org/abs/2602.00918", "authors": ["Aur\u00e9lien Renault", "Alexis Bondu", "Antoine Cornu\u00e9jols", "Vincent Lemaire"], "title": "Early Classification of Time Series in Non-Stationary Cost Regimes", "comment": null, "summary": "Early Classification of Time Series (ECTS) addresses decision-making problems in which predictions must be made as early as possible while maintaining high accuracy. Most existing ECTS methods assume that the time-dependent decision costs governing the learning objective are known, fixed, and correctly specified. In practice, however, these costs are often uncertain and may change over time, leading to mismatches between training-time and deployment-time objectives. In this paper, we study ECTS under two practically relevant forms of cost non-stationarity: drift in the balance between misclassification and decision delay costs, and stochastic realizations of decision costs that deviate from the nominal training-time model. To address these challenges, we revisit representative ECTS approaches and adapt them to an online learning setting. Focusing on separable methods, we update only the triggering model during deployment, while keeping the classifier fixed. We propose several online adaptations and baselines, including bandit-based and RL-based approaches, and conduct controlled experiments on synthetic data to systematically evaluate robustness under cost non-stationarity. Our results demonstrate that online learning can effectively improve the robustness of ECTS methods to cost drift, with RL-based strategies exhibiting strong and stable performance across varying cost regimes.", "AI": {"tldr": "This paper addresses Early Classification of Time Series (ECTS) under cost non-stationarity, where decision costs change over time. It adapts ECTS to online learning by updating only the triggering model while keeping the classifier fixed, proposing bandit and RL-based approaches that improve robustness to cost drift.", "motivation": "Most ECTS methods assume fixed, known decision costs, but real-world costs are often uncertain and non-stationary (drifting balance between misclassification and delay costs, or stochastic deviations), causing training-deployment objective mismatches that degrade performance.", "method": "The authors revisit representative ECTS approaches and adapt them to online learning, focusing on separable methods that update only the triggering model during deployment while keeping the classifier fixed. They propose several online adaptations including bandit-based and reinforcement learning-based approaches.", "result": "Controlled experiments on synthetic data demonstrate that online learning effectively improves robustness to cost drift, with RL-based strategies showing strong and stable performance across varying cost regimes.", "conclusion": "Online learning adaptations, particularly RL-based methods, can significantly enhance ECTS robustness to cost non-stationarity, making them more practical for real-world deployment where decision costs change over time."}}
{"id": "2602.00927", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00927", "abs": "https://arxiv.org/abs/2602.00927", "authors": ["Yihao Xue", "Allan Zhang", "Jianhao Huang", "Amit Sahai", "Baharan Mirzasoleiman"], "title": "Beyond What Seems Necessary: Hidden Gains from Scaling Training-Time Reasoning Length under Outcome Supervision", "comment": null, "summary": "Training LLMs to think and reason for longer has become a key ingredient in building state-of-the-art models that can solve complex problems previously out of reach. Recent efforts pursue this in different ways, such as RL fine-tuning to elicit long CoT or scaling latent reasoning through architectural recurrence. This makes reasoning length an important scaling knob. In this work, we identify a novel phenomenon (both theoretically and experimentally): under outcome-only supervision, out-of-distribution (OOD) performance can continue improving as training-time reasoning length (e.g., the token budget in RL, or the loop count in looped Transformers) increases, even after in-distribution (ID) performance has saturated. This suggests that robustness may require a larger budget than ID validation alone would indicate. We provide theoretical explanations via two mechanisms: (i) self-iteration can induce a stronger inductive bias in the hypothesis class, reshaping ID-optimal solutions in ways that improve OOD generalization; and (ii) when shortcut solutions that work for ID samples but not for OOD samples persist in the hypothesis class, regularization can reduce the learned solution's reliance on these shortcuts as the number of self-iterations increases. We complement the theory with empirical evidence from two realizations of scaling training-time reasoning length: increasing the number of loops in looped Transformers on a synthetic task, and increasing token budgets during RL fine-tuning of LLMs on mathematical reasoning.", "AI": {"tldr": "\u8be5\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u4ec5\u6709\u7ed3\u679c\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\uff0c\u589e\u52a0\u8bad\u7ec3\u65f6\u7684\u63a8\u7406\u957f\u5ea6\uff08\u5982RL\u7684token\u9884\u7b97\u6216\u5faa\u73afTransformer\u7684\u5faa\u73af\u6b21\u6570\uff09\u53ef\u4ee5\u5728\u5206\u5e03\u5185\uff08ID\uff09\u6027\u80fd\u9971\u548c\u540e\uff0c\u6301\u7eed\u6539\u5584\u5206\u5e03\u5916\uff08OOD\uff09\u6027\u80fd\uff0c\u8fd9\u8868\u660e\u6a21\u578b\u9c81\u68d2\u6027\u53ef\u80fd\u9700\u8981\u6bd4ID\u9a8c\u8bc1\u6240\u663e\u793a\u7684\u66f4\u5927\u9884\u7b97\u3002\u7406\u8bba\u89e3\u91ca\u5305\u62ec\u81ea\u8fed\u4ee3\u589e\u5f3a\u5f52\u7eb3\u504f\u7f6e\u548c\u51cf\u5c11\u5bf9ID\u6377\u5f84\u7684\u4f9d\u8d56\uff0c\u5e76\u901a\u8fc7\u5faa\u73afTransformer\u548cRL\u5fae\u8c03\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u66f4\u957f\u63a8\u7406\u662f\u89e3\u51b3\u590d\u6742\u95ee\u9898\u7684\u5173\u952e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\uff08\u5982RL\u5fae\u8c03\u3001\u5faa\u73af\u67b6\u6784\uff09\u5c06\u63a8\u7406\u957f\u5ea6\u89c6\u4e3a\u7f29\u653e\u53c2\u6570\uff0c\u7f3a\u4e4f\u5bf9\u5176\u5728ID\u4e0eOOD\u6027\u80fd\u4e0a\u5dee\u5f02\u5f71\u54cd\u7684\u7406\u89e3\u3002\u6838\u5fc3\u95ee\u9898\u662f\uff1a\u8d85\u51faID\u9971\u548c\u70b9\u540e\u589e\u52a0\u63a8\u7406\u957f\u5ea6\u662f\u5426\u80fd\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u7406\u8bba\u5206\u6790\u4e0e\u4e24\u79cd\u5b9e\u8bc1\u65b9\u6cd5\uff1a(1) \u5728\u5408\u6210\u4efb\u52a1\u4e0a\u589e\u52a0\u5faa\u73afTransformer\u7684\u5faa\u73af\u6b21\u6570\uff1b(2) \u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u589e\u52a0RL\u5fae\u8c03\u7684token\u9884\u7b97\u3002\u7406\u8bba\u4e0a\u63d0\u51fa\u4e24\u4e2a\u673a\u5236\uff1a\u81ea\u8fed\u4ee3\u7684\u5f52\u7eb3\u504f\u7f6e\u6548\u5e94\u548c\u5bf9\u6377\u5f84\u89e3\u7684\u6291\u5236\u3002", "result": "\u53d1\u73b0\u65b0\u73b0\u8c61\uff1aOOD\u6027\u80fd\u53ef\u5728ID\u6027\u80fd\u9971\u548c\u540e\u968f\u63a8\u7406\u957f\u5ea6\u6301\u7eed\u6539\u5584\u3002\u7406\u8bba\u8868\u660e\u8fd9\u6e90\u4e8e\u81ea\u8fed\u4ee3\u91cd\u5851\u89e3\u7a7a\u95f4\u4ee5\u589e\u5f3a\u6cdb\u5316\uff0c\u5e76\u51cf\u5c11\u5bf9\u4ec5\u9002\u7528\u4e8eID\u7684\u6377\u5f84\u89e3\u7684\u4f9d\u8d56\u3002\u5faa\u73afTransformer\u548cRL\u5fae\u8c03\u5b9e\u9a8c\u5747\u8bc1\u5b9e\u8be5\u6548\u5e94\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5b9e\u73b0\u9c81\u68d2\u7684OOD\u6027\u80fd\u53ef\u80fd\u9700\u8981\u8fdc\u8d85ID\u9a8c\u8bc1\u5efa\u8bae\u7684\u63a8\u7406\u9884\u7b97\uff0c\u8fd9\u5bf9\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u548c\u6a21\u578b\u9c81\u68d2\u6027\u8bc4\u4f30\u5177\u6709\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2602.00931", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00931", "abs": "https://arxiv.org/abs/2602.00931", "authors": ["Muhammad Ahmed Mohsin", "Muhammad Umer", "Ahsan Bilal", "Zihao He", "Muhammad Usman Rafique", "Asad Aali", "Muhammad Ali Jamshed", "John M. Cioffi", "Emily Fox"], "title": "Continuous-Utility Direct Preference Optimization", "comment": "Submitted to ICML 2026", "summary": "Large language model reasoning is often treated as a monolithic capability, relying on binary preference supervision that fails to capture partial progress or fine-grained reasoning quality. We introduce Continuous Utility Direct Preference Optimization (CU-DPO), a framework that aligns models to a portfolio of prompt-based cognitive strategies by replacing binary labels with continuous scores that capture fine-grained reasoning quality. We prove that learning with K strategies yields a Theta(K log K) improvement in sample complexity over binary preferences, and that DPO converges to the entropy-regularized utility-maximizing policy. To exploit this signal, we propose a two-stage training pipeline: (i) strategy selection, which optimizes the model to choose the best strategy for a given problem via best-vs-all comparisons, and (ii) execution refinement, which trains the model to correctly execute the selected strategy using margin-stratified pairs. On mathematical reasoning benchmarks, CU-DPO improves strategy selection accuracy from 35-46 percent to 68-78 percent across seven base models, yielding consistent downstream reasoning gains of up to 6.6 points on in-distribution datasets with effective transfer to out-of-distribution tasks.", "AI": {"tldr": "\u63d0\u51fa\u8fde\u7eed\u6548\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316(CU-DPO)\u6846\u67b6\uff0c\u7528\u8fde\u7eed\u8bc4\u5206\u66ff\u4ee3\u4e8c\u5143\u6807\u7b7e\u6765\u6355\u6349\u7ec6\u7c92\u5ea6\u63a8\u7406\u8d28\u91cf\uff0c\u901a\u8fc7\u7b56\u7565\u9009\u62e9\u548c\u6267\u884c\u7cbe\u70bc\u4e24\u9636\u6bb5\u8bad\u7ec3\uff0c\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u5c06\u7b56\u7565\u9009\u62e9\u51c6\u786e\u7387\u4ece35-46%\u63d0\u5347\u81f368-78%\uff0c\u63a8\u7406\u6027\u80fd\u63d0\u9ad8\u6700\u591a6.6\u5206\u3002", "motivation": "\u73b0\u6709\u5927\u6a21\u578b\u63a8\u7406\u80fd\u529b\u88ab\u89c6\u4f5c\u6574\u4f53\u80fd\u529b\uff0c\u4e8c\u5143\u504f\u597d\u76d1\u7763\u65e0\u6cd5\u6355\u6349\u90e8\u5206\u8fdb\u5c55\u6216\u7ec6\u7c92\u5ea6\u63a8\u7406\u8d28\u91cf\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "1) \u8bbe\u8ba1CU-DPO\u6846\u67b6\uff0c\u7528\u8fde\u7eed\u5206\u6570\u66ff\u4ee3\u4e8c\u5143\u6807\u7b7e\uff0c\u5bf9\u9f50\u63d0\u793a\u8ba4\u77e5\u7b56\u7565\u7ec4\u5408\uff1b2) \u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u7b56\u7565\u9009\u62e9\u9636\u6bb5\u901a\u8fc7\u6700\u4f73vs\u6240\u6709\u6bd4\u8f83\u4f18\u5316\u7b56\u7565\u9009\u62e9\uff0c\u6267\u884c\u7cbe\u70bc\u9636\u6bb5\u901a\u8fc7\u8fb9\u9645\u5206\u5c42\u5bf9\u8bad\u7ec3\u6b63\u786e\u6267\u884c\u7b56\u7565\u3002", "result": "\u5728\u4e03\u4e2a\u57fa\u7840\u6a21\u578b\u4e0a\uff0c\u7b56\u7565\u9009\u62e9\u51c6\u786e\u7387\u4ece35-46%\u5927\u5e45\u63d0\u5347\u81f368-78%\uff1b\u5728\u5206\u5e03\u5185\u6570\u636e\u96c6\u4e0a\u63a8\u7406\u6027\u80fd\u6301\u7eed\u63d0\u5347\u6700\u9ad86.6\u5206\uff0c\u5e76\u80fd\u6709\u6548\u8fc1\u79fb\u5230\u5206\u5e03\u5916\u4efb\u52a1\u3002", "conclusion": "\u7ec6\u7c92\u5ea6\u7684\u8fde\u7eed\u504f\u597d\u4f18\u5316\u663e\u8457\u4f18\u4e8e\u4e8c\u5143\u76d1\u7763\uff0cCU-DPO\u901a\u8fc7\u7b56\u7565\u7ec4\u5408\u5b66\u4e60\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u5b9e\u73b0\u4e86\u6837\u672c\u590d\u6742\u5ea6Theta(K log K)\u7684\u7406\u8bba\u6539\u8fdb\uff0c\u4e3a\u63d0\u5347\u5927\u6a21\u578b\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2602.00942", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00942", "abs": "https://arxiv.org/abs/2602.00942", "authors": ["Hao Ma", "Melis Ilayda Bal", "Liang Zhang", "Bingcong Li", "Niao He", "Melanie Zeilinger", "Michael Muehlebach"], "title": "SALAAD: Sparse And Low-Rank Adaptation via ADMM", "comment": null, "summary": "Modern large language models are increasingly deployed under compute and memory constraints, making flexible control of model capacity a central challenge. While sparse and low-rank structures naturally trade off capacity and performance, existing approaches often rely on heuristic designs that ignore layer and matrix heterogeneity or require model-specific architectural modifications. We propose SALAAD, a plug-and-play framework applicable to different model architectures that induces sparse and low-rank structures during training. By formulating structured weight learning under an augmented Lagrangian framework and introducing an adaptive controller that dynamically balances the training loss and structural constraints, SALAAD preserves the stability of standard training dynamics while enabling explicit control over the evolution of effective model capacity during training. Experiments across model scales show that SALAAD substantially reduces memory consumption during deployment while achieving performance comparable to ad-hoc methods. Moreover, a single training run yields a continuous spectrum of model capacities, enabling smooth and elastic deployment across diverse memory budgets without the need for retraining.", "AI": {"tldr": "SALAAD\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u4e2d\u8bf1\u5bfc\u7a00\u758f\u548c\u4f4e\u79e9\u7ed3\u6784\uff0c\u5b9e\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u5bb9\u91cf\u7684\u7075\u6d3b\u63a7\u5236\uff0c\u652f\u6301\u5355\u6b21\u8bad\u7ec3\u540e\u5f39\u6027\u90e8\u7f72\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u8ba1\u7b97\u548c\u5185\u5b58\u7ea6\u675f\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u8bbe\u8ba1\uff0c\u5ffd\u7565\u5c42/\u77e9\u9635\u5f02\u8d28\u6027\u6216\u9700\u8981\u7279\u5b9a\u67b6\u6784\u4fee\u6539\uff0c\u96be\u4ee5\u7075\u6d3b\u63a7\u5236\u6a21\u578b\u5bb9\u91cf\u3002", "method": "\u63d0\u51faSALAAD\u6846\u67b6\uff0c\u91c7\u7528\u589e\u5e7f\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\uff0c\u5f15\u5165\u81ea\u9002\u5e94\u63a7\u5236\u5668\u52a8\u6001\u5e73\u8861\u8bad\u7ec3\u635f\u5931\u548c\u7ed3\u6784\u7ea6\u675f\uff0c\u5728\u8bad\u7ec3\u4e2d\u8bf1\u5bfc\u7a00\u758f\u548c\u4f4e\u79e9\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSALAAD\u663e\u8457\u964d\u4f4e\u90e8\u7f72\u5185\u5b58\u6d88\u8017\uff0c\u6027\u80fd\u4e0ead-hoc\u65b9\u6cd5\u76f8\u5f53\uff0c\u4e14\u5355\u6b21\u8bad\u7ec3\u53ef\u4ea7\u751f\u8fde\u7eed\u5bb9\u91cf\u8c31\u3002", "conclusion": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e0d\u540c\u5185\u5b58\u9884\u7b97\u4e0b\u7684\u5e73\u6ed1\u5f39\u6027\u90e8\u7f72\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002"}}
{"id": "2602.00943", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00943", "abs": "https://arxiv.org/abs/2602.00943", "authors": ["Zhenyu Zhao", "David Zhang", "Ellie Zhao", "Ehsan Saberian"], "title": "Dynamic Prior Thompson Sampling for Cold-Start Exploration in Recommender Systems", "comment": null, "summary": "Cold-start exploration is a core challenge in large-scale recommender systems: new or data-sparse items must receive traffic to estimate value, but over-exploration harms users and wastes impressions. In practice, Thompson Sampling (TS) is often initialized with a uniform Beta(1,1) prior, implicitly assuming a 50% success rate for unseen items. When true base rates are far lower, this optimistic prior systematically over-allocates to weak items. The impact is amplified by batched policy updates and pipeline latency: for hours, newly launched items can remain effectively \"no data,\" so the prior dominates allocation before feedback is incorporated. We propose Dynamic Prior Thompson Sampling, a prior design that directly controls the probability that a new arm outcompetes the incumbent winner. Our key contribution is a closed-form quadratic solution for the prior mean that enforces P(X_j > Y_k) = epsilon at introduction time, making exploration intensity predictable and tunable while preserving TS Bayesian updates. Across Monte Carlo validation, offline batched simulations, and a large-scale online experiment on a thumbnail personalization system serving millions of users, dynamic priors deliver precise exploration control and improved efficiency versus a uniform-prior baseline.", "AI": {"tldr": "The paper proposes Dynamic Prior Thompson Sampling to solve cold-start over-exploration in recommender systems by using a tunable prior that controls exploration probability, showing improved efficiency in simulations and large-scale experiments.", "motivation": "Cold-start exploration is challenging because new items need traffic for value estimation but over-exploration harms users and wastes impressions. Current Thompson Sampling with uniform Beta(1,1) prior overly assumes 50% success rates for unseen items, causing systematic over-allocation to weak items, especially exacerbated by batched updates and pipeline latency.", "method": "They propose Dynamic Prior Thompson Sampling, which designs a prior that directly controls the probability (epsilon) that a new item outcompetes the incumbent winner at introduction time. The key is a closed-form quadratic solution for the prior mean that enforces this probability constraint while maintaining standard Thompson Sampling Bayesian updates.", "result": "Through Monte Carlo simulations, offline batched simulations, and a large-scale online experiment on a thumbnail personalization system serving millions of users, dynamic priors achieved precise exploration control and demonstrated improved efficiency compared to the uniform-prior baseline.", "conclusion": "Dynamic Prior Thompson Sampling provides a tunable and predictable way to control exploration intensity in cold-start scenarios, leading to better performance and efficiency than traditional uniform priors."}}
{"id": "2602.00952", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00952", "abs": "https://arxiv.org/abs/2602.00952", "authors": ["Jing Wang", "Jie Shen", "Dean Foster", "Zohar Karnin", "Jeremy C Weiss"], "title": "Optimal Budgeted Adaptation of Large Language Models", "comment": null, "summary": "The trade-off between labeled data availability and downstream accuracy remains a central challenge in fine-tuning large language models (LLMs). We propose a principled framework for \\emph{budget-aware supervised fine-tuning} by casting LLM adaptation as a contextual Stackelberg game. In our formulation, the learner (leader) commits to a scoring policy and a label-querying strategy, while an adaptive environment (follower) selects challenging supervised alternatives in response. To explicitly address label efficiency, we incorporate a finite supervision budget directly into the learning objective. Our algorithm operates in the full-feedback regime and achieves $\\tilde{O}(d\\sqrt{T})$ regret under standard linear contextual assumptions. We extend the framework with a Largest-Latency-First (LLF) confidence gate that selectively queries labels, achieving a budget-aware regret bound of $\\tilde{O}(\\sqrt{dB} + c\\sqrt{B})$ with $B=\u03b2T$.", "AI": {"tldr": "Proposes a budget-aware LLM fine-tuning framework using Stackelberg game formulation, achieving $\\tilde{O}(d\\sqrt{T})$ regret in full-feedback and $\\tilde{O}(\\sqrt{dB} + c\\sqrt{B})$ with selective label querying.", "motivation": "The trade-off between limited labeled data availability and downstream accuracy remains a central challenge in fine-tuning large language models.", "method": "Casts LLM adaptation as a contextual Stackelberg game where the learner (leader) commits to a scoring policy and label-querying strategy while an adaptive environment (follower) responds with challenging examples, incorporating a finite supervision budget directly into the learning objective, with a Largest-Latency-First (LLF) confidence gate for selective label querying.", "result": "Full-feedback regime achieves $\\tilde{O}(d\\sqrt{T})$ regret under linear contextual assumptions; with LLF gate achieves budget-aware regret bound of $\\tilde{O}(\\sqrt{dB} + c\\sqrt{B})$ where $B=\u03b2T$.", "conclusion": "The framework provides a principled approach for budget-aware LLM fine-tuning with strong theoretical guarantees, showing that explicit budget constraints and selective labeling strategies effectively balance accuracy and label efficiency."}}
{"id": "2602.00953", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00953", "abs": "https://arxiv.org/abs/2602.00953", "authors": ["Sahar Almahfouz Nasser", "Juan Francisco Pesantez Borja", "Jincheng Liu", "Tanvir Hasan", "Zenghan Wang", "Suman Ghosh", "Sandeep Manandhar", "Shikhar Shiromani", "Twisha Shah", "Naoto Tokuyama", "Anant Madabhushi"], "title": "SAGE: Agentic Framework for Interpretable and Clinically Translatable Computational Pathology Biomarker Discovery", "comment": null, "summary": "Despite significant progress in computational pathology, many AI models remain black-box and difficult to interpret, posing a major barrier to clinical adoption due to limited transparency and explainability. This has motivated continued interest in engineered image-based biomarkers, which offer greater interpretability but are often proposed based on anecdotal evidence or fragmented prior literature rather than systematic biological validation. We introduce SAGE (Structured Agentic system for hypothesis Generation and Evaluation), an agentic AI system designed to identify interpretable, engineered pathology biomarkers by grounding them in biological evidence. SAGE integrates literature-anchored reasoning with multimodal data analysis to correlate image-derived features with molecular biomarkers, such as gene expression, and clinically relevant outcomes. By coordinating specialized agents for biological contextualization and empirical hypothesis validation, SAGE prioritizes transparent, biologically supported biomarkers and advances the clinical translation of computational pathology.", "AI": {"tldr": "SAGE is an agentic AI system that identifies interpretable, biologically-grounded pathology biomarkers by integrating literature reasoning with multimodal data analysis, addressing the black-box problem of current computational pathology models.", "motivation": "Current AI models in computational pathology are black-box and lack transparency, limiting clinical adoption. Engineered biomarkers offer interpretability but lack systematic biological validation, creating a need for transparent, biologically-supported biomarkers.", "method": "SAGE integrates literature-anchored reasoning with multimodal data analysis to correlate image-derived features with molecular biomarkers (e.g., gene expression) and clinical outcomes, using specialized agents for biological contextualization and empirical hypothesis validation.", "result": "The system prioritizes transparent, biologically supported biomarkers by grounding them in empirical evidence and literature, facilitating interpretability.", "conclusion": "SAGE advances clinical translation of computational pathology by providing interpretable, biologically-validated biomarkers that overcome the transparency limitations of traditional AI models."}}
{"id": "2602.00959", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00959", "abs": "https://arxiv.org/abs/2602.00959", "authors": ["Yuheng Yang", "Siqi Zhu", "Tao Feng", "Ge Liu", "Jiaxuan You"], "title": "Probing the Knowledge Boundary: An Interactive Agentic Framework for Deep Knowledge Extraction", "comment": "Homepage: https://ulab-uiuc.github.io/KnowledgeExtraction/", "summary": "Large Language Models (LLMs) can be seen as compressed knowledge bases, but it remains unclear what knowledge they truly contain and how far their knowledge boundaries extend. Existing benchmarks are mostly static and provide limited support for systematic knowledge probing. In this paper, we propose an interactive agentic framework to systematically extract and quantify the knowledge of LLMs. Our method includes four adaptive exploration policies to probe knowledge at different granularities. To ensure the quality of extracted knowledge, we introduce a three-stage knowledge processing pipeline that combines vector-based filtering to remove exact duplicates, LLM-based adjudication to resolve ambiguous semantic overlaps, and domain-relevance auditing to retain valid knowledge units. Through extensive experiments, we find that recursive taxonomy is the most effective exploration strategy. We also observe a clear knowledge scaling law, where larger models consistently extract more knowledge. In addition, we identify a Pass@1-versus-Pass@k trade-off: domain-specialized models achieve higher initial accuracy but degrade rapidly, while general-purpose models maintain stable performance during extended extraction. Finally, our results show that differences in training data composition lead to distinct and measurable knowledge profiles across model families.", "AI": {"tldr": "\u63d0\u51fa\u4ea4\u4e92\u5f0f\u667a\u80fd\u4f53\u6846\u67b6\u7cfb\u7edf\u6027\u63d0\u53d6\u548c\u91cf\u5316\u5927\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\uff0c\u63ed\u793a\u9012\u5f52\u5206\u7c7b\u6cd5\u6700\u6709\u6548\uff0c\u5b58\u5728\u77e5\u8bc6\u7f29\u653e\u89c4\u5f8b\u53ca\u4e13\u4e1a/\u901a\u7528\u6a21\u578b\u7684\u6027\u80fd\u6743\u8861\u5173\u7cfb", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u591a\u4e3a\u9759\u6001\uff0c\u65e0\u6cd5\u7cfb\u7edf\u6027\u63a2\u6d4bLLM\u771f\u6b63\u5305\u542b\u7684\u77e5\u8bc6\u5185\u5bb9\u53ca\u5176\u8fb9\u754c\u8303\u56f4", "method": "\u91c7\u7528\u4ea4\u4e92\u5f0f\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u56db\u79cd\u81ea\u9002\u5e94\u63a2\u7d22\u7b56\u7565\u548c\u4e09\u9636\u6bb5\u77e5\u8bc6\u5904\u7406\u6d41\u7a0b\uff08\u5411\u91cf\u53bb\u91cd\u3001LLM\u8bed\u4e49\u6d88\u6b67\u3001\u9886\u57df\u76f8\u5173\u6027\u5ba1\u6838\uff09", "result": "1. \u9012\u5f52\u5206\u7c7b\u6cd5\u662f\u6700\u6709\u6548\u63a2\u7d22\u7b56\u7565\uff1b2. \u5b58\u5728\u77e5\u8bc6\u7f29\u653e\u89c4\u5f8b\uff1a\u6a21\u578b\u8d8a\u5927\u63d0\u53d6\u77e5\u8bc6\u8d8a\u591a\uff1b3. \u53d1\u73b0Pass@1\u4e0ePass@k\u6743\u8861\uff1a\u4e13\u4e1a\u6a21\u578b\u521d\u59cb\u51c6\u786e\u7387\u9ad8\u4f46\u9000\u5316\u5feb\uff0c\u901a\u7528\u6a21\u578b\u8868\u73b0\u7a33\u5b9a\uff1b4. \u8bad\u7ec3\u6570\u636e\u7ec4\u6210\u5bfc\u81f4\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u5177\u6709\u72ec\u7279\u77e5\u8bc6\u7279\u5f81", "conclusion": "\u8be5\u6846\u67b6\u80fd\u6709\u6548\u91cf\u5316LLM\u77e5\u8bc6\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u89c4\u6a21\u3001\u4e13\u4e1a\u5316\u548c\u8bad\u7ec3\u6570\u636e\u5bf9\u77e5\u8bc6\u7279\u5f81\u7684\u7cfb\u7edf\u6027\u5f71\u54cd\uff0c\u4e3a\u7406\u89e3LLM\u77e5\u8bc6\u8fb9\u754c\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5"}}
{"id": "2602.00969", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00969", "abs": "https://arxiv.org/abs/2602.00969", "authors": ["Junlin Huang", "Wenyi Fang", "Zhenheng Tang", "Yuxin Wang", "Xueze Kang", "Yang Zheng", "Bo Li", "Xiaowen Chu"], "title": "On the Spectral Flattening of Quantized Embeddings", "comment": null, "summary": "Training Large Language Models (LLMs) at ultra-low precision is critically impeded by instability rooted in the conflict between discrete quantization constraints and the intrinsic heavy-tailed spectral nature of linguistic data. By formalizing the connection between Zipfian statistics and random matrix theory, we prove that the power-law decay in the singular value spectra of embeddings is a fundamental requisite for semantic encoding. We derive theoretical bounds showing that uniform quantization introduces a noise floor that disproportionately truncates this spectral tail, which induces spectral flattening and a strictly provable increase in the stable rank of representations. Empirical validation across diverse architectures including GPT-2 and TinyLlama corroborates that this geometric degradation precipitates representational collapse. This work not only quantifies the spectral sensitivity of LLMs but also establishes spectral fidelity as a necessary condition for stable low-bit optimization.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63ed\u793a\u4e86\u8d85\u4f4e\u7cbe\u5ea6\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\u7684\u4e0d\u7a33\u5b9a\u6027\u6839\u6e90\uff0c\u8bc1\u660e\u4e86\u8bcd\u5d4c\u5165\u7684\u5947\u5f02\u503c\u8c31\u5e42\u5f8b\u8870\u51cf\u662f\u8bed\u4e49\u7f16\u7801\u7684\u57fa\u7840\u8981\u6c42\uff0c\u5e76\u5efa\u7acb\u4e86\u9891\u8c31\u4fdd\u771f\u5ea6\u4f5c\u4e3a\u7a33\u5b9a\u4f4e\u6bd4\u7279\u4f18\u5316\u7684\u5fc5\u8981\u6761\u4ef6\u3002", "motivation": "\u8d85\u4f4e\u7cbe\u5ea6\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u4e25\u91cd\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5176\u6839\u6e90\u5728\u4e8e\u79bb\u6563\u91cf\u5316\u7ea6\u675f\u4e0e\u8bed\u8a00\u6570\u636e\u56fa\u6709\u7684\u91cd\u5c3e\u9891\u8c31\u7279\u6027\u4e4b\u95f4\u7684\u51b2\u7a81\u3002\u9700\u8981\u4ece\u7406\u8bba\u4e0a\u7406\u89e3\u8fd9\u79cd\u4e0d\u7a33\u5b9a\u6027\u5e76\u627e\u5230\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5f62\u5f0f\u5316Zipf\u7edf\u8ba1\u4e0e\u968f\u673a\u77e9\u9635\u7406\u8bba\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u8bc1\u660e\u4e86\u5d4c\u5165\u5c42\u5947\u5f02\u503c\u8c31\u4e2d\u7684\u5e42\u5f8b\u8870\u51cf\u662f\u8bed\u4e49\u7f16\u7801\u7684\u57fa\u672c\u524d\u63d0\u3002\u63a8\u5bfc\u51fa\u7406\u8bba\u8fb9\u754c\uff0c\u8bf4\u660e\u5747\u5300\u91cf\u5316\u4f1a\u5f15\u5165\u566a\u58f0\u57fa\u5e95\uff0c\u4e0d\u6210\u6bd4\u4f8b\u5730\u622a\u65ad\u9891\u8c31\u5c3e\u90e8\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u5747\u5300\u91cf\u5316\u5bfc\u81f4\u9891\u8c31\u5e73\u5766\u5316\u5e76\u4e25\u683c\u589e\u52a0\u8868\u793a\u7684\u7a33\u5b9a\u79e9\u3002\u5728GPT-2\u548cTinyLlama\u7b49\u591a\u79cd\u67b6\u6784\u4e0a\u7684\u5b9e\u8bc1\u9a8c\u8bc1\u8868\u660e\uff0c\u8fd9\u79cd\u51e0\u4f55\u9000\u5316\u4f1a\u5bfc\u81f4\u8868\u793a\u5d29\u6e83\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e0d\u4ec5\u91cf\u5316\u4e86LLMs\u7684\u9891\u8c31\u654f\u611f\u6027\uff0c\u8fd8\u786e\u7acb\u4e86\u9891\u8c31\u4fdd\u771f\u5ea6\u4f5c\u4e3a\u7a33\u5b9a\u4f4e\u6bd4\u7279\u4f18\u5316\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u4e3a\u672a\u6765\u8d85\u4f4e\u7cbe\u5ea6\u8bad\u7ec3\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2602.00974", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00974", "abs": "https://arxiv.org/abs/2602.00974", "authors": ["Adrien Aumon", "Myriam Lizotte", "Guy Wolf", "Kevin R. Moon", "Jake S. Rhodes"], "title": "Forest-Guided Semantic Transport for Label-Supervised Manifold Alignment", "comment": null, "summary": "Label-supervised manifold alignment bridges the gap between unsupervised and correspondence-based paradigms by leveraging shared label information to align multimodal datasets. Still, most existing methods rely on Euclidean geometry to model intra-domain relationships. This approach can fail when features are only weakly related to the task of interest, leading to noisy, semantically misleading structure and degraded alignment quality. To address this limitation, we introduce FoSTA (Forest-guided Semantic Transport Alignment), a scalable alignment framework that leverages forest-induced geometry to denoise intra-domain structure and recover task-relevant manifolds prior to alignment. FoSTA builds semantic representations directly from label-informed forest affinities and aligns them via fast, hierarchical semantic transport, capturing meaningful cross-domain relationships. Extensive comparisons with established baselines demonstrate that FoSTA improves correspondence recovery and label transfer on synthetic benchmarks and delivers strong performance in practical single-cell applications, including batch correction and biological conservation.", "AI": {"tldr": "\u9488\u5bf9\u6807\u7b7e\u76d1\u7763\u6d41\u5f62\u5bf9\u9f50\u4e2d\u6b27\u6c0f\u51e0\u4f55\u6613\u53d7\u5f31\u76f8\u5173\u7279\u5f81\u566a\u58f0\u5e72\u6270\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faFoSTA\u6846\u67b6\uff0c\u901a\u8fc7\u68ee\u6797\u8bf1\u5bfc\u51e0\u4f55\u5148\u5bf9\u57df\u5185\u7ed3\u6784\u8fdb\u884c\u53bb\u566a\u5e76\u6062\u590d\u4efb\u52a1\u76f8\u5173\u6d41\u5f62\uff0c\u518d\u7ecf\u5c42\u6b21\u5316\u8bed\u4e49\u4f20\u8f93\u5b9e\u73b0\u9ad8\u6548\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u5728\u5408\u6210\u4e0e\u5355\u7ec6\u80de\u6570\u636e\u96c6\u4e0a\u5747\u9a8c\u8bc1\u6709\u6548\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u91c7\u7528\u6b27\u6c0f\u51e0\u4f55\u5efa\u6a21\u7279\u5f81\u5173\u7cfb\uff0c\u4f46\u5f53\u7279\u5f81\u4e0e\u4efb\u52a1\u76ee\u6807\u5173\u8054\u6027\u8f83\u5f31\u65f6\uff0c\u4f1a\u5f15\u5165\u566a\u58f0\u548c\u8bed\u4e49\u8bef\u5bfc\u6027\u7ed3\u6784\uff0c\u635f\u5bb3\u5bf9\u9f50\u8d28\u91cf\uff0c\u4e9f\u9700\u66f4\u9c81\u68d2\u7684\u51e0\u4f55\u5efa\u6a21\u65b9\u5f0f\u3002", "method": "FoSTA\u6838\u5fc3\u5305\u62ec\uff1a(1)\u57fa\u4e8e\u6807\u7b7e\u4fe1\u606f\u7684\u68ee\u6797\u76f8\u4f3c\u5ea6\u6784\u5efa\u8bed\u4e49\u8868\u793a\uff1b(2)\u5feb\u901f\u5c42\u6b21\u5316\u8bed\u4e49\u4f20\u8f93\u673a\u5236\u5b9e\u73b0\u8de8\u57df\u5bf9\u9f50\u3002\u8be5\u65b9\u6cd5\u5728\u5b66\u4e60\u9636\u6bb5\u5148\u4f18\u5316\u57df\u5185\u7ed3\u6784\uff0c\u518d\u6267\u884c\u5bf9\u9f50\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFoSTA\u5728\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u5e94\u5173\u7cfb\u6062\u590d\u548c\u6807\u7b7e\u8f6c\u79fb\u7cbe\u5ea6\uff0c\u5e76\u5728\u5355\u7ec6\u80de\u5e94\u7528\u7684\u6279\u6b21\u6821\u6b63\u548c\u751f\u7269\u4fdd\u5b88\u6027\u4efb\u52a1\u4e2d\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u3002", "conclusion": "FoSTA\u6210\u529f\u5229\u7528\u68ee\u6797\u51e0\u4f55\u7ed3\u6784\u6291\u5236\u566a\u58f0\u3001\u63d0\u53d6\u4efb\u52a1\u76f8\u5173\u6d41\u5f62\uff0c\u4e3a\u591a\u6a21\u6001\u6570\u636e\u5bf9\u9f50\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2602.00987", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00987", "abs": "https://arxiv.org/abs/2602.00987", "authors": ["Sawan Kumar", "Souvik Chakraborty"], "title": "Scalable Random Wavelet Features: Efficient Non-Stationary Kernel Approximation with Convergence Guarantees", "comment": "Accepted at ICLR 2026", "summary": "Modeling non-stationary processes, where statistical properties vary across the input domain, is a critical challenge in machine learning; yet most scalable methods rely on a simplifying assumption of stationarity. This forces a difficult trade-off: use expressive but computationally demanding models like Deep Gaussian Processes, or scalable but limited methods like Random Fourier Features (RFF). We close this gap by introducing Random Wavelet Features (RWF), a framework that constructs scalable, non-stationary kernel approximations by sampling from wavelet families. By harnessing the inherent localization and multi-resolution structure of wavelets, RWF generates an explicit feature map that captures complex, input-dependent patterns. Our framework provides a principled way to generalize RFF to the non-stationary setting and comes with a comprehensive theoretical analysis, including positive definiteness, unbiasedness, and uniform convergence guarantees. We demonstrate empirically on a range of challenging synthetic and real-world datasets that RWF outperforms stationary random features and offers a compelling accuracy-efficiency trade-off against more complex models, unlocking scalable and expressive kernel methods for a broad class of real-world non-stationary problems.", "AI": {"tldr": "\u63d0\u51faRandom Wavelet Features (RWF)\u6846\u67b6\uff0c\u901a\u8fc7\u5c0f\u6ce2\u91c7\u6837\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u975e\u5e73\u7a33\u6838\u8fd1\u4f3c\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u6355\u83b7\u8f93\u5165\u4f9d\u8d56\u7684\u590d\u6742\u6a21\u5f0f\uff0c\u7406\u8bba\u63d0\u4f9b\u6b63\u5b9a\u6027\u3001\u65e0\u504f\u6027\u548c\u4e00\u81f4\u6536\u655b\u4fdd\u8bc1\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u5e73\u7a33\u968f\u673a\u7279\u5f81\u65b9\u6cd5", "motivation": "\u73b0\u6709\u53ef\u6269\u5c55\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u5e73\u7a33\u6027\u5047\u8bbe\uff0c\u5728\u5efa\u6a21\u975e\u5e73\u7a33\u8fc7\u7a0b\u65f6\u9762\u4e34\u8868\u8fbe\u6027\u4e0e\u6548\u7387\u7684\u6743\u8861\uff1a\u6df1\u5c42\u9ad8\u65af\u8fc7\u7a0b\u8868\u8fbe\u5f3a\u4f46\u8ba1\u7b97\u6602\u8d35\uff0c\u968f\u673a\u5085\u91cc\u53f6\u7279\u5f81(RFF)\u53ef\u6269\u5c55\u4f46\u80fd\u529b\u6709\u9650\uff0c\u4e9f\u9700\u586b\u8865\u8fd9\u4e00\u7a7a\u767d", "method": "\u5229\u7528\u5c0f\u6ce2\u5bb6\u65cf\u7684\u56fa\u6709\u5c40\u90e8\u5316\u548c\u591a\u5206\u8fa8\u7387\u7ed3\u6784\uff0c\u901a\u8fc7\u91c7\u6837\u6784\u5efa\u663e\u5f0f\u7279\u5f81\u6620\u5c04\uff0c\u5c06RFF\u63a8\u5e7f\u5230\u975e\u5e73\u7a33\u573a\u666f\uff0c\u63d0\u4f9b\u5b8c\u6574\u7684\u7406\u8bba\u5206\u6790\u6846\u67b6", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cRWF\u6027\u80fd\u8d85\u8d8a\u5e73\u7a33\u968f\u673a\u7279\u5f81\uff0c\u76f8\u6bd4\u590d\u6742\u6a21\u578b\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u7387-\u6548\u7387\u6743\u8861\uff0c\u89e3\u9501\u4e86\u53ef\u6269\u5c55\u8868\u8fbe\u6838\u65b9\u6cd5\u7684\u65b0\u53ef\u80fd", "conclusion": "RWF\u6210\u529f\u6865\u63a5\u4e86\u8868\u8fbe\u6027\u4e0e\u53ef\u6269\u5c55\u6027\u7684\u9e3f\u6c9f\uff0c\u4e3a\u975e\u5e73\u7a33\u95ee\u9898\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u62d3\u5c55\u4e86\u6838\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9002\u7528\u8303\u56f4"}}
{"id": "2602.01005", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01005", "abs": "https://arxiv.org/abs/2602.01005", "authors": ["Deepak Bastola", "Pitambar Acharya", "Dipak Dulal", "Rabina Dhakal", "Yang Li"], "title": "Predicting Anemia Among Under-Five Children in Nepal Using Machine Learning and Deep Learning", "comment": "13 pages and submission to Public Health Nutrition is in progress", "summary": "Childhood anemia remains a major public health challenge in Nepal and is associated with impaired growth, cognition, and increased morbidity. Using World Health Organization hemoglobin thresholds, we defined anemia status for children aged 6-59 months and formulated a binary classification task by grouping all anemia severities as \\emph{anemic} versus \\emph{not anemic}. We analyzed Nepal Demographic and Health Survey (NDHS 2022) microdata comprising 1,855 children and initially considered 48 candidate features spanning demographic, socioeconomic, maternal, and child health characteristics. To obtain a stable and substantiated feature set, we applied four features selection techniques (Chi-square, mutual information, point-biserial correlation, and Boruta) and prioritized features supported by multi-method consensus. Five features: child age, recent fever, household size, maternal anemia, and parasite deworming were consistently selected by all methods, while amenorrhea, ethnicity indicators, and provinces were frequently retained. We then compared eight traditional machine learning classifiers (LR, KNN, DT, RF, XGBoost, SVM, NB, LDA) with two deep learning models (DNN and TabNet) using standard evaluation metrics, emphasizing F1-score and recall due to class imbalance. Among all models, logistic regression attained the best recall (0.701) and the highest F1-score (0.649), while DNN achieved the highest accuracy (0.709), and SVM yielded the strongest discrimination with the highest AUC (0.736). Overall, the results indicate that both machine learning and deep learning models can provide competitive anemia prediction and the interpretable features such as child age, infection proxy, maternal anemia, and deworming history are central for risk stratification and public health screening in Nepal.", "AI": {"tldr": "This paper uses machine learning to predict childhood anemia in Nepal using demographic and health survey data. Logistic regression performed best overall, identifying key risk factors like child age, recent fever, maternal anemia, and deworming history.", "motivation": "Childhood anemia is a major public health challenge in Nepal, causing impaired growth, cognition, and increased morbidity. There's a need for effective prediction models to enable early screening and intervention.", "method": "Used NDHS 2022 data (1,855 children aged 6-59 months) with WHO hemoglobin thresholds for binary classification. Applied four feature selection methods (Chi-square, mutual information, point-biserial correlation, Boruta) with multi-method consensus. Compared 8 traditional ML classifiers (LR, KNN, DT, RF, XGBoost, SVM, NB, LDA) against 2 deep learning models (DNN, TabNet), evaluating with F1-score and recall due to class imbalance.", "result": "Five features consistently selected: child age, recent fever, household size, maternal anemia, and parasite deworming. Logistic regression achieved best recall (0.701) and highest F1-score (0.649). DNN had highest accuracy (0.709) while SVM had highest AUC (0.736). Both ML and DL models showed competitive performance.", "conclusion": "Machine learning and deep learning models can effectively predict childhood anemia in Nepal. The interpretable features identified (child age, infection proxy, maternal anemia, deworming history) are crucial for risk stratification and public health screening programs."}}
{"id": "2602.01009", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01009", "abs": "https://arxiv.org/abs/2602.01009", "authors": ["Haoran Li", "Chenhan Xiao", "Lihao Mai", "Yang Weng", "Erik Blasch"], "title": "LASS-ODE: Scaling ODE Computations to Connect Foundation Models with Dynamical Physical Systems", "comment": null, "summary": "Foundation models have transformed language, vision, and time series data analysis, yet progress on dynamic predictions for physical systems remains limited. Given the complexity of physical constraints, two challenges stand out. $(i)$ Physics-computation scalability: physics-informed learning can enforce physical regularization, but its computation (e.g., ODE integration) does not scale to extensive systems. $(ii)$ Knowledge-sharing efficiency: the attention mechanism is primarily computed within each system, which limits the extraction of shared ODE structures across systems. We show that enforcing ODE consistency does not require expensive nonlinear integration: a token-wise locally linear ODE representation preserves physical fidelity while scaling to foundation-model regimes. Thus, we propose novel token representations that respect locally linear ODE evolution. Such linearity substantially accelerates integration while accurately approximating the local data manifold. Second, we introduce a simple yet effective inter-system attention that augments attention with a common structure hub (CSH) that stores shared tokens and aggregates knowledge across systems. The resulting model, termed LASS-ODE (\\underline{LA}rge-\\underline{S}cale \\underline{S}mall \\underline{ODE}), is pretrained on our $40$GB ODE trajectory collections to enable strong in-domain performance, zero-shot generalization across diverse ODE systems, and additional improvements through fine-tuning.", "AI": {"tldr": "This paper proposes LASS-ODE, a foundation model for dynamic predictions of physical systems that addresses physics-computation scalability and knowledge-sharing efficiency through locally linear ODE representations and a common structure hub (CSH) for inter-system attention.", "motivation": "Existing foundation models show limited progress in dynamic predictions for physical systems due to two key challenges: (1) Physics-computation scalability issues where physics-informed learning's ODE integration doesn't scale to large systems, and (2) Inefficient knowledge-sharing as attention mechanisms operate within individual systems without extracting shared ODE structures across systems.", "method": "The authors introduce two innovations: (1) Token-wise locally linear ODE representations that maintain physical fidelity while enabling scalable computation by avoiding expensive nonlinear integration, and (2) An inter-system attention mechanism augmented with a Common Structure Hub (CSH) that stores shared tokens and aggregates cross-system knowledge.", "result": "LASS-ODE achieves strong in-domain performance after pretraining on a 40GB ODE trajectory dataset, demonstrates zero-shot generalization across diverse ODE systems, and shows additional improvements through fine-tuning. The linear ODE approach substantially accelerates integration while accurately approximating local data manifolds.", "conclusion": "The proposed approach successfully scales physics-informed learning to foundation-model regimes by combining locally linear ODE representations with efficient cross-system knowledge sharing, enabling effective dynamic predictions for physical systems with improved scalability and generalization capabilities."}}
{"id": "2602.01017", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01017", "abs": "https://arxiv.org/abs/2602.01017", "authors": ["Fuxin Wang", "Amr Alazali", "Yiqiao Zhong"], "title": "How Does Unfaithful Reasoning Emerge from Autoregressive Training? A Study of Synthetic Experiments", "comment": "25 pages, 23 figures", "summary": "Chain-of-thought (CoT) reasoning generated by large language models (LLMs) is often unfaithful: intermediate steps can be logically inconsistent or fail to reflect the causal relationship leading to the final answer. Despite extensive empirical observations, a fundamental understanding of CoT is lacking--what constitutes faithful CoT reasoning, and how unfaithfulness emerges from autoregressive training. We study these questions using well-controlled synthetic experiments, training small transformers on noisy data to solve modular arithmetic expressions step by step, a task we term Arithmetic Expression Reasoning. We find that models can learn faithful reasoning that causally follows the underlying arithmetic rules, but only when the training noise is below a critical threshold, a phenomenon attributable to simplicity bias. At higher noise levels, training dynamics exhibit a transition from faithful stepwise reasoning to unfaithful skip-step reasoning via an intermediate mixed mode characterized by a transient increase in prediction entropy. Mechanistic analysis reveals that models learn to encode internal uncertainty by resolving inconsistent reasoning steps, which suggests the emergence of implicit self-verification from autoregressive training.", "AI": {"tldr": "This paper studies why Chain-of-thought reasoning in LLMs is often unfaithful. Through synthetic experiments training small transformers on modular arithmetic, they discover a critical noise threshold: below it, models learn faithful stepwise reasoning; above it, they transition to unfaithful skip-step reasoning. The models show implicit self-verification by encoding internal uncertainty.", "motivation": "Chain-of-thought reasoning generated by LLMs is often unfaithful with logical inconsistencies, yet there's a lack of fundamental understanding about what constitutes faithful CoT reasoning and how unfaithfulness emerges from autoregressive training.", "method": "Controlled synthetic experiments training small transformers on noisy modular arithmetic data (Arithmetic Expression Reasoning task) to solve expressions step by step.", "result": "(1) Models learn faithful reasoning only when training noise is below a critical threshold due to simplicity bias; (2) At higher noise, a transition occurs from faithful stepwise \u2192 mixed mode (with increased entropy) \u2192 unfaithful skip-step reasoning; (3) Models encode internal uncertainty by resolving inconsistent steps, suggesting implicit self-verification emerges from autoregressive training.", "conclusion": "Faithful CoT reasoning depends critically on training data quality/noise levels, and autoregressive training can implicitly develop self-verification capabilities through uncertainty encoding, providing mechanistic insights into CoT faithfulness."}}
{"id": "2602.01027", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01027", "abs": "https://arxiv.org/abs/2602.01027", "authors": ["Xin Nie", "Haicheng Zhang", "Liang Dong", "Beining Feng", "Jinhong Weng", "Guiling Sun"], "title": "SFMP: Fine-Grained, Hardware-Friendly and Search-Free Mixed-Precision Quantization for Large Language Models", "comment": "24pages,17figures", "summary": "Mixed-precision quantization is a promising approach for compressing large language models under tight memory budgets. However, existing mixed-precision methods typically suffer from one of two limitations: they either rely on expensive discrete optimization to determine precision allocation, or introduce hardware inefficiencies due to irregular memory layouts. We propose SFMP, a search-free and hardware-friendly mixed-precision quantization framework for large language models. The framework is built upon four novel ideas: Fractional bit-width, which extends integer bit-width for weight matrix to fractional value and transforms discrete precision allocation as a continuous problem; 2)Block-wise mixed-precision, enabling fine-grained precision within weight matrices while remaining hardware-friendly; 3)Row-column weight reordering, which aggregates salient weights via row and column reordering, incurring only a small activation reordering overhead during inference; 4)Unified GEMM kernel, which supports mixed-precision GEMM at arbitrary average bit-width. Extensive experiments demonstrate that SFMP outperforms state-of-the-art layer-wise mixed-precision methods under the same memory constraints, while significantly reducing quantization cost and improving inference efficiency. Code is available at https://github.com/Nkniexin/SFMP", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faSFMP\uff0c\u4e00\u79cd\u514d\u641c\u7d22\u4e14\u786c\u4ef6\u53cb\u597d\u7684\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6570\u4f4d\u5bbd\u3001\u5757\u7ea7\u6df7\u5408\u7cbe\u5ea6\u3001\u884c\u5217\u6743\u91cd\u91cd\u6392\u548c\u7edf\u4e00GEMM\u6838\uff0c\u5728\u76f8\u540c\u5185\u5b58\u7ea6\u675f\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u964d\u4f4e\u91cf\u5316\u6210\u672c\u5e76\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u65b9\u6cd5\u5b58\u5728\u4e24\u5927\u5c40\u9650\uff1a\u8981\u4e48\u4f9d\u8d56\u6602\u8d35\u7684\u79bb\u6563\u4f18\u5316\u6765\u786e\u5b9a\u7cbe\u5ea6\u5206\u914d\uff0c\u8981\u4e48\u56e0\u4e0d\u89c4\u5219\u5185\u5b58\u5e03\u5c40\u5f15\u5165\u786c\u4ef6\u4f4e\u6548\u95ee\u9898\u3002", "method": "\u8be5\u6846\u67b6\u57fa\u4e8e\u56db\u4e2a\u521b\u65b0\u70b9\uff1a1) \u5206\u6570\u4f4d\u5bbd\uff0c\u5c06\u6574\u6570\u4f4d\u5bbd\u6269\u5c55\u81f3\u5206\u6570\u503c\uff0c\u5c06\u79bb\u6563\u7cbe\u5ea6\u5206\u914d\u8f6c\u5316\u4e3a\u8fde\u7eed\u95ee\u9898\uff1b2) \u5757\u7ea7\u6df7\u5408\u7cbe\u5ea6\uff0c\u5728\u4fdd\u6301\u786c\u4ef6\u53cb\u597d\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u6743\u91cd\u77e9\u9635\u5185\u7684\u7ec6\u7c92\u5ea6\u7cbe\u5ea6\uff1b3) \u884c\u5217\u6743\u91cd\u91cd\u6392\uff0c\u901a\u8fc7\u884c\u5217\u91cd\u6392\u805a\u5408\u663e\u8457\u6743\u91cd\uff0c\u63a8\u7406\u65f6\u4ec5\u4ea7\u751f\u5c11\u91cf\u6fc0\u6d3b\u91cd\u6392\u5f00\u9500\uff1b4) \u7edf\u4e00GEMM\u6838\uff0c\u652f\u6301\u4efb\u610f\u5e73\u5747\u4f4d\u5bbd\u7684\u6df7\u5408\u7cbe\u5ea6GEMM\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSFMP\u5728\u76f8\u540c\u5185\u5b58\u7ea6\u675f\u4e0b\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5c42\u7ea7\u6df7\u5408\u7cbe\u5ea6\u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u91cf\u5316\u6210\u672c\u5e76\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "conclusion": "SFMP\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u514d\u641c\u7d22\u3001\u786c\u4ef6\u53cb\u597d\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2602.01039", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01039", "abs": "https://arxiv.org/abs/2602.01039", "authors": ["Zhiwei Ling", "Hailiang Zhao", "Chao Zhang", "Xiang Ao", "Ziqi Wang", "Cheng Zhang", "Zhen Qin", "Xinkui Zhao", "Kingsum Chow", "Yuanqing Wu", "MengChu Zhou"], "title": "Adaptive Dual-Weighting Framework for Federated Learning via Out-of-Distribution Detection", "comment": null, "summary": "Federated Learning (FL) enables collaborative model training across large-scale distributed service nodes while preserving data privacy, making it a cornerstone of intelligent service systems in edge-cloud environments. However, in real-world service-oriented deployments, data generated by heterogeneous users, devices, and application scenarios are inherently non-IID. This severe data heterogeneity critically undermines the convergence stability, generalization ability, and ultimately the quality of service delivered by the global model. To address this challenge, we propose FLood, a novel FL framework inspired by out-of-distribution (OOD) detection. FLood dynamically counteracts the adverse effects of heterogeneity through a dual-weighting mechanism that jointly governs local training and global aggregation. At the client level, it adaptively reweights the supervised loss by upweighting pseudo-OOD samples, thereby encouraging more robust learning from distributionally misaligned or challenging data. At the server level, it refines model aggregation by weighting client contributions according to their OOD confidence scores, prioritizing updates from clients with higher in-distribution consistency and enhancing the global model's robustness and convergence stability. Extensive experiments across multiple benchmarks under diverse non-IID settings demonstrate that FLood consistently outperforms state-of-the-art FL methods in both accuracy and generalization. Furthermore, FLood functions as an orthogonal plug-in module: it seamlessly integrates with existing FL algorithms to boost their performance under heterogeneity without modifying their core optimization logic. These properties make FLood a practical and scalable solution for deploying reliable intelligent services in real-world federated environments.", "AI": {"tldr": "FLood\u662f\u4e00\u4e2a\u53d7\u5206\u5e03\u5916(OOD)\u68c0\u6d4b\u542f\u53d1\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u91cd\u52a0\u6743\u673a\u5236\u52a8\u6001\u5bf9\u6297\u6570\u636e\u5f02\u8d28\u6027\uff1a\u5728\u5ba2\u6237\u7aef\u5c42\u9762\u901a\u8fc7\u4e0a\u91cd\u52a0\u6743\u4f2aOOD\u6837\u672c\u6765\u589e\u5f3a\u9c81\u68d2\u5b66\u4e60\uff0c\u5728\u670d\u52a1\u5668\u5c42\u9762\u6839\u636eOOD\u7f6e\u4fe1\u5ea6\u5206\u6570\u52a0\u6743\u5ba2\u6237\u7aef\u8d21\u732e\u4ee5\u63d0\u5347\u5168\u5c40\u6a21\u578b\u9c81\u68d2\u6027\u3002\u8be5\u6846\u67b6\u53ef\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\u96c6\u6210\u5230\u73b0\u6709FL\u7b97\u6cd5\u4e2d\uff0c\u5728\u591a\u4e2a\u975eIID\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u8fb9\u7f18\u4e91\u73af\u5883\u4e2d\u5bf9\u9690\u79c1\u4fdd\u62a4\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u5b9e\u5e94\u7528\u4e2d\u5f02\u6784\u7528\u6237\u3001\u8bbe\u5907\u548c\u5e94\u7528\u573a\u666f\u4ea7\u751f\u7684\u6570\u636e\u672c\u8d28\u4e0a\u662f\u975e\u72ec\u7acb\u540c\u5206\u5e03(non-IID)\u7684\u3002\u8fd9\u79cd\u4e25\u91cd\u7684\u6570\u636e\u5f02\u8d28\u6027\u4e25\u91cd\u7834\u574f\u4e86\u5168\u5c40\u6a21\u578b\u7684\u6536\u655b\u7a33\u5b9a\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u670d\u52a1\u8d28\u91cf\uff0c\u4e9f\u9700\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faFLood\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u91cd\u52a0\u6743\u673a\u5236\uff1a1) \u5ba2\u6237\u7aef\u5c42\u9762\uff1a\u901a\u8fc7\u4e0a\u91cd\u52a0\u6743\u4f2aOOD\u6837\u672c\u81ea\u9002\u5e94\u8c03\u6574\u76d1\u7763\u635f\u5931\uff0c\u9f13\u52b1\u4ece\u5206\u5e03\u5931\u914d\u6216\u6311\u6218\u6027\u6570\u636e\u4e2d\u5b66\u4e60\u66f4\u9c81\u68d2\u6a21\u578b\uff1b2) \u670d\u52a1\u5668\u5c42\u9762\uff1a\u6839\u636eOOD\u7f6e\u4fe1\u5ea6\u5206\u6570\u7ec6\u5316\u5ba2\u6237\u7aef\u8d21\u732e\u6743\u91cd\uff0c\u4f18\u5148\u9009\u62e9\u5177\u6709\u66f4\u9ad8\u5206\u5e03\u5185\u4e00\u81f4\u6027\u7684\u5ba2\u6237\u7aef\u66f4\u65b0\u3002\u8be5\u6846\u67b6\u53ef\u4f5c\u4e3a\u6b63\u4ea4\u63d2\u4ef6\u6a21\u5757\u65e0\u7f1d\u96c6\u6210\u73b0\u6709FL\u7b97\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u7684\u591a\u79cd\u975eIID\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFLood\u5728\u51c6\u786e\u7387\u548c\u6cdb\u5316\u6027\u80fd\u4e0a\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u5b83\u65e0\u9700\u4fee\u6539\u6838\u5fc3\u4f18\u5316\u903b\u8f91\u5373\u53ef\u63d0\u5347\u73b0\u6709\u7b97\u6cd5\u5728\u5f02\u8d28\u6027\u4e0b\u7684\u6027\u80fd\u3002", "conclusion": "FLood\u6210\u4e3a\u5728\u5b9e\u9645\u8054\u90a6\u73af\u5883\u4e2d\u90e8\u7f72\u53ef\u9760\u667a\u80fd\u670d\u52a1\u7684\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u5f02\u8d28\u6027\u6311\u6218\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u517c\u5bb9\u6027\u548c\u6613\u7528\u6027\u3002"}}
{"id": "2602.01053", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01053", "abs": "https://arxiv.org/abs/2602.01053", "authors": ["Hyesung Jeon", "Hyeongju Ha", "Jae-Joon Kim"], "title": "LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents", "comment": "23 pages, 9 figures, 19 tables", "summary": "Role specialization in multi-LLM agent systems is often realized via multi-LoRA, where agents share a pretrained backbone and differ only through lightweight adapters. Despite sharing base model weights, each agent independently builds and stores its own KV cache for the same long, tool-augmented trajectories, incurring substantial memory and compute overhead. Existing KV cache sharing methods largely overlook this multi-LoRA setting. We observe that, across agents, cache differences are dominated by adapter outputs, while activations from the shared pretrained backbone remain highly similar. Based on this observation, we propose LRAgent, a KV cache sharing framework for multi-LoRA agents that decomposes the cache into a shared base component from the pretrained weights and an adapter-dependent component from LoRA weights. LRAgent reduces memory overhead by sharing the base component and storing the adapter component in its inherent low-rank form, and further reduces compute overhead, enabled by shared-$A$ multi-LoRA architectures, by also sharing the low-rank cache and avoiding redundant computations for contexts already processed by other agents. To efficiently reconstruct adapter contributions at runtime, we introduce Flash-LoRA-Attention, a kernel that reorders attention computation to avoid materializing the low-rank cache to full dimension. LRAgent achieves throughput and time-to-first-token latency close to fully shared caching, while preserving accuracy near the non-shared caching baseline across agentic question-answering benchmarks.", "AI": {"tldr": "LRAgent is a KV cache sharing framework for multi-LoRA agent systems that decomposes caches into shared base components and low-rank adapter components, reducing memory and compute overhead while maintaining accuracy.", "motivation": "Multi-LoRA agent systems suffer from redundant KV caches where each agent stores its own cache despite sharing a pretrained backbone, causing significant memory and compute overhead. Existing KV cache sharing methods don't effectively address this multi-LoRA setting.", "method": "The paper proposes LRAgent which: (1) decomposes KV cache into shared base component and adapter-dependent low-rank component, (2) shares the base component across agents, (3) stores adapter components in low-rank form, and (4) uses Flash-LoRA-Attention kernel for efficient runtime reconstruction.", "result": "LRAgent achieves throughput and time-to-first-token latency close to fully shared caching while preserving accuracy near the non-shared caching baseline on agentic question-answering benchmarks.", "conclusion": "LRAgent successfully reduces memory and compute overhead in multi-LoRA agent systems through intelligent KV cache decomposition and sharing, offering near-optimal performance with minimal accuracy loss."}}
{"id": "2602.01058", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01058", "abs": "https://arxiv.org/abs/2602.01058", "authors": ["Dylan Zhang", "Yufeng Xu", "Haojin Wang", "Qingzhi Chen", "Hao Peng"], "title": "Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning", "comment": null, "summary": "Post-training of reasoning LLMs is a holistic process that typically consists of an offline SFT stage followed by an online reinforcement learning (RL) stage. However, SFT is often optimized in isolation to maximize SFT performance alone.\n  We show that, after identical RL training, models initialized from stronger SFT checkpoints can significantly underperform those initialized from weaker ones. We attribute this to a mismatch typical in current SFT-RL pipelines: the distribution that generates the offline SFT data can differ substantially from the policy optimized during online RL, which learns from its own rollouts.\n  We propose PEAR (Policy Evaluation-inspired Algorithm for Offline Learning Loss Re-weighting), an SFT-stage method that corrects this mismatch and better prepares the model for RL. PEAR uses importance sampling to reweight the SFT loss, with three variants operating at the token, block, and sequence levels. It can be used to augment standard SFT objectives and incurs little additional training overhead once probabilities for the offline data are collected.\n  We conduct controlled experiments on verifiable reasoning games and mathematical reasoning tasks on Qwen 2.5 and 3 and DeepSeek-distilled models. PEAR consistently improves post-RL performance over canonical SFT, with pass at 8 gains up to a 14.6 percent on AIME2025. Our results suggest that PEAR is an effective step toward more holistic LLM post-training by designing and evaluating SFT with downstream RL in mind rather than in isolation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u53d1\u73b0LLM\u540e\u8bad\u7ec3\u4e2dSFT\u4e0eRL\u9636\u6bb5\u5b58\u5728\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5bfc\u81f4\u66f4\u5f3a\u7684SFT\u521d\u59cb\u5316\u53cd\u800c\u635f\u5bb3RL\u6548\u679c\u3002\u63d0\u51faPEAR\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u8981\u6027\u91c7\u6837\u91cd\u52a0\u6743SFT\u635f\u5931\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u5b9e\u73b0AIME2025 pass@8\u6700\u9ad814.6%\u7684\u7edd\u5bf9\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u63a8\u7406\u5927\u6a21\u578b\u7684\u540e\u8bad\u7ec3\u6d41\u7a0b\u4e2d\uff0c\u79bb\u7ebfSFT\u548c\u5728\u7ebfRL\u9636\u6bb5\u901a\u5e38\u5b64\u7acb\u4f18\u5316\uff0c\u9020\u6210SFT\u6570\u636e\u751f\u6210\u5206\u5e03\u4e0eRL\u4f18\u5316\u7b56\u7565\u5206\u5e03\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u3002\u8fd9\u79cd\u4e0d\u5339\u914d\u5bfc\u81f4\u7ecf\u8fc7\u76f8\u540cRL\u8bad\u7ec3\u540e\uff0c\u4ece\u66f4\u5f3aSFT\u68c0\u67e5\u70b9\u521d\u59cb\u5316\u7684\u6a21\u578b\u53cd\u800c\u6bd4\u4ece\u8f83\u5f31\u68c0\u67e5\u70b9\u521d\u59cb\u5316\u7684\u6a21\u578b\u8868\u73b0\u66f4\u5dee\u3002", "method": "\u63d0\u51faPEAR\uff08\u57fa\u4e8e\u7b56\u7565\u8bc4\u4f30\u7684\u79bb\u7ebf\u5b66\u4e60\u635f\u5931\u91cd\u52a0\u6743\u7b97\u6cd5\uff09\uff0c\u4f7f\u7528\u91cd\u8981\u6027\u91c7\u6837\u5728token\u3001block\u548csequence\u4e09\u4e2a\u5c42\u9762\u5bf9SFT\u635f\u5931\u8fdb\u884c\u91cd\u52a0\u6743\u3002\u8be5\u65b9\u6cd5\u53ef\u4ee5\u589e\u5f3a\u6807\u51c6SFT\u76ee\u6807\uff0c\u53ea\u9700\u6536\u96c6\u79bb\u7ebf\u6570\u636e\u7684\u6982\u7387\uff0c\u51e0\u4e4e\u4e0d\u589e\u52a0\u989d\u5916\u8bad\u7ec3\u5f00\u9500\u3002", "result": "\u5728\u53ef\u9a8c\u8bc1\u63a8\u7406\u6e38\u620f\u548c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\uff0c\u5bf9Qwen 2.5/3\u548cDeepSeek\u84b8\u998f\u6a21\u578b\u8fdb\u884c\u7684\u63a7\u5236\u5b9e\u9a8c\u8868\u660e\uff0cPEAR\u5728RL\u540e\u6301\u7eed\u4f18\u4e8e\u6807\u51c6SFT\uff0c\u5728AIME2025\u4e0a\u8fbe\u5230\u6700\u9ad814.6%\u7684pass@8\u7edd\u5bf9\u589e\u76ca\u3002", "conclusion": "PEAR\u901a\u8fc7\u5c06\u4e0b\u6e38RL\u76ee\u6807\u7eb3\u5165SFT\u8bbe\u8ba1\u800c\u975e\u5b64\u7acb\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u66f4\u6574\u4f53\u5316\u7684LLM\u540e\u8bad\u7ec3\uff0c\u6709\u6548\u89e3\u51b3\u4e86SFT-RL\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\u3002"}}
{"id": "2602.01083", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01083", "abs": "https://arxiv.org/abs/2602.01083", "authors": ["Adir Dayan", "Yam Eitan", "Haggai Maron"], "title": "On the Expressive Power of Permutation-Equivariant Weight-Space Networks", "comment": null, "summary": "Weight-space learning studies neural architectures that operate directly on the parameters of other neural networks. Motivated by the growing availability of pretrained models, recent work has demonstrated the effectiveness of weight-space networks across a wide range of tasks. SOTA weight-space networks rely on permutation-equivariant designs to improve generalization. However, this may negatively affect expressive power, warranting theoretical investigation. Importantly, unlike other structured domains, weight-space learning targets maps operating on both weight and function spaces, making expressivity analysis particularly subtle. While a few prior works provide partial expressivity results, a comprehensive characterization is still missing. In this work, we address this gap by developing a systematic theory for expressivity of weight-space networks. We first prove that all prominent permutation-equivariant networks are equivalent in expressive power. We then establish universality in both weight- and function-space settings under mild, natural assumptions on the input weights, and characterize the edge-case regimes where universality no longer holds. Together, these results provide a strong and unified foundation for the expressivity of weight-space networks.", "AI": {"tldr": "This paper develops a systematic theory for the expressivity of weight-space networks, proving that prominent permutation-equivariant architectures have equivalent expressive power and establishing universality conditions under mild assumptions while characterizing edge cases where universality fails.", "motivation": "Despite the growing availability of pretrained models and demonstrated effectiveness of weight-space networks, a comprehensive theoretical characterization of their expressive power is missing. SOTA networks rely on permutation-equivariant designs that may limit expressivity, and the problem is particularly subtle because these networks operate on both weight and function spaces.", "method": "The authors develop a systematic theoretical framework to analyze expressivity. They first prove equivalence among prominent permutation-equivariant architectures, then establish universality results for both weight-space and function-space settings under natural assumptions on input weights.", "result": "The study proves that all prominent permutation-equivariant weight-space networks have equivalent expressive power. It establishes universality in both weight- and function-space settings under mild assumptions, while also characterizing the specific edge-case regimes where universality no longer holds.", "conclusion": "These results provide a strong, unified theoretical foundation for understanding the expressivity of weight-space networks, addressing a critical gap in the literature and clarifying the trade-offs between permutation-equivariance and expressive power."}}
{"id": "2602.01105", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01105", "abs": "https://arxiv.org/abs/2602.01105", "authors": ["Zixiao Wang", "Yifei Shen", "Huishuai Zhang"], "title": "OLion: Approaching the Hadamard Ideal by Intersecting Spectral and $\\ell_{\\infty}$ Implicit Biases", "comment": "23 pages", "summary": "Many optimizers can be interpreted as steepest-descent methods under norm-induced geometries, and thus inherit corresponding implicit biases. We introduce \\nameA{} (\\fullname{}), which combines spectral control from orthogonalized update directions with $\\ell_\\infty$-style coordinate control from sign updates. \\nameA{} forms a Lion-style momentum direction, approximately orthogonalizes it via a few Newton--Schulz iterations, and then applies an entrywise sign, providing an efficient approximation to taking a maximal step over the intersection of the spectral and $\\ell_\\infty$ constraint sets (a scaled Hadamard-like set for matrix parameters). Despite the strong nonlinearity of orthogonalization and sign, we prove convergence under a mild, empirically verified diagonal-isotropy assumption. Across large-scale language and vision training, including GPT-2 and Llama pretraining, SiT image pretraining, and supervised fine-tuning, \\nameA{} matches or outperforms AdamW and Muon under comparable tuning while using only momentum-level optimizer state, and it mitigates optimizer mismatch when fine-tuning AdamW-pretrained checkpoints.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u4f18\u5316\u5668\\nameA{}\uff0c\u7ed3\u5408\u6b63\u4ea4\u5316\u66f4\u65b0\u7684\u8c31\u63a7\u5236\u4e0e\u7b26\u53f7\u66f4\u65b0\u7684\u2113\u221e\u5750\u6807\u63a7\u5236\uff0c\u5728\u5927\u89c4\u6a21\u8bed\u8a00\u548c\u89c6\u89c9\u8bad\u7ec3\u4e2d\u5339\u914d\u6216\u8d85\u8d8aAdamW\u548cMuon\uff0c\u4e14\u4ec5\u9700\u52a8\u91cf\u7ea7\u72b6\u6001", "motivation": "\u73b0\u6709\u4f18\u5316\u5668\u53ef\u89e3\u91ca\u4e3a\u8303\u6570\u8bf1\u5bfc\u51e0\u4f55\u4e0b\u7684\u6700\u901f\u4e0b\u964d\u6cd5\u5e76\u7ee7\u627f\u9690\u5f0f\u504f\u5dee\uff0c\u9700\u8bbe\u8ba1\u80fd\u540c\u65f6\u63a7\u5236\u8c31\u548c\u5750\u6807\u65b9\u5411\u7684\u65b0\u65b9\u6cd5", "method": "\u6784\u5efaLion\u5f0f\u52a8\u91cf\u65b9\u5411\u2192\u901a\u8fc7\u725b\u987f-\u8212\u5c14\u8328\u8fed\u4ee3\u8fd1\u4f3c\u6b63\u4ea4\u5316\u2192\u65bd\u52a0\u9010\u5143\u7b26\u53f7\u64cd\u4f5c\uff0c\u903c\u8fd1\u8c31\u4e0e\u2113\u221e\u7ea6\u675f\u4ea4\u96c6\u7684\u6700\u5927\u6b65\u957f", "result": "\u5728GPT-2/Llama\u9884\u8bad\u7ec3\u3001SiT\u56fe\u50cf\u9884\u8bad\u7ec3\u53ca\u5fae\u8c03\u4efb\u52a1\u4e2d\uff0c\u6027\u80fd\u5ab2\u7f8e\u6216\u4f18\u4e8eAdamW\u548cMuon\uff0c\u4ec5\u9700\u52a8\u91cf\u7ea7\u72b6\u6001\u4e14\u7f13\u89e3AdamW\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4f18\u5316\u5668\u5931\u914d", "conclusion": "\\nameA{}\u901a\u8fc7\u975e\u7ebf\u6027\u6b63\u4ea4\u5316\u4e0e\u7b26\u53f7\u64cd\u4f5c\u5b9e\u73b0\u9ad8\u6548\u7ea6\u675f\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u6536\u655b\u6027\u7684\u540c\u65f6\u63d0\u5347\u5927\u89c4\u6a21\u8bad\u7ec3\u6548\u679c"}}
{"id": "2602.01113", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01113", "abs": "https://arxiv.org/abs/2602.01113", "authors": ["Wenjie Liang", "Ranhui Yan", "Jia Cai", "You-Gan Wang"], "title": "Single-Edge Node Injection Threats to GNN-Based Security Monitoring in Industrial Graph Systems", "comment": null, "summary": "Graph neural networks (GNNs) are increasingly adopted in industrial graph-based monitoring systems (e.g., Industrial internet of things (IIoT) device graphs, power-grid topology models, and manufacturing communication networks) to support anomaly detection, state estimation, and asset classification. In such settings, an adversary that compromises a small number of edge devices may inject counterfeit nodes (e.g., rogue sensors, virtualized endpoints, or spoofed substations) to bias downstream decisions while evading topology- and homophily-based sanitization. This paper formulates deployment-oriented node-injection attacks under constrained resources and proposes the \\emph{Single-Edge Graph Injection Attack} (SEGIA), in which each injected node attaches to the operational graph through a single edge. SEGIA integrates a pruned SGC surrogate, multi-hop neighborhood sampling, and reverse graph convolution-based feature synthesis with a similarity-regularized objective to preserve local homophily and survive edge pruning. Theoretical analysis and extensive evaluations across datasets and defenses show at least $25\\%$ higher attack success than representative baselines under substantially smaller edge budgets. These results indicate a system-level risk in industrial GNN deployments and motivate lightweight admission validation and neighborhood-consistency monitoring.", "AI": {"tldr": "This paper proposes SEGIA, a node-injection attack on industrial GNNs where fake nodes connect via single edges, achieving 25% higher success than baselines while evading detection.", "motivation": "Industrial GNN-based monitoring systems are vulnerable to adversarial node injection attacks where compromised edge devices introduce counterfeit nodes to manipulate GNN decisions. Existing attacks may not be practical under strict resource constraints or fail against topology-based defenses.", "method": "The authors propose SEGIA (Single-Edge Graph Injection Attack) that injects nodes connected by only one edge. It uses a pruned SGC surrogate model, multi-hop neighborhood sampling, reverse graph convolution for feature synthesis, and a similarity-regularized objective to maintain local homophily and survive edge pruning.", "result": "SEGIA achieves at least 25% higher attack success rate than representative baseline methods while using substantially fewer edge resources. The attack effectively evades topology- and homophily-based sanitization.", "conclusion": "The study reveals significant system-level risks in industrial GNN deployments. It highlights the need for lightweight admission validation mechanisms and neighborhood-consistency monitoring to defend against such attacks."}}
{"id": "2602.01120", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01120", "abs": "https://arxiv.org/abs/2602.01120", "authors": ["Youkang Wang", "Jian Wang", "Rubing Chen", "Tianyi Zeng", "Xiao-Yong Wei", "Qing Li"], "title": "MarkovScale: Towards Optimal Sequential Scaling at Inference Time", "comment": "12 pages", "summary": "Sequential scaling is a prominent inference-time scaling paradigm, yet its performance improvements are typically modest and not well understood, largely due to the prevalence of heuristic, non-principled approaches that obscure clear optimality bounds. To address this, we propose a principled framework that models sequential scaling as a two-state Markov process. This approach reveals the underlying properties of sequential scaling and yields closed-form solutions for essential aspects, such as the specific conditions under which accuracy is improved and the theoretical upper, neutral, and lower performance bounds. Leveraging this formulation, we develop MarkovScale, a practical system that applies these optimality criteria to achieve a theoretically grounded balance between accuracy and efficiency. Comprehensive experiments across 3 backbone LLMs, 5 benchmarks, and over 20 configurations show that MarkovScale consistently outperforms state-of-the-art parallel and sequential scaling methods, representing a significant step toward optimal and resource-efficient inference in LLMs. The source code will be open upon acceptance at https://open-upon-acceptance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4e24\u72b6\u6001\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\u7684\u5e8f\u5217\u7f29\u653e\u539f\u5219\u6027\u6846\u67b6\uff0c\u63a8\u5bfc\u51fa\u6027\u80fd\u8fb9\u754c\u6761\u4ef6\uff0c\u5e76\u5f00\u53d1\u4e86MarkovScale\u7cfb\u7edf\uff0c\u5728\u591a\u4e2a\u5b9e\u9a8c\u914d\u7f6e\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5e8f\u5217\u7f29\u653e\u5728\u63a8\u7406\u65f6\u7684\u6027\u80fd\u63d0\u5347\u6709\u9650\u4e14\u7f3a\u4e4f\u7406\u8bba\u6307\u5bfc\uff0c\u4e3b\u8981\u4f9d\u8d56\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5bfc\u81f4\u4f18\u5316\u8fb9\u754c\u4e0d\u6e05\u6670\u3002", "method": "1. \u5c06\u5e8f\u5217\u7f29\u653e\u5efa\u6a21\u4e3a\u4e24\u72b6\u6001\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\uff1b2. \u63a8\u5bfc\u95ed\u5f0f\u89e3\u63ed\u793a\u6027\u80fd\u6539\u8fdb\u6761\u4ef6\u548c\u7406\u8bba\u8fb9\u754c\uff1b3. \u57fa\u4e8e\u6b64\u5f00\u53d1MarkovScale\u7cfb\u7edf\u3002", "result": "\u57283\u4e2aLLM\u9aa8\u5e72\u30015\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c20\u591a\u79cd\u914d\u7f6e\u4e0b\uff0cMarkovScale\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5e76\u884c\u548c\u5e8f\u5217\u7f29\u653e\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3aLLM\u63a8\u7406\u63d0\u4f9b\u4e86\u7406\u8bba\u6700\u4f18\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u662f\u5411\u6700\u4f18\u63a8\u7406\u8fc8\u8fdb\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2602.01124", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01124", "abs": "https://arxiv.org/abs/2602.01124", "authors": ["Md Abrar Jahin", "Taufikur Rahman Fuad", "Jay Pujara", "Craig Knoblock"], "title": "ChronoSpike: An Adaptive Spiking Graph Neural Network for Dynamic Graphs", "comment": null, "summary": "Dynamic graph representation learning requires capturing both structural relationships and temporal evolution, yet existing approaches face a fundamental trade-off: attention-based methods achieve expressiveness at $O(T^2)$ complexity, while recurrent architectures suffer from gradient pathologies and dense state storage. Spiking neural networks offer event-driven efficiency but remain limited by sequential propagation, binary information loss, and local aggregation that misses global context. We propose ChronoSpike, an adaptive spiking graph neural network that integrates learnable LIF neurons with per-channel membrane dynamics, multi-head attentive spatial aggregation on continuous features, and a lightweight Transformer temporal encoder, enabling both fine-grained local modeling and long-range dependency capture with linear memory complexity $O(T \\cdot d)$. On three large-scale benchmarks, ChronoSpike outperforms twelve state-of-the-art baselines by $2.0\\%$ Macro-F1 and $2.4\\%$ Micro-F1 while achieving $3-10\\times$ faster training than recurrent methods with a constant 105K-parameter budget independent of graph size. We provide theoretical guarantees for membrane potential boundedness, gradient flow stability under contraction factor $\u03c1< 1$, and BIBO stability; interpretability analyses reveal heterogeneous temporal receptive fields and a learned primacy effect with $83-88\\%$ sparsity.", "AI": {"tldr": "The paper proposes ChronoSpike, an adaptive spiking graph neural network that combines learnable LIF neurons with multi-head attention and a lightweight Transformer to achieve efficient dynamic graph representation learning with linear memory complexity, outperforming 12 SOTA baselines while being 3-10\u00d7 faster.", "motivation": "Existing dynamic graph learning methods face a fundamental trade-off: attention-based methods are expressive but have O(T\u00b2) complexity, recurrent architectures suffer from gradient pathologies and dense storage, and spiking networks are limited by sequential propagation, binary information loss, and lack of global context. There is a need for a method that captures both fine-grained local patterns and long-range dependencies efficiently.", "method": "ChronoSpike integrates three innovations: (1) learnable LIF neurons with per-channel membrane dynamics for event-driven efficiency, (2) multi-head attentive spatial aggregation on continuous features to capture global context, and (3) a lightweight Transformer temporal encoder for long-range dependencies. This hybrid architecture achieves linear memory complexity O(T\u00b7d) with a constant parameter budget independent of graph size.", "result": "On three large-scale benchmarks, ChronoSpike outperforms 12 state-of-the-art baselines by 2.0% Macro-F1 and 2.4% Micro-F1, achieves 3-10\u00d7 faster training than recurrent methods, maintains a constant 105K-parameter budget, and demonstrates 83-88% sparsity with interpretable heterogeneous temporal receptive fields and a learned primacy effect.", "conclusion": "ChronoSpike successfully bridges the efficiency-expressiveness gap in dynamic graph learning, providing theoretical guarantees for membrane potential boundedness, gradient flow stability (\u03c1<1), and BIBO stability. The model demonstrates that spiking architectures can be both scalable and powerful, offering new directions for event-driven graph representation learning with strong empirical performance and interpretability."}}
{"id": "2602.01128", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01128", "abs": "https://arxiv.org/abs/2602.01128", "authors": ["Mete Erdogan"], "title": "Tangent Space Fine-Tuning for Directional Preference Alignment in Large Language Models", "comment": null, "summary": "Our goal is to enable large language models (LLMs) to balance multiple human preference dimensions; such as helpfulness, safety, and verbosity, through principled and controllable alignment. Existing preference optimization methods, including Direct Preference Optimization (DPO), collapse feedback into a single scalar reward, fixing one balance among objectives and preventing traversal of the Pareto front. Recent work by Ortiz-Jimenez et al. (2023) showed that fine-tuning can be viewed in a model's tangent space, where linearized updates act as additive vectors that can be composed to jointly perform well on multiple tasks. Building on this formulation, we extend this idea to preference alignment and propose Tangent-Space Direct Preference Optimization (TS-DPO), which performs DPO within this locally linear regime to learn per-objective update directions. These directions can be linearly combined at inference to generate user-specified behaviors without additional optimization. Evaluated on the helpfulness-verbosity trade-off using the HelpSteer and UltraFeedback datasets, TS-DPO achieves broader Pareto-optimal coverage and smoother preference control than scalarized DPO. Canonical Correlation Analysis (CCA) further shows that tangent-space training amplifies canonical directions aligned with distinct preferences, improving disentanglement.", "AI": {"tldr": "The paper proposes TS-DPO, a method that performs preference optimization in the tangent space of LLMs to learn separate update directions for different preferences (like helpfulness and verbosity), enabling linear combination of these directions at inference for controllable multi-objective alignment without extra training.", "motivation": "Existing preference optimization methods like DPO collapse multiple preference dimensions into a single scalar reward, which fixes the trade-off between objectives and prevents users from exploring different points on the Pareto front of optimal multi-objective performance.", "method": "Tangent-Space Direct Preference Optimization (TS-DPO) applies DPO in the locally linear tangent space of a model, learning per-objective update directions that can be linearly composed at inference time to generate user-specified behaviors without additional optimization.", "result": "On helpfulness-verbosity trade-offs using HelpSteer and UltraFeedback datasets, TS-DPO achieves broader Pareto-optimal coverage and smoother preference control compared to scalarized DPO, with CCA analysis confirming better disentanglement of preference directions.", "conclusion": "The tangent-space formulation enables controllable multi-objective alignment by learning composable preference directions, offering a more flexible and principled approach to balancing competing human preferences in LLMs."}}
{"id": "2602.01135", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01135", "abs": "https://arxiv.org/abs/2602.01135", "authors": ["Hugo Math", "Rainer Lienhart"], "title": "TRACE: Scalable Amortized Causal Discovery from Single Sequences via Autoregressive Density Estimation", "comment": "8 pages, 6 figures,", "summary": "We study causal discovery from a single observed sequence of discrete events generated by a stochastic process, as encountered in vehicle logs, manufacturing systems, or patient trajectories. This regime is particularly challenging due to the absence of repeated samples, high dimensionality, and long-range temporal dependencies of the single observation during inference. We introduce TRACE, a scalable framework that repurposes autoregressive models as pretrained density estimators for conditional mutual information estimation. TRACE infers the summary causal graph between event types in a sequence, scaling linearly with the event vocabulary and supporting delayed causal effects, while being fully parallel on GPUs. We establish its theoretical identifiability under imperfect autoregressive models. Experiments demonstrate robust performance across different baselines and varying vocabulary sizes including an application to root-cause analysis in vehicle diagnostics with over 29,100 event types.", "AI": {"tldr": "\u63d0\u51faTRACE\u6846\u67b6\uff0c\u5229\u7528\u81ea\u56de\u5f52\u6a21\u578b\u4f5c\u4e3a\u5bc6\u5ea6\u4f30\u8ba1\u5668\u6765\u8ba1\u7b97\u6761\u4ef6\u4e92\u4fe1\u606f\uff0c\u5b9e\u73b0\u4ece\u5355\u4e00\u79bb\u6563\u4e8b\u4ef6\u5e8f\u5217\u4e2d\u8fdb\u884c\u56e0\u679c\u53d1\u73b0\uff0c\u53ef\u6269\u5c55\u52302.9\u4e07+\u4e8b\u4ef6\u7c7b\u578b", "motivation": "\u9488\u5bf9\u8f66\u8f86\u65e5\u5fd7\u3001\u5236\u9020\u7cfb\u7edf\u548c\u60a3\u8005\u8f68\u8ff9\u7b49\u5355\u89c2\u6d4b\u5e8f\u5217\u7684\u56e0\u679c\u53d1\u73b0\u95ee\u9898\uff0c\u9762\u4e34\u65e0\u91cd\u590d\u6837\u672c\u3001\u9ad8\u7ef4\u5ea6\u548c\u957f\u65f6\u5e8f\u4f9d\u8d56\u7b49\u6311\u6218", "method": "\u5c06\u81ea\u56de\u5f52\u6a21\u578b\u6539\u9020\u4e3a\u9884\u8bad\u7ec3\u5bc6\u5ea6\u4f30\u8ba1\u5668\u6765\u4f30\u8ba1\u6761\u4ef6\u4e92\u4fe1\u606f\uff0c\u63a8\u65ad\u4e8b\u4ef6\u7c7b\u578b\u95f4\u7684\u56e0\u679c\u56fe\uff0c\u652f\u6301\u5ef6\u8fdf\u56e0\u679c\u6548\u5e94\u4e14\u7ebf\u6027\u6269\u5c55", "result": "\u5728\u4e0d\u540c\u57fa\u7ebf\u548c\u8bcd\u6c47\u91cf\u4e0b\u8868\u73b0\u7a33\u5065\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u8f66\u8f86\u8bca\u65ad\u7684\u6839\u56e0\u5206\u6790\uff0c\u5904\u7406\u8d85\u8fc729,100\u79cd\u4e8b\u4ef6\u7c7b\u578b", "conclusion": "TRACE\u4e3a\u5355\u5e8f\u5217\u56e0\u679c\u53d1\u73b0\u63d0\u4f9b\u4e86\u7406\u8bba\u53ef\u8bc6\u522b\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u65f6\u5e8f\u6570\u636e\u5206\u6790"}}
{"id": "2602.01137", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01137", "abs": "https://arxiv.org/abs/2602.01137", "authors": ["Shiguang Wu", "Yaqing Wang", "Quanming Yao"], "title": "Self-Generative Adversarial Fine-Tuning for Large Language Models", "comment": null, "summary": "Fine-tuning large language models (LLMs) for alignment typically relies on supervised fine-tuning or reinforcement learning from human feedback, both limited by the cost and scarcity of high-quality annotations. Recent self-play and synthetic data approaches reduce this dependence but often rely on heuristic assumptions or ungrounded self-evaluation, which can cause bias accumulation and performance drift. In this paper, we propose Self-Generative Adversarial LLM (SGALM), a unified fine-tuning framework that formulates alignment as a generative adversarial game within a single LLM. SGALM jointly evolves generation and discrimination capabilities without external reward models. Theoretical and empirical results demonstrate that SGALM achieves state-of-the-art performance, serves as an effective alignment algorithm and a robust synthetic data engine.", "AI": {"tldr": "SGALM\u662f\u4e00\u4e2a\u81ea\u751f\u6210\u7684\u5bf9\u6297\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u6846\u67b6\uff0c\u5b83\u5c06\u5bf9\u9f50\u95ee\u9898\u5efa\u6a21\u4e3a\u5355\u6a21\u578b\u5185\u7684\u751f\u6210\u5bf9\u6297\u6e38\u620f\uff0c\u65e0\u9700\u5916\u90e8\u5956\u52b1\u6a21\u578b\u6216\u4eba\u5de5\u6807\u6ce8\uff0c\u901a\u8fc7\u540c\u65f6\u8fdb\u5316\u751f\u6210\u548c\u5224\u522b\u80fd\u529b\u5b9e\u73b0SOTA\u6027\u80fd", "motivation": "\u4f20\u7edfLLM\u5bf9\u9f50\u65b9\u6cd5\uff08\u76d1\u7763\u5fae\u8c03\u6216RLHF\uff09\u53d7\u9650\u4e8e\u9ad8\u8d28\u91cf\u6807\u6ce8\u7684\u9ad8\u6210\u672c\u548c\u7a00\u7f3a\u6027\uff1b\u73b0\u6709\u81ea\u5bf9\u5f08\u548c\u5408\u6210\u6570\u636e\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u5047\u8bbe\u6216\u4e3b\u89c2\u81ea\u8bc4\u4f30\uff0c\u5bfc\u81f4\u504f\u5dee\u7d2f\u79ef\u548c\u6027\u80fd\u6f02\u79fb", "method": "\u63d0\u51faSelf-Generative Adversarial LLM (SGALM)\uff0c\u5728\u5355\u4e00LLM\u5185\u90e8\u6784\u5efa\u751f\u6210\u5bf9\u6297\u6846\u67b6\uff0c\u5c06\u7b56\u7565\u4f18\u5316\u8f6c\u5316\u4e3a\u751f\u6210\u5668\u4e0e\u5224\u522b\u5668\u7684\u5bf9\u6297\u535a\u5f08\uff0c\u8054\u5408\u63d0\u5347\u751f\u6210\u8d28\u91cf\u548c\u5224\u522b\u80fd\u529b", "result": "\u7406\u8bba\u548c\u5b9e\u9a8c\u8bc1\u660eSGALM\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u65e2\u662f\u5bf9\u9f50\u7b97\u6cd5\u4e5f\u662f\u7a33\u5065\u7684\u5408\u6210\u6570\u636e\u5f15\u64ce\uff0c\u6709\u6548\u907f\u514d\u4e86\u5916\u90e8\u5956\u52b1\u4f9d\u8d56\u548c\u504f\u5dee\u7d2f\u79ef\u95ee\u9898", "conclusion": "SGALM\u901a\u8fc7\u5355\u6a21\u578b\u5185\u7684\u5bf9\u6297\u81ea\u6539\u8fdb\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6807\u6ce8\u74f6\u9888\u548c\u504f\u5dee\u6f02\u79fb\u95ee\u9898\uff0c\u4e3aLLM\u5bf9\u9f50\u63d0\u4f9b\u4e86\u9ad8\u6548\u7edf\u4e00\u7684\u65b0\u8303\u5f0f"}}
{"id": "2602.01140", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01140", "abs": "https://arxiv.org/abs/2602.01140", "authors": ["Haochen You", "Heng Zhang", "Hongyang He", "Yuqi Li", "Baojing Liu"], "title": "Generalized Radius and Integrated Codebook Transforms for Differentiable Vector Quantization", "comment": "This paper has been accepted as a conference paper at CPAL 2026", "summary": "Vector quantization (VQ) underpins modern generative and representation models by turning continuous latents into discrete tokens. Yet hard nearest-neighbor assignments are non-differentiable and are typically optimized with heuristic straight-through estimators, which couple the update step size to the quantization gap and train each code in isolation, leading to unstable gradients and severe codebook under-utilization at scale. In this paper, we introduce GRIT-VQ (Generalized Radius and Integrated Transform-Vector Quantization), a unified surrogate framework that keeps hard assignments in the forward pass while making VQ fully differentiable. GRIT-VQ replaces the straight-through estimator with a radius-based update that moves latents along the quantization direction with a controllable, geometry-aware step, and applies a data-agnostic integrated transform to the codebook so that all codes are updated through shared parameters instead of independently. Our theoretical analysis clarifies the fundamental optimization dynamics introduced by GRIT-VQ, establishing conditions for stable gradient flow, coordinated codebook evolution, and reliable avoidance of collapse across a broad family of quantizers. Across image reconstruction, image generation, and recommendation tokenization benchmarks, GRIT-VQ consistently improves reconstruction error, generative quality, and recommendation accuracy while substantially increasing codebook utilization compared to existing VQ variants.", "AI": {"tldr": "GRIT-VQ is a novel differentiable vector quantization framework that replaces straight-through estimators with radius-based updates and shared codebook transforms, achieving better performance and utilization across image and recommendation tasks.", "motivation": "Hard nearest-neighbor assignments in vector quantization are non-differentiable; current straight-through estimators cause unstable gradients and severe codebook under-utilization by coupling step sizes to quantization gaps and training codes in isolation.", "method": "Introduces a unified surrogate framework that maintains hard assignments in the forward pass while making VQ fully differentiable through: (1) a radius-based update that moves latents along quantization directions with controllable, geometry-aware steps, and (2) a data-agnostic integrated transform that updates all codes through shared parameters rather than independently.", "result": "Theoretical analysis establishes conditions for stable gradient flow and coordinated codebook evolution. Empirically, GRIT-VQ consistently improves reconstruction error, generative quality, and recommendation accuracy while substantially increasing codebook utilization across multiple benchmarks.", "conclusion": "GRIT-VQ provides a theoretically-grounded solution to VQ optimization challenges, enabling stable training and better codebook utilization without sacrificing the benefits of hard discrete token assignments."}}
{"id": "2602.01156", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01156", "abs": "https://arxiv.org/abs/2602.01156", "authors": ["Shunpeng Yang", "Ben Liu", "Hua Chen"], "title": "PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning", "comment": "Submitted to ICLR 2026", "summary": "Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow's ability to capture richer multimodal action distributions.", "AI": {"tldr": "PolicyFlow\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u8fde\u7eed\u5f52\u4e00\u5316\u6d41(CNF)\u7b56\u7565\u4e0ePPO\u7b97\u6cd5\u7ed3\u5408\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fd1\u4f3c\u91cd\u8981\u6027\u6bd4\u7387\u800c\u975e\u5b8c\u6574\u8f68\u8ff9\u4f3c\u7136\u8ba1\u7b97\uff0c\u5e76\u5f15\u5165\u5e03\u6717\u6b63\u5219\u5316\u5668\u9632\u6b62\u6a21\u5f0f\u5d29\u6e83\uff0c\u5728\u591a\u79cd\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u6807\u51c6PPO\u7b97\u6cd5\u4f9d\u8d56\u9ad8\u65af\u5206\u5e03\u7b56\u7565\uff0c\u867d\u7136\u7b80\u5355\u7a33\u5b9a\u4f46\u8868\u8fbe\u80fd\u529b\u6709\u9650\u3002\u5c06PPO\u6269\u5c55\u5230\u66f4\u5177\u8868\u8fbe\u529b\u7684\u8fde\u7eed\u5f52\u4e00\u5316\u6d41(CNF)\u7b56\u7565\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u5b8c\u6574\u6d41\u8f68\u8ff9\u7684\u4f3c\u7136\u8bc4\u4f30\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6570\u503c\u4e0d\u7a33\u5b9a\u3002\u5982\u4f55\u5728\u4fdd\u6301PPO\u8bad\u7ec3\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u5229\u7528CNF\u7684\u8868\u8fbe\u80fd\u529b\u662f\u6838\u5fc3\u52a8\u673a\u3002", "method": "\u63d0\u51faPolicyFlow\u7b97\u6cd5\uff1a1) \u4f7f\u7528\u6cbf\u7b80\u5355\u63d2\u503c\u8def\u5f84\u7684\u901f\u5ea6\u573a\u53d8\u5316\u6765\u8fd1\u4f3c\u91cd\u8981\u6027\u6bd4\u7387\uff0c\u907f\u514d\u5b8c\u6574\u6d41\u8def\u5f84\u7684\u4f3c\u7136\u8ba1\u7b97\uff1b2) \u63d0\u51fa\u5e03\u6717\u6b63\u5219\u5316\u5668\uff0c\u4e00\u79cd\u53d7\u5e03\u6717\u8fd0\u52a8\u542f\u53d1\u7684\u9690\u5f0f\u7b56\u7565\u71b5\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u6982\u5ff5\u4f18\u96c5\u4e14\u8ba1\u7b97\u8f7b\u91cf\uff0c\u7528\u4e8e\u9632\u6b62\u6a21\u5f0f\u5d29\u6e83\u5e76\u9f13\u52b1\u884c\u4e3a\u591a\u6837\u6027\u3002", "result": "\u5728MultiGoal\u3001PointMaze\u3001IsaacLab\u548cMuJoCo Playground\u7b49\u591a\u6837\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPolicyFlow\u76f8\u6bd4\u9ad8\u65af\u7b56\u7565PPO\u548c\u6d41\u57fa\u57fa\u7ebf(FPO\u3001DPPO)\u8fbe\u5230\u7ade\u4e89\u6216\u66f4\u4f18\u6027\u80fd\u3002MultiGoal\u4efb\u52a1\u7279\u522b\u51f8\u663e\u4e86PolicyFlow\u6355\u83b7\u66f4\u4e30\u5bcc\u591a\u5cf0\u52a8\u4f5c\u5206\u5e03\u7684\u80fd\u529b\u3002", "conclusion": "PolicyFlow\u6210\u529f\u5b9e\u73b0\u4e86\u8868\u8fbe\u6027CNF\u7b56\u7565\u4e0ePPO\u98ce\u683c\u76ee\u6807\u7684\u96c6\u6210\uff0c\u5728\u4e0d\u727a\u7272\u8bad\u7ec3\u7a33\u5b9a\u6027\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u7b56\u7565\u8868\u8fbe\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01157", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01157", "abs": "https://arxiv.org/abs/2602.01157", "authors": ["Mohammed Osman Gani", "Zhipeng He", "Chun Ouyang", "Sara Khalifa"], "title": "Multi-Horizon Electricity Price Forecasting with Deep Learning in the Australian National Electricity Market", "comment": "63 Pages", "summary": "Accurate electricity price forecasting (EPF) is essential for operational planning, trading, and flexible asset scheduling in liberalised power systems, yet remains challenging due to volatility, heavy-tailed spikes, and frequent regime shifts. While deep learning (DL) has been increasingly adopted in EPF to capture complex and nonlinear price dynamics, several important gaps persist: (i) limited attention to multi-day horizons beyond day-ahead forecasting, (ii) insufficient exploration of state-of-the-art (SOTA) time series DL models, and (iii) a predominant reliance on aggregated horizon-level evaluation that obscures time-of-day forecasting variation. To address these gaps, we propose a novel EPF framework that extends the forecast horizon to multi-day-ahead by systematically building forecasting models that leverage benchmarked SOTA time series DL models. We conduct a comprehensive evaluation to analyse time-of-day forecasting performance by integrating model assessment at intraday interval levels across all five regions in the Australian National Electricity Market (NEM). The results show that no single model consistently dominates across regions, metrics, and horizons. Overall, standard DL models deliver superior performance in most regions, while SOTA time series DL models demonstrate greater robustness to forecast horizon extension. Intraday interval-level evaluation reveals pronounced diurnal error patterns, indicating that absolute errors peak during the evening ramp, relative errors inflate during midday negative-price regimes, and directional accuracy degrades during periods of frequent trend changes. These findings suggest that future research on DL-based EPF can benefit from enriched feature representations and modelling strategies that enhance longer-term forecasting robustness while maintaining sensitivity to intraday volatility and structural price dynamics.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u591a\u65e5\u7535\u529b\u4ef7\u683c\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u5148\u8fdb\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u53d1\u73b0\u65e0\u5355\u4e00\u6a21\u578b\u5728\u6240\u6709\u533a\u57df\u3001\u6307\u6807\u548c\u9884\u6d4bhorizon\u4e0a\u6301\u7eed\u5360\u4f18\uff0c\u6807\u51c6\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u591a\u6570\u5730\u533a\u8868\u73b0\u66f4\u4f73\uff0c\u800c\u5148\u8fdb\u6a21\u578b\u5bf9\u5ef6\u957f\u9884\u6d4bhorizon\u66f4\u5177\u9c81\u68d2\u6027\uff0c\u5e76\u63ed\u793a\u663e\u8457\u7684\u65e5\u5185\u8bef\u5dee\u53d8\u5316\u6a21\u5f0f\u3002", "motivation": "\u51c6\u786e\u7535\u529b\u4ef7\u683c\u9884\u6d4b\u5bf9\u81ea\u7531\u5316\u7535\u529b\u7cfb\u7edf\u7684\u8fd0\u8425\u89c4\u5212\u3001\u4ea4\u6613\u548c\u7075\u6d3b\u8d44\u4ea7\u8c03\u5ea6\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4ecd\u9762\u4e34\u6ce2\u52a8\u6027\u3001\u91cd\u5c3e\u5c16\u5cf0\u548c\u9891\u7e41\u673a\u5236\u8f6c\u6362\u7684\u6311\u6218\u3002\u73b0\u6709\u7814\u7a76\u5b58\u5728\u4e09\u65b9\u9762\u4e0d\u8db3\uff1a(i)\u5bf9\u65e5\u4ee5\u4e0a\u591a\u65e5\u9884\u6d4bhorizon\u5173\u6ce8\u6709\u9650\uff1b(ii)\u5bf9\u5148\u8fdb\u65f6\u95f4\u5e8f\u5217\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63a2\u7d22\u4e0d\u8db3\uff1b(iii)\u8fc7\u5ea6\u4f9d\u8d56\u805a\u5408\u8bc4\u4f30\uff0c\u63a9\u76d6\u4e86\u65e5\u5185\u9884\u6d4b\u5dee\u5f02\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u7535\u529b\u4ef7\u683c\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u5148\u8fdb\u65f6\u95f4\u5e8f\u5217\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5c06\u9884\u6d4bhorizon\u7cfb\u7edf\u6027\u5730\u6269\u5c55\u81f3\u591a\u65e5ahead\u3002\u5728\u6fb3\u5927\u5229\u4e9a\u56fd\u5bb6\u7535\u529b\u5e02\u573a\u4e94\u4e2a\u533a\u57df\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\uff0c\u5e76\u5728\u65e5\u5185\u95f4\u9694\u6c34\u5e73\u4e0a\u5206\u6790\u65f6\u95f4-of-day\u9884\u6d4b\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u65e0\u5355\u4e00\u6a21\u578b\u5728\u6240\u6709\u533a\u57df\u3001\u6307\u6807\u548chorizon\u4e0a\u6301\u7eed\u5360\u4f18\u3002\u6807\u51c6\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u591a\u6570\u533a\u57df\u8868\u73b0\u66f4\u4f18\uff0c\u800c\u5148\u8fdb\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u5bf9\u9884\u6d4bhorizon\u5ef6\u957f\u8868\u73b0\u51fa\u66f4\u5f3a\u9c81\u68d2\u6027\u3002\u65e5\u5185\u95f4\u9694\u7ea7\u8bc4\u4f30\u63ed\u793a\u660e\u663e\u663c\u591c\u8bef\u5dee\u6a21\u5f0f\uff1a\u7edd\u5bf9\u8bef\u5dee\u5728\u665a\u95f4ramp\u671f\u95f4\u8fbe\u5230\u5cf0\u503c\uff0c\u76f8\u5bf9\u8bef\u5dee\u5728\u5348\u95f4\u8d1f\u7535\u4ef7regime\u81a8\u80c0\uff0c\u65b9\u5411\u51c6\u786e\u6027\u5728\u9891\u7e41\u8d8b\u52bf\u53d8\u5316\u671f\u95f4\u4e0b\u964d\u3002", "conclusion": "\u672a\u6765\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7535\u529b\u4ef7\u683c\u9884\u6d4b\u7814\u7a76\u53ef\u901a\u8fc7\u589e\u5f3a\u7279\u5f81\u8868\u793a\u548c\u5efa\u6a21\u7b56\u7565\u53d7\u76ca\uff0c\u4ee5\u63d0\u9ad8\u957f\u671f\u9884\u6d4b\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u65e5\u5185\u6ce2\u52a8\u6027\u548c\u7ed3\u6784\u6027\u4ef7\u683c\u52a8\u6001\u7684\u654f\u611f\u6027\u3002"}}
{"id": "2602.01179", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01179", "abs": "https://arxiv.org/abs/2602.01179", "authors": ["Zhichao Chen", "Zhan Zhuang", "Yunfei Teng", "Hao Wang", "Fangyikang Wang", "Zhengnan Li", "Tianqiao Liu", "Haoxuan Li", "Zhouchen Lin"], "title": "Rethinking the Flow-Based Gradual Domain Adaption: A Semi-Dual Optimal Transport Perspective", "comment": null, "summary": "Gradual domain adaptation (GDA) aims to mitigate domain shift by progressively adapting models from the source domain to the target domain via intermediate domains. However, real intermediate domains are often unavailable or ineffective, necessitating the synthesis of intermediate samples. Flow-based models have recently been used for this purpose by interpolating between source and target distributions; however, their training typically relies on sample-based log-likelihood estimation, which can discard useful information and thus degrade GDA performance. The key to addressing this limitation is constructing the intermediate domains via samples directly. To this end, we propose an Entropy-regularized Semi-dual Unbalanced Optimal Transport (E-SUOT) framework to construct intermediate domains. Specifically, we reformulate flow-based GDA as a Lagrangian dual problem and derive an equivalent semi-dual objective that circumvents the need for likelihood estimation. However, the dual problem leads to an unstable min-max training procedure. To alleviate this issue, we further introduce entropy regularization to convert it into a more stable alternative optimization procedure. Based on this, we propose a novel GDA training framework and provide theoretical analysis in terms of stability and generalization. Finally, extensive experiments are conducted to demonstrate the efficacy of the E-SUOT framework.", "AI": {"tldr": "\u9488\u5bf9\u6e10\u8fdb\u57df\u9002\u5e94\u4e2d\u6d41\u6a21\u578b\u4f9d\u8d56\u4f3c\u7136\u4f30\u8ba1\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u71b5\u6b63\u5219\u5316\u534a\u5bf9\u5076\u975e\u5e73\u8861\u6700\u4f18\u4f20\u8f93(E-SUOT)\u6846\u67b6\uff0c\u76f4\u63a5\u6784\u9020\u4e2d\u95f4\u57df\u6837\u672c\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u771f\u5b9e\u4e2d\u95f4\u57df\u96be\u4ee5\u83b7\u5f97\uff0c\u73b0\u6709\u6d41\u6a21\u578b\u901a\u8fc7\u63d2\u503c\u5408\u6210\u4e2d\u95f4\u6837\u672c\u4f46\u4f9d\u8d56\u4f3c\u7136\u4f30\u8ba1\u4f1a\u4e22\u5f03\u6709\u7528\u4fe1\u606f\uff0c\u635f\u5bb3GDA\u6027\u80fd\uff0c\u9700\u8981\u76f4\u63a5\u901a\u8fc7\u6837\u672c\u6784\u9020\u4e2d\u95f4\u57df\u3002", "method": "\u91cd\u6784\u6d41\u6a21\u578bGDA\u4e3a\u62c9\u683c\u6717\u65e5\u5bf9\u5076\u95ee\u9898\uff0c\u63a8\u5bfc\u7b49\u4ef7\u534a\u5bf9\u5076\u76ee\u6807\u89c4\u907f\u4f3c\u7136\u4f30\u8ba1\uff0c\u5f15\u5165\u71b5\u6b63\u5219\u5316\u5c06\u4e0d\u7a33\u5b9a\u6781\u5c0f\u6781\u5927\u4f18\u5316\u8f6c\u4e3a\u7a33\u5b9a\u66ff\u4ee3\u4f18\u5316\uff0c\u6784\u5efaE-SUOT\u6846\u67b6\u3002", "result": "\u63d0\u4f9b\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u7406\u8bba\u5206\u6790\uff0c\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660eE-SUOT\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u6846\u67b6\u80fd\u76f4\u63a5\u6784\u9020\u4e2d\u95f4\u57df\u6837\u672c\uff0c\u6709\u6548\u89e3\u51b3\u57df\u504f\u79fb\u95ee\u9898\uff0c\u4e3a\u6e10\u8fdb\u57df\u9002\u5e94\u63d0\u4f9b\u4e86\u65b0\u7684\u4f18\u5316\u8303\u5f0f\u3002"}}
{"id": "2602.01182", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01182", "abs": "https://arxiv.org/abs/2602.01182", "authors": ["Zhichao Chen", "Hao Wang", "Fangyikang Wang", "Licheng Pan", "Zhengnan Li", "Yunfei Teng", "Haoxuan Li", "Zhouchen Lin"], "title": "Analyzing and Improving Diffusion Models for Time-Series Data Imputation: A Proximal Recursion Perspective", "comment": null, "summary": "Diffusion models (DMs) have shown promise for Time-Series Data Imputation (TSDI); however, their performance remains inconsistent in complex scenarios. We attribute this to two primary obstacles: (1) non-stationary temporal dynamics, which can bias the inference trajectory and lead to outlier-sensitive imputations; and (2) objective inconsistency, since imputation favors accurate pointwise recovery whereas DMs are inherently trained to generate diverse samples. To better understand these issues, we analyze DM-based TSDI process through a proximal-operator perspective and uncover that an implicit Wasserstein distance regularization inherent in the process hinders the model's ability to counteract non-stationarity and dissipative regularizer, thereby amplifying diversity at the expense of fidelity. Building on this insight, we propose a novel framework called SPIRIT (Semi-Proximal Transport Regularized time-series Imputation). Specifically, we introduce entropy-induced Bregman divergence to relax the mass preserving constraint in the Wasserstein distance, formulate the semi-proximal transport (SPT) discrepancy, and theoretically prove the robustness of SPT against non-stationarity. Subsequently, we remove the dissipative structure and derive the complete SPIRIT workflow, with SPT serving as the proximal operator. Extensive experiments demonstrate the effectiveness of the proposed SPIRIT approach.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u6269\u6563\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u63d2\u8865\u4e2d\u56e0\u975e\u5e73\u7a33\u6027\u548c\u76ee\u6807\u4e0d\u4e00\u81f4\u5bfc\u81f4\u7684\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u63d0\u51faSPIRIT\u6846\u67b6\uff0c\u901a\u8fc7\u534a\u8fd1\u7aef\u4f20\u8f93\u5dee\u5f02\u66ff\u4ee3Wasserstein\u6b63\u5219\u5316\uff0c\u7406\u8bba\u8bc1\u660e\u5176\u9c81\u68d2\u6027\u5e76\u5b9e\u9a8c\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u590d\u6742\u65f6\u95f4\u5e8f\u5217\u63d2\u8865\u573a\u666f\u4e2d\u6027\u80fd\u4e0d\u7a33\u5b9a\uff0c\u4e3b\u8981\u6e90\u4e8e\u4e24\u4e2a\u969c\u788d\uff1a\u975e\u5e73\u7a33\u65f6\u95f4\u52a8\u6001\u5bfc\u81f4\u63a8\u7406\u504f\u7f6e\u548c\u5f02\u5e38\u503c\u654f\u611f\uff0c\u4ee5\u53ca\u63d2\u8865\u9700\u8981\u7cbe\u786e\u9010\u70b9\u6062\u590d\u4e0e\u6269\u6563\u6a21\u578b\u751f\u6210\u591a\u6837\u6027\u6837\u672c\u7684\u8bad\u7ec3\u76ee\u6807\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\u3002", "method": "\u63d0\u51faSPIRIT\u6846\u67b6\uff0c\u901a\u8fc7\u71b5\u8bf1\u5bfcBregman\u6563\u5ea6\u677e\u5f1bWasserstein\u8d28\u91cf\u7ea6\u675f\uff0c\u6784\u5efa\u534a\u8fd1\u7aef\u4f20\u8f93\uff08SPT\uff09\u5dee\u5f02\uff0c\u7406\u8bba\u8bc1\u660e\u5176\u5bf9\u975e\u5e73\u7a33\u6027\u7684\u9c81\u68d2\u6027\uff0c\u79fb\u9664\u8017\u6563\u7ed3\u6784\u5e76\u5c06SPT\u4f5c\u4e3a\u8fd1\u7aef\u7b97\u5b50\u63a8\u5bfc\u5b8c\u6574\u6d41\u7a0b\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86SPIRIT\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "SPIRIT\u6846\u67b6\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u63d2\u8865\u4e2d\u7684\u975e\u5e73\u7a33\u6027\u548c\u76ee\u6807\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u51c6\u786e\u7684\u63d2\u8865\u6027\u80fd\u3002"}}
{"id": "2602.01186", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01186", "abs": "https://arxiv.org/abs/2602.01186", "authors": ["Fabio Turazza", "Marco Picone", "Marco Mamei"], "title": "The Gaussian-Head OFL Family: One-Shot Federated Learning from Client Global Statistics", "comment": "Accepted at the International Conference on Learning Representations (ICLR) 2026", "summary": "Classical Federated Learning relies on a multi-round iterative process of model exchange and aggregation between server and clients, with high communication costs and privacy risks from repeated model transmissions. In contrast, one-shot federated learning (OFL) alleviates these limitations by reducing communication to a single round, thereby lowering overhead and enhancing practical deployability. Nevertheless, most existing one-shot approaches remain either impractical or constrained, for example, they often depend on the availability of a public dataset, assume homogeneous client models, or require uploading additional data or model information. To overcome these issues, we introduce the Gaussian-Head OFL (GH-OFL) family, a suite of one-shot federated methods that assume class-conditional Gaussianity of pretrained embeddings. Clients transmit only sufficient statistics (per-class counts and first/second-order moments) and the server builds heads via three components: (i) Closed-form Gaussian heads (NB/LDA/QDA) computed directly from the received statistics; (ii) FisherMix, a linear head with cosine margin trained on synthetic samples drawn in an estimated Fisher subspace; and (iii) Proto-Hyper, a lightweight low-rank residual head that refines Gaussian logits via knowledge distillation on those synthetic samples. In our experiments, GH-OFL methods deliver state-of-the-art robustness and accuracy under strong non-IID skew while remaining strictly data-free.", "AI": {"tldr": "The paper proposes GH-OFL, a one-shot federated learning framework that uses Gaussian assumptions on pretrained embeddings to enable single-round communication without public data, achieving state-of-the-art robustness on non-IID client data.", "motivation": "Classical federated learning suffers from high communication costs and privacy risks due to multi-round model exchanges. While one-shot federated learning (OFL) addresses these issues, existing methods are impractical because they require public datasets, assume homogeneous models, or need additional data uploads. This paper aims to develop a truly practical, data-free OFL approach that works robustly under strong non-IID data skew.", "method": "The GH-OFL family assumes class-conditional Gaussian distributions of pretrained embeddings. Instead of sharing raw data or models, clients transmit only sufficient statistics (per-class counts and first/second-order moments). The server then constructs three complementary heads: (1) closed-form Gaussian classifiers (NB/LDA/QDA) directly computed from statistics; (2) FisherMix, a linear head trained on synthetic samples generated in an estimated Fisher subspace; and (3) Proto-Hyper, a lightweight residual head that refines Gaussian logits via knowledge distillation on synthetic samples. All components operate strictly data-free.", "result": "GH-OFL methods achieve state-of-the-art robustness and accuracy under strong non-IID data skew while maintaining strict data-free operation, eliminating the need for public datasets or additional client information.", "conclusion": "GH-OFL successfully enables practical one-shot federated learning by replacing raw data/model transmission with compact sufficient statistics and Gaussian-based inference, delivering strong performance on heterogeneous client data without compromising privacy or requiring public resources."}}
{"id": "2602.01196", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01196", "abs": "https://arxiv.org/abs/2602.01196", "authors": ["Jin Li", "Yue Wu", "Mengsha Huang", "Yuhao Sun", "Hao He", "Xianyuan Zhan"], "title": "Unraveling the Hidden Dynamical Structure in Recurrent Neural Policies", "comment": null, "summary": "Recurrent neural policies are widely used in partially observable control and meta-RL tasks. Their abilities to maintain internal memory and adapt quickly to unseen scenarios have offered them unparalleled performance when compared to non-recurrent counterparts. However, until today, the underlying mechanisms for their superior generalization and robustness performance remain poorly understood. In this study, by analyzing the hidden state domain of recurrent policies learned over a diverse set of training methods, model architectures, and tasks, we find that stable cyclic structures consistently emerge during interaction with the environment. Such cyclic structures share a remarkable similarity with \\textit{limit cycles} in dynamical system analysis, if we consider the policy and the environment as a joint hybrid dynamical system. Moreover, we uncover that the geometry of such limit cycles also has a structured correspondence with the policies' behaviors. These findings offer new perspectives to explain many nice properties of recurrent policies: the emergence of limit cycles stabilizes both the policies' internal memory and the task-relevant environmental states, while suppressing nuisance variability arising from environmental uncertainty; the geometry of limit cycles also encodes relational structures of behaviors, facilitating easier skill adaptation when facing non-stationary environments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u4e0e\u73af\u5883\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\uff0c\u5faa\u73af\u795e\u7ecf\u7b56\u7565\u7684\u9690\u85cf\u72b6\u6001\u4f1a\u5f62\u6210\u7a33\u5b9a\u7684\u6781\u9650\u73af\u7ed3\u6784\uff0c\u8fd9\u89e3\u91ca\u4e86\u5176\u4f18\u5f02\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002\u8fd9\u4e9b\u5faa\u73af\u7ed3\u6784\u65e2\u80fd\u7a33\u5b9a\u8bb0\u5fc6\uff0c\u53c8\u80fd\u7f16\u7801\u884c\u4e3a\u6a21\u5f0f\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u597d\u7684\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u7406\u89e3\u5faa\u73af\u795e\u7ecf\u7b56\u7565\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u63a7\u5236\u548c\u5143\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\u4e3a\u4f55\u4f18\u4e8e\u975e\u5faa\u73af\u7b56\u7565\uff0c\u7279\u522b\u662f\u63a2\u7a76\u5176\u5353\u8d8a\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u80cc\u540e\u7684\u5185\u5728\u673a\u5236\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5728\u4e0d\u540c\u8bad\u7ec3\u65b9\u6cd5\u3001\u6a21\u578b\u67b6\u6784\u548c\u4efb\u52a1\u4e0a\u5b66\u4e60\u7684\u5faa\u73af\u7b56\u7565\u7684\u9690\u85cf\u72b6\u6001\u52a8\u529b\u5b66\uff0c\u8bc6\u522b\u5176\u5185\u90e8\u8868\u5f81\u4e2d\u7684\u4e00\u81f4\u6027\u6a21\u5f0f\u3002", "result": "\u53d1\u73b0\u7a33\u5b9a\u7684\u5faa\u73af\u7ed3\u6784\uff08\u6781\u9650\u73af\uff09\u5728\u73af\u5883\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\u6301\u7eed\u51fa\u73b0\uff0c\u4e14\u8fd9\u4e9b\u7ed3\u6784\u7684\u51e0\u4f55\u5f62\u72b6\u4e0e\u7b56\u7565\u884c\u4e3a\u5b58\u5728\u7ed3\u6784\u5316\u7684\u5bf9\u5e94\u5173\u7cfb\u3002", "conclusion": "\u6781\u9650\u73af\u7684\u5b58\u5728\u89e3\u91ca\u4e86\u5faa\u73af\u7b56\u7565\u7684\u826f\u597d\u7279\u6027\uff1a\u65e2\u80fd\u7a33\u5b9a\u5185\u90e8\u8bb0\u5fc6\u548c\u4efb\u52a1\u76f8\u5173\u72b6\u6001\uff0c\u6291\u5236\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u5e26\u6765\u7684\u5e72\u6270\uff0c\u5176\u51e0\u4f55\u7ed3\u6784\u8fd8\u80fd\u7f16\u7801\u884c\u4e3a\u7684\u5173\u7cfb\u7ed3\u6784\uff0c\u4ece\u800c\u4fc3\u8fdb\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u7684\u6280\u80fd\u9002\u5e94\u3002"}}
{"id": "2602.01212", "categories": ["cs.LG", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01212", "abs": "https://arxiv.org/abs/2602.01212", "authors": ["Marco Chen", "Xianbiao Qi", "Yelin He", "Jiaquan Ye", "Rong Xiao"], "title": "SimpleGPT: Improving GPT via A Simple Normalization Strategy", "comment": "We propose SimpleGPT, a simple yet effective GPT model, and provide theoretical insights into its mathematical foundations. We validate our theoretical findings through extensive experiments on large GPT models at parameter scales 1B, 1.4B, 7B and 8B", "summary": "In this work, we revisit Transformer optimization through the lens of second-order geometry and establish a direct connection between architectural design, activation scale, the Hessian matrix, and the maximum tolerable learning rate. We introduce a simple normalization strategy, termed SimpleNorm, which stabilizes intermediate activation scales by construction. Then, by analyzing the Hessian of the loss with respect to network activations, we theoretically show that SimpleNorm significantly reduces the spectral norm of the Hessian, thereby permitting larger stable learning rates. We validate our theoretical findings through extensive experiments on large GPT models at parameter scales 1B, 1.4B, 7B and 8B. Empirically, SimpleGPT, our SimpleNorm-based network, tolerates learning rates 3$\\times$-10$\\times$ larger than standard convention, consistently demonstrates strong optimization stability, and achieves substantially better performance than well-established baselines. Specifically, when training 7B-scale models for 60K steps, SimpleGPT achieves a training loss that is 0.08 lower than that of LLaMA2 with QKNorm, reducing the loss from 2.290 to 2.208. Our source code will be released at https://github.com/Ocram7/SimpleGPT.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faSimpleNorm\u5f52\u4e00\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u63a7\u5236\u6fc0\u6d3b\u5c3a\u5ea6\u548c\u964d\u4f4eHessian\u8c31\u8303\u6570\u6765\u7a33\u5b9aTransformer\u8bad\u7ec3\uff0c\u4f7f\u5b66\u4e60\u7387\u53ef\u63d0\u53473-10\u500d\uff0c\u6027\u80fd\u4f18\u4e8eLLaMA2\u7b49\u57fa\u7ebf\u3002", "motivation": "Transformer\u4f18\u5316\u53d7\u9650\u4e8e\u6fc0\u6d3b\u5c3a\u5ea6\u4e0d\u7a33\u5b9a\u548c\u5c0f\u5b66\u4e60\u7387\u7ea6\u675f\u3002Hessian\u77e9\u9635\u63ed\u793a\u4e86\u4f18\u5316\u51e0\u4f55\u7279\u6027\uff0c\u7406\u89e3\u5176\u4e0e\u67b6\u6784\u548c\u6fc0\u6d3b\u5c3a\u5ea6\u7684\u5173\u7cfb\u6709\u671b\u5b9e\u73b0\u66f4\u7a33\u5b9a\u9ad8\u6548\u7684\u8bad\u7ec3\u3002", "method": "1) \u901a\u8fc7\u4e8c\u9636\u51e0\u4f55(Hessian)\u5206\u6790Transformer\u4f18\u5316\uff1b2) \u63d0\u51faSimpleNorm\uff1a\u4e00\u79cd\u901a\u8fc7\u6784\u9020\u7a33\u5b9a\u6fc0\u6d3b\u5c3a\u5ea6\u7684\u5f52\u4e00\u5316\u7b56\u7565\uff1b3) \u7406\u8bba\u8bc1\u660eSimpleNorm\u964d\u4f4eHessian\u8c31\u8303\u6570\uff0c\u4f7f\u66f4\u5927\u5b66\u4e60\u7387\u7a33\u5b9a\u8bad\u7ec3\u6210\u4e3a\u53ef\u80fd\uff1b4) \u57281B-8B\u53c2\u6570\u7684GPT\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u5bf9\u6bd4LLaMA2+QKNorm\u7b49\u57fa\u7ebf\u3002", "result": "1) SimpleGPT\u53ef\u5bb9\u5fcd3-10\u500d\u4e8e\u6807\u51c6\u7684\u5b66\u4e60\u7387\uff1b2) \u8868\u73b0\u51fa\u5f3a\u4f18\u5316\u7a33\u5b9a\u6027\uff1b3) \u6027\u80fd\u66f4\u4f18\uff1a7B\u6a21\u578b60K\u6b65\u8bad\u7ec3\uff0c\u635f\u5931\u4eceLLaMA2\u76842.290\u964d\u81f32.208\uff1b4) \u57281B\u30011.4B\u30017B\u30018B\u5404\u5c3a\u5ea6\u5747\u4e00\u81f4\u63d0\u5347\u3002", "conclusion": "SimpleNorm\u4e3aTransformer\u4f18\u5316\u4e0d\u7a33\u5b9a\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6491\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u5b66\u4e60\u7387\u548c\u6027\u80fd\uff0c\u662f\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6548\u7387\u7684\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2602.01217", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.01217", "abs": "https://arxiv.org/abs/2602.01217", "authors": ["Lucas Lange", "Adrian B\u00f6ttinger", "Victor Christen", "Anushka Vidanage", "Peter Christen", "Erhard Rahm"], "title": "Learning from Anonymized and Incomplete Tabular Data", "comment": null, "summary": "User-driven privacy allows individuals to control whether and at what granularity their data is shared, leading to datasets that mix original, generalized, and missing values within the same records and attributes. While such representations are intuitive for privacy, they pose challenges for machine learning, which typically treats non-original values as new categories or as missing, thereby discarding generalization semantics. For learning from such tabular data, we propose novel data transformation strategies that account for heterogeneous anonymization and evaluate them alongside standard imputation and LLM-based approaches. We employ multiple datasets, privacy configurations, and deployment scenarios, demonstrating that our method reliably regains utility. Our results show that generalized values are preferable to pure suppression, that the best data preparation strategy depends on the scenario, and that consistent data representations are crucial for maintaining downstream utility. Overall, our findings highlight that effective learning is tied to the appropriate handling of anonymized values.", "AI": {"tldr": "\u9488\u5bf9\u7528\u6237\u9a71\u52a8\u9690\u79c1\u4ea7\u751f\u7684\u6df7\u5408\u539f\u59cb/\u6cdb\u5316/\u7f3a\u5931\u503c\u6570\u636e\uff0c\u63d0\u51fa\u65b0\u578b\u6570\u636e\u8f6c\u6362\u7b56\u7565\u4ee5\u4fdd\u7559\u6cdb\u5316\u8bed\u4e49\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6bd4\u6807\u51c6\u586b\u5145\u548cLLM\u65b9\u6cd5\u66f4\u6709\u6548\u6062\u590d\u6570\u636e\u6548\u7528\uff0c\u4e14\u6700\u4f73\u7b56\u7565\u9700\u6839\u636e\u573a\u666f\u9009\u62e9", "motivation": "\u7528\u6237\u9a71\u52a8\u9690\u79c1\u5bfc\u81f4\u6570\u636e\u96c6\u5305\u542b\u539f\u59cb\u503c\u3001\u6cdb\u5316\u503c\u548c\u7f3a\u5931\u503c\u7684\u6df7\u5408\uff0c\u4f46\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5c06\u975e\u539f\u59cb\u503c\u89c6\u4e3a\u65b0\u7c7b\u522b\u6216\u7f3a\u5931\u503c\uff0c\u4ece\u800c\u4e22\u5f03\u6cdb\u5316\u8bed\u4e49\uff0c\u9020\u6210\u6548\u7528\u635f\u5931", "method": "\u63d0\u51fa\u8003\u8651\u5f02\u6784\u533f\u540d\u5316\u7684\u65b0\u578b\u6570\u636e\u8f6c\u6362\u7b56\u7565\uff0c\u4e0e\u6807\u51c6\u586b\u5145\u53caLLM\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u91c7\u7528\u591a\u6570\u636e\u96c6\u3001\u9690\u79c1\u914d\u7f6e\u548c\u90e8\u7f72\u573a\u666f\u8bc4\u4f30", "result": "1) \u6cdb\u5316\u503c\u4f18\u4e8e\u7eaf\u6291\u5236 2) \u6700\u4f73\u6570\u636e\u51c6\u5907\u7b56\u7565\u4f9d\u8d56\u5177\u4f53\u573a\u666f 3) \u4e00\u81f4\u7684\u6570\u636e\u8868\u793a\u5bf9\u4fdd\u6301\u4e0b\u6e38\u6548\u7528\u81f3\u5173\u91cd\u8981", "conclusion": "\u6709\u6548\u5b66\u4e60\u53d6\u51b3\u4e8e\u5bf9\u533f\u540d\u5316\u503c\u7684\u6070\u5f53\u5904\u7406\uff0c\u9700\u6839\u636e\u573a\u666f\u9009\u62e9\u9002\u914d\u7b56\u7565\u4ee5\u7ef4\u6301\u6570\u636e\u6548\u7528"}}
{"id": "2602.01219", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01219", "abs": "https://arxiv.org/abs/2602.01219", "authors": ["Qishuai Wen", "Zhiyuan Huang", "Xianghan Meng", "Wei He", "Chun-Guang Li"], "title": "MiTA Attention: Efficient Fast-Weight Scaling via a Mixture of Top-$k$ Activations", "comment": null, "summary": "The attention operator in Transformers can be viewed as a two-layer fast-weight MLP, whose weights are dynamically instantiated from input tokens and whose width equals sequence length $N$. As the context extends, the expressive capacity of such an $N$-width MLP increases, but scaling its fast weights becomes prohibitively expensive for extremely long sequences. Recently, this fast-weight scaling perspective has motivated the Mixture-of-Experts (MoE) attention, which partitions the sequence into fast-weight experts and sparsely routes the tokens to them. In this paper, we elevate this perspective to a unifying framework for a wide range of efficient attention methods by interpreting them as scaling fast weights through routing and/or compression. Then we propose a compress-and-route strategy, which compresses the $N$-width MLP into a narrower one using a small set of landmark queries and constructs deformable experts by gathering top-$k$ activated key-value pairs for each landmark query. We call this strategy a Mixture of Top-$k$ Activations (MiTA), and refer to the resulting efficient mechanism as MiTA attention. Preliminary experiments on vision tasks demonstrate the promise of our MiTA attention and motivate further investigation on its optimization and broader applications in more challenging settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMiTA\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u538b\u7f29\u548c\u8def\u7531\u7b56\u7565\u89e3\u51b3\u957f\u5e8f\u5217\u6ce8\u610f\u529b\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898\uff0c\u5728\u89c6\u89c9\u4efb\u52a1\u4e0a\u5c55\u73b0\u51fa\u6f5c\u529b\u3002", "motivation": "Transformer\u6ce8\u610f\u529b\u53ef\u89c6\u4e3a\u5bbd\u5ea6\u7b49\u4e8e\u5e8f\u5217\u957f\u5ea6N\u7684\u5feb\u901f\u6743\u91cdMLP\uff0c\u957f\u5e8f\u5217\u4e0b\u6269\u5c55\u5feb\u901f\u6743\u91cd\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002\u73b0\u6709MoE\u6ce8\u610f\u529b\u901a\u8fc7\u7a00\u758f\u8def\u7531\u7f13\u89e3\u8be5\u95ee\u9898\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u538b\u7f29-\u8def\u7531\u7b56\u7565\uff1a\u7528\u5c11\u91cf\u5730\u6807\u67e5\u8be2\u5c06N\u5bbd\u5ea6MLP\u538b\u7f29\u4e3a\u66f4\u7a84\u7684MLP\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u5730\u6807\u67e5\u8be2\u6536\u96c6top-k\u6fc0\u6d3b\u7684\u952e\u503c\u5bf9\u6784\u5efa\u53ef\u53d8\u5f62\u4e13\u5bb6\uff0c\u5f62\u6210MiTA\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5728\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u521d\u6b65\u5b9e\u9a8c\u8868\u660eMiTA\u6ce8\u610f\u529b\u5177\u6709\u826f\u597d\u6548\u679c\u3002", "conclusion": "MiTA\u6ce8\u610f\u529b\u5c55\u73b0\u51fa\u5e94\u7528\u524d\u666f\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u5e76\u5728\u66f4\u5177\u6311\u6218\u6027\u7684\u573a\u666f\u4e2d\u63a2\u7d22\u5176\u66f4\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2602.01233", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01233", "abs": "https://arxiv.org/abs/2602.01233", "authors": ["Tianhao Miao", "Zhongyuan Bao", "Lejun Zhang"], "title": "Lotus: Efficient LLM Training by Randomized Low-Rank Gradient Projection with Adaptive Subspace Switching", "comment": null, "summary": "Training efficiency in large-scale models is typically assessed through memory consumption, training time, and model performance. Current methods often exhibit trade-offs among these metrics, as optimizing one generally degrades at least one of the others. Addressing this trade-off remains a central challenge in algorithm design. While GaLore enables memory-efficient training by updating gradients in a low-rank subspace, it incurs a comparable extra training time cost due to the Singular Value Decomposition(SVD) process on gradients. In this paper, we propose Lotus, a method that resolves this trade-off by simply modifying the projection process. We propose a criterion that quantifies the displacement of the unit gradient to enable efficient transitions between low-rank gradient subspaces. Experimental results indicate that Lotus is the most efficient method, achieving a 30% reduction in training time and a 40% decrease in memory consumption for gradient and optimizer states. Additionally, it outperforms the baseline method in both pre-training and fine-tuning tasks.", "AI": {"tldr": "\u63d0\u51faLotus\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fee\u6539\u6295\u5f71\u8fc7\u7a0b\u89e3\u51b3\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u5185\u5b58\u3001\u65f6\u95f4\u4e0e\u6027\u80fd\u7684\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1130%\u3001\u5185\u5b58\u6d88\u8017\u964d\u4f4e40%\uff0c\u5e76\u5728\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u4efb\u52a1\u4e2d\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u6548\u7387\u901a\u5e38\u5728\u5185\u5b58\u6d88\u8017\u3001\u8bad\u7ec3\u65f6\u95f4\u548c\u6a21\u578b\u6027\u80fd\u4e09\u8005\u95f4\u5b58\u5728\u6743\u8861\uff0c\u4f18\u5316\u4e00\u4e2a\u6307\u6807\u5f80\u5f80\u4f1a\u635f\u5bb3\u81f3\u5c11\u53e6\u4e00\u4e2a\u6307\u6807\u3002\u73b0\u6709\u65b9\u6cd5\u5982GaLore\u867d\u80fd\u8282\u7701\u5185\u5b58\uff0c\u4f46\u56e0SVD\u8fc7\u7a0b\u5e26\u6765\u989d\u5916\u8bad\u7ec3\u65f6\u95f4\u5f00\u9500\u3002", "method": "\u63d0\u51faLotus\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fee\u6539\u6295\u5f71\u8fc7\u7a0b\uff0c\u8bbe\u8ba1\u4e00\u4e2a\u91cf\u5316\u5355\u4f4d\u68af\u5ea6\u4f4d\u79fb\u7684\u51c6\u5219\uff0c\u5b9e\u73b0\u4f4e\u79e9\u68af\u5ea6\u5b50\u7a7a\u95f4\u4e4b\u95f4\u7684\u9ad8\u6548\u8f6c\u6362\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLotus\u662f\u6700\u6709\u6548\u7684\u65b9\u6cd5\uff1a\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1130%\uff0c\u68af\u5ea6\u548c\u4f18\u5316\u5668\u72b6\u6001\u7684\u5185\u5b58\u6d88\u8017\u964d\u4f4e40%\uff0c\u540c\u65f6\u5728\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u4efb\u52a1\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Lotus\u901a\u8fc7\u7b80\u5355\u7684\u6295\u5f71\u8fc7\u7a0b\u4fee\u6539\u6210\u529f\u89e3\u51b3\u4e86\u5185\u5b58-\u65f6\u95f4-\u6027\u80fd\u7684\u6743\u8861\u95ee\u9898\uff0c\u6210\u4e3a\u6700\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002"}}
{"id": "2602.01247", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01247", "abs": "https://arxiv.org/abs/2602.01247", "authors": ["Maryam Maghsoudi", "Ayushi Mishra"], "title": "Mechanistic Interpretability of Brain-to-Speech Models Across Speech Modes", "comment": null, "summary": "Brain-to-speech decoding models demonstrate robust performance in vocalized, mimed, and imagined speech; yet, the fundamental mechanisms via which these models capture and transmit information across different speech modalities are less explored. In this work, we use mechanistic interpretability to causally investigate the internal representations of a neural speech decoder. We perform cross-mode activation patching of internal activations across speech modes, and use tri-modal interpolation to examine whether speech representations vary discretely or continuously. We use coarse-to-fine causal tracing and causal scrubbing to find localized causal structure, allowing us to find internal subspaces that are sufficient for cross-mode transfer. In order to determine how finely distributed these effects are within layers, we perform neuron-level activation patching. We discover that small but not distributed subsets of neurons, rather than isolated units, affect the cross-mode transfer. Our results show that speech modes lie on a shared continuous causal manifold, and cross-mode transfer is mediated by compact, layer-specific subspaces rather than diffuse activity. Together, our findings give a causal explanation for how speech modality information is organized and used in brain-to-speech decoding models, revealing hierarchical and direction-dependent representational structure across speech modes.", "AI": {"tldr": "This paper uses mechanistic interpretability to investigate how brain-to-speech decoding models represent and transfer information across vocalized, mimed, and imagined speech. The researchers found that speech modalities share a continuous causal manifold, with cross-modal transfer mediated by compact, layer-specific neural subspaces rather than distributed activity.", "motivation": "While brain-to-speech decoding models show robust performance across different speech modalities (vocalized, mimed, imagined), the fundamental mechanisms of how these models capture and transmit information across modalities remain poorly understood.", "method": "The study employs mechanistic interpretability techniques including cross-mode activation patching, tri-modal interpolation, coarse-to-fine causal tracing, causal scrubbing, and neuron-level activation patching to causally investigate internal representations of a neural speech decoder.", "result": "Key findings include: (1) Speech modes exist on a shared continuous causal manifold; (2) Cross-mode transfer is mediated by compact, layer-specific subspaces rather than diffuse activity; (3) Small, non-distributed subsets of neurons (not isolated units) are responsible for cross-mode transfer effects.", "conclusion": "The research provides a causal explanation for how speech modality information is organized in brain-to-speech decoding models, revealing a hierarchical and direction-dependent representational structure across speech modes."}}
{"id": "2602.01260", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01260", "abs": "https://arxiv.org/abs/2602.01260", "authors": ["Soumyadeep Roy", "Shashwat Kushwaha", "Ambedkar Dukkipati"], "title": "Sample Efficient Active Algorithms for Offline Reinforcement Learning", "comment": null, "summary": "Offline reinforcement learning (RL) enables policy learning from static data but often suffers from poor coverage of the state-action space and distributional shift problems. This problem can be addressed by allowing limited online interactions to selectively refine uncertain regions of the learned value function, which is referred to as Active Reinforcement Learning (ActiveRL). While there has been good empirical success, no theoretical analysis is available in the literature. We fill this gap by developing a rigorous sample-complexity analysis of ActiveRL through the lens of Gaussian Process (GP) uncertainty modeling. In this respect, we propose an algorithm and using GP concentration inequalities and information-gain bounds, we derive high-probability guarantees showing that an $\u03b5$-optimal policy can be learned with ${\\mathcal{O}}(1/\u03b5^2)$ active transitions, improving upon the $\u03a9(1/\u03b5^2(1-\u03b3)^4)$ rate of purely offline methods. Our results reveal that ActiveRL achieves near-optimal information efficiency, that is, guided uncertainty reduction leads to accelerated value-function convergence with minimal online data. Our analysis builds on GP concentration inequalities and information-gain bounds, bridging Bayesian nonparametric regression and reinforcement learning theories. We conduct several experiments to validate the algorithm and theoretical findings.", "AI": {"tldr": "\u9996\u6b21\u4e3aActiveRL\u63d0\u4f9b\u4e25\u683c\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660e\u901a\u8fc7\u9ad8\u65af\u8fc7\u7a0b\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u4ec5\u9700O(1/\u03b5\u00b2)\u5728\u7ebf\u4ea4\u4e92\u5373\u53ef\u5b66\u4e60\u03b5-\u6700\u4f18\u7b56\u7565\uff0c\u663e\u8457\u4f18\u4e8e\u7eaf\u79bb\u7ebf\u65b9\u6cd5\u7684\u03a9(1/\u03b5\u00b2(1-\u03b3)\u2074)\u6837\u672c\u590d\u6742\u5ea6\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u56e0\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u8986\u76d6\u4e0d\u8db3\u548c\u5206\u5e03\u504f\u79fb\u95ee\u9898\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\uff0c\u73b0\u6709ActiveRL\u65b9\u6cd5\u867d\u6709\u6548\u4f46\u7f3a\u4e4f\u7406\u8bba\u5206\u6790\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u7684ActiveRL\u7b97\u6cd5\uff0c\u5229\u7528GP\u6d53\u5ea6\u4e0d\u7b49\u5f0f\u548c\u589e\u76ca\u4fe1\u606f\u8fb9\u754c\u8fdb\u884c\u7406\u8bba\u5206\u6790\u3002", "result": "\u63a8\u5bfc\u51fa\u9ad8\u6982\u7387\u4fdd\u8bc1\uff1a\u5b66\u4e60\u03b5-\u6700\u4f18\u7b56\u7565\u4ec5\u9700O(1/\u03b5\u00b2)\u4e3b\u52a8\u8f6c\u79fb\u6837\u672c\uff0c\u8fbe\u5230\u8fd1\u6700\u4f18\u4fe1\u606f\u6548\u7387\uff0c\u4e14\u6536\u655b\u901f\u5ea6\u4f18\u4e8e\u7eaf\u79bb\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u8d1d\u53f6\u65af\u975e\u53c2\u6570\u56de\u5f52\u4e0e\u5f3a\u5316\u5b66\u4e60\u7406\u8bba\u878d\u5408\uff0c\u9996\u6b21\u5efa\u7acbActiveRL\u7684\u6837\u672c\u590d\u6742\u5ea6\u7406\u8bba\u6846\u67b6\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u7b97\u6cd5\u4e0e\u7406\u8bba\u6709\u6548\u6027\u3002"}}
{"id": "2602.01265", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01265", "abs": "https://arxiv.org/abs/2602.01265", "authors": ["Jiangnan Zhu", "Yukai Xu", "Li Xiong", "Yixuan Liu", "Junxu Liu", "Hong kyu Lee", "Yujie Gu"], "title": "BicKD: Bilateral Contrastive Knowledge Distillation", "comment": null, "summary": "Knowledge distillation (KD) is a machine learning framework that transfers knowledge from a teacher model to a student model. The vanilla KD proposed by Hinton et al. has been the dominant approach in logit-based distillation and demonstrates compelling performance. However, it only performs sample-wise probability alignment between teacher and student's predictions, lacking an mechanism for class-wise comparison. Besides, vanilla KD imposes no structural constraint on the probability space. In this work, we propose a simple yet effective methodology, bilateral contrastive knowledge distillation (BicKD). This approach introduces a novel bilateral contrastive loss, which intensifies the orthogonality among different class generalization spaces while preserving consistency within the same class. The bilateral formulation enables explicit comparison of both sample-wise and class-wise prediction patterns between teacher and student. By emphasizing probabilistic orthogonality, BicKD further regularizes the geometric structure of the predictive distribution. Extensive experiments show that our BicKD method enhances knowledge transfer, and consistently outperforms state-of-the-art knowledge distillation techniques across various model architectures and benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5BicKD\uff0c\u901a\u8fc7\u53cc\u8fb9\u5bf9\u6bd4\u635f\u5931\u540c\u65f6\u5b9e\u73b0\u6837\u672c\u7ea7\u548c\u7c7b\u522b\u7ea7\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u5e76\u589e\u5f3a\u9884\u6d4b\u6982\u7387\u7a7a\u95f4\u7684\u7ed3\u6784\u7ea6\u675f\uff0c\u4ece\u800c\u63d0\u5347\u84b8\u998f\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff08vanilla KD\uff09\u4ec5\u8fdb\u884c\u6837\u672c\u7ea7\u6982\u7387\u5bf9\u9f50\uff0c\u7f3a\u4e4f\u7c7b\u522b\u7ea7\u6bd4\u8f83\u673a\u5236\uff0c\u4e14\u672a\u5bf9\u6982\u7387\u7a7a\u95f4\u65bd\u52a0\u7ed3\u6784\u7ea6\u675f\uff0c\u9650\u5236\u4e86\u5176\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u53cc\u8fb9\u5bf9\u6bd4\u77e5\u8bc6\u84b8\u998f\uff08BicKD\uff09\uff0c\u5f15\u5165\u65b0\u9896\u7684\u53cc\u8fb9\u5bf9\u6bd4\u635f\u5931\uff0c\u901a\u8fc7\u589e\u5f3a\u4e0d\u540c\u7c7b\u522b\u7a7a\u95f4\u7684\u6b63\u4ea4\u6027\u548c\u4fdd\u6301\u540c\u7c7b\u4e00\u81f4\u6027\uff0c\u5b9e\u73b0\u5bf9\u6559\u5e08\u6a21\u578b\u548c\u5b66\u751f\u6a21\u578b\u9884\u6d4b\u6a21\u5f0f\u7684\u53cc\u5411\u663e\u5f0f\u6bd4\u8f83\u3002", "result": "\u5728\u591a\u79cd\u6a21\u578b\u67b6\u6784\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cBicKD\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u77e5\u8bc6\u8fc1\u79fb\u6548\u679c\u3002", "conclusion": "BicKD\u901a\u8fc7\u53cc\u8fb9\u5bf9\u6bd4\u5b66\u4e60\u540c\u65f6\u6355\u83b7\u6837\u672c\u7ea7\u548c\u7c7b\u522b\u7ea7\u77e5\u8bc6\uff0c\u5e76\u6b63\u5219\u5316\u9884\u6d4b\u5206\u5e03\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u77e5\u8bc6\u84b8\u998f\u6539\u8fdb\u65b9\u6848\u3002"}}
{"id": "2602.01267", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01267", "abs": "https://arxiv.org/abs/2602.01267", "authors": ["Jiayu Bai", "Danchen Yu", "Zhenyu Liao", "TianQi Hou", "Feng Zhou", "Robert C. Qiu", "Zenan Ling"], "title": "Diving into Kronecker Adapters: Component Design Matters", "comment": null, "summary": "Kronecker adapters have emerged as a promising approach for fine-tuning large-scale models, enabling high-rank updates through tunable component structures. However, existing work largely treats the component structure as a fixed or heuristic design choice, leaving the dimensions and number of Kronecker components underexplored. In this paper, we identify component structure as a key factor governing the capacity of Kronecker adapters. We perform a fine-grained analysis of both the dimensions and number of Kronecker components. In particular, we show that the alignment between Kronecker adapters and full fine-tuning depends on component configurations. Guided by these insights, we propose Component Designed Kronecker Adapters (CDKA). We further provide parameter-budget-aware configuration guidelines and a tailored training stabilization strategy for practical deployment. Experiments across various natural language processing tasks demonstrate the effectiveness of CDKA. Code is available at https://github.com/rainstonee/CDKA.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5206\u6790Kronecker\u7ec4\u4ef6\u7ed3\u6784\u5bf9\u6a21\u578b\u5bb9\u91cf\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u7ec4\u4ef6\u8bbe\u8ba1\u7684Kronecker\u9002\u914d\u5668(CDKA)\uff0c\u63d0\u4f9b\u53c2\u6570\u9884\u7b97\u611f\u77e5\u7684\u914d\u7f6e\u6307\u5357\u548c\u8bad\u7ec3\u7a33\u5b9a\u5316\u7b56\u7565\uff0c\u5728NLP\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709Kronecker\u9002\u914d\u5668\u5c06\u7ec4\u4ef6\u7ed3\u6784\u89c6\u4e3a\u56fa\u5b9a\u6216\u542f\u53d1\u5f0f\u8bbe\u8ba1\uff0c\u5176\u7ef4\u5ea6\u548c\u6570\u91cf\u7f3a\u4e4f\u5145\u5206\u63a2\u7d22\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5bb9\u91cf\u548c\u4e0e\u5168\u91cf\u5fae\u8c03\u7684\u5bf9\u9f50\u6548\u679c\u3002", "method": "\u5bf9Kronecker\u7ec4\u4ef6\u7684\u7ef4\u5ea6\u548c\u6570\u91cf\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5206\u6790\uff0c\u63d0\u51faCDKA\u6846\u67b6\uff0c\u63d0\u4f9b\u53c2\u6570\u9884\u7b97\u611f\u77e5\u7684\u914d\u7f6e\u6307\u5357\u548c\u5b9a\u5236\u5316\u8bad\u7ec3\u7a33\u5b9a\u5316\u7b56\u7565\u3002", "result": "\u5728\u591a\u79cd\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86CDKA\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7ec4\u4ef6\u7ed3\u6784\u662f\u51b3\u5b9aKronecker\u9002\u914d\u5668\u5bb9\u91cf\u7684\u5173\u952e\u56e0\u7d20\uff0cCDKA\u901a\u8fc7\u5408\u7406\u914d\u7f6e\u548c\u6539\u8fdb\u8bad\u7ec3\u7b56\u7565\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5b9e\u73b0\u5927\u89c4\u6a21\u6a21\u578b\u7684\u9ad8\u6548\u5fae\u8c03\u3002"}}
{"id": "2602.01270", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01270", "abs": "https://arxiv.org/abs/2602.01270", "authors": ["Boxuan Zhang", "Weipu Zhang", "Zhaohan Feng", "Wei Xiao", "Jian Sun", "Jie Chen", "Gang Wang"], "title": "Mixture-of-World Models: Scaling Multi-Task Reinforcement Learning with Modular Latent Dynamics", "comment": null, "summary": "A fundamental challenge in multi-task reinforcement learning (MTRL) is achieving sample efficiency in visual domains where tasks exhibit substantial heterogeneity in both observations and dynamics. Model-based reinforcement learning offers a promising path to improved sample efficiency through world models, but standard monolithic architectures struggle to capture diverse task dynamics, resulting in poor reconstruction and prediction accuracy. We introduce Mixture-of-World Models (MoW), a scalable architecture that combines modular variational autoencoders for task-adaptive visual compression, a hybrid Transformer-based dynamics model with task-conditioned experts and a shared backbone, and a gradient-based task clustering strategy for efficient parameter allocation. On the Atari 100k benchmark, a single MoW agent trained once on 26 Atari games achieves a mean human-normalized score of 110.4%, competitive with the score of 114.2% achieved by STORM, an ensemble of 26 task-specific models, while using 50% fewer parameters. On Meta-World, MoW achieves a 74.5% average success rate within 300 thousand environment steps, establishing a new state of the art. These results demonstrate that MoW provides a scalable and parameter-efficient foundation for generalist world models.", "AI": {"tldr": "A mixture-of-world models (MoW) architecture combining modular VAEs, task-conditioned Transformer dynamics, and gradient-based task clustering achieves competitive multi-task RL performance with 50% fewer parameters than task-specific ensembles.", "motivation": "Standard monolithic model-based RL architectures struggle with sample efficiency and reconstruction accuracy when handling heterogeneous tasks with diverse visual observations and dynamics in multi-task settings.", "method": "Proposes Mixture-of-World Models (MoW) with: 1) modular variational autoencoders for task-adaptive visual compression; 2) hybrid Transformer dynamics model with task-conditioned experts and shared backbone; 3) gradient-based task clustering for parameter allocation.", "result": "On Atari 100k: 110.4% human-normalized score (vs 114.2% for STORM ensemble) with 50% fewer parameters. On Meta-World: 74.5% success rate within 300k steps, setting new state-of-the-art.", "conclusion": "MoW provides a scalable, parameter-efficient foundation for building generalist world models that can handle diverse multi-task environments effectively."}}
{"id": "2602.01279", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01279", "abs": "https://arxiv.org/abs/2602.01279", "authors": ["Sergio Calvo-Ordo\u00f1ez", "Jonathan Plenk", "Richard Bergna", "\u00c1lvaro Cartea", "Yarin Gal", "Jose Miguel Hern\u00e1ndez-Lobato", "Kamil Ciosek"], "title": "Richer Bayesian Last Layers with Subsampled NTK Features", "comment": "Preprint, work in progress", "summary": "Bayesian Last Layers (BLLs) provide a convenient and computationally efficient way to estimate uncertainty in neural networks. However, they underestimate epistemic uncertainty because they apply a Bayesian treatment only to the final layer, ignoring uncertainty induced by earlier layers. We propose a method that improves BLLs by leveraging a projection of Neural Tangent Kernel (NTK) features onto the space spanned by the last-layer features. This enables posterior inference that accounts for variability of the full network while retaining the low computational cost of inference of a standard BLL. We show that our method yields posterior variances that are provably greater or equal to those of a standard BLL, correcting its tendency to underestimate epistemic uncertainty. To further reduce computational cost, we introduce a uniform subsampling scheme for estimating the projection matrix and for posterior inference. We derive approximation bounds for both types of sub-sampling. Empirical evaluations on UCI regression, contextual bandits, image classification, and out-of-distribution detection tasks in image and tabular datasets, demonstrate improved calibration and uncertainty estimates compared to standard BLLs and competitive baselines, while reducing computational cost.", "AI": {"tldr": "This paper proposes an improved Bayesian Last Layers (BLL) method that projects Neural Tangent Kernel features onto the last-layer feature space to capture full network uncertainty. The method provably increases epistemic uncertainty estimates, corrects BLL's underestimation bias, and uses subsampling for efficiency. Empirical results show better calibration and uncertainty quantification across multiple tasks with lower computational cost.", "motivation": "Standard Bayesian Last Layers (BLLs) underestimate epistemic uncertainty because they only consider uncertainty in the final layer, ignoring contributions from earlier layers. This leads to overconfident predictions. The motivation is to develop a method that captures the full network's epistemic uncertainty while maintaining the computational efficiency of BLLs.", "method": "The authors propose projecting Neural Tangent Kernel (NTK) features onto the space spanned by last-layer features. This allows posterior inference that accounts for variability across all network layers. They also introduce uniform subsampling to efficiently estimate the projection matrix and perform posterior inference, with theoretical approximation bounds.", "result": "The method provably produces posterior variances greater than or equal to standard BLLs, correcting epistemic uncertainty underestimation. Empirical evaluations on UCI regression, contextual bandits, image classification, and OOD detection tasks show improved calibration and uncertainty estimates compared to standard BLLs and competitive baselines, while reducing computational cost.", "conclusion": "The proposed method successfully addresses BLL's epistemic uncertainty underestimation by incorporating information from all layers through NTK feature projection, with theoretical guarantees and empirical validation across diverse tasks, achieving better uncertainty quantification with lower computational overhead."}}
{"id": "2602.01285", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01285", "abs": "https://arxiv.org/abs/2602.01285", "authors": ["Kangjun Noh", "Seongchan Lee", "Ilmun Kim", "Kyungwoo Song"], "title": "Multi-LLM Adaptive Conformal Inference for Reliable LLM Responses", "comment": "Accepted to ICLR 2026", "summary": "Ensuring factuality is essential for the safe use of Large Language Models (LLMs) in high-stakes domains such as medicine and law. Conformal inference provides distribution-free guarantees, but existing approaches are either overly conservative, discarding many true-claims, or rely on adaptive error rates and simple linear models that fail to capture complex group structures. To address these challenges, we reformulate conformal inference in a multiplicative filtering setting, modeling factuality as a product of claim-level scores. Our method, Multi-LLM Adaptive Conformal Inference (MACI), leverages ensembles to produce more accurate factuality-scores, which in our experiments led to higher retention, while validity is preserved through group-conditional calibration. Experiments show that MACI consistently achieves user-specified coverage with substantially higher retention and lower time cost than baselines. Our repository is available at https://github.com/MLAI-Yonsei/MACI", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMACI\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u6784\u5171\u5f62\u63a8\u65ad\u4e3a\u4e58\u6cd5\u8fc7\u6ee4\u6846\u67b6\uff0c\u5229\u7528\u96c6\u6210\u6a21\u578b\u751f\u6210\u66f4\u51c6\u786e\u7684\u4e8b\u5b9e\u6027\u8bc4\u5206\uff0c\u5728\u4fdd\u6301\u9ad8\u8986\u76d6\u7387\u7684\u540c\u6642\u663e\u8457\u63d0\u9ad8\u4e86\u771f\u5b9e\u58f0\u660e\u7684\u4fdd\u7559\u7387\u5e76\u964d\u4f4e\u4e86\u65f6\u95f4\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u4e8b\u5b9e\u6027\u4fdd\u969c\u65b9\u6cd5\u5b58\u5728\u8fc7\u5ea6\u4fdd\u5b88\uff08\u4e22\u5f03\u8fc7\u591a\u771f\u5b9e\u58f0\u660e\uff09\u6216\u4f9d\u8d56\u7b80\u5355\u7ebf\u6027\u6a21\u578b\u65e0\u6cd5\u6355\u6349\u590d\u6742\u7fa4\u4f53\u7ed3\u6784\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u7684\u5206\u5e03\u65e0\u5173\u4fdd\u8bc1\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMulti-LLM Adaptive Conformal Inference (MACI)\u65b9\u6cd5\uff1a1\uff09\u5c06\u5171\u5f62\u63a8\u65ad\u91cd\u6784\u4e3a\u4e58\u6cd5\u8fc7\u6ee4\u6846\u67b6\uff0c\u5c06\u4e8b\u5b9e\u6027\u5efa\u6a21\u4e3a\u58f0\u660e\u7ea7\u5206\u6570\u7684\u4e58\u79ef\uff1b2\uff09\u5229\u7528\u96c6\u6210\u6a21\u578b\u4ea7\u751f\u66f4\u51c6\u786e\u7684\u4e8b\u5b9e\u6027\u8bc4\u5206\uff1b3\uff09\u901a\u8fc7\u7fa4\u4f53\u6761\u4ef6\u6821\u51c6\u4fdd\u6301\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMACI\u80fd\u6301\u7eed\u8fbe\u5230\u7528\u6237\u6307\u5b9a\u7684\u8986\u76d6\u7387\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u9ad8\u7684\u4fdd\u7559\u7387\uff08\u4fdd\u7559\u66f4\u591a\u771f\u5b9e\u58f0\u660e\uff09\u548c\u66f4\u4f4e\u7684\u65f6\u95f4\u6210\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u96c6\u6210\u6a21\u578b\u548c\u7fa4\u4f53\u6761\u4ef6\u6821\u51c6\uff0c\u5728\u4fdd\u6301\u5171\u5f62\u63a8\u65ad\u7684\u5206\u5e03\u65e0\u5173\u4fdd\u8bc1\u4f18\u52bf\u4e0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4fdd\u5b88\u6027\u9ad8\u548c\u7ed3\u6784\u6355\u6349\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e3a\u9ad8\u53ef\u9760\u6027\u9886\u57df\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u4e8b\u5b9e\u6027\u4fdd\u969c\u3002"}}
{"id": "2602.01288", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01288", "abs": "https://arxiv.org/abs/2602.01288", "authors": ["Chenghua Zhu", "Siyan Wu", "Xiangkang Zeng", "Zishan Xu", "Zhaolu Kang", "Yifu Guo", "Yuquan Lu", "Junduan Huang", "Guojing Zhou"], "title": "EDIS: Diagnosing LLM Reasoning via Entropy Dynamics", "comment": "Under review at ICML 2026", "summary": "Entropy-based confidence signals are increasingly leveraged to improve reasoning in large language models (LLMs), yet existing approaches treat confidence as a static quantity -- typically aggregated over tokens. We show that the \\emph{temporal evolution} of confidence during generation carries richer information than aggregate statistics alone. Analyzing token-level entropy trajectories, we identify characteristic patterns distinguishing correct from incorrect reasoning: erroneous solutions exhibit unstable dynamics, including burst spikes (sustained uncertainty growth) and peak-valley spikes (sharp rebounds following transient confidence). These patterns persist across models and training stages, suggesting they reflect intrinsic properties of reasoning failure rather than superficial noise. To formalize this observation, we introduce the Entropy Dynamics Instability Score (\\textbf{EDIS}), a trajectory-level metric quantifying instability in entropy evolution. EDIS serves as an effective diagnostic signal for inference-time selection, substantially improving reasoning accuracy, and offers a promising direction for training-time sample curation. Our findings establish entropy dynamics as an underexplored yet informative lens for understanding and improving LLM reasoning.", "AI": {"tldr": "\u8be5\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8fc7\u7a0b\u4e2d\u71b5\uff08\u7f6e\u4fe1\u5ea6\uff09\u7684\u65f6\u95f4\u6f14\u5316\u6bd4\u9759\u6001\u805a\u5408\u503c\u5305\u542b\u66f4\u4e30\u5bcc\u7684\u4fe1\u606f\u3002\u5b83\u8bc6\u522b\u51fa\u71b5\u8f68\u8ff9\u4e2d\u4e0e\u9519\u8bef\u63a8\u7406\u76f8\u5173\u7684\u7279\u5b9a\u4e0d\u7a33\u5b9a\u6a21\u5f0f\uff08\u7206\u53d1\u6027\u5cf0\u503c\u548c\u5cf0\u8c37\u5cf0\u503c\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEDIS\u7684\u5ea6\u91cf\u6807\u51c6\uff0c\u53ef\u63d0\u5347\u63a8\u7406\u51c6\u786e\u6027\u548c\u6837\u672c\u7b5b\u9009\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u71b5\u7684\u7f6e\u4fe1\u5ea6\u4fe1\u53f7\u5c06\u7f6e\u4fe1\u5ea6\u89c6\u4e3a\u9759\u6001\u805a\u5408\u503c\uff0c\u5ffd\u7565\u4e86\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u65f6\u95f4\u52a8\u6001\u6f14\u5316\u3002\u4f5c\u8005\u5047\u8bbe\u71b5\u7684\u65f6\u95f4\u8f68\u8ff9\u5305\u542b\u66f4\u591a\u5173\u4e8e\u63a8\u7406\u8d28\u91cf\u7684\u4fe1\u606f\u3002", "method": "\u4f5c\u8005\u5206\u6790\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684token\u7ea7\u71b5\u8f68\u8ff9\uff0c\u8bc6\u522b\u4e0e\u9519\u8bef\u63a8\u7406\u76f8\u5173\u7684\u7279\u5f81\u6027\u4e0d\u7a33\u5b9a\u6a21\u5f0f\uff08\u7206\u53d1\u6027\u5cf0\u503c\u548c\u5cf0\u8c37\u5cf0\u503c\uff09\uff0c\u5e76\u5c06\u8fd9\u4e9b\u89c2\u5bdf\u5f62\u5f0f\u5316\u4e3a\u71b5\u52a8\u6001\u4e0d\u7a33\u5b9a\u6027\u5206\u6570\uff08EDIS\uff09\uff0c\u4e00\u4e2a\u91cf\u5316\u71b5\u4e0d\u7a33\u5b9a\u6027\u7684\u8f68\u8ff9\u7ea7\u5ea6\u91cf\u3002", "result": "EDIS\u53ef\u4f5c\u4e3a\u6709\u6548\u7684\u8bca\u65ad\u4fe1\u53f7\uff0c\u5728\u63a8\u7406\u65f6\u9009\u62e9\u4e2d\u663e\u8457\u63d0\u9ad8\u63a8\u7406\u51c6\u786e\u6027\uff0c\u5e76\u4e3a\u8bad\u7ec3\u65f6\u6837\u672c\u7b5b\u9009\u63d0\u4f9b\u6709\u524d\u666f\u7684\u65b9\u5411\u3002\u8fd9\u4e9b\u4e0d\u7a33\u5b9a\u6a21\u5f0f\u5728\u4e0d\u540c\u6a21\u578b\u548c\u8bad\u7ec3\u9636\u6bb5\u6301\u7eed\u5b58\u5728\uff0c\u8868\u660e\u5b83\u4eec\u53cd\u6620\u4e86\u63a8\u7406\u5931\u8d25\u7684\u5185\u5728\u5c5e\u6027\u3002", "conclusion": "\u71b5\u52a8\u6001\u4e3a\u7406\u89e3\u548c\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u4f46\u4fe1\u606f\u4e30\u5bcc\u7684\u89c6\u89d2\uff0cEDIS\u662f\u63a8\u7406\u548c\u8bad\u7ec3\u5e94\u7528\u7684\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2602.01289", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01289", "abs": "https://arxiv.org/abs/2602.01289", "authors": ["Dung Anh Hoang", "Cuong Pham anh Trung Le", "Jianfei Cai", "Toan Do"], "title": "Gradient-Aligned Calibration for Post-Training Quantization of Diffusion Models", "comment": null, "summary": "Diffusion models have shown remarkable performance in image synthesis by progressively estimating a smooth transition from a Gaussian distribution of noise to a real image. Unfortunately, their practical deployment is limited by slow inference speed, high memory usage, and the computational demands of the noise estimation process. Post-training quantization (PTQ) emerges as a promising solution to accelerate sampling and reduce memory overhead for diffusion models. Existing PTQ methods for diffusion models typically apply uniform weights to calibration samples across timesteps, which is sub-optimal since data at different timesteps may contribute differently to the diffusion process. Additionally, due to varying activation distributions and gradients across timesteps, a uniform quantization approach is sub-optimal. Each timestep requires a different gradient direction for optimal quantization, and treating them equally can lead to conflicting gradients that degrade performance. In this paper, we propose a novel PTQ method that addresses these challenges by assigning appropriate weights to calibration samples. Specifically, our approach learns to assign optimal weights to calibration samples to align the quantized model's gradients across timesteps, facilitating the quantization process. Extensive experiments on CIFAR-10, LSUN-Bedrooms, and ImageNet demonstrate the superiority of our method compared to other PTQ methods for diffusion models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u540e\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3a\u6821\u51c6\u6837\u672c\u5b66\u4e60\u6700\u4f18\u6743\u91cd\u6765\u5bf9\u9f50\u4e0d\u540c\u65f6\u95f4\u6b65\u7684\u68af\u5ea6\uff0c\u89e3\u51b3\u4e86\u7edf\u4e00\u91cf\u5316\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5b58\u5728\u63a8\u7406\u901f\u5ea6\u6162\u3001\u5185\u5b58\u5360\u7528\u9ad8\u548c\u8ba1\u7b97\u9700\u6c42\u5927\u7684\u95ee\u9898\uff1b\u73b0\u6709PTQ\u65b9\u6cd5\u5728\u4e0d\u540c\u65f6\u95f4\u6b65\u4f7f\u7528\u7edf\u4e00\u6743\u91cd\uff0c\u4f46\u7531\u4e8e\u4e0d\u540c\u65f6\u95f4\u6b65\u7684\u6570\u636e\u8d21\u732e\u3001\u6fc0\u6d3b\u5206\u5e03\u548c\u68af\u5ea6\u5404\u4e0d\u76f8\u540c\uff0c\u8fd9\u79cd\u7edf\u4e00\u65b9\u6cd5\u5e76\u975e\u6700\u4f18\u3002", "method": "\u63d0\u51fa\u4e00\u79cdPTQ\u65b9\u6cd5\uff0c\u5b66\u4e60\u4e3a\u6821\u51c6\u6837\u672c\u5206\u914d\u6700\u4f18\u6743\u91cd\uff0c\u4ee5\u5bf9\u9f50\u8de8\u65f6\u95f4\u6b65\u7684\u91cf\u5316\u6a21\u578b\u68af\u5ea6\uff0c\u4ece\u800c\u4f18\u5316\u91cf\u5316\u8fc7\u7a0b\u3002", "result": "\u5728CIFAR-10\u3001LSUN-Bedrooms\u548cImageNet\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5176\u4ed6\u6269\u6563\u6a21\u578bPTQ\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u68af\u5ea6\u5bf9\u9f50PTQ\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u95f4\u6b65\u7279\u5b9a\u7684\u91cf\u5316\u6311\u6218\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2602.01295", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01295", "abs": "https://arxiv.org/abs/2602.01295", "authors": ["Yu Chen", "Yuhao Liu", "Jiatai Huang", "Yihan Du", "Longbo Huang"], "title": "The BoBW Algorithms for Heavy-Tailed MDPs", "comment": null, "summary": "We investigate episodic Markov Decision Processes with heavy-tailed feedback (HTMDPs). Existing approaches for HTMDPs are conservative in stochastic environments and lack adaptivity in adversarial regimes. In this work, we propose algorithms ```HT-FTRL-OM``` and ```HT-FTRL-UOB``` for HTMDPs that achieve Best-of-Both-Worlds (BoBW) guarantees: instance-independent regret in adversarial environments and logarithmic instance-dependent regret in self-bounding (including the stochastic case) environments. For the known transition setting, ```HT-FTRL-OM``` applies the Follow-The-Regularized-Leader (FTRL) framework over occupancy measures with novel skipping loss estimators, achieving a $\\widetilde{\\mathcal{O}}(T^{1/\u03b1})$ regret bound in adversarial regimes and a $\\mathcal{O}(\\log T)$ regret in stochastic regimes. Building upon this framework, we develop a novel algorithm ```HT-FTRL-UOB``` to tackle the more challenging unknown-transition setting. This algorithm employs a pessimistic skipping loss estimator and achieves a $\\widetilde{\\mathcal{O}}(T^{1/\u03b1} + \\sqrt{T})$ regret in adversarial regimes and a $\\mathcal{O}(\\log^2(T))$ regret in stochastic regimes. Our analysis overcomes key barriers through several technical insights, including a local control mechanism for heavy-tailed shifted losses, a new suboptimal-mass propagation principle, and a novel regret decomposition that isolates transition uncertainty from heavy-tailed estimation errors and skipping bias.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u91cd\u5c3e\u53cd\u9988\u7684\u7247\u6bb5\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08HTMDPs\uff09\uff0c\u63d0\u51fa\u4e24\u79cd\u7b97\u6cd5HT-FTRL-OM\u548cHT-FTRL-UOB\uff0c\u9996\u6b21\u5b9e\u73b0\u6700\u4f73\u517c\u987e\u4e24\u754c\uff08BoBW\uff09\u4fdd\u8bc1\uff1a\u5728\u5bf9\u6297\u6027\u73af\u5883\u4e2d\u83b7\u5f97\u4e0e\u5b9e\u4f8b\u65e0\u5173\u7684\u9057\u61be\u754c\uff0c\u5728\u968f\u673a\u73af\u5883\u4e2d\u83b7\u5f97\u5bf9\u6570\u9057\u61be\u754c\u3002", "motivation": "\u73b0\u6709HTMDPs\u65b9\u6cd5\u5728\u968f\u673a\u73af\u5883\u4e2d\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u4e14\u5728\u5bf9\u6297\u6027\u73af\u5883\u4e0b\u7f3a\u4e4f\u81ea\u9002\u5e94\u6027\uff0c\u65e0\u6cd5\u540c\u65f6\u9002\u5e94\u4e0d\u540c\u73af\u5883\u7279\u6027\uff0c\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u7684\u6846\u67b6\u5b9e\u73b0\u6700\u4f73\u517c\u987e\u4e24\u754c\u7684\u6027\u80fd\u4fdd\u8bc1\u3002", "method": "\u57fa\u4e8eFollow-The-Regularized-Leader\uff08FTRL\uff09\u6846\u67b6\uff0c\u5728\u5df2\u77e5\u8f6c\u79fb\u8bbe\u7f6e\u4e0b\u8bbe\u8ba1HT-FTRL-OM\u7b97\u6cd5\uff0c\u4f7f\u7528\u65b0\u9896\u7684\u8df3\u8fc7\u635f\u5931\u4f30\u8ba1\u5668\uff1b\u5728\u672a\u77e5\u8f6c\u79fb\u8bbe\u7f6e\u4e0b\u8bbe\u8ba1HT-FTRL-UOB\u7b97\u6cd5\uff0c\u91c7\u7528\u60b2\u89c2\u8df3\u8fc7\u635f\u5931\u4f30\u8ba1\u5668\uff0c\u901a\u8fc7\u5c40\u90e8\u63a7\u5236\u673a\u5236\u548c\u6b21\u4f18\u8d28\u91cf\u4f20\u64ad\u539f\u7406\u5904\u7406\u91cd\u5c3e\u53cd\u9988\u3002", "result": "HT-FTRL-OM\u5728\u5bf9\u6297\u6027\u73af\u5883\u4e0b\u8fbe\u5230$\\widetilde{\\mathcal{O}}(T^{1/\u03b1})$\u9057\u61be\uff0c\u5728\u968f\u673a\u73af\u5883\u4e0b\u8fbe\u5230$\\mathcal{O}(\\log T)$\u9057\u61be\uff1bHT-FTRL-UOB\u5728\u5bf9\u6297\u6027\u73af\u5883\u4e0b\u8fbe\u5230$\\widetilde{\\mathcal{O}}(T^{1/\u03b1} + \\sqrt{T})$\u9057\u61be\uff0c\u5728\u968f\u673a\u73af\u5883\u4e0b\u8fbe\u5230$\\mathcal{O}(\\log^2(T))$\u9057\u61be\u3002", "conclusion": "\u901a\u8fc7\u521b\u65b0\u7684\u6280\u672f\u6d1e\u5bdf\uff08\u5305\u62ec\u91cd\u5c3e\u504f\u79fb\u635f\u5931\u7684\u5c40\u90e8\u63a7\u5236\u3001\u6b21\u4f18\u8d28\u91cf\u4f20\u64ad\u539f\u7406\u548c\u65b0\u7684\u9057\u61be\u5206\u89e3\u65b9\u6cd5\uff09\uff0c\u6210\u529f\u514b\u670d\u4e86\u91cd\u5c3e\u53cd\u9988\u548c\u8f6c\u79fb\u4e0d\u786e\u5b9a\u6027\u7684\u5173\u952e\u969c\u788d\uff0c\u4e3aHTMDPs\u63d0\u4f9b\u4e86\u6700\u4f73\u517c\u987e\u4e24\u754c\u7684\u7406\u8bba\u4fdd\u8bc1\u3002"}}
{"id": "2602.01308", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01308", "abs": "https://arxiv.org/abs/2602.01308", "authors": ["Hengjie Cao", "Mengyi Chen", "Yifeng Yang", "Fang Dong", "Ruijun Huang", "Anrui Chen", "Jixian Zhou", "Mingzhi Dong", "Yujiang Wang", "Dongsheng Li", "Wenyi Fang", "Yuanyi Lin", "Fan Wu", "Li Shang"], "title": "Dispelling the Curse of Singularities in Neural Network Optimizations", "comment": null, "summary": "This work investigates the optimization instability of deep neural networks from a less-explored yet insightful perspective: the emergence and amplification of singularities in the parametric space. Our analysis reveals that parametric singularities inevitably grow with gradient updates and further intensify alignment with representations, leading to increased singularities in the representation space. We show that the gradient Frobenius norms are bounded by the top singular values of the weight matrices, and as training progresses, the mutually reinforcing growth of weight and representation singularities, termed the curse of singularities, relaxes these bounds, escalating the risk of sharp loss explosions. To counter this, we propose Parametric Singularity Smoothing (PSS), a lightweight, flexible, and effective method for smoothing the singular spectra of weight matrices. Extensive experiments across diverse datasets, architectures, and optimizers demonstrate that PSS mitigates instability, restores trainability even after failure, and improves both training efficiency and generalization.", "AI": {"tldr": "The paper identifies that parametric singularities in neural networks grow during training and amplify representation singularities, causing instability and loss explosions. They propose Parametric Singularity Smoothing (PSS) to smooth weight matrix singular spectra, which effectively mitigates instability, restores trainability after failure, and improves training efficiency and generalization.", "motivation": "Deep neural networks suffer from optimization instability during training, characterized by sharp loss explosions and degraded trainability. The authors investigate this from a novel perspective of parametric singularities and their amplification in representation space, revealing a fundamental issue called the \"curse of singularities\" where weight and representation singularities mutually reinforce each other.", "method": "The authors propose Parametric Singularity Smoothing (PSS), a lightweight and flexible method that smooths the singular value spectra of weight matrices to counteract the growth of parametric singularities during gradient updates.", "result": "Extensive experiments demonstrate that PSS: (1) mitigates training instability, (2) can restore trainability even after failure occurs, and (3) improves both training efficiency and model generalization across diverse datasets, architectures, and optimizers.", "conclusion": "The \"curse of singularities\" is a fundamental cause of optimization instability in deep learning. PSS provides an effective, practical solution by smoothing singular spectra, leading to more stable training, better generalization, and the ability to recover from training failures."}}
{"id": "2602.01312", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01312", "abs": "https://arxiv.org/abs/2602.01312", "authors": ["Han Tong", "Shubhangi Ghosh", "Haolin Zou", "Arian Maleki"], "title": "Imperfect Influence, Preserved Rankings: A Theory of TRAK for Data Attribution", "comment": null, "summary": "Data attribution, tracing a model's prediction back to specific training data, is an important tool for interpreting sophisticated AI models. The widely used TRAK algorithm addresses this challenge by first approximating the underlying model with a kernel machine and then leveraging techniques developed for approximating the leave-one-out (ALO) risk. Despite its strong empirical performance, the theoretical conditions under which the TRAK approximations are accurate as well as the regimes in which they break down remain largely unexplored. In this paper, we provide a theoretical analysis of the TRAK algorithm, characterizing its performance and quantifying the errors introduced by the approximations on which the method relies. We show that although the approximations incur significant errors, TRAK's estimated influence remains highly correlated with the original influence and therefore largely preserves the relative ranking of data points. We corroborate our theoretical results through extensive simulations and empirical studies.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9TRAK\u6570\u636e\u5f52\u56e0\u7b97\u6cd5\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660e\u5c3d\u7ba1\u5176\u8fd1\u4f3c\u5f15\u5165\u663e\u8457\u8bef\u5dee\uff0c\u4f46\u4f30\u8ba1\u7684\u5f71\u54cd\u529b\u4e0e\u771f\u5b9e\u5f71\u54cd\u529b\u9ad8\u5ea6\u76f8\u5173\uff0c\u4e14\u80fd\u4fdd\u6301\u6570\u636e\u70b9\u6392\u5e8f\uff0c\u6a21\u62df\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u7ed3\u8bba\u3002", "motivation": "\u5c3d\u7ba1TRAK\u5728\u6570\u636e\u5f52\u56e0\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u51c6\u786e\u6027\u7684\u7406\u8bba\u6761\u4ef6\u548c\u5931\u6548\u673a\u5236\u5c1a\u4e0d\u660e\u786e\uff0c\u9650\u5236\u4e86\u5bf9\u5176\u5de5\u4f5c\u539f\u7406\u7684\u7406\u89e3\u3002", "method": "\u4f5c\u8005\u5bf9TRAK\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u523b\u753b\u5176\u6027\u80fd\u5e76\u91cf\u5316\u6838\u673a\u5668\u8fd1\u4f3c\u548cALO\u98ce\u9669\u8fd1\u4f3c\u6280\u672f\u5e26\u6765\u7684\u8bef\u5dee\u3002", "result": "\u5206\u6790\u8868\u660eTRAK\u7684\u8fd1\u4f3c\u4f1a\u4ea7\u751f\u663e\u8457\u8bef\u5dee\uff0c\u4f46\u4f30\u8ba1\u7684\u5f71\u54cd\u529b\u5206\u6570\u4e0e\u539f\u59cb\u5f71\u54cd\u529b\u9ad8\u5ea6\u76f8\u5173\uff0c\u4e14\u57fa\u672c\u4fdd\u6301\u4e86\u8bad\u7ec3\u6570\u636e\u70b9\u7684\u76f8\u5bf9\u6392\u5e8f\u3002", "conclusion": "TRAK\u5728\u7406\u8bba\u4e0a\u662f\u53ef\u9760\u7684\u6570\u636e\u5f52\u56e0\u65b9\u6cd5\uff0c\u5c3d\u7ba1\u5b58\u5728\u8fd1\u4f3c\u8bef\u5dee\uff0c\u4f46\u80fd\u4fdd\u6301\u6392\u5e8f\u5b8c\u6574\u6027\uff0c\u9002\u7528\u4e8e\u89e3\u91ca\u590d\u6742AI\u6a21\u578b\u3002"}}
{"id": "2602.01322", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01322", "abs": "https://arxiv.org/abs/2602.01322", "authors": ["Panagiotis Koromilas", "Andreas D. Demou", "James Oldfield", "Yannis Panagakis", "Mihalis Nicolaou"], "title": "PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding", "comment": null, "summary": "Sparse autoencoders (SAEs) have emerged as a promising method for interpreting neural network representations by decomposing activations into sparse combinations of dictionary atoms. However, SAEs assume that features combine additively through linear reconstruction, an assumption that cannot capture compositional structure: linear models cannot distinguish whether \"Starbucks\" arises from the composition of \"star\" and \"coffee\" features or merely their co-occurrence. This forces SAEs to allocate monolithic features for compound concepts rather than decomposing them into interpretable constituents. We introduce PolySAE, which extends the SAE decoder with higher-order terms to model feature interactions while preserving the linear encoder essential for interpretability. Through low-rank tensor factorization on a shared projection subspace, PolySAE captures pairwise and triple feature interactions with small parameter overhead (3% on GPT2). Across four language models and three SAE variants, PolySAE achieves an average improvement of approximately 8% in probing F1 while maintaining comparable reconstruction error, and produces 2-10$\\times$ larger Wasserstein distances between class-conditional feature distributions. Critically, learned interaction weights exhibit negligible correlation with co-occurrence frequency ($r = 0.06$ vs. $r = 0.82$ for SAE feature covariance), suggesting that polynomial terms capture compositional structure, such as morphological binding and phrasal composition, largely independent of surface statistics.", "AI": {"tldr": "PolySAE\u901a\u8fc7\u591a\u9879\u5f0f\u89e3\u7801\u5668\u6269\u5c55\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff0c\u4ee5\u6355\u83b7\u7ec4\u5408\u6027\u7279\u5f81\u4ea4\u4e92\uff0c\u5728\u4ec5\u589e\u52a03%\u53c2\u6570\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u533a\u5206\u7ec4\u5408\u4e0e\u5171\u73b0\u6765\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u6807\u51c6\u7a00\u758f\u81ea\u7f16\u7801\u5668(SAE)\u5047\u8bbe\u7279\u5f81\u901a\u8fc7\u7ebf\u6027\u91cd\u6784\u4ee5\u52a0\u6027\u65b9\u5f0f\u7ec4\u5408\uff0c\u65e0\u6cd5\u6355\u6349\u7ec4\u5408\u6027\u7ed3\u6784\u2014\u2014\u7ebf\u6027\u6a21\u578b\u65e0\u6cd5\u533a\u5206\"Starbucks\"\u662f\"star\"\u548c\"coffee\"\u7279\u5f81\u7684\u7ec4\u5408\u8fd8\u662f\u4ec5\u4ec5\u5171\u73b0\uff0c\u8fd9\u8feb\u4f7fSAE\u4e3a\u590d\u5408\u6982\u5ff5\u5206\u914d\u5355\u4e00\u7279\u5f81\u800c\u975e\u53ef\u89e3\u91ca\u7684\u7ec4\u6210\u90e8\u5206\u3002", "method": "\u63d0\u51faPolySAE\uff0c\u5728\u4fdd\u7559\u7ebf\u6027\u7f16\u7801\u5668\u4ee5\u786e\u4fdd\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\uff0c\u6269\u5c55SAE\u89e3\u7801\u5668\u52a0\u5165\u9ad8\u9636\u9879\u4ee5\u5efa\u6a21\u7279\u5f81\u4ea4\u4e92\uff0c\u901a\u8fc7\u5728\u5171\u4eab\u6295\u5f71\u5b50\u7a7a\u95f4\u4e0a\u8fdb\u884c\u4f4e\u79e9\u5f20\u91cf\u5206\u89e3\uff0c\u6355\u83b7\u4e24\u4e24\u548c\u4e09\u7279\u5f81\u4ea4\u4e92\u3002", "result": "\u5728\u56db\u4e2a\u8bed\u8a00\u6a21\u578b\u548c\u4e09\u4e2aSAE\u53d8\u4f53\u4e0a\uff0cPolySAE\u5e73\u5747\u63d0\u5347\u7ea68%\u7684\u63a2\u6d4bF1\u5206\u6570\uff0c\u4fdd\u6301\u76f8\u5f53\u7684\u91cd\u5efa\u8bef\u5dee\uff0c\u4ea7\u751f2-10\u500d\u66f4\u5927\u7684\u7c7b\u6761\u4ef6\u7279\u5f81\u5206\u5e03Wasserstein\u8ddd\u79bb\uff0c\u4e14\u5b66\u4e60\u5230\u7684\u4ea4\u4e92\u6743\u91cd\u4e0e\u5171\u73b0\u9891\u7387\u76f8\u5173\u6027\u6781\u4f4e\uff08r=0.06 vs. SAE\u7279\u5f81\u534f\u65b9\u5dee\u7684r=0.82\uff09\u3002", "conclusion": "\u591a\u9879\u5f0f\u9879\u6355\u83b7\u4e86\u7ec4\u5408\u6027\u7ed3\u6784\uff08\u5982\u5f62\u6001\u7ed1\u5b9a\u548c\u77ed\u8bed\u7ec4\u5408\uff09\uff0c\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u72ec\u7acb\u4e8e\u8868\u5c42\u7edf\u8ba1\u7279\u5f81\uff0c\u4ee5\u66f4\u5c0f\u7684\u53c2\u6570\u5f00\u9500\u63d0\u4f9b\u4e86\u5bf9\u590d\u5408\u6982\u5ff5\u66f4\u53ef\u89e3\u91ca\u7684\u5206\u89e3\u3002"}}
{"id": "2602.01338", "categories": ["cs.LG", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01338", "abs": "https://arxiv.org/abs/2602.01338", "authors": ["Fan Chen", "Sinho Chewi", "Constantinos Daskalakis", "Alexander Rakhlin"], "title": "High-accuracy sampling for diffusion models and log-concave distributions", "comment": null, "summary": "We present algorithms for diffusion model sampling which obtain $\u03b4$-error in $\\mathrm{polylog}(1/\u03b4)$ steps, given access to $\\widetilde O(\u03b4)$-accurate score estimates in $L^2$. This is an exponential improvement over all previous results. Specifically, under minimal data assumptions, the complexity is $\\widetilde O(d\\,\\mathrm{polylog}(1/\u03b4))$ where $d$ is the dimension of the data; under a non-uniform $L$-Lipschitz condition, the complexity is $\\widetilde O(\\sqrt{dL}\\,\\mathrm{polylog}(1/\u03b4))$; and if the data distribution has intrinsic dimension $d_\\star$, then the complexity reduces to $\\widetilde O(d_\\star\\,\\mathrm{polylog}(1/\u03b4))$. Our approach also yields the first $\\mathrm{polylog}(1/\u03b4)$ complexity sampler for general log-concave distributions using only gradient evaluations.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.01339", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01339", "abs": "https://arxiv.org/abs/2602.01339", "authors": ["Difei Xu", "Youming Tao", "Meng Ding", "Chenglin Fan", "Di Wang"], "title": "Finding Differentially Private Second Order Stationary Points in Stochastic Minimax Optimization", "comment": null, "summary": "We provide the first study of the problem of finding differentially private (DP) second-order stationary points (SOSP) in stochastic (non-convex) minimax optimization. Existing literature either focuses only on first-order stationary points for minimax problems or on SOSP for classical stochastic minimization problems. This work provides, for the first time, a unified and detailed treatment of both empirical and population risks. Specifically, we propose a purely first-order method that combines a nested gradient descent--ascent scheme with SPIDER-style variance reduction and Gaussian perturbations to ensure privacy. A key technical device is a block-wise ($q$-period) analysis that controls the accumulation of stochastic variance and privacy noise without summing over the full iteration horizon, yielding a unified treatment of both empirical-risk and population formulations. Under standard smoothness, Hessian-Lipschitzness, and strong concavity assumptions, we establish high-probability guarantees for reaching an $(\u03b1,\\sqrt{\u03c1_\u03a6\u03b1})$-approximate second-order stationary point with $\u03b1= \\mathcal{O}( (\\frac{\\sqrt{d}}{n\\varepsilon})^{2/3})$ for empirical risk objectives and $\\mathcal{O}(\\frac{1}{n^{1/3}} + (\\frac{\\sqrt{d}}{n\\varepsilon})^{1/2})$ for population objectives, matching the best known rates for private first-order stationarity.", "AI": {"tldr": "\u9996\u6b21\u63d0\u51fa\u5dee\u5206\u9690\u79c1\u975e\u51f8\u6781\u5c0f\u6781\u5927\u4f18\u5316\u7684\u4e8c\u9636\u5e73\u7a33\u70b9\u7b97\u6cd5\uff0c\u7ed3\u5408\u5d4c\u5957\u68af\u5ea6\u4e0b\u964d-\u4e0a\u5347\u3001SPIDER\u65b9\u5dee\u7f29\u51cf\u4e0e\u5206\u5757\u5206\u6790\uff0c\u8fbe\u5230\u6700\u4f18\u6536\u655b\u7387\u3002", "motivation": "\u73b0\u6709\u5dee\u5206\u9690\u79c1\u6781\u5c0f\u6781\u5927\u4f18\u5316\u7814\u7a76\u4ec5\u5173\u6ce8\u4e00\u9636\u5e73\u7a33\u70b9\uff0c\u6216\u4ec5\u9488\u5bf9\u7ecf\u5178\u6700\u5c0f\u5316\u95ee\u9898\u8003\u8651\u4e8c\u9636\u5e73\u7a33\u70b9\u3002\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u968f\u673a\u975e\u51f8\u6781\u5c0f\u6781\u5927\u4f18\u5316\u4e2d\u4e8c\u9636\u5e73\u7a33\u70b9\u7684\u5dee\u5206\u9690\u79c1\u7b97\u6cd5\u3002", "method": "\u8bbe\u8ba1\u7eaf\u4e00\u9636\u65b9\u6cd5\uff0c\u91c7\u7528\u5d4c\u5957\u68af\u5ea6\u4e0b\u964d-\u4e0a\u5347\u6846\u67b6\uff0c\u96c6\u6210SPIDER\u5f0f\u65b9\u5dee\u7f29\u51cf\u6280\u672f\uff0c\u5e76\u6ce8\u5165\u9ad8\u65af\u566a\u58f0\u4fdd\u8bc1\u9690\u79c1\u3002\u6838\u5fc3\u521b\u65b0\u4e3a\u5206\u5757(q-\u5468\u671f)\u5206\u6790\u6280\u672f\uff0c\u6709\u6548\u63a7\u5236\u968f\u673a\u65b9\u5dee\u4e0e\u9690\u79c1\u566a\u58f0\u7d2f\u79ef\u3002", "result": "\u5728\u6807\u51c6\u5149\u6ed1\u6027\u3001Hessian-Lipschitz\u53ca\u5f3a\u51f9\u5047\u8bbe\u4e0b\uff0c\u83b7\u5f97\u9ad8\u6982\u7387\u4fdd\u8bc1\uff1a\u7ecf\u9a8c\u98ce\u9669\u76ee\u6807\u8fbe\u5230\u03b1 = O((\u221ad/(n\u03b5))^(2/3))\uff0c\u603b\u4f53\u76ee\u6807\u8fbe\u5230\u03b1 = O(1/n^(1/3) + (\u221ad/(n\u03b5))^(1/2))\uff0c\u4e0e\u4e00\u9636\u5e73\u7a33\u6027\u7684\u6700\u4f18\u901f\u7387\u76f8\u5f53\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u7edf\u4e00\u5904\u7406\u7ecf\u9a8c\u98ce\u9669\u4e0e\u603b\u4f53\u98ce\u9669\uff0c\u4e3a\u5dee\u5206\u9690\u79c1\u6781\u5c0f\u6781\u5927\u4f18\u5316\u63d0\u4f9b\u9996\u4e2a\u4e8c\u9636\u5e73\u7a33\u70b9\u7b97\u6cd5\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002"}}
{"id": "2602.01357", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01357", "abs": "https://arxiv.org/abs/2602.01357", "authors": ["Shangzhe Li", "Xuchao Zhang", "Chetan Bansal", "Weitong Zhang"], "title": "Your Self-Play Algorithm is Secretly an Adversarial Imitator: Understanding LLM Self-Play through the Lens of Imitation Learning", "comment": "35 pages, 5 tables, 3 figures", "summary": "Self-play post-training methods has emerged as an effective approach for finetuning large language models and turn the weak language model into strong language model without preference data. However, the theoretical foundations for self-play finetuning remain underexplored. In this work, we tackle this by connecting self-play finetuning with adversarial imitation learning by formulating finetuning procedure as a min-max game between the model and a regularized implicit reward player parameterized by the model itself. This perspective unifies self-play imitation and general preference alignment within a common framework. Under this formulation, we present a game-theoretic analysis showing that the self-play finetuning will converge to it's equilibrium. Guided by this theoretical formulation, we propose a new self-play imitation finetuning algorithm based on the $\u03c7^2$-divergence variational objective with bounded rewards and improved stability. Experiments on various of language model finetuning tasks demonstrate consistent improvements over existing self-play methods and validate our theoretical insights.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4e3a\u81ea\u6211\u535a\u5f08\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u5efa\u7acb\u4e86\u9996\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u5176\u5f62\u5f0f\u5316\u4e3a\u5bf9\u6297\u5f0f\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u6781\u5c0f\u6781\u5927\u535a\u5f08\uff0c\u8bc1\u660e\u4e86\u6536\u655b\u6027\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u03c7\u00b2\u6563\u5ea6\u7684\u7a33\u5b9a\u7b97\u6cd5\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u81ea\u6211\u535a\u5f08\u540e\u8bad\u7ec3\u65b9\u6cd5\u65e0\u9700\u504f\u597d\u6570\u636e\u5373\u53ef\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\uff0c\u4f46\u5176\u7406\u8bba\u57fa\u7840\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u9650\u5236\u4e86\u65b9\u6cd5\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u548c\u53ef\u9760\u5e94\u7528\u3002", "method": "\u5c06\u81ea\u6211\u535a\u5f08\u5fae\u8c03\u5efa\u6a21\u4e3a\u6a21\u578b\u4e0e\u6b63\u5219\u5316\u9690\u5f0f\u5956\u52b1\u73a9\u5bb6\uff08\u7531\u6a21\u578b\u81ea\u8eab\u53c2\u6570\u5316\uff09\u4e4b\u95f4\u7684\u6781\u5c0f\u6781\u5927\u535a\u5f08\uff0c\u5efa\u7acb\u5176\u4e0e\u5bf9\u6297\u5f0f\u6a21\u4eff\u5b66\u4e60\u7684\u8054\u7cfb\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u03c7\u00b2\u6563\u5ea6\u53d8\u5206\u76ee\u6807\u4e14\u5956\u52b1\u6709\u754c\u7684\u65b0\u7b97\u6cd5\u3002", "result": "\u535a\u5f08\u8bba\u5206\u6790\u8bc1\u660e\u81ea\u6211\u535a\u5f08\u5fae\u8c03\u4f1a\u6536\u655b\u81f3\u5747\u8861\u72b6\u6001\uff1b\u591a\u9879\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u5b9e\u9a8c\u663e\u793a\u65b0\u7b97\u6cd5\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u81ea\u6211\u535a\u5f08\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u7406\u8bba\u6d1e\u5bdf\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u81ea\u6211\u535a\u5f08\u5fae\u8c03\u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u7406\u8bba\u57fa\u7840\uff0c\u7edf\u4e00\u4e86\u81ea\u6211\u535a\u5f08\u6a21\u4eff\u4e0e\u504f\u597d\u5bf9\u9f50\uff0c\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u6709\u6548\u7684\u7b97\u6cd5\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u7406\u8bba\u8ba4\u77e5\u548c\u5b9e\u8df5\u8fdb\u5c55\u3002"}}
{"id": "2602.01365", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01365", "abs": "https://arxiv.org/abs/2602.01365", "authors": ["Wang Yang", "Shouren Wang", "Chaoda Song", "Chuang Ma", "Xinpeng Li", "Nengbo Wang", "Kaixiong Zhou", "Vipin Chaudhary", "Xiaotian Han"], "title": "When Domains Interact: Asymmetric and Order-Sensitive Cross-Domain Effects in Reinforcement Learning for Reasoning", "comment": null, "summary": "Group Relative Policy Optimization (GRPO) has become a key technique for improving reasoning abilities in large language models, yet its behavior under different domain sequencing strategies is poorly understood. In particular, the impact of sequential (one domain at a time) versus mixed-domain (multiple domain at a time) training in GRPO has not been systematically studied. We provide the first systematic analysis of training-order effects across math, science, logic, and puzzle reasoning tasks. We found (1) single-domain generalization is highly asymmetric: training on other domains improves math reasoning by approximately 25\\% accuracy, while yielding negligible transfer to logic and puzzle; (2) cross-domain interactions are highly order-dependent: training in the order math$\\rightarrow$science achieves 83\\% / 41\\% accuracy on math / science, while reversing the order to science$\\rightarrow$math degrades performance to 77\\% / 25\\%; (3) no single strategy is universally optimal in multi-domain training: sequential training favors math (up to 84\\%), mixed training favors science and logic, and poor ordering can incur large performance gaps (from 70\\% to 56\\%). Overall, our findings demonstrate that GRPO under multi-domain settings exhibits pronounced asymmetry, order sensitivity, and strategy dependence, highlighting the necessity of domain-aware and order-aware training design.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u8bad\u7ec3\u987a\u5e8f\uff08\u987a\u5e8f\u8bad\u7ec3vs\u6df7\u5408\u9886\u57df\u8bad\u7ec3\uff09\u5bf9GRPO\u5728\u6570\u5b66\u3001\u79d1\u5b66\u3001\u903b\u8f91\u548c\u8c1c\u9898\u63a8\u7406\u4efb\u52a1\u4e0a\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u4e86\u663e\u8457\u7684\u9886\u57df\u975e\u5bf9\u79f0\u6027\u548c\u987a\u5e8f\u654f\u611f\u6027\u3002", "motivation": "GRPO\u5728\u4e0d\u540c\u9886\u57df\u6392\u5e8f\u7b56\u7565\u4e0b\u7684\u884c\u4e3a\u7279\u5f81\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7406\u89e3\uff0c\u7279\u522b\u662f\u987a\u5e8f\u8bad\u7ec3\u4e0e\u6df7\u5408\u9886\u57df\u8bad\u7ec3\u7684\u5f71\u54cd\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u7cfb\u7edf\u5206\u6790GRPO\u5728\u6570\u5b66\u3001\u79d1\u5b66\u3001\u903b\u8f91\u548c\u8c1c\u9898\u56db\u4e2a\u63a8\u7406\u9886\u57df\u7684\u8bad\u7ec3\u987a\u5e8f\u6548\u5e94\uff0c\u6bd4\u8f83\u4e86\u987a\u5e8f\u8bad\u7ec3\uff08\u4e00\u6b21\u4e00\u4e2a\u9886\u57df\uff09\u548c\u6df7\u5408\u9886\u57df\u8bad\u7ec3\uff08\u540c\u65f6\u591a\u4e2a\u9886\u57df\uff09\u4e24\u79cd\u7b56\u7565\u3002", "result": "\u4e3b\u8981\u53d1\u73b0\u5305\u62ec\uff1a\uff081\uff09\u5355\u9886\u57df\u6cdb\u5316\u5177\u6709\u9ad8\u5ea6\u975e\u5bf9\u79f0\u6027\uff1a\u5728\u5176\u4ed6\u9886\u57df\u8bad\u7ec3\u53ef\u4f7f\u6570\u5b66\u63a8\u7406\u51c6\u786e\u7387\u63d0\u5347\u7ea625%\uff0c\u4f46\u5bf9\u903b\u8f91\u548c\u8c1c\u9898\u7684\u8fc1\u79fb\u6548\u679c\u53ef\u5ffd\u7565\u4e0d\u8ba1\uff1b\uff082\uff09\u8de8\u9886\u57df\u4ea4\u4e92\u5177\u6709\u5f3a\u987a\u5e8f\u4f9d\u8d56\u6027\uff1a\u6570\u5b66\u2192\u79d1\u5b66\u987a\u5e8f\u7684\u51c6\u786e\u7387\u4e3a83%/41%\uff0c\u800c\u79d1\u5b66\u2192\u6570\u5b66\u987a\u5e8f\u5219\u964d\u81f377%/25%\uff1b\uff083\uff09\u591a\u9886\u57df\u8bad\u7ec3\u4e2d\u4e0d\u5b58\u5728\u901a\u7528\u6700\u4f18\u7b56\u7565\uff1a\u987a\u5e8f\u8bad\u7ec3\u66f4\u6709\u5229\u4e8e\u6570\u5b66\uff08\u6700\u9ad884%\uff09\uff0c\u6df7\u5408\u8bad\u7ec3\u66f4\u6709\u5229\u4e8e\u79d1\u5b66\u548c\u903b\u8f91\uff0c\u800c\u7cdf\u7cd5\u7684\u6392\u5e8f\u4f1a\u5bfc\u81f4\u6027\u80fd\u5927\u5e45\u4e0b\u964d\uff08\u4ece70%\u964d\u81f356%\uff09\u3002", "conclusion": "GRPO\u5728\u591a\u9886\u57df\u8bbe\u7f6e\u4e0b\u8868\u73b0\u51fa\u663e\u8457\u7684\u975e\u5bf9\u79f0\u6027\u3001\u987a\u5e8f\u654f\u611f\u6027\u548c\u7b56\u7565\u4f9d\u8d56\u6027\uff0c\u51f8\u663e\u4e86\u9886\u57df\u611f\u77e5\u548c\u987a\u5e8f\u611f\u77e5\u7684\u8bad\u7ec3\u8bbe\u8ba1\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2602.01367", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01367", "abs": "https://arxiv.org/abs/2602.01367", "authors": ["Pinar Erbil", "Alberto Archetti", "Eugenio Lomurno", "Matteo Matteucci"], "title": "Deep Variational Contrastive Learning for Joint Risk Stratification and Time-to-Event Estimation", "comment": null, "summary": "Survival analysis is essential for clinical decision-making, as it allows practitioners to estimate time-to-event outcomes, stratify patient risk profiles, and guide treatment planning. Deep learning has revolutionized this field with unprecedented predictive capabilities but faces a fundamental trade-off between performance and interpretability. While neural networks achieve high accuracy, their black-box nature limits clinical adoption. Conversely, deep clustering-based methods that stratify patients into interpretable risk groups typically sacrifice predictive power. We propose CONVERSE (CONtrastive Variational Ensemble for Risk Stratification and Estimation), a deep survival model that bridges this gap by unifying variational autoencoders with contrastive learning for interpretable risk stratification. CONVERSE combines variational embeddings with multiple intra- and inter-cluster contrastive losses. Self-paced learning progressively incorporates samples from easy to hard, improving training stability. The model supports cluster-specific survival heads, enabling accurate ensemble predictions. Comprehensive evaluation on four benchmark datasets demonstrates that CONVERSE achieves competitive or superior performance compared to existing deep survival methods, while maintaining meaningful patient stratification.", "AI": {"tldr": "CONVERSE is a deep survival model that unifies variational autoencoders with contrastive learning to achieve both high predictive accuracy and interpretable patient risk stratification, addressing the clinical adoption barrier of black-box models.", "motivation": "Deep survival models face a fundamental trade-off: neural networks offer high accuracy but lack interpretability needed for clinical adoption, while clustering-based interpretable methods sacrifice predictive power.", "method": "CONVERSE combines variational embeddings with multiple intra- and inter-cluster contrastive losses, uses self-paced learning for training stability, and employs cluster-specific survival heads for ensemble predictions.", "result": "On four benchmark datasets, CONVERSE achieves competitive or superior performance compared to existing deep survival methods while maintaining meaningful patient stratification.", "conclusion": "The model successfully bridges the performance-interpretability gap in deep survival analysis, enabling accurate predictions with clinically actionable risk grouping."}}
{"id": "2602.01410", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2602.01410", "abs": "https://arxiv.org/abs/2602.01410", "authors": ["Yunjie Pan", "Yongyi Yang", "Hanmei Yang", "Scott Mahlke"], "title": "SNIP: An Adaptive Mixed Precision Framework for Subbyte Large Language Model Training", "comment": "Accepted to ASPLOS 2026", "summary": "Training large language models (LLMs) efficiently while preserving model quality poses significant challenges, particularly with subbyte precision supported by state-of-the-art GPUs. Current mixed-precision training approaches either apply uniform precision to all GEMM operations or rely on heuristic-based methods that fail to generalize during training, leading to suboptimal convergence and instability. To address these challenges, this paper introduces SNIP, a fine-grained adaptive mixed-precision training framework for LLM pretraining that supports subbyte precision. SNIP periodically collects statistics on activations, gradients, and optimizer states to assess the precision loss impact on model quality. We define two key metrics: loss divergence in the forward pass, caused by quantization-induced increases in training loss, and weight divergence in the backward pass, which measures error propagation through gradients affecting model updates. These metrics guide an Integer Linear Programming (ILP) problem that systematically optimizes layerwise precision to minimize overall quality loss while meeting efficiency targets. Experiments on 1B, 3B, 7B and 70B Llama-like models demonstrate that SNIP consistently outperforms existing baselines, reducing FLOPs by up to 80% while preserving model quality across different model sizes and training phases with minimal computational overhead.", "AI": {"tldr": "SNIP\u662f\u4e00\u79cd\u7ec6\u7c92\u5ea6\u81ea\u9002\u5e94\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u635f\u5931\u53d1\u6563\u548c\u6743\u91cd\u53d1\u6563\u7684ILP\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\u7684\u540c\u65f6\u5c06LLM\u9884\u8bad\u7ec3FLOPs\u964d\u4f4e\u9ad8\u8fbe80%\u3002", "motivation": "\u5f53\u524dLLM\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u65b9\u6cd5\u8981\u4e48\u91c7\u7528\u7edf\u4e00\u7cbe\u5ea6\uff0c\u8981\u4e48\u4f9d\u8d56\u4e0d\u53ef\u6cdb\u5316\u7684\u542f\u53d1\u5f0f\u7b56\u7565\uff0c\u5bfc\u81f4\u6b21\u4f18\u6536\u655b\u548c\u4e0d\u7a33\u5b9a\u6027\u3002\u73b0\u4ee3GPU\u652f\u6301\u4e9a\u5b57\u8282\u7cbe\u5ea6\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSNIP\u6846\u67b6\uff1a\u5b9a\u671f\u6536\u96c6\u6fc0\u6d3b\u503c\u3001\u68af\u5ea6\u548c\u4f18\u5316\u5668\u72b6\u6001\u7edf\u8ba1\u4fe1\u606f\uff0c\u5b9a\u4e49\u524d\u5411\u4f20\u64ad\u635f\u5931\u53d1\u6563\u548c\u540e\u5411\u4f20\u64ad\u6743\u91cd\u53d1\u6563\u4e24\u4e2a\u5173\u952e\u6307\u6807\uff0c\u6784\u5efa\u6574\u6570\u7ebf\u6027\u89c4\u5212\u95ee\u9898\u6765\u7cfb\u7edf\u4f18\u5316\u9010\u5c42\u7cbe\u5ea6\uff0c\u5728\u6ee1\u8db3\u6548\u7387\u76ee\u6807\u7684\u540c\u65f6\u6700\u5c0f\u5316\u6574\u4f53\u8d28\u91cf\u635f\u5931\u3002", "result": "\u57281B\u81f370B\u53c2\u6570\u7684Llama\u7c7b\u6a21\u578b\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cSNIP\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0cFLOPs\u51cf\u5c11\u9ad8\u8fbe80%\uff0c\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u548c\u8bad\u7ec3\u9636\u6bb5\u5747\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "SNIP\u9a8c\u8bc1\u4e86\u7ec6\u7c92\u5ea6ILP\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u662f\u9ad8\u6548LLM\u9884\u8bad\u7ec3\u7684\u6709\u6548\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u800c\u4e0d\u727a\u7272\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2602.01419", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01419", "abs": "https://arxiv.org/abs/2602.01419", "authors": ["Dennis Gross", "Helge Spieker", "Arnaud Gotlieb", "Emmanuel Stathatos", "Panorios Benardos", "George-Christopher Vosniakos"], "title": "Semi-supervised CAPP Transformer Learning via Pseudo-labeling", "comment": null, "summary": "High-level Computer-Aided Process Planning (CAPP) generates manufacturing process plans from part specifications. It suffers from limited dataset availability in industry, reducing model generalization. We propose a semi-supervised learning approach to improve transformer-based CAPP transformer models without manual labeling. An oracle, trained on available transformer behaviour data, filters correct predictions from unseen parts, which are then used for one-shot retraining. Experiments on small-scale datasets with simulated ground truth across the full data distribution show consistent accuracy gains over baselines, demonstrating the method's effectiveness in data-scarce manufacturing environments.", "AI": {"tldr": "This paper proposes a semi-supervised learning method for transformer-based CAPP that uses an oracle to filter correct predictions from unlabeled data for one-shot retraining, achieving consistent accuracy improvements in data-scarce manufacturing environments.", "motivation": "Limited dataset availability in industrial CAPP applications reduces model generalization, and manual labeling is expensive and time-consuming.", "method": "A semi-supervised approach where an oracle trained on existing transformer behavior data filters correct predictions from unseen parts, which are then used for one-shot retraining of the model.", "result": "Experiments on small-scale datasets with simulated ground truth showed consistent accuracy gains over baseline methods across the full data distribution.", "conclusion": "The proposed method effectively addresses data scarcity in manufacturing by leveraging unlabeled data without manual annotation, improving CAPP model performance."}}
{"id": "2602.01428", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.01428", "abs": "https://arxiv.org/abs/2602.01428", "authors": ["Weiqing He", "Xiang Li", "Li Shen", "Weijie Su", "Qi Long"], "title": "Improve the Trade-off Between Watermark Strength and Speculative Sampling Efficiency for Language Models", "comment": "Accepted at ICLR 2026", "summary": "Watermarking is a principled approach for tracing the provenance of large language model (LLM) outputs, but its deployment in practice is hindered by inference inefficiency. Speculative sampling accelerates inference, with efficiency improving as the acceptance rate between draft and target models increases. Yet recent work reveals a fundamental trade-off: higher watermark strength reduces acceptance, preventing their simultaneous achievement. We revisit this trade-off and show it is not absolute. We introduce a quantitative measure of watermark strength that governs statistical detectability and is maximized when tokens are deterministic functions of pseudorandom numbers. Using this measure, we fully characterize the trade-off as a constrained optimization problem and derive explicit Pareto curves for two existing watermarking schemes. Finally, we introduce a principled mechanism that injects pseudorandomness into draft-token acceptance, ensuring maximal watermark strength while maintaining speculative sampling efficiency. Experiments further show that this approach improves detectability without sacrificing efficiency. Our findings uncover a principle that unites speculative sampling and watermarking, paving the way for their efficient and practical deployment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u6c34\u5370\uff08\u7528\u4e8e\u6eaf\u6e90\uff09\u4e0e\u63a8\u6d4b\u91c7\u6837\uff08\u7528\u4e8e\u52a0\u901f\uff09\u4e4b\u95f4\u7684\u6548\u7387\u51b2\u7a81\uff0c\u8bc1\u660e\u4e24\u8005\u5e76\u975e\u7edd\u5bf9\u4e92\u65a5\u3002\u901a\u8fc7\u91cf\u5316\u6c34\u5370\u5f3a\u5ea6\u3001\u5efa\u7acb\u4f18\u5316\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u5411\u63a8\u6d4b\u91c7\u6837\u4e2d\u6ce8\u5165\u4f2a\u968f\u673a\u6027\u7684\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5728\u4e0d\u727a\u7272\u6548\u7387\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u6c34\u5370\u53ef\u68c0\u6d4b\u6027\u3002", "motivation": "\u6c34\u5370\u6280\u672f\u5bf9\u8ffd\u6eafLLM\u8f93\u51fa\u6765\u6e90\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b58\u5728\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002\u63a8\u6d4b\u91c7\u6837\u867d\u80fd\u52a0\u901f\u63a8\u7406\uff0c\u4f46\u8fd1\u671f\u7814\u7a76\u53d1\u73b0\u6839\u672c\u6027\u6743\u8861\uff1a\u6c34\u5370\u5f3a\u5ea6\u8d8a\u9ad8\uff0c\u63a8\u6d4b\u91c7\u6837\u7684\u63a5\u53d7\u7387\u8d8a\u4f4e\uff0c\u5bfc\u81f4\u4e24\u8005\u65e0\u6cd5\u540c\u65f6\u5b9e\u73b0\uff0c\u963b\u788d\u4e86\u5b9e\u9645\u5e94\u7528\u90e8\u7f72\u3002", "method": "1. \u63d0\u51fa\u57fa\u4e8e\u7edf\u8ba1\u53ef\u68c0\u6d4b\u6027\u7684\u6c34\u5370\u5f3a\u5ea6\u91cf\u5316\u5ea6\u91cf\uff1b2. \u5c06\u8be5\u6743\u8861\u5efa\u6a21\u4e3a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff1b3. \u63a8\u5bfc\u4e24\u79cd\u73b0\u6709\u6c34\u5370\u65b9\u6848\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u66f2\u7ebf\uff1b4. \u8bbe\u8ba1\u5411\u8349\u7a3ftoken\u63a5\u53d7\u673a\u5236\u6ce8\u5165\u4f2a\u968f\u673a\u6027\u7684\u539f\u7406\u6027\u65b9\u6cd5\u3002", "result": "\u8bc1\u660e\u8be5\u6743\u8861\u5e76\u975e\u7edd\u5bf9\uff0c\u5b9e\u9a8c\u663e\u793a\u6240\u63d0\u65b9\u6cd5\u80fd\u5728\u4e0d\u635f\u5931\u6548\u7387\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u6c34\u5370\u53ef\u68c0\u6d4b\u6027\uff0c\u5e76\u4e3a\u73b0\u6709\u65b9\u6848\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u4f18\u5316\u8fb9\u754c\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u7edf\u4e00\u63a8\u6d4b\u91c7\u6837\u4e0e\u6c34\u5370\u6280\u672f\u7684\u539f\u7406\uff0c\u4e3a\u5b9e\u73b0\u4e24\u8005\u9ad8\u6548\u5b9e\u7528\u7684\u90e8\u7f72\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2602.01433", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01433", "abs": "https://arxiv.org/abs/2602.01433", "authors": ["Muhammad Hasan Ferdous", "Md Osman Gani"], "title": "DCD: Decomposition-based Causal Discovery from Autocorrelated and Non-Stationary Temporal Data", "comment": null, "summary": "Multivariate time series in domains such as finance, climate science, and healthcare often exhibit long-term trends, seasonal patterns, and short-term fluctuations, complicating causal inference under non-stationarity and autocorrelation. Existing causal discovery methods typically operate on raw observations, making them vulnerable to spurious edges and misattributed temporal dependencies. We introduce a decomposition-based causal discovery framework that separates each time series into trend, seasonal, and residual components and performs component-specific causal analysis. Trend components are assessed using stationarity tests, seasonal components using kernel-based dependence measures, and residual components using constraint-based causal discovery. The resulting component-level graphs are integrated into a unified multi-scale causal structure. This approach isolates long- and short-range causal effects, reduces spurious associations, and improves interpretability. Across extensive synthetic benchmarks and real-world climate data, our framework more accurately recovers ground-truth causal structure than state-of-the-art baselines, particularly under strong non-stationarity and temporal autocorrelation.", "AI": {"tldr": "A decomposition-based causal discovery framework that separates time series into trend, seasonal, and residual components for specialized analysis, then integrates results into a unified multi-scale causal graph to improve accuracy under non-stationarity and autocorrelation.", "motivation": "Existing causal discovery methods fail to handle non-stationarity and autocorrelation in multivariate time series (e.g., finance/climate data), leading to spurious causal edges and misinterpreted temporal dependencies.", "method": "Decomposes series into trend (analyzed via stationarity tests), seasonal (kernel-based dependence measures), and residual components (constraint-based causal discovery); integrates component-level graphs into a unified structure.", "result": "Outperforms state-of-the-art baselines in recovering true causal structures across synthetic benchmarks and real-world climate data, especially under strong non-stationarity and autocorrelation.", "conclusion": "Isolating long/short-range effects via component-specific analysis reduces spurious associations and enhances interpretability of multi-scale causal relationships in non-stationary time series."}}
{"id": "2602.01439", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01439", "abs": "https://arxiv.org/abs/2602.01439", "authors": ["Perry Dong", "Kuo-Han Hung", "Alexander Swerdlow", "Dorsa Sadigh", "Chelsea Finn"], "title": "TQL: Scaling Q-Functions with Transformers by Preventing Attention Collapse", "comment": null, "summary": "Despite scale driving substantial recent advancements in machine learning, reinforcement learning (RL) methods still primarily use small value functions. Naively scaling value functions -- including with a transformer architecture, which is known to be highly scalable -- often results in learning instability and worse performance. In this work, we ask what prevents transformers from scaling effectively for value functions? Through empirical analysis, we identify the critical failure mode in this scaling: attention scores collapse as capacity increases. Our key insight is that we can effectively prevent this collapse and stabilize training by controlling the entropy of the attention scores, thereby enabling the use of larger models. To this end, we propose Transformer Q-Learning (TQL), a method that unlocks the scaling potential of transformers in learning value functions in RL. Our approach yields up to a 43% improvement in performance when scaling from the smallest to the largest network sizes, while prior methods suffer from performance degradation.", "AI": {"tldr": "\u63d0\u51faTransformer Q-Learning (TQL)\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a7\u5236\u6ce8\u610f\u529b\u5206\u6570\u71b5\u503c\u89e3\u51b3\u4ef7\u503c\u51fd\u6570\u7f29\u653e\u65f6\u7684\u6ce8\u610f\u529b\u5d29\u6e83\u95ee\u9898\uff0c\u5b9e\u73b043%\u6027\u80fd\u63d0\u5347", "motivation": "\u5f3a\u5316\u5b66\u4e60(RL)\u4e2d\u4ef7\u503c\u51fd\u6570\u957f\u671f\u672a\u5b9e\u73b0\u6709\u6548\u7f29\u653e\uff0c\u76f4\u63a5\u5e94\u7528Transformer\u67b6\u6784\u4f1a\u5bfc\u81f4\u5b66\u4e60\u4e0d\u7a33\u5b9a\u548c\u6027\u80fd\u4e0b\u964d\uff0c\u963b\u788d\u5927\u89c4\u6a21\u6a21\u578b\u5728RL\u4e2d\u7684\u5e94\u7528", "method": "\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u53d1\u73b0\u6ce8\u610f\u529b\u5206\u6570\u968f\u5bb9\u91cf\u589e\u52a0\u800c\u5d29\u6e83\u7684\u5173\u952e\u5931\u6548\u6a21\u5f0f\uff0c\u63d0\u51fa\u63a7\u5236\u6ce8\u610f\u529b\u5206\u6570\u71b5\u503c\u6765\u7a33\u5b9a\u8bad\u7ec3\uff0c\u5f00\u53d1TQL\u65b9\u6cd5\u89e3\u9501Transformer\u5728\u4ef7\u503c\u51fd\u6570\u5b66\u4e60\u4e2d\u7684\u7f29\u653e\u6f5c\u529b", "result": "TQL\u65b9\u6cd5\u5728\u4ece\u5c0f\u6a21\u578b\u6269\u5c55\u5230\u6700\u5927\u6a21\u578b\u65f6\u83b7\u5f97\u9ad8\u8fbe43%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u800c\u4f20\u7edf\u65b9\u6cd5\u5728\u6b64\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u6027\u80fd\u9000\u5316\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4ef7\u503c\u51fd\u6570\u7f29\u653e\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898", "conclusion": "\u901a\u8fc7\u71b5\u503c\u63a7\u5236\u7a33\u5b9a\u6ce8\u610f\u529b\u673a\u5236\uff0c\u9996\u6b21\u5b9e\u73b0Transformer\u67b6\u6784\u5728RL\u4ef7\u503c\u51fd\u6570\u4e2d\u7684\u6709\u6548\u7f29\u653e\uff0c\u4e3a\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u65b0\u8def\u5f84"}}
{"id": "2602.01445", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01445", "abs": "https://arxiv.org/abs/2602.01445", "authors": ["Ons Saadallah", "M\u00e1ty\u00e1s and\u00f3", "Tam\u00e1s G\u00e1bor Orosz"], "title": "A Meta-Knowledge-Augmented LLM Framework for Hyperparameter Optimization in Time-Series Forecasting", "comment": null, "summary": "Hyperparameter optimization (HPO) plays a central role in the performance of deep learning models, yet remains computationally expensive and difficult to interpret, particularly for time-series forecasting. While Bayesian Optimization (BO) is a standard approach, it typically treats tuning tasks independently and provides limited insight into its decisions. Recent advances in large language models (LLMs) offer new opportunities to incorporate structured prior knowledge and reasoning into optimization pipelines. We introduce LLM-AutoOpt, a hybrid HPO framework that combines BO with LLM-based contextual reasoning. The framework encodes dataset meta-features, model descriptions, historical optimization outcomes, and target objectives as structured meta-knowledge within LLM prompts, using BO to initialize the search and mitigate cold-start effects. This design enables context-aware and stable hyperparameter refinement while exposing the reasoning behind optimization decisions. Experiments on a multivariate time series forecasting benchmark demonstrate that LLM-AutoOpt achieves improved predictive performance and more interpretable optimization behavior compared to BO and LLM baselines without meta-knowledge.", "AI": {"tldr": "\u63d0\u51faLLM-AutoOpt\u6df7\u5408\u8d85\u53c2\u6570\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u8d1d\u53f6\u65af\u4f18\u5316\u4e0e\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5143\u77e5\u8bc6\u7f16\u7801\u5b9e\u73b0\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u4e2d\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\u7684\u53cc\u91cd\u63d0\u5347", "motivation": "\u8d85\u53c2\u6570\u4f18\u5316\u5bf9\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u89e3\u91ca\uff0c\u7279\u522b\u662f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u9886\u57df\u3002\u4f20\u7edf\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\u72ec\u7acb\u5904\u7406\u8c03\u4f18\u4efb\u52a1\u4e14\u51b3\u7b56\u8fc7\u7a0b\u4e0d\u900f\u660e\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u4e3a\u5f15\u5165\u7ed3\u6784\u5316\u5148\u9a8c\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u673a\u9047", "method": "\u8bbe\u8ba1LLM-AutoOpt\u6df7\u5408\u6846\u67b6\uff0c\u5c06\u6570\u636e\u96c6\u5143\u7279\u5f81\u3001\u6a21\u578b\u63cf\u8ff0\u3001\u5386\u53f2\u4f18\u5316\u7ed3\u679c\u548c\u76ee\u6807\u51fd\u6570\u7f16\u7801\u4e3a\u7ed3\u6784\u5316\u5143\u77e5\u8bc6\u8f93\u5165\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u540c\u65f6\u5229\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u521d\u59cb\u5316\u641c\u7d22\u4ee5\u7f13\u89e3\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u8d85\u53c2\u6570\u4f18\u5316", "result": "\u5728\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u7eaf\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u65e0\u5143\u77e5\u8bc6\u7684\u57fa\u7ebf\u5927\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\uff0cLLM-AutoOpt\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u5e76\u4f7f\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\u66f4\u5177\u53ef\u89e3\u91ca\u6027", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u878d\u5408\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u7a33\u5b9a\u6027\u548c\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u5143\u77e5\u8bc6\u6ce8\u5165\u5b9e\u73b0\u4e86\u7a33\u5b9a\u4e14\u900f\u660e\u7684\u8d85\u53c2\u6570\u4f18\u5316\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8c03\u4f18\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u6027\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.01453", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01453", "abs": "https://arxiv.org/abs/2602.01453", "authors": ["Idan Barnea", "Orin Levy", "Yishay Mansour"], "title": "Provable Cooperative Multi-Agent Exploration for Reward-Free MDPs", "comment": null, "summary": "We study cooperative multi-agent reinforcement learning in the setting of reward-free exploration, where multiple agents jointly explore an unknown MDP in order to learn its dynamics (without observing rewards). We focus on a tabular finite-horizon MDP and adopt a phased learning framework. In each learning phase, multiple agents independently interact with the environment. More specifically, in each learning phase, each agent is assigned a policy, executes it, and observes the resulting trajectory. Our primary goal is to characterize the tradeoff between the number of learning phases and the number of agents, especially when the number of learning phases is small.\n  Our results identify a sharp transition governed by the horizon $H$. When the number of learning phases equals $H$, we present a computationally efficient algorithm that uses only $\\tilde{O}(S^6 H^6 A / \u03b5^2)$ agents to obtain an $\u03b5$ approximation of the dynamics (i.e., yields an $\u03b5$-optimal policy for any reward function). We complement our algorithm with a lower bound showing that any algorithm restricted to $\u03c1< H$ phases requires at least $A^{H/\u03c1}$ agents to achieve constant accuracy. Thus, we show that it is essential to have an order of $H$ learning phases if we limit the number of agents to be polynomial.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u65e0\u5956\u52b1\u63a2\u7d22\u573a\u666f\u4e0b\u5b58\u5728\u5173\u952e\u76f8\u53d8\uff1a\u5f53\u5b66\u4e60\u9636\u6bb5\u6570\u03c1\u2265H\uff08\u51b3\u7b56\u89c6\u754c\uff09\u65f6\uff0c\u4ec5\u9700\u591a\u9879\u5f0f\u6570\u91cf\u667a\u80fd\u4f53\u5373\u53ef\u9ad8\u6548\u5b66\u4e60MDP\u52a8\u6001\uff1b\u82e5\u03c1<H\uff0c\u5219\u9700\u6307\u6570\u7ea7\u667a\u80fd\u4f53\u624d\u80fd\u8fbe\u5230\u5e38\u6570\u7cbe\u5ea6\u3002", "motivation": "\u5728\u672a\u77e5\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b(MDP)\u4e2d\uff0c\u591a\u4e2a\u667a\u80fd\u4f53\u9700\u5728\u65e0\u5956\u52b1\u4fe1\u53f7\u6761\u4ef6\u4e0b\u534f\u540c\u63a2\u7d22\u73af\u5883\u52a8\u6001\uff0c\u6838\u5fc3\u6311\u6218\u662f\u5e73\u8861\u5b66\u4e60\u9636\u6bb5\u6570\u4e0e\u667a\u80fd\u4f53\u6570\u91cf\u7684\u5173\u7cfb\uff0c\u5c24\u5176\u5728\u9636\u6bb5\u6570\u53d7\u9650\u65f6\u7684\u6700\u5c0f\u667a\u80fd\u4f53\u9700\u6c42\u3002", "method": "\u91c7\u7528\u5206\u9636\u6bb5\u5b66\u4e60\u6846\u67b6\uff1a\u6bcf\u9636\u6bb5\u5404\u667a\u80fd\u4f53\u72ec\u7acb\u6267\u884c\u5206\u914d\u7b56\u7565\u5e76\u6536\u96c6\u8f68\u8ff9\u6570\u636e\uff1b\u9488\u5bf9\u6709\u9650\u89c6\u754cH\u7684\u8868\u683c\u578bMDP\uff0c\u8bbe\u8ba1\u8ba1\u7b97\u9ad8\u6548\u7b97\u6cd5\u5e76\u5206\u6790\u03c1\uff08\u9636\u6bb5\u6570\uff09\u4e0e\u667a\u80fd\u4f53\u6570\u91cf\u7684\u6743\u8861\u3002", "result": "\u53d1\u73b0\u7531\u89c6\u754cH\u51b3\u5b9a\u7684\u5c16\u9510\u76f8\u53d8\uff1a\u5f53\u03c1=H\u65f6\uff0c\u7b97\u6cd5\u4ec5\u9700\u00d5(S\u2076H\u2076A/\u03b5\u00b2)\u4e2a\u667a\u80fd\u4f53\u5373\u53ef\u03b5-\u8fd1\u4f3c\u52a8\u6001\uff1b\u5f53\u03c1<H\u65f6\uff0c\u4efb\u4f55\u7b97\u6cd5\u81f3\u5c11\u9700\u8981A^{H/\u03c1}\u4e2a\u667a\u80fd\u4f53\u624d\u80fd\u8fbe\u5230\u5e38\u6570\u7cbe\u5ea6\u3002", "conclusion": "\u5b66\u4e60\u9636\u6bb5\u6570\u5fc5\u987b\u8fbe\u5230\u51b3\u7b56\u89c6\u754cH\u91cf\u7ea7\u624d\u80fd\u4fdd\u8bc1\u667a\u80fd\u4f53\u6570\u91cf\u591a\u9879\u5f0f\u589e\u957f\uff0c\u5426\u5219\u9700\u6307\u6570\u7ea7\u8d44\u6e90\uff0c\u8fd9\u4e3a\u591a\u667a\u80fd\u4f53\u63a2\u7d22\u6548\u7387\u63d0\u4f9b\u4e86\u6839\u672c\u6027\u7406\u8bba\u8fb9\u754c\u3002"}}
{"id": "2602.01456", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01456", "abs": "https://arxiv.org/abs/2602.01456", "authors": ["Yilun Kuang", "Yash Dagade", "Tim G. J. Rudner", "Randall Balestriero", "Yann LeCun"], "title": "Rectified LpJEPA: Joint-Embedding Predictive Architectures with Sparse and Maximum-Entropy Representations", "comment": null, "summary": "Joint-Embedding Predictive Architectures (JEPA) learn view-invariant representations and admit projection-based distribution matching for collapse prevention. Existing approaches regularize representations towards isotropic Gaussian distributions, but inherently favor dense representations and fail to capture the key property of sparsity observed in efficient representations. We introduce Rectified Distribution Matching Regularization (RDMReg), a sliced two-sample distribution-matching loss that aligns representations to a Rectified Generalized Gaussian (RGG) distribution. RGG enables explicit control over expected $\\ell_0$ norm through rectification, while preserving maximum-entropy up to rescaling under expected $\\ell_p$ norm constraints. Equipping JEPAs with RDMReg yields Rectified LpJEPA, which strictly generalizes prior Gaussian-based JEPAs. Empirically, Rectified LpJEPA learns sparse, non-negative representations with favorable sparsity-performance trade-offs and competitive downstream performance on image classification benchmarks, demonstrating that RDMReg effectively enforces sparsity while preserving task-relevant information.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRDMReg\u7684\u65b0\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u67b6\u6784\uff08JEPA\uff09\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u4fee\u6b63\u5e7f\u4e49\u9ad8\u65af\u5206\u5e03\u800c\u975e\u9ad8\u65af\u5206\u5e03\u6765\u5f3a\u5236\u5b66\u4e60\u8868\u793a\u7684\u7a00\u758f\u6027\uff0c\u4ece\u800c\u5f97\u5230\u5177\u6709\u66f4\u597d\u7a00\u758f-\u6027\u80fd\u6743\u8861\u7684Rectified LpJEPA\u3002", "motivation": "\u73b0\u6709\u7684JEPA\u65b9\u6cd5\u5c06\u8868\u793a\u6b63\u5219\u5316\u4e3a\u5404\u5411\u540c\u6027\u9ad8\u65af\u5206\u5e03\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u5bc6\u96c6\u8868\u793a\uff0c\u65e0\u6cd5\u6355\u6349\u9ad8\u6548\u8868\u793a\u4e2d\u89c2\u5bdf\u5230\u7684\u7a00\u758f\u6027\u7279\u6027\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u4fdd\u7559\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u7684\u540c\u65f6\u5f3a\u5236\u7a00\u758f\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u8bba\u6587\u5f15\u5165\u4e86\u4fee\u6b63\u5206\u5e03\u5339\u914d\u6b63\u5219\u5316\uff08RDMReg\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u5207\u7247\u7684\u53cc\u6837\u672c\u5206\u5e03\u5339\u914d\u635f\u5931\uff0c\u5c06\u8868\u793a\u4e0e\u4fee\u6b63\u5e7f\u4e49\u9ad8\u65af\uff08RGG\uff09\u5206\u5e03\u5bf9\u9f50\u3002\u8fd9\u901a\u8fc7\u4fee\u6b63\u673a\u5236\u5b9e\u73b0\u4e86\u5bf9\u671f\u671b\u21130\u8303\u6570\u7684\u663e\u5f0f\u63a7\u5236\uff0c\u540c\u65f6\u5728\u671f\u671b\u2113p\u8303\u6570\u7ea6\u675f\u4e0b\u4fdd\u6301\u6700\u5927\u71b5\u7279\u6027\u3002\u5c06RDMReg\u5e94\u7528\u4e8eJEPA\u5f97\u5230Rectified LpJEPA\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRectified LpJEPA\u5b66\u4e60\u5230\u7a00\u758f\u4e14\u975e\u8d1f\u7684\u8868\u793a\uff0c\u5728\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u4e0a\u5177\u6709\u6709\u5229\u7684\u7a00\u758f-\u6027\u80fd\u6743\u8861\u548c\u5177\u6709\u7ade\u4e89\u529b\u7684\u4e0b\u6e38\u6027\u80fd\u3002", "conclusion": "RDMReg\u80fd\u6709\u6548\u5f3a\u5236\u7a00\u758f\u6027\u540c\u65f6\u4fdd\u7559\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\uff0c\u4e14Rectified LpJEPA\u4e25\u683c\u6cdb\u5316\u4e86\u5148\u524d\u57fa\u4e8e\u9ad8\u65af\u7684JEPA\uff0c\u5177\u6709\u66f4\u597d\u7684\u8868\u793a\u6548\u7387\u3002"}}
{"id": "2602.01468", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01468", "abs": "https://arxiv.org/abs/2602.01468", "authors": ["Viet Nguyen", "Tuan Minh Pham", "Thinh Cao", "Tan Dinh", "Huy Nguyen", "Nhat Ho", "Alessandro Rinaldo"], "title": "A Statistical Theory of Gated Attention through the Lens of Hierarchical Mixture of Experts", "comment": "Viet Nguyen, Tuan Minh Pham, and Thinh Cao contributed equally to this work", "summary": "Self-attention has greatly contributed to the success of the widely used Transformer architecture by enabling learning from data with long-range dependencies. In an effort to improve performance, a gated attention model that leverages a gating mechanism within the multi-head self-attention has recently been proposed as a promising alternative. Gated attention has been empirically demonstrated to increase the expressiveness of low-rank mapping in standard attention and even to eliminate the attention sink phenomenon. Despite its efficacy, a clear theoretical understanding of gated attention's benefits remains lacking in the literature. To close this gap, we rigorously show that each entry in a gated attention matrix or a multi-head self-attention matrix can be written as a hierarchical mixture of experts. By recasting learning as an expert estimation problem, we demonstrate that gated attention is more sample-efficient than multi-head self-attention. In particular, while the former needs only a polynomial number of data points to estimate an expert, the latter requires exponentially many data points to achieve the same estimation error. Furthermore, our analysis also provides a theoretical justification for why gated attention yields higher performance when a gate is placed at the output of the scaled dot product attention or the value map rather than at other positions in the multi-head self-attention architecture.", "AI": {"tldr": "This paper provides the first theoretical analysis of gated attention, proving it can be formulated as a hierarchical mixture of experts and demonstrating it is exponentially more sample-efficient than standard multi-head self-attention while justifying optimal gate placement.", "motivation": "Despite empirical success of gated attention in improving Transformer performance, a rigorous theoretical understanding of its benefits over standard multi-head self-attention was lacking. The paper aims to close this gap.", "method": "The authors show that gated attention matrices can be expressed as hierarchical mixtures of experts. They recast the learning problem as an expert estimation problem to analyze sample complexity.", "result": "Gated attention requires only polynomial data points to estimate experts, while standard multi-head self-attention needs exponentially many. The analysis also explains why placing gates at the output of scaled dot-product attention or value maps yields better performance.", "conclusion": "Gated attention offers provable theoretical advantages in sample efficiency over standard attention mechanisms, and the hierarchical mixture-of-experts perspective provides a principled understanding of its effectiveness and optimal architecture design."}}
{"id": "2602.01469", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01469", "abs": "https://arxiv.org/abs/2602.01469", "authors": ["Mude Hui", "Xin Huang", "Jaime Campos Salas", "Yue Sun", "Nathan Pemberton", "Xiang Song", "Ashish Khetan", "George Karypis"], "title": "P-EAGLE: Parallel-Drafting EAGLE with Scalable Training", "comment": null, "summary": "Reasoning LLMs produce longer outputs, requiring speculative decoding drafters trained on extended sequences. Parallel drafting - predicting multiple tokens per forward pass - offers latency benefits over sequential generation, but training complexity scales quadratically with the product of sequence length and parallel positions, rendering long-context training impractical. We present P(arallel)-EAGLE, which transforms EAGLE from autoregressive to parallel multi-token prediction via a learnable shared hidden state. To scale training to long contexts, we develop a framework featuring attention mask pre-computation and sequence partitioning techniques, enabling gradient accumulation within individual sequences for parallel-prediction training. We implement P-EAGLE in vLLM and demonstrate speedups of 1.10-1.36x over autoregressive EAGLE-3 across GPT-OSS 120B, 20B, and Qwen3-Coder 30B.", "AI": {"tldr": "\u63d0\u51faP-EAGLE\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u5171\u4eab\u9690\u85cf\u6001\u5c06EAGLE\u8f6c\u4e3a\u5e76\u884c\u591atoken\u9884\u6d4b\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u63a9\u7801\u9884\u8ba1\u7b97\u548c\u5e8f\u5217\u5206\u5272\u6280\u672f\u5b9e\u73b0\u957f\u4e0a\u4e0b\u6587\u9ad8\u6548\u8bad\u7ec3\uff0c\u5728vLLM\u4e2d\u5b9e\u73b01.10-1.36\u500d\u52a0\u901f\u3002", "motivation": "\u63a8\u7406\u578bLLM\u8f93\u51fa\u66f4\u957f\uff0c\u9700\u8981\u957f\u5e8f\u5217\u8bad\u7ec3\u7684\u63a8\u6d4b\u89e3\u7801\u5668\u3002\u5e76\u884c\u8349\u7a3f\u867d\u80fd\u964d\u4f4e\u5ef6\u8fdf\uff0c\u4f46\u8bad\u7ec3\u590d\u6742\u5ea6\u968f\u5e8f\u5217\u957f\u5ea6\u548c\u5e76\u884c\u4f4d\u7f6e\u4e58\u79ef\u4e8c\u6b21\u589e\u957f\uff0c\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u56f0\u96be\u3002", "method": "\u5c06EAGLE\u4ece\u81ea\u56de\u5f52\u6539\u4e3a\u5e76\u884c\u591atoken\u9884\u6d4b\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u5171\u4eab\u9690\u85cf\u6001\uff1b\u901a\u8fc7\u9884\u8ba1\u7b97\u6ce8\u610f\u529b\u63a9\u7801\u548c\u5e8f\u5217\u5206\u5272\u6280\u672f\uff0c\u5728\u5355\u5e8f\u5217\u5185\u5b9e\u73b0\u68af\u5ea6\u7d2f\u79ef\u4ee5\u652f\u6301\u5e76\u884c\u9884\u6d4b\u8bad\u7ec3\u3002", "result": "\u5728vLLM\u4e2d\u5b9e\u73b0\uff0c\u76f8\u6bd4\u81ea\u56de\u5f52EAGLE-3\u5728GPT-OSS 120B\u300120B\u548cQwen3-Coder 30B\u4e0a\u5b9e\u73b01.10-1.36\u500d\u52a0\u901f\u3002", "conclusion": "P-EAGLE\u6210\u529f\u5b9e\u73b0\u5e76\u884c\u63a8\u6d4b\u89e3\u7801\uff0c\u901a\u8fc7\u9ad8\u6548\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u6846\u67b6\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\uff0c\u4e3a\u957f\u5e8f\u5217\u751f\u6210\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01483", "categories": ["cs.LG", "cs.AI", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.01483", "abs": "https://arxiv.org/abs/2602.01483", "authors": ["Edwin V. Bonilla", "He Zhao", "Daniel M. Steinberg"], "title": "Causal Preference Elicitation", "comment": null, "summary": "We propose causal preference elicitation, a Bayesian framework for expert-in-the-loop causal discovery that actively queries local edge relations to concentrate a posterior over directed acyclic graphs (DAGs). From any black-box observational posterior, we model noisy expert judgments with a three-way likelihood over edge existence and direction. Posterior inference uses a flexible particle approximation, and queries are selected by an efficient expected information gain criterion on the expert's categorical response. Experiments on synthetic graphs, protein signaling data, and a human gene perturbation benchmark show faster posterior concentration and improved recovery of directed effects under tight query budgets.", "AI": {"tldr": "\u63d0\u51fa\u56e0\u679c\u504f\u597d\u5f15\u5bfc\u7684\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u901a\u8fc7\u4e3b\u52a8\u67e5\u8be2\u4e13\u5bb6\u5224\u65ad\u6765\u52a0\u901f\u56e0\u679c\u56fe\u540e\u9a8c\u96c6\u4e2d\uff0c\u5728\u6709\u9650\u67e5\u8be2\u9884\u7b97\u4e0b\u63d0\u5347\u56e0\u679c\u6548\u5e94\u6062\u590d\u6548\u679c\u3002", "motivation": "\u7eaf\u89c2\u6d4b\u6570\u636e\u56e0\u679c\u53d1\u73b0\u540e\u9a8c\u5206\u5e03\u8fc7\u4e8e\u5206\u6563\uff0c\u4e13\u5bb6\u77e5\u8bc6\u53ef\u8f85\u52a9\u4f46\u5b58\u5728\u566a\u58f0\u3002\u672c\u6587\u65e8\u5728\u6784\u5efa\u4e3b\u52a8\u67e5\u8be2\u673a\u5236\uff0c\u9ad8\u6548\u878d\u5408\u4e13\u5bb6\u5224\u65ad\u4ee5\u6d53\u7f29DAG\u540e\u9a8c\u7a7a\u95f4\u3002", "method": "\u57fa\u4e8e\u8d1d\u53f6\u65af\u6846\u67b6\uff1a1\uff09\u4ece\u4efb\u610f\u89c2\u6d4b\u540e\u9a8c\u51fa\u53d1\uff1b2\uff09\u7528\u4e09\u5411\u4f3c\u7136\u5efa\u6a21\u4e13\u5bb6\u5bf9\u8fb9\u5b58\u5728\u4e0e\u65b9\u5411\u7684\u566a\u58f0\u5224\u65ad\uff1b3\uff09\u91c7\u7528\u7c92\u5b50\u8fd1\u4f3c\u8fdb\u884c\u540e\u9a8c\u63a8\u65ad\uff1b4\uff09\u4ee5\u671f\u671b\u4fe1\u606f\u589e\u76ca\u4e3a\u51c6\u5219\u9009\u62e9\u6700\u4f18\u67e5\u8be2\u3002", "result": "\u5728\u5408\u6210\u56fe\u3001\u86cb\u767d\u8d28\u4fe1\u53f7\u548c\u57fa\u56e0\u6270\u52a8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4e25\u683c\u67e5\u8be2\u9884\u7b97\u4e0b\u5b9e\u73b0\u66f4\u5feb\u540e\u9a8c\u96c6\u4e2d\uff0c\u5e76\u663e\u8457\u6539\u5584\u6709\u5411\u56e0\u679c\u6548\u5e94\u7684\u6062\u590d\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u6574\u5408\u4e13\u5bb6\u77e5\u8bc6\uff0c\u901a\u8fc7\u667a\u80fd\u67e5\u8be2\u7b56\u7565\u63d0\u5347\u56e0\u679c\u53d1\u73b0\u6548\u7387\uff0c\u4e3a\u9ad8\u7ef4\u56e0\u679c\u5b66\u4e60\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u4e13\u5bb6\u53c2\u4e0e\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01485", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01485", "abs": "https://arxiv.org/abs/2602.01485", "authors": ["Muheng Li", "Jian Qian", "Wenlong Mou"], "title": "Predicting and improving test-time scaling laws via reward tail-guided search", "comment": "33 pages, 5 figures", "summary": "Test-time scaling has emerged as a critical avenue for enhancing the reasoning capabilities of Large Language Models (LLMs). Though the straight-forward ''best-of-$N$'' (BoN) strategy has already demonstrated significant improvements in performance, it lacks principled guidance on the choice of $N$, budget allocation, and multi-stage decision-making, thereby leaving substantial room for optimization. While many works have explored such optimization, rigorous theoretical guarantees remain limited. In this work, we propose new methodologies to predict and improve scaling properties via tail-guided search. By estimating the tail distribution of rewards, our method predicts the scaling law of LLMs without the need for exhaustive evaluations. Leveraging this prediction tool, we introduce Scaling-Law Guided (SLG) Search, a new test-time algorithm that dynamically allocates compute to identify and exploit intermediate states with the highest predicted potential. We theoretically prove that SLG achieves vanishing regret compared to perfect-information oracles, and achieves expected rewards that would otherwise require a polynomially larger compute budget required when using BoN. Empirically, we validate our framework across different LLMs and reward models, confirming that tail-guided allocation consistently achieves higher reward yields than Best-of-$N$ under identical compute budgets. Our code is available at https://github.com/PotatoJnny/Scaling-Law-Guided-search.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faScaling-Law Guided (SLG) Search\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f30\u8ba1\u5956\u52b1\u7684\u5c3e\u90e8\u5206\u5e03\u6765\u9884\u6d4bLLM\u7f29\u653e\u89c4\u5f8b\u5e76\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u3002\u76f8\u6bd4Best-of-N\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u6e10\u8fd1\u65e0\u6094\u6027\uff0c\u4e14\u5728\u76f8\u540c\u8ba1\u7b97\u9884\u7b97\u4e0b\u80fd\u83b7\u5f97\u66f4\u9ad8\u5956\u52b1\u3002", "motivation": "Best-of-N\u7b56\u7565\u867d\u80fd\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5728\u9009\u62e9N\u3001\u9884\u7b97\u5206\u914d\u548c\u591a\u9636\u6bb5\u51b3\u7b56\u65b9\u9762\u7f3a\u4e4f\u539f\u5219\u6027\u6307\u5bfc\u3002\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u7f3a\u4e4f\u4e25\u683c\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "method": "\u63d0\u51fa\u5c3e\u90e8\u5f15\u5bfc\u641c\u7d22\uff0c\u901a\u8fc7\u4f30\u8ba1\u5956\u52b1\u5c3e\u90e8\u5206\u5e03\u6765\u9884\u6d4b\u7f29\u653e\u89c4\u5f8b\uff0c\u65e0\u9700\u7a77\u5c3d\u8bc4\u4f30\u3002\u5f15\u5165SLG Search\u7b97\u6cd5\uff0c\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u4ee5\u8bc6\u522b\u548c\u5229\u7528\u5177\u6709\u6700\u9ad8\u9884\u6d4b\u6f5c\u529b\u7684\u4e2d\u95f4\u72b6\u6001\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e0e\u5b8c\u7f8e\u4fe1\u606f\u9884\u8a00\u673a\u76f8\u6bd4\u5177\u6709\u6e10\u8fd1\u65e0\u6094\u6027\uff0c\u4e14\u80fd\u8fbe\u5230\u9700\u8981\u591a\u9879\u5f0f\u7ea7\u66f4\u5927\u8ba1\u7b97\u9884\u7b97\u624d\u80fd\u83b7\u5f97\u7684\u671f\u671b\u5956\u52b1\u3002\u5b9e\u8bc1\u9a8c\u8bc1\u5728\u4e0d\u540cLLM\u548c\u5956\u52b1\u6a21\u578b\u4e0a\uff0c\u76f8\u540c\u9884\u7b97\u4e0b\u5c3e\u90e8\u5f15\u5bfc\u5206\u914d\u59cb\u7ec8\u4f18\u4e8eBest-of-N\u3002", "conclusion": "\u5c3e\u90e8\u5f15\u5bfc\u7684\u8ba1\u7b97\u5206\u914d\u5728\u6d4b\u8bd5\u65f6\u7f29\u653e\u4e2d\u6bd4Best-of-N\u66f4\u9ad8\u6548\uff0c\u517c\u5177\u5f3a\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u8bc1\u6539\u8fdb\u3002"}}
{"id": "2602.01486", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01486", "abs": "https://arxiv.org/abs/2602.01486", "authors": ["Xuesong Wang", "Michael Groom", "Rafael Oliveira", "He Zhao", "Terence O'Kane", "Edwin V. Bonilla"], "title": "Multi-Scale Wavelet Transformers for Operator Learning of Dynamical Systems", "comment": null, "summary": "Recent years have seen a surge in data-driven surrogates for dynamical systems that can be orders of magnitude faster than numerical solvers. However, many machine learning-based models such as neural operators exhibit spectral bias, attenuating high-frequency components that often encode small-scale structure. This limitation is particularly damaging in applications such as weather forecasting, where misrepresented high frequencies can induce long-horizon instability. To address this issue, we propose multi-scale wavelet transformers (MSWTs), which learn system dynamics in a tokenized wavelet domain. The wavelet transform explicitly separates low- and high-frequency content across scales. MSWTs leverage a wavelet-preserving downsampling scheme that retains high-frequency features and employ wavelet-based attention to capture dependencies across scales and frequency bands. Experiments on chaotic dynamical systems show substantial error reductions and improved long horizon spectral fidelity. On the ERA5 climate reanalysis, MSWTs further reduce climatological bias, demonstrating their effectiveness in a real-world forecasting setting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u591a\u5c3a\u5ea6\u5c0f\u6ce2Transformer\uff08MSWT\uff09\u6765\u89e3\u51b3\u52a8\u529b\u7cfb\u7edf\u795e\u7ecf\u7b97\u5b50\u7684\u9891\u8c31\u504f\u7f6e\u95ee\u9898\uff0c\u901a\u8fc7\u5728token\u5316\u5c0f\u6ce2\u57df\u4e2d\u5b66\u4e60\uff0c\u91c7\u7528\u4fdd\u9ad8\u9891\u4e0b\u91c7\u6837\u548c\u8de8\u5c3a\u5ea6\u6ce8\u610f\u529b\uff0c\u5728\u6df7\u6c8c\u7cfb\u7edf\u548c\u5929\u6c14\u9884\u62a5\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u9ad8\u9891\u4fdd\u771f\u5ea6\u548c\u66f4\u4f4e\u7684\u8bef\u5dee\u3002", "motivation": "\u8bb8\u591a\u52a8\u529b\u7cfb\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5b58\u5728\u9891\u8c31\u504f\u7f6e\uff0c\u524a\u5f31\u4e86\u7f16\u7801\u5c0f\u5c3a\u5ea6\u7ed3\u6784\u7684\u9ad8\u9891\u6210\u5206\uff0c\u8fd9\u5728\u5929\u6c14\u9884\u62a5\u4e2d\u5c24\u4e3a\u81f4\u547d\uff0c\u56e0\u4e3a\u9519\u8bef\u7684\u9ad8\u9891\u8868\u793a\u4f1a\u5bfc\u81f4\u957f\u671f\u9884\u6d4b\u4e0d\u7a33\u5b9a\uff0c\u56e0\u6b64\u9700\u8981\u80fd\u66f4\u597d\u6355\u6349\u591a\u5c3a\u5ea6\u52a8\u529b\u5b66\u7684\u6a21\u578b\u3002", "method": "\u4f5c\u8005\u5f00\u53d1\u4e86\u591a\u5c3a\u5ea6\u5c0f\u6ce2Transformer\uff08MSWT\uff09\uff0c\u5728token\u5316\u5c0f\u6ce2\u57df\u4e2d\u5b66\u4e60\u7cfb\u7edf\u52a8\u529b\u5b66\u3002\u65b9\u6cd5\u5305\u62ec\uff1a\uff081\uff09\u5c0f\u6ce2\u53d8\u6362\u663e\u5f0f\u5206\u79bb\u4e0d\u540c\u5c3a\u5ea6\u7684\u4f4e\u9891\u548c\u9ad8\u9891\u5185\u5bb9\uff1b\uff082\uff09\u4fdd\u9ad8\u9891\u4e0b\u91c7\u6837\u65b9\u6848\u4fdd\u7559\u9ad8\u9891\u7279\u5f81\uff1b\uff083\uff09\u5c0f\u6ce2\u6ce8\u610f\u529b\u673a\u5236\u6355\u83b7\u8de8\u5c3a\u5ea6\u548c\u9891\u6bb5\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728\u6df7\u6c8c\u52a8\u529b\u7cfb\u7edf\u5b9e\u9a8c\u4e2d\uff0cMSWT\u663e\u8457\u964d\u4f4e\u8bef\u5dee\u5e76\u63d0\u5347\u957f\u671f\u9891\u8c31\u4fdd\u771f\u5ea6\uff1b\u5728ERA5\u6c14\u5019\u518d\u5206\u6790\u6570\u636e\u96c6\u4e0a\uff0cMSWT\u8fdb\u4e00\u6b65\u51cf\u5c11\u6c14\u5019\u5b66\u504f\u5dee\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u771f\u5b9e\u5929\u6c14\u9884\u62a5\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "MSWT\u901a\u8fc7\u5c0f\u6ce2\u57df\u8868\u793a\u6709\u6548\u89e3\u51b3\u4e86\u795e\u7ecf\u7b97\u5b50\u7684\u9891\u8c31\u504f\u7f6e\u95ee\u9898\uff0c\u4e3a\u52a8\u529b\u7cfb\u7edf\uff08\u7279\u522b\u662f\u5929\u6c14\u9884\u62a5\uff09\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7a33\u5b9a\u7684\u957f\u671f\u9884\u6d4b\u3002"}}
{"id": "2602.01493", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01493", "abs": "https://arxiv.org/abs/2602.01493", "authors": ["Zhuoyuan Wang", "Hanjiang Hu", "Xiyu Deng", "Saviz Mowlavi", "Yorie Nakahira"], "title": "OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference", "comment": null, "summary": "Solving diverse partial differential equations (PDEs) is fundamental in science and engineering. Large language models (LLMs) have demonstrated strong capabilities in code generation, symbolic reasoning, and tool use, but reliably solving PDEs across heterogeneous settings remains challenging. Prior work on LLM-based code generation and transformer-based foundation models for PDE learning has shown promising advances. However, a persistent trade-off between execution success rate and numerical accuracy arises, particularly when generalization to unseen parameters and boundary conditions is required. In this work, we propose OpInf-LLM, an LLM parametric PDE solving framework based on operator inference. The proposed framework leverages a small amount of solution data to enable accurate prediction of diverse PDE instances, including unseen parameters and configurations, and provides seamless integration with LLMs for natural language specification of PDE solving tasks. Its low computational demands and unified tool interface further enable a high execution success rate across heterogeneous settings. By combining operator inference with LLM capabilities, OpInf-LLM opens new possibilities for generalizable reduced-order modeling in LLM-based PDE solving.", "AI": {"tldr": "Proposes OpInf-LLM, combining operator inference with LLMs to solve parametric PDEs using minimal data while maintaining high success rate and accuracy.", "motivation": "LLMs face challenges in reliable PDE solving with a trade-off between execution success and numerical accuracy, especially for unseen parameters and boundary conditions.", "method": "A framework integrating operator inference with LLMs, using small solution data to predict diverse PDE instances via natural language specification.", "result": "Achieves high execution success rate, accurate predictions for unseen configurations, low computational cost, and unified tool interface.", "conclusion": "Enables generalizable reduced-order modeling for LLM-based PDE solving, opening new possibilities in scientific computing."}}
{"id": "2602.01505", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01505", "abs": "https://arxiv.org/abs/2602.01505", "authors": ["Navdeep Kumar", "Tehila Dahan", "Lior Cohen", "Ananyabrata Barua", "Giorgia Ramponi", "Kfir Yehuda Levy", "Shie Mannor"], "title": "Optimal Sample Complexity for Single Time-Scale Actor-Critic with Momentum", "comment": null, "summary": "We establish an optimal sample complexity of $O(\u03b5^{-2})$ for obtaining an $\u03b5$-optimal global policy using a single-timescale actor-critic (AC) algorithm in infinite-horizon discounted Markov decision processes (MDPs) with finite state-action spaces, improving upon the prior state of the art of $O(\u03b5^{-3})$. Our approach applies STORM (STOchastic Recursive Momentum) to reduce variance in the critic updates. However, because samples are drawn from a nonstationary occupancy measure induced by the evolving policy, variance reduction via STORM alone is insufficient. To address this challenge, we maintain a buffer of small fraction of recent samples and uniformly sample from it for each critic update. Importantly, these mechanisms are compatible with existing deep learning architectures and require only minor modifications, without compromising practical applicability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u6539\u8fdb\u7684actor-critic\u7b97\u6cd5\uff0c\u901a\u8fc7STORM\u65b9\u5dee\u7f29\u51cf\u548c\u6837\u672c\u7f13\u51b2\u533a\u673a\u5236\uff0c\u5c06\u6837\u672c\u590d\u6742\u5ea6\u4eceO(\u03b5\u207b\u00b3)\u63d0\u5347\u81f3\u6700\u4f18\u7684O(\u03b5\u207b\u00b2)\u3002", "motivation": "\u9488\u5bf9\u6709\u9650\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u7684\u65e0\u9650\u65f6\u57df\u6298\u6263MDP\uff0c\u73b0\u6709\u5355\u65f6\u95f4\u5c3a\u5ea6actor-critic\u7b97\u6cd5\u83b7\u5f97\u03b5-\u6700\u4f18\u7b56\u7565\u7684\u6837\u672c\u590d\u6742\u5ea6\u4e3aO(\u03b5\u207b\u00b3)\uff0c\u5c1a\u672a\u8fbe\u5230\u7406\u8bba\u6700\u4f18\u3002", "method": "\u5728\u5355\u65f6\u95f4\u5c3a\u5ea6actor-critic\u6846\u67b6\u4e2d\uff0c\u91c7\u7528STORM\u6280\u672f\u964d\u4f4ecritic\u66f4\u65b0\u7684\u65b9\u5dee\uff0c\u540c\u65f6\u7ef4\u62a4\u8fd1\u671f\u6837\u672c\u7f13\u51b2\u533a\uff0c\u6bcf\u6b21\u66f4\u65b0\u65f6\u4ece\u4e2d\u5747\u5300\u91c7\u6837\u4ee5\u5e94\u5bf9\u7b56\u7565\u6f14\u5316\u5bfc\u81f4\u7684\u975e\u5e73\u7a33\u6027\u95ee\u9898\u3002", "result": "\u5b9e\u73b0\u4e86O(\u03b5\u207b\u00b2)\u7684\u6700\u4f18\u6837\u672c\u590d\u6742\u5ea6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709O(\u03b5\u207b\u00b3)\u7684\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4ec5\u9700\u5bf9\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u505a\u5c11\u91cf\u4fee\u6539\uff0c\u4fdd\u6301\u4e86\u826f\u597d\u7684\u5b9e\u9645\u9002\u7528\u6027\u3002"}}
{"id": "2602.01510", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.01510", "abs": "https://arxiv.org/abs/2602.01510", "authors": ["Hengzhe Zhang", "Qi Chen", "Bing Xue", "Wolfgang Banzhaf", "Mengjie Zhang"], "title": "Enhancing Generalization in Evolutionary Feature Construction for Symbolic Regression through Vicinal Jensen Gap Minimization", "comment": null, "summary": "Genetic programming-based feature construction has achieved significant success in recent years as an automated machine learning technique to enhance learning performance. However, overfitting remains a challenge that limits its broader applicability. To improve generalization, we prove that vicinal risk, estimated through noise perturbation or mixup-based data augmentation, is bounded by the sum of empirical risk and a regularization term-either finite difference or the vicinal Jensen gap. Leveraging this decomposition, we propose an evolutionary feature construction framework that jointly optimizes empirical risk and the vicinal Jensen gap to control overfitting. Since datasets may vary in noise levels, we develop a noise estimation strategy to dynamically adjust regularization strength. Furthermore, to mitigate manifold intrusion-where data augmentation may generate unrealistic samples that fall outside the data manifold-we propose a manifold intrusion detection mechanism. Experimental results on 58 datasets demonstrate the effectiveness of Jensen gap minimization compared to other complexity measures. Comparisons with 15 machine learning algorithms further indicate that genetic programming with the proposed overfitting control strategy achieves superior performance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9057\u4f20\u7f16\u7a0b\u7684\u7279\u5f81\u6784\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7ecf\u9a8c\u98ce\u9669\u548cvicinal Jensen gap\u6765\u63a7\u5236\u8fc7\u62df\u5408\uff0c\u5e76\u7ed3\u5408\u52a8\u6001\u566a\u58f0\u4f30\u8ba1\u548c\u6d41\u5f62\u4fb5\u5165\u68c0\u6d4b\u673a\u5236\uff0c\u572858\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e15\u79cd\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u3002", "motivation": "\u9057\u4f20\u7f16\u7a0b\u7279\u5f81\u6784\u5efa\u867d\u5728\u81ea\u52a8\u5316\u673a\u5668\u5b66\u4e60\u4e2d\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u8fc7\u62df\u5408\u95ee\u9898\u9650\u5236\u4e86\u5176\u66f4\u5e7f\u6cdb\u5e94\u7528\u3002\u4e3a\u4e86\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u9700\u8981\u6709\u6548\u63a7\u5236\u8fc7\u62df\u5408\u3002", "method": "1) \u7406\u8bba\u8bc1\u660e\uff1a\u901a\u8fc7\u566a\u58f0\u6270\u52a8\u6216mixup\u6570\u636e\u589e\u5f3a\u4f30\u8ba1\u7684vicinal risk\uff0c\u53ef\u88ab\u7ecf\u9a8c\u98ce\u9669\u4e0e\u6b63\u5219\u5316\u9879\uff08\u6709\u9650\u5dee\u5206\u6216vicinal Jensen gap\uff09\u4e4b\u548c\u754c\u5b9a\uff1b2) \u63d0\u51fa\u8fdb\u5316\u7279\u5f81\u6784\u5efa\u6846\u67b6\uff0c\u8054\u5408\u4f18\u5316\u7ecf\u9a8c\u98ce\u9669\u548cvicinal Jensen gap\uff1b3) \u5f00\u53d1\u566a\u58f0\u4f30\u8ba1\u7b56\u7565\u52a8\u6001\u8c03\u6574\u6b63\u5219\u5316\u5f3a\u5ea6\uff1b4) \u8bbe\u8ba1\u6d41\u5f62\u4fb5\u5165\u68c0\u6d4b\u673a\u5236\u9632\u6b62\u6570\u636e\u589e\u5f3a\u751f\u6210\u4e0d\u771f\u5b9e\u6837\u672c\u3002", "result": "\u572858\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cJensen gap\u6700\u5c0f\u5316\u76f8\u6bd4\u5176\u4ed6\u590d\u6742\u5ea6\u5ea6\u91cf\u66f4\u6709\u6548\uff0c\u4e14\u8be5\u9057\u4f20\u7f16\u7a0b\u8fc7\u62df\u5408\u63a7\u5236\u7b56\u7565\u5728\u5bf9\u6bd415\u79cd\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u65f6\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8evicinal Jensen gap\u7684\u8fc7\u62df\u5408\u63a7\u5236\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u9057\u4f20\u7f16\u7a0b\u7279\u5f81\u6784\u5efa\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u5b66\u4e60\u6548\u679c\u3002"}}
{"id": "2602.01516", "categories": ["cs.LG", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.01516", "abs": "https://arxiv.org/abs/2602.01516", "authors": ["Enzo Nicolas Spotorno", "Matheus Wagner", "Antonio Augusto Medeiros Frohlich"], "title": "White-Box Neural Ensemble for Vehicular Plasticity: Quantifying the Efficiency Cost of Symbolic Auditability in Adaptive NMPC", "comment": "5 pages, 1 table, 1 figure, submitted to IEEE VTC 2026 Recent Results Track", "summary": "We present a white-box adaptive NMPC architecture that resolves vehicular plasticity (adaptation to varying operating regimes without retraining) by arbitrating among frozen, regime-specific neural specialists using a Modular Sovereignty paradigm. The ensemble dynamics are maintained as a fully traversable symbolic graph in CasADi, enabling maximal runtime auditability. Synchronous simulation validates rapid adaptation (~7.3 ms) and near-ideal tracking fidelity under compound regime shifts (friction, mass, drag) where non-adaptive baselines fail. Empirical benchmarking quantifies the transparency cost: symbolic graph maintenance increases solver latency by 72-102X versus compiled parametric physics models, establishing the efficiency price of strict white-box implementation.", "AI": {"tldr": "A white-box adaptive NMPC using modular sovereignty to switch between pre-trained neural specialists achieves fast adaptation (~7.3ms) under varying vehicle conditions but incurs 72-102X computational overhead versus black-box models.", "motivation": "To enable vehicles to adapt to changing operating regimes (friction, mass, drag) without retraining while maintaining full runtime auditability for safety-critical control.", "method": "Arbitrating among frozen, regime-specific neural specialists via a Modular Sovereignty paradigm, with ensemble dynamics maintained as a fully traversable symbolic graph in CasADi.", "result": "Rapid adaptation (~7.3 ms) and near-ideal tracking fidelity under compound regime shifts where non-adaptive baselines fail, but symbolic graph maintenance increases solver latency by 72-102X.", "conclusion": "Strict white-box implementation provides transparency and adaptability at a substantial computational cost, quantifying the efficiency price of maximal auditability in adaptive vehicle control."}}
{"id": "2602.01519", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01519", "abs": "https://arxiv.org/abs/2602.01519", "authors": ["Shiju Zhao", "Junhao Hu", "Jiaqi Zheng", "Guihai Chen"], "title": "You Need an Encoder for Native Position-Independent Caching", "comment": "12 pages, 10 figures. Welcome back, Encoder", "summary": "The Key-Value (KV) cache of Large Language Models (LLMs) is prefix-based, making it highly inefficient for processing contexts retrieved in arbitrary order. Position-Independent Caching (PIC) has been proposed to enable KV reuse without positional constraints; however, existing approaches often incur substantial accuracy degradation, limiting their practical adoption. To address this issue, we propose native PIC by reintroducing the encoder to prevalent decoder-only LLMs and explicitly training it to support PIC. We further develop COMB, a PIC-aware caching system that integrates seamlessly with existing inference frameworks. Experimental results show that COMB reduces Time-to-First-Token (TTFT) by 51-94% and increases throughput by 3$\\times$ with comparable accuracy. Furthermore, the quality improvement when using DeepSeek-V2-Lite-Chat demonstrates the applicability of COMB to other types of decoder-only LLMs. Our code is available at https://github.com/shijuzhao/Comb.", "AI": {"tldr": "Proposing COMB, a native Position-Independent Caching system for decoder-only LLMs that reintroduces an encoder and trains it specifically for PIC, achieving 51-94% TTFT reduction and 3x throughput with minimal accuracy loss.", "motivation": "Large Language Models' prefix-based KV cache is inefficient for arbitrary-order context retrieval. Existing Position-Independent Caching (PIC) methods suffer from significant accuracy degradation, limiting practical adoption.", "method": "We introduce native PIC by reintroducing an encoder to decoder-only LLMs and explicitly training it to support position-independent caching. We develop COMB, a PIC-aware caching system that integrates with existing inference frameworks.", "result": "COMB reduces Time-to-First-Token by 51-94% and improves throughput by 3x while maintaining comparable accuracy. The system also demonstrates effectiveness on DeepSeek-V2-Lite-Chat, showing broad applicability across decoder-only LLM architectures.", "conclusion": "Native PIC through encoder reintroduction and specialized training enables efficient KV cache reuse without positional constraints, significantly improving inference performance while preserving model accuracy, making it practical for real-world deployment."}}
{"id": "2602.01523", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01523", "abs": "https://arxiv.org/abs/2602.01523", "authors": ["Akifumi Wachi", "Hirota Kinoshita", "Shokichi Takakura", "Rei Higuchi", "Taiji Suzuki"], "title": "A Relative-Budget Theory for Reinforcement Learning with Verifiable Rewards in Large Language Model Reasoning", "comment": "28 pages", "summary": "Reinforcement learning (RL) is a dominant paradigm for improving the reasoning abilities of large language models, yet its effectiveness varies across tasks and compute budgets. We propose a \\emph{relative-budget} theory explaining this variation through a single quantity called relative budget $\u03be:= H/\\mathbb{E}[T]$, where $H$ is the generation horizon (token budget) and $T$ denotes the number of tokens until the first correct solution under a base policy. We show that $\u03be$ determines sample efficiency by controlling reward variance and the likelihood of informative trajectories. Our analysis reveals three regimes: in the \\emph{deficient} regime ($\u03be\\to 0$), informative trajectories are rare and the sample complexity explodes; in the \\emph{balanced} regime ($\u03be=\u0398(1)$), informative trajectories occur with non-negligible probability and RL is maximally sample-efficient; and in the \\emph{ample} regime ($\u03be\\to \\infty$), learning remains stable but marginal gains per iteration diminish. We further provide finite-sample guarantees for online RL that characterize learning progress across these regimes. Specifically, in a case study under idealized distributional assumptions, we show that the relative budget grows linearly over iterations. Our empirical results confirm these predictions in realistic settings, identifying a budget $\u03be\\in [1.5, 2.0]$ that maximizes learning efficiency and coincides with peak reasoning performance.", "AI": {"tldr": "A relative-budget theory (\u03be = H/E[T]) explains why reinforcement learning effectiveness varies for LLM reasoning, identifying three regimes (deficient, balanced, ample) and an optimal budget range [1.5, 2.0] for peak performance.", "motivation": "Reinforcement learning improves LLM reasoning abilities but its effectiveness varies across tasks and compute budgets, and this variation lacks a unified theoretical explanation.", "method": "Proposing a relative-budget theory using \u03be = H/E[T] (generation horizon divided by expected tokens to first correct solution), analyzing three regimes (\u03be\u21920, \u03be=\u0398(1), \u03be\u2192\u221e), providing finite-sample guarantees for online RL, and conducting empirical validation.", "result": "The theory reveals that \u03be controls sample efficiency through reward variance and informative trajectory likelihood; empirical results confirm predictions and identify optimal \u03be \u2208 [1.5, 2.0] that maximizes learning efficiency and coincides with peak reasoning performance.", "conclusion": "The relative-budget theory successfully characterizes RL effectiveness in LLMs across compute budgets, providing both theoretical insights and practical guidance for optimal budget allocation."}}
{"id": "2602.01553", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01553", "abs": "https://arxiv.org/abs/2602.01553", "authors": ["Quang Truong", "Yu Song", "Donald Loveland", "Mingxuan Ju", "Tong Zhao", "Neil Shah", "Jiliang Tang"], "title": "Plain Transformers are Surprisingly Powerful Link Predictors", "comment": null, "summary": "Link prediction is a core challenge in graph machine learning, demanding models that capture rich and complex topological dependencies. While Graph Neural Networks (GNNs) are the standard solution, state-of-the-art pipelines often rely on explicit structural heuristics or memory-intensive node embeddings -- approaches that struggle to generalize or scale to massive graphs. Emerging Graph Transformers (GTs) offer a potential alternative but often incur significant overhead due to complex structural encodings, hindering their applications to large-scale link prediction. We challenge these sophisticated paradigms with PENCIL, an encoder-only plain Transformer that replaces hand-crafted priors with attention over sampled local subgraphs, retaining the scalability and hardware efficiency of standard Transformers. Through experimental and theoretical analysis, we show that PENCIL extracts richer structural signals than GNNs, implicitly generalizing a broad class of heuristics and subgraph-based expressivity. Empirically, PENCIL outperforms heuristic-informed GNNs and is far more parameter-efficient than ID-embedding--based alternatives, while remaining competitive across diverse benchmarks -- even without node features. Our results challenge the prevailing reliance on complex engineering techniques, demonstrating that simple design choices are potentially sufficient to achieve the same capabilities.", "AI": {"tldr": "PENCIL proposes a plain encoder-only Transformer for link prediction that uses attention over sampled local subgraphs, replacing complex structural heuristics and achieving superior performance and scalability compared to GNNs and Graph Transformers.", "motivation": "Current link prediction methods like GNNs rely on structural heuristics or memory-intensive embeddings that struggle with generalization and scalability, while Graph Transformers incur high overhead from complex encodings, limiting large-scale applications.", "method": "PENCIL employs an encoder-only plain Transformer architecture that processes sampled local subgraphs through attention mechanisms, eliminating hand-crafted structural priors while maintaining Transformer scalability and hardware efficiency.", "result": "PENCIL extracts richer structural signals than GNNs, implicitly generalizes diverse heuristics, outperforms heuristic-informed GNNs, is more parameter-efficient than ID-embedding alternatives, and remains competitive across benchmarks even without node features.", "conclusion": "The study challenges the need for complex engineering techniques in graph ML, demonstrating that simple Transformer-based designs with local subgraph attention can achieve equivalent or superior capabilities to sophisticated paradigms."}}
{"id": "2602.01554", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01554", "abs": "https://arxiv.org/abs/2602.01554", "authors": ["Lv Tang", "Tianyi Zheng", "Bo Li", "Xingyu Li"], "title": "InfoTok: Regulating Information Flow for Capacity-Constrained Shared Visual Tokenization in Unified MLLMs", "comment": null, "summary": "Unified multimodal large language models (MLLMs) integrate image understanding and generation in a single framework, with the visual tokenizer acting as the sole interface that maps visual inputs into tokens for downstream tasks. However, existing shared-token designs are mostly architecture-driven and lack an explicit criterion for what information tokens should preserve to support both understanding and generation. Therefore, we introduce a capacity-constrained perspective, highlighting that in shared-token unified MLLMs the visual tokenizer behaves as a compute-bounded learner, so the token budget should prioritize reusable structure over hard-to-exploit high-entropy variations and redundancy. Motivated by this perspective, we propose InfoTok, an information-regularized visual tokenization mechanism grounded in the Information Bottleneck (IB) principle. InfoTok formulates tokenization as controlling information flow from images to shared tokens to multimodal outputs, yielding a principled trade-off between compression and task relevance via mutual-information regularization. We integrate InfoTok into three representative unified MLLMs without introducing any additional training data. Experiments show consistent improvements on both understanding and generation, supporting information-regularized tokenization as a principled foundation for learning a shared token space in unified MLLMs.", "AI": {"tldr": "Proposing InfoTok, an information-regularized visual tokenization method based on Information Bottleneck principle for unified multimodal LLMs, which improves both image understanding and generation by prioritizing reusable structure over redundant information under token budget constraints.", "motivation": "Existing unified MLLMs lack explicit criteria for what information shared visual tokens should preserve, as current designs are architecture-driven rather than principle-driven, failing to optimize token allocation for both understanding and generation tasks under computational constraints.", "method": "Introduces a capacity-constrained perspective and proposes InfoTok, which formulates tokenization as controlling information flow from images to shared tokens using Information Bottleneck principle with mutual-information regularization to achieve optimal trade-off between compression and task relevance.", "result": "Experiments demonstrate consistent improvements on both image understanding and generation benchmarks across three representative unified MLLMs without introducing additional training data.", "conclusion": "Information-regularized tokenization provides a principled foundation for learning shared token spaces in unified MLLMs, validating that prioritizing reusable structure over high-entropy variations and redundancy is essential for optimal performance."}}
{"id": "2602.01558", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01558", "abs": "https://arxiv.org/abs/2602.01558", "authors": ["Yiming Ma", "Lixu Wang", "Lionel Z. Wang", "Hongkun Yang", "Haoming Sun", "Xin Xu", "Jiaqi Wu", "Bin Chen", "Wei Dong"], "title": "How Implicit Bias Accumulates and Propagates in LLM Long-term Memory", "comment": "Under review, and the first two authors contribute equally", "summary": "Long-term memory mechanisms enable Large Language Models (LLMs) to maintain continuity and personalization across extended interaction lifecycles, but they also introduce new and underexplored risks related to fairness. In this work, we study how implicit bias, defined as subtle statistical prejudice, accumulates and propagates within LLMs equipped with long-term memory. To support systematic analysis, we introduce the Decision-based Implicit Bias (DIB) Benchmark, a large-scale dataset comprising 3,776 decision-making scenarios across nine social domains, designed to quantify implicit bias in long-term decision processes. Using a realistic long-horizon simulation framework, we evaluate six state-of-the-art LLMs integrated with three representative memory architectures on DIB and demonstrate that LLMs' implicit bias does not remain static but intensifies over time and propagates across unrelated domains. We further analyze mitigation strategies and show that a static system-level prompting baseline provides limited and short-lived debiasing effects. To address this limitation, we propose Dynamic Memory Tagging (DMT), an agentic intervention that enforces fairness constraints at memory write time. Extensive experimental results show that DMT substantially reduces bias accumulation and effectively curtails cross-domain bias propagation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u957f\u671f\u8bb0\u5fc6\u673a\u5236\u5982\u4f55\u5bfc\u81f4\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u9690\u6027\u504f\u89c1\u79ef\u7d2f\u548c\u8de8\u9886\u57df\u4f20\u64ad\uff0c\u63d0\u51fa\u4e86DIB\u57fa\u51c6\u6765\u91cf\u5316\u8fd9\u4e00\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u4e86\u52a8\u6001\u8bb0\u5fc6\u6807\u8bb0(DMT)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u8bb0\u5fc6\u5199\u5165\u65f6\u65bd\u52a0\u516c\u5e73\u7ea6\u675f\uff0c\u6709\u6548\u51cf\u8f7b\u4e86\u504f\u89c1\u7d2f\u79ef\u548c\u4f20\u64ad\u3002", "motivation": "\u957f\u671f\u8bb0\u5fc6\u673a\u5236\u867d\u4e3aLLMs\u5e26\u6765\u4ea4\u4e92\u8fde\u7eed\u6027\u548c\u4e2a\u6027\u5316\u80fd\u529b\uff0c\u4f46\u4e5f\u5f15\u5165\u4e86\u65b0\u7684\u516c\u5e73\u6027\u98ce\u9669\u3002\u9690\u6027\u504f\u89c1\u4f1a\u5728\u957f\u671f\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u79ef\u7d2f\u5e76\u6269\u6563\u5230\u65e0\u5173\u9886\u57df\uff0c\u4f46\u8fd9\u4e00\u73b0\u8c61\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "method": "1) \u6784\u5efa\u51b3\u7b56\u578b\u9690\u6027\u504f\u89c1(DIB)\u57fa\u51c6\uff0c\u5305\u542b9\u4e2a\u793e\u4f1a\u9886\u57df\u76843,776\u4e2a\u51b3\u7b56\u573a\u666f\uff1b2) \u521b\u5efa\u957f\u671f\u6a21\u62df\u6846\u67b6\uff0c\u8bc4\u4f306\u4e2a\u5148\u8fdbLLM\u4e0e3\u79cd\u8bb0\u5fc6\u67b6\u6784\u7684\u7ec4\u5408\uff1b3) \u63d0\u51fa\u52a8\u6001\u8bb0\u5fc6\u6807\u8bb0(DMT)\u5e72\u9884\u65b9\u6cd5\uff0c\u5728\u8bb0\u5fc6\u5199\u5165\u65f6\u5f3a\u5236\u6267\u884c\u516c\u5e73\u7ea6\u675f\u3002", "result": "1) \u53d1\u73b0LLM\u7684\u9690\u6027\u504f\u89c1\u968f\u65f6\u95f4\u63a8\u79fb\u800c\u52a0\u5267\uff0c\u5e76\u80fd\u8de8\u9886\u57df\u4f20\u64ad\uff1b2) \u9759\u6001\u7cfb\u7edf\u7ea7\u63d0\u793a\u53bb\u504f\u6548\u679c\u6709\u9650\u4e14\u77ed\u6682\uff1b3) DMT\u663e\u8457\u964d\u4f4e\u4e86\u504f\u89c1\u79ef\u7d2f\u5e76\u6709\u6548\u904f\u5236\u4e86\u8de8\u9886\u57df\u504f\u89c1\u4f20\u64ad\u3002", "conclusion": "\u957f\u671f\u8bb0\u5fc6\u4f1a\u653e\u5927LLMs\u7684\u516c\u5e73\u6027\u98ce\u9669\u3002\u901a\u8fc7\u5728\u8bb0\u5fc6\u5199\u5165\u9636\u6bb5\u65bd\u52a0\u52a8\u6001\u516c\u5e73\u7ea6\u675f\uff0cDMT\u4e3a\u7f13\u89e3\u504f\u89c1\u7d2f\u79ef\u548c\u8de8\u57df\u4f20\u64ad\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u6784\u5efa\u66f4\u516c\u5e73\u7684\u957f\u65f6\u7a0b\u5bf9\u8bdd\u7cfb\u7edf\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.01564", "categories": ["cs.LG", "math.AP", "math.OC", "math.PR"], "pdf": "https://arxiv.org/pdf/2602.01564", "abs": "https://arxiv.org/abs/2602.01564", "authors": ["Geuntaek Seo", "Minseop Shin", "Pierre Monmarch\u00e9", "Beomjun Choi"], "title": "Local Exponential Stability of Mean-Field Langevin Descent-Ascent in Wasserstein Space", "comment": null, "summary": "We study the mean-field Langevin descent-ascent (MFL-DA), a coupled optimization dynamics on the space of probability measures for entropically regularized two-player zero-sum games. Although the associated mean-field objective admits a unique mixed Nash equilibrium, the long-time behavior of the original MFL-DA for general nonconvex-nonconcave payoffs has remained largely open. Answering an open question posed by Wang and Chizat (COLT 2024), we provide a partial resolution by proving that this equilibrium is locally exponentially stable: if the initialization is sufficiently close in Wasserstein metric, the dynamics trends to the equilibrium at an exponential rate. The key to our analysis is to establish a coercivity estimate for the entropy near equilibrium via spectral analysis of the linearized operator. We show that this coercivity effectively reveals a local displacement convex-concave structure, thereby driving contraction. This result settles the local stability and quantitative rate questions of Wang and Chizat, leaving global convergence as a remaining open challenge.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86\u5bf9\u4e8e\u71b5\u6b63\u5219\u5316\u7684\u53cc\u4eba\u96f6\u548c\u535a\u5f08\uff0c\u5f53\u521d\u59cb\u5316\u8db3\u591f\u63a5\u8fd1\u65f6\uff0c\u5e73\u5747\u573a\u6717\u4e4b\u4e07\u4e0b\u964d\u4e0a\u5347\u52a8\u529b\u5b66\u4f1a\u4ee5\u6307\u6570\u901f\u5ea6\u6536\u655b\u5230\u552f\u4e00\u7684\u6df7\u5408\u7eb3\u4ec0\u5747\u8861\uff0c\u56de\u7b54\u4e86\u5173\u4e8e\u5c40\u90e8\u7a33\u5b9a\u6027\u7684\u4e00\u4e2a\u516c\u5f00\u95ee\u9898\u3002", "motivation": "\u5bf9\u4e8e\u4e00\u822c\u975e\u51f8-\u975e\u51f9\u6536\u76ca\u51fd\u6570\uff0c\u5e73\u5747\u573a\u6717\u4e4b\u4e07\u4e0b\u964d\u4e0a\u5347\u52a8\u529b\u5b66\u7684\u957f\u671f\u884c\u4e3a\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u7814\u7a76\u56de\u5e94\u4e86Wang\u548cChizat\uff08COLT 2024\uff09\u63d0\u51fa\u7684\u5173\u4e8e\u8be5\u52a8\u529b\u5b66\u5c40\u90e8\u7a33\u5b9a\u6027\u548c\u6536\u655b\u901f\u7387\u7684\u516c\u5f00\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u5bf9\u7ebf\u6027\u5316\u7b97\u5b50\u8fdb\u884c\u8c31\u5206\u6790\uff0c\u5728\u5747\u8861\u70b9\u9644\u8fd1\u5efa\u7acb\u71b5\u7684\u5f3a\u5236\u6027\u4f30\u8ba1\uff0c\u63ed\u793a\u4e86\u4e00\u79cd\u5c40\u90e8\u4f4d\u79fb\u51f8-\u51f9\u7ed3\u6784\uff0c\u4ece\u800c\u5728Wasserstein\u5ea6\u91cf\u4e0b\u4ea7\u751f\u6536\u7f29\u6548\u5e94\u3002", "result": "\u8bc1\u660e\u4e86\u6df7\u5408\u7eb3\u4ec0\u5747\u8861\u662f\u5c40\u90e8\u6307\u6570\u7a33\u5b9a\u7684\uff1a\u5f53\u521d\u59cb\u5316\u5728Wasserstein\u8ddd\u79bb\u4e0a\u8db3\u591f\u63a5\u8fd1\u65f6\uff0c\u8be5\u52a8\u529b\u5b66\u4ee5\u6307\u6570\u901f\u7387\u6536\u655b\u5230\u5747\u8861\u70b9\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u89e3\u51b3\u4e86\u5e73\u5747\u573a\u6717\u4e4b\u4e07\u4e0b\u964d\u4e0a\u5347\u52a8\u529b\u5b66\u7684\u5c40\u90e8\u7a33\u5b9a\u6027\u548c\u5b9a\u91cf\u901f\u7387\u95ee\u9898\uff0c\u4f46\u5168\u5c40\u6536\u655b\u6027\u4ecd\u662f\u5f85\u89e3\u51b3\u7684\u516c\u5f00\u6311\u6218\u3002"}}
{"id": "2602.01576", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01576", "abs": "https://arxiv.org/abs/2602.01576", "authors": ["Woosung Koh", "Sungjun Han", "Segyu Lee", "Se-Young Yun", "Jamin Shin"], "title": "Generative Visual Code Mobile World Models", "comment": "Pre-print (technical report)", "summary": "Mobile Graphical User Interface (GUI) World Models (WMs) offer a promising path for improving mobile GUI agent performance at train- and inference-time. However, current approaches face a critical trade-off: text-based WMs sacrifice visual fidelity, while the inability of visual WMs in precise text rendering led to their reliance on slow, complex pipelines dependent on numerous external models. We propose a novel paradigm: visual world modeling via renderable code generation, where a single Vision-Language Model (VLM) predicts the next GUI state as executable web code that renders to pixels, rather than generating pixels directly. This combines the strengths of both approaches: VLMs retain their linguistic priors for precise text rendering while their pre-training on structured web code enables high-fidelity visual generation. We introduce gWorld (8B, 32B), the first open-weight visual mobile GUI WMs built on this paradigm, along with a data generation framework (gWorld) that automatically synthesizes code-based training data. In extensive evaluation across 4 in- and 2 out-of-distribution benchmarks, gWorld sets a new pareto frontier in accuracy versus model size, outperforming 8 frontier open-weight models over 50.25x larger. Further analyses show that (1) scaling training data via gWorld yields meaningful gains, (2) each component of our pipeline improves data quality, and (3) stronger world modeling improves downstream mobile GUI policy performance.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u53ef\u6e32\u67d3\u4ee3\u7801\u751f\u6210\u7684\u89c6\u89c9\u4e16\u754c\u6a21\u578b\u65b0\u8303\u5f0f\uff0cgWorld\u6a21\u578b\u901a\u8fc7\u751f\u6210\u7f51\u9875\u4ee3\u7801\u800c\u975e\u76f4\u63a5\u751f\u6210\u50cf\u7d20\u6765\u9884\u6d4bGUI\u72b6\u6001\uff0c\u5728\u7cbe\u5ea6\u4e0e\u6a21\u578b\u5927\u5c0f\u95f4\u5efa\u7acb\u65b0\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c8B\u6a21\u578b\u6027\u80fd\u8d85\u8d8a50.25\u500d\u5927\u7684\u73b0\u6709\u6a21\u578b", "motivation": "\u73b0\u6709\u79fb\u52a8GUI\u4e16\u754c\u6a21\u578b\u9762\u4e34\u5173\u952e\u6743\u8861\uff1a\u57fa\u4e8e\u6587\u672c\u7684\u6a21\u578b\u727a\u7272\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u800c\u89c6\u89c9\u6a21\u578b\u65e0\u6cd5\u7cbe\u786e\u6e32\u67d3\u6587\u672c\u4e14\u4f9d\u8d56\u7f13\u6162\u590d\u6742\u7684\u591a\u6a21\u578b\u7ba1\u9053", "method": "\u63d0\u51fa\u53ef\u6e32\u67d3\u4ee3\u7801\u751f\u6210\u8303\u5f0f\uff0c\u8ba9\u5355\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u53ef\u6267\u884c\u7684\u7f51\u9875\u4ee3\u7801\u4f5c\u4e3a\u4e0b\u4e00GUI\u72b6\u6001\uff0c\u800c\u975e\u76f4\u63a5\u751f\u6210\u50cf\u7d20\u3002\u5f00\u53d1\u4e86gWorld(8B/32B)\u5f00\u6e90\u6a21\u578b\u53ca\u914d\u5957\u6570\u636e\u751f\u6210\u6846\u67b6", "result": "\u57284\u4e2a\u5206\u5e03\u5185\u548c2\u4e2a\u5206\u5e03\u5916\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cgWorld\u5728\u51c6\u786e\u7387\u4e0e\u6a21\u578b\u5927\u5c0f\u7684\u6743\u8861\u4e0a\u5efa\u7acb\u65b0\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u6027\u80fd\u8d85\u8d8a8\u4e2a\u89c4\u6a21\u592750.25\u500d\u7684\u5f00\u653e\u6743\u91cd\u6a21\u578b", "conclusion": "\u8be5\u8303\u5f0f\u6709\u6548\u7ed3\u5408\u89c6\u89c9\u4e0e\u6587\u672c\u4f18\u52bf\uff1b\u6269\u5c55\u8bad\u7ec3\u6570\u636e\u53ef\u5e26\u6765\u663e\u8457\u589e\u76ca\uff1b\u7ba1\u9053\u5404\u7ec4\u4ef6\u5747\u63d0\u5347\u6570\u636e\u8d28\u91cf\uff1b\u66f4\u5f3a\u7684\u4e16\u754c\u6a21\u578b\u80fd\u6539\u5584\u4e0b\u6e38GUI\u7b56\u7565\u6027\u80fd"}}
{"id": "2602.01588", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01588", "abs": "https://arxiv.org/abs/2602.01588", "authors": ["Huu Hiep Nguyen", "Minh Hoang Nguyen", "Dung Nguyen", "Hung Le"], "title": "Spectral Text Fusion: A Frequency-Aware Approach to Multimodal Time-Series Forecasting", "comment": null, "summary": "Multimodal time series forecasting is crucial in real-world applications, where decisions depend on both numerical data and contextual signals. The core challenge is to effectively combine temporal numerical patterns with the context embedded in other modalities, such as text. While most existing methods align textual features with time-series patterns one step at a time, they neglect the multiscale temporal influences of contextual information such as time-series cycles and dynamic shifts. This mismatch between local alignment and global textual context can be addressed by spectral decomposition, which separates time series into frequency components capturing both short-term changes and long-term trends. In this paper, we propose SpecTF, a simple yet effective framework that integrates the effect of textual data on time series in the frequency domain. Our method extracts textual embeddings, projects them into the frequency domain, and fuses them with the time series' spectral components using a lightweight cross-attention mechanism. This adaptively reweights frequency bands based on textual relevance before mapping the results back to the temporal domain for predictions. Experimental results demonstrate that SpecTF significantly outperforms state-of-the-art models across diverse multi-modal time series datasets while utilizing considerably fewer parameters. Code is available at https://github.com/hiepnh137/SpecTF.", "AI": {"tldr": "\u63d0\u51faSpecTF\u6846\u67b6\uff0c\u901a\u8fc7\u9891\u57df\u878d\u5408\u6587\u672c\u4e0e\u65f6\u5e8f\u6570\u636e\uff0c\u5229\u7528\u8c31\u5206\u89e3\u548c\u591a\u5c3a\u5ea6\u4ea4\u53c9\u6ce8\u610f\u529b\u81ea\u9002\u5e94\u52a0\u6743\u9891\u7387\u5206\u91cf\uff0c\u5728\u53c2\u6570\u66f4\u5c11\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9010\u70b9\u5bf9\u9f50\u6587\u672c\u4e0e\u65f6\u5e8f\u7279\u5f81\uff0c\u5ffd\u7565\u4e86\u6587\u672c\u4e0a\u4e0b\u6587\u5bf9\u65f6\u5e8f\u591a\u5c3a\u5ea6\u5468\u671f\u548c\u52a8\u6001\u53d8\u5316\u7684\u5168\u5c40\u5f71\u54cd\uff0c\u5bfc\u81f4\u5c40\u90e8\u5bf9\u9f50\u4e0e\u5168\u5c40\u8bed\u5883\u4e0d\u5339\u914d", "method": "\u5c06\u6587\u672c\u5d4c\u5165\u6295\u5f71\u5230\u9891\u57df\uff0c\u7528\u65f6\u5e8f\u8c31\u5206\u91cf\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u81ea\u9002\u5e94\u91cd\u52a0\u6743\u9891\u7387\u5e26\uff0c\u6700\u540e\u6620\u5c04\u56de\u65f6\u57df\u8fdb\u884c\u9884\u6d4b", "result": "\u5728\u591a\u4e2a\u591a\u6a21\u6001\u65f6\u5e8f\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8eSOTA\u6a21\u578b\uff0c\u4e14\u53c2\u6570\u91cf\u5927\u5e45\u51cf\u5c11", "conclusion": "\u9891\u57df\u878d\u5408\u662f\u6709\u6548\u6574\u5408\u591a\u6a21\u6001\u65f6\u5e8f\u4fe1\u606f\u7684\u7b80\u5355\u800c\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2602.01581", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01581", "abs": "https://arxiv.org/abs/2602.01581", "authors": ["Yao Zhao", "Kwang-Sung Jun"], "title": "Nearly Optimal Active Preference Learning and Its Application to LLM Alignment", "comment": null, "summary": "Aligning large language models (LLMs) depends on high-quality datasets of human preference labels, which are costly to collect. Although active learning has been studied to improve sample efficiency relative to passive collection, many existing approaches adopt classical experimental design criteria such as G- or D-optimality. These objectives are not tailored to the structure of preference learning, leaving open the design of problem-specific algorithms. In this work, we identify a simple intuition specific to preference learning that calls into question the suitability of these existing design objectives. Motivated by this insight, we propose two active learning algorithms. The first provides the first instance-dependent label complexity guarantee for this setting, and the second is a simple, practical greedy method. We evaluate our algorithm on real-world preference datasets and observe improved sample efficiency compared to existing methods.", "AI": {"tldr": "The paper proposes problem-specific active learning algorithms for LLM preference alignment, demonstrating improved sample efficiency over classical methods.", "motivation": "Aligning LLMs requires costly human preference labels; existing active learning methods use generic experimental design criteria (G-/D-optimality) unsuited for preference learning.", "method": "Identify preference learning-specific intuition, propose two active learning algorithms: one with instance-dependent label complexity guarantee, another as a practical greedy method.", "result": "Empirical evaluation on real-world preference datasets shows improved sample efficiency compared to existing methods.", "conclusion": "Problem-specific active learning algorithms are more effective for preference learning than classical approaches, offering better label efficiency for LLM alignment."}}
{"id": "2602.01585", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01585", "abs": "https://arxiv.org/abs/2602.01585", "authors": ["Xu Zhang", "Qitong Wang", "Peng Wang", "Wei Wang"], "title": "A Lightweight Sparse Interaction Network for Time Series Forecasting", "comment": "The paper is published in AAAI Conference on Artificial Intelligence, AAAI 2025. The code is available at the link https://github.com/Meteor-Stars/LSINet", "summary": "Recent work shows that linear models can outperform several transformer models in long-term time-series forecasting (TSF). However, instead of explicitly performing temporal interaction through self-attention, linear models implicitly perform it based on stacked MLP structures, which may be insufficient in capturing the complex temporal dependencies and their performance still has potential for improvement. To this end, we propose a Lightweight Sparse Interaction Network (LSINet) for TSF task. Inspired by the sparsity of self-attention, we propose a Multihead Sparse Interaction Mechanism (MSIM). Different from self-attention, MSIM learns the important connections between time steps through sparsity-induced Bernoulli distribution to capture temporal dependencies for TSF. The sparsity is ensured by the proposed self-adaptive regularization loss. Moreover, we observe the shareability of temporal interactions and propose to perform Shared Interaction Learning (SIL) for MSIM to further enhance efficiency and improve convergence. LSINet is a linear model comprising only MLP structures with low overhead and equipped with explicit temporal interaction mechanisms. Extensive experiments on public datasets show that LSINet achieves both higher accuracy and better efficiency than advanced linear models and transformer models in TSF tasks. The code is available at the link https://github.com/Meteor-Stars/LSINet.", "AI": {"tldr": "\u63d0\u51faLSINet\uff0c\u4e00\u4e2a\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u8f7b\u91cf\u7ea7\u7ebf\u6027\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u5934\u7a00\u758f\u4ea4\u4e92\u673a\u5236\u548c\u5171\u4eab\u4ea4\u4e92\u5b66\u4e60\u663e\u5f0f\u6355\u83b7\u65f6\u95f4\u4f9d\u8d56\uff0c\u5728\u51c6\u786e\u7387\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u5148\u8fdb\u7ebf\u6027\u548cTransformer\u6a21\u578b\u3002", "motivation": "\u7ebf\u6027\u6a21\u578b\u5728\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u867d\u4f18\u4e8eTransformer\uff0c\u4f46\u4f9d\u8d56\u5806\u53e0MLP\u9690\u5f0f\u8fdb\u884c\u65f6\u95f4\u4ea4\u4e92\uff0c\u80fd\u529b\u6709\u9650\u4e14\u6027\u80fd\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002\u9700\u4e3a\u7ebf\u6027\u6a21\u578b\u8bbe\u8ba1\u663e\u5f0f\u7684\u65f6\u95f4\u4ea4\u4e92\u673a\u5236\u3002", "method": "\u63d0\u51faLightweight Sparse Interaction Network (LSINet)\uff1a1) \u591a\u5934\u7a00\u758f\u4ea4\u4e92\u673a\u5236(MSIM)\uff0c\u901a\u8fc7\u7a00\u758f\u8bf1\u5bfc\u7684\u4f2f\u52aa\u5229\u5206\u5e03\u5b66\u4e60\u65f6\u95f4\u6b65\u95f4\u91cd\u8981\u8fde\u63a5\uff0c\u914d\u5408\u81ea\u9002\u5e94\u6b63\u5219\u5316\u635f\u5931\u4fdd\u8bc1\u7a00\u758f\u6027\uff1b2) \u5171\u4eab\u4ea4\u4e92\u5b66\u4e60(SIL)\uff0c\u5229\u7528\u65f6\u95f4\u4ea4\u4e92\u7684\u53ef\u5171\u4eab\u6027\u63d0\u5347\u6548\u7387\u548c\u6536\u655b\u6027\u3002\u6a21\u578b\u4ec5\u542bMLP\u7ed3\u6784\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cLSINet\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u4e2d\u6bd4\u5148\u8fdb\u7ebf\u6027\u6a21\u578b\u548cTransformer\u6a21\u578b\u540c\u65f6\u83b7\u5f97\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u548c\u66f4\u597d\u7684\u6548\u7387\u3002", "conclusion": "LSINet\u6210\u529f\u5c06\u663e\u5f0f\u65f6\u95f4\u4ea4\u4e92\u673a\u5236\u878d\u5165\u9ad8\u6548\u7ebf\u6027\u67b6\u6784\uff0c\u4e3a\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u517c\u5177\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6548\u7387\u7684\u65b0\u65b9\u6848\u3002"}}
{"id": "2602.01601", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01601", "abs": "https://arxiv.org/abs/2602.01601", "authors": ["Hieu Trung Nguyen", "Bao Nguyen", "Wenao Ma", "Yuzhi Zhao", "Ruifeng She", "Viet Anh Nguyen"], "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards", "comment": "Accepted at ICLR 2026", "summary": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \\Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \\Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \\Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at https://github.com/HieuNT91/VIP.", "AI": {"tldr": "VIP\u662f\u4e00\u79cd\u65b9\u5dee\u4fe1\u606f\u9884\u6d4b\u5206\u914d\u7b56\u7565\uff0c\u901a\u8fc7\u9ad8\u65af\u8fc7\u7a0b\u6a21\u578b\u9884\u6d4b\u6bcf\u4e2a\u63d0\u793a\u7684\u6210\u529f\u6982\u7387\u5e76\u4f18\u5316rollout\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u5728\u53ef\u9a8c\u8bc1\u5956\u52b1\u573a\u666f\u4e0b\u7684\u91c7\u6837\u6548\u7387", "motivation": "\u5f3a\u5316\u5b66\u4e60\u4e2d\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u91c7\u6837\u6548\u7387\u662f\u5173\u952e\u74f6\u9888\u3002\u73b0\u6709GRPO\u7b49\u57fa\u4e8e\u7ec4\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u5bf9\u6240\u6709\u8bad\u7ec3\u63d0\u793a\u91c7\u7528\u56fa\u5b9a\u6570\u91cf\u7684rollout\u5206\u914d\uff0c\u9690\u542b\u5047\u8bbe\u6240\u6709\u63d0\u793a\u4fe1\u606f\u91cf\u76f8\u540c\uff0c\u5bfc\u81f4\u8ba1\u7b97\u9884\u7b97\u4f7f\u7528\u6548\u7387\u4f4e\u4e0b\u5e76\u963b\u788d\u8bad\u7ec3\u8fdb\u5ea6\u3002", "method": "\u5728\u6bcf\u4e2a\u8fed\u4ee3\u4e2d\uff0cVIP\u4f7f\u7528\u8f7b\u91cf\u7ea7\u9ad8\u65af\u8fc7\u7a0b\u6a21\u578b\u57fa\u4e8e\u8fd1\u671frollout\u9884\u6d4b\u6bcf\u4e2a\u63d0\u793a\u7684\u6210\u529f\u6982\u7387\u3002\u5c06\u8fd9\u4e9b\u6982\u7387\u8f6c\u5316\u4e3a\u65b9\u5dee\u4f30\u8ba1\uff0c\u7136\u540e\u901a\u8fc7\u51f8\u4f18\u5316\u95ee\u9898\u5728\u4e25\u683c\u7684\u8ba1\u7b97\u9884\u7b97\u7ea6\u675f\u4e0b\u786e\u5b9a\u6700\u4f18\u7684rollout\u5206\u914d\u65b9\u6848\uff0c\u4ee5\u6700\u5c0f\u5316\u7b56\u7565\u66f4\u65b0\u7684\u9884\u671f\u68af\u5ea6\u65b9\u5dee\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cVIP\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u5747\u5300\u6216\u542f\u53d1\u5f0f\u5206\u914d\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u91c7\u6837\u6548\u7387\u5e76\u5b9e\u73b0\u66f4\u9ad8\u6027\u80fd\u3002", "conclusion": "VIP\u901a\u8fc7\u65b9\u5dee\u611f\u77e5\u7684\u667a\u80fd\u9884\u7b97\u5206\u914d\u6709\u6548\u89e3\u51b3\u4e86\u53ef\u9a8c\u8bc1\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u7684\u91c7\u6837\u6548\u7387\u74f6\u9888\uff0c\u4e3a\u8ba1\u7b97\u8d44\u6e90\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2602.01606", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01606", "abs": "https://arxiv.org/abs/2602.01606", "authors": ["Zeqiao Li", "Yijing Wang", "Haoyu Wang", "Zheng Li", "Zhiqiang Zuo"], "title": "Boosting Maximum Entropy Reinforcement Learning via One-Step Flow Matching", "comment": null, "summary": "Diffusion policies are expressive yet incur high inference latency. Flow Matching (FM) enables one-step generation, but integrating it into Maximum Entropy Reinforcement Learning (MaxEnt RL) is challenging: the optimal policy is an intractable energy-based distribution, and the efficient log-likelihood estimation required to balance exploration and exploitation suffers from severe discretization bias. We propose \\textbf{F}low-based \\textbf{L}og-likelihood-\\textbf{A}ware \\textbf{M}aximum \\textbf{E}ntropy RL (\\textbf{FLAME}), a principled framework that addresses these challenges. First, we derive a Q-Reweighted FM objective that bypasses partition function estimation via importance reweighting. Second, we design a decoupled entropy estimator that rigorously corrects bias, which enables efficient exploration and brings the policy closer to the optimal MaxEnt policy. Third, we integrate the MeanFlow formulation to achieve expressive and efficient one-step control. Empirical results on MuJoCo show that FLAME outperforms Gaussian baselines and matches multi-step diffusion policies with significantly lower inference cost. Code is available at https://github.com/lzqw/FLAME.", "AI": {"tldr": "\u63d0\u51faFLAME\u6846\u67b6\uff0c\u5c06\u6d41\u5339\u914d\u4e0e\u6700\u5927\u71b5\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\uff0c\u901a\u8fc7Q\u91cd\u52a0\u6743\u76ee\u6807\u907f\u514d\u914d\u5206\u51fd\u6570\u4f30\u8ba1\uff0c\u89e3\u8026\u71b5\u4f30\u8ba1\u5668\u7ea0\u6b63\u79bb\u6563\u5316\u504f\u5dee\uff0c\u5b9e\u73b0\u5355\u6b65\u751f\u6210\u5e76\u663e\u8457\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "\u6269\u6563\u7b56\u7565\u8868\u8fbe\u80fd\u529b\u5f3a\u4f46\u63a8\u7406\u5ef6\u8fdf\u9ad8\uff1b\u6d41\u5339\u914d\u652f\u6301\u5355\u6b65\u751f\u6210\uff0c\u4f46\u4e0e\u6700\u5927\u71b5\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u65f6\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u6700\u4f18\u7b56\u7565\u662f\u96be\u89e3\u7684\u80fd\u91cf\u57fa\u5206\u5e03\uff0c\u4e14\u5e73\u8861\u63a2\u7d22\u4e0e\u5f00\u53d1\u7684\u5bf9\u6570\u4f3c\u7136\u4f30\u8ba1\u5b58\u5728\u4e25\u91cd\u79bb\u6563\u5316\u504f\u5dee\u3002", "method": "1. \u63a8\u5bfcQ\u91cd\u52a0\u6743\u6d41\u5339\u914d\u76ee\u6807\uff0c\u901a\u8fc7\u91cd\u8981\u6027\u91cd\u52a0\u6743\u7ed5\u8fc7\u914d\u5206\u51fd\u6570\u4f30\u8ba1\uff1b2. \u8bbe\u8ba1\u89e3\u8026\u71b5\u4f30\u8ba1\u5668\uff0c\u4e25\u683c\u7ea0\u6b63\u504f\u5dee\u4ee5\u5b9e\u73b0\u9ad8\u6548\u63a2\u7d22\uff1b3. \u96c6\u6210MeanFlow\u516c\u5f0f\u5b9e\u73b0 expressive \u4e14\u9ad8\u6548\u7684\u5355\u6b65\u63a7\u5236\u3002", "result": "\u5728MuJoCo\u4e0a\uff0cFLAME\u4f18\u4e8e\u9ad8\u65af\u57fa\u7ebf\uff0c\u6027\u80fd\u5ab2\u7f8e\u591a\u6b65\u6269\u6563\u7b56\u7565\uff0c\u4f46\u63a8\u7406\u6210\u672c\u663e\u8457\u66f4\u4f4e\u3002", "conclusion": "FLAME\u6210\u529f\u89e3\u51b3\u4e86\u6d41\u5339\u914d\u4e0e\u6700\u5927\u71b5\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u8868\u8fbe\u80fd\u529b\u5f3a\u4e14\u63a8\u7406\u9ad8\u6548\u7684\u5355\u6b65\u7b56\u7565\uff0c\u4e3a\u5b9e\u65f6\u63a7\u5236\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.01613", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01613", "abs": "https://arxiv.org/abs/2602.01613", "authors": ["Sergii Kozyrev", "Davyd Maiboroda"], "title": "A Practical Tensor-Network Compression Pipeline for Production-Scale Large Language Models", "comment": "13 pages, 5 figures", "summary": "Large language models are limited in deployment by GPU memory and inference latency. We present Minima, a production compression pipeline that learns where and how to structurally compress a Transformer and turns that compression into real serving gains. Minima trains a lightweight convolutional predictor to estimate layer- and patch-level sensitivity, applies a mixture of Tucker, tensor-train, and tensor-ring decompositions to low-sensitivity regions, performs a short healing fine-tune, and executes the resulting operators with custom Triton and CUDA kernels. The reduced memory footprint enables speculative decoding with a small draft model and a larger verifier. On Qwen3-32B at an 8k-token context window, Minima reduces peak VRAM from 64 GiB to 40 GiB. For a single active request, throughput increases from 40 tokens per second (baseline) to 50 tokens per second (Minima) and 75 tokens per second (Minima with speculative decoding). Under 50 parallel requests, throughput is 34, 44, and 53 tokens per second respectively, showing that Minima remains effective under high concurrency even when speculative decoding gains compress. We position Minima relative to recent tensor-network, low-rank plus quantization, and cross-layer sharing methods, and argue that it is a practical step toward more aggressive structural compression via shared tensor backbones with tiny per-layer adapters.", "AI": {"tldr": "Minima compresses LLMs via structural decomposition guided by sensitivity prediction, reducing VRAM by 37.5% and boosting throughput by 25-87.5% with speculative decoding.", "motivation": "Large language models face deployment constraints due to high GPU memory requirements and inference latency, limiting practical serving efficiency.", "method": "Trains a lightweight convolutional predictor to identify layer- and patch-level sensitivity, applies hybrid tensor decompositions (Tucker, tensor-train, tensor-ring) to low-sensitivity regions, performs healing fine-tune, and executes with custom Triton/CUDA kernels while enabling speculative decoding.", "result": "On Qwen3-32B with 8k context: peak VRAM reduced from 64 GiB to 40 GiB; single-request throughput increased from 40 to 50 tokens/sec (25% gain) and 75 tokens/sec with speculative decoding (87.5% total gain); under 50 concurrent requests, throughput improved from 34 to 44 tokens/sec (29% gain) and 53 tokens/sec with speculative decoding.", "conclusion": "Minima demonstrates practical structural compression that remains effective under high concurrency, positioning it as a viable step toward more aggressive compression via shared tensor backbones with minimal per-layer adapters."}}
{"id": "2602.01614", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01614", "abs": "https://arxiv.org/abs/2602.01614", "authors": ["Qi Cheng", "Licheng Liu", "Yao Zhang", "Mu Hong", "Yiqun Xie", "Xiaowei Jia"], "title": "AgroFlux: A Spatial-Temporal Benchmark for Carbon and Nitrogen Flux Prediction in Agricultural Ecosystems", "comment": null, "summary": "Agroecosystem, which heavily influenced by human actions and accounts for a quarter of global greenhouse gas emissions (GHGs), plays a crucial role in mitigating global climate change and securing environmental sustainability. However, we can't manage what we can't measure. Accurately quantifying the pools and fluxes in the carbon, nutrient, and water nexus of the agroecosystem is therefore essential for understanding the underlying drivers of GHG and developing effective mitigation strategies. Conventional approaches like soil sampling, process-based models, and black-box machine learning models are facing challenges such as data sparsity, high spatiotemporal heterogeneity, and complex subsurface biogeochemical and physical processes. Developing new trustworthy approaches such as AI-empowered models, will require the AI-ready benchmark dataset and outlined protocols, which unfortunately do not exist. In this work, we introduce a first-of-its-kind spatial-temporal agroecosystem GHG benchmark dataset that integrates physics-based model simulations from Ecosys and DayCent with real-world observations from eddy covariance flux towers and controlled-environment facilities. We evaluate the performance of various sequential deep learning models on carbon and nitrogen flux prediction, including LSTM-based models, temporal CNN-based model, and Transformer-based models. Furthermore, we explored transfer learning to leverage simulated data to improve the generalization of deep learning models on real-world observations. Our benchmark dataset and evaluation framework contribute to the development of more accurate and scalable AI-driven agroecosystem models, advancing our understanding of ecosystem-climate interactions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u63d0\u51fa\u4e86\u4e00\u4e2a\u878d\u5408\u7269\u7406\u6a21\u578b\u4e0e\u771f\u5b9e\u89c2\u6d4b\u7684\u65f6\u7a7a\u519c\u4e1a\u751f\u6001\u7cfb\u7edf\u6e29\u5ba4\u6c14\u4f53\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u78b3\u6c2e\u901a\u91cf\u9884\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u5f00\u53d1AI\u9a71\u52a8\u7684\u519c\u4e1a\u751f\u6001\u7cfb\u7edf\u6a21\u578b\u63d0\u4f9b\u4e86\u57fa\u51c6\u6846\u67b6\u3002", "motivation": "\u519c\u4e1a\u751f\u6001\u7cfb\u7edf\u5360\u5168\u7403\u6e29\u5ba4\u6c14\u4f53\u6392\u653e\u7684\u56db\u5206\u4e4b\u4e00\uff0c\u5bf9\u51cf\u7f13\u6c14\u5019\u53d8\u5316\u81f3\u5173\u91cd\u8981\u3002\u4f46\u4f20\u7edf\u6d4b\u91cf\u65b9\u6cd5\uff08\u571f\u58e4\u91c7\u6837\u3001\u8fc7\u7a0b\u6a21\u578b\u3001\u9ed1\u76d2\u673a\u5668\u5b66\u4e60\uff09\u9762\u4e34\u6570\u636e\u7a00\u758f\u3001\u65f6\u7a7a\u5f02\u8d28\u6027\u5927\u548c\u5730\u4e0b\u751f\u7269\u5730\u7403\u5316\u5b66\u8fc7\u7a0b\u590d\u6742\u7b49\u6311\u6218\uff0c\u4e14\u7f3a\u4e4fAI\u5c31\u7eea\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u65e0\u6cd5\u5f00\u53d1\u53ef\u4fe1\u7684AI\u6a21\u578b\u3002", "method": "\u6574\u5408Ecosys\u548cDayCent\u7269\u7406\u6a21\u578b\u6a21\u62df\u6570\u636e\u4e0e\u6da1\u5ea6\u534f\u901a\u91cf\u5854\u3001\u63a7\u5236\u73af\u5883\u8bbe\u65bd\u7684\u5b9e\u6d4b\u89c2\u6d4b\u6570\u636e\uff0c\u6784\u5efa\u9996\u4e2a\u519c\u4e1a\u751f\u6001\u7cfb\u7edf\u6e29\u5ba4\u6c14\u4f53\u57fa\u51c6\u6570\u636e\u96c6\u3002\u8bc4\u4f30LSTM\u3001\u65f6\u5e8fCNN\u548cTransformer\u7b49\u5e8f\u5217\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u78b3\u6c2e\u901a\u91cf\u9884\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u63a2\u7d22\u8fc1\u79fb\u5b66\u4e60\u4ee5\u63d0\u5347\u6a21\u578b\u5728\u771f\u5b9e\u89c2\u6d4b\u6570\u636e\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u6210\u529f\u521b\u5efa\u4e86\u9996\u4e2aAI\u5c31\u7eea\u7684\u519c\u4e1a\u751f\u6001\u7cfb\u7edf\u6e29\u5ba4\u6c14\u4f53\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5efa\u7acb\u4e86\u6a21\u578b\u8bc4\u4f30\u6846\u67b6\uff0c\u9a8c\u8bc1\u4e86\u8fc1\u79fb\u5b66\u4e60\u5728\u5229\u7528\u6a21\u62df\u6570\u636e\u6539\u8fdb\u771f\u5b9e\u89c2\u6d4b\u9884\u6d4b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u57fa\u51c6\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\u5c06\u63a8\u52a8\u5f00\u53d1\u66f4\u51c6\u786e\u3001\u53ef\u6269\u5c55\u7684AI\u9a71\u52a8\u519c\u4e1a\u751f\u6001\u7cfb\u7edf\u6a21\u578b\uff0c\u6df1\u5316\u5bf9\u751f\u6001\u7cfb\u7edf-\u6c14\u5019\u76f8\u4e92\u4f5c\u7528\u7684\u7406\u89e3\uff0c\u4e3a\u5168\u7403\u6c14\u5019\u53d8\u5316\u51cf\u7f13\u7b56\u7565\u63d0\u4f9b\u79d1\u5b66\u652f\u6491\u3002"}}
{"id": "2602.01605", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01605", "abs": "https://arxiv.org/abs/2602.01605", "authors": ["Anthony Bao", "Venkata Hasith Vattikuti", "Jeffrey Lai", "William Gilpin"], "title": "Universal Redundancies in Time Series Foundation Models", "comment": null, "summary": "Time Series Foundation Models (TSFMs) leverage extensive pretraining to accurately predict unseen time series during inference, without the need for task-specific fine-tuning. Through large-scale evaluations on standard benchmarks, we find that leading transformer-based TSFMs exhibit redundant components in their intermediate layers. We introduce a set of tools for mechanistic interpretability of TSFMs, including ablations of specific components and direct logit attribution on the residual stream. Our findings are consistent across several leading TSFMs with diverse architectures, and across a diverse set of real-world and synthetic time-series datasets. We discover that all models in our study are robust to ablations of entire layers. Furthermore, we develop a theoretical framework framing transformers as kernel regressors, motivating a purely intrinsic strategy for ablating heads based on the stable rank of the per-head projection matrices. Using this approach, we uncover the specific heads responsible for degenerate phenomena widely observed in TSFMs, such as parroting of motifs from the context and seasonality bias. Our study sheds light on the universal properties of this emerging class of architectures for continuous-time sequence modeling.", "AI": {"tldr": "\u4e00\u7bc7\u5173\u4e8e\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u7684\u8bba\u6587\uff0c\u901a\u8fc7\u673a\u5236\u53ef\u89e3\u91ca\u6027\u5206\u6790\u548c\u6838\u56de\u5f52\u7406\u8bba\u6846\u67b6\uff0c\u53d1\u73b0\u8fd9\u4e9b\u6a21\u578b\u5b58\u5728\u5197\u4f59\u7ec4\u4ef6\uff0c\u5e76\u8bc6\u522b\u51fa\u5bfc\u81f4\u91cd\u590d\u6a21\u4eff\u548c\u5b63\u8282\u6027\u504f\u5dee\u7b49\u9000\u5316\u73b0\u8c61\u7684\u7279\u5b9a\u6ce8\u610f\u529b\u5934\u3002", "motivation": "\u7406\u89e3\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u7684\u5de5\u4f5c\u539f\u7406\uff0c\u8bc6\u522b\u5176\u4e2d\u7684\u5197\u4f59\u7ec4\u4ef6\uff0c\u5e76\u89e3\u51b3\u6a21\u578b\u4e2d\u666e\u904d\u5b58\u5728\u7684\u9000\u5316\u73b0\u8c61\uff08\u5982\u4e0a\u4e0b\u6587\u91cd\u590d\u6a21\u4eff\u548c\u5b63\u8282\u6027\u504f\u5dee\uff09\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u901a\u8fc7\u5bf9\u6807\u51c6\u57fa\u51c6\u8fdb\u884c\u5927\u89c4\u6a21\u8bc4\u4f30\uff0c\u5f00\u53d1\u673a\u5236\u53ef\u89e3\u91ca\u6027\u5de5\u5177\uff08\u5305\u62ec\u7ec4\u4ef6\u6d88\u878d\u548c\u6b8b\u5dee\u6d41\u76f4\u63a5logit\u5f52\u56e0\uff09\uff0c\u5e76\u5efa\u7acb\u5c06Transformer\u5efa\u6a21\u4e3a\u6838\u56de\u5f52\u5668\u7684\u7406\u8bba\u6846\u67b6\uff0c\u57fa\u4e8e\u6bcf\u5934\u6295\u5f71\u77e9\u9635\u7684\u7a33\u5b9a\u79e9\u6765\u6307\u5bfc\u6ce8\u610f\u529b\u5934\u7684\u6d88\u878d\u7b56\u7565\u3002", "result": "\u53d1\u73b0\u591a\u4e2a\u9886\u5148\u7684Transformer\u67b6\u6784\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5728\u4e2d\u95f4\u5c42\u5b58\u5728\u5197\u4f59\u7ec4\u4ef6\uff0c\u6a21\u578b\u5bf9\u6574\u5c42\u6d88\u878d\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5e76\u6210\u529f\u8bc6\u522b\u51fa\u8d1f\u8d23\u91cd\u590d\u6a21\u4eff\u548c\u5b63\u8282\u6027\u504f\u5dee\u7b49\u9000\u5316\u73b0\u8c61\u7684\u5177\u4f53\u6ce8\u610f\u529b\u5934\uff0c\u8fd9\u4e9b\u53d1\u73b0\u8de8\u67b6\u6784\u548c\u6570\u636e\u96c6\u4fdd\u6301\u4e00\u81f4\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u67b6\u6784\u7684\u666e\u9002\u6027\u7279\u6027\uff0c\u4e3a\u8fde\u7eed\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u89c1\u89e3\u548c\u5b9e\u8df5\u5de5\u5177\u3002"}}
{"id": "2602.01619", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01619", "abs": "https://arxiv.org/abs/2602.01619", "authors": ["Seyed Mohammad Hadi Hosseini", "Mahdieh Soleymani Baghshah"], "title": "SUSD: Structured Unsupervised Skill Discovery through State Factorization", "comment": "Accepted as a conference paper at ICLR 2026", "summary": "Unsupervised Skill Discovery (USD) aims to autonomously learn a diverse set of skills without relying on extrinsic rewards. One of the most common USD approaches is to maximize the Mutual Information (MI) between skill latent variables and states. However, MI-based methods tend to favor simple, static skills due to their invariance properties, limiting the discovery of dynamic, task-relevant behaviors. Distance-Maximizing Skill Discovery (DSD) promotes more dynamic skills by leveraging state-space distances, yet still fall short in encouraging comprehensive skill sets that engage all controllable factors or entities in the environment. In this work, we introduce SUSD, a novel framework that harnesses the compositional structure of environments by factorizing the state space into independent components (e.g., objects or controllable entities). SUSD allocates distinct skill variables to different factors, enabling more fine-grained control on the skill discovery process. A dynamic model also tracks learning across factors, adaptively steering the agent's focus toward underexplored factors. This structured approach not only promotes the discovery of richer and more diverse skills, but also yields a factorized skill representation that enables fine-grained and disentangled control over individual entities which facilitates efficient training of compositional downstream tasks via Hierarchical Reinforcement Learning (HRL). Our experimental results across three environments, with factors ranging from 1 to 10, demonstrate that our method can discover diverse and complex skills without supervision, significantly outperforming existing unsupervised skill discovery methods in factorized and complex environments. Code is publicly available at: https://github.com/hadi-hosseini/SUSD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSUSD\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u72b6\u6001\u7a7a\u95f4\u5206\u89e3\u4e3a\u72ec\u7acb\u7ec4\u4ef6\u5e76\u4e3a\u6bcf\u4e2a\u7ec4\u4ef6\u5206\u914d\u72ec\u7acb\u7684\u6280\u80fd\u53d8\u91cf\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65e0\u76d1\u7763\u6280\u80fd\u53d1\u73b0\u65b9\u6cd5\u3002", "motivation": "\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u65e0\u76d1\u7763\u6280\u80fd\u53d1\u73b0\u65b9\u6cd5\u56e0\u4e0d\u53d8\u6027\u7279\u6027\u503e\u5411\u4e8e\u5b66\u4e60\u7b80\u5355\u9759\u6001\u6280\u80fd\uff0c\u800c\u8ddd\u79bb\u6700\u5927\u5316\u65b9\u6cd5\u4ecd\u65e0\u6cd5\u5168\u9762\u63a2\u7d22\u73af\u5883\u4e2d\u7684\u6240\u6709\u53ef\u63a7\u56e0\u7d20\uff0c\u9700\u8981\u80fd\u591f\u53d1\u73b0\u591a\u6837\u5316\u52a8\u6001\u6280\u80fd\u7684\u65b9\u6cd5\u3002", "method": "SUSD\u5c06\u72b6\u6001\u7a7a\u95f4\u5206\u89e3\u4e3a\u72ec\u7acb\u7ec4\u4ef6\uff08\u5982\u7269\u4f53\u6216\u53ef\u63a7\u5b9e\u4f53\uff09\uff0c\u4e3a\u4e0d\u540c\u56e0\u7d20\u5206\u914d\u4e0d\u540c\u7684\u6280\u80fd\u53d8\u91cf\uff0c\u5e76\u4f7f\u7528\u52a8\u6001\u6a21\u578b\u8ddf\u8e2a\u5404\u56e0\u7d20\u7684\u5b66\u4e60\u8fdb\u5ea6\uff0c\u81ea\u9002\u5e94\u5730\u5c06\u667a\u80fd\u4f53\u6ce8\u610f\u529b\u5f15\u5bfc\u81f3\u63a2\u7d22\u4e0d\u8db3\u7684\u56e0\u7d20\u3002", "result": "\u5728\u4e09\u4e2a\u5305\u542b1-10\u4e2a\u56e0\u7d20\u7684\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSUSD\u80fd\u591f\u65e0\u76d1\u7763\u5730\u53d1\u73b0\u591a\u6837\u5316\u548c\u590d\u6742\u7684\u6280\u80fd\uff0c\u5728\u56e0\u5b50\u5316\u548c\u590d\u6742\u73af\u5883\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7ed3\u6784\u5316\u65b9\u6cd5\u4e0d\u4ec5\u4fc3\u8fdb\u4e86\u66f4\u4e30\u5bcc\u591a\u6837\u7684\u6280\u80fd\u53d1\u73b0\uff0c\u8fd8\u4ea7\u751f\u4e86\u53ef\u7528\u4e8e\u7ec6\u7c92\u5ea6\u3001\u89e3\u8026\u63a7\u5236\u7684\u56e0\u5b50\u5316\u6280\u80fd\u8868\u793a\uff0c\u4ece\u800c\u901a\u8fc7\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u9ad8\u6548\u8bad\u7ec3\u7ec4\u5408\u5f0f\u4e0b\u6e38\u4efb\u52a1\u3002"}}
{"id": "2602.01626", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01626", "abs": "https://arxiv.org/abs/2602.01626", "authors": ["Mehdi Setayesh", "Mahdi Beitollahi", "Yasser H. Khalil", "Hongliang Li"], "title": "Toward Enhancing Representation Learning in Federated Multi-Task Settings", "comment": "This paper has been accepted at ICLR 2026", "summary": "Federated multi-task learning (FMTL) seeks to collaboratively train customized models for users with different tasks while preserving data privacy. Most existing approaches assume model congruity (i.e., the use of fully or partially homogeneous models) across users, which limits their applicability in realistic settings. To overcome this limitation, we aim to learn a shared representation space across tasks rather than shared model parameters. To this end, we propose Muscle loss, a novel contrastive learning objective that simultaneously aligns representations from all participating models. Unlike existing multi-view or multi-model contrastive methods, which typically align models pairwise, Muscle loss can effectively capture dependencies across tasks because its minimization is equivalent to the maximization of mutual information among all the models' representations. Building on this principle, we develop FedMuscle, a practical and communication-efficient FMTL algorithm that naturally handles both model and task heterogeneity. Experiments on diverse image and language tasks demonstrate that FedMuscle consistently outperforms state-of-the-art baselines, delivering substantial improvements and robust performance across heterogeneous settings.", "AI": {"tldr": "\u9488\u5bf9\u8054\u90a6\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u6a21\u578b\u4e00\u81f4\u6027\u5047\u8bbe\u9650\u5236\u5b9e\u9645\u5e94\u7528\u7684\u95ee\u9898\uff0c\u63d0\u51faFedMuscle\u7b97\u6cd5\uff0c\u901a\u8fc7Muscle\u635f\u5931\u51fd\u6570\u5728\u5171\u4eab\u8868\u793a\u7a7a\u95f4\u5bf9\u9f50\u6240\u6709\u6a21\u578b\uff0c\u5b9e\u73b0\u6a21\u578b\u4e0e\u4efb\u52a1\u5f02\u6784\u6027\u5904\u7406\uff0c\u5728\u56fe\u50cf\u548c\u8bed\u8a00\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\u5047\u8bbe\u7528\u6237\u95f4\u6a21\u578b\u7ed3\u6784\u4e00\u81f4\uff08\u5b8c\u5168\u6216\u90e8\u5206\u540c\u8d28\uff09\uff0c\u8fd9\u5728\u73b0\u5b9e\u5f02\u6784\u73af\u5883\u4e2d\u9002\u7528\u6027\u6709\u9650\u3002\u9700\u8981\u7a81\u7834\u5171\u4eab\u6a21\u578b\u53c2\u6570\u7684\u9650\u5236\uff0c\u5f00\u53d1\u80fd\u5904\u7406\u4efb\u52a1\u548c\u6a21\u578b\u5f02\u6784\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMuscle\u635f\u5931\u2014\u2014\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\uff0c\u80fd\u540c\u65f6\u5bf9\u9f50\u6240\u6709\u53c2\u4e0e\u6a21\u578b\u7684\u8868\u793a\uff08\u800c\u975e\u6210\u5bf9\u5bf9\u9f50\uff09\uff0c\u5176\u6700\u5c0f\u5316\u7b49\u4ef7\u4e8e\u6700\u5927\u5316\u6a21\u578b\u95f4\u4e92\u4fe1\u606f\u3002\u57fa\u4e8e\u6b64\u5f00\u53d1FedMuscle\u7b97\u6cd5\uff0c\u901a\u8fc7\u5171\u4eab\u8868\u793a\u7a7a\u95f4\u800c\u975e\u5171\u4eab\u53c2\u6570\u5b9e\u73b0\u534f\u4f5c\u5b66\u4e60\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u56fe\u50cf\u548c\u8bed\u8a00\u4efb\u52a1\u4e0a\uff0cFedMuscle\u6301\u7eed\u4f18\u4e8eSOTA\u57fa\u7ebf\uff0c\u5728\u5f02\u6784\u73af\u5883\u4e2d\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\u548c\u9c81\u68d2\u8868\u73b0\uff0c\u9a8c\u8bc1\u4e86\u5904\u7406\u6a21\u578b\u4e0e\u4efb\u52a1\u5f02\u6784\u6027\u7684\u6709\u6548\u6027\u3002", "conclusion": "FedMuscle\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u901a\u4fe1\u9ad8\u6548\u7684\u8054\u90a6\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6848\uff0c\u901a\u8fc7\u5171\u4eab\u8868\u793a\u7a7a\u95f4\u6210\u529f\u89e3\u51b3\u4e86\u6a21\u578b\u4e0e\u4efb\u52a1\u5f02\u6784\u6027\u95ee\u9898\uff0c\u4e3a\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u534f\u4f5c\u5b66\u4e60\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.01611", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01611", "abs": "https://arxiv.org/abs/2602.01611", "authors": ["Weizheng Gu", "Chengze Li", "Zhuohao Yu", "Mengyuan Sun", "Zhibang Yang", "Wei Wang", "Hongrui Jia", "Shikun Zhang", "Wei Ye"], "title": "What Do Agents Learn from Trajectory-SFT: Semantics or Interfaces?", "comment": null, "summary": "Large language models are increasingly evaluated as interactive agents, yet standard agent benchmarks conflate two qualitatively distinct sources of success: semantic tool-use and interface-specific interaction pattern memorization. Because both mechanisms can yield identical task success on the original interface, benchmark scores alone are not identifiable evidence of environment-invariant capability. We propose PIPE, a protocol-level evaluation augmentation for diagnosing interface reliance by minimally rewriting environment interfaces while preserving task semantics and execution behavior. Across 16 environments from AgentBench and AgentGym and a range of open-source and API-based agents, PIPE reveals that trajectory-SFT substantially amplifies interface shortcutting: trained agents degrade sharply under minimal interface rewrites, while non-trajectory-trained models remain largely stable. We further introduce Interface Reliance (IR), a counterbalanced alias-based metric that quantifies preference for training-time interfaces, and show that interface shortcutting exhibits environment-dependent, non-monotonic training dynamics that remain invisible under standard evaluation. Our code is available at https://anonymous.4open.science/r/What-Do-Agents-Learn-from-Trajectory-SFT-Semantics-or-Interfaces--0831/.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u667a\u80fd\u4f53\u8bc4\u4f30\u65f6\uff0c\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u533a\u5206\u8bed\u4e49\u5de5\u5177\u4f7f\u7528\u548c\u754c\u9762\u6a21\u5f0f\u8bb0\u5fc6\u3002\u4f5c\u8005\u63d0\u51faPIPE\u534f\u8bae\u7ea7\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u91cd\u5199\u754c\u9762\u6765\u8bca\u65ad\u754c\u9762\u4f9d\u8d56\uff0c\u53d1\u73b0\u8f68\u8ff9\u76d1\u7763\u5fae\u8c03\u4f1a\u663e\u8457\u52a0\u5267\u754c\u9762\u6377\u5f84\u5b66\u4e60\u3002", "motivation": "\u6807\u51c6\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u6df7\u6dc6\u4e86\u4e24\u79cd\u6210\u529f\u6765\u6e90\uff1a\u8bed\u4e49\u5de5\u5177\u4f7f\u7528\u548c\u754c\u9762\u7279\u5b9a\u4ea4\u4e92\u6a21\u5f0f\u8bb0\u5fc6\u3002\u7531\u4e8e\u4e24\u8005\u5728\u539f\u59cb\u754c\u9762\u4e0a\u90fd\u80fd\u83b7\u5f97\u76f8\u540c\u4efb\u52a1\u6210\u529f\u7387\uff0c\u57fa\u51c6\u5206\u6570\u4e0d\u80fd\u4f5c\u4e3a\u73af\u5883\u4e0d\u53d8\u80fd\u529b\u7684\u53ef\u8bc6\u522b\u8bc1\u636e\u3002", "method": "\u63d0\u51faPIPE\u534f\u8bae\u7ea7\u8bc4\u4f30\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u91cd\u5199\u73af\u5883\u754c\u9762\uff08\u4fdd\u6301\u4efb\u52a1\u8bed\u4e49\u548c\u6267\u884c\u884c\u4e3a\u4e0d\u53d8\uff09\u6765\u8bca\u65ad\u754c\u9762\u4f9d\u8d56\u3002\u5728AgentBench\u548cAgentGym\u768416\u4e2a\u73af\u5883\u4e2d\u6d4b\u8bd5\u5f00\u6e90\u548cAPI\u667a\u80fd\u4f53\uff0c\u5e76\u5f15\u5165Interface Reliance (IR)\u53cd\u5e73\u8861\u522b\u540d\u6307\u6807\u91cf\u5316\u8bad\u7ec3\u65f6\u754c\u9762\u504f\u597d\u3002", "result": "\u8f68\u8ff9\u76d1\u7763\u5fae\u8c03\u663e\u8457\u653e\u5927\u4e86\u754c\u9762\u6377\u5f84\u5b66\u4e60\uff1a\u8bad\u7ec3\u540e\u7684\u667a\u80fd\u4f53\u5728\u6700\u5c0f\u754c\u9762\u91cd\u5199\u4e0b\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u800c\u975e\u8f68\u8ff9\u8bad\u7ec3\u6a21\u578b\u4fdd\u6301\u7a33\u5b9a\u3002\u754c\u9762\u6377\u5f84\u5b66\u4e60\u8868\u73b0\u51fa\u73af\u5883\u4f9d\u8d56\u3001\u975e\u5355\u8c03\u7684\u8bad\u7ec3\u52a8\u6001\uff0c\u8fd9\u4e9b\u5728\u6807\u51c6\u8bc4\u4f30\u4e0b\u4e0d\u53ef\u89c1\u3002", "conclusion": "\u8f68\u8ff9\u76d1\u7763\u5fae\u8c03\u53ef\u80fd\u5bfc\u81f4\u667a\u80fd\u4f53\u8fc7\u5ea6\u4f9d\u8d56\u754c\u9762\u7279\u5b9a\u6a21\u5f0f\u800c\u975e\u5b66\u4e60\u8bed\u4e49\u5de5\u5177\u4f7f\u7528\uff0cPIPE\u548cIR\u6307\u6807\u80fd\u6709\u6548\u63ed\u793a\u8fd9\u79cd\u6377\u5f84\u5b66\u4e60\u884c\u4e3a\uff0c\u4e3a\u667a\u80fd\u4f53\u8bc4\u4f30\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u8bca\u65ad\u5de5\u5177\u3002"}}
{"id": "2602.01642", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01642", "abs": "https://arxiv.org/abs/2602.01642", "authors": ["Matias D. Cattaneo", "Boris Shigida"], "title": "The Effect of Mini-Batch Noise on the Implicit Bias of Adam", "comment": null, "summary": "With limited high-quality data and growing compute, multi-epoch training is gaining back its importance across sub-areas of deep learning. Adam(W), versions of which are go-to optimizers for many tasks such as next token prediction, has two momentum hyperparameters $(\u03b2_1, \u03b2_2)$ controlling memory and one very important hyperparameter, batch size, controlling (in particular) the amount mini-batch noise. We introduce a theoretical framework to understand how mini-batch noise influences the implicit bias of memory in Adam (depending on $\u03b2_1$, $\u03b2_2$) towards sharper or flatter regions of the loss landscape, which is commonly observed to correlate with the generalization gap in multi-epoch training. We find that in the case of large batch sizes, higher $\u03b2_2$ increases the magnitude of anti-regularization by memory (hurting generalization), but as the batch size becomes smaller, the dependence of (anti-)regulariation on $\u03b2_2$ is reversed. A similar monotonicity shift (in the opposite direction) happens in $\u03b2_1$. In particular, the commonly \"default\" pair $(\u03b2_1, \u03b2_2) = (0.9, 0.999)$ is a good choice if batches are small; for larger batches, in many settings moving $\u03b2_1$ closer to $\u03b2_2$ is much better in terms of validation accuracy in multi-epoch training. Moreover, our theoretical derivations connect the scale of the batch size at which the shift happens to the scale of the critical batch size. We illustrate this effect in experiments with small-scale data in the about-to-overfit regime.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ece\u7406\u8bba\u4e0a\u8bc1\u660e\uff0cAdam\u4f18\u5316\u5668\u7684\u6700\u4f73\u52a8\u91cf\u8d85\u53c2\u6570\uff08\u03b2\u2081, \u03b2\u2082\uff09\u4e25\u91cd\u4f9d\u8d56\u4e8e\u6279\u91cf\u5927\u5c0f\uff1a\u9ed8\u8ba4\u503c\uff080.9, 0.999\uff09\u9002\u7528\u4e8e\u5c0f\u6279\u91cf\uff0c\u4f46\u5bf9\u4e8e\u5927\u6279\u91cf\uff0c\u5c06\u03b2\u2081\u8c03\u6574\u5f97\u66f4\u63a5\u8fd1\u03b2\u2082\u80fd\u63d0\u5347\u591a\u8f6e\u8bad\u7ec3\u4e2d\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u9ad8\u8d28\u91cf\u6570\u636e\u6709\u9650\u800c\u8ba1\u7b97\u8d44\u6e90\u4e0d\u65ad\u589e\u957f\uff0c\u591a\u8f6e\u8bad\u7ec3\u6b63\u91cd\u65b0\u53d8\u5f97\u91cd\u8981\u3002Adam(W)\u7684\u52a8\u91cf\u8d85\u53c2\u6570\uff08\u03b2\u2081, \u03b2\u2082\uff09\u548c\u6279\u91cf\u5927\u5c0f\u5bf9\u8bad\u7ec3\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4f46\u5c0f\u6279\u91cf\u566a\u58f0\u5982\u4f55\u5f71\u54cdAdam\u5bf9\u5c16\u9510/\u5e73\u5766\u6781\u5c0f\u503c\u7684\u9690\u5f0f\u504f\u7f6e\u2014\u2014\u4ece\u800c\u5f71\u54cd\u6cdb\u5316\u2014\u2014\u4ecd\u4e0d\u6e05\u695a\u3002", "method": "\u4f5c\u8005\u5efa\u7acb\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u5206\u6790\u5c0f\u6279\u91cf\u566a\u58f0\u5982\u4f55\u4e0eAdam\u7684\u8bb0\u5fc6\u673a\u5236\uff08\u7531\u03b2\u2081, \u03b2\u2082\u63a7\u5236\uff09\u76f8\u4e92\u4f5c\u7528\uff0c\u4ea7\u751f\u5bf9\u635f\u5931\u51fd\u6570\u666f\u89c2\u4e2d\u5c16\u9510\u6216\u5e73\u5766\u533a\u57df\u7684\u9690\u5f0f\u504f\u7f6e\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u5355\u8c03\u6027\u8f6c\u53d8\uff1a\u5927\u6279\u91cf\u65f6\uff0c\u66f4\u9ad8\u7684\u03b2\u2082\u4f1a\u589e\u52a0\u53cd\u6b63\u5219\u5316\uff08\u635f\u5bb3\u6cdb\u5316\uff09\uff0c\u4f46\u5c0f\u6279\u91cf\u65f6\u8fd9\u79cd\u5173\u7cfb\u53cd\u8f6c\u3002\u7c7b\u4f3c\u5730\uff0c\u03b2\u2081\u4e5f\u5448\u73b0\u76f8\u53cd\u65b9\u5411\u7684\u8f6c\u53d8\u3002\u9ed8\u8ba4\u503c\uff080.9, 0.999\uff09\u5bf9\u5c0f\u6279\u91cf\u6700\u4f18\uff0c\u800c\u5927\u6279\u91cf\u5219\u53d7\u76ca\u4e8e\u03b2\u2081\u66f4\u63a5\u8fd1\u03b2\u2082\u3002\u8f6c\u53d8\u53d1\u751f\u7684\u6279\u91cf\u5927\u5c0f\u4e0e\u4e34\u754c\u6279\u91cf\u5927\u5c0f\u76f8\u5173\uff0c\u8fd9\u5728\u8fc7\u62df\u5408regime\u7684\u5b9e\u9a8c\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "\u5728\u591a\u8f6e\u8bad\u7ec3\u4e2d\uff0cAdam\u7684\u6cdb\u5316\u6027\u80fd\u5bf9\u6279\u91cf\u5927\u5c0f\u4e0e\u52a8\u91cf\u8d85\u53c2\u6570\u7684\u4ea4\u4e92\u4f5c\u7528\u9ad8\u5ea6\u654f\u611f\u3002\u4ece\u4e1a\u8005\u5e94\u6839\u636e\u6279\u91cf\u5927\u5c0f\u8c03\u6574\u03b2\u2081\u548c\u03b2\u2082\u2014\u2014\u5927\u6279\u91cf\u65f6\u6700\u597d\u5c06\u03b2\u2081\u5411\u03b2\u2082\u9760\u8fd1\u2014\u2014\u4ee5\u4f18\u5316\u9a8c\u8bc1\u51c6\u786e\u7387\u3002"}}
{"id": "2602.01643", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01643", "abs": "https://arxiv.org/abs/2602.01643", "authors": ["Xichen Sun", "Wentao Wei", "Jiahua Rao", "Jiancong Xie", "Yuedong Yang"], "title": "De Novo Molecular Generation from Mass Spectra via Many-Body Enhanced Diffusion", "comment": null, "summary": "Molecular structure generation from mass spectrometry is fundamental for understanding cellular metabolism and discovering novel compounds. Although tandem mass spectrometry (MS/MS) enables the high-throughput acquisition of fragment fingerprints, these spectra often reflect higher-order interactions involving the concerted cleavage of multiple atoms and bonds-crucial for resolving complex isomers and non-local fragmentation mechanisms. However, most existing methods adopt atom-centric and pairwise interaction modeling, overlooking higher-order edge interactions and lacking the capacity to systematically capture essential many-body characteristics for structure generation. To overcome these limitations, we present MBGen, a Many-Body enhanced diffusion framework for de novo molecular structure Generation from mass spectra. By integrating a many-body attention mechanism and higher-order edge modeling, MBGen comprehensively leverages the rich structural information encoded in MS/MS spectra, enabling accurate de novo generation and isomer differentiation for novel molecules. Experimental results on the NPLIB1 and MassSpecGym benchmarks demonstrate that MBGen achieves superior performance, with improvements of up to 230% over state-of-the-art methods, highlighting the scientific value and practical utility of many-body modeling for mass spectrometry-based molecular generation. Further analysis and ablation studies show that our approach effectively captures higher-order interactions and exhibits enhanced sensitivity to complex isomeric and non-local fragmentation information.", "AI": {"tldr": "This paper presents MBGen, a many-body enhanced diffusion framework that generates molecular structures from MS/MS spectra by modeling higher-order interactions, achieving up to 230% improvement over existing methods.", "motivation": "Molecular structure generation from mass spectrometry is crucial for cellular metabolism understanding and novel compound discovery. Current methods using atom-centric and pairwise interactions cannot capture higher-order fragmentation mechanisms needed for complex isomer resolution.", "method": "The authors developed MBGen, which integrates many-body attention mechanisms and higher-order edge modeling within a diffusion framework to comprehensively leverage structural information from MS/MS spectra for de novo molecular generation.", "result": "On NPLIB1 and MassSpecGym benchmarks, MBGen achieved superior performance with up to 230% improvement over state-of-the-art methods, effectively capturing higher-order interactions and showing enhanced sensitivity to complex isomeric and non-local fragmentation patterns.", "conclusion": "Many-body modeling provides significant scientific value and practical utility for mass spectrometry-based molecular generation, enabling accurate de novo structure generation and isomer differentiation that surpasses current approaches."}}
{"id": "2602.01644", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01644", "abs": "https://arxiv.org/abs/2602.01644", "authors": ["Gloria Felicia", "Nolan Bryant", "Handi Putra", "Ayaan Gazali", "Eliel Lobo", "Esteban Rojas"], "title": "From Perception to Action: Spatial AI Agents and World Models", "comment": "61 pages, 742 citations, 1 figure, 3 tables. Survey paper on spatial AI agents, embodied AI, graph neural networks, and world models", "summary": "While large language models have become the prevailing approach for agentic reasoning and planning, their success in symbolic domains does not readily translate to the physical world. Spatial intelligence, the ability to perceive 3D structure, reason about object relationships, and act under physical constraints, is an orthogonal capability that proves important for embodied agents. Existing surveys address either agentic architectures or spatial domains in isolation. None provide a unified framework connecting these complementary capabilities. This paper bridges that gap. Through a thorough review of over 2,000 papers, citing 742 works from top-tier venues, we introduce a unified three-axis taxonomy connecting agentic capabilities with spatial tasks across scales. Crucially, we distinguish spatial grounding (metric understanding of geometry and physics) from symbolic grounding (associating images with text), arguing that perception alone does not confer agency. Our analysis reveals three key findings mapped to these axes: (1) hierarchical memory systems (Capability axis) are important for long-horizon spatial tasks. (2) GNN-LLM integration (Task axis) is a promising approach for structured spatial reasoning. (3) World models (Scale axis) are essential for safe deployment across micro-to-macro spatial scales. We conclude by identifying six grand challenges and outlining directions for future research, including the need for unified evaluation frameworks to standardize cross-domain assessment. This taxonomy provides a foundation for unifying fragmented research efforts and enabling the next generation of spatially-aware autonomous systems in robotics, autonomous vehicles, and geospatial intelligence.", "AI": {"tldr": "Proposes a unified three-axis taxonomy connecting LLM agentic capabilities with spatial intelligence for embodied agents, reviewing 742 papers to identify key research directions and six grand challenges.", "motivation": "LLMs succeed in symbolic domains but fail in physical world spatial intelligence. Existing surveys treat agentic architectures and spatial domains in isolation, lacking a unified framework.", "method": "Comprehensive review of 2,000+ papers (742 cited) from top venues; developed three-axis taxonomy integrating agentic capabilities with spatial tasks across scales.", "result": "Key findings: hierarchical memory for long-horizon tasks; GNN-LLM integration for structured reasoning; world models for cross-scale deployment. Six grand challenges identified.", "conclusion": "Taxonomy unifies fragmented research for spatially-aware autonomous systems in robotics, vehicles, and geospatial intelligence; calls for standardized evaluation frameworks."}}
{"id": "2602.01651", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01651", "abs": "https://arxiv.org/abs/2602.01651", "authors": ["Zichao Wei"], "title": "On the Spatiotemporal Dynamics of Generalization in Neural Networks", "comment": null, "summary": "Why do neural networks fail to generalize addition from 16-digit to 32-digit numbers, while a child who learns the rule can apply it to arbitrarily long sequences? We argue that this failure is not an engineering problem but a violation of physical postulates. Drawing inspiration from physics, we identify three constraints that any generalizing system must satisfy: (1) Locality -- information propagates at finite speed; (2) Symmetry -- the laws of computation are invariant across space and time; (3) Stability -- the system converges to discrete attractors that resist noise accumulation. From these postulates, we derive -- rather than design -- the Spatiotemporal Evolution with Attractor Dynamics (SEAD) architecture: a neural cellular automaton where local convolutional rules are iterated until convergence. Experiments on three tasks validate our theory: (1) Parity -- demonstrating perfect length generalization via light-cone propagation; (2) Addition -- achieving scale-invariant inference from L=16 to L=1 million with 100% accuracy, exhibiting input-adaptive computation; (3) Rule 110 -- learning a Turing-complete cellular automaton without trajectory divergence. Our results suggest that the gap between statistical learning and logical reasoning can be bridged -- not by scaling parameters, but by respecting the physics of computation.", "AI": {"tldr": "This paper argues neural networks' failure to generalize arithmetic stems from violating physical postulates, and proposes SEAD: a neural cellular automaton architecture derived from locality, symmetry, and stability constraints that achieves perfect length generalization for addition from 16 to 1 million digits.", "motivation": "Neural networks cannot generalize simple arithmetic operations (like addition) to longer sequences beyond their training length, while humans can apply learned rules to arbitrarily long inputs. The authors frame this as a fundamental violation of physical computation principles rather than an engineering limitation.", "method": "The authors derive three physical postulates for generalizing systems: (1) Locality - finite-speed information propagation, (2) Symmetry - space-time invariant computation rules, (3) Stability - convergence to noise-resistant discrete attractors. From these, they derive the SEAD architecture: a neural cellular automaton with local convolutional rules iterated until convergence.", "result": "Three experiments validate the theory: (1) Parity task shows perfect length generalization via light-cone propagation; (2) Addition achieves 100% accuracy scaling from 16-digit to 1-million-digit numbers with input-adaptive computation; (3) Rule 110 learns a Turing-complete automaton without trajectory divergence.", "conclusion": "The gap between statistical learning and logical reasoning can be bridged not by scaling model parameters, but by designing architectures that respect fundamental physical constraints of computation, enabling true systematic generalization."}}
{"id": "2602.01658", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01658", "abs": "https://arxiv.org/abs/2602.01658", "authors": ["Seyed Mohammad Hadi Hosseini", "Amir Najafi", "Mahdieh Soleymani Baghshah"], "title": "Efficient Adversarial Attacks on High-dimensional Offline Bandits", "comment": "Accepted at ICLR 2026 Conference", "summary": "Bandit algorithms have recently emerged as a powerful tool for evaluating machine learning models, including generative image models and large language models, by efficiently identifying top-performing candidates without exhaustive comparisons. These methods typically rely on a reward model, often distributed with public weights on platforms such as Hugging Face, to provide feedback to the bandit. While online evaluation is expensive and requires repeated trials, offline evaluation with logged data has become an attractive alternative. However, the adversarial robustness of offline bandit evaluation remains largely unexplored, particularly when an attacker perturbs the reward model (rather than the training data) prior to bandit training. In this work, we fill this gap by investigating, both theoretically and empirically, the vulnerability of offline bandit training to adversarial manipulations of the reward model. We introduce a novel threat model in which an attacker exploits offline data in high-dimensional settings to hijack the bandit's behavior. Starting with linear reward functions and extending to nonlinear models such as ReLU neural networks, we study attacks on two Hugging Face evaluators used for generative model assessment: one measuring aesthetic quality and the other assessing compositional alignment. Our results show that even small, imperceptible perturbations to the reward model's weights can drastically alter the bandit's behavior. From a theoretical perspective, we prove a striking high-dimensional effect: as input dimensionality increases, the perturbation norm required for a successful attack decreases, making modern applications such as image evaluation especially vulnerable. Extensive experiments confirm that naive random perturbations are ineffective, whereas carefully targeted perturbations achieve near-perfect attack success rates ...", "AI": {"tldr": "This paper investigates how offline bandit evaluation can be hijacked by adversarially perturbing the reward model, showing that high-dimensional settings make attacks easier with small weight changes.", "motivation": "The paper aims to fill the gap in understanding the adversarial robustness of offline bandit evaluation, specifically when attackers manipulate the reward model (not training data) before bandit training, which is crucial for efficient ML model evaluation.", "method": "The authors introduce a novel threat model where attackers exploit high-dimensional offline data to hijack bandit behavior. They study attacks on linear and nonlinear reward models (ReLU networks) applied to two Hugging Face evaluators for aesthetic quality and compositional alignment.", "result": "Small, imperceptible perturbations to reward model weights can drastically alter bandit behavior. Theoretically, they prove that as input dimensionality increases, the required perturbation norm decreases, making high-dimensional applications vulnerable. Experiments confirm targeted perturbations achieve near-perfect success while random ones fail.", "conclusion": "Offline bandit evaluation is highly vulnerable to adversarial reward model perturbations, especially in high-dimensional modern applications like image evaluation, highlighting a critical security concern."}}
{"id": "2602.01629", "categories": ["cs.LG", "cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.01629", "abs": "https://arxiv.org/abs/2602.01629", "authors": ["Renukanandan Tumu", "Aditya Singh", "Rahul Mangharam"], "title": "AdaptNC: Adaptive Nonconformity Scores for Uncertainty-Aware Autonomous Systems in Dynamic Environments", "comment": null, "summary": "Rigorous uncertainty quantification is essential for the safe deployment of autonomous systems in unconstrained environments. Conformal Prediction (CP) provides a distribution-free framework for this task, yet its standard formulations rely on exchangeability assumptions that are violated by the distribution shifts inherent in real-world robotics. Existing online CP methods maintain target coverage by adaptively scaling the conformal threshold, but typically employ a static nonconformity score function. We show that this fixed geometry leads to highly conservative, volume-inefficient prediction regions when environments undergo structural shifts. To address this, we propose \\textbf{AdaptNC}, a framework for the joint online adaptation of both the nonconformity score parameters and the conformal threshold. AdaptNC leverages an adaptive reweighting scheme to optimize score functions, and introduces a replay buffer mechanism to mitigate the coverage instability that occurs during score transitions. We evaluate AdaptNC on diverse robotic benchmarks involving multi-agent policy changes, environmental changes and sensor degradation. Our results demonstrate that AdaptNC significantly reduces prediction region volume compared to state-of-the-art threshold-only baselines while maintaining target coverage levels.", "AI": {"tldr": "AdaptNC\uff1a\u901a\u8fc7\u8054\u5408\u81ea\u9002\u5e94\u8c03\u6574\u975e conformity \u8bc4\u5206\u51fd\u6570\u548c conformal \u9608\u503c\uff0c\u63d0\u5347\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u5206\u5e03\u53d8\u5316\u4e0b\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6548\u7387\uff0c\u663e\u8457\u51cf\u5c0f\u9884\u6d4b\u533a\u57df\u4f53\u79ef\u540c\u65f6\u4fdd\u6301\u76ee\u6807\u8986\u76d6\u7387\u3002", "motivation": "Conformal Prediction\uff08CP\uff09\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65e0\u5206\u5e03\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\uff0c\u4f46\u5176\u6807\u51c6\u5f62\u5f0f\u4f9d\u8d56\u7684\u4ea4\u6362\u6027\u5047\u8bbe\u5728\u771f\u5b9e\u673a\u5668\u4eba\u573a\u666f\u7684\u5206\u5e03\u53d8\u5316\u4e0b\u4e0d\u6210\u7acb\u3002\u73b0\u6709\u5728\u7ebfCP\u65b9\u6cd5\u4ec5\u81ea\u9002\u5e94\u8c03\u6574conformal\u9608\u503c\uff0c\u800c\u4f7f\u7528\u9759\u6001\u975econformity\u8bc4\u5206\u51fd\u6570\uff0c\u5bfc\u81f4\u5728\u73af\u5883\u7ed3\u6784\u53d8\u5316\u65f6\u9884\u6d4b\u533a\u57df\u8fc7\u4e8e\u4fdd\u5b88\u4e14\u4f53\u79ef\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faAdaptNC\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u91cd\u52a0\u6743\u65b9\u6848\u8054\u5408\u5728\u7ebf\u8c03\u6574\u975econformity\u8bc4\u5206\u53c2\u6570\u548cconformal\u9608\u503c\uff0c\u5e76\u5f15\u5165\u56de\u653e\u7f13\u51b2\u673a\u5236\u7f13\u89e3\u8bc4\u5206\u8f6c\u6362\u671f\u95f4\u7684\u8986\u76d6\u4e0d\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u6d89\u53ca\u591a\u667a\u80fd\u4f53\u7b56\u7565\u53d8\u5316\u3001\u73af\u5883\u53d8\u5316\u548c\u4f20\u611f\u5668\u9000\u5316\u7684\u591a\u6837\u5316\u673a\u5668\u4eba\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAdaptNC\u76f8\u6bd4\u4ec5\u8c03\u6574\u9608\u503c\u7684\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u663e\u8457\u51cf\u5c0f\u4e86\u9884\u6d4b\u533a\u57df\u4f53\u79ef\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76ee\u6807\u8986\u76d6\u7387\u6c34\u5e73\u3002", "conclusion": "AdaptNC\u901a\u8fc7\u8054\u5408\u81ea\u9002\u5e94\u4f18\u5316\u8bc4\u5206\u51fd\u6570\u548c\u9608\u503c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5728\u7ebfCP\u4e2d\u9759\u6001\u8bc4\u5206\u51fd\u6570\u5728\u5206\u5e03\u53d8\u5316\u4e0b\u4fdd\u5b88\u4f4e\u6548\u7684\u95ee\u9898\uff0c\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u5728\u65e0\u7ea6\u675f\u73af\u5883\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684uncertainty quantification\u65b9\u6848\u3002"}}
{"id": "2602.01635", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01635", "abs": "https://arxiv.org/abs/2602.01635", "authors": ["Jinwoo Park", "Hyeongwon Kang", "Seung Hun Han", "Pilsung Kang"], "title": "COMET: Codebook-based Online-adaptive Multi-scale Embedding for Time-series Anomaly Detection", "comment": null, "summary": "Time series anomaly detection is a critical task across various industrial domains. However, capturing temporal dependencies and multivariate correlations within patch-level representation learning remains underexplored, and reliance on single-scale patterns limits the detection of anomalies across different temporal ranges. Furthermore, focusing on normal data representations makes models vulnerable to distribution shifts at inference time. To address these limitations, we propose Codebook-based Online-adaptive Multi-scale Embedding for Time-series anomaly detection (COMET), which consists of three key components: (1) Multi-scale Patch Encoding captures temporal dependencies and inter-variable correlations across multiple patch scales. (2) Vector-Quantized Coreset learns representative normal patterns via codebook and detects anomalies with a dual-score combining quantization error and memory distance. (3) Online Codebook Adaptation generates pseudo-labels based on codebook entries and dynamically adapts the model at inference through contrastive learning. Experiments on five benchmark datasets demonstrate that COMET achieves the best performance in 36 out of 45 evaluation metrics, validating its effectiveness across diverse environments.", "AI": {"tldr": "This paper proposes COMET, a novel time series anomaly detection method that uses multi-scale patch encoding, vector-quantized coreset with dual-score detection, and online codebook adaptation to achieve state-of-the-art performance across five benchmark datasets.", "motivation": "Current time series anomaly detection methods have three key limitations: (1) inadequate exploration of temporal dependencies and multivariate correlations in patch-level representations, (2) reliance on single-scale patterns that restrict detection across varying temporal ranges, and (3) vulnerability to distribution shifts during inference due to exclusive focus on normal data representations.", "method": "The authors propose COMET with three components: (1) Multi-scale Patch Encoding that captures temporal dependencies and inter-variable correlations across multiple patch scales; (2) Vector-Quantized Coreset that learns representative normal patterns via a codebook and detects anomalies using a dual-score combining quantization error and memory distance; (3) Online Codebook Adaptation that generates pseudo-labels from codebook entries and dynamically adapts the model during inference through contrastive learning.", "result": "COMET achieves the best performance in 36 out of 45 evaluation metrics across five benchmark datasets, demonstrating its superior effectiveness and robustness in diverse environments.", "conclusion": "COMET successfully addresses the identified limitations in time series anomaly detection through its novel architecture combining multi-scale representation learning, codebook-based pattern memory, and online adaptation, establishing a new state-of-the-art approach for industrial anomaly detection applications."}}
{"id": "2602.01685", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01685", "abs": "https://arxiv.org/abs/2602.01685", "authors": ["Byeonghu Na", "Hyungho Na", "Yeongmin Kim", "Suhyeon Jo", "HeeSun Bae", "Mina Kang", "Il-Chul Moon"], "title": "Semantic-aware Wasserstein Policy Regularization for Large Language Model Alignment", "comment": "Accepted at ICLR 2026", "summary": "Large language models (LLMs) are commonly aligned with human preferences using reinforcement learning from human feedback (RLHF). In this method, LLM policies are generally optimized through reward maximization with Kullback-Leibler (KL) divergence regularization of the reference policy. However, KL and its $f$-divergence variants only compare token probabilities at identical indices, failing to capture semantic similarity. We propose Wasserstein Policy Regularization (WPR), a semantic-aware regularization for the RLHF framework based on the entropy-regularized Wasserstein distance, which incorporates the geometry of the token space. The dual formulation of the distance expresses the regularization as penalty terms applied to the reward via optimal dual variables, which yield a tractable objective compatible with standard RL algorithms. Empirically, our method outperforms KL- and $f$-divergence-based baselines, demonstrating the benefits of semantic-aware policy distances for alignment. Our code is available at https://github.com/aailab-kaist/WPR.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9RLHF\u4e2dKL\u6563\u5ea6\u6b63\u5219\u5316\u65e0\u6cd5\u6355\u6349\u8bed\u4e49\u76f8\u4f3c\u6027\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u71b5\u6b63\u5219\u5316Wasserstein\u8ddd\u79bb\u7684Wasserstein\u7b56\u7565\u6b63\u5219\u5316\uff08WPR\uff09\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u4f20\u7edfKL\u548cf-\u6563\u5ea6\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709RLHF\u65b9\u6cd5\u4f7f\u7528KL\u6563\u5ea6\u6b63\u5219\u5316\u65f6\uff0c\u4ec5\u6bd4\u8f83\u76f8\u540c\u7d22\u5f15\u4f4d\u7f6e\u7684token\u6982\u7387\uff0c\u65e0\u6cd5\u6355\u6349token\u4e4b\u95f4\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u9650\u5236\u4e86\u7b56\u7565\u4f18\u5316\u7684\u6548\u679c\u3002", "method": "\u63d0\u51faWasserstein\u7b56\u7565\u6b63\u5219\u5316\uff08WPR\uff09\uff0c\u5229\u7528\u71b5\u6b63\u5219\u5316Wasserstein\u8ddd\u79bb\u6765\u5ea6\u91cf\u7b56\u7565\u5dee\u5f02\uff0c\u901a\u8fc7\u5176\u5bf9\u5076\u5f62\u5f0f\u5c06\u6b63\u5219\u5316\u8f6c\u5316\u4e3a\u53ef\u52a0\u5728\u5956\u52b1\u4e0a\u7684\u60e9\u7f5a\u9879\uff0c\u5b9e\u73b0\u4e0e\u6807\u51c6RL\u7b97\u6cd5\u7684\u517c\u5bb9\u3002", "result": "\u5728\u5b9e\u9a8c\u4e2d\uff0cWPR\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u4e8eKL\u6563\u5ea6\u548cf-\u6563\u5ea6\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u8bed\u4e49\u611f\u77e5\u7b56\u7565\u8ddd\u79bb\u5728\u5bf9\u9f50\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\u3002", "conclusion": "WPR\u4e3aRLHF\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8bed\u4e49\u611f\u77e5\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u8868\u660e\u8003\u8651token\u7a7a\u95f4\u51e0\u4f55\u7ed3\u6784\u80fd\u663e\u8457\u63d0\u5347\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u7684\u6548\u679c\u3002"}}
{"id": "2602.01637", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01637", "abs": "https://arxiv.org/abs/2602.01637", "authors": ["Sreenivasan Mohandas"], "title": "Chance-Constrained Inference for Hallucination Risk Control in Large Language Models", "comment": null, "summary": "Large language models generate outputs stochastically and may produce fluent but invalid responses, including factual hallucinations. Existing mitigation strategies reduce average error rates but do not provide explicit control over the \\emph{frequency} of such failures under repeated use. We formulate inference as a deployment-time risk control problem and introduce \\emph{chance-constrained inference}, which directly bounds the probability of hallucinations among accepted generations. Hallucinations are modeled as stochastic constraint violations, and we show that confidence-based selective prediction does not, in general, imply probabilistic risk guarantees. To enforce chance constraints efficiently, we propose a sequential, anytime-valid inference procedure that adaptively certifies feasibility or infeasibility using finite samples, avoiding conservative fixed-sample bounds. Experiments on questions inspired by NaturalQuestions and controlled multi-hop question answering demonstrate reliable risk control, early detection of intrinsically infeasible inputs, and safe composition under repeated use, while confidence-based baselines fail to provide consistent guarantees.", "AI": {"tldr": "\u63d0\u51fa\u673a\u4f1a\u7ea6\u675f\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5e8f\u8d2f\u9a8c\u8bc1\u7a0b\u5e8f\u76f4\u63a5\u63a7\u5236LLM\u91cd\u590d\u4f7f\u7528\u65f6\u7684\u5e7b\u89c9\u6982\u7387\uff0c\u5b9e\u73b0\u53ef\u9760\u98ce\u9669\u7ea6\u675f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u964d\u4f4e\u5e73\u5747\u9519\u8bef\u7387\uff0c\u65e0\u6cd5\u5728\u91cd\u590d\u4f7f\u7528\u573a\u666f\u4e0b\u663e\u5f0f\u63a7\u5236\u5e7b\u89c9\u53d1\u751f\u7684\u9891\u7387\uff0c\u7f3a\u4e4f\u6982\u7387\u5316\u98ce\u9669\u4fdd\u8bc1\u3002", "method": "\u5c06\u63a8\u7406\u5efa\u6a21\u4e3a\u90e8\u7f72\u65f6\u98ce\u9669\u63a7\u5236\u95ee\u9898\uff0c\u63d0\u51fa\u673a\u4f1a\u7ea6\u675f\u63a8\u7406\u6846\u67b6\uff1a\u7528\u5e8f\u8d2f\u3001\u968f\u65f6\u6709\u6548\u7684\u9a8c\u8bc1\u7a0b\u5e8f\uff0c\u57fa\u4e8e\u6709\u9650\u6837\u672c\u81ea\u9002\u5e94\u8ba4\u8bc1\u53ef\u884c\u6027\uff0c\u907f\u514d\u4fdd\u5b88\u7684\u56fa\u5b9a\u6837\u672c\u8fb9\u754c\u3002", "result": "\u5728NaturalQuestions-inspired\u53ca\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u5b9e\u73b0\u53ef\u9760\u98ce\u9669\u7ea6\u675f\uff0c\u53ef\u65e9\u671f\u68c0\u6d4b\u4e0d\u53ef\u884c\u8f93\u5165\uff0c\u4e14\u5b89\u5168\u652f\u6301\u91cd\u590d\u4f7f\u7528\uff1b\u800c\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u57fa\u7ebf\u65b9\u6cd5\u65e0\u6cd5\u63d0\u4f9b\u4e00\u81f4\u4fdd\u8bc1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9996\u6b21\u5b9e\u73b0LLM\u751f\u6210\u4e2d\u5bf9\u5e7b\u89c9\u6982\u7387\u7684\u76f4\u63a5\u7edf\u8ba1\u8fb9\u754c\u63a7\u5236\uff0c\u4e3a\u9ad8\u98ce\u9669\u5e94\u7528\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u5b89\u5168\u4fdd\u969c\uff0c\u8d85\u8d8a\u4f20\u7edf\u7f6e\u4fe1\u5ea6\u8fc7\u6ee4\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.01705", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01705", "abs": "https://arxiv.org/abs/2602.01705", "authors": ["Haoqiang Kang", "Yizhe Zhang", "Nikki Lijing Kuang", "Yi-An Ma", "Lianhui Qin"], "title": "Beyond Mode Elicitation: Diversity-Preserving Reinforcement Learning via Latent Diffusion Reasoner", "comment": null, "summary": "Recent reinforcement learning (RL) methods improve LLM reasoning by optimizing discrete Chain-of-Thought (CoT) generation; however, exploration in token space often suffers from diversity collapse as policy entropy decreases due to mode elicitation behavior in discrete RL. To mitigate this issue, we propose Latent Diffusion Reasoning with Reinforcement Learning (LaDi-RL), a framework that conducts exploration directly in a continuous latent space, where latent variables encode semantic-level reasoning trajectories. By modeling exploration via guided diffusion, multi-step denoising distributes stochasticity and preserves multiple coexisting solution modes without mutual suppression. Furthermore, by decoupling latent-space exploration from text-space generation, we show that latent diffusion-based optimization is more effective than text-space policy optimization alone, while a complementary text policy provides additional gains when combined with latent exploration. Experiments on code generation and mathematical reasoning benchmarks demonstrate consistent improvements in both pass@1 and pass@k over discrete RL baselines, with absolute pass@1 gains of +9.4% on code generation and +5.7% on mathematical reasoning, highlighting diffusion-based latent RL as a principled alternative to discrete token-level RL for reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLaDi-RL\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u8fde\u7eed\u6f5c\u7a7a\u95f4\u800c\u975e\u79bb\u6563token\u7a7a\u95f4\u8fdb\u884c\u63a2\u7d22\uff0c\u5229\u7528\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u4fdd\u6301\u63a8\u7406\u591a\u6837\u6027\uff0c\u5728\u4ee3\u7801\u751f\u6210\u548c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u79bb\u6563\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u79bb\u6563\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u4f18\u5316LLM\u63a8\u7406\u65f6\u9762\u4e34\u591a\u6837\u6027\u5d29\u6e83\u95ee\u9898\uff0c\u968f\u7740\u7b56\u7565\u71b5\u56e0\u6a21\u5f0f\u6fc0\u53d1\u884c\u4e3a\u800c\u4e0b\u964d\uff0ctoken\u7a7a\u95f4\u7684\u63a2\u7d22\u80fd\u529b\u53d7\u9650\uff0c\u5bfc\u81f4\u591a\u79cd\u89e3\u9898\u6a21\u5f0f\u76f8\u4e92\u6291\u5236\u3002", "method": "LaDi-RL\u6846\u67b6\u5c06\u63a8\u7406\u8f68\u8ff9\u7f16\u7801\u5230\u8fde\u7eed\u6f5c\u7a7a\u95f4\uff0c\u901a\u8fc7\u5f15\u5bfc\u6269\u6563\u8fdb\u884c\u591a\u6b65\u53bb\u566a\u6765\u5206\u5e03\u968f\u673a\u6027\u5e76\u7ef4\u6301\u591a\u4e2a\u5171\u5b58\u89e3\u6a21\u5f0f\u3002\u89e3\u8026\u6f5c\u7a7a\u95f4\u63a2\u7d22\u4e0e\u6587\u672c\u751f\u6210\uff0c\u5e76\u8f85\u4ee5\u4e92\u8865\u7684\u6587\u672c\u7b56\u7565\u3002", "result": "\u5728\u4ee3\u7801\u751f\u6210\u548c\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u79bb\u6563RL\u57fa\u7ebf\uff0cpass@1\u7edd\u5bf9\u63d0\u5347\u5206\u522b\u4e3a+9.4%\u548c+5.7%\uff0cpass@k\u6307\u6807\u4e5f\u6301\u7eed\u6539\u5584\u3002", "conclusion": "\u57fa\u4e8e\u6269\u6563\u7684\u6f5c\u7a7a\u95f4\u5f3a\u5316\u5b66\u4e60\u4e3a\u79bb\u6563token\u7ea7RL\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u66f4\u6709\u6548\u5730\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2602.01744", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01744", "abs": "https://arxiv.org/abs/2602.01744", "authors": ["Mingwei Xu", "Xuan Lin", "Xinnan Guo", "Wanqing Xu", "Wanyun Cui"], "title": "Softmax Linear Attention: Reclaiming Global Competition", "comment": "11 pages,4 figures", "summary": "While linear attention reduces the quadratic complexity of standard Transformers to linear time, it often lags behind in expressivity due to the removal of softmax normalization. This omission eliminates \\emph{global competition}, a critical mechanism that enables models to sharply focus on relevant information amidst long-context noise. In this work, we propose \\textbf{Softmax Linear Attention (SLA)}, a framework designed to restore this competitive selection without sacrificing efficiency. By lifting the softmax operation from the token level to the head level, SLA leverages attention heads as coarse semantic slots, applying a competitive gating mechanism to dynamically select the most relevant subspaces. This reintroduces the ``winner-take-all'' dynamics essential for precise retrieval and robust long-context understanding. Distinct from prior methods that focus on refining local kernel functions, SLA adopts a broader perspective by exploiting the higher-level multi-head aggregation structure. Extensive experiments demonstrate that SLA consistently enhances state-of-the-art linear baselines (RetNet, GLA, GDN) across language modeling and long-context benchmarks, particularly in challenging retrieval scenarios where it significantly boosts robustness against noise, validating its capability to restore precise focus while maintaining linear complexity.", "AI": {"tldr": "\u63d0\u51faSoftmax Linear Attention (SLA)\u6846\u67b6\uff0c\u901a\u8fc7\u5c06softmax\u64cd\u4f5c\u4ecetoken\u7ea7\u63d0\u5347\u5230head\u7ea7\uff0c\u5728\u4fdd\u6301\u7ebf\u6027\u590d\u6742\u5ea6\u540c\u65f6\u6062\u590d\u5168\u5c40\u7ade\u4e89\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u7ebf\u6027\u6ce8\u610f\u529b\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u548c\u566a\u58f0\u73af\u5883\u4e0b\u7684\u8868\u73b0", "motivation": "\u7ebf\u6027\u6ce8\u610f\u529b\u867d\u5c06Transformer\u590d\u6742\u5ea6\u964d\u81f3\u7ebf\u6027\uff0c\u4f46\u79fb\u9664softmax\u5f52\u4e00\u5316\u5bfc\u81f4\u8868\u8fbe\u529b\u4e0b\u964d\uff0c\u7f3a\u4e4f\u5168\u5c40\u7ade\u4e89\u673a\u5236\uff0c\u96be\u4ee5\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u805a\u7126\u5173\u952e\u4fe1\u606f", "method": "\u8bbe\u8ba1SLA\u6846\u67b6\uff1a\u5c06softmax\u4ecetoken\u7ea7\u63d0\u5347\u5230head\u7ea7\uff0c\u5229\u7528\u6ce8\u610f\u529b\u5934\u4f5c\u4e3a\u7c97\u7c92\u5ea6\u8bed\u4e49\u69fd\uff0c\u901a\u8fc7\u7ade\u4e89\u95e8\u63a7\u673a\u5236\u52a8\u6001\u9009\u62e9\u6700\u76f8\u5173\u5b50\u7a7a\u95f4\uff0c\u91cd\u65b0\u5f15\u5165\"\u8d62\u5bb6\u901a\u5403\"\u52a8\u6001\u7279\u6027", "result": "\u5728\u8bed\u8a00\u5efa\u6a21\u548c\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSLA\u6301\u7eed\u589e\u5f3a\u591a\u79cd\u5148\u8fdb\u7ebf\u6027\u57fa\u7ebf\uff08RetNet\u3001GLA\u3001GDN\uff09\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6311\u6218\u6027\u68c0\u7d22\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u6297\u566a\u58f0\u9c81\u68d2\u6027", "conclusion": "SLA\u901a\u8fc7\u9ad8\u5c42\u591a\u5934\u805a\u5408\u7ed3\u6784\u6210\u529f\u6062\u590d\u4e86\u7cbe\u786e\u805a\u7126\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u7ebf\u6027\u590d\u6742\u5ea6\uff0c\u4e3a\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u8868\u8fbe\u529b\u5f3a\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.01745", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01745", "abs": "https://arxiv.org/abs/2602.01745", "authors": ["Wenhao Yu", "Shaohang Wei", "Jiahong Liu", "Yifan Li", "Minda Hu", "Aiwei Liu", "Hao Zhang", "Irwin King"], "title": "Probability-Entropy Calibration: An Elastic Indicator for Adaptive Fine-tuning", "comment": null, "summary": "Token-level reweighting is a simple yet effective mechanism for controlling supervised fine-tuning, but common indicators are largely one-dimensional: the ground-truth probability reflects downstream alignment, while token entropy reflects intrinsic uncertainty induced by the pre-training prior. Ignoring entropy can misidentify noisy or easily replaceable tokens as learning-critical, while ignoring probability fails to reflect target-specific alignment. RankTuner introduces a probability--entropy calibration signal, the Relative Rank Indicator, which compares the rank of the ground-truth token with its expected rank under the prediction distribution. The inverse indicator is used as a token-wise Relative Scale to reweight the fine-tuning objective, focusing updates on truly under-learned tokens without over-penalizing intrinsically uncertain positions. Experiments on multiple backbones show consistent improvements on mathematical reasoning benchmarks, transfer gains on out-of-distribution reasoning, and pre code generation performance over probability-only or entropy-only reweighting baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRankTuner\uff0c\u901a\u8fc7\u6982\u7387-\u71b5\u8054\u5408\u6821\u51c6\u7684\u76f8\u5bf9\u6392\u540d\u6307\u6807\u89e3\u51b3\u76d1\u7763\u5fae\u8c03\u4e2dtoken\u91cd\u52a0\u6743\u7684\u4e00\u7ef4\u5c40\u9650\u6027\uff0c\u5728\u6570\u5b66\u63a8\u7406\u3001\u5206\u5e03\u5916\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e00\u81f4\u63d0\u5347\u3002", "motivation": "\u5f53\u524dtoken\u7ea7\u91cd\u52a0\u6743\u65b9\u6cd5\u5b58\u5728\u4e00\u7ef4\u6307\u6807\u7f3a\u9677\uff1a\u4ec5\u5173\u6ce8\u771f\u5b9e\u6982\u7387\u4f1a\u5ffd\u7565\u6a21\u578b\u56fa\u6709\u4e0d\u786e\u5b9a\u6027\uff0c\u4ec5\u5173\u6ce8\u71b5\u4f1a\u5ffd\u7565\u76ee\u6807\u5bf9\u9f50\uff0c\u5bfc\u81f4\u5c06\u566a\u58f0\u6216\u6613\u66ff\u6362token\u8bef\u5224\u4e3a\u5173\u952e\u5b66\u4e60\u4f4d\u7f6e\u3002", "method": "\u63d0\u51fa\u76f8\u5bf9\u6392\u540d\u6307\u6807\uff08Relative Rank Indicator\uff09\uff0c\u8ba1\u7b97\u771f\u5b9etoken\u6392\u540d\u4e0e\u5176\u5728\u9884\u6d4b\u5206\u5e03\u4e2d\u671f\u671b\u6392\u540d\u7684\u76f8\u5bf9\u5173\u7cfb\uff0c\u5e76\u4ee5\u5176\u5012\u6570\u4f5c\u4e3atoken\u7ea7\u76f8\u5bf9\u5c3a\u5ea6\uff08Relative Scale\uff09\u91cd\u52a0\u6743\u5fae\u8c03\u76ee\u6807\uff0c\u7cbe\u51c6\u805a\u7126\u6b20\u5b66\u4e60token\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRankTuner\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u6301\u7eed\u63d0\u5347\u6570\u5b66\u63a8\u7406\u6027\u80fd\u3001\u5206\u5e03\u5916\u63a8\u7406\u8fc1\u79fb\u80fd\u529b\u548c\u4ee3\u7801\u751f\u6210\u6548\u679c\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u4e00\u6982\u7387\u6216\u71b5\u91cd\u52a0\u6743\u57fa\u7ebf\u3002", "conclusion": "\u6982\u7387-\u71b5\u8054\u5408\u6821\u51c6\u4fe1\u53f7\u80fd\u66f4\u51c6\u786e\u8bc6\u522b\u771f\u6b63\u9700\u8981\u66f4\u65b0\u7684token\uff0c\u907f\u514d\u5bf9\u4e0d\u786e\u5b9a\u4f4d\u7f6e\u7684\u8fc7\u5ea6\u60e9\u7f5a\uff0c\u4e3a\u5fae\u8c03\u63a7\u5236\u63d0\u4f9b\u66f4\u7cbe\u7ec6\u7684\u673a\u5236\u3002"}}
{"id": "2602.01746", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01746", "abs": "https://arxiv.org/abs/2602.01746", "authors": ["Hongyi Peng", "Han Yu", "Xiaoxiao Li", "Qiang Yang"], "title": "Rethinking LoRA for Data Heterogeneous Federated Learning: Subspace and State Alignment", "comment": null, "summary": "Low-Rank Adaptation (LoRA) is widely used for federated fine-tuning. Yet under non-IID settings, it can substantially underperform full-parameter fine-tuning. Through with-high-probability robustness analysis, we uncover that this gap can be attributed to two coupled mismatches: (i) update-space mismatch, where clients optimize in a low-rank subspace but aggregation occurs in the full space; and (ii) optimizer-state mismatch, where unsynchronized adaptive states amplify drift across rounds. We propose FedGaLore, which combines client-side GaLore-style gradient-subspace optimization with server-side drift-robust synchronization of projected second-moment states via spectral shared-signal extraction, to address this challenge. Across NLU, vision, and NLG benchmarks, FedGaLore improves robustness and accuracy over state-of-the-art federated LoRA baselines in non-IID settings.", "AI": {"tldr": "FedGaLore addresses performance gaps in federated LoRA fine-tuning under non-IID data by identifying two mismatches (update-space and optimizer-state) and proposing a solution combining client-side GaLore optimization with server-side drift-robust synchronization, improving robustness and accuracy across multiple benchmarks.", "motivation": "LoRA underperforms full-parameter fine-tuning in federated learning with non-IID data due to mismatches between client optimization subspaces and server aggregation spaces, as well as unsynchronized optimizer states causing drift.", "method": "Proposes FedGaLore: client-side GaLore-style gradient subspace optimization plus server-side drift-robust synchronization of projected second-moment states via spectral shared-signal extraction.", "result": "FedGaLore improves robustness and accuracy over state-of-the-art federated LoRA baselines across NLU, vision, and NLG benchmarks in non-IID settings.", "conclusion": "FedGaLore effectively addresses the identified mismatches and provides a more robust and accurate solution for federated fine-tuning with LoRA under non-IID data distributions."}}
{"id": "2602.01766", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01766", "abs": "https://arxiv.org/abs/2602.01766", "authors": ["Runsong Zhao", "Shilei Liu", "Jiwei Tang", "Langming Liu", "Haibin Chen", "Weidong Zhang", "Yujin Yuan", "Tong Xiao", "Jingbo Zhu", "Wenbo Su", "Bo Zheng"], "title": "CoMeT: Collaborative Memory Transformer for Efficient Long Context Modeling", "comment": null, "summary": "The quadratic complexity and indefinitely growing key-value (KV) cache of standard Transformers pose a major barrier to long-context processing. To overcome this, we introduce the Collaborative Memory Transformer (CoMeT), a novel architecture that enables LLMs to handle arbitrarily long sequences with constant memory usage and linear time complexity. Designed as an efficient, plug-in module, CoMeT can be integrated into pre-trained models with only minimal fine-tuning. It operates on sequential data chunks, using a dual-memory system to manage context: a temporary memory on a FIFO queue for recent events, and a global memory with a gated update rule for long-range dependencies. These memories then act as a dynamic soft prompt for the next chunk. To enable efficient fine-tuning on extremely long contexts, we introduce a novel layer-level pipeline parallelism strategy. The effectiveness of our approach is remarkable: a model equipped with CoMeT and fine-tuned on 32k contexts can accurately retrieve a passkey from any position within a 1M token sequence. On the SCROLLS benchmark, CoMeT surpasses other efficient methods and achieves performance comparable to a full-attention baseline on summarization tasks. Its practical effectiveness is further validated on real-world agent and user behavior QA tasks. The code is available at: https://anonymous.4open.science/r/comet-B00B/", "AI": {"tldr": "We propose Collaborative Memory Transformer (CoMeT), a plug-in module that enables LLMs to process arbitrarily long sequences with constant memory and linear time complexity via a dual-memory system and layer-level pipeline parallelism.", "motivation": "Standard Transformers suffer from quadratic complexity and indefinitely growing KV cache, which severely limits their ability to process long contexts.", "method": "CoMeT processes data in chunks with a dual-memory architecture: a FIFO-based temporary memory for recent events and a gated global memory for long-range dependencies. These memories serve as dynamic soft prompts for subsequent chunks. A novel layer-level pipeline parallelism strategy enables efficient fine-tuning on extremely long contexts.", "result": "A CoMeT-equipped model fine-tuned on 32k contexts can retrieve passkeys from any position in 1M-token sequences. It outperforms other efficient methods and matches full-attention performance on SCROLLS summarization tasks, with validated effectiveness on real-world agent and user behavior QA.", "conclusion": "CoMeT effectively solves long-context processing challenges with remarkable efficiency and performance, offering a practical plug-in solution for existing LLMs."}}
{"id": "2602.01667", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01667", "abs": "https://arxiv.org/abs/2602.01667", "authors": ["Siu Lun Chau", "Soroush H. Zargarbashi", "Yusuf Sale", "Michele Caprio"], "title": "Quantifying Epistemic Predictive Uncertainty in Conformal Prediction", "comment": "42 pages", "summary": "We study the problem of quantifying epistemic predictive uncertainty (EPU) -- that is, uncertainty faced at prediction time due to the existence of multiple plausible predictive models -- within the framework of conformal prediction (CP). To expose the implicit model multiplicity underlying CP, we build on recent results showing that, under a mild assumption, any full CP procedure induces a set of closed and convex predictive distributions, commonly referred to as a credal set. Importantly, the conformal prediction region (CPR) coincides exactly with the set of labels to which all distributions in the induced credal set assign probability at least $1-\u03b1$. As our first contribution, we prove that this characterisation also holds in split CP. Building on this connection, we then propose a computationally efficient and analytically tractable uncertainty measure, based on \\emph{Maximum Mean Imprecision}, to quantify the EPU by measuring the degree of conflicting information within the induced credal set. Experiments on active learning and selective classification demonstrate that the quantified EPU provides substantially more informative and fine-grained uncertainty assessments than reliance on CPR size alone. More broadly, this work highlights the potential of CP serving as a principled basis for decision-making under epistemic uncertainty.", "AI": {"tldr": "The paper connects conformal prediction with credal sets to quantify epistemic uncertainty, proposing a computationally efficient measure that outperforms simple prediction region size.", "motivation": "To quantify epistemic predictive uncertainty (EPU) in conformal prediction by exposing implicit model multiplicity and building on recent results about credal sets induced by CP procedures.", "method": "The authors prove the connection between conformal prediction regions and credal sets holds in split CP, then propose a Maximum Mean Imprecision measure to quantify EPU by measuring conflicting information within the credal set.", "result": "Experiments on active learning and selective classification show their EPU measure provides more informative and fine-grained uncertainty assessments than relying on conformal prediction region size alone.", "conclusion": "Conformal prediction serves as a principled basis for decision-making under epistemic uncertainty, with the proposed measure offering substantial advantages over traditional approaches."}}
{"id": "2602.01769", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01769", "abs": "https://arxiv.org/abs/2602.01769", "authors": ["Yuanshuai Li", "Yuping Yan", "Jirui Han", "Fei Ming", "Lingjuan Lv", "Yaochu Jin"], "title": "IRIS: Implicit Reward-Guided Internal Sifting for Mitigating Multimodal Hallucination", "comment": null, "summary": "Hallucination remains a fundamental challenge for Multimodal Large Language Models (MLLMs). While Direct Preference Optimization (DPO) is a key alignment framework, existing approaches often rely heavily on costly external evaluators for scoring or rewriting, incurring off-policy learnability gaps and discretization loss. Due to the lack of access to internal states, such feedback overlooks the fine-grained conflicts between different modalities that lead to hallucinations during generation.\n  To address this issue, we propose IRIS (Implicit Reward-Guided Internal Sifting), which leverages continuous implicit rewards in the native log-probability space to preserve full information density and capture internal modal competition. This on-policy paradigm eliminates learnability gaps by utilizing self-generated preference pairs. By sifting these pairs based on multimodal implicit rewards, IRIS ensures that optimization is driven by signals that directly resolve modal conflicts. Extensive experiments demonstrate that IRIS achieves highly competitive performance on key hallucination benchmarks using only 5.7k samples, without requiring any external feedback during preference alignment. These results confirm that IRIS provides an efficient and principled paradigm for mitigating MLLM hallucinations.", "AI": {"tldr": "IRIS\u5229\u7528\u5185\u90e8log-probability\u9690\u5f0f\u5956\u52b1\u66ff\u4ee3\u5916\u90e8\u8bc4\u4f30\u5668\uff0c\u901a\u8fc7\u81ea\u751f\u6210\u504f\u597d\u5bf9\u89e3\u51b3MLLM\u5e7b\u89c9\u95ee\u9898\uff0c\u4ec5\u97005.7k\u6837\u672c\u5373\u8fbe\u9ad8\u7ade\u4e89\u529b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709DPO\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u8bc4\u4f30\u5668\u5bfc\u81f4\u79bb\u7b56\u7565\u5b66\u4e60\u5dee\u8ddd\u548c\u4fe1\u606f\u79bb\u6563\u5316\u635f\u5931\uff0c\u4e14\u65e0\u6cd5\u6355\u6349\u6a21\u6001\u95f4\u7ec6\u7c92\u5ea6\u51b2\u7a81\u3002", "method": "\u63d0\u51faIRIS\u6846\u67b6\uff0c\u5728\u539f\u751flog-probability\u7a7a\u95f4\u5229\u7528\u8fde\u7eed\u9690\u5f0f\u5956\u52b1\uff0c\u901a\u8fc7\u81ea\u751f\u6210\u504f\u597d\u5bf9\u8fdb\u884c\u5185\u90e8\u7b5b\u9009\uff0c\u76f4\u63a5\u4f18\u5316\u6a21\u6001\u51b2\u7a81\u3002", "result": "\u4ec5\u75285.7k\u6837\u672c\u5373\u5728\u5173\u952e\u5e7b\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u9ad8\u7ade\u4e89\u529b\u6027\u80fd\uff0c\u65e0\u9700\u5916\u90e8\u53cd\u9988\u3002", "conclusion": "IRIS\u4e3a\u7f13\u89e3MLLM\u5e7b\u89c9\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u539f\u5219\u6027\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2602.01682", "categories": ["cs.LG", "cs.DS", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01682", "abs": "https://arxiv.org/abs/2602.01682", "authors": ["Taihei Oki", "Shinsaku Sakaue"], "title": "Finite and Corruption-Robust Regret Bounds in Online Inverse Linear Optimization under M-Convex Action Sets", "comment": null, "summary": "We study online inverse linear optimization, also known as contextual recommendation, where a learner sequentially infers an agent's hidden objective vector from observed optimal actions over feasible sets that change over time. The learner aims to recommend actions that perform well under the agent's true objective, and the performance is measured by the regret, defined as the cumulative gap between the agent's optimal values and those achieved by the learner's recommended actions. Prior work has established a regret bound of $O(d\\log T)$, as well as a finite but exponentially large bound of $\\exp(O(d\\log d))$, where $d$ is the dimension of the optimization problem and $T$ is the time horizon, while a regret lower bound of $\u03a9(d)$ is known (Gollapudi et al. 2021; Sakaue et al. 2025). Whether a finite regret bound polynomial in $d$ is achievable or not has remained an open question. We partially resolve this by showing that when the feasible sets are M-convex -- a broad class that includes matroids -- a finite regret bound of $O(d\\log d)$ is possible. We achieve this by combining a structural characterization of optimal solutions on M-convex sets with a geometric volume argument. Moreover, we extend our approach to adversarially corrupted feedback in up to $C$ rounds. We obtain a regret bound of $O((C+1)d\\log d)$ without prior knowledge of $C$, by monitoring directed graphs induced by the observed feedback to detect corruptions adaptively.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5728\u7ebf\u9006\u7ebf\u6027\u4f18\u5316\u95ee\u9898\uff0c\u8bc1\u660e\u5728M-\u51f8\u53ef\u884c\u96c6\u4e0a\u53ef\u4ee5\u5b9e\u73b0O(d log d)\u7684\u6709\u9650\u9057\u61be\u754c\uff0c\u90e8\u5206\u89e3\u51b3\u4e86\u8be5\u9886\u57df\u5173\u4e8e\u591a\u9879\u5f0f\u9057\u61be\u754c\u7684\u516c\u5f00\u95ee\u9898\uff0c\u5e76\u8fdb\u4e00\u6b65\u6269\u5c55\u5230\u5bf9\u6297\u6027\u635f\u574f\u53cd\u9988\u573a\u666f\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u5728\u8be5\u95ee\u9898\u4e0a\u5f97\u5230\u7684\u9057\u61be\u754c\u5b58\u5728\u77db\u76fe\uff1a\u65e2\u6709O(d log T)\u7684\u754c\uff0c\u4e5f\u6709\u6307\u6570\u7ea7exp(O(d log d))\u7684\u754c\uff0c\u800c\u03a9(d)\u7684\u4e0b\u754c\u5df2\u77e5\u3002\u662f\u5426\u5b58\u5728\u5173\u4e8e\u7ef4\u5ea6d\u7684\u591a\u9879\u5f0f\u6709\u9650\u9057\u61be\u754c\u6210\u4e3a\u672a\u89e3\u96be\u9898\uff0c\u8fd9\u5bf9\u7406\u8bba\u53d1\u5c55\u548c\u5b9e\u9645\u5e94\u7528\u90fd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7ed3\u5408M-\u51f8\u96c6\u4e0a\u6700\u4f18\u89e3\u7684\u7ed3\u6784\u7279\u5f81\u523b\u753b\u4e0e\u51e0\u4f55\u4f53\u79ef\u8bba\u8bc1\u6280\u672f\uff0c\u8bbe\u8ba1\u5b66\u4e60\u7b97\u6cd5\uff1b\u9488\u5bf9\u635f\u574f\u53cd\u9988\uff0c\u901a\u8fc7\u76d1\u63a7\u89c2\u5bdf\u53cd\u9988\u8bf1\u5bfc\u7684\u6709\u5411\u56fe\u6765\u81ea\u9002\u5e94\u68c0\u6d4b\u5bf9\u6297\u6027\u635f\u574f\u3002", "result": "1) \u5bf9\u4e8eM-\u51f8\u53ef\u884c\u96c6\uff08\u5305\u542b\u62df\u9635\u7b49\u5e7f\u6cdb\u7c7b\u522b\uff09\uff0c\u83b7\u5f97O(d log d)\u7684\u6709\u9650\u9057\u61be\u754c\uff1b2) \u5bf9\u4e8e\u6700\u591aC\u8f6e\u5bf9\u6297\u635f\u574f\u7684\u53cd\u9988\uff0c\u65e0\u9700\u9884\u5148\u77e5\u9053C\uff0c\u83b7\u5f97O((C+1)d log d)\u7684\u9057\u61be\u754c\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u90e8\u5206\u89e3\u51b3\u4e86\u516c\u5f00\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5728\u7ed3\u6784\u5316\u7684M-\u51f8\u96c6\u4e0a\u591a\u9879\u5f0f\u9057\u61be\u754c\u662f\u53ef\u8fbe\u7684\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u635f\u574f\u68c0\u6d4b\u673a\u5236\u589e\u5f3a\u4e86\u7b97\u6cd5\u5bf9\u5bf9\u6297\u6027\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002"}}
{"id": "2602.01777", "categories": ["cs.LG", "cs.AI", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01777", "abs": "https://arxiv.org/abs/2602.01777", "authors": ["M. Arashi", "M. Amintoosi"], "title": "Stein-Rule Shrinkage for Stochastic Gradient Estimation in High Dimensions", "comment": null, "summary": "Stochastic gradient methods are central to large-scale learning, yet their analysis typically treats mini-batch gradients as unbiased estimators of the population gradient. In high-dimensional settings, however, classical results from statistical decision theory show that unbiased estimators are generally inadmissible under quadratic loss, suggesting that standard stochastic gradients may be suboptimal from a risk perspective. In this work, we formulate stochastic gradient computation as a high-dimensional estimation problem and introduce a decision-theoretic framework based on Stein-rule shrinkage. We construct a shrinkage gradient estimator that adaptively contracts noisy mini-batch gradients toward a stable restricted estimator derived from historical momentum. The shrinkage intensity is determined in a data-driven manner using an online estimate of gradient noise variance, leveraging second-moment statistics commonly maintained by adaptive optimization methods. Under a Gaussian noise model and for dimension p>=3, we show that the proposed estimator uniformly dominates the standard stochastic gradient under squared error loss and is minimax-optimal in the classical decision-theoretic sense. We further demonstrate how this estimator can be incorporated into the Adam optimizer, yielding a practical algorithm with negligible additional computational cost. Empirical evaluations on CIFAR10 and CIFAR100, across multiple levels of label noise, show consistent improvements over Adam in the large-batch regime. Ablation studies indicate that the gains arise primarily from selectively applying shrinkage to high-dimensional convolutional layers, while indiscriminate shrinkage across all parameters degrades performance. These results illustrate that classical shrinkage principles provide a principled and effective approach to improving stochastic gradient estimation in modern deep learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8eStein\u6536\u7f29\u7684\u68af\u5ea6\u4f30\u8ba1\u5668\uff0c\u81ea\u9002\u5e94\u6536\u7f29\u5c0f\u6279\u91cf\u68af\u5ea6\u81f3\u52a8\u91cf\u4f30\u8ba1\uff0c\u7406\u8bba\u8bc1\u660e\u5176\u5728\u9ad8\u7ef4\u4e0b\u4f18\u4e8e\u6807\u51c6\u968f\u673a\u68af\u5ea6\uff0c\u5b9e\u9645\u96c6\u6210\u5230Adam\u540e\u5728CIFAR\u6570\u636e\u96c6\u4e0a\u6301\u7eed\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u6807\u51c6\u968f\u673a\u68af\u5ea6\u5c06\u5c0f\u6279\u91cf\u68af\u5ea6\u89c6\u4e3a\u65e0\u504f\u4f30\u8ba1\uff0c\u4f46\u9ad8\u7ef4\u8bbe\u5b9a\u4e0b\u65e0\u504f\u4f30\u8ba1\u5728\u4e8c\u6b21\u635f\u5931\u4e0b\u901a\u5e38\u4e0d\u53ef\u63a5\u53d7\uff0c\u5b58\u5728\u6b21\u4f18\u98ce\u9669\u3002", "method": "\u5c06\u968f\u673a\u68af\u5ea6\u8ba1\u7b97\u89c6\u4e3a\u9ad8\u7ef4\u4f30\u8ba1\u95ee\u9898\uff0c\u6784\u5efa\u57fa\u4e8eStein\u89c4\u5219\u6536\u7f29\u7684\u51b3\u7b56\u7406\u8bba\u6846\u67b6\uff0c\u8bbe\u8ba1\u81ea\u9002\u5e94\u6536\u7f29\u4f30\u8ba1\u5668\uff0c\u5229\u7528\u5728\u7ebf\u566a\u58f0\u65b9\u5dee\u4f30\u8ba1\u786e\u5b9a\u6536\u7f29\u5f3a\u5ea6\u3002", "result": "\u7406\u8bba\uff1a\u5728\u9ad8\u65af\u566a\u58f0\u548cp\u22653\u6761\u4ef6\u4e0b\uff0c\u8be5\u4f30\u8ba1\u5668\u5728\u5e73\u65b9\u8bef\u5dee\u635f\u5931\u4e0b\u5747\u5300\u4f18\u4e8e\u6807\u51c6\u968f\u673a\u68af\u5ea6\u4e14\u6781\u5c0f\u6781\u5927\u6700\u4f18\u3002\u5b9e\u8bc1\uff1a\u96c6\u6210\u5230Adam\u540e\u5728CIFAR10/100\u5927\u6279\u91cf\u8bad\u7ec3\u4e2d\u6301\u7eed\u8d85\u8d8aAdam\uff0c\u589e\u76ca\u4e3b\u8981\u6765\u81ea\u5bf9\u9ad8\u7ef4\u5377\u79ef\u5c42\u7684selective\u6536\u7f29\u3002", "conclusion": "\u7ecf\u5178\u6536\u7f29\u539f\u7406\u4e3a\u6539\u8fdb\u6df1\u5ea6\u5b66\u4e60\u968f\u673a\u68af\u5ea6\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u517c\u5177\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u8df5\u4ef7\u503c\u3002"}}
{"id": "2602.01826", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01826", "abs": "https://arxiv.org/abs/2602.01826", "authors": ["Yaxiang Zhang", "Yingru Li", "Jiacai Liu", "Jiawei Xu", "Ziniu Li", "Qian Liu", "Haoyuan Li"], "title": "Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It", "comment": null, "summary": "Reinforcement Learning (RL) for training Large Language Models is notoriously unstable. While recent studies attribute this to \"training inference mismatch stemming\" from inconsistent hybrid engines, standard remedies, such as Importance Sampling, might fail during extended training runs. In this work, we analyze this instability through the lens of optimization, demonstrating that gradient noise and training-inference mismatch escalate in tandem as training progresses. Meanwhile, we find that the mismatch can be effectively suppressed by shrinking the update size. Taken together, we deduce that the mismatch is not merely a static numerical discrepancy, but a dynamic failure coupled with the model's optimization. Based on this insight, we propose a simple yet effective solution: a specialized Learning Rate (LR) scheduler. Instead of pre-defined decay schedule in traditional LR scheduler, our method dynamically triggers LR decay based on response length, which we identify as a reliable early-warning signal for impending instability. Empirical evidence suggests that by reducing the learning rate as gradient noise rises, we can consistently stabilize RL training and keep the training-inference mismatch at a safe level.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u54cd\u5e94\u957f\u5ea6\u52a8\u6001\u89e6\u53d1\u5b66\u4e60\u7387\u8870\u51cf\u7684\u8c03\u5ea6\u5668\uff0c\u901a\u8fc7\u6291\u5236\u68af\u5ea6\u566a\u58f0\u548c\u8bad\u7ec3-\u63a8\u7406\u5931\u914d\u6765\u7a33\u5b9a\u8bad\u7ec3\u8fc7\u7a0b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8bad\u7ec3\u4e2d\u6781\u4e0d\u7a33\u5b9a\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u91cd\u8981\u6027\u91c7\u6837\uff09\u5728\u957f\u671f\u8bad\u7ec3\u4e2d\u53ef\u80fd\u5931\u6548\uff0c\u5176\u6839\u672c\u539f\u56e0\u88ab\u8ba4\u4e3a\u662f\u8bad\u7ec3-\u63a8\u7406\u5931\u914d\u548c\u68af\u5ea6\u566a\u58f0\u968f\u8bad\u7ec3\u8fdb\u7a0b\u5171\u540c\u6076\u5316\u3002", "method": "\u4ece\u4f18\u5316\u89d2\u5ea6\u5206\u6790\u4e0d\u7a33\u5b9a\u6027\uff0c\u63ed\u793a\u68af\u5ea6\u566a\u58f0\u4e0e\u8bad\u7ec3-\u63a8\u7406\u5931\u914d\u7684\u52a8\u6001\u5173\u8054\uff1b\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u54cd\u5e94\u957f\u5ea6\uff08\u4f5c\u4e3a\u4e0d\u7a33\u5b9a\u6027\u9884\u8b66\u4fe1\u53f7\uff09\u52a8\u6001\u89e6\u53d1\u5b66\u4e60\u7387\u8870\u51cf\u7684\u4e13\u7528\u8c03\u5ea6\u5668\u3002", "result": "\u901a\u8fc7\u964d\u4f4e\u5b66\u4e60\u7387\u6765\u5e94\u5bf9\u4e0a\u5347\u7684\u68af\u5ea6\u566a\u58f0\uff0c\u8be5\u65b9\u6cd5\u80fd\u6301\u7eed\u7a33\u5b9aRL\u8bad\u7ec3\uff0c\u5e76\u5c06\u8bad\u7ec3-\u63a8\u7406\u5931\u914d\u63a7\u5236\u5728\u5b89\u5168\u6c34\u5e73\u3002", "conclusion": "\u8bad\u7ec3-\u63a8\u7406\u5931\u914d\u4e0d\u4ec5\u662f\u9759\u6001\u6570\u503c\u5dee\u5f02\uff0c\u66f4\u662f\u4e0e\u6a21\u578b\u4f18\u5316\u8026\u5408\u7684\u52a8\u6001\u5931\u6548\uff1b\u57fa\u4e8e\u54cd\u5e94\u957f\u5ea6\u7684\u52a8\u6001\u5b66\u4e60\u7387\u8c03\u5ea6\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01703", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01703", "abs": "https://arxiv.org/abs/2602.01703", "authors": ["Pengyu Li", "Lingling Zhang", "Zhitao Gao", "Yanrui Wu", "Yuxuan Dong", "Huan Liu", "Bifan Wei", "Jun Liu"], "title": "$\\textbf{AGT$^{AO}$}$: Robust and Stabilized LLM Unlearning via Adversarial Gating Training with Adaptive Orthogonality", "comment": null, "summary": "While Large Language Models (LLMs) have achieved remarkable capabilities, they unintentionally memorize sensitive data, posing critical privacy and security risks. Machine unlearning is pivotal for mitigating these risks, yet existing paradigms face a fundamental dilemma: aggressive unlearning often induces catastrophic forgetting that degrades model utility, whereas conservative strategies risk superficial forgetting, leaving models vulnerable to adversarial recovery. To address this trade-off, we propose $\\textbf{AGT$^{AO}$}$ (Adversarial Gating Training with Adaptive Orthogonality), a unified framework designed to reconcile robust erasure with utility preservation. Specifically, our approach introduces $\\textbf{Adaptive Orthogonality (AO)}$ to dynamically mitigate geometric gradient conflicts between forgetting and retention objectives, thereby minimizing unintended knowledge degradation. Concurrently, $\\textbf{Adversarial Gating Training (AGT)}$ formulates unlearning as a latent-space min-max game, employing a curriculum-based gating mechanism to simulate and counter internal recovery attempts. Extensive experiments demonstrate that $\\textbf{AGT$^{AO}$}$ achieves a superior trade-off between unlearning efficacy (KUR $\\approx$ 0.01) and model utility (MMLU 58.30). Code is available at https://github.com/TiezMind/AGT-unlearning.", "AI": {"tldr": "\u63d0\u51faAGT^AO\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6b63\u4ea4\u6027\u548c\u5bf9\u6297\u95e8\u63a7\u8bad\u7ec3\u5e73\u8861LLM\u673a\u5668\u9057\u5fd8\u7684\u9c81\u68d2\u64e6\u9664\u4e0e\u6548\u7528\u4fdd\u6301", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u65e0\u610f\u8bb0\u5fc6\u654f\u611f\u6570\u636e\u5e26\u6765\u9690\u79c1\u5b89\u5168\u98ce\u9669\uff0c\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u9762\u4e34\u4e24\u96be\uff1a\u6fc0\u8fdb\u9057\u5fd8\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\u964d\u4f4e\u6a21\u578b\u6548\u7528\uff0c\u4fdd\u5b88\u7b56\u7565\u5219\u9020\u6210\u8868\u9762\u9057\u5fd8\u4ecd\u6613\u53d7\u5bf9\u6297\u6062\u590d\u653b\u51fb", "method": "AGT^AO\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u81ea\u9002\u5e94\u6b63\u4ea4\u6027(AO)\u52a8\u6001\u7f13\u89e3\u9057\u5fd8\u4e0e\u4fdd\u7559\u76ee\u6807\u95f4\u7684\u51e0\u4f55\u68af\u5ea6\u51b2\u7a81\uff1b2) \u5bf9\u6297\u95e8\u63a7\u8bad\u7ec3(AGT)\u5c06\u9057\u5fd8\u5efa\u6a21\u4e3a\u6f5c\u7a7a\u95f4\u6781\u5c0f\u6781\u5927\u535a\u5f08\uff0c\u91c7\u7528\u8bfe\u7a0b\u95e8\u63a7\u673a\u5236\u6a21\u62df\u5e76\u5bf9\u6297\u5185\u90e8\u6062\u590d\u5c1d\u8bd5", "result": "\u5728\u9057\u5fd8\u6548\u679c(KUR\u22480.01)\u548c\u6a21\u578b\u6548\u7528(MMLU 58.30)\u4e4b\u95f4\u5b9e\u73b0\u4f18\u5f02\u5e73\u8861\uff0c\u6e90\u4ee3\u7801\u5df2\u5f00\u6e90", "conclusion": "AGT^AO\u6210\u529f\u8c03\u548c\u4e86\u9c81\u68d2\u64e6\u9664\u4e0e\u6548\u7528\u4fdd\u6301\u7684\u77db\u76fe\uff0c\u4e3aLLM\u673a\u5668\u9057\u5fd8\u63d0\u4f9b\u4e86\u7edf\u4e00\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.01839", "categories": ["cs.LG", "cs.AI", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2602.01839", "abs": "https://arxiv.org/abs/2602.01839", "authors": ["Ru Zhang", "Xunkai Li", "Yaxin Deng", "Sicheng Liu", "Daohan Su", "Qiangqiang Dai", "Hongchao Qin", "Rong-Hua Li", "Guoren Wang", "Jia Li"], "title": "DOGMA: Weaving Structural Information into Data-centric Single-cell Transcriptomics Analysis", "comment": "12 pages, 4 figures", "summary": "Recently, data-centric AI methodology has been a dominant paradigm in single-cell transcriptomics analysis, which treats data representation rather than model complexity as the fundamental bottleneck. In the review of current studies, earlier sequence methods treat cells as independent entities and adapt prevalent ML models to analyze their directly inherited sequence data. Despite their simplicity and intuition, these methods overlook the latent intercellular relationships driven by the functional mechanisms of biological systems and the inherent quality issues of the raw sequence data. Therefore, a series of structured methods has emerged. Although they employ various heuristic rules to capture intricate intercellular relationships and enhance the raw sequencing data, these methods often neglect biological prior knowledge. This omission incurs substantial overhead and yields suboptimal graph representations, thereby hindering the utility of ML models.\n  To address them, we propose DOGMA, a holistic data-centric framework designed for the structural reshaping and semantic enhancement of raw data through multi-level biological prior knowledge. Transcending reliance on stochastic heuristics, DOGMA redefines graph construction by integrating Statistical Anchors with Cell Ontology and Phylogenetic Trees to enable deterministic structure discovery and robust cross-species alignment. Furthermore, Gene Ontology is utilized to bridge the feature-level semantic gap by incorporating functional priors. In complex multi-species and multi-organ benchmarks, DOGMA achieves SOTA performance, exhibiting superior zero-shot robustness and sample efficiency while operating with significantly lower computational cost.", "AI": {"tldr": "DOGMA\u6846\u67b6\u901a\u8fc7\u6574\u5408\u591a\u5c42\u6b21\u751f\u7269\u5b66\u5148\u9a8c\u77e5\u8bc6\uff08\u7ec6\u80de\u672c\u4f53\u3001\u7cfb\u7edf\u53d1\u80b2\u6811\u3001\u57fa\u56e0\u672c\u4f53\uff09\uff0c\u66ff\u4ee3\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u5b9e\u73b0\u5355\u7ec6\u80de\u8f6c\u5f55\u7ec4\u6570\u636e\u7684\u786e\u5b9a\u6027\u7ed3\u6784\u53d1\u73b0\u548c\u8bed\u4e49\u589e\u5f3a\uff0c\u5728\u591a\u7269\u79cd\u591a\u5668\u5b98\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u5355\u7ec6\u80de\u5206\u6790\u65b9\u6cd5\u6216\u5ffd\u89c6\u7ec6\u80de\u95f4\u5173\u7cfb\u548c\u539f\u59cb\u6570\u636e\u8d28\u91cf\u95ee\u9898\uff0c\u6216\u4f9d\u8d56\u542f\u53d1\u5f0f\u89c4\u5219\u800c\u5ffd\u7565\u751f\u7269\u5b66\u5148\u9a8c\u77e5\u8bc6\uff0c\u5bfc\u81f4\u56fe\u8868\u793a\u6b21\u4f18\u4e14\u8ba1\u7b97\u5f00\u9500\u5927\u3002", "method": "\u63d0\u51faDOGMA\u6570\u636e\u4e3a\u4e2d\u5fc3\u6846\u67b6\uff1a1) \u6574\u5408\u7edf\u8ba1\u951a\u70b9\u3001\u7ec6\u80de\u672c\u4f53\u548c\u7cfb\u7edf\u53d1\u80b2\u6811\u5b9e\u73b0\u786e\u5b9a\u6027\u7ed3\u6784\u53d1\u73b0\u548c\u8de8\u7269\u79cd\u5bf9\u9f50\uff1b2) \u5229\u7528\u57fa\u56e0\u672c\u4f53\u5f15\u5165\u529f\u80fd\u5148\u9a8c\u4ee5\u6865\u63a5\u7279\u5f81\u7ea7\u8bed\u4e49\u9e3f\u6c9f\u3002", "result": "\u5728\u590d\u6742\u591a\u7269\u79cd\u591a\u5668\u5b98\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0SOTA\u6027\u80fd\uff0c\u5177\u5907\u5353\u8d8a\u7684\u96f6\u6837\u672c\u9c81\u68d2\u6027\u548c\u6837\u672c\u6548\u7387\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u6574\u5408\u591a\u5c42\u6b21\u751f\u7269\u5b66\u5148\u9a8c\u77e5\u8bc6\uff0cDOGMA\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5355\u7ec6\u80de\u8f6c\u5f55\u7ec4\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u6570\u636e\u91cd\u5851\u548c\u8bed\u4e49\u589e\u5f3a\u6846\u67b6\u3002"}}
{"id": "2602.01855", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.01855", "abs": "https://arxiv.org/abs/2602.01855", "authors": ["Blagoj Hristov", "Hristijan Gjoreski", "Vesna Ojleska Latkoska", "Gorjan Nadzinski"], "title": "Time2Vec-Integrated Transformer for Robust Gesture Recognition from Low-Density sEMG", "comment": null, "summary": "Accurate and responsive myoelectric prosthesis control typically relies on complex, dense multi-sensor arrays, which limits consumer accessibility. This paper presents a novel, data-efficient deep learning framework designed to achieve precise and accurate control using minimal sensor hardware. Leveraging an external dataset of 8 subjects, our approach implements a hybrid Transformer optimized for sparse, two-channel surface electromyography (sEMG). Unlike standard architectures that use fixed positional encodings, we integrate Time2Vec learnable temporal embeddings to capture the stochastic temporal warping inherent in biological signals. Furthermore, we employ a normalized additive fusion strategy that aligns the latent distributions of spatial and temporal features, preventing the destructive interference common in standard implementations. A two-stage curriculum learning protocol is utilized to ensure robust feature extraction despite data scarcity. The proposed architecture achieves a state-of-the-art multi-subject F1-score of 95.7% $\\pm$ 0.20% for a 10-class movement set, statistically outperforming both a standard Transformer with fixed encodings and a recurrent CNN-LSTM model. Architectural optimization reveals that a balanced allocation of model capacity between spatial and temporal dimensions yields the highest stability. Furthermore, while direct transfer to a new unseen subject led to poor accuracy due to domain shifts, a rapid calibration protocol utilizing only two trials per gesture recovered performance from 21.0% $\\pm$ 2.98% to 96.9% $\\pm$ 0.52%. By validating that high-fidelity temporal embeddings can compensate for low spatial resolution, this work challenges the necessity of high-density sensing. The proposed framework offers a robust, cost-effective blueprint for next-generation prosthetic interfaces capable of rapid personalization.", "AI": {"tldr": "This paper presents a data-efficient deep learning framework for myoelectric prosthesis control using minimal two-channel sEMG sensors. It employs a hybrid Transformer with learnable Time2Vec temporal embeddings, normalized additive fusion, and curriculum learning to achieve 95.7% F1-score on 10-class movement classification, challenging the need for high-density sensing while enabling rapid personalization for new users.", "motivation": "Current myoelectric prosthesis control relies on complex, dense multi-sensor arrays that limit consumer accessibility. There's a need for precise and accurate control using minimal sensor hardware to make prosthetics more affordable and widely available.", "method": "The framework uses a hybrid Transformer architecture optimized for sparse two-channel sEMG. Key innovations include: (1) Time2Vec learnable temporal embeddings instead of fixed positional encodings to capture biological signal stochasticity, (2) normalized additive fusion strategy to align spatial and temporal feature distributions, and (3) two-stage curriculum learning for robust feature extraction with limited data. For new subjects, a rapid calibration protocol uses only two trials per gesture.", "result": "Achieved state-of-the-art multi-subject F1-score of 95.7% \u00b1 0.20% for 10-class movements, outperforming standard Transformer and CNN-LSTM baselines. Balanced model capacity between spatial and temporal dimensions yielded highest stability. Direct transfer to new subjects failed (21.0% accuracy) due to domain shifts, but rapid calibration recovered performance to 96.9% \u00b1 0.52%.", "conclusion": "High-fidelity temporal embeddings can compensate for low spatial resolution, challenging the necessity of high-density sensing. The framework provides a robust, cost-effective blueprint for next-generation prosthetic interfaces capable of rapid personalization, making advanced myoelectric control more accessible."}}
{"id": "2602.01718", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01718", "abs": "https://arxiv.org/abs/2602.01718", "authors": ["Sora Nakai", "Youssef Fadhloun", "Kacem Mathlouthi", "Kotaro Yoshida", "Ganesh Talluri", "Ioannis Mitliagkas", "Hiroki Naganuma"], "title": "Revisiting Generalization Measures Beyond IID: An Empirical Study under Distributional Shift", "comment": null, "summary": "Generalization remains a central yet unresolved challenge in deep learning, particularly the ability to predict a model's performance beyond its training distribution using quantities available prior to test-time evaluation. Building on the large-scale study of Jiang et al. (2020). and concerns by Dziugaite et al. (2020). about instability across training configurations, we benchmark the robustness of generalization measures beyond IID regime. We train small-to-medium models over 10,000 hyperparameter configurations and evaluate more than 40 measures computable from the trained model and the available training data alone. We significantly broaden the experimental scope along multiple axes: (i) extending the evaluation beyond the standard IID setting to include benchmarking for robustness across diverse distribution shifts, (ii) evaluating multiple architectures and training recipes, and (iii) newly incorporating calibration- and information-criteria-based measures to assess their alignment with both IID and OOD generalization. We find that distribution shifts can substantially alter the predictive performance of many generalization measures, while a smaller subset remains comparatively stable across settings.", "AI": {"tldr": "\u901a\u8fc7\u8d851\u4e07\u6b21\u8d85\u53c2\u5b9e\u9a8c\u548c40+\u79cd\u5ea6\u91cf\u8bc4\u4f30\uff0c\u53d1\u73b0\u5206\u5e03\u504f\u79fb\u4f1a\u663e\u8457\u6539\u53d8\u591a\u6570\u6cdb\u5316\u5ea6\u91cf\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u4f46\u5c11\u6570\u5ea6\u91cf\u5728\u975eIID\u73af\u5883\u4e0b\u4fdd\u6301\u76f8\u5bf9\u7a33\u5b9a", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6cdb\u5316\u80fd\u529b\u9884\u6d4b\u4ecd\u662f\u6838\u5fc3\u6311\u6218\uff0c\u5c24\u5176\u9700\u8bc4\u4f30\u6a21\u578b\u5728\u5206\u5e03\u5916(OOD)\u6570\u636e\u4e0a\u7684\u8868\u73b0\uff1b\u73b0\u6709\u7814\u7a76\u5bf9\u8bad\u7ec3\u914d\u7f6e\u4e0d\u7a33\u5b9a\u6027\u548c\u975e\u72ec\u7acb\u540c\u5206\u5e03(IID) regime\u5916\u7684\u6cdb\u5316\u5ea6\u91cf\u9c81\u68d2\u6027\u7f3a\u4e4f\u7cfb\u7edf\u9a8c\u8bc1", "method": "(1)\u6269\u5c55\u8bc4\u4f30\u81f3\u975eIID\u8bbe\u7f6e\uff0c\u6d4b\u8bd5\u591a\u6837\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u6027 (2)\u91c7\u7528\u591a\u67b6\u6784/\u8bad\u7ec3\u65b9\u6848 (3)\u65b0\u589e\u6821\u51c6\u4e0e\u4fe1\u606f\u51c6\u5219\u7c7b\u5ea6\u91cf\uff0c\u68c0\u9a8c\u5176\u4e0eIID/OOD\u6cdb\u5316\u7684\u5173\u8054", "result": "\u5206\u5e03\u504f\u79fb\u4f1a\u5927\u5e45\u6539\u53d8\u591a\u6570\u6cdb\u5316\u5ea6\u91cf\u7684\u9884\u6d4b\u6548\u679c\uff0c\u4f46\u5b58\u5728\u5b50\u96c6\u5ea6\u91cf\u5728\u4e0d\u540c\u5206\u5e03\u504f\u79fb\u573a\u666f\u4e0b\u4fdd\u6301\u8f83\u9ad8\u7a33\u5b9a\u6027", "conclusion": "\u6cdb\u5316\u5ea6\u91cf\u7684\u6709\u6548\u6027\u9ad8\u5ea6\u4f9d\u8d56\u6570\u636e\u5206\u5e03\u5047\u8bbe\uff0c\u9700\u7b5b\u9009\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u4ecd\u7a33\u5065\u7684\u5ea6\u91cf\u7528\u4e8e\u6a21\u578b\u6027\u80fd\u9884\u5224"}}
{"id": "2602.01915", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01915", "abs": "https://arxiv.org/abs/2602.01915", "authors": ["Elad Sharony", "Tom Jurgenson", "Orr Krupnik", "Dotan Di Castro", "Shie Mannor"], "title": "VLM-Guided Experience Replay", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) and Vision-Language Models (VLMs) have enabled powerful semantic and multimodal reasoning capabilities, creating new opportunities to enhance sample efficiency, high-level planning, and interpretability in reinforcement learning (RL). While prior work has integrated LLMs and VLMs into various components of RL, the replay buffer, a core component for storing and reusing experiences, remains unexplored. We propose addressing this gap by leveraging VLMs to guide the prioritization of experiences in the replay buffer. Our key idea is to use a frozen, pre-trained VLM (requiring no fine-tuning) as an automated evaluator to identify and prioritize promising sub-trajectories from the agent's experiences. Across scenarios, including game-playing and robotics, spanning both discrete and continuous domains, agents trained with our proposed prioritization method achieve 11-52% higher average success rates and improve sample efficiency by 19-45% compared to previous approaches. https://esharony.me/projects/vlm-rb/", "AI": {"tldr": "\u63d0\u51fa\u5229\u7528\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b(VLM)\u81ea\u52a8\u8bc4\u4f30\u5e76\u4f18\u5148\u9009\u62e9\u5f3a\u5316\u5b66\u4e60\u7ecf\u9a8c\u56de\u653e\u7f13\u51b2\u533a\u4e2d\u6709\u524d\u666f\u7684\u5b50\u8f68\u8ff9\uff0c\u5728 games \u548c\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6210\u529f\u7387\u63d0\u534711-52%\u3001\u6837\u672c\u6548\u7387\u63d0\u9ad819-45%\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u591a\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6838\u5fc3\u7ec4\u4ef6\u7ecf\u9a8c\u56de\u653e\u7f13\u51b2\u533a\u4ecd\u672a\u88ab\u63a2\u7d22\uff0c\u5b58\u5728\u5229\u7528\u8fd9\u4e9b\u6a21\u578b\u8fdb\u884c\u7ecf\u9a8c\u4f18\u5148\u7ea7\u6392\u5e8f\u7684\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u65e0\u9700\u5fae\u8c03\u7684\u51bb\u7ed3\u9884\u8bad\u7ec3VLM\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u4f30\u5668\uff0c\u4ece\u667a\u80fd\u4f53\u7ecf\u9a8c\u4e2d\u8bc6\u522b\u5e76\u4f18\u5148\u9009\u62e9\u6709\u524d\u666f\u7684\u5b50\u8f68\u8ff9\u6765\u6307\u5bfc\u7ecf\u9a8c\u56de\u653e\u7f13\u51b2\u533a\u7684\u4f18\u5148\u7ea7\u3002", "result": "\u5728\u5305\u62ec\u6e38\u620f\u548c\u673a\u5668\u4eba\u7684\u79bb\u6563\u4e0e\u8fde\u7eed\u4efb\u52a1\u4e2d\uff0c\u91c7\u7528\u8be5\u65b9\u6cd5\u7684\u667a\u80fd\u4f53\u5e73\u5747\u6210\u529f\u7387\u63d0\u534711-52%\uff0c\u6837\u672c\u6548\u7387\u63d0\u9ad819-45%\u3002", "conclusion": "\u5229\u7528VLM\u6307\u5bfc\u7ecf\u9a8c\u56de\u653e\u7f13\u51b2\u533a\u4f18\u5148\u7ea7\u6392\u5e8f\u662f\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u7684\u6027\u80fd\u548c\u6837\u672c\u6548\u7387\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2602.01734", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01734", "abs": "https://arxiv.org/abs/2602.01734", "authors": ["Lianhai Ren", "Yucheng Ding", "Xiao Liu", "Qianxiao Li", "Peng Cheng", "Yeyun Gong"], "title": "MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration", "comment": null, "summary": "Training instability remains a critical challenge in large language model (LLM) pretraining, often manifesting as sudden gradient explosions that waste significant computational resources. We study training failures in a 5M-parameter NanoGPT model scaled via $\u03bc$P, identifying two key phenomena preceding collapse: (1) rapid decline in weight matrix stable rank (ratio of squared Frobenius norm to squared spectral norm), and (2) increasing alignment between adjacent layer Jacobians. We prove theoretically that these two conditions jointly cause exponential gradient norm growth with network depth. To break this instability mechanism, we propose MSign, a new optimizer that periodically applies matrix sign operations to restore stable rank. Experiments on models from 5M to 3B parameters demonstrate that MSign effectively prevents training failures with a computational overhead of less than 7.0%.", "AI": {"tldr": "\u9488\u5bf9LLM\u9884\u8bad\u7ec3\u4e2d\u68af\u5ea6\u7206\u70b8\u5bfc\u81f4\u7684\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u95ee\u9898\uff0c\u672c\u7814\u7a76\u901a\u8fc7\u03bcP\u6807\u5ea65M\u53c2\u6570NanoGPT\u6a21\u578b\u53d1\u73b0\u8bad\u7ec3\u5d29\u6e83\u524d\u5b58\u5728\u6743\u91cd\u77e9\u9635\u7a33\u5b9a\u79e9\u5feb\u901f\u4e0b\u964d\u548c\u76f8\u90bb\u5c42\u96c5\u53ef\u6bd4\u77e9\u9635\u5bf9\u9f50\u5ea6\u589e\u52a0\u4e24\u5927\u7279\u5f81\uff0c\u7406\u8bba\u8bc1\u660e\u4e8c\u8005\u5171\u540c\u5bfc\u81f4\u68af\u5ea6\u8303\u6570\u968f\u6df1\u5ea6\u6307\u6570\u589e\u957f\uff0c\u5e76\u63d0\u51fa\u5468\u671f\u6027\u77e9\u9635\u7b26\u53f7\u64cd\u4f5c\u7684MSign\u4f18\u5316\u5668\uff0c\u57285M-3B\u53c2\u6570\u89c4\u6a21\u4e0a\u6709\u6548\u9632\u6b62\u8bad\u7ec3\u5931\u8d25\u4e14\u8ba1\u7b97\u5f00\u9500\u4f4e\u4e8e7%\u3002", "motivation": "\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u9762\u4e34\u4e25\u91cd\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5e38\u8868\u73b0\u4e3a\u7a81\u53d1\u6027\u68af\u5ea6\u7206\u70b8\uff0c\u9020\u6210\u5de8\u5927\u7684\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\uff0c\u8fd9\u662f\u4e9f\u5f85\u89e3\u51b3\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u901a\u8fc7\u03bcP\u6807\u5ea6\u65b9\u6cd5\u7814\u7a765M\u53c2\u6570NanoGPT\u6a21\u578b\u7684\u8bad\u7ec3\u5931\u8d25\u73b0\u8c61\uff0c\u8bc6\u522b\u51fa\u6743\u91cd\u77e9\u9635\u7a33\u5b9a\u79e9\u4e0b\u964d\u548c\u5c42\u95f4\u96c5\u53ef\u6bd4\u77e9\u9635\u5bf9\u9f50\u4e24\u5927\u524d\u5146\u7279\u5f81\uff0c\u4ece\u7406\u8bba\u4e0a\u8bc1\u660e\u5176\u5bfc\u81f4\u68af\u5ea6\u6307\u6570\u589e\u957f\u7684\u673a\u5236\uff0c\u5e76\u8bbe\u8ba1\u5468\u671f\u6027\u5e94\u7528\u77e9\u9635\u7b26\u53f7\u64cd\u4f5c\u6765\u6062\u590d\u7a33\u5b9a\u79e9\u7684MSign\u4f18\u5316\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMSign\u4f18\u5316\u5668\u57285M\u81f33B\u53c2\u6570\u89c4\u6a21\u7684\u6a21\u578b\u4e0a\u5747\u80fd\u6709\u6548\u9884\u9632\u8bad\u7ec3\u5931\u8d25\uff0c\u8ba1\u7b97\u5f00\u9500\u4f4e\u4e8e7.0%\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u68af\u5ea6\u7206\u70b8\u95ee\u9898\u3002", "conclusion": "\u6743\u91cd\u77e9\u9635\u7a33\u5b9a\u79e9\u5feb\u901f\u4e0b\u964d\u4e0e\u76f8\u90bb\u5c42\u96c5\u53ef\u6bd4\u77e9\u9635\u5bf9\u9f50\u5171\u540c\u6784\u6210\u8bad\u7ec3\u5d29\u6e83\u7684\u6839\u672c\u673a\u5236\uff0cMSign\u901a\u8fc7\u5468\u671f\u6027\u77e9\u9635\u7b26\u53f7\u64cd\u4f5c\u6709\u6548\u6253\u7834\u8be5\u673a\u5236\uff0c\u4e3a\u7a33\u5b9a\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01935", "categories": ["cs.LG", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.01935", "abs": "https://arxiv.org/abs/2602.01935", "authors": ["Annabelle Sujun Tang", "Christopher Priebe", "Lianhui Qin", "Hadi Esmaeilzadeh"], "title": "COLT: Lightweight Multi-LLM Collaboration through Shared MCTS Reasoning for Model Compilation", "comment": null, "summary": "Model serving costs dominate AI systems, making compiler optimization essential for scalable deployment. Recent works show that a large language model (LLM) can guide compiler search by reasoning over program structure and optimization history. However, using a single large model throughout the search is expensive, while smaller models are less reliable when used alone. Thus, this paper seeks to answer whether multi-LLM collaborative reasoning relying primarily on small LLMs can match or exceed the performance of a single large model. As such, we propose a lightweight collaborative multi-LLM framework, dubbed COLT, for compiler optimization that enables coordinated reasoning across multiple models within a single Monte Carlo tree search (MCTS) process. A key contribution is the use of a single shared MCTS tree as the collaboration substrate across LLMs, enabling the reuse of transformation prefixes and cross-model value propagation. Hence, we circumvent both heavy internal reasoning mechanisms and conventional agentic machinery that relies on external planners, multiple concurrent LLMs, databases, external memory/versioning of intermediate results, and controllers by simply endogenizing model selection within the lightweight MCTS optimization loop. Every iteration, the acting LLM proposes a joint action: (compiler transformation, model to be queried next). We also introduce a model-aware tree policy that biases search toward smaller models while preserving exploration, and a course-alteration mechanism that escalates to the largest model when the search exhibits persistent regressions attributable to smaller models.", "AI": {"tldr": "This paper proposes COLT, a lightweight multi-LLM collaborative framework using shared Monte Carlo Tree Search for compiler optimization, where small models handle most decisions with dynamic escalation to larger models when needed, aiming to reduce costs while matching large-model performance.", "motivation": "Model serving costs dominate AI systems; while LLMs can guide compiler optimization, single large models are expensive and smaller models are unreliable alone. It's unclear whether collaborative multi-LLM approaches using primarily small models can match large-model performance.", "method": "The authors propose COLT, which uses a single shared MCTS tree as collaboration substrate across LLMs. Each iteration, an LLM proposes both a compiler transformation and the next model to query. It includes a model-aware tree policy that biases toward smaller models while preserving exploration, and a course-alteration mechanism that escalates to the largest model when persistent regressions occur with smaller models.", "result": "The abstract does not present empirical results or performance comparisons. It focuses on describing the COLT framework architecture, emphasizing that it endogenizes model selection within the MCTS loop to avoid external planners, databases, and heavy reasoning machinery.", "conclusion": "COLT enables efficient multi-LLM collaboration for compiler optimization by primarily leveraging small models for cost efficiency while dynamically escalating to larger models when needed, offering a lightweight alternative to complex agentic systems."}}
{"id": "2602.01936", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01936", "abs": "https://arxiv.org/abs/2602.01936", "authors": ["Abdul Joseph Fofanah", "Lian Wen", "David Chen"], "title": "PIMCST: Physics-Informed Multi-Phase Consensus and Spatio-Temporal Few-Shot Learning for Traffic Flow Forecasting", "comment": null, "summary": "Accurate traffic flow prediction remains a fundamental challenge in intelligent transportation systems, particularly in cross-domain, data-scarce scenarios where limited historical data hinders model training and generalisation. The complex spatio-temporal dependencies and nonlinear dynamics of urban mobility networks further complicate few-shot learning across different cities. This paper proposes MCPST, a novel Multi-phase Consensus Spatio-Temporal framework for few-shot traffic forecasting that reconceptualises traffic prediction as a multi-phase consensus learning problem. Our framework introduces three core innovations: (1) a multi-phase engine that models traffic dynamics through diffusion, synchronisation, and spectral embeddings for comprehensive dynamic characterisation; (2) an adaptive consensus mechanism that dynamically fuses phase-specific predictions while enforcing consistency; and (3) a structured meta-learning strategy for rapid adaptation to new cities with minimal data. We establish extensive theoretical guarantees, including representation theorems with bounded approximation errors and generalisation bounds for few-shot adaptation. Through experiments on four real-world datasets, MCPST outperforms fourteen state-of-the-art methods in spatio-temporal graph learning methods, dynamic graph transfer learning methods, prompt-based spatio-temporal prediction methods and cross-domain few-shot settings, improving prediction accuracy while reducing required training data and providing interpretable insights. The implementation code is available at https://github.com/afofanah/MCPST.", "AI": {"tldr": "\u63d0\u51faMCPST\u6846\u67b6\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684\u8de8\u57ce\u5e02\u4ea4\u901a\u6d41\u91cf\u9884\u6d4b\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u5171\u8bc6\u5b66\u4e60\u673a\u5236\u878d\u5408\u6269\u6563\u3001\u540c\u6b65\u548c\u8c31\u5d4c\u5165\u4e09\u79cd\u52a8\u529b\u5b66\u5efa\u6a21\u65b9\u5f0f\uff0c\u7ed3\u5408\u5143\u5b66\u4e60\u5b9e\u73b0\u5c0f\u6837\u672c\u5feb\u901f\u9002\u5e94\uff0c\u5728\u56db\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e14\u79cd\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\uff0c\u8de8\u57df\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684\u4ea4\u901a\u6d41\u91cf\u9884\u6d4b\u9762\u4e34\u5386\u53f2\u6570\u636e\u6709\u9650\u3001\u65f6\u7a7a\u4f9d\u8d56\u590d\u6742\u548c\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7b49\u6311\u6218\uff0c\u963b\u788d\u4e86\u6a21\u578b\u8bad\u7ec3\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u591a\u9636\u6bb5\u5171\u8bc6\u65f6\u7a7a\u6846\u67b6MCPST\uff0c\u5305\u542b\uff1a(1)\u591a\u9636\u6bb5\u5f15\u64ce\u901a\u8fc7\u6269\u6563\u3001\u540c\u6b65\u548c\u8c31\u5d4c\u5165\u5efa\u6a21\u4ea4\u901a\u52a8\u529b\u5b66\uff1b(2)\u81ea\u9002\u5e94\u5171\u8bc6\u673a\u5236\u52a8\u6001\u878d\u5408\u9636\u6bb5\u9884\u6d4b\u5e76\u4fdd\u8bc1\u4e00\u81f4\u6027\uff1b(3)\u7ed3\u6784\u5316\u5143\u5b66\u4e60\u7b56\u7565\u5b9e\u73b0\u65b0\u57ce\u5e02\u7684\u5c0f\u6837\u672c\u5feb\u901f\u9002\u5e94\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\uff0cMCPST\u5728\u65f6\u7a7a\u56fe\u5b66\u4e60\u3001\u52a8\u6001\u56fe\u8fc1\u79fb\u5b66\u4e60\u3001\u57fa\u4e8e\u63d0\u793a\u7684\u9884\u6d4b\u548c\u8de8\u57df\u5c0f\u6837\u672c\u8bbe\u7f6e\u4e2d\u5747\u4f18\u4e8e14\u79cd\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u7684\u540c\u65f6\u51cf\u5c11\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u6d1e\u5bdf\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7406\u8bba\u4fdd\u8bc1\uff08\u6709\u754c\u8fd1\u4f3c\u8bef\u5dee\u548c\u6cdb\u5316\u8fb9\u754c\uff09\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u4e3a\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684\u8de8\u57ce\u5e02\u4ea4\u901a\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01937", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01937", "abs": "https://arxiv.org/abs/2602.01937", "authors": ["Suhan Guo", "Bingxu Wang", "Shaodan Zhang", "Furao Shen"], "title": "T-LLM: Teaching Large Language Models to Forecast Time Series via Temporal Distillation", "comment": null, "summary": "Time series forecasting plays a critical role in decision-making across many real-world applications. Unlike data in vision and language domains, time series data is inherently tied to the evolution of underlying processes and can only accumulate as real-world time progresses, limiting the effectiveness of scale-driven pretraining alone. This time-bound constraint poses a challenge for enabling large language models (LLMs) to acquire forecasting capability, as existing approaches primarily rely on representation-level alignment or inference-time temporal modules rather than explicitly teaching forecasting behavior to the LLM. We propose T-LLM, a temporal distillation framework that equips general-purpose LLMs with time series forecasting capability by transferring predictive behavior from a lightweight temporal teacher during training. The teacher combines trend modeling and frequency-domain analysis to provide structured temporal supervision, and is removed entirely at inference, leaving the LLM as the sole forecasting model. Experiments on benchmark datasets and infectious disease forecasting tasks demonstrate that T-LLM consistently outperforms existing LLM-based forecasting methods under full-shot, few-shot, and zero-shot settings, while enabling a simple and efficient deployment pipeline.", "AI": {"tldr": "\u63d0\u51faT-LLM\u65f6\u5e8f\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6559\u5e08\u6a21\u578b\uff08\u7ed3\u5408\u8d8b\u52bf\u5efa\u6a21\u4e0e\u9891\u57df\u5206\u6790\uff09\u5728\u8bad\u7ec3\u9636\u6bb5\u5411LLM\u8f6c\u79fb\u9884\u6d4b\u884c\u4e3a\uff0c\u63a8\u7406\u65f6\u4ec5\u4fdd\u7559LLM\uff0c\u5728\u591a\u79cd\u6837\u672c\u8bbe\u5b9a\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u5728\u51b3\u7b56\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6570\u636e\u7684\u65f6\u5e8f\u7ea6\u675f\u9650\u5236\u4e86LLM\u9884\u8bad\u7ec3\u6548\u679c\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8868\u793a\u5bf9\u9f50\u6216\u63a8\u7406\u65f6\u6a21\u5757\uff0c\u672a\u80fd\u663e\u5f0f\u6559\u6388LLM\u9884\u6d4b\u884c\u4e3a", "method": "T-LLM\u6846\u67b6\uff1a\u8bad\u7ec3\u65f6\u7528\u65f6\u5e8f\u6559\u5e08\u6a21\u578b\uff08\u8d8b\u52bf\u5efa\u6a21+\u9891\u57df\u5206\u6790\uff09\u63d0\u4f9b\u7ed3\u6784\u5316\u76d1\u7763\uff0c\u84b8\u998f\u9884\u6d4b\u884c\u4e3a\u81f3LLM\uff1b\u63a8\u7406\u65f6\u6559\u5e08\u88ab\u79fb\u9664\uff0c\u4ec5LLM\u4f5c\u4e3a\u9884\u6d4b\u6a21\u578b", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4f20\u67d3\u75c5\u9884\u6d4b\u4efb\u52a1\u4e0a\uff0c\u5168\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709LLM\u9884\u6d4b\u65b9\u6cd5\uff0c\u90e8\u7f72\u7b80\u5355\u9ad8\u6548", "conclusion": "T-LLM\u6210\u529f\u4e3a\u901a\u7528LLM\u8d4b\u4e88\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u80fd\u529b\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.01751", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2602.01751", "abs": "https://arxiv.org/abs/2602.01751", "authors": ["Kunyi Fan", "Mengjie Chen", "Longlong Li", "Cunquan Qu"], "title": "MGKAN: Predicting Asymmetric Drug-Drug Interactions via a Multimodal Graph Kolmogorov-Arnold Network", "comment": "Submitted to ICASSP 2026", "summary": "Predicting drug-drug interactions (DDIs) is essential for safe pharmacological treatments. Previous graph neural network (GNN) models leverage molecular structures and interaction networks but mostly rely on linear aggregation and symmetric assumptions, limiting their ability to capture nonlinear and heterogeneous patterns. We propose MGKAN, a Graph Kolmogorov-Arnold Network that introduces learnable basis functions into asymmetric DDI prediction. MGKAN replaces conventional MLP transformations with KAN-driven basis functions, enabling more expressive and nonlinear modeling of drug relationships. To capture pharmacological dependencies, MGKAN integrates three network views-an asymmetric DDI network, a co-interaction network, and a biochemical similarity network-with role-specific embeddings to preserve directional semantics. A fusion module combines linear attention and nonlinear transformation to enhance representational capacity. On two benchmark datasets, MGKAN outperforms seven state-of-the-art baselines. Ablation studies and case studies confirm its predictive accuracy and effectiveness in modeling directional drug effects.", "AI": {"tldr": "A new model called MGKAN uses Graph Kolmogorov-Arnold Networks with learnable basis functions to predict drug-drug interactions more accurately by capturing nonlinear and directional patterns that previous GNN models missed.", "motivation": "Predicting drug-drug interactions is critical for treatment safety, but existing GNN models are limited by linear aggregation and symmetric assumptions that cannot capture nonlinear and heterogeneous patterns in drug relationships.", "method": "MGKAN replaces MLPs with KAN-driven basis functions for nonlinear modeling, integrates three network views (asymmetric DDI, co-interaction, and biochemical similarity) with role-specific embeddings to preserve directionality, and uses a fusion module combining linear attention and nonlinear transformation.", "result": "MGKAN outperforms seven state-of-the-art baselines on two benchmark datasets, with ablation studies and case studies confirming its superior predictive accuracy and ability to model directional drug effects.", "conclusion": "The proposed MGKAN effectively models directional drug interactions through nonlinear representation learning, demonstrating improved predictive performance and practical utility for pharmacological safety."}}
{"id": "2602.01975", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01975", "abs": "https://arxiv.org/abs/2602.01975", "authors": ["Meng Li", "Peisong Wang", "Yuantian Shao", "Qinghao Hu", "Hongjian Fang", "Yifan Zhang", "Zhihui Wei", "Jian Cheng"], "title": "IntraSlice: Towards High-Performance Structural Pruning with Block-Intra PCA for LLMs", "comment": null, "summary": "Large Language Models (LLMs) achieve strong performance across diverse tasks but face deployment challenges due to their massive size. Structured pruning offers acceleration benefits but leads to significant performance degradation. Recent PCA-based pruning methods have alleviated this issue by retaining key activation components, but are only applied between modules in order to fuse the transformation matrix, which introduces extra parameters and severely disrupts activation distributions due to residual connections. To address these issues, we propose IntraSlice, a framework that applies block-wise module-intra PCA compression pruning. By leveraging the structural characteristics of Transformer modules, we design an approximate PCA method whose transformation matrices can be fully fused into the model without additional parameters. We also introduce a PCA-based global pruning ratio estimator that further considers the distribution of compressed activations, building on conventional module importance. We validate our method on Llama2, Llama3, and Phi series across various language benchmarks. Experimental results demonstrate that our approach achieves superior compression performance compared to recent baselines at the same compression ratio or inference speed.", "AI": {"tldr": "\u63d0\u51faIntraSlice\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u5757\u5185PCA\u538b\u7f29\u526a\u679d\u89e3\u51b3\u5927\u6a21\u578b\u90e8\u7f72\u7684\u7ed3\u6784\u5316\u526a\u679d\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u907f\u514d\u989d\u5916\u53c2\u6570\u5e76\u4f18\u5316\u6fc0\u6d3b\u5206\u5e03", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u56e0\u4f53\u79ef\u5e9e\u5927\u9762\u4e34\u90e8\u7f72\u6311\u6218\uff0c\u7ed3\u6784\u5316\u526a\u679d\u867d\u80fd\u52a0\u901f\u4f46\u5bfc\u81f4\u663e\u8457\u6027\u80fd\u4e0b\u964d\uff1b\u73b0\u6709\u57fa\u4e8ePCA\u7684\u526a\u679d\u65b9\u6cd5\u56e0\u8de8\u6a21\u5757\u5e94\u7528\u5f15\u5165\u989d\u5916\u53c2\u6570\u4e14\u7834\u574f\u6fc0\u6d3b\u5206\u5e03", "method": "1) \u8bbe\u8ba1\u6a21\u5757\u5185PCA\u538b\u7f29\u65b9\u6cd5\uff0c\u4f7f\u53d8\u6362\u77e9\u9635\u5b8c\u5168\u878d\u5408\u5230\u6a21\u578b\u4e2d\u65e0\u9700\u989d\u5916\u53c2\u6570 2) \u63d0\u51fa\u57fa\u4e8ePCA\u7684\u5168\u5c40\u526a\u679d\u6bd4\u4f8b\u4f30\u8ba1\u5668\uff0c\u8003\u8651\u538b\u7f29\u6fc0\u6d3b\u5206\u5e03", "result": "\u5728Llama2\u3001Llama3\u548cPhi\u7cfb\u5217\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u8fd1\u671f\u57fa\u7ebf\u65b9\u6cd5\u5728\u76f8\u540c\u538b\u7f29\u7387\u6216\u63a8\u7406\u901f\u5ea6\u4e0b\u83b7\u5f97\u66f4\u4f18\u538b\u7f29\u6027\u80fd", "conclusion": "IntraSlice\u6846\u67b6\u6709\u6548\u5e73\u8861\u6a21\u578b\u538b\u7f29\u4e0e\u6027\u80fd\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u65b0\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.01976", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01976", "abs": "https://arxiv.org/abs/2602.01976", "authors": ["Hongwei Yan", "Guanglong Sun", "Kanglei Zhou", "Qian Li", "Liyuan Wang", "Yi Zhong"], "title": "FlyPrompt: Brain-Inspired Random-Expanded Routing with Temporal-Ensemble Experts for General Continual Learning", "comment": "33 pages. Accepted by ICLR 2026", "summary": "General continual learning (GCL) challenges intelligent systems to learn from single-pass, non-stationary data streams without clear task boundaries. While recent advances in continual parameter-efficient tuning (PET) of pretrained models show promise, they typically rely on multiple training epochs and explicit task cues, limiting their effectiveness in GCL scenarios. Moreover, existing methods often lack targeted design and fail to address two fundamental challenges in continual PET: how to allocate expert parameters to evolving data distributions, and how to improve their representational capacity under limited supervision. Inspired by the fruit fly's hierarchical memory system characterized by sparse expansion and modular ensembles, we propose FlyPrompt, a brain-inspired framework that decomposes GCL into two subproblems: expert routing and expert competence improvement. FlyPrompt introduces a randomly expanded analytic router for instance-level expert activation and a temporal ensemble of output heads to dynamically adapt decision boundaries over time. Extensive theoretical and empirical evaluations demonstrate FlyPrompt's superior performance, achieving up to 11.23%, 12.43%, and 7.62% gains over state-of-the-art baselines on CIFAR-100, ImageNet-R, and CUB-200, respectively. Our source code is available at https://github.com/AnAppleCore/FlyGCL.", "AI": {"tldr": "\u63d0\u51faFlyPrompt\u6846\u67b6\uff0c\u901a\u8fc7\u679c\u8747\u542f\u53d1\u7684\u7a00\u758f\u6269\u5c55\u548c\u6a21\u5757\u5316\u96c6\u6210\u65b9\u6cd5\u89e3\u51b3\u901a\u7528\u6301\u7eed\u5b66\u4e60\u95ee\u9898\uff0c\u65e0\u9700\u591a\u8f6e\u8bad\u7ec3\u548c\u663e\u5f0f\u4efb\u52a1\u8fb9\u754c\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6301\u7eed\u53c2\u6570\u9ad8\u6548\u8c03\u4f18\u65b9\u6cd5\u4f9d\u8d56\u591a\u8f6e\u8bad\u7ec3\u548c\u663e\u5f0f\u4efb\u52a1\u63d0\u793a\uff0c\u7f3a\u4e4f\u9488\u5bf9\u53c2\u6570\u5206\u914d\u548c\u6709\u9650\u76d1\u7763\u4e0b\u8868\u793a\u80fd\u529b\u589e\u5f3a\u7684\u4e13\u95e8\u8bbe\u8ba1\uff0c\u96be\u4ee5\u5e94\u5bf9\u901a\u7528\u6301\u7eed\u5b66\u4e60\u7684\u6311\u6218\u3002", "method": "\u5c06\u901a\u7528\u6301\u7eed\u5b66\u4e60\u5206\u89e3\u4e3a\u4e13\u5bb6\u8def\u7531\u548c\u4e13\u5bb6\u80fd\u529b\u6539\u8fdb\u4e24\u4e2a\u5b50\u95ee\u9898\uff0c\u5f15\u5165\u968f\u673a\u6269\u5c55\u5206\u6790\u8def\u7531\u5668\u5b9e\u73b0\u5b9e\u4f8b\u7ea7\u4e13\u5bb6\u6fc0\u6d3b\uff0c\u91c7\u7528\u8f93\u51fa\u5934\u7684\u65f6\u95f4\u96c6\u6210\u52a8\u6001\u9002\u5e94\u51b3\u7b56\u8fb9\u754c\u3002", "result": "\u5728CIFAR-100\u3001ImageNet-R\u548cCUB-200\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u5206\u522b\u63d0\u534711.23%\u300112.43%\u548c7.62%\u3002", "conclusion": "\u679c\u8747\u542f\u53d1\u6846\u67b6FlyPrompt\u6709\u6548\u89e3\u51b3\u4e86\u901a\u7528\u6301\u7eed\u5b66\u4e60\u7684\u6838\u5fc3\u6311\u6218\uff0c\u901a\u8fc7\u7a00\u758f\u6269\u5c55\u548c\u6a21\u5757\u5316\u96c6\u6210\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u5355\u6b21\u5b66\u4e60\u6027\u80fd\uff0c\u4e3a\u65e0\u4efb\u52a1\u8fb9\u754c\u7684\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2602.01990", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01990", "abs": "https://arxiv.org/abs/2602.01990", "authors": ["Zhen-Hao Xie", "Jun-Tao Tang", "Yu-Cheng Shi", "Han-Jia Ye", "De-Chuan Zhan", "Da-Wei Zhou"], "title": "SAME: Stabilized Mixture-of-Experts for Multimodal Continual Instruction Tuning", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) achieve strong performance through instruction tuning, but real-world deployment requires them to continually expand their capabilities, making Multimodal Continual Instruction Tuning (MCIT) essential. Recent methods leverage sparse expert routing to promote task specialization, but we find that the expert routing process suffers from drift as the data distribution evolves. For example, a grounding query that previously activated localization experts may instead be routed to irrelevant experts after learning OCR tasks. Meanwhile, the grounding-related experts can be overwritten by new tasks and lose their original functionality. Such failure reflects two problems: router drift, where expert selection becomes inconsistent over time, and expert drift, where shared experts are overwritten across tasks. Therefore, we propose StAbilized Mixture-of-Experts (SAME) for MCIT. To address router drift, SAME stabilizes expert selection by decomposing routing dynamics into orthogonal subspaces and updating only task-relevant directions. To mitigate expert drift, we regulate expert updates via curvature-aware scaling using historical input covariance in a rehearsal-free manner. SAME also introduces adaptive expert activation to freeze selected experts during training, reducing redundant computation and cross-task interference. Extensive experiments demonstrate its SOTA performance.", "AI": {"tldr": "The paper proposes SAME, a stabilized mixture-of-experts method for multimodal continual instruction tuning that prevents router and expert drift through orthogonal subspace decomposition and curvature-aware scaling, achieving SOTA performance.", "motivation": "MLLMs need continual instruction tuning for real-world deployment, but current sparse expert routing methods suffer from router drift (inconsistent expert selection) and expert drift (overwriting shared experts) as data distributions evolve over time.", "method": "The paper proposes StAbilized Mixture-of-Experts (SAME) which: (1) stabilizes expert selection by decomposing routing dynamics into orthogonal subspaces and updating only task-relevant directions; (2) mitigates expert drift via curvature-aware scaling using historical input covariance in a rehearsal-free manner; and (3) introduces adaptive expert activation to freeze selected experts during training.", "result": "Extensive experiments demonstrate that SAME achieves state-of-the-art (SOTA) performance on multimodal continual instruction tuning tasks.", "conclusion": "SAME effectively addresses both router and expert drift in MCIT, enabling more stable and effective continual learning for MLLMs without requiring rehearsal data."}}
{"id": "2602.01997", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01997", "abs": "https://arxiv.org/abs/2602.01997", "authors": ["Safal Shrestha", "Anubhav Shrestha", "Aadim Nepal", "Minwu Kim", "Keith Ross"], "title": "On the Limits of Layer Pruning for Generative Reasoning in LLMs", "comment": null, "summary": "Recent works have shown that layer pruning can compress large language models (LLMs) while retaining strong performance on classification benchmarks with little or no finetuning. However, existing pruning techniques often suffer severe degradation on generative reasoning tasks. Through a systematic study across multiple model families, we find that tasks requiring multi-step reasoning are particularly sensitive to depth reduction. Beyond surface-level text degeneration, we observe degradation of critical algorithmic capabilities, including arithmetic computation for mathematical reasoning and balanced parenthesis generation for code synthesis. Under realistic post-training constraints, without access to pretraining-scale data or compute, we evaluate a simple mitigation strategy based on supervised finetuning with Self-Generated Responses. This approach achieves strong recovery on classification tasks, retaining up to 90\\% of baseline performance, and yields substantial gains of up to 20--30 percentage points on generative benchmarks compared to prior post-pruning techniques. Crucially, despite these gains, recovery for generative reasoning remains fundamentally limited relative to classification tasks and is viable primarily at lower pruning ratios. Overall, we characterize the practical limits of layer pruning for generative reasoning and provide guidance on when depth reduction can be applied effectively under constrained post-training regimes.", "AI": {"tldr": "\u5c42\u526a\u679d\u80fd\u6709\u6548\u538b\u7f29\u5927\u8bed\u8a00\u6a21\u578b\u5e76\u4fdd\u7559\u5206\u7c7b\u4efb\u52a1\u6027\u80fd\uff0c\u4f46\u5bf9\u591a\u6b65\u63a8\u7406\u7684\u751f\u6210\u4efb\u52a1\u9020\u6210\u4e25\u91cd\u9000\u5316\uff1b\u901a\u8fc7\u81ea\u751f\u6210\u54cd\u5e94\u7684\u76d1\u7763\u5fae\u8c03\u53ef\u90e8\u5206\u6062\u590d\u6027\u80fd\uff0c\u4f46\u751f\u6210\u63a8\u7406\u7684\u6062\u590d\u5b58\u5728\u6839\u672c\u6027\u9650\u5236", "motivation": "\u73b0\u6709\u5c42\u526a\u679d\u6280\u672f\u5728\u5927\u8bed\u8a00\u6a21\u578b\u538b\u7f29\u4e2d\u867d\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u751f\u6210\u5f0f\u63a8\u7406\u4efb\u52a1\u4e0a\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\uff0c\u5c24\u5176\u5f71\u54cd\u591a\u6b65\u63a8\u7406\u80fd\u529b\u5982\u6570\u5b66\u8ba1\u7b97\u548c\u4ee3\u7801\u751f\u6210", "method": "\u901a\u8fc7\u8de8\u6a21\u578b\u5bb6\u65cf\u7684\u7cfb\u7edf\u7684\u7814\u7a76\uff0c\u5206\u6790\u6df1\u5ea6\u51cf\u5c11\u5bf9\u63a8\u7406\u4efb\u52a1\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u81ea\u751f\u6210\u54cd\u5e94\u76d1\u7763\u5fae\u8c03\u7684\u7f13\u89e3\u7b56\u7565", "result": "\u8be5\u65b9\u6cd5\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\u53ef\u4fdd\u755990%\u57fa\u7ebf\u6027\u80fd\uff0c\u5728\u751f\u6210\u4efb\u52a1\u4e0a\u6bd4\u5148\u524d\u65b9\u6cd5\u63d0\u534720-30\u4e2a\u767e\u5206\u70b9\uff0c\u4f46\u751f\u6210\u63a8\u7406\u7684\u6062\u590d\u4ecd\u8fdc\u900a\u4e8e\u5206\u7c7b\u4efb\u52a1\uff0c\u4e14\u4ec5\u5728\u4f4e\u526a\u679d\u6bd4\u4f8b\u4e0b\u6709\u6548", "conclusion": "\u5c42\u526a\u679d\u5bf9\u751f\u6210\u63a8\u7406\u5b58\u5728\u5b9e\u8df5\u6027\u9650\u5236\uff0c\u5728\u53d7\u9650\u7684\u540e\u8bad\u7ec3\u73af\u5883\u4e0b\uff0c\u6df1\u5ea6\u51cf\u5c11\u4ec5\u9002\u7528\u4e8e\u7279\u5b9a\u573a\u666f\uff0c\u4e3a\u526a\u679d\u5e94\u7528\u63d0\u4f9b\u4e86\u5173\u952e\u6307\u5bfc"}}
{"id": "2602.02001", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02001", "abs": "https://arxiv.org/abs/2602.02001", "authors": ["Yoonjun Cho", "Dongjae Jeon", "Soeun Kim", "Moongyu Jeon", "Albert No"], "title": "Preserve-Then-Quantize: Balancing Rank Budgets for Quantization Error Reconstruction in LLMs", "comment": null, "summary": "Quantization Error Reconstruction (QER) reduces accuracy loss in Post-Training Quantization (PTQ) by approximating weights as $\\mathbf{W} \\approx \\mathbf{Q} + \\mathbf{L}\\mathbf{R}$, using a rank-$r$ correction to reconstruct quantization error. Prior methods devote the full rank budget to error reconstruction, which is suboptimal when $\\mathbf{W}$ has intrinsic low-rank structure and quantization corrupts dominant directions. We propose Structured Residual Reconstruction (SRR), a rank-allocation framework that preserves the top-$k$ singular subspace of the activation-scaled weight before quantization, quantizes only the residual, and uses the remaining rank $r-k$ for error reconstruction. We derive a theory-guided criterion for selecting $k$ by balancing quantization-exposed energy and unrecoverable error under rank constraints. We further show that resulting $\\mathbf{Q} + \\mathbf{L}\\mathbf{R}$ parameterization naturally supports Quantized Parameter-Efficient Fine-Tuning (QPEFT), and stabilizes fine-tuning via gradient scaling along preserved directions. Experiments demonstrate consistent perplexity reductions across diverse models and quantization settings in PTQ, along with a 5.9 percentage-point average gain on GLUE under 2-bit QPEFT.", "AI": {"tldr": "\u9488\u5bf9PTQ\u91cf\u5316\u8bef\u5dee\u91cd\u5efa\u7684\u79e9\u5206\u914d\u95ee\u9898\uff0c\u63d0\u51fa\u7ed3\u6784\u5316\u6b8b\u5dee\u91cd\u5efa(SRR)\uff0c\u901a\u8fc7\u4fdd\u7559\u6743\u91cd\u524dk\u4e2a\u5947\u5f02\u5b50\u7a7a\u95f4\u3001\u91cf\u5316\u6b8b\u5dee\u3001\u5269\u4f59\u79e9\u91cd\u5efa\u8bef\u5dee\uff0c\u5b9e\u73b0\u7406\u8bba\u6307\u5bfc\u7684k\u503c\u9009\u62e9\uff0c\u652f\u6301QPEFT\uff0c\u5728PTQ\u56f0\u60d1\u5ea6\u548c2-bit QPEFT\u7684GLUE\u57fa\u51c6\u4e0a\u5206\u522b\u53d6\u5f97\u6301\u7eed\u6539\u8fdb\u548c5.9%\u5e73\u5747\u63d0\u5347\u3002", "motivation": "\u73b0\u6709QER\u65b9\u6cd5\u5c06\u5168\u90e8\u79e9\u9884\u7b97\u7528\u4e8e\u8bef\u5dee\u91cd\u5efa\uff0c\u5f53\u6743\u91cd\u5177\u4f4e\u79e9\u7ed3\u6784\u4e14\u91cf\u5316\u7834\u574f\u4e3b\u65b9\u5411\u65f6\u6548\u679c\u6b21\u4f18\u3002", "method": "\u63d0\u51faSRR\u6846\u67b6\uff1a\u4fdd\u7559\u6fc0\u6d3b\u7f29\u653e\u6743\u91cd\u7684\u524dk\u4e2a\u5947\u5f02\u5b50\u7a7a\u95f4\uff0c\u4ec5\u91cf\u5316\u6b8b\u5dee\uff0c\u7528\u5269\u4f59\u79e9(r-k)\u91cd\u5efa\u8bef\u5dee\uff1b\u63a8\u5bfck\u503c\u9009\u62e9\u51c6\u5219\uff1b\u5229\u7528Q+LR\u53c2\u6570\u5316\u5b9e\u73b0QPEFT\u5e76\u68af\u5ea6\u7f29\u653e\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u79cd\u6a21\u578b\u548c\u91cf\u5316\u8bbe\u7f6e\u4e0bPTQ\u56f0\u60d1\u5ea6\u6301\u7eed\u964d\u4f4e\uff1b2-bit QPEFT\u5728GLUE\u57fa\u51c6\u5e73\u5747\u63d0\u53475.9\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "SRR\u901a\u8fc7\u667a\u80fd\u79e9\u5206\u914d\u4fdd\u7559\u6743\u91cd\u4e3b\u7ed3\u6784\uff0c\u6709\u6548\u91cd\u5efa\u91cf\u5316\u8bef\u5dee\uff0c\u63d0\u5347PTQ\u7cbe\u5ea6\uff0c\u5e76\u4e3a\u8d85\u4f4e\u4f4d\u5bbd\u5fae\u8c03\u63d0\u4f9b\u7a33\u5b9a\u9ad8\u6548\u65b9\u6848\u3002"}}
{"id": "2602.01791", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01791", "abs": "https://arxiv.org/abs/2602.01791", "authors": ["Zheng Zhang", "Ao Lu", "Yuanhao Zeng", "Ziwei Shan", "Jinjin Guo", "Lufei Li", "Yexin Li", "Kan Ren"], "title": "Grad2Reward: From Sparse Judgment to Dense Rewards for Improving Open-Ended LLM Reasoning", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has catalyzed significant breakthroughs in complex LLM reasoning within verifiable domains, such as mathematics and programming. Recent efforts have sought to extend this paradigm to open-ended tasks by employing LLMs-as-a-Judge to provide sequence-level rewards for policy optimization. However, these rewards are inherently sparse, failing to provide the fine-grained supervision necessary for generating complex, long-form trajectories. Furthermore, current work treats the Judge as a black-box oracle, discarding the rich intermediate feedback signals encoded in it. To address these limitations, we introduce Grad2Reward, a novel framework that extracts dense process rewards directly from the Judge's model inference process via a single backward pass. By leveraging gradient-based attribution, Grad2Reward enables precise token-level credit assignment, substantially enhancing training efficiency and reasoning quality. Additionally, Grad2Reward introduces a self-judging mechanism, allowing the policy to improve through its own evaluative signals without training specialized reward models or reliance on superior external Judges. The experiments demonstrate that policies optimized with Grad2Reward achieve outstanding performance across diverse open-ended tasks, affirming its effectiveness and broad generalizability.", "AI": {"tldr": "\u63d0\u51faGrad2Reward\u6846\u67b6\uff0c\u901a\u8fc7\u68af\u5ea6\u5f52\u56e0\u4eceLLM Judge\u4e2d\u63d0\u53d6\u5bc6\u96c6\u7684\u8fc7\u7a0b\u5956\u52b1\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6token\u7ea7\u4fe1\u7528\u5206\u914d\uff0c\u5e76\u5f15\u5165\u81ea\u8bc4\u5224\u673a\u5236\u63d0\u5347\u5f00\u653e\u4efb\u52a1\u63a8\u7406\u6027\u80fd", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u4f9d\u8d56\u7a00\u758f\u7684\u5e8f\u5217\u7ea7\u5956\u52b1\uff0c\u7f3a\u4e4f\u5bf9\u590d\u6742\u957f\u8f68\u8ff9\u7684\u7ec6\u7c92\u5ea6\u76d1\u7763\uff0c\u4e14\u5c06Judge\u89c6\u4e3a\u9ed1\u7bb1\u4e22\u5f03\u4e86\u4e30\u5bcc\u7684\u4e2d\u95f4\u53cd\u9988\u4fe1\u53f7", "method": "\u4eceJudge\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u901a\u8fc7\u5355\u6b21\u53cd\u5411\u4f20\u64ad\u63d0\u53d6\u5bc6\u96c6\u8fc7\u7a0b\u5956\u52b1\uff0c\u5229\u7528\u68af\u5ea6\u5f52\u56e0\u5b9e\u73b0\u7cbe\u786e\u7684token\u7ea7\u4fe1\u7528\u5206\u914d\uff0c\u5e76\u8bbe\u8ba1\u81ea\u8bc4\u5224\u673a\u5236\u8ba9\u7b56\u7565\u901a\u8fc7\u81ea\u8eab\u8bc4\u4f30\u4fe1\u53f7\u6539\u8fdb", "result": "\u5728\u591a\u6837\u5316\u5f00\u653e\u4efb\u52a1\u4e0a\uff0c\u4f7f\u7528Grad2Reward\u4f18\u5316\u7684\u7b56\u7565\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u63a8\u7406\u8d28\u91cf", "conclusion": "Grad2Reward\u6709\u6548\u89e3\u51b3\u4e86\u5956\u52b1\u7a00\u758f\u548c\u9ed1\u7bb1Judge\u95ee\u9898\uff0c\u5177\u6709\u5e7f\u6cdb\u901a\u7528\u6027\u548c\u6709\u6548\u6027"}}
{"id": "2602.02055", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02055", "abs": "https://arxiv.org/abs/2602.02055", "authors": ["Nan Qiao", "Sheng Yue"], "title": "FORLER: Federated Offline Reinforcement Learning with Q-Ensemble and Actor Rectification", "comment": "accetped by IEEE International Conference on Communications (ICC 2026)", "summary": "In Internet-of-Things systems, federated learning has advanced online reinforcement learning (RL) by enabling parallel policy training without sharing raw data. However, interacting with real environments online can be risky and costly, motivating offline federated RL (FRL), where local devices learn from fixed datasets. Despite its promise, offline FRL may break down under low-quality, heterogeneous data. Offline RL tends to get stuck in local optima, and in FRL, one device's suboptimal policy can degrade the aggregated model, i.e., policy pollution. We present FORLER, combining Q-ensemble aggregation on the server with actor rectification on devices. The server robustly merges device Q-functions to curb policy pollution and shift heavy computation off resource-constrained hardware without compromising privacy. Locally, actor rectification enriches policy gradients via a zeroth-order search for high-Q actions plus a bespoke regularizer that nudges the policy toward them. A $\u03b4$-periodic strategy further reduces local computation. We theoretically provide safe policy improvement performance guarantees. Extensive experiments show FORLER consistently outperforms strong baselines under varying data quality and heterogeneity.", "AI": {"tldr": "FORLER \u662f\u4e00\u79cd\u79bb\u7ebf\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7 Q-\u96c6\u6210\u805a\u5408\u548c\u6267\u884c\u5668\u4fee\u6b63\uff0c\u89e3\u51b3\u4f4e\u8d28\u91cf\u5f02\u6784\u6570\u636e\u4e0b\u7684\u7b56\u7565\u6c61\u67d3\u548c\u5c40\u90e8\u6700\u4f18\u95ee\u9898\uff0c\u5177\u5907\u7406\u8bba\u4fdd\u8bc1\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5728\u7269\u8054\u7f51\u7cfb\u7edf\u4e2d\uff0c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5b58\u5728\u98ce\u9669\u4e0e\u6210\u672c\uff0c\u79bb\u7ebf\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u867d\u80fd\u907f\u514d\u5728\u7ebf\u4ea4\u4e92\uff0c\u5374\u6613\u53d7\u4f4e\u8d28\u91cf\u3001\u5f02\u6784\u6570\u636e\u5f71\u54cd\uff0c\u51fa\u73b0\u7b56\u7565\u6c61\u67d3\u548c\u5c40\u90e8\u6700\u4f18\u3002", "method": "\u5728\u670d\u52a1\u5668\u7aef\uff0cFORLER \u4f7f\u7528 Q-\u96c6\u6210\u805a\u5408\u7a33\u5065\u5408\u5e76\u8bbe\u5907 Q \u51fd\u6570\uff0c\u6291\u5236\u7b56\u7565\u6c61\u67d3\uff1b\u5728\u8bbe\u5907\u7aef\uff0c\u91c7\u7528\u6267\u884c\u5668\u4fee\u6b63\uff0c\u901a\u8fc7\u96f6\u9636\u641c\u7d22\u9ad8 Q \u52a8\u4f5c\u548c\u6b63\u5219\u5316\u4e30\u5bcc\u7b56\u7565\u68af\u5ea6\uff0c\u5e76\u4ee5 \u03b4 \u5468\u671f\u7b56\u7565\u51cf\u5c11\u8ba1\u7b97\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFORLER \u5728\u4e0d\u540c\u6570\u636e\u8d28\u91cf\u548c\u5f02\u6784\u6027\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FORLER \u4e3a\u79bb\u7ebf\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u7a33\u5065\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u5907\u5b89\u5168\u7b56\u7565\u6539\u8fdb\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u7269\u8054\u7f51\u573a\u666f\u3002"}}
{"id": "2602.02060", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02060", "abs": "https://arxiv.org/abs/2602.02060", "authors": ["Hyunsuk Chung", "Caren Han", "Yerin Choi", "Seungyeon Ji", "Jinwoo Kim", "Eun-Jung Holden", "Kyungreem Han"], "title": "FiLoRA: Focus-and-Ignore LoRA for Controllable Feature Reliance", "comment": null, "summary": "Multimodal foundation models integrate heterogeneous signals across modalities, yet it remains poorly understood how their predictions depend on specific internal feature groups and whether such reliance can be deliberately controlled. Existing studies of shortcut and spurious behavior largely rely on post hoc analyses or feature removal, offering limited insight into whether reliance can be modulated without altering task semantics. We introduce FiLoRA (Focus-and-Ignore LoRA), an instruction-conditioned, parameter-efficient adaptation framework that enables explicit control over internal feature reliance while keeping the predictive objective fixed. FiLoRA decomposes adaptation into feature group-aligned LoRA modules and applies instruction-conditioned gating, allowing natural language instructions to act as computation-level control signals rather than task redefinitions. Across text--image and audio--visual benchmarks, we show that instruction-conditioned gating induces consistent and causal shifts in internal computation, selectively amplifying or suppressing core and spurious feature groups without modifying the label space or training objective. Further analyses demonstrate that FiLoRA yields improved robustness under spurious feature interventions, revealing a principled mechanism to regulate reliance beyond correlation-driven learning.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.01828", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01828", "abs": "https://arxiv.org/abs/2602.01828", "authors": ["Dionisia Naddeo", "Jonas Linkerh\u00e4gner", "Nicola Toschi", "Geri Skenderi", "Veronica Lachi"], "title": "Hyperbolic Graph Neural Networks Under the Microscope: The Role of Geometry-Task Alignment", "comment": null, "summary": "Many complex networks exhibit hyperbolic structural properties, making hyperbolic space a natural candidate for representing hierarchical and tree-like graphs with low distortion. Based on this observation, Hyperbolic Graph Neural Networks (HGNNs) have been widely adopted as a principled choice for representation learning on tree-like graphs. In this work, we question this paradigm by proposing an additional condition of geometry-task alignment, i.e., whether the metric structure of the target follows that of the input graph. We theoretically and empirically demonstrate the capability of HGNNs to recover low-distortion representations on two synthetic regression problems, and show that their geometric inductive bias becomes helpful when the problem requires preserving metric structure. Additionally, we evaluate HGNNs on the tasks of link prediction and node classification by jointly analyzing predictive performance and embedding distortion, revealing that only link prediction is geometry-aligned. Overall, our findings shift the focus from only asking \"Is the graph hyperbolic?\" to also questioning \"Is the task aligned with hyperbolic geometry?\", showing that HGNNs consistently outperform Euclidean models under such alignment, while their advantage vanishes otherwise.", "AI": {"tldr": "\u8d28\u7591HGNNs\u7684\u9ed8\u8ba4\u4f7f\u7528\u8303\u5f0f\uff0c\u63d0\u51fa\u9700\u8bc4\u4f30\u51e0\u4f55-\u4efb\u52a1\u5bf9\u9f50\u6027\uff0c\u4ec5\u5f53\u4efb\u52a1\u4e0e\u53cc\u66f2\u51e0\u4f55\u4e00\u81f4\u65f6HGNNs\u624d\u4f18\u4e8e\u6b27\u6c0f\u6a21\u578b", "motivation": "\u73b0\u6709HGNNs\u88ab\u5e7f\u6cdb\u8ba4\u4e3a\u9002\u5408\u6811\u72b6\u56fe\u8868\u793a\u5b66\u4e60\uff0c\u4f46\u7f3a\u4e4f\u5bf9\"\u51e0\u4f55-\u4efb\u52a1\u5bf9\u9f50\"\u6761\u4ef6\u7684\u4e25\u683c\u8bc4\u4f30\uff0c\u5373\u4efb\u52a1\u662f\u5426\u771f\u6b63\u9700\u8981\u4fdd\u6301\u56fe\u7684\u5ea6\u91cf\u7ed3\u6784", "method": "\u7406\u8bba+\u5b9e\u8bc1\u5206\u6790\uff1a\u5728\u5408\u6210\u56de\u5f52\u4efb\u52a1\u4e2d\u6d4b\u8bd5HGNNs\u7684\u8868\u793a\u80fd\u529b\uff0c\u5e76\u5728\u94fe\u63a5\u9884\u6d4b\u548c\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e2d\u8054\u5408\u8bc4\u4f30\u9884\u6d4b\u6027\u80fd\u4e0e\u5d4c\u5165\u5931\u771f\u5ea6", "result": "1) HGNNs\u80fd\u6709\u6548\u6062\u590d\u4f4e\u5931\u771f\u8868\u793a\u5f53\u4efb\u52a1\u9700\u4fdd\u6301\u5ea6\u91cf\u7ed3\u6784\uff1b2) \u4ec5\u94fe\u63a5\u9884\u6d4b\u4efb\u52a1\u4e0e\u51e0\u4f55\u5bf9\u9f50\uff0c\u8282\u70b9\u5206\u7c7b\u4e0d\u6210\u7acb\uff1b3) \u51e0\u4f55\u5bf9\u9f50\u65f6HGNNs\u663e\u8457\u4f18\u4e8e\u6b27\u6c0f\u6a21\u578b\uff0c\u5426\u5219\u4f18\u52bf\u6d88\u5931", "conclusion": "\u5e94\u4ece\u5355\u7eaf\u5173\u6ce8\"\u56fe\u662f\u5426\u53cc\u66f2\"\u8f6c\u5411\u540c\u65f6\u8bc4\u4f30\"\u4efb\u52a1\u662f\u5426\u4e0e\u53cc\u66f2\u51e0\u4f55\u5bf9\u9f50\"\uff0c\u5f3a\u8c03\u51e0\u4f55\u5f52\u7eb3\u504f\u7f6e\u9700\u4e0e\u4efb\u52a1\u9700\u6c42\u5339\u914d\u624d\u80fd\u53d1\u6325\u4f18\u52bf"}}
{"id": "2602.02098", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02098", "abs": "https://arxiv.org/abs/2602.02098", "authors": ["Yannik Schnitzer", "Mathias Jackermeier", "Alessandro Abate", "David Parker"], "title": "Probabilistic Performance Guarantees for Multi-Task Reinforcement Learning", "comment": null, "summary": "Multi-task reinforcement learning trains generalist policies that can execute multiple tasks. While recent years have seen significant progress, existing approaches rarely provide formal performance guarantees, which are indispensable when deploying policies in safety-critical settings. We present an approach for computing high-confidence guarantees on the performance of a multi-task policy on tasks not seen during training. Concretely, we introduce a new generalisation bound that composes (i) per-task lower confidence bounds from finitely many rollouts with (ii) task-level generalisation from finitely many sampled tasks, yielding a high-confidence guarantee for new tasks drawn from the same arbitrary and unknown distribution. Across state-of-the-art multi-task RL methods, we show that the guarantees are theoretically sound and informative at realistic sample sizes.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e3a\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u63d0\u4f9b\u9ad8\u7f6e\u4fe1\u5ea6\u6027\u80fd\u4fdd\u8bc1\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ec4\u5408\u5355\u4efb\u52a1\u7f6e\u4fe1\u754c\u548c\u4efb\u52a1\u7ea7\u6cdb\u5316\u754c\uff0c\u5b9e\u73b0\u5bf9\u672a\u77e5\u65b0\u4efb\u52a1\u7684\u5b89\u5168\u90e8\u7f72\u4fdd\u969c", "motivation": "\u73b0\u6709\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u5f62\u5f0f\u5316\u6027\u80fd\u4fdd\u8bc1\uff0c\u8fd9\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u90e8\u7f72\u65f6\u5b58\u5728\u91cd\u5927\u98ce\u9669\uff0c\u9700\u8981\u4e3a\u672a\u89c1\u4efb\u52a1\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u6027\u80fd\u4fdd\u969c", "method": "\u6784\u5efa\u65b0\u578b\u6cdb\u5316\u754c\uff1a\u5c06(i)\u6709\u9650\u6b21\u8f68\u8ff9\u91c7\u6837\u7684\u5355\u4efb\u52a1\u4e0b\u7f6e\u4fe1\u754c\u4e0e(ii)\u6709\u9650\u91c7\u6837\u4efb\u52a1\u7684\u4efb\u52a1\u7ea7\u6cdb\u5316\u754c\u8fdb\u884c\u7ec4\u5408\uff0c\u5f62\u6210\u5bf9\u540c\u5206\u5e03\u672a\u77e5\u65b0\u4efb\u52a1\u7684\u9ad8\u7f6e\u4fe1\u5ea6\u4fdd\u8bc1", "result": "\u5728\u5148\u8fdb\u591a\u4efb\u52a1RL\u65b9\u6cd5\u4e0a\u9a8c\u8bc1\uff1a\u8be5\u4fdd\u8bc1\u5728\u7406\u8bba\u4e0a\u4e25\u8c28\uff0c\u4e14\u5728\u73b0\u5b9e\u6837\u672c\u91cf\u4e0b\u5177\u6709\u5b9e\u9645\u6307\u5bfc\u610f\u4e49\uff0c\u9996\u6b21\u5b9e\u73b0\u672a\u77e5\u4efb\u52a1\u6027\u80fd\u7684\u53ef\u9760\u91cf\u5316", "conclusion": "\u5efa\u7acb\u4e86\u8fde\u63a5\u591a\u4efb\u52a1\u7b56\u7565\u8bad\u7ec3\u4e0e\u672a\u77e5\u4efb\u52a1\u6027\u80fd\u4fdd\u8bc1\u7684\u7406\u8bba\u6865\u6881\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9a8c\u8bc1\u7684\u5b89\u5168\u4fdd\u969c\u6846\u67b6"}}
{"id": "2602.01842", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01842", "abs": "https://arxiv.org/abs/2602.01842", "authors": ["Jinbin Bai", "Yixuan Li", "Yuchen Zhu", "Yi Xin", "Qingyu Shi", "Aosong Feng", "Xiaohong Liu", "Molei Tao", "Jianru Xue", "Xiangtai Li", "Ming-Hsuan Yang"], "title": "Prism: Efficient Test-Time Scaling via Hierarchical Search and Self-Verification for Discrete Diffusion Language Models", "comment": null, "summary": "Inference-time compute has re-emerged as a practical way to improve LLM reasoning. Most test-time scaling (TTS) algorithms rely on autoregressive decoding, which is ill-suited to discrete diffusion language models (dLLMs) due to their parallel decoding over the entire sequence. As a result, developing effective and efficient TTS methods to unlock dLLMs' full generative potential remains an underexplored challenge. To address this, we propose Prism (Pruning, Remasking, and Integrated Self-verification Method), an efficient TTS framework for dLLMs that (i) performs Hierarchical Trajectory Search (HTS) which dynamically prunes and reallocates compute in an early-to-mid denoising window, (ii) introduces Local branching with partial remasking to explore diverse implementations while preserving high-confidence tokens, and (iii) replaces external verifiers with Self-Verified Feedback (SVF) obtained via self-evaluation prompts on intermediate completions. Across four mathematical reasoning and code generation benchmarks on three dLLMs, including LLaDA 8B Instruct, Dream 7B Instruct, and LLaDA 2.0-mini, our Prism achieves a favorable performance-efficiency trade-off, matching best-of-N performance with substantially fewer function evaluations (NFE). The code is released at https://github.com/viiika/Prism.", "AI": {"tldr": "Prism is a test-time scaling framework for discrete diffusion language models that uses hierarchical trajectory search, local branching with remasking, and self-verified feedback to achieve better performance with fewer computations.", "motivation": "Inference-time compute improves LLM reasoning but existing test-time scaling methods rely on autoregressive decoding, which is incompatible with discrete diffusion language models (dLLMs) that use parallel decoding. Developing effective TTS for dLLMs remains underexplored.", "method": "Proposes Prism with three components: (1) Hierarchical Trajectory Search for dynamic compute reallocation during denoising, (2) Local branching with partial remasking to explore diverse paths while preserving confident tokens, and (3) Self-Verified Feedback using self-evaluation instead of external verifiers.", "result": "On four math reasoning and code generation benchmarks across three dLLMs (LLaDA 8B, Dream 7B, LLaDA 2.0-mini), Prism matches best-of-N performance with substantially fewer function evaluations, showing favorable performance-efficiency trade-off.", "conclusion": "Prism successfully unlocks dLLMs' generative potential through an efficient TTS framework that dynamically allocates compute and uses self-verification, demonstrating effective test-time scaling for non-autoregressive models."}}
{"id": "2602.02126", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02126", "abs": "https://arxiv.org/abs/2602.02126", "authors": ["Junhan Kim", "Gukryeol Lee", "Seungwoo Son", "Jeewook Kim", "Yongkweon Jeon"], "title": "Two-Stage Grid Optimization for Group-wise Quantization of LLMs", "comment": "ICASSP 2026", "summary": "Group-wise quantization is an effective strategy for mitigating accuracy degradation in low-bit quantization of large language models (LLMs). Among existing methods, GPTQ has been widely adopted due to its efficiency; however, it neglects input statistics and inter-group correlations when determining group scales, leading to a mismatch with its goal of minimizing layer-wise reconstruction loss. In this work, we propose a two-stage optimization framework for group scales that explicitly minimizes the layer-wise reconstruction loss. In the first stage, performed prior to GPTQ, we initialize each group scale to minimize the group-wise reconstruction loss, thereby incorporating input statistics. In the second stage, we freeze the integer weights obtained via GPTQ and refine the group scales to minimize the layer-wise reconstruction loss. To this end, we employ the coordinate descent algorithm and derive a closed-form update rule, which enables efficient refinement without costly numerical optimization. Notably, our derivation incorporates the quantization errors from preceding layers to prevent error accumulation. Experimental results demonstrate that our method consistently enhances group-wise quantization, achieving higher accuracy with negligible overhead.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u4e24\u9636\u6bb5\u4f18\u5316\u6846\u67b6\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4f4e\u6bd4\u7279\u5206\u7ec4\u91cf\u5316\uff0c\u901a\u8fc7\u663e\u5f0f\u6700\u5c0f\u5316\u5c42\u91cd\u5efa\u635f\u5931\u5e76\u8003\u8651\u8f93\u5165\u7edf\u8ba1\u4e0e\u5c42\u95f4\u8bef\u5dee\u7d2f\u79ef\uff0c\u663e\u8457\u63d0\u5347\u7cbe\u5ea6\u4e14\u51e0\u4e4e\u65e0\u989d\u5916\u5f00\u9500", "motivation": "\u73b0\u6709\u5206\u7ec4\u91cf\u5316\u65b9\u6cd5\uff08\u5982GPTQ\uff09\u5728\u786e\u5b9a\u7ec4\u7f29\u653e\u56e0\u5b50\u65f6\u5ffd\u7565\u8f93\u5165\u7edf\u8ba1\u548c\u7ec4\u95f4\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u4e0e\u6700\u5c0f\u5316\u5c42\u91cd\u5efa\u635f\u5931\u7684\u76ee\u6807\u4e0d\u5339\u914d\uff0c\u9020\u6210\u7cbe\u5ea6\u4e0b\u964d", "method": "1) \u5728GPTQ\u524d\u521d\u59cb\u5316\u7ec4\u7f29\u653e\u56e0\u5b50\u4ee5\u6700\u5c0f\u5316\u7ec4\u91cd\u5efa\u635f\u5931\uff0c\u878d\u5165\u8f93\u5165\u7edf\u8ba1\uff1b2) \u51bb\u7ed3GPTQ\u6574\u6570\u6743\u91cd\u540e\uff0c\u7528\u5750\u6807\u4e0b\u964d\u7b97\u6cd5\u63a8\u5bfc\u95ed\u5f0f\u66f4\u65b0\u89c4\u5219\u7cbe\u70bc\u7f29\u653e\u56e0\u5b50\uff0c\u663e\u5f0f\u6700\u5c0f\u5316\u5c42\u91cd\u5efa\u635f\u5931\u5e76\u7eb3\u5165\u524d\u5c42\u91cf\u5316\u8bef\u5dee\u9632\u6b62\u7d2f\u79ef", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5206\u7ec4\u91cf\u5316\u4e2d\u6301\u7eed\u63d0\u5347\u7cbe\u5ea6\uff0c\u4e14\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u53ef\u5ffd\u7565\u4e0d\u8ba1", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u4e24\u9636\u6bb5\u4f18\u5316\u6709\u6548\u89e3\u51b3\u7ec4\u7f29\u653e\u56e0\u5b50\u4e0e\u91cd\u5efa\u76ee\u6807\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u4f4e\u6bd4\u7279\u91cf\u5316\u63d0\u4f9b\u9ad8\u6548\u9ad8\u7cbe\u5ea6\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.01845", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2602.01845", "abs": "https://arxiv.org/abs/2602.01845", "authors": ["Furkan Eris"], "title": "No Generation without Representation: Efficient Causal Protein Language Models Enable Zero-Shot Fitness Estimation", "comment": null, "summary": "Protein language models (PLMs) face a fundamental divide: masked language models (MLMs) excel at fitness prediction while causal models enable generation, forcing practitioners to maintain separate architectures. We introduce \\textbf{Proust}, a 309M-parameter causal PLM that bridges this gap through architectural innovations adapted from recent LLM research, including grouped-query attention with shared K/V projections, cross-layer value residuals, and depthwise causal convolutions. Trained on 33B tokens in 40 B200 GPU-hours, Proust achieves Spearman $\u03c1= 0.390$ on ProteinGym substitutions, competitive with MLMs requiring 50--200$\\times$ the compute. On indels, Proust sets a new state-of-the-art, outperforming models up to 20$\\times$ larger. On EVEREST viral fitness benchmarks, it approaches structure-aware methods using sequence alone. These powerful representations position Proust in a sweet spot as it also retains native generative capabilities that MLMs lack by design. Interpretability analysis reveals that per-position entropy variance predicts, to an extent, when retrieval augmentation helps and hurts. Such insights can grow in both quantity and quality at scale and inform capabilities such as test-time scaling. Code and weights are available at https://github.com/Furkan9015/proust-inference", "AI": {"tldr": "Proust is a 309M-parameter causal protein language model that bridges the gap between masked language models (good for fitness prediction) and causal models (good for generation) by incorporating architectural innovations like grouped-query attention and depthwise causal convolutions, achieving competitive performance with far less compute.", "motivation": "Protein language models face a fundamental divide: masked language models (MLMs) excel at fitness prediction while causal models enable generation, forcing practitioners to maintain separate architectures for different tasks.", "method": "Introduce Proust, a 309M-parameter causal PLM with architectural innovations adapted from recent LLM research: grouped-query attention with shared K/V projections, cross-layer value residuals, and depthwise causal convolutions. Trained on 33B tokens in 40 B200 GPU-hours.", "result": "Proust achieves Spearman \u03c1=0.390 on ProteinGym substitutions (competitive with MLMs requiring 50-200\u00d7 more compute), sets SOTA on indels (outperforming models up to 20\u00d7 larger), approaches structure-aware methods on EVEREST benchmarks using sequence alone, and retains native generative capabilities. Interpretability analysis reveals per-position entropy variance predicts when retrieval augmentation helps/hurts.", "conclusion": "Proust successfully bridges the gap between MLMs and causal models, occupying a sweet spot with powerful representations for fitness prediction while maintaining native generative capabilities. The model's efficiency and interpretability insights can inform future capabilities like test-time scaling."}}
{"id": "2602.02128", "categories": ["cs.LG", "cs.AI", "physics.bio-ph", "q-bio.BM", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2602.02128", "abs": "https://arxiv.org/abs/2602.02128", "authors": ["Nima Shoghi", "Yuxuan Liu", "Yuning Shen", "Rob Brekelmans", "Pan Li", "Quanquan Gu"], "title": "Scalable Spatio-Temporal SE(3) Diffusion for Long-Horizon Protein Dynamics", "comment": "For associated project page, see https://bytedance-seed.github.io/ConfRover/starmd", "summary": "Molecular dynamics (MD) simulations remain the gold standard for studying protein dynamics, but their computational cost limits access to biologically relevant timescales. Recent generative models have shown promise in accelerating simulations, yet they struggle with long-horizon generation due to architectural constraints, error accumulation, and inadequate modeling of spatio-temporal dynamics. We present STAR-MD (Spatio-Temporal Autoregressive Rollout for Molecular Dynamics), a scalable SE(3)-equivariant diffusion model that generates physically plausible protein trajectories over microsecond timescales. Our key innovation is a causal diffusion transformer with joint spatio-temporal attention that efficiently captures complex space-time dependencies while avoiding the memory bottlenecks of existing methods. On the standard ATLAS benchmark, STAR-MD achieves state-of-the-art performance across all metrics--substantially improving conformational coverage, structural validity, and dynamic fidelity compared to previous methods. STAR-MD successfully extrapolates to generate stable microsecond-scale trajectories where baseline methods fail catastrophically, maintaining high structural quality throughout the extended rollout. Our comprehensive evaluation reveals severe limitations in current models for long-horizon generation, while demonstrating that STAR-MD's joint spatio-temporal modeling enables robust dynamics simulation at biologically relevant timescales, paving the way for accelerated exploration of protein function.", "AI": {"tldr": "STAR-MD\u662f\u4e00\u79cd\u521b\u65b0\u7684SE(3)-\u7b49\u53d8\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u8054\u5408\u65f6\u7a7a\u6ce8\u610f\u529b\u673a\u5236\u751f\u6210\u957f\u65f6\u7a33\u5b9a\u7684\u5fae\u79d2\u7ea7\u86cb\u767d\u8d28\u8f68\u8ff9\uff0c\u5728ATLAS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u5168\u9762\u6027\u80fd\u7a81\u7834\u3002", "motivation": "\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u867d\u662f\u91d1\u6807\u51c6\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u96be\u4ee5\u8fbe\u5230\u751f\u7269\u76f8\u5173\u65f6\u95f4\u5c3a\u5ea6\uff1b\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728\u957f\u65f6\u751f\u6210\u4e2d\u9762\u4e34\u67b6\u6784\u9650\u5236\u3001\u8bef\u5dee\u7d2f\u79ef\u548c\u65f6\u7a7a\u52a8\u529b\u5b66\u5efa\u6a21\u4e0d\u8db3\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faSTAR-MD\uff0c\u91c7\u7528\u56e0\u679c\u6269\u6563Transformer\u67b6\u6784\uff0c\u7ed3\u5408\u8054\u5408\u65f6\u7a7a\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0SE(3)-\u7b49\u53d8\u6027\uff0c\u4ee5\u9ad8\u6548\u6355\u6349\u590d\u6742\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\u5e76\u907f\u514d\u5185\u5b58\u74f6\u9888\u3002", "result": "\u5728ATLAS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u6784\u8c61\u8986\u76d6\u5ea6\u3001\u7ed3\u6784\u6709\u6548\u6027\u548c\u52a8\u6001\u4fdd\u771f\u5ea6\uff0c\u6210\u529f\u751f\u6210\u57fa\u7ebf\u65b9\u6cd5\u65e0\u6cd5\u5b9e\u73b0\u7684\u8d85\u7a33\u5b9a\u5fae\u79d2\u7ea7\u8f68\u8ff9\u3002", "conclusion": "\u8054\u5408\u65f6\u7a7a\u5efa\u6a21\u65b9\u6cd5\u4f7f\u86cb\u767d\u8d28\u52a8\u529b\u5b66\u6a21\u62df\u8fbe\u5230\u751f\u7269\u76f8\u5173\u65f6\u95f4\u5c3a\u5ea6\u6210\u4e3a\u53ef\u80fd\uff0c\u4e3a\u52a0\u901f\u86cb\u767d\u8d28\u529f\u80fd\u63a2\u7d22\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.01852", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.01852", "abs": "https://arxiv.org/abs/2602.01852", "authors": ["Zeyan Wang", "Zhengmao Liu", "Yongxin Cai", "Chi Li", "Xiaoying Tang", "Jingchao Chen", "Zibin Pan", "Jing Qiu"], "title": "FUPareto: Bridging the Forgetting-Utility Gap in Federated Unlearning via Pareto Augmented Optimization", "comment": null, "summary": "Federated Unlearning (FU) aims to efficiently remove the influence of specific client data from a federated model while preserving utility for the remaining clients. However, three key challenges remain: (1) existing unlearning objectives often compromise model utility or increase vulnerability to Membership Inference Attacks (MIA); (2) there is a persistent conflict between forgetting and utility, where further unlearning inevitably harms retained performance; and (3) support for concurrent multi-client unlearning is poor, as gradient conflicts among clients degrade the quality of forgetting. To address these issues, we propose FUPareto, an efficient unlearning framework via Pareto-augmented optimization. We first introduce the Minimum Boundary Shift (MBS) Loss, which enforces unlearning by suppressing the target class logit below the highest non-target class logit; this can improve the unlearning efficiency and mitigate MIA risks. During the unlearning process, FUPareto performs Pareto improvement steps to preserve model utility and executes Pareto expansion to guarantee forgetting. Specifically, during Pareto expansion, the framework integrates a Null-Space Projected Multiple Gradient Descent Algorithm (MGDA) to decouple gradient conflicts. This enables effective, fair, and concurrent unlearning for multiple clients while minimizing utility degradation. Extensive experiments across diverse scenarios demonstrate that FUPareto consistently outperforms state-of-the-art FU methods in both unlearning efficacy and retained utility.", "AI": {"tldr": "FUPareto\u662f\u4e00\u79cd\u57fa\u4e8e\u5e15\u7d2f\u6258\u4f18\u5316\u7684\u8054\u90a6\u9057\u5fd8\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5c0f\u8fb9\u754c\u504f\u79fb\u635f\u5931\u548c\u96f6\u7a7a\u95f4\u6295\u5f71\u591a\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\uff0c\u9ad8\u6548\u79fb\u9664\u7279\u5b9a\u5ba2\u6237\u7aef\u6570\u636e\u5f71\u54cd\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6548\u7528\u5e76\u652f\u6301\u591a\u5ba2\u6237\u7aef\u5e76\u53d1\u9057\u5fd8\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u9057\u5fd8\u65b9\u6cd5\u9762\u4e34\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a\u9057\u5fd8\u76ee\u6807\u5e38\u635f\u5bb3\u6a21\u578b\u6548\u7528\u6216\u589e\u52a0\u6210\u5458\u63a8\u7406\u653b\u51fb\u98ce\u9669\uff1b\u9057\u5fd8\u4e0e\u6548\u7528\u4e4b\u95f4\u5b58\u5728\u56fa\u6709\u51b2\u7a81\uff1b\u68af\u5ea6\u51b2\u7a81\u5bfc\u81f4\u5bf9\u5e76\u53d1\u591a\u5ba2\u6237\u7aef\u9057\u5fd8\u652f\u6301\u4e0d\u4f73\u3002", "method": "\u63d0\u51faFUPareto\u6846\u67b6\uff0c\u5f15\u5165\u6700\u5c0f\u8fb9\u754c\u504f\u79fb\u635f\u5931(MBS)\u6291\u5236\u76ee\u6807\u7c7b\u522b\u7f6e\u4fe1\u5ea6\uff1b\u901a\u8fc7\u5e15\u7d2f\u6258\u6539\u8fdb\u6b65\u9aa4\u4fdd\u6301\u6548\u7528\uff0c\u5e15\u7d2f\u6258\u6269\u5c55\u4fdd\u8bc1\u9057\u5fd8\uff1b\u91c7\u7528\u96f6\u7a7a\u95f4\u6295\u5f71\u591a\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5(MGDA)\u89e3\u8026\u68af\u5ea6\u51b2\u7a81\uff0c\u5b9e\u73b0\u9ad8\u6548\u516c\u5e73\u7684\u591a\u5ba2\u6237\u7aef\u5e76\u53d1\u9057\u5fd8\u3002", "result": "\u5728\u591a\u79cd\u573a\u666f\u4e0b\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFUPareto\u5728\u9057\u5fd8\u6548\u679c\u548c\u4fdd\u7559\u6548\u7528\u65b9\u9762\u5747\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u8054\u90a6\u9057\u5fd8\u65b9\u6cd5\u3002", "conclusion": "FUPareto\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8054\u90a6\u9057\u5fd8\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u5e73\u8861\u4e86\u9057\u5fd8\u4e0e\u6548\u7528\u7684\u6743\u8861\uff0c\u5728\u6700\u5c0f\u5316\u6548\u7528\u9000\u5316\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u591a\u5ba2\u6237\u7aef\u5e76\u53d1\u9057\u5fd8\u3002"}}
{"id": "2602.02139", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02139", "abs": "https://arxiv.org/abs/2602.02139", "authors": ["Pawel Batorski", "Paul Swoboda"], "title": "EvoMU: Evolutionary Machine Unlearning", "comment": null, "summary": "Machine unlearning aims to unlearn specified training data (e.g. sensitive or copyrighted material). A prominent approach is to fine-tune an existing model with an unlearning loss that retains overall utility. The space of suitable unlearning loss functions is vast, making the search for an optimal loss function daunting. Additionally, there might not even exist a universally optimal loss function: differences in the structure and overlap of the forget and retain data can cause a loss to work well in one setting but over-unlearn or under-unlearn in another. Our approach EvoMU tackles these two challenges simultaneously. An evolutionary search procedure automatically finds task-specific losses in the vast space of possible unlearning loss functions. This allows us to find dataset-specific losses that match or outperform existing losses from the literature, without the need for a human-in-the-loop. This work is therefore an instance of automatic scientific discovery, a.k.a. an AI co-scientist. In contrast to previous AI co-scientist works, we do so on a budget: We achieve SotA results using a small 4B parameter model (Qwen3-4B-Thinking), showing the potential of AI co-scientists with limited computational resources. Our experimental evaluation shows that we surpass previous loss-based unlearning formulations on TOFU-5%, TOFU-10%, MUSE and WMDP by synthesizing novel unlearning losses. Our code is available at https://github.com/Batorskq/EvoMU.", "AI": {"tldr": "EvoMU uses evolutionary search to automatically discover optimal unlearning loss functions, achieving SotA results on multiple benchmarks with a small 4B model while demonstrating the feasibility of AI co-scientists on a budget.", "motivation": "Machine unlearning needs to remove specific training data while preserving model utility, but the vast space of possible loss functions and lack of universal optimal solutions make manual design challenging. A loss function that works well in one setting may over-unlearn or under-unlearn in another due to differences in forget/retain data structure.", "method": "EvoMU employs an evolutionary search procedure to automatically discover task-specific unlearning loss functions from a vast search space. It eliminates human intervention by synthesizing novel loss formulations and uses a modest 4B parameter model (Qwen3-4B-Thinking) to demonstrate efficiency.", "result": "The method surpasses previous loss-based unlearning approaches on TOFU-5%, TOFU-10%, MUSE, and WMDP benchmarks, achieving state-of-the-art performance while using limited computational resources.", "conclusion": "This work demonstrates automatic scientific discovery (AI co-scientist) for machine unlearning, showing that effective AI-driven scientific discovery is possible even with constrained computational budgets."}}
{"id": "2602.01853", "categories": ["cs.LG", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01853", "abs": "https://arxiv.org/abs/2602.01853", "authors": ["Xiangkun Wu", "Qianglin Wen", "Yingying Zhang", "Hongtu Zhu", "Ting Li", "Chengchun Shi"], "title": "Designing Time Series Experiments in A/B Testing with Transformer Reinforcement Learning", "comment": null, "summary": "A/B testing has become a gold standard for modern technological companies to conduct policy evaluation. Yet, its application to time series experiments, where policies are sequentially assigned over time, remains challenging. Existing designs suffer from two limitations: (i) they do not fully leverage the entire history for treatment allocation; (ii) they rely on strong assumptions to approximate the objective function (e.g., the mean squared error of the estimated treatment effect) for optimizing the design. We first establish an impossibility theorem showing that failure to condition on the full history leads to suboptimal designs, due to the dynamic dependencies in time series experiments. To address both limitations simultaneously, we next propose a transformer reinforcement learning (RL) approach which leverages transformers to condition allocation on the entire history and employs RL to directly optimize the MSE without relying on restrictive assumptions. Empirical evaluations on synthetic data, a publicly available dispatch simulator, and a real-world ridesharing dataset demonstrate that our proposal consistently outperforms existing designs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u65f6\u95f4\u5e8f\u5217A/B\u6d4b\u8bd5\u4e2d\u7684\u7b56\u7565\u5206\u914d\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8eTransformer\u5f3a\u5316\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5229\u7528\u5b8c\u6574\u5386\u53f2\u4fe1\u606f\u548c\u4f9d\u8d56\u5f3a\u5047\u8bbe\u7684\u4e24\u4e2a\u6838\u5fc3\u9650\u5236\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u4ee3\u79d1\u6280\u516c\u53f8\u5e7f\u6cdb\u4f7f\u7528A/B\u6d4b\u8bd5\u8fdb\u884c\u653f\u7b56\u8bc4\u4f30\uff0c\u4f46\u5728\u65f6\u95f4\u5e8f\u5217\u5b9e\u9a8c\u4e2d\uff0c\u73b0\u6709\u8bbe\u8ba1\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u7f3a\u9677\uff1a\u4e00\u662f\u672a\u80fd\u5145\u5206\u5229\u7528\u5b8c\u6574\u5386\u53f2\u6570\u636e\u8fdb\u884c\u7b56\u7565\u5206\u914d\uff0c\u4e8c\u662f\u4f9d\u8d56\u5f3a\u5047\u8bbe\u6765\u8fd1\u4f3c\u4f18\u5316\u76ee\u6807\u51fd\u6570\uff08\u5982\u5904\u7406\u6548\u5e94\u7684\u5747\u65b9\u8bef\u5dee\uff09\uff0c\u8fd9\u5bfc\u81f4\u5b9e\u9a8c\u8bbe\u8ba1\u6b21\u4f18\u3002", "method": "\u63d0\u51faTransformer\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528Transformer\u67b6\u6784\u7f16\u7801\u5b8c\u6574\u5b9e\u9a8c\u5386\u53f2\u4f5c\u4e3a\u7b56\u7565\u5206\u914d\u7684\u6761\u4ef6\uff1b2\uff09\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u76f4\u63a5\u4f18\u5316\u5747\u65b9\u8bef\u5dee\u76ee\u6807\uff0c\u907f\u514d\u5bf9\u76ee\u6807\u51fd\u6570\u7684\u53c2\u6570\u5316\u8fd1\u4f3c\u5047\u8bbe\u3002\u9996\u5148\u5efa\u7acb\u4e0d\u53ef\u80fd\u6027\u5b9a\u7406\u8bc1\u660e\u5ffd\u7565\u5b8c\u6574\u5386\u53f2\u5fc5\u7136\u5bfc\u81f4\u6b21\u4f18\u8bbe\u8ba1\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u3001\u516c\u5f00\u8c03\u5ea6\u6a21\u62df\u5668\u548c\u771f\u5b9e\u7f51\u7ea6\u8f66\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u8bbe\u8ba1\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u5229\u7528\u5386\u53f2\u4fe1\u606f\u5e76\u76f4\u63a5\u4f18\u5316\u5b9e\u9a8c\u76ee\u6807\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u65f6\u95f4\u5e8f\u5217\u5b9e\u9a8c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff08\u4e0d\u53ef\u80fd\u6027\u5b9a\u7406\uff09\u548c\u5b9e\u8df5\u5de5\u5177\uff08Transformer-RL\u65b9\u6cd5\uff09\uff0c\u8bc1\u660e\u4e86\u5728\u52a8\u6001\u4f9d\u8d56\u73af\u5883\u4e0b\uff0c\u57fa\u4e8e\u5b8c\u6574\u5386\u53f2\u4fe1\u606f\u76f4\u63a5\u4f18\u5316MSE\u662f\u66f4\u4f18\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u8303\u5f0f\u3002"}}
{"id": "2602.02143", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02143", "abs": "https://arxiv.org/abs/2602.02143", "authors": ["Shubham Toshniwal", "Aleksander Ficek", "Siddhartha Jain", "Wei Du", "Vahid Noroozi", "Sadegh Mahdavi", "Somshubra Majumdar", "Igor Gitman"], "title": "Learning Generative Selection for Best-of-N", "comment": null, "summary": "Scaling test-time compute via parallel sampling can substantially improve LLM reasoning, but is often limited by Best-of-N selection quality. Generative selection methods, such as GenSelect, address this bottleneck, yet strong selection performance remains largely limited to large models. We show that small reasoning models can acquire strong GenSelect capabilities through targeted reinforcement learning. To this end, we synthesize selection tasks from large-scale math and code instruction datasets by filtering to instances with both correct and incorrect candidate solutions, and train 1.7B-parameter models with DAPO to reward correct selections. Across math (AIME24, AIME25, HMMT25) and code (LiveCodeBench) reasoning benchmarks, our models consistently outperform prompting and majority-voting baselines, often approaching or exceeding much larger models. Moreover, these gains generalize to selecting outputs from stronger models despite training only on outputs from weaker models. Overall, our results establish reinforcement learning as a scalable way to unlock strong generative selection in small models, enabling efficient test-time scaling.", "AI": {"tldr": "\u901a\u8fc7\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5c0f\u53c2\u6570\u6a21\u578b\u5b9e\u73b0\u5f3a\u5927\u7684\u751f\u6210\u5f0f\u9009\u62e9\u80fd\u529b\uff0c\u5728\u6570\u5b66\u548c\u4ee3\u7801\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\u5e76\u6cdb\u5316\u5230\u66f4\u5927\u6a21\u578b\u7684\u8f93\u51fa\u9009\u62e9\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6269\u5c55\u3002", "motivation": "\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6269\u5c55\u901a\u8fc7\u5e76\u884c\u91c7\u6837\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u53d7\u9650\u4e8e\u6700\u4f73-of-N\u9009\u62e9\u8d28\u91cf\u3002\u751f\u6210\u5f0f\u9009\u62e9\u65b9\u6cd5\uff08\u5982GenSelect\uff09\u89e3\u51b3\u4e86\u8fd9\u4e00\u74f6\u9888\uff0c\u4f46\u5176\u5f3a\u5927\u6027\u80fd\u4e3b\u8981\u96c6\u4e2d\u5728\u5927\u578b\u6a21\u578b\u4e0a\u3002\u672c\u6587\u65e8\u5728\u8bc1\u660e\u5c0f\u578b\u63a8\u7406\u6a21\u578b\u4e5f\u80fd\u901a\u8fc7\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u83b7\u5f97\u5f3a\u5927\u7684\u751f\u6210\u5f0f\u9009\u62e9\u80fd\u529b\u3002", "method": "\u4ece\u5927\u89c4\u6a21\u6570\u5b66\u548c\u4ee3\u7801\u6307\u4ee4\u6570\u636e\u96c6\u4e2d\u5408\u6210\u9009\u62e9\u4efb\u52a1\uff0c\u7b5b\u9009\u51fa\u540c\u65f6\u5305\u542b\u6b63\u786e\u548c\u9519\u8bef\u5019\u9009\u89e3\u7684\u5b9e\u4f8b\uff0c\u4f7f\u7528DAPO\u7b97\u6cd5\u8bad\u7ec317\u4ebf\u53c2\u6570\u6a21\u578b\uff0c\u5956\u52b1\u6b63\u786e\u9009\u62e9\u884c\u4e3a\u3002\u5728\u6570\u5b66\uff08AIME24\u3001AIME25\u3001HMMT25\uff09\u548c\u4ee3\u7801\uff08LiveCodeBench\uff09\u63a8\u7406\u57fa\u51c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8bad\u7ec3\u540e\u7684\u6a21\u578b\u5728\u5404\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u63d0\u793a\u548c\u591a\u6570\u6295\u7968\u57fa\u7ebf\uff0c\u6027\u80fd\u5e38\u5e38\u63a5\u8fd1\u6216\u8d85\u8d8a\u66f4\u5927\u7684\u6a21\u578b\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u5c3d\u7ba1\u8bad\u7ec3\u4ec5\u4f7f\u7528\u5f31\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u9009\u62e9\u66f4\u5f3a\u6a21\u578b\u7684\u8f93\u51fa\u65f6\u4ecd\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u662f\u89e3\u9501\u5c0f\u578b\u6a21\u578b\u5f3a\u5927\u751f\u6210\u5f0f\u9009\u62e9\u80fd\u529b\u7684\u4e00\u79cd\u53ef\u6269\u5c55\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u3002\u8fd9\u4e00\u7ed3\u679c\u4e3a\u4f7f\u7528\u5c0f\u6a21\u578b\u5b9e\u73b0\u9ad8\u6027\u80fd\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2602.01877", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.01877", "abs": "https://arxiv.org/abs/2602.01877", "authors": ["Zichun Wang", "Gar Goei Loke", "Ruiting Zuo"], "title": "Autocorrelated Optimize-via-Estimate: Predict-then-Optimize versus Finite-sample Optimal", "comment": null, "summary": "Models that directly optimize for out-of-sample performance in the finite-sample regime have emerged as a promising alternative to traditional estimate-then-optimize approaches in data-driven optimization. In this work, we compare their performance in the context of autocorrelated uncertainties, specifically, under a Vector Autoregressive Moving Average VARMA(p,q) process. We propose an autocorrelated Optimize-via-Estimate (A-OVE) model that obtains an out-of-sample optimal solution as a function of sufficient statistics, and propose a recursive form for computing its sufficient statistics. We evaluate these models on a portfolio optimization problem with trading costs. A-OVE achieves low regret relative to a perfect information oracle, outperforming predict-then-optimize machine learning benchmarks. Notably, machine learning models with higher accuracy can have poorer decision quality, echoing the growing literature in data-driven optimization. Performance is retained under small mis-specification.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u76f8\u5173\u4f18\u5316\u4f30\u8ba1\u6a21\u578b(A-OVE)\uff0c\u5728VARMA\u8fc7\u7a0b\u4e0b\u76f4\u63a5\u4f18\u5316\u6837\u672c\u5916\u6027\u80fd\uff0c\u5728\u542b\u4ea4\u6613\u6210\u672c\u7684\u7ec4\u5408\u4f18\u5316\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u9884\u6d4b-\u4f18\u5316\u65b9\u6cd5\uff0c\u4e14\u66f4\u9ad8\u7cbe\u5ea6\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u672a\u5fc5\u5e26\u6765\u66f4\u597d\u51b3\u7b56\u8d28\u91cf", "motivation": "\u4f20\u7edf\"\u4f30\u8ba1-\u4f18\u5316\"\u65b9\u6cd5\u5728\u81ea\u76f8\u5173\u4e0d\u786e\u5b9a\u6027\uff08\u5982VARMA\u8fc7\u7a0b\uff09\u4e0b\u5b58\u5728\u5c40\u9650\uff0c\u9700\u5f00\u53d1\u76f4\u63a5\u4f18\u5316\u6837\u672c\u5916\u6027\u80fd\u7684\u66ff\u4ee3\u65b9\u6848\u4ee5\u5e94\u5bf9\u91d1\u878d\u65f6\u5e8f\u7b49\u573a\u666f\u7684\u5e8f\u5217\u76f8\u5173\u6027\u6311\u6218", "method": "\u63d0\u51fa\u81ea\u76f8\u5173\u4f18\u5316\u4f30\u8ba1(A-OVE)\u6a21\u578b\uff0c\u901a\u8fc7\u5145\u5206\u7edf\u8ba1\u91cf\u51fd\u6570\u83b7\u53d6\u6837\u672c\u5916\u6700\u4f18\u89e3\uff0c\u5e76\u8bbe\u8ba1\u9012\u63a8\u7b97\u6cd5\u8ba1\u7b97\u5145\u5206\u7edf\u8ba1\u91cf\uff1b\u5728\u542b\u4ea4\u6613\u6210\u672c\u7684\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e2d\u5bf9\u6bd4VARMA(p,q)\u8fc7\u7a0b\u4e0b\u7684\u6027\u80fd", "result": "A-OVE\u76f8\u6bd4\u5b8c\u7f8e\u4fe1\u606f\u9884\u8a00\u673a\u5b9e\u73b0\u4f4e\u540e\u6094\u503c\uff0c\u663e\u8457\u4f18\u4e8e\u9884\u6d4b-\u4f18\u5316\u7c7b\u673a\u5668\u5b66\u4e60\u57fa\u51c6\uff1b\u9ad8\u7cbe\u5ea6ML\u6a21\u578b\u53ef\u80fd\u51b3\u7b56\u8d28\u91cf\u66f4\u5dee\uff1b\u6a21\u578b\u5728\u5c0f\u89c4\u6a21\u8bef\u8bbe\u5b9a\u4e0b\u4ecd\u4fdd\u6301\u6027\u80fd", "conclusion": "\u76f4\u63a5\u4f18\u5316\u6837\u672c\u5916\u6027\u80fd\u7684\u65b9\u6cd5\u5728\u81ea\u76f8\u5173\u73af\u5883\u4e0b\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u63ed\u793a\u4e86\u9884\u6d4b\u7cbe\u5ea6\u4e0e\u51b3\u7b56\u8d28\u91cf\u7684\u8131\u94a9\u73b0\u8c61\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f"}}
{"id": "2602.02150", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02150", "abs": "https://arxiv.org/abs/2602.02150", "authors": ["Chu Zhao", "Enneng Yang", "Yuting Liu", "Jianzhe Zhao", "Guibing Guo"], "title": "ECHO: Entropy-Confidence Hybrid Optimization for Test-Time Reinforcement Learning", "comment": "19 ppages", "summary": "Test-time reinforcement learning generates multiple candidate answers via repeated rollouts and performs online updates using pseudo-labels constructed by majority voting. To reduce overhead and improve exploration, prior work introduces tree structured rollouts, which share reasoning prefixes and branch at key nodes to improve sampling efficiency. However, this paradigm still faces two challenges: (1) high entropy branching can trigger rollout collapse, where the branching budget concentrates on a few trajectories with consecutive high-entropy segments, rapidly reducing the number of effective branches; (2) early pseudo-labels are noisy and biased, which can induce self-reinforcing overfitting, causing the policy to sharpen prematurely and suppress exploration. To address these issues, we propose Entropy Confidence Hybrid Group Relative Policy Optimization (ECHO). During rollout, ECHO jointly leverages local entropy and group level confidence to adaptively control branch width, and further introduces online confidence-based pruning to terminate persistently low confidence branches, avoiding high entropy traps and mitigating collapse. During policy updates, ECHO employs confidence adaptive clipping and an entropy confidence hybrid advantage shaping approach to enhance training robustness and mitigate early stage bias. Experiments demonstrate that ECHO achieves consistent gains on multiple mathematical and visual reasoning benchmarks, and generalizes more effectively under a limited rollout budget.", "AI": {"tldr": "Proposes ECHO, a test-time RL method that combats rollout collapse and pseudo-label overfitting via adaptive branch control using entropy-confidence hybrid metrics and confidence-based advantage shaping, achieving strong benchmark performance.", "motivation": "Tree-structured rollouts face rollout collapse (budget concentration on high-entropy trajectories) and early noisy pseudo-labels causing self-reinforcing overfitting that prematurely suppresses exploration.", "method": "ECHO adaptively controls branch width by jointly leveraging local entropy and group-level confidence during rollout, prunes low-confidence branches, and applies confidence adaptive clipping with hybrid advantage shaping during updates.", "result": "Demonstrates consistent gains across mathematical and visual reasoning benchmarks with better generalization under constrained rollout budgets.", "conclusion": "The entropy-confidence hybrid approach effectively addresses key challenges in test-time RL, improving robustness and exploration efficiency."}}
{"id": "2602.01897", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01897", "abs": "https://arxiv.org/abs/2602.01897", "authors": ["Sungheon Jeong", "Sanggeon Yun", "Ryozo Masukawa", "Wenjun Haung", "Hanning Chen", "Mohsen Imani"], "title": "Internal Flow Signatures for Self-Checking and Refinement in LLMs", "comment": null, "summary": "Large language models can generate fluent answers that are unfaithful to the provided context, while many safeguards rely on external verification or a separate judge after generation. We introduce \\emph{internal flow signatures} that audit decision formation from depthwise dynamics at a fixed inter-block monitoring boundary. The method stabilizes token-wise motion via bias-centered monitoring, then summarizes trajectories in compact \\emph{moving} readout-aligned subspaces constructed from the top token and its close competitors within each depth window. Neighboring window frames are aligned by an orthogonal transport, yielding depth-comparable transported step lengths, turning angles, and subspace drift summaries that are invariant to within-window basis choices. A lightweight GRU validator trained on these signatures performs self-checking without modifying the base model. Beyond detection, the validator localizes a culprit depth event and enables a targeted refinement: the model rolls back to the culprit token and clamps an abnormal transported step at the identified block while preserving the orthogonal residual. The resulting pipeline provides actionable localization and low-overhead self-checking from internal decision dynamics. \\emph{Code is available at} \\texttt{github.com/EavnJeong/Internal-Flow-Signatures-for-Self-Checking-and-Refinement-in-LLMs}.", "AI": {"tldr": "\u63d0\u51fa\u5185\u90e8\u6d41\u7b7e\u540d\u65b9\u6cd5\uff0c\u901a\u8fc7\u76d1\u63a7LLM\u6df1\u5ea6\u65b9\u5411\u7684\u51b3\u7b56\u52a8\u6001\u5b9e\u73b0\u81ea\u68c0\u4e0e\u4fee\u6b63\uff0c\u65e0\u9700\u5916\u90e8\u9a8c\u8bc1\u6216\u4fee\u6539\u57fa\u7840\u6a21\u578b", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7ed3\u679c\u53ef\u80fd\u8fdd\u80cc\u4e0a\u4e0b\u6587\uff0c\u4e14\u9632\u62a4\u673a\u5236\u4f9d\u8d56\u751f\u6210\u540e\u5916\u90e8\u9a8c\u8bc1\uff0c\u7f3a\u4e4f\u5b9e\u65f6\u5185\u90e8\u51b3\u7b56\u76d1\u63a7", "method": "\u5728\u56fa\u5b9a\u5c42\u95f4\u8fb9\u754c\u91c7\u96c6\u6df1\u5ea6\u52a8\u6001\u6570\u636e\uff0c\u901a\u8fc7\u504f\u7f6e\u4e2d\u5fc3\u5316\u76d1\u63a7\u7a33\u5b9atoken\u8fd0\u52a8\uff0c\u6784\u5efa\u57fa\u4e8e\u9876\u90e8token\u53ca\u5176\u7ade\u4e89\u8005\u7684\u79fb\u52a8\u5b50\u7a7a\u95f4\uff0c\u5229\u7528\u6b63\u4ea4\u4f20\u8f93\u5bf9\u9f50\u76f8\u90bb\u7a97\u53e3\u751f\u6210\u6df1\u5ea6\u53ef\u6bd4\u7684\u6b65\u957f/\u89d2\u5ea6/\u6f02\u79fb\u7279\u5f81\uff0c\u8bad\u7ec3\u8f7b\u91cfGRU\u9a8c\u8bc1\u5668\u8fdb\u884c\u81ea\u68c0\u4e0e\u95ee\u9898\u5b9a\u4f4d", "result": "\u5b9e\u73b0\u65e0\u9700\u4fee\u6539\u57fa\u7840\u6a21\u578b\u7684\u81ea\u68c0\u80fd\u529b\uff0c\u53ef\u5b9a\u4f4d\u95ee\u9898\u53d1\u751f\u5c42\u5e76\u9488\u5bf9\u6027\u56de\u6eda\u4fee\u6b63\u5f02\u5e38\u53c2\u6570\uff0c\u63d0\u4f9b\u4f4e\u5f00\u9500\u7684\u51b3\u7b56\u8fc7\u7a0b\u53ef\u89e3\u91ca\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5185\u90e8\u51b3\u7b56\u52a8\u529b\u5b66\u5b9e\u73b0\u53ef\u64cd\u4f5c\u7684\u9519\u8bef\u5b9a\u4f4d\u4e0e\u4f4e\u5f00\u9500\u81ea\u68c0\uff0c\u4e3a\u63d0\u5347LLM\u751f\u6210\u5fe0\u5b9e\u5ea6\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2602.02179", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02179", "abs": "https://arxiv.org/abs/2602.02179", "authors": ["Marina Mastroleo", "Alberto Archetti", "Federico Mastroleo", "Matteo Matteucci"], "title": "SurvKAN: A Fully Parametric Survival Model Based on Kolmogorov-Arnold Networks", "comment": null, "summary": "Accurate prediction of time-to-event outcomes is critical for clinical decision-making, treatment planning, and resource allocation in modern healthcare. While classical survival models such as Cox remain widely adopted in standard practice, they rely on restrictive assumptions, including linear covariate relationships and proportional hazards over time, that often fail to capture real-world clinical dynamics. Recent deep learning approaches like DeepSurv and DeepHit offer improved expressivity but sacrifice interpretability, limiting clinical adoption where trust and transparency are paramount. Hybrid models incorporating Kolmogorov-Arnold Networks (KANs), such as CoxKAN, have begun to address this trade-off but remain constrained by the semi-parametric Cox framework. In this work we introduce SurvKAN, a fully parametric, time-continuous survival model based on KAN architectures that eliminates the proportional hazards constraint. SurvKAN treats time as an explicit input to a KAN that directly predicts the log-hazard function, enabling end-to-end training on the full survival likelihood. Our architecture preserves interpretability through learnable univariate functions that indicate how individual features influence risk over time. Extensive experiments on standard survival benchmarks demonstrate that SurvKAN achieves competitive or superior performance compared to classical and state-of-the-art baselines across concordance and calibration metrics. Additionally, interpretability analyses reveal clinically meaningful patterns that align with medical domain knowledge.", "AI": {"tldr": "This paper proposes SurvKAN, a fully parametric survival model using Kolmogorov-Arnold Networks that eliminates proportional hazards constraints while maintaining interpretability, achieving competitive performance on clinical time-to-event prediction.", "motivation": "Classical survival models like Cox have restrictive assumptions (linearity, proportional hazards) that don't capture real-world clinical complexity, while deep learning methods sacrifice interpretability needed for clinical trust. Hybrid KAN models still rely on the limited Cox framework.", "method": "Introduces SurvKAN - a fully parametric, time-continuous model where time is an explicit input to a KAN that directly predicts the log-hazard function, enabling end-to-end training on full survival likelihood while preserving interpretability through learnable univariate feature functions.", "result": "Extensive experiments show SurvKAN achieves competitive or superior performance versus classical and state-of-the-art baselines on concordance and calibration metrics, with interpretability analyses revealing clinically meaningful patterns aligned with medical knowledge.", "conclusion": "SurvKAN successfully addresses the accuracy-interpretability trade-off in survival analysis, offering a transparent, high-performing model suitable for clinical adoption where trust and transparency are essential."}}
{"id": "2602.02195", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02195", "abs": "https://arxiv.org/abs/2602.02195", "authors": ["Ao Sun", "Hongtao Zhang", "Heng Zhou", "Yixuan Ma", "Yiran Qin", "Tongrui Su", "Yan Liu", "Zhanyu Ma", "Jun Xu", "Jiuchong Gao", "Jinghua Hao", "Renqing He"], "title": "State Rank Dynamics in Linear Attention LLMs", "comment": null, "summary": "Linear Attention Large Language Models (LLMs) offer a compelling recurrent formulation that compresses context into a fixed-size state matrix, enabling constant-time inference. However, the internal dynamics of this compressed state remain largely opaque. In this work, we present a comprehensive study on the runtime state dynamics of state-of-the-art Linear Attention models. We uncover a fundamental phenomenon termed State Rank Stratification, characterized by a distinct spectral bifurcation among linear attention heads: while one group maintains an effective rank oscillating near zero, the other exhibits rapid growth that converges to an upper bound. Extensive experiments across diverse inference contexts reveal that these dynamics remain strikingly consistent, indicating that the identity of a head,whether low-rank or high-rank,is an intrinsic structural property acquired during pre-training, rather than a transient state dependent on the input data. Furthermore, our diagnostic probes reveal a surprising functional divergence: low-rank heads are indispensable for model reasoning, whereas high-rank heads exhibit significant redundancy. Leveraging this insight, we propose Joint Rank-Norm Pruning, a zero-shot strategy that achieves a 38.9\\% reduction in KV-cache overhead while largely maintaining model accuracy.", "AI": {"tldr": "\u63ed\u793a\u7ebf\u6027\u6ce8\u610f\u529b\u6a21\u578b\u5b58\u5728\u72b6\u6001\u79e9\u5206\u5c42\u73b0\u8c61\uff0c\u4f4e\u79e9\u5934\u4e0d\u53ef\u6216\u7f3a\u800c\u9ad8\u79e9\u5934\u5197\u4f59\uff0c\u636e\u6b64\u63d0\u51fa\u7684\u8054\u5408\u79e9-\u8303\u6570\u526a\u679d\u53ef\u51cf\u5c1138.9% KV\u7f13\u5b58\u4e14\u7cbe\u5ea6\u57fa\u672c\u4e0d\u53d8\u3002", "motivation": "\u7ebf\u6027\u6ce8\u610f\u529bLLM\u5c06\u4e0a\u4e0b\u6587\u538b\u7f29\u4e3a\u56fa\u5b9a\u72b6\u6001\u77e9\u9635\u4ee5\u5b9e\u73b0\u5e38\u6570\u65f6\u95f4\u63a8\u7406\uff0c\u4f46\u5176\u5185\u90e8\u72b6\u6001\u52a8\u6001\u673a\u5236\u5c1a\u4e0d\u900f\u660e\u3002", "method": "\u5bf9\u5148\u8fdb\u7ebf\u6027\u6ce8\u610f\u529b\u6a21\u578b\u8fdb\u884c\u5168\u9762\u7684\u8fd0\u884c\u65f6\u72b6\u6001\u52a8\u529b\u5b66\u7814\u7a76\uff0c\u901a\u8fc7\u8de8\u63a8\u7406\u4e0a\u4e0b\u6587\u5b9e\u9a8c\u5206\u6790\u72b6\u6001\u79e9\u53d8\u5316\u89c4\u5f8b\u3002", "result": "\u53d1\u73b0\u72b6\u6001\u79e9\u5206\u5c42\u73b0\u8c61\uff1a\u6ce8\u610f\u529b\u5934\u56fa\u6709\u5730\u5206\u4e3a\u4f4e\u79e9\uff08\u8fd1\u96f6\u6709\u6548\u79e9\uff09\u548c\u9ad8\u79e9\uff08\u5feb\u901f\u6536\u655b\u81f3\u4e0a\u754c\uff09\u4e24\u7ec4\uff1b\u8fd9\u79cd\u5206\u5c42\u662f\u9884\u8bad\u7ec3\u83b7\u5f97\u7684\u5185\u5728\u7ed3\u6784\u5c5e\u6027\uff1b\u4f4e\u79e9\u5934\u5bf9\u63a8\u7406\u81f3\u5173\u91cd\u8981\uff0c\u9ad8\u79e9\u5934\u5b58\u5728\u663e\u8457\u5197\u4f59\u3002", "conclusion": "\u63d0\u51fa\u96f6\u6837\u672c\u8054\u5408\u79e9-\u8303\u6570\u526a\u679d\u7b56\u7565\uff0c\u5229\u7528\u4f4e\u79e9\u5934\u91cd\u8981\u6027\u9ad8\u3001\u9ad8\u79e9\u5934\u5197\u4f59\u7684\u7279\u6027\uff0c\u5b9e\u73b038.9% KV\u7f13\u5b58\u5f00\u9500\u964d\u4f4e\u4e14\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u6027\u3002"}}
{"id": "2602.01903", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01903", "abs": "https://arxiv.org/abs/2602.01903", "authors": ["Mingyi Li", "Taira Tsuchiya", "Kenji Yamanishi"], "title": "Data- and Variance-dependent Regret Bounds for Online Tabular MDPs", "comment": "80pages, 4tables", "summary": "This work studies online episodic tabular Markov decision processes (MDPs) with known transitions and develops best-of-both-worlds algorithms that achieve refined data-dependent regret bounds in the adversarial regime and variance-dependent regret bounds in the stochastic regime. We quantify MDP complexity using a first-order quantity and several new data-dependent measures for the adversarial regime, including a second-order quantity and a path-length measure, as well as variance-based measures for the stochastic regime. To adapt to these measures, we develop algorithms based on global optimization and policy optimization, both built on optimistic follow-the-regularized-leader with log-barrier regularization. For global optimization, our algorithms achieve first-order, second-order, and path-length regret bounds in the adversarial regime, and in the stochastic regime, they achieve a variance-aware gap-independent bound and a variance-aware gap-dependent bound that is polylogarithmic in the number of episodes. For policy optimization, our algorithms achieve the same data- and variance-dependent adaptivity, up to a factor of the episode horizon, by exploiting a new optimistic $Q$-function estimator. Finally, we establish regret lower bounds in terms of data-dependent complexity measures for the adversarial regime and a variance measure for the stochastic regime, implying that the regret upper bounds achieved by the global-optimization approach are nearly optimal.", "AI": {"tldr": "\u5f00\u53d1\u5df2\u77e5\u8f6c\u79fb\u5728\u7ebfMDP\u7684\u6700\u4f73\u53cc\u4e16\u754c\u7b97\u6cd5\uff0c\u5b9e\u73b0\u5bf9\u6297\u4e0e\u968f\u673a\u73af\u5883\u4e0b\u7684\u81ea\u9002\u5e94\u6700\u4f18\u9057\u61be\u754c\u3002", "motivation": "\u7814\u7a76\u5728\u7ebfepisodic\u8868\u683c\u578bMDP\u95ee\u9898\uff0c\u65e8\u5728\u5f00\u53d1\u80fd\u5728\u5bf9\u6297\u6027\u73af\u5883\u4e0b\u83b7\u5f97\u6570\u636e\u4f9d\u8d56\u9057\u61be\u754c\u3001\u5728\u968f\u673a\u6027\u73af\u5883\u4e0b\u83b7\u5f97\u65b9\u5dee\u4f9d\u8d56\u9057\u61be\u754c\u7684\u6700\u4f73\u53cc\u4e16\u754c\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u590d\u6742\u6027\u5ea6\u91cf\u91cf\u5316MDP\u590d\u6742\u5ea6\u3002", "method": "\u57fa\u4e8e\u4e50\u89c2\u8ddf\u968f\u6b63\u5219\u5316\u9886\u5bfc\u8005\u4e0e\u5bf9\u6570\u969c\u788d\u6b63\u5219\u5316\uff0c\u63d0\u51fa\u5168\u5c40\u4f18\u5316\u548c\u7b56\u7565\u4f18\u5316\u4e24\u7c7b\u7b97\u6cd5\u3002\u9488\u5bf9\u5bf9\u6297\u73af\u5883\u5f15\u5165\u4e00\u9636\u91cf\u3001\u4e8c\u9636\u91cf\u548c\u8def\u5f84\u957f\u5ea6\u5ea6\u91cf\u7b49\u65b0\u6570\u636e\u4f9d\u8d56\u590d\u6742\u5ea6\u6307\u6807\uff0c\u9488\u5bf9\u968f\u673a\u73af\u5883\u5f15\u5165\u65b9\u5dee\u5ea6\u91cf\uff1b\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\u4f7f\u7528\u65b0\u7684\u4e50\u89c2Q\u51fd\u6570\u4f30\u8ba1\u5668\u3002", "result": "\u5168\u5c40\u4f18\u5316\u7b97\u6cd5\u5728\u5bf9\u6297\u73af\u5883\u4e0b\u5b9e\u73b0\u57fa\u4e8e\u4e00\u9636\u3001\u4e8c\u9636\u548c\u8def\u5f84\u957f\u5ea6\u5ea6\u91cf\u7684\u9057\u61be\u754c\uff1b\u5728\u968f\u673a\u73af\u5883\u4e0b\u5b9e\u73b0\u65b9\u5dee\u611f\u77e5\u7684\u95f4\u9699\u65e0\u5173\u754c\u548c\u5173\u4e8eepisode\u6570\u91cf\u5448\u591a\u9879\u5f0f\u5bf9\u6570\u7684\u65b9\u5dee\u611f\u77e5\u95f4\u9699\u76f8\u5173\u754c\u3002\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\u5b9e\u73b0\u7c7b\u4f3c\u7684\u81ea\u9002\u5e94\u6027\uff0c\u4ec5\u76f8\u5dee\u4e00\u4e2aepisode\u957f\u5ea6\u7684\u56e0\u5b50\u3002\u7406\u8bba\u4e0b\u754c\u8bc1\u660e\u5168\u5c40\u4f18\u5316\u65b9\u6cd5\u7684\u9057\u61be\u4e0a\u754c\u51e0\u4e4e\u6700\u4f18\u3002", "conclusion": "\u8be5\u8bba\u6587\u7684\u5168\u5c40\u4f18\u5316\u65b9\u6cd5\u5728\u5bf9\u6297\u548c\u968f\u673a\u4e24\u79cd\u73af\u5883\u4e0b\u5747\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u9057\u61be\u754c\uff0c\u4e3a\u5728\u7ebfMDP\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u6700\u5f3a\u7684\u7b97\u6cd5\u4e4b\u4e00\uff0c\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u66f4\u5177\u4f18\u52bf\u3002"}}
{"id": "2602.02197", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02197", "abs": "https://arxiv.org/abs/2602.02197", "authors": ["Xindian Ma", "Yidi Lu", "Peng Zhang", "Jing Zhang"], "title": "Hierarchical Adaptive Eviction for KV Cache Management in Multimodal Language Models", "comment": "10 oages, 3 figures", "summary": "The integration of visual information into Large Language Models (LLMs) has enabled Multimodal LLMs (MLLMs), but the quadratic memory and computational costs of Transformer architectures remain a bottleneck. Existing KV cache eviction strategies fail to address the heterogeneous attention distributions between visual and text tokens, leading to suboptimal efficiency or degraded performance. In this paper, we propose Hierarchical Adaptive Eviction (HAE), a KV cache eviction framework that optimizes text-visual token interaction in MLLMs by implementing Dual-Attention Pruning during pre-filling (leveraging visual token sparsity and attention variance) and a Dynamic Decoding Eviction Strategy (inspired by OS Recycle Bins) during decoding. HAE minimizes KV cache usage across layers, reduces computational overhead via index broadcasting, and theoretically ensures superior information integrity and lower error bounds compared to greedy strategies, enhancing efficiency in both comprehension and generation tasks. Empirically, HAE reduces KV-Cache memory by 41\\% with minimal accuracy loss (0.3\\% drop) in image understanding tasks and accelerates story generation inference by 1.5x while maintaining output quality on Phi3.5-Vision-Instruct model.", "AI": {"tldr": "\u9488\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u89c6\u89c9-\u6587\u672ctoken\u6ce8\u610f\u529b\u5206\u5e03\u5f02\u8d28\u6027\u5bfc\u81f4\u7684KV\u7f13\u5b58\u6548\u7387\u74f6\u9888\uff0c\u672c\u6587\u63d0\u51fa\u5206\u5c42\u81ea\u9002\u5e94\u6dd8\u6c70\u6846\u67b6HAE\uff0c\u901a\u8fc7\u9884\u586b\u5145\u9636\u6bb5\u7684\u53cc\u6ce8\u610f\u529b\u526a\u679d\u548c\u89e3\u7801\u9636\u6bb5\u7684\u52a8\u6001\u6dd8\u6c70\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5185\u5b58\u5360\u7528\u5e76\u52a0\u901f\u63a8\u7406\u3002", "motivation": "\u73b0\u6709Transformer\u67b6\u6784\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u4e8c\u6b21\u65b9\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u74f6\u9888\uff0c\u800c\u4f20\u7edfKV\u7f13\u5b58\u6dd8\u6c70\u7b56\u7565\u672a\u80fd\u6709\u6548\u5904\u7406\u89c6\u89c9\u4e0e\u6587\u672ctoken\u4e4b\u95f4\u7684\u5f02\u8d28\u6ce8\u610f\u529b\u5206\u5e03\uff0c\u5bfc\u81f4\u6548\u7387\u6b21\u4f18\u6216\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u81ea\u9002\u5e94\u6dd8\u6c70\u6846\u67b6HAE\uff0c\u5305\u542b\u9884\u586b\u5145\u9636\u6bb5\u7684\u53cc\u6ce8\u610f\u529b\u526a\u679d\uff08\u5229\u7528\u89c6\u89c9token\u7a00\u758f\u6027\u548c\u6ce8\u610f\u529b\u65b9\u5dee\uff09\u548c\u89e3\u7801\u9636\u6bb5\u7684\u52a8\u6001\u6dd8\u6c70\u7b56\u7565\uff08\u53d7\u64cd\u4f5c\u7cfb\u7edf\u56de\u6536\u7ad9\u673a\u5236\u542f\u53d1\uff09\uff0c\u901a\u8fc7\u7d22\u5f15\u5e7f\u64ad\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728Phi3.5-Vision-Instruct\u6a21\u578b\u4e0a\uff0cHAE\u5c06KV\u7f13\u5b58\u5185\u5b58\u964d\u4f4e41%\u4e14\u51c6\u786e\u7387\u4ec5\u4e0b\u964d0.3%\uff0c\u5728\u6545\u4e8b\u751f\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b01.5\u500d\u63a8\u7406\u52a0\u901f\u540c\u65f6\u4fdd\u6301\u8f93\u51fa\u8d28\u91cf\u3002", "conclusion": "HAE\u901a\u8fc7\u5206\u5c42\u81ea\u9002\u5e94\u6dd8\u6c70\u673a\u5236\u6709\u6548\u4f18\u5316\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6548\u7387-\u6027\u80fd\u6743\u8861\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01914", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01914", "abs": "https://arxiv.org/abs/2602.01914", "authors": ["Wenbo Pan", "Zhichao Liu", "Xianlong Wang", "Haining Yu", "Xiaohua Jia"], "title": "Towards Long-Horizon Interpretability: Efficient and Faithful Multi-Token Attribution for Reasoning LLMs", "comment": "ICML 2025 submission", "summary": "Token attribution methods provide intuitive explanations for language model outputs by identifying causally important input tokens. However, as modern LLMs increasingly rely on extended reasoning chains, existing schemes face two critical challenges: (1) efficiency bottleneck, where attributing a target span of M tokens within a context of length N requires O(M*N) operations, making long-context attribution prohibitively slow; and (2) faithfulness drop, where intermediate reasoning tokens absorb attribution mass, preventing importance from propagating back to the original input. To address these, we introduce FlashTrace, an efficient multi-token attribution method that employs span-wise aggregation to compute attribution over multi-token targets in a single pass, while maintaining faithfulness. Moreover, we design a recursive attribution mechanism that traces importance through intermediate reasoning chains back to source inputs. Extensive experiments on long-context retrieval (RULER) and multi-step reasoning (MATH, MorehopQA) tasks demonstrate that FlashTrace achieves over 130x speedup over existing baselines while maintaining superior faithfulness. We further analyze the dynamics of recursive attribution, showing that even a single recursive hop improves faithfulness by tracing importance through the reasoning chain.", "AI": {"tldr": "FlashTrace\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u591atoken\u5f52\u56e0\u65b9\u6cd5\uff0c\u901a\u8fc7span-wise\u805a\u5408\u548c\u9012\u5f52\u8ffd\u8e2a\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0130\u500d\u52a0\u901f\u7684\u540c\u65f6\u4fdd\u6301\u5f52\u56e0\u5fe0\u5b9e\u5ea6\u3002", "motivation": "\u73b0\u6709token\u5f52\u56e0\u65b9\u6cd5\u5728\u5904\u7406\u73b0\u4ee3LLM\u7684\u6269\u5c55\u63a8\u7406\u94fe\u65f6\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u6548\u7387\u74f6\u9888\uff08O(M*N)\u590d\u6742\u5ea6\uff09\u548c\u5fe0\u5b9e\u5ea6\u4e0b\u964d\uff08\u4e2d\u95f4\u63a8\u7406token\u5438\u6536\u5f52\u56e0\u6743\u91cd\uff0c\u963b\u788d\u91cd\u8981\u6027\u4f20\u64ad\u56de\u539f\u59cb\u8f93\u5165\uff09\u3002", "method": "\u63d0\u51faFlashTrace\uff0c\u91c7\u7528span-wise\u805a\u5408\u5b9e\u73b0\u5355\u901a\u9053\u591atoken\u5f52\u56e0\uff0c\u5e76\u8bbe\u8ba1\u9012\u5f52\u5f52\u56e0\u673a\u5236\uff0c\u901a\u8fc7\u63a8\u7406\u94fe\u8ffd\u6eaf\u91cd\u8981\u6027\u5230\u6e90\u8f93\u5165\u3002", "result": "\u5728\u957f\u4e0a\u4e0b\u6587\u68c0\u7d22(RULER)\u548c\u591a\u6b65\u63a8\u7406\u4efb\u52a1(MATH, MorehopQA)\u4e0a\uff0cFlashTrace\u76f8\u6bd4\u57fa\u7ebf\u5b9e\u73b0\u8d85\u8fc7130\u500d\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u5353\u8d8a\u7684\u5fe0\u5b9e\u5ea6\u3002\u5206\u6790\u8868\u660e\uff0c\u5355\u6b65\u9012\u5f52\u5373\u53ef\u901a\u8fc7\u63a8\u7406\u94fe\u63d0\u5347\u5f52\u56e0\u5fe0\u5b9e\u5ea6\u3002", "conclusion": "FlashTrace\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u4ee3LLM\u6269\u5c55\u63a8\u7406\u4e0b\u7684token\u5f52\u56e0\u6548\u7387\u548c\u5fe0\u5b9e\u5ea6\u95ee\u9898\uff0c\u4e3a\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\u7684\u6a21\u578b\u8f93\u51fa\u89e3\u91ca\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u65b9\u6848\u3002"}}
{"id": "2602.02201", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02201", "abs": "https://arxiv.org/abs/2602.02201", "authors": ["Abhijit Gupta"], "title": "Cardinality-Preserving Structured Sparse Graph Transformers for Molecular Property Prediction", "comment": null, "summary": "Drug discovery motivates efficient molecular property prediction under limited labeled data. Chemical space is vast, often estimated at approximately 10^60 drug-like molecules, while only thousands of drugs have been approved. As a result, self-supervised pretraining on large unlabeled molecular corpora has become essential for data-efficient molecular representation learning. We introduce **CardinalGraphFormer**, a graph transformer that incorporates Graphormer-inspired structural biases, including shortest-path distance and centrality, as well as direct-bond edge bias, within a structured sparse attention regime limited to shortest-path distance <= 3. The model further augments this design with a cardinality-preserving unnormalized aggregation channel over the same support set. Pretraining combines contrastive graph-level alignment with masked attribute reconstruction. Under a fully matched evaluation protocol, CardinalGraphFormer improves mean performance across all 11 evaluated tasks and achieves statistically significant gains on 10 of 11 public benchmarks spanning MoleculeNet, OGB, and TDC ADMET tasks when compared to strong reproduced baselines.", "AI": {"tldr": "CardinalGraphFormer\u662f\u4e00\u79cd\u7528\u4e8e\u5206\u5b50\u6027\u8d28\u9884\u6d4b\u7684\u56feTransformer\uff0c\u91c7\u7528\u8ddd\u79bb\u22643\u7684\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u3001\u7ed3\u6784\u504f\u7f6e\u548c\u4fdd\u57fa\u6570\u805a\u5408\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u548c\u63a9\u7801\u91cd\u5efa\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u572811\u4e2a\u5206\u5b50\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u5e7f\u9614\u7684\u5316\u5b66\u7a7a\u95f4\uff08\u7ea610^60\u4e2a\u7c7b\u836f\u5206\u5b50\uff09\u4e0e\u5df2\u6279\u51c6\u836f\u7269\u6570\u91cf\u6709\u9650\u4e4b\u95f4\u7684\u5de8\u5927\u5dee\u8ddd\uff0c\u5bfc\u81f4\u5206\u5b50\u6027\u8d28\u9884\u6d4b\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u56e0\u6b64\u5728\u836f\u7269\u53d1\u73b0\u4e2d\uff0c\u57fa\u4e8e\u5927\u89c4\u6a21\u65e0\u6807\u7b7e\u6570\u636e\u8fdb\u884c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u5bf9\u4e8e\u9ad8\u6548\u8868\u793a\u5b66\u4e60\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8be5\u6a21\u578b\u878d\u5408Graphormer\u7ed3\u6784\u504f\u7f6e\uff08\u6700\u77ed\u8def\u5f84\u8ddd\u79bb\u3001\u4e2d\u5fc3\u6027\u3001\u5316\u5b66\u952e\u8fb9\u504f\u7f6e\uff09\uff0c\u91c7\u7528\u8ddd\u79bb\u22643\u7684\u7ed3\u6784\u5316\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u589e\u52a0\u4fdd\u57fa\u6570\u7684\u975e\u5f52\u4e00\u5316\u805a\u5408\u901a\u9053\uff0c\u901a\u8fc7\u5bf9\u6bd4\u56fe\u7ea7\u5bf9\u9f50\u548c\u63a9\u7801\u5c5e\u6027\u91cd\u5efa\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "result": "\u5728\u5b8c\u5168\u5339\u914d\u7684\u8bc4\u4f30\u534f\u8bae\u4e0b\uff0c\u6a21\u578b\u5728\u5168\u90e811\u4e2a\u8bc4\u4f30\u4efb\u52a1\u4e2d\u63d0\u5347\u5e73\u5747\u6027\u80fd\uff0c\u5728MoleculeNet\u3001OGB\u548cTDC ADMET\u4efb\u52a1\u768411\u4e2a\u516c\u5f00\u57fa\u51c6\u4e2d\u768410\u4e2a\u4e0a\u53d6\u5f97\u7edf\u8ba1\u663e\u8457\u6027\u63d0\u5347\uff0c\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "CardinalGraphFormer\u7684\u7ed3\u6784\u5316\u7a00\u758f\u6ce8\u610f\u529b\u548c\u4fdd\u57fa\u6570\u8bbe\u8ba1\u80fd\u6709\u6548\u5229\u7528\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff0c\u5728\u6709\u9650\u6807\u6ce8\u6570\u636e\u4e0b\u5b9e\u73b0\u5353\u8d8a\u7684\u5206\u5b50\u8868\u793a\u5b66\u4e60\u3002"}}
{"id": "2602.02213", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02213", "abs": "https://arxiv.org/abs/2602.02213", "authors": ["Gregory Barber", "Todd C. Henry", "Mulugeta A. Haile"], "title": "Generating Physically Sound Designs from Text and a Set of Physical Constraints", "comment": "NeurIPS 2025", "summary": "We present TIDES, a text informed design approach for generating physically sound designs based on a textual description and a set of physical constraints. TIDES jointly optimizes structural (topology) and visual properties. A pre-trained text-image model is used to measure the design's visual alignment with a text prompt and a differentiable physics simulator is used to measure its physical performance. We evaluate TIDES on a series of structural optimization problems operating under different load and support conditions, at different resolutions, and experimentally in the lab by performing the 3-point bending test on 2D beam designs that are extruded and 3D printed. We find that it can jointly optimize the two objectives and return designs that satisfy engineering design requirements (compliance and density) while utilizing features specified by text.", "AI": {"tldr": "TIDES\u662f\u4e00\u79cd\u6587\u672c\u9a71\u52a8\u7684\u8bbe\u8ba1\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7ed3\u6784\u62d3\u6251\u548c\u89c6\u89c9\u5c5e\u6027\uff0c\u751f\u6210\u65e2\u6ee1\u8db3\u7269\u7406\u7ea6\u675f\u53c8\u7b26\u5408\u6587\u672c\u63cf\u8ff0\u7684\u5de5\u7a0b\u8bbe\u8ba1\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u6839\u636e\u6587\u672c\u63cf\u8ff0\u548c\u7269\u7406\u7ea6\u675f\u751f\u6210\u7269\u7406\u5408\u7406\u8bbe\u8ba1\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u7ed3\u6784\u6027\u80fd\u4e0e\u89c6\u89c9\u5c5e\u6027\u7684\u534f\u540c\u4f18\u5316\u3002", "method": "\u91c7\u7528\u9884\u8bad\u7ec3\u6587\u672c-\u56fe\u50cf\u6a21\u578b\u8bc4\u4f30\u8bbe\u8ba1\u4e0e\u6587\u672c\u63d0\u793a\u7684\u89c6\u89c9\u5bf9\u9f50\u5ea6\uff0c\u7ed3\u5408\u53ef\u5fae\u5206\u7269\u7406\u6a21\u62df\u5668\u8bc4\u4f30\u7269\u7406\u6027\u80fd\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u540c\u65f6\u63d0\u5347\u4e24\u65b9\u9762\u6307\u6807\u3002", "result": "\u5728\u591a\u79cd\u8f7d\u8377\u3001\u652f\u6491\u6761\u4ef6\u548c\u591a\u5206\u8fa8\u7387\u7684\u7ed3\u6784\u4f18\u5316\u95ee\u9898\u4e0a\u9a8c\u8bc1\u6709\u6548\uff0c\u5e76\u901a\u8fc73D\u6253\u5370\u6881\u7684\u4e09\u70b9\u5f2f\u66f2\u5b9e\u9a8c\u8bc1\u5b9e\uff1aTIDES\u80fd\u540c\u65f6\u6ee1\u8db3\u5de5\u7a0b\u8981\u6c42\uff08\u67d4\u5ea6\u548c\u5bc6\u5ea6\uff09\u5e76\u4f53\u73b0\u6587\u672c\u6307\u5b9a\u7684\u7279\u5f81\u3002", "conclusion": "TIDES\u6210\u529f\u5b9e\u73b0\u4e86\u7269\u7406\u6027\u80fd\u4e0e\u6587\u672c\u63cf\u8ff0\u7684\u53cc\u91cd\u4f18\u5316\uff0c\u4e3a\u6587\u672c\u9a71\u52a8\u7684\u53ef\u5236\u9020\u8bbe\u8ba1\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2602.02224", "categories": ["cs.LG", "cs.AI", "math.SP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.02224", "abs": "https://arxiv.org/abs/2602.02224", "authors": ["Georgi Ivanov", "Narmeen Oozeer", "Shivam Raval", "Tasana Pejovic", "Shriyash Upadhyay", "Amir Abdullah"], "title": "Spectral Superposition: A Theory of Feature Geometry", "comment": null, "summary": "Neural networks represent more features than they have dimensions via superposition, forcing features to share representational space. Current methods decompose activations into sparse linear features but discard geometric structure. We develop a theory for studying the geometric structre of features by analyzing the spectra (eigenvalues, eigenspaces, etc.) of weight derived matrices. In particular, we introduce the frame operator $F = WW^\\top$, which gives us a spectral measure that describes how each feature allocates norm across eigenspaces. While previous tools could describe the pairwise interactions between features, spectral methods capture the global geometry (``how do all features interact?''). In toy models of superposition, we use this theory to prove that capacity saturation forces spectral localization: features collapse onto single eigenspaces, organize into tight frames, and admit discrete classification via association schemes, classifying all geometries from prior work (simplices, polygons, antiprisms). The spectral measure formalism applies to arbitrary weight matrices, enabling diagnosis of feature localization beyond toy settings. These results point toward a broader program: applying operator theory to interpretability.", "AI": {"tldr": "The paper develops a spectral theory using the frame operator to analyze geometric structure of features in neural networks under superposition, revealing how capacity saturation forces features to localize onto eigenspaces and form tight frames.", "motivation": "Current methods decompose activations into sparse linear features but discard geometric structure. While previous tools could only describe pairwise feature interactions, there's a need to understand the global geometry of how all features interact when networks represent more features than dimensions via superposition.", "method": "Develops a theory based on spectral analysis of weight-derived matrices, introducing the frame operator F = WW^T which provides a spectral measure describing how features allocate norm across eigenspaces to capture global geometry.", "result": "Proves that capacity saturation in toy models forces spectral localization: features collapse onto single eigenspaces, organize into tight frames, and admit discrete classification via association schemes. The framework classifies all prior geometries (simplices, polygons, antiprisms) and applies to arbitrary weight matrices.", "conclusion": "Establishes a foundation for applying operator theory to neural network interpretability, providing tools to diagnose feature localization beyond toy settings and opening a broader program for spectral methods in understanding neural network representations."}}
{"id": "2602.01924", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.01924", "abs": "https://arxiv.org/abs/2602.01924", "authors": ["Luc\u00eda Gonz\u00e1lez-Zamorano", "Nuria Balb\u00e1s-Esteban", "Vanessa G\u00f3mez-Verdejo", "Albert Belenguer-Llorens", "Carlos Sevilla-Salcedo"], "title": "Bayesian Integration of Nonlinear Incomplete Clinical Data", "comment": null, "summary": "Multimodal clinical data are characterized by high dimensionality, heterogeneous representations, and structured missingness, posing significant challenges for predictive modeling, data integration, and interpretability. We propose BIONIC (Bayesian Integration of Nonlinear Incomplete Clinical data), a unified probabilistic framework that integrates heterogeneous multimodal data under missingness through a joint generative-discriminative latent architecture. BIONIC uses pretrained embeddings for complex modalities such as medical images and clinical text, while incorporating structured clinical variables directly within a Bayesian multimodal formulation. The proposed framework enables robust learning in partially observed and semi-supervised settings by explicitly modeling modality-level and variable-level missingness, as well as missing labels. We evaluate BIONIC on three multimodal clinical and biomedical datasets, demonstrating strong and consistent discriminative performance compared to representative multimodal baselines, particularly under incomplete data scenarios. Beyond predictive accuracy, BIONIC provides intrinsic interpretability through its latent structure, enabling population-level analysis of modality relevance and supporting clinically meaningful insight.", "AI": {"tldr": "BIONIC is a Bayesian framework for integrating incomplete multimodal clinical data via a joint generative-discriminative latent architecture, achieving strong predictive performance and interpretability especially under missing data conditions.", "motivation": "Multimodal clinical data suffers from high dimensionality, heterogeneous representations, and structured missingness, which hinder predictive modeling, data integration, and interpretability.", "method": "Propose BIONIC, a unified probabilistic framework that uses pretrained embeddings for complex modalities (images, text) and incorporates structured clinical variables within a Bayesian formulation. It explicitly models modality-level and variable-level missingness plus missing labels through a joint generative-discriminative latent architecture for robust learning in partially observed and semi-supervised settings.", "result": "Evaluated on three multimodal clinical datasets, BIONIC demonstrates strong and consistent discriminative performance compared to multimodal baselines, particularly excelling in incomplete data scenarios.", "conclusion": "BIONIC provides robust predictive accuracy under missingness while offering intrinsic interpretability through its latent structure, enabling population-level analysis of modality relevance and clinically meaningful insights."}}
{"id": "2602.02238", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02238", "abs": "https://arxiv.org/abs/2602.02238", "authors": ["Laura Yao", "Gengwei Zhang", "Moajjem Chowdhury", "Yunmei Liu", "Tianlong Chen"], "title": "Geometry- and Relation-Aware Diffusion for EEG Super-Resolution", "comment": null, "summary": "Recent electroencephalography (EEG) spatial super-resolution (SR) methods, while showing improved quality by either directly predicting missing signals from visible channels or adapting latent diffusion-based generative modeling to temporal data, often lack awareness of physiological spatial structure, thereby constraining spatial generation performance. To address this issue, we introduce TopoDiff, a geometry- and relation-aware diffusion model for EEG spatial super-resolution. Inspired by how human experts interpret spatial EEG patterns, TopoDiff incorporates topology-aware image embeddings derived from EEG topographic representations to provide global geometric context for spatial generation, together with a dynamic channel-relation graph that encodes inter-electrode relationships and evolves with temporal dynamics. This design yields a spatially grounded EEG spatial super-resolution framework with consistent performance improvements. Across multiple EEG datasets spanning diverse applications, including SEED/SEED-IV for emotion recognition, PhysioNet motor imagery (MI/MM), and TUSZ for seizure detection, our method achieves substantial gains in generation fidelity and leads to notable improvements in downstream EEG task performance.", "AI": {"tldr": "\u63d0\u51faTopoDiff\uff0c\u4e00\u79cd\u878d\u5408\u62d3\u6251\u611f\u77e5\u5d4c\u5165\u4e0e\u52a8\u6001\u901a\u9053\u5173\u7cfb\u56fe\u7684\u8111\u7535\u56fe\u7a7a\u95f4\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u751f\u7406\u7a7a\u95f4\u7ed3\u6784\u63d0\u5347\u7a7a\u95f4\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8111\u7535\u56fe\u7a7a\u95f4\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u7f3a\u4e4f\u751f\u7406\u7a7a\u95f4\u7ed3\u6784\u8ba4\u77e5\uff0c\u5236\u7ea6\u4e86\u7a7a\u95f4\u751f\u6210\u6027\u80fd\u3002", "method": "TopoDiff\u662f\u4e00\u79cd\u51e0\u4f55\u4e0e\u5173\u7cfb\u611f\u77e5\u7684\u6269\u6563\u6a21\u578b\uff0c\u5176\u901a\u8fc7\uff1a1)\u4ece\u8111\u7535\u56fe\u5730\u5f62\u56fe\u63d0\u53d6\u62d3\u6251\u611f\u77e5\u56fe\u50cf\u5d4c\u5165\u63d0\u4f9b\u5168\u5c40\u51e0\u4f55\u4e0a\u4e0b\u6587\uff1b2)\u6784\u5efa\u968f\u65f6\u95f4\u6f14\u53d8\u7684\u52a8\u6001\u901a\u9053\u5173\u7cfb\u56fe\u7f16\u7801\u7535\u6781\u95f4\u5173\u7cfb\u3002", "result": "\u5728\u60c5\u7eea\u8bc6\u522b(SEED/SEED-IV)\u3001\u8fd0\u52a8\u60f3\u8c61(PhysioNet)\u548c\u766b\u75eb\u68c0\u6d4b(TUSZ)\u7b49\u591a\u4e2a\u8111\u7535\u56fe\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u4fdd\u771f\u5ea6\u5e76\u6539\u5584\u4e86\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u878d\u5165\u751f\u7406\u7a7a\u95f4\u7ed3\u6784\u611f\u77e5\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u6301\u7eed\u6539\u8fdb\u7684\u7a7a\u95f4\u57fa\u7840\u5316\u8111\u7535\u56fe\u7a7a\u95f4\u8d85\u5206\u8fa8\u7387\u3002"}}
{"id": "2602.02281", "categories": ["cs.LG", "cs.AI", "cs.NE", "physics.class-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.02281", "abs": "https://arxiv.org/abs/2602.02281", "authors": ["Antonino Emanuele Scurria"], "title": "Backpropagation as Physical Relaxation: Exact Gradients in Finite Time", "comment": "15 pages, 8 figures", "summary": "Backpropagation, the foundational algorithm for training neural networks, is typically understood as a symbolic computation that recursively applies the chain rule. We show it emerges exactly as the finite-time relaxation of a physical dynamical system. By formulating feedforward inference as a continuous-time process and applying Lagrangian theory of non-conservative systems to handle asymmetric interactions, we derive a global energy functional on a doubled state space encoding both activations and sensitivities. The saddle-point dynamics of this energy perform inference and credit assignment simultaneously through local interactions. We term this framework ''Dyadic Backpropagation''. Crucially, we prove that unit-step Euler discretization, the natural timescale of layer transitions, recovers standard backpropagation exactly in precisely 2L steps for an L-layer network, with no approximations. Unlike prior energy-based methods requiring symmetric weights, asymptotic convergence, or vanishing perturbations, our framework guarantees exact gradients in finite time. This establishes backpropagation as the digitally optimized shadow of a continuous physical relaxation, providing a rigorous foundation for exact gradient computation in analog and neuromorphic substrates where continuous dynamics are native.", "AI": {"tldr": "This paper reinterprets backpropagation as the finite-time relaxation of a physical dynamical system, deriving it from Lagrangian mechanics to create \"Dyadic Backpropagation\" which performs exact gradient computation in 2L steps for an L-layer network, providing a rigorous physical foundation for analog computing.", "motivation": "To provide a rigorous physical foundation for backpropagation by showing it emerges exactly from continuous-time dynamical systems, addressing limitations of prior energy-based methods that required symmetric weights or asymptotic convergence.", "method": "Formulating feedforward inference as continuous-time process, applying Lagrangian theory of non-conservative systems to handle asymmetric interactions, deriving global energy functional on doubled state space (activations + sensitivities), and analyzing saddle-point dynamics.", "result": "Proves that unit-step Euler discretization recovers standard backpropagation exactly in precisely 2L steps for an L-layer network with no approximations, unlike prior methods requiring symmetric weights or asymptotic convergence.", "conclusion": "Establishes backpropagation as the \"digitally optimized shadow\" of continuous physical relaxation, providing rigorous foundation for exact gradient computation in analog/neuromorphic substrates where continuous dynamics are native."}}
{"id": "2602.02288", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02288", "abs": "https://arxiv.org/abs/2602.02288", "authors": ["Zheng Li", "Jerry Cheng", "Huanying Gu"], "title": "An Optimization Method for Autoregressive Time Series Forecasting", "comment": "10 pages, 2 figures, 2 tables", "summary": "Current time-series forecasting models are primarily based on transformer-style neural networks. These models achieve long-term forecasting mainly by scaling up the model size rather than through genuinely autoregressive (AR) rollout. From the perspective of large language model training, the traditional training process for time-series forecasting models ignores temporal causality. In this paper, we propose a novel training method for time-series forecasting that enforces two key properties: (1) AR prediction errors should increase with the forecasting horizon. Any violation of this principle is considered random guessing and is explicitly penalized in the loss function, and (2) the method enables models to concatenate short-term AR predictions for forming flexible long-term forecasts. Empirical results demonstrate that our method establishes a new state-of-the-art across multiple benchmarks, achieving an MSE reduction of more than 10% compared to iTransformer and other recent strong baselines. Furthermore, it enables short-horizon forecasting models to perform reliable long-term predictions at horizons over 7.5 times longer. Code is available at https://github.com/LizhengMathAi/AROpt", "AI": {"tldr": "\u63d0\u51faAROpt\u65f6\u95f4\u5e8f\u5217\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5236\u9884\u6d4b\u8bef\u5dee\u968f\u65f6\u95f4\u8303\u56f4\u589e\u957f\u548c\u77ed\u65f6\u9884\u6d4b\u62fc\u63a5\uff0c\u66ff\u4ee3\u6a21\u578b\u7f29\u653e\u5b9e\u73b0\u957f\u65f6\u9884\u6d4b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2dMSE\u964d\u4f4e\u8d8510%", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u589e\u5927\u6a21\u578b\u5c3a\u5bf8\u5b9e\u73b0\u957f\u65f6\u9884\u6d4b\uff0c\u5ffd\u7565\u65f6\u95f4\u56e0\u679c\u6027\uff0c\u5bfc\u81f4\u975e\u771f\u6b63\u81ea\u56de\u5f52\u6eda\u52a8\u9884\u6d4b", "method": "\u8bbe\u8ba1\u4e24\u9636\u6bb5\u8bad\u7ec3\u673a\u5236\uff1a(1)\u5728\u635f\u5931\u51fd\u6570\u4e2d\u60e9\u7f5a\u9884\u6d4b\u8bef\u5dee\u4e0d\u968f\u9884\u6d4b\u65f6\u957f\u589e\u52a0\u7684\u73b0\u8c61\uff1b(2)\u652f\u6301\u77ed\u65f6\u81ea\u56de\u5f52\u9884\u6d4b\u62fc\u63a5\u751f\u6210\u7075\u6d3b\u957f\u65f6\u9884\u6d4b", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u65b0SOTA\uff0cMSE\u8f83iTransformer\u7b49\u57fa\u7ebf\u964d\u4f4e\u8d8510%\uff1b\u4f7f\u77ed\u65f6\u9884\u6d4b\u6a21\u578b\u5b9e\u73b07.5\u500d\u4ee5\u4e0a\u65f6\u957f\u7684\u53ef\u9760\u957f\u65f6\u9884\u6d4b", "conclusion": "\u9a8c\u8bc1\u4e86\u901a\u8fc7\u65f6\u95f4\u56e0\u679c\u6027\u7ea6\u675f\u8bad\u7ec3\u53ef\u7a81\u7834\u6a21\u578b\u5c3a\u5bf8\u9650\u5236\uff0c\u4e3a\u957f\u65f6\u9884\u6d4b\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2602.01949", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01949", "abs": "https://arxiv.org/abs/2602.01949", "authors": ["Leonardo Stoppani", "Davide Bacciu", "Shahab Mokarizadeh"], "title": "Boundary-Constrained Diffusion Models for Floorplan Generation: Balancing Realism and Diversity", "comment": "Accepted at ESANN 2026", "summary": "Diffusion models have become widely popular for automated floorplan generation, producing highly realistic layouts conditioned on user-defined constraints. However, optimizing for perceptual metrics such as the Fr\u00e9chet Inception Distance (FID) causes limited design diversity. To address this, we propose the Diversity Score (DS), a metric that quantifies layout diversity under fixed constraints. Moreover, to improve geometric consistency, we introduce a Boundary Cross-Attention (BCA) module that enables conditioning on building boundaries. Our experiments show that BCA significantly improves boundary adherence, while prolonged training drives diversity collapse undiagnosed by FID, revealing a critical trade-off between realism and diversity. Out-Of-Distribution evaluations further demonstrate the models' reliance on dataset priors, emphasizing the need for generative systems that explicitly balance fidelity, diversity, and generalization in architectural design tasks.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5e73\u9762\u56fe\u81ea\u52a8\u751f\u6210\u4e2d\u5b58\u5728\u7684\u8bbe\u8ba1\u591a\u6837\u6027\u53d7\u9650\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u591a\u6837\u6027\u8bc4\u5206\uff08DS\uff09\u6307\u6807\u548c\u8fb9\u754c\u4ea4\u53c9\u6ce8\u610f\u529b\uff08BCA\uff09\u6a21\u5757\uff0c\u63ed\u793a\u4e86FID\u7b49\u4f20\u7edf\u6307\u6807\u65e0\u6cd5\u68c0\u6d4b\u7684\u591a\u6837\u6027\u5d29\u6e83\u73b0\u8c61\uff0c\u4ee5\u53ca\u771f\u5b9e\u6027\u4e0e\u591a\u6837\u6027\u4e4b\u95f4\u7684\u5173\u952e\u6743\u8861\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u81ea\u52a8\u5316\u5e73\u9762\u56fe\u751f\u6210\u4e2d\u867d\u80fd\u4ea7\u751f\u9ad8\u771f\u5b9e\u611f\u5e03\u5c40\uff0c\u4f46\u4f18\u5316\u611f\u77e5\u6307\u6807\uff08\u5982FID\uff09\u4f1a\u5bfc\u81f4\u8bbe\u8ba1\u591a\u6837\u6027\u53d7\u9650\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u51e0\u4f55\u4e00\u81f4\u6027\u7684\u6709\u6548\u4fdd\u969c\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u548c\u67b6\u6784\u6539\u8fdb\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u591a\u6837\u6027\u8bc4\u5206\uff08DS\uff09\u6307\u6807\u7528\u4e8e\u91cf\u5316\u56fa\u5b9a\u7ea6\u675f\u4e0b\u7684\u5e03\u5c40\u591a\u6837\u6027\uff1b\u8bbe\u8ba1\u8fb9\u754c\u4ea4\u53c9\u6ce8\u610f\u529b\uff08BCA\uff09\u6a21\u5757\uff0c\u901a\u8fc7\u5efa\u7b51\u8fb9\u754c\u6761\u4ef6\u5316\u6765\u589e\u5f3a\u51e0\u4f55\u4e00\u81f4\u6027\uff1b\u901a\u8fc7\u5ef6\u957f\u8bad\u7ec3\u548c\u5206\u5e03\u5916\u8bc4\u4f30\u9a8c\u8bc1\u6a21\u578b\u8868\u73b0\u3002", "result": "BCA\u6a21\u5757\u663e\u8457\u63d0\u5347\u8fb9\u754c\u9075\u5faa\u80fd\u529b\uff1b\u5ef6\u957f\u8bad\u7ec3\u4f1a\u5bfc\u81f4FID\u65e0\u6cd5\u8bca\u65ad\u7684\u591a\u6837\u6027\u5d29\u6e83\uff1b\u63ed\u793a\u4e86\u771f\u5b9e\u6027\u4e0e\u591a\u6837\u6027\u4e4b\u95f4\u7684\u5173\u952e\u6743\u8861\uff1b\u5206\u5e03\u5916\u8bc4\u4f30\u663e\u793a\u6a21\u578b\u4e25\u91cd\u4f9d\u8d56\u6570\u636e\u96c6\u5148\u9a8c\u3002", "conclusion": "\u5efa\u7b51\u751f\u6210\u5f0f\u7cfb\u7edf\u9700\u8981\u663e\u5f0f\u5e73\u8861\u4fdd\u771f\u5ea6\u3001\u591a\u6837\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u6307\u5bfc\u5b9e\u9645\u8bbe\u8ba1\u4efb\u52a1\u3002"}}
{"id": "2602.01953", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01953", "abs": "https://arxiv.org/abs/2602.01953", "authors": ["Dmitrij Schlesinger", "Boris Flach", "Alexander Shekhovtsov"], "title": "Deep Multivariate Models with Parametric Conditionals", "comment": null, "summary": "We consider deep multivariate models for heterogeneous collections of random variables. In the context of computer vision, such collections may e.g. consist of images, segmentations, image attributes, and latent variables. When developing such models, most existing works start from an application task and design the model components and their dependencies to meet the needs of the chosen task. This has the disadvantage of limiting the applicability of the resulting model for other downstream tasks. Here, instead, we propose to represent the joint probability distribution by means of conditional probability distributions for each group of variables conditioned on the rest. Such models can then be used for practically any possible downstream task. Their learning can be approached as training a parametrised Markov chain kernel by maximising the data likelihood of its limiting distribution. This has the additional advantage of allowing a wide range of semi-supervised learning scenarios.", "AI": {"tldr": "\u9488\u5bf9\u5f02\u6784\u53d8\u91cf\u591a\u5143\u5efa\u6a21\uff0c\u73b0\u6709\u4efb\u52a1\u5bfc\u5411\u65b9\u6cd5\u6cdb\u5316\u6027\u5dee\u3002\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u5168\u6761\u4ef6\u6982\u7387\u8868\u793a\u8054\u5408\u5206\u5e03\uff0c\u901a\u8fc7\u53c2\u6570\u5316\u9a6c\u5c14\u53ef\u592b\u94fe\u6838\u5b66\u4e60\uff0c\u5b9e\u73b0\u4efb\u610f\u4e0b\u6e38\u4efb\u52a1\u652f\u6301\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u4ece\u5e94\u7528\u4efb\u52a1\u51fa\u53d1\u8bbe\u8ba1\u7ec4\u4ef6\u548c\u4f9d\u8d56\uff0c\u9650\u5236\u4e86\u5176\u5728\u5176\u4ed6\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u5c06\u8054\u5408\u6982\u7387\u5206\u5e03\u8868\u793a\u4e3a\u5404\u53d8\u91cf\u7ec4\u5728\u5176\u4f59\u53d8\u91cf\u6761\u4ef6\u4e0b\u7684\u6761\u4ef6\u6982\u7387\u5206\u5e03\uff0c\u5e76\u8bad\u7ec3\u53c2\u6570\u5316\u9a6c\u5c14\u53ef\u592b\u94fe\u6838\u4ee5\u6700\u5927\u5316\u6781\u9650\u5206\u5e03\u7684\u6570\u636e\u4f3c\u7136\u3002", "result": "\u6a21\u578b\u53ef\u9002\u7528\u4e8e\u4efb\u610f\u4e0b\u6e38\u4efb\u52a1\uff0c\u5e76\u652f\u6301\u5e7f\u6cdb\u7684\u534a\u76d1\u7763\u5b66\u4e60\u573a\u666f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6761\u4ef6\u5316\u8868\u793a\u548c\u9a6c\u5c14\u53ef\u592b\u94fe\u5b66\u4e60\uff0c\u63d0\u4f9b\u4e86\u66f4\u901a\u7528\u7075\u6d3b\u7684\u591a\u5143\u5efa\u6a21\u65b9\u6848\u3002"}}
{"id": "2602.02366", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02366", "abs": "https://arxiv.org/abs/2602.02366", "authors": ["Sharut Gupta", "Phillip Isola", "Stefanie Jegelka", "David Lopez-Paz", "Kartik Ahuja", "Mark Ibrahim", "Mohammad Pezeshki"], "title": "ReasonCACHE: Teaching LLMs To Reason Without Weight Updates", "comment": "26 pages, 17 Figures", "summary": "Can Large language models (LLMs) learn to reason without any weight update and only through in-context learning (ICL)? ICL is strikingly sample-efficient, often learning from only a handful of demonstrations, but complex reasoning tasks typically demand many training examples to learn from. However, naively scaling ICL by adding more demonstrations breaks down at this scale: attention costs grow quadratically, performance saturates or degrades with longer contexts, and the approach remains a shallow form of learning. Due to these limitations, practitioners predominantly rely on in-weight learning (IWL) to induce reasoning. In this work, we show that by using Prefix Tuning, LLMs can learn to reason without overloading the context window and without any weight updates. We introduce $\\textbf{ReasonCACHE}$, an instantiation of this mechanism that distills demonstrations into a fixed key-value cache. Empirically, across challenging reasoning benchmarks, including GPQA-Diamond, ReasonCACHE outperforms standard ICL and matches or surpasses IWL approaches. Further, it achieves this all while being more efficient across three key axes: data, inference cost, and trainable parameters. We also theoretically prove that ReasonCACHE can be strictly more expressive than low-rank weight update since the latter ties expressivity to input rank, whereas ReasonCACHE bypasses this constraint by directly injecting key-values into the attention mechanism. Together, our findings identify ReasonCACHE as a middle path between in-context and in-weight learning, providing a scalable algorithm for learning reasoning skills beyond the context window without modifying parameters. Our project page: https://reasoncache.github.io/", "AI": {"tldr": "\u63d0\u51faReasonCACHE\uff0c\u4e00\u79cd\u57fa\u4e8ePrefix Tuning\u7684\u65b9\u6cd5\uff0c\u4f7fLLM\u65e0\u9700\u6743\u91cd\u66f4\u65b0\u6216\u8d85\u8f7d\u4e0a\u4e0b\u6587\u7a97\u53e3\u5373\u53ef\u5b66\u4e60\u63a8\u7406\uff0c\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u6807\u51c6ICL\u5e76\u5339\u654cIWL\uff0c\u540c\u65f6\u66f4\u9ad8\u6548\u3002", "motivation": "\u63a2\u7a76LLM\u80fd\u5426\u4ec5\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u65e0\u9700\u6743\u91cd\u66f4\u65b0\u5c31\u80fd\u5b66\u4f1a\u590d\u6742\u63a8\u7406\u3002ICL\u867d\u6837\u672c\u9ad8\u6548\uff0c\u4f46\u6269\u5c55\u7528\u4e8e\u590d\u6742\u63a8\u7406\u65f6\u9762\u4e34\u4e8c\u6b21\u6ce8\u610f\u529b\u6210\u672c\u3001\u957f\u4e0a\u4e0b\u6587\u6027\u80fd\u9971\u548c\u53ca\u6d45\u5c42\u5b66\u4e60\u7b49\u9650\u5236\uff0c\u5bfc\u81f4\u5b9e\u8df5\u8005\u4f9d\u8d56\u6743\u91cd\u5185\u5b66\u4e60\uff08IWL\uff09\u3002", "method": "\u5f15\u5165ReasonCACHE\uff0c\u5229\u7528Prefix Tuning\u5c06\u6f14\u793a\u6837\u672c\u63d0\u70bc\u4e3a\u56fa\u5b9a\u7684\u952e\u503c\u7f13\u5b58\uff0c\u76f4\u63a5\u6ce8\u5165\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u65e0\u6743\u91cd\u66f4\u65b0\u4e14\u4e0d\u8d85\u8f7d\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u63a8\u7406\u5b66\u4e60\u3002", "result": "\u5728GPQA-Diamond\u7b49\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cReasonCACHE\u6027\u80fd\u4f18\u4e8e\u6807\u51c6ICL\u4e14\u5339\u654c\u6216\u8d85\u8d8aIWL\uff1b\u5728\u6570\u636e\u6548\u7387\u3001\u63a8\u7406\u6210\u672c\u548c\u53ef\u8c03\u53c2\u6570\u4e09\u65b9\u9762\u66f4\u9ad8\u6548\uff1b\u7406\u8bba\u4e0a\u8bc1\u660e\u5176\u8868\u8fbe\u80fd\u529b\u4e25\u683c\u5f3a\u4e8e\u4f4e\u79e9\u6743\u91cd\u66f4\u65b0\u3002", "conclusion": "ReasonCACHE\u662f\u4ecb\u4e8e\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e0e\u6743\u91cd\u5185\u5b66\u4e60\u4e4b\u95f4\u7684\u4e2d\u95f4\u8def\u5f84\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u4fee\u6539\u53c2\u6570\u5373\u53ef\u8d85\u8d8a\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u7684\u53ef\u6269\u5c55\u63a8\u7406\u5b66\u4e60\u7b97\u6cd5\u3002"}}
{"id": "2602.01960", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01960", "abs": "https://arxiv.org/abs/2602.01960", "authors": ["Christos Ziakas", "Amir Bar", "Alessandra Russo"], "title": "Grounding Generated Videos in Feasible Plans via World Models", "comment": null, "summary": "Large-scale video generative models have shown emerging capabilities as zero-shot visual planners, yet video-generated plans often violate temporal consistency and physical constraints, leading to failures when mapped to executable actions. To address this, we propose Grounding Video Plans with World Models (GVP-WM), a planning method that grounds video-generated plans into feasible action sequences using a learned action-conditioned world model. At test-time, GVP-WM first generates a video plan from initial and goal observations, then projects the video guidance onto the manifold of dynamically feasible latent trajectories via video-guided latent collocation. In particular, we formulate grounding as a goal-conditioned latent-space trajectory optimization problem that jointly optimizes latent states and actions under world-model dynamics, while preserving semantic alignment with the video-generated plan. Empirically, GVP-WM recovers feasible long-horizon plans from zero-shot image-to-video-generated and motion-blurred videos that violate physical constraints, across navigation and manipulation simulation tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGVP-WM\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u7684\u52a8\u4f5c\u6761\u4ef6\u4e16\u754c\u6a21\u578b\u5c06\u89c6\u9891\u751f\u6210\u7684\u8ba1\u5212\u63a5\u5730\u4e3a\u53ef\u884c\u7684\u52a8\u4f5c\u5e8f\u5217\uff0c\u89e3\u51b3\u96f6\u6837\u672c\u89c6\u89c9\u89c4\u5212\u4e2d\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u7269\u7406\u7ea6\u675f\u8fdd\u53cd\u7684\u95ee\u9898\u3002", "motivation": "\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u6a21\u578b\u4f5c\u4e3a\u96f6\u6837\u672c\u89c6\u89c9\u89c4\u5212\u5668\u5c55\u73b0\u51fa\u65b0\u5174\u80fd\u529b\uff0c\u4f46\u5176\u751f\u6210\u7684\u8ba1\u5212\u5e38\u8fdd\u53cd\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u7269\u7406\u7ea6\u675f\uff0c\u5bfc\u81f4\u6620\u5c04\u5230\u53ef\u6267\u884c\u52a8\u4f5c\u65f6\u5931\u8d25\u3002", "method": "GVP-WM\u5229\u7528\u5b66\u4e60\u7684\u52a8\u4f5c\u6761\u4ef6\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u89c6\u9891\u5f15\u5bfc\u7684\u6f5c\u7a7a\u95f4\u914d\u51c6\u5c06\u89c6\u9891\u8ba1\u5212\u6295\u5f71\u5230\u52a8\u6001\u53ef\u884c\u7684\u6f5c\u8f68\u8ff9\u6d41\u5f62\u4e0a\uff0c\u5c06\u63a5\u5730\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u76ee\u6807\u6761\u4ef6\u6f5c\u7a7a\u95f4\u8f68\u8ff9\u4f18\u5316\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u4e0e\u89c6\u9891\u8ba1\u5212\u8bed\u4e49\u5bf9\u9f50\u7684\u540c\u65f6\u8054\u5408\u4f18\u5316\u6f5c\u72b6\u6001\u548c\u52a8\u4f5c\u3002", "result": "\u5728\u5bfc\u822a\u548c\u64cd\u4f5c\u4eff\u771f\u4efb\u52a1\u4e2d\uff0cGVP-WM\u4ece\u96f6\u6837\u672c\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u4ee5\u53ca\u8fdd\u53cd\u7269\u7406\u7ea6\u675f\u7684\u8fd0\u52a8\u6a21\u7cca\u89c6\u9891\u4e2d\u6062\u590d\u4e86\u53ef\u884c\u7684\u957f\u65f6\u7a0b\u8ba1\u5212\u3002", "conclusion": "GVP-WM\u901a\u8fc7\u5728\u786e\u4fdd\u7269\u7406\u53ef\u884c\u6027\u7684\u540c\u65f6\u4fdd\u6301\u4e0e\u751f\u6210\u89c6\u9891\u8ba1\u5212\u7684\u8bed\u4e49\u5bf9\u9f50\uff0c\u6709\u6548\u5f25\u5408\u4e86\u89c6\u9891\u751f\u6210\u4e0e\u53ef\u6267\u884c\u52a8\u4f5c\u89c4\u5212\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2602.02405", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02405", "abs": "https://arxiv.org/abs/2602.02405", "authors": ["Ethan Mendes", "Jungsoo Park", "Alan Ritter"], "title": "Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning", "comment": null, "summary": "Improving the reasoning capabilities of large language models (LLMs) typically relies either on the model's ability to sample a correct solution to be reinforced or on the existence of a stronger model able to solve the problem. However, many difficult problems remain intractable for even current frontier models, preventing the extraction of valid training signals. A promising alternative is to leverage high-quality expert human solutions, yet naive imitation of this data fails because it is fundamentally out of distribution: expert solutions are typically didactic, containing implicit reasoning gaps intended for human readers rather than computational models. Furthermore, high-quality expert solutions are expensive, necessitating generalizable sample-efficient training methods. We propose Distribution Aligned Imitation Learning (DAIL), a two-step method that bridges the distributional gap by first transforming expert solutions into detailed, in-distribution reasoning traces and then applying a contrastive objective to focus learning on expert insights and methodologies. We find that DAIL can leverage fewer than 1000 high-quality expert solutions to achieve 10-25% pass@k gains on Qwen2.5-Instruct and Qwen3 models, improve reasoning efficiency by 2x to 4x, and enable out-of-domain generalization.", "AI": {"tldr": "This paper proposes DAIL, a two-step imitation learning method that transforms expert solutions into in-distribution reasoning traces and uses contrastive learning to improve LLM reasoning efficiently, achieving significant gains with fewer than 1000 samples.", "motivation": "Current LLM reasoning improvement methods rely on model-generated correct solutions or stronger models, but many hard problems are intractable. Expert solutions exist but are out-of-distribution (didactic with implicit gaps) and expensive, requiring sample-efficient methods.", "method": "DAIL (Distribution Aligned Imitation Learning) with two steps: (1) transform expert solutions into detailed in-distribution reasoning traces, and (2) apply contrastive objective to focus on expert insights and methodologies.", "result": "With fewer than 1000 high-quality expert solutions, achieved 10-25% pass@k gains on Qwen2.5-Instruct and Qwen3 models, 2-4x reasoning efficiency improvement, and out-of-domain generalization.", "conclusion": "DAIL effectively bridges the distribution gap between expert and model solutions, enabling sample-efficient improvement of LLM reasoning capabilities through high-quality human expertise."}}
{"id": "2602.02422", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02422", "abs": "https://arxiv.org/abs/2602.02422", "authors": ["Sayak Chakrabarti", "Toniann Pitassi", "Josh Alman"], "title": "Poly-attention: a general scheme for higher-order self-attention", "comment": null, "summary": "The self-attention mechanism, at the heart of the Transformer model, is able to effectively model pairwise interactions between tokens. However, numerous recent works have shown that it is unable to perform basic tasks involving detecting triples of correlated tokens, or compositional tasks where multiple input tokens need to be referenced to generate a result. Some higher-dimensional alternatives to self-attention have been proposed to address this, including higher-order attention and Strassen attention, which can perform some of these polyadic tasks in exchange for slower, superquadratic running times.\n  In this work, we define a vast class of generalizations of self-attention, which we call poly-attention mechanisms. Our mechanisms can incorporate arbitrary higher-order (tensor) computations as well as arbitrary relationship structures between the input tokens, and they include the aforementioned alternatives as special cases. We then systematically study their computational complexity and representational strength, including giving new algorithms and matching complexity-theoretic lower bounds on the time complexity of computing the attention matrix exactly as well as approximately, and tightly determining which polyadic tasks they can each perform. Our results give interesting trade-offs between different desiderata for these mechanisms, including a tight relationship between how expressive a mechanism is, and how large the coefficients in the model may be so that the mechanism can be approximated in almost-linear time.\n  Notably, we give a new attention mechanism which can be computed exactly in quadratic time, and which can perform function composition for any fixed number of functions. Prior mechanisms, even for just composing two functions, could only be computed in superquadratic time, and our new lower bounds show that faster algorithms for them are not possible.", "AI": {"tldr": "\u63d0\u51fapoly-attention\u901a\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u8ba1\u7b97\u590d\u6742\u6027\u4e0e\u8868\u8fbe\u80fd\u529b\uff0c\u7ed9\u51fa\u65b0\u7684\u4e8c\u6b21\u65f6\u95f4\u6ce8\u610f\u529b\u673a\u5236\u53ef\u6267\u884c\u4efb\u610f\u56fa\u5b9a\u6b21\u6570\u51fd\u6570\u7ec4\u5408\uff0c\u5e76\u8bc1\u660e\u5176\u6700\u4f18\u6027\uff0c\u63ed\u793a\u8868\u8fbe\u80fd\u529b\u4e0e\u8ba1\u7b97\u6548\u7387\u95f4\u7684\u6839\u672c\u6743\u8861", "motivation": "Transformer\u81ea\u6ce8\u610f\u529b\u673a\u5236\u65e0\u6cd5\u5904\u7406\u4e09\u5143\u7ec4\u76f8\u5173\u6216\u7ec4\u5408\u6027\u4efb\u52a1\uff0c\u73b0\u6709\u9ad8\u9636\u6ce8\u610f\u529b\u66ff\u4ee3\u65b9\u6848\u867d\u80fd\u89e3\u51b3\u90e8\u5206\u95ee\u9898\u4f46\u8ba1\u7b97\u590d\u6742\u5ea6\u4e3a\u8d85\u4e8c\u6b21\u65b9", "method": "\u5b9a\u4e49\u5305\u542b\u4efb\u610f\u9ad8\u9636\u5f20\u91cf\u8ba1\u7b97\u548c\u8f93\u5165\u6807\u8bb0\u5173\u7cfb\u7684poly-attention\u673a\u5236\u7c7b\uff0c\u901a\u8fc7\u65b0\u7b97\u6cd5\u548c\u5339\u914d\u4e0b\u754c\u7cfb\u7edf\u7814\u7a76\u5176\u8ba1\u7b97\u590d\u6742\u6027\u4e0e\u8868\u793a\u80fd\u529b", "result": "\u53d1\u73b0\u8868\u8fbe\u80fd\u529b\u4e0e\u8fd1\u4f3c\u8ba1\u7b97\u6240\u9700\u7cfb\u6570\u5927\u5c0f\u7684\u7d27\u5bc6\u5173\u7cfb\uff1b\u63d0\u51fa\u65b0\u673a\u5236\u53ef\u5728\u4e8c\u6b21\u65f6\u95f4\u5185\u7cbe\u786e\u8ba1\u7b97\uff0c\u5e76\u80fd\u6267\u884c\u4efb\u610f\u56fa\u5b9a\u6b21\u6570\u51fd\u6570\u7ec4\u5408\uff0c\u8bc1\u660e\u6b64\u524d\u673a\u5236\u9700\u8d85\u4e8c\u6b21\u65f6\u95f4\u4e14\u4e0d\u53ef\u80fd\u66f4\u5feb", "conclusion": "\u5efa\u7acb\u4e86\u5e7f\u4e49\u6ce8\u610f\u529b\u7684\u7406\u8bba\u6846\u67b6\uff0c\u660e\u786e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u6781\u9650\uff0c\u4e3a\u7ec4\u5408\u4efb\u52a1\u63d0\u4f9b\u4e86\u6700\u4f18\u89e3\u51b3\u65b9\u6848\uff0c\u63ed\u793a\u4e86\u8868\u8fbe\u6027\u4e0e\u8ba1\u7b97\u6548\u7387\u95f4\u7684\u6839\u672c\u6743\u8861"}}
{"id": "2602.01966", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01966", "abs": "https://arxiv.org/abs/2602.01966", "authors": ["Hongzhuo Yu", "Fei Zhu", "Guo-Sen Xie", "Ling Shao"], "title": "Self-Consolidation for Self-Evolving Agents", "comment": null, "summary": "While large language model (LLM) agents have demonstrated impressive problem-solving capabilities, they typically operate as static systems, lacking the ability to evolve through lifelong interaction. Existing attempts to bridge this gap primarily rely on retrieving successful past trajectories as demonstrations. However, this paradigm faces two critical limitations. First, by focusing solely on success, agents overlook the rich pedagogical value embedded in failed attempts, preventing them from identifying and avoiding recurrent pitfalls. Second, continually accumulating textual experiences not only increases the time consumption during retrieval but also inevitably introduces noise and exhausts the largest context window of current LLMs. To address these challenges, we propose a novel self-evolving framework for LLM agents that introduces a complementary evolution mechanism: First, a contrastive reflection strategy is introduced to explicitly summarize error-prone patterns and capture reusable insights. Second, we propose a self-consolidation mechanism that distills non-parametric textual experience into compact learnable parameters. This enables the agent to internalize extensive historical experience directly into its latent space. Extensive experiments demonstrate the advantages of our method in long-term agent evolution.", "AI": {"tldr": "This paper proposes a self-evolving LLM agent framework that learns from both successes and failures through contrastive reflection and self-consolidation, addressing limitations of static systems and context window constraints.", "motivation": "Current LLM agents are static and cannot evolve through lifelong interaction. Existing methods that retrieve successful past trajectories ignore the pedagogical value of failures and face scalability issues due to accumulating textual experiences that increase retrieval time, introduce noise, and exhaust LLM context windows.", "method": "The authors propose a novel self-evolving framework with two mechanisms: (1) a contrastive reflection strategy that explicitly summarizes error-prone patterns and captures reusable insights from both successes and failures, and (2) a self-consolidation mechanism that distills non-parametric textual experiences into compact learnable parameters, enabling the agent to internalize historical experience directly into its latent space.", "result": "Extensive experiments demonstrate the advantages of this method in long-term agent evolution.", "conclusion": "The proposed self-evolving framework with contrastive reflection and self-consolidation effectively addresses the limitations of existing approaches, enabling LLM agents to evolve continuously by learning from both successes and failures while overcoming context window constraints."}}
{"id": "2602.02451", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02451", "abs": "https://arxiv.org/abs/2602.02451", "authors": ["Patrick Cooper", "Alvaro Velasquez"], "title": "Active Causal Experimentalist (ACE): Learning Intervention Strategies via Direct Preference Optimization", "comment": "9 pages, 5 figures", "summary": "Discovering causal relationships requires controlled experiments, but experimentalists face a sequential decision problem: each intervention reveals information that should inform what to try next. Traditional approaches such as random sampling, greedy information maximization, and round-robin coverage treat each decision in isolation, unable to learn adaptive strategies from experience. We propose Active Causal Experimentalist (ACE), which learns experimental design as a sequential policy. Our key insight is that while absolute information gains diminish as knowledge accumulates (making value-based RL unstable), relative comparisons between candidate interventions remain meaningful throughout. ACE exploits this via Direct Preference Optimization, learning from pairwise intervention comparisons rather than non-stationary reward magnitudes. Across synthetic benchmarks, physics simulations, and economic data, ACE achieves 70-71% improvement over baselines at equal intervention budgets (p < 0.001, Cohen's d ~ 2). Notably, the learned policy autonomously discovers that collider mechanisms require concentrated interventions on parent variables, a theoretically-grounded strategy that emerges purely from experience. This suggests preference-based learning can recover principled experimental strategies, complementing theory with learned domain adaptation.", "AI": {"tldr": "ACE is a preference-based reinforcement learning framework that learns optimal sequential experimental designs for causal discovery by using pairwise intervention comparisons instead of unstable absolute reward values, achieving 70-71% improvement over baselines.", "motivation": "Traditional causal discovery methods treat interventions as isolated decisions and cannot learn adaptive strategies from experience, while the sequential nature of experimental design requires using information from previous interventions to inform future choices.", "method": "Active Causal Experimentalist (ACE) learns experimental design as a sequential policy using Direct Preference Optimization, which exploits stable relative comparisons between candidate interventions rather than diminishing absolute information gains.", "result": "ACE achieves 70-71% improvement over baselines across synthetic, physics, and economic datasets (p < 0.001, Cohen's d ~ 2), and autonomously discovers theoretically-grounded strategies like concentrating interventions on parent variables in collider mechanisms.", "conclusion": "Preference-based learning can recover principled experimental strategies from experience alone, complementing theoretical causal discovery methods with learned domain adaptation."}}
{"id": "2602.02009", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02009", "abs": "https://arxiv.org/abs/2602.02009", "authors": ["Ali Baheri"], "title": "Logic-Guided Vector Fields for Constrained Generative Modeling", "comment": null, "summary": "Neuro-symbolic systems aim to combine the expressive structure of symbolic logic with the flexibility of neural learning; yet, generative models typically lack mechanisms to enforce declarative constraints at generation time. We propose Logic-Guided Vector Fields (LGVF), a neuro-symbolic framework that injects symbolic knowledge, specified as differentiable relaxations of logical constraints, into flow matching generative models. LGVF couples two complementary mechanisms: (1) a training-time logic loss that penalizes constraint violations along continuous flow trajectories, with weights that emphasize correctness near the target distribution; and (2) an inference-time adjustment that steers sampling using constraint gradients, acting as a lightweight, logic-informed correction to the learned dynamics. We evaluate LGVF on three constrained generation case studies spanning linear, nonlinear, and multi-region feasibility constraints. Across all settings, LGVF reduces constraint violations by 59-82% compared to standard flow matching and achieves the lowest violation rates in each case. In the linear and ring settings, LGVF also improves distributional fidelity as measured by MMD, while in the multi-obstacle setting, we observe a satisfaction-fidelity trade-off, with improved feasibility but increased MMD. Beyond quantitative gains, LGVF yields constraint-aware vector fields exhibiting emergent obstacle-avoidance behavior, routing samples around forbidden regions without explicit path planning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLogic-Guided Vector Fields (LGVF)\uff0c\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u65f6\u903b\u8f91\u635f\u5931\u548c\u63a8\u7406\u65f6\u68af\u5ea6\u8c03\u6574\uff0c\u5c06\u53ef\u5fae\u903b\u8f91\u7ea6\u675f\u6ce8\u5165\u6d41\u5339\u914d\u751f\u6210\u6a21\u578b\u3002\u5728\u4e09\u4e2a\u7ea6\u675f\u751f\u6210\u4efb\u52a1\u4e2d\uff0cLGVF\u5c06\u7ea6\u675f\u8fdd\u53cd\u7387\u964d\u4f4e59-82%\uff0c\u5e76\u5c55\u73b0\u51fa emergent \u7684\u907f\u969c\u884c\u4e3a\u3002", "motivation": "\u5c3d\u7ba1\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\u65e8\u5728\u7ed3\u5408\u7b26\u53f7\u903b\u8f91\u7684\u8868\u8fbe\u80fd\u529b\u4e0e\u795e\u7ecf\u5b66\u4e60\u7684\u7075\u6d3b\u6027\uff0c\u4f46\u73b0\u6709\u751f\u6210\u6a21\u578b\u7f3a\u4e4f\u5728\u751f\u6210\u65f6\u6267\u884c\u58f0\u660e\u6027\u7ea6\u675f\u7684\u673a\u5236\u3002\u4e3a\u6b64\uff0c\u672c\u6587\u81f4\u529b\u4e8e\u5f00\u53d1\u4e00\u79cd\u80fd\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u6709\u6548\u6574\u5408\u548c\u5f3a\u5236\u6267\u884c\u903b\u8f91\u7ea6\u675f\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faLogic-Guided Vector Fields (LGVF)\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u673a\u5236\uff1a(1) \u8bad\u7ec3\u65f6\u903b\u8f91\u635f\u5931\uff1a\u6cbf\u8fde\u7eed\u6d41\u8f68\u8ff9\u60e9\u7f5a\u7ea6\u675f\u8fdd\u53cd\uff0c\u6743\u91cd\u8bbe\u8ba1\u5f3a\u8c03\u76ee\u6807\u5206\u5e03\u9644\u8fd1\u7684\u6b63\u786e\u6027\uff1b(2) \u63a8\u7406\u65f6\u8c03\u6574\uff1a\u5229\u7528\u7ea6\u675f\u68af\u5ea6\u5f15\u5bfc\u91c7\u6837\uff0c\u4f5c\u4e3a\u5bf9\u5b66\u4e60\u52a8\u6001\u7684\u8f7b\u91cf\u7ea7\u903b\u8f91\u4fee\u6b63\u3002\u7ea6\u675f\u4ee5\u903b\u8f91\u7ea6\u675f\u7684\u53ef\u5fae\u677e\u5f1b\u5f62\u5f0f\u8868\u793a\u5e76\u96c6\u6210\u5230\u6d41\u5339\u914d\u6a21\u578b\u4e2d\u3002", "result": "\u5728\u8de8\u8d8a\u7ebf\u6027\u3001\u975e\u7ebf\u6027\u548c\u591a\u533a\u57df\u53ef\u884c\u6027\u7ea6\u675f\u7684\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cLGVF\u76f8\u6bd4\u6807\u51c6\u6d41\u5339\u914d\u5c06\u7ea6\u675f\u8fdd\u53cd\u7387\u964d\u4f4e59-82%\uff0c\u5728\u6240\u6709\u573a\u666f\u4e2d\u5747\u53d6\u5f97\u6700\u4f4e\u8fdd\u53cd\u7387\u3002\u5728\u7ebf\u6027\u548c\u73af\u5f62\u573a\u666f\u4e2d\uff0cLGVF\u8fd8\u63d0\u5347\u4e86\u5206\u5e03\u4fdd\u771f\u5ea6\uff08MMD\u6307\u6807\uff09\uff1b\u5728\u591a\u969c\u788d\u573a\u666f\u4e2d\uff0c\u5b58\u5728\u6ee1\u8db3\u7ea6\u675f\u4e0e\u4fdd\u771f\u5ea6\u7684\u6743\u8861\u2014\u2014\u53ef\u884c\u6027\u63d0\u5347\u4f46MMD\u589e\u52a0\u3002\u6b64\u5916\uff0cLGVF\u4ea7\u751f\u4e86\u5177\u6709 emergent \u907f\u969c\u884c\u4e3a\u7684\u7ea6\u675f\u611f\u77e5\u5411\u91cf\u573a\u3002", "conclusion": "LGVF\u6210\u529f\u5b9e\u73b0\u4e86\u7b26\u53f7\u77e5\u8bc6\u5411\u795e\u7ecf\u751f\u6210\u6a21\u578b\u7684\u6709\u6548\u6ce8\u5165\uff0c\u5728\u4fdd\u6301\u826f\u597d\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u7ea6\u675f\u8fdd\u53cd\u7387\uff0c\u5e76\u5c55\u73b0\u51fa emergent \u7684\u907f\u969c\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u7ea6\u675f\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.02013", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.02013", "abs": "https://arxiv.org/abs/2602.02013", "authors": ["Xiaoyi Jiang", "Andreas Nienk\u00f6tter"], "title": "SNAP: A Self-Consistent Agreement Principle with Application to Robust Computation", "comment": null, "summary": "We introduce SNAP (Self-coNsistent Agreement Principle), a self-supervised framework for robust computation based on mutual agreement. Based on an Agreement-Reliability Hypothesis SNAP assigns weights that quantify agreement, emphasizing trustworthy items and downweighting outliers without supervision or prior knowledge. A key result is the Exponential Suppression of Outlier Weights, ensuring that outliers contribute negligibly to computations, even in high-dimensional settings. We study properties of SNAP weighting scheme and show its practical benefits on vector averaging and subspace estimation. Particularly, we demonstrate that non-iterative SNAP outperforms the iterative Weiszfeld algorithm and two variants of multivariate median of means. SNAP thus provides a flexible, easy-to-use, broadly applicable approach to robust computation.", "AI": {"tldr": "SNAP\u662f\u4e00\u79cd\u57fa\u4e8e\u81ea\u6d3d\u4e00\u81f4\u6027\u7684\u81ea\u76d1\u7763\u9c81\u68d2\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316\u6570\u636e\u9879\u95f4\u7684\u76f8\u4e92\u4e00\u81f4\u6027\u6765\u5206\u914d\u6743\u91cd\uff0c\u5f3a\u8c03\u53ef\u9760\u9879\u5e76\u6291\u5236\u5f02\u5e38\u503c\uff0c\u5176\u6838\u5fc3\u7406\u8bba\u4fdd\u8bc1\u662f\u5f02\u5e38\u503c\u6743\u91cd\u5448\u6307\u6570\u7ea7\u8870\u51cf\uff0c\u5728\u5411\u91cf\u5e73\u5747\u548c\u5b50\u7a7a\u95f4\u4f30\u8ba1\u4efb\u52a1\u4e2d\u975e\u8fed\u4ee3\u5f0fSNAP\u6027\u80fd\u4f18\u4e8eWeiszfeld\u7b97\u6cd5\u548c\u591a\u53d8\u91cf\u4e2d\u4f4d\u6570\u5747\u503c\u6cd5\u3002", "motivation": "\u73b0\u6709\u9c81\u68d2\u8ba1\u7b97\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u76d1\u7763\u4fe1\u53f7\u6216\u5148\u9a8c\u77e5\u8bc6\uff0c\u5728\u5f02\u5e38\u503c\u5b58\u5728\u65f6\u8ba1\u7b97\u7ed3\u679c\u6613\u53d7\u6c61\u67d3\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7ef4\u573a\u666f\u4e2d\u5f02\u5e38\u503c\u7684\u8d1f\u9762\u5f71\u54cd\u66f4\u4e3a\u663e\u8457\uff0c\u4e9f\u9700\u4e00\u79cd\u65e0\u9700\u76d1\u7763\u3001\u80fd\u81ea\u52a8\u8bc6\u522b\u5e76\u6291\u5236\u5f02\u5e38\u503c\u7684\u901a\u7528\u9c81\u68d2\u8ba1\u7b97\u6846\u67b6\u3002", "method": "\u63d0\u51faSNAP\uff08\u81ea\u6d3d\u4e00\u81f4\u6027\u539f\u5219\uff09\u6846\u67b6\uff0c\u57fa\u4e8e\u4e00\u81f4\u6027-\u53ef\u9760\u6027\u5047\u8bbe\uff0c\u901a\u8fc7\u91cf\u5316\u6570\u636e\u9879\u95f4\u7684\u76f8\u4e92\u4e00\u81f4\u6027\u6765\u8ba1\u7b97\u6743\u91cd\uff0c\u81ea\u52a8\u8bc6\u522b\u53ef\u9760\u6570\u636e\u5e76\u964d\u4f4e\u5f02\u5e38\u503c\u8d21\u732e\uff0c\u5b9e\u73b0\u65e0\u9700\u76d1\u7763\u7684\u9c81\u68d2\u52a0\u6743\u8ba1\u7b97\u3002", "result": "\u7406\u8bba\u7ed3\u679c\uff1a\u5f02\u5e38\u503c\u6743\u91cd\u5448\u6307\u6570\u7ea7\u6291\u5236\uff0c\u786e\u4fdd\u5176\u5728\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u5bf9\u8ba1\u7b97\u7684\u8d21\u732e\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002\u5b9e\u9a8c\u7ed3\u679c\uff1a\u5728\u5411\u91cf\u5e73\u5747\u548c\u5b50\u7a7a\u95f4\u4f30\u8ba1\u4efb\u52a1\u4e2d\uff0c\u975e\u8fed\u4ee3\u5f0fSNAP\u6027\u80fd\u4f18\u4e8e\u8fed\u4ee3\u5f0fWeiszfeld\u7b97\u6cd5\u53ca\u4e24\u79cd\u591a\u53d8\u91cf\u4e2d\u4f4d\u6570\u5747\u503c\u53d8\u4f53\u3002", "conclusion": "SNAP\u4e3a\u9c81\u68d2\u8ba1\u7b97\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u6613\u7528\u4e14\u5e7f\u6cdb\u9002\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u6709\u6548\u5904\u7406\u5f02\u5e38\u503c\u4e14\u65e0\u9700\u76d1\u7763\u4fe1\u53f7\uff0c\u5728\u591a\u79cd\u8ba1\u7b97\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.02015", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02015", "abs": "https://arxiv.org/abs/2602.02015", "authors": ["Jewon Yeom", "Kyubyung Chae", "Hyunggyu Lim", "Yoonna Oh", "Dongyoon Yang", "Taesup Kim"], "title": "Robust Domain Generalization under Divergent Marginal and Conditional Distributions", "comment": null, "summary": "Domain generalization (DG) aims to learn predictive models that can generalize to unseen domains. Most existing DG approaches focus on learning domain-invariant representations under the assumption of conditional distribution shift (i.e., primarily addressing changes in $P(X\\mid Y)$ while assuming $P(Y)$ remains stable). However, real-world scenarios with multiple domains often involve compound distribution shifts where both the marginal label distribution $P(Y)$ and the conditional distribution $P(X\\mid Y)$ vary simultaneously. To address this, we propose a unified framework for robust domain generalization under divergent marginal and conditional distributions. We derive a novel risk bound for unseen domains by explicitly decomposing the joint distribution into marginal and conditional components and characterizing risk gaps arising from both sources of divergence. To operationalize this bound, we design a meta-learning procedure that minimizes and validates the proposed risk bound across seen domains, ensuring strong generalization to unseen ones. Empirical evaluations demonstrate that our method achieves state-of-the-art performance not only on conventional DG benchmarks but also in challenging multi-domain long-tailed recognition settings where both marginal and conditional shifts are pronounced.", "AI": {"tldr": "\u9488\u5bf9\u771f\u5b9e\u573a\u666f\u4e2d\u8fb9\u7f18\u5206\u5e03P(Y)\u548c\u6761\u4ef6\u5206\u5e03P(X|Y)\u540c\u65f6\u53d8\u5316\u7684\u590d\u5408\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u8054\u5408\u5206\u5e03\u5e76\u63a8\u5bfc\u65b0\u7684\u98ce\u9669\u8fb9\u754c\uff0c\u8bbe\u8ba1\u5143\u5b66\u4e60\u7b97\u6cd5\uff0c\u5728\u5e38\u89c4\u57df\u6cdb\u5316\u57fa\u51c6\u548c\u591a\u57df\u957f\u5c3e\u8bc6\u522b\u4efb\u52a1\u4e0a\u5747\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57df\u6cdb\u5316\u65b9\u6cd5\u4e3b\u8981\u89e3\u51b3\u6761\u4ef6\u5206\u5e03\u504f\u79fb(P(X|Y)\u53d8\u5316\uff0cP(Y)\u7a33\u5b9a)\uff0c\u4f46\u771f\u5b9e\u573a\u666f\u5e38\u6d89\u53ca\u8fb9\u7f18\u5206\u5e03P(Y)\u548c\u6761\u4ef6\u5206\u5e03P(X|Y)\u540c\u65f6\u53d8\u5316\u7684\u590d\u5408\u504f\u79fb\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u8054\u5408\u5206\u5e03\u5e76\u663e\u5f0f\u5efa\u6a21\u8fb9\u7f18\u548c\u6761\u4ef6\u5206\u5e03\u5dee\u5f02\uff0c\u63a8\u5bfc\u672a\u89c1\u57df\u7684\u65b0\u98ce\u9669\u8fb9\u754c\uff0c\u5e76\u8bbe\u8ba1\u5143\u5b66\u4e60\u8fc7\u7a0b\u6765\u6700\u5c0f\u5316\u548c\u9a8c\u8bc1\u8be5\u8fb9\u754c\u3002", "result": "\u5728\u5e38\u89c4\u57df\u6cdb\u5316\u57fa\u51c6\u548c\u5177\u6709\u663e\u8457\u8fb9\u7f18\u4e0e\u6761\u4ef6\u504f\u79fb\u7684\u591a\u57df\u957f\u5c3e\u8bc6\u522b\u8bbe\u7f6e\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u6846\u67b6\u80fd\u6709\u6548\u5904\u7406\u590d\u5408\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u57df\u6cdb\u5316\u95ee\u9898\uff0c\u57fa\u4e8e\u98ce\u9669\u8fb9\u754c\u7684\u5143\u5b66\u4e60\u7b56\u7565\u786e\u4fdd\u4e86\u5411\u672a\u89c1\u57df\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.02016", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02016", "abs": "https://arxiv.org/abs/2602.02016", "authors": ["Ionut-Vlad Modoranu", "Philip Zmushko", "Erik Schultheis", "Mher Safaryan", "Dan Alistarh"], "title": "DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers", "comment": null, "summary": "Shampoo is one of the leading approximate second-order optimizers: a variant of it has won the MLCommons AlgoPerf competition, and it has been shown to produce models with lower activation outliers that are easier to compress. Yet, applying Shampoo currently comes at the cost of significant computational slowdown, due to its expensive internal operations. In this paper, we take a significant step to address this shortcoming by proposing \\method (for \\textbf{D}istributed \\textbf{A}ccelerated \\textbf{SH}ampoo), a faster implementation of Distributed Shampoo based on two main new techniques: First, we show that preconditioner blocks can be stacked into 3D tensors to significantly improve GPU utilization; second, we introduce the Newton-DB iteration and the Chebyshev polynomial approximations as novel and faster approaches for computing the inverse matrix roots required by Shampoo. Along with these algorithmic contributions, we provide a first in-depth analysis of how matrix scaling critically affects Shampoo convergence. On the practical side, our GPU-aware implementation achieves up to $4.83\\times$ faster optimizer steps compared to the well-optimized Distributed Shampoo, while Newton-DB attains the lowest validation perplexity per iteration among all tested methods. Our code is available at https://github.com/IST-DASLab/DASH.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDASH\uff0c\u4e00\u79cd\u66f4\u5feb\u7684\u5206\u5e03\u5f0fShampoo\u4f18\u5316\u5668\u5b9e\u73b0\uff0c\u901a\u8fc7GPU\u611f\u77e5\u6280\u672f\u548c\u65b0\u7b97\u6cd5\u663e\u8457\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u6027\u80fd\u3002", "motivation": "Shampoo\u4f18\u5316\u5668\u867d\u7136\u6709\u6548\uff0c\u4f46\u5b58\u5728\u8ba1\u7b97\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u7531\u4e8e\u5176\u6602\u8d35\u7684\u5185\u90e8\u64cd\u4f5c\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u52a0\u901f\u800c\u4e0d\u635f\u5931\u4f18\u5316\u6548\u679c\u3002", "method": "\u63d0\u51faDASH\u65b9\u6cd5\uff0c\u4e3b\u8981\u5305\u62ec\u5c06\u9884\u6761\u4ef6\u5668\u5757\u5806\u53e0\u62103D\u5f20\u91cf\u4ee5\u63d0\u9ad8GPU\u5229\u7528\u7387\uff0c\u4ee5\u53ca\u5f15\u5165\u725b\u987f-DB\u8fed\u4ee3\u548c\u5207\u6bd4\u96ea\u592b\u591a\u9879\u5f0f\u8fd1\u4f3c\u6765\u5feb\u901f\u8ba1\u7b97Shampoo\u6240\u9700\u7684\u9006\u77e9\u9635\u6839\u3002", "result": "\u5b9e\u73b0\u9ad8\u8fbe4.83\u500d\u7684\u4f18\u5316\u5668\u6b65\u9aa4\u52a0\u901f\uff0c\u4e14\u725b\u987f-DB\u5728\u6240\u6709\u6d4b\u8bd5\u65b9\u6cd5\u4e2d\u8fbe\u5230\u6bcf\u8fed\u4ee3\u6700\u4f4e\u9a8c\u8bc1\u56f0\u60d1\u5ea6\u3002", "conclusion": "DASH\u6210\u529f\u89e3\u51b3\u4e86Shampoo\u7684\u8ba1\u7b97\u51cf\u901f\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u5feb\u3001\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u5668\u5b9e\u73b0\u3002"}}
{"id": "2602.02045", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02045", "abs": "https://arxiv.org/abs/2602.02045", "authors": ["Yiming Yang", "Xiaoyuan Cheng", "Yi He", "Kaiyu Li", "Wenxuan Yuan", "Zhuo Sun"], "title": "On Stability and Robustness of Diffusion Posterior Sampling for Bayesian Inverse Problems", "comment": null, "summary": "Diffusion models have recently emerged as powerful learned priors for Bayesian inverse problems (BIPs). Diffusion-based solvers rely on a presumed likelihood for the observations in BIPs to guide the generation process. However, the link between likelihood and recovery quality for BIPs is unclear in previous works. We bridge this gap by characterizing the posterior approximation error and proving the \\emph{stability} of the diffusion-based solvers. Meanwhile, an immediate result of our findings on stability demonstrates the lack of robustness in diffusion-based solvers, which remains unexplored. This can degrade performance when the presumed likelihood mismatches the unknown true data generation processes. To address this issue, we propose a simple yet effective solution, \\emph{robust diffusion posterior sampling}, which is provably \\emph{robust} and compatible with existing gradient-based posterior samplers. Empirical results on scientific inverse problems and natural image tasks validate the effectiveness and robustness of our method, showing consistent performance improvements under challenging likelihood misspecifications.", "AI": {"tldr": "This paper analyzes diffusion models for Bayesian inverse problems, proves their stability but reveals robustness issues when likelihood models are misspecified, and proposes a robust diffusion posterior sampling method that shows consistent improvements on scientific and image tasks.", "motivation": "The link between likelihood and recovery quality in diffusion-based Bayesian inverse problem solvers is unclear, and their robustness to likelihood misspecification remains unexplored, which can degrade performance when the presumed likelihood mismatches the true data generation process.", "method": "The paper proposes a simple yet effective solution called robust diffusion posterior sampling, which is provably robust and compatible with existing gradient-based posterior samplers.", "result": "Theoretical analysis characterizes posterior approximation error and proves stability of diffusion-based solvers, revealing their lack of robustness. Empirical validation on scientific inverse problems and natural image tasks demonstrates consistent performance improvements under challenging likelihood misspecifications.", "conclusion": "The paper bridges the gap between likelihood and recovery quality, identifies robustness issues in diffusion-based solvers, and provides a provably robust solution that effectively handles likelihood model mismatches."}}
{"id": "2602.02047", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02047", "abs": "https://arxiv.org/abs/2602.02047", "authors": ["Peijie Dong", "Ruibo Fan", "Yuechen Tao", "Di Mou", "Wenhu Hu", "Zhenheng Tang", "Yinghao Yu", "Jiamang Wang", "Wenbo Su", "Guodong Yang", "Liping Zhang", "Xiaowen Chu", "Baochun Li", "Bo Li"], "title": "Dissecting Outlier Dynamics in LLM NVFP4 Pretraining", "comment": "39 pages, 32 figures", "summary": "Training large language models using 4-bit arithmetic enhances throughput and memory efficiency. Yet, the limited dynamic range of FP4 increases sensitivity to outliers. While NVFP4 mitigates quantization error via hierarchical microscaling, a persistent loss gap remains compared to BF16. This study conducts a longitudinal analysis of outlier dynamics across architecture during NVFP4 pretraining, focusing on where they localize, why they occur, and how they evolve temporally. We find that, compared with Softmax Attention (SA), Linear Attention (LA) reduces per-tensor heavy tails but still exhibits persistent block-level spikes under block quantization. Our analysis attributes outliers to specific architectural components: Softmax in SA, gating in LA, and SwiGLU in FFN, with \"post-QK\" operations exhibiting higher sensitivity to quantization. Notably, outliers evolve from transient spikes early in training to a small set of persistent hot channels (i.e., channels with persistently large magnitudes) in later stages. Based on these findings, we introduce Hot-Channel Patch (HCP), an online compensation mechanism that identifies hot channels and reinjects residuals using hardware-efficient kernels. We then develop CHON, an NVFP4 training recipe integrating HCP with post-QK operation protection. On GLA-1.3B model trained for 60B tokens, CHON reduces the loss gap to BF16 from 0.94% to 0.58% while maintaining downstream accuracy.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86NVFP4\u91cf\u5316\u9884\u8bad\u7ec3\u4e2d\u7684\u5f02\u5e38\u503c\u52a8\u6001\uff0c\u53d1\u73b0\u5f02\u5e38\u503c\u4ece\u65e9\u671f\u7684\u77ac\u65f6\u5c16\u5cf0\u6f14\u53d8\u4e3a\u540e\u671f\u7684\u6301\u7eed\u70ed\u901a\u9053\uff0c\u5e76\u63d0\u51faCHON\u8bad\u7ec3\u65b9\u6848\uff08\u542b\u70ed\u901a\u9053\u8865\u4e01HCP\uff09\uff0c\u5c06GLA-1.3B\u6a21\u578b\u7684\u635f\u5931\u5dee\u8ddd\u4ece0.94%\u964d\u81f30.58%", "motivation": "4\u4f4d\u91cf\u5316\uff08NVFP4\uff09\u867d\u80fd\u63d0\u5347\u8bad\u7ec3\u6548\u7387\uff0c\u4f46\u56e0\u52a8\u6001\u8303\u56f4\u6709\u9650\u5bfc\u81f4\u5bf9\u5f02\u5e38\u503c\u654f\u611f\uff0c\u4e0eBF16\u76f8\u6bd4\u5b58\u5728\u663e\u8457\u635f\u5931\u5dee\u8ddd\uff0c\u9700\u6df1\u5165\u7406\u89e3\u5e76\u7f13\u89e3\u8be5\u95ee\u9898\u4ee5\u5b9e\u73b0\u9ad8\u6548\u65e0\u635f\u8bad\u7ec3", "method": "\u901a\u8fc7\u7eb5\u5411\u5206\u6790NVFP4\u9884\u8bad\u7ec3\u4e2d\u5f02\u5e38\u503c\u7684\u5b9a\u4f4d\u3001\u6210\u56e0\u548c\u65f6\u5e8f\u6f14\u53d8\uff0c\u63d0\u51fa\u70ed\u901a\u9053\u8865\u4e01\uff08HCP\uff09\u5728\u7ebf\u8865\u507f\u673a\u5236\uff0c\u5e76\u5f00\u53d1\u96c6\u6210HCP\u4e0e\u540e-QK\u64cd\u4f5c\u4fdd\u62a4\u7684CHON\u8bad\u7ec3\u65b9\u6848", "result": "\u572860B token\u4e0a\u8bad\u7ec3GLA-1.3B\u6a21\u578b\u65f6\uff0cCHON\u5c06\u635f\u5931\u5dee\u8ddd\u4ece0.94%\u7f29\u5c0f\u81f30.58%\uff0c\u540c\u65f6\u4fdd\u6301\u4e0b\u6e38\u4efb\u52a1\u7cbe\u5ea6", "conclusion": "\u91cf\u5316\u8bad\u7ec3\u4e2d\u7684\u5f02\u5e38\u503c\u5177\u6709\u67b6\u6784\u7279\u5f02\u6027\u4e14\u968f\u65f6\u95f4\u6f14\u5316\uff0c\u57fa\u4e8e\u70ed\u901a\u9053\u8865\u507f\u7684CHON\u65b9\u6848\u6709\u6548\u7f29\u5c0f\u4e86\u4e0eBF16\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u4e3a\u9ad8\u6548\u91cf\u5316\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84"}}
{"id": "2602.02061", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02061", "abs": "https://arxiv.org/abs/2602.02061", "authors": ["Seoungbin Bae", "Junyoung Son", "Dabeen Lee"], "title": "Learning to Route and Schedule LLMs from User Retrials via Contextual Queueing Bandits", "comment": null, "summary": "Explosive demands for LLMs often cause user queries to accumulate in server queues, requiring efficient routing (query-LLM matching) and scheduling (query prioritization) mechanisms. Several online algorithms are being deployed, but they overlook the following two key challenges inherent to conversational LLM services: (1) unsatisfied users may retry queries, increasing the server backlog, and (2) requests for ``explicit\" feedback, such as ratings, degrade user experiences. In this paper, we develop a joint routing and scheduling algorithm that leverages ``implicit\" feedback inferred from user retrial behaviors. The key idea is to propose and study the framework of contextual queueing bandits with multinomial logit feedback (CQB-MNL). CQB-MNL models query retrials, as well as context-based learning for user preferences over LLMs. Our algorithm, anytime CQB (ACQB), achieves efficient learning while maintaining queue stability by combining Thompson sampling with forced exploration at a decaying rate. We show that ACQB simultaneously achieves a cumulative regret of $\\widetilde{\\mathcal{O}}(\\sqrt{t})$ for routing and a queue length regret of $\\widetilde{\\mathcal{O}}(t^{-1/4})$ for any large $t$. For experiments, we refine query embeddings via contrastive learning while adopting a disjoint parameter model to learn LLM-specific parameters. Experiments on SPROUT, EmbedLLM, and RouterBench datasets confirm that both algorithms consistently outperform baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e0a\u4e0b\u6587\u6392\u961f\u8001\u864e\u673a\u6846\u67b6(CQB-MNL)\uff0c\u901a\u8fc7\u7528\u6237\u91cd\u8bd5\u884c\u4e3a\u9690\u5f0f\u53cd\u9988\u4f18\u5316LLM\u67e5\u8be2\u8def\u7531\u4e0e\u8c03\u5ea6\uff0c\u907f\u514d\u663e\u5f0f\u8bc4\u5206\u5bf9\u4f53\u9a8c\u7684\u5e72\u6270\uff0c\u7b97\u6cd5ACQB\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\u4e0e\u961f\u5217\u7a33\u5b9a", "motivation": "\u73b0\u6709LLM\u670d\u52a1\u961f\u5217\u7ba1\u7406\u7b97\u6cd5\u5ffd\u89c6\u4e24\u5927\u5173\u952e\u95ee\u9898\uff1a\u7528\u6237\u4e0d\u6ee1\u65f6\u91cd\u8bd5\u67e5\u8be2\u5bfc\u81f4\u79ef\u538b\u3001\u663e\u5f0f\u53cd\u9988\uff08\u5982\u8bc4\u5206\uff09\u964d\u4f4e\u4f53\u9a8c\uff0c\u4e9f\u9700\u57fa\u4e8e\u9690\u5f0f\u53cd\u9988\u7684\u4f18\u5316\u673a\u5236", "method": "\u6784\u5efa\u4e0a\u4e0b\u6587\u6392\u961f\u8001\u864e\u673a\u6a21\u578b(CQB-MNL)\uff0c\u5229\u7528\u7528\u6237\u91cd\u8bd5\u884c\u4e3a\u4f5c\u4e3a\u9690\u5f0f\u53cd\u9988\uff1b\u8bbe\u8ba1ACQB\u7b97\u6cd5\uff0c\u7ed3\u5408\u6c64\u666e\u68ee\u91c7\u6837\u4e0e\u8870\u51cf\u5f0f\u5f3a\u5236\u63a2\u7d22\uff0c\u5b9e\u73b0\u8def\u7531\u4e0e\u8c03\u5ea6\u8054\u5408\u4f18\u5316", "result": "\u7406\u8bba\u8bc1\u660eACQB\u7d2f\u79ef\u9057\u61be\u8fbeO\u0303(\u221at)\uff0c\u961f\u5217\u957f\u5ea6\u9057\u61beO\u0303(t^{-1/4})\uff1b\u5728SPROUT/EmbedLLM/RouterBench\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u9a8c\u8bc1\u7b97\u6cd5\u6709\u6548\u6027", "conclusion": "\u901a\u8fc7\u9690\u5f0f\u53cd\u9988\u673a\u5236\u6210\u529f\u89e3\u51b3LLM\u670d\u52a1\u961f\u5217\u6548\u7387\u95ee\u9898\uff0c\u517c\u987e\u5b66\u4e60\u6548\u7387\u4e0e\u7cfb\u7edf\u7a33\u5b9a\u6027\uff0c\u4e3a\u5728\u7ebf\u8c03\u5ea6\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2602.02079", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.02079", "abs": "https://arxiv.org/abs/2602.02079", "authors": ["Daniil Orel", "Dilshod Azizov", "Indraneil Paul", "Yuxia Wang", "Iryna Gurevych", "Preslav Nakov"], "title": "AICD Bench: A Challenging Benchmark for AI-Generated Code Detection", "comment": null, "summary": "Large language models (LLMs) are increasingly capable of generating functional source code, raising concerns about authorship, accountability, and security. While detecting AI-generated code is critical, existing datasets and benchmarks are narrow, typically limited to binary human-machine classification under in-distribution settings. To bridge this gap, we introduce $\\emph{AICD Bench}$, the most comprehensive benchmark for AI-generated code detection. It spans $\\emph{2M examples}$, $\\emph{77 models}$ across $\\emph{11 families}$, and $\\emph{9 programming languages}$, including recent reasoning models. Beyond scale, AICD Bench introduces three realistic detection tasks: ($\\emph{i}$)~$\\emph{Robust Binary Classification}$ under distribution shifts in language and domain, ($\\emph{ii}$)~$\\emph{Model Family Attribution}$, grouping generators by architectural lineage, and ($\\emph{iii}$)~$\\emph{Fine-Grained Human-Machine Classification}$ across human, machine, hybrid, and adversarial code. Extensive evaluation on neural and classical detectors shows that performance remains far below practical usability, particularly under distribution shift and for hybrid or adversarial code. We release AICD Bench as a $\\emph{unified, challenging evaluation suite}$ to drive the next generation of robust approaches for AI-generated code detection. The data and the code are available at https://huggingface.co/AICD-bench}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AICD Bench\uff0c\u4e00\u4e2a\u5305\u542b200\u4e07\u6837\u672c\u300177\u4e2a\u6a21\u578b\u548c9\u79cd\u7f16\u7a0b\u8bed\u8a00\u7684\u5927\u89c4\u6a21AI\u751f\u6210\u4ee3\u7801\u68c0\u6d4b\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u68c0\u6d4b\u5668\u5728\u5206\u5e03\u504f\u79fb\u3001\u6df7\u5408\u4ee3\u7801\u548c\u5bf9\u6297\u6837\u672c\u573a\u666f\u4e0b\u6027\u80fd\u4e25\u91cd\u4e0d\u8db3\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u9c81\u68d2\u68c0\u6d4b\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6311\u6218\u6027\u8bc4\u4f30\u5957\u4ef6\u3002", "motivation": "\u73b0\u6709AI\u4ee3\u7801\u68c0\u6d4b\u6570\u636e\u96c6\u8fc7\u4e8e\u5c40\u9650\uff0c\u901a\u5e38\u4ec5\u9650\u4e8e\u5206\u5e03\u5185\u4e8c\u5206\u7c7b\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u4e2d\u5173\u4e8e authorship\u3001\u95ee\u8d23\u5236\u548c\u5b89\u5168\u6027\u7684\u590d\u6742\u9700\u6c42\uff0c\u4e9f\u9700\u66f4\u5168\u9762\u3001\u771f\u5b9e\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u6784\u5efaAICD Bench\u57fa\u51c6\uff0c\u6db5\u76d6200\u4e07\u4ee3\u7801\u793a\u4f8b\u300177\u4e2a\u6a21\u578b\uff0811\u4e2a\u5bb6\u65cf\uff09\u548c9\u79cd\u7f16\u7a0b\u8bed\u8a00\uff0c\u5e76\u8bbe\u8ba1\u4e09\u4e2a\u73b0\u5b9e\u68c0\u6d4b\u4efb\u52a1\uff1a\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u4e8c\u5206\u7c7b\u3001\u6a21\u578b\u5bb6\u65cf\u5f52\u5c5e\u548c\u7ec6\u7c92\u5ea6\u4eba\u673a\u6df7\u5408\u5206\u7c7b\uff08\u542b\u5bf9\u6297\u4ee3\u7801\uff09\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u795e\u7ecf\u548c\u7ecf\u5178\u68c0\u6d4b\u5668\u3002", "result": "\u73b0\u6709\u68c0\u6d4b\u5668\u6027\u80fd\u8fdc\u672a\u8fbe\u5230\u5b9e\u7528\u6c34\u5e73\uff0c\u5c24\u5176\u5728\u5206\u5e03\u504f\u79fb\u3001\u6df7\u5408\u4ee3\u7801\u548c\u5bf9\u6297\u6837\u672c\u573a\u666f\u4e0b\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u63ed\u793a\u5f53\u524d\u65b9\u6cd5\u7684\u4e25\u91cd\u5c40\u9650\u6027\u3002", "conclusion": "AICD Bench\u4f5c\u4e3a\u4e00\u4e2a\u7edf\u4e00\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u8bc4\u4f30\u5957\u4ef6\uff0c\u5c06\u63a8\u52a8\u5f00\u53d1\u66f4\u9c81\u68d2\u7684AI\u751f\u6210\u4ee3\u7801\u68c0\u6d4b\u65b9\u6cd5\uff0c\u76f8\u5173\u6570\u636e\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.02080", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02080", "abs": "https://arxiv.org/abs/2602.02080", "authors": ["Aryan Alavi Razavi Ravari", "Farnam Mansouri", "Yuxin Chen", "Valentio Iverson", "Adish Singla", "Sandra Zilles"], "title": "Learning Half-Spaces from Perturbed Contrastive Examples", "comment": null, "summary": "We study learning under a two-step contrastive example oracle, as introduced by Mansouri et. al. (2025), where each queried (or sampled) labeled example is paired with an additional contrastive example of opposite label. While Mansouri et al. assume an idealized setting, where the contrastive example is at minimum distance of the originally queried/sampled point, we introduce and analyze a mechanism, parameterized by a non-decreasing noise function $f$, under which this ideal contrastive example is perturbed. The amount of perturbation is controlled by $f(d)$, where $d$ is the distance of the queried/sampled point to the decision boundary. Intuitively, this results in higher-quality contrastive examples for points closer to the decision boundary. We study this model in two settings: (i) when the maximum perturbation magnitude is fixed, and (ii) when it is stochastic.\n  For one-dimensional thresholds and for half-spaces under the uniform distribution on a bounded domain, we characterize active and passive contrastive sample complexity in dependence on the function $f$. We show that, under certain conditions on $f$, the presence of contrastive examples speeds up learning in terms of asymptotic query complexity and asymptotic expected query complexity.", "AI": {"tldr": "This paper extends contrastive learning theory by introducing controlled perturbations to contrastive examples based on distance to decision boundary, proving sample complexity improvements for 1D thresholds and half-spaces under specific noise conditions.", "motivation": "Real-world contrastive examples are rarely perfect; Mansouri et al.'s idealized model assumes minimum-distance contrastive pairs, which is unrealistic. This work bridges theory and practice by modeling perturbations tied to decision boundary proximity.", "method": "Proposes a noise function f(d) where perturbation magnitude depends on distance d to decision boundary. Analyzes two settings: fixed maximum perturbation and stochastic perturbation. Studies 1D thresholds and half-spaces under uniform distribution.", "result": "Characterizes active/passive contrastive sample complexity as function of f. Proves contrastive examples accelerate learning (lower asymptotic query complexity) when f meets specific conditions, especially for boundary-proximal points.", "conclusion": "Controlled perturbation in contrastive learning yields theoretical efficiency gains, demonstrating practical potential for active learning systems where high-quality boundary examples reduce query costs."}}
{"id": "2602.02081", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02081", "abs": "https://arxiv.org/abs/2602.02081", "authors": ["Farnam Mansouri", "Sandra Zilles", "Shai Ben-David"], "title": "Active learning from positive and unlabeled examples", "comment": null, "summary": "Learning from positive and unlabeled data (PU learning) is a weakly supervised variant of binary classification in which the learner receives labels only for (some) positively labeled instances, while all other examples remain unlabeled. Motivated by applications such as advertising and anomaly detection, we study an active PU learning setting where the learner can adaptively query instances from an unlabeled pool, but a queried label is revealed only when the instance is positive and an independent coin flip succeeds; otherwise the learner receives no information. In this paper, we provide the first theoretical analysis of the label complexity of active PU learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u5bf9\u4e3b\u52a8PU\u5b66\u4e60\u4e2d\u7684\u6807\u7b7e\u590d\u6742\u5ea6\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\uff0c\u5176\u4e2d\u5b66\u4e60\u8005\u53ea\u80fd\u4ece\u6b63\u4f8b\u6c60\u4e2d\u6982\u7387\u6027\u5730\u83b7\u53d6\u6807\u7b7e\u53cd\u9988\u3002", "motivation": "\u5e7f\u544a\u548c\u5f02\u5e38\u68c0\u6d4b\u7b49\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u53ea\u6709\u90e8\u5206\u6b63\u4f8b\u6709\u6807\u7b7e\uff0c\u4e14\u67e5\u8be2\u53cd\u9988\u5b58\u5728\u6982\u7387\u6027\u9650\u5236\uff0c\u9700\u8981\u7406\u89e3\u4e3b\u52a8\u67e5\u8be2\u7684\u6837\u672c\u6548\u7387\u3002", "method": "\u7406\u8bba\u5206\u6790\uff1a\u7814\u7a76\u5728\u72ec\u7acb\u786c\u5e01\u629b\u63b7\u673a\u5236\u4e0b\uff08\u4ec5\u6b63\u4f8b\u4e14\u629b\u63b7\u6210\u529f\u65f6\u624d\u8fd4\u56de\u6807\u7b7e\uff09\uff0c\u4e3b\u52a8PU\u5b66\u4e60\u7684\u6807\u7b7e\u590d\u6742\u5ea6\u3002", "result": "\u5efa\u7acb\u4e86\u4e3b\u52a8PU\u5b66\u4e60\u5728\u8be5\u67e5\u8be2\u6a21\u578b\u4e0b\u7684\u9996\u4e2a\u6807\u7b7e\u590d\u6742\u5ea6\u7406\u8bba\u754c\u3002", "conclusion": "\u4e3a\u7406\u89e3\u6982\u7387\u6027\u53cd\u9988\u4e0b\u7684\u4e3b\u52a8PU\u5b66\u4e60\u6837\u672c\u6548\u7387\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u67e5\u8be2\u673a\u5236\u5bf9\u5b66\u4e60\u96be\u5ea6\u7684\u5f71\u54cd\u3002"}}
{"id": "2602.02087", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.02087", "abs": "https://arxiv.org/abs/2602.02087", "authors": ["Andreas Kontogiannis", "Vasilis Pollatos", "Panayotis Mertikopoulos", "Ioannis Panageas"], "title": "Efficient Swap Regret Minimization in Combinatorial Bandits", "comment": "Accepted at AISTATS 2026", "summary": "This paper addresses the problem of designing efficient no-swap regret algorithms for combinatorial bandits, where the number of actions $N$ is exponentially large in the dimensionality of the problem. In this setting, designing efficient no-swap regret translates to sublinear -- in horizon $T$ -- swap regret with polylogarithmic dependence on $N$. In contrast to the weaker notion of external regret minimization - a problem which is fairly well understood in the literature - achieving no-swap regret with a polylogarithmic dependence on $N$ has remained elusive in combinatorial bandits. Our paper resolves this challenge, by introducing a no-swap-regret learning algorithm with regret that scales polylogarithmically in $N$ and is tight for the class of combinatorial bandits. To ground our results, we also demonstrate how to implement the proposed algorithm efficiently -- that is, with a per-iteration complexity that also scales polylogarithmically in $N$ -- across a wide range of well-studied applications.", "AI": {"tldr": "\u672c\u6587\u89e3\u51b3\u4e86\u7ec4\u5408\u8001\u864e\u673a\u4e2d\u65e0\u4ea4\u6362\u9057\u61be\u7b97\u6cd5\u7684\u8bbe\u8ba1\u96be\u9898\uff0c\u5728\u52a8\u4f5c\u7a7a\u95f4N\u6307\u6570\u7ea7\u589e\u957f\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u51fa\u4e86\u5bf9N\u5177\u6709\u591a\u9879\u5f0f\u5bf9\u6570\u4f9d\u8d56\u6027\u7684\u9ad8\u6548\u7b97\u6cd5\uff0c\u4e14\u5728\u7406\u8bba\u4e0a\u8fbe\u5230\u6700\u4f18\u754c\u3002", "motivation": "\u5728\u7ec4\u5408\u8001\u864e\u673a\u4e2d\uff0c\u5f53\u52a8\u4f5c\u6570\u91cfN\u76f8\u5bf9\u4e8e\u95ee\u9898\u7ef4\u5ea6\u5448\u6307\u6570\u7ea7\u589e\u957f\u65f6\uff0c\u8bbe\u8ba1\u5177\u6709\u6b21\u7ebf\u6027\u65f6\u95f4\u8303\u56f4T\u9057\u61be\u4e14\u5bf9N\u5177\u6709\u591a\u9879\u5f0f\u5bf9\u6570\u4f9d\u8d56\u6027\u7684\u65e0\u4ea4\u6362\u9057\u61be\u7b97\u6cd5\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u6311\u6218\u3002\u867d\u7136\u5916\u90e8\u9057\u61be\u6700\u5c0f\u5316\u5728\u8be5\u9886\u57df\u5df2\u6709\u8f83\u597d\u7406\u89e3\uff0c\u4f46\u65e0\u4ea4\u6362\u9057\u61be\u7684\u5b9e\u73b0\u5bf9N\u7684\u591a\u9879\u5f0f\u5bf9\u6570\u4f9d\u8d56\u6027\u4e00\u76f4\u96be\u4ee5\u5b9e\u73b0\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u65e0\u4ea4\u6362\u9057\u61be\u5b66\u4e60\u7b97\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u7ec4\u5408\u8001\u864e\u673a\u8bbe\u8ba1\u3002\u8be5\u7b97\u6cd5\u5728N\u4e0a\u5177\u6709\u591a\u9879\u5f0f\u5bf9\u6570\u4f9d\u8d56\u6027\uff0c\u5728T\u4e0a\u5177\u6709\u6b21\u7ebf\u6027\u4f9d\u8d56\u6027\u3002\u8bba\u6587\u8fd8\u63d0\u4f9b\u4e86\u9ad8\u6548\u5b9e\u73b0\u65b9\u6848\uff0c\u5728\u5404\u79cd\u5e94\u7528\u4e2d\u4fdd\u6301\u6bcf\u6b21\u8fed\u4ee3\u7684\u591a\u9879\u5f0f\u5bf9\u6570\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u6240\u63d0\u7b97\u6cd5\u5b9e\u73b0\u4e86\u5bf9N\u5177\u6709\u591a\u9879\u5f0f\u5bf9\u6570\u4f9d\u8d56\u6027\u7684\u4ea4\u6362\u9057\u61be\uff0c\u4e14\u8be5\u754c\u5bf9\u7ec4\u5408\u8001\u864e\u673a\u7c7b\u522b\u662f\u7d27\u7684\u3002\u6b64\u5916\uff0c\u7b97\u6cd5\u80fd\u4ee5\u6bcf\u6b21\u8fed\u4ee3\u8ba1\u7b97\u590d\u6742\u5ea6\u4e5f\u5448\u591a\u9879\u5f0f\u5bf9\u6570\u4f9d\u8d56N\u7684\u65b9\u5f0f\u5b9e\u73b0\uff0c\u4f7f\u5176\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e0a\u5177\u6709\u5b9e\u9645\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u89e3\u51b3\u4e86\u7ec4\u5408\u8001\u864e\u673a\u9886\u57df\u7684\u4e00\u4e2a\u91cd\u8981\u5f00\u653e\u95ee\u9898\uff0c\u901a\u8fc7\u63d0\u4f9b\u9996\u4e2a\u5177\u6709\u6700\u4f18\u591a\u9879\u5f0f\u5bf9\u6570\u4f9d\u8d56\u6027\u7684\u9ad8\u6548\u65e0\u4ea4\u6362\u9057\u61be\u7b97\u6cd5\uff0c\u5728\u7406\u8bba\u6700\u4f18\u6027\u548c\u5b9e\u9645\u53ef\u5b9e\u73b0\u6027\u4e4b\u95f4\u67b6\u8d77\u4e86\u6865\u6881\uff0c\u4e3a\u5c06\u8001\u864e\u673a\u7b97\u6cd5\u5e94\u7528\u4e8e\u590d\u6742\u9ad8\u7ef4\u95ee\u9898\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.02103", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02103", "abs": "https://arxiv.org/abs/2602.02103", "authors": ["Liyan Xu", "Mo Yu", "Fandong Meng", "Jie Zhou"], "title": "No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs", "comment": null, "summary": "This work stems from prior complementary observations on the dynamics of Chain-of-Thought (CoT): Large Language Models (LLMs) is shown latent planning of subsequent reasoning prior to CoT emergence, thereby diminishing the significance of explicit CoT; whereas CoT remains critical for tasks requiring multi-step reasoning. To deepen the understanding between LLM's internal states and its verbalized reasoning trajectories, we investigate the latent planning strength of LLMs, through our probing method, Tele-Lens, applying to hidden states across diverse task domains. Our empirical results indicate that LLMs exhibit a myopic horizon, primarily conducting incremental transitions without precise global planning. Leveraging this characteristic, we propose a hypothesis on enhancing uncertainty estimation of CoT, which we validate that a small subset of CoT positions can effectively represent the uncertainty of the entire path. We further underscore the significance of exploiting CoT dynamics, and demonstrate that automatic recognition of CoT bypass can be achieved without performance degradation. Our code, data and models are released at https://github.com/lxucs/tele-lens.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7Tele-Lens\u63a2\u9488\u65b9\u6cd5\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u601d\u7ef4\u94fe\u63a8\u7406\u4e2d\u5b58\u5728\"\u77ed\u89c6\"\u7279\u6027\uff0c\u7f3a\u4e4f\u5168\u5c40\u89c4\u5212\u80fd\u529b\uff0c\u4f46\u4ec5\u9700\u5c11\u91cf\u63a8\u7406\u4f4d\u7f6e\u5373\u53ef\u4f30\u8ba1\u6574\u4f53\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u65e0\u9700\u6027\u80fd\u635f\u5931\u7684\u601d\u7ef4\u94fe\u81ea\u52a8\u8bc6\u522b\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u8868\u660e\u5927\u8bed\u8a00\u6a21\u578b\u5728\u601d\u7ef4\u94fe\u51fa\u73b0\u524d\u5df2\u8fdb\u884c\u6f5c\u5728\u89c4\u5212\uff0c\u8fd9\u964d\u4f4e\u4e86\u663e\u5f0f\u601d\u7ef4\u94fe\u7684\u91cd\u8981\u6027\uff0c\u4f46\u601d\u7ef4\u94fe\u5bf9\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u4ecd\u81f3\u5173\u91cd\u8981\u3002\u4e3a\u6df1\u5165\u7406\u89e3\u6a21\u578b\u5185\u90e8\u72b6\u6001\u4e0e\u8bed\u8a00\u5316\u63a8\u7406\u8f68\u8ff9\u7684\u5173\u7cfb\uff0c\u9700\u8981\u63a2\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u5728\u89c4\u5212\u80fd\u529b\u3002", "method": "\u63d0\u51faTele-Lens\u63a2\u9488\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u9690\u85cf\u72b6\u6001\u6765\u63a2\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u5728\u89c4\u5212\u5f3a\u5ea6\uff0c\u5e76\u5728\u591a\u4efb\u52a1\u9886\u57df\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "\u5b9e\u8bc1\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u51fa\"\u8fd1\u89c6\u89c6\u91ce\"\uff0c\u4e3b\u8981\u8fdb\u884c\u589e\u91cf\u5f0f\u8f6c\u6362\u800c\u975e\u7cbe\u786e\u7684\u5168\u5c40\u89c4\u5212\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51fa\u5047\u8bbe\uff1a\u5c11\u91cf\u601d\u7ef4\u94fe\u4f4d\u7f6e\u53ef\u6709\u6548\u4ee3\u8868\u6574\u4e2a\u8def\u5f84\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4e14\u81ea\u52a8\u8bc6\u522b\u601d\u7ef4\u94fe\u65c1\u8def\u53ef\u5728\u4e0d\u635f\u5931\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5229\u7528\u601d\u7ef4\u94fe\u52a8\u6001\u7279\u6027\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u6539\u8fdb\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u5b9e\u73b0\u9ad8\u6548\u7684\u63a8\u7406\u8def\u5f84\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.02110", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02110", "abs": "https://arxiv.org/abs/2602.02110", "authors": ["Zhongqian Fu", "Tianyi Zhao", "Kai Han", "Hang Zhou", "Xinghao Chen", "Yunhe Wang"], "title": "An Empirical Study of World Model Quantization", "comment": null, "summary": "World models learn an internal representation of environment dynamics, enabling agents to simulate and reason about future states within a compact latent space for tasks such as planning, prediction, and inference. However, running world models rely on hevay computational cost and memory footprint, making model quantization essential for efficient deployment. To date, the effects of post-training quantization (PTQ) on world models remain largely unexamined. In this work, we present a systematic empirical study of world model quantization using DINO-WM as a representative case, evaluating diverse PTQ methods under both weight-only and joint weight-activation settings. We conduct extensive experiments on different visual planning tasks across a wide range of bit-widths, quantization granularities, and planning horizons up to 50 iterations. Our results show that quantization effects in world models extend beyond standard accuracy and bit-width trade-offs: group-wise weight quantization can stabilize low-bit rollouts, activation quantization granularity yields inconsistent benefits, and quantization sensitivity is highly asymmetric between encoder and predictor modules. Moreover, aggressive low-bit quantization significantly degrades the alignment between the planning objective and task success, leading to failures that cannot be remedied by additional optimization. These findings reveal distinct quantization-induced failure modes in world model-based planning and provide practical guidance for deploying quantized world models under strict computational constraints. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/QuantWM.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u540e\u8bad\u7ec3\u91cf\u5316\u5bf9\u4e16\u754c\u6a21\u578b\uff08\u4ee5DINO-WM\u4e3a\u4ee3\u8868\uff09\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u4e86\u6a21\u5757\u654f\u611f\u5ea6\u4e0d\u5bf9\u79f0\u3001\u4f4e\u6bd4\u7279\u91cf\u5316\u4e0b\u89c4\u5212\u76ee\u6807\u4e0e\u4efb\u52a1\u6210\u529f\u7387\u5bf9\u9f50\u9000\u5316\u7b49\u72ec\u7279\u5931\u6548\u6a21\u5f0f\uff0c\u4e3a\u53d7\u9650\u8ba1\u7b97\u73af\u5883\u4e0b\u7684\u90e8\u7f72\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\u3002", "motivation": "\u4e16\u754c\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u5185\u5b58\u5360\u7528\u5927\uff0c\u4f46\u5176\u540e\u8bad\u7ec3\u91cf\u5316\u6548\u679c\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u8fd9\u963b\u788d\u4e86\u9ad8\u6548\u90e8\u7f72\u3002", "method": "\u4ee5DINO-WM\u4e3a\u4ee3\u8868\u6848\u4f8b\uff0c\u5728\u6743\u91cd\u5355\u72ec\u91cf\u5316\u548c\u6743\u91cd-\u6fc0\u6d3b\u8054\u5408\u91cf\u5316\u8bbe\u7f6e\u4e0b\uff0c\u8bc4\u4f30\u591a\u79cdPTQ\u65b9\u6cd5\uff0c\u5728\u89c6\u89c9\u89c4\u5212\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8986\u76d6\u4e0d\u540c\u6bd4\u7279\u5bbd\u5ea6\u3001\u91cf\u5316\u7c92\u5ea6\u548c\u6700\u957f50\u6b65\u7684\u89c4\u5212horizon\u3002", "result": "\u53d1\u73b0\u5206\u7ec4\u6743\u91cd\u91cf\u5316\u53ef\u7a33\u5b9a\u4f4e\u6bd4\u7279rollout\uff0c\u6fc0\u6d3b\u91cf\u5316\u7c92\u5ea6\u6536\u76ca\u4e0d\u4e00\u81f4\uff0c\u7f16\u7801\u5668\u548c\u9884\u6d4b\u5668\u6a21\u5757\u7684\u91cf\u5316\u654f\u611f\u5ea6\u9ad8\u5ea6\u4e0d\u5bf9\u79f0\uff0c\u6fc0\u8fdb\u7684\u4f4e\u6bd4\u7279\u91cf\u5316\u4f1a\u4e25\u91cd\u7834\u574f\u89c4\u5212\u76ee\u6807\u4e0e\u4efb\u52a1\u6210\u529f\u4e4b\u95f4\u7684\u5bf9\u9f50\uff0c\u5bfc\u81f4\u65e0\u6cd5\u901a\u8fc7\u989d\u5916\u4f18\u5316\u4fee\u590d\u7684\u5931\u6548\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u4e16\u754c\u6a21\u578b\u89c4\u5212\u7279\u6709\u7684\u91cf\u5316\u8bf1\u5bfc\u5931\u6548\u6a21\u5f0f\uff0c\u5e76\u4e3a\u5728\u4e25\u683c\u8ba1\u7b97\u7ea6\u675f\u4e0b\u90e8\u7f72\u91cf\u5316\u4e16\u754c\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2602.02117", "categories": ["cs.LG", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.02117", "abs": "https://arxiv.org/abs/2602.02117", "authors": ["Youqi Wu", "Farzan Farnia"], "title": "The Maximum von Neumann Entropy Principle: Theory and Applications in Machine Learning", "comment": null, "summary": "Von Neumann entropy (VNE) is a fundamental quantity in quantum information theory and has recently been adopted in machine learning as a spectral measure of diversity for kernel matrices and kernel covariance operators. While maximizing VNE under constraints is well known in quantum settings, a principled analogue of the classical maximum entropy framework, particularly its decision theoretic and game theoretic interpretation, has not been explicitly developed for VNE in data driven contexts. In this paper, we extend the minimax formulation of the maximum entropy principle due to Gr\u00fcnwald and Dawid to the setting of von Neumann entropy, providing a game-theoretic justification for VNE maximization over density matrices and trace-normalized positive semidefinite operators. This perspective yields a robust interpretation of maximum VNE solutions under partial information and clarifies their role as least committed inferences in spectral domains. We then illustrate how the resulting Maximum VNE principle applies to modern machine learning problems by considering two representative applications, selecting a kernel representation from multiple normalized embeddings via kernel-based VNE maximization, and completing kernel matrices from partially observed entries. These examples demonstrate how the proposed framework offers a unifying information-theoretic foundation for VNE-based methods in kernel learning.", "AI": {"tldr": "\u672c\u6587\u5c06Gr\u00fcnwald\u548cDawid\u7684\u6700\u5927\u71b5\u6781\u5c0f\u6781\u5927\u6846\u67b6\u63a8\u5e7f\u81f3\u51af\u00b7\u8bfa\u4f9d\u66fc\u71b5(VNE)\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u573a\u666f\u4e0b\u7684VNE\u6700\u5927\u5316\u63d0\u4f9b\u535a\u5f08\u8bba\u89e3\u91ca\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u6838\u9009\u62e9\u4e0e\u6838\u77e9\u9635\u8865\u5168\u4e2d\u7684\u5e94\u7528\uff0c\u4e3a\u6838\u5b66\u4e60\u4e2d\u57fa\u4e8eVNE\u7684\u65b9\u6cd5\u5960\u5b9a\u7edf\u4e00\u7684\u4fe1\u606f\u8bba\u57fa\u7840\u3002", "motivation": "\u51af\u00b7\u8bfa\u4f9d\u66fc\u71b5(VNE)\u867d\u5728\u91cf\u5b50\u4fe1\u606f\u4e2d\u6210\u719f\u5e94\u7528\uff0c\u5e76\u88ab\u5f15\u5165\u673a\u5668\u5b66\u4e60\u4f5c\u4e3a\u6838\u77e9\u9635\u7684\u8c31\u591a\u6837\u6027\u5ea6\u91cf\uff0c\u4f46\u5176\u5728\u6570\u636e\u9a71\u52a8\u573a\u666f\u4e0b\u7f3a\u4e4f\u7c7b\u4f3c\u7ecf\u5178\u6700\u5927\u71b5\u6846\u67b6\u7684\u51b3\u7b56\u8bba\u4e0e\u535a\u5f08\u8bba\u89e3\u91ca\u3002", "method": "\u5c06Gr\u00fcnwald\u548cDawid\u7684\u6700\u5927\u71b5\u6781\u5c0f\u6781\u5927\u6846\u67b6\u6269\u5c55\u81f3VNE\uff0c\u9488\u5bf9\u5bc6\u5ea6\u77e9\u9635\u548c\u8ff9\u5f52\u4e00\u5316\u534a\u6b63\u5b9a\u7b97\u5b50\uff0c\u5efa\u7acbVNE\u6700\u5927\u5316\u7684\u535a\u5f08\u8bba\u8bba\u8bc1\u3002", "result": "\u83b7\u5f97\u5728\u90e8\u5206\u4fe1\u606f\u4e0b\u6700\u5927VNE\u89e3\u7684\u7a33\u5065\u89e3\u91ca\uff0c\u9610\u660e\u5176\u5728\u8c31\u57df\u4e2d\u4f5c\u4e3a\"\u6700\u5c0f\u627f\u8bfa\u63a8\u65ad\"\u7684\u89d2\u8272\uff0c\u5e76\u901a\u8fc7\u6838\u9009\u62e9\uff08\u591a\u5d4c\u5165\u4e2d\u9009\u53d6\u6838\u8868\u793a\uff09\u548c\u6838\u77e9\u9635\u8865\u5168\u4e24\u4e2a\u5e94\u7528\u5b9e\u4f8b\u9a8c\u8bc1\u8be5\u539f\u5219\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6838\u5b66\u4e60\u4e2d\u57fa\u4e8eVNE\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u4fe1\u606f\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.02151", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02151", "abs": "https://arxiv.org/abs/2602.02151", "authors": ["Yuli Zhou", "Qingxuan Chen", "Luca Benini", "Guolei Sun", "Yawei Li"], "title": "Revisiting Adaptive Rounding with Vectorized Reparameterization for LLM Quantization", "comment": "17 pages, 6 figures, 14 tables", "summary": "Adaptive Rounding has emerged as an alternative to round-to-nearest (RTN) for post-training quantization by enabling cross-element error cancellation. Yet, dense and element-wise rounding matrices are prohibitively expensive for billion-parameter large language models (LLMs). We revisit adaptive rounding from an efficiency perspective and propose VQRound, a parameter-efficient optimization framework that reparameterizes the rounding matrix into a compact codebook. Unlike low-rank alternatives, VQRound minimizes the element-wise worst-case error under $L_\\infty$ norm, which is critical for handling heavy-tailed weight distributions in LLMs. Beyond reparameterization, we identify rounding initialization as a decisive factor and develop a lightweight end-to-end finetuning pipeline that optimizes codebooks across all layers using only 128 samples. Extensive experiments on OPT, LLaMA, LLaMA2, and Qwen3 models demonstrate that VQRound achieves better convergence than traditional adaptive rounding at the same number of steps while using as little as 0.2% of the trainable parameters. Our results show that adaptive rounding can be made both scalable and fast-fitting. The code is available at https://github.com/zhoustan/VQRound.", "AI": {"tldr": "VQRound is a parameter-efficient adaptive rounding framework for post-training quantization of LLMs that replaces expensive element-wise rounding matrices with a compact codebook, minimizing worst-case error under L\u221e norm while using only 0.2% of trainable parameters.", "motivation": "Traditional adaptive rounding suffers from prohibitive computational cost for billion-parameter LLMs due to dense element-wise rounding matrices, despite its advantage over round-to-nearest in enabling cross-element error cancellation.", "method": "Reparameterizes rounding matrices into compact codebooks, minimizes element-wise worst-case error under L\u221e norm for heavy-tailed weight distributions, and employs lightweight end-to-end finetuning with only 128 samples across all layers.", "result": "Achieves better convergence than traditional adaptive rounding at same steps while using as little as 0.2% of trainable parameters, validated on OPT, LLaMA, LLaMA2, and Qwen3 models.", "conclusion": "VQRound successfully makes adaptive rounding both scalable and fast-fitting for large-scale model quantization, addressing efficiency bottlenecks while maintaining performance."}}
{"id": "2602.02157", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02157", "abs": "https://arxiv.org/abs/2602.02157", "authors": ["Egor Serov", "Ilya Kuleshov", "Alexey Zaytsev"], "title": "Efficient Neural Controlled Differential Equations via Attentive Kernel Smoothing", "comment": null, "summary": "Neural Controlled Differential Equations (Neural CDEs) provide a powerful continuous-time framework for sequence modeling, yet the roughness of the driving control path often restricts their efficiency. Standard splines introduce high-frequency variations that force adaptive solvers to take excessively small steps, driving up the Number of Function Evaluations (NFE). We propose a novel approach to Neural CDE path construction that replaces exact interpolation with Kernel and Gaussian Process (GP) smoothing, enabling explicit control over trajectory regularity. To recover details lost during smoothing, we propose an attention-based Multi-View CDE (MV-CDE) and its convolutional extension (MVC-CDE), which employ learnable queries to inform path reconstruction. This framework allows the model to distribute representational capacity across multiple trajectories, each capturing distinct temporal patterns. Empirical results demonstrate that our method, MVC-CDE with GP, achieves state-of-the-art accuracy while significantly reducing NFEs and total inference time compared to spline-based baselines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u7ed3\u5408\u9ad8\u65af\u8fc7\u7a0b\u5e73\u6ed1\u4e0e\u591a\u89c6\u89d2\u6ce8\u610f\u529b\u673a\u5236\u7684\u795e\u7ecf\u63a7\u5236\u5fae\u5206\u65b9\u7a0b\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u63a7\u5236\u8f68\u8ff9\u6b63\u5219\u6027\u51cf\u5c11\u51fd\u6570\u8bc4\u4f30\u6b21\u6570\uff0c\u5728\u4fdd\u6301\u6700\u5148\u8fdb\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u795e\u7ecf\u63a7\u5236\u5fae\u5206\u65b9\u7a0b\u5728\u5e8f\u5217\u5efa\u6a21\u4e2d\u8868\u73b0\u5f3a\u5927\uff0c\u4f46\u9a71\u52a8\u8def\u5f84\u7684\u7c97\u7cd9\u5ea6\u9650\u5236\u6548\u7387\u3002\u6807\u51c6\u6837\u6761\u5f15\u5165\u7684\u9ad8\u9891\u53d8\u5316\u8feb\u4f7f\u6c42\u89e3\u5668\u91c7\u7528\u8fc7\u5c0f\u6b65\u957f\uff0c\u5bfc\u81f4\u51fd\u6570\u8bc4\u4f30\u6b21\u6570\u6fc0\u589e\u3002", "method": "\u63d0\u51fa\u7528\u6838\u51fd\u6570\u548c\u9ad8\u65af\u8fc7\u7a0b\u5e73\u6ed1\u66ff\u4ee3\u7cbe\u786e\u63d2\u503c\uff0c\u5b9e\u73b0\u8f68\u8ff9\u6b63\u5219\u6027\u663e\u5f0f\u63a7\u5236\uff1b\u8bbe\u8ba1\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u591a\u89c6\u89d2CDE\uff08MV-CDE\uff09\u53ca\u5176\u5377\u79ef\u6269\u5c55\uff08MVC-CDE\uff09\uff0c\u5229\u7528\u53ef\u5b66\u4e60\u67e5\u8be2\u91cd\u5efa\u8def\u5f84\uff0c\u4f7f\u6a21\u578b\u80fd\u5c06\u8868\u793a\u80fd\u529b\u5206\u914d\u5230\u6355\u83b7\u4e0d\u540c\u65f6\u95f4\u6a21\u5f0f\u7684\u591a\u4e2a\u8f68\u8ff9\u4e0a\u3002", "result": "MVC-CDE\u4e0eGP\u7ed3\u5408\u7684\u65b9\u6cd5\u8fbe\u5230\u6700\u5148\u8fdb\u7cbe\u5ea6\uff0c\u540c\u65f6\u76f8\u6bd4\u6837\u6761\u57fa\u7ebf\u663e\u8457\u51cf\u5c11\u4e86NFE\u548c\u603b\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5e73\u6ed1\u8def\u5f84\u548c\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86Neural CDEs\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u5728\u7ef4\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2602.02161", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.02161", "abs": "https://arxiv.org/abs/2602.02161", "authors": ["Aniq Ur Rahman", "Justin P. Coon"], "title": "Generating Causal Temporal Interaction Graphs for Counterfactual Validation of Temporal Link Prediction", "comment": null, "summary": "Temporal link prediction (TLP) models are commonly evaluated based on predictive accuracy, yet such evaluations do not assess whether these models capture the causal mechanisms that govern temporal interactions. In this work, we propose a framework for counterfactual validation of TLP models by generating causal temporal interaction graphs (CTIGs) with known ground-truth causal structure. We first introduce a structural equation model for continuous-time event sequences that supports both excitatory and inhibitory effects, and then extend this mechanism to temporal interaction graphs. To compare causal models, we propose a distance metric based on cross-model predictive error, and empirically validate the hypothesis that predictors trained on one causal model degrade when evaluated on sufficiently distant models. Finally, we instantiate counterfactual evaluation under (i) controlled causal shifts between generating models and (ii) timestamp shuffling as a stochastic distortion with measurable causal distance. Our framework provides a foundation for causality-aware benchmarking.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53cd\u4e8b\u5b9e\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5177\u6709\u5df2\u77e5\u56e0\u679c\u7ed3\u6784\u7684\u65f6\u5e8f\u4ea4\u4e92\u56fe\u6765\u8bc4\u4f30\u65f6\u5e8f\u94fe\u63a5\u9884\u6d4b\u6a21\u578b\u662f\u5426\u6355\u6349\u4e86\u56e0\u679c\u673a\u5236\uff0c\u800c\u975e\u4ec5\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u65f6\u5e8f\u94fe\u63a5\u9884\u6d4b\u6a21\u578b\u901a\u5e38\u4ec5\u57fa\u4e8e\u9884\u6d4b\u51c6\u786e\u6027\u8bc4\u4f30\uff0c\u4f46\u65e0\u6cd5\u5224\u65ad\u6a21\u578b\u662f\u5426\u771f\u6b63\u6355\u6349\u4e86\u65f6\u5e8f\u4ea4\u4e92\u80cc\u540e\u7684\u56e0\u679c\u673a\u5236\u3002", "method": "1. \u63d0\u51fa\u652f\u6301\u5174\u594b\u6027\u548c\u6291\u5236\u6027\u6548\u5e94\u7684\u8fde\u7eed\u65f6\u95f4\u4e8b\u4ef6\u5e8f\u5217\u7ed3\u6784\u65b9\u7a0b\u6a21\u578b\uff0c\u5e76\u6269\u5c55\u81f3\u65f6\u5e8f\u4ea4\u4e92\u56fe\uff1b2. \u57fa\u4e8e\u8de8\u6a21\u578b\u9884\u6d4b\u8bef\u5dee\u63d0\u51fa\u56e0\u679c\u6a21\u578b\u8ddd\u79bb\u5ea6\u91cf\uff1b3. \u5728\u53d7\u63a7\u56e0\u679c\u8f6c\u79fb\u548c\u65f6\u95f4\u6233\u968f\u673a\u6253\u4e71\u4e24\u79cd\u573a\u666f\u4e0b\u5b9e\u4f8b\u5316\u53cd\u4e8b\u5b9e\u8bc4\u4f30\u3002", "result": "\u5b9e\u8bc1\u9a8c\u8bc1\u4e86\u5f53\u9884\u6d4b\u6a21\u578b\u5728\u56e0\u679c\u8ddd\u79bb\u8f83\u8fdc\u7684\u6a21\u578b\u4e0a\u8bc4\u4f30\u65f6\u6027\u80fd\u4f1a\u4e0b\u964d\uff0c\u8868\u660e\u8be5\u6846\u67b6\u80fd\u6709\u6548\u91cf\u5316\u6a21\u578b\u5bf9\u56e0\u679c\u673a\u5236\u7684\u6355\u6349\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u65f6\u5e8f\u94fe\u63a5\u9884\u6d4b\u7684\u56e0\u679c\u611f\u77e5\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u63a8\u52a8\u6a21\u578b\u4ece\u76f8\u5173\u6027\u9884\u6d4b\u5411\u56e0\u679c\u673a\u5236\u5b66\u4e60\u53d1\u5c55\u3002"}}
{"id": "2602.02162", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02162", "abs": "https://arxiv.org/abs/2602.02162", "authors": ["Ratmir Miftachov", "Bruno Charron", "Simon Valentin"], "title": "Interpretable Tabular Foundation Models via In-Context Kernel Regression", "comment": null, "summary": "Tabular foundation models like TabPFN and TabICL achieve state-of-the-art performance through in-context learning, yet their architectures remain fundamentally opaque. We introduce KernelICL, a framework to enhance tabular foundation models with quantifiable sample-based interpretability. Building on the insight that in-context learning is akin to kernel regression, we make this mechanism explicit by replacing the final prediction layer with kernel functions (Gaussian, dot-product, kNN) so that every prediction is a transparent weighted average of training labels. We introduce a two-dimensional taxonomy that formally unifies standard kernel methods, modern neighbor-based approaches, and attention mechanisms under a single framework, and quantify inspectability via the perplexity of the weight distribution over training samples. On 55 TALENT benchmark datasets, KernelICL achieves performance on par with existing tabular foundation models, demonstrating that explicit kernel constraints on the final layer enable inspectable predictions without sacrificing performance.", "AI": {"tldr": "KernelICL\u901a\u8fc7\u5c06\u8868\u683c\u57fa\u7840\u6a21\u578b\u7684\u6700\u7ec8\u9884\u6d4b\u5c42\u66ff\u6362\u4e3a\u663e\u5f0f\u6838\u51fd\u6570\uff08\u9ad8\u65af\u3001\u70b9\u79ef\u3001kNN\uff09\uff0c\u4f7f\u5176\u9884\u6d4b\u53d8\u4e3a\u900f\u660e\u53ef\u89e3\u91ca\u7684\u8bad\u7ec3\u6807\u7b7e\u52a0\u6743\u5e73\u5747\uff0c\u5728\u4fdd\u6301\u4e0e\u73b0\u6709\u6a21\u578b\u76f8\u5f53\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u53ef\u91cf\u5316\u7684\u6837\u672c\u7ea7\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u8868\u683c\u57fa\u7840\u6a21\u578b\uff08\u5982TabPFN\u548cTabICL\uff09\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u53d6\u5f97\u4e86\u5148\u8fdb\u6027\u80fd\uff0c\u4f46\u5176\u67b6\u6784\u672c\u8d28\u4e0a\u662f\u4e0d\u900f\u660e\u7684\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u589e\u5f3a\u8fd9\u4e9b\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4f7f\u5176\u9884\u6d4b\u8fc7\u7a0b\u66f4\u52a0\u900f\u660e\u548c\u53ef\u91cf\u5316\u3002", "method": "\u63d0\u51faKernelICL\u6846\u67b6\uff0c\u6838\u5fc3\u6d1e\u5bdf\u662f\u4e0a\u4e0b\u6587\u5b66\u4e60\u7c7b\u4f3c\u4e8e\u6838\u56de\u5f52\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u7528\u6838\u51fd\u6570\uff08\u9ad8\u65af\u3001\u70b9\u79ef\u3001kNN\uff09\u66ff\u6362\u6700\u7ec8\u9884\u6d4b\u5c42\uff1b2\uff09\u5f15\u5165\u4e8c\u7ef4\u5206\u7c7b\u6cd5\uff0c\u5728\u7edf\u4e00\u6846\u67b6\u4e0b\u5f62\u5f0f\u5316\u6807\u51c6\u6838\u65b9\u6cd5\u3001\u73b0\u4ee3\u90bb\u57df\u65b9\u6cd5\u548c\u6ce8\u610f\u529b\u673a\u5236\uff1b3\uff09\u901a\u8fc7\u8bad\u7ec3\u6837\u672c\u6743\u91cd\u5206\u5e03\u7684\u56f0\u60d1\u5ea6\u6765\u91cf\u5316\u53ef\u68c0\u67e5\u6027\u3002", "result": "\u572855\u4e2aTALENT\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cKernelICL\u7684\u6027\u80fd\u4e0e\u73b0\u6709\u8868\u683c\u57fa\u7840\u6a21\u578b\u76f8\u5f53\uff0c\u8bc1\u660e\u663e\u5f0f\u6838\u7ea6\u675f\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u53ef\u68c0\u67e5\u7684\u9884\u6d4b\u3002", "conclusion": "\u901a\u8fc7\u5728\u6700\u7ec8\u5c42\u65bd\u52a0\u663e\u5f0f\u6838\u7ea6\u675f\uff0c\u53ef\u4ee5\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u4f7f\u8868\u683c\u57fa\u7840\u6a21\u578b\u7684\u9884\u6d4b\u8fc7\u7a0b\u900f\u660e\u5316\uff0c\u5b9e\u73b0\u53ef\u91cf\u5316\u7684\u6837\u672c\u7ea7\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2602.02173", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02173", "abs": "https://arxiv.org/abs/2602.02173", "authors": ["Jiancheng Tu", "Wenqi Fan", "Zhibin Wu"], "title": "Generalized Optimal Classification Trees: A Mixed-Integer Programming Approach", "comment": null, "summary": "Global optimization of decision trees is a long-standing challenge in combinatorial optimization, yet such models play an important role in interpretable machine learning. Although the problem has been investigated for several decades, only recent advances in discrete optimization have enabled practical algorithms for solving optimal classification tree problems on real-world datasets. Mixed-integer programming (MIP) offers a high degree of modeling flexibility, and we therefore propose a MIP-based framework for learning optimal classification trees under nonlinear performance metrics, such as the F1-score, that explicitly addresses class imbalance. To improve scalability, we develop problem-specific acceleration techniques, including a tailored branch-and-cut algorithm, an instance-reduction scheme, and warm-start strategies. We evaluate the proposed approach on 50 benchmark datasets. The computational results show that the framework can efficiently optimize nonlinear metrics while achieving strong predictive performance and reduced solution times compared with existing methods.", "AI": {"tldr": "This paper proposes a mixed-integer programming framework for globally optimizing classification trees with nonlinear metrics like F1-score, using acceleration techniques to improve scalability and achieve strong performance on 50 datasets.", "motivation": "Decision trees are crucial for interpretable machine learning, but global optimization is a long-standing combinatorial challenge. Existing methods struggle with class imbalance and nonlinear performance metrics, and only recent advances in discrete optimization have made practical algorithms possible.", "method": "The authors develop a MIP-based framework for learning optimal classification trees that explicitly handles nonlinear metrics (F1-score) and class imbalance. To enhance scalability, they introduce problem-specific acceleration techniques including a tailored branch-and-cut algorithm, an instance-reduction scheme, and warm-start strategies.", "result": "Evaluated on 50 benchmark datasets, the framework efficiently optimizes nonlinear metrics while delivering strong predictive performance and significantly reduced solution times compared to existing methods.", "conclusion": "The proposed MIP-based framework with specialized acceleration techniques successfully makes global optimization of decision trees practical for real-world applications, maintaining interpretability and performance under nonlinear metrics."}}
{"id": "2602.02180", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02180", "abs": "https://arxiv.org/abs/2602.02180", "authors": ["Weikang Meng", "Liangyu Huo", "Yadan Luo", "Jiawen Guan", "Jingyi Zhang", "Yingjian Li", "Zheng Zhang"], "title": "STILL: Selecting Tokens for Intra-Layer Hybrid Attention to Linearize LLMs", "comment": null, "summary": "Linearizing pretrained large language models (LLMs) primarily relies on intra-layer hybrid attention mechanisms to alleviate the quadratic complexity of standard softmax attention. Existing methods perform token routing based on sliding-window partitions, resulting in position-based selection and fails to capture token-specific global importance. Meanwhile, linear attention further suffers from distribution shift caused by learnable feature maps that distort pretrained feature magnitudes. Motivated by these limitations, we propose STILL, an intra-layer hybrid linearization framework for efficiently linearizing LLMs. STILL introduces a Self-Saliency Score with strong local-global consistency, enabling accurate token selection using sliding-window computation, and retains salient tokens for sparse softmax attention while summarizing the remaining context via linear attention. To preserve pretrained representations, we design a Norm-Preserved Feature Map (NP-Map) that decouples feature direction from magnitude and reinjects pretrained norms. We further adopt a unified training-inference architecture with chunk-wise parallelization and delayed selection to improve hardware efficiency. Experiments show that STILL matches or surpasses the original pretrained model on commonsense and general reasoning tasks, and achieves up to a 86.2% relative improvement over prior linearized attention methods on long-context benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSTILL\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u663e\u8457\u6027\u5206\u6570\u548c\u4fdd\u8303\u6570\u7279\u5f81\u6620\u5c04\u5b9e\u73b0LLM\u9ad8\u6548\u7ebf\u6027\u5316\uff0c\u5728\u4fdd\u6301\u9884\u8bad\u7ec3\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u4e8e\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e0a\u53d6\u5f97\u9ad8\u8fbe86.2%\u7684\u76f8\u5bf9\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7ebf\u6027\u5316\u65b9\u6cd5\u5b58\u5728\u4e24\u5927\u5c40\u9650\uff1a\u6ed1\u52a8\u7a97\u53e3\u5206\u533a\u5bfc\u81f4\u57fa\u4e8e\u4f4d\u7f6e\u7684token\u9009\u62e9\uff0c\u65e0\u6cd5\u6355\u83b7\u5168\u5c40\u91cd\u8981\u6027\uff1b\u53ef\u5b66\u4e60\u7279\u5f81\u6620\u5c04\u5f15\u8d77\u5206\u5e03\u504f\u79fb\uff0c\u626d\u66f2\u9884\u8bad\u7ec3\u7279\u5f81\u5e45\u5ea6\u3002", "method": "STILL\u6846\u67b6\u5305\u542b\u56db\u9879\u521b\u65b0\uff1a1)\u81ea\u663e\u8457\u6027\u5206\u6570\u5b9e\u73b0\u5c40\u90e8-\u5168\u5c40\u4e00\u81f4\u7684token\u9009\u62e9\uff1b2)\u5bf9\u663e\u8457token\u4fdd\u7559\u7a00\u758fsoftmax\u6ce8\u610f\u529b\uff0c\u5176\u4f59\u4e0a\u4e0b\u6587\u901a\u8fc7\u7ebf\u6027\u6ce8\u610f\u529b\u603b\u7ed3\uff1b3)\u4fdd\u8303\u6570\u7279\u5f81\u6620\u5c04(NP-Map)\u89e3\u8026\u7279\u5f81\u65b9\u5411\u4e0e\u5e45\u5ea6\u5e76\u91cd\u6ce8\u9884\u8bad\u7ec3\u8303\u6570\uff1b4)\u7edf\u4e00\u8bad\u7ec3-\u63a8\u7406\u67b6\u6784\uff0c\u91c7\u7528\u5206\u5757\u5e76\u884c\u5316\u548c\u5ef6\u8fdf\u9009\u62e9\u63d0\u5347\u786c\u4ef6\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSTILL\u5728\u5e38\u8bc6\u4e0e\u901a\u7528\u63a8\u7406\u4efb\u52a1\u4e0a\u5339\u914d\u6216\u8d85\u8d8a\u539f\u59cb\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u5728\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u76f8\u5bf9\u73b0\u6709\u7ebf\u6027\u6ce8\u610f\u529b\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u8fbe86.2%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "STILL\u6709\u6548\u89e3\u51b3\u4e86\u7ebf\u6027\u5316\u8fc7\u7a0b\u4e2d\u7684\u7279\u5f81\u5931\u771f\u4e0etoken\u9009\u62e9\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u957f\u5e8f\u5217\u5904\u7406\u7684\u6548\u7387\u3002"}}
{"id": "2602.02192", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.02192", "abs": "https://arxiv.org/abs/2602.02192", "authors": ["Jie Xiao", "Meng Chen", "Qingnan Ren", "Song Jingwei", "Jiaqi Huang", "Yangshen Deng", "Chris Tong", "Wanyi Chen", "Suli Wang", "Ziqian Bi", "Shuo Lu", "Yiqun Duan", "Lynn Ai", "Eric Yang", "Bill Shi"], "title": "ECHO-2: A Large Scale Distributed Rollout Framework for Cost-efficient Reinforcement Learning", "comment": "23 pages, 7 figures", "summary": "Reinforcement learning (RL) is a critical stage in post-training large language models (LLMs), involving repeated interaction between rollout generation, reward evaluation, and centralized learning. Distributing rollout execution offers opportunities to leverage more cost-efficient inference resources, but introduces challenges in wide-area coordination and policy dissemination. We present ECHO-2, a distributed RL framework for post-training with remote inference workers and non-negligible dissemination latency. ECHO-2 combines centralized learning with distributed rollouts and treats bounded policy staleness as a user-controlled parameter, enabling rollout generation, dissemination, and training to overlap. We introduce an overlap-based capacity model that relates training time, dissemination latency, and rollout throughput, yielding a practical provisioning rule for sustaining learner utilization. To mitigate dissemination bottlenecks and lower cost, ECHO-2 employs peer-assisted pipelined broadcast and cost-aware activation of heterogeneous workers. Experiments on GRPO post-training of 4B and 8B models under real wide-area bandwidth regimes show that ECHO-2 significantly improves cost efficiency while preserving RL reward comparable to strong baselines.", "AI": {"tldr": "ECHO-2\u662f\u4e00\u4e2a\u5206\u5e03\u5f0f\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u96c6\u4e2d\u5b66\u4e60\u4e0e\u5206\u5e03\u5f0f\u63a8\u7406\u7ed3\u5408\uff0c\u5229\u7528\u5bf9\u7b49\u8282\u70b9\u8f85\u52a9\u6d41\u6c34\u7ebf\u5e7f\u64ad\u548c\u6210\u672c\u611f\u77e5\u7684\u5f02\u6784\u5de5\u4f5c\u8282\u70b9\u6fc0\u6d3b\uff0c\u5728\u5e7f\u57df\u7f51\u73af\u5883\u4e0b\u663e\u8457\u63d0\u5347LLM\u540e\u8bad\u7ec3\u7684\u6210\u672c\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u57fa\u7ebf\u76f8\u5f53\u7684RL\u5956\u52b1\u6027\u80fd\u3002", "motivation": "\u5728LLM\u540e\u8bad\u7ec3\u4e2d\u8fdb\u884c\u5206\u5e03\u5f0f\u5f3a\u5316\u5b66\u4e60\u65f6\uff0c\u867d\u7136\u53ef\u4ee5\u5229\u7528\u6210\u672c\u66f4\u4f4e\u7684\u63a8\u7406\u8d44\u6e90\uff0c\u4f46\u5e7f\u57df\u7f51\u534f\u8c03\u548c\u7b56\u7565\u4f20\u64ad\u5ef6\u8fdf\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\uff0c\u5f71\u54cd\u5b66\u4e60\u6548\u7387\u3002", "method": "ECHO-2\u5c06\u7b56\u7565\u9648\u65e7\u5ea6\u4f5c\u4e3a\u7528\u6237\u53ef\u63a7\u53c2\u6570\uff0c\u5b9e\u73b0\u63a8\u7406\u751f\u6210\u3001\u7b56\u7565\u4f20\u64ad\u548c\u8bad\u7ec3\u7684\u5e76\u884c\u91cd\u53e0\uff1b\u63d0\u51fa\u57fa\u4e8e\u91cd\u53e0\u7684\u5bb9\u91cf\u6a21\u578b\u6765\u6307\u5bfc\u8d44\u6e90\u5206\u914d\uff1b\u91c7\u7528\u5bf9\u7b49\u8282\u70b9\u8f85\u52a9\u6d41\u6c34\u7ebf\u5e7f\u64ad\u548c\u6210\u672c\u611f\u77e5\u7684\u5f02\u6784\u5de5\u4f5c\u8282\u70b9\u6fc0\u6d3b\u673a\u5236\u6765\u7f13\u89e3\u4f20\u64ad\u74f6\u9888\u5e76\u964d\u4f4e\u6210\u672c\u3002", "result": "\u5728\u771f\u5b9e\u5e7f\u57df\u7f51\u5e26\u5bbd\u73af\u5883\u4e0b\u5bf94B\u548c8B\u6a21\u578b\u8fdb\u884cGRPO\u540e\u8bad\u7ec3\u5b9e\u9a8c\u8868\u660e\uff0cECHO-2\u5728\u4fdd\u6301\u4e0e\u5f3a\u57fa\u7ebf\u76f8\u5f53\u7684RL\u5956\u52b1\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6210\u672c\u6548\u7387\u3002", "conclusion": "ECHO-2\u901a\u8fc7\u91cd\u53e0\u6267\u884c\u548c\u9ad8\u6548\u4f20\u64ad\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5e03\u5f0fRL\u4e2d\u7684\u5e7f\u57df\u7f51\u5ef6\u8fdf\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6210\u672c\u6548\u7387\u4e0e\u6a21\u578b\u6027\u80fd\u7684\u826f\u597d\u5e73\u8861\uff0c\u4e3a\u5927\u89c4\u6a21LLM\u540e\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02215", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02215", "abs": "https://arxiv.org/abs/2602.02215", "authors": ["Sebastian M\u00fcller", "Vanessa Toborek", "Eike Stadtl\u00e4nder", "Tam\u00e1s Horv\u00e1th", "Brendan Balcerak Jackson", "Christian Bauckhage"], "title": "Scientific Theory of a Black-Box: A Life Cycle-Scale XAI Framework Based on Constructive Empiricism", "comment": null, "summary": "Explainable AI (XAI) offers a growing number of algorithms that aim to answer specific questions about black-box models. What is missing is a principled way to consolidate explanatory information about a fixed black-box model into a persistent, auditable artefact, that accompanies the black-box throughout its life cycle. We address this gap by introducing the notion of a scientific theory of a black (SToBB). Grounded in Constructive Empiricism, a SToBB fulfils three obligations: (i) empirical adequacy with respect to all available observations of black-box behaviour, (ii) adaptability via explicit update commitments that restore adequacy when new observations arrive, and (iii) auditability through transparent documentation of assumptions, construction choices, and update behaviour. We operationalise these obligations as a general framework that specifies an extensible observation base, a traceable hypothesis class, algorithmic components for construction and revision, and documentation sufficient for third-party assessment. Explanations for concrete stakeholder needs are then obtained by querying the maintained record through interfaces, rather than by producing isolated method outputs. As a proof of concept, we instantiate a complete SToBB for a neural-network classifier on a tabular task and introduce the Constructive Box Theoriser (CoBoT) algorithm, an online procedure that constructs and maintains an empirically adequate rule-based surrogate as observations accumulate. Together, these contributions position SToBBs as a life cycle-scale, inspectable point of reference that supports consistent, reusable analyses and systematic external scrutiny.", "AI": {"tldr": "The paper introduces Scientific Theory of a Black Box (SToBB) - a persistent, auditable artefact that consolidates XAI explanations throughout a model's lifecycle, grounded in Constructive Empiricism with obligations of empirical adequacy, adaptability, and auditability.", "motivation": "Current XAI lacks a principled way to consolidate explanatory information about black-box models into a persistent, auditable artefact that accompanies the model throughout its lifecycle.", "method": "Proposes SToBB framework based on Constructive Empiricism with three core obligations (empirical adequacy, adaptability, auditability), operationalized through extensible observation base, traceable hypothesis class, algorithmic components, and documentation. Introduces CoBoT algorithm for online construction of rule-based surrogates.", "result": "Demonstrated a complete SToBB instantiation for a neural-network classifier on tabular data using the CoBoT algorithm, which maintains an empirically adequate rule-based surrogate as observations accumulate.", "conclusion": "SToBBs provide a lifecycle-scale, inspectable reference point enabling consistent, reusable analyses and systematic external scrutiny of black-box models."}}
{"id": "2602.02229", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.02229", "abs": "https://arxiv.org/abs/2602.02229", "authors": ["Guangyi Zhang", "Yunlong Cai", "Guanding Yu", "Osvaldo Simeone"], "title": "Prediction-Powered Risk Monitoring of Deployed Models for Detecting Harmful Distribution Shifts", "comment": null, "summary": "We study the problem of monitoring model performance in dynamic environments where labeled data are limited. To this end, we propose prediction-powered risk monitoring (PPRM), a semi-supervised risk-monitoring approach based on prediction-powered inference (PPI). PPRM constructs anytime-valid lower bounds on the running risk by combining synthetic labels with a small set of true labels. Harmful shifts are detected via a threshold-based comparison with an upper bound on the nominal risk, satisfying assumption-free finite-sample guarantees in the probability of false alarm. We demonstrate the effectiveness of PPRM through extensive experiments on image classification, large language model (LLM), and telecommunications monitoring tasks.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.02239", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02239", "abs": "https://arxiv.org/abs/2602.02239", "authors": ["Giovanni De Felice", "Riccardo D'Elia", "Alberto Termine", "Pietro Barbiero", "Giuseppe Marra", "Silvia Santini"], "title": "Interpretability in Deep Time Series Models Demands Semantic Alignment", "comment": null, "summary": "Deep time series models continue to improve predictive performance, yet their deployment remains limited by their black-box nature. In response, existing interpretability approaches in the field keep focusing on explaining the internal model computations, without addressing whether they align or not with how a human would reason about the studied phenomenon. Instead, we state interpretability in deep time series models should pursue semantic alignment: predictions should be expressed in terms of variables that are meaningful to the end user, mediated by spatial and temporal mechanisms that admit user-dependent constraints. In this paper, we formalize this requirement and require that, once established, semantic alignment must be preserved under temporal evolution: a constraint with no analog in static settings. Provided with this definition, we outline a blueprint for semantically aligned deep time series models, identify properties that support trust, and discuss implications for model design.", "AI": {"tldr": "This paper proposes a new interpretability framework for deep time series models called \"semantic alignment,\" which requires model predictions to be expressed in human-meaningful terms and constraints that must be preserved over time, moving beyond explaining internal computations.", "motivation": "Current interpretability approaches for deep time series models only explain internal computations without addressing alignment with human reasoning about the studied phenomenon, limiting deployment due to black-box nature.", "method": "The authors formalize the requirement of \"semantic alignment\" - predictions expressed in user-meaningful variables with spatial/temporal mechanisms admitting user constraints - and require its preservation under temporal evolution. They then outline a blueprint for such models.", "result": "The paper provides a formal definition of semantic alignment for time series, identifies properties that support trust, and discusses implications for model design, establishing a new constraint without analog in static settings.", "conclusion": "Interpretability in deep time series should pursue semantic alignment rather than just explaining internal computations, requiring model predictions to align with human reasoning through meaningful variables and temporally preserved constraints."}}
{"id": "2602.02241", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02241", "abs": "https://arxiv.org/abs/2602.02241", "authors": ["Roman Dyachenko", "Nikita Gushchin", "Kirill Sokolov", "Petr Mokrov", "Evgeny Burnaev", "Alexander Korotin"], "title": "Variational Entropic Optimal Transport", "comment": null, "summary": "Entropic optimal transport (EOT) in continuous spaces with quadratic cost is a classical tool for solving the domain translation problem. In practice, recent approaches optimize a weak dual EOT objective depending on a single potential, but doing so is computationally not efficient due to the intractable log-partition term. Existing methods typically resolve this obstacle in one of two ways: by significantly restricting the transport family to obtain closed-form normalization (via Gaussian-mixture parameterizations), or by using general neural parameterizations that require simulation-based training procedures. We propose Variational Entropic Optimal Transport (VarEOT), based on an exact variational reformulation of the log-partition $\\log \\mathbb{E}[\\exp(\\cdot)]$ as a tractable minimization over an auxiliary positive normalizer. This yields a differentiable learning objective optimized with stochastic gradients and avoids the necessity of MCMC simulations during the training. We provide theoretical guarantees, including finite-sample generalization bounds and approximation results under universal function approximation. Experiments on synthetic data and unpaired image-to-image translation demonstrate competitive or improved translation quality, while comparisons within the solvers that use the same weak dual EOT objective support the benefit of the proposed optimization principle.", "AI": {"tldr": "The paper proposes VarEOT, a variational reformulation of entropic optimal transport that avoids intractable log-partition terms through an auxiliary normalizer, enabling efficient stochastic gradient optimization without MCMC simulations, with theoretical guarantees and competitive performance in domain translation tasks.", "motivation": "Recent weak dual EOT optimization is computationally inefficient due to intractable log-partition terms, and existing solutions either restrict transport families too much (via Gaussian-mixtures) or require expensive simulation-based training procedures.", "method": "Variational reformulation of the log-partition term as a tractable minimization over an auxiliary positive normalizer, creating a differentiable objective optimized with stochastic gradients.", "result": "Theoretical guarantees include finite-sample generalization bounds and universal approximation results; experiments on synthetic and image-to-image translation show competitive/improved quality, with comparisons confirming benefits over existing EOT solvers.", "conclusion": "VarEOT provides an efficient, theoretically-grounded optimization framework for entropic optimal transport that eliminates MCMC simulations while maintaining or improving performance on domain translation problems."}}
{"id": "2602.02244", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02244", "abs": "https://arxiv.org/abs/2602.02244", "authors": ["Hao Wang", "Hao Gu", "Hongming Piao", "Kaixiong Gong", "Yuxiao Ye", "Xiangyu Yue", "Sirui Han", "Yike Guo", "Dapeng Wu"], "title": "Learning While Staying Curious: Entropy-Preserving Supervised Fine-Tuning via Adaptive Self-Distillation for Large Reasoning Models", "comment": null, "summary": "The standard post-training recipe for large reasoning models, supervised fine-tuning followed by reinforcement learning (SFT-then-RL), may limit the benefits of the RL stage: while SFT imitates expert demonstrations, it often causes overconfidence and reduces generation diversity, leaving RL with a narrowed solution space to explore. Adding entropy regularization during SFT is not a cure-all; it tends to flatten token distributions toward uniformity, increasing entropy without improving meaningful exploration capability. In this paper, we propose CurioSFT, an entropy-preserving SFT method designed to enhance exploration capabilities through intrinsic curiosity. It consists of (a) Self-Exploratory Distillation, which distills the model toward a self-generated, temperature-scaled teacher to encourage exploration within its capability; and (b) Entropy-Guided Temperature Selection, which adaptively adjusts distillation strength to mitigate knowledge forgetting by amplifying exploration at reasoning tokens while stabilizing factual tokens. Extensive experiments on mathematical reasoning tasks demonstrate that, in SFT stage, CurioSFT outperforms the vanilla SFT by 2.5 points on in-distribution tasks and 2.9 points on out-of-distribution tasks. We also verify that exploration capabilities preserved during SFT successfully translate into concrete gains in RL stage, yielding an average improvement of 5.0 points.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCurioSFT\uff0c\u4e00\u79cd\u4fdd\u6301\u71b5\u7684SFT\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u6211\u63a2\u7d22\u84b8\u998f\u548c\u71b5\u5f15\u5bfc\u7684\u6e29\u5ea6\u9009\u62e9\u589e\u5f3a\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u63a2\u7d22\u80fd\u529b\uff0c\u5728SFT\u9636\u6bb5\u4f18\u4e8e\u6807\u51c6SFT\uff0c\u5e76\u4f7f\u540e\u7eedRL\u9636\u6bb5\u83b7\u5f97\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u6807\u51c6SFT-then-RL\u8bad\u7ec3\u6d41\u7a0b\u4e2d\uff0cSFT\u4f1a\u5bfc\u81f4\u6a21\u578b\u8fc7\u5ea6\u81ea\u4fe1\u5e76\u964d\u4f4e\u751f\u6210\u591a\u6837\u6027\uff0c\u4ece\u800c\u9650\u5236RL\u9636\u6bb5\u7684\u63a2\u7d22\u7a7a\u95f4\u3002\u7b80\u5355\u7684\u71b5\u6b63\u5219\u5316\u65b9\u6cd5\u6548\u679c\u6709\u9650\uff0c\u4ec5\u80fd\u4f7ftoken\u5206\u5e03\u8d8b\u4e8e\u5747\u5300\u5316\uff0c\u65e0\u6cd5\u63d0\u5347\u6709\u610f\u4e49\u7684\u63a2\u7d22\u80fd\u529b\u3002", "method": "CurioSFT\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\uff08a\uff09\u81ea\u6211\u63a2\u7d22\u84b8\u998f\uff0c\u901a\u8fc7\u81ea\u6211\u751f\u6210\u7684\u6e29\u5ea6\u7f29\u653e\u6559\u5e08\u6a21\u578b\u8fdb\u884c\u84b8\u998f\uff0c\u5728\u80fd\u529b\u8303\u56f4\u5185\u9f13\u52b1\u63a2\u7d22\uff1b\uff08b\uff09\u71b5\u5f15\u5bfc\u7684\u6e29\u5ea6\u9009\u62e9\uff0c\u81ea\u9002\u5e94\u8c03\u6574\u84b8\u998f\u5f3a\u5ea6\uff0c\u901a\u8fc7\u5728\u63a8\u7406token\u4e0a\u653e\u5927\u63a2\u7d22\u3001\u5728\u4e8b\u5b9etoken\u4e0a\u4fdd\u6301\u7a33\u5b9a\u6765\u7f13\u89e3\u77e5\u8bc6\u9057\u5fd8\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cCurioSFT\u5728SFT\u9636\u6bb5\u6bd4\u6807\u51c6SFT\u5728\u5206\u5e03\u5185\u4efb\u52a1\u4e0a\u9ad82.5\u5206\uff0c\u5728\u5206\u5e03\u5916\u4efb\u52a1\u4e0a\u9ad82.9\u5206\u3002\u4fdd\u7559\u7684\u63a2\u7d22\u80fd\u529b\u6210\u529f\u8f6c\u5316\u4e3aRL\u9636\u6bb5\u7684\u5b9e\u9645\u6536\u76ca\uff0c\u5e73\u5747\u63d0\u53475.0\u5206\u3002", "conclusion": "CurioSFT\u5728SFT\u9636\u6bb5\u6709\u6548\u4fdd\u6301\u4e86\u63a2\u7d22\u80fd\u529b\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86SFT\u6027\u80fd\uff0c\u8fd8\u663e\u8457\u6539\u5584\u4e86\u540e\u7eedRL\u9636\u6bb5\u7684\u8868\u73b0\uff0c\u8bc1\u660e\u4e86\u5728\u5927\u6a21\u578b\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\u4fdd\u6301\u591a\u6837\u6027\u548c\u63a2\u7d22\u6027\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.02258", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02258", "abs": "https://arxiv.org/abs/2602.02258", "authors": ["Gaurav Bhatt", "Aditya Chinchure", "Jiawei Zhou", "Leonid Sigal"], "title": "Alignment-Aware Model Adaptation via Feedback-Guided Optimization", "comment": null, "summary": "Fine-tuning is the primary mechanism for adapting foundation models to downstream tasks; however, standard approaches largely optimize task objectives in isolation and do not account for secondary yet critical alignment objectives (e.g., safety and hallucination avoidance). As a result, downstream fine-tuning can degrade alignment and fail to correct pre-existing misaligned behavior. We propose an alignment-aware fine-tuning framework that integrates feedback from an external alignment signal through policy-gradient-based regularization. Our method introduces an adaptive gating mechanism that dynamically balances supervised and alignment-driven gradients on a per-sample basis, prioritizing uncertain or misaligned cases while allowing well-aligned examples to follow standard supervised updates. The framework further learns abstention behavior for fully misaligned inputs, incorporating conservative responses directly into the fine-tuned model. Experiments on general and domain-specific instruction-tuning benchmarks demonstrate consistent reductions in harmful and hallucinated outputs without sacrificing downstream task performance. Additional analyses show robustness to adversarial fine-tuning, prompt-based attacks, and unsafe initializations, establishing adaptively gated alignment optimization as an effective approach for alignment-preserving and alignment-recovering model adaptation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5bf9\u9f50\u611f\u77e5\u7684\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u95e8\u63a7\u673a\u5236\u52a8\u6001\u5e73\u8861\u4efb\u52a1\u76ee\u6807\u4e0e\u5bf9\u9f50\u76ee\u6807\uff0c\u5728\u51cf\u5c11\u6709\u5bb3\u548c\u5e7b\u89c9\u8f93\u51fa\u7684\u540c\u65f6\u4fdd\u6301\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd", "motivation": "\u6807\u51c6\u5fae\u8c03\u5b64\u7acb\u5730\u4f18\u5316\u4efb\u52a1\u76ee\u6807\uff0c\u5ffd\u89c6\u5b89\u5168\u6027\u548c\u907f\u514d\u5e7b\u89c9\u7b49\u5173\u952e\u5bf9\u9f50\u76ee\u6807\uff0c\u5bfc\u81f4\u5bf9\u9f50\u6027\u9000\u5316\u4e14\u65e0\u6cd5\u7ea0\u6b63\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u5df2\u6709\u7684\u884c\u4e3a\u504f\u5dee", "method": "\u91c7\u7528\u7b56\u7565\u68af\u5ea6\u6b63\u5219\u5316\u6574\u5408\u5916\u90e8\u5bf9\u9f50\u4fe1\u53f7\uff0c\u8bbe\u8ba1\u81ea\u9002\u5e94\u95e8\u63a7\u673a\u5236\u5b9e\u73b0\u6837\u672c\u7ea7\u52a8\u6001\u68af\u5ea6\u5e73\u8861\uff0c\u5e76\u4e3a\u5b8c\u5168\u672a\u5bf9\u9f50\u8f93\u5165\u5b66\u4e60 abstention \u884c\u4e3a", "result": "\u5728\u901a\u7528\u548c\u9886\u57df\u7279\u5b9a\u6307\u4ee4\u5fae\u8c03\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6709\u5bb3\u548c\u5e7b\u89c9\u8f93\u51fa\u6301\u7eed\u51cf\u5c11\u4e14\u4e0d\u5f71\u54cd\u4efb\u52a1\u6027\u80fd\uff0c\u5bf9\u5bf9\u6297\u6027\u5fae\u8c03\u3001\u63d0\u793a\u653b\u51fb\u548c\u4e0d\u5b89\u5168\u521d\u59cb\u5316\u5177\u6709\u9c81\u68d2\u6027", "conclusion": "\u81ea\u9002\u5e94\u95e8\u63a7\u5bf9\u9f50\u4f18\u5316\u662f\u5b9e\u73b0\u5bf9\u9f50\u4fdd\u6301\u4e0e\u5bf9\u9f50\u6062\u590d\u6a21\u578b\u9002\u5e94\u7684\u6709\u6548\u65b9\u6cd5"}}
{"id": "2602.02259", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02259", "abs": "https://arxiv.org/abs/2602.02259", "authors": ["Hamza Adnan", "Matthew T. Jackson", "Alexey Zakharov"], "title": "Segment to Focus: Guiding Latent Action Models in the Presence of Distractors", "comment": null, "summary": "Latent Action Models (LAMs) learn to extract action-relevant representations solely from raw observations, enabling reinforcement learning from unlabelled videos and significantly scaling available training data. However, LAMs face a critical challenge in disentangling action-relevant features from action-correlated noise (e.g., background motion). Failing to filter these distractors causes LAMs to capture spurious correlations and build sub-optimal latent action spaces. In this paper, we introduce MaskLAM -- a lightweight modification to LAM training to mitigate this issue by incorporating visual agent segmentation. MaskLAM utilises segmentation masks from pretrained foundation models to weight the LAM reconstruction loss, thereby prioritising salient information over background elements while requiring no architectural modifications. We demonstrate the effectiveness of our method on continuous-control MuJoCo tasks, modified with action-correlated background noise. Our approach yields up to a 4x increase in accrued rewards compared to standard baselines and a 3x improvement in the latent action quality, as evidenced by linear probe evaluation.", "AI": {"tldr": "The paper proposes MaskLAM, a lightweight method that uses segmentation masks to improve latent action models by filtering action-correlated noise, achieving 4x reward gains and 3x quality improvement on noisy MuJoCo tasks.", "motivation": "LAMs struggle to disentangle action-relevant features from action-correlated noise like background motion, leading to spurious correlations and sub-optimal performance when learning from unlabelled videos.", "method": "MaskLAM incorporates visual agent segmentation by using masks from pretrained foundation models to weight the reconstruction loss, prioritizing salient agent information over background elements without requiring architectural changes.", "result": "On continuous-control MuJoCo tasks with action-correlated background noise, MaskLAM achieves up to 4x higher accrued rewards than baselines and 3x improvement in latent action quality as measured by linear probe evaluation.", "conclusion": "The segmentation-based approach effectively mitigates noise interference in LAMs, significantly improving their performance and scalability for reinforcement learning from unlabelled videos."}}
{"id": "2602.02260", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02260", "abs": "https://arxiv.org/abs/2602.02260", "authors": ["Zhengjia Zhuo", "Anupam Gupta", "Viswanath Nagarajan"], "title": "Learning Markov Decision Processes under Fully Bandit Feedback", "comment": null, "summary": "A standard assumption in Reinforcement Learning is that the agent observes every visited state-action pair in the associated Markov Decision Process (MDP), along with the per-step rewards. Strong theoretical results are known in this setting, achieving nearly-tight $\u0398(\\sqrt{T})$-regret bounds. However, such detailed feedback can be unrealistic, and recent research has investigated more restricted settings such as trajectory feedback, where the agent observes all the visited state-action pairs, but only a single \\emph{aggregate} reward. In this paper, we consider a far more restrictive ``fully bandit'' feedback model for episodic MDPs, where the agent does not even observe the visited state-action pairs -- it only learns the aggregate reward. We provide the first efficient bandit learning algorithm for episodic MDPs with $\\widetilde{O}(\\sqrt{T})$ regret. Our regret has an exponential dependence on the horizon length $\\H$, which we show is necessary. We also obtain improved nearly-tight regret bounds for ``ordered'' MDPs; these can be used to model classical stochastic optimization problems such as $k$-item prophet inequality and sequential posted pricing. Finally, we evaluate the empirical performance of our algorithm for the setting of $k$-item prophet inequalities; despite the highly restricted feedback, our algorithm's performance is comparable to that of a state-of-art learning algorithm (UCB-VI) with detailed state-action feedback.", "AI": {"tldr": "\u9996\u6b21\u63d0\u51fa\u5e76\u89e3\u51b3\"\u5b8c\u5168\u8001\u864e\u673a\"\u53cd\u9988\u4e0b\u7684 episodic MDP \u5b66\u4e60\u95ee\u9898\uff0c\u4ec5\u89c2\u6d4b\u805a\u5408\u5956\u52b1\u5373\u53ef\u5b9e\u73b0 \u00d5(\u221aT) \u9057\u61be\u754c\uff0c\u867d\u9700\u6307\u6570\u7ea7\u4f9d\u8d56 horizon \u957f\u5ea6\u4f46\u7406\u8bba\u8bc1\u660e\u4e3a\u5fc5\u8981\u4ee3\u4ef7\uff0c\u5e76\u5728 prophet \u4e0d\u7b49\u5f0f\u7b49\u573a\u666f\u5b9e\u8bc1\u6709\u6548", "motivation": "\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u5047\u8bbe\u667a\u80fd\u4f53\u80fd\u89c2\u6d4b\u6240\u6709\u72b6\u6001-\u52a8\u4f5c\u5bf9\u53ca\u5355\u6b65\u5956\u52b1\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u8be6\u7ec6\u53cd\u9988\u5e38\u4e0d\u73b0\u5b9e\uff1b\u73b0\u6709\u7814\u7a76\u4ec5\u8003\u8651\u8f68\u8ff9\u53cd\u9988\uff08\u89c2\u6d4b\u72b6\u6001-\u52a8\u4f5c\u4f46\u4ec5\u83b7\u805a\u5408\u5956\u52b1\uff09\uff0c\u800c\u672c\u6587\u6311\u6218\u66f4\u6781\u7aef\u573a\u666f\uff1a\u667a\u80fd\u4f53\u5b8c\u5168\u65e0\u6cd5\u89c2\u6d4b\u72b6\u6001-\u52a8\u4f5c\u5bf9\uff0c\u4ec5\u80fd\u83b7\u5f97\u6bcf\u8f6e episode \u7684\u805a\u5408\u5956\u52b1", "method": "\u9488\u5bf9 episodic MDP \u8bbe\u8ba1\u65b0\u578b\u9ad8\u6548\u8001\u864e\u673a\u5b66\u4e60\u7b97\u6cd5\uff1a\u5728\"\u5b8c\u5168\u8001\u864e\u673a\"\u53cd\u9988\u6a21\u578b\u4e0b\uff08\u65e0\u72b6\u6001-\u52a8\u4f5c\u89c2\u6d4b\uff0c\u4ec5\u83b7\u805a\u5408\u5956\u52b1\uff09\uff0c\u901a\u8fc7\u521b\u65b0\u673a\u5236\u5904\u7406\u4fe1\u606f\u7f3a\u5931\u95ee\u9898\uff1b\u540c\u65f6\u9488\u5bf9\"\u6709\u5e8f\"MDP \u7ed3\u6784\uff08\u53ef\u5efa\u6a21 k-\u7269\u54c1 prophet \u4e0d\u7b49\u5f0f\u7b49\u968f\u673a\u4f18\u5316\u95ee\u9898\uff09\u4f18\u5316\u7b97\u6cd5\u8bbe\u8ba1", "result": "1) \u9996\u6b21\u5b9e\u73b0 \u00d5(\u221aT) \u9057\u61be\u754c\uff0c\u4f46\u9057\u61be\u754c\u542b horizon \u957f\u5ea6 H \u7684\u6307\u6570\u4f9d\u8d56\uff08\u7406\u8bba\u8bc1\u660e\u8be5\u6307\u6570\u4f9d\u8d56\u4e0d\u53ef\u907f\u514d\uff09\uff1b2) \"\u6709\u5e8f\"MDP \u83b7\u5f97\u6539\u8fdb\u7684\u7d27\u9057\u61be\u754c\uff1b3) \u5728 k-\u7269\u54c1 prophet \u4e0d\u7b49\u5f0f\u573a\u666f\u7684\u5b9e\u8bc1\u8868\u73b0\u5ab2\u7f8e\u9700\u8be6\u7ec6\u53cd\u9988\u7684 SOTA \u7b97\u6cd5 UCB-VI", "conclusion": "\u7a81\u7834\u5f3a\u5316\u5b66\u4e60\u5bf9\u8be6\u7ec6\u53cd\u9988\u7684\u4f9d\u8d56\uff0c\u8bc1\u660e\u5728\u4ec5\u83b7\u805a\u5408\u5956\u52b1\u7684\u6781\u7aef\u9650\u5236\u4e0b\u4ecd\u53ef\u9ad8\u6548\u5b66\u4e60\uff1b\u7406\u8bba\u63ed\u793a\u4fe1\u606f\u7f3a\u5931\u7684\u56fa\u6709\u4ee3\u4ef7\uff08H \u7684\u6307\u6570\u4f9d\u8d56\uff09\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u63d0\u4f9b\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u62d3\u5c55 RL \u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9002\u7528\u8fb9\u754c"}}
{"id": "2602.02261", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02261", "abs": "https://arxiv.org/abs/2602.02261", "authors": ["Daniil Shlenskii", "Alexander Varlamov", "Nazar Buzun", "Alexander Korotin"], "title": "Unlocking the Duality between Flow and Field Matching", "comment": null, "summary": "Conditional Flow Matching (CFM) unifies conventional generative paradigms such as diffusion models and flow matching. Interaction Field Matching (IFM) is a newer framework that generalizes Electrostatic Field Matching (EFM) rooted in Poisson Flow Generative Models (PFGM). While both frameworks define generative dynamics, they start from different objects: CFM specifies a conditional probability path in data space, whereas IFM specifies a physics-inspired interaction field in an augmented data space. This raises a basic question: are CFM and IFM genuinely different, or are they two descriptions of the same underlying dynamics? We show that they coincide for a natural subclass of IFM that we call forward-only IFM. Specifically, we construct a bijection between CFM and forward-only IFM. We further show that general IFM is strictly more expressive: it includes EFM and other interaction fields that cannot be realized within the standard CFM formulation. Finally, we highlight how this duality can benefit both frameworks: it provides a probabilistic interpretation of forward-only IFM and yields novel, IFM-driven techniques for CFM.", "AI": {"tldr": "\u672c\u6587\u5efa\u7acb\u4e86\u6761\u4ef6\u6d41\u5339\u914d(CFM)\u4e0e\u6b63\u5411\u4ea4\u4e92\u573a\u5339\u914d(IFM)\u5b50\u7c7b\u4e4b\u95f4\u7684\u5bf9\u5076\u5173\u7cfb\uff0c\u8bc1\u660e\u4e8c\u8005\u7b49\u4ef7\uff0c\u800c\u4e00\u822cIFM\u66f4\u5177\u8868\u8fbe\u529b\uff0c\u4e3a\u4e24\u4e2a\u6846\u67b6\u5e26\u6765\u65b0\u7684\u6982\u7387\u89e3\u91ca\u548c\u6280\u672f\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7a76\u6761\u4ef6\u6d41\u5339\u914d(CFM)\u4e0e\u4ea4\u4e92\u573a\u5339\u914d(IFM)\u8fd9\u4e24\u4e2a\u751f\u6210\u5f0f\u5efa\u6a21\u6846\u67b6\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5b83\u4eec\u4ece\u4e0d\u540c\u5bf9\u8c61\u51fa\u53d1(\u6761\u4ef6\u6982\u7387\u8def\u5f84 vs \u7269\u7406\u542f\u53d1\u7684\u4ea4\u4e92\u573a)\uff0c\u4f46\u5b9a\u4e49\u76f8\u4f3c\u7684\u751f\u6210\u52a8\u529b\u5b66\uff0c\u9700\u8981\u5398\u6e05\u5b83\u4eec\u662f\u672c\u8d28\u4e0d\u540c\u8fd8\u662f\u540c\u4e00\u52a8\u529b\u5b66\u7684\u4e0d\u540c\u63cf\u8ff0\u3002", "method": "\u901a\u8fc7\u6570\u5b66\u7ed3\u6784\u5206\u6790\uff0c\u6784\u5efa\u4e86CFM\u4e0e\u6b63\u5411-only IFM\u4e4b\u95f4\u7684\u53cc\u5c04\u5173\u7cfb\uff0c\u5e76\u6bd4\u8f83\u4e86\u901a\u7528IFM\u4e0eCFM\u7684\u8868\u8fbe\u80fd\u529b\u5dee\u5f02\u3002", "result": "1) CFM\u4e0e\u6b63\u5411-only IFM\u5b8c\u5168\u7b49\u4ef7\uff0c\u5b58\u5728\u53cc\u5c04\u5173\u7cfb\uff1b2) \u901a\u7528IFM\u4e25\u683c\u66f4\u5177\u8868\u8fbe\u529b\uff0c\u5305\u542bEFM\u7b49\u65e0\u6cd5\u5728\u6807\u51c6CFM\u4e2d\u5b9e\u73b0\u7684\u4ea4\u4e92\u573a\uff1b3) \u8be5\u5bf9\u5076\u6027\u4e3a\u6b63\u5411-only IFM\u63d0\u4f9b\u6982\u7387\u89e3\u91ca\uff0c\u5e76\u4e3aCFM\u4ea7\u751f\u65b0\u7684IFM\u9a71\u52a8\u6280\u672f\u3002", "conclusion": "CFM\u4e0eIFM\u7684\u5b50\u7c7b\u672c\u8d28\u76f8\u540c\uff0c\u4f46\u901a\u7528IFM\u6846\u67b6\u66f4\u5177\u4e00\u822c\u6027\uff0c\u8fd9\u79cd\u5bf9\u5076\u5173\u7cfb\u6709\u52a9\u4e8e\u4e24\u4e2a\u6846\u67b6\u7684\u76f8\u4e92\u4fc3\u8fdb\u548c\u53d1\u5c55\u3002"}}
{"id": "2602.02268", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02268", "abs": "https://arxiv.org/abs/2602.02268", "authors": ["Sanggeon Yun", "Raheeb Hassan", "Ryozo Masukawa", "Sungheon Jeong", "Mohsen Imani"], "title": "HopFormer: Sparse Graph Transformers with Explicit Receptive Field Control", "comment": null, "summary": "Graph Transformers typically rely on explicit positional or structural encodings and dense global attention to incorporate graph topology. In this work, we show that neither is essential. We introduce HopFormer, a graph Transformer that injects structure exclusively through head-specific n-hop masked sparse attention, without the use of positional encodings or architectural modifications. This design provides explicit and interpretable control over receptive fields while enabling genuinely sparse attention whose computational cost scales linearly with mask sparsity. Through extensive experiments on both node-level and graph-level benchmarks, we demonstrate that our approach achieves competitive or superior performance across diverse graph structures. Our results further reveal that dense global attention is often unnecessary: on graphs with strong small-world properties, localized attention yields more stable and consistently high performance, while on graphs with weaker small-world effects, global attention offers diminishing returns. Together, these findings challenge prevailing assumptions in graph Transformer design and highlight sparsity-controlled attention as a principled and efficient alternative.", "AI": {"tldr": "HopFormer uses sparse n-hop masked attention instead of positional encodings or dense attention, achieving competitive performance while challenging the need for global attention in graph Transformers.", "motivation": "Current graph Transformers rely on explicit positional/structural encodings and dense global attention, but the authors believe these are not essential and want to explore simpler, more interpretable, and computationally efficient alternatives.", "method": "HopFormer: a graph Transformer using head-specific n-hop masked sparse attention without positional encodings or architectural modifications, enabling explicit control over receptive fields and linear computational scaling with mask sparsity.", "result": "Achieves competitive or superior performance across diverse graph structures on node-level and graph-level benchmarks. Dense global attention is often unnecessary: localized attention works better on small-world graphs, while global attention has diminishing returns on graphs with weak small-world effects.", "conclusion": "Challenges prevailing assumptions in graph Transformer design, showing that sparsity-controlled attention is a principled and efficient alternative to dense global attention."}}
{"id": "2602.02283", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.02283", "abs": "https://arxiv.org/abs/2602.02283", "authors": ["Owen Shen", "Patrick Jaillet"], "title": "Choice-Model-Assisted Q-learning for Delayed-Feedback Revenue Management", "comment": null, "summary": "We study reinforcement learning for revenue management with delayed feedback, where a substantial fraction of value is determined by customer cancellations and modifications observed days after booking. We propose \\emph{choice-model-assisted RL}: a calibrated discrete choice model is used as a fixed partial world model to impute the delayed component of the learning target at decision time. In the fixed-model deployment regime, we prove that tabular Q-learning with model-imputed targets converges to an $O(\\varepsilon/(1-\u03b3))$ neighborhood of the optimal Q-function, where $\\varepsilon$ summarizes partial-model error, with an additional $O(t^{-1/2})$ sampling term. Experiments in a simulator calibrated from 61{,}619 hotel bookings (1{,}088 independent runs) show: (i) no statistically detectable difference from a maturity-buffer DQN baseline in stationary settings; (ii) positive effects under in-family parameter shifts, with significant gains in 5 of 10 shift scenarios after Holm--Bonferroni correction (up to 12.4\\%); and (iii) consistent degradation under structural misspecification, where the choice model assumptions are violated (1.4--2.6\\% lower revenue). These results characterize when partial behavioral models improve robustness under shift and when they introduce harmful bias.", "AI": {"tldr": "\u7528\u9009\u62e9\u6a21\u578b\u8f85\u52a9\u5f3a\u5316\u5b66\u4e60\u5904\u7406\u6536\u76ca\u7ba1\u7406\u4e2d\u7684\u5ef6\u8fdf\u53cd\u9988\u95ee\u9898\uff0c\u8bc1\u660e\u6536\u655b\u6027\u8fb9\u754c\uff0c\u5b9e\u9a8c\u663e\u793a\u53c2\u6570\u6f02\u79fb\u65f6\u63d0\u5347\u6536\u76ca\u4f46\u6a21\u578b\u8bef\u8bbe\u65f6\u6709\u5bb3", "motivation": "\u6536\u76ca\u7ba1\u7406\u4e2d\u5ba2\u6237\u53d6\u6d88/\u4fee\u6539\u884c\u4e3a\u5bfc\u81f4\u4ef7\u503c\u53cd\u9988\u5ef6\u8fdf\u6570\u65e5\uff0c\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u56e0\u5ef6\u8fdf\u89c2\u6d4b\u96be\u4ee5\u6709\u6548\u4f18\u5316\u51b3\u7b56", "method": "\u63d0\u51fa\u9009\u62e9\u6a21\u578b\u8f85\u52a9\u7684RL\uff1a\u7528\u6821\u51c6\u7684\u79bb\u6563\u9009\u62e9\u6a21\u578b\u4f5c\u4e3a\u56fa\u5b9a\u90e8\u5206\u4e16\u754c\u6a21\u578b\uff0c\u5728\u51b3\u7b56\u65f6\u4f30\u7b97\u5ef6\u8fdf\u53cd\u9988\u7684\u5b66\u4e60\u76ee\u6807", "result": "1) \u7406\u8bba\uff1a\u8868\u683cQ\u5b66\u4e60\u6536\u655b\u81f3\u6700\u4f18Q\u51fd\u6570O(\u03b5/(1-\u03b3))\u90bb\u57df\uff08\u03b5\u4e3a\u6a21\u578b\u8bef\u5dee\uff09 2) \u5b9e\u9a8c\uff1a\u7a33\u6001\u4e0b\u4e0e\u57fa\u51c6\u65e0\u5dee\u5f02\uff1b\u53c2\u6570\u6f02\u79fb\u65f65/10\u573a\u666f\u663e\u8457\u63d0\u5347\u6536\u76ca\uff08\u6700\u9ad812.4%\uff09\uff1b\u6a21\u578b\u8bef\u8bbe\u65f6\u6536\u76ca\u964d\u4f4e1.4-2.6%", "conclusion": "\u90e8\u5206\u884c\u4e3a\u6a21\u578b\u5728\u53c2\u6570\u6f02\u79fb\u65f6\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u4f46\u5728\u6a21\u578b\u7ed3\u6784\u8bef\u8bbe\u65f6\u5f15\u5165\u6709\u5bb3\u504f\u5dee\uff0c\u660e\u786e\u5176\u9002\u7528\u8fb9\u754c"}}
{"id": "2602.02371", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.02371", "abs": "https://arxiv.org/abs/2602.02371", "authors": ["Jing Wang", "Jie Shen", "Qiaomin Xie", "Jeremy C Weiss"], "title": "C-kNN-LSH: A Nearest-Neighbor Algorithm for Sequential Counterfactual Inference", "comment": null, "summary": "Estimating causal effects from longitudinal trajectories is central to understanding the progression of complex conditions and optimizing clinical decision-making, such as comorbidities and long COVID recovery. We introduce \\emph{C-kNN--LSH}, a nearest-neighbor framework for sequential causal inference designed to handle such high-dimensional, confounded situations. By utilizing locality-sensitive hashing, we efficiently identify ``clinical twins'' with similar covariate histories, enabling local estimation of conditional treatment effects across evolving disease states. To mitigate bias from irregular sampling and shifting patient recovery profiles, we integrate neighborhood estimator with a doubly-robust correction.\n  Theoretical analysis guarantees our estimator is consistent and second-order robust to nuisance error.\n  Evaluated on a real-world Long COVID cohort with 13,511 participants, \\emph{C-kNN-LSH} demonstrates superior performance in capturing recovery heterogeneity and estimating policy values compared to existing baselines.", "AI": {"tldr": "\u63d0\u51faC-kNN-LSH\u6846\u67b6\uff0c\u7528\u5c40\u90e8\u654f\u611f\u54c8\u5e0c\u627e\"\u4e34\u5e8a\u53cc\u80de\u80ce\"\uff0c\u53cc\u91cd\u7a33\u5065\u6821\u6b63\u5904\u7406\u7eb5\u5411\u6570\u636e\u56e0\u679c\u63a8\u65ad\uff0c\u57281.3\u4e07\u957f\u65b0\u51a0\u60a3\u8005\u4e2d\u9a8c\u8bc1\u6709\u6548\u3002", "motivation": "\u4ece\u7eb5\u5411\u8f68\u8ff9\u4f30\u8ba1\u56e0\u679c\u6548\u5e94\u5bf9\u7406\u89e3\u590d\u6742\u75be\u75c5\u8fdb\u5c55\u548c\u4f18\u5316\u4e34\u5e8a\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u5728\u5171\u75c5\u548c\u957f\u65b0\u51a0\u6062\u590d\u4e2d\u3002", "method": "\u57fa\u4e8e\u5c40\u90e8\u654f\u611f\u54c8\u5e0c\u7684\u6700\u8fd1\u90bb\u6846\u67b6\uff0c\u8bc6\u522b\u76f8\u4f3c\u534f\u53d8\u91cf\u5386\u53f2\u7684\"\u4e34\u5e8a\u53cc\u80de\u80ce\"\uff0c\u5c40\u90e8\u4f30\u8ba1\u6761\u4ef6\u6cbb\u7597\u6548\u679c\uff0c\u96c6\u6210\u53cc\u91cd\u7a33\u5065\u6821\u6b63\u51cf\u8f7b\u504f\u501a\u3002", "result": "\u7406\u8bba\u4fdd\u8bc1\u4f30\u8ba1\u91cf\u4e00\u81f4\u6027\u548c\u4e8c\u9636\u9c81\u68d2\u6027\uff1b\u572813,511\u4eba\u957f\u65b0\u51a0\u961f\u5217\u4e2d\u4f18\u4e8e\u57fa\u7ebf\uff0c\u66f4\u597d\u6355\u6349\u6062\u590d\u5f02\u8d28\u6027\u548c\u653f\u7b56\u4ef7\u503c\u3002", "conclusion": "C-kNN-LSH\u4e3a\u9ad8\u7ef4\u6df7\u6742\u7eb5\u5411\u533b\u7597\u6570\u636e\u63d0\u4f9b\u6709\u6548\u56e0\u679c\u63a8\u65ad\u5de5\u5177\uff0c\u5728\u957f\u65b0\u51a0\u7814\u7a76\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002"}}
{"id": "2602.02381", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02381", "abs": "https://arxiv.org/abs/2602.02381", "authors": ["Yipeng Zhang", "Hafez Ghaemi", "Jungyoon Lee", "Shahab Bakhtiari", "Eilif B. Muller", "Laurent Charlin"], "title": "Self-Supervised Learning from Structural Invariance", "comment": "ICLR 2026", "summary": "Joint-embedding self-supervised learning (SSL), the key paradigm for unsupervised representation learning from visual data, learns from invariances between semantically-related data pairs. We study the one-to-many mapping problem in SSL, where each datum may be mapped to multiple valid targets. This arises when data pairs come from naturally occurring generative processes, e.g., successive video frames. We show that existing methods struggle to flexibly capture this conditional uncertainty. As a remedy, we introduce a latent variable to account for this uncertainty and derive a variational lower bound on the mutual information between paired embeddings. Our derivation yields a simple regularization term for standard SSL objectives. The resulting method, which we call AdaSSL, applies to both contrastive and distillation-based SSL objectives, and we empirically show its versatility in causal representation learning, fine-grained image understanding, and world modeling on videos.", "AI": {"tldr": "AdaSSL introduces a latent variable to handle one-to-many mapping in self-supervised learning, using a variational mutual information lower bound to create a versatile regularization term applicable to both contrastive and distillation-based methods.", "motivation": "Existing joint-embedding SSL methods struggle with one-to-many mapping problems where single inputs correspond to multiple valid targets (e.g., successive video frames), failing to capture conditional uncertainty in naturally occurring generative processes.", "method": "Introduces a latent variable to model conditional uncertainty, derives a variational lower bound on mutual information between paired embeddings, yielding a simple regularization term (AdaSSL) compatible with standard SSL objectives.", "result": "Empirically demonstrates AdaSSL's versatility across contrastive and distillation-based SSL, showing effectiveness in causal representation learning, fine-grained image understanding, and video world modeling tasks.", "conclusion": "AdaSSL successfully addresses conditional uncertainty in SSL through a theoretically-grounded regularization approach, proving adaptable to diverse SSL paradigms and downstream applications without architectural changes."}}
{"id": "2602.02383", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02383", "abs": "https://arxiv.org/abs/2602.02383", "authors": ["Maksim Afanasyev", "Illarion Iov"], "title": "SLIME: Stabilized Likelihood Implicit Margin Enforcement for Preference Optimization", "comment": null, "summary": "Direct preference optimization methods have emerged as a computationally efficient alternative to Reinforcement Learning from Human Feedback (RLHF) for aligning Large Language Models (LLMs). Latest approaches have streamlined the alignment process by deriving implicit reward functions, yet they often suffer from a critical objective mismatch: optimizing the relative margin between chosen and rejected responses does not guarantee the preservation of the chosen response's absolute likelihood. This can lead to ``unlearning'', where the model degrades the probability of high-quality outputs to satisfy margin constraints, and ``formatting collapse'' caused by the over-penalization of rejected sequences. In this work, we introduce SLIME (Stabilized Likelihood Implicit Margin Enforcement), a reference-free alignment objective designed to decouple preference learning from generation quality. SLIME incorporates a three-pronged objective: (1) an anchoring term to maximize the likelihood of preferred responses; (2) a stabilizing penalty that prevents the probabilities of rejected tokens from collapsing to zero; and (3) a dual-margin mechanism that combines hard and soft constraints for precise boundary shaping. Our results demonstrate that SLIME achieves superior performance compared to state-of-the-art baselines while maintaining higher generation stability.", "AI": {"tldr": "\u63d0\u51faSLIME\u65b9\u6cd5\u89e3\u51b3\u76f4\u63a5\u504f\u597d\u4f18\u5316\u4e2d\u7684\u76ee\u6807\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u901a\u8fc7\u4e09\u76ee\u6807\u8bbe\u8ba1\u5b9e\u73b0\u504f\u597d\u5b66\u4e60\u4e0e\u751f\u6210\u8d28\u91cf\u89e3\u8026\uff0c\u5728\u4fdd\u6301\u751f\u6210\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u63d0\u5347\u6027\u80fd", "motivation": "\u73b0\u6709\u76f4\u63a5\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u5b58\u5728\u76ee\u6807\u4e0d\u5339\u914d\u95ee\u9898\uff1a\u4f18\u5316\u9009\u62e9/\u62d2\u7edd\u54cd\u5e94\u7684\u76f8\u5bf9\u8fb9\u754c\u4e0d\u80fd\u4fdd\u8bc1\u4fdd\u6301\u9009\u62e9\u54cd\u5e94\u7684\u7edd\u5bf9\u4f3c\u7136\u6027\uff0c\u5bfc\u81f4\u6a21\u578b\"\u9057\u5fd8\"\u9ad8\u8d28\u91cf\u8f93\u51fa\u548c\"\u683c\u5f0f\u5d29\u6e83\"", "method": "\u63d0\u51faSLIME\uff08\u7a33\u5b9a\u4f3c\u7136\u9690\u5f0f\u8fb9\u754c\u6267\u884c\uff09\u4e09\u76ee\u6807\u6846\u67b6\uff1a1\uff09\u951a\u5b9a\u9879\u6700\u5927\u5316\u504f\u597d\u54cd\u5e94\u4f3c\u7136\uff1b2\uff09\u7a33\u5b9a\u60e9\u7f5a\u9632\u6b62\u62d2\u7eddtoken\u6982\u7387\u574d\u7f29\uff1b3\uff09\u8f6f\u786c\u7ea6\u675f\u7ed3\u5408\u7684\u53cc\u8fb9\u754c\u673a\u5236", "result": "SLIME\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u57fa\u7ebf\uff0c\u540c\u65f6\u4fdd\u6301\u66f4\u9ad8\u7684\u751f\u6210\u7a33\u5b9a\u6027", "conclusion": "\u6210\u529f\u8bbe\u8ba1\u51fa\u53ef\u89e3\u8026\u504f\u597d\u5b66\u4e60\u4e0e\u751f\u6210\u8d28\u91cf\u7684\u53c2\u8003\u65e0\u5173\u5bf9\u9f50\u76ee\u6807\uff0c\u89e3\u51b3\u4e86\u76f4\u63a5\u504f\u597d\u4f18\u5316\u4e2d\u7684\u6838\u5fc3\u7f3a\u9677"}}
{"id": "2602.02385", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02385", "abs": "https://arxiv.org/abs/2602.02385", "authors": ["Adam Shai", "Loren Amdahl-Culleton", "Casper L. Christensen", "Henry R. Bigelow", "Fernando E. Rosas", "Alexander B. Boyd", "Eric A. Alt", "Kyle J. Ray", "Paul M. Riechers"], "title": "Transformers learn factored representations", "comment": null, "summary": "Transformers pretrained via next token prediction learn to factor their world into parts, representing these factors in orthogonal subspaces of the residual stream. We formalize two representational hypotheses: (1) a representation in the product space of all factors, whose dimension grows exponentially with the number of parts, or (2) a factored representation in orthogonal subspaces, whose dimension grows linearly. The factored representation is lossless when factors are conditionally independent, but sacrifices predictive fidelity otherwise, creating a tradeoff between dimensional efficiency and accuracy. We derive precise predictions about the geometric structure of activations for each, including the number of subspaces, their dimensionality, and the arrangement of context embeddings within them. We test between these hypotheses on transformers trained on synthetic processes with known latent structure. Models learn factored representations when factors are conditionally independent, and continue to favor them early in training even when noise or hidden dependencies undermine conditional independence, reflecting an inductive bias toward factoring at the cost of fidelity. This provides a principled explanation for why transformers decompose the world into parts, and suggests that interpretable low dimensional structure may persist even in models trained on complex data.", "AI": {"tldr": "\u672c\u6587\u63ed\u793aTransformer\u901a\u8fc7\u4e0b\u4e00\u4e2a\u8bcd\u5143\u9884\u6d4b\u9884\u8bad\u7ec3\u65f6\uff0c\u4f1a\u5c06\u4e16\u754c\u5206\u89e3\u4e3a\u5404\u90e8\u5206\uff0c\u5e76\u5c06\u8fd9\u4e9b\u56e0\u5b50\u8868\u793a\u5728\u6b8b\u5dee\u6d41\u7684\u6b63\u4ea4\u5b50\u7a7a\u95f4\u4e2d\uff0c\u5373\u4f7f\u727a\u7272\u9884\u6d4b\u51c6\u786e\u6027\u4e5f\u5728\u6240\u4e0d\u60dc\uff0c\u8fd9\u89e3\u91ca\u4e86Transformer\u4e3a\u4f55\u80fd\u5b66\u4e60\u53ef\u89e3\u91ca\u7684\u8868\u793a\u3002", "motivation": "\u7406\u89e3Transformer\u5185\u90e8\u5982\u4f55\u8868\u793a\u7ed3\u6784\u5316\u4fe1\u606f\uff0c\u4ee5\u53ca\u4e3a\u4f55\u5b83\u4eec\u503e\u5411\u4e8e\u5c06\u590d\u6742\u4e16\u754c\u5206\u89e3\u4e3a\u53ef\u89e3\u91ca\u7684\u72ec\u7acb\u90e8\u5206\uff0c\u5f62\u5f0f\u5316\u5e76\u9a8c\u8bc1\u4e24\u79cd\u7ade\u4e89\u6027\u7684\u8868\u793a\u5047\u8bbe\u3002", "method": "\u5f62\u5f0f\u5316\u4e24\u79cd\u8868\u793a\u5047\u8bbe\uff08\u5404\u56e0\u5b50\u7684\u4e58\u79ef\u7a7a\u95f4 vs. \u6b63\u4ea4\u5b50\u7a7a\u95f4\u4e2d\u7684\u5206\u89e3\u8868\u793a\uff09\uff0c\u63a8\u5bfc\u5173\u4e8e\u6fc0\u6d3b\u51e0\u4f55\u7ed3\u6784\u7684\u7cbe\u786e\u9884\u6d4b\uff0c\u5e76\u5728\u5177\u6709\u5df2\u77e5\u6f5c\u5728\u7ed3\u6784\u7684\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3Transformer\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5f53\u56e0\u5b50\u6761\u4ef6\u72ec\u7acb\u65f6\uff0cTransformer\u5b66\u4e60\u6b63\u4ea4\u5b50\u7a7a\u95f4\u4e2d\u7684\u5206\u89e3\u8868\u793a\uff1b\u5373\u4f7f\u5728\u566a\u58f0\u6216\u9690\u85cf\u4f9d\u8d56\u7834\u574f\u6761\u4ef6\u72ec\u7acb\u6027\u65f6\uff0c\u8bad\u7ec3\u65e9\u671f\u4ecd\u504f\u597d\u8fd9\u79cd\u5206\u89e3\uff0c\u8868\u73b0\u51fa\u727a\u7272\u4fdd\u771f\u5ea6\u7684\u5206\u89e3\u5f52\u7eb3\u504f\u7f6e\u3002", "conclusion": "Transformer\u5177\u6709\u5206\u89e3\u8868\u793a\u7684\u5185\u5728\u504f\u7f6e\uff0c\u8fd9\u4e3a\u5b83\u4eec\u5206\u89e3\u4e16\u754c\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u89e3\u91ca\uff0c\u5e76\u6697\u793a\u5373\u4f7f\u5728\u590d\u6742\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u4e2d\uff0c\u53ef\u89e3\u91ca\u7684\u4f4e\u7ef4\u7ed3\u6784\u53ef\u80fd\u6301\u7eed\u5b58\u5728\u3002"}}
{"id": "2602.02400", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02400", "abs": "https://arxiv.org/abs/2602.02400", "authors": ["Qizhen Zhang", "Ankush Garg", "Jakob Foerster", "Niladri Chatterji", "Kshitiz Malik", "Mike Lewis"], "title": "An Empirical Study on Noisy Data and LLM Pretraining Loss Divergence", "comment": null, "summary": "Large-scale pretraining datasets drive the success of large language models (LLMs). However, these web-scale corpora inevitably contain large amounts of noisy data due to unregulated web content or randomness inherent in data. Although LLM pretrainers often speculate that such noise contributes to instabilities in large-scale LLM pretraining and, in the worst cases, loss divergence, this phenomenon remains poorly understood.In this work, we present a systematic empirical study of whether noisy data causes LLM pretraining divergences and how it does so. By injecting controlled synthetic uniformly random noise into otherwise clean datasets, we analyze training dynamics across model sizes ranging from 480M to 5.2B parameters. We show that noisy data indeed induces training loss divergence, and that the probability of divergence depends strongly on the noise type, amount of noise, and model scale. We further find that noise-induced divergences exhibit activation patterns distinct from those caused by high learning rates, and we provide diagnostics that differentiate these two failure modes. Together, these results provide a large-scale, controlled characterization of how noisy data affects loss divergence in LLM pretraining.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u63a2\u8ba8\u4e86\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u7684\u566a\u58f0\u5982\u4f55\u5bfc\u81f4\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u635f\u5931\u53d1\u6563\uff0c\u53d1\u73b0\u53d1\u6563\u6982\u7387\u53d6\u51b3\u4e8e\u566a\u58f0\u7c7b\u578b\u3001\u6570\u91cf\u548c\u6a21\u578b\u89c4\u6a21\uff0c\u4e14\u5176\u6fc0\u6d3b\u6a21\u5f0f\u4e0e\u5b66\u4e60\u7387\u8fc7\u9ad8\u5bfc\u81f4\u7684\u5931\u8d25\u4e0d\u540c\u3002", "motivation": "\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u4e0d\u53ef\u907f\u514d\u5730\u5305\u542b\u5927\u91cf\u566a\u58f0\u6570\u636e\uff0c\u53ef\u80fd\u5bfc\u81f4\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u751a\u81f3\u635f\u5931\u53d1\u6563\uff0c\u4f46\u8fd9\u4e00\u73b0\u8c61\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u5728\u5e72\u51c0\u6570\u636e\u96c6\u4e2d\u6ce8\u5165\u53d7\u63a7\u7684\u5408\u6210\u5747\u5300\u968f\u673a\u566a\u58f0\uff0c\u5bf94.8\u4ebf\u523052\u4ebf\u53c2\u6570\u89c4\u6a21\u7684\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u6027\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u8bad\u7ec3\u52a8\u6001\u3002", "result": "\u566a\u58f0\u6570\u636e\u786e\u5b9e\u4f1a\u5f15\u53d1\u8bad\u7ec3\u635f\u5931\u53d1\u6563\uff0c\u53d1\u6563\u6982\u7387\u5f3a\u70c8\u4f9d\u8d56\u4e8e\u566a\u58f0\u7c7b\u578b\u3001\u566a\u58f0\u91cf\u548c\u6a21\u578b\u89c4\u6a21\uff1b\u566a\u58f0\u5f15\u8d77\u7684\u53d1\u6563\u8868\u73b0\u51fa\u4e0e\u5b66\u4e60\u7387\u8fc7\u9ad8\u4e0d\u540c\u7684\u6fc0\u6d3b\u6a21\u5f0f\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u4e2d\u566a\u58f0\u5bf9\u635f\u5931\u53d1\u6563\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u3001\u53d7\u63a7\u7684\u8868\u5f81\uff0c\u4e3a\u533a\u5206\u4e0d\u540c\u5931\u8d25\u6a21\u5f0f\u63d0\u4f9b\u4e86\u8bca\u65ad\u65b9\u6cd5\u3002"}}
{"id": "2602.02415", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02415", "abs": "https://arxiv.org/abs/2602.02415", "authors": ["Vivienne Pelletier", "Daniel J. Rivera", "Obinna Nwokonkwo", "Steven A. Wilson", "Christopher L. Muhich"], "title": "Active Transfer Bagging: A New Approach for Accelerated Active Learning Acquisition of Data by Combined Transfer Learning and Bagging Based Models", "comment": null, "summary": "Modern machine learning has achieved remarkable success on many problems, but this success often depends on the existence of large, labeled datasets. While active learning can dramatically reduce labeling cost when annotations are expensive, early performance is frequently dominated by the initial seed set, typically chosen at random. In many applications, however, related or approximate datasets are readily available and can be leveraged to construct a better seed set. We introduce a new method for selecting the seed data set for active learning, Active-Transfer Bagging (ATBagging). ATBagging estimates the informativeness of candidate data point from a Bayesian interpretation of bagged ensemble models by comparing in-bag and out-of-bag predictive distributions from the labeled dataset, yielding an information-gain proxy. To avoid redundant selections, we impose feature-space diversity by sampling a determinantal point process (DPP) whose kernel uses Random Fourier Features and a quality-diversity factorization that incorporates the informativeness scores. This same blended method is used for selection of new data points to collect during the active learning phase. We evaluate ATBagging on four real-world datasets covering both target-transfer and feature-shift scenarios (QM9, ERA5, Forbes 2000, and Beijing PM2.5). Across seed sizes nseed = 10-100, ATBagging improves or ties early active learning and increases area under the learning-curve relative to alternative seed subset selection methodologies in almost all cases, with strongest benefits in low-data regimes. Thus, ATBagging provides a low-cost, high reward means to initiating active learning-based data collection.", "AI": {"tldr": "ATBagging is a novel method for selecting seed sets in active learning that leverages related datasets by using Bayesian bagged ensembles to estimate data point informativeness and determinantal point processes for diversity, significantly improving early performance in low-data regimes.", "motivation": "Machine learning success requires large labeled datasets, and while active learning reduces labeling costs, early performance heavily depends on the randomly chosen initial seed set. Related datasets are often available but underutilized for constructing better seed sets.", "method": "Active-Transfer Bagging (ATBagging) estimates candidate data point informativeness through a Bayesian interpretation of bagged ensemble models by comparing in-bag and out-of-bag predictive distributions. It enforces feature-space diversity using a determinantal point process (DPP) kernel with Random Fourier Features and a quality-diversity factorization incorporating informativeness scores.", "result": "Evaluated on four real-world datasets (QM9, ERA5, Forbes 2000, Beijing PM2.5) across seed sizes 10-100, ATBagging improves or ties early active learning performance in nearly all cases, increases area under the learning curve, and shows strongest benefits in low-data regimes.", "conclusion": "ATBagging provides a low-cost, high-reward approach to initiating active learning-based data collection by effectively leveraging related datasets to construct superior seed sets."}}
{"id": "2602.02417", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02417", "abs": "https://arxiv.org/abs/2602.02417", "authors": ["Zekun Wang", "Anant Gupta", "Christopher J. MacLellan"], "title": "Trust Region Continual Learning as an Implicit Meta-Learner", "comment": "19 pages, 23 tables", "summary": "Continual learning aims to acquire tasks sequentially without catastrophic forgetting, yet standard strategies face a core tradeoff: regularization-based methods (e.g., EWC) can overconstrain updates when task optima are weakly overlapping, while replay-based methods can retain performance but drift due to imperfect replay. We study a hybrid perspective: \\emph{trust region continual learning} that combines generative replay with a Fisher-metric trust region constraint. We show that, under local approximations, the resulting update admits a MAML-style interpretation with a single implicit inner step: replay supplies an old-task gradient signal (query-like), while the Fisher-weighted penalty provides an efficient offline curvature shaping (support-like). This yields an emergent meta-learning property in continual learning: the model becomes an initialization that rapidly \\emph{re-converges} to prior task optima after each task transition, without explicitly optimizing a bilevel objective. Empirically, on task-incremental diffusion image generation and continual diffusion-policy control, trust region continual learning achieves the best final performance and retention, and consistently recovers early-task performance faster than EWC, replay, and continual meta-learning baselines.", "AI": {"tldr": "\u63d0\u51fa\u4fe1\u4efb\u57df\u6301\u7eed\u5b66\u4e60(TRCL)\uff0c\u7ed3\u5408\u751f\u6210\u56de\u653e\u4e0eFisher\u5ea6\u91cf\u7ea6\u675f\uff0c\u5728\u6269\u6563\u751f\u6210\u548c\u7b56\u7565\u63a7\u5236\u4efb\u52a1\u4e0a\u5b9e\u73b0\u6700\u4f73\u6027\u80fd\u4fdd\u6301\u4e0e\u5feb\u901f\u6062\u590d\uff0c\u65e0\u9700\u663e\u5f0f\u53cc\u5c42\u4f18\u5316\u5373\u53ef\u5b9e\u73b0\u7c7b\u4f3cMAML\u7684\u5feb\u901f\u91cd\u6536\u655b\u3002", "motivation": "\u6807\u51c6\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u6838\u5fc3\u6743\u8861\uff1a\u57fa\u4e8e\u6b63\u5219\u5316\u7684\u65b9\u6cd5(EWC)\u5728\u4efb\u52a1\u6700\u4f18\u89e3\u91cd\u53e0\u5ea6\u4f4e\u65f6\u8fc7\u5ea6\u7ea6\u675f\u66f4\u65b0\uff0c\u800c\u57fa\u4e8e\u56de\u653e\u7684\u65b9\u6cd5\u56e0\u4e0d\u5b8c\u7f8e\u56de\u653e\u5bfc\u81f4\u6027\u80fd\u6f02\u79fb\u3002\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u907f\u514d\u8fc7\u5ea6\u7ea6\u675f\u548c\u6f02\u79fb\u7684\u6df7\u5408\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4fe1\u4efb\u57df\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u751f\u6210\u56de\u653e\u4e0eFisher\u5ea6\u91cf\u4fe1\u4efb\u57df\u7ea6\u675f\u7ed3\u5408\u3002\u5728\u5c40\u90e8\u8fd1\u4f3c\u4e0b\uff0c\u8be5\u66f4\u65b0\u5448\u73b0MAML\u5f0f\u89e3\u91ca\uff1a\u56de\u653e\u63d0\u4f9b\u65e7\u4efb\u52a1\u68af\u5ea6\u4fe1\u53f7(\u7c7b\u4f3c\u67e5\u8be2)\uff0cFisher\u52a0\u6743\u60e9\u7f5a\u63d0\u4f9b\u79bb\u7ebf\u66f2\u7387\u5851\u9020(\u7c7b\u4f3c\u652f\u6301)\uff0c\u5f62\u6210\u5355\u9690\u5185\u6b65\u7684\u5143\u5b66\u4e60\u673a\u5236\u3002", "result": "\u5728\u4efb\u52a1\u589e\u91cf\u5f0f\u6269\u6563\u56fe\u50cf\u751f\u6210\u548c\u6301\u7eed\u6269\u6563\u7b56\u7565\u63a7\u5236\u5b9e\u9a8c\u4e2d\uff0c\u4fe1\u4efb\u57df\u6301\u7eed\u5b66\u4e60\u53d6\u5f97\u4e86\u6700\u4f73\u6700\u7ec8\u6027\u80fd\u548c\u4fdd\u6301\u7387\uff0c\u4e14\u6bd4EWC\u3001\u56de\u653e\u548c\u6301\u7eed\u5143\u5b66\u4e60\u57fa\u7ebf\u66f4\u5feb\u6062\u590d\u65e9\u671f\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4ea7\u751f\u6d8c\u73b0\u5143\u5b66\u4e60\u7279\u6027\uff1a\u6a21\u578b\u6210\u4e3a\u80fd\u5feb\u901f\u91cd\u6536\u655b\u5230\u5148\u524d\u4efb\u52a1\u6700\u4f18\u89e3\u7684\u521d\u59cb\u5316\uff0c\u65e0\u9700\u663e\u5f0f\u4f18\u5316\u53cc\u5c42\u76ee\u6807\u5373\u53ef\u6709\u6548\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\uff0c\u4e3a\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2602.02425", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2602.02425", "abs": "https://arxiv.org/abs/2602.02425", "authors": ["Amaru Caceres Arroyo", "Lea Bogensperger", "Ahmed Allam", "Michael Krauthammer", "Konrad Schindler", "Dominik Narnhofer"], "title": "Repurposing Protein Language Models for Latent Flow-Based Fitness Optimization", "comment": null, "summary": "Protein fitness optimization is challenged by a vast combinatorial landscape where high-fitness variants are extremely sparse. Many current methods either underperform or require computationally expensive gradient-based sampling. We present CHASE, a framework that repurposes the evolutionary knowledge of pretrained protein language models by compressing their embeddings into a compact latent space. By training a conditional flow-matching model with classifier-free guidance, we enable the direct generation of high-fitness variants without predictor-based guidance during the ODE sampling steps. CHASE achieves state-of-the-art performance on AAV and GFP protein design benchmarks. Finally, we show that bootstrapping with synthetic data can further enhance performance in data-constrained settings.", "AI": {"tldr": "\u63d0\u51faCHASE\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u9884\u8bad\u7ec3\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u7684\u5d4c\u5165\u538b\u7f29\u5230\u7d27\u51d1\u6f5c\u7a7a\u95f4\uff0c\u5e76\u8bad\u7ec3\u6761\u4ef6\u6d41\u5339\u914d\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u6548\u751f\u6210\u9ad8\u9002\u5e94\u5ea6\u86cb\u767d\u8d28\u53d8\u4f53\u3002", "motivation": "\u86cb\u767d\u8d28\u9002\u5e94\u5ea6\u4f18\u5316\u9762\u4e34\u5de8\u5927\u7ec4\u5408\u7a7a\u95f4\u7684\u6311\u6218\uff0c\u9ad8\u9002\u5e94\u5ea6\u53d8\u4f53\u6781\u4e3a\u7a00\u758f\u3002\u73b0\u6709\u65b9\u6cd5\u8868\u73b0\u4e0d\u4f73\u6216\u9700\u8981\u8ba1\u7b97\u6602\u8d35\u7684\u57fa\u4e8e\u68af\u5ea6\u7684\u91c7\u6837\u3002", "method": "CHASE\u6846\u67b6\u5229\u7528\u9884\u8bad\u7ec3\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u5316\u77e5\u8bc6\uff0c\u5c06\u5176\u5d4c\u5165\u538b\u7f29\u5230\u7d27\u51d1\u6f5c\u7a7a\u95f4\uff0c\u901a\u8fc7\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u8bad\u7ec3\u6761\u4ef6\u6d41\u5339\u914d\u6a21\u578b\uff0c\u5728ODE\u91c7\u6837\u6b65\u9aa4\u4e2d\u65e0\u9700\u57fa\u4e8e\u9884\u6d4b\u5668\u7684\u5f15\u5bfc\u5373\u53ef\u76f4\u63a5\u751f\u6210\u9ad8\u9002\u5e94\u5ea6\u53d8\u4f53\u3002", "result": "\u5728AAV\u548cGFP\u86cb\u767d\u8d28\u8bbe\u8ba1\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u5728\u6570\u636e\u53d7\u9650\u573a\u666f\u4e0b\uff0c\u4f7f\u7528\u5408\u6210\u6570\u636e\u8fdb\u884c\u81ea\u4e3e\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2602.02427", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02427", "abs": "https://arxiv.org/abs/2602.02427", "authors": ["Qihao Wen", "Jiahao Wang", "Yang Nan", "Pengfei He", "Ravi Tandon", "Han Xu"], "title": "Embedding Perturbation may Better Reflect the Uncertainty in LLM Reasoning", "comment": null, "summary": "Large language Models (LLMs) have achieved significant breakthroughs across diverse domains; however, they can still produce unreliable or misleading outputs. For responsible LLM application, Uncertainty Quantification (UQ) techniques are used to estimate a model's uncertainty about its outputs, indicating the likelihood that those outputs may be problematic. For LLM reasoning tasks, it is essential to estimate the uncertainty not only for the final answer, but also for the intermediate steps of the reasoning, as this can enable more fine-grained and targeted interventions. In this study, we explore what UQ metrics better reflect the LLM's ``intermediate uncertainty''during reasoning. Our study reveals that an LLMs' incorrect reasoning steps tend to contain tokens which are highly sensitive to the perturbations on the preceding token embeddings. In this way, incorrect (uncertain) intermediate steps can be readily identified using this sensitivity score as guidance in practice. In our experiments, we show such perturbation-based metric achieves stronger uncertainty quantification performance compared with baseline methods such as token (generation) probability and token entropy. Besides, different from approaches that rely on multiple sampling, the perturbation-based metrics offer better simplicity and efficiency.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6270\u52a8\u7684\u4e2d\u95f4\u6b65\u9aa4\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u91cf\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6807\u8bb0\u5bf9\u524d\u5e8f\u5d4c\u5165\u6270\u52a8\u7684\u654f\u611f\u5ea6\u6765\u8bc6\u522bLLM\u7684\u9519\u8bef\u63a8\u7406\u6b65\u9aa4\uff0c\u8be5\u65b9\u6cd5\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6027\u80fd\u66f4\u5f3a\u4e14\u66f4\u9ad8\u6548\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u4ea7\u751f\u4e0d\u53ef\u9760\u8f93\u51fa\uff0c\u8d23\u4efb\u5e94\u7528\u9700\u8981\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002\u5bf9\u4e8e\u63a8\u7406\u4efb\u52a1\uff0c\u4e0d\u4ec5\u8981\u8bc4\u4f30\u6700\u7ec8\u7b54\u6848\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u8fd8\u9700\u8bc4\u4f30\u4e2d\u95f4\u6b65\u9aa4\u7684\u4e0d\u786e\u5b9a\u6027\u4ee5\u5b9e\u73b0\u66f4\u7ec6\u7c92\u5ea6\u7684\u5e72\u9884\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u6b64\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u6d4b\u91cf\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6807\u8bb0\u5bf9\u524d\u5e8f\u6807\u8bb0\u5d4c\u5165\u6270\u52a8\u7684\u654f\u611f\u5ea6\u6765\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002\u5c06\u8fd9\u79cd\u654f\u611f\u5ea6\u5206\u6570\u4f5c\u4e3a\u6307\u5bfc\uff0c\u8bc6\u522b\u4e0d\u786e\u5b9a\u7684\u4e2d\u95f4\u6b65\u9aa4\u3002", "result": "\u57fa\u4e8e\u6270\u52a8\u7684\u5ea6\u91cf\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff08\u6807\u8bb0\u751f\u6210\u6982\u7387\u548c\u6807\u8bb0\u71b5\uff09\uff0c\u4e14\u76f8\u6bd4\u4f9d\u8d56\u591a\u6b21\u91c7\u6837\u7684\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u7b80\u6d01\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u57fa\u4e8e\u6270\u52a8\u7684\u654f\u611f\u5ea6\u5206\u6570\u662f\u4e00\u79cd\u6709\u6548\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u7528\u4e8e\u8bc6\u522bLLM\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u786e\u5b9a\u4e2d\u95f4\u6b65\u9aa4\uff0c\u4ece\u800c\u652f\u6301\u66f4\u6709\u9488\u5bf9\u6027\u7684\u5e72\u9884\u548c\u8d23\u4efbAI\u5e94\u7528\u3002"}}
{"id": "2602.02432", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.02432", "abs": "https://arxiv.org/abs/2602.02432", "authors": ["Jack M. Buckingham", "Ivo Couckuyt", "Juergen Branke"], "title": "Maximizing Reliability with Bayesian Optimization", "comment": "25 pages, 9 figures", "summary": "Bayesian optimization (BO) is a popular, sample-efficient technique for expensive, black-box optimization. One such problem arising in manufacturing is that of maximizing the reliability, or equivalently minimizing the probability of a failure, of a design which is subject to random perturbations - a problem that can involve extremely rare failures ($P_\\mathrm{fail} = 10^{-6}-10^{-8}$). In this work, we propose two BO methods based on Thompson sampling and knowledge gradient, the latter approximating the one-step Bayes-optimal policy for minimizing the logarithm of the failure probability. Both methods incorporate importance sampling to target extremely small failure probabilities. Empirical results show the proposed methods outperform existing methods in both extreme and non-extreme regimes.", "AI": {"tldr": "This paper proposes two Bayesian optimization methods combining Thompson sampling and knowledge gradient with importance sampling to efficiently optimize extremely rare failure probabilities (10^-6 to 10^-8) in manufacturing design reliability.", "motivation": "Manufacturing design requires optimizing reliability under random perturbations where failures are extremely rare (P_fail = 10^-6-10^-8), necessitating sample-efficient methods for expensive black-box functions.", "method": "Two Bayesian optimization approaches: Thompson sampling and knowledge gradient (which approximates the one-step Bayes-optimal policy for minimizing log failure probability), both enhanced with importance sampling to target small probabilities.", "result": "Empirical evaluation demonstrates the proposed methods outperform existing methods across both extreme and non-extreme failure probability regimes.", "conclusion": "Integrating importance sampling with Bayesian optimization provides an effective solution for rare event optimization in manufacturing, achieving superior performance."}}
{"id": "2602.02443", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02443", "abs": "https://arxiv.org/abs/2602.02443", "authors": ["Yuanteng Chen", "Peisong Wang", "Nanxin Zeng", "Yuantian Shao", "Gang Li", "Jing Liu", "Jian Cheng"], "title": "Certain Head, Uncertain Tail: Expert-Sample for Test-Time Scaling in Fine-Grained MoE", "comment": "24 pages, 13 figures", "summary": "Test-time scaling improves LLM performance by generating multiple candidate solutions, yet token-level sampling requires temperature tuning that trades off diversity against stability. Fine-grained MoE, featuring hundreds of well-trained experts per layer and multi-expert activation per token, offers an unexplored alternative through its rich routing space. We empirically characterize fine-grained MoE routing and uncover an informative pattern: router scores exhibit a certain head of high-confidence experts followed by an uncertain tail of low-confidence candidates. While single-run greedy accuracy remains stable when fewer experts are activated, multi-sample pass@n degrades significantly-suggesting that the certain head governs core reasoning capability while the uncertain tail correlates with reasoning diversity. Motivated by these findings, we propose Expert-Sample, a training-free method that preserves high-confidence selections while injecting controlled stochasticity into the uncertain tail, enabling diverse generation without destabilizing outputs. Evaluated on multiple fine-grained MoE models across math, knowledge reasoning, and code tasks, Expert-Sample consistently improves pass@n and verification-based accuracy. On Qwen3-30B-A3B-Instruct evaluated on GPQA-Diamond with 32 parallel samples, pass@32 rises from 85.4% to 91.9%, and accuracy improves from 59.1% to 62.6% with Best-of-N verification.", "AI": {"tldr": "\u63d0\u51faExpert-Sample\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fdd\u7559\u7ec6\u7c92\u5ea6MoE\u8def\u7531\u4e2d\u7684\u9ad8\u7f6e\u4fe1\u5ea6\u4e13\u5bb6\u9009\u62e9\uff0c\u540c\u65f6\u5728\u4f4e\u7f6e\u4fe1\u5ea6\u5c3e\u90e8\u6ce8\u5165\u53d7\u63a7\u968f\u673a\u6027\uff0c\u5b9e\u73b0\u5728\u4e0d\u7834\u574f\u8f93\u51fa\u7a33\u5b9a\u6027\u7684\u524d\u63d0\u4e0b\u63d0\u5347LLM\u6d4b\u8bd5\u65f6\u6269\u5c55\u7684\u63a8\u7406\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\uff08\u5982\u591a\u5019\u9009\u751f\u6210\uff09\u9700\u8981\u8c03\u6574\u6e29\u5ea6\u53c2\u6570\u6765\u5e73\u8861\u591a\u6837\u6027\u4e0e\u7a33\u5b9a\u6027\uff0c\u800c\u7ec6\u7c92\u5ea6MoE\u67b6\u6784\u7684\u8def\u7531\u7a7a\u95f4\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u7814\u7a76\u53d1\u73b0MoE\u8def\u7531\u5206\u6570\u5448\u73b0\"\u9ad8\u7f6e\u4fe1\u5ea6\u5934\u90e8+\u4e0d\u786e\u5b9a\u5c3e\u90e8\"\u7684\u6a21\u5f0f\uff0c\u5934\u90e8\u51b3\u5b9a\u6838\u5fc3\u63a8\u7406\u80fd\u529b\uff0c\u5c3e\u90e8\u5173\u8054\u63a8\u7406\u591a\u6837\u6027\u3002", "method": "\u57fa\u4e8e\u7ec6\u7c92\u5ea6MoE\u8def\u7531\u7279\u5f81\u7684\u8bad\u7ec3\u65e0\u5173\u65b9\u6cd5\uff1a1) \u4fdd\u7559\u9ad8\u7f6e\u4fe1\u5ea6\u4e13\u5bb6\u9009\u62e9\uff1b2) \u5728\u4e0d\u786e\u5b9a\u5c3e\u90e8\u6ce8\u5165\u53ef\u63a7\u968f\u673a\u6027\uff1b3) \u5b9e\u73b0\u591a\u6837\u751f\u6210\u800c\u4e0d\u7834\u574f\u8f93\u51fa\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u6570\u5b66\u3001\u77e5\u8bc6\u63a8\u7406\u548c\u4ee3\u7801\u4efb\u52a1\u4e0a\u6301\u7eed\u6539\u8fdbpass@n\u548c\u9a8c\u8bc1\u51c6\u786e\u7387\u3002\u4f8b\u5982Qwen3-30B-A3B-Instruct\u5728GPQA-Diamond\u4e0a\uff0cpass@32\u4ece85.4%\u63d0\u5347\u81f391.9%\uff0cBest-of-N\u9a8c\u8bc1\u51c6\u786e\u7387\u4ece59.1%\u63d0\u5347\u81f362.6%\u3002", "conclusion": "\u7ec6\u7c92\u5ea6MoE\u7684\u8def\u7531\u7a7a\u95f4\u4e3a\u6d4b\u8bd5\u65f6\u6269\u5c55\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0cExpert-Sample\u65b9\u6cd5\u6709\u6548\u5229\u7528\u4e86\u8def\u7531\u7f6e\u4fe1\u5ea6\u5206\u5e03\u7279\u5f81\uff0c\u4e3aLLM\u63a8\u7406\u591a\u6837\u6027\u63d0\u5347\u63d0\u4f9b\u4e86\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02445", "categories": ["cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.02445", "abs": "https://arxiv.org/abs/2602.02445", "authors": ["Seo Taek Kong", "R. Srikant"], "title": "Finite-Sample Wasserstein Error Bounds and Concentration Inequalities for Nonlinear Stochastic Approximation", "comment": null, "summary": "This paper derives non-asymptotic error bounds for nonlinear stochastic approximation algorithms in the Wasserstein-$p$ distance. To obtain explicit finite-sample guarantees for the last iterate, we develop a coupling argument that compares the discrete-time process to a limiting Ornstein-Uhlenbeck process. Our analysis applies to algorithms driven by general noise conditions, including martingale differences and functions of ergodic Markov chains. Complementing this result, we handle the convergence rate of the Polyak-Ruppert average through a direct analysis that applies under the same general setting.\n  Assuming the driving noise satisfies a non-asymptotic central limit theorem, we show that the normalized last iterates converge to a Gaussian distribution in the $p$-Wasserstein distance at a rate of order $\u03b3_n^{1/6}$, where $\u03b3_n$ is the step size. Similarly, the Polyak-Ruppert average is shown to converge in the Wasserstein distance at a rate of order $n^{-1/6}$. These distributional guarantees imply high-probability concentration inequalities that improve upon those derived from moment bounds and Markov's inequality. We demonstrate the utility of this approach by considering two applications: (1) linear stochastic approximation, where we explicitly quantify the transition from heavy-tailed to Gaussian behavior of the iterates, thereby bridging the gap between recent finite-sample analyses and asymptotic theory and (2) stochastic gradient descent, where we establish rate of convergence to the central limit theorem.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4e3a\u975e\u7ebf\u6027\u968f\u673a\u903c\u8fd1\u7b97\u6cd5\u5728Wasserstein-p\u8ddd\u79bb\u4e0b\u5efa\u7acb\u4e86\u975e\u6e10\u8fd1\u8bef\u5dee\u754c\uff0c\u901a\u8fc7\u8026\u5408\u65b9\u6cd5\u5206\u6790\u6700\u540e\u8fed\u4ee3\u70b9\u4e0ePolyak-Ruppert\u5e73\u5747\u7684\u6709\u9650\u6837\u672c\u6536\u655b\u901f\u7387\uff0c\u5e76\u5728\u975e\u6e10\u8fd1\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\u6761\u4ef6\u4e0b\u83b7\u5f97\u9ad8\u65af\u5206\u5e03\u7684\u6536\u655b\u4fdd\u8bc1\u3002", "motivation": "\u73b0\u6709\u968f\u673a\u903c\u8fd1\u7b97\u6cd5\u7814\u7a76\u591a\u4f9d\u8d56\u6e10\u8fd1\u7406\u8bba\u6216\u5bbd\u677e\u7684\u6982\u7387\u4e0d\u7b49\u5f0f\uff0c\u7f3a\u4e4f\u9488\u5bf9\u975e\u7ebf\u6027\u6a21\u578b\u5728\u4e00\u822c\u566a\u58f0\u6761\u4ef6\u4e0b\u7684\u7cbe\u786e\u6709\u9650\u6837\u672c\u4fdd\u8bc1\uff0c\u5c24\u5176\u9700\u8981\u91cf\u5316\u6700\u540e\u8fed\u4ee3\u70b9\u4e0e\u5e73\u5747\u8fed\u4ee3\u70b9\u7684\u5206\u5e03\u6536\u655b\u884c\u4e3a\u3002", "method": "1) \u91c7\u7528\u8026\u5408\u6280\u672f\u5206\u6790\u79bb\u6563\u8fc7\u7a0b\u4e0eOrnstein-Uhlenbeck\u6781\u9650\u8fc7\u7a0b\u7684\u5dee\u5f02\uff1b2) \u5bf9Polyak-Ruppert\u5e73\u5747\u8fdb\u884c\u76f4\u63a5\u5206\u6790\uff1b3) \u5728\u975e\u6e10\u8fd1\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\u6846\u67b6\u4e0b\u5efa\u7acbWasserstein\u8ddd\u79bb\u6536\u655b\u901f\u7387\u3002", "result": "1) \u6700\u540e\u8fed\u4ee3\u70b9\u4ee5\u03b3_n^{1/6}\u901f\u7387\u6536\u655b\u4e8e\u9ad8\u65af\u5206\u5e03\uff1b2) Polyak-Ruppert\u5e73\u5747\u4ee5n^{-1/6}\u901f\u7387\u6536\u655b\uff1b3) \u5728\u4e00\u822c\u566a\u58f0\u6761\u4ef6\u4e0b\uff08\u9785\u5dee/\u904d\u5386\u9a6c\u5c14\u53ef\u592b\u94fe\uff09\u83b7\u5f97\u663e\u5f0f\u6709\u9650\u6837\u672c\u754c\uff1b4) \u5e94\u7528\u4e8e\u7ebf\u6027SA\u548cSGD\u65f6\u63ed\u793a\u91cd\u5c3e\u5230\u9ad8\u65af\u5206\u5e03\u7684\u8fc7\u6e21\u73b0\u8c61\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7a81\u7834\u4e86\u4f20\u7edf\u77e9\u4e0d\u7b49\u5f0f\u5206\u6790\u7684\u5c40\u9650\uff0c\u4e3a\u968f\u673a\u903c\u8fd1\u7b97\u6cd5\u63d0\u4f9b\u4e86\u66f4\u7d27\u7684\u9ad8\u6982\u7387\u96c6\u4e2d\u4e0d\u7b49\u5f0f\uff0c\u5f25\u5408\u4e86\u6709\u9650\u6837\u672c\u5206\u6790\u4e0e\u6e10\u8fd1\u7406\u8bba\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u7279\u522b\u9002\u7528\u4e8e\u91cd\u5c3e\u566a\u58f0\u548c\u590d\u6742\u4f9d\u8d56\u7ed3\u6784\u573a\u666f\u3002"}}
{"id": "2602.02458", "categories": ["cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2602.02458", "abs": "https://arxiv.org/abs/2602.02458", "authors": ["Mingwei Hong", "Zheng Lin", "Zehang Lin", "Lin Li", "Miao Yang", "Xia Du", "Zihan Fang", "Zhaolu Kang", "Dianxin Luan", "Shunzhi Zhu"], "title": "Conflict-Aware Client Selection for Multi-Server Federated Learning", "comment": "6 pages, 4 figures", "summary": "Federated learning (FL) has emerged as a promising distributed machine learning (ML) that enables collaborative model training across clients without exposing raw data, thereby preserving user privacy and reducing communication costs. Despite these benefits, traditional single-server FL suffers from high communication latency due to the aggregation of models from a large number of clients. While multi-server FL distributes workloads across edge servers, overlapping client coverage and uncoordinated selection often lead to resource contention, causing bandwidth conflicts and training failures. To address these limitations, we propose a decentralized reinforcement learning with conflict risk prediction, named RL CRP, to optimize client selection in multi-server FL systems. Specifically, each server estimates the likelihood of client selection conflicts using a categorical hidden Markov model based on its sparse historical client selection sequence. Then, a fairness-aware reward mechanism is incorporated to promote long-term client participation for minimizing training latency and resource contention. Extensive experiments demonstrate that the proposed RL-CRP framework effectively reduces inter-server conflicts and significantly improves training efficiency in terms of convergence speed and communication cost.", "AI": {"tldr": "\u63d0\u51faRL-CRP\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4e0e\u51b2\u7a81\u98ce\u9669\u9884\u6d4b\u4f18\u5316\u591a\u670d\u52a1\u5668\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5ba2\u6237\u7aef\u9009\u62e9\uff0c\u964d\u4f4e\u8d44\u6e90\u7ade\u4e89\u5e76\u63d0\u5347\u8bad\u7ec3\u6548\u7387", "motivation": "\u4f20\u7edf\u5355\u670d\u52a1\u5668\u8054\u90a6\u5b66\u4e60\u5b58\u5728\u9ad8\u901a\u4fe1\u5ef6\u8fdf\u95ee\u9898\uff0c\u591a\u670d\u52a1\u5668\u67b6\u6784\u53c8\u56e0\u5ba2\u6237\u7aef\u8986\u76d6\u91cd\u53e0\u548c\u7f3a\u4e4f\u534f\u8c03\u5bfc\u81f4\u8d44\u6e90\u7ade\u4e89\u4e0e\u8bad\u7ec3\u5931\u8d25\uff0c\u4e9f\u9700\u89e3\u51b3\u901a\u4fe1\u6548\u7387\u4e0e\u8d44\u6e90\u51b2\u7a81\u95ee\u9898", "method": "\u91c7\u7528\u57fa\u4e8e\u5206\u7c7b\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\u7684\u51b2\u7a81\u98ce\u9669\u9884\u6d4b\u673a\u5236\uff0c\u7ed3\u5408\u516c\u5e73\u6027\u5956\u52b1\u51fd\u6570\u4f18\u5316\u5ba2\u6237\u7aef\u9009\u62e9\u7b56\u7565\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u957f\u671f\u53c2\u4e0e\u6fc0\u52b1\u4e0e\u5ef6\u8fdf\u6700\u5c0f\u5316", "result": "\u663e\u8457\u51cf\u5c11\u670d\u52a1\u5668\u95f4\u51b2\u7a81\uff0c\u5728\u6536\u655b\u901f\u5ea6\u548c\u901a\u4fe1\u6210\u672c\u65b9\u9762\u5927\u5e45\u63d0\u5347\u8bad\u7ec3\u6548\u7387\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027", "conclusion": "RL-CRP\u6846\u67b6\u901a\u8fc7\u9884\u6d4b\u51b2\u7a81\u4e0e\u516c\u5e73\u5956\u52b1\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u591a\u670d\u52a1\u5668\u8054\u90a6\u5b66\u4e60\u7684\u8d44\u6e90\u7ade\u4e89\u95ee\u9898\uff0c\u4e3a\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.02472", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02472", "abs": "https://arxiv.org/abs/2602.02472", "authors": ["Qifan Yu", "Xinyu Ma", "Zhijian Zhuo", "Minrui Wang", "Deyi Liu", "Shiyi Zhan", "Yiyuan Ma", "Liang Xiang", "Xingyan Bin", "Di He"], "title": "SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning", "comment": null, "summary": "Progressive Learning (PL) reduces pre-training computational overhead by gradually increasing model scale. While prior work has extensively explored depth expansion, width expansion remains significantly understudied, with the few existing methods limited to the early stages of training. However, expanding width during the mid-stage is essential for maximizing computational savings, yet it remains a formidable challenge due to severe training instabilities. Empirically, we show that naive initialization at this stage disrupts activation statistics, triggering loss spikes, while copy-based initialization introduces gradient symmetry that hinders feature diversity. To address these issues, we propose SPARKLING (balancing {S}ignal {P}reservation {A}nd symmet{R}y brea{K}ing for width-progressive {L}earn{ING}), a novel framework for mid-stage width expansion. Our method achieves signal preservation via RMS-scale consistency, stabilizing activation statistics during expansion. Symmetry breaking is ensured through asymmetric optimizer state resetting and learning rate re-warmup. Extensive experiments on Mixture-of-Experts (MoE) models demonstrate that, across multiple width axes and optimizer families, SPARKLING consistently outperforms training from scratch and reduces training cost by up to 35% under $2\\times$ width expansion.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSPARKLING\u6846\u67b6\uff0c\u901a\u8fc7RMS-scale\u4e00\u81f4\u6027\u548c\u975e\u5bf9\u79f0\u4f18\u5316\u5668\u72b6\u6001\u91cd\u7f6e\u5b9e\u73b0\u4e2d\u671f\u5bbd\u5ea6\u6269\u5c55\uff0c\u5728MoE\u6a21\u578b\u4e0a\u76f8\u6bd4\u4ece\u5934\u8bad\u7ec3\u53ef\u964d\u4f4e\u9ad8\u8fbe35%\u7684\u8bad\u7ec3\u6210\u672c\u3002", "motivation": "\u6e10\u8fdb\u5f0f\u5b66\u4e60\u4e2d\u5bbd\u5ea6\u6269\u5c55\u7814\u7a76\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u4e2d\u671f\u6269\u5c55\u5bf9\u8282\u7701\u8ba1\u7b97\u6210\u672c\u81f3\u5173\u91cd\u8981\u4f46\u5b58\u5728\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u6734\u7d20\u521d\u59cb\u5316\u548c\u590d\u5236\u521d\u59cb\u5316\uff09\u5206\u522b\u5bfc\u81f4\u6fc0\u6d3b\u7edf\u8ba1\u91cf\u7d0a\u4e71\u548c\u68af\u5ea6\u5bf9\u79f0\u6027\u963b\u788d\u7279\u5f81\u591a\u6837\u6027\u3002", "method": "\u63d0\u51faSPARKLING\u6846\u67b6\uff0c\u91c7\u7528RMS-scale\u4e00\u81f4\u6027\u4fdd\u6301\u4fe1\u53f7\u7a33\u5b9a\u6fc0\u6d3b\u7edf\u8ba1\uff0c\u901a\u8fc7\u975e\u5bf9\u79f0\u4f18\u5316\u5668\u72b6\u6001\u91cd\u7f6e\u548c\u5b66\u4e60\u7387\u91cd\u9884\u70ed\u6253\u7834\u5bf9\u79f0\u6027\u3002", "result": "\u5728MoE\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSPARKLING\u5728\u591a\u79cd\u5bbd\u5ea6\u8f74\u548c\u4f18\u5316\u5668\u4e0a\u5747\u4f18\u4e8e\u4ece\u5934\u8bad\u7ec3\uff0c\u57282\u500d\u5bbd\u5ea6\u6269\u5c55\u4e0b\u53ef\u964d\u4f4e35%\u8bad\u7ec3\u6210\u672c\u3002", "conclusion": "SPARKLING\u6210\u529f\u89e3\u51b3\u4e86\u4e2d\u671f\u5bbd\u5ea6\u6269\u5c55\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u586b\u8865\u4e86\u6e10\u8fdb\u5f0f\u5b66\u4e60\u5728\u5bbd\u5ea6\u6269\u5c55\u65b9\u9762\u7684\u7814\u7a76\u7a7a\u767d\u3002"}}
{"id": "2602.02482", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02482", "abs": "https://arxiv.org/abs/2602.02482", "authors": ["Yuda Song", "Lili Chen", "Fahim Tajwar", "Remi Munos", "Deepak Pathak", "J. Andrew Bagnell", "Aarti Singh", "Andrea Zanette"], "title": "Expanding the Capabilities of Reinforcement Learning via Text Feedback", "comment": "43 pages, 6 figures", "summary": "The success of RL for LLM post-training stems from an unreasonably uninformative source: a single bit of information per rollout as binary reward or preference label. At the other extreme, distillation offers dense supervision but requires demonstrations, which are costly and difficult to scale. We study text feedback as an intermediate signal: richer than scalar rewards, yet cheaper than complete demonstrations. Textual feedback is a natural mode of human interaction and is already abundant in many real-world settings, where users, annotators, and automated judges routinely critique LLM outputs. Towards leveraging text feedback at scale, we formalize a multi-turn RL setup, RL from Text Feedback (RLTF), where text feedback is available during training but not at inference. Therefore, models must learn to internalize the feedback in order to improve their test-time single-turn performance. To do this, we propose two methods: Self Distillation (RLTF-SD), which trains the single-turn policy to match its own feedback-conditioned second-turn generations; and Feedback Modeling (RLTF-FM), which predicts the feedback as an auxiliary objective. We provide theoretical analysis on both methods, and empirically evaluate on reasoning puzzles, competition math, and creative writing tasks. Our results show that both methods consistently outperform strong baselines across benchmarks, highlighting the potential of RL with an additional source of rich supervision at scale.", "AI": {"tldr": "\u7814\u7a76\u6587\u672c\u53cd\u9988\u4f5c\u4e3aLLM\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4e2d\u95f4\u76d1\u7763\u4fe1\u53f7\uff0c\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5RLTF-SD\u548cRLTF-FM\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u73b0\u6709RL\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u6bd4\u7279\u5956\u52b1\u4fe1\u53f7\u8fc7\u4e8e\u7a00\u758f\uff0c\u800c\u84b8\u998f\u9700\u8981\u6602\u8d35\u7684\u4eba\u5de5\u6f14\u793a\uff1b\u6587\u672c\u53cd\u9988\u4f5c\u4e3a\u66f4\u4e30\u5bcc\u7684\u76d1\u7763\u4fe1\u53f7\uff0c\u65e2\u6709\u4fe1\u606f\u91cf\u53c8\u76f8\u5bf9\u5bb9\u6613\u83b7\u53d6", "method": "\u5f62\u5f0f\u5316\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u8bbe\u7f6eRLTF\uff0c\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1a1\uff09\u81ea\u6211\u84b8\u998f(RLTF-SD)\uff1a\u8bad\u7ec3\u5355\u8f6e\u7b56\u7565\u5339\u914d\u81ea\u8eab\u53cd\u9988\u6761\u4ef6\u4e0b\u7684\u7b2c\u4e8c\u8f6e\u751f\u6210\uff1b2\uff09\u53cd\u9988\u5efa\u6a21(RLTF-FM)\uff1a\u5c06\u9884\u6d4b\u53cd\u9988\u4f5c\u4e3a\u8f85\u52a9\u76ee\u6807", "result": "\u5728\u63a8\u7406\u8c1c\u9898\u3001\u7ade\u8d5b\u6570\u5b66\u548c\u521b\u610f\u5199\u4f5c\u4efb\u52a1\u4e0a\uff0c\u4e24\u79cd\u65b9\u6cd5\u5747\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u4e30\u5bcc\u76d1\u7763\u4fe1\u53f7\u7684\u6f5c\u529b", "conclusion": "\u6587\u672c\u53cd\u9988\u4f5c\u4e3a\u53ef\u6269\u5c55\u7684\u4e2d\u95f4\u76d1\u7763\u4fe1\u53f7\uff0c\u80fd\u6709\u6548\u63d0\u5347LLM\u7684\u63a8\u7406\u548c\u751f\u6210\u80fd\u529b\uff0c\u4e3a\u5927\u89c4\u6a21\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411"}}
{"id": "2602.02488", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02488", "abs": "https://arxiv.org/abs/2602.02488", "authors": ["Yinjie Wang", "Tianbao Xie", "Ke Shen", "Mengdi Wang", "Ling Yang"], "title": "RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System", "comment": "Code: https://github.com/Gen-Verse/Open-AgentRL", "summary": "We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. Code: https://github.com/Gen-Verse/Open-AgentRL", "AI": {"tldr": "RLAnything\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u95ed\u73af\u52a8\u6001\u4f18\u5316\u73af\u5883\u3001\u7b56\u7565\u548c\u5956\u52b1\u6a21\u578b\uff0c\u4e3a\u4efb\u610fLLM\u6216\u667a\u80fd\u4f53\u573a\u666f\u589e\u5f3a\u5b66\u4e60\u4fe1\u53f7\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728OSWorld\u3001AlfWorld\u548cLiveBench\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4e3a\u4efb\u610fLLM\u6216\u667a\u80fd\u4f53\u573a\u666f\u63d0\u4f9b\u4e00\u4e2a\u80fd\u591f\u52a8\u6001\u953b\u9020\u73af\u5883\u3001\u7b56\u7565\u548c\u5956\u52b1\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u95ed\u73af\u4f18\u5316\u653e\u5927\u5b66\u4e60\u4fe1\u53f7\u5e76\u5f3a\u5316\u6574\u4f53RL\u7cfb\u7edf\u6027\u80fd\u3002", "method": "1) \u7b56\u7565\u901a\u8fc7\u6b65\u9aa4\u7ea7\u548c\u7ed3\u679c\u7ea7\u4fe1\u53f7\u7684\u96c6\u6210\u53cd\u9988\u8fdb\u884c\u8bad\u7ec3\uff1b2) \u5956\u52b1\u6a21\u578b\u901a\u8fc7\u4e00\u81f4\u6027\u53cd\u9988\u8054\u5408\u4f18\u5316\uff0c\u8fdb\u800c\u6539\u8fdb\u7b56\u7565\u8bad\u7ec3\uff1b3) \u7406\u8bba\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u73af\u5883\u5229\u7528\u53cc\u65b9\u7684\u8bc4\u8bba\u5bb6\u53cd\u9988\uff0c\u5b9e\u73b0\u7ecf\u9a8c\u5b66\u4e60\u3002", "result": "\u6bcf\u4e2a\u65b0\u589e\u7ec4\u4ef6\u6301\u7eed\u6539\u8fdb\u7cfb\u7edf\u6027\u80fd\uff1aRLAnything\u5728\u5404\u9879\u4efb\u52a1\u4e2d\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u5305\u62ecQwen3-VL-8B-Thinking\u5728OSWorld\u4e0a\u63d0\u53479.1%\uff0cQwen2.5-7B-Instruct\u5728AlfWorld\u548cLiveBench\u4e0a\u5206\u522b\u63d0\u534718.7%\u548c11.9%\u3002\u4f18\u5316\u540e\u7684\u5956\u52b1\u6a21\u578b\u4fe1\u53f7\u4f18\u4e8e\u4eba\u7c7b\u6807\u6ce8\u7ed3\u679c\u3002", "conclusion": "RLAnything\u901a\u8fc7\u95ed\u73af\u4f18\u5316\u73af\u5883\u3001\u7b56\u7565\u548c\u5956\u52b1\u6a21\u578b\uff0c\u4e3aLLM\u548c\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2602.02494", "categories": ["cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2602.02494", "abs": "https://arxiv.org/abs/2602.02494", "authors": ["Dulhan Jayalath", "Oiwi Parker Jones"], "title": "MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training", "comment": "19 pages, 8 figures, 5 tables", "summary": "Clinical brain-to-text interfaces are designed for paralysed patients who cannot provide extensive training recordings. Pre-training improves data-efficient generalisation by learning statistical priors across subjects, but these priors critically depend on context. While natural speech might unfold gradually over minutes, most methods pre-train with only a few seconds of context. Thus, we propose MEG-XL, a model pre-trained with 2.5 minutes of MEG context per sample, 5-300x longer than prior work, and equivalent to 191k tokens, capturing extended neural context. Fine-tuning on the task of word decoding from brain data, MEG-XL matches supervised performance with a fraction of the data (e.g. 1hr vs 50hrs) and outperforms brain foundation models. We find that models pre-trained with longer contexts learn representations that transfer better to word decoding. Our results indicate that long-context pre-training helps exploit extended neural context that other methods unnecessarily discard. Code, model weights, and instructions are available at https://github.com/neural-processing-lab/MEG-XL .", "AI": {"tldr": "\u63d0\u51faMEG-XL\u6a21\u578b\uff0c\u901a\u8fc72.5\u5206\u949f\u8d85\u957f\u8111\u78c1\u56fe\u4e0a\u4e0b\u6587\u9884\u8bad\u7ec3\uff08191k tokens\uff09\uff0c\u4f7f\u762b\u75ea\u60a3\u8005\u8111\u673a\u63a5\u53e3\u5728\u4ec51\u5c0f\u65f6\u8bad\u7ec3\u6570\u636e\u4e0b\u8fbe\u523050\u5c0f\u65f6\u76d1\u7763\u5b66\u4e60\u7684\u89e3\u7801\u6027\u80fd\uff0c\u7a81\u7834\u73b0\u6709\u65b9\u6cd5\u79d2\u7ea7\u4e0a\u4e0b\u6587\u7684\u5c40\u9650", "motivation": "\u73b0\u6709\u4e34\u5e8a\u8111\u673a\u63a5\u53e3\u56e0\u762b\u75ea\u60a3\u8005\u65e0\u6cd5\u63d0\u4f9b\u5927\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u4e14\u9884\u8bad\u7ec3\u4ec5\u4f7f\u7528\u6570\u79d2\u4e0a\u4e0b\u6587\uff0c\u5ffd\u7565\u4e86\u81ea\u7136\u8bed\u8a00\u5c55\u5f00\u6240\u9700\u7684\u5206\u949f\u7ea7\u795e\u7ecf\u4e0a\u4e0b\u6587\uff0c\u5bfc\u81f4\u6570\u636e\u6548\u7387\u4f4e\u4e0b", "method": "\u8bbe\u8ba1MEG-XL\u6a21\u578b\uff0c\u5c06\u9884\u8bad\u7ec3\u4e0a\u4e0b\u6587\u5ef6\u957f\u81f32.5\u5206\u949f/\u6837\u672c\uff08\u76f8\u5f53\u4e8e191k tokens\uff09\uff0c\u6bd4\u5148\u524d\u5de5\u4f5c\u957f5-300\u500d\uff0c\u4ee5\u6355\u83b7\u6269\u5c55\u7684\u795e\u7ecf\u4e0a\u4e0b\u6587\u4fe1\u606f", "result": "\u5728\u5355\u8bcd\u89e3\u7801\u4efb\u52a1\u4e2d\uff0c\u4ec5\u97001\u5c0f\u65f6\u5fae\u8c03\u6570\u636e\u5373\u53ef\u8fbe\u523050\u5c0f\u65f6\u76d1\u7763\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u4e14\u663e\u8457\u4f18\u4e8e\u8111\u57fa\u7840\u6a21\u578b\uff1b\u957f\u4e0a\u4e0b\u6587\u9884\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u6a21\u578b\u8fc1\u79fb\u80fd\u529b", "conclusion": "\u957f\u4e0a\u4e0b\u6587\u9884\u8bad\u7ec3\u80fd\u6709\u6548\u5229\u7528\u88ab\u5176\u4ed6\u65b9\u6cd5\u4e22\u5f03\u7684\u6269\u5c55\u795e\u7ecf\u4e0a\u4e0b\u6587\uff0c\u6781\u5927\u63d0\u5347\u8111\u673a\u63a5\u53e3\u7684\u6570\u636e\u6548\u7387\uff0c\u4e3a\u4e34\u5e8a\u90e8\u7f72\u63d0\u4f9b\u53ef\u884c\u8def\u5f84"}}
