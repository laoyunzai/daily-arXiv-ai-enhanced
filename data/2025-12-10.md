<div id=toc></div>

# Table of Contents

- [nlin.CD](#nlin.CD) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 2]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 9]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 8]
- [cs.LG](#cs.LG) [Total: 56]
- [cs.AI](#cs.AI) [Total: 23]
- [quant-ph](#quant-ph) [Total: 49]


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [1] [Transcript-based estimators for characterizing interactions](https://arxiv.org/abs/2512.08570)
*Manuel Adams,José M. Amigó,Klaus Lehnertz*

Main category: nlin.CD

TL;DR: 转录本概念用于分析时间序列间的相互作用关系，基于序数模式的代数关系，可估计相互作用的强度、方向和复杂性。本文重新审视该方法，展示其在耦合动力系统和人类大脑动态分析中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 转录本概念自2009年提出以来，虽然提供了分析时间序列相互作用的方法，但尚未在真实世界系统的研究中得到广泛应用。本文旨在重新审视这一概念，展示其在复杂系统相互作用分析中的实用价值。

Method: 基于转录本概念，利用时间序列导出的序数模式之间的代数关系，构建相互作用强度、方向和复杂性的估计器。通过耦合动力系统的范例分析，以及多通道、多天记录的人类大脑动态数据的时间分辨分析来验证方法。

Result: 转录本方法成功应用于不同复杂度的耦合动力系统分析，并在人类大脑动态研究中展现出潜力，能够提供关于不同警觉状态下大脑复杂时空相互作用的新见解。

Conclusion: 转录本方法为分析复杂系统间的相互作用提供了有效的工具，特别是在人类大脑动态研究中具有重要应用价值，能够揭示不同警觉状态下大脑时空相互作用的复杂模式。

Abstract: The concept of transcripts was introduced in 2009 as a means to characterize various aspects of the functional relationship between time series of interacting systems. Based on this concept that utilizes algebraic relations between ordinal patterns derived from time series, estimators for the strength, direction, and complexity of interactions have been introduced. These estimators, however, have not yet found widespread application in studies of interactions between real-world systems. Here, we revisit the concept of transcripts and showcase the usage of transcript-based estimators for a time-series-based investigation of interactions between coupled paradigmatic dynamical systems of varying complexity. At the example of a time-resolved analysis of multichannel and multiday recordings of ongoing human brain dynamics, we demonstrate the potential of the methods to provide novel insights into the intricate spatial-temporal interactions in the human brain underlying different vigilance states.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [2] [Effect of superconductivity on non-uniform magnetization in dirty SF junctions](https://arxiv.org/abs/2512.08044)
*A. V. Levin,P. M. Ostrovsky*

Main category: cond-mat.dis-nn

TL;DR: 研究超导体与铁磁体隧道结中的邻近效应，发现超导库珀对穿透会破坏铁磁体的均匀磁序，导致从均匀到非均匀磁态的相变


<details>
  <summary>Details</summary>
Motivation: 研究超导体与铁磁体界面处的邻近效应如何影响铁磁体的磁序稳定性，探索超导-铁磁异质结中的新奇物理现象

Method: 使用准经典Usadel方程推导相变的Landau泛函，构建完整的相图，分析共振点特性和强非均匀磁态的性质

Result: 发现存在从均匀到非均匀磁态的二级相变，识别出共振点（邻近效应特征能标等于铁磁体交换场），此时均匀磁态即使在大的刚度极限下也不稳定

Conclusion: 超导邻近效应能显著改变铁磁体的磁序结构，导致复杂的相变行为，为超导-铁磁异质结的设计和应用提供了理论基础

Abstract: We study proximity effect in a tunnel junction between a bulk superconductor and a thin disordered ferromagnetic layer on its surface. Cooper pairs penetrating from the superconductor into the ferromagnet tend to destabilize its uniform magnetic order. The competition of this effect and the intrinsic magnetic stiffness of the ferromagnet leads to a second order phase transition between uniform and non-uniform magnetic states. Using the quasiclassical Usadel equation, we derive the Landau functional for this transition and construct the complete phase diagram of the effect. We identify a special point of "resonance" at which the characteristic energy scale of the proximity effect equals the exchange field of the ferromagnet. At this point, the uniform magnetic state is unstable even in the limit of large stiffness. We further explore the parameter regime far beyond the transition and determine the properties of the resulting strongly non-uniform magnetic state.

</details>


### [3] [Order parameter for non-mean-field spin glasses](https://arxiv.org/abs/2512.08691)
*Michele Castellana*

Main category: cond-mat.dis-nn

TL;DR: 提出了一种基于最小化原则的自发涌现序参量的重整化群方法，用于非平均场自旋玻璃模型，在分层晶格上验证了临界指数预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统重整化群方法通常基于对系统的先验假设，本文旨在发展一种不依赖先验假设、基于最小化原则的重整化群方法，以更好地理解现实自旋玻璃系统的临界有序性。

Method: 采用基于信息论的最小化原则，在分层晶格自旋玻璃模型上实施重整化群方法。通过去耦过程，从系统对称性和重整化变换的自相似性中自发涌现出新的序参量——自旋构型在系统基态上的投影。用这种新序参量取代铁磁系统中的Kadanoff多数规则，基态作为模式在不同长度尺度间转换自旋构型。

Result: 在上临界维度以下，对描述关联长度临界发散的临界指数ν的预测与本文及先前研究的数值模拟结果高度一致。该方法可应用于立方晶格和最近邻耦合的自旋玻璃模型，直接模拟AuFe、CuMn等磁性合金材料。

Conclusion: 这项研究为理解现实自旋玻璃的临界有序性开辟了新途径，提出的基于最小化原则的重整化群方法能够自发涌现序参量，为自旋玻璃材料的建模提供了新工具。

Abstract: We propose a novel renormalization group (RG) method for non mean-field models of spin glasses, which leads to the emergence of a novel order parameter. Unlike previous approaches where the RG procedure is based on a priori notions on the system, our analysis follows a minimality principle, where no a priori assumption is made. We apply our approach to a spin-glass model built on a hierarchical lattice. In the RG decimation procedure, a novel order parameter spontaneously emerges from the system symmetries, and self-similarity features of the RG transformation only. This order parameter is the projection of the spin configurations on the ground state of the system. Kadanoff's majority rule for ferromagnetic systems is replaced by a more complex scheme, which involves such novel order parameter. The ground state thus acts as a pattern which translates spin configurations from one length scale to another. The rescaling RG procedure is based on a minimal, information-theory approach and, combined with the decimation, it yields a complete RG transformation.
  Below the upper critical dimension, the predictions for the critical exponent $ν$, which describes the critical divergence of the correlation length, are in excellent agreement with numerical simulations from both this and previous studies. Overall, this study opens new avenues in the understanding of the critical ordering of realistic spin glasses, and it can be applied to spin-glass models on a cubic lattice and nearest-neighbor couplings which directly model spin-glass materials, such as AuFe, CuMn and other magnetic alloys.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [4] [Monte Carlo simulation of spin-reorientation transition in weak ferrimagnets YFeCrO3](https://arxiv.org/abs/2512.07916)
*E. V. Vasinovich,V. A. Ulitko,A. S. Moskvin*

Main category: cond-mat.str-el

TL;DR: 使用经典蒙特卡洛方法模拟YFe1-xCrxO3混合正交铁氧体-正交铬酸盐中的磁性3d亚晶格，发现考虑混合成分中Dzyaloshinskii矢量的竞争时，会出现磁矩补偿和对应自旋重取向的角磁构型


<details>
  <summary>Details</summary>
Motivation: 研究混合正交铁氧体-正交铬酸盐YFe1-xCrxO3中的磁性行为，特别是Dzyaloshinskii矢量竞争对磁矩补偿和自旋重取向的影响

Method: 使用经典蒙特卡洛方法对磁性3d亚晶格进行建模，考虑混合成分中Dzyaloshinskii矢量的竞争效应

Result: 在考虑混合成分中Dzyaloshinskii矢量竞争的情况下，观察到磁矩补偿现象以及对应自旋重取向的角磁构型

Conclusion: 混合正交铁氧体-正交铬酸盐YFe1-xCrxO3中的Dzyaloshinskii矢量竞争是导致磁矩补偿和自旋重取向现象的关键机制

Abstract: This work presents the modeling of the magnetic 3d sublattice in mixed orthoferrites-orthochromites YFe1-xCrxO3 using classical Monte Carlo methods. It is shown that, when taking into account the competition of the Dzyaloshinskii vectors in the mixed compositions, magnetic moment compensations are observed, as well as angular magnetic configurations corresponding to the spin reorientation.

</details>


### [5] [Environment-matrix-product operator for boundary-free large-scale quantum many-body simulations](https://arxiv.org/abs/2512.07923)
*Souta Shimozono,Chisa Hotta*

Main category: cond-mat.str-el

TL;DR: 提出了一种替代无限密度矩阵重整化方法，通过构造环境矩阵乘积算子来模拟热力学极限下的量子多体态，消除有限尺寸效应


<details>
  <summary>Details</summary>
Motivation: 现有无限密度矩阵重整化方法在模拟热力学极限时存在局限性，需要一种能够在有限尺寸计算中忠实模拟热力学极限的新方法

Method: 构造半无限区域哈密顿量的环境矩阵乘积算子，从有限尺寸基态矩阵乘积态出发，通过收缩哈密顿量表示生成有效环境MPO，然后以递归方式将其附加到更新的有限系统中

Result: 该方法实现了无边界反射的超长实时动力学模拟，系统被驱动到具有可忽略有限尺寸效应的体态，且不需要均匀性假设

Conclusion: 该方法为在有限尺寸计算中访问热力学极限下的量子多体态提供了一种有效的替代方案，特别适用于非均匀系统的长时间动力学研究

Abstract: We propose an alternative to the infinite density-matrix renormalization approach for accessing quantum many-body states within a finite-size calculation that faithfully mimics the thermodynamic limit. Our method constructs environment matrix product operators (MPOs) representing the Hamiltonian of semi-infinite regions surrounding the target system. Starting from the finite-size ground-state MPS, we contract its Hamiltonian representation to generate effective environment MPOs, which are then attached to a renewed finite system in a recursive manner. This iterative embedding drives the system toward a bulk-like state with negligible finite-size effects. The scheme requires no assumption of homogeneity and achieves unprecedentedly long real-time dynamics free from boundary reflections.

</details>


### [6] [Is disorder a friend or a foe to melting of Wigner-Mott insulators?](https://arxiv.org/abs/2512.07932)
*Mohammed Hammam,Cyprian Lewandowski,Vladimir Dobrosavljevic,Sandeep Joy*

Main category: cond-mat.str-el

TL;DR: 无序环境显著稳定了二维维格纳晶体，使其能在更高温度和密度下存在，但同时也使熔化转变变得模糊，产生固液共存区域。


<details>
  <summary>Details</summary>
Motivation: 维格纳晶体极其脆弱，源于长程库仑相互作用导致的强几何阻挫。在平移不变系统中，剪切密度涨落是无能隙激发，具有很小的特征能量尺度。无序破坏了平移不变性，抑制了无能隙激发，从而稳定了维格纳晶格。

Method: 通过显式的微观模型计算来说明这一普遍原理，并分析无序对二维维格纳晶体熔化转变的影响。

Result: 无序能非常有效地稳定无序维格纳晶格，使其在比清洁极限下更高的温度和密度下存在。但在二维情况下，无序显著"模糊"了熔化转变，产生固态和液态区域的空间共存现象，这与最近的STM实验结果一致。

Conclusion: 研究结果为二维维格纳-莫特固体的熔化提供了新的物理图像，对应于具有空间变化局域电子带宽的莫特-哈伯德模型。

Abstract: Wigner crystals are extremely fragile, which is shown to result from very strong geometric frustration germane to long-range Coulomb interactions. Physically, this is manifested by a very small characteristic energy scale for shear density fluctuations, which are gapless excitations in a translationally invariant system. The presence of disorder, however, breaks translational invariance, thus suppressing gapless excitations and pushing them to higher density. We illustrate this general principle by explicit microscopic model calculations, showing that this mechanism very effectively stabilizes disordered Wigner lattices to much higher temperatures and densities than in the clean limit. On the other hand, we argue that in two dimensions disorder significantly ``smears" the melting transition, producing spatial coexistence of solid-like and liquid-like regions -- just as recently observed in STM experiments. Our results paint a new physical picture for melting of Wigner-Mott solids in two dimensions, corresponding to a Mott-Hubbard model with spatially varying local electronic bandwidth.

</details>


### [7] [Dynamics of Quantum Chiral Solitons](https://arxiv.org/abs/2512.08220)
*Leandro M. Chinellato,Oleg A. Starykh,Cristian D. Batista*

Main category: cond-mat.str-el

TL;DR: 该论文提出了一个非微扰框架，用于量化相互作用量子自旋链中的手性孤子，建立了正弦-戈登模型与Thirring模型之间S-对偶性的晶格扩展，揭示了手性孤子算符的非常规动力学及其在激发谱和关联函数中的表现。


<details>
  <summary>Details</summary>
Motivation: 研究动机是弥合连续场论对偶性与其晶格对应物之间的差距，为在凝聚态物理系统中实验探测量子场论的非微扰特征开辟途径。通过构建量子手性孤子算符，探索其在相互作用量子自旋链中的动力学行为。

Method: 方法包括：1）建立非微扰框架量化手性孤子；2）构建正弦-戈登模型与Thirring模型之间S-对偶性的直接晶格扩展；3）显式构造量子手性孤子算符；4）分析这些算符在激发谱和关联函数中的动力学表现；5）研究主导孤子隧穿振幅的符号交替特性。

Result: 主要结果：1）主导孤子隧穿振幅具有符号交替特性：sgn(t₁₊) = (-1)^{2S+1}，这明确区分了半奇数自旋链与整数自旋链；2）手性激发在动态自旋结构因子中具有特征性信号，可通过非弹性中子散射观测；3）建立了连续场论对偶性与晶格系统之间的桥梁。

Conclusion: 该研究为在凝聚态物理环境中实验探测对偶量子场论的非微扰特征开辟了新途径。通过显式构造量子手性孤子算符并揭示其动力学特性，建立了连续场论对偶性与晶格系统之间的直接联系，为实验观测提供了理论基础。

Abstract: We introduce a non-perturbative framework for quantizing chiral solitons in interacting quantum spin chains. This approach provides a direct lattice extension of the well-established $S$-duality between the sine-Gordon and Thirring models, thereby bridging the gap between continuum dualities and their lattice counterparts. By constructing the quantum chiral-soliton operators explicitly, we show how their unconventional dynamics appear in the excitation spectrum and correlation functions across the full Brillouin zone. A key result is that the dominant soliton tunneling amplitude alternates in sign, $\operatorname{sgn}(t_{1+}) = (-1)^{2S+1}$, sharply distinguishing half-odd-integer from integer spin chains. We further identify characteristic signatures of these chiral excitations in the dynamical spin structure factor, demonstrating their visibility in inelastic neutron scattering. Our results open a route to experimentally probing non-perturbative features of dual quantum field theories in condensed-matter settings.

</details>


### [8] [Topological spin-up triplet excitonic condensation in two-dimensional electron-hole systems](https://arxiv.org/abs/2512.08370)
*Van-Nham Phan*

Main category: cond-mat.str-el

TL;DR: 该研究探索了二维电子-空穴系统中拓扑自旋向上三重态激子凝聚及其与其他稳定性的竞争，考虑了Rashba自旋轨道耦合和外部磁场的影响。


<details>
  <summary>Details</summary>
Motivation: 研究拓扑激子凝聚相，特别是在考虑自旋轨道耦合和磁场条件下，探索自旋选择性激子凝聚的拓扑性质及其在真实材料系统中的实现途径。

Method: 采用无限制Hartree-Fock方法，自洽计算自旋选择性激子凝聚序参数和陈数，分析动态激子磁化率谱来揭示激子涨落特征。

Result: 在磁场和库仑相互作用依赖的基态相图中，发现了具有非零陈数的自旋向上三重态激子凝聚相，该相在拓扑平凡的单重态和自旋向下三重态激子凝聚区域之外独特出现。

Conclusion: 研究确立了一类由激子相干性驱动的拓扑量子相，并提出了在扭曲Janus单层过渡金属二硫化物或某些扭曲范德华异质结构中实现该相的现实途径。

Abstract: We investigate topological spin-up triplet excitonic condensation and its competition with other stabilities in a two-dimensional interacting electron-hole system taking into account Rashba spin-orbit coupling and external magnetic fields. Using an unrestricted Hartree-Fock approach, we self-consistently evaluate spin-selective excitonic condensate order parameters and the Chern number. The ground state phase diagram in the dependence on magnetic field and Coulomb interaction shows a spin-up triplet excitonic condensate (EC) with a nonzero Chern number, emerging uniquely away from the topologically trivial singlet and spin-down triplet EC regions. Strong spin-polarized triplet excitonic fluctuations preceding the condensation are further revealed through the signatures of the dynamical excitonic susceptibility spectra. Our results establish a class of topological quantum phases driven by excitonic coherence and suggest a realistic pathway to its realization in a distorted Janus monolayer of transition metal dichalcogenides or some twisted van der Waals heterostructures.

</details>


### [9] [Decay of spin helices in XXZ quantum spin chains with single-ion anisotropy](https://arxiv.org/abs/2512.08421)
*Florian Lange,Frank Göhmann,Gerhard Wellein,Holger Fehske*

Main category: cond-mat.str-el

TL;DR: 研究反铁磁XXZ链中自旋螺旋态的衰减动力学，单离子各向异性可稳定螺旋态，通过数值模拟和自旋波近似分析最稳定波数


<details>
  <summary>Details</summary>
Motivation: 长寿命自旋螺旋态有助于研究量子磁体中的非平衡动力学，但单离子各向异性会阻止螺旋态成为哈密顿量的本征态，需要研究其衰减特性

Method: 使用无限时间演化块截断（iTEBD）数值模拟计算热力学极限下局部磁化的时间演化；同时采用自旋波近似理论分析

Result: 单离子各向异性下，适当选择的波数仍能使螺旋态保持长寿命；易轴交换各向异性时，单离子各向异性甚至能稳定螺旋态；自旋波近似给出的最稳定波数条件与数值结果定性一致

Conclusion: 单离子各向异性反铁磁XXZ链中，自旋螺旋态在特定波数下可保持长寿命，为研究量子磁体非平衡动力学提供了可控平台

Abstract: Long-lived spin-helix states facilitate the study of non-equilibrium dynamics in quantum magnets. We consider the decay of transverse spin-helices in antiferromagnetic spin-$S$ XXZ chains with single-ion anisostropy. The spin-helix decay is observable in the time evolution of the local magnetization that we calculate numerically for the system in the thermodynamic limit using infinite time-evolving block decimation simulations. Although the single-ion anisotropy prevents helix states from being eigenstates of the Hamiltonian, they still can be long-lived for appropriately chosen wave numbers. In case of an easy-axis exchange anisotropy the single-ion anisotropy may even stabilize the helices. Within a spin-wave approximation, we obtain a condition giving an estimate for the most stable wave number $Q$ that agrees qualitatively with our numerical results.

</details>


### [10] [Diffusion and relaxation of topological excitations in layered spin liquids](https://arxiv.org/abs/2512.08712)
*Aprem P. Joy,Roman Lange,Achim Rosch*

Main category: cond-mat.str-el

TL;DR: 该研究探讨了拓扑相中分数化激发的弛豫过程，通过非线性扩散方程和随机模拟，揭示了泵浦-探测实验如何检测3D材料中的2D拓扑激发特征。


<details>
  <summary>Details</summary>
Motivation: 研究拓扑相（如量子自旋液体）中弛豫过程的动机在于理解分数化激发的动力学和相互作用。在层状材料中，基本准粒子只能在层内自由扩散，而只有成对（或更多）的激发才能在层间跃迁，这是拓扑序的基本结果。研究旨在探索如何通过泵浦-探测实验检测3D材料中2D拓扑激发的存在。

Method: 采用精确求解涌现非线性扩散方程和基于粒子的随机模拟方法，研究泵浦-探测实验中拓扑激发的动力学行为。

Result: 研究发现：1）实验特征时间尺度与初始激发密度（由泵浦强度设定）成反比；2）无湮灭过程时，样品表面均匀激发密度以亚扩散方式向体材料传播，平均深度随t^{1/3}变化；3）允许成对湮灭时，传播变为对数关系（~log t）；4）层间成对扩散导致总密度以(log^2 t)/t的规律衰减，比纯2D系统更慢。

Conclusion: 泵浦-探测实验能够提供3D材料中2D拓扑激发的独特特征信号，这些特征包括特定的时间尺度依赖关系、亚扩散传播行为以及特殊的密度衰减规律。研究讨论了有限宽度样品中泵浦-探测实验的可能实验意义。

Abstract: Relaxation processes in topological phases such as quantum spin liquids are controlled by the dynamics and interaction of fractionalized excitations. In layered materials hosting two-dimensional topological phases, elementary quasiparticles can diffuse freely within the layer, whereas only pairs (or more) can hop between layers - a fundamental consequence of topological order. Using exact solutions of emergent nonlinear diffusion equations and particle-based stochastic simulations, we explore how pump-probe experiments can provide unique signatures of the presence of $2d$ topological excitations in a $3d$ material. Here we show that the characteristic time scale of such experiments is inversely proportional to the initial excitation density, set by the pump intensity. A uniform excitation density created on the surface of a sample spreads subdiffusively into the bulk with a mean depth $\bar z$ scaling as $\sim t^{1/3}$ when annihilation processes are absent. The propagation becomes logarithmic, $\bar z \sim \log t$, when pair-annihilation is allowed. Furthermore, pair-diffusion between layers leads to a new decay law for the total density, $n(t) \sim (\log^2 t)/t$ - slower than in a purely $2d$ system. We discuss possible experimental implications for pump-probe experiments in samples of finite width.

</details>


### [11] [Disentangling the unusual magnetic anisotropy of the near-room-temperature ferromagnet Fe$_{4}$GeTe$_{2}$](https://arxiv.org/abs/2512.08722)
*Riju Pal,Joyal J. Abraham,Alexander Mistonov,Swarnamayee Mishra,Nina Stilkerich,Suchanda Mondal,Prabhat Mandal,Atindra Nath Pal,Jochen Geck,Bernd Büchner,Vladislav Kataev,Alexey Alfonsov*

Main category: cond-mat.str-el

TL;DR: 该研究通过电子自旋共振光谱研究了Fe₄GeTe₂的磁性各向异性，揭示了其在不同温度下的复杂磁行为，发现磁性各向异性与电子输运特性之间存在内在耦合。


<details>
  <summary>Details</summary>
Motivation: Fe₄GeTe₂作为一种近室温铁磁二维材料，具有独特的自旋重取向转变，但其磁性各向异性随温度演化的机制尚不清楚，这限制了对其磁稳定性和器件应用的优化。

Method: 采用电子自旋共振（ESR）光谱技术对Fe₄GeTe₂进行定量研究，分析其磁性各向异性随温度的变化，并与输运测量结果进行对比。

Result: 研究发现：高温下总磁性各向异性主要由退磁效应主导；在T_shape~150K以下，易轴型本征磁性各向异性增强；在T_SR~110K时样品呈现表观各向同性；在T_d~50K以下，本征磁性各向异性变得更加复杂。ESR实验发现的特征温度与输运测量结果一致。

Conclusion: Fe₄GeTe₂中磁性各向异性与电子自由度之间存在内在耦合，这一发现以及观测到的二维本征特征为优化该材料在磁电子器件中的应用提供了重要指导，即使在单层极限下也具潜力。

Abstract: In the quest for two-dimensional conducting materials with high ferromagnetic ordering temperature the new family of the layered Fe$_{n}$GeTe$_{2}$ compounds, especially the near-room-temperature ferromagnet Fe$_{4}$GeTe$_{2}$, receives a significant attention. Fe$_{4}$GeTe$_{2}$ features a peculiar spin reorientation transition at $T_\mathrm{SR} \sim 110$ K suggesting a non-trivial temperature evolution of the magnetic anisotropy (MA) - one of the main contributors to the stabilization of the magnetic order in the low-D systems. An electron spin resonance (ESR) spectroscopic study reported here provides quantitative insights into the unusual magnetic anisotropy of Fe$_{4}$GeTe$_{2}$. At high temperatures the total MA is mostly given by the demagnetization effect with a small contribution of the counteracting intrinsic magnetic anisotropy of an easy-axis type, whose growth below a characteristic temperature $T_{\rm shape} \sim 150$ K renders the sample seemingly isotropic at $T_\mathrm{SR}$. Below one further temperature $T_{\rm d} \sim 50$ K the intrinsic MA becomes even more complex. Importantly, all the characteristic temperatures found in the ESR experiment match those observed in transport measurements, suggesting an inherent coupling between magnetic and electronic degrees of freedom in Fe$_{4}$GeTe$_{2}$. This finding together with the observed signatures of the intrinsic two-dimensionality should facilitate optimization routes for the use of Fe$_{4}$GeTe$_{2}$ in the magneto-electronic devices, potentially even in the monolayer limit.

</details>


### [12] [Triangular $J_1$-$J_2$ Heisenberg Antiferromagnet in a Magnetic Field](https://arxiv.org/abs/2512.08768)
*Thomas Bader,Shi Feng,Sasank Budaraju,Federico Becca,Johannes Knolle,Frank Pollmann*

Main category: cond-mat.str-el

TL;DR: J1-J2三角晶格海森堡反铁磁体在磁场中的相图研究，通过三种互补方法揭示了磁场诱导磁序和磁化平台的竞争关系。


<details>
  <summary>Details</summary>
Motivation: 尽管经过数十年的研究，J1-J2三角晶格海森堡反铁磁体在磁场中的行为仍然存在争议。该研究旨在通过多种方法系统地绘制其相图，解决不同磁场诱导磁序和磁化平台之间的竞争关系。

Method: 采用三种互补方法：自洽非线性自旋波理论、密度矩阵重整化群和变分蒙特卡洛方法，系统地研究J1-J2三角晶格海森堡反铁磁体在磁场中的相图。

Result: 在经典受挫参数范围内，特别是在J2/J1=1/8附近的参数区域，研究发现：i) 施加外场时，无能隙量子自旋液体获得有限密度的单极子；ii) 进一步增加磁场时，在m=1/3和m=1/2处获得两个明显的磁化平台。

Conclusion: 连续的磁化平台转变可以作为底层量子自旋液体相的重要实验特征，这对于理解J1-J2三角晶格海森堡反铁磁体在磁场中的行为具有重要意义。

Abstract: The behavior of the paradigmatic $J_1-J_2$ triangular lattice Heisenberg antiferromagnet in a magnetic field remains unsettled despite decades of study. We map out the phase diagram using three complementary approaches, including self-consistent nonlinear spin-wave theory, density-matrix renormalization group, and variational Monte Carlo. This combined analysis resolves the competition among different field-induced magnetic orders and magnetization plateaux across the classically frustrated parameter range. In particular, there is a finite range in the parameter regime around $J_2/J_1=\frac{1}{8}$ in which i) upon the application of the external field, the gapless quantum spin liquid acquires a finite density of monopoles, and ii) by further increasing the field, two plateaux are clearly obtained at $m=\frac{1}{3}$ and $m=\frac{1}{2}$. We discuss the experimental importance of the consecutive magnetization plateaux transitions as a signature of an underlying quantum spin-liquid phase.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [13] [Minimal Models of Entropic Order](https://arxiv.org/abs/2512.07980)
*Xiaoyang Huang,Zohar Komargodski,Andrew Lucas,Fedor K. Popov,Tin Sulejmanpasic*

Main category: cond-mat.stat-mech

TL;DR: 该论文提出了基于熵效应的最小模型，证明在极高温度下也能出现自发对称性破缺和有序相


<details>
  <summary>Details</summary>
Motivation: 研究熵效应如何导致量子或经典系统在高能态下出现有序相，挑战传统高温导致无序的认知

Method: 提出算术伊辛模型及其量子版本，用非负整数替代传统伊辛自旋；使用大味展开和数值模拟分析

Result: 发现经典和量子模型的高温相都是有序的；还引入了经典气体模型，其相互作用在高温下驱动系统形成晶体

Conclusion: 熵效应可以在极高温度下产生有序相，这为理解高温对称性破缺提供了新的理论框架

Abstract: Due to entropic effects, it is possible that generic high-energy states of a quantum or classical system are ordered. This leads to spontaneous symmetry breaking at arbitrarily high temperatures. We present minimal models of entropic order that arise from very simple interactions. Our main examples are the Arithmetic Ising Model (AIM) and its quantum analogue, where usual Ising spins are replaced by non-negative integers. Using a large-flavor expansion together with numerical simulations, we find that the high-temperature phase is ordered in the classical and quantum models. We also introduce classical gas models whose interactions drive the system to a crystal at high temperatures.

</details>


### [14] [Free fermionic and parafermionic multispin quantum chains with non-homogeneous interacting ranges](https://arxiv.org/abs/2512.08011)
*Francisco C. Alcaraz*

Main category: cond-mat.stat-mech

TL;DR: 本文扩展了具有Z(N)对称性的量子自旋模型，从均匀相互作用范围推广到非均匀相互作用范围，并建立了保证自由粒子谱的一般条件。


<details>
  <summary>Details</summary>
Motivation: 现有具有自由粒子谱的Z(N)对称量子自旋模型都假设多自旋相互作用的范围是均匀的。本文旨在探索当相互作用范围不均匀（依赖于格点位置）时，是否仍能保持自由粒子谱的特性。

Method: 扩展Z(N)交换代数，引入具有非均匀相互作用范围的新模型。建立保证自由粒子谱的一般条件，研究相互作用范围在偶数和奇数格点上为常数的特殊情况，并计算动力学临界指数。

Result: 成功推导出非均匀相互作用范围模型保持自由粒子谱所需的一般条件。通过具体例子验证了理论框架，并在特殊情况下计算了动力学临界指数。

Conclusion: 本文成功将具有自由粒子谱的Z(N)对称量子自旋模型从均匀相互作用范围推广到非均匀情况，建立了理论框架并展示了具体应用，为研究更复杂的量子多体系统提供了新工具。

Abstract: A large family of multispin interacting one-dimensional quantum spin models with $Z(N)$ symmetry and a free-particle eigenspectra are known in the literature. They are free-fermionic ($N=2$) and free-parafermionic ($N\geq 2$) quantum chains. The essential ingredient that implies the free-particle spectra is the fact that these Hamiltonians are expressed in terms of generators of a $Z(N)$ exchange algebra. In all these known quantum chains the number of spins in all the multispin interactions (range of interactions) is the same and therefore, the models have homogeneous interacting range. In this paper we extend the $Z(N)$ exchange algebra, by introducing new models with a free-particle spectra, where the interaction ranges of the multispin interactions are not uniform anymore and depends on the lattice sites (non-homogeneous interacting range). We obtain the general conditions that the site-dependent ranges of the multispin interactions have to satisfy to ensure a free-particle spectra. Several simple examples are introduced. We study in detail the critical properties in the case where the range of interactions of the even (odd) sites are constant. The dynamical critical exponent is evaluated in several cases.

</details>


### [15] [Emergent memory in cell-like active systems](https://arxiv.org/abs/2512.08058)
*Marc Besse,Raphaël Voituriez*

Main category: cond-mat.stat-mech

TL;DR: 论文提出了一种超越传统自驱动粒子模型的理论框架，通过引入内部状态动力学和环境感知能力，揭示了环境记忆如何自发产生并导致新的行为类别。


<details>
  <summary>Details</summary>
Motivation: 传统自驱动粒子模型通常假设无记忆动力学且内部主动力与环境无耦合，这无法解释像活细胞这样具有多时间尺度记忆效应的系统。需要建立一个更一般的理论框架来纳入内部状态动力学和环境感知。

Method: 引入了一个包含内部状态动力学和环境感知的理论框架，其中智能体的自推进取决于具有复杂动力学的内部变量，这些变量受局部环境线索调制。该框架将内部状态动态与环境反馈相结合。

Result: 当智能体的自推进依赖于内部变量时，环境记忆会自发产生，导致新的行为类别：记忆诱导响应、复杂景观中的适应性定位、运动诱导相分离的抑制以及增强的堵塞转变。

Conclusion: 非平衡智能体（如活细胞）内在的最小信息处理能力可以深刻影响个体和集体行为。该框架连接了细胞尺度活动和大规模智能运动，为从合成胶体到生物集体和机器人集群的系统定量分析和设计开辟了道路。

Abstract: Active systems across scales, ranging from molecular machines to human crowds, are usually modeled as assemblies of self-propelled particles driven by internally generated forces. However, these models often assume memoryless dynamics and no coupling of internal active forces to the environment. Here, guided by the example of living cells, which have recently been shown to display multi-timescale memory effects, we introduce a general theoretical framework that goes beyond this paradigm by incorporating internal state dynamics and environmental sensing into active particle models. We show that when the self-propulsion of an agent depends on internal variables with their own complex dynamics - modulated by local environmental cues - environmental memory spontaneously emerges and gives rise to new classes of behaviours. These include memory-induced responses, adaptable localization in complex landscapes, suppression of motility-induced phase separation, and enhanced jamming transitions. Our results demonstrate how minimal information processing capabilities, intrinsic to non-equilibrium agents with internal states like living cells, can profoundly influence both individual and collective behaviours. This framework bridges cell-scale activity and large-scale intelligent motion in cell assemblies, and opens the way to the quantitative analysis and design of systems ranging from synthetic colloids to biological collectives and robotic swarms.

</details>


### [16] [Nonreciprocal dynamics with weak noise: aperiodic "Escher cycles" and their quasipotential landscape](https://arxiv.org/abs/2512.08210)
*Janik Schüttler,Robert L. Jack,Michael E. Cates*

Main category: cond-mat.stat-mech

TL;DR: 该研究构建了具有非互易相互作用的二维随机系统的Freidlin-Wentzell拟势，揭示了噪声诱导的Escher循环动力学和奇异势能景观结构。


<details>
  <summary>Details</summary>
Motivation: 研究非平衡随机系统中罕见事件的几何结构，特别是非互易相互作用如何影响噪声诱导的相变和势能景观，为理解复杂非平衡动力学提供解析可处理的模型。

Method: 采用Freidlin-Wentzell拟势理论，对具有两个自由度和非互易相互作用的随机系统进行显式构造。通过一阶非互易性展开解析计算拟势，沿连接吸引子的一维反应坐标表征，并在微扰理论主导阶获得完整的二维势能景观。

Result: 发现系统在四个亚稳态吸引子之间经历噪声诱导的Escher循环转变，势能景观呈现平坦区域、扩展平台和非可微切换线等奇异结构。这些结构源于竞争过渡路径之间的主导权交接和吸引盆竞争两种几何机制。

Conclusion: 该研究提供了一个罕见的解析可处理案例，完全解析了非平衡罕见事件的几何结构，展示了在多个坐标中捕获丰富非平衡特征的拟势，为理解复杂非平衡动力学提供了重要理论框架。

Abstract: We present an explicit construction of the Freidlin-Wentzell quasipotential of a stochastic system with two degrees of freedom and nonreciprocal interactions. This model undergoes noise-induced transitions between four metastable attractors, forming recurrent but aperiodic ``Escher cycles,'' similar to the cyclic nucleation dynamics observed in the nonreciprocal Ising model. We calculate the quasipotential analytically to first order in nonreciprocality. We characterise it along a one-dimensional reaction coordinate that connects the attractors, and we also obtain the full two-dimensional landscape, at leading order in perturbation theory. The resulting landscapes feature flat regions and extended plateaus, together with non-differentiable switching lines. These singular structures arise from two geometric mechanisms: the handover of dominance between competing transition paths, and the competition between basins of attraction. The system provides a rare case where the geometry of nonequilibrium rare events can be fully resolved, and a simple analytically tractable example of a quasipotential in more than one coordinate that captures a rich set of nonequilibrium features.

</details>


### [17] [Supercritical-subcritical correspondence, asymmetric effects and antisymmetric corrections near a critical point](https://arxiv.org/abs/2512.08553)
*Xinyang Li,Yuliang Jin*

Main category: cond-mat.stat-mech

TL;DR: 该研究探讨了非对称性对临界现象标度律的影响，提出了超临界-亚临界对应关系，预测了非对称系统在超临界边界线上会产生具有反对称系数的普适标度修正，并通过实验数据和模型验证了这些预测。


<details>
  <summary>Details</summary>
Motivation: 伊辛模型和液-气系统的二阶相变虽然共享普适类和临界指数，但液-气系统的哈密顿量缺乏Z_2对称性，这种矛盾突显了临界现象中的一个核心谜题：非对称性对标度律有何影响？研究旨在探究非对称性在超临界区域的影响。

Method: 提出了超临界-亚临界对应关系，将亚临界共存曲线与最近定义的超临界边界线（L^±线）进行形式类比。理论预测物理场的线性混合（非对称系统的特征）会在这些超临界轨迹中产生具有反对称系数的普适标度修正。使用NIST数据库的液-气数据和液-液相变模型验证预测，并证明相同的非对称标度框架支配着序参量分布的高阶累积量的行为。

Result: 理论预测得到验证：非对称系统在超临界边界线上确实产生具有反对称系数的普适标度修正。液-气数据和液-液相变模型的实验结果与理论预测一致，同时证明了相同的非对称标度框架也适用于序参量分布的高阶累积量。

Conclusion: 非对称性对标度律的影响不仅限于亚临界区域，也延伸到超临界区域。提出的超临界-亚临界对应关系为理解非对称临界现象提供了新视角，统一的非对称标度框架能够同时描述超临界边界线和序参量分布的高阶累积量行为，深化了对非对称临界现象的理解。

Abstract: The second-order phase transitions in the Ising model and liquid-gas systems share a universality class and critical exponents, despite the absence of $Z_2$ symmetry in the liquid-gas Hamiltonian. This discrepancy highlights a central puzzle in critical phenomena: what is the influence of asymmetry on scaling laws? For over a century, this question has been explored through examining violations of the empirical ``rectilinear diameter law'' for the subcritical coexistence curve, where asymmetry could generate singular corrections. Here, we extend this investigation to the supercritical regime. We propose a supercritical-subcritical correspondence, drawing a formal analogy between the subcritical coexistence curve and recently defined supercritical boundary lines ($L^\pm$ lines). Our theory predicts that the linear mixing of physical fields - a hallmark of asymmetric systems - produces universal scaling corrections, with antisymmetric coefficients, in these supercritical loci. We verify these predictions using liquid-gas data from the NIST database and a model liquid-liquid transition. Furthermore, we demonstrate that the same asymmetric scaling framework governs the behavior of higher-order cumulants in the order parameter distribution.

</details>


### [18] [Spontaneous Ratchet Currents and Transition Dynamics in Active Wetting](https://arxiv.org/abs/2512.08761)
*Noah Grodzinski,Robert L. Jack,Michael E. Cates*

Main category: cond-mat.stat-mech

TL;DR: 活性物质在排斥屏障上表现出类似平衡润湿的行为，但具有非平衡特性，包括临界润湿转变和自发对称破缺的棘轮电流。


<details>
  <summary>Details</summary>
Motivation: 研究活性物质在排斥屏障上的润湿现象，探索其与平衡润湿的关系，并识别活性带来的非平衡效应。

Method: 使用精确（无噪声）的活性晶格气体流体力学框架，在具有周期性边界条件的狭缝几何结构中分析活性物质的润湿行为。

Result: 活性物质表现出完全润湿和部分润湿状态，两者之间存在临界润湿转变；在部分润湿状态中发现了自发对称破缺的棘轮电流，导致体密度偏离其双节线值，并揭示了从完全润湿到部分润湿转变的新动力学路径。

Conclusion: 该研究在建立活性润湿与平衡润湿直接联系的同时，也识别了活性带来的非平衡后果，为理解活性物质的润湿现象提供了新视角。

Abstract: Self-propelled particles accumulate on repulsive barriers in so-called active wetting, but the relationship between this process and equilibrium wetting remains unclear. Using an exact (noiseless) hydrodynamic framework for an active lattice gas, we show, using a slit geometry with periodic boundary conditions, that active matter exhibits both fully- and partially-wet states, with a critical wetting transition between them. Furthermore, we demonstrate the existence of a spontaneous-symmetry-breaking ratchet current in the partially wet state, leading to departure of the bulk densities from their binodal values and the emergence of a novel dynamical pathway for the full-to-partial wetting transition. We elucidate this modified dynamical pathway using a minimal model. The results, while establishing a direct connection between active and equilibrium wetting, also identify the nonequilibrium consequences of activity.

</details>


### [19] [Commissioning of an experiment for thermodynamic and spectroscopic studies of hydrogen isotopologues at cryogenic conditions](https://arxiv.org/abs/2512.08788)
*Joshua Kohpeiß,Dominic Batzler,Beate Bornschein,Lutz Bornschein,Robin Größle,Daniel Kurz,Ralph Lietzow,Alexander Marsteller,Michael Sturm,Stefan Welte*

Main category: cond-mat.stat-mech

TL;DR: T2ApIR实验装置在TLK建成并调试，用于研究氢同位素在低温高密度下的热力学性质和相空间行为，使用红外和拉曼光谱方法，可处理高剂量氚并满足安全要求。


<details>
  <summary>Details</summary>
Motivation: 研究氢同位素在低温（10K-300K）和高密度（最高2.5bar）条件下的热力学性质和动态相空间行为，特别是氚化氢在不同相态、正/仲态下的特性。

Method: 使用T2ApIR实验装置，包含完全氚兼容的低温恒温器、光学池、正/仲转换器和光学窗口，通过两级低温冷却器冷却至氢三相点以下，采用红外和拉曼光谱等光学方法进行研究。

Result: 实验装置成功建成并完成调试，包括低温性能测试、非放射性气体调试实验和分析仪器测试，解决了在有限空间内处理14克高剂量氚同时满足TLK安全要求的挑战。

Conclusion: T2ApIR实验装置已准备就绪，可完全集成到TLK闭环氚基础设施中，为研究氢同位素在极端条件下的行为提供了先进实验平台。

Abstract: To study thermodynamic properties and dynamic phase space behavior of hydrogen isotopologues (Q$_2$) at cryogenic temperatures and at high density, the Tritium Absorption InfraRed Spectroscopy 2 (T$_2$ApIR) experiment has been set up and commissioned at Tritium Laboratory Karlsruhe (TLK). In the frame of the experiment, Q$_2$ behavior in different phases, ortho/para states, temperatures (10 K - 300 K) and pressures (up to 2.5 bar a) will be investigated with optical methods, infrared and Raman spectroscopy. The facility consists of a fully tritium compatible cryostat, which includes an optical cell, ortho/para converter and windows for optical and spectroscopic studies. The cryostat can be cooled below the H$_2$ triple point by a two-stage cryocooler and contains openings in the cryogenic shielding for the optical access. The challenge of combining these scientific requirements in a design with high amounts of tritium (14 g), in a limited space, all while maintaining the TLK safety philosophy was solved by the presented design. The experiment is ready to be fully integrated into the TLK closed loop tritium infrastructure. This contribution reports a comprehensive overview of the commissioning phase of the experimental facility and the results of the first commissioning experiments, including cryogenic performance tests, commissioning experiments with non-radioactive gases, and tests of the analytical instruments.

</details>


### [20] [Langevin equation with potential of mean force: The case of anchored bath](https://arxiv.org/abs/2512.08793)
*Alex V. Plyukhin*

Main category: cond-mat.stat-mech

TL;DR: 该研究探讨了平均力势在非平衡系统中的适用性问题，发现对于线性力系统，平均力势可以替代外部势能，但对于一般情况，朗之万方程无法仅用平均力势闭合描述。


<details>
  <summary>Details</summary>
Motivation: 平均力势通常定义在平衡系综中，但在非平衡系统中如何应用尚不明确。研究旨在探索平均力势在描述非平衡系统时的有效性和局限性。

Method: 采用单粒子系统模型，通过锚定势作用在浴粒子上产生非平凡的平均力势。研究分析平均力势如何影响耗散核和噪声的统计特性，特别考察了线性力系统的情况。

Result: 研究发现平均力势不仅替代外部势能，还会使耗散核和噪声统计特性依赖于系统位置。这种依赖性由内部浴和系统-浴相互作用决定，导致一般情况下的朗之万方程无法闭合。但对于线性力系统，这种位置依赖性可以消除。

Conclusion: 平均力势在非平衡系统中的直接应用受到限制，只有在特定条件下（如线性力系统）才能有效替代外部势能，形成可操作的广义朗之万方程。

Abstract: The potential of mean force (PMF) is an effective average potential acting on an open system, renormalized due to the interaction with the surrounding thermal bath. The PMF is defined for an equilibrium ensemble, and generally it is not clear how to use it when the system is out of equilibrium and described by a (generalized) Langevin equation. We study a model where the system is a single particle (so there are no complications related to internal forces) and a non-trivial PMF is due to the presence of on-site (anchor) potentials applied to the bath particles. We found that the PMF does not merely replace the external potential, but also makes the dissipation kernel and statistical properties of noise dependent on the system's position. That dependence is determined by the internal bath and system-bath interactions and is a priori unknown. Therefore, in the general case the Langevin equation with the PMF is not closed and thus inoperable. However, for systems with linear forces the aforementioned dependence on the system's position may be canceled. As an example, we consider a model where the bath is formed by the Klein-Gordon chain, i. e. a harmonic chain with on-site harmonic potentials. In that case, the generalized Langevin equation has the standard form with an external potential replaced by a quadratic PMF.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [21] [Softly Symbolifying Kolmogorov-Arnold Networks](https://arxiv.org/abs/2512.07875)
*James Bagrow,Josh Bongard*

Main category: cs.LG

TL;DR: S2KAN在KAN基础上引入符号化原语和可学习门控，通过可微分稀疏化和最小描述长度目标，实现符号化与稠密样条的平衡，在保持精度的同时获得可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统KAN虽然具有可解释性潜力，但训练后的激活函数往往缺乏符号保真度，学习到的分解没有有意义的对应关系，无法实现真正的可解释性。

Method: 提出Softly Symbolified KANs (S2KAN)，将符号原语直接集成到训练中：每个激活函数从符号项和稠密项的字典中抽取，使用可学习门控实现稀疏表示；通过可微分稀疏化和最小描述长度目标进行端到端优化。

Result: 在符号基准测试、动力系统预测和真实世界预测任务中，S2KAN实现了竞争性或更优的精度，同时模型规模显著减小；观察到即使没有正则化压力也会出现自稀疏化现象。

Conclusion: S2KAN能够在符号项足够时发现可解释形式，不足时优雅地退化为稠密样条，实现了可解释性与准确性的平衡，为可解释机器学习提供了新途径。

Abstract: Kolmogorov-Arnold Networks (KANs) offer a promising path toward interpretable machine learning: their learnable activations can be studied individually, while collectively fitting complex data accurately. In practice, however, trained activations often lack symbolic fidelity, learning pathological decompositions with no meaningful correspondence to interpretable forms. We propose Softly Symbolified Kolmogorov-Arnold Networks (S2KAN), which integrate symbolic primitives directly into training. Each activation draws from a dictionary of symbolic and dense terms, with learnable gates that sparsify the representation. Crucially, this sparsification is differentiable, enabling end-to-end optimization, and is guided by a principled Minimum Description Length objective. When symbolic terms suffice, S2KAN discovers interpretable forms; when they do not, it gracefully degrades to dense splines. We demonstrate competitive or superior accuracy with substantially smaller models across symbolic benchmarks, dynamical systems forecasting, and real-world prediction tasks, and observe evidence of emergent self-sparsification even without regularization pressure.

</details>


### [22] [ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models](https://arxiv.org/abs/2512.07843)
*Long Lian,Sida Wang,Felix Juefei-Xu,Tsu-Jui Fu,Xiuyu Li,Adam Yala,Trevor Darrell,Alane Suhr,Yuandong Tian,Xi Victoria Lin*

Main category: cs.LG

TL;DR: ThreadWeaver是一个自适应并行推理框架，在保持与顺序推理模型相当准确率的同时，显著降低推理延迟，在数学推理任务上实现1.53倍加速。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理时固有的顺序解码导致高延迟，现有并行推理方法要么局限于监督行为克隆，要么相比顺序链式思维基线准确率显著下降，且需要定制化推理引擎，部署复杂。

Method: 1）两阶段并行轨迹生成器产生大规模高质量并行标注的CoT数据用于监督微调；2）基于trie树的训练-推理协同设计，无需修改位置嵌入或KV缓存即可在现成自回归推理引擎上实现并行推理；3）并行化感知的强化学习框架，教导模型在准确率和有效并行化之间取得平衡。

Result: 在六个数学推理基准测试中，基于Qwen3-8B训练的ThreadWeaver达到与前沿顺序推理模型相当的准确率（平均71.9%，AIME24上79.9%），同时实现最高1.53倍的平均token延迟加速，建立了准确率与效率之间的新帕累托前沿。

Conclusion: ThreadWeaver框架成功实现了自适应并行推理，在保持准确率的同时显著提升推理效率，无需定制化推理引擎即可部署，为LLM推理优化提供了有效解决方案。

Abstract: Scaling inference-time computation has enabled Large Language Models (LLMs) to achieve strong reasoning performance, but inherently sequential decoding leads to substantial latency, especially on complex tasks. Recent work on adaptive parallel reasoning aims to improve inference efficiency by decomposing the problem-solving process into concurrent reasoning threads when beneficial. However, existing methods on realistic tasks are either limited to supervised behavior cloning or exhibit significant accuracy drops compared to widely-used sequential long chain-of-thought (CoT) baselines. Moreover, many require customized inference engines, complicating deployment. We introduce ThreadWeaver, a framework for adaptive parallel reasoning that achieves accuracy on par with popular sequential reasoning models of comparable size while significantly reducing inference latency. ThreadWeaver's performance stems from three key innovations: 1) a two-stage parallel trajectory generator that produces large-scale, high-quality CoT data with parallel annotations for supervised fine-tuning; 2) a trie-based training-inference co-design that enables parallel reasoning on any off-the-shelf autoregressive inference engine without modifying position embeddings or KV caches; and 3) a parallelization-aware reinforcement learning framework that teaches the model to balance accuracy with effective parallelization. Across six challenging mathematical reasoning benchmarks, ThreadWeaver trained atop Qwen3-8B achieves accuracy comparable to cutting-edge sequential reasoning models (71.9% on average and 79.9% on AIME24) while delivering up to 1.53x average speedup in token latency, establishing a new Pareto frontier between accuracy and efficiency.

</details>


### [23] [RaX-Crash: A Resource Efficient and Explainable Small Model Pipeline with an Application to City Scale Injury Severity Prediction](https://arxiv.org/abs/2512.07848)
*Di Zhu,Chen Xie,Ziwei Wang,Haoyun Zhang*

Main category: cs.LG

TL;DR: RaX-Crash是一个资源高效、可解释的小模型管道，用于预测纽约市机动车碰撞事故的伤害严重程度，使用树集成模型在结构化数据上表现优于小型语言模型。


<details>
  <summary>Details</summary>
Motivation: 纽约市每年发生超过10万起机动车碰撞事故，造成严重的伤害和公共卫生负担，需要高效且可解释的预测方法来分析事故严重程度。

Method: 整合三个包含数千万条记录的关联表，构建统一特征模式，在分区存储上训练紧凑的树集成模型（随机森林和XGBoost），并与使用文本摘要的小型语言模型进行比较。

Result: XGBoost和随机森林在时间保留测试集上分别达到0.7828和0.7794的准确率，明显优于小型语言模型（0.594和0.496）；类别不平衡分析显示简单类别加权可提高致命事故召回率。

Conclusion: 可解释的小模型集成仍然是城市规模伤害分析的强大基线，而将表格预测器与SLM生成的叙述相结合的混合管道可以在不牺牲可扩展性的情况下改善沟通。

Abstract: New York City reports over one hundred thousand motor vehicle collisions each year, creating substantial injury and public health burden. We present RaX-Crash, a resource efficient and explainable small model pipeline for structured injury severity prediction on the official NYC Motor Vehicle Collisions dataset. RaX-Crash integrates three linked tables with tens of millions of records, builds a unified feature schema in partitioned storage, and trains compact tree based ensembles (Random Forest and XGBoost) on engineered tabular features, which are compared against locally deployed small language models (SLMs) prompted with textual summaries. On a temporally held out test set, XGBoost and Random Forest achieve accuracies of 0.7828 and 0.7794, clearly outperforming SLMs (0.594 and 0.496); class imbalance analysis shows that simple class weighting improves fatal recall with modest accuracy trade offs, and SHAP attribution highlights human vulnerability factors, timing, and location as dominant drivers of predicted severity. Overall, RaX-Crash indicates that interpretable small model ensembles remain strong baselines for city scale injury analytics, while hybrid pipelines that pair tabular predictors with SLM generated narratives improve communication without sacrificing scalability.

</details>


### [24] [GPU Memory Prediction for Multimodal Model Training](https://arxiv.org/abs/2512.07853)
*Jinwoo Jeong,Minchul Kang,Younghun Go,Changyong Shin,Hyunho Lee,Junho Yoon,Gyeongsik Yang,Chuck Yoo*

Main category: cs.LG

TL;DR: 提出一个预测多模态模型GPU内存峰值使用量的框架，通过分解模型架构和分析训练行为来准确预测内存使用，避免内存溢出错误


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI系统中深度学习模型规模和复杂度的增加，GPU内存需求经常超过可用容量，导致内存溢出错误。现有研究仅关注单模态架构，无法推广到多模态模型，而多模态模型在智能体AI系统中很常见

Method: 提出一个框架，将多模态模型分解为组成层，并应用因子化方法来估计每层的内存使用量，通过分析模型架构和训练行为来预测GPU内存峰值使用

Result: 评估显示该框架实现了约8.7%的平均MAPE（平均绝对百分比误差）预测准确率

Conclusion: 该框架能够准确预测多模态模型的GPU内存峰值使用，解决了现有方法无法处理多模态模型的局限性，有助于防止内存溢出错误和计算资源浪费

Abstract: As deep learning models in agentic AI systems grow in scale and complexity, GPU memory requirements increase and often exceed the available GPU memory capacity, so that out-of-memory (OoM) errors occur. It is well known that OoM interrupts the whole training itself and wastes substantial computational resources. Therefore, to prevent OoM, accurate prediction of GPU memory usage is essential. However, previous studies focus only on unimodal architectures and fail to generalize to multimodal models, even though the multimodal models are a common choice in agentic AI systems. To address this limitation, we propose a framework that predicts the peak GPU memory usage by analyzing the model architecture and training behavior of multimodal models. Specifically, the framework decomposes the multimodal model into its constituent layers and applies factorization to estimate the memory usage of each layer. Our evaluation shows that our framework achieves high prediction accuracy of ~8.7% average MAPE.

</details>


### [25] [LAPA: Log-Domain Prediction-Driven Dynamic Sparsity Accelerator for Transformer Model](https://arxiv.org/abs/2512.07855)
*Huizheng Wang,Hongbin Wang,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.LG

TL;DR: LAPA提出了一种基于对数域注意力预测的算法-架构协同设计，通过消除昂贵乘法运算和减少累积开销，实现跨阶段稀疏Transformer加速，能效比现有方法提升2.79-3.52倍。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在不同阶段存在动态计算瓶颈，需要跨阶段稀疏加速策略。现有稀疏Transformer方法多为单阶段设计，其稀疏预测机制在多阶段应用中会产生显著功耗开销。

Method: 提出LAPA方法：1）设计非对称前导一计算（ALOC）方案消除昂贵乘法；2）提出混合精度多轮移位累积（MRSA）机制减少累积开销；3）设计数据特征依赖滤波器（DDF）与MRSA协同工作；4）设计专用加速器将理论改进转化为实际硬件提升。

Result: 实验结果显示，LAPA相比现有最佳方法Spatten、Sanger和FACT，能效分别提升3.52倍、3.24倍和2.79倍。

Conclusion: LAPA通过算法-架构协同设计有效解决了跨阶段稀疏Transformer的加速问题，显著提升了能效，为动态计算瓶颈的Transformer模型提供了高效的硬件加速方案。

Abstract: Attention-based Transformers have revolutionized natural language processing (NLP) and shown strong performance in computer vision (CV) tasks. However, as the input sequence varies, the computational bottlenecks in Transformer models exhibit dynamic behavior across stages, which calls for a cross-stage sparse acceleration strategy. Unfortunately, most existing sparse Transformer approaches are single-stage based, and their sparsity prediction mechanisms lead to significant power overhead when applied across multiple stages. To this end, this paper proposes a log-domain attention prediction algorithm-architecture co-design, named LAPA. First, an asymmetric leading one computing (ALOC) scheme is designed to eliminate expensive multiplications. Next, a mixed-precision multi-round shifting accumulation (MRSA) mechanism is further proposed to mitigate the accumulation overhead. A data-feature dependent filter (DDF) strategy is designed to work in concert with the MRSA process. Finally, an elaborate accelerator is designed to translate the theoretical enhancement into practical hardware improvement. Experimental results show that LAPA achieves 3.52x, 3.24x and 2.79x higher energy efficiency than the state-of-the-art (SOTA) works Spatten, Sanger and FACT, respectively.

</details>


### [26] [Medical Test-free Disease Detection Based on Big Data](https://arxiv.org/abs/2512.07856)
*Haokun Zhao,Yingzhe Bai,Qingyang Xu,Lixin Zhou,Jianxin Chen,Jicong Fan*

Main category: cs.LG

TL;DR: CLDD是一种基于图的深度学习模型，通过利用疾病间的关联和患者间的相似性进行协同学习，实现无需依赖医疗检测的大规模疾病预测。


<details>
  <summary>Details</summary>
Motivation: 疾病检测需要大量医疗测试且成本高昂，难以对每个患者进行所有可能的检测来诊断数千种疾病。需要一种能够减少诊断成本、提高可及性的方法。

Method: 提出CLDD模型，将疾病检测构建为协同学习任务，利用疾病间的关联和患者间的相似性，整合患者-疾病交互和人口统计特征，无需依赖相应医疗测试。

Result: 在MIMIC-IV数据集（61,191患者，2,000疾病）上，CLDD在多个指标上优于基准方法，召回率提升6.33%，精确率提升7.63%。案例研究表明模型能成功恢复掩盖的疾病。

Conclusion: CLDD通过降低诊断成本和提高可及性，有望用于大规模疾病筛查和社会健康保障，展示了疾病预测的可解释性和可靠性。

Abstract: Accurate disease detection is of paramount importance for effective medical treatment and patient care. However, the process of disease detection is often associated with extensive medical testing and considerable costs, making it impractical to perform all possible medical tests on a patient to diagnose or predict hundreds or thousands of diseases. In this work, we propose Collaborative Learning for Disease Detection (CLDD), a novel graph-based deep learning model that formulates disease detection as a collaborative learning task by exploiting associations among diseases and similarities among patients adaptively. CLDD integrates patient-disease interactions and demographic features from electronic health records to detect hundreds or thousands of diseases for every patient, with little to no reliance on the corresponding medical tests. Extensive experiments on a processed version of the MIMIC-IV dataset comprising 61,191 patients and 2,000 diseases demonstrate that CLDD consistently outperforms representative baselines across multiple metrics, achieving a 6.33\% improvement in recall and 7.63\% improvement in precision. Furthermore, case studies on individual patients illustrate that CLDD can successfully recover masked diseases within its top-ranked predictions, demonstrating both interpretability and reliability in disease prediction. By reducing diagnostic costs and improving accessibility, CLDD holds promise for large-scale disease screening and social health security.

</details>


### [27] [SA^2GFM: Enhancing Robust Graph Foundation Models with Structure-Aware Semantic Augmentation](https://arxiv.org/abs/2512.07857)
*Junhua Shi,Qingyun Sun,Haonan Yuan,Xingcheng Fu*

Main category: cs.LG

TL;DR: SA^2GFM是一个鲁棒的图基础模型框架，通过结构感知语义增强提升领域自适应表示能力，在节点和图分类任务中对抗随机噪声和对抗性扰动表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前图基础模型在各种任务中取得显著进展，但其对领域噪声、结构扰动和对抗攻击的鲁棒性尚未充分探索。关键限制在于对层次结构语义的建模不足，而这对泛化能力至关重要。

Method: 提出SA^2GFM框架：1）通过将基于熵的编码树转换为结构感知文本提示来编码层次结构先验进行特征增强；2）使用自监督信息瓶颈机制通过结构引导压缩蒸馏鲁棒可迁移表示；3）引入专家自适应路由机制（混合专家架构+空专家设计）解决跨领域适应的负迁移问题；4）提出微调模块通过联合社区内和社区间结构学习优化层次结构。

Result: 大量实验表明，SA^2GFM在节点和图分类任务中，在对抗随机噪声和对抗性扰动的有效性和鲁棒性方面优于9个最先进的基线方法。

Conclusion: SA^2GFM通过结构感知语义增强显著提升了图基础模型的鲁棒性和领域自适应能力，为图基础模型的可靠应用提供了有效解决方案。

Abstract: We present Graph Foundation Models (GFMs) which have made significant progress in various tasks, but their robustness against domain noise, structural perturbations, and adversarial attacks remains underexplored. A key limitation is the insufficient modeling of hierarchical structural semantics, which are crucial for generalization. In this paper, we propose SA^2GFM, a robust GFM framework that improves domain-adaptive representations through Structure-Aware Semantic Augmentation. First, we encode hierarchical structural priors by transforming entropy-based encoding trees into structure-aware textual prompts for feature augmentation. The enhanced inputs are processed by a self-supervised Information Bottleneck mechanism that distills robust, transferable representations via structure-guided compression. To address negative transfer in cross-domain adaptation, we introduce an expert adaptive routing mechanism, combining a mixture-of-experts architecture with a null expert design. For efficient downstream adaptation, we propose a fine-tuning module that optimizes hierarchical structures through joint intra- and inter-community structure learning. Extensive experiments demonstrate that SA^2GFM outperforms 9 state-of-the-art baselines in terms of effectiveness and robustness against random noise and adversarial perturbations for node and graph classification.

</details>


### [28] [FAIM: Frequency-Aware Interactive Mamba for Time Series Classification](https://arxiv.org/abs/2512.07858)
*Da Zhang,Bingyu Li,Zhiyuan Zhao,Yanhan Zhang,Junyu Gao,Feiping Nie,Xuelong Li*

Main category: cs.LG

TL;DR: FAIM：一种轻量级的频率感知交互式Mamba模型，用于时间序列分类，通过自适应滤波块和交互式Mamba块实现高效的多粒度信息交互，在多个基准测试中优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 时间序列分类在环境监测、医疗诊断等应用中至关重要，但现有深度学习模型存在计算成本高、对噪声敏感、在小数据集上容易过拟合等问题，需要更轻量高效的解决方案。

Method: 提出FAIM模型，包含自适应滤波块（AFB）和交互式Mamba块（IMB）。AFB利用傅里叶变换提取频域特征，采用可学习的自适应阈值动态抑制噪声；IMB促进高效的多粒度信息交互；同时加入自监督预训练机制增强模型鲁棒性。

Result: 在多个基准测试上的广泛实验表明，FAIM始终优于现有的最先进方法，在准确性和效率之间实现了优越的平衡，并在各种领域和高噪声场景中表现出色。

Conclusion: FAIM通过频率感知设计和交互式Mamba架构，有效解决了时间序列分类中的计算效率、噪声鲁棒性和小数据集过拟合问题，为TSC任务提供了强大且高效的解决方案。

Abstract: Time series classification (TSC) is crucial in numerous real-world applications, such as environmental monitoring, medical diagnosis, and posture recognition. TSC tasks require models to effectively capture discriminative information for accurate class identification. Although deep learning architectures excel at capturing temporal dependencies, they often suffer from high computational cost, sensitivity to noise perturbations, and susceptibility to overfitting on small-scale datasets. To address these challenges, we propose FAIM, a lightweight Frequency-Aware Interactive Mamba model. Specifically, we introduce an Adaptive Filtering Block (AFB) that leverages Fourier Transform to extract frequency-domain features from time series data. The AFB incorporates learnable adaptive thresholds to dynamically suppress noise and employs element-wise coupling of global and local semantic adaptive filtering, enabling in-depth modeling of the synergy among different frequency components. Furthermore, we design an Interactive Mamba Block (IMB) to facilitate efficient multi-granularity information interaction, balancing the extraction of fine-grained discriminative features and comprehensive global contextual information, thereby endowing FAIM with powerful and expressive representations for TSC tasks. Additionally, we incorporate a self-supervised pre-training mechanism to enhance FAIM's understanding of complex temporal patterns and improve its robustness across various domains and high-noise scenarios. Extensive experiments on multiple benchmarks demonstrate that FAIM consistently outperforms existing state-of-the-art (SOTA) methods, achieving a superior trade-off between accuracy and efficiency and exhibits outstanding performance.

</details>


### [29] [SetAD: Semi-Supervised Anomaly Learning in Contextual Sets](https://arxiv.org/abs/2512.07863)
*Jianling Gao,Chongyang Tao,Xuelian Lin,Junfeng Liu,Shuai Ma*

Main category: cs.LG

TL;DR: SetAD将半监督异常检测重新定义为集合级任务，通过注意力集合编码器和分级学习目标，直接建模定义异常的复杂群体级交互，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有半监督异常检测方法主要关注单个点或简单点对的评分，这种点中心或对中心的视角忽略了异常的上下文性质（异常通常由相对于群体的偏离定义），也未能充分利用集合组合产生的丰富监督信号，难以利用数据中的高阶交互来学习判别性表示。

Method: 提出SetAD框架，将半监督异常检测重构为集合级异常检测任务。使用基于注意力的集合编码器，通过分级学习目标训练模型学习量化整个集合的异常程度。此外，提出上下文校准的异常评分机制，通过聚合点在多个不同上下文集合中相对于同伴行为的归一化偏离来评估点的异常分数。

Result: 在10个真实世界数据集上的广泛实验表明，SetAD显著优于最先进的模型。特别值得注意的是，模型性能随着集合大小的增加而持续提升，为基于集合的异常检测公式提供了强有力的实证支持。

Conclusion: SetAD通过集合级视角重新定义异常检测，直接建模群体级交互，提供了一种更自然、更有效的半监督异常检测方法，能够更好地利用有限的标注数据并学习判别性表示。

Abstract: Semi-supervised anomaly detection (AD) has shown great promise by effectively leveraging limited labeled data. However, existing methods are typically structured around scoring individual points or simple pairs. Such {point- or pair-centric} view not only overlooks the contextual nature of anomalies, which are defined by their deviation from a collective group, but also fails to exploit the rich supervisory signals that can be generated from the combinatorial composition of sets. Consequently, such models struggle to exploit the high-order interactions within the data, which are critical for learning discriminative representations. To address these limitations, we propose SetAD, a novel framework that reframes semi-supervised AD as a Set-level Anomaly Detection task. SetAD employs an attention-based set encoder trained via a graded learning objective, where the model learns to quantify the degree of anomalousness within an entire set. This approach directly models the complex group-level interactions that define anomalies. Furthermore, to enhance robustness and score calibration, we propose a context-calibrated anomaly scoring mechanism, which assesses a point's anomaly score by aggregating its normalized deviations from peer behavior across multiple, diverse contextual sets. Extensive experiments on 10 real-world datasets demonstrate that SetAD significantly outperforms state-of-the-art models. Notably, we show that our model's performance consistently improves with increasing set size, providing strong empirical support for the set-based formulation of anomaly detection.

</details>


### [30] [Pattern Recognition of Ozone-Depleting Substance Exports in Global Trade Data](https://arxiv.org/abs/2512.07864)
*Muhammad Sukri Bin Ramli*

Main category: cs.LG

TL;DR: 本文提出了一种基于无监督机器学习的框架，用于分析海关数据以监测环境条约（如蒙特利尔议定书）的执行情况，通过聚类、异常检测和启发式标记识别可疑贸易模式，并生成优先级评分供监管部门审查。


<details>
  <summary>Details</summary>
Motivation: 需要新方法来监测环境条约（如蒙特利尔议定书）的执行情况，通过审查庞大复杂的海关数据集来识别违规贸易活动。

Method: 采用无监督机器学习框架，结合K-Means聚类发现贸易原型，使用Isolation Forest和IQR进行异常检测识别"巨型贸易"和异常价格，辅以启发式标记识别模糊描述等规避策略，最终将这些层组合成优先级评分。

Result: 应用于10万条贸易记录，成功识别出1,351个价格异常值和1,288个高优先级货物供海关审查。高优先级商品显示出与普通商品不同的价值重量比。通过可解释AI（SHAP）验证，确认模糊描述和高价值是最重要的风险预测因子。模型成功检测到2021年初"巨型贸易"的激增，这与美国AIM法案的实际监管影响直接相关。

Conclusion: 这项工作提出了一个可重复的无监督学习流程，能够将原始贸易数据转化为优先排序、可用的监管情报，为监管机构提供有效的监测工具。

Abstract: New methods are needed to monitor environmental treaties, like the Montreal Protocol, by reviewing large, complex customs datasets. This paper introduces a framework using unsupervised machine learning to systematically detect suspicious trade patterns and highlight activities for review. Our methodology, applied to 100,000 trade records, combines several ML techniques. Unsupervised Clustering (K-Means) discovers natural trade archetypes based on shipment value and weight. Anomaly Detection (Isolation Forest and IQR) identifies rare "mega-trades" and shipments with commercially unusual price-per-kilogram values. This is supplemented by Heuristic Flagging to find tactics like vague shipment descriptions. These layers are combined into a priority score, which successfully identified 1,351 price outliers and 1,288 high-priority shipments for customs review. A key finding is that high-priority commodities show a different and more valuable value-to-weight ratio than general goods. This was validated using Explainable AI (SHAP), which confirmed vague descriptions and high value as the most significant risk predictors. The model's sensitivity was validated by its detection of a massive spike in "mega-trades" in early 2021, correlating directly with the real-world regulatory impact of the US AIM Act. This work presents a repeatable unsupervised learning pipeline to turn raw trade data into prioritized, usable intelligence for regulatory groups.

</details>


### [31] [Using Text-Based Life Trajectories from Swedish Register Data to Predict Residential Mobility with Pretrained Transformers](https://arxiv.org/abs/2512.07865)
*Philipp Stark,Alexandros Sopasakis,Ola Hall,Markus Grillitsch*

Main category: cs.LG

TL;DR: 将瑞典大规模登记数据转化为文本化生命轨迹，解决分类变量高基数和编码不一致问题，预测居住流动性，比较多种NLP模型效果


<details>
  <summary>Details</summary>
Motivation: 解决数据分析中长期存在的两个挑战：分类变量的高基数和随时间变化的编码方案不一致性，利用瑞典独特的全面人口登记数据进行纵向预测

Method: 将690万个体（2001-2013）的登记数据转化为语义丰富的文本生命轨迹，结合人口统计信息和年度居住、工作、教育、收入、家庭状况变化，使用LSTM、DistilBERT、BERT、Qwen等多种NLP架构预测2013-2017年的居住流动性

Result: 序列化和基于Transformer的模型比基线模型更有效地捕捉时间和语义结构，文本化的登记数据保留了有关个体路径的有意义信息，支持复杂、可扩展的建模

Conclusion: 语义丰富的登记数据与现代语言模型相结合，可以显著推进社会科学中的纵向分析，为开发和评估新的序列建模方法提供了严格的测试平台

Abstract: We transform large-scale Swedish register data into textual life trajectories to address two long-standing challenges in data analysis: high cardinality of categorical variables and inconsistencies in coding schemes over time. Leveraging this uniquely comprehensive population register, we convert register data from 6.9 million individuals (2001-2013) into semantically rich texts and predict individuals' residential mobility in later years (2013-2017). These life trajectories combine demographic information with annual changes in residence, work, education, income, and family circumstances, allowing us to assess how effectively such sequences support longitudinal prediction. We compare multiple NLP architectures (including LSTM, DistilBERT, BERT, and Qwen) and find that sequential and transformer-based models capture temporal and semantic structure more effectively than baseline models. The results show that textualized register data preserves meaningful information about individual pathways and supports complex, scalable modeling. Because few countries maintain longitudinal microdata with comparable coverage and precision, this dataset enables analyses and methodological tests that would be difficult or impossible elsewhere, offering a rigorous testbed for developing and evaluating new sequence-modeling approaches. Overall, our findings demonstrate that combining semantically rich register data with modern language models can substantially advance longitudinal analysis in social sciences.

</details>


### [32] [Advancing physiological time series reconstruction and imputation via mixture of receptive fields and experts fusion](https://arxiv.org/abs/2512.07873)
*Ci Zhang,Huayu Li,Changdi Yang,Jiangnan Xia,Yanzhi Wang,Xiaolong Ma,Jin Lu,Geng Yuan*

Main category: cs.LG

TL;DR: 提出基于混合专家(MoE)的噪声估计器在分数扩散框架中，用于医学时间序列信号重建，通过RFAMoE模块自适应选择感受野，Fusion MoE模块并行生成多个噪声信号并融合，单次推理完成重建，显著提升性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 医学时间序列信号具有多变量、高时间变异性、高噪声和易受伪影影响等独特特性，使得基于深度学习的方法在插补等任务中仍然具有挑战性。现有扩散模型在医学时间序列领域的应用尚未充分探索，且多推理平均方法虽然能减少重建误差但计算成本高。

Method: 1. 提出基于混合专家(MoE)的噪声估计器，在分数扩散框架中工作；2. 设计RFAMoE模块，使每个通道在扩散过程中自适应选择所需感受野；3. 设计Fusion MoE模块，利用MoE特性并行生成K个噪声信号，通过路由机制融合，单次推理完成信号重建。

Result: 提出的框架在不同任务和数据集上持续优于基于扩散的SOTA方法，不仅提升了性能，还消除了多次推理过程带来的显著计算成本和延迟。

Conclusion: 该研究提出了一种创新的MoE-based扩散框架，有效解决了医学时间序列信号重建中的挑战，通过自适应感受野选择和并行噪声生成融合机制，实现了性能提升和计算效率的平衡。

Abstract: Recent studies show that using diffusion models for time series signal reconstruc- tion holds great promise. However, such approaches remain largely unexplored in the domain of medical time series. The unique characteristics of the physiological time series signals, such as multivariate, high temporal variability, highly noisy, and artifact-prone, make deep learning-based approaches still challenging for tasks such as imputation. Hence, we propose a novel Mixture of Experts (MoE)-based noise estimator within a score-based diffusion framework. Specifically, the Receptive Field Adaptive MoE (RFAMoE) module is designed to enable each channel to adap- tively select desired receptive fields throughout the diffusion process. Moreover, recent literature has found that when generating a physiological signal, performing multiple inferences and averaging the reconstructed signals can effectively reduce reconstruction errors, but at the cost of significant computational and latency over- head. We design a Fusion MoE module and innovatively leverage the nature of MoE module to generate K noise signals in parallel, fuse them using a routing mechanism, and complete signal reconstruction in a single inference step. This design not only improves performance over previous methods but also eliminates the substantial computational cost and latency associated with multiple inference processes. Extensive results demonstrate that our proposed framework consistently outperforms diffusion-based SOTA works on different tasks and datasets.

</details>


### [33] [Graph Contrastive Learning via Spectral Graph Alignment](https://arxiv.org/abs/2512.07878)
*Manh Nguyen,Joshua Cape*

Main category: cs.LG

TL;DR: 提出SpecMatch-CL损失函数，通过最小化视图特定图-图的归一化拉普拉斯矩阵差异来对齐图嵌入，在无监督和半监督图学习任务中取得SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有对比学习方法（如InfoNCE）优化图嵌入的成对对齐，但无法控制由这些嵌入构建的视图特定图-图的全局结构。需要一种机制来对齐这些图-图的全局结构。

Method: 提出SpecMatch-CL损失函数，通过最小化视图特定图-图的归一化拉普拉斯矩阵差异来对齐图嵌入。理论上证明归一化拉普拉斯矩阵差异为理想完美对齐对比损失与当前损失之间的差异提供了上界。

Result: 在8个TU基准测试的无监督学习和低标签率半监督学习中达到新的SOTA，在PPI-306K和ZINC 2M数据集的迁移学习中获得一致性能提升。

Conclusion: SpecMatch-CL通过对齐视图特定图-图的全局结构，显著提升了图对比学习的性能，为图表示学习提供了新的有效方法。

Abstract: Given augmented views of each input graph, contrastive learning methods (e.g., InfoNCE) optimize pairwise alignment of graph embeddings across views while providing no mechanism to control the global structure of the view specific graph-of-graphs built from these embeddings. We introduce SpecMatch-CL, a novel loss function that aligns the view specific graph-of-graphs by minimizing the difference between their normalized Laplacians. Theoretically, we show that under certain assumptions, the difference between normalized Laplacians provides an upper bound not only for the difference between the ideal Perfect Alignment contrastive loss and the current loss, but also for the Uniformly loss. Empirically, SpecMatch-CL establishes new state of the art on eight TU benchmarks under unsupervised learning and semi-supervised learning at low label rates, and yields consistent gains in transfer learning on PPI-306K and ZINC 2M datasets.

</details>


### [34] [Nonnegative Matrix Factorization through Cone Collapse](https://arxiv.org/abs/2512.07879)
*Manh Nguyen,Daniel Pimentel-Alarcón*

Main category: cs.LG

TL;DR: 论文提出Cone Collapse算法，从几何视角重新审视NMF，通过收缩非负象限来恢复数据的最小生成锥，并在此基础上开发了锥感知正交NMF模型（CC-NMF），在聚类任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有NMF算法主要从优化视角出发，未能充分利用NMF诱导的锥几何特性。数据点位于凸锥中，其极端射线编码了基本方向或"主题"，从几何角度理解NMF可以带来更好的聚类性能。

Method: 提出Cone Collapse算法：从完整的非负象限开始，迭代收缩锥体朝向数据生成的最小锥；在恢复的极端射线上应用单正交NMF，得到锥感知正交NMF模型（CC-NMF）。

Result: 在16个基准基因表达、文本和图像数据集上，CC-NMF在聚类纯度方面一致匹配或优于强基线方法（包括乘法更新、ANLS、投影NMF、ONMF和稀疏NMF）。

Conclusion: 显式恢复数据锥可以产生既有理论依据又有实证优势的NMF聚类方法，几何视角为NMF提供了新的理解和改进途径。

Abstract: Nonnegative matrix factorization (NMF) is a widely used tool for learning parts-based, low-dimensional representations of nonnegative data, with applications in vision, text, and bioinformatics. In clustering applications, orthogonal NMF (ONMF) variants further impose (approximate) orthogonality on the representation matrix so that its rows behave like soft cluster indicators. Existing algorithms, however, are typically derived from optimization viewpoints and do not explicitly exploit the conic geometry induced by NMF: data points lie in a convex cone whose extreme rays encode fundamental directions or "topics". In this work we revisit NMF from this geometric perspective and propose Cone Collapse, an algorithm that starts from the full nonnegative orthant and iteratively shrinks it toward the minimal cone generated by the data. We prove that, under mild assumptions on the data, Cone Collapse terminates in finitely many steps and recovers the minimal generating cone of $\mathbf{X}^\top$ . Building on this basis, we then derive a cone-aware orthogonal NMF model (CC-NMF) by applying uni-orthogonal NMF to the recovered extreme rays. Across 16 benchmark gene-expression, text, and image datasets, CC-NMF consistently matches or outperforms strong NMF baselines-including multiplicative updates, ANLS, projective NMF, ONMF, and sparse NMF-in terms of clustering purity. These results demonstrate that explicitly recovering the data cone can yield both theoretically grounded and empirically strong NMF-based clustering methods.

</details>


### [35] [Semi-Supervised Contrastive Learning with Orthonormal Prototypes](https://arxiv.org/abs/2512.07880)
*Huanran Li,Manh Nguyen,Daniel Pimentel-Alarcón*

Main category: cs.LG

TL;DR: 论文提出CLOP损失函数，通过促进类别嵌入形成正交线性子空间来防止对比学习中的维度坍缩问题，在图像分类和目标检测任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 对比学习在深度学习中表现出色，但维度坍缩问题（嵌入收敛到低维空间）在半监督和自监督设置中构成重大挑战。作者首先识别了学习率阈值，超过该阈值标准对比损失会收敛到坍缩解。

Method: 提出CLOP（Contrastive Learning with Orthogonal Projections）损失函数，通过促进类别嵌入形成正交线性子空间来防止维度坍缩。该方法建立在识别学习率阈值的基础上，设计专门针对半监督设置的损失函数。

Result: 在真实和合成数据集上的广泛实验表明，CLOP在图像分类和目标检测任务中提高了性能，同时在不同学习率和批次大小下表现出更好的稳定性。

Conclusion: CLOP通过防止维度坍缩有效解决了对比学习中的关键问题，为半监督对比学习提供了更稳定和高效的解决方案，在多种视觉任务中展现出优越性能。

Abstract: Contrastive learning has emerged as a powerful method in deep learning, excelling at learning effective representations through contrasting samples from different distributions. However, dimensional collapse, where embeddings converge into a lower-dimensional space, poses a significant challenge, especially in semi-supervised and self-supervised setups. In this paper, we first identify a critical learning-rate threshold, beyond which standard contrastive losses converge to collapsed solutions. Building on these insights, we propose CLOP, a novel semi-supervised loss function designed to prevent dimensional collapse by promoting the formation of orthogonal linear subspaces among class embeddings. Through extensive experiments on real and synthetic datasets, we demonstrate that CLOP improves performance in image classification and object detection tasks while also exhibiting greater stability across different learning rates and batch sizes.

</details>


### [36] [GSPN-2: Efficient Parallel Sequence Modeling](https://arxiv.org/abs/2512.07884)
*Hongjun Wang,Yitong Jiang,Collin McCarthy,David Wehr,Hanrong Ye,Xinhao Li,Ka Chun Cheung,Wonmin Byeon,Jinwei Gu,Ke Chen,Kai Han,Hongxu Yin,Pavlo Molchanov,Jan Kautz,Sifei Liu*

Main category: cs.LG

TL;DR: GSPN-2是对GSPN的算法-系统联合重新设计，通过减少GPU内核启动、优化内存访问和引入紧凑通道传播策略，显著提升了视觉Transformer在高分辨率图像和长视频应用中的效率。


<details>
  <summary>Details</summary>
Motivation: 现有GSPN实现虽然通过线扫描传播方案降低了计算复杂度，但仍存在GPU内核重复启动开销大、全局内存数据传输过多以及每个通道单独传播权重导致冗余计算的问题。

Method: 1. 系统优化：将数千个微内核启动合并为单个2D内核，为每个通道片固定一个warp，将前一列的激活暂存到共享内存；2. 算法优化：引入紧凑通道传播策略，替代每通道矩阵，减少参数，并与Transformer注意力中的亲和力图自然对齐。

Result: GSPN-2在图像分类和文本到图像合成任务中表现出色，在保持Transformer级别准确性的同时显著降低了计算成本，为视觉应用中的全局空间上下文建模建立了新的效率前沿。

Conclusion: GSPN-2通过结构化矩阵变换和GPU优化实现的独特组合，为高分辨率图像和长视频相关应用提供了高效的视觉Transformer解决方案。

Abstract: Efficient vision transformer remains a bottleneck for high-resolution images and long-video related real-world applications. Generalized Spatial Propagation Network (GSPN) addresses this by replacing quadratic self-attention with a line-scan propagation scheme, bringing the cost close to linear in the number of rows or columns, while retaining accuracy. Despite this advancement, the existing GSPN implementation still suffers from (i) heavy overhead due to repeatedly launching GPU kernels, (ii) excessive data transfers from global GPU memory, and (iii) redundant computations caused by maintaining separate propagation weights for each channel. We introduce GSPN-2, a joint algorithm-system redesign. In particular, we eliminate thousands of micro-launches from the previous implementation into one single 2D kernel, explicitly pin one warp to each channel slice, and stage the previous column's activations in shared memory. On the model side, we introduce a compact channel propagation strategy that replaces per-channel matrices, trimming parameters, and align naturally with the affinity map used in transformer attention. Experiments demonstrate GSPN-2's effectiveness across image classification and text-to-image synthesis tasks, matching transformer-level accuracy with significantly lower computational cost. GSPN-2 establishes a new efficiency frontier for modeling global spatial context in vision applications through its unique combination of structured matrix transformations and GPU-optimized implementation. Project page: https://whj363636.github.io/GSPN2/

</details>


### [37] [Towards symbolic regression for interpretable clinical decision scores](https://arxiv.org/abs/2512.07961)
*Guilherme Seidyo Imai Aldeia,Joseph D. Romano,Fabricio Olivetti de Franca,Daniel S. Herman,William G. La Cava*

Main category: cs.LG

TL;DR: Brush是一种符号回归算法，将决策树分割与非线性常数优化结合，能够生成包含规则逻辑的临床风险评分模型


<details>
  <summary>Details</summary>
Motivation: 传统符号回归只能搜索连续函数形式，难以建模医疗决策中结合风险方程和规则的算法，而符号回归具有数据驱动、可解释的优势，有望开发数据驱动的临床风险评分

Method: Brush算法结合决策树式分割算法和非线性常数优化，实现规则逻辑与符号回归/分类模型的无缝集成

Result: 在SRBench上达到帕累托最优性能，成功复现两个广泛使用的临床评分系统，获得高准确率和可解释模型；相比决策树、随机森林和其他符号回归方法，Brush获得相当或更优的预测性能，同时生成更简单的模型

Conclusion: Brush算法能够有效建模医疗决策中的规则逻辑，为开发数据驱动的临床风险评分提供了有前景的工具

Abstract: Medical decision-making makes frequent use of algorithms that combine risk equations with rules, providing clear and standardized treatment pathways. Symbolic regression (SR) traditionally limits its search space to continuous function forms and their parameters, making it difficult to model this decision-making. However, due to its ability to derive data-driven, interpretable models, SR holds promise for developing data-driven clinical risk scores. To that end we introduce Brush, an SR algorithm that combines decision-tree-like splitting algorithms with non-linear constant optimization, allowing for seamless integration of rule-based logic into symbolic regression and classification models. Brush achieves Pareto-optimal performance on SRBench, and was applied to recapitulate two widely used clinical scoring systems, achieving high accuracy and interpretable models. Compared to decision trees, random forests, and other SR methods, Brush achieves comparable or superior predictive performance while producing simpler models.

</details>


### [38] [CIP-Net: Continual Interpretable Prototype-based Network](https://arxiv.org/abs/2512.07981)
*Federico Di Valerio,Michela Proietti,Alessio Ragno,Roberto Capobianco*

Main category: cs.LG

TL;DR: CIP-Net：一种无需示例的自解释原型模型，用于持续学习，避免存储过去示例，保持简单架构，同时提供有用解释和强大性能。


<details>
  <summary>Details</summary>
Motivation: 持续学习面临灾难性遗忘的挑战，现有可解释方法大多使用事后解释或需要为每个新任务额外存储内存，导致可扩展性有限。

Method: 引入CIP-Net，一种无需示例的自解释原型模型，避免存储过去示例，保持简单架构，同时提供有用解释。

Result: CIP-Net在任务增量和类别增量设置中，相比之前的无示例和自解释方法实现了最先进的性能，同时显著降低了内存相关开销。

Conclusion: CIP-Net为持续学习提供了一个实用且可解释的解决方案，在保持高性能的同时减少了内存开销。

Abstract: Continual learning constrains models to learn new tasks over time without forgetting what they have already learned. A key challenge in this setting is catastrophic forgetting, where learning new information causes the model to lose its performance on previous tasks. Recently, explainable AI has been proposed as a promising way to better understand and reduce forgetting. In particular, self-explainable models are useful because they generate explanations during prediction, which can help preserve knowledge. However, most existing explainable approaches use post-hoc explanations or require additional memory for each new task, resulting in limited scalability. In this work, we introduce CIP-Net, an exemplar-free self-explainable prototype-based model designed for continual learning. CIP-Net avoids storing past examples and maintains a simple architecture, while still providing useful explanations and strong performance. We demonstrate that CIPNet achieves state-of-the-art performances compared to previous exemplar-free and self-explainable methods in both task- and class-incremental settings, while bearing significantly lower memory-related overhead. This makes it a practical and interpretable solution for continual learning.

</details>


### [39] [Bridging the Clinical Expertise Gap: Development of a Web-Based Platform for Accessible Time Series Forecasting and Analysis](https://arxiv.org/abs/2512.07992)
*Aaron D. Mullen,Daniel R. Harris,Svetla Slavova,V. K. Cody Bumgardner*

Main category: cs.LG

TL;DR: 开发了一个面向医疗健康领域的时间序列预测Web平台，降低技术门槛，让研究人员和临床医生能够轻松进行数据分析、模型训练和结果解释


<details>
  <summary>Details</summary>
Motivation: 时间序列预测在医疗健康领域有广泛应用，但技术门槛较高，需要专业知识进行数据分析、模型构建和结果解释，这限制了研究人员和临床医生的使用

Method: 开发了一个Web平台，支持数据上传、可视化分析、多种可定制化的预测模型训练，并集成大型语言模型提供参数建议和结果解释

Result: 创建了一个易用的平台，使非技术用户能够进行时间序列预测分析，支持多种模型和技术，提供智能建议和解释功能

Conclusion: 该平台降低了时间序列预测的技术门槛，旨在集成到学习型健康系统中，实现临床数据管道的持续收集和推断

Abstract: Time series forecasting has applications across domains and industries, especially in healthcare, but the technical expertise required to analyze data, build models, and interpret results can be a barrier to using these techniques. This article presents a web platform that makes the process of analyzing and plotting data, training forecasting models, and interpreting and viewing results accessible to researchers and clinicians. Users can upload data and generate plots to showcase their variables and the relationships between them. The platform supports multiple forecasting models and training techniques which are highly customizable according to the user's needs. Additionally, recommendations and explanations can be generated from a large language model that can help the user choose appropriate parameters for their data and understand the results for each model. The goal is to integrate this platform into learning health systems for continuous data collection and inference from clinical pipelines.

</details>


### [40] [CLARITY: Medical World Model for Guiding Treatment Decisions by Modeling Context-Aware Disease Trajectories in Latent Space](https://arxiv.org/abs/2512.08029)
*Tianxingjian Ding,Yuanhao Zou,Chen Chen,Mubarak Shah,Yu Tian*

Main category: cs.LG

TL;DR: CLARITY是一个医学世界模型，通过在结构化潜在空间中直接预测疾病演化，整合时间和临床上下文，生成个体化治疗计划，并在胶质瘤数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前肿瘤临床决策需要预测动态疾病演化，但现有静态AI预测器无法完成此任务。现有医学世界模型通常忽略患者特定的时间和临床上下文，缺乏将预测与治疗决策连接的反馈机制。

Method: CLARITY是一个医学世界模型，在结构化潜在空间中直接预测疾病演化。它明确整合时间间隔（时间上下文）和患者特定数据（临床上下文），将治疗条件下的进展建模为平滑、可解释的轨迹，从而生成生理上可信的个体化治疗计划。此外，CLARITY引入了新颖的预测到决策框架，将潜在推演转化为透明、可操作的推荐。

Result: CLARITY在治疗规划方面展示了最先进的性能。在MU-Glioma-Post数据集上，该方法比最近的MeWM高出12%，并显著超越所有其他医学特定的大型语言模型。

Conclusion: CLARITY通过整合时间和临床上下文，在结构化潜在空间中直接预测疾病演化，提供了一个能够生成个体化治疗计划并转化为可操作决策的医学世界模型框架。

Abstract: Clinical decision-making in oncology requires predicting dynamic disease evolution, a task current static AI predictors cannot perform. While world models (WMs) offer a paradigm for generative prediction, existing medical applications remain limited. Existing methods often rely on stochastic diffusion models, focusing on visual reconstruction rather than causal, physiological transitions. Furthermore, in medical domain, models like MeWM typically ignore patient-specific temporal and clinical contexts and lack a feedback mechanism to link predictions to treatment decisions. To address these gaps, we introduce CLARITY, a medical world model that forecasts disease evolution directly within a structured latent space. It explicitly integrates time intervals (temporal context) and patient-specific data (clinical context) to model treatment-conditioned progression as a smooth, interpretable trajectory, and thus generate physiologically faithful, individualized treatment plans. Finally, CLARITY introduces a novel prediction-to-decision framework, translating latent rollouts into transparent, actionable recommendations. CLARITY demonstrates state-of-the-art performance in treatment planning. On the MU-Glioma-Post dataset, our approach outperforms recent MeWM by 12\%, and significantly surpasses all other medical-specific large language models.

</details>


### [41] [LUNA: Linear Universal Neural Attention with Generalization Guarantees](https://arxiv.org/abs/2512.08061)
*Ashkan Shahbazi,Ping He,Ali Abbasi,Yikun Bai,Xinran Liu,Elaheh Akbari,Darian Salehi,Navid NaderiAlizadeh,Soheil Kolouri*

Main category: cs.LG

TL;DR: LUNA是一种核化线性注意力机制，通过可学习的核特征映射在保持线性计算成本的同时达到甚至超越二次注意力的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统softmax注意力存在O(n²)的二次计算成本，限制了其在长序列领域的应用。现有线性注意力机制虽然将成本降至O(n)，但依赖固定的随机特征映射，导致模型精度与计算效率之间存在根本性权衡。

Method: LUNA采用可学习的核特征映射，而不是固定的先验特征映射。该方法参数化核函数，学习针对特定数据和任务的特征基，同时保持正定核性质并支持流式处理，实现序列长度的线性时间和内存扩展。

Result: 在Long Range Arena基准测试中，LUNA在计算对等条件下（相同参数量、训练步数和近似FLOPs）取得了高效Transformer中的最先进平均准确率。在后验转换实验中，将微调后的BERT和ViT-B/16检查点中的softmax替换为LUNA并简要微调，能够恢复大部分原始性能，显著优于固定线性化方法。

Conclusion: LUNA通过可学习的核特征映射消除了线性注意力中精度与效率的权衡，在保持线性计算成本的同时实现了与二次注意力相当甚至更优的性能，为长序列处理提供了有效的解决方案。

Abstract: Scaling attention faces a critical bottleneck: the $\mathcal{O}(n^2)$ quadratic computational cost of softmax attention, which limits its application in long-sequence domains. While linear attention mechanisms reduce this cost to $\mathcal{O}(n)$, they typically rely on fixed random feature maps, such as random Fourier features or hand-crafted functions. This reliance on static, data-agnostic kernels creates a fundamental trade-off, forcing practitioners to sacrifice significant model accuracy for computational efficiency. We introduce \textsc{LUNA}, a kernelized linear attention mechanism that eliminates this trade-off, retaining linear cost while matching and surpassing the accuracy of quadratic attention. \textsc{LUNA} is built on the key insight that the kernel feature map itself should be learned rather than fixed a priori. By parameterizing the kernel, \textsc{LUNA} learns a feature basis tailored to the specific data and task, overcoming the expressive limitations of fixed-feature methods. \textsc{Luna} implements this with a learnable feature map that induces a positive-definite kernel and admits a streaming form, yielding linear time and memory scaling in the sequence length. Empirical evaluations validate our approach across diverse settings. On the Long Range Arena (LRA), \textsc{Luna} achieves state-of-the-art average accuracy among efficient Transformers under compute parity, using the same parameter count, training steps, and approximate FLOPs. \textsc{Luna} also excels at post-hoc conversion: replacing softmax in fine-tuned BERT and ViT-B/16 checkpoints and briefly fine-tuning recovers most of the original performance, substantially outperforming fixed linearizations.

</details>


### [42] [Deep Kernel Aalen-Johansen Estimator: An Interpretable and Flexible Neural Net Framework for Competing Risks](https://arxiv.org/abs/2512.08063)
*Xiaobin Shen,George H. Chen*

Main category: cs.LG

TL;DR: 提出DKAJ模型，将经典Aalen-Johansen估计器推广到深度学习框架，通过自动学习核函数衡量数据点相似性，在保持竞争性能的同时提供可视化解释。


<details>
  <summary>Details</summary>
Motivation: 传统Aalen-Johansen估计器在竞争风险分析中是非参数方法，但缺乏对个体异质性的建模能力。需要一种既能保持经典方法优势，又能提供个体化预测和解释性的深度学习方法。

Method: DKAJ模型将每个数据点表示为聚类的加权组合，权重来自自动学习的核函数，用于衡量数据点之间的相似性。当数据点仅对一个聚类有非零权重时，其预测的累积发生率函数对应于该聚类的经典Aalen-Johansen估计。

Result: 在四个标准竞争风险数据集上，DKAJ与最先进的基线方法性能相当，同时能够提供可视化辅助模型解释。

Conclusion: DKAJ成功地将经典Aalen-Johansen估计器推广到深度学习框架，在保持预测性能的同时提供了更好的模型可解释性，为竞争风险分析提供了新的工具。

Abstract: We propose an interpretable deep competing risks model called the Deep Kernel Aalen-Johansen (DKAJ) estimator, which generalizes the classical Aalen-Johansen nonparametric estimate of cumulative incidence functions (CIFs). Each data point (e.g., patient) is represented as a weighted combination of clusters. If a data point has nonzero weight only for one cluster, then its predicted CIFs correspond to those of the classical Aalen-Johansen estimator restricted to data points from that cluster. These weights come from an automatically learned kernel function that measures how similar any two data points are. On four standard competing risks datasets, we show that DKAJ is competitive with state-of-the-art baselines while being able to provide visualizations to assist model interpretation.

</details>


### [43] [CAMO: Causality-Guided Adversarial Multimodal Domain Generalization for Crisis Classification](https://arxiv.org/abs/2512.08071)
*Pingchuan Ma,Chengshuai Zhao,Bohan Jiang,Saketh Vishnubhatla,Ujun Jeong,Alimohammad Beigi,Adrienne Raglin,Huan Liu*

Main category: cs.LG

TL;DR: 提出因果引导的多模态域泛化框架，通过对抗解缠和统一表示学习解决社交媒体危机分类中的域泛化问题


<details>
  <summary>Details</summary>
Motivation: 现有社交媒体危机分类方法在未见危机类型上泛化能力差，主要因为：1. 未能解缠虚假特征和因果特征，导致域偏移下性能下降；2. 未能对齐异构模态表示，阻碍单模态域泛化技术向多模态场景的迁移

Method: 提出因果引导的多模态域泛化框架，结合对抗解缠和统一表示学习。对抗目标鼓励模型解缠并关注域不变的因果特征；统一表示将不同模态特征对齐到共享潜在空间，使单模态域泛化策略能无缝扩展到多模态学习

Result: 在不同数据集上的实验表明，该方法在未见灾难场景中取得了最佳性能

Conclusion: 通过因果引导的多模态域泛化框架，能够有效提升社交媒体危机分类在未见危机类型上的泛化能力，为紧急响应提供更可靠的信息支持

Abstract: Crisis classification in social media aims to extract actionable disaster-related information from multimodal posts, which is a crucial task for enhancing situational awareness and facilitating timely emergency responses. However, the wide variation in crisis types makes achieving generalizable performance across unseen disasters a persistent challenge. Existing approaches primarily leverage deep learning to fuse textual and visual cues for crisis classification, achieving numerically plausible results under in-domain settings. However, they exhibit poor generalization across unseen crisis types because they 1. do not disentangle spurious and causal features, resulting in performance degradation under domain shift, and 2. fail to align heterogeneous modality representations within a shared space, which hinders the direct adaptation of established single-modality domain generalization (DG) techniques to the multimodal setting. To address these issues, we introduce a causality-guided multimodal domain generalization (MMDG) framework that combines adversarial disentanglement with unified representation learning for crisis classification. The adversarial objective encourages the model to disentangle and focus on domain-invariant causal features, leading to more generalizable classifications grounded in stable causal mechanisms. The unified representation aligns features from different modalities within a shared latent space, enabling single-modality DG strategies to be seamlessly extended to multimodal learning. Experiments on the different datasets demonstrate that our approach achieves the best performance in unseen disaster scenarios.

</details>


### [44] [Scalable Offline Model-Based RL with Action Chunks](https://arxiv.org/abs/2512.08108)
*Kwanyoung Park,Seohong Park,Youngwoon Lee,Sergey Levine*

Main category: cs.LG

TL;DR: MAC提出了一种基于模型的离线强化学习方法，通过动作块模型减少长期预测误差，结合拒绝采样避免模型利用，在复杂长时域任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决基于模型的强化学习在离线设置中处理复杂长时域任务时的挑战：传统模型基于价值扩展方法在增加预测步长n时，虽然减少了价值引导的偏差，但会放大模型误差的累积，导致长期预测性能下降。

Method: 1. 动作块模型：预测从动作序列（动作块）到未来状态，而不是单步动作，减少误差累积；2. 拒绝采样：从表达性强的行为动作块策略中进行拒绝采样，避免模型被分布外动作利用；3. 整体框架称为MAC（Model-Based RL with Action Chunks）。

Result: 在包含高达1亿个转移的大规模数据集上的高度挑战性任务中，MAC在离线基于模型的强化学习算法中取得了最佳性能，特别是在挑战性的长时域任务上表现突出。

Conclusion: MAC通过动作块模型减少长期预测误差，结合拒绝采样策略，为离线基于模型的强化学习处理复杂长时域任务提供了可扩展的解决方案。

Abstract: In this paper, we study whether model-based reinforcement learning (RL), in particular model-based value expansion, can provide a scalable recipe for tackling complex, long-horizon tasks in offline RL. Model-based value expansion fits an on-policy value function using length-n imaginary rollouts generated by the current policy and a learned dynamics model. While larger n reduces bias in value bootstrapping, it amplifies accumulated model errors over long horizons, degrading future predictions. We address this trade-off with an \emph{action-chunk} model that predicts a future state from a sequence of actions (an "action chunk") instead of a single action, which reduces compounding errors. In addition, instead of directly training a policy to maximize rewards, we employ rejection sampling from an expressive behavioral action-chunk policy, which prevents model exploitation from out-of-distribution actions. We call this recipe \textbf{Model-Based RL with Action Chunks (MAC)}. Through experiments on highly challenging tasks with large-scale datasets of up to 100M transitions, we show that MAC achieves the best performance among offline model-based RL algorithms, especially on challenging long-horizon tasks.

</details>


### [45] [Balanced Accuracy: The Right Metric for Evaluating LLM Judges - Explained through Youden's J statistic](https://arxiv.org/abs/2512.08121)
*Stephane Collot,Colin Fraser,Justin Zhao,William F. Shen,Timon Willi,Ilias Leontiadis*

Main category: cs.LG

TL;DR: 论文提出在评估大语言模型时，使用Youden's J统计量或平衡准确率来选择分类器（评判者），比传统指标更可靠。


<details>
  <summary>Details</summary>
Motivation: 当前评估大语言模型时依赖分类器（如LLM-as-a-judge或人工标注者）来估计行为发生率，但常用的准确率、精确率、F1等指标对类别不平衡和正类选择敏感，可能导致选择扭曲发生率估计的分类器。

Method: 提出使用Youden's J统计量作为选择最佳评判者的理论基础，证明平衡准确率是J的线性变换，通过理论分析和实证模拟验证该方法。

Result: Youden's J统计量在理论上与选择最佳模型比较评判者一致，平衡准确率作为等效指标，能产生更好、更稳健的分类器选择。

Conclusion: 在评估大语言模型时，应使用Youden's J统计量或平衡准确率来选择分类器，以获得更可靠、更稳健的模型比较结果。

Abstract: Rigorous evaluation of large language models (LLMs) relies on comparing models by the prevalence of desirable or undesirable behaviors, such as task pass rates or policy violations. These prevalence estimates are produced by a classifier, either an LLM-as-a-judge or human annotators, making the choice of classifier central to trustworthy evaluation. Common metrics used for this choice, such as Accuracy, Precision, and F1, are sensitive to class imbalance and to arbitrary choices of positive class, and can favor judges that distort prevalence estimates. We show that Youden's $J$ statistic is theoretically aligned with choosing the best judge to compare models, and that Balanced Accuracy is an equivalent linear transformation of $J$. Through both analytical arguments and empirical examples and simulations, we demonstrate how selecting judges using Balanced Accuracy leads to better, more robust classifier selection.

</details>


### [46] [Improving the Sensitivity of Backdoor Detectors via Class Subspace Orthogonalization](https://arxiv.org/abs/2512.08129)
*Guangmingmei Yang,David J. Miller,George Kesidis*

Main category: cs.LG

TL;DR: 提出CSO方法，通过正交化抑制内在特征，增强后门检测的敏感性，解决传统方法在易区分类别和隐蔽后门攻击上的失效问题


<details>
  <summary>Details</summary>
Motivation: 传统后门检测方法依赖目标类别的极端异常统计量，但在两类情况下会失效：1）某些非目标类别天然容易区分，也会产生极端统计量；2）后门特征过于隐蔽，相对于内在类别区分特征较弱。关键观察是后门目标类别的检测统计量同时来自后门触发器和内在特征，而非目标类别仅来自内在特征。

Method: 提出类子空间正交化（CSO）方法：利用少量干净样本，通过约束优化问题，在优化检测统计量的同时，正交化抑制类别的内在特征。对于非目标类别，这种抑制会大幅降低可达到的统计量，而对于目标类别，来自后门触发器的显著贡献仍然保留。

Result: CSO方法作为即插即用方案，在具有挑战性的混合标签攻击和自适应攻击中进行了评估，显示出更强的检测敏感性。

Conclusion: 通过抑制内在特征并保留后门贡献，CSO方法能够更敏感地检测后门攻击，特别是针对传统方法难以处理的易区分类别和隐蔽后门场景。

Abstract: Most post-training backdoor detection methods rely on attacked models exhibiting extreme outlier detection statistics for the target class of an attack, compared to non-target classes. However, these approaches may fail: (1) when some (non-target) classes are easily discriminable from all others, in which case they may naturally achieve extreme detection statistics (e.g., decision confidence); and (2) when the backdoor is subtle, i.e., with its features weak relative to intrinsic class-discriminative features. A key observation is that the backdoor target class has contributions to its detection statistic from both the backdoor trigger and from its intrinsic features, whereas non-target classes only have contributions from their intrinsic features. To achieve more sensitive detectors, we thus propose to suppress intrinsic features while optimizing the detection statistic for a given class. For non-target classes, such suppression will drastically reduce the achievable statistic, whereas for the target class the (significant) contribution from the backdoor trigger remains. In practice, we formulate a constrained optimization problem, leveraging a small set of clean examples from a given class, and optimizing the detection statistic while orthogonalizing with respect to the class's intrinsic features. We dub this plug-and-play approach Class Subspace Orthogonalization (CSO) and assess it against challenging mixed-label and adaptive attacks.

</details>


### [47] [Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models I: The Task-Query Architecture](https://arxiv.org/abs/2512.08130)
*Gary Ackerman,Brandon Behlendorf,Zachary Kallenborn,Sheriff Almakki,Doug Clifford,Jenna LaTourette,Hayley Peterson,Noah Sheinbaum,Olivia Shoemaker,Anna Wetzel*

Main category: cs.LG

TL;DR: 本文提出了一个生物威胁基准生成框架，专门用于评估AI模型（特别是大语言模型）在生物安全风险方面的潜在危害，重点关注细菌生物威胁。


<details>
  <summary>Details</summary>
Motivation: 随着前沿AI模型的快速发展，需要量化这些模型（特别是大语言模型）可能被用于生物恐怖主义或获取生物武器的风险。当前缺乏能够全面评估这种生物安全风险的基准测试方法。

Method: 开发了一个生物威胁基准生成框架，采用分层结构的生物威胁类别、要素和任务，构建了细菌生物威胁模式，并以此为基础开发任务对齐的查询。

Result: 提出了细菌生物威胁模式作为BBG框架的第一个组件，为评估AI模型的生物安全风险提供了可重复使用的结构框架。

Conclusion: BBG框架（包括细菌生物威胁模式）旨在提供一个稳健、可重复使用的结构，用于评估LLM在细菌生物风险方面的潜在危害，涵盖技术性和操作性风险因素，并考虑不同攻击者能力水平。

Abstract: Both model developers and policymakers seek to quantify and mitigate the risk of rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons. An important element of such efforts is the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper describes the first component of a novel Biothreat Benchmark Generation (BBG) Framework. The BBG approach is designed to help model developers and evaluators reliably measure and assess the biosecurity risk uplift and general harm potential of existing and future AI models, while accounting for key aspects of the threat itself that are often overlooked in other benchmarking efforts, including different actor capability levels, and operational (in addition to purely technical) risk factors. As a pilot, the BBG is first being developed to address bacterial biological threats only. The BBG is built upon a hierarchical structure of biothreat categories, elements and tasks, which then serves as the basis for the development of task-aligned queries. This paper outlines the development of this biothreat task-query architecture, which we have named the Bacterial Biothreat Schema, while future papers will describe follow-on efforts to turn queries into model prompts, as well as how the resulting benchmarks can be implemented for model evaluation. Overall, the BBG Framework, including the Bacterial Biothreat Schema, seeks to offer a robust, re-usable structure for evaluating bacterial biological risks arising from LLMs across multiple levels of aggregation, which captures the full scope of technical and operational requirements for biological adversaries, and which accounts for a wide spectrum of biological adversary capabilities.

</details>


### [48] [Robust Agents in Open-Ended Worlds](https://arxiv.org/abs/2512.08139)
*Mikayel Samvelyan*

Main category: cs.LG

TL;DR: 该论文提出了一系列方法，通过开放式学习和多智能体学习来训练和评估能够泛化到新环境、分布外输入以及与其他智能体交互的鲁棒AI智能体。


<details>
  <summary>Details</summary>
Motivation: 随着AI在各种应用中的普及，需要能够成功导航和适应不断变化的开放世界的智能体。关键挑战是确保这些AI智能体不仅能在训练期间熟悉的设置中表现出色，还能有效地泛化到以前未见过的多样化场景。

Method: 1. 引入MiniHack：基于NetHack游戏的沙盒框架，通过程序化内容生成创建多样化环境；2. 提出Maestro：生成对抗性课程的新方法，逐步增强RL智能体在双人零和游戏中的鲁棒性和泛化能力；3. 使用质量多样性方法系统识别复杂足球游戏领域中预训练RL策略的漏洞；4. 将鲁棒性探索扩展到LLMs领域，使用进化搜索生成多样化有效输入来诊断和增强LLMs对抗对抗性提示的鲁棒性。

Result: 该研究为AI鲁棒性的未来进展铺平了道路，使智能体不仅能够适应不断发展的世界，还能在面对不可预见的挑战和交互时茁壮成长。

Conclusion: 通过结合开放式学习和多智能体学习方法，该论文提出了一套全面的框架和工具来训练和评估鲁棒AI智能体，这些智能体能够有效泛化到新环境、分布外输入以及与其他智能体的复杂交互，为开发更适应现实世界复杂性的AI系统奠定了基础。

Abstract: The growing prevalence of artificial intelligence (AI) in various applications underscores the need for agents that can successfully navigate and adapt to an ever-changing, open-ended world. A key challenge is ensuring these AI agents are robust, excelling not only in familiar settings observed during training but also effectively generalising to previously unseen and varied scenarios. In this thesis, we harness methodologies from open-endedness and multi-agent learning to train and evaluate robust AI agents capable of generalising to novel environments, out-of-distribution inputs, and interactions with other co-player agents. We begin by introducing MiniHack, a sandbox framework for creating diverse environments through procedural content generation. Based on the game of NetHack, MiniHack enables the construction of new tasks for reinforcement learning (RL) agents with a focus on generalisation. We then present Maestro, a novel approach for generating adversarial curricula that progressively enhance the robustness and generality of RL agents in two-player zero-sum games. We further probe robustness in multi-agent domains, utilising quality-diversity methods to systematically identify vulnerabilities in state-of-the-art, pre-trained RL policies within the complex video game football domain, characterised by intertwined cooperative and competitive dynamics. Finally, we extend our exploration of robustness to the domain of LLMs. Here, our focus is on diagnosing and enhancing the robustness of LLMs against adversarial prompts, employing evolutionary search to generate a diverse range of effective inputs that aim to elicit undesirable outputs from an LLM. This work collectively paves the way for future advancements in AI robustness, enabling the development of agents that not only adapt to an ever-evolving world but also thrive in the face of unforeseen challenges and interactions.

</details>


### [49] [PolyLingua: Margin-based Inter-class Transformer for Robust Cross-domain Language Detection](https://arxiv.org/abs/2512.08143)
*Ali Lotfi Rezaabad,Bikram Khanal,Shashwat Chaurasia,Lu Zeng,Dezhi Hong,Hossein Beshashati,Thomas Butler,Megan Ganji*

Main category: cs.LG

TL;DR: PolyLingua是一个轻量级Transformer模型，用于领域内语言检测和细粒度语言分类，通过两级对比学习框架在计算和延迟受限的环境中实现高精度语言识别。


<details>
  <summary>Details</summary>
Motivation: 语言识别是多语言系统的关键第一步，现有工具在特定场景（如音乐请求）中表现不佳，开源工具速度快但精度低，大型语言模型有效但成本过高，不适合低延迟或低资源环境。

Method: 采用轻量级Transformer架构，结合两级对比学习框架：实例级分离和类级对齐，使用自适应边距，为相近语言生成紧凑且分离良好的嵌入表示。

Result: 在Amazon Massive（多语言数字助手话语）和Song数据集（频繁代码切换的音乐请求）上分别达到99.25%和98.15%的F1分数，超越Sonnet 3.5，同时使用参数减少10倍。

Conclusion: PolyLingua在保持高精度的同时显著减少计算资源需求，特别适合计算和延迟受限的环境，解决了现有语言识别工具在特定场景下的局限性。

Abstract: Language identification is a crucial first step in multilingual systems such as chatbots and virtual assistants, enabling linguistically and culturally accurate user experiences. Errors at this stage can cascade into downstream failures, setting a high bar for accuracy. Yet, existing language identification tools struggle with key cases--such as music requests where the song title and user language differ. Open-source tools like LangDetect, FastText are fast but less accurate, while large language models, though effective, are often too costly for low-latency or low-resource settings. We introduce PolyLingua, a lightweight Transformer-based model for in-domain language detection and fine-grained language classification. It employs a two-level contrastive learning framework combining instance-level separation and class-level alignment with adaptive margins, yielding compact and well-separated embeddings even for closely related languages. Evaluated on two challenging datasets--Amazon Massive (multilingual digital assistant utterances) and a Song dataset (music requests with frequent code-switching)--PolyLingua achieves 99.25% F1 and 98.15% F1, respectively, surpassing Sonnet 3.5 while using 10x fewer parameters, making it ideal for compute- and latency-constrained environments.

</details>


### [50] [TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models](https://arxiv.org/abs/2512.08153)
*Zheng Ding,Weirui Ye*

Main category: cs.LG

TL;DR: TreeGRPO是一种新颖的强化学习框架，通过将去噪过程重构为搜索树，显著提高了生成模型与人类偏好对齐的训练效率，实现2.4倍加速训练。


<details>
  <summary>Details</summary>
Motivation: 强化学习后训练对于将生成模型与人类偏好对齐至关重要，但其高昂的计算成本阻碍了广泛应用。需要开发更高效的训练方法。

Method: 将去噪过程重构为搜索树，从共享的初始噪声样本出发，策略性地分支生成多个候选轨迹，同时高效复用它们的共同前缀。采用树结构方法实现高效样本利用、细粒度信用分配和摊销计算。

Result: 在扩散模型和流模型上的实验表明，TreeGRPO实现了2.4倍的训练加速，在效率-奖励权衡空间中建立了更优的帕累托前沿，在多个基准测试和奖励模型上一致优于GRPO基线。

Conclusion: TreeGRPO为基于强化学习的视觉生成模型对齐提供了一条可扩展且有效的途径，通过树结构方法显著提高了训练效率。

Abstract: Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce \textbf{TreeGRPO}, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) \emph{High sample efficiency}, achieving better performance under same training samples (2) \emph{Fine-grained credit assignment} via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) \emph{Amortized computation} where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves \textbf{2.4$\times$ faster training} while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.

</details>


### [51] [MobileFineTuner: A Unified End-to-End Framework for Fine-Tuning LLMs on Mobile Phones](https://arxiv.org/abs/2512.08211)
*Jiaxiang Geng,Lunyu Zhao,Yiyi Lu,Bing Luo*

Main category: cs.LG

TL;DR: MobileFineTuner是一个开源框架，支持在普通手机上直接进行端到端的大语言模型微调，解决了移动设备内存和能耗限制的问题。


<details>
  <summary>Details</summary>
Motivation: 随着高质量公共数据接近枯竭，利用私有用户数据进行设备端微调成为重要方向，但现有方法主要基于模拟或依赖IoT设备和PC，缺乏针对普通手机的实用开源框架。

Method: 提出MobileFineTuner统一开源框架，支持全参数微调和参数高效微调，引入参数分片、梯度累积和能量感知计算调度等系统级优化来解决移动设备的内存和能耗限制。

Result: 在真实手机上成功微调了GPT-2、Gemma 3和Qwen 2.5模型，通过大量实验和消融研究验证了所提优化的有效性，证明MobileFineTuner是设备端LLM训练研究的可行基础。

Conclusion: MobileFineTuner填补了在普通手机上进行实际LLM微调的开源框架空白，为未来设备端LLM训练研究提供了可行的基础平台。

Abstract: Mobile phones are the most ubiquitous end devices, generating vast amounts of human-authored data and serving as the primary platform for end-side applications. As high-quality public data for large language models (LLMs) approaches exhaustion, on-device fine-tuning provides an opportunity to leverage private user data while preserving privacy. However, existing approaches are predominantly simulation-based or rely on IoT devices and PCs, leaving commodity mobile phones largely unexplored. A key gap is the absence of an open-source framework that enables practical LLM fine-tuning on mobile phones. We present MobileFineTuner, a unified open-source framework that enables end-to-end LLM fine-tuning directly on commodity mobile phones. MobileFineTuner is designed for efficiency, scalability, and usability, supporting full-parameters fine-tuning (Full-FT) and parameter-efficient fine-tuning (PEFT). To address the memory and energy limitations inherent to mobile phones, we introduce system-level optimizations including parameter sharding, gradient accumulation, and energy-aware computation scheduling. We demonstrate the practicality of MobileFineTuner by fine-tuning GPT-2, Gemma 3, and Qwen 2.5 on real mobile phones. Extensive experiments and ablation studies validate the effectiveness of the proposed optimizations and establish MobileFineTuner as a viable foundation for future research on on-device LLM training.

</details>


### [52] [Correction of Decoupled Weight Decay](https://arxiv.org/abs/2512.08217)
*Jason Chuan-Chih Chou*

Main category: cs.LG

TL;DR: 该论文挑战了传统AdamW优化器中权重衰减与学习率成正比的假设，提出权重衰减应与学习率的平方成正比，并基于稳态假设推导出这一结论，证明这种设置能带来更稳定的训练动态和更好的模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统AdamW优化器中解耦权重衰减通常设置为与学习率γ成正比，但这一假设缺乏理论依据。最近有研究者基于正交性论证提出权重衰减应与γ²成正比，但作者发现这种正交性论证对训练动态影响很小，需要寻找更合理的理论基础。

Method: 作者基于"在稳态时更新与权重无关"的简单假设，推导出解耦权重衰减应与学习率的平方成正比（∝ γ²）。同时推导并实证验证了Scion优化器中每个小批量的总更新贡献（TUC）更好地由动量相关的有效学习率表征。

Result: 研究发现权重衰减∝ γ²的设置能带来稳定的权重范数和梯度范数，更好地控制训练动态，并提高模型性能。这种设置下的最优值具有可迁移性，能改善优化器的整体表现。

Conclusion: 解耦权重衰减应与学习率的平方成正比，而不是传统认为的与学习率成正比。这种设置基于稳态时更新与权重无关的假设，能提供更稳定的训练动态并提升模型性能，为优化器设计提供了新的理论指导。

Abstract: Decoupled weight decay, solely responsible for the performance advantage of AdamW over Adam, has long been set to proportional to learning rate $γ$ without questioning. Some researchers have recently challenged such assumption and argued that decoupled weight decay should be set $\propto γ^2$ instead based on orthogonality arguments at steady state. To the contrary, we find that eliminating the contribution of the perpendicular component of the update to the weight norm leads to little change to the training dynamics. Instead, we derive that decoupled weight decay $\propto γ^2$ results in stable weight norm based on the simple assumption that updates become independent of the weights at steady state, regardless of the nature of the optimizer. Based on the same assumption, we derive and empirically verify that the Total Update Contribution (TUC) of a minibatch under the Scion optimizer is better characterized by the momentum-dependent effective learning rate whose optimal value transfers and we show that decoupled weight decay $\propto γ^2$ leads to stable weight and gradient norms and allows us to better control the training dynamics and improve the model performance.

</details>


### [53] [SPROCKET: Extending ROCKET to Distance-Based Time-Series Transformations With Prototypes](https://arxiv.org/abs/2512.08246)
*Nicholas Harner*

Main category: cs.LG

TL;DR: SPROCKET是一种基于原型的新型时间序列特征工程方法，在多数UCR和UEA数据集上性能与现有卷积算法相当，其MR-HY-SP集成模型超越了之前最好的卷积集成HYDRA-MR。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列分类算法主要依赖特征工程策略，其中ROCKET通过随机核特征取得了优异性能。作者希望探索基于原型的新特征工程策略，以进一步提升时间序列分类的准确性和鲁棒性。

Method: 提出了SPROCKET（Selected Prototype Random Convolutional Kernel Transform）方法，这是一种基于原型的特征工程策略。该方法通过原型选择来实现特征变换，并与多核卷积方法结合形成MR-HY-SP集成模型。

Result: 在大多数UCR和UEA时间序列分类数据集上，SPROCKET取得了与现有卷积算法相当的性能。MR-HY-SP集成模型的平均准确率排名超过了之前最好的卷积集成HYDRA-MR。

Conclusion: 基于原型的特征变换能够增强时间序列分类的准确性和鲁棒性，SPROCKET方法为时间序列分类提供了新的有效特征工程策略。

Abstract: Classical Time Series Classification algorithms are dominated by feature engineering strategies. One of the most prominent of these transforms is ROCKET, which achieves strong performance through random kernel features. We introduce SPROCKET (Selected Prototype Random Convolutional Kernel Transform), which implements a new feature engineering strategy based on prototypes. On a majority of the UCR and UEA Time Series Classification archives, SPROCKET achieves performance comparable to existing convolutional algorithms and the new MR-HY-SP ( MultiROCKET-HYDRA-SPROCKET) ensemble's average accuracy ranking exceeds HYDRA-MR, the previous best convolutional ensemble's performance. These experimental results demonstrate that prototype-based feature transformation can enhance both accuracy and robustness in time series classification.

</details>


### [54] [Geometric-Stochastic Multimodal Deep Learning for Predictive Modeling of SUDEP and Stroke Vulnerability](https://arxiv.org/abs/2512.08257)
*Preksha Girish,Rachana Mysore,Mahanthesha U,Shrey Kumar,Misbah Fatimah Annigeri,Tanish Jain*

Main category: cs.LG

TL;DR: 提出一个统一的几何-随机多模态深度学习框架，整合EEG、ECG、呼吸、SpO2、EMG和fMRI信号，用于建模癫痫猝死（SUDEP）和中风易感性。


<details>
  <summary>Details</summary>
Motivation: 癫痫猝死和急性缺血性中风是危及生命的疾病，涉及皮层、脑干和自主神经系统之间的复杂相互作用。需要一种能够整合多种生理信号、提供早期检测和风险分层的数学基础框架。

Method: 结合黎曼流形嵌入、李群不变特征表示、分数随机动力学、哈密顿能量流建模和跨模态注意力机制。使用分数流行病扩散在结构脑图上建模中风传播。

Result: 在MULTI-CLARID数据集上的实验显示预测准确性提高，并从流形曲率、分数记忆指数、注意力熵和扩散中心性中提取可解释的生物标志物。

Conclusion: 该框架为神经自主神经疾病的早期检测、风险分层和可解释的多模态建模提供了数学原理基础。

Abstract: Sudden Unexpected Death in Epilepsy (SUDEP) and acute ischemic stroke are life-threatening conditions involving complex interactions across cortical, brainstem, and autonomic systems. We present a unified geometric-stochastic multimodal deep learning framework that integrates EEG, ECG, respiration, SpO2, EMG, and fMRI signals to model SUDEP and stroke vulnerability. The approach combines Riemannian manifold embeddings, Lie-group invariant feature representations, fractional stochastic dynamics, Hamiltonian energy-flow modeling, and cross-modal attention mechanisms. Stroke propagation is modeled using fractional epidemic diffusion over structural brain graphs. Experiments on the MULTI-CLARID dataset demonstrate improved predictive accuracy and interpretable biomarkers derived from manifold curvature, fractional memory indices, attention entropy, and diffusion centrality. The proposed framework provides a mathematically principled foundation for early detection, risk stratification, and interpretable multimodal modeling in neural-autonomic disorders.

</details>


### [55] [Jacobian Aligned Random Forests](https://arxiv.org/abs/2512.08306)
*Sarwesh Rauniyar*

Main category: cs.LG

TL;DR: JARF通过计算随机森林预测的梯度，构建全局线性预处理器旋转特征空间，使轴对齐决策树能处理旋转决策边界，保持简单性的同时达到斜向森林的效果。


<details>
  <summary>Details</summary>
Motivation: 轴对齐决策树在处理旋转或特征交互依赖的决策边界时表现不佳，而斜向森林虽然能解决这个问题但计算成本高、实现复杂。需要一种既能处理复杂边界又能保持轴对齐树简单性的方法。

Method: 首先训练轴对齐随机森林估计类别概率或回归输出，计算预测相对于每个特征的有限差分梯度，聚合为期望雅可比外积（扩展EGOP），将其作为全局线性预处理器旋转特征空间，然后在变换后的数据上重新训练标准轴对齐森林。

Result: 在表格分类和回归基准测试中，这种预处理器方法持续改进轴对齐森林的性能，经常匹配或超越斜向基线方法，同时显著减少训练时间。

Conclusion: 监督预处理器能够恢复斜向森林的大部分准确性，同时保留轴对齐树的简单性和鲁棒性，为处理复杂决策边界提供了一种高效实用的替代方案。

Abstract: Axis-aligned decision trees are fast and stable but struggle on datasets with rotated or interaction-dependent decision boundaries, where informative splits require linear combinations of features rather than single-feature thresholds. Oblique forests address this with per-node hyperplane splits, but at added computational cost and implementation complexity. We propose a simple alternative: JARF, Jacobian-Aligned Random Forests. Concretely, we first fit an axis-aligned forest to estimate class probabilities or regression outputs, compute finite-difference gradients of these predictions with respect to each feature, aggregate them into an expected Jacobian outer product that generalizes the expected gradient outer product (EGOP), and use it as a single global linear preconditioner for all inputs. This supervised preconditioner applies a single global rotation of the feature space, then hands the transformed data back to a standard axis-aligned forest, preserving off-the-shelf training pipelines while capturing oblique boundaries and feature interactions that would otherwise require many axis-aligned splits to approximate. The same construction applies to any model that provides gradients, though we focus on random forests and gradient-boosted trees in this work. On tabular classification and regression benchmarks, this preconditioning consistently improves axis-aligned forests and often matches or surpasses oblique baselines while improving training time. Our experimental results and theoretical analysis together indicate that supervised preconditioning can recover much of the accuracy of oblique forests while retaining the simplicity and robustness of axis-aligned trees.

</details>


### [56] [Minimizing Layerwise Activation Norm Improves Generalization in Federated Learning](https://arxiv.org/abs/2512.08314)
*M Yashwanth,Gaurav Kumar Nayak,Harsh Rangwani,Arya Singh,R. Venkatesh Babu,Anirban Chakraborty*

Main category: cs.LG

TL;DR: 提出一种名为MAN的联邦学习正则化方法，通过最小化激活范数来约束优化问题的平坦性，从而提高联邦学习模型的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习框架中，多个客户端协同训练全局模型时，聚合的模型可能收敛到"尖锐最小值"，这会损害模型的泛化能力。因此需要改进联邦学习中的泛化性能。

Method: 引入平坦性约束的联邦学习优化问题，通过约束训练损失Hessian矩阵的最大特征值来实现平坦性。提出MAN正则化技术，在客户端模型上最小化每层激活范数，理论上证明这能减少客户端损失函数的层间Hessian最大特征值。

Result: 将提出的平坦性约束优化应用于现有联邦学习技术，获得了显著改进，建立了新的最先进水平。

Conclusion: MAN正则化技术通过最小化激活范数来约束优化平坦性，有效提高了联邦学习模型的泛化性能，为联邦学习提供了新的优化方法。

Abstract: Federated Learning (FL) is an emerging machine learning framework that enables multiple clients (coordinated by a server) to collaboratively train a global model by aggregating the locally trained models without sharing any client's training data. It has been observed in recent works that learning in a federated manner may lead the aggregated global model to converge to a 'sharp minimum' thereby adversely affecting the generalizability of this FL-trained model. Therefore, in this work, we aim to improve the generalization performance of models trained in a federated setup by introducing a 'flatness' constrained FL optimization problem. This flatness constraint is imposed on the top eigenvalue of the Hessian computed from the training loss. As each client trains a model on its local data, we further re-formulate this complex problem utilizing the client loss functions and propose a new computationally efficient regularization technique, dubbed 'MAN,' which Minimizes Activation's Norm of each layer on client-side models. We also theoretically show that minimizing the activation norm reduces the top eigenvalue of the layer-wise Hessian of the client's loss, which in turn decreases the overall Hessian's top eigenvalue, ensuring convergence to a flat minimum. We apply our proposed flatness-constrained optimization to the existing FL techniques and obtain significant improvements, thereby establishing new state-of-the-art.

</details>


### [57] [Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models III: Implementing the Bacterial Biothreat Benchmark (B3) Dataset](https://arxiv.org/abs/2512.08459)
*Gary Ackerman,Theodore Wilson,Zachary Kallenborn,Olivia Shoemaker,Anna Wetzel,Hayley Peterson,Abigail Danfora,Jenna LaTourette,Brandon Behlendorf,Douglas Clifford*

Main category: cs.LG

TL;DR: 论文介绍了细菌生物威胁基准（B3）数据集的试点实施，这是生物威胁基准生成（BBG）框架的第三部分，旨在评估大型语言模型在生物安全风险方面的表现。


<details>
  <summary>Details</summary>
Motivation: 前沿人工智能模型（特别是大型语言模型）可能被用于生物恐怖主义或获取生物武器，这引发了政策、学术界和公众的担忧。需要开发能够评估特定模型生物安全风险的基准测试方法。

Method: 通过试点实施B3数据集，在样本前沿AI模型上运行基准测试，然后对模型响应进行人工评估，并从多个维度对结果进行应用风险分析。

Result: 试点研究表明，B3数据集提供了一种可行且细致的方法，能够快速评估LLM的生物安全风险，识别风险的关键来源，并为优先缓解领域提供指导。

Conclusion: B3数据集是评估大型语言模型生物安全风险的有效工具，能够帮助模型开发者和政策制定者量化和缓解相关风险。

Abstract: The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper discusses the pilot implementation of the Bacterial Biothreat Benchmark (B3) dataset. It is the third in a series of three papers describing an overall Biothreat Benchmark Generation (BBG) framework, with previous papers detailing the development of the B3 dataset. The pilot involved running the benchmarks through a sample frontier AI model, followed by human evaluation of model responses, and an applied risk analysis of the results along several dimensions. Overall, the pilot demonstrated that the B3 dataset offers a viable, nuanced method for rapidly assessing the biosecurity risk posed by a LLM, identifying the key sources of that risk and providing guidance for priority areas of mitigation priority.

</details>


### [58] [A Multivariate Bernoulli-Based Sampling Method for Multi-Label Data with Application to Meta-Research](https://arxiv.org/abs/2512.08371)
*Simon Chung,Colby J. Vorland,Donna L. Maney,Andrew W. Brown*

Main category: cs.LG

TL;DR: 提出一种考虑标签依赖关系的多标签数据采样算法，通过估计多元伯努利分布参数和计算标签组合权重，获得具有目标分布特性的加权样本，解决标签不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 多标签数据集中标签通常不互斥且频率差异大，传统采样方法难以同时保证稀缺标签的充分代表性和已知的分布偏差，需要一种能考虑标签依赖关系的采样方法。

Method: 使用多元伯努利分布作为多标签问题的底层分布，基于观测标签频率估计分布参数，计算每个标签组合的权重，通过加权采样获得具有目标分布特性的样本。

Result: 在Web of Science的64个生物医学主题类别文章样本上应用，成功保持了类别频率顺序，减少了最常⻅和最不常⻅类别间的频率差异，考虑了类别依赖关系，生成了更平衡的子样本。

Conclusion: 提出的采样算法能有效处理多标签不平衡问题，通过考虑标签依赖关系生成更平衡的样本，增强少数类别的代表性，适用于标签不互斥且频率差异大的多标签数据集。

Abstract: Datasets may contain observations with multiple labels. If the labels are not mutually exclusive, and if the labels vary greatly in frequency, obtaining a sample that includes sufficient observations with scarcer labels to make inferences about those labels, and which deviates from the population frequencies in a known manner, creates challenges. In this paper, we consider a multivariate Bernoulli distribution as our underlying distribution of a multi-label problem. We present a novel sampling algorithm that takes label dependencies into account. It uses observed label frequencies to estimate multivariate Bernoulli distribution parameters and calculate weights for each label combination. This approach ensures the weighted sampling acquires target distribution characteristics while accounting for label dependencies. We applied this approach to a sample of research articles from Web of Science labeled with 64 biomedical topic categories. We aimed to preserve category frequency order, reduce frequency differences between most and least common categories, and account for category dependencies. This approach produced a more balanced sub-sample, enhancing the representation of minority categories.

</details>


### [59] [Can TabPFN Compete with GNNs for Node Classification via Graph Tabularization?](https://arxiv.org/abs/2512.08798)
*Jeongwhan Choi,Woosung Kang,Minseo Kim,Jongwoo Kim,Noseong Park*

Main category: cs.LG

TL;DR: TabPFN-GN将图节点分类重新定义为表格学习问题，通过特征工程将图数据转换为表格特征，在异配图上优于GNNs


<details>
  <summary>Details</summary>
Motivation: 探索图节点分类是否能有效重新表述为表格学习问题，利用TabPFN在表格数据上的成功，为图学习提供无需图特定训练或语言模型依赖的替代方案

Method: TabPFN-GN方法：通过提取节点属性、结构特征、位置编码和可选的平滑邻域特征，将图数据转换为表格特征，然后使用TabPFN进行直接节点分类

Result: 在12个基准数据集上，TabPFN-GN在同配图上与GNNs竞争，在异配图上始终优于GNNs，展示了特征工程能有效桥接表格和图领域

Conclusion: 原则性的特征工程可以弥合表格和图领域之间的差距，为特定任务的GNN训练和依赖LLM的图基础模型提供了实用的替代方案

Abstract: Foundation models pretrained on large data have demonstrated remarkable zero-shot generalization capabilities across domains. Building on the success of TabPFN for tabular data and its recent extension to time series, we investigate whether graph node classification can be effectively reformulated as a tabular learning problem. We introduce TabPFN-GN, which transforms graph data into tabular features by extracting node attributes, structural properties, positional encodings, and optionally smoothed neighborhood features. This enables TabPFN to perform direct node classification without any graph-specific training or language model dependencies. Our experiments on 12 benchmark datasets reveal that TabPFN-GN achieves competitive performance with GNNs on homophilous graphs and consistently outperforms them on heterophilous graphs. These results demonstrate that principled feature engineering can bridge the gap between tabular and graph domains, providing a practical alternative to task-specific GNN training and LLM-dependent graph foundation models.

</details>


### [60] [Transformers for Multimodal Brain State Decoding: Integrating Functional Magnetic Resonance Imaging Data and Medical Metadata](https://arxiv.org/abs/2512.08462)
*Danial Jafarzadeh Jazi,Maryam Hajiesmaeili*

Main category: cs.LG

TL;DR: 本文提出了一种结合fMRI数据和DICOM元数据的多模态Transformer框架，用于解码大脑状态，提高模型准确性、可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习和深度学习方法在处理fMRI数据时，未能充分利用DICOM元数据提供的丰富上下文信息，限制了大脑状态解码的准确性和应用潜力。

Method: 提出新颖的多模态Transformer框架，整合fMRI数据和DICOM元数据，利用注意力机制捕捉复杂的时空模式和上下文关系。

Result: 该框架增强了模型准确性、可解释性和鲁棒性，在临床诊断、认知神经科学和个性化医学等领域具有广泛应用潜力。

Conclusion: 该多模态Transformer框架有效利用了DICOM元数据的上下文信息，提升了fMRI大脑状态解码性能，同时讨论了元数据变异性和计算需求等局限性，并提出了未来优化方向。

Abstract: Decoding brain states from functional magnetic resonance imaging (fMRI) data is vital for advancing neuroscience and clinical applications. While traditional machine learning and deep learning approaches have made strides in leveraging the high-dimensional and complex nature of fMRI data, they often fail to utilize the contextual richness provided by Digital Imaging and Communications in Medicine (DICOM) metadata. This paper presents a novel framework integrating transformer-based architectures with multimodal inputs, including fMRI data and DICOM metadata. By employing attention mechanisms, the proposed method captures intricate spatial-temporal patterns and contextual relationships, enhancing model accuracy, interpretability, and robustness. The potential of this framework spans applications in clinical diagnostics, cognitive neuroscience, and personalized medicine. Limitations, such as metadata variability and computational demands, are addressed, and future directions for optimizing scalability and generalizability are discussed.

</details>


### [61] [Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents](https://arxiv.org/abs/2512.08870)
*Xiang Chen,Yuling Shi,Qizhen Lan,Yuchao Qiu,Xiaodong Gu*

Main category: cs.LG

TL;DR: Fed-SE是一个联邦自进化框架，用于在隐私约束下优化LLM智能体，通过局部进化-全局聚合范式解决异构任务和稀疏奖励带来的梯度冲突问题。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在复杂交互任务中广泛部署，但隐私约束阻碍了集中式优化和跨动态环境的协同进化。虽然联邦学习在静态数据集上有效，但在开放式的智能体自进化方面仍待探索。直接应用标准联邦学习面临挑战：异构任务和稀疏的轨迹级奖励会导致严重的梯度冲突，破坏全局优化过程。

Method: Fed-SE采用局部进化-全局聚合范式。局部层面，智能体在过滤的高回报轨迹上使用参数高效微调实现稳定的梯度更新。全局层面，Fed-SE在低秩子空间内聚合更新，解耦环境特定动态，有效减少客户端间的负迁移。

Result: 在五个异构环境中的实验表明，Fed-SE相比联邦基线将平均任务成功率提高了约18%，验证了其在隐私约束部署中实现鲁棒跨环境知识转移的有效性。

Conclusion: Fed-SE成功解决了LLM智能体在联邦学习环境下的自进化问题，通过创新的局部优化和全局聚合机制，在保护隐私的同时实现了跨异构环境的有效知识转移和性能提升。

Abstract: LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. While Federated Learning (FL) has proven effective on static datasets, its extension to the open-ended self-evolution of agents remains underexplored. Directly applying standard FL is challenging: heterogeneous tasks and sparse, trajectory-level rewards introduce severe gradient conflicts, destabilizing the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents. Fed-SE establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace that disentangles environment-specific dynamics, effectively reducing negative transfer across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by approximately 18% over federated baselines, validating its effectiveness in robust cross-environment knowledge transfer in privacy-constrained deployments.

</details>


### [62] [When Tables Leak: Attacking String Memorization in LLM-Based Tabular Data Generation](https://arxiv.org/abs/2512.08875)
*Joshua Ward,Bochao Gu,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

TL;DR: LLM生成表格数据时存在隐私泄露风险，数字序列容易被记忆并复制，研究者提出了LevAtt攻击方法和两种防御策略


<details>
  <summary>Details</summary>
Motivation: LLM在生成高质量表格合成数据方面表现出色，但现有方法（微调小模型或提示大模型）存在泄露训练数据隐私的风险，特别是数字序列容易被记忆和复制

Method: 提出了LevAtt无盒成员推理攻击，仅基于生成的合成数据，针对数字字符串序列进行分析；同时提出了两种防御方法，包括一种新颖的采样策略，在生成过程中策略性地扰动数字

Result: LevAtt攻击在各种模型和数据集上都暴露了显著的隐私泄露，在某些情况下甚至成为最先进模型的完美成员分类器；提出的防御方法能够有效抵御攻击，同时保持合成数据的保真度和实用性损失最小

Conclusion: LLM基于的合成数据生成存在独特的隐私漏洞，需要有效的防御措施；提出的数字扰动策略能够在不显著影响数据质量的情况下保护隐私

Abstract: Large Language Models (LLMs) have recently demonstrated remarkable performance in generating high-quality tabular synthetic data. In practice, two primary approaches have emerged for adapting LLMs to tabular data generation: (i) fine-tuning smaller models directly on tabular datasets, and (ii) prompting larger models with examples provided in context. In this work, we show that popular implementations from both regimes exhibit a tendency to compromise privacy by reproducing memorized patterns of numeric digits from their training data. To systematically analyze this risk, we introduce a simple No-box Membership Inference Attack (MIA) called LevAtt that assumes adversarial access to only the generated synthetic data and targets the string sequences of numeric digits in synthetic observations. Using this approach, our attack exposes substantial privacy leakage across a wide range of models and datasets, and in some cases, is even a perfect membership classifier on state-of-the-art models. Our findings highlight a unique privacy vulnerability of LLM-based synthetic data generation and the need for effective defenses. To this end, we propose two methods, including a novel sampling strategy that strategically perturbs digits during generation. Our evaluation demonstrates that this approach can defeat these attacks with minimal loss of fidelity and utility of the synthetic data.

</details>


### [63] [Optimal Perturbation Budget Allocation for Data Poisoning in Offline Reinforcement Learning](https://arxiv.org/abs/2512.08485)
*Junnan Qiu,Jie Li*

Main category: cs.LG

TL;DR: 本文提出了一种针对离线强化学习的全局预算分配攻击策略，通过基于TD误差的样本重要性分配扰动预算，相比传统均匀扰动方法更高效且隐蔽。


<details>
  <summary>Details</summary>
Motivation: 现有离线RL数据投毒攻击通常采用局部均匀扰动策略，对所有样本不加区分地处理。这种方法效率低下，浪费扰动预算在低影响力样本上，同时由于显著的统计偏差而缺乏隐蔽性。

Method: 提出全局预算分配攻击策略，利用理论洞察：样本对价值函数收敛的影响与其TD误差成正比。将攻击建模为全局资源分配问题，推导出闭式解：在全局L2约束下，扰动幅度按TD误差敏感度成比例分配。

Result: 在D4RL基准测试上的实证结果表明，该方法显著优于基线策略，仅需最小扰动即可实现高达80%的性能下降，并能逃避最先进的统计和频谱防御检测。

Conclusion: 全局预算分配攻击策略通过智能分配扰动预算到高影响力样本，实现了更高效和隐蔽的离线RL数据投毒攻击，揭示了现有防御机制的脆弱性。

Abstract: Offline Reinforcement Learning (RL) enables policy optimization from static datasets but is inherently vulnerable to data poisoning attacks. Existing attack strategies typically rely on locally uniform perturbations, which treat all samples indiscriminately. This approach is inefficient, as it wastes the perturbation budget on low-impact samples, and lacks stealthiness due to significant statistical deviations. In this paper, we propose a novel Global Budget Allocation attack strategy. Leveraging the theoretical insight that a sample's influence on value function convergence is proportional to its Temporal Difference (TD) error, we formulate the attack as a global resource allocation problem. We derive a closed-form solution where perturbation magnitudes are assigned proportional to the TD-error sensitivity under a global L2 constraint. Empirical results on D4RL benchmarks demonstrate that our method significantly outperforms baseline strategies, achieving up to 80% performance degradation with minimal perturbations that evade detection by state-of-the-art statistical and spectral defenses.

</details>


### [64] [DAO-GP Drift Aware Online Non-Linear Regression Gaussian-Process](https://arxiv.org/abs/2512.08879)
*Mohammad Abu-Shaira,Ajita Rattani,Weishi Shi*

Main category: cs.LG

TL;DR: DAO-GP是一种针对概念漂移问题的在线高斯过程模型，具有漂移检测和自适应机制，无需手动调整超参数，在非平稳数据环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集常呈现随时间演化的数据分布（概念漂移），忽视这一现象会显著降低模型预测精度。现有在线高斯过程方法存在多个关键限制：缺乏漂移感知能力、依赖固定超参数、易受数据窥探影响、缺乏原则性衰减机制以及内存效率低下。

Method: 提出DAO-GP（漂移感知在线高斯过程），这是一种完全自适应、无需超参数、具有衰减机制和稀疏性的非线性回归模型。该模型内置漂移检测和自适应机制，能够根据漂移严重程度动态调整模型行为。

Result: 广泛的实证评估证实DAO-GP在平稳条件、多种漂移类型（突变、增量、渐进）和不同数据特征下都具有鲁棒性。分析显示其具备动态自适应能力、高效的内存和衰减管理以及演化诱导点。与最先进的参数和非参数模型相比，DAO-GP始终实现优越或具有竞争力的性能。

Conclusion: DAO-GP被确立为在线非线性回归的漂移弹性解决方案，能够有效应对概念漂移问题，在动态数据环境中保持高性能。

Abstract: Real-world datasets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. Gaussian Process (GP) models offer powerful non-parametric regression capabilities with uncertainty quantification, making them ideal for modeling complex data relationships in an online setting. However, conventional online GP methods face several critical limitations, including a lack of drift-awareness, reliance on fixed hyperparameters, vulnerability to data snooping, absence of a principled decay mechanism, and memory inefficiencies. In response, we propose DAO-GP (Drift-Aware Online Gaussian Process), a novel, fully adaptive, hyperparameter-free, decayed, and sparse non-linear regression model. DAO-GP features a built-in drift detection and adaptation mechanism that dynamically adjusts model behavior based on the severity of drift. Extensive empirical evaluations confirm DAO-GP's robustness across stationary conditions, diverse drift types (abrupt, incremental, gradual), and varied data characteristics. Analyses demonstrate its dynamic adaptation, efficient in-memory and decay-based management, and evolving inducing points. Compared with state-of-the-art parametric and non-parametric models, DAO-GP consistently achieves superior or competitive performance, establishing it as a drift-resilient solution for online non-linear regression.

</details>


### [65] [Long-Sequence LSTM Modeling for NBA Game Outcome Prediction Using a Novel Multi-Season Dataset](https://arxiv.org/abs/2512.08591)
*Charles Rios,Longzhen Han,Almas Baimagambetov,Nikolaos Polatidis*

Main category: cs.LG

TL;DR: 该研究构建了覆盖2004-05至2024-25赛季的纵向NBA数据集，并开发了基于LSTM的深度学习框架，通过长达9840场比赛（相当于8个完整赛季）的序列长度来预测NBA比赛结果，相比传统机器学习方法取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 预测NBA比赛结果对于教练策略、球迷参与和体育博彩越来越重要，但现有模型存在概念漂移、时间上下文有限和跨赛季不稳定的问题，需要能够捕捉长期趋势的预测系统。

Method: 构建了覆盖20个赛季的纵向NBA数据集，提出了基于LSTM的深度学习框架，使用长达9840场比赛（8个完整赛季）的序列长度来建模球队动态和赛季间依赖关系，并与逻辑回归、随机森林、MLP和CNN等传统方法进行对比。

Result: LSTM模型在所有指标上表现最佳：准确率72.35%，精确率73.15%，AUC-ROC 76.13%，证明了长序列时间建模在篮球结果预测中的重要性。

Conclusion: 研究证明了长序列时间建模在NBA比赛预测中的有效性，新构建的多赛季数据集对于开发稳健、可泛化的预测系统具有重要价值，LSTM架构能够有效捕捉球队的长期动态变化。

Abstract: Predicting the outcomes of professional basketball games, particularly in the National Basketball Association (NBA), has become increasingly important for coaching strategy, fan engagement, and sports betting. However, many existing prediction models struggle with concept drift, limited temporal context, and instability across seasons. To advance forecasting in this domain, we introduce a newly constructed longitudinal NBA dataset covering the 2004-05 to 2024-25 seasons and present a deep learning framework designed to model long-term performance trends. Our primary contribution is a Long Short-Term Memory (LSTM) architecture that leverages an extended sequence length of 9,840 games equivalent to eight full NBA seasons to capture evolving team dynamics and season-over-season dependencies. We compare this model against several traditional Machine Learning (ML) and Deep Learning (DL) baselines, including Logistic Regression, Random Forest, Multi-Layer Perceptron (MLP), and Convolutional Neural Network (CNN). The LSTM achieves the best performance across all metrics, with 72.35 accuracy, 73.15 precision and 76.13 AUC-ROC. These results demonstrate the importance of long-sequence temporal modeling in basketball outcome prediction and highlight the value of our new multi-season dataset for developing robust, generalizable NBA forecasting systems.

</details>


### [66] [DS FedProxGrad: Asymptotic Stationarity Without Noise Floor in Fair Federated Learning](https://arxiv.org/abs/2512.08671)
*Huzaifa Arif*

Main category: cs.LG

TL;DR: 本文改进了FedProxGrad算法的收敛性分析，提出了DS FedProxGrad框架，证明了在Robbins-Monro步长调度下，算法能够达到渐近平稳点，消除了原分析中的方差诱导噪声层依赖。


<details>
  <summary>Details</summary>
Motivation: 现有FedProxGrad算法在解决非凸复合优化问题时，其收敛分析仅能证明收敛到噪声主导的平稳邻域，且显式依赖于方差诱导的噪声层。这限制了算法的理论保证和实际应用效果。

Method: 提出了DS FedProxGrad（衰减步长FedProxGrad）分析框架，该框架包含不精确的局部近端解和显式公平正则化。采用Robbins-Monro步长调度策略，并在局部不精确性满足温和衰减条件下进行分析。

Result: 证明了liminf_{r→∞} E[‖∇F(x^r)‖^2] = 0，即算法能够达到渐近平稳点，收敛速率不再依赖于方差诱导的噪声层，显著改进了原FedProxGrad算法的收敛性保证。

Conclusion: DS FedProxGrad框架在理论上显著提升了FedProxGrad算法的收敛性能，消除了噪声层依赖，为群体公平联邦学习中的非凸复合优化问题提供了更强的收敛保证。

Abstract: Recent work \cite{arifgroup} introduced Federated Proximal Gradient \textbf{(\texttt{FedProxGrad})} for solving non-convex composite optimization problems in group fair federated learning. However, the original analysis established convergence only to a \textit{noise-dominated neighborhood of stationarity}, with explicit dependence on a variance-induced noise floor. In this work, we provide an improved asymptotic convergence analysis for a generalized \texttt{FedProxGrad}-type analytical framework with inexact local proximal solutions and explicit fairness regularization. We call this extended analytical framework \textbf{DS \texttt{FedProxGrad}} (Decay Step Size \texttt{FedProxGrad}). Under a Robbins-Monro step-size schedule \cite{robbins1951stochastic} and a mild decay condition on local inexactness, we prove that $\liminf_{r\to\infty} \mathbb{E}[\|\nabla F(\mathbf{x}^r)\|^2] = 0$, i.e., the algorithm is asymptotically stationary and the convergence rate does not depend on a variance-induced noise floor.

</details>


### [67] [An Additive Manufacturing Part Qualification Framework: Transferring Knowledge of Stress-strain Behaviors from Additively Manufactured Polymers to Metals](https://arxiv.org/abs/2512.08699)
*Chenglong Duan,Dazhong Wu*

Main category: cs.LG

TL;DR: 提出基于动态时间规整（DTW）和迁移学习（TL）的框架，用于增材制造零件认证，通过将低成本聚合物的应力-应变行为知识迁移到金属材料。


<details>
  <summary>Details</summary>
Motivation: 增材制造零件认证对确保关键应用中的零件质量和可靠性至关重要，但预测复杂应力-应变行为具有挑战性。需要开发能够跨材料类型有效预测性能的方法。

Method: 开发DTW-TL框架：使用DTW算法选择与目标金属数据集最相关的聚合物数据集作为源域，然后采用LSTM模型进行知识迁移。实验使用四种聚合物（尼龙、PLA、CF-ABS、树脂）和三种金属（AlSi10Mg、Ti6Al4V、碳钢）。

Result: DTW-TL框架能识别聚合物与金属之间的最佳匹配，选择单一聚合物数据集作为源域。当三个金属作为目标域时，DTW-TL模型获得最低平均绝对百分比误差（12.41%）和最高决定系数（0.96），优于无迁移学习的LSTM模型和基于四个聚合物数据集预训练的TL模型。

Conclusion: DTW-TL框架能有效实现从聚合物到金属的应力-应变行为知识迁移，为增材制造零件认证提供了一种准确且高效的方法，有助于确保零件在关键应用中的可靠性。

Abstract: Part qualification is crucial in additive manufacturing (AM) because it ensures that additively manufactured parts can be consistently produced and reliably used in critical applications. Part qualification aims at verifying that an additively manufactured part meets performance requirements; therefore, predicting the complex stress-strain behaviors of additively manufactured parts is critical. We develop a dynamic time warping (DTW)-transfer learning (TL) framework for additive manufacturing part qualification by transferring knowledge of the stress-strain behaviors of additively manufactured low-cost polymers to metals. Specifically, the framework employs DTW to select a polymer dataset as the source domain that is the most relevant to the target metal dataset. Using a long short-term memory (LSTM) model, four source polymers (i.e., Nylon, PLA, CF-ABS, and Resin) and three target metals (i.e., AlSi10Mg, Ti6Al4V, and carbon steel) that are fabricated by different AM techniques are utilized to demonstrate the effectiveness of the DTW-TL framework. Experimental results show that the DTW-TL framework identifies the closest match between polymers and metals to select one single polymer dataset as the source domain. The DTW-TL model achieves the lowest mean absolute percentage error of 12.41% and highest coefficient of determination of 0.96 when three metals are used as the target domain, respectively, outperforming the vanilla LSTM model without TL as well as the TL model pre-trained on four polymer datasets as the source domain.

</details>


### [68] [Exposing Hidden Biases in Text-to-Image Models via Automated Prompt Search](https://arxiv.org/abs/2512.08724)
*Manos Plitsis,Giorgos Bouritsas,Vassilis Katsouros,Yannis Panagakis*

Main category: cs.LG

TL;DR: BGPS框架自动生成能最大化TTI模型偏见的提示词，通过LLM生成属性中性提示，并用属性分类器引导LLM解码过程，发现Stable Diffusion 1.5和去偏模型中的多种微妙偏见。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型存在社会偏见，现有去偏方法依赖人工或LLM生成的提示数据集，成本高且可能遗漏未预期的偏见触发提示。需要自动发现模型偏见的框架。

Method: 提出Bias-Guided Prompt Search (BGPS)框架：1) 使用LLM生成属性中性提示；2) 利用TTI内部表示上的属性分类器引导LLM解码过程，使其向能放大目标图像属性的提示空间区域移动。

Result: 在Stable Diffusion 1.5和最先进去偏模型上发现大量微妙且先前未记录的偏见，严重降低公平性指标。发现的提示词可解释，相比硬提示优化方法在困惑度指标上有定量改进。

Conclusion: BGPS揭示了TTI模型的脆弱性，扩展了偏见搜索空间，可作为新的偏见缓解评估工具，发现的提示词可被典型用户使用，有助于更全面地评估模型公平性。

Abstract: Text-to-image (TTI) diffusion models have achieved remarkable visual quality, yet they have been repeatedly shown to exhibit social biases across sensitive attributes such as gender, race and age. To mitigate these biases, existing approaches frequently depend on curated prompt datasets - either manually constructed or generated with large language models (LLMs) - as part of their training and/or evaluation procedures. Beside the curation cost, this also risks overlooking unanticipated, less obvious prompts that trigger biased generation, even in models that have undergone debiasing. In this work, we introduce Bias-Guided Prompt Search (BGPS), a framework that automatically generates prompts that aim to maximize the presence of biases in the resulting images. BGPS comprises two components: (1) an LLM instructed to produce attribute-neutral prompts and (2) attribute classifiers acting on the TTI's internal representations that steer the decoding process of the LLM toward regions of the prompt space that amplify the image attributes of interest. We conduct extensive experiments on Stable Diffusion 1.5 and a state-of-the-art debiased model and discover an array of subtle and previously undocumented biases that severely deteriorate fairness metrics. Crucially, the discovered prompts are interpretable, i.e they may be entered by a typical user, quantitatively improving the perplexity metric compared to a prominent hard prompt optimization counterpart. Our findings uncover TTI vulnerabilities, while BGPS expands the bias search space and can act as a new evaluation tool for bias mitigation.

</details>


### [69] [Neural Ordinary Differential Equations for Simulating Metabolic Pathway Dynamics from Time-Series Multiomics Data](https://arxiv.org/abs/2512.08732)
*Udesh Habaraduwa,Andrei Lixandru*

Main category: cs.LG

TL;DR: 该研究提出使用神经常微分方程（NODEs）框架来学习蛋白质组和代谢组之间的复杂相互作用，应用于工程化大肠杆菌的时间序列数据，相比传统机器学习方法在预测精度和推理速度上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 人类健康寿命和生物工程的发展严重依赖于预测复杂生物系统的行为。虽然高通量多组学数据日益丰富，但将这些数据转化为可操作的预测模型仍然是一个瓶颈。需要高容量、数据驱动的模拟系统来直接从观测数据中推断潜在相互作用，实现时间轨迹模拟和个性化医疗、合成生物学中下游干预效果的预测。

Method: 引入神经常微分方程（NODEs）作为动态框架，用于学习蛋白质组和代谢组之间的复杂相互作用。将该框架应用于工程化大肠杆菌菌株的时间序列数据，对代谢途径的连续动力学进行建模。

Result: NODE架构在捕捉系统动力学方面表现出优于传统机器学习管道的性能。在柠檬烯途径数据集上RMSE改善高达94.38%，在异戊烯醇途径数据集上改善高达97.65%，总体改善超过90%。此外，NODE模型的推理时间加速了1000倍。

Conclusion: 神经常微分方程模型被确立为可扩展、高保真度的工具，适用于下一代代谢工程和生物发现，能够显著提升生物系统预测的准确性和效率。

Abstract: The advancement of human healthspan and bioengineering relies heavily on predicting the behavior of complex biological systems. While high-throughput multiomics data is becoming increasingly abundant, converting this data into actionable predictive models remains a bottleneck. High-capacity, datadriven simulation systems are critical in this landscape; unlike classical mechanistic models restricted by prior knowledge, these architectures can infer latent interactions directly from observational data, allowing for the simulation of temporal trajectories and the anticipation of downstream intervention effects in personalized medicine and synthetic biology. To address this challenge, we introduce Neural Ordinary Differential Equations (NODEs) as a dynamic framework for learning the complex interplay between the proteome and metabolome. We applied this framework to time-series data derived from engineered Escherichia coli strains, modeling the continuous dynamics of metabolic pathways. The proposed NODE architecture demonstrates superior performance in capturing system dynamics compared to traditional machine learning pipelines. Our results show a greater than 90% improvement in root mean squared error over baselines across both Limonene (up to 94.38% improvement) and Isopentenol (up to 97.65% improvement) pathway datasets. Furthermore, the NODE models demonstrated a 1000x acceleration in inference time, establishing them as a scalable, high-fidelity tool for the next generation of metabolic engineering and biological discovery.

</details>


### [70] [Learning and Editing Universal Graph Prompt Tuning via Reinforcement Learning](https://arxiv.org/abs/2512.08763)
*Jinfeng Xu,Zheyu Chen,Shuo Yang,Jinze Li,Hewei Wang,Yijie Li,Edith C. H. Ngai*

Main category: cs.LG

TL;DR: LEAP提出了一种新的通用图提示调优方法，通过在所有节点添加提示来保持理论基础，同时使用强化学习选择节点并编辑提示，在各种任务和场景中优于微调和其他提示方法。


<details>
  <summary>Details</summary>
Motivation: 现有选择性节点图提示调优方法会损害通用图提示调优的理论基础，需要在保持理论基础的同时追求更理想的提示效果。

Method: 提出LEAP模型：1）构建基本通用图提示以保持理论基础；2）使用actor-critic强化学习选择节点并编辑提示。

Result: 在图级和节点级任务的各种预训练策略中，无论是全样本还是少样本场景，LEAP都持续优于微调和其他基于提示的方法。

Conclusion: 通过在所有节点添加提示来保持通用图提示调优的理论基础，并结合强化学习进行节点选择和提示编辑，能够实现更优的性能表现。

Abstract: Early graph prompt tuning approaches relied on task-specific designs for Graph Neural Networks (GNNs), limiting their adaptability across diverse pre-training strategies. In contrast, another promising line of research has investigated universal graph prompt tuning, which operates directly in the input graph's feature space and builds a theoretical foundation that universal graph prompt tuning can theoretically achieve an equivalent effect of any prompting function, eliminating dependence on specific pre-training strategies. Recent works propose selective node-based graph prompt tuning to pursue more ideal prompts. However, we argue that selective node-based graph prompt tuning inevitably compromises the theoretical foundation of universal graph prompt tuning. In this paper, we strengthen the theoretical foundation of universal graph prompt tuning by introducing stricter constraints, demonstrating that adding prompts to all nodes is a necessary condition for achieving the universality of graph prompts. To this end, we propose a novel model and paradigm, Learning and Editing Universal GrAph Prompt Tuning (LEAP), which preserves the theoretical foundation of universal graph prompt tuning while pursuing more ideal prompts. Specifically, we first build the basic universal graph prompts to preserve the theoretical foundation and then employ actor-critic reinforcement learning to select nodes and edit prompts. Extensive experiments on graph- and node-level tasks across various pre-training strategies in both full-shot and few-shot scenarios show that LEAP consistently outperforms fine-tuning and other prompt-based approaches.

</details>


### [71] [Identifying counterfactual probabilities using bivariate distributions and uplift modeling](https://arxiv.org/abs/2512.08805)
*Théo Verhelst,Gianluca Bontempi*

Main category: cs.LG

TL;DR: 提出一种基于提升建模的反事实估计方法，使用双变量贝塔分布拟合预测的提升分数，获得反事实结果的后验分布


<details>
  <summary>Details</summary>
Motivation: 提升建模只能估计干预的因果效应（处理组与控制组的差异），而反事实识别旨在恢复潜在结果的联合分布（如"如果给客户提供营销优惠，他们是否仍会流失"）。反事实分布比提升信息更丰富但更难估计，但两者具有协同作用：可以利用提升模型进行反事实估计

Method: 提出一种反事实估计器，将双变量贝塔分布拟合到预测的提升分数上，从而获得反事实结果的后验分布。该方法除了提升建模所需的因果假设外，不需要额外的因果假设

Result: 模拟实验显示了该方法的有效性，可应用于电信客户流失等问题，揭示了标准机器学习或单独使用提升模型无法获得的洞察

Conclusion: 通过将提升建模与反事实估计相结合，提出了一种有效的方法来获得更丰富的因果推断信息，扩展了提升模型的应用范围

Abstract: Uplift modeling estimates the causal effect of an intervention as the difference between potential outcomes under treatment and control, whereas counterfactual identification aims to recover the joint distribution of these potential outcomes (e.g., "Would this customer still have churned had we given them a marketing offer?"). This joint counterfactual distribution provides richer information than the uplift but is harder to estimate. However, the two approaches are synergistic: uplift models can be leveraged for counterfactual estimation. We propose a counterfactual estimator that fits a bivariate beta distribution to predicted uplift scores, yielding posterior distributions over counterfactual outcomes. Our approach requires no causal assumptions beyond those of uplift modeling. Simulations show the efficacy of the approach, which can be applied, for example, to the problem of customer churn in telecom, where it reveals insights unavailable to standard ML or uplift models alone.

</details>


### [72] [Forecasting Fails: Unveiling Evasion Attacks in Weather Prediction Models](https://arxiv.org/abs/2512.08832)
*Huzaifa Arif,Pin-Yu Chen,Alex Gittens,James Diffenderfer,Bhavya Kailkhura*

Main category: cs.LG

TL;DR: WAAPO框架生成针对AI天气预报模型的对抗性扰动，通过通道稀疏性、空间定位和平滑性约束确保扰动有效且隐蔽，揭示了AI天气预报系统的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在天气预报中的广泛应用，需要评估其对对抗性扰动的脆弱性，以保护操作预报系统免受恶意攻击。

Method: 提出Weather Adaptive Adversarial Perturbation Optimization (WAAPO)框架，通过通道稀疏性、空间定位和平滑性约束生成目标对抗性扰动，确保扰动物理真实且不易察觉。使用ERA5数据集和FourCastNet模型进行实验验证。

Result: WAAPO能够生成与预定目标紧密对齐的对抗性轨迹，即使在约束条件下也能成功。实验表明，初始条件的微小扰动可导致预测天气模式的显著偏差，揭示了AI驱动预报模型的关键漏洞。

Conclusion: AI天气预报模型存在严重安全漏洞，需要建立强大的防护机制来防止对抗性攻击在操作预报系统中的利用。

Abstract: With the increasing reliance on AI models for weather forecasting, it is imperative to evaluate their vulnerability to adversarial perturbations. This work introduces Weather Adaptive Adversarial Perturbation Optimization (WAAPO), a novel framework for generating targeted adversarial perturbations that are both effective in manipulating forecasts and stealthy to avoid detection. WAAPO achieves this by incorporating constraints for channel sparsity, spatial localization, and smoothness, ensuring that perturbations remain physically realistic and imperceptible. Using the ERA5 dataset and FourCastNet (Pathak et al. 2022), we demonstrate WAAPO's ability to generate adversarial trajectories that align closely with predefined targets, even under constrained conditions. Our experiments highlight critical vulnerabilities in AI-driven forecasting models, where small perturbations to initial conditions can result in significant deviations in predicted weather patterns. These findings underscore the need for robust safeguards to protect against adversarial exploitation in operational forecasting systems.

</details>


### [73] [Reinforcement Learning From State and Temporal Differences](https://arxiv.org/abs/2512.08855)
*Lex Weaver,Jonathan Baxter*

Main category: cs.LG

TL;DR: 本文提出了一种改进的TD(λ)算法STD(λ)，通过关注状态值的相对排序而非绝对值来优化策略，解决了传统TD(λ)在某些情况下会收敛到次优策略的问题。


<details>
  <summary>Details</summary>
Motivation: 传统TD(λ)算法在函数逼近中最小化状态值的平方误差，但对于策略优化而言，状态值的相对排序比绝对值更重要。作者发现TD(λ)在某些情况下（如简单的两状态和三状态系统以及西洋双陆棋中）会从最优策略收敛到次优策略，这促使他们开发新的算法。

Method: 提出了STD(λ)算法，这是一种改进的TD(λ)方法，在二元决策问题中基于状态值的相对值来训练函数逼近器。该方法包括理论分析，证明了两状态系统中STD(λ)的策略单调改进特性，并与Bertsekas的差分训练方法进行了比较。

Result: STD(λ)在两状态系统和著名的acrobot问题的变体上取得了成功演示。理论分析表明STD(λ)在两状态系统中具有策略单调改进的特性，优于传统TD(λ)方法。

Conclusion: STD(λ)通过关注状态值的相对排序而非绝对值，解决了传统TD(λ)在某些强化学习问题中收敛到次优策略的缺陷，为基于函数逼近的策略优化提供了更有效的方法。

Abstract: TD($λ$) with function approximation has proved empirically successful for some complex reinforcement learning problems. For linear approximation, TD($λ$) has been shown to minimise the squared error between the approximate value of each state and the true value. However, as far as policy is concerned, it is error in the relative ordering of states that is critical, rather than error in the state values. We illustrate this point, both in simple two-state and three-state systems in which TD($λ$)--starting from an optimal policy--converges to a sub-optimal policy, and also in backgammon. We then present a modified form of TD($λ$), called STD($λ$), in which function approximators are trained with respect to relative state values on binary decision problems. A theoretical analysis, including a proof of monotonic policy improvement for STD($λ$) in the context of the two-state system, is presented, along with a comparison with Bertsekas' differential training method [1]. This is followed by successful demonstrations of STD($λ$) on the two-state system and a variation on the well known acrobot problem.

</details>


### [74] [Refining Diffusion Models for Motion Synthesis with an Acceleration Loss to Generate Realistic IMU Data](https://arxiv.org/abs/2512.08859)
*Lars Ole Häusler,Lena Uhlenberg,Göran Köber,Diyora Salimova,Oliver Amft*

Main category: cs.LG

TL;DR: 提出了一种文本到IMU的运动合成框架，通过使用基于加速度的二阶损失(L_acc)微调预训练扩散模型，生成更真实的IMU数据，显著提升人类活动识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本到运动生成模型缺乏对IMU传感器特定加速度模式的专门优化，导致生成的IMU数据与真实记录存在差异，影响下游任务如人类活动识别的性能。

Method: 提出基于加速度的二阶损失(L_acc)，强制生成运动在离散二阶时间差分上的一致性，将L_acc集成到现有扩散模型的训练目标中，通过微调获得IMU特定的运动先验，结合表面建模和虚拟传感器仿真生成IMU数据。

Result: L_acc损失相对原始模型降低12.7%，高动态活动(如跑步、跳跃)改进显著大于低动态活动(如坐、站)；合成IMU数据在低维嵌入中更接近真实IMU记录分布；仅使用精炼合成IMU数据训练的人类活动识别性能比原始扩散模型提升8.7%，比最佳对比模型提升7.6%。

Conclusion: 加速度感知的扩散精炼为对齐运动生成和IMU合成提供了有效方法，展示了深度学习管道在将通用文本到运动先验专门化到传感器特定任务方面的灵活性。

Abstract: We propose a text-to-IMU (inertial measurement unit) motion-synthesis framework to obtain realistic IMU data by fine-tuning a pretrained diffusion model with an acceleration-based second-order loss (L_acc). L_acc enforces consistency in the discrete second-order temporal differences of the generated motion, thereby aligning the diffusion prior with IMU-specific acceleration patterns. We integrate L_acc into the training objective of an existing diffusion model, finetune the model to obtain an IMU-specific motion prior, and evaluate the model with an existing text-to-IMU framework that comprises surface modelling and virtual sensor simulation. We analysed acceleration signal fidelity and differences between synthetic motion representation and actual IMU recordings. As a downstream application, we evaluated Human Activity Recognition (HAR) and compared the classification performance using data of our method with the earlier diffusion model and two additional diffusion model baselines. When we augmented the earlier diffusion model objective with L_acc and continued training, L_acc decreased by 12.7% relative to the original model. The improvements were considerably larger in high-dynamic activities (i.e., running, jumping) compared to low-dynamic activities~(i.e., sitting, standing). In a low-dimensional embedding, the synthetic IMU data produced by our refined model shifts closer to the distribution of real IMU recordings. HAR classification trained exclusively on our refined synthetic IMU data improved performance by 8.7% compared to the earlier diffusion model and by 7.6% over the best-performing comparison diffusion model. We conclude that acceleration-aware diffusion refinement provides an effective approach to align motion generation and IMU synthesis and highlights how flexible deep learning pipelines are for specialising generic text-to-motion priors to sensor-specific tasks.

</details>


### [75] [Unsupervised Learning of Density Estimates with Topological Optimization](https://arxiv.org/abs/2512.08895)
*Suina Tanweer,Firas A. Khasawneh*

Main category: cs.LG

TL;DR: 提出一种基于拓扑数据分析的损失函数，用于无监督自动选择核密度估计的最优带宽


<details>
  <summary>Details</summary>
Motivation: 核密度估计在机器学习、贝叶斯推断等领域广泛应用，但带宽选择是关键超参数，需要平衡偏差-方差权衡。传统方法需要手动调参，而拓扑数据分析可以在高维空间中量化拓扑特征

Method: 提出基于拓扑的损失函数，用于无监督自动选择最优带宽，通过拓扑特征（如连通分量、环、空洞等）来指导带宽选择

Result: 该方法在不同维度下进行了基准测试，展示了其潜力，能够自动选择最优带宽而不需要人工干预

Conclusion: 基于拓扑的损失函数为核密度估计的带宽选择提供了一种有效的无监督自动化方法，在高维数据中特别有优势

Abstract: Kernel density estimation is a key component of a wide variety of algorithms in machine learning, Bayesian inference, stochastic dynamics and signal processing. However, the unsupervised density estimation technique requires tuning a crucial hyperparameter: the kernel bandwidth. The choice of bandwidth is critical as it controls the bias-variance trade-off by over- or under-smoothing the topological features. Topological data analysis provides methods to mathematically quantify topological characteristics, such as connected components, loops, voids et cetera, even in high dimensions where visualization of density estimates is impossible. In this paper, we propose an unsupervised learning approach using a topology-based loss function for the automated and unsupervised selection of the optimal bandwidth and benchmark it against classical techniques -- demonstrating its potential across different dimensions.

</details>


### [76] [Open Polymer Challenge: Post-Competition Report](https://arxiv.org/abs/2512.08896)
*Gang Liu,Sobin Alosious,Subhamoy Mahajan,Eric Inae,Yihan Zhu,Yuhan Liu,Renzheng Zhang,Jiaxin Xu,Addison Howard,Ying Li,Tengfei Luo,Meng Jiang*

Main category: cs.LG

TL;DR: Open Polymer Challenge (OPC) 发布了首个聚合物信息学社区基准数据集，包含10K聚合物和5种性质，通过多任务预测竞赛推动可持续聚合物材料的机器学习发现。


<details>
  <summary>Details</summary>
Motivation: 机器学习在发现可持续聚合物材料方面具有巨大潜力，但进展受到缺乏大规模、高质量、开放可访问的聚合物数据集的限制。

Method: 发布包含10K聚合物和5种性质（热导率、回转半径、密度、自由体积分数、玻璃化转变温度）的基准数据集，组织多任务聚合物性质预测竞赛，参赛者在数据量小、标签不平衡、模拟源异质等现实约束下开发模型，使用特征增强、迁移学习、自监督预训练、针对性集成策略等技术。

Result: 竞赛揭示了数据准备、分布偏移和跨组模拟一致性等方面的重要经验，为未来大规模聚合物数据集的最佳实践提供了指导。生成的模型、分析和发布数据为聚合物科学中的分子AI建立了新基础。

Conclusion: Open Polymer Challenge 通过发布基准数据集和竞赛结果，为可持续和节能材料的开发加速提供了新基础，同时发布了测试数据集和数据生成管道。

Abstract: Machine learning (ML) offers a powerful path toward discovering sustainable polymer materials, but progress has been limited by the lack of large, high-quality, and openly accessible polymer datasets. The Open Polymer Challenge (OPC) addresses this gap by releasing the first community-developed benchmark for polymer informatics, featuring a dataset with 10K polymers and 5 properties: thermal conductivity, radius of gyration, density, fractional free volume, and glass transition temperature. The challenge centers on multi-task polymer property prediction, a core step in virtual screening pipelines for materials discovery. Participants developed models under realistic constraints that include small data, label imbalance, and heterogeneous simulation sources, using techniques such as feature-based augmentation, transfer learning, self-supervised pretraining, and targeted ensemble strategies. The competition also revealed important lessons about data preparation, distribution shifts, and cross-group simulation consistency, informing best practices for future large-scale polymer datasets. The resulting models, analysis, and released data create a new foundation for molecular AI in polymer science and are expected to accelerate the development of sustainable and energy-efficient materials. Along with the competition, we release the test dataset at https://www.kaggle.com/datasets/alexliu99/neurips-open-polymer-prediction-2025-test-data. We also release the data generation pipeline at https://github.com/sobinalosious/ADEPT, which simulates more than 25 properties, including thermal conductivity, radius of gyration, and density.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [77] [Impact of Data-Oriented and Object-Oriented Design on Performance and Cache Utilization with Artificial Intelligence Algorithms in Multi-Threaded CPUs](https://arxiv.org/abs/2512.07841)
*Gabriel M. Arantes,Richard F. Pinto,Bruno L. Dalmazo,Eduardo N. Borges,Giancarlo Lucca,Viviane L. D. de Mattos,Fabian C. Cardoso,Rafael A. Berri*

Main category: cs.AI

TL;DR: 该研究比较了数据导向设计(DOD)与传统面向对象设计(OOD)在多线程环境下的性能表现，通过A*搜索算法的四种实现版本测试发现，DOD在多线程场景中表现出更好的执行效率和缓存利用率。


<details>
  <summary>Details</summary>
Motivation: 随着多核CPU与主内存之间性能差距的增大，需要硬件感知的软件设计范式。本研究旨在全面分析数据导向设计与传统面向对象设计在多线程环境中的性能差异，特别关注缓存利用效率。

Method: 研究开发并比较了A*搜索算法的四种实现版本：单线程OOD、单线程DOD、多线程OOD、多线程DOD。评估基于执行时间、内存使用和CPU缓存未命中等指标。

Result: 在多线程测试中，DOD实现表现出显著的性能优势，具有更快的执行时间和更少的系统调用及缓存未命中。对于A*算法这类细粒度任务，线程管理开销导致单线程版本在两个范式中都明显优于多线程版本。

Conclusion: 尽管在简单算法中性能差异可能不明显，但DOD在关键指标上的一致优势凸显了其架构优越性，表明在复杂、大规模AI和并行计算任务中，DOD是最大化硬件效率的更有效方法。

Abstract: The growing performance gap between multi-core CPUs and main memory necessitates hardware-aware software design paradigms. This study provides a comprehensive performance analysis of Data Oriented Design (DOD) versus the traditional Object-Oriented Design (OOD), focusing on cache utilization and efficiency in multi-threaded environments. We developed and compared four distinct versions of the A* search algorithm: single-threaded OOD (ST-OOD), single-threaded DOD (ST-DOD), multi-threaded OOD (MT-OOD), and multi-threaded DOD (MT-DOD). The evaluation was based on metrics including execution time, memory usage, and CPU cache misses. In multi-threaded tests, the DOD implementation demonstrated considerable performance gains, with faster execution times and a lower number of raw system calls and cache misses. While OOD occasionally showed marginal advantages in memory usage or percentage-based cache miss rates, DOD's efficiency in data-intensive operations was more evident. Furthermore, our findings reveal that for a fine-grained task like the A* algorithm, the overhead associated with thread management led to single-threaded versions significantly outperforming their multi-threaded counterparts in both paradigms. We conclude that even when performance differences appear subtle in simple algorithms, the consistent advantages of DOD in critical metrics highlight its foundational architectural superiority, suggesting it is a more effective approach for maximizing hardware efficiency in complex, large-scale AI and parallel computing tasks.

</details>


### [78] [SkipKV: Selective Skipping of KV Generation and Storage for Efficient Inference with Large Reasoning Models](https://arxiv.org/abs/2512.07993)
*Jiayi Tian,Seyedarmin Azizi,Yequan Zhao,Erfan Baghaei Potraghloo,Sean McPherson,Sharath Nittur Sridhar,Zhengyang Wang,Zheng Zhang,Massoud Pedram,Souvik Kundu*

Main category: cs.AI

TL;DR: SkipKV是一种无需训练的KV缓存压缩方法，通过句子级评分识别和移除高度相似的句子，同时动态调整引导向量来抑制冗余生成，从而在保持推理准确性的同时显著减少KV缓存开销。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在推理过程中KV缓存随思维链线性增长，导致内存和吞吐量瓶颈。现有KV缓存逐出方法在多批次设置中因不稳定的令牌级评分和填充令牌导致的KV预算减少而无法保持准确性，且往往生成更长的序列。

Method: 提出SkipKV方法：1）使用句子评分指标识别并移除高度相似的句子，进行粗粒度的句子级序列删除；2）动态调整引导向量更新隐藏激活状态，强制模型生成简洁响应；3）无需训练，直接在推理时操作。

Result: 在多个推理基准测试中，SkipKV在相似压缩预算下比替代方法保持高达26.7%的准确性改进，同时生成长度减少1.6倍，吞吐量提高1.7倍。

Conclusion: SkipKV通过句子级KV压缩和动态引导向量调整，有效解决了现有KV缓存逐出方法在多批次设置中的准确性问题，同时减少了冗余生成，显著提升了推理效率和性能。

Abstract: Large reasoning models (LRMs) often cost significant key-value (KV) cache overhead, due to their linear growth with the verbose chain-of-thought (CoT) reasoning process. This costs both memory and throughput bottleneck limiting their efficient deployment. Towards reducing KV cache size during inference, we first investigate the effectiveness of existing KV cache eviction methods for CoT reasoning. Interestingly, we find that due to unstable token-wise scoring and the reduced effective KV budget caused by padding tokens, state-of-the-art (SoTA) eviction methods fail to maintain accuracy in the multi-batch setting. Additionally, these methods often generate longer sequences than the original model, as semantic-unaware token-wise eviction leads to repeated revalidation during reasoning. To address these issues, we present \textbf{SkipKV}, a \textbf{\textit{training-free}} KV compression method for selective \textit{eviction} and \textit{generation} operating at a coarse-grained sentence-level sequence removal for efficient CoT reasoning. In specific, it introduces a \textit{sentence-scoring metric} to identify and remove highly similar sentences while maintaining semantic coherence. To suppress redundant generation, SkipKV dynamically adjusts a steering vector to update the hidden activation states during inference enforcing the LRM to generate concise response. Extensive evaluations on multiple reasoning benchmarks demonstrate the effectiveness of SkipKV in maintaining up to $\mathbf{26.7}\%$ improved accuracy compared to the alternatives, at a similar compression budget. Additionally, compared to SoTA, SkipKV yields up to $\mathbf{1.6}\times$ fewer generation length while improving throughput up to $\mathbf{1.7}\times$.

</details>


### [79] [Toward an AI Reasoning-Enabled System for Patient-Clinical Trial Matching](https://arxiv.org/abs/2512.08026)
*Caroline N. Leach,Mitchell A. Klusty,Samuel E. Armstrong,Justine C. Pickarski,Kristen L. Hankins,Emily B. Collier,Maya Shah,Aaron D. Mullen,V. K. Cody Bumgardner*

Main category: cs.AI

TL;DR: 开发了一个基于大语言模型的AI增强患者-临床试验匹配系统，通过生成结构化资格评估和可解释推理链来支持人工审查，旨在减少协调员负担并提高匹配效率。


<details>
  <summary>Details</summary>
Motivation: 目前患者临床试验资格筛查仍依赖人工，过程耗时且资源密集，需要自动化解决方案来应对异构电子健康记录数据整合、专家审查支持和安全标准维护等关键挑战。

Method: 利用开源、具备推理能力的大语言模型构建概念验证系统，超越二元分类，生成结构化资格评估和可解释推理链，支持人机协同审查，将资格表示为动态状态而非固定判定。

Result: 系统能够识别可用匹配，提供可操作建议使患者未来可能符合资格，减少协调员负担，智能扩展每位患者考虑的试验范围，并保证所有AI生成输出的全面可审计性。

Conclusion: 该决策支持工具通过AI增强的患者-试验匹配，解决了关键实施挑战，有望改善临床试验招募流程，提高效率并支持更全面的患者匹配。

Abstract: Screening patients for clinical trial eligibility remains a manual, time-consuming, and resource-intensive process. We present a secure, scalable proof-of-concept system for Artificial Intelligence (AI)-augmented patient-trial matching that addresses key implementation challenges: integrating heterogeneous electronic health record (EHR) data, facilitating expert review, and maintaining rigorous security standards. Leveraging open-source, reasoning-enabled large language models (LLMs), the system moves beyond binary classification to generate structured eligibility assessments with interpretable reasoning chains that support human-in-the-loop review. This decision support tool represents eligibility as a dynamic state rather than a fixed determination, identifying matches when available and offering actionable recommendations that could render a patient eligible in the future. The system aims to reduce coordinator burden, intelligently broaden the set of trials considered for each patient and guarantee comprehensive auditability of all AI-generated outputs.

</details>


### [80] [Large Language Models for Education and Research: An Empirical and User Survey-based Analysis](https://arxiv.org/abs/2512.08057)
*Md Mostafizer Rahman,Ariful Islam Shiplu,Md Faizul Ibne Amin,Yutaka Watanobe,Lu Peng*

Main category: cs.AI

TL;DR: 本研究对ChatGPT和DeepSeek两大LLM在教育研究领域进行综合评估，包括技术分析、实验测试和用户调查，比较它们在文本生成、编程和专门问题解决方面的性能差异。


<details>
  <summary>Details</summary>
Motivation: 随着预训练大语言模型在教育研究领域展现巨大潜力，需要系统评估当前最先进的ChatGPT和DeepSeek模型，了解它们在准确性、计算效率和用户体验方面的权衡，为教育研究应用提供指导。

Method: 采用综合评估方法：1）背景技术分析；2）实证实验（文本生成、编程、专门问题解决任务）；3）真实用户调查（学生、教育工作者、研究人员）。

Result: ChatGPT在通用语言理解和文本生成方面表现更优，DeepSeek凭借效率导向设计在编程任务中表现更佳；两者都能提供医学准确的诊断输出并有效解决复杂数学问题；用户调查揭示了这些模型的实际益处和局限性。

Conclusion: ChatGPT和DeepSeek各有优势，在不同教育研究场景中具有互补性，综合评估为理解这些模型在推动教育和研究进步中的作用提供了深入见解。

Abstract: Pretrained Large Language Models (LLMs) have achieved remarkable success across diverse domains, with education and research emerging as particularly impactful areas. Among current state-of-the-art LLMs, ChatGPT and DeepSeek exhibit strong capabilities in mathematics, science, medicine, literature, and programming. In this study, we present a comprehensive evaluation of these two LLMs through background technology analysis, empirical experiments, and a real-world user survey. The evaluation explores trade-offs among model accuracy, computational efficiency, and user experience in educational and research affairs. We benchmarked these LLMs performance in text generation, programming, and specialized problem-solving. Experimental results show that ChatGPT excels in general language understanding and text generation, while DeepSeek demonstrates superior performance in programming tasks due to its efficiency- focused design. Moreover, both models deliver medically accurate diagnostic outputs and effectively solve complex mathematical problems. Complementing these quantitative findings, a survey of students, educators, and researchers highlights the practical benefits and limitations of these models, offering deeper insights into their role in advancing education and research.

</details>


### [81] [Scalable Back-End for an AI-Based Diabetes Prediction Application](https://arxiv.org/abs/2512.08147)
*Henry Anand Septian Radityo,Bernardus Willson,Reynard Tanadi,Latifa Dwiyanti,Saiful Akbar*

Main category: cs.AI

TL;DR: 开发了一个用于糖尿病预测移动应用的可扩展后端系统，采用水平扩展、数据库分片和消息队列技术，83%的功能满足性能目标，能处理1万并发用户。


<details>
  <summary>Details</summary>
Motivation: 全球糖尿病患病率上升需要早期检测，AI预测应用需要响应迅速且可扩展的后端架构来服务大量用户。

Method: 采用水平扩展、数据库分片和通过消息队列（RabbitMQ）的异步通信架构，构建可扩展的后端系统。

Result: 83%的系统功能（24个中的20个）满足性能目标（故障率低于5%，平均延迟低于1000ms），用户档案管理、活动跟踪和读取密集型预测操作表现良好，能处理1万并发用户。

Conclusion: 该可扩展后端系统成功满足糖尿病预测应用的性能要求，RabbitMQ的异步通信在重负载下有效降低错误率并确保系统可靠性。

Abstract: The rising global prevalence of diabetes necessitates early detection to prevent severe complications. While AI-powered prediction applications offer a promising solution, they require a responsive and scalable back-end architecture to serve a large user base effectively. This paper details the development and evaluation of a scalable back-end system designed for a mobile diabetes prediction application. The primary objective was to maintain a failure rate below 5% and an average latency of under 1000 ms. The architecture leverages horizontal scaling, database sharding, and asynchronous communication via a message queue. Performance evaluation showed that 83% of the system's features (20 out of 24) met the specified performance targets. Key functionalities such as user profile management, activity tracking, and read-intensive prediction operations successfully achieved the desired performance. The system demonstrated the ability to handle up to 10,000 concurrent users without issues, validating its scalability. The implementation of asynchronous communication using RabbitMQ proved crucial in minimizing the error rate for computationally intensive prediction requests, ensuring system reliability by queuing requests and preventing data loss under heavy load.

</details>


### [82] [Beyond Traditional Diagnostics: Transforming Patient-Side Information into Predictive Insights with Knowledge Graphs and Prototypes](https://arxiv.org/abs/2512.08261)
*Yibowen Zhao,Yinan Zhang,Zhixiang Su,Lizhen Cui,Chunyan Miao*

Main category: cs.AI

TL;DR: KPI框架通过整合医学知识图谱、构建疾病原型和对比学习，解决基于患者信息的疾病预测中的不平衡分布和可解释性问题，并利用大语言模型生成个性化医学解释。


<details>
  <summary>Details</summary>
Motivation: 基于患者信息（人口统计和自述症状）的疾病预测研究受到关注，但现有方法面临疾病分布不平衡和缺乏可解释性的挑战，导致预测结果存在偏差或不可靠。

Method: 提出KPI框架：1) 将结构化可信医学知识整合为统一疾病知识图谱；2) 构建临床有意义的疾病原型；3) 使用对比学习提高预测准确性；4) 利用大语言模型生成患者特异性医学解释。

Result: 在真实世界数据集上的实验表明，KPI在预测准确性上优于现有最先进方法，并能提供与患者叙述高度一致的临床有效解释，展示了其在以患者为中心的医疗保健中的实用价值。

Conclusion: KPI框架通过整合医学知识、增强长尾疾病预测能力、提高可解释性，为基于患者信息的疾病预测提供了有效的解决方案，具有重要的临床应用价值。

Abstract: Predicting diseases solely from patient-side information, such as demographics and self-reported symptoms, has attracted significant research attention due to its potential to enhance patient awareness, facilitate early healthcare engagement, and improve healthcare system efficiency. However, existing approaches encounter critical challenges, including imbalanced disease distributions and a lack of interpretability, resulting in biased or unreliable predictions. To address these issues, we propose the Knowledge graph-enhanced, Prototype-aware, and Interpretable (KPI) framework. KPI systematically integrates structured and trusted medical knowledge into a unified disease knowledge graph, constructs clinically meaningful disease prototypes, and employs contrastive learning to enhance predictive accuracy, which is particularly important for long-tailed diseases. Additionally, KPI utilizes large language models (LLMs) to generate patient-specific, medically relevant explanations, thereby improving interpretability and reliability. Extensive experiments on real-world datasets demonstrate that KPI outperforms state-of-the-art methods in predictive accuracy and provides clinically valid explanations that closely align with patient narratives, highlighting its practical value for patient-centered healthcare delivery.

</details>


### [83] [Reasoning Models Ace the CFA Exams](https://arxiv.org/abs/2512.08270)
*Jaisal Patel,Yunzhe Chen,Kaiwen He,Keyi Wang,David Li,Kairong Xiao,Xiao-Yang Liu*

Main category: cs.AI

TL;DR: 最新推理模型在CFA各级考试中表现优异，多个模型通过所有级别，其中Gemini 3.0 Pro在Level I创下97.6%的最高分记录


<details>
  <summary>Details</summary>
Motivation: 先前研究表明大语言模型在CFA考试中表现不佳，但近期推理模型在各类专业考试中取得突破，需要评估最新模型在CFA考试中的实际表现

Method: 使用980道模拟题（包括Level I三套、Level II两套、Level III三套）评估最先进的推理模型，采用与先前研究相同的通过/失败标准

Result: 大多数模型通过了所有三个级别，按总体表现排序：Gemini 3.0 Pro、Gemini 2.5 Pro、GPT-5、Grok 4、Claude Opus 4.1、DeepSeek-V3.1。Gemini 3.0 Pro在Level I获得97.6%创纪录分数，GPT-5在Level II以94.3%领先，Gemini 2.5 Pro在Level III选择题获得86.4%，Gemini 3.0 Pro在构建回答题获得92.0%

Conclusion: 最新推理模型在CFA考试中表现出色，显著超越了先前大语言模型的性能，展示了推理模型在专业金融认证考试中的强大能力

Abstract: Previous research has reported that large language models (LLMs) demonstrate poor performance on the Chartered Financial Analyst (CFA) exams. However, recent reasoning models have achieved strong results on graduate-level academic and professional examinations across various disciplines. In this paper, we evaluate state-of-the-art reasoning models on a set of mock CFA exams consisting of 980 questions across three Level I exams, two Level II exams, and three Level III exams. Using the same pass/fail criteria from prior studies, we find that most models clear all three levels. The models that pass, ordered by overall performance, are Gemini 3.0 Pro, Gemini 2.5 Pro, GPT-5, Grok 4, Claude Opus 4.1, and DeepSeek-V3.1. Specifically, Gemini 3.0 Pro achieves a record score of 97.6% on Level I. Performance is also strong on Level II, led by GPT-5 at 94.3%. On Level III, Gemini 2.5 Pro attains the highest score with 86.4% on multiple-choice questions while Gemini 3.0 Pro achieves 92.0% on constructed-response questions.

</details>


### [84] [Towards a Science of Scaling Agent Systems](https://arxiv.org/abs/2512.08296)
*Yubin Kim,Ken Gu,Chanwoo Park,Chunjong Park,Samuel Schmidgall,A. Ali Heydari,Yao Yan,Zhihan Zhang,Yuchen Zhuang,Mark Malhotra,Paul Pu Liang,Hae Won Park,Yuzhe Yang,Xuhai Xu,Yilun Du,Shwetak Patel,Tim Althoff,Daniel McDuff,Xin Liu*

Main category: cs.AI

TL;DR: 该研究通过系统实验揭示了多智能体系统的量化扩展原则，发现工具协调权衡、能力饱和和拓扑依赖错误放大是影响性能的三大主导效应，并建立了可预测最佳协调策略的框架。


<details>
  <summary>Details</summary>
Motivation: 尽管基于语言模型的智能体系统在现实AI应用中日益普及，但其性能决定原则仍缺乏深入探索，导致从业者只能依赖启发式方法而非基于原则的设计选择。

Method: 研究使用五种典型架构（单智能体、独立、集中式、分散式、混合式）在三个LLM家族上实例化，在四个多样化基准测试（Finance-Agent、BrowseComp-Plus、PlanCraft、Workbench）上进行受控评估，涵盖180种配置，使用标准化工具和令牌预算。

Result: 建立了基于经验协调指标（效率、开销、错误放大、冗余）的预测模型，交叉验证R^2=0.513。识别出三大主导效应：工具协调权衡、能力饱和、拓扑依赖错误放大。集中式协调在可并行任务上提升80.9%，分散式协调在动态网页导航上表现更优，但所有多智能体变体在顺序推理任务上性能下降39-70%。框架对87%的保留配置预测了最佳协调策略。

Conclusion: 该研究为智能体系统提供了基于可测量任务属性的预测性扩展原则，能够指导从业者根据具体任务特性选择最优的协调策略，实现更高效的多智能体系统设计。

Abstract: Agents, language model (LM)-based systems that are capable of reasoning, planning, and acting are becoming the dominant paradigm for real-world AI applications. Despite this widespread adoption, the principles that determine their performance remain underexplored, leaving practitioners to rely on heuristics rather than principled design choices. We address this gap by deriving quantitative scaling principles for agent systems. We evaluate this across four diverse benchmarks: Finance-Agent, BrowseComp-Plus, PlanCraft, and Workbench. Using five canonical architectures (Single, Independent, Centralized, Decentralized, Hybrid) instantiated across three LLM families, we perform a controlled evaluation spanning 180 configurations with standardized tools and token budgets. We derive a predictive model using empirical coordination metrics, including efficiency, overhead, error amplification, and redundancy, that achieves cross-validated R^2=0.513. We identify three dominant effects: (1) a tool-coordination trade-off: under fixed computational budgets, tool-heavy tasks suffer disproportionately from multi-agent overhead. (2) a capability saturation: coordination yields diminishing or negative returns (beta=-0.408, p<0.001) once single-agent baselines exceed ~45%. (3) topology-dependent error amplification: independent agents amplify errors 17.2x through unchecked propagation, while centralized coordination contains this to 4.4x. Centralized coordination improves performance by 80.9% on parallelizable tasks like financial reasoning, while decentralized coordination excels on dynamic web navigation (+9.2% vs. +0.2%). Yet for sequential reasoning tasks, all multi-agent variants degraded performance by 39-70%. The framework predicts the optimal coordination strategy for 87% of held-out configurations, providing a predictive principle of agentic scaling based on measurable task properties.

</details>


### [85] [rSIM: Incentivizing Reasoning Capabilities of LLMs via Reinforced Strategy Injection](https://arxiv.org/abs/2512.08300)
*Sijia Chen,Baochun Li,Di Niu*

Main category: cs.AI

TL;DR: 该论文提出了一种强化策略注入机制(rSIM)，通过小型规划器引导大语言模型的思维链，使其进化为推理语言模型(RLM)。


<details>
  <summary>Details</summary>
Motivation: 大语言模型(LLMs)通过强化学习后训练可以进化为推理语言模型(RLMs)，其标志性特征是能够在思维链中表现出"顿悟"时刻，执行自我反思和深度思考等策略。受此启发，研究者希望开发一种机制，使任何LLM都能通过自适应注入推理策略来提升推理能力。

Method: 提出强化策略注入机制(rSIM)，采用小型规划器(领导者智能体)引导LLM(跟随者智能体)的思维链。基于领导者-跟随者框架，使用多智能体强化学习(MARL)联合训练规划器和LLM，采用简单的基于规则的奖励机制。

Result: 实验结果显示，rSIM使Qwen2.5-0.5B进化为RLM，性能显著超过Qwen2.5-14B。规划器具有通用性：只需训练一次，即可作为插件大幅提升现有LLMs的推理能力。此外，规划器支持跨任务的持续学习，其规划能力能够逐步提升并泛化到更广泛的问题。

Conclusion: rSIM机制通过强化策略注入，有效提升了语言模型的推理能力，使小型模型能够超越大型模型，且规划器具有良好的通用性和持续学习能力，为LLMs的推理能力提升提供了新的有效途径。

Abstract: Large language models (LLMs) are post-trained through reinforcement learning (RL) to evolve into Reasoning Language Models (RLMs), where the hallmark of this advanced reasoning is ``aha'' moments when they start to perform strategies, such as self-reflection and deep thinking, within chain of thoughts (CoTs). Motivated by this, this paper proposes a novel reinforced strategy injection mechanism (rSIM), that enables any LLM to become an RLM by employing a small planner to guide the LLM's CoT through the adaptive injection of reasoning strategies. To achieve this, the planner (leader agent) is jointly trained with an LLM (follower agent) using multi-agent RL (MARL), based on a leader-follower framework and straightforward rule-based rewards. Experimental results show that rSIM enables Qwen2.5-0.5B to become an RLM and significantly outperform Qwen2.5-14B. Moreover, the planner is generalizable: it only needs to be trained once and can be applied as a plug-in to substantially improve the reasoning capabilities of existing LLMs. In addition, the planner supports continual learning across various tasks, allowing its planning abilities to gradually improve and generalize to a wider range of problems.

</details>


### [86] [Predicting California Bearing Ratio with Ensemble and Neural Network Models: A Case Study from Türkiye](https://arxiv.org/abs/2512.08340)
*Abdullah Hulusi Kökçam,Uğur Dağdeviren,Talas Fikret Kurnaz,Alparslan Serhat Demir,Caner Erden*

Main category: cs.AI

TL;DR: 该研究开发了一个机器学习框架，使用土耳其382个土壤样本的物理化学特性来预测加州承载比(CBR)，随机森林回归器表现最佳，为岩土工程提供了传统实验室测试的有效替代方案。


<details>
  <summary>Details</summary>
Motivation: 传统CBR实验室测试虽然准确，但耗时、成本高且不适用于大规模或多样化土壤剖面。随着人工智能特别是机器学习的发展，需要开发更快速、精确的数据驱动方法来预测土壤承载能力。

Method: 研究收集了土耳其不同地理气候区域的382个土壤样本，包含与承载能力相关的物理化学特性。测试了12种机器学习算法，包括决策树、随机森林、极端树、梯度提升、XGBoost、K近邻、支持向量回归、多层感知器、AdaBoost、Bagging、投票和堆叠回归器。所有模型都经过训练、验证和评估以测试其泛化能力和鲁棒性。

Result: 随机森林回归器表现最佳，获得了训练集R²=0.95、验证集R²=0.76和测试集R²=0.83的优异结果。这表明该模型具有很强的非线性映射能力，适合预测性岩土工程任务。

Conclusion: 该研究支持将智能、数据中心的模型整合到岩土工程中，为传统方法提供了有效替代方案，促进了基础设施分析和设计的数字化转型。机器学习框架在CBR预测方面显示出巨大潜力。

Abstract: The California Bearing Ratio (CBR) is a key geotechnical indicator used to assess the load-bearing capacity of subgrade soils, especially in transportation infrastructure and foundation design. Traditional CBR determination relies on laboratory penetration tests. Despite their accuracy, these tests are often time-consuming, costly, and can be impractical, particularly for large-scale or diverse soil profiles. Recent progress in artificial intelligence, especially machine learning (ML), has enabled data-driven approaches for modeling complex soil behavior with greater speed and precision. This study introduces a comprehensive ML framework for CBR prediction using a dataset of 382 soil samples collected from various geoclimatic regions in Türkiye. The dataset includes physicochemical soil properties relevant to bearing capacity, allowing multidimensional feature representation in a supervised learning context. Twelve ML algorithms were tested, including decision tree, random forest, extra trees, gradient boosting, xgboost, k-nearest neighbors, support vector regression, multi-layer perceptron, adaboost, bagging, voting, and stacking regressors. Each model was trained, validated, and evaluated to assess its generalization and robustness. Among them, the random forest regressor performed the best, achieving strong R2 scores of 0.95 (training), 0.76 (validation), and 0.83 (test). These outcomes highlight the model's powerful nonlinear mapping ability, making it a promising tool for predictive geotechnical tasks. The study supports the integration of intelligent, data-centric models in geotechnical engineering, offering an effective alternative to traditional methods and promoting digital transformation in infrastructure analysis and design.

</details>


### [87] [Soil Compaction Parameters Prediction Based on Automated Machine Learning Approach](https://arxiv.org/abs/2512.08343)
*Caner Erden,Alparslan Serhat Demir,Abdullah Hulusi Kokcam,Talas Fikret Kurnaz,Ugur Dagdeviren*

Main category: cs.AI

TL;DR: 本研究提出了一种基于自动机器学习（AutoML）的方法来预测土壤最优含水率（OMC）和最大干密度（MDD），以替代传统实验室试验和有限适用性的经验回归模型。通过自动化算法选择和超参数优化，XGBoost算法在独立数据集上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 传统确定土壤最优含水率和最大干密度的方法依赖劳动密集型的实验室试验，而经验回归模型在不同土壤类型中的适用性和准确性有限。机器学习方法虽然被提出作为替代方案，但在预测准确性和泛化能力方面仍存在挑战，特别是在处理代表不同土壤类型的异构数据集时。

Method: 本研究采用自动机器学习（AutoML）方法预测土壤最优含水率和最大干密度。AutoML自动化了算法选择和超参数优化过程，通过广泛的实验比较不同机器学习算法的性能。

Result: 研究发现极端梯度提升（XGBoost）算法表现最佳，在独立数据集上对最大干密度的预测R平方值为80.4%，对最优含水率的预测R平方值为89.1%。这些结果表明AutoML方法在预测不同土壤类型的压实参数方面具有有效性。

Conclusion: AutoML方法能够有效预测土壤压实参数，异构数据集对提高机器学习模型的泛化能力和性能至关重要。这项研究通过增强土壤压实参数的预测能力，为更高效可靠的施工实践做出了贡献。

Abstract: Soil compaction is critical in construction engineering to ensure the stability of structures like road embankments and earth dams. Traditional methods for determining optimum moisture content (OMC) and maximum dry density (MDD) involve labor-intensive laboratory experiments, and empirical regression models have limited applicability and accuracy across diverse soil types. In recent years, artificial intelligence (AI) and machine learning (ML) techniques have emerged as alternatives for predicting these compaction parameters. However, ML models often struggle with prediction accuracy and generalizability, particularly with heterogeneous datasets representing various soil types. This study proposes an automated machine learning (AutoML) approach to predict OMC and MDD. AutoML automates algorithm selection and hyperparameter optimization, potentially improving accuracy and scalability. Through extensive experimentation, the study found that the Extreme Gradient Boosting (XGBoost) algorithm provided the best performance, achieving R-squared values of 80.4% for MDD and 89.1% for OMC on a separate dataset. These results demonstrate the effectiveness of AutoML in predicting compaction parameters across different soil types. The study also highlights the importance of heterogeneous datasets in improving the generalization and performance of ML models. Ultimately, this research contributes to more efficient and reliable construction practices by enhancing the prediction of soil compaction parameters.

</details>


### [88] [DeepFeature: Iterative Context-aware Feature Generation for Wearable Biosignals](https://arxiv.org/abs/2512.08379)
*Kaiwei Liu,Yuting He,Bufang Yang,Mu Yuan,Chun Man Victor Wong,Ho Pong Andrew Sze,Zhenyu Yan,Hongkai Chen*

Main category: cs.AI

TL;DR: DeepFeature：首个基于LLM的上下文感知特征生成框架，用于可穿戴生物信号，通过多源特征生成、迭代特征精炼和鲁棒代码转换，在8个任务上平均AUROC提升4.21-9.67%


<details>
  <summary>Details</summary>
Motivation: 当前可穿戴设备生物信号的特征提取方法缺乏任务特定的上下文知识，在高维特征空间中难以找到最优设置，且容易产生代码生成和自动化错误

Method: 提出DeepFeature框架：1）多源特征生成机制，整合专家知识和任务设置；2）迭代特征精炼过程，基于特征评估反馈重新选择特征；3）鲁棒的多层过滤和验证方法，确保特征到代码的可靠转换

Result: 在8个多样化任务上，DeepFeature相比基线方法平均AUROC提升4.21-9.67%，在5个任务上优于最先进方法，其余任务保持相当性能

Conclusion: DeepFeature是首个LLM赋能的上下文感知特征生成框架，通过整合专家知识、迭代精炼和鲁棒代码转换，显著提升了可穿戴生物信号的特征提取效果

Abstract: Biosignals collected from wearable devices are widely utilized in healthcare applications. Machine learning models used in these applications often rely on features extracted from biosignals due to their effectiveness, lower data dimensionality, and wide compatibility across various model architectures. However, existing feature extraction methods often lack task-specific contextual knowledge, struggle to identify optimal feature extraction settings in high-dimensional feature space, and are prone to code generation and automation errors. In this paper, we propose DeepFeature, the first LLM-empowered, context-aware feature generation framework for wearable biosignals. DeepFeature introduces a multi-source feature generation mechanism that integrates expert knowledge with task settings. It also employs an iterative feature refinement process that uses feature assessment-based feedback for feature re-selection. Additionally, DeepFeature utilizes a robust multi-layer filtering and verification approach for robust feature-to-code translation to ensure that the extraction functions run without crashing. Experimental evaluation results show that DeepFeature achieves an average AUROC improvement of 4.21-9.67% across eight diverse tasks compared to baseline methods. It outperforms state-of-the-art approaches on five tasks while maintaining comparable performance on the remaining tasks.

</details>


### [89] [Using reinforcement learning to probe the role of feedback in skill acquisition](https://arxiv.org/abs/2512.08463)
*Antonio Terpin,Raffaello D'Andrea*

Main category: cs.AI

TL;DR: 研究通过强化学习智能体控制旋转圆柱体在水流中的阻力，发现学习高性能技能需要比执行技能更丰富的信息反馈，且学习条件的好坏取决于目标而非系统复杂性。


<details>
  <summary>Details</summary>
Motivation: 研究人类在无外部反馈情况下执行高性能技能（如花样滑冰、投球）的学习过程，但绕过人类受试者，在完全受控条件下探索技能获取机制。

Method: 使用通用强化学习智能体直接控制桌面循环水槽中的旋转圆柱体，通过最大化或最小化阻力来学习控制策略。实验设置包括高维流动反馈和无反馈两种训练条件。

Result: 高维流动反馈让智能体在几分钟内发现高性能阻力控制策略；无反馈执行时性能几乎相同。无流动反馈训练时，智能体在阻力最大化任务中失败，在阻力最小化中表现较差且不稳定。

Conclusion: 学习高性能技能需要比执行技能更丰富的信息反馈，学习条件的好坏（温和或恶劣）仅取决于目标，而非系统动力学或策略复杂性。

Abstract: Many high-performance human activities are executed with little or no external feedback: think of a figure skater landing a triple jump, a pitcher throwing a curveball for a strike, or a barista pouring latte art. To study the process of skill acquisition under fully controlled conditions, we bypass human subjects. Instead, we directly interface a generalist reinforcement learning agent with a spinning cylinder in a tabletop circulating water channel to maximize or minimize drag. This setup has several desirable properties. First, it is a physical system, with the rich interactions and complex dynamics that only the physical world has: the flow is highly chaotic and extremely difficult, if not impossible, to model or simulate accurately. Second, the objective -- drag minimization or maximization -- is easy to state and can be captured directly in the reward, yet good strategies are not obvious beforehand. Third, decades-old experimental studies provide recipes for simple, high-performance open-loop policies. Finally, the setup is inexpensive and far easier to reproduce than human studies. In our experiments we find that high-dimensional flow feedback lets the agent discover high-performance drag-control strategies with only minutes of real-world interaction. When we later replay the same action sequences without any feedback, we obtain almost identical performance. This shows that feedback, and in particular flow feedback, is not needed to execute the learned policy. Surprisingly, without flow feedback during training the agent fails to discover any well-performing policy in drag maximization, but still succeeds in drag minimization, albeit more slowly and less reliably. Our studies show that learning a high-performance skill can require richer information than executing it, and learning conditions can be kind or wicked depending solely on the goal, not on dynamics or policy complexity.

</details>


### [90] [Autonomous Issue Resolver: Towards Zero-Touch Code Maintenance](https://arxiv.org/abs/2512.08492)
*Aliaksei Kaliutau*

Main category: cs.AI

TL;DR: 提出从代码属性图转向数据转换图的新范式，通过多智能体框架实现仓库级程序修复，解决了现有RAG系统的语义陷阱问题


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在函数级代码生成方面取得进展，但仓库级自动程序修复仍面临挑战。现有方法采用控制中心范式，迫使智能体处理复杂的目录结构和无关的控制逻辑，存在局限性。

Method: 提出从标准代码属性图转向数据转换图的概念，将数据状态建模为节点、函数建模为边，通过数据谱系而非控制流追踪逻辑缺陷。引入多智能体框架，协调数据完整性导航与控制流逻辑，并实现自主问题解决器系统。

Result: 在多个软件工程基准测试中取得良好结果，在SWE-Verified基准测试上达到87.1%的解决率，直接解决了当前AI代码辅助工具的核心限制。

Conclusion: 该方法通过数据转换图和多智能体框架，解决了现代编码智能体中标准RAG系统固有的"语义陷阱"，为零接触代码维护提供了可扩展的逻辑修复基础。

Abstract: Recent advances in Large Language Models have revolutionized function-level code generation; however, repository-scale Automated Program Repair (APR) remains a significant challenge. Current approaches typically employ a control-centric paradigm, forcing agents to navigate complex directory structures and irrelevant control logic. In this paper, we propose a paradigm shift from the standard Code Property Graphs (CPGs) to the concept of Data Transformation Graph (DTG) that inverts the topology by modeling data states as nodes and functions as edges, enabling agents to trace logic defects through data lineage rather than control flow. We introduce a multi-agent framework that reconciles data integrity navigation with control flow logic. Our theoretical analysis and case studies demonstrate that this approach resolves the "Semantic Trap" inherent in standard RAG systems in modern coding agents. We provide a comprehensive implementation in the form of Autonomous Issue Resolver (AIR), a self-improvement system for zero-touch code maintenance that utilizes neuro-symbolic reasoning and uses the DTG structure for scalable logic repair. Our approach has demonstrated good results on several SWE benchmarks, reaching a resolution rate of 87.1% on SWE-Verified benchmark. Our approach directly addresses the core limitations of current AI code-assistant tools and tackles the critical need for a more robust foundation for our increasingly software-dependent world.

</details>


### [91] [Principles2Plan: LLM-Guided System for Operationalising Ethical Principles into Plans](https://arxiv.org/abs/2512.08536)
*Tammy Zhong,Yang Song,Maurice Pagnucco*

Main category: cs.AI

TL;DR: Principles2Plan：一个交互式研究原型，展示人类与大型语言模型如何协作生成情境敏感的伦理规则，指导自动化规划


<details>
  <summary>Details</summary>
Motivation: 机器人在人类环境中操作需要伦理意识，但现有自动化规划工具对此支持不足。手动指定伦理规则劳动密集且高度情境特定，需要更实用的解决方案。

Method: 提出Principles2Plan系统：领域专家提供规划领域、问题细节和相关高级原则（如仁爱、隐私）；系统生成与这些原则一致的可操作伦理规则；用户可以审查、优先排序并提供给规划器生成伦理知情计划。

Result: 开发了一个交互式研究原型，首次支持用户在经典规划情境中生成基于原则的伦理规则，展示了人类-LLM协作在伦理自动化规划中的潜力。

Conclusion: Principles2Plan展示了人类与大型语言模型协作的潜力，使伦理自动化规划更加实用可行，填补了现有系统在原则基础伦理规则生成方面的空白。

Abstract: Ethical awareness is critical for robots operating in human environments, yet existing automated planning tools provide little support. Manually specifying ethical rules is labour-intensive and highly context-specific. We present Principles2Plan, an interactive research prototype demonstrating how a human and a Large Language Model (LLM) can collaborate to produce context-sensitive ethical rules and guide automated planning. A domain expert provides the planning domain, problem details, and relevant high-level principles such as beneficence and privacy. The system generates operationalisable ethical rules consistent with these principles, which the user can review, prioritise, and supply to a planner to produce ethically-informed plans. To our knowledge, no prior system supports users in generating principle-grounded rules for classical planning contexts. Principles2Plan showcases the potential of human-LLM collaboration for making ethical automated planning more practical and feasible.

</details>


### [92] [CogMCTS: A Novel Cognitive-Guided Monte Carlo Tree Search Framework for Iterative Heuristic Evolution with Large Language Models](https://arxiv.org/abs/2512.08609)
*Hui Wang,Yang Liu,Xiaoyu Zhang,Chaoxu Mu*

Main category: cs.AI

TL;DR: CogMCTS框架将LLM认知引导与MCTS结合，通过多轮认知反馈、双轨节点扩展和策略变异，实现高效自动启发式设计，在稳定性、效率和解决方案质量上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的进化方法容易陷入局部最优，而LLM与MCTS结合的方法在多轮认知整合和搜索多样性方面仍有局限，需要克服这些限制以实现更高效的自动启发式设计。

Method: 提出CogMCTS框架：1) 多轮认知反馈整合历史经验、节点信息和负面结果；2) 双轨节点扩展结合精英启发式管理平衡探索与利用；3) 策略变异修改启发式形式和参数增强多样性。

Result: 实验结果表明，CogMCTS在稳定性、效率和解决方案质量方面优于现有的基于LLM的自动启发式设计方法。

Conclusion: CogMCTS通过紧密集成LLM认知引导与MCTS，有效解决了现有方法的局限性，实现了更高效的自动启发式优化。

Abstract: Automatic Heuristic Design (AHD) is an effective1 framework for solving complex optimization prob-2 lems. The development of large language mod-3 els (LLMs) enables the automated generation of4 heuristics. Existing LLM-based evolutionary meth-5 ods rely on population strategies and are prone6 to local optima. Integrating LLMs with Monte7 Carlo Tree Search (MCTS) improves the trade-off8 between exploration and exploitation, but multi-9 round cognitive integration remains limited and10 search diversity is constrained. To overcome these11 limitations, this paper proposes a novel cognitive-12 guided MCTS framework (CogMCTS). CogMCTS13 tightly integrates the cognitive guidance mecha-14 nism of LLMs with MCTS to achieve efficient au-15 tomated heuristic optimization. The framework16 employs multi-round cognitive feedback to incor-17 porate historical experience, node information, and18 negative outcomes, dynamically improving heuris-19 tic generation. Dual-track node expansion com-20 bined with elite heuristic management balances the21 exploration of diverse heuristics and the exploita-22 tion of high-quality experience. In addition, strate-23 gic mutation modifies the heuristic forms and pa-24 rameters to further enhance the diversity of the so-25 lution and the overall optimization performance.26 The experimental results indicate that CogMCTS27 outperforms existing LLM-based AHD methods in28 stability, efficiency, and solution quality.

</details>


### [93] [Protein Secondary Structure Prediction Using Transformers](https://arxiv.org/abs/2512.08613)
*Manzi Kevin Maxime*

Main category: cs.AI

TL;DR: 基于Transformer的蛋白质二级结构预测模型，使用注意力机制处理氨基酸序列，通过滑动窗口数据增强技术提升性能


<details>
  <summary>Details</summary>
Motivation: 从氨基酸序列预测蛋白质二级结构（如α螺旋、β折叠、无规则卷曲）对于理解蛋白质功能至关重要，传统方法在处理变长序列和长程残基相互作用方面存在局限

Method: 提出基于Transformer的模型，应用注意力机制处理蛋白质序列数据；在CB513数据集上使用滑动窗口数据增强技术扩展训练样本；模型能够处理变长序列并有效捕捉局部和长程残基相互作用

Result: Transformer模型在蛋白质二级结构预测任务中表现出强大的泛化能力，能够有效处理变长序列，并成功捕捉残基间的局部和长程相互作用

Conclusion: Transformer架构适用于蛋白质二级结构预测任务，其注意力机制能够有效建模氨基酸序列中的复杂模式，为蛋白质结构预测提供了新的有效方法

Abstract: Predicting protein secondary structures such as alpha helices, beta sheets, and coils from amino acid sequences is essential for understanding protein function. This work presents a transformer-based model that applies attention mechanisms to protein sequence data to predict structural motifs. A sliding-window data augmentation technique is used on the CB513 dataset to expand the training samples. The transformer shows strong ability to generalize across variable-length sequences while effectively capturing both local and long-range residue interactions.

</details>


### [94] [Multi-Agent Intelligence for Multidisciplinary Decision-Making in Gastrointestinal Oncology](https://arxiv.org/abs/2512.08674)
*Rongzhao Zhang,Junqiao Wang,Shuyun Yang,Mouxiao Bian,Chao Ding,Yuwei Bai,Chihao Zhang,Yuguang Shen,Lei Wang,Lei Zheng,Qiujuan Yan,Yun Zhong,Meiling Liu,Jiwei Yu,Zheng Wang,Jie Xu,Meng Luo*

Main category: cs.AI

TL;DR: 提出分层多智能体框架模拟多学科团队协作，解决多模态大语言模型在胃肠肿瘤临床推理中的上下文稀释和幻觉问题，显著提升推理逻辑和医学准确性。


<details>
  <summary>Details</summary>
Motivation: 胃肠肿瘤学中的多模态临床推理需要整合内镜图像、放射学数据和生化标志物。尽管多模态大语言模型展现出潜力，但在处理复杂、异质的医疗历史时经常面临上下文稀释和幻觉等挑战。

Method: 提出分层多智能体框架，模拟人类多学科团队的协作工作流程，以解决传统MLLM在处理复杂医疗数据时的局限性。

Result: 系统获得4.60/5.00的综合专家评估分数，相比单体基线有显著提升。基于智能体的架构在推理逻辑和医学准确性方面带来最显著的改进。

Conclusion: 模拟性、基于智能体的合作为肿瘤学中的自动化决策支持提供了一个可扩展、可解释且临床稳健的范式。

Abstract: Multimodal clinical reasoning in the field of gastrointestinal (GI) oncology necessitates the integrated interpretation of endoscopic imagery, radiological data, and biochemical markers. Despite the evident potential exhibited by Multimodal Large Language Models (MLLMs), they frequently encounter challenges such as context dilution and hallucination when confronted with intricate, heterogeneous medical histories. In order to address these limitations, a hierarchical Multi-Agent Framework is proposed, which emulates the collaborative workflow of a human Multidisciplinary Team (MDT). The system attained a composite expert evaluation score of 4.60/5.00, thereby demonstrating a substantial improvement over the monolithic baseline. It is noteworthy that the agent-based architecture yielded the most substantial enhancements in reasoning logic and medical accuracy. The findings indicate that mimetic, agent-based collaboration provides a scalable, interpretable, and clinically robust paradigm for automated decision support in oncology.

</details>


### [95] [Towards Foundation Models with Native Multi-Agent Intelligence](https://arxiv.org/abs/2512.08743)
*Shuyue Hu,Haoyang Yan,Yiqun Zhang,Yang Chen,Dongzhan Zhou,Lei Bai*

Main category: cs.AI

TL;DR: 基础模型在多智能体场景中表现不佳，需要专门研究来赋予其原生多智能体智能


<details>
  <summary>Details</summary>
Motivation: 虽然基础模型正成为AI智能体的"大脑"，但目前主要关注单智能体能力，而多智能体智能是下一个前沿领域。研究发现强大的单智能体性能并不能自动转化为稳健的多智能体智能。

Method: 通过41个大语言模型的广泛实证研究，识别了基础模型在多智能体环境中的四个核心能力：理解、规划、高效通信和适应。基于研究发现，提出了构建具有原生多智能体智能的基础模型的关键研究方向。

Result: 实证研究表明，强大的单智能体性能并不能自动产生稳健的多智能体智能，揭示了当前基础模型在多智能体场景中的局限性。

Conclusion: 需要专门的研究方向（包括数据集构建、评估、训练范式和安全性考虑）来构建具有原生多智能体智能的基础模型，这是AI发展的下一个关键前沿。

Abstract: Foundation models (FMs) are increasingly assuming the role of the "brain" of AI agents. While recent efforts have begun to equip FMs with native single-agent abilities -- such as GUI interaction or integrated tool use -- we argue that the next frontier is endowing FMs with native multi-agent intelligence. We identify four core capabilities of FMs in multi-agent contexts: understanding, planning, efficient communication, and adaptation. Contrary to assumptions about the spontaneous emergence of such abilities, we provide extensive empirical evidence across 41 large language models showing that strong single-agent performance alone does not automatically yield robust multi-agent intelligence. To address this gap, we outline key research directions -- spanning dataset construction, evaluation, training paradigms, and safety considerations -- for building FMs with native multi-agent intelligence.

</details>


### [96] [A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows](https://arxiv.org/abs/2512.08769)
*Eranga Bandara,Ross Gore,Peter Foytik,Sachin Shetty,Ravi Mukkamala,Abdul Rahman,Xueping Liang,Safdar H. Bouk,Amin Hass,Sachini Rajapakse,Ng Wee Keong,Kasun De Zoysa,Aruna Withanage,Nilaan Loganathan*

Main category: cs.AI

TL;DR: 本文提供了一个端到端的实用指南，用于设计、开发和部署生产级智能体AI系统，介绍了结构化工程生命周期和九大核心最佳实践，并通过多模态新闻分析案例进行验证。


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI在各行业加速采用，组织面临如何设计、工程化和运营生产级智能体AI工作流的挑战，需要确保其可靠性、可观察性、可维护性，并符合安全和治理要求。

Method: 提出结构化工程生命周期，包括工作流分解、多智能体设计模式、模型上下文协议(MCP)、工具集成、确定性编排、负责任AI考虑和环境感知部署策略，并介绍九大核心最佳实践。

Result: 通过一个多模态新闻分析和媒体生成工作流的全面案例研究，展示了这些原则在实际应用中的有效性，为构建健壮、可扩展和生产就绪的智能体AI工作流提供了基础参考。

Conclusion: 本文提供了构建生产级智能体AI系统的综合指南，结合架构指导、操作模式和实践实施见解，为开发可靠、可扩展的智能体AI工作流奠定了坚实基础。

Abstract: Agentic AI marks a major shift in how autonomous systems reason, plan, and execute multi-step tasks. Unlike traditional single model prompting, agentic workflows integrate multiple specialized agents with different Large Language Models(LLMs), tool-augmented capabilities, orchestration logic, and external system interactions to form dynamic pipelines capable of autonomous decision-making and action. As adoption accelerates across industry and research, organizations face a central challenge: how to design, engineer, and operate production-grade agentic AI workflows that are reliable, observable, maintainable, and aligned with safety and governance requirements. This paper provides a practical, end-to-end guide for designing, developing, and deploying production-quality agentic AI systems. We introduce a structured engineering lifecycle encompassing workflow decomposition, multi-agent design patterns, Model Context Protocol(MCP), and tool integration, deterministic orchestration, Responsible-AI considerations, and environment-aware deployment strategies. We then present nine core best practices for engineering production-grade agentic AI workflows, including tool-first design over MCP, pure-function invocation, single-tool and single-responsibility agents, externalized prompt management, Responsible-AI-aligned model-consortium design, clean separation between workflow logic and MCP servers, containerized deployment for scalable operations, and adherence to the Keep it Simple, Stupid (KISS) principle to maintain simplicity and robustness. To demonstrate these principles in practice, we present a comprehensive case study: a multimodal news-analysis and media-generation workflow. By combining architectural guidance, operational patterns, and practical implementation insights, this paper offers a foundational reference to build robust, extensible, and production-ready agentic AI workflows.

</details>


### [97] [CARLoS: Retrieval via Concise Assessment Representation of LoRAs at Scale](https://arxiv.org/abs/2512.08826)
*Shahar Sarfaty,Adi Haviv,Uri Hacohen,Niva Elkin-Koren,Roi Livni,Amit H. Bermano*

Main category: cs.AI

TL;DR: CARLoS是一个大规模LoRA表征框架，通过分析650多个LoRA组件，使用CLIP嵌入和与基础模型生成的差异，定义方向、强度和一致性三个维度的表示，实现高效的语义检索系统。


<details>
  <summary>Details</summary>
Motivation: 生成式组件（如LoRA）的快速扩散创建了一个庞大但非结构化的生态系统。现有的发现方法依赖于不可靠的用户描述或有偏见的流行度指标，影响了可用性。

Method: 通过分析650多个LoRA，在各种提示和种子下进行图像生成来评估其行为。使用CLIP嵌入及其与基础模型生成的差异，定义三部分表示：方向（定义语义偏移）、强度（量化效果显著性）、一致性（量化效果稳定性）。

Result: 开发了一个高效的检索框架，能够语义匹配文本查询到相关LoRA，同时过滤过强或不稳定的组件，在自动和人工评估中优于文本基线。

Conclusion: CARLoS不仅作为检索系统表现出色，其表示方法还能支持分析强度、一致性与版权法中实质性、意志性等法律概念的关联，使其成为具有更广泛相关性的实用LoRA分析系统。

Abstract: The rapid proliferation of generative components, such as LoRAs, has created a vast but unstructured ecosystem. Existing discovery methods depend on unreliable user descriptions or biased popularity metrics, hindering usability. We present CARLoS, a large-scale framework for characterizing LoRAs without requiring additional metadata. Analyzing over 650 LoRAs, we employ them in image generation over a variety of prompts and seeds, as a credible way to assess their behavior. Using CLIP embeddings and their difference to a base-model generation, we concisely define a three-part representation: Directions, defining semantic shift; Strength, quantifying the significance of the effect; and Consistency, quantifying how stable the effect is. Using these representations, we develop an efficient retrieval framework that semantically matches textual queries to relevant LoRAs while filtering overly strong or unstable ones, outperforming textual baselines in automated and human evaluations. While retrieval is our primary focus, the same representation also supports analyses linking Strength and Consistency to legal notions of substantiality and volition, key considerations in copyright, positioning CARLoS as a practical system with broader relevance for LoRA analysis.

</details>


### [98] [Interpolation in Knowledge Representation](https://arxiv.org/abs/2512.08833)
*Jean Christoph Jung,Patrick Koopmann,Matthias Knorr*

Main category: cs.AI

TL;DR: 本文探讨了描述逻辑和逻辑编程中的Craig插值和均匀插值，包括理论结果和实际计算方法


<details>
  <summary>Details</summary>
Motivation: Craig插值和均匀插值在知识表示中有重要应用，包括可解释性、遗忘、模块化和重用等，但许多知识表示形式化方法通常缺乏这些插值特性，且实际计算插值具有挑战性

Method: 深入分析两种主要的知识表示形式化方法：描述逻辑和逻辑编程，讨论计算插值的理论结果和实用方法

Result: 论文探讨了这两种形式化方法中插值的理论性质，并提供了实际计算插值的方法

Conclusion: Craig插值和均匀插值在知识表示中具有重要价值，需要进一步研究描述逻辑和逻辑编程中的插值理论和计算方法

Abstract: Craig interpolation and uniform interpolation have many applications in knowledge representation, including explainability, forgetting, modularization and reuse, and even learning. At the same time, many relevant knowledge representation formalisms do in general not have Craig or uniform interpolation, and computing interpolants in practice is challenging. We have a closer look at two prominent knowledge representation formalisms, description logics and logic programming, and discuss theoretical results and practical methods for computing interpolants.

</details>


### [99] [Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs](https://arxiv.org/abs/2512.08923)
*Angela van Sprang,Laurens Samson,Ana Lucic,Erman Acar,Sennay Ghebreab,Yuki M. Asano*

Main category: cs.AI

TL;DR: 论文提出了REST和REST+两个新基准，用于系统评估多模态大语言模型中的跨模态不一致性问题，发现当前最先进的MLLMs在不同模态（图像、文本、混合）中无法保持一致的推理能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型被训练在同一个嵌入空间中表示视觉和语言信息，但它们无法在不同模态中执行相同的任务。为了系统评估这种跨模态不一致性，需要专门的基准测试工具。

Method: 创建了REST和REST+两个基准，包含相同语义信息但呈现于三种不同模态（纯图像、纯文本、混合模态）的样本。评估了15个MLLMs，分析了文本识别（OCR）问题、视觉特征（文本颜色、分辨率、字体）以及视觉token数量对模型性能的影响。

Result: 研究发现：1）不同MLLMs的模态不一致程度差异很大；2）将文本渲染为图像或将图像渲染为文本都无法解决不一致性问题；3）即使OCR正确，视觉特征（文本颜色和分辨率）和视觉token数量仍影响模型性能；4）一致性得分与文本和图像之间的模态差距相关。

Conclusion: 多模态大语言模型存在显著的跨模态不一致问题，REST基准能够有效评估这一问题。研究揭示了模态不一致的机制性解释，为改进MLLMs的跨模态一致性提供了重要见解。

Abstract: We introduce two new benchmarks REST and REST+(Render-Equivalence Stress Tests) to enable systematic evaluation of cross-modal inconsistency in multimodal large language models (MLLMs). MLLMs are trained to represent vision and language in the same embedding space, yet they cannot perform the same tasks in both modalities. Our benchmarks contain samples with the same semantic information in three modalities (image, text, mixed) and we show that state-of-the-art MLLMs cannot consistently reason over these different modalities. We evaluate 15 MLLMs and find that the degree of modality inconsistency varies substantially, even when accounting for problems with text recognition (OCR). Neither rendering text as image nor rendering an image as text solves the inconsistency. Even if OCR is correct, we find that visual characteristics (text colour and resolution, but not font) and the number of vision tokens have an impact on model performance. Finally, we find that our consistency score correlates with the modality gap between text and images, highlighting a mechanistic interpretation of cross-modal inconsistent MLLMs.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [100] [Quantum Circuit Reasoning Models: A Variational Framework for Differentiable Logical Inference](https://arxiv.org/abs/2512.07871)
*Andrew Kiruluta*

Main category: quant-ph

TL;DR: 量子电路推理模型（QCRM）将变分量子电路从能量最小化和分类任务扩展到结构化逻辑推理，利用量子力学操作映射推理原语，实现可微分的混合推理架构。


<details>
  <summary>Details</summary>
Motivation: 将量子计算的基本操作（叠加、纠缠、干涉、测量）与推理的基本原语（假设分支、约束传播、一致性执行、决策制定）建立映射关系，扩展变分量子电路在结构化逻辑推理中的应用。

Method: 开发QCRM的数学基础，定义参数化电路架构，将逻辑规则编码为命题量子比特状态上的幺正变换，基于经典梯度下降制定训练目标，并在经典硬件上实现模拟。

Result: 提出量子推理层（QRL）作为可微分的混合组件，适用于科学、生物医学和化学推理领域，实现推理作为振幅演化和干涉驱动的一致性状态选择过程。

Conclusion: QCRM框架成功将量子启发式计算与可微分优化相结合，为结构化逻辑推理提供了新的量子计算范式，展示了量子力学操作与推理原语之间的自然映射关系。

Abstract: This report introduces a novel class of reasoning architectures, termed Quantum Circuit Reasoning Models (QCRM), which extend the concept of Variational Quantum Circuits (VQC) from energy minimization and classification tasks to structured logical inference and reasoning. We posit that fundamental quantum mechanical operations, superposition, entanglement, interference, and measurement, naturally map to essential reasoning primitives such as hypothesis branching, constraint propagation, consistency enforcement, and decision making. The resulting framework combines quantum-inspired computation with differentiable optimization, enabling reasoning to emerge as a process of amplitude evolution and interference-driven selection of self-consistent states. We develop the mathematical foundation of QCRM, define its parameterized circuit architecture, and show how logical rules can be encoded as unitary transformations over proposition-qubit states. We further formalize a training objective grounded in classical gradient descent over circuit parameters and discuss simulation-based implementations on classical hardware. Finally, we propose the Quantum Reasoning Layer (QRL) as a differentiable hybrid component for composable reasoning models applicable to scientific, biomedical, and chemical inference domains.

</details>


### [101] [The State-Operator Clifford Compatibility: A Real Algebraic Framework for Quantum Information](https://arxiv.org/abs/2512.07902)
*Kagwe A. Muchane*

Main category: quant-ph

TL;DR: 该论文提出了一种基于实代数框架的N量子比特计算新方法，使用Clifford代数张量积结构，将Pauli操作表示为Clifford元素的左作用，并建立了状态-算子兼容性定律。


<details>
  <summary>Details</summary>
Motivation: 重新审视Pauli-Clifford联系，为N量子比特量子计算引入一个实代数框架，避免传统复数表示，利用几何代数结构更自然地描述量子计算操作。

Method: 基于张量积结构Cℓ₂,₀(ℝ)⊗ᴺ，使用双向量J=e₁₂提供复结构，Pauli操作作为Clifford元素的左作用，采用规范稳定子映射将计算基态表示为实代数幂等元的张量积。

Result: 建立了状态-算子Clifford兼容性定律，该定律在N量子比特的几何积下稳定，并使符号Clifford乘法与希尔伯特空间上的酉演化保持一致。

Conclusion: 该实代数框架为量子计算提供了更自然的数学描述，将Pauli-Clifford联系系统化，并建立了代数操作与量子演化之间的直接对应关系。

Abstract: We revisit the Pauli-Clifford connection to introduce a real, grade-preserving algebraic framework for $N$-qubit quantum computation based on the tensor product structure $C\ell_{2,0}(\mathbb{R})^{\otimes N}$. In this setting the bivector $J = e_{12}$ satisfies $J^{2} = -1$ and supplies the complex structure on a minimal left ideal via right-multiplication, while Pauli operations arise as left actions of suitable Clifford elements. Adopting a canonical stabilizer mapping, the $N$-qubit computational basis state $|0\cdots 0\rangle$ is represented natively by a tensor product of real algebraic idempotents. This structural choice leads to a State-Operator Clifford Compatibility law that is stable under the geometric product for $N$ qubits and aligns symbolic Clifford multiplication with unitary evolution on the Hilbert space.

</details>


### [102] [Quantum catalysis-enhanced extract energy in qubit quantum battery](https://arxiv.org/abs/2512.07906)
*Shun-Cai Zhao*

Main category: quant-ph

TL;DR: 量子催化通过诱导瞬态负热流（能量回流）来增强开放系统中量子电池的性能，这种负热流主动抵消退相干损耗，快速推动量子比特进入非被动态，显著提高可提取功


<details>
  <summary>Details</summary>
Motivation: 探究量子催化在开放系统中提升量子电池性能的物理机制，特别是理解催化剂如何通过热力学过程增强量子电池的工作能力

Method: 研究外部场驱动的量子比特量子电池与谐振子催化剂的耦合系统，分析催化剂诱导的瞬态负热流（能量回流）效应，利用量子第一定律精确量化负热流与量子电池性能增强之间的因果关系

Result: 发现催化剂诱导的瞬态负热流（J(t)<0）是量子催化增强量子电池性能的关键机制，这种能量回流主动抵消退相干损耗，快速推动量子比特进入非被动态，导致可提取功（Ergotropy）的显著增强

Conclusion: 揭示了瞬态热力学回流在量子催化中的基本作用，为高性能量子能量存储设备提供了关键蓝图，证明了负热流与量子电池性能增强之间的直接因果关系

Abstract: What physical mechanism enables quantum catalysis to boost quantum battery (QB) performance in open systems? We investigate an external-field-driven qubit QB coupled to a harmonic oscillator catalyst, revealing a key thermodynamic mechanism: the catalyst induces transient negative heat flow ($J(t)<0$, or energy backflow) into the battery. This backflow actively counters dephasing losses, rapidly pushing the qubit into non-passive states, and results in a drastic enhancement of extractable work (Ergotropy). Leveraging the quantum first law, we precisely quantify this causal link between negative heat flux and QB performance enhancement. Our work uncovers the fundamental role of transient thermodynamic backflow in quantum catalysis, offering a crucial blueprint for high-performance quantum energy storage devices.

</details>


### [103] [Symmetry-Based Quantum Codes Beyond the Pauli Group](https://arxiv.org/abs/2512.07908)
*Zachary P. Bradshaw,Margarite L. LaBorde,Dillon Montero*

Main category: quant-ph

TL;DR: 提出基于群表示论的量子纠错码通用框架，将对称性纳入编码设计，可被动缓解特定错误并检测其他错误，统一了现有稳定子码


<details>
  <summary>Details</summary>
Motivation: 传统稳定子码不考虑具体系统的对称性结构，需要开发能利用系统对称性的纠错码框架

Method: 基于有限群的表示论构建量子码，码空间在群作用下不变，对表示像中的错误提供被动纠错，其他错误通过不可约表示的等型分量投影测量检测

Result: 证明所有稳定子码（包括qudit稳定子码）都是该框架的特例，构建了二面体群相关的单逻辑量子比特码

Conclusion: 提供了统一现有码并促进针对特定系统的对称性感知码设计的框架

Abstract: Typical stabilizer codes aim to solve the general problem of fault-tolerance without regard for the structure of a specific system. By incorporating a broader representation-theoretic perspective, we provide a generalized framework that allows the code designer to take this structure into account. For any representation of a finite group, we produce a quantum code with a code space invariant under the group action, providing passive error mitigation against errors belonging to the image of the representation. Furthermore, errors outside this scope are detected and diagnosed by performing a projective measurement onto the isotypic components corresponding to irreducible representations of the chosen group, effectively generalizing syndrome extraction to symmetry-resolved quantum measurements. We show that all stabilizer codes are a special case of this construction, including qudit stabilizer codes, and show that there is a natural one logical qubit code associated to the dihedral group. Thus we provide a unifying framework for existing codes while simultaneously facilitating symmetry-aware codes tailored to specific systems.

</details>


### [104] [Fair Benchmarking of Optimisation Applications](https://arxiv.org/abs/2512.07915)
*Frank Phillipson*

Main category: quant-ph

TL;DR: 该论文提出了量子优化公平基准测试的原则和协议，强调端到端工作流程、调优透明度、问题多样性，避免投机性声明，旨在建立可复现、可比较且可信的评估框架。


<details>
  <summary>Details</summary>
Motivation: 量子优化作为新兴方法，其性能评估面临挑战。传统基于数字复杂性理论的基准测试方法无法直接捕捉量子系统的连续动态、概率结果和工作流程开销，需要建立更公平的评估框架。

Method: 提出量子优化公平基准测试的原则和协议，包括：强调端到端工作流程、调优和报告透明度、问题多样性、避免投机性声明。借鉴经典基准测试经验，结合应用驱动和能源感知指标。

Result: 构建了一个框架，使实践者能够负责任地评估量子方法，确保报告结果的可复现性、可比较性和可信度，为量子优化性能评估提供系统化方法。

Conclusion: 通过建立公平的基准测试原则和协议，能够更准确地评估量子优化性能，促进该领域的健康发展，确保研究成果的可信度和实用性。

Abstract: Quantum optimisation is emerging as a promising approach alongside classical heuristics and specialised hardware, yet its performance is often difficult to assess fairly. Traditional benchmarking methods, rooted in digital complexity theory, do not directly capture the continuous dynamics, probabilistic outcomes, and workflow overheads of quantum and hybrid systems. This paper proposes principles and protocols for fair benchmarking of quantum optimisation, emphasising end-to-end workflows, transparency in tuning and reporting, problem diversity, and avoidance of speculative claims. By extending lessons from classical benchmarking and incorporating application-driven and energy-aware metrics, we outline a framework that enables practitioners to evaluate quantum methods responsibly, ensuring reproducibility, comparability, and trust in reported results.

</details>


### [105] [Quantum computing of nonlinear reacting flows via the probability density function method](https://arxiv.org/abs/2512.07918)
*Jizhi Zhang,Ziang Yang,Zhaoyuan Meng,Zhen Lu,Yue Yang*

Main category: quant-ph

TL;DR: 本文提出了一种量子计算框架，用于解决非线性反应流的模拟问题。通过概率密度函数(PDF)方法将非线性方程转化为高维线性方程，并利用历史状态方法将整个时间演化作为单个大型线性系统求解，避免了传统时间步进方案的测量瓶颈，充分发挥量子线性系统算法的优势。


<details>
  <summary>Details</summary>
Motivation: 量子计算在科学计算中具有加速潜力，但其在反应流领域的应用受到非线性源项和时变模拟挑战的阻碍。本文旨在解决这些问题，为量子计算在非线性反应流中的应用开辟途径。

Method: 1. 采用概率密度函数(PDF)方法将非线性反应流控制方程转化为高维线性方程；2. 使用历史状态方法将整个时间演化作为单个大型线性系统求解；3. 开发高效算法测量PDF的统计矩，避免昂贵的全状态层析；4. 通过计算复杂性分析评估潜在加速效果。

Result: 计算复杂性分析表明该方法相对于经典算法具有近指数级加速潜力。通过完美搅拌反应器的模拟验证了框架的有效性，证明了其能够捕捉非线性反应系统的PDF演化和统计特性。

Conclusion: 该工作为量子计算在非线性反应流中的应用建立了可行途径，通过PDF方法、历史状态技术和高效测量算法的结合，克服了非线性项和时变模拟的挑战，展示了量子计算在复杂流体动力学模拟中的潜力。

Abstract: Quantum computing offers the promise of speedups for scientific computations, but its application to reacting flows is hindered by nonlinear source terms and the challenges of time-dependent simulations. We present a quantum framework to address these issues. We employ a probability density function (PDF) formulation to transform the nonlinear reacting-flow governing equations into high-dimensional linear ones. The entire temporal evolution is then solved as a single large linear system using the history state method, which avoids the measurement bottleneck of conventional time-marching schemes and fully leverages the advantages of quantum linear system algorithms. To extract the quantity of interest from the resulting quantum state, we develop an efficient algorithm to measure the statistical moments of the PDF, bypassing the need for costly full-state tomography. A computational complexity analysis indicates the potential for a near-exponential speedup over classical algorithms. We validate the framework by simulating a perfectly stirred reactor, demonstrating its capability to capture the PDF evolution and statistics of a nonlinear reactive system. This work establishes a pathway for applying quantum computing to nonlinear reacting flows.

</details>


### [106] [Quantum algorithms for viscosity solutions to nonlinear Hamilton-Jacobi equations based on an entropy penalisation method](https://arxiv.org/abs/2512.07919)
*Shi Jin,Nana Liu*

Main category: quant-ph

TL;DR: 本文提出了一种高效提取凸哈密顿量非线性Hamilton-Jacobi方程粘性解的框架，该方法基于熵惩罚方法，将粘性Hamilton-Jacobi动力学重新表述为离散时间线性动力学，适用于量子模拟。


<details>
  <summary>Details</summary>
Motivation: Hamilton-Jacobi方程的粘性解在波前传播、平均场博弈、最优控制、机器学习等领域具有核心作用，但现有量子算法在处理非线性偏微分方程时面临主要障碍，特别是在长时间尺度上。

Method: 基于Gomes和Valdinoci提出的熵惩罚方法，将Cole-Hopf变换从二次型推广到一般凸哈密顿量，将粘性Hamilton-Jacobi动力学重新表述为离散时间线性动力学，近似于线性热方程，并可扩展到连续时间动力学。

Result: 该方法适用于任意对应凸哈密顿量的非线性问题，且适用于任意长时间尺度，克服了大多数非线性偏微分方程量子算法的主要障碍。提供了模拟和数字量子算法，用于提取粘性解的点值、梯度、最小值及在最小化器处的函数评估。

Conclusion: 该框架为高效提取凸哈密顿量非线性Hamilton-Jacobi方程的粘性解提供了量子算法，无需非线性更新或完整状态重构，适用于量子模拟，具有广泛的应用前景。

Abstract: We present a framework for efficient extraction of the viscosity solutions of nonlinear Hamilton-Jacobi equations with convex Hamiltonians. These viscosity solutions play a central role in areas such as front propagation, mean-field games, optimal control, machine learning, and a direct application to the forced Burgers' equation. Our method is based on an entropy penalisation method proposed by Gomes and Valdinoci, which generalises the Cole-Hopf transform from quadratic to general convex Hamiltonians, allowing a reformulation of viscous Hamilton-Jacobi dynamics by a discrete-time linear dynamics which approximates a linear heat-like parabolic equation, and can also extend to continuous-time dynamics. This makes the method suitable for quantum simulation. The validity of these results hold for arbitrary nonlinearity that correspond to convex Hamiltonians, and for arbitrarily long times, thus obviating a chief obstacle in most quantum algorithms for nonlinear partial differential equations. We provide quantum algorithms, both analog and digital, for extracting pointwise values, gradients, minima, and function evaluations at the minimiser of the viscosity solution, without requiring nonlinear updates or full state reconstruction.

</details>


### [107] [Exchange Symmetry in Multiphoton Quantum Interference](https://arxiv.org/abs/2512.07953)
*Shreya Kumar,Alex E Jones,Daniel Bhatti,Stefanie Barz*

Main category: quant-ph

TL;DR: 该研究展示了多光子系统中丰富的交换对称性，通过三光子实验揭示了超越传统玻色子行为的混合对称性，并能通过调节光子对对称性来控制量子干涉


<details>
  <summary>Details</summary>
Motivation: 虽然双光子系统中已观察到非玻色子行为，但多光子场景下的交换对称性和干涉效应仍未被充分探索。研究旨在揭示多光子系统中丰富的对称性景观及其对量子干涉的影响

Method: 通过实验研究三光子系统的干涉现象，探索不同光子对组合的交换对称性（玻色子、费米子或任意子对称性），并分析这些对称性配置如何影响观测到的干涉模式

Result: 发现三光子系统中存在多种混合对称性配置，这些配置在双光子系统中无法实现。实验证明可以通过调节组成光子对的对称性来有效开启或关闭多光子干涉

Conclusion: 多光子系统提供了访问和调节新量子统计的平台，这不仅深化了对量子系统的理解，也对依赖量子干涉的量子技术具有重要应用价值

Abstract: Photons are bosons, and yet, when prepared in specific entangled states, they can exhibit non-bosonic behaviour. While this phenomenon has so far been studied in two-photon systems, exchange symmetries and interference effects in multi-photon scenarios remain largely unexplored. In this work, we show that multi-photon states uncover a rich landscape of exchange symmetries. With three photons already, multiple pairwise combinations are possible, where each pair of photons can exhibit either bosonic, fermionic, or anyonic exchange symmetry. This gives rise to mixed symmetry systems that are not possible to achieve with two photon alone. We experimentally investigate how these symmetry configurations manifest themselves in the observed interference of three photons. We show that multi-photon interference can be effectively turned on and off by tuning the symmetry of the constituent pairs. The possibility of accessing and tuning new quantum statistics in a scalable photonic platform not only deepens our understanding of quantum systems, but is also highly relevant for quantum technologies that rely on quantum interference.

</details>


### [108] [Coherence-limited digital control of a superconducting qubit using a Josephson pulse generator at 3 K](https://arxiv.org/abs/2512.07962)
*M. A. Castellanos-Beltran,A. J. Sirois,L. Howe,D. I. Olaya,J. Biesecker,S. P. Benz,P. F. Hopkins*

Main category: quant-ph

TL;DR: 本文比较了在3K温度下运行的约瑟夫森脉冲发生器与传统半导体控制电子器件对超导量子比特的控制性能，在2D transmon器件中实现了与室温控制相当的相干时间和门保真度，门误差降低至0.46%，比先前工作提升了一个数量级。


<details>
  <summary>Details</summary>
Motivation: 传统室温半导体控制电子器件在量子处理器扩展性方面存在限制，而单磁通量子电路虽然能提供低温控制方案，但之前与量子比特共置时会导致退相干问题。本研究旨在开发适用于可扩展2D transmon器件的低温控制方案，避免准粒子中毒问题。

Method: 将约瑟夫森脉冲发生器电路置于稀释制冷机的3K阶段，优化量子比特设计和控制线路以适应2D transmon器件。通过直接比较JPG电路与传统半导体控制电子器件对量子比特寿命T1、相干时间T2*和门保真度的影响，使用随机基准测试和交错随机基准测试评估性能。

Result: JPG控制与传统控制相比，T1和T2*在日波动范围内一致，随机基准测试结果在10%内一致。交错随机基准测试显示平均单门误差为0.46%，与基于量子比特相干性和高态泄漏的预期相符，比先前工作提升了一个数量级。

Conclusion: 在3K温度下运行的约瑟夫森微波源是实现可扩展量子比特控制的有前景组件，能够在避免准粒子中毒问题的同时，提供与传统室温控制相当甚至更好的性能，为大规模量子处理器的发展提供了重要技术支持。

Abstract: Compared to traditional semiconductor control electronics (TSCE) located at room temperature, cryogenic single flux quantum (SFQ) electronics can provide qubit measurement and control alternatives that address critical issues related to scalability of cryogenic quantum processors. Single-qubit control and readout have been demonstrated recently using SFQ circuits coupled to superconducting qubits. Experiments where the SFQ electronics are co-located with the qubit have suffered from excess decoherence and loss due to quasiparticle poisoning of the qubit. A previous experiment by our group showed that moving the control electronics to the 3 K stage of the dilution refrigerator avoided this source of decoherence in a high-coherence 3D transmon geometry. In this paper, we also generate the pulses at the 3 K stage but have optimized the qubit design and control lines for scalable 2D transmon devices. We directly compare the qubit lifetime $T_1$, coherence time $T_2^*$ and gate fidelity when the qubit is controlled by the Josephson pulse generator (JPG) circuit versus the TSCE setup. We find agreement to within the daily fluctuations for $T_1$ and $T_2^*$, and agreement to within 10% for randomized benchmarking. We also performed interleaved randomized benchmarking on individual JPG gates demonstrating an average error per gate of $0.46$% showing good agreement with what is expected based on the qubit coherence and higher-state leakage. These results are an order of magnitude improvement in gate fidelity over our previous work and demonstrate that a Josephson microwave source operated at 3 K is a promising component for scalable qubit control.

</details>


### [109] [Measurement-and Feedback-Driven Non-Equilibrium Phase Transitions on a Quantum Processor](https://arxiv.org/abs/2512.07966)
*Zhiyi Wu,Xuandong Sun,Songlei Wang,Jiawei Zhang,Xiaohan Yang,Ji Chu,Jingjing Niu,Youpeng Zhong,Xiao Chen,Zhi-Cheng Yang,Dapeng Yu*

Main category: quant-ph

TL;DR: 该研究开发了具有高保真度全局中电路测量和快速条件反馈的超导量子处理器，实验观测了量子信道中的吸收态相变和量子轨迹层面的测量诱导纠缠相变，发现两个相变发生在不同的调谐参数值。


<details>
  <summary>Details</summary>
Motivation: 中电路测量和基于测量结果的条件反馈操作对于量子硬件上的量子纠错至关重要，但现有设备在保真度和实时反馈延迟方面存在限制，难以实验观测量子多体动力学中的非平衡相变。

Method: 开发了新型超导量子处理器，实现了平均98.7%量子非破坏性保真度的全局中电路测量，以及200纳秒实时决策延迟的快速条件反馈操作。

Result: 实验观测到量子信道中的吸收态相变和量子轨迹层面的测量诱导纠缠相变共存，吸收态相变的临界指数与定向渗流普适类高度一致，且两个相变发生在不同的调谐参数值。

Conclusion: 自适应量子电路为探索非平衡量子多体动力学提供了强大平台，实验验证了量子信道和量子轨迹层面不同相变的存在及其临界行为。

Abstract: Mid-circuit measurements and feedback operations conditioned on the measurement outcomes are essential for implementing quantum error-correction on quantum hardware. When integrated in quantum many-body dynamics, they can give rise to novel non-equilibrium phase transitions both at the level of each individual quantum trajectory and the averaged quantum channel. Experimentally resolving both transitions on realistic devices has been challenging due to limitations on the fidelity and the significant latency for performing mid-circuit measurements and feedback operations in real time. Here, we develop a superconducting quantum processor that enables global mid-circuit measurement with an average quantum non-demolition (QND) fidelity of 98.7% and fast conditional feedback with a 200 ns real-time decision latency. Using this platform, we demonstrate the coexistence of an absorbing-state transition in the quantum channel and a measurement-induced entanglement transition at the level of individual quantum trajectories. For the absorbing-state transition, we experimentally extract a set of critical exponents at the transition point, which is in excellent agreement with the directed percolation universality class. Crucially, the two transitions occur at distinct values of the tuning parameter. Our results demonstrate that adaptive quantum circuits provide a powerful platform for exploring non-equilibrium quantum many-body dynamics.

</details>


### [110] [Information-Theoretic Analysis of Weak Measurements and Their Reversal](https://arxiv.org/abs/2512.08015)
*Luis D. Zambrano Palma,Yusef Maleki,M. Suhail Zubairy*

Main category: quant-ph

TL;DR: 研究量子系统在零结果弱测量下的信息提取权衡关系，通过信息论量分析弱测量动力学过程


<details>
  <summary>Details</summary>
Motivation: 研究量子系统在零结果弱测量过程中的信息提取特性，其中未检测到光子的情况会持续更新系统状态，探索信息提取的动力学本质和可逆性

Method: 对量子比特和量子三能级系统进行详细分析，并研究多能级量子系统的一般框架。通过香农熵、互信息、保真度和相对熵等时间依赖的信息论量来表征弱测量动力学

Result: 开发了零结果弱测量的动力学表征方法，量化随时间提取的信息量，揭示了获得的信息量以及信息积累的速率。结果提供了弱测量过程的信息论分析

Conclusion: 该研究突出了弱测量过程中信息提取的动力学本质和可逆性，为理解量子系统在弱测量下的信息提取权衡关系提供了信息论框架

Abstract: We study trade-off relations in information extraction from quantum systems subject to null-result weak measurements, where the absence of a detected photon continuously updates the system state. We present a detailed analysis of qubit and qutrit systems and investigate a general framework for a multilevel quantum system. We develop a dynamical characterization of null-result weak measurements that quantifies the information extracted over time, revealing the amount of the obtained information and also the rate of the information accumulation. The characterizations are obtained by examining the time-dependent evolution of the information theoretic quantities. More specifically, we consider Shannon entropy, mutual information, fidelity, and relative entropy to characterize the weak measurement dynamics. Our results provide an information theoretic analysis of the weak measurement process and highlight the dynamical nature of information extraction and reversibility in the weak measurement processes.

</details>


### [111] [Classical and quantum dynamics of a particle confined in a paraboloidal cavity](https://arxiv.org/abs/2512.08021)
*Ángel E. Reyna-Cruz,Julio C. Gutiérrez-Vega*

Main category: quant-ph

TL;DR: 该论文对三维抛物面腔中受限粒子进行了经典和量子分析，系统可积且存在三个独立运动常数，推导了闭轨条件并分类轨迹，量子方面通过抛物坐标分离薛定谔方程得到惠特克函数描述的模态，建立了经典轨迹与量子本征态的直接对应关系。


<details>
  <summary>Details</summary>
Motivation: 研究三维抛物面腔中受限粒子的动力学行为，探索经典可积系统与量子对应关系，分析几何约束对粒子运动的影响，为理解受限量子系统的经典-量子对应提供理论框架。

Method: 采用经典力学和量子力学相结合的方法：经典方面基于哈密顿-雅可比方程的可分离性推导运动常数和闭轨条件；量子方面在抛物坐标系中分离薛定谔方程，使用惠特克函数描述本征态；通过几何分析和数值计算研究能谱和简并性。

Result: 推导出闭轨的解析条件并用指标(s,t,ℓ)分类轨迹；确定了能量谱并发现除方位角对称外的腔体形变引起的简并；建立了经典轨迹与量子概率密度分布的对应关系，显示概率密度集中在经典允许区域并有控制地渗透到禁戒区。

Conclusion: 三维抛物面腔系统是完全可积的，经典闭轨条件与量子本征态存在直接对应关系，几何约束不仅影响经典运动轨迹也决定量子态的简并特性，为研究受限量子系统的经典-量子对应提供了理想模型。

Abstract: We present a classical and quantum analysis of a particle confined in a three-dimensional paraboloidal cavity formed by two confocal paraboloids. Classically, the system is integrable and presents three independent constants of motion, namely, the energy, the $z$-component of the angular momentum, and a third dynamical constant associated with the paraboloidal geometry, which can be derived from the separability of the Hamilton--Jacobi equation. We derive closed-form analytical expressions for the actions, which allow us to determine the two conditions to get periodic closed trajectories. We classify these trajectories through the indices $(s,t,\ell)$. The caustic paraboloids that bound the motion provide a complete geometric characterization of admissible trajectories. Quantum mechanically, separability of the Schrödinger equation in parabolic coordinates yields eigenmodes described by Whittaker functions. We determine the energy spectrum and identify degeneracies arising not only from azimuthal symmetry but also from specific cavity deformations. A direct correspondence between classical trajectories and quantum eigenstates reveals that probability densities concentrate in the classically allowed region with controlled penetration into forbidden zones.

</details>


### [112] [F2: Offline Reinforcement Learning for Hamiltonian Simulation via Free-Fermionic Subroutine Compilation](https://arxiv.org/abs/2512.08023)
*Ethan Decker,Christopher Watson,Junyu Zhou,Yuhao Liu,Chenxu Liu,Ang Li,Gushu Li,Samuel Stein*

Main category: quant-ph

TL;DR: F2是一个利用离线强化学习和自由费米子结构来编译哈密顿量模拟量子电路的框架，相比现有方法平均减少47%门数和38%电路深度，同时保持10^-7的平均误差。


<details>
  <summary>Details</summary>
Motivation: 哈密顿量模拟的浅层精确量子电路编译面临硬件限制和组合复杂性挑战。现有优化方法依赖手工设计的经典启发式算法，无法学习输入依赖的结构，因此错失了大幅减少电路规模的机会。

Method: F2是一个离线强化学习框架，利用自由费米子结构编译基于Trotter的哈密顿量模拟电路。包含：(1)基于经典可模拟自由费米子子程序的强化学习环境；(2)稳定长时程价值学习的架构和目标级归纳偏置；(3)可逆合成轨迹生成机制，持续产生丰富且保证成功的离线数据。

Result: 在晶格模型、蛋白质片段和晶体材料（12-222量子比特）的基准测试中，F2相比强基线（Qiskit, Cirq/OpenFermion）平均减少47%门数和38%电路深度，同时保持10^-7的平均误差。

Conclusion: 将深度强化学习量子动力学代数结构对齐，能够在电路合成中实现显著改进，为可扩展的基于学习的量子编译提供了有前景的方向。

Abstract: Compiling shallow and accurate quantum circuits for Hamiltonian simulation remains challenging due to hardware constraints and the combinatorial complexity of minimizing gate count and circuit depth. Existing optimization method pipelines rely on hand-engineered classical heuristics, which cannot learn input-dependent structure and therefore miss substantial opportunities for circuit reduction.
  We introduce \textbf{F2}, an offline reinforcement learning framework that exploits free-fermionic structure to efficiently compile Trotter-based Hamiltonian simulation circuits. F2 provides (i) a reinforcement-learning environment over classically simulatable free-fermionic subroutines, (ii) architectural and objective-level inductive biases that stabilize long-horizon value learning, and (iii) a reversible synthetic-trajectory generation mechanism that consistently yields abundant, guaranteed-successful offline data.
  Across benchmarks spanning lattice models, protein fragments, and crystalline materials (12-222 qubits), F2 reduces gate count by 47\% and depth by 38\% on average relative to strong baselines (Qiskit, Cirq/OpenFermion) while maintaining average errors of $10^{-7}$. These results show that aligning deep reinforcement learning with the algebraic structure of quantum dynamics enables substantial improvements in circuit synthesis, suggesting a promising direction for scalable, learning-based quantum compilation

</details>


### [113] [Observation of a Topological Berry Phase with a Single Phonon in an Ion Microtrap Array](https://arxiv.org/abs/2512.08037)
*Justin F. Niedermeyer,Nathan K. Lysne,Katherine C. McCormick,Jonas Keller,Craig W. Hogle,Matthew G. Blain,Roman Schmied,Robert Jördens,Susanna L. Todaro,David J. Wineland,Andrew C. Wilson,Daniel H. Slichter,Dietrich Leibfried*

Main category: quant-ph

TL;DR: 研究人员展示了在二维离子阵列中通过调节局部囚禁势实现离子运动模式的精确控制，实现了单个声子在2-3个离子间的相干共享，并观测到了拓扑Berry相位。


<details>
  <summary>Details</summary>
Motivation: 传统离子晶体中离子的运动模式是强耦合的，所有本征模式都涉及多个离子，这虽然有利于研究多体物理，但限制了系统作为量子平台的灵活性和可调性。需要开发能够精确控制单个离子运动模式的系统。

Method: 构建了单个囚禁位点的离子阵列，通过调节每个离子的局部囚禁势来控制运动模式的耦合和解耦。在边长为30μm的等边三角形顶点上囚禁离子，实现单个声子在2-3个离子间的相干共享。

Result: 成功实现了单个声子在多个离子间的相干共享，通过调节运动模式在构型空间中沿闭合路径演化，观测到了拓扑Berry相位。通过单声子干涉测量该相位，并研究了非绝热调节下的相位破坏。

Conclusion: 精确的单个离子运动量子控制在二维阵列中为量子多体效应研究提供了独特途径，展示了拓扑量子现象的可控实现。

Abstract: Controlled quantum mechanical motion of trapped atomic ions can be used to simulate and explore collective quantum phenomena and to process quantum information. Groups of cold atomic ions in an externally applied trapping potential self-organize into "Coulomb crystals" due to their mutual electrostatic repulsion. The motion of the ions in these crystals is strongly coupled, and the eigenmodes of motion all involve multiple ions. While this enables studies of many-body physics, it limits the flexibility and tunability of the system as a quantum platform. Here, we demonstrate an array of trapped ions in individual trapping sites whose motional modes can be controllably coupled and decoupled by tuning the local applied confining potential for each ion. We show that a single motional quantum, or phonon, can be coherently shared among two or three ions confined at the vertices of an equilateral triangle 30 $μ$m on a side. We can adiabatically tune the ion participation in the motional modes around a closed contour in configuration space, observing that the single-phonon wavefunction acquires a topological Berry phase if the contour encircles a conical intersection of motional eigenvalue surfaces. We observe this phase by single-phonon interference and study its breakdown as the motional mode tuning becomes non-adiabiatic. Our results show that precise, individual quantum control of ion motion in a two-dimensional array can provide unique access to quantum multi-body effects.

</details>


### [114] [Geometry-driven transitions in sparse long-range spin models with cold atoms](https://arxiv.org/abs/2512.08709)
*Alex Gunning,Aydin Deger,Sridevi Kuriyattil,Andrew J. Daley*

Main category: quant-ph

TL;DR: 该研究探索了几何结构对稀疏长程自旋模型临界行为的影响，通过可调相互作用诱导耦合图度量、拓扑和维度的变化，并将此框架与光镊阵列中的里德伯激发实现联系起来。


<details>
  <summary>Details</summary>
Motivation: 研究几何结构在稀疏长程自旋模型临界行为中的作用，探索如何通过改变相互作用来调控耦合图的几何特性，从而理解几何如何驱动临界现象。

Method: 构建一个相互作用可连续调节的模型，该模型能够诱导耦合图在度量、拓扑和维度方面的变化。通过分析图结构变化与相边界的关系，研究几何如何驱动临界行为，并将该框架与光镊阵列中的里德伯激发实现联系起来。

Result: 几何结构作为临界行为的驱动因素，图的结构变化与相边界重合并决定相边界。在某些情况下，有效几何可以通过光镊中原子的布局来实现，从而保留相变的普适特征，简化近期实验的实现。

Conclusion: 几何结构在稀疏长程自旋模型的临界行为中起着决定性作用，通过调控相互作用可以改变耦合图的几何特性，从而控制相变行为。该框架与光镊阵列实验自然连接，为在近期实验中实现保留普适特征的相变提供了简化途径。

Abstract: We explore the influence of geometry in the critical behavior of sparse long-range spin models. We examine a model with interactions that can be continuously tuned to induce distinct changes in the metric, topology, and dimensionality of the coupling graph. This underlying geometry acts as the driver of criticality, with structural changes in the graph coinciding with and dictating the phase boundaries. We further discuss how this framework connects naturally to realizations in tweezer arrays with Rydberg excitations. In certain cases, the effective geometry can be incorporated in the layout of atoms in tweezers to realize phase transitions that preserve universal features, simplifying their implementation in near-term experiments.

</details>


### [115] [Coherent and compact van der Waals transmon qubits](https://arxiv.org/abs/2512.08059)
*Jesse Balgley,Jinho Park,Xuanjing Chu,Jiru Liu,Madisen Holbrook,Kenji Watanabe,Takashi Taniguchi,Archana Kamal,Leonardo Ranzani,Martin V. Gustafsson,James Hone,Kin Chung Fong*

Main category: quant-ph

TL;DR: 研究人员首次实现了完全由范德华材料制成的量子相干超导量子比特，展示了微秒级寿命和超紧凑结构，无需外部分流电容器。


<details>
  <summary>Details</summary>
Motivation: 当前超导量子比特依赖有限的薄膜材料，限制了性能提升和功能扩展。范德华材料提供了高度模块化的晶体平台，有望实现栅极可调性、更高温度操作和紧凑结构，但尚不清楚全范德华超导量子比特是否能支持量子相干性。

Method: 研究人员开发了完全由范德华约瑟夫森结制成的合并元件transmon量子比特，使用范德华材料构建整个量子比特结构，包括电容器和约瑟夫森结，无需外部组件。

Result: 第一代全晶体量子比特实现了微秒级寿命，在超紧凑结构中运行。能量弛豫测量和微波表征表明，在高达数百毫开尔文的温度下，介电损耗是主要的弛豫通道。

Conclusion: 范德华材料被确立为紧凑型超导量子器件的可行平台，为扩展量子比特材料库、提高性能和引入新功能开辟了道路。

Abstract: State-of-the-art superconducting qubits rely on a limited set of thin-film materials. Expanding their materials palette can improve performance, extend operating regimes, and introduce new functionalities, but conventional thin-film fabrication hinders systematic exploration of new material combinations. Van der Waals (vdW) materials offer a highly modular crystalline platform that facilitates such exploration while enabling gate-tunability, higher-temperature operation, and compact qubit geometries. Yet it remains unknown whether a fully vdW superconducting qubit can support quantum coherence and what mechanisms dominate loss at both low and elevated temperatures in such a device. Here we demonstrate quantum-coherent merged-element transmons made entirely from vdW Josephson junctions. These first-generation, fully crystalline qubits achieve microsecond lifetimes in an ultra-compact footprint without external shunt capacitors. Energy relaxation measurements, together with microwave characterization of vdW capacitors, point to dielectric loss as the dominant relaxation channel up to hundreds of millikelvin. These results establish vdW materials as a viable platform for compact superconducting quantum devices.

</details>


### [116] [Non-abelian quantum double models from iterated gauging](https://arxiv.org/abs/2512.08749)
*David Blanik,José Garre-Rubio*

Main category: quant-ph

TL;DR: 从边界对称性通过重复应用规范过程重构所有有限群的(2+1)D量子双重模型，将现有阿贝尔群构造扩展到非阿贝尔群


<details>
  <summary>Details</summary>
Motivation: 扩展现有阿贝尔群量子双重模型的构造方法到所有有限群，通过边界对称性重构模型，并探索高维推广的可能性

Method: 使用基于矩阵乘积算子(MPO)的范畴规范框架，推导Rep G对称性的适当规范过程，明确描述对偶涌现的G对称性，并迭代应用规范过程

Result: 成功从边界对称性重构了所有有限群的(2+1)D量子双重模型，建立了量子双重模型的可能能隙边界与一维输入态量子相的关系，并提出了二维晶格上1-form Rep G对称性的规范过程

Conclusion: 通过迭代规范过程成功构建了(2+1)D量子双重模型，并将该方法扩展到(3+1)D量子双重模型的构造，为从对称性角度理解拓扑序提供了系统框架

Abstract: We reconstruct all (2+1)D quantum double models of finite groups from their boundary symmetries through the repeated application of a gauging procedure, extending the existing construction for abelian groups. We employ the recently proposed categorical gauging framework, based on matrix product operators (MPOs), to derive the appropriate gauging procedure for the $\mathsf{Rep}\, G$ symmetries appearing in our construction and give an explicit description of the dual emergent $G$ symmetry, which is our main technical contribution. Furthermore, we relate the possible gapped boundaries of the quantum double models to the quantum phases of the one-dimensional input state to the iterated gauging procedure. Finally, we propose a gauging procedure for 1-form $\mathsf{Rep}\, G$ symmetries on a two-dimensional lattice and use it to extend our results to the construction of (3+1)D quantum doubles models through the iterative gauging of (2+1)-dimensional symmetries.

</details>


### [117] [On Dirac-type correlations](https://arxiv.org/abs/2512.08068)
*James Fullwood,Boyu Yang*

Main category: quant-ph

TL;DR: 该论文提出了"局部密度算子"理论，作为非类空间隔量子系统的联合状态，推广了Gleason定理到时空量子关联


<details>
  <summary>Details</summary>
Motivation: 量子关联常常无法用经典物理的基本概念（如因果性、局域性和实在性）来解释。虽然类空间隔系统的量子关联数学理论自1930年代以来已很成熟，但非类空间隔系统的关联数学理论发展较少。需要建立统一的数学框架来描述时空量子关联。

Method: 提出了"局部密度算子"的概念，其迹为1且边缘是真正的密度算子。建立了局部密度算子与"狄拉克测度"之间的一一对应关系，狄拉克测度是定义在两个量子系统可分离投影空间上的复值测度。

Result: 证明了局部密度算子与狄拉克测度的一一对应关系。当其中一个系统是平凡量子系统（一维希尔伯特空间）时，该结果恢复了Gleason定理，表明量子理论中的玻恩规则是唯一能以非上下文方式为量子系统测量结果分配概率的方法。

Conclusion: 该工作将Gleason定理推广到可能非类空间隔系统的测量，从而将量子关联的数学理论从空间扩展到时空领域，为理解时空量子关联提供了统一的数学框架。

Abstract: Quantum correlations often defy an explanation in terms of fundamental notions of classical physics, such as causality, locality, and realism. While the mathematical theory underpinning quantum correlations between spacelike separated systems has been well-established since the 1930s, the mathematical theory for correlations between non-spacelike separated systems is much less developed. In this work, we develop the theory of what we refer to as "local-density operators", which we view as joint states for possibly non-spacelike separated quantum systems. Local-density operators are unit trace operators whose marginals are genuine density operators, which we show not only subsumes the notion of density operator, but also several extensions of the notion of density operator into the spatiotemporal domain, such as pseudo-density operators and quantum states over time. More importantly, we prove a result which establishes a one-to-one correspondence between local-density operators and what we refer to as "Dirac measures", which are complex-valued measures on the space of separable projectors associated with two quantum systems. In the case that one of the systems is the trivial quantum system with a one-dimensional Hilbert space, our result recovers the fundamental result known as Gleason's Theorem, which implies that the Born rule from quantum theory is the only way in which one may assign probabilities to the outcomes of measurements performed on quantum systems in a non-contextual manner. As such, our results establish a direct generalization of Gleason's Theorem to measurements performed on possibly non-spacelike separated systems, thus extending the mathematical theory of quantum correlations across space to quantum correlations across space and time.

</details>


### [118] [Deterministic Equations for Feedback Control of Open Quantum Systems II: Properties of the memory function](https://arxiv.org/abs/2512.08085)
*Alberto J. B. Rosal,Patrick P. Potts,Gabriel T. Landi*

Main category: quant-ph

TL;DR: 该论文研究量子反馈控制中的记忆函数，将记忆视为与量子系统耦合的经典系统，建立了混合二分态框架来分析系统与记忆之间的相关性，并开发了表征记忆统计特性的通用框架。


<details>
  <summary>Details</summary>
Motivation: 量子反馈控制使用过去的检测结果来动态修改量子系统，这些结果可以存储在记忆函数中。目前缺乏对受任意反馈动力学影响的通用记忆函数主要特性的系统研究，需要建立理论框架来量化系统与记忆之间的相关性。

Method: 将记忆视为与监测量子系统耦合的经典系统，用混合二分态描述它们的联合演化。引入信息论度量来量化系统与记忆之间的相关性，开发通用框架来表征记忆的统计特性（矩、累积量和相关函数）。

Result: 建立了量子反馈控制中记忆函数的理论框架，证明了记忆可以作为经典系统处理。该框架能够量化系统-记忆相关性，并适用于分析一般反馈控制协议和无反馈的监测系统。

Conclusion: 该研究为量子反馈控制中的记忆函数提供了系统的理论框架，能够分析系统与记忆之间的相关性，并表征记忆的统计特性。应用示例展示了该框架在稳定激发态布居和拉比振荡方面的实用性。

Abstract: Feedback uses past detection outcomes to dynamically modify a quantum system and is central to quantum control. These outcomes can be stored in a memory, defined as a stochastic function of past measurements. In this work, we investigate the main properties of a general memory function subject to arbitrary feedback dynamics. We show that the memory can be treated as a classical system coupled to the monitored quantum system, and that their joint evolution is described by a hybrid bipartite state. This framework allows us to introduce information-theoretic measures that quantify the correlations between the system and the memory. Furthermore, we develop a general framework to characterize the statistics of the memory -- such as moments, cumulants, and correlation functions -- which can be applied both to general feedback-control protocols and to monitored systems without feedback. As an application, we analyze feedback schemes based on detection events in a two-level system coupled to a thermal bath, focusing on protocols that stabilize either the excited-state population or Rabi oscillations against thermal dissipation.

</details>


### [119] [On the Emergence of Time and Space in Closed Quantum Systems](https://arxiv.org/abs/2512.08120)
*Tommaso Favalli*

Main category: quant-ph

TL;DR: 该论文探讨时间、空间和量子纠缠之间的深层联系，提出纠缠可能是封闭量子系统中时间和空间涌现的基础，扩展了Page-Wootters理论，并建立了与统计力学基础的联系。


<details>
  <summary>Details</summary>
Motivation: 探索时间、空间和量子纠缠这三个物理基本现象之间的本质联系，研究纠缠如何作为封闭量子系统中时间和空间涌现的基础，解决这些基本物理概念的起源问题。

Method: 重新审视并扩展Page-Wootters理论，该理论通过子系统间的纠缠来描述时间在全局静态量子宇宙中的涌现。建立与统计力学基础研究的联系，提出热化过程的新理解。将框架推广到空间自由度，构建3+1维量子时空从"无时间"和"无位置"宇宙中子系统间纠缠涌现的模型。通过Page-Wootters理论处理引力场中量子时钟的演化。

Result: 提出了一个完整的理论框架，表明时间和空间可以从量子纠缠中涌现。获得了与史瓦西解一致的时间膨胀效应，为量子引力理论提供了新的视角。建立了量子纠缠、热力学和时空几何之间的深刻联系。

Conclusion: 时间、空间和量子纠缠是紧密相连的基本物理现象，纠缠可能是时间和空间在封闭量子系统中涌现的基础。Page-Wootters理论的扩展为理解量子引力、统计力学基础以及时空本质提供了统一的理论框架。

Abstract: Time, space and entanglement are the main characters in this work. Their nature is still a great mystery in physics and we study here the possibility that these three phenomena are closely connected, showing how entanglement can be at the basis of the emergence of time and space within closed quantum systems. We revisit and extend the Page and Wootters theory that was originally introduced in order to describe the emergence of time through entanglement between subsystems in a globally static, quantum Universe. In the book, after providing a complete review of the salient aspects of the theory, we establish a connection with recent research on the foundations of statistical mechanics and we propose a new understanding of the thermalization process. Furthermore, we generalize the framework in order describe the spatial degree of freedom and we provide a model of 3+1 dimensional, quantum spacetime emerging from entanglement among different subsystems in a globally "timeless" and "positionless" Universe. Finally, via the Page and Wootters theory, the evolution of quantum clocks within a gravitational field is treated and a time dilation effect is obtained in agreement with the Schwarzschild solution.

</details>


### [120] [The strength of weak coupling](https://arxiv.org/abs/2512.08141)
*Alastair Kay,Christino Tamon*

Main category: quant-ph

TL;DR: 该论文通过数学方法严格证明了量子传输中的一个悖论：将弱耦合边缘附加到大型基图上可以创建高保真度的量子态传输，并展示了这一思想在规避自旋链中的安德森局域化和加速量子搜索方面的应用。


<details>
  <summary>Details</summary>
Motivation: 量子传输中存在一个悖论性观点：将弱耦合边缘附加到大型基图上反而能实现高保真度的量子态传输。这一观点在量子计算和量子信息处理中具有重要意义，但缺乏严格的数学证明。论文旨在为这一传统观点提供严格的数学基础，并探索其在规避量子系统中的局域化现象和加速量子搜索方面的实际应用。

Method: 采用基于微扰理论的Feshbach-Schur方法进行数学证明。该方法提供了一种处理弱耦合系统的有效框架，通过将系统分解为不同子空间来分析弱耦合边缘对量子态传输的影响。证明过程采用初等数学方法，使得理论分析更加清晰和严谨。

Result: 1. 严格证明了弱耦合边缘能增强大型基图中的量子态传输保真度这一悖论性观点；2. 展示了该方法能有效规避自旋链中的安德森局域化现象；3. 发现了在量子搜索中加速命中时间的应用潜力。

Conclusion: 该研究为量子传输中的弱耦合边缘增强效应提供了严格的数学基础，证明了这一看似悖论的现象确实成立。更重要的是，这一思想具有实际应用价值，能够帮助规避量子系统中的局域化问题，并为量子搜索算法提供加速机制，对量子计算和量子信息处理领域具有重要意义。

Abstract: A paradoxical idea in quantum transport is that attaching weakly-coupled edges to a large base graph creates high-fidelity quantum state transfer. We provide a mathematical treatment that rigorously prove this folklore idea. Our proofs are elementary and build upon the Feshbach-Schur method from perturbation theory. We also show the idea is effective in circumventing Anderson localization in spin chains and finding speedups in hitting times useful for quantum search.

</details>


### [121] [Detecting quantum many-body states with imperfect measuring devices](https://arxiv.org/abs/2512.08150)
*K. Uriostegui,C. Pineda,C. Chryssomalakos,V. Rascón Barajas,I. Vázquez Mota*

Main category: quant-ph

TL;DR: 研究多粒子量子系统中粒子寻址不完整和不完美导致的粗粒化映射，分析粗粒化状态的概率分布及其随系统规模的变化规律。


<details>
  <summary>Details</summary>
Motivation: 研究量子系统中由于实验限制导致的粒子寻址不完整和不完美问题，这种不完美的寻址会产生粗粒化映射，需要理解这种映射如何影响量子状态的描述和测量。

Method: 1. 对于两量子比特系统，使用几何方法分析粗粒化到单量子比特的概率密度；2. 对于更大系统，采用随机矩阵方法；3. 计算逆状态以表征粗粒化下的有效动力学；4. 通过蒙特卡洛模拟验证理论预测。

Result: 1. 随着量子比特数增加，概率密度急剧集中在最大混合态附近，使得近乎纯的粗粒化状态越来越不可能；2. 对于两量子比特，最大混合态的平均原像包含有限的单重态成分；3. 理论预测与蒙特卡洛生成的粗粒化统计推断结果一致。

Conclusion: 量子系统的粗粒化映射导致状态信息损失，随着系统规模增大，粗粒化状态趋向于最大混合态，这为理解实验限制下的量子状态描述提供了理论框架。

Abstract: We study a coarse-graining map arising from incomplete and imperfect addressing of particles in a multipartite quantum system. In its simplest form, corresponding to a two-qubit state, the resulting channel produces a convex mixture of the two partial traces. We derive the probability density of obtaining a given coarse-grained state, using geometric arguments for two qubits coarse-grained to one, and random-matrix methods for larger systems. As the number of qubits increases, the probability density sharply concentrates around the maximally mixed state, making nearly pure coarse-grained states increasingly unlikely. For two qubits, we also compute the inverse state needed to characterize the effective dynamics under coarse-graining and find that the average preimage of the maximally mixed state contains a finite singlet component. Finally, we validate the analytical predictions by inferring the underlying probabilities from Monte-Carlo-generated coarse-grained statistics.

</details>


### [122] [The utility of noiseless linear amplification and attenuation in single-rail discrete-variable quantum communications](https://arxiv.org/abs/2512.08255)
*Ozlem Erkilic,Aritra Das,Angela A. Baiju,Nicholas Zaunders,Biveen Shajilal,Timothy C. Ralph*

Main category: quant-ph

TL;DR: 该研究探讨如何通过优化测量操作来缓解量子通信中信道损耗对量子隐形传态和超密编码性能的影响，发现优化的POVM测量与无噪声衰减/放大电路效果相当，能显著提升传输保真度和量子优势。


<details>
  <summary>Details</summary>
Motivation: 量子通信中的信道损耗会降低共享纠缠态的质量，从而影响量子隐形传态的保真度和超密编码的信息优势。需要找到方法来缓解这些损耗效应，提高量子通信协议的实际性能。

Method: 将问题形式化为对一般正算子值测量（POVM）的优化问题，并与物理上可实现的两种电路进行比较：无噪声衰减（NA）和无噪声线性放大（NLA）电路。

Result: 对于量子隐形传态，NLA/NA和优化的POVM能将平均保真度提高达78%，同时保持可行的成功概率。对于超密编码，在某些情况下能将量子优势相对于经典信道容量的提升超过100%，并扩展了可容忍的损耗范围。

Conclusion: 最优的POVM测量实际上简化为NA或NLA操作，表明简单且实验上可访问的操作已经能够捕捉到主要的性能提升，为实际量子通信系统提供了实用的优化方案。

Abstract: Quantum communication offers many applications, with teleportation and superdense coding being two of the most fundamental. In these protocols, pre-shared entanglement enables either the faithful transfer of quantum states or the transmission of more information than is possible classically. However, channel losses degrade the shared states, reducing teleportation fidelity and the information advantage in superdense coding. Here, we investigate how to mitigate these effects by optimising the measurements applied by the communicating parties. We formulate the problem as an optimisation over general positive operator-valued measurements (POVMs) and compare the results with physically realisable noiseless attenuation (NA) and noiseless linear amplification (NLA) circuits. For teleportation, NLA/NA and optimised POVMs improve the average fidelity by up to 78% while maintaining feasible success probabilities. For superdense coding, they enhance the quantum advantage over the classical channel capacity by more than 100% in some regimes and shift the break-even point, thereby extending the tolerable range of losses. Notably, the optimal POVMs effectively reduce to NA or NLA, showing that simple, experimentally accessible operations already capture the essential performance gains.

</details>


### [123] [Programmable Open Quantum Systems](https://arxiv.org/abs/2512.08279)
*Mingrui Jing,Mengbo Guo,Lin Zhu,Hongshun Yao,Xin Wang*

Main category: quant-ph

TL;DR: 该论文开发了一个框架来表征和量化Lindbladian半群的可编程性，结合物理可实现的检索映射和时变程序状态，识别了由对称性和随机结构实现的量子可编程类，并引入了操作编程成本的概念。


<details>
  <summary>Details</summary>
Motivation: 量子计算中，可编程性是通过固定处理器和程序状态实现量子变换族的重要范式。虽然开放系统已从单纯视为误差源转变为计算资源，但其可编程性尚未得到充分探索。本文旨在开发一个框架来研究Lindbladian半群的可编程性。

Method: 开发了一个结合物理可实现检索映射和时变程序状态的框架，通过对称性和随机结构识别量子可编程类（如协变半群和完全耗散Pauli Lindbladians），分析物理可编程性的必要条件，为非物理可编程情况构建显式协议，并引入基于样本数量的操作编程成本概念。

Result: 识别了由对称性和随机结构实现的量子可编程类，提供了物理可编程性的必要条件（排除了相干生成元和典型的振幅阻尼耗散器），为非物理可编程情况构建了有限资源协议，建立了编程成本的核心结构特性（如连续性和忠实性）。

Conclusion: 该研究为Lindbladians提供了编程成本的概念，桥接了可编程通道理论和开放系统动力学，产生了对称性驱动的压缩方案，并为噪声量子技术中的半群模拟和控制提供了可操作的资源估计。

Abstract: Programmability is a unifying paradigm for enacting families of quantum transformations via fixed processors and program states, with a fundamental role and broad impact in quantum computation and control. While there has been a shift from viewing open systems solely as a source of error to treating them as a computational resource, their programmability remains largely unexplored. In this work, we develop a framework that characterizes and quantifies the programmability of Lindbladian semigroups by combining physically implementable retrieval maps with time varying program states. Within this framework, we identify quantum programmable classes enabled by symmetry and stochastic structure, including covariant semigroups and fully dissipative Pauli Lindbladians with finite program dimension. We further provide a necessary condition for physical programmability that rules out coherent generators and typical dissipators generating amplitude damping. For such nonphysically programmable cases, we construct explicit protocols with finite resources. Finally, we introduce an operational programming cost, defined via the number of samples required to program the Lindbladian, and establish its core structural properties, such as continuity and faithfulness. These results provide a notion of programming cost for Lindbladians, bridge programmable channel theory and open system dynamics, and yield symmetry driven compression schemes and actionable resource estimates for semigroup simulation and control in noisy quantum technologies.

</details>


### [124] [Discovering novel quantum dynamics with NISQ simulators](https://arxiv.org/abs/2512.08293)
*Pedram Roushan,Leigh S. Martin*

Main category: quant-ph

TL;DR: 量子模拟器已在量子多体动力学领域取得重要进展，挑战并完善了传统认知


<details>
  <summary>Details</summary>
Motivation: 理解复杂量子多体系统的行为是当前核心挑战，费曼提出的量子模拟器概念为实现这一目标提供了途径

Method: 回顾和分析量子模拟器在过去几十年的发展，评估其在揭示新物理现象方面的实际贡献

Result: 量子模拟器已在多个重要实例中推进了对多体量子动力学的理解，这些发现虽然理论上也可获得，但首次实现是通过量子处理器

Conclusion: 量子模拟器已开始挑战并完善传统认知，虽然非平衡动力学之外的广阔问题领域仍有待探索，但前景令人鼓舞

Abstract: Major technological advances of the past century are rooted in our understanding of quantum physics in the non-interacting limit. A central challenge today is to understand the behavior of complex quantum many-body systems, where interactions play an essential role. About four decades ago, Richard Feynman proposed using controllable quantum systems to efficiently simulate complex physics and chemistry problems, envisioning quantum orreries, highly tunable quantum devices built to emulate less understood quantum systems. Here we ask whether quantum simulators have already uncovered new physical phenomena-and, if so, in which areas and with what impact. We find that, in several notable instances, they have advanced our understanding of many-body quantum dynamics. Although many of these insights could in principle have been obtained theoretically or numerically, they were nevertheless first achieved using quantum processors. While a broad landscape of problems beyond non-equilibrium dynamics still awaits exploration, it is encouraging that quantum simulators are already beginning to challenge and refine our conventional wisdom.

</details>


### [125] [Photonic Quantum-Accelerated Machine Learning](https://arxiv.org/abs/2512.08318)
*Markus Rambach,Abhishek Roy,Alexei Gilchrist,Akitada Sakurai,William J. Munro,Kae Nemoto,Andrew G. White*

Main category: quant-ph

TL;DR: 该研究提出了一种基于玻色采样的量子加速器，用于增强经典机器学习，通过量子指纹提升储层计算性能，并在实际量子硬件上验证了其加速和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 机器学习在现代社会中广泛应用，但尚未充分利用量子资源的独特优势。玻色采样作为一种基于量子干涉的采样协议，是经典计算机难以模拟的资源，可以在当前量子硬件上实现。

Method: 使用玻色采样为储层计算提供高维量子指纹，构建量子加速器。该方法在各种条件下测试：包括不完美的光子源（甚至完全可区分）、严重类别不平衡场景（手写数字和生物医学图像分类）、以及稀疏数据情况。

Result: 展示了鲁棒的性能改进：在光子源不完美情况下仍能工作；在严重类别不平衡场景中有效分类；在稀疏数据条件下，仅用二十分之一的训练数据仍能保持模型准确性。最重要的是，在光子量子处理单元上验证了方案的加速和可扩展性。

Conclusion: 这是首次实验验证玻色采样增强学习在实际量子硬件上能带来真实的性能提升，为量子加速经典机器学习提供了实际可行的路径。

Abstract: Machine learning is widely applied in modern society, but has yet to capitalise on the unique benefits offered by quantum resources. Boson sampling -- a quantum-interference based sampling protocol -- is a resource that is classically hard to simulate and can be implemented on current quantum hardware. Here, we present a quantum accelerator for classical machine learning, using boson sampling to provide a high-dimensional quantum fingerprint for reservoir computing. We show robust performance improvements under various conditions: imperfect photon sources down to complete distinguishability; scenarios with severe class imbalances, classifying both handwritten digits and biomedical images; and sparse data, maintaining model accuracy with twenty times less training data. Crucially, we demonstrate the acceleration and scalability of our scheme on a photonic quantum processing unit, providing the first experimental validation that boson-sampling-enhanced learning delivers real performance gains on actual quantum hardware.

</details>


### [126] [Constraint-oriented biased quantum search for general constrained combinatorial optimization problems](https://arxiv.org/abs/2512.08384)
*Sören Wilkening*

Main category: quant-ph

TL;DR: 提出了一种基于Grover算法的量子启发式方法，用于处理具有任意高效可计算目标函数和约束函数的组合优化问题，相比之前主要限于线性约束的方法有更广泛的适用性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Grover的量子启发式方法主要局限于线性约束问题，无法处理更广泛的组合优化问题。为了扩展量子算法在组合优化领域的应用范围，需要开发能够处理任意高效可计算约束函数的方法。

Method: 开发了一种量子算法例程，基于Grover算法框架，能够处理任意高效可计算的目标函数和约束函数。该方法在假设存在足够先进的逻辑量子硬件的前提下进行评估，将之前主要限于线性约束的方法推广到更广泛的问题类别。

Result: 在假设存在足够先进的逻辑量子硬件条件下，该方法在运行时间缩放和解决方案质量方面有潜力超越最先进的经典求解器和启发式方法。逻辑量子算法可以实现高达10^2-10^3倍的运行时间节省，对于更现实的实现也可能具有优势。

Conclusion: 该研究扩展了基于Grover的量子启发式方法的应用范围，使其能够处理更广泛的组合优化问题。虽然评估基于理想化的硬件假设，但结果表明量子算法在组合优化领域具有显著潜力，可能为实际问题带来实质性改进。

Abstract: We present a quantum algorithmic routine that extends the realm of Grover-based heuristics for tackling combinatorial optimization problems with arbitrary efficiently computable objective and constraint functions. Building on previously developed quantum methods that were primarily restricted to linear constraints, we generalize the approach to encompass a broader class of problems in discrete domains. To evaluate the potential of our algorithm, we assume the existence of sufficiently advanced logical quantum hardware. With this assumption, we demonstrate that our method has the potential to outperform state-of-the-art classical solvers and heuristics in terms of both runtime scaling and solution quality. The same may be true for more realistic implementations, as the logical quantum algorithm can achieve runtime savings of up to $10^2-10^3$.

</details>


### [127] [Single-Step Phase-Engineered Pulse for Active Readout Cavity Reset in Superconducting Circuits](https://arxiv.org/abs/2512.08393)
*Ren-Ze Zhao,Ze-An Zhao,Tian-Le Wang,Peng Wang,Sheng Zhang,Xiao-Yan Yang,Hai-Feng Zhang,Zhi-Fei Li,Yuan Wu,Zi-Hao Fu,Sheng-Ri Liu,Peng Duan,Guo-Ping Guo*

Main category: quant-ph

TL;DR: 实验演示了一种简单高效的SSPE脉冲方案，用于主动清空读取腔，通过添加相位工程化的重置段加速光子耗散，相比被动衰减快6倍，且对量子比特的扰动最小。


<details>
  <summary>Details</summary>
Motivation: 在电路QED架构中，需要快速、平滑、低干扰的清空读取腔的方法，以提升量子计算系统的效率和可扩展性。

Method: 在标准方形读取脉冲后添加一个具有定制幅度和相位的重置段，在线性响应区域内优化重置参数，通过表征腔光子动力学验证效果。

Result: SSPE脉冲将光子耗散加速了6倍，相比方形脉冲和CLEAR脉冲，对量子比特的激发和弛豫率最低，校准过程显著简化。

Conclusion: SSPE方案是一种实用且可扩展的方法，适用于超导量子电路中实现快速、平滑、低干扰的腔重置。

Abstract: In a circuit QED architecture, we experimentally demonstrate a simple and hardware-efficient Single-Step Phase-Engineered (SSPE) pulse scheme for actively depopulating the readout cavity. The method appends a reset segment with tailored amplitude and phase to a normal square readout pulse. Within the linear-response regime, the optimal reset amplitude scales proportionally with the readout amplitude, while the optimal reset phase remains nearly invariant, significantly simplifying the calibration process. By characterizing the cavity photons dynamics, we show that the SSPE pulse accelerates photon depletion by up to a factor of six compared to passive free decay. We further quantify the qubit backaction induced by the readout pulse and find that the SSPE pulse yields the lowest excitation and relaxation rates compared to a Square and CLEAR pulses. Our results establish the SSPE scheme as a practical and scalable approach for achieving fast, smooth, low-backaction cavity reset in superconducting quantum circuits.

</details>


### [128] [Universal recoverability of quantum states in tracial von-Neumann algebras](https://arxiv.org/abs/2512.08418)
*Saptak Bhattacharya*

Main category: quant-ph

TL;DR: 本文讨论了在迹von-Neumann代数上对夹心拟相对熵$\mathcal{S}_2$的量子数据处理不等式的改进，主要结果是获得了Petz恢复映射的普适可恢复性界


<details>
  <summary>Details</summary>
Motivation: 量子数据处理不等式在量子信息理论中具有重要意义，但现有结果主要限于有限维情况。本文旨在将Petz恢复映射的普适可恢复性界推广到更一般的迹von-Neumann代数框架

Method: 使用夹心拟相对熵$\mathcal{S}_2$作为度量工具，在迹von-Neumann代数上进行分析，主要技术包括量子信息理论和算子代数方法

Result: 成功将Petz恢复映射的普适可恢复性界从有限维情况推广到迹von-Neumann代数框架，为量子数据处理不等式提供了更一般的理论结果

Conclusion: 该研究扩展了量子数据处理不等式的适用范围，为量子信息理论在无限维系统中的应用提供了理论基础，具有重要的理论意义

Abstract: In this paper, we discuss a refinement of quantum data processing inequality for the sandwiched quasi-relative entropy $\mathcal{S}_2$ on a tracial von-Neumann algebra. The main result is a universal recoverability bound with the Petz recovery map, which was previously obtained in the finite dimensional setup.

</details>


### [129] [A Grover-compatible manifold optimization algorithm for quantum search](https://arxiv.org/abs/2512.08432)
*Zhijian Lai,Dong An,Jiang Hu,Zaiwen Wen*

Main category: quant-ph

TL;DR: 该论文将Grover算法的非结构化搜索问题重新表述为酉流形上的最大化问题，通过黎曼梯度上升方法求解，并引入Grover兼容回缩来确保更新对应物理可实现的量子算子，最终获得了与Grover算法相同的二次加速效果。


<details>
  <summary>Details</summary>
Motivation: Grover算法通过交替应用物理可实现的oracle和扩散算子为无结构搜索问题提供二次加速。本文旨在从优化角度重新审视这一问题，探索是否可以通过优化方法获得新的概念见解并推动量子算法设计的新进展。

Method: 将无结构搜索重新表述为酉流形上的最大化问题，采用黎曼梯度上升方法求解。为解决通用RGA更新通常不对应物理可实现的量子算子的问题，引入了Grover兼容回缩来限制RGA更新为有效的oracle和扩散算子。

Result: 建立了局部黎曼μ-Polyak-Łojasiewicz不等式（μ=1/2），获得了1-κ^{-1}的线性收敛速率。考虑到酉流形的几何特性和成本函数的特殊结构，证明了L_Rie = O(√N)，从而得到O(√N log(1/ε))的迭代复杂度，与Grover算法的二次加速O(√N)相匹配。

Conclusion: 优化视角能够为量子算法设计提供新的概念见解并推动新进展。该方法不仅重现了Grover算法的二次加速效果，还展示了黎曼优化框架在量子计算中的潜力。

Abstract: Grover's algorithm is a fundamental quantum algorithm that offers a quadratic speedup for the unstructured search problem by alternately applying physically implementable oracle and diffusion operators. In this paper, we reformulate the unstructured search as a maximization problem on the unitary manifold and solve it via the Riemannian gradient ascent (RGA) method. To overcome the difficulty that generic RGA updates do not, in general, correspond to physically implementable quantum operators, we introduce Grover-compatible retractions to restrict RGA updates to valid oracle and diffusion operators. Theoretically, we establish a local Riemannian $μ$-Polyak-Łojasiewicz (PL) inequality with $μ= \tfrac{1}{2}$, which yields a linear convergence rate of $1 - κ^{-1}$ toward the global solution. Here, the condition number $κ= L_{\mathrm{Rie}} / μ$, where $L_{\mathrm{Rie}}$ denotes the Riemannian Lipschitz constant of the gradient. Taking into account both the geometry of the unitary manifold and the special structure of the cost function, we show that $L_{\mathrm{Rie}} = O(\sqrt{N})$ for problem size $N = 2^n$. Consequently, the resulting iteration complexity is $O(\sqrt{N} \log(1/\varepsilon))$ for attaining an $\varepsilon$-accurate solution, which matches the quadratic speedup of $O(\sqrt{N})$ achieved by Grover's algorithm. These results demonstrate that an optimization-based viewpoint can offer fresh conceptual insights and lead to new advances in the design of quantum algorithms.

</details>


### [130] [Benchmarking Gaussian and non-Gaussian input states with a hybrid sampling platform](https://arxiv.org/abs/2512.08433)
*Michael Stefszky,Kai-Hong Luo,Jan-Lucas Eickmann,Simone Atzeni,Florian Lütkewitte,Cheeranjiv Pandey,Fabian Schlue,Jonas Lammers,Mikhail Roiz,Timon Schapeler,Laura Ares,Milad Yahyapour,Alexander Kastner,Joschua Martinek,Michael Mittermair,Carlos Sevilla,Marius Leyendecker,Oskar Kohout,Dmitriy Mitin,Ronald Holzwarth,Jan Sperling,Tim Bartley,Fabian Steinlechner,Benjamin Brecht,Christine Silberhorn*

Main category: quant-ph

TL;DR: 该研究介绍了Paderborn量子采样器(PaQS)，这是一个能够使用高斯和非高斯输入态进行采样实验的混合平台，通过半设备无关框架验证量子优势，并观察到非高斯输入态带来的性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着玻色采样实验规模的扩大，实验上要求苛刻的单光子源被高斯态取代，这减少了可用的非高斯性——一种关键的量子资源。社区从最初的采样任务扩展到可能的实际应用时，需要量化减少非高斯资源的性能成本，并对使用不同输入态的采样平台进行基准测试。

Method: 研究者引入了Paderborn量子采样器(PaQS)，这是一个混合平台，能够在单次实验运行中使用8个高斯或非高斯输入态在12模式干涉仪中进行采样实验。该架构允许在相同条件下直接并行比较不同采样机制。通过采用半设备无关框架，提供不依赖于干涉仪或输入态先验知识的认证，验证观测数据无法被任何经典模型重现。

Result: 应用该框架，研究者观察到非高斯输入态带来的明显性能提升，验证了观测数据无法被任何经典模型重现，这是展示量子优势的先决条件。

Conclusion: PaQS平台为直接比较不同采样机制提供了有效工具，通过半设备无关框架验证了量子优势，并证实了非高斯资源在量子采样中的重要性，为未来量子计算的实际应用提供了重要基准。

Abstract: The original boson sampling paradigm-consisting of multiple single-photon input states, a large interferometer, and multi-channel click detection-was originally proposed as a photonic route to quantum computational advantage. Its non-Gaussian resources, essential for outperforming any classical system, are provided by single-photon inputs and click detection. Yet the drive toward larger experiments has led to the replacement of experimentally demanding single-photon sources with Gaussian states, thereby diminishing the available non-Gaussianity-a critical quantum resource. As the community broadens its focus from the initial sampling task to possible real-world applications, it becomes crucial to quantify the performance cost associated with reducing non-Gaussian resources and to benchmark sampling platforms that employ different input states.
  To address this need, we introduce the Paderborn Quantum Sampler (PaQS), a hybrid platform capable of performing sampling experiments with eight Gaussian or non-Gaussian input states in a 12-mode interferometer within a single experimental run. This architecture enables direct, side-by-side benchmarking of distinct sampling regimes under otherwise identical conditions. By employing a semi-device-independent framework, offering certification that does not rely on prior knowledge of the interferometer or the input states, we verify that the observed data cannot be reproduced by any classical model-a prerequisite for demonstrating quantum advantage. Applying this framework, we observe clear performance gains arising from non-Gaussian input states.

</details>


### [131] [High-OAM Deep Ultraviolet Twisted Light Generation for RF-Photoinjector Applications](https://arxiv.org/abs/2512.08442)
*A. S. Dyatlov,D. M. Dolgintsev,V. V. Gerasimov,V. V. Kobets,V. P. Nazmov,M. A. Nozdrin,A. N. Sergeev,D. S. Shokin,K. E. Yunenko,D. V. Karlovets*

Main category: quant-ph

TL;DR: 该研究首次在深紫外波段（266nm）使用三种衍射光学元件成功生成并表征了高轨道角动量扭曲光，验证了其在电子RF光注入器中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 开发与高功率紫外激光系统兼容的深紫外高轨道角动量光束生成技术，为粒子加速器设施中的结构光阴极照明和相对论涡旋电子生成提供实用途径。

Method: 使用三种制造的衍射光学元件：反射叉形光栅、高电荷螺旋相位板（SPP）和二元轴棱镜，将其集成到电子RF光注入器的驱动激光束线中，在加速器相关条件下直接评估性能。

Result: SPP产生了OAM l=64的高纯度拉盖尔-高斯模式，转换效率约80%；二元轴棱镜生成了拓扑电荷高达m=10的准贝塞尔扭曲光，具有低发散度和稳定的多瓣环结构；叉形光栅可靠产生了l=2-8的低阶模式，模拟与柱面透镜诊断结果一致。

Conclusion: 这是首次通过制造DOEs生成深紫外高OAM光束并经过模式转换测量验证的综合实验演示，所展示的技术与RF光注入器中使用的高功率紫外激光系统兼容，为结构光阴极照明和相对论涡旋电子生成提供了实用途径。

Abstract: We report on the generation and characterization of ultraviolet (wavelength 266 nm) twisted light with high orbital angular momentum (OAM) using three types of fabricated diffractive optical elements (DOEs): a reflective fork grating, a high-charge spiral phase plate (SPP), and binary axicons. All elements were integrated into a drive-laser beamline of an electron RF-photoinjector, enabling direct evaluation under accelerator-relevant conditions.
  The SPP produced a high-purity Laguerre-Gaussian mode with OAM l = 64 and a measured conversion efficiency of approximately 80\%. Binary axicons generated quasi-Bessel twisted light with topological charges up to m = 10, exhibiting low divergence and stable multi-lobe ring structures. The fork grating reliably produced lower-order modes, l = 2-8, with good agreement between simulations and cylindrical-lens diagnostics.
  These results constitute, to our knowledge, the first comprehensive experimental demonstration of deep-UV high-OAM beams generated with fabricated DOEs and validated through mode-conversion measurements. The demonstrated techniques are compatible with high-power UV laser systems used in RF-photoinjectors and offer a practical route toward structured photocathode illumination and the generation of relativistic vortex electrons at a particle accelerator facility.

</details>


### [132] [Heralded generation of a three-mode NOON state](https://arxiv.org/abs/2512.08458)
*Sukhjit P. Singh,Elnaz Bazzazi,Diego N. Bernal-García,Simon White,Hassan Jamal Latief,Alison Goldingay,Sven Rogge,Sergei Slussarenko,Farzad Ghafari,Emanuele Polino,Nora Tischler*

Main category: quant-ph

TL;DR: 该论文报道了通过单光子探测在辅助模式中实现三模双光子NOON态确定性生成的实验，验证了高保真度和真实多体纠缠，为线性光学量子信息提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 光子纠缠态是量子通信、计算和计量学的基础，但传统生成方法受限于光子间缺乏固有相互作用，导致生成过程本质上是概率性的而非确定性的。后选择技术会破坏纠缠态，而预兆技术通过测量辅助光子来指示态生成而不破坏目标态。

Method: 采用预兆方案生成三模双光子NOON态，通过检测一个辅助模式中的单光子来指示三个目标模式中纠缠态的存在。实验验证通过估计与理想三模NOON态的保真度并认证真实多体纠缠。

Result: 实验成功生成三模双光子NOON态，估计保真度为0.823 ± 0.018，并认证了真实多体纠缠。方案具有高成功概率和小资源开销。

Conclusion: 该工作为纠缠多模态生成提供了理论和实验基础，在当前技术下可实现，代表了线性光学量子信息中与多量子比特态编码互补的关键方向。

Abstract: Entangled states of photons form the foundation of quantum communication, computation, and metrology. Yet their generation remains fundamentally constrained: in the absence of intrinsic photon-photon interactions, the generation of such states is inherently probabilistic rather than deterministic. The prevalent technique of post-selection verifies the creation of an entangled state by detecting and thus destroying it. Heralding offers a solution in which measuring ancillary photons in auxiliary modes signals the state generation without the need to measure it. Here, we report an experiment to generate a three-mode two-photon NOON state, where the detection of a single photon in one heralding mode signifies the presence of the state in three target modes. We validate the generated state by estimating a fidelity of 0.823 +/- 0.018 with respect to an ideal three-mode NOON state and certifying genuine multipartite entanglement. By virtue of the high success probability and small resource overhead of our scheme, our work provides a theoretical and experimental stepping stone for entangled multi-mode state generation, which is realizable with current technology. These multi-mode entangled states represent a key direction for linear optical quantum information that is complementary to multi-qubit state encoding.

</details>


### [133] [Higher Josephson harmonics in a tunable double-junction transmon qubit](https://arxiv.org/abs/2512.08470)
*Ksenia Shagalov,David Feldstein-Bofill,Leo Uhre Jakobsen,Zhenhai Sun,Casper Wied,Amalie T. J. Paulsen,Johann Bock Severin,Malthe A. Marciniak,Clinton A. Potts,Anders Kringhøj,Jacob Hastrup,Karsten Flensberg,Svend Krøjer,Morten Kjaergaard*

Main category: quant-ph

TL;DR: 论文展示了一种可调谐约瑟夫森谐波超导电路元件，通过隧道结与SQUID环串联实现约瑟夫森势能谐波内容的高度磁通可调性，为量子比特设计提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 传统约瑟夫森结的非线性主要来自基频谐波，限制了量子比特设计的灵活性。开发可调谐约瑟夫森谐波元件可以创建更优化的量子比特设计，实现更好的相干性和可定制非线性微波器件。

Method: 采用隧道结与SQUID环串联的超导电路结构，通过外部磁通量高度可调地控制约瑟夫森势能的谐波内容。通过包含内部模式的电路模型分析前四个量子比特跃迁的光谱，量化二次谐波与基频谐波的比例。

Result: 实验结果显示二次谐波可达基频谐波的约10%。发现通过平衡内部模式和量子比特模式的色散耦合，可以实现色散位移为零的甜点位置。该结构具有高度可调性。

Conclusion: 这种高度可调的约瑟夫森谐波元件为受保护量子比特的实现提供了途径，并为可定制非线性微波器件开辟了新的设计可能性。

Abstract: Tunable Josephson harmonics open up for new qubit design. We demonstrate a superconducting circuit element with a tunnel junction in series with a SQUID loop, yielding a highly magnetic-flux tunable harmonic content of the Josephson potential. We analyze spectroscopy of the first four qubit transitions with a circuit model which includes the internal mode, revealing a second harmonic up to $\sim10\%$ of the fundamental harmonic. Interestingly, a sweet spot where the dispersive shift vanishes is achieved by balancing the dispersive couplings to the internal and qubit modes. The highly tunable set-up provides a route toward protected qubits, and customizable nonlinear microwave devices.

</details>


### [134] [Syntactic Structure, Quantum Weights](https://arxiv.org/abs/2512.08507)
*Kentaro Imafuku*

Main category: quant-ph

TL;DR: 论文从最小描述约束出发，解释了为什么局部作用和指数欧几里得权重在经典、统计和量子理论中普遍出现。作者证明这些普遍形式源于有限前缀自由编码的纯句法要求，而非特定物理假设。


<details>
  <summary>Details</summary>
Motivation: 解释为什么局部作用和指数欧几里得权重在经典、统计和量子理论中普遍出现。传统上这些形式被视为经验事实或基本假设，但作者希望从更基本的描述性约束中找到结构性的解释。

Method: 假设历史允许有限、自定界（前缀自由）的生成编码，这些编码可以按顺序解码。从这些纯句法要求出发，定义最小描述成本（可解释为平滑的最小程序长度），该成本在局部段上是可加的。然后分析这种描述结构如何强制确定作用量的普遍形式。

Result: 1. 任何连续的局部可加成本，如果其平稳扇区与经验识别的经典变分扇区重合，则被迫进入唯一的欧拉-拉格朗日等价类。因此作用的普遍形式仅由描述结构决定，而具体的微观拉格朗日量和耦合仍然是系统依赖的语义输入。
2. 有限前缀自由语言表现出指数冗余：许多不同的程序编码相同的粗历史，这种冗余诱导了历史的普遍指数多重性权重。要求该权重为实数且有下界，为稳定的局部玻色子系统选择了实欧几里得代表，产生了标准欧几里得路径积分形式。

Conclusion: 局部作用和指数欧几里得权重的普遍性源于有限前缀自由描述的基本句法约束，而非特定的物理假设。当Osterwalder-Schrader反射正定性成立时，欧几里得测度重构了幺正洛伦兹振幅。这表明物理理论的数学形式可以从信息论和计算理论的基本约束中推导出来。

Abstract: Why do local actions and exponential Euclidean weights arise so universally in classical, statistical, and quantum theories? We offer a structural explanation from minimal constraints on finite descriptions of admissible histories. Assume that histories admit finite, self-delimiting (prefix-free) generative codes that can be decoded sequentially in a single forward pass. These purely syntactic requirements define a minimal descriptive cost, interpretable as a smoothed minimal program length, that is additive over local segments. First, any continuous local additive cost whose stationary sector coincides with the empirically identified classical variational sector is forced into a unique Euler--Lagrange equivalence class. Hence the universal form of an action is fixed by descriptional structure alone, while the specific microscopic Lagrangian and couplings remain system-dependent semantic input. Second, independently of microscopic stochasticity, finite prefix-free languages exhibit exponential redundancy: many distinct programs encode the same coarse history, and this redundancy induces a universal exponential multiplicity weight on histories. Requiring this weight to be real and bounded below selects a real Euclidean representative for stable local bosonic systems, yielding the standard Euclidean path-integral form. When Osterwalder--Schrader reflection positivity holds, the Euclidean measure reconstructs a unitary Lorentzian amplitude.

</details>


### [135] [Tunable passive squeezing of squeezed light through unbalanced double homodyne detection](https://arxiv.org/abs/2512.08540)
*Niels Tripier-Mondancin,David Barral,Ganaël Roeland,Raúl Leonardo Rincon Celis,Yann Bouchereau,Nicolas Treps*

Main category: quant-ph

TL;DR: 本文展示了不平衡双零差检测不仅是被动测量装置，还能主动对量子态进行有效压缩或反压缩变换，实现量子态的操控与表征一体化。


<details>
  <summary>Details</summary>
Motivation: 量子光态的完整表征是量子光学和信息科学的核心任务。传统双零差检测虽然能直接测量Husimi Q准概率分布，但仅限于被动测量。研究者希望探索该检测方案是否还能主动操控量子态。

Method: 通过有意不平衡分割量子信号的输入分束器，使检测方案本身对被测态执行有效的压缩或反压缩变换。使用稳健的偏振编码双零差检测技术，通过调节分束器反射率这一可调实验参数来控制压缩强度。

Result: 实验实现了对压缩真空态的表征，展示了测量Q函数相空间分布的可控变形，证实了不平衡双零差检测能够直接采样经过压缩变换的输入态Q函数。

Conclusion: 不平衡双零差检测是一种多功能工具，能够同时实现量子态操控和表征，为量子态测量提供了新的主动控制维度。

Abstract: The full characterization of quantum states of light is a central task in quantum optics and information science. Double homodyne detection provides a powerful method for the direct measurement of the Husimi Q quasi-probability distribution, offering a complete state representation in a simple experimental setting and a limited time frame. Here, we demonstrate that double homodyne detection can serve as more than a passive measurement apparatus. By intentionally unbalancing the input beamsplitter that splits the quantum signal, we show that the detection scheme itself performs an effective squeezing or anti-squeezing transformation on the state being measured. The resulting measurement directly samples the Q function of the input state as if it were acted upon by a squeezing operator whose strength is a tunable experimental parameter : the beamsplitter's reflectivity. We experimentally realize this technique using a robust polarization-encoded double homodyne detection to characterize a squeezed vacuum state. Our results demonstrate the controlled deformation of the measured Q function's phase-space distribution, confirming that unbalanced double homodyne detection is a versatile tool for simultaneous quantum state manipulation and characterization.

</details>


### [136] [Quantum simulation in the entanglement picture](https://arxiv.org/abs/2512.08565)
*D. -S. Wang,X. Xu,Y. -D. Liu*

Main category: quant-ph

TL;DR: 提出基于通道-态对偶的纠缠图像新概念，展示其在量子算法中的多种应用


<details>
  <summary>Details</summary>
Motivation: "图像"概念在量子力学中具有基础性地位，本文旨在基于量子信息科学中重要的通道-态对偶关系，提出一种新的理论框架

Method: 提出纠缠图像新概念，基于通道-态对偶关系构建理论框架

Result: 展示了纠缠图像在多体动力学模拟、量子场论、热物理以及更一般物理量计算等量子算法中的应用

Conclusion: 纠缠图像为量子力学提供了新的理论视角，在量子信息科学中具有重要价值，能够应用于多种量子算法领域

Abstract: The notion of ``picture'' is fundamental in quantum mechanics. In this work, a new picture, which we call entanglement picture, is proposed based on the novel channel-state duality, whose importance is revealed in quantum information science. We illustrate the application of entanglement picture in quantum algorithms for the simulation of many-body dynamics, quantum field theory, thermal physics, and more generic quantities.

</details>


### [137] [$\mathcal{PT}$-symmetric cavity magnomechanics with gain-assisted transparency and amplification](https://arxiv.org/abs/2512.08612)
*Cham Oumie,Wu-Ming Liu,Kashif Ammar Yasir*

Main category: quant-ph

TL;DR: 该研究探讨了宇称-时间对称腔磁机械系统中磁机械诱导透明现象，通过行波场诱导的非厄米性实现了从单窗口到多窗口透明谱的调控，并展示了增益辅助的不对称传输和Fano型线形。


<details>
  <summary>Details</summary>
Motivation: 研究宇称-时间对称腔磁机械系统中的磁机械诱导透明现象，探索非厄米耦合如何影响透明谱特性，为可重构量子信号处理和增强传感提供新平台。

Method: 构建微波腔模式与钇铁石榴石球体中磁振子耦合的系统，通过磁致伸缩相互作用使磁振子与机械振动模式杂化。在厄米区域研究强光子-磁振子耦合，引入非厄米耦合后进入宇称-时间破缺区域，分析透明谱、Fano型线形和群延迟特性。

Result: 在厄米区域，强光子-磁振子耦合产生单透明窗口，磁机械耦合使其分裂为双峰。非厄米耦合导致增益辅助的不对称传输，一侧放大一侧增强吸收。通过调节腔失谐，磁机械透明转变为具有强非洛伦兹相位色散的Fano型线形。群延迟分析显示慢光和快光行为可通过耦合参数广泛调控。

Conclusion: 宇称-时间对称腔磁机械系统为可重构量子信号处理和增强传感提供了有前景的平台，能够通过调控光子-磁振子耦合、磁机械耦合和非厄米强度来广泛调节透明谱特性和群延迟行为。

Abstract: We investigate magnomechanically induced transparency in a parity-time-symmetric cavity magnomechanical system with traveling-field-induced non-Hermiticity. The setup consists of a microwave cavity mode coupled to magnons in a single-crystal yttrium iron garnet sphere, which in turn are hybridized with a vibrational mechanical mode through magnetostrictive interaction. In the Hermitian regime, strong photon-magnon coupling generates a single transparency window in the cavity transmission, which splits into a doublet when the magnon is coherently hybridized with the mechanical mode via magnomechanical coupling. This establishes a versatile platform in which the transparency spectrum can be engineered from single- to multi-window response using experimentally accessible, scaled magnomechanical interactions. When a non-Hermitian coupling is introduced, the system enters a parity-time-broken regime in which the transparency ceases to be purely passive and becomes gain assisted, leading to asymmetric transmission with amplification on one side of the resonance and enhanced absorption on the other. By tuning the cavity detuning, we convert magnomechanical transparency into Fano-type line shapes with strongly non-Lorentzian phase dispersion and map their deformation into asymmetric, gain-assisted Fano ridges in the joint space of probe and magnon detunings. Finally, we analyze the associated group delay and show that both slow- and fast-light behavior can be widely tuned by varying the photon-magnon and magnomechanical couplings together with the non-Hermitian strength, highlighting parity-time-symmetric cavity magnomechanics as a promising platform for reconfigurable quantum signal processing and enhanced sensing.

</details>


### [138] [An Efficient Secret Communication Scheme for the Bosonic Wiretap Channel](https://arxiv.org/abs/2512.08623)
*Esther Hänggi,Iyán Méndez Veiga,Ligong Wang*

Main category: quant-ph

TL;DR: 提出了一种基于玻色窃听信道的新型保密通信方案，使用激光器和直接光电探测器等现成硬件，结合随机性提取器、脉冲位置调制和Reed-Solomon码实现计算高效，能抵抗窃听者的相干联合测量攻击。


<details>
  <summary>Details</summary>
Motivation: 开发一种实用的量子保密通信方案，利用现有硬件设备实现高效安全的通信，解决量子信道中的窃听安全问题。

Method: 采用随机性提取器、脉冲位置调制和Reed-Solomon编码技术，结合激光器和直接光电探测器等现成硬件，构建计算高效的保密通信系统。

Result: 该方案能抵抗窃听者进行相干联合测量攻击，在低光子流极限下达到渐近最优，获得与相同信道保密容量相同的主导项。

Conclusion: 提出的方案为玻色窃听信道提供了一种实用、计算高效且安全的保密通信方法，在低光子流条件下能达到理论最优性能。

Abstract: We propose a new secret communication scheme over the bosonic wiretap channel. It uses readily available hardware such as lasers and direct photodetectors. The scheme is based on randomness extractors, pulse-position modulation, and Reed-Solomon codes and is therefore computationally efficient. It is secure against an eavesdropper performing coherent joint measurements on the quantum states it observes. In the low-photon-flow limit, the scheme is asymptotically optimal and achieves the same dominant term as the secrecy capacity of the same channel.

</details>


### [139] [Parity erasure: a foundational principle for indefinite causal order](https://arxiv.org/abs/2512.08635)
*Zixuan Liu,Ognyan Oreshkov*

Main category: quant-ph

TL;DR: 该论文提出了一个称为"奇偶性擦除"的信息论原理，用于完全表征具有不确定因果顺序的过程，这一原理不仅适用于量子理论，也适用于更广泛的操作概率理论。


<details>
  <summary>Details</summary>
Motivation: 当量子理论在局部有效时，会出现不确定因果顺序的过程。研究者希望找到一种不依赖于量子理论形式的信息论原理来完全表征这类过程。

Method: 研究者提出了"奇偶性擦除"这一信息论原理，该原理基于一般操作概率理论的一组公理推导而来，不依赖于量子理论本身的形式体系。

Result: 奇偶性擦除原理能够完全表征具有不确定因果顺序的过程，这一原理不仅适用于量子理论，也适用于一大类超越量子理论的理论。

Conclusion: 这种信息论方法揭示了在具有不确定因果结构的场景中信息交换的基本性质，为理解这类过程提供了理论基础。

Abstract: Processes with indefinite causal order can arise when quantum theory is locally valid. Here, we identify an information-theoretic principle, termed parity erasure, that completely characterizes such processes. Our characterization does not rely on the formalism of quantum theory itself, but instead is derived from a set of axioms for general operational probabilistic theories, and thus holds also for a large class of theories beyond quantum theory. This informational approach reveals a fundamental property of information exchange in scenarios with indefinite causal structure.

</details>


### [140] [Strain sensitivity enhancement in a Grover-Michelson interferometer](https://arxiv.org/abs/2512.08638)
*Anthony D. Manni,Christopher R. Schwarze,David S. Simon,Abdoulaye Ndao,Alexander V. Sergienko*

Main category: quant-ph

TL;DR: 使用Grover硬币等新型无偏四端口散射体替代传统分束器可提升Michelson干涉仪的相位检测分辨率，分析了引力波引起的相位扰动在Grover-Michelson干涉仪中产生的边带频率信噪比。


<details>
  <summary>Details</summary>
Motivation: 传统Michelson干涉仪在引力波探测中的相位检测分辨率有限，需要寻找新的光学配置来提升灵敏度。

Method: 使用Grover硬币等方向无偏四端口散射体替代传统分束器，构建Grover-Michelson干涉仪，并分析其边带频率信噪比，同时结合光回收配置进一步提升性能。

Result: Grover-Michelson干涉仪配置能够增强信号，结合光回收安排可进一步改善性能，为引力波探测提供更高的相位检测分辨率。

Conclusion: 新型方向无偏四端口散射体在Michelson干涉仪中的应用显著提升了相位检测能力，为高精度引力波探测提供了有前景的技术路径。

Abstract: The Michelson interferometric phase detection resolution can be enhanced by replacing conventional beam splitters with novel directionally unbiased four-port scatterers, such as Grover coins. We present a quantitative analysis of the noise-to-signal ratio of sideband frequencies generated by gravitational wave-induced phase perturbations in a Grover-Michelson interferometer (GMI). We discuss the principles of GMI signal enhancement and demonstrate how combining this configuration with additional light-recycling arrangements further enhances the performance.

</details>


### [141] [Quantum Brownian Motion as a Classical Stochastic Process in Phase Space](https://arxiv.org/abs/2512.08641)
*Dmitriy Kondaurov,Evgeny Polyakov*

Main category: quant-ph

TL;DR: 该论文证明Caldeira-Leggett模型中布朗粒子的精确量子动力学可以在任意温度下映射到相空间中的经典非马尔可夫随机过程。


<details>
  <summary>Details</summary>
Motivation: 建立量子系统与经典随机过程之间的精确对应关系，为模拟复杂驱动耗散量子协议提供新框架。

Method: 从粒子与热浴的相关热平衡态出发，对于二次势能证明精确对应；对于一般光滑势能，利用密度矩阵在坐标表示中强准对角化的特性，以浴谱截断作为控制参数进行近似。

Result: 证明了量子动力学到经典随机过程的精确映射，能够处理任意初始量子态（包括高度非经典叠加态），通过Wigner函数作为轨迹系综的统计权重，并支持外部操作和测量的模拟。

Conclusion: 该框架为量子开放系统的精确模拟提供了新途径，特别适用于复杂驱动耗散量子协议的仿真，将量子动力学问题转化为经典随机过程计算问题。

Abstract: We establish that the exact quantum dynamics of a Brownian particle in the Caldeira-Leggett model can be mapped, at any temperature, onto a classical, non-Markovian stochastic process in phase space. Starting from a correlated thermal equilibrium state between the particle and bath, we prove that this correspondence is exact for quadratic potentials under arbitrary quantum state preparations of the particle itself. For more general, smooth potentials, we identify and exploit a natural small parameter: the density matrix becomes strongly quasidiagonal in the coordinate representation, with its off-diagonal width shrinking as the bath's spectral cutoff increases, providing a controlled parameter for accurate approximation. The framework is fully general: arbitrary initial quantum states-including highly non-classical superpositions-are incorporated via their Wigner functions, which serve as statistical weights for trajectory ensembles. Furthermore, the formalism naturally accommodates external manipulations and measurements modeled by preparation functions acting at arbitrary times, enabling the simulation of complex driven-dissipative quantum protocols.

</details>


### [142] [Perfect continuous-variable quantum microcombs](https://arxiv.org/abs/2512.08650)
*Kangkang Li,Yue Wang,Ze Wang,Xin Zhou,Jincheng Li,Yinke Cheng,Binyan Wu,Qihuang Gong,Bei-Bei Li,Qi-Fan Yang*

Main category: quant-ph

TL;DR: 该论文实现了包含14个独立双模压缩态的连续变量量子微梳，每个态在0.7 THz带宽内展现出超过4 dB的原始压缩（最高达4.3 dB），为可扩展的集成CV量子技术提供了关键进展。


<details>
  <summary>Details</summary>
Motivation: 量子微梳作为连续变量量子信息处理的紧凑、多路复用纠缠源具有重要价值，但现有技术难以实现频谱均匀的压缩态，主要受限于色散轮廓的不对称性和异常。

Method: 结合工程化模式谱的微谐振腔和优化泵浦条件，克服色散不对称性限制，实现均匀的双模压缩态生成。

Result: 成功实现了包含14个独立双模压缩态的CV量子微梳，每个态在0.7 THz带宽内展现出超过4 dB的原始压缩（最高达4.3 dB），压缩性能均匀且优异。

Conclusion: 该均匀、高性能的量子资源代表了向可扩展、集成CV量子技术迈出的关键一步，为超越经典极限的量子信息处理奠定了基础。

Abstract: Quantum microcombs generated in high-Q microresonators provide compact, multiplexed sources of entangled modes for continuous-variable (CV) quantum information processing. While deterministic generation of CV states via Kerr-induced two-mode squeezing has been demonstrated, achieving spectrally uniform squeezing remains challenging because of asymmetry and anomalies in the dispersion profile. Here we overcome these limitations by combining a microresonator with an engineered mode spectrum and optimized pump conditions. We realize a CV quantum microcomb comprising 14 independent two-mode squeezed states, each exhibiting more than 4 dB of raw squeezing (up to 4.3 dB) across a 0.7 THz bandwidth. This uniform, high-performance quantum resource represents a key step toward scalable, integrated CV quantum technologies operating beyond classical limits.

</details>


### [143] [A Unified Framework for Optimizing Uniformly Controlled Structures in Quantum Circuits](https://arxiv.org/abs/2512.08675)
*Chengzhuo Xu,Xiao Chen,Xi Li,Zhihao Liu,Zhigang Li*

Main category: quant-ph

TL;DR: 该论文提出了一种统一的代数模型rUCG来分析和优化量子电路中普遍存在的均匀控制门结构，将门复杂度从O(n2^n)降低到O(2^n)，电路深度从O(2^n log n)降低到O(2^n log n/n)。


<details>
  <summary>Details</summary>
Motivation: 量子算法中普遍存在形式为Σ_c|c⟩⟨c|⊗U_c的酉算子，这类算子不仅包括标准均匀控制门，还涵盖大量具有均匀控制结构的电路。然而，其电路深度和门数量的复杂度尚未在统一框架下得到系统分析。

Method: 引入受限均匀控制门(rUCG)作为统一的代数模型，该模型由2可除阿贝尔群定义，能够建模控制门集合。该模型捕捉均匀控制旋转、多量子比特均匀控制门和对角酉算子。进一步扩展到k稀疏版本(k-rUCG)，其中只有部分控制比特参与每个多量子比特门。基于此代数模型开发通用分解框架。

Result: 对于n控制rUCG，框架将门复杂度从O(n2^n)降低到O(2^n)，电路深度从O(2^n log n)降低到O(2^n log n/n)。通过利用控制空间中的稀疏性，框架为k-rUCG提供了系统的规模和深度界限，优化系数与rUCG相同。在代表性QAOA电路和量子态制备上的实证评估证实了深度和规模的减少。

Conclusion: rUCG模型及其相关的分解框架将先前被认为结构不同的电路统一在单一、渐近最优的合成范式下，为量子电路优化提供了系统化的理论工具。

Abstract: Quantum unitaries of the form ${Σ_{c}\ket{c}\bra{c}\otimes U_{c}}$ are ubiquitous in quantum algorithms. This class encompasses not only standard uniformly controlled gates (UCGs) but also a wide range of circuits with uniformly controlled structures. However, their circuit-depth and gate-count complexities have not been systematically analyzed within a unified framework. In this work, we study the general decomposition problem for UCG and UCG-like structure. We then introduce the restricted Uniformly Controlled Gates (rUCGs) as a unified algebraic model, defined by a 2-divisible Abelian group that models the controlled gate set. This model captures uniformly controlled rotations, multi-qubit uniformly controlled gates, and diagonal unitaries. Furthermore, this model also naturally incorporates k-sparse version (k-rUCGs), where only a subset of control qubits participate in each multi-qubit gate. Building on this algebraic model, we develop a general framework. For an n-control rUCG, the framework reduce the gate complexity from ${O(n2^n)}$ to ${O(2^n})$ and the circuit depth from ${O(2^n\log n)}$ to ${O(2^n\log n/n)}$. The framework further provides systematic size and depth bounds for k-rUCGs by exploiting sparsity in the control space, with same optimization coefficient as rUCG, respectively. Empirical evaluations on representative QAOA circuits and quantum state preparation both confirm reductions in depth and size. Crucially, these results highlight that the rUCG model and its associated decomposition framework unify circuits previously considered structurally distinct under a single, asymptotically optimal synthesis paradigm.

</details>


### [144] [Non-Hermitian symmetry breaking and Lee-Yang theory for quantum XYZ and clock models](https://arxiv.org/abs/2512.08687)
*Tian-Yi Gu,Gaoyong Sun*

Main category: quant-ph

TL;DR: 论文将Lee-Yang理论推广到量子相变研究，通过引入复外磁场分析XY、XXZ、XYZ和ℤ₃时钟模型，发现复磁场破坏宇称对称性并诱导基态振荡，产生保真度零点。


<details>
  <summary>Details</summary>
Motivation: 将Lee-Yang理论从经典相变和动力学量子相变扩展到静态量子相变的研究，通过复外磁场探索量子模型中的对称性破缺和保真度零点现象。

Method: 研究一维XY模型、XXZ模型、XYZ模型和ℤ₃时钟模型在复外磁场下的行为，分析复磁场对宇称对称性的破坏效应，以及基态在宇称扇区间的振荡，计算保真度零点。

Result: 对于XY、XXZ和XYZ模型，复磁场破坏宇称对称性，诱导基态在两个宇称扇区间振荡，在有序相内产生保真度零点。对于ℤ₃时钟模型，复磁场将基态能量实部分裂为中性扇区(q=0)和带电扇区(q=1,2)，但保持带电扇区内的简并性，仅当投影掉一个带电扇区时才出现保真度零点。

Conclusion: 该研究成功将Lee-Yang理论推广到量子相变领域，通过复外磁场揭示了量子模型中对称性破缺与保真度零点之间的深刻联系，为理解量子相变提供了新的理论框架。

Abstract: Lee-Yang theory offers a unifying framework for understanding classical phase transitions and dynamical quantum phase transitions through the analysis of partition functions and Loschmidt echoes. Recently, this framework is extended to characterize quantum phase transitions in arXiv:2509.20258 by introducing the concepts of non-Hermitian symmetry breaking and fidelity zeros. Here, we generalize the theory by studying a broad class of quantum models, including the XY model, the XXZ model, the XYZ model, and the $\mathbb{Z}_3$ clock model in one dimension, subject to complex external magnetic field. For the XY, XXZ and XYZ models, we find that the complex field breaks parity symmetry and induces oscillations of the ground state between the two parity sectors, giving rise to fidelity zeros within the ordered phases. For the $\mathbb{Z}_3$ clock model, the complex field splits the real part of the ground-state energy between the neutral sector ($q=0$) and the charged sectors ($q=1,2$), while preserving the degeneracy within the charged sector. Fidelity zeros arise only after projecting out one of the charged sectors, and the finite-size scaling of these zeros produces critical exponents fully consistent with analytical predictions.

</details>


### [145] [Floquet Topological Frequency-Converting Amplifier](https://arxiv.org/abs/2512.08880)
*Adrian Parra-Rodriguez,Miguel Clavero-Rubio,Philippe Gigon,Tomás Ramos,Álvaro Gómez-León,Diego Porras*

Main category: quant-ph

TL;DR: 该论文提出了一种基于驱动-耗散Floquet模型的非厄米拓扑放大器，通过单个频率调制的谐振腔在频率空间中实现具有有效电场梯度的合成晶格，支持定向放大和频率转换。


<details>
  <summary>Details</summary>
Motivation: 探索在量子技术中实现非厄米拓扑放大的简单可行方案，利用现有平台如超导电路，构建具有拓扑特性的放大系统。

Method: 采用驱动-耗散Floquet模型，通过单个谐振腔的频率调制和衰减，在频率空间中构建非厄米合成晶格。使用Floquet-Green函数及其加倍空间表示分析拓扑特性，并通过局部绕数描述拓扑相。

Result: 识别出支持定向放大和频率转换的拓扑区域，发现其模式结构可用Jackiw-Rebbi类连续理论描述，包含狄拉克锥和合成频率中的孤子零模。

Conclusion: 该工作建立了一种简单且实验可行的非厄米拓扑放大实现路径，可直接应用于当前量子技术如超导电路中，为拓扑放大提供了新的平台。

Abstract: We introduce a driven-dissipative Floquet model in which a single harmonic oscillator with modulated frequency and decay realizes a non-Hermitian synthetic lattice with an effective electric field gradient in frequency space. Using the Floquet-Green's function and its doubled-space representation, we identify a topological regime that supports directional amplification and frequency conversion, accurately captured by a local winding number. The underlying mode structure is well described by a Jackiw-Rebbi-like continuum theory with Dirac cones and solitonic zero modes in synthetic frequency. Our results establish a simple and experimentally feasible route to non-Hermitian topological amplification, naturally implementable in current quantum technologies such as superconducting circuits.

</details>


### [146] [Emergent Non-Markovianity in Logical Qubit Dynamics](https://arxiv.org/abs/2512.08893)
*Jalan A. Ziyad,Robin Blume-Kohout,Kenneth Rudinger*

Main category: quant-ph

TL;DR: 量子纠错码编码的逻辑量子比特即使底层物理噪声是马尔可夫的，也可能表现出非马尔可夫动力学演化。论文定义了适用于逻辑门操作的马尔可夫性条件，并研究了逻辑操作与其物理实现的关系，发现在某些条件下非马尔可夫性会从马尔可夫物理操作中涌现。


<details>
  <summary>Details</summary>
Motivation: 研究量子纠错码编码的逻辑量子比特为何会表现出非马尔可夫动力学演化，即使底层物理噪声是马尔可夫的。理解这种涌现的非马尔可夫性对于早期容错量子设备的可靠表征至关重要。

Method: 定义适用于逻辑门操作的马尔可夫性条件，将逻辑操作与其物理实现（对编码逻辑量子比特的数据量子比特的操作）联系起来。应用于小型量子纠错码，分析简单物理噪声模型下的动力学行为。

Result: 研究表明，即使对于非常简单的物理噪声模型，量子纠错码也会表现出非马尔可夫动力学。非马尔可夫性可以从马尔可夫物理操作中涌现，当且仅当物理量子比特在每一轮量子纠错后不一定返回到码子空间时。在这种情况下，综合征量子比特可以充当记忆，介导时间相关性并导致马尔可夫条件的违反。

Conclusion: 量子纠错码编码的逻辑量子比特可以表现出涌现的非马尔可夫性，即使底层物理噪声是马尔可夫的。这种非马尔可夫性源于量子纠错过程中综合征量子比特作为记忆的作用。论文量化了简单示例中的涌现非马尔可夫性，并提出了在早期容错量子设备中可靠使用基于门的表征技术（如门集层析）的充分条件。

Abstract: Logical qubits encoded in quantum error correcting codes can exhibit non-Markovian dynamical evolution, even when the underlying physical noise is Markovian. To understand this emergent non-Markovianity, we define a Markovianity condition appropriate to logical gate operations, and study it by relating logical operations to their physical implementation (operations on the data qubits into which the logical qubit is encoded). We apply our analysis to small quantum codes, and show that they exhibit non-Markovian dynamics even for very simple physical noise models. We show that non-Markovianity can emerge from Markovian physical operations if (and only if) the physical qubits are not necessarily returned to the code subspace after every round of QEC. In this situation, the syndrome qubits can act as a memory, mediating time correlations and enabling violation of the Markov condition. We quantify the emergent non-Markovianity in simple examples, and propose sufficient conditions for reliable use of gate-based characterization techniques like gate set tomography in early fault-tolerant quantum devices.

</details>


### [147] [Deterministic randomness extraction for semi-device-independent quantum random number generation](https://arxiv.org/abs/2512.08900)
*Pablo Tikas Pueyo,Tomás Fernández Martos,Gabriel Senno*

Main category: quant-ph

TL;DR: 该论文将确定性随机数提取器从设备无关场景扩展到准备-测量场景，在量子态重叠假设下实现了半设备无关的随机数生成。


<details>
  <summary>Details</summary>
Motivation: 经典信息论中，确定性过程无法从任意熵源提取接近理想的随机性，但如果有额外知识（如独立伯努利试验序列）则可以实现。量子熵源中，Foreman和Masanes的工作在设备无关场景下实现了确定性提取器，本文旨在将其扩展到准备-测量场景。

Method: 将Foreman和Masanes的确定性提取器构造扩展到准备-测量场景，证明在量子态重叠假设下，这些函数对于半设备无关设置中的无记忆设备也是提取器。然后在新型实验相关行为族上模拟随机数生成协议。

Result: 成功将提取器扩展到准备-测量场景，在量子态重叠假设下实现了半设备无关的确定性随机数提取。模拟显示在7×10³轮次下即可获得正密钥率。

Conclusion: 该工作扩展了确定性随机数提取器到准备-测量场景，为半设备无关的量子随机数生成提供了理论框架，并在实验相关场景中验证了可行性。

Abstract: It is a well-known fact in classical information theory that no deterministic procedure can extract close-to-ideal randomness from an arbitrary entropy source. On the other hand, if additional knowledge about the source is available -- e.g., that it is a sequence of independent Bernoulli trials -- then deterministic extractors do exist. For quantum entropy sources, where in addition to classical random variables we consider quantum side information, the use of extra knowledge about their structure was pioneered in a recent publication [C. Foreman and L. Masanes, Quantum 9, 1654 (2025)]. In that work, the authors provide deterministic extractors for device-independent randomness generation with memoryless devices achieving a sufficiently high CHSH score. In this work, we extend their construction to the prepare-and-measure scenario. Specifically, we prove that the considered functions are also extractors for memoryless devices in a semi-device-independent setting under an overlap assumption on the prepared quantum states. We then simulate the resulting randomness generation protocol on a novel and experimentally relevant family of behaviors, observing positive key rates already for $7\times 10^3$ rounds.

</details>


### [148] [Autonomous multi-ion optical clock with on-chip integrated photonic light delivery](https://arxiv.org/abs/2512.08921)
*Tharon D. Morrison,Joonhyuk Kwon,Matthew A. Delaney,David R. Leibrandt,Daniel Stick,Hayden J. McGuinness*

Main category: quant-ph

TL;DR: 研究人员开发了一种基于片上波导的自主运行光学时钟，使用四个镱离子在室温表面电极阱中实现，短期频率不稳定性达到3.14(5)×10⁻¹⁴/√τ，展示了全系统集成和自动化操作能力。


<details>
  <summary>Details</summary>
Motivation: 集成光子学在囚禁离子系统中对于便携式光学原子钟和可扩展量子计算机等应用至关重要，但系统级集成所有必需功能仍然是一个关键挑战。

Method: 使用四个镱离子在室温多站点表面电极阱中，通过片上波导提供所有时钟操作所需的光，采用自动离子穿梭和重载机制来缓解离子损失。

Result: 实现了短期频率不稳定性为3.14(5)×10⁻¹⁴/√τ的自主运行光学时钟，展示了系统在持续自主操作中的鲁棒性。

Conclusion: 这项工作超越了组件级功能，为下一代便携式多离子量子传感器和计算机建立了可行且稳健的架构。

Abstract: Integrated photonics in trapped-ion systems are critical for the realization of applications such as portable optical atomic clocks and scalable quantum computers. However, system-level integration of all required functionalities remains a key challenge. In this work, we demonstrate an autonomously operating optical clock having a short-term frequency instability of $3.14(5)\times 10^{-14} / \sqrtτ$ using an ensemble of four \ybion ions trapped in a multi-site surface-electrode trap at room temperature. All clock operations are performed with light delivered via on-chip waveguides. We showcase the system's resilience through sustained, autonomous operation featuring automated ion shuttling and reloading to mitigate ion loss during interleaved clock measurements. This work paves the way beyond component-level functionality to establish a viable and robust architecture for the next generation of portable, multi-ion quantum sensors and computers.

</details>
