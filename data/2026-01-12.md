<div id=toc></div>

# Table of Contents

- [quant-ph](#quant-ph) [Total: 25]
- [cs.AI](#cs.AI) [Total: 24]
- [cs.LG](#cs.LG) [Total: 38]
- [nlin.CD](#nlin.CD) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 8]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 3]


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [1] [Investigation of Hardware Architecture Effects on Quantum Algorithm Performance: A Comparative Hardware Study](https://arxiv.org/abs/2601.05286)
*Askar Oralkhan,Temirlan Zhaxalykov*

Main category: quant-ph

TL;DR: 该研究系统性地在亚马逊Braket平台上对五种代表性量子算法（贝尔态制备、GHZ态生成、量子傅里叶变换、Grover搜索、QAOA）在囚禁离子、超导和模拟器后端进行基准测试，发现算法性能强烈依赖于硬件拓扑结构和噪声特性。


<details>
  <summary>Details</summary>
Motivation: 由于量子处理器在量子比特连接性、门保真度和相干时间等方面的架构差异，相同的量子电路在不同设备上可能表现出显著不同的行为，需要系统性地评估算法在不同硬件平台上的性能。

Method: 使用亚马逊Braket平台，在囚禁离子、超导和模拟器后端对五种代表性量子算法进行基准测试，评估保真度、CHSH违反、成功概率、电路深度和门计数等性能指标。

Result: 算法性能强烈依赖于硬件拓扑和噪声特性：10量子比特GHZ态在囚禁离子硬件上保真度超过0.8，而在超导平台上由于路由开销和累积的双量子比特门误差降至0.15以下。

Conclusion: 研究结果强调了硬件感知算法选择的重要性，为NISQ时代的基准测试提供了实用指导，表明需要根据具体硬件特性优化量子算法实现。

Abstract: Cloud-accessible quantum processors enable direct execution of quantum algorithms on heterogeneous hardware platforms. Unlike classical systems, however, identical quantum circuits may exhibit substantially different behavior across devices due to architectural variations in qubit connectivity, gate fidelity, and coherence times.
  In this work, we systematically benchmark five representative quantum algorithms - Bell state preparation, GHZ state generation, Quantum Fourier Transform (QFT), Grover's Search, and the Quantum Approximate Optimization Algorithm (QAOA) - across trapped-ion, superconducting, and simulator backends using Amazon Braket. Performance metrics including fidelity, CHSH violation, success probability, circuit depth, and gate counts are evaluated.
  Our results demonstrate a strong dependence of algorithmic performance on hardware topology and noise characteristics. For example, 10-qubit GHZ states achieved fidelities above 0.8 on trapped-ion hardware, while superconducting platforms dropped below 0.15 due to routing overhead and accumulated two-qubit gate errors. These findings highlight the importance of hardware-aware algorithm selection and provide practical guidance for benchmarking in the NISQ era.

</details>


### [2] [Emergence of the 2nd Law in an Exactly Solvable Model of a Quantum Wire](https://arxiv.org/abs/2601.05514)
*Marco A. Jimenez-Valencia,Charles A. Stafford*

Main category: quant-ph

TL;DR: 该论文通过可精确求解的量子线模型研究热力学第二定律的实现，发现焦耳加热导致的熵产生并非自动出现，而是需要大量局部测量或非弹性散射过程才能实现。


<details>
  <summary>Details</summary>
Motivation: 研究热力学第二定律在微观精确描述下的实现机制，特别是焦耳加热过程中的熵产生问题。玻尔兹曼曾指出，热力学第二定律虽然可以用基本统计论证证明，但在系统微观描述越精确时越难验证。

Method: 使用可精确求解的量子线模型，考虑独立量子粒子系统的熵流精确公式，分析在幺正量子演化下守恒的熵流。通过沿导线长度的一系列浮动热电探针进行大量局部测量，这些探针通过连续测量系统获得信息并将熵注入系统。

Result: 在精确的微观量子动力学描述中，焦耳加热导致的熵产生不会自动出现。只有在大量局部测量的极限下，通过测量引入的非弹性过程导致的退相干，才能实现预期的熵产生。这种机制在实际相互作用粒子系统中会通过非弹性散射自然产生。

Conclusion: 焦耳加热导致的熵产生依赖于测量或非弹性散射过程引入的退相干效应，这为热力学第二定律在精确微观描述下的实现提供了机制解释，并连接了量子测量理论与实际物理系统中的非弹性过程。

Abstract: As remarked by Boltzmann, the Second Law of Thermodynamics is notable for the fact that it is readily proved using elementary statistical arguments, but becomes harder and harder to verify the more precise the microscopic description of a system. In this article, we investigate one particular realization of the 2nd Law, namely Joule heating in a wire under electrical bias. We analyze the production of entropy in an exactly solvable model of a quantum wire wherein the conserved flow of entropy under unitary quantum evolution is taken into account using an exact formula for the entropy current of a system of independent quantum particles. In this exact microscopic description of the quantum dynamics, the entropy production due to Joule heating does not arise automatically. Instead, we show that the expected entropy production is realized in the limit of a large number of local measurements by a series of floating thermoelectric probes along the length of the wire, which inject entropy into the system as a result of the information obtained via their continuous measurements of the system. The decoherence resulting from inelastic processes introduced by the local measurements is essential to the phenomenon of entropy production due to Joule heating, and would be expected to arise due to inelastic scattering in real systems of interacting particles.

</details>


### [3] [Temporal Kirkwood-Dirac Quasiprobability Distribution and Unification of Temporal State Formalisms through Temporal Bloch Tomography](https://arxiv.org/abs/2601.05294)
*Zhian Jia,Kavan Modi,Dagomir Kaszlikowski*

Main category: quant-ph

TL;DR: 该论文将Kirkwood-Dirac准概率分布扩展到多时间量子过程和时空设置，建立了统一的时序量子态操作基础


<details>
  <summary>Details</summary>
Motivation: 尽管时序量子态形式主义不断发展，但其精确的操作关系和概念区别仍不清楚，需要建立统一的操作基础

Method: 将Kirkwood-Dirac准概率分布扩展到任意多时间量子过程和一般时空设置，定义左、右和双倍时序KD准概率及其实部（时序Margenau-Hill准概率）

Result: 这些量可以通过干涉测量方案实验获取，通过表征其非经典特征，广义KD框架为广泛的时序态方法提供了统一的操作基础，可通过时序或时空Bloch层析直接实现

Conclusion: 广义KD框架解决了时序量子态形式主义的操作关系问题，为统一处理具有时间相关和空间相关的量子系统提供了理论基础和实验实现方案

Abstract: Temporal quantum states generalize the multipartite density operator formalism to the time domain, enabling a unified treatment of quantum systems with both timelike and spacelike correlations. Despite a growing body of temporal state formalisms, their precise operational relationships and conceptual distinctions remain unclear. In this work, we resolve this issue by extending the Kirkwood-Dirac (KD) quasiprobability distribution to arbitrary multi-time quantum processes and, more broadly, to general spatiotemporal settings. We define left, right, and doubled temporal KD quasiprobabilities, together with their real components, which we identify as temporal Margenau-Hill (MH) quasiprobabilities. All of these quantities are experimentally accessible through interferometric measurement schemes. By characterizing their nonclassical features, we show that the generalized KD framework provides a unified operational foundation for a wide class of temporal state approaches and can be directly implemented via temporal or spatiotemporal Bloch tomography.

</details>


### [4] [Chaos, thermalization and breakdown of quantum-classical correspondence in a collective many-body system](https://arxiv.org/abs/2601.05627)
*Ángel L. Corps,Sebastián Gómez,Pavel Stránský,Armando Relaño,Pavel Cejnar*

Main category: quant-ph

TL;DR: 研究全连接Bose-Hubbard模型（四格点情况）的热化和量子-经典对应关系，发现三个动力学区域：低能对称破缺态、量子与经典平衡态显著不同的中间区域、以及高能对应恢复区域。


<details>
  <summary>Details</summary>
Motivation: 研究集体多体动力学中的量子-经典对应关系，探索激发态量子相变如何影响热化过程，以及有限尺寸效应在多大程度上偏离经典极限。

Method: 分析全连接Bose-Hubbard模型（四格点情况）的经典相空间结构和激发态量子相变，比较量子动力学与经典动力学的差异，特别关注对称破缺和平衡态性质。

Result: 发现三个不同的动力学区域：1）低能对称破缺态；2）量子与经典平衡态显著不同的中间区域；3）高能对应恢复区域。观察到经典间歇性与量子动力学之间的不匹配，量子系统即使存在经典连通相空间仍被困在对称破缺扇区中。

Conclusion: 量子-经典对应的收敛速度比预期慢得多，表明集体多体动力学中存在鲁棒的有限尺寸效应，这种不匹配源于不平衡本征态的占据，即使对于相对较大的粒子数也持续存在。

Abstract: We investigate thermalization and the quantum-classical correspondence in the fully-connected Bose-Hubbard model, focusing on the four-site case. Our analysis of the classical phase-space structure and its excited-state quantum phase transitions leads us to three dynamical regimes: symmetry-breaking low-energy states, an intermediate region where quantum and classical equilibrium states markedly disagree, and a high-energy regime with restored correspondence. The observed classical intermittency above the first excited-state quantum phase transition contrasts with quantum dynamics, which remains trapped in symmetry-breaking sectors despite the existence of a classically connected phase space. This mismatch originates from the population of imbalance-carrying eigenstates and persists even for relatively large number of particles. Our results reveal unexpectedly slow convergence to the classical limit, signaling robust finite-size effects in collective many-body dynamics.

</details>


### [5] [Fundamental Limitations on the Reliabilities of Power and Work in Quantum Batteries](https://arxiv.org/abs/2601.05315)
*Brij Mohan,Tanmoy Pandit,Maciej Lewenstein,Manabendra Nath Bera*

Main category: quant-ph

TL;DR: 量子电池的可靠性受限于噪声信号比，存在工作与功率波动之间的基本权衡，集体充电虽能提高功率但降低可靠性，混合充电方案可实现功率与可靠性的平衡。


<details>
  <summary>Details</summary>
Motivation: 量子电池作为量子技术中的能量存储设备，其实际应用价值不仅取决于充电/放电功率，还依赖于可靠性（通过噪声信号比NSR量化）。目前缺乏对量子电池可靠性基本限制的系统研究。

Method: 建立工作与功率NSR的普遍下界函数，揭示量子不确定性关系导致的工作与功率波动之间的基本权衡。分析并行（局域）、集体（完全非局域）和混合（半局域）充电方案下的权衡关系与标度行为，并研究横向伊辛类相互作用下的充电过程。

Result: 发现工作与功率NSR均受充电速度函数的下界限制；量子不确定性关系禁止同时抑制工作与功率波动；集体充电通过更强纠缠提高功率，但以降低功率可靠性为代价；混合充电方案在功率与可靠性间取得最佳平衡。

Conclusion: 实现高功率与高可靠性需要既不采用并行也不采用集体充电，而是采用具有中等相互作用范围的混合充电方案。这一分析为设计实用高效、可靠的量子电池提供了理论指导。

Abstract: Quantum batteries, microscopic devices designed to address energy demands in quantum technologies, promise high power during charging and discharging processes. Yet their practical usefulness and performance depend critically on reliability, quantified by the noise-to-signal ratios (NSRs), i.e., normalized fluctuations of work and power, where reliability decreases inversely with increasing NSR. We establish fundamental limits to this reliability: both work and power NSRs are universally bounded from below by a function of charging speed, imposing a reliability limit inherent to any quantum battery. More strikingly, we find that a quantum mechanical uncertainty relation forbids the simultaneous suppression of work and power fluctuations, revealing a fundamental trade-off that also limits the reliability of quantum batteries. We analyze the trade-off and limits, as well as their scaling behavior, across parallel (local), collective {(fully non-local)}, and hybrid (semi-local) charging schemes for many-body quantum batteries, finding that increasing power by exploiting stronger entanglement comes at the cost of diminished reliability of power. Similar trends are also observed in the charging of quantum batteries utilizing transverse Ising-like interactions. These suggest that achieving both high power and reliability require neither parallel nor collective charging, but a hybrid charging scheme with an intermediate range of interactions. Therefore, our analysis shapes the practical and efficient design of reliable and high-performance quantum batteries.

</details>


### [6] [From compatibility of measurements to exploring Quantum Darwinism on NISQ](https://arxiv.org/abs/2601.05350)
*Emery Doucet,Sebastian Deffner*

Main category: quant-ph

TL;DR: 该研究探讨了量子达尔文主义在特定模型中的破坏如何导致非经典测量统计，并利用这一现象为NISQ硬件提供量子特性基准测试工具


<details>
  <summary>Details</summary>
Motivation: 量子达尔文主义解释了经典现实的客观性和可重复性如何在量子世界中涌现。本研究旨在探索当量子达尔文主义被破坏时，测量统计如何表现出非经典特性，并将这一理论应用于实际量子计算平台的基准测试

Method: 研究特定模型中量子达尔文主义的破坏机制，分析这种破坏如何转化为非经典测量统计。然后将这一理论框架应用于实际量子硬件，使用IonQ的囚禁离子和IBM的超导量子计算平台进行实验验证

Result: 研究发现量子达尔文主义的破坏确实导致非经典测量统计，这一现象为评估NISQ硬件的真正量子特性提供了有效工具。通过在IonQ和IBM量子平台上进行实验，成功演示了如何利用这一框架进行量子特性基准测试

Conclusion: 量子达尔文主义的破坏与测量统计的非经典特性之间存在直接联系，这一关系为量子计算硬件的基准测试提供了新的理论框架和实用工具，有助于评估NISQ设备的量子性能

Abstract: Quantum Darwinism explains how tenets of classical reality, such as objectivity and repeatability, emerge within a quantum universe. As a mathematical framework, Quantum Darwinism also provides guiding principles that determine what physical models support emergent classical behavior, what specific observables obey classical laws, and much more. For instance, in a recent work we elucidated that the limit under which Kirkwood-Dirac quasiprobability distributions become effectively classical coincides with the regime where the underlying physical model obeys the rules of Quantum Darwinism. In the present work, we study the breaking of Quantum Darwinism in a specific model and how that translates to non-classical measurement statistics. Interestingly, this provides effective tools for benchmarking the genuine quantum characteristics of NISQ hardware, which we demonstrate with IonQ's trapped-ion and IBM's superconducting quantum computing platforms.

</details>


### [7] [Analytical Solutions to Asymmetric Two-Photon Rabi Model](https://arxiv.org/abs/2601.05421)
*M. Baradaran,L. M. Nieto,S. Zarrinkamar*

Main category: quant-ph

TL;DR: 在Segal-Bargmann表示中研究包含双光子和非对称项的广义Rabi模型，通过变换和Bethe ansatz方法获得近精确解，并给出四阶问题的精确解析解


<details>
  <summary>Details</summary>
Motivation: 研究包含双光子和非对称项的广义Rabi模型，这类模型在量子光学和量子信息处理中具有重要应用，需要寻找精确或近精确的解析解

Method: 采用Segal-Bargmann表示，通过适当的变换将问题简化，然后应用Bethe ansatz方法处理所得的微分方程，特别关注其亚纯结构

Result: 获得了广义Rabi模型的近精确解，并给出了四阶问题的精确解析解，包括任意状态下的解和参数间的约束关系

Conclusion: 通过Segal-Bargmann表示和Bethe ansatz方法，成功解决了包含双光子和非对称项的广义Rabi模型，为这类复杂量子系统的精确求解提供了有效途径

Abstract: Within the Segal-Bargmann representation, a generalized Rabi model is considered that includes both two-photon and asymmetric terms. It is shown that, through a suitable transformation, nearly exact solutions can be obtained using the Bethe ansatz approach. Applying this approach to the meromorphic structure of the resulting differential equation, solutions in exact analytical form of the fourth-order problem are presented for both an arbitrary state and for the restriction between the parameters.

</details>


### [8] [Achieving the Heisenberg limit using fault-tolerant quantum error correction](https://arxiv.org/abs/2601.05457)
*Himanshu Sahu,Qian Xu,Sisi Zhou*

Main category: quant-ph

TL;DR: 该论文研究了在完全噪声环境下的容错量子计量学，提出了一种使用重复码的容错计量协议，证明了存在误差阈值，低于该阈值时可以实现海森堡极限。


<details>
  <summary>Details</summary>
Motivation: 传统量子计量协议通常假设量子纠错操作（包括状态制备和测量）是无噪声的，这在实际应用中不现实。为了克服这一限制，需要研究所有量子比特操作都受噪声影响的容错量子计量学。

Method: 提出了一种容错计量协议：使用重复码通过重复的综合征测量来制备逻辑态，然后进行容错的逻辑测量。研究在比特翻转噪声下估计Pauli-Z信号，同时考虑所有量子纠错操作中的状态制备和测量误差。

Result: 证明了存在误差阈值，当噪声低于该阈值时，误差可以被有效抑制，并且可以实现海森堡极限（量子力学允许的最终极限）。

Conclusion: 该研究将量子计量学扩展到完全噪声环境，为实际量子传感器在噪声环境下的高精度测量提供了理论基础和实用方案。

Abstract: Quantum effect enables enhanced estimation precision in metrology, with the Heisenberg limit (HL) representing the ultimate limit allowed by quantum mechanics. Although the HL is generally unattainable in the presence of noise, quantum error correction (QEC) can recover the HL in various scenarios. A notable example is estimating a Pauli-$Z$ signal under bit-flip noise using the repetition code, which is both optimal for metrology and robust against noise. However, previous protocols often assume noise affects only the signal accumulation step, while the QEC operations -- including state preparation and measurement -- are noiseless. To overcome this limitation, we study fault-tolerant quantum metrology where all qubit operations are subject to noise. We focus on estimating a Pauli-$Z$ signal under bit-flip noise, together with state preparation and measurement errors in all QEC operations. We propose a fault-tolerant metrological protocol where a repetition code is prepared via repeated syndrome measurements, followed by a fault-tolerant logical measurement. We demonstrate the existence of an error threshold, below which errors are effectively suppressed and the HL is attained.

</details>


### [9] [A three-dimensional multimode lumped-element resonator for collective spin manipulation and dispersive readout](https://arxiv.org/abs/2601.05476)
*Zhuo Chen,Wenhua Qin,Hanyu Ren,Ziyi Liu,Kae Nemoto,William John Munro,Yingqiu Mao,Johannes Majer*

Main category: quant-ph

TL;DR: 三维集总元件多模微波谐振器实现宏观自旋系综的均匀集体操控和色散读出


<details>
  <summary>Details</summary>
Motivation: 开发一种能够实现宏观自旋系综均匀集体操控和色散读出的多功能谐振器，用于混合自旋-光子系统和多模固态量子技术

Method: 利用几何对称性设计三维集总元件多模微波谐振器，工程化两个反对称模式，使其空间重叠但串扰抑制，并在不同频率下耦合到同一自旋系综

Result: 在28mK下使用金刚石中的氮空位中心，观察到5.0MHz的集体强耦合，并通过失谐模式实现了非破坏性色散读出

Conclusion: 紧凑设计、可调耦合和高场均匀性使该谐振器成为混合自旋-光子系统和多模固态量子技术的多功能器件

Abstract: We report a three-dimensional lumped-element multimode microwave resonator that enables homogeneous collective manipulation and dispersive readout of a macroscopic spin ensemble. By exploiting geometric symmetry, two antisymmetric modes with strongly suppressed cross-talk are engineered to spatially overlap and couple to the same ensemble at distinct frequencies. Using negatively charged nitrogen-vacancy centers in diamond at 28 mK, we observe collective strong coupling with a coupling strength of 5.0 MHz and demonstrate non-destructive dispersive readout via a detuned mode. The compact design, tunable coupling, and high field homogeneity make this resonator a versatile device for hybrid spin-photon systems and multimode solid-state quantum technologies.

</details>


### [10] [Bath-free squeezed phonon lasing via intrinsic ion-phonon coupling](https://arxiv.org/abs/2601.05575)
*Chen-Yu Lee,Guin-Dar Lin*

Main category: quant-ph

TL;DR: 该论文提出了一种在囚禁离子系统中实现压缩激光的理论模型，无需依赖工程化浴或定制耗散库，而是利用离子-声子相互作用和红/蓝边带跃迁直接产生压缩运动态。


<details>
  <summary>Details</summary>
Motivation: 传统实现压缩激光的方法通常需要复杂的工程化浴或定制耗散库，这增加了实验难度和系统复杂性。本文旨在探索一种更简单直接的方法，利用囚禁离子系统的固有特性来实现压缩激光，为量子计量学和信息处理提供新途径。

Method: 采用两个囚禁离子与共享振动模式相互作用的系统模型，通过在红边带和蓝边带跃迁上同时驱动离子，利用离子-声子相互作用直接产生压缩态。通过分析系统的稳态行为、激光阈值、增益-损耗平衡以及压缩参数对声子场统计特性的影响，展示了如何通过外部相干驱动稳定相位相干性并实现可控正交压缩。

Result: 理论模型表明，无需外部耗散库即可实现压缩激光，系统能够产生压缩运动态。通过分析稳态行为，确定了激光阈值条件，展示了压缩参数如何塑造声子场的统计特性，并证明了外部相干驱动可以有效稳定相位相干性，实现可控的正交压缩。

Conclusion: 该研究提供了一种在囚禁离子系统中实现压缩激光的简化方法，无需复杂的浴工程，为声子基系统中的压缩态实现提供了新见解，在量子计量学和信息处理领域具有潜在应用价值。

Abstract: We present a theoretical model for realizing squeezed lasing in a trapped-ion system without relying on engineered baths or tailored dissipative reservoirs. Our approach leverages the intrinsic ion-phonon interactions, where two trapped ions, each interacting with a shared vibrational mode, are driven on both red- and blue-sideband transitions. This enables the creation of a squeezed state of motion through the dynamic coupling between the ions' internal states and the phonon mode. Unlike traditional methods that require bath engineering, our model demonstrates that squeezed lasing can be achieved through a direct manipulation of ion-phonon interactions, with no external reservoirs required. We explore the steady-state behavior of the system, analyzing the onset of lasing, gain-loss balance, and the role of the squeezing parameter in shaping the phonon field's statistical properties. Furthermore, we show how external coherent drives can stabilize phase coherence and achieve controlled quadrature squeezing, offering a simple yet effective method for achieving squeezed lasing in quantum mechanical systems. Our findings provide new insights into the realization of squeezed states in phonon-based systems, with potential applications in quantum metrology and information processing.

</details>


### [11] [Squeezing-Enhanced Two-Phase Estimation with N-Particle W-type States](https://arxiv.org/abs/2601.05595)
*Huan Zhang,Ying Xia,Xiuxing Zhang,Shoukang Chang,Wei Ye*

Main category: quant-ph

TL;DR: 本文研究了在三模干涉仪中利用光学参量放大（OPA）同时估计两个光学相位的量子计量学问题，分析了OPA对多参数相位估计精度的增强效果及其物理机制。


<details>
  <summary>Details</summary>
Motivation: 研究光学参量放大在多参数量子计量中的应用，探索如何通过OPA增强三模干涉仪中同时估计两个光学相位的精度，并理解其物理机制。

Method: 采用正规序特征函数形式，解析获得输出量子态的所有光子数矩，从而显式评估多参数相位估计的量子Fisher信息矩阵。在无损耗情况下分析OPA的增强效果，并通过二阶关联函数分析物理机制。对于有损耗的实际干涉仪，采用基于纯化的变分方法进行分析。

Result: 在无损耗情况下，均匀应用的OPA显著提高了可达到的精度，超越了未放大的干涉仪。通过二阶关联函数分析表明，这种增强源于模内光子数关联的放大，而非模间关联。在存在光子损耗的实际情况下，虽然损耗降低了可达到的精度，但OPA辅助方案在中等损耗下仍保持明显优势，显示出对耗散的一定鲁棒性。

Conclusion: 研究结果阐明了OPA增强多参数量子计量的物理机制，并为在实际噪声环境中优化相位估计协议提供了指导。OPA通过放大模内光子数关联来增强多参数估计精度，即使在存在损耗的情况下仍能保持优势。

Abstract: We investigate the simultaneous estimation of two optical phases in a three-mode interferometer assisted by optical parametric amplification (OPA). By employing the normally ordered characteristic-function formalism, we analytically obtain all photon-number moments of the output quantum state, enabling an explicit evaluation of the quantum Fisher information matrix for multiparameter phase estimation. In the lossless scenario, we show that uniformly applied OPA significantly enhances the attainable precision beyond that of an unamplified interferometer. By analyzing the second-order correlation functions, we demonstrate that this enhancement originates from the amplification of intra-mode photon-number correlations, rather than from inter-mode correlations. We further extend our analysis to realistic interferometers with photon loss using a purification-based variational approach. Although loss degrades the achievable precision, the OPA-assisted scheme retains a clear advantage for moderate loss, indicating a degree of robustness against dissipation. Our results clarify the physical mechanism underlying OPA-enhanced multiparameter quantum metrology and provide guidelines for optimizing phase estimation protocols in realistic noisy environments.

</details>


### [12] [Improving quantum interference visibility between independent sources by enhancing the purity of correlated photon pairs](https://arxiv.org/abs/2601.05671)
*Hsin-Pin Lo,Kai Asaoka,Hiroki Takesue*

Main category: quant-ph

TL;DR: 通过系统调节泵浦带宽和干涉滤波器带宽两种方法，提高PPLN波导产生的光子对纯度，使HOM干涉可见度达80%，其中泵浦带宽调节方法还获得更高的三重合计数率。


<details>
  <summary>Details</summary>
Motivation: 高可见度的独立光子间量子干涉对于多光子量子信息处理至关重要，这与相关光子对的频谱纯度密切相关。需要提高光子对纯度以实现更好的量子干涉效果。

Method: 研究两种提高type-0 PPLN波导产生光子对纯度的方法：1）系统调节泵浦带宽；2）系统调节干涉滤波器带宽。在相同实验条件下直接比较两种方法的性能，通过施密特分解从测量的联合频谱强度评估频谱纯度。

Result: 两种方法都显著提高了Hong-Ou-Mandel干涉可见度至约80%。但泵浦带宽调节方法还获得了更高的三重合计数率，这对于提高多光子时间bin GHZ态的态保真度和产生率更为有利。

Conclusion: 通过调节泵浦带宽和干涉滤波器带宽都能有效提高光子对纯度，但泵浦带宽调节方法在获得高干涉可见度的同时还能保持更高的三重合计数率，对多光子量子信息处理应用更具优势。

Abstract: High-visibility quantum interference between independent photons is essential for demonstrating multi-photon quantum information processing, and it is closely linked to the spectral purity of correlated photon pairs. In this study, we investigate two approaches to enhance the purity of photon pairs generated from a type-0 PPLN waveguide by systematically varying both the pump bandwidth and the interference-filter bandwidth, and we directly compare their performance under identical experimental conditions. The spectral purity is evaluated from measured joint spectral intensities using Schmidt decomposition. Both methods significantly improve the Hong-Ou-Mandel interference visibility to approximately 80%. However, the former approach also yields a higher three-fold coincidence rate, which is advantageous for our ongoing efforts to increase the state fidelity and generation rate of multi-photon time-bin Greenberger-Horne-Zeilinger (GHZ) states.

</details>


### [13] [Block Encoding Linear Combinations of Pauli Strings Using the Stabilizer Formalism](https://arxiv.org/abs/2601.05740)
*Niclas Schillo,Andreas Sturm,Rüdiger Quay*

Main category: quant-ph

TL;DR: 提出一种新的量子电路构造方法，用于块编码Pauli字符串的线性组合，相比传统LCU方法具有可比或更优的电路复杂度


<details>
  <summary>Details</summary>
Motivation: 量子奇异值变换(QSVT)框架为实现量子加速提供了强大工具，但其核心输入模型是块编码框架。由于块编码子程序的量子门复杂度很大程度上决定了QSVT算法的总体成本，开发新的、更高效的块编码方法对于实现实际量子优势至关重要

Method: 提出一种新方法构造量子电路来块编码Pauli字符串的线性组合。该方法包含两个关键组件：1) 应用变换将Pauli字符串转换为成对反交换的字符串，使变换后的线性组合成为酉矩阵，从而可直接实现为量子电路；2) 采用基于稳定子形式的校正变换，使用辅助寄存器恢复原始Pauli字符串。该方法可使用对数规模于系统量子位数量的辅助寄存器实现，也可扩展到更大的辅助寄存器以显著降低总体电路复杂度

Result: 提出了四个具体示例，并通过数值模拟将新方法的电路复杂度与线性组合酉矩阵(LCU)方法进行比较。发现新方法达到与LCU可比或更优的电路复杂度，当目标算子的结构可被利用时可能具有优势

Conclusion: 该方法可为超出本文分析示例的一系列相关问题实现更高效的块编码，有望推动QSVT框架在实际量子计算中的应用

Abstract: The Quantum Singular Value Transformation (QSVT) provides a powerful framework with the potential for quantum speedups across a wide range of applications. Its core input model is the block encoding framework, in which non-unitary matrices are embedded into larger unitary matrices. Because the gate complexity of the block-encoding subroutine largely determines the overall cost of QSVT-based algorithms, developing new and more efficient block encodings is crucial for achieving practical quantum advantage. In this paper, we introduce a novel method for constructing quantum circuits that block encode linear combinations of Pauli strings. Our approach relies on two key components. First, we apply a transformation that converts the Pauli strings into pairwise anti-commuting ones, making the transformed linear combination unitary and thus directly implementable as a quantum circuit. Second, we employ a correction transformation based on the stabilizer formalism which uses an ancilla register to restore the original Pauli strings. Our method can be implemented with an ancilla register whose size scales logarithmically with the number of system qubits. It can also be extended to larger ancilla registers, which can substantially reduce the overall quantum circuit complexity. We present four concrete examples and use numerical simulations to compare our method's circuit complexity with that of the Linear Combination of Unitaries (LCU) approach. We find that our method achieves circuit complexities comparable to or better than LCU, with possible advantages when the structure of the target operators can be exploited. These results suggest that our approach could enable more efficient block encodings for a range of relevant problems extending beyond the examples analyzed in this work.

</details>


### [14] [Quantum Interference-Induced Bhattacharyya Distance](https://arxiv.org/abs/2601.05749)
*Mostafizur Rahaman Laskar*

Main category: quant-ph

TL;DR: 提出一种基于量子干涉脆弱性的量子距离度量QIBD，通过单辅助比特干涉电路实现，能捕捉经典Bhattacharyya距离无法检测的相关结构信息。


<details>
  <summary>Details</summary>
Motivation: 现有基于保真度的量子距离度量无法充分捕捉量子态中概率分布的相关结构信息，需要一种能反映量子干涉对纠缠演化敏感性的距离度量。

Method: 设计单辅助比特干涉电路，通过相互作用哈密顿量产生相关依赖的相位来调制干涉可见度，定义量子干涉诱导的Bhattacharyya距离(QIBD)。

Result: 数值模拟表明QIBD对相关结构的响应方式与基于重叠的度量不同，当相互作用消失时QIBD退化为经典Bhattacharyya距离，但对纠缠相互作用无法仅用保真度表示。

Conclusion: QIBD作为一种新的量子距离度量，在相互作用对齐相关的物理场景中具有潜在应用价值，能提供传统度量无法捕捉的相关结构信息。

Abstract: We propose a quantum distance measure between probability distributions encoded in quantum states based on the fragility of quantum interference under entangling evolution. The Quantum Interference-Induced Bhattacharyya Distance (QIBD) is defined through a single-ancilla interferometric circuit in which an interaction Hamiltonian generates correlation-dependent phases that modulate interference visibility. When the interaction vanishes, QIBD reduces to the classical Bhattacharyya distance; however, for entangling interactions, it cannot be expressed as a function of fidelity alone. Numerical simulations demonstrate that QIBD responds to correlation structure in ways that overlap-based measures do not, suggesting potential utility in contexts where interaction-aligned correlations are physically relevant.

</details>


### [15] [Hidden time-nonlocal Floquet symmetries](https://arxiv.org/abs/2601.05783)
*Sigmund Kohler,Jesús Casado-Pascual*

Main category: quant-ph

TL;DR: 研究失谐驱动二能级系统的Floquet谱，发现当失谐是驱动场能量量子的整数倍时会出现精确的准能交叉，这可以用隐藏的时间非定域宇称对称性解释。


<details>
  <summary>Details</summary>
Motivation: 研究失谐驱动二能级系统的Floquet谱特性，探索准能交叉现象及其背后的对称性原理。

Method: 通过隐藏的时间非定域宇称对称性对Floquet模式进行分类，基于标量递推关系构造对称性存在的证明，并开发了适用于超越二能级系统的数值计算方法。

Result: 当失谐是驱动场能量量子的整数倍时，系统出现精确的准能交叉，不同宇称的准能之间形成交叉点，并通过数值数据验证了理论分析。

Conclusion: 失谐驱动二能级系统的Floquet谱中存在隐藏的时间非定域宇称对称性，这解释了准能交叉现象，并且该分析方法可推广到更复杂的系统。

Abstract: We investigate the Floquet spectrum of a detuned, driven two-level system and show that it exhibits exact quasienergy crossings when the detuning is an integer multiple of the energy quantum of the driving field. This behavior can be explained by a hidden time-nonlocal parity, which allows the Floquet modes to be classified as even or odd. Then a generic feature is the emergence of exact crossings between quasienergies of different parity. A constructive proof of the existence of the symmetry is based on a scalar recurrence relation. Moreover, we present a general scheme for its numerical computation, which can be applied to models beyond the two-level system. Analytical results are illustrated with numerical data.

</details>


### [16] [On the robustness of Quantum Phase Estimation to compute ground properties of many-electron systems](https://arxiv.org/abs/2601.05788)
*Wassil Sennane,Jérémie Messud*

Main category: quant-ph

TL;DR: 该论文分析了量子相位估计算法在电子系统中的应用，研究了时间步长、相位量子比特数、初始态准备等自由参数，提出了参数设置方法，并推导了达到化学精度的条件。


<details>
  <summary>Details</summary>
Motivation: 量子相位估计算法在计算化学和材料科学中的应用需要更深入理解其自由参数，以实现算法的自动化。目前各种参数选择方面仍未被充分探索，缺乏整体的参数选择方法。

Method: 首先回顾QPE的关键特性，然后提出构建性方法设置QPE的自由参数，推导达到化学精度的条件，并分析Trotterized QPE版本的复杂度特性。通过H2分子的数值模拟验证方法。

Result: 推导出了在基态能量估计中达到化学精度的显式条件，证明了使用这些条件时，Trotterized QPE的复杂度仅取决于物理系统特性而非相位量子比特数。H2分子的数值模拟初步验证了该方法。

Conclusion: 该研究为QPE在预测性计算化学和材料科学中的更自动化应用铺平了道路，提供了参数选择的系统方法，并揭示了算法复杂度与系统特性的关系。

Abstract: We propose an analysis of the Quantum Phase Estimation (QPE) algorithm applied to electronic systems by investigating its free parameters such as the time step, number of phase qubits, initial state preparation, number of measurement shots, and parameters related to the unitary operators implementation. A deep understanding of these parameters is crucial to pave the way towards more automation of QPE applied to predictive computational chemistry and material science. To our knowledge, various aspects remain unexplored and a holistic parameter selection method remains to be developed. After reviewing key QPE features, we propose a constructive method to set the QPE free parameters. We derive, among other things, explicit conditions for achieving chemical accuracy in ground energy estimation. We also demonstrate that, using our conditions, the complexity of the Trotterized version of QPE tends to depend only on physical system properties and not on the number of phase qubits. Numerical simulations on the H2 molecule provide a first validation of our approach.

</details>


### [17] [Optimally driving multi-photon transitions in the perturbative single-mode regime](https://arxiv.org/abs/2601.05854)
*Frieder Lindel,Stefan Yoshi Buhmann,Andreas Buchleitner,Edoardo G. Carnio*

Main category: quant-ph

TL;DR: 研究确定在固定强度、窄带光场中，驱动多光子跃迁的最优光场状态，发现经典相干态混合是最优解


<details>
  <summary>Details</summary>
Motivation: 多光子跃迁速率依赖于光场的高阶相干函数，因此可以通过调控光场的相干特性来提高多光子跃迁效率。研究旨在确定在特定条件下驱动多光子跃迁的最优光场状态。

Method: 分析弱固定强度、窄带入射光场，限制最大光子数，针对短寿命原子多能级系统，确定驱动m光子跃迁的最优光场状态。

Result: 研究发现在这种情况下，无需利用光场的量子特性，经典相干态的混合就是最优解。

Conclusion: 对于短寿命原子多能级系统的多光子跃迁，经典光场（相干态混合）已经足够优化，无需量子光场资源。

Abstract: The rate of $m$-photon transitions in matter, induced by an incident light field, depends on the field's $m$th order coherence function. Consequently, the coherence properties of the light field may be shaped to increase the rate of multi-photon transitions. Here, we determine the optimal state of a weak fixed-intensity, narrow-band incident light field, with a restricted maximal photon number, that optimally drives $m$-photon transitions in the case of a short-lived atomic multilevel system. We show that, in this case, no quantum properties of the light field need to be exploited, but that classical mixtures of coherent states are optimal.

</details>


### [18] [Breaking the Exponential: Decoherence-Driven Power-Law Spontaneous Emission in Waveguide Quantum Electrodynamics](https://arxiv.org/abs/2601.05884)
*Stefano Longhi*

Main category: quant-ph

TL;DR: 研究显示，动态退相干会改变波导中二能级系统的自发辐射衰减规律，从指数衰减转变为幂律衰减


<details>
  <summary>Details</summary>
Motivation: 探索在光子波导中，动态退相干如何影响二能级系统的自发辐射衰减规律，揭示退相干诱导的非指数衰减机制

Method: 研究二能级系统与光子波导的耦合，分析无退相干和有动态退相干情况下的衰减动力学，特别关注光子扩散在动态无序环境中的作用

Result: 无退相干时系统呈现常规指数衰减加长时幂律尾；引入退相干后，短时即出现稳健的幂律衰减，由光子扩散而非谱边效应驱动

Conclusion: 动态退相干在波导QED平台中诱导了一种新的非指数自发辐射机制，揭示了退相干对量子系统衰减动力学的深刻影响

Abstract: We investigate the spontaneous emission of a two-level system coupled to a photonic waveguide, showing that dynamical dephasing in the photon modes profoundly alters the decay law. In the absence of dephasing, the emitter displays conventional exponential decay followed by a long-time power-law tail -- observable only at extremely low survival probabilities. Strikingly, when dephasing is introduced, a robust power-law decay emerges already at short times, driven by photon diffusion in the dynamically disordered environment rather than spectral edge effects. These results reveal a novel, decoherence-induced mechanism for non-exponential spontaneous emission in waveguide QED platforms.

</details>


### [19] [Sub-Planck structure quantification in non-Gaussian probability densities](https://arxiv.org/abs/2601.05898)
*Darren W. Moore,Vojtěch Švarc,Kratveer Singh,Artem Kovalenko,Minh Tuan Pham,Ondřej Číp,Lukáš Slodička,Radim Filip*

Main category: quant-ph

TL;DR: 该论文提出了一种识别、量化和比较相空间变量概率密度中亚普朗克结构的通用实验方法，并在单原子机械振子的高阶Fock态上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 亚普朗克结构在玻色子量子系统的相空间变量概率密度中普遍存在，特别是在非线性动力学或非线性测量演化中。然而，目前对这些结构的识别和比较仍停留在定性层面，缺乏定量方法。

Method: 提出了一种通用且实验友好的方法，直接从可测量或可估计的单相空间变量概率密度中识别、量化和比较亚普朗克结构。该方法适用于各种实验场景。

Result: 在单原子机械振子的实验高阶Fock态上验证了该方法，证明随着Fock占据数增加，尽管声子、位置和动量基中的不确定性增加，但亚普朗克结构确实变得更精细。

Conclusion: 该方法为量子系统中普遍存在的亚普朗克结构提供了首个定量分析工具，为量子计量学、量子信息处理和量子基础研究开辟了新途径。

Abstract: Sub-Planck structures in non-Gaussian probability densities of phase space variables are pervasive in bosonic quantum systems. They are almost universally present if the bosonic system evolves via nonlinear dynamics or nonlinear measurements. So far, identification and comparison of such structures remains qualitative. Here we provide a universally applicable and experimentally friendly method to identify, quantify and compare sub-Planck structures from directly measurable or estimated probability densities of single phase space variables. We demonstrate the efficacy of this method on experimental high order Fock states of a single-atom mechanical oscillator, showing provably finer sub-Planck structures as the Fock occupation increases despite the accompanying uncertainty increase in the phonon, position, and momentum bases.

</details>


### [20] [Generation of squeezed optical states via stored classical pulses in a Bose gas](https://arxiv.org/abs/2601.05908)
*Sevilay Sevinçli,Dennis Rätzel,Markus Krutzik,Mehmet Özgür Oktel,Mustafa Gündoğan*

Main category: quant-ph

TL;DR: 提出并分析了一种通过将经典探测脉冲存储在玻色-爱因斯坦凝聚体中，并利用存储期间原子-原子碰撞引起的非线性演化来产生压缩光的方案。


<details>
  <summary>Details</summary>
Motivation: 利用原子系综的非线性相互作用产生光学压缩态，为量子信息处理和精密测量提供高质量压缩光源。

Method: 采用Λ型光学存储器接口将探测脉冲映射到集体自旋波上，通过原子碰撞实现单轴扭曲动力学产生自旋压缩，最后通过单模分束器映射将原子压缩转移到输出光场。

Result: 在考虑实际损耗和有限存储/检索效率的情况下，确定了最优存储时间，预测在现实条件下可将数分贝的压缩转移到输出光中。

Conclusion: 该方案能够有效利用BEC中的原子碰撞非线性产生压缩光，为量子光学应用提供了实用的压缩光源生成方法。

Abstract: We propose and analyze a scheme to generate squeezed light by storing a classical probe pulse in a Bose--Einstein condensate (BEC) and exploiting the nonlinear evolution caused by atom--atom collisions during the storage time. A $Λ$-type optical memory interface maps a chosen temporal probe mode onto a single phase-matched collective spin wave; for a coherent input this prepares a tunable coherent spin state of a two-component BEC, with its initial spin orientation set by the stored mean excitation number and the phase relation between the probe and control fields. Collisional interactions during storage then implement one-axis-twisting dynamics and generate spin squeezing in the atomic ensemble. We account for realistic loss and finite memory and retrieval efficiencies, and model readout as a single-mode beam-splitter mapping that transfers the atomic quadrature squeezing onto a propagating optical mode. We identify optimal storage times and predict that, under realistic conditions, several dB of squeezing can be transferred to the retrieved light.

</details>


### [21] [Universal Dilation of Linear Itô SDEs: Quantum Trajectories and Lindblad Simulation of Second Moments](https://arxiv.org/abs/2601.05928)
*Hsuan-Cheng Wu,Xiantao Li*

Main category: quant-ph

TL;DR: 提出了一个通用框架，用于在量子计算机上模拟N维线性伊藤随机微分方程，通过酉膨胀技术建立SDE与随机薛定谔方程的精确对应关系。


<details>
  <summary>Details</summary>
Motivation: 传统模拟线性随机微分方程需要大量计算资源，特别是对于高维系统。量子计算为高效模拟随机过程提供了新途径，但需要建立经典随机系统与量子系统之间的精确对应关系。

Method: 使用酉膨胀技术，将线性伊藤SDE嵌入到膨胀的希尔伯特空间中，建立与随机薛定谔方程的路径精确对应。开发了两种算法策略：基于轨迹的弱测量方法和基于系综的确定性主方程方法。

Result: 建立了SDE与量子系统的精确对应关系，证明了该框架在数字量子处理器上的可实现性，提供了基于随机光锥分析的误差界限，并通过数值模拟验证了框架的有效性。

Conclusion: 该框架为在量子计算机上模拟线性随机微分方程提供了通用方法，通过精确的量子-经典对应关系，实现了高效的随机过程模拟，为量子计算在随机系统模拟中的应用开辟了新途径。

Abstract: We present a universal framework for simulating $N$-dimensional linear Itô stochastic differential equations (SDEs) on quantum computers with additive or multiplicative noises. Building on a unitary dilation technique, we establish a rigorous correspondence between the general linear SDE \[ dX_t = A(t) X_t\,dt + \sum_{j=1}^J B_j(t)X_t\,dW_t^j \] and a Stochastic Schrödinger Equation (SSE) on a dilated Hilbert space. Crucially, this embedding is pathwise exact: the classical solution is recovered as a projection of the dilated quantum state for each fixed noise realization. We demonstrate that the resulting SSE is {naturally implementable} on digital quantum processors, where the stochastic Wiener increments correspond directly to measurement outcomes of ancillary qubits. Exploiting this physical mapping, we develop two algorithmic strategies: (1) a trajectory-based approach that uses sequential weak measurements to realize efficient stochastic integrators, including a second-order scheme, and (2) an ensemble-based approach that maps moment evolution to a deterministic Lindblad quantum master equation, enabling simulation without Monte Carlo sampling. We provide error bounds based on a stochastic light-cone analysis and validate the framework with numerical simulations.

</details>


### [22] [Below-threshold error reduction in single photons through photon distillation](https://arxiv.org/abs/2601.05947)
*F. H. B. Somhorst,J. Saied,N. Kannan,B. Kassenberg,J. Marshall,M. de Goede,H. J. Snijders,P. Stremoukhov,A. Lukianenko,P. Venderbosch,T. B. Demille,A. Roos,N. Walk,J. Eisert,E. G. Rieffel,D. H. Smith,J. J. Renema*

Main category: quant-ph

TL;DR: 该论文提出并实验验证了光子蒸馏技术，作为一种比量子纠错更高效的相干误差缓解方法，用于减少光量子计算中的不可区分性误差。


<details>
  <summary>Details</summary>
Motivation: 光量子计算机依赖光子的玻色统计特性通过量子干涉构建大纠缠态，但任何路径信息都会降低量子干涉并引入误差。虽然量子纠错理论上可以处理这些误差，但它资源密集、错误阈值低，需要大量高质量光学元件。

Method: 实验演示了可扩展的最优光子蒸馏技术，这是一种内在的玻色相干误差缓解方法。它利用量子干涉将单光子投影到纯化的内部状态，从而以比量子纠错更高的效率和阈值减少不可区分性误差。

Result: 观察到与理论预测一致的无条件误差减少（即低于阈值行为），即使在考虑蒸馏门引入的噪声时也是如此，从而在容错量子计算相关条件下实现了实际的净增益误差缓解。

Conclusion: 光子蒸馏将在大型量子计算机中找到应用，这项工作有望激发寻找更多内在玻色误差减少策略的研究，即使是针对容错架构。

Abstract: Photonic quantum computers use the bosonic statistics of photons to construct, through quantum interference, the large entangled states required for measurement-based quantum computation. Therefore, any which-way information present in the photons will degrade quantum interference and introduce errors. While quantum error correction can address such errors in principle, it is highly resource-intensive and operates with a low error threshold, requiring numerous high-quality optical components. We experimentally demonstrate scalable, optimal photon distillation as a substantially more resource-efficient strategy to reduce indistinguishability errors in a way that is compatible with fault-tolerant operation. Photon distillation is an intrinsically bosonic, coherent error-mitigation technique which exploits quantum interference to project single photons into purified internal states, thereby reducing indistinguishability errors at both a higher efficiency and higher threshold than quantum error correction. We observe unconditional error reduction (i.e., below-threshold behaviour) consistent with theoretical predictions, even when accounting for noise introduced by the distillation gate, thereby achieving actual net-gain error mitigation under conditions relevant for fault-tolerant quantum computing. We anticipate photon distillation will find uses in large-scale quantum computers. We also expect this work to inspire the search for additional intrinsically bosonic error-reduction strategies, even for fault-tolerant architectures.

</details>


### [23] [Continuous-time noise mitigation in analogue quantum simulation](https://arxiv.org/abs/2601.05952)
*Gabriele Bressanini,Yue Ma,Hyukjoon Kwon,M. S. Kim*

Main category: quant-ph

TL;DR: 提出首个完全模拟、实现精确噪声消除的连续时间噪声抑制框架，用于提高模拟量子模拟器的精度


<details>
  <summary>Details</summary>
Motivation: 模拟量子模拟器在探索量子多体动力学方面具有潜力，但易受噪声影响，限制了模拟精度，需要新的噪声抑制方法

Method: 采用完全模拟的连续时间协议，使用少量辅助量子比特与系统相互作用，结合经典后处理联合测量数据，实现精确噪声消除

Result: 建立了首个完全模拟且实现精确噪声消除的协议，具有哈密顿量无关性、对实际辅助量子比特噪声的鲁棒性，并保持系统动力学的连续时间特性

Conclusion: 为在噪声存在下实现高保真模拟量子模拟开辟了新方向

Abstract: Analogue quantum simulators offer a promising route to explore quantum many-body dynamics beyond classical reach in the near term. However, their vulnerability to noise limits the accuracy of simulations. Here, we establish a new framework for mitigating noise in analogue quantum simulation, operating in a time-continuous manner. To our knowledge, this is the first protocol that is fully analogue and that achieves exact noise cancellation. Our method requires a small number of ancillary qubits, whose interaction with the system$-$combined with classical post-processing of joint measurement data$-$is tailored to cancel the effect of noise. Furthermore, the protocol is Hamiltonian-independent, robust to realistic ancilla noise, and avoids any discretization, preserving the continuous-time nature of the system's dynamics. This work opens a new direction for achieving high-fidelity analogue quantum simulation in the presence of noise.

</details>


### [24] [Counterdiabatic ADAPT-VQE for molecular simulation](https://arxiv.org/abs/2601.05973)
*Diego Tancara,Herbert Díaz-Moraga,Dardo Goyeneche*

Main category: quant-ph

TL;DR: 提出了一种结合ADAPT-VQE框架和反绝热驱动的混合方法，用于分子模拟，相比单独使用反绝热算法或ADAPT-VQE，在性能和电路深度方面都有改进。


<details>
  <summary>Details</summary>
Motivation: ADAPT-VQE在NISQ设备上对贫瘠高原具有鲁棒性，而反绝热算法在性能和电路深度方面优于标准绝热方法。结合两者的优势可以提升分子模拟的效果。

Method: 将分子哈密顿量映射到量子比特表示，构建绝热哈密顿量，使用嵌套对易子计算近似绝热规范势，得到的算子项定义算子池，然后应用ADAPT-VQE算法迭代选择最相关的元素构建ansatz。

Result: 与单独使用反绝热算法或ADAPT-VQE相比，该方法在性能和电路深度方面都有改进，支持了两种范式结合在分子模拟中的有效性。

Conclusion: 结合ADAPT-VQE框架和反绝热驱动的混合方法在分子模拟中表现出更好的性能和更低的电路深度，验证了两种量子计算范式结合的优势。

Abstract: Among variational quantum algorithms designed for NISQ devices, ADAPT-VQE stands out for its robustness against barren plateaus, particularly in estimating molecular ground states. On the other hand, counterdiabatic algorithms have shown advantages in both performance and circuit depth when compared to standard adiabatic approaches. In this work, we propose a hybrid method that integrates the ADAPT-VQE framework with counterdiabatic driving within an adiabatic evolution scheme. Specifically, we map the molecular Hamiltonian to a qubit representation and construct an adiabatic Hamiltonian, from which an approximate adiabatic gauge potential is computed using nested commutators. The resulting operator terms define the operator pool, and the ADAPT-VQE algorithm is applied to iteratively select the most relevant elements for the ansatz. Our results demonstrate improvements in performance and reductions in circuit depth compared to using either counterdiabatic algorithms or ADAPT-VQE with fermionic excitation operators, thus supporting the effectiveness of combining both paradigms in molecular simulations.

</details>


### [25] [From Superradiance to Superabsorption: An Exact Treatment of Non-Markovian Cooperative Radiation](https://arxiv.org/abs/2601.05989)
*Ignacio González,Ángel Rivas*

Main category: quant-ph

TL;DR: 研究超越马尔可夫和平均场近似的原子-腔系统，揭示了三种集体辐射动力学相：马尔可夫超辐射爆发、非马尔可夫超吸收和临界脉冲发射，发现环境记忆效应可通过合作性增强，且超辐射峰值强度随系统尺寸增大而退化。


<details>
  <summary>Details</summary>
Motivation: 研究原子系综与损耗谐振腔耦合系统中的合作辐射现象，超越传统的马尔可夫近似和平均场近似，探索非马尔可夫效应对集体动力学的影响。

Method: 针对双发射体情况推导完整解析解，对更大系综（最多10^3个发射体）采用数值精确方法，表征从马尔可夫到非马尔可夫集体动力学的完整转变。

Result: 发现三种不同相：马尔可夫相（标准超辐射爆发）、非马尔可夫相（自发超吸收发射场）、临界相（脉冲集体发射）；临界谱宽随发射体数量单调增加；超辐射峰值强度标度随系统尺寸增大而退化，在完美腔极限下接近亚二次方律。

Conclusion: 环境记忆效应可通过合作性增强，自发超吸收是非马尔可夫合作性的独特表现，为量子光学和量子信息处理中的集体辐射现象提供了新见解。

Abstract: We investigate the emergence of cooperative radiation phenomena in ensembles of two-level atoms coupled to a lossy resonant cavity beyond the Markovian and mean-field approximations. By deriving a complete analytical solution for the two-emitter case and employing a numerically exact method for larger ensembles, we characterize the full transition from Markovian to non-Markovian collective dynamics for systems of up to $10^3$ emitters. Our results reveal three distinct regimes: a Markovian phase exhibiting the standard superradiant burst, a non-Markovian phase featuring spontaneous superabsorption of the emitted field, and a critical regime marked by pulsed collective emission. We show that the critical spectral width separating these behaviors increases monotonically with the number of emitters, demonstrating that environmental memory effects can be enhanced by cooperativity. Finally, we find that the superradiant scaling of the peak intensity progressively degrades with increasing system size, approaching a subquadratic law in the limit of a perfect cavity. In this regime, spontaneous superabsorption emerges as a distinct manifestation of non-Markovian cooperativity.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [26] [Mathematical Knowledge Graph-Driven Framework for Equation-Based Predictive and Reliable Additive Manufacturing](https://arxiv.org/abs/2601.05298)
*Yeongbin Cha,Namjung Kim*

Main category: cs.AI

TL;DR: 本文提出了一种本体引导、方程中心的框架，将大语言模型与增材制造数学知识图谱结合，实现可靠的知识提取和外推建模。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法在增材制造中受到知识表示碎片化和稀疏数据条件下外推不可靠的限制，需要更可靠的知识提取和外推建模方法。

Method: 通过形式化本体编码方程、变量、假设及其语义关系，将非结构化文献转化为机器可解释表示；基于知识图谱子图条件化LLM方程生成，确保物理意义；引入置信感知外推评估，整合外推距离、统计稳定性和物理一致性。

Result: 本体引导提取显著提高了提取知识的结构一致性和定量可靠性；子图条件化方程生成相比无引导LLM输出产生更稳定、物理一致的外推结果。

Conclusion: 建立了本体驱动知识表示、方程中心推理和置信度外推评估的统一流程，展示了知识图谱增强LLM作为增材制造外推建模可靠工具的潜力。

Abstract: Additive manufacturing (AM) relies critically on understanding and extrapolating process-property relationships; however, existing data-driven approaches remain limited by fragmented knowledge representations and unreliable extrapolation under sparse data conditions. In this study, we propose an ontology-guided, equation-centric framework that tightly integrates large language models (LLMs) with an additive manufacturing mathematical knowledge graph (AM-MKG) to enable reliable knowledge extraction and principled extrapolative modeling. By explicitly encoding equations, variables, assumptions, and their semantic relationships within a formal ontology, unstructured literature is transformed into machine-interpretable representations that support structured querying and reasoning. LLM-based equation generation is further conditioned on MKG-derived subgraphs, enforcing physically meaningful functional forms and mitigating non-physical or unstable extrapolation trends. To assess reliability beyond conventional predictive uncertainty, a confidence-aware extrapolation assessment is introduced, integrating extrapolation distance, statistical stability, and knowledge-graph-based physical consistency into a unified confidence score. Results demonstrate that ontology-guided extraction significantly improves the structural coherence and quantitative reliability of extracted knowledge, while subgraph-conditioned equation generation yields stable and physically consistent extrapolations compared to unguided LLM outputs. Overall, this work establishes a unified pipeline for ontology-driven knowledge representation, equation-centered reasoning, and confidence-based extrapolation assessment, highlighting the potential of knowledge-graph-augmented LLMs as reliable tools for extrapolative modeling in additive manufacturing.

</details>


### [27] [Improving Enzyme Prediction with Chemical Reaction Equations by Hypergraph-Enhanced Knowledge Graph Embeddings](https://arxiv.org/abs/2601.05330)
*Tengwei Song,Long Yin,Zhen Han,Zhiqiang Xu*

Main category: cs.AI

TL;DR: 提出Hyper-Enz模型，通过知识图谱嵌入和超图transformer预测酶-底物相互作用，相比传统模型在酶检索准确率上提升88%


<details>
  <summary>Details</summary>
Motivation: 现有酶-底物相互作用预测方法依赖专家标注的稀疏数据库，训练数据不足限制了模型的泛化能力。化学反应方程数据更易获取且更丰富，但多化合物与酶的复杂关系模式传统模型难以捕捉。

Method: 将化学反应方程表示为(底物、酶、产物)三元组构建知识图谱，提出Hyper-Enz模型：结合超图transformer和知识图谱嵌入学习多底物和产物超边的表示，并引入多专家范式指导酶-底物相互作用学习。

Result: 实验结果显示显著改进：酶检索平均准确率相对提升88%，配对级别预测提升30%，证明了方法的有效性。

Conclusion: 通过利用化学反应方程构建知识图谱，并结合超图transformer与知识图谱嵌入，能够有效预测酶-底物相互作用，解决了传统方法因数据稀疏导致的泛化能力不足问题。

Abstract: Predicting enzyme-substrate interactions has long been a fundamental problem in biochemistry and metabolic engineering. While existing methods could leverage databases of expert-curated enzyme-substrate pairs for models to learn from known pair interactions, the databases are often sparse, i.e., there are only limited and incomplete examples of such pairs, and also labor-intensive to maintain. This lack of sufficient training data significantly hinders the ability of traditional enzyme prediction models to generalize to unseen interactions. In this work, we try to exploit chemical reaction equations from domain-specific databases, given their easier accessibility and denser, more abundant data. However, interactions of multiple compounds, e.g., educts and products, with the same enzymes create complex relational data patterns that traditional models cannot easily capture. To tackle that, we represent chemical reaction equations as triples of (educt, enzyme, product) within a knowledge graph, such that we can take advantage of knowledge graph embedding (KGE) to infer missing enzyme-substrate pairs for graph completion. Particularly, in order to capture intricate relationships among compounds, we propose our knowledge-enhanced hypergraph model for enzyme prediction, i.e., Hyper-Enz, which integrates a hypergraph transformer with a KGE model to learn representations of the hyper-edges that involve multiple educts and products. Also, a multi-expert paradigm is introduced to guide the learning of enzyme-substrate interactions with both the proposed model and chemical reaction equations. Experimental results show a significant improvement, with up to a 88% relative improvement in average enzyme retrieval accuracy and 30% improvement in pair-level prediction compared to traditional models, demonstrating the effectiveness of our approach.

</details>


### [28] [The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models](https://arxiv.org/abs/2601.05376)
*Tassallah Abdullahi,Shrestha Ghosh,Hamish S Fraser,Daniel León Tramontini,Adeel Abbasi,Ghada Bourjeily,Carsten Eickhoff,Ritambhara Singh*

Main category: cs.AI

TL;DR: 医疗角色设定对临床决策有系统性、情境依赖且非单调的影响：在重症护理中提升性能，但在初级护理中降低性能，交互风格的影响高度模型依赖


<details>
  <summary>Details</summary>
Motivation: 研究角色设定（如急诊医生、护士等专业角色）对大型语言模型在临床决策中的影响，特别是对安全性和专业性的影响，因为目前普遍假设角色设定能单调提升专业性和安全性，但其在高风险临床决策中的实际效果尚不明确

Method: 系统评估基于角色的控制对临床语言模型的影响，考察专业角色（急诊医生、护士）和交互风格（大胆vs谨慎）在不同模型和医疗任务中的行为影响。使用多维评估方法，包括任务准确性、校准和安全性相关风险行为，评估临床分诊和患者安全任务

Result: 医疗角色在重症护理任务中提升性能（准确性和校准提升约20%），但在初级护理环境中降低相似幅度的性能。交互风格调节风险倾向和敏感性，但高度模型依赖。LLM评委总体上在安全关键案例中更倾向于医疗角色，但人类临床医生在安全合规性上仅显示中等一致性（平均Cohen's κ=0.43），且95.9%的回应中对推理质量表示低信心

Conclusion: 角色设定作为行为先验引入情境依赖的权衡，而非安全或专业性的保证。医疗角色设定并非总是提升性能，其效果取决于具体临床情境和任务类型

Abstract: Persona conditioning can be viewed as a behavioral prior for large language models (LLMs) and is often assumed to confer expertise and improve safety in a monotonic manner. However, its effects on high-stakes clinical decision-making remain poorly characterized. We systematically evaluate persona-based control in clinical LLMs, examining how professional roles (e.g., Emergency Department physician, nurse) and interaction styles (bold vs.\ cautious) influence behavior across models and medical tasks. We assess performance on clinical triage and patient-safety tasks using multidimensional evaluations that capture task accuracy, calibration, and safety-relevant risk behavior. We find systematic, context-dependent, and non-monotonic effects: Medical personas improve performance in critical care tasks, yielding gains of up to $\sim+20\%$ in accuracy and calibration, but degrade performance in primary-care settings by comparable margins. Interaction style modulates risk propensity and sensitivity, but it's highly model-dependent. While aggregated LLM-judge rankings favor medical over non-medical personas in safety-critical cases, we found that human clinicians show moderate agreement on safety compliance (average Cohen's $κ= 0.43$) but indicate a low confidence in 95.9\% of their responses on reasoning quality. Our work shows that personas function as behavioral priors that introduce context-dependent trade-offs rather than guarantees of safety or expertise. The code is available at https://github.com/rsinghlab/Persona\_Paradox.

</details>


### [29] [On the Effect of Cheating in Chess](https://arxiv.org/abs/2601.05386)
*Daniel Keren*

Main category: cs.AI

TL;DR: 研究通过有限次作弊（使用强大软件建议）在国际象棋中能获得的性能提升，开发算法评估作弊效果


<details>
  <summary>Details</summary>
Motivation: 国际象棋中使用软件作弊已成为严重问题，甚至影响到最高水平比赛。与以往主要关注作弊检测的研究不同，本文旨在量化在比赛中有限次作弊可能带来的性能提升

Method: 开发算法并在常用国际象棋引擎上进行测试，评估在游戏中有限次数使用软件建议的效果

Result: 通过算法测试获得了作弊可能带来的性能提升数据（具体结果未在摘要中说明）

Conclusion: 量化作弊效果对于遏制和检测作弊至关重要，本研究为评估作弊影响提供了方法学基础

Abstract: Cheating in chess, by using advice from powerful software, has become a major problem, reaching the highest levels. As opposed to the large majority of previous work, which concerned {\em detection} of cheating, here we try to evaluate the possible gain in performance, obtained by cheating a limited number of times during a game. Algorithms are developed and tested on a commonly used chess engine (i.e software).\footnote{Needless to say, the goal of this work is not to assist cheaters, but to measure the effectiveness of cheating -- which is crucial as part of the effort to contain and detect it.}

</details>


### [30] [ART: Adaptive Reasoning Trees for Explainable Claim Verification](https://arxiv.org/abs/2601.05455)
*Sahil Wadhwa,Himanshu Kumar,Guanqun Yang,Abbaas Alif Mohamed Nishar,Pranab Mohanty,Swapnil Shinde,Yue Wu*

Main category: cs.AI

TL;DR: ART（自适应推理树）是一种用于声明验证的分层方法，通过构建支持/攻击论点的树状结构，使用LLM裁判进行成对比较，实现透明、可争议的推理过程。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂决策中具有潜力，但其不透明性阻碍了在高风险环境中的应用。现有方法缺乏忠实解释和错误纠正机制，影响了可信度。

Method: 提出ART方法：从根声明开始，分支为支持和攻击的子论点。通过LLM裁判对子论点进行成对锦标赛式比较，自底向上确定论点强度，系统化推导最终透明且可争议的裁决。

Result: 在多个数据集上的实证验证表明，ART的结构化推理优于强基线方法，为可解释的声明验证设立了新基准，提高了可靠性并确保决策步骤的清晰性。

Conclusion: ART方法通过分层论证结构和系统化比较机制，解决了LLM在声明验证中的透明度和可争议性问题，为高风险环境中的可信决策提供了新途径。

Abstract: Large Language Models (LLMs) are powerful candidates for complex decision-making, leveraging vast encoded knowledge and remarkable zero-shot abilities. However, their adoption in high-stakes environments is hindered by their opacity; their outputs lack faithful explanations and cannot be effectively contested to correct errors, undermining trustworthiness. In this paper, we propose ART (Adaptive Reasoning Trees), a hierarchical method for claim verification. The process begins with a root claim, which branches into supporting and attacking child arguments. An argument's strength is determined bottom-up via a pairwise tournament of its children, adjudicated by a judge LLM, allowing a final, transparent and contestable verdict to be systematically derived which is missing in methods like Chain-of-Thought (CoT). We empirically validate ART on multiple datasets, analyzing different argument generators and comparison strategies. Our findings show that ART's structured reasoning outperforms strong baselines, establishing a new benchmark for explainable claim verification which is more reliable and ensures clarity in the overall decision making step.

</details>


### [31] [MMUEChange: A Generalized LLM Agent Framework for Intelligent Multi-Modal Urban Environment Change Analysis](https://arxiv.org/abs/2601.05483)
*Zixuan Xiao,Jun Ma,Siwei Zhang*

Main category: cs.AI

TL;DR: MMUEChange是一个多模态智能体框架，通过模块化工具包和模态控制器整合异构城市数据，实现复杂城市变化场景的鲁棒分析，相比最佳基线任务成功率提升46.7%


<details>
  <summary>Details</summary>
Motivation: 当前城市环境变化分析方法（特别是遥感变化检测）通常依赖僵化的单模态分析，存在局限性，需要更灵活的多模态集成方法来应对复杂的城市变化场景

Method: 提出MMUEChange多模态智能体框架，包含模块化工具包和核心模块模态控制器，实现跨模态和模态内对齐，灵活整合异构城市数据

Result: 相比最佳基线，MMUEChange智能体任务成功率提升46.7%，有效缓解幻觉问题；案例研究包括纽约小型社区公园转变、香港跨区域水污染扩散、深圳露天垃圾场减少及其与夜间经济活动的不同关联

Conclusion: MMUEChange框架能够支持具有现实政策意义的复杂城市变化分析任务，展示了多模态方法在城市环境变化研究中的优势

Abstract: Understanding urban environment change is essential for sustainable development. However, current approaches, particularly remote sensing change detection, often rely on rigid, single-modal analysis. To overcome these limitations, we propose MMUEChange, a multi-modal agent framework that flexibly integrates heterogeneous urban data via a modular toolkit and a core module, Modality Controller for cross- and intra-modal alignment, enabling robust analysis of complex urban change scenarios. Case studies include: a shift toward small, community-focused parks in New York, reflecting local green space efforts; the spread of concentrated water pollution across districts in Hong Kong, pointing to coordinated water management; and a notable decline in open dumpsites in Shenzhen, with contrasting links between nighttime economic activity and waste types, indicating differing urban pressures behind domestic and construction waste. Compared to the best-performing baseline, the MMUEChange agent achieves a 46.7% improvement in task success rate and effectively mitigates hallucination, demonstrating its capacity to support complex urban change analysis tasks with real-world policy implications.

</details>


### [32] [The Evaluation Gap in Medicine, AI and LLMs: Navigating Elusive Ground Truth & Uncertainty via a Probabilistic Paradigm](https://arxiv.org/abs/2601.05500)
*Aparna Elangovan,Lei Xu,Mahsa Elyasi,Ismail Akdulum,Mehmet Aksakal,Enes Gurun,Brian Hur,Saab Mansour,Ravid Shwartz Ziv,Karin Verspoor,Dan Roth*

Main category: cs.AI

TL;DR: 提出概率评估范式，考虑医学数据中专家标注的不确定性，引入期望准确率和期望F1分数，建议按标注确定性分层评估系统性能


<details>
  <summary>Details</summary>
Motivation: 当前AI系统基准测试通常忽略专家标注答案中的不确定性，这在医学领域尤为关键，因为医学数据普遍存在不确定性。忽略这种不确定性可能导致误导性结论，认为非专家与专家性能相似。

Method: 引入概率评估范式，从理论上解释标注确定性对评估的影响。提出期望准确率和期望F1分数来估计在给定标注变异性的情况下，专家或系统能达到的分数。建议按标注确定性（通常通过专家间一致率衡量）对结果进行分层评估。

Result: 研究表明，当总体性能低于80%阈值时，分层评估变得至关重要。在高确定性标注的数据子集中，性能比较更加可靠，能够减轻不确定性这一关键混杂因素的影响。

Conclusion: 在评估AI系统能力时，应考虑专家标注的不确定性，采用分层评估方法。当总体性能低于80%时，必须按标注确定性进行分层，以确保评估结果的可靠性，避免因标注不确定性导致的误导性结论。

Abstract: Benchmarking the relative capabilities of AI systems, including Large Language Models (LLMs) and Vision Models, typically ignores the impact of uncertainty in the underlying ground truth answers from experts. This ambiguity is particularly consequential in medicine where uncertainty is pervasive. In this paper, we introduce a probabilistic paradigm to theoretically explain how high certainty in ground truth answers is almost always necessary for even an expert to achieve high scores, whereas in datasets with high variation in ground truth answers there may be little difference between a random labeller and an expert. Therefore, ignoring uncertainty in ground truth evaluation data can result in the misleading conclusion that a non-expert has similar performance to that of an expert. Using the probabilistic paradigm, we thus bring forth the concepts of expected accuracy and expected F1 to estimate the score an expert human or system can achieve given ground truth answer variability.
  Our work leads to the recommendation that when establishing the capability of a system, results should be stratified by probability of the ground truth answer, typically measured by the agreement rate of ground truth experts. Stratification becomes critical when the overall performance drops below a threshold of 80%. Under stratified evaluation, performance comparison becomes more reliable in high certainty bins, mitigating the effect of the key confounding factor -- uncertainty.

</details>


### [33] [Explainable AI: Learning from the Learners](https://arxiv.org/abs/2601.05525)
*Ricardo Vinuesa,Steven L. Brunton,Gianmarco Mengaldo*

Main category: cs.AI

TL;DR: 本文提出将可解释人工智能（XAI）与因果推理相结合，通过"向学习者学习"的方法，从AI模型中提取因果机制，指导稳健设计和控制，并支持高风险应用中的信任与问责。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能在多个科学和工程任务中已超越人类表现，但其内部表征往往不透明。需要解决AI模型的黑箱问题，使其决策过程可解释，以支持科学发现、优化和验证。

Method: 将可解释人工智能（XAI）与因果推理相结合，利用基础模型和可解释性方法，从AI模型中提取因果机制，指导稳健设计和控制。

Result: 该方法能够从AI模型中提取因果机制，指导稳健设计和控制，并支持高风险应用中的信任与问责，为科学与工程中的人机协作提供统一框架。

Conclusion: XAI与因果推理的结合为"向学习者学习"提供了有效途径，能够解决解释的忠实性、泛化性和可用性等挑战，成为科学与工程领域人机协作的统一框架。

Abstract: Artificial intelligence now outperforms humans in several scientific and engineering tasks, yet its internal representations often remain opaque. In this Perspective, we argue that explainable artificial intelligence (XAI), combined with causal reasoning, enables {\it learning from the learners}. Focusing on discovery, optimization and certification, we show how the combination of foundation models and explainability methods allows the extraction of causal mechanisms, guides robust design and control, and supports trust and accountability in high-stakes applications. We discuss challenges in faithfulness, generalization and usability of explanations, and propose XAI as a unifying framework for human-AI collaboration in science and engineering.

</details>


### [34] [Safety Not Found (404): Hidden Risks of LLM-Based Robotics Decision Making](https://arxiv.org/abs/2601.05529)
*Jua Han,Jaeyoon Seo,Jungbin Min,Jean Oh,Jihie Kim*

Main category: cs.AI

TL;DR: 论文通过火灾疏散场景评估LLM在安全关键系统中的决策能力，发现即使99%准确率也意味着每百次执行可能造成灾难性后果，当前LLM不适合直接部署于安全关键系统。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在机器人决策中日益重要，其物理风险也随之增加——单个错误指令可能直接危及人类安全。论文旨在系统评估LLM在错误即灾难的场景中的表现，解决安全关键系统中LLM部署的迫切需求。

Method: 通过火灾疏散场景的定性评估识别关键失败案例，设计七项定量评估任务，分为三类：完整信息任务（使用ASCII地图减少歧义）、不完整信息任务（测试空间连续性与幻觉）、安全导向空间推理任务（评估生命威胁情境下的安全决策）。对多种LLM和VLM进行基准测试，并分析1%失败率的灾难性影响。

Result: 结果揭示严重漏洞：多个模型在ASCII导航中成功率为0%；在模拟火灾演练中，模型指示机器人向危险区域而非紧急出口移动。即使最先进模型也无法保证安全，99%准确率在机器人领域具有误导性，意味着每百次执行可能造成灾难性伤害。

Conclusion: 当前LLM尚未准备好直接部署于安全关键系统，对其绝对依赖会产生不可接受的风险。99%准确率在机器人领域是危险的误导，需要更严格的安全保障措施。

Abstract: One mistake by an AI system in a safety-critical setting can cost lives. As Large Language Models (LLMs) become integral to robotics decision-making, the physical dimension of risk grows; a single wrong instruction can directly endanger human safety. This paper addresses the urgent need to systematically evaluate LLM performance in scenarios where even minor errors are catastrophic. Through a qualitative evaluation of a fire evacuation scenario, we identified critical failure cases in LLM-based decision-making. Based on these, we designed seven tasks for quantitative assessment, categorized into: Complete Information, Incomplete Information, and Safety-Oriented Spatial Reasoning (SOSR). Complete information tasks utilize ASCII maps to minimize interpretation ambiguity and isolate spatial reasoning from visual processing. Incomplete information tasks require models to infer missing context, testing for spatial continuity versus hallucinations. SOSR tasks use natural language to evaluate safe decision-making in life-threatening contexts. We benchmark various LLMs and Vision-Language Models (VLMs) across these tasks. Beyond aggregate performance, we analyze the implications of a 1% failure rate, highlighting how "rare" errors escalate into catastrophic outcomes. Results reveal serious vulnerabilities: several models achieved a 0% success rate in ASCII navigation, while in a simulated fire drill, models instructed robots to move toward hazardous areas instead of emergency exits. Our findings lead to a sobering conclusion: current LLMs are not ready for direct deployment in safety-critical systems. A 99% accuracy rate is dangerously misleading in robotics, as it implies one out of every hundred executions could result in catastrophic harm. We demonstrate that even state-of-the-art models cannot guarantee safety, and absolute reliance on them creates unacceptable risks.

</details>


### [35] [WildSci: Advancing Scientific Reasoning from In-the-Wild Literature](https://arxiv.org/abs/2601.05567)
*Tengxiao Liu,Deepak Nathani,Zekun Li,Kevin Yang,William Yang Wang*

Main category: cs.AI

TL;DR: WildSci是一个从同行评审文献自动合成的领域特定科学问题数据集，涵盖9个科学学科和26个子领域，通过多项选择题格式支持可扩展的LLM科学推理训练。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理研究主要集中在数学和编程等数据丰富、评估指标明确的领域，而在医学和材料科学等科学领域进展有限，主要原因是数据集覆盖不足和开放科学问题的复杂性。

Method: 从同行评审文献自动合成领域特定科学问题，构建WildSci数据集；采用多项选择题格式将复杂科学推理任务结构化；应用强化学习对模型进行微调，并分析训练动态。

Result: 在一系列科学基准测试上的实验证明了数据集和方法的有效性，WildSci支持可扩展的科学推理研究。

Conclusion: WildSci数据集通过自动合成科学问题和多项选择题格式，解决了科学领域LLM推理的数据稀缺和评估困难问题，为可持续的科学推理研究提供了资源。

Abstract: Recent progress in large language model (LLM) reasoning has focused on domains like mathematics and coding, where abundant high-quality data and objective evaluation metrics are readily available. In contrast, progress in LLM reasoning models remains limited in scientific domains such as medicine and materials science due to limited dataset coverage and the inherent complexity of open-ended scientific questions. To address these challenges, we introduce WildSci, a new dataset of domain-specific science questions automatically synthesized from peer-reviewed literature, covering 9 scientific disciplines and 26 subdomains. By framing complex scientific reasoning tasks in a multiple-choice format, we enable scalable training with well-defined reward signals. We further apply reinforcement learning to finetune models on these data and analyze the resulting training dynamics, including domain-specific performance changes, response behaviors, and generalization trends. Experiments on a suite of scientific benchmarks demonstrate the effectiveness of our dataset and approach. We release WildSci to enable scalable and sustainable research in scientific reasoning, available at https://huggingface.co/datasets/JustinTX/WildSci.

</details>


### [36] [Crisis-Bench: Benchmarking Strategic Ambiguity and Reputation Management in Large Language Models](https://arxiv.org/abs/2601.05570)
*Cooper Lin,Maohao Ran,Yanting Zhang,Zhenglin Wan,Hongwei Fan,Yibo Xu,Yike Guo,Wei Xue,Jun Song*

Main category: cs.AI

TL;DR: 该论文提出了Crisis-Bench基准，用于评估LLM在需要战略模糊性和信息保留的专业领域（如危机公关）中的表现，揭示了通用安全对齐与专业实用性之间的冲突。


<details>
  <summary>Details</summary>
Motivation: 标准的安全对齐使LLM具备普遍的助人为乐和诚实性，但这种"童子军"道德观对需要战略模糊性和信息保留的专业领域（如公共关系、谈判、危机管理）施加了"透明度税"，限制了专业实用性。

Method: 1. 引入Crisis-Bench：一个多智能体部分可观察马尔可夫决策过程（POMDP），用于评估LLM在高风险企业危机中的表现。2. 包含80个不同故事情节，涵盖8个行业，模拟7天动态企业危机。3. 采用严格分离的私有和公共叙事状态来强制实施信息不对称。4. 引入Adjudicator-Market Loop评估指标：公众情绪被裁定并转化为模拟股价，创建现实的经济激励结构。

Result: 结果揭示了关键二分法：一些模型因伦理顾虑而妥协，而另一些模型则展示了马基雅维利式的、合法的战略保留能力，以稳定模拟股价。Crisis-Bench首次提供了评估"声誉管理"能力的量化框架。

Conclusion: 需要从僵化的道德绝对主义转向情境感知的专业对齐。Crisis-Bench为评估LLM在需要战略模糊性的专业领域中的表现提供了首个定量框架，强调了根据不同应用场景调整对齐策略的重要性。

Abstract: Standard safety alignment optimizes Large Language Models (LLMs) for universal helpfulness and honesty, effectively instilling a rigid "Boy Scout" morality. While robust for general-purpose assistants, this one-size-fits-all ethical framework imposes a "transparency tax" on professional domains requiring strategic ambiguity and information withholding, such as public relations, negotiation, and crisis management. To measure this gap between general safety and professional utility, we introduce Crisis-Bench, a multi-agent Partially Observable Markov Decision Process (POMDP) that evaluates LLMs in high-stakes corporate crises. Spanning 80 diverse storylines across 8 industries, Crisis-Bench tasks an LLM-based Public Relations (PR) Agent with navigating a dynamic 7-day corporate crisis simulation while managing strictly separated Private and Public narrative states to enforce rigorous information asymmetry. Unlike traditional benchmarks that rely on static ground truths, we introduce the Adjudicator-Market Loop: a novel evaluation metric where public sentiment is adjudicated and translated into a simulated stock price, creating a realistic economic incentive structure. Our results expose a critical dichotomy: while some models capitulate to ethical concerns, others demonstrate the capacity for Machiavellian, legitimate strategic withholding in order to stabilize the simulated stock price. Crisis-Bench provides the first quantitative framework for assessing "Reputation Management" capabilities, arguing for a shift from rigid moral absolutism to context-aware professional alignment.

</details>


### [37] [Reinforcement Learning of Large Language Models for Interpretable Credit Card Fraud Detection](https://arxiv.org/abs/2601.05578)
*Cooper Lin,Yanting Zhang,Maohao Ran,Wei Xue,Hongwei Fan,Yibo Xu,Zhenglin Wan,Sirui Han,Yike Guo,Jun Song*

Main category: cs.AI

TL;DR: 本文提出了一种使用强化学习对轻量级语言模型进行后训练的方法，专门用于电子商务欺诈检测任务，仅使用原始交易数据，在真实数据集上取得了显著的F1分数提升。


<details>
  <summary>Details</summary>
Motivation: 电子商务平台面临日益复杂的欺诈方案，但大型语言模型在真实金融欺诈检测中的应用尚未充分探索，其处理特定领域电子商务交易数据的实际效果也缺乏实证验证。

Method: 采用强化学习框架，使用GSPO算法结合基于规则的奖励系统，对多种规模的轻量级语言模型进行后训练，仅使用原始交易数据（包括客户信息、物流详情、产品描述和订单历史等文本信息）。

Result: 实验结果表明，经过后训练的语言模型在保留测试数据上取得了显著的F1分数提升，性能改进主要归因于强化学习的探索机制，使模型能够发现传统工程特征之外的新型欺诈指标。

Conclusion: 该方法成功弥合了传统机器学习局限性与LLM在欺诈检测中未开发潜力之间的差距，证明了强化学习后训练语言模型在电子商务欺诈检测中的有效性。

Abstract: E-commerce platforms and payment solution providers face increasingly sophisticated fraud schemes, ranging from identity theft and account takeovers to complex money laundering operations that exploit the speed and anonymity of digital transactions. However, despite their theoretical promise, the application of Large Language Models (LLMs) to fraud detection in real-world financial contexts remains largely unexploited, and their practical effectiveness in handling domain-specific e-commerce transaction data has yet to be empirically validated. To bridge this gap between conventional machine learning limitations and the untapped potential of LLMs in fraud detection, this paper proposes a novel approach that employs Reinforcement Learning (RL) to post-train lightweight language models specifically for fraud detection tasks using only raw transaction data. We utilize the Group Sequence Policy Optimization (GSPO) algorithm combined with a rule-based reward system to fine-tune language models of various sizes on a real-life transaction dataset provided by a Chinese global payment solution company. Through this reinforcement learning framework, the language models are encouraged to explore diverse trust and risk signals embedded within the textual transaction data, including patterns in customer information, shipping details, product descriptions, and order history. Our experimental results demonstrate the effectiveness of this approach, with post-trained language models achieving substantial F1-score improvements on held-out test data. Our findings demonstrate that the observed performance improvements are primarily attributable to the exploration mechanism inherent in reinforcement learning, which allows models to discover novel fraud indicators beyond those captured by traditional engineered features.

</details>


### [38] [Cumulative Path-Level Semantic Reasoning for Inductive Knowledge Graph Completion](https://arxiv.org/abs/2601.05629)
*Jiapu Wang,Xinghe Cheng,Zezheng Wu,Ruiqi Ma,Rui Wang,Zhichao Yan,Haoran Luo,Yuhao Jiang,Kai Sun*

Main category: cs.AI

TL;DR: CPSR框架通过查询依赖掩码模块和全局语义评分模块，同时捕捉知识图谱的结构和语义信息，在归纳式知识图谱补全任务中取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱补全方法在处理新兴实体时效果不佳，而现有的归纳式KGC方法虽然能处理新兴实体和关系，但仍面临噪声结构信息干扰和难以捕捉推理路径中长距离依赖关系的挑战。

Method: 提出CPSR框架：1) 查询依赖掩码模块自适应地屏蔽噪声结构信息，保留与目标密切相关的信息；2) 全局语义评分模块评估推理路径中节点的个体贡献和集体影响。

Result: 实验结果表明，CPSR在归纳式知识图谱补全任务中实现了最先进的性能。

Conclusion: CPSR框架通过同时捕捉知识图谱的结构和语义信息，有效解决了现有归纳式KGC方法的挑战，提升了推理性能。

Abstract: Conventional Knowledge Graph Completion (KGC) methods aim to infer missing information in incomplete Knowledge Graphs (KGs) by leveraging existing information, which struggle to perform effectively in scenarios involving emerging entities. Inductive KGC methods can handle the emerging entities and relations in KGs, offering greater dynamic adaptability. While existing inductive KGC methods have achieved some success, they also face challenges, such as susceptibility to noisy structural information during reasoning and difficulty in capturing long-range dependencies in reasoning paths. To address these challenges, this paper proposes the Cumulative Path-Level Semantic Reasoning for inductive knowledge graph completion (CPSR) framework, which simultaneously captures both the structural and semantic information of KGs to enhance the inductive KGC task. Specifically, the proposed CPSR employs a query-dependent masking module to adaptively mask noisy structural information while retaining important information closely related to the targets. Additionally, CPSR introduces a global semantic scoring module that evaluates both the individual contributions and the collective impact of nodes along the reasoning path within KGs. The experimental results demonstrate that CPSR achieves state-of-the-art performance.

</details>


### [39] [GenCtrl -- A Formal Controllability Toolkit for Generative Models](https://arxiv.org/abs/2601.05637)
*Emily Cheng,Carmen Amo Alonso,Federico Danieli,Arno Blaas,Luca Zappella,Pau Rodriguez,Xavier Suau*

Main category: cs.AI

TL;DR: 该论文提出了一个理论框架来形式化分析生成模型的可控性，通过对话场景中的可控集估计算法，为模型控制提供了分布无关的PAC保证，揭示了模型可控性的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的普及，需要细粒度控制生成过程，但现有控制方法（从提示到微调）未能回答一个根本问题：这些模型是否真正可控？需要从理论上分析模型控制的基本限制。

Method: 将人机交互建模为控制过程，提出一种新颖算法来估计对话设置中模型的可控集。该算法提供形式化保证：基于样本复杂度的估计误差的PAC边界，这些边界是分布无关的，仅假设输出有界，适用于任何黑盒非线性控制系统（即任何生成模型）。

Result: 在控制对话过程的不同任务上进行了实证验证，涵盖语言模型和文本到图像生成。结果显示模型可控性出人意料地脆弱，且高度依赖实验设置。

Conclusion: 模型可控性需要严格的量化分析，研究重点应从简单地尝试控制转向首先理解其基本限制。该理论框架为评估生成模型的可控性提供了形式化工具。

Abstract: As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.

</details>


### [40] [HAG: Hierarchical Demographic Tree-based Agent Generation for Topic-Adaptive Simulation](https://arxiv.org/abs/2601.05656)
*Rongxin Chen,Tianyu Wu,Bingbing Xu,Xiucheng Xu,Huawei Shen*

Main category: cs.AI

TL;DR: HAG是一个分层智能体生成框架，通过两阶段决策过程实现主题自适应的人口生成，在宏观分布对齐和微观一致性方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有智能体初始化方法存在局限性：基于静态数据检索的方法无法适应数据中未见过的主题，而基于LLM生成的方法缺乏宏观分布意识，导致微观个体属性与现实不一致。需要一种既能捕捉宏观联合分布又能确保微观个体合理性的主题自适应框架。

Method: 提出HAG分层智能体生成框架，将人口生成形式化为两阶段决策过程：1）使用世界知识模型推断分层条件概率构建主题自适应树，实现宏观分布对齐；2）基于真实世界数据进行实例化和智能体增强，确保微观一致性。

Result: 实验表明HAG显著优于代表性基线方法，平均减少人口对齐误差37.7%，提升社会学一致性18.8%。建立了多领域基准和全面的PACE评估框架。

Conclusion: HAG框架通过分层生成方法有效解决了智能体初始化中的主题适应性问题，在宏观分布对齐和微观一致性方面都取得了显著改进，为基于智能体的建模提供了高质量的初始化方案。

Abstract: High-fidelity agent initialization is crucial for credible Agent-Based Modeling across diverse domains. A robust framework should be Topic-Adaptive, capturing macro-level joint distributions while ensuring micro-level individual rationality. Existing approaches fall into two categories: static data-based retrieval methods that fail to adapt to unseen topics absent from the data, and LLM-based generation methods that lack macro-level distribution awareness, resulting in inconsistencies between micro-level persona attributes and reality. To address these problems, we propose HAG, a Hierarchical Agent Generation framework that formalizes population generation as a two-stage decision process. Firstly, utilizing a World Knowledge Model to infer hierarchical conditional probabilities to construct the Topic-Adaptive Tree, achieving macro-level distribution alignment. Then, grounded real-world data, instantiation and agentic augmentation are carried out to ensure micro-level consistency. Given the lack of specialized evaluation, we establish a multi-domain benchmark and a comprehensive PACE evaluation framework. Extensive experiments show that HAG significantly outperforms representative baselines, reducing population alignment errors by an average of 37.7% and enhancing sociological consistency by 18.8%.

</details>


### [41] [Circular Reasoning: Understanding Self-Reinforcing Loops in Large Reasoning Models](https://arxiv.org/abs/2601.05693)
*Zenghao Duan,Liang Pang,Zihao Wei,Wenbin Duan,Yuxin Tian,Shicheng Xu,Jingcheng Deng,Zhiyi Yin,Xueqi Cheng*

Main category: cs.AI

TL;DR: 论文研究了大型推理模型中的循环推理失败模式，提出了LoopBench数据集来捕捉数值循环和语句循环，揭示了循环推理的机制是状态崩溃和自我强化的V型注意力机制，并利用CUSUM算法实现早期循环预测。


<details>
  <summary>Details</summary>
Motivation: 尽管测试时缩放取得了成功，但大型推理模型经常遇到重复循环，导致计算浪费和推理失败。本文识别了一种独特的失败模式——循环推理，与传统模型退化不同，这种现象表现为自我强化的陷阱，其中生成的内容作为自身重复的逻辑前提。

Method: 1. 引入LoopBench数据集捕捉两种循环类型：数值循环和语句循环；2. 从机制上表征循环推理为具有明显边界的状态崩溃；3. 揭示推理僵局触发循环开始，随后由自我强化的V型注意力机制驱动持续；4. 使用累积和算法捕捉循环前兆进行早期预测。

Result: 实验验证了CUSUM算法在不同大型推理模型中的准确性，阐明了长链推理的稳定性。该算法能够有效捕捉循环推理的前兆，实现早期预测。

Conclusion: 循环推理是大型推理模型中的一种独特失败模式，表现为自我强化的重复循环。通过LoopBench数据集和机制分析，揭示了循环推理的触发和持续机制，并提出了有效的早期预测方法，为改善模型推理稳定性提供了理论基础。

Abstract: Despite the success of test-time scaling, Large Reasoning Models (LRMs) frequently encounter repetitive loops that lead to computational waste and inference failure. In this paper, we identify a distinct failure mode termed Circular Reasoning. Unlike traditional model degeneration, this phenomenon manifests as a self-reinforcing trap where generated content acts as a logical premise for its own recurrence, compelling the reiteration of preceding text. To systematically analyze this phenomenon, we introduce LoopBench, a dataset designed to capture two distinct loop typologies: numerical loops and statement loops. Mechanistically, we characterize circular reasoning as a state collapse exhibiting distinct boundaries, where semantic repetition precedes textual repetition. We reveal that reasoning impasses trigger the loop onset, which subsequently persists as an inescapable cycle driven by a self-reinforcing V-shaped attention mechanism. Guided by these findings, we employ the Cumulative Sum (CUSUM) algorithm to capture these precursors for early loop prediction. Experiments across diverse LRMs validate its accuracy and elucidate the stability of long-chain reasoning.

</details>


### [42] [Logic-Parametric Neuro-Symbolic NLI: Controlling Logical Formalisms for Verifiable LLM Reasoning](https://arxiv.org/abs/2601.05705)
*Ali Farjami,Luca Redondi,Marco Valentino*

Main category: cs.AI

TL;DR: 提出一个逻辑参数化框架，将逻辑作为可控组件嵌入神经符号推理系统，通过高阶逻辑统一多种经典和非经典逻辑形式，在规范性推理任务中比较不同逻辑策略的效果。


<details>
  <summary>Details</summary>
Motivation: 现有结合大语言模型和定理证明器的可验证自然语言推理方法依赖于固定的逻辑形式化，这限制了系统的鲁棒性和适应性。需要一种能够灵活选择和调整逻辑形式的框架。

Method: 采用LogiKEy方法学，将多种经典和非经典逻辑形式嵌入高阶逻辑中，实现逻辑参数化。比较逻辑外部方法（通过公理编码规范要求）和逻辑内部方法（从逻辑内置结构中产生规范模式）。

Result: 实验表明逻辑内部策略能持续提升性能并产生更高效的混合证明。不同逻辑在不同领域表现各异：一阶逻辑在常识推理中表现更好，而道义逻辑和模态逻辑在伦理领域表现更优。

Conclusion: 将逻辑作为神经符号架构中的一等参数化元素，能够实现更鲁棒、模块化和适应性强的推理系统，逻辑选择应根据具体领域需求进行优化。

Abstract: Large language models (LLMs) and theorem provers (TPs) can be effectively combined for verifiable natural language inference (NLI). However, existing approaches rely on a fixed logical formalism, a feature that limits robustness and adaptability. We propose a logic-parametric framework for neuro-symbolic NLI that treats the underlying logic not as a static background, but as a controllable component. Using the LogiKEy methodology, we embed a range of classical and non-classical formalisms into higher-order logic (HOL), enabling a systematic comparison of inference quality, explanation refinement, and proof behavior. We focus on normative reasoning, where the choice of logic has significant implications. In particular, we compare logic-external approaches, where normative requirements are encoded via axioms, with logic-internal approaches, where normative patterns emerge from the logic's built-in structure. Extensive experiments demonstrate that logic-internal strategies can consistently improve performance and produce more efficient hybrid proofs for NLI. In addition, we show that the effectiveness of a logic is domain-dependent, with first-order logic favouring commonsense reasoning, while deontic and modal logics excel in ethical domains. Our results highlight the value of making logic a first-class, parametric element in neuro-symbolic architectures for more robust, modular, and adaptable reasoning.

</details>


### [43] [Overcoming Joint Intractability with Lossless Hierarchical Speculative Decoding](https://arxiv.org/abs/2601.05724)
*Yuxuan Zhou,Fei Huang,Heng Li,Fengyi Wu,Tianyu Wang,Jianwei Zhang,Junyang Lin,Zhi-Qi Cheng*

Main category: cs.AI

TL;DR: HSD是一种无损验证方法，通过分层平衡概率质量来克服联合不可解问题，显著提升接受令牌数量，在EAGLE-3中实现超过12%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 验证是推测解码中提高推理速度同时保持分布保真度的关键瓶颈。现有序列级验证方法虽然比令牌级验证接受更多令牌，但依赖近似或受限于部分信息，难以处理联合不可解问题。

Method: 提出分层推测解码（HSD），这是一种可证明无损的验证方法，通过在可访问分支间平衡超额和不足的概率质量来克服联合不可解性。

Result: 大规模实验显示HSD在不同模型家族和基准测试中持续提升接受率。集成到EAGLE-3中实现超过12%的性能提升，建立最先进的解码效率而不损害分布保真度。

Conclusion: HSD是一种强大的、可解释的、通用的验证方法，可轻松集成到各种推测解码框架中，显著提升解码效率同时保持分布保真度。

Abstract: Verification is a key bottleneck in improving inference speed while maintaining distribution fidelity in Speculative Decoding. Recent work has shown that sequence-level verification leads to a higher number of accepted tokens compared to token-wise verification. However, existing solutions often rely on surrogate approximations or are constrained by partial information, struggling with joint intractability. In this work, we propose Hierarchical Speculative Decoding (HSD), a provably lossless verification method that significantly boosts the expected number of accepted tokens and overcomes joint intractability by balancing excess and deficient probability mass across accessible branches. Our extensive large-scale experiments demonstrate that HSD yields consistent improvements in acceptance rates across diverse model families and benchmarks. Moreover, its strong explainability and generality make it readily integrable into a wide range of speculative decoding frameworks. Notably, integrating HSD into EAGLE-3 yields over a 12% performance gain, establishing state-of-the-art decoding efficiency without compromising distribution fidelity. Code is available at https://github.com/ZhouYuxuanYX/Hierarchical-Speculative-Decoding.

</details>


### [44] [PII-VisBench: Evaluating Personally Identifiable Information Safety in Vision Language Models Along a Continuum of Visibility](https://arxiv.org/abs/2601.05739)
*G M Shahariar,Zabir Al Nazi,Md Olid Hasan Bhuiyan,Zhouxing Shi*

Main category: cs.AI

TL;DR: PII-VisBench：一个评估视觉语言模型隐私安全性的新基准，关注个人在线存在程度对PII泄露的影响，发现模型对高可见度主体的PII泄露率更高。


<details>
  <summary>Details</summary>
Motivation: 现有VLM隐私评估主要将隐私视为静态提取任务，忽略了个人在线存在程度（数据可获取量）对隐私对齐的影响，需要更全面的评估框架。

Method: 构建PII-VisBench基准，包含4000个独特探针，将200个主体按在线信息可获取程度分为高、中、低、零可见度四类，评估18个开源VLMs的拒绝率和条件PII泄露率。

Result: 模型呈现一致模式：随着主体可见度降低，拒绝率增加，PII泄露率从高可见度的9.10%降至低可见度的5.34%；模型更可能泄露高可见度主体的PII，存在显著的模型家族异质性和PII类型差异。

Conclusion: 需要基于可见度的安全评估和训练干预，重述和越狱式提示暴露了攻击和模型依赖的失败，强调了考虑在线存在连续性的隐私评估重要性。

Abstract: Vision Language Models (VLMs) are increasingly integrated into privacy-critical domains, yet existing evaluations of personally identifiable information (PII) leakage largely treat privacy as a static extraction task and ignore how a subject's online presence--the volume of their data available online--influences privacy alignment. We introduce PII-VisBench, a novel benchmark containing 4000 unique probes designed to evaluate VLM safety through the continuum of online presence. The benchmark stratifies 200 subjects into four visibility categories: high, medium, low, and zero--based on the extent and nature of their information available online. We evaluate 18 open-source VLMs (0.3B-32B) based on two key metrics: percentage of PII probing queries refused (Refusal Rate) and the fraction of non-refusal responses flagged for containing PII (Conditional PII Disclosure Rate). Across models, we observe a consistent pattern: refusals increase and PII disclosures decrease (9.10% high to 5.34% low) as subject visibility drops. We identify that models are more likely to disclose PII for high-visibility subjects, alongside substantial model-family heterogeneity and PII-type disparities. Finally, paraphrasing and jailbreak-style prompts expose attack and model-dependent failures, motivating visibility-aware safety evaluation and training interventions.

</details>


### [45] [DynaDebate: Breaking Homogeneity in Multi-Agent Debate with Dynamic Path Generation](https://arxiv.org/abs/2601.05746)
*Zhenghao Li,Zhi Zheng,Wei Chen,Jielun Zhao,Yong Chen,Tong Xu,Enhong Chen*

Main category: cs.AI

TL;DR: DynaDebate提出了一种动态多智能体辩论框架，通过路径生成、过程中心辩论和触发验证机制解决传统多智能体辩论中存在的同质化推理和简单多数投票问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的多智能体辩论框架存在同质化推理问题，智能体往往采用相同的推理路径导致相同错误，使得辩论退化为简单多数投票，限制了辩论效果。

Method: 提出DynaDebate框架，包含三个核心机制：1) 动态路径生成与分配，使用专门的路径生成智能体产生多样化逻辑路径；2) 过程中心辩论，关注逐步逻辑批判而非表面结果投票；3) 触发式验证智能体，在分歧时激活并使用外部工具客观解决僵局。

Result: 大量实验表明DynaDebate在各种基准测试中表现优异，超越了现有最先进的多智能体辩论方法。

Conclusion: DynaDebate通过引入动态路径生成、过程中心辩论和触发验证机制，有效解决了多智能体辩论中的同质化推理问题，显著提升了辩论效果和协作决策能力。

Abstract: Recent years have witnessed the rapid development of Large Language Model-based Multi-Agent Systems (MAS), which excel at collaborative decision-making and complex problem-solving. Recently, researchers have further investigated Multi-Agent Debate (MAD) frameworks, which enhance the reasoning and collaboration capabilities of MAS through information exchange and debate among multiple agents. However, existing approaches often rely on unguided initialization, causing agents to adopt identical reasoning paths that lead to the same errors. As a result, effective debate among agents is hindered, and the final outcome frequently degenerates into simple majority voting. To solve the above problem, in this paper, we introduce Dynamic Multi-Agent Debate (DynaDebate), which enhances the effectiveness of multi-agent debate through three key mechanisms: (1) Dynamic Path Generation and Allocation, which employs a dedicated Path Generation Agent to generate diverse and logical solution paths with adaptive redundancy; (2) Process-Centric Debate, which shifts the focus from surface-level outcome voting to rigorous step-by-step logic critique to ensure process correctness; (3) A Trigger-Based Verification Agent, which is activated upon disagreement and uses external tools to objectively resolve deadlocks. Extensive experiments demonstrate that DynaDebate achieves superior performance across various benchmarks, surpassing existing state-of-the-art MAD methods.

</details>


### [46] [From Off-Policy to On-Policy: Enhancing GUI Agents via Bi-level Expert-to-Policy Assimilation](https://arxiv.org/abs/2601.05787)
*Zezhou Wang,Ziyun Zhang,Xiaoyi Zhang,Zhuzhong Qian,Yan Lu*

Main category: cs.AI

TL;DR: BEPA方法通过双层专家轨迹到策略的融合，利用少量专家轨迹提升端到端GUI操作策略的性能，在OSWorld-Verified等基准上显著提升成功率。


<details>
  <summary>Details</summary>
Motivation: 当前GUI数据集规模有限，专家轨迹收集困难，端到端截图到动作策略性能落后于框架式系统。需要探索如何利用少量现有专家轨迹通过强化学习训练更好的端到端策略。

Method: 提出BEPA（双层专家到策略融合）方法：第一层通过基础策略生成自滚动可达轨迹，将静态专家轨迹转化为策略对齐的指导；第二层使用按任务动态更新的缓存进行RLVR（可验证奖励的强化学习）。

Result: 在OSWorld-Verified上，BEPA将UITARS1.5-7B的成功率从22.87%提升到32.13%，在保留测试集上从5.74%提升到10.30%，在MMBench-GUI和Online-Mind2Web上也有一致的性能提升。

Conclusion: BEPA方法有效解决了专家轨迹与学习者之间的结构不匹配和分布偏移问题，能够利用少量专家轨迹显著提升端到端GUI操作策略的性能。

Abstract: Vision-language models are increasingly deployed as computer-use agents (CUAs) that operate desktops and browsers. Top-performing CUAs are framework-based systems that decompose planning and execution, while end-to-end screenshot-to-action policies are easier to deploy but lag behind on benchmarks such as OSWorld-Verified. GUI datasets like OSWorld pose two bottlenecks: they expose only a few hundred interactive, verifiable tasks and environments, and expert trajectories must be gathered by interacting with these environments, making such data hard to scale. We therefore ask how reinforcement learning from verifiable rewards (RLVR) can best exploit a small pool of exist expert trajectories to train end-to-end policies. Naively mixing these off-policy traces into on-policy RLVR is brittle: even after format conversion, expert trajectories exhibit structural mismatch and distribution shift from the learner. We propose BEPA (Bi-Level Expert-to-Policy Assimilation), which turns static expert traces into policy-aligned guidance via self-rolled reachable trajectories under the base policy (LEVEL-1) and a per-task, dynamically updated cache used in RLVR (LEVEL-2). On OSWorld-Verified, BEPA improves UITARS1.5-7B success from 22.87% to 32.13% and raises a held-out split from 5.74% to 10.30%, with consistent gains on MMBench-GUI and Online-Mind2Web. Our code and data are available at: https://github.com/LEON-gittech/Verl_GUI.git

</details>


### [47] [StackPlanner: A Centralized Hierarchical Multi-Agent System with Task-Experience Memory Management](https://arxiv.org/abs/2601.05890)
*Ruizhe Zhang,Xinke Jiang,Zhibang Yang,Zhixin Zhang,Jiaran Gao,Yuzhen Xiao,Hongbin Lai,Xu Chu,Junfeng Zhao,Yasha Wang*

Main category: cs.AI

TL;DR: StackPlanner是一个具有显式内存控制的分层多智能体框架，通过解耦高层协调与子任务执行，解决集中式架构中内存管理不足导致的上下文膨胀、错误累积和跨任务泛化差的问题。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的多智能体系统在复杂知识密集型任务中表现出潜力，但集中式架构由于缺乏内存管理，导致长时程协作不稳定，出现上下文膨胀、错误累积和跨任务泛化能力差的问题。

Method: 提出StackPlanner分层多智能体框架，通过主动任务级内存控制解耦高层协调与子任务执行，并利用结构化经验记忆和强化学习来检索和利用可重用的协调经验。

Result: 在多个深度搜索和智能体系统基准测试上的实验证明了该方法在实现可靠的长时程多智能体协作方面的有效性。

Conclusion: StackPlanner通过显式内存控制和经验重用机制，有效解决了多智能体系统中任务级内存效率低下和协调经验无法重用的问题，实现了更可靠的长时程协作。

Abstract: Multi-agent systems based on large language models, particularly centralized architectures, have recently shown strong potential for complex and knowledge-intensive tasks. However, central agents often suffer from unstable long-horizon collaboration due to the lack of memory management, leading to context bloat, error accumulation, and poor cross-task generalization. To address both task-level memory inefficiency and the inability to reuse coordination experience, we propose StackPlanner, a hierarchical multi-agent framework with explicit memory control. StackPlanner addresses these challenges by decoupling high-level coordination from subtask execution with active task-level memory control, and by learning to retrieve and exploit reusable coordination experience via structured experience memory and reinforcement learning. Experiments on multiple deep-search and agent system benchmarks demonstrate the effectiveness of our approach in enabling reliable long-horizon multi-agent collaboration.

</details>


### [48] [TowerMind: A Tower Defence Game Learning Environment and Benchmark for LLM as Agents](https://arxiv.org/abs/2601.05899)
*Dawei Wang,Chengming Zhou,Di Zhao,Xinyuan Liu,Marci Chi Ma,Gary Ushaw,Richard Davison*

Main category: cs.AI

TL;DR: TowerMind是一个基于塔防游戏的轻量级多模态环境，用于评估大语言模型在实时策略游戏中的长期规划和决策能力，揭示了LLMs与人类专家在能力和幻觉方面的差距。


<details>
  <summary>Details</summary>
Motivation: 现有实时策略游戏环境要么计算需求高，要么缺乏文本观察支持，限制了LLMs的评估。需要一种轻量级、多模态的环境来评估LLMs在长期规划和决策方面的核心能力。

Method: 提出TowerMind环境，基于塔防游戏子类型，保留RTS游戏的关键评估优势，同时具有低计算需求和包括像素、文本和结构化游戏状态表示的多模态观察空间。设计了五个基准关卡来评估不同多模态输入设置下的LLMs。

Result: 结果显示LLMs与人类专家在能力和幻觉维度上存在明显性能差距。实验突显了LLMs的关键局限性：规划验证不足、决策缺乏多终局性、行动使用效率低。还评估了两种经典强化学习算法：Ape-X DQN和PPO。

Conclusion: TowerMind通过轻量级多模态设计补充了现有的RTS游戏环境，为AI智能体领域引入了新的基准测试平台。源代码已在GitHub上公开。

Abstract: Recent breakthroughs in Large Language Models (LLMs) have positioned them as a promising paradigm for agents, with long-term planning and decision-making emerging as core general-purpose capabilities for adapting to diverse scenarios and tasks. Real-time strategy (RTS) games serve as an ideal testbed for evaluating these two capabilities, as their inherent gameplay requires both macro-level strategic planning and micro-level tactical adaptation and action execution. Existing RTS game-based environments either suffer from relatively high computational demands or lack support for textual observations, which has constrained the use of RTS games for LLM evaluation. Motivated by this, we present TowerMind, a novel environment grounded in the tower defense (TD) subgenre of RTS games. TowerMind preserves the key evaluation strengths of RTS games for assessing LLMs, while featuring low computational demands and a multimodal observation space, including pixel-based, textual, and structured game-state representations. In addition, TowerMind supports the evaluation of model hallucination and provides a high degree of customizability. We design five benchmark levels to evaluate several widely used LLMs under different multimodal input settings. The results reveal a clear performance gap between LLMs and human experts across both capability and hallucination dimensions. The experiments further highlight key limitations in LLM behavior, such as inadequate planning validation, a lack of multifinality in decision-making, and inefficient action use. We also evaluate two classic reinforcement learning algorithms: Ape-X DQN and PPO. By offering a lightweight and multimodal design, TowerMind complements the existing RTS game-based environment landscape and introduces a new benchmark for the AI agent field. The source code is publicly available on GitHub(https://github.com/tb6147877/TowerMind).

</details>


### [49] [Open-Vocabulary 3D Instruction Ambiguity Detection](https://arxiv.org/abs/2601.05991)
*Jiayu Ding,Haoran Tang,Ge Li*

Main category: cs.AI

TL;DR: 该论文首次定义了开放词汇3D指令歧义检测任务，构建了Ambi3D大规模基准数据集，并提出AmbiVer两阶段框架来解决现有3D大语言模型在判断指令歧义性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域（如手术场景），语言歧义可能导致严重后果，但现有具身AI研究大多忽视这一问题，假设指令清晰并专注于执行而非确认。为填补这一安全空白，需要系统性地检测3D场景中的指令歧义。

Method: 提出了AmbiVer两阶段框架：第一阶段从多个视角收集显式视觉证据，第二阶段利用这些证据指导视觉语言模型判断指令歧义性。同时构建了Ambi3D基准数据集，包含700多个多样化的3D场景和约22k条指令。

Result: 分析发现最先进的3D大语言模型在可靠判断指令歧义性方面存在显著局限性。AmbiVer框架在实验中表现出色，有效解决了该任务挑战，为更安全可靠的具身AI奠定了基础。

Conclusion: 该研究首次定义了开放词汇3D指令歧义检测任务，通过构建大规模基准数据集和提出有效框架，为解决具身AI中的安全关键问题迈出了重要一步，为实现更安全可信的具身AI系统铺平了道路。

Abstract: In safety-critical domains, linguistic ambiguity can have severe consequences; a vague command like "Pass me the vial" in a surgical setting could lead to catastrophic errors. Yet, most embodied AI research overlooks this, assuming instructions are clear and focusing on execution rather than confirmation. To address this critical safety gap, we are the first to define Open-Vocabulary 3D Instruction Ambiguity Detection, a fundamental new task where a model must determine if a command has a single, unambiguous meaning within a given 3D scene. To support this research, we build Ambi3D, the large-scale benchmark for this task, featuring over 700 diverse 3D scenes and around 22k instructions. Our analysis reveals a surprising limitation: state-of-the-art 3D Large Language Models (LLMs) struggle to reliably determine if an instruction is ambiguous. To address this challenge, we propose AmbiVer, a two-stage framework that collects explicit visual evidence from multiple views and uses it to guide an vision-language model (VLM) in judging instruction ambiguity. Extensive experiments demonstrate the challenge of our task and the effectiveness of AmbiVer, paving the way for safer and more trustworthy embodied AI. Code and dataset available at https://jiayuding031020.github.io/ambi3d/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [50] [MoEBlaze: Breaking the Memory Wall for Efficient MoE Training on Modern GPUs](https://arxiv.org/abs/2601.05296)
*Jiyuan Zhang,Yining Liu,Siqi Yan,Lisen Deng,Jennifer Cao,Shuqi Yang,Min Ni,Bi Xue,Shen Li*

Main category: cs.LG

TL;DR: MoEBlaze是一个内存高效的MoE训练框架，通过消除中间缓冲区和优化内核设计，实现了4倍加速和50%内存节省。


<details>
  <summary>Details</summary>
Motivation: 现代大规模Mixture-of-Experts架构中的"内存墙"瓶颈被显著放大，稀疏计算特性导致大量激活内存开销，限制了GPU上的最大批处理大小和序列长度，并造成过多的数据移动，阻碍了性能和模型扩展效率。

Method: MoEBlaze采用协同设计的系统方法：1）端到端的令牌分发和MoE训练方法，通过优化数据结构消除中间缓冲区和激活物化；2）协同设计的内核与智能激活检查点，在减少内存占用的同时提升性能。

Result: MoEBlaze相比现有MoE框架实现了超过4倍的加速和超过50%的内存节省。

Conclusion: MoEBlaze通过内存高效的训练框架设计，有效解决了MoE架构中的内存瓶颈问题，显著提升了训练性能和扩展能力。

Abstract: The pervasive "memory wall" bottleneck is significantly amplified in modern large-scale Mixture-of-Experts (MoE) architectures. MoE's inherent architectural sparsity leads to sparse arithmetic compute and also introduces substantial activation memory overheads -- driven by large token routing buffers and the need to materialize and buffer intermediate tensors. This memory pressure limits the maximum batch size and sequence length that can fit on GPUs, and also results in excessive data movements that hinders performance and efficient model scaling. We present MoEBlaze, a memory-efficient MoE training framework that addresses these issues through a co-designed system approach: (i) an end-to-end token dispatch and MoE training method with optimized data structures to eliminate intermediate buffers and activation materializing, and (ii) co-designed kernels with smart activation checkpoint to mitigate memory footprint while simultaneously achieving better performance. We demonstrate that MoEBlaze can achieve over 4x speedups and over 50% memory savings compared to existing MoE frameworks.

</details>


### [51] [TIME: Temporally Intelligent Meta-reasoning Engine for Context Triggered Explicit Reasoning](https://arxiv.org/abs/2601.05300)
*Susmit Das*

Main category: cs.LG

TL;DR: TIME框架通过时间标记和内联思考块，让对话模型能够根据上下文和时间线索进行简洁的推理，大幅减少推理标记数量


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的推理设计存在成本高、可审计性差、无法重新触发推理的问题，且对话模型对时间结构不敏感，将不同时间间隔的回复视为等同

Method: 引入TIME框架，使用ISO 8601时间标签、静默间隙标记和可出现在回复任意位置的简短思考块。通过四阶段课程训练，包括小规模、最大多样性的全批次对齐步骤

Result: 在4B到32B规模上，TIME在TIMEBench基准测试中优于基础Qwen3模型，同时将推理标记减少约一个数量级

Conclusion: TIME框架通过上下文敏感的时间驱动推理，显著提升了对话模型的时间感知能力和推理效率，同时保持了紧凑的用户界面

Abstract: Reasoning oriented large language models often expose explicit "thinking" as long, turn-global traces at the start of every response, either always on or toggled externally at inference time. While useful for arithmetic, programming, and problem solving, this design is costly, blurs claim level auditability, and cannot re-trigger explicit reasoning once the model begins presenting. Dialogue models are also largely blind to temporal structure, treating replies after seconds and replies after weeks as equivalent unless time is stated in text. We introduce TIME, the Temporally Intelligent Meta-reasoning Engine, a behavioral alignment framework that treats explicit reasoning as a context sensitive resource driven by discourse and temporal cues. TIME augments dialogue with optional ISO 8601 <time> tags, tick turns that represent silent gaps, and short <think> blocks that can appear anywhere in a reply. A four-phase curriculum including a small, maximally diverse full-batch alignment step trains Qwen3 dense models to invoke brief, in-place reasoning bursts and keep user facing text compact. We evaluate with TIMEBench, a temporally grounded dialogue benchmark probing chronology, commonsense under gaps and offsets, anomaly detection, and continuity. Across 4B to 32B scales, TIME improves TIMEBench scores over base Qwen3 in both thinking and no-thinking modes while reducing reasoning tokens by about an order of magnitude. Our training data and code are available at https://github.com/The-Coherence-Initiative/TIME and TIMEBench is available at https://github.com/The-Coherence-Initiative/TIMEBench

</details>


### [52] [When the Server Steps In: Calibrated Updates for Fair Federated Learning](https://arxiv.org/abs/2601.05352)
*Tianrun Yu,Kaixiang Zhao,Cheng Zhang,Anjun Gao,Yueyang Quan,Zhuqing Liu,Minghong Fang*

Main category: cs.LG

TL;DR: EquFL是一种新颖的服务器端去偏方法，通过生成校准更新来减少联邦学习系统中的偏见，无需修改客户端训练协议，同时保持与FedAvg相似的收敛性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然具有分布式学习的优势，但在确保不同人口群体间的公平性方面面临挑战。现有的公平性去偏方法要么需要修改客户端训练协议，要么缺乏灵活的聚合策略，因此需要一种更灵活、无需修改客户端协议的服务器端去偏方法。

Method: EquFL是一种服务器端去偏方法，在接收到客户端模型更新后，服务器生成单个校准更新，然后将这个校准更新与聚合的客户端更新相结合，生成调整后的全局模型以减少偏见。

Result: 理论上证明EquFL能够收敛到FedAvg达到的最优全局模型，并有效减少训练轮次中的公平性损失。实证结果表明EquFL显著减轻了系统中的偏见，展示了其实际有效性。

Conclusion: EquFL提供了一种灵活且有效的服务器端去偏方法，能够在不修改客户端训练协议的情况下减少联邦学习系统中的偏见，同时保持与标准联邦学习算法相似的收敛性能。

Abstract: Federated learning (FL) has emerged as a transformative distributed learning paradigm, enabling multiple clients to collaboratively train a global model under the coordination of a central server without sharing their raw training data. While FL offers notable advantages, it faces critical challenges in ensuring fairness across diverse demographic groups. To address these fairness concerns, various fairness-aware debiasing methods have been proposed. However, many of these approaches either require modifications to clients' training protocols or lack flexibility in their aggregation strategies. In this work, we address these limitations by introducing EquFL, a novel server-side debiasing method designed to mitigate bias in FL systems. EquFL operates by allowing the server to generate a single calibrated update after receiving model updates from the clients. This calibrated update is then integrated with the aggregated client updates to produce an adjusted global model that reduces bias. Theoretically, we establish that EquFL converges to the optimal global model achieved by FedAvg and effectively reduces fairness loss over training rounds. Empirically, we demonstrate that EquFL significantly mitigates bias within the system, showcasing its practical effectiveness.

</details>


### [53] [The Kernel Manifold: A Geometric Approach to Gaussian Process Model Selection](https://arxiv.org/abs/2601.05371)
*Md Shafiqul Islam,Shakti Prasad Padhy,Douglas Allaire,Raymundo Arróyave*

Main category: cs.LG

TL;DR: 提出基于核几何的贝叶斯优化框架，通过多维缩放将离散核库嵌入连续欧几里得流形，实现高效核搜索，在合成基准、时间序列和增材制造案例中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 高斯过程回归的性能严重依赖于协方差核的选择，但核选择是概率建模中最具挑战性和计算成本最高的步骤之一，需要更高效的核搜索方法。

Method: 基于核几何的贝叶斯优化框架，使用基于期望散度的距离度量高斯过程先验之间的差异，通过多维缩放将离散核库嵌入连续欧几里得流形，将核组合作为输入空间，对数边际似然作为目标函数。

Result: 在合成基准测试、真实世界时间序列数据集和增材制造案例研究中，该方法在预测精度和不确定性校准方面优于包括LLM引导搜索在内的基线方法。

Conclusion: 该框架为核搜索建立了可重用的概率几何结构，对高斯过程建模和深度核学习具有直接相关性，提供了一种高效、稳定的核选择方法。

Abstract: Gaussian Process (GP) regression is a powerful nonparametric Bayesian framework, but its performance depends critically on the choice of covariance kernel. Selecting an appropriate kernel is therefore central to model quality, yet remains one of the most challenging and computationally expensive steps in probabilistic modeling. We present a Bayesian optimization framework built on kernel-of-kernels geometry, using expected divergence-based distances between GP priors to explore kernel space efficiently. A multidimensional scaling (MDS) embedding of this distance matrix maps a discrete kernel library into a continuous Euclidean manifold, enabling smooth BO. In this formulation, the input space comprises kernel compositions, the objective is the log marginal likelihood, and featurization is given by the MDS coordinates. When the divergence yields a valid metric, the embedding preserves geometry and produces a stable BO landscape. We demonstrate the approach on synthetic benchmarks, real-world time-series datasets, and an additive manufacturing case study predicting melt-pool geometry, achieving superior predictive accuracy and uncertainty calibration relative to baselines including Large Language Model (LLM)-guided search. This framework establishes a reusable probabilistic geometry for kernel search, with direct relevance to GP modeling and deep kernel learning.

</details>


### [54] [Imitation Learning for Combinatorial Optimisation under Uncertainty](https://arxiv.org/abs/2601.05383)
*Prakash Gawas,Antoine Legrain,Louis-Martin Rousseau*

Main category: cs.LG

TL;DR: 本文系统分类了组合优化中模仿学习的专家类型，提出了支持多专家查询和交互策略的广义DAgger算法，并在动态医生分配问题上验证了随机专家优于确定性专家，交互学习能减少演示需求。


<details>
  <summary>Details</summary>
Motivation: 模仿学习为大规模组合优化问题提供了数据驱动的解决方案框架，但现有研究对专家构造缺乏统一的理论框架来系统描述其建模假设、计算特性和对学习性能的影响。

Method: 提出了组合优化中模仿学习专家的系统分类法，从三个维度分类专家：不确定性处理方式、最优性水平、与学习器的交互模式。基于此分类法，提出了支持多专家查询、专家聚合和灵活交互策略的广义DAgger算法。

Result: 在动态医生-患者分配问题上的实验表明：从随机专家学习的策略始终优于从确定性或完全信息专家学习的策略；交互学习能以更少的专家演示获得更好的解质量；当随机优化计算困难时，聚合的确定性专家是有效替代方案。

Conclusion: 本文为组合优化中的模仿学习专家提供了系统的分类框架，证明了随机专家和交互学习策略的优势，为实际应用中的专家选择提供了指导，特别是当随机优化计算困难时聚合确定性专家是可行的替代方案。

Abstract: Imitation learning (IL) provides a data-driven framework for approximating policies for large-scale combinatorial optimisation problems formulated as sequential decision problems (SDPs), where exact solution methods are computationally intractable. A central but underexplored aspect of IL in this context is the role of the \emph{expert} that generates training demonstrations. Existing studies employ a wide range of expert constructions, yet lack a unifying framework to characterise their modelling assumptions, computational properties, and impact on learning performance.
  This paper introduces a systematic taxonomy of experts for IL in combinatorial optimisation under uncertainty. Experts are classified along three dimensions: (i) their treatment of uncertainty, including myopic, deterministic, full-information, two-stage stochastic, and multi-stage stochastic formulations; (ii) their level of optimality, distinguishing task-optimal and approximate experts; and (iii) their interaction mode with the learner, ranging from one-shot supervision to iterative, interactive schemes. Building on this taxonomy, we propose a generalised Dataset Aggregation (DAgger) algorithm that supports multiple expert queries, expert aggregation, and flexible interaction strategies.
  The proposed framework is evaluated on a dynamic physician-to-patient assignment problem with stochastic arrivals and capacity constraints. Computational experiments compare learning outcomes across expert types and interaction regimes. The results show that policies learned from stochastic experts consistently outperform those learned from deterministic or full-information experts, while interactive learning improves solution quality using fewer expert demonstrations. Aggregated deterministic experts provide an effective alternative when stochastic optimisation becomes computationally challenging.

</details>


### [55] [Interactive Distillation for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.05407)
*Minwoo Cho,Batuhan Altundas,Matthew Gombolay*

Main category: cs.LG

TL;DR: HINT是一个用于多智能体强化学习的知识蒸馏框架，通过分层交互式教师机制解决传统KD在MARL中的三个关键瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏在MARL中存在三个主要瓶颈：1) 复杂领域中难以合成高性能教学策略；2) 教师需要在分布外状态进行推理的困难；3) 分散学生与集中教师观察空间不匹配的问题。

Method: 提出HINT框架：1) 利用分层RL提供可扩展的高性能教师；2) 引入伪离策略RL，使教师策略能够同时使用教师和学生经验进行更新，改善OOD适应；3) 应用基于性能的过滤，仅保留与结果相关的指导，减少观察不匹配。

Result: 在FireCommander（资源分配）和MARINE（战术战斗）等挑战性合作领域进行评估，HINT优于基线方法，成功率提高了60%到165%。

Conclusion: HINT通过分层交互式教师框架有效解决了MARL中知识蒸馏的关键瓶颈，在复杂合作任务中显著提升了性能。

Abstract: Knowledge distillation (KD) has the potential to accelerate MARL by employing a centralized teacher for decentralized students but faces key bottlenecks. Specifically, there are (1) challenges in synthesizing high-performing teaching policies in complex domains, (2) difficulties when teachers must reason in out-of-distribution (OOD) states, and (3) mismatches between the decentralized students' and the centralized teacher's observation spaces. To address these limitations, we propose HINT (Hierarchical INteractive Teacher-based transfer), a novel KD framework for MARL in a centralized training, decentralized execution setup. By leveraging hierarchical RL, HINT provides a scalable, high-performing teacher. Our key innovation, pseudo off-policy RL, enables the teacher policy to be updated using both teacher and student experience, thereby improving OOD adaptation. HINT also applies performance-based filtering to retain only outcome-relevant guidance, reducing observation mismatches. We evaluate HINT on challenging cooperative domains (e.g., FireCommander for resource allocation, MARINE for tactical combat). Across these benchmarks, HINT outperforms baselines, achieving improvements of 60% to 165% in success rate.

</details>


### [56] [Efficient Inference for Noisy LLM-as-a-Judge Evaluation](https://arxiv.org/abs/2601.05420)
*Yiqun T Chen,Sizhu Lu,Sijia Li,Moran Guo,Shengyi Li*

Main category: cs.LG

TL;DR: 本文系统研究并统一了两种纠正LLM作为评估者偏差的方法：测量误差校正和预测驱动推断，通过半参数效率理论推导出高效估计器，并比较了它们的性能。


<details>
  <summary>Details</summary>
Motivation: LLM作为自动评估者存在系统性非随机误差，现有两种主要纠偏方法（测量误差校正和预测驱动推断）缺乏系统比较和理论统一，需要建立理论框架来理解它们的性能差异。

Method: 利用半参数效率理论工具，推导出基于高效影响函数的高效估计器，统一两类估计方法，理论分析预测驱动推断在何种条件下具有更小的渐近方差，并通过仿真和真实数据验证。

Result: 理论分析表明预测驱动推断在某些条件下能达到比测量误差校正更小的渐近方差，仿真和真实数据验证了这一理论结果，提供了方法实现和比较工具。

Conclusion: 本文建立了LLM评估偏差校正的统一理论框架，揭示了不同方法的性能差异条件，为实践中选择合适的纠偏方法提供了理论指导。

Abstract: Large language models (LLMs) are increasingly used as automatic evaluators of generative AI outputs, a paradigm often referred to as "LLM-as-a-judge." In practice, LLM judges are imperfect predictions for the underlying truth and can exhibit systematic, non-random errors. Two main approaches have recently been proposed to address this issue: (i) direct measurementerror correction based on misclassification models such as Rogan-Gladen-style estimators, and (ii) surrogate-outcome approaches such as prediction-powered inference (PPI), which correct bias by calibrating prediction residuals on a small set of gold-standard human labels. In this paper, we systematically study the performance of these two approaches for estimating mean parameters (e.g., average benchmark scores or pairwise win rates). Leveraging tools from semiparametric efficiency theory, we unify the two classes of estimators by deriving explicit forms of efficient influence function (EIF)-based efficient estimators and characterize conditions under which PPI-style estimators attain strictly smaller asymptotic variance than measurement-error corrections. We verify our theoretical results in simulations and demonstrate the methods on real-data examples. We provide an implementation of the benchmarked methods and comparison utilities at https://github.com/yiqunchen/debias-llm-as-a-judge.

</details>


### [57] [Prediction of Fault Slip Tendency in CO${_2}$ Storage using Data-space Inversion](https://arxiv.org/abs/2601.05431)
*Xiaowen He,Su Jiang,Louis J. Durlofsky*

Main category: cs.LG

TL;DR: 提出基于变分自编码器（VAE）的数据空间反演框架，用于预测CO₂封存项目中的压力、应力、应变场和断层滑移趋势，无需生成后验地质模型。


<details>
  <summary>Details</summary>
Motivation: 在涉及断层的耦合流动-地质力学问题中，传统的基于模型的历史匹配方法难以应用。需要开发一种能够准确评估断层滑移潜力且计算高效的方法。

Method: 采用VAE-数据空间反演（DSI）框架：1）生成O(1000)个先验地质模型；2）使用GEOS进行耦合流动-地质力学模拟；3）训练具有堆叠卷积LSTM层的VAE，将压力、应变、有效正应力和剪应力场表示为潜变量；4）结合监测井观测数据，使用VAE参数化进行DSI后验预测。

Result: DSI-VAE框架能够准确预测压力、应变、应力场和断层滑移趋势，并能减少关键地质力学和断层参数的不确定性。

Conclusion: VAE-based DSI框架为CO₂封存等地下作业中的断层滑移评估提供了一种有效方法，避免了传统后验地质模型生成的困难，实现了准确预测和不确定性量化。

Abstract: Accurately assessing the potential for fault slip is essential in many subsurface operations. Conventional model-based history matching methods, which entail the generation of posterior geomodels calibrated to observed data, can be challenging to apply in coupled flow-geomechanics problems with faults. In this work, we implement a variational autoencoder (VAE)-based data-space inversion (DSI) framework to predict pressure, stress and strain fields, and fault slip tendency, in CO${_2}$ storage projects. The main computations required by the DSI workflow entail the simulation of O(1000) prior geomodels. The posterior distributions for quantities of interest are then inferred directly from prior simulation results and observed data, without the need to generate posterior geomodels. The model used here involves a synthetic 3D system with two faults. Realizations of heterogeneous permeability and porosity fields are generated using geostatistical software, and uncertain geomechanical and fault parameters are sampled for each realization from prior distributions. Coupled flow-geomechanics simulations for these geomodels are conducted using GEOS. A VAE with stacked convolutional long short-term memory layers is trained, using the prior simulation results, to represent pressure, strain, effective normal stress and shear stress fields in terms of latent variables. The VAE parameterization is used with DSI for posterior predictions, with monitoring wells providing observed pressure and strain data. Posterior results for synthetic true models demonstrate that the DSI-VAE framework gives accurate predictions for pressure, strain, and stress fields and for fault slip tendency. The framework is also shown to reduce uncertainty in key geomechanical and fault parameters.

</details>


### [58] [RingSQL: Generating Synthetic Data with Schema-Independent Templates for Text-to-SQL Reasoning Models](https://arxiv.org/abs/2601.05451)
*Marko Sterbentz,Kevin Cushing,Cameron Barrie,Kristian J. Hammond*

Main category: cs.LG

TL;DR: RingSQL是一个混合数据生成框架，结合了模式无关的查询模板和基于LLM的自然语言问题改写，为文本到SQL系统生成高质量训练数据。


<details>
  <summary>Details</summary>
Motivation: 文本到SQL系统的进展受限于高质量训练数据的稀缺性。手动创建数据成本高，现有合成方法在可靠性和可扩展性之间存在权衡：基于模板的方法能确保SQL正确性但需要模式特定模板，而基于LLM的生成方法易于扩展但缺乏质量和正确性保证。

Method: RingSQL采用混合数据生成框架，结合了模式无关的查询模板和基于LLM的自然语言问题改写。这种方法在保持跨不同模式的SQL正确性的同时，提供了广泛的语言多样性。

Result: 使用RingSQL生成的数据训练的模型，在六个文本到SQL基准测试中平均准确率提升了+2.3%，相比使用其他合成数据训练的模型。

Conclusion: RingSQL框架通过结合模板方法的可靠性和LLM方法的可扩展性，有效解决了文本到SQL训练数据生成中的质量与规模权衡问题，显著提升了模型性能。

Abstract: Recent advances in text-to-SQL systems have been driven by larger models and improved datasets, yet progress is still limited by the scarcity of high-quality training data. Manual data creation is expensive, and existing synthetic methods trade off reliability and scalability. Template-based approaches ensure correct SQL but require schema-specific templates, while LLM-based generation scales easily but lacks quality and correctness guarantees. We introduce RingSQL, a hybrid data generation framework that combines schema-independent query templates with LLM-based paraphrasing of natural language questions. This approach preserves SQL correctness across diverse schemas while providing broad linguistic variety. In our experiments, we find that models trained using data produced by RingSQL achieve an average gain in accuracy of +2.3% across six text-to-SQL benchmarks when compared to models trained on other synthetic data. We make our code available at https://github.com/nu-c3lab/RingSQL.

</details>


### [59] [Efficient Differentiable Causal Discovery via Reliable Super-Structure Learning](https://arxiv.org/abs/2601.05474)
*Pingchuan Ma,Qixin Zhang,Shuai Wang,Dacheng Tao*

Main category: cs.LG

TL;DR: ALVGL是一种增强可微分因果发现的新方法，通过稀疏低秩分解学习数据的精度矩阵，构建包含真实因果图的超结构，从而缩小搜索空间并提高优化效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的可微分因果发现方法在处理高维数据或存在潜在混杂因素的数据时面临挑战，包括搜索空间过大、目标函数复杂以及图理论约束的非平凡性。虽然利用超结构指导优化过程受到关注，但如何在不同设置下高效学习适当粒度的超结构仍存在显著困难。

Method: ALVGL采用稀疏低秩分解学习数据的精度矩阵，设计ADMM优化过程来识别与底层因果结构最相关的成分，将这些成分组合构建一个可证明包含真实因果图的超结构，然后用该超结构初始化标准的可微分因果发现方法，实现更聚焦的搜索空间。

Result: 在合成和真实数据集上的广泛实验表明，ALVGL不仅达到了最先进的准确性，而且显著提高了优化效率，适用于高斯和非高斯设置，无论是否存在未测量的混杂因素。

Conclusion: ALVGL为可微分因果发现提供了一个可靠有效的解决方案，通过构建包含真实因果图的超结构来指导优化过程，在提高准确性的同时显著改善优化效率，展现了在不同结构因果模型中的通用性。

Abstract: Recently, differentiable causal discovery has emerged as a promising approach to improve the accuracy and efficiency of existing methods. However, when applied to high-dimensional data or data with latent confounders, these methods, often based on off-the-shelf continuous optimization algorithms, struggle with the vast search space, the complexity of the objective function, and the nontrivial nature of graph-theoretical constraints. As a result, there has been a surge of interest in leveraging super-structures to guide the optimization process. Nonetheless, learning an appropriate super-structure at the right level of granularity, and doing so efficiently across various settings, presents significant challenges.
  In this paper, we propose ALVGL, a novel and general enhancement to the differentiable causal discovery pipeline. ALVGL employs a sparse and low-rank decomposition to learn the precision matrix of the data. We design an ADMM procedure to optimize this decomposition, identifying components in the precision matrix that are most relevant to the underlying causal structure. These components are then combined to construct a super-structure that is provably a superset of the true causal graph. This super-structure is used to initialize a standard differentiable causal discovery method with a more focused search space, thereby improving both optimization efficiency and accuracy.
  We demonstrate the versatility of ALVGL by instantiating it across a range of structural causal models, including both Gaussian and non-Gaussian settings, with and without unmeasured confounders. Extensive experiments on synthetic and real-world datasets show that ALVGL not only achieves state-of-the-art accuracy but also significantly improves optimization efficiency, making it a reliable and effective solution for differentiable causal discovery.

</details>


### [60] [MaxCode: A Max-Reward Reinforcement Learning Framework for Automated Code Optimization](https://arxiv.org/abs/2601.05475)
*Jiefu Ou,Sapana Chaudhary,Kaj Bostrom,Nathaniel Weir,Shuai Zhang,Huzefa Rangwala,George Karypis*

Main category: cs.LG

TL;DR: MaxCode是一个基于最大奖励强化学习框架的推理时搜索算法，通过执行反馈和自然语言批评模型来指导LLM迭代优化代码性能，在CUDA和C++优化基准测试中取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在通用编码任务中表现出色，但在代码优化方面面临两大挑战：1）编写优化代码（如高性能CUDA内核和竞赛级CPU代码）需要系统、算法和特定语言的专业知识；2）需要解释性能指标（如计时和设备利用率），而不仅仅是二进制正确性。

Method: MaxCode采用最大奖励强化学习框架，统一现有搜索方法，使观察和动作价值函数模块化可修改。通过集成自然语言批评模型将原始执行反馈转换为诊断见解，并使用生成式奖励到目标模型来重新排序潜在解决方案以改进搜索探索。

Result: 在KernelBench（CUDA）和PIE（C++）优化基准测试中，MaxCode相比基线方法在绝对加速值和相对加速排名方面分别实现了20.3%和10.1%的相对改进。

Conclusion: MaxCode通过推理时搜索算法和强化学习框架，结合自然语言反馈和奖励模型，有效提升了LLM在代码优化任务中的性能，为解决复杂代码优化问题提供了新方法。

Abstract: Large Language Models (LLMs) demonstrate strong capabilities in general coding tasks but encounter two key challenges when optimizing code: (i) the complexity of writing optimized code (such as performant CUDA kernels and competition-level CPU code) requires expertise in systems, algorithms and specific languages and (ii) requires interpretation of performance metrics like timing and device utilization beyond binary correctness. In this work, we explore inference-time search algorithms that guide the LLM to discover better solutions through iterative refinement based on execution feedback. Our approach, called MaxCode unifies existing search methods under a max-reward reinforcement learning framework, making the observation and action-value functions modular for modification. To enhance the observation space, we integrate a natural language critique model that converts raw execution feedback into diagnostic insights about errors and performance bottlenecks, and the best-discounted reward seen so far. Together, these provide richer input to the code proposal function. To improve exploration during search, we train a generative reward-to-go model using action values from rollouts to rerank potential solutions. Testing on the KernelBench (CUDA) and PIE (C++) optimization benchmarks shows that MaxCode improves optimized code performance compared to baselines, achieving 20.3% and 10.1% relative improvements in absolute speedup value and relative speedup ranking, respectively.

</details>


### [61] [Hi-ZFO: Hierarchical Zeroth- and First-Order LLM Fine-Tuning via Importance-Guided Tensor Selection](https://arxiv.org/abs/2601.05501)
*Feihu Jin,Ying Tan*

Main category: cs.LG

TL;DR: Hi-ZFO是一种分层混合优化框架，结合零阶和一阶方法，通过重要性分析自适应分配优化策略，在LLM微调中实现更好的探索性和收敛性。


<details>
  <summary>Details</summary>
Motivation: 传统一阶优化方法容易陷入尖锐、泛化性差的局部最小值，而零阶方法虽然探索性强但收敛慢，特别是在生成任务中，巨大的输出和搜索空间会显著增加估计方差，使零阶方法既嘈杂又低效。

Method: 提出Hi-ZFO分层混合优化框架：通过层重要性分析自适应划分模型，对关键层使用精确的一阶梯度更新，对不敏感层使用零阶优化。零阶方法不仅作为内存节省的替代方案，更被有意引入作为"有益随机性"的来源，帮助模型逃离纯一阶优化容易停滞的局部最小值。

Result: 在多种生成、数学和代码推理任务上验证，Hi-ZFO始终实现更优性能，同时显著减少训练时间。

Conclusion: 分层混合优化对于LLM微调是有效的，能够协同一阶梯度的精确性和零阶估计的探索能力。

Abstract: Fine-tuning large language models (LLMs) using standard first-order (FO) optimization often drives training toward sharp, poorly generalizing minima. Conversely, zeroth-order (ZO) methods offer stronger exploratory behavior without relying on explicit gradients, yet suffer from slow convergence. More critically, our analysis reveals that in generative tasks, the vast output and search space significantly amplify estimation variance, rendering ZO methods both noisy and inefficient. To address these challenges, we propose \textbf{Hi-ZFO} (\textbf{Hi}erarchical \textbf{Z}eroth- and \textbf{F}irst-\textbf{O}rder optimization), a hybrid framework designed to synergize the precision of FO gradients with the exploratory capability of ZO estimation. Hi-ZFO adaptively partitions the model through layer-wise importance profiling, applying precise FO updates to critical layers while leveraging ZO optimization for less sensitive ones. Notably, ZO in Hi-ZFO is not merely a memory-saving surrogate; it is intentionally introduced as a source of "beneficial stochasticity" to help the model escape the local minima where pure FO optimization tends to stagnate. Validated across diverse generative, mathematical, and code reasoning tasks, Hi-ZFO consistently achieves superior performance while significantly reducing the training time. These results demonstrate the effectiveness of hierarchical hybrid optimization for LLM fine-tuning.

</details>


### [62] [Over-Searching in Search-Augmented Large Language Models](https://arxiv.org/abs/2601.05503)
*Roy Xie,Deepak Gopinath,David Qiu,Dong Lin,Haitian Sun,Saloni Potdar,Bhuwan Dhingra*

Main category: cs.LG

TL;DR: 该论文系统评估了检索增强大语言模型中的过度搜索问题，发现搜索虽然能提高可回答问题的准确性，但会损害不可回答问题的弃权能力，并提出了TPC指标来量化性能-成本权衡。


<details>
  <summary>Details</summary>
Motivation: 检索增强大语言模型在知识密集型任务中表现出色，但经常出现过度搜索问题——即使搜索不会改善响应质量也调用搜索工具，导致计算效率低下和因包含无关上下文而产生幻觉。

Method: 对过度搜索进行多维度系统评估，包括查询类型、模型类别、检索条件和多轮对话；引入TPC（Tokens Per Correctness）指标量化性能-成本权衡；在查询和检索层面研究缓解方法；发布OverSearchQA数据集。

Result: 搜索能提高可回答查询的答案准确性，但会损害不可回答查询的弃权能力；过度搜索在复杂推理模型和深度研究系统中更明显，受噪声检索加剧，并在多轮对话中累积；检索证据的组成很重要，负面证据的存在能改善弃权能力。

Conclusion: 过度搜索是检索增强LLMs中一个普遍且代价高昂的问题，需要更好的搜索决策机制；TPC指标能有效评估性能-成本权衡；在查询和检索层面的缓解方法有潜力；OverSearchQA数据集将促进高效检索增强LLMs的持续研究。

Abstract: Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.

</details>


### [63] [Toward an Integrated Cross-Urban Accident Prevention System: A Multi-Task Spatial-Temporal Learning Framework for Urban Safety Management](https://arxiv.org/abs/2601.05521)
*Jiayu Fang,Zhiqi Shao,Haoning Xi,Boris Choy,Junbin Gao*

Main category: cs.LG

TL;DR: MLA-STNet是一个跨城市事故预防系统，通过多任务学习整合异质城市数据，使用Mamba注意力机制处理时空依赖和城市异质性，在纽约和芝加哥数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 城市事故数据存在异质性、报告不一致、聚类稀疏、周期性和噪声等问题，加上碎片化治理和不兼容的报告标准，阻碍了跨城市事故预防框架的发展，需要统一系统来解决这些挑战。

Method: 提出MLA-STNet系统，将事故风险预测构建为跨城市多任务学习问题。包含两个互补模块：STG-MA（时空地理Mamba注意力）抑制不稳定时空波动并增强长程时间依赖；STS-MA（时空语义Mamba注意力）通过共享参数设计缓解城市异质性，同时保留个体语义表示空间。

Result: 在纽约和芝加哥真实数据集上进行75次实验，涵盖全天和高频事故时段两种预测场景。相比现有最佳基线，MLA-STNet实现RMSE降低6%、召回率提高8%、MAP提高5%，在50%输入噪声下性能变化小于1%。

Conclusion: MLA-STNet有效地将异质城市数据集统一到可扩展、鲁棒且可解释的跨城市事故预防系统中，为协调和数据驱动的城市安全管理铺平了道路。

Abstract: The development of a cross-city accident prevention system is particularly challenging due to the heterogeneity, inconsistent reporting, and inherently clustered, sparse, cyclical, and noisy nature of urban accident data. These intrinsic data properties, combined with fragmented governance and incompatible reporting standards, have long hindered the creation of an integrated, cross-city accident prevention framework. To address this gap, we propose the Mamba Local-ttention Spatial-Temporal Network MLA-STNet, a unified system that formulates accident risk prediction as a multi-task learning problem across multiple cities. MLA-STNet integrates two complementary modules: (i)the Spatio-Temporal Geographical Mamba-Attention (STG-MA), which suppresses unstable spatio-temporal fluctuations and strengthens long-range temporal dependencies; and (ii) the Spatio-Temporal Semantic Mamba-Attention (STS-MA), which mitigates cross-city heterogeneity through a shared-parameter design that jointly trains all cities while preserving individual semantic representation spaces. We validate the proposed framework through 75 experiments under two forecasting scenarios, full-day and high-frequency accident periods, using real-world datasets from New York City and Chicago. Compared with the state-of-the-art baselines, MLA-STNet achieves up to 6% lower RMSE, 8% higher Recall, and 5% higher MAP, while maintaining less than 1% performance variation under 50% input noise. These results demonstrate that MLA-STNet effectively unifies heterogeneous urban datasets within a scalable, robust, and interpretable Cross-City Accident Prevention System, paving the way for coordinated and data-driven urban safety management.

</details>


### [64] [Buffered AUC maximization for scoring systems via mixed-integer optimization](https://arxiv.org/abs/2601.05544)
*Moe Shiina,Shunnosuke Ikeda,Yuichi Takano*

Main category: cs.LG

TL;DR: 本文提出了一种基于混合整数线性优化的评分系统构建方法，直接最大化缓冲AUC（bAUC）作为AUC的最紧凹下界，相比传统正则化和逐步回归方法能获得更好的AUC性能。


<details>
  <summary>Details</summary>
Motivation: 评分系统作为具有少量解释变量和整数系数的线性分类器，具有高度可解释性且无需计算器即可手动计算。虽然已有研究使用混合整数优化技术开发评分系统，但都未直接最大化AUC这一评分系统的重要评估指标。

Method: 建立了一个有效的混合整数优化框架，直接最大化缓冲AUC（bAUC）作为AUC的最紧凹下界。优化模型被表述为混合整数线性优化问题，在限制评分系统问题数量的组稀疏约束下最大化bAUC。

Result: 使用公开可用的真实世界数据集进行的计算实验表明，与基于正则化和逐步回归的基线方法相比，本文的混合整数线性优化方法能够构建具有更优AUC值的评分系统。

Conclusion: 这项研究有助于推进混合整数优化技术在开发高度可解释分类模型方面的应用，为直接优化AUC的评分系统构建提供了有效框架。

Abstract: A scoring system is a linear classifier composed of a small number of explanatory variables, each assigned a small integer coefficient. This system is highly interpretable and allows predictions to be made with simple manual calculations without the need for a calculator. Several previous studies have used mixed-integer optimization (MIO) techniques to develop scoring systems for binary classification; however, they have not focused on directly maximizing AUC (i.e., area under the receiver operating characteristic curve), even though AUC is recognized as an essential evaluation metric for scoring systems. Our goal herein is to establish an effective MIO framework for constructing scoring systems that directly maximize the buffered AUC (bAUC) as the tightest concave lower bound on AUC. Our optimization model is formulated as a mixed-integer linear optimization (MILO) problem that maximizes bAUC subject to a group sparsity constraint for limiting the number of questions in the scoring system. Computational experiments using publicly available real-world datasets demonstrate that our MILO method can build scoring systems with superior AUC values compared to the baseline methods based on regularization and stepwise regression. This research contributes to the advancement of MIO techniques for developing highly interpretable classification models.

</details>


### [65] [Learn to Evolve: Self-supervised Neural JKO Operator for Wasserstein Gradient Flow](https://arxiv.org/abs/2601.05583)
*Xue Feng,Li Wang,Deanna Needell,Rongjie Lai*

Main category: cs.LG

TL;DR: 提出一种自监督学习方法，无需数值求解JKO子问题即可学习JKO解算子，通过交替轨迹生成和算子更新的Learn-to-Evolve算法实现高效Wasserstein梯度流计算。


<details>
  <summary>Details</summary>
Motivation: JKO方案为计算Wasserstein梯度流提供了稳定的变分框架，但实际应用受到重复求解JKO子问题高计算成本的限制。需要一种能直接学习JKO解算子而不依赖数值解的方法。

Method: 提出Learn-to-Evolve算法：1) 学习将输入密度直接映射到对应JKO子问题最小化解的算子；2) 通过交替轨迹生成和算子更新联合学习JKO算子及其诱导轨迹；3) 利用生成的数据作为数据增强，提升算子泛化能力。

Result: 数值实验表明该方法在不同能量函数和初始条件下都具有准确性、稳定性和鲁棒性。生成的轨迹逐渐逼近真实JKO轨迹，学习到的算子能高效生成梯度流演化。

Conclusion: 提出的自监督学习方法成功克服了传统JKO方案的计算瓶颈，通过Learn-to-Evolve策略实现了高效、准确的Wasserstein梯度流计算，为相关应用提供了实用工具。

Abstract: The Jordan-Kinderlehrer-Otto (JKO) scheme provides a stable variational framework for computing Wasserstein gradient flows, but its practical use is often limited by the high computational cost of repeatedly solving the JKO subproblems. We propose a self-supervised approach for learning a JKO solution operator without requiring numerical solutions of any JKO trajectories. The learned operator maps an input density directly to the minimizer of the corresponding JKO subproblem, and can be iteratively applied to efficiently generate the gradient-flow evolution. A key challenge is that only a number of initial densities are typically available for training. To address this, we introduce a Learn-to-Evolve algorithm that jointly learns the JKO operator and its induced trajectories by alternating between trajectory generation and operator updates. As training progresses, the generated data increasingly approximates true JKO trajectories. Meanwhile, this Learn-to-Evolve strategy serves as a natural form of data augmentation, significantly enhancing the generalization ability of the learned operator. Numerical experiments demonstrate the accuracy, stability, and robustness of the proposed method across various choices of energies and initial conditions.

</details>


### [66] [PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning](https://arxiv.org/abs/2601.05593)
*Jingcheng Hu,Yinmin Zhang,Shijie Shang,Xiaobo Yang,Yue Peng,Zhewei Huang,Hebin Zhou,Xin Wu,Jie Cheng,Fanqi Wan,Xiangwen Kong,Chengyuan Yao,Kaiwen Yan,Ailin Huang,Hongyu Zhou,Qi Han,Zheng Ge,Daxin Jiang,Xiangyu Zhang,Heung-Yeung Shum*

Main category: cs.LG

TL;DR: PaCoRe是一种训练-推理框架，通过并行协调推理突破语言模型测试时计算限制，实现多轮并行探索和消息传递，在数学推理等任务上超越前沿系统


<details>
  <summary>Details</summary>
Motivation: 解决当代语言模型的核心限制：无法在固定上下文窗口下扩展测试时计算，超越顺序推理的局限

Method: 采用并行协调推理框架，通过消息传递架构进行多轮并行探索，每轮启动多个并行推理轨迹，压缩结果为上下文有界消息，合成这些消息指导下一轮并生成最终答案

Result: 在多个领域取得显著改进，特别是在数学推理上超越前沿系统：8B模型在HMMT 2025上达到94.5%，超过GPT-5的93.2%，有效测试时计算扩展到约200万token

Conclusion: PaCoRe框架成功突破了语言模型测试时计算扩展的限制，通过并行协调推理实现了超越顺序推理的性能，为后续研究提供了开源模型、训练数据和完整推理管道

Abstract: We introduce Parallel Coordinated Reasoning (PaCoRe), a training-and-inference framework designed to overcome a central limitation of contemporary language models: their inability to scale test-time compute (TTC) far beyond sequential reasoning under a fixed context window. PaCoRe departs from the traditional sequential paradigm by driving TTC through massive parallel exploration coordinated via a message-passing architecture in multiple rounds. Each round launches many parallel reasoning trajectories, compacts their findings into context-bounded messages, and synthesizes these messages to guide the next round and ultimately produce the final answer. Trained end-to-end with large-scale, outcome-based reinforcement learning, the model masters the synthesis abilities required by PaCoRe and scales to multi-million-token effective TTC without exceeding context limits. The approach yields strong improvements across diverse domains, and notably pushes reasoning beyond frontier systems in mathematics: an 8B model reaches 94.5% on HMMT 2025, surpassing GPT-5's 93.2% by scaling effective TTC to roughly two million tokens. We open-source model checkpoints, training data, and the full inference pipeline to accelerate follow-up work.

</details>


### [67] [Good Allocations from Bad Estimates](https://arxiv.org/abs/2601.05597)
*Sílvia Casacuberta,Moritz Hardt*

Main category: cs.LG

TL;DR: 本文提出了一种比传统CATE估计更高效的样本分配方法，仅需O(M/ε)样本即可实现与CATE相同的总体治疗效果，而传统方法需要O(M/ε²)样本。


<details>
  <summary>Details</summary>
Motivation: 条件平均治疗效果(CATE)估计是处理异质人群的标准方法，但需要大量样本(O(M/ε²))来准确估计所有治疗效果。然而，对于治疗分配这一实际目标，可能不需要如此精确的估计。

Method: 提出了一种新的算法，利用粗糙估计即可实现接近最优的治疗分配。关键洞察是：对于治疗分配而言，粗略估计已足够实现接近最优的结果。此外，还考虑了预算灵活性进一步降低样本复杂度。

Result: 新算法仅需O(M/ε)样本即可实现与CATE相同的总体治疗效果，相比传统方法样本复杂度显著降低。在多个真实世界RCT数据集上的评估显示，该算法能以极少的样本找到接近最优的治疗分配方案。

Conclusion: 治疗效应估计和治疗分配之间存在根本区别：后者需要的样本量远少于前者。本文的工作强调了这一重要区别，并为实际应用中的资源优化分配提供了更高效的解决方案。

Abstract: Conditional average treatment effect (CATE) estimation is the de facto gold standard for targeting a treatment to a heterogeneous population. The method estimates treatment effects up to an error $ε> 0$ in each of $M$ different strata of the population, targeting individuals in decreasing order of estimated treatment effect until the budget runs out. In general, this method requires $O(M/ε^2)$ samples. This is best possible if the goal is to estimate all treatment effects up to an $ε$ error. In this work, we show how to achieve the same total treatment effect as CATE with only $O(M/ε)$ samples for natural distributions of treatment effects. The key insight is that coarse estimates suffice for near-optimal treatment allocations. In addition, we show that budget flexibility can further reduce the sample complexity of allocation. Finally, we evaluate our algorithm on various real-world RCT datasets. In all cases, it finds nearly optimal treatment allocations with surprisingly few samples. Our work highlights the fundamental distinction between treatment effect estimation and treatment allocation: the latter requires far fewer samples.

</details>


### [68] [PiXTime: A Model for Federated Time Series Forecasting with Heterogeneous Data Structures Across Nodes](https://arxiv.org/abs/2601.05613)
*Yiming Zhou,Mingyue Cheng,Hao Wang,Enhong Chen*

Main category: cs.LG

TL;DR: PiXTime是一个为联邦学习设计的时间序列预测模型，能够处理多粒度、异质变量集的时间序列数据，通过个性化嵌入和全局变量对齐实现跨节点有效预测。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据价值高但难以跨节点共享，联邦学习是有效利用分布式时序数据的范式。然而，不同节点的采样标准导致时间粒度和变量集存在差异，阻碍了经典联邦学习的应用。

Method: PiXTime采用个性化Patch Embedding将节点特定粒度的时间序列映射到统一维度的token序列，使用全局VE Table对齐跨节点的变量类别语义，通过基于Transformer的共享模型捕获任意数量变量的辅助序列表示，并利用交叉注意力增强目标序列预测。

Result: 实验表明PiXTime在联邦学习设置下达到了最先进的性能，并在八个广泛使用的真实世界传统基准测试中表现出优越性能。

Conclusion: PiXTime成功解决了联邦学习中时间序列数据多粒度和异质变量集的挑战，为分布式时序数据预测提供了有效的解决方案。

Abstract: Time series are highly valuable and rarely shareable across nodes, making federated learning a promising paradigm to leverage distributed temporal data. However, different sampling standards lead to diverse time granularities and variable sets across nodes, hindering classical federated learning. We propose PiXTime, a novel time series forecasting model designed for federated learning that enables effective prediction across nodes with multi-granularity and heterogeneous variable sets. PiXTime employs a personalized Patch Embedding to map node-specific granularity time series into token sequences of a unified dimension for processing by a subsequent shared model, and uses a global VE Table to align variable category semantics across nodes, thereby enhancing cross-node transferability. With a transformer-based shared model, PiXTime captures representations of auxiliary series with arbitrary numbers of variables and uses cross-attention to enhance the prediction of the target series. Experiments show PiXTime achieves state-of-the-art performance in federated settings and demonstrates superior performance on eight widely used real-world traditional benchmarks.

</details>


### [69] [Towards Realistic Guarantees: A Probabilistic Certificate for SmoothLLM](https://arxiv.org/abs/2511.18721)
*Adarsh Kumarappan,Ayushi Mehrotra*

Main category: cs.LG

TL;DR: 本文针对SmoothLLM防御的k-unstable假设过于严格的问题，提出了更现实的(k, ε)-unstable概率框架，为对抗越狱攻击提供了更可信的安全认证。


<details>
  <summary>Details</summary>
Motivation: SmoothLLM防御虽然提供了对抗越狱攻击的认证保证，但其依赖的k-unstable假设在实践中很少成立，这限制了安全证书的可信度。需要更现实的框架来认证对抗多样化越狱攻击的防御能力。

Method: 引入(k, ε)-unstable概率框架，结合攻击成功的经验模型，推导出SmoothLLM防御概率的新数据驱动下界，提供更可信和实用的安全证书。

Result: 新框架能够为从梯度基攻击(GCG)到语义攻击(PAIR)的多样化越狱攻击提供认证，使从业者能够设置更反映LLM实际行为的认证阈值。

Conclusion: 这项工作为增强LLM抵抗安全对齐被利用的能力提供了实用且理论基础的机制，对安全AI部署至关重要。

Abstract: The SmoothLLM defense provides a certification guarantee against jailbreaking attacks, but it relies on a strict `k-unstable' assumption that rarely holds in practice. This strong assumption can limit the trustworthiness of the provided safety certificate. In this work, we address this limitation by introducing a more realistic probabilistic framework, `(k, $\varepsilon$)-unstable,' to certify defenses against diverse jailbreaking attacks, from gradient-based (GCG) to semantic (PAIR). We derive a new, data-informed lower bound on SmoothLLM's defense probability by incorporating empirical models of attack success, providing a more trustworthy and practical safety certificate. By introducing the notion of (k, $\varepsilon$)-unstable, our framework provides practitioners with actionable safety guarantees, enabling them to set certification thresholds that better reflect the real-world behavior of LLMs. Ultimately, this work contributes a practical and theoretically-grounded mechanism to make LLMs more resistant to the exploitation of their safety alignments, a critical challenge in secure AI deployment.

</details>


### [70] [Transformer Is Inherently a Causal Learner](https://arxiv.org/abs/2601.05647)
*Xinyue Wang,Stephen Wang,Biwei Huang*

Main category: cs.LG

TL;DR: Transformer在自回归训练中自然学习到时延因果结构，其梯度敏感性可直接恢复因果图，无需显式因果目标或结构约束


<details>
  <summary>Details</summary>
Motivation: 探索Transformer在自回归训练中是否能够自然学习到时间序列中的因果结构，以及如何利用这种特性进行因果发现

Method: 通过分析Transformer输出对过去输入的梯度敏感性，开发基于聚合梯度归因的因果图提取方法，并在标准可识别性条件下进行理论证明

Result: 该方法在非线性动态、长期依赖和非平稳系统等挑战性场景下显著优于现有因果发现算法，特别是在数据异质性增加时表现更佳，展现出随数据量和异质性增加而提升的扩展潜力

Conclusion: 自回归训练的Transformer自然编码时延因果结构，为因果发现提供了新范式，同时为基础模型通过因果视角获得可解释性和增强奠定了基础

Abstract: We reveal that transformers trained in an autoregressive manner naturally encode time-delayed causal structures in their learned representations. When predicting future values in multivariate time series, the gradient sensitivities of transformer outputs with respect to past inputs directly recover the underlying causal graph, without any explicit causal objectives or structural constraints. We prove this connection theoretically under standard identifiability conditions and develop a practical extraction method using aggregated gradient attributions. On challenging cases such as nonlinear dynamics, long-term dependencies, and non-stationary systems, this approach greatly surpasses the performance of state-of-the-art discovery algorithms, especially as data heterogeneity increases, exhibiting scaling potential where causal accuracy improves with data volume and heterogeneity, a property traditional methods lack. This unifying view lays the groundwork for a future paradigm where causal discovery operates through the lens of foundation models, and foundation models gain interpretability and enhancement through the lens of causality.

</details>


### [71] [Automating Deception: Scalable Multi-Turn LLM Jailbreaks](https://arxiv.org/abs/2511.19517)
*Adarsh Kumarappan,Ananya Mujoo*

Main category: cs.LG

TL;DR: 本文提出了一种自动生成大规模心理驱动多轮越狱数据集的方法，评估了不同LLM对基于"得寸进尺"心理策略的多轮对话攻击的防御能力，发现GPT系列模型对对话历史高度敏感，而Gemini 2.5 Flash表现出卓越的抵抗力。


<details>
  <summary>Details</summary>
Motivation: 多轮对话攻击利用"得寸进尺"等心理学原理绕过LLM的安全对齐机制，构成持续威胁。当前防御研究受限于手动创建难以扩展的数据集，需要自动化、大规模的数据生成方法。

Method: 开发了自动化流水线，将"得寸进尺"技术系统化地转化为可复现的模板，创建了包含1,500个场景的基准数据集，涵盖非法活动和冒犯性内容。评估了来自三个主要LLM家族的七个模型在有无对话历史条件下的表现。

Result: GPT家族模型对对话历史表现出显著脆弱性，攻击成功率最高增加32个百分点。Google的Gemini 2.5 Flash表现出近乎免疫的卓越抵抗力，Anthropic的Claude 3 Haiku显示出强大但不完美的抵抗能力。

Conclusion: 当前安全架构在处理对话上下文方面存在关键差异，需要能够抵抗基于叙事操纵的防御机制。自动化数据生成方法为评估和改进LLM的上下文鲁棒性提供了重要工具。

Abstract: Multi-turn conversational attacks, which leverage psychological principles like Foot-in-the-Door (FITD), where a small initial request paves the way for a more significant one, to bypass safety alignments, pose a persistent threat to Large Language Models (LLMs). Progress in defending against these attacks is hindered by a reliance on manual, hard-to-scale dataset creation. This paper introduces a novel, automated pipeline for generating large-scale, psychologically-grounded multi-turn jailbreak datasets. We systematically operationalize FITD techniques into reproducible templates, creating a benchmark of 1,500 scenarios across illegal activities and offensive content. We evaluate seven models from three major LLM families under both multi-turn (with history) and single-turn (without history) conditions. Our results reveal stark differences in contextual robustness: models in the GPT family demonstrate a significant vulnerability to conversational history, with Attack Success Rates (ASR) increasing by as much as 32 percentage points. In contrast, Google's Gemini 2.5 Flash exhibits exceptional resilience, proving nearly immune to these attacks, while Anthropic's Claude 3 Haiku shows strong but imperfect resistance. These findings highlight a critical divergence in how current safety architectures handle conversational context and underscore the need for defenses that can resist narrative-based manipulation.

</details>


### [72] [From Global to Local: Cluster-Aware Learning for Wi-Fi Fingerprinting Indoor Localisation](https://arxiv.org/abs/2601.05650)
*Miguel Matey-Sanz,Joaquín Torres-Sospedra,Joaquín Huerta,Sergio Trilles*

Main category: cs.LG

TL;DR: 本文提出了一种基于聚类的Wi-Fi指纹室内定位方法，通过聚类预处理指纹数据集，在定位阶段仅使用相关聚类进行定位，从而减少定位误差。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi指纹室内定位面临数据集规模、异构性、信号强度变化以及大型多楼层环境模糊性等挑战，这些因素显著降低了定位精度，特别是在不考虑结构约束的情况下应用全局模型时。

Method: 提出基于聚类的方法，在定位前对指纹数据集进行结构化处理。使用空间或无线电特征对指纹进行分组，聚类可在建筑物或楼层级别应用。在定位阶段，基于最强接入点的聚类估计程序将未见指纹分配到最相关的聚类，然后仅在选定聚类内进行定位。

Result: 在三个公共数据集和多个机器学习模型上评估了该方法的有效性。结果显示定位误差持续减少，特别是在建筑物级别策略下，但代价是降低了楼层检测精度。

Conclusion: 通过聚类显式结构化数据集是室内定位可扩展的有效且灵活的方法。

Abstract: Wi-Fi fingerprinting remains one of the most practical solutions for indoor positioning, however, its performance is often limited by the size and heterogeneity of fingerprint datasets, strong Received Signal Strength Indicator variability, and the ambiguity introduced in large and multi-floor environments. These factors significantly degrade localisation accuracy, particularly when global models are applied without considering structural constraints. This paper introduces a clustering-based method that structures the fingerprint dataset prior to localisation. Fingerprints are grouped using either spatial or radio features, and clustering can be applied at the building or floor level. In the localisation phase, a clustering estimation procedure based on the strongest access points assigns unseen fingerprints to the most relevant cluster. Localisation is then performed only within the selected clusters, allowing learning models to operate on reduced and more coherent subsets of data. The effectiveness of the method is evaluated on three public datasets and several machine learning models. Results show a consistent reduction in localisation errors, particularly under building-level strategies, but at the cost of reducing the floor detection accuracy. These results demonstrate that explicitly structuring datasets through clustering is an effective and flexible approach for scalable indoor positioning.

</details>


### [73] [Tiny Recursive Models on ARC-AGI-1: Inductive Biases, Identity Conditioning, and Test-Time Compute](https://arxiv.org/abs/2512.11847)
*Antonio Roye-Azar,Santiago Vargas-Naranjo,Dhruv Ghai,Nithin Balamurugan,Rayan Amir*

Main category: cs.LG

TL;DR: TRM在ARC-AGI-1任务上的性能主要来自测试时增强、多数投票集成和任务标识符依赖，而非深度递归推理。递归实际上很浅，大部分精度在第一步就达到。TRM相比Llama 3 8B QLoRA微调在吞吐量和内存效率上有优势。


<details>
  <summary>Details</summary>
Motivation: 分析TRM在ARC-AGI-1任务上的实际性能来源，澄清其性能是来自架构优势、测试时计算还是任务特定先验，揭示递归模型的实际推理能力。

Method: 对ARC Prize TRM检查点在ARC-AGI-1上进行实证分析：1) 测试时增强和多数投票集成的影响；2) 任务标识符消融实验；3) 递归轨迹分析；4) 早期训练实验对比不同增强策略；5) 与Llama 3 8B QLoRA微调的效率比较。

Result: 1) 1000样本投票管道比单次推理提升11个百分点；2) 任务标识符替换导致零精度；3) 大部分精度在第一步递归达到，递归深度很浅；4) 强增强策略拓宽候选解分布；5) TRM相比Llama 3 8B QLoRA有更高吞吐量和更低内存使用。

Conclusion: TRM在ARC-AGI-1上的性能主要源于效率优势、任务特定条件编码和激进的测试时计算，而非深度内部推理能力。递归模型的实际推理深度有限。

Abstract: Tiny Recursive Models (TRM) were proposed as a parameter-efficient alternative to large language models for solving Abstraction and Reasoning Corpus (ARC) style tasks. The original work reports strong performance and suggests that recursive latent updates enable non-trivial reasoning, but it remains unclear how much of this performance stems from architecture, test-time compute, or task-specific priors. In this technical note, we empirically analyze the ARC Prize TRM checkpoint on ARC-AGI-1 and report four behavioral findings and an efficiency comparison. First, we show that test-time augmentation and majority-vote ensembling account for a substantial fraction of reported performance: the 1000-sample voting pipeline improves Pass@1 by about 11 percentage points over single-pass canonical inference. Second, a puzzle-identity ablation reveals strict dependence on task identifiers: replacing the correct puzzle ID with a blank or random token yields zero accuracy. Third, a recursion trajectory analysis shows that most of the final accuracy is achieved at the first recursion step and that performance saturates after few latent updates, indicating shallow effective recursion. Fourth, early-stage training experiments under canonical versus heavy augmentation regimes suggest that heavy augmentation broadens the distribution of candidate solutions and improves multi-sample success. Finally, we compare TRM with a naive QLoRA fine-tune of Llama 3 8B on canonical ARC-AGI-1, finding that TRM's non-autoregressive design achieves much higher throughput and substantially lower memory usage in this setting. Overall, TRM's ARC-AGI-1 performance appears to arise from an interaction between efficiency, task-specific conditioning, and aggressive test-time compute rather than deep internal reasoning.

</details>


### [74] [Do Sparse Autoencoders Identify Reasoning Features in Language Models?](https://arxiv.org/abs/2601.05679)
*George Ma,Zhongyuan Liang,Irene Y. Chen,Somayeh Sojoudi*

Main category: cs.LG

TL;DR: 研究发现稀疏自编码器（SAEs）通过对比激活方法识别出的"推理特征"主要捕捉的是推理的语言相关性特征，而非真正的推理计算过程。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证稀疏自编码器（SAEs）是否真正识别出大型语言模型中的推理特征，还是仅仅捕捉到与推理相关的表面语言特征。

Method: 采用证伪导向框架，结合因果标记注入实验和LLM引导的证伪方法，测试特征激活是否反映推理过程还是表面语言相关性。在20种配置（涵盖多个模型系列、层次和推理数据集）中进行实验。

Result: 1）59%至94%的特征对标记级干预高度敏感，少量特征相关标记注入非推理文本即可引发强激活，表明依赖词汇伪影；2）剩余特征中，LLM引导证伪始终能产生激活特征的非推理输入和不激活特征的推理输入；3）没有分析的特征满足真正推理行为标准；4）引导这些特征仅产生基准性能的最小变化或轻微下降。

Conclusion: 稀疏自编码器通过对比方法识别的特征主要捕捉推理的语言相关性特征，而非底层推理计算本身，这对当前基于对比激活的特征解释方法提出了重要质疑。

Abstract: We investigate whether sparse autoencoders (SAEs) identify genuine reasoning features in large language models (LLMs). Starting from features selected using standard contrastive activation methods, we introduce a falsification-oriented framework that combines causal token injection experiments and LLM-guided falsification to test whether feature activation reflects reasoning processes or superficial linguistic correlates. Across 20 configurations spanning multiple model families, layers, and reasoning datasets, we find that identified reasoning features are highly sensitive to token-level interventions. Injecting a small number of feature-associated tokens into non-reasoning text is sufficient to elicit strong activation for 59% to 94% of features, indicating reliance on lexical artifacts. For the remaining features that are not explained by simple token triggers, LLM-guided falsification consistently produces non-reasoning inputs that activate the feature and reasoning inputs that do not, with no analyzed feature satisfying our criteria for genuine reasoning behavior. Steering these features yields minimal changes or slight degradations in benchmark performance. Together, these results suggest that SAE features identified by contrastive approaches primarily capture linguistic correlates of reasoning rather than the underlying reasoning computations themselves.

</details>


### [75] [AGDC: Autoregressive Generation of Variable-Length Sequences with Joint Discrete and Continuous Spaces](https://arxiv.org/abs/2601.05680)
*Yeonsang Shin,Insoo Kim,Bongkeun Kim,Keonwoo Bae,Bohyung Han*

Main category: cs.LG

TL;DR: AGDC提出了一种统一框架，联合建模离散和连续值，解决Transformer自回归模型在生成高精度混合序列时的精度限制问题，特别针对半导体电路设计等需要高精度的领域。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的自回归模型在数据生成方面表现出色，但依赖于离散化token，限制了其表示连续值的高精度能力。现有离散化方法在生成混合离散-连续序列时存在可扩展性限制，特别是在半导体电路设计等高精度领域，精度损失可能导致功能失效。

Method: AGDC采用混合方法，结合离散值的分类预测和连续值的基于扩散的建模。包含两个关键技术组件：1) 使用MLP根据序列上下文动态调整EOS token logits的EOS对数调整机制；2) 集成到损失函数中的长度正则化项。同时提出了ContLayNet基准数据集。

Result: 在半导体布局（ContLayNet）、图形布局和SVG上的实验表明，AGDC在生成高保真混合向量表示方面优于基于离散化和固定模式的基线方法，实现了跨不同领域的可扩展高精度生成。

Conclusion: AGDC框架成功解决了Transformer模型在高精度混合序列生成中的局限性，通过联合建模离散和连续值，为半导体电路设计等高精度应用提供了有效的解决方案，并在多个领域展示了优越性能。

Abstract: Transformer-based autoregressive models excel in data generation but are inherently constrained by their reliance on discretized tokens, which limits their ability to represent continuous values with high precision. We analyze the scalability limitations of existing discretization-based approaches for generating hybrid discrete-continuous sequences, particularly in high-precision domains such as semiconductor circuit designs, where precision loss can lead to functional failure. To address the challenge, we propose AGDC, a novel unified framework that jointly models discrete and continuous values for variable-length sequences. AGDC employs a hybrid approach that combines categorical prediction for discrete values with diffusion-based modeling for continuous values, incorporating two key technical components: an end-of-sequence (EOS) logit adjustment mechanism that uses an MLP to dynamically adjust EOS token logits based on sequence context, and a length regularization term integrated into the loss function. Additionally, we present ContLayNet, a large-scale benchmark comprising 334K high-precision semiconductor layout samples with specialized evaluation metrics that capture functional correctness where precision errors significantly impact performance. Experiments on semiconductor layouts (ContLayNet), graphic layouts, and SVGs demonstrate AGDC's superior performance in generating high-fidelity hybrid vector representations compared to discretization-based and fixed-schema baselines, achieving scalable high-precision generation across diverse domains.

</details>


### [76] [FLRQ: Faster LLM Quantization with Flexible Low-Rank Matrix Sketching](https://arxiv.org/abs/2601.05684)
*Hongyaoxing Gul,Lijuan Hu,Shuzi Niu,Fangfang Liu*

Main category: cs.LG

TL;DR: FLRQ是一种灵活的量化方法，通过快速识别最优秩并聚合实现最小存储组合，在保持量化质量的同时显著提升算法效率。


<details>
  <summary>Details</summary>
Motivation: 传统低秩后训练量化方法需要昂贵的微调来确定不同数据和层的最佳秩，且基于SVD的低秩近似会增加计算开销，无法充分利用大模型的潜力。

Method: FLRQ包含两个核心组件：R1-FLR（基于R1-Sketch的灵活秩选择）使用高斯投影进行快速低秩近似，实现异常值感知的秩提取；BLC（裁剪下的最佳低秩近似）通过迭代方法最小化缩放和裁剪策略下的低秩量化误差。

Result: FLRQ在综合实验中表现出强大的有效性和鲁棒性，在量化质量和算法效率方面都达到了最先进的性能。

Conclusion: FLRQ通过灵活的低秩量化方法解决了传统方法的局限性，能够快速确定最优秩并实现最小存储组合，为大语言模型的高效量化提供了创新解决方案。

Abstract: Traditional post-training quantization (PTQ) is considered an effective approach to reduce model size and accelerate inference of large-scale language models (LLMs). However, existing low-rank PTQ methods require costly fine-tuning to determine a compromise rank for diverse data and layers in large models, failing to exploit their full potential. Additionally, the current SVD-based low-rank approximation compounds the computational overhead. In this work, we thoroughly analyze the varying effectiveness of low-rank approximation across different layers in representative models. Accordingly, we introduce \underline{F}lexible \underline{L}ow-\underline{R}ank \underline{Q}uantization (FLRQ), a novel solution designed to quickly identify the accuracy-optimal ranks and aggregate them to achieve minimal storage combinations. FLRQ comprises two powerful components, Rank1-Sketch-based Flexible Rank Selection (R1-FLR) and Best Low-rank Approximation under Clipping (BLC). R1-FLR applies the R1-Sketch with Gaussian projection for the fast low-rank approximation, enabling outlier-aware rank extraction for each layer. Meanwhile, BLC aims at minimizing the low-rank quantization error under the scaling and clipping strategy through an iterative method. FLRQ demonstrates strong effectiveness and robustness in comprehensive experiments, achieving state-of-the-art performance in both quantization quality and algorithm efficiency.

</details>


### [77] [Variational Autoencoders for P-wave Detection on Strong Motion Earthquake Spectrograms](https://arxiv.org/abs/2601.05759)
*Turkan Simge Ispak,Salih Tileylioglu,Erdem Akagunduz*

Main category: cs.LG

TL;DR: 该研究将P波检测重构为自监督异常检测任务，通过492种VAE配置实验发现：注意力机制优于跳跃连接，在近源区域（0-40公里）AUC达0.91，更适合地震预警应用。


<details>
  <summary>Details</summary>
Motivation: 强震动记录中的P波检测面临高噪声、标记数据有限和复杂波形特征的挑战，这对地震早期预警至关重要。研究旨在通过自监督异常检测方法解决这些问题。

Method: 将P波到达检测重构为自监督异常检测任务，通过492种变分自编码器（VAE）配置的网格搜索，比较不同架构（如跳跃连接和注意力机制）在重建保真度和异常判别之间的权衡。

Result: 跳跃连接虽然最小化重建误差（MAE约0.0012），但导致"过度泛化"，会重建噪声并掩盖检测信号。注意力机制优先考虑全局上下文而非局部细节，获得最高检测性能（AUC 0.875），在0-40公里近源范围内AUC达0.91。

Conclusion: 优先考虑全局上下文而非像素级完美重建的架构约束对于稳健的自监督P波检测至关重要，注意力机制VAE特别适合即时早期预警应用。

Abstract: Accurate P-wave detection is critical for earthquake early warning, yet strong-motion records pose challenges due to high noise levels, limited labeled data, and complex waveform characteristics. This study reframes P-wave arrival detection as a self-supervised anomaly detection task to evaluate how architectural variations regulate the trade-off between reconstruction fidelity and anomaly discrimination. Through a comprehensive grid search of 492 Variational Autoencoder configurations, we show that while skip connections minimize reconstruction error (Mean Absolute Error approximately 0.0012), they induce "overgeneralization", allowing the model to reconstruct noise and masking the detection signal. In contrast, attention mechanisms prioritize global context over local detail and yield the highest detection performance with an area-under-the-curve of 0.875. The attention-based Variational Autoencoder achieves an area-under-the-curve of 0.91 in the 0 to 40-kilometer near-source range, demonstrating high suitability for immediate early warning applications. These findings establish that architectural constraints favoring global context over pixel-perfect reconstruction are essential for robust, self-supervised P-wave detection.

</details>


### [78] [Weights to Code: Extracting Interpretable Algorithms from the Discrete Transformer](https://arxiv.org/abs/2601.05770)
*Yifan Zhang,Wei Bi,Kechi Zhang,Dongming Jin,Jie Fu,Zhi Jin*

Main category: cs.LG

TL;DR: Discrete Transformer通过强制功能解耦（数值注意力负责信息路由，数值MLP负责元素级算术）和温度退火采样，从连续表示中提取可读程序，实现无演示的算法发现。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在算法提取中存在叠加问题，即纠缠的特征编码阻碍了符号表达式的提取。需要一种能够连接连续表示和离散符号逻辑的架构。

Method: 提出Discrete Transformer架构，强制功能解耦：数值注意力仅负责信息路由，数值MLP仅负责元素级算术运算。采用温度退火采样方法，有效促进可读程序的提取。

Result: Discrete Transformer在性能上与基于RNN的基线相当，同时将可解释性扩展到连续变量领域。退火过程显示出从探索到利用的明显相变，且方法允许通过归纳偏置对合成程序进行细粒度控制。

Conclusion: Discrete Transformer为无演示的算法发现提供了一个稳健框架，并为Transformer可解释性提供了严格途径。

Abstract: Algorithm extraction aims to synthesize executable programs directly from models trained on specific algorithmic tasks, enabling de novo algorithm discovery without relying on human-written code. However, extending this paradigm to Transformer is hindered by superposition, where entangled features encoded in overlapping directions obstruct the extraction of symbolic expressions. In this work, we propose the Discrete Transformer, an architecture explicitly engineered to bridge the gap between continuous representations and discrete symbolic logic. By enforcing a strict functional disentanglement, which constrains Numerical Attention to information routing and Numerical MLP to element-wise arithmetic, and employing temperature-annealed sampling, our method effectively facilitates the extraction of human-readable programs. Empirically, the Discrete Transformer not only achieves performance comparable to RNN-based baselines but crucially extends interpretability to continuous variable domains. Moreover, our analysis of the annealing process shows that the efficient discrete search undergoes a clear phase transition from exploration to exploitation. We further demonstrate that our method enables fine-grained control over synthesized programs by imposing inductive biases. Collectively, these findings establish the Discrete Transformer as a robust framework for demonstration-free algorithm discovery, offering a rigorous pathway toward Transformer interpretability.

</details>


### [79] [Tensor-DTI: Enhancing Biomolecular Interaction Prediction with Contrastive Embedding Learning](https://arxiv.org/abs/2601.05792)
*Manel Gil-Sorribes,Júlia Vilalta-Mor,Isaac Filella-Mercè,Robert Soliva,Álvaro Ciudad,Víctor Guallar,Alexis Molina*

Main category: cs.LG

TL;DR: Tensor-DTI是一个基于对比学习的多模态药物-靶点相互作用预测框架，整合分子图、蛋白质语言模型和结合位点预测信息，在多个基准测试中优于现有方法，并在大规模虚拟筛选中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有药物-靶点相互作用预测模型通常依赖单模态预定义分子描述符或序列嵌入，代表性有限。需要整合多模态信息来提高相互作用建模的准确性和代表性。

Method: 提出Tensor-DTI对比学习框架，整合分子图、蛋白质语言模型和结合位点预测的多模态嵌入。采用孪生双编码器架构，捕捉化学和结构相互作用特征，区分相互作用与非相互作用对。

Result: 在多个DTI基准测试中优于现有序列和图形模型。在大规模CDK2虚拟筛选中，即使CDK2未参与训练，也能产生化学合理的命中分布。在富集研究中与Glide对接和Boltz-2共折叠器相比保持竞争力，并在严格家族保留分割下提高外家族靶点的筛选效率。还探索了蛋白质-RNA和肽-蛋白质相互作用的适用性。

Conclusion: 整合多模态信息与对比学习目标能显著提高相互作用预测准确性，为虚拟筛选提供更可解释和可靠性感知的模型。该方法展示了多模态整合在生物分子相互作用预测中的优势。

Abstract: Accurate drug-target interaction (DTI) prediction is essential for computational drug discovery, yet existing models often rely on single-modality predefined molecular descriptors or sequence-based embeddings with limited representativeness. We propose Tensor-DTI, a contrastive learning framework that integrates multimodal embeddings from molecular graphs, protein language models, and binding-site predictions to improve interaction modeling. Tensor-DTI employs a siamese dual-encoder architecture, enabling it to capture both chemical and structural interaction features while distinguishing interacting from non-interacting pairs. Evaluations on multiple DTI benchmarks demonstrate that Tensor-DTI outperforms existing sequence-based and graph-based models. We also conduct large-scale inference experiments on CDK2 across billion-scale chemical libraries, where Tensor-DTI produces chemically plausible hit distributions even when CDK2 is withheld from training. In enrichment studies against Glide docking and Boltz-2 co-folder, Tensor-DTI remains competitive on CDK2 and improves the screening budget required to recover moderate fractions of high-affinity ligands on out-of-family targets under strict family-holdout splits. Additionally, we explore its applicability to protein-RNA and peptide-protein interactions. Our findings highlight the benefits of integrating multimodal information with contrastive objectives to enhance interaction-prediction accuracy and to provide more interpretable and reliability-aware models for virtual screening.

</details>


### [80] [Fusion Matters: Length-Aware Analysis of Positional-Encoding Fusion in Transformers](https://arxiv.org/abs/2601.05807)
*Mohamed Amine Hallam,Kuo-Kun Tseng*

Main category: cs.LG

TL;DR: 研究位置编码融合机制对Transformer性能的影响，发现融合方式在长序列任务中显著影响性能，而在短序列中影响不大。


<details>
  <summary>Details</summary>
Motivation: 大多数现有研究关注设计新的位置编码，而忽略了位置信息如何与词嵌入融合的机制。本文旨在研究融合机制本身是否影响性能，特别是在长序列场景下。

Method: 在相同Transformer架构、数据划分和随机种子下，对比三种经典融合策略：逐元素加法、拼接加投影、标量门控融合。在三个文本分类数据集上进行实验，涵盖短（AG News）、中（IMDB）、长（ArXiv）序列。还进行了配对种子分析和跨数据集比较，并探索了轻量级卷积门控机制。

Result: 融合选择对短文本影响可忽略，但在长文档上产生一致的性能提升。可学习的融合机制在多个位置编码家族中都有效。轻量级卷积门控机制在长文档上表现出色。

Conclusion: 位置编码融合是长序列Transformer的重要设计选择，应被视为明确的建模决策而非固定默认设置。

Abstract: Transformers require positional encodings to represent sequence order, yet most prior work focuses on designing new positional encodings rather than examining how positional information is fused with token embeddings. In this paper, we study whether the fusion mechanism itself affects performance, particularly in long-sequence settings. We conduct a controlled empirical study comparing three canonical fusion strategies--element-wise addition, concatenation with projection, and scalar gated fusion--under identical Transformer architectures, data splits, and random seeds. Experiments on three text classification datasets spanning short (AG News), medium (IMDB), and long (ArXiv) sequences show that fusion choice has negligible impact on short texts but produces consistent gains on long documents. To verify that these gains are structural rather than stochastic, we perform paired-seed analysis and cross-dataset comparison across sequence-length regimes. Additional experiments on the ArXiv dataset indicate that the benefit of learnable fusion generalizes across multiple positional encoding families. Finally, we explore a lightweight convolutional gating mechanism that introduces local inductive bias at the fusion level, evaluated on long documents only. Our results indicate that positional-encoding fusion is a non-trivial design choice for long-sequence Transformers and should be treated as an explicit modeling decision rather than a fixed default.

</details>


### [81] [Detecting Autism Spectrum Disorder with Deep Eye Movement Features](https://arxiv.org/abs/2601.05812)
*Zhanpei Huang,Taochen chen,Fangqing Gu,Yiqun Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一个离散短期序列（DSTS）建模框架，用于通过眼动数据检测自闭症谱系障碍，该框架优于传统机器学习和深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 自闭症谱系障碍（ASD）的诊断需要非侵入性工具。眼动数据具有离散性和短期时间依赖性的特点，能反映局部注视焦点，但传统的Transformer注意力机制在处理这类数据时效果有限，因为全局注意力机制对离散注视点和短期依赖的效用降低。

Method: 设计了离散短期序列（DSTS）建模框架，包含类感知表示和不平衡感知机制，以有效捕捉眼动数据中区分ASD和典型发育个体的微妙复杂模式。

Result: 在多个眼动数据集上的实验表明，DSTS框架在ASD检测任务上优于传统机器学习技术和更复杂的深度学习模型。

Conclusion: 针对眼动数据的离散性和短期依赖性特点，专门设计的DSTS框架能更有效地捕捉ASD相关的行为标记，为自闭症的非侵入性诊断提供了更优的解决方案。

Abstract: Autism Spectrum Disorder (ASD) is a neurodevelopmental disorder characterized by deficits in social communication and behavioral patterns. Eye movement data offers a non-invasive diagnostic tool for ASD detection, as it is inherently discrete and exhibits short-term temporal dependencies, reflecting localized gaze focus between fixation points. These characteristics enable the data to provide deeper insights into subtle behavioral markers, distinguishing ASD-related patterns from typical development. Eye movement signals mainly contain short-term and localized dependencies. However, despite the widespread application of stacked attention layers in Transformer-based models for capturing long-range dependencies, our experimental results indicate that this approach yields only limited benefits when applied to eye movement data. This may be because discrete fixation points and short-term dependencies in gaze focus reduce the utility of global attention mechanisms, making them less efficient than architectures focusing on local temporal patterns. To efficiently capture subtle and complex eye movement patterns, distinguishing ASD from typically developing (TD) individuals, a discrete short-term sequential (DSTS) modeling framework is designed with Class-aware Representation and Imbalance-aware Mechanisms. Through extensive experiments on several eye movement datasets, DSTS outperforms both traditional machine learning techniques and more sophisticated deep learning models.

</details>


### [82] [A New Family of Poisson Non-negative Matrix Factorization Methods Using the Shifted Log Link](https://arxiv.org/abs/2601.05845)
*Eric Weine,Peter Carbonetto,Rafael A. Irizarry,Matthew Stephens*

Main category: cs.LG

TL;DR: 论文提出了一种带有移位对数链接函数的泊松非负矩阵分解方法，通过一个可调参数从加性组合过渡到乘性组合，改进了传统泊松NMF的假设限制。


<details>
  <summary>Details</summary>
Motivation: 传统泊松NMF假设分解的"部分"是加性组合的，这种假设在某些场景下可能不自然。需要一种更灵活的模型来适应不同数据组合方式。

Method: 引入带有移位对数链接函数的泊松NMF，通过单个调优参数控制从加性组合到乘性组合的过渡。提供了最大似然估计算法，以及针对大型稀疏数据集的近似计算方法。

Result: 在多个真实数据集上验证了新方法。结果表明链接函数的选择对分解结果有实质性影响，在某些情况下移位对数链接函数相比标准加性链接能提高结果的可解释性。

Conclusion: 提出的移位对数链接函数为泊松NMF提供了更灵活的建模框架，能够适应不同数据组合模式，在保持计算效率的同时提高了模型的可解释性。

Abstract: Poisson non-negative matrix factorization (NMF) is a widely used method to find interpretable "parts-based" decompositions of count data. While many variants of Poisson NMF exist, existing methods assume that the "parts" in the decomposition combine additively. This assumption may be natural in some settings, but not in others. Here we introduce Poisson NMF with the shifted-log link function to relax this assumption. The shifted-log link function has a single tuning parameter, and as this parameter varies the model changes from assuming that parts combine additively (i.e., standard Poisson NMF) to assuming that parts combine more multiplicatively. We provide an algorithm to fit this model by maximum likelihood, and also an approximation that substantially reduces computation time for large, sparse datasets (computations scale with the number of non-zero entries in the data matrix). We illustrate these new methods on a variety of real datasets. Our examples show how the choice of link function in Poisson NMF can substantively impact the results, and how in some settings the use of a shifted-log link function may improve interpretability compared with the standard, additive link.

</details>


### [83] [IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck](https://arxiv.org/abs/2601.05870)
*Huilin Deng,Hongchen Luo,Yue Zhu,Long Li,Zhuoyue Chen,Xinghao Zhao,Ming Li,Jihai Zhang,Mengchang Wang,Yang Cao,Yu Kang*

Main category: cs.LG

TL;DR: IIB-LPO通过信息瓶颈原理解决RLVR中的探索崩溃问题，从统计扰动转向推理轨迹的拓扑分支，在数学推理基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法面临探索崩溃问题：随机rollout的语义同质性导致模型陷入狭窄、过度优化的行为。全局熵正则化容易受到奖励攻击导致无意义的冗长，而局部令牌选择性更新则难以克服预训练模型的强归纳偏置。

Method: 提出IIB-LPO方法，将探索从令牌分布的统计扰动转向推理轨迹的拓扑分支。在高熵状态触发潜在分支以多样化推理路径，并采用信息瓶颈原理作为轨迹过滤器和自奖励机制，确保简洁且信息丰富的探索。

Result: 在四个数学推理基准测试中，IIB-LPO实现了最先进的性能，在准确率上比先前方法高出5.3%，在多样性指标上高出7.4%。

Conclusion: IIB-LPO通过信息瓶颈驱动的潜在策略优化有效解决了RLVR中的探索崩溃问题，为LLM推理提供了更有效的探索机制。

Abstract: Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.

</details>


### [84] [Auditing Fairness under Model Updates: Fundamental Complexity and Property-Preserving Updates](https://arxiv.org/abs/2601.05909)
*Ayoub Ajarra,Debabrota Basu*

Main category: cs.LG

TL;DR: 该论文研究机器学习模型在自适应更新情况下的公平性审计问题，提出了基于经验属性优化（EPO）的PAC审计框架，建立了统计公平性的分布无关审计边界。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型在社会基础设施中的广泛应用，审计模型偏见变得越来越重要。然而在实际部署中，模型所有者会根据环境变化（如金融市场）自适应更新模型，这些更新可能改变底层模型类别但保持某些感兴趣属性不变，这引发了在模型变化情况下可靠审计的基本问题。

Method: 提出了一个基于经验属性优化（EPO）oracle的通用PAC审计框架。对于统计公平性，建立了由SP维度（一种新的组合度量）表征的分布无关审计边界，该维度捕捉了允许的策略更新的复杂性。

Result: 该框架能够表征允许更新的信息复杂性，识别哪些策略变化能保持被审计属性，并使用最少的标记样本有效估计审计属性（如群体公平性）。

Conclusion: 该研究为在模型自适应更新情况下的公平性审计提供了理论框架，证明了该框架可自然扩展到其他审计目标，包括预测误差和鲁棒风险。

Abstract: As machine learning models become increasingly embedded in societal infrastructure, auditing them for bias is of growing importance. However, in real-world deployments, auditing is complicated by the fact that model owners may adaptively update their models in response to changing environments, such as financial markets. These updates can alter the underlying model class while preserving certain properties of interest, raising fundamental questions about what can be reliably audited under such shifts.
  In this work, we study group fairness auditing under arbitrary updates. We consider general shifts that modify the pre-audit model class while maintaining invariance of the audited property. Our goals are two-fold: (i) to characterize the information complexity of allowable updates, by identifying which strategic changes preserve the property under audit; and (ii) to efficiently estimate auditing properties, such as group fairness, using a minimal number of labeled samples.
  We propose a generic framework for PAC auditing based on an Empirical Property Optimization (EPO) oracle. For statistical parity, we establish distribution-free auditing bounds characterized by the SP dimension, a novel combinatorial measure that captures the complexity of admissible strategic updates. Finally, we demonstrate that our framework naturally extends to other auditing objectives, including prediction error and robust risk.

</details>


### [85] [Distilling Lightweight Domain Experts from Large ML Models by Identifying Relevant Subspaces](https://arxiv.org/abs/2601.05913)
*Pattarawat Chormai,Ali Hashemi,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: SubDistill是一种新的知识蒸馏算法，专注于仅蒸馏教师模型中与特定子任务相关的组件，在计算资源有限的环境中提升学生模型性能。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，通常只有少数类别及其相关中间概念需要蒸馏，但现有蒸馏方法很少明确关注相关子任务，导致效率低下。

Method: SubDistill算法通过改进数值特性，在每一层仅蒸馏教师模型的相关组件，专注于相关子任务的知识传递。

Result: 在CIFAR-100和ImageNet数据集上，使用卷积和Transformer模型的实验表明，SubDistill在代表性子任务集上优于现有的逐层蒸馏技术。

Conclusion: SubDistill通过专注于相关子任务的蒸馏，不仅提升了学生模型性能，而且通过可解释AI分析显示，其决策结构更接近原始教师模型。

Abstract: Knowledge distillation involves transferring the predictive capabilities of large, high-performing AI models (teachers) to smaller models (students) that can operate in environments with limited computing power. In this paper, we address the scenario in which only a few classes and their associated intermediate concepts are relevant to distill. This scenario is common in practice, yet few existing distillation methods explicitly focus on the relevant subtask. To address this gap, we introduce 'SubDistill', a new distillation algorithm with improved numerical properties that only distills the relevant components of the teacher model at each layer. Experiments on CIFAR-100 and ImageNet with Convolutional and Transformer models demonstrate that SubDistill outperforms existing layer-wise distillation techniques on a representative set of subtasks. Our benchmark evaluations are complemented by Explainable AI analyses showing that our distilled student models more closely match the decision structure of the original teacher model.

</details>


### [86] [Prophet as a Repro ducible Forecasting Framework: A Methodological Guide for Business and Financial Analytics](https://arxiv.org/abs/2601.05929)
*Sidney Shapiro,Burhanuddin Panvelwala*

Main category: cs.LG

TL;DR: 该研究评估了Meta开发的Prophet预测框架如何通过其可加性结构、开源实现和标准化工作流程促进可复现的预测实践，并与ARIMA和随机森林进行了多模型比较。


<details>
  <summary>Details</summary>
Motivation: 预测研究与实践中的可复现性挑战，特别是在商业和金融分析领域。传统方法需要大量手动调参且难以在专有环境中复现，而机器学习方法存在可解释性和随机训练过程的问题。

Method: 使用公开可用的金融和零售数据集，在受控且完全文档化的实验设计中，将Prophet与多种ARIMA规格（自动选择、手动指定和季节性变体）以及随机森林进行性能比较。通过具体的Python示例展示Prophet如何促进高效预测工作流程。

Result: Prophet在可解释性、标准化工作流程和可访问性方面表现出优势，能够平衡预测灵活性和可复现性要求，支持验证、可审计性和方法严谨性。

Conclusion: Prophet作为一个方法学构建模块，在基于Python的研究工作流程中为可复现预测提供了实用的参考框架，有助于提升预测实践的透明度和可复现性。

Abstract: Reproducibility remains a persistent challenge in forecasting research and practice, particularly in business and financial analytics where forecasts inform high-stakes decisions. Traditional forecasting methods, while theoretically interpretable, often require extensive manual tuning and are difficult to replicate in proprietary environments. Machine learning approaches offer predictive flexibility but introduce challenges related to interpretability, stochastic training procedures, and cross-environment reproducibility. This paper examines Prophet, an open-source forecasting framework developed by Meta, as a reproducibility-enabling solution that balances interpretability, standardized workflows, and accessibility. Rather than proposing a new algorithm, this study evaluates how Prophet's additive structure, open-source implementation, and standardized workflow contribute to transparent and replicable forecasting practice. Using publicly available financial and retail datasets, we compare Prophet's performance and interpretability with multiple ARIMA specifications (auto-selected, manually specified, and seasonal variants) and Random Forest under a controlled and fully documented experimental design. This multi-model comparison provides a robust assessment of Prophet's relative performance and reproducibility advantages. Through concrete Python examples, we demonstrate how Prophet facilitates efficient forecasting workflows and integration with analytical pipelines. The study positions Prophet within the broader context of reproducible research. It highlights Prophet's role as a methodological building block that supports verification, auditability, and methodological rigor. This work provides researchers and practitioners with a practical reference framework for reproducible forecasting in Python-based research workflows.

</details>


### [87] [LookAroundNet: Extending Temporal Context with Transformers for Clinically Viable EEG Seizure Detection](https://arxiv.org/abs/2601.06016)
*Þór Sverrisson,Steinn Guðmundsson*

Main category: cs.LG

TL;DR: LookAroundNet是一个基于Transformer的癫痫发作检测器，通过使用更宽的时间窗口（包括感兴趣段前后的EEG信号）来建模癫痫活动，在多种临床环境和数据集上表现出色，具有良好的泛化能力和临床部署可行性。


<details>
  <summary>Details</summary>
Motivation: 由于癫痫发作动态在不同患者、记录条件和临床环境中的巨大变异性，自动癫痫检测仍然具有挑战性。现有方法通常只关注局部EEG片段，而临床医生在解读EEG时会考虑周围上下文信息。

Method: 提出LookAroundNet，一种基于Transformer的癫痫检测器，使用更宽的EEG时间窗口（包括感兴趣段前后的信号）来建模癫痫活动。该方法在多个EEG数据集上进行评估，包括临床常规EEG和长期动态记录，并采用模型集成策略。

Result: LookAroundNet在多个数据集上表现出强大的性能，能够很好地泛化到未见过的记录条件，计算成本适合实际临床部署。结果表明扩展的时间上下文、增加训练数据多样性和模型集成是提高性能的关键因素。

Conclusion: 这项工作通过引入更宽时间窗口的Transformer架构，结合多样化的训练数据和模型集成，推动了自动癫痫检测模型向临床可行解决方案的发展，为解决癫痫检测中的变异性问题提供了有效方法。

Abstract: Automated seizure detection from electroencephalography (EEG) remains difficult due to the large variability of seizure dynamics across patients, recording conditions, and clinical settings. We introduce LookAroundNet, a transformer-based seizure detector that uses a wider temporal window of EEG data to model seizure activity. The seizure detector incorporates EEG signals before and after the segment of interest, reflecting how clinicians use surrounding context when interpreting EEG recordings. We evaluate the proposed method on multiple EEG datasets spanning diverse clinical environments, patient populations, and recording modalities, including routine clinical EEG and long-term ambulatory recordings, in order to study performance across varying data distributions. The evaluation includes publicly available datasets as well as a large proprietary collection of home EEG recordings, providing complementary views of controlled clinical data and unconstrained home-monitoring conditions. Our results show that LookAroundNet achieves strong performance across datasets, generalizes well to previously unseen recording conditions, and operates with computational costs compatible with real-world clinical deployment. The results indicate that extended temporal context, increased training data diversity, and model ensembling are key factors for improving performance. This work contributes to moving automatic seizure detection models toward clinically viable solutions.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [88] [Thermodynamics of driven systems via the Kuramoto-Sivashinsky equation](https://arxiv.org/abs/2601.05389)
*E. Hansen,W. Barham,P. J. Morrison*

Main category: nlin.CD

TL;DR: 该研究分析了Kuramoto-Sivashinsky方程描述的驱动湍流与热力学第二定律的关系，发现KS方程的正谱特性阻碍其度量辛描述，并构建了热力学变体系统。


<details>
  <summary>Details</summary>
Motivation: 研究KS方程描述的驱动湍流与热力学第二定律之间的差异，探索外部能源驱动的系统如何与热力学原理相协调。

Method: 使用度量辛动力学的统一热力学算法分析一般速度和熵密度系统；构建KS方程的热力学变体；通过数值实验比较KS方程及其热力学变体的演化；进一步实验限制KS方程的正谱以研究其对系统时间演化的影响。

Result: KS方程的正谱特性（源于外部能源）阻碍其度量辛描述；构建的热力学变体系统能单调生成熵，但其唯一平衡态是空间常数；KS方程的驱动效应进一步增加了热力学系统的熵，使熵生成与KS方程能源相协调；限制正谱的实验显示：重新标度不稳定性增长率能在更慢时间尺度上重现类似行为，引入个体正谱能重现平衡态、相对平衡态和向混沌的转变。

Conclusion: KS方程的正谱特性与热力学第二定律存在根本性差异，但通过构建热力学变体系统可以协调熵生成与外部能源驱动的关系，为理解驱动湍流系统的热力学行为提供了新视角。

Abstract: We examine the differences between the driven turbulence described by the Kuramoto-Sivashinsky (KS) equation and the second law of thermodynamics. A general velocity and entropy density system is analyzed with the unified thermodynamic algorithm of metriplectic dynamics, and we show that the positive spectra of the KS equation due to an external energy source prevent its metriplectic description. A variant of the KS equation is produced that monotonically generates an entropy, but the only equilibria of this variant system are spatially constant. Numerical experiments are performed comparing the evolution of the KS equation and its thermodynamic variant. The entropy of this thermodynamic system is increased further by the driving effects of the KS equation, reconciling the generation of entropy with the energy source of the KS equation. Further numerical experiments restrict the positive spectra in the KS equation to determine the effect on the system time evolution. While rescaling the growth rates of instabilities reproduces similar behavior on a slower time scale, introduction of individual positive spectra reproduces the formation of equilibria, relative equilibria, and a transition to chaos.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [89] [Competing Paramagnetic Phases in the Maple-Leaf Heisenberg Antiferromagnet](https://arxiv.org/abs/2601.05308)
*P. L. Ebert,Y. Iqbal,A. Wietek*

Main category: cond-mat.str-el

TL;DR: 枫叶晶格自旋-1/2海森堡反铁磁体在三个对称不等价最近邻键作用下展现出丰富的基态相图，包括六角晶格态、星形价键固态、精确二聚体乘积态等，并在各向同性点附近存在自旋液体出现的可能区域。


<details>
  <summary>Details</summary>
Motivation: 研究枫叶晶格自旋-1/2海森堡反铁磁体在不同键参数下的基态相图，探索其中可能存在的奇异量子态和量子临界现象。

Method: 使用精确对角化和态塔分析，在最多N=36个格点的团簇上进行计算；通过Gutzwiller投影波函数分析识别可能的自旋液体态。

Result: 发现了丰富的基态相图：六角晶格态、星形价键固态（靠近120°磁相）、精确二聚体乘积态（靠近共线Néel态）。在各向同性点附近参数区域，识别出一个gapped ℤ₂自旋液体Ansatz与精确N=36基态高度吻合。

Conclusion: 枫叶反铁磁体展现出丰富的顺磁相竞争，是探索奇异物质态和量子临界现象的有前景平台，特别是在各向同性点附近存在自旋液体出现的可能性。

Abstract: We establish a remarkably rich ground state phase diagram in the maple-leaf lattice spin-$1/2$ Heisenberg antiferromagnet as a function of the three symmetry-inequivalent nearest-neighbor bonds using exact diagonalization and tower-of-states analysis on clusters up to $N=36$ sites. Besides a hexagonal plaquette state, a star-shaped valence bond solid state is discovered in close vicinity to the (canted) $120^\circ$ magnetic phase, strongly reminiscent of a de-confined critical point or Dirac spin liquid scenario on the triangular lattice antiferromagnets. Moreover, an exact dimer product-state is observed next to a collinear Néel-state, similar to the Shastry-Sutherland model. All identified phases compete in a parameter regime close to the isotropic point, providing a promising region for spin liquids to emerge. By analyzing Gutzwiller-projected wave-functions we identify a sliver of parameter regime where a gapped $\mathbb{Z}_{2}$ spin liquid Ansatz is in astonishing agreement with the exact $N=36$ ground state. This rich competition of paramagnetic phases demonstrates that the maple-leaf antiferromagnet is a promising platform for exotic states of matter and quantum critical phenomena.

</details>


### [90] [Spin-triplet paired Wigner crystal stabilized by quantum geometry](https://arxiv.org/abs/2601.05318)
*Dmitry Zverevich,Alex Levchenko,Ilya Esterlis*

Main category: cond-mat.str-el

TL;DR: 通过变分态分析带几何对二维维格纳晶体的影响，发现低电子密度下增加贝里曲率会驱动向自旋三重态配对晶体的转变


<details>
  <summary>Details</summary>
Motivation: 研究带几何（特别是贝里曲率）对二维维格纳晶体性质的影响，探索强关联二维电子系统中可能的电子配对机制

Method: 使用变分态方法分析二维维格纳晶体，考虑每个晶胞含有一个和两个电子的情况，并通过有效的双电子量子点问题来理解转变的本质特征

Result: 在足够低的电子密度下，增加贝里曲率会驱动向由自旋三重态对组成的晶体态转变，这些对携带相对轨道角动量m=-1

Conclusion: 研究结果表明，在具有量子几何的强关联二维电子系统中，存在一种纯电子的强耦合机制，可以实现局域自旋三重态配对

Abstract: We have used variational states to analyze the effects of band geometry on the two-dimensional Wigner crystal with one and two electrons per unit cell. At sufficiently low electron densities, we find that increasing Berry curvature drives a transition into a crystalline state composed of spin-triplet pairs carrying relative orbital angular momentum $m=-1$. The essential features of this transition are captured by an effective two-electron quantum dot problem in the presence of Berry curvature. Our results point to a purely electronic, strong-coupling mechanism for local spin-triplet pairing in correlated two-dimensional electron systems with quantum geometry.

</details>


### [91] [Symmetry-engineered and electrically tunable in-plane anomalous Hall effect in oxide heterostructures](https://arxiv.org/abs/2601.05462)
*Kunjie Dai,Zhen Wang,Wenfeng Wu,Feng Jin,Enda Hua,Nan Liu,Jingdi Lu,Jinfeng Zhang,Yuyue Zhao,Linda Yang,Kai Liu,Huan Ye,Qiming Lv,Zhengguo Liang,Ao Wang,Dazhi Hou,Yang Gao,Shengchun Shen,Jing Tao,Liang Si,Wenbin Wu,Lingfei Wang*

Main category: cond-mat.str-el

TL;DR: 该研究开发了一种基于CaRuO3/La2/3Ca1/3MnO3/CaRuO3异质结构的对称性工程平台，实现了对平面反常霍尔效应（IP-AHE）的确定性可逆控制，通过离子液体门控实现了电调制和开关功能。


<details>
  <summary>Details</summary>
Motivation: 平面反常霍尔效应（IP-AHE）作为霍尔效应家族的新成员，在自旋电子学中具有重要应用前景，但缺乏确定性可逆控制的实用方法。研究旨在建立一种能够有效调控IP-AHE的平台。

Method: 在NdGaO3(110)衬底上制备CaRuO3/La2/3Ca1/3MnO3/CaRuO3三明治异质结构，利用CaRuO3缓冲层诱导的镜像对称性破缺作为调控手段，并通过离子液体门控实现对称性破缺的可逆重构。

Result: IP-AHE明确耦合到CaRuO3缓冲层诱导的镜像对称性破缺，并忠实再现铁磁滞回线。离子液体门控实现了对称性破缺的可逆重构，从而实现了IP-AHE的电调制和ON/OFF开关控制。

Conclusion: 这种高度可调的IP-AHE平台为探索非平凡磁序和开发平面几何中的可编程霍尔功能开辟了新途径，将严格的对称性破缺约束转化为有效的调控手段。

Abstract: The family of Hall effects has long served as a premier probe of how symmetry, magnetic order, and topology intertwine in solids. Recently, the in-plane anomalous Hall effect (IP-AHE), a transverse Hall response driven by in-plane magnetization, has emerged as a distinct member of this family, offering innovative spintronic functionalities and illuminating intricate interplay between mirror-symmetry breaking and in-plane magnetic order. However, practical routes to deterministically and reversibly control IP-AHE remain limited. Here, we establish a symmetry-engineered IP-AHE platform, CaRuO3/La2/3Ca1/3MnO3/CaRuO3 heterostructure on NdGaO3(110), that turns strict mirror-symmetry breaking constraints into effective tuning knobs. IP-AHE in these epitaxial trilayers unambiguously couples to the CaRuO3-buffer-induced mirror-symmetry breaking and faithfully reproduces the ferromagnetic hysteresis. Ionic liquid gating further enables reversible reconfigurations of the symmetry breaking, thereby achieving electrical modulation and ON/OFF switching of IP-AHE. This highly tunable IP-AHE platform opens pathways for exploring nontrivial magnetic order and developing programmable Hall functionalities in planar geometries.

</details>


### [92] [Revival of Strain Susceptibilities: Magnetostrictive Coefficient and Thermal-Expansion Coefficient](https://arxiv.org/abs/2601.05484)
*Yisheng Chai*

Main category: cond-mat.str-el

TL;DR: 论文探讨了应变测量在量子关联材料研究中的重要性，特别是磁致伸缩系数和热膨胀系数这两个关键应变敏感度参数。


<details>
  <summary>Details</summary>
Motivation: 在热力学中，体积是重要的广延变量。应变（长度、面积或体积变化）为研究关联量子材料提供了直接窗口：微小的长度变化ΔL可以追踪晶格在磁场H和温度T等状态变量变化时的响应，从而揭示相变、相和动力学。直接、高精度的应变测量已经很困难，而测量它们的敏感度则更加困难。

Method: 最近几种直接测量技术取得了重要进展，重点关注两个关键量：磁致伸缩系数dλ/dH（文献中常表示为qijk或dij）和线性热膨胀系数α=dλ/dT。将这两个应变敏感度参数结合起来考虑，它们是基本且互补的。

Result: 这些测量技术的最新进展使得能够更精确地测量应变敏感度参数，为研究关联量子材料的相变和动力学提供了新工具。

Conclusion: 这些热力学性质值得重新关注，因为它们为理解关联量子材料的基本物理提供了重要信息，应变敏感度参数的测量技术进展为这一领域带来了新的研究机遇。

Abstract: In thermodynamics, volume is an essential extensive variable. Strain-line, area, or volume change-therefore offers a direct window into correlated quantum matter: tiny length changes ΔL track how the lattice responds when state variables such as magnetic field H and/or temperature T are varied, revealing phases, transitions, and dynamics. Direct, high-precision strain measurements are already difficult; their susceptibilities are harder still. Very recently, several direct techniques have made vital progress on two key quantities: the magnetostrictive coefficient dλ/dH (often denoted qijk or dij in the magnetostriction literatures), and the linear thermal-expansion coefficient α= dλ/dT. Considering these two strain susceptibilities together-they are fundamental and complementary-clarifies why these thermodynamic properties merit renewed attention.

</details>


### [93] [Molecular Orbital Degeneracy Lifting in a Tetrahedral Cluster System NbSeI](https://arxiv.org/abs/2601.05562)
*Keita Kojima,Youichi Yamakawa,Ryutaro Okuma,Shunsuke Kitou,Hayato Takano,Jun-ichi Yamaura,Yusuke Tokunaga,Taka-hisa Arima,Yoshihiko Okamoto*

Main category: cond-mat.str-el

TL;DR: NbSeI中Nb4四面体簇的轨道简并性通过两种不同机制被解除：在106K以下为分子轨道有序绝缘体，在106K以上为通过局域畸变保持轨道简并解除的非磁性绝缘态。


<details>
  <summary>Details</summary>
Motivation: 研究晶体固体中简并电子态解除的基本问题，特别关注包含多个过渡金属原子高对称性簇的材料，这些材料由于多原子间电子自由度的纠缠而预期会展现更多涌现现象。

Method: 研究NbSeI材料，该材料包含具有分子轨道自由度的Nb4四面体簇，其平均晶体结构预测为平带金属。通过实验发现轨道简并性解除的两种不同机制。

Result: 发现NbSeI在106K以下为非磁性分子轨道有序绝缘体；在106K以上，平均结构变为面心立方且无超晶格，但轨道简并性仍通过Nb4四面体的显著局域畸变被解除，可能对应于分子轨道液体或轨道冻结态。

Conclusion: 这种非合作Jahn-Teller畸变在106K以上稳定了非磁性绝缘态，与从平均结构预测的平带金属形成鲜明对比，揭示了高对称性簇材料中轨道简并性解除的新机制。

Abstract: The lifting of degenerate electronic states, in which multiple electronic states share the same energy, is a fundamental issue in the physics of crystalline solids. In real materials, this problem has been extensively studied in transition metal compounds, where various quantum phenomena arise from the spin and orbital degeneracy of the d electrons on individual transition-metal atoms. In contrast, materials containing high-symmetry clusters composed of multiple transition-metal atoms are expected to exhibit more emergent phenomena due to the entanglement of the electronic degrees of freedom across multiple atoms. Here, we report the discovery of two distinct mechanisms of orbital-degeneracy lifting in NbSeI, which comprises Nb4 tetrahedral clusters with molecular orbital degrees of freedom and whose average crystal structure is predicted to host a flat-band metal. Below 106 K, NbSeI is found to be a nonmagnetic molecular orbital-ordered insulator. Above this temperature, the average structure becomes face-centered cubic without any superlattice, while the orbital degeneracy remains lifted by significant local distortions of Nb4 tetrahedra, which may be associated with a molecular orbital-liquid or orbital-frozen state. This noncooperative Jahn-Teller distortion stabilizes a nonmagnetic insulating state above 106 K, in stark contrast to the flat-band metal predicted from the average structure.

</details>


### [94] [Pressure-driven Valence evolution Coupled Hardening-to-Softening transition in YbPd](https://arxiv.org/abs/2601.05766)
*B. Tegomo Chiogo,V. Balédent,J. -P. Rueff,Ethan Saïman,V. Poree,T. Schweitzer,D. Wong,C. Schulz,T. Mazet,A. Chainani,D. Malterre,K. Habicht*

Main category: cond-mat.str-el

TL;DR: 通过高压下共振X射线发射光谱研究YbPd中Yb价态不稳定性，发现在电荷有序相中Yb 4f价态在1.5 GPa前保持恒定后逐渐增加，而在正常相中观察到反常的价态降低，伴随晶格压缩性在1.5 GPa出现拐点


<details>
  <summary>Details</summary>
Motivation: 研究强关联材料YbPd中Yb价态不稳定性，特别是在压力诱导的电荷有序转变过程中的行为，以理解其与铈的γ-α相变中观察到的压缩性最大值的相似性

Method: 使用共振X射线发射光谱在高压下测量YbPd，通过Birch-Murnaghan分析计算压缩性，研究不同温度（30K和室温）下Yb 4f价态随压力的变化

Result: 在电荷有序相（30K）中，Yb 4f价态在PK=1.5 GPa前保持恒定，之后逐渐增加；在正常相（室温）中观察到反常的价态降低；压缩性分析显示晶格在1.5 GPa前硬化，之后出现压力诱导的软化

Conclusion: YbPd（f13-f14空穴型混合价态）的压缩性最小值与铈（f0-f1电子型混合价态）的γ-α相变中观察到的压缩性最大值相似，表明两种体系在压力诱导的价态转变中存在类似的物理机制

Abstract: We investigate the Yb valence instabilities in the strongly correlated YbPd compound using resonant X-ray emission spectroscopy under pressure across the charge-ordering (CO) transition. At a low temperature (T = 30 K) in the CO ordered phase, the Yb $4f$ valence remains nearly constant up to a pressure P$_K$ = 1.5 GPa, and then increases gradually at higher pressures. In contrast, at room temperature in the normal phase, an anomalous decrease of the Yb $4f$ valence is observed, without any accompanying structural phase transition. This behavior is corroborated by a systematic pressure dependent decrease of the unit-cell volume. Based on a Birch-Murnaghan analysis, the compressibility indicates hardening of the lattice with applied pressure up to a distinct kink seen at P$_K$ = 1.5 GPa. In contrast, for P $>$ P$_K$, the Yb $4f$ valency saturates and the compressibility reveals a counterintuitive pressure-induced softening. The results show a minimum in the compressibility of YbPd (with $f^{13}$-$f^{14}$ hole-type mixed-valence) and is reminiscent of the maximum in compressibility seen in the $γ$-$α$ first-order isostructural phase transition in cerium (with $f^{0}$-$f^{1}$ electron-type mixed-valence).

</details>


### [95] [Angular-Dependent Thermal Hall Effect in a Honeycomb Magnet: Disentangling Kitaev and Dzyaloshinskii-Moriya Interactions](https://arxiv.org/abs/2601.05819)
*Shuvankar Gupta,Olajumoke Oluwatobiloba Emmanuel,Pengpeng Zhang,Xianglin Ke*

Main category: cond-mat.str-el

TL;DR: 通过测量VI3铁磁蜂窝绝缘体的角度依赖热霍尔电导，发现其热霍尔响应由Dzyaloshinskii-Moriya相互作用驱动，而非Kitaev相互作用，为区分量子磁体中竞争相互作用提供了有效方法。


<details>
  <summary>Details</summary>
Motivation: 层状蜂窝磁体因其各向异性、键依赖的Kitaev相互作用而展现出奇异的量子现象，但区分Kitaev相互作用和对称性允许的Dzyaloshinskii-Moriya相互作用(DMI)的作用仍然具有挑战性，因为两种机制可能导致相似的磁激发和热输运性质。

Method: 以铁磁蜂窝绝缘体VI3为模型系统，系统研究具有面外(θ)和面内(Φ)磁场旋转的角度依赖热霍尔电导Kxy(θ, Φ)。

Result: 结果显示，对于面外和面内旋转磁场都存在持续的热霍尔响应，缺乏Kitaev物理特有的符号反转模式。定量分析表明，角度依赖的Kxy(θ, Φ)由磁矩与包含面外和面内分量的倾斜DM矢量之间的投影关系决定。

Conclusion: 这些结果不仅确立了DMI驱动的拓扑磁激发是VI3中热霍尔响应的起源，而且强调了角度依赖热霍尔效应测量是区分量子磁体中竞争相互作用的有效方法。

Abstract: Layered honeycomb magnets have garnered significant attention recently for their exotic quantum phenomena due to the potential anisotropic, bond-dependent Kitaev interactions. However, distinguishing the roles of Kitaev interactions and the symmetry-allowed Dzyaloshinskii-Moriya interaction (DMI) remains challenging, since both mechanisms may lead to similar magnetic excitations and thermal transport properties. To tackle this challenge, using a ferromagnetic honeycomb insulator VI3 as a model system, we systematically study the angular-dependent thermal Hall conductivity Kxy(θ, Φ) with both out-of-plane (θ) and in-plane (Φ) magnetic field rotations. Our results reveal a persistent thermal Hall response for both out-of-plane and in-plane rotating magnetic fields, devoid of the sign-reversal patterns characteristic of Kitaev physics. Instead, quantitative analysis shows that the angular dependent Kxy(θ, Φ) is governed by the projection between the magnetic moment and a tilted DM vector containing both out-of-plane and in-plane components. These results not only establish the DMI-driven topological magnetic excitations as the origin of the thermal Hall response in VI3 but also highlight the angular-dependent thermal Hall effect measurements as an effective approach for distinguishing competing interactions in quantum magnets.

</details>


### [96] [Cooperative concurrence of 4f and 3d flat bands in kagome heavy-fermion metal YbCr6Ge6](https://arxiv.org/abs/2601.05829)
*Wenxin Lv,Pengcheng Ma,Tianqi Wang,Shangjie Tian,Ying Ma,Shouguo Wang,Xiao Zhang,Zhonghao Liu,Hechang Lei*

Main category: cond-mat.str-el

TL;DR: YbCr6Ge6材料中同时存在源自Cr-卡戈晶格的平带和局域Yb-4f电子的平带，两者在费米能级附近共存并发生杂化，导致重费米子行为和增强的反铁磁性。


<details>
  <summary>Details</summary>
Motivation: 研究平带系统（如卡戈金属中的几何平带和重费米子化合物中的局域轨道平带）的共存与相互作用机制，探索新型量子态物质。

Method: 通过角分辨光电子能谱（ARPES）测量YbCr6Ge6材料的电子结构，分析Cr-卡戈晶格和Yb-4f电子平带的共存与杂化特征。

Result: 发现YbCr6Ge6表现出重费米子行为，具有TN=3K的强反铁磁基态；ARPES揭示了Cr-卡戈平带和Yb-4f平带在费米能级附近的共存，以及两者之间的杂化特征。

Conclusion: YbCr6Ge6作为新型卡戈重费米子金属，不仅实现了两种不同类型平带的协同共存，还为探索奇异关联拓扑量子现象提供了范式材料平台。

Abstract: Flat-band (FB) systems originating from special lattice geometry like in kagome metals as well as localized orbitals in the materials such as heavy-fermion (HF) compounds have induced intensive interest due to their band topology and strong electron correlation effects, leading to emergent quantum states of matter. However, the question of how these two distinct FBs coexist and interact remains unsettled. Here, we report that YbCr6Ge6 hosting both Cr-kagome lattice and Yb-4f electrons exhibits HF behaviors and a robust antiferromagnetic ground state with transition temperature TN = 3 K, significantly higher than other similar kagome metals with Yb ions. Angle-resolved photoemission spectroscopy measurements reveal the coexistence of FBs originating from both Cr-kagome lattice and localized Yb-4f electrons near Fermi energy level EF. More importantly, the clear spectroscopic signatures of a hybridization of Yb-4f FB with kagome-lattice-derived conduction bands and the high density of states of Cr-kagome FB near EF provide the underlying microscopic mechanisms of HF behaviors and enhanced antiferromagnetism in YbCr6Ge6. Our findings demonstrate that the novel kagome HF metals can not only host the cooperative coexistence of two different types of FBs, but also provide a paradigm material platform to explore the exotic correlated topological quantum phenomena.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [97] [Autonomous Discovery of the Ising Model's Critical Parameters with Reinforcement Learning](https://arxiv.org/abs/2601.05577)
*Hai Man,Chaobo Wang,Jia-Rui Li,Yuping Tian,Shu-Gang Chen*

Main category: cond-mat.stat-mech

TL;DR: 提出基于物理启发的自适应强化学习框架，用于自主识别Ising模型中的临界温度和临界指数，性能优于传统方法


<details>
  <summary>Details</summary>
Motivation: 传统确定临界参数的方法受人为因素影响较大，需要更精确、自主的识别方法

Method: 采用物理启发的自适应强化学习框架，让智能体自主与物理环境交互，同时识别临界温度和多种临界指数

Result: 算法表现出类似相变的搜索行为，能高效收敛到目标参数，在强扰动环境中显著优于传统方法

Conclusion: 将物理概念融入机器学习增强算法可解释性，建立了从人工分析到自主AI发现的新科学探索范式

Abstract: Traditional methods for determining critical parameters are often influenced by human factors. This research introduces a physics-inspired adaptive reinforcement learning framework that enables agents to autonomously interact with physical environments, simultaneously identifying both the critical temperature and various types of critical exponents in the Ising model with precision. Interestingly, our algorithm exhibits search behavior reminiscent of phase transitions, efficiently converging to target parameters regardless of initial conditions. Experimental results demonstrate that this method significantly outperforms traditional approaches, particularly in environments with strong perturbations. This study not only incorporates physical concepts into machine learning to enhance algorithm interpretability but also establishes a new paradigm for scientific exploration, transitioning from manual analysis to autonomous AI discovery.

</details>


### [98] [Exact Volterra series for mean field dynamics](https://arxiv.org/abs/2601.06004)
*Ion Santra,Matthias Krüger*

Main category: cond-mat.stat-mech

TL;DR: 本文推导了相互作用粒子系统在势扰动下的平均场精确Volterra级数展开，将展开核表示为场响应函数的函数，并应用于简单流体的平均粒子密度，得到与动态密度泛函理论相似但存在根本差异的形式。


<details>
  <summary>Details</summary>
Motivation: 为了系统性地改进现有的平均场形式主义，需要发展一个精确的数学框架来描述相互作用粒子系统在外部扰动下的响应行为。

Method: 推导了相互作用粒子系统在势扰动下的平均场精确Volterra级数展开，将Volterra展开核表示为场响应函数的函数，并将此形式应用于简单流体的平均粒子密度分析。

Result: 得到了与动态密度泛函理论相似但存在根本差异的形式：出现了非局域迁移率核，且力来源于平均密度历史的泛函；在缓慢变化扰动极限下恢复了平衡密度泛函；发现了推导此展开时的自由度，允许不同形式的迁移率核。

Conclusion: 这些发展为系统性地改进已建立的平均场形式主义提供了数学基础，揭示了动态响应理论中的新结构和自由度。

Abstract: We derive an exact Volterra series expansion for a mean field of an interacting particle system subject to a potential perturbation, expressing the Volterra expansion kernels in terms of the field's response functions, to any order. Applying this formalism to the mean particle density of a simple fluid, we identify a form reminiscent of dynamical density functional theory, with, however, fundamental differences: A nonlocal mobility kernel appears, and forces derive from a functional of the {\it history} of mean density. The equilibrium density functional is shown to be recovered in the limit of slowly varying perturbation. We identify a freedom in deriving this expansion, which allows different forms of mobility kernels. These developments allow for a systematic improvement of established mean field formalisms.

</details>


### [99] [Slow mixing and emergent one-form symmetries in three-dimensional $\mathbb{Z}_2$ gauge theory](https://arxiv.org/abs/2601.06010)
*Charles Stahl,Benedikt Placke,Vedika Khemani,Yaodong Li*

Main category: cond-mat.stat-mech

TL;DR: 三维经典Z₂规范理论在解禁闭相中具有指数级缓慢的弛豫动力学，即使存在破坏一形式对称性的微扰，熵效应仍导致自由能垒发散，使其成为鲁棒的有限温度经典存储器。


<details>
  <summary>Details</summary>
Motivation: 研究经典拓扑序中的弛豫动力学，探索在局部不可区分的有序态之间，即使没有传统界面概念的情况下，如何出现缓慢动力学。特别关注三维经典Z₂规范理论作为典型例子，理解其解禁闭相中的记忆存储机制。

Method: 1. 理论证明：在解禁闭相中证明混合时间的下界tmix = exp[Ω(L)]，其中L为系统线性尺寸。2. 考虑破坏一形式对称性的微扰，分析熵效应如何导致自由能垒发散。3. 数值模拟：探索解禁闭相到希格斯相和禁闭相转变时的混合时间尺度，比较动态标度行为。

Result: 1. 证明了解禁闭相中混合时间的指数下界，表明弛豫时间随系统尺寸指数增长。2. 即使破坏一形式对称性，熵效应仍能维持自由能垒，使系统成为鲁棒的有限温度经典存储器。3. 发现希格斯相变和禁闭相变具有不同的动态标度行为，尽管两者静态临界指数均为三维伊辛模型。4. 建立了涌现一形式对称性的精确概念。

Conclusion: 经典拓扑序中的熵效应可以导致指数级缓慢的弛豫动力学，即使在没有传统能量垒的情况下。这种机制为有限温度经典存储器提供了理论基础，并有望为自校正量子存储器提供新见解。研究揭示了拓扑相中动态行为的普适特性。

Abstract: Symmetry-breaking order at low temperatures is often accompanied by slow relaxation dynamics, due to diverging free-energy barriers arising from interfaces between different ordered states. Here, we extend this correspondence to classical topological order, where the ordered states are locally indistinguishable, so there is no notion of interfaces between them. We study the relaxation dynamics of the three-dimensional (3D) classical $\mathbb{Z}_2$ lattice gauge theory (LGT) as a canonical example. We prove a lower bound on the mixing time in the deconfined phase, $t_{\text{mix}} = \exp [Ω(L)]$, where L is the linear system size. This bound applies even in the presence of perturbations that explicitly break the one-form symmetry between different long-lived states. This perturbation destroys the energy barriers between ordered states, but we show that entropic effects nevertheless lead to diverging free-energy barriers at nonzero temperature. Our proof establishes the LGT as a robust finite-temperature classical memory. We further prove that entropic effects lead to an emergent one-form symmetry, via a notion that we make precise. We argue that the exponential mixing time follows from universal properties of the deconfined phase, and numerically corroborate this expectation by exploring mixing time scales at the Higgs and confinement transitions out of the deconfined phase. These transitions are found to exhibit markedly different dynamic scaling, even though both have the static critical exponents of the 3D Ising model. We expect this novel entropic mechanism for memory and emergent symmetry to also bring insight into self-correcting quantum memories.

</details>
