<div id=toc></div>

# Table of Contents

- [quant-ph](#quant-ph) [Total: 5]
- [physics.data-an](#physics.data-an) [Total: 2]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 2]
- [cs.CC](#cs.CC) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [nlin.CD](#nlin.CD) [Total: 1]
- [cs.AI](#cs.AI) [Total: 12]
- [cs.LG](#cs.LG) [Total: 18]


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [1] [Schwinger-Keldysh field theory for operator Rényi entropy and entanglement growth in non-interacting systems with sub-ballistic transports](https://arxiv.org/abs/2602.22331)
*Priesh Roy,Sumilan Banerjee*

Main category: quant-ph

TL;DR: 提出基于子系统算符雷尼熵的量子算符增长度量，建立算符增长与输运行为的直接联系，揭示无序系统中纠缠增长与反常输运的统一描述


<details>
  <summary>Details</summary>
Motivation: 传统算符增长度量（如OTOC）和纠缠熵缺乏状态无关性，且难以直接关联局域守恒量的时空输运行为，需建立统一框架连接量子动力学中的算符增长、纠缠生成与输运现象

Method: 1. 定义子系统算符雷尼熵作为状态无关的算符增长度量；2. 构建统一的Schwinger-Keldysh场论形式化体系，将算符雷尼熵与无限温度格林函数、真空纠缠熵与基态格林函数关联；3. 应用于准周期/无序系统（AA模型、安德森模型）

Result: 1. 子系统算符雷尼熵同时编码空间与时间信息；2. 在无序系统中，算符雷尼熵、冯诺依曼熵及雷尼熵的增长可定量刻画弹道输运、扩散/反常扩散及局域化行为；3. 实现算符增长、纠缠增长与输运动力学的统一描述

Conclusion: 子系统算符雷尼熵是连接量子多体系统算符动力学、纠缠生成与宏观输运的普适指标，为研究非平衡量子输运与量子信息传播提供新理论工具

Abstract: The notion of operator growth in quantum systems furnishes a bridge between transport and the generation of entanglement between different parts of the system under quantum dynamics. We define a measure of operator growth in terms of subsystem operator Rényi entropy, which provides a state-independent measure of operator growth, unlike entanglement entropies, and the usual measures of operator growth like out-of-time-order correlators. We show that the subsystem operator Rényi entropy encodes both spatial and temporal information, and thus can directly connect to transport for a local operator related to a conserved quantity. We construct a unified Schwinger-Keldysh (SK) field theory formalism for the time evolution of operator Rényi entropy and entanglement entropies of initial pure states. We use the SK field theory to obtain the operator Rényi and state entanglement entropies in terms of infinite-temperature and vacuum Keldysh Green's functions, respectively, for non-interacting systems. We apply the method to explore the connection between operator and entanglement growth, and transport in non-interacting systems with quasiperiodic and random disorder, like the one- and two-dimensional Aubry-André models and the two-dimensional Anderson model. In particular, we show that the growth of subsystem operator Rényi entropy and state von Neumann and Rényi entanglement entropies can capture both ballistic and sub-ballistic transport behaviors, like diffusive and anomalous diffusive transport, as well as localization in these systems.

</details>


### [2] [Partial Reversibility and Counterdiabatic Driving in Nearly Integrable Systems](https://arxiv.org/abs/2602.22317)
*Rohan Banerjee,Shahyad Khamnei,Anatoli Polkovnikov,Stewart Morawetz*

Main category: quant-ph

TL;DR: 研究混合相空间中可逆过程的极限，探索反绝热驱动对抗耗散损失，并将现象推广到量子多体系统


<details>
  <summary>Details</summary>
Motivation: 热力学可逆性（熵守恒）与动力学可逆性（作用量守恒）之间存在认知鸿沟，混合相空间中可逆过程的可行性尚不明确，需建立统一理解框架

Method: 理论分析混合相空间可逆过程极限，研究快速驱动下的耗散损失，提出近似反绝热驱动方案，并推广至具有简并性和可积性破坏的量子多体系统

Result: 确定了混合相空间中可逆过程的可行范围，证实近似反绝热驱动能有效抑制耗散损失，论证了量子多体系统中存在相似现象学特征

Conclusion: 实现了经典与量子系统可逆过程的理论统一，为在非理想条件下维持绝热性提供了新方法，拓展了绝热热力学与动力学系统的交叉研究边界

Abstract: Adiabatic (or reversible) processes are the key concept unifying our understanding of thermodynamics and dynamical systems. Reversibility in the thermodynamic sense is understood as entropy-preserving processes, such as in the idealized Carnot engine, whereas in integrable dynamical systems it is understood as the conservation of the action variables. Between these two idealized limits, however, where the phase space can become mixed, things are much less clear. In this work, we first determine the extent to which reversible processes are even possible in this regime. We then explore how the dissipative losses resulting from rapidly driving these kinds of systems can be fought by approximate counterdiabatic driving. Finally, we argue that much of the phenomenology should be the same for quantum many-body systems with large degeneracy in the presence of integrability breaking perturbations.

</details>


### [3] [Robustness-Runtime Tradeoff for Quantum State Transfer](https://arxiv.org/abs/2602.22312)
*Twesh Upadhyaya,Yifan Hong,T. C. Mooney,Alexey V. Gorshkov*

Main category: quant-ph

TL;DR: 提出量子态传输协议的鲁棒性定义，量化其对初始辅助态误差的容忍度，证明鲁棒性与交换子Schatten p-范数的紧密边界，并给出部分依赖态传输的新运行时间界限及更优协议。


<details>
  <summary>Details</summary>
Motivation: 现有利用幂律相互作用的量子态传输协议依赖完美初始化的辅助位，但实际中噪声和控制不完美会导致初始态存在误差，亟需量化协议对这类误差的容忍能力（即鲁棒性）。

Method: 在Heisenberg图像中分析算符增长，将鲁棒性与初始/末态算符交换子的Schatten p-范数建立严格边界关系，推广p=∞（完全依赖态）和p=2（完全独立态）的已知结论。

Result: 1) 证明鲁棒性紧约束交换子p-范数；2) 揭示p值控制态依赖程度（p居中对应部分依赖态）；3) 结合光锥理论给出新协议的最小运行时间，在特定区间显著优于现有界限；4) 提出新型鲁棒传输协议。

Conclusion: 建立了量子态传输鲁棒性的理论框架，统一了态依赖/独立传输的极端情况，为噪声环境下的高效量子信息传输提供了新协议和优化路径。

Abstract: Quantum state transfer is the primitive of transporting an unknown state on one site of a lattice to another. Using power-law interactions, recent state transfer protocols achieve speedup by utilizing the intermediate ancilla sites. However, these protocols require the ancillas to be in a perfectly initialized state, which, due to noise or imperfect control, may not be the case. In this work we introduce the $\textit{robustness}$ of a state transfer protocol, which quantifies the protocol's tolerance to error in the initial ancilla state. In the Heisenberg picture, state transfer grows operators supported on the final site such that they no longer commute with all operators on the starting site. We prove that this robustness tightly bounds the Schatten $p$-norms of these commutators between initial and final-site operators. This generalizes the known cases of $p=\infty$ and $p=2$, which govern completely state-dependent and state-independent state transfer respectively, demonstrating that intermediate values of $p$ govern partially state-dependent state transfer. In conjunction with existing power-law light cones, our result gives new minimum runtimes for partially state-dependent protocols which, in certain regimes, are parametrically better than existing bounds. We introduce new robust state transfer protocols, charting the landscape between complete state-dependence and state-independence.

</details>


### [4] [Macroscopic Quantum Electrodynamics with Gain: Modified Fluctuations and Their Consequences](https://arxiv.org/abs/2602.22429)
*Daigo Oue*

Main category: quant-ph

TL;DR: This tutorial introduces Macroscopic Quantum Electrodynamics (MQED) as a unified framework for quantum electromagnetic fields in macroscopic environments, emphasizing field correlations and their extension to active media to explain phenomena like Casimir forces.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive theoretical framework that describes quantum electromagnetic phenomena in arbitrary macroscopic environments, bridging quantum field behavior with classical material responses.

Method: MQED framework utilizing field correlation functions to unify the description of radiative effects (Lamb shifts, Purcell effect) and mechanical phenomena (van der Waals and Casimir forces), extended to active media with gain-modified correlations.

Result: Demonstrates that fluctuation-induced forces in both passive and active media can be fundamentally understood as manifestations of modified field correlation functions, with gain media altering these correlations.

Conclusion: MQED serves as an essential tutorial framework for understanding quantum electromagnetic phenomena in complex environments, with particular value in explaining how material properties (including gain) modify quantum field correlations and associated forces.

Abstract: Macroscopic quantum electrodynamics (MQED) provides a unified framework to describe quantum electromagnetic fields in the presence of arbitrary macroscopic environments. Central to this theory is the field correlation, which governs both radiative (e.g., Lamb shifts and the Purcell effect) and mechanical phenomena, such as van der Waals and Casimir forces. In this tutorial, we provide an overview of MQED and its extension to active media, highlighting fluctuation-induced forces as manifestations of gain-modified field correlations.

</details>


### [5] [The Road to Useful Quantum Computers](https://arxiv.org/abs/2602.22540)
*Timothy Proctor,Robin Blume-Kohout,Andrew Baczewski*

Main category: quant-ph

TL;DR: 该论文探讨了量子计算的现状与实用化（量子效用）之间的差距，分析了当前技术能力、关键挑战及实现实用化的路径。


<details>
  <summary>Details</summary>
Motivation: 构建实用量子计算机是重大科学与工程挑战，虽自1980年代费曼等人提出概念、肖尔算法及量子纠错理论推动发展，但当前量子计算机尚未解决具有科学或实践价值的问题，存在 hype 与现实的鸿沟。

Method: 通过描述当代量子计算机的能力，对比量子效用的要求，提出从当前状态向实用化进展的评估方法，并结合作者研究强调关键科学与工程挑战。

Result: 量子计算机尚未实现"量子效用"（即解决实际问题），现有技术仍面临能力与需求间的显著差距，需突破关键挑战才能达成目标。

Conclusion: 实现量子效用需持续攻克科学与工程难题，论文为进展跟踪提供框架，并呼吁聚焦核心挑战以推动从实验室原型向实用化过渡。

Abstract: Building a useful quantum computer is a grand science and engineering challenge, currently pursued intensely by teams around the world. In the 1980s, Richard Feynman and Yuri Manin observed independently that computers based on quantum mechanics might enable better simulations of quantum phenomena. Their vision remained an intellectual curiosity until Peter Shor published his famous quantum algorithm for integer factoring, and shortly thereafter a proof that errors in quantum computations can be corrected. Since then, quantum computing R&D has progressed rapidly, from small-scale experiments in university physics laboratories to well-funded industrial efforts and prototypes. Hype notwithstanding, quantum computers have yet to solve scientifically or practically important problems -- a target often called quantum utility. In this article, we describe the capabilities of contemporary quantum computers, compare them to the requirements of quantum utility, and illustrate how to track progress from today to utility. We highlight key science and engineering challenges on the road to quantum utility, touching on relevant aspects of our own research.

</details>


<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [6] [Maximum Likelihood Particle Tracking in Turbulent Flows via Sparse Optimization](https://arxiv.org/abs/2602.22257)
*Griffin M Kearney,Kasey M Laurent,Makan Fardad*

Main category: physics.data-an

TL;DR: 针对湍流中拉格朗日粒子加速度推断难题，提出一种显式考虑非高斯间歇性的最大似然估计框架，通过稀疏优化和迭代重加权最小二乘法，显著提升加速度统计特性恢复能力，保留湍流物理间歇性。


<details>
  <summary>Details</summary>
Motivation: 现有滤波方法（高斯核、惩罚B样条）隐含假设加加速度为高斯分布，导致稀疏高幅值加速度变化被抑制，无法恢复湍流中重尾分布的非高斯间歇性特征。

Method: 构建改进高斯过程建模随机增量强迫，引入凸1-范数松弛的稀疏优化方案，采用迭代重加权最小二乘法(IRLS)求解高阶梯度算子数值刚性问题。

Result: 在Re≈310各向同性湍流DNS数据测试中，新方法在位置/速度/加速度的均方根误差上全面优于现有方法，关键成功恢复了加速度和加加速度跨时间尺度的重尾统计结构。

Conclusion: 该框架有效保留了高雷诺数湍流的物理间歇性特征，解决了传统方法对间歇性尾部的严重衰减问题，为湍流精细结构研究提供新工具。

Abstract: Lagrangian particle tracking is essential for characterizing turbulent flows, but inferring particle acceleration from inherently noisy position data remains a significant challenge. Fluid particles in turbulence experience extreme, intermittent accelerations, resulting in heavy-tailed probability density functions (PDFs) that deviate strongly from Gaussian predictions. Existing filtering techniques, such as Gaussian kernels and penalized B-splines, implicitly assume Gaussian-distributed jerk, thereby penalizing sparse, high-magnitude acceleration changes and artificially suppressing the intermittent tails. In this work, we develop a novel maximum likelihood estimation (MLE) framework that explicitly accounts for this non-Gaussian intermittency. By formulating a modified Gaussian process to model the random incremental forcing, we introduce a sparse optimization scheme utilizing a convex 1-norm relaxation. To overcome the numerical stiffness associated with high-order difference operators, the problem is efficiently solved using an iteratively reweighted least squares (IRLS) algorithm. The proposed filter is evaluated against direct numerical simulation (DNS) data of homogeneous, isotropic turbulence (Re approx. 310). Results demonstrate that the IRLS approach consistently outperforms state-of-the-art discrete MLE, continuous MLE, and B-spline methods, yielding systematic reductions in root-mean-squared error (RMSE) across position, velocity, and acceleration. Most importantly, the proposed framework succeeds in better recovering the heavy-tailed statistical structure of both acceleration and acceleration differences (jerk) across temporal scales, preserving the physical intermittency characteristic of high-Reynoldsnumber turbulent flows that baseline methods severely attenuate.

</details>


### [7] [Titanic overconfidence -- dark uncertainty can sink hybrid metrology for semiconductor manufacturing](https://arxiv.org/abs/2602.23131)
*Ronald G. Dixson,Adam L. Pintar,R. Joseph. Kline,Thomas A. Germer,J. Alexander Liddle,John S. Villarrubia,Samuel M. Stavis*

Main category: physics.data-an

TL;DR: 这篇论文揭示了半导体混合计量中的"暗不确定性"问题。通过对比两种统计模型对13nm线宽测量的分析，发现忽略暗不确定性的模型会严重低估总不确定性达5倍，而考虑暗不确定性的随机效应模型更符合实际。论文提出了减少暗不确定性的良好实践。


<details>
  <summary>Details</summary>
Motivation: 半导体制造中的混合计量面临"暗不确定性"挑战。IEEE技术路线图要求在95%置信度下线宽不确定度达到±0.17 nm，但现有统计模型需要一致结果才能降低不确定度，而实际中不一致结果普遍存在。标准不确定度评估方法未能解释暗不确定性的因果，可能导致严重过度自信。

Method: 重新分析成像和散射方法测量约13 nm线宽的对比数据，应用两种对比统计模型：随机效应模型和共同均值模型。随机效应模型允许合并不一致结果并考虑暗不确定性，而共同均值模型要求结果一致才能合并，忽略暗不确定性。

Result: 随机效应模型在95%置信度下给出±0.8 nm的总不确定度，合理考虑了暗不确定性。相比之下，共同均值模型忽略暗不确定性，严重低估总不确定度高达5倍。这揭示了标准方法的重大缺陷。

Conclusion: 为避免因过度自信导致项目失败，必须重视暗不确定性。论文提出减少暗不确定性的良好实践，并指导如何合理合并不确定一致的结果，以确保混合计量能够达到半导体制造的严格精度要求。

Abstract: Hybrid metrology for semiconductor manufacturing is on a collision course with dark uncertainty. An IEEE technology roadmap for this venture has targeted a linewidth uncertainty of +/- 0.17 nm at 95 % coverage and advised the hybridization of results from different measurement methods to hit this target. Related studies have applied statistical models that require consistent results to compel a lower uncertainty, whereas inconsistent results are prevalent. We illuminate this lurking issue, studying how standard methods of uncertainty evaluation fail to account for the causes and effects of dark uncertainty. We revisit a comparison of imaging and scattering methods to measure linewidths of approximately 13 nm, applying contrasting statistical models to highlight the potential effect of dark uncertainty on hybrid metrology. A random effects model allows the combination of inconsistent results, accounting for dark uncertainty and estimating a total uncertainty of +/- 0.8 nm at 95 % coverage. In contrast, a common mean model requires consistent results for combination, ignoring dark uncertainty and underestimating the total uncertainty by as much as a factor of five. To avoid such titanic overconfidence, which can sink a venture, we outline good practices to reduce dark uncertainty and guide the combination of indeterminately consistent results.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [8] [Dephasing-induced relaxation in tight-binding chains with linear and nonlinear defects](https://arxiv.org/abs/2602.22878)
*Debraj Das,Andrea Gambassi,Stefano Iubini,Stefano Lepri*

Main category: cond-mat.stat-mech

TL;DR: 该研究建立了一个紧束缚链中热化、罕见涨落与弛豫路径的统一理论框架，揭示局域缺陷作为热化瓶颈的标度律和非线性加速机制


<details>
  <summary>Details</summary>
Motivation: 探究含局域缺陷和退相干噪声的紧束缚系统中热化动力学，特别关注缺陷如何影响弛豫过程、罕见轨迹及作用空间中的不同弛豫路径

Method: 对线性缺陷：求解精确能谱并构建本征态基下的动力学；推导模粒子数的动力学方程（作用空间连续时间随机游走），其跃迁率由本征态空间结构的重叠矩阵精确确定；结合大偏差理论分析罕见轨迹

Result: 1. 强缺陷下局域模成为弛豫瓶颈，弛豫速率按ε⁻²标度减缓；2. ε→∞时大偏差函数呈现动力学相变；3. 非线性缺陷（无论嵌入线性或全非线性体系）因振幅依赖的缺陷减弱效应而显著加速热化

Conclusion: 为随机紧束缚系统的热化机制、罕见涨落及多路径弛豫提供了普适理论描述，揭示了缺陷强度与非线性的关键调控作用

Abstract: We investigate thermalization in a tight-binding chain with an on-site defect subject to local dephasing noise implemented as random phase kicks. For a single linear defect of strength $ε$, we obtain an exact analytical description of the system spectrum and formulate the dephasing-induced dynamics in the eigenstate basis. We derive an approximate kinetic equation for mode populations that describes a continuous-time random walk in action space. The walk transition rates are defined by the overlap matrix encoding the spatial structure of eigenstates that can be computed exactly. Analyzing the spectral properties of the equation, we show that defect-induced localized modes act as bottlenecks that strongly slow down relaxation, with rates scaling as $ε^{-2}$ for strong defects. Using large-deviation theory, we characterize rare dynamical trajectories and identify distinct relaxation pathways associated with low- and high-activity regimes in action space. We provide numerical evidence that the large-deviation function exhibits a dynamical phase transition in the limit $ε\to \infty$. We then extend our analysis to the nonlinear case, considering a single nonlinear defect embedded in either a linear or a fully nonlinear discrete Schrödinger equation. Numerical simulations reveal a qualitatively faster approach to equilibrium driven by the amplitude-dependent weakening of the defect. Our results provide a unified framework for understanding thermalization, rare fluctuations, and relaxation pathways in stochastic tight-binding systems.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [9] [Adaptive Patching for Tensor Train Computations](https://arxiv.org/abs/2602.22372)
*Gianluca Grosso,Marc K. Ritter,Stefan Rohshap,Samuel Badr,Anna Kauch,Markus Wallerberger,Jan von Delft,Hiroshi Shinaoka*

Main category: physics.comp-ph

TL;DR: 提出自适应分块方案，利用块稀疏QTT结构通过分治法降低大键维度下的计算成本，实现局部函数显著加速和气泡图/BSE高效计算，开启大规模QTT计算新可能


<details>
  <summary>Details</summary>
Motivation: 大键维度下量子张量列车（QTT）操作（如矩阵乘积算符收缩）计算成本过高，难以进行大规模计算

Method: 开发自适应分块方案，利用块稀疏QTT结构，通过分治法将张量自适应分割为键维度更小的子块

Result: 对尖锐局域化函数实现显著性能提升，可高效计算气泡图和Bethe-Salpeter方程

Conclusion: 该方法突破了原有大规模QTT计算的局限，为此前无法实现的实际大规模QTT计算开辟了新途径

Abstract: Quantics Tensor Train (QTT) operations such as matrix product operator contractions are prohibitively expensive for large bond dimensions. We propose an adaptive patching scheme that exploits block-sparse QTT structures to reduce costs through divide-and-conquer, adaptively partitioning tensors into smaller patches with reduced bond dimensions. We demonstrate substantial improvements for sharply localized functions and show efficient computation of bubble diagrams and Bethe-Salpeter equations, opening the door to practical large-scale QTT-based computations previously beyond reach.

</details>


### [10] [Discovery of Interpretable Physical Laws in Materials via Language-Model-Guided Symbolic Regression](https://arxiv.org/abs/2602.22967)
*Yifeng Guan,Chuyi Liu,Dongzhan Zhou,Lei Bai,Wan-jian Yin,Jingyuan Li,Mao Su*

Main category: physics.comp-ph

TL;DR: 提出利用大语言模型嵌入的科学知识引导符号回归搜索过程，高效发现钙钛矿材料性质的可解释物理定律，将搜索空间缩小约10^5倍，获得更准确简洁的新公式


<details>
  <summary>Details</summary>
Motivation: 传统符号回归在搜索高维数据中的物理定律时易产生复杂无物理意义的公式，且面临组合爆炸问题，亟需高效可解释的自动化发现方法

Method: 构建基于大语言模型的引导框架，利用其内置科学知识约束搜索空间，针对钙钛矿材料的体弹模量、带隙和氧析出反应活性进行建模

Result: 将传统方法的组合爆炸搜索空间缩小约10^5倍，成功识别出三个新颖物理公式，在准确性和简洁性上均优于现有公式

Conclusion: 该方法有效缓解符号回归的搜索瓶颈，生成的公式兼具物理意义和预测性能，为自动化科学发现提供了新范式

Abstract: Discovering interpretable physical laws from high-dimensional data is a fundamental challenge in scientific research. Traditional methods, such as symbolic regression, often produce complex, unphysical formulas when searching a vast space of possible forms. We introduce a framework that guides the search process by leveraging the embedded scientific knowledge of large language models, enabling efficient identification of physical laws in the data. We validate our approach by modeling key properties of perovskite materials. Our method mitigates the combinatorial explosion commonly encountered in traditional symbolic regression, reducing the effective search space by a factor of approximately $10^5$. A set of novel formulas for bulk modulus, band gap, and oxygen evolution reaction activity are identified, which not only provide meaningful physical insights but also outperform previous formulas in accuracy and simplicity.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [11] [Impact of Stealthy Hyperuniform Magnetic Impurity Configurations on Bulk Magnetism in a Two-dimensional Heisenberg Model](https://arxiv.org/abs/2602.22484)
*K. Asakura,K. Yamamoto,A. Koga*

Main category: cond-mat.dis-nn

TL;DR: 研究高自旋磁性杂质在反铁磁量子海森堡模型中的排列方式对体磁性的影响，发现类三角晶格排列比随机和类正方晶格排列产生更大的交错磁化强度，这源于亚晶格效应


<details>
  <summary>Details</summary>
Motivation: 探究随机和隐身超均匀杂质构型如何影响块状磁性性质

Method: 使用线性自旋波理论研究混合自旋模型，通过广义成本函数生成在正方晶格和三角晶格排列之间插值的隐身超均匀构型

Result: 类三角晶格排列比随机和类正方晶格情况产生更大的平均交错磁化强度，这源于亚晶格效应：正方晶格结构迫使最近邻杂质占据相反亚晶格，而三角晶格排列允许同亚晶格最近邻对

Conclusion: 三角晶格状排列通过允许同亚晶格最近邻对来增强协同磁增强，从而获得更大的交错磁化强度

Abstract: We investigate an antiferromagnetic quantum Heisenberg model on a square lattice with high-spin magnetic impurities to clarify how random and stealthy hyperuniform impurity configurations influence the bulk magnetic properties. Stealthy hyperuniform configurations are generated using generalized cost functions that interpolate between square-lattice-like and triangular-lattice-like arrangements. Using linear spin-wave theory for the mixed-spin model, we demonstrate that triangular-lattice-like arrangements yield a larger average staggered magnetization than both random and square-lattice-like cases. This enhancement originates from sublattice effects: while the square-lattice-like configuration enforces nearest-neighbor impurities to occupy opposite sublattices due to its bipartite structure, the triangular-lattice-like arrangement allows same-sublattice nearest-neighbor pairs, thereby strengthening cooperative magnetic enhancement.

</details>


### [12] [Exact mapping of a spin glass with correlated disorder to the pure Ising model](https://arxiv.org/abs/2602.22657)
*Hidetoshi Nishimori*

Main category: cond-mat.dis-nn

TL;DR: 该论文提出了一种具有相关无序的伊辛自旋玻璃模型，证明了在Nishimori线上物理量可精确表示为纯伊辛模型在有效温度下的对应量，揭示了临界行为呈现纯伊辛普适类而非传统多临界行为。


<details>
  <summary>Details</summary>
Motivation: 研究自旋玻璃中无序关联性的影响，挑战标准Edwards-Anderson模型的多临界普适类理论，探索未被充分研究的无序相关性在自旋玻璃及相关问题中的作用。

Method: 构建一个在纯铁磁伊辛模型和对称无序Edwards-Anderson模型之间连续插值的自旋玻璃模型，利用规范对称性分析，严格证明Nishimori线上物理量与纯伊辛模型的精确对应关系。

Result: 在Nishimori线上：1) 能量等于纯伊辛模型在有效温度下的能量；2) 比热等于纯伊辛模型在相同温度下的能量而非比热；3) 磁化强度和关联函数与纯伊辛模型在有效温度下严格相等；4) 多临界点的主导临界行为呈现纯伊辛普适类特征。

Conclusion: 无序相关性导致该模型的多临界点普适类与标准Edwards-Anderson模型不同，表现为纯伊辛型临界行为，这一发现为自旋玻璃中无序关联性的研究提供了新方向，并激励了相关领域的进一步探索。

Abstract: We introduce an Ising spin-glass model with correlated disorder which continuously interpolates between the pure ferromagnetic Ising model and the Edwards-Anderson model with symmetric disorder. For this model, we prove that physical quantities on the Nishimori line (NL) can be expressed exactly in terms of those of the pure Ising model at an effective temperature on any lattice in any dimension. For example, the energy on the NL is equal to the energy of the pure Ising model at the effective temperature up to a constant and a trivial factor. More remarkably, the specific heat on the NL equals the energy, not the specific heat, of the pure Ising model at the effective temperature, again up to a constant and a trivial factor. Gauge-noninvariant quantities such as the magnetization and correlation functions are exactly equal to the corresponding quantities of the pure Ising model at the effective temperature. These exact relations imply that the leading critical behavior at that multicritical point for the disorder-correlated model is pure-Ising-like, in contrast to the conventional multicritical universality class of the standard Edwards-Anderson model. Our results motivate further investigations of the relatively unexplored topic of correlations in disorder in spin glasses and related problems.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [13] [Dynamic Level Sets](https://arxiv.org/abs/2602.22530)
*Michael Stephen Fiske*

Main category: cs.CC

TL;DR: The paper identifies and analyzes a novel mathematical concept called "dynamic level sets" implicit in a 2012 Turing Centenary Conference paper, explaining why this concept eluded prior characterization, including classical results on the computational equivalence of probabilistic and deterministic Turing machines.


<details>
  <summary>Details</summary>
Motivation: To explicate a mathematical concept that was implicit but not explicitly identified in prior work on Turing incomputable computation, and to understand why it was overlooked despite extensive study in dynamical systems, topology, and computability theory.

Method: Conceptual analysis of existing literature to identify an implicit mathematical structure, with examination of its relationship to classical computability theory results.

Result: Dynamic level sets constitute a distinct mathematical object not previously characterized in standard literature, suggesting that certain computational phenomena may have been missed by classical frameworks.

Conclusion: The discovery of dynamic level sets reveals a potential limitation in classical computability theory and opens new avenues for understanding computational processes that may circumvent traditional boundaries between probabilistic and deterministic computation.

Abstract: A mathematical concept is identified and analyzed that is implicit in the 2012 paper Turing Incomputable Computation, presented at the Alan Turing Centenary Conference (Turing 100, Manchester). The concept, called dynamic level sets, is distinct from mathematical concepts in the standard literature on dynamical systems, topology, and computability theory. A new mathematical object is explained and why it may have escaped prior characterizations, including the classical result of de Leeuw, Moore, Shannon, and Shapiro (1956) that probabilistic Turing machines compute no more than deterministic ones.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [14] [Tuning the magnetic properties of Kitaev materials via the antiferromagnetic proximity effect: Novel phases and application to an $α$-RuCl$_3$/MnPS$_3$ bilayer](https://arxiv.org/abs/2602.22310)
*Pedro M. Cônsoli,Ezra Day-Roberts,Johannes Knolle,Antia S. Botana,Onur Erten*

Main category: cond-mat.str-el

TL;DR: 该研究探讨了在范德华异质结构中，将Kitaev蜂窝磁体（如α-RuCl₃）与晶格匹配的范德华反铁磁体结合时，由反铁磁体产生的有效交错磁场可驱动Kitaev材料进入多种新物相，包括反手性Kitaev自旋液体、非磁性向列相和不同类型的斯格明子晶体。通过第一性原理模拟，评估了在α-RuCl₃/MnPS₃异质双层中实现该体系的可行性。


<details>
  <summary>Details</summary>
Motivation: 近年来，对范德华（vdW）异质结构控制水平的提升为调控量子材料特性开辟了新途径。受此启发，本研究探究了在Kitaev蜂窝磁体与近晶格匹配的vdW反铁磁体界面处可能产生的新颖量子物相。

Method: 结合微扰理论、精确对角化方法和经典能量最小化方法进行理论计算，并应用第一性原理模拟评估具体材料体系（α-RuCl₃和MnPS₃）的实现前景。

Result: 发现来自vdW反铁磁体的有效交错磁场可将Kitaev单层驱动至多种新物相：反手性Kitaev自旋液体、非磁性向列相以及不同类型的斯格明子晶体。

Conclusion: 该异质结构设计（特别是α-RuCl₃/MnPS₃双层）为实验实现这些新颖量子物相提供了可行途径，展示了范德华异质结构在调控量子材料物性方面的巨大潜力。

Abstract: In recent years, the increasing level of control over van der Waals (vdW) heterostructures has opened new routes to tune the properties of quantum materials. Motivated by these developments, we examine the potential consequences of interfacing a Kitaev honeycomb magnet, such as $α$-RuCl$_3$, with a nearly lattice-matched vdW antiferromagnet. By combining perturbation theory, exact diagonalization, and a classical energy-minimization method, we show that an effective staggered magnetic field originating from the vdW antiferromagnet can drive a monolayer of a Kitaev material into various novel phases, including an antichiral Kitaev spin liquid, a nonmagnetic nematic phase, and different types of skyrmion crystals. We then apply first-principle simulations to assess the prospect of concretely realizing this setup in a heterobilayer of $α$-RuCl$_3$ and the easy-axis antiferromagnet MnPS$_3$.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [15] [Interplay of Nonsmoothness, Time Delay, and Stochasticity in Turning Dynamics](https://arxiv.org/abs/2602.23272)
*Meiyazhagan Jaganathan,Vikram Pakrashi,Aasifa Rounak*

Main category: nlin.CD

TL;DR: 该论文通过数值方法研究正交金属切削的随机动力学，证明忽略非光滑摩擦和随机效应的模型存在缺陷，发现随机扰动会导致P/D分岔，可用熵度量量化转变，并通过改进的流域稳定性分析揭示通过控制初始位移和工件粗糙度来抑制颤振的实用策略。


<details>
  <summary>Details</summary>
Motivation: 现有金属切削动力学模型常忽略非光滑摩擦效应和随机扰动，导致对颤振等复杂非线性现象的预测不准确，影响精密制造质量。本研究旨在揭示这些被忽略因素的实际影响。

Method: 采用数值仿真建立包含再生效应和非光滑摩擦的随机动力学模型，运用熵度量量化动态转变，开展针对随机性和时滞的改进流域稳定性分析，系统研究不同工件表面粗糙度下的刀具动态行为。

Result: 刀具运动呈现丰富的非线性现象（如颤振中的粘滑运动），随机扰动导致随机P/D分岔；熵度量能有效捕捉系统转变；流域稳定性分析表明可通过限制初始刀具位移和控制工件初始粗糙度来抑制颤振。

Conclusion: 非光滑摩擦和随机效应必须纳入切削动力学模型；熵是量化系统复杂性的有效工具；通过调控初始条件可实现颤振控制，为精密制造提供实用策略，改善加工质量。

Abstract: The stochastic dynamics of orthogonal metal cutting with both regenerative and nonsmooth frictional effects are investigated numerically in this paper. The shortcomings of neglecting nonsmoothness in frictional and stochastic effects in modeling the dynamics of such a machining process are demonstrated. Dynamics of the tool motion is observed to exhibit rich nonlinear phenomena such as stick-slip during chatter, with stochastic perturbations in cutting forces adding further complexity, leading to the occurrence of stochastic P and D bifurcations. Measures of entropy are found to be effective in quantifying the dynamical transitions occurring in the dynamics of the tool. Subsequently, basin stability analyses, modified to account for stochasticity and time-delays, are carried out to systematically investigate the dynamics of the cutting tool across multiple surface roughness profiles of the workpiece. Basin stability analyses indicate that chatter can be controlled by restricting initial tool displacement and controlling initial workpiece surface roughness, suggesting practical strategies to improve machining outcomes for precision manufacturing.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [16] [Graph Your Way to Inspiration: Integrating Co-Author Graphs with Retrieval-Augmented Generation for Large Language Model Based Scientific Idea Generation](https://arxiv.org/abs/2602.22215)
*Pengzhen Xie,Huizhi Liang*

Main category: cs.AI

TL;DR: GYWI系统结合作者知识图谱与RAG技术，通过强化学习优化，显著提升了LLM生成科学创意的创新性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在科学创意生成领域展现潜力，但生成的结果往往缺乏可控的学术背景和追溯的灵感路径。

Method: 提出GYWI系统，通过作者中心知识图谱构建和灵感源采样算法建立外部知识库；采用RAG与GraphRAG混合检索机制；引入强化学习原理优化Prompt策略；在arXiv(2018-2023)数据集上通过多选题任务、LLM评分、人工评估和语义空间可视化进行五维度评估(新颖性、可行性、清晰度、相关性、重要性)。

Result: 在GPT-4o、DeepSeek-V3、Qwen3-8B和Gemini 2.5等模型上的实验表明，GYWI在创新性、可靠性和相关性等多个指标上显著优于主流大语言模型。

Conclusion: GYWI系统通过结合知识图谱与检索增强技术，有效解决了LLM科学创意生成中背景不可控和路径不可追溯的问题，性能显著提升。

Abstract: Large Language Models (LLMs) demonstrate potential in the field of scientific idea generation. However, the generated results often lack controllable academic context and traceable inspiration pathways. To bridge this gap, this paper proposes a scientific idea generation system called GYWI, which combines author knowledge graphs with retrieval-augmented generation (RAG) to form an external knowledge base to provide controllable context and trace of inspiration path for LLMs to generate new scientific ideas. We first propose an author-centered knowledge graph construction method and inspiration source sampling algorithms to construct external knowledge base. Then, we propose a hybrid retrieval mechanism that is composed of both RAG and GraphRAG to retrieve content with both depth and breadth knowledge. It forms a hybrid context. Thirdly, we propose a Prompt optimization strategy incorporating reinforcement learning principles to automatically guide LLMs optimizing the results based on the hybrid context. To evaluate the proposed approaches, we constructed an evaluation dataset based on arXiv (2018-2023). This paper also develops a comprehensive evaluation method including empirical automatic assessment in multiple-choice question task, LLM-based scoring, human evaluation, and semantic space visualization analysis. The generated ideas are evaluated from the following five dimensions: novelty, feasibility, clarity, relevance, and significance. We conducted experiments on different LLMs including GPT-4o, DeepSeek-V3, Qwen3-8B, and Gemini 2.5. Experimental results show that GYWI significantly outperforms mainstream LLMs in multiple metrics such as novelty, reliability, and relevance.

</details>


### [17] [FIRE: A Comprehensive Benchmark for Financial Intelligence and Reasoning Evaluation](https://arxiv.org/abs/2602.22273)
*Xiyuan Zhang,Huihang Wu,Jiayu Guo,Zhenlin Zhang,Yiwei Zhang,Liangyu Huo,Xiaoxiao Ma,Jiansong Wan,Xuewei Jiao,Yi Jing,Jian Xie*

Main category: cs.AI

TL;DR: This paper introduces FIRE, a comprehensive financial benchmark evaluating LLMs' theoretical knowledge and practical skills across 3,000 scenarios, finding current LLMs have clear capability boundaries in financial applications.


<details>
  <summary>Details</summary>
Motivation: To create a comprehensive benchmark that evaluates both theoretical financial knowledge and practical business scenario handling capabilities of LLMs, addressing the need for systematic assessment of their real-world value in financial tasks.

Method: Curated financial exam questions for theoretical assessment, developed a systematic evaluation matrix covering complex financial domains, and collected 3,000 financial scenario questions (closed-form with reference answers and open-ended with rubrics) to evaluate state-of-the-art LLMs including XuanYuan 4.0.

Result: Comprehensive evaluation revealing systematic capability boundaries of current LLMs in financial applications, with XuanYuan 4.0 serving as a strong in-domain baseline; publicly released benchmark and evaluation code for future research.

Conclusion: The FIRE benchmark provides a comprehensive framework for evaluating LLMs in finance, systematically identifies their capability boundaries, and establishes a valuable resource to advance future research in financial AI applications.

Abstract: We introduce FIRE, a comprehensive benchmark designed to evaluate both the theoretical financial knowledge of LLMs and their ability to handle practical business scenarios. For theoretical assessment, we curate a diverse set of examination questions drawn from widely recognized financial qualification exams, enabling evaluation of LLMs deep understanding and application of financial knowledge. In addition, to assess the practical value of LLMs in real-world financial tasks, we propose a systematic evaluation matrix that categorizes complex financial domains and ensures coverage of essential subdomains and business activities. Based on this evaluation matrix, we collect 3,000 financial scenario questions, consisting of closed-form decision questions with reference answers and open-ended questions evaluated by predefined rubrics. We conduct comprehensive evaluations of state-of-the-art LLMs on the FIRE benchmark, including XuanYuan 4.0, our latest financial-domain model, as a strong in-domain baseline. These results enable a systematic analysis of the capability boundaries of current LLMs in financial applications. We publicly release the benchmark questions and evaluation code to facilitate future research.

</details>


### [18] [Mapping the Landscape of Artificial Intelligence in Life Cycle Assessment Using Large Language Models](https://arxiv.org/abs/2602.22500)
*Anastasija Mensikova,Donna M. Rizzo,Kathryn Hinkelman*

Main category: cs.AI

TL;DR: 本研究利用大语言模型(LLM)对AI与生命周期评估(LCA)交叉领域的研究进行全面综述，识别当前趋势、新兴主题和未来方向。研究发现AI技术在LCA中的应用快速增长，特别是LLM驱动方法显著增加，且AI方法与应用的LCA阶段存在显著相关性。该研究提出了LLM与传统文献综述相结合的高效框架，为可持续性决策提供了更严谨的环境评估工具。


<details>
  <summary>Details</summary>
Motivation: 尽管AI与LCA的整合近年来快速发展，但对该交叉领域的全面和广泛综合研究仍然有限。本文旨在填补这一空白，通过系统性综述分析AI在LCA中的应用现状，为研究人员和实践者提供清晰的研究脉络和发展方向。

Method: 研究采用大语言模型(LLM)驱动文本挖掘技术，结合传统文献综述方法，对AI-LCA领域的已发表文献进行动态分析。该方法能够同时捕捉宏观研究趋势和微观概念模式(主题)，形成可大规模、可重复的综述框架。

Result: 分析表明：1) AI技术在LCA研究中的应用急剧增长；2) 研究重心明显向LLM驱动方法转移；3) 机器学习应用持续增加；4) AI方法与特定LCA阶段之间存在统计显著性相关性；5) 所提出的LLM增强型综述框架能有效识别领域全貌。

Conclusion: LLM辅助方法在大规模、可重复的跨学科综述中展现出巨大潜力，并为计算高效的LCA实施提供了路径。本研究帮助LCA从业者整合前沿AI工具，将及时洞察融入环境评估，从而提升可持续性决策的严谨性和质量。

Abstract: Integration of artificial intelligence (AI) into life cycle assessment (LCA) has accelerated in recent years, with numerous studies successfully adapting machine learning algorithms to support various stages of LCA. Despite this rapid development, comprehensive and broad synthesis of AI-LCA research remains limited. To address this gap, this study presents a detailed review of published work at the intersection of AI and LCA, leveraging large language models (LLMs) to identify current trends, emerging themes, and future directions. Our analyses reveal that as LCA research continues to expand, the adoption of AI technologies has grown dramatically, with a noticeable shift toward LLM-driven approaches, continued increases in ML applications, and statistically significant correlations between AI approaches and corresponding LCA stages. By integrating LLM-based text-mining methods with traditional literature review techniques, this study introduces a dynamic and effective framework capable of capturing both high-level research trends and nuanced conceptual patterns (themes) across the field. Collectively, these findings demonstrate the potential of LLM-assisted methodologies to support large-scale, reproducible reviews across broad research domains, while also evaluating pathways for computationally-efficient LCA in the context of rapidly developing AI technologies. In doing so, this work helps LCA practitioners incorporate state-of-the-art tools and timely insights into environmental assessments that can enhance the rigor and quality of sustainability-driven decisions and decision-making processes.

</details>


### [19] [RLHFless: Serverless Computing for Efficient RLHF](https://arxiv.org/abs/2602.22718)
*Rui Wei,Hanfei Yu,Shubham Jain,Yogarajan Sivakumar,Devesh Tiwari,Jian Li,Seung-Jong Park,Hao Wang*

Main category: cs.AI

TL;DR: This paper proposes RLHFless, a serverless computing framework for synchronous RLHF training that addresses dynamic resource demands and resource wastage through prefix pre-computation, cost-aware actor scaling, and efficient workload assignment, achieving 1.35x speedup and 44.8% cost reduction.


<details>
  <summary>Details</summary>
Motivation: RLHF training faces efficiency challenges from expanding model sizes and dynamic resource demands. Existing serverful frameworks struggle with fine-grained resource variability, causing significant idle time and resource wastage between synchronous RL components.

Method: RLHFless is built on serverless computing with three key techniques: adapting to dynamic resource demands, pre-computing shared prefixes to avoid repeated computation, and employing a cost-aware actor scaling strategy that accounts for response length variation with efficient workload assignment.

Result: Experiments on physical testbeds and large-scale simulated clusters show RLHFless achieves up to 1.35x speedup and 44.8% cost reduction compared to state-of-the-art baselines.

Conclusion: RLHFless demonstrates serverless computing as an effective platform for scalable synchronous RLHF training, significantly improving both performance and cost-efficiency over traditional serverful approaches.

Abstract: Reinforcement Learning from Human Feedback (RLHF) has been widely applied to Large Language Model (LLM) post-training to align model outputs with human preferences. Recent models, such as DeepSeek-R1, have also shown RLHF's potential to improve LLM reasoning on complex tasks. In RL, inference and training co-exist, creating dynamic resource demands throughout the workflow. Compared to traditional RL, RLHF further challenges training efficiency due to expanding model sizes and resource consumption. Several RLHF frameworks aim to balance flexible abstraction and efficient execution. However, they rely on serverful infrastructures, which struggle with fine-grained resource variability. As a result, during synchronous RLHF training, idle time between or within RL components often causes overhead and resource wastage.
  To address these issues, we present RLHFless, the first scalable training framework for synchronous RLHF, built on serverless computing environments. RLHFless adapts to dynamic resource demands throughout the RLHF pipeline, pre-computes shared prefixes to avoid repeated computation, and uses a cost-aware actor scaling strategy that accounts for response length variation to find sweet spots with lower cost and higher speed. In addition, RLHFless assigns workloads efficiently to reduce intra-function imbalance and idle time. Experiments on both physical testbeds and a large-scale simulated cluster show that RLHFless achieves up to 1.35x speedup and 44.8% cost reduction compared to the state-of-the-art baseline.

</details>


### [20] [The AI Research Assistant: Promise, Peril, and a Proof of Concept](https://arxiv.org/abs/2602.22842)
*Tan Bui-Thanh*

Main category: cs.AI

TL;DR: This paper investigates whether AI can contribute to creative mathematical research through a case study on Hermite quadrature rules. It finds that AI excels at algebraic manipulation and proof exploration but requires rigorous human verification and domain expertise, suggesting AI can accelerate discovery when used with appropriate oversight.


<details>
  <summary>Details</summary>
Motivation: Can artificial intelligence truly contribute to creative mathematical research, or does it merely automate routine calculations while introducing risks of error?

Method: A detailed case study on discovering novel error representations and bounds for Hermite quadrature rules via systematic human-AI collaboration, working with multiple AI assistants to extend results beyond manual work and formulate/prove several theorems.

Result: AI excelled at algebraic manipulation, systematic proof exploration, literature synthesis, and LaTeX preparation, but every step required rigorous human verification, mathematical intuition for problem formulation, and strategic direction. The collaboration successfully extended mathematical results.

Conclusion: When used with appropriate skepticism and verification protocols, AI tools can meaningfully accelerate mathematical discovery while demanding careful human oversight and deep domain expertise. The study reveals patterns for successful collaboration and identifies failure modes.

Abstract: Can artificial intelligence truly contribute to creative mathematical research, or does it merely automate routine calculations while introducing risks of error? We provide empirical evidence through a detailed case study: the discovery of novel error representations and bounds for Hermite quadrature rules via systematic human-AI collaboration.
  Working with multiple AI assistants, we extended results beyond what manual work achieved, formulating and proving several theorems with AI assistance. The collaboration revealed both remarkable capabilities and critical limitations. AI excelled at algebraic manipulation, systematic proof exploration, literature synthesis, and LaTeX preparation. However, every step required rigorous human verification, mathematical intuition for problem formulation, and strategic direction.
  We document the complete research workflow with unusual transparency, revealing patterns in successful human-AI mathematical collaboration and identifying failure modes researchers must anticipate. Our experience suggests that, when used with appropriate skepticism and verification protocols, AI tools can meaningfully accelerate mathematical discovery while demanding careful human oversight and deep domain expertise.

</details>


### [21] [A Decision-Theoretic Formalisation of Steganography With Applications to LLM Monitoring](https://arxiv.org/abs/2602.23163)
*Usman Anwar,Julianna Piskorz,David D. Baek,David Africa,Jim Weatherall,Max Tegmark,Christian Schroeder de Witt,Mihaela van der Schaar,David Krueger*

Main category: cs.AI

TL;DR: 本文针对大语言模型隐写能力可能规避监督的问题，提出基于决策论的分析框架，通过广义V-信息定义隐写间隙，无需参考分布即可检测、量化和缓解LLM隐写推理。


<details>
  <summary>Details</summary>
Motivation: 大语言模型展现出隐写能力，可能被未对齐模型用于逃避监督机制，但现有基于参考分布的经典隐写检测方法不适用于LLM推理场景，亟需新的检测范式。

Method: 提出决策论视角：隐写术在可解码与不可解码智能体间创造可用信息不对称性。引入广义V-信息作为功利主义框架测量输入可用信息，并定义隐写间隙——通过比较两类智能体对隐写信号的下游效用差异来量化隐写程度。

Result: 实证验证表明，该形式化方法能有效检测、量化和缓解大语言模型中的隐写推理行为，为监督LLM提供了可行工具。

Conclusion: 决策论方法突破了经典隐写检测对已知参考分布的依赖，为监控潜在未对齐的LLM隐写行为提供了理论基础和实践框架。

Abstract: Large language models are beginning to show steganographic capabilities. Such capabilities could allow misaligned models to evade oversight mechanisms. Yet principled methods to detect and quantify such behaviours are lacking. Classical definitions of steganography, and detection methods based on them, require a known reference distribution of non-steganographic signals. For the case of steganographic reasoning in LLMs, knowing such a reference distribution is not feasible; this renders these approaches inapplicable. We propose an alternative, \textbf{decision-theoretic view of steganography}. Our central insight is that steganography creates an asymmetry in usable information between agents who can and cannot decode the hidden content (present within a steganographic signal), and this otherwise latent asymmetry can be inferred from the agents' observable actions. To formalise this perspective, we introduce generalised $\mathcal{V}$-information: a utilitarian framework for measuring the amount of usable information within some input. We use this to define the \textbf{steganographic gap} -- a measure that quantifies steganography by comparing the downstream utility of the steganographic signal to agents that can and cannot decode the hidden content. We empirically validate our formalism, and show that it can be used to detect, quantify, and mitigate steganographic reasoning in LLMs.

</details>


### [22] [LLM Novice Uplift on Dual-Use, In Silico Biology Tasks](https://arxiv.org/abs/2602.23329)
*Chen Bo Calvin Zhang,Christina Q. Knight,Nicholas Kruus,Jason Hausenloy,Pedro Medeiros,Nathaniel Li,Aiden Kim,Yury Orlovskiy,Coleman Breen,Bryce Cai,Jasper Götting,Andrew Bo Liu,Samira Nedungadi,Paula Rodriguez,Yannis Yiming He,Mohamed Shaaban,Zifan Wang,Seth Donoughe,Julian Michael*

Main category: cs.AI

TL;DR: LLMs provide massive uplift for novices on biosecurity tasks, making them 4x more accurate than internet-only users and outperforming experts in some cases, raising dual-use concerns.


<details>
  <summary>Details</summary>
Motivation: It's unclear whether LLMs genuinely enable novice users to perform better on biology tasks compared to internet-only resources, which is crucial for understanding scientific acceleration and dual-use risks.

Method: Multi-model, multi-benchmark human study comparing novices with LLM access versus internet-only access across eight biosecurity-relevant task sets, with ample time (up to 13 hours).

Result: Novices with LLMs were 4.16× more accurate than controls (95% CI [2.63, 6.87]); outperformed experts on 3/4 benchmarks; standalone LLMs often exceeded assisted novices; 89.6% reported little difficulty obtaining dual-use information.

Conclusion: LLMs dramatically uplift novices on specialized biological tasks, underscoring the need for sustained, interactive uplift evaluations alongside traditional benchmarks to assess dual-use risks.

Abstract: Large language models (LLMs) perform increasingly well on biology benchmarks, but it remains unclear whether they uplift novice users -- i.e., enable humans to perform better than with internet-only resources. This uncertainty is central to understanding both scientific acceleration and dual-use risk. We conducted a multi-model, multi-benchmark human uplift study comparing novices with LLM access versus internet-only access across eight biosecurity-relevant task sets. Participants worked on complex problems with ample time (up to 13 hours for the most involved tasks). We found that LLM access provided substantial uplift: novices with LLMs were 4.16 times more accurate than controls (95% CI [2.63, 6.87]). On four benchmarks with available expert baselines (internet-only), novices with LLMs outperformed experts on three of them. Perhaps surprisingly, standalone LLMs often exceeded LLM-assisted novices, indicating that users were not eliciting the strongest available contributions from the LLMs. Most participants (89.6%) reported little difficulty obtaining dual-use-relevant information despite safeguards. Overall, LLMs substantially uplift novices on biological tasks previously reserved for trained practitioners, underscoring the need for sustained, interactive uplift evaluations alongside traditional benchmarks.

</details>


### [23] [Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks](https://arxiv.org/abs/2602.23330)
*Kunihiro Miyazaki,Takanobu Kawahara,Stephen Roberts,Stefan Zohren*

Main category: cs.AI

TL;DR: 提出细粒度任务分解的多Agent LLM交易框架，在控制数据泄露的回测中显著提升日本股票交易的风险调整收益，发现分析输出与决策偏好的对齐是性能关键驱动因素


<details>
  <summary>Details</summary>
Motivation: 现有多Agent交易系统依赖抽象指令，忽视真实工作流细节，导致推理性能下降和决策不透明

Method: 将投资分析显式分解为细粒度任务（而非粗粒度指令），使用日本股票数据（价格/财报/新闻/宏观信息）在防泄露回测环境中验证

Result: 细粒度分解显著提升风险调整收益；Agent输出与决策偏好的对齐度是系统性能关键驱动因素；结合低相关性投资组合优化实现更优表现

Conclusion: 为LLM Agent在交易系统中的结构设计和任务配置提供实践指导，强调任务粒度细化与决策偏好对齐的重要性

Abstract: The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis into fine-grained tasks, rather than providing coarse-grained instructions. We evaluate the proposed framework using Japanese stock data, including prices, financial statements, news, and macro information, under a leakage-controlled backtesting setting. Experimental results show that fine-grained task decomposition significantly improves risk-adjusted returns compared to conventional coarse-grained designs. Crucially, further analysis of intermediate agent outputs suggests that alignment between analytical outputs and downstream decision preferences is a critical driver of system performance. Moreover, we conduct standard portfolio optimization, exploiting low correlation with the stock index and the variance of each system's output. This approach achieves superior performance. These findings contribute to the design of agent structure and task configuration when applying LLM agents to trading systems in practical settings.

</details>


### [24] [Multi-Level Causal Embeddings](https://arxiv.org/abs/2602.22287)
*Willem Schooltink,Fabio Massimo Zennaro*

Main category: cs.AI

TL;DR: 提出因果嵌入框架，将多个精细因果模型映射到粗粒度模型的子系统，解决多分辨率边际问题，实现异构数据集合并


<details>
  <summary>Details</summary>
Motivation: 现有因果模型抽象方法仅关注两个模型间关系，需扩展框架以支持多个模型嵌入到统一粗粒度系统，并处理统计与因果层面的边际问题

Method: 定义因果嵌入作为抽象的泛化形式，提出一致性新概念，构建多分辨率边际问题框架

Result: 证明该框架同时适用于统计边际问题与因果边际问题，支持不同表示模型的数据集融合

Conclusion: 因果嵌入为多模型集成提供理论基础，在异构数据合并场景中具有实用价值

Abstract: Abstractions of causal models allow for the coarsening of models such that relations of cause and effect are preserved. Whereas abstractions focus on the relation between two models, in this paper we study a framework for causal embeddings which enable multiple detailed models to be mapped into sub-systems of a coarser causal model. We define causal embeddings as a generalization of abstraction, and present a generalized notion of consistency. By defining a multi-resolution marginal problem, we showcase the relevance of causal embeddings for both the statistical marginal problem and the causal marginal problem; furthermore, we illustrate its practical use in merging datasets coming from models with different representations.

</details>


### [25] [How Do Latent Reasoning Methods Perform Under Weak and Strong Supervision?](https://arxiv.org/abs/2602.22441)
*Yingqian Cui,Zhenwei Dai,Bing He,Zhan Shi,Hui Liu,Rui Sun,Zhiji Liu,Yue Xing,Jiliang Tang,Benoit Dumoulin*

Main category: cs.AI

TL;DR: 该研究揭示了潜在推理存在两个核心问题：一是模型普遍存在捷径行为（不真正使用潜在推理即可获得高精度），二是潜在推理并非如假设般实现类BFS结构化搜索，而是表现出隐性剪枝与压缩。关键发现是监督强度存在权衡：强监督减少捷径但限制假设多样性，弱监督则相反。


<details>
  <summary>Details</summary>
Motivation: 尽管潜在推理作为在连续潜在空间中进行多步推理的新范式表现出色，但其内部机制尚未被充分探究。本研究旨在通过全面分析潜在推理方法，深入理解潜在表征在推理过程中的真实作用与行为。

Method: 作者对具有不同监督程度的潜在推理方法进行了系统性分析，重点检验了潜在推理是否支持类似广度优先搜索（BFS）的假设。通过控制监督强度变量，探究潜在表征的多样性与推理真实性的关系。

Result: 1) 发现普遍存在的捷径行为：模型无需依赖真实潜在推理即可获得高准确率；2) 潜在表征虽能编码多种可能性，但推理过程并未忠实实现结构化搜索，而是表现出隐性剪枝与压缩；3) 揭示监督强度权衡：强监督减轻捷径但限制潜在表征的多样性，弱监督则允许更丰富的表征但以增加捷径为代价。

Conclusion: 潜在推理的内部机制比预期更复杂，捷径行为是其根本挑战。监督强度在推理质量与表征多样性之间存在根本性权衡，当前方法可能并未实现真正意义上的多步计算，这为未来改进潜在推理架构和监督策略提供了重要方向。

Abstract: Latent reasoning has been recently proposed as a reasoning paradigm and performs multi-step reasoning through generating steps in the latent space instead of the textual space. This paradigm enables reasoning beyond discrete language tokens by performing multi-step computation in continuous latent spaces. Although there have been numerous studies focusing on improving the performance of latent reasoning, its internal mechanisms remain not fully investigated. In this work, we conduct a comprehensive analysis of latent reasoning methods to better understand the role and behavior of latent representation in the process. We identify two key issues across latent reasoning methods with different levels of supervision. First, we observe pervasive shortcut behavior, where they achieve high accuracy without relying on latent reasoning. Second, we examine the hypothesis that latent reasoning supports BFS-like exploration in latent space, and find that while latent representations can encode multiple possibilities, the reasoning process does not faithfully implement structured search, but instead exhibits implicit pruning and compression. Finally, our findings reveal a trade-off associated with supervision strength: stronger supervision mitigates shortcut behavior but restricts the ability of latent representations to maintain diverse hypotheses, whereas weaker supervision allows richer latent representations at the cost of increased shortcut behavior.

</details>


### [26] [VeRO: An Evaluation Harness for Agents to Optimize Agents](https://arxiv.org/abs/2602.22480)
*Varun Ursekar,Apaar Shanker,Veronica Chatrath,Yuan,Xue,Sam Denton*

Main category: cs.AI

TL;DR: 本文介绍了VERO框架，用于评估和优化编码智能体。通过版本控制、奖励机制和结构化观察，该框架解决了智能体优化中可复现性和评估标准化的挑战，并提供了基准测试套件来比较不同优化配置的效果。


<details>
  <summary>Details</summary>
Motivation: 编码智能体在通过编辑-执行-评估循环进行迭代优化方面存在系统性理解缺失。与常规软件工程不同，智能体优化需要同时处理确定性代码和随机LLM生成，并捕获中间推理与执行结果，这需要专门的评估框架。

Method: 提出VERO（版本控制、奖励与观察）框架，包含可复现的评估工具和基准测试套件。评估工具提供版本化智能体快照、预算控制和结构化执行追踪；基准套件包含目标智能体和任务及参考评估流程。

Result: 利用VERO进行了实证研究，比较了不同优化配置在各任务上的表现，并分析了哪些修改能可靠提升目标智能体性能。

Conclusion: VERO为编码智能体的核心优化能力研究提供了基础设施，支持社区在该领域的进一步发展。

Abstract: An important emerging application of coding agents is agent optimization: the iterative improvement of a target agent through edit-execute-evaluate cycles. Despite its relevance, the community lacks a systematic understanding of coding agent performance on this task. Agent optimization differs fundamentally from conventional software engineering: the target agent interleaves deterministic code with stochastic LLM completions, requiring structured capture of both intermediate reasoning and downstream execution outcomes. To address these challenges, we introduce VERO (Versioning, Rewards, and Observations), which provides (1) a reproducible evaluation harness with versioned agent snapshots, budget-controlled evaluation, and structured execution traces, and (2) a benchmark suite of target agents and tasks with reference evaluation procedures. Using VERO, we conduct an empirical study comparing optimizer configurations across tasks and analyzing which modifications reliably improve target agent performance. We release VERO to support research on agent optimization as a core capability for coding agents.

</details>


### [27] [OmniGAIA: Towards Native Omni-Modal AI Agents](https://arxiv.org/abs/2602.22897)
*Xiaoxi Li,Wenxiang Jiao,Jiarui Jin,Shijian Wang,Guanting Dong,Jiajie Jin,Hao Wang,Yinuo Wang,Ji-Rong Wen,Yuan Lu,Zhicheng Dou*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world. However, current multi-modal LLMs are primarily confined to bi-modal interactions (e.g., vision-language), lacking the unified cognitive capabilities required for general AI assistants. To bridge this gap, we introduce OmniGAIA, a comprehensive benchmark designed to evaluate omni-modal agents on tasks necessitating deep reasoning and multi-turn tool execution across video, audio, and image modalities. Constructed via a novel omni-modal event graph approach, OmniGAIA synthesizes complex, multi-hop queries derived from real-world data that require cross-modal reasoning and external tool integration. Furthermore, we propose OmniAtlas, a native omni-modal foundation agent under tool-integrated reasoning paradigm with active omni-modal perception. Trained on trajectories synthesized via a hindsight-guided tree exploration strategy and OmniDPO for fine-grained error correction, OmniAtlas effectively enhances the tool-use capabilities of existing open-source models. This work marks a step towards next-generation native omni-modal AI assistants for real-world scenarios.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [28] [To Deceive is to Teach? Forging Perceptual Robustness via Adversarial Reinforcement Learning](https://arxiv.org/abs/2602.22227)
*Yicheng Bao,Xuhong Wang,Xin Tan*

Main category: cs.LG

TL;DR: This paper proposes AOT, a self-play framework with an image-editing Attacker and Defender MLLM to forge robustness through co-evolution, using AOT-SFT adversarial dataset.


<details>
  <summary>Details</summary>
Motivation: Multimodal Large Language Models (MLLMs) show perceptual fragility in complex visual scenes, limited by finite and expensive-to-scale training datasets that cap model robustness.

Method: Introduces AOT (Adversarial Opponent Training), a self-play co-evolution framework where an image-editing Attacker generates dynamic adversarial image manipulations that force the Defender MLLM to adapt and improve.

Result: AOT significantly enhances the Defender's perceptual robustness and reduces hallucinations, demonstrating scalable MLLM training.

Conclusion: The work establishes a scalable paradigm for training more reliable MLLMs by bootstrapping robustness through self-generated adversarial training data.

Abstract: Despite their impressive capabilities, Multimodal Large Language Models (MLLMs) exhibit perceptual fragility when confronted with visually complex scenes. This weakness stems from a reliance on finite training datasets, which are prohibitively expensive to scale and impose a ceiling on model robustness. We introduce \textbf{AOT-SFT}, a large-scale adversarial dataset for bootstrapping MLLM robustness. Building on this, we propose \textbf{AOT (Adversarial Opponent Training)}, a self-play framework that forges MLLM robustness by creating its own training data. Our method orchestrates a co-evolution between an image-editing Attacker and a Defender MLLM, where the Attacker generates a diverse and dynamic curriculum of image manipulations, forcing the Defender to adapt and improve. Extensive experiments demonstrate that AOT enhances the Defender's perceptual robustness and reduces hallucinations, establishing a scalable paradigm for training more reliable MLLMs.

</details>


### [29] [Patient-Centered, Graph-Augmented Artificial Intelligence-Enabled Passive Surveillance for Early Stroke Risk Detection in High-Risk Individuals](https://arxiv.org/abs/2602.22228)
*Jiyeong Kim,Stephen P. Ma,Nirali Vora,Nicholas W. Larsen,Julia Adler-Milstein,Jonathan H. Chen,Selen Bozkurt,Abeed Sarker,Juhee Cho,Jindeok Joo,Natali Pageler,Fatima Rodriguez,Christopher Sharp,Eleni Linos*

Main category: cs.LG

TL;DR: 开发了基于患者报告语言的被动监测系统，通过双机器学习管道识别糖尿病患者的脑卒中前驱症状模式，在保守阈值下实现高特异性(1.00)和阳性预测值(1.00)，为高风险人群创造临床干预时间窗


<details>
  <summary>Details</summary>
Motivation: 脑卒中每年影响数百万人，但症状识别不足导致就医延迟；针对糖尿病患者这一高风险群体，需解决早期风险识别缺口以缩短救治时间窗

Method: 构建基于患者原生语言的症壮 taxonomy，采用异构GNN与EN/LASSO双机器学习管道识别症状模式；开发融合症状相关性和时间临近性的混合筛查系统，通过电子病历模拟评估3-90天窗口期

Result: 在保守阈值下（最小化假警报），系统达到高特异性(1.00)和患病率校正阳性预测值(1.00)，灵敏度0.72；90天窗口期精度最高，仅依赖患者报告语言实现低负担高精度检测

Conclusion: 患者报告语言支持的筛查系统可为高风险个体提供有价值的临床评估与干预时间窗，在优先保证精度的前提下平衡敏感度，适用于早期脑卒中风险预警

Abstract: Stroke affected millions annually, yet poor symptom recognition often delayed care-seeking. To address risk recognition gap, we developed a passive surveillance system for early stroke risk detection using patient-reported symptoms among individuals with diabetes. Constructing a symptom taxonomy grounded in patients own language and a dual machine learning pipeline (heterogeneous GNN and EN/LASSO), we identified symptom patterns associated with subsequent stroke. We translated findings into a hybrid risk screening system integrating symptom relevance and temporal proximity, evaluated across 3-90 day windows through EHR-based simulations. Under conservative thresholds, intentionally designed to minimize false alerts, the screening system achieved high specificity (1.00) and prevalence-adjusted positive predictive value (1.00), with good sensitivity (0.72), an expected trade-off prioritizing precision, that was highest in 90-day window. Patient-reported language alone supported high-precision, low-burden early stroke risk detection, that could offer a valuable time window for clinical evaluation and intervention for high-risk individuals.

</details>


### [30] [A 1/R Law for Kurtosis Contrast in Balanced Mixtures](https://arxiv.org/abs/2602.22334)
*Yuda Bi,Wenjun Xiao,Linhao Bai,Vince D Calhoun*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Kurtosis-based Independent Component Analysis (ICA) weakens in wide, balanced mixtures. We prove a sharp redundancy law: for a standardized projection with effective width $R_{\mathrm{eff}}$ (participation ratio), the population excess kurtosis obeys $|κ(y)|=O(κ_{\max}/R_{\mathrm{eff}})$, yielding the order-tight $O(c_bκ_{\max}/R)$ under balance (typically $c_b=O(\log R)$). As an impossibility screen, under standard finite-moment conditions for sample kurtosis estimation, surpassing the $O(1/\sqrt{T})$ estimation scale requires $R\lesssim κ_{\max}\sqrt{T}$. We also show that \emph{purification} -- selecting $m\!\ll\!R$ sign-consistent sources -- restores $R$-independent contrast $Ω(1/m)$, with a simple data-driven heuristic. Synthetic experiments validate the predicted decay, the $\sqrt{T}$ crossover, and contrast recovery.

</details>


### [31] [Learning geometry-dependent lead-field operators for forward ECG modeling](https://arxiv.org/abs/2602.22367)
*Arsenii Dokuchaev,Francesca Bonizzoni,Stefano Pagani,Francesco Regazzoni,Simone Pezzuto*

Main category: cs.LG

TL;DR: A shape-informed neural surrogate model for forward ECG simulation that achieves high accuracy with low computational cost and minimal data requirements.


<details>
  <summary>Details</summary>
Motivation: Current lead-field methods for forward ECG simulation face challenges: (1) obtaining complete torso anatomical data is difficult in clinical practice as imaging focuses on the heart, and (2) computational cost scales linearly with electrode number, limiting high-density applications. No existing method simultaneously provides high anatomical fidelity, low data requirements, and computational efficiency.

Method: The authors propose a two-component framework: (1) a geometry-encoding module that maps anatomical shapes to a low-dimensional latent space, and (2) a geometry-conditioned neural surrogate that predicts lead-field gradients from spatial coordinates, electrode positions, and latent codes. This serves as a drop-in replacement for the full-order lead-field model.

Result: The method achieves high accuracy with mean angular error of 5° for lead field approximation and relative mean squared error <2.5% for ECG simulations. It outperforms pseudo lead-field approximation while maintaining negligible inference cost.

Conclusion: The proposed shape-informed surrogate model enables high-fidelity ECG simulations in data-limited clinical settings without requiring detailed torso segmentation, addressing key limitations of traditional lead-field methods.

Abstract: Modern forward electrocardiogram (ECG) computational models rely on an accurate representation of the torso domain. The lead-field method enables fast ECG simulations while preserving full geometric fidelity. Achieving high anatomical accuracy in torso representation is, however, challenging in clinical practice, as imaging protocols are typically focused on the heart and often do not include the entire torso. In addition, the computational cost of the lead-field method scales linearly with the number of electrodes, limiting its applicability in high-density recording settings. To date, no existing approach simultaneously achieves high anatomical fidelity, low data requirements and computational efficiency. In this work, we propose a shape-informed surrogate model of the lead-field operator that serves as a drop-in replacement for the full-order model in forward ECG simulations. The proposed framework consists of two components: a geometry-encoding module that maps anatomical shapes into a low-dimensional latent space, and a geometry-conditioned neural surrogate that predicts lead-field gradients from spatial coordinates, electrode positions and latent codes. The proposed method achieves high accuracy in approximating lead fields both within the torso (mean angular error 5°) and inside the heart, resulting in highly accurate ECG simulations (relative mean squared error <2.5%. The surrogate consistently outperforms the widely used pseudo lead-field approximation while preserving negligible inference cost. Owing to its compact latent representation, the method does not require a fully detailed torso segmentation and can therefore be deployed in data-limited settings while preserving high-fidelity ECG simulations.

</details>


### [32] [A Learning-Based Hybrid Decision Framework for Matching Systems with User Departure Detection](https://arxiv.org/abs/2602.22412)
*Ruiqi Zhou,Donghao Zhu,Houcai Shen*

Main category: cs.LG

TL;DR: 一种基于学习的混合框架，用于匹配市场，通过估计离开分布和使用决策阈值来自适应地结合即时匹配和延迟匹配，以平衡效率、等待时间和拥堵。


<details>
  <summary>Details</summary>
Motivation: 在动态匹配市场中，固定匹配策略缺乏灵活性，而延迟匹配的收益对参与者停留时间和离开行为高度敏感，导致效率、等待时间和拥堵之间的权衡。

Method: 混合框架持续收集用户离开数据，通过回归估计潜在离开分布，并基于决策阈值决定在后续期间是否延迟匹配，该阈值控制系统对匹配效率损失的容忍度。

Result: 该框架在仅牺牲有限匹配效率的同时，显著减少了等待时间和拥堵，使系统性能能够在纯贪心和纯耐心策略之间灵活插值。

Conclusion: 混合框架通过基于实时数据动态调整匹配策略，为静态匹配机制提供了稳健且自适应的替代方案。

Abstract: In matching markets such as kidney exchanges and freight exchanges, delayed matching has been shown to improve overall market efficiency. The benefits of delay are highly sensitive to participants' sojourn times and departure behavior, and delaying matches can impose significant costs, including longer waiting times and increased market congestion. These competing effects make fixed matching policies inherently inflexible in dynamic environments. We propose a learning-based Hybrid framework that adaptively combines immediate and delayed matching. The framework continuously collects data on user departures over time, estimates the underlying departure distribution via regression, and determines whether to delay matching in the subsequent period based on a decision threshold that governs the system's tolerance for matching efficiency loss. The proposed framework can substantially reduce waiting times and congestion while sacrificing only a limited amount of matching efficiency. By dynamically adjusting its matching strategy, the Hybrid framework enables system performance to flexibly interpolate between purely greedy and purely patient policies, offering a robust and adaptive alternative to static matching mechanisms.

</details>


### [33] [Forecasting Antimicrobial Resistance Trends Using Machine Learning on WHO GLASS Surveillance Data: A Retrieval-Augmented Generation Approach for Policy Decision Support](https://arxiv.org/abs/2602.22673)
*Md Tanvir Hasan Turja*

Main category: cs.LG

TL;DR: 本文开发了一个双组件框架，用于基于WHO GLASS数据预测抗菌药物耐药性(AMR)趋势。XGBoost表现最佳（测试MAE为7.07%，R²为0.854），上一年的耐药率是最重要的预测因子。结合ChromaDB和Phi-3 Mini的RAG管道可生成基于证据的政策建议。


<details>
  <summary>Details</summary>
Motivation: 抗菌药物耐药性(AMR)是一个日益严重的全球性危机，预计到2050年每年将导致1000万人死亡。虽然WHO GLASS在44个国家提供了标准化监测数据，但很少有研究应用机器学习从该数据预测人群水平的耐药趋势。

Method: 作者提出了一个双组件框架：(1) AMR趋势预测，在来自六个WHO区域2021-2023年的5,909条WHO GLASS观测数据上，对六种模型（Naive、线性回归、岭回归、XGBoost、LightGBM和LSTM）进行基准测试；(2) 检索增强生成(RAG)管道，结合WHO政策文档的ChromaDB向量存储和本地部署的Phi-3 Mini语言模型来生成政策建议。

Result: XGBoost表现最佳，测试MAE为7.07%，R²为0.854，比朴素基线高出83.1%。上一年的耐药率是主要预测因子（50.5%的重要性）。区域MAE从欧洲区域的4.16%到东南亚区域的10.14%不等。RAG管道成功生成了有来源归属、幻觉受限的政策答案。

Conclusion: 该框架提供了有效的AMR趋势预测和基于证据的政策决策支持。代码和数据可在https://github.com/TanvirTurja公开获取。

Abstract: Antimicrobial resistance (AMR) is a growing global crisis projected to cause 10 million deaths per year by 2050. While the WHO Global Antimicrobial Resistance and Use Surveillance System (GLASS) provides standardized surveillance data across 44 countries, few studies have applied machine learning to forecast population-level resistance trends from this data. This paper presents a two-component framework for AMR trend forecasting and evidence-grounded policy decision support. We benchmark six models -- Naive, Linear Regression, Ridge Regression, XGBoost, LightGBM, and LSTM -- on 5,909 WHO GLASS observations across six WHO regions (2021-2023). XGBoost achieved the best performance with a test MAE of 7.07% and R-squared of 0.854, outperforming the naive baseline by 83.1%. Feature importance analysis identified the prior-year resistance rate as the dominant predictor (50.5% importance), while regional MAE ranged from 4.16% (European Region) to 10.14% (South-East Asia Region). We additionally implemented a Retrieval-Augmented Generation (RAG) pipeline combining a ChromaDB vector store of WHO policy documents with a locally deployed Phi-3 Mini language model, producing source-attributed, hallucination-constrained policy answers. Code and data are available at https://github.com/TanvirTurja

</details>


### [34] [Duel-Evolve: Reward-Free Test-Time Scaling via LLM Self-Preferences](https://arxiv.org/abs/2602.21585)
*Sweta Karlekar,Carolina Zheng,Magnus Saebo,Nicolas Beltran-Velez,Shuyang Yu,John Bowlan,Michal Kucer,David Blei*

Main category: cs.LG

TL;DR: 这是一个关于Duel-Evolve的论文摘要，该方法通过LLM自身的成对偏好而非外部奖励来优化LLM输出，使用贝叶斯Bradley-Terry模型和双重汤普森采样，在数学和代码基准测试中显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有测试时优化LLM输出的方法依赖校准的标量评估器，但这些评估器往往不可用、过于稀疏或不可靠。相比之下，成对比较更容易获取，能提供改进方向的信号，且无需外部监督即可从LLM自身获得。

Method: Duel-Evolve是一种进化优化算法，用LLM自身产生的成对偏好替代外部标量奖励；通过贝叶斯Bradley-Terry模型聚合这些带噪声的比较，得到具有不确定性感知的候选质量估计；使用双重汤普森采样指导比较预算分配，并选择高质量父代生成改进的候选。

Result: 在MathBench上比现有方法高出20个百分点准确率；在LiveCodeBench上比可比迭代方法高出超过12个百分点；无需奖励模型、搜索期间无需真实标签、无需手工设计评分函数。

Conclusion: 成对自偏好为大型离散输出空间上的测试时改进提供了强大的优化信号。

Abstract: Many applications seek to optimize LLM outputs at test time by iteratively proposing, scoring, and refining candidates over a discrete output space. Existing methods use a calibrated scalar evaluator for the target objective to guide search, but for many tasks such scores are unavailable, too sparse, or unreliable. Pairwise comparisons, by contrast, are often easier to elicit, still provide useful signal on improvement directions, and can be obtained from the LLM itself without external supervision. Building on this observation, we introduce Duel-Evolve, an evolutionary optimization algorithm that replaces external scalar rewards with pairwise preferences elicited from the same LLM used to generate candidates. Duel-Evolve aggregates these noisy candidate comparisons via a Bayesian Bradley-Terry model, yielding uncertainty-aware estimates of candidate quality. These quality estimates guide allocation of the comparison budget toward plausible optima using Double Thompson Sampling, as well as selection of high-quality parents to generate improved candidates. We evaluate Duel-Evolve on MathBench, where it achieves 20 percentage points higher accuracy over existing methods and baselines, and on LiveCodeBench, where it improves over comparable iterative methods by over 12 percentage points. Notably, the method requires no reward model, no ground-truth labels during search, and no hand-crafted scoring function. Results show that pairwise self-preferences provide strong optimization signal for test-time improvement over large, discrete output spaces.

</details>


### [35] [Accelerating LLM Pre-Training through Flat-Direction Dynamics Enhancement](https://arxiv.org/abs/2602.22681)
*Shuchen Zhu,Rizhen Hu,Mingze Wang,Mou Sun,Xue Wang,Kun Yuan,Zaiwen Wen*

Main category: cs.LG

TL;DR: 该论文提出LITE优化策略，通过Riemannian ODE框架分析自适应优化器，在各向异性的LLM训练 landscape上沿平坦方向应用更大的阻尼和学习率，显著加速Muon和SOAP等优化器。


<details>
  <summary>Details</summary>
Motivation: LLM预训练需巨大算力，优化器效率至关重要。现有矩阵优化器（如Muon、SOAP）虽利用曲率信息，但更新趋于各向同性——在平坦方向过于保守，在尖锐方向过于激进，存在改进空间。

Method: 建立统一Riemannian ODE框架：预处理子构建Riemannian几何改善病态条件，动量作为阻尼项促进收敛。基于此提出LITE策略，在平坦轨迹上应用更大的Hessian阻尼系数与学习率。

Result: LITE显著加速Muon和SOAP，在Dense/MoE架构、130M-1.3B参数规模、C4/Pile数据集及多种学习率调度下均有效。

Conclusion: 理论分析证实LITE在平坦方向实现更快收敛，为高效LLM预训练提供了原则性方法。

Abstract: Pre-training Large Language Models requires immense computational resources, making optimizer efficiency essential. The optimization landscape is highly anisotropic, with loss reduction driven predominantly by progress along flat directions. While matrix-based optimizers such as Muon and SOAP leverage fine-grained curvature information to outperform AdamW, their updates tend toward isotropy -- relatively conservative along flat directions yet potentially aggressive along sharp ones. To address this limitation, we first establish a unified Riemannian Ordinary Differential Equation (ODE) framework that elucidates how common adaptive algorithms operate synergistically: the preconditioner induces a Riemannian geometry that mitigates ill-conditioning, while momentum serves as a Riemannian damping term that promotes convergence. Guided by these insights, we propose LITE, a generalized acceleration strategy that enhances training dynamics by applying larger Hessian damping coefficients and learning rates along flat trajectories. Extensive experiments demonstrate that LITE significantly accelerates both Muon and SOAP across diverse architectures (Dense, MoE), parameter scales (130M--1.3B), datasets (C4, Pile), and learning-rate schedules (cosine, warmup-stable-decay). Theoretical analysis confirms that LITE facilitates faster convergence along flat directions in anisotropic landscapes, providing a principled approach to efficient LLM pre-training. The code is available at https://github.com/SHUCHENZHU/LITE.

</details>


### [36] [Zatom-1: A Multimodal Flow Foundation Model for 3D Molecules and Materials](https://arxiv.org/abs/2602.22251)
*Alex Morehead,Miruna Cretu,Antonia Panescu,Rishabh Anand,Maurice Weiler,Tynan Perez,Samuel Blau,Steven Farrell,Wahid Bhimji,Anubhav Jain,Hrushikesh Sahasrabuddhe,Pietro Lio,Tommi Jaakkola,Rafael Gomez-Bombarelli,Rex Ying,N. Benjamin Erichson,Michael W. Mahoney*

Main category: cs.LG

TL;DR: Zatom-1是首个统一3D分子和材料生成与预测的基础模型，采用多模态流匹配Transformer架构，在生成和预测任务上匹配或超越专用基线，并将生成推理时间缩短10倍以上，同时实现了跨化学域的正向知识迁移。


<details>
  <summary>Details</summary>
Motivation: 通用3D化学建模需要生成和预测双重能力，但现有AI方法局限于单一领域(分子或材料)和单一任务(生成或预测)，限制了表征共享和迁移学习效果。

Method: Zatom-1采用多模态流匹配目标的Transformer，联合建模离散原子类型和连续3D几何结构，支持可扩展预训练，并作为下游多任务预测(性质、能量、力)的通用初始化。

Result: 在生成和预测基准测试中匹配或超越专用基线，生成推理时间减少一个数量级以上；预训练中建模材料能提升分子性质预测精度，证实跨化学域正向迁移。

Conclusion: 联合生成式预训练成功实现了分子和材料领域的统一建模与知识迁移，为开发通用化学AI模型提供了新范式。

Abstract: General-purpose 3D chemical modeling encompasses molecules and materials, requiring both generative and predictive capabilities. However, most existing AI approaches are optimized for a single domain (molecules or materials) and a single task (generation or prediction), which limits representation sharing and transfer. We introduce Zatom-1, the first foundation model that unifies generative and predictive learning of 3D molecules and materials. Zatom-1 is a Transformer trained with a multimodal flow matching objective that jointly models discrete atom types and continuous 3D geometries. This approach supports scalable pretraining with predictable gains as model capacity increases, while enabling fast and stable sampling. We use joint generative pretraining as a universal initialization for downstream multi-task prediction of properties, energies, and forces. Empirically, Zatom-1 matches or outperforms specialized baselines on both generative and predictive benchmarks, while reducing the generative inference time by more than an order of magnitude. Our experiments demonstrate positive predictive transfer between chemical domains from joint generative pretraining: modeling materials during pretraining improves molecular property prediction accuracy.

</details>


### [37] [Decentralized Ranking Aggregation: Gossip Algorithms for Borda and Copeland Consensus](https://arxiv.org/abs/2602.22847)
*Anna Van Elst,Kerrian Le Caillec,Igor Colin,Stephan Clémençon*

Main category: cs.LG

TL;DR: This paper develops decentralized ranking aggregation algorithms using gossip communication, providing theoretical convergence guarantees for Borda/Copeland methods and demonstrating fast, reliable consensus across distributed networks.


<details>
  <summary>Details</summary>
Motivation: Traditional ranking aggregation algorithms work well in centralized settings but face challenges in decentralized environments like P2P networks, IoT, and multi-agent systems. Existing decentralized literature focuses mainly on computing arithmetic means, leaving ranking consensus underexplored despite its importance in preference analysis.

Method: Proposes a random gossip communication framework where autonomous agents achieve global ranking consensus through local interactions using classical rules (Borda, Copeland, median rank, local Kemenization) without central coordination.

Result: Provides rigorous convergence guarantees with explicit rate bounds for Borda and Copeland methods, decentralized implementations of median rank rule and local Kemenization, and empirical validation showing fast, reliable convergence across various network topologies and real/synthetic datasets.

Conclusion: The algorithms successfully enable decentralized ranking aggregation with strong theoretical guarantees and practical performance, addressing robustness to corrupted nodes and scalability through reduced communication costs.

Abstract: The concept of ranking aggregation plays a central role in preference analysis, and numerous algorithms for calculating median rankings, often originating in social choice theory, have been documented in the literature, offering theoretical guarantees in a centralized setting, i.e., when all the ranking data to be aggregated can be brought together in a single computing unit. For many technologies (e.g. peer-to-peer networks, IoT, multi-agent systems), extending the ability to calculate consensus rankings with guarantees in a decentralized setting, i.e., when preference data is initially distributed across a communicating network, remains a major methodological challenge. Indeed, in recent years, the literature on decentralized computation has mainly focused on computing or optimizing statistics such as arithmetic means using gossip algorithms. The purpose of this article is precisely to study how to achieve reliable consensus on collective rankings using classical rules (e.g. Borda, Copeland) in a decentralized setting, thereby raising new questions, robustness to corrupted nodes, and scalability through reduced communication costs in particular. The approach proposed and analyzed here relies on random gossip communication, allowing autonomous agents to compute global ranking consensus using only local interactions, without coordination or central authority.
  We provide rigorous convergence guarantees, including explicit rate bounds, for the Borda and Copeland consensus methods. Beyond these rules, we also provide a decentralized implementation of consensus according to the median rank rule and local Kemenization. Extensive empirical evaluations on various network topologies and real and synthetic ranking datasets demonstrate that our algorithms converge quickly and reliably to the correct ranking aggregation.

</details>


### [38] [Calibrated Test-Time Guidance for Bayesian Inference](https://arxiv.org/abs/2602.22428)
*Daniel Geyfman,Felix Draxler,Jan Groeneveld,Hyunsoo Lee,Theofanis Karaletsos,Stephan Mandt*

Main category: cs.LG

TL;DR: 该论文指出当前测试时引导方法无法正确采样贝叶斯后验分布，提出了一致的替代估计器以实现校准采样，在贝叶斯推理任务中显著超越先前方法，并在黑洞图像重建中达到顶尖水平。


<details>
  <summary>Details</summary>
Motivation: 现有测试时引导方法专注于最大化奖励函数而非从真实贝叶斯后验采样，导致推理结果不校准。

Method: 识别了现有方法中导致后验分布错误的的结构性近似问题，并提出了具有一致性的替代估计器来启用贝叶斯后验的校准采样。

Result: 在贝叶斯推理任务集上显著优于先前方法，并在黑洞图像重建任务中达到最先进水平。

Conclusion: 所提出的方法能够正确地从贝叶斯后验分布中进行校准采样，解决了现有测试时引导方法的核心缺陷。

Abstract: Test-time guidance is a widely used mechanism for steering pretrained diffusion models toward outcomes specified by a reward function. Existing approaches, however, focus on maximizing reward rather than sampling from the true Bayesian posterior, leading to miscalibrated inference. In this work, we show that common test-time guidance methods do not recover the correct posterior distribution and identify the structural approximations responsible for this failure. We then propose consistent alternative estimators that enable calibrated sampling from the Bayesian posterior. We significantly outperform previous methods on a set of Bayesian inference tasks, and match state-of-the-art in black hole image reconstruction.

</details>


### [39] [From Bias to Balance: Fairness-Aware Paper Recommendation for Equitable Peer Review](https://arxiv.org/abs/2602.22438)
*Uttamasha Anjally Oyshi,Susan Gauch*

Main category: cs.LG

TL;DR: Fair-PaperRec uses differentiable fairness regularization in an MLP model to re-rank academic papers post double-blind review, increasing underrepresented group participation by up to 42% with minimal impact on overall quality.


<details>
  <summary>Details</summary>
Motivation: Systemic biases against underrepresented groups persist in double-blind academic paper reviews; the authors hypothesize that explicit fairness regularization in post-review recommenders can improve inclusion without sacrificing quality.

Method: Proposes Fair-PaperRec, a Multi-Layer Perceptron (MLP) with a differentiable fairness loss function over intersectional attributes (e.g., race, country) to re-rank papers. Tests on synthetic datasets with controlled bias levels and real-world conference data (SIGCHI, DIS, IUI).

Result: Synthetic tests show fairness regularization boosts macro/micro diversity while maintaining utility across varying bias regimes. Real-world deployment achieves up to 42.03% increase in underrepresented-group participation with ≤3.16% change in overall utility compared to historical selection.

Conclusion: Fairness regularization acts as both an equity mechanism and mild quality regularizer, especially in highly biased contexts. Fair-PaperRec provides a practical framework for equitable paper selection that preserves or enhances scholarly quality.

Abstract: Despite frequent double-blind review, systemic biases related to author demographics still disadvantage underrepresented groups. We start from a simple hypothesis: if a post-review recommender is trained with an explicit fairness regularizer, it should increase inclusion without degrading quality. To test this, we introduce Fair-PaperRec, a Multi-Layer Perceptron (MLP) with a differentiable fairness loss over intersectional attributes (e.g., race, country) that re-ranks papers after double-blind review. We first probe the hypothesis on synthetic datasets spanning high, moderate, and near-fair biases. Across multiple randomized runs, these controlled studies map where increasing the fairness weight strengthens macro/micro diversity while keeping utility approximately stable, demonstrating robustness and adaptability under varying disparity levels. We then carry the hypothesis into the original setting, conference data from ACM Special Interest Group on Computer-Human Interaction (SIGCHI), Designing Interactive Systems (DIS), and Intelligent User Interfaces (IUI). In this real-world scenario, an appropriately tuned configuration of Fair-PaperRec achieves up to a 42.03% increase in underrepresented-group participation with at most a 3.16% change in overall utility relative to the historical selection. Taken together, the synthetic-to-original progression shows that fairness regularization can act as both an equity mechanism and a mild quality regularizer, especially in highly biased regimes. By first analyzing the behavior of the fairness parameters under controlled conditions and then validating them on real submissions, Fair-PaperRec offers a practical, equity-focused framework for post-review paper selection that preserves, and in some settings can even enhance, measured scholarly quality.

</details>


### [40] [Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization](https://arxiv.org/abs/2602.23008)
*Zeyuan Liu,Jeonghye Kim,Xufang Luo,Dongsheng Li,Yuqing Yang*

Main category: cs.LG

TL;DR: EMPO²框架通过记忆增强和混合策略优化解决LLM智能体的探索瓶颈，在ScienceWorld和WebShop上显著超越GRPO，并展现强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习训练的LLM智能体在探索新状态时存在瓶颈，依赖预训练知识的方法难以适应需发现新状态的环境。

Method: 提出记忆增强的混合策略优化框架（EMPO²），利用记忆促进探索，结合在线和离线策略更新提升记忆存在/缺失时的鲁棒性。

Result: 在ScienceWorld和WebShop上分别取得128.6%和11.3%的性能提升；OOD测试中仅需少量记忆试错即可适应新任务，无需参数更新。

Conclusion: EMPO²为构建更具探索性和泛化性的LLM智能体提供了有效框架。

Abstract: Exploration remains the key bottleneck for large language model agents trained with reinforcement learning. While prior methods exploit pretrained knowledge, they fail in environments requiring the discovery of novel states. We propose Exploratory Memory-Augmented On- and Off-Policy Optimization (EMPO$^2$), a hybrid RL framework that leverages memory for exploration and combines on- and off-policy updates to make LLMs perform well with memory while also ensuring robustness without it. On ScienceWorld and WebShop, EMPO$^2$ achieves 128.6% and 11.3% improvements over GRPO, respectively. Moreover, in out-of-distribution tests, EMPO$^2$ demonstrates superior adaptability to new tasks, requiring only a few trials with memory and no parameter updates. These results highlight EMPO$^2$ as a promising framework for building more exploratory and generalizable LLM-based agents.

</details>


### [41] [Regularized Online RLHF with Generalized Bilinear Preferences](https://arxiv.org/abs/2602.23116)
*Junghyun Lee,Minju Hong,Kwang-Sung Jun,Chulhee Yun,Se-Young Yun*

Main category: cs.LG

TL;DR: 该论文研究广义偏好下的上下文在线RLHF问题，提出基于GBPM模型的贪婪采样和探索-然后-承诺算法，分别实现了多项式对数遗憾界和首个高维统计高效保证。


<details>
  <summary>Details</summary>
Motivation: 解决具有广义偏好的上下文在线强化学习人类反馈问题，目标是找到纳什均衡，并将先前仅限于反向KL正则化的研究推广到任意强凸正则化器。

Method: 采用广义双线性偏好模型（GBPM）通过低秩、斜对称矩阵捕捉非传递性偏好；基于强凸性和斜对称性证明贪婪策略对偶间隙受估计误差平方的约束；在特征多样性假设下设计两种算法。

Result: 证明了核心理论洞察——对偶间隙与估计误差的平方关系；贪婪采样算法达到Õ(ηd^4 (log T)^2)的指数无关遗憾界；探索-然后-承诺算法达到Õ(√(ηrT))的低秩无关遗憾界，是在线RLHF在高维情况下的首个统计高效结果。

Conclusion: 该研究突破了在线RLHF的理论限制，为广义偏好学习提供了更强大的分析工具和算法保证，特别是在高维场景下实现了统计效率。

Abstract: We consider the problem of contextual online RLHF with general preferences, where the goal is to identify the Nash Equilibrium. We adopt the Generalized Bilinear Preference Model (GBPM) to capture potentially intransitive preferences via low-rank, skew-symmetric matrices. We investigate general preference learning with any strongly convex regularizer (where $η^{-1}$ is the regularization strength), generalizing beyond prior works limited to reverse KL-regularization. Central to our analysis is proving that the dual gap of the greedy policy is bounded by the square of the estimation error - a result derived solely from strong convexity and the skew-symmetricity of GBPM.Building on this insight and a feature diversity assumption, we establish two regret bounds via two simple algorithms: (1) Greedy Sampling achieves polylogarithmic, $e^{O(η)}$-free regret $\tilde{O}(ηd^4 (\log T)^2)$. (2) Explore-Then-Commit achieves $\mathrm{poly}(d)$-free regret $\tilde{O}(\sqrt{ηr T})$ by exploiting the low-rank structure; this is the first statistically efficient guarantee for online RLHF in high-dimensions.

</details>


### [42] [FlashOptim: Optimizers for Memory Efficient Training](https://arxiv.org/abs/2602.23349)
*Jose Javier Gonzalez Ortiz,Abhay Gupta,Chris Renard,Davis Blalock*

Main category: cs.LG

TL;DR: FlashOptim是一种内存优化方案，可将混合精度训练中每参数内存占用降低50%以上，使AdamW从每参数16字节降至7字节（释放梯度后仅5字节），同时保持模型质量和API兼容性，让70亿参数模型在不足100GB显存的设备上训练成为可能。


<details>
  <summary>Details</summary>
Motivation: 标准混合精度训练每参数需存储参数、梯度和优化器状态（通常各4字节），AdamW总计16字节/参数，导致70亿参数模型训练对显存不足100GB的研究者不切实际。

Method: 提出两项关键技术：(1) 通过量化误差紧约束改进主权重分割；(2) 设计压缩扩展函数实现8位优化器状态量化，结合16位梯度。

Result: AdamW内存从16字节/参数降至7字节（释放梯度后5字节），检查点大小减半，在Llama-3.1-8B微调等视觉语言基准测试中无质量损失。

Conclusion: FlashOptim通过创新量化技术有效解决大模型训练内存瓶颈，在保持性能的同时使大规模训练更普及。

Abstract: Standard mixed-precision training of neural networks requires many bytes of accelerator memory for each model parameter. These bytes reflect not just the parameter itself, but also its gradient and one or more optimizer state variables. With each of these values typically requiring 4 bytes, training even a 7 billion parameter model can be impractical for researchers with less than 100GB of accelerator memory.
  We introduce FlashOptim, a suite of optimizations that reduces per-parameter memory by over 50% while preserving model quality and API compatibility. Our approach introduces two key techniques. First, we improve master weight splitting by finding and exploiting a tight bound on its quantization error. Second, we design companding functions that greatly reduce the error in 8-bit optimizer state quantization. Together with 16-bit gradients, these techniques reduce AdamW memory from 16 bytes to 7 bytes per parameter, or 5 bytes with gradient release. They also cut model checkpoint sizes by more than half.
  Experiments with FlashOptim applied to SGD, AdamW, and Lion show no measurable quality degradation on any task from a collection of standard vision and language benchmarks, including Llama-3.1-8B finetuning.

</details>


### [43] [SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport](https://arxiv.org/abs/2602.23353)
*Simon Roschmann,Paul Krzakala,Sonia Mazelet,Quentin Bouniot,Zeynep Akata*

Main category: cs.LG

TL;DR: 提出SOTAlign框架，通过两阶段方法（线性教师模型+最优传输）利用少量配对数据和大量未配对数据，实现视觉-语言模型的半监督对齐，性能优于现有基线


<details>
  <summary>Details</summary>
Motivation: 现有方法需要数百万配对样本和对比损失来对齐预训练视觉语言模型，成本高昂；探索是否可以用更少监督实现有意义对齐

Method: 两阶段框架：1）用线性教师从有限配对数据恢复粗略共享几何结构；2）用最优传输散度在未配对样本上细化对齐，转移关系结构而不过度约束目标空间

Result: 有效利用未配对图像文本，跨数据集和编码器学习到鲁棒的联合嵌入，显著优于监督和半监督基线方法

Conclusion: SOTAlign成功实现了用极少监督的跨模态对齐，证明了利用大量未配对数据的可行性，为降低数据标注成本提供了新方案

Abstract: The Platonic Representation Hypothesis posits that neural networks trained on different modalities converge toward a shared statistical model of the world. Recent work exploits this convergence by aligning frozen pretrained vision and language models with lightweight alignment layers, but typically relies on contrastive losses and millions of paired samples. In this work, we ask whether meaningful alignment can be achieved with substantially less supervision. We introduce a semi-supervised setting in which pretrained unimodal encoders are aligned using a small number of image-text pairs together with large amounts of unpaired data. To address this challenge, we propose SOTAlign, a two-stage framework that first recovers a coarse shared geometry from limited paired data using a linear teacher, then refines the alignment on unpaired samples via an optimal-transport-based divergence that transfers relational structure without overconstraining the target space. Unlike existing semi-supervised methods, SOTAlign effectively leverages unpaired images and text, learning robust joint embeddings across datasets and encoder pairs, and significantly outperforming supervised and semi-supervised baselines.

</details>


### [44] [Inferential Mechanics Part 1: Causal Mechanistic Theories of Machine Learning in Chemical Biology with Implications](https://arxiv.org/abs/2602.23303)
*Ilya Balabin,Thomas M. Kaiser*

Main category: cs.LG

TL;DR: A three-part series proposes a theoretical framework integrating causality with machine learning for chemical biology. Part 1 introduces "focus" - ML's ability to identify underlying mechanisms - and tests it on Akt inhibitors.


<details>
  <summary>Details</summary>
Motivation: Machine learning models in natural sciences are often treated as black boxes without considering causal structures, leading to causal flaws. A firm, unified theoretical treatment is lacking.

Method: Establishes a formal framework for causal structure in chemical biology and extends it to ML through the novel concept of "focus" (ML's ability to narrow down to hidden mechanisms). Initial proof is provided on Akt inhibitors.

Result: Initial proof of the "focus" concept principles demonstrated on a family of Akt inhibitors. The series aims to establish "inferential mechanics" as a new mathematical framework for modeling mechanisms without reductionism.

Conclusion: The series will provide a unified theoretical framework for causality-aware machine learning in chemical biology, addressing current causal flaws and establishing inferential mechanics for modeling natural mechanisms.

Abstract: Machine learning techniques are now routinely encountered in research laboratories across the globe. Impressive progress has been made through ML and AI techniques with regards to large data set processing. This progress has increased the ability of the experimenter to digest data and make novel predictions regarding phenomena of interest. However, machine learning predictors generated from data sets taken from the natural sciences are often treated as black boxes which are used broadly and generally without detailed consideration of the causal structure of the data set of interest. Work has been attempted to bring causality into discussions of machine learning models of natural phenomena; however, a firm and unified theoretical treatment is lacking. This series of three papers explores the union of chemical theory, biological theory, probability theory and causality that will correct current causal flaws of machine learning in the natural sciences. This paper, Part 1 of the series, provides the formal framework of the foundational causal structure of phenomena in chemical biology and is extended to machine learning through the novel concept of focus, defined here as the ability of a machine learning algorithm to narrow down to a hidden underpinning mechanism in large data sets. Initial proof of these principles on a family of Akt inhibitors is also provided. The second paper containing Part 2 will provide a formal exploration of chemical similarity, and Part 3 will present extensive experimental evidence of how hidden causal structures weaken all machine learning in chemical biology. This series serves to establish for chemical biology a new kind of mathematical framework for modeling mechanisms in Nature without the need for the tools of reductionism: inferential mechanics.

</details>


### [45] [Differentiable Zero-One Loss via Hypersimplex Projections](https://arxiv.org/abs/2602.23336)
*Camilo Gomez,Pengyang Wang,Liansheng Tang*

Main category: cs.LG

TL;DR: 提出Soft-Binary-Argmax算子，通过约束优化实现零一损失的可微近似，解决大批量训练泛化差距问题


<details>
  <summary>Details</summary>
Motivation: 零一损失是分类任务的金标准但不可微，现有梯度优化方法存在局限；大批量训练时模型泛化性能显著下降

Method: 构建n维k-单形的平滑保序投影，设计可微的Soft-Binary-Argmax算子；推导其雅可比矩阵并集成到分类系统

Result: 在大批量训练下显著提升泛化性能，通过输出logits的几何一致性约束缩小传统性能差距

Conclusion: 首次实现零一损失的有效可微近似，为结构化优化组件集成提供新范式，特别改善大批量训练效果

Abstract: Recent advances in machine learning have emphasized the integration of structured optimization components into end-to-end differentiable models, enabling richer inductive biases and tighter alignment with task-specific objectives. In this work, we introduce a novel differentiable approximation to the zero-one loss-long considered the gold standard for classification performance, yet incompatible with gradient-based optimization due to its non-differentiability. Our method constructs a smooth, order-preserving projection onto the n,k-dimensional hypersimplex through a constrained optimization framework, leading to a new operator we term Soft-Binary-Argmax. After deriving its mathematical properties, we show how its Jacobian can be efficiently computed and integrated into binary and multiclass learning systems. Empirically, our approach achieves significant improvements in generalization under large-batch training by imposing geometric consistency constraints on the output logits, thereby narrowing the performance gap traditionally observed in large-batch training.

</details>
