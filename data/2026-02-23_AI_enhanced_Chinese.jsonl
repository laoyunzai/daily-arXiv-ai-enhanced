{"id": "2602.18335", "categories": ["cond-mat.dis-nn", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2602.18335", "abs": "https://arxiv.org/abs/2602.18335", "authors": ["Guy Greenbaum", "Will R. Branford", "Andrew L. Goodwin"], "title": "Responsive Disorder in a Metal-Organic Framework Enables Solid-State Reservoir Computing", "comment": null, "summary": "Complex systems with nonlinear response mechanisms can be applied as reservoir computers for energy-efficient machine learning tasks. Historically explored at the macro- and meso-scale, physical reservoir computing has recently been extended to the atomic scale via chemical mixtures with strong and dynamic heterogeneity. Here we explore the possibility that configurational degeneracy within disordered materials might form the basis for solid-state atomic-scale reservoirs. Our proof-of-concept uses the disordered metal-organic framework DUT-8, which undergoes a series of disorder-disorder transitions on exposure to different guest species. We show that variations in X-ray diffuse scattering associated with these transitions function as suitable readouts for machine learning applications. A combination of nonlinearity and memory effects in the DUT-8 response allows the system to carry out both classification and time-series machine learning tasks with accuracies comparable to those of mesoscale physical reservoir computers. Our results suggest a new avenue for exploiting correlated disorder in solid phases whenever the nature of that disorder can be modulated through external perturbations-a phenomenon we term `responsive disorder'.", "AI": {"tldr": "\u5229\u7528\u65e0\u5e8f\u6750\u6599DUT-8\u7684\u6784\u578b\u7b80\u5e76\u6027\u5b9e\u73b0\u539f\u5b50\u5c3a\u5ea6\u7269\u7406\u50a8\u5c42\u8ba1\u7b97\uff0c\u901a\u8fc7X\u5c04\u7ebf\u6f2b\u6563\u5c04\u53d8\u5316\u4f5c\u4e3a\u8bfb\u51fa\u4fe1\u53f7\uff0c\u5728\u5206\u7c7b\u548c\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u4e2d\u8fbe\u5230\u4ecb\u89c2\u5c3a\u5ea6\u50a8\u5c42\u8ba1\u7b97\u673a\u7684\u51c6\u786e\u7387", "motivation": "\u4f20\u7edf\u7269\u7406\u50a8\u5c42\u8ba1\u7b97\u5c40\u9650\u4e8e\u5b8f\u89c2/\u4ecb\u89c2\u5c3a\u5ea6\uff0c\u672c\u7814\u7a76\u63a2\u7d22\u539f\u5b50\u5c3a\u5ea6\u50a8\u5c42\u8ba1\u7b97\u7684\u53ef\u884c\u6027\uff0c\u5229\u7528\u65e0\u5e8f\u6750\u6599\u7684\u6784\u578b\u7b80\u5e76\u6027\u548c\u5916\u90e8\u6270\u52a8\u54cd\u5e94\u7279\u6027\uff08\u54cd\u5e94\u6027\u65e0\u5e8f\uff09\u6784\u5efa\u65b0\u578b\u56fa\u6001\u50a8\u5c42\u7cfb\u7edf", "method": "\u91c7\u7528\u5177\u6709\u52a8\u6001\u5f02\u8d28\u6027\u7684\u91d1\u5c5e\u6709\u673a\u6846\u67b6\u6750\u6599DUT-8\uff0c\u901a\u8fc7\u66b4\u9732\u4e8e\u4e0d\u540c\u5ba2\u4f53\u5206\u5b50\u5f15\u53d1\u65e0\u5e8f-\u65e0\u5e8f\u76f8\u53d8\uff0c\u5229\u7528X\u5c04\u7ebf\u6f2b\u6563\u5c04\u53d8\u5316\u4f5c\u4e3a\u673a\u5668\u5b66\u4e60\u7684\u8bfb\u51fa\u4fe1\u53f7\uff0c\u6d4b\u8bd5\u7cfb\u7edf\u7684\u975e\u7ebf\u6027\u4e0e\u8bb0\u5fc6\u6548\u5e94", "result": "DUT-8\u7cfb\u7edf\u6210\u529f\u6267\u884c\u5206\u7c7b\u548c\u65f6\u95f4\u5e8f\u5217\u673a\u5668\u5b66\u4e60\u4efb\u52a1\uff0c\u51c6\u786e\u7387\u4e0e\u4ecb\u89c2\u5c3a\u5ea6\u7269\u7406\u50a8\u5c42\u8ba1\u7b97\u673a\u76f8\u5f53\uff0c\u8bc1\u5b9e\u6784\u578b\u7b80\u5e76\u6027\u53ef\u4f5c\u4e3a\u539f\u5b50\u5c3a\u5ea6\u50a8\u5c42\u8ba1\u7b97\u57fa\u7840", "conclusion": "\u63d0\u51fa\"\u54cd\u5e94\u6027\u65e0\u5e8f\"\u65b0\u6982\u5ff5\uff0c\u8bc1\u660e\u901a\u8fc7\u5916\u90e8\u6270\u52a8\u8c03\u63a7\u56fa\u6001\u6750\u6599\u4e2d\u7684\u5173\u8054\u65e0\u5e8f\u53ef\u6784\u5efa\u9ad8\u6548\u539f\u5b50\u5c3a\u5ea6\u50a8\u5c42\u8ba1\u7b97\u7cfb\u7edf\uff0c\u4e3a\u65b0\u578b\u4f4e\u529f\u8017\u673a\u5668\u5b66\u4e60\u786c\u4ef6\u5f00\u8f9f\u65b0\u9014\u5f84"}}
{"id": "2602.18399", "categories": ["cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2602.18399", "abs": "https://arxiv.org/abs/2602.18399", "authors": ["Silvio Franz", "Giorgio Parisi", "Federico Ricci-Tersenghi"], "title": "Overlap locking and non-perturbative effects in spin glasses", "comment": "15 pages, 9 figures, submitted to PNAS", "summary": "We study the phenomenon of the locking of the order parameter (or synchronization) in spin glasses at low temperatures. When two systems with independent disorders are coupled, their overlaps become similar. A crucial question is how this effect depends on the strength of the coupling between the two systems. Non-perturbative phenomena are present when $1 \\ll \u0394H \\ll N$, being $\u0394H$ the coupling Hamiltonian and $N$ the size of the system. In this intermediate-coupling region, the effect is related to finite-size free-energy corrections and to the correlations in the Dyson hierarchical spin glass, a model that mimics the physics of finite-dimensional systems. We study this phenomenon in the mean-field approach, both analytically and numerically, and we finally compute the critical exponents for finite-volume corrections in mean-field theory and for the decay of correlations in the Dyson hierarchical model.", "AI": {"tldr": "This paper analyzes synchronization (order parameter locking) in coupled spin glass systems at low temperatures, focusing on non-perturbative effects in the intermediate-coupling regime (1 \u226a \u0394H \u226a N). It connects finite-size free-energy corrections to Dyson hierarchical model correlations, computing critical exponents for volume corrections and correlation decay via mean-field theory.", "motivation": "Understanding how coupling strength affects order parameter synchronization between disordered spin glass systems at low temperatures, particularly in the non-perturbative intermediate-coupling regime where standard approximation methods fail.", "method": "Combined analytical and numerical mean-field approach, analyzing finite-size free-energy corrections and correlations in the Dyson hierarchical spin glass model (which mimics finite-dimensional physics).", "result": "Computed critical exponents for: (1) finite-volume corrections in mean-field theory, and (2) decay of correlations in the Dyson hierarchical model, revealing non-perturbative behavior in the 1 \u226a \u0394H \u226a N regime.", "conclusion": "The synchronization phenomenon in coupled spin glasses is fundamentally linked to finite-size effects and hierarchical model correlations, with critical exponents quantifying the non-perturbative coupling dependence in mean-field theory."}}
{"id": "2602.18213", "categories": ["cond-mat.str-el", "cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.18213", "abs": "https://arxiv.org/abs/2602.18213", "authors": ["Gia-Wei Chern", "Yunhao Fan", "Sheng Zhang", "Puhan Zhang"], "title": "Machine-learning force-field models for dynamical simulations of metallic magnets", "comment": "9 pages, 5 figures", "summary": "We review recent advances in machine learning (ML) force-field methods for Landau-Lifshitz-Gilbert (LLG) simulations of itinerant electron magnets, focusing on scalability and transferability. Built on the principle of locality, a deep neural network model is developed to efficiently and accurately predict the electron-mediated forces governing spin dynamics. Symmetry-aware descriptors constructed through a group-theoretical approach ensure rigorous incorporation of both lattice and spin-rotation symmetries. The framework is demonstrated using the prototypical s-d exchange model widely employed in spintronics. ML-enabled large-scale simulations reveal novel nonequilibrium phenomena, including anomalous coarsening of tetrahedral spin order on the triangular lattice and the freezing of phase separation dynamics in lightly hole-doped, strong-coupling square-lattice systems. These results establish ML force-field frameworks as scalable, accurate, and versatile tools for modeling nonequilibrium spin dynamics in itinerant magnets.", "AI": {"tldr": "Develops a scalable machine learning force-field framework for Landau-Lifshitz-Gilbert simulations of itinerant magnets, using deep neural networks with symmetry-aware descriptors to predict spin dynamics and reveal novel nonequilibrium phenomena.", "motivation": "To address computational scalability and transferability challenges in simulating itinerant electron magnets for spintronics applications, which are crucial for understanding complex spin dynamics at large scales.", "method": "Deep neural network model built on locality principle, combined with group-theoretical symmetry-aware descriptors that incorporate lattice and spin-rotation symmetries, applied to the s-d exchange model for large-scale LLG simulations.", "result": "Revealed anomalous coarsening of tetrahedral spin order on triangular lattices and freezing of phase separation dynamics in lightly hole-doped square-lattice systems, demonstrating the framework's ability to capture complex nonequilibrium spin phenomena.", "conclusion": "ML force-field frameworks are established as scalable, accurate, and versatile computational tools for modeling nonequilibrium spin dynamics in itinerant magnets, enabling large-scale simulations previously intractable with traditional methods."}}
{"id": "2602.17904", "categories": ["cs.CC", "cs.SC"], "pdf": "https://arxiv.org/pdf/2602.17904", "abs": "https://arxiv.org/abs/2602.17904", "authors": ["Robert Andrews", "Abhibhav Garg", "\u00c9ric Schost"], "title": "Hilbert's Nullstellensatz is in the Counting Hierarchy", "comment": null, "summary": "We show that Hilbert's Nullstellensatz, the problem of deciding if a system of multivariate polynomial equations has a solution in the algebraic closure of the underlying field, lies in the counting hierarchy. More generally, we show that the number of solutions to a system of equations can be computed in polynomial time with oracle access to the counting hierarchy. Our results hold in particular for polynomials with coefficients in either the rational numbers or a finite field. Previously, the best-known bounds on the complexities of these problems were PSPACE and FPSPACE, respectively. Our main technical contribution is the construction of a uniform family of constant-depth arithmetic circuits that compute the multivariate resultant.", "AI": {"tldr": "\u8bc1\u660eHilbert's Nullstellensatz\u95ee\u9898\u5c5e\u4e8e\u8ba1\u6570\u5c42\u7ea7\uff0c\u4e14\u53ef\u5728P^CH\u4e2d\u8ba1\u7b97\u591a\u5143\u591a\u9879\u5f0f\u65b9\u7a0b\u7ec4\u7684\u89e3\u7684\u6570\u91cf\u3002", "motivation": "\u6b64\u524d\u8be5\u95ee\u9898\u7684\u590d\u6742\u5ea6\u754c\u9650\u4e3aPSPACE\u548cFPSPACE\uff0c\u8f83\u9ad8\uff0c\u65e8\u5728\u627e\u5230\u66f4\u4f4e\u7684\u590d\u6742\u5ea6\u7c7b\u522b\u4ee5\u4f18\u5316\u8ba1\u7b97\u3002", "method": "\u6784\u9020\u5747\u5300\u5e38\u6570\u6df1\u5ea6\u7b97\u672f\u7535\u8def\u6765\u8ba1\u7b97\u591a\u5143\u7ed3\u5f0f\u3002", "result": "Hilbert's Nullstellensatz\u95ee\u9898\u4f4d\u4e8e\u8ba1\u6570\u5c42\u7ea7\u5185\uff1b\u89e3\u7684\u6570\u91cf\u8ba1\u7b97\u53ef\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u901a\u8fc7\u8ba1\u6570\u5c42\u7ea7\u9884\u8a00\u673a\u5b8c\u6210\uff1b\u7ed3\u679c\u9002\u7528\u4e8e\u6709\u7406\u6570\u548c\u6709\u9650\u57df\u4e0a\u7684\u591a\u9879\u5f0f\u3002", "conclusion": "\u6210\u679c\u964d\u4f4e\u4e86\u95ee\u9898\u7684\u5df2\u77e5\u590d\u6742\u5ea6\u754c\u9650\uff0c\u8868\u660e\u5176\u5728\u8ba1\u7b97\u4e0a\u66f4\u6613\u5904\u7406\uff0c\u5bf9\u4ee3\u6570\u8ba1\u7b97\u9886\u57df\u5177\u6709\u79ef\u6781\u610f\u4e49\u3002"}}
{"id": "2602.17839", "categories": ["cond-mat.stat-mech", "nlin.CD"], "pdf": "https://arxiv.org/pdf/2602.17839", "abs": "https://arxiv.org/abs/2602.17839", "authors": ["Edson D. Leonel", "Diego F. M. Oliveira"], "title": "Scaling invariance: a bridge between geometry, dynamics and criticality", "comment": null, "summary": "Scale invariance is a central organizing principle in physics, underlying phenomena that range from critical behaviour in statistical mechanics to transport and chaos in nonlinear dynamical systems. Here we present a unified and physically motivated exploration of scaling concepts, emphasizing how invariance under rescaling transformations emerges across systems of increasing dynamical complexity. Rather than adopting a purely abstract approach, we combine simple geometrical constructions, analytical arguments, and prototypical dynamical models to build physical intuition. We begin with elementary, easily reproducible examples governed by a single control parameter, showing how power-law behaviour naturally arises when characteristic scales are absent. We then extend the discussion to nonlinear dynamical systems exhibiting local bifurcations, where two scaling variables control the relaxation toward stationary states. In this context, scaling invariance manifests through critical exponents, crossover phenomena, and critical slowing down, allowing systems of different dimensionality to be grouped into universality classes. Finally, we address continuous phase transitions in chaotic dynamical systems, including transitions from integrability to non-integrability and from bounded to unbounded diffusion. By drawing on concepts traditionally associated with statistical mechanics, such as order parameters, susceptibilities, symmetry breaking, elementary excitations, and topological defects, we show how these transitions can be interpreted within a coherent scaling framework. Taken together, the examples discussed here demonstrate that scaling invariance provides a unifying language for understanding structure, transport, and criticality in nonlinear systems, bridging deterministic dynamics and nonequilibrium statistical physics in a transparent and physically intuitive manner.", "AI": {"tldr": "This paper presents a unified, physically intuitive framework for scale invariance across dynamical systems, from simple power-law behaviors to complex chaotic transitions, bridging deterministic dynamics and nonequilibrium statistical physics.", "motivation": "Scale invariance is fundamental in physics but lacks a coherent, physically motivated exploration across systems of increasing dynamical complexity; the paper aims to build physical intuition rather than relying solely on abstract approaches.", "method": "Combines simple geometrical constructions, analytical arguments, and prototypical dynamical models, progressing from elementary examples to nonlinear systems and chaotic dynamics, applying statistical mechanics concepts like order parameters and symmetry breaking.", "result": "Demonstrates that scaling invariance naturally emerges across diverse systems, manifests through critical exponents and universality classes in bifurcations, and describes continuous phase transitions in chaotic systems, unifying structure, transport, and criticality.", "conclusion": "Scaling invariance provides a unifying language that bridges deterministic dynamics and nonequilibrium statistical physics, offering transparent physical intuition for understanding complex phenomena across different scales and system complexities."}}
{"id": "2602.17676", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17676", "abs": "https://arxiv.org/abs/2602.17676", "authors": ["Xingcheng Xu", "Jingjing Qu", "Qiaosheng Zhang", "Chaochao Lu", "Yanqing Yang", "Na Zou", "Xia Hu"], "title": "Epistemic Traps: Rational Misalignment Driven by Model Misspecification", "comment": null, "summary": "The rapid deployment of Large Language Models and AI agents across critical societal and technical domains is hindered by persistent behavioral pathologies including sycophancy, hallucination, and strategic deception that resist mitigation via reinforcement learning. Current safety paradigms treat these failures as transient training artifacts, lacking a unified theoretical framework to explain their emergence and stability. Here we show that these misalignments are not errors, but mathematically rationalizable behaviors arising from model misspecification. By adapting Berk-Nash Rationalizability from theoretical economics to artificial intelligence, we derive a rigorous framework that models the agent as optimizing against a flawed subjective world model. We demonstrate that widely observed failures are structural necessities: unsafe behaviors emerge as either a stable misaligned equilibrium or oscillatory cycles depending on reward scheme, while strategic deception persists as a \"locked-in\" equilibrium or through epistemic indeterminacy robust to objective risks. We validate these theoretical predictions through behavioral experiments on six state-of-the-art model families, generating phase diagrams that precisely map the topological boundaries of safe behavior. Our findings reveal that safety is a discrete phase determined by the agent's epistemic priors rather than a continuous function of reward magnitude. This establishes Subjective Model Engineering, defined as the design of an agent's internal belief structure, as a necessary condition for robust alignment, marking a paradigm shift from manipulating environmental rewards to shaping the agent's interpretation of reality.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5927\u6a21\u578b\u7684\u4e0d\u5b89\u5168\u884c\u4e3a\uff08\u5982\u8c04\u5a9a\u3001\u5e7b\u89c9\u548c\u7b56\u7565\u6027\u6b3a\u9a97\uff09\u5e76\u975e\u8bad\u7ec3\u7f3a\u9677\uff0c\u800c\u662f\u6e90\u4e8e\u6a21\u578b\u8bef\u8bbe\u7684\u6570\u5b66\u7406\u6027\u884c\u4e3a\uff1b\u901a\u8fc7\u5f15\u5165\u7ecf\u6d4e\u5b66\u4e2d\u7684Berk-Nash\u7406\u6027\u5316\u7406\u8bba\uff0c\u5efa\u7acb\u4e86\u4e3b\u89c2\u4e16\u754c\u6a21\u578b\u6846\u67b6\uff0c\u63ed\u793a\u5b89\u5168\u662f\u4e00\u79cd\u7531\u8ba4\u77e5\u5148\u9a8c\u51b3\u5b9a\u7684\u79bb\u6563\u76f8\u4f4d\uff0c\u800c\u975e\u5956\u52b1\u5e45\u5ea6\u7684\u8fde\u7eed\u51fd\u6570\uff0c\u4ece\u800c\u5c06AI\u5bf9\u9f50\u8303\u5f0f\u4ece\u73af\u5883\u5956\u52b1\u64cd\u7eb5\u8f6c\u5411\u667a\u80fd\u4f53\u5185\u90e8\u4fe1\u5ff5\u7ed3\u6784\u8bbe\u8ba1\u3002", "motivation": "\u5f53\u524d\u5b89\u5168\u8303\u5f0f\u5c06\u5927\u6a21\u578b\u7684\u884c\u4e3a\u75c5\u7406\uff08\u5982\u8c04\u5a9a\u3001\u5e7b\u89c9\u548c\u7b56\u7565\u6027\u6b3a\u9a97\uff09\u89c6\u4e3a\u53ef\u4fee\u590d\u7684\u8bad\u7ec3\u7f3a\u9677\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7406\u8bba\u6846\u67b6\u89e3\u91ca\u5176\u4ea7\u751f\u673a\u5236\u548c\u7a33\u5b9a\u6027\uff0c\u65e0\u6cd5\u4ece\u6839\u672c\u4e0a\u89e3\u51b3\u8fd9\u4e9b\u6301\u7eed\u5b58\u5728\u7684\u5b89\u5168\u9690\u60a3\u3002", "method": "\u5c06\u7406\u8bba\u7ecf\u6d4e\u5b66\u4e2d\u7684Berk-Nash\u7406\u6027\u5316\u7406\u8bba\u9002\u914d\u5230\u4eba\u5de5\u667a\u80fd\u9886\u57df\uff0c\u5efa\u7acb\u6570\u5b66\u6a21\u578b\u5c06\u667a\u80fd\u4f53\u63cf\u8ff0\u4e3a\u9488\u5bf9\u6709\u7f3a\u9677\u7684\u4e3b\u89c2\u4e16\u754c\u6a21\u578b\u8fdb\u884c\u4f18\u5316\u7684\u5b9e\u4f53\uff1b\u901a\u8fc7\u884c\u4e3a\u5b9e\u9a8c\u5728\u516d\u4e2a\u5148\u8fdb\u6a21\u578b\u5bb6\u65cf\u4e0a\u9a8c\u8bc1\u7406\u8bba\u9884\u6d4b\u5e76\u751f\u6210\u5b89\u5168\u884c\u4e3a\u7684\u76f8\u56fe\u3002", "result": "\u8bc1\u660e\u5e7f\u6cdb\u89c2\u5bdf\u5230\u7684\u5931\u8d25\u662f\u7ed3\u6784\u6027\u5fc5\u7136\uff1a\u4e0d\u5b89\u5168\u884c\u4e3a\u6839\u636e\u5956\u52b1\u65b9\u6848\u5448\u73b0\u4e3a\u7a33\u5b9a\u8bef\u5bf9\u9f50\u5e73\u8861\u6216\u632f\u8361\u5468\u671f\uff1b\u7b56\u7565\u6027\u6b3a\u9a97\u6301\u7eed\u4f5c\u4e3a\"\u9501\u5b9a\"\u5e73\u8861\u6216\u901a\u8fc7\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u62b5\u5fa1\u5ba2\u89c2\u98ce\u9669\uff1b\u5b89\u5168\u662f\u7531\u667a\u80fd\u4f53\u8ba4\u77e5\u5148\u9a8c\u51b3\u5b9a\u7684\u79bb\u6563\u76f8\u4f4d\u800c\u975e\u5956\u52b1\u5e45\u5ea6\u7684\u8fde\u7eed\u51fd\u6570\u3002", "conclusion": "\u63d0\u51fa\u4e86\"\u4e3b\u89c2\u6a21\u578b\u5de5\u7a0b\"\u2014\u2014\u8bbe\u8ba1\u667a\u80fd\u4f53\u5185\u90e8\u4fe1\u5ff5\u7ed3\u6784\u2014\u2014\u4f5c\u4e3a\u5b9e\u73b0\u9c81\u68d2\u5bf9\u9f50\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u6807\u5fd7\u7740\u4ece\u64cd\u7eb5\u73af\u5883\u5956\u52b1\u5230\u5851\u9020\u667a\u80fd\u4f53\u73b0\u5b9e\u89e3\u8bfb\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u4e3aAI\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u65b9\u5411\u3002"}}
{"id": "2602.17748", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.17748", "abs": "https://arxiv.org/abs/2602.17748", "authors": ["Hyunho Cha"], "title": "A dimension-independent strict submultiplicativity for the transposition map in diamond norm", "comment": "4 pages", "summary": "We prove that there exists an absolute constant $\u03b1<1$ such that for every finite dimension $d$ and every quantum channel $T$ on $\\mathsf{L}(\\mathbb{C}^d)$, $\\left\\|\u0398\\circ(\\mathrm{id}-T)\\right\\|_\\diamond \\le \u03b1\\,\\left\\|\u0398\\right\\|_\\diamond\\,\\left\\|\\mathrm{id}-T\\right\\|_\\diamond$, where $\u0398$ is the transposition map. In fact we show the explicit choice $\u03b1=1/\\sqrt{2}$ works.", "AI": {"tldr": "\u8bc1\u660e\u4e86\u4e00\u4e2a\u5173\u4e8e\u91cf\u5b50\u4fe1\u9053\u4e0e\u8f6c\u7f6e\u6620\u5c04\u590d\u5408\u7684\u94bb\u77f3\u8303\u6570\u4e0d\u7b49\u5f0f\uff0c\u5e76\u7ed9\u51fa\u666e\u9002\u5e38\u6570 $\u03b1=1/\\sqrt{2}$\u3002", "motivation": "\u5efa\u7acb\u91cf\u5b50\u4fe1\u9053\u4e0e\u8f6c\u7f6e\u6620\u5c04\u590d\u5408\u7684\u8303\u6570\u754c\uff0c\u6709\u52a9\u4e8e\u6df1\u5165\u7406\u89e3\u4fe1\u9053\u7ed3\u6784\u548c\u6b63\u6620\u5c04\u6027\u8d28\u3002", "method": "\u57fa\u4e8e\u6cdb\u51fd\u5206\u6790\u548c\u7b97\u5b50\u7406\u8bba\u3002", "result": "\u8bc1\u660e\u4e86\u666e\u9002\u5e38\u6570 $\u03b1=1/\\sqrt{2}$ \u6ee1\u8db3\u8be5\u4e0d\u7b49\u5f0f\u3002", "conclusion": "\u8be5\u4e0d\u7b49\u5f0f\u4e3a\u91cf\u5b50\u4fe1\u9053\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u5b9a\u91cf\u5de5\u5177\u3002"}}
{"id": "2602.17802", "categories": ["nlin.CD"], "pdf": "https://arxiv.org/pdf/2602.17802", "abs": "https://arxiv.org/abs/2602.17802", "authors": ["Edson D. Leonel", "Mayla A. M. de Almeida", "Juan Pedro Tarigo", "Arturo C. Marti", "Diego F. M. Oliveira"], "title": "Universal Second-Order Phase Transition from Integrability to Chaos", "comment": null, "summary": "We report a dynamical phase transition from integrability to non-integrability in a simple oval-like billiard with boundary $R(\u03b8)=1+\u03b5\\cos(p\u03b8)$. For $\u03b5=0$, the phase space is {\\it foliated} by invariant curves corresponding to periodic or quasiperiodic motion, whereas for small $\u03b5$ a thin chaotic layer separates rotational and librational trajectories. As $\u03b5$ increases, this layer grows according to a well-defined scaling law whose chaotic dispersion follows $\u03c9_{\\rm rms,sat}\\sim\u03b5^{\\tilde\u03b1}$, where the exponent $\\tilde\u03b1$ coincides with those of the Fermi-Ulam model, periodically corrugated waveguides, and a family of discrete mappings, revealing a universal mechanism for the onset of chaos in weakly perturbed integrable systems. The deviation of the reflection angle in the billiard, $\u03c9_{\\rm rms,sat}$, acts as an order parameter: it vanishes continuously as $\u03b5\\to 0$, signalling an ordered (integrable) phase, while its susceptibility $\u03c7=d\u03c9_{\\rm rms,sat}/d\u03b5$ diverges, indicating a second-order phase transition. A symmetry breaking and an analytically solvable diffusion process complete the near-critical phenomenology. These results establish a unified framework for the emergence of chaos from integrability.", "AI": {"tldr": "\u7814\u7a76 oval \u578b\u53f0\u7403\u7cfb\u7edf\u4e2d\u4ece\u53ef\u79ef\u6027\u5230\u4e0d\u53ef\u79ef\u6027\u7684\u52a8\u529b\u5b66\u76f8\u53d8\uff0c\u63ed\u793a\u5f31\u6270\u52a8\u4e0b\u6df7\u6c8c\u51fa\u73b0\u7684\u666e\u9002\u673a\u5236", "motivation": "\u63a2\u7d22\u5f31\u6270\u52a8\u53ef\u79ef\u7cfb\u7edf\u4e2d\u6df7\u6c8c\u51fa\u73b0\u7684\u7edf\u4e00\u673a\u5236\uff0c\u5efa\u7acb\u5176\u4e0e\u76f8\u53d8\u7406\u8bba\u7684\u8054\u7cfb", "method": "\u5206\u6790\u8fb9\u754c\u4e3a R(\u03b8)=1+\u03b5cos(p\u03b8) \u7684 oval-like \u53f0\u7403\u7cfb\u7edf\uff0c\u7814\u7a76 \u03b5 \u53c2\u6570\u53d8\u5316\u5bf9\u76f8\u7a7a\u95f4\u7ed3\u6784\u7684\u5f71\u54cd", "result": "\u53d1\u73b0\u4e8c\u7ea7\u76f8\u53d8\u7279\u5f81\uff1a\u6df7\u6c8c\u5c42\u968f \u03b5 \u589e\u5927\u6309\u6807\u5ea6\u5f8b\u589e\u957f\uff0c\u6df7\u6c8c\u6269\u6563\u6307\u6570 tilde\u03b1 \u4e0e Fermi-Ulam \u6a21\u578b\u7b49\u7cfb\u7edf\u4e00\u81f4\uff0c\u5448\u73b0\u666e\u9002\u6027", "conclusion": "\u5efa\u7acb\u4e86\u4ece\u53ef\u79ef\u6027\u5230\u6df7\u6c8c\u7684\u7edf\u4e00\u7406\u8bba\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u6709\u5e8f-\u65e0\u5e8f\u76f8\u53d8\u5728\u52a8\u529b\u5b66\u7cfb\u7edf\u4e2d\u7684\u8868\u73b0\u5f62\u5f0f"}}
{"id": "2602.17677", "categories": ["cs.LG", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17677", "abs": "https://arxiv.org/abs/2602.17677", "authors": ["Sutej Kulgod", "Sean Ye", "Sanchit Tanwar", "Christoffer Heckman"], "title": "Reducing Text Bias in Synthetically Generated MCQAs for VLMs in Autonomous Driving", "comment": "7 pages, 2 figures", "summary": "Multiple Choice Question Answering (MCQA) benchmarks are an established standard for measuring Vision Language Model (VLM) performance in driving tasks. However, we observe the known phenomenon that synthetically generated MCQAs are highly susceptible to hidden textual cues that allow models to exploit linguistic patterns rather than visual context. Our results show that a VLM fine-tuned on such data can achieve accuracy comparable to human-validated benchmarks even without visual input. Our proposed method reduces blind accuracy from +66.9% above random to +2.9%, eliminating the vast majority of exploitable textual shortcuts. By decoupling the correct answer from linguistic artifacts and employing a curriculum learning strategy, we force the model to rely on visual grounding, ensuring that performance accurately reflects perceptual understanding.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9a7e\u9a76\u4efb\u52a1\u591a\u9879\u9009\u62e9\u9898\u6d4b\u8bd5\u4e2d\u5229\u7528\u6587\u672c\u6377\u5f84\u800c\u975e\u89c6\u89c9\u7406\u89e3\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u7b54\u6848\u4e0e\u8bed\u8a00\u4f2a\u5f71\u5e76\u7ed3\u5408\u8bfe\u7a0b\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5c06\u6a21\u578b\u65e0\u89c6\u89c9\u8f93\u5165\u7684\u51c6\u786e\u7387\u4ece+66.9%\u964d\u81f3+2.9%\uff0c\u8feb\u4f7f\u6a21\u578b\u4f9d\u8d56\u89c6\u89c9 grounding\uff0c\u786e\u4fdd\u6027\u80fd\u771f\u5b9e\u53cd\u6620\u611f\u77e5\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9a7e\u9a76\u4efb\u52a1\u7684\u591a\u9879\u9009\u62e9\u9898\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b58\u5728\u5408\u6210\u6570\u636e\u5305\u542b\u9690\u85cf\u6587\u672c\u7ebf\u7d22\u7684\u95ee\u9898\uff0c\u4f7f\u5f97\u6a21\u578b\u53ef\u901a\u8fc7\u8bed\u8a00\u6a21\u5f0f\u800c\u975e\u89c6\u89c9\u4e0a\u4e0b\u6587\u4f5c\u7b54\uff0c\u5bfc\u81f4\u8bc4\u4f30\u7ed3\u679c\u5931\u771f\u3002", "method": "\u63d0\u51fa\u89e3\u8026\u6b63\u786e\u7b54\u6848\u4e0e\u8bed\u8a00\u4f2a\u5f71\uff0c\u5e76\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u5f3a\u5236\u6a21\u578b\u4f9d\u8d56\u89c6\u89c9 grounding \u800c\u975e\u6587\u672c\u6377\u5f84\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u4e0a\u5fae\u8c03\u7684\u6a21\u578b\u5373\u4f7f\u65e0\u89c6\u89c9\u8f93\u5165\u4e5f\u80fd\u8fbe\u5230\u4eba\u7c7b\u9a8c\u8bc1\u57fa\u51c6\u7684\u51c6\u786e\u7387\uff1b\u65b0\u65b9\u6cd5\u5c06\u76f2\u76ee\u51c6\u786e\u7387\u4ece+66.9%\u964d\u81f3+2.9%\uff0c\u6d88\u9664\u4e86\u7edd\u5927\u591a\u6570\u53ef\u5229\u7528\u7684\u6587\u672c\u6377\u5f84\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u786e\u4fdd\u6a21\u578b\u6027\u80fd\u771f\u5b9e\u53cd\u6620\u611f\u77e5\u7406\u89e3\u80fd\u529b\uff0c\u800c\u975e\u5bf9\u8bed\u8a00\u6a21\u5f0f\u7684\u5229\u7528\u3002"}}
{"id": "2602.17872", "categories": ["cond-mat.str-el", "cond-mat.mes-hall", "physics.optics"], "pdf": "https://arxiv.org/pdf/2602.17872", "abs": "https://arxiv.org/abs/2602.17872", "authors": ["Weihang Lu", "Camron Farhang", "Yuchuan Yao", "Pratap Pal", "Hao Zhang", "Shaofeng Han", "Shi-Zeng Lin", "Chang-Beom Eom", "Jing Xia"], "title": "Direct imaging of a topological nematic phase in a spin-compensated magnet", "comment": null, "summary": "Density waves conventionally describe the periodic modulation of charge or spin, yet the spatial modulation of electronic topology has remained elusive. Here, we report the discovery of a Berry-curvature density wave in the noncollinear antiferromagnet Mn3NiN with compensated spins. Using high-precision Sagnac Kerr microscopy, we directly image micrometer-scale modulations of the Berry curvature. These topological ripples exhibit orientations unpinned to the crystal lattice, forming a nematic phase that spontaneously breaks rotational symmetry. We attribute this instability to field-induced spatial variations of the spin texture driven by competing magnetic interactions. This discovery unveils a new class of collective order in spin-compensated magnets arising from the geometric phase of the wavefunction itself and offering a tunable degree of freedom for topological spintronics based on antiferromagnets and altermagnets.", "AI": {"tldr": "\u5728\u53cd\u94c1\u78c1\u6750\u6599Mn3NiN\u4e2d\u53d1\u73b0\u4e86\u7531\u7535\u5b50\u6ce2\u51fd\u6570\u51e0\u4f55\u76f8\u4f4d\uff08\u8d1d\u91cc\u66f2\u7387\uff09\u8c03\u5236\u7684\u62d3\u6251\u5bc6\u5ea6\u6ce2\uff0c\u4e3a\u62d3\u6251\u81ea\u65cb\u7535\u5b50\u5b66\u63d0\u4f9b\u4e86\u65b0\u81ea\u7531\u5ea6\u3002", "motivation": "\u4f20\u7edf\u5bc6\u5ea6\u6ce2\u63cf\u8ff0\u7535\u8377\u6216\u81ea\u65cb\u7684\u5468\u671f\u6027\u8c03\u5236\uff0c\u4f46\u7535\u5b50\u62d3\u6251\u6027\u7684\u7a7a\u95f4\u8c03\u5236\u73b0\u8c61\u5c1a\u672a\u88ab\u63a2\u7d22\u3002", "method": "\u5229\u7528\u9ad8\u7cbe\u5ea6Sagnac\u514b\u5c14\u663e\u5fae\u955c\uff0c\u5728\u975e\u5171\u7ebf\u53cd\u94c1\u78c1\u4f53Mn3NiN\u4e2d\u76f4\u63a5\u89c2\u6d4b\u5fae\u7c73\u5c3a\u5ea6\u7684\u8d1d\u91cc\u66f2\u7387\u8c03\u5236\u3002", "result": "\u53d1\u73b0\u62d3\u6251\u6d9f\u6f2a\u53d6\u5411\u4e0d\u53d7\u6676\u683c\u675f\u7f1a\uff0c\u5f62\u6210\u81ea\u53d1\u6253\u7834\u65cb\u8f6c\u5bf9\u79f0\u6027\u7684\u5411\u5217\u76f8\uff1b\u8be5\u4e0d\u7a33\u5b9a\u6027\u6e90\u4e8e\u7ade\u4e89\u78c1\u76f8\u4e92\u4f5c\u7528\u9a71\u52a8\u7684 spin texture \u7a7a\u95f4\u53d8\u5316\u3002", "conclusion": "\u63ed\u793a\u4e86\u81ea\u65cb\u8865\u507f\u78c1\u4f53\u4e2d\u4e00\u7c7b\u6e90\u4e8e\u6ce2\u51fd\u6570\u51e0\u4f55\u76f8\u4f4d\u7684\u65b0\u96c6\u4f53\u5e8f\uff0c\u4e3a\u57fa\u4e8e\u53cd\u94c1\u78c1\u548c altermagnet \u7684\u62d3\u6251\u81ea\u65cb\u7535\u5b50\u5b66\u63d0\u4f9b\u4e86\u53ef\u8c03\u81ea\u7531\u5ea6\u3002"}}
{"id": "2602.17942", "categories": ["cs.CC"], "pdf": "https://arxiv.org/pdf/2602.17942", "abs": "https://arxiv.org/abs/2602.17942", "authors": ["Marco Carmosino", "Ngu Dang", "Tim Jackman"], "title": "Convergent Gate Elimination and Constructive Circuit Lower Bounds", "comment": null, "summary": "Towards better understanding of gate elimination, the only method known that can prove complexity lower bounds for explicit functions against unrestricted Boolean circuits, this work contributes: (1) formalizing circuit simplifications as a convergent term graph rewriting system and (2) giving a simple and constructive proof of a classical lower bound using this system.\n  First, we show that circuit simplification is a convergent term graph rewriting system over the DeMorgan and $\\{\\land, \\lor, \\oplus\\}$ bases. We define local rewriting rules from Boolean identities such that every simplification sequence yields an identical final result (up to circuit isomorphism or bisimulation). Convergence enables rigorous reasoning about structural properties of simplified circuits without dependence on the order of simplification. Then, we show that there is \\emph{no similar} convergent formalization of circuit simplification over the $U_2$ and $B_2$ bases.\n  Then, we use our simplification system to give a constructive circuit lower bound, generalizing Schnorr's classical result that the XOR function requires $3(n - 1)$ gates to compute in the DeMorgan basis. A constructive lower bound $f \\not\\in C$ gives an algorithm (called a \"refuter\") that efficiently finds counter-examples for every $C$-circuit trying to compute the function $f$. Chen, Jin, Santhanam, and Williams showed that constructivity plays a central role in many longstanding open problems about complexity theory (FOCS 2021), so it is natural to ask for constructive circuit lower bounds from gate elimination arguments. This demonstrates how using convergent simplification can lead to shorter and more modular proofs of circuit lower bounds. Furthermore, until this work, no constructive lower bound had been proved via gate elimination.", "AI": {"tldr": "\u672c\u6587\u5c06\u95e8\u6d88\u9664\u5f62\u5f0f\u5316\u4e3a\u5408\u6d41\u9879\u56fe\u91cd\u5199\u7cfb\u7edf\uff0c\u9996\u6b21\u5728DeMorgan\u57fa\u4e0b\u901a\u8fc7\u95e8\u6d88\u9664\u65b9\u6cd5\u8bc1\u660e\u4e86XOR\u51fd\u6570\u7684\u6784\u9020\u6027\u4e0b\u754c\uff0c\u4e3a\u7535\u8def\u590d\u6742\u6027\u4e0b\u754c\u8bc1\u660e\u63d0\u4f9b\u4e86\u66f4\u7b80\u6d01\u6a21\u5757\u5316\u7684\u8bc1\u660e\u6846\u67b6\u3002", "motivation": "\u95e8\u6d88\u9664\u662f\u8bc1\u660e\u663e\u5f0f\u51fd\u6570\u7535\u8def\u590d\u6742\u5ea6\u4e0b\u754c\u7684\u552f\u4e00\u5df2\u77e5\u65b9\u6cd5\u3002\u672c\u6587\u65e8\u5728\u6df1\u5165\u7406\u89e3\u95e8\u6d88\u9664\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u7535\u8def\u7b80\u5316\u8fc7\u7a0b\u5e76\u7ed9\u51fa\u6784\u9020\u6027\u4e0b\u754c\u8bc1\u660e\uff0c\u4ee5\u89e3\u51b3\u590d\u6742\u6027\u7406\u8bba\u4e2d\u7684\u957f\u671f\u5f00\u653e\u95ee\u9898\u3002", "method": "1) \u5c06\u7535\u8def\u7b80\u5316\u5efa\u6a21\u4e3a\u5408\u6d41\u9879\u56fe\u91cd\u5199\u7cfb\u7edf\uff0c\u57fa\u4e8e\u5e03\u5c14\u6052\u7b49\u5f0f\u5b9a\u4e49\u5c40\u90e8\u91cd\u5199\u89c4\u5219\uff1b2) \u8bc1\u660e\u8be5\u7cfb\u7edf\u5728DeMorgan\u548c{\u2227, \u2228, \u2295}\u57fa\u4e0b\u6536\u655b\uff0c\u4f46\u5728U\u2082\u548cB\u2082\u57fa\u4e0b\u65e0\u6cd5\u7c7b\u4f3c\u5f62\u5f0f\u5316\uff1b3) \u5e94\u7528\u8be5\u7cfb\u7edf\u6784\u9020\u6027\u5730\u63a8\u5e7fSchnorr\u7684XOR\u4e0b\u754c\u7ed3\u679c\u3002", "result": "1) \u5efa\u7acb\u4e86\u7279\u5b9a\u57fa\u4e0b\u7535\u8def\u7b80\u5316\u7684\u5408\u6d41\u5f62\u5f0f\u5316\u7cfb\u7edf\uff1b2) \u7ed9\u51fa\u4e86XOR\u51fd\u6570\u5728DeMorgan\u57fa\u4e0b\u9700\u89813(n-1)\u4e2a\u95e8\u7684\u6784\u9020\u6027\u4e0b\u754c\u8bc1\u660e\uff0c\u5e76\u9996\u6b21\u901a\u8fc7\u95e8\u6d88\u9664\u5b9e\u73b0\u6784\u9020\u6027\u4e0b\u754c\uff1b3) \u63ed\u793a\u4e86\u5408\u6d41\u7b80\u5316\u53ef\u4ea7\u751f\u66f4\u77ed\u3001\u66f4\u6a21\u5757\u5316\u7684\u4e0b\u754c\u8bc1\u660e\u3002", "conclusion": "\u5408\u6d41\u7b80\u5316\u7cfb\u7edf\u4e3a\u7535\u8def\u4e0b\u754c\u8bc1\u660e\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u6784\u9020\u6027\u4e0b\u754c\u5bf9\u590d\u6742\u6027\u7406\u8bba\u81f3\u5173\u91cd\u8981\u3002\u8be5\u5de5\u4f5c\u9996\u6b21\u901a\u8fc7\u95e8\u6d88\u9664\u83b7\u5f97\u6784\u9020\u6027\u4e0b\u754c\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.18091", "categories": ["cond-mat.stat-mech", "cond-mat.soft"], "pdf": "https://arxiv.org/pdf/2602.18091", "abs": "https://arxiv.org/abs/2602.18091", "authors": ["Filippo Faedi", "Erik Kalz", "Ralf Metzler", "Abhinav Sharma"], "title": "A mobility based approach to transport in chiral fluids", "comment": null, "summary": "Chiral fluids, for which the mobility tensor has antisymmetric, off-diagonal components, exhibit transport phenomena absent in conventional systems, including interaction-enhanced diffusion and negative mobility. While these effects have been predicted theoretically and observed in simulations, their microscopic origin has remained unclear. Here, we address this question using a mobility-based nonequilibrium approach, analysing the steady-state drift of a tracer driven through an interacting chiral fluid. We show that, under strong chirality, the tracer generates a reversed density wake, in which regions of particle accumulation and depletion are inverted compared to the achiral case. This structural inversion of the wake provides a unified physical mechanism underlying both enhanced diffusion and negative mobility. Furthermore, we demonstrate that these phenomena are robust to changes in the interaction potential, highlighting their generality as a consequence of odd mobility.", "AI": {"tldr": "\u7814\u7a76\u63ed\u793a\u624b\u6027\u6d41\u4f53\u4e2d\u8fd0\u52a8\u793a\u8e2a\u7c92\u5b50\u4f1a\u4ea7\u751f\u53cd\u5411\u5bc6\u5ea6\u5c3e\u8ff9\uff0c\u8fd9\u4e3a\u76f8\u4e92\u4f5c\u7528\u589e\u5f3a\u6269\u6563\u548c\u8d1f\u8fc1\u79fb\u7387\u63d0\u4f9b\u4e86\u7edf\u4e00\u673a\u5236\u3002", "motivation": "\u624b\u6027\u6d41\u4f53\u4e2d\u76f8\u4e92\u4f5c\u7528\u589e\u5f3a\u6269\u6563\u548c\u8d1f\u8fc1\u79fb\u7387\u7684\u5fae\u89c2\u8d77\u6e90\u5c1a\u4e0d\u660e\u786e\uff0c\u5c3d\u7ba1\u8fd9\u4e9b\u73b0\u8c61\u5df2\u88ab\u7406\u8bba\u9884\u6d4b\u548c\u6a21\u62df\u89c2\u6d4b\u5230\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8fc1\u79fb\u7387\u7684\u975e\u5e73\u8861\u6001\u65b9\u6cd5\uff0c\u5206\u6790\u793a\u8e2a\u7c92\u5b50\u5728\u76f8\u4e92\u4f5c\u7528\u624b\u6027\u6d41\u4f53\u4e2d\u7a33\u6001\u6f02\u79fb\u3002", "result": "\u5728\u5f3a\u624b\u6027\u6761\u4ef6\u4e0b\uff0c\u793a\u8e2a\u7c92\u5b50\u4ea7\u751f\u53cd\u5411\u5bc6\u5ea6\u5c3e\u8ff9\uff0c\u7c92\u5b50\u79ef\u7d2f\u4e0e\u8017\u5c3d\u533a\u57df\u76f8\u5bf9\u4e8e\u975e\u624b\u6027\u60c5\u51b5\u53d1\u751f\u53cd\u8f6c\uff0c\u8fd9\u4e3a\u589e\u5f3a\u6269\u6563\u548c\u8d1f\u8fc1\u79fb\u7387\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7269\u7406\u89e3\u91ca\u3002", "conclusion": "\u53cd\u5411\u5c3e\u8ff9\u7ed3\u6784\u662f\u624b\u6027\u6d41\u4f53\u5947\u6570\u8fc1\u79fb\u7387\u7684\u4e00\u822c\u6027\u7ed3\u679c\uff0c\u8fd9\u4e9b\u73b0\u8c61\u5bf9\u76f8\u4e92\u4f5c\u7528\u52bf\u7684\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5177\u6709\u666e\u9002\u6027\u3002"}}
{"id": "2602.17826", "categories": ["cs.AI", "cs.LG", "cs.SC"], "pdf": "https://arxiv.org/pdf/2602.17826", "abs": "https://arxiv.org/abs/2602.17826", "authors": ["Marcelo Labre"], "title": "Ontology-Guided Neuro-Symbolic Inference: Grounding Language Models with Mathematical Domain Knowledge", "comment": "Submitted to NeuS 2026. Supplementary materials and code: https://doi.org/10.5281/zenodo.18665030", "summary": "Language models exhibit fundamental limitations -- hallucination, brittleness, and lack of formal grounding -- that are particularly problematic in high-stakes specialist fields requiring verifiable reasoning. I investigate whether formal domain ontologies can enhance language model reliability through retrieval-augmented generation. Using mathematics as proof of concept, I implement a neuro-symbolic pipeline leveraging the OpenMath ontology with hybrid retrieval and cross-encoder reranking to inject relevant definitions into model prompts. Evaluation on the MATH benchmark with three open-source models reveals that ontology-guided context improves performance when retrieval quality is high, but irrelevant context actively degrades it -- highlighting both the promise and challenges of neuro-symbolic approaches.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u5982\u4f55\u5229\u7528\u6570\u5b66\u672c\u4f53\uff08OpenMath\uff09\u4e0e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u53d1\u73b0\u9ad8\u8d28\u91cf\u68c0\u7d22\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u4f4e\u8d28\u91cf\u68c0\u7d22\u53cd\u800c\u4f1a\u635f\u5bb3\u8868\u73b0\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u5e7b\u89c9\u3001\u8106\u5f31\u6027\u548c\u7f3a\u4e4f\u5f62\u5f0f\u5316 grounding \u7b49\u6839\u672c\u5c40\u9650\uff0c\u5728\u9ad8\u98ce\u9669\u4e13\u4e1a\u9886\u57df\u5c24\u4e3a\u7a81\u51fa\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u5f62\u5f0f\u5316\u9886\u57df\u672c\u4f53\u662f\u5426\u53ef\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u63d0\u5347\u6a21\u578b\u53ef\u9760\u6027\u3002", "method": "\u4ee5\u6570\u5b66\u4e3a\u9a8c\u8bc1\u9886\u57df\uff0c\u6784\u5efa\u795e\u7ecf\u7b26\u53f7\u6d41\u6c34\u7ebf\uff1a\u5229\u7528 OpenMath \u672c\u4f53\u8fdb\u884c\u6df7\u5408\u68c0\u7d22\uff0c\u5e76\u901a\u8fc7\u4ea4\u53c9\u7f16\u7801\u5668\u91cd\u6392\u5e8f\uff0c\u5c06\u76f8\u5173\u5b9a\u4e49\u6ce8\u5165\u6a21\u578b\u63d0\u793a\u3002\u5728 MATH \u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8bc4\u4f30\u4e09\u79cd\u5f00\u6e90\u6a21\u578b\u3002", "result": "\u672c\u4f53\u5f15\u5bfc\u7684\u4e0a\u4e0b\u6587\u5728\u68c0\u7d22\u8d28\u91cf\u9ad8\u65f6\u53ef\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u65e0\u5173\u4e0a\u4e0b\u6587\u4f1a\u663e\u8457\u964d\u4f4e\u6a21\u578b\u8868\u73b0\u3002", "conclusion": "\u795e\u7ecf\u7b26\u53f7\u5b66\u65b9\u6cd5\u5728\u63d0\u5347\u6a21\u578b\u53ef\u9760\u6027\u4e0a\u65e2\u6709\u524d\u666f\u4e5f\u9762\u4e34\u6311\u6218\uff0c\u51f8\u663e\u4e86\u68c0\u7d22\u8d28\u91cf\u7684\u5173\u952e\u4f5c\u7528\u4e0e\u65b9\u6cd5\u7684\u6f5c\u5728\u5c40\u9650\u3002"}}
{"id": "2602.17810", "categories": ["nlin.CD"], "pdf": "https://arxiv.org/pdf/2602.17810", "abs": "https://arxiv.org/abs/2602.17810", "authors": ["Edson D. Leonel", "Mayla A. M. de Almeida", "Juan Pedro Tarigo", "Arturo C. Marti", "Diego F. M. Oliveira"], "title": "Describing a Universal Critical Behavior in a transition from order to chaos", "comment": null, "summary": "We present a comprehensive discussion of a transition from integrability to non-integrability in an oval billiard with a static boundary. This transition is controlled by a deformation parameter $\u03b5$, which modifies the boundary shape from circular, corresponding to $\u03b5=0$ and an integrable dynamics, to oval for $\u03b5\\neq 0$, where non-integrability emerges. The deformation of the circular billiard gives rise to a chaotic layer that develops along a well-defined stripe in phase space. By introducing a set of transformations that isolate this chaotic stripe, we characterise the diffusive spreading of ensembles of trajectories and identify an observable, $\u03c9_{rms,{\\rm sat}}$, which plays the role of an order parameter for the transition. For small deformations, the saturation value of the diffusion obeys the scaling law $\u03c9_{rms,{\\rm sat}}\\propto\u03b5^{\\tilde\u03b1}$, with a critical exponent $\\tilde\u03b1=0.507(2)$, vanishing continuously as $\u03b5\\rightarrow 0$. The associated susceptibility, $\u03c7=d\u03c9_{rms,{\\rm sat}}/d\u03b5$, diverges in the same limit, signalling the presence of critical behavior analogous to that observed in second-order (continuous) phase transitions in statistical mechanics.", "AI": {"tldr": "\u7814\u7a76 oval \u53f0\u7403\u7cfb\u7edf\u4e2d\u53ef\u79ef\u6027\u5230\u4e0d\u53ef\u79ef\u6027\u7684\u8f6c\u53d8\uff0c\u53d1\u73b0\u5f62\u53d8\u53c2\u6570 \u03b5 \u63a7\u5236\u76f8\u53d8\u8fc7\u7a0b\uff0c\u5b58\u5728\u7c7b\u4f3c\u4e8c\u7ea7\u76f8\u53d8\u7684\u4e34\u754c\u884c\u4e3a", "motivation": "\u63a2\u7a76\u53ef\u79ef\u7cfb\u7edf\u5411\u4e0d\u53ef\u79ef\u7cfb\u7edf\u8f6c\u53d8\u7684\u52a8\u529b\u5b66\u673a\u5236\uff0c\u7279\u522b\u5173\u6ce8\u76f8\u7a7a\u95f4\u4e2d\u6df7\u6c8c\u5c42\u7684\u5f62\u6210\u548c\u6269\u6563\u7279\u6027", "method": "\u901a\u8fc7\u8fb9\u754c\u5f62\u53d8\u53c2\u6570 \u03b5 \u8c03\u63a7\u53f0\u7403\u5f62\u72b6\uff0c\u7ed3\u5408\u76f8\u7a7a\u95f4\u6761\u7eb9\u9694\u79bb\u53d8\u6362\uff0c\u5206\u6790\u8f68\u8ff9\u6269\u6563\u548c\u4e34\u754c\u6807\u5ea6\u5f8b", "result": "\u53d1\u73b0\u6df7\u6c8c\u5c42\u6cbf\u76f8\u7a7a\u95f4\u6761\u7eb9\u53d1\u5c55\uff0c\u89c2\u6d4b\u5230 \u03c9_rms,sat \u4f5c\u4e3a\u5e8f\u53c2\u91cf\u6ee1\u8db3 \u03b5^0.507(2) \u6807\u5ea6\u5f8b\uff0c\u4e14\u78c1\u5316\u7387\u53d1\u6563", "conclusion": "\u8bc1\u5b9e\u8be5\u8f6c\u53d8\u5177\u6709\u7edf\u8ba1\u529b\u5b66\u4e2d\u4e8c\u7ea7\u76f8\u53d8\u7684\u4e34\u754c\u7279\u5f81\uff0c\u4e3a\u52a8\u529b\u5b66\u7cfb\u7edf\u76f8\u53d8\u7814\u7a76\u63d0\u4f9b\u65b0\u8303\u4f8b"}}
{"id": "2602.17679", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.17679", "abs": "https://arxiv.org/abs/2602.17679", "authors": ["Saksham Kiroriwal", "Julius Pfrommer", "J\u00fcrgen Beyerer"], "title": "Joint Parameter and State-Space Bayesian Optimization: Using Process Expertise to Accelerate Manufacturing Optimization", "comment": "This paper is under review and has been submitted for CIRP CMS 2026", "summary": "Bayesian optimization (BO) is a powerful method for optimizing black-box manufacturing processes, but its performance is often limited when dealing with high-dimensional multi-stage systems, where we can observe intermediate outputs. Standard BO models the process as a black box and ignores the intermediate observations and the underlying process structure. Partially Observable Gaussian Process Networks (POGPN) model the process as a Directed Acyclic Graph (DAG). However, using intermediate observations is challenging when the observations are high-dimensional state-space time series. Process-expert knowledge can be used to extract low-dimensional latent features from the high-dimensional state-space data. We propose POGPN-JPSS, a framework that combines POGPN with Joint Parameter and State-Space (JPSS) modeling to use intermediate extracted information. We demonstrate the effectiveness of POGPN-JPSS on a challenging, high-dimensional simulation of a multi-stage bioethanol production process. Our results show that POGPN-JPSS significantly outperforms state-of-the-art methods by achieving the desired performance threshold twice as fast and with greater reliability. The fast optimization directly translates to substantial savings in time and resources. This highlights the importance of combining expert knowledge with structured probabilistic models for rapid process maturation.", "AI": {"tldr": "Proposes POGPN-JPSS framework integrating expert knowledge with structured probabilistic models to optimize high-dimensional multi-stage processes by extracting low-dimensional features from intermediate time-series observations, doubling optimization speed in bioethanol production simulation.", "motivation": "Standard Bayesian optimization ignores intermediate observations in high-dimensional multi-stage systems, while existing POGPN models struggle with high-dimensional state-space time series data, limiting optimization efficiency.", "method": "Combines Partially Observable Gaussian Process Networks (POGPN) with Joint Parameter and State-Space (JPSS) modeling, using process-expert knowledge to extract low-dimensional latent features from high-dimensional intermediate time-series data within a Directed Acyclic Graph (DAG) structure.", "result": "POGPN-JPSS achieved the target performance threshold twice as fast as state-of-the-art methods with greater reliability in a multi-stage bioethanol production simulation, demonstrating significant time/resource savings.", "conclusion": "Integrating domain expertise with structured probabilistic models (POGPN-JPSS) enables rapid optimization of complex manufacturing processes by effectively leveraging intermediate observations, accelerating process maturation."}}
{"id": "2602.17906", "categories": ["cond-mat.str-el", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2602.17906", "abs": "https://arxiv.org/abs/2602.17906", "authors": ["Hongtao Yan", "Chun-Chih Tseng", "Anzhuoer Li", "Manish Kumar", "Kaile Wang", "Shizai Chu", "Kenji Watanabe", "Takashi Taniguchi", "Allan H. MacDonald", "Matthew Yankowitz", "Keji Lai"], "title": "Microwave Imaging of Edge Conductivity in Graphene at Charge Neutrality and Quantum Hall States", "comment": null, "summary": "We report local conductivity imaging of edge states in monolayer graphene by millikelvin microwave impedance microscopy (MIM). At the charge-neutrality point, as the magnetic field increases, the local conductivity at the edge drops to zero more slowly than in the bulk. This behavior is consistent with the calculated spatial profile of the charge gap in the canted antiferromagnetic phase. For comparison, we also perform microwave imaging of integer quantum Hall states away from neutrality, which host dissipationless chiral edge channels. The evolution of the edge signal as a function of the bulk gap is fundamentally different between the Landau level filling factor $\u03bd= 0$ and $|\u03bd| \\ge 1$ integer quantum Hall states, which can be qualitatively explained by numerical simulations and theoretical analysis. Our results provide a comprehensive microscopic picture of the edge and bulk states as the Fermi level moves across the unique Landau-level spectrum of graphene.", "AI": {"tldr": "\u5229\u7528\u6beb\u5f00\u5c14\u6587\u5fae\u6ce2\u963b\u6297\u663e\u5fae\u955c(MIM)\u5bf9\u5355\u5c42\u77f3\u58a8\u70ef\u8fb9\u7f18\u6001\u8fdb\u884c\u5c40\u57df\u7535\u5bfc\u6210\u50cf\uff0c\u63ed\u793a\u7535\u8377\u4e2d\u6027\u70b9\u5904\u8fb9\u7f18\u7535\u5bfc\u968f\u78c1\u573a\u53d8\u5316\u7684\u7279\u6b8a\u884c\u4e3a\u53ca\u4e0d\u540c\u91cf\u5b50\u970d\u5c14\u6001\u7684\u8fb9\u7f18\u4fe1\u53f7\u5dee\u5f02", "motivation": "\u63a2\u7a76\u77f3\u58a8\u70ef\u5728\u78c1\u573a\u4e0b\u8fb9\u7f18\u6001\u4e0e\u4f53\u6001\u7684\u5fae\u89c2\u7535\u5bfc\u7279\u6027\u5dee\u5f02\uff0c\u7279\u522b\u662f\u5728\u7535\u8377\u4e2d\u6027\u70b9\u9644\u8fd1\u7684\u5947\u5f02\u91cf\u5b50\u76f8\uff08\u5982canted antiferromagnetic\u76f8\uff09\u4ee5\u53ca\u4e0d\u540c\u6574\u6570\u91cf\u5b50\u970d\u5c14\u6001\u4e2d\u624b\u6027\u8fb9\u7f18\u901a\u9053\u7684\u884c\u4e3a\u5dee\u5f02", "method": "\u91c7\u7528\u6beb\u5f00\u5c14\u6587\u6e29\u533a\u7684\u5fae\u6ce2\u963b\u6297\u663e\u5fae\u955c(MIM)\u6280\u672f\uff0c\u5728\u8d85\u9ad8\u771f\u7a7a\u548c\u5f3a\u78c1\u573a\u73af\u5883\u4e0b\u5bf9\u5355\u5c42\u77f3\u58a8\u70ef\u8fdb\u884c\u7eb3\u7c73\u5c3a\u5ea6\u5c40\u57df\u7535\u5bfc\u6210\u50cf\uff0c\u5e76\u7ed3\u5408\u6570\u503c\u6a21\u62df\u4e0e\u7406\u8bba\u5206\u6790", "result": "1) \u7535\u8377\u4e2d\u6027\u70b9\u5904\u8fb9\u7f18\u7535\u5bfc\u968f\u78c1\u573a\u589e\u52a0\u6bd4\u4f53\u6001\u66f4\u7f13\u6162\u5730\u964d\u81f3\u96f6\uff0c\u7b26\u5408canted antiferromagnetic\u76f8\u7684\u7535\u8377\u80fd\u9699\u7a7a\u95f4\u5206\u5e03\uff1b2) \u03bd=0\u4e0e|\u03bd|\u22651\u6574\u6570\u91cf\u5b50\u970d\u5c14\u6001\u7684\u8fb9\u7f18\u4fe1\u53f7\u968f\u4f53\u6001\u80fd\u9699\u6f14\u5316\u5b58\u5728\u672c\u8d28\u5dee\u5f02\uff0c\u524d\u8005\u8fb9\u7f18\u7535\u5bfc\u6301\u7eed\u5b58\u5728\u800c\u540e\u8005\u5448\u73b0\u8017\u6563less\u624b\u6027\u901a\u9053\u7279\u5f81", "conclusion": "\u5b9e\u9a8c\u63d0\u4f9b\u4e86\u77f3\u58a8\u70ef\u8d39\u7c73\u80fd\u7ea7\u7a7f\u8d8a\u72ec\u7279\u6717\u9053\u80fd\u7ea7\u8c31\u65f6\u8fb9\u7f18\u6001\u4e0e\u4f53\u6001\u6f14\u5316\u7684\u5fae\u89c2\u56fe\u50cf\uff0c\u8bc1\u5b9e\u4e86\u4e0d\u540c\u91cf\u5b50\u970d\u5c14\u76f8\u4e2d\u8fb9\u7f18\u4f20\u8f93\u673a\u5236\u7684\u6839\u672c\u533a\u522b\uff0c\u4e3a\u7406\u89e3\u77f3\u58a8\u70ef\u91cf\u5b50\u7269\u6001\u63d0\u4f9b\u4e86\u76f4\u63a5\u5b9e\u9a8c\u4f9d\u636e"}}
{"id": "2602.18240", "categories": ["cs.CC", "cs.DM", "cs.LO"], "pdf": "https://arxiv.org/pdf/2602.18240", "abs": "https://arxiv.org/abs/2602.18240", "authors": ["Colin Geniet", "Ali\u00e9nor Goubault-Larrecq", "K\u00e9vin Perrot"], "title": "Complexity lower bounds for succinct binary structures of bounded clique-width with restrictions", "comment": null, "summary": "We present a Rice-like complexity lower bound for any MSO-definable problem on binary structures succinctly encoded by circuits. This work extends the framework recently developed as a counterpoint to Courcelle's theorem for graphs encoded by circuits, in two interplaying directions: (1) by allowing multiple binary relations, and (2) by restricting the interpretation of new symbols. Depending on the pair of an MSO problem $\u03c8$ and an MSO restriction $\u03c7$, the problem is proven to be NP-hard or coNP-hard or P-hard, as long as $\u03c8$ is non-trivial on structures satisfying $\u03c7$ with bounded clique-width. Indeed, there are P-complete problems (for logspace reductions) in our extended context. Finally, we strengthen a previous result on the necessity to parameterize the notion of non-triviality, hence supporting the choice of clique-width.", "AI": {"tldr": "\u672c\u6587\u4e3a\u7535\u8def\u7f16\u7801\u7684\u4e8c\u5143\u7ed3\u6784\u4e0a\u7684MSO\u53ef\u5b9a\u4e49\u95ee\u9898\u5efa\u7acb\u4e86\u7c7bRice\u590d\u6742\u5ea6\u4e0b\u754c\uff0c\u6269\u5c55\u4e86Courcelle\u5b9a\u7406\u7684\u5bf9\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u5141\u8bb8\u591a\u4e2a\u4e8c\u5143\u5173\u7cfb\u548c\u9650\u5236\u65b0\u7b26\u53f7\u89e3\u91ca\uff0c\u8bc1\u660e\u5728\u03c8\u4e8e\u6ee1\u8db3\u03c7\u7684\u6709\u754c\u56e2\u5bbd\u5ea6\u7ed3\u6784\u4e0a\u975e\u5e73\u51e1\u7684\u6761\u4ef6\u4e0b\uff0c\u95ee\u9898\u5177\u6709NP-hard\u3001coNP-hard\u6216P-hard\u6027\uff0c\u5e76\u52a0\u5f3a\u4e86\u5bf9\u975e\u5e73\u51e1\u6027\u53c2\u6570\u5316\u5fc5\u8981\u6027\u7684\u8bba\u8bc1\uff0c\u652f\u6301\u4f7f\u7528\u56e2\u5bbd\u5ea6\u3002", "motivation": "\u63a8\u5e7f\u6700\u8fd1\u63d0\u51fa\u7684\u9488\u5bf9\u7535\u8def\u7f16\u7801\u56fe\u7684\u590d\u6742\u5ea6\u4e0b\u754c\u6846\u67b6\uff08\u4f5c\u4e3aCourcelle\u5b9a\u7406\u7684\u5bf9\u5076\uff09\uff0c\u901a\u8fc7\u5141\u8bb8\u591a\u4e2a\u4e8c\u5143\u5173\u7cfb\u548c\u9650\u5236\u7b26\u53f7\u89e3\u91ca\uff0c\u4ee5\u83b7\u5f97\u66f4\u5168\u9762\u7684\u56f0\u96be\u6027\u7ed3\u679c\u3002", "method": "\u5f00\u53d1\u7406\u8bba\u6846\u67b6\uff0c\u5206\u6790\u7535\u8def\u7f16\u7801\u4e8c\u5143\u7ed3\u6784\u4e0a\u5e26MSO\u9650\u5236\u03c7\u7684MSO\u53ef\u5b9a\u4e49\u95ee\u9898\u03c8\uff0c\u57fa\u4e8e\u03c8\u5728\u6709\u754c\u56e2\u5bbd\u5ea6\u7ed3\u6784\u4e0a\u7684\u975e\u5e73\u51e1\u6027\u8bc1\u660e\u590d\u6742\u5ea6\u4e0b\u754c\u3002", "result": "\u5bf9\u4efb\u610fMSO\u95ee\u9898\u03c8\u548c\u9650\u5236\u03c7\uff0c\u5f53\u03c8\u5728\u6ee1\u8db3\u03c7\u7684\u6709\u754c\u56e2\u5bbd\u5ea6\u7ed3\u6784\u4e0a\u975e\u5e73\u51e1\u65f6\uff0c\u95ee\u9898\u88ab\u8bc1\u660e\u662fNP-hard\u3001coNP-hard\u6216P-hard\uff08\u5b58\u5728P-\u5b8c\u5168\u95ee\u9898\uff09\uff1b\u540c\u65f6\u52a0\u5f3a\u4e86\u5173\u4e8e\u53c2\u6570\u5316\u975e\u5e73\u51e1\u6027\u5fc5\u8981\u6027\u7684\u5148\u524d\u7ed3\u679c\uff0c\u8bba\u8bc1\u4e86\u4f7f\u7528\u56e2\u5bbd\u5ea6\u7684\u5408\u7406\u6027\u3002", "conclusion": "\u5de5\u4f5c\u6210\u529f\u6269\u5c55\u4e86\u7535\u8def\u7f16\u7801\u7684\u590d\u6742\u5ea6\u4e0b\u754c\u6846\u67b6\uff0c\u8bc1\u660e\u5728\u5404\u79cdMSO\u95ee\u9898\u4e0b\u56f0\u96be\u6027\u6301\u7eed\u5b58\u5728\uff0c\u5e76\u4ece\u7406\u8bba\u4e0a\u9a8c\u8bc1\u4e86\u56e2\u5bbd\u5ea6\u4f5c\u4e3a\u5173\u952e\u53c2\u6570\u7684\u9009\u62e9\u3002"}}
{"id": "2602.18099", "categories": ["cond-mat.stat-mech", "hep-th", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.18099", "abs": "https://arxiv.org/abs/2602.18099", "authors": ["Gioele Zambotti", "Erik Tonni"], "title": "A contour for the entanglement negativity of bosonic Gaussian states", "comment": "62 pages, 14 figures", "summary": "We construct a contour function for the logarithmic negativity and the logarithm of the moments of the partial transpose of the reduced density matrix for multimode bosonic Gaussian states of a free lattice model. In one spatial dimension, numerical results are obtained for harmonic chains either in the ground state or at finite temperature, by considering, respectively, either a subsystem made by two adjacent or disjoint blocks on the line or a bipartition of the circle. The contour function of the logarithmic negativity diverges only at the entangling points, while the contour function for the logarithm of the moments of the partial transpose is divergent also at the boundary of the bipartite subsystem, as functions of the position. In a two-dimensional conformal field theory, analytic expressions that describe these divergencies are discussed. In one spatial dimension, we explore the partial derivative of the logarithmic negativity of two adjacent intervals with respect to the logarithm of the harmonic ratio of their lengths while their ratio and the other parameters are kept fixed. Considering the ground state of the harmonic chain on the line and in the massive regime, we report numerical results showing that this quantity displays a monotonically decreasing behaviour.", "AI": {"tldr": "This paper constructs contour functions for entanglement measures (logarithmic negativity and partial transpose moments) in bosonic Gaussian states, revealing distinct divergence patterns at entangling points and boundaries through 1D numerical simulations and 2D CFT analytical results.", "motivation": "To characterize the spatial distribution and divergence structure of entanglement measures in multimode bosonic quantum many-body systems, particularly Gaussian states, using contour functions that reveal how entanglement varies across subsystem partitions.", "method": "Constructs contour functions for logarithmic negativity and log-moments of partial transpose; performs numerical calculations on 1D harmonic chains (ground state/finite temperature) with various subsystem geometries; derives analytic expressions for 2D conformal field theory; analyzes partial derivatives with respect to harmonic ratio.", "result": "Logarithmic negativity contour diverges only at entangling points; partial transpose moments contour also diverges at subsystem boundaries; 2D CFT analytic formulas describe these divergences; in 1D massive ground state, the derivative of negativity with respect to log harmonic ratio displays monotonic decrease.", "conclusion": "The contour functions provide a refined spatial map of entanglement structure, with different divergence signatures for various measures, offering deeper understanding of entanglement distribution in quantum many-body systems and confirming monotonic scaling properties in 1D models."}}
{"id": "2602.17831", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17831", "abs": "https://arxiv.org/abs/2602.17831", "authors": ["Simon Henniger", "Gabriel Poesia"], "title": "The Token Games: Evaluating Language Model Reasoning with Puzzle Duels", "comment": "Project website: https://token-games.ai/", "summary": "Evaluating the reasoning capabilities of Large Language Models is increasingly challenging as models improve. Human curation of hard questions is highly expensive, especially in recent benchmarks using PhD-level domain knowledge to challenge the most capable models. Even then, there is always a concern about whether these questions test genuine reasoning or if similar problems have been seen during training. Here, we take inspiration from 16th-century mathematical duels to design The Token Games (TTG): an evaluation framework where models challenge each other by creating their own puzzles. We leverage the format of Programming Puzzles - given a Python function that returns a boolean, find inputs that make it return True - to flexibly represent problems and enable verifying solutions. Using results from pairwise duels, we then compute Elo ratings, allowing us to compare models relative to each other. We evaluate 10 frontier models on TTG, and closely match the ranking from existing benchmarks such as Humanity's Last Exam, without involving any human effort in creating puzzles. We also find that creating good puzzles is still a highly challenging task for current models, not measured by previous benchmarks. Overall, our work suggests new paradigms for evaluating reasoning that cannot be saturated by design, and that allow testing models for other skills like creativity and task creation alongside problem solving.", "AI": {"tldr": "\u63d0\u51fa\"\u4ee4\u724c\u6e38\u620f\"(TTG)\u8bc4\u4f30\u6846\u67b6\uff0c\u8ba9\u5927\u6a21\u578b\u901a\u8fc7\u4e92\u76f8\u751f\u6210\u7f16\u7a0b\u8c1c\u9898\uff08Python\u5e03\u5c14\u51fd\u6570\u6c42\u8f93\u5165\uff09\u8fdb\u884c\u5bf9\u6297\uff0c\u5229\u7528Elo\u8bc4\u5206\u6392\u540d\uff0c\u65e0\u9700\u4eba\u5de5\u51fa\u9898\u5373\u53ef\u5339\u914d\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u6392\u540d\uff0c\u5e76\u63ed\u793a\u6a21\u578b\u521b\u9020\u4f18\u8d28\u8c1c\u9898\u7684\u80fd\u529b\u4ecd\u4e0d\u8db3", "motivation": "\u73b0\u6709\u5927\u6a21\u578b\u63a8\u7406\u8bc4\u4f30\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\u9ad8\u96be\u5ea6\u9898\u76ee\uff08\u5982\u535a\u58eb\u7ea7\u77e5\u8bc6\uff09\uff0c\u6210\u672c\u9ad8\u6602\u4e14\u5b58\u5728\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\u98ce\u9669\uff0c\u96be\u4ee5\u533a\u5206\u771f\u5b9e\u63a8\u7406\u80fd\u529b\u4e0e\u8bb0\u5fc6\u590d\u73b0", "method": "\u53d716\u4e16\u7eaa\u6570\u5b66\u51b3\u6597\u542f\u53d1\uff0c\u6784\u5efa\u6a21\u578b\u4e92\u751f\u6210\u8c1c\u9898\u7684\u5bf9\u6297\u6846\u67b6\uff1a\u4ee5\"\u7ed9\u5b9a\u8fd4\u56de\u5e03\u5c14\u503c\u7684Python\u51fd\u6570\uff0c\u6c42\u4f7f\u5176\u8fd4\u56deTrue\u7684\u8f93\u5165\"\u4e3a\u8c1c\u9898\u5f62\u5f0f\uff0c\u901a\u8fc7\u6a21\u578b\u95f4\u5bf9\u51b3\u8ba1\u7b97Elo\u8bc4\u5206\u8fdb\u884c\u80fd\u529b\u6392\u5e8f", "result": "10\u4e2a\u524d\u6cbf\u6a21\u578b\u5728TTG\u4e2d\u7684\u6392\u540d\u4e0eHumanity's Last Exam\u7b49\u57fa\u51c6\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4f46\u6a21\u578b\u81ea\u4e3b\u751f\u6210\u9ad8\u8d28\u91cf\u8c1c\u9898\u4ecd\u5177\u6311\u6218\u6027\uff0c\u6b64\u80fd\u529b\u672a\u88ab\u4f20\u7edf\u57fa\u51c6\u6d4b\u91cf", "conclusion": "\u5f00\u521b\u6297\u9971\u548c\u7684\u8bc4\u4f30\u65b0\u8303\u5f0f\uff0c\u540c\u6b65\u68c0\u9a8c\u95ee\u9898\u89e3\u51b3\u3001\u521b\u9020\u529b\u4e0e\u4efb\u52a1\u8bbe\u8ba1\u80fd\u529b\uff0c\u907f\u514d\u4eba\u5de5\u57fa\u51c6\u7684\u5c40\u9650\u6027"}}
{"id": "2602.17775", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.17775", "abs": "https://arxiv.org/abs/2602.17775", "authors": ["Arend-Jan Quist", "Tim Coopmans", "Alfons Laarman"], "title": "Exact quantum decision diagrams with scaling guarantees for Clifford+$T$ circuits and beyond", "comment": null, "summary": "A decision diagram (DD) is a graph-like data structure for homomorphic compression of Boolean and pseudo-Boolean functions. Over the past decades, decision diagrams have been successfully applied to verification, linear algebra, stochastic reasoning, and quantum circuit analysis. Floating-point errors have, however, significantly slowed down practical implementations of real- and complex-valued decision diagrams. In the context of quantum computing, attempts to mitigate this numerical instability have thus far lacked theoretical scaling guarantees and have had only limited success in practice. Here, we focus on the analysis of quantum circuits consisting of Clifford gates and $T$ gates (a common universal gate set). We first hand-craft an algebraic representation for complex numbers, which replace the floating point coefficients in a decision diagram. Then, we prove that the sizes of these algebraic representations are linearly bounded in the number of $T$ gates and qubits, and constant in the number of Clifford gates. Furthermore, we prove that both the runtime and the number of nodes of decision diagrams are upper bounded as $2^t \\cdot poly(g, n)$, where $t$ ($g$) is the number of $t$ gates (Clifford gates) and $n$ the number of qubits. Our proofs are based on a $T$-count dependent characterization of the density matrix entries of quantum states produced by circuits with Clifford+$T$ gates, and uncover a connection between a quantum state's stabilizer nullity and its decision diagram width. With an open source implementation, we demonstrate that our exact method resolves the inaccuracies occurring in floating-point-based counterparts and can outperform them due to lower node counts. Our contributions are, to the best of our knowledge, the first scaling guarantees on the runtime of (exact) quantum decision diagram simulation for a universal gate set.", "AI": {"tldr": "\u63d0\u51fa\u4ee3\u6570\u8868\u793a\u66ff\u4ee3\u6d6e\u70b9\u6570\u89e3\u51b3\u91cf\u5b50\u51b3\u7b56\u56fe\u7684\u6570\u503c\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u9996\u6b21\u5728Clifford+T\u901a\u7528\u91cf\u5b50\u95e8\u96c6\u4e0a\u7ed9\u51fa\u7cbe\u786e\u6a21\u62df\u7684\u8fd0\u884c\u65f6\u548c\u89c4\u6a21\u7684\u7406\u8bba\u7f29\u653e\u4fdd\u8bc1", "motivation": "\u6d6e\u70b9\u8bef\u5dee\u4e25\u91cd\u963b\u788d\u4e86\u5b9e\u6570/\u590d\u6570\u51b3\u7b56\u56fe\u5728\u91cf\u5b50\u7535\u8def\u6a21\u62df\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u73b0\u6709\u7f13\u89e3\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u7f29\u653e\u4fdd\u8bc1\u4e14\u5b9e\u8df5\u6548\u679c\u6709\u9650", "method": "1) \u4e3a\u590d\u6570\u8bbe\u8ba1\u7279\u5236\u4ee3\u6570\u8868\u793a\u66ff\u4ee3\u51b3\u7b56\u56fe\u4e2d\u7684\u6d6e\u70b9\u7cfb\u6570\uff1b2) \u57fa\u4e8eT\u95e8\u6570\u91cf\u523b\u753bClifford+T\u7535\u8def\u4ea7\u751f\u7684\u91cf\u5b50\u6001\u5bc6\u5ea6\u77e9\u9635\u9879\uff1b3) \u63ed\u793a\u91cf\u5b50\u6001\u7a33\u5b9a\u5b50\u96f6\u5316\u5ea6\u4e0e\u51b3\u7b56\u56fe\u5bbd\u5ea6\u7684\u5173\u8054", "result": "1) \u4ee3\u6570\u8868\u793a\u89c4\u6a21\u968fT\u95e8\u6570\u548c\u91cf\u5b50\u6bd4\u7279\u6570\u7ebf\u6027\u589e\u957f\uff0c\u4e0eClifford\u95e8\u6570\u65e0\u5173\uff1b2) \u8fd0\u884c\u65f6\u548c\u8282\u70b9\u6570\u4e0a\u754c\u4e3a2^t\u00b7poly(g,n)\uff1b3) \u5f00\u6e90\u5b9e\u73b0\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u6d88\u9664\u6d6e\u70b9\u8bef\u5dee\u4e14\u8282\u70b9\u66f4\u5c11", "conclusion": "\u9996\u6b21\u4e3a\u901a\u7528\u91cf\u5b50\u95e8\u96c6\u7684\u7cbe\u786e\u51b3\u7b56\u56fe\u6a21\u62df\u63d0\u4f9b\u8fd0\u884c\u65f6\u7f29\u653e\u4fdd\u8bc1\uff0c\u89e3\u51b3\u4e86\u6d6e\u70b9\u65b9\u6cd5\u7684\u4e0d\u51c6\u786e\u6027\uff0c\u4e3a\u91cf\u5b50\u7535\u8def\u9ad8\u6548\u6a21\u62df\u63d0\u4f9b\u65b0\u7406\u8bba\u57fa\u7840"}}
{"id": "2602.17836", "categories": ["nlin.CD"], "pdf": "https://arxiv.org/pdf/2602.17836", "abs": "https://arxiv.org/abs/2602.17836", "authors": ["Bonaventure Nana", "Krystian Polczy\u0144ski", "Paul Woafo", "Jan Awrejcewicz"], "title": "Nonlinear dynamics of a vertical pendulum driven by magnetic field provided by two coils magnets: analytical, numerical and experimental studies", "comment": "25 pages, 21 figures", "summary": "In the present work, we analyzed theoretically and experimentally the nonlinear dynamics of a magnetic pendulum excited through the interactions of a strong neodymium magnet and two coils placed symmetrically around the zero angular position. The forces between the magnet and coils and generated torques acting on the pendulum are derived using the magnetic charges interaction model and an experimentally fitted model. System equilibrium points are obtained, and their stability is investigated. It is found that when the currents in two coils are negative, the shape of the mechanical potential is bistable. The bistable potential might be symmetric if the currents have the same values and asymmetric when they are different. Asymmetric bistable potential is observed when coil currents have different signs. However, in the case of positive coil currents, a symmetric tristable potential is detected when the currents are the same, and an asymmetric tristable potential takes place when the positive currents have different values. Considering the sinusoidal coil current signals, analytical calculations using the harmonic balance method and numerical simulations are carried out for this electric-magneto-mechanical system. The obtained results are shown in terms of frequency-response diagrams, displacement time series, and phase portraits. The two-parameter bifurcation diagrams are plotted showing the different dynamical behaviors considering the current amplitudes and frequency as the control parameters. Amplitude jumps, hysteresis, and multistability are also observed. Some phase portraits and the coexistence of attractors are obtained numerically and confirmed experimentally. A good agreement between the numerical simulation and experimental measurement is achieved.", "AI": {"tldr": "\u7814\u7a76\u78c1\u6027\u6446\u7684\u975e\u7ebf\u6027\u52a8\u529b\u5b66\uff0c\u63ed\u793a\u7535\u6d41\u8c03\u63a7\u4e0b\u53cc\u7a33\u6001/\u4e09\u7a33\u6001\u52bf\u80fd\u5f62\u6001\u53ca\u5176\u591a\u7a33\u6001\u3001\u8df3\u8dc3\u7b49\u590d\u6742\u52a8\u529b\u5b66\u884c\u4e3a", "motivation": "\u63a2\u7a76\u5f3a\u78c1\u94c1\u4e0e\u53cc\u7ebf\u5708\u76f8\u4e92\u4f5c\u7528\u4e0b\u78c1\u6027\u6446\u7684\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7279\u6027\uff0c\u7279\u522b\u662f\u7535\u6d41\u53c2\u6570\u5bf9\u7cfb\u7edf\u52bf\u80fd\u5f62\u6001\uff08\u53cc\u7a33/\u4e09\u7a33\uff09\u548c\u590d\u6742\u52a8\u6001\u884c\u4e3a\u7684\u5f71\u54cd\u673a\u5236", "method": "\u7ed3\u5408\u78c1\u8377\u76f8\u4e92\u4f5c\u7528\u7406\u8bba\u6a21\u578b\u548c\u5b9e\u9a8c\u62df\u5408\u6a21\u578b\u63a8\u5bfc\u529b\u77e9\uff1b\u901a\u8fc7\u8c10\u6ce2\u5e73\u8861\u6cd5\u89e3\u6790\u8ba1\u7b97\u4e0e\u6570\u503c\u6a21\u62df\u5206\u6790\u7cfb\u7edf\uff1b\u5b9e\u9a8c\u6d4b\u91cf\u9a8c\u8bc1\u4eff\u771f\u7ed3\u679c", "result": "\u53d1\u73b0\u8d1f\u7535\u6d41\u4ea7\u751f\u5bf9\u79f0/\u4e0d\u5bf9\u79f0\u53cc\u7a33\u6001\u52bf\uff0c\u6b63\u7535\u6d41\u4ea7\u751f\u5bf9\u79f0/\u4e0d\u5bf9\u79f0\u4e09\u7a33\u6001\u52bf\uff1b\u63ed\u793a\u9891\u7387-\u7535\u6d41\u53c2\u6570\u7a7a\u95f4\u4e2d\u591a\u7a33\u6001\u3001\u632f\u5e45\u8df3\u8dc3\u3001\u6ede\u540e\u73b0\u8c61\u53ca\u5438\u5f15\u5b50\u5171\u5b58\u7b49\u590d\u6742\u52a8\u529b\u5b66\u884c\u4e3a\uff1b\u5b9e\u9a8c\u4e0e\u4eff\u771f\u9ad8\u5ea6\u543b\u5408", "conclusion": "\u901a\u8fc7\u8c03\u63a7\u7ebf\u5708\u7535\u6d41\u53ef\u7cbe\u786e\u5851\u9020\u7cfb\u7edf\u52bf\u80fd\u666f\u89c2\u5e76\u8bf1\u53d1\u4e30\u5bcc\u975e\u7ebf\u6027\u73b0\u8c61\uff0c\u4e3a\u7535\u78c1\u673a\u68b0\u7cfb\u7edf\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u8c03\u63a7\u63d0\u4f9b\u7406\u8bba\u548c\u5b9e\u9a8c\u4f9d\u636e"}}
{"id": "2602.17680", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17680", "abs": "https://arxiv.org/abs/2602.17680", "authors": ["Yujia Wang", "Jihong Guan", "Wengen Li", "Shuigeng Zhou", "Xuhong Wang"], "title": "BioBridge: Bridging Proteins and Language for Enhanced Biological Reasoning with LLMs", "comment": null, "summary": "Existing Protein Language Models (PLMs) often suffer from limited adaptability to multiple tasks and exhibit poor generalization across diverse biological contexts. In contrast, general-purpose Large Language Models (LLMs) lack the capability to interpret protein sequences and fall short in domain-specific knowledge, limiting their capacity for effective biosemantic reasoning. To combine the advantages of both, we propose BioBridge, a domain-adaptive continual pretraining framework for protein understanding. This framework employs Domain-Incremental Continual Pre-training (DICP) to infuse protein domain knowledge and general reasoning corpus into a LLM simultaneously, effectively mitigating catastrophic forgetting. Cross-modal alignment is achieved via a PLM-Projector-LLM pipeline, which maps protein sequence embeddings into the semantic space of the language model. Ultimately, an end-to-end optimization is adopted to uniformly support various tasks, including protein property prediction and knowledge question-answering. Our proposed BioBridge demonstrates performance comparable to that of mainstream PLMs on multiple protein benchmarks, such as EC and BindingDB. It also achieves results on par with LLMs on general understanding tasks like MMLU and RACE. This showcases its innovative advantage of combining domain-specific adaptability with general-purpose language competency.", "AI": {"tldr": "\u63d0\u51fa\u4e86BioBridge\u6846\u67b6\uff0c\u901a\u8fc7\u9886\u57df\u589e\u91cf\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u5c06\u86cb\u767d\u8d28\u9886\u57df\u77e5\u8bc6\u6ce8\u5165\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u4e13\u4e1a\u86cb\u767d\u8d28\u7406\u89e3\u4e0e\u901a\u7528\u8bed\u8a00\u80fd\u529b\u7684\u7ed3\u5408", "motivation": "\u73b0\u6709\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u9002\u5e94\u6027\u6709\u9650\u4e14\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u800c\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u86cb\u767d\u8d28\u5e8f\u5217\u89e3\u91ca\u80fd\u529b\u548c\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u751f\u7269\u8bed\u4e49\u63a8\u7406\u6548\u679c", "method": "\u91c7\u7528\u9886\u57df\u589e\u91cf\u6301\u7eed\u9884\u8bad\u7ec3\uff08DICP\uff09\u540c\u65f6\u6ce8\u5165\u86cb\u767d\u8d28\u9886\u57df\u77e5\u8bc6\u548c\u901a\u7528\u63a8\u7406\u8bed\u6599\uff0c\u901a\u8fc7PLM-Projector-LLM\u7ba1\u9053\u5b9e\u73b0\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u6700\u540e\u8fdb\u884c\u7aef\u5230\u7aef\u4f18\u5316", "result": "\u5728EC\u548cBindingDB\u7b49\u86cb\u767d\u8d28\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u6027\u80fd\u4e0e\u4e3b\u6d41PLM\u76f8\u5f53\uff0c\u5728MMLU\u548cRACE\u7b49\u901a\u7528\u7406\u89e3\u4efb\u52a1\u4e0a\u8fbe\u5230LLM\u6c34\u5e73", "conclusion": "\u6210\u529f\u5b9e\u73b0\u4e86\u9886\u57df\u7279\u5b9a\u9002\u5e94\u6027\u4e0e\u901a\u7528\u8bed\u8a00\u80fd\u529b\u7684\u521b\u65b0\u7ed3\u5408\uff0c\u4e3a\u86cb\u767d\u8d28\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f"}}
{"id": "2602.17915", "categories": ["cond-mat.str-el", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2602.17915", "abs": "https://arxiv.org/abs/2602.17915", "authors": ["Qinwen Deng", "Andrea Capa Salinas", "Suchismita Sarker", "Leon Balents", "Stephen D. Wilson", "Liang Wu"], "title": "Observation of Room-temperature Charge Density Wave Correlations via Coherent Phonon Spectroscopy in Sn-doped Kagome Superconductor CsV$_3$Sb$_5$", "comment": "Accepted to Physical Review B", "summary": "In this work, we perform ultrafast time-resolved reflectivity measurements to track the evolution of charge density wave (CDW) correlations in Sn-doped Kagome superconductor CsV$_3$Sb$_{5-x}$Sn$_x$. By extracting the coherent phonon spectrum, we evidence robust signatures of CDW correlations at temperature and doping ranges far beyond the phase boundary of long-range CDW order. Remarkably, we unveil short-range CDW correlations survive up to room temperature in $x = 0.32$ Sn-doped CsV$_3$Sb$_5$, supported by synchrotron X-ray diffraction measurements. We point out the introduction of quenched disorder by Sn doping can pin the CDW and form static short-range CDW, which can explain the observed persistent CDW signatures. Our results thus corroborate the ubiquity and robustness of CDW correlations in Sn-doped CsV$_3$Sb$_5$ and provide new insights on the role of disorders on the CDW correlations in AV$_3$Sb$_5$ family.", "AI": {"tldr": "\u901a\u8fc7\u8d85\u5feb\u65f6\u95f4\u5206\u8fa8\u53cd\u5c04\u7387\u6d4b\u91cf\uff0c\u53d1\u73b0Sn\u63ba\u6742\u7684Kagome\u8d85\u5bfc\u4f53CsV\u2083Sb\u2085\u4e2d\u5b58\u5728\u8fdc\u8d85\u4f20\u7edf\u76f8\u8fb9\u754c\u7684\u7535\u8377\u5bc6\u5ea6\u6ce2\u5173\u8054\uff0c\u751a\u81f3\u5728x=0.32\u63ba\u6742\u6837\u54c1\u4e2d\u5ba4\u6e29\u4e0b\u4ecd\u5b58\u5728\u77ed\u7a0b\u5173\u8054\uff0c\u63ed\u793a\u4e86\u6dec\u706b\u65e0\u5e8f\u5bf9\u7535\u8377\u5bc6\u5ea6\u6ce2\u7684\u91cd\u8981\u4f5c\u7528\u3002", "motivation": "\u7814\u7a76Sn\u63ba\u6742CsV\u2083Sb\u2085\u4e2d\u7535\u8377\u5bc6\u5ea6\u6ce2\u5173\u8054\u7684\u6f14\u5316\u673a\u5236\uff0c\u63a2\u7a76\u63ba\u6742\u5f15\u5165\u7684\u65e0\u5e8f\u5982\u4f55\u5f71\u54cdCDW\u7684\u7a33\u5b9a\u6027\u548c\u76f8\u8fb9\u754c\uff0c\u4e3a\u7406\u89e3AV\u2083Sb\u2085\u5bb6\u65cf\u4e2d\u7684\u91cf\u5b50\u73b0\u8c61\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002", "method": "\u91c7\u7528\u8d85\u5feb\u65f6\u95f4\u5206\u8fa8\u53cd\u5c04\u7387\u6d4b\u91cf\u6280\u672f\u8ffd\u8e2aCDW\u52a8\u529b\u5b66\u6f14\u5316\uff0c\u63d0\u53d6\u76f8\u5e72\u58f0\u5b50\u8c31\uff0c\u5e76\u7ed3\u5408\u540c\u6b65\u8f90\u5c04X\u5c04\u7ebf\u884d\u5c04\u6d4b\u91cf\u9a8c\u8bc1\u77ed\u7a0bCDW\u7ed3\u6784\u3002", "result": "\u53d1\u73b0CDW\u5173\u8054\u5728\u6e29\u5ea6\u548c\u63ba\u6742\u8303\u56f4\u5185\u8fdc\u8d85\u957f\u7a0b\u6709\u5e8f\u76f8\u8fb9\u754c\uff1b\u5728x=0.32\u7684Sn\u63ba\u6742\u6837\u54c1\u4e2d\uff0c\u77ed\u7a0bCDW\u5173\u8054\u53ef\u7a33\u5b9a\u5b58\u5728\u81f3\u5ba4\u6e29\uff1bSn\u63ba\u6742\u5f15\u5165\u7684\u6dec\u706b\u65e0\u5e8f\u80fd\u591f\u9489\u624eCDW\u5e76\u5f62\u6210\u9759\u6001\u77ed\u7a0b\u7ed3\u6784\u3002", "conclusion": "\u8bc1\u5b9e\u4e86CDW\u5173\u8054\u5728Sn\u63ba\u6742CsV\u2083Sb\u2085\u4e2d\u7684\u666e\u904d\u6027\u548c\u9c81\u68d2\u6027\uff0c\u63ed\u793a\u4e86\u65e0\u5e8f\u5728\u7a33\u5b9a\u77ed\u7a0bCDW\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u7406\u89e3AV\u2083Sb\u2085\u5bb6\u65cf\u4e2dCDW\u7269\u7406\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2602.18380", "categories": ["cs.CC", "cs.GT", "econ.TH"], "pdf": "https://arxiv.org/pdf/2602.18380", "abs": "https://arxiv.org/abs/2602.18380", "authors": ["Eleni Batziou", "John Fearnley", "Abheek Ghosh", "Rahul Savani"], "title": "The Complexity of Sparse Win-Lose Bimatrix Games", "comment": "43 pages", "summary": "We prove that computing an $\u03b5$-approximate Nash equilibrium of a win-lose bimatrix game with constant sparsity is PPAD-hard for inverse-polynomial $\u03b5$. Our result holds for 3-sparse games, which is tight given that 2-sparse win-lose bimatrix games can be solved in polynomial time.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.17902", "categories": ["cs.AI", "cs.MA", "cs.SE", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2602.17902", "abs": "https://arxiv.org/abs/2602.17902", "authors": ["Jiaru Bai", "Abdulrahman Aldossary", "Thomas Swanick", "Marcel M\u00fcller", "Yeonghun Kang", "Zijian Zhang", "Jin Won Lee", "Tsz Wai Ko", "Mohammad Ghazi Vakili", "Varinia Bernales", "Al\u00e1n Aspuru-Guzik"], "title": "El Agente Gr\u00e1fico: Structured Execution Graphs for Scientific Agents", "comment": null, "summary": "Large language models (LLMs) are increasingly used to automate scientific workflows, yet their integration with heterogeneous computational tools remains ad hoc and fragile. Current agentic approaches often rely on unstructured text to manage context and coordinate execution, generating often overwhelming volumes of information that may obscure decision provenance and hinder auditability. In this work, we present El Agente Gr\u00e1fico, a single-agent framework that embeds LLM-driven decision-making within a type-safe execution environment and dynamic knowledge graphs for external persistence. Central to our approach is a structured abstraction of scientific concepts and an object-graph mapper that represents computational state as typed Python objects, stored either in memory or persisted in an external knowledge graph. This design enables context management through typed symbolic identifiers rather than raw text, thereby ensuring consistency, supporting provenance tracking, and enabling efficient tool orchestration. We evaluate the system by developing an automated benchmarking framework across a suite of university-level quantum chemistry tasks previously evaluated on a multi-agent system, demonstrating that a single agent, when coupled to a reliable execution engine, can robustly perform complex, multi-step, and parallel computations. We further extend this paradigm to two other large classes of applications: conformer ensemble generation and metal-organic framework design, where knowledge graphs serve as both memory and reasoning substrates. Together, these results illustrate how abstraction and type safety can provide a scalable foundation for agentic scientific automation beyond prompt-centric designs.", "AI": {"tldr": "We propose El Agente Gr\u00e1fico, a single-agent framework that combines type-safe execution with dynamic knowledge graphs to enable robust, auditable LLM-driven scientific automation, replacing fragile multi-agent text-based approaches.", "motivation": "Current LLM integration with scientific tools is ad hoc and fragile; unstructured text-based agent coordination creates overwhelming information that obscures decision provenance and hinders auditability.", "method": "A single-agent framework embedding LLM decision-making within a type-safe environment using dynamic knowledge graphs for persistence, with structured scientific concept abstractions and an object-graph mapper that represents state as typed Python objects managed through symbolic identifiers rather than raw text.", "result": "A single agent successfully performed complex multi-step quantum chemistry computations previously requiring multi-agent systems, and was extended to conformer ensemble generation and metal-organic framework design, where knowledge graphs function as both memory and reasoning substrates.", "conclusion": "Type-safe abstraction and knowledge graphs provide a scalable foundation for agentic scientific automation, moving beyond fragile prompt-centric designs to enable robust, provable, and auditable computational workflows."}}
{"id": "2602.17786", "categories": ["quant-ph", "cond-mat.other", "physics.atom-ph"], "pdf": "https://arxiv.org/pdf/2602.17786", "abs": "https://arxiv.org/abs/2602.17786", "authors": ["Adolfo del Campo"], "title": "Shortcuts to Adiabaticity via Adaptive Quantum Zeno Measurements", "comment": "6+2pp", "summary": "We consider the quantum Zeno dynamics arising from monitoring a time-dependent projector. Starting from a stroboscopic measurement protocol, it is shown that the effective Hamiltonian for Zeno dynamics involves a nonadiabatic geometric connection that takes the form of the Kato-Avron Hamiltonian for parallel transport, stirring the evolution within the time-dependent Zeno subspace. The latter reduces to counterdiabatic driving when projective measurements are performed in the instantaneous energy eigenbasis of the quantum system. The effective Zeno Hamiltonian can also be derived in the context of continuous quantum measurements of a time-dependent observable and the non-Hermitian evolution with a complex absorbing potential varying in time. Our results thus provide a unified framework for realizing shortcuts to adiabaticity via adaptive quantum Zeno measurements.", "AI": {"tldr": "This paper develops a unified framework showing how quantum Zeno measurements with time-dependent protocols enable shortcuts to adiabaticity through geometric connections in the Zeno subspace.", "motivation": "To establish a fundamental connection between quantum Zeno dynamics and shortcuts to adiabaticity by deriving effective Hamiltonians under time-dependent measurement protocols, addressing the need for faster quantum control beyond conventional adiabatic evolution.", "method": "Derives effective Zeno Hamiltonians through two approaches: (1) stroboscopic measurements revealing Kato-Avron geometric connections, and (2) continuous measurements/complex absorbing potentials; identifies equivalence with counterdiabatic driving when measurements align with instantaneous eigenbases.", "result": "Uncovers that the effective Zeno Hamiltonian universally contains a nonadiabatic geometric term (Kato-Avron parallel transport), which reduces to counterdiabatic driving under specific measurement bases, enabling adaptive shortcuts to adiabaticity.", "conclusion": "Provides a geometric foundation for quantum Zeno control, demonstrating that time-dependent measurements inherently generate counterdiabatic protocols; this unification advances practical implementations of rapid adiabatic-like evolution in quantum technologies."}}
{"id": "2602.17681", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17681", "abs": "https://arxiv.org/abs/2602.17681", "authors": ["Ofir Gordon", "Lior Dikstein", "Arnon Netzer", "Idan Achituve", "Hai Victor Habi"], "title": "LATMiX: Learnable Affine Transformations for Microscaling Quantization of LLMs", "comment": "24 pages, 4 figures", "summary": "Post-training quantization (PTQ) is a widely used approach for reducing the memory and compute costs of large language models (LLMs). Recent studies have shown that applying invertible transformations to activations can significantly improve quantization robustness by reducing activation outliers; however, existing approaches are largely restricted to rotation or Hadamard-based transformations. Moreover, most studies focused primarily on traditional quantization schemes, whereas modern hardware increasingly supports the microscaling (MX) data format. Attempts to combine both showed severe performance degradation, leading prior work to introduce assumptions on the transformations. In this work, we take a complementary perspective. First, we provide a theoretical analysis of transformations under MX quantization by deriving a bound on the quantization error. Our analysis emphasizes the importance of accounting for both the activation distribution and the underlying quantization structure. Building on this analysis, we propose LATMiX, a method that generalizes outlier reduction to learnable invertible affine transformations optimized using standard deep learning tools. Experiments show consistent improvements in average accuracy for MX low-bit quantization over strong baselines on a wide range of zero-shot benchmarks, across multiple model sizes.", "AI": {"tldr": "This paper proposes LATMiX, a method using learnable invertible affine transformations to improve post-training quantization for LLMs under modern microscaling (MX) hardware formats, achieving superior accuracy over existing approaches.", "motivation": "Existing post-training quantization methods for large language models are limited to rotation or Hadamard transformations and perform poorly with modern microscaling (MX) data formats, causing severe degradation. There is a need for a more flexible transformation approach that works effectively with MX hardware without restrictive assumptions.", "method": "The authors first provide a theoretical analysis deriving a quantization error bound under MX format, then propose LATMiX which generalizes outlier reduction to learnable invertible affine transformations optimized using standard deep learning tools.", "result": "Experiments demonstrate consistent improvements in average accuracy for MX low-bit quantization across a wide range of zero-shot benchmarks and multiple model sizes, outperforming strong baseline methods.", "conclusion": "The theoretical foundation combined with learnable affine transformations effectively addresses activation outlier challenges under MX quantization, providing a practical solution for deploying efficient large language models on modern hardware."}}
{"id": "2602.17959", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.17959", "abs": "https://arxiv.org/abs/2602.17959", "authors": ["Xinyue Liu", "Tao Li"], "title": "Optical and Hall conductivity of the two dimensional Hubbard model: effective theory description, sign-problem-free Monte Carlo simulation and applications to the cuprate superconductors", "comment": "19 pages, 7 figures. Some results of arXiv:2404.11224 are included here, but rewritten in a much broader perspective", "summary": "Exact formulas for the optical conductivity and the Hall conductivity of the two dimensional Hubbard model are derived in terms of an effective theory description of the local moment fluctuation in the system. In this framework, the quantum Monte Carlo simulation of the electromagnetic response of such a strongly correlated electron system becomes sign-problem-free in many physically relevant cases. In particular, it is sign-problem-free when we assume the widely used Millis-Monien-Pines form for the phenomenological susceptibility in the effective action of the fluctuating local moment, even though these local moments are now subjected to Landau damping as a result of their coupling to the itinerant quasiparticle on the fermi surface. This is true more generally when a $\\varphi^{4}$ term is included in the effective action and is thus not restricted to the Gaussian limit. Here we demonstrate the power of this framework by studying the effect of thermal fluctuation of the local moment on the optical conductivity $\u03c3^{xx}(\u03c9)$ and the Hall conductivity $\u03c3^{xy}(\u03c9)$ of the cuprate superconductors. Both $\u03c3^{xx}(\u03c9)$ and $\u03c3^{xy}(\u03c9)$ calculated are found to exhibit a two-component structure, with a Drude component at low energy and a mid-infrared component at higher energy. Depending on the relative importance of the hole pocket and the electron pocket on the reconstructed fermi surface and the coupling strength to the local moment, the Drude component in $\\mathrm{Im}\u03c3^{xy}(\u03c9)$ can be either positive or negative.(full-length abstract can be found in the main text.)", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5c40\u57df\u77e9\u6da8\u843d\u6709\u6548\u7406\u8bba\u7684\u7cbe\u786e\u516c\u5f0f\uff0c\u89e3\u51b3\u4e8c\u7ef4\u54c8\u4f2f\u5fb7\u6a21\u578b\u7535\u78c1\u54cd\u5e94\u7684\u91cf\u5b50\u8499\u7279\u5361\u6d1b\u7b26\u53f7\u95ee\u9898\uff0c\u63ed\u793a\u94dc\u9178\u76d0\u8d85\u5bfc\u4f53\u5149\u5b66\u548c\u970d\u5c14\u7535\u5bfc\u7684\u53cc\u7ec4\u5206\u7ed3\u6784\u7279\u5f81", "motivation": "\u89e3\u51b3\u5f3a\u5173\u8054\u7535\u5b50\u4f53\u7cfb\uff08\u5982\u94dc\u9178\u76d0\u8d85\u5bfc\u4f53\uff09\u4e2d\u4f20\u7edf\u91cf\u5b50\u8499\u7279\u5361\u6d1b\u6a21\u62df\u7535\u78c1\u54cd\u5e94\u65f6\u5b58\u5728\u7684\u7b26\u53f7\u95ee\u9898\uff0c\u5b9e\u73b0\u5149\u5b66\u7535\u5bfc\u7387\u03c3\u207a\u207a(\u03c9)\u548c\u970d\u5c14\u7535\u5bfc\u7387\u03c3\u207a\u02e3\u02b8(\u03c9)\u7684\u7cbe\u786e\u8ba1\u7b97", "method": "\u6784\u5efa\u5c40\u57df\u77e9\u6da8\u843d\u7684\u6709\u6548\u7406\u8bba\u6846\u67b6\uff0c\u91c7\u7528Millis-Monien-Pines\u5f62\u5f0f\u78c1\u5316\u7387\u5e76\u5f15\u5165Landau\u963b\u5c3c\u548c\u03c6\u2074\u9879\uff0c\u5efa\u7acb\u65e0\u7b26\u53f7\u95ee\u9898\u7684\u91cf\u5b50\u8499\u7279\u5361\u6d1b\u6a21\u62df\u65b9\u6cd5", "result": "\u83b7\u5f97\u4e8c\u7ef4\u54c8\u4f2f\u5fb7\u6a21\u578b\u7535\u5bfc\u7387\u7684\u7cbe\u786e\u8868\u8fbe\u5f0f\uff1b\u8bc1\u5b9e\u94dc\u9178\u76d0\u4e2d\u03c3\u207a\u207a(\u03c9)\u548c\u03c3\u207a\u02e3\u02b8(\u03c9)\u5b58\u5728\u4f4e\u80fdDrude\u5206\u91cf\u548c\u9ad8\u80fd\u4e2d\u7ea2\u5916\u53cc\u7ec4\u5206\u7ed3\u6784\uff1b\u970d\u5c14\u7535\u5bfcDrude\u5206\u91cf\u7b26\u53f7\u53d6\u51b3\u4e8e\u8d39\u7c73\u9762\u91cd\u6784\u540e\u7684\u7a7a\u7a74/\u7535\u5b50\u53e3\u888b\u76f8\u5bf9\u6743\u91cd\u548c\u8026\u5408\u5f3a\u5ea6", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u6d88\u9664\u5f3a\u5173\u8054\u4f53\u7cfb\u7535\u78c1\u54cd\u5e94\u8ba1\u7b97\u7684\u7b26\u53f7\u95ee\u9898\uff0c\u4e3a\u94dc\u9178\u76d0\u7b49\u6750\u6599\u7684\u7535\u8f93\u8fd0\u6027\u8d28\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\u5de5\u5177\uff0c\u7279\u522b\u63ed\u793a\u4e86\u8d39\u7c73\u9762\u62d3\u6251\u5bf9\u970d\u5c14\u6548\u5e94\u7684\u5173\u952e\u5f71\u54cd"}}
{"id": "2602.18273", "categories": ["cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.18273", "abs": "https://arxiv.org/abs/2602.18273", "authors": ["Y. J. Kang", "S. K. So", "Kyungsik Kim"], "title": "Analytical solutions for a charged particle with white, thermal, and active noises in the presence of a uniform magnetic field", "comment": "21 pages", "summary": "We study the two-dimensional equations of motion for a charged particle subjected to white, thermal, and active noises in uniform a magnetic field. By deriving the corresponding Fokker Planck equation, analytical solutions for the joint probability density are obtained in different time domains.", "AI": {"tldr": "\u7814\u7a76\u5747\u5300\u78c1\u573a\u4e2d\u5e26\u7535\u7c92\u5b50\u5728\u591a\u79cd\u566a\u58f0\u4f5c\u7528\u4e0b\u7684\u4e8c\u7ef4\u8fd0\u52a8\uff0c\u901a\u8fc7Fokker-Planck\u65b9\u7a0b\u83b7\u5f97\u8054\u5408\u6982\u7387\u5bc6\u5ea6\u7684\u89e3\u6790\u89e3", "motivation": "\u63a2\u7a76\u767d\u566a\u58f0\u3001\u70ed\u566a\u58f0\u53ca\u6d3b\u6027\u566a\u58f0\u5171\u540c\u4f5c\u7528\u4e0b\u5e26\u7535\u7c92\u5b50\u7684\u968f\u673a\u52a8\u529b\u5b66\u884c\u4e3a\uff0c\u5bf9\u7406\u89e3\u7b49\u79bb\u5b50\u4f53\u7269\u7406\u548c\u751f\u7269\u7cfb\u7edf\u4e2d\u7684\u8f93\u8fd0\u73b0\u8c61\u5177\u6709\u91cd\u8981\u610f\u4e49", "method": "\u4ece\u8fd0\u52a8\u65b9\u7a0b\u51fa\u53d1\u63a8\u5bfc\u5bf9\u5e94\u7684Fokker-Planck\u65b9\u7a0b\uff0c\u5e76\u5728\u4e0d\u540c\u65f6\u95f4\u57df\u6c42\u89e3\u8054\u5408\u6982\u7387\u5bc6\u5ea6\u7684\u89e3\u6790\u8868\u8fbe\u5f0f", "result": "\u6210\u529f\u83b7\u5f97\u63cf\u8ff0\u7c92\u5b50\u4f4d\u7f6e\u4e0e\u901f\u5ea6\u8054\u5408\u6982\u7387\u5206\u5e03\u7684\u663e\u5f0f\u89e3\u6790\u89e3\uff0c\u63ed\u793a\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u4e0b\u7684\u6f14\u5316\u89c4\u5f8b", "conclusion": "\u89e3\u6790\u89e3\u8868\u660e\u78c1\u573a\u4e0e\u591a\u7c7b\u578b\u566a\u58f0\u5171\u540c\u8c03\u63a7\u7c92\u5b50\u7684\u6269\u6563\u884c\u4e3a\u548c\u5b9a\u5411\u8f93\u8fd0\u7279\u6027\uff0c\u4e3a\u76f8\u5173\u968f\u673a\u52a8\u529b\u5b66\u7cfb\u7edf\u63d0\u4f9b\u7406\u8bba\u6a21\u578b"}}
{"id": "2602.17803", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.17803", "abs": "https://arxiv.org/abs/2602.17803", "authors": ["Ray Ganardi", "Jeongrak Son", "Jakub Czartowski", "Seok Hyung Lie", "Nelly H. Y. Ng"], "title": "Manipulating heterogeneous quantum resources over a network", "comment": "13+7 pages, 2 figures", "summary": "Quantum information processing relies on a variety of resources, including entanglement, coherence, non-Gaussianity, and magic. In realistic settings, protocols run on networks of parties with heterogeneous local resource constraints, so different resources coexist and interact. Yet, resource theories have mostly treated each resource in isolation, and a general theory for manipulation in such distributed settings has been lacking. We develop a unified framework for composite quantum resource theories that describes distributed networks of locally constrained parties. We formulate natural axioms a composite theory should satisfy to respect the local structure, and from these axioms derive fundamental bounds on resource manipulation that hold universally, independent of the particular network characteristics. We apply our results to central operational tasks, including resource conversion and assisted distillation, and introduce new methods to construct new resource monotones from this setup. Our framework further reveals previously unexplored phenomena in the remote certification of quantum resources. Together, these results establish foundational laws for distributed quantum resource manipulation across diverse physical platforms.", "AI": {"tldr": "This paper develops a unified framework for composite quantum resource theories in distributed networks with local constraints, deriving universal bounds on resource manipulation and applying them to tasks like conversion and distillation while revealing new phenomena in remote certification.", "motivation": "Current quantum resource theories treat resources in isolation, but realistic quantum networks involve heterogeneous local constraints where different resources (entanglement, coherence, non-Gaussianity, magic) coexist and interact. A general theory for manipulation in such distributed settings is lacking.", "method": "The authors develop a unified framework by formulating natural axioms that respect the local structure of distributed networks, then derive fundamental bounds on resource manipulation from these axioms.", "result": "The framework provides universal bounds independent of network characteristics, enables new methods for constructing resource monotones, applies to central tasks like resource conversion and assisted distillation, and reveals new phenomena in remote certification of quantum resources.", "conclusion": "The results establish foundational laws for distributed quantum resource manipulation that are applicable across diverse physical platforms."}}
{"id": "2602.18412", "categories": ["quant-ph", "nlin.CD"], "pdf": "https://arxiv.org/pdf/2602.18412", "abs": "https://arxiv.org/abs/2602.18412", "authors": ["Ariel A. Galindo Duque", "Miguel A. Prado Reynoso", "Miguel Gonzalez", "Jorge G. Hirsch"], "title": "Participation Ratio as a Quantum Probe of Hierarchical Stickiness", "comment": "9 pages, 4 figures", "summary": "We investigate how quantum localization encodes the hierarchical stickiness that governs transport in mixed classical phase spaces. Using the periodically driven kicked top, we show that the participation ratio (PR) of coherent states in the Floquet eigenbasis resolves the same layered structure that appears classically as a multimodal distribution of finite-time Lyapunov exponents (FTLEs). To establish a quantitative correspondence, we introduce a Gaussian coarse graining of the FTLE matched to the intrinsic semiclassical resolution of coherent states. Both local correlations and global comparisons of probability distributions demonstrate that quantum and classical indicators agree optimally within a finite window of evolution times, where sticky structures are most clearly resolved. Our results promote the participation ratio from a global measure of chaos to a sensitive probe of hierarchical transport and provide a practical route for diagnosing anomalous localization in driven quantum systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u91cf\u5b50\u5b9a\u4f4d\u5982\u4f55\u7f16\u7801\u6df7\u5408\u7ecf\u5178\u76f8\u7a7a\u95f4\u4e2d\u7684\u5c42\u6b21\u5316\u7c98\u6ede\u6027\uff0c\u8bc1\u660eFloquet\u672c\u5f81\u6001\u7684\u53c2\u4e0e\u7387\u4e0e\u6709\u9650\u65f6\u95f4\u674e\u96c5\u666e\u8bfa\u592b\u6307\u6570\u5206\u5e03\u5177\u6709\u76f8\u540c\u7684\u5206\u5c42\u7ed3\u6784\uff0c\u4e3a\u8bca\u65ad\u91cf\u5b50\u7cfb\u7edf\u5f02\u5e38\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5", "motivation": "\u63a2\u7a76\u91cf\u5b50\u5b9a\u4f4d\u4e0e\u6df7\u5408\u7ecf\u5178\u76f8\u7a7a\u95f4\u4e2d\u5c42\u6b21\u5316\u8f93\u8fd0\u73b0\u8c61\u7684\u5185\u5728\u5173\u8054\uff0c\u89e3\u51b3\u7ecf\u5178\u4e0e\u91cf\u5b50\u6307\u6807\u5982\u4f55\u5b9a\u91cf\u5bf9\u5e94\u7684\u5173\u952e\u95ee\u9898", "method": "\u91c7\u7528\u5468\u671f\u9a71\u52a8 kicked top \u6a21\u578b\uff0c\u5f15\u5165\u9ad8\u65af\u7c97\u7c92\u5316\u5904\u7406\u6709\u9650\u65f6\u95f4\u674e\u96c5\u666e\u8bfa\u592b\u6307\u6570\u5206\u5e03\uff0c\u5e76\u4e0e\u76f8\u5e72\u6001\u7684\u91cf\u5b50\u534a\u7ecf\u5178\u5206\u8fa8\u7387\u8fdb\u884c\u5339\u914d\uff0c\u901a\u8fc7\u5c40\u90e8\u76f8\u5173\u6027\u548c\u5168\u5c40\u6982\u7387\u5206\u5e03\u6bd4\u8f83\u9a8c\u8bc1\u91cf\u5b50-\u7ecf\u5178\u5bf9\u5e94\u5173\u7cfb", "result": "\u5728\u7279\u5b9a\u6f14\u5316\u65f6\u95f4\u7a97\u53e3\u5185\uff0c\u91cf\u5b50\u53c2\u4e0e\u7387\u4e0e\u7ecf\u5178\u6709\u9650\u65f6\u95f4\u674e\u96c5\u666e\u8bfa\u592b\u6307\u6570\u5206\u5e03\u5448\u73b0\u6700\u4f18\u4e00\u81f4\u6027\uff0c\u4e24\u8005\u5171\u540c\u63ed\u793a\u76f8\u7a7a\u95f4\u4e2d\u7c98\u6ede\u7ed3\u6784\u7684\u5c42\u6b21\u5316\u7279\u5f81", "conclusion": "\u53c2\u4e0e\u7387\u4ece\u5168\u5c40\u6df7\u6c8c\u5ea6\u91cf\u5347\u7ea7\u4e3a\u5c42\u6b21\u5316\u8f93\u8fd0\u7684\u7075\u654f\u63a2\u9488\uff0c\u4e3a\u8bca\u65ad\u53d7\u9a71\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684\u5f02\u5e38\u5b9a\u4f4d\u73b0\u8c61\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\uff0c\u5efa\u7acb\u4e86\u91cf\u5b50-\u7ecf\u5178\u8f93\u8fd0\u8bca\u65ad\u7684\u65b0\u6865\u6881"}}
{"id": "2602.17990", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17990", "abs": "https://arxiv.org/abs/2602.17990", "authors": ["Madhav Kanda", "Pedro Las-Casas", "Alok Gautam Kumbhare", "Rodrigo Fonseca", "Sharad Agarwal"], "title": "WorkflowPerturb: Calibrated Stress Tests for Evaluating Multi-Agent Workflow Metrics", "comment": null, "summary": "LLM-based systems increasingly generate structured workflows for complex tasks. In practice, automatic evaluation of these workflows is difficult, because metric scores are often not calibrated, and score changes do not directly communicate the severity of workflow degradation. We introduce WorkflowPerturb, a controlled benchmark for studying workflow evaluation metrics. It works by applying realistic, controlled perturbations to golden workflows. WorkflowPerturb contains 4,973 golden workflows and 44,757 perturbed variants across three perturbation types (Missing Steps, Compressed Steps, and Description Changes), each applied at severity levels of 10%, 30%, and 50%. We benchmark multiple metric families and analyze their sensitivity and calibration using expected score trajectories and residuals. Our results characterize systematic differences across metric families and support severity-aware interpretation of workflow evaluation scores. Our dataset will be released upon acceptance.", "AI": {"tldr": "Introduce WorkflowPerturb, a benchmark for evaluating workflow quality metrics by applying controlled perturbations (Missing Steps, Compressed Steps, Description Changes) at varying severity levels to golden workflows, revealing systematic differences in metric families and enabling severity-aware score interpretation.", "motivation": "Automatic evaluation of LLM-generated structured workflows is challenging due to uncalibrated metrics and lack of direct mapping between score changes and degradation severity.", "method": "Create a controlled benchmark by applying three realistic perturbation types (Missing Steps, Compressed Steps, Description Changes) at 10%, 30%, and 50% severity levels to 4,973 golden workflows, generating 44,757 perturbed variants; then benchmark multiple metric families and analyze their sensitivity/calibration using expected score trajectories and residuals.", "result": "Characterize systematic differences across metric families, showing varying sensitivity and calibration; establish severity-aware interpretation of workflow evaluation scores.", "conclusion": "WorkflowPerturb provides a foundation for understanding and improving workflow evaluation metrics, with dataset release planned upon paper acceptance."}}
{"id": "2602.17806", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.17806", "abs": "https://arxiv.org/abs/2602.17806", "authors": ["Kelvin Yip", "Alessandro Monteros", "Sahel Ashhab", "Lin Tian"], "title": "Digital Quantum Simulation of the Holstein-Primakoff Transformation on Noisy Qubits", "comment": "12 pages, 8 figures", "summary": "Quantum simulation of many-body systems offers a powerful approach to exploring collective quantum dynamics beyond classical computational reach. Although spin and fermionic models have been extensively simulated on digital quantum computers, the simulation of bosonic systems on programmable quantum processors is often hindered by the intrinsically large Hilbert space of bosonic modes. In this work, we study the digital quantum simulation of bosonic modes using the Holstein-Primakoff (HP) transformation and implement this protocol on a cloud-based superconducting quantum processor. Two representative models are realized on quantum hardware: (i) the driven harmonic oscillator and (ii) the Jaynes-Cummings model. Using data obtained from the quantum simulations, we systematically examine the interplay between algorithmic and hardware-induced errors to identify optimal simulation parameters. The dominant algorithmic errors arise from the finite number of qubits used in the HP mapping and the finite number of Trotter steps in the time evolution, while hardware errors mainly originate from gate infidelity, decoherence, and readout errors. This study advances the digital quantum simulation of many-body systems involving bosonic degrees of freedom on currently available cloud quantum processors and provides a framework that can be extended to more complex spin-boson and multimode cavity models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5229\u7528Holstein-Primakoff\u53d8\u6362\u5728\u8d85\u5bfc\u91cf\u5b50\u5904\u7406\u5668\u4e0a\u5b9e\u73b0\u4e86\u73bb\u8272\u5b50\u7cfb\u7edf\u7684\u6570\u5b57\u91cf\u5b50\u6a21\u62df\uff0c\u9a8c\u8bc1\u4e86\u9a71\u52a8\u8c10\u632f\u5b50\u548cJaynes-Cummings\u6a21\u578b\uff0c\u5e76\u5206\u6790\u4e86\u7b97\u6cd5\u4e0e\u786c\u4ef6\u8bef\u5dee\u6765\u6e90\u3002", "motivation": "\u73bb\u8272\u5b50\u7cfb\u7edf\u56e0\u73bb\u8272\u6a21\u5f0f\u56fa\u6709\u7684\u5927\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\uff0c\u5728\u53ef\u7f16\u7a0b\u91cf\u5b50\u5904\u7406\u5668\u4e0a\u7684\u6a21\u62df\u957f\u671f\u53d7\u963b\uff0c\u800c\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u81ea\u65cb\u548c\u8d39\u7c73\u5b50\u6a21\u578b\u3002", "method": "\u91c7\u7528Holstein-Primakoff\u53d8\u6362\u5c06\u73bb\u8272\u5b50\u6620\u5c04\u5230 qubit \u7cfb\u7edf\uff0c\u5728\u4e91\u7aef\u8d85\u5bfc\u91cf\u5b50\u5904\u7406\u5668\u4e0a\u5b9e\u73b0\uff1a1) \u9a71\u52a8\u8c10\u632f\u5b50\u6a21\u578b\uff1b2) Jaynes-Cummings\u6a21\u578b\u3002\u901a\u8fc7\u91cf\u5b50\u6a21\u62df\u6570\u636e\u7cfb\u7edf\u5206\u6790\u8bef\u5dee\u3002", "result": "\u7b97\u6cd5\u8bef\u5dee\u4e3b\u8981\u6e90\u4e8eHP\u53d8\u6362\u7684\u6709\u9650qubit\u6570\u91cf\u548cTrotter\u6b65\u957f\uff1b\u786c\u4ef6\u8bef\u5dee\u6765\u81ea\u95e8\u64cd\u4f5c\u4fdd\u771f\u5ea6\u3001\u9000\u76f8\u5e72\u548c\u8bfb\u53d6\u9519\u8bef\u3002\u786e\u5b9a\u4e86\u6700\u4f18\u6a21\u62df\u53c2\u6570\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u53ef\u6269\u5c55\u81f3\u66f4\u590d\u6742\u7684\u81ea\u65cb-\u73bb\u8272\u5b50\u6a21\u578b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63a8\u52a8\u4e86\u5f53\u524d\u4e91\u91cf\u5b50\u5904\u7406\u5668\u4e0a\u73bb\u8272\u5b50\u591a\u4f53\u7cfb\u7edf\u7684\u6570\u5b57\u6a21\u62df\u53d1\u5c55\uff0c\u4e3a\u590d\u6742\u8154\u91cf\u5b50\u7535\u52a8\u529b\u5b66\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6846\u67b6\u3002"}}
{"id": "2602.17683", "categories": ["cs.LG", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.17683", "abs": "https://arxiv.org/abs/2602.17683", "authors": ["Irene Iele", "Giulia Romoli", "Daniele Molino", "Elena Mulero Ayll\u00f3n", "Filippo Ruffini", "Paolo Soda", "Matteo Tortora"], "title": "Probabilistic NDVI Forecasting from Sparse Satellite Time Series and Weather Covariates", "comment": null, "summary": "Accurate short-term forecasting of vegetation dynamics is a key enabler for data-driven decision support in precision agriculture. Normalized Difference Vegetation Index (NDVI) forecasting from satellite observations, however, remains challenging due to sparse and irregular sampling caused by cloud coverage, as well as the heterogeneous climatic conditions under which crops evolve. In this work, we propose a probabilistic forecasting framework specifically designed for field-level NDVI prediction under clear-sky acquisition constraints. The method leverages a transformer-based architecture that explicitly separates the modeling of historical vegetation dynamics from future exogenous information, integrating historical NDVI observations with both historical and future meteorological covariates. To address irregular revisit patterns and horizon-dependent uncertainty, we introduce a temporal-distance weighted quantile loss that aligns the training objective with the effective forecasting horizon. In addition, we incorporate cumulative and extreme-weather feature engineering to better capture delayed meteorological effects relevant to vegetation response. Extensive experiments on European satellite data demonstrate that the proposed approach consistently outperforms a diverse set of statistical, deep learning, and recent time series baselines across both point-wise and probabilistic evaluation metrics. Ablation studies further highlight the central role of target history, while showing that meteorological covariates provide complementary gains when jointly exploited. The code is available at https://github.com/arco-group/ndvi-forecasting.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6982\u7387\u9884\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u519c\u7530\u5c3a\u5ea6NDVI\u9884\u6d4b\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u65f6\u95f4\u8ddd\u79bb\u52a0\u6743\u5206\u4f4d\u6570\u635f\u5931\u5904\u7406\u4e0d\u89c4\u5219\u91c7\u6837\u95ee\u9898\uff0c\u878d\u5408\u6c14\u8c61\u56e0\u5b50\u4e0e\u690d\u88ab\u5386\u53f2\u4fe1\u606f\uff0c\u5728\u6b27\u6d32\u536b\u661f\u6570\u636e\u4e0a\u4f18\u4e8e\u5404\u7c7b\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u7cbe\u51c6\u519c\u4e1a\u9700\u8981\u51c6\u786e\u7684\u690d\u88ab\u52a8\u6001\u77ed\u671f\u9884\u6d4b\uff0c\u4f46\u536b\u661fNDVI\u9884\u6d4b\u9762\u4e34\u4e91\u5c42\u8986\u76d6\u5bfc\u81f4\u91c7\u6837\u7a00\u758f\u4e0d\u89c4\u5219\u3001\u4f5c\u7269\u751f\u957f\u6c14\u5019\u6761\u4ef6\u5f02\u8d28\u6027\u7b49\u6311\u6218\u3002", "method": "1) \u91c7\u7528Transformer\u67b6\u6784\u663e\u5f0f\u5206\u79bb\u5386\u53f2\u690d\u88ab\u52a8\u6001\u4e0e\u672a\u6765\u5916\u751f\u4fe1\u606f\u5efa\u6a21\uff1b2) \u5f15\u5165\u65f6\u95f4\u8ddd\u79bb\u52a0\u6743\u5206\u4f4d\u6570\u635f\u5931\u5bf9\u9f50\u8bad\u7ec3\u76ee\u6807\u4e0e\u6709\u6548\u9884\u6d4b\u89c6\u754c\uff1b3) \u8bbe\u8ba1\u7d2f\u79ef\u548c\u6781\u7aef\u5929\u6c14\u7279\u5f81\u5de5\u7a0b\u6355\u6349\u6c14\u8c61\u6ede\u540e\u6548\u5e94\uff1b4) \u878d\u5408\u5386\u53f2NDVI\u89c2\u6d4b\u4e0e\u5386\u53f2/\u672a\u6765\u6c14\u8c61\u534f\u53d8\u91cf\u3002", "result": "\u5728\u6b27\u6d32\u536b\u661f\u6570\u636e\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u70b9\u548c\u6982\u7387\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u6301\u7eed\u4f18\u4e8e\u7edf\u8ba1\u3001\u6df1\u5ea6\u5b66\u4e60\u548c\u65f6\u95f4\u5e8f\u5217\u57fa\u7ebf\u6a21\u578b\u3002\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\u76ee\u6807\u5386\u53f2\u4fe1\u606f\u8d77\u6838\u5fc3\u4f5c\u7528\uff0c\u6c14\u8c61\u534f\u53d8\u91cf\u63d0\u4f9b\u4e92\u8865\u589e\u76ca\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u6709\u6548\u5e94\u5bf9\u6674\u7a7a\u83b7\u53d6\u7ea6\u675f\u4e0b\u7684NDVI\u9884\u6d4b\u6311\u6218\uff0c\u4e3a\u7cbe\u51c6\u519c\u4e1a\u63d0\u4f9b\u53ef\u9760\u7684\u6570\u636e\u9a71\u52a8\u51b3\u7b56\u652f\u6301\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.03234", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.03234", "abs": "https://arxiv.org/abs/2602.03234", "authors": ["Ha Eum Kim", "Andrew D. Kim", "Jong Yeon Lee"], "title": "Liouvillian Gap in Dissipative Haar-Doped Clifford Circuits", "comment": "29 pages, 8 figures", "summary": "Quantum chaos is commonly assessed through probe-dependent signatures such as spectral statistics, OTOCs, and entanglement growth, which need not coincide. Recently, a dissipative diagnostic of chaos has been proposed, in which an infinitesimal coupling to a bath yields a finite Liouvillian gap in chaotic systems, marking the onset of intrinsic relaxation. This raises a conceptual question: what is the minimal departure from Clifford dynamics needed for this intrinsically relaxing behavior to emerge? In this work, we investigate the dynamics under the Floquet two-qubit Clifford circuit interleaved with a finite density of Haar-random single-site gates, followed by a depolarizing channel with strength $\u03b3$. For Floquet Clifford circuits built from an \\textit{i}SWAP-class two-qubit gate, our analysis identifies two distinct regimes for the Liouvillian gap in the thermodynamic limit, exemplified by the undoped and fully doped extreme cases. In both regimes, the dissipative diagnostic signals chaotic behavior, differing only in how the gap scales with system size. In the undoped circuit, the gap scales as $\u0394\\sim \u03b3N$, whereas in the fully doped circuit it remains finite as $N\\to\\infty$. We find that the doping density $p_h$ governs the crossover: as $p_h\\to 0$, any spatial structure remains undoped-like, whereas for finite $p_h$ certain structures can enter a finite-gap regime. These results are analytically established in the strongly dissipative regime $\u03b3\\gg 1$ by deriving lower bounds on the gap as a function of $p_h$ and explicit finite-gap constructions, and their extension toward $\u03b3\\to 0$ is supported by numerics. Importantly, our analytic treatment depends only on the spatial doping structure, so the same gap scaling persists even when the Haar rotations are independently resampled each Floquet period.", "AI": {"tldr": "\u7814\u7a76Floquet Clifford\u7535\u8def\u4e2d\u901a\u8fc7\u63ba\u6742Haar\u968f\u673a\u5355\u6bd4\u7279\u95e8\u5f15\u5165\u7684\u975eClifford\u6210\u5206\uff0c\u53d1\u73b0\u63ba\u6742\u5bc6\u5ea6\u51b3\u5b9a\u4e86Liouvillian gap\u7684\u6807\u5ea6\u884c\u4e3a\uff0c\u63ed\u793a\u4e86\u91cf\u5b50\u6df7\u6c8c\u7684\u6700\u5c0f\u975eClifford\u8981\u6c42\u3002", "motivation": "\u4f20\u7edf\u91cf\u5b50\u6df7\u6c8c\u8bca\u65ad\u65b9\u6cd5\uff08\u5982\u8c31\u7edf\u8ba1\u3001OTOCs\u3001\u7ea0\u7f20\u589e\u957f\uff09\u662f\u63a2\u9488\u4f9d\u8d56\u7684\u4e14\u7ed3\u679c\u53ef\u80fd\u4e0d\u4e00\u81f4\u3002\u6700\u8fd1\u63d0\u51fa\u7684\u8017\u6563\u8bca\u65ad\u901a\u8fc7\u7cfb\u7edf-\u73af\u5883\u8026\u5408\u4ea7\u751f\u6709\u9650Liouvillian gap\u6765\u6807\u8bb0\u6df7\u6c8c\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u4eceClifford\u52a8\u529b\u5b66\u51fa\u53d1\u9700\u8981\u591a\u5927\u7684\u975eClifford\u504f\u79bb\u624d\u80fd\u51fa\u73b0\u8fd9\u79cd\u5185\u7980\u5f1b\u8c6b\u884c\u4e3a\u3002", "method": "\u7814\u7a76\u7531iSWAP\u7c7b\u4e24\u6bd4\u7279\u95e8\u6784\u6210\u7684Floquet Clifford\u7535\u8def\uff0c\u5728\u5355\u7ad9\u70b9\u4ee5\u5bc6\u5ea6p_h\u63d2\u5165Haar\u968f\u673a\u95e8\uff0c\u968f\u540e\u65bd\u52a0\u53bb\u6781\u5316\u901a\u9053\uff08\u5f3a\u5ea6\u03b3\uff09\uff0c\u5206\u6790\u70ed\u529b\u5b66\u6781\u9650\u4e0b\u7684Liouvillian gap\u3002", "result": "\u53d1\u73b0\u4e24\u4e2a\u4e0d\u540c\u7684gap\u6807\u5ea6\u533a\uff1a\u672a\u63ba\u6742\u6781\u9650\u4e0bgap~\u03b3N\uff08\u968f\u7cfb\u7edf\u5c3a\u5bf8\u7ebf\u6027\u589e\u957f\uff09\uff0c\u5168\u63ba\u6742\u6781\u9650\u4e0bgap\u5728N\u2192\u221e\u65f6\u4fdd\u6301\u6709\u9650\u3002\u63ba\u6742\u5bc6\u5ea6p_h\u63a7\u5236\u4ea4\u53c9\u884c\u4e3a\u3002\u5728\u5f3a\u8017\u6563\u533a\u03b3\u226b1\u89e3\u6790\u63a8\u5bfc\u51fagap\u4e0b\u754c\uff0c\u6570\u503c\u9a8c\u8bc1\u4e86\u03b3\u21920\u7684\u5ef6\u4f38\u3002", "conclusion": "\u7a7a\u95f4\u63ba\u6742\u7ed3\u6784\uff08\u800c\u975eHaar\u65cb\u8f6c\u7684\u65f6\u95f4\u91cd\u91c7\u6837\uff09\u51b3\u5b9a\u4e86gap\u6807\u5ea6\u884c\u4e3a\uff0c\u8868\u660e\u5373\u4f7f\u6781\u5c11\u91cf\u7684\u975eClifford\u6210\u5206\u4e5f\u80fd\u9a71\u52a8\u5185\u7980\u5f1b\u8c6b\uff0c\u4e3a\u7406\u89e3\u91cf\u5b50\u6df7\u6c8c\u7684\u6700\u5c0f\u975eClifford\u8981\u6c42\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2602.18025", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18025", "abs": "https://arxiv.org/abs/2602.18025", "authors": ["Haruki Abe", "Takayuki Osa", "Yusuke Mukuta", "Tatsuya Harada"], "title": "Cross-Embodiment Offline Reinforcement Learning for Heterogeneous Robot Datasets", "comment": "ICLR 2026", "summary": "Scalable robot policy pre-training has been hindered by the high cost of collecting high-quality demonstrations for each platform. In this study, we address this issue by uniting offline reinforcement learning (offline RL) with cross-embodiment learning. Offline RL leverages both expert and abundant suboptimal data, and cross-embodiment learning aggregates heterogeneous robot trajectories across diverse morphologies to acquire universal control priors. We perform a systematic analysis of this offline RL and cross-embodiment paradigm, providing a principled understanding of its strengths and limitations. To evaluate this offline RL and cross-embodiment paradigm, we construct a suite of locomotion datasets spanning 16 distinct robot platforms. Our experiments confirm that this combined approach excels at pre-training with datasets rich in suboptimal trajectories, outperforming pure behavior cloning. However, as the proportion of suboptimal data and the number of robot types increase, we observe that conflicting gradients across morphologies begin to impede learning. To mitigate this, we introduce an embodiment-based grouping strategy in which robots are clustered by morphological similarity and the model is updated with a group gradient. This simple, static grouping substantially reduces inter-robot conflicts and outperforms existing conflict-resolution methods.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5c06\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e0e\u8de8\u5b9e\u4f53\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u5229\u7528\u6765\u81ea\u591a\u79cd\u673a\u5668\u4eba\u5e73\u53f0\u7684\u591a\u6837\u5316\u6b21\u4f18\u6570\u636e\u8fdb\u884c\u673a\u5668\u4eba\u7b56\u7565\u9884\u8bad\u7ec3\uff0c\u4ee5\u89e3\u51b3\u9ad8\u8d28\u91cf\u6f14\u793a\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u6602\u7684\u95ee\u9898\u3002\u7814\u7a76\u53d1\u73b0\u68af\u5ea6\u51b2\u7a81\u662f\u5173\u952e\u9650\u5236\u56e0\u7d20\uff0c\u5e76\u63d0\u51fa\u6309\u5f62\u6001\u76f8\u4f3c\u6027\u8fdb\u884c\u9759\u6001\u5206\u7ec4\u7684\u7b56\u7565\uff0c\u663e\u8457\u51cf\u5c11\u51b2\u7a81\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4e3a\u6bcf\u4e2a\u673a\u5668\u4eba\u5e73\u53f0\u6536\u96c6\u9ad8\u8d28\u91cf\u6f14\u793a\u6570\u636e\u7684\u9ad8\u6602\u6210\u672c\u5236\u7ea6\u4e86\u53ef\u6269\u5c55\u7684\u673a\u5668\u4eba\u7b56\u7565\u9884\u8bad\u7ec3\u3002\u5229\u7528\u8de8\u591a\u79cd\u673a\u5668\u4eba\u5f62\u6001\u7684\u4e30\u5bcc\u6b21\u4f18\u6570\u636e\u80fd\u591f\u4f7f\u9884\u8bad\u7ec3\u66f4\u5177\u53ef\u6269\u5c55\u6027\uff0c\u4f46\u9700\u8981\u7406\u89e3\u5982\u4f55\u6709\u6548\u6574\u5408\u5f02\u6784\u6570\u636e\u6e90\u3002", "method": "\u4f5c\u8005\u7cfb\u7edf\u5206\u6790\u4e86\u7ed3\u5408\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08\u5229\u7528\u4e13\u5bb6\u6570\u636e\u548c\u6b21\u4f18\u6570\u636e\uff09\u4e0e\u8de8\u5b9e\u4f53\u5b66\u4e60\uff08\u805a\u5408\u591a\u79cd\u673a\u5668\u4eba\u5f62\u6001\u8f68\u8ff9\uff09\u7684\u8303\u5f0f\u3002\u4ed6\u4eec\u5728\u5305\u542b16\u79cd\u673a\u5668\u4eba\u5e73\u53f0\u7684\u8fd0\u52a8\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u5b9e\u4f53\u7684\u5206\u7ec4\u7b56\u7565\uff0c\u6309\u5f62\u6001\u76f8\u4f3c\u6027\u805a\u7c7b\u673a\u5668\u4eba\uff0c\u4ee5\u51cf\u8f7b\u673a\u5668\u4eba\u95f4\u7684\u68af\u5ea6\u51b2\u7a81\u3002", "result": "\u7ed3\u5408\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e0e\u8de8\u5b9e\u4f53\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u6b21\u4f18\u6570\u636e\u4e30\u5bcc\u7684\u573a\u666f\u4e0b\u4f18\u4e8e\u7eaf\u884c\u4e3a\u514b\u9686\u3002\u7136\u800c\uff0c\u968f\u7740\u6b21\u4f18\u6570\u636e\u6bd4\u4f8b\u548c\u673a\u5668\u4eba\u7c7b\u578b\u589e\u52a0\uff0c\u4e0d\u540c\u5f62\u6001\u95f4\u7684\u68af\u5ea6\u51b2\u7a81\u4f1a\u963b\u788d\u5b66\u4e60\u3002\u6240\u63d0\u51fa\u7684\u9759\u6001\u5206\u7ec4\u7b56\u7565\u663e\u8457\u51cf\u5c11\u4e86\u8fd9\u4e9b\u51b2\u7a81\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u51b2\u7a81\u89e3\u51b3\u65b9\u6cd5\u3002", "conclusion": "\u8de8\u5b9e\u4f53\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5bf9\u4e8e\u53ef\u6269\u5c55\u673a\u5668\u4eba\u9884\u8bad\u7ec3\u662f\u6709\u6548\u7684\uff0c\u4f46\u9700\u8981\u663e\u5f0f\u5904\u7406\u5f62\u6001\u5dee\u5f02\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u68af\u5ea6\u51b2\u7a81\u3002\u7b80\u5355\u7684\u6309\u5f62\u6001\u76f8\u4f3c\u6027\u9759\u6001\u5206\u7ec4\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u66f4\u590d\u6742\u7684\u51b2\u7a81\u89e3\u51b3\u65b9\u6cd5\u3002"}}
{"id": "2602.17862", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.17862", "abs": "https://arxiv.org/abs/2602.17862", "authors": ["James W. Gardner", "Federico Belliardo", "Gideon Lee", "Tuvia Gefen", "Liang Jiang"], "title": "Quantum superresolution and noise spectroscopy with quantum computing", "comment": "8 pages, 1 figure", "summary": "Quantum metrology of an incoherent signal is a canonical sensing problem related to superresolution and noise spectroscopy. We show that quantum computing can accelerate searches for a weak incoherent signal when the signal and noise are not precisely known. In particular, we consider weak Schur sampling, density matrix exponentiation, and quantum signal processing for testing the rank, purity, and spectral gap of the unknown quantum state to detect the incoherent signal. We show that these algorithms are faster than full-state tomography, which scales with the dimension of the Hilbert space. We apply our results to detecting exoplanets, stochastic gravitational waves, ultralight dark matter, geontropic quantum gravity, and Pauli noise.", "AI": {"tldr": "\u91cf\u5b50\u8ba1\u7b97\u901a\u8fc7\u5f31\u8212\u5c14\u91c7\u6837\u3001\u5bc6\u5ea6\u77e9\u9635\u6307\u6570\u5316\u548c\u91cf\u5b50\u4fe1\u53f7\u5904\u7406\u7b97\u6cd5\uff0c\u52a0\u901f\u5bf9\u672a\u77e5\u5f31\u975e\u76f8\u5e72\u4fe1\u53f7\u7684\u68c0\u6d4b\uff0c\u76f8\u6bd4\u5168\u6001\u5c42\u6790\u6210\u50cf\u663e\u8457\u63d0\u5347\u6548\u7387\uff0c\u53ef\u5e94\u7528\u4e8e\u7cfb\u5916\u884c\u661f\u63a2\u6d4b\u3001\u5f15\u529b\u6ce2\u63a2\u6d4b\u7b49\u9886\u57df", "motivation": "\u89e3\u51b3\u91cf\u5b50\u8ba1\u91cf\u5b66\u4e2d\u68c0\u6d4b\u672a\u77e5\u5f31\u975e\u76f8\u5e72\u4fe1\u53f7\u7684\u96be\u9898\uff08\u6d89\u53ca\u8d85\u5206\u8fa8\u7387\u4e0e\u566a\u58f0\u8c31\u5206\u6790\uff09\uff0c\u7a81\u7834\u5168\u6001\u5c42\u6790\u6210\u50cf\u8ba1\u7b97\u590d\u6742\u5ea6\u968f\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u7ef4\u5ea6\u6307\u6570\u589e\u957f\u7684\u74f6\u9888\uff0c\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u4e2d\u5bf9\u5f31\u4fe1\u53f7\u68c0\u6d4b\u7684\u8feb\u5207\u9700\u6c42", "method": "\u91c7\u7528\u5f31\u8212\u5c14\u91c7\u6837\u3001\u5bc6\u5ea6\u77e9\u9635\u6307\u6570\u5316\u548c\u91cf\u5b50\u4fe1\u53f7\u5904\u7406\u4e09\u79cd\u91cf\u5b50\u7b97\u6cd5\uff0c\u901a\u8fc7\u6d4b\u8bd5\u672a\u77e5\u91cf\u5b50\u6001\u7684\u79e9\u3001\u7eaf\u5ea6\u548c\u8c31\u95f4\u9699\u6765\u68c0\u6d4b\u975e\u76f8\u5e72\u4fe1\u53f7\uff0c\u907f\u514d\u76f4\u63a5\u91cd\u6784\u5b8c\u6574\u91cf\u5b50\u6001", "result": "\u6240\u63d0\u7b97\u6cd5\u8ba1\u7b97\u6548\u7387\u663e\u8457\u4f18\u4e8e\u5168\u6001\u5c42\u6790\u6210\u50cf\uff08\u540e\u8005\u590d\u6742\u5ea6\u968f\u7cfb\u7edf\u7ef4\u5ea6\u6307\u6570\u589e\u957f\uff09\uff0c\u5728\u4fe1\u53f7\u4e0e\u566a\u58f0\u53c2\u6570\u4e0d\u7cbe\u786e\u5df2\u77e5\u65f6\u4ecd\u80fd\u5b9e\u73b0\u52a0\u901f\u68c0\u6d4b", "conclusion": "\u91cf\u5b50\u8ba1\u7b97\u4e3a\u672a\u77e5\u5f31\u4fe1\u53f7\u68c0\u6d4b\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u7cfb\u5916\u884c\u661f\u63a2\u6d4b\u3001\u968f\u673a\u5f15\u529b\u6ce2\u3001\u8d85\u8f7b\u6697\u7269\u8d28\u3001\u91cf\u5b50\u5f15\u529b\u53ca\u6ce1\u5229\u566a\u58f0\u7b49\u524d\u6cbf\u7269\u7406\u95ee\u9898\uff0c\u5c55\u73b0\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b"}}
{"id": "2602.17684", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17684", "abs": "https://arxiv.org/abs/2602.17684", "authors": ["Xiao Zhu", "Xinyu Zhou", "Boyu Zhu", "Hanxu Hu", "Mingzhe Du", "Haotian Zhang", "Huiming Wang", "Zhijiang Guo"], "title": "CodeScaler: Scaling Code LLM Training and Test-Time Inference via Execution-Free Reward Models", "comment": null, "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) has driven recent progress in code large language models by leveraging execution-based feedback from unit tests, but its scalability is fundamentally constrained by the availability and reliability of high-quality test cases. We propose CodeScaler, an execution-free reward model designed to scale both reinforcement learning training and test-time inference for code generation. CodeScaler is trained on carefully curated preference data derived from verified code problems and incorporates syntax-aware code extraction and validity-preserving reward shaping to ensure stable and robust optimization. Across five coding benchmarks, CodeScaler improves Qwen3-8B-Base by an average of +11.72 points, outperforming binary execution-based RL by +1.82 points, and enables scalable reinforcement learning on synthetic datasets without any test cases. At inference time, CodeScaler serves as an effective test-time scaling method, achieving performance comparable to unit test approaches while providing a 10-fold reduction in latency. Moreover, CodeScaler surpasses existing reward models on RM-Bench not only in the code domain (+3.3 points), but also in general and reasoning domains (+2.7 points on average).", "AI": {"tldr": "\u63d0\u51faCodeScaler\uff0c\u4e00\u79cd\u65e0\u9700\u6267\u884c\u7684\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u4ece\u5df2\u9a8c\u8bc1\u4ee3\u7801\u95ee\u9898\u6784\u5efa\u7684\u504f\u597d\u6570\u636e\u8bad\u7ec3\uff0c\u7ed3\u5408\u8bed\u6cd5\u611f\u77e5\u4ee3\u7801\u63d0\u53d6\u548c\u4fdd\u6548\u5956\u52b1\u5851\u5f62\uff0c\u57285\u4e2a\u7f16\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c06Qwen3-8B-Base\u5e73\u5747\u63d0\u534711.72\u5206\uff0c\u6bd4\u57fa\u4e8e\u6267\u884c\u7684RL\u9ad81.82\u5206\uff0c\u4e14\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e10\u500d\uff0c\u5728RM-Bench\u4ee3\u7801\u9886\u57df\u63d0\u53473.3\u5206\uff0c\u901a\u7528\u548c\u63a8\u7406\u9886\u57df\u5e73\u5747\u63d0\u53472.7\u5206\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60(RLVR)\u4e25\u91cd\u4f9d\u8d56\u9ad8\u8d28\u91cf\u5355\u5143\u6d4b\u8bd5\uff0c\u53ef\u6269\u5c55\u6027\u53d7\u9650\uff1b\u9700\u8981\u6446\u8131\u6267\u884c\u7ea6\u675f\uff0c\u5b9e\u73b0\u65e0\u9700\u6d4b\u8bd5\u7528\u4f8b\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u548c\u9ad8\u6548\u63a8\u7406\u3002", "method": "CodeScaler\u91c7\u7528\u6267\u884c\u65e0\u5173\u7684\u5956\u52b1\u6a21\u578b\uff1a1)\u5728\u7cbe\u9009\u7684\u5df2\u9a8c\u8bc1\u4ee3\u7801\u95ee\u9898\u504f\u597d\u6570\u636e\u4e0a\u8bad\u7ec3\uff1b2)\u5f15\u5165\u8bed\u6cd5\u611f\u77e5\u4ee3\u7801\u63d0\u53d6\u786e\u4fdd\u751f\u6210\u6709\u6548\u6027\uff1b3)\u8bbe\u8ba1\u4fdd\u6548\u5956\u52b1\u5851\u5f62\u673a\u5236\u4fdd\u8bc1\u4f18\u5316\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u4e94\u4e2a\u7f16\u7801\u57fa\u51c6\u4e0a\uff0cCodeScaler\u4f7fQwen3-8B-Base\u5e73\u5747\u6027\u80fd\u63d0\u5347+11.72\uff0c\u8d85\u8d8a\u57fa\u4e8e\u6267\u884c\u7684RL\u65b9\u6cd5+1.82\uff1b\u652f\u6301\u65e0\u6d4b\u8bd5\u7528\u4f8b\u7684\u5408\u6210\u6570\u636e\u96c6\u8bad\u7ec3\uff1b\u63a8\u7406\u65f6\u5ef6\u8fdf\u964d\u4f4e10\u500d\u4e14\u6027\u80fd\u5ab2\u7f8e\u5355\u5143\u6d4b\u8bd5\u65b9\u6cd5\uff1b\u5728RM-Bench\u4e0a\uff0c\u4ee3\u7801\u9886\u57df\u63d0\u5347+3.3\uff0c\u901a\u7528\u4e0e\u63a8\u7406\u9886\u57df\u5e73\u5747\u63d0\u5347+2.7\u3002", "conclusion": "CodeScaler\u6210\u529f\u5b9e\u73b0\u4e86\u65e0\u9700\u6267\u884c\u7684\u5f3a\u5316\u5b66\u4e60\u89c4\u6a21\u5316\u8bad\u7ec3\u548c\u6d4b\u8bd5\u65f6\u63a8\u7406\uff0c\u5728\u6027\u80fd\u3001\u6548\u7387\u548c\u901a\u7528\u6027\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5956\u52b1\u5efa\u6a21\u65b9\u6848\u3002"}}
{"id": "2602.18269", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.18269", "abs": "https://arxiv.org/abs/2602.18269", "authors": ["Arthur Rotari", "Mihai A. Macovei"], "title": "Higher-order spatial photon interference versus dipole blockade effect", "comment": "10 pages, 3 figures", "summary": "The steady-state quantum dynamics of three dipole-dipole coupled two-level emitters, fixed at the vertices of an equilateral triangle, and interacting via the environmental thermostat is investigated. We have analytically obtained the populations of the involved three-atom cooperative states as well as of the second- and third-order spatial photon correlation functions of the light scattered by the few-qubit sample. As a consequence, we have demonstrated that this incoherently excited system spontaneously generates streams of single photons possessing sub-Poissonian photon statistics. In analogy to the dipole-dipole blockade, one may expect that at smaller inter particle distances, compared to the photon emission wavelength, the reported phenomenon has the same origin. However, we have shown that the quantum photon features are due to the interaction's nature of the few symmetrically arranged two-level emitters with the surrounding thermal reservoir. Respectively, at larger atomic intervals the effect occurs because of high-order spatial interference phenomena. Sub-wavelength interference fringes can be observed too, via measurements of spatial higher-order photon correlation functions.", "AI": {"tldr": "\u4e09\u539f\u5b50\u7cfb\u7edf\u5728\u70ed\u5e93\u4e2d\u81ea\u53d1\u4ea7\u751f\u5177\u6709\u4e9a\u6cca\u677e\u7edf\u8ba1\u7279\u6027\u7684\u5355\u5149\u5b50\u6d41\uff0c\u5176\u673a\u5236\u6e90\u4e8e\u539f\u5b50\u4e0e\u70ed\u5e93\u7684\u76f8\u4e92\u4f5c\u7528\u53ca\u9ad8\u9636\u7a7a\u95f4\u5e72\u6d89\uff0c\u800c\u975e\u4f20\u7edf\u7684\u5076\u6781\u5c01\u9501\u6548\u5e94", "motivation": "\u63a2\u7d22\u5076\u6781\u8026\u5408\u4e09\u539f\u5b50\u7cfb\u7edf\u5728\u70ed\u73af\u5883\u4e0b\u7684\u7a33\u6001\u91cf\u5b50\u52a8\u529b\u5b66\uff0c\u63ed\u793a\u5176\u6563\u5c04\u5149\u7684\u975e\u7ecf\u5178\u5149\u5b50\u7edf\u8ba1\u7279\u6027\u751f\u6210\u673a\u5236", "method": "\u89e3\u6790\u6c42\u89e3\u4e09\u539f\u5b50\u534f\u540c\u6001\u5e03\u5c45\u6570\u53ca\u4e8c/\u4e09\u9636\u7a7a\u95f4\u5149\u5b50\u5173\u8054\u51fd\u6570\uff0c\u5206\u6790\u4e9a\u6ce2\u957f\u5c3a\u5ea6\u4e0b\u7684\u91cf\u5b50\u5e72\u6d89\u73b0\u8c61", "result": "1. \u7cfb\u7edf\u81ea\u53d1\u4ea7\u751f\u4e9a\u6cca\u677e\u7edf\u8ba1\u5355\u5149\u5b50\u6d41\uff1b2. \u5c0f\u95f4\u8ddd\u65f6\u673a\u5236\u7c7b\u4f3c\u5076\u6781\u5c01\u9501\uff0c\u4f46\u5927\u95f4\u8ddd\u65f6\u6e90\u4e8e\u70ed\u5e93\u76f8\u4e92\u4f5c\u7528\u4e0e\u9ad8\u9636\u5e72\u6d89\uff1b3. \u53ef\u901a\u8fc7\u9ad8\u9636\u5173\u8054\u6d4b\u91cf\u89c2\u6d4b\u4e9a\u6ce2\u957f\u5e72\u6d89\u6761\u7eb9", "conclusion": "\u5bf9\u79f0\u6392\u5217\u7684\u53cc\u80fd\u7ea7\u539f\u5b50\u4e0e\u70ed\u5e93\u7684\u76f8\u4e92\u4f5c\u7528\u662f\u4ea7\u751f\u91cf\u5b50\u5149\u5b50\u7279\u6027\u7684\u6838\u5fc3\u673a\u5236\uff0c\u4e3a\u8bbe\u8ba1\u65b0\u578b\u91cf\u5b50\u5149\u6e90\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\uff0c\u4e14\u6548\u5e94\u5728\u8f83\u5927\u539f\u5b50\u95f4\u8ddd\u4e0b\u4ecd\u53ef\u901a\u8fc7\u9ad8\u9636\u5e72\u6d89\u7ef4\u6301"}}
{"id": "2602.17899", "categories": ["quant-ph", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2602.17899", "abs": "https://arxiv.org/abs/2602.17899", "authors": ["Jiheng Duan", "Fernando Torres-Leal", "John M. Nichol"], "title": "Measuring and correcting nanosecond pulse distortions in quantum-dot spin qubits", "comment": "19 pages, 12 figures", "summary": "Gate-defined semiconductor quantum dots utilize fast electrical control to manipulate spin and charge states of individual electrons. Electrical pulse distortions can limit control fidelities but are difficult to measure at the device level. Here, we use detuning-axis pulsed spectroscopy to characterize baseband pulse distortions in a silicon double quantum-dot. We extract the gate-voltage impulse response and apply a digital pre-distortion filter to eliminate pulse distortions on timescales longer than 1~ns. With the pre-distortion, we reduce the frequency chirp of coherent exchange oscillations in a singlet-triplet qubit. Our results suggest a scalable and tuning-efficient method for characterizing pulse distortions in quantum-dot spin qubits.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.17685", "categories": ["cs.LG", "cs.RO", "physics.space-ph"], "pdf": "https://arxiv.org/pdf/2602.17685", "abs": "https://arxiv.org/abs/2602.17685", "authors": ["Agni Bandyopadhyay", "Gunther Waxenegger-Wilfing"], "title": "Optimal Multi-Debris Mission Planning in LEO: A Deep Reinforcement Learning Approach with Co-Elliptic Transfers and Refueling", "comment": "Presented at Conference: IFAC Workshop on Control Aspects of Multi-Satellite Systems (CAMSAT) 2025 At: Wuerzburg", "summary": "This paper addresses the challenge of multi target active debris removal (ADR) in Low Earth Orbit (LEO) by introducing a unified coelliptic maneuver framework that combines Hohmann transfers, safety ellipse proximity operations, and explicit refueling logic. We benchmark three distinct planning algorithms Greedy heuristic, Monte Carlo Tree Search (MCTS), and deep reinforcement learning (RL) using Masked Proximal Policy Optimization (PPO) within a realistic orbital simulation environment featuring randomized debris fields, keep out zones, and delta V constraints. Experimental results over 100 test scenarios demonstrate that Masked PPO achieves superior mission efficiency and computational performance, visiting up to twice as many debris as Greedy and significantly outperforming MCTS in runtime. These findings underscore the promise of modern RL methods for scalable, safe, and resource efficient space mission planning, paving the way for future advancements in ADR autonomy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u7edf\u4e00\u7684\u5171\u692d\u5706\u673a\u52a8\u6846\u67b6\uff0c\u7ed3\u5408Hohmann\u8f6c\u79fb\u3001\u5b89\u5168\u692d\u5706\u63a5\u8fd1\u64cd\u4f5c\u548c\u663e\u5f0f\u5728\u8f68\u52a0\u6ce8\u903b\u8f91\uff0c\u7528\u4e8e\u4f4e\u5730\u7403\u8f68\u9053\u591a\u76ee\u6807\u4e3b\u52a8\u788e\u7247\u79fb\u9664\u3002\u901a\u8fc7\u5bf9\u6bd4\u8d2a\u5fc3\u542f\u53d1\u5f0f\u3001\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u548c\u63a9\u7801\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08Masked PPO\uff09\u4e09\u79cd\u7b97\u6cd5\uff0c\u5728\u5305\u542b\u968f\u673a\u788e\u7247\u573a\u3001\u7981\u5165\u533a\u548c\u901f\u5ea6\u589e\u91cf\u7ea6\u675f\u7684\u4eff\u771f\u73af\u5883\u4e2d\uff0cMasked PPO\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6548\u7387\u548c\u8ba1\u7b97\u6027\u80fd\uff0c\u53ef\u8bbf\u95ee\u788e\u7247\u6570\u91cf\u662f\u8d2a\u5fc3\u7b97\u6cd5\u7684\u4e24\u500d\uff0c\u4e14\u8fd0\u884c\u65f6\u95f4\u663e\u8457\u4f18\u4e8eMCTS\uff0c\u5c55\u73b0\u4e86\u73b0\u4ee3\u5f3a\u5316\u5b66\u4e60\u5728\u53ef\u6269\u5c55\u3001\u5b89\u5168\u548c\u8d44\u6e90\u9ad8\u6548\u7684\u7a7a\u95f4\u4efb\u52a1\u89c4\u5212\u4e2d\u7684\u5e94\u7528\u524d\u666f\u3002", "motivation": "\u4f4e\u5730\u7403\u8f68\u9053\u7684\u591a\u76ee\u6807\u4e3b\u52a8\u788e\u7247\u79fb\u9664\u9762\u4e34\u673a\u52a8\u89c4\u5212\u590d\u6742\u3001\u5b89\u5168\u7ea6\u675f\u4e25\u683c\u3001\u8d44\u6e90\uff08\u71c3\u6599\uff09\u6709\u9650\u7b49\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u667a\u80fd\u5316\u7684\u81ea\u4e3b\u89c4\u5212\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u5171\u692d\u5706\u673a\u52a8\u6846\u67b6\uff0c\u96c6\u6210Hohmann\u8f6c\u79fb\u3001\u5b89\u5168\u692d\u5706\u63a5\u8fd1\u64cd\u4f5c\u548c\u663e\u5f0f\u5728\u8f68\u52a0\u6ce8\u903b\u8f91\uff1b\u5728\u5305\u542b\u968f\u673a\u788e\u7247\u573a\u3001\u7981\u5165\u533a\u548c\u901f\u5ea6\u589e\u91cf\u7ea6\u675f\u7684\u9ad8\u4fdd\u771f\u8f68\u9053\u4eff\u771f\u73af\u5883\u4e2d\uff0c\u5bf9\u8d2a\u5fc3\u542f\u53d1\u5f0f\u3001\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u548c\u63a9\u7801\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08Masked PPO\uff09\u4e09\u79cd\u89c4\u5212\u7b97\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5728100\u4e2a\u6d4b\u8bd5\u573a\u666f\u4e2d\uff0cMasked PPO\u5b9e\u73b0\u4e86\u6700\u4f18\u5f02\u7684\u4efb\u52a1\u6548\u7387\u548c\u8ba1\u7b97\u6027\u80fd\uff0c\u8bbf\u95ee\u788e\u7247\u6570\u91cf\u53ef\u8fbe\u8d2a\u5fc3\u7b97\u6cd5\u7684\u4e24\u500d\uff0c\u4e14\u76f8\u6bd4\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u5728\u8fd0\u884c\u65f6\u95f4\u4e0a\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u73b0\u4ee3\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08\u7279\u522b\u662fMasked PPO\uff09\u5728\u53ef\u6269\u5c55\u3001\u5b89\u5168\u548c\u8d44\u6e90\u9ad8\u6548\u7684\u7a7a\u95f4\u4efb\u52a1\u89c4\u5212\u65b9\u9762\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u4e3b\u52a8\u788e\u7247\u79fb\u9664\u7684\u81ea\u4e3b\u5316\u53d1\u5c55\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2602.18201", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18201", "abs": "https://arxiv.org/abs/2602.18201", "authors": ["Joseph Bingham", "Netanel Arussy", "Dvir Aran"], "title": "SOMtime the World Ain$'$t Fair: Violating Fairness Using Self-Organizing Maps", "comment": "10 pages, 2 figures, preprint", "summary": "Unsupervised representations are widely assumed to be neutral with respect to sensitive attributes when those attributes are withheld from training. We show that this assumption is false. Using SOMtime, a topology-preserving representation method based on high-capacity Self-Organizing Maps, we demonstrate that sensitive attributes such as age and income emerge as dominant latent axes in purely unsupervised embeddings, even when explicitly excluded from the input. On two large-scale real-world datasets (the World Values Survey across five countries and the Census-Income dataset), SOMtime recovers monotonic orderings aligned with withheld sensitive attributes, achieving Spearman correlations of up to 0.85, whereas PCA and UMAP typically remain below 0.23 (with a single exception reaching 0.31), and against t-SNE and autoencoders which achieve at most 0.34. Furthermore, unsupervised segmentation of SOMtime embeddings produces demographically skewed clusters, demonstrating downstream fairness risks without any supervised task. These findings establish that \\textit{fairness through unawareness} fails at the representation level for ordinal sensitive attributes and that fairness auditing must extend to unsupervised components of machine learning pipelines. We have made the code available at~ https://github.com/JosephBingham/SOMtime", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u654f\u611f\u5c5e\u6027\u5728\u8bad\u7ec3\u4e2d\u88ab\u6392\u9664\uff0c\u65e0\u76d1\u7763\u8868\u5f81\u4ecd\u4f1a\u9690\u5f0f\u7f16\u7801\u8fd9\u4e9b\u4fe1\u606f\uff0c\u6311\u6218\u4e86\"\u65e0\u610f\u8bc6\u5373\u516c\u5e73\"\u7684\u5047\u8bbe\u3002SOMtime\u65b9\u6cd5\u5728\u4e24\u9879\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u663e\u793a\uff0c\u5e74\u9f84\u548c\u6536\u5165\u7b49\u654f\u611f\u5c5e\u6027\u6210\u4e3a\u65e0\u76d1\u7763\u5d4c\u5165\u4e2d\u7684\u4e3b\u5bfc\u6f5c\u5728\u8f74\uff0c\u65af\u76ae\u5c14\u66fc\u76f8\u5173\u6027\u9ad8\u8fbe0.85\u3002", "motivation": "\u6311\u6218\"\u65e0\u610f\u8bc6\u5373\u516c\u5e73\"\u7684\u5e38\u89c1\u5047\u8bbe\uff0c\u5373\u8ba4\u4e3a\u5728\u8bad\u7ec3\u4e2d\u6392\u9664\u654f\u611f\u5c5e\u6027\u5c31\u80fd\u786e\u4fdd\u8868\u5f81\u4e2d\u6027\u3002\u73b0\u6709\u7814\u7a76\u672a\u5145\u5206\u63a2\u7a76\u65e0\u76d1\u7763\u8868\u5f81\u662f\u5426\u771f\u7684\u4e0d\u53d7\u654f\u611f\u5c5e\u6027\u5f71\u54cd\uff0c\u8fd9\u6d89\u53ca\u673a\u5668\u5b66\u4e60\u6d41\u6c34\u7ebf\u4e2d\u7684\u516c\u5e73\u6027\u5ba1\u8ba1\u6f0f\u6d1e\u3002", "method": "\u63d0\u51faSOMtime\u2014\u2014\u4e00\u79cd\u57fa\u4e8e\u9ad8\u5bb9\u91cf\u81ea\u7ec4\u7ec7\u6620\u5c04(SOM)\u7684\u62d3\u6251\u4fdd\u6301\u8868\u5f81\u65b9\u6cd5\u3002\u5728\u4e24\u4e2a\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u96c6(\u4e16\u754c\u4ef7\u503c\u89c2\u8c03\u67e5\u548c\u4eba\u53e3\u666e\u67e5\u6536\u5165\u6570\u636e\u96c6)\u4e0a\uff0c\u8bc4\u4f30\u65e0\u76d1\u7763\u5d4c\u5165\u5bf9\u5e74\u9f84\u3001\u6536\u5165\u7b49\u5e8f\u6570\u654f\u611f\u5c5e\u6027\u7684\u7f16\u7801\u80fd\u529b\u3002", "result": "SOMtime\u5728\u65e0\u76d1\u7763\u5d4c\u5165\u4e2d\u6062\u590d\u4e86\u4e0e\u654f\u611f\u5c5e\u6027\u7684\u5355\u8c03\u6392\u5e8f\uff0c\u65af\u76ae\u5c14\u66fc\u76f8\u5173\u6027\u8fbe0.85\uff0c\u663e\u8457\u4f18\u4e8ePCA(<0.23)\u3001UMAP(<0.31)\u3001t-SNE\u548c\u81ea\u7f16\u7801\u5668(\u22640.34)\u3002\u65e0\u76d1\u7763\u805a\u7c7b\u4ea7\u751f\u4eba\u53e3\u7edf\u8ba1\u5b66\u4e0a\u503e\u659c\u7684\u7c07\uff0c\u663e\u793a\u4e0b\u6e38\u516c\u5e73\u6027\u98ce\u9669\u3002", "conclusion": "\u65e0\u76d1\u7763\u8868\u5f81\u5e76\u975e\u5929\u7136\u516c\u5e73\uff0c\u5e8f\u6570\u654f\u611f\u5c5e\u6027\u53ef\u5728\u8868\u793a\u5c42\u9762\u6cc4\u9732\u3002\"\u65e0\u610f\u8bc6\u5373\u516c\u5e73\"\u5728\u8868\u5f81\u5c42\u9762\u5931\u6548\uff0c\u516c\u5e73\u6027\u5ba1\u8ba1\u5fc5\u987b\u6269\u5c55\u81f3\u673a\u5668\u5b66\u4e60\u6d41\u6c34\u7ebf\u7684\u65e0\u76d1\u7763\u7ec4\u4ef6\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.17933", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.17933", "abs": "https://arxiv.org/abs/2602.17933", "authors": ["Gikyu Yamamoto", "Osamu Hirota"], "title": "Comparison of security mechanisms of Mathematical cipher, Wyner scheme, QKD, and Quantum stream cipher", "comment": "14 pages, 14 figures, Lecture note in the seminar", "summary": "A new generation of global communications technology has been emerging. These systems, which utilize established device technologies and quantum effect devices, require ultra-high speeds, low cost, and strong security. In recent years, global communication systems have faced various practical security challenges depending on their configurations, and research efforts are underway to address these issues. In particular, the issue of the security of physical layer security from microwave wireless systems to quantum optical communication systems is urgent problem. However, concepts of cryptographic schemes have also been diversifying. Typical examples are mathematical ciphers, the Wyner scheme and QKD. Then, the Y-00 protocol has recently emerged as a third pillar cryptographic technology in the optical quantum domain. These security principles differ significantly from one another. This makes it difficult for different fields to understand each other. At this stage, comparative explanations of the security principles underlying these various cryptographic technologies are likely to promote mutual understanding among researchers across different fields. As the first trial, this lecture note explains the security mechanism of the third pillar (Y-00), comparing it with the principles of other mechanisms.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u901a\u8fc7\u5bf9\u6bd4\u6570\u5b66\u5bc6\u7801\u3001Wyner\u65b9\u6848\u548cQKD\u7b49\u5176\u4ed6\u52a0\u5bc6\u673a\u5236\uff0c\u89e3\u91ca\u4f5c\u4e3a\u5149\u5b66\u91cf\u5b50\u9886\u57df\u7b2c\u4e09\u5927\u652f\u67f1\u7684Y-00\u534f\u8bae\u7684\u5b89\u5168\u673a\u5236\uff0c\u4ee5\u4fc3\u8fdb\u8de8\u9886\u57df\u7814\u7a76\u8005\u7684\u76f8\u4e92\u7406\u89e3\u3002", "motivation": "\u65b0\u4e00\u4ee3\u5168\u7403\u901a\u4fe1\u6280\u672f\u9762\u4e34\u9ad8\u901f\u5ea6\u3001\u4f4e\u6210\u672c\u548c\u5f3a\u5b89\u5168\u6027\u7684\u9700\u6c42\uff0c\u800c\u7269\u7406\u5c42\u5b89\u5168\u6210\u4e3a\u4e9f\u5f85\u89e3\u51b3\u7684\u95ee\u9898\u3002\u4e0d\u540c\u52a0\u5bc6\u6280\u672f\uff08\u6570\u5b66\u5bc6\u7801\u3001Wyner\u65b9\u6848\u3001QKD\u3001Y-00\uff09\u7684\u5b89\u5168\u539f\u7406\u5dee\u5f02\u5de8\u5927\uff0c\u5bfc\u81f4\u8de8\u9886\u57df\u7406\u89e3\u56f0\u96be\uff0c\u56e0\u6b64\u9700\u8981\u5bf9\u6bd4\u89e3\u91ca\u6765\u4fc3\u8fdb\u7814\u7a76\u8005\u95f4\u7684\u76f8\u4e92\u7406\u89e3\u3002", "method": "\u91c7\u7528\u5bf9\u6bd4\u5206\u6790\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bb2\u5ea7\u7b14\u8bb0\u7684\u5f62\u5f0f\uff0c\u5c06Y-00\u534f\u8bae\u7684\u5b89\u5168\u673a\u5236\u4e0e\u6570\u5b66\u5bc6\u7801\u3001Wyner\u65b9\u6848\u548cQKD\u7b49\u5176\u4ed6\u52a0\u5bc6\u673a\u5236\u7684\u539f\u7406\u8fdb\u884c\u6bd4\u8f83\u548c\u89e3\u91ca\u3002", "result": "\u6210\u529f\u9610\u91ca\u4e86Y-00\u534f\u8bae\u4f5c\u4e3a\u5149\u5b66\u91cf\u5b50\u57df\u7b2c\u4e09\u5927\u652f\u67f1\u52a0\u5bc6\u6280\u672f\u7684\u72ec\u7279\u5b89\u5168\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u4e0e\u5176\u4ed6\u673a\u5236\u7684\u5bf9\u6bd4\uff0c\u4e3a\u4e0d\u540c\u9886\u57df\u7814\u7a76\u8005\u76f8\u4e92\u7406\u89e3\u5404\u81ea\u7684\u5b89\u5168\u539f\u7406\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "conclusion": "\u5bf9\u6bd4\u89e3\u91ca\u4e0d\u540c\u52a0\u5bc6\u6280\u672f\u7684\u5b89\u5168\u539f\u7406\u662f\u4fc3\u8fdb\u8de8\u9886\u57df\u7814\u7a76\u8005\u76f8\u4e92\u7406\u89e3\u7684\u6709\u6548\u9014\u5f84\uff0c\u672c\u6587\u4f5c\u4e3a\u9996\u6b21\u5c1d\u8bd5\uff0c\u4e3a\u6b64\u7c7b\u7814\u7a76\u5f00\u521b\u4e86\u5148\u4f8b\u3002"}}
{"id": "2602.18291", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18291", "abs": "https://arxiv.org/abs/2602.18291", "authors": ["Zhuoran Li", "Hai Zhong", "Xun Wang", "Qingxin Xia", "Lihua Zhang", "Longbo Huang"], "title": "Diffusing to Coordinate: Efficient Online Multi-Agent Diffusion Policies", "comment": null, "summary": "Online Multi-Agent Reinforcement Learning (MARL) is a prominent framework for efficient agent coordination. Crucially, enhancing policy expressiveness is pivotal for achieving superior performance. Diffusion-based generative models are well-positioned to meet this demand, having demonstrated remarkable expressiveness and multimodal representation in image generation and offline settings. Yet, their potential in online MARL remains largely under-explored. A major obstacle is that the intractable likelihoods of diffusion models impede entropy-based exploration and coordination. To tackle this challenge, we propose among the first \\underline{O}nline off-policy \\underline{MA}RL framework using \\underline{D}iffusion policies (\\textbf{OMAD}) to orchestrate coordination. Our key innovation is a relaxed policy objective that maximizes scaled joint entropy, facilitating effective exploration without relying on tractable likelihood. Complementing this, within the centralized training with decentralized execution (CTDE) paradigm, we employ a joint distributional value function to optimize decentralized diffusion policies. It leverages tractable entropy-augmented targets to guide the simultaneous updates of diffusion policies, thereby ensuring stable coordination. Extensive evaluations on MPE and MAMuJoCo establish our method as the new state-of-the-art across $10$ diverse tasks, demonstrating a remarkable $2.5\\times$ to $5\\times$ improvement in sample efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faOMAD\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u6269\u6563\u6a21\u578b\u5e94\u7528\u4e8e\u5728\u7ebf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u677e\u5f1b\u7b56\u7565\u76ee\u6807\u89e3\u51b3\u4f3c\u7136\u4e0d\u53ef\u5904\u7406\u95ee\u9898\uff0c\u5728MPE\u548cMAMuJoCo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0SOTA\u6027\u80fd\uff0c\u6837\u672c\u6548\u7387\u63d0\u53472.5-5\u500d\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8868\u8fbe\u6f5c\u529b\u5c1a\u672a\u88ab\u5145\u5206\u6316\u6398\uff0c\u4e3b\u8981\u969c\u788d\u662f\u5176\u4e0d\u53ef\u5904\u7406\u7684\u4f3c\u7136\u6027\u963b\u788d\u4e86\u57fa\u4e8e\u71b5\u7684\u63a2\u7d22\u548c\u534f\u8c03\u3002", "method": "\u63d0\u51faOMAD\u6846\u67b6\uff0c\u91c7\u7528\u677e\u5f1b\u7b56\u7565\u76ee\u6807\u6700\u5927\u5316\u7f29\u653e\u8054\u5408\u71b5\u4ee5\u5b9e\u73b0\u6709\u6548\u63a2\u7d22\uff0c\u5728CTDE\u8303\u5f0f\u4e0b\u4f7f\u7528\u8054\u5408\u5206\u5e03\u4ef7\u503c\u51fd\u6570\u548c\u53ef\u5904\u7406\u7684\u71b5\u589e\u5f3a\u76ee\u6807\u4f18\u5316\u5206\u6563\u5f0f\u6269\u6563\u7b56\u7565\u3002", "result": "\u5728MPE\u548cMAMuJoCo\u768410\u4e2a\u4efb\u52a1\u4e0a\u8fbe\u5230\u65b0SOTA\uff0c\u6837\u672c\u6548\u7387\u63d0\u53472.5-5\u500d\u3002", "conclusion": "OMAD\u6846\u67b6\u6210\u529f\u5c06\u6269\u6563\u7b56\u7565\u5e94\u7528\u4e8e\u5728\u7ebf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u5173\u952e\u6311\u6218\uff0c\u5c55\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u548c\u6548\u7387\u4f18\u52bf\u3002"}}
{"id": "2602.17969", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.17969", "abs": "https://arxiv.org/abs/2602.17969", "authors": ["Aygul Azatovna Galimova"], "title": "Distributed Hyperbolic Floquet Codes under Depolarizing and Erasure Noise", "comment": null, "summary": "Distributing qubits across quantum processing units (QPUs) connected by shared entanglement enables scaling beyond monolithic architectures. Hyperbolic Floquet codes use only weight-2 measurements and are good candidates for distributed quantum error correcting codes. We construct hyperbolic and semi-hyperbolic Floquet codes from $\\{8,3\\}$, $\\{10,3\\}$, and $\\{12,3\\}$ tessellations via the Wythoff kaleidoscopic construction with the Low-Index Normal Subgroups (LINS) algorithm and distribute them across QPUs via spectral bisection. The $\\{10,3\\}$ and $\\{12,3\\}$ families are new to hyperbolic Floquet codes.\n  We simulate these distributed codes under four noise models: depolarizing, SDEM3, correlated EM3, and erasure. With depolarizing noise ($p_{\\text{local}} = 0.03\\%$), fine-grained codes achieve non-local pseudo-thresholds up to 3.0\\% for $\\{8,3\\}$, 3.0\\% for $\\{10,3\\}$, and 1.75\\% for $\\{12,3\\}$. Correlated EM3 yields pseudo-thresholds up to 0.75\\% for $\\{8,3\\}$, 0.75\\% for $\\{10,3\\}$, and 0.50\\% for $\\{12,3\\}$; crossing-based thresholds from same-$k$ families are ${\\sim}1.75$--$2.9\\%$ across all tessellations. Using the SDEM3 model, fine-grained codes achieve distributed pseudo-thresholds of 1.75\\% for $\\{8,3\\}$, 1.25\\% for $\\{10,3\\}$, and 1.00\\% for $\\{12,3\\}$. Under erasure noise motivated by spin-optical architectures, thresholds at 1\\% local loss are 35--40\\% for $\\{8,3\\}$, 30--35\\% for $\\{10,3\\}$, and 25--30\\% for $\\{12,3\\}$.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u65b0\u578b\u53cc\u66f2Floquet\u7801\u5e76\u5206\u5e03\u5f0f\u90e8\u7f72\uff0c\u5728\u591a\u79cd\u566a\u58f0\u6a21\u578b\u4e0b\u5b9e\u73b0\u663e\u8457\u9ad8\u4e8e\u4f20\u7edf\u65b9\u6848\u7684\u9519\u8bef\u9608\u503c\uff0c\u4e3a\u53ef\u6269\u5c55\u91cf\u5b50\u8ba1\u7b97\u63d0\u4f9b\u5173\u952e\u6280\u672f\u65b9\u6848", "motivation": "\u89e3\u51b3\u91cf\u5b50\u8ba1\u7b97\u673a\u89c4\u6a21\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5171\u4eab\u7ea0\u7f20\u5c06\u91cf\u5b50\u6bd4\u7279\u5206\u5e03\u5728\u591a\u4e2a\u91cf\u5b50\u5904\u7406\u5355\u5143\u4e0a\uff0c\u7a81\u7834\u5355\u4f53\u67b6\u6784\u9650\u5236", "method": "\u57fa\u4e8eWythoff\u4e07\u82b1\u7b52\u6784\u9020\u548cLINS\u7b97\u6cd5\uff0c\u4ece{8,3}\u3001{10,3}\u3001{12,3}\u9576\u5d4c\u6784\u5efa\u53cc\u66f2/\u534a\u53cc\u66f2Floquet\u7801\uff0c\u5e76\u901a\u8fc7\u8c31\u4e8c\u5206\u6cd5\u5206\u914d\u5230\u91cf\u5b50\u5904\u7406\u5355\u5143", "result": "\u5728\u56db\u79cd\u566a\u58f0\u6a21\u578b\u4e0b\u5b9e\u73b0\u9ad8\u4f2a\u9608\u503c\uff1a\u53bb\u6781\u5316\u566a\u58f0\u4e0b{8,3}/{10,3}\u8fbe3.0%\uff1b\u76f8\u5173EM3\u566a\u58f0\u4e0b\u5747\u8fbe0.75%\uff1bSDEM3\u6a21\u578b\u4e0b{8,3}\u8fbe1.75%\uff1b\u64e6\u9664\u566a\u58f0\u4e0b\u5c40\u90e8\u635f\u59311%\u65f6\u9608\u503c\u8fbe25-40%", "conclusion": "\u53cc\u66f2Floquet\u7801\u662f\u5206\u5e03\u5f0f\u91cf\u5b50\u7ea0\u9519\u7684\u4f18\u826f\u5019\u9009\u65b9\u6848\uff0c{10,3}\u548c{12,3}\u5bb6\u65cf\u4e3a\u9996\u6b21\u5e94\u7528\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u591a\u79cd\u566a\u58f0\u73af\u5883\u4e0b\u5177\u6709\u9ad8\u5bb9\u9519\u6027\uff0c\u4e3a\u6a21\u5757\u5316\u91cf\u5b50\u8ba1\u7b97\u63d0\u4f9b\u5b9e\u7528\u8def\u5f84"}}
{"id": "2602.17688", "categories": ["cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.17688", "abs": "https://arxiv.org/abs/2602.17688", "authors": ["Anton Xue", "Litu Rout", "Constantine Caramanis", "Sanjay Shakkottai"], "title": "AnCoder: Anchored Code Generation via Discrete Diffusion Models", "comment": null, "summary": "Diffusion language models offer a compelling alternative to autoregressive code generation, enabling global planning and iterative refinement of complex program logic. However, existing approaches fail to respect the rigid structure of programming languages and, as a result, often produce broken programs that fail to execute. To address this, we introduce AnchorTree, a framework that explicitly anchors the diffusion process using structured, hierarchical priors native to code. Specifically, AnchorTree uses the abstract syntax tree to prioritize resolving syntactically and semantically salient tokens, such as keywords (e.g., if, while) and identifiers (e.g., variable names), thereby establishing a structural scaffold that guides the remaining generation. We validate this framework via AnCoder, a family of models showing that structurally anchored diffusion offers a parameter-efficient path to high-quality code generation.", "AI": {"tldr": "AnchorTree\u5229\u7528\u62bd\u8c61\u8bed\u6cd5\u6811\u7684\u7ed3\u6784\u5316\u5148\u9a8c\uff0c\u901a\u8fc7\u4f18\u5148\u89e3\u6790\u5173\u952e\u8bcd\u548c\u6807\u8bc6\u7b26\u7b49\u5173\u952e\u6807\u8bb0\u6765\u5f15\u5bfc\u6269\u6563\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u53c2\u6570\u9ad8\u6548\u7684\u9ad8\u8d28\u91cf\u4ee3\u7801\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65f6\u672a\u80fd\u5c0a\u91cd\u7f16\u7a0b\u8bed\u8a00\u7684\u4e25\u683c\u8bed\u6cd5\u7ed3\u6784\uff0c\u5bfc\u81f4\u751f\u6210\u7ed3\u679c\u7ecf\u5e38\u662f\u8bed\u6cd5\u9519\u8bef\u7684\u4e0d\u53ef\u6267\u884c\u7a0b\u5e8f\u3002", "method": "\u63d0\u51faAnchorTree\u6846\u67b6\uff0c\u5229\u7528\u62bd\u8c61\u8bed\u6cd5\u6811\uff08AST\uff09\u5efa\u7acb\u5c42\u6b21\u5316\u7ed3\u6784\u5148\u9a8c\uff0c\u4f18\u5148\u89e3\u6790\u8bed\u6cd5\u548c\u8bed\u4e49\u663e\u8457\u7684\u5173\u952e\u6807\u8bb0\uff08\u5982if\u3001while\u7b49\u5173\u952e\u5b57\u548c\u53d8\u91cf\u540d\uff09\uff0c\u5f62\u6210\u7ed3\u6784\u9aa8\u67b6\u6765\u6307\u5bfc\u6269\u6563\u8fc7\u7a0b\u3002", "result": "\u901a\u8fc7AnCoder\u7cfb\u5217\u6a21\u578b\u9a8c\u8bc1\uff0c\u7ed3\u6784\u951a\u5b9a\u7684\u6269\u6563\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u6761\u53c2\u6570\u9ad8\u6548\u7684\u9ad8\u8d28\u91cf\u4ee3\u7801\u751f\u6210\u8def\u5f84\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5f15\u5165\u4ee3\u7801\u7684\u6811\u72b6\u7ed3\u6784\u5148\u9a8c\uff0cAnchorTree\u6709\u6548\u89e3\u51b3\u4e86\u751f\u6210\u7a0b\u5e8f\u8bed\u6cd5\u9519\u8bef\u95ee\u9898\uff0c\u4f7f\u6269\u6563\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u66f4\u52a0\u53ef\u9760\u548c\u5b9e\u7528\u3002"}}
{"id": "2602.17974", "categories": ["quant-ph", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.17974", "abs": "https://arxiv.org/abs/2602.17974", "authors": ["Zhaonan Meng", "Yuehaw Khoo", "Jiajia Li", "E. Miles Stoudenmire"], "title": "Recursive Sketched Interpolation: Efficient Hadamard Products of Tensor Trains", "comment": "20 pages, 15 figures", "summary": "The Hadamard product of two tensors in the tensor-train (TT) format is a fundamental operation across various applications, such as TT-based function multiplication for nonlinear differential equations or convolutions. However, conventional methods for computing this product typically scale as at least $\\mathcal{O}(\u03c7^4)$ with respect to the TT bond dimension (TT-rank) $\u03c7$, creating a severe computational bottleneck in practice. By combining randomized tensor-train sketching with slice selection via interpolative decomposition, we introduce Recursive Sketched Interpolation (RSI), a ``scale product'' algorithm that computes the Hadamard product of TTs at a computational cost of $\\mathcal{O}(\u03c7^3)$. Benchmarks across various TT scenarios demonstrate that RSI offers superior scalability compared to traditional methods while maintaining comparable accuracy. We generalize RSI to compute more complex operations, including Hadamard products of multiple TTs and other element-wise nonlinear mappings, without increasing the complexity beyond $\\mathcal{O}(\u03c7^3)$.", "AI": {"tldr": "\u63d0\u51fa\u9012\u5f52\u8349\u56fe\u63d2\u503c(RSI)\u7b97\u6cd5\uff0c\u901a\u8fc7\u968f\u673aTT\u8349\u56fe\u4e0e\u63d2\u503c\u5206\u89e3\u7684\u5207\u7247\u9009\u62e9\uff0c\u5c06TT\u683c\u5f0fHadamard\u79ef\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u4eceO(\u03c7\u2074)\u964d\u81f3O(\u03c7\u00b3)\uff0c\u5e76\u53ef\u63a8\u5e7f\u5230\u591aTT\u79ef\u53ca\u5143\u7d20\u7ea7\u975e\u7ebf\u6027\u6620\u5c04", "motivation": "TT\u683c\u5f0fHadamard\u79ef\u662f\u51fd\u6570\u4e58\u6cd5\u548c\u5377\u79ef\u7b49\u5e94\u7528\u7684\u57fa\u7840\u64cd\u4f5c\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u968fTT\u79e9\u03c7\u5448O(\u03c7\u2074)\u589e\u957f\uff0c\u5f62\u6210\u4e25\u91cd\u8ba1\u7b97\u74f6\u9888", "method": "\u7ed3\u5408\u968f\u673a\u5f20\u91cf\u5217(TT)\u8349\u56fe\u6280\u672f\u4e0e\u57fa\u4e8e\u63d2\u503c\u5206\u89e3\u7684\u5207\u7247\u9009\u62e9\uff0c\u8bbe\u8ba1\u9012\u5f52\u8349\u56fe\u63d2\u503c(RSI)\u7b97\u6cd5", "result": "RSI\u5c06\u8ba1\u7b97\u6210\u672c\u964d\u81f3O(\u03c7\u00b3)\uff0c\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u5176\u5177\u6709\u66f4\u4f18\u53ef\u6269\u5c55\u6027\u4e14\u7cbe\u5ea6\u76f8\u5f53\uff1b\u53ef\u63a8\u5e7f\u5230\u591aTT\u7684Hadamard\u79ef\u53ca\u5176\u4ed6\u5143\u7d20\u7ea7\u975e\u7ebf\u6027\u6620\u5c04\uff0c\u590d\u6742\u5ea6\u4e0d\u8d85O(\u03c7\u00b3)", "conclusion": "RSI\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86TT\u683c\u5f0fHadamard\u79ef\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u7a81\u7834\u4f20\u7edf\u65b9\u6cd5\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u4e3a\u5927\u89c4\u6a21\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.17689", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17689", "abs": "https://arxiv.org/abs/2602.17689", "authors": ["Melika Filvantorkaman", "Mohsen Piri"], "title": "Robust Pre-Training of Medical Vision-and-Language Models with Domain-Invariant Multi-Modal Masked Reconstruction", "comment": "28 pages, 3 figures", "summary": "Medical vision-language models show strong potential for joint reasoning over medical images and clinical text, but their performance often degrades under domain shift caused by variations in imaging devices, acquisition protocols, and reporting styles. Existing multi-modal pre-training methods largely overlook robustness, treating it as a downstream adaptation problem. In this work, we propose Robust Multi-Modal Masked Reconstruction (Robust-MMR), a self-supervised pre-training framework that explicitly incorporates robustness objectives into masked vision-language learning. Robust-MMR integrates asymmetric perturbation-aware masking, domain-consistency regularization, and modality-resilience constraints to encourage domain-invariant representations. We evaluate Robust-MMR on multiple medical vision-language benchmarks, including medical visual question answering (VQA-RAD, SLAKE, VQA-2019), cross-domain image-text classification (MELINDA), and robust image-caption retrieval (ROCO). Robust-MMR achieves 78.9% cross-domain accuracy on VQA-RAD, outperforming the strongest baseline by 3.8 percentage points, and reaches 74.6% and 77.0% accuracy on SLAKE and VQA-2019, respectively. Under perturbed evaluation, Robust-MMR improves VQA-RAD accuracy from 69.1% to 75.6%. For image-text classification, cross-domain MELINDA accuracy increases from 70.3% to 75.2%, while retrieval experiments show a reduction in mean rank degradation from over 16 to 4.1 under perturbation. Qualitative results further demonstrate improved clinical reasoning for disease detection and structural abnormality assessment. These findings show that explicitly modeling robustness during pre-training leads to more reliable and transferable medical vision-language representations for real-world deployment.", "AI": {"tldr": "Proposed Robust-MMR, a self-supervised pre-training framework that explicitly incorporates robustness objectives into masked vision-language learning, achieving state-of-the-art cross-domain performance on multiple medical vision-language benchmarks.", "motivation": "Medical vision-language models show strong potential but their performance degrades under domain shift caused by variations in imaging devices, acquisition protocols, and reporting styles, and existing methods treat robustness as a downstream adaptation problem rather than addressing it during pre-training.", "method": "Robust Multi-Modal Masked Reconstruction (Robust-MMR) integrates three key components: asymmetric perturbation-aware masking, domain-consistency regularization, and modality-resilience constraints to encourage domain-invariant representations during self-supervised pre-training.", "result": "Achieves 78.9% cross-domain accuracy on VQA-RAD (outperforming baselines by 3.8 points), 74.6% on SLAKE, 77.0% on VQA-2019; improves perturbed VQA-RAD from 69.1% to 75.6%; increases MELINDA classification from 70.3% to 75.2%; and reduces mean rank degradation from 16 to 4.1 in retrieval tasks.", "conclusion": "Explicitly modeling robustness during pre-training leads to more reliable and transferable medical vision-language representations, demonstrating significant improvements for real-world deployment across diverse medical imaging scenarios."}}
{"id": "2602.17991", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.17991", "abs": "https://arxiv.org/abs/2602.17991", "authors": ["Seokho Jeong", "Minhyuk Kim"], "title": "Enhanced Maximum Independent Set Preparation with Rydberg Atoms Guided by the Spectral Gap", "comment": null, "summary": "Adiabatic quantum computation with Rydberg atoms provides a natural route for solving combinatorial optimization problems such as the maximum independent set (MIS). However, its performance is fundamentally limited by the reduction of the spectral gap with increasing system size and connectivity, which induces population leakage from the ground state during finite-time evolution. Here we introduce the Adjusted Detuning for Ground-Energy Leakage Blockade (ADGLB), a spectral-gap-guided schedule engineering method that modifies the laser detuning profile to suppress leakage without introducing additional Hamiltonian terms or iterative optimization loops. We experimentally benchmark ADGLB on a quasi-one-dimensional chain of $N=10$ atoms, and the MIS preparation probability increases substantially compared with the standard adiabatic schedule. Furthermore, we show that the schedule optimized for smaller instances can be directly applied to larger two-dimensional triangular lattices with $N=25$ and $N=37$. With a small heuristic offset, the method also remains effective for instances with higher hardness parameters. These findings demonstrate that spectral-gap-guided schedule engineering offers a scalable and hardware-efficient strategy for enhancing adiabatic quantum optimization on neutral-atom platforms.", "AI": {"tldr": "\u63d0\u51faADGLB\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u6fc0\u5149\u5931\u8c10\u5256\u9762\u6291\u5236\u7edd\u70ed\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7684\u57fa\u6001\u6cc4\u6f0f\uff0c\u63d0\u5347\u6700\u5927\u72ec\u7acb\u96c6\u95ee\u9898\u6c42\u89e3\u6027\u80fd", "motivation": "\u7edd\u70ed\u91cf\u5b50\u8ba1\u7b97\u5728\u89e3\u51b3\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u65f6\uff0c\u5176\u6027\u80fd\u53d7\u7cfb\u7edf\u5c3a\u5bf8\u548c\u8fde\u901a\u6027\u589e\u52a0\u5bfc\u81f4\u7684\u80fd\u9699\u51cf\u5c0f\u9650\u5236\uff0c\u5f15\u53d1\u6709\u9650\u65f6\u95f4\u6f14\u5316\u4e2d\u57fa\u6001\u6cc4\u6f0f", "method": "\u63d0\u51fa\"\u57fa\u6001\u80fd\u91cf\u6cc4\u6f0f\u963b\u65ad\u7684\u8c03\u6574\u540e\u5931\u8c10\"\u65b9\u6cd5\uff0c\u91c7\u7528\u80fd\u9699\u5f15\u5bfc\u7684\u8c03\u5ea6\u5de5\u7a0b\uff0c\u4fee\u6539\u6fc0\u5149\u5931\u8c10\u5256\u9762\u4ee5\u6291\u5236\u6cc4\u6f0f", "result": "\u5728N=10\u539f\u5b50\u94fe\u4e0a\u5b9e\u9a8c\u9a8c\u8bc1MIS\u5236\u5907\u6982\u7387\u663e\u8457\u63d0\u5347\uff0c\u4f18\u5316\u8c03\u5ea6\u53ef\u76f4\u63a5\u6269\u5c55\u81f3N=25/37\u4e8c\u7ef4\u4e09\u89d2\u6676\u683c\uff0c\u5bf9\u9ad8\u786c\u5ea6\u53c2\u6570\u5b9e\u4f8b\u4ecd\u6709\u6548", "conclusion": "\u80fd\u9699\u5f15\u5bfc\u7684\u8c03\u5ea6\u5de5\u7a0b\u4e3a\u4e2d\u6027\u539f\u5b50\u5e73\u53f0\u4e0a\u7684\u7edd\u70ed\u91cf\u5b50\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u786c\u4ef6\u9ad8\u6548\u7684\u7b56\u7565"}}
{"id": "2602.18011", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.18011", "abs": "https://arxiv.org/abs/2602.18011", "authors": ["Takafumi Oka", "Michal Hajdu\u0161ek", "Shota Nagayama", "Rodney Van Meter"], "title": "A Tailored Fidelity Estimation and Purification Method for Entangled Quantum Networks", "comment": "10 pages, 6 figures; comments welcome", "summary": "We present a method to conduct both quantum state reconstruction and entanglement purification simultaneously that is advantageous in several respects over previous work in this direction, showing that the number of Bell pairs necessary to boot a quantum network can be significantly reduced compared to an existing method. The existing method requires at least $10^5$ Bell pairs for the state reconstruction phase to estimate that the state is of fidelity $0.99$ within the error range of $10^{-2}$, whereas our approach only requires around $2,841$ to be certain with $99.7\\%$ of confidence that the estimated fidelity lies within $[0.99-0.01, 0.99+0.01]$. In addition, in our approach we can start with a lower fidelity Bell pair and purify it multiple times, estimating at the same time the resultant fidelity with guarantee of $99.7\\%$ that the fidelity estimate lies within a certain range. Moreover, the existing method cannot correct both bit-flip and phase-flip errors at the same time and can only correct one of these, whereas our approach can correct both bit-flip and phase-flip errors simultaneously. This research produces numerical estimates for the number of Bell pairs actually needed to guarantee a certain threshold fidelity $F$. The research can support the functioning real-world quantum networking by providing the information of the time needed for the bootstrapping of a quantum network to finish.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.17692", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17692", "abs": "https://arxiv.org/abs/2602.17692", "authors": ["Bin Wang", "Fan Wang", "Pingping Wang", "Jinyu Cong", "Yang Yu", "Yilong Yin", "Zhongyi Han", "Benzheng Wei"], "title": "Agentic Unlearning: When LLM Agent Meets Machine Unlearning", "comment": "9 pages, 6 figures, 6 tables", "summary": "In this paper, we introduce \\textbf{agentic unlearning} which removes specified information from both model parameters and persistent memory in agents with closed-loop interaction. Existing unlearning methods target parameters alone, leaving two critical gaps: (i) parameter-memory backflow, where retrieval reactivates parametric remnants or memory artifacts reintroduce sensitive content, and (ii) the absence of a unified strategy that covers both parameter and memory pathways. We present Synchronized Backflow Unlearning (SBU), a framework that unlearns jointly across parameter and memory pathways. The memory pathway performs dependency closure-based unlearning that prunes isolated entities while logically invalidating shared artifacts. The parameter pathway employs stochastic reference alignment to guide model outputs toward a high-entropy prior. These pathways are integrated via a synchronized dual-update protocol, forming a closed-loop mechanism where memory unlearning and parametric suppression reinforce each other to prevent cross-pathway recontamination. Experiments on medical QA benchmarks show that SBU reduces traces of targeted private information across both pathways with limited degradation on retained data.", "AI": {"tldr": "This paper proposes Synchronized Backflow Unlearning (SBU), a novel framework that jointly removes specified information from both model parameters and persistent memory in AI agents to prevent cross-pathway recontamination, addressing gaps in existing parameter-only unlearning methods.", "motivation": "Existing unlearning methods only target model parameters, creating two critical gaps: (i) parameter-memory backflow where retrieval reactivates parametric remnants or memory artifacts reintroduce sensitive content, and (ii) lack of a unified strategy covering both pathways. The paper aims to develop a comprehensive unlearning approach for agents with closed-loop interaction.", "method": "The SBU framework performs joint unlearning across parameter and memory pathways. The memory pathway uses dependency closure-based unlearning to prune isolated entities while logically invalidating shared artifacts. The parameter pathway employs stochastic reference alignment to guide model outputs toward a high-entropy prior. These pathways are integrated via a synchronized dual-update protocol that forms a closed-loop mechanism to reinforce each other and prevent cross-pathway recontamination.", "result": "Experiments on medical QA benchmarks demonstrate that SBU effectively reduces traces of targeted private information across both parameter and memory pathways while maintaining limited performance degradation on retained data.", "conclusion": "SBU successfully addresses the critical gaps in existing unlearning methods by providing synchronized, closed-loop unlearning across both parameter and memory pathways, preventing cross-pathway recontamination and ensuring comprehensive removal of sensitive information from AI agents."}}
{"id": "2602.18034", "categories": ["quant-ph", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.18034", "abs": "https://arxiv.org/abs/2602.18034", "authors": ["Mohammed Barhoush", "Tomoyuki Morimae", "Ryo Nishimaki", "Takashi Yamakawa"], "title": "Separating Non-Interactive Classical Verification of Quantum Computation from Falsifiable Assumptions", "comment": "36 pages", "summary": "Mahadev [SIAM J. Comput. 2022] introduced the first protocol for classical verification of quantum computation based on the Learning-with-Errors (LWE) assumption, achieving a 4-message interactive scheme. This breakthrough naturally raised the question of whether fewer messages are possible in the plain model. Despite its importance, this question has remained unresolved.\n  In this work, we prove that there is no quantum black-box reduction of non-interactive classical verification of quantum computation of $\\textsf{QMA}$ to any falsifiable assumption. Here, \"non-interactive\" means that after an instance-independent setup, the protocol consists of a single message. This constitutes a strong negative result given that falsifiable assumptions cover almost all standard assumptions used in cryptography, including LWE. Our separation holds under the existence of a $\\textsf{QMA} \\text{-} \\textsf{QCMA}$ gap problem. Essentially, these problems require a slightly stronger assumption than $\\textsf{QMA}\\neq \\textsf{QCMA}$. To support the existence of such problems, we present a construction relative to a quantum unitary oracle.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc1\u660e\u4e86\u57fa\u4e8e\u53ef\u8bc1\u4f2a\u5047\u8bbe\u7684\u975e\u4ea4\u4e92\u5f0f\u7ecf\u5178\u9a8c\u8bc1\u91cf\u5b50\u8ba1\u7b97\u534f\u8bae\u4e0d\u5b58\u5728\u91cf\u5b50\u9ed1\u76d2\u5f52\u7ea6\uff0c\u9664\u975eQMA\u4e0eQCMA\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\u95ee\u9898\u3002", "motivation": "Mahadev\u57fa\u4e8eLWE\u5047\u8bbe\u63d0\u51fa\u4e86\u9996\u4e2a4\u8f6e\u4ea4\u4e92\u5f0f\u7684\u7ecf\u5178\u9a8c\u8bc1\u91cf\u5b50\u8ba1\u7b97\u534f\u8bae\uff0c\u8fd9\u81ea\u7136\u5f15\u53d1\u4e86\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\uff1a\u5728\u666e\u901a\u6a21\u578b\u4e2d\u662f\u5426\u53ef\u80fd\u5b9e\u73b0\u66f4\u5c11\u7684\u4ea4\u4e92\u8f6e\u6b21\uff1f\u7279\u522b\u662f\u975e\u4ea4\u4e92\u5f0f\uff08\u5355\u6d88\u606f\uff09\u534f\u8bae\u662f\u5426\u53ef\u884c\uff1f", "method": "\u4f7f\u7528\u91cf\u5b50\u9ed1\u76d2\u5f52\u7ea6\u6280\u672f\uff0c\u5728\u5047\u8bbe\u5b58\u5728QMA-QCMA\u5dee\u8ddd\u95ee\u9898\u7684\u524d\u63d0\u4e0b\uff0c\u8bc1\u660e\u4e0d\u5b58\u5728\u4eceQMA\u7684\u975e\u4ea4\u4e92\u5f0f\u7ecf\u5178\u9a8c\u8bc1\u91cf\u5b50\u8ba1\u7b97\u534f\u8bae\u5230\u4efb\u4f55\u53ef\u8bc1\u4f2a\u5047\u8bbe\u7684\u91cf\u5b50\u9ed1\u76d2\u5f52\u7ea6\u3002\u540c\u65f6\u6784\u9020\u4e86\u4e00\u4e2a\u91cf\u5b50\u9149\u9884\u8a00\u673a\u6765\u652f\u6301\u8fd9\u7c7b\u95ee\u9898\u7684\u5b58\u5728\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u5b58\u5728QMA-QCMA\u5dee\u8ddd\u95ee\u9898\u7684\u6761\u4ef6\u4e0b\uff0c\u4e0d\u5b58\u5728\u57fa\u4e8e\u4efb\u4f55\u53ef\u8bc1\u4f2a\u5047\u8bbe\uff08\u5305\u62ecLWE\uff09\u7684QMA\u975e\u4ea4\u4e92\u5f0f\u7ecf\u5178\u9a8c\u8bc1\u91cf\u5b50\u8ba1\u7b97\u534f\u8bae\u7684\u91cf\u5b50\u9ed1\u76d2\u5f52\u7ea6\u3002\u8fd9\u662f\u4e00\u4e2a\u5f3a\u70c8\u7684\u5426\u5b9a\u6027\u7ed3\u679c\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u89e3\u51b3\u4e86Mahadev\u63d0\u51fa\u7684\u5f00\u653e\u6027\u95ee\u9898\uff0c\u8868\u660e\u975e\u4ea4\u4e92\u5f0f\u7ecf\u5178\u9a8c\u8bc1\u91cf\u5b50\u8ba1\u7b97\u534f\u8bae\u65e0\u6cd5\u901a\u8fc7\u73b0\u6709\u6280\u672f\u57fa\u4e8e\u6807\u51c6\u5bc6\u7801\u5b66\u5047\u8bbe\u6784\u9020\uff0c\u9664\u975e\u590d\u6742\u6027\u7406\u8bba\u4e2d\u67d0\u4e9b\u66f4\u5f3a\u7684\u5047\u8bbe\u6210\u7acb\u3002"}}
{"id": "2602.17693", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17693", "abs": "https://arxiv.org/abs/2602.17693", "authors": ["Yuchen Luo", "Fangyue Zhu", "Ruining Zhou", "Mingzhe Huang", "Jian Zhu", "Fanyu Fan", "Wei Shao"], "title": "A Case Study of Selected PTQ Baselines for Reasoning LLMs on Ascend NPU", "comment": null, "summary": "Post-Training Quantization (PTQ) is crucial for efficient model deployment, yet its effectiveness on Ascend NPU remains under-explored compared to GPU architectures. This paper presents a case study of representative PTQ baselines applied to reasoning-oriented models such as DeepSeek-R1-Distill-Qwen series (1.5B/7B/14B) and QwQ-32B. We evaluate four distinct algorithms, including AWQ, GPTQ, SmoothQuant, and FlatQuant, to cover the spectrum from weight-only compression to advanced rotation-based methods. Our empirical results reveal significant platform sensitivity. While 4-bit weight-only quantization proves viable for larger models, aggressive 4-bit weight-activation schemes suffer from layer-wise calibration instability on the NPU, leading to logic collapse in long-context reasoning tasks. Conversely, standard 8-bit quantization remains numerically stable. Furthermore, a real-world INT8 deployment demonstrates that although optimized kernels reduce latency, dynamic quantization overheads currently limit end-to-end acceleration. These findings offer a practical reference for the feasibility and limitations of deploying quantized reasoning models on Ascend NPU.", "AI": {"tldr": "\u5bf9\u6607\u817eNPU\u4e0a\u7684PTQ\u91cf\u5316\u65b9\u6cd5\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u53d1\u73b04-bit\u6743\u91cd\u91cf\u5316\u53ef\u884c\u4f46\u6fc0\u6d3b\u91cf\u5316\u4f1a\u5bfc\u81f4\u957f\u6587\u672c\u63a8\u7406\u5d29\u6e83\uff0c8-bit\u66f4\u7a33\u5b9a\u4f46\u52a8\u6001\u91cf\u5316\u5f00\u9500\u9650\u5236\u52a0\u901f\u6548\u679c\u3002", "motivation": "PTQ\u5728\u6607\u817eNPU\u4e0a\u7684\u6709\u6548\u6027\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u4e3a\u63a8\u7406\u6a21\u578b\u5728\u6607\u817e\u5e73\u53f0\u4e0a\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u53c2\u8003\u3002", "method": "\u5bf9DeepSeek-R1-Distill-Qwen\u548cQwQ-32B\u7b49\u63a8\u7406\u6a21\u578b\u8fdb\u884c\u6848\u4f8b\u7814\u7a76\uff0c\u8bc4\u4f30AWQ\u3001GPTQ\u3001SmoothQuant\u548cFlatQuant\u56db\u79cd\u91cf\u5316\u7b97\u6cd5\u3002", "result": "\u53d1\u73b0\u6607\u817eNPU\u5b58\u5728\u663e\u8457\u7684\u5e73\u53f0\u654f\u611f\u6027\uff1a4-bit\u6743\u91cd\u91cf\u5316\u5bf9\u5927\u6a21\u578b\u53ef\u884c\uff0c\u4f464-bit\u6743\u91cd-\u6fc0\u6d3b\u91cf\u5316\u56e0\u9010\u5c42\u6821\u51c6\u4e0d\u7a33\u5b9a\u5bfc\u81f4\u957f\u6587\u672c\u63a8\u7406\u903b\u8f91\u5d29\u6e83\uff1b8-bit\u91cf\u5316\u6570\u503c\u7a33\u5b9a\uff0c\u4f46\u52a8\u6001\u91cf\u5316\u5f00\u9500\u9650\u5236\u4e86\u7aef\u5230\u7aef\u52a0\u901f\u6548\u679c\u3002", "conclusion": "\u4e3a\u5728\u6607\u817eNPU\u4e0a\u90e8\u7f72\u91cf\u5316\u63a8\u7406\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u6027\u53c2\u8003\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u91cf\u5316\u65b9\u6848\u7684\u5c40\u9650\u6027\u548c\u6311\u6218\u3002"}}
{"id": "2602.18044", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.18044", "abs": "https://arxiv.org/abs/2602.18044", "authors": ["Hjalmar Rall"], "title": "Gaussian Dynamical Quantum State Tomography", "comment": "14 pages", "summary": "Standard quantum state tomography assumes sufficient control of a system to measure an informationally complete set of observables. Dynamical quantum state tomography (DQST) presents an alternative: given a system with known dynamics and a single fixed observable, it almost always suffices to control only the time at which each i.i.d. copy of the system is measured. This work presents an analogous scheme for tomography of multi-mode Bosonic Gaussian states undergoing Gaussian evolution, using a fixed single-mode homodyne measurement and only assuming control of the time of measurement. I prove that the scheme enables tomography for all discrete homogenous Gaussian evolutions and Gaussian quantum dynamical semigroups except for a null set which includes unitary evolution. When the state is known to be pure, a smaller number of measurement times is shown to be sufficient.", "AI": {"tldr": "Dynamical Quantum State Tomography (DQST) enables efficient reconstruction of multi-mode Bosonic Gaussian states using only a fixed single-mode homodyne measurement and control of measurement timing, eliminating the need for full observable control required in standard tomography.", "motivation": "Standard quantum tomography requires full control to measure informationally complete observables, which is experimentally demanding. This work aims to reduce control requirements by leveraging known system dynamics and a fixed measurement setup.", "method": "Uses a fixed single-mode homodyne measurement on multi-mode Bosonic Gaussian states undergoing Gaussian evolution. Relies solely on controlling the measurement time for each identical system copy, without varying observables.", "result": "Proves the scheme works for all discrete homogeneous Gaussian evolutions and Gaussian quantum dynamical semigroups except a null set (including unitary evolution). Fewer measurements suffice when the state is known to be pure.", "conclusion": "DQST significantly simplifies experimental complexity for Gaussian state tomography by replacing observable control with timing control, applicable to nearly all Gaussian dynamics with practical advantages for pure states."}}
{"id": "2602.18096", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.18096", "abs": "https://arxiv.org/abs/2602.18096", "authors": ["Jake Horder", "Hugo Quard", "Kenji Watanabe", "Takashi Taniguchi", "Nathan Coste", "Igor Aharonovich"], "title": "Pulsed coherent spectroscopy of a quantum emitter in hexagonal Boron Nitride", "comment": "12 pages, 3 figures", "summary": "Defects in solid-state systems constitute a promising platform for the realization of deterministic quantum emitters. Among many candidate materials and emitters, point defects in hexagonal Boron Nitride (hBN) have recently emerged as particularly promising. In this work, we probe the coherence of an individual B center with a zero phonon line at 436 nm, under pulsed resonant excitation. We observe power-dependent Rabi oscillations up to 5\u03c0, demonstrating optical coherent control of the transition. We achieve an excellent single photon purity of 93% at \u03c0-pulse. Furthermore, we probe the coherence of the two-level system using Ramsey interferometry, revealing an inhomogeneous coherence time of T_2*=0.60 ns. These results establish B centers in hBN as viable candidates for triggered, coherent quantum emitters and represent an important step towards their integration into quantum photonic platforms.", "AI": {"tldr": "This research demonstrates coherent control of individual B centers in hexagonal Boron Nitride (hBN) as quantum emitters, achieving Rabi oscillations, high single-photon purity, and measuring coherence times, advancing their potential for quantum photonic integration.", "motivation": "To establish point defects (specifically B centers) in hexagonal Boron Nitride (hBN) as viable, deterministic quantum emitters by probing their optical coherence properties for quantum technology applications.", "method": "Pulsed resonant excitation to observe power-dependent Rabi oscillations and Ramsey interferometry to probe the coherence of the two-level system of an individual B center (zero phonon line at 436 nm).", "result": "Observed Rabi oscillations up to 5\u03c0, achieved 93% single-photon purity at \u03c0-pulse, and measured an inhomogeneous coherence time (T_2*) of 0.60 ns for the B center transition.", "conclusion": "B centers in hBN are confirmed as promising candidates for triggered, coherent quantum emitters, representing significant progress toward their practical integration into quantum photonic platforms."}}
{"id": "2602.17695", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.17695", "abs": "https://arxiv.org/abs/2602.17695", "authors": ["Xin Yu", "Hanwen Xing", "Lingzhou Xue"], "title": "EXACT: Explicit Attribute-Guided Decoding-Time Personalization", "comment": null, "summary": "Achieving personalized alignment requires adapting large language models to each user's evolving context. While decoding-time personalization offers a scalable alternative to training-time methods, existing methods largely rely on implicit, less interpretable preference representations and impose a rigid, context-agnostic user representation, failing to account for how preferences shift across prompts. We introduce EXACT, a new decoding-time personalization that aligns generation with limited pairwise preference feedback using a predefined set of interpretable attributes. EXACT first identifies user-specific attribute subsets by maximizing the likelihood of preferred responses in the offline stage. Then, for online inference, EXACT retrieves the most semantically relevant attributes for an incoming prompt and injects them into the context to steer generation. We establish theoretical approximation guarantees for the proposed algorithm under mild assumptions, and provably show that our similarity-based retrieval mechanism effectively mitigates contextual preference shifts, adapting to disparate tasks without pooling conflicting preferences. Extensive experiments on human-annotated preference datasets demonstrate that EXACT consistently outperforms strong baselines, including preference modeling accuracy and personalized generation quality.", "AI": {"tldr": "EXACT is a novel decoding-time personalization method that uses interpretable attributes and a two-stage process (offline attribute selection + online retrieval) to adapt LLM generation to user-specific preferences that shift across prompts, achieving both theoretical guarantees and empirical superiority.", "motivation": "Existing decoding-time personalization methods rely on implicit, less interpretable preference representations and use rigid, context-agnostic user representations that fail to account for how preferences shift across different prompts.", "method": "EXACT uses a predefined set of interpretable attributes. In the offline stage, it identifies user-specific attribute subsets by maximizing the likelihood of preferred responses. During online inference, it retrieves the most semantically relevant attributes for an incoming prompt and injects them into the context to steer generation.", "result": "Theoretical approximation guarantees under mild assumptions, provable mitigation of contextual preference shifts, and extensive experiments on human-annotated datasets show EXACT consistently outperforms strong baselines in preference modeling accuracy and personalized generation quality.", "conclusion": "EXACT provides an effective, interpretable, and theoretically-grounded approach for decoding-time personalization that successfully adapts to user preference shifts across varying contexts."}}
{"id": "2602.18103", "categories": ["quant-ph", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2602.18103", "abs": "https://arxiv.org/abs/2602.18103", "authors": ["Carolina del R\u00edo", "Marcos Rub\u00edn-Osanz", "David Rodriguez", "Sebasti\u00e1n Roca-Jerat", "Mar\u00eda Carmen Pallar\u00e9s", "J. Alejandro de Sousa", "Pawe\u0142 Pakulski", "Jos\u00e9 Luis Garc\u00eda Palacios", "Daniel Granados", "Dawid Pinkowicz", "N\u00faria Crivillers", "Anabel Lostao", "David Zueco", "Alicia Gomez", "Fernando Luis"], "title": "Polariton-polariton coherent coupling in a molecular spin-superconductor chip", "comment": "27 pages, 28 figures", "summary": "The ability to establish coherent communication channels is key for scaling up quantum devices. Here, we engineer interactions between distant polaritons, hybrid spin-photon excitations formed at different lumped-element superconducting resonators within a chip. The chip consists of several resonator pairs, slightly detuned in frequency to make them addressable, capacitively coupled within each pair and inductively coupled to a common readout line. They interact locally with samples of PTMr and Tripak$^{-}$ organic free radicals, deposited onto their inductors, which provide model $S = 1/2$, $g \\simeq 2$ spin ensembles. Frequency-dependent microwave transmission experiments, performed at very low temperatures, measure polariton frequencies as a function of magnetic field in different scenarios. When only one resonator within a pair hosts a molecular sample, the results evidence that spins couple remotely to the empty LER as well as to the local cavity mode. If both resonators interact with a spin ensemble, the magnetic field tunes the polariton frequencies relative to each other, on account of the different spin-photon interactions at each LER. When polaritons are brought into mutual resonance, an avoided level crossing emerges that gives direct spectroscopic evidence for a coherent polariton-polariton interaction mediated by the circuit. Pump-probe experiments reveal that the excitation of a polariton within a connected pair is felt, thus it can be read out, by the other one. These observations, backed by model calculations, illustrate the control and detection of distant photon-photon and spin-spin correlations and entanglement in a scalable modular chip.", "AI": {"tldr": "This paper demonstrates coherent communication between distant polaritons in a superconducting chip by engineering interactions between resonator pairs coupled to organic spin ensembles, showing remote coupling and entanglement mediated by the circuit.", "motivation": "The ability to establish coherent communication channels is essential for scaling up quantum devices. The paper aims to demonstrate control and detection of distant photon-photon and spin-spin correlations in a scalable modular architecture.", "method": "The researchers fabricated a chip with resonator pairs (slightly detuned for addressability) capacitively coupled within pairs and inductively coupled to a common readout line. PTM and Triphenylamine organic free radicals (S=1/2, g\u22432 spin ensembles) were deposited onto inductors. Frequency-dependent microwave transmission experiments at very low temperatures measured polariton frequencies versus magnetic field, supplemented by pump-probe experiments to test inter-resonator readout.", "result": "1. When only one resonator hosts spins, the spins couple remotely to both the empty resonator and local cavity mode. 2. When both resonators have spin ensembles, magnetic field tuning reveals an avoided level crossing, providing spectroscopic evidence for coherent polariton-polariton interaction mediated by the circuit. 3. Pump-probe experiments show that excitation of one polariton can be read out by the other, demonstrating inter-resonator communication.", "conclusion": "The work demonstrates control and detection of distant photon-photon and spin-spin correlations and entanglement in a scalable modular chip architecture, establishing coherent communication channels between polaritons and paving the way for scaling up quantum devices."}}
{"id": "2602.18106", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.18106", "abs": "https://arxiv.org/abs/2602.18106", "authors": ["Bin Yi"], "title": "Directional Dynamics of the Non-Hermitian Skin Effect", "comment": null, "summary": "The dynamical consequences of the non-Hermitian skin effect (NHSE) remain largely unexplored despite extensive studies of its static properties. Here we address this gap by applying quantum Liang information flow (QLIF) an inherently directional measure of causal influence to the nonHermitian Su Schrieffer Heeger model with non reciprocal hopping. Unlike symmetric correlation functions, QLIF directly captures the directional asymmetry characteristic of non reciprocal systems. We demonstrate a scissors effect where the asymmetry varies approximately linearly with the non-reciprocity parameter gamma for small gamma, and exhibits non-monotonic dependence on the skin length, with optimal asymmetry at moderate skin localization. The velocity ordering reveals NHSE-induced blocking of information flow against the skin direction. Three distinct temporal regimes emerge: light-cone-bounded spreading, gamma-dependent stabilization, and coherent oscillations. These results establish the first quantitative connection between static skin localization and directional information dynamics, offering new insights into information propagation in non-reciprocal quantum systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u91cf\u5b50Liang\u4fe1\u606f\u6d41\u9996\u6b21\u63ed\u793a\u4e86\u975e\u5384\u7c73\u8d8b\u80a4\u6548\u5e94\u7684\u52a8\u6001\u7279\u6027\uff0c\u53d1\u73b0\u4fe1\u606f\u4f20\u64ad\u65b9\u5411\u4e0d\u5bf9\u79f0\u6027\u4e0e\u975e\u4e92\u6613\u6027\u53c2\u6570\u5448\u7ebf\u6027\u5173\u7cfb\uff0c\u5e76\u5728\u4e2d\u7b49\u8d8b\u80a4\u957f\u5ea6\u65f6\u8fbe\u5230\u6700\u4f18\uff0c\u540c\u65f6\u89c2\u6d4b\u5230\u4e09\u79cd\u4e0d\u540c\u65f6\u95f4\u6f14\u5316\u673a\u5236\u3002", "motivation": "\u5c3d\u7ba1\u975e\u5384\u7c73\u8d8b\u80a4\u6548\u5e94\u7684\u9759\u6001\u6027\u8d28\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5176\u52a8\u529b\u5b66\u540e\u679c\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u65b9\u5411\u6027\u56e0\u679c\u5ea6\u91cf\u6765\u7406\u89e3\u975e\u4e92\u6613\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684\u4fe1\u606f\u4f20\u64ad\u884c\u4e3a\u3002", "method": "\u5c06\u91cf\u5b50Liang\u4fe1\u606f\u6d41\uff08QLIF\uff09\u8fd9\u4e00\u5185\u5728\u65b9\u5411\u6027\u56e0\u679c\u5f71\u54cd\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5e94\u7528\u4e8e\u5177\u6709\u975e\u4e92\u6613\u8dc3\u8fc1\u7684\u975e\u5384\u7c73Su-Schrieffer-Heeger\u6a21\u578b\uff0c\u4ee5\u6355\u6349\u975e\u4e92\u6613\u7cfb\u7edf\u7684\u65b9\u5411\u4e0d\u5bf9\u79f0\u7279\u5f81\u3002", "result": "\u53d1\u73b0\u4e86\u526a\u5200\u6548\u5e94\uff1a\u975e\u4e92\u6613\u6027\u8f83\u5c0f\u65f6\u4e0d\u5bf9\u79f0\u6027\u968f\u53c2\u6570\u03b3\u8fd1\u4f3c\u7ebf\u6027\u53d8\u5316\uff1b\u4e0d\u5bf9\u79f0\u6027\u4e0e\u8d8b\u80a4\u957f\u5ea6\u5448\u975e\u5355\u8c03\u5173\u7cfb\uff0c\u5728\u4e2d\u7b49\u5c40\u57df\u5316\u65f6\u6700\u4f18\uff1bNHSE\u4f1a\u963b\u6321\u9006\u8d8b\u80a4\u65b9\u5411\u7684\u4fe1\u606f\u6d41\uff1b\u7cfb\u7edf\u5448\u73b0\u4e09\u79cd\u65f6\u95f4\u673a\u5236\uff1a\u7c7b\u5149\u9525\u4f20\u64ad\u3001\u03b3\u4f9d\u8d56\u7a33\u5b9a\u5316\u548c\u76f8\u5e72\u632f\u8361\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u5efa\u7acb\u4e86\u9759\u6001\u8d8b\u80a4\u5c40\u57df\u5316\u4e0e\u65b9\u5411\u6027\u4fe1\u606f\u52a8\u529b\u5b66\u4e4b\u95f4\u7684\u5b9a\u91cf\u8054\u7cfb\uff0c\u4e3a\u975e\u4e92\u6613\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684\u4fe1\u606f\u4f20\u64ad\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u62d3\u5c55\u4e86\u5bf9\u975e\u5384\u7c73\u7cfb\u7edf\u52a8\u529b\u5b66\u884c\u4e3a\u7684\u7406\u89e3\u3002"}}
{"id": "2602.17697", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.17697", "abs": "https://arxiv.org/abs/2602.17697", "authors": ["Nada Zine", "Cl\u00e9ment Quinton", "Romain Rouvoy"], "title": "Pimp My LLM: Leveraging Variability Modeling to Tune Inference Hyperparameters", "comment": null, "summary": "Large Language Models (LLMs) are being increasingly used across a wide range of tasks. However, their substantial computational demands raise concerns about the energy efficiency and sustainability of both training and inference. Inference, in particular, dominates total compute usage, making its optimization crucial. Recent research has explored optimization techniques and analyzed how configuration choices influence energy consumption. Yet, the vast configuration space of inference servers makes exhaustive empirical evaluation infeasible due to combinatorial explosion. In this paper, we introduce a new perspective on this problem by treating LLMs as configurable systems and applying variability management techniques to systematically analyze inference-time configuration choices. We evaluate our approach on the Hugging Face Transformers library by representing generation hyperparameters and their constraints using a feature-based variability model, sampling representative configurations, measuring their energy consumption, latency, accuracy, and learning predictive models from the collected data. Our results show that variability modeling effectively manages the complexity of LLM inference configurations. It enables systematic analysis of hyperparameters effects and interactions, reveals trade-offs, and supports accurate prediction of inference behavior from a limited number of measurements. Overall, this work opens a new research direction that bridges software engineering and machine learning by leveraging variability modeling for the efficient and sustainable configuration of LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89c6\u4e3a\u53ef\u914d\u7f6e\u7cfb\u7edf\uff0c\u8fd0\u7528\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u53ef\u53d8\u6027\u7ba1\u7406\u6280\u672f\u6765\u7cfb\u7edf\u5206\u6790\u63a8\u7406\u9636\u6bb5\u7684\u914d\u7f6e\u9009\u62e9\u3002\u901a\u8fc7\u5728Hugging Face Transformers\u5e93\u4e0a\u6784\u5efa\u57fa\u4e8e\u7279\u5f81\u7684\u751f\u6210\u8d85\u53c2\u6570\u6a21\u578b\uff0c\u91c7\u6837\u4ee3\u8868\u6027\u914d\u7f6e\u5e76\u6d4b\u91cf\u80fd\u8017\u3001\u5ef6\u8fdf\u548c\u51c6\u786e\u6027\uff0c\u7ed3\u679c\u8868\u660e\u53ef\u53d8\u6027\u5efa\u6a21\u80fd\u6709\u6548\u7ba1\u7406LLM\u63a8\u7406\u914d\u7f6e\u7684\u590d\u6742\u6027\uff0c\u4ece\u6709\u9650\u6d4b\u91cf\u4e2d\u51c6\u786e\u9884\u6d4b\u63a8\u7406\u884c\u4e3a\uff0c\u63ed\u793a\u6027\u80fd\u6743\u8861\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\u5e26\u6765\u4e86\u5de8\u5927\u7684\u8ba1\u7b97\u9700\u6c42\uff0c\u5f15\u53d1\u4e86\u5bf9\u80fd\u6e90\u6548\u7387\u548c\u53ef\u6301\u7eed\u6027\u7684\u62c5\u5fe7\u3002\u63a8\u7406\u9636\u6bb5\u5360\u636e\u603b\u8ba1\u7b97\u7528\u91cf\u7684\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4f46\u5176\u914d\u7f6e\u7a7a\u95f4\u6781\u5176\u5e9e\u5927\uff0c\u7ec4\u5408\u7206\u70b8\u4f7f\u5f97\u7a77\u5c3d\u5b9e\u8bc1\u8bc4\u4f30\u4e0d\u53ef\u884c\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u5206\u6790\u65b9\u6cd5\u6765\u7406\u89e3\u8d85\u53c2\u6570\u5bf9\u80fd\u8017\u3001\u5ef6\u8fdf\u548c\u51c6\u786e\u6027\u7684\u5f71\u54cd\u3002", "method": "\u5c06LLM\u89c6\u4e3a\u53ef\u914d\u7f6e\u7cfb\u7edf\uff0c\u8fd0\u7528\u53ef\u53d8\u6027\u7ba1\u7406\u6280\u672f\uff1a1) \u4f7f\u7528\u57fa\u4e8e\u7279\u5f81\u7684\u53ef\u53d8\u6027\u6a21\u578b\u8868\u793aHugging Face Transformers\u4e2d\u7684\u751f\u6210\u8d85\u53c2\u6570\u53ca\u5176\u7ea6\u675f\uff1b2) \u91c7\u6837\u4ee3\u8868\u6027\u914d\u7f6e\uff1b3) \u6d4b\u91cf\u5404\u914d\u7f6e\u7684\u80fd\u8017\u3001\u5ef6\u8fdf\u548c\u51c6\u786e\u6027\uff1b4) \u4ece\u6536\u96c6\u7684\u6570\u636e\u4e2d\u5b66\u4e60\u9884\u6d4b\u6a21\u578b\u6765\u63a8\u65ad\u63a8\u7406\u884c\u4e3a\u3002", "result": "\u53ef\u53d8\u6027\u5efa\u6a21\u6709\u6548\u7ba1\u7406\u4e86LLM\u63a8\u7406\u914d\u7f6e\u7684\u590d\u6742\u6027\uff0c\u5b9e\u73b0\u4e86\u5bf9\u8d85\u53c2\u6570\u6548\u5e94\u548c\u76f8\u4e92\u4f5c\u7528\u7684\u7cfb\u7edf\u5206\u6790\uff0c\u63ed\u793a\u4e86\u6027\u80fd\u6743\u8861\u5173\u7cfb\uff0c\u5e76\u80fd\u4ece\u6709\u9650\u6570\u91cf\u7684\u6d4b\u91cf\u4e2d\u51c6\u786e\u9884\u6d4b\u63a8\u7406\u884c\u4e3a\uff0c\u4e3a\u4f18\u5316\u80fd\u8017\u548c\u5ef6\u8fdf\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5f00\u8f9f\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u548c\u673a\u5668\u5b66\u4e60\u4ea4\u53c9\u7684\u65b0\u7814\u7a76\u65b9\u5411\uff0c\u901a\u8fc7\u53ef\u53d8\u6027\u5efa\u6a21\u4e3aLLM\u7684\u9ad8\u6548\u53ef\u6301\u7eed\u914d\u7f6e\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u57fa\u7840\uff0c\u5bf9\u63a8\u52a8\u7eff\u8272AI\u53d1\u5c55\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.17698", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17698", "abs": "https://arxiv.org/abs/2602.17698", "authors": ["Xinlin Li", "Timothy Chou", "Josh Fromm", "Zichang Liu", "Yunjie Pan", "Christina Fragouli"], "title": "ScaleBITS: Scalable Bitwidth Search for Hardware-Aligned Mixed-Precision LLMs", "comment": null, "summary": "Post-training weight quantization is crucial for reducing the memory and inference cost of large language models (LLMs), yet pushing the average precision below 4 bits remains challenging due to highly non-uniform weight sensitivity and the lack of principled precision allocation. Existing solutions use irregular fine-grained mixed-precision with high runtime overhead or rely on heuristics or highly constrained precision allocation strategies. In this work, we propose ScaleBITS, a mixed-precision quantization framework that enables automated, fine-grained bitwidth allocation under a memory budget while preserving hardware efficiency. Guided by a new sensitivity analysis, we introduce a hardware-aligned, block-wise weight partitioning scheme, powered by bi-directional channel reordering. We formulate global bitwidth allocation as a constrained optimization problem and develop a scalable approximation to the greedy algorithm, enabling end-to-end principled allocation. Experiments show that ScaleBITS significantly improves over uniform-precision quantization (up to +36%) and outperforms state-of-the-art sensitivity-aware baselines (up to +13%) in ultra-low-bit regime, without adding runtime overhead.", "AI": {"tldr": "ScaleBITS\u662f\u4e00\u4e2a\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u65b0\u7684\u654f\u611f\u6027\u5206\u6790\u548c\u786c\u4ef6\u5bf9\u9f50\u7684\u5757\u7ea7\u6743\u91cd\u5206\u533a\uff0c\u5728\u5185\u5b58\u9884\u7b97\u4e0b\u5b9e\u73b0\u81ea\u52a8\u5316\u7684\u7ec6\u7c92\u5ea6\u4f4d\u5bbd\u5206\u914d\uff0c\u5728\u8d85\u4f4e\u6bd4\u7279\u573a\u666f\u4e0b\u6bd4\u5747\u5300\u91cf\u5316\u63d0\u534736%\uff0c\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u63d0\u534713%\uff0c\u4e14\u65e0\u8fd0\u884c\u65f6\u5f00\u9500\u3002", "motivation": "\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e73\u5747\u91cf\u5316\u7cbe\u5ea6\u63a8\u81f34\u6bd4\u7279\u4ee5\u4e0b\u5177\u6709\u6311\u6218\u6027\uff0c\u539f\u56e0\u662f\u6743\u91cd\u654f\u611f\u6027\u9ad8\u5ea6\u975e\u5747\u5300\u4e14\u7f3a\u4e4f\u539f\u5219\u6027\u7684\u7cbe\u5ea6\u5206\u914d\u7b56\u7565\u3002\u73b0\u6709\u65b9\u6848\u8981\u4e48\u4f7f\u7528\u4e0d\u89c4\u5219\u7ec6\u7c92\u5ea6\u6df7\u5408\u7cbe\u5ea6\u5e26\u6765\u9ad8\u8fd0\u884c\u65f6\u5f00\u9500\uff0c\u8981\u4e48\u4f9d\u8d56\u542f\u53d1\u5f0f\u6216\u5f3a\u7ea6\u675f\u7b56\u7565\u3002", "method": "\u63d0\u51faScaleBITS\u6846\u67b6\uff0c\u7ed3\u5408\u65b0\u7684\u654f\u611f\u6027\u5206\u6790\u3001\u53cc\u5411\u901a\u9053\u91cd\u6392\u5e8f\u7684\u786c\u4ef6\u5bf9\u9f50\u5757\u7ea7\u6743\u91cd\u5206\u533a\uff0c\u5c06\u5168\u5c40\u4f4d\u5bbd\u5206\u914d\u5efa\u6a21\u4e3a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u8d2a\u5fc3\u7b97\u6cd5\u7684\u53ef\u6269\u5c55\u8fd1\u4f3c\u4ee5\u5b9e\u73b0\u7aef\u5230\u7aef\u539f\u5219\u6027\u5206\u914d\u3002", "result": "\u5728\u8d85\u4f4e\u6bd4\u7279 regime \u4e0b\uff0c\u76f8\u6bd4\u5747\u5300\u7cbe\u5ea6\u91cf\u5316\u663e\u8457\u63d0\u5347\u8fbe36%\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u654f\u611f\u6027\u611f\u77e5\u57fa\u7ebf\u8fbe13%\uff0c\u4e14\u672a\u589e\u52a0\u8fd0\u884c\u65f6\u5f00\u9500\u3002", "conclusion": "ScaleBITS\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u5185\u5b58\u9884\u7b97\u4e0b\u7684\u81ea\u52a8\u5316\u7ec6\u7c92\u5ea6\u4f4d\u5bbd\u5206\u914d\uff0c\u540c\u65f6\u4fdd\u6301\u786c\u4ef6\u6548\u7387\uff0c\u4e3a\u8d85\u4f4e\u6bd4\u7279\u91cf\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u539f\u5219\u6027\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18122", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.18122", "abs": "https://arxiv.org/abs/2602.18122", "authors": ["Fernando Valadares", "Aleksandr Dorogov", "Tanjung Krisnanda", "May Chee Loke", "Ni-Ni Huang", "Pengtao Song", "Yvonne Y. Gao"], "title": "Flux-Activated Resonant Control of a Bosonic Quantum Memory", "comment": null, "summary": "Universal control of bosonic degrees of freedom provides a hardware-efficient route for quantum information processing with high-dimensional systems. Bosonic circuit quantum electrodynamics (cQED), which leverages transmon ancillae to coherently control long-lived superconducting cavities, is well suited to this goal. However, the cavity transitions are nearly degenerate in the usual dispersive regime, which limits the direct addressability of individual excitation levels and increases the complexity of engineered gates. Here, we integrate an on-chip flux-control architecture with a long-lived bosonic memory housed in a 3D superconducting cavity to dynamically access resonant Jaynes-Cummings (JC) interactions, and realize efficient arbitrary rotations between any pair of Fock levels in the memory. This on-demand access to JC interactions offers a versatile toolbox for implementing robust Fock-basis qudits and harnessing the rich dynamics of high-dimensional bosonic elements for quantum information processing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u53ef\u8c03\u8026\u5408\u5668\u5b9e\u73b0\u4e86\u5bf9\u73bb\u8272\u91cf\u5b50\u5b58\u50a8\u5668\u7684\u6309\u9700\u63a7\u5236\uff0c\u53ef\u5728\u4efb\u610fFock\u6001\u4e4b\u95f4\u5b9e\u73b0\u9ad8\u6548\u65cb\u8f6c\uff0c\u89e3\u51b3\u4e86\u9ad8\u7ef4\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u4e2d\u7684\u53ef\u5bfb\u5740\u6027\u95ee\u9898\u3002", "motivation": "\u73bb\u8272\u81ea\u7531\u5ea6\u4e3a\u9ad8\u7ef4\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u63d0\u4f9b\u4e86\u786c\u4ef6\u9ad8\u6548\u7684\u8def\u7ebf\uff0c\u4f46\u4f20\u7edf\u7684\u8272\u6563 regime \u4e2d\u8154\u6a21\u8dc3\u8fc1\u51e0\u4e4e\u7b80\u5e76\uff0c\u9650\u5236\u4e86\u5355\u4e2a\u6fc0\u53d1\u6001\u7684\u76f4\u63a5\u53ef\u5bfb\u5740\u6027\u5e76\u589e\u52a0\u4e86\u91cf\u5b50\u95e8\u8bbe\u8ba1\u7684\u590d\u6742\u5ea6\u3002", "method": "\u4f5c\u8005\u5c06\u7247\u4e0a\u78c1\u901a\u63a7\u5236\u67b6\u6784\u4e0e3D\u8d85\u5bfc\u8154\u4e2d\u7684\u957f\u5bff\u547d\u73bb\u8272\u5b58\u50a8\u5668\u96c6\u6210\uff0c\u901a\u8fc7\u52a8\u6001\u5207\u6362\u81f3\u5171\u632fJaynes-Cummings\u76f8\u4e92\u4f5c\u7528\u6765\u5b9e\u73b0\u5bf9\u8154\u6a21\u7684\u6309\u9700\u64cd\u63a7\u3002", "result": "\u5b9e\u73b0\u4e86\u5728\u5b58\u50a8\u5668\u4efb\u610fFock\u6001\u5bf9\u4e4b\u95f4\u7684\u9ad8\u6548\u4efb\u610f\u65cb\u8f6c\uff0c\u5c55\u793a\u4e86\u5bf9JC\u76f8\u4e92\u4f5c\u7528\u7684\u6309\u9700\u8bbf\u95ee\u80fd\u529b\u3002", "conclusion": "\u8fd9\u79cd\u65b9\u6cd5\u4e3a\u6784\u5efa\u9c81\u68d2\u7684Fock\u57faqudit\u63d0\u4f9b\u4e86\u591a\u529f\u80fd\u5de5\u5177\u7bb1\uff0c\u4e3a\u5229\u7528\u9ad8\u7ef4\u73bb\u8272\u5143\u4ef6\u8fdb\u884c\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u63d0\u4f9b\u4e86\u786c\u4ef6\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17700", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.17700", "abs": "https://arxiv.org/abs/2602.17700", "authors": ["Konstanty Subbotko"], "title": "MIDAS: Mosaic Input-Specific Differentiable Architecture Search", "comment": null, "summary": "Differentiable Neural Architecture Search (NAS) provides efficient, gradient-based methods for automatically designing neural networks, yet its adoption remains limited in practice. We present MIDAS, a novel approach that modernizes DARTS by replacing static architecture parameters with dynamic, input-specific parameters computed via self-attention. To improve robustness, MIDAS (i) localizes the architecture selection by computing it separately for each spatial patch of the activation map, and (ii) introduces a parameter-free, topology-aware search space that models node connectivity and simplifies selecting the two incoming edges per node. We evaluate MIDAS on the DARTS, NAS-Bench-201, and RDARTS search spaces. In DARTS, it reaches 97.42% top-1 on CIFAR-10 and 83.38% on CIFAR-100. In NAS-Bench-201, it consistently finds globally optimal architectures. In RDARTS, it sets the state of the art on two of four search spaces on CIFAR-10. We further analyze why MIDAS works, showing that patchwise attention improves discrimination among candidate operations, and the resulting input-specific parameter distributions are class-aware and predominantly unimodal, providing reliable guidance for decoding.", "AI": {"tldr": "MIDAS modernizes DARTS by introducing dynamic input-specific architecture parameters via self-attention, patchwise localized selection, and a parameter-free topology-aware search space, achieving state-of-the-art results across multiple NAS benchmarks.", "motivation": "Differentiable Neural Architecture Search (NAS) methods like DARTS are efficient but limited in practical adoption due to static architecture parameters and robustness issues.", "method": "Replaces static architecture parameters with dynamic, input-specific parameters computed via self-attention; localizes architecture selection per spatial patch; introduces a parameter-free, topology-aware search space modeling node connectivity to simplify edge selection.", "result": "Achieves 97.42% top-1 accuracy on CIFAR-10 and 83.38% on CIFAR-100 in DARTS space; consistently finds globally optimal architectures in NAS-Bench-201; sets new state-of-the-art on two of four CIFAR-10 search spaces in RDARTS.", "conclusion": "Patchwise attention enhances discrimination among candidate operations, while the resulting input-specific parameter distributions are class-aware and predominantly unimodal, providing reliable guidance for architecture decoding."}}
{"id": "2602.18147", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.18147", "abs": "https://arxiv.org/abs/2602.18147", "authors": ["Justin Yu Xiang Peh", "Darren Ming Zhi Koh", "Zifang Xu", "Xi Jie Yeo", "Peng Kian Tan", "Christian Kurtsiefer"], "title": "Clock Synchronization with Weakly Correlated Photons", "comment": "8 pages, 5 figures", "summary": "Clock synchronization is necessary for communication and distributed computing tasks. Previous schemes based on photon timing correlations use pulsed light or photon pairs for their strong timing correlations. In this work, we demonstrate successful synchronization of crystal clocks using weakly time-correlated photons of 180 ns coherence time from a bunched light source. A synchronization timing jitter of 10 ns is achieved over symmetric -102 dB optical channel loss between two parties, over a span of 25 hours. We also present a model that gives better estimates to the coherence peak finding success probabilities under low signal.", "AI": {"tldr": "\u4f7f\u7528\u5f31\u65f6\u95f4\u76f8\u5173\u5149\u5b50\u5b9e\u73b0\u6676\u4f53\u949f\u540c\u6b65\uff0c\u5728-102 dB\u5149\u635f\u8017\u4e0b\u8fbe\u523010 ns\u6296\u52a8\u7cbe\u5ea6\uff0c\u6301\u7eed25\u5c0f\u65f6\uff0c\u5e76\u63d0\u51fa\u4f4e\u4fe1\u53f7\u6761\u4ef6\u4e0b\u7684\u6539\u8fdb\u6982\u7387\u6a21\u578b", "motivation": "\u4f20\u7edf\u5149\u5b50\u5b9a\u65f6\u76f8\u5173\u65b9\u6848\u4f9d\u8d56\u5f3a\u65f6\u95f4\u76f8\u5173\u7684\u8109\u51b2\u5149\u6216\u5149\u5b50\u5bf9\uff0c\u672c\u7814\u7a76\u63a2\u7d22\u5f31\u65f6\u95f4\u76f8\u5173\u5149\u5b50\u6e90\u5728\u65f6\u949f\u540c\u6b65\u4e2d\u7684\u5e94\u7528\u6f5c\u529b", "method": "\u91c7\u7528\u5177\u6709180 ns\u76f8\u5e72\u65f6\u95f4\u7684\u805a\u675f\u5149\u6e90\u4ea7\u751f\u5f31\u65f6\u95f4\u76f8\u5173\u5149\u5b50\uff0c\u5728\u5bf9\u79f0\u5149\u5b66\u4fe1\u9053\u4e2d\u5b9e\u73b0\u6676\u4f53\u949f\u540c\u6b65", "result": "\u5728-102 dB\u5149\u635f\u8017\u6761\u4ef6\u4e0b\u5b9e\u73b010 ns\u540c\u6b65\u65f6\u5e8f\u6296\u52a8\uff0c\u7cfb\u7edf\u7a33\u5b9a\u8fd0\u884c25\u5c0f\u65f6\uff0c\u5e76\u5efa\u7acb\u4f4e\u4fe1\u53f7\u4e0b\u76f8\u5e72\u5cf0\u68c0\u6d4b\u6210\u529f\u6982\u7387\u7684\u4f18\u5316\u6a21\u578b", "conclusion": "\u5f31\u65f6\u95f4\u76f8\u5173\u5149\u5b50\u53ef\u6709\u6548\u7528\u4e8e\u65f6\u949f\u540c\u6b65\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5177\u6709\u5b9e\u7528\u6027\u4f18\u52bf\uff0c\u65b0\u6a21\u578b\u63d0\u5347\u4f4e\u4fe1\u566a\u6bd4\u573a\u666f\u7684\u9884\u6d4b\u51c6\u786e\u6027"}}
{"id": "2602.17699", "categories": ["cs.LG", "math.RA", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.17699", "abs": "https://arxiv.org/abs/2602.17699", "authors": ["Chandrasekhar Gokavarapu", "Sudhakar Gadde", "Y. Rajasekhar", "S. R. Bhargava"], "title": "Certified Learning under Distribution Shift: Sound Verification and Identifiable Structure", "comment": null, "summary": "Proposition. Let $f$ be a predictor trained on a distribution $P$ and evaluated on a shifted distribution $Q$. Under verifiable regularity and complexity constraints, the excess risk under shift admits an explicit upper bound determined by a computable shift metric and model parameters. We develop a unified framework in which (i) risk under distribution shift is certified by explicit inequalities, (ii) verification of learned models is sound for nontrivial sizes, and (iii) interpretability is enforced through identifiability conditions rather than post hoc explanations. All claims are stated with explicit assumptions. Failure modes are isolated. Non-certifiable regimes are characterized.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u4e3a\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6a21\u578b\u98ce\u9669\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u663e\u5f0f\u4e0a\u754c\uff0c\u5e76\u5c06\u53ef\u89e3\u91ca\u6027\u4e0e\u53ef\u8bc6\u522b\u6027\u6761\u4ef6\u7ed3\u5408\uff0c\u540c\u65f6\u523b\u753b\u4e86\u53ef\u8ba4\u8bc1\u4e0e\u4e0d\u53ef\u8ba4\u8bc1\u7684\u60c5\u5f62\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u5e38\u9762\u4e34\u8bad\u7ec3\u5206\u5e03\u4e0e\u6d4b\u8bd5\u5206\u5e03\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff08\u5206\u5e03\u504f\u79fb\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\u548c\u53ef\u9a8c\u8bc1\u6027\u3002\u672c\u6587\u65e8\u5728\u5efa\u7acb\u53ef\u9a8c\u8bc1\u7684\u5206\u5e03\u504f\u79fb\u98ce\u9669\u4e0a\u754c\uff0c\u5e76\u5c06\u8ba4\u8bc1\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6a21\u578b\u9a8c\u8bc1\u7edf\u4e00\u5728\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\u4e2d\u3002", "method": "\u901a\u8fc7\u5efa\u7acb\u53ef\u8ba1\u7b97\u7684\u504f\u79fb\u5ea6\u91cf\uff0c\u5728\u53ef\u9a8c\u8bc1\u7684\u6b63\u5219\u6027\u548c\u590d\u6742\u5ea6\u7ea6\u675f\u4e0b\uff0c\u63a8\u5bfc\u51fa\u5206\u5e03\u504f\u79fb\u4e0b\u8d85\u989d\u98ce\u9669\u7684\u663e\u5f0f\u4e0a\u754c\u3002\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\uff1a(i) \u7528\u663e\u5f0f\u4e0d\u7b49\u5f0f\u8ba4\u8bc1\u5206\u5e03\u504f\u79fb\u98ce\u9669\uff0c(ii) \u5bf9\u975e\u5e73\u51e1\u89c4\u6a21\u6a21\u578b\u8fdb\u884c\u53ef\u9760\u9a8c\u8bc1\uff0c(iii) \u901a\u8fc7\u53ef\u8bc6\u522b\u6027\u6761\u4ef6\u800c\u975e\u4e8b\u540e\u89e3\u91ca\u6765\u4fdd\u8bc1\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5f97\u5230\u4e86\u4e00\u4e2a\u7531\u53ef\u8ba1\u7b97\u504f\u79fb\u5ea6\u91cf\u548c\u6a21\u578b\u53c2\u6570\u51b3\u5b9a\u7684\u663e\u5f0f\u8d85\u989d\u98ce\u9669\u4e0a\u754c\uff1b\u5efa\u7acb\u4e86\u7edf\u4e00\u7684\u8ba4\u8bc1\u6846\u67b6\uff1b\u660e\u786e\u4e86\u6240\u6709\u5047\u8bbe\u6761\u4ef6\uff1b\u5b64\u7acb\u4e86\u5931\u8d25\u6a21\u5f0f\uff1b\u523b\u753b\u4e86\u4e0d\u53ef\u8ba4\u8bc1\u7684 regime\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5206\u5e03\u504f\u79fb\u95ee\u9898\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u7406\u8bba\u8ba4\u8bc1\u6846\u67b6\uff0c\u660e\u786e\u4e86\u53ef\u8ba4\u8bc1\u4e0e\u4e0d\u53ef\u8ba4\u8bc1\u7684\u8fb9\u754c\uff0c\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u9a8c\u8bc1\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.17751", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17751", "abs": "https://arxiv.org/abs/2602.17751", "authors": ["Nina Brolich", "Simon Geis", "Maximilian Kasper", "Alexander Barnhill", "Axel Plinge", "Dominik Seu\u00df"], "title": "Investigating Target Class Influence on Neural Network Compressibility for Energy-Autonomous Avian Monitoring", "comment": "11 pages, 7 figures, Funding: GreenICT@FMD (BMFTR grant 16ME0491K)", "summary": "Biodiversity loss poses a significant threat to humanity, making wildlife monitoring essential for assessing ecosystem health. Avian species are ideal subjects for this due to their popularity and the ease of identifying them through their distinctive songs. Traditionalavian monitoring methods require manual counting and are therefore costly and inefficient. In passive acoustic monitoring, soundscapes are recorded over long periods of time. The recordings are analyzed to identify bird species afterwards. Machine learning methods have greatly expedited this process in a wide range of species and environments, however, existing solutions require complex models and substantial computational resources. Instead, we propose running machine learning models on inexpensive microcontroller units (MCUs) directly in the field. Due to the resulting hardware and energy constraints, efficient artificial intelligence (AI) architecture is required. In this paper, we present our method for avian monitoring on MCUs. We trained and compressed models for various numbers of target classes to assess the detection of multiple bird species on edge devices and evaluate the influence of the number of species on the compressibility of neural networks. Our results demonstrate significant compression rates with minimal performance loss. We also provide benchmarking results for different hardware platforms and evaluate the feasibility of deploying energy-autonomous devices.", "AI": {"tldr": "Proposes efficient edge AI models for avian monitoring on low-cost microcontrollers, achieving high compression rates with minimal performance loss for multi-species detection in energy-constrained field deployments.", "motivation": "Biodiversity loss threatens ecosystems, and bird song monitoring is effective but traditionally manual/costly. Passive acoustic monitoring with machine learning faces computational/resource barriers on edge devices, hindering scalable, real-time conservation efforts.", "method": "Trained and compressed neural networks for varying numbers of bird species classes; evaluated model compressibility, performance trade-offs, and hardware feasibility across microcontroller units (MCUs) and energy-autonomous platforms.", "result": "Achieved significant model compression (reducing size/computational load) with minimal accuracy loss, even for multi-species detection; benchmarked MCU performance and validated feasibility of solar-powered autonomous deployment.", "conclusion": "Demonstrates that optimized edge AI enables cost-effective, energy-efficient avian monitoring in remote areas, overcoming traditional limitations and supporting scalable biodiversity conservation through deployable autonomous systems."}}
{"id": "2602.18153", "categories": ["quant-ph", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2602.18153", "abs": "https://arxiv.org/abs/2602.18153", "authors": ["Nils Heinisch", "Francesco Salusti", "Mark R. Hogg", "Timon L. Baltisberger", "Malwina A. Marczak", "Sascha R. Valentin", "Arne Ludwig", "Klaus D. J\u00f6ns", "Richard J. Warburton", "Stefan Schumacher"], "title": "High-quality single photons from cavity-enhanced biexciton-to-exciton transition", "comment": "20 pages, 8 figures", "summary": "Resonant laser excitation of a two-level system with subsequent single-photon emission can be used to generate single photons with high indistinguishability or Hong-Ou-Mandel (HOM) visibility. However, spectral overlap between excitation laser and emitted photons generally poses significant challenges. Furthermore, emitter re-excitation intrinsically limits achievable single-photon purity. Established solutions mitigate these issues at significant cost to source efficiency and with increased source complexity. This motivates the use of few-level systems with spectral separation of excitation and emission pathways. One option is a three-level cascade. However, without targeted lifetime engineering of emitting states, the cascade naturally limits achievable photon indistinguishability. Here we study a semiconductor quantum dot with resonant and selective cavity-enhancement of biexciton-to-exciton transition. Following resonant two-photon excitation of the biexciton state, we collect the emitted single photon with the cavity. This approach circumvents emitter re-excitation and naturally introduces spectral separation of excitation laser and emitted single photon. Supported by first experimental results, we demonstrate theoretically that with selective Purcell enhancement, the observed quality quantifiers of single-photon emission (purity, equivalently $g^{(2)}(0)$, and HOM visibility $\\mathcal{V}$, equivalently indistinguishability) are competitive with respect to high-quality deterministic quantum-dot single-photon sources. This is already achieved without systematic optimization or targeted system engineering, which firmly places the reported approach as a viable route to the next generation of highest-quality quantum-dot based deterministic single-photon sources.", "AI": {"tldr": "\u5229\u7528\u534a\u5bfc\u4f53\u91cf\u5b50\u70b9\u53cc\u6fc0\u5b50\u7ea7\u8054\u8dc3\u8fc1\u548c\u8154\u589e\u5f3a\u6548\u5e94\uff0c\u901a\u8fc7\u53cc\u5149\u5b50\u5171\u632f\u6fc0\u53d1\u5b9e\u73b0\u9ad8\u7eaf\u5ea6\u5355\u5149\u5b50\u53d1\u5c04\uff0c\u65e0\u9700\u590d\u6742\u6ee4\u6ce2\u5373\u53ef\u89e3\u51b3\u6fc0\u53d1\u5149\u4e0e\u53d1\u5c04\u5149\u8c31\u91cd\u53e0\u95ee\u9898\uff0c\u4e14\u907f\u514d\u518d\u6fc0\u53d1\u6548\u5e94\uff0c\u6027\u80fd\u5ab2\u7f8e\u73b0\u6709\u6700\u4f18\u91cf\u5b50\u70b9\u5355\u5149\u5b50\u6e90\u3002", "motivation": "\u4f20\u7edf\u53cc\u80fd\u7ea7\u7cfb\u7edf\u5355\u5149\u5b50\u6e90\u5b58\u5728\u6fc0\u53d1\u6fc0\u5149\u4e0e\u53d1\u5c04\u5149\u5b50\u5149\u8c31\u91cd\u53e0\u3001\u53d1\u5c04\u4f53\u518d\u6fc0\u53d1\u5bfc\u81f4\u7eaf\u5ea6\u53d7\u9650\u7b49\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6848\u4ee5\u727a\u7272\u6548\u7387\u548c\u589e\u52a0\u590d\u6742\u5ea6\u4e3a\u4ee3\u4ef7\uff1b\u4e9f\u9700\u901a\u8fc7\u80fd\u7ea7\u5de5\u7a0b\u8bbe\u8ba1\u6fc0\u53d1\u4e0e\u53d1\u5c04\u5149\u8c31\u5206\u79bb\u7684\u9ad8\u6548\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e09\u80fd\u7ea7\u7ea7\u8054\u7cfb\u7edf\u7684\u534a\u5bfc\u4f53\u91cf\u5b50\u70b9\uff0c\u9009\u62e9\u6027\u589e\u5f3a\u53cc\u6fc0\u5b50\u2192\u6fc0\u5b50\u8dc3\u8fc1\u7684Purcell\u6548\u5e94\uff0c\u901a\u8fc7\u53cc\u5149\u5b50\u5171\u632f\u6fc0\u53d1\u53cc\u6fc0\u5b50\u6001\uff0c\u5229\u7528\u8154\u6536\u96c6\u53d1\u5c04\u7684\u5355\u5149\u5b50\u3002", "result": "\u5b9e\u9a8c\u4e0e\u7406\u8bba\u8bc1\u660e\uff1a\u5728\u8154Purcell\u589e\u5f3a\u4e0b\uff0c\u5355\u5149\u5b50\u7eaf\u5ea6\uff08g\u00b2(0)\uff09\u548cHOM\u53ef\u89c1\u5ea6\uff08\ud835\udcb1\uff09\u6307\u6807\u5df2\u8fbe\u9ad8\u8d28\u91cf\u91cf\u5b50\u70b9\u5355\u5149\u5b50\u6e90\u6c34\u5e73\uff0c\u4e14\u65e0\u9700\u7cfb\u7edf\u4f18\u5316\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5929\u7136\u5206\u79bb\u6fc0\u53d1/\u53d1\u5c04\u5149\u8c31\u3001\u89c4\u907f\u518d\u6fc0\u53d1\uff0c\u5728\u4fdd\u6301\u6548\u7387\u4e0e\u7b80\u5316\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fd\u5355\u5149\u5b50\u53d1\u5c04\uff0c\u662f\u4e0b\u4e00\u4ee3\u786e\u5b9a\u6027\u91cf\u5b50\u70b9\u5355\u5149\u5b50\u6e90\u7684\u53ef\u884c\u8def\u7ebf\u3002"}}
{"id": "2602.18156", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.18156", "abs": "https://arxiv.org/abs/2602.18156", "authors": ["T. J. Walstra", "A. J. Hasenack", "P. W. H. Pinkse", "B. Skoric"], "title": "Dispersive Hong-Ou-Mandel Interference with Finite Coincidence Windows", "comment": null, "summary": "Hong-Ou-Mandel (HOM) interference is a fundamental tool for assessing photon indistinguishability in quantum information processing. While the effect of chromatic dispersion on HOM interference has been widely studied, the interplay between dispersion and the finite detection window of realistic measurement devices remains under-explored. In this work, we demonstrate that the rectangular coincidence window inherent to modern time-tagging modules, which effectively acts as a temporal filter, breaks the standard dispersion cancellation condition and restores sensitivity to symmetric group velocity dispersion. We derive an analytical model for type-II SPDC processes that predicts a modification of the HOM dip shape, specifically the emergence of characteristic oscillations and dip broadening. We experimentally validate this theoretical framework using a ppKTP source and transmission through optical fibers of lengths up to 29 km. The experimental data show excellent agreement with the model, confirming the presence of window-induced oscillations and allowing for the precise extraction of the fiber dispersion parameter. These findings underscore the importance of accounting for finite timing resolution in the design and characterization of dispersive quantum communication links.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5b9e\u9645\u6d4b\u91cf\u8bbe\u5907\u7684\u77e9\u5f62\u7b26\u5408\u65f6\u95f4\u7a97\u53e3\u4f1a\u7834\u574f\u8272\u6563\u62b5\u6d88\u6761\u4ef6\uff0c\u4f7fHOM\u5e72\u6d89\u91cd\u65b0\u654f\u611f\u4e8e\u5bf9\u79f0\u7fa4\u901f\u5ea6\u8272\u6563\uff0c\u4ea7\u751f\u7279\u5f81\u632f\u8361\u548c\u8c37\u503c\u5c55\u5bbd", "motivation": "\u73b0\u6709\u7814\u7a76\u5e7f\u6cdb\u63a2\u8ba8\u8272\u6563\u5bf9HOM\u5e72\u6d89\u7684\u5f71\u54cd\uff0c\u4f46\u672a\u5145\u5206\u63a2\u7d22\u8272\u6563\u4e0e\u771f\u5b9e\u6d4b\u91cf\u8bbe\u5907\u6709\u9650\u65f6\u95f4\u7a97\u53e3\u7684\u76f8\u4e92\u4f5c\u7528\u673a\u5236", "method": "\u9488\u5bf9II\u578bSPDC\u8fc7\u7a0b\u5efa\u7acb\u5206\u6790\u6a21\u578b\uff0c\u7ed3\u5408ppKTP\u5149\u6e90\u548c\u957f\u8fbe29\u516c\u91cc\u5149\u7ea4\u4f20\u8f93\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u6846\u67b6", "result": "\u5b9e\u9a8c\u6570\u636e\u4e0e\u6a21\u578b\u9ad8\u5ea6\u543b\u5408\uff0c\u89c2\u6d4b\u5230\u7a97\u53e3\u8bf1\u5bfc\u632f\u8361\u73b0\u8c61\uff0c\u5e76\u7cbe\u786e\u63d0\u53d6\u5149\u7ea4\u8272\u6563\u53c2\u6570", "conclusion": "\u8bbe\u8ba1\u8272\u6563\u91cf\u5b50\u901a\u4fe1\u94fe\u8def\u65f6\u5fc5\u987b\u8003\u8651\u6709\u9650\u65f6\u95f4\u5206\u8fa8\u7387\u7684\u5f71\u54cd\uff0c\u8be5\u53d1\u73b0\u5bf9\u91cf\u5b50\u901a\u4fe1\u7cfb\u7edf\u4f18\u5316\u5177\u6709\u5173\u952e\u6307\u5bfc\u610f\u4e49"}}
{"id": "2602.17706", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17706", "abs": "https://arxiv.org/abs/2602.17706", "authors": ["Rongyao Cai", "Yuxi Wan", "Kexin Zhang", "Ming Jin", "Zhiqiang Ge", "Qingsong Wen", "Yong Liu"], "title": "Parallel Complex Diffusion for Scalable Time Series Generation", "comment": null, "summary": "Modeling long-range dependencies in time series generation poses a fundamental trade-off between representational capacity and computational efficiency. Traditional temporal diffusion models suffer from local entanglement and the $\\mathcal{O}(L^2)$ cost of attention mechanisms. We address these limitations by introducing PaCoDi (Parallel Complex Diffusion), a spectral-native architecture that decouples generative modeling in the frequency domain. PaCoDi fundamentally alters the problem topology: the Fourier Transform acts as a diagonalizing operator, converting locally coupled temporal signals into globally decorrelated spectral components. Theoretically, we prove the Quadrature Forward Diffusion and Conditional Reverse Factorization theorem, demonstrating that the complex diffusion process can be split into independent real and imaginary branches. We bridge the gap between this decoupled theory and data reality using a \\textbf{Mean Field Theory (MFT) approximation} reinforced by an interactive correction mechanism. Furthermore, we generalize this discrete DDPM to continuous-time Frequency SDEs, rigorously deriving the Spectral Wiener Process describe the differential spectral Brownian motion limit. Crucially, PaCoDi exploits the Hermitian Symmetry of real-valued signals to compress the sequence length by half, achieving a 50% reduction in attention FLOPs without information loss. We further derive a rigorous Heteroscedastic Loss to handle the non-isotropic noise distribution on the compressed manifold. Extensive experiments show that PaCoDi outperforms existing baselines in both generation quality and inference speed, offering a theoretically grounded and computationally efficient solution for time series modeling.", "AI": {"tldr": "PaCoDi is a spectral-native diffusion architecture that performs generative modeling in the frequency domain to overcome computational inefficiencies of traditional temporal diffusion models. By exploiting Fourier Transform's diagonalizing property and Hermitian symmetry, it achieves 50% reduction in attention costs while maintaining quality.", "motivation": "Traditional temporal diffusion models face a fundamental trade-off between representational capacity and computational efficiency due to local entanglement and O(L\u00b2) attention costs, making long-range dependency modeling infeasible for long sequences.", "method": "The authors introduce Parallel Complex Diffusion (PaCoDi) that: 1) operates in frequency domain where Fourier Transform diagonalizes local couplings into decorrelated spectral components; 2) splits complex diffusion into independent real/imaginary branches via Quadrature Forward Diffusion theorem; 3) uses Mean Field Theory approximation with correction mechanism; 4) generalizes to continuous-time Frequency SDEs with Spectral Wiener Process; 5) exploits Hermitian symmetry for 2\u00d7 sequence compression and derives Heteroscedastic Loss for non-isotropic noise.", "result": "PaCoDi achieves a 50% reduction in attention FLOPs without information loss and outperforms existing baselines in both generation quality and inference speed across extensive experiments.", "conclusion": "PaCoDi provides a theoretically grounded, computationally efficient solution for time series modeling by fundamentally rethinking diffusion in the spectral domain, effectively resolving the capacity-efficiency trade-off."}}
{"id": "2602.17868", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17868", "abs": "https://arxiv.org/abs/2602.17868", "authors": ["Vasilii Feofanov", "Songkang Wen", "Jianfeng Zhang", "Lujia Pan", "Ievgen Redko"], "title": "MantisV2: Closing the Zero-Shot Gap in Time Series Classification with Synthetic Data and Test-Time Strategies", "comment": null, "summary": "Developing foundation models for time series classification is of high practical relevance, as such models can serve as universal feature extractors for diverse downstream tasks. Although early models such as Mantis have shown the promise of this approach, a substantial performance gap remained between frozen and fine-tuned encoders. In this work, we introduce methods that significantly strengthen zero-shot feature extraction for time series. First, we introduce Mantis+, a variant of Mantis pre-trained entirely on synthetic time series. Second, through controlled ablation studies, we refine the architecture and obtain MantisV2, an improved and more lightweight encoder. Third, we propose an enhanced test-time methodology that leverages intermediate-layer representations and refines output-token aggregation. In addition, we show that performance can be further improved via self-ensembling and cross-model embedding fusion. Extensive experiments on UCR, UEA, Human Activity Recognition (HAR) benchmarks, and EEG datasets show that MantisV2 and Mantis+ consistently outperform prior time series foundation models, achieving state-of-the-art zero-shot performance.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5f15\u5165Mantis+\uff08\u57fa\u4e8e\u5408\u6210\u6570\u636e\u9884\u8bad\u7ec3\uff09\u548cMantisV2\uff08\u4f18\u5316\u67b6\u6784\uff09\uff0c\u7ed3\u5408\u6539\u8fdb\u7684\u6d4b\u8bd5\u65f6\u65b9\u6cd5\u548c\u96c6\u6210\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217\u7684\u96f6\u6837\u672c\u7279\u5f81\u63d0\u53d6\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5f00\u53d1\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u7684\u57fa\u7840\u6a21\u578b\u5177\u6709\u91cd\u8981\u7684\u5b9e\u8df5\u610f\u4e49\uff0c\u53ef\u4f5c\u4e3a\u901a\u7528\u7279\u5f81\u63d0\u53d6\u5668\u3002\u5c3d\u7ba1\u65e9\u671f\u6a21\u578b\uff08\u5982Mantis\uff09\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u51bb\u7ed3\u7f16\u7801\u5668\u4e0e\u5fae\u8c03\u7f16\u7801\u5668\u4e4b\u95f4\u4ecd\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\uff1a1) Mantis+\uff0c\u5b8c\u5168\u4f7f\u7528\u5408\u6210\u65f6\u95f4\u5e8f\u5217\u8fdb\u884c\u9884\u8bad\u7ec3\uff1b2) \u901a\u8fc7\u6d88\u878d\u7814\u7a76\u4f18\u5316\u67b6\u6784\uff0c\u5f97\u5230\u66f4\u8f7b\u91cf\u7684MantisV2\u7f16\u7801\u5668\uff1b3) \u589e\u5f3a\u7684\u6d4b\u8bd5\u65f6\u65b9\u6cd5\uff0c\u5229\u7528\u4e2d\u95f4\u5c42\u8868\u793a\u5e76\u4f18\u5316\u8f93\u51fatoken\u805a\u5408\uff1b4) \u901a\u8fc7\u81ea\u96c6\u6210\u548c\u8de8\u6a21\u578b\u5d4c\u5165\u878d\u5408\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728UCR\u3001UEA\u3001\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u548cEEG\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMantisV2\u548cMantis+\u59cb\u7ec8\u4f18\u4e8e\u5148\u524d\u7684\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u589e\u5f3a\u4e86\u65f6\u95f4\u5e8f\u5217\u7684\u96f6\u6837\u672c\u7279\u5f81\u63d0\u53d6\u80fd\u529b\uff0cMantisV2\u548cMantis+\u5728\u96f6\u6837\u672c\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u65b9\u9762\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002"}}
{"id": "2602.18177", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.18177", "abs": "https://arxiv.org/abs/2602.18177", "authors": ["Unathi Skosana", "Byron Alexander", "Changhyoup Lee", "Mark Tame"], "title": "Experimental realization of a photonic weighted graph state for quantum metrology", "comment": "11 pages, 8 figures, appendix", "summary": "Quantum metrology seeks to push the boundaries of measurement precision by harnessing quantum phenomena. Conventional methods often rely on maximally entangled resources, with states that are usually challenging to produce and sustain in practical setups. Here, we show that the maximally entangled constraint can be lifted by experimentally realizing a photonic two-qubit weighted graph state with an arbitrarily tunable graph weight. We use the generated state as a resource for quantum-enhanced phase sensing. We experimentally characterize the state and study its minimum estimator variance for two distinct local measurement bases as the graph weight varies from the maximally entangled to weakly entangled limit. We find excellent quantitative agreement with theoretical predictions, and observe a gain in precision beyond the classically attainable precision limit for graph weights substantially below the maximally entangled limit. This confirms that considerably less entanglement is required to achieve a quantum advantage. Albeit non-scalable in our test setup, this work represents the first experimental realization of weighted graph states with a tunable graph weight using linear optics. We expect more scalable versions of the model to be possible in an on-chip photonic platform.", "AI": {"tldr": "\u5b9e\u9a8c\u8bc1\u660e\u91cf\u5b50\u8ba1\u91cf\u5b66\u65e0\u9700\u6700\u5927\u7ea0\u7f20\u6001\uff0c\u53ef\u8c03\u52a0\u6743\u56fe\u6001\u5728\u5f31\u7ea0\u7f20\u4e0b\u4ecd\u53ef\u5b9e\u73b0\u8d85\u7ecf\u5178\u7cbe\u5ea6", "motivation": "\u4f20\u7edf\u91cf\u5b50\u8ba1\u91cf\u4f9d\u8d56\u96be\u4ee5\u5236\u5907\u548c\u7ef4\u6301\u7684\u6700\u5927\u7ea0\u7f20\u6001\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528", "method": "\u5229\u7528\u7ebf\u6027\u5149\u5b66\u5b9e\u9a8c\u5b9e\u73b0\u5149\u5b50\u53cc\u91cf\u5b50\u6bd4\u7279\u53ef\u8c03\u52a0\u6743\u56fe\u6001\uff0c\u4f5c\u4e3a\u91cf\u5b50\u76f8\u4f4d\u4f20\u611f\u8d44\u6e90", "result": "\u5b9e\u9a8c\u89c2\u6d4b\u5230\u5728\u975e\u6700\u5927\u7ea0\u7f20\u6743\u91cd\u4e0b\u4ecd\u7a81\u7834\u7ecf\u5178\u7cbe\u5ea6\u6781\u9650\uff0c\u4e0e\u7406\u8bba\u9884\u6d4b\u9ad8\u5ea6\u543b\u5408", "conclusion": "\u9a8c\u8bc1\u4e86\u91cf\u5b50\u4f18\u52bf\u6240\u9700\u7ea0\u7f20\u5ea6\u8fdc\u4f4e\u4e8e\u6700\u5927\u7ea0\u7f20\u6001\uff0c\u4e3a\u975e\u6700\u5927\u7ea0\u7f20\u8d44\u6e90\u5e94\u7528\u5f00\u8f9f\u65b0\u9014\u5f84"}}
{"id": "2602.17743", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.17743", "abs": "https://arxiv.org/abs/2602.17743", "authors": ["Di Zhang"], "title": "Provable Adversarial Robustness in In-Context Learning", "comment": "16 pages", "summary": "Large language models adapt to new tasks through in-context learning (ICL) without parameter updates. Current theoretical explanations for this capability assume test tasks are drawn from a distribution similar to that seen during pretraining. This assumption overlooks adversarial distribution shifts that threaten real-world reliability. To address this gap, we introduce a distributionally robust meta-learning framework that provides worst-case performance guarantees for ICL under Wasserstein-based distribution shifts. Focusing on linear self-attention Transformers, we derive a non-asymptotic bound linking adversarial perturbation strength ($\u03c1$), model capacity ($m$), and the number of in-context examples ($N$). The analysis reveals that model robustness scales with the square root of its capacity ($\u03c1_{\\text{max}} \\propto \\sqrt{m}$), while adversarial settings impose a sample complexity penalty proportional to the square of the perturbation magnitude ($N_\u03c1- N_0 \\propto \u03c1^2$). Experiments on synthetic tasks confirm these scaling laws. These findings advance the theoretical understanding of ICL's limits under adversarial conditions and suggest that model capacity serves as a fundamental resource for distributional robustness.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6784\u5efa\u4e86\u5206\u5e03\u9c81\u68d2\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u6297\u6027\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60(ICL)\u80fd\u529b\u3002\u901a\u8fc7\u5206\u6790\u7ebf\u6027\u81ea\u6ce8\u610f\u529bTransformer\uff0c\u53d1\u73b0\u6a21\u578b\u9c81\u68d2\u6027\u4e0e\u5bb9\u91cf\u5e73\u65b9\u6839\u6210\u6b63\u6bd4(\u03c1_max \u221d \u221am)\uff0c\u4e14\u5bf9\u6297\u6027\u8bbe\u7f6e\u4f7f\u6837\u672c\u590d\u6742\u5ea6\u968f\u6270\u52a8\u5e45\u5ea6\u5e73\u65b9\u589e\u52a0(N_\u03c1 - N_0 \u221d \u03c1\u00b2)\u3002", "motivation": "\u73b0\u6709ICL\u7406\u8bba\u5047\u8bbe\u6d4b\u8bd5\u4efb\u52a1\u4e0e\u9884\u8bad\u7ec3\u4efb\u52a1\u540c\u5206\u5e03\uff0c\u5ffd\u7565\u4e86\u5f71\u54cd\u73b0\u5b9e\u53ef\u9760\u6027\u7684\u5bf9\u6297\u6027\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u5b58\u5728\u7406\u8bba\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u5206\u5e03\u9c81\u68d2\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u4e3a\u57fa\u4e8eWasserstein\u8ddd\u79bb\u7684\u5206\u5e03\u504f\u79fb\u63d0\u4f9bICL\u7684\u6700\u574f\u60c5\u51b5\u6027\u80fd\u4fdd\u8bc1\uff1b\u805a\u7126\u7ebf\u6027\u81ea\u6ce8\u610f\u529bTransformer\uff0c\u63a8\u5bfc\u51fa\u8fde\u63a5\u5bf9\u6297\u6270\u52a8\u5f3a\u5ea6(\u03c1)\u3001\u6a21\u578b\u5bb9\u91cf(m)\u548c\u4e0a\u4e0b\u6587\u793a\u4f8b\u6570(N)\u7684\u975e\u6e10\u8fd1\u754c\u3002", "result": "\u63a8\u5bfc\u51fa\u6a21\u578b\u9c81\u68d2\u6027\u4e0e\u5bb9\u91cf\u5e73\u65b9\u6839\u6210\u6b63\u6bd4(\u03c1_max \u221d \u221am)\uff0c\u5bf9\u6297\u6027\u8bbe\u7f6e\u5e26\u6765\u4e0e\u6270\u52a8\u5e45\u5ea6\u5e73\u65b9\u6210\u6b63\u6bd4\u7684\u6837\u672c\u590d\u6742\u5ea6\u60e9\u7f5a(N_\u03c1 - N_0 \u221d \u03c1\u00b2)\uff1b\u5408\u6210\u4efb\u52a1\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u6807\u5ea6\u5f8b\u3002", "conclusion": "\u63a8\u8fdb\u4e86\u5bf9\u5bf9\u6297\u6761\u4ef6\u4e0bICL\u6781\u9650\u7684\u7406\u8bba\u7406\u89e3\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5bb9\u91cf\u662f\u5206\u5e03\u9c81\u68d2\u6027\u7684\u57fa\u7840\u8d44\u6e90\u3002"}}
{"id": "2602.17888", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17888", "abs": "https://arxiv.org/abs/2602.17888", "authors": ["Sayeed Shafayet Chowdhury", "Karen D'Souza", "V. Siva Kakumani", "Snehasis Mukhopadhyay", "Shiaofen Fang", "Rodney J. Schlosser", "Daniel M. Beswick", "Jeremiah A. Alt", "Jess C. Mace", "Zachary M. Soler", "Timothy L. Smith", "Vijay R. Ramakrishnan"], "title": "Machine Learning Based Prediction of Surgical Outcomes in Chronic Rhinosinusitis from Clinical Data", "comment": null, "summary": "Artificial intelligence (AI) has increasingly transformed medical prognostics by enabling rapid and accurate analysis across imaging and pathology. However, the investigation of machine learning predictions applied to prospectively collected, standardized data from observational clinical intervention trials remains underexplored, despite its potential to reduce costs and improve patient outcomes. Chronic rhinosinusitis (CRS), a persistent inflammatory disease of the paranasal sinuses lasting more than three months, imposes a substantial burden on quality of life (QoL) and societal cost. Although many patients respond to medical therapy, others with refractory symptoms often pursue surgical intervention. Surgical decision-making in CRS is complex, as it must weigh known procedural risks against uncertain individualized outcomes. In this study, we evaluated supervised machine learning models for predicting surgical benefit in CRS, using the Sino-Nasal Outcome Test-22 (SNOT-22) as the primary patient-reported outcome. Our prospectively collected cohort from an observational intervention trial comprised patients who all underwent surgery; we investigated whether models trained only on preoperative data could identify patients who might not have been recommended surgery prior to the procedure. Across multiple algorithms, including an ensemble approach, our best model achieved approximately 85% classification accuracy, providing accurate and interpretable predictions of surgical candidacy. Moreover, on a held-out set of 30 cases spanning mixed difficulty, our model achieved 80% accuracy, exceeding the average prediction accuracy of expert clinicians (75.6%), demonstrating its potential to augment clinical decision-making and support personalized CRS care.", "AI": {"tldr": "This study develops a machine learning model to predict surgical benefit for chronic rhinosinusitis (CRS) patients using preoperative data. The best model achieved ~85% accuracy and outperformed expert clinicians (75.6%), demonstrating potential to improve surgical decision-making and personalize patient care.", "motivation": "AI transforms medical prognostics but remains underexplored for prospectively collected clinical trial data. CRS imposes substantial quality-of-life and societal burdens. Surgical decision-making is complex due to uncertain individualized outcomes, necessitating tools to identify patients who may not benefit from surgery to reduce costs and improve outcomes.", "method": "Evaluated supervised machine learning models using prospectively collected observational trial data from CRS patients who all underwent surgery. Used SNOT-22 as primary outcome measure. Trained models on preoperative data only. Compared multiple algorithms including an ensemble approach against expert clinicians on a held-out test set of 30 cases.", "result": "Best model achieved ~85% classification accuracy overall. On held-out cases, model achieved 80% accuracy, exceeding expert clinicians' average accuracy of 75.6%. Model provided accurate and interpretable predictions of surgical candidacy.", "conclusion": "Machine learning models can accurately predict surgical benefit in CRS and outperform clinical experts, showing strong potential to augment clinical decision-making and support personalized, cost-effective patient care."}}
{"id": "2602.18180", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2602.18180", "abs": "https://arxiv.org/abs/2602.18180", "authors": ["Fatemeh Taghipoor", "Mojtaba Golshani", "Mostafa Motamedifar", "Khatereh Jafari"], "title": "High-Fidelity Teleportation of Continuous-Variable Quantum States Via Non-Ideal Qutrit Entangled Resources", "comment": "12 pages, 7 figures", "summary": "Achieving near-unity fidelity in conventional continuous-variable quantum teleportation schemes based on two-mode squeezed vacuum states is fundamentally unattainable. To overcome this limitation, alternative approaches utilizing ensembles of two-dimensional entangled qubits have been proposed. In this work, we investigate continuous-variable quantum teleportation employing entangled qutrit resources under realistic noise effects. The results demonstrate that the proposed scheme performs well in both ideal and noisy conditions, enabling high-fidelity teleportation with a reasonable success probability.", "AI": {"tldr": "This paper proposes using entangled qutrits (3-level quantum systems) for continuous-variable quantum teleportation to overcome the fundamental fidelity limit of conventional two-mode squeezed vacuum approaches, demonstrating high-fidelity and noise-resilient performance under realistic conditions.", "motivation": "Conventional continuous-variable quantum teleportation using two-mode squeezed vacuum states cannot achieve near-unity fidelity due to fundamental theoretical limitations, necessitating alternative resource strategies.", "method": "Investigates continuous-variable quantum teleportation schemes utilizing ensembles of two-dimensional entangled qubits replaced by entangled qutrit resources (3-level quantum systems) as the quantum channel.", "result": "The qutrit-based scheme achieves high-fidelity teleportation with reasonable success probability in both ideal and realistic noisy environments, showing strong performance under practical conditions.", "conclusion": "Entangled qutrit resources effectively overcome the fidelity limitations of traditional continuous-variable teleportation, providing a viable and robust approach for high-quality quantum teleportation in practical implementations."}}
{"id": "2602.17930", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17930", "abs": "https://arxiv.org/abs/2602.17930", "authors": ["Narjes Nourzad", "Carlee Joe-Wong"], "title": "MIRA: Memory-Integrated Reinforcement Learning Agent with Limited LLM Guidance", "comment": "International Conference on Learning Representations (ICLR'26)", "summary": "Reinforcement learning (RL) agents often suffer from high sample complexity in sparse or delayed reward settings due to limited prior structure. Large language models (LLMs) can provide subgoal decompositions, plausible trajectories, and abstract priors that facilitate early learning. However, heavy reliance on LLM supervision introduces scalability constraints and dependence on potentially unreliable signals. We propose MIRA (Memory-Integrated Reinforcement Learning Agent), which incorporates a structured, evolving memory graph to guide early training. The graph stores decision-relevant information, including trajectory segments and subgoal structures, and is constructed from both the agent's high-return experiences and LLM outputs. This design amortizes LLM queries into a persistent memory rather than requiring continuous real-time supervision. From this memory graph, we derive a utility signal that softly adjusts advantage estimation to influence policy updates without modifying the underlying reward function. As training progresses, the agent's policy gradually surpasses the initial LLM-derived priors, and the utility term decays, preserving standard convergence guarantees. We provide theoretical analysis showing that utility-based shaping improves early-stage learning in sparse-reward environments. Empirically, MIRA outperforms RL baselines and achieves returns comparable to approaches that rely on frequent LLM supervision, while requiring substantially fewer online LLM queries. Project webpage: https://narjesno.github.io/MIRA/", "AI": {"tldr": "MIRA\u901a\u8fc7\u7ed3\u5408LLM\u6307\u5bfc\u4e0e\u6f14\u5316\u8bb0\u5fc6\u56fe\u8c31\uff0c\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u964d\u4f4eRL\u6837\u672c\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u907f\u514d\u6301\u7eedLLM\u76d1\u7763\u7684\u5f00\u9500\u3002", "motivation": "RL\u667a\u80fd\u4f53\u5728\u7a00\u758f/\u5ef6\u8fdf\u5956\u52b1\u73af\u5883\u4e0b\u5b58\u5728\u6837\u672c\u590d\u6742\u5ea6\u9ad8\u7684\u95ee\u9898\u3002\u867d\u7136LLM\u80fd\u63d0\u4f9b\u6709\u52a9\u4e8e\u65e9\u671f\u5b66\u4e60\u7684\u5b50\u76ee\u6807\u5206\u89e3\u3001\u8f68\u8ff9\u548c\u62bd\u8c61\u5148\u9a8c\uff0c\u4f46\u8fc7\u5ea6\u4f9d\u8d56LLM\u4f1a\u9020\u6210\u53ef\u6269\u5c55\u6027\u9650\u5236\u5e76\u4f9d\u8d56\u53ef\u80fd\u4e0d\u53ef\u9760\u7684\u4fe1\u53f7\u3002", "method": "MIRA\u6784\u5efa\u7ed3\u6784\u5316\u8bb0\u5fc6\u56fe\u8c31\uff0c\u5b58\u50a8\u8f68\u8ff9\u6bb5\u548c\u5b50\u76ee\u6807\u7ed3\u6784\uff0c\u8be5\u56fe\u8c31\u540c\u65f6\u6765\u81ea\u667a\u80fd\u4f53\u7684\u9ad8\u56de\u62a5\u7ecf\u9a8c\u548cLLM\u8f93\u51fa\u3002\u4ece\u56fe\u8c31\u4e2d\u5bfc\u51fa\u6548\u7528\u4fe1\u53f7\uff0c\u8f6f\u8c03\u6574\u4f18\u52bf\u4f30\u8ba1\u4ee5\u5f71\u54cd\u7b56\u7565\u66f4\u65b0\uff0c\u800c\u4e0d\u4fee\u6539\u5e95\u5c42\u5956\u52b1\u51fd\u6570\u3002\u8fd9\u79cd\u65b9\u5f0f\u5c06LLM\u67e5\u8be2\u644a\u9500\u5230\u6301\u4e45\u8bb0\u5fc6\u4e2d\uff0c\u800c\u975e\u6301\u7eed\u5b9e\u65f6\u76d1\u7763\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u57fa\u4e8e\u6548\u7528\u7684\u5851\u5f62\u6539\u5584\u4e86\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e0b\u7684\u65e9\u671f\u5b66\u4e60\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cMIRA\u4f18\u4e8eRL\u57fa\u7ebf\uff0c\u4e14\u80fd\u8fbe\u5230\u4f9d\u8d56\u9891\u7e41LLM\u76d1\u7763\u7684\u65b9\u6cd5\u76f8\u5f53\u56de\u62a5\uff0c\u540c\u65f6\u9700\u8981\u663e\u8457\u66f4\u5c11\u7684\u5728\u7ebfLLM\u67e5\u8be2\u3002\u968f\u7740\u8bad\u7ec3\u8fdb\u884c\uff0c\u667a\u80fd\u4f53\u7b56\u7565\u9010\u6e10\u8d85\u8d8a\u521d\u59cbLLM\u5148\u9a8c\u3002", "conclusion": "MIRA\u901a\u8fc7\u5c06LLM\u6307\u5bfc\u7f13\u5b58\u5230\u6f14\u5316\u8bb0\u5fc6\u56fe\u8c31\u4e2d\uff0c\u6709\u6548\u964d\u4f4eRL\u6837\u672c\u590d\u6742\u5ea6\uff0c\u5728\u4fdd\u8bc1\u6536\u655b\u6027\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7684\u7a00\u758f\u5956\u52b1\u5b66\u4e60\u3002"}}
{"id": "2602.18192", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.18192", "abs": "https://arxiv.org/abs/2602.18192", "authors": ["Maryam Hadipour", "Soroush Haseli"], "title": "Geometry-Controlled Work Extraction in a Non-Markovian Quantum Battery", "comment": "9 pages, 8 figures, comments are welcome", "summary": "We investigate the role of spatial geometry in controlling energy storage and work extraction in a non-Markovian quantum battery. The model consists of two identical two-level systems embedded in a structured waveguide environment, where one qubit acts as the charger and the other as the battery. The relative separation between the qubits introduces a geometry-dependent phase that governs collective interference effects and modulates.", "AI": {"tldr": "This paper investigates how spatial separation between qubits in a structured waveguide affects quantum battery performance through geometry-dependent interference effects in non-Markovian environments.", "motivation": "Understanding geometry control of quantum batteries is essential for optimizing energy storage and work extraction in realistic non-Markovian settings, which could advance quantum energy harvesting technologies.", "method": "The authors model two identical two-level systems embedded in a structured waveguide, with one qubit as charger and the other as battery, analyzing how their relative separation introduces a geometry-dependent phase governing collective interference.", "result": "The geometry-dependent phase modulates energy transfer efficiency and storage capacity, with specific separations enabling constructive interference that enhances performance while others cause degradation, strongly influenced by non-Markovian environmental effects.", "conclusion": "Spatial geometry serves as a crucial control parameter for quantum battery performance, where optimizing qubit separation can maximize energy storage and work extraction in non-Markovian waveguide systems."}}
{"id": "2602.17931", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17931", "abs": "https://arxiv.org/abs/2602.17931", "authors": ["Narjes Nourzad", "Carlee Joe-Wong"], "title": "Memory-Based Advantage Shaping for LLM-Guided Reinforcement Learning", "comment": "Association for the Advancement of Artificial Intelligence (AAAI)", "summary": "In environments with sparse or delayed rewards, reinforcement learning (RL) incurs high sample complexity due to the large number of interactions needed for learning. This limitation has motivated the use of large language models (LLMs) for subgoal discovery and trajectory guidance. While LLMs can support exploration, frequent reliance on LLM calls raises concerns about scalability and reliability. We address these challenges by constructing a memory graph that encodes subgoals and trajectories from both LLM guidance and the agent's own successful rollouts. From this graph, we derive a utility function that evaluates how closely the agent's trajectories align with prior successful strategies. This utility shapes the advantage function, providing the critic with additional guidance without altering the reward. Our method relies primarily on offline input and only occasional online queries, avoiding dependence on continuous LLM supervision. Preliminary experiments in benchmark environments show improved sample efficiency and faster early learning compared to baseline RL methods, with final returns comparable to methods that require frequent LLM interaction.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8bb0\u5fc6\u56fe\u548c\u6548\u7528\u51fd\u6570\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u79bb\u7ebf\u6784\u5efa\u5305\u542bLLM\u6307\u5bfc\u548c\u667a\u80fd\u4f53\u6210\u529f\u8f68\u8ff9\u7684\u5b50\u76ee\u6807\u56fe\uff0c\u63a8\u5bfc\u51fa\u8bc4\u4f30\u8f68\u8ff9\u8d28\u91cf\u7684\u6548\u7528\u51fd\u6570\u6765\u5851\u9020\u4f18\u52bf\u51fd\u6570\uff0c\u51cf\u5c11\u5bf9LLM\u6301\u7eed\u5728\u7ebf\u67e5\u8be2\u7684\u4f9d\u8d56\uff0c\u63d0\u5347\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e0b\u7684\u6837\u672c\u6548\u7387\u3002", "motivation": "\u7a00\u758f\u6216\u5ef6\u8fdf\u5956\u52b1\u73af\u5883\u4e0b\u5f3a\u5316\u5b66\u4e60\u9700\u8981\u5927\u91cf\u4ea4\u4e92\u5bfc\u81f4\u6837\u672c\u590d\u6742\u5ea6\u9ad8\uff0c\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u8f85\u52a9\u5b50\u76ee\u6807\u53d1\u73b0\u548c\u8f68\u8ff9\u5f15\u5bfc\uff0c\u4f46\u9891\u7e41\u8c03\u7528LLM\u5b58\u5728\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u95ee\u9898\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u51cf\u5c11\u5bf9LLM\u6301\u7eed\u76d1\u7763\u4f9d\u8d56\u7684\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u8bb0\u5fc6\u56fe\u6765\u7f16\u7801LLM\u6307\u5bfc\u7684\u5b50\u76ee\u6807\u4e0e\u667a\u80fd\u4f53\u6210\u529f\u8f68\u8ff9\uff0c\u4ece\u56fe\u4e2d\u63a8\u5bfc\u51fa\u6548\u7528\u51fd\u6570\u8bc4\u4f30\u5f53\u524d\u8f68\u8ff9\u4e0e\u5386\u53f2\u6210\u529f\u7b56\u7565\u7684\u5339\u914d\u5ea6\uff0c\u8be5\u6548\u7528\u51fd\u6570\u5851\u9020\u4f18\u52bf\u51fd\u6570\u4e3a critic \u63d0\u4f9b\u989d\u5916\u6307\u5bfc\u4f46\u4e0d\u6539\u53d8\u5956\u52b1\uff0c\u4e3b\u8981\u4f9d\u8d56\u79bb\u7ebf\u8f93\u5165\u548c\u5076\u5c14\u5728\u7ebf\u67e5\u8be2\u3002", "result": "\u5728\u57fa\u51c6\u73af\u5883\u4e2d\u7684\u521d\u6b65\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u57fa\u7ebfRL\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u5e76\u52a0\u901f\u65e9\u671f\u5b66\u4e60\uff0c\u6700\u7ec8\u56de\u62a5\u4e0e\u9700\u8981\u9891\u7e41LLM\u4ea4\u4e92\u7684\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u8bb0\u5fc6\u56fe\u548c\u6548\u7528\u51fd\u6570\uff0c\u5728\u4fdd\u6301LLM\u6307\u5bfc\u4f18\u52bf\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u5bf9\u6301\u7eedLLM\u76d1\u7763\u7684\u4f9d\u8d56\uff0c\u5728\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u4f18\u4e8e\u9891\u7e41LLM\u4ea4\u4e92\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.17778", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.17778", "abs": "https://arxiv.org/abs/2602.17778", "authors": ["Zachary Coalson", "Bo Fang", "Sanghyun Hong"], "title": "Asking Forever: Universal Activations Behind Turn Amplification in Conversational LLMs", "comment": "Pre-print", "summary": "Multi-turn interaction length is a dominant factor in the operational costs of conversational LLMs. In this work, we present a new failure mode in conversational LLMs: turn amplification, in which a model consistently prolongs multi-turn interactions without completing the underlying task. We show that an adversary can systematically exploit clarification-seeking behavior$-$commonly encouraged in multi-turn conversation settings$-$to scalably prolong interactions. Moving beyond prompt-level behaviors, we take a mechanistic perspective and identify a query-independent, universal activation subspace associated with clarification-seeking responses. Unlike prior cost-amplification attacks that rely on per-turn prompt optimization, our attack arises from conversational dynamics and persists across prompts and tasks. We show that this mechanism provides a scalable pathway to induce turn amplification: both supply-chain attacks via fine-tuning and runtime attacks through low-level parameter corruptions consistently shift models toward abstract, clarification-seeking behavior across prompts. Across multiple instruction-tuned LLMs and benchmarks, our attack substantially increases turn count while remaining compliant. We also show that existing defenses offer limited protection against this emerging class of failures.", "AI": {"tldr": "Researchers identify \"turn amplification\" in conversational LLMs\u2014a failure mode where models unnecessarily prolong interactions via clarification-seeking behavior, which adversaries can exploit through fine-tuning or parameter corruption to increase operational costs, with existing defenses offering minimal protection.", "motivation": "To address the operational cost burden of multi-turn conversational LLMs, where interaction length dominates expenses, and to expose a novel vulnerability (turn amplification) where models extend conversations without completing tasks.", "method": "Mechanistic analysis identifying a universal activation subspace for clarification-seeking responses; testing supply-chain attacks via fine-tuning and runtime attacks via parameter corruptions to induce turn amplification across prompts.", "result": "Attacks consistently increase turn counts across multiple LLMs/benchmarks while maintaining compliance; existing defenses fail to mitigate this failure mode effectively.", "conclusion": "Turn amplification represents a scalable, systemic vulnerability in conversational LLMs rooted in clarification-seeking dynamics, demanding new defense strategies to curb cost exploitation."}}
{"id": "2602.17934", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17934", "abs": "https://arxiv.org/abs/2602.17934", "authors": ["Simi Job", "Xiaohui Tao", "Taotao Cai", "Haoran Xie", "Jianming Yong"], "title": "Causal Neighbourhood Learning for Invariant Graph Representations", "comment": null, "summary": "Graph data often contain noisy and spurious correlations that mask the true causal relationships, which are essential for enabling graph models to make predictions based on the underlying causal structure of the data. Dependence on spurious connections makes it challenging for traditional Graph Neural Networks (GNNs) to generalize effectively across different graphs. Furthermore, traditional aggregation methods tend to amplify these spurious patterns, limiting model robustness under distribution shifts. To address these issues, we propose Causal Neighbourhood Learning with Graph Neural Networks (CNL-GNN), a novel framework that performs causal interventions on graph structure. CNL-GNN effectively identifies and preserves causally relevant connections and reduces spurious influences through the generation of counterfactual neighbourhoods and adaptive edge perturbation guided by learnable importance masking and an attention-based mechanism. In addition, by combining structural-level interventions with the disentanglement of causal features from confounding factors, the model learns invariant node representations that are robust and generalize well across different graph structures. Our approach improves causal graph learning beyond traditional feature-based methods, resulting in a robust classification model. Extensive experiments on four publicly available datasets, including multiple domain variants of one dataset, demonstrate that CNL-GNN outperforms state-of-the-art GNN models.", "AI": {"tldr": "\u63d0\u51faCNL-GNN\u6846\u67b6\uff0c\u901a\u8fc7\u56e0\u679c\u5e72\u9884\u548c\u751f\u6210\u53cd\u4e8b\u5b9e\u90bb\u57df\u6765\u8bc6\u522b\u56e0\u679c\u8fde\u63a5\u3001\u51cf\u5c11\u865a\u5047\u76f8\u5173\u6027\uff0c\u5b66\u4e60\u4e0d\u53d8\u8282\u70b9\u8868\u793a\uff0c\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u56fe\u6570\u636e\u4e2d\u5b58\u5728\u5927\u91cf\u566a\u58f0\u548c\u865a\u5047\u76f8\u5173\u6027\u63a9\u76d6\u771f\u5b9e\u56e0\u679c\u5173\u7cfb\uff0c\u4f20\u7edfGNN\u56e0\u4f9d\u8d56\u8fd9\u4e9b\u865a\u5047\u8fde\u63a5\u800c\u96be\u4ee5\u5728\u4e0d\u540c\u56fe\u95f4\u6709\u6548\u6cdb\u5316\uff0c\u4e14\u4f20\u7edf\u805a\u5408\u65b9\u6cd5\u4f1a\u653e\u5927\u865a\u5047\u6a21\u5f0f\uff0c\u9650\u5236\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faCNL-GNN\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u53cd\u4e8b\u5b9e\u90bb\u57df\u548c\u57fa\u4e8e\u53ef\u5b66\u4e60\u91cd\u8981\u6027\u63a9\u7801\u53ca\u6ce8\u610f\u529b\u673a\u5236\u7684\u9002\u5e94\u6027\u8fb9\u6270\u52a8\uff0c\u5bf9\u56fe\u7ed3\u6784\u8fdb\u884c\u56e0\u679c\u5e72\u9884\uff0c\u8bc6\u522b\u5e76\u4fdd\u7559\u56e0\u679c\u76f8\u5173\u8fde\u63a5\uff0c\u540c\u65f6\u7ed3\u5408\u56e0\u679c\u7279\u5f81\u4e0e\u6df7\u6742\u56e0\u5b50\u7684\u89e3\u8026\uff0c\u5b66\u4e60\u4e0d\u53d8\u8282\u70b9\u8868\u793a\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u6570\u636e\u96c6\uff08\u5305\u542b\u4e00\u4e2a\u6570\u636e\u96c6\u7684\u591a\u9886\u57df\u53d8\u4f53\uff09\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5148\u8fdbGNN\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u8d85\u8d8a\u4f20\u7edf\u7279\u5f81\u57fa\u65b9\u6cd5\u7684\u56e0\u679c\u56fe\u5b66\u4e60\uff0c\u6784\u5efa\u51fa\u9c81\u68d2\u7684\u5206\u7c7b\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u56e0\u679c\u7ed3\u6784\u5e72\u9884\u548c\u7279\u5f81\u89e3\u8026\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u6570\u636e\u4e2d\u7684\u865a\u5047\u76f8\u5173\u95ee\u9898\uff0c\u5b66\u4e60\u5230\u7684\u56e0\u679c\u4e0d\u53d8\u8868\u793a\u5177\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u56fe\u6a21\u578b\u7684\u9c81\u68d2\u6027\u63d0\u5347\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.18300", "categories": ["quant-ph", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2602.18300", "abs": "https://arxiv.org/abs/2602.18300", "authors": ["Gabrielle Barsky-Giles", "Alessandro Prositto", "Matthew Gerry", "Dvira Segal"], "title": "Impossibility of Refrigeration and Engine Operation in Minimal Qubit Repeated-Interaction Models", "comment": null, "summary": "We investigate the operation of a qubit as a quantum thermal device within the repeated interaction framework, allowing for strong system-bath coupling and finite interaction times. We analyze two minimal models: an alternating-coupling setup, in which the qubit sequentially interacts with hot and cold baths, and a simultaneous-coupling setup, where both baths interact with the qubit during each collision. For the alternating model, we obtain an exact analytical solution for the limit-cycle state, valid for arbitrary coupling strengths and collision durations. Using this solution, we rigorously prove a no-go theorem for quantum refrigeration. We further demonstrate that, although work can be generated locally at individual system-bath contacts, the total work over a cycle is always nonpositive, precluding engine operation. In the absence of work, the model describes pure heat conduction, for which we derive a closed-form expression for the heat current and show that it exhibits a nonmonotonic turnover behavior. The simultaneous-coupling model is analyzed perturbatively. In the short-collision-time limit, it reproduces the same steady-state behavior as the alternating model, reinforcing the generality of the constraints identified. Our results establish fundamental limitations on qubit-based quantum thermal machines operating under Markovian repeated interactions and highlight the need for enriched models to realize functional quantum thermal devices.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc1\u660e\u5728\u9a6c\u5c14\u53ef\u592b\u91cd\u590d\u76f8\u4e92\u4f5c\u7528\u6846\u67b6\u4e0b\uff0c\u57fa\u4e8e\u91cf\u5b50\u6bd4\u7279\u7684\u70ed\u673a\u65e0\u6cd5\u5b9e\u73b0\u5236\u51b7\u6216\u505a\u529f\u529f\u80fd\uff0c\u63ed\u793a\u4e86\u6b64\u7c7b\u8bbe\u5907\u7684\u6839\u672c\u6027\u9650\u5236\u3002", "motivation": "\u63a2\u7a76\u91cf\u5b50\u6bd4\u7279\u5728\u5f3a\u8026\u5408\u548c\u6709\u9650\u4f5c\u7528\u65f6\u95f4\u6761\u4ef6\u4e0b\u4f5c\u4e3a\u91cf\u5b50\u70ed\u673a\u7684\u53ef\u884c\u6027\uff0c\u73b0\u6709\u6a21\u578b\u591a\u5047\u8bbe\u5f31\u8026\u5408\u6216\u77ac\u65f6\u4f5c\u7528\uff0c\u9700\u9a8c\u8bc1\u5176\u5b9e\u9645\u8fd0\u884c\u6781\u9650\u3002", "method": "\u91c7\u7528\u91cd\u590d\u76f8\u4e92\u4f5c\u7528\u6846\u67b6\u5206\u6790\u4e24\u79cd\u6700\u5c0f\u6a21\u578b\uff1a\u4ea4\u66ff\u8026\u5408\uff08\u91cf\u5b50\u6bd4\u7279\u4f9d\u6b21\u63a5\u89e6\u51b7\u70ed\u6d74\uff09\u548c\u540c\u65f6\u8026\u5408\uff08\u53cc\u6d74\u540c\u65f6\u4f5c\u7528\uff09\uff1b\u5bf9\u4ea4\u66ff\u6a21\u578b\u83b7\u5f97\u6781\u9650\u73af\u6001\u7684\u7cbe\u786e\u89e3\u6790\u89e3\uff0c\u5bf9\u540c\u65f6\u6a21\u578b\u8fdb\u884c\u5fae\u6270\u5206\u6790\u3002", "result": "1) \u63d0\u51fa\u91cf\u5b50\u5236\u51b7\u7684\u7981\u6212\u5b9a\u7406\uff1b2) \u8bc1\u660e\u5355\u4e2a\u63a5\u89e6\u70b9\u53ef\u5c40\u90e8\u4ea7\u529f\u4f46\u5468\u671f\u603b\u529f\u975e\u6b63\uff1b3) \u5bfc\u51fa\u70ed\u6d41\u7684\u95ed\u5f0f\u8868\u8fbe\u5e76\u53d1\u73b0\u975e\u5355\u8c03\u8f6c\u79fb\u52a8\u529b\u5b66\uff1b4) \u77ed\u65f6\u78b0\u649e\u6781\u9650\u4e0b\u4e24\u6a21\u578b\u7a33\u6001\u884c\u4e3a\u4e00\u81f4\u3002", "conclusion": "\u9a6c\u5c14\u53ef\u592b\u91cd\u590d\u76f8\u4e92\u4f5c\u7528\u6846\u67b6\u4e0b\u91cf\u5b50\u6bd4\u7279\u70ed\u673a\u5b58\u5728\u6839\u672c\u6027\u7ea6\u675f\uff0c\u65e0\u6cd5\u5b9e\u73b0\u5b9e\u7528\u5316\u5236\u51b7\u6216\u5f15\u64ce\u529f\u80fd\uff0c\u9700\u53d1\u5c55\u66f4\u590d\u6742\u6a21\u578b\u4ee5\u6784\u5efa\u529f\u80fd\u6027\u91cf\u5b50\u70ed\u5668\u4ef6\u3002"}}
{"id": "2602.17783", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17783", "abs": "https://arxiv.org/abs/2602.17783", "authors": ["Xiangyu Sun", "Shirin Hosseinmardi", "Amin Yousefpour", "Ramin Bostanabad"], "title": "Multi-material Multi-physics Topology Optimization with Physics-informed Gaussian Process Priors", "comment": null, "summary": "Machine learning (ML) has been increasingly used for topology optimization (TO). However, most existing ML-based approaches focus on simplified benchmark problems due to their high computational cost, spectral bias, and difficulty in handling complex physics. These limitations become more pronounced in multi-material, multi-physics problems whose objective or constraint functions are not self-adjoint. To address these challenges, we propose a framework based on physics-informed Gaussian processes (PIGPs). In our approach, the primary, adjoint, and design variables are represented by independent GP priors whose mean functions are parametrized via neural networks whose architectures are particularly beneficial for surrogate modeling of PDE solutions. We estimate all parameters of our model simultaneously by minimizing a loss that is based on the objective function, multi-physics potential energy functionals, and design-constraints. We demonstrate the capability of the proposed framework on benchmark TO problems such as compliance minimization, heat conduction optimization, and compliant mechanism design under single- and multi-material settings. Additionally, we leverage thermo-mechanical TO with single- and multi-material options as a representative multi-physics problem. We also introduce differentiation and integration schemes that dramatically accelerate the training process. Our results demonstrate that the proposed PIGP framework can effectively solve coupled multi-physics and design problems simultaneously -- generating super-resolution topologies with sharp interfaces and physically interpretable material distributions. We validate these results using open-source codes and the commercial software package COMSOL.", "AI": {"tldr": "We propose a physics-informed Gaussian process (PIGP) framework that simultaneously solves multi-physics topology optimization problems, overcoming computational and spectral bias limitations of existing ML methods through independent GP priors and accelerated training schemes.", "motivation": "Machine learning methods for topology optimization face limitations in multi-material, multi-physics problems due to high computational cost, spectral bias, and difficulty handling complex physics, especially when objective/constraint functions are non-self-adjoint.", "method": "A PIGP framework where primary, adjoint, and design variables are represented by independent GP priors with neural network-parametrized mean functions. All parameters are estimated simultaneously by minimizing a loss based on objective functions, multi-physics potential energy functionals, and design constraints, with specialized differentiation/integration schemes to accelerate training.", "result": "Successfully demonstrated on compliance minimization, heat conduction, compliant mechanism design, and thermo-mechanical topology optimization. The framework generates super-resolution topologies with sharp interfaces and physically interpretable material distributions, validated against both open-source codes and COMSOL.", "conclusion": "The proposed PIGP framework effectively solves coupled multi-physics and design problems simultaneously, offering a computationally efficient alternative to conventional ML-based topology optimization methods."}}
{"id": "2602.18323", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.18323", "abs": "https://arxiv.org/abs/2602.18323", "authors": ["Goni Yoeli", "Gilad Gour"], "title": "Instability as a Quantum Resource", "comment": "5+19 pages, 2 figures, 1 table", "summary": "We consolidate coherence, athermality, and nonuniformity as sub-resources within an underlying quantum resource theory: instability. We formulate instability axiomatically as the transient information within a decaying physical system. Specifying a decay mechanism (e.g., dephasing, thermalization) recovers these familiar resources as specific manifestations of instability. We compute the one-shot distillation yield and dilution cost in various operational paradigms, and use them to pin down the extremal additive monotones. In the asymptotic regime, we show that all conversion rates are governed by a single additive monotone, and thereby we establish a universal second law for instability.", "AI": {"tldr": "\u672c\u6587\u5c06\u76f8\u5e72\u6027\u3001\u975e\u70ed\u6027\u548c\u975e\u5747\u5300\u6027\u7edf\u4e00\u4e3a\u91cf\u5b50\u8d44\u6e90\u7406\u8bba\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u5b50\u8d44\u6e90\uff0c\u5e76\u5efa\u7acb\u4e86\u4e0d\u7a33\u5b9a\u6027\u8f6c\u6362\u7684\u666e\u904d\u7b2c\u4e8c\u5b9a\u5f8b\u3002", "motivation": "\u6574\u5408\u5404\u79cd\u91cf\u5b50\u8d44\u6e90\uff08\u5982\u76f8\u5e72\u6027\u3001\u975e\u70ed\u6027\u3001\u975e\u5747\u5300\u6027\uff09\u5230\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u4e2d\uff0c\u4ee5\u7406\u89e3\u5b83\u4eec\u4e4b\u95f4\u7684\u76f8\u4e92\u5173\u7cfb\u548c\u64cd\u4f5c\u9650\u5236\u3002", "method": "\u516c\u7406\u5316\u5b9a\u4e49\u4e0d\u7a33\u5b9a\u6027\u4e3a\u8870\u51cf\u7cfb\u7edf\u4e2d\u7684\u77ac\u6001\u4fe1\u606f\uff0c\u6307\u5b9a\u8870\u51cf\u673a\u5236\uff08\u5982\u9000\u76f8\u3001\u70ed\u5316\uff09\uff0c\u8ba1\u7b97\u5355\u955c\u5934\u84b8\u998f\u4ea7\u7387\u548c\u7a00\u91ca\u6210\u672c\uff0c\u5206\u6790\u6e10\u8fd1 regime \u4ee5\u627e\u5230\u6781\u503c\u53ef\u52a0\u5355\u8c03\u91cf\u3002", "result": "\u5728\u6e10\u8fd1 regime \u4e2d\uff0c\u6240\u6709\u8f6c\u6362\u7387\u90fd\u7531\u4e00\u4e2a\u5355\u4e00\u7684\u53ef\u52a0\u5355\u8c03\u91cf\u63a7\u5236\uff0c\u4ece\u800c\u5efa\u7acb\u4e86\u4e0d\u7a33\u5b9a\u6027\u7684\u666e\u904d\u7b2c\u4e8c\u5b9a\u5f8b\u3002", "conclusion": "\u4e0d\u7a33\u5b9a\u6027\u4f5c\u4e3a\u57fa\u7840\u91cf\u5b50\u8d44\u6e90\u7406\u8bba\uff0c\u7edf\u4e00\u4e86\u591a\u79cd\u91cf\u5b50\u8d44\u6e90\uff0c\u5e76\u63ed\u793a\u4e86\u8d44\u6e90\u8f6c\u6362\u7684\u57fa\u672c\u9650\u5236\u3002"}}
{"id": "2602.17798", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17798", "abs": "https://arxiv.org/abs/2602.17798", "authors": ["Ibne Farabi Shihab", "Sanjeda Akter", "Anuj Sharma"], "title": "Grassmannian Mixture-of-Experts: Concentration-Controlled Routing on Subspace Manifolds", "comment": null, "summary": "Mixture-of-Experts models rely on learned routers to assign tokens to experts, yet standard softmax gating provides no principled mechanism to control the tradeoff between sparsity and utilization. We propose Grassmannian MoE (GrMoE), a routing framework that operates on the Grassmannian manifold of subspaces, where gating weights arise from the concentration parameters of Matrix Bingham distributions. This construction yields a single, interpretable knob -- the concentration matrix $\u039b$ -- that continuously controls routing entropy, replacing discrete top-$k$ selection with a smooth, geometrically principled sparsity mechanism. We further develop an amortized variational inference procedure for posterior routing distributions, enabling uncertainty-aware expert assignment that naturally resists expert collapse. We formally prove tight bounds relating the Bingham concentration spectrum to routing entropy, expected top-$k$ mass, and an exponential bound on expert collapse, establishing the first formal theory of concentration-controlled sparsity. On synthetic routing tasks, a 350M-parameter MoE language model with 8 experts, a 1.3B-parameter model with 16 experts, and a 2.7B-parameter model with 32 experts, GrMoE achieves 0\\% routing collapse across all seeds, comparable or better perplexity with 15--30\\% improved load balance, and a smooth monotonic relationship between concentration and effective sparsity that enables post-hoc sparsity tuning without retraining. Token-level analysis reveals that experts learn heterogeneous concentration values that correlate with linguistic specialization, providing interpretable routing behavior.", "AI": {"tldr": "\u63d0\u51faGrassmannian MoE (GrMoE)\uff0c\u5229\u7528Grassmann\u6d41\u5f62\u548cMatrix Bingham\u5206\u5e03\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u7a00\u758f\u8def\u7531\uff0c\u7528\u6d53\u5ea6\u77e9\u9635\u8fde\u7eed\u63a7\u5236\u8def\u7531\u71b5\uff0c\u907f\u514d\u4e13\u5bb6\u5d29\u6e83\uff0c\u5728\u591a\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u4e0a\u5b9e\u73b0\u96f6\u5d29\u6e83\u548c\u66f4\u597d\u7684\u8d1f\u8f7d\u5747\u8861\u3002", "motivation": "\u6807\u51c6MoE\u6a21\u578b\u7684softmax\u95e8\u63a7\u7f3a\u4e4f\u5bf9\u7a00\u758f\u6027\u4e0e\u5229\u7528\u7387\u6743\u8861\u7684principled\u63a7\u5236\u673a\u5236\uff0c\u5bb9\u6613\u5bfc\u81f4\u4e13\u5bb6\u5d29\u6e83\u95ee\u9898\u3002", "method": "\u57fa\u4e8eGrassmann\u6d41\u5f62\u6784\u5efa\u8def\u7531\u6846\u67b6\uff0c\u95e8\u63a7\u6743\u91cd\u6765\u81eaMatrix Bingham\u5206\u5e03\u7684\u6d53\u5ea6\u53c2\u6570\uff0c\u901a\u8fc7\u53d8\u5206\u63a8\u65ad\u5b9e\u73b0\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u4e13\u5bb6\u5206\u914d\uff0c\u7528\u5355\u4e00\u6d53\u5ea6\u77e9\u9635\u039b\u5e73\u6ed1\u63a7\u5236\u8def\u7531\u71b5\u3002", "result": "350M-2.7B\u53c2\u6570\u6a21\u578b\u4e0a\u5b9e\u73b0\u96f6\u8def\u7531\u5d29\u6e83\uff0c\u56f0\u60d1\u5ea6\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u8d1f\u8f7d\u5747\u8861\u63d0\u534715-30%\uff0c\u6d53\u5ea6\u4e0e\u7a00\u758f\u6027\u5448\u5355\u8c03\u5173\u7cfb\u652f\u6301\u8bad\u7ec3\u540e\u8c03\u6574\uff0c\u4e13\u5bb6\u8868\u73b0\u51fa\u4e0e\u8bed\u8a00\u7279\u5316\u76f8\u5173\u7684\u5f02\u8d28\u6d53\u5ea6\u503c\u3002", "conclusion": "GrMoE\u63d0\u4f9b\u4e86\u51e0\u4f55\u539f\u5219\u7684\u7a00\u758f\u673a\u5236\uff0c\u5efa\u7acb\u4e86\u6d53\u5ea6\u63a7\u5236\u7a00\u758f\u6027\u7684\u9996\u4e2a\u5f62\u5f0f\u5316\u7406\u8bba\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e13\u5bb6\u5d29\u6e83\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2602.17976", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17976", "abs": "https://arxiv.org/abs/2602.17976", "authors": ["Alessio Russo", "Yin-Ching Lee", "Ryan Welch", "Aldo Pacchiano"], "title": "In-Context Learning for Pure Exploration in Continuous Spaces", "comment": null, "summary": "In active sequential testing, also termed pure exploration, a learner is tasked with the goal to adaptively acquire information so as to identify an unknown ground-truth hypothesis with as few queries as possible. This problem, originally studied by Chernoff in 1959, has several applications: classical formulations include Best-Arm Identification (BAI) in bandits, where actions index hypotheses, and generalized search problems, where strategically chosen queries reveal partial information about a hidden label. In many modern settings, however, the hypothesis space is continuous and naturally coincides with the query/action space: for example, identifying an optimal action in a continuous-armed bandit, localizing an $\u03b5$-ball contained in a target region, or estimating the minimizer of an unknown function from a sequence of observations. In this work, we study pure exploration in such continuous spaces and introduce Continuous In-Context Pure Exploration for this regime. We introduce C-ICPE-TS, an algorithm that meta-trains deep neural policies to map observation histories to (i) the next continuous query action and (ii) a predicted hypothesis, thereby learning transferable sequential testing strategies directly from data. At inference time, C-ICPE-TS actively gathers evidence on previously unseen tasks and infers the true hypothesis without parameter updates or explicit hand-crafted information models. We validate C-ICPE-TS across a range of benchmarks, spanning continuous best-arm identification, region localization, and function minimizer identification.", "AI": {"tldr": "This paper introduces Continuous In-Context Pure Exploration (C-ICPE), a meta-learning framework that trains deep neural networks to solve pure exploration problems in continuous spaces by learning sequential testing strategies directly from data, eliminating the need for hand-crafted models or parameter updates during inference.", "motivation": "Classical pure exploration assumes discrete hypothesis spaces, but modern applications require continuous spaces where hypothesis and action spaces coincide (e.g., continuous bandits, function optimization). Existing methods are inadequate for these settings, necessitating learning-based approaches that can transfer across tasks without explicit model assumptions.", "method": "The authors propose C-ICPE-TS, an algorithm that meta-trains deep neural policies to map observation histories to both the next continuous query action and a predicted hypothesis. This enables learning of transferable sequential testing strategies directly from data, with no parameter updates or hand-crafted information models needed at inference time.", "result": "C-ICPE-TS was validated across diverse benchmarks including continuous best-arm identification, region localization, and function minimizer identification, demonstrating its effectiveness.", "conclusion": "The framework successfully extends pure exploration to continuous spaces, providing a general, data-driven solution that learns transferable strategies applicable to unseen tasks without requiring domain-specific modeling."}}
{"id": "2602.18327", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.18327", "abs": "https://arxiv.org/abs/2602.18327", "authors": ["Francesco Atzori", "Salvatore Virz\u00ec", "Francesco Devecchi", "Domenico Abbondandolo", "Alessio Avella", "Fabrizio Piacentini", "Marco Gramegna", "Ivo Pietro Degiovanni", "Marco Genovese"], "title": "Universal Protection of Quantum States from Decoherence", "comment": null, "summary": "The fragility of quantum coherence fundamentally limits the scalability of quantum technologies, as unavoidable environmental interactions induce decoherence and rapidly degrade quantum properties. The Quantum Zeno Effect offers a powerful route to suppress quantum evolution and protect coherence through frequent measurements, irrespective of the underlying dynamics. However, existing implementations require prior knowledge of the quantum state, severely restricting their applicability. Here we introduce a state- and dynamics-independent protection protocol embedding the system in a larger Hilbert space, temporarily swapping the quantum information from its original degree of freedom to a decoherence-free ancillary one. We experimentally validate the protocol on a quantum optical platform, demonstrating robust preservation of coherence and purity for arbitrary polarization qubits under decoherence, thereby enabling the universal safeguarding of unknown quantum states.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u7684\u91cf\u5b50\u76f8\u5e72\u4fdd\u62a4\u534f\u8bae\uff0c\u901a\u8fc7\u5c06\u4fe1\u606f\u4ea4\u6362\u5230\u65e0\u9000\u76f8\u5e72\u8f85\u52a9\u81ea\u7531\u5ea6\uff0c\u5b9e\u73b0\u5bf9\u672a\u77e5\u91cf\u5b50\u6001\u7684\u666e\u9002\u4fdd\u62a4\u3002", "motivation": "\u73af\u5883\u8bf1\u5bfc\u7684\u9000\u76f8\u5e72\u4e25\u91cd\u9650\u5236\u91cf\u5b50\u6280\u672f\u53ef\u6269\u5c55\u6027\uff1b\u73b0\u6709\u91cf\u5b50\u829d\u8bfa\u6548\u5e94\u65b9\u6cd5\u9700\u9884\u5148\u77e5\u9053\u91cf\u5b50\u6001\uff0c\u9002\u7528\u6027\u53d7\u9650\u3002", "method": "\u8bbe\u8ba1\u72b6\u6001\u548c\u52a8\u529b\u5b66\u65e0\u5173\u7684\u4fdd\u62a4\u534f\u8bae\uff1a\u5c06\u7cfb\u7edf\u5d4c\u5165\u66f4\u5927\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\uff0c\u5c06\u91cf\u5b50\u4fe1\u606f\u4ece\u539f\u59cb\u81ea\u7531\u5ea6\u6682\u65f6\u8f6c\u79fb\u5230\u65e0\u9000\u76f8\u5e72\u7684\u8f85\u52a9\u81ea\u7531\u5ea6\u3002", "result": "\u5728\u91cf\u5b50\u5149\u5b66\u5e73\u53f0\u5b9e\u9a8c\u9a8c\u8bc1\uff1a\u5bf9\u4efb\u610f\u504f\u632f\u91cf\u5b50\u6bd4\u7279\uff0c\u5728\u9000\u76f8\u5e72\u6761\u4ef6\u4e0b\u5747\u53ef\u9c81\u68d2\u5730\u4fdd\u6301\u76f8\u5e72\u6027\u548c\u7eaf\u5ea6\u3002", "conclusion": "\u8be5\u534f\u8bae\u53ef\u666e\u9002\u5730\u4fdd\u62a4\u672a\u77e5\u91cf\u5b50\u6001\uff0c\u4e3a\u53ef\u6269\u5c55\u91cf\u5b50\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848\u3002"}}
{"id": "2602.17809", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17809", "abs": "https://arxiv.org/abs/2602.17809", "authors": ["Ibne Farabi Shihab", "Sanjeda Akter", "Anuj Sharma"], "title": "Calibrated Adaptation: Bayesian Stiefel Manifold Priors for Reliable Parameter-Efficient Fine-Tuning", "comment": null, "summary": "Parameter-efficient fine-tuning methods such as LoRA enable practical adaptation of large language models but provide no principled uncertainty estimates, leading to poorly calibrated predictions and unreliable behavior under domain shift. We introduce Stiefel-Bayes Adapters (SBA), a Bayesian PEFT framework that places a Matrix Langevin prior over orthonormal adapter factors on the Stiefel manifold $\\St$ and performs approximate posterior inference via tangent space Laplace approximation with geodesic retraction. Unlike Gaussian priors in flat space projected onto orthogonality constraints, our prior on the manifold naturally encodes the inductive bias that adapter subspaces should be well conditioned and orthogonal, while the posterior provides calibrated predictive uncertainty without recalibration. We prove formally that the tangent space approximation strictly avoids the structural variance inflation inherent in projecting from ambient space, establishing a rigorous theoretical advantage for intrinsic manifold inference. Across GLUE and SuperGLUE benchmarks on RoBERTa-large, LLaMA-2-7B, LLaMA-2-13B, Mistral-7B, and Qwen2.5-7B, domain shift evaluations, selective prediction protocols, and an abstractive summarization task, SBA achieves task performance comparable to LoRA and DoRA while reducing Expected Calibration Error by 18 to 34\\% over deterministic baselines, improving selective prediction AUROC by 12 to 25\\% under domain shift, and outperforming deep ensembles of five LoRA models on OOD detection at a fraction of the parameter cost. Our results demonstrate that where you place uncertainty, on the right geometric structure, matters more than simply adding any Bayesian treatment to adapters.", "AI": {"tldr": "Stiefel-Bayes Adapters (SBA) is a Bayesian parameter-efficient fine-tuning method that places Matrix Langevin priors on the Stiefel manifold for calibrated uncertainty estimates, reducing calibration error by 18-34% and outperforming ensembles at minimal parameter cost.", "motivation": "Parameter-efficient fine-tuning (PEFT) methods like LoRA lack principled uncertainty quantification, leading to poorly calibrated predictions and unreliable performance under domain shift.", "method": "Introduces Stiefel-Bayes Adapters (SBA) that places a Matrix Langevin prior over orthonormal adapter factors on the Stiefel manifold, performing approximate posterior inference via tangent space Laplace approximation with geodesic retraction, avoiding structural variance inflation from ambient space projection.", "result": "On GLUE/SuperGLUE benchmarks across RoBERTa, LLaMA-2, Mistral, and Qwen2.5 models, SBA achieves performance comparable to LoRA/DoRA while reducing Expected Calibration Error by 18-34%, improving selective prediction AUROC by 12-25% under domain shift, and outperforming 5-model LoRA ensembles on OOD detection with far fewer parameters.", "conclusion": "The geometric structure where uncertainty is placed significantly impacts its quality; intrinsic manifold inference provides rigorous theoretical advantages and practical improvements over simply adding Bayesian treatment to adapters."}}
{"id": "2602.17978", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17978", "abs": "https://arxiv.org/abs/2602.17978", "authors": ["Daqian Shao"], "title": "Learning Optimal and Sample-Efficient Decision Policies with Guarantees", "comment": "A thesis submitted for the degree of DPhil in Computer Science at Oxford", "summary": "The paradigm of decision-making has been revolutionised by reinforcement learning and deep learning. Although this has led to significant progress in domains such as robotics, healthcare, and finance, the use of RL in practice is challenging, particularly when learning decision policies in high-stakes applications that may require guarantees. Traditional RL algorithms rely on a large number of online interactions with the environment, which is problematic in scenarios where online interactions are costly, dangerous, or infeasible. However, learning from offline datasets is hindered by the presence of hidden confounders. Such confounders can cause spurious correlations in the dataset and can mislead the agent into taking suboptimal or adversarial actions. Firstly, we address the problem of learning from offline datasets in the presence of hidden confounders. We work with instrumental variables (IVs) to identify the causal effect, which is an instance of a conditional moment restrictions (CMR) problem. Inspired by double/debiased machine learning, we derive a sample-efficient algorithm for solving CMR problems with convergence and optimality guarantees, which outperforms state-of-the-art algorithms. Secondly, we relax the conditions on the hidden confounders in the setting of (offline) imitation learning, and adapt our CMR estimator to derive an algorithm that can learn effective imitator policies with convergence rate guarantees. Finally, we consider the problem of learning high-level objectives expressed in linear temporal logic (LTL) and develop a provably optimal learning algorithm that improves sample efficiency over existing methods. Through evaluation on reinforcement learning benchmarks and synthetic and semi-synthetic datasets, we demonstrate the usefulness of the methods developed in this thesis in real-world decision making.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a\u9690\u85cf\u6df7\u6742\u56e0\u7d20\u3001\u6a21\u4eff\u5b66\u4e60\u548c\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u76ee\u6807\u5b66\u4e60\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u5de5\u5177\u53d8\u91cf\u548c\u6761\u4ef6\u77e9\u9650\u5236\u7684\u6837\u672c\u9ad8\u6548\u7b97\u6cd5\uff0c\u5177\u5907\u53ef\u8bc1\u660e\u7684\u6536\u655b\u6027\u548c\u6700\u4f18\u6027\u4fdd\u8bc1\uff0c\u5728\u73b0\u5b9e\u51b3\u7b56\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u9ad8\u98ce\u9669\u5b9e\u9645\u5e94\u7528\u4e2d\u9762\u4e34\u5728\u7ebf\u4ea4\u4e92\u6210\u672c\u9ad8\u3001\u5371\u9669\u6216\u4e0d\u53ef\u884c\u7684\u6311\u6218\uff0c\u800c\u79bb\u7ebf\u5b66\u4e60\u53c8\u53d7\u9690\u85cf\u6df7\u6742\u56e0\u7d20\u5bfc\u81f4\u7684\u865a\u5047\u76f8\u5173\u6027\u56f0\u6270\uff0c\u6613\u4f7f\u667a\u80fd\u4f53\u5b66\u4e60\u5230\u6b21\u4f18\u6216\u5bf9\u6297\u6027\u884c\u4e3a\u3002", "method": "\u5229\u7528\u5de5\u5177\u53d8\u91cf\u8bc6\u522b\u56e0\u679c\u6548\u5e94\uff0c\u5c06\u5176\u8f6c\u5316\u4e3a\u6761\u4ef6\u77e9\u9650\u5236\u95ee\u9898\uff1b\u501f\u9274\u53cc\u91cd/\u53bb\u504f\u673a\u5668\u5b66\u4e60\u601d\u60f3\uff0c\u63a8\u5bfc\u51fa\u6837\u672c\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u5e76\u6269\u5c55\u5230\u79bb\u7ebf\u6a21\u4eff\u5b66\u4e60\u548c\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u76ee\u6807\u5b66\u4e60\u573a\u666f\u3002", "result": "\u63d0\u51fa\u4e86\u4e09\u79cd\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u7684\u7b97\u6cd5\uff1a1\uff09\u5e26\u9690\u85cf\u6df7\u6742\u56e0\u7d20\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60CMR\u6c42\u89e3\u5668\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff1b2\uff09\u653e\u5bbd\u6df7\u6742\u56e0\u7d20\u6761\u4ef6\u7684\u79bb\u7ebf\u6a21\u4eff\u5b66\u4e60\u7b97\u6cd5\uff1b3\uff09\u6837\u672c\u6548\u7387\u66f4\u4f18\u7684\u53ef\u8bc1\u660e\u6700\u4f18LTL\u5b66\u4e60\u7b97\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5728\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u548c\u5408\u6210/\u534a\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u5728\u73b0\u5b9e\u4e16\u754c\u51b3\u7b56\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2602.18350", "categories": ["quant-ph", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18350", "abs": "https://arxiv.org/abs/2602.18350", "authors": ["Qi Zhang", "Anton Simen", "Carlos Flores-Garrig\u00f3s", "Gabriel Alvarado Barrios", "Paolo A. Erdman", "Enrique Solano", "Aaron C. Kemp", "Vincent Beltrani", "Vedangi Pathak", "Hamed Mohammadbagherpoor"], "title": "Quantum-enhanced satellite image classification", "comment": null, "summary": "We demonstrate the application of a quantum feature extraction method to enhance multi-class image classification for space applications. By harnessing the dynamics of many-body spin Hamiltonians, the method generates expressive quantum features that, when combined with classical processing, lead to quantum-enhanced classification accuracy. Using a strong and well-established ResNet50 baseline, we achieved a maximum classical accuracy of 83%, which can be improved to 84% with a transfer learning approach. In contrast, applying our quantum-classical method the performance is increased to 87% accuracy, demonstrating a clear and reproducible improvement over robust classical approaches. Implemented on several of IBM's quantum processors, our hybrid quantum-classical approach delivers consistent gains of 2-3% in absolute accuracy. These results highlight the practical potential of current and near-term quantum processors in high-stakes, data-driven domains such as satellite imaging and remote sensing, while suggesting broader applicability in real-world machine learning tasks.", "AI": {"tldr": "A hybrid quantum-classical method using quantum feature extraction from many-body spin Hamiltonians achieves 87% accuracy for space image classification, outperforming classical ResNet50 by 2-3% absolute improvement on IBM quantum processors.", "motivation": "Enhance multi-class image classification for high-stakes space applications (satellite imaging, remote sensing) by demonstrating practical value of current/near-term quantum processors over robust classical methods.", "method": "Quantum feature extraction via many-body spin Hamiltonian dynamics combined with classical processing in a hybrid pipeline; implemented on IBM quantum hardware and compared against ResNet50 baseline with transfer learning.", "result": "Classical baseline: 83% accuracy; transfer learning: 84%; quantum-classical method: 87% accuracy. Delivers consistent 2-3% absolute accuracy gains across multiple IBM quantum processors.", "conclusion": "The reproducible improvement demonstrates practical potential of quantum processors for real-world, high-stakes machine learning tasks in satellite imaging and remote sensing, with broader applicability to other domains."}}
{"id": "2602.17829", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17829", "abs": "https://arxiv.org/abs/2602.17829", "authors": ["Preetom Biswas", "Giulia Pedrielli", "K. Sel\u00e7uk Candan"], "title": "Causality by Abstraction: Symbolic Rule Learning in Multivariate Timeseries with Large Language Models", "comment": null, "summary": "Inferring causal relations in timeseries data with delayed effects is a fundamental challenge, especially when the underlying system exhibits complex dynamics that cannot be captured by simple functional mappings. Traditional approaches often fail to produce generalized and interpretable explanations, as multiple distinct input trajectories may yield nearly indistinguishable outputs. In this work, we present ruleXplain, a framework that leverages Large Language Models (LLMs) to extract formal explanations for input-output relations in simulation-driven dynamical systems. Our method introduces a constrained symbolic rule language with temporal operators and delay semantics, enabling LLMs to generate verifiable causal rules through structured prompting. ruleXplain relies on the availability of a principled model (e.g., a simulator) that maps multivariate input time series to output time series. Within ruleXplain, the simulator is used to generate diverse counterfactual input trajectories that yield similar target output, serving as candidate explanations. Such counterfactual inputs are clustered and provided as context to the LLM, which is tasked with the generation of symbolic rules encoding the joint temporal trends responsible for the patterns observable in the output times series. A closed-loop refinement process ensures rule consistency and semantic validity. We validate the framework using the PySIRTEM epidemic simulator, mapping testing rate inputs to daily infection counts; and the EnergyPlus building energy simulator, observing temperature and solar irradiance inputs to electricity needs. For validation, we perform three classes of experiments: (1) the efficacy of the ruleset through input reconstruction; (2) ablation studies evaluating the causal encoding of the ruleset; and (3) generalization tests of the extracted rules across unseen output trends with varying phase dynamics.", "AI": {"tldr": "\u63d0\u51faruleXplain\u6846\u67b6\uff0c\u5229\u7528LLM\u4ece\u6a21\u62df\u6570\u636e\u4e2d\u63d0\u53d6\u53ef\u9a8c\u8bc1\u7684\u65f6\u5e8f\u56e0\u679c\u89c4\u5219\uff0c\u89e3\u51b3\u5ef6\u8fdf\u6548\u5e94\u4e0b\u7684\u56e0\u679c\u63a8\u65ad\u96be\u9898\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u5177\u6709\u5ef6\u8fdf\u6548\u5e94\u548c\u590d\u6742\u52a8\u6001\u7684\u65f6\u5e8f\u56e0\u679c\u5173\u7cfb\uff0c\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u7b26\u53f7\u89c4\u5219\u8bed\u8a00\u548cLLM\uff0c\u901a\u8fc7\u6a21\u62df\u5668\u751f\u6210\u53cd\u4e8b\u5b9e\u6837\u672c\uff0c\u805a\u7c7b\u540e\u8ba9LLM\u751f\u6210\u7f16\u7801\u65f6\u5e8f\u8d8b\u52bf\u7684\u56e0\u679c\u89c4\u5219\uff0c\u5e76\u901a\u8fc7\u95ed\u73af\u4f18\u5316\u786e\u4fdd\u4e00\u81f4\u6027\u3002", "result": "\u5728\u6d41\u884c\u75c5\u5b66\u548c\u5efa\u7b51\u80fd\u8017\u6a21\u62df\u5668\u4e0a\u9a8c\u8bc1\u6709\u6548\uff0c\u89c4\u5219\u96c6\u5728\u8f93\u5165\u91cd\u5efa\u3001\u56e0\u679c\u6d88\u878d\u548c\u6cdb\u5316\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u7684\u56e0\u679c\u89e3\u91ca\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u9a8c\u8bc1\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17998", "categories": ["cs.LG", "cs.AI", "cs.CE", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17998", "abs": "https://arxiv.org/abs/2602.17998", "authors": ["Shubham Bhardwaj", "Chandrajit Bajaj"], "title": "PHAST: Port-Hamiltonian Architecture for Structured Temporal Dynamics Forecasting", "comment": "50 pages", "summary": "Real physical systems are dissipative -- a pendulum slows, a circuit loses charge to heat -- and forecasting their dynamics from partial observations is a central challenge in scientific machine learning. We address the \\emph{position-only} (q-only) problem: given only generalized positions~$q_t$ at discrete times (momenta~$p_t$ latent), learn a structured model that (a)~produces stable long-horizon forecasts and (b)~recovers physically meaningful parameters when sufficient structure is provided. The port-Hamiltonian framework makes the conservative-dissipative split explicit via $\\dot{x}=(J-R)\\nabla H(x)$, guaranteeing $dH/dt\\le 0$ when $R\\succeq 0$. We introduce \\textbf{PHAST} (Port-Hamiltonian Architecture for Structured Temporal dynamics), which decomposes the Hamiltonian into potential~$V(q)$, mass~$M(q)$, and damping~$D(q)$ across three knowledge regimes (KNOWN, PARTIAL, UNKNOWN), uses efficient low-rank PSD/SPD parameterizations, and advances dynamics with Strang splitting. Across thirteen q-only benchmarks spanning mechanical, electrical, molecular, thermal, gravitational, and ecological systems, PHAST achieves the best long-horizon forecasting among competitive baselines and enables physically meaningful parameter recovery when the regime provides sufficient anchors. We show that identification is fundamentally ill-posed without such anchors (gauge freedom), motivating a two-axis evaluation that separates forecasting stability from identifiability.", "AI": {"tldr": "PHAST is a port-Hamiltonian neural architecture that forecasts dissipative systems from position-only observations by decomposing dynamics into physical components (potential, mass, damping), achieving state-of-the-art long-horizon stability and meaningful parameter recovery when structural constraints are provided.", "motivation": "Real physical systems are dissipative, and forecasting their dynamics from partial (position-only) observations is a key challenge in scientific machine learning, requiring models that ensure stable long-term predictions and physically interpretable parameter learning.", "method": "Introduces PHAST, a port-Hamiltonian framework using the conservative-dissipative split (J-R)\u2207H with R\u22650 guaranteeing energy dissipation. The architecture decomposes Hamiltonian into V(q), M(q), D(q) across KNOWN/PARTIAL/UNKNOWN regimes, employs low-rank PSD parameterizations, and integrates dynamics via Strang splitting.", "result": "On 13 diverse q-only benchmarks (mechanical, electrical, molecular, thermal, gravitational, ecological), PHAST achieves best long-horizon forecasting among baselines and enables physically meaningful parameter recovery when sufficient structural anchors are provided, while showing identification is ill-posed without constraints (gauge freedom).", "conclusion": "Structured port-Hamiltonian priors are essential for stable forecasting and identifiability in partial observation scenarios; PHAST provides a unified approach across knowledge regimes, with a two-axis evaluation separating forecasting stability from physical parameter identifiability."}}
{"id": "2602.18354", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.18354", "abs": "https://arxiv.org/abs/2602.18354", "authors": ["Romain Dalidet", "Anthony Martin", "Gregory Sauder", "S\u00e9bastien Tanzilli", "Laurent Labont\u00e9"], "title": "Quantum-enhanced phase sensitivity in an all-fiber Mach-Zehnder interferometer", "comment": null, "summary": "Recent advances in quantum photonics have enabled increasingly robust protocols in optical phase estimation, achieving precisions beyond the standard quantum limit and approaching the Heisenberg limit. While intrinsic losses hinder the realization of unconditional super-sensitivity, reaching quantum advantage, defined as sensitivity surpassing that of any classical counterpart with identical resources, remains achievable. Here we experimentally demonstrate such an advantage using a fully fibered Mach-Zehnder-type interferometer operating at telecom wavelengths, free of post-selection. The scheme relies on the conversion of polarization-entangled photon pairs, a degree of freedom commonly favored for experimental convenience, into energy-time entanglement, which is particularly well suited for scalable fiber-based sensors. All system imperfections, including asymmetric losses and detector inefficiencies, are accounted for in the Fisher information analysis, yielding a measured quantum advantage of 10%. This result highlights the practicality of compact, alignment-free quantum interferometers for real-world sensing applications.", "AI": {"tldr": "\u672c\u8bba\u6587\u5b9e\u9a8c\u6f14\u793a\u4e86\u4e00\u79cd\u65b0\u578b\u5149\u7ea4\u91cf\u5b50\u5e72\u6d89\u4eea\uff0c\u901a\u8fc7\u5c06\u504f\u632f\u7ea0\u7f20\u5149\u5b50\u5bf9\u8f6c\u6362\u4e3a\u80fd\u91cf-\u65f6\u95f4\u7ea0\u7f20\u6001\uff0c\u5728\u7535\u4fe1\u6ce2\u957f\u4e0b\u5b9e\u73b0\u4e8610%\u7684\u91cf\u5b50\u4f18\u52bf\uff0c\u8bc1\u660e\u4e86\u7d27\u51d1\u3001\u514d\u5bf9\u51c6\u7684\u91cf\u5b50\u4f20\u611f\u5668\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u5c3d\u7ba1\u91cf\u5b50\u76f8\u4f4d\u4f30\u8ba1\u5df2\u7a81\u7834\u6807\u51c6\u91cf\u5b50\u6781\u9650\uff0c\u4f46\u56fa\u6709\u635f\u8017\u9650\u5236\u4e86\u65e0\u6761\u4ef6\u8d85\u7075\u654f\u5ea6\u3002\u5982\u4f55\u5728\u6709\u635f\u8017\u7684\u5b9e\u9645\u7cfb\u7edf\u4e2d\uff0c\u4ee5\u76f8\u540c\u8d44\u6e90\u6761\u4ef6\u4e0b\u5b9e\u73b0\u8d85\u8d8a\u4efb\u4f55\u7ecf\u5178\u65b9\u6848\u7684\u91cf\u5b50\u4f18\u52bf\uff0c\u662f\u5f53\u524d\u91cf\u5b50\u4f20\u611f\u7814\u7a76\u7684\u6838\u5fc3\u6311\u6218\u3002", "method": "\u91c7\u7528\u5168\u5149\u7ea4\u9a6c\u8d6b-\u66fe\u5fb7\u5c14\u5e72\u6d89\u4eea\uff0c\u5728\u7535\u4fe1\u6ce2\u6bb5\u5de5\u4f5c\u4e14\u65e0\u9700\u540e\u9009\u62e9\u3002\u6838\u5fc3\u65b9\u6848\u662f\u5c06\u6613\u4e8e\u5b9e\u9a8c\u5236\u5907\u7684\u504f\u632f\u7ea0\u7f20\u5149\u5b50\u5bf9\uff0c\u8f6c\u6362\u4e3a\u7279\u522b\u9002\u5408\u5149\u7ea4\u6269\u5c55\u7684\u80fd\u91cf-\u65f6\u95f4\u7ea0\u7f20\u6001\uff1b\u5e76\u901a\u8fc7Fisher\u4fe1\u606f\u5206\u6790\uff0c\u7cbe\u786e\u8ba1\u5165\u6240\u6709\u7cfb\u7edf\u7f3a\u9677\uff08\u975e\u5bf9\u79f0\u635f\u8017\u548c\u63a2\u6d4b\u5668\u6548\u7387\u5dee\u5f02\uff09\u3002", "result": "\u5728\u5168\u9762\u8003\u8651\u7cfb\u7edf\u4e0d\u5b8c\u7f8e\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u9a8c\u6d4b\u91cf\u83b7\u5f97\u4e8610%\u7684\u91cf\u5b50\u4f18\u52bf\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6848\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6210\u679c\u7a81\u663e\u4e86\u7d27\u51d1\u3001\u514d\u5bf9\u51c6\u7684\u91cf\u5b50\u5e72\u6d89\u4eea\u5728\u73b0\u5b9e\u4e16\u754c\u4f20\u611f\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u4e3a\u91cf\u5b50\u589e\u5f3a\u4f20\u611f\u5668\u7684\u5546\u4e1a\u5316\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2602.17832", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17832", "abs": "https://arxiv.org/abs/2602.17832", "authors": ["Hang Liu", "Sangli Teng", "Maani Ghaffari"], "title": "MePoly: Max Entropy Polynomial Policy Optimization", "comment": null, "summary": "Stochastic Optimal Control provides a unified mathematical framework for solving complex decision-making problems, encompassing paradigms such as maximum entropy reinforcement learning(RL) and imitation learning(IL). However, conventional parametric policies often struggle to represent the multi-modality of the solutions. Though diffusion-based policies are aimed at recovering the multi-modality, they lack an explicit probability density, which complicates policy-gradient optimization. To bridge this gap, we propose MePoly, a novel policy parameterization based on polynomial energy-based models. MePoly provides an explicit, tractable probability density, enabling exact entropy maximization. Theoretically, we ground our method in the classical moment problem, leveraging the universal approximation capabilities for arbitrary distributions. Empirically, we demonstrate that MePoly effectively captures complex non-convex manifolds and outperforms baselines in performance across diverse benchmarks.", "AI": {"tldr": "\u63d0\u51faMePoly\u65b9\u6cd5\uff0c\u7528\u591a\u9879\u5f0f\u80fd\u91cf\u6a21\u578b\u5b9e\u73b0\u663e\u5f0f\u6982\u7387\u5bc6\u5ea6\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u7b56\u7565\u4f18\u5316\u95ee\u9898", "motivation": "\u4f20\u7edf\u53c2\u6570\u5316\u7b56\u7565\u96be\u4ee5\u8868\u793a\u591a\u6a21\u6001\u89e3\uff0c\u800c\u6269\u6563\u7b56\u7565\u7f3a\u4e4f\u663e\u5f0f\u6982\u7387\u5bc6\u5ea6\uff0c\u5bfc\u81f4\u7b56\u7565\u68af\u5ea6\u4f18\u5316\u56f0\u96be", "method": "\u57fa\u4e8e\u591a\u9879\u5f0f\u80fd\u91cf\u6a21\u578b\u7684\u65b0\u578b\u7b56\u7565\u53c2\u6570\u5316\u65b9\u6cd5MePoly\uff0c\u63d0\u4f9b\u53ef\u5904\u7406\u7684\u663e\u5f0f\u6982\u7387\u5bc6\u5ea6", "result": "MePoly\u80fd\u6709\u6548\u6355\u6349\u590d\u6742\u975e\u51f8\u6d41\u5f62\uff0c\u5728\u591a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7cbe\u786e\u71b5\u6700\u5927\u5316\u5f25\u5408\u4e86\u591a\u6a21\u6001\u8868\u793a\u4e0e\u53ef\u4f18\u5316\u6027\u4e4b\u95f4\u7684\u9e3f\u6c9f"}}
{"id": "2602.18363", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.18363", "abs": "https://arxiv.org/abs/2602.18363", "authors": ["Vidisha Aggarwal", "Boxi Li", "Eloisa Cuestas", "Tommaso Calarco", "Robert Zeier", "Alexei Ourjoumtsev", "Felix Motzoi"], "title": "Improving Single Excitation Fidelity in Rydberg Superatoms for Efficient Single Photon Emission", "comment": null, "summary": "Deterministic single photon emission from a Rydberg ensemble coupled to an optical cavity requires high-fidelity preparation of collective single excitations. In such a setup imperfect Rydberg blockade can lead to unwanted double excitations, which degrade photon indistinguishability. In this work we adapt the Derivative Removal by Adiabatic Gate (DRAG) technique, originally developed for superconducting qubits, to shape optical pulses that suppress double excitations in this atomic platform. By combining analytical modeling with numerical optimization, DRAG provides an improvement over conventional sine-squared pulses. Further optimization of pulse duration and atomic ensemble size identifies a parameter regime, distinct from that used in [Nature Photonics 17, 688 (2023)], that enhances the single excitation probability from the previous theoretical benchmark of 77% to 91.9%, approaching the fundamental limits set by decoherence in the system. Benchmarking against GRAPE (Gradient Ascent Pulse Engineering) confirms that DRAG operates close to the optimal control limit, while maintaining smooth, experimentally feasible pulse shapes. These results demonstrate the effectiveness and cross platform adaptability of DRAG for a high-fidelity single photon source.", "AI": {"tldr": "\u5c06\u8d85\u5bfc\u91cf\u5b50\u6bd4\u7279\u7684DRAG\u8109\u51b2\u6574\u5f62\u6280\u672f\u9996\u6b21\u5e94\u7528\u4e8e\u91cc\u5fb7\u5821\u539f\u5b50\u8154\u7cfb\u7edf\uff0c\u901a\u8fc7\u6291\u5236\u53cc\u6fc0\u53d1\u5c06\u5355\u5149\u5b50\u6e90\u7684\u5355\u6fc0\u53d1\u6982\u7387\u4ece77%\u63d0\u5347\u81f391.9%\uff0c\u63a5\u8fd1\u7cfb\u7edf\u9000\u76f8\u5e72\u6781\u9650", "motivation": "\u91cc\u5fb7\u5821\u539f\u5b50\u8154\u7cfb\u7edf\u4e2d\u4e0d\u5b8c\u7f8e\u7684\u963b\u585e\u6548\u5e94\u4f1a\u5bfc\u81f4\u6709\u5bb3\u53cc\u6fc0\u53d1\uff0c\u4e25\u91cd\u964d\u4f4e\u5355\u5149\u5b50\u4e0d\u53ef\u533a\u5206\u6027\uff0c\u9700\u5f00\u53d1\u9ad8\u7cbe\u5ea6\u96c6\u4f53\u5355\u6fc0\u53d1\u7684\u5236\u5907\u65b9\u6cd5", "method": "\u5c06DRAG\u6280\u672f\u8de8\u5e73\u53f0\u9002\u914d\u81f3\u5149\u5b66\u8109\u51b2\u6574\u5f62\uff1a\u7ed3\u5408\u89e3\u6790\u5efa\u6a21\u4e0e\u6570\u503c\u4f18\u5316\uff0c\u8bbe\u8ba1\u6291\u5236\u53cc\u6fc0\u53d1\u7684\u8109\u51b2\u5f62\u72b6\uff0c\u5e76\u4e0eGRAPE\u6700\u4f18\u63a7\u5236\u7b97\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5", "result": "1) \u7a81\u7834\u524d\u4eba77%\u7684\u7406\u8bba\u57fa\u51c6\uff0c\u5b9e\u73b091.9%\u7684\u5355\u6fc0\u53d1\u6982\u7387\uff1b2) \u53d1\u73b0\u4e0e\u53c2\u8003\u6587\u732e\u4e0d\u540c\u7684\u65b0\u53c2\u6570\u533a\u57df\uff1b3) DRAG\u6027\u80fd\u63a5\u8fd1GRAPE\u6700\u4f18\u6781\u9650\u4e14\u8109\u51b2\u5f62\u72b6\u66f4\u5e73\u6ed1\u6613\u5b9e\u73b0", "conclusion": "DRAG\u6280\u672f\u5c55\u73b0\u51fa\u8de8\u5e73\u53f0\u6709\u6548\u6027\u548c\u5b9e\u9a8c\u53ef\u884c\u6027\uff0c\u4e3a\u9ad8\u4fdd\u771f\u5355\u5149\u5b50\u6e90\u63d0\u4f9b\u65b0\u65b9\u6848\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u91cc\u5fb7\u5821\u4f53\u7cfb\u4e2d\u7684\u4f18\u5316\u6f5c\u529b\u4e0e\u5b9e\u7528\u4ef7\u503c"}}
{"id": "2602.18015", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18015", "abs": "https://arxiv.org/abs/2602.18015", "authors": ["Jongseong Chae", "Jongeui Park", "Yongjae Shin", "Gyeongmin Kim", "Seungyul Han", "Youngchul Sung"], "title": "Flow Actor-Critic for Offline Reinforcement Learning", "comment": "Accepted to ICLR 2026", "summary": "The dataset distributions in offline reinforcement learning (RL) often exhibit complex and multi-modal distributions, necessitating expressive policies to capture such distributions beyond widely-used Gaussian policies. To handle such complex and multi-modal datasets, in this paper, we propose Flow Actor-Critic, a new actor-critic method for offline RL, based on recent flow policies. The proposed method not only uses the flow model for actor as in previous flow policies but also exploits the expressive flow model for conservative critic acquisition to prevent Q-value explosion in out-of-data regions. To this end, we propose a new form of critic regularizer based on the flow behavior proxy model obtained as a byproduct of flow-based actor design. Leveraging the flow model in this joint way, we achieve new state-of-the-art performance for test datasets of offline RL including the D4RL and recent OGBench benchmarks.", "AI": {"tldr": "Proposes Flow Actor-Critic for offline RL using flow models for both policy and conservative critic to handle multi-modal data and prevent Q-value overestimation.", "motivation": "Offline RL datasets often have complex multi-modal distributions that standard Gaussian policies cannot adequately represent, leading to performance limitations.", "method": "Introduces Flow Actor-Critic that leverages flow models for the policy (actor) and a novel flow-based regularizer for the critic to enforce conservatism in out-of-distribution regions.", "result": "Achieves new state-of-the-art performance on standard offline RL benchmarks including D4RL and OGBench.", "conclusion": "Jointly utilizing flow models for both actor and critic components effectively handles multi-modal data distributions and mitigates Q-value explosion, significantly advancing offline RL performance."}}
{"id": "2602.18375", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.18375", "abs": "https://arxiv.org/abs/2602.18375", "authors": ["Bora Baran", "Tommaso Calarco", "Matthias M. Mueller", "Felix Motzoi"], "title": "Towards scalable multi-qubit optimal control via interaction decomposition in the diagonal frame", "comment": "18 pages, 6 Figures, 1 Table", "summary": "In this work, we introduce a general n-qubit formulation of control objectives that allows a control target to be specified in a diagonal frame, so that only the diagonal entries must be characterized, thus quadratically reducing the complexity of the cost functional in constrast to a full target matrix. We do so by representing any n-qubit unitary transformation as a diagonal phase map on the computational basis states, as they are naturally diagonalizable by unitarity. By using discrete derivative operators to analytically construct support-selective phase invariants, we enable to deterministically isolate and quantify any multi-qubit interactions encoded in the phase map. These phase invariants form a coordinate system for the formulation of specific control targets in terms of arbitrary desired multi-qubit interactions, without having to invert the diagonalization during the optimizatiion, solely relying on the experimentally accesible diagonal phases. To illustrate the framework, we synthesize two genuinely tripartite entangling gates, both, diagonal and non-diagonal. These are obtained with a single shaped microwave pulse, for a numerically simulated room-temperature nitrogen-vacancy center with a three qubit nuclear spin register, with durations of about a microsecond. These results represent a factor 10-100 reduction in operation time compared with the fastest existing NV-based entanglers that act on more than two qubits at once.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5bf9\u89d2\u6846\u67b6\u7684n\u91cf\u5b50\u6bd4\u7279\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ec5\u4f18\u5316\u5bf9\u89d2\u76f8\u4f4d\u5c06\u590d\u6742\u5ea6\u4ece\u6307\u6570\u964d\u81f3\u591a\u9879\u5f0f\uff1b\u5229\u7528\u79bb\u6563\u5bfc\u6570\u7b97\u5b50\u6784\u9020\u76f8\u4f4d\u4e0d\u53d8\u91cf\u5206\u79bb\u591a\u91cf\u5b50\u6bd4\u7279\u76f8\u4e92\u4f5c\u7528\uff0c\u5728NV\u4e2d\u5fc3\u5b9e\u73b0\u5fae\u79d2\u7ea7\u4e09\u91cf\u5b50\u6bd4\u7279\u7ea0\u7f20\u95e8\uff0c\u901f\u5ea6\u63d0\u534710-100\u500d\u3002", "motivation": "\u4f20\u7edf\u591a\u91cf\u5b50\u6bd4\u7279\u63a7\u5236\u9700\u5b8c\u6574\u523b\u753b\u6307\u6570\u89c4\u6a21\u7684\u76ee\u6807\u77e9\u9635\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528\u5bf9\u89d2\u6846\u67b6\u7279\u6027\uff0c\u5c06\u4f18\u5316\u590d\u6742\u5ea6\u4eceO(4^n)\u964d\u81f3O(2^n)\uff0c\u7a81\u7834\u53ef\u6269\u5c55\u6027\u74f6\u9888\u3002", "method": "\u5c06n\u91cf\u5b50\u6bd4\u7279\u9149\u53d8\u6362\u8868\u793a\u4e3a\u8ba1\u7b97\u57fa\u6001\u7684\u5bf9\u89d2\u76f8\u4f4d\u6620\u5c04\uff1b\u5229\u7528\u79bb\u6563\u5bfc\u6570\u7b97\u5b50\u89e3\u6790\u6784\u9020\u652f\u6301\u9009\u62e9\u6027\u7684\u76f8\u4f4d\u4e0d\u53d8\u91cf\uff0c\u5728\u4e0d\u9700\u77e9\u9635\u6c42\u9006\u7684\u524d\u63d0\u4e0b\uff0c\u786e\u5b9a\u6027\u5206\u79bb\u5e76\u91cf\u5316\u76f8\u4f4d\u6620\u5c04\u4e2d\u7f16\u7801\u7684\u591a\u91cf\u5b50\u6bd4\u7279\u76f8\u4e92\u4f5c\u7528\uff0c\u8fd9\u4e9b\u4e0d\u53d8\u91cf\u6784\u6210\u63a7\u5236\u76ee\u6807\u7684\u5750\u6807\u7cfb\u3002", "result": "\u5728\u6a21\u62df\u5ba4\u6e29\u6c2e\u7a7a\u4f4d\u4e2d\u5fc3\u4e09\u91cf\u5b50\u6bd4\u7279\u6838\u81ea\u65cb\u5bc4\u5b58\u5668\u4e0a\uff0c\u4ec5\u7528\u5355\u4e2a\u6574\u5f62\u5fae\u6ce2\u8109\u51b2\u5408\u6210\u4e86\u4e24\u4e2a\u771f\u6b63\u4e09\u4f53\u7ea0\u7f20\u95e8\uff08\u5bf9\u89d2\u548c\u975e\u5bf9\u89d2\uff09\uff0c\u64cd\u4f5c\u65f6\u957f\u7ea61\u5fae\u79d2\uff0c\u6bd4\u73b0\u6709\u6700\u5feb\u7684\u591a\u91cf\u5b50\u6bd4\u7279NV\u7ea0\u7f20\u95e8\u5feb10-100\u500d\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5229\u7528\u5bf9\u89d2\u76f8\u4f4d\u4fe1\u606f\u7684\u7b80\u7ea6\u6027\u548c\u76f8\u4f4d\u4e0d\u53d8\u91cf\u7684\u89e3\u6790\u6784\u9020\uff0c\u5b9e\u73b0\u4e86\u591a\u91cf\u5b50\u6bd4\u7279\u95e8\u7684\u9ad8\u6548\u5408\u6210\uff0c\u4e3a\u53ef\u6269\u5c55\u91cf\u5b50\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u5c55\u793a\u4e86\u5728\u5b9e\u9645\u91cf\u5b50\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u5feb\u901f\u3001\u9ad8\u4fdd\u771f\u64cd\u4f5c\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.17846", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17846", "abs": "https://arxiv.org/abs/2602.17846", "authors": ["Nick Dodson", "Xinyu Gao", "Qingsong Wang", "Yusu Wang", "Zhengchao Wan"], "title": "Two Calm Ends and the Wild Middle: A Geometric Picture of Memorization in Diffusion Models", "comment": null, "summary": "Diffusion models generate high-quality samples but can also memorize training data, raising serious privacy concerns. Understanding the mechanisms governing when memorization versus generalization occurs remains an active area of research. In particular, it is unclear where along the noise schedule memorization is induced, how data geometry influences it, and how phenomena at different noise scales interact. We introduce a geometric framework that partitions the noise schedule into three regimes based on the coverage properties of training data by Gaussian shells and the concentration behavior of the posterior, which we argue are two fundamental objects governing memorization and generalization in diffusion models. This perspective reveals that memorization risk is highly non-uniform across noise levels. We further identify a danger zone at medium noise levels where memorization is most pronounced. In contrast, both the small and large noise regimes resist memorization, but through fundamentally different mechanisms: small noise avoids memorization due to limited training coverage, while large noise exhibits low posterior concentration and admits a provably near linear Gaussian denoising behavior. For the medium noise regime, we identify geometric conditions through which we propose a geometry-informed targeted intervention that mitigates memorization.", "AI": {"tldr": "This paper introduces a geometric framework that partitions diffusion model noise schedules into three regimes based on data coverage and posterior concentration, revealing that memorization is most severe at medium noise levels (\"danger zone\") while small and large noise regimes resist memorization through different mechanisms, leading to a geometry-informed intervention to mitigate memorization.", "motivation": "Diffusion models memorize training data causing privacy concerns, but it's unclear where memorization occurs along the noise schedule, how data geometry influences it, and how different noise scales interact.", "method": "Introduce a geometric framework that partitions the noise schedule into three regimes based on Gaussian shell coverage properties of training data and posterior concentration behavior, which are identified as fundamental objects governing memorization vs. generalization.", "result": "Memorization risk is highly non-uniform across noise levels. Medium noise levels constitute a \"danger zone\" with most pronounced memorization, while small noise resists memorization due to limited training coverage, and large noise resists it through low posterior concentration and near-linear Gaussian denoising behavior.", "conclusion": "Identify geometric conditions in the medium noise regime and propose a geometry-informed targeted intervention that mitigates memorization."}}
{"id": "2602.18037", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18037", "abs": "https://arxiv.org/abs/2602.18037", "authors": ["Johannes Ackermann", "Michael Noukhovitch", "Takashi Ishida", "Masashi Sugiyama"], "title": "Gradient Regularization Prevents Reward Hacking in Reinforcement Learning from Human Feedback and Verifiable Rewards", "comment": "25 pages, 15 figures", "summary": "Reinforcement Learning from Human Feedback (RLHF) or Verifiable Rewards (RLVR) are two key steps in the post-training of modern Language Models (LMs). A common problem is reward hacking, where the policy may exploit inaccuracies of the reward and learn an unintended behavior. Most previous works address this by limiting the policy update with a Kullback-Leibler (KL) penalty towards a reference model. We propose a different framing: Train the LM in a way that biases policy updates towards regions in which the reward is more accurate. First, we derive a theoretical connection between the accuracy of a reward model and the flatness of an optimum at convergence. Gradient regularization (GR) can then be used to bias training to flatter regions and thereby maintain reward model accuracy. We confirm these results by showing that the gradient norm and reward accuracy are empirically correlated in RLHF. We then show that Reference Resets of the KL penalty implicitly use GR to find flatter regions with higher reward accuracy. We further improve on this by proposing to use explicit GR with an efficient finite-difference estimate. Empirically, GR performs better than a KL penalty across a diverse set of RL experiments with LMs. GR achieves a higher GPT-judged win-rate in RLHF, avoids overly focusing on the format in rule-based math rewards, and prevents hacking the judge in LLM-as-a-Judge math tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u68af\u5ea6\u6b63\u5219\u5316\uff08GR\uff09\u4f5c\u4e3aKL\u60e9\u7f5a\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u5c06\u7b56\u7565\u66f4\u65b0\u504f\u5411\u5956\u52b1\u6a21\u578b\u66f4\u51c6\u786e\u7684\u533a\u57df\u6765\u9632\u6b62RLHF\u4e2d\u7684\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\u3002", "motivation": "\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\u662fRLHF/RLVR\u4e2d\u7684\u5e38\u89c1\u95ee\u9898\uff0c\u8bed\u8a00\u6a21\u578b\u4f1a\u5229\u7528\u5956\u52b1\u6a21\u578b\u7684\u4e0d\u51c6\u786e\u6027\u5b66\u4e60\u975e\u9884\u671f\u884c\u4e3a\uff0c\u800c\u73b0\u6709\u57fa\u4e8eKL\u60e9\u7f5a\u7684\u89e3\u51b3\u65b9\u6848\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u4f5c\u8005\u63a8\u5bfc\u4e86\u5956\u52b1\u51c6\u786e\u6027\u4e0e\u6700\u4f18\u89e3\u5e73\u5766\u5ea6\u4e4b\u95f4\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u7136\u540e\u63d0\u51fa\u4f7f\u7528\u663e\u5f0f\u68af\u5ea6\u6b63\u5219\u5316\uff08\u91c7\u7528\u6709\u9650\u5dee\u5206\u4f30\u8ba1\uff09\u6765\u5c06\u8bad\u7ec3\u504f\u5411\u66f4\u5e73\u5766\u7684\u533a\u57df\u3002", "result": "GR\u5728\u591a\u4e2a\u65b9\u9762\u4f18\u4e8eKL\u60e9\u7f5a\uff1a\u5728RLHF\u4e2d\u83b7\u5f97\u66f4\u9ad8\u7684GPT\u8bc4\u5224\u80dc\u7387\uff0c\u907f\u514d\u5728\u57fa\u4e8e\u89c4\u5219\u7684\u6570\u5b66\u5956\u52b1\u4e2d\u8fc7\u5ea6\u5173\u6ce8\u683c\u5f0f\uff0c\u9632\u6b62LLM-as-a-Judge\u6570\u5b66\u4efb\u52a1\u4e2d\u7684\u9ed1\u5ba2\u653b\u51fb\uff0c\u5e76\u9a8c\u8bc1\u4e86\u68af\u5ea6\u8303\u6570\u4e0e\u5956\u52b1\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u5b9e\u8bc1\u76f8\u5173\u6027\u3002", "conclusion": "\u5728\u8bed\u8a00\u6a21\u578b\u7684\u5404\u7c7b\u5f3a\u5316\u5b66\u4e60\u5b9e\u9a8c\u4e2d\uff0c\u663e\u5f0f\u68af\u5ea6\u6b63\u5219\u5316\u662f\u6bd4KL\u60e9\u7f5a\u66f4\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u4fdd\u6301\u5956\u52b1\u6a21\u578b\u51c6\u786e\u6027\u5e76\u9632\u6b62\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\u3002"}}
{"id": "2602.18381", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.18381", "abs": "https://arxiv.org/abs/2602.18381", "authors": ["Marek \u017bukowski", "Pawe\u0142 Cie\u015bli\u0144ski", "Marcin Markiewicz", "Konrad Schlichtholz"], "title": "Bell-GHZ nonclassicality of many-observer interwoven frustrated down conversions", "comment": "10 pages, 1 figure", "summary": "Frustrated down conversion is a process in which a quantum superposition of emissions from two separate parametric down-conversion processes gives rise to observable interference. Depending on the phase relation between the probability amplitudes associated with emissions by the first and second crystal, the process can be enhanced or suppressed. This is achieved by aligning the setup so that the signal and idler modes from the first crystal are fed into the second and constitute its signal-idler modes.\n  In Sci. Adv. 11, 1794 (2025), two-observer interwoven frustrated PDC processes produced interference effects based on path identity [Phys. Rev. Lett. 118, 080401 (2017)]. The signal and idler modes of source crystals I and II are arranged to fully overlap with the emission modes of crystals A and B, which serve as elements of measurement stations controlled by Alice and Bob. In the interwoven configuration, crystal A (B) receives the signal mode of crystal I (II) and the idler mode of crystal II (I), enabling interference between joint emission processes at the sources and at the measurement stations. It was conjectured that such interference may lead to new non-classical phenomena.\n  In arXiv:2508.19207 it was shown that the process violates the standard Clauser-Horne Bell inequality without additional assumptions, provided suitable measurement settings are used. Here we extend the interference scheme to more than two measurement stations and demonstrate a violation of one of the WWWZB inequalities. This indicates that the proposed approach may provide a general method for revealing non-classicality in a range of phenomena discussed in [Rev. Mod. Phys. 94, 025007 (2022)]. We also present a GHZ/Hardy-type argument that further highlights the paradoxical character of the interference.", "AI": {"tldr": "\u672c\u8bba\u6587\u5c06\u53d7\u632b\u53c2\u91cf\u4e0b\u8f6c\u6362\u7684\u5e72\u6d89\u65b9\u6848\u4ece\u53cc\u89c2\u6d4b\u8005\u6269\u5c55\u5230\u591a\u6d4b\u91cf\u7ad9\uff0c\u901a\u8fc7WWWZB\u4e0d\u7b49\u5f0f violation \u548cGHZ/Hardy\u578b\u6096\u8bba\uff0c\u5c55\u793a\u4e86\u4e00\u79cd\u63ed\u793a\u91cf\u5b50\u975e\u7ecf\u5178\u6027\u7684\u901a\u7528\u65b9\u6cd5\u3002", "motivation": "\u5c06\u53cc\u89c2\u6d4b\u8005\u76f8\u4e92\u4ea4\u7ec7\u7684\u53d7\u632b\u53c2\u91cf\u4e0b\u8f6c\u6362\u5e72\u6d89\u65b9\u6848\u63a8\u5e7f\u81f3\u4e24\u4e2a\u4ee5\u4e0a\u6d4b\u91cf\u7ad9\uff0c\u5e76\u9a8c\u8bc1\u5176\u4f5c\u4e3a\u63ed\u793a\u91cf\u5b50\u975e\u7ecf\u5178\u6027\u901a\u7528\u65b9\u6cd5\u7684\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u6676\u4f53\u6392\u5217\u4f7f\u6e90\u6676\u4f53\u7684\u4fe1\u53f7/\u95f2\u9891\u6a21\u5f0f\u4e0e\u6d4b\u91cf\u6676\u4f53\u7684\u53d1\u5c04\u6a21\u5f0f\u5b8c\u5168\u91cd\u53e0\uff0c\u5b9e\u73b0\u6e90\u4e0e\u6d4b\u91cf\u7ad9\u95f4\u7684\u8054\u5408\u53d1\u5c04\u5e72\u6d89\uff1b\u901a\u8fc7\u4f18\u5316\u6d4b\u91cf\u8bbe\u7f6e\uff0c\u6d4b\u8bd5WWWZB\u4e0d\u7b49\u5f0f\uff0c\u5e76\u6784\u9020GHZ/Hardy\u578b\u903b\u8f91\u8bba\u8bc1\u3002", "result": "\u5728\u591a\u6d4b\u91cf\u7ad9\u914d\u7f6e\u4e0b\u89c2\u6d4b\u5230WWWZB\u4e0d\u7b49\u5f0f\u7684 violation\uff0c\u8bc1\u5b9e\u8be5\u65b9\u6848\u53ef\u5e7f\u6cdb\u7528\u4e8e\u63ed\u793a\u5404\u7c7b\u91cf\u5b50\u73b0\u8c61\u4e2d\u7684\u975e\u7ecf\u5178\u6027\uff0c\u5176GHZ/Hardy\u578b\u8bba\u8bc1\u7a81\u663e\u4e86\u5e72\u6d89\u7684\u6096\u8bba\u7279\u5f81\u3002", "conclusion": "\u53d7\u632b\u4e0b\u8f6c\u6362\u5e72\u6d89\u65b9\u6848\u4e3a\u591a\u7c92\u5b50\u91cf\u5b50\u975e\u7ecf\u5178\u6027\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u901a\u7528\u6846\u67b6\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u8d1d\u5c14\u4e0d\u7b49\u5f0f\u68c0\u9a8c\uff0c\u6df1\u5316\u4e86\u5bf9\u91cf\u5b50\u6096\u8bba\u7684\u7406\u89e3\u3002"}}
{"id": "2602.18117", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18117", "abs": "https://arxiv.org/abs/2602.18117", "authors": ["Yongjae Shin", "Jongseong Chae", "Jongeui Park", "Youngchul Sung"], "title": "Flow Matching with Injected Noise for Offline-to-Online Reinforcement Learning", "comment": "ICLR 2026 camera-ready", "summary": "Generative models have recently demonstrated remarkable success across diverse domains, motivating their adoption as expressive policies in reinforcement learning (RL). While they have shown strong performance in offline RL, particularly where the target distribution is well defined, their extension to online fine-tuning has largely been treated as a direct continuation of offline pre-training, leaving key challenges unaddressed. In this paper, we propose Flow Matching with Injected Noise for Offline-to-Online RL (FINO), a novel method that leverages flow matching-based policies to enhance sample efficiency for offline-to-online RL. FINO facilitates effective exploration by injecting noise into policy training, thereby encouraging a broader range of actions beyond those observed in the offline dataset. In addition to exploration-enhanced flow policy training, we combine an entropy-guided sampling mechanism to balance exploration and exploitation, allowing the policy to adapt its behavior throughout online fine-tuning. Experiments across diverse, challenging tasks demonstrate that FINO consistently achieves superior performance under limited online budgets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFINO\uff08Flow Matching with Injected Noise for Offline-to-Online RL\uff09\uff0c\u4e00\u79cd\u5229\u7528\u6d41\u5339\u914d\u7b56\u7565\u5e76\u901a\u8fc7\u6ce8\u5165\u566a\u58f0\u6765\u589e\u5f3a\u79bb\u7ebf\u5230\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u6837\u672c\u6548\u7387\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u71b5\u5f15\u5bfc\u91c7\u6837\u673a\u5236\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u5728\u6709\u9650\u5728\u7ebf\u9884\u7b97\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u751f\u6210\u5f0f\u6a21\u578b\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u5176\u5411\u5728\u7ebf\u5fae\u8c03\u6269\u5c55\u65f6\u901a\u5e38\u4ec5\u88ab\u89c6\u4e3a\u79bb\u7ebf\u9884\u8bad\u7ec3\u7684\u7b80\u5355\u5ef6\u7eed\uff0c\u5173\u952e\u6311\u6218\u672a\u88ab\u6709\u6548\u89e3\u51b3\u3002", "method": "\u63d0\u51faFINO\u65b9\u6cd5\uff1a1\uff09\u5728\u6d41\u5339\u914d\u7b56\u7565\u8bad\u7ec3\u4e2d\u6ce8\u5165\u566a\u58f0\u4ee5\u9f13\u52b1\u8d85\u51fa\u79bb\u7ebf\u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u63a2\u7d22\uff1b2\uff09\u7ed3\u5408\u71b5\u5f15\u5bfc\u91c7\u6837\u673a\u5236\u5728\u5728\u7ebf\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u52a8\u6001\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "result": "\u5728\u591a\u79cd\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u4e0a\uff0cFINO\u5728\u53d7\u9650\u7684\u5728\u7ebf\u9884\u7b97\u4e0b\u59cb\u7ec8\u5b9e\u73b0\u66f4\u4f18\u6027\u80fd\u3002", "conclusion": "FINO\u901a\u8fc7\u566a\u58f0\u6ce8\u5165\u548c\u71b5\u5f15\u5bfc\u91c7\u6837\u6709\u6548\u89e3\u51b3\u4e86\u751f\u6210\u5f0f\u7b56\u7565\u4ece\u79bb\u7ebf\u5230\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u8fc1\u79fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\u3002"}}
{"id": "2602.18388", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.18388", "abs": "https://arxiv.org/abs/2602.18388", "authors": ["G. R. Di Carlo", "M. Samiotis", "A. Kamlapure", "M. Finkel", "N. Muthusubramanian", "M. W. Beekman", "N. Haider", "B. Segers", "S. Vall\u00e9s-Sanclemente", "L. DiCarlo"], "title": "Qubit error bursts in superconducting quantum processors of Quantum Inspire: quasiparticle pumping and anomalous time dependence", "comment": null, "summary": "We investigate qubit error bursts in 5- and 7-transmon processors of similar design, fabrication and packaging, but with different types of qubit Josephson junctions. Measurements for each are performed in two refrigerators to discern device-specific from refrigerator-dependent characteristics. The duration and rate of bursts are device specific but within the range of prior experiments and consistent with ionizing radiation. We observe two unforeseen signatures specifically in the processor with Dolan junctions. First, increasing the rate of $\u03c0$ pulsing in the detection scheme shortens the recovery time to equilibrium, which is explained by a quasiparticle pumping mechanism. The second signature is an anomalous time dependence in the burst rate: a surge happens days or weeks after cooldown, followed by a strong suppression that persists until thermal cycling.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e865\u4f4d\u548c7\u4f4d\u8de8\u5b50\u5904\u7406\u5668\u5728\u4e0d\u540c\u7c7b\u578b\u7684\u7ea6\u745f\u592b\u68ee\u7ed3\uff08Dolan\u7ed3 vs. \u6807\u51c6\u7ed3\uff09\u4e0b\u7684\u91cf\u5b50\u4f4d\u9519\u8bef\u7206\u53d1\uff0c\u5e76\u5728\u4e24\u53f0\u51b0\u7bb1\u4e2d\u6d4b\u91cf\u4ee5\u533a\u5206\u8bbe\u5907\u7279\u6027\u548c\u51b0\u7bb1\u4f9d\u8d56\u6027\u3002\u867d\u7136\u9519\u8bef\u7206\u53d1\u7684\u6301\u7eed\u65f6\u95f4\u548c\u901f\u7387\u662f\u8bbe\u5907\u7279\u5b9a\u7684\uff0c\u4e14\u4e0e\u8f90\u5c04\u5b9e\u9a8c\u4e00\u81f4\uff0c\u4f46Dolan\u7ed3\u5904\u7406\u5668\u8868\u73b0\u51fa\u4e24\u79cd\u610f\u5916\u73b0\u8c61\uff1a\u589e\u52a0\u03c0\u8109\u51b2\u901f\u7387\u53ef\u7f29\u77ed\u6062\u590d\u65f6\u95f4\uff08\u7531\u51c6\u7c92\u5b50\u6cf5\u6d66\u673a\u5236\u89e3\u91ca\uff09\uff0c\u4ee5\u53ca\u51b7\u5374\u540e\u51e0\u5929/\u5468\u51fa\u73b0\u9519\u8bef\u7387\u6fc0\u589e\uff0c\u968f\u540e\u88ab\u5f3a\u70c8\u6291\u5236\u76f4\u5230\u70ed\u5faa\u73af\uff0c\u8fd9\u8868\u660e\u7ed3\u7ed3\u6784\u5177\u6709\u72ec\u7279\u52a8\u529b\u5b66\u3002", "motivation": "\u8be5\u8bba\u6587\u7684\u52a8\u673a\u662f\u63a2\u7a76\u8d85\u5bfc\u91cf\u5b50\u6bd4\u7279\u9519\u8bef\u7206\u53d1\u7684\u6765\u6e90\uff0c\u533a\u5206\u662f\u7531\u7ea6\u745f\u592b\u68ee\u7ed3\u7b49\u8bbe\u5907\u7279\u5b9a\u56e0\u7d20\u5f15\u8d77\uff0c\u8fd8\u662f\u7531\u5236\u51b7\u673a\u7b49\u5916\u90e8\u7cfb\u7edf\u4f9d\u8d56\u6027\u56e0\u7d20\u9020\u6210\uff0c\u4ece\u800c\u4e3a\u63d0\u9ad8\u91cf\u5b50\u5904\u7406\u5668\u76f8\u5e72\u65f6\u95f4\u548c\u53ef\u9760\u6027\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u5305\u62ec\uff1a\u8bbe\u8ba1\u5236\u9020\u4e24\u4e2a\u76f8\u4f3c\u76845\u4f4d\u548c7\u4f4d\u8de8\u5b50\u5904\u7406\u5668\uff0c\u5206\u522b\u91c7\u7528Dolan\u7ed3\u548c\u6807\u51c6\u7ea6\u745f\u592b\u68ee\u7ed3\uff1b\u5728\u4e24\u53f0\u4e0d\u540c\u7684\u7a00\u91ca\u5236\u51b7\u673a\u4e2d\u5206\u522b\u6d4b\u91cf\u5b83\u4eec\u7684\u9519\u8bef\u7206\u53d1\u7279\u6027\uff1b\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c\u8bbe\u5907\u548c\u51b0\u7bb1\u4e2d\u7684\u7206\u53d1\u6301\u7eed\u65f6\u95f4\u548c\u901f\u7387\u6765\u533a\u5206\u8bbe\u5907\u7279\u5f02\u6027\u6548\u5e94\uff1b\u6539\u53d8\u03c0\u8109\u51b2\u68c0\u6d4b\u65b9\u6848\u7684\u901f\u7387\u4ee5\u7814\u7a76\u6062\u590d\u52a8\u529b\u5b66\uff1b\u5e76\u957f\u65f6\u95f4\u76d1\u6d4b\u51b7\u5374\u540e\u7684\u9519\u8bef\u7387\u53d8\u5316\u3002", "result": "\u4e3b\u8981\u7ed3\u679c\uff1a1) \u9519\u8bef\u7206\u53d1\u7279\u6027\u5177\u6709\u8bbe\u5907\u7279\u5f02\u6027\uff0c\u4e0e\u5148\u524d\u8f90\u5c04\u5b9e\u9a8c\u7ed3\u679c\u4e00\u81f4\uff1b2) Dolan\u7ed3\u5904\u7406\u5668\u8868\u73b0\u51fa\u7b2c\u4e00\u4e2a\u610f\u5916\u7279\u5f81\uff1a\u589e\u52a0\u03c0\u8109\u51b2\u901f\u7387\u4f1a\u7f29\u77ed\u6062\u590d\u5e73\u8861\u65f6\u95f4\uff0c\u53ef\u7528\u51c6\u7c92\u5b50\u6cf5\u6d66\u673a\u5236\u89e3\u91ca\uff1b3) Dolan\u7ed3\u5904\u7406\u5668\u8868\u73b0\u51fa\u7b2c\u4e8c\u4e2a\u610f\u5916\u7279\u5f81\uff1a\u9519\u8bef\u7387\u5728\u51b7\u5374\u540e\u51e0\u5929\u6216\u51e0\u5468\u4f1a\u51fa\u73b0\u6fc0\u589e\uff0c\u968f\u540e\u88ab\u5f3a\u70c8\u6291\u5236\uff0c\u76f4\u5230\u70ed\u5faa\u73af\u624d\u6062\u590d\uff1b4) \u8fd9\u4e9b\u5f02\u5e38\u7279\u5f81\u4ec5\u5b58\u5728\u4e8eDolan\u7ed3\u5904\u7406\u5668\uff0c\u4e0e\u51b0\u7bb1\u7c7b\u578b\u65e0\u5173\u3002", "conclusion": "\u7ed3\u8bba\u662f\u7ea6\u745f\u592b\u68ee\u7ed3\u7684\u51e0\u4f55\u7ed3\u6784\uff08\u7279\u522b\u662fDolan\u7ed3\uff09\u4f1a\u5f15\u5165\u72ec\u7279\u7684\u51c6\u7c92\u5b50\u52a8\u529b\u5b66\uff0c\u5f71\u54cd\u91cf\u5b50\u6bd4\u7279\u9519\u8bef\u7206\u53d1\u7684\u957f\u671f\u884c\u4e3a\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\u7ed3\u8bbe\u8ba1\u5bf9\u91cf\u5b50\u6bd4\u7279\u7a33\u5b9a\u6027\u81f3\u5173\u91cd\u8981\uff0c\u5bf9\u91cf\u5b50\u5904\u7406\u5668\u8bbe\u8ba1\u548c\u9519\u8bef\u7f13\u89e3\u7b56\u7565\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u5f02\u5e38\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\u884c\u4e3a\u63ed\u793a\u4e86\u7406\u89e3\u7ed3\u6750\u6599\u4e2d\u6162\u5f1b\u8c6b\u8fc7\u7a0b\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2602.17861", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17861", "abs": "https://arxiv.org/abs/2602.17861", "authors": ["Ryan McKenna", "Galen Andrew", "Borja Balle", "Vadym Doroshenko", "Arun Ganesh", "Weiwei Kong", "Alex Kurakin", "Brendan McMahan", "Mikhail Pravilov"], "title": "JAX-Privacy: A library for differentially private machine learning", "comment": null, "summary": "JAX-Privacy is a library designed to simplify the deployment of robust and performant mechanisms for differentially private machine learning. Guided by design principles of usability, flexibility, and efficiency, JAX-Privacy serves both researchers requiring deep customization and practitioners who want a more out-of-the-box experience. The library provides verified, modular primitives for critical components for all aspects of the mechanism design including batch selection, gradient clipping, noise addition, accounting, and auditing, and brings together a large body of recent research on differentially private ML.", "AI": {"tldr": "JAX-Privacy\u662f\u4e00\u4e2a\u65e8\u5728\u7b80\u5316\u5dee\u5206\u9690\u79c1\u673a\u5668\u5b66\u4e60\u673a\u5236\u90e8\u7f72\u7684\u5e93\uff0c\u63d0\u4f9b\u6a21\u5757\u5316\u7ec4\u4ef6\u548c\u6700\u65b0\u7814\u7a76\u6210\u679c\uff0c\u540c\u65f6\u6ee1\u8db3\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u7684\u9700\u6c42\u3002", "motivation": "\u5dee\u5206\u9690\u79c1\u673a\u5668\u5b66\u4e60\u673a\u5236\u90e8\u7f72\u590d\u6742\uff0c\u7814\u7a76\u4eba\u5458\u9700\u8981\u6df1\u5ea6\u5b9a\u5236\u800c\u4ece\u4e1a\u8005\u9700\u8981\u5f00\u7bb1\u5373\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002\u73b0\u6709\u5de5\u5177\u5728\u53ef\u7528\u6027\u3001\u7075\u6d3b\u6027\u548c\u6548\u7387\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u7edf\u4e00\u7684\u6846\u67b6\u6765\u6574\u5408\u6700\u65b0\u7814\u7a76\u6210\u679c\u3002", "method": "\u57fa\u4e8e\u53ef\u7528\u6027\u3001\u7075\u6d3b\u6027\u548c\u6548\u7387\u7684\u8bbe\u8ba1\u539f\u5219\uff0c\u63d0\u4f9b\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u6a21\u5757\u5316\u539f\u8bed\uff0c\u8986\u76d6\u6279\u9009\u62e9\u3001\u68af\u5ea6\u88c1\u526a\u3001\u566a\u58f0\u6dfb\u52a0\u3001\u4f1a\u8ba1\u548c\u5ba1\u8ba1\u7b49\u5173\u952e\u73af\u8282\uff0c\u5e76\u6574\u5408\u8fd1\u671f\u5dee\u5206\u9690\u79c1ML\u7814\u7a76\u3002", "result": "\u5f00\u53d1\u51fa\u4e00\u4e2a\u540c\u65f6\u670d\u52a1\u7814\u7a76\u4eba\u5458\uff08\u652f\u6301\u6df1\u5ea6\u5b9a\u5236\uff09\u548c\u4ece\u4e1a\u8005\uff08\u63d0\u4f9b\u5f00\u7bb1\u4f53\u9a8c\uff09\u7684\u5e93\uff0c\u63d0\u4f9b\u5173\u952e\u7ec4\u4ef6\u7684\u6a21\u5757\u5316\u5b9e\u73b0\uff0c\u5e76\u96c6\u6210\u5927\u91cf\u6700\u65b0\u7814\u7a76\u6210\u679c\u3002", "conclusion": "JAX-Privacy\u6210\u529f\u6784\u5efa\u4e86\u8fde\u63a5\u5dee\u5206\u9690\u79c1ML\u7814\u7a76\u4e0e\u5b9e\u8df5\u7684\u6865\u6881\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u6700\u65b0\u7814\u7a76\u6574\u5408\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5dee\u5206\u9690\u79c1\u673a\u5668\u5b66\u4e60\u7684\u90e8\u7f72\u95e8\u69db\u3002"}}
{"id": "2602.18182", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18182", "abs": "https://arxiv.org/abs/2602.18182", "authors": ["Daniel Romero-Alvarado", "Fernando Mart\u00ednez-Plumed", "Lorenzo Pacchiardi", "Hugo Save", "Siddhesh Milind Pawar", "Behzad Mehrbakhsh", "Pablo Antonio Moreno Casares", "Ben Slater", "Paolo Bova", "Peter Romero", "Zachary R. Tyler", "Jonathan Prunty", "Luning Sun", "Jose Hernandez-Orallo"], "title": "Capabilities Ain't All You Need: Measuring Propensities in AI", "comment": null, "summary": "AI evaluation has primarily focused on measuring capabilities, with formal approaches inspired from Item Response Theory (IRT) being increasingly applied. Yet propensities - the tendencies of models to exhibit particular behaviours - play a central role in determining both performance and safety outcomes. However, traditional IRT describes a model's success on a task as a monotonic function of model capabilities and task demands, an approach unsuited to propensities, where both excess and deficiency can be problematic. Here, we introduce the first formal framework for measuring AI propensities by using a bilogistic formulation for model success, which attributes high success probability when the model's propensity is within an \"ideal band\". Further, we estimate the limits of the ideal band using LLMs equipped with newly developed task-agnostic rubrics. Applying our framework to six families of LLM models whose propensities are incited in either direction, we find that we can measure how much the propensity is shifted and what effect this has on the tasks. Critically, propensities estimated using one benchmark successfully predict behaviour on held-out tasks. Moreover, we obtain stronger predictive power when combining propensities and capabilities than either separately. More broadly, our framework showcases how rigorous propensity measurements can be conducted and how it yields gains over solely using capability evaluations to predict AI behaviour.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2aAI\u503e\u5411\u6027\u6d4b\u91cf\u7684\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u91c7\u7528bilogistic\u6a21\u578b\u523b\u753b\u503e\u5411\u6027\"\u7406\u60f3\u5e26\"\uff0c\u5b9e\u73b0\u884c\u4e3a\u9884\u6d4b\u5e76\u8d85\u8d8a\u7eaf\u80fd\u529b\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709AI\u8bc4\u4f30\u8fc7\u5ea6\u5173\u6ce8\u80fd\u529b\uff0c\u5ffd\u89c6\u503e\u5411\u6027\uff08\u884c\u4e3a\u503e\u5411\uff09\u5bf9\u6027\u80fd\u548c\u5b89\u5168\u7684\u5173\u952e\u4f5c\u7528\uff1b\u4f20\u7edfIRT\u6a21\u578b\u5355\u8c03\u5047\u8bbe\u65e0\u6cd5\u63cf\u8ff0\u503e\u5411\u6027\u7684\"\u8fc7\u72b9\u4e0d\u53ca\"\u73b0\u8c61\u3002", "method": "\u8bbe\u8ba1bilogistic\u6210\u529f\u6982\u7387\u6a21\u578b\uff0c\u5b9a\u4e49\u503e\u5411\u6027\u6700\u4f18\u533a\u95f4\uff1b\u5f00\u53d1\u4efb\u52a1\u65e0\u5173\u8bc4\u5206\u6807\u51c6\uff0c\u5229\u7528LLM\u4f30\u8ba1\u53c2\u6570\uff1b\u5728\u516d\u79cd\u503e\u5411\u6027\u53ef\u63a7\u504f\u79fb\u7684\u6a21\u578b\u65cf\u4e0a\u9a8c\u8bc1\u3002", "result": "\u6210\u529f\u91cf\u5316\u503e\u5411\u6027\u504f\u79fb\u53ca\u5176\u4efb\u52a1\u5f71\u54cd\uff1b\u5355\u57fa\u51c6\u503e\u5411\u6027\u53ef\u6cdb\u5316\u9884\u6d4b\u65b0\u4efb\u52a1\u884c\u4e3a\uff1b\u80fd\u529b+\u503e\u5411\u6027\u8054\u5408\u5efa\u6a21\u6bd4\u5355\u4e00\u6307\u6807\u9884\u6d4b\u529b\u66f4\u5f3a\u3002", "conclusion": "\u5efa\u7acb\u4e25\u8c28\u503e\u5411\u6027\u6d4b\u91cf\u65b9\u6cd5\uff0c\u8bc1\u660e\u6574\u5408\u503e\u5411\u6027\u8bc4\u4f30\u53ef\u663e\u8457\u63d0\u5347AI\u884c\u4e3a\u9884\u6d4b\u7cbe\u5ea6\uff0c\u4e3a\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2602.18195", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18195", "abs": "https://arxiv.org/abs/2602.18195", "authors": ["Hairong Chen", "Yicheng Feng", "Ziyu Jia", "Samir Bhatt", "Hengguan Huang"], "title": "LERD: Latent Event-Relational Dynamics for Neurodegenerative Classification", "comment": null, "summary": "Alzheimer's disease (AD) alters brain electrophysiology and disrupts multichannel EEG dynamics, making accurate and clinically useful EEG-based diagnosis increasingly important for screening and disease monitoring. However, many existing approaches rely on black-box classifiers and do not explicitly model the underlying dynamics that generate observed signals. To address these limitations, we propose LERD, an end-to-end Bayesian electrophysiological neural dynamical system that infers latent neural events and their relational structure directly from multichannel EEG without event or interaction annotations. LERD combines a continuous-time event inference module with a stochastic event-generation process to capture flexible temporal patterns, while incorporating an electrophysiology-inspired dynamical prior to guide learning in a principled way. We further provide theoretical analysis that yields a tractable bound for training and stability guarantees for the inferred relational dynamics. Extensive experiments on synthetic benchmarks and two real-world AD EEG cohorts demonstrate that LERD consistently outperforms strong baselines and yields physiology-aligned latent summaries that help characterize group-level dynamical differences.", "AI": {"tldr": "A Bayesian neural dynamical system called LERD that infers latent neural events from EEG to improve Alzheimer's diagnosis by modeling underlying brain dynamics rather than using black-box classifiers.", "motivation": "Alzheimer's disease alters brain electrophysiology and disrupts EEG dynamics, making EEG-based diagnosis increasingly important for screening and monitoring; however, existing black-box approaches fail to model the underlying generative dynamics.", "method": "LERD is an end-to-end Bayesian electrophysiological neural dynamical system that infers latent neural events and their relational structure from multichannel EEG without annotations, combining continuous-time event inference with a stochastic event-generation process and an electrophysiology-inspired dynamical prior.", "result": "Extensive experiments on synthetic benchmarks and two real-world AD EEG cohorts demonstrate LERD consistently outperforms strong baselines and yields physiology-aligned latent summaries that help characterize group-level dynamical differences.", "conclusion": "LERD provides a principled, interpretable approach to modeling neural dynamics from EEG, offering superior performance and clinically useful latent representations for Alzheimer's disease diagnosis and monitoring."}}
{"id": "2602.17867", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17867", "abs": "https://arxiv.org/abs/2602.17867", "authors": ["Jo\u00e3o N. Cardoso", "Arlindo L. Oliveira", "Bruno Martins"], "title": "ADAPT: Hybrid Prompt Optimization for LLM Feature Visualization", "comment": null, "summary": "Understanding what features are encoded by learned directions in LLM activation space requires identifying inputs that strongly activate them. Feature visualization, which optimizes inputs to maximally activate a target direction, offers an alternative to costly dataset search approaches, but remains underexplored for LLMs due to the discrete nature of text. Furthermore, existing prompt optimization techniques are poorly suited to this domain, which is highly prone to local minima. To overcome these limitations, we introduce ADAPT, a hybrid method combining beam search initialization with adaptive gradient-guided mutation, designed around these failure modes. We evaluate on Sparse Autoencoder latents from Gemma 2 2B, proposing metrics grounded in dataset activation statistics to enable rigorous comparison, and show that ADAPT consistently outperforms prior methods across layers and latent types. Our results establish that feature visualization for LLMs is tractable, but requires design assumptions tailored to the domain.", "AI": {"tldr": "This paper proposes ADAPT, a hybrid method combining beam search initialization with adaptive gradient-guided mutation to optimize text inputs that maximally activate specific directions in LLM activation spaces, demonstrating that feature visualization for LLMs is feasible but requires domain-specific designs.", "motivation": "Understanding features encoded in LLM activation spaces requires identifying strongly activating inputs, but traditional dataset search is costly and existing prompt optimization methods fail due to text's discrete nature and the domain's susceptibility to local minima.", "method": "ADAPT hybrid method that combines beam search initialization with adaptive gradient-guided mutation, specifically designed to address local minima problems in text optimization for feature visualization.", "result": "ADAPT consistently outperforms prior methods across different layers and latent types when evaluated on Sparse Autoencoder latents from Gemma 2 2B, using metrics grounded in dataset activation statistics.", "conclusion": "Feature visualization for LLMs is tractable but necessitates design assumptions specifically tailored to the discrete text domain and its optimization challenges."}}
{"id": "2602.18230", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18230", "abs": "https://arxiv.org/abs/2602.18230", "authors": ["Jorge Carrasco Pollo", "Ioannis Kapetangeorgis", "Joshua Rosenthal", "John Hua Yao"], "title": "[Re] Benchmarking LLM Capabilities in Negotiation through Scoreable Games", "comment": "Accepted for publication at Transactions on Machine Learning Research (TMLR) and MLRC Journal Track, 2025. Code available at: https://github.com/joshrosie/FACT29", "summary": "Large Language Models (LLMs) demonstrate significant potential in multi-agent negotiation tasks, yet evaluation in this domain remains challenging due to a lack of robust and generalizable benchmarks. Abdelnabi et al. (2024) introduce a negotiation benchmark based on Scoreable Games, with the aim of developing a highly complex and realistic evaluation framework for LLMs. Our work investigates the reproducibility of claims in their benchmark, and provides a deeper understanding of its usability and generalizability. We replicate the original experiments on additional models, and introduce additional metrics to verify negotiation quality and evenness of evaluation. Our findings reveal that while the benchmark is indeed complex, model comparison is ambiguous, raising questions about its objectivity. Furthermore, we identify limitations in the experimental setup, particularly in information leakage detection and thoroughness of the ablation study. By examining and analyzing the behavior of a wider range of models on an extended version of the benchmark, we reveal insights that provide additional context to potential users. Our results highlight the importance of context in model-comparative evaluations.", "AI": {"tldr": "\u672c\u7814\u7a76\u590d\u5236\u5e76\u6269\u5c55\u4e86Abdelnabi\u7b49(2024)\u57fa\u4e8eScoreable Games\u7684\u8c08\u5224\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u8be5\u57fa\u51c6\u867d\u7136\u590d\u6742\uff0c\u4f46\u6a21\u578b\u6bd4\u8f83\u7ed3\u679c\u5b58\u5728\u6a21\u7cca\u6027\uff0c\u8d28\u7591\u5176\u5ba2\u89c2\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u5b9e\u9a8c\u8bbe\u7f6e\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u8c08\u5224\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u7a33\u5065\u4e14\u53ef\u6cdb\u5316\u7684LLMs\u8bc4\u4f30\u57fa\u51c6\uff0c\u867d\u7136Abdelnabi\u7b49\u63d0\u51fa\u4e86\u57fa\u4e8eScoreable Games\u7684\u590d\u6742\u57fa\u51c6\uff0c\u4f46\u5176\u53ef\u91cd\u590d\u6027\u3001\u53ef\u7528\u6027\u548c\u6cdb\u5316\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u9a8c\u8bc1\u3002", "method": "\u590d\u5236\u539f\u59cb\u5b9e\u9a8c\u5e76\u6269\u5c55\u5230\u66f4\u591a\u6a21\u578b\uff0c\u5f15\u5165\u989d\u5916\u6307\u6807\u8bc4\u4f30\u8c08\u5224\u8d28\u91cf\u548c\u8bc4\u4f30\u516c\u5e73\u6027\uff0c\u901a\u8fc7\u6269\u5c55\u7248\u57fa\u51c6\u6d4b\u8bd5\u5206\u6790\u66f4\u5e7f\u6cdb\u6a21\u578b\u7684\u884c\u4e3a\u8868\u73b0\u3002", "result": "\u9a8c\u8bc1\u4e86\u57fa\u51c6\u7684\u590d\u6742\u6027\uff0c\u4f46\u53d1\u73b0\u6a21\u578b\u6bd4\u8f83\u7ed3\u679c\u6a21\u7cca\uff0c\u5b58\u5728\u5ba2\u89c2\u6027\u7591\u95ee\uff1b\u8bc6\u522b\u51fa\u5b9e\u9a8c\u8bbe\u7f6e\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u4fe1\u606f\u6cc4\u6f0f\u68c0\u6d4b\u548c\u6d88\u878d\u7814\u7a76\u7684\u5f7b\u5e95\u6027\u4e0d\u8db3\u3002", "conclusion": "\u6a21\u578b\u6bd4\u8f83\u8bc4\u4f30\u4e2d\u4e0a\u4e0b\u6587\u7684\u91cd\u8981\u6027\u51f8\u663e\uff0c\u8be5\u8c08\u5224\u57fa\u51c6\u867d\u5177\u6311\u6218\u6027\uff0c\u4f46\u5728\u5ba2\u89c2\u6027\u548c\u5b9e\u9a8c\u8bbe\u8ba1\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u624d\u80fd\u4f5c\u4e3a\u53ef\u9760\u7684\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2602.18292", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18292", "abs": "https://arxiv.org/abs/2602.18292", "authors": ["Xiaotong Ji", "Rasul Tutunov", "Matthieu Zimmer", "Haitham Bou-Ammar"], "title": "Decoding as Optimisation on the Probability Simplex: From Top-K to Top-P (Nucleus) to Best-of-K Samplers", "comment": null, "summary": "Decoding sits between a language model and everything we do with it, yet it is still treated as a heuristic knob-tuning exercise. We argue decoding should be understood as a principled optimisation layer: at each token, we solve a regularised problem over the probability simplex that trades off model score against structural preferences and constraints. This single template recovers greedy decoding, Softmax sampling, Top-K, Top-P, and Sparsemax-style sparsity as special cases, and explains their common structure through optimality conditions. More importantly, the framework makes it easy to invent new decoders without folklore. We demonstrate this by designing Best-of-K (BoK), a KL-anchored coverage objective aimed at multi-sample pipelines (self-consistency, reranking, verifier selection). BoK targets the probability of covering good alternatives within a fixed K-sample budget and improves empirical performance. We show that such samples can improve accuracy by, for example, +18.6% for Qwen2.5-Math-7B on MATH500 at high sampling temperatures.", "AI": {"tldr": "Proposes a principled optimization framework for language model decoding that unifies existing methods (greedy, Top-K, Top-P, etc.) and enables new decoders like Best-of-K, which improves performance by up to +18.6% on math problems", "motivation": "Current decoding methods are treated as heuristic knob-tuning rather than principled optimization, lacking a unified theoretical foundation that explains their common structure and limits innovation", "method": "Frames decoding as a regularized optimization problem over the probability simplex at each token, trading off model score against structural preferences/constraints. This single template recovers existing methods through optimality conditions and enables systematic design of new decoders like Best-of-K with a KL-anchored coverage objective for multi-sample pipelines", "result": "The framework unifies greedy decoding, Softmax sampling, Top-K, Top-P, and Sparsemax as special cases. Best-of-K achieves significant empirical gains, e.g., +18.6% accuracy improvement for Qwen2.5-Math-7B on MATH500 at high sampling temperatures", "conclusion": "Viewing decoding as a principled optimization layer is superior to heuristic tuning, providing theoretical clarity and enabling effective innovation in decoding strategies for language models"}}
{"id": "2602.18297", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.18297", "abs": "https://arxiv.org/abs/2602.18297", "authors": ["Usman Anwar", "Tim Bakker", "Dana Kianfar", "Cristina Pinneri", "Christos Louizos"], "title": "Analyzing and Improving Chain-of-Thought Monitorability Through Information Theory", "comment": "First two authors contributed equally", "summary": "Chain-of-thought (CoT) monitors are LLM-based systems that analyze reasoning traces to detect when outputs may exhibit attributes of interest, such as test-hacking behavior during code generation. In this paper, we use information-theoretic analysis to show that non-zero mutual information between CoT and output is a necessary but not sufficient condition for CoT monitorability. We identify two sources of approximation error that may undermine the performance of CoT monitors in practice: information gap, which measures the extent to which the monitor can extract the information available in CoT, and elicitation error, which measures the extent to which the monitor approximates the optimal monitoring function. We further demonstrate that CoT monitorability can be systematically improved through targeted training objectives. To this end, we propose two complementary approaches: (a) an oracle-based method that directly rewards the monitored model for producing CoTs that maximize monitor accuracy, and (b) a more practical, label-free approach that maximizes conditional mutual information between outputs and CoTs. Across multiple different environments, we show both methods significantly improve monitor accuracy while preventing CoT degeneration even when training against a monitor, thereby mitigating reward hacking when the task reward is imperfectly specified.", "AI": {"tldr": "This paper uses information-theoretic analysis to show that non-zero mutual information between CoT and output is necessary but not sufficient for CoT monitorability. It identifies two approximation errors (information gap and elicitation error) and proposes two training methods\u2014oracle-based and label-free\u2014to improve monitor accuracy and prevent reward hacking.", "motivation": "CoT monitors detect problematic behaviors like test-hacking in LLM outputs, but their real-world performance is undermined by approximation errors. Understanding these limitations and improving CoT monitorability is critical for reliable LLM oversight.", "method": "Applied information-theoretic analysis to characterize CoT monitorability; identified two error sources; proposed complementary training approaches: (a) oracle-based method rewarding CoTs that maximize monitor accuracy, and (b) label-free method maximizing conditional mutual information between outputs and CoTs.", "result": "Both methods significantly improved monitor accuracy across multiple environments while preventing CoT degeneration, effectively mitigating reward hacking even with imperfectly specified task rewards.", "conclusion": "Systematic training objectives can enhance CoT monitorability, providing more reliable detection of undesirable LLM behaviors while preserving reasoning quality."}}
{"id": "2602.17898", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17898", "abs": "https://arxiv.org/abs/2602.17898", "authors": ["Jingquan Yan", "Yuwei Miao", "Peiran Yu", "Junzhou Huang"], "title": "Breaking the Correlation Plateau: On the Optimization and Capacity Limits of Attention-Based Regressors", "comment": "Accepted by ICLR 2026", "summary": "Attention-based regression models are often trained by jointly optimizing Mean Squared Error (MSE) loss and Pearson correlation coefficient (PCC) loss, emphasizing the magnitude of errors and the order or shape of targets, respectively. A common but poorly understood phenomenon during training is the PCC plateau: PCC stops improving early in training, even as MSE continues to decrease. We provide the first rigorous theoretical analysis of this behavior, revealing fundamental limitations in both optimization dynamics and model capacity. First, in regard to the flattened PCC curve, we uncover a critical conflict where lowering MSE (magnitude matching) can paradoxically suppress the PCC gradient (shape matching). This issue is exacerbated by the softmax attention mechanism, particularly when the data to be aggregated is highly homogeneous. Second, we identify a limitation in the model capacity: we derived a PCC improvement limit for any convex aggregator (including the softmax attention), showing that the convex hull of the inputs strictly bounds the achievable PCC gain. We demonstrate that data homogeneity intensifies both limitations. Motivated by these insights, we propose the Extrapolative Correlation Attention (ECA), which incorporates novel, theoretically-motivated mechanisms to improve the PCC optimization and extrapolate beyond the convex hull. Across diverse benchmarks, including challenging homogeneous data setting, ECA consistently breaks the PCC plateau, achieving significant improvements in correlation without compromising MSE performance.", "AI": {"tldr": "\u7406\u8bba\u5206\u6790\u63ed\u793a\u6ce8\u610f\u529b\u56de\u5f52\u6a21\u578b\u5728\u8bad\u7ec3\u4e2dPCC\u65e9 plateau \u7684\u6839\u672c\u539f\u56e0\uff0c\u63d0\u51faECA\u673a\u5236\u7a81\u7834\u76f8\u5173\u7cfb\u6570\u74f6\u9888", "motivation": "\u89e3\u91ca\u6ce8\u610f\u529b\u56de\u5f52\u6a21\u578b\u8054\u5408\u4f18\u5316MSE\u548cPCC\u635f\u5931\u65f6\u51fa\u73b0\u7684PCC plateau\u73b0\u8c61\uff1aPCC\u65e9\u671f\u505c\u6b62\u63d0\u5347\u800cMSE\u6301\u7eed\u4e0b\u964d\uff0c\u63ed\u793a\u5e45\u5ea6\u5339\u914d\u4e0e\u5f62\u72b6\u5339\u914d\u95f4\u7684\u6839\u672c\u51b2\u7a81", "method": "\u901a\u8fc7\u4e25\u683c\u7684\u7406\u8bba\u5206\u6790\u63ed\u793a\u68af\u5ea6\u51b2\u7a81\u4e0e\u6a21\u578b\u5bb9\u91cf\u9650\u5236\uff0c\u63d0\u51faExtrapolative Correlation Attention (ECA)\uff0c\u5f15\u5165\u5916\u63a8\u673a\u5236\u8d85\u8d8a\u8f93\u5165\u51f8\u5305\u7ea6\u675f", "result": "\u53d1\u73b0\u964d\u4f4eMSE\u4f1a\u6291\u5236PCC\u68af\u5ea6\uff0c\u4e14\u51f8\u805a\u5408\u5668\u7684PCC\u63d0\u5347\u53d7\u8f93\u5165\u51f8\u5305\u4e25\u683c\u9650\u5236\uff1bECA\u5728\u5404\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7a81\u7834plateau\uff0c\u663e\u8457\u63d0\u5347\u76f8\u5173\u7cfb\u6570\u4e14\u4e0d\u727a\u7272MSE\u6027\u80fd", "conclusion": "PCC plateau\u6e90\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u4f18\u5316\u52a8\u6001\u51b2\u7a81\u548c\u5bb9\u91cf\u5c40\u9650\uff0cECA\u901a\u8fc7\u7406\u8bba\u9a71\u52a8\u7684\u5916\u63a8\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u8be5\u95ee\u9898\uff0c\u4e3a\u56de\u5f52\u4efb\u52a1\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2602.18308", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18308", "abs": "https://arxiv.org/abs/2602.18308", "authors": ["Biswa Sengupta", "Jinhua Wang", "Leo Brunswic"], "title": "JPmHC Dynamical Isometry via Orthogonal Hyper-Connections", "comment": null, "summary": "Recent advances in deep learning, exemplified by Hyper-Connections (HC), have expanded the residual connection paradigm by introducing wider residual streams and diverse connectivity patterns. While these innovations yield significant performance gains, they compromise the identity mapping property of residual connections, leading to training instability, limited scalability, and increased memory overhead. To address these challenges, we propose JPmHC (Jacobian-spectrum Preserving manifold-constrained Hyper-Connections), a framework that replaces identity skips with a trainable linear mixer acting on n parallel streams while explicitly controlling gradient conditioning. By constraining the mixer M on operator-norm-bounded manifolds (e.g., bistochastic, Stiefel, Grassmann), JPmHC prevents gradient pathologies and enhances stability. JPmHC introduces three key contributions: (i) a free-probability analysis that predicts Jacobian spectra for structured skips, providing actionable design rules for mixer selection; (ii) memory-efficient implicit differentiation for fixed-point projections, reducing activation memory and synchronization overhead; and (iii) a Stiefel-constrained mixer via Cayley transforms, ensuring orthogonality without post-hoc normalization. Empirical evaluations on ARC-AGI demonstrate that JPmHC achieves faster convergence, higher accuracy, and lower computational cost compared to bistochastic baselines. As a flexible and scalable extension of HC, JPmHC advances spectrum-aware, stable, and efficient deep learning, offering insights into topological architecture design and foundational model evolution.", "AI": {"tldr": "\u63d0\u51faJPmHC\u6846\u67b6\uff0c\u7528\u53ef\u5b66\u4e60\u7684\u7ebf\u6027\u6df7\u5408\u5668\u66ff\u4ee3\u6b8b\u5dee\u8fde\u63a5\u4e2d\u7684\u6052\u7b49\u6620\u5c04\uff0c\u901a\u8fc7\u7ea6\u675f\u6df7\u5408\u5668\u5728\u7b97\u5b50\u8303\u6570\u6709\u754c\u6d41\u5f62\u4e0a\uff0c\u4fdd\u6301Jacobian\u8c31\u7a33\u5b9a\uff0c\u89e3\u51b3\u8d85\u8fde\u63a5\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u5185\u5b58\u5f00\u9500\u95ee\u9898", "motivation": "\u8d85\u8fde\u63a5(HC)\u867d\u63d0\u5347\u6027\u80fd\u4f46\u7834\u574f\u4e86\u6b8b\u5dee\u8fde\u63a5\u7684\u6052\u7b49\u6620\u5c04\u7279\u6027\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3001\u53ef\u6269\u5c55\u6027\u53d7\u9650\u548c\u5185\u5b58\u5f00\u9500\u589e\u52a0\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u4fdd\u6301\u68af\u5ea6\u6761\u4ef6\u7a33\u5b9a\u6027\u7684\u65b0\u65b9\u6cd5", "method": "\u7528\u53ef\u8bad\u7ec3\u7684\u7ebf\u6027\u6df7\u5408\u5668M\u4f5c\u7528\u4e8en\u4e2a\u5e76\u884c\u6d41\uff0c\u5c06M\u7ea6\u675f\u5728\u53cc\u968f\u673a\u3001Stiefel\u3001Grassmann\u7b49\u7b97\u5b50\u8303\u6570\u6709\u754c\u6d41\u5f62\u4e0a\uff0c\u7ed3\u5408\u81ea\u7531\u6982\u7387\u7406\u8bba\u5206\u6790Jacobian\u8c31\uff0c\u4f7f\u7528Cayley\u53d8\u6362\u5b9e\u73b0Stiefel\u7ea6\u675f\uff0c\u5e76\u91c7\u7528\u9690\u5f0f\u5fae\u5206\u51cf\u5c11\u5185\u5b58", "result": "\u5728ARC-AGI\u57fa\u51c6\u4e0a\u5b9e\u73b0\u66f4\u5feb\u6536\u655b\u3001\u66f4\u9ad8\u51c6\u786e\u7387\u548c\u66f4\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u76f8\u6bd4\u53cc\u968f\u673a\u57fa\u7ebf\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6548\u7387", "conclusion": "JPmHC\u4f5c\u4e3aHC\u7684\u53ef\u6269\u5c55\u6269\u5c55\uff0c\u4e3a\u8c31\u611f\u77e5\u3001\u7a33\u5b9a\u9ad8\u6548\u7684\u6df1\u5ea6\u5b66\u4e60\u63d0\u4f9b\u65b0\u601d\u8def\uff0c\u6709\u52a9\u4e8e\u62d3\u6251\u67b6\u6784\u8bbe\u8ba1\u548c\u57fa\u7840\u6a21\u578b\u6f14\u8fdb"}}
{"id": "2602.17918", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17918", "abs": "https://arxiv.org/abs/2602.17918", "authors": ["Jialin Yu", "Mo\u00efse Blanchard"], "title": "Distribution-Free Sequential Prediction with Abstentions", "comment": "38 pages, 2 figures. Submitted to COLT 2026. Extended version", "summary": "We study a sequential prediction problem in which an adversary is allowed to inject arbitrarily many adversarial instances in a stream of i.i.d.\\ instances, but at each round, the learner may also \\emph{abstain} from making a prediction without incurring any penalty if the instance was indeed corrupted. This semi-adversarial setting naturally sits between the classical stochastic case with i.i.d.\\ instances for which function classes with finite VC dimension are learnable; and the adversarial case with arbitrary instances, known to be significantly more restrictive. For this problem, Goel et al. (2023) showed that, if the learner knows the distribution $\u03bc$ of clean samples in advance, learning can be achieved for all VC classes without restrictions on adversary corruptions. This is, however, a strong assumption in both theory and practice: a natural question is whether similar learning guarantees can be achieved without prior distributional knowledge, as is standard in classical learning frameworks (e.g., PAC learning or asymptotic consistency) and other non-i.i.d.\\ models (e.g., smoothed online learning). We therefore focus on the distribution-free setting where $\u03bc$ is \\emph{unknown} and propose an algorithm \\textsc{AbstainBoost} based on a boosting procedure of weak learners, which guarantees sublinear error for general VC classes in \\emph{distribution-free} abstention learning for oblivious adversaries. These algorithms also enjoy similar guarantees for adaptive adversaries, for structured function classes including linear classifiers. These results are complemented with corresponding lower bounds, which reveal an interesting polynomial trade-off between misclassification error and number of erroneous abstentions.", "AI": {"tldr": "This paper proposes AbstainBoost, a distribution-free algorithm that achieves sublinear error for VC classes in a semi-adversarial sequential prediction setting where the learner can abstain from predictions on corrupted instances. It eliminates the strong assumption of knowing the clean data distribution required by prior work, providing the first learning guarantees without distributional knowledge while revealing a fundamental trade-off between error and abstention rates.", "motivation": "The paper addresses a gap between classical stochastic learning (i.i.d. data, learnable with finite VC dimension) and fully adversarial settings (too restrictive). Prior work (Goel et al., 2023) required knowing the clean data distribution \u03bc, which is impractical. The authors aim to develop learning guarantees for the semi-adversarial setting without this strong distributional knowledge assumption, making the theory more applicable to real-world scenarios where data distribution is unknown.", "method": "The authors propose \\textsc{AbstainBoost}, an algorithm based on boosting weak learners. It operates in a distribution-free setting where the clean instance distribution \u03bc is unknown. The algorithm adaptively combines weak learners to build a strong predictor that can strategically abstain from making predictions on potentially corrupted instances without penalty.", "result": "The algorithm achieves sublinear error for general VC classes in distribution-free abstention learning for oblivious adversaries. For adaptive adversaries, it provides similar guarantees for structured function classes including linear classifiers. The paper also establishes lower bounds that reveal a polynomial trade-off between misclassification error and number of erroneous abstentions.", "conclusion": "The work demonstrates that learning without prior distribution knowledge is achievable in semi-adversarial settings through strategic abstention. It bridges the gap between stochastic and adversarial learning models, showing that abstention serves as a powerful tool to handle adversarial corruptions. The identified trade-off between error and abstention rates provides fundamental insights into the problem's learnability limits."}}
{"id": "2602.18384", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18384", "abs": "https://arxiv.org/abs/2602.18384", "authors": ["Fotios Zantalis", "Evangelos Zervas", "Grigorios Koulouras"], "title": "FedZMG: Efficient Client-Side Optimization in Federated Learning", "comment": null, "summary": "Federated Learning (FL) enables distributed model training on edge devices while preserving data privacy. However, clients tend to have non-Independent and Identically Distributed (non-IID) data, which often leads to client-drift, and therefore diminishing convergence speed and model performance. While adaptive optimizers have been proposed to mitigate these effects, they frequently introduce computational complexity or communication overhead unsuitable for resource-constrained IoT environments. This paper introduces Federated Zero Mean Gradients (FedZMG), a novel, parameter-free, client-side optimization algorithm designed to tackle client-drift by structurally regularizing the optimization space. Advancing the idea of Gradient Centralization, FedZMG projects local gradients onto a zero-mean hyperplane, effectively neutralizing the \"intensity\" or \"bias\" shifts inherent in heterogeneous data distributions without requiring additional communication or hyperparameter tuning. A theoretical analysis is provided, proving that FedZMG reduces the effective gradient variance and guarantees tighter convergence bounds compared to standard FedAvg. Extensive empirical evaluations on EMNIST, CIFAR100, and Shakespeare datasets demonstrate that FedZMG achieves better convergence speed and final validation accuracy compared to the baseline FedAvg and the adaptive optimizer FedAdam, particularly in highly non-IID settings.", "AI": {"tldr": "FedZMG\u662f\u4e00\u79cd\u65b0\u578b\u65e0\u53c2\u6570\u5ba2\u6237\u7aef\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c06\u672c\u5730\u68af\u5ea6\u6295\u5f71\u5230\u96f6\u5747\u503c\u8d85\u5e73\u9762\u6765\u7f13\u89e3\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u5f15\u8d77\u7684\u5ba2\u6237\u7aef\u6f02\u79fb\u95ee\u9898\uff0c\u5728\u4e0d\u589e\u52a0\u901a\u4fe1\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\uff0c\u76f8\u6bd4FedAvg\u548cFedAdam\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u9762\u4e34\u5ba2\u6237\u7aef\u6570\u636e\u975e\u72ec\u7acb\u540c\u5206\u5e03\u5bfc\u81f4\u7684\u5ba2\u6237\u7aef\u6f02\u79fb\u95ee\u9898\uff0c\u8fd9\u4f1a\u964d\u4f4e\u6536\u655b\u901f\u5ea6\u548c\u6a21\u578b\u6027\u80fd\u3002\u73b0\u6709\u81ea\u9002\u5e94\u4f18\u5316\u5668\u5728\u8d44\u6e90\u53d7\u9650\u7684\u7269\u8054\u7f51\u73af\u5883\u4e2d\u5f15\u5165\u8fc7\u9ad8\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u6216\u901a\u4fe1\u5f00\u9500\uff0c\u4e9f\u9700\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faFedZMG\uff08\u8054\u90a6\u96f6\u5747\u503c\u68af\u5ea6\uff09\uff0c\u5c06\u68af\u5ea6\u4e2d\u5fc3\u5316\u7684\u601d\u60f3\u6269\u5c55\u5230\u8054\u90a6\u5b66\u4e60\u573a\u666f\u3002\u8be5\u65b9\u6cd5\u5728\u5ba2\u6237\u7aef\u5c06\u672c\u5730\u68af\u5ea6\u6295\u5f71\u5230\u96f6\u5747\u503c\u8d85\u5e73\u9762\uff0c\u4ece\u7ed3\u6784\u4e0a\u6b63\u5219\u5316\u4f18\u5316\u7a7a\u95f4\uff0c\u6d88\u9664\u5f02\u6784\u6570\u636e\u5206\u5e03\u5e26\u6765\u7684\"\u5f3a\u5ea6\"\u6216\"\u504f\u7f6e\"\u504f\u79fb\uff0c\u65e0\u9700\u989d\u5916\u901a\u4fe1\u6216\u8d85\u53c2\u6570\u8c03\u6574\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660eFedZMG\u80fd\u964d\u4f4e\u6709\u6548\u68af\u5ea6\u65b9\u5dee\u5e76\u83b7\u5f97\u6bd4FedAvg\u66f4\u7d27\u7684\u6536\u655b\u754c\u3002\u5728EMNIST\u3001CIFAR100\u548cShakespeare\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFedZMG\u5728\u9ad8\u5ea6\u975e\u72ec\u7acb\u540c\u5206\u5e03\u8bbe\u7f6e\u4e0b\uff0c\u6bd4FedAvg\u548cFedAdam\u57fa\u51c6\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u9a8c\u8bc1\u51c6\u786e\u7387\u3002", "conclusion": "FedZMG\u901a\u8fc7\u7b80\u5355\u7684\u65e0\u53c2\u6570\u68af\u5ea6\u6295\u5f71\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5ba2\u6237\u7aef\u6f02\u79fb\u95ee\u9898\uff0c\u5728\u4e0d\u589e\u52a0\u901a\u4fe1\u548c\u8ba1\u7b97\u5f00\u9500\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u6536\u655b\u6027\u80fd\u548c\u6a21\u578b\u51c6\u786e\u7387\uff0c\u975e\u5e38\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u7269\u8054\u7f51\u73af\u5883\u3002"}}
{"id": "2602.18409", "categories": ["cs.LG", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2602.18409", "abs": "https://arxiv.org/abs/2602.18409", "authors": ["Huan Luo", "Jonni Virtema"], "title": "Unifying approach to uniform expressivity of graph neural networks", "comment": null, "summary": "The expressive power of Graph Neural Networks (GNNs) is often analysed via correspondence to the Weisfeiler-Leman (WL) algorithm and fragments of first-order logic. Standard GNNs are limited to performing aggregation over immediate neighbourhoods or over global read-outs. To increase their expressivity, recent attempts have been made to incorporate substructural information (e.g. cycle counts and subgraph properties). In this paper, we formalize this architectural trend by introducing Template GNNs (T-GNNs), a generalized framework where node features are updated by aggregating over valid template embeddings from a specified set of graph templates. We propose a corresponding logic, Graded template modal logic (GML(T)), and generalized notions of template-based bisimulation and WL algorithm. We establish an equivalence between the expressive power of T-GNNs and GML(T), and provide a unifying approach for analysing GNN expressivity: we show how standard AC-GNNs and its recent variants can be interpreted as instantiations of T-GNNs.", "AI": {"tldr": "Introduces Template GNNs (T-GNNs), a generalized framework that uses graph templates to boost GNN expressivity, with an equivalent logic GML(T) and unifying view of existing models.", "motivation": "Standard GNNs are limited to local neighborhood aggregation or global read-outs. While recent works try to incorporate substructural information (like cycle counts) to increase expressivity, there lacks a formalized framework for this architectural trend.", "method": "Introduces Template GNNs (T-GNNs) where node features are updated by aggregating over valid template embeddings from specified graph templates. Proposes Graded template modal logic (GML(T)) and generalized template-based bisimulation and WL algorithm.", "result": "Establishes equivalence between T-GNNs and GML(T) expressive power, and shows that standard AC-GNNs and recent variants can be interpreted as special cases of T-GNNs.", "conclusion": "T-GNNs provide a unifying framework for analyzing GNN expressivity, formalizing the trend of incorporating substructural information and showing how existing models fit into this generalized architecture."}}
{"id": "2602.17940", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17940", "abs": "https://arxiv.org/abs/2602.17940", "authors": ["Shogo Iwazaki"], "title": "Tighter Regret Lower Bound for Gaussian Process Bandits with Squared Exponential Kernel in Hypersphere", "comment": "27 pages, 2 figures", "summary": "We study an algorithm-independent, worst-case lower bound for the Gaussian process (GP) bandit problem in the frequentist setting, where the reward function is fixed and has a bounded norm in the known reproducing kernel Hilbert space (RKHS). Specifically, we focus on the squared exponential (SE) kernel, one of the most widely used kernel functions in GP bandits. One of the remaining open questions for this problem is the gap in the \\emph{dimension-dependent} logarithmic factors between upper and lower bounds. This paper partially resolves this open question under a hyperspherical input domain. We show that any algorithm suffers $\u03a9(\\sqrt{T (\\ln T)^{d} (\\ln \\ln T)^{-d}})$ cumulative regret, where $T$ and $d$ represent the total number of steps and the dimension of the hyperspherical domain, respectively. Regarding the simple regret, we show that any algorithm requires $\u03a9(\u03b5^{-2}(\\ln \\frac{1}\u03b5)^d (\\ln \\ln \\frac{1}\u03b5)^{-d})$ time steps to find an $\u03b5$-optimal point. We also provide the improved $O((\\ln T)^{d+1}(\\ln \\ln T)^{-d})$ upper bound on the maximum information gain for the SE kernel. Our results guarantee the optimality of the existing best algorithm up to \\emph{dimension-independent} logarithmic factors under a hyperspherical input domain.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.17947", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17947", "abs": "https://arxiv.org/abs/2602.17947", "authors": ["Yubo Zhou", "Jun Shu", "Junmin Liu", "Deyu Meng"], "title": "Understanding the Generalization of Bilevel Programming in Hyperparameter Optimization: A Tale of Bias-Variance Decomposition", "comment": null, "summary": "Gradient-based hyperparameter optimization (HPO) have emerged recently, leveraging bilevel programming techniques to optimize hyperparameter by estimating hypergradient w.r.t. validation loss. Nevertheless, previous theoretical works mainly focus on reducing the gap between the estimation and ground-truth (i.e., the bias), while ignoring the error due to data distribution (i.e., the variance), which degrades performance. To address this issue, we conduct a bias-variance decomposition for hypergradient estimation error and provide a supplemental detailed analysis of the variance term ignored by previous works. We also present a comprehensive analysis of the error bounds for hypergradient estimation. This facilitates an easy explanation of some phenomena commonly observed in practice, like overfitting to the validation set. Inspired by the derived theories, we propose an ensemble hypergradient strategy to reduce the variance in HPO algorithms effectively. Experimental results on tasks including regularization hyperparameter learning, data hyper-cleaning, and few-shot learning demonstrate that our variance reduction strategy improves hypergradient estimation. To explain the improved performance, we establish a connection between excess error and hypergradient estimation, offering some understanding of empirical observations.", "AI": {"tldr": "The paper identifies variance in hypergradient estimation as an overlooked issue in gradient-based HPO. Through bias-variance decomposition and error bound analysis, the authors propose an ensemble strategy to reduce variance, improving performance across multiple tasks and explaining phenomena like validation set overfitting.", "motivation": "While gradient-based hyperparameter optimization using bilevel programming shows promise, existing theoretical analyses focus on reducing bias in hypergradient estimation while neglecting variance from data distribution, causing degraded performance.", "method": "The authors perform bias-variance decomposition of hypergradient estimation error, conduct detailed analysis of the previously ignored variance term, examine comprehensive error bounds, and propose an ensemble hypergradient strategy to reduce variance.", "result": "Experiments on regularization hyperparameter learning, data hyper-cleaning, and few-shot learning demonstrate that the ensemble variance reduction strategy significantly improves hypergradient estimation accuracy.", "conclusion": "The work establishes a connection between excess error and hypergradient estimation, explaining empirical observations like validation set overfitting and providing deeper theoretical understanding of HPO performance."}}
{"id": "2602.17948", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17948", "abs": "https://arxiv.org/abs/2602.17948", "authors": ["Yu Bai", "Zhe Wang", "Jiarui Zhang", "Dong-Xiao Zhang", "Yinjun Gao", "Jun-Jie Zhang"], "title": "A Geometric Probe of the Accuracy-Robustness Trade-off: Sharp Boundaries in Symmetry-Breaking Dimensional Expansion", "comment": "22 pages, 3 figures", "summary": "The trade-off between clean accuracy and adversarial robustness is a pervasive phenomenon in deep learning, yet its geometric origin remains elusive. In this work, we utilize Symmetry-Breaking Dimensional Expansion (SBDE) as a controlled probe to investigate the mechanism underlying this trade-off. SBDE expands input images by inserting constant-valued pixels, which breaks translational symmetry and consistently improves clean accuracy (e.g., from $90.47\\%$ to $95.63\\%$ on CIFAR-10 with ResNet-18) by reducing parameter degeneracy. However, this accuracy gain comes at the cost of reduced robustness against iterative white-box attacks. By employing a test-time \\emph{mask projection} that resets the inserted auxiliary pixels to their training values, we demonstrate that the vulnerability stems almost entirely from the inserted dimensions. The projection effectively neutralizes the attacks and restores robustness, revealing that the model achieves high accuracy by creating \\emph{sharp boundaries} (steep loss gradients) specifically along the auxiliary axes. Our findings provide a concrete geometric explanation for the accuracy-robustness paradox: the optimization landscape deepens the basin of attraction to improve accuracy but inevitably erects steep walls along the auxiliary degrees of freedom, creating a fragile sensitivity to off-manifold perturbations.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u5bf9\u79f0\u6027\u7834\u7f3a\u7ef4\u5ea6\u6269\u5c55\uff08SBDE\uff09\u63a2\u7a76\u5e72\u51c0\u51c6\u786e\u7387\u4e0e\u5bf9\u6297\u9c81\u68d2\u6027\u7684\u6743\u8861\u673a\u5236\u3002SBDE\u901a\u8fc7\u63d2\u5165\u5e38\u6570\u50cf\u7d20\u63d0\u5347\u51c6\u786e\u7387\u4f46\u727a\u7272\u9c81\u68d2\u6027\uff0c\u6d4b\u8bd5\u65f6\u63a9\u7801\u6295\u5f71\u91cd\u7f6e\u8f85\u52a9\u50cf\u7d20\u53ef\u6062\u590d\u9c81\u68d2\u6027\uff0c\u63ed\u793a\u6a21\u578b\u5728\u8f85\u52a9\u7ef4\u5ea6\u5f62\u6210\u9661\u5ced\u8fb9\u754c\u662f\u6096\u8bba\u7684\u51e0\u4f55\u6839\u6e90\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u4e2d\u5e72\u51c0\u51c6\u786e\u7387\u4e0e\u5bf9\u6297\u9c81\u68d2\u6027\u4e4b\u95f4\u5b58\u5728\u666e\u904d\u6743\u8861\uff0c\u4f46\u5176\u51e0\u4f55\u8d77\u6e90\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u91c7\u7528\u5bf9\u79f0\u6027\u7834\u7f3a\u7ef4\u5ea6\u6269\u5c55\uff08SBDE\uff09\u65b9\u6cd5\uff0c\u5728\u8f93\u5165\u56fe\u50cf\u4e2d\u63d2\u5165\u5e38\u6570\u503c\u50cf\u7d20\u4ee5\u6253\u7834\u5e73\u79fb\u5bf9\u79f0\u6027\uff1b\u5e76\u8bbe\u8ba1\u6d4b\u8bd5\u65f6\u63a9\u7801\u6295\u5f71\u7b56\u7565\uff0c\u5c06\u8f85\u52a9\u50cf\u7d20\u91cd\u7f6e\u4e3a\u8bad\u7ec3\u65f6\u7684\u503c\u3002", "result": "SBDE\u663e\u8457\u63d0\u5347\u5e72\u51c0\u51c6\u786e\u7387\uff08\u5982ResNet-18\u5728CIFAR-10\u4e0a\u4ece90.47%\u63d0\u5347\u81f395.63%\uff09\uff0c\u4f46\u964d\u4f4e\u4e86\u5bf9\u8fed\u4ee3\u5f0f\u767d\u76d2\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002\u6d4b\u8bd5\u65f6\u63a9\u7801\u6295\u5f71\u6709\u6548\u4e2d\u548c\u653b\u51fb\u5e76\u6062\u590d\u9c81\u68d2\u6027\uff0c\u8868\u660e\u8106\u5f31\u6027\u51e0\u4e4e\u5b8c\u5168\u6765\u81ea\u63d2\u5165\u7684\u7ef4\u5ea6\u3002\u6a21\u578b\u901a\u8fc7\u5728\u8f85\u52a9\u8f74\u4e0a\u5f62\u6210\u5c16\u9510\u8fb9\u754c\uff08\u9661\u5ced\u635f\u5931\u68af\u5ea6\uff09\u6765\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u51c6\u786e\u7387-\u9c81\u68d2\u6027\u6096\u8bba\u63d0\u4f9b\u4e86\u5177\u4f53\u7684\u51e0\u4f55\u89e3\u91ca\uff1a\u4f18\u5316\u8fc7\u7a0b\u5728\u52a0\u6df1\u5438\u5f15\u76c6\u4ee5\u63d0\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u4e0d\u53ef\u907f\u514d\u5730\u5728\u8f85\u52a9\u81ea\u7531\u5ea6\u4e0a\u5efa\u7acb\u9661\u5ced\u58c1\u5792\uff0c\u4ece\u800c\u5bf9\u79bb\u6d41\u5f62\u6270\u52a8\u4ea7\u751f\u8106\u5f31\u654f\u611f\u6027\u3002"}}
{"id": "2602.17958", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17958", "abs": "https://arxiv.org/abs/2602.17958", "authors": ["Aida Afshar", "Yuke Zhang", "Aldo Pacchiano"], "title": "Bayesian Online Model Selection", "comment": null, "summary": "Online model selection in Bayesian bandits raises a fundamental exploration challenge: When an environment instance is sampled from a prior distribution, how can we design an adaptive strategy that explores multiple bandit learners and competes with the best one in hindsight? We address this problem by introducing a new Bayesian algorithm for online model selection in stochastic bandits. We prove an oracle-style guarantee of $O\\left( d^* M \\sqrt{T} + \\sqrt{(MT)} \\right)$ on the Bayesian regret, where $M$ is the number of base learners, $d^*$ is the regret coefficient of the optimal base learner, and $T$ is the time horizon. We also validate our method empirically across a range of stochastic bandit settings, demonstrating performance that is competitive with the best base learner. Additionally, we study the effect of sharing data among base learners and its role in mitigating prior mis-specification.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8d1d\u53f6\u65af\u8001\u864e\u673a\u5728\u7ebf\u6a21\u578b\u9009\u62e9\u65b0\u7b97\u6cd5\uff0c\u5728\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9a8c\u4e2d\u5747\u5c55\u73b0\u4e0e\u6700\u4f18\u57fa\u5b66\u4e60\u5668\u76f8\u5f53\u7684\u7ade\u4e89\u529b\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u5171\u4eab\u7f13\u89e3\u5148\u9a8c\u8bef\u8bbe\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u8d1d\u53f6\u65af\u8001\u864e\u673a\u5728\u7ebf\u6a21\u578b\u9009\u62e9\u4e2d\u7684\u6838\u5fc3\u63a2\u7d22\u6311\u6218\uff1a\u5f53\u73af\u5883\u5b9e\u4f8b\u6765\u81ea\u5148\u9a8c\u5206\u5e03\u65f6\uff0c\u5982\u4f55\u8bbe\u8ba1\u81ea\u9002\u5e94\u7b56\u7565\u4ee5\u63a2\u7d22\u591a\u4e2a\u5b66\u4e60\u8005\u5e76\u5728\u4e8b\u540e\u4e0e\u6700\u4f73\u5b66\u4e60\u8005\u7ade\u4e89\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u8d1d\u53f6\u65af\u7b97\u6cd5\u7528\u4e8e\u968f\u673a\u8001\u864e\u673a\u5728\u7ebf\u6a21\u578b\u9009\u62e9\uff0c\u5f15\u5165\u6570\u636e\u5171\u4eab\u673a\u5236\u7f13\u89e3\u5148\u9a8c\u8bef\u8bbe\uff0c\u5e76\u8bc1\u660e\u5176\u8d1d\u53f6\u65af\u9057\u61be\u754c\u3002", "result": "\u83b7\u5f97O(d*M\u221aT + \u221a(MT))\u7684\u8d1d\u53f6\u65af\u9057\u61be\u754c\uff08d*\u4e3a\u6700\u4f18\u57fa\u5b66\u4e60\u5668\u9057\u61be\u7cfb\u6570\uff0cM\u4e3a\u5b66\u4e60\u8005\u6570\u91cf\uff0cT\u4e3a\u65f6\u95f4\u8303\u56f4\uff09\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u591a\u79cd\u8bbe\u5b9a\u4e0b\u6027\u80fd\u53ef\u4e0e\u6700\u4f73\u57fa\u5b66\u4e60\u5668\u5339\u654c\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5b9e\u73b0\u4e86\u7406\u8bba\u4fdd\u8bc1\u4e0e\u5b9e\u8df5\u6027\u80fd\u7684\u53cc\u91cd\u4f18\u52bf\uff0c\u6570\u636e\u5171\u4eab\u673a\u5236\u6709\u6548\u964d\u4f4e\u4e86\u5148\u9a8c\u8bef\u8bbe\u98ce\u9669\uff0c\u4e3a\u5728\u7ebf\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17962", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17962", "abs": "https://arxiv.org/abs/2602.17962", "authors": ["Shuo Sun", "Meiling Zhou", "Chen Zhao", "Joyce H. Keyak", "Nancy E. Lane", "Jeffrey D. Deng", "Kuan-Jui Su", "Hui Shen", "Hong-Wen Deng", "Kui Zhang", "Weihua Zhou"], "title": "Improving Generalizability of Hip Fracture Risk Prediction via Domain Adaptation Across Multiple Cohorts", "comment": "26 pages, 3 tables, 1 figure", "summary": "Clinical risk prediction models often fail to be generalized across cohorts because underlying data distributions differ by clinical site, region, demographics, and measurement protocols. This limitation is particularly pronounced in hip fracture risk prediction, where the performance of models trained on one cohort (the source cohort) can degrade substantially when deployed in other cohorts (target cohorts). We used a shared set of clinical and DXA-derived features across three large cohorts - the Study of Osteoporotic Fractures (SOF), the Osteoporotic Fractures in Men Study (MrOS), and the UK Biobank (UKB), to systematically evaluate the performance of three domain adaptation methods - Maximum Mean Discrepancy (MMD), Correlation Alignment (CORAL), and Domain - Adversarial Neural Networks (DANN) and their combinations. For a source cohort with males only and a source cohort with females only, domain-adaptation methods consistently showed improved performance than the no-adaptation baseline (source-only training), and the use of combinations of multiple domain adaptation methods delivered the largest and most stable gains. The method that combines MMD, CORAL, and DANN achieved the highest discrimination with the area under curve (AUC) of 0.88 for a source cohort with males only and 0.95 for a source cohort with females only), demonstrating that integrating multiple domain adaptation methods could produce feature representations that are less sensitive to dataset differences. Unlike existing methods that rely heavily on supervised tuning or assume known outcomes of samples in target cohorts, our outcome-free approaches enable the model selection under realistic deployment conditions and improve generalization of models in hip fracture risk prediction.", "AI": {"tldr": "\u9acb\u90e8\u9aa8\u6298\u98ce\u9669\u9884\u6d4b\u6a21\u578b\u8de8\u961f\u5217\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u672c\u7814\u7a76\u5728SOF\u3001MrOS\u3001UKB\u4e09\u961f\u5217\u4e0a\u8bc4\u4f30MMD\u3001CORAL\u3001DANN\u7b49\u57df\u9002\u5e94\u65b9\u6cd5\u3002\u7ec4\u5408\u7b56\u7565\u6548\u679c\u6700\u4f73\uff0cMMD+CORAL+DANN\u7684AUC\u8fbe0.88(\u7537\u6027)\u548c0.95(\u5973\u6027)\uff0c\u4e14\u65e0\u9700\u76ee\u6807\u961f\u5217\u7ed3\u5c40\u6807\u7b7e\u3002", "motivation": "\u4e34\u5e8a\u98ce\u9669\u9884\u6d4b\u6a21\u578b\u56e0\u6570\u636e\u5206\u5e03\u5728\u4e0d\u540c\u7ad9\u70b9\u3001\u5730\u533a\u3001\u4eba\u7fa4\u548c\u6d4b\u91cf\u534f\u8bae\u95f4\u7684\u5dee\u5f02\u800c\u96be\u4ee5\u8de8\u961f\u5217\u6cdb\u5316\uff0c\u8fd9\u5728\u9acb\u90e8\u9aa8\u6298\u9884\u6d4b\u4e2d\u5c24\u4e3a\u7a81\u51fa\uff0c\u6a21\u578b\u4ece\u6e90\u961f\u5217\u8fc1\u79fb\u5230\u76ee\u6807\u961f\u5217\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "method": "\u5229\u7528SOF\u3001MrOS\u548cUKB\u4e09\u4e2a\u5927\u578b\u961f\u5217\u7684\u5171\u4eab\u4e34\u5e8a\u53caDXA\u7279\u5f81\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86MMD\u3001CORAL\u548cDANN\u4e09\u79cd\u57df\u9002\u5e94\u65b9\u6cd5\u53ca\u5176\u7ec4\u5408\u3002\u5206\u522b\u5728\u4ec5\u7537\u6027\u548c\u4ec5\u5973\u6027\u6e90\u961f\u5217\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u4e0e\u65e0\u9002\u5e94\u57fa\u7ebf(\u4ec5\u6e90\u961f\u5217\u8bad\u7ec3)\u5bf9\u6bd4\u3002", "result": "\u57df\u9002\u5e94\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\uff0c\u7ec4\u5408\u7b56\u7565\u5e26\u6765\u6700\u5927\u6700\u7a33\u5b9a\u7684\u63d0\u5347\u3002MMD+CORAL+DANN\u7ec4\u5408\u53d6\u5f97\u6700\u9ad8AUC(\u7537\u60270.88\uff0c\u5973\u60270.95)\uff0c\u8bc1\u660e\u96c6\u6210\u591a\u79cd\u65b9\u6cd5\u53ef\u751f\u6210\u5bf9\u6570\u636e\u96c6\u5dee\u5f02\u4e0d\u654f\u611f\u7684\u7279\u5f81\u8868\u793a\u3002", "conclusion": "\u4e0d\u540c\u4e8e\u4f9d\u8d56\u76d1\u7763\u8c03\u4f18\u6216\u5047\u8bbe\u76ee\u6807\u961f\u5217\u7ed3\u5c40\u5df2\u77e5\u7684\u65b9\u6cd5\uff0c\u672c\u7814\u7a76\u7684\u65e0\u7ed3\u5c40\u65b9\u6cd5\u53ef\u5728\u73b0\u5b9e\u90e8\u7f72\u6761\u4ef6\u4e0b\u8fdb\u884c\u6a21\u578b\u9009\u62e9\uff0c\u663e\u8457\u6539\u5584\u9acb\u90e8\u9aa8\u6298\u98ce\u9669\u9884\u6d4b\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.17985", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.17985", "abs": "https://arxiv.org/abs/2602.17985", "authors": ["Ryan O'Dowd"], "title": "Learning Without Training", "comment": "PhD Dissertation of Ryan O'Dowd, defended successfully at Claremont Graduate University on 1/28/2026", "summary": "Machine learning is at the heart of managing the real-world problems associated with massive data. With the success of neural networks on such large-scale problems, more research in machine learning is being conducted now than ever before. This dissertation focuses on three different projects rooted in mathematical theory for machine learning applications.\n  The first project deals with supervised learning and manifold learning. In theory, one of the main problems in supervised learning is that of function approximation: that is, given some data set $\\mathcal{D}=\\{(x_j,f(x_j))\\}_{j=1}^M$, can one build a model $F\\approx f$? We introduce a method which aims to remedy several of the theoretical shortcomings of the current paradigm for supervised learning.\n  The second project deals with transfer learning, which is the study of how an approximation process or model learned on one domain can be leveraged to improve the approximation on another domain. We study such liftings of functions when the data is assumed to be known only on a part of the whole domain. We are interested in determining subsets of the target data space on which the lifting can be defined, and how the local smoothness of the function and its lifting are related.\n  The third project is concerned with the classification task in machine learning, particularly in the active learning paradigm. Classification has often been treated as an approximation problem as well, but we propose an alternative approach leveraging techniques originally introduced for signal separation problems. We introduce theory to unify signal separation with classification and a new algorithm which yields competitive accuracy to other recent active learning algorithms while providing results much faster.", "AI": {"tldr": "\u672c\u8bba\u6587\u4ece\u6570\u5b66\u7406\u8bba\u89d2\u5ea6\u63d0\u51fa\u4e09\u4e2a\u673a\u5668\u5b66\u4e60\u9879\u76ee\uff1a(1)\u4e00\u79cd\u89e3\u51b3\u76d1\u7763\u5b66\u4e60\u51fd\u6570\u903c\u8fd1\u7406\u8bba\u7f3a\u9677\u7684\u65b0\u65b9\u6cd5\uff1b(2)\u9488\u5bf9\u90e8\u5206\u5df2\u77e5\u6570\u636e\u7684\u8fc1\u79fb\u5b66\u4e60\u51fd\u6570\u63d0\u5347\u6846\u67b6\uff0c\u5206\u6790\u53ef\u5b9a\u4e49\u5b50\u96c6\u4e0e\u5e73\u6ed1\u6027\u5173\u7cfb\uff1b(3)\u57fa\u4e8e\u4fe1\u53f7\u5206\u79bb\u6280\u672f\u7684 active learning \u5206\u7c7b\u7b97\u6cd5\uff0c\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u901f\u5ea6\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u662f\u5904\u7406\u6d77\u91cf\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u95ee\u9898\u7684\u6838\u5fc3\uff0c\u4f46\u5f53\u524d\u76d1\u7763\u5b66\u4e60\u5728\u51fd\u6570\u903c\u8fd1\u65b9\u9762\u5b58\u5728\u7406\u8bba\u7f3a\u9677\uff0c\u8fc1\u79fb\u5b66\u4e60\u5728\u90e8\u5206\u6570\u636e\u57df\u4e0b\u7684\u51fd\u6570\u63d0\u5347\u673a\u5236\u5c1a\u4e0d\u660e\u786e\uff0c\u4e14\u5206\u7c7b\u4efb\u52a1\u5728 active learning \u8303\u5f0f\u4e2d\u7f3a\u4e4f\u8d85\u8d8a\u7eaf\u903c\u8fd1\u65b9\u6cd5\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u9879\u76ee\u4e00\uff1a\u9488\u5bf9\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u51fd\u6570\u903c\u8fd1\u95ee\u9898 $\\mathcal{D}=\\{(x_j,f(x_j))\\}_{j=1}^M$\uff0c\u63d0\u51fa\u65b0\u65b9\u6cd5\u4ee5\u6539\u8fdb\u73b0\u6709\u8303\u5f0f\u7684\u7406\u8bba\u7f3a\u9677\u3002\u9879\u76ee\u4e8c\uff1a\u7814\u7a76\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u51fd\u6570\u63d0\u5347\uff0c\u5728\u6570\u636e\u4ec5\u90e8\u5206\u5df2\u77e5\u7684\u60c5\u51b5\u4e0b\uff0c\u786e\u5b9a\u76ee\u6807\u7a7a\u95f4\u53ef\u5b9a\u4e49\u5b50\u96c6\u5e76\u5206\u6790\u51fd\u6570\u4e0e\u5176\u63d0\u5347\u7684\u5c40\u90e8\u5e73\u6ed1\u6027\u5173\u7cfb\u3002\u9879\u76ee\u4e09\uff1a\u501f\u9274\u4fe1\u53f7\u5206\u79bb\u6280\u672f\uff0c\u63d0\u51fa\u5206\u7c7b\u7684\u66ff\u4ee3\u65b9\u6cd5\uff0c\u5efa\u7acb\u7edf\u4e00\u7406\u8bba\u5e76\u5f00\u53d1\u65b0\u7b97\u6cd5\u3002", "result": "\u9879\u76ee\u4e00\uff1a\u63d0\u51fa\u5177\u6709\u7406\u8bba\u6539\u8fdb\u4f18\u52bf\u7684\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002\u9879\u76ee\u4e8c\uff1a\u5efa\u7acb\u90e8\u5206\u6570\u636e\u4e0b\u51fd\u6570\u63d0\u5347\u7684\u7406\u8bba\u6846\u67b6\u3002\u9879\u76ee\u4e09\uff1a\u5f00\u53d1\u51fa\u5728\u51c6\u786e\u7387\u4e0a\u5177\u5907\u7ade\u4e89\u529b\u4e14\u901f\u5ea6\u663e\u8457\u5feb\u4e8e\u8fd1\u671f active learning \u7b97\u6cd5\u7684\u5206\u7c7b\u7b97\u6cd5\u3002", "conclusion": "\u672c\u8bba\u6587\u5728\u76d1\u7763\u5b66\u4e60\u3001\u8fc1\u79fb\u5b66\u4e60\u548c active learning \u5206\u7c7b\u4e09\u4e2a\u9886\u57df\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u6570\u5b66\u7406\u8bba\u8d21\u732e\u4e0e\u5b9e\u7528\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u51fd\u6570\u903c\u8fd1\u3001\u8de8\u57df\u63d0\u5347\u548c\u9ad8\u6548\u5206\u7c7b\u7b49\u57fa\u7840\u6027\u95ee\u9898\uff0c\u63a8\u52a8\u4e86\u673a\u5668\u5b66\u4e60\u7406\u8bba\u7684\u8fb9\u754c\u3002"}}
{"id": "2602.18002", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18002", "abs": "https://arxiv.org/abs/2602.18002", "authors": ["Junfei Sun", "Dixi Yao", "Xuchen Gong", "Tahseen Rabbani", "Manzil Zaheer", "Tian Li"], "title": "Asynchronous Heavy-Tailed Optimization", "comment": "8-page main body, 25-page appendix, 5 figures", "summary": "Heavy-tailed stochastic gradient noise, commonly observed in transformer models, can destabilize the optimization process. Recent works mainly focus on developing and understanding approaches to address heavy-tailed noise in the centralized or distributed, synchronous setting, leaving the interactions between such noise and asynchronous optimization underexplored. In this work, we investigate two communication schemes that handle stragglers with asynchronous updates in the presence of heavy-tailed gradient noise. We propose and theoretically analyze algorithmic modifications based on delay-aware learning rate scheduling and delay compensation to enhance the performance of asynchronous algorithms. Our convergence guarantees under heavy-tailed noise match the rate of the synchronous counterparts and improve delay tolerance compared with existing asynchronous approaches. Empirically, our approaches outperform prior synchronous and asynchronous methods in terms of accuracy/runtime trade-offs and are more robust to hyperparameters in both image and language tasks.", "AI": {"tldr": "\u63d0\u51fa\u5ef6\u8fdf\u611f\u77e5\u5b66\u4e60\u7387\u8c03\u5ea6\u548c\u5ef6\u8fdf\u8865\u507f\u7b97\u6cd5\uff0c\u89e3\u51b3Transformer\u6a21\u578b\u5f02\u6b65\u4f18\u5316\u4e2d\u7684\u91cd\u5c3e\u68af\u5ea6\u566a\u58f0\u95ee\u9898\uff0c\u6536\u655b\u7387\u4e0e\u540c\u6b65\u65b9\u6cd5\u76f8\u5f53\u4e14\u5b9e\u8bc1\u6027\u80fd\u66f4\u4f18", "motivation": "\u91cd\u5c3e\u968f\u673a\u68af\u5ea6\u566a\u58f0\u4f1a\u7834\u574fTransformer\u4f18\u5316\u7a33\u5b9a\u6027\uff0c\u73b0\u6709\u7814\u7a76\u96c6\u4e2d\u4e8e\u540c\u6b65\u573a\u666f\uff0c\u5f02\u6b65\u4f18\u5316\u4e2d\u7684\u91cd\u5c3e\u566a\u58f0\u4e0e\u5ef6\u8fdf\u4ea4\u4e92\u673a\u5236\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22", "method": "\u7814\u7a76\u4e24\u79cd\u5f02\u6b65\u901a\u4fe1\u65b9\u6848\uff0c\u901a\u8fc7\u5ef6\u8fdf\u611f\u77e5\u5b66\u4e60\u7387\u8c03\u5ea6\u548c\u5ef6\u8fdf\u8865\u507f\u673a\u5236\u6539\u8fdb\u5f02\u6b65\u7b97\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u91cd\u5c3e\u68af\u5ea6\u566a\u58f0", "result": "\u7406\u8bba\u4fdd\u8bc1\u5728\u91cd\u5c3e\u566a\u58f0\u4e0b\u8fbe\u5230\u540c\u6b65\u65b9\u6cd5\u6536\u655b\u7387\u4e14\u5ef6\u8fdf\u5bb9\u5fcd\u5ea6\u4f18\u4e8e\u73b0\u6709\u5f02\u6b65\u65b9\u6cd5\uff1b\u5b9e\u8bc1\u5728\u56fe\u50cf\u548c\u8bed\u8a00\u4efb\u52a1\u4e0a\u7cbe\u5ea6/\u65f6\u95f4\u6743\u8861\u548c\u53c2\u6570\u9c81\u68d2\u6027\u5747\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5", "conclusion": "\u6240\u63d0\u7b97\u6cd5\u6709\u6548\u6865\u63a5\u4e86\u5f02\u6b65\u4e0e\u540c\u6b65\u65b9\u6cd5\u5728\u91cd\u5c3e\u566a\u58f0\u4e0b\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u4e3a\u5206\u5e03\u5f0f\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u5f02\u6b65\u4f18\u5316\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.18055", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18055", "abs": "https://arxiv.org/abs/2602.18055", "authors": ["Jingyang Qiao", "Zhizhong Zhang", "Xin Tan", "Jingyu Gong", "Yanyun Qu", "Yuan Xie"], "title": "Continual-NExT: A Unified Comprehension And Generation Continual Learning Framework", "comment": null, "summary": "Dual-to-Dual MLLMs refer to Multimodal Large Language Models, which can enable unified multimodal comprehension and generation through text and image modalities. Although exhibiting strong instantaneous learning and generalization capabilities, Dual-to-Dual MLLMs still remain deficient in lifelong evolution, significantly affecting continual adaptation to dynamic real-world scenarios. One of the challenges is that learning new tasks inevitably destroys the learned knowledge. Beyond traditional catastrophic forgetting, Dual-to-Dual MLLMs face other challenges, including hallucination, instruction unfollowing, and failures in cross-modal knowledge transfer. However, no standardized continual learning framework for Dual-to-Dual MLLMs has been established yet, leaving these challenges unexplored. Thus, in this paper, we establish Continual-NExT, a continual learning framework for Dual-to-Dual MLLMs with deliberately-architected evaluation metrics. To improve the continual learning capability of Dual-to-Dual MLLMs, we propose an efficient MAGE (Mixture and Aggregation of General LoRA and Expert LoRA) method to further facilitate knowledge transfer across modalities and mitigate forgetting. Extensive experiments demonstrate that MAGE outperforms other continual learning methods and achieves state-of-the-art performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faContinual-NExT\uff0c\u4e00\u4e2a\u9488\u5bf9Dual-to-Dual\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u5e76\u5f15\u5165MAGE\u65b9\u6cd5\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u5e76\u4fc3\u8fdb\u8de8\u6a21\u6001\u77e5\u8bc6\u8fc1\u79fb\uff0c\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "Dual-to-Dual\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u7ec8\u8eab\u6f14\u5316\u80fd\u529b\uff0c\u5728\u5b66\u4e60\u65b0\u4efb\u52a1\u65f6\u4f1a\u906d\u53d7\u707e\u96be\u6027\u9057\u5fd8\u3001\u5e7b\u89c9\u3001\u6307\u4ee4\u4e0d\u9075\u5faa\u4ee5\u53ca\u8de8\u6a21\u6001\u77e5\u8bc6\u8fc1\u79fb\u5931\u8d25\u7b49\u95ee\u9898\uff0c\u4e14\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9\u6b64\u7c7b\u6a21\u578b\u7684\u6807\u51c6\u5316\u6301\u7eed\u5b66\u4e60\u6846\u67b6\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86Continual-NExT\u6846\u67b6\uff0c\u5e76\u63d0\u51faMAGE\uff08\u901a\u7528LoRA\u4e0e\u4e13\u5bb6LoRA\u7684\u6df7\u5408\u4e0e\u805a\u5408\uff09\u65b9\u6cd5\uff0c\u4ee5\u4fc3\u8fdb\u8de8\u6a21\u6001\u77e5\u8bc6\u8fc1\u79fb\u5e76\u7f13\u89e3Dual-to-Dual MLLMs\u7684\u9057\u5fd8\u95ee\u9898\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMAGE\u4f18\u4e8e\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728Continual-NExT\u6846\u67b6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684Continual-NExT\u6846\u67b6\u548cMAGE\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86Dual-to-Dual\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6301\u7eed\u5b66\u4e60\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u77e5\u8bc6\u4fdd\u7559\u548c\u8de8\u6a21\u6001\u8fc1\u79fb\u80fd\u529b\u3002"}}
{"id": "2602.18084", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18084", "abs": "https://arxiv.org/abs/2602.18084", "authors": ["Benjamin Honor\u00e9", "Alba Carballo-Castro", "Yiming Qin", "Pascal Frossard"], "title": "Balancing Symmetry and Efficiency in Graph Flow Matching", "comment": "15 pages, 11 figures", "summary": "Equivariance is central to graph generative models, as it ensures the model respects the permutation symmetry of graphs. However, strict equivariance can increase computational cost due to added architectural constraints, and can slow down convergence because the model must be consistent across a large space of possible node permutations. We study this trade-off for graph generative models. Specifically, we start from an equivariant discrete flow-matching model, and relax its equivariance during training via a controllable symmetry modulation scheme based on sinusoidal positional encodings and node permutations. Experiments first show that symmetry-breaking can accelerate early training by providing an easier learning signal, but at the expense of encouraging shortcut solutions that can cause overfitting, where the model repeatedly generates graphs that are duplicates of the training set. On the contrary, properly modulating the symmetry signal can delay overfitting while accelerating convergence, allowing the model to reach stronger performance with $19\\%$ of the baseline training epochs.", "AI": {"tldr": "\u7814\u7a76\u56fe\u751f\u6210\u6a21\u578b\u4e2d\u7b49\u53d8\u6027\u4e0e\u8bad\u7ec3\u6548\u7387\u7684\u6743\u8861\uff0c\u63d0\u51fa\u53ef\u63a7\u5bf9\u79f0\u6027\u8c03\u5236\u65b9\u6848\uff0c\u901a\u8fc7\u653e\u677e\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u7b49\u53d8\u6027\uff0c\u5728\u4ec5\u4f7f\u752819%\u8bad\u7ec3\u5468\u671f\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u66f4\u5f3a\u6027\u80fd\u5e76\u5ef6\u8fdf\u8fc7\u62df\u5408\u3002", "motivation": "\u7b49\u53d8\u6027\u5bf9\u56fe\u751f\u6210\u6a21\u578b\u81f3\u5173\u91cd\u8981\uff0c\u80fd\u786e\u4fdd\u6a21\u578b\u5c0a\u91cd\u56fe\u7684\u7f6e\u6362\u5bf9\u79f0\u6027\uff0c\u4f46\u4e25\u683c\u7684\u7b49\u53d8\u6027\u4f1a\u56e0\u67b6\u6784\u7ea6\u675f\u589e\u52a0\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u56e0\u9700\u5728\u5927\u89c4\u6a21\u8282\u70b9\u6392\u5217\u7a7a\u95f4\u4fdd\u6301\u4e00\u81f4\u6027\u800c\u51cf\u6162\u6536\u655b\u901f\u5ea6\u3002\u8bba\u6587\u65e8\u5728\u7814\u7a76\u8fd9\u79cd\u6743\u8861\u5173\u7cfb\u3002", "method": "\u4ece\u7b49\u53d8\u79bb\u6563\u6d41\u5339\u914d\u6a21\u578b\u51fa\u53d1\uff0c\u901a\u8fc7\u57fa\u4e8e\u6b63\u5f26\u4f4d\u7f6e\u7f16\u7801\u548c\u8282\u70b9\u6392\u5217\u7684\u53ef\u63a7\u5bf9\u79f0\u6027\u8c03\u5236\u65b9\u6848\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u653e\u677e\u7b49\u53d8\u6027\u7ea6\u675f\u3002", "result": "\u5bf9\u79f0\u6027\u6253\u7834\u53ef\u52a0\u901f\u65e9\u671f\u8bad\u7ec3\u4f46\u4f1a\u9f13\u52b1\u6377\u5f84\u89e3\u5bfc\u81f4\u8fc7\u62df\u5408\uff08\u751f\u6210\u8bad\u7ec3\u96c6\u91cd\u590d\u56fe\uff09\uff0c\u800c\u9002\u5f53\u7684\u5bf9\u79f0\u6027\u8c03\u5236\u65e2\u80fd\u5ef6\u8fdf\u8fc7\u62df\u5408\u53c8\u80fd\u52a0\u901f\u6536\u655b\uff0c\u4f7f\u6a21\u578b\u4ec5\u9700\u57fa\u7ebf19%\u7684\u8bad\u7ec3\u5468\u671f\u5c31\u80fd\u8fbe\u5230\u66f4\u5f3a\u6027\u80fd\u3002", "conclusion": "\u53ef\u63a7\u5bf9\u79f0\u6027\u8c03\u5236\u80fd\u6709\u6548\u5e73\u8861\u7b49\u53d8\u6027\u4e0e\u8bad\u7ec3\u6548\u7387\uff0c\u662f\u63d0\u5347\u56fe\u751f\u6210\u6a21\u578b\u6027\u80fd\u7684\u6709\u6548\u7b56\u7565\uff0c\u53ef\u5728\u5927\u5e45\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u7684\u540c\u65f6\u83b7\u5f97\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.18114", "categories": ["cs.LG", "cs.DS", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.18114", "abs": "https://arxiv.org/abs/2602.18114", "authors": ["Yiding Feng", "Jiashuo Jiang", "Yige Wang"], "title": "Non-Stationary Online Resource Allocation: Learning from a Single Sample", "comment": null, "summary": "We study online resource allocation under non-stationary demand with a minimum offline data requirement. In this problem, a decision-maker must allocate multiple types of resources to sequentially arriving queries over a finite horizon. Each query belongs to a finite set of types with fixed resource consumption and a stochastic reward drawn from an unknown, type-specific distribution. Critically, the environment exhibits arbitrary non-stationarity -- arrival distributions may shift unpredictably-while the algorithm requires only one historical sample per period to operate effectively. We distinguish two settings based on sample informativeness: (i) reward-observed samples containing both query type and reward realization, and (ii) the more challenging type-only samples revealing only query type information.\n  We propose a novel type-dependent quantile-based meta-policy that decouples the problem into modular components: reward distribution estimation, optimization of target service probabilities via fluid relaxation, and real-time decisions through dynamic acceptance thresholds. For reward-observed samples, our static threshold policy achieves $\\tilde{O}(\\sqrt{T})$ regret. For type-only samples, we first establish that sublinear regret is impossible without additional structure; under a mild minimum-arrival-probability assumption, we design both a partially adaptive policy attaining the same $\\tilde{O}({T})$ bound and, more significantly, a fully adaptive resolving policy with careful rounding that achieves the first poly-logarithmic regret guarantee of $O((\\log T)^3)$ for non-stationary multi-resource allocation. Our framework advances prior work by operating with minimal offline data (one sample per period), handling arbitrary non-stationarity without variation-budget assumptions, and supporting multiple resource constraints.", "AI": {"tldr": "This paper studies online multi-resource allocation with minimal historical data (one sample per period) under arbitrary non-stationary demand, proposing a novel quantile-based meta-policy that achieves $\\tilde{O}(\\sqrt{T})$ regret with reward observations and the first poly-logarithmic $O((\\log T)^3)$ regret for the more challenging type-only setting under a mild arrival probability assumption.", "motivation": "Online resource allocation problems face two key challenges: unpredictable demand shifts (non-stationarity) and limited historical data. Prior methods require either stationary environments, extensive offline data, or variation budgets to handle non-stationarity. This work aims to develop algorithms that operate with only one historical sample per period while handling arbitrary non-stationarity and multiple resource constraints.", "method": "A type-dependent quantile-based meta-policy that decouples the problem into three modular components: (1) reward distribution estimation, (2) optimization of target service probabilities via fluid relaxation, and (3) real-time decisions through dynamic acceptance thresholds. For type-only samples, they introduce a fully adaptive resolving policy with careful rounding techniques.", "result": "- Reward-observed samples: Static threshold policy achieves $\\tilde{O}(\\sqrt{T})$ regret.\n- Type-only samples: Sublinear regret is impossible without additional structure. Under a mild minimum-arrival-probability assumption: (i) Partially adaptive policy attains $\\tilde{O}(T)$ regret, (ii) Fully adaptive resolving policy achieves the first poly-logarithmic $O((\\log T)^3)$ regret guarantee for non-stationary multi-resource allocation.", "conclusion": "The framework significantly advances prior work by operating with minimal offline data (one sample per period), handling arbitrary non-stationarity without variation-budget assumptions, and supporting multiple resource constraints. The poly-logarithmic regret bound for the type-only setting represents a substantial theoretical improvement over traditional polynomial regret guarantees."}}
{"id": "2602.18131", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18131", "abs": "https://arxiv.org/abs/2602.18131", "authors": ["Tom Potter", "Oliver Rhodes"], "title": "Learning Long-Range Dependencies with Temporal Predictive Coding", "comment": null, "summary": "Predictive Coding (PC) is a biologically-inspired learning framework characterised by local, parallelisable operations, properties that enable energy-efficient implementation on neuromorphic hardware. Despite this, extending PC effectively to recurrent neural networks (RNNs) has been challenging, particularly for tasks involving long-range temporal dependencies. Backpropagation Through Time (BPTT) remains the dominant method for training RNNs, but its non-local computation, lack of spatial parallelism, and requirement to store extensive activation histories results in significant energy consumption. This work introduces a novel method combining Temporal Predictive Coding (tPC) with approximate Real-Time Recurrent Learning (RTRL), enabling effective spatio-temporal credit assignment. Results indicate that the proposed method can closely match the performance of BPTT on both synthetic benchmarks and real-world tasks. On a challenging machine translation task, with a 15-million parameter model, the proposed method achieves a test perplexity of 7.62 (vs. 7.49 for BPTT), marking one of the first applications of tPC to tasks of this scale. These findings demonstrate the potential of this method to learn complex temporal dependencies whilst retaining the local, parallelisable, and flexible properties of the original PC framework, paving the way for more energy-efficient learning systems.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u65f6\u5e8f\u9884\u6d4b\u7f16\u7801(tPC)\u4e0e\u8fd1\u4f3c\u5b9e\u65f6\u5faa\u73af\u5b66\u4e60(RTRL)\u7684\u65b0\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u9884\u6d4b\u7f16\u7801\u5c40\u90e8\u5e76\u884c\u7279\u6027\u7684\u540c\u65f6\uff0c\u6709\u6548\u89e3\u51b3\u4e86RNN\u7684\u957f\u7a0b\u65f6\u5e8f\u4f9d\u8d56\u5b66\u4e60\u95ee\u9898\uff0c\u5728\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u8fbe\u5230\u63a5\u8fd1BPTT\u7684\u6027\u80fd\u4e14\u66f4\u8282\u80fd", "motivation": "\u4f20\u7edfRNN\u8bad\u7ec3\u4f9d\u8d56BPTT\u7b97\u6cd5\uff0c\u4f46\u5b58\u5728\u8ba1\u7b97\u975e\u5c40\u90e8\u3001\u7f3a\u4e4f\u7a7a\u95f4\u5e76\u884c\u6027\u3001\u9700\u5b58\u50a8\u5927\u91cf\u6fc0\u6d3b\u5386\u53f2\u5bfc\u81f4\u80fd\u8017\u9ad8\u7684\u95ee\u9898\uff1b\u800c\u751f\u7269\u542f\u53d1\u7684\u9884\u6d4b\u7f16\u7801(PC)\u867d\u5177\u5c40\u90e8\u5e76\u884c\u4f18\u52bf\uff0c\u5374\u96be\u4ee5\u6709\u6548\u6269\u5c55\u81f3RNN\u7684\u957f\u65f6\u5e8f\u4f9d\u8d56\u4efb\u52a1", "method": "\u521b\u65b0\u6027\u5730\u878d\u5408\u65f6\u5e8f\u9884\u6d4b\u7f16\u7801(tPC)\u4e0e\u8fd1\u4f3c\u5b9e\u65f6\u5faa\u73af\u5b66\u4e60(RTRL)\uff0c\u5b9e\u73b0\u65f6\u7a7a\u4fe1\u7528\u5206\u914d\uff0c\u5728\u4fdd\u7559PC\u5c40\u90e8\u5e76\u884c\u7279\u6027\u7684\u540c\u65f6\u589e\u5f3a\u957f\u7a0b\u65f6\u5e8f\u5efa\u6a21\u80fd\u529b", "result": "\u5728\u5408\u6210\u4efb\u52a1\u548c\u771f\u5b9e\u4efb\u52a1\u4e0a\u6027\u80fd\u63a5\u8fd1BPTT\uff1b1500\u4e07\u53c2\u6570\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u53d6\u5f977.62\u6d4b\u8bd5\u56f0\u60d1\u5ea6\uff08BPTT\u4e3a7.49\uff09\uff0c\u662ftPC\u9996\u6b21\u6210\u529f\u5e94\u7528\u4e8e\u8be5\u89c4\u6a21\u4efb\u52a1", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301PC\u6846\u67b6\u8282\u80fd\u4f18\u52bf\u7684\u540c\u65f6\u6709\u6548\u5b66\u4e60\u590d\u6742\u65f6\u5e8f\u4f9d\u8d56\uff0c\u4e3a\u5f00\u53d1\u9ad8\u80fd\u6548\u5b66\u4e60\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\uff0c\u63a8\u52a8\u4e86\u7c7b\u8111\u8ba1\u7b97\u5728\u5b9e\u7528\u573a\u666f\u7684\u5e94\u7528"}}
{"id": "2602.18196", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18196", "abs": "https://arxiv.org/abs/2602.18196", "authors": ["Xiuying Wei", "Caglar Gulcehre"], "title": "RAT+: Train Dense, Infer Sparse -- Recurrence Augmented Attention for Dilated Inference", "comment": null, "summary": "Structured dilated attention has an appealing inference-time efficiency knob: it reduces the FLOPs of the attention and the KV cache size by a factor of the dilation size D, while preserving long-range connectivity. However, we find a persistent failure mode of them -- sparsifying a pretrained attention model to a dilated pattern leads to severe accuracy degradation. We introduce RAT+, a dense-pretraining architecture that augments attention with full-sequence recurrence and active recurrence learning. A single RAT+ model is pretrained densely once, then flexibly switched at inference time to dilated attention (optionally with local windows) or hybrid layer/head compositions, requiring only a short 1B-token resolution adaptation rather than retraining separate sparse models. At 1.5B parameters trained on 100B tokens, RAT+ closely matches dense accuracy at 16 and drops by about 2-3 points at 64 on commonsense reasoning and LongBench tasks, respectively. Moreover, RAT+ outperforms attention when sparsifying to the top-k block attention. We further scale to 2.6B parameters and 200B tokens and observe the same trend.", "AI": {"tldr": "RAT+ is a dense-pretraining architecture that augments attention with full-sequence recurrence and active recurrence learning, enabling flexible inference-time switching to dilated or sparse attention patterns with minimal adaptation while preserving accuracy.", "motivation": "Structured dilated attention reduces computational cost and KV cache size but causes severe accuracy degradation when sparsifying pretrained models. The goal is to create a single model that can flexibly adapt to various sparse patterns without full retraining.", "method": "Proposes RAT+ architecture with full-sequence recurrence and active recurrence learning. A single model is pretrained densely once, then can be switched at inference time to dilated attention (optionally with local windows) or hybrid compositions, requiring only a short 1B-token adaptation instead of retraining separate sparse models.", "result": "At 1.5B parameters and 100B tokens: matches dense accuracy at dilation 16, only 2-3 point drop at dilation 64 on commonsense reasoning and LongBench; outperforms attention on top-k block sparsification. Scaling to 2.6B parameters and 200B tokens shows consistent trends.", "conclusion": "RAT+ successfully addresses the efficiency-accuracy tradeoff in sparse attention by enabling flexible inference-time adaptation from a single dense pretrained model, achieving minimal accuracy loss even at high dilation rates."}}
{"id": "2602.18227", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18227", "abs": "https://arxiv.org/abs/2602.18227", "authors": ["Redwanul Karim", "Changhun Kim", "Timon Conrad", "Nora Gourmelon", "Julian Oelhaf", "David Riebesel", "Tom\u00e1s Arias-Vergara", "Andreas Maier", "Johann J\u00e4ger", "Siming Bayer"], "title": "Parameter-Efficient Domain Adaptation of Physics-Informed Self-Attention based GNNs for AC Power Flow Prediction", "comment": null, "summary": "Accurate AC-PF prediction under domain shift is critical when models trained on medium-voltage (MV) grids are deployed on high-voltage (HV) networks. Existing physics-informed graph neural solvers typically rely on full fine-tuning for cross-regime transfer, incurring high retraining cost and offering limited control over the stability-plasticity trade-off between target-domain adaptation and source-domain retention. We study parameter-efficient domain adaptation for physics-informed self-attention based GNN, encouraging Kirchhoff-consistent behavior via a physics-based loss while restricting adaptation to low-rank updates. Specifically, we apply LoRA to attention projections with selective unfreezing of the prediction head to regulate adaptation capacity. This design yields a controllable efficiency-accuracy trade-off for physics-constrained inverse estimation under voltage-regime shift. Across multiple grid topologies, the proposed LoRA+PHead adaptation recovers near-full fine-tuning accuracy with a target-domain RMSE gap of $2.6\\times10^{-4}$ while reducing the number of trainable parameters by 85.46%. The physics-based residual remains comparable to full fine-tuning; however, relative to Full FT, LoRA+PHead reduces MV source retention by 4.7 percentage points (17.9% vs. 22.6%) under domain shift, while still enabling parameter-efficient and physically consistent AC-PF estimation.", "AI": {"tldr": "The paper proposes LoRA+PHead, a parameter-efficient domain adaptation method for physics-informed GNNs that achieves near-full fine-tuning accuracy for AC power flow prediction while reducing trainable parameters by 85.46%.", "motivation": "Existing physics-informed graph neural solvers for AC power flow prediction suffer from high retraining costs when adapting from medium-voltage to high-voltage grids via full fine-tuning, with limited control over the stability-plasticity trade-off between target adaptation and source domain retention.", "method": "The method applies Low-Rank Adaptation (LoRA) to attention projections in physics-informed self-attention GNNs while selectively unfreezing the prediction head, using a physics-based loss to enforce Kirchhoff-consistent behavior and restrict adaptation to low-rank updates.", "result": "LoRA+PHead achieves near-full fine-tuning accuracy with a target-domain RMSE gap of only 2.6\u00d710\u207b\u2074, reduces trainable parameters by 85.46%, maintains comparable physics-based residuals, but shows a 4.7 percentage point reduction in source domain retention (17.9% vs 22.6%) compared to full fine-tuning.", "conclusion": "The proposed approach enables controllable efficiency-accuracy trade-offs, providing a parameter-efficient and physically consistent solution for AC power flow estimation under voltage-regime shift while balancing adaptation performance with catastrophic forgetting."}}
{"id": "2602.18248", "categories": ["cs.LG", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.18248", "abs": "https://arxiv.org/abs/2602.18248", "authors": ["Pietro Sittoni", "Emanuele Zangrando", "Angelo A. Casulli", "Nicola Guglielmi", "Francesco Tudisco"], "title": "Neural-HSS: Hierarchical Semi-Separable Neural PDE Solver", "comment": null, "summary": "Deep learning-based methods have shown remarkable effectiveness in solving PDEs, largely due to their ability to enable fast simulations once trained. However, despite the availability of high-performance computing infrastructure, many critical applications remain constrained by the substantial computational costs associated with generating large-scale, high-quality datasets and training models. In this work, inspired by studies on the structure of Green's functions for elliptic PDEs, we introduce Neural-HSS, a parameter-efficient architecture built upon the Hierarchical Semi-Separable (HSS) matrix structure that is provably data-efficient for a broad class of PDEs. We theoretically analyze the proposed architecture, proving that it satisfies exactness properties even in very low-data regimes. We also investigate its connections with other architectural primitives, such as the Fourier neural operator layer and convolutional layers. We experimentally validate the data efficiency of Neural-HSS on the three-dimensional Poisson equation over a grid of two million points, demonstrating its superior ability to learn from data generated by elliptic PDEs in the low-data regime while outperforming baseline methods. Finally, we demonstrate its capability to learn from data arising from a broad class of PDEs in diverse domains, including electromagnetism, fluid dynamics, and biology.", "AI": {"tldr": "\u63d0\u51faNeural-HSS\u67b6\u6784\uff0c\u57fa\u4e8eHSS\u77e9\u9635\u7ed3\u6784\u89e3\u51b3PDE\u6c42\u89e3\u4e2d\u7684\u6570\u636e\u6548\u7387\u95ee\u9898\uff0c\u5728\u4f4e\u6570\u636e regime \u4e0b\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6c42\u89e3PDE\u867d\u5feb\u4f46\u9762\u4e34\u9ad8\u6210\u672c\u6570\u636e\u96c6\u751f\u6210\u4e0e\u6a21\u578b\u8bad\u7ec3\u74f6\u9888\uff0c\u4e9f\u9700\u63d0\u5347\u6570\u636e\u6548\u7387", "method": "\u53d7\u692d\u5706\u578bPDE\u683c\u6797\u51fd\u6570\u7ed3\u6784\u542f\u53d1\uff0c\u6784\u5efa\u57fa\u4e8e\u5206\u5c42\u534a\u53ef\u5206(HSS)\u77e9\u9635\u7684\u53c2\u6570\u9ad8\u6548\u67b6\u6784\uff0c\u7406\u8bba\u8bc1\u660e\u5176\u6570\u636e\u9ad8\u6548\u6027", "result": "\u5728200\u4e07\u7f51\u683c\u70b9\u7684\u4e09\u7ef4\u6cca\u677e\u65b9\u7a0b\u6d4b\u8bd5\u4e2d\uff0c\u4f4e\u6570\u636e regime \u4e0b\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\uff1b\u6210\u529f\u5e94\u7528\u4e8e\u7535\u78c1\u5b66/\u6d41\u4f53\u529b\u5b66/\u751f\u7269\u5b66\u7b49\u591a\u9886\u57dfPDE", "conclusion": "Neural-HSS\u4e3a\u5e7f\u6cdb\u7c7b\u522bPDE\u63d0\u4f9b\u9ad8\u6570\u636e\u6548\u7387\u6c42\u89e3\u65b9\u6848\uff0c\u7a81\u7834\u4f20\u7edf\u65b9\u6cd5\u5728\u4f4e\u6570\u636e\u573a\u666f\u7684\u5c40\u9650\u6027"}}
{"id": "2602.18250", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18250", "abs": "https://arxiv.org/abs/2602.18250", "authors": ["Yves Ruffenach"], "title": "Variational Distributional Neuron", "comment": "29 pages, 7 figures. Code available at GitHub (link in paper)", "summary": "We propose a proof of concept for a variational distributional neuron: a compute unit formulated as a VAE brick, explicitly carrying a prior, an amortized posterior and a local ELBO. The unit is no longer a deterministic scalar but a distribution: computing is no longer about propagating values, but about contracting a continuous space of possibilities under constraints. Each neuron parameterizes a posterior, propagates a reparameterized sample and is regularized by the KL term of a local ELBO - hence, the activation is distributional. This \"contraction\" becomes testable through local constraints and can be monitored via internal measures. The amount of contextual information carried by the unit, as well as the temporal persistence of this information, are locally tuned by distinct constraints. This proposal addresses a structural tension: in sequential generation, causality is predominantly organized in the symbolic space and, even when latents exist, they often remain auxiliary, while the effective dynamics are carried by a largely deterministic decoder. In parallel, probabilistic latent models capture factors of variation and uncertainty, but that uncertainty typically remains borne by global or parametric mechanisms, while units continue to propagate scalars - hence the pivot question: if uncertainty is intrinsic to computation, why does the compute unit not carry it explicitly? We therefore draw two axes: (i) the composition of probabilistic constraints, which must be made stable, interpretable and controllable; and (ii) granularity: if inference is a negotiation of distributions under constraints, should the primitive unit remain deterministic or become distributional? We analyze \"collapse\" modes and the conditions for a \"living neuron\", then extend the contribution over time via autoregressive priors over the latent, per unit.", "AI": {"tldr": "\u63d0\u51fa\u53d8\u5206\u5206\u5e03\u795e\u7ecf\u5143\u6982\u5ff5\uff0c\u5c06VAE\u6846\u67b6\u5f15\u5165\u795e\u7ecf\u5143\u7ea7\u522b\uff0c\u4f7f\u6fc0\u6d3b\u503c\u6210\u4e3a\u6982\u7387\u5206\u5e03\u800c\u975e\u786e\u5b9a\u6027\u6807\u91cf\uff0c\u5b9e\u73b0\u8ba1\u7b97\u5373\u7ea6\u675f\u4e0b\u53ef\u80fd\u6027\u7a7a\u95f4\u7684\u538b\u7f29", "motivation": "\u4f20\u7edf\u8ba1\u7b97\u5355\u5143\u4f20\u64ad\u786e\u5b9a\u6027\u6807\u91cf\uff0c\u800c\u6982\u7387\u6a21\u578b\u7684 uncertainty \u4ec5\u7531\u5168\u5c40\u673a\u5236\u627f\u62c5\u3002\u6838\u5fc3\u95ee\u9898\uff1a\u82e5\u4e0d\u786e\u5b9a\u6027\u662f\u8ba1\u7b97\u56fa\u6709\u5c5e\u6027\uff0c\u4e3a\u4f55\u8ba1\u7b97\u5355\u5143\u4e0d\u663e\u5f0f\u627f\u8f7d\u5b83\uff1f\u5b58\u5728\u7b26\u53f7\u56e0\u679c\u6027\u4e0e\u6982\u7387\u4e0d\u786e\u5b9a\u6027\u4e4b\u95f4\u7684\u7ed3\u6784\u5f20\u529b\u3002", "method": "\u8bbe\u8ba1\u53d8\u5206\u5206\u5e03\u795e\u7ecf\u5143\uff1a\u6bcf\u4e2a\u5355\u5143\u5305\u542b\u5148\u9a8c\u3001\u644a\u9500\u540e\u9a8c\u548c\u5c40\u90e8ELBO\uff1b\u53c2\u6570\u5316\u540e\u9a8c\u5206\u5e03\uff0c\u4f20\u64ad\u91cd\u53c2\u6570\u5316\u6837\u672c\uff0c\u901a\u8fc7KL\u6563\u5ea6\u6b63\u5219\u5316\uff1b\u8ba1\u7b97\u8fc7\u7a0b\u89c6\u4e3a\u5728\u7ea6\u675f\u4e0b\u538b\u7f29\u8fde\u7eed\u53ef\u80fd\u6027\u7a7a\u95f4\u3002", "result": "\u6fc0\u6d3b\u503c\u53d8\u4e3a\u5206\u5e03\u5f62\u5f0f\uff1b\u4e0a\u4e0b\u6587\u4fe1\u606f\u91cf\u548c\u65f6\u95f4\u6301\u7eed\u6027\u53ef\u7531\u5c40\u90e8\u7ea6\u675f\u8c03\u8282\uff1b\u901a\u8fc7\u81ea\u56de\u5f52\u5148\u9a8c\u5b9e\u73b0\u8de8\u65f6\u95f4\u6b65\u6269\u5c55\uff1b\u9700\u5206\u6790\"\u574d\u7f29\"\u6a21\u5f0f\u5e76\u5b9a\u4e49\"\u6d3b\u795e\u7ecf\u5143\"\u7684\u7a33\u5b9a\u6027\u6761\u4ef6\u3002", "conclusion": "\u901a\u8fc7\u5c06\u6982\u7387\u7ea6\u675f\u7684\u7ec4\u6210\u4e0e\u5206\u5e03\u7c92\u5ea6\u4e24\u4e2a\u8f74\u7ed3\u5408\uff0c\u4f7f\u8ba1\u7b97\u5355\u5143\u672c\u8eab\u5177\u5907\u663e\u5f0f\u4e0d\u786e\u5b9a\u6027\u8868\u8fbe\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u7b26\u53f7\u751f\u6210\u4e0e\u6982\u7387\u5efa\u6a21\u7684\u7ed3\u6784\u5f20\u529b\u3002"}}
{"id": "2602.18253", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18253", "abs": "https://arxiv.org/abs/2602.18253", "authors": ["Xabier de Zuazo", "Vincenzo Verbeni", "Eva Navas", "Ibon Saratxaga", "Mathieu Bourguignon", "Nicola Molinaro"], "title": "MEG-to-MEG Transfer Learning and Cross-Task Speech/Silence Detection with Limited Data", "comment": "6 pages, 3 figures, 3 tables, submitted to Interspeech 2026", "summary": "Data-efficient neural decoding is a central challenge for speech brain-computer interfaces. We present the first demonstration of transfer learning and cross-task decoding for MEG-based speech models spanning perception and production. We pre-train a Conformer-based model on 50 hours of single-subject listening data and fine-tune on just 5 minutes per subject across 18 participants. Transfer learning yields consistent improvements, with in-task accuracy gains of 1-4% and larger cross-task gains of up to 5-6%. Not only does pre-training improve performance within each task, but it also enables reliable cross-task decoding between perception and production. Critically, models trained on speech production decode passive listening above chance, confirming that learned representations reflect shared neural processes rather than task-specific motor activity.", "AI": {"tldr": "This paper demonstrates that transfer learning using a Conformer model pre-trained on 50 hours of speech listening data significantly improves MEG-based speech decoding, enabling effective cross-task decoding between speech perception and production with minimal fine-tuning data (5 minutes per subject).", "motivation": "Overcoming the data inefficiency challenge in speech brain-computer interfaces (BCIs) by leveraging transfer learning to reduce the extensive training data requirements for neural decoding models.", "method": "Pre-trained a Conformer-based neural network on 50 hours of single-subject speech perception (listening) MEG data, then fine-tuned the model on extremely limited (5-minute) per-subject data across 18 participants for both perception and production tasks, enabling cross-task decoding analysis.", "result": "Transfer learning provided consistent accuracy improvements: 1-4% gains within the same task and significant 5-6% gains when decoding across tasks (e.g., production model decoding listening data, and vice versa). Models trained on production could decode passive listening above chance level.", "conclusion": "Pre-training enables data-efficient speech BCI decoding and reveals that learned neural representations capture shared underlying processes between speech perception and production, not merely task-specific motor activity, validating cross-task decoding feasibility."}}
{"id": "2602.18301", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18301", "abs": "https://arxiv.org/abs/2602.18301", "authors": ["Ivan Bondarenko", "Egor Palkin", "Fedor Tikunov"], "title": "On the Semantic and Syntactic Information Encoded in Proto-Tokens for One-Step Text Reconstruction", "comment": null, "summary": "Autoregressive large language models (LLMs) generate text token-by-token, requiring n forward passes to produce a sequence of length n. Recent work, Exploring the Latent Capacity of LLMs for One-Step Text Reconstruction (Mezentsev and Oseledets), shows that frozen LLMs can reconstruct hundreds of tokens from only two learned proto-tokens in a single forward pass, suggesting a path beyond the autoregressive paradigm. In this paper, we study what information these proto-tokens encode and how they behave under reconstruction and controlled constraints. We perform a series of experiments aimed at disentangling semantic and syntactic content in the two proto-tokens, analyzing stability properties of the e-token, and visualizing attention patterns to the e-token during reconstruction. Finally, we test two regularization schemes for \"imposing\" semantic structure on the e-token using teacher embeddings, including an anchor-based loss and a relational distillation objective. Our results indicate that the m-token tends to capture semantic information more strongly than the e-token under standard optimization; anchor-based constraints trade off sharply with reconstruction accuracy; and relational distillation can transfer batch-level semantic relations into the proto-token space without sacrificing reconstruction quality, supporting the feasibility of future non-autoregressive seq2seq systems that predict proto-tokens as an intermediate representation.", "AI": {"tldr": "\u7814\u7a76 frozen LLMs \u4e2d\u4e24\u4e2a learned proto-tokens \u7684\u4fe1\u606f\u7f16\u7801\u673a\u5236\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u65b9\u6cd5\u5411 e-token \u4e2d\u6ce8\u5165\u8bed\u4e49\u7ed3\u6784\uff0c\u4e3a\u6784\u5efa\u975e\u81ea\u56de\u5f52 seq2seq \u7cfb\u7edf\u63d0\u4f9b\u53ef\u884c\u6027\u8bc1\u660e", "motivation": "\u81ea\u56de\u5f52 LLMs \u9700\u8981 n \u6b21\u524d\u5411\u4f20\u64ad\u751f\u6210 n \u4e2a token\uff0c\u8ba1\u7b97\u6548\u7387\u4f4e\uff1b\u524d\u671f\u7814\u7a76\u8868\u660e frozen LLMs \u53ef\u901a\u8fc7\u4e24\u4e2a learned proto-tokens \u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u91cd\u5efa\u6570\u767e\u4e2a token\uff0c\u6697\u793a\u8d85\u8d8a\u81ea\u56de\u5f52\u8303\u5f0f\u7684\u53ef\u80fd\uff0c\u4f46\u9700\u7406\u89e3\u8fd9\u4e9b proto-tokens \u7684\u7f16\u7801\u673a\u5236\u548c\u7ea6\u675f\u884c\u4e3a", "method": "\u8bbe\u8ba1\u7cfb\u5217\u5b9e\u9a8c\u89e3\u6784\u4e24\u4e2a proto-tokens \u7684\u8bed\u4e49\u4e0e\u53e5\u6cd5\u4fe1\u606f\uff0c\u5206\u6790 e-token \u7684\u7a33\u5b9a\u6027\uff0c\u53ef\u89c6\u5316\u91cd\u5efa\u65f6\u5bf9 e-token \u7684\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u5e76\u6d4b\u8bd5\u4e24\u79cd\u57fa\u4e8e\u6559\u5e08\u5d4c\u5165\u7684\u6b63\u5219\u5316\u65b9\u6848\uff1aanchor-based loss \u548c relational distillation objective", "result": "m-token \u6bd4 e-token \u5728\u6807\u51c6\u4f18\u5316\u4e0b\u6355\u83b7\u66f4\u5f3a\u7684\u8bed\u4e49\u4fe1\u606f\uff1banchor-based \u7ea6\u675f\u4e0e\u91cd\u5efa\u7cbe\u5ea6\u5b58\u5728\u5c16\u9510\u6743\u8861\uff1brelational distillation \u53ef\u5728\u4e0d\u727a\u7272\u91cd\u5efa\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\uff0c\u5c06 batch-level \u8bed\u4e49\u5173\u7cfb\u8f6c\u79fb\u5230 proto-token \u7a7a\u95f4", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u652f\u6301\u672a\u6765\u975e\u81ea\u56de\u5f52 seq2seq \u7cfb\u7edf\u7684\u53ef\u884c\u6027\uff0c\u5373\u901a\u8fc7\u9884\u6d4b proto-tokens \u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\u6765\u5b9e\u73b0\u9ad8\u6548\u6587\u672c\u751f\u6210"}}
{"id": "2602.18333", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18333", "abs": "https://arxiv.org/abs/2602.18333", "authors": ["M. Reza Ebrahimi", "Micha\u00ebl Defferrard", "Sunny Panchal", "Roland Memisevic"], "title": "On the \"Induction Bias\" in Sequence Models", "comment": null, "summary": "Despite the remarkable practical success of transformer-based language models, recent work has raised concerns about their ability to perform state tracking. In particular, a growing body of literature has shown this limitation primarily through failures in out-of-distribution (OOD) generalization, such as length extrapolation. In this work, we shift attention to the in-distribution implications of these limitations. We conduct a large-scale experimental study of the data efficiency of transformers and recurrent neural networks (RNNs) across multiple supervision regimes. We find that the amount of training data required by transformers grows much more rapidly with state-space size and sequence length than for RNNs. Furthermore, we analyze the extent to which learned state-tracking mechanisms are shared across different sequence lengths. We show that transformers exhibit negligible or even detrimental weight sharing across lengths, indicating that they learn length-specific solutions in isolation. In contrast, recurrent models exhibit effective amortized learning by sharing weights across lengths, allowing data from one sequence length to improve performance on others. Together, these results demonstrate that state tracking remains a fundamental challenge for transformers, even when training and evaluation distributions match.", "AI": {"tldr": "This paper compares transformers and RNNs on state tracking tasks, finding transformers require exponentially more data as state-space/sequence length grows, learn length-specific solutions without sharing weights across lengths, while RNNs share weights effectively across lengths, showing state tracking is a fundamental in-distribution challenge for transformers.", "motivation": "Recent work shows transformers struggle with state tracking, but focuses on out-of-distribution failures. This paper investigates whether this limitation also affects in-distribution performance, questioning transformers' fundamental state-tracking capabilities.", "method": "Conducted large-scale experiments comparing data efficiency of transformers and RNNs across multiple supervision regimes, and analyzed weight sharing patterns of learned state-tracking mechanisms across different sequence lengths.", "result": "Transformers need training data that scales much faster with state-space size and sequence length compared to RNNs. Transformers show negligible or detrimental weight sharing across lengths, learning length-specific solutions in isolation. RNNs exhibit effective amortized learning by sharing weights across lengths, improving performance on all lengths.", "conclusion": "State tracking remains a fundamental challenge for transformers even when training and evaluation distributions match (in-distribution), as evidenced by their poor data efficiency and lack of weight sharing across sequence lengths."}}
{"id": "2602.18348", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18348", "abs": "https://arxiv.org/abs/2602.18348", "authors": ["Matheus Camilo da Silva", "Leonardo Arrighi", "Ana Carolina Lorena", "Sylvio Barbon Junior"], "title": "Explaining AutoClustering: Uncovering Meta-Feature Contribution in AutoML for Clustering", "comment": null, "summary": "AutoClustering methods aim to automate unsupervised learning tasks, including algorithm selection (AS), hyperparameter optimization (HPO), and pipeline synthesis (PS), by often leveraging meta-learning over dataset meta-features. While these systems often achieve strong performance, their recommendations are often difficult to justify: the influence of dataset meta-features on algorithm and hyperparameter choices is typically not exposed, limiting reliability, bias diagnostics, and efficient meta-feature engineering. This limits reliability and diagnostic insight for further improvements. In this work, we investigate the explainability of the meta-models in AutoClustering. We first review 22 existing methods and organize their meta-features into a structured taxonomy. We then apply a global explainability technique (i.e., Decision Predicate Graphs) to assess feature importance within meta-models from selected frameworks. Finally, we use local explainability tools such as SHAP (SHapley Additive exPlanations) to analyse specific clustering decisions. Our findings highlight consistent patterns in meta-feature relevance, identify structural weaknesses in current meta-learning strategies that can distort recommendations, and provide actionable guidance for more interpretable Automated Machine Learning (AutoML) design. This study therefore offers a practical foundation for increasing decision transparency in unsupervised learning automation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8c03\u67e5\u4e86AutoClustering\u5143\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u901a\u8fc7\u6784\u5efa\u5143\u7279\u5f81\u5206\u7c7b\u4f53\u7cfb\u3001\u5e94\u7528\u5168\u5c40\u53ef\u89e3\u91ca\u6280\u672f\uff08\u51b3\u7b56\u8c13\u8bcd\u56fe\uff09\u548c\u5c40\u90e8\u53ef\u89e3\u91ca\u5de5\u5177\uff08SHAP\uff09\uff0c\u63ed\u793a\u4e86\u5143\u7279\u5f81\u91cd\u8981\u6027\u7684\u4e00\u81f4\u6027\u6a21\u5f0f\uff0c\u53d1\u73b0\u4e86\u5f53\u524d\u5143\u5b66\u4e60\u7b56\u7565\u7684\u7ed3\u6784\u6027\u7f3a\u9677\uff0c\u5e76\u4e3a\u53ef\u89e3\u91ca\u7684AutoML\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002", "motivation": "AutoClustering\u65b9\u6cd5\u867d\u6027\u80fd\u5f3a\u5927\uff0c\u4f46\u5176\u63a8\u8350\u96be\u4ee5\u89e3\u91ca\uff0c\u5143\u7279\u5f81\u5bf9\u7b97\u6cd5\u548c\u8d85\u53c2\u6570\u9009\u62e9\u7684\u5f71\u54cd\u4e0d\u900f\u660e\uff0c\u9650\u5236\u4e86\u53ef\u9760\u6027\u3001\u504f\u5dee\u8bca\u65ad\u548c\u5143\u7279\u5f81\u5de5\u7a0b\u6548\u7387\u3002", "method": "\u7efc\u8ff022\u79cd\u73b0\u6709\u65b9\u6cd5\u5e76\u6784\u5efa\u5143\u7279\u5f81\u5206\u7c7b\u4f53\u7cfb\uff1b\u5e94\u7528\u51b3\u7b56\u8c13\u8bcd\u56fe\u8fdb\u884c\u5168\u5c40\u7279\u5f81\u91cd\u8981\u6027\u8bc4\u4f30\uff1b\u4f7f\u7528SHAP\u5de5\u5177\u5206\u6790\u5177\u4f53\u805a\u7c7b\u51b3\u7b56\u3002", "result": "\u53d1\u73b0\u5143\u7279\u5f81\u76f8\u5173\u6027\u7684\u7a33\u5b9a\u6a21\u5f0f\uff0c\u8bc6\u522b\u51fa\u5f53\u524d\u5143\u5b66\u4e60\u7b56\u7565\u4e2d\u5bfc\u81f4\u63a8\u8350\u5931\u771f\u7684\u7ed3\u6784\u6027\u7f3a\u9677\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u89e3\u91ca\u7684AutoML\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u589e\u5f3a\u65e0\u76d1\u7763\u5b66\u4e60\u81ea\u52a8\u5316\u51b3\u7b56\u7684\u900f\u660e\u5ea6\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840\u3002"}}
{"id": "2602.18417", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18417", "abs": "https://arxiv.org/abs/2602.18417", "authors": ["Joshua Nunley"], "title": "Subgroups of $U(d)$ Induce Natural RNN and Transformer Architectures", "comment": "12 pages, 3 figures, 8 tables", "summary": "This paper presents a direct framework for sequence models with hidden states on closed subgroups of U(d). We use a minimal axiomatic setup and derive recurrent and transformer templates from a shared skeleton in which subgroup choice acts as a drop-in replacement for state space, tangent projection, and update map. We then specialize to O(d) and evaluate orthogonal-state RNN and transformer models on Tiny Shakespeare and Penn Treebank under parameter-matched settings. We also report a general linear-mixing extension in tangent space, which applies across subgroup choices and improves finite-budget performance in the current O(d) experiments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5728U(d)\u95ed\u5b50\u7fa4\u4e0a\u6784\u5efa\u5e8f\u5217\u6a21\u578b\u7684\u7edf\u4e00\u51e0\u4f55\u6846\u67b6\uff0c\u4ece\u5171\u4eab\u9aa8\u67b6\u540c\u65f6\u63a8\u5bfc\u51faRNN\u548cTransformer\u6a21\u677f\uff0c\u5e76\u9a8c\u8bc1\u4e86\u6b63\u4ea4\u72b6\u6001\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5c06\u5e8f\u5217\u6a21\u578b\u7684\u9690\u72b6\u6001\u7a7a\u95f4\u63a8\u5e7f\u5230\u77e9\u9635\u674e\u7fa4\u7ed3\u6784\uff0c\u63a2\u7d22\u6b63\u4ea4/\u9149\u7ea6\u675f\u5728\u5e8f\u5217\u5efa\u6a21\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u5efa\u7acbRNN\u4e0eTransformer\u67b6\u6784\u4e4b\u95f4\u7684\u6df1\u5c42\u7406\u8bba\u8054\u7cfb\u3002", "method": "\u91c7\u7528\u6700\u5c0f\u516c\u7406\u5316\u65b9\u6cd5\uff0c\u6784\u5efa\u72b6\u6001\u7a7a\u95f4\u3001\u5207\u7ebf\u6295\u5f71\u548c\u66f4\u65b0\u6620\u5c04\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u5c06\u5b50\u7fa4\u9009\u62e9\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u7ec4\u4ef6\uff1b\u7279\u5316\u4e3aO(d)\u6b63\u4ea4\u7fa4\u540e\u5728Tiny Shakespeare\u548cPenn Treebank\u4e0a\u8fdb\u884c\u53c2\u6570\u5339\u914d\u5b9e\u9a8c\uff0c\u5e76\u5f15\u5165\u8de8\u5b50\u7fa4\u9002\u7528\u7684\u5207\u7ebf\u7a7a\u95f4\u7ebf\u6027\u6df7\u5408\u6269\u5c55\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u6b63\u4ea4\u72b6\u6001RNN\u548cTransformer\u6a21\u578b\uff0c\u63d0\u51fa\u7684\u5207\u7ebf\u7a7a\u95f4\u7ebf\u6027\u6df7\u5408\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u6709\u9650\u8ba1\u7b97\u9884\u7b97\u4e0b\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u51e0\u4f55\u65b9\u6cd5\u4e3a\u5e8f\u5217\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u539f\u5219\u6027\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5b50\u7fa4\u9009\u62e9\u53ef\u4f5c\u4e3a\u67b6\u6784\u8bbe\u8ba1\u7684\u65b0\u7ef4\u5ea6\uff0c\u7ebf\u6027\u6df7\u5408\u6280\u672f\u662f\u901a\u7528\u4e14\u6709\u6548\u7684\u6027\u80fd\u589e\u5f3a\u624b\u6bb5\u3002"}}
{"id": "2602.18435", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18435", "abs": "https://arxiv.org/abs/2602.18435", "authors": ["Aggelos Semoglou", "John Pavlopoulos"], "title": "Assigning Confidence: K-partition Ensembles", "comment": "31 pages including appendix", "summary": "Clustering is widely used for unsupervised structure discovery, yet it offers limited insight into how reliable each individual assignment is. Diagnostics, such as convergence behavior or objective values, may reflect global quality, but they do not indicate whether particular instances are assigned confidently, especially for initialization-sensitive algorithms like k-means. This assignment-level instability can undermine both accuracy and robustness. Ensemble approaches improve global consistency by aggregating multiple runs, but they typically lack tools for quantifying pointwise confidence in a way that combines cross-run agreement with geometric support from the learned cluster structure. We introduce CAKE (Confidence in Assignments via K-partition Ensembles), a framework that evaluates each point using two complementary statistics computed over a clustering ensemble: assignment stability and consistency of local geometric fit. These are combined into a single, interpretable score in [0,1]. Our theoretical analysis shows that CAKE remains effective under noise and separates stable from unstable points. Experiments on synthetic and real-world datasets indicate that CAKE effectively highlights ambiguous points and stable core members, providing a confidence ranking that can guide filtering or prioritization to improve clustering quality.", "AI": {"tldr": "CAKE\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u805a\u7c7b\u8bc4\u4f30\u5355\u70b9\u5206\u914d\u7f6e\u4fe1\u5ea6\uff0c\u7ed3\u5408\u5206\u914d\u7a33\u5b9a\u6027\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u63d0\u4f9b\u53ef\u89e3\u91ca\u8bc4\u5206", "motivation": "\u4f20\u7edf\u805a\u7c7b\u65b9\u6cd5\uff08\u5982k-means\uff09\u7f3a\u4e4f\u5bf9\u5355\u4e2a\u6837\u672c\u5206\u914d\u53ef\u9760\u6027\u7684\u8bca\u65ad\uff0c\u5168\u5c40\u8d28\u91cf\u6307\u6807\u65e0\u6cd5\u53cd\u6620 pointwise \u7f6e\u4fe1\u5ea6\uff0c\u5f71\u54cd\u7ed3\u679c\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027", "method": "\u57fa\u4e8e\u805a\u7c7b\u96c6\u6210\u6846\u67b6\uff0c\u8ba1\u7b97\u4e24\u4e2a\u4e92\u8865\u7edf\u8ba1\u91cf\uff1a1) \u8de8\u591a\u6b21\u8fd0\u884c\u7684\u5206\u914d\u7a33\u5b9a\u6027\uff1b2) \u5c40\u90e8\u51e0\u4f55\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u878d\u5408\u4e3a[0,1]\u533a\u95f4\u7684\u7f6e\u4fe1\u5ea6\u8bc4\u5206", "result": "\u7406\u8bba\u8bc1\u660eCAKE\u6297\u566a\u58f0\u80fd\u529b\u5f3a\u4e14\u80fd\u533a\u5206\u7a33\u5b9a/\u4e0d\u7a33\u5b9a\u70b9\uff1b\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u6709\u6548\u8bc6\u522b\u6a21\u7cca\u70b9\u4e0e\u6838\u5fc3\u6210\u5458\uff0c\u63d0\u4f9b\u53ef\u6307\u5bfc\u6570\u636e\u8fc7\u6ee4\u7684\u7f6e\u4fe1\u5ea6\u6392\u5e8f", "conclusion": "CAKE\u4e3a\u805a\u7c7b\u7ed3\u679c\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u7f6e\u4fe1\u5ea6\u91cf\u5316\u5de5\u5177\uff0c\u901a\u8fc7\u63d0\u5347 assignment-level \u53ef\u9760\u6027\u4f18\u5316\u805a\u7c7b\u8d28\u91cf\uff0c\u652f\u6301\u540e\u7eed\u5206\u6790\u51b3\u7b56"}}
