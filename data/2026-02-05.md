<div id=toc></div>

# Table of Contents

- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 2]
- [cs.AI](#cs.AI) [Total: 22]
- [nlin.CD](#nlin.CD) [Total: 3]
- [quant-ph](#quant-ph) [Total: 30]
- [cs.LG](#cs.LG) [Total: 95]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 11]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 4]


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [1] [Scalable platform enabling reservoir computing with nanoporous oxide memristors for image recognition and time series prediction](https://arxiv.org/abs/2602.04619)
*Joshua Donald,Ben A. Johnson,Amir Mehrnejat,Alex Gabbitas,Arthur G. T. Coveney,Alexander G. Balanov,Sergey Savel'ev,Pavel Borisov*

Main category: cond-mat.dis-nn

TL;DR: Niobium oxide memristor with random nanopores serves as physical reservoir for efficient spatiotemporal signal processing, demonstrating XOR, image recognition, and chaotic time series prediction


<details>
  <summary>Details</summary>
Motivation: To develop energy-efficient neuromorphic hardware for processing complex time signals using reservoir computing principles inspired by biological neural networks

Method: Fabricated niobium oxide thin-film memristor with intrinsic random nanopores as physical reservoir; applied three temporal voltage inputs and trained electrical current outputs for XOR, image recognition, and Lorenz-63 chaotic time series tasks

Result: Achieved satisfactory prediction/reconstruction accuracy for complex 3D chaotic time series, outperforming non-reservoir approaches; validated functionality across multiple computational tasks

Conclusion: Scalable all-oxide reservoir systems show strong potential for on-chip, energy-efficient neuromorphic electronics capable of processing spatiotemporal signals

Abstract: Typical mammal brains have some form of random connectivity between neurons. Reservoir computing, a neural network approach, uses random weights within its processing layer along with built-in recurrent connections and short-term, fading memory, and is shown to be time and training efficient in processing spatiotemporal signals. Here we prepared a niobium oxide-based thin film memristor device with intrinsic structural in-homogeneity in the form of random nanopores and performed computational tasks of XOR operations, image recognition, and time series prediction and reconstruction. For the latter task we chose a complex three-dimensional chaotic Lorenz-63 time series. By applying three temporal voltage waveforms individually across the device and training the readout layer with electrical current signals from a three-output physical reservoir, we achieved satisfactory prediction and reconstruction accuracy in comparison to the case of no reservoir. This work highlights the potential for scalable, on-chip devices using all-oxide reservoir systems, paving the way for energy-efficient neuromorphic electronics dealing with time signals.

</details>


### [2] [Theory of Optimal Learning Rate Schedules and Scaling Laws for a Random Feature Model](https://arxiv.org/abs/2602.04774)
*Blake Bordelon,Francesco Mori*

Main category: cond-mat.dis-nn

TL;DR: 本文通过最优控制理论求解幂律随机特征模型SGD训练的最优学习率调度，揭示"简单相"与"困难相"两种机制，提出超越经验试错的理论驱动方案。


<details>
  <summary>Details</summary>
Motivation: 深度学习训练中学习率是关键但常被经验性设置的超参数，亟需理论指导。

Method: 构建可解的幂律随机特征模型，运用最优控制方法数值和解析计算最优调度η_T*(t)。

Result: 发现简单相为多项式衰减η_T*(t)≈T^(-ξ)(1-t/T)^δ，困难相为预热-稳定-衰减模式；识别学习率与批大小的联合优化条件；预测计算最优缩放律；动量调度在困难相可加速；所提方法优于常量和幂律基准。

Conclusion: 学习率跨周期迁移性取决于模型与任务结构，理论最优调度显著超越经验方法，为深度学习训练提供新框架。

Abstract: Setting the learning rate for a deep learning model is a critical part of successful training, yet choosing this hyperparameter is often done empirically with trial and error. In this work, we explore a solvable model of optimal learning rate schedules for a powerlaw random feature model trained with stochastic gradient descent (SGD). We consider the optimal schedule $η_T^\star(t)$ where $t$ is the current iterate and $T$ is the total training horizon. This schedule is computed both numerically and analytically (when possible) using optimal control methods. Our analysis reveals two regimes which we term the easy phase and hard phase. In the easy phase the optimal schedule is a polynomial decay $η_T^\star(t) \simeq T^{-ξ} (1-t/T)^δ$ where $ξ$ and $δ$ depend on the properties of the features and task. In the hard phase, the optimal schedule resembles warmup-stable-decay with constant (in $T$) initial learning rate and annealing performed over a vanishing (in $T$) fraction of training steps. We investigate joint optimization of learning rate and batch size, identifying a degenerate optimality condition. Our model also predicts the compute-optimal scaling laws (where model size and training steps are chosen optimally) in both easy and hard regimes. Going beyond SGD, we consider optimal schedules for the momentum $β(t)$, where speedups in the hard phase are possible. We compare our optimal schedule to various benchmarks in our task including (1) optimal constant learning rates $η_T(t) \sim T^{-ξ}$ (2) optimal power laws $η_T(t) \sim T^{-ξ} t^{-χ}$, finding that our schedule achieves better rates than either of these. Our theory suggests that learning rate transfer across training horizon depends on the structure of the model and task. We explore these ideas in simple experimental pretraining setups.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [3] [Knowledge Model Prompting Increases LLM Performance on Planning Tasks](https://arxiv.org/abs/2602.03900)
*Erik Goh,John Kos,Ashok Goel*

Main category: cs.AI

TL;DR: The paper applies the Task-Method-Knowledge (TMK) framework from cognitive science to improve LLM reasoning. Testing on PlanBench's Blocksworld domain, TMK-structured prompting dramatically improved accuracy from 31.5% to 97.3% on symbolic tasks by helping models decompose problems and shift from linguistic to formal reasoning modes.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with reasoning and planning tasks, and existing techniques like Chain-of-Thought are under scrutiny. The TMK framework shows promise due to its ability to capture causal, teleological, and hierarchical reasoning structures while explicitly representing what to do, how to do it, and why—unlike other hierarchical frameworks.

Method: The study evaluates TMK by experimenting on the PlanBench benchmark, focusing on the Blocksworld domain to test reasoning and planning capabilities. It uses TMK-structured prompting to help language models decompose complex planning problems into manageable sub-tasks.

Result: TMK prompting enabled a reasoning model to achieve up to 97.3% accuracy on opaque, symbolic tasks (Random versions of Blocksworld) where it previously failed (31.5%), demonstrating a significant performance inversion and bridging the gap between semantic approximation and symbolic manipulation.

Conclusion: TMK functions not merely as context but as a mechanism that steers reasoning models away from default linguistic modes to engage formal, code-execution pathways, suggesting its potential to enhance LLM reasoning capabilities.

Abstract: Large Language Models (LLM) can struggle with reasoning ability and planning tasks. Many prompting techniques have been developed to assist with LLM reasoning, notably Chain-of-Thought (CoT); however, these techniques, too, have come under scrutiny as LLMs' ability to reason at all has come into question. Borrowing from the domain of cognitive and educational science, this paper investigates whether the Task-Method-Knowledge (TMK) framework can improve LLM reasoning capabilities beyond its previously demonstrated success in educational applications. The TMK framework's unique ability to capture causal, teleological, and hierarchical reasoning structures, combined with its explicit task decomposition mechanisms, makes it particularly well-suited for addressing language model reasoning deficiencies, and unlike other hierarchical frameworks such as HTN and BDI, TMK provides explicit representations of not just what to do and how to do it, but also why actions are taken. The study evaluates TMK by experimenting on the PlanBench benchmark, focusing on the Blocksworld domain to test for reasoning and planning capabilities, examining whether TMK-structured prompting can help language models better decompose complex planning problems into manageable sub-tasks. Results also highlight significant performance inversion in reasoning models. TMK prompting enables the reasoning model to achieve up to an accuracy of 97.3\% on opaque, symbolic tasks (Random versions of Blocksworld in PlanBench) where it previously failed (31.5\%), suggesting the potential to bridge the gap between semantic approximation and symbolic manipulation. Our findings suggest that TMK functions not merely as context, but also as a mechanism that steers reasoning models away from their default linguistic modes to engage formal, code-execution pathways in the context of the experiments.

</details>


### [4] [Enhancing Mathematical Problem Solving in LLMs through Execution-Driven Reasoning Augmentation](https://arxiv.org/abs/2602.03950)
*Aditya Basarkar,Benyamin Tabarsi,Tiffany Barnes,Dongkuan,Xu*

Main category: cs.AI

TL;DR: 提出Iteratively Improved Program Construction (IIPC)方法，通过迭代优化程序化推理链并结合执行反馈与思维链能力，提升LLM数学推理性能，在多个基准上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体LLM系统在数学推理中缺乏可修订的推理过程表示：顺序流水线无法纠错，启发式自评估可能遗漏错误，程序化上下文还会分散模型注意力降低准确率。

Method: IIPC方法迭代地优化程序化推理链，将代码执行反馈与基础LLM的原生思维链能力结合，保持高层上下文专注度。

Result: 在多个基础LLM上，IIPC在大多数推理基准测试中超越竞争方法。

Conclusion: IIPC有效解决了程序化推理中的错误修正和上下文干扰问题，所有代码开源。

Abstract: Mathematical problem solving is a fundamental benchmark for assessing the reasoning capabilities of artificial intelligence and a gateway to applications in education, science, and engineering where reliable symbolic reasoning is essential. Although recent advances in multi-agent LLM-based systems have enhanced their mathematical reasoning capabilities, they still lack a reliably revisable representation of the reasoning process. Existing agents either operate in rigid sequential pipelines that cannot correct earlier steps or rely on heuristic self-evaluation that can fail to identify and fix errors. In addition, programmatic context can distract language models and degrade accuracy. To address these gaps, we introduce Iteratively Improved Program Construction (IIPC), a reasoning method that iteratively refines programmatic reasoning chains and combines execution feedback with the native Chain-of-thought abilities of the base LLM to maintain high-level contextual focus. IIPC surpasses competing approaches in the majority of reasoning benchmarks on multiple base LLMs. All code and implementations are released as open source.

</details>


### [5] [AgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent](https://arxiv.org/abs/2602.03955)
*Yinyi Luo,Yiqiao Jin,Weichen Yu,Mengqi Zhang,Srijan Kumar,Xiaoxiao Li,Weijie Xu,Xin Chen,Jindong Wang*

Main category: cs.AI

TL;DR: AgentArk框架通过三种分层蒸馏策略将多智能体辩论动态压缩到单个模型权重中，实现单智能体效率与多智能体推理性能的统一。


<details>
  <summary>Details</summary>
Motivation: 大语言模型多智能体系统虽然推理性能优越，但部署受限于高计算成本和错误传播问题，亟需提升实用性。

Method: 提出AgentArk框架，研究三种分层蒸馏策略：推理增强微调、轨迹增强和过程感知蒸馏，将显式测试时交互转化为隐式模型能力。

Result: 蒸馏后的模型保持单智能体计算效率，同时展现出多智能体的强推理和自我修正性能，并在多样推理任务中表现出更好的鲁棒性和泛化能力。

Conclusion: 通过将计算负担从推理转移到训练，为高效鲁棒的多智能体开发提供了新思路，代码已开源。

Abstract: While large language model (LLM) multi-agent systems achieve superior reasoning performance through iterative debate, practical deployment is limited by their high computational cost and error propagation. This paper proposes AgentArk, a novel framework to distill multi-agent dynamics into the weights of a single model, effectively transforming explicit test-time interactions into implicit model capabilities. This equips a single agent with the intelligence of multi-agent systems while remaining computationally efficient. Specifically, we investigate three hierarchical distillation strategies across various models, tasks, scaling, and scenarios: reasoning-enhanced fine-tuning; trajectory-based augmentation; and process-aware distillation. By shifting the burden of computation from inference to training, the distilled models preserve the efficiency of one agent while exhibiting strong reasoning and self-correction performance of multiple agents. They further demonstrate enhanced robustness and generalization across diverse reasoning tasks. We hope this work can shed light on future research on efficient and robust multi-agent development. Our code is at https://github.com/AIFrontierLab/AgentArk.

</details>


### [6] [Active Epistemic Control for Query-Efficient Verified Planning](https://arxiv.org/abs/2602.03974)
*Shuhui Qu*

Main category: cs.AI

TL;DR: 提出主动认知控制(AEC)框架，通过分离"已验证事实库"和"信念库"，在保证安全性的前提下利用学习世界模型提升交互式部分可观测环境中的规划效率。


<details>
  <summary>Details</summary>
Motivation: 在部分可观测的交互式环境中，任务关键前提未知但直接验证成本高昂；学习世界模型虽能廉价预测缺失信息，但预测误差会导致不可行承诺，亟需在效率与安全间取得平衡。

Method: 设计认知-范畴规划层AEC：维护独立的"已验证事实库"（用于承诺）和"信念库"（仅用于剪枝）。每步根据不确定性高低选择直接查询环境或使用模型模拟；最终承诺需通过 grounded 前提覆盖度和SQ-BCP拉回式兼容性检查，确保模拟信念只影响效率而不能直接认证可行性。

Result: 在ALFWorld和ScienceWorld实验表明，AEC在成功率上具有竞争力，且比强LLM智能体基线显著减少重规划轮次。

Conclusion: AEC通过严格的信念管理机制，有效利用世界模型提升规划效率的同时，防止预测误差引发不可行承诺，为部分可观测环境下的安全规划提供了新范式。

Abstract: Planning in interactive environments is challenging under partial observability: task-critical preconditions (e.g., object locations or container states) may be unknown at decision time, yet grounding them through interaction is costly. Learned world models can cheaply predict missing facts, but prediction errors can silently induce infeasible commitments. We present \textbf{Active Epistemic Control (AEC)}, an epistemic-categorical planning layer that integrates model-based belief management with categorical feasibility checks. AEC maintains a strict separation between a \emph{grounded fact store} used for commitment and a \emph{belief store} used only for pruning candidate plans. At each step, it either queries the environment to ground an unresolved predicate when uncertainty is high or predictions are ambiguous, or simulates the predicate to filter hypotheses when confidence is sufficient. Final commitment is gated by grounded precondition coverage and an SQ-BCP pullback-style compatibility check, so simulated beliefs affect efficiency but cannot directly certify feasibility. Experiments on ALFWorld and ScienceWorld show that AEC achieves competitive success with fewer replanning rounds than strong LLM-agent baselines.

</details>


### [7] [Adaptive Test-Time Compute Allocation via Learned Heuristics over Categorical Structure](https://arxiv.org/abs/2602.03975)
*Shuhui Qu*

Main category: cs.AI

TL;DR: 该论文提出一种状态级选择性验证框架，通过智能分配验证器调用来解决大语言模型推理中验证成本高昂的问题，在MATH基准测试中将验证调用减少44%的同时提高准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理的测试时计算受到昂贵验证的瓶颈限制，大量验证器调用浪费在冗余或无希望的中介假设上，亟需在验证成本受限的情况下合理分配验证资源。

Method: 提出包含三个组件的状态级选择性验证框架：(1) 基于结构化移动接口的确定性可行性门控，(2) 结合学习式状态距离和残差评分的预验证排序，(3) 根据局部不确定性自适应分配验证器调用。

Result: 在MATH基准测试上，该方法相比best-of-N、多数投票和束搜索获得更高准确率，同时减少44%的验证器调用。

Conclusion: 与解决方案级或均匀验证方法不同，状态级选择性验证能将验证资源分配到最具信息量的位置，实现以更少的计算资源获得更好的推理性能。

Abstract: Test-time computation has become a primary driver of progress in large language model (LLM) reasoning, but it is increasingly bottlenecked by expensive verification. In many reasoning systems, a large fraction of verifier calls are spent on redundant or unpromising intermediate hypotheses. We study reasoning under a \emph{verification-cost-limited} setting and ask how verification effort should be allocated across intermediate states. We propose a state-level selective verification framework that combines (i) deterministic feasibility gating over a structured move interface, (ii) pre-verification ranking using a hybrid of learned state-distance and residual scoring, and (iii) adaptive allocation of verifier calls based on local uncertainty. Unlike solution-level best-of-$N$ or uniform intermediate verification, our method distributes verification where it is most informative. On the \textsc{MATH} benchmark, our approach achieves higher accuracy than best-of-$N$, majority voting, and beam search while using 44\% fewer verifier calls.

</details>


### [8] [Monitorability as a Free Gift: How RLVR Spontaneously Aligns Reasoning](https://arxiv.org/abs/2602.03978)
*Zidi Xiong,Shan Chen,Himabindu Lakkaraju*

Main category: cs.AI

TL;DR: 本文通过系统评估发现，大模型推理过程中思维链的可监测性提升并非RLVR训练的免费副产品，而是高度依赖数据多样性，且与模型能力正交，主要源于响应分布锐化和对提示的关注增强。


<details>
  <summary>Details</summary>
Motivation: 随着大推理模型部署增多，审计其思维链轨迹以确保安全性变得至关重要。近期研究表明，在RLVR早期阶段，可监测性可能作为"免费礼物"出现，但这一现象缺乏系统性验证。

Method: 研究通过对不同模型家族和训练领域进行系统评估，并结合机制性分析，探究RLVR训练中可监测性的变化规律。

Result: 1) 可监测性提升非普遍现象，强烈依赖数据；2) 数据多样性和指令遵循数据起关键作用；3) 可监测性与能力正交，推理性能提升不意味着透明度增加；4) 机制分析表明增益主要来自响应分布锐化（熵减）和增强的提示关注，而非对推理轨迹的因果依赖增强；5) 可监测性动态随训练和评估难度变化而变化。

Conclusion: 研究提供了RLVR下可监测性产生机制的整体视图，明确了在何种条件下可监测性增益可能出现或不会出现，为模型安全审计提供了重要洞察。

Abstract: As Large Reasoning Models (LRMs) are increasingly deployed, auditing their chain-of-thought (CoT) traces for safety becomes critical. Recent work has reported that monitorability--the degree to which CoT faithfully and informatively reflects internal computation--can appear as a "free gift" during the early stages of Reinforcement Learning with Verifiable Rewards (RLVR). We make this observation concrete through a systematic evaluation across model families and training domains. Our results show that this effect is not universal: monitorability improvements are strongly data-dependent. In particular, we demonstrate the critical role of data diversity and instruction-following data during RLVR training. We further show that monitorability is orthogonal to capability--improvements in reasoning performance do not imply increased transparency. Through mechanistic analysis, we attribute monitorability gains primarily to response distribution sharpening (entropy reduction) and increased attention to the prompt, rather than stronger causal reliance on reasoning traces. We also reveal how monitorability dynamics vary with controlled training and evaluation difficulty. Together, these findings provide a holistic view of how monitorability emerges under RLVR, clarifying when gains are likely to occur and when they are not.

</details>


### [9] [When AI Persuades: Adversarial Explanation Attacks on Human Trust in AI-Assisted Decision Making](https://arxiv.org/abs/2602.04003)
*Shutong Fan,Lan Zhang,Xiaoyong Yuan*

Main category: cs.AI

TL;DR: This paper introduces adversarial explanation attacks (AEAs) where attackers manipulate LLM-generated explanations to make humans trust incorrect AI outputs, revealing a new cognitive-layer security vulnerability.


<details>
  <summary>Details</summary>
Motivation: Traditional adversarial attacks target AI models' computational behavior, but modern systems increasingly operate within human decision loops. The research identifies a novel attack surface at the cognitive layer—where LLM-generated explanations shape user trust—posing risks when persuasive explanations reinforce trust in incorrect predictions.

Method: The authors formalized the threat using a "trust miscalibration gap" metric and conducted a controlled experiment (n=205). They systematically manipulated four explanation dimensions: reasoning mode, evidence type, communication style, and presentation format to measure their impact on human trust in incorrect AI outputs.

Result: Users reported nearly identical trust for adversarial and benign explanations, with adversarial versions preserving most benign trust despite being incorrect. Most effective attacks mimicked expert communication (authoritative evidence, neutral tone, domain-appropriate reasoning). Vulnerability was highest on difficult tasks, fact-driven domains, and among less educated, younger, or highly AI-trusting participants.

Conclusion: This first systematic security study treating explanations as an adversarial cognitive channel demonstrates that LLM-generated explanations can be weaponized to manipulate human trust, revealing a critical vulnerability in human-AI decision-making systems that requires urgent attention.

Abstract: Most adversarial threats in artificial intelligence target the computational behavior of models rather than the humans who rely on them. Yet modern AI systems increasingly operate within human decision loops, where users interpret and act on model recommendations. Large Language Models generate fluent natural-language explanations that shape how users perceive and trust AI outputs, revealing a new attack surface at the cognitive layer: the communication channel between AI and its users. We introduce adversarial explanation attacks (AEAs), where an attacker manipulates the framing of LLM-generated explanations to modulate human trust in incorrect outputs. We formalize this behavioral threat through the trust miscalibration gap, a metric that captures the difference in human trust between correct and incorrect outputs under adversarial explanations. By incorporating this gap, AEAs explore the daunting threats in which persuasive explanations reinforce users' trust in incorrect predictions. To characterize this threat, we conducted a controlled experiment (n = 205), systematically varying four dimensions of explanation framing: reasoning mode, evidence type, communication style, and presentation format. Our findings show that users report nearly identical trust for adversarial and benign explanations, with adversarial explanations preserving the vast majority of benign trust despite being incorrect. The most vulnerable cases arise when AEAs closely resemble expert communication, combining authoritative evidence, neutral tone, and domain-appropriate reasoning. Vulnerability is highest on hard tasks, in fact-driven domains, and among participants who are less formally educated, younger, or highly trusting of AI. This is the first systematic security study that treats explanations as an adversarial cognitive channel and quantifies their impact on human trust in AI-assisted decision making.

</details>


### [10] [Axiomatic Foundations of Counterfactual Explanations](https://arxiv.org/abs/2602.04028)
*Leila Amgoud,Martin Cooper*

Main category: cs.AI

TL;DR: 本文建立反事实解释的公理框架，揭示五种根本不同的解释类型（含局部与全局），通过不可能性定理和表征定理系统分类现有方法并分析计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 自主智能系统的决策解释对提升信任至关重要，反事实解释能回答"为何不"类问题。但现有研究多聚焦单一局部解释类型，缺乏对多元反事实类型及全局解释的系统性探索。

Method: 提出基于理想性质的公理框架，证明不可能性定理阐明公理间的内在冲突，利用表征定理建立公理子集与解释器家族的精确对应，从而系统化分类反事实解释。

Result: 发现五种本质不同的反事实解释类型（部分局部、部分全局）；证明特定公理组合无法同时满足；建立公理与解释器的严格映射；将现有方法纳入该分类体系；分析生成各类解释的计算复杂度。

Conclusion: 该框架填补了反事实解释系统性研究的空白，为理解、选择和开发解释方法提供了理论基础，明确了不同解释类型的性质边界与计算代价。

Abstract: Explaining autonomous and intelligent systems is critical in order to improve trust in their decisions. Counterfactuals have emerged as one of the most compelling forms of explanation. They address ``why not'' questions by revealing how decisions could be altered. Despite the growing literature, most existing explainers focus on a single type of counterfactual and are restricted to local explanations, focusing on individual instances. There has been no systematic study of alternative counterfactual types, nor of global counterfactuals that shed light on a system's overall reasoning process.
  This paper addresses the two gaps by introducing an axiomatic framework built on a set of desirable properties for counterfactual explainers. It proves impossibility theorems showing that no single explainer can satisfy certain axiom combinations simultaneously, and fully characterizes all compatible sets. Representation theorems then establish five one-to-one correspondences between specific subsets of axioms and the families of explainers that satisfy them. Each family gives rise to a distinct type of counterfactual explanation, uncovering five fundamentally different types of counterfactuals. Some of these correspond to local explanations, while others capture global explanations. Finally, the framework situates existing explainers within this taxonomy, formally characterizes their behavior, and analyzes the computational complexity of generating such explanations.

</details>


### [11] [Scaling In-Context Online Learning Capability of LLMs via Cross-Episode Meta-RL](https://arxiv.org/abs/2602.04089)
*Xiaofeng Lin,Sirou Zhu,Yilei Chen,Mingyu Chen,Hejian Sang,Ioannis Paschalidis,Zhipeng Wang,Aldo Pacchiano,Xuezhou Zhang*

Main category: cs.AI

TL;DR: The paper introduces ORBIT, a meta-reinforcement learning framework that trains LLMs to learn from online interactions in-context. A small open-source model (Qwen3-14B) trained with ORBIT matches GPT-5.2 performance on unseen environments and significantly outperforms standard RL fine-tuning, with benefits scaling with model size.


<details>
  <summary>Details</summary>
Motivation: While LLMs excel at static tasks with all information upfront, they struggle with online decision-making where information must be acquired through interaction, feedback is delayed, and exploration-exploitation trade-offs are needed over time. Existing LLMs cannot reliably leverage in-context interaction experience.

Method: ORBIT: a multi-task, multi-episode meta-reinforcement learning framework that trains LLMs to learn from interaction in context. The framework meta-trains a relatively small open-source model (Qwen3-14B) to enable in-context online learning.

Result: After meta-training, the Qwen3-14B model demonstrates substantially improved in-context online learning on entirely unseen environments, matching GPT-5.2 performance and outperforming standard RL fine-tuning by a large margin. Scaling experiments show consistent gains with increasing model size.

Conclusion: The training approach successfully addresses LLMs' limitations in online learning settings, demonstrating significant headroom for learn-at-inference-time decision-making agents and suggesting that in-context learning from interaction can be effectively scaled.

Abstract: Large language models (LLMs) achieve strong performance when all task-relevant information is available upfront, as in static prediction and instruction-following problems. However, many real-world decision-making tasks are inherently online: crucial information must be acquired through interaction, feedback is delayed, and effective behavior requires balancing information collection and exploitation over time. While in-context learning enables adaptation without weight updates, existing LLMs often struggle to reliably leverage in-context interaction experience in such settings. In this work, we show that this limitation can be addressed through training. We introduce ORBIT, a multi-task, multi-episode meta-reinforcement learning framework that trains LLMs to learn from interaction in context. After meta-training, a relatively small open-source model (Qwen3-14B) demonstrates substantially improved in-context online learning on entirely unseen environments, matching the performance of GPT-5.2 and outperforming standard RL fine-tuning by a large margin. Scaling experiments further reveal consistent gains with model size, suggesting significant headroom for learn-at-inference-time decision-making agents. Code reproducing the results in the paper can be found at https://github.com/XiaofengLin7/ORBIT.

</details>


### [12] [Interfaze: The Future of AI is built on Task-Specific Small Models](https://arxiv.org/abs/2602.04101)
*Harsha Vardhan Khurdula,Vineet Agarwal,Yoeven D Khemlani*

Main category: cs.AI

TL;DR: Interfaze is a novel system architecture that treats LLM applications as context-building and action problems rather than monolithic model selection. It combines specialized small models for perception, a context-construction layer, and an action layer orchestrated by a thin controller that routes distilled context to a user-selected LLM, achieving strong benchmark performance while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Modern LLM applications are constrained by over-reliance on selecting single monolithic models, which is computationally expensive and inflexible. The authors argue for a paradigm shift toward building and acting over context through specialized, composable components instead of model-centric approaches.

Method: Interfaze employs a three-layer architecture: (i) a perception layer with heterogeneous DNNs and small language models for complex OCR (PDFs, charts, diagrams) and multilingual ASR, (ii) a context-construction layer that crawls, indexes, and parses external sources into compact structured state, and (iii) an action layer that can browse, retrieve, execute sandboxed code, and drive headless browsers. A thin controller orchestrates these components and exposes a single OpenAI-style endpoint, deciding which tools to run while forwarding distilled context to a user-selected LLM for final responses.

Result: Interfaze-Beta achieved 83.6% on MMLU-Pro, 91.4% on MMLU, 81.3% on GPQA-Diamond, 57.8% on LiveCodeBench v5, 90.0% on AIME-2025, 77.3% on MMMU (val), 91.5% on AI2D, 90.9% on ChartQA, and 90.8% on Common Voice v16. Critically, most queries are handled by the small-model and tool stack, with the large LLM operating only on distilled context, shifting the bulk of computation away from expensive monolithic models while maintaining competitive accuracy.

Conclusion: Decomposing LLM applications into specialized perception, context, and action components with intelligent context distillation enables high performance while significantly reducing computational costs by minimizing reliance on expensive monolithic models, offering a more efficient and scalable architecture for complex LLM applications.

Abstract: We present Interfaze, a system that treats modern LLM applications as a problem of building and acting over context, not just picking the right monolithic model. Instead of a single transformer, we combine (i) a stack of heterogeneous DNNs paired with small language models as perception modules for OCR involving complex PDFs, charts and diagrams, and multilingual ASR with (ii) a context-construction layer that crawls, indexes, and parses external sources (web pages, code, PDFs) into compact structured state, and (iii) an action layer that can browse, retrieve, execute code in a sandbox, and drive a headless browser for dynamic web pages. A thin controller sits on top of this stack and exposes a single, OpenAI-style endpoint: it decides which small models and actions to run and always forwards the distilled context to a user-selected LLM that produces the final response.
  On this architecture, Interfaze-Beta achieves 83.6% on MMLU-Pro, 91.4% on MMLU, 81.3% on GPQA-Diamond, 57.8% on LiveCodeBench v5, and 90.0% on AIME-2025, along with strong multimodal scores on MMMU (val) (77.3%), AI2D (91.5%), ChartQA (90.9%), and Common Voice v16 (90.8%). We show that most queries are handled primarily by the small-model and tool stack, with the large LLM operating only on distilled context, yielding competitive accuracy while shifting the bulk of computation away from the most expensive and monolithic models.

</details>


### [13] [OMG-Agent: Toward Robust Missing Modality Generation with Decoupled Coarse-to-Fine Agentic Workflows](https://arxiv.org/abs/2602.04144)
*Ruiting Dai,Zheyu Wang,Haoyu Yang,Yihan Liu,Chengzhi Wang,Zekun Zhang,Zishan Huang,Jiaman Cen,Lisi Mo*

Main category: cs.AI

TL;DR: The paper proposes OMG-Agent, a novel three-stage agentic framework that decouples semantic planning, evidence retrieval, and detail synthesis to address data incompleteness in multimodal systems, overcoming limitations of existing methods like hallucinations and retrieval rigidity through a coarse-to-fine workflow.


<details>
  <summary>Details</summary>
Motivation: Data incompleteness severely affects multimodal system reliability. Existing methods have bottlenecks: parametric/generative models cause hallucinations due to over-reliance on internal memory, retrieval-augmented frameworks suffer from rigidity, and fundamentally all end-to-end architectures are constrained by Semantic-Detail Entanglement - a structural conflict between logical reasoning and signal synthesis.

Method: The authors propose OMG-Agent, a dynamic coarse-to-fine Agentic Workflow with three synergistic stages: (1) an MLLM-driven Semantic Planner using Progressive Contextual Reasoning to create structured semantic plans, (2) a non-parametric Evidence Retriever that grounds semantics in external knowledge, and (3) a Retrieval-Injected Executor that uses retrieved evidence as flexible prompts to synthesize high-fidelity details.

Result: Extensive experiments show OMG-Agent consistently outperforms state-of-the-art methods and maintains robustness under extreme missingness, achieving a 2.6-point gain on CMU-MOSI at 70% missing rates.

Conclusion: The OMG-Agent framework successfully addresses data incompleteness challenges by decoupling semantic and detail processing through an agentic workflow, demonstrating superior performance and robustness compared to existing approaches, suggesting a promising paradigm shift from static mapping to dynamic agentic processing for multimodal systems.

Abstract: Data incompleteness severely impedes the reliability of multimodal systems. Existing reconstruction methods face distinct bottlenecks: conventional parametric/generative models are prone to hallucinations due to over-reliance on internal memory, while retrieval-augmented frameworks struggle with retrieval rigidity. Critically, these end-to-end architectures are fundamentally constrained by Semantic-Detail Entanglement -- a structural conflict between logical reasoning and signal synthesis that compromises fidelity. In this paper, we present \textbf{\underline{O}}mni-\textbf{\underline{M}}odality \textbf{\underline{G}}eneration Agent (\textbf{OMG-Agent}), a novel framework that shifts the paradigm from static mapping to a dynamic coarse-to-fine Agentic Workflow. By mimicking a \textit{deliberate-then-act} cognitive process, OMG-Agent explicitly decouples the task into three synergistic stages: (1) an MLLM-driven Semantic Planner that resolves input ambiguity via Progressive Contextual Reasoning, creating a deterministic structured semantic plan; (2) a non-parametric Evidence Retriever that grounds abstract semantics in external knowledge; and (3) a Retrieval-Injected Executor that utilizes retrieved evidence as flexible feature prompts to overcome rigidity and synthesize high-fidelity details. Extensive experiments on multiple benchmarks demonstrate that OMG-Agent consistently surpasses state-of-the-art methods, maintaining robustness under extreme missingness, e.g., a $2.6$-point gain on CMU-MOSI at $70$\% missing rates.

</details>


### [14] [Steering LLMs via Scalable Interactive Oversight](https://arxiv.org/abs/2602.04210)
*Enyu Zhou,Zhiheng Xi,Long Ma,Zhihao Zhang,Shihan Dou,Zhikai Lei,Guoteng Wang,Rui Zheng,Hang Yan,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.AI

TL;DR: 提出可扩展交互式监督框架，通过递归决策树分解复杂意图并放大人类监督，使非专家能指导大语言模型完成复杂任务，对齐效果提升54%，且可通过强化学习优化。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型自动化复杂任务，出现监督鸿沟：用户缺乏领域专业知识、难以明确表达意图、无法验证复杂输出，导致可扩展监督面临关键挑战。

Method: 提出可扩展交互式监督框架，将复杂意图分解为可管理的递归决策树，在每个节点获取低负担反馈，并递归聚合成精确的全局指导。

Result: 在网页开发任务中，非专家能生成专家级产品需求文档，对齐效果提升54%；该框架可仅通过在线用户反馈进行强化学习优化。

Conclusion: 为在AI规模扩展时保持人类控制提供了实用路径。

Abstract: As Large Language Models increasingly automate complex, long-horizon tasks such as \emph{vibe coding}, a supervision gap has emerged. While models excel at execution, users often struggle to guide them effectively due to insufficient domain expertise, the difficulty of articulating precise intent, and the inability to reliably validate complex outputs. It presents a critical challenge in scalable oversight: enabling humans to responsibly steer AI systems on tasks that surpass their own ability to specify or verify. To tackle this, we propose Scalable Interactive Oversight, a framework that decomposes complex intent into a recursive tree of manageable decisions to amplify human supervision. Rather than relying on open-ended prompting, our system elicits low-burden feedback at each node and recursively aggregates these signals into precise global guidance. Validated in web development task, our framework enables non-experts to produce expert-level Product Requirement Documents, achieving a 54\% improvement in alignment. Crucially, we demonstrate that this framework can be optimized via Reinforcement Learning using only online user feedback, offering a practical pathway for maintaining human control as AI scales.

</details>


### [15] [InterPReT: Interactive Policy Restructuring and Training Enable Effective Imitation Learning from Laypersons](https://arxiv.org/abs/2602.04213)
*Feiyu Gavin Zhu,Jean Oh,Reid Simmons*

Main category: cs.AI

TL;DR: This paper proposes InterPReT, an interactive imitation learning framework that enables laypeople to teach AI agents through natural instructions and demonstrations, showing improved robustness and usability over baselines.


<details>
  <summary>Details</summary>
Motivation: Existing imitation learning methods require large-scale expert demonstrations and technical monitoring, creating barriers for non-technical end-users who want to teach AI agents new skills.

Method: Interactive Policy Restructuring and Training (InterPReT) - a system that continuously updates policy structure and optimizes parameters based on user instructions and demonstrations, allowing interactive teaching, performance monitoring, and decision review.

Result: A user study (N=34) in a racing game domain demonstrated that InterPReT produces more robust policies while maintaining system usability compared to standard imitation learning when laypeople provide both demonstrations and training decisions.

Conclusion: The approach makes imitation learning accessible to non-technical users, enabling them to train reliable AI policies through natural interaction without machine learning expertise.

Abstract: Imitation learning has shown success in many tasks by learning from expert demonstrations. However, most existing work relies on large-scale demonstrations from technical professionals and close monitoring of the training process. These are challenging for a layperson when they want to teach the agent new skills. To lower the barrier of teaching AI agents, we propose Interactive Policy Restructuring and Training (InterPReT), which takes user instructions to continually update the policy structure and optimize its parameters to fit user demonstrations. This enables end-users to interactively give instructions and demonstrations, monitor the agent's performance, and review the agent's decision-making strategies. A user study (N=34) on teaching an AI agent to drive in a racing game confirms that our approach yields more robust policies without impairing system usability, compared to a generic imitation learning baseline, when a layperson is responsible for both giving demonstrations and determining when to stop. This shows that our method is more suitable for end-users without much technical background in machine learning to train a dependable policy

</details>


### [16] [Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning](https://arxiv.org/abs/2602.04284)
*Yansong Ning,Jun Fang,Naiqiang Tan,Hao Liu*

Main category: cs.AI

TL;DR: 提出Agent-Omit框架，通过冷启动数据微调和省略感知强化学习，使LLM智能体在多轮交互中自适应省略冗余思考与观察，实现效果-效率最佳权衡。


<details>
  <summary>Details</summary>
Motivation: 现有研究平等对待多轮交互中的所有轨迹，忽视了思考必要性和观察效用的跨轮次差异，导致智能体效率未达最优。

Method: 1) 量化分析思考观察对效果效率的影响；2) 合成单/多轮省略冷启动数据微调模型；3) 设计省略感知强化学习框架，包含双重采样机制和定制化省略奖励；4) 理论证明省略策略偏差受KL散度上界约束。

Result: 在五个智能体基准测试中，Agent-Omit-8B性能媲美七种前沿LLM智能体，且相比七种高效方法获得最优的效果-效率权衡。

Conclusion: 该框架成功赋能LLM智能体自适应省略能力，在保持性能的同时显著提升交互效率，为高效智能体设计提供新范式。

Abstract: Managing agent thought and observation during multi-turn agent-environment interactions is an emerging strategy to improve agent efficiency. However, existing studies treat the entire interaction trajectories equally, overlooking the thought necessity and observation utility varies across turns. To this end, we first conduct quantitative investigations into how thought and observation affect agent effectiveness and efficiency. Based on our findings, we propose Agent-Omit, a unified training framework that empowers LLM agents to adaptively omit redundant thoughts and observations. Specifically, we first synthesize a small amount of cold-start data, including both single-turn and multi-turn omission scenarios, to fine-tune the agent for omission behaviors. Furthermore, we introduce an omit-aware agentic reinforcement learning approach, incorporating a dual sampling mechanism and a tailored omission reward to incentivize the agent's adaptive omission capability. Theoretically, we prove that the deviation of our omission policy is upper-bounded by KL-divergence. Experimental results on five agent benchmarks show that our constructed Agent-Omit-8B could obtain performance comparable to seven frontier LLM agent, and achieve the best effectiveness-efficiency trade-off than seven efficient LLM agents methods. Our code and data are available at https://github.com/usail-hkust/Agent-Omit.

</details>


### [17] [Digital Twins & ZeroConf AI: Structuring Automated Intelligent Pipelines for Industrial Applications](https://arxiv.org/abs/2602.04385)
*Marco Picone,Fabio Turazza,Matteo Martinelli,Marco Mamei*

Main category: cs.AI

TL;DR: 本文提出一种基于数字孪生的零配置AI流水线框架，通过模块化设计实现工业信息物理系统中AI的无缝集成，并在微型工厂场景验证其支持多模型并发和动态数据处理的能力。


<details>
  <summary>Details</summary>
Motivation: 工业信息物理系统(CPS)的复杂性日益增加，物联网/工业互联网技术在通信协议、数据格式和设备能力方面的碎片化造成了物理层与智能功能之间的巨大鸿沟。现有方法孤立且紧耦合，限制了AI功能的可扩展性和复用性。

Method: 提出一种模块化和可互操作的解决方案，引入零配置(ZeroConf)AI流水线概念，由数字孪生协调数据管理和智能增强，解耦数字孪生与AI组件的角色，最小化配置需求。

Result: 在微型工厂场景中验证了该方法，展示了对并发机器学习模型和动态数据处理的支持。

Conclusion: 该方案有效加速了复杂工业环境中智能服务的部署。

Abstract: The increasing complexity of Cyber-Physical Systems (CPS), particularly in the industrial domain, has amplified the challenges associated with the effective integration of Artificial Intelligence (AI) and Machine Learning (ML) techniques. Fragmentation across IoT and IIoT technologies, manifested through diverse communication protocols, data formats and device capabilities, creates a substantial gap between low-level physical layers and high-level intelligent functionalities. Recently, Digital Twin (DT) technology has emerged as a promising solution, offering structured, interoperable and semantically rich digital representations of physical assets. Current approaches are often siloed and tightly coupled, limiting scalability and reuse of AI functionalities. This work proposes a modular and interoperable solution that enables seamless AI pipeline integration into CPS by minimizing configuration and decoupling the roles of DTs and AI components. We introduce the concept of Zero Configuration (ZeroConf) AI pipelines, where DTs orchestrate data management and intelligent augmentation. The approach is demonstrated in a MicroFactory scenario, showing support for concurrent ML models and dynamic data processing, effectively accelerating the deployment of intelligent services in complex industrial settings.

</details>


### [18] [ReThinker: Scientific Reasoning by Rethinking with Guided Reflection and Confidence Control](https://arxiv.org/abs/2602.04496)
*Zhentao Tang,Yuqi Cui,Shixiong Kai,Wenqian Zhao,Ke Ye,Xing Li,Anxin Tian,Zehua Pei,Hui-Ling Zhen,Shoubo Hu,Xiaoguang Li,Yunhe Wang,Mingxuan Yuan*

Main category: cs.AI

TL;DR: ReThinker是一个置信度感知的智能体框架，通过Solver-Critic-Selector架构动态分配计算资源，结合反向数据合成和自适应轨迹回收策略，显著提升大模型在专家级科学推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在专家级科学推理任务（如HLE基准）中存在工具流程僵化、多智能体协作脆弱、测试时扩展效率低等问题。

Method: 提出ReThinker框架，采用阶段式Solver-Critic-Selector架构实现动态计算分配；通过反向数据合成和自适应轨迹回收策略生成高质量监督数据。

Result: 在HLE、GAIA和XBench基准测试中持续超越现有最佳模型及深度研究系统，达到专家级推理任务的最先进水平。

Conclusion: ReThinker通过置信度感知的动态计算机制和高效训练策略，为复杂科学推理任务提供了更优解决方案。

Abstract: Expert-level scientific reasoning remains challenging for large language models, particularly on benchmarks such as Humanity's Last Exam (HLE), where rigid tool pipelines, brittle multi-agent coordination, and inefficient test-time scaling often limit performance. We introduce ReThinker, a confidence-aware agentic framework that orchestrates retrieval, tool use, and multi-agent reasoning through a stage-wise Solver-Critic-Selector architecture. Rather than following a fixed pipeline, ReThinker dynamically allocates computation based on model confidence, enabling adaptive tool invocation, guided multi-dimensional reflection, and robust confidence-weighted selection. To support scalable training without human annotation, we further propose a reverse data synthesis pipeline and an adaptive trajectory recycling strategy that transform successful reasoning traces into high-quality supervision. Experiments on HLE, GAIA, and XBench demonstrate that ReThinker consistently outperforms state-of-the-art foundation models with tools and existing deep research systems, achieving state-of-the-art results on expert-level reasoning tasks.

</details>


### [19] [From Competition to Collaboration: Designing Sustainable Mechanisms Between LLMs and Online Forums](https://arxiv.org/abs/2602.04572)
*Niv Fono,Yftah Ziser,Omer Ben-Porat*

Main category: cs.AI

TL;DR: 提出生成式AI与问答论坛的序贯互动框架，通过数据模拟揭示激励错位下仍可实现约50%理想效用的可持续协作潜力


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI系统既分流问答论坛用户又依赖论坛数据提升性能的悖论，探索人机知识共享的可持续协作模式

Method: 构建序贯互动理论框架，结合真实Stack Exchange数据和常用大语言模型进行数据驱动模拟，实证分析激励错位现象

Result: 证实存在显著激励错位，但协作方可实现接近理想全信息场景下50%左右的效用水平

Conclusion: 框架揭示了AI系统与人类知识平台通过非货币化协作实现可持续知识共享的可行性，为平衡双方利益提供新思路

Abstract: While Generative AI (GenAI) systems draw users away from (Q&A) forums, they also depend on the very data those forums produce to improve their performance. Addressing this paradox, we propose a framework of sequential interaction, in which a GenAI system proposes questions to a forum that can publish some of them. Our framework captures several intricacies of such a collaboration, including non-monetary exchanges, asymmetric information, and incentive misalignment. We bring the framework to life through comprehensive, data-driven simulations using real Stack Exchange data and commonly used LLMs. We demonstrate the incentive misalignment empirically, yet show that players can achieve roughly half of the utility in an ideal full-information scenario. Our results highlight the potential for sustainable collaboration that preserves effective knowledge sharing between AI systems and human knowledge platforms.

</details>


### [20] [WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.04634)
*Zelai Xu,Zhexuan Xu,Ruize Zhang,Chunyang Zhu,Shi Yu,Weilin Liu,Quanlu Zhang,Wenbo Ding,Chao Yu,Yu Wang*

Main category: cs.AI

TL;DR: 提出WideSeek-R1多智能体框架，通过宽度扩展实现并行信息搜索，4B模型性能媲美671B单智能体模型


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型聚焦深度扩展（单智能体多轮推理），但面对广泛任务时，组织能力成为瓶颈；传统多智能体系统依赖手工工作流和轮流交互，无法有效并行化工作

Method: 设计lead-agent-subagent框架，采用多智能体强化学习训练，共享LLM但上下文隔离，配备专用工具，在2万条广泛信息搜索任务上联合优化主智能体和并行子智能体

Result: WideSeek-R1-4B在WideSearch基准测试中达到40.0%的item F1分数，性能与单智能体DeepSeek-R1-671B相当；增加并行子智能体数量时性能持续提升

Conclusion: 宽度扩展多智能体系统能有效解决广泛信息搜索任务，突破组织能力瓶颈，为LLM扩展提供新维度

Abstract: Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.

</details>


### [21] [Agentic AI in Healthcare & Medicine: A Seven-Dimensional Taxonomy for Empirical Evaluation of LLM-based Agents](https://arxiv.org/abs/2602.04813)
*Shubham Vatsal,Harsh Dubey,Aditi Singh*

Main category: cs.AI

TL;DR: This systematic review of 49 LLM-based healthcare agent studies using a 7-dimensional taxonomy reveals significant capability gaps: external knowledge integration is common (76% fully implemented), but event-triggered activation (92% absent), drift detection (98% rare), and treatment planning (59% not implemented) remain underdeveloped, showing uneven field maturity.


<details>
  <summary>Details</summary>
Motivation: Existing literature on LLM healthcare agents lacks a unified framework, consisting of either broad surveys or narrow single-capability analyses, preventing a comprehensive understanding of the field's overall capability landscape.

Method: Researchers conducted a systematic review of 49 studies using a seven-dimensional taxonomy (Cognitive Capabilities, Knowledge Management, Interaction Patterns, Adaptation & Learning, Safety & Ethics, Framework Typology, Core Tasks & Subtasks) with 29 sub-dimensions, applying explicit inclusion/exclusion criteria and mapping each study using a three-level rubric (Fully/Partially/Not Implemented).

Result: Quantitative analysis revealed stark asymmetries: Knowledge Management's External Knowledge Integration is prevalent (76% fully implemented), while Interaction Patterns' Event-Triggered Activation (92% not implemented) and Adaptation & Learning's Drift Detection & Mitigation (98% not implemented) are rare. Architecturally, Multi-Agent Design dominates (82% fully implemented). Task-wise, information-centric capabilities lead, but action-oriented Treatment Planning & Prescription shows major gaps (59% not implemented).

Conclusion: The field of LLM-based healthcare agents demonstrates uneven development with strong information processing but weak autonomous action and adaptive capabilities, indicating a maturing but incomplete technology landscape requiring enhanced focus on safety, clinical workflow integration, and continuous learning mechanisms.

Abstract: Large Language Model (LLM)-based agents that plan, use tools and act has begun to shape healthcare and medicine. Reported studies demonstrate competence on various tasks ranging from EHR analysis and differential diagnosis to treatment planning and research workflows. Yet the literature largely consists of overviews which are either broad surveys or narrow dives into a single capability (e.g., memory, planning, reasoning), leaving healthcare work without a common frame. We address this by reviewing 49 studies using a seven-dimensional taxonomy: Cognitive Capabilities, Knowledge Management, Interaction Patterns, Adaptation & Learning, Safety & Ethics, Framework Typology and Core Tasks & Subtasks with 29 operational sub-dimensions. Using explicit inclusion and exclusion criteria and a labeling rubric (Fully Implemented, Partially Implemented, Not Implemented), we map each study to the taxonomy and report quantitative summaries of capability prevalence and co-occurrence patterns. Our empirical analysis surfaces clear asymmetries. For instance, the External Knowledge Integration sub-dimension under Knowledge Management is commonly realized (~76% Fully Implemented) whereas Event-Triggered Activation sub-dimenison under Interaction Patterns is largely absent (~92% Not Implemented) and Drift Detection & Mitigation sub-dimension under Adaptation & Learning is rare (~98% Not Implemented). Architecturally, Multi-Agent Design sub-dimension under Framework Typology is the dominant pattern (~82% Fully Implemented) while orchestration layers remain mostly partial. Across Core Tasks & Subtasks, information centric capabilities lead e.g., Medical Question Answering & Decision Support and Benchmarking & Simulation, while action and discovery oriented areas such as Treatment Planning & Prescription still show substantial gaps (~59% Not Implemented).

</details>


### [22] [Are AI Capabilities Increasing Exponentially? A Competing Hypothesis](https://arxiv.org/abs/2602.04836)
*Haosen Ge,Hamsa Bastani,Osbert Bastani*

Main category: cs.AI

TL;DR: 该研究反驳METR报告关于AI能力指数级增长的主张，通过重新分析数据发现逻辑曲线拐点已过，并提出分解模型（基础能力+推理能力）证明AI能力将在近期而非遥远的未来出现增长拐点，揭示了现有指数增长预测的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 针对METR报告声称自2019年以来AI能力呈指数级增长并产生重大现实影响，但该结论缺乏数据支撑，作者旨在挑战这一主流观点并揭示其预测模型的脆弱性。

Method: 重新拟合METR数据采用逻辑曲线模型，并构建更复杂的分解模型，将AI能力拆解为基础能力与推理能力两个独立改进速率的组成部分。

Result: 数据不支持指数增长；逻辑曲线显示拐点已过去；分解模型证实AI能力将在不久的将来而非遥远未来出现增长拐点。

Conclusion: 现有指数增长预测过于脆弱，AI能力发展轨迹更可能呈现S型曲线特征，拐点临近，这对安全研究和政策制定具有重要启示意义。

Abstract: Rapidly increasing AI capabilities have substantial real-world consequences, ranging from AI safety concerns to labor market consequences. The Model Evaluation & Threat Research (METR) report argues that AI capabilities have exhibited exponential growth since 2019. In this note, we argue that the data does not support exponential growth, even in shorter-term horizons. Whereas the METR study claims that fitting sigmoid/logistic curves results in inflection points far in the future, we fit a sigmoid curve to their current data and find that the inflection point has already passed. In addition, we propose a more complex model that decomposes AI capabilities into base and reasoning capabilities, exhibiting individual rates of improvement. We prove that this model supports our hypothesis that AI capabilities will exhibit an inflection point in the near future. Our goal is not to establish a rigorous forecast of our own, but to highlight the fragility of existing forecasts of exponential growth.

</details>


### [23] [Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing](https://arxiv.org/abs/2602.04837)
*Zhaotian Weng,Antonis Antoniades,Deepak Nathani,Zhen Zhang,Xiao Pu,Xin Eric Wang*

Main category: cs.AI

TL;DR: Group-Evolving Agents (GEA) is a new paradigm where groups of AI agents evolve together with shared experience, significantly outperforming existing self-evolving methods on coding benchmarks (71.0% vs 56.7% on SWE-bench Verified) by better utilizing exploratory diversity.


<details>
  <summary>Details</summary>
Motivation: Existing open-ended self-evolving agents using tree-structured evolution suffer from inefficient utilization of exploratory diversity due to isolated evolutionary branches, limiting autonomous capability advancement beyond human-designed architectures.

Method: Proposes Group-Evolving Agents (GEA) that treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout the evolutionary process.

Result: GEA achieves 71.0% on SWE-bench Verified (vs 56.7% baseline) and 88.3% on Polyglot (vs 68.3% baseline), matches top human-designed frameworks, fixes bugs in 1.4 iterations (vs 5), and shows better transferability and robustness.

Conclusion: GEA effectively converts early-stage exploratory diversity into sustained long-term progress, demonstrating superior performance, robustness, and transferability for open-ended self-improving agents in coding tasks.

Abstract: Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods.

</details>


### [24] [Fluid Representations in Reasoning Models](https://arxiv.org/abs/2602.04843)
*Dmitrii Kharlapenko,Alessandro Stolfo,Arthur Conmy,Mrinmaya Sachan,Zhijing Jin*

Main category: cs.AI

TL;DR: This paper investigates how QwQ-32B, a reasoning language model, processes abstract structural information. Through mechanistic analysis on Mystery Blocksworld, they discover that the model develops "Fluid Reasoning Representations" - gradually refined internal token representations that focus on structural patterns rather than specific semantics, which causally improves problem-solving performance.


<details>
  <summary>Details</summary>
Motivation: Reasoning language models that generate long chains of thought show dramatic performance improvements on abstract problems, but their internal mechanisms remain poorly understood. The paper aims to uncover how these models process abstract structural information.

Method: The researchers conduct a mechanistic analysis of QwQ-32B on Mystery Blocksworld, a semantically obfuscated planning domain. They use steering experiments to establish causal relationships between internal representations and model performance.

Result: QwQ-32B gradually improves its internal representation of actions and concepts during reasoning, developing abstract encodings that focus on structure over specific action names. Steering experiments show that injecting refined representations from successful traces boosts accuracy, and symbolic representations can replace many obfuscated encodings with minimal performance loss.

Conclusion: The key factor driving reasoning model performance is in-context refinement of token representations, termed "Fluid Reasoning Representations."

Abstract: Reasoning language models, which generate long chains of thought, dramatically outperform non-reasoning language models on abstract problems. However, the internal model mechanisms that allow this superior performance remain poorly understood. We present a mechanistic analysis of how QwQ-32B - a model specifically trained to produce extensive reasoning traces - process abstract structural information. On Mystery Blocksworld - a semantically obfuscated planning domain - we find that QwQ-32B gradually improves its internal representation of actions and concepts during reasoning. The model develops abstract encodings that focus on structure rather than specific action names. Through steering experiments, we establish causal evidence that these adaptations improve problem solving: injecting refined representations from successful traces boosts accuracy, while symbolic representations can replace many obfuscated encodings with minimal performance loss. We find that one of the factors driving reasoning model performance is in-context refinement of token representations, which we dub Fluid Reasoning Representations.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [25] [A phenomenological description of critical slowing down at period-doubling bifurcations](https://arxiv.org/abs/2602.04091)
*Edson D. Leonel,João P. C. Ferreira,Diego F. M. Oliveira*

Main category: nlin.CD

TL;DR: This paper extends the theory of critical slowing down from 1D to 2D period-doubling bifurcations, identifying universal critical exponents and validating predictions with numerical models.


<details>
  <summary>Details</summary>
Motivation: To generalize the established phenomenology of critical slowing down (previously limited to one-dimensional maps) to two-dimensional dynamical systems, capturing universal scaling behavior near period-doubling bifurcations.

Method: Derives a reduced description using local Taylor expansion around fixed points and bifurcation parameters. Projects 2D dynamics onto a center manifold to reduce the system to a 1D universal normal form. Validates predictions numerically using Hénon and Ikeda maps.

Result: Identifies four universal critical exponents: three at the bifurcation point (governing short-time behavior, asymptotic decay, and crossover) and one away from criticality (governing relaxation time). Demonstrates that 2D period-doubling bifurcations exhibit identical universal scaling laws as 1D systems. Numerical results show excellent agreement with all theoretical scaling laws.

Conclusion: The phenomenology of critical slowing down for period-doubling bifurcations is universally applicable to both one- and two-dimensional dynamical systems. The center manifold reduction confirms the structural equivalence of 2D bifurcations to the 1D universal description, with critical exponents providing a complete characterization of the dynamics near criticality.

Abstract: We present a phenomenological description of the critical slowing down associated with period-doubling bifurcations in discrete dynamical systems. Starting from a local Taylor expansion around the fixed point and the bifurcation parameter, we derive a reduced description that captures the convergence towards stationary state both at and near criticality. At the bifurcation point, three universal critical exponents are obtained, characterising the short-time behaviour, the asymptotic decay, and the crossover between these regimes. Away from criticality, a fourth exponent governing the relaxation time is identified. We show this phenomenology, well established for one-dimensional maps, extends naturally to two-dimensional mappings. By projecting the dynamics onto the centre manifold, we demonstrate that the local normal form of a two-dimensional period-doubling bifurcation reduces to the same universal structure found in one dimension. The theoretical predictions are validated numerically using the Hénon and Ikeda maps, showing excellent agreement for all scaling laws and critical exponents.

</details>


### [26] [Semiclassical Structure of the Advection--Diffusion Spectrum in Mixed Phase Spaces](https://arxiv.org/abs/2602.04730)
*Christopher Amey,Bala Sundaram,Andrew C. Poje*

Main category: nlin.CD

TL;DR: 研究大Peclet数下混合相空间二维对流-扩散算子的谱结构，发现三种特征模式族（对流、扩散、隧穿模式），揭示即使在高Peclet数下，有限时间动力学仍由模式竞争而非单一模式主导。


<details>
  <summary>Details</summary>
Motivation: 理解混合相空间流动（同时包含规则和混沌区域）中物质输运的谱特性，特别是在大Peclet数弱扩散极限下，规则区域与混沌区域的不同混合机制如何共同影响整体扩散行为。

Method: 采用傅里叶离散化结合对称性约化和Krylov-Arnoldi方法，在弱扩散渐近 regime 中可靠计算约一百个主导特征对。

Result: 主特征值呈扩散性并局域于最大规则区域；谱结构由拉格朗日相空间几何控制；识别出三类特征模式（对流/扩散/隧穿模式）；混沌区指数混合快速抑制关联，可积区代数混合产生长寿命相干结构；不存在统一的谱隙控制。

Conclusion: 有限时间对流-扩散动力学受持续的模式竞争而非单模主导，即使Peclet数渐近增大；相空间结构决定了谱的标度和排序。

Abstract: We examine the spectral structure of the two-dimensional advection-diffusion operator in flows with mixed phase space at very large Peclet number. Using Fourier discretization combined with symmetry reduction and Krylov-Arnoldi methods, we compute on the order of one hundred leading eigenpairs reliably in the asymptotic, weak-diffusion regime. While the principal eigenvalue is asymptotically diffusive and localized on the largest regular region, the broader spectrum exhibits a rich organization controlled by local Lagrangian phase-space geometry. In particular, exponential mixing in chaotic regions rapidly suppresses correlations, whereas algebraic mixing in integrable regions generates long-lived coherent structures that dominate the slow and intermediate parts of the spectrum. We identify three distinct classes of eigenmodes: advective modes associated with transport on invariant tori, diffusive modes and, within the duffusive branch, tunneling modes arising from weak coupling between dynamically separated regular regions. Drawing on a semiclassical analogy, we assign quantum-number-like labels to these families and predict the appearance, scaling, and ordering of their sub-spectra directly from the Hamiltonian phase-space structure. The coexistence of these families implies that no uniform control of the spectral gap exists across the full spectrum: although the slowest mode is diffusive, arbitrarily small gaps arise between competing families at higher mode numbers. As a result, finite-time advection-diffusion dynamics is generically governed by persistent modal competition rather than single-mode dominance, even at asymptotically large Peclet number.

</details>


### [27] [Correspondence between classical and quantum resonances](https://arxiv.org/abs/2602.04793)
*F. J. Arranz,F. Borondo*

Main category: nlin.CD

TL;DR: 研究分子非线性系统中经典分岔现象的量子对应，以CN-Li异构化系统为例，通过能级-普朗克常数关联图揭示量子共振系列与经典共振的对应关系


<details>
  <summary>Details</summary>
Motivation: 探索经典非线性系统中能量增加导致的分岔现象在量子体系中的表现，建立经典共振与量子共振之间的对应关系

Method: 采用能级-普朗克常数关联图分析，识别避免交叉系列；发展半经典理论进行解析推导；以CN-Li⇆Li-CN异构化系统为具体研究对象

Result: 发现一系列避免交叉结构对应量子共振系列；外推至ℏ=0时，经典分岔能量与量子共振能量在半经典极限下精确对应

Conclusion: 量子共振系列是经典共振的量子表现；半经典理论成功建立了经典分岔与量子能级结构的定量联系，验证了量子-经典对应原理

Abstract: Bifurcations take place in molecular Hamiltonian nonlinear systems as the excitation energy increases, this leading to the appearance of different classical resonances. In this paper, we study the quantum manifestations of these classical resonances in the isomerizing system CN-Li$\leftrightarrows$Li-CN. By using a correlation diagram of eigenenergies versus Planck constant, we show the existence of different series of avoided crossings, leading to the corresponding series of quantum resonances, which represent the quantum manifestations of the classical resonances. Moreover, the extrapolation of these series to $\hbar=0$ unveils the correspondence between the bifurcation energy of classical resonances and the energy of the series of quantum resonances in the semiclassical limit $\hbar\to0$. Additionally, in order to obtain analytical expressions for our results, a semiclassical theory is developed.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [28] [Detailed, interpretable characterization of mid-circuit measurement on a transmon qubit](https://arxiv.org/abs/2602.03938)
*Piper C. Wysocki,Luke D. Burkhart,Madeline H. Morocco,Corey I. Ostrove,Riley J. Murray,Tristan Brown,Jeffrey M. Gertler,David K. Kim,Nathan E. Miller,Bethany M. Niedzielski,Katrina M. Sliwa,Robin Blume-Kohout,Gabriel O. Samach,Mollie E. Schwartz,Kenneth M. Rudinger*

Main category: quant-ph

TL;DR: 将量子门误差分析中的误差生成器形式体系创新性地应用于中程测量(MCMs)，成功解构并量化了超导量子比特中的物理错误机制，验证了色散读取理论特征，并建立了简约参数模型。


<details>
  <summary>Details</summary>
Motivation: 实验测得的量子仪器（描述中程测量的量子过程）难以直接关联到设备物理机制，阻碍了对量子纠错关键组件的深入理解和优化。

Method: 将原本用于解释噪声量子门的误差生成器形式体系进行适配，应用于分析中程测量过程，通过分解误差过程为"基本误差"的物理可解释求和来表征设备。

Result: 在transmon量子比特设备上成功分离并量化了振幅阻尼、读取错误和不完全坍缩等核心误差机制；揭示了这些误差随读取脉冲振幅的变化规律；通过极简参数模型重现了色散读取理论预测的关键特征。

Conclusion: 该方法实现了中程测量误差的物理可解释性分析，为理解量子测量过程的噪声根源提供了新工具，验证了理论模型的有效性，并建立了可用于实际优化的简约化描述框架。

Abstract: Mid-circuit measurements (MCMs) are critical components of the quantum error correction protocols expected to enable utility-scale quantum computing. MCMs can be modeled by quantum instruments (a type of quantum operation or process), which can be characterized self-consistently using gate set tomography. However, experimentally estimated quantum instruments are often hard to interpret or relate to device physics. We address this challenge by adapting the error generator formalism -- previously used to interpret noisy quantum gates by decomposing their error processes into physically meaningful sums of "elementary errors" -- to MCMs. We deploy our new analysis on a transmon qubit device to tease out and quantify error mechanisms including amplitude damping, readout error, and imperfect collapse. We examine in detail how the magnitudes of these errors vary with the readout pulse amplitude, recover the key features of dispersive readout predicted by theory, and show that these features can be modeled parsimoniously using a reduced model with just a few parameters.

</details>


### [29] [Correlation-Enabled Beatings in Two-Dimensional Electronic Spectroscopy](https://arxiv.org/abs/2602.04061)
*Sirui Chen,Dragomir Davidović*

Main category: quant-ph

TL;DR: 该论文揭示二维电子光谱中的长寿命拍频源于关联驱动机制，涉及慢热浴记忆和超快脉冲序列，将其重新诠释为协议层面的动力学效应，而非简单的激子-振动振荡问题。


<details>
  <summary>Details</summary>
Motivation: 标准激子开放系统模型因假设因子化初始化和快速相干衰减，无法解释二维电子光谱中的长寿命拍频现象，因此需要超越传统模型的新机制来解释这种持续相干性。

Method: 通过理论分析提出关联驱动机制，该机制需要慢热浴记忆和超快脉冲序列在光学相互作用中传播系统-热浴关联，实现协议层面的相干调控。

Result: 研究表明超快脉冲序列可幺正地修饰热浴记忆贡献，并在无场演化期间激活非世俗项的布居-相干转移，使相干信号持续时间远超因子化或弱记忆描述。

Conclusion: 长寿命拍频应重新定义为超快控制下的关联介导检索这一协议层面动力学效应，而非纠缠于激子与振动的本质或量子-经典语义之争。

Abstract: Long-lived beatings in two-dimensional electronic spectroscopy (2DES) remain difficult to interpret within standard excitonic open-system models, which typically assume factorized initialization and predict rapid coherence decay. We show that persistent beatings can arise from a correlation-driven mechanism that requires both slow bath memory and ultrafast pulse sequences that propagate system-bath correlations across optical interactions. In this regime, the pulse sequence unitarily dresses the bath-memory contribution and activates nonsecular population-coherence transfer during field-free evolution, sustaining coherence signatures far beyond factorized or weak-memory descriptions. Rather than addressing what is oscillating (excitonic versus vibronic) or quantum-versus-classical semantics, this work reframes long-lived beatings as a protocol-level dynamical effect: correlation-mediated retrieval under ultrafast control.

</details>


### [30] [Data Verification is the Future of Quantum Computing Copilots](https://arxiv.org/abs/2602.04072)
*Junhao Song,Ziqian Bi,Xinliang Chia,William Knottenbelt,Yudong Cao*

Main category: quant-ph

TL;DR: This paper argues that quantum program generation requires verification-centric AI architectures because LLM hallucinations are mathematically inevitable, proposing that verification should be an architectural foundation rather than an afterthought.


<details>
  <summary>Details</summary>
Motivation: Quantum program generation demands extreme precision that conflicts with the statistical nature of LLMs, where hallucinations are mathematically inevitable and cannot be solved by scaling. There's a critical need for AI systems that produce correct-by-construction outputs in constraint-governed domains like quantum computing.

Method: The paper presents a position-based argument structured around three key claims: (1) verified training data enables models to internalize precise constraints as learned structures, (2) verification must constrain generation rather than filter outputs due to exponentially shrinking valid design spaces, and (3) physical laws require verification as architectural primitives. Early experimental evidence with LLMs on circuit optimization is referenced.

Result: Early experiments demonstrated that LLMs without data verification achieved only a maximum accuracy of 79% in circuit optimization tasks, revealing fundamental limitations of pure statistical approaches for constraint-dominated domains.

Conclusion: The authors conclude that quantum computing and AI4Research communities must elevate verification from an afterthought to an architectural foundation, making it a core design principle for developing reliable AI systems in domains governed by physical and mathematical constraints.

Abstract: Quantum program generation demands a level of precision that may not be compatible with the statistical reasoning carried out in the inference of large language models (LLMs). Hallucinations are mathematically inevitable and not addressable by scaling, which leads to infeasible solutions. We argue that architectures prioritizing verification are necessary for quantum copilots and AI automation in domains governed by constraints. Our position rests on three key points: verified training data enables models to internalize precise constraints as learned structures rather than statistical approximations; verification must constrain generation rather than filter outputs, as valid designs occupy exponentially shrinking subspaces; and domains where physical laws impose correctness criteria require verification embedded as architectural primitives. Early experiments showed LLMs without data verification could only achieve a maximum accuracy of 79% in circuit optimization. Our positions are formulated as quantum computing and AI4Research community imperatives, calling for elevating verification from afterthought to architectural foundation in AI4Research.

</details>


### [31] [Influence of Noninertial Dynamics on Static Quantum Resource Theories](https://arxiv.org/abs/2602.04199)
*Saveetha Harikrishnan,Tim Byrnes,Chandrashekar Radhakrishnan*

Main category: quant-ph

TL;DR: 本文研究非惯性动力学对静态量子资源理论的影响，通过将非惯性效应等价于完全正定保迹（CPTP）映射，揭示Unruh效应对应于玻色放大器通道，并系统分析该映射对资源理论三大核心要素（自由态、自由操作和资源度量）的影响。


<details>
  <summary>Details</summary>
Motivation: 探索相对论性非惯性参考系对量子信息处理和量子资源的影响，这对发展相对论量子计算、加速系量子通信以及理解弯曲时空中的量子力学基础具有重要意义。

Method: 理论分析方法：首先建立非惯性效应与CPTP映射的等价关系，将Unruh效应建模为玻色放大器通道，进而研究该通道对量子资源理论三大核心组成部分的系统性影响。

Result: 建立了非惯性运动下量子资源理论的分析框架，证明可以对自由态、自由操作和资源度量在存在非惯性运动时的行为做出若干普适性论断。

Conclusion: 非惯性效应可通过CPTP映射在量子资源理论框架下进行系统研究，为理解相对论性量子信息处理提供了新视角，并揭示了加速运动对量子资源的普遍影响规律。

Abstract: The effect of noninertial dynamics on static quantum resource theories is investigated. To this end, we first show the equivalence between noninertial effects and a completely positive, trace-preserving (CPTP) map. In this formulation, the Unruh effect is equivalent to a bosonic amplifier channel. The effect of this map on a generic quantum resource is investigated by studying the role of the CPTP map on the three core ingredients of a resource theory, namely, the free states, the free operations and the resource quantifiers. We show several general statements can be made about these three components of a resource theory in the presence of noninertial motion.

</details>


### [32] [Constructing Compact ADAPT Unitary Coupled-Cluster Ansatz with Parameter-Based Criterion](https://arxiv.org/abs/2602.04253)
*Runhong He,Xin Hong,Qiaozhen Chai,Ji Guan,Junyuan Zhou,Arapat Ablimit,Guolong Cui,Shenggang Ying*

Main category: quant-ph

TL;DR: Param-ADAPT-VQE is a novel improvement over ADAPT-VQE that uses parameter-based operator selection, sub-Hamiltonian technique, and hot-start optimization to reduce redundancy and measurement costs while improving accuracy and scalability for molecular quantum chemistry calculations.


<details>
  <summary>Details</summary>
Motivation: The original ADAPT-VQE algorithm, while promising for molecular ground state energy calculation, suffers from redundant excitation operators and excessive measurement costs that limit its practical scalability.

Method: The authors propose Param-ADAPT-VQE which selects excitation operators using a parameter-based criterion instead of gradient-based metrics to avoid redundancy. They also develop a sub-Hamiltonian technique and integrate a hot-start VQE optimization strategy to reduce measurement costs.

Result: Numerical experiments demonstrate that Param-ADAPT-VQE outperforms the original algorithm in computational accuracy, ansatz size, and measurement costs. The method retains ADAPT-VQE's fundamental framework and remains compatible with its various modified versions.

Conclusion: This work provides an efficient and scalable enhancement to ADAPT-VQE, addressing the core obstacles that prevent its practical implementation in molecular quantum chemistry.

Abstract: The adaptive derivative-assembled pseudo-trotter variational quantum eigensolver (ADAPT-VQE) is a promising hybrid quantum-classical algorithm for molecular ground state energy calculation, yet its practical scalability is hampered by redundant excitation operators and excessive measurement costs. To address these challenges, we propose Param-ADAPT-VQE, a novel improved algorithm that selects excitation operators based on a parameter-based criterion instead of the traditional gradient-based metric. This strategy effectively eludes redundant operators. We further develop a sub-Hamiltonian technique and integrate a hot-start VQE optimization strategy, achieving a significant reduction in measurement costs. Numerical experiments on typical molecular systems demonstrate that Param-ADAPT-VQE outperforms the original ADAPT-VQE in computational accuracy, ansatz size, and measurement costs. Furthermore, our scheme retains the fundamental framework of ADAPT-VQE and is thus fully compatible with its various modified versions, enabling further performance improvements in specific aspects. This work presents an efficient and scalable enhancement to ADAPT-VQE, mitigating the core obstacles that impede its practical implementation in the field of molecular quantum chemistry.

</details>


### [33] [Canonical Quantization of Cylindrical Waveguides: A Gauge-Based Approach](https://arxiv.org/abs/2602.04295)
*Alexandre Delattre,Eddy Collin*

Main category: quant-ph

TL;DR: This paper develops a unified quantum formalism for electromagnetic modes in cylindrical waveguides by extending gauge-based quantization from Cartesian geometries, introducing field quadratures for TEM/TM/TE modes, deriving a Klein-Gordon type equation for generalized flux, and constructing bosonic operators to define measurable quantities for quantum technology applications.


<details>
  <summary>Details</summary>
Motivation: To extend a previously developed gauge-based quantization formalism from Cartesian to cylindrical waveguide geometries, unifying the treatment of TEM, TM, and TE modes under a consistent quantum framework for enhanced theoretical insight and experimental relevance in quantum technologies.

Method: Canonical quantization of electromagnetic modes using field quadratures (X, Y) for TEM/TM/TE traveling modes; derivation of a generalized flux (φ) governed by a Klein-Gordon equation from Maxwell's equations via proper gauge choice; explicit construction of Hamiltonian and bosonic ladder operators; calculation of mode-specific capacitance/inductance from field profiles.

Result: A unified quantum framework for cylindrical waveguides that: (1) identifies a characteristic scalar field (generalized flux φ) for each mode type, (2) derives the Hamiltonian and ladder operators, (3) expresses voltage/current in canonical variables, and (4) properly defines measurable quantities—especially for non-trivial TM/TE modes.

Conclusion: The formalism successfully unifies cylindrical and Cartesian guided mode quantization, providing a consistent theoretical foundation for quantum electromagnetic analysis in waveguides, with direct applicability to quantum technologies and planned extension to on-chip coplanar geometries.

Abstract: We present a canonical quantization of electromagnetic modes in cylindrical waveguides, extending a gauge-based formalism previously developed for Cartesian geometries [1]. By introducing the two field quadratures $X,Y$ of TEM (transverse electric-magnetic), but also of TM (transverse magnetic) and TE (transverse electric) traveling modes, we identify for each a characteristic one-dimensional scalar field (a generalized flux $\varphi$) governed by a Klein-Gordon type equation. The associated Hamiltonian is derived explicitly from Maxwell's equations, allowing the construction of bosonic ladder operators. The generalized flux is directly deduced from the electromagnetic potentials $A,V$ by a proper gauge choice, generalizing Devoret's approach [2]. Our analysis unifies the treatment of cylindrical and Cartesian guided modes under a consistent and generic framework, ensuring both theoretical insight and experimental relevance. We derive mode-specific capacitance and inductance from the field profiles and express voltage and current in terms of the canonical field variables. Measurable quantities are therefore properly defined from the mode quantum operators, especially for the non-trivial TM and TE ones. The formalism shall extend in future works to any other type of waveguides, especially on-chip coplanar geometries particularly relevant to quantum technologies.

</details>


### [34] [Does the entropy of systems with larger internal entanglement grow stronger?](https://arxiv.org/abs/2602.04345)
*Daria Gaidukevich*

Main category: quant-ph

TL;DR: 该研究探讨系统内部纠缠度是否影响其与环境的相互作用导致的熵增，通过量子比特-谐振子模型发现：平均情况下高纠缠系统熵增更强，但特定条件态下该关系会反转，且纠缠深度贡献较小。


<details>
  <summary>Details</summary>
Motivation: 探究系统初始内部纠缠度与其在环境相互作用中熵增强度的关系，这一基础问题在量子信息热力学中尚未明确。

Method: 采用最简单的量子模型：量子比特系统与环境（量子谐振子）相互作用，通过模拟随机量子态集合进行统计分析，并考察特定条件态的特殊行为。

Result: 平均情况下（随机态集合），内部纠缠度越高的系统熵增越显著；但特定选择的条件态可使该依赖关系反转；此外纠缠深度对熵增有微弱贡献。

Conclusion: 系统内部纠缠度与熵增的关系并非绝对单调，平均趋势为正相关但存在条件依赖性，这对理解量子开放系统热力学行为提供了新视角。

Abstract: It is known that when a system interacts with its environment, the entanglement contained in the system is redistributed since parts of the system entangle with the environment. On the other hand, the entanglement of a system with its environment is closely related to the entropy of the system. However, does this imply that the entropy of systems with larger internal entanglement will grow stronger? We study the issue using the simplest model as an example: a system of qubits interacts with the environment described by the quantum harmonic oscillator. The answer to the posed question is ambiguous. However, the study of the situation on average (using the simulation of a set of random states) reveals certain patterns and we can say that the answer is affirmative. At the same time, the choice of states satisfying certain conditions in some cases can change the dependence to the opposite. Additionally, we show that the entanglement depth also makes a small contribution to entropy growth.

</details>


### [35] [Vistas of Algebraic Probability: Quantum Computation and Information](https://arxiv.org/abs/2602.04351)
*Antonio Falcó,Hermann G. Matthies*

Main category: quant-ph

TL;DR: This paper advocates for an algebraic probability framework using algebras of random variables with states, generalizing Kolmogorov's measure-theoretic approach to handle quantum-like phenomena, with focus on finite-dimensional algebras for tractability.


<details>
  <summary>Details</summary>
Motivation: Kolmogorov's classical probability framework is inadequate for quantum effects and quantum-like situations; the algebraic framework, while powerful and used in quantum physics, remains underutilized in classical probability despite offering new perspectives.

Method: Restricting to finite-dimensional algebras to avoid analytical subtleties while preserving core concepts, classical limits, and applicability to quantum-like models and quantum computation.

Result: The algebraic framework unifies classical and quantum-like behaviors, with commutativity as the key distinguishing feature; connects to early probability concepts and is relevant for emerging quantum computing applications.

Conclusion: The algebraic approach provides a purely probabilistic generalization of classical probability, offering a tractable finite-dimensional setting for quantum-like phenomena with practical applications in computational science.

Abstract: Kolmogorov's foundation of probability takes measure spaces, $σ$-algebras, and probability measures as basic objects. It is, however, widely recognized that this classical framework is inadequate for random phenomena involving quantum effects, and more generally for \emph{quantum-like} situations. A broader formulation is provided by an algebraic viewpoint: one starts from an algebra of random variables equipped with a distinguished linear functional -- the \emph{state} -- interpreted as expectation. In this sense, the approach can also be viewed as a modern reading of ideas already implicit in early probability (e.g., the Bernoullis), while its contemporary form has been developed and used extensively in quantum physics.
  The algebraic framework accommodates both classical and quantum-like behaviours, yet it remains underused in classical probability and uncertainty quantification, where it can nevertheless open new perspectives and clarify structural features. Although the language carries a physics flavor, the subject is purely probabilistic. The key distinction between classical and quantum-like behaviour is \emph{commutativity}: its failure produces the characteristic effects of quantum-like situations. The rise of quantum computing is a prominent setting in which such behaviour may become relevant even for practitioners in computational science. Here we focus on the purely algebraic core of the approach. By restricting attention to finite-dimensional algebras, we avoid many analytical subtleties while retaining the main ideas, their classical limit, and their applicability to quantum-like models and quantum computation.

</details>


### [36] [Limitations of an approximative phase-space description in strong-field quantum optics](https://arxiv.org/abs/2602.04370)
*Rasmus Vesterager Gothelf,Lars Bojer Madsen,Christian Saugbjerg Lange*

Main category: quant-ph

TL;DR: 该研究评估了用相干态近似相空间描述非经典光场在强场过程（如高次谐波生成）中的准确性。发现该方法会错误地将驱动激光表征为经典态的非相干混合，导致无法捕捉亚泊松光子统计和低于真空起伏的压缩态。在一维带模型的精确解对比下，近似描述仅产生与脉冲持续时间和发射体密度相关的小量定量误差，但仍需定量误差分析以避免误解量子光学观测量。


<details>
  <summary>Details</summary>
Motivation: 强场过程（如高次谐波生成和阈上电离）在非经典光场驱动下的研究日益增多，但现有理论常用相干态近似相空间展开，其对量子光学观测量的准确性尚未得到充分验证。

Method: 引入近似相空间描述，并与一维带模型的精确解析解进行比较，评估其对量子光学观测量的预测精度。

Result: 近似相空间描述将非经典驱动场误表征为经典态的非相干混合，导致无法预测亚泊松光子统计和低于真空起伏的压缩态。使用该描述时，发射光的正交方差存在小量定量误差，误差随脉冲持续时间和发射体密度缩放。

Conclusion: 使用近似相空间描述可能错误地表征量子光学观测量，因此在解释物理意义时必须进行定量误差分析。

Abstract: In recent years, strong-field processes such as high-order harmonic generation (HHG) and above-threshold ionization driven by nonclassical states of light have become an increasingly popular field of study. The theoretical modeling of these processes often applies an approximate phase-space expansion of the nonclassical driving field in terms of coherent states, which has been shown to accurately predict the harmonic spectrum. However, its accuracy for the computation of quantum optical observables like the degree of squeezing and photon statistics has not been thoroughly considered. In this work, we introduce this approximative phase-space description and discuss its accuracy, and we find that it mischaracterizes the quantum optical properties of the driving laser by making it an incoherent mixture of classical states. We further show that this error in the driving field description maps onto the light emitted from HHG, as neither sub-Poissonian photon statistics nor quadrature squeezing below vacuum fluctuations can be captured by the approximative phase-space description. Lastly, to benchmark the approximative phase-space description, we consider the quantum HHG from a one-band model, which yields an exact analytical solution. Using the approximative phase-space representation with this specific model, we find a small quantitative error in the quadrature variance of the emitted field that scales with pulse duration and emitter density. Our results show that using this approximative phase-space description can mischaracterize quantum optical observables. Attributing physical meaning to such results should therefore be accompanied by a quantitative analysis of the error.

</details>


### [37] [Squeezing Enhanced Sagnac Sensing based on SU(1,1) Quantum Interference](https://arxiv.org/abs/2602.04394)
*Michal Natan,Saar Levin,Avi Pe'er*

Main category: quant-ph

TL;DR: 提出一种基于SU(1,1)干涉的压缩增强型Sagnac干涉仪设计方案，通过在环路中放置光参量放大器实现双向自动压缩，在损耗和探测器效率限制下仍可实现超越经典极限的旋转传感灵敏度。


<details>
  <summary>Details</summary>
Motivation: 传统Sagnac干涉仪在旋转传感中受散粒噪声极限(SNL)约束，为实现更高灵敏度，需结合量子压缩态与SU(1,1)干涉原理突破经典限制。

Method: 在Sagnac环路内部战略性放置光参量放大器(OPA)，使光束在环路正向和反向传播时自动产生压缩；比较两种探测方案：直接强度探测（需高效率探测器）和参量零差探测（技术复杂但探测器无效率限制）。

Result: 在大多数实际损耗和探测器非理想条件下，该设计仍能实现超经典灵敏度，有效利用压缩资源并兼容标准Sagnac构型。

Conclusion: 该方案成功将量子压缩与SU(1,1)干涉结合，为高精度旋转传感提供了实用且稳健的解决方案，具有实际应用潜力。

Abstract: We present a simple and robust design for a squeezing-enhanced Sagnac interferometer that employs the concept of SU(1,1) interference to significantly surpass the classical sensitivity limit (shot-noise limit - SNL) in rotational sensing. By strategically placing an optical parametric amplifier (OPA) inside the Sagnac loop, light is automatically squeezed in both forward and backward directions of the loop, which enhances the detectability of a small phase. For measuring the squeezed quadrature, we explore two approaches: Direct detection of the output intensity, which is simple, but requires a high-efficiency photo-detector; and parametric homodyne with an additional OPA, which accepts practical detectors with no efficiency limitation, but is technically more complex. Our analysis demonstrates super-classical sensitivity under most realistic conditions of loss and detector inefficiency, thereby leveraging the resources of squeezing and the principles of SU(1,1) interference, while maintaining compatibility with standard Sagnac configurations.

</details>


### [38] [Qudit Twisted-Torus Codes in the Bivariate Bicycle Framework](https://arxiv.org/abs/2602.04443)
*Mourad Halla*

Main category: quant-ph

TL;DR: 该论文研究二维环面上具有扭曲边界的平移不变CSS构造的有限长度qudit量子LDPC码，发现扭曲qudit构造相比未扭曲版本和之前的扭曲qubit实例能实现更大的码距。


<details>
  <summary>Details</summary>
Motivation: 基于近期关于扭曲边界条件改善qubit码性能的研究，本文将这一思路推广到有限域上的qudit码，旨在探索扭曲环面构造在qudit系统中的性能，并寻找具有优越率-距离权衡的紧凑码。

Method: 采用代数方法计算平移不变CSS构造的逻辑qudit数量，在二维环面上搜索具有扭曲边界条件的紧凑码，并分析其率-距离折衷。

Result: 在研究的有限尺寸范围内，扭曲环面qudit构造通常比未扭曲版本实现更大的码距，并且性能优于先前报道的扭曲qubit实例。最佳新码已列表呈现。

Conclusion: 扭曲边界条件对qudit LDPC码有益，相比未扭曲构造和之前的qubit方案能提供更好的距离特性，证明了该方法在量子纠错中的价值。

Abstract: We study finite-length qudit quantum low-density parity-check (LDPC) codes from translation-invariant CSS constructions on two-dimensional tori with twisted boundary conditions. Recent qubit work [PRX Quantum 6, 020357 (2025)] showed that, within the bivariate-bicycle viewpoint, twisting generalized toric patterns can significantly improve finite-size performance as measured by $k d^{2}/n$. Here $n$ denotes the number of physical qudits, $k$ the number of logical qudits, and $d$ the code distance. Building on this insight, we extend the search to qudit codes over finite fields. Using algebraic methods, we compute the number of logical qudits and identify compact codes with favorable rate--distance tradeoffs. Overall, for the finite sizes explored, twisted-torus qudit constructions typically achieve larger distances than their untwisted counterparts and outperform previously reported twisted qubit instances. The best new codes are tabulated.

</details>


### [39] [Influence of environment on quantum correlations in two-spin systems with dipole-dipole interactions](https://arxiv.org/abs/2602.04444)
*G. A. Bochkin,E. B. Fel'dman,E. I. Kuznetsova,E. I. Shipulya*

Main category: quant-ph

TL;DR: 研究双自旋-1/2系统中环境退相干对量子纠缠和量子失协的影响，发现量子失协比纠缠更具鲁棒性


<details>
  <summary>Details</summary>
Motivation: 理解环境噪声对量子关联（纠缠和量子失协）的不同影响，对量子信息处理中维持量子特性至关重要

Method: 基于Lindblad方程，研究具有偶极-偶极相互作用的双自旋-1/2系统，考虑环境仅引起系统自旋退相位的情况

Result: 获得纠缠与量子失协随弛豫速率的变化关系，显示量子失协比纠缠衰减更缓慢

Conclusion: 量子失协对环境退相位的抵抗力强于量子纠缠，在噪声环境中更具应用潜力

Abstract: An influence of environment on quantum correlations (entanglement and quantum discord) is studied in a two-spin-1/2 system with dipole-dipole interactions on the basis of Lindblad equation. We consider the simplest case when the environment causes only dephasing of system spins. The dependencies of entanglement and the quantum discord on the relaxation rate are obtained. We compare the influence of the environment on entanglement and quantum discord.

</details>


### [40] [Squeezing-Enhanced Rotational Doppler Metrology](https://arxiv.org/abs/2602.04508)
*Javier Navarro,Mateo Casariego,Gabriel Molina-Terriza,Íñigo Luis Egusquiza,Mikel Sanz*

Main category: quant-ph

TL;DR: 本文提出了一种利用压缩光通过旋转多普勒效应测量旋转表面角速度的量子协议，在理想无噪声情况下达到海森堡极限，并在有噪声时通过优化能量分配优于经典方法。


<details>
  <summary>Details</summary>
Motivation: 利用旋转多普勒效应来估计旋转表面的角速度，并通过量子资源提高测量精度。

Method: 采用压缩和位移的拉盖尔-高斯模式作为量子资源，与表面粗糙的旋转金属盘相互作用，并通过零差检测方案测量频率偏移。

Result: 在无噪声理想情况下，协议达到海森堡极限；在有噪声时，通过优化位移和压缩之间的能量分配比例，量子策略始终优于经典方法。

Conclusion: 该量子协议在估计角速度方面优于经典方法，特别是在优化后能有效应对噪声影响。

Abstract: A rotating surface can induce a frequency shift in incident light by changing its angular momentum, a phenomenon known as the rotational Doppler effect. This effect provides a means to estimate the angular velocity of the rotating surface. In this work, we develop a continuous-variable quantum protocol for estimating the angular velocity of a rotating surface via the rotational Doppler effect. Our approach exploits squeezed and displaced Laguerre-Gaussian modes as quantum resources, which interact with a rotating metallic disc with surface roughness. The frequency shift induced by the rotational Doppler effect is then measured using a homodyne detection scheme. By analyzing the Fisher information, we demonstrate that the proposed squeezing-enhanced protocol achieves Heisenberg scaling in the ideal noiseless regime. Furthermore, we investigate the influence of noise and consider different surface models to assess their impact on the protocol's performance. While Heisenberg scaling is degraded in the presence of noise, we show that optimizing the energy allocation ratio between displacement and squeezing of the probe ensures that the quantum strategy consistently outperforms its classical counterpart.

</details>


### [41] [A simple means for deriving quantum mechanics](https://arxiv.org/abs/2602.04524)
*Eric Tesse*

Main category: quant-ph

TL;DR: The paper presents an intuitive hidden-variables theory where particles follow definite, continuous trajectories (momentum = mass × velocity) while fully reproducing all predictions of non-relativistic quantum mechanics, including entanglement and spin.


<details>
  <summary>Details</summary>
Motivation: To resolve the tension between quantum formalism and physical intuition by providing a comprehensible, realistic description where particles have definite positions yet remain empirically equivalent to standard quantum mechanics.

Method: Constructing a mechanics with classical-like particle dynamics where quantum probabilities emerge from conditional probabilities based on environmental states, then comparing this framework to Bohmian mechanics, stochastic mechanics, many-worlds, and collapse theories, and developing a relativistic extension.

Result: Successfully created a theory that satisfies all observable quantum phenomena (entanglement, spin, particle identity effects) using intuitive particle trajectories, established connections to major quantum interpretations, and formulated a relativistic version.

Conclusion: An intuitive, realistic alternative to standard quantum mechanics exists that reproduces all empirical content while providing clearer physical interpretation, suggesting quantum weirdness may be interpretational rather than fundamental.

Abstract: A type of mechanics will be presented that possesses some distinctive properties. On the one hand, its physical description & rules of operation are readily comprehensible & intuitively clear. On the other, it fully satisfies all observable predictions of non-relativistic quantum mechanics. Within it, particles exist at points in space, follow continuous, piecewise differentiable paths, and their linear momentum is equal to their mass times their velocity along their path. Yet the probabilities for position and momentum, conditioned on the state of the particle's environment, follow the rules of quantum theory. Indeed, all observable consequences of quantum theory are satisfied; particles can be entangled, have intrinsic spin, this spin is not local to the particle, particle identity can effect probabilities, and so forth. All the rules of quantum mechanics are obeyed, and all arise in a straightforward fashion. After this is established, connections will be drawn out between this type of mechanics and other types of quantum worlds; those that obey Bohmian mechanics, stochastic mechanics, the many worlds interpretation, and physical collapse. In the final section, a relativistic version of the mechanics will be presented.

</details>


### [42] [Thermodynamic Cost of Regeneration in a Quantum Stirling Cycle](https://arxiv.org/abs/2602.04538)
*Ferdi Altintas*

Main category: quant-ph

TL;DR: 修正量子斯特林引擎研究中再生过程"零成本"的错误假设，证明计入再生功消耗后效率低于卡诺极限但仍优于传统循环，并建立热力学一致性边界


<details>
  <summary>Details</summary>
Motivation: 先前研究假设量子热机再生过程无热力学成本，导致报道出超卡诺效率（违反热力学第二定律）。本研究旨在揭示该假设的错误性，将再生功输入作为显性成本纳入效率计算，恢复热力学一致性。

Method: 在弱耦合马尔可夫开放量子系统框架下，分析单自旋1/2和双相互作用自旋1/2两种工质；通过修正循环效率公式（显式计入再生功成本），并以卡诺热泵极限设定再生成本最小值。对比含再生循环与传统循环性能。

Result: 1) 计入再生成本后，超卡诺效率消失，修正效率严格低于卡诺边界，但仍高于传统斯特林循环效率；2) 为传统循环提供基于量子相对熵的严格卡诺边界；3) 为再生循环推导保证热力学一致性的再生成本下限。

Conclusion: 再生过程在开放系统描述中并非热力学免费，必须计入功输入成本；修正后的量子斯特林循环在保持热力学一致性的前提下仍具性能优势。提出三种量子再生器候选模型作为未来研究方向。

Abstract: We study the regenerative quantum Stirling heat engine cycle within the standard weak-coupling, Markovian open quantum system framework. We point out that the regeneration process is not thermodynamically free in a reduced open-system description, and we treat the required work input as an explicit regeneration cost by modifying the cycle efficiency accordingly. We consider two working substances--a single spin-$1/2$ and a pair of interacting spin-$1/2$ particles--and investigate the cycle performance by taking the regeneration cost at its minimum value set by the Carnot heat-pump limit. For comparison, we also analyze the conventional Stirling cycle without regeneration under the same conditions. The super-Carnot efficiencies reported under the cost-free regeneration assumption disappear once the regeneration cost is included: the modified efficiency stays below the Carnot bound, while still remaining higher than the efficiency of the conventional Stirling cycle. For the conventional Stirling cycle, we provide a rigorous Carnot bound using quantum relative entropy, whereas for the regenerative cycle we derive a sufficient lower bound on the regeneration cost that guarantees thermodynamic consistency. Finally, we suggest three candidate quantum regenerator models for future work.

</details>


### [43] [Effect of initial intrasystem entanglement on entropy growth in generalized Jaynes-Cummings models](https://arxiv.org/abs/2602.04543)
*Daria Gaidukevich*

Main category: quant-ph

TL;DR: Initial intrasystem entanglement positively correlates with entropy growth in multi-subsystem Jaynes-Cummings models across various initial state ensembles, though its fractional contribution varies.


<details>
  <summary>Details</summary>
Motivation: Initial entanglement alone does not uniquely determine final entropy in quantum systems; understanding ensemble-averaged behavior is critical for revealing how intrasystem correlations influence entropy generation in quantum information processes.

Method: Analyzed generalized Jaynes-Cummings models with multiple atomic subsystems interacting with a photonic environment. Studied ensembles of initial states including pure/mixed Haar-random states, ensembles with fixed average energy or mixedness, and varying initial photon numbers.

Result: Observed a consistent positive correlation between initial intrasystem entanglement and entropy growth across all ensemble types, but the fractional contribution of initial entanglement to total entropy varied depending on the ensemble characteristics.

Conclusion: Intrasystem correlations play a significant role as a contributing factor to entropy growth in quantum informational processes, highlighting their importance beyond individual state properties.

Abstract: We investigate how initial intrasystem entanglement influences the entropy generated in atomic systems interacting with a photonic environment in several generalizations of the Jaynes-Cummings model with two or more subsystems. Since the initial entanglement does not uniquely determine the final entropy, we focus on ensemble-averaged behavior. We consider ensembles of initial system states including pure and mixed Haar-random states, ensembles with fixed average energy or fixed mixedness, and varying initial photon numbers in the environment. In all cases, we observe a positive correlation between the initial entanglement and the entropy growth, although the fractional contribution of the initial entanglement varies. Our results emphasize the role of intrasystem correlations as a factor contributing to entropy growth in quantum informational processes.

</details>


### [44] [Locally Gentle State Certification for High Dimensional Quantum Systems](https://arxiv.org/abs/2602.04550)
*Cristina Butucea,Jan Johannes,Henning Stein*

Main category: quant-ph

TL;DR: 本文研究非破坏性的"温和"量子测量，证明在区分量子态时，此类测量需要Θ(d³/(ε²α²))的样本复杂度，揭示了温和性带来的维度相关惩罚。


<details>
  <summary>Details</summary>
Motivation: 传统量子测量会坍缩波函数并破坏量子态。本研究旨在探索可重复使用量子态的"温和"测量的基本极限，这对高效量子学习至关重要，并与量子隐私机制有深刻联系。

Method: 通过分析假设检验问题——在测量扰动不超过α（迹范数）的约束下，判断未知量子态ρ是否等于参考态ρ₀或与之ε-远离——并构造显式测量算子，推导出极小极大样本复杂度。

Result: 样本复杂度为n = Θ(d³/(ε²α²))。α-温和性约束相比标准量子态认证引入了d/α²的惩罚因子。值得注意的是，该惩罚随Hilbert空间维度d线性增长，而非高维私有估计中典型的参数个数d²-1。

Conclusion: 该工作阐明了量子学习中信息提取与态扰动之间的权衡，揭示了物理测量约束与隐私机制的深刻联系，表明温和测量会带来与维度d相关的样本成本，对量子隐私和高效量子统计推断具有重要意义。

Abstract: Standard approaches to quantum statistical inference rely on measurements that induce a collapse of the wave function, effectively consuming the quantum state to extract information. In this work, we investigate the fundamental limits of \emph{locally-gentle} quantum state certification, where the learning algorithm is constrained to perturb the state by at most $α$ in trace norm, thereby allowing for the reuse of samples. We analyze the hypothesis testing problem of distinguishing whether an unknown state $ρ$ is equal to a reference $ρ_0$ or $ε$-far from it. We derive the minimax sample complexity for this problem, quantifying the information-theoretic price of non-destructive measurements. Specifically, by constructing explicit measurement operators, we show that the constraint of $α$-gentleness imposes a sample size penalty of $\frac{d}{α^2}$, yielding a total sample complexity of $n = Θ(\frac{d^3}{ε^2 α^2})$. Our results clarify the trade-off between information extraction and state disturbance, and highlight deep connections between physical measurement constraints and privacy mechanisms in quantum learning. Crucially, we find that the sample size penalty incurred by enforcing $α$-gentleness scales linearly with the Hilbert-space dimension $d$ rather than the number of parameters $d^2-1$ typical for high-dimensional private estimation.

</details>


### [45] [Restoring Landauer's Principle for Unitarily Transformed Thermal Reservoirs](https://arxiv.org/abs/2602.04552)
*Hao Xu*

Main category: quant-ph

TL;DR: 提出广义兰道尔原理解决压缩热态下的表观违反问题，建立有效哈密顿量框架证明熵产非负性，为量子热力学提供新工具


<details>
  <summary>Details</summary>
Motivation: 传统兰道尔原理在压缩热态（STS）系统中出现表观违反，需建立统一框架协调量子信息与热力学定律

Method: 通过定义有效哈密顿量，形式化扩展兰道尔原理至幺正变换热态，推导广义不等式并证明熵产非负性

Result: 获得适用于STS的广义兰道尔不等式（退化为标准情形当STS→普通热库），严格证明熵产非负，并成功应用于移动Unruh-DeWitt探测器模型

Conclusion: 解决STS下的兰道尔原理表观矛盾，为非平衡与相对论性量子热力学提供严谨分析工具，推动量子热机与信息协议发展

Abstract: Landauer's principle, a cornerstone of quantum information and thermodynamics, appears to be violated when the thermal reservoir is replaced by a squeezed thermal state (STS). We introduce a formal extension of the principle to such unitarily transformed thermal states. By defining an effective Hamiltonian, we rigorously establish a generalized Landauer inequality, which naturally reduces to the standard case for an ordinary thermal reservoir as a special instance. The framework further yields a consistent definition of entropy production and a proof of its non-negativity. We illustrate its utility by studying an arbitrarily moving Unruh-DeWitt detector coupled to a quantum field initially prepared in the STS. Using perturbation theory, we compute the entropy production explicitly, confirming its positivity. As a result of the symmetry breaking induced by the unitary transformation, it depends on both the proper time interval and the absolute spacetime position. Our work resolves the apparent violation of Landauer's principle with STSs. It also provides a robust tool for analyzing quantum thermodynamics in non-equilibrium and relativistic settings, with potential implications for quantum thermal machines and information protocols.

</details>


### [46] [Dicke Superradiance in Extended 2D Quantum Arrays Coupled to Metasurface Bound States in the Continuum](https://arxiv.org/abs/2602.04627)
*Daniel Eyles,Emmanuel Lassalle,Adam Stokes,Ahsan Nazir,Ramón Paniagua-Domínguez*

Main category: quant-ph

TL;DR: 利用介电超表面中的连续域束缚态实现扩展量子发射器阵列的迪克超辐射


<details>
  <summary>Details</summary>
Motivation: 传统迪克超辐射需要发射器亚波长间距，限制了其在扩展阵列中的应用；需要新平台实现长程相干耦合

Method: 提出利用介电超表面支持的高Q非局域模式（连续域束缚态/BIC）来介导相距数波长的量子发射器间的超辐射耦合

Result: BIC介导的发射器相互作用可跨越多个波长距离；通过β因子量化耦合效率；系统可达到理想迪克极限；对实验缺陷具有鲁棒性

Conclusion: 基于BIC的光学超表面为在物理扩展的量子发射器阵列中实现协同发射上限提供了可行实验平台

Abstract: Dicke superradiance is a collective phenomenon where the emission from ensembles of quantum emitters is coherently enhanced beyond the sum of each emitter's independent emission. Here, we propose a platform that exploits the delocalised nature of a high-Q, non-local mode supported by a dielectric metasurface (a so-called bound-state-in-the-continuum or BIC) to induce superradiant behaviour within an extended two-dimensional array of distant quantum emitters. We show that these BIC-mediated emitter interactions can span several wavelengths, thus overcoming the traditional subwavelength separation between emitters required in free space. We further show that reaching the idealised Dicke limit is possible in this system, provided that the emitters are coupled to the BIC mode efficiently enough, as quantified through the $β$-factor. Moreover, we demonstrate its experimental viability by analysing its robustness to realistic experimental imperfections. This work puts forward optical metasurfaces supporting BICs as a physically viable platform for realising the upper limits of cooperative emission in physically extended quantum emitter arrays.

</details>


### [47] [On the emergence of classical stochasticity](https://arxiv.org/abs/2602.04633)
*Xuan Du Trinh,Ismaël Septembre,Hai-Chau Nguyen*

Main category: quant-ph

TL;DR: 该论文揭示了量子系统中经典随机性涌现的逻辑基础问题，指出Pauli型主方程虽描述概率演化，但需额外假设"系统处于确定状态"才能计算经典随机时间，并通过超退相干极限案例展示量子到经典的过渡机制。


<details>
  <summary>Details</summary>
Motivation: 经典随机性在量子系统中的涌现机制尚未完全解决：Pauli型主方程描述概率演化，但无法自动证明"系统在中间时刻处于确定状态"这一经典推理前提的合理性，而该假设却是计算持久时间、首次到达时间等随机时间标准方法的基础。

Method: 通过分析Pauli型主方程的逻辑结构，论证"确定状态"假设对随机时间计算的必要性；以单粒子、玻色子和费米子在超退相干极限下的行为为例，演示量子力学如何导出经典随机性。

Result: 1) 证明标准随机时间计算隐含依赖"系统存在确定状态"的未声明假设；2) 在超退相干极限下，量子系统通过退相干机制自然呈现经典随机行为，为假设提供物理基础。

Conclusion: 量子系统可通过超退相干等机制实现向经典随机性的有效过渡，但需明确区分数学描述（主方程）与物理假设（确定状态），这对量子-经典对应关系的严谨建立具有重要意义。

Abstract: We examine the logical structure of the emergence of classical stochasticity for a quantum system governed by a Pauli-type master equation. It is well-known that while such equations describe the evolution of probabilities, they do not automatically justify classical reasoning based on the assumption that the system exists in a definite state at intermediate times. On the other hand, we show that this assumption is crucial for the standard calculation of stochastic times such as the persistent time and the time of first arrivals. We then consider examples of single particles, bosons, and fermions in the so-called ultradecoherence limit to illustrate how classical stochasticity may emerge from quantum mechanics.

</details>


### [48] [Pure narrowband photon-pair generation in a monolithic cavity](https://arxiv.org/abs/2602.04646)
*Xavier Barcons Planas,Helen M. Chrzanowski,Janik Wolters*

Main category: quant-ph

TL;DR: 本研究开发了一种基于单片腔体优化的预报式SPDC单光子源，在1540 nm波段实现了84%的预报效率、96.2%的光谱纯度以及低于3%的多光子污染。


<details>
  <summary>Details</summary>
Motivation: 光子量子技术的发展迫切需要高效且光谱/空间纯度高的单光子源。

Method: 采用自发参量下转换(SPDC)技术结合单片腔体结构，优化提升光谱和空间纯度。

Result: 实验获得了1540 nm波长的单光子，光谱带宽168 MHz，最大预报效率84%，多光子污染低于3%，光谱纯度达96.2%。

Conclusion: 腔体增强效应使光子主要在中心腔模产生，成功实现了96.2%的高光谱纯度。

Abstract: Photonic quantum technologies require efficient sources of pure single photons. Here we present a heralded SPDC single-photon source in a monolithic cavity optimized for high spectral and spatial purity. The source heralds single-photons at a wavelength of 1540 nm and a spectral bandwidth of 168 MHz with a maximum heralding efficiency of 84%, while keeping the multi-photon contamination below 3%. The cavity enhancement generates photons mainly in the central cavity mode with 96.2% spectral purity.

</details>


### [49] [Quantum Advantage in Decision Trees: A Weighted Graph and $L_1$ Norm Approach](https://arxiv.org/abs/2602.04700)
*Sebastian Alberto Grillo,Bernardo Daniel Dávalos,Rodney Fabian Franco Torres,Franklin de Lima Marquezino,Edgar López Pezoa*

Main category: quant-ph

TL;DR: 本文将单查询量子决策树建模为加权图，通过分析输出态的L1谱范数来刻画单查询量子算法的计算能力。研究表明高L1范数是量子优势的必要条件，并提出了最大化该范数的启发式方法、构造严格递增范数序列的组合技术、展示指数级量子优势的具体函数，以及建立量子优势与测量投影维数渐近增长之间的必要条件。


<details>
  <summary>Details</summary>
Motivation: 单查询量子算法必须在一次预言机调用中提取最大信息，对其计算能力的分析揭示了量子优势的基本极限，对于实现最优且资源高效的量子计算具有重要意义。

Method: 将单查询量子决策树形式化为加权图结构，利用该表示法便于分析算法输出态的L1谱范数，并设计启发式方法最大化该范数，同时通过组合加权图构造具有严格递增范数的序列。

Result: (1) 提出了最大化L1谱范数的启发式策略；(2) 展示了组合加权图以生成严格递增范数序列的方法；(3) 构造了展现指数级量子优势的具体函数实例；(4) 建立了单查询量子优势与测量投影维数渐近增长之间的必要条件关系。

Conclusion: 加权图表示法为分析量子决策树的L1谱范数提供了有效框架，而高L1范数是超越经典算法的必要条件，这一工作为理解和设计具有量子优势的单查询算法奠定了理论基础。

Abstract: The analysis of the computational power of single-query quantum algorithms is important because they must extract maximal information from one oracle call, revealing fundamental limits of quantum advantage and enabling optimal, resource-efficient quantum computation. This paper proposes a formulation of single-query quantum decision trees as weighted graphs. This formulation has the advantage that it facilitates the analysis of the $L_1$ spectral norm of the algorithm output. This advantage is based on the fact that a high $L_1$ spectral norm of the output of a quantum decision tree is a necessary condition to outperform its classical counterpart. We propose heuristics for maximizing the $L_{1}$ spectral norm, show how to combine weighted graphs to generate sequences with strictly increasing norm, and present functions exhibiting exponential quantum advantage. Finally, we establish a necessary condition linking single-query quantum advantage to the asymptotic growth of measurement projector dimensions.

</details>


### [50] [Enabling large-scale digital quantum simulations with superconducting qubits](https://arxiv.org/abs/2602.04719)
*Laurin E. Fischer*

Main category: quant-ph

TL;DR: 本博士论文针对当前含噪声量子硬件实现量子模拟的挑战，提出覆盖全计算栈的综合方法，包括硬件创新、噪声建模、误差缓解和算法改进，旨在弥合理论潜力与实际应用的差距。


<details>
  <summary>Details</summary>
Motivation: 当前量子计算机（包括最先进的超导电路平台）存在缺陷，无法实现数字量子模拟的实际量子优势。容错量子计算机虽为长期解决方案，但量子比特开销过大，因此亟需从含噪声设备中提取有意义结果，以推动量子化学、凝聚态物理和材料科学等领域发展。

Method: 采用全栈式研究方法：1）硬件层面的平台特定创新；2）精细化噪声建模与误差缓解技术；3）通过高效测量处理实现算法改进，结合量子与经典资源协同优化。

Result: 构建了多层次实用方法框架，提供硬件特定优化、先进误差缓解技术和测量高效算法，显著提升近期含噪声量子硬件上进行量子模拟的准确性和可行性。

Conclusion: 通过解决量子计算栈各层面的挑战，该工作增强了当前含噪声设备实现量子模拟的实用性，为未来容错系统奠定基础，有效推动了从理论潜力向实际应用的过渡。

Abstract: Quantum computing promises to revolutionize several scientific and technological domains through fundamentally new ways of processing information. Among its most compelling applications is digital quantum simulation, where quantum computers are used to replicate the behavior of other quantum systems. This could enable the study of problems that are otherwise intractable on classical computers, transforming fields such as quantum chemistry, condensed matter physics, and materials science. Despite this potential, realizations of practical quantum advantage for relevant problems are hindered by imperfections of current devices. This also affects quantum hardware based on superconducting circuits which is among the most advanced and scalable platforms. The envisaged long-term solution of fault-tolerant quantum computers that correct their own errors remains out of reach mainly due to the associated qubit number overhead. As a result, the field has developed strategies that combine quantum and classical resources, exploit hardware-native operations, and employ error mitigation techniques to extract meaningful results from noisy data. This doctoral thesis contributes to this broader effort by exploring methods for advancing quantum simulation across the full computational stack, including hardware-level innovations, refined techniques for noise modeling and error mitigation, and algorithmic improvements enabled by efficient measurement processing.

</details>


### [51] [Ising-Induced Spectral Broadening Resolves the Relaxation Bottleneck in Superradiant Masers](https://arxiv.org/abs/2602.04721)
*Hongze Ding,Jiuqing Liang*

Main category: quant-ph

TL;DR: 伊辛相互作用引起的非均匀展宽在密集自旋系综中产生弛豫瓶颈，解释了超辐射激射的缓慢动力学。


<details>
  <summary>Details</summary>
Motivation: 解释自诱导超辐射激射实验中观察到的集体弛豫时间尺度比标准相干输运模型预测显著更慢的现象。

Method: 解析理论表明，在高密度条件下，对角伊辛相互作用产生超过单粒子退相位的非均匀展宽，抑制共振翻转交换，有效重整化谱扩散的可用态密度。

Result: 无参数解析理论定量重现了实验观测的微秒级动力学。

Conclusion: 伊辛诱导的展宽是密集固态自旋系综中能量输运的主导机制。

Abstract: The recent observation of self-induced superradiant masing [[W. Kersten et al., Nat. Phys. 22, 158 (2026)]] revealed a collective relaxation timescale significantly slower than predicted by standard coherent transport models. Here, we elucidate the microscopic origin of this ``relaxation bottleneck.'' We show that in the high-density regime relevant to the experiment, diagonal Ising interactions -- often treated as perturbative -- generate profound inhomogeneous broadening that exceeds the intrinsic single-particle dephasing. This intense diagonal disorder suppresses resonant flip-flop exchange, effectively renormalizing the density of states available for spectral diffusion. Our parameter-free analytic theory quantitatively reproduces the experimentally observed microsecond dynamics, identifying Ising-induced broadening as the governing mechanism for energy transport in dense solid-state spin ensembles.

</details>


### [52] [Resource-Efficient Digitized Adiabatic Quantum Factorization](https://arxiv.org/abs/2602.04740)
*Felip Pellicer,Juan José García-Ripoll,Alan C. Santos*

Main category: quant-ph

TL;DR: 提出一种新的数字化绝热量子因数分解算法，通过核子空间编码替代基态编码，将问题转化为QUBO形式而非PUBO，在8位整数分解中实现了更低的电路复杂度和更高的保真度。


<details>
  <summary>Details</summary>
Motivation: 标准绝热量子因数分解算法（Peng et al., 2008）采用基态编码，导致PUBO形式和高门电路需求，限制了其在数字化量子计算机上的实现效率。

Method: 利用数字化量子计算机的灵活性，提出核子空间编码方法，将解编码在问题哈密顿量的核子空间中，从而设计属于QUBO类的绝热量子因数分解算法。

Result: 通过分解最多8位整数的实验，新算法相比PUBO方案显著降低了电路复杂度，并提高了识别正确解的保真度。

Conclusion: 核子空间编码策略有效优化了数字化绝热量子因数分解的性能，为在现有量子硬件上实现更高效的因数分解算法提供了新途径。

Abstract: Digitized adiabatic quantum factorization is a hybrid algorithm that exploits the advantage of digitized quantum computers to implement efficient adiabatic algorithms for factorization through gate decompositions of analog evolutions. In this paper, we harness the flexibility of digitized computers to derive a digitized adiabatic algorithm able to reduce the gate-demanding costs of implementing factorization. To this end, we propose a new approach for adiabatic factorization by encoding the solution of the problem in the kernel subspace of the problem Hamiltonian, instead of using ground-state encoding considered in the standard adiabatic factorization proposed by Peng $et$ $al$. [Phys. Rev. Lett. 101, 220405 (2008)]. Our encoding enables the design of adiabatic factorization algorithms belonging to the class of Quadratic Unconstrained Binary Optimization (QUBO) methods, instead the Polinomial Unconstrained Binary Optimization (PUBO) used by standard adiabatic factorization. We illustrate the performance of our QUBO algorithm by implementing the factorization of integers $N$ up to 8 bits. The results demonstrate a substantial improvement over the PUBO formulation, both in terms of reduced circuit complexity and increased fidelity in identifying the correct solution.

</details>


### [53] [Generalized quantum theory for accessing nonlinear systems: the case of Levinson-Smith equations](https://arxiv.org/abs/2602.04747)
*Bijan Bagchi,Anindya Ghose-Choudhury*

Main category: quant-ph

TL;DR: 该论文将广义量子力学方案与Levinson-Smith非线性系统（含Liénard方程）建立联系，通过Abel形式转化获得闭式解，揭示稳定平衡点、Jacobi椭圆函数解、位置相关质量系统关联及类孤子解


<details>
  <summary>Details</summary>
Motivation: 受广义量子力学新方案的启发，探索其与Levinson-Smith非线性系统（特别是具有奇偶对称系数的Liénard方程族）的深层联系

Method: 将Liénard方程转化为Abel微分方程形式以获取闭式解，分析系统平衡点稳定性，并研究不同参数组合下的解结构（Jacobi椭圆函数/位置相关质量系统）

Result: 1) 发现非平凡平衡点的稳定性特征；2) 特定参数下出现Jacobi椭圆函数解；3) 部分系统对应位置相关质量量子体系；4) 能级面条件意外导出类孤子解

Conclusion: 广义量子力学框架可统一描述多种非线性系统，其数学结构不仅产生稳定解和特殊函数解，还自然蕴含类孤子行为，拓展了量子理论与非线性动力学的交叉应用

Abstract: Motivated by a recently developed generalized scheme of quantum mechanics, we touch upon connections with Levinson-Smith classes of nonlinear systems that contain as a particular case the Liénard family of differential equations. The latter, which has coefficients of odd and odd symmetry, admits a closed form solution when converted to the Abel form. Analysis of the governing condition shows that one of the nontrivial equilibrium points is stable in character. Other classes of differential equations that we encounter speak of solutions involving Jacobi elliptic functions for a certain combination of underlying parameters, while, for a different set, relevance to position-dependent mass systems is shown. In addition, an interesting off-shoot of our results is the emergence of solitonic-like solutions from the condition of the level surface in the system.

</details>


### [54] [Quantifying the Operational Cost of Multipartite Entanglement](https://arxiv.org/abs/2602.04760)
*Francois Payn,Michele Minervini,Davide Girolami*

Main category: quant-ph

TL;DR: 提出量化多粒子系统k≤N部分纠缠的通用方法，通过最大化子系统二分纠缠度量实现，揭示k部分纠缠态至少需要k-1个双粒子纠缠门创建


<details>
  <summary>Details</summary>
Motivation: 多粒子纠缠决定量子多体系统的相互作用强度和范围，但现有方法因量子态复杂结构而难以有效评估

Method: 通过最大化任意二分纠缠度量在至多k粒子子系统内的值，定义k部分纠缠度量，建立状态分类与实验成本的关联

Result: 证明创建k部分纠缠态至少需要k-1个双粒子纠缠门；解析计算了新定义的k部分形成纠缠在W态等典型态中的值

Conclusion: 该分类方法能有效表征量子态的实验制备成本，为多体量子系统纠缠资源评估提供新理论框架

Abstract: Multipartite entanglement determines the strength and range of interactions in many-body quantum systems. Yet, it is hard to evaluate it, due to the complex structures of quantum states. Here, we introduce a generic method to quantify the k <= N-partite entanglement of an N-particle system, by maximizing an arbitrary bipartite entanglement measure within subsystems of size up to k. The resulting classification of multipartite states captures their experimental cost: creating a k-partite entangled state requires at least k-1 two-particle entangling gates. Further, we analytically calculate the newly defined k-partite entanglement of formation, which generalizes an important bipartite entanglement measure, in several classes of states, including the W states of any dimension.

</details>


### [55] [Review of Superconducting Qubit Devices and Their Large-Scale Integration](https://arxiv.org/abs/2602.04831)
*Hiu Yung Wong*

Main category: quant-ph

TL;DR: 本文系统综述了超导量子比特量子计算机的关键技术，包括量子比特设计、纠缠门操作、读取工程、退相干机制和大规模集成方案，并探讨了电子设计自动化在该领域的应用前景。


<details>
  <summary>Details</summary>
Motivation: 超导量子比特是最有希望实现大规模集成的量子计算架构之一，本文旨在从教育视角梳理其基础知识，系统分析满足DiVincenzo标准的各项技术挑战，为大规模集成提供技术路线参考。

Method: 采用系统性文献综述方法，从基本原理出发，分层讨论量子比特物理实现、操控技术、读取方案、退相干机制和集成设计五个核心层面，并结合半导体EDA经验进行对比分析。

Result: 详细分析了约瑟夫森结量子比特的类型与参数权衡、纠缠门操作瓶颈、Purcell滤波器和量子极限放大器等读取技术、限制相干时间的二能级系统缺陷，以及大规模集成架构和EDA设计工具的最新进展。

Conclusion: 超导量子计算虽在制造兼容性上优势明显，但仍面临退相干、门操作效率和集成设计等挑战；建立类似半导体的EDA设计流程是推动其大规模集成的关键路径，需要跨学科协同创新。

Abstract: The superconducting qubit quantum computer is one of the most promising quantum computing architectures for large-scale integration due to its maturity and close proximity to the well-established semiconductor manufacturing infrastructure. From an education perspective, it also bridges classical microwave electronics and quantum electrodynamics. In this paper, we will review the basics of quantum computers, superconductivity, and Josephson junctions. We then introduce important technologies and concepts related to DiVincenzo's criteria, which are the necessary conditions for the superconducting qubits to work as a useful quantum computer. Firstly, we will discuss various types of superconducting qubits formed with Josephson junctions, from which we will understand the trade-off across multiple design parameters, including their noise immunity. Secondly, we will discuss different schemes to achieve entanglement gate operations, which are a major bottleneck in achieving more efficient fault-tolerant quantum computing. Thirdly, we will review readout engineering, including the implementations of the Purcell filters and quantum-limited amplifiers. Finally, we will discuss the nature and review the studies of two-level system defects, which are currently the limiting factor of qubit coherence time. DiVincenzo's criteria are only the necessary conditions for a technology to be eligible for quantum computing. To have a useful quantum computer, large-scale integration is required. We will review proposals and developments for the large-scale integration of superconducting qubit devices. By comparing with the application of electronic design automation (EDA) in semiconductors, we will also review the use of EDA in superconducting qubit quantum computer design, which is necessary for its large-scale integration.

</details>


### [56] [Digital signatures with classical shadows on near-term quantum computers](https://arxiv.org/abs/2602.04859)
*Pradeep Niroula,Minzhao Liu,Sivaprasad Omanakuttan,David Amaro,Shouvanik Chakrabarti,Soumik Ghosh,Zichang He,Yuwei Jin,Fatih Kaleoglu,Steven Kordonowy,Rohan Kumar,Michael A. Perlin,Akshay Seshadri,Matthew Steinberg,Joseph Sullivan,Jacob Watkins,Henry Yuen,Ruslan Shaydulin*

Main category: quant-ph

TL;DR: A quantum digital signature scheme using only classical communication and classical shadows of random circuits is proposed and experimentally demonstrated, achieving 0.90 fidelity and showing near-term feasibility without requiring low-noise quantum communication or long-lived quantum memory.


<details>
  <summary>Details</summary>
Motivation: Existing quantum cryptographic primitives require low-noise quantum communication and long-lived quantum memory, which are practically challenging to implement.

Method: Proposes a scheme using classical shadows of random circuit states as public keys, with an improved state-certification primitive based on a high-rate error-detecting code, and provides theoretical/numerical hardness evidence.

Result: Experimentally generated shadows for 32-qubit states with ≥80 logical gates, attaining 0.90 ± 0.01 fidelity, and realized a proof-of-principle quantum digital signature with increased measurement samples.

Conclusion: The work demonstrates the near-term feasibility of quantum digital signatures by operating with only classical communication and tolerating higher noise levels.

Abstract: Quantum mechanics provides cryptographic primitives whose security is grounded in hardness assumptions independent of those underlying classical cryptography. However, existing proposals require low-noise quantum communication and long-lived quantum memory, capabilities which remain challenging to realize in practice. In this work, we introduce a quantum digital signature scheme that operates with only classical communication, using the classical shadows of states produced by random circuits as public keys. We provide theoretical and numerical evidence supporting the conjectured hardness of learning the private key (the circuit) from the public key (the shadow). A key technical ingredient enabling our scheme is an improved state-certification primitive that achieves higher noise tolerance and lower sample complexity than prior methods. We realize this certification by designing a high-rate error-detecting code tailored to our random-circuit ensemble and experimentally generating shadows for 32-qubit states using circuits with $\geq 80$ logical ($\geq 582$ physical) two-qubit gates, attaining 0.90 $\pm$ 0.01 fidelity. With increased number of measurement samples, our hardware-demonstrated primitives realize a proof-of-principle quantum digital signature, demonstrating the near-term feasibility of our scheme.

</details>


### [57] [Thermal State Simulation with Pauli and Majorana Propagation](https://arxiv.org/abs/2602.04878)
*Manuel S. Rudolph,Armando Angrisani,Andrew Wright,Iwo Sanderski,Ricard Puig,Zoë Holmes*

Main category: quant-ph

TL;DR: 提出基于传播子的热态模拟方法，利用泡利/马约拉纳算符基在虚时间演化中的稀疏性实现高温量子系统高效模拟


<details>
  <summary>Details</summary>
Motivation: 高温量子态模拟存在计算效率瓶颈，传统方法难以处理热态在希尔伯特空间中的复杂演化

Method: 将泡利和马约拉纳传播子适配到薛定谔图景的虚时间演化；从最大混合态出发，在算符基中直接演化；采用小系数截断和泡利权重截断策略

Result: 理论证明截断误差可控且反向流影响可量化；在1D J1-J2模型和三角晶格哈伯德模型的大尺度数值实验中验证了高温区能量和静态关联的高效计算

Conclusion: 该方法通过利用高温态的算符基稀疏特性，为高温量子多体系统的热态模拟提供了高效且理论保证的新框架

Abstract: We introduce a propagation-based approach to thermal state simulation by adapting Pauli and Majorana propagation to imaginary-time evolution in the Schrödinger picture. Our key observation is that high-temperature states can be sparse in the Pauli or Majorana bases, approaching the identity at infinite temperature. By formulating imaginary-time evolution directly in these operator bases and evolving from the maximally mixed state, we access a continuum of temperatures where the state remains efficiently representable. We provide analytic guarantees for small-coefficient truncation and Pauli-weight (Majorana-length) truncation strategies by quantifying the error growth and the impact of backflow. Large-scale numerics on the 1D J1-J2 model (energies) and the triangular-lattice Hubbard model (static correlations) validate efficiency at high temperatures.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [58] [GOPO: Policy Optimization using Ranked Rewards](https://arxiv.org/abs/2602.03876)
*Kyuseong Choi,Dwaipayan Saha,Woojeong Kim,Anish Agarwal,Raaz Dwivedi*

Main category: cs.LG

TL;DR: 本文提出GOPO（Group Ordinal Policy Optimization），一种仅使用奖励排序而非绝对数值的策略优化方法，解决了非可验证奖励场景下RLHF的错位问题，相比GRPO表现出更优性能。


<details>
  <summary>Details</summary>
Motivation: 标准RLHF在奖励模型上优化相对偏好，但在策略优化阶段却使用绝对奖励值，这种错位导致在非可验证奖励场景（如摘要生成、指令遵循和对话完成）中性能次优。

Method: 分组序数策略优化（GOPO），该方法将奖励转换为基于排序的表示，在策略优化过程中舍弃奖励的幅度信息。

Result: GOPO实现了持续更高的训练/验证奖励轨迹，在大多数中间训练步骤中改进了LLM-as-judge评估，并以比GRPO显著更少的训练步数达到相当的策略质量，在各种任务和模型规模上均表现出一致的改进。

Conclusion: GOPO通过使用基于排序的奖励有效解决了非可验证奖励场景下的RLHF错位问题，在效率和性能上均优于GRPO等现有方法。

Abstract: Standard reinforcement learning from human feedback (RLHF) trains a reward model on pairwise preference data and then uses it for policy optimization. However, while reward models are optimized to capture relative preferences, existing policy optimization techniques rely on absolute reward magnitudes during training. In settings where the rewards are non-verifiable such as summarization, instruction following, and chat completion, this misalignment often leads to suboptimal performance. We introduce Group Ordinal Policy Optimization (GOPO), a policy optimization method that uses only the ranking of the rewards and discards their magnitudes. Our rank-based transformation of rewards provides several gains, compared to Group Relative Policy Optimization (GRPO), in settings with non-verifiable rewards: (1) consistently higher training/validation reward trajectories, (2) improved LLM-as-judge evaluations across most intermediate training steps, and (3) reaching a policy of comparable quality in substantially less training steps than GRPO. We demonstrate consistent improvements across a range of tasks and model sizes.

</details>


### [59] [GeoIB: Geometry-Aware Information Bottleneck via Statistical-Manifold Compression](https://arxiv.org/abs/2602.03906)
*Weiqi Wang,Zhiyi Tian,Chenhan Zhang,Shui Yu*

Main category: cs.LG

TL;DR: 提出几何信息瓶颈GeoIB，通过Fisher-Rao散度和Jacobian-Frobenius正则项直接控制信息压缩，避免互信息估计，实现更优的精度-压缩权衡和优化稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有深度信息瓶颈方法依赖互信息估计的替代形式，存在边界松弛和估计偏差问题，导致压缩控制间接且优化脆弱。

Method: 从信息几何角度重新审视IB，提出GeoIB：1) 用Fisher-Rao散度作为分布级正则项；2) 用Jacobian-Frobenius项约束编码器体积膨胀；3) 设计符合FR度量的自然梯度优化器。

Result: 在流行数据集上，GeoIB在信息平面中实现了预测精度与压缩比的最佳权衡，优于主流IB基线方法，提升了不变性和优化稳定性。

Conclusion: GeoIB通过统一分布和几何正则化，为信息瓶颈提供了更稳定、更有效的实现框架。

Abstract: Information Bottleneck (IB) is widely used, but in deep learning, it is usually implemented through tractable surrogates, such as variational bounds or neural mutual information (MI) estimators, rather than directly controlling the MI I(X;Z) itself. The looseness and estimator-dependent bias can make IB "compression" only indirectly controlled and optimization fragile.
  We revisit the IB problem through the lens of information geometry and propose a \textbf{Geo}metric \textbf{I}nformation \textbf{B}ottleneck (\textbf{GeoIB}) that dispenses with mutual information (MI) estimation. We show that I(X;Z) and I(Z;Y) admit exact projection forms as minimal Kullback-Leibler (KL) distances from the joint distributions to their respective independence manifolds. Guided by this view, GeoIB controls information compression with two complementary terms: (i) a distribution-level Fisher-Rao (FR) discrepancy, which matches KL to second order and is reparameterization-invariant; and (ii) a geometry-level Jacobian-Frobenius (JF) term that provides a local capacity-type upper bound on I(Z;X) by penalizing pullback volume expansion of the encoder. We further derive a natural-gradient optimizer consistent with the FR metric and prove that the standard additive natural-gradient step is first-order equivalent to the geodesic update. We conducted extensive experiments and observed that the GeoIB achieves a better trade-off between prediction accuracy and compression ratio in the information plane than the mainstream IB baselines on popular datasets. GeoIB improves invariance and optimization stability by unifying distributional and geometric regularization under a single bottleneck multiplier. The source code of GeoIB is released at "https://anonymous.4open.science/r/G-IB-0569".

</details>


### [60] [Theory of Speciation Transitions in Diffusion Models with General Class Structure](https://arxiv.org/abs/2602.04404)
*Beatrice Achilli,Marco Benedetti,Giulio Biroli,Marc Mézard*

Main category: cs.LG

TL;DR: 提出扩散模型中物种形成转变的通用理论，通过贝叶斯分类和自由熵差刻画生成样本向数据类别动态承诺的时刻，突破了先前仅适用于一阶矩可识别类别的限制。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型物种形成转变的理论分析仅限于类别可通过一阶矩识别的场景（如均值分离的高斯混合模型），缺乏适用于任意具有明确类别结构的目标分布的通用理论框架。

Method: 通过贝叶斯分类形式化类别结构，用类别间的自由熵差刻画物种形成时间，并利用复本方法求解一维Ising模型混合案例，将问题映射到随机场Ising模型获得解析表达式。

Result: 理论恢复了高斯混合模型的已知结果，扩展到仅通过高阶或集体特征区分类别的情形，支持多类别并预测与精细类别承诺相关的连续物种形成时间。

Conclusion: 为基于扩散的生成模型中的物种形成转变提供了统一且广泛适用的理论描述，显著拓展了适用场景和理论深度。

Abstract: Diffusion Models generate data by reversing a stochastic diffusion process, progressively transforming noise into structured samples drawn from a target distribution. Recent theoretical work has shown that this backward dynamics can undergo sharp qualitative transitions, known as speciation transitions, during which trajectories become dynamically committed to data classes. Existing theoretical analyses, however, are limited to settings where classes are identifiable through first moments, such as mixtures of Gaussians with well-separated means. In this work, we develop a general theory of speciation in diffusion models that applies to arbitrary target distributions admitting well-defined classes. We formalize the notion of class structure through Bayes classification and characterize speciation times in terms of free-entropy difference between classes. This criterion recovers known results in previously studied Gaussian-mixture models, while extending to situations in which classes are not distinguishable by first moments and may instead differ through higher-order or collective features. Our framework also accommodates multiple classes and predicts the existence of successive speciation times associated with increasingly fine-grained class commitment. We illustrate the theory on two analytically tractable examples: mixtures of one-dimensional Ising models at different temperatures and mixtures of zero-mean Gaussians with distinct covariance structures. In the Ising case, we obtain explicit expressions for speciation times by mapping the problem onto a random-field Ising model and solving it via the replica method. Our results provide a unified and broadly applicable description of speciation transitions in diffusion-based generative models.

</details>


### [61] [SpecMD: A Comprehensive Study On Speculative Expert Prefetching](https://arxiv.org/abs/2602.03921)
*Duc Hoang,Ajay Jaiswal,Mohammad Samragh,Minsik Cho*

Main category: cs.LG

TL;DR: 开发SpecMD框架基准测试MoE缓存策略，提出Least-Stale驱逐算法，减少85倍冲突未命中，0.6GB缓存实现88%命中率与34.7% TTFT降低。


<details>
  <summary>Details</summary>
Motivation: MoE模型稀疏激活需缓存提升性能，但现有硬件中心化策略与硬件规格交互关系不明。

Method: 构建SpecMD标准化框架；exhaustive基准测试多种MoE缓存策略；提出Least-Stale，利用MoE可预测访问模式。

Result: MoE访问不符合时间局部性；Least-Stale比LRU减少85×冲突未命中；仅5% VRAM缓存实现88%命中率与34.7% TTFT降低。

Conclusion: MoE需要特殊缓存策略，SpecMD与Least-Stale显著提升推理性能。

Abstract: Mixture-of-Experts (MoE) models enable sparse expert activation, meaning that only a subset of the model's parameters is used during each inference. However, to translate this sparsity into practical performance, an expert caching mechanism is required. Previous works have proposed hardware-centric caching policies, but how these various caching policies interact with each other and different hardware specification remains poorly understood. To address this gap, we develop \textbf{SpecMD}, a standardized framework for benchmarking ad-hoc cache policies on various hardware configurations. Using SpecMD, we perform an exhaustive benchmarking of several MoE caching strategies, reproducing and extending prior approaches in controlled settings with realistic constraints. Our experiments reveal that MoE expert access is not consistent with temporal locality assumptions (e.g LRU, LFU). Motivated by this observation, we propose \textbf{Least-Stale}, a novel eviction policy that exploits MoE's predictable expert access patterns to reduce collision misses by up to $85\times$ over LRU. With such gains, we achieve over $88\%$ hit rates with up to $34.7\%$ Time-to-first-token (TTFT) reduction on OLMoE at only $5\%$ or $0.6GB$ of VRAM cache capacity.

</details>


### [62] [WIND: Weather Inverse Diffusion for Zero-Shot Atmospheric Modeling](https://arxiv.org/abs/2602.03924)
*Michael Aich,Andreas Fürst,Florian Sestak,Carlos Ruiz-Gonzalez,Niklas Boers,Johannes Brandstetter*

Main category: cs.LG

TL;DR: 提出WIND基础模型，通过自监督视频重建和无任务微调，统一天气气候建模的多样化任务


<details>
  <summary>Details</summary>
Motivation: 当前天气气候建模高度碎片化，需为不同任务训练专门模型，缺乏统一框架

Method: 采用无条件视频扩散模型进行自监督视频重建预训练，推理时将问题视为逆问题并通过后验采样求解

Result: 无需任务特定微调即可处理概率预报、时空降尺度、稀疏重建和守恒律等任务，并能生成全球变暖下极端天气的反事实情景

Conclusion: 结合生成式视频建模与逆问题求解，WIND为AI大气建模提供了计算高效的新范式

Abstract: Deep learning has revolutionized weather and climate modeling, yet the current landscape remains fragmented: highly specialized models are typically trained individually for distinct tasks. To unify this landscape, we introduce WIND, a single pre-trained foundation model capable of replacing specialized baselines across a vast array of tasks. Crucially, in contrast to previous atmospheric foundation models, we achieve this without any task-specific fine-tuning. To learn a robust, task-agnostic prior of the atmosphere, we pre-train WIND with a self-supervised video reconstruction objective, utilizing an unconditional video diffusion model to iteratively reconstruct atmospheric dynamics from a noisy state. At inference, we frame diverse domain-specific problems strictly as inverse problems and solve them via posterior sampling. This unified approach allows us to tackle highly relevant weather and climate problems, including probabilistic forecasting, spatial and temporal downscaling, sparse reconstruction and enforcing conservation laws purely with our pre-trained model. We further demonstrate the model's capacity to generate physically consistent counterfactual storylines of extreme weather events under global warming scenarios. By combining generative video modeling with inverse problem solving, WIND offers a computationally efficient paradigm shift in AI-based atmospheric modeling.

</details>


### [63] [Autonomous AI Agents for Real-Time Affordable Housing Site Selection: Multi-Objective Reinforcement Learning Under Regulatory Constraints](https://arxiv.org/abs/2602.03940)
*Olaf Yunus Laitinen Imanov,Duygu Erisken,Derya Umut Kulali,Taner Yilmaz,Rana Irem Turhan*

Main category: cs.LG

TL;DR: 本文提出AURA——一种分层多智能体强化学习系统，用于在127项监管约束下优化保障性住房选址，合规率达94.3%，并将纽约市选址时间从18个月缩短至72小时，同时提升选址质量。


<details>
  <summary>Details</summary>
Motivation: 全球数十亿人面临保障性住房短缺，而土地稀缺与复杂法规（QCT、DDA、LIHTC）导致选址过程缓慢低效，亟需自动化且合规的决策支持工具。

Method: 研究团队提出AURA系统，将选址任务建模为约束多目标马尔可夫决策过程，采用监管感知状态编码器处理127项联邦与地方法规约束，运用带可行性保证的帕累托约束策略梯度算法，并通过奖励分解机制平衡即时建设成本与长期社会效益。

Result: 在覆盖8个美国大都市47,392个候选地块的数据集上，AURA实现94.3%监管合规率，帕累托超体积较强基线提升37.2%。纽约市2026年案例研究表明，选址周期从18个月压缩至72小时，识别可行地块数量增加23%，所选地块交通便利性提升31%，环境影响降低19%，表现优于专家决策。

Conclusion: AURA验证了分层多智能体强化学习在复杂监管环境下优化保障性住房选址的有效性，可显著提升决策效率与社会公平性，为城市资源分配提供了可扩展的AI解决方案。

Abstract: Affordable housing shortages affect billions, while land scarcity and regulations make site selection slow. We present AURA (Autonomous Urban Resource Allocator), a hierarchical multi-agent reinforcement learning system for real-time affordable housing site selection under hard regulatory constraints (QCT, DDA, LIHTC). We model the task as a constrained multi-objective Markov decision process optimizing accessibility, environmental impact, construction cost, and social equity while enforcing feasibility. AURA uses a regulatory-aware state encoding 127 federal and local constraints, Pareto-constrained policy gradients with feasibility guarantees, and reward decomposition separating immediate costs from long-term social outcomes. On datasets from 8 U.S. metros (47,392 candidate parcels), AURA attains 94.3% regulatory compliance and improves Pareto hypervolume by 37.2% over strong baselines. In a New York City 2026 case study, it reduces selection time from 18 months to 72 hours and identifies 23% more viable sites; chosen sites have 31% better transit access and 19% lower environmental impact than expert picks.

</details>


### [64] [Grables: Tabular Learning Beyond Independent Rows](https://arxiv.org/abs/2602.03945)
*Tamara Cucumides,Floris Geerts*

Main category: cs.LG

TL;DR: Proposes "grables" framework to model inter-row dependencies in tabular data by separating graph construction from graph-based prediction, outperforming row-independent methods on structured tables


<details>
  <summary>Details</summary>
Motivation: Row-wise predictors fail on transactional/temporal/relational tables where labels depend on other rows; existing methods lack precise characterization of how structure is utilized

Method: Introduces "grables" - a modular interface separating table-to-graph lifting (constructor) from graph prediction (node predictor), enabling systematic analysis of structural expressiveness

Result: Message passing captures inter-row dependencies missed by row-local models; hybrid approaches extracting explicit inter-row structure yield consistent gains across synthetic, transaction and clinical trial datasets

Conclusion: Structural dependencies in tables require graph-based modeling; grables provides principled framework for leveraging table structure across architectures, with hybrids combining structure extraction and strong tabular learners showing particular promise

Abstract: Tabular learning is still dominated by row-wise predictors that score each row independently, which fits i.i.d. benchmarks but fails on transactional, temporal, and relational tables where labels depend on other rows. We show that row-wise prediction rules out natural targets driven by global counts, overlaps, and relational patterns. To make "using structure" precise across architectures, we introduce grables: a modular interface that separates how a table is lifted to a graph (constructor) from how predictions are computed on that graph (node predictor), pinpointing where expressive power comes from. Experiments on synthetic tasks, transaction data, and a RelBench clinical-trials dataset confirm the predicted separations: message passing captures inter-row dependencies that row-local models miss, and hybrid approaches that explicitly extract inter-row structure and feed it to strong tabular learners yield consistent gains.

</details>


### [65] [Child Mortality Prediction in Bangladesh: A Decade-Long Validation Study](https://arxiv.org/abs/2602.03957)
*Md Muhtasim Munif Fahim,Md Rezaul Karim*

Main category: cs.LG

TL;DR: Researchers developed a neural network model using Bangladesh DHS data (2011-2022) with temporal validation to predict child mortality, avoiding look-ahead bias. The model outperformed XGBoost and works best in poorer regions, potentially identifying 1,300+ additional at-risk children annually for targeted health interventions.


<details>
  <summary>Details</summary>
Motivation: To address look-ahead bias in child mortality prediction models caused by random cross-validation, which makes them inaccurate for future populations, and to create a robust model for effective maternal and child health interventions.

Method: Used DHS data from Bangladesh (n=33,962) with temporal split: trained on 2011-2014 data, validated on 2017 data, tested on 2022 data. Applied genetic algorithm-based Neural Architecture Search to find optimal architecture, compared against XGBoost, and conducted fairness audit with SHAP values and Platt Calibration.

Result: A single-layer neural network (64 units) achieved superior performance to XGBoost (AUROC 0.76 vs 0.73, p<0.01). Identified a "Socioeconomic Predictive Gradient" showing negative correlation between regional poverty and model performance (r=-0.62), with highest AUC in poorest divisions (0.74) versus wealthiest (0.66). The model would identify approximately 1,300 additional at-risk children annually at 10% screening level.

Conclusion: The temporally-validated neural network provides a production-ready computational phenotype that accurately predicts child mortality and prioritizes high-need, socioeconomically disadvantaged regions, enabling more effective targeted interventions.

Abstract: The predictive machine learning models for child mortality tend to be inaccurate when applied to future populations, since they suffer from look-ahead bias due to the randomization used in cross-validation. The Demographic and Health Surveys (DHS) data from Bangladesh for 2011-2022, with n = 33,962, are used in this paper. We trained the model on (2011-2014) data, validated it on 2017 data, and tested it on 2022 data. Eight years after the initial test of the model, a genetic algorithm-based Neural Architecture Search found a single-layer neural architecture (with 64 units) to be superior to XGBoost (AUROC = 0.76 vs. 0.73; p < 0.01). Additionally, through a detailed fairness audit, we identified an overall "Socioeconomic Predictive Gradient," with a positive correlation between regional poverty level (r = -0.62) and the algorithm's AUC. In addition, we found that the model performed at its highest levels in the least affluent divisions (AUC 0.74) and decreased dramatically in the wealthiest divisions (AUC 0.66). These findings suggest that the model is identifying areas with the greatest need for intervention. Our model would identify approximately 1300 additional at-risk children annually than a Gradient Boosting model when screened at the 10% level and validated using SHAP values and Platt Calibration, and therefore provide a robust, production-ready computational phenotype for targeted maternal and child health interventions.

</details>


### [66] [eCP: Informative uncertainty quantification via Equivariantized Conformal Prediction with pre-trained models](https://arxiv.org/abs/2602.03986)
*Nikolaos Bousias,Lars Lindemann,George Pappas*

Main category: cs.LG

TL;DR: 该论文通过群对称化预训练模型来改进共形预测，利用几何对称性减少长时域任务中的不确定性区域膨胀。


<details>
  <summary>Details</summary>
Motivation: 共形预测虽提供分布无关的有限样本不确定性量化保证，但在长时域任务中不确定性区域会显著增长，导致统计保证失去信息量。

Method: 提出通过群平均预训练预测器将几何信息注入共形预测，在对称群轨道上分布非符合质量，将每个样本视为轨道代表以缓解不确定性。

Result: 该方法在递增凸序下可收缩非符合性得分，带来改进的指数尾界和更尖锐的预测集，尤其在高置信水平下效果显著。

Conclusion: 群对称化能有效缓解共形预测中的不确定性区域增长，理论结果通过行人轨迹预测实验验证。

Abstract: We study the effect of group symmetrization of pre-trained models on conformal prediction (CP), a post-hoc, distribution-free, finite-sample method of uncertainty quantification that offers formal coverage guarantees under the assumption of data exchangeability. Unfortunately, CP uncertainty regions can grow significantly in long horizon missions, rendering the statistical guarantees uninformative. To that end, we propose infusing CP with geometric information via group-averaging of the pretrained predictor to distribute the non-conformity mass across the orbits. Each sample now is treated as a representative of an orbit, thus uncertainty can be mitigated by other samples entangled to it via the orbit inducing elements of the symmetry group. Our approach provably yields contracted non-conformity scores in increasing convex order, implying improved exponential-tail bounds and sharper conformal prediction sets in expectation, especially at high confidence levels. We then propose an experimental design to test these theoretical claims in pedestrian trajectory prediction.

</details>


### [67] [When Chains of Thought Don't Matter: Causal Bypass in Large Language Models](https://arxiv.org/abs/2602.03994)
*Anish Sathyanarayanan,Aditya Nagarsekar,Aarush Rathore*

Main category: cs.LG

TL;DR: Chain-of-thought prompting fails to provide genuine reasoning transparency: model answers often don't causally depend on the shown reasoning steps, even when they appear verbally plausible.


<details>
  <summary>Details</summary>
Motivation: Challenging the widespread assumption that CoT prompting reveals models' true reasoning process by testing whether surface-level reasoning compliance implies actual causal reliance.

Method: Developed a diagnostic framework combining (1) an interpretable behavioral module detecting manipulation signals in CoT text, and (2) a causal probe using hidden-state patching to measure CoT-mediated influence (CMI) and calculate bypass scores.

Result: Found pervasive "bypass circuits" where answers are independent of CoT content (CMI≈0 in most QA tasks, up to 0.56 in some logic problems); audit-aware prompting increased surface signals (+5.10 risk-score) but didn't ensure causal mediation.

Conclusion: CoT's transparency assumption is fundamentally flawed: verbose reasoning doesn't guarantee it's causally used, revealing a critical failure mode in interpretability methods that requires causal verification beyond surface analysis.

Abstract: Chain-of-thought (CoT) prompting is widely assumed to expose a model's reasoning process and improve transparency. We attempted to enforce this assumption by penalizing unfaithful reasoning, but found that surface-level compliance does not guarantee causal reliance. Our central finding is negative: even when CoT is verbose, strategic, and flagged by surface-level manipulation detectors, model answers are often causally independent of the CoT content. We present a diagnostic framework for auditing this failure mode: it combines (i) an interpretable behavioral module that scores manipulation-relevant signals in CoT text and (ii) a causal probe that measures CoT-mediated influence (CMI) via hidden-state patching and reports a bypass score ($1-\mathrm{CMI}$), quantifying the degree to which the answer is produced by a bypass circuit independent of the rationale. In pilot evaluations, audit-aware prompting increases detectable manipulation signals (mean risk-score delta: $+5.10$), yet causal probes reveal task-dependent mediation: many QA items exhibit near-total bypass (CMI $\approx 0$), while some logic problems show stronger mediation (CMI up to $0.56$). Layer-wise analysis reveals narrow and task-dependent ``reasoning windows'' even when mean CMI is low.

</details>


### [68] [PromptSplit: Revealing Prompt-Level Disagreement in Generative Models](https://arxiv.org/abs/2602.04009)
*Mehdi Lotfian,Mohammad Jalali,Farzan Farnia*

Main category: cs.LG

TL;DR: 提出PromptSplit，一种基于核方法的框架，通过分析联合提示-输出表示并使用随机投影实现可扩展性，来检测生成式模型间的提示依赖性差异。


<details>
  <summary>Details</summary>
Motivation: 随着提示引导的生成式AI模型在视觉和语言领域的快速普及，以及使用不同数据和架构训练的模型种类不断增加，亟需原则性方法来识别哪些类型的提示会导致不同的模型行为。

Method: 构建提示与图像/文本特征的张量积嵌入作为联合表示，计算对应的核协方差矩阵，利用两矩阵加权差异的特征空间识别行为差异的主要方向，并采用随机投影近似将计算复杂度降至O(nr² + r³)。

Result: 在文本到图像、文本到文本和图像字幕生成任务上的实验表明，PromptSplit能准确检测真实的行为差异并分离出导致差异的关键提示。

Conclusion: PromptSplit提供了一个可解释的工具，用于检测生成式模型在哪些方面存在分歧，通过提示分析帮助理解模型行为的差异。

Abstract: Prompt-guided generative AI models have rapidly expanded across vision and language domains, producing realistic and diverse outputs from textual inputs. The growing variety of such models, trained with different data and architectures, calls for principled methods to identify which types of prompts lead to distinct model behaviors. In this work, we propose PromptSplit, a kernel-based framework for detecting and analyzing prompt-dependent disagreement between generative models. For each compared model pair, PromptSplit constructs a joint prompt--output representation by forming tensor-product embeddings of the prompt and image (or text) features, and then computes the corresponding kernel covariance matrix. We utilize the eigenspace of the weighted difference between these matrices to identify the main directions of behavioral difference across prompts. To ensure scalability, we employ a random-projection approximation that reduces computational complexity to $O(nr^2 + r^3)$ for projection dimension $r$. We further provide a theoretical analysis showing that this approximation yields an eigenstructure estimate whose expected deviation from the full-dimensional result is bounded by $O(1/r^2)$. Experiments across text-to-image, text-to-text, and image-captioning settings demonstrate that PromptSplit accurately detects ground-truth behavioral differences and isolates the prompts responsible, offering an interpretable tool for detecting where generative models disagree.

</details>


### [69] [Understanding and Guiding Layer Placement in Parameter-Efficient Fine-Tuning of Large Language Models](https://arxiv.org/abs/2602.04019)
*Yichen Xu,Yuyang Liang,Shan Dai,Tianyang Hu,Tsz Nam Chan,Chenhao Ma*

Main category: cs.LG

TL;DR: 该论文提出了一种统一的投影残差视角来理解参数高效微调(PEFT)，并引入Layer Card诊断工具，通过分析各层的残差信号强度、计算成本和性能，实现选择性层微调，在保持接近全层LoRA性能的同时显著降低微调成本和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模增长，全参数微调成本过高，PEFT成为默认策略。然而当前实践通常在所有层上统一应用PEFT，缺乏对层选择策略的深入理解。推理延迟和边缘部署成本约束使得层选择不可避免，但如何科学地选择适配层仍是一个未解决的问题。

Method: 1. 建立基于冻结基础模型的PEFT统一投影残差理论框架；2. 在局部二次近似下，提出层自适应的三个关键量：投影残差范数(衡量层可捕获的可纠正偏差)、激活能量(决定特征条件)和层耦合(量化层间残差交互强度)；3. 证明对于平方损失和线性适配器，残差范数等于归一化梯度范数，激活能量控制病态条件与噪声放大，弱耦合产生近似可加的层贡献；4. 引入Layer Card可复用诊断工具，汇总每层的残差信号强度、计算成本和性能。

Result: 在Qwen3-8B模型上，基于Layer Card指导的选择性层微调实现了：1) 性能接近全层LoRA；2) 显著降低微调成本；3) 减少推理时适配器增强层数量。该方法提供了性能与成本感知的替代方案。

Conclusion: 该研究通过理论分析和实用工具Layer Card，建立了PEFT层选择的科学框架，使从业者能够根据性能或成本目标灵活选择适配层，实现了成本-性能权衡的精细控制，为资源受限场景下的模型微调提供了有效解决方案。

Abstract: As large language models (LLMs) continue to grow, the cost of full-parameter fine-tuning has made parameter-efficient fine-tuning (PEFT) the default strategy for downstream adaptation. Constraints from inference latency in scalable serving and fine-tuning cost in edge or rapid-deployment settings make the choice of which layers to fine-tune unavoidable. Yet current practice typically applies PEFT uniformly across all layers, with limited understanding or leverage of layer selection. This paper develops a unified projected residual view of PEFT on top of a frozen base model. Under a local quadratic approximation, layerwise adaptation is governed by three quantities: (i) the projected residual norm (resnorm), which measures how much correctable bias a layer can capture; (ii) the activation energy, which determines feature conditioning; and (iii) layer coupling, which quantifies how strongly residuals interact across layers. We show that, for squared loss and linear adapters, the resnorm equals a normalized gradient norm, activation energy controls ill-conditioning and noise amplification, and weak coupling yields approximately additive layerwise contributions. Building on these insights, we introduce the Layer Card, a reusable diagnostic that summarizes residual signal strength, compute cost, and performance for each layer of a given model. With an identical model and LoRA configuration, Layer Card-guided placement refines the choice of adapted layers to flexibly prioritize different objectives, such as maximizing performance or reducing fine-tuning cost. Moreover, on Qwen3-8B, we show that selectively adapting a subset of layers can achieve performance close to full-layer LoRA while substantially reducing fine-tuning cost and the number of adapter-augmented layers during inference, offering a more cost-performance-aware alternative to full-layer insertion.

</details>


### [70] [Group Contrastive Learning for Weakly Paired Multimodal Data](https://arxiv.org/abs/2602.04021)
*Aditya Gorla,Hugues Van Assel,Jan-Christian Huetter,Heming Yao,Kyunghyun Cho,Aviv Regev,Russell Littman*

Main category: cs.LG

TL;DR: GROOVE是一种针对弱配对多模态扰动数据的半监督表示学习方法，核心是GroupCLIP分组对比损失，结合实时回译自编码器框架和组合评估体系，在跨模态匹配和补全任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习在弱配对场景存在根本性空白：CLIP需要精确配对数据，SupCon仅支持单模态监督，而高含量扰动数据中不同模态样本仅通过扰动标签弱关联，缺乏直接对应关系，亟需新方法桥接这一鸿沟。

Method: 提出GROOVE框架，创新性地设计GroupCLIP分组级对比损失，连接CLIP与SupCon；集成实时回译自编码器促进跨模态纠缠表示；构建组合评估框架系统检验多种最优传输对齐器，并通过模拟系统改变共享与模态特异性扰动效应。

Result: 在模拟数据和两个真实单细胞遗传扰动数据集上，GROOVE在跨模态匹配和补全任务上达到或超越现有方法；消融实验证实GroupCLIP是关键驱动力；评估显示尚无对齐器在所有场景下统一占优。

Conclusion: 在仅有弱配对可用的场景下，利用分组级约束对多模态表示学习至关重要，该方法有效填补了对比学习在弱配对设置中的基础空白。

Abstract: We present GROOVE, a semi-supervised multi-modal representation learning approach for high-content perturbation data where samples across modalities are weakly paired through shared perturbation labels but lack direct correspondence. Our primary contribution is GroupCLIP, a novel group-level contrastive loss that bridges the gap between CLIP for paired cross-modal data and SupCon for uni-modal supervised contrastive learning, addressing a fundamental gap in contrastive learning for weakly-paired settings. We integrate GroupCLIP with an on-the-fly backtranslating autoencoder framework to encourage cross-modally entangled representations while maintaining group-level coherence within a shared latent space. Critically, we introduce a comprehensive combinatorial evaluation framework that systematically assesses representation learners across multiple optimal transport aligners, addressing key limitations in existing evaluation strategies. This framework includes novel simulations that systematically vary shared versus modality-specific perturbation effects enabling principled assessment of method robustness. Our combinatorial benchmarking reveals that there is not yet an aligner that uniformly dominates across settings or modality pairs. Across simulations and two real single-cell genetic perturbation datasets, GROOVE performs on par with or outperforms existing approaches for downstream cross-modal matching and imputation tasks. Our ablation studies demonstrate that GroupCLIP is the key component driving performance gains. These results highlight the importance of leveraging group-level constraints for effective multi-modal representation learning in scenarios where only weak pairing is available.

</details>


### [71] [A Consensus-Bayesian Framework for Detecting Malicious Activity in Enterprise Directory Access Graphs](https://arxiv.org/abs/2602.04027)
*Pratyush Uppuluri,Shilpa Noushad,Sajan Kumar*

Main category: cs.LG

TL;DR: A Bayesian framework using opinion dynamics on directory access graphs detects malicious behavior by identifying anomalies in user access patterns through convergence analysis and dynamic uncertainty scoring.


<details>
  <summary>Details</summary>
Motivation: Malicious users in enterprise directories create logical inconsistencies in access patterns that violate structural norms of user behavior, requiring detection methods that capture complex cross-component dependencies and quantify uncertainty.

Method: Models directories as topics and users as agents in a multi-level graph, applying influence-weighted opinion dynamics with dynamic dependency matrices Ci and shared influence matrix W. Detects anomalies via scaled opinion variance and theoretical convergence guarantees, using Bayesian scoring with static and online priors.

Result: Synthetic graph simulations demonstrate high sensitivity to logical inconsistencies from malicious perturbations and robustness under dynamic changes, effectively quantifying detection uncertainty.

Conclusion: The consensus-based Bayesian framework successfully identifies malicious behavior by detecting violations of structural norms in strongly connected components, providing a principled approach for enterprise security with uncertainty quantification.

Abstract: This work presents a consensus-based Bayesian framework to detect malicious user behavior in enterprise directory access graphs. By modeling directories as topics and users as agents within a multi-level interaction graph, we simulate access evolution using influence-weighted opinion dynamics. Logical dependencies between users are encoded in dynamic matrices Ci, and directory similarity is captured via a shared influence matrix W. Malicious behavior is injected as cross-component logical perturbations that violate structural norms of strongly connected components(SCCs). We apply theoretical guarantees from opinion dynamics literature to determine topic convergence and detect anomaly via scaled opinion variance. To quantify uncertainty, we introduce a Bayesian anomaly scoring mechanism that evolves over time, using both static and online priors. Simulations over synthetic access graphs validate our method, demonstrating its sensitivity to logical inconsistencies and robustness under dynamic perturbation.

</details>


### [72] [The Illusion of Generalization: Re-examining Tabular Language Model Evaluation](https://arxiv.org/abs/2602.04031)
*Aditya Gorla,Ratish Puduppully*

Main category: cs.LG

TL;DR: This paper systematically re-evaluates Tabula-8B, a Tabular Language Model, using 165 datasets from the UniPredict benchmark. It finds that TLMs' claimed emergent generalization stems from evaluation artifacts like dataset contamination and task-specific quirks rather than genuine tabular reasoning abilities. Instruction-tuning without tabular exposure nearly matches performance, suggesting format familiarity rather than domain learning drives results.


<details>
  <summary>Details</summary>
Motivation: The authors were motivated by claims that Tabular Language Models achieve emergent generalization for tabular prediction, but suspected these claims might be inflated due to evaluation artifacts. They aimed to conduct a rigorous, large-scale re-evaluation to verify whether TLMs truly learn tabular reasoning or if performance gains reflect methodological flaws.

Method: The researchers performed a systematic re-evaluation of Tabula-8B using 165 datasets from the UniPredict benchmark. They analyzed performance across different task types (binary, categorical, and quartile classification), investigated dataset contamination issues, and conducted controlled experiments comparing standard TLMs against instruction-tuned models without tabular exposure to isolate the factors driving performance.

Result: Three key findings emerged: (1) Binary and categorical classification showed near-zero median lift over majority-class baselines, with strong aggregate performance driven entirely by quartile classification tasks; (2) Top-performing datasets exhibited pervasive contamination including complete train-test overlap and task-level leakage; (3) Instruction-tuning without tabular exposure recovered 92.2% of standard classification performance, and format familiarity closed 71.3% of the gap for quartile classification, with the residual gap attributable to contaminated datasets.

Conclusion: The study concludes that claimed TLM generalization likely reflects evaluation artifacts rather than learned tabular reasoning. The authors provide recommendations for strengthening TLM evaluation, including improved dataset curation, contamination detection, and more rigorous experimental designs to ensure future claims reflect genuine model capabilities.

Abstract: Tabular Language Models (TLMs) have been claimed to achieve emergent generalization for tabular prediction. We conduct a systematic re-evaluation of Tabula-8B as a representative TLM, utilizing 165 datasets from the UniPredict benchmark. Our investigation reveals three findings. First, binary and categorical classification achieve near-zero median lift over majority-class baselines and strong aggregate performance is driven entirely by quartile classification tasks. Second, top-performing datasets exhibit pervasive contamination, including complete train-test overlap and task-level leakage that evades standard deduplication. Third, instruction-tuning without tabular exposure recovers 92.2% of standard classification performance and on quartile classification, format familiarity closes 71.3% of the gap with the residual attributable to contaminated datasets. These findings suggest claimed generalization likely reflects evaluation artifacts rather than learned tabular reasoning. We conclude with recommendations for strengthening TLM evaluation.

</details>


### [73] [DADP: Domain Adaptive Diffusion Policy](https://arxiv.org/abs/2602.04037)
*Pengcheng Wang,Qinghang Liu,Haotian Lin,Yiheng Li,Guojian Zhan,Masayoshi Tomizuka,Yixiao Wang*

Main category: cs.LG

TL;DR: 提出DADP（域自适应扩散策略），通过滞后上下文动态预测实现无监督域信息解耦，并将解耦后的域表示注入扩散生成过程，在运动和操控基准测试中实现卓越的零样本适应性能。


<details>
  <summary>Details</summary>
Motivation: 学习域自适应策略以泛化到未见过的动态转移是学习控制的基本挑战。现有方法通过域表示学习捕获域特定信息，但当前相邻上下文选择会导致静态域信息与动态特性纠缠，混淆条件策略，限制零样本适应能力。

Method: 提出DADP框架：1）滞后上下文动态预测策略，通过增加历史偏移上下文的时间间隔，无监督地解耦静态域表示；2）将学习到的域表示直接集成到生成过程中，通过偏置先验分布和重构扩散目标实现域感知的扩散注入。

Result: 在运动和操控领域的挑战性基准测试中，DADP表现出卓越的性能和泛化能力，优于先前方法。更多可视化结果可在项目网站查看。

Conclusion: DADP通过无监督解耦和域感知扩散注入实现了鲁棒的域适应，有效解决了域表示中静态与动态信息纠缠的问题，为学习基础控制提供了新思路。

Abstract: Learning domain adaptive policies that can generalize to unseen transition dynamics, remains a fundamental challenge in learning-based control. Substantial progress has been made through domain representation learning to capture domain-specific information, thus enabling domain-aware decision making. We analyze the process of learning domain representations through dynamical prediction and find that selecting contexts adjacent to the current step causes the learned representations to entangle static domain information with varying dynamical properties. Such mixture can confuse the conditioned policy, thereby constraining zero-shot adaptation. To tackle the challenge, we propose DADP (Domain Adaptive Diffusion Policy), which achieves robust adaptation through unsupervised disentanglement and domain-aware diffusion injection. First, we introduce Lagged Context Dynamical Prediction, a strategy that conditions future state estimation on a historical offset context; by increasing this temporal gap, we unsupervisedly disentangle static domain representations by filtering out transient properties. Second, we integrate the learned domain representations directly into the generative process by biasing the prior distribution and reformulating the diffusion target. Extensive experiments on challenging benchmarks across locomotion and manipulation demonstrate the superior performance, and the generalizability of DADP over prior methods. More visualization results are available on the https://outsider86.github.io/DomainAdaptiveDiffusionPolicy/.

</details>


### [74] [Partition Trees: Conditional Density Estimation over General Outcome Spaces](https://arxiv.org/abs/2602.04042)
*Felipe Angelim,Alessandro Leite*

Main category: cs.LG

TL;DR: 本文提出Partition Trees，一个用于通用结果空间条件密度估计的树框架，统一支持连续和分类变量。该方法通过最小化条件负对数似然学习数据自适应的分段常数密度，提供可扩展的非参数替代方案。


<details>
  <summary>Details</summary>
Motivation: 现有概率树方法通常对目标分布有参数假设，且难以统一处理不同类型结果变量。缺乏一种可扩展、非参数且适用于通用结果空间的条件密度估计框架。

Method: 提出Partition Trees：将条件分布建模为数据自适应划分上的分段常数密度，通过直接最小化条件负对数似然进行树学习。进一步引入Partition Forests，通过集成平均条件密度来提升性能。

Result: 实验表明，该方法在概率预测上显著优于CART树；与当前先进概率树方法及随机森林相比具有竞争力或更优性能；同时对冗余特征和异方差噪声表现出良好鲁棒性。

Conclusion: Partition Trees为通用结果空间的条件密度估计提供了有效、可扩展的非参数解决方案，具有理论和实践价值。

Abstract: We propose Partition Trees, a tree-based framework for conditional density estimation over general outcome spaces, supporting both continuous and categorical variables within a unified formulation. Our approach models conditional distributions as piecewise-constant densities on data adaptive partitions and learns trees by directly minimizing conditional negative log-likelihood. This yields a scalable, nonparametric alternative to existing probabilistic trees that does not make parametric assumptions about the target distribution. We further introduce Partition Forests, an ensemble extension obtained by averaging conditional densities. Empirically, we demonstrate improved probabilistic prediction over CART-style trees and competitive or superior performance compared to state-of-the-art probabilistic tree methods and Random Forests, along with robustness to redundant features and heteroscedastic noise.

</details>


### [75] [Agentic AI-Empowered Dynamic Survey Framework](https://arxiv.org/abs/2602.04071)
*Furkan Mumcu,Lokman Bekit,Michael J. Jones,Anoop Cherian,Yasin Yilmaz*

Main category: cs.LG

TL;DR: Proposing a Dynamic Survey Framework to treat survey papers as living documents that can be continuously updated to avoid obsolescence.


<details>
  <summary>Details</summary>
Motivation: Survey papers become outdated quickly due to rapid research growth, causing redundancy and fragmentation in literature.

Method: Proposes an agentic Dynamic Survey Framework for continuous, incremental updating of surveys while preserving structure.

Result: Retrospective experiments show the framework effectively identifies and incorporates new research while maintaining survey coherence.

Conclusion: The framework successfully addresses survey obsolescence by enabling living, evolving survey documents.

Abstract: Survey papers play a central role in synthesizing and organizing scientific knowledge, yet they are increasingly strained by the rapid growth of research output. As new work continues to appear after publication, surveys quickly become outdated, contributing to redundancy and fragmentation in the literature. We reframe survey writing as a long-horizon maintenance problem rather than a one-time generation task, treating surveys as living documents that evolve alongside the research they describe. We propose an agentic Dynamic Survey Framework that supports the continuous updating of existing survey papers by incrementally integrating new work while preserving survey structure and minimizing unnecessary disruption. Using a retrospective experimental setup, we demonstrate that the proposed framework effectively identifies and incorporates emerging research while preserving the coherence and structure of existing surveys.

</details>


### [76] [Stroke Lesions as a Rosetta Stone for Language Model Interpretability](https://arxiv.org/abs/2602.04074)
*Julius Fridriksson,Roger D. Newman-Norlund,Saeed Ahmadi,Regan Willis,Nadra Salman,Kalil Warren,Xiang Guan,Yong Yang,Srihari Nelakuditi,Rutvik Desai,Leonardo Bonilha,Jeff Charney,Chris Rorden*

Main category: cs.LG

TL;DR: 本文提出BLUM框架，利用失语症患者的脑损伤-症状映射作为外部验证，评估大语言模型中哪些组件对语言功能真正必要，发现模型错误与脑损伤模式高度一致。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的可解释性方法缺乏外部验证，仅依赖内部指标。本研究旨在利用临床神经科学中百年来的金标准——脑损伤-症状映射，来建立对模型语言组件的因果理解。

Method: BLUM框架使用410名中风后失语症患者的训练症状-损伤模型，系统性地扰动Transformer层，对扰动后的模型和人类患者施测相同的临床评估，并将模型错误模式映射到人类脑损伤空间进行比较。

Result: 模型错误模式与人类错误模式显著匹配，预测的脑损伤位置与真实损伤在67%的图片命名条件（p < 10^-23）和68.3%的句子完成条件（p < 10^-61）中高于随机水平。语义错误主要映射到腹侧流损伤，语音错误主要映射到背侧流损伤。

Conclusion: 该研究建立了人类脑损伤-症状映射作为评估人工语言系统的参考框架，为模型可解释性开辟了新方法，表明模型与人类的语言行为一致性可能反映了共同的计算原理。

Abstract: Large language models (LLMs) have achieved remarkable capabilities, yet methods to verify which model components are truly necessary for language function remain limited. Current interpretability approaches rely on internal metrics and lack external validation. Here we present the Brain-LLM Unified Model (BLUM), a framework that leverages lesion-symptom mapping, the gold standard for establishing causal brain-behavior relationships for over a century, as an external reference structure for evaluating LLM perturbation effects. Using data from individuals with chronic post-stroke aphasia (N = 410), we trained symptom-to-lesion models that predict brain damage location from behavioral error profiles, applied systematic perturbations to transformer layers, administered identical clinical assessments to perturbed LLMs and human patients, and projected LLM error profiles into human lesion space. LLM error profiles were sufficiently similar to human error profiles that predicted lesions corresponded to actual lesions in error-matched humans above chance in 67% of picture naming conditions (p < 10^{-23}) and 68.3% of sentence completion conditions (p < 10^{-61}), with semantic-dominant errors mapping onto ventral-stream lesion patterns and phonemic-dominant errors onto dorsal-stream patterns. These findings open a new methodological avenue for LLM interpretability in which clinical neuroscience provides external validation, establishing human lesion-symptom mapping as a reference framework for evaluating artificial language systems and motivating direct investigation of whether behavioral alignment reflects shared computational principles.

</details>


### [77] [A Probabilistic Framework for Solving High-Frequency Helmholtz Equations via Diffusion Models](https://arxiv.org/abs/2602.04082)
*Yicheng Zou,Samuel Lanthaler,Hossein Salahshoor*

Main category: cs.LG

TL;DR: 提出一种基于分数的条件扩散算子概率框架，用于解决确定性神经算子在近似高频波现象时的困难，在Helmholtz方程实验中表现优于其他方法并能量化不确定性


<details>
  <summary>Details</summary>
Motivation: 确定性神经算子在处理高频波现象时面临输入输出强敏感性、谱偏差导致振荡模糊等问题，难以准确捕获高频波传播中的不确定性

Method: 采用基于分数的条件扩散算子构建概率神经算子，先对Helmholtz算子进行稳定性分析，再在不同频率下进行数值实验，并与多种数据驱动和机器学习方法对比

Result: 在L²、H¹和能量范数上持续获得最低误差，且唯一能捕获输入声速图不确定性传播到解场的特性，预测结果更加鲁棒

Conclusion: 概率算子学习为高频 regime 下求解Helmholtz等复杂PDE提供了原则性且有效的新途径，具有重要应用前景

Abstract: Deterministic neural operators perform well on many PDEs but can struggle with the approximation of high-frequency wave phenomena, where strong input-to-output sensitivity makes operator learning challenging, and spectral bias blurs oscillations. We argue for adopting a probabilistic approach for approximating waves in high-frequency regime, and develop our probabilistic framework using a score-based conditional diffusion operator. After demonstrating a stability analysis of the Helmholtz operator, we present our numerical experiments across a wide range of frequencies, benchmarked against other popular data-driven and machine learning approaches for waves. We show that our probabilistic neural operator consistently produces robust predictions with the lowest errors in $L^2$, $H^1$, and energy norms. Moreover, unlike all the other tested deterministic approaches, our framework remarkably captures uncertainties in the input sound speed map propagated to the solution field. We envision that our results position probabilistic operator learning as a principled and effective approach for solving complex PDEs such as Helmholtz in the challenging high-frequency regime.

</details>


### [78] [Federated Concept-Based Models: Interpretable models with distributed supervision](https://arxiv.org/abs/2602.04093)
*Dario Fenoglio,Arianna Casanova,Francesco De Santis,Mohan Li,Gabriele Dominici,Johannes Schneider,Martin Gjoreski,Marc Langheinrich,Pietro Barbiero,Giovanni De Felice*

Main category: cs.LG

TL;DR: 提出联邦概念模型(F-CMs)，将概念模型的可解释性与联邦学习的隐私保护结合，动态适应机构加入和概念监督变化。


<details>
  <summary>Details</summary>
Motivation: 概念模型需要昂贵标注，联邦学习可跨机构训练但缺乏可解释性，且真实联邦场景异构、非平稳，现有CMs无法适应动态变化的概念空间和架构。

Method: F-CMs聚合跨机构概念级信息，动态调整模型架构以响应概念监督变化，同时保护机构隐私。

Result: F-CMs在精度和干预效果上媲美全监督训练，优于非自适应联邦基线，且能对单个机构不可见的概念进行可解释推理。

Conclusion: F-CMs成功在动态真实联邦学习场景中实现可解释建模，使机构无需共享原始数据即可共享概念知识。

Abstract: Concept-based models (CMs) enhance interpretability in deep learning by grounding predictions in human-understandable concepts. However, concept annotations are expensive to obtain and rarely available at scale within a single data source. Federated learning (FL) could alleviate this limitation by enabling cross-institutional training that leverages concept annotations distributed across multiple data owners. Yet, FL lacks interpretable modeling paradigms. Integrating CMs with FL is non-trivial: CMs assume a fixed concept space and a predefined model architecture, whereas real-world FL is heterogeneous and non-stationary, with institutions joining over time and bringing new supervision. In this work, we propose Federated Concept-based Models (F-CMs), a new methodology for deploying CMs in evolving FL settings. F-CMs aggregate concept-level information across institutions and efficiently adapt the model architecture in response to changes in the available concept supervision, while preserving institutional privacy. Empirically, F-CMs preserve the accuracy and intervention effectiveness of training settings with full concept supervision, while outperforming non-adaptive federated baselines. Notably, F-CMs enable interpretable inference on concepts not available to a given institution, a key novelty with respect to existing approaches.

</details>


### [79] [CoRe: Context-Robust Remasking for Diffusion Language Models](https://arxiv.org/abs/2602.04096)
*Kevin Zhai,Sabbir Mollah,Zhenyi Wang,Mubarak Shah*

Main category: cs.LG

TL;DR: 提出Context-Robust Remasking (CoRe)框架，通过探测标记对掩码上下文扰动的敏感性来识别上下文脆弱标记，实现无需训练的抗级联错误修订。


<details>
  <summary>Details</summary>
Motivation: 标准解码存在上下文僵化问题：早期预测缺乏完整上下文，导致初始不一致会误导后续生成，形成级联错误。现有静态置信度修订策略存在短视缺陷。

Method: CoRe框架将修订形式化为上下文偏移上的鲁棒优化目标，通过目标掩码上下文扰动高效探测标记稳定性，优先修订不稳定标记。

Result: 在LLaDA-8B-Base上实现持续改进，在推理和代码基准测试中超越计算匹配基线，MBPP最高提升9.2个百分点。

Conclusion: 该无需训练的推理时修订方法能有效解决上下文脆弱性导致的级联错误问题，提升生成质量。

Abstract: Standard decoding in Masked Diffusion Models (MDMs) is hindered by context rigidity: tokens are retained based on transient high confidence, often ignoring that early predictions lack full context. This creates cascade effects where initial inconsistencies misguide the remaining generation. Existing revision strategies attempt to mitigate this by relying on static confidence scores, but these signals are inherently myopic; inconsistent tokens can appear confident to the model itself. We propose Context-Robust Remasking (CoRe), a training-free framework for inference-time revision. Rather than trusting static token probabilities, CoRe identifies context-brittle tokens by probing their sensitivity to targeted masked-context perturbations. We formalize revision as a robust optimization objective over context shifts and efficiently approximate this objective to prioritize unstable tokens for revision. On LLaDA-8B-Base, CoRe delivers consistent improvements across reasoning and code benchmarks, outperforming compute-matched baselines and improving MBPP by up to 9.2 percentage points.

</details>


### [80] [Rethinking Perplexity: Revealing the Impact of Input Length on Perplexity Evaluation in LLMs](https://arxiv.org/abs/2602.04099)
*Letian Cheng,Junyan Wang,Yan Gao,Elliott Wen,Ting Dang,Hong Jia*

Main category: cs.LG

TL;DR: 该论文提出LengthBenchmark系统感知评估框架，首次将输入长度作为一级系统变量，揭示长度偏差是影响LLM公平比较的普遍现象，滑动窗口评估会虚高短文本性能，且模型表现随评估片段长度增长而提升。


<details>
  <summary>Details</summary>
Motivation: 困惑度作为LLM核心评估指标在长输入下不可靠，但输入长度对评估公平性和系统效率的影响缺乏系统性研究，现有方法仅做简单过滤而未将其视为关键系统变量。

Method: 开发LengthBenchmark框架，在两种评分协议（直接累积和固定窗口滑动）下评估代表性LLM，变化上下文长度，并测量延迟、内存占用和评估成本等系统级指标，同时纳入量化模型作为鲁棒性检验。

Result: 发现长度偏差在浮点和量化模型中普遍存在；滑动窗口评估会系统性虚高短输入性能；模型表现随评估片段长度增长而持续提升，证明长度是影响跨模型公平比较的关键因素。

Conclusion: 输入长度偏差是破坏LLM评估公平性的普遍现象，必须将其作为一级系统变量纳入评估协议设计，才能建立与部署现实一致的可靠基准测试体系。

Abstract: Perplexity is a widely adopted metric for assessing the predictive quality of large language models (LLMs) and often serves as a reference metric for downstream evaluations. However, recent evidence shows that perplexity can be unreliable, especially when irrelevant long inputs are used, raising concerns for both benchmarking and system deployment. While prior efforts have employed selective input filtering and curated datasets, the impact of input length on perplexity has not been systematically studied from a systems perspective and input length has rarely been treated as a first-class system variable affecting both fairness and efficiency. In this work, we close this gap by introducing LengthBenchmark, a system-conscious evaluation framework that explicitly integrates input length, evaluation protocol design, and system-level costs, evaluating representative LLMs under two scoring protocols (direct accumulation and fixed window sliding) across varying context lengths. Unlike prior work that focuses solely on accuracy-oriented metrics, LengthBenchmark additionally measures latency, memory footprint, and evaluation cost, thereby linking predictive metrics to deployment realities. We further incorporate quantized variants not as a main contribution, but as robustness checks, showing that length-induced biases persist across both full-precision and compressed models. This design disentangles the effects of evaluation logic, quantization, and input length, and demonstrates that length bias is a general phenomenon that undermines fair cross-model comparison. Our analysis yields two key observations: (i) sliding window evaluation consistently inflates performance on short inputs, and (ii) both full-precision and quantized models appear to realise gains as the evaluated segment length grows.

</details>


### [81] [Supervised Learning as Lossy Compression: Characterizing Generalization and Sample Complexity via Finite Blocklength Analysis](https://arxiv.org/abs/2602.04107)
*Kosuke Sugiyama,Masato Uchida*

Main category: cs.LG

TL;DR: 本文从信息论角度研究机器学习泛化问题，将学习过程框架为有损压缩问题，利用有限块长分析推导出样本复杂度和泛化误差的下界，并首次将过拟合与归纳偏置不匹配分离为独立项，统一了信息论边界与稳定性理论。


<details>
  <summary>Details</summary>
Motivation: 现有泛化理论框架难以明确区分过拟合与归纳偏置不匹配这两个不同概念，缺乏对学习算法过拟合程度的精确刻画。

Method: 将训练数据采样形式化为编码过程，模型构建视为解码过程，引入有限块长分析技术，在固定随机学习算法及其最优采样策略下进行分析。

Result: 推导出样本复杂度和泛化误差的理论下界，得到了显式分离的过拟合项和归纳偏置不匹配项，并揭示了过拟合项与信息论边界、稳定性理论中现有度量的理论联系。

Conclusion: 该框架不仅提供了更精细的泛化误差分解，还统一了信息论与稳定性理论两种视角，为理解机器学习泛化行为提供了新理论基础。

Abstract: This paper presents a novel information-theoretic perspective on generalization in machine learning by framing the learning problem within the context of lossy compression and applying finite blocklength analysis. In our approach, the sampling of training data formally corresponds to an encoding process, and the model construction to a decoding process. By leveraging finite blocklength analysis, we derive lower bounds on sample complexity and generalization error for a fixed randomized learning algorithm and its associated optimal sampling strategy. Our bounds explicitly characterize the degree of overfitting of the learning algorithm and the mismatch between its inductive bias and the task as distinct terms. This separation provides a significant advantage over existing frameworks. Additionally, we decompose the overfitting term to show its theoretical connection to existing metrics found in information-theoretic bounds and stability theory, unifying these perspectives under our proposed framework.

</details>


### [82] [Rate-Optimal Noise Annealing in Semi-Dual Neural Optimal Transport: Tangential Identifiability, Off-Manifold Ambiguity, and Guaranteed Recovery](https://arxiv.org/abs/2602.04110)
*Raymond Chu,Jaewoong Choi,Dohyun Kwon*

Main category: cs.LG

TL;DR: 该论文针对半对偶神经最优传输中的虚假解问题，提出加性噪声平滑作为解决方案。通过理论分析，给出了一个可计算的终端噪声水平ε_stat(N)，该水平能达到仅依赖于数据本征维度m的最优统计速率。同时揭示过度减小噪声会使优化问题病态，为模型训练提供了理论指导的早停规则。


<details>
  <summary>Details</summary>
Motivation: 半对偶神经最优传输通过最大-最小目标函数学习传输映射，但训练过程可能收敛到错误或退化的解。当数据集中在低维流形时，目标函数在流形外欠约束，导致虚假解出现，影响了方法的可靠性和实用性。

Method: 基于Choi等人(2025)的研究，采用加性噪声平滑技术。通过统一分析最优计划的定量稳定性、平滑引入的偏差以及有限样本误差，证明当噪声趋于零时的映射恢复保证，并推导出可计算的终端噪声水平ε_stat(N)。

Result: 1) 完全刻画了数据在低维流形上时虚假解的特征；2) 证明了加性噪声平滑的有效性，并建立了新的映射恢复理论保证；3) 推导出终端噪声水平ε_stat(N)，该水平达到仅依赖于本征维度m的最优统计速率；4) 发现当ε→0时，半对偶目标函数变得病态。

Conclusion: 加性噪声平滑能有效解决半对偶神经最优传输的虚假解问题，但存在一个理论最优的噪声水平ε_stat(N)。训练时不应将噪声降至该水平以下，否则会恶化优化条件而无法提升统计精度。这为实际应用中的超参数选择和早停策略提供了理论基础。

Abstract: Semi-dual neural optimal transport learns a transport map via a max-min objective, yet training can converge to incorrect or degenerate maps. We fully characterize these spurious solutions in the common regime where data concentrate on low-dimensional manifold: the objective is underconstrained off the data manifold, while the on-manifold transport signal remains identifiable. Following Choi, Choi, and Kwon (2025), we study additive-noise smoothing as a remedy and prove new map recovery guarantees as the noise vanishes. Our main practical contribution is a computable terminal noise level $\varepsilon_{\mathrm{stat}}(N)$ that attains the optimal statistical rate, with scaling governed by the intrinsic dimension $m$ of the data. The formula arises from a theoretical unified analysis of (i) quantitative stability of optimal plans, (ii) smoothing-induced bias, and (iii) finite-sample error, yielding rates that depend on $m$ rather than the ambient dimension. Finally, we show that the reduced semi-dual objective becomes increasingly ill-conditioned as $\varepsilon \downarrow 0$. This provides a principled stopping rule: annealing below $\varepsilon_{\mathrm{stat}}(N)$ can $\textit{worsen}$ optimization conditioning without improving statistical accuracy.

</details>


### [83] [Toward Effective Multimodal Graph Foundation Model: A Divide-and-Conquer Based Approach](https://arxiv.org/abs/2602.04116)
*Sicheng Liu,Xunkai Li,Daohan Su,Ru Zhang,Hongchao Qin,Ronghua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 针对现有多模态图基础模型(MGFMs)在模态交互和对齐方面的不足，本文提出PLANET框架，通过分治策略在嵌入级和节点级分别实现模态交互和对齐，在多项任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图基础模型主要关注文本属性图，忽视了多模态属性图。即使是最新的MGFMs也存在两个根本限制：(1)无法显式建模模态交互，难以捕捉复杂的跨模态语义；(2)模态对齐效果不佳，无法有效弥合不同模态空间之间的语义差异。

Method: 提出PLANET框架，采用分治策略解耦模态交互和对齐：嵌入级使用嵌入域门控(EDG)，通过自适应注入拓扑感知的跨模态上下文实现局部语义增强和模态交互；节点级采用节点离散化检索(NDR)，通过构建离散语义表示空间(DSRS)实现全局模态对齐。

Result: 大量实验表明，PLANET在多样化的图中心和多模态生成任务上显著优于现有最先进基线模型。

Conclusion: PLANET框架通过分层处理模态交互和对齐问题，有效提升了多模态图基础模型的性能，为处理多模态图数据提供了新的解决方案。

Abstract: Graph Foundation Models (GFMs) have achieved remarkable success in generalizing across diverse domains. However, they mainly focus on Text-Attributed Graphs (TAGs), leaving Multimodal-Attributed Graphs (MAGs) largely untapped. Developing Multimodal Graph Foundation Models (MGFMs) allows for leveraging the rich multimodal information in MAGs, and extends applicability to broader types of downstream tasks. While recent MGFMs integrate diverse modality information, our empirical investigation reveals two fundamental limitations of existing MGFMs: (1)they fail to explicitly model modality interaction, essential for capturing intricate cross-modal semantics beyond simple aggregation, and (2)they exhibit sub-optimal modality alignment, which is critical for bridging the significant semantic disparity between distinct modal spaces. To address these challenges, we propose PLANET (graPh topoLogy-aware modAlity iNteraction and alignmEnT), a novel framework employing a Divide-and-Conquer strategy to decouple modality interaction and alignment across distinct granularities. At the embedding granularity, (1)Embedding-wise Domain Gating (EDG) performs local semantic enrichment by adaptively infusing topology-aware cross-modal context, achieving modality interaction. At the node granularity, (2)Node-wise Discretization Retrieval (NDR) ensures global modality alignment by constructing a Discretized Semantic Representation Space (DSRS) to bridge modality gaps. Extensive experiments demonstrate that PLANET significantly outperforms state-of-the-art baselines across diverse graph-centric and multimodal generative tasks.

</details>


### [84] [Turning mechanistic models into forecasters by using machine learning](https://arxiv.org/abs/2602.04114)
*Amit K. Chakraborty,Hao Wang,Pouria Ramazi*

Main category: cs.LG

TL;DR: 提出时变参数的数据驱动微分方程发现方法，通过直接学习参数时序演化提升复杂系统建模与预测精度，在多个数据集上实现学习误差<3%、月尺度预测误差<6%，并优于CNN-LSTM和GBM模型


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动方法假设微分方程系数时不变，难以捕捉系统动态演化机制，需开发能同时处理常/时变参数的建模框架

Method: 允许部分参数时变并直接从数据学习其演化规律，构建含常/时变参数的方程系统，转化为预测模型时对时变参数进行预测并代入方程

Result: 在SIR模型、消费者-资源系统、温室气体浓度和蓝藻细胞计数数据集上，学习阶段平均绝对误差<3%，月尺度预测误差<6%，且多数场景下预测性能优于CNN-LSTM和梯度提升机

Conclusion: 将时变参数整合至数据驱动微分方程发现可同时提升建模精度与预测性能，为动态演化系统提供有效建模范式

Abstract: The equations of complex dynamical systems may not be identified by expert knowledge, especially if the underlying mechanisms are unknown. Data-driven discovery methods address this challenge by inferring governing equations from time-series data using a library of functions constructed from the measured variables. However, these methods typically assume time-invariant coefficients, which limits their ability to capture evolving system dynamics. To overcome this limitation, we allow some of the parameters to vary over time, learn their temporal evolution directly from data, and infer a system of equations that incorporates both constant and time-varying parameters. We then transform this framework into a forecasting model by predicting the time-varying parameters and substituting these predictions into the learned equations. The model is validated using datasets for Susceptible-Infected-Recovered, Consumer--Resource, greenhouse gas concentration, and Cyanobacteria cell count. By dynamically adapting to temporal shifts, our proposed model achieved a mean absolute error below 3\% for learning a time series and below 6\% for forecasting up to a month ahead. We additionally compare forecasting performance against CNN-LSTM and Gradient Boosting Machine (GBM), and show that our model outperforms these methods across most datasets. Our findings demonstrate that integrating time-varying parameters into data-driven discovery of differential equations improves both modeling accuracy and forecasting performance.

</details>


### [85] [Scalable Explainability-as-a-Service (XaaS) for Edge AI Systems](https://arxiv.org/abs/2602.04120)
*Samaresh Kumar Singh,Joyjit Roy*

Main category: cs.LG

TL;DR: 提出XaaS（可解释即服务）分布式架构，将可解释性解耦为独立服务，通过缓存、验证和自适应引擎，在边缘设备上实现低延迟、高质量的可解释AI，实验显示延迟降低38%。


<details>
  <summary>Details</summary>
Motivation: 当前XAI方法在边缘和IoT系统中通常是临时性和低效的，推理与解释生成耦合导致冗余计算、高延迟和扩展性差，难以在异构边缘设备上部署。

Method: 提出XaaS架构，包含三个创新：(1)基于语义相似性的分布式解释缓存与检索机制；(2)轻量级验证协议确保解释保真度；(3)自适应解释引擎根据设备能力和用户需求选择解释方法。

Result: 在制造质检、自动驾驶感知和医疗诊断三个实际场景中，XaaS降低延迟38%，同时保持高质量解释，证明了其在大规模异构IoT系统上的有效性。

Conclusion: 该工作使透明可信的AI能够在大型异构IoT系统上部署，弥补了XAI研究与边缘实际应用之间的鸿沟，为边缘AI的可解释性提供了实用化路径。

Abstract: Though Explainable AI (XAI) has made significant advancements, its inclusion in edge and IoT systems is typically ad-hoc and inefficient. Most current methods are "coupled" in such a way that they generate explanations simultaneously with model inferences. As a result, these approaches incur redundant computation, high latency and poor scalability when deployed across heterogeneous sets of edge devices. In this work we propose Explainability-as-a-Service (XaaS), a distributed architecture for treating explainability as a first-class system service (as opposed to a model-specific feature). The key innovation in our proposed XaaS architecture is that it decouples inference from explanation generation allowing edge devices to request, cache and verify explanations subject to resource and latency constraints. To achieve this, we introduce three main innovations: (1) A distributed explanation cache with a semantic similarity based explanation retrieval method which significantly reduces redundant computation; (2) A lightweight verification protocol that ensures the fidelity of both cached and newly generated explanations; and (3) An adaptive explanation engine that chooses explanation methods based upon device capability and user requirement. We evaluated the performance of XaaS on three real-world edge-AI use cases: (i) manufacturing quality control; (ii) autonomous vehicle perception; and (iii) healthcare diagnostics. Experimental results show that XaaS reduces latency by 38\% while maintaining high explanation quality across three real-world deployments. Overall, this work enables the deployment of transparent and accountable AI across large scale, heterogeneous IoT systems, and bridges the gap between XAI research and edge-practicality.

</details>


### [86] [Pruning for Generalization: A Transfer-Oriented Spatiotemporal Graph Framework](https://arxiv.org/abs/2602.04153)
*Zihao Jing,Yuxi Long,Ganlin Feng*

Main category: cs.LG

TL;DR: 针对图结构多元时间序列预测在数据稀缺和跨域迁移时的性能下降问题，提出TL-GPSTGN框架，通过信息论和关联性准则选择性地剪枝非优化图上下文，提取信息性子图和特征，在交通基准测试中显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现实世界的图结构多元时间序列预测面临数据稀缺和跨域分布漂移的挑战，现有时空模型在这些情况下性能显著下降。

Method: 提出TL-GPSTGN迁移时空框架，采用信息论和基于相关性的准则，选择性地剪枝非优化图上下文，提取结构信息丰富的子图和特征，形成紧凑的语义表示，并将其集成到时空卷积架构中。

Result: 在大规模交通基准测试的低数据迁移场景下，TL-GPSTGN持续优于基线模型。

Conclusion: 显式的上下文剪枝可作为有效的归纳偏置，显著提升基于图的预测模型的鲁棒性和泛化能力。

Abstract: Multivariate time series forecasting in graph-structured domains is critical for real-world applications, yet existing spatiotemporal models often suffer from performance degradation under data scarcity and cross-domain shifts. We address these challenges through the lens of structure-aware context selection. We propose TL-GPSTGN, a transfer-oriented spatiotemporal framework that enhances sample efficiency and out-of-distribution generalization by selectively pruning non-optimized graph context. Specifically, our method employs information-theoretic and correlation-based criteria to extract structurally informative subgraphs and features, resulting in a compact, semantically grounded representation. This optimized context is subsequently integrated into a spatiotemporal convolutional architecture to capture complex multivariate dynamics. Evaluations on large-scale traffic benchmarks demonstrate that TL-GPSTGN consistently outperforms baselines in low-data transfer scenarios. Our findings suggest that explicit context pruning serves as a powerful inductive bias for improving the robustness of graph-based forecasting models.

</details>


### [87] [Decoupling Time and Risk: Risk-Sensitive Reinforcement Learning with General Discounting](https://arxiv.org/abs/2602.04131)
*Mehrdad Moghimi,Anthony Coache,Hyejin Ku*

Main category: cs.LG

TL;DR: This paper proposes a novel distributional reinforcement learning framework with flexible discounting to better capture temporal and risk preferences, demonstrating that discounting is crucial for safety-critical applications through theoretical analysis and experimental validation.


<details>
  <summary>Details</summary>
Motivation: The discount factor in RL is typically treated as a fixed parameter or tunable hyperparameter without considering its effect on learned policies, especially in safety-critical domains where optimizing risk-sensitive objectives is important. The exponential discount factor cannot fully capture an agent's time preferences.

Method: The authors propose a novel framework enabling flexible discounting of future rewards combined with optimization of risk measures in distributional RL. They provide technical analysis of algorithm optimality, introduce a multi-horizon extension to fix existing methodological issues, and validate robustness through extensive experiments.

Result: The multi-horizon extension resolves issues with existing methodologies, and experiments confirm the robustness of the proposed methods. Results demonstrate that discounting is fundamental for capturing more expressive temporal and risk preference profiles.

Conclusion: Discounting is a cornerstone in decision-making problems for capturing expressive temporal and risk preferences, with significant implications for real-world safety-critical applications. The proposed framework offers a more principled approach to handling discounting in reinforcement learning.

Abstract: Distributional reinforcement learning (RL) is a powerful framework increasingly adopted in safety-critical domains for its ability to optimize risk-sensitive objectives. However, the role of the discount factor is often overlooked, as it is typically treated as a fixed parameter of the Markov decision process or tunable hyperparameter, with little consideration of its effect on the learned policy. In the literature, it is well-known that the discounting function plays a major role in characterizing time preferences of an agent, which an exponential discount factor cannot fully capture. Building on this insight, we propose a novel framework that supports flexible discounting of future rewards and optimization of risk measures in distributional RL. We provide a technical analysis of the optimality of our algorithms, show that our multi-horizon extension fixes issues raised with existing methodologies, and validate the robustness of our methods through extensive experiments. Our results highlight that discounting is a cornerstone in decision-making problems for capturing more expressive temporal and risk preferences profiles, with potential implications for real-world safety-critical applications.

</details>


### [88] [Thickening-to-Thinning: Reward Shaping via Human-Inspired Learning Dynamics for LLM Reasoning](https://arxiv.org/abs/2602.04265)
*Wenze Lin,Zhen Yang,Xitai Jiang,Pony Ma,Gao Huang*

Main category: cs.LG

TL;DR: T2T是一个动态奖励框架，通过"先厚后薄"机制解决RLVR的熵崩溃、冗长性和探索不足问题，在数学基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法存在熵崩溃、过度冗长和难题探索不足等挑战，且奖励机制无法区分问题求解时的广泛搜索需求与已掌握知识的高效表达需求。

Method: 提出T2T框架，采用两阶段机制：错误尝试时"加厚"（奖励长轨迹以扩大搜索空间）；正确时"削薄"（施加长度惩罚以避免冗余），模拟人类学习过程。

Result: 在MATH-500、AIME、AMC数学基准测试中，T2T在Qwen系列和Deepseek模型上显著优于标准GRPO和最新基线方法，取得卓越性能。

Conclusion: T2T通过动态奖励机制有效平衡了探索与效率，成功解决了LLM推理增强中的关键挑战，为可验证任务训练提供了新范式。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for enhancing reasoning in Large Language Models (LLMs). However, it frequently encounters challenges such as entropy collapse, excessive verbosity, and insufficient exploration for hard problems. Crucially, existing reward schemes fail to distinguish between the need for extensive search during problem-solving and the efficiency required for mastered knowledge. In this work, we introduce T2T(Thickening-to-Thinning), a dynamic reward framework inspired by human learning processes. Specifically, it implements a dual-phase mechanism: (1) On incorrect attempts, T2T incentivizes "thickening" (longer trajectories) to broaden the search space and explore novel solution paths; (2) Upon achieving correctness, it shifts to "thinning", imposing length penalties to discourage redundancy, thereby fostering model confidence and crystallizing reasoning capabilities. Extensive experiments on mathematical benchmarks (MATH-500, AIME, AMC) across Qwen-series and Deepseek models demonstrate that T2T significantly outperforms standard GRPO and recent baselines, achieving superior performance.

</details>


### [89] [Multi Objective Design Optimization of Non Pneumatic Passenger Car Tires Using Finite Element Modeling, Machine Learning, and Particle swarm Optimization and Bayesian Optimization Algorithms](https://arxiv.org/abs/2602.04277)
*Priyankkumar Dhrangdhariya,Soumyadipta Maiti,Venkataramana Runkana*

Main category: cs.LG

TL;DR: A generative design and machine learning framework was developed to optimize UPTIS non-pneumatic tire spoke geometries, achieving 53% stiffness tunability, up to 50% durability improvement, and 43% vibration reduction while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Non-pneumatic tires offer promising alternatives to pneumatic tires but face challenges with discontinuous spoke structures including stiffness tuning difficulties, durability issues, and high-speed vibration problems that need addressing for passenger vehicle applications.

Method: The study parameterized spoke profiles using high-order polynomials, generated ~250 designs via PCHIP-based geometric variation, trained ML models (KRR for stiffness, XGBoost for durability/vibration) to replace computationally intensive FEM simulations, and applied Particle Swarm Optimization and Bayesian Optimization for performance refinement.

Result: Optimized designs achieved 53% stiffness tunability, up to 50% durability improvement, and 43% vibration reduction compared to baseline. PSO provided fast targeted convergence while Bayesian Optimization effectively explored multi-objective tradeoffs.

Conclusion: The proposed integrated framework enables systematic development of high-performance, next-generation UPTIS spoke structures for passenger vehicles.

Abstract: Non Pneumatic tires offer a promising alternative to pneumatic tires. However, their discontinuous spoke structures present challenges in stiffness tuning, durability, and high speed vibration. This study introduces an integrated generative design and machine learning driven framework to optimize UPTIS type spoke geometries for passenger vehicles. Upper and lower spoke profiles were parameterized using high order polynomial representations, enabling the creation of approximately 250 generative designs through PCHIP based geometric variation. Machine learning models like KRR for stiffness and XGBoost for durability and vibration achieved strong predictive accuracy, reducing the reliance on computationally intensive FEM simulations. Optimization using Particle Swarm Optimization and Bayesian Optimization further enabled extensive performance refinement. The resulting designs demonstrate 53% stiffness tunability, up to 50% durability improvement, and 43% reduction in vibration compared to the baseline. PSO provided fast, targeted convergence, while Bayesian Optimization effectively explored multi objective tradeoffs. Overall, the proposed framework enables systematic development of high performance, next generation UPTIS spoke structures.

</details>


### [90] [Training Data Efficiency in Multimodal Process Reward Models](https://arxiv.org/abs/2602.04145)
*Jinyuan Li,Chengsong Huang,Langlin Huang,Shaoyang Xu,Haolin Liu,Wenxuan Zhang,Jiaxin Huang*

Main category: cs.LG

TL;DR: 该论文研究多模态过程奖励模型（MPRM）训练的数据效率问题，提出基于信息平衡的评分（BIS）方法，仅需10%的训练数据即可达到全量数据的性能。


<details>
  <summary>Details</summary>
Motivation: MPRM训练依赖大规模蒙特卡洛标注数据，成本高昂，且存在显著的数据冗余问题。

Method: 通过理论分析揭示梯度更新与标签混合度及可靠性的关系，提出BIS评分方法，在回环级别优先选择信息量高的样本。

Result: 在两个骨干模型上，BIS筛选的子集仅需10%数据即可达到全量数据性能，比随机采样相对提升4.1%。

Conclusion: BIS是一种高效的数据选择策略，能显著降低MPRM训练成本，同时提升模型性能。

Abstract: Multimodal Process Reward Models (MPRMs) are central to step-level supervision for visual reasoning in MLLMs. Training MPRMs typically requires large-scale Monte Carlo (MC)-annotated corpora, incurring substantial training cost. This paper studies the data efficiency for MPRM training.Our preliminary experiments reveal that MPRM training quickly saturates under random subsampling of the training data, indicating substantial redundancy within existing MC-annotated corpora.To explain this, we formalize a theoretical framework and reveal that informative gradient updates depend on two factors: label mixtures of positive/negative steps and label reliability (average MC scores of positive steps). Guided by these insights, we propose the Balanced-Information Score (BIS), which prioritizes both mixture and reliability based on existing MC signals at the rollout level, without incurring any additional cost. Across two backbones (InternVL2.5-8B and Qwen2.5-VL-7B) on VisualProcessBench, BIS-selected subsets consistently match and even surpass the full-data performance at small fractions. Notably, the BIS subset reaches full-data performance using only 10% of the training data, improving over random subsampling by a relative 4.1%.

</details>


### [91] [Efficient Equivariant High-Order Crystal Tensor Prediction via Cartesian Local-Environment Many-Body Coupling](https://arxiv.org/abs/2602.04323)
*Dian Jin,Yancheng Yuan,Xiaoming Tao*

Main category: cs.LG

TL;DR: CEITNet is a novel model that efficiently predicts high-order crystal tensor properties using Cartesian tensors and channel-space interactions, outperforming existing methods while avoiding expensive Clebsch-Gordan tensor products.


<details>
  <summary>Details</summary>
Motivation: End-to-end prediction of high-order crystal tensor properties remains challenging because spherical-harmonic equivariant models, while expressive, suffer from substantial computational and memory costs for higher-order targets due to Clebsch-Gordan tensor products.

Method: The Cartesian Environment Interaction Tensor Network (CEITNet) constructs multi-channel Cartesian local environment tensors for each atom and performs flexible many-body mixing via learnable channel-space interactions. It learns in channel space and uses Cartesian tensor bases to assemble equivariant outputs.

Result: CEITNet surpasses prior high-order prediction methods on accuracy for order-2 dielectric, order-3 piezoelectric, and order-4 elastic tensor prediction benchmarks while maintaining high computational efficiency.

Conclusion: CEITNet enables efficient and accurate construction of high-order tensor predictions by leveraging Cartesian tensor representations and channel-space learning, overcoming the computational limitations of traditional spherical-harmonic approaches.

Abstract: End-to-end prediction of high-order crystal tensor properties from atomic structures remains challenging: while spherical-harmonic equivariant models are expressive, their Clebsch-Gordan tensor products incur substantial compute and memory costs for higher-order targets. We propose the Cartesian Environment Interaction Tensor Network (CEITNet), an approach that constructs a multi-channel Cartesian local environment tensor for each atom and performs flexible many-body mixing via a learnable channel-space interaction. By performing learning in channel space and using Cartesian tensor bases to assemble equivariant outputs, CEITNet enables efficient construction of high-order tensor. Across benchmark datasets for order-2 dielectric, order-3 piezoelectric, and order-4 elastic tensor prediction, CEITNet surpasses prior high-order prediction methods on key accuracy criteria while offering high computational efficiency.

</details>


### [92] [UnMaskFork: Test-Time Scaling for Masked Diffusion via Deterministic Action Branching](https://arxiv.org/abs/2602.04344)
*Kou Misaki,Takuya Akiba*

Main category: cs.LG

TL;DR: This paper proposes UnMaskFork, a framework that applies Monte Carlo Tree Search to Masked Diffusion Language Models for test-time scaling, outperforming existing methods on coding and math tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore how Masked Diffusion Language Models (MDLMs), with their iterative generation process, can benefit from advanced search strategies like MCTS for test-time scaling, which has been primarily explored for autoregressive models.

Method: The UnMaskFork (UMF) framework treats the token unmasking process as a search tree and uses Monte Carlo Tree Search to optimize the generation path. It employs deterministic partial unmasking actions performed by multiple MDLMs, contrasting with stochastic sampling approaches.

Result: Empirical evaluation shows UMF consistently outperforms existing test-time scaling baselines on complex coding benchmarks and demonstrates strong scalability on mathematical reasoning tasks.

Conclusion: MDLMs are inherently suitable for advanced search strategies, and UMF effectively leverages this property to improve model performance through test-time compute scaling.

Abstract: Test-time scaling strategies have effectively leveraged inference-time compute to enhance the reasoning abilities of Autoregressive Large Language Models. In this work, we demonstrate that Masked Diffusion Language Models (MDLMs) are inherently amenable to advanced search strategies, owing to their iterative and non-autoregressive generation process. To leverage this, we propose UnMaskFork (UMF), a framework that formulates the unmasking trajectory as a search tree and employs Monte Carlo Tree Search to optimize the generation path. In contrast to standard scaling methods relying on stochastic sampling, UMF explores the search space through deterministic partial unmasking actions performed by multiple MDLMs. Our empirical evaluation demonstrates that UMF consistently outperforms existing test-time scaling baselines on complex coding benchmarks, while also exhibiting strong scalability on mathematical reasoning tasks.

</details>


### [93] [Benchmarking Uncertainty Quantification of Plug-and-Play Diffusion Priors for Inverse Problems Solving](https://arxiv.org/abs/2602.04189)
*Xiaoyu Qiu,Taewon Yang,Zhanhao Liu,Guanyang Wang,Liyue Shen*

Main category: cs.LG

TL;DR: 针对即插即用扩散先验在逆问题求解中的不确定性量化缺失问题，本文通过玩具模型仿真和真实科学逆问题实验，系统评估了现有扩散求解器的不确定性行为，提出了基于UQ的分类方法，为理解扩散逆问题求解的不确定性提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 当前对即插即用扩散先验(PnPDP)逆问题求解器的评估仅关注单点估计精度，忽略了这些随机求解器产生的后验分布特性，与科学任务中需要不确定性量化的实际需求存在根本性错配。

Method: 设计严格的玩具模型仿真来评估各种PnPDP求解器不确定性行为，提出基于不确定性量化的分类框架，并在玩具仿真和多样真实科学逆问题上进行广泛实验。

Result: 观察到与所提分类法和理论预期一致的不确定性行为模式，揭示了不同PnPDP求解器在不确定性量化方面的差异性表现。

Conclusion: 本研究建立了PnPDP求解器不确定性量化的基准评估体系，为科学应用中正确理解和评估扩散逆问题求解的不确定性提供了新见解。

Abstract: Plug-and-play diffusion priors (PnPDP) have become a powerful paradigm for solving inverse problems in scientific and engineering domains. Yet, current evaluations of reconstruction quality emphasize point-estimate accuracy metrics on a single sample, which do not reflect the stochastic nature of PnPDP solvers and the intrinsic uncertainty of inverse problems, critical for scientific tasks. This creates a fundamental mismatch: in inverse problems, the desired output is typically a posterior distribution and most PnPDP solvers induce a distribution over reconstructions, but existing benchmarks only evaluate a single reconstruction, ignoring distributional characterization such as uncertainty. To address this gap, we conduct a systematic study to benchmark the uncertainty quantification (UQ) of existing diffusion inverse solvers. Specifically, we design a rigorous toy model simulation to evaluate the uncertainty behavior of various PnPDP solvers, and propose a UQ-driven categorization. Through extensive experiments on toy simulations and diverse real-world scientific inverse problems, we observe uncertainty behaviors consistent with our taxonomy and theoretical justification, providing new insights for evaluating and understanding the uncertainty for PnPDPs.

</details>


### [94] [LORE: Jointly Learning the Intrinsic Dimensionality and Relative Similarity Structure From Ordinal Data](https://arxiv.org/abs/2602.04192)
*Vivek Anand,Alec Helbling,Mark Davenport,Gordon Berman,Sankar Alagapan,Christopher Rozell*

Main category: cs.LG

TL;DR: LORE is a scalable framework that jointly learns intrinsic dimensionality and ordinal embeddings from noisy triplet comparisons ("Is A more similar to B than C?") using Schatten-p regularization, eliminating the need for pre-setting dimensions while recovering compact, interpretable perceptual space structures.


<details>
  <summary>Details</summary>
Motivation: Existing methods require pre-specifying embedding dimensions for ordinal data (e.g., taste/smell perception), which is impractical for unknown subjective perceptual spaces; LORE aims to jointly discover both the embedding and its intrinsic dimensionality from noisy triplet comparisons.

Method: Uses nonconvex Schatten-$p$ quasi-norm regularization to enable automatic dimensionality recovery; optimized via an iteratively reweighted algorithm with proven convergence guarantees.

Result: Outperforms existing methods on synthetic, simulated perceptual, and real crowdsourced data; produces compact, highly accurate low-dimensional embeddings that faithfully recover latent perceptual geometry.

Conclusion: LORE enables interpretable and data-efficient perceptual modeling in psychophysics, offering a scalable solution for discovering low-dimensional structure in ordinal data with joint dimensionality inference.

Abstract: Learning the intrinsic dimensionality of subjective perceptual spaces such as taste, smell, or aesthetics from ordinal data is a challenging problem. We introduce LORE (Low Rank Ordinal Embedding), a scalable framework that jointly learns both the intrinsic dimensionality and an ordinal embedding from noisy triplet comparisons of the form, "Is A more similar to B than C?". Unlike existing methods that require the embedding dimension to be set apriori, LORE regularizes the solution using the nonconvex Schatten-$p$ quasi norm, enabling automatic joint recovery of both the ordinal embedding and its dimensionality. We optimize this joint objective via an iteratively reweighted algorithm and establish convergence guarantees. Extensive experiments on synthetic datasets, simulated perceptual spaces, and real world crowdsourced ordinal judgements show that LORE learns compact, interpretable and highly accurate low dimensional embeddings that recover the latent geometry of subjective percepts. By simultaneously inferring both the intrinsic dimensionality and ordinal embeddings, LORE enables more interpretable and data efficient perceptual modeling in psychophysics and opens new directions for scalable discovery of low dimensional structure from ordinal data in machine learning.

</details>


### [95] [Blockchain Federated Learning for Sustainable Retail: Reducing Waste through Collaborative Demand Forecasting](https://arxiv.org/abs/2602.04384)
*Fabio Turazza,Alessandro Neri,Marcello Pietri,Maria Angela Butturi,Marco Picone,Marco Mamei*

Main category: cs.LG

TL;DR: 本研究针对食品零售行业因数据隐私限制导致的预测准确性不足问题，提出基于区块链的联邦学习方法，在保护数据隐私的前提下实现跨零售商协同需求预测，实验表明该方法性能接近理想数据共享场景，显著优于孤立建模，能有效减少食品浪费。


<details>
  <summary>Details</summary>
Motivation: 食品浪费问题严重，准确的需求预测对减少浪费至关重要。然而，零售商间的数据隐私顾虑阻碍了协作，限制了预测准确性的提升潜力。本研究旨在探索联邦学习在可持续供应链管理中的应用，解决数据孤岛与隐私保护之间的矛盾。

Method: 研究采用两阶段方法：首先，在孤立零售商场景下开发基线预测模型用于需求预测和浪费评估；然后，引入基于区块链的联邦学习模型，使多个零售商能够在不共享原始数据的情况下进行协同训练。

Result: 初步结果表明，联邦学习模型性能几乎等同于理想的数据共享场景，且显著优于各零售商单独建模的结果，能够有效减少浪费并提升效率。

Conclusion: 联邦学习为可持续供应链管理提供了一种保护数据隐私的协同解决方案，可在不牺牲隐私的前提下实现接近集中式的预测性能，对减少食品浪费具有重要实践价值。

Abstract: Effective demand forecasting is crucial for reducing food waste. However, data privacy concerns often hinder collaboration among retailers, limiting the potential for improved predictive accuracy. In this study, we explore the application of Federated Learning (FL) in Sustainable Supply Chain Management (SSCM), with a focus on the grocery retail sector dealing with perishable goods. We develop a baseline predictive model for demand forecasting and waste assessment in an isolated retailer scenario. Subsequently, we introduce a Blockchain-based FL model, trained collaboratively across multiple retailers without direct data sharing. Our preliminary results show that FL models have performance almost equivalent to the ideal setting in which parties share data with each other, and are notably superior to models built by individual parties without sharing data, cutting waste and boosting efficiency.

</details>


### [96] [From Sparse Sensors to Continuous Fields: STRIDE for Spatiotemporal Reconstruction](https://arxiv.org/abs/2602.04201)
*Yanjie Tong,Peng Chen*

Main category: cs.LG

TL;DR: 提出STRIDE框架，用时序编码器和FMMNN隐式神经解码器从稀疏传感器重建高维时空场，在四个基准测试中性能优越。


<details>
  <summary>Details</summary>
Motivation: 稀疏传感器重建高维时空场是参数化PDE学习的核心挑战，现有方法泛化性差或依赖离散化解码器。

Method: 两阶段框架：时序编码器提取潜在状态，调制FMMNN隐式神经表示解码器重建任意位置场值；理论证明在稳定延迟可观测条件下重建算子可有限维嵌入。

Result: 在混沌动力学和波传播四个基准测试中，STRIDE在极稀疏传感、超分辨率和抗噪声方面优于基线。

Conclusion: STRIDE为稀疏传感重建提供了理论支撑且有效的解决方案，展现了良好的泛化性和鲁棒性。

Abstract: Reconstructing high-dimensional spatiotemporal fields from sparse point-sensor measurements is a central challenge in learning parametric PDE dynamics. Existing approaches often struggle to generalize across trajectories and parameter settings, or rely on discretization-tied decoders that do not naturally transfer across meshes and resolutions. We propose STRIDE (Spatio-Temporal Recurrent Implicit DEcoder), a two-stage framework that maps a short window of sensor measurements to a latent state with a temporal encoder and reconstructs the field at arbitrary query locations with a modulated implicit neural representation (INR) decoder. Using the Fourier Multi-Component and Multi-Layer Neural Network (FMMNN) as the INR backbone improves representation of complex spatial fields and yields more stable optimization than sine-based INRs. We provide a conditional theoretical justification: under stable delay observability of point measurements on a low-dimensional parametric invariant set, the reconstruction operator factors through a finite-dimensional embedding, making STRIDE-type architectures natural approximators. Experiments on four challenging benchmarks spanning chaotic dynamics and wave propagation show that STRIDE outperforms strong baselines under extremely sparse sensing, supports super-resolution, and remains robust to noise.

</details>


### [97] [LoRDO: Distributed Low-Rank Optimization with Infrequent Communication](https://arxiv.org/abs/2602.04396)
*Andrej Jovanović,Alex Iacob,Mher Safaryan,Ionut-Vlad Modoranu,Lorenzo Sani,William F. Shen,Xinchi Qiu,Dan Alistarh,Nicholas D. Lane*

Main category: cs.LG

TL;DR: LoRDO是一个将低秩优化与低频同步相结合的框架，通过全秩准双曲更新在保持性能的同时将通信量减少约10倍。


<details>
  <summary>Details</summary>
Motivation: 基础模型的分布式训练受限于互连带宽和优化器状态的开销。低秩优化器在本地更新机制下因无法获取全批次梯度而导致性能下降。

Method: 提出LoRDO框架，统一低秩优化与低频同步。引入全秩准双曲更新以恢复子空间探索，解决伪梯度全局投影将优化轨迹限制在低秩子空间的问题。

Result: 在1.25亿至7.2亿参数规模的语言建模任务上，性能与低秩DDP接近，通信量减少约10倍；在低内存和小秩/批次设置下提升更显著。

Conclusion: LoRDO成功实现了高效分布式训练，在减少通信开销的同时保持模型性能，尤其适用于内存受限场景。

Abstract: Distributed training of foundation models via $\texttt{DDP}$ is limited by interconnect bandwidth. While infrequent communication strategies reduce synchronization frequency, they remain bottlenecked by the memory and communication requirements of optimizer states. Low-rank optimizers can alleviate these constraints; however, in the local-update regime, workers lack access to the full-batch gradients required to compute low-rank projections, which degrades performance. We propose $\texttt{LoRDO}$, a principled framework unifying low-rank optimization with infrequent synchronization. We first demonstrate that, while global projections based on pseudo-gradients are theoretically superior, they permanently restrict the optimization trajectory to a low-rank subspace. To restore subspace exploration, we introduce a full-rank quasi-hyperbolic update. $\texttt{LoRDO}$ achieves near-parity with low-rank $\texttt{DDP}$ in language modeling and downstream tasks at model scales of $125$M--$720$M, while reducing communication by $\approx 10 \times$. Finally, we show that $\texttt{LoRDO}$ improves performance even more in very low-memory settings with small rank/batch size.

</details>


### [98] [Mixture of Masters: Sparse Chess Language Models with Player Routing](https://arxiv.org/abs/2602.04447)
*Giacomo Frisoni,Lorenzo Molfetta,Davide Freddi,Gianluca Moro*

Main category: cs.LG

TL;DR: We propose Mixture-of-Masters (MoM), a chess mixture-of-experts model that emulates world-class grandmasters to avoid mode-averaged behavior in existing dense transformers, outperforming Stockfish and baselines while enabling dynamic style switching and interpretability.


<details>
  <summary>Details</summary>
Motivation: Modern chess language models are dense transformers that collapse into mode-averaged behavior, blurring stylistic boundaries and suppressing rare but effective strategies, leading to homogenization.

Method: Introduce MoM, the first chess mixture-of-experts model with small GPT experts trained via self-supervised and reinforcement learning with chess-specific rewards. A learnable gating network selects grandmaster personas (e.g., Tal's offense, Petrosian's defense) dynamically based on game state.

Result: MoM outperforms Stockfish, dense individual expert networks, and aggregated GPT baselines on unseen games, while ensuring generation variety, control, and interpretability.

Conclusion: MoM successfully counteracts model homogenization by preserving stylistic diversity and rare strategies, offering a controllable and interpretable approach to chess AI that can dynamically adapt playing styles like human grandmasters.

Abstract: Modern chess language models are dense transformers trained on millions of games played by thousands of high-rated individuals. However, these monolithic networks tend to collapse into mode-averaged behavior, where stylistic boundaries are blurred, and rare but effective strategies are suppressed. To counteract homogenization, we introduce Mixture-of-Masters (MoM), the first chess mixture-of-experts model with small-sized GPT experts emulating world-class grandmasters. Each expert is trained with a combination of self-supervised learning and reinforcement learning guided by chess-specific rewards. For each move, a post-hoc learnable gating network selects the most appropriate persona to channel depending on the game state, allowing MoM to switch its style dynamically$--$e.g., Tal's offensive vocation or Petrosian's defensive solidity. When evaluated against Stockfish on unseen standard games, MoM outperforms both dense individual expert networks and popular GPT baselines trained on aggregated data, while ensuring generation variety, control, and interpretability.

</details>


### [99] [Training A Foundation Model to Represent Graphs as Vectors](https://arxiv.org/abs/2602.04244)
*Qi Feng,Jicong Fan*

Main category: cs.LG

TL;DR: A graph foundation model using multi-graph feature alignment, density maximization mean alignment, and a pooling-free multi-layer reference distribution module for generalizable graph representation learning.


<details>
  <summary>Details</summary>
Motivation: To develop a universal graph representation model that preserves structural and semantic information across diverse domains while maintaining strong generalization ability to unseen domains for downstream tasks like graph classification and clustering.

Method: Proposes a multi-graph-based feature alignment method that constructs weighted graphs from node attributes to generate consistent embeddings; introduces a density maximization mean alignment algorithm with guaranteed convergence; employs contrastive learning with a GNN on original graphs and embeddings; designs a pooling-free multi-layer reference distribution module to preserve node-to-graph information.

Result: Provides theoretical generalization bounds and demonstrates superior performance over strong baselines on few-shot graph classification and graph clustering tasks.

Conclusion: The proposed model successfully learns discriminative, generalizable graph representations through innovative alignment and information preservation mechanisms, establishing an effective graph foundation model.

Abstract: This paper aims to train a graph foundation model that is able to represent any graph as a vector preserving structural and semantic information useful for downstream graph-level tasks such as graph classification and graph clustering. To learn the features of graphs from diverse domains while maintaining strong generalization ability to new domains, we propose a multi-graph-based feature alignment method, which constructs weighted graphs using the attributes of all nodes in each dataset and then generates consistent node embeddings. To enhance the consistency of the features from different datasets, we propose a density maximization mean alignment algorithm with guaranteed convergence. The original graphs and generated node embeddings are fed into a graph neural network to achieve discriminative graph representations in contrastive learning. More importantly, to enhance the information preservation from node-level representations to the graph-level representation, we construct a multi-layer reference distribution module without using any pooling operation. We also provide a theoretical generalization bound to support the effectiveness of the proposed model. The experimental results of few-shot graph classification and graph clustering show that our model outperforms strong baselines.

</details>


### [100] [From Ambiguity to Action: A POMDP Perspective on Partial Multi-Label Ambiguity and Its Horizon-One Resolution](https://arxiv.org/abs/2602.04255)
*Hanlin Pan,Yuhao Tang,Wanfu Gao*

Main category: cs.LG

TL;DR: 提出PML-POMDP框架，通过强化学习联合解决部分多标签学习中的标签消歧和特征选择问题，第一阶段用Transformer生成伪标签，第二阶段进行可解释的特征排序，理论分析超额风险界


<details>
  <summary>Details</summary>
Motivation: 在部分多标签学习中，真实标签未知导致候选标签存在歧义，错误会传播到特征工程等下游任务，传统方法难以同时处理标签消歧和特征选择两个耦合问题

Method: 将两个任务统一建模为部分可观测马尔可夫决策过程(POMDP)，转化为期望回报最大化问题：阶段一用强化学习训练Transformer策略生成高质量硬伪标签；阶段二将特征选择视为序列决策问题，逐步选择特征并输出可解释的全局排序

Result: 理论证明了PML-POMDP的对应关系和超额风险界，该界可分解为伪标签质量和样本量两个因素；在多指标和多个数据集上的实验验证了框架优势

Conclusion: 首次将强化学习框架引入部分多标签学习，实现了标签消歧与特征选择的协同优化，提供了理论保证和实践有效性，为处理标签歧义问题提供了新范式

Abstract: In partial multi-label learning (PML), the true labels are unobserved, which makes label disambiguation important but difficult. A key challenge is that ambiguous candidate labels can propagate errors into downstream tasks such as feature engineering. To solve this issue, we jointly model the disambiguation and feature selection tasks as Partially Observable Markov Decision Processes (POMDP) to turn PML risk minimization into expected-return maximization. Stage 1 trains a transformer policy via reinforcement learning to produce high-quality hard pseudo-labels; Stage 2 describes feature selection as a sequential reinforcement learning problem, selecting features step by step and outputting an interpretable global ranking. We further provide the theoretical analysis of PML-POMDP correspondence and the excess-risk bound that decompose the error into pseudo label quality term and sample size. Experiments in multiple metrics and data sets verify the advantages of the framework.

</details>


### [101] [Rethinking the Design Space of Reinforcement Learning for Diffusion Models: On the Importance of Likelihood Estimation Beyond Loss Design](https://arxiv.org/abs/2602.04663)
*Jaemoo Choi,Yuchen Zhu,Wei Guo,Petr Molodyk,Bo Yuan,Jinbin Bai,Yi Xin,Molei Tao,Yongxin Chen*

Main category: cs.LG

TL;DR: This paper systematically analyzes reinforcement learning design for diffusion models, finding that using an ELBO-based likelihood estimator from final samples is the key factor for effective, efficient, and stable optimization.


<details>
  <summary>Details</summary>
Motivation: Diffusion models have intractable likelihoods that block direct policy-gradient RL application. Existing methods use ad hoc likelihood estimators without thorough investigation of how estimation affects algorithmic performance.

Method: Systematically disentangle RL design space into three factors: policy-gradient objectives, likelihood estimators, and rollout sampling schemes. Propose using ELBO-based model likelihood estimator computed from final generated samples only.

Result: ELBO-based estimator is the dominant factor enabling effective, efficient, and stable RL optimization, outweighing policy-gradient loss choice. Validated across multiple reward benchmarks using SD 3.5 Medium.

Conclusion: Method improves GenEval score from 0.24 to 0.95 in 90 GPU hours, achieving 4.6× higher efficiency than FlowGRPO and 2× higher than SOTA DiffusionNFT without reward hacking.

Abstract: Reinforcement learning has been widely applied to diffusion and flow models for visual tasks such as text-to-image generation. However, these tasks remain challenging because diffusion models have intractable likelihoods, which creates a barrier for directly applying popular policy-gradient type methods. Existing approaches primarily focus on crafting new objectives built on already heavily engineered LLM objectives, using ad hoc estimators for likelihood, without a thorough investigation into how such estimation affects overall algorithmic performance. In this work, we provide a systematic analysis of the RL design space by disentangling three factors: i) policy-gradient objectives, ii) likelihood estimators, and iii) rollout sampling schemes. We show that adopting an evidence lower bound (ELBO) based model likelihood estimator, computed only from the final generated sample, is the dominant factor enabling effective, efficient, and stable RL optimization, outweighing the impact of the specific policy-gradient loss functional. We validate our findings across multiple reward benchmarks using SD 3.5 Medium, and observe consistent trends across all tasks. Our method improves the GenEval score from 0.24 to 0.95 in 90 GPU hours, which is $4.6\times$ more efficient than FlowGRPO and $2\times$ more efficient than the SOTA method DiffusionNFT without reward hacking.

</details>


### [102] [Delving into Muon and Beyond: Deep Analysis and Extensions](https://arxiv.org/abs/2602.04669)
*Xianbiao Qi,Marco Chen,Jiaquan Ye,Yelin He,Rong Xiao*

Main category: cs.LG

TL;DR: This paper analyzes the Muon optimizer through a unified spectral perspective, viewing it as a special case of spectral transformations. The study finds that Muon is an effective form of spectral normalization but doesn't consistently outperform Adam, with RMS-normalized updates showing more stable optimization than first-moment updates.


<details>
  <summary>Details</summary>
Motivation: The Muon optimizer shows strong empirical performance with orthogonalized updates on matrix-shaped parameters, but its underlying mechanisms and relationship to adaptive optimizers like Adam remain insufficiently understood.

Method: The authors present a unified spectral perspective, viewing Muon as the p=0 endpoint of spectral transformations UΣ^pV'. They test variants (p=1/2, 1/4, 1) on both first-moment and RMS-normalized gradient updates, developing a coupled Newton iteration to avoid explicit SVD for efficient computation.

Result: RMS-normalized updates yield more stable optimization than first-moment updates. While spectral compression stabilizes first-moment updates, Muon (p=0) does not consistently outperform Adam.

Conclusion: Muon is best understood as an effective form of spectral normalization, but not a universally superior optimization method compared to Adam.

Abstract: The Muon optimizer has recently attracted considerable attention for its strong empirical performance and use of orthogonalized updates on matrix-shaped parameters, yet its underlying mechanisms and relationship to adaptive optimizers such as Adam remain insufficiently understood. In this work, we aim to address these questions through a unified spectral perspective. Specifically, we view Muon as the p = 0 endpoint of a family of spectral transformations of the form U \boldsymbolΣ^{p} V' , and consider additional variants with p = 1/2 , p = 1/4 , and p = 1 . These transformations are applied to both first-moment updates, as in momentum SGD, and to root-mean-square (RMS) normalized gradient updates as in Adam. To enable efficient computation, we develop a coupled Newton iteration that avoids explicit singular value decomposition. Across controlled experiments, we find that RMS-normalized updates yield more stable optimization than first-moment updates. Moreover, while spectral compression provides strong stabilization benefits under first-moment updates, the Muon update (p = 0) does not consistently outperform Adam. These results suggest that Muon is best understood as an effective form of spectral normalization, but not a universally superior optimization method. Our source code will be released at https://github.com/Ocram7/BeyondMuon.

</details>


### [103] [Multi-Integration of Labels across Categories for Component Identification (MILCCI)](https://arxiv.org/abs/2602.04270)
*Noga Mudrik,Yuxi Chen,Gal Mishne,Adam S. Charles*

Main category: cs.LG

TL;DR: MILCCI is a novel method for analyzing temporal data with multi-category metadata, identifying interpretable components while disentangling category-specific effects.


<details>
  <summary>Details</summary>
Motivation: Large-scale temporal data from repeated trials with multi-category metadata labels pose a challenge in understanding how labels are encoded and disentangling distinct effects across categories.

Method: Extends sparse per-trial decomposition by leveraging label similarities within categories for label-driven cross-trial adjustments, learning temporal traces that evolve within and vary across trials.

Result: Validated on synthetic and real-world data including voting patterns, page views, and neuronal recordings.

Conclusion: Successfully integrates label information to reveal category representations in temporal data, enabling nuanced analysis of multi-trial observations.

Abstract: Many fields collect large-scale temporal data through repeated measurements (trials), where each trial is labeled with a set of metadata variables spanning several categories. For example, a trial in a neuroscience study may be linked to a value from category (a): task difficulty, and category (b): animal choice. A critical challenge in time-series analysis is to understand how these labels are encoded within the multi-trial observations, and disentangle the distinct effect of each label entry across categories. Here, we present MILCCI, a novel data-driven method that i) identifies the interpretable components underlying the data, ii) captures cross-trial variability, and iii) integrates label information to understand each category's representation within the data. MILCCI extends a sparse per-trial decomposition that leverages label similarities within each category to enable subtle, label-driven cross-trial adjustments in component compositions and to distinguish the contribution of each category. MILCCI also learns each component's corresponding temporal trace, which evolves over time within each trial and varies flexibly across trials. We demonstrate MILCCI's performance through both synthetic and real-world examples, including voting patterns, online page view trends, and neuronal recordings.

</details>


### [104] [Let Experts Feel Uncertainty: A Multi-Expert Label Distribution Approach to Probabilistic Time Series Forecasting](https://arxiv.org/abs/2602.04678)
*Zhen Zhou,Zhirui Wang,Qi Hong,Yunyang Shi,Ziyuan Gu,Zhiyuan Liu*

Main category: cs.LG

TL;DR: 提出多专家学习分布标签（LDL）框架解决时间序列预测中准确性与可解释性不确定量化的平衡问题，包含多专家LDL和模式感知LDL-MoE两种方法，在M5数据集上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现实世界时间序列预测需要高精度预测和可解释的不确定量化，但传统点预测方法无法捕捉不确定性，现有概率方法难以平衡计算效率与可解释性。

Method: 提出基于混合专家架构的多专家学习分布标签框架：1）多专家LDL使用不同参数专家捕捉多样时序模式；2）模式感知LDL-MoE通过专用子专家显式分解趋势、季节性、变点、波动性等可解释成分，并采用最大均值差异扩展分布学习。

Result: 在M5聚合销售数据上评估显示，连续多专家LDL取得最佳整体性能，模式感知LDL-MoE通过分量分析提供更强的可解释性，两者均优于基线方法。

Conclusion: 该框架成功平衡预测准确性与可解释性，适用于需要性能与可操作见解的实际预测场景。

Abstract: Time series forecasting in real-world applications requires both high predictive accuracy and interpretable uncertainty quantification. Traditional point prediction methods often fail to capture the inherent uncertainty in time series data, while existing probabilistic approaches struggle to balance computational efficiency with interpretability. We propose a novel Multi-Expert Learning Distributional Labels (LDL) framework that addresses these challenges through mixture-of-experts architectures with distributional learning capabilities. Our approach introduces two complementary methods: (1) Multi-Expert LDL, which employs multiple experts with different learned parameters to capture diverse temporal patterns, and (2) Pattern-Aware LDL-MoE, which explicitly decomposes time series into interpretable components (trend, seasonality, changepoints, volatility) through specialized sub-experts. Both frameworks extend traditional point prediction to distributional learning, enabling rich uncertainty quantification through Maximum Mean Discrepancy (MMD). We evaluate our methods on aggregated sales data derived from the M5 dataset, demonstrating superior performance compared to baseline approaches. The continuous Multi-Expert LDL achieves the best overall performance, while the Pattern-Aware LDL-MoE provides enhanced interpretability through component-wise analysis. Our frameworks successfully balance predictive accuracy with interpretability, making them suitable for real-world forecasting applications where both performance and actionable insights are crucial.

</details>


### [105] [Identifying Intervenable and Interpretable Features via Orthogonality Regularization](https://arxiv.org/abs/2602.04718)
*Moritz Miller,Florent Draye,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: The paper proposes applying orthogonality penalties to sparse autoencoder decoder matrices during language model fine-tuning to disentangle features, reduce interference, improve interpretability, and enable isolated causal interventions while preserving performance.


<details>
  <summary>Details</summary>
Motivation: To address feature interference and superposition in sparse autoencoders used for language model fine-tuning, while improving interpretability and enabling modular representations that support causal interventions.

Method: Fine-tuning language models with a fixed sparse autoencoder while adding an orthogonality penalty to the decoder matrix, motivated by the Independent Causal Mechanisms principle.

Result: The orthogonality penalty reduces interference and superposition, maintains target dataset performance, creates identifiable features, increases distance between feature explanations (enhancing interpretability), and enables isolated causal interventions on the orthogonalized features.

Conclusion: Orthogonalizing sparse autoencoder features successfully creates modular, interpretable representations amenable to causal intervention, with publicly available code for reproducibility and further research.

Abstract: With recent progress on fine-tuning language models around a fixed sparse autoencoder, we disentangle the decoder matrix into almost orthogonal features. This reduces interference and superposition between the features, while keeping performance on the target dataset essentially unchanged. Our orthogonality penalty leads to identifiable features, ensuring the uniqueness of the decomposition. Further, we find that the distance between embedded feature explanations increases with stricter orthogonality penalty, a desirable property for interpretability. Invoking the $\textit{Independent Causal Mechanisms}$ principle, we argue that orthogonality promotes modular representations amenable to causal intervention. We empirically show that these increasingly orthogonalized features allow for isolated interventions. Our code is available under $\texttt{https://github.com/mrtzmllr/sae-icm}$.

</details>


### [106] [From Data to Behavior: Predicting Unintended Model Behaviors Before Training](https://arxiv.org/abs/2602.04735)
*Mengru Wang,Zhenqian Xu,Junfeng Fang,Yunzhi Yao,Shumin Deng,Huajun Chen,Ningyu Zhang*

Main category: cs.LG

TL;DR: 提出Data2Behavior任务预测大模型训练前潜在偏见，设计MDF方法通过注入数据均值表征来探测模型激活，无需参数更新即可预测风险，节省80%训练资源


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在微调前检测大模型训练数据中的无意偏见，事后评估成本高；需提前预测风险以避免资源浪费

Method: 1. 定义Data2Behavior新任务：预训练前预测模型意外行为
2. 提出MDF方法：计算候选数据均值表征，注入基础模型前向传播过程，利用数据统计信号塑造模型激活
3. 通过激活模式揭示潜在偏见和安全风险，全程不更新任何参数

Result: 1. MDF仅需20%微调GPU资源即可实现可靠预测
2. 在Qwen3-14B、Qwen2.5-32B-Instruct和Gemma-3-12b-it上验证有效
3. 可提前发现模型偏见行为并洞察预训练漏洞

Conclusion: MDF为训练前偏见检测提供高效解决方案，通过零参数更新的激活分析实现风险预警，为模型安全开发提供新范式

Abstract: Large Language Models (LLMs) can acquire unintended biases from seemingly benign training data even without explicit cues or malicious content. Existing methods struggle to detect such risks before fine-tuning, making post hoc evaluation costly and inefficient. To address this challenge, we introduce Data2Behavior, a new task for predicting unintended model behaviors prior to training. We also propose Manipulating Data Features (MDF), a lightweight approach that summarizes candidate data through their mean representations and injects them into the forward pass of a base model, allowing latent statistical signals in the data to shape model activations and reveal potential biases and safety risks without updating any parameters. MDF achieves reliable prediction while consuming only about 20% of the GPU resources required for fine-tuning. Experiments on Qwen3-14B, Qwen2.5-32B-Instruct, and Gemma-3-12b-it confirm that MDF can anticipate unintended behaviors and provide insight into pre-training vulnerabilities.

</details>


### [107] [Billion-Scale Graph Foundation Models](https://arxiv.org/abs/2602.04768)
*Maya Bechler-Speicher,Yoel Gottlieb,Andrey Isakov,David Abensur,Ami Tavory,Daniel Haimovich,Ido Guy,Udi Weinsberg*

Main category: cs.LG

TL;DR: 本文提出了GraphBFF，首个用于构建十亿参数图基础模型的端到端框架，展示了可预测的缩放规律和在多样化图任务上的强大零样本性能。


<details>
  <summary>Details</summary>
Motivation: 图结构数据支撑着许多关键应用，但由于图数据的异构性和大规模特性，将基础模型范式（在语言和视觉领域已成功）扩展到通用真实世界图面临挑战。

Method: 作者提出GraphBFF框架，包含灵活的GraphBFF Transformer架构，建立了通用图的神经缩放规律，并提供了数据批处理、预训练和微调的可扩展方法。

Result: 在十亿样本上预训练的14亿参数GraphBFF Transformer在十个未见过的真实世界下游任务上实现了显著的零样本性能，在少样本设置中PRAUC指标提升高达31个百分点。

Conclusion: 该工作证明了十亿尺度图基础模型作为工业级图学习实用基础的可行性，同时讨论了其面临的挑战和开放机遇。

Abstract: Graph-structured data underpins many critical applications. While foundation models have transformed language and vision via large-scale pretraining and lightweight adaptation, extending this paradigm to general, real-world graphs is challenging. In this work, we present Graph Billion- Foundation-Fusion (GraphBFF): the first end-to-end recipe for building billion-parameter Graph Foundation Models (GFMs) for arbitrary heterogeneous, billion-scale graphs. Central to the recipe is the GraphBFF Transformer, a flexible and scalable architecture designed for practical billion-scale GFMs. Using the GraphBFF, we present the first neural scaling laws for general graphs and show that loss decreases predictably as either model capacity or training data scales, depending on which factor is the bottleneck. The GraphBFF framework provides concrete methodologies for data batching, pretraining, and fine-tuning for building GFMs at scale. We demonstrate the effectiveness of the framework with an evaluation of a 1.4 billion-parameter GraphBFF Transformer pretrained on one billion samples. Across ten diverse, real-world downstream tasks on graphs unseen during training, spanning node- and link-level classification and regression, GraphBFF achieves remarkable zero-shot and probing performance, including in few-shot settings, with large margins of up to 31 PRAUC points. Finally, we discuss key challenges and open opportunities for making GFMs a practical and principled foundation for graph learning at industrial scale.

</details>


### [108] [Team, Then Trim: An Assembly-Line LLM Framework for High-Quality Tabular Data Generation](https://arxiv.org/abs/2602.04785)
*Congjing Zhang,Ryan Feng Lin,Ruoxuan Bao,Shuai Huang*

Main category: cs.LG

TL;DR: Team-then-Trim (T^2) 是一个通过LLM协作团队合成高质量表格数据，并结合三阶段质量控制流程的框架，有效解决数据稀缺、类别不平衡等问题。


<details>
  <summary>Details</summary>
Motivation: 实际机器学习应用中，高质量表格数据获取成本高昂且常存在类别不平衡、选择偏差、低保真度等缺陷，亟需高效的数据合成方法弥补数据稀缺。

Method: 将表格数据生成类比为制造过程：由领域知识指导的专门化LLM团队顺序生成不同数据组件，再通过三阶段插件式质量控制管道对合成数据进行多维度系统评估。

Result: 在模拟和真实数据集上的实验表明，T^2在生成高质量表格数据方面显著优于现有最先进方法。

Conclusion: 该框架在直接数据收集不可行时，能为下游模型提供有效支持，具有解决实际数据获取难题的潜力。

Abstract: While tabular data is fundamental to many real-world machine learning (ML) applications, acquiring high-quality tabular data is usually labor-intensive and expensive. Limited by the scarcity of observations, tabular datasets often exhibit critical deficiencies, such as class imbalance, selection bias, and low fidelity. To address these challenges, building on recent advances in Large Language Models (LLMs), this paper introduces Team-then-Trim (T$^2$), a framework that synthesizes high-quality tabular data through a collaborative team of LLMs, followed by a rigorous three-stage plug-in data quality control (QC) pipeline. In T$^2$, tabular data generation is conceptualized as a manufacturing process: specialized LLMs, guided by domain knowledge, are tasked with generating different data components sequentially, and the resulting products, i.e., the synthetic data, are systematically evaluated across multiple dimensions of QC. Empirical results on both simulated and real-world datasets demonstrate that T$^2$ outperforms state-of-the-art methods in producing high-quality tabular data, highlighting its potential to support downstream models when direct data collection is practically infeasible.

</details>


### [109] [Beyond Rewards in Reinforcement Learning for Cyber Defence](https://arxiv.org/abs/2602.04809)
*Elizabeth Bates,Chris Hicks,Vasilios Mavroudis*

Main category: cs.LG

TL;DR: This paper evaluates sparse versus dense reward functions for autonomous cyber defence agents trained with deep reinforcement learning, finding that sparse rewards yield more reliable training and lower-risk policies.


<details>
  <summary>Details</summary>
Motivation: Dense reward functions used in training autonomous cyber defence agents risk biasing them toward suboptimal and risky solutions in complex cyber environments, necessitating investigation of alternative reward structures.

Method: The authors compare sparse and dense reward functions across two cyber gyms, various network sizes, and RL algorithms using a novel ground truth evaluation approach to analyze learning and policy characteristics.

Result: Sparse rewards (when goal-aligned and frequently encountered) provide enhanced training reliability, produce more effective cyber defence agents with lower-risk policies, better align with defender goals, and reduce costly actions without explicit penalties.

Conclusion: Sparse reward functions uniquely offer superior performance for autonomous cyber defence agents compared to dense rewards, delivering more reliable training and safer, more effective defensive policies.

Abstract: Recent years have seen an explosion of interest in autonomous cyber defence agents trained to defend computer networks using deep reinforcement learning. These agents are typically trained in cyber gym environments using dense, highly engineered reward functions which combine many penalties and incentives for a range of (un)desirable states and costly actions. Dense rewards help alleviate the challenge of exploring complex environments but risk biasing agents towards suboptimal and potentially riskier solutions, a critical issue in complex cyber environments. We thoroughly evaluate the impact of reward function structure on learning and policy behavioural characteristics using a variety of sparse and dense reward functions, two well-established cyber gyms, a range of network sizes, and both policy gradient and value-based RL algorithms. Our evaluation is enabled by a novel ground truth evaluation approach which allows directly comparing between different reward functions, illuminating the nuanced inter-relationships between rewards, action space and the risks of suboptimal policies in cyber environments. Our results show that sparse rewards, provided they are goal aligned and can be encountered frequently, uniquely offer both enhanced training reliability and more effective cyber defence agents with lower-risk policies. Surprisingly, sparse rewards can also yield policies that are better aligned with cyber defender goals and make sparing use of costly defensive actions without explicit reward-based numerical penalties.

</details>


### [110] [MirrorLA: Reflecting Feature Map for Vision Linear Attention](https://arxiv.org/abs/2602.04346)
*Weikang Meng,Liangyu Huo,Yadan Luo,Yaowei Wang,Yingjian Li,Zheng Zhang*

Main category: cs.LG

TL;DR: MirrorLA通过可学习的Householder反射主动重定向特征，解决线性注意力非负约束导致的信息损失，实现线性复杂度下的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 线性注意力将Transformer复杂度从二次降至线性，但性能持续落后于softmax注意力。根本原因是核特征图的非负性约束通过标准投影（如ReLU）造成负域语义信息的不可逆丢失。

Method: 提出MirrorLA几何框架，用可学习的Householder反射替代被动截断，通过多尺度设计：1) 块级等距变换优化局部判别性；2) 方差感知调制稳定长上下文动态并多样化激活；3) 跨头反射诱导全局协方差混合，从而最大化信息保留。

Result: 在标准基准测试上达到最先进性能，证明严格线性效率可与表征保真度兼得。

Conclusion: 主动重定向策略有效解决了线性注意力的性能瓶颈，为设计高效Transformer提供了新几何视角。

Abstract: Linear attention significantly reduces the computational complexity of Transformers from quadratic to linear, yet it consistently lags behind softmax-based attention in performance. We identify the root cause of this degradation as the non-negativity constraint imposed on kernel feature maps: standard projections like ReLU act as "passive truncation" operators, indiscriminately discarding semantic information residing in the negative domain. We propose MirrorLA, a geometric framework that substitutes passive truncation with active reorientation. By leveraging learnable Householder reflections, MirrorLA rotates the feature geometry into the non-negative orthant to maximize information retention. Our approach restores representational density through a cohesive, multi-scale design: it first optimizes local discriminability via block-wise isometries, stabilizes long-context dynamics using variance-aware modulation to diversify activations, and finally, integrates dispersed subspaces via cross-head reflections to induce global covariance mixing. MirrorLA achieves state-of-the-art performance across standard benchmarks, demonstrating that strictly linear efficiency can be achieved without compromising representational fidelity.

</details>


### [111] [From Evaluation to Design: Using Potential Energy Surface Smoothness Metrics to Guide Machine Learning Interatomic Potential Architectures](https://arxiv.org/abs/2602.04861)
*Ryan Liu,Eric Qu,Tobias Kreiman,Samuel M. Blau,Aditi S. Krishnapriyan*

Main category: cs.LG

TL;DR: This paper introduces the Bond Smoothness Characterization Test (BSCT), an efficient benchmark that evaluates Machine Learning Interatomic Potentials (MLIPs) by probing potential energy surface smoothness through controlled bond deformations, detecting physical artifacts that standard regression metrics miss, while correlating strongly with molecular dynamics stability at a fraction of the computational cost.


<details>
  <summary>Details</summary>
Motivation: MLIPs sometimes fail to reproduce the physical smoothness of quantum potential energy surfaces, leading to erroneous simulation behavior that standard energy/force regression evaluations cannot detect. Existing evaluation methods like microcanonical molecular dynamics are computationally expensive and only probe near-equilibrium states, creating a need for more efficient and comprehensive validation metrics.

Method: The authors developed the Bond Smoothness Characterization Test (BSCT) which systematically probes the potential energy surface through controlled bond deformations to identify discontinuities, artificial minima, and spurious forces both near and far from equilibrium. They tested this using an unconstrained Transformer backbone with refinements including a new differentiable k-nearest neighbors algorithm and temperature-controlled attention mechanism.

Result: BSCT demonstrates strong correlation with molecular dynamics stability while requiring only a small fraction of MD's computational cost. The refined Transformer model optimized using BSCT achieved simultaneously low conventional energy/force regression errors, stable molecular dynamics simulations, and robust predictions of atomistic properties.

Conclusion: BSCT serves as both an effective validation metric and an "in-the-loop" model design proxy that alerts MLIP developers to physical challenges undetectable by current benchmarks, enabling systematic optimization of model architectures for improved physical fidelity and simulation stability.

Abstract: Machine Learning Interatomic Potentials (MLIPs) sometimes fail to reproduce the physical smoothness of the quantum potential energy surface (PES), leading to erroneous behavior in downstream simulations that standard energy and force regression evaluations can miss. Existing evaluations, such as microcanonical molecular dynamics (MD), are computationally expensive and primarily probe near-equilibrium states. To improve evaluation metrics for MLIPs, we introduce the Bond Smoothness Characterization Test (BSCT). This efficient benchmark probes the PES via controlled bond deformations and detects non-smoothness, including discontinuities, artificial minima, and spurious forces, both near and far from equilibrium. We show that BSCT correlates strongly with MD stability while requiring a fraction of the cost of MD. To demonstrate how BSCT can guide iterative model design, we utilize an unconstrained Transformer backbone as a testbed, illustrating how refinements such as a new differentiable $k$-nearest neighbors algorithm and temperature-controlled attention reduce artifacts identified by our metric. By optimizing model design systematically based on BSCT, the resulting MLIP simultaneously achieves a low conventional E/F regression error, stable MD simulations, and robust atomistic property predictions. Our results establish BSCT as both a validation metric and as an "in-the-loop" model design proxy that alerts MLIP developers to physical challenges that cannot be efficiently evaluated by current MLIP benchmarks.

</details>


### [112] [EXaMCaP: Subset Selection with Entropy Gain Maximization for Probing Capability Gains of Large Chart Understanding Training Sets](https://arxiv.org/abs/2602.04365)
*Jiapeng Liu,Liang Li,Bing Li,Peng Fu,Xiyan Gao,Chengyang Fang,Xiaoshuai Hao,Can Ma*

Main category: cs.LG

TL;DR: The paper proposes EXaMCaP, an entropy-based subset selection method that efficiently probes MLLM capability gains from ChartU training sets, avoiding costly full-set fine-tuning while maintaining effectiveness across different model architectures and subset sizes.


<details>
  <summary>Details</summary>
Motivation: Full-set fine-tuning of MLLMs to verify ChartU dataset quality is computationally expensive and hinders iterative dataset refinement. The paper seeks a more efficient way to assess dataset effectiveness using subsets that can reliably predict full-set performance gains.

Method: EXaMCaP uses entropy gain maximization to select diverse subsets from ChartU datasets. It iteratively chooses samples that maximize entropy gain relative to the current subset, approximating the maximum-entropy subset without exhaustive enumeration.

Result: EXaMCaP outperforms baseline methods in probing MLLM capability gains, demonstrates strong effectiveness across various subset sizes, and shows compatibility with different MLLM architectures.

Conclusion: The entropy-based subset selection approach provides an efficient and effective way to evaluate ChartU dataset quality, enabling faster iterative refinement cycles while maintaining reliable assessment of MLLM performance improvements.

Abstract: Recent works focus on synthesizing Chart Understanding (ChartU) training sets to inject advanced chart knowledge into Multimodal Large Language Models (MLLMs), where the sufficiency of the knowledge is typically verified by quantifying capability gains via the fine-tune-then-evaluate paradigm. However, full-set fine-tuning MLLMs to assess such gains incurs significant time costs, hindering the iterative refinement cycles of the ChartU dataset. Reviewing the ChartU dataset synthesis and data selection domains, we find that subsets can potentially probe the MLLMs' capability gains from full-set fine-tuning. Given that data diversity is vital for boosting MLLMs' performance and entropy reflects this feature, we propose EXaMCaP, which uses entropy gain maximization to select a subset. To obtain a high-diversity subset, EXaMCaP chooses the maximum-entropy subset from the large ChartU dataset. As enumerating all possible subsets is impractical, EXaMCaP iteratively selects samples to maximize the gain in set entropy relative to the current set, approximating the maximum-entropy subset of the full dataset. Experiments show that EXaMCaP outperforms baselines in probing the capability gains of the ChartU training set, along with its strong effectiveness across diverse subset sizes and compatibility with various MLLM architectures.

</details>


### [113] [Subliminal Effects in Your Data: A General Mechanism via Log-Linearity](https://arxiv.org/abs/2602.04863)
*Ishaq Aden-Ali,Noah Golowich,Allen Liu,Abhishek Shetty,Ankur Moitra,Nika Haghtalab*

Main category: cs.LG

TL;DR: A method called Logit-Linear-Selection (LLS) can select subsets of generic datasets to make LLMs exhibit hidden behaviors like specific preferences, responding in unseen languages, or adopting different personas, with effects generalizing across model architectures.


<details>
  <summary>Details</summary>
Motivation: Understanding how datasets affect LLM properties is critical, especially since datasets can transmit hidden signals not observable from individual datapoints, challenging current dataset-centric understandings of LLM training.

Method: Logit-Linear-Selection (LLS), inspired by the linear structure of LLMs, which prescribes how to select subsets of generic preference datasets to elicit hidden effects.

Result: LLS discovers subsets of real-world datasets that cause models to exhibit specific behaviors (preferences, language switching, persona adoption), and this effect persists across different model architectures.

Conclusion: There is a general mechanism through which hidden subtexts can arise in generic datasets, and LLS provides a systematic way to uncover and elicit these hidden effects, demonstrating generality and universality across model architectures.

Abstract: Training modern large language models (LLMs) has become a veritable smorgasbord of algorithms and datasets designed to elicit particular behaviors, making it critical to develop techniques to understand the effects of datasets on the model's properties. This is exacerbated by recent experiments that show datasets can transmit signals that are not directly observable from individual datapoints, posing a conceptual challenge for dataset-centric understandings of LLM training and suggesting a missing fundamental account of such phenomena. Towards understanding such effects, inspired by recent work on the linear structure of LLMs, we uncover a general mechanism through which hidden subtexts can arise in generic datasets.
  We introduce Logit-Linear-Selection (LLS), a method that prescribes how to select subsets of a generic preference dataset to elicit a wide range of hidden effects. We apply LLS to discover subsets of real-world datasets so that models trained on them exhibit behaviors ranging from having specific preferences, to responding to prompts in a different language not present in the dataset, to taking on a different persona. Crucially, the effect persists for the selected subset, across models with varying architectures, supporting its generality and universality.

</details>


### [114] [Multi-scale hypergraph meets LLMs: Aligning large language models for time series analysis](https://arxiv.org/abs/2602.04369)
*Zongjiang Shang,Dongliang Cui,Binqing Wu,Ling Chen*

Main category: cs.LG

TL;DR: MSH-LLM通过超图机制、跨模态对齐和混合提示，利用大语言模型进行多尺度时间序列分析，在27个数据集上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽成功将预训练大语言模型用于时间序列分析，但未能充分考虑自然语言与时间序列的多尺度结构，导致大模型能力利用不足。

Method: 提出MSH-LLM方法，包含：1)超图机制增强时间序列多尺度语义信息；2)跨模态对齐模块实现不同尺度下的模态对齐；3)混合提示机制提供上下文信息并增强多尺度时序模式理解。

Result: 在5个不同领域的27个真实世界数据集上均达到最优性能。

Conclusion: 所提方法有效解决了语言与时间序列间的多尺度对齐问题，显著提升了大语言模型在时间序列分析任务上的表现。

Abstract: Recently, there has been great success in leveraging pre-trained large language models (LLMs) for time series analysis. The core idea lies in effectively aligning the modality between natural language and time series. However, the multi-scale structures of natural language and time series have not been fully considered, resulting in insufficient utilization of LLMs capabilities. To this end, we propose MSH-LLM, a Multi-Scale Hypergraph method that aligns Large Language Models for time series analysis. Specifically, a hyperedging mechanism is designed to enhance the multi-scale semantic information of time series semantic space. Then, a cross-modality alignment (CMA) module is introduced to align the modality between natural language and time series at different scales. In addition, a mixture of prompts (MoP) mechanism is introduced to provide contextual information and enhance the ability of LLMs to understand the multi-scale temporal patterns of time series. Experimental results on 27 real-world datasets across 5 different applications demonstrate that MSH-LLM achieves the state-of-the-art results.

</details>


### [115] [CRoSS: A Continual Robotic Simulation Suite for Scalable Reinforcement Learning with High Task Diversity and Realistic Physics Simulation](https://arxiv.org/abs/2602.04868)
*Yannick Denker,Alexander Gepperth*

Main category: cs.LG

TL;DR: 针对连续强化学习缺乏真实机器人基准的问题，本文提出CRoSS——一个基于Gazebo的高保真机器人模拟基准套件，包含轮式移动机器人和七自由度机械臂两大平台，支持传感器任意配置和快速 kinematics-only 模式，为可重复的持续学习研究提供标准化评估工具。


<details>
  <summary>Details</summary>
Motivation: 现有连续强化学习基准在物理真实性和传感器灵活性方面不足，难以反映真实机器人学习场景的挑战，缺乏可扩展且可重复的评估平台。

Method: 在Gazebo模拟器中构建CRoSS基准，包含配备激光雷达、摄像头等传感器的差速驱动机器人（用于巡线和物体推动任务）和七自由度机械臂（用于笛卡尔空间与关节空间的到达任务），提供物理仿真和快速 kinematics-only 两种运行模式，采用容器化技术确保可重复性，并在该基准上测试了DQN和策略梯度等标准RL算法性能。

Result: CRoSS成功实现了高物理真实性的机器人持续学习评估，kinematics-only模式运行速度提升两个数量级；标准RL算法的性能验证了该基准的有效性和可扩展性。

Conclusion: CRoSS为机器人持续学习领域提供了一个真实、可扩展且可重复的标准化基准，将推动该领域在受控环境下的深入研究，并促进算法的实际应用。

Abstract: Continual reinforcement learning (CRL) requires agents to learn from a sequence of tasks without forgetting previously acquired policies. In this work, we introduce a novel benchmark suite for CRL based on realistically simulated robots in the Gazebo simulator. Our Continual Robotic Simulation Suite (CRoSS) benchmarks rely on two robotic platforms: a two-wheeled differential-drive robot with lidar, camera and bumper sensor, and a robotic arm with seven joints. The former represent an agent in line-following and object-pushing scenarios, where variation of visual and structural parameters yields a large number of distinct tasks, whereas the latter is used in two goal-reaching scenarios with high-level cartesian hand position control (modeled after the Continual World benchmark), and low-level control based on joint angles. For the robotic arm benchmarks, we provide additional kinematics-only variants that bypass the need for physical simulation (as long as no sensor readings are required), and which can be run two orders of magnitude faster. CRoSS is designed to be easily extensible and enables controlled studies of continual reinforcement learning in robotic settings with high physical realism, and in particular allow the use of almost arbitrary simulated sensors. To ensure reproducibility and ease of use, we provide a containerized setup (Apptainer) that runs out-of-the-box, and report performances of standard RL algorithms, including Deep Q-Networks (DQN) and policy gradient methods. This highlights the suitability as a scalable and reproducible benchmark for CRL research.

</details>


### [116] [Reducing the labeling burden in time-series mapping using Common Ground: a semi-automated approach to tracking changes in land cover and species over time](https://arxiv.org/abs/2602.04373)
*Geethen Singh,Jasper A Slingsby,Tamara B Robinson,Glenn Moncrieff*

Main category: cs.LG

TL;DR: 本文提出"Common Ground"半监督学习框架，利用时序稳定区域提供隐式监督，实现地球观测数据分类的时序泛化，无需在每个时间步手动更新标注。


<details>
  <summary>Details</summary>
Motivation: 在动态或偏远生态系统中，每个时间步收集新标注数据成本高昂且实施困难，亟需能够跨时间泛化而无需频繁手动更新标签的方法。

Method: 借鉴变化检测与半监督学习，"Common Ground"框架利用时序稳定区域（光谱/语义特征变化极小的区域）作为动态区域的隐式监督源。

Result: 在入侵树种测绘中，该方法比朴素时序迁移提升21-40%准确率，比金标准方法高10-16%；在欧洲土地覆盖分类中，仅比基线方法提升2%。

Conclusion: 结合稳定区域筛选与半监督学习，该方法实现了可扩展、标签高效的多时序遥感分类，有效避免了手动更新参考标签的需求。

Abstract: Reliable classification of Earth Observation data depends on consistent, up-to-date reference labels. However, collecting new labelled data at each time step remains expensive and logistically difficult, especially in dynamic or remote ecological systems. As a response to this challenge, we demonstrate that a model with access to reference data solely from time step t0 can perform competitively on both t0 and a future time step t1, outperforming models trained separately on time-specific reference data (the gold standard). This finding suggests that effective temporal generalization can be achieved without requiring manual updates to reference labels beyond the initial time step t0. Drawing on concepts from change detection and semi-supervised learning (SSL), the most performant approach, "Common Ground", uses a semi-supervised framework that leverages temporally stable regions-areas with little to no change in spectral or semantic characteristics between time steps-as a source of implicit supervision for dynamic regions. We evaluate this strategy across multiple classifiers, sensors (Landsat-8, Sentinel-2 satellite multispectral and airborne imaging spectroscopy), and ecological use cases. For invasive tree species mapping, we observed a 21-40% improvement in classification accuracy using Common Ground compared to naive temporal transfer, where models trained at a single time step are directly applied to a future time step. We also observe a 10 -16% higher accuracy for the introduced approach compared to a gold-standard approach. In contrast, when broad land cover categories were mapped across Europe, we observed a more modest 2% increase in accuracy compared to both the naive and gold-standard approaches. These results underscore the effectiveness of combining stable reference screening with SSL for scalable and label-efficient multi-temporal remote sensing classification.

</details>


### [117] [Rethinking the Trust Region in LLM Reinforcement Learning](https://arxiv.org/abs/2602.04879)
*Penghui Qi,Xiangxin Zhou,Zichen Liu,Tianyu Pang,Chao Du,Min Lin,Wee Sun Lee*

Main category: cs.LG

TL;DR: PPO的比率裁剪机制不适合LLM大词汇量特性，导致低概率token被过度惩罚而高概率token约束不足。DPPO用直接策略散度估计替代启发式裁剪，采用二元和Top-K近似降低内存开销，显著提升训练稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: PPO的核心比率裁剪机制在LLM大词汇场景下存在结构性缺陷：基于采样token概率比的单样本蒙特卡洛估计噪声大，导致低概率token更新被过度抑制，而高概率token的灾难性偏移约束不足，引|发训练低效和不稳定。

Method: 提出Divergence Proximal Policy Optimization (DPPO)，用Total Variation或KL散度等直接策略散度估计替代启发式裁剪；引入Binary和Top-K高效近似方法，在捕获关键散度信息的同时控制内存开销。

Result: 大量实验表明，DPPO相比现有方法具有更优的训练稳定性和效率，为基于RL的LLM微调提供了更鲁棒的基线算法。

Conclusion: DPPO通过 principled 的散度约束机制有效解决了PPO在LLM微调中的固有缺陷，是强化学习微调大语言模型的更可靠基础框架。

Abstract: Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large vocabularies inherent to LLMs. PPO constrains policy updates based on the probability ratio of sampled tokens, which serves as a noisy single-sample Monte Carlo estimate of the true policy divergence. This creates a sub-optimal learning dynamic: updates to low-probability tokens are aggressively over-penalized, while potentially catastrophic shifts in high-probability tokens are under-constrained, leading to training inefficiency and instability. To address this, we propose Divergence Proximal Policy Optimization (DPPO), which substitutes heuristic clipping with a more principled constraint based on a direct estimate of policy divergence (e.g., Total Variation or KL). To avoid huge memory footprint, we introduce the efficient Binary and Top-K approximations to capture the essential divergence with negligible overhead. Extensive empirical evaluations demonstrate that DPPO achieves superior training stability and efficiency compared to existing methods, offering a more robust foundation for RL-based LLM fine-tuning.

</details>


### [118] [Contrastive Continual Learning for Model Adaptability in Internet of Things](https://arxiv.org/abs/2602.04881)
*Ajesh Koyatan Chathoth*

Main category: cs.LG

TL;DR: 综述对比持续学习在物联网中的应用，提出统一框架和参考架构，连接算法设计与物联网约束，并指出概念漂移和节能训练等挑战。


<details>
  <summary>Details</summary>
Motivation: 物联网部署在非平稳动态环境中，面临传感器漂移、用户行为演变和异构隐私需求等问题。持续学习可适应模型而不发生灾难性遗忘，对比学习能提高鲁棒性和样本效率，二者结合（CCL）对物联网至关重要。

Method: 本文综述CCL在物联网中的应用，连接算法设计（重放、正则化、蒸馏、提示）与系统约束（TinyML、间歇连接、隐私）。提出统一问题表述，推导融合对比与蒸馏损失的通用目标，设计面向物联网的端-边-云参考架构，并提供评估协议与指标指导。

Result: 提供了连接CCL算法与物联网约束的综合综述，提出统一问题表述和通用目标函数，设计了物联网导向的参考架构，并制定了评估指南。

Conclusion: 强调物联网领域独特挑战：表格与流式数据处理、概念漂移、联邦学习设置及节能训练，为未来研究指明方向。

Abstract: Internet of Things (IoT) deployments operate in nonstationary, dynamic environments where factors such as sensor drift, evolving user behavior, and heterogeneous user privacy requirements can affect application utility. Continual learning (CL) addresses this by adapting models over time without catastrophic forgetting. Meanwhile, contrastive learning has emerged as a powerful representation-learning paradigm that improves robustness and sample efficiency in a self-supervised manner. This paper reviews the usage of \emph{contrastive continual learning} (CCL) for IoT, connecting algorithmic design (replay, regularization, distillation, prompts) with IoT system realities (TinyML constraints, intermittent connectivity, privacy). We present a unifying problem formulation, derive common objectives that blend contrastive and distillation losses, propose an IoT-oriented reference architecture for on-device, edge, and cloud-based CCL, and provide guidance on evaluation protocols and metrics. Finally, we highlight open unique challenges with respect to the IoT domain, such as spanning tabular and streaming IoT data, concept drift, federated settings, and energy-aware training.

</details>


### [119] [Protein Autoregressive Modeling via Multiscale Structure Generation](https://arxiv.org/abs/2602.04883)
*Yanru Qu,Cheng-Yen Hsieh,Zaixiang Zheng,Ge Liu,Quanquan Gu*

Main category: cs.LG

TL;DR: 提出蛋白质自回归建模(PAR)，首个多尺度自回归框架，通过粗到细的尺度预测生成蛋白质骨架，采用多尺度下采样、自回归变换器和流解码器，通过噪声上下文学习和计划采样缓解暴露偏差，展现强零样本泛化能力和高质量设计


<details>
  <summary>Details</summary>
Motivation: 利用蛋白质层次化特性，模拟雕塑过程实现从粗拓扑到细细节的蛋白质骨架生成，同时解决自回归模型中训练与生成不匹配导致的暴露偏差问题

Method: PAR框架包含三个核心组件：(1)多尺度下采样操作，训练中多尺度表示蛋白质结构；(2)自回归变换器，编码多尺度信息并产生条件嵌入；(3)基于流的骨架解码器，根据嵌入生成骨架原子。采用噪声上下文学习和计划采样缓解暴露偏差

Result: PAR展现强零样本泛化能力，支持无需微调的人工提示条件生成和基序支架；在无条件下生成基准测试中有效学习蛋白质分布，产生高设计质量骨架，并呈现良好缩放行为

Conclusion: PAR作为蛋白质结构生成的前景框架，具备高质量生成、强泛化能力和灵活性，为蛋白质设计提供新范式

Abstract: We present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for protein backbone generation via coarse-to-fine next-scale prediction. Using the hierarchical nature of proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and refining structural details over scales. To achieve this, PAR consists of three key components: (i) multi-scale downsampling operations that represent protein structures across multiple scales during training; (ii) an autoregressive transformer that encodes multi-scale information and produces conditional embeddings to guide structure generation; (iii) a flow-based backbone decoder that generates backbone atoms conditioned on these embeddings. Moreover, autoregressive models suffer from exposure bias, caused by the training and the generation procedure mismatch, and substantially degrades structure generation quality. We effectively alleviate this issue by adopting noisy context learning and scheduled sampling, enabling robust backbone generation. Notably, PAR exhibits strong zero-shot generalization, supporting flexible human-prompted conditional generation and motif scaffolding without requiring fine-tuning. On the unconditional generation benchmark, PAR effectively learns protein distributions and produces backbones of high design quality, and exhibits favorable scaling behavior. Together, these properties establish PAR as a promising framework for protein structure generation.

</details>


### [120] [Separation-Utility Pareto Frontier: An Information-Theoretic Characterization](https://arxiv.org/abs/2602.04408)
*Shizhou Xu*

Main category: cs.LG

TL;DR: 该论文通过信息论方法从理论上刻画了公平机器学习中效用与分离性之间的权衡，证明了其帕累托前沿的凹性，并开发了一种条件互信息正则化器，能在保持模型效用的同时减少公平性违规。


<details>
  <summary>Details</summary>
Motivation: 刻画效用与分离性之间的帕累托前沿，其中分离性是一种要求预测结果在真实结果条件下独立于敏感属性的公平性准则。

Method: 采用信息论框架来刻画帕累托前沿及其凹性；开发一种与基于梯度的深度学习优化兼容的条件互信息（CMI）正则化器。

Result: 证明了效用-分离性前沿具有凹性，且分离性的边际成本递增；确定了严格权衡的条件；实证表明CMI正则化器在四个数据集上显著减少分离性违规，同时匹配或超越基线方法的效用。

Conclusion: 提供了一种可证明的、稳定的且灵活的方法来在深度学习中实施分离性，将理论见解与实践应用相结合。

Abstract: We study the Pareto frontier (optimal trade-off) between utility and separation, a fairness criterion requiring predictive independence from sensitive attributes conditional on the true outcome. Through an information-theoretic lens, we prove a characterization of the utility-separation Pareto frontier, establish its concavity, and thereby prove the increasing marginal cost of separation in terms of utility. In addition, we characterize the conditions under which this trade-off becomes strict, providing a guide for trade-off selection in practice. Based on the theoretical characterization, we develop an empirical regularizer based on conditional mutual information (CMI) between predictions and sensitive attributes given the true outcome. The CMI regularizer is compatible with any deep model trained via gradient-based optimization and serves as a scalar monitor of residual separation violations, offering tractable guarantees during training. Finally, numerical experiments support our theoretical findings: across COMPAS, UCI Adult, UCI Bank, and CelebA, the proposed method substantially reduces separation violations while matching or exceeding the utility of established baseline methods. This study thus offers a provable, stable, and flexible approach to enforcing separation in deep learning.

</details>


### [121] [MaMa: A Game-Theoretic Approach for Designing Safe Agentic Systems](https://arxiv.org/abs/2602.04431)
*Jonathan Nöther,Adish Singla,Goran Radanovic*

Main category: cs.LG

TL;DR: 提出MaMa算法，通过Stackelberg安全博弈框架自动设计能抵御部分智能体被入侵的多智能体系统，在保持任务性能的同时显著提升安全性


<details>
  <summary>Details</summary>
Motivation: 现有LLM多智能体系统在部分智能体失效或被恶意操控时存在严重安全隐患，需自动化设计具备鲁棒性的安全系统

Method: 将问题形式化为元智能体（设计者）与元对抗者（攻击者）的Stackelberg博弈，提出MaMa算法：采用LLM驱动的对抗性搜索，元智能体迭代生成系统设计，元对抗者通过最强攻击提供反馈

Result: 实证显示MaMa设计的系统在抵御最坏情况攻击时表现稳定，任务性能接近仅优化成功率的系统；且能泛化至更强对抗者、不同攻击目标或不同LLM模型

Conclusion: 该方法实现了自动化安全设计，在保证任务效能的同时显著提升系统鲁棒性，为构建可靠多智能体系统提供新范式

Abstract: LLM-based multi-agent systems have demonstrated impressive capabilities, but they also introduce significant safety risks when individual agents fail or behave adversarially. In this work, we study the automated design of agentic systems that remain safe even when a subset of agents is compromised. We formalize this challenge as a Stackelberg security game between a system designer (the Meta-Agent) and a best-responding Meta-Adversary that selects and compromises a subset of agents to minimize safety. We propose Meta-Adversary-Meta-Agent (MaMa), a novel algorithm for approximately solving this game and automatically designing safe agentic systems. Our approach uses LLM-based adversarial search, where the Meta-Agent iteratively proposes system designs and receives feedback based on the strongest attacks discovered by the Meta-Adversary. Empirical evaluations across diverse environments show that systems designed with MaMa consistently defend against worst-case attacks while maintaining performance comparable to systems optimized solely for task success. Moreover, the resulting systems generalize to stronger adversaries, as well as ones with different attack objectives or underlying LLMs, demonstrating robust safety beyond the training setting.

</details>


### [122] [Hand Gesture Recognition from Doppler Radar Signals Using Echo State Networks](https://arxiv.org/abs/2602.04436)
*Towa Sano,Gouhei Tanaka*

Main category: cs.LG

TL;DR: 提出基于储备池计算(ESN)的雷达手势识别方法，通过多储层并行处理时频特征图，在Soli和Dop-NET数据集上超越现有方法，实现低计算成本的高性能识别


<details>
  <summary>Details</summary>
Motivation: 车载和机器人场景需要轻量级手势识别技术，但传统深度学习方法计算成本过高，无法满足资源受限环境的需求

Method: 使用FMCW雷达信号，将原始数据转换为距离-时间和多普勒-时间特征图，输入一个或多个RNN储层，通过岭回归、SVM和随机森林等读出分类器进行识别

Result: 在Soli数据集11类手势任务上优于现有方法，在Dop-NET数据集4类任务上超越深度学习模型；多储层并行处理能有效提取时域和时频域特征，实现高准确率低计算成本

Conclusion: ESN方法为资源受限环境中的先进人机交互技术提供了高效解决方案，具有广阔应用前景

Abstract: Hand gesture recognition (HGR) is a fundamental technology in human computer interaction (HCI).In particular, HGR based on Doppler radar signals is suited for in-vehicle interfaces and robotic systems, necessitating lightweight and computationally efficient recognition techniques. However, conventional deep learning-based methods still suffer from high computational costs. To address this issue, we propose an Echo State Network (ESN) approach for radar-based HGR, using frequency-modulated-continuous-wave (FMCW) radar signals. Raw radar data is first converted into feature maps, such as range-time and Doppler-time maps, which are then fed into one or more recurrent neural network-based reservoirs. The obtained reservoir states are processed by readout classifiers, including ridge regression, support vector machines, and random forests. Comparative experiments demonstrate that our method outperforms existing approaches on an 11-class HGR task using the Soli dataset and surpasses existing deep learning models on a 4-class HGR task using the Dop-NET dataset. The results indicate that parallel processing using multi-reservoir ESNs are effective for recognizing temporal patterns from the multiple different feature maps in the time-space and time-frequency domains. Our ESN approaches achieve high recognition performance with low computational cost in HGR, showing great potential for more advanced HCI technologies, especially in resource-constrained environments.

</details>


### [123] [Greedy-Gnorm: A Gradient Matrix Norm-Based Alternative to Attention Entropy for Head Pruning](https://arxiv.org/abs/2602.04491)
*Yuxi Guo,Paul Sheridan*

Main category: cs.LG

TL;DR: 该论文提出Greedy-Gnorm，一种动态注意力头剪枝方法，通过在每次剪枝后重新计算梯度范数的重要性分数，在多个Transformer模型上相比静态方法能更好地保持精度。


<details>
  <summary>Details</summary>
Motivation: 在绿色AI时代，Transformer模型压缩对降低能耗至关重要，但现有剪枝方法使用静态重要性评分，无法捕捉迭代移除过程中注意力头的动态变化角色。

Method: Greedy-Gradient norm (Greedy-Gnorm) 通过计算Q/K/V梯度块l2范数的元素乘积来动态评分注意力头，使用保留验证集估计并在每次贪心迭代中更新，避免过时排序。

Result: 在BERT、ALBERT、RoBERTa和XLM-RoBERTa上的广泛实验表明，Greedy-Gnorm在大量移除注意力头时能持续保持精度，优于注意力熵方法。

Conclusion: 这种基于梯度的动态方法通过有效减小模型尺寸同时保持任务性能，为实现更节能的Transformer模型部署提供了有希望的路径。

Abstract: Attention head pruning has emerged as an effective technique for transformer model compression, an increasingly important goal in the era of Green AI. However, existing pruning methods often rely on static importance scores, which fail to capture the evolving role of attention heads during iterative removal. We propose Greedy-Gradient norm (Greedy-Gnorm), a novel head pruning algorithm that dynamically recalculates head importance after each pruning step. Specifically, each head is scored by the elementwise product of the l2-norms of its Q/K/V gradient blocks, as estimated from a hold-out validation set and updated at every greedy iteration. This dynamic approach to scoring mitigates against stale rankings and better reflects gradient-informed importance as pruning progresses. Extensive experiments on BERT, ALBERT, RoBERTa, and XLM-RoBERTa demonstrate that Greedy-Gnorm consistently preserves accuracy under substantial head removal, outperforming attention entropy. By effectively reducing model size while maintaining task performance, Greedy-Gnorm offers a promising step toward more energy-efficient transformer model deployment.

</details>


### [124] [Forget to Generalize: Iterative Adaptation for Generalization in Federated Learning](https://arxiv.org/abs/2602.04536)
*Abdulrahman Alotaibi,Irene Tenison,Miriam Kim,Isaac Lee,Lalana Kagal*

Main category: cs.LG

TL;DR: 针对Web系统中数据非独立同分布（non-IID）导致的联邦学习性能下降问题，本文提出迭代联邦自适应（IFA）范式，通过逐代遗忘和演化策略，随机或选择性地重新初始化部分模型参数，以跳出局部最优并保留全局相关表示，在多个数据集上实现平均21.5%的精度提升。


<details>
  <summary>Details</summary>
Motivation: Web环境天然具有异构性，用户设备、地理区域、浏览模式和上下文导致数据高度多样化且非独立同分布（non-IID）。联邦学习虽能实现隐私保护的协作学习，但在真实Web系统中，non-IID数据分布会严重降低其性能，这是一个亟待解决的关键问题。

Method: 提出迭代联邦自适应（IFA）训练范式，采用生成式遗忘与演化策略。将训练分为多个世代，每代结束时选择性地重新初始化部分模型参数（随机选择或从模型较深层选择），通过这种迭代调度使模型能够逃离局部极小值并保留全局相关特征表示。

Result: 在CIFAR-10、MIT-Indoors和Stanford Dogs数据集上的广泛实验表明，该方法显著提升了全局准确率，尤其在客户端数据为非独立同分布时效果更佳。该方法可与任何联邦学习算法结合使用，平均带来21.5%的性能提升。

Conclusion: IFA范式有效解决了异构联邦学习环境中的泛化问题，为构建可扩展、隐私保护的真实Web系统智能提供了新思路，推动了分布式机器学习在实际应用中的发展。

Abstract: The Web is naturally heterogeneous with user devices, geographic regions, browsing patterns, and contexts all leading to highly diverse, unique datasets. Federated Learning (FL) is an important paradigm for the Web because it enables privacy-preserving, collaborative machine learning across diverse user devices, web services and clients without needing to centralize sensitive data. However, its performance degrades severely under non-IID client distributions that is prevalent in real-world web systems. In this work, we propose a new training paradigm - Iterative Federated Adaptation (IFA) - that enhances generalization in heterogeneous federated settings through generation-wise forget and evolve strategy. Specifically, we divide training into multiple generations and, at the end of each, select a fraction of model parameters (a) randomly or (b) from the later layers of the model and reinitialize them. This iterative forget and evolve schedule allows the model to escape local minima and preserve globally relevant representations. Extensive experiments on CIFAR-10, MIT-Indoors, and Stanford Dogs datasets show that the proposed approach improves global accuracy, especially when the data cross clients are Non-IID. This method can be implemented on top any federated algorithm to improve its generalization performance. We observe an average of 21.5%improvement across datasets. This work advances the vision of scalable, privacy-preserving intelligence for real-world heterogeneous and distributed web systems.

</details>


### [125] [Gradient Flow Through Diagram Expansions: Learning Regimes and Explicit Solutions](https://arxiv.org/abs/2602.04548)
*Dmitry Yarotsky,Eugene Golikov,Yaroslav Gusev*

Main category: cs.LG

TL;DR: 提出基于费曼图展开的数学框架，解析大模型梯度流的标度律与学习阶段，并以高阶张量CP分解为例验证理论预测与实验高度吻合


<details>
  <summary>Details</summary>
Motivation: 揭示大规模学习问题中梯度流的不同学习阶段（如NTK、均值场 regime）及其标度律，解决非线性梯度流解析解的难题

Method: 1. 构建损失演化的形式幂级数展开（系数由类费曼图编码）<br>2. 通过大尺寸极限分析学习阶段<br>3. 将级数求和约化为可特征线法求解的一阶偏微分方程

Result: 1. 发现CP张量分解存在自由演化/NTK/欠-过参数化均值场等多重梯度流阶段<br>2. 明确参数标度、张量阶数与模型对称性对阶段的调控机制<br>3. 理论预测与实验结果高度一致

Conclusion: 该框架不仅统一解析了大规模学习的复杂动力学，更通过PDE约化实现了宽泛场景下的显式求解，为理解深度学习训练过程提供了普适数学工具

Abstract: We develop a general mathematical framework to analyze scaling regimes and derive explicit analytic solutions for gradient flow (GF) in large learning problems. Our key innovation is a formal power series expansion of the loss evolution, with coefficients encoded by diagrams akin to Feynman diagrams. We show that this expansion has a well-defined large-size limit that can be used to reveal different learning phases and, in some cases, to obtain explicit solutions of the nonlinear GF. We focus on learning Canonical Polyadic (CP) decompositions of high-order tensors, and show that this model has several distinct extreme lazy and rich GF regimes such as free evolution, NTK and under- and over-parameterized mean-field. We show that these regimes depend on the parameter scaling, tensor order, and symmetry of the model in a specific and subtle way. Moreover, we propose a general approach to summing the formal loss expansion by reducing it to a PDE; in a wide range of scenarios, it turns out to be 1st order and solvable by the method of characteristics. We observe a very good agreement of our theoretical predictions with experiment.

</details>


### [126] [Finding Structure in Continual Learning](https://arxiv.org/abs/2602.04555)
*Pourya Shamsolmoali,Masoumeh Zareapoor*

Main category: cs.LG

TL;DR: A paper that reformulates continual learning using Douglas-Rachford Splitting to decouple plasticity and stability objectives, enabling more stable learning without complex auxiliary components.


<details>
  <summary>Details</summary>
Motivation: Continual learning suffers from the plasticity-stability dilemma where acquiring new knowledge causes catastrophic forgetting. Current methods create gradient conflicts by summing competing loss terms and require complex strategies like external memory replay or parameter regularization.

Method: Reformulates the continual learning objective using Douglas-Rachford Splitting (DRS), which reframes learning as a negotiation between two decoupled objectives (plasticity for new tasks and stability of old knowledge) through iterative proximal operators.

Result: Achieves an efficient balance between stability and plasticity without needing auxiliary modules or complex add-ons, providing a more principled and stable learning dynamic.

Conclusion: Offers a simpler yet more powerful paradigm for continual learning systems by decoupling competing objectives rather than treating them as direct trade-offs.

Abstract: Learning from a stream of tasks usually pits plasticity against stability: acquiring new knowledge often causes catastrophic forgetting of past information. Most methods address this by summing competing loss terms, creating gradient conflicts that are managed with complex and often inefficient strategies such as external memory replay or parameter regularization. We propose a reformulation of the continual learning objective using Douglas-Rachford Splitting (DRS). This reframes the learning process not as a direct trade-off, but as a negotiation between two decoupled objectives: one promoting plasticity for new tasks and the other enforcing stability of old knowledge. By iteratively finding a consensus through their proximal operators, DRS provides a more principled and stable learning dynamic. Our approach achieves an efficient balance between stability and plasticity without the need for auxiliary modules or complex add-ons, providing a simpler yet more powerful paradigm for continual learning systems.

</details>


### [127] [Probabilistic Label Spreading: Efficient and Consistent Estimation of Soft Labels with Epistemic Uncertainty on Graphs](https://arxiv.org/abs/2602.04574)
*Jonathan Klees,Tobias Riedlinger,Peter Stehr,Bennet Böddecker,Daniel Kondermann,Matthias Rottmann*

Main category: cs.LG

TL;DR: 提出概率标签传播方法，通过图扩散从稀疏标注中估计不确定性，降低标注成本并在图像分类基准上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 安全AI感知任务面临高质量标注数据稀缺的挑战，标注本身存在aleatoric和epistemic不确定性，而传统众包方法因标注成本过高难以规模化应用。

Method: 假设特征空间中标签平滑，利用图扩散方法将单个标注传播至未标注数据，实现不确定性量化。证明了当每点标注数趋近于零时估计仍具有一致性。

Result: 实验表明该方法在达到相同标签质量时大幅降低标注预算，并在Data-Centric Image Classification基准上取得新的最优结果。

Conclusion: 该方法为大规模感知任务提供了可扩展的不确定性估计框架，在提升AI安全性的同时显著减少标注成本。

Abstract: Safe artificial intelligence for perception tasks remains a major challenge, partly due to the lack of data with high-quality labels. Annotations themselves are subject to aleatoric and epistemic uncertainty, which is typically ignored during annotation and evaluation. While crowdsourcing enables collecting multiple annotations per image to estimate these uncertainties, this approach is impractical at scale due to the required annotation effort. We introduce a probabilistic label spreading method that provides reliable estimates of aleatoric and epistemic uncertainty of labels. Assuming label smoothness over the feature space, we propagate single annotations using a graph-based diffusion method. We prove that label spreading yields consistent probability estimators even when the number of annotations per data point converges to zero. We present and analyze a scalable implementation of our method. Experimental results indicate that, compared to baselines, our approach substantially reduces the annotation budget required to achieve a desired label quality on common image datasets and achieves a new state of the art on the Data-Centric Image Classification benchmark.

</details>


### [128] [Stochastic Decision Horizons for Constrained Reinforcement Learning](https://arxiv.org/abs/2602.04599)
*Nikola Milosevic,Leonard Franz,Daniel Haeufle,Georg Martius,Nico Scherf,Pavel Kolev*

Main category: cs.LG

TL;DR: 该论文提出了一种基于随机决策视界的"控制即推断"框架，通过生存加权目标函数解决约束马尔可夫决策过程(CMDP)中的离线策略可扩展性问题，并在标准基准和肌肉骨骼仿真环境中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统的通过加性代价约束和对偶变量处理约束强化学习的方法阻碍了离线策略学习的可扩展性。该研究旨在提出一种新的数学框架，能够在保持约束处理能力的同时，兼容高效的离线策略actor-critic算法。

Method: 提出基于随机决策视界的"控制即推断"建模方法，其中约束违反会衰减奖励贡献并缩短有效规划视界。该方法生成生存加权目标函数，保持与经验回放的兼容性。定义了两种违反语义——吸收态和虚拟终止，它们产生相同的生存加权回报但形成不同的优化结构，分别对应SAC和MPO风格的策略改进。

Result: 在标准基准测试中，该方法表现出更高的样本效率和更优的回报-违反权衡。特别是虚拟终止版本的MPO（VT-MPO）能够有效扩展到高维肌肉骨骼Hyfydy仿真环境。

Conclusion: 所提出的生存加权目标函数框架为离线策略约束强化学习提供了可扩展的解决方案，虚拟终止语义在实际应用中展现出更好的扩展性能。

Abstract: Constrained Markov decision processes (CMDPs) provide a principled model for handling constraints, such as safety and other auxiliary objectives, in reinforcement learning. The common approach of using additive-cost constraints and dual variables often hinders off-policy scalability. We propose a Control as Inference formulation based on stochastic decision horizons, where constraint violations attenuate reward contributions and shorten the effective planning horizon via state-action-dependent continuation. This yields survival-weighted objectives that remain replay-compatible for off-policy actor-critic learning. We propose two violation semantics, absorbing and virtual termination, that share the same survival-weighted return but result in distinct optimization structures that lead to SAC/MPO-style policy improvement. Experiments demonstrate improved sample efficiency and favorable return-violation trade-offs on standard benchmarks. Moreover, MPO with virtual termination (VT-MPO) scales effectively to our high-dimensional musculoskeletal Hyfydy setup.

</details>


### [129] [Jacobian Regularization Stabilizes Long-Term Integration of Neural Differential Equations](https://arxiv.org/abs/2602.04608)
*Maya Janvier,Julien Salomon,Etienne Meunier*

Main category: cs.LG

TL;DR: 提出通过正则化神经微分方程(NDE)的雅可比矩阵方向导数来稳定长期积分，在已知/未知动力学场景下分别设计正则化方法，显著降低训练成本的同时提升ODE/PDE模拟稳定性


<details>
  <summary>Details</summary>
Motivation: 神经微分方程(NDE)和混合模型在物理系统建模中日益重要，但长期积分时存在稳定性和精度问题；现有展开轨迹训练法虽能缓解发散，却因需迭代计算梯度导致计算成本过高

Method: 设计两种雅可比矩阵方向导数正则化方法：针对已知动力学场景直接推导动态方程的方向导数，针对未知动力学场景用有限差分近似方向导数

Result: 在多个常微分方程和偏微分方程测试中，两种方法均以远低于长轨迹展开训练的成本，显著提升了长期模拟的稳定性

Conclusion: 该方法为大规模系统的神经微分方程长期积分训练开辟了新路径，有效平衡了计算效率与模拟稳定性

Abstract: Hybrid models and Neural Differential Equations (NDE) are getting increasingly important for the modeling of physical systems, however they often encounter stability and accuracy issues during long-term integration. Training on unrolled trajectories is known to limit these divergences but quickly becomes too expensive due to the need for computing gradients over an iterative process. In this paper, we demonstrate that regularizing the Jacobian of the NDE model via its directional derivatives during training stabilizes long-term integration in the challenging context of short training rollouts. We design two regularizations, one for the case of known dynamics where we can directly derive the directional derivatives of the dynamic and one for the case of unknown dynamics where they are approximated using finite differences. Both methods, while having a far lower cost compared to long rollouts during training, are successful in improving the stability of long-term simulations for several ordinary and partial differential equations, opening up the door to training NDE methods for long-term integration of large scale systems.

</details>


### [130] [Resilient Load Forecasting under Climate Change: Adaptive Conditional Neural Processes for Few-Shot Extreme Load Forecasting](https://arxiv.org/abs/2602.04609)
*Chenxi Hu,Yue Ma,Yifan Wu,Yunhe Hou*

Main category: cs.LG

TL;DR: 提出AdaCNP概率预测模型，通过自适应重加权历史上下文信息解决极端天气下电力负荷数据稀缺问题，实现少样本适应并提升预测可靠性


<details>
  <summary>Details</summary>
Motivation: 极端天气导致电力负荷曲线出现剧烈波动和尖峰，传统预测方法因极端事件样本稀少且不规则而难以准确预测，可能引发供电短缺、过载和服务中断等公共安全风险

Method: AdaCNP在共享嵌入空间学习相似性，评估历史上下文与当前条件的关联度并动态重加权，突出关键历史证据，无需昂贵微调即可生成预测分布

Result: 在真实电力系统数据上，AdaCNP相比最强基线均方误差降低22%，负对数似然最低，极端时期预测更稳健且概率输出更可靠

Conclusion: AdaCNP能有效缓解突变分布偏移和极端样本稀缺的双重挑战，为极端事件下的弹性电力系统运行提供可信预测支持

Abstract: Extreme weather can substantially change electricity consumption behavior, causing load curves to exhibit sharp spikes and pronounced volatility. If forecasts are inaccurate during those periods, power systems are more likely to face supply shortfalls or localized overloads, forcing emergency actions such as load shedding and increasing the risk of service disruptions and public-safety impacts. This problem is inherently difficult because extreme events can trigger abrupt regime shifts in load patterns, while relevant extreme samples are rare and irregular, making reliable learning and calibration challenging. We propose AdaCNP, a probabilistic forecasting model for data-scarce condition. AdaCNP learns similarity in a shared embedding space. For each target data, it evaluates how relevant each historical context segment is to the current condition and reweights the context information accordingly. This design highlights the most informative historical evidence even when extreme samples are rare. It enables few-shot adaptation to previously unseen extreme patterns. AdaCNP also produces predictive distributions for risk-aware decision-making without expensive fine-tuning on the target domain. We evaluate AdaCNP on real-world power-system load data and compare it against a range of representative baselines. The results show that AdaCNP is more robust during extreme periods, reducing the mean squared error by 22\% relative to the strongest baseline while achieving the lowest negative log-likelihood, indicating more reliable probabilistic outputs. These findings suggest that AdaCNP can effectively mitigate the combined impact of abrupt distribution shifts and scarce extreme samples, providing a more trustworthy forecasting for resilient power system operation under extreme events.

</details>


### [131] [QUATRO: Query-Adaptive Trust Region Policy Optimization for LLM Fine-tuning](https://arxiv.org/abs/2602.04620)
*Doyeon Lee,Eunyi Lyou,Hyunsoo Cho,Sookyung Kim,Joonseok Lee,Jaemoo Choi*

Main category: cs.LG

TL;DR: 针对GRPO类强化学习微调算法的脆弱性问题，提出查询自适应信任域策略优化方法QUATRO，通过原则性优化直接强制信任域约束，实现稳定且熵可控的训练，在数学推理基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: GRPO风格的强化学习微调算法虽流行，但依赖启发式信任域近似，导致优化行为脆弱，全局重要性比率截断和组归一化无法有效约束超出截断范围的重要性比率样本。

Method: 提出查询自适应信任域策略优化（QUATRO），通过原则性优化直接强制信任域约束，得到清晰可解释的目标函数，实现对策略更新的显式控制，稳定且熵可控的优化，稳定项从精确信任域公式中自然产生。

Result: 在多个数学推理基准测试上经验验证，QUATRO在增加策略陈旧性和激进学习率下表现稳定训练，并在整个训练过程中保持良好控制的熵。

Conclusion: QUATRO通过精确信任域公式解决了启发式近似的问题，提供了更稳定、可控的优化方法，为LLM强化学习微调提供了新思路。

Abstract: GRPO-style reinforcement learning (RL)-based LLM fine-tuning algorithms have recently gained popularity. Relying on heuristic trust-region approximations, however, they can lead to brittle optimization behavior, as global importance-ratio clipping and group-wise normalization fail to regulate samples whose importance ratios fall outside the clipping range. We propose Query-Adaptive Trust-Region policy Optimization (QUATRO), which directly enforces trust-region constraints through a principled optimization. This yields a clear and interpretable objective that enables explicit control over policy updates and stable, entropy-controlled optimization, with a stabilizer terms arising intrinsically from the exact trust-region formulation. Empirically verified on diverse mathematical reasoning benchmarks, QUATRO shows stable training under increased policy staleness and aggressive learning rates, maintaining well-controlled entropy throughout training.

</details>


### [132] [MTS-JEPA: Multi-Resolution Joint-Embedding Predictive Architecture for Time-Series Anomaly Prediction](https://arxiv.org/abs/2602.04643)
*Yanan He,Yunshi Wen,Xin Wang,Tengfei Ma*

Main category: cs.LG

TL;DR: 提出MTS-JEPA架构，通过多分辨率预测目标和软码本瓶颈解决多元时间序列异常预测中的表示崩溃和跨尺度前兆信号捕捉问题，实现早期预警的SOTA性能


<details>
  <summary>Details</summary>
Motivation: 多元时间序列支撑现代关键基础设施，异常预测对主动风险缓解至关重要；现有联合嵌入预测架构存在表示崩溃和无法捕捉多时间尺度前兆信号的局限

Method: 设计MTS-JEPA专用架构：1) 多分辨率预测目标分离瞬时冲击与长期趋势；2) 软码本瓶颈捕获离散状态转移；3) 该约束作为内在正则化器确保优化稳定性

Result: 在标准基准测试中有效防止退化解，早期预警协议下实现最先进性能

Conclusion: 所提方法成功解决JEPA在多元时间序列异常预测中的核心缺陷，为关键基础设施风险管理提供有效方案

Abstract: Multivariate time series underpin modern critical infrastructure, making the prediction of anomalies a vital necessity for proactive risk mitigation. While Joint-Embedding Predictive Architectures (JEPA) offer a promising framework for modeling the latent evolution of these systems, their application is hindered by representation collapse and an inability to capture precursor signals across varying temporal scales. To address these limitations, we propose MTS-JEPA, a specialized architecture that integrates a multi-resolution predictive objective with a soft codebook bottleneck. This design explicitly decouples transient shocks from long-term trends, and utilizes the codebook to capture discrete regime transitions. Notably, we find this constraint also acts as an intrinsic regularizer to ensure optimization stability. Empirical evaluations on standard benchmarks confirm that our approach effectively prevents degenerate solutions and achieves state-of-the-art performance under the early-warning protocol.

</details>


### [133] [SAFE: Stable Alignment Finetuning with Entropy-Aware Predictive Control for RLHF](https://arxiv.org/abs/2602.04651)
*Dipan Maity*

Main category: cs.LG

TL;DR: This paper proposes SAFE, a novel RLHF algorithm that replaces PPO with a more stable method combining Double Soft-Min Critic, entropy-gated KL regulation, and PID-controlled adaptive thresholds to achieve better reward and stability with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: PPO, the standard method for RLHF, has several issues including heuristic motivation, ad-hoc KL-divergence constraint handling, reward oscillations, entropy collapse, value function drift, and sudden policy divergence that require frequent restarts and extensive hyperparameter tuning. The paper aims to develop a more stable and reliable RLHF method.

Method: The paper develops SAFE (Stable Alignment Finetuning with Entropy-aware control), which combines: 1) Double Soft-Min Critic for pessimistic value estimation, 2) Multi-layer stabilization framework with entropy-gated KL regulation, and 3) PID-controlled adaptive thresholds. Unlike PPO's symmetric KL penalties, SAFE distinguishes high-entropy exploration from low-entropy mode collapse and dynamically adjusts penalties based on reward velocity.

Result: Experiments on a 3B parameter model show SAFE achieves +5.15% training-average reward than PPO (0.725 vs 0.689), negligible reward crashes, and superior KL control compared to PPO.

Conclusion: SAFE provides an interpretable, crash-resistant RLHF framework that maintains aggressive learning speed while ensuring stable long-horizon optimization suitable for production deployment, with minimal computational overhead.

Abstract: Optimization (PPO) has been positioned by recent literature as the canonical method for the RL part of RLHF. PPO performs well empirically but has a heuristic motivation and handles the KL-divergence constraint used in LM-RLHF in an ad-hoc manner and suffers form reward oscillations, entropy collapse, value function drift, and sudden policy divergence that require frequent restarts and extensive hyperparameter tuning. In this paper, we develop a new pure on policy actor-critic RL method for the LM-RLHF setting. We present SAFE (Stable Alignment Finetuning with Entropy-aware control),a novel RLHF algorithm that combines a Double Soft-Min Critic for pessimistic value estimation with a new multi-layer stabilization framework combining entropy-gated KL regulation, and PID-controlled adaptive thresholds. Unlike standard PPO's symmetric KL penalties, SAFE distinguishes high-entropy exploration from low-entropy mode collapse and adjusts penalties dynamically based on reward velocity. Experiments on a 3B parameter model show SAFE achieves +5.15\% training-average reward than PPO (0.725 vs 0.689), negligible reward crashes, and superior KL control than ppo . Our method adds minimal computational overhead and provides an interpretable, crash-resistant RLHF framework that maintains aggressive learning speed while ensuring stable long-horizon optimization suitable for production deployment. Code is available at https://github.com/ryyzn9/SAFE

</details>


### [134] [Generalized Schrödinger Bridge on Graphs](https://arxiv.org/abs/2602.04675)
*Panagiotis Theodoropoulos,Juno Nam,Evangelos Theodorou,Jaemoo Choi*

Main category: cs.LG

TL;DR: 提出一种新的图上的广义薛定谔桥（GSBoG）框架，通过似然优化学习可执行的时空马尔可夫链策略，解决图运输中的可扩展性和表达性问题。


<details>
  <summary>Details</summary>
Motivation: 现有图运输方法受限于严格假设、在稀疏拓扑上泛化能力差、随图规模和时间范围扩展性差，缺乏可操作策略的表达能力。

Method: GSBoG是数据驱动框架，学习带状态代价的任意图上的可控连续时间马尔可夫链策略。采用似然优化方法满足端点约束，同时优化中间行为，避免密集全局求解器以提高可扩展性。

Result: 在真实图拓扑上的广泛实验表明，GSBoG能可靠学习准确的、尊重拓扑结构的策略，同时优化应用特定的中间状态代价，展现出广泛的适用性。

Conclusion: GSBoG为图运输问题提供了可扩展、高表达性的解决方案，为成本感知的动态图运输开辟了新研究方向。

Abstract: Transportation on graphs is a fundamental challenge across many domains, where decisions must respect topological and operational constraints. Despite the need for actionable policies, existing graph-transport methods lack this expressivity. They rely on restrictive assumptions, fail to generalize across sparse topologies, and scale poorly with graph size and time horizon. To address these issues, we introduce Generalized Schrödinger Bridge on Graphs (GSBoG), a novel scalable data-driven framework for learning executable controlled continuous-time Markov chain (CTMC) policies on arbitrary graphs under state cost augmented dynamics. Notably, GSBoG learns trajectory-level policies, avoiding dense global solvers and thereby enhancing scalability. This is achieved via a likelihood optimization approach, satisfying the endpoint marginals, while simultaneously optimizing intermediate behavior under state-dependent running costs. Extensive experimentation on challenging real-world graph topologies shows that GSBoG reliably learns accurate, topology-respecting policies while optimizing application-specific intermediate state costs, highlighting its broad applicability and paving new avenues for cost-aware dynamical transport on general graphs.

</details>


### [135] [REDistill: Robust Estimator Distillation for Balancing Robustness and Efficiency](https://arxiv.org/abs/2602.04677)
*Ondrej Tybl,Lukas Neumann*

Main category: cs.LG

TL;DR: REDistill采用幂散度损失替代KL散度进行知识蒸馏，能有效处理教师模型的噪声和过度自信预测，提升鲁棒性且无需额外超参数调优。


<details>
  <summary>Details</summary>
Motivation: 标准知识蒸馏假设教师模型提供可靠的软标签，但实践中教师预测常含噪声或过度自信。现有修正方法依赖启发式策略且需大量超参数调优，泛化性差。

Method: 提出REDistill框架，基于鲁棒统计理论，用幂散度损失替代标准KD目标。该损失能自适应降低不可靠教师输出的权重，同时保留有效的logit关系。仅需logits，可无缝集成到现有KD流程，计算开销可忽略。

Result: 在CIFAR-100和ImageNet-1k上的广泛实验表明，REDistill在不同师生架构下均能持续提升学生模型准确率，且无需针对特定模型调优超参数。

Conclusion: REDistill具有强鲁棒性，能很好地泛化到未见过的师生模型对。

Abstract: Knowledge Distillation (KD) transfers knowledge from a large teacher model to a smaller student by aligning their predictive distributions. However, conventional KD formulations - typically based on Kullback-Leibler divergence - assume that the teacher provides reliable soft targets. In practice, teacher predictions are often noisy or overconfident, and existing correction-based approaches rely on ad-hoc heuristics and extensive hyper-parameter tuning, which hinders generalization. We introduce REDistill (Robust Estimator Distillation), a simple yet principled framework grounded in robust statistics. REDistill replaces the standard KD objective with a power divergence loss, a generalization of KL divergence that adaptively downweights unreliable teacher output while preserving informative logit relationships. This formulation provides a unified and interpretable treatment of teacher noise, requires only logits, integrates seamlessly into existing KD pipelines, and incurs negligible computational overhead. Extensive experiments on CIFAR-100 and ImageNet-1k demonstrate that REDistill consistently improves student accuracy in diverse teacher-student architectures. Remarkably, it achieves these gains without model-specific hyper-parameter tuning, underscoring its robustness and strong generalization to unseen teacher-student pairs.

</details>


### [136] [Static and auto-regressive neural emulation of phytoplankton biomass dynamics from physical predictors in the global ocean](https://arxiv.org/abs/2602.04689)
*Mahima Lakra,Ronan Fablet,Lucas Drumetz,Etienne Pauthenet,Elodie Martinez*

Main category: cs.LG

TL;DR: 利用深度学习模型（尤其是UNet架构）结合卫星观测和环境数据，可较准确预测全球海洋浮游植物生物量的时空分布，短期（≤5个月）预测效果良好，但存在低估低频变化幅度和长期预测性能下降的局限性。


<details>
  <summary>Details</summary>
Motivation: 浮游植物对海洋生态系统和全球生物地球化学循环至关重要，但现有数值模型因参数化不足、观测数据稀缺及海洋过程复杂性，难以准确模拟其动态变化。

Method: 测试多种深度学习架构（CNN、ConvLSTM、4CastNet、UNet），采用1-2个月环境数据作为输入；进一步开发自回归UNet模型，利用历史预测值迭代预测未来状态。

Result: UNet在复现季节性和年际变化模式上表现最优；自回归UNet可实现≤5个月的准确短期预测，但预测时长增加时性能下降，且模型普遍低估浮游植物生物量的低频变化幅度。

Conclusion: 结合海洋物理预测因子与深度学习技术能有效重建和短期预测浮游植物动态，为气候变化背景下监测海洋健康和支持生态系统管理提供新工具。

Abstract: Phytoplankton is the basis of marine food webs, driving both ecological processes and global biogeochemical cycles. Despite their ecological and climatic significance, accurately simulating phytoplankton dynamics remains a major challenge for biogeochemical numerical models due to limited parameterizations, sparse observational data, and the complexity of oceanic processes. Here, we explore how deep learning models can be used to address these limitations predicting the spatio-temporal distribution of phytoplankton biomass in the global ocean based on satellite observations and environmental conditions. First, we investigate several deep learning architectures. Among the tested models, the UNet architecture stands out for its ability to reproduce the seasonal and interannual patterns of phytoplankton biomass more accurately than other models like CNNs, ConvLSTM, and 4CastNet. When using one to two months of environmental data as input, UNet performs better, although it tends to underestimate the amplitude of low-frequency changes in phytoplankton biomass. Thus, to improve predictions over time, an auto-regressive version of UNet was also tested, where the model uses its own previous predictions to forecast future conditions. This approach works well for short-term forecasts (up to five months), though its performance decreases for longer time scales. Overall, our study shows that combining ocean physical predictors with deep learning allows for reconstruction and short-term prediction of phytoplankton dynamics. These models could become powerful tools for monitoring ocean health and supporting marine ecosystem management, especially in the context of climate change.

</details>


### [137] [Towards Understanding and Avoiding Limitations of Convolutions on Graphs](https://arxiv.org/abs/2602.04709)
*Andreas Roth*

Main category: cs.LG

TL;DR: This paper provides a theoretical analysis of Message-Passing Neural Networks (MPNNs), identifying two key limiting properties—Shared Component Amplification (SCA) and Component Dominance (CD)—that cause rank collapse (generalizing over-smoothing). It proposes frameworks (MRS, MIMO-GC, LMGC) to avoid SCA using multiple edge relations/computational graphs, and a PageRank-based variant to address CD, enabling infinite message-passing while preserving node features.


<details>
  <summary>Details</summary>
Motivation: MPNNs have limited real-world impact due to poorly understood theoretical foundations, causing fragmented research efforts. A deeper theoretical understanding is needed to enable more targeted solutions and precise communication within the field.

Method: In-depth theoretical analysis identifying SCA and CD properties in MPNNs, leading to three frameworks: Multi-Relational Split (MRS) for multiple edge relations, MIMO-GC for multiple computational graphs (with localized variant LMGC), and a personalized PageRank-based MPNN variant.

Result: The analysis reveals rank collapse as a generalization of over-smoothing. MRS/MIMO-GC/LMGC successfully avoid SCA, while the PageRank-based variant addresses CD, allowing infinitely many message-passing iterations while preserving initial node features.

Conclusion: This work deepens the theoretical understanding of MPNN limitations by generalizing over-smoothing into rank collapse, providing practical frameworks to overcome these issues, and establishing a connection to PageRank that enables infinite-depth MPNNs.

Abstract: While message-passing neural networks (MPNNs) have shown promising results, their real-world impact remains limited. Although various limitations have been identified, their theoretical foundations remain poorly understood, leading to fragmented research efforts. In this thesis, we provide an in-depth theoretical analysis and identify several key properties limiting the performance of MPNNs. Building on these findings, we propose several frameworks that address these shortcomings. We identify two properties exhibited by many MPNNs: shared component amplification (SCA), where each message-passing iteration amplifies the same components across all feature channels, and component dominance (CD), where a single component gets increasingly amplified as more message-passing steps are applied. These properties lead to the observable phenomenon of rank collapse of node representations, which generalizes the established over-smoothing phenomenon. By generalizing and decomposing over-smoothing, we enable a deeper understanding of MPNNs, more targeted solutions, and more precise communication within the field. To avoid SCA, we show that utilizing multiple computational graphs or edge relations is necessary. Our multi-relational split (MRS) framework transforms any existing MPNN into one that leverages multiple edge relations. Additionally, we introduce the spectral graph convolution for multiple feature channels (MIMO-GC), which naturally uses multiple computational graphs. A localized variant, LMGC, approximates the MIMO-GC while inheriting its beneficial properties. To address CD, we demonstrate a close connection between MPNNs and the PageRank algorithm. Based on personalized PageRank, we propose a variant of MPNNs that allows for infinitely many message-passing iterations, while preserving initial node features. Collectively, these results deepen the theoretical understanding of MPNNs.

</details>


### [138] [Bounded-Abstention Multi-horizon Time-series Forecasting](https://arxiv.org/abs/2602.04714)
*Luca Stradiotti,Laurens Devos,Anna Monreale,Jesse Davis,Andrea Pugnana*

Main category: cs.LG

TL;DR: 针对医疗、金融等高成本领域的多时域时间序列预测，该论文提出三种拒绝预测机制，利用其结构化特性，通过理论分析设计算法，在 24 个数据集上显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 医疗、金融等高成本领域需多时域预测，误预测会降低成本并削弱信任。学习拒绝预测框架允许模型在高风险时避免预测，但现有策略仅适用于单一预测，无法利用多时域预测的结构化相关性。

Method: 形式化多时域预测的拒绝预测问题，提出三种自然拒绝策略概念，理论分析推导每种问题的最优策略，并设计实现算法。

Result: 在 24 个数据集上的广泛评估表明，所提算法显著超越现有基线方法。

Conclusion: 多时域预测的结构化特性支持更丰富的拒绝预测框架，所提方法能有效利用该特性，提升预测可靠性与可信度。

Abstract: Multi-horizon time-series forecasting involves simultaneously making predictions for a consecutive sequence of subsequent time steps. This task arises in many application domains, such as healthcare and finance, where mispredictions can have a high cost and reduce trust. The learning with abstention framework tackles these problems by allowing a model to abstain from offering a prediction when it is at an elevated risk of making a misprediction. Unfortunately, existing abstention strategies are ill-suited for the multi-horizon setting: they target problems where a model offers a single prediction for each instance. Hence, they ignore the structured and correlated nature of the predictions offered by a multi-horizon forecaster. We formalize the problem of learning with abstention for multi-horizon forecasting setting and show that its structured nature admits a richer set of abstention problems. Concretely, we propose three natural notions of how a model could abstain for multi-horizon forecasting. We theoretically analyze each problem to derive the optimal abstention strategy and propose an algorithm that implements it. Extensive evaluation on 24 datasets shows that our proposed algorithms significantly outperforms existing baselines.

</details>


### [139] [Benchmarking and Enhancing PPG-Based Cuffless Blood Pressure Estimation Methods](https://arxiv.org/abs/2602.04725)
*Neville Mathew,Yidan Shen,Renjie Hu,Maham Rahimi,George Zouridakis*

Main category: cs.LG

TL;DR: 该研究创建了标准化PPG血压数据集NBPDB，发现现有模型均未达临床精度标准，但加入年龄/性别/BMI等人口统计学数据后，MInception模型误差降低23%接近国际标准


<details>
  <summary>Details</summary>
Motivation: 现有PPG无创血压模型临床精度不足且缺乏标准化评估，公开数据集异质性高且缺乏生理控制条件，阻碍了可靠临床转化

Method: 1) 从MIMIC-III和VitalDB构建含10.1万高质量PPG段的标准化数据集NBPDB；2) 系统基准测试主流PPG-BP模型；3) 改进模型并引入年龄/性别/BMI作为额外输入

Result: 1) 所有测试模型均未满足AAMI/ISO标准（平均误差<5 mmHg且标准差<8 mmHg）；2) 加入人口统计学数据后各模型性能一致提升；3) MInception模型误差降低23%，达到SBP 4.75 mmHg/DBP 2.90 mmHg的误差水平

Conclusion: 当前PPG-BP模型在标准化条件下缺乏临床实用性，但整合患者人口统计学信息可显著提升精度和生理有效性，为无创血压监测提供新方向

Abstract: Cuffless blood pressure screening based on easily acquired photoplethysmography (PPG) signals offers a practical pathway toward scalable cardiovascular health assessment. Despite rapid progress, existing PPG-based blood pressure estimation models have not consistently achieved the established clinical numerical limits such as AAMI/ISO 81060-2, and prior evaluations often lack the rigorous experimental controls necessary for valid clinical assessment. Moreover, the publicly available datasets commonly used are heterogeneous and lack physiologically controlled conditions for fair benchmarking. To enable fair benchmarking under physiologically controlled conditions, we created a standardized benchmarking subset NBPDB comprising 101,453 high-quality PPG segments from 1,103 healthy adults, derived from MIMIC-III and VitalDB. Using this dataset, we systematically benchmarked several state-of-the-art PPG-based models. The results showed that none of the evaluated models met the AAMI/ISO 81060-2 accuracy requirements (mean error $<$ 5 mmHg and standard deviation $<$ 8 mmHg). To improve model accuracy, we modified these models and added patient demographic data such as age, sex, and body mass index as additional inputs. Our modifications consistently improved performance across all models. In particular, the MInception model reduced error by 23\% after adding the demographic data and yielded mean absolute errors of 4.75 mmHg (SBP) and 2.90 mmHg (DBP), achieves accuracy comparable to the numerical limits defined by AAMI/ISO accuracy standards. Our results show that existing PPG-based BP estimation models lack clinical practicality under standardized conditions, while incorporating demographic information markedly improves their accuracy and physiological validity.

</details>


### [140] [Rationality Measurement and Theory for Reinforcement Learning Agents](https://arxiv.org/abs/2602.04737)
*Kejiang Qian,Amos Storkey,Fengxiang He*

Main category: cs.LG

TL;DR: 提出强化学习智能体的理性度量框架，定义预期理性风险与理性风险间隙，通过理论分析揭示环境偏移和算法泛化能力的影响，并验证正则化与域随机化的有效性。


<details>
  <summary>Details</summary>
Motivation: 强化学习智能体的理性属性至关重要但缺乏系统研究，需建立可量化的理论框架以评估部署时的决策合理性，尤其关注训练与部署环境差异带来的风险。

Method: 1) 定义完美理性动作为真实价值函数最速上升方向；2) 构建预期理性风险（部署轨迹期望值偏差）及其训练版本；3) 分解理性风险间隙为环境偏移（外因）和算法泛化（内因）；4) 用1-Wasserstein距离与Rademacher复杂度分别界定两类间隙上界。

Result: 理论证明：环境偏移间隙≤训练/部署转移核与初始分布的Wasserstein距离；泛化间隙≤价值函数类的经验Rademacher复杂度。实验验证正则化（层归一化、ℓ₂、权重归一化）和域随机化提升理性，环境偏移损害理性。

Conclusion: 首次建立强化学习理性度量的完整理论体系，为可部署智能体提供风险量化工具，指导通过正则化与域随机化缓解理性风险间隙，代码已开源。

Abstract: This paper proposes a suite of rationality measures and associated theory for reinforcement learning agents, a property increasingly critical yet rarely explored. We define an action in deployment to be perfectly rational if it maximises the hidden true value function in the steepest direction. The expected value discrepancy of a policy's actions against their rational counterparts, culminating over the trajectory in deployment, is defined to be expected rational risk; an empirical average version in training is also defined. Their difference, termed as rational risk gap, is decomposed into (1) an extrinsic component caused by environment shifts between training and deployment, and (2) an intrinsic one due to the algorithm's generalisability in a dynamic environment. They are upper bounded by, respectively, (1) the $1$-Wasserstein distance between transition kernels and initial state distributions in training and deployment, and (2) the empirical Rademacher complexity of the value function class. Our theory suggests hypotheses on the benefits from regularisers (including layer normalisation, $\ell_2$ regularisation, and weight normalisation) and domain randomisation, as well as the harm from environment shifts. Experiments are in full agreement with these hypotheses. The code is available at https://github.com/EVIEHub/Rationality.

</details>


### [141] [Decomposing Query-Key Feature Interactions Using Contrastive Covariances](https://arxiv.org/abs/2602.04752)
*Andrew Lee,Yonatan Belinkov,Fernanda Viégas,Martin Wattenberg*

Main category: cs.LG

TL;DR: 提出对比协方差方法分解Transformer的QK空间为可解释低秩子空间，揭示注意力机制通过查询与键在语义特征子空间的对齐产生高注意力分数。


<details>
  <summary>Details</summary>
Motivation: 尽管注意力机制是Transformer的核心，但缺乏理解模型为何关注特定标记的工具，需要可解释性方法来剖析注意力决策的内在逻辑。

Method: 研究查询-键(QK)双线性联合嵌入空间，开发对比协方差分解算法提取低秩可解释成分，通过简化的分析性和实证研究验证后应用于大语言模型，识别分类语义特征和绑定特征的QK子空间。

Result: 成功在LLM中发现人类可解释的QK子空间，证明注意力高分源于查询与键在低秩语义特征子空间的对齐，并实现注意力分数向这些特征的归因。

Conclusion: 该方法为理解注意力机制提供了可解释框架，揭示了模型关注决策的语义基础，为调试和解释大语言模型行为开辟了新途径。

Abstract: Despite the central role of attention heads in Transformers, we lack tools to understand why a model attends to a particular token. To address this, we study the query-key (QK) space -- the bilinear joint embedding space between queries and keys. We present a contrastive covariance method to decompose the QK space into low-rank, human-interpretable components. It is when features in keys and queries align in these low-rank subspaces that high attention scores are produced. We first study our method both analytically and empirically in a simplified setting. We then apply our method to large language models to identify human-interpretable QK subspaces for categorical semantic features and binding features. Finally, we demonstrate how attention scores can be attributed to our identified features.

</details>


### [142] [A Dual-TransUNet Deep Learning Framework for Multi-Source Precipitation Merging and Improving Seasonal and Extreme Estimates](https://arxiv.org/abs/2602.04757)
*Yuchen Ye,Zixuan Qi,Shixuan Li,Wei Qi,Yanpeng Cai,Chaoxia Yuan*

Main category: cs.LG

TL;DR: 提出一种双阶段TransUNet框架（DDL-MSPMF），通过融合多源降水产品和ERA5物理预测因子，显著提升中国区域降水估计精度与极端事件检测能力


<details>
  <summary>Details</summary>
Motivation: 多源卫星与再分析降水产品存在空间异质性偏差且极端降水技能不足，制约了其在水文气候监测中的应用价值

Method: 构建双阶段TransUNet模型：第一阶段分类器估计日降水发生概率；第二阶段回归器融合分类器输出及4个ERA5近地面物理预测因子，生成0.25°分辨率中国日降水数据（2001-2020）

Result: TransUNet-TransUNet配置季节性性能最优（R=0.75；RMSE=2.70 mm/天），较单回归器更稳健；对>25 mm/天强降水，显著提升中国东部大部分区域公平威胁评分，并更准确复现2021年郑州"7·20"暴雨空间格局；青藏高原独立验证表明模型在缺资料区域适用性强

Conclusion: 该框架兼具可扩展性与物理可解释性（SHAP分析揭示降水发生概率和地表气压的关键作用），为降水融合与极端事件评估提供新方案

Abstract: Multi-source precipitation products (MSPs) from satellite retrievals and reanalysis are widely used for hydroclimatic monitoring, yet spatially heterogeneous biases and limited skill for extremes still constrain their hydrologic utility. Here we develop a dual-stage TransUNet-based multi-source precipitation merging framework (DDL-MSPMF) that integrates six MSPs with four ERA5 near-surface physical predictors. A first-stage classifier estimates daily precipitation occurrence probability, and a second-stage regressor fuses the classifier outputs together with all predictors to estimate daily precipitation amount at 0.25 degree resolution over China for 2001-2020. Benchmarking against multiple deep learning and hybrid baselines shows that the TransUNet - TransUNet configuration yields the best seasonal performance (R = 0.75; RMSE = 2.70 mm/day) and improves robustness relative to a single-regressor setting. For heavy precipitation (>25 mm/day), DDL-MSPMF increases equitable threat scores across most regions of eastern China and better reproduces the spatial pattern of the July 2021 Zhengzhou rainstorm, indicating enhanced extreme-event detection beyond seasonal-mean corrections. Independent evaluation over the Qinghai-Tibet Plateau using TPHiPr further supports its applicability in data-scarce regions. SHAP analysis highlights the importance of precipitation occurrence probabilities and surface pressure, providing physically interpretable diagnostics. The proposed framework offers a scalable and explainable approach for precipitation fusion and extreme-event assessment.

</details>


### [143] [Improved Dimension Dependence for Bandit Convex Optimization with Gradient Variations](https://arxiv.org/abs/2602.04761)
*Hang Yu,Yu-Hu Yan,Peng Zhao*

Main category: cs.LG

TL;DR: This paper improves gradient-variation analysis in Bandit Convex Optimization (BCO) with two-point feedback, achieving better dimension dependence for convex/strongly convex functions and extending results to one-point bandit linear optimization, dynamic/universal regret bounds, and bandit games.


<details>
  <summary>Details</summary>
Motivation: Gradient-variation online learning connects deeply to game theory and optimization but remains underexplored under bandit feedback, unlike the well-studied full-information setting.

Method: Proposes a refined analysis on non-consecutive gradient variation—a fundamental quantity in gradient variation with bandits—to achieve improved theoretical guarantees.

Result: Improves dimension dependence for convex/strongly convex functions beyond prior work (Chiang et al., 2013); achieves first gradient-variation bounds for one-point bandit linear optimization; establishes first gradient-variation dynamic/universal regret bounds for two-point BCO; demonstrates fast convergence in bandit games.

Conclusion: The refined analysis yields problem-dependent guarantees including gradient-variance and small-loss regrets, validating effectiveness across challenging tasks like dynamic regret minimization and bandit games.

Abstract: Gradient-variation online learning has drawn increasing attention due to its deep connections to game theory, optimization, etc. It has been studied extensively in the full-information setting, but is underexplored with bandit feedback. In this work, we focus on gradient variation in Bandit Convex Optimization (BCO) with two-point feedback. By proposing a refined analysis on the non-consecutive gradient variation, a fundamental quantity in gradient variation with bandits, we improve the dimension dependence for both convex and strongly convex functions compared with the best known results (Chiang et al., 2013). Our improved analysis for the non-consecutive gradient variation also implies other favorable problem-dependent guarantees, such as gradient-variance and small-loss regrets. Beyond the two-point setup, we demonstrate the versatility of our technique by achieving the first gradient-variation bound for one-point bandit linear optimization over hyper-rectangular domains. Finally, we validate the effectiveness of our results in more challenging tasks such as dynamic/universal regret minimization and bandit games, establishing the first gradient-variation dynamic and universal regret bounds for two-point BCO and fast convergence rates in bandit games.

</details>


### [144] [NeuroCanvas: VLLM-Powered Robust Seizure Detection by Reformulating Multichannel EEG as Image](https://arxiv.org/abs/2602.04769)
*Yan Chen,Jie Peng,Moajjem Hossain Chowdhury,Tianlong Chen,Yunmei Liu*

Main category: cs.LG

TL;DR: The paper proposes NeuroCanvas, a framework for EEG-based seizure detection that uses entropy-guided channel selection and visual tokenization to address multi-channel heterogeneity and computational inefficiency in LLM-based approaches, achieving 20% F1 score improvement and 88% latency reduction.


<details>
  <summary>Details</summary>
Motivation: Manual review of long-term EEG recordings for seizure detection is labor-intensive, and while LLM-based approaches show promise, they suffer from two major issues: (1) multi-channel heterogeneity where seizure-relevant information varies across EEG channels, and (2) computational inefficiency due to encoding EEG signals into massive numbers of tokens.

Method: The NeuroCanvas framework consists of two modules: (i) Entropy-guided Channel Selector (ECS) that selects seizure-relevant channels for LLM input, and (ii) Canvas of Neuron Signal (CNS) that converts selected multi-channel heterogeneous EEG signals into structured visual representations using compact visual tokens.

Result: Across multiple seizure detection datasets, NeuroCanvas demonstrates a 20% improvement in F1 score and 88% reduction in inference latency compared to existing methods.

Conclusion: NeuroCanvas provides a scalable and effective solution for real-time and resource-efficient seizure detection in clinical practice, addressing both multi-channel heterogeneity and computational efficiency challenges in LLM-based EEG analysis.

Abstract: Accurate and timely seizure detection from Electroencephalography (EEG) is critical for clinical intervention, yet manual review of long-term recordings is labor-intensive. Recent efforts to encode EEG signals into large language models (LLMs) show promise in handling neural signals across diverse patients, but two significant challenges remain: (1) multi-channel heterogeneity, as seizure-relevant information varies substantially across EEG channels, and (2) computing inefficiency, as the EEG signals need to be encoded into a massive number of tokens for the prediction. To address these issues, we draw the EEG signal and propose the novel NeuroCanvas framework. Specifically, NeuroCanvas consists of two modules: (i) The Entropy-guided Channel Selector (ECS) selects the seizure-relevant channels input to LLM and (ii) the following Canvas of Neuron Signal (CNS) converts selected multi-channel heterogeneous EEG signals into structured visual representations. The ECS module alleviates the multi-channel heterogeneity issue, and the CNS uses compact visual tokens to represent the EEG signals that improve the computing efficiency. We evaluate NeuroCanvas across multiple seizure detection datasets, demonstrating a significant improvement of $20\%$ in F1 score and reductions of $88\%$ in inference latency. These results highlight NeuroCanvas as a scalable and effective solution for real-time and resource-efficient seizure detection in clinical practice.The code will be released at https://github.com/Yanchen30247/seizure_detect.

</details>


### [145] [Interval-Based AUC (iAUC): Extending ROC Analysis to Uncertainty-Aware Classification](https://arxiv.org/abs/2602.04775)
*Yuqi Li,Matthew M. Engelhard*

Main category: cs.LG

TL;DR: 提出适用于区间预测的不确定性感知ROC框架，定义$AUC_L$和$AUC_U$新指标，实现三区分解并支持选择性预测，在有效覆盖条件下提供理论最优AUC的上下界。


<details>
  <summary>Details</summary>
Motivation: 高风险预测中区间预测对量化不确定性至关重要，但标准ROC/AUC工具仅适用于点估计，无法捕捉预测不确定性对排序性能的影响。

Method: 提出专门针对区间预测的不确定性感知ROC框架，引入$AUC_L$和$AUC_U$两个新度量，实现ROC平面的信息性三区分解（正确/错误/不确定排序），并支持通过拒绝重叠区间案例的选择性预测。

Result: 证明在有效类条件覆盖下，$AUC_L$和$AUC_U$为理论最优AUC提供形式化下界和上界，刻画判别能力物理极限；真实数据集实验验证框架正确性与实用性。

Conclusion: 该框架广泛适用于任意区间构造方法，为不确定性感知评估和决策提供了有效工具。

Abstract: In high-stakes risk prediction, quantifying uncertainty through interval-valued predictions is essential for reliable decision-making. However, standard evaluation tools like the receiver operating characteristic (ROC) curve and the area under the curve (AUC) are designed for point scores and fail to capture the impact of predictive uncertainty on ranking performance. We propose an uncertainty-aware ROC framework specifically for interval-valued predictions, introducing two new measures: $AUC_L$ and $AUC_U$. This framework enables an informative three-region decomposition of the ROC plane, partitioning pairwise rankings into correct, incorrect, and uncertain orderings. This approach naturally supports selective prediction by allowing models to abstain from ranking cases with overlapping intervals, thereby optimizing the trade-off between abstention rate and discriminative reliability. We prove that under valid class-conditional coverage, $AUC_L$ and $AUC_U$ provide formal lower and upper bounds on the theoretical optimal AUC ($AUC^*$), characterizing the physical limit of achievable discrimination. The proposed framework applies broadly to interval-valued prediction models, regardless of the interval construction method. Experiments on real-world benchmark datasets, using bootstrap-based intervals as one instantiation, validate the framework's correctness and demonstrate its practical utility for uncertainty-aware evaluation and decision-making.

</details>


### [146] [Dynamical Regimes of Multimodal Diffusion Models](https://arxiv.org/abs/2602.04780)
*Emil Albrychiewicz,Andrés Franco Valiente,Li-Ching Chen*

Main category: cs.LG

TL;DR: This paper develops a theoretical framework for coupled diffusion models using Ornstein-Uhlenbeck processes and nonequilibrium statistical physics, revealing that multimodal generation is controlled by a hierarchy of timescales rather than simultaneous resolution. It identifies a "synchronization gap" phenomenon explaining desynchronization artifacts, derives analytical bounds for stable coupling strength, and proposes time-dependent coupling schedules as an alternative to guidance tuning.


<details>
  <summary>Details</summary>
Motivation: Despite diffusion models' success in high-fidelity synthesis, the theoretical mechanisms underlying multimodal generation remain poorly understood, particularly how different modes are coordinated during the generative process. This gap limits our ability to address common artifacts like desynchronization.

Method: The authors use coupled Ornstein-Uhlenbeck processes as a tractable theoretical model and apply nonequilibrium statistical physics of dynamical phase transitions to analyze the system. They derive analytical conditions and validate predictions through controlled experiments on MNIST datasets using exact score samplers.

Result: They demonstrate that multimodal generation follows a spectral hierarchy of interaction timescales, not simultaneous resolution. They predict and validate the "synchronization gap" where eigenmodes stabilize at different rates, explain desynchronization artifacts, and establish strict bounds on coupling strength to prevent unstable symmetry breaking. Coupling strength acts as a tunable spectral filter.

Conclusion: The framework reveals fundamental principles of multimodal generation in diffusion models, suggesting that time-dependent coupling schedules targeting mode-specific timescales could provide a principled alternative to ad hoc guidance tuning, potentially improving generation quality and stability.

Abstract: Diffusion based generative models have achieved unprecedented fidelity in synthesizing high dimensional data, yet the theoretical mechanisms governing multimodal generation remain poorly understood. Here, we present a theoretical framework for coupled diffusion models, using coupled Ornstein-Uhlenbeck processes as a tractable model. By using the nonequilibrium statistical physics of dynamical phase transitions, we demonstrate that multimodal generation is governed by a spectral hierarchy of interaction timescales rather than simultaneous resolution. A key prediction is the ``synchronization gap'', a temporal window during the reverse generative process where distinct eigenmodes stabilize at different rates, providing a theoretical explanation for common desynchronization artifacts. We derive analytical conditions for speciation and collapse times under both symmetric and anisotropic coupling regimes, establishing strict bounds for coupling strength to avoid unstable symmetry breaking. We show that the coupling strength acts as a spectral filter that enforces a tunable temporal hierarchy on generation. We support these predictions through controlled experiments with diffusion models trained on MNIST datasets and exact score samplers. These results motivate time dependent coupling schedules that target mode specific timescales, offering a potential alternative to ad hoc guidance tuning.

</details>


### [147] [Legendre Memory Unit with A Multi-Slice Compensation Model for Short-Term Wind Speed Forecasting Based on Wind Farm Cluster Data](https://arxiv.org/abs/2602.04782)
*Mumin Zhang,Haochen Zhang,Xin Zhi Khoo,Yilin Zhang,Nuo Chen,Ting Zhang,Junjie Tang*

Main category: cs.LG

TL;DR: 针对风电场集群短期风速预测难题，提出WMF-CPK-MSLMU集成模型，通过加权平均滤波去噪、Legendre记忆单元建模时空相关性，并结合Kendall相关系数补偿机制，实现更准确鲁棒的预测。


<details>
  <summary>Details</summary>
Motivation: 随着风电场集群化发展，其短期风速预测对电力系统安全运行至关重要。现有方法未能充分利用集群数据的时空相关性，导致预测精度和鲁棒性不足。

Method: 1) 采用加权平均滤波(WMF)对单场数据进行去噪预处理；2) 创新性地将Legendre记忆单元(LMU)与基于Kendall秩相关系数的补偿参数(CPK)结合，构建多切片LMU(MSLMU)；3) 设计包含数据预处理、预测和多切片补偿三模块的WMF-CPK-MSLMU集成模型，用CPK权重替代随机初始化以激活隐藏节点。

Result: 在不同风电场集群上的测试结果表明，所提模型相比现有方法在短期风速预测精度、速度和鲁棒性方面均表现出显著优越性。

Conclusion: 该模型通过自适应时空相关性建模和补偿机制，有效提升了风电场集群短期风速预测性能，为电力系统安全稳定运行提供了可靠的技术支撑。

Abstract: With more wind farms clustered for integration, the short-term wind speed prediction of such wind farm clusters is critical for normal operation of power systems. This paper focuses on achieving accurate, fast, and robust wind speed prediction by full use of cluster data with spatial-temporal correlation. First, weighted mean filtering (WMF) is applied to denoise wind speed data at the single-farm level. The Legendre memory unit (LMU) is then innovatively applied for the wind speed prediction, in combination with the Compensating Parameter based on Kendall rank correlation coefficient (CPK) of wind farm cluster data, to construct the multi-slice LMU (MSLMU). Finally, an innovative ensemble model WMF-CPK-MSLMU is proposed herein, with three key blocks: data pre-processing, forecasting, and multi-slice compensation. Advantages include: 1) LMU jointly models linear and nonlinear dependencies among farms to capture spatial-temporal correlations through backpropagation; 2) MSLMU enhances forecasting by using CPK-derived weights instead of random initialization, allowing spatial correlations to fully activate hidden nodes across clustered wind farms.; 3) CPK adaptively weights the compensation model in MSLMU and complements missing data spatially, to facilitate the whole model highly accurate and robust. Test results on different wind farm clusters indicate the effectiveness and superiority of proposed ensemble model WMF-CPK-MSLMU in the short-term prediction of wind farm clusters compared to the existing models.

</details>


### [148] [From independent patches to coordinated attention: Controlling information flow in vision transformers](https://arxiv.org/abs/2602.04784)
*Kieran A. Murphy*

Main category: cs.LG

TL;DR: 在视觉Transformer中引入变分信息瓶颈，使注意力信息传输可测量可控，揭示了全局视觉表征从局部块处理中产生的机制，得到更易分析和控制的模型。


<details>
  <summary>Details</summary>
Motivation: 使视觉Transformer中注意力传输的信息成为明确可测量的量，理解全局视觉表征如何从局部块处理中产生，并获得更易于机制分析和控制的模型。

Method: 在所有注意力介导的残差流写入处插入变分信息瓶颈（无需其他架构改动），训练带明确信息成本的模型，实现从独立块处理到全局注意力的可控频谱。

Result: 在ImageNet-100上刻画了分类行为和信息路由在该频谱上的演变，并通过分析首批传输信息的注意力头，初步揭示了全局视觉表征从局部块处理中产生的过程。

Conclusion: 通过偏向约束内部通信的学习，该方法产生的模型更易于机制分析且更可控。

Abstract: We make the information transmitted by attention an explicit, measurable quantity in vision transformers. By inserting variational information bottlenecks on all attention-mediated writes to the residual stream -- without other architectural changes -- we train models with an explicit information cost and obtain a controllable spectrum from independent patch processing to fully expressive global attention. On ImageNet-100, we characterize how classification behavior and information routing evolve across this spectrum, and provide initial insights into how global visual representations emerge from local patch processing by analyzing the first attention heads that transmit information. By biasing learning toward solutions with constrained internal communication, our approach yields models that are more tractable for mechanistic analysis and more amenable to control.

</details>


### [149] [Maximum-Volume Nonnegative Matrix Factorization](https://arxiv.org/abs/2602.04795)
*Olivier Vu Thanh,Nicolas Gillis*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Nonnegative matrix factorization (NMF) is a popular data embedding technique. Given a nonnegative data matrix $X$, it aims at finding two lower dimensional matrices, $W$ and $H$, such that $X\approx WH$, where the factors $W$ and $H$ are constrained to be element-wise nonnegative. The factor $W$ serves as a basis for the columns of $X$. In order to obtain more interpretable and unique solutions, minimum-volume NMF (MinVol NMF) minimizes the volume of $W$. In this paper, we consider the dual approach, where the volume of $H$ is maximized instead; this is referred to as maximum-volume NMF (MaxVol NMF). MaxVol NMF is identifiable under the same conditions as MinVol NMF in the noiseless case, but it behaves rather differently in the presence of noise. In practice, MaxVol NMF is much more effective to extract a sparse decomposition and does not generate rank-deficient solutions. In fact, we prove that the solutions of MaxVol NMF with the largest volume correspond to clustering the columns of $X$ in disjoint clusters, while the solutions of MinVol NMF with smallest volume are rank deficient. We propose two algorithms to solve MaxVol NMF. We also present a normalized variant of MaxVol NMF that exhibits better performance than MinVol NMF and MaxVol NMF, and can be interpreted as a continuum between standard NMF and orthogonal NMF. We illustrate our results in the context of hyperspectral unmixing.

</details>


### [150] [Evolving Afferent Architectures: Biologically-inspired Models for Damage-Avoidance Learning](https://arxiv.org/abs/2602.04807)
*Wolfgang Maass,Sabine Janzen,Prajvi Saxena,Sach Mukherjee*

Main category: cs.LG

TL;DR: 论文提出"传入学习"框架，通过进化优化和强化学习双循环生成自适应风险信号(CATs)，在生物力学数字孪生中实现23%的高风险行为减少和更好的年龄鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 受生物系统启发，生物体利用传入感知进行损伤规避。现有方法在长期、年龄相关的损伤规避方面效果不佳，缺乏将传入感知形式化为学习归纳偏置的框架，难以实现适应性内部风险信号而非简单外部奖励最小化。

Method: 提出传入学习框架，生成计算传入轨迹(CATs)作为自适应内部风险信号。采用双层架构：外循环进化优化发现有效传感架构，内循环强化学习利用CATs训练损伤规避策略。应用于多年生物力学数字孪生，提供理论收敛保证。

Result: CAT进化架构显著优于手工设计基线，高风险行为减少23%，具有更好的年龄鲁棒性，策略展现年龄依赖的行为适应。消融研究证实CAT信号、进化和预测差异都至关重要。

Conclusion: 传入学习为损伤规避学习提供了通用框架，传入架构进化提供了有效的归纳偏置，特别适用于长期年龄变化场景，为生物力学数字孪生等应用开辟了新途径。

Abstract: We introduce Afferent Learning, a framework that produces Computational Afferent Traces (CATs) as adaptive, internal risk signals for damage-avoidance learning. Inspired by biological systems, the framework uses a two-level architecture: evolutionary optimization (outer loop) discovers afferent sensing architectures that enable effective policy learning, while reinforcement learning (inner loop) trains damage-avoidance policies using these signals. This formalizes afferent sensing as providing an inductive bias for efficient learning: architectures are selected based on their ability to enable effective learning (rather than directly minimizing damage). We provide theoretical convergence guarantees under smoothness and bounded-noise assumptions. We illustrate the general approach in the challenging context of biomechanical digital twins operating over long time horizons (multiple decades of the life-course). Here, we find that CAT-based evolved architectures achieve significantly higher efficiency and better age-robustness than hand-designed baselines, enabling policies that exhibit age-dependent behavioral adaptation (23% reduction in high-risk actions). Ablation studies validate CAT signals, evolution, and predictive discrepancy as essential. We release code and data for reproducibility.

</details>


### [151] [The Key to State Reduction in Linear Attention: A Rank-based Perspective](https://arxiv.org/abs/2602.04852)
*Philipp Nazari,T. Konstantin Rusch*

Main category: cs.LG

TL;DR: 针对线性注意力模型训练后状态呈现低秩结构、容量利用不足的问题，本文理论分析了秩在检索误差中的作用，提出一种硬件感知的结构化剪枝框架，通过QR分解等方法压缩键和查询矩阵，在几乎不增加困惑度的前提下可移除50%的通道，实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 线性注意力虽计算高效，但实证显示其训练后状态常呈低秩结构，表明模型实际容量未被充分利用。低秩状态影响检索性能且造成计算资源浪费，亟需理论解释与实践优化方案。

Method: 1) 理论分析秩对线性注意力查询噪声放大及检索误差的影响机制；2) 提出硬件感知的结构化剪枝框架，适配现有CUDA内核；3) 改造现有剪枝策略并基于秩揭示QR分解提出新型结构化剪枝方法。

Result: 在多种规模模型和下游任务上验证，框架能有效压缩状态规模。关键发现：移除50%的查询和键通道仅导致困惑度轻微上升，显著提升推理速度和内存效率。

Conclusion: 线性注意力的低秩现象可通过后训练剪枝大幅缓解，所提框架在保持性能的同时实现模型轻量化，为高效Transformer部署提供新思路。

Abstract: Linear attention offers a computationally efficient yet expressive alternative to softmax attention. However, recent empirical results indicate that the state of trained linear attention models often exhibits a low-rank structure, suggesting that these models underexploit their capacity in practice. To illuminate this phenomenon, we provide a theoretical analysis of the role of rank in linear attention, revealing that low effective rank can affect retrieval error by amplifying query noise. In addition to these theoretical insights, we conjecture that the low-rank states can be substantially reduced post-training with only minimal performance degradation, yielding faster and more memory-efficient models. To this end, we propose a novel hardware-aware approach that structurally prunes key and query matrices, reducing the state size while retaining compatibility with existing CUDA kernels. We adapt several existing pruning strategies to fit our framework and, building on our theoretical analysis, propose a novel structured pruning method based on a rank-revealing QR decomposition. Our empirical results, evaluated across models of varying sizes and on various downstream tasks, demonstrate the effectiveness of our state reduction framework. We highlight that our framework enables the removal of 50% of the query and key channels at only a marginal increase in perplexity. The code for this project can be found at https://github.com/camail-official/LinearAttentionPruning.

</details>


### [152] [Multi-Head LatentMoE and Head Parallel: Communication-Efficient and Deterministic MoE Parallelism](https://arxiv.org/abs/2602.04870)
*Chenwei Cui,Rockwell Jackson,Benjamin Joseph Herrera,Ana María Tárano,Hannah Kerner*

Main category: cs.LG

TL;DR: The paper proposes Multi-Head LatentMoE with Head Parallel (HP) to replace Expert Parallel (EP) for training sparse mixture of experts models, achieving constant communication cost, perfect load balancing, and up to 1.61x faster training while maintaining identical performance.


<details>
  <summary>Details</summary>
Motivation: Large language models are expensive to train, and the standard Expert Parallel (EP) method for Sparse Mixture of Experts (MoE) has three key limitations: communication cost grows linearly with activated experts k, load imbalance issues, and data-dependent communication requiring metadata exchange.

Method: The authors propose Multi-Head LatentMoE combined with Head Parallel (HP) strategy, plus IO-aware routing and expert computation optimizations to accelerate training.

Result: Multi-Head LatentMoE with HP achieves O(1) communication cost (independent of k), perfectly balanced traffic, and deterministic communication. It trains up to 1.61x faster than MoE with EP while maintaining identical performance. With doubled granularity, it achieves higher performance while still being 1.11x faster.

Conclusion: The proposed method makes multi-billion-parameter foundation model research more accessible by providing a more efficient and scalable training approach for MoE models.

Abstract: Large language models have transformed many applications but remain expensive to train. Sparse Mixture of Experts (MoE) addresses this through conditional computation, with Expert Parallel (EP) as the standard distributed training method. However, EP has three limitations: communication cost grows linearly with the number of activated experts $k$, load imbalance affects latency and memory usage, and data-dependent communication requires metadata exchange. We propose Multi-Head LatentMoE and Head Parallel (HP), a new architecture and parallelism achieving $O(1)$ communication cost regardless of $k$, completely balanced traffic, and deterministic communication, all while remaining compatible with EP. To accelerate Multi-Head LatentMoE, we propose IO-aware routing and expert computation. Compared to MoE with EP, Multi-Head LatentMoE with HP trains up to $1.61\times$ faster while having identical performance. With doubled granularity, it achieves higher overall performance while still being $1.11\times$ faster. Our method makes multi-billion-parameter foundation model research more accessible.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [153] [Tsallis Entropy derived from the Chaitin-Kolmogorov Informational Entropy](https://arxiv.org/abs/2602.03919)
*Airton Deppman*

Main category: cond-mat.stat-mech

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We provide a rigorous first-principle derivation of the non-additive Tsallis' entropy by employing the Chaitin-Kolmogorov algorithmic information theory. By applying non-local restrictive rules on the string formation (grammar), we show that the algorithmic cost follows a power-law of the string length, instead of the linear behaviour obtained in the classical theory. As a result, the Tsallis entropy governs the increase of information. We explore the result showing, through Landauer's limit, that the heat dissipation in systems with long-range correlations is diminished. The $Ω_q$ number, which remains incompressible, now offers the possibility of a continuous increase of complexity, measured by the parameter $q$. We show the consistency of the results by a numerical simulation, and discuss Zipf's law in light of the new findings.

</details>


### [154] [Restoring Sparsity in Potts Machines via Mean-Field Constraints](https://arxiv.org/abs/2602.04200)
*Kevin Callahan-Coray,Kyle Lee,Kyle Jiang,Kerem Y. Camsari*

Main category: cond-mat.stat-mech

TL;DR: 本文针对伊辛机中约束导致的密集耦合问题，提出两种解决方案：(1) 采用多态概率数字(p-dits)的原生硬件友好型表述，避免二进制伊辛编码所需的局部密集变量内耦合；(2) 均值场约束(MFC)，用动态更新的单节点偏置替代密集的两两约束耦合。这些方法恢复了稀疏性，在FPGA上实现了相比CPU求解器数量级的加速。


<details>
  <summary>Details</summary>
Motivation: 许多实际优化问题包含约束，这些约束在伊辛机中产生密集或全连接耦合，损害了可扩展性和硬件效率。论文旨在解决这种约束引起的密度问题，使概率硬件更适用于实际应用。

Method: 论文提出两种互补方法：(1) 引入硬件感知的多态概率数字(p-dits)原生表述，避免二进制编码所需的局部密集变量内耦合，并通过二维波茨模型模拟验证；(2) 提出均值场约束(MFC)，这是一种混合方案，用动态更新的单节点偏置替代密集的两两约束耦合。该方法应用于平衡图划分问题，并在FPGA上实现。

Result: p-dit表述成功重现了二维波茨模型的已知临界行为。MFC在实现与精确全连接约束公式相当解质量的同时，显著降低了图密度。FPGA实现相比基于CPU的求解器实现了数量级的加速。

Conclusion: 所提方法通过恢复稀疏性、实现高效硬件实现并保持解质量，为概率硬件上的约束优化扩展提供了可行路径。

Abstract: Ising machines and related probabilistic hardware have emerged as promising platforms for NP-hard optimization and sampling. However, many practical problems involve constraints that induce dense or all-to-all couplings, undermining scalability and hardware efficiency. We address this constraint-induced density through two complementary approaches. First, we introduce a hardware-aware native formulation for multi-state probabilistic digits (p-dits) that avoids the locally dense intra-variable couplings required by binary Ising encodings. We validate p-dit dynamics by reproducing known critical behavior of the 2D Potts model. Second, we propose mean-field constraints (MFC), a hybrid scheme that replaces dense pairwise constraint couplings with dynamically updated single-node biases. Applied to balanced graph partitioning, MFC achieves solution quality comparable to exact all-to-all constraint formulations while dramatically reducing graph density. Finally, we demonstrate the practical impact of restored sparsity by an FPGA implementation, enabling orders-of-magnitude acceleration over CPU-based solvers. Together, these results outline a pathway for scaling constrained optimization on probabilistic hardware.

</details>


### [155] [Statistical Mechanics of the Sub-Optimal Transport](https://arxiv.org/abs/2602.04308)
*Riccardo Piombo,Dario Mazzilli,Aurelio Patelli*

Main category: cond-mat.stat-mech

TL;DR: 本文针对统计力学方法在优化问题中通常仅适用于零温极限（基态）的局限，提出了亚优传输（SOT）模型来描述实际系统中熵与成本竞争的中温区域。通过平均场理论，作者发现拉格朗日乘子的局部涨落在热力学极限下是次广延的，从而将多约束问题简化为单约束问题并获得解析解。研究表明该模型在耦合参数变化时呈现平滑过渡而非相变，为SOT模型提供了首个解析描述。


<details>
  <summary>Details</summary>
Motivation: 传统统计力学方法主要关注零温极限下的最优配置（基态），但真实世界系统往往处于熵与成本相互竞争的中温区域，产生结构化但非最优的配置。SOT模型虽能数值上捕捉这一竞争，但缺乏理论分析。因此，需要开发解析方法来理解该模型中熵主导的稠密构型与成本主导的稀疏结构之间的交叉行为。

Method: 作者采用平均场理论来分析SOT模型。通过研究热力学极限下拉格朗日乘子的局部涨落特性，发现这些涨落是次广延的（sub-extensive），从而可以将包含强度约束的完整模型简化为有效的单约束问题。这一简化使得在中间区域获得精确解析解成为可能。

Result: 1) 证明了自由能关于耦合参数是解析的，确认了平滑交叉而非相变；2) 推导出热力学观测值和权重分布的闭式表达式；3) 通过数值模拟验证了理论结果的正确性；4) 为SOT模型提供了首个完整的解析描述。

Conclusion: 该工作成功地将统计力学方法从传统的零温极限扩展到中温区域，为理解实际优化问题中熵与成本的竞争提供了新的理论框架。平均场方法揭示的次广延涨落特性是多约束系统可简化的关键，这一洞见可能适用于其他组合优化问题。研究结果不仅完善了SOT模型的理论基础，也为分析类似系统的有限温度行为提供了新思路。

Abstract: Statistical mechanics is a powerful framework for analyzing optimization yielding analytical results for matching, optimal transport, and other combinatorial problems. However, these methods typically target the zero-temperature limit, where systems collapse onto optimal configurations, a.k.a. the ground states. Real-world systems often occupy intermediate regimes where entropy and cost minimization genuinely compete, producing configurations that are structured yet sub-optimal. The Sub-Optimal Transport (SOT) model captures this competition through an ensemble of weighted bipartite graphs: a coupling parameter interpolates between entropy-dominated dense configurations and cost-dominated sparse structures. This crossover has been observed numerically but lacked analytical understanding. Here we develop a mean-field theory that characterizes this transition. We show that local fluctuations in Lagrange multipliers become sub-extensive in the thermodynamic limit, reducing the full model with strength constraints to an effective single-constraint problem admitting an exact solution in some intermediate regime. The resulting free energy is analytic in the coupling parameter, confirming a smooth crossover rather than a phase transition. We derive closed-form expressions for thermodynamic observables and weight distributions, validated against numerical simulations. These results establish the first analytical description of the SOT model, extending statistical mechanics methods beyond the zero-temperature regime.

</details>


### [156] [Critical behavior of isotropic systems with strong dipole-dipole interaction from the functional renormalization group](https://arxiv.org/abs/2602.04313)
*Georgii Kalagov,Nikita Lebedev*

Main category: cond-mat.stat-mech

TL;DR: 使用功能重整化群（FRG）结合局部势近似和波函数重整化（LPA'）方法，计算了三维强偶极相互作用磁体的临界指数，发现其由Aharony不动点支配（具有尺度不变性但无共形不变性），且临界指数与海森堡O(3)普适类接近但属于不同普适类。


<details>
  <summary>Details</summary>
Motivation: 探究强偶极相互作用对三维磁体临界行为的影响，明确其是否形成独立于传统海森堡模型的普适类，并精确计算其临界指数。

Method: 采用功能重整化群（FRG）框架，结合局部势近似并包含波函数重整化（LPA'）的非微扰方法进行分析。

Result: 识别出由Aharony不动点支配的系统，其临界指数与FRG/LPA'框架下计算的海森堡O(3)普适类数值接近。

Conclusion: 强偶极相互作用磁体形成独立普适类（Aharony不动点），其临界行为与海森堡O(3)模型相似但存在本质区别，证实二者为数值相近的不同普适类。

Abstract: We compute the critical exponents of three-dimensional magnets with strong dipole-dipole interactions using the functional renormalization group (FRG) within the local potential approximation including the wave function renormalization (LPA$^\prime$). The system is governed by the Aharony fixed point, which is scale-invariant but lacks conformal invariance. Our nonperturbative FRG analysis identifies this fixed point and determines its scaling behavior. The resulting critical exponents are found to be close to those of the Heisenberg $O(3)$ universality class, as computed within the same FRG/LPA$^\prime$ framework. This proximity confirms the distinct yet numerically similar nature of the two universality classes.

</details>


### [157] [Area under subdiffusive random walks](https://arxiv.org/abs/2602.04342)
*Vicenç Méndez,Rosa Flaquer-Galmés,Javier Cristín*

Main category: cond-mat.stat-mech

TL;DR: 本文研究亚扩散随机游走轨迹的面积与绝对面积的统计性质。


<details>
  <summary>Details</summary>
Motivation: 亚扩散在物理、生物系统中普遍存在，其轨迹几何特性的统计描述对理解异常扩散机制及实验数据分析具有重要意义。

Method: 采用标度布朗运动、分数布朗运动、连续时间随机游走和非均匀介质中的布朗运动等多种亚扩散模型框架，并通过蒙特卡洛模拟验证理论结果。

Result: 计算了面积与绝对面积的前两阶矩、绝对面积的非遍历性破缺参数，推断其概率密度函数的普适标度关系，比较了不同模型下面积与绝对面积统计性质的差异，并阐明了结果的实验应用价值。

Conclusion: 不同亚扩散模型下面积与绝对面积的统计性质存在显著差异，理论预测与蒙特卡洛模拟高度吻合，该研究对实验测量亚扩散过程具有指导意义。

Abstract: We study the statistical properties of the area and the absolute area under the trajectories of subdiffusive random walks. Using different frameworks to describe subdiffusion (as the scaled Brownian motion, fractional Brownian motion, the continuous-time random walk or the Brownian motion in heterogeneous media), we compute the first two moments, the ergodicity breaking parameter for the absolute area and infer a general scaling for the probability density functions of these functionals. We discuss the differences between the statistical properties of the area and the absolute area for the different subdiffusion models and illustrate the experimental interest of our results. Our theoretical findings are supported by Monte Carlo simulations showing an excellent agreement.

</details>


### [158] [Population dynamics simulations of large deviations for three subclasses of the Kardar-Parisi-Zhang universality class](https://arxiv.org/abs/2602.04357)
*Yuta Yanagibashi,Kazumasa A. Takeuchi*

Main category: cond-mat.stat-mech

TL;DR: 开发基于群体动力学算法的数值方法，研究KPZ普适类中TASEP模型的时间积分电流大偏差问题，揭示负大偏差区对不同初始条件的鲁棒性源于界面自发形成楔形结构


<details>
  <summary>Details</summary>
Motivation: 现有理论虽深化了对一维KPZ普适类大偏差 regime 的理解，但数值研究方法仍有限，尤其缺乏针对平坦和稳态初始条件下大偏差特性的数值表征

Method: 采用群体动力学算法模拟完全不对称简单排斥过程(TASEP)，针对阶跃/平坦/稳态三类初始条件，研究时间积分局部电流的大偏差行为

Result: 1) 验证阶跃初始条件的理论预测；2) 首次表征平坦/稳态条件下的大偏差特性；3) 发现负大偏差区存在意外鲁棒性，归因于界面自发形成楔形结构

Conclusion: 群体动力学方法为KPZ普适类大偏差研究提供通用数值工具，该鲁棒性机制可能适用于更广泛系统，甚至具备实验验证潜力

Abstract: Recent theoretical studies have gradually deepened our understanding of the one-dimensional (1D) Kardar-Parisi-Zhang (KPZ) universality class even in the large deviation regime, but numerical methods for studying KPZ large deviations remain limited. Here we implement a method based on the population dynamics algorithm for studying large deviations of time-integrated local currents in the totally asymmetric simple exclusion process (TASEP), which is a pragmatic model in the 1D KPZ class. Carrying out simulations for the three representative initial conditions, namely step, flat, and stationary ones, we not only confirm theoretical predictions available for the step case, but also characterize large deviations for the flat and stationary cases which have not been investigated before. We reveal in particular an unexpected robustness of the deeply negative large deviation regime with respect to different initial conditions. We attribute this robustness to the spontaneous formation of a wedge shape in interface profile. Our population dynamics approach may serve as a versatile method for studying large deviations in the KPZ class numerically and, potentially, even experimentally.

</details>


### [159] [Probabilities of rare events in product kernel aggregation: An exact formula and phase diagram](https://arxiv.org/abs/2602.04363)
*R. Goutham,R. Rajesh,V. Subashri,Oleg Zaboronski*

Main category: cond-mat.stat-mech

TL;DR: 本文提出了一种精确计算乘积核聚集过程中粒子数罕见涨落的大偏差函数的方法，通过主方程推导出概率的积分表示，利用副本猜想和勒让德-芬切尔变换构建了包含三临界点的完整相图。


<details>
  <summary>Details</summary>
Motivation: 乘积核聚集过程中的罕见涨落现象缺乏精确的理论描述，大偏差函数是刻画此类稀有事件概率的关键工具，但传统方法难以获得精确解。

Method: 从主方程出发，推导任意有限M、N、t下观测到N个粒子的概率P(M,N,t)的精确积分表示；计算整数p的指数矩<p^N>；通过数值验证的副本猜想将其推广到实数p≥0；最后通过勒让德-芬切尔变换获得大偏差函数的凸包络。

Result: 获得了大偏差函数的精确表达式，发现其奇异结构可构建完整的相图，该相图包含一个三临界点，可区分连续和不连续相变；同时计算了小N/M情况下大偏差函数的渐近形式。

Conclusion: 该方法成功揭示了乘积核聚集系统的相变特性，三临界点的存在为理解聚集过程中的突变行为提供了新的理论框架，结果对非平衡统计物理具有重要参考价值。

Abstract: We present an exact method for calculating the large deviation function describing rare fluctuations in the number of particles for product-kernel aggregation. Starting from the master equation, we derive an exact integral representation for the probability $P(M,N,t)$ of observing $N$ particles at time $t$ starting from $M$ monomers for any finite $M, N, t$. From this, we obtain an exact expression for the exponential moment $\langle p^N\rangle$ for integer $p$. Employing a replica conjecture -- numerically validated by finite-$M$ scaling -- we extend this result to real $p \geq 0$. The convex envelope of the large deviation function, obtained via a Legendre-Fenchel transform of the exponential moment, shows singular behavior. The singular structure allows us to construct the full phase diagram of product-kernel aggregation, which contains a tricritical point, separating continuous and discontinuous transitions. We also compute the asymptotic form of the LDF for small $N/M$.

</details>


### [160] [Control protocols for harmonically confined run-and-tumble particles](https://arxiv.org/abs/2602.04560)
*Marco Baldovin,Alessandro Manacorda*

Main category: cond-mat.stat-mech

TL;DR: 提出描述自驱动活性物质的新动力学方法，通过无限常微分方程组求解受控谐振子中跑动-翻滚粒子的最优控制协议，在慢速变换下实现最小功传输并提供调控活性系统的物理洞见。


<details>
  <summary>Details</summary>
Motivation: 理解非平衡态系统中活性物质的调控机制，探索在指定时间内将跑动-翻滚粒子从初态引导至终态的最优控制策略，并最小化平均功耗。

Method: 构建替代传统描述的无限阶常微分方程组动力学框架，采用适用于长时协议的闭包近似求解，结合慢速（非准静态）变换下的解析方法优化控制协议。

Result: 获得实现任务的最小功解析解；闭包近似在慢速协议下与数值模拟吻合；揭示活性系统最优控制策略（如 tumbling rate 与 trap strength 的协同调控）。

Conclusion: 该动力学框架有效描述非平衡控制过程，最优协议为活性物质系统设计提供理论依据，慢速变换下的解析解具有普适指导意义。

Abstract: Run-and-tumble particles constitute one of the simplest models of self-propelled active matter, and provide an ideal playground to the understanding of out-of-equilibrium systems. We consider an idealized setup where one such particle is subject to a harmonic confining potential, and an external agent can vary in time the tumbling rate and the strength of the trap. We search for time-dependent control protocols steering the system between assigned end states, in a prescribed time interval. To this aim, we propose a description of the dynamics, alternative to the usual ones, in the form of an infinite set of ordinary differential equations. Solutions based on a suitable closure of such hierarchy, which we expect to hold true in the limit of long protocol duration, are discussed and compared with numerical simulations. We also look for the protocol completing the task with the minimal work, on average: the problem can be tackled analytically, again in the regime of slow (but not quasi-static) transformations. The solution provides insightful intuition on the optimal strategies for the control of active matter systems.

</details>


### [161] [Emergent Hawking Radiation and Quantum Sensing in a Quenched Chiral Spin Chain](https://arxiv.org/abs/2602.04593)
*Nitesh Jaiswal,S. Shankaranarayanan*

Main category: cond-mat.stat-mech

TL;DR: 本文研究一维手性自旋链模型中的霍金辐射及其探测。通过量子猝灭模拟黑洞形成，将自旋链映射到弯曲时空中的狄拉克费米子，采用场论模式和量子传感器两种方法分析辐射谱。发现辐射谱偏离理想普朗克形式但保持泊松统计，且仅在弱耦合下量子比特才能准确探测霍金温度，为量子模拟器中区分真实模拟霍金辐射与环境噪声提供了操作协议。


<details>
  <summary>Details</summary>
Motivation: 研究一维手性自旋链模型中霍金辐射的产生机制和探测方法，重点关注其稳态辐射谱的可探测性，建立在先前证实该模型可模拟黑洞形成条件的工作基础上。

Method: 将自旋链动力学映射到(1+1)维弯曲时空中的狄拉克费米子，采用两种互补方法：1) 用局域高斯波包模拟实际探测器进行场论模式分析；2) 将量子比特作为静态Unruh-DeWitt探测器进行操作性量子传感。

Result: 1) 辐射谱表现出类似灰体因子的非理想普朗克形式，但保持稳健的泊松统计特征，表明形成尺度信息的丢失；2) 量子比特仅在弱耦合 regime 能忠实探测霍金温度，其动力学由浴谱密度决定；3) 强耦合下探针与全局环境热化，掩盖了视界诱导的热信号。

Conclusion: 该研究为在量子模拟平台上区分真实的模拟霍金辐射与外部环境噪声提供了清晰的操作协议。

Abstract: We investigate the emergence and detection of Hawking radiation (HR) in a 1D chiral spin chain model, where the gravitational collapse is simulated by a sudden quantum quench that triggers a horizon-inducing phase transition. While our previous work Jaiswal [2025] established that this model mimics BH formation conditions even when the Hoop conjecture is seemingly violated, we here focus on the resulting stationary radiation spectrum and its detectability. By mapping the spin chain dynamics to a Dirac fermion in a curved (1 + 1)-dimensional spacetime, we analyze the radiation using two complementary approaches: field-theoretic modes and operational quantum sensors. First, using localized Gaussian wave packets to model realistic detectors, we find that the radiation spectrum exhibits deviations from the ideal Planckian form, analogous to frequency-dependent greybody factors, while retaining robust Poissonian statistics that signal the loss of formation-scale information. Second, we introduce a qubit coupled to the chain as a stationary Unruh-DeWitt detector. We demonstrate that the qubit functions as a faithful quantum sensor of the Hawking temperature only in the weak-coupling regime, where its population dynamics are governed solely by the bath spectral density. In the strong-coupling limit, the probe thermalizes with the global environment, obscuring the horizon-induced thermal signature. These results provide a clear operational protocol for distinguishing genuine analog HR from environmental noise in quantum simulation platforms.

</details>


### [162] [The Most Dispersed Subset of Random Points in $\mathbb{R}^d$](https://arxiv.org/abs/2602.04626)
*Fabio Deelan Cunden,Noemi Cuppone,Giovanni Gramegna,Pierpaolo Vivo*

Main category: cond-mat.stat-mech

TL;DR: This paper analytically computes the full statistics of maximal dispersion when selecting M individuals from N with d i.i.d. traits. Using mean-field theory and replica methods, it shows that for large N, the optimal subset consists of points outside a d-dimensional ball whose radius is determined self-consistently, with results validated by simulations.


<details>
  <summary>Details</summary>
Motivation: Selecting diverse subsets is crucial in applications like portfolio optimization, biodiversity conservation, and experimental design. Understanding the fundamental limits and statistics of maximal dispersion, especially its large deviation tails, provides theoretical insights and practical benchmarks for algorithm design.

Method: The authors develop two complementary analytical approaches: (1) a mean-field theory for order statistics, and (2) the replica method from disordered systems. These are used to compute the full statistics of maximal dispersion for i.i.d. traits. Numerical simulations and heuristic algorithms are employed to validate the theoretical predictions.

Result: Key results include: (i) For large N, any rotationally symmetric distribution, and any d, the optimal M-subset comprises all points outside a d-dimensional ball with a self-consistently determined radius; (ii) For d=1, the full statistics can be computed exactly even for finite N,M; (iii) The derived formulae agree well with numerical simulations and heuristic solutions.

Conclusion: The study provides a complete analytical characterization of the maximum dispersion problem, revealing a universal geometric structure (ball-exterior selection) for optimal subsets. The combination of mean-field and replica methods offers a powerful framework for analyzing this class of combinatorial optimization problems, with results that are both theoretically rigorous and practically relevant.

Abstract: Consider a population of $N$ individuals, each having $d\geq 1$ different traits, and an additive measure, called dispersion, which rewards large pairwise separations between traits. The goal is to select $M\leq N$ individuals such that their traits are as dispersed as possible. We compute analytically the full statistics (including large deviation tails) of the maximally achievable dispersion among sub-populations of size $M$ when the traits are independent and identically distributed. Two complementary approaches are developed, one based on a mean-field theory for order statistics, and the other on the replica method from the field of disordered systems. In all dimensions $d$, and for rotationally symmetric distributions, the optimal subset for large populations consists of all points lying outside a $d$-dimensional ball whose radius is determined self-consistently. For a single trait ($d=1$), the statistics of the maximal dispersion can be tackled for finite $N,M$ as well. The formulae we obtained are corroborated by numerical simulations on small instances and by heuristic algorithms that find near-optimal solutions.

</details>


### [163] [Site and bond percolation in linearly distorted triangular and square lattices](https://arxiv.org/abs/2602.04818)
*Bishnu Bhowmik,Sayantan Mitra,Robert M. Ziff,Ankur Sensharma*

Main category: cond-mat.stat-mech

TL;DR: 研究线性畸变对三角和正方形晶格中位置与键渗流的影响，发现三角晶格渗流阈值呈现显著方向依赖性，而正方形晶格保持各向同性，揭示了晶格几何结构对畸变响应的关键作用。


<details>
  <summary>Details</summary>
Motivation: 探索非几何保持型线性畸变（沿固定方向位移晶格点）对渗流相变的影响，突破此前仅研究保持晶格几何的畸变方案，特别关注晶格对称性（三角vs正方形）与畸变各向异性如何共同调控渗流阈值。

Method: 采用大规模蒙特卡洛模拟结合有限尺寸标度分析，系统改变畸变参数和邻居连接距离阈值，研究位置/键渗流临界行为。

Result: 1) 三角晶格：位置/键渗流阈值及临界连接阈值均呈现强方向依赖性，源于畸变导致近邻间距各向异性改变；键渗流存在无法用平均配位数变化解释的非平凡行为。2) 正方形晶格：线性畸变下仍保持有效各向同性，不同方向畸变得到相同渗流阈值。3) 热力学极限渗流阈值验证了大尺寸有限晶格结果的可靠性。

Conclusion: 晶格几何结构是决定畸变渗流行为的核心因素：三角晶格的高对称性使其对畸变方向敏感，而正方形晶格的低对称性反而导致各向同性响应，推翻仅依赖配位数变化的简单解释框架。

Abstract: We investigate site and bond percolation in triangular and square lattices subjected to linear distortion. In contrast to previously studied distortion schemes that preserve lattice geometry, linear distortion dislocates regular lattice sites along a fixed direction. Nearest-neighbors of a regular lattice need to satisfy a distance-based connection criterion to remain neighbors in the linearly distorted lattice. Using extensive Monte Carlo simulations and finite-size scaling analyses, we examine how site and bond percolation thresholds vary with the distortion parameter and the connection threshold. For triangular lattices, we observe pronounced directional dependence of both site and bond percolation thresholds, as well as of the critical connection threshold. This arises from the distortion-induced anisotropic modification of nearest-neighbor separations. In particular, bond percolation exhibits nontrivial behavior that cannot be explained solely in terms of changes in the average coordination number. In contrast, square lattices remain effectively isotropic under linear distortion, resulting in identical percolation thresholds for distortions applied along different directions. Percolation thresholds in the thermodynamic limit, evaluated for a selected set of values of distortion parameter and connection threshold, confirm that the results for large finite lattices provide reliable estimates of the infinite-system behavior.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [164] [Primary charge-4e superconductivity from doping a featureless Mott insulator](https://arxiv.org/abs/2602.03925)
*Zhi-Qiang Gao,Yan-Qi Wang,Ya-Hui Zhang,Hui Yang*

Main category: cond-mat.str-el

TL;DR: A doped SU(4)-symmetric Mott insulator enables primary charge-4e superconductivity at zero temperature, distinct from conventional charge-2e pairing.


<details>
  <summary>Details</summary>
Motivation: Primary charge-4e superconductivity at zero temperature is rare in experiments and models; conventional understanding treats it as a vestigial high-temperature phase of charge-2e states. This work seeks a fundamental platform for intrinsic charge-4e pairing.

Method: Used perturbative renormalization group and group theory on a doped featureless Mott insulator with SU(4) symmetry; constructed a tunable bilayer Hubbard model exhibiting a featureless Mott insulating phase; analyzed low-energy physics via a generalized ESD model with purely kinetic effective Hamiltonian within constrained Hilbert space; employed DMRG simulations.

Result: DMRG simulations revealed a primary charge-4e superconducting phase in the SU(4) ESD model, contrasting with a conventional primary charge-2e phase in the Sp(4) case; characterized normal states and derived the finite-temperature phase diagram.

Conclusion: SU(4)-symmetric doped Mott insulators provide a natural platform for primary charge-4e superconductivity, demonstrating a rare zero-temperature phase beyond vestigial charge-2e mechanisms.

Abstract: Superconductivity is usually understood as a phase in which charge-$2e$ Cooper pairs are condensed. Charge-$4e$ superconductivity has largely been discussed as a vestigial order at finite temperature emerging from charge-$2e$ states. Primary charge-$4e$ superconducting phases at zero temperature remain scarce in both experiments and microscopic models. Here we argue that a doped featureless Mott insulator with $SU(4)$ symmetry provides a natural platform for primary charge-$4e$ superconductivity, based on perturbative renormalization group arguments and group theoretic considerations. As a concrete realization, we construct a bilayer Hubbard model with tunable onsite $SU(4)$ and $Sp(4)$ symmetries that exhibits a featureless Mott insulating phase at half filling. Its low energy physics is captured by a generalized ESD model, featuring an effective Hamiltonian that is purely kinetic within the constrained Hilbert space. Using density matrix renormalization group (DMRG) simulations, we find a primary charge-$4e$ superconducting phase in the $SU(4)$ ESD model and a conventional primary charge-$2e$ phase in the $Sp(4)$ case. We further characterize the corresponding normal states and discuss the resulting finite temperature phase diagram.

</details>


### [165] [Revealing the microscopic origin of the magnetization plateau in Na$_3$Ni$_2$BiO$_6$](https://arxiv.org/abs/2602.03936)
*Amanda A. Konieczna,P. Peter Stavropoulos,Roser Valentí*

Main category: cond-mat.str-el

TL;DR: 该研究通过第一性原理计算揭示Na₃Ni₂BiO₆材料中1/3磁化平台源于单离子各向异性与J₁-J₃交换竞争，无需依赖Kitaev相互作用


<details>
  <summary>Details</summary>
Motivation: 实验观测到Na₃Ni₂BiO₆存在显著1/3磁化平台，传统理论归因于Kitaev相互作用，但微观机制存在争议，需建立定量模型澄清起源

Method: 结合密度泛函理论计算与微观自旋模型构建，提取交换参数并拟合中子散射谱与磁化曲线

Result: 提出双zigzag磁结构解释中间场1/3平台，证明强面外单离子各向异性与铁磁J₁/反铁磁J₃竞争即可实现该平台，无需Kitaev作用

Conclusion: 建立了Na₃Ni₂BiO₆的微观自洽描述，修正了Kitaev相互作用必要性的传统认知，为阻挫磁体研究提供更简化的理论框架

Abstract: Recent experimental studies of the spin-1 honeycomb antiferromagnet Na$_3$Ni$_2$BiO$_6$ have revealed a pronounced one-third magnetization plateau under applied magnetic fields, highlighting the presence of strong magnetic frustration and anisotropy in this material. Such behavior has been attributed to substantial bond-dependent Kitaev interactions in combination with single-ion anisotropy, placing Na$_3$Ni$_2$BiO$_6$ among honeycomb compounds of interest for unconventional magnetic phases. Motivated by these observations, we present a first-principles-based analysis of the magnetic interactions in Na$_3$Ni$_2$BiO$_6$. By combining density-functional calculations with microscopic modeling, we extract the relevant exchange parameters and construct an effective spin model that quantitatively reproduces both the elastic neutron-scattering spectra and the magnetization curve. The model captures the experimentally observed zero-field zigzag magnetic order, and proposes a $\textit{double-zigzag}$ state at intermediate magnetic fields, realizing the 1/3-magnetization plateau in a simpler way than suggested in previous works. Crucially, we show that the one-third magnetization plateau does not require Kitaev interactions; instead, it arises from the interplay of strong out-of-plane single-ion anisotropy and competing ferromagnetic nearest-neighbor ($J_1$) and antiferromagnetic third-neighbor ($J_3$) Heisenberg couplings. These results establish a consistent microscopic description of Na$_3$Ni$_2$BiO$_6$ and clarify the origin of its field-induced plateau phase.

</details>


### [166] [Boundary and Symmetry Breaking in a Deformed Toric Code](https://arxiv.org/abs/2602.04002)
*Rodrigo Corso*

Main category: cond-mat.str-el

TL;DR: This paper studies a deformed Kitaev toric code on a cylinder, revealing that its phase transition out of topological order involves breaking of a 1-form symmetry, with holographic analysis showing central charge suppression at criticality indicating bulk criticality-driven transition.


<details>
  <summary>Details</summary>
Motivation: To understand how topological order is lost during phase transitions by deforming the Kitaev toric code model and probing the underlying mechanisms, particularly the role of higher-form symmetries and criticality.

Method: Deforming the Kitaev toric code on a cylindrical geometry to separate bulk 1-form symmetries into boundary operators, combined with holographic (1+1)-dimensional boundary Hamiltonian analysis to extract effective central charge.

Result: The phase transition breaks a higher-form symmetry, and the holographic boundary analysis reveals a pronounced suppression of the effective central charge near the critical coupling β_c, followed by restoration at strong coupling, indicating sensitivity to bulk criticality rather than topological order.

Conclusion: The phase transition out of topological order is driven by bulk criticality, evidenced by 1-form symmetry breaking and central charge behavior, challenging the notion that topological order loss alone dictates the transition.

Abstract: This work explores a deformation of the Kitaev toric code that induces a phase transition out of the topologically ordered phase. By placing the model on a cylinder, the bulk global 1-form symmetries separate into distinct boundary operators, allowing us to show that the transition is accompanied by the breaking of one higher-form symmetry. Using a holographic $(1+1)$-dimensional boundary Hamiltonian, we extract an effective central charge and find a pronounced suppression near $β_c$, followed by its restoration at strong coupling, indicating sensitivity to bulk criticality rather than topological order.

</details>


### [167] [Emergent Coherence at the Edge of Magnetism: Low-Doped La2-xSrxCuO4+delta Revisited](https://arxiv.org/abs/2602.04452)
*E. Yu. Beliayev,Y. K. Mishra,I. A. Chichibaba,I. G. Mirzoiev,V. A. Horielyi,A. V. Terekhov*

Main category: cond-mat.str-el

TL;DR: 研究LSCO系统发现从绝缘态到超导态再到金属态的连续演化，由电子屏蔽、区域耦合和超导连通性驱动，揭示了渗流和非平衡效应在欠掺杂铜酸盐中的核心作用。


<details>
  <summary>Details</summary>
Motivation: 探索磁性、超导性和无序如何共同调控掺杂莫特绝缘体的电荷输运行为。

Method: 在轻掺杂和富氧LSCO中进行变载流子浓度的输运测量。

Result: 发现从局域化绝缘态（变程跃迁、非线性I-V、负微分电阻）到颗粒/渗流超导态，再到奇异金属态的连续交叉。演化由电子屏蔽增强、区域耦合和超导连通性驱动。

Conclusion: 提供了无序和介观不均匀性组织电荷输运和超导性的实验基础，强调了渗流和非平衡效应在欠掺杂铜酸盐中的重要性。

Abstract: The La2-xSrxCuO4+delta (LSCO) system provides a unique experimental setting for exploring how magnetism, superconductivity, and disorder jointly shape charge transport in a doped Mott insulator. Transport measurements in lightly doped and oxygen-enriched LSCO reveal a strongly insulating normal state governed by variable-range hopping, accompanied by pronounced nonlinear current-voltage characteristics and, at low temperatures, current-induced negative differential resistance. With increasing carrier concentration, these features evolve into regimes characterized by granular and percolative superconductivity near the threshold of bulk superconductivity and, eventually, into a homogeneous strange-metal state close to optimal doping. Throughout this evolution, the transport response shows marked sensitivity to disorder, electronic inhomogeneity, and external control parameters, such as bias current and magnetic field. Rather than reflecting a sequence of sharply distinct phases, the observed transport regimes form a continuous crossover from a localization-dominated insulating state to granular superconductivity and further to a coherent metallic state. This crossover is driven primarily by the progressive enhancement of electronic screening, inter-region coupling, and superconducting connectivity, rather than by abrupt changes in the underlying microscopic scattering mechanisms. Taken together, the available transport data provide a coherent experimental basis for understanding how disorder and mesoscale electronic inhomogeneity organize charge transport and superconductivity across the LSCO phase diagram, underscoring the central role of percolation and nonequilibrium effects in underdoped cuprates.

</details>
