<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 6]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [On the Origin of Algorithmic Progress in AI](https://arxiv.org/abs/2511.21622)
*Hans Gundlach,Alex Fogelson,Jayson Lynch,Ana Trisovic,Jonathan Rosenfeld,Anmol Sandhu,Neil Thompson*

Main category: cs.LG

TL;DR: 本文通过实验发现，2012-2023年间AI训练效率提升的22,000倍中，传统算法创新仅能解释不到100倍，大部分效率提升来自于具有规模依赖性的算法（如LSTM到Transformer的转变），这改变了算法效率评估的参考依赖性认知。


<details>
  <summary>Details</summary>
Motivation: 研究2012-2023年间AI训练效率提升22,000倍的真实原因，挑战传统认为算法创新是主要驱动因素的观点。

Method: 进行小规模消融实验分析关键创新，开展LSTM与Transformer的扩展实验，比较它们在计算最优扩展定律中的指数差异。

Result: 传统算法创新仅能解释不到100倍效率提升，而规模依赖的LSTM到Transformer转变解释了6,930倍效率提升，表明算法效率与计算规模密切相关。

Conclusion: 小规模模型的算法进展远慢于预期，算法效率的衡量具有强烈的参考依赖性，规模依赖的算法转变是效率提升的主要驱动力。

Abstract: Algorithms have been estimated to increase AI training FLOP efficiency by a factor of 22,000 between 2012 and 2023 [Ho et al., 2024]. Running small-scale ablation experiments on key innovations from this time period, we are able to account for less than 10x of these gains. Surveying the broader literature, we estimate that additional innovations not included in our ablations account for less than 10x, yielding a total under 100x. This leads us to conduct scaling experiments, which reveal that much of this efficiency gap can be explained by algorithms with scale-dependent efficiency improvements. In particular, we conduct scaling experiments between LSTMs and Transformers, finding exponent differences in their compute-optimal scaling law while finding little scaling difference for many other innovations. These experiments demonstrate that - contrary to standard assumptions - an algorithm's efficiency gains are tied to compute scale. Using experimental extrapolation and literature estimates, we account for 6,930x efficiency gains over the same time period, with the scale-dependent LSTM-to-Transformer transition accounting for the majority of gains. Our results indicate that algorithmic progress for small models has been far slower than previously assumed, and that measures of algorithmic efficiency are strongly reference-dependent.

</details>


### [2] [Mechanisms of Non-Monotonic Scaling in Vision Transformers](https://arxiv.org/abs/2511.21635)
*Anantha Padmanaban Krishna Kumar*

Main category: cs.LG

TL;DR: 研究发现深层Vision Transformers性能不如浅层，识别出Cliff-Plateau-Climb三阶段模式，[CLS]令牌重要性逐渐降低，信息扩散增加但不提升任务性能。


<details>
  <summary>Details</summary>
Motivation: 探索为什么深层Vision Transformers性能反而下降，挑战传统的缩放假设。

Method: 通过系统实证分析ViT-S、ViT-B和ViT-L在ImageNet上的表现，使用信息混杂指数量化信息混合模式。

Result: 发现[CLS]令牌被边缘化，信息扩散增加但任务性能未提升，ViT-L的信息-任务权衡比ViT-B晚10层出现。

Conclusion: Transformer架构应更注重精心校准的深度而非简单增加参数，信息混杂指数可作为有用诊断工具。

Abstract: Deeper Vision Transformers often perform worse than shallower ones, which challenges common scaling assumptions. Through a systematic empirical analysis of ViT-S, ViT-B, and ViT-L on ImageNet, we identify a consistent three-phase Cliff-Plateau-Climb pattern that governs how representations evolve with depth. We observe that better performance is associated with progressive marginalization of the [CLS] token, originally designed as a global aggregation hub, in favor of distributed consensus among patch tokens. We quantify patterns of information mixing with an Information Scrambling Index, and show that in ViT-L the information-task tradeoff emerges roughly 10 layers later than in ViT-B, and that these additional layers correlate with increased information diffusion rather than improved task performance. Taken together, these results suggest that transformer architectures in this regime may benefit more from carefully calibrated depth that executes clean phase transitions than from simply increasing parameter count. The Information Scrambling Index provides a useful diagnostic for existing models and suggests a potential design target for future architectures. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/Cliff-Plateau-Climb.

</details>


### [3] [Escaping the Verifier: Learning to Reason via Demonstrations](https://arxiv.org/abs/2511.21667)
*Locke Cai,Ivan Provilkov*

Main category: cs.LG

TL;DR: RARO方法通过逆向强化学习从专家演示中学习推理能力，使用策略和相对批评者的对抗交互，无需任务特定验证器即可实现强大的推理性能。


<details>
  <summary>Details</summary>
Motivation: 许多现实世界的推理密集型任务缺乏验证器，但拥有丰富的专家演示，这些演示在推理训练中未被充分利用。

Method: 建立策略（生成器）和相对批评者（判别器）之间的对抗交互：策略学习模仿专家答案，批评者学习比较和区分策略与专家答案，通过强化学习联合持续训练。

Result: RARO在Countdown、DeepMath和Poetry Writing等评估任务上显著优于无验证器基线，并展现出与可验证任务上RL相同的稳健扩展趋势。

Conclusion: 该方法仅从专家演示中就能有效激发强大的推理性能，即使在任务特定验证器不可用时也能实现稳健的推理学习。

Abstract: Training Large Language Models (LLMs) to reason often relies on Reinforcement Learning (RL) with task-specific verifiers. However, many real-world reasoning-intensive tasks lack verifiers, despite offering abundant expert demonstrations that remain under-utilized for reasoning-focused training. We introduce RARO (Relativistic Adversarial Reasoning Optimization) that learns strong reasoning capabilities from only expert demonstrations via Inverse Reinforcement Learning. Our method sets up an adversarial interaction between a policy (generator) and a relativistic critic (discriminator): the policy learns to mimic expert answers, while the critic learns to compare and distinguish between policy and expert answers. Our method trains both the policy and the critic jointly and continuously via RL, and we identify the key stabilization techniques required for robust learning. Empirically, RARO significantly outperforms strong verifier-free baselines on all of our evaluation tasks -- Countdown, DeepMath, and Poetry Writing -- and enjoys the same robust scaling trends as RL on verifiable tasks. These results demonstrate that our method effectively elicits strong reasoning performance from expert demonstrations alone, enabling robust reasoning learning even when task-specific verifiers are unavailable.

</details>


### [4] [Through the telecom lens: Are all training samples important?](https://arxiv.org/abs/2511.21668)
*Shruti Bothe,Illyyne Saffar,Aurelie Boisbunon,Hasan Farooq,Julien Forgeat,Md Moin Uddin Chowdhury*

Main category: cs.LG

TL;DR: 该论文质疑电信AI训练中所有样本同等重要的假设，通过样本级梯度分析识别影响力和冗余模式，提出选择性优先处理重要数据的框架，在保持性能的同时减少计算需求和能耗。


<details>
  <summary>Details</summary>
Motivation: 电信AI应用中数据量大、噪声多、标注成本高，但标准工作流仍假设所有训练样本同等重要。下一代系统需要准确、高效且可持续的AI模型，因此需要优化计算和能源使用。

Method: 进行跨epoch的样本级梯度分析，识别模型学习中的影响力和冗余模式，提出选择性优先处理重要数据的样本重要性框架。

Result: 在三个真实世界电信数据集上的实验表明，该方法在保持性能的同时减少了数据需求和计算开销。

Conclusion: 该方法通过选择性数据优先级处理，在不影响准确性的情况下优化计算和能源使用，推进了电信领域可持续AI的目标。

Abstract: The rise of AI in telecommunications, from optimizing Radio Access Networks to managing user experience, has sharply increased data volumes and training demands. Telecom data is often noisy, high-dimensional, costly to store, process, and label. Despite Ai's critical role, standard workflows still assume all training samples contribute equally. On the other hand, next generation systems require AI models that are accurate, efficient, and sustainable.The paper questions the assumptions of equal importance by focusing on applying and analyzing the roles of individual samples in telecom training and assessing whether the proposed model optimizes computation and energy use. we perform sample-level gradient analysis across epochs to identify patterns of influence and redundancy in model learning. Based on this, we propose a sample importance framework thats electively prioritizes impactful data and reduces computation without compromising accuracy. Experiments on three real-world telecom datasets show that our method [reserves performance while reducing data needs and computational overhead while advancing the goals of sustainable AI in telecommunications.

</details>


### [5] [Visualizing LLM Latent Space Geometry Through Dimensionality Reduction](https://arxiv.org/abs/2511.21594)
*Alex Ning,Vainateya Rangaraju*

Main category: cs.LG

TL;DR: 本文通过降维技术提取、处理和可视化基于Transformer的语言模型的潜在状态几何结构，发现注意力机制和MLP组件输出在中间层存在明显分离，并揭示了GPT-2位置嵌入的高维螺旋结构等几何模式。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言任务中表现出色，但其内部机制难以解释。本文旨在通过可视化Transformer模型的潜在状态几何结构，支持系统性的可解释性研究。

Method: 使用主成分分析(PCA)和均匀流形逼近(UMAP)等降维技术，在Transformer块内的多个点捕获层间激活，对GPT-2和LLaMa模型进行系统分析。

Result: 发现了注意力机制和MLP组件输出在中间层的明显分离模式，识别了初始序列位置潜在状态的高范数特征，可视化了GPT-2位置嵌入的高维螺旋结构和LLaMa的序列级几何模式。

Conclusion: 通过几何可视化方法能够揭示Transformer模型内部的有趣模式，为可复现的可解释性研究提供了支持工具，相关代码已开源。

Abstract: Large language models (LLMs) achieve state-of-the-art results across many natural language tasks, but their internal mechanisms remain difficult to interpret. In this work, we extract, process, and visualize latent state geometries in Transformer-based language models through dimensionality reduction. We capture layerwise activations at multiple points within Transformer blocks and enable systematic analysis through Principal Component Analysis (PCA) and Uniform Manifold Approximation (UMAP). We demonstrate experiments on GPT-2 and LLaMa models, where we uncover interesting geometric patterns in latent space. Notably, we identify a clear separation between attention and MLP component outputs across intermediate layers, a pattern not documented in prior work to our knowledge. We also characterize the high norm of latent states at the initial sequence position and visualize the layerwise evolution of latent states. Additionally, we demonstrate the high-dimensional helical structure of GPT-2's positional embeddings, the sequence-wise geometric patterns in LLaMa, and experiment with repeating token sequences. We aim to support systematic analysis of Transformer internals with the goal of enabling further reproducible interpretability research. We make our code available at https://github.com/Vainateya/Feature_Geometry_Visualization.

</details>


### [6] [Aligning LLMs Toward Multi-Turn Conversational Outcomes Using Iterative PPO](https://arxiv.org/abs/2511.21638)
*Daniel R. Jiang,Jalaj Bhandari,Yukai Yang,Rémi Munos,Tyler Lu*

Main category: cs.LG

TL;DR: 本文提出了一种将多轮对话强化学习问题转化为单轮RLHF问题的方法，通过迭代PPO算法交替拟合Q函数和改进策略，实现稳定且易于实施的优化方案。


<details>
  <summary>Details</summary>
Motivation: 优化大型语言模型在多轮对话中的表现面临挑战，特别是在目标导向场景中，存在奖励稀疏、规划与生成不一致等问题。

Method: 将多轮RL问题形式化地转化为一系列单轮RLHF问题，使用学习到的多轮Q函数作为单轮问题的奖励模型，并采用迭代PPO算法进行策略改进。

Result: 证明了用标准token级PPO解决单轮RL问题等价于在多轮问题中进行策略改进步骤，开发出可直接利用现有单轮RLHF工具的实用算法。

Conclusion: 该方法在完全在线和完全离线方法之间找到了平衡点，既保持了在线更新的适应性，又获得了离线训练的稳定性优势。

Abstract: Optimizing large language models (LLMs) for multi-turn conversational outcomes remains a significant challenge, especially in goal-oriented settings like AI marketing or sales agents who facilitate transactions via messaging platforms. The difficulty stems from sparse, long-horizon rewards and the discrepancy between response-level planning and token-level generation. In this technical note, we propose a formal reduction of the multi-turn RL problem into a sequence of single-turn RLHF-style problems. This is achieved by setting a learned multi-turn Q-function as the reward model for the single-turn problem. We demonstrate and prove a key insight: solving this single-turn RL problem with standard token-level PPO is equivalent to a policy improvement step within the multi-turn problem. This insight naturally leads to Iterative PPO, a batch online policy iteration algorithm that alternates between fitting Q-functions from logged conversation trajectories and improving the policy. A major practical advantage is that Iterative PPO directly leverages stable, off-the-shelf single-turn RLHF tools, making it straightforward to implement. Our method occupies a middle ground between fully online and fully offline approaches, retaining the adaptability of online updates while gaining the stability benefits of offline training.

</details>
