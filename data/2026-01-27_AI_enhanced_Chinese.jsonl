{"id": "2601.17070", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.17070", "abs": "https://arxiv.org/abs/2601.17070", "authors": ["Andrei Khrennikov"], "title": "The Double Covariance Model: A Stochastic Reconstruction of Quantum Entangled States via Interplay of Micro-Macro Time Scales", "comment": null, "summary": "This article presents a concrete mathematical framework for the generation of entangled quantum states from classical stochastic processes. We demonstrate that any density operator $\u03c1_{AB}$ of a composite system can be derived from the correlations between two underlying stochastic processes, $X(t)$ and $Y(t)$, representing the random fluctuations of its subsystems. This construction utilizes a two-scale temporal scheme - micro and macro time - where quantum correlations emerge as macro-correlations derived from underlying micro-correlations. We propose the Double Covariance Model (DCM), which reproduces the fundamental properties of quantum theory by treating the quantum state as the fourth-order moment structure of an underlying classical probability space.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u6570\u5b66\u6846\u67b6\uff0c\u8bc1\u660e\u4efb\u4f55\u590d\u5408\u91cf\u5b50\u7cfb\u7edf\u7684\u5bc6\u5ea6\u7b97\u5b50\u5747\u53ef\u901a\u8fc7\u4e24\u4e2a\u7ecf\u5178\u968f\u673a\u8fc7\u7a0b\u7684\u76f8\u5173\u6027\u5bfc\u51fa\uff0c\u7528\u91cf\u5b50\u6001\u4f5c\u4e3a\u7ecf\u5178\u6982\u7387\u7a7a\u95f4\u7684\u56db\u9636\u77e9\u7ed3\u6784\u6765\u91cd\u6784\u91cf\u5b50\u7406\u8bba\u3002", "motivation": "\u63a2\u7d22\u91cf\u5b50\u7ea0\u7f20\u7684\u7ecf\u5178\u8d77\u6e90\uff0c\u5efa\u7acb\u91cf\u5b50\u7406\u8bba\u4e0e\u7ecf\u5178\u968f\u673a\u8fc7\u7a0b\u4e4b\u95f4\u7684\u6865\u6881\uff0c\u6311\u6218\u91cf\u5b50\u5173\u8054\u5fc5\u987b\u975e\u7ecf\u5178\u7684\u4f20\u7edf\u8ba4\u77e5\u3002", "method": "\u91c7\u7528\u53cc\u65f6\u95f4\u5c3a\u5ea6\uff08\u5fae\u89c2/\u5b8f\u89c2\uff09\u65b9\u6848\uff1a\u5fae\u89c2\u65f6\u95f4\u63cf\u8ff0\u5b50\u7cfb\u7edf\u968f\u673a\u6ce2\u52a8X(t)/Y(t)\uff0c\u5b8f\u89c2\u65f6\u95f4\u6d8c\u73b0\u91cf\u5b50\u5173\u8054\uff1b\u63d0\u51fa\u53cc\u534f\u65b9\u5dee\u6a21\u578b(DCM)\uff0c\u5c06\u91cf\u5b50\u6001\u5b9a\u4e49\u4e3a\u5e95\u5c42\u7ecf\u5178\u6982\u7387\u7a7a\u95f4\u7684\u56db\u9636\u77e9\u7ed3\u6784\u3002", "result": "\u8bc1\u660e\u4efb\u610f\u590d\u5408\u7cfb\u7edf\u5bc6\u5ea6\u7b97\u5b50\u03c1_AB\u5747\u53ef\u7531\u4e24\u4e2a\u7ecf\u5178\u968f\u673a\u8fc7\u7a0b\u7684\u76f8\u5173\u6027\u7cbe\u786e\u91cd\u6784\uff0c\u4e14DCM\u6210\u529f\u590d\u73b0\u91cf\u5b50\u7406\u8bba\u6838\u5fc3\u7279\u6027\uff08\u5982\u7ea0\u7f20\uff09\u3002", "conclusion": "\u91cf\u5b50\u7ea0\u7f20\u53ef\u80fd\u6e90\u4e8e\u7ecf\u5178\u968f\u673a\u8fc7\u7a0b\u7684\u9ad8\u9636\u7edf\u8ba1\u7279\u6027\uff0c\u4e3a\u91cf\u5b50\u57fa\u7840\u63d0\u4f9b\u65b0\u89c6\u89d2\uff1a\u91cf\u5b50\u7406\u8bba\u6216\u53ef\u89c6\u4e3a\u7ecf\u5178\u6982\u7387\u8bba\u7684\u6269\u5c55\u800c\u975e\u98a0\u8986\u3002"}}
{"id": "2601.17208", "categories": ["quant-ph", "physics.ed-ph"], "pdf": "https://arxiv.org/pdf/2601.17208", "abs": "https://arxiv.org/abs/2601.17208", "authors": ["Alejandro R. Urz\u00faa"], "title": "A pedagogical derivation of the first-order effective Hamiltonian for the two-mode Jaynes-Cummings model", "comment": "10 pages, 2 figures", "summary": "This work presents a pedagogical and self-contained derivation of the first-order effective Hamiltonian for the two-mode Jaynes-Cummings model in the dispersive regime. A perturbative unitary transformation removes nonresonant atom-field terms, revealing dispersive frequency shifts leading to an atom-induced effective beam-splitter interaction between the field modes. The resulting Hamiltonian is diagonalized through a simple geometric rotation in the two-mode bosonic space, providing a transparent interpretation of the underlying dynamics. The exposition emphasized clarity and physical insight, making effective Hamiltonian methods accessible for teaching and learning in multimode light-matter interactions.", "AI": {"tldr": "This paper provides a clear, teaching-focused derivation of the effective Hamiltonian for a two-mode quantum system, revealing how atom-field interactions create a beam-splitter effect between light modes.", "motivation": "To make advanced effective Hamiltonian methods in multimode light-matter interactions accessible for teaching and learning by emphasizing physical insight over complex mathematics.", "method": "Uses a perturbative unitary transformation to eliminate nonresonant terms in the two-mode Jaynes-Cummings model, followed by diagonalization via geometric rotation in bosonic mode space.", "result": "Derives an atom-induced effective beam-splitter interaction between field modes, with dispersive frequency shifts that are interpretable through a simple geometric rotation.", "conclusion": "Successfully demonstrates a transparent pedagogical approach to effective Hamiltonians, enhancing understanding of dispersive regime dynamics for educational purposes in quantum optics."}}
{"id": "2601.17384", "categories": ["quant-ph", "gr-qc"], "pdf": "https://arxiv.org/pdf/2601.17384", "abs": "https://arxiv.org/abs/2601.17384", "authors": ["John Gough", "Dylon Rees"], "title": "The Universe as a Detector: A Quantum Filtering Formulation of the Di\u00f3si-Penrose Model", "comment": null, "summary": "We consider the Di\u00f3si-Penrose problem but rather than postulating background gravitational fluctuations, we instead consider the quantum filter that arises from space-time homodyning the continuum of output quadrature described in the open quantum stochastic model presented here. This is described by a quantum Kushner-Stratonovich equation, typical of the form appearing in continuous-time collapse of the wave-function models in Quantum Decoherence Theory", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9Di\u00f3si-Penrose\u95ee\u9898\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u7a7a\u96f6\u5dee\u63a2\u6d4b\u91cf\u5b50\u6ee4\u6ce2\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u5f00\u653e\u91cf\u5b50\u968f\u673a\u6a21\u578b\u4e2d\u7684\u8f93\u51fa\u6b63\u4ea4\u5206\u91cf\u8fde\u7eed\u7edf\uff0c\u5efa\u7acb\u4e86\u91cf\u5b50Kushner-Stratonovich\u65b9\u7a0b\uff0c\u907f\u514d\u4e86\u9884\u8bbe\u80cc\u666f\u5f15\u529b\u6da8\u843d\u7684\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfDi\u00f3si-Penrose\u6a21\u578b\u4f9d\u8d56\u9884\u8bbe\u7684\u80cc\u666f\u5f15\u529b\u6da8\u843d\u6765\u89e3\u91ca\u6ce2\u51fd\u6570\u574d\u7f29\uff0c\u8fd9\u4e00\u5047\u8bbe\u5b58\u5728\u7406\u8bba\u4e0d\u786e\u5b9a\u6027\u3002\u4f5c\u8005\u65e8\u5728\u4ece\u91cf\u5b50\u6d4b\u91cf\u548c\u6ee4\u6ce2\u7406\u8bba\u7684\u57fa\u672c\u539f\u7406\u51fa\u53d1\uff0c\u5bfb\u627e\u4e00\u79cd\u4e0d\u4f9d\u8d56\u5f15\u529b\u6da8\u843d\u5047\u8bbe\u7684\u66ff\u4ee3\u6027\u7406\u8bba\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\u5f00\u653e\u91cf\u5b50\u968f\u673a\u7cfb\u7edf\u7406\u8bba\uff0c\u91c7\u7528\u65f6\u7a7a\u96f6\u5dee\u63a2\u6d4b\u6280\u672f\u5bf9\u8f93\u51fa\u6b63\u4ea4\u5206\u91cf\u7684\u8fde\u7eed\u7edf\u8fdb\u884c\u91cf\u5b50\u6ee4\u6ce2\uff0c\u63a8\u5bfc\u51fa\u63cf\u8ff0\u6ce2\u51fd\u6570\u8fde\u7eed\u574d\u7f29\u7684\u91cf\u5b50Kushner-Stratonovich\u65b9\u7a0b\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u4e0e\u4f20\u7edfDi\u00f3si-Penrose\u6a21\u578b\u5e73\u884c\u7684\u91cf\u5b50\u6ee4\u6ce2\u8868\u8ff0\uff0c\u5f97\u5230\u4e86\u76f8\u5e94\u7684\u91cf\u5b50Kushner-Stratonovich\u65b9\u7a0b\uff0c\u5e76\u5c06\u8be5\u6a21\u578b\u4e0e\u91cf\u5b50\u9000\u76f8\u5e72\u7406\u8bba\u4e2d\u7684\u8fde\u7eed\u574d\u7f29\u6a21\u578b\u5efa\u7acb\u4e86\u8054\u7cfb\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aDi\u00f3si-Penrose\u95ee\u9898\u63d0\u4f9b\u4e86\u4e0d\u4f9d\u8d56\u5f15\u529b\u6da8\u843d\u5047\u8bbe\u7684\u91cf\u5b50\u6ee4\u6ce2\u7406\u8bba\u6846\u67b6\uff0c\u4e30\u5bcc\u4e86\u6ce2\u51fd\u6570\u574d\u7f29\u673a\u5236\u7684\u7406\u8bba\u4f53\u7cfb\uff0c\u5e76\u4e0e\u73b0\u6709\u91cf\u5b50\u9000\u76f8\u5e72\u7406\u8bba\u5f62\u6210\u4e86\u826f\u597d\u8854\u63a5\u3002"}}
{"id": "2601.17394", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2601.17394", "abs": "https://arxiv.org/abs/2601.17394", "authors": ["Ramandeep Dewan"], "title": "Non-Markovian Decoherence Times in Finite-Memory Environments", "comment": "An earlier biologically motivated analysis of finite-memory effects in decoherence was presented in arXiv:2601.07689. The present work generalizes and reframes those results in a model-agnostic, operational framework applicable to arbitrary finite-memory environments. 15 pages, 2 figures", "summary": "Decoherence is often modeled using Markovian master equations that predict exponential suppression of coherence and are frequently used as effective bounds on quantum behavior in complex environments. Such descriptions, however, correspond to the singular physical limit of vanishing environmental memory. Here we formulate decoherence using a general time-nonlocal decoherence functional determined solely by the environmental force correlation function, with Markovian dynamics recovered explicitly as a limiting case.\n  For arbitrary stationary environments with finite temporal correlations, we show that the decoherence functional exhibits quadratic short-time growth that is model-independent within the finite-memory class considered. Consequently, the decoherence time defined operationally-without assuming exponential decay-scales as the square root of the environmental correlation time, independent of the detailed form of the bath correlation kernel.\n  These results are illustrated analytically for Gaussian-correlated, soft power-law, and Ornstein-Uhlenbeck environments. In the Ornstein-Uhlenbeck case, the non-Markovian dynamics admit an exact analytical closure, yielding a closed evolution equation for the coherence. Exact numerical simulations based on a pseudomode mapping confirm the predicted scaling and show that exponential decoherence emerges only in the memoryless limit.\n  Beyond coherence decay, we distinguish decoherence rates from observable loss of quantum signatures by analyzing purity and von Neumann entropy dynamics. We show that suppression of a specific coherence element need not coincide with irreversible entropy production. Finally, we introduce an inferred-memory perspective in which the environmental correlation time is treated as an operationally extractable parameter from dynamical data.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u6709\u9650\u8bb0\u5fc6\u73af\u5883\u4e2d\u9000\u76f8\u5e72\u5728\u77ed\u65f6\u5448\u4e8c\u6b21\u589e\u957f\u800c\u975e\u6307\u6570\u589e\u957f\uff0c\u9000\u76f8\u5e72\u65f6\u95f4\u6309\u5173\u8054\u65f6\u95f4\u7684\u5e73\u65b9\u6839\u7f29\u653e\uff0c\u6311\u6218\u4e86\u6807\u51c6\u7684\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u9a6c\u5c14\u53ef\u592b\u4e3b\u65b9\u7a0b\u63cf\u8ff0\u9000\u76f8\u5e72\u65f6\u5047\u8bbe\u73af\u5883\u8bb0\u5fc6\u6d88\u5931\uff0c\u9884\u6d4b\u76f8\u5e72\u6027\u6307\u6570\u6291\u5236\uff0c\u4f46\u8fd9\u5bf9\u5e94\u4e8e\u5947\u5f02\u6781\u9650\u3002\u5bf9\u4e8e\u5177\u6709\u6709\u9650\u8bb0\u5fc6\u65f6\u95f4\u7684\u5b9e\u9645\u73af\u5883\uff0c\u8fd9\u79cd\u63cf\u8ff0\u53ef\u80fd\u4e0d\u51c6\u786e\u3002", "method": "\u63d0\u51fa\u4e00\u822c\u6027\u65f6\u95f4\u975e\u5c40\u57df\u9000\u76f8\u5e72\u6cdb\u51fd\uff0c\u4ec5\u7531\u73af\u5883\u529b\u5173\u8054\u51fd\u6570\u51b3\u5b9a\u3002\u9a6c\u5c14\u53ef\u592b\u52a8\u529b\u5b66\u4f5c\u4e3a\u6781\u9650\u60c5\u51b5\u6062\u590d\u3002\u5bf9\u9ad8\u65af\u3001\u8f6f\u5e42\u5f8b\u548cOrnstein-Uhlenbeck\u73af\u5883\u8fdb\u884c\u89e3\u6790\u5206\u6790\uff0c\u91c7\u7528\u4f2a\u6a21\u6620\u5c04\u8fdb\u884c\u7cbe\u786e\u6570\u503c\u6a21\u62df\uff0c\u5e76\u5206\u6790\u7eaf\u5ea6\u548c\u51af\u8bfa\u4f9d\u66fc\u71b5\u52a8\u529b\u5b66\u3002", "result": "\u53d1\u73b0\u9000\u76f8\u5e72\u6cdb\u51fd\u5728\u77ed\u65f6\u5448\u4e8c\u6b21\u589e\u957f\uff0c\u4e0e\u6a21\u578b\u65e0\u5173\u3002\u64cd\u4f5c\u5b9a\u4e49\u7684\u9000\u76f8\u5e72\u65f6\u95f4\uff08\u4e0d\u5047\u8bbe\u6307\u6570\u8870\u51cf\uff09\u6309\u73af\u5883\u5173\u8054\u65f6\u95f4\u7684\u5e73\u65b9\u6839\u7f29\u653e\u3002Ornstein-Uhlenbeck\u60c5\u5f62\u4e0b\u53ef\u5f97\u5230\u7cbe\u786e\u89e3\u6790\u95ed\u5305\u3002\u4ec5\u5728\u65e0\u8bb0\u5fc6\u6781\u9650\u4e0b\u624d\u51fa\u73b0\u6307\u6570\u9000\u76f8\u5e72\u3002\u6291\u5236\u7279\u5b9a\u76f8\u5e72\u5143\u7d20\u4e0d\u4e00\u5b9a\u4e0e\u4e0d\u53ef\u9006\u71b5\u4ea7\u751f\u4e00\u81f4\u3002", "conclusion": "\u975e\u9a6c\u5c14\u53ef\u592b\u6548\u5e94\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u4e86\u9000\u76f8\u5e72\u884c\u4e3a\u3002\u73af\u5883\u5173\u8054\u65f6\u95f4\u662f\u53ef\u4ece\u52a8\u529b\u5b66\u6570\u636e\u4e2d\u63d0\u53d6\u7684\u64cd\u4f5c\u53c2\u6570\u3002\u9700\u8981\u533a\u5206\u76f8\u5e72\u6027\u8870\u51cf\u4e0e\u53ef\u89c2\u6d4b\u91cf\u5b50\u7279\u6027\u635f\u5931\u3002"}}
{"id": "2601.17370", "categories": ["nlin.CD"], "pdf": "https://arxiv.org/pdf/2601.17370", "abs": "https://arxiv.org/abs/2601.17370", "authors": ["Vinesh Vijayan"], "title": "Finite-scale geometric invariants for chaotic and weakly chaotic dynamics", "comment": "6 pages, 2 figures", "summary": "We introduce a finite scale geometric observable that quantifies the growth rate of localized sets under time evolution in dissipative dynamical systems. Defined at finite time and resolution without reference to symbolic dynamics or Markov partitions this observable converges, in uniformly hyperbolic systems, to a resolution dependent plateau whose logarithmic scaling coefficient equals the Kolmogorov Sinai entropy. In merely hyperbolic systems, it decays to zero, reflecting the absence of entropy production, while remaining well defined at finite scales. Numerical results for the Henon map and Feigenbaum point illustrate these behaviors. Our findings yield a finite scale geometric characterization of chaotic dynamics, consistent with classical entropy theory where applicable. We further demonstrate that the observable remains well defined in open intermittent systems, where trajectories escape and classical asymptotic invariants fail, revealing finite scale signatures of transient weak chaos.", "AI": {"tldr": "\u63d0\u51fa\u6709\u9650\u5c3a\u5ea6\u51e0\u4f55\u89c2\u6d4b\u91cf\uff0c\u91cf\u5316\u8017\u6563\u7cfb\u7edf\u4e2d\u5c40\u90e8\u96c6\u5408\u7684\u65f6\u95f4\u6f14\u5316\u589e\u957f\u7387\uff1b\u5728\u4e00\u81f4\u53cc\u66f2\u7cfb\u7edf\u4e2d\u6536\u655b\u4e8e\u4e0e\u67ef\u5c14\u83ab\u54e5\u6d1b\u592b-\u897f\u5948\u71b5\u76f8\u5173\u7684\u5e73\u53f0\uff0c\u5728\u975e\u4e00\u81f4\u53cc\u66f2\u7cfb\u7edf\u4e2d\u8870\u51cf\u81f3\u96f6\uff0c\u4e3a\u6df7\u6c8c\u52a8\u529b\u5b66\u63d0\u4f9b\u6709\u9650\u5c3a\u5ea6\u8868\u5f81", "motivation": "\u4f20\u7edf\u6df7\u6c8c\u91cf\u5316\u4f9d\u8d56\u6e10\u8fd1\u4e0d\u53d8\u91cf\uff08\u5982\u71b5\uff09\u548c\u7b26\u53f7\u52a8\u529b\u5b66/\u9a6c\u5c14\u53ef\u592b\u5212\u5206\uff0c\u96be\u4ee5\u63cf\u8ff0\u6709\u9650\u5c3a\u5ea6\u3001\u77ac\u6001\u6df7\u6c8c\u53ca\u5f00\u653e\u95f4\u6b47\u7cfb\u7edf\uff1b\u9700\u5efa\u7acb\u65e0\u9700\u5148\u9a8c\u5212\u5206\u7684\u51e0\u4f55\u5316\u6709\u9650\u5c3a\u5ea6\u89c2\u6d4b\u65b9\u6cd5", "method": "\u5b9a\u4e49\u6709\u9650\u65f6\u95f4\u548c\u5206\u8fa8\u7387\u4e0b\u7684\u51e0\u4f55\u89c2\u6d4b\u91cf\uff0c\u901a\u8fc7\u5c40\u90e8\u96c6\u5408\u7684\u6f14\u5316\u589e\u957f\u7387\u6355\u6349\u52a8\u529b\u5b66\u884c\u4e3a\uff1b\u907f\u514d\u4f7f\u7528\u7b26\u53f7\u52a8\u529b\u5b66\u6216\u9a6c\u5c14\u53ef\u592b\u5212\u5206", "result": "1. \u4e00\u81f4\u53cc\u66f2\u7cfb\u7edf\uff1a\u89c2\u6d4b\u91cf\u6536\u655b\u81f3\u5206\u8fa8\u7387\u4f9d\u8d56\u7684\u5e73\u53f0\uff0c\u5176\u5bf9\u6570\u6807\u5ea6\u7cfb\u6570\u7b49\u4e8e\u67ef\u5c14\u83ab\u54e5\u6d1b\u592b-\u897f\u5948\u71b5\uff1b2. \u4ec5\u53cc\u66f2\u7cfb\u7edf\uff1a\u89c2\u6d4b\u91cf\u8870\u51cf\u81f3\u96f6\uff0c\u53cd\u6620\u65e0\u71b5\u4ea7\u751f\uff1b3. \u5f00\u653e\u95f4\u6b47\u7cfb\u7edf\uff1a\u4ecd\u4fdd\u6301\u6709\u9650\u5b9a\u4e49\uff0c\u63ed\u793a\u77ac\u6001\u5f31\u6df7\u6c8c\u7684\u6709\u9650\u5c3a\u5ea6\u7279\u5f81\uff1b4. \u6570\u503c\u9a8c\u8bc1\uff1a\u5e94\u7528\u4e8eH\u00e9non\u6620\u5c04\u548cFeigenbaum\u70b9", "conclusion": "\u8be5\u89c2\u6d4b\u91cf\u5b9e\u73b0\u4e86\u6df7\u6c8c\u52a8\u529b\u5b66\u7684\u6709\u9650\u5c3a\u5ea6\u51e0\u4f55\u8868\u5f81\uff0c\u4e0e\u7ecf\u5178\u71b5\u7406\u8bba\u4e00\u81f4\uff08\u5728\u9002\u7528\u573a\u666f\uff09\uff0c\u5e76\u6269\u5c55\u81f3\u7ecf\u5178\u65b9\u6cd5\u5931\u6548\u7684\u5f00\u653e\u7cfb\u7edf\uff0c\u4e3a\u77ac\u6001\u6df7\u6c8c\u5206\u6790\u63d0\u4f9b\u65b0\u5de5\u5177"}}
{"id": "2601.18659", "categories": ["physics.data-an", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2601.18659", "abs": "https://arxiv.org/abs/2601.18659", "authors": ["Brian Richard Pauw", "Ingo Bre\u00dfler"], "title": "McSAS3: improved Monte Carlo small-angle scattering analysis software for dilute and dense scatterers", "comment": "15 pages, 4 figures", "summary": "McSAS3 is the refactored successor to the original McSAS Monte Carlo small-angle scattering analysis software. It is intended to be integrated in automated data processing pipelines, but can also be used to process individual (batches of) scattering data.\n  McSAS3 comes with a graphical user interface (McSAS3GUI), complete with guides, examples and videos. McSAS3GUI will help to generate and test the three configuration files that McSAS3 needs for data read-in, Monte Carlo optimization and histogramming. The user interface can also be used to process individual files or batches, and can be augmented with machine-specific use templates.\n  The Monte Carlo (MC) approach is able to fit most practical scattering patterns extremely well, resulting in form-free model parameter distributions. Theoretically, these can be distributions on any model parameter, but in practice the MC-optimized parameter is usually a (volume-weighted) size distribution, in absolute volume fraction for absolute-scaled data.", "AI": {"tldr": "McSAS3\u662f\u65b0\u4e00\u4ee3\u5c0f\u89d2\u6563\u5c04\u5206\u6790\u8f6f\u4ef6\uff0c\u91c7\u7528\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u5b9e\u73b0\u65e0\u6a21\u578b\u53c2\u6570\u5206\u5e03\u62df\u5408\uff0c\u652f\u6301\u81ea\u52a8\u5316\u6d41\u7a0b\u548c\u5355/\u6279\u91cf\u6570\u636e\u5904\u7406\uff0c\u914d\u5907\u7528\u6237\u53cb\u597d\u7684\u56fe\u5f62\u754c\u9762", "motivation": "\u539f\u59cbMcSAS\u8f6f\u4ef6\u9700\u8981\u5347\u7ea7\u4ee5\u9002\u5e94\u81ea\u52a8\u5316\u6570\u636e\u5904\u7406\u6d41\u7a0b\u7684\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u5355/\u6279\u91cf\u6563\u5c04\u6570\u636e\u7684\u5904\u7406\u80fd\u529b", "method": "\u91c7\u7528\u8499\u7279\u5361\u6d1b\u4f18\u5316\u7b97\u6cd5\u8fdb\u884c\u5c0f\u89d2\u6563\u5c04\u6570\u636e\u62df\u5408\uff0c\u901a\u8fc7\u56fe\u5f62\u754c\u9762(McSAS3GUI)\u751f\u6210\u548c\u6d4b\u8bd5\u914d\u7f6e\u6587\u4ef6\uff0c\u652f\u6301\u673a\u5668\u4e13\u7528\u6a21\u677f\u6269\u5c55", "result": "\u80fd\u591f\u9ad8\u7cbe\u5ea6\u62df\u5408\u5404\u7c7b\u5b9e\u9645\u6563\u5c04\u56fe\u6848\uff0c\u8f93\u51fa\u7edd\u5bf9\u4f53\u79ef\u523b\u5ea6\u6570\u636e\u4e0b\u7684\u65e0\u6a21\u578b\u53c2\u6570\u5206\u5e03\uff08\u901a\u5e38\u4e3a\u4f53\u79ef\u52a0\u6743\u5c3a\u5bf8\u5206\u5e03\uff09", "conclusion": "McSAS3\u6210\u529f\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\u96c6\u6210\u4e0e\u4ea4\u4e92\u5f0f\u4f7f\u7528\u7684\u53cc\u91cd\u76ee\u6807\uff0c\u5176\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u53c2\u6570\u5206\u5e03\u5206\u6790\u80fd\u529b\uff0cGUI\u663e\u8457\u964d\u4f4e\u4e86\u4f7f\u7528\u95e8\u69db"}}
{"id": "2601.17304", "categories": ["nlin.AO", "math.AP", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2601.17304", "abs": "https://arxiv.org/abs/2601.17304", "authors": ["Liangze Yang", "Hui Yu", "Jie Du"], "title": "From microscopic social force models to macroscopic continuum models for pedestrian flow", "comment": null, "summary": "The pedestrian flow is one of the most complex systems, involving large populations of interacting agents. Models at microscopic and macroscopic scales offer different advantages for studying related problems. In general, microscopic models can describe interaction forces at the individual level. Macroscopic models, on the other hand, provide analytical insights into global interactions and long-term overall dynamics, along with efficient numerical simulations and predictions. However, the relationship between models at different scales has rarely been explored. In this study, based on the original microscopic social force model with a reactive optimal route choice strategy, we first derive kinetic equations at the mesoscopic level. By varying the interaction force in different scenarios, we then derive several continuum models at the macroscopic level. Finally, numerical examples are given to evaluate the behaviors of the social force model and our continuum models.", "AI": {"tldr": "This paper bridges microscopic and macroscopic pedestrian flow models by deriving mesoscopic kinetic equations from a social force model, then developing continuum macroscopic models through varying interaction forces, with numerical validation showing consistent behavioral predictions across scales.", "motivation": "Existing pedestrian flow models operate at isolated scales (microscopic for individual interactions, macroscopic for global dynamics), with no established framework connecting these levels; this gap limits comprehensive analysis of crowd behavior.", "method": "Derived mesoscopic kinetic equations from a reactive optimal-route microscopic social force model, then systematically varied interaction force parameters to generate multiple macroscopic continuum models, followed by comparative numerical simulations.", "result": "Successfully linked three modeling scales (micro-meso-macro) and demonstrated that the derived continuum models accurately reproduced key behavioral patterns of the original social force model under diverse scenarios.", "conclusion": "The proposed multiscale framework establishes a rigorous mathematical connection between individual-level interactions and population-level dynamics, enabling more robust and versatile pedestrian flow analysis for complex crowd systems."}}
{"id": "2601.17282", "categories": ["cond-mat.stat-mech", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.17282", "abs": "https://arxiv.org/abs/2601.17282", "authors": ["Nathan Shettell", "Alexia Auff\u00e8ves"], "title": "Entropic Efficiency of Bayesian Inference Protocols", "comment": null, "summary": "Inference is a versatile tool that underlies scientific discovery, machine learning, and everyday decision-making: it describes how an agent updates a probability distribution as partial information is acquired from multiple measurements, reducing ignorance about a system's latent state. We define an inferential efficiency as the ratio of information gain to cumulative memory erasure cost, with inefficiency arising from unexploited correlations between the measured system and memories, and/or between memories and environment (noise). Using this efficiency, we benchmark two limiting measurement paradigms: sequential, in which the same memory is exploited iteratively, and parallel, in which many memories are exploited simultaneously. In both cases, the minimal erasure cost reflects correlations across memories: temporal in sequential, spatial in parallel. Remarkably, when all system-memory correlations are exploited for inference, both paradigms attain the same minimal erasure cost, even in the presence of noise. Conversely, the parallel paradigm performs better in the presence of unexploited correlations, stemming from hidden memories' degrees of freedom. This approach provides a quantitative, physically grounded criterion to compare inference strategies, determine their efficiency, and link target information gains to their minimal entropic cost.", "AI": {"tldr": "The paper defines inferential efficiency as information gain per memory erasure cost to benchmark sequential versus parallel measurement strategies, finding they achieve equal minimal thermodynamic cost when correlations are fully exploited, but parallel processing excels when correlations remain hidden.", "motivation": "Inference underlies scientific discovery, machine learning, and decision-making, yet lacks a complete physical understanding of its thermodynamic costs and optimal memory architectures for efficient information processing.", "method": "Defines an inferential efficiency metric (information gain divided by cumulative memory erasure cost) and benchmarks two limiting measurement paradigms: sequential (iterative memory use) and parallel (simultaneous memory exploitation) to analyze how correlations affect minimal erasure costs.", "result": "When all system-memory correlations are exploited, both paradigms achieve identical minimal erasure costs even with noise; when correlations are unexploited, parallel paradigm outperforms sequential due to hidden memory degrees of freedom; minimal cost reflects temporal correlations in sequential and spatial correlations in parallel architectures.", "conclusion": "Provides a quantitative, physically grounded framework for comparing inference strategies, determining their efficiency, and linking target information gains to minimal entropic costs, revealing when parallel processing offers thermodynamic advantages."}}
{"id": "2601.17121", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2601.17121", "abs": "https://arxiv.org/abs/2601.17121", "authors": ["Bogeng Wen", "Jiefu Cen", "Hae-Young Kee"], "title": "Ferrichiral skyrmions with sublattice-resolved chirality in extended Kitaev model in triangular lattice", "comment": "8 pages, 5 figures", "summary": "We study an extended Kitaev model on the triangular lattice in a limit where the symmetric off-diagonal bond-dependent and Heisenberg interactions together map onto an XXZ model, in addition to the Kitaev interaction. Within the previously identified $\\mathbb{Z}_2$ vortex regime, we uncover a ferrichiral skyrmion phase characterized by a sublattice-resolved scalar chirality: two of the three sublattices carry unit skyrmion charge, while the third remains nonchiral. Using classical Monte Carlo simulations, we show that this ferrichiral skyrmion phase emerges at zero temperature and in the absence of both an external magnetic field and Dzyaloshinskii-Moriya interactions, in sharp contrast to conventional skyrmion-hosting systems. The phase is stable over a wide parameter window and persists to relatively high temperatures. Our results reveal an unconventional route to skyrmion physics driven purely by frustrated exchange interactions and highlight the emergence of rich topological structures. Since both XXZ anisotropy and Kitaev interactions originate from the same spin-orbit-coupling mechanism, materials traditionally classified as XXZ magnets are expected to host finite Kitaev interactions as well. The potential for ferrichirality in these systems therefore warrants further investigation.", "AI": {"tldr": "\u5728\u4e09\u89d2\u6676\u683c\u6269\u5c55Kitaev\u6a21\u578b\u4e2d\uff0c\u7814\u7a76\u4eba\u5458\u901a\u8fc7\u7ecf\u5178\u8499\u7279\u5361\u6d1b\u6a21\u62df\u53d1\u73b0\u4e86\u4e00\u79cd\u65e0\u9700\u5916\u78c1\u573a\u6216Dzyaloshinskii-Moriya\u76f8\u4e92\u4f5c\u7528\u7684\"\u94c1\u624b\u6027\u65af\u683c\u660e\u5b50\u76f8\"\uff0c\u8be5\u76f8\u5728\u96f6\u6e29\u548c\u8f83\u5bbd\u53c2\u6570\u8303\u56f4\u5185\u7a33\u5b9a\uff0c\u63ed\u793a\u4e86\u7eaf\u963b\u632b\u4ea4\u6362\u76f8\u4e92\u4f5c\u7528\u9a71\u52a8\u7684\u975e\u4f20\u7edf\u65af\u683c\u660e\u5b50\u7269\u7406\u673a\u5236\u3002", "motivation": "\u4f20\u7edf\u65af\u683c\u660e\u5b50\u76f8\u901a\u5e38\u4f9d\u8d56\u5916\u78c1\u573a\u6216Dzyaloshinskii-Moriya\u76f8\u4e92\u4f5c\u7528\u7a33\u5b9a\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u7eaf\u7531\u963b\u632b\u4ea4\u6362\u76f8\u4e92\u4f5c\u7528\u9a71\u52a8\u7684\u62d3\u6251\u78c1\u7ed3\u6784\uff0c\u7279\u522b\u662f\u5728Kitaev\u6a21\u578b\u6846\u67b6\u4e0b\u5bfb\u627e\u4e0d\u4f9d\u8d56\u5916\u90e8\u6761\u4ef6\u7684\u65af\u683c\u660e\u5b50\u76f8\uff0c\u4ee5\u6df1\u5316\u5bf9\u91cf\u5b50\u963b\u632b\u7cfb\u7edf\u4e2d\u65b0\u5947\u62d3\u6251\u7269\u6001\u7684\u7406\u89e3\u3002", "method": "\u91c7\u7528\u7ecf\u5178\u8499\u7279\u5361\u6d1b\u6a21\u62df\u65b9\u6cd5\uff0c\u7cfb\u7edf\u7814\u7a76\u4e09\u89d2\u6676\u683c\u4e0a\u6269\u5c55Kitaev\u6a21\u578b\u5728\u7279\u5b9a\u53c2\u6570\u6781\u9650\u4e0b\u7684\u76f8\u884c\u4e3a\uff0c\u91cd\u70b9\u5206\u6790\u5bf9\u79f0\u975e\u5bf9\u89d2\u952e\u4f9d\u8d56\u76f8\u4e92\u4f5c\u7528\u548c\u6d77\u68ee\u5821\u76f8\u4e92\u4f5c\u7528\u6620\u5c04\u5230XXZ\u6a21\u578b\u7684\u884c\u4e3a\uff0c\u5e76\u7ed3\u5408Kitaev\u76f8\u4e92\u4f5c\u7528\u7684\u5f71\u54cd\u3002", "result": "\u5728\u5148\u524d\u8bc6\u522b\u7684Z\u2082\u6da1\u65cb\u533a\u5185\u53d1\u73b0\u4e86\u4e00\u79cd\u94c1\u624b\u6027\u65af\u683c\u660e\u5b50\u76f8\uff0c\u5176\u7279\u5f81\u662f\u4e09\u4e2a\u4e9a\u683c\u5b50\u4e2d\u4e24\u4e2a\u643a\u5e26\u5355\u4f4d\u65af\u683c\u660e\u5b50\u7535\u8377\uff0c\u7b2c\u4e09\u4e2a\u4fdd\u6301\u975e\u624b\u6027\u3002\u8be5\u76f8\u5728\u96f6\u6e29\u5ea6\u3001\u65e0\u5916\u78c1\u573a\u548c\u65e0Dzyaloshinskii-Moriya\u76f8\u4e92\u4f5c\u7528\u7684\u6761\u4ef6\u4e0b\u7a33\u5b9a\u5b58\u5728\uff0c\u4e14\u80fd\u5728\u8f83\u5bbd\u53c2\u6570\u7a97\u53e3\u548c\u76f8\u5bf9\u8f83\u9ad8\u6e29\u5ea6\u4e0b\u4fdd\u6301\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u7531\u963b\u632b\u4ea4\u6362\u76f8\u4e92\u4f5c\u7528\u7eaf\u9a71\u52a8\u7684\u975e\u4f20\u7edf\u65af\u683c\u660e\u5b50\u5f62\u6210\u8def\u5f84\uff0c\u8868\u660e\u4f20\u7edfXXZ\u78c1\u4f53\u6750\u6599\u7531\u4e8e\u5177\u6709\u76f8\u540c\u81ea\u65cb-\u8f68\u9053\u8026\u5408\u673a\u5236\uff0c\u5f88\u53ef\u80fd\u4e5f\u5b58\u5728\u6709\u9650Kitaev\u76f8\u4e92\u4f5c\u7528\u548c\u94c1\u624b\u6027\u6001\uff0c\u4e3a\u672a\u6765\u5728\u76f8\u5173\u6750\u6599\u4f53\u7cfb\u4e2d\u63a2\u7d22\u62d3\u6251\u78c1\u7ed3\u6784\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2601.17427", "categories": ["cond-mat.dis-nn", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2601.17427", "abs": "https://arxiv.org/abs/2601.17427", "authors": ["Cristopher Erazo", "Santiago Acevedo", "Alessandro Ingrosso"], "title": "The dimensionality of the Hopfield model", "comment": null, "summary": "We use the Binary Intrinsic Dimension (BID), a geometrical measure designed for binary data, to analyze the Hopfield model, a paradigmatic spin system from statistical mechanics, machine learning and neuroscience. The BID allows us to characterize the phases and transitions of this system, and moreover it is robust against finite-size effects that interfere with the correct numerical estimation of the spin-glass order parameter ($q$). We observe that the BID scales linearly with system size in the retrieval and paramagnetic phases, where the correlations between spins are small, and exhibits sublinear scaling in the whole spin-glass phase, highlighting its correlated structure. Furthermore, we establish a direct relationship between the BID and the overlap distribution, unveiling a novel connection between the geometry of the state-space and standard spin order parameters.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u4e13\u4e3a\u4e8c\u8fdb\u5236\u6570\u636e\u8bbe\u8ba1\u7684\u4e8c\u5143\u672c\u5f81\u7ef4\u5ea6(BID)\u51e0\u4f55\u6d4b\u5ea6\u5206\u6790Hopfield\u6a21\u578b\uff0c\u53d1\u73b0BID\u5728\u4e0d\u540c\u76f8\u4e2d\u5448\u73b0\u4e0d\u540c\u7684\u6807\u5ea6\u884c\u4e3a\uff0c\u5e76\u4e0e\u91cd\u53e0\u5206\u5e03\u5b58\u5728\u76f4\u63a5\u5173\u8054\uff0c\u4e3a\u7406\u89e3\u81ea\u65cb\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u51e0\u4f55\u89c6\u89d2\u3002", "motivation": "\u7531\u4e8e\u6709\u9650\u5c3a\u5bf8\u6548\u5e94\u4f1a\u5e72\u6270\u81ea\u65cb\u73bb\u7483\u5e8f\u53c2\u91cf(q)\u7684\u51c6\u786e\u6570\u503c\u4f30\u8ba1\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5e94\u7528\u4e13\u4e3a\u4e8c\u8fdb\u5236\u6570\u636e\u8bbe\u8ba1\u7684\u4e8c\u5143\u672c\u5f81\u7ef4\u5ea6(BID)\u8fd9\u4e00\u51e0\u4f55\u6d4b\u5ea6\uff0c\u6765\u5206\u6790\u5728\u7edf\u8ba1\u529b\u5b66\u3001\u673a\u5668\u5b66\u4e60\u548c\u795e\u7ecf\u79d1\u5b66\u4e2d\u5177\u6709\u8303\u5f0f\u610f\u4e49\u7684Hopfield\u6a21\u578b\uff0c\u4ee5\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u4e8c\u5143\u672c\u5f81\u7ef4\u5ea6(BID)\u8fd9\u4e00\u51e0\u4f55\u6d4b\u5ea6\uff0c\u5bf9Hopfield\u81ea\u65cb\u7cfb\u7edf\u8fdb\u884c\u5b9a\u91cf\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u81ea\u65cb\u95f4\u5173\u8054\u8f83\u5c0f\u7684\u68c0\u7d22\u76f8\u548c\u987a\u78c1\u76f8\u4e2d\uff0cBID\u968f\u7cfb\u7edf\u5c3a\u5bf8\u7ebf\u6027\u7f29\u653e\uff1b\u800c\u5728\u6574\u4e2a\u81ea\u65cb\u73bb\u7483\u76f8\u4e2d\uff0cBID\u5448\u73b0\u6b21\u7ebf\u6027\u7f29\u653e\uff0c\u63ed\u793a\u4e86\u5176\u5173\u8054\u7ed3\u6784\u3002\u6b64\u5916\uff0c\u7814\u7a76\u5efa\u7acb\u4e86BID\u4e0e\u91cd\u53e0\u5206\u5e03\u4e4b\u95f4\u7684\u76f4\u63a5\u5173\u7cfb\uff0c\u53d1\u73b0\u4e86\u72b6\u6001\u7a7a\u95f4\u51e0\u4f55\u7ed3\u6784\u4e0e\u6807\u51c6\u81ea\u65cb\u5e8f\u53c2\u91cf\u4e4b\u95f4\u7684\u65b0\u8054\u7cfb\u3002", "conclusion": "BID\u80fd\u591f\u6709\u6548\u8868\u5f81Hopfield\u6a21\u578b\u7684\u76f8\u548c\u76f8\u53d8\uff0c\u5e76\u63ed\u793a\u72b6\u6001\u7a7a\u95f4\u51e0\u4f55\u7ed3\u6784\u4e0e\u81ea\u65cb\u5e8f\u53c2\u91cf\u4e4b\u95f4\u7684\u6df1\u523b\u8054\u7cfb\uff0c\u4e3a\u5206\u6790\u4e8c\u8fdb\u5236\u6570\u636e\u7cfb\u7edf\u7684\u51e0\u4f55\u7279\u6027\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2601.17459", "categories": ["quant-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2601.17459", "abs": "https://arxiv.org/abs/2601.17459", "authors": ["Lachlan G. Bishop"], "title": "Qhronology: A Python package for studying quantum models of closed timelike curves", "comment": "80 pages, 4 figures", "summary": "Qhronology is a novel scientific-computing package for studying quantum models of closed timelike curves (CTCs) and simulating general quantum information processing and computation. Written in Python, the program provides a comprehensive framework for analyzing quantum theories of antichronological time travel, including functionality to calculate quantum resolutions to temporal paradoxes. It also operates as a complete quantum circuit simulator, enabling the examination of quantum algorithms and protocols in both numerical and symbolic capacities. In this paper, we formally introduce Qhronology, beginning with discussion on aspects of its design philosophy and architecture. An overview of its basic usage is then presented, along with a collection of examples demonstrating its various capabilities within a variety of distinct contexts. Lastly, the performance of the package's circuit simulation component is characterized by way of some simple empirical benchmarking.", "AI": {"tldr": "This paper introduces Qhronology, a Python-based scientific computing package for simulating quantum models of closed timelike curves (CTCs) and general quantum information processing.", "motivation": "To provide a comprehensive computational framework for studying quantum theories of antichronological time travel and analyzing quantum resolutions to temporal paradoxes, while also serving as a complete quantum circuit simulator.", "method": "Developing a Python package that implements quantum circuit simulation with both numerical and symbolic capabilities, specifically designed to handle CTC quantum models and temporal paradox calculations.", "result": "The paper presents Qhronology's design philosophy, architecture, basic usage, demonstrates its capabilities through various examples, and provides empirical benchmarking of its circuit simulation performance.", "conclusion": "Qhronology successfully bridges quantum computing and theoretical time travel research, offering researchers a versatile tool for both specialized CTC studies and general quantum algorithm development."}}
{"id": "2601.17724", "categories": ["quant-ph", "math-ph", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2601.17724", "abs": "https://arxiv.org/abs/2601.17724", "authors": ["Noufal Jaseem", "Sergi Ramos-Calderer", "Gauthameshwar S.", "Dingzu Wang", "Jos\u00e9 Ignacio Latorre", "Dario Poletti"], "title": "Quantum-Inspired Algorithms beyond Unitary Circuits: the Laplace Transform", "comment": "9 pages", "summary": "Quantum-inspired algorithms can deliver substantial speedups over classical state-of-the-art methods by executing quantum algorithms with tensor networks on conventional hardware. Unlike circuit models restricted to unitary gates, tensor networks naturally accommodate non-unitary maps. This flexibility lets us design quantum-inspired methods that start from a quantum algorithmic structure, yet go beyond unitarity to achieve speedups. Here we introduce a tensor-network approach to compute the discrete Laplace transform, a non-unitary, aperiodic transform (in contrast to the Fourier transform). We encode a length-$N$ signal on two paired $n$-qubit registers and decompose the overall map into a non-unitary exponential Damping Transform followed by a Quantum Fourier Transform, both compressed in a single matrix-product operator. This decomposition admits strong MPO compression to low bond dimension resulting in significant acceleration. We demonstrate simulations up to $N=2^{30}$ input data points, with up to $2^{60}$ output data points, and quantify how bond dimension controls runtime and accuracy, including precise and efficient pole identification.", "AI": {"tldr": "This paper introduces a tensor-network-based quantum-inspired algorithm to efficiently compute the non-unitary discrete Laplace transform on classical hardware, achieving significant speedups for large-scale data (up to 2^30 inputs) via MPO compression and a novel decomposition combining damping and Fourier transforms.", "motivation": "Quantum algorithms typically rely on unitary operations, but many important transforms (like the discrete Laplace transform) are non-unitary and aperiodic. Existing quantum circuit models cannot efficiently handle such operations, limiting practical applications. This work aims to bridge this gap by leveraging tensor networks' flexibility to accommodate non-unitary maps for acceleration.", "method": "The approach encodes input signals across two paired n-qubit registers, decomposes the Laplace transform into a compressed non-unitary Damping Transform followed by a Quantum Fourier Transform (QFT), and represents the entire operation as a single matrix-product operator (MPO). Critical to the method is aggressive MPO compression to low bond dimension, enabling efficient execution on classical hardware.", "result": "Simulations successfully processed datasets up to N=2^30 input points (yielding 2^60 output points), demonstrating that low bond dimensions in the MPO representation enable substantial computational acceleration while maintaining controllable accuracy. The method also allows precise pole identification within the transform.", "conclusion": "By extending quantum-inspired algorithms beyond unitarity through tensor-network flexibility and MPO compression, this work achieves practical speedups for non-unitary transforms like the discrete Laplace transform on classical hardware, opening avenues for efficient large-scale signal processing and mathematical computations."}}
{"id": "2601.18018", "categories": ["nlin.AO", "cs.ET", "eess.SP", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2601.18018", "abs": "https://arxiv.org/abs/2601.18018", "authors": ["Boran A. Kilic", "Ozgur B. Akan"], "title": "Neural-Inspired Multi-Agent Molecular Communication Networks for Collective Intelligence", "comment": "5 pages, 4 figures, submitted for MOLCOM 26 Workshop", "summary": "Molecular Communication (MC) is a pivotal enabler for the Internet of Bio-Nano Things (IoBNT). However, current research often relies on super-capable individual agents with complex transceiver architectures that defy the energy and processing constraints of realistic nanomachines. This paper proposes a paradigm shift towards collective intelligence, inspired by the cortical networks of the biological brain. We introduce a decentralized network architecture where simple nanomachines interact via a diffusive medium using a threshold-based firing mechanism modeled by Greenberg-Hastings (GH) cellular automata. We derive fixed-point equations for steady-state populations via mean-field analysis and validate them against stochastic simulations. We demonstrate that the network undergoes a second-order phase transition at a specific activation threshold. Crucially, we show that both pairwise and collective mutual information peak exactly at this critical transition point, confirming that the system maximizes information propagation and processing capacity at the edge of chaos.", "AI": {"tldr": "This paper proposes a collective intelligence approach for Molecular Communication in IoBNT, using simple nanomachines with Greenberg-Hastings cellular automata. It shows that the system achieves maximum information processing at a critical phase transition point (edge of chaos), validated through mean-field analysis and simulations.", "motivation": "Current MC research relies on overly complex, energy-intensive individual nanomachines that are impractical for real-world IoBNT applications due to severe energy and processing constraints. There's a need for simpler, more scalable solutions.", "method": "The authors propose a decentralized architecture where simple nanomachines interact through a diffusive medium using a threshold-based firing mechanism modeled by Greenberg-Hastings cellular automata. They employ mean-field analysis to derive fixed-point equations for steady-state populations and validate these theoretical predictions against stochastic simulations.", "result": "The network exhibits a second-order phase transition at a specific activation threshold. Both pairwise and collective mutual information peak precisely at this critical point, demonstrating that the system maximizes information propagation and processing capacity at the edge of chaos.", "conclusion": "Collective intelligence through simple nanomachines can enable efficient molecular communication for IoBNT, with optimal information processing occurring at the critical phase transition point, offering a biologically-inspired paradigm shift."}}
{"id": "2601.17488", "categories": ["cond-mat.stat-mech", "cond-mat.mtrl-sci", "cond-mat.soft", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2601.17488", "abs": "https://arxiv.org/abs/2601.17488", "authors": ["Quinn M. Gallagher", "Ryan J. Szukalo", "Nicolas Giovambattista", "Pablo G. Debenedetti", "Michael A. Webb"], "title": "A Local Structural Basis to Resolve Amorphous Ices", "comment": null, "summary": "Phases with distinct thermodynamic properties must differ in their underlying distributions of microscopic structures. While ordered phases are readily distinguished by unit cells and space groups, the local structural basis differentiating amorphous phases is less apparent. Here, using a new probabilistic data-driven framework applied to molecular simulation data on water, we identify local collective variables that discriminate low-density and high-density amorphous (LDA and HDA) ices and characterize pressure-induced transitions between these phases. As expected, descriptors related to local density capably distinguish LDA and HDA; however, phase identity is surprisingly encoded within the first coordination shell. Furthermore, LDA transitions to HDA by a simple redistribution of LDA- and HDA-like environments with no evident intermediate structures, in accordance with a first-order-like transition that contrasts with the gradual evolution observed in other amorphous systems such as metallic glasses. These findings are robust across force fields, which themselves exhibit structural differences, and exemplify how other systems lacking obvious distinguishing features can be characterized.", "AI": {"tldr": "This paper develops a probabilistic data-driven framework to identify local structural variables distinguishing low-density and high-density amorphous ices, revealing that phase identity is encoded in the first coordination shell and transitions occur via simple redistribution without intermediates, exhibiting first-order-like behavior.", "motivation": "Amorphous phases lack ordered structures like unit cells, making their differentiation challenging. While ordered phases are distinguished by symmetry, the microscopic basis for distinguishing amorphous phases remains unclear, necessitating new methods to identify their distinguishing local structural features.", "method": "A new probabilistic data-driven framework is applied to molecular simulation data of water to identify local collective variables that discriminate LDA and HDA phases and characterize their pressure-induced transitions, analyzing robustness across different force fields.", "result": "Local density descriptors distinguish LDA/HDA; phase identity is encoded within the first coordination shell; LDA transitions to HDA via simple redistribution of environments without intermediates, showing first-order-like behavior unlike gradual transitions in metallic glasses; findings are robust across force fields.", "conclusion": "Amorphous phases can be characterized by local structural features in the first coordination shell, providing a general framework for systems lacking obvious distinguishing features, and reveals fundamental differences in phase transition mechanisms between amorphous materials."}}
{"id": "2601.18331", "categories": ["cond-mat.str-el", "cond-mat.dis-nn", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.18331", "abs": "https://arxiv.org/abs/2601.18331", "authors": ["Junmo Jeon", "Shiro Sakai"], "title": "Quantum Hyperuniformity and Quantum Weight", "comment": "15 pages, 9 figures", "summary": "Extending hyperuniformity from classical to quantum fluctuations in electron systems yields a framework that identifies quantum phase transitions and reveals underlying gap structures through the quantum weight. We study long-wavelength fluctuations of many-body ground states through the charge-density structure factor by incorporating intrinsic quantum fluctuations into hyperuniformity. Although charge fluctuations at zero temperature are generally suppressed by particle-number conservation, their long-wavelength scaling reveals distinct universal behaviors that define quantum hyperuniformity classes. By exemplifying the Aubry-Andre model, we find that gapped, gapless, and localized-critical-extended phases are sharply distinguished by the quantum hyperuniformity classes. Notably, at the critical point, multifractal wave functions generate anomalous scaling behavior. We further show that, in quantum-hyperuniform gapped phases, the quantum weight provides a quantitative measure of the gap size through a universal power-law scaling. Along with classical hyperuniformity, quantum hyperuniformity serves a direct fingerprint of quantum criticality and a practical probe of quantum phase transitions in aperiodic electron systems.", "AI": {"tldr": "Extending hyperuniformity to quantum fluctuations in electron systems creates a framework for identifying quantum phase transitions and gap structures via \"quantum weight,\" distinguishing different phases (gapped, gapless, localized-critical-extended) as demonstrated with the Aubry-Andr\u00e9 model.", "motivation": "To generalize hyperuniformity from classical to quantum systems for characterizing quantum phase transitions and underlying gap structures in electron systems, especially in aperiodic/disordered environments.", "method": "Analyzing long-wavelength fluctuations of many-body ground states through the charge-density structure factor, incorporating intrinsic quantum fluctuations into hyperuniformity theory, and exemplifying with the Aubry-Andr\u00e9 model.", "result": "Different quantum phases (gapped, gapless, localized-critical-extended) are sharply distinguished by quantum hyperuniformity classes; critical points exhibit anomalous scaling from multifractal wave functions; in gapped phases, quantum weight quantitatively measures gap size via universal power-law scaling.", "conclusion": "Quantum hyperuniformity serves as a direct fingerprint of quantum criticality and a practical probe for quantum phase transitions in aperiodic electron systems, providing a powerful new framework for characterizing quantum phases."}}
{"id": "2601.17465", "categories": ["quant-ph", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.17465", "abs": "https://arxiv.org/abs/2601.17465", "authors": ["Akram Youssry", "Stefan Todd", "Patrick Murton", "Muhammad Junaid Arshad", "Alberto Peruzzo", "Cristian Bonato"], "title": "Bayesian quantum sensing using graybox machine learning", "comment": null, "summary": "Quantum sensors offer significant advantages over classical devices in spatial resolution and sensitivity, enabling transformative applications across materials science, healthcare, and beyond. Their practical performance, however, is often constrained by unmodelled effects, including noise, imperfect state preparation, and non-ideal control fields.\n  In this work, we report the first experimental implementation of a graybox modelling strategy for a solid-state open quantum system. The graybox framework integrates a physics-based system model with a data-driven description of experimental imperfections, achieving higher fidelity than purely analytical (whitebox) approaches while requiring fewer training resources than fully deep-learning models. We experimentally validate the method on the task of estimating a static magnetic field using a single-spin quantum sensor, performing Bayesian inference with a graybox model trained on prior experimental data. Using roughly 10,000 training datapoints, the graybox model yields several orders of magnitude improvement in mean squared error over the corresponding physics-only model. These results are broadly applicable to a wide range of quantum sensing platforms, not limited to single-spin systems, and are particularly valuable for real-time adaptive protocols, where model inaccuracies can otherwise lead to suboptimal control and degraded performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7070\u7bb1\u5efa\u6a21\u7b56\u7565\uff0c\u5c06\u57fa\u4e8e\u7269\u7406\u7684\u6a21\u578b\u4e0e\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u76f8\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cf\u5b50\u4f20\u611f\u5668\u7684\u6027\u80fd\uff0c\u5e76\u5728\u5355\u81ea\u65cb\u91cf\u5b50\u4f20\u611f\u5668\u4e0a\u9a8c\u8bc1\u4e86\u5176\u5728\u78c1\u573a\u4f30\u8ba1\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u91cf\u5b50\u4f20\u611f\u5668\u5728\u7a7a\u95f4\u5206\u8fa8\u7387\u548c\u7075\u654f\u5ea6\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4f46\u5176\u5b9e\u9645\u6027\u80fd\u5e38\u53d7\u5230\u566a\u58f0\u3001\u4e0d\u5b8c\u7f8e\u7684\u6001\u5236\u5907\u548c\u975e\u7406\u60f3\u63a7\u5236\u573a\u7b49\u672a\u5efa\u6a21\u6548\u5e94\u7684\u9650\u5236\u3002", "method": "\u7814\u7a76\u56e2\u961f\u9996\u6b21\u5728\u56fa\u6001\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u4e2d\u5b9e\u9a8c\u5b9e\u65bd\u4e86\u7070\u7bb1\u5efa\u6a21\u7b56\u7565\uff0c\u5c06\u57fa\u4e8e\u7269\u7406\u7684\u7cfb\u7edf\u6a21\u578b\u4e0e\u5b9e\u9a8c\u7f3a\u9677\u7684\u6570\u636e\u9a71\u52a8\u63cf\u8ff0\u76f8\u7ed3\u5408\uff0c\u5229\u7528\u7ea610,000\u4e2a\u8bad\u7ec3\u6570\u636e\u70b9\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u63a8\u7406\u8fdb\u884c\u78c1\u573a\u4f30\u8ba1\u3002", "result": "\u7070\u7bb1\u6a21\u578b\u6bd4\u7eaf\u7269\u7406\u6a21\u578b\u7684\u5747\u65b9\u8bef\u5dee\u63d0\u9ad8\u4e86\u51e0\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5404\u79cd\u91cf\u5b50\u4f20\u611f\u5e73\u53f0\uff0c\u5bf9\u4e8e\u5b9e\u65f6\u81ea\u9002\u5e94\u534f\u8bae\u5c24\u5176\u6709\u4ef7\u503c\uff0c\u56e0\u4e3a\u5728\u8fd9\u4e9b\u534f\u8bae\u4e2d\u6a21\u578b\u4e0d\u51c6\u786e\u4f1a\u5bfc\u81f4\u6b21\u4f18\u63a7\u5236\u548c\u6027\u80fd\u4e0b\u964d\u3002"}}
{"id": "2601.18653", "categories": ["nlin.AO", "cond-mat.dis-nn", "cond-mat.soft", "cond-mat.stat-mech", "hep-th"], "pdf": "https://arxiv.org/pdf/2601.18653", "abs": "https://arxiv.org/abs/2601.18653", "authors": ["Igor Halperin"], "title": "Order Out of Noise and Disorder: Fate of the Frustrated Manifold", "comment": "60 pages, 10 figures", "summary": "We study Langevin dynamics of $N$ Brownian particles on compact two-dimensional Riemannian manifolds, interacting through pairwise potentials linear in geodesic distance with quenched random couplings. These \\emph{frustrated Brownian particles} experience the competing demands of random attractive and repulsive interactions while confined to curved surfaces. We consider three geometries: the sphere $S^2$, torus $T^2$ (closed manifolds), and bounded cylinder $S^1 \\times [0,H]$ (manifold with boundary). Our central finding is disorder-induced dimension reduction accompanied by spontaneous breaking of rotational symmetry: order emerges from the combination of two sources of randomness (thermal noise and quenched disorder), with the manifold topology determining the character of the emerging structures. Glassy relaxation drives particles from 2D distributions to quasi-1D structures, specifically bands on $S^2$, rings on $T^2$, and localized clusters on the cylinder. The symmetry breaking pattern depends on topology: SO(3)$\\to$SO(2) on the sphere, SO(2)$\\times$SO(2)$\\to$SO(2)$\\times\\mathbb{Z}_2$ on the torus, and SO(2)$\\to\\mathbb{Z}_2$ on the cylinder. Unlike conventional spontaneous symmetry breaking, the symmetry-breaking direction is not frozen but evolves slowly via thermal noise through type-A diffusive Nambu-Goldstone dynamics, while the reduced-dimensional structure persists. Unlike conventional self-organizing systems that require external driving or fine-tuned nonlinearities, our model demonstrates that geometry and topology alone can channel randomness into order. We discuss connections to spin glass theory, quantum field theory, astrophysical structure formation, and self-organizing systems, providing a geometric framework for understanding how disorder generates emergent spatial order on curved spaces.", "AI": {"tldr": "\u7814\u7a76\u53d7\u632b\u5e03\u6717\u7c92\u5b50\u5728\u5f2f\u66f2\u66f2\u9762\u4e0a\u7684\u52a8\u529b\u5b66\uff0c\u53d1\u73b0\u65e0\u5e8f\u53ef\u8bf1\u5bfc\u7ef4\u5ea6\u964d\u4f4e\u548c\u5bf9\u79f0\u6027\u7834\u7f3a\uff0c\u4ece\u800c\u5f62\u6210\u6709\u5e8f\u7ed3\u6784\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5728\u5f2f\u66f2\u7a7a\u95f4\u4e2d\u901a\u8fc7\u51e0\u4f55\u548c\u62d3\u6251\u7ed3\u6784\u5c06\u968f\u673a\u6027\u8f6c\u5316\u4e3a\u6709\u5e8f\u6027\uff0c\u4e3a\u65e0\u5e8f\u7cfb\u7edf\u4ea7\u751f\u6d8c\u73b0\u7a7a\u95f4\u79e9\u5e8f\u63d0\u4f9b\u51e0\u4f55\u6846\u67b6\uff0c\u5e76\u4e0e\u4f20\u7edf\u9700\u8981\u5916\u90e8\u9a71\u52a8\u6216\u7cbe\u7ec6\u8c03\u8282\u7684\u975e\u7ebf\u6027\u81ea\u7ec4\u7ec7\u7cfb\u7edf\u5f62\u6210\u5bf9\u6bd4\u3002", "method": "\u91c7\u7528\u6717\u4e4b\u4e07\u52a8\u529b\u5b66\u65b9\u6cd5\u7814\u7a76N\u4e2a\u5e03\u6717\u7c92\u5b50\u5728\u7d27\u51d1\u4e8c\u7ef4\u9ece\u66fc\u6d41\u5f62\u4e0a\u7684\u884c\u4e3a\uff0c\u7c92\u5b50\u901a\u8fc7\u6d4b\u5730\u8ddd\u79bb\u7ebf\u6027\u52bf\u76f8\u4e92\u4f5c\u7528\uff0c\u5e76\u5f15\u5165\u6dec\u706b\u968f\u673a\u8026\u5408\u3002\u5728\u7403\u9762S\u00b2\u3001\u73af\u9762T\u00b2\u548c\u6709\u754c\u5706\u67f1S\u00b9\u00d7[0,H]\u4e09\u79cd\u51e0\u4f55\u7ed3\u6784\u4e0a\u8fdb\u884c\u6a21\u62df\u3002", "result": "\u53d1\u73b0\u73bb\u7483\u5316\u5f1b\u8c6b\u9a71\u52a8\u7c92\u5b50\u4ece\u4e8c\u7ef4\u5206\u5e03\u8f6c\u53d8\u4e3a\u51c6\u4e00\u7ef4\u7ed3\u6784\uff1a\u7403\u9762\u4e0a\u5f62\u6210\u5e26\u72b6\u7ed3\u6784\uff0c\u73af\u9762\u4e0a\u5f62\u6210\u73af\u72b6\u7ed3\u6784\uff0c\u5706\u67f1\u4e0a\u5f62\u6210\u5c40\u57df\u5316\u56e2\u7c07\u3002\u5bf9\u79f0\u6027\u7834\u7f3a\u6a21\u5f0f\u53d6\u51b3\u4e8e\u62d3\u6251\uff1aSO(3)\u2192SO(2)\uff08\u7403\u9762\uff09\u3001SO(2)\u00d7SO(2)\u2192SO(2)\u00d7Z\u2082\uff08\u73af\u9762\uff09\u3001SO(2)\u2192Z\u2082\uff08\u5706\u67f1\uff09\u3002\u5bf9\u79f0\u6027\u7834\u7f3a\u65b9\u5411\u901a\u8fc7A\u578b\u6269\u6563\u578bNambu-Goldstone\u52a8\u529b\u5b66\u7f13\u6162\u6f14\u5316\u3002", "conclusion": "\u51e0\u4f55\u548c\u62d3\u6251\u7ed3\u6784\u672c\u8eab\u5c31\u80fd\u5c06\u968f\u673a\u6027\u5f15\u5bfc\u6210\u6709\u5e8f\uff0c\u65e0\u9700\u5916\u90e8\u9a71\u52a8\u6216\u7cbe\u7ec6\u8c03\u8282\u7684\u975e\u7ebf\u6027\u673a\u5236\u3002\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u65e0\u5e8f\u5728\u5f2f\u66f2\u7a7a\u95f4\u4ea7\u751f\u6d8c\u73b0\u7a7a\u95f4\u79e9\u5e8f\u7684\u51e0\u4f55\u6846\u67b6\uff0c\u5e76\u4e0e\u81ea\u65cb\u73bb\u7483\u7406\u8bba\u3001\u91cf\u5b50\u573a\u8bba\u3001\u5929\u4f53\u7269\u7406\u7ed3\u6784\u5f62\u6210\u7b49\u9886\u57df\u5efa\u7acb\u4e86\u8054\u7cfb\u3002"}}
{"id": "2601.17691", "categories": ["cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2601.17691", "abs": "https://arxiv.org/abs/2601.17691", "authors": ["Shan Suo", "Ao Zhou", "Yanting Chen", "Shujie Cheng", "Gao Xianlong"], "title": "Wigner distribution, Wigner entropy and Quantum Refrigerator of a One-Dimensional Off-diagonal Quasicrystal", "comment": "9 pages, 6 figures", "summary": "We investigate an off-diagonal quasicrystal featuring simultaneous off-diagonal and diagonal quasiperiodic modulations. By analyzing the fractal dimension, we map out the delocalization-localization phase diagram. We demonstrate that delocalized and localized states can be distinguished via the Wigner distribution, while extended, critical, and localized phases are separated using the Wigner entropy. Furthermore, we explore the quantum thermodynamic properties, revealing that localized states facilitate the emergence of a quantum heater mode, alongside the appearance of a refrigerator mode. These findings enhance our understanding of localization phenomena and expand the thermodynamic applications of quasiperiodic systems.", "AI": {"tldr": "\u7814\u7a76\u5177\u6709\u53cc\u91cd\u51c6\u5468\u671f\u8c03\u5236\u975e\u5bf9\u89d2\u51c6\u6676\u4f53\u7684\u5c40\u57df\u5316\u73b0\u8c61\uff0c\u901a\u8fc7\u5206\u5f62\u7ef4\u6570\u548c\u7ef4\u683c\u7eb3\u5206\u6790\u7ed8\u5236\u76f8\u56fe\uff0c\u63ed\u793a\u91cf\u5b50\u70ed\u673a/\u5236\u51b7\u673a\u6a21\u5f0f\u3002", "motivation": "\u63a2\u7a76\u591a\u91cd\u8c03\u5236\u51c6\u6676\u4f53\u7684\u5c40\u57df\u5316\u76f8\u53d8\u673a\u5236\uff0c\u62d3\u5c55\u51c6\u5468\u671f\u7cfb\u7edf\u5728\u91cf\u5b50\u70ed\u529b\u5b66\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u5206\u5f62\u7ef4\u6570\u5206\u6790\u7ed8\u5236\u9000\u5c40\u57df\u5316-\u5c40\u57df\u5316\u76f8\u56fe\uff0c\u5229\u7528\u7ef4\u683c\u7eb3\u5206\u5e03\u533a\u5206\u9000\u5c40\u57df\u5316/\u5c40\u57df\u5316\u6001\uff0c\u91c7\u7528\u7ef4\u683c\u7eb3\u71b5\u5206\u79bb\u6269\u5c55\u76f8\u3001\u4e34\u754c\u76f8\u548c\u5c40\u57df\u5316\u76f8\u3002", "result": "\u83b7\u5f97\u9000\u5c40\u57df\u5316-\u5c40\u57df\u5316\u76f8\u56fe\uff1b\u7ef4\u683c\u7eb3\u5206\u5e03\u53ef\u533a\u5206\u9000\u5c40\u57df\u5316\u4e0e\u5c40\u57df\u5316\u6001\uff1b\u7ef4\u683c\u7eb3\u71b5\u53ef\u5206\u79bb\u6269\u5c55\u3001\u4e34\u754c\u548c\u5c40\u57df\u5316\u76f8\uff1b\u5c40\u57df\u5316\u6001\u4fc3\u8fdb\u91cf\u5b50\u52a0\u70ed\u6a21\u5f0f\u51fa\u73b0\uff0c\u540c\u65f6\u4ea7\u751f\u5236\u51b7\u6a21\u5f0f\u3002", "conclusion": "\u6df1\u5316\u4e86\u5bf9\u5c40\u57df\u5316\u73b0\u8c61\u7684\u7406\u89e3\uff0c\u62d3\u5c55\u4e86\u51c6\u5468\u671f\u7cfb\u7edf\u5728\u91cf\u5b50\u70ed\u529b\u5b66\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2601.17455", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2601.17455", "abs": "https://arxiv.org/abs/2601.17455", "authors": ["Sagar Mahapatra", "Francesco De Angelis", "Dibyata Rout", "Priyanshi Tiwari", "Martin Etter", "Edmund Welter", "M. P. Saravanan", "Rajeev Rawat", "Satoshi Nishimoto", "Carlo Meneghini", "Surjeet Singh"], "title": "Emergent Random Spin Singlets in Disordered Spin-1/2 perovskite BaCu$_{1/3}$Ta$_{2/3}$O$_3$", "comment": null, "summary": "We investigate the disordered perovskite BaCu$_{1/3}$Ta$_{2/3}$O$_3$, where Cu (spin-1/2) and Ta randomly occupy a pseudo-cubic lattice. Synchrotron X-ray diffraction and X-ray absorption spectroscopy establish the local nature of the disorder, revealing the presence of structurally constrained magnetic exchange paths. No magnetic ordering or spin freezing is observed down to 0.1 K. The low-temperature magnetic and thermodynamic behavior is captured by a broad but non-singular distribution $P(J)$ of exchange couplings $J$. These results open the possibility of realizing a disordered quantum ground state where the exchange randomness is broad yet intrinsically bounded, departing from the conventional infinite-randomness fixed point driven random-singlet phase.", "AI": {"tldr": "\u7814\u7a76\u65e0\u5e8f\u9499\u949b\u77ffBaCu$_{1/3}$Ta$_{2/3}$O$_3$\u53d1\u73b0\u5176\u5728\u6781\u4f4e\u6e29\u4e0b\u65e0\u78c1\u5e8f\uff0c\u4f46\u5b58\u5728\u5bbd\u800c\u6709\u9650\u754c\u7684\u4ea4\u6362\u8026\u5408\u5206\u5e03\uff0c\u63ed\u793a\u4e86\u4e00\u79cd\u65b0\u578b\u53d7\u9650\u968f\u673a\u91cf\u5b50\u57fa\u6001", "motivation": "\u63a2\u7d22\u5177\u6709\u672c\u5f81\u53d7\u9650\u968f\u673a\u6027\u7684\u65e0\u5e8f\u91cf\u5b50\u78c1\u4f53\u57fa\u6001\uff0c\u6311\u6218\u4f20\u7edf\u65e0\u9650\u968f\u673a\u6027\u56fa\u5b9a\u70b9\u7406\u8bba\u63cf\u8ff0\u7684\u968f\u673a\u5355\u6001\u76f8", "method": "\u5229\u7528\u540c\u6b65\u8f90\u5c04X\u5c04\u7ebf\u884d\u5c04\u548cX\u5c04\u7ebf\u5438\u6536\u5149\u8c31\u8868\u5f81\u5c40\u57df\u65e0\u5e8f\u7ed3\u6784\uff0c\u5206\u6790\u4f4e\u6e29\u78c1\u6027\u4e0e\u70ed\u529b\u5b66\u884c\u4e3a", "result": "\u89c2\u5bdf\u5230Cu/Ta\u968f\u673a\u5360\u4f4d\u5bfc\u81f4\u7684\u7ed3\u6784\u53d7\u9650\u78c1\u4ea4\u6362\u8def\u5f84\uff0c0.1K\u4ee5\u4e0b\u65e0\u78c1\u5e8f\u6216\u81ea\u65cb\u51bb\u7ed3\uff0c\u5448\u73b0\u5bbd\u4f46\u975e\u5947\u5f02\u7684\u4ea4\u6362\u8026\u5408\u5206\u5e03P(J)", "conclusion": "\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u4ea4\u6362\u968f\u673a\u6027\u5bbd\u800c\u672c\u5f81\u53d7\u9650\u7684\u91cf\u5b50\u57fa\u6001\u5b58\u5728\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u65e0\u9650\u968f\u673a\u6027\u56fa\u5b9a\u70b9\u7406\u8bba\u6846\u67b6"}}
{"id": "2601.17514", "categories": ["quant-ph", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.17514", "abs": "https://arxiv.org/abs/2601.17514", "authors": ["Nitin Jha", "Abhishek Parakh"], "title": "Are Quantum Voting Protocols Practical?", "comment": null, "summary": "Quantum voting protocols aim to offer ballot secrecy and publicly verifiable tallies using physical guarantees from quantum mechanics, rather than relying solely on computational hardness. This article surveys whether such quantum voting protocols are practical. We begin by outlining core mathematical ideas such as the superposition principle, the no-cloning theorem, and quantum entanglement. We then define a common system and threat model, identifying key actors, trust assumptions, and security goals. Representative protocol families are reviewed, including entanglement-based schemes with central tallying, self-tallying designs that enable public verification, and authority-minimized approaches that certify untrusted devices through observable correlations. Finally, we evaluate implementation challenges, including loss, noise, device imperfections, scalability, and coercion resistance, and discuss realistic near-term deployment scenarios for small-scale elections.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u91cf\u5b50\u6295\u7968\u534f\u8bae\u7684\u5b9e\u9645\u53ef\u884c\u6027\uff0c\u5206\u6790\u5176\u7406\u8bba\u57fa\u7840\u3001\u4e3b\u8981\u67b6\u6784\u7c7b\u578b\u3001\u5173\u952e\u5b9e\u65bd\u6311\u6218\uff08\u635f\u8017\u3001\u566a\u58f0\u3001\u53ef\u6269\u5c55\u6027\u3001\u6297\u80c1\u8feb\uff09\uff0c\u5e76\u63a2\u8ba8\u8fd1\u671f\u5728\u5c0f\u89c4\u6a21\u9009\u4e3e\u4e2d\u7684\u90e8\u7f72\u524d\u666f\u3002", "motivation": "\u4f20\u7edf\u6295\u7968\u4f9d\u8d56\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u91cf\u5b50\u6295\u7968\u5229\u7528\u91cf\u5b50\u529b\u5b66\u7279\u6027\uff08\u5982\u4e0d\u53ef\u514b\u9686\u5b9a\u7406\uff09\u5b9e\u73b0\u9009\u7968\u4fdd\u5bc6\u548c\u516c\u5f00\u53ef\u9a8c\u8bc1\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u8bc4\u4f30\u8fd9\u4e9b\u534f\u8bae\u7684\u5b9e\u9645\u53ef\u884c\u6027\u3002", "method": "1) \u9610\u8ff0\u53e0\u52a0\u3001\u4e0d\u53ef\u514b\u9686\u3001\u7ea0\u7f20\u7b49\u91cf\u5b50\u539f\u7406\uff1b2) \u5efa\u7acb\u7edf\u4e00\u5a01\u80c1\u6a21\u578b\uff1b3) \u7efc\u8ff0\u4e09\u7c7b\u534f\u8bae\uff1a\u7ea0\u7f20\u4e2d\u5fc3\u5316\u8ba1\u7968\u3001\u81ea\u8ba1\u7968\u516c\u5f00\u9a8c\u8bc1\u3001\u6700\u5c0f\u5316\u6743\u5a01\u8ba4\u8bc1\uff1b4) \u8bc4\u4f30\u6280\u672f\u6311\u6218\uff1b5) \u8ba8\u8bba\u90e8\u7f72\u573a\u666f\u3002", "result": "\u8bc6\u522b\u51fa\u635f\u8017\u3001\u566a\u58f0\u3001\u8bbe\u5907\u7f3a\u9677\u3001\u53ef\u6269\u5c55\u6027\u548c\u6297\u80c1\u8feb\u7b49\u6838\u5fc3\u6311\u6218\uff0c\u6307\u51fa\u77ed\u671f\u5185\u4ec5\u9002\u7528\u4e8e\u5c0f\u89c4\u6a21\u9009\u4e3e\u3002", "conclusion": "\u91cf\u5b50\u6295\u7968\u867d\u5177\u7406\u8bba\u4f18\u52bf\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u9762\u4e34\u91cd\u5927\u6280\u672f\u969c\u788d\u3002\u8fd1\u671f\u4ec5\u53ef\u80fd\u5728\u53c2\u4e0e\u65b9\u8f83\u5c11\u7684\u5c0f\u89c4\u6a21\u9009\u4e3e\u4e2d\u90e8\u7f72\uff0c\u5927\u89c4\u6a21\u5e94\u7528\u9700\u6280\u672f\u7a81\u7834\u3002"}}
{"id": "2601.18136", "categories": ["cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2601.18136", "abs": "https://arxiv.org/abs/2601.18136", "authors": ["Daniel Maria Busiello", "Shiling Liang", "Simone Pigolotti"], "title": "Non-equilibrium symmetry of cyclic first-passage times", "comment": null, "summary": "We study the sum of first passage times along an arbitrary cycle made up of N>2 states of a small physical system. We show that, if the system is at thermodynamic equilibrium, this sum follows the same probability distribution regardless of whether the cycle is explored clockwise or counterclockwise. Out of equilibrium, the distributions of clockwise and counterclockwise cyclic first passage times are related by a detailed fluctuation theorem. This result descends from a symmetry of clockwise and counterclockwise trajectories, which combines time reversal with swapping portions of the trajectories. We then relate the entropy produced along the cycle with the entropy production of the whole system using large deviation theory. Our results reveal a novel symmetry in stochastic systems, of potential broad applicability in non-equilibrium physics.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5c0f\u7269\u7406\u7cfb\u7edf\u4e2d\u4efb\u610f\u5faa\u73af\u7684\u9996\u6b21\u901a\u8fc7\u65f6\u95f4\u4e4b\u548c\u5728\u70ed\u529b\u5b66\u5e73\u8861\u65f6\u4e0e\u65b9\u5411\u65e0\u5173\uff0c\u975e\u5e73\u8861\u65f6\u5219\u6ee1\u8db3\u6da8\u843d\u5b9a\u7406\uff0c\u63ed\u793a\u4e86\u4e00\u79cd\u7ed3\u5408\u65f6\u95f4\u53cd\u6f14\u548c\u8f68\u8ff9\u4ea4\u6362\u7684\u65b0\u5bf9\u79f0\u6027\uff0c\u5e76\u5c06\u5faa\u73af\u71b5\u4ea7\u751f\u4e0e\u7cfb\u7edf\u6574\u4f53\u71b5\u4ea7\u751f\u5173\u8054\u3002", "motivation": "\u63a2\u7d22\u968f\u673a\u7cfb\u7edf\u7279\u522b\u662f\u975e\u5e73\u8861\u7269\u7406\u4e2d\u5faa\u73af\u9996\u6b21\u901a\u8fc7\u65f6\u95f4\u7684\u5bf9\u79f0\u6027\u89c4\u5f8b\uff0c\u7406\u89e3\u71b5\u4ea7\u751f\u5728\u5c40\u90e8\u5faa\u73af\u4e0e\u6574\u4f53\u7cfb\u7edf\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u91c7\u7528\u9996\u6b21\u901a\u8fc7\u65f6\u95f4\u5206\u6790\u3001\u5927\u504f\u5dee\u7406\u8bba\uff0c\u4ee5\u53ca\u7ed3\u5408\u65f6\u95f4\u53cd\u6f14\u548c\u8f68\u8ff9\u7247\u6bb5\u4ea4\u6362\u7684\u5bf9\u79f0\u6027\u8bba\u8bc1\u65b9\u6cd5\u3002", "result": "1) \u5e73\u8861\u6001\u4e0b\uff0c\u987a\u65f6\u9488\u548c\u9006\u65f6\u9488\u63a2\u7d22\u5faa\u73af\u7684\u9996\u6b21\u901a\u8fc7\u65f6\u95f4\u603b\u548c\u670d\u4ece\u76f8\u540c\u6982\u7387\u5206\u5e03\uff1b2) \u975e\u5e73\u8861\u6001\u4e0b\uff0c\u4e24\u4e2a\u65b9\u5411\u7684\u5206\u5e03\u6ee1\u8db3\u8be6\u7ec6\u7684\u6da8\u843d\u5b9a\u7406\uff1b3) \u53d1\u73b0\u4e86\u4e00\u79cd\u65b0\u7684\u8f68\u8ff9\u5bf9\u79f0\u6027\uff1b4) \u5efa\u7acb\u4e86\u5faa\u73af\u71b5\u4ea7\u751f\u4e0e\u7cfb\u7edf\u6574\u4f53\u71b5\u4ea7\u751f\u7684\u7406\u8bba\u8054\u7cfb\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u7684\u968f\u673a\u7cfb\u7edf\u65b0\u5bf9\u79f0\u6027\u5728\u975e\u5e73\u8861\u7269\u7406\u4e2d\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6f5c\u529b\u3002"}}
{"id": "2601.17718", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2601.17718", "abs": "https://arxiv.org/abs/2601.17718", "authors": ["Bo Zhan", "Jia-Lin Chen", "Zhen Fan", "Tao Xiang"], "title": "Tree tensor network solver for real-time quantum impurity dynamics", "comment": null, "summary": "We introduce a tree tensor network (TTN) impurity solver that enables highly efficient and accurate real-time simulations of quantum impurity models. By decomposing a noninteracting bath Hamiltonian into a Cayley tree, the method provides a tensor network representation that naturally captures the multiscale entanglement structure intrinsic to impurity-bath systems. This geometry differs from conventional chain-based mappings and yields a substantial reduction of entanglement, allowing accurate ground-state properties and long-time dynamics to be captured at significantly lower bond dimensions. Benchmark calculations for the single-impurity Anderson model demonstrate that the TTN solver achieves markedly enhanced resolution of real-frequency spectral functions, without invoking analytic continuation. This impurity solver provides a balanced, scale-uniform description of impurity physics and offers a versatile approach for real-time dynamical mean-field theory and related applications involving quantum impurity models.", "AI": {"tldr": "A tree tensor network (TTN) impurity solver using Cayley tree decomposition enables efficient, accurate real-time simulations of quantum impurity models with reduced entanglement and lower computational cost compared to conventional chain-based methods.", "motivation": "To enable highly efficient and accurate real-time simulations of quantum impurity models by capturing the multiscale entanglement structure of impurity-bath systems, addressing limitations of conventional chain-based mappings.", "method": "Decompose a noninteracting bath Hamiltonian into a Cayley tree to create a tensor network representation that naturally captures multiscale entanglement structure.", "result": "Substantial entanglement reduction allows accurate ground-state and long-time dynamics at lower bond dimensions; benchmark calculations on single-impurity Anderson model show enhanced resolution of real-frequency spectral functions without analytic continuation.", "conclusion": "The TTN solver provides a balanced, scale-uniform description of impurity physics and offers a versatile approach for real-time dynamical mean-field theory and quantum impurity model applications."}}
{"id": "2601.17515", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.17515", "abs": "https://arxiv.org/abs/2601.17515", "authors": ["Rudraksh Sharma"], "title": "Quantum Phase Transitions in the Transverse-Field Ising Model: A Comparative Study of Exact, Variational, and Hardware-Based Approaches", "comment": null, "summary": "The quantum phase transitions provide a paradigm for studying collective quantum phenomena that are a result of competing non-commuting interactions. This paper will study the ground state properties and quantum critical dynamics of the one-dimensional transverse field Ising model through a combined perspective that includes exact diagonalisation, variational quantum eigensolver (VQE) simulations, and simulations on realistic physical quantum devices. We focus on a lattice of four spins, where we calculate the ground-state energies, magnetic order parameters and correlation functions at uniformly applied conditions, which is repeated by all systems. Precise diagonalisation provides both a benchmark, which is symmetry-conserving, and a depth-two, physics inspired variational approximation, which provides simulations accessible to hardware. The circuits that have been optimised identically are then placed on the IQM Garnet quantum processor, using a resource-efficient batched protocol. We find that the ground-state energies of shallow variational circuits are reliably captured by the circuit over the entire parameter space; the magnetic arrangement parameters and observables sensitive to correlation signal significantly more noise. The error analysis of quantitative analysis reveals a strong broadening of critical crossover on hardware, which is consistent with the noise attenuation of long-range correlations. These findings highlight the current capabilities as well as the fundamental limitations of noisy intermediate-scale quantum systems in modelling quantum critical phenomena as a benchmark to future enhancements in obtaining quantum hardware and quantum algorithms development.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u7cbe\u786e\u5bf9\u89d2\u5316\u3001\u53d8\u5206\u91cf\u5b50\u672c\u5f81\u6c42\u89e3\u5668(VQE)\u6a21\u62df\u548c\u771f\u5b9e\u91cf\u5b50\u5904\u7406\u5668\u5b9e\u9a8c\uff0c\u7814\u7a76\u4e86\u56db\u81ea\u65cb\u4e00\u7ef4\u6a2a\u573a\u4f0a\u8f9b\u6a21\u578b\u7684\u57fa\u6001\u6027\u8d28\u4e0e\u91cf\u5b50\u4e34\u754c\u52a8\u529b\u5b66\uff0c\u53d1\u73b0\u6d45\u5c42\u53d8\u5206\u7535\u8def\u80fd\u53ef\u9760\u8ba1\u7b97\u57fa\u6001\u80fd\u91cf\uff0c\u4f46\u78c1\u5e8f\u53c2\u6570\u548c\u76f8\u5173\u51fd\u6570\u89c2\u6d4b\u53d7\u566a\u58f0\u5f71\u54cd\u663e\u8457\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u542b\u566a\u58f0\u4e2d\u7b49\u89c4\u6a21\u91cf\u5b50(NISQ)\u7cfb\u7edf\u5728\u6a21\u62df\u91cf\u5b50\u4e34\u754c\u73b0\u8c61\u65f6\u7684\u80fd\u529b\u4e0e\u5c40\u9650\u3002", "motivation": "\u8bc4\u4f30\u542b\u566a\u58f0\u4e2d\u7b49\u89c4\u6a21\u91cf\u5b50(NISQ)\u8bbe\u5907\u5728\u6a21\u62df\u91cf\u5b50\u4e34\u754c\u73b0\u8c61\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765\u91cf\u5b50\u786c\u4ef6\u548c\u7b97\u6cd5\u53d1\u5c55\u63d0\u4f9b\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u7ed3\u5408\u7cbe\u786e\u5bf9\u89d2\u5316(\u57fa\u51c6)\u3001\u53d8\u5206\u91cf\u5b50\u672c\u5f81\u6c42\u89e3\u5668(VQE)\u6a21\u62df\u548c\u8d44\u6e90\u9ad8\u6548\u7684\u6279\u5904\u7406\u534f\u8bae\uff0c\u5728IQM Garnet\u91cf\u5b50\u5904\u7406\u5668\u4e0a\u6a21\u62df\u56db\u81ea\u65cb\u6a2a\u573a\u4f0a\u8f9b\u6a21\u578b\uff0c\u8ba1\u7b97\u57fa\u6001\u80fd\u91cf\u3001\u78c1\u5e8f\u53c2\u6570\u548c\u76f8\u5173\u51fd\u6570\u3002", "result": "\u6d45\u5c42\u53d8\u5206\u7535\u8def\u5728\u5168\u53c2\u6570\u7a7a\u95f4\u5185\u80fd\u53ef\u9760\u6355\u83b7\u57fa\u6001\u80fd\u91cf\uff1b\u4f46\u78c1\u5e8f\u53c2\u6570\u548c\u76f8\u5173\u51fd\u6570\u654f\u611f\u91cf\u53d7\u566a\u58f0\u5f71\u54cd\u663e\u8457\uff0c\u786c\u4ef6\u5b9e\u9a8c\u663e\u793a\u4e34\u754c\u4ea4\u53c9\u533a\u57df\u51fa\u73b0\u5f3a\u70c8\u5c55\u5bbd\uff0c\u4e0e\u957f\u7a0b\u5173\u8054\u7684\u566a\u58f0\u8870\u51cf\u4e00\u81f4\u3002", "conclusion": "\u5f53\u524dNISQ\u7cfb\u7edf\u53ef\u90e8\u5206\u6a21\u62df\u91cf\u5b50\u4e34\u754c\u73b0\u8c61\uff0c\u4f46\u566a\u58f0\u4e25\u91cd\u9650\u5236\u4e86\u5bf9\u5173\u8054\u654f\u611f\u91cf\u7684\u7cbe\u786e\u6d4b\u91cf\uff1b\u8be5\u5de5\u4f5c\u4e3a\u91cf\u5b50\u786c\u4ef6\u4f18\u5316\u548c\u7b97\u6cd5\u5f00\u53d1\u63d0\u4f9b\u4e86\u5173\u952e\u57fa\u51c6\uff0c\u6307\u660e\u4e86\u672a\u6765\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2601.18215", "categories": ["cond-mat.stat-mech", "cond-mat.soft"], "pdf": "https://arxiv.org/pdf/2601.18215", "abs": "https://arxiv.org/abs/2601.18215", "authors": ["Yan-Wei Li", "Rui Ding", "Wen-Hao Ma"], "title": "Separating Energy and Entropy Contributions to the Hexatic-Liquid Transitions in Two-Dimensional Repulsive Systems", "comment": "9 pages, 5 figures", "summary": "Over the past decades, research on two-dimensional melting has established that both first-order and continuous hexatic-liquid transitions can occur, influenced by various factors in the potential energy and system details. The fundamental thermodynamic origins of this sensitivity remains elusive. Here, by decomposing the Helmholtz free energy across three representative repulsive systems, we reveal a universal competition between energy and entropy that dictates the melting pathway. The energetic contribution consistently imparts convexity to the free energy, whereas entropy imparts concavity. A first-order transition occurs when concave entropy dominates; otherwise, the transition is continuous. Further decomposition shows that vibrational entropy drives the concave total entropic curvature, while the configurational entropy's curvature switches from convex (first-order) to concave (continuous), mirroring defect proliferation measured by Shannon entropy. The convexity of the energy is dominated by the inherent potential, with minimal vibrational influence. Finally, we predict and verify that the first-order transition becomes continuous at zero temperature, where entropic effects vanish. Our work establishes the curvature of different thermodynamic quantities as a fundamental principle for understanding the nature of two-dimensional melting.", "AI": {"tldr": "\u63ed\u793a\u4e86\u4e8c\u7ef4\u7194\u5316\u8def\u5f84\u7531\u80fd\u91cf\uff08\u51f8\u6027\uff09\u4e0e\u71b5\uff08\u51f9\u6027\uff09\u7684\u7ade\u4e89\u51b3\u5b9a\uff1a\u71b5\u4e3b\u5bfc\u65f6\u4e3a\u4e00\u9636\u76f8\u53d8\uff0c\u5426\u5219\u4e3a\u8fde\u7eed\u76f8\u53d8\uff0c\u5efa\u7acb\u4e86\u70ed\u529b\u5b66\u91cf\u66f2\u7387\u4f5c\u4e3a\u7406\u89e3\u4e8c\u7ef4\u7194\u5316\u7684\u57fa\u672c\u539f\u7406\u3002", "motivation": "\u4e8c\u7ef4\u7194\u5316\u7814\u7a76\u4e2d\uff0c\u7cfb\u7edf\u5728\u52bf\u80fd\u548c\u7ec6\u8282\u5f71\u54cd\u4e0b\u53ef\u53d1\u751f\u4e00\u9636\u6216\u8fde\u7eed\u516d\u8fb9\u5f62-\u6db2\u4f53\u76f8\u53d8\uff0c\u4f46\u5176\u6839\u672c\u70ed\u529b\u5b66\u8d77\u6e90\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u901a\u8fc7\u5206\u89e3\u4e09\u79cd\u5178\u578b\u6392\u65a5\u7cfb\u7edf\u7684\u4ea5\u59c6\u970d\u5179\u81ea\u7531\u80fd\uff0c\u5206\u6790\u80fd\u91cf\u4e0e\u71b5\u7684\u8d21\u732e\u66f2\u7387\uff0c\u5e76\u8fdb\u4e00\u6b65\u5206\u79bb\u632f\u52a8\u71b5\u548c\u6784\u578b\u71b5\u7684\u4f5c\u7528\u3002", "result": "\u80fd\u91cf\u8d21\u732e\u59cb\u7ec8\u4f7f\u81ea\u7531\u80fd\u5448\u51f8\u6027\uff0c\u71b5\u8d21\u732e\u5448\u51f9\u6027\uff1b\u71b5\u51f9\u6027\u4e3b\u5bfc\u65f6\u53d1\u751f\u4e00\u9636\u76f8\u53d8\uff0c\u5426\u5219\u4e3a\u8fde\u7eed\u76f8\u53d8\u3002\u6784\u578b\u71b5\u66f2\u7387\u4ece\u51f8\uff08\u4e00\u9636\uff09\u5230\u51f9\uff08\u8fde\u7eed\uff09\u7684\u8f6c\u53d8\u4e0e\u7f3a\u9677\u589e\u6b96\u7684\u9999\u519c\u71b5\u4e00\u81f4\uff0c\u4e14\u96f6\u6e29\u4e0b\u4e00\u9636\u76f8\u53d8\u56e0\u71b5\u6548\u5e94\u6d88\u5931\u800c\u8f6c\u4e3a\u8fde\u7eed\u3002", "conclusion": "\u4e0d\u540c\u70ed\u529b\u5b66\u91cf\u7684\u66f2\u7387\u662f\u51b3\u5b9a\u4e8c\u7ef4\u7194\u5316\u672c\u8d28\u7684\u6839\u672c\u539f\u7406\uff0c\u9610\u660e\u4e86\u80fd\u91cf-\u71b5\u7ade\u4e89\u5bf9\u76f8\u53d8\u8def\u5f84\u7684\u666e\u9002\u8c03\u63a7\u673a\u5236\u3002"}}
{"id": "2601.18273", "categories": ["cond-mat.str-el", "cs.LG", "hep-lat"], "pdf": "https://arxiv.org/pdf/2601.18273", "abs": "https://arxiv.org/abs/2601.18273", "authors": ["Janik Kreit", "Andrea Bulgarelli", "Lena Funcke", "Thomas Luu", "Dominic Schuh", "Simran Singh", "Lorenzo Verzichelli"], "title": "Toward Scalable Normalizing Flows for the Hubbard Model", "comment": "10 pages, 5 figues, The 42nd International Symposium on Lattice Field Theory", "summary": "Normalizing flows have recently demonstrated the ability to learn the Boltzmann distribution of the Hubbard model, opening new avenues for generative modeling in condensed matter physics. In this work, we investigate the steps required to extend such simulations to larger lattice sizes and lower temperatures, with a focus on enhancing stability and efficiency. Additionally, we present the scaling behavior of stochastic normalizing flows and non-equilibrium Markov chain Monte Carlo methods for this fermionic system.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u5c06\u6b63\u89c4\u5316\u6d41\u6a21\u62df\u6269\u5c55\u5230\u66f4\u5927\u7684\u6676\u683c\u5c3a\u5bf8\u548c\u66f4\u4f4e\u6e29\u5ea6\uff0c\u5e76\u6bd4\u8f83\u4e86\u968f\u673a\u6b63\u89c4\u5316\u6d41\u4e0e\u975e\u5e73\u8861\u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u5728\u8d39\u7c73\u5b50\u7cfb\u7edf\u4e0a\u7684\u6269\u5c55\u884c\u4e3a\u3002", "motivation": "\u6b63\u89c4\u5316\u6d41\u5df2\u8bc1\u660e\u80fd\u591f\u5b66\u4e60\u54c8\u4f2f\u5fb7\u6a21\u578b\u7684\u73bb\u5c14\u5179\u66fc\u5206\u5e03\uff0c\u4e3a\u51dd\u805a\u6001\u7269\u7406\u4e2d\u7684\u751f\u6210\u5efa\u6a21\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002\u672c\u7814\u7a76\u65e8\u5728\u5c06\u6b64\u7c7b\u6a21\u62df\u6269\u5c55\u5230\u66f4\u5177\u6311\u6218\u6027\u7684\u7269\u7406\u53c2\u6570\u533a\u95f4\uff08\u66f4\u5927\u6676\u683c\u3001\u66f4\u4f4e\u6e29\u5ea6\uff09\uff0c\u540c\u65f6\u63d0\u9ad8\u7a33\u5b9a\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5b9e\u73b0\u6269\u5c55\u6240\u9700\u7684\u6b65\u9aa4\uff0c\u5e76\u5bf9\u4e24\u79cd\u65b9\u6cd5\u2014\u2014\u968f\u673a\u6b63\u89c4\u5316\u6d41\u548c\u975e\u5e73\u8861\u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u6d1b\u2014\u2014\u5728\u54c8\u4f2f\u5fb7\u8d39\u7c73\u5b50\u7cfb\u7edf\u4e0a\u7684\u6269\u5c55\u884c\u4e3a\u8fdb\u884c\u7cfb\u7edf\u6027\u6bd4\u8f83\u3002", "result": "\u6458\u8981\u672a\u7ed9\u51fa\u5177\u4f53\u6570\u503c\u7ed3\u679c\uff0c\u4ec5\u8868\u660e\u5448\u73b0\u4e86\u4e24\u79cd\u65b9\u6cd5\u7684\u6269\u5c55\u884c\u4e3a\u5206\u6790\u3002", "conclusion": "\u6458\u8981\u672a\u660e\u786e\u9648\u8ff0\u7ed3\u8bba\uff0c\u4f46\u5de5\u4f5c\u6697\u793a\u7406\u89e3\u6269\u5c55\u884c\u4e3a\u5bf9\u4e8e\u5c06\u8fd9\u4e9b\u65b9\u6cd5\u63a8\u5e7f\u5230\u66f4\u5927\u3001\u66f4\u5177\u7269\u7406\u610f\u4e49\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2601.17552", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.17552", "abs": "https://arxiv.org/abs/2601.17552", "authors": ["Mohamed Hatifi"], "title": "Autonomous phonon maser in levitated spin-mechanics", "comment": null, "summary": "Levitated nanodiamonds hosting a single nitrogen-vacancy (NV) center provide an ultra-low-frequency mechanical mode with widely tunable dissipation and spin backaction under microwave dressing and optical pumping. We demonstrate that the driven NV spin can be tuned to act as an inverted gain medium for the center-of-mass motion, thereby stabilizing an autonomous phonon maser. In the separation-of-timescales regime where spin dynamics is fast, adiabatic elimination yields a reduced mechanical master equation with closed-form, detuning-dependent transition rates and a sharp threshold given by the sign change of the phonon-number damping. For representative levitated-NV parameters, we find that a percent-level dressed-basis inversion is sufficient to reach the threshold, and the small-signal gain can exceed the intrinsic mechanical loss by orders of magnitude. Full master-equation simulations confirm above-threshold self-oscillation and a phase-diffusing, coherent steady state, whose saturation follows the Maxwell-Bloch prediction.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6f14\u793a\u4e86\u60ac\u6d6e\u7eb3\u7c73\u91d1\u521a\u77f3\u4e2d\u7684\u5355\u4e2a\u6c2e\u7a7a\u4f4d\u4e2d\u5fc3\u5728\u5fae\u6ce2\u548c\u5149\u6cf5\u6d66\u4e0b\u53ef\u4f5c\u4e3a\u53cd\u8f6c\u589e\u76ca\u4ecb\u8d28\uff0c\u5b9e\u73b0\u81ea\u4e3b phonon maser \u7684\u7a33\u5b9a\u8f93\u51fa\uff0c\u5e76\u63ed\u793a\u4e86\u5176\u9608\u503c\u7279\u6027\u3002", "motivation": "\u60ac\u6d6e\u7eb3\u7c73\u91d1\u521a\u77f3\u4e2d\u7684\u6c2e\u7a7a\u4f4d\u4e2d\u5fc3\u63d0\u4f9b\u4e86\u8d85\u4f4e\u97f3\u673a\u68b0\u6a21\u5f0f\u53ca\u53ef\u8c03\u7684\u8017\u6563\u4e0e\u81ea\u65cb\u53cd\u4f5c\u7528\uff0c\u63a2\u7d22\u5176\u5b9e\u73b0\u58f0\u5b50 maser \u5bf9\u4e8e\u91cf\u5b50\u4f20\u611f\u3001\u57fa\u7840\u7269\u7406\u548c\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "method": "\u91c7\u7528\u5fae\u6ce2 dressing \u548c\u5149\u6cf5\u6d66\u6280\u672f\uff0c\u5728\u5206\u79bb\u65f6\u95f4\u5c3a\u5ea6 regime \u4e0b\u901a\u8fc7\u7edd\u70ed elimination \u65b9\u6cd5\u63a8\u5bfc\u51fa\u5177\u6709\u5931\u8c10\u4f9d\u8d56\u8dc3\u8fc1\u7387\u7684\u7ea6\u5316\u673a\u68b0\u4e3b\u65b9\u7a0b\uff0c\u5e76\u901a\u8fc7\u5b8c\u6574\u4e3b\u65b9\u7a0b\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u7406\u8bba\u9884\u6d4b\u3002", "result": "\u7814\u7a76\u8868\u660e\u4ec5\u9700\u767e\u5206\u4e4b\u4e00\u91cf\u7ea7\u7684 dressed-basis \u53cd\u8f6c\u5373\u53ef\u8fbe\u5230\u9608\u503c\uff0c\u5c0f\u4fe1\u53f7\u589e\u76ca\u53ef\u6bd4\u672c\u5f81\u673a\u68b0\u635f\u8017\u9ad8\u51fa\u6570\u4e2a\u91cf\u7ea7\uff0c\u5168\u6570\u503c\u6a21\u62df\u8bc1\u5b9e\u4e86\u9608\u503c\u4ee5\u4e0a\u7684\u81ea\u6301\u632f\u8361\u548c\u7b26\u5408 Maxwell-Bloch \u7406\u8bba\u7684\u9971\u548c\u884c\u4e3a\u3002", "conclusion": "\u60ac\u6d6e NV \u7cfb\u7edf\u80fd\u591f\u9ad8\u6548\u5730\u5c06\u81ea\u65cb\u5e03\u5c45\u53cd\u8f6c\u8f6c\u5316\u4e3a\u673a\u68b0\u632f\u8361\uff0c\u4e3a\u5b9e\u73b0\u91cf\u5b50\u6781\u9650\u653e\u5927\u5668\u3001\u9ad8\u7075\u654f\u5ea6\u4f20\u611f\u5668\u4ee5\u53ca\u63a2\u7d22\u91cf\u5b50\u975e\u7ebf\u6027\u529b\u5b66\u52a8\u529b\u5b66\u63d0\u4f9b\u4e86\u53ef\u884c\u5e73\u53f0\u3002"}}
{"id": "2601.18458", "categories": ["cond-mat.stat-mech", "cond-mat.mes-hall", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.18458", "abs": "https://arxiv.org/abs/2601.18458", "authors": ["Katha Ganguly", "Bijay Kumar Agarwalla"], "title": "Measurement induced faster symmetry restoration in quantum trajectories", "comment": "13 pages including end matter and supplementary material, 8 figures", "summary": "Continuous measurement of quantum systems provides a standard route to quantum trajectories through the successive acquisition of information which further results in measurement back-action. In this work, we harness this back-action as a resource for global $U(1)$ symmetry restoration where continuous measurement is combined with a $U(1)$-preserving unitary evolution. Starting from a $U(1)$ symmetry-broken initial state, we simulate quantum trajectories generated by continuous measurements of both global and local observables. We show that under global monitoring, states containing superpositions of distant charge sectors restore symmetry faster than those involving nearby sectors. We establish the universality of this behavior across different measurement protocols. Finally, we demonstrate that local monitoring can further accelerate symmetry restoration for certain states that relax slowly under global monitoring.", "AI": {"tldr": "\u672c\u6587\u5229\u7528\u8fde\u7eed\u6d4b\u91cf\u7684\u53cd\u4f5c\u7528\u4f5c\u4e3a\u8d44\u6e90\uff0c\u901a\u8fc7\u7ed3\u5408U(1)\u4fdd\u6301\u7684\u5e7a\u6b63\u6f14\u5316\uff0c\u5b9e\u73b0\u5168\u5c40U(1)\u5bf9\u79f0\u6027\u6062\u590d\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u5168\u5c40\u76d1\u6d4b\u4e0b\uff0c\u5305\u542b\u8fdc\u8ddd\u79bb\u7535\u8377\u533a\u7684\u53e0\u52a0\u6001\u6bd4\u8fd1\u8ddd\u79bb\u533a\u7684\u6001\u6062\u590d\u66f4\u5feb\uff0c\u4e14\u8be5\u73b0\u8c61\u5728\u4e0d\u540c\u6d4b\u91cf\u534f\u8bae\u4e0b\u5177\u6709\u666e\u9002\u6027\u3002\u5c40\u90e8\u76d1\u6d4b\u53ef\u8fdb\u4e00\u6b65\u52a0\u901f\u67d0\u4e9b\u5728\u5168\u5c40\u76d1\u6d4b\u4e0b\u6062\u590d\u7f13\u6162\u7684\u6001\u7684\u5bf9\u79f0\u6027\u6062\u590d\u3002", "motivation": "\u8fde\u7eed\u6d4b\u91cf\u4f1a\u5bfc\u81f4\u91cf\u5b50\u8f68\u8ff9\u548c\u6d4b\u91cf\u53cd\u4f5c\u7528\u3002\u672c\u5de5\u4f5c\u7684\u52a8\u673a\u662f\u5c06\u8fd9\u79cd\u6d4b\u91cf\u53cd\u4f5c\u7528\u4f5c\u4e3a\u4e00\u79cd\u8d44\u6e90\uff0c\u7528\u4e8e\u6062\u590d\u5168\u5c40U(1)\u5bf9\u79f0\u6027\u3002", "method": "\u5c06\u8fde\u7eed\u6d4b\u91cf\u4e0eU(1)\u4fdd\u6301\u7684\u5e7a\u6b63\u6f14\u5316\u76f8\u7ed3\u5408\uff0c\u4eceU(1)\u5bf9\u79f0\u6027\u7834\u7f3a\u7684\u521d\u59cb\u6001\u51fa\u53d1\uff0c\u6a21\u62df\u7531\u5168\u5c40\u548c\u5c40\u90e8\u89c2\u6d4b\u91cf\u8fde\u7eed\u6d4b\u91cf\u4ea7\u751f\u7684\u91cf\u5b50\u8f68\u8ff9\u3002", "result": "\u5728\u5168\u5c40\u76d1\u6d4b\u4e0b\uff0c\u5305\u542b\u8fdc\u8ddd\u79bb\u7535\u8377\u533a\u53e0\u52a0\u7684\u6001\u6bd4\u8fd1\u8ddd\u79bb\u533a\u7684\u6001\u66f4\u5feb\u6062\u590d\u5bf9\u79f0\u6027\uff1b\u8be5\u884c\u4e3a\u5728\u4e0d\u540c\u6d4b\u91cf\u534f\u8bae\u4e0b\u5177\u6709\u666e\u9002\u6027\uff1b\u5c40\u90e8\u76d1\u6d4b\u80fd\u8fdb\u4e00\u6b65\u52a0\u901f\u67d0\u4e9b\u5728\u5168\u5c40\u76d1\u6d4b\u4e0b\u6062\u590d\u7f13\u6162\u7684\u6001\u7684\u5bf9\u79f0\u6027\u6062\u590d\u3002", "conclusion": "\u6d4b\u91cf\u53cd\u4f5c\u7528\u53ef\u4ee5\u6709\u6548\u5730\u7528\u4e8e\u6062\u590dU(1)\u5bf9\u79f0\u6027\uff0c\u4e14\u901a\u8fc7\u9009\u62e9\u5408\u9002\u7684\u6d4b\u91cf\u65b9\u5f0f\uff08\u5168\u5c40\u6216\u5c40\u90e8\uff09\u53ef\u4ee5\u4f18\u5316\u6062\u590d\u901f\u5ea6\u3002"}}
{"id": "2601.18508", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2601.18508", "abs": "https://arxiv.org/abs/2601.18508", "authors": ["Yiping Wang", "Gillian E. Minarik", "Weijie Li", "Yves Kwan", "Shuai Yuan", "Eric Anderson", "Chaowei Hu", "Julian Ingham", "Jeongheon Choe", "Takashi Taniguchi", "Kenji Watanabe", "Xavier Roy", "Jiun-Haw Chu", "Raquel Queiroz", "James C. Hone", "N. Regnault", "Xiaodong Xu", "Xiaoyang Zhu"], "title": "Magnetic Signatures of a Putative Fractional Topological Insulator in Twisted MoTe2", "comment": "42 pages, 5 figures, and 13 SI figures", "summary": "The interplay among electronic correlation, topology, and time-reversal-symmetry (TRS) often leads to exotic quantum states of matter. Primary examples include the recently realized fractional Chern insulators (FCIs) in twisted MoTe2 bilayers (tMoTe2) and multilayer graphene aligned with hBN, where TRS is broken in partially filled flat moire Chern bands. Among the FCIs in tMoTe2, the most robust is at a hole filling of v = -2/3 per moire unit cell. Interestingly, transient optical sensing and more recent transport measurements revealed a correlated state at v = -4/3, twice the filling factor for the v = -2/3 FCI. Here, employing pump-probe circular dichroism (CD) measurements on tMoTe2 with twist angles = 3.9 degree and 3.7 degree, we find that the v = -4/3 state exhibits vanishing magnetization (m = 0) in finite windows of out-of-plane magnetic field less than ~2-4 mT, and a first order phase transition to + - m states at higher fields. This out-of-plane antiferromagnetic (AFM) like behavior is notably absent for all other correlated states and disappears for the v = -4/3 state at higher or lower twist angles = 4.0 degree and 3.3 degree. The observed magnetic signature at v = -4/3 is consistent with a predicted fractional topological insulator (FTI) with TRS, consisting of two copies of -2/3 FCIs with opposite chiralities. We support these findings with calculations in the interacting continuum model of tMoTe2. Our work presents a candidate for fractional topological insulators with TRS.", "AI": {"tldr": "This paper reports experimental evidence of a fractional topological insulator with time-reversal symmetry at filling factor v = -4/3 in twisted MoTe2 bilayers, showing unique vanishing magnetization and antiferromagnetic-like behavior under out-of-plane magnetic fields, which disappears at slightly different twist angles.", "motivation": "To investigate exotic quantum states arising from electronic correlation, topology, and time-reversal-symmetry (TRS), specifically the nature of a correlated state at v = -4/3 in twisted MoTe2 bilayers that is twice the filling factor of the known v = -2/3 fractional Chern insulator, and to determine if it represents a predicted TRS-preserving fractional topological insulator.", "method": "Employed pump-probe circular dichroism (CD) measurements on twisted MoTe2 bilayers with twist angles of 3.9\u00b0 and 3.7\u00b0, supported by theoretical calculations using an interacting continuum model of tMoTe2.", "result": "The v = -4/3 state exhibits vanishing magnetization (m = 0) in finite windows of out-of-plane magnetic field below ~2-4 mT, and a first-order phase transition to \u00b1m states at higher fields. This out-of-plane antiferromagnetic-like behavior is unique to v = -4/3 and disappears at twist angles of 4.0\u00b0 and 3.3\u00b0. The magnetic signature matches predictions for a fractional topological insulator with TRS comprising two copies of -2/3 FCIs with opposite chiralities.", "conclusion": "The work identifies a candidate fractional topological insulator with time-reversal symmetry at v = -4/3 in twisted MoTe2 bilayers at twist angles of approximately 3.7-3.9\u00b0."}}
{"id": "2601.17592", "categories": ["quant-ph", "cond-mat.stat-mech", "physics.atom-ph"], "pdf": "https://arxiv.org/pdf/2601.17592", "abs": "https://arxiv.org/abs/2601.17592", "authors": ["Sivaprasad Omanakuttan", "Tyler Thurtell", "Andrew K. Forbes", "Vikas Buchemmavari", "Ben Q. Baragiola"], "title": "Holstein Primakoff spin codes for local and collective noise", "comment": "Comments and suggestions are Welcome", "summary": "Quantum error correction is essential for fault-tolerant quantum computation, yet most existing codes rely on local control and stabilizer measurements that are difficult to implement in systems dominated by collective interactions. Inspired by spin-GKP codes in PhysRevA.108.022428, we develop a general framework for Holstein-Primakoff spin codes, which maps continuous-variable bosonic codes onto permutation-symmetric spin ensembles via the Holstein-Primakoff approximation. We show that HP codes are robust to both collective and local-spin noise and propose an explicit measurement-free local error recovery procedure to map local noise into correctable collective-spin errors.", "AI": {"tldr": "\u63d0\u51faHolstein-Primakoff\u81ea\u65cb\u7801\u6846\u67b6\uff0c\u5c06\u73bb\u8272\u7801\u6620\u5c04\u5230\u5bf9\u79f0\u81ea\u65cb\u7cfb\u7efc\uff0c\u5b9e\u73b0\u6297\u96c6\u4f53\u548c\u5c40\u57df\u566a\u58f0\u7684\u91cf\u5b50\u7ea0\u9519\uff0c\u65e0\u9700\u6d4b\u91cf\u5373\u53ef\u5c06\u5c40\u57df\u9519\u8bef\u8f6c\u5316\u4e3a\u53ef\u7ea0\u6b63\u7684\u96c6\u4f53\u9519\u8bef", "motivation": "\u73b0\u6709\u91cf\u5b50\u7ea0\u9519\u7801\u4f9d\u8d56\u5c40\u57df\u63a7\u5236\u548c\u7a33\u5b9a\u5b50\u6d4b\u91cf\uff0c\u96be\u4ee5\u5728\u96c6\u4f53\u76f8\u4e92\u4f5c\u7528\u4e3b\u5bfc\u7684\u7cfb\u7edf\u4e2d\u5b9e\u73b0\uff0c\u9700\u8981\u9002\u914d\u96c6\u4f53\u566a\u58f0\u573a\u666f\u7684\u65b0\u7f16\u7801\u65b9\u6848", "method": "\u57fa\u4e8eHolstein-Primakoff\u8fd1\u4f3c\uff0c\u5efa\u7acb\u4ece\u8fde\u7eed\u53d8\u91cf\u73bb\u8272\u7801\u5230\u7f6e\u6362\u5bf9\u79f0\u81ea\u65cb\u7cfb\u7efc\u7684\u6620\u5c04\u6846\u67b6\uff0c\u6784\u5efaHP\u81ea\u65cb\u7801", "result": "HP\u7801\u5bf9\u96c6\u4f53\u566a\u58f0\u548c\u5c40\u57df\u81ea\u65cb\u566a\u58f0\u5747\u5177\u9c81\u68d2\u6027\uff0c\u5e76\u63d0\u51fa\u65e0\u6d4b\u91cf\u7684\u5c40\u57df\u9519\u8bef\u6062\u590d\u534f\u8bae\uff0c\u53ef\u5c06\u5c40\u57df\u566a\u58f0\u8f6c\u5316\u4e3a\u53ef\u7ea0\u6b63\u7684\u96c6\u4f53\u81ea\u65cb\u9519\u8bef", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u96c6\u4f53\u76f8\u4e92\u4f5c\u7528\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u7ea0\u9519\u65b9\u6848\uff0c\u964d\u4f4e\u4e86\u6d4b\u91cf\u96be\u5ea6\uff0c\u62d3\u5c55\u4e86\u73bb\u8272\u7801\u5728\u81ea\u65cb\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528"}}
{"id": "2601.18614", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2601.18614", "abs": "https://arxiv.org/abs/2601.18614", "authors": ["Moallison F. Cavalcante"], "title": "Edge States Effects in Quantum Work Statistics", "comment": null, "summary": "Motivated by the objective of quantifying the energetic cost of accessing boundary phases through local control, we investigate here a simple, analytically tractable quantum impurity model. This model exhibits a rich boundary phase diagram, characterized by phases with different numbers of edge states. By considering a local quench protocol that drives the system out of equilibrium, we calculate exactly the resulting quantum work distribution across these phases. Our results show that the presence of edge states strongly alters this distribution. In particular, we analytically determine key fingerprints of these states both near the low-energy threshold and in the high-energy region.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u91cf\u5b50\u6742\u8d28\u6a21\u578b\u548c\u5c40\u57df\u6dec\u706b\u534f\u8bae\uff0c\u7cbe\u786e\u8ba1\u7b97\u4e86\u7cfb\u7edf\u5728\u975e\u5e73\u8861\u6001\u4e0b\u7684\u91cf\u5b50\u529f\u5206\u5e03\uff0c\u63ed\u793a\u4e86\u8fb9\u754c\u8fb9\u7f18\u6001\u5bf9\u529f\u5206\u5e03\u7684\u663e\u8457\u5f71\u54cd\u53ca\u5176\u5728\u4f4e\u80fd\u548c\u9ad8\u80fd\u533a\u7684\u7279\u5f81\u6307\u7eb9\u3002", "motivation": "\u91cf\u5316\u901a\u8fc7\u5c40\u57df\u63a7\u5236\u8bbf\u95ee\u8fb9\u754c\u76f8\u6240\u9700\u7684\u80fd\u91cf\u4ee3\u4ef7\uff0c\u63a2\u7d22\u8fb9\u7f18\u6001\u4e0e\u91cf\u5b50\u529f\u7edf\u8ba1\u7684\u5173\u8054\u3002", "method": "\u91c7\u7528\u89e3\u6790\u53ef\u89e3\u7684\u91cf\u5b50\u6742\u8d28\u6a21\u578b\uff0c\u8bbe\u8ba1\u5c40\u57df\u6dec\u706b\u534f\u8bae\u9a71\u52a8\u7cfb\u7edf\u8fdc\u79bb\u5e73\u8861\u6001\uff0c\u5e76\u7cbe\u786e\u8ba1\u7b97\u91cf\u5b50\u529f\u5206\u5e03\u3002", "result": "\u8fb9\u7f18\u6001\u663e\u8457\u6539\u53d8\u91cf\u5b50\u529f\u5206\u5e03\uff0c\u5728\u4f4e\u80fd\u9608\u503c\u548c\u9ad8\u80fd\u533a\u5448\u73b0\u53ef\u8bc6\u522b\u7684\u7279\u5f81\u6307\u7eb9\u3002", "conclusion": "\u91cf\u5b50\u529f\u5206\u5e03\u53ef\u4f5c\u4e3a\u63a2\u6d4b\u8fb9\u754c\u76f8\u548c\u8fb9\u7f18\u6001\u7684\u6709\u6548\u975e\u5e73\u8861\u8bca\u65ad\u5de5\u5177\u3002"}}
{"id": "2601.17659", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.17659", "abs": "https://arxiv.org/abs/2601.17659", "authors": ["Shan Gao"], "title": "Generalized Aharonov-Bohm Effect", "comment": "12 pages, no figures", "summary": "The Aharonov-Bohm (AB) effect highlights the fundamental role of electromagnetic potentials in quantum mechanics, manifesting as a phase shift for a charged particle in field-free regions. While well-established for static magnetic fluxes, the effect's behavior under time-varying fluxes remains an open and debated question. Employing the WKB method, we derive the AB phase shift for a time-dependent magnetic vector potential, demonstrating that for circular paths in the quasistatic regime, it is proportional to the time-averaged enclosed magnetic flux, \\(\u0394\u03c6_{\\rm AB} = \\frac{1}{T} \\int_0^T e \u03a6(t) \\, dt\\), with the total phase shift, including kinetic contributions, equaling \\(e \u03a6(0)\\). For non-circular paths, the phase shift depends on both the flux history and path geometry, revealing the effect's hybrid nature involving gauge potentials and induced electric fields. We verify the consistency of our gauge choice with Maxwell's equations and discuss the implications for local versus nonlocal interpretations of the AB effect. We also generalize the results to scenarios with nonzero external magnetic fields, where the enclosed flux is through the actual electron paths, and for circular paths of radius $R$, the AB phase shift is also proportional to the time average of the enclosed flux \\(\u03a6_{\\rm enc}(R,t)\\), with the total phase shift depending only on the initial enclosed flux \\(e \u03a6_{\\rm enc}(R,0)\\); for general non-circular paths, the external magnetic field affects trajectories and phase accumulation through the Lorentz force, leading to additional path dependence. These findings clarify the role of gauge-dependent potentials and induced fields in the generalized AB effect, offering new theoretical insights and potential applications in quantum technologies.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528WKB\u65b9\u6cd5\u63ed\u793a\u4e86\u65f6\u95f4\u4f9d\u8d56\u78c1\u901a\u91cf\u4e0bAharonov-Bohm\u6548\u5e94\u7684\u65b0\u884c\u4e3a\uff1a\u5728\u51c6\u9759\u6001\u8fd1\u4f3c\u4e0b\uff0c\u5706\u5f62\u8def\u5f84\u7684\u76f8\u4f4d\u79fb\u52a8\u4e0e\u65f6\u95f4\u5e73\u5747\u78c1\u901a\u6210\u6b63\u6bd4\uff0c\u800c\u975e\u5706\u5f62\u8def\u5f84\u5219\u8868\u73b0\u51fa\u78c1\u901a\u5386\u53f2\u548c\u51e0\u4f55\u8def\u5f84\u7684\u6df7\u5408\u4f9d\u8d56\u6027\uff0c\u6f84\u6e05\u4e86\u89c4\u8303\u52bf\u4e0e\u611f\u5e94\u7535\u573a\u7684\u4f5c\u7528\u673a\u5236\u3002", "motivation": "\u867d\u7136Aharonov-Bohm\u6548\u5e94\u5728\u9759\u6001\u78c1\u901a\u4e0b\u5df2\u5f97\u5230\u5145\u5206\u9a8c\u8bc1\uff0c\u4f46\u5176\u5728\u65f6\u53d8\u78c1\u901a\u6761\u4ef6\u4e0b\u7684\u884c\u4e3a\u4ecd\u662f\u4e00\u4e2a\u60ac\u800c\u672a\u51b3\u4e14\u6709\u4e89\u8bae\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u7406\u8bba\u6f84\u6e05\u3002", "method": "\u91c7\u7528WKB\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u63a8\u5bfc\u65f6\u95f4\u4f9d\u8d56\u78c1\u77e2\u52bf\u4e0b\u7684Aharonov-Bohm\u76f8\u4f4d\u79fb\u52a8\uff0c\u5e76\u9a8c\u8bc1\u89c4\u8303\u9009\u62e9\u4e0e\u9ea6\u514b\u65af\u97e6\u65b9\u7a0b\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5bf9\u4e8e\u51c6\u9759\u6001\u4e0b\u7684\u5706\u5f62\u8def\u5f84\uff0cAB\u76f8\u4f4d\u79fb\u52a8\u0394\u03c6_AB = (1/T)\u222b\u2080\u1d40 e\u03a6(t)dt\uff0c\u603b\u76f8\u4f4d\u79fb\u52a8\uff08\u542b\u52a8\u529b\u5b66\u8d21\u732e\uff09\u7b49\u4e8ee\u03a6(0)\uff1b\u975e\u5706\u5f62\u8def\u5f84\u7684\u76f8\u4f4d\u79fb\u52a8\u540c\u65f6\u4f9d\u8d56\u4e8e\u78c1\u901a\u5386\u53f2\u548c\u8def\u5f84\u51e0\u4f55\uff0c\u4f53\u73b0\u89c4\u8303\u52bf\u4e0e\u611f\u5e94\u7535\u573a\u7684\u6df7\u5408\u4f5c\u7528\uff1b\u63a8\u5e7f\u5230\u975e\u96f6\u5916\u78c1\u573a\u65f6\uff0c\u5706\u5f62\u8def\u5f84\u7684\u76f8\u4f4d\u79fb\u52a8\u4e0e\u65f6\u95f4\u5e73\u5747\u5305\u56f4\u78c1\u901a\u6210\u6b63\u6bd4\uff0c\u603b\u76f8\u4f4d\u4ec5\u53d6\u51b3\u4e8e\u521d\u59cb\u78c1\u901ae\u03a6_enc(R,0)\uff0c\u800c\u4e00\u822c\u8def\u5f84\u53d7\u6d1b\u4f26\u5179\u529b\u5f71\u54cd\u4ea7\u751f\u989d\u5916\u8def\u5f84\u4f9d\u8d56\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u660e\u786e\u4e86\u65f6\u53d8\u6761\u4ef6\u4e0b\u89c4\u8303\u4f9d\u8d56\u52bf\u4e0e\u611f\u5e94\u573a\u5728\u5e7f\u4e49AB\u6548\u5e94\u4e2d\u7684\u4f5c\u7528\uff0c\u4e3a\u91cf\u5b50\u6280\u672f\u4e2d\u7684\u76f8\u4f4d\u8c03\u63a7\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u89c1\u89e3\uff0c\u5e76\u6df1\u5316\u4e86\u5bf9AB\u6548\u5e94\u5c40\u57df\u4e0e\u975e\u5c40\u57df\u89e3\u91ca\u7684\u7406\u89e3\u3002"}}
{"id": "2601.18648", "categories": ["cond-mat.str-el", "cond-mat.stat-mech", "hep-lat"], "pdf": "https://arxiv.org/pdf/2601.18648", "abs": "https://arxiv.org/abs/2601.18648", "authors": ["Jin-Xiang Hao", "Zheng Zhu", "Yang Qi"], "title": "Multi-target density matrix renormalization group for 3D CFTs on the fuzzy sphere", "comment": null, "summary": "The fuzzy sphere regularization provides a powerful framework for studying three-dimensional (3D) conformal field theories (CFTs) by mapping them onto numerically tractable lattice models on the spherical lowest Landau level. However, the system sizes accessible to this method have been limited by the exact diagonalization (ED). In this work, we transcend this limitation by combining the fuzzy sphere regularization with a sophisticated multi-target density matrix renormalization group (DMRG) algorithm. Focusing on the 3D Ising-type model on the spherical lowest Landau level, we calculate the 24 low-lying energies at a larger system size than previously feasible with ED. At criticality, we extract the scaling dimensions of six primary operators, and the results show significantly improved agreement with bootstrap benchmarks compared to previous ED results at smaller sizes. Our approach allows us to efficiently target multiple excited states in larger systems beyond the reach of exact diagonalization. This study establishes the fuzzy sphere regularization combined with advanced DMRG techniques as a powerful and general framework for precision physics in 3D CFTs.", "AI": {"tldr": "\u7ed3\u5408\u6a21\u7cca\u7403\u6b63\u5219\u5316\u4e0e\u591a\u76ee\u6807DMRG\u7b97\u6cd5\uff0c\u7a81\u7834\u7cbe\u786e\u5bf9\u89d2\u5316\u9650\u5236\uff0c\u5728\u66f4\u5927\u4f53\u7cfb\u5c3a\u5bf8\u4e0b\u7814\u7a763D\u5171\u5f62\u573a\u8bba\uff0c\u663e\u8457\u63d0\u5347\u4e0ebootstrap\u57fa\u51c6\u7684\u543b\u5408\u5ea6", "motivation": "\u6a21\u7cca\u7403\u6b63\u5219\u5316\u4e3a\u7814\u7a76\u4e09\u7ef4\u5171\u5f62\u573a\u8bba\u63d0\u4f9b\u4e86\u5f3a\u5927\u6846\u67b6\uff0c\u4f46\u53d7\u9650\u4e8e\u7cbe\u786e\u5bf9\u89d2\u5316\u65b9\u6cd5\u53ef\u5904\u7406\u7684\u4f53\u7cfb\u5c3a\u5bf8", "method": "\u5c06\u6a21\u7cca\u7403\u6b63\u5219\u5316\u4e0e\u591a\u76ee\u6807\u5bc6\u5ea6\u77e9\u9635\u91cd\u6b63\u5316\u7fa4(DMRG)\u7b97\u6cd5\u7ed3\u5408\uff0c\u7814\u7a76\u7403\u5f62\u6700\u4f4e\u6717\u9053\u80fd\u7ea7\u4e0a\u7684\u4e09\u7ef4\u4f0a\u8f9b\u6a21\u578b", "result": "\u6210\u529f\u8ba1\u7b97\u4e86\u6bd4\u4ee5\u5f80\u7cbe\u786e\u5bf9\u89d2\u5316\u66f4\u5927\u4f53\u7cfb\u5c3a\u5bf8\u4e0b\u768424\u4e2a\u4f4e\u80fd\u6001\uff0c\u5728\u4e34\u754c\u70b9\u63d0\u53d6\u4e86\u516d\u4e2a\u521d\u7ea7\u7b97\u7b26\u7684\u6807\u5ea6\u7ef4\u6570\uff0c\u7ed3\u679c\u4e0ebootstrap\u57fa\u51c6\u7684\u543b\u5408\u5ea6\u663e\u8457\u4f18\u4e8e\u5c0f\u5c3a\u5bf8\u4e0b\u7684\u7cbe\u786e\u5bf9\u89d2\u5316\u7ed3\u679c\uff0c\u5e76\u80fd\u9ad8\u6548\u8ba1\u7b97\u5927\u4f53\u7cfb\u4e2d\u7684\u591a\u4e2a\u6fc0\u53d1\u6001", "conclusion": "\u6a21\u7cca\u7403\u6b63\u5219\u5316\u4e0e\u5148\u8fdbDMRG\u6280\u672f\u76f8\u7ed3\u5408\uff0c\u4e3a\u4e09\u7ef4\u5171\u5f62\u573a\u8bba\u7684\u9ad8\u7cbe\u5ea6\u7269\u7406\u7814\u7a76\u5efa\u7acb\u4e86\u5f3a\u5927\u4e14\u666e\u9002\u7684\u6846\u67b6"}}
{"id": "2601.17662", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.17662", "abs": "https://arxiv.org/abs/2601.17662", "authors": ["Shan Gao"], "title": "From Joint to Single-System Psi-Onticity Without Preparation Independence", "comment": "12 pages, no figures", "summary": "The Pusey-Barrett-Rudolph (PBR) theorem establishes $\u03c8$-onticity for individual quantum systems, but its standard formulation relies on the Preparation Independence Postulate (PIP). This has led to a prevalent view that rejecting PIP leaves open the possibility of $\u03c8$-epistemic models for individual systems. In this work, we show that this understanding is incomplete: once the PBR theorem establishes $\u03c8$-onticity for composite systems prepared in product states, the $\u03c8$-onticity of the individual subsystems follows directly from the tensor-product structure of quantum mechanics, without invoking PIP or any further auxiliary assumptions. This result removes a key auxiliary assumption from the PBR theorem, closes a persistent loophole for preserving $\u03c8$-epistemic models, and strengthens the conceptual foundations of $\u03c8$-ontology.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\uff1aPBR\u5b9a\u7406\u5bf9\u590d\u5408\u7cfb\u7edf\u03c8-\u672c\u4f53\u6027\u7684\u8bc1\u660e\uff0c\u65e0\u9700PIP\u5047\u8bbe\u5373\u53ef\u901a\u8fc7\u91cf\u5b50\u529b\u5b66\u5f20\u91cf\u79ef\u7ed3\u6784\u76f4\u63a5\u63a8\u51fa\u5b50\u7cfb\u7edf\u03c8-\u672c\u4f53\u6027\uff0c\u4ece\u800c\u5835\u4f4f\u4e86\u03c8-\u8ba4\u77e5\u6a21\u578b\u7684\u6f0f\u6d1e\u3002", "motivation": "\u6311\u6218\"\u62d2\u7edd\u5236\u5907\u72ec\u7acb\u6027\u5047\u8bbe(PIP)\u5c31\u80fd\u4fdd\u6301\u03c8-\u8ba4\u77e5\u6a21\u578b\u53ef\u80fd\u6027\"\u7684\u4e3b\u6d41\u89c2\u70b9\uff0c\u6307\u51fa\u8be5\u7406\u89e3\u5b58\u5728\u6839\u672c\u6027\u4e0d\u5b8c\u6574\uff0c\u65e8\u5728\u6d88\u9664PBR\u5b9a\u7406\u4e2d\u8fd9\u4e00\u957f\u671f\u5b58\u5728\u7684\u7406\u8bba\u6f0f\u6d1e\u3002", "method": "\u91c7\u7528\u7406\u8bba\u63a8\u5bfc\u65b9\u6cd5\uff0c\u57fa\u4e8e\u91cf\u5b50\u529b\u5b66\u7684\u5f20\u91cf\u79ef\u7ed3\u6784\uff0c\u4ece\u590d\u5408\u7cfb\u7edf\u4ea7\u54c1\u6001\u7684\u03c8-\u672c\u4f53\u6027\u76f4\u63a5\u903b\u8f91\u63a8\u51fa\u5b50\u7cfb\u7edf\u03c8-\u672c\u4f53\u6027\uff0c\u7ed5\u8fc7PIP\u5047\u8bbe\u3002", "result": "\u53d1\u73b0\u5b50\u7cfb\u7edf\u03c8-\u672c\u4f53\u6027\u4e0d\u4f9d\u8d56\u4e8ePIP\uff0c\u800c\u662f\u76f4\u63a5\u6e90\u4e8e\u91cf\u5b50\u529b\u5b66\u7684\u57fa\u672c\u7ed3\u6784\uff0c\u4ece\u800c\u65e0\u9700\u4efb\u4f55\u989d\u5916\u8f85\u52a9\u5047\u8bbe\u5373\u53ef\u786e\u7acb\u3002", "conclusion": "\u8be5\u53d1\u73b0\u7b80\u5316\u4e86PBR\u5b9a\u7406\u7684\u5047\u8bbe\u4f53\u7cfb\uff0c\u5f7b\u5e95\u5173\u95ed\u4e86\u901a\u8fc7\u62d2\u7eddPIP\u6765\u7ef4\u62a4\u03c8-\u8ba4\u77e5\u6a21\u578b\u7684\u8def\u5f84\uff0c\u4e3a\u03c8-\u672c\u4f53\u8bba\u63d0\u4f9b\u4e86\u66f4\u7a33\u56fa\u7684\u6982\u5ff5\u57fa\u7840\u3002"}}
{"id": "2601.18737", "categories": ["cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2601.18737", "abs": "https://arxiv.org/abs/2601.18737", "authors": ["In\u00e9s Corte", "Federico Holik", "Lorena Reb\u00f3n", "Flavia A. G\u00f3mez Albarrac\u00edn"], "title": "Quantum skyrmions in the antiferromagnetic triangular lattice", "comment": "11 pages, 12 figures", "summary": "Magnetic skyrmions are topological quasiparticles potentially useful for memory and computing devices. Antiferromagnetic (AF) skyrmions present no transverse deflection, making them suitable candidates for data storage applications. After the discovery of skyrmions with length scales comparable to the lattice constant, several works presented quantum analogues of classical ferromagnetic skyrmions in spin systems. However, studies about quantum analogues of AF skyrmions are still lacking. Here, we explore the phases of the AF quantum spin-1/2 Heisenberg model with Dzyaloshinskii-Moriya interactions on the triangular lattice using the density matrix renormalization group (DMRG) algorithm. We study the magnetization profile, spin structure factor and quantum entanglement of the resulting ground states to characterize the corresponding phases and signal the emergence of quantum AF skyrmions. Our results support that three-sublattice quantum antiferromagnetic skyrmion textures are stabilized in a wide range of magnetic fields.", "AI": {"tldr": "Researchers used DMRG simulations to discover stable quantum antiferromagnetic skyrmions in a spin-1/2 triangular lattice model, demonstrating their potential for spintronics applications.", "motivation": "To investigate quantum analogues of antiferromagnetic skyrmions, which are theoretically promising for data storage due to lacking transverse deflection, but have not been previously studied despite existing research on quantum ferromagnetic skyrmions.", "method": "Density matrix renormalization group (DMRG) simulations of an antiferromagnetic quantum spin-1/2 Heisenberg model with Dzyaloshinskii-Moriya interactions on a triangular lattice, with analysis via magnetization profiles, spin structure factors, and quantum entanglement measures.", "result": "The DMRG calculations reveal that three-sublattice quantum antiferromagnetic skyrmion textures emerge as stable ground states across a broad range of applied magnetic fields.", "conclusion": "This work establishes the existence of quantum antiferromagnetic skyrmions, providing a foundation for developing quantum spintronic devices based on topological spin textures."}}
{"id": "2601.17665", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.17665", "abs": "https://arxiv.org/abs/2601.17665", "authors": ["Shan Gao"], "title": "Comment on \"Aharonov-Bohm Phase is Locally Generated Like All Other Quantum Phases\"", "comment": "Comment on arXiv:1906.03440", "summary": "Marletto and Vedral [Phys. Rev. Lett. 125, 040401 (2020)] propose that the Aharonov-Bohm (AB) phase is locally mediated by entanglement between a charged particle and the quantized electromagnetic field, asserting gauge independence for non-closed paths. In this Comment, we critically analyze their model and demonstrate that the AB phase arises from the interaction with the vector potential \\(\\mathbf{A}\\), not from entanglement, which is a byproduct of the quantum electrodynamics (QED) framework. We show that their field-based energy formulation, intended to reflect local electromagnetic interactions, is mathematically flawed due to an incorrect prefactor and yields \\( +q \\mathbf{v} \\cdot \\mathbf{A}_{\\mathbf{s}} \\) in the Coulomb gauge, conflicting with QED's \\( -q \\mathbf{v} \\cdot \\mathbf{A}_{\\mathbf{s}} \\). This equivalence to \\( q \\mathbf{v} \\cdot \\mathbf{A}_{\\mathbf{s}} \\) holds only approximately in the Coulomb gauge under static conditions, failing for time-dependent fields and other gauges, undermining their claim of a gauge-independent local mechanism. Furthermore, we confirm that the AB phase is gauge-dependent for non-closed paths, contradicting their assertion. Our analysis reaffirms the conventional explanation in the semi-classical picture, where the AB phase is driven by the vector potential \\(\\mathbf{A}\\), with entanglement playing no causal role in its generation.", "AI": {"tldr": "\u672c\u6587\u8bc4\u8bba\u5e76\u53cd\u9a73\u4e86Marletto\u548cVedral\u5173\u4e8eAharonov-Bohm\u76f8\u7531\u7ea0\u7f20\u5c40\u90e8\u4ecb\u5bfc\u7684\u4e3b\u5f20\uff0c\u901a\u8fc7\u6570\u5b66\u5206\u6790\u6307\u51fa\u5176\u6a21\u578b\u7684\u9519\u8bef\uff0c\u5e76\u91cd\u7533\u8be5\u76f8\u4f4d\u4e8e\u4f20\u7edf\u91cf\u5b50\u7535\u52a8\u529b\u5b66\u4e2d\u7531\u77e2\u91cf\u52bfA\u9a71\u52a8\uff0c\u800c\u975e\u7ea0\u7f20\u6240\u81f4\u3002", "motivation": "\u7ea0\u6b63\u5bf9Aharonov-Bohm\u6548\u5e94\u7684\u8bef\u89e3\uff0c\u6311\u6218\u539f\u8bba\u6587\u4e2d\"\u7ea0\u7f20\u662fAB\u76f8\u56e0\u679c\u673a\u5236\"\u7684\u89c2\u70b9\uff0c\u7ef4\u62a4\u91cf\u5b50\u7535\u52a8\u529b\u5b66\u4e2d\u77e2\u91cf\u52bfA\u7684\u6838\u5fc3\u5730\u4f4d\u3002", "method": "\u5bf9\u539f\u6a21\u578b\u8fdb\u884c\u6279\u5224\u6027\u6570\u5b66\u5206\u6790\uff0c\u68c0\u9a8c\u5176\u57fa\u4e8e\u573a\u7684\u80fd\u91cf\u8868\u8fbe\u5f0f\uff0c\u9a8c\u8bc1\u89c4\u8303\u72ec\u7acb\u6027\u58f0\u660e\uff0c\u5e76\u4e0e\u6807\u51c6\u91cf\u5b50\u7535\u52a8\u529b\u5b66\u9884\u6d4b\u5bf9\u6bd4\u3002", "result": "\u53d1\u73b0\u539f\u6a21\u578b\u5b58\u5728\u6570\u5b66\u7f3a\u9677\uff1a\u524d\u7f6e\u56e0\u5b50\u9519\u8bef\u5bfc\u81f4Coulomb\u89c4\u8303\u4e0b\u51fa\u73b0+qv\u00b7A_s\u800c\u975e\u6b63\u786e\u7684-qv\u00b7A_s\uff1b\u8be5\u7b49\u4ef7\u6027\u4ec5\u5728\u9759\u6001Coulomb\u89c4\u8303\u4e0b\u8fd1\u4f3c\u6210\u7acb\uff0c\u4e0d\u9002\u7528\u4e8e\u65f6\u53d8\u573a\u6216\u5176\u4ed6\u89c4\u8303\uff1b\u975e\u95ed\u5408\u8def\u5f84\u7684AB\u76f8\u5177\u6709\u89c4\u8303\u4f9d\u8d56\u6027\u3002", "conclusion": "AB\u76f8\u5e76\u975e\u7531\u7ea0\u7f20\u4ea7\u751f\uff0c\u800c\u662f\u6e90\u4e8e\u5e26\u7535\u7c92\u5b50\u4e0e\u77e2\u91cf\u52bfA\u7684\u76f8\u4e92\u4f5c\u7528\uff1b\u7ea0\u7f20\u4ec5\u662f\u91cf\u5b50\u7535\u52a8\u529b\u5b66\u7684\u526f\u4ea7\u54c1\uff0c\u4e0d\u8d77\u56e0\u679c\u4f5c\u7528\uff1b\u4f20\u7edf\u7684\u534a\u7ecf\u5178\u89e3\u91ca\u5f97\u5230\u786e\u8ba4\u3002"}}
{"id": "2601.18756", "categories": ["quant-ph", "cond-mat.stat-mech", "cond-mat.str-el", "hep-lat", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2601.18756", "abs": "https://arxiv.org/abs/2601.18756", "authors": ["Marko Male\u017ei\u010d", "Johann Ostmeyer"], "title": "Efficient Trotter-Suzuki Schemes for Long-time Quantum Dynamics", "comment": "27 + 7 pages, 11 figures, 4 tables and 1 animation", "summary": "Accurately simulating long-time dynamics of many-body systems is a challenge in both classical and quantum computing due to the accumulation of Trotter errors. While low-order Trotter-Suzuki decompositions are straightforward to implement, their rapidly growing error limits access to long-time observables. We present a framework for constructing efficient high-order Trotter-Suzuki schemes by identifying their structure and directly optimizing their parameters over a high-dimensional space. This method enables the discovery of new schemes with significantly improved efficiency compared to traditional constructions, such as those by Suzuki and Yoshida. Based on the theoretical efficiency and practical performance, we recommend two novel highly efficient schemes at $4^{\\textrm{th}}$ and $6^{\\textrm{th}}$ order. We also demonstrate the effectiveness of these decompositions on the Heisenberg model and the quantum harmonic oscillator, and find that for a fixed final time they perform better across the computational cost. Even when using large time steps, they surpass established low-order schemes like the Leapfrog. Finally, we investigate the in-practice performance of different Trotter schemes and find the decompositions with more uniform coefficients tend to feature improved error accumulation over long times. We have included this observation into our choice of recommended schemes.", "AI": {"tldr": "The paper presents a framework for optimizing high-order Trotter-Suzuki decompositions by parameter optimization, discovering new 4th and 6th order schemes that significantly outperform traditional low-order methods in simulating long-time many-body system dynamics.", "motivation": "Low-order Trotter-Suzuki decompositions have rapidly growing errors that limit long-time simulations of many-body systems, despite being straightforward to implement. There's a need for more efficient high-order schemes.", "method": "The authors developed a framework that identifies the structure of Trotter-Suzuki schemes and directly optimizes their parameters over a high-dimensional space to construct efficient high-order decompositions.", "result": "Discovered new schemes with significantly improved efficiency over traditional Suzuki and Yoshida constructions; recommended two novel schemes at 4th and 6th order; demonstrated effectiveness on Heisenberg model and quantum harmonic oscillator; outperformed low-order schemes like Leapfrog even with large time steps; found that decompositions with more uniform coefficients feature improved error accumulation.", "conclusion": "The proposed optimization framework successfully creates highly efficient high-order Trotter-Suzuki schemes, particularly those with uniform coefficients, making long-time many-body dynamics simulations more feasible with reduced error accumulation."}}
{"id": "2601.18718", "categories": ["quant-ph", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2601.18718", "abs": "https://arxiv.org/abs/2601.18718", "authors": ["Taisanul Haque"], "title": "Nontrivial bounds on extractable energy in quantum energy teleportation for gapped manybody systems with a unique ground state", "comment": "4 pages + Under review in Physics Letters A", "summary": "We establish a universal, exponentially decaying upper bound on the average energy that can be extracted in quantum energy teleportation (QET) protocols executed on finite-range gapped lattice systems possessing a unique ground state. Under mild regularity assumptions on the Hamiltonian and uniform operator-norm bounds on the local measurement operators, there exist positive constants $C$ and $\u03bc$ (determined by the spectral gap, interaction range and local operator norms) such that for any local measurement performed in a region $A$ and any outcome-dependent local unitaries implemented in a disjoint region $B$ separated by distance $d=\\operatorname{dist}(A,B)$ one has $|E_A-E_B|\\le C\\,e^{-\u03bcd}.$ The bound is nonperturbative, explicit up to model-dependent constants, and follows from the variational characterization of the ground state combined with exponential clustering implied by the spectral gap.", "AI": {"tldr": "This paper proves that energy extraction in quantum energy teleportation decays exponentially with distance between measurement and operation regions in gapped lattice systems, establishing a fundamental spatial limitation.", "motivation": "Understanding fundamental limits of energy manipulation in quantum systems is crucial for quantum information processing and thermodynamics. Quantum energy teleportation (QET) protocols require quantification of how far energy can be effectively extracted, which this work addresses for realistic lattice systems.", "method": "The authors employ a nonperturbative approach using the variational characterization of ground states combined with exponential clustering properties guaranteed by the spectral gap. They assume mild regularity conditions on the Hamiltonian and uniform operator-norm bounds on local measurements.", "result": "They establish a universal bound: |E_A-E_B| \u2264 C e^(-\u03bcd), where d is the distance between regions A (measurement) and B (unitary operations), and C, \u03bc are positive constants determined by the spectral gap, interaction range, and local operator norms. This bound is explicit up to model-dependent constants.", "conclusion": "The work demonstrates that quantum energy teleportation faces an intrinsic exponential distance limitation in gapped systems, providing fundamental constraints for protocol design and revealing that long-range energy extraction becomes exponentially suppressed, which has significant implications for quantum thermodynamic device engineering."}}
{"id": "2601.17675", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.17675", "abs": "https://arxiv.org/abs/2601.17675", "authors": ["Sangil Kwon", "Daisuke Hoshi", "Toshiaki Nagase", "Daichi Sugiyama", "Hiroto Mukai", "Kengo Takemura", "Rintaro Kojima", "Yu Zhou", "Shohei Watabe", "Fumiki Yoshihara", "Jaw-Shen Tsai"], "title": "Realisation of Protected Cat Qutrit via Engineered Quantum Tunnelling", "comment": null, "summary": "Engineering quantum tunnelling in phase space has emerged as a viable method for creating a protected qubit with biased-noise properties. A promising approach is to combine a Kerr nonlinearity with multi-photon transitions, resulting in a system known as a Kerr parametric oscillator (KPO). In this work, we implement a three-photon KPO and explore its potential as a protected qutrit. We confirm quantum coherence by demonstrating three-photon Rabi oscillations and performing direct Wigner function measurements that reveal three-component cat-like states. We observe breathing-like dynamics in phase space, arising from exotic temporal interference between the qutrit and excited states. The frequency of this interference corresponds to the energy gap between the qutrit and excited manifolds, thereby providing an experimental hallmark of qutrit space protection. We also identify a higher-order pump term as the main mechanism suppressing photon occupation; mitigating this term is necessary to maximize protection. Our findings elucidate the basic quantum properties of the three-photon KPO and establish the first step toward its use as an alternative qutrit platform.", "AI": {"tldr": "Researchers implemented a three-photon Kerr parametric oscillator (KPO) as a protected qutrit, demonstrating quantum coherence through Rabi oscillations and Wigner function measurements, observing breathing dynamics that reveal protection mechanisms, and identifying ways to improve photon occupation suppression.", "motivation": "To create protected qubits/qutrits with biased-noise properties using engineered quantum tunnelling in phase space, specifically exploring multi-photon transitions combined with Kerr nonlinearity as an alternative to conventional qubit platforms.", "method": "Implemented a three-photon Kerr parametric oscillator (KPO) system and conducted experiments including three-photon Rabi oscillations and direct Wigner function measurements to characterize quantum coherence and state properties.", "result": "Demonstrated quantum coherence with three-photon Rabi oscillations, observed three-component cat-like states via Wigner functions, discovered breathing-like phase space dynamics from qutrit-excited state interference (frequency matching energy gap as protection hallmark), and identified higher-order pump terms as the main mechanism limiting photon occupation.", "conclusion": "The three-photon KPO shows basic quantum properties suitable for protected qutrit operation; mitigating identified pump terms is necessary to maximize protection, establishing this as a viable first step toward an alternative qutrit platform."}}
{"id": "2601.17686", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.17686", "abs": "https://arxiv.org/abs/2601.17686", "authors": ["Vicky Choi"], "title": "Exponential Quantum Speedup on Structured Hard Instances of Maximum Independent Set", "comment": "18 pages, 8 figures, 2 tables", "summary": "Establishing quantum speedup for computationally hard problems of practical relevance, particularly combinatorial optimization problems, remains a central challenge in quantum computation. In this work, we identify a structurally defined family of classically hard maximum independent set (MIS) instances, and design and analyze a non-stoquastic adiabatic quantum optimization algorithm that exploits this structure. The algorithm runs in polynomial time and achieves an exponential speedup over both transverse-field quantum annealing and state-of-the-art classical solvers on these instances, under assumptions supported by analytical and numerical evidence. We identify the essential quantum mechanism enabling the speedup as the use of a non-stoquastic XX-driver to access a larger sign-structured admissible subspace beyond the stoquastic regime, which allows sign-generating quantum interference to create smooth evolution paths that bypass tunneling. This identifies a distinctive quantum mechanism underlying the speedup and explains why no efficient classical analogue is likely to exist. In addition, our analysis produces scalable small-scale models, derived from our structural reduction, that capture the essential dynamics of the algorithm. These models provide a concrete opportunity for verification of the quantum advantage mechanism on currently available universal quantum computers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u975estoquastic\u7edd\u70ed\u91cf\u5b50\u7b97\u6cd5\uff0c\u5728\u7ed3\u6784\u5316\u6700\u5927\u72ec\u7acb\u96c6\u95ee\u9898\u4e0a\u5b9e\u73b0\u6307\u6570\u7ea7\u52a0\u901f\uff0c\u5229\u7528XX-driver\u4ea7\u751f\u91cf\u5b50\u5e72\u6d89\u7ed5\u8fc7\u96a7\u7a7f\uff0c\u5e76\u63d0\u4f9b\u53ef\u5728\u5f53\u524d\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u9a8c\u8bc1\u7684\u5c0f\u89c4\u6a21\u6a21\u578b\u3002", "motivation": "\u4e3a\u5b9e\u9645\u76f8\u5173\u7684\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u5efa\u7acb\u91cf\u5b50\u52a0\u901f\u662f\u91cf\u5b50\u8ba1\u7b97\u9886\u57df\u7684\u6838\u5fc3\u6311\u6218\u3002", "method": "\u8bc6\u522b\u4e00\u7c7b\u7ed3\u6784\u5316\u7684\u7ecf\u5178\u56f0\u96be\u6700\u5927\u72ec\u7acb\u96c6\u5b9e\u4f8b\uff0c\u8bbe\u8ba1\u5e76\u5206\u6790\u5229\u7528\u8be5\u7ed3\u6784\u7684\u975estoquastic\u7edd\u70ed\u91cf\u5b50\u4f18\u5316\u7b97\u6cd5\u3002", "result": "\u8be5\u7b97\u6cd5\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u8fd0\u884c\uff0c\u76f8\u5bf9\u4e8e\u6a2a\u573a\u91cf\u5b50\u9000\u706b\u548c\u7ecf\u5178\u6700\u4f18\u6c42\u89e3\u5668\u5b9e\u73b0\u6307\u6570\u7ea7\u52a0\u901f\u3002\u5173\u952e\u673a\u5236\u662f\u975estoquastic XX-driver\u901a\u8fc7\u7b26\u53f7\u751f\u6210\u91cf\u5b50\u5e72\u6d89\u521b\u5efa\u5e73\u6ed1\u6f14\u5316\u8def\u5f84\u4ee5\u7ed5\u8fc7\u96a7\u7a7f\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5c0f\u89c4\u6a21\u6a21\u578b\u7528\u4e8e\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "conclusion": "\u975estoquastic\u9a71\u52a8\u901a\u8fc7\u91cf\u5b50\u5e72\u6d89\u5b9e\u73b0\u91cf\u5b50\u52a0\u901f\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e0d\u592a\u53ef\u80fd\u5b58\u5728\u9ad8\u6548\u7ecf\u5178\u6a21\u62df\u7684\u91cf\u5b50\u4f18\u52bf\u673a\u5236\uff0c\u5e76\u4e3a\u5f53\u524d\u91cf\u5b50\u8ba1\u7b97\u673a\u7684\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u5177\u4f53\u6a21\u578b\u3002"}}
{"id": "2601.17688", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.17688", "abs": "https://arxiv.org/abs/2601.17688", "authors": ["Sudhanva Joshi", "Sunil Kumar Mishra"], "title": "Traversability dynamics of minimal Sachdev-Ye-Kitaev Wormhole-inspired teleportation protocol with a parity-time ($\\mathcal{PT}$)-symmetric non-Hermitian deformation", "comment": "13 pages, 7 figures. To be communicated to a Journal", "summary": "Holography-inspired teleportation has recently emerged as a significant area of research in quantum many-body systems. In this work, we investigate the effects of $\\mathcal{PT}$ symmetric non-unitary deformations on the traversability of the wormhole-inspired teleportation protocol modeled by coupled Sachdev-Ye-Kitaev systems prepared in a Thermofield Double state bath. By introducing balanced gain and loss terms to the boundary Hamiltonians, we identify a phase transition driven by spectral exceptional points, where the real energy eigenvalues of the effective Hamiltonian coalesce and bifurcate into complex conjugate pairs. We demonstrate that the $\\mathcal{PT}$-broken phase acts as an amplifier, enabling exponential growth in the norm of the teleported signal while preserving the causal time window for the wormhole's traversability. A statistical study of disorder realizations reveals that the critical non-Hermiticity threshold $\u03b3_c$ follows a log-normal distribution, reflecting the sensitivity of the transition to the microscopic level spacing of the chaotic SYK spectrum. Furthermore, we observe a ``Purification\" effect deep in the broken phase, where the teleportation channel acts as an entanglement distiller, yielding near-perfect teleportation fidelity for post-selected states. Our results suggest that the non-Hermitian topology can be harnessed to enhance holographic quantum communication, providing a robust mechanism for signal amplification in noisy, minimal quantum many-body systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8PT\u5bf9\u79f0\u975e\u5e7a\u6b63\u5f62\u53d8\u5bf9\u57fa\u4e8eSYK\u6a21\u578b\u7684\u5168\u606f\u9690\u5f62\u4f20\u6001\u4e2d\u866b\u6d1e\u53ef\u7a7f\u8d8a\u6027\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u5f15\u5165\u589e\u76ca/\u635f\u8017\uff0c\u53d1\u73b0\u8c31\u5947\u5f02\u70b9\u9a71\u52a8\u7684\u76f8\u53d8\uff0cPT\u7834\u7f3a\u76f8\u53ef\u6307\u6570\u653e\u5927\u4fe1\u53f7\u5e76\u4fdd\u6301\u56e0\u679c\u6027\u3002\u4e34\u754c\u9608\u503c\u5448\u5bf9\u6570\u6b63\u6001\u5206\u5e03\uff0c\u6df1\u5904\u5b58\u5728\u7eaf\u5316\u6548\u5e94\uff0c\u5b9e\u73b0\u8fd1\u5b8c\u7f8e\u4fdd\u771f\u5ea6\u3002", "motivation": "\u63a2\u7d22\u5229\u7528\u975e\u5384\u7c73\u62d3\u6251\u589e\u5f3a\u5168\u606f\u91cf\u5b50\u901a\u4fe1\u7684\u9c81\u68d2\u6027\u673a\u5236\uff0c\u89e3\u51b3\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u4e2d\u4fe1\u53f7\u653e\u5927\u4e0e\u566a\u58f0\u95ee\u9898\uff0c\u5e76\u7814\u7a76\u866b\u6d1e\u53ef\u7a7f\u8d8a\u6027\u7684\u8c03\u63a7\u65b9\u5f0f\u3002", "method": "\u91c7\u7528\u8026\u5408Sachdev-Ye-Kitaev\u7cfb\u7edf\u4f5c\u4e3a\u5168\u606f\u9690\u5f62\u4f20\u6001\u6a21\u578b\uff0c\u5236\u5907\u4e8e\u70ed\u573a\u53cc\u6001\u6d74\u4e2d\uff0c\u5411\u8fb9\u754c\u54c8\u5bc6\u987f\u91cf\u5f15\u5165\u5e73\u8861\u7684\u589e\u76ca\u548c\u635f\u8017\u9879\uff0c\u5206\u6790\u6709\u6548\u54c8\u5bc6\u987f\u91cf\u7684\u80fd\u8c31\u6f14\u5316\u3001\u5947\u5f02\u70b9\u548c\u975e\u5e7a\u6b63\u52a8\u529b\u5b66\u884c\u4e3a\uff0c\u5e76\u8fdb\u884c\u65e0\u5e8f\u5b9e\u73b0\u7684\u7edf\u8ba1\u7814\u7a76\u3002", "result": "1. \u5728\u8fb9\u754c\u54c8\u5bc6\u987f\u91cf\u4e2d\u5f15\u5165\u5e73\u8861\u589e\u76ca/\u635f\u8017\u540e\uff0c\u7cfb\u7edf\u51fa\u73b0\u7531\u8c31\u5947\u5f02\u70b9\u9a71\u52a8\u7684\u975e\u5384\u7c73\u76f8\u53d8\uff0c\u5b9e\u80fd\u91cf\u672c\u5f81\u503c\u5728\u5947\u5f02\u70b9\u5904\u5408\u5e76\u4e3a\u590d\u5171\u8f6d\u5bf9\uff1b2. PT\u7834\u7f3a\u76f8\u8d77\u5230\u653e\u5927\u5668\u4f5c\u7528\uff0c\u4f7f\u9690\u5f62\u4f20\u6001\u4fe1\u53f7\u6307\u6570\u589e\u957f\uff0c\u540c\u65f6\u4fdd\u6301\u866b\u6d1e\u53ef\u7a7f\u8d8a\u7684\u56e0\u679c\u65f6\u95f4\u7a97\uff1b3. \u4e34\u754c\u975e\u5384\u7c73\u9608\u503c\u03b3_c\u670d\u4ece\u5bf9\u6570\u6b63\u6001\u5206\u5e03\uff0c\u8868\u660e\u76f8\u53d8\u5bf9SYK\u8c31\u5fae\u89c2\u80fd\u7ea7\u95f4\u8ddd\u9ad8\u5ea6\u654f\u611f\uff1b4. \u5728PT\u7834\u7f3a\u76f8\u6df1\u5904\u89c2\u5bdf\u5230\"\u7eaf\u5316\"\u6548\u5e94\uff0c\u8be5\u4f20\u6001\u901a\u9053\u53ef\u4f5c\u4e3a\u7ea0\u7f20\u84b8\u998f\u5668\uff0c\u5bf9\u540e\u9009\u62e9\u6001\u5b9e\u73b0\u8fd1\u5b8c\u7f8e\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u975e\u5384\u7c73\u62d3\u6251\u7ed3\u6784\u53ef\u7528\u4e8e\u589e\u5f3a\u5168\u606f\u91cf\u5b50\u901a\u4fe1\uff0c\u4e3a\u542b\u566a\u3001\u6700\u5c0f\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u4e2d\u7684\u4fe1\u53f7\u653e\u5927\u63d0\u4f9b\u9c81\u68d2\u673a\u5236\uff0c\u8fd9\u5bf9\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u548c\u91cf\u5b50\u5f15\u529b\u7814\u7a76\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2601.17725", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.17725", "abs": "https://arxiv.org/abs/2601.17725", "authors": ["Eunok Bae", "Jeonghyeon Shin", "Minjin Choi"], "title": "Reducing Circuit Resources in Grover's Algorithm via Constraint-Aware Initialization", "comment": "11 pages, 6 figures", "summary": "Grover's search algorithm provides a quadratic speedup over classical brute-force search in terms of query complexity and is widely used as a versatile subroutine in numerous quantum algorithms, including those for combinatorial problems with large search spaces. For such problems, it is natural to reduce the effective search space by incorporating problem constraints at the initialization step, which in Grover's algorithm can be achieved by preparing structured initial states that encode constraint information. In this work, we present a systematic framework with a simple preprocessing procedure for constraint-aware initialization in Grover's algorithm, focusing on problems with linear constraints. While such structured initial states can reduce the number of oracle queries required to obtain a solution, their preparation incurs additional circuit-level costs. We therefore offer a conservative circuit-level resource analysis, showing that the resulting constraint-aware initialization can improve resource efficiency in terms of gate counts and circuit depth. The validity of the framework is further demonstrated numerically using the exact-cover problem. Overall, our results indicate that this approach serves as a practical baseline for achieving more resource-efficient implementations of Grover's algorithm compared to the standard uniform initialization.", "AI": {"tldr": "\u63d0\u51fa\u7ea6\u675f\u611f\u77e5\u521d\u59cb\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u521d\u59cb\u6001\u51cf\u5c11Grover\u7b97\u6cd5\u641c\u7d22\u7a7a\u95f4\uff0c\u5728\u7cbe\u786e\u8986\u76d6\u95ee\u9898\u4e2d\u9a8c\u8bc1\u5176\u8d44\u6e90\u6548\u7387\u4f18\u4e8e\u6807\u51c6\u5747\u5300\u521d\u59cb\u5316", "motivation": "Grover\u7b97\u6cd5\u867d\u63d0\u4f9b\u4e8c\u6b21\u52a0\u901f\uff0c\u4f46\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u7684\u5927\u641c\u7d22\u7a7a\u95f4\u4ecd\u5b58\u6311\u6218\uff1b\u901a\u8fc7\u521d\u59cb\u6001\u7f16\u7801\u7ea6\u675f\u53ef\u7f29\u5c0f\u6709\u6548\u641c\u7d22\u7a7a\u95f4\uff0c\u9700\u5e73\u8861\u67e5\u8be2\u590d\u6742\u5ea6\u4e0e\u7535\u8def\u5f00\u9500", "method": "\u5efa\u7acb\u7ebf\u6027\u7ea6\u675f\u7684\u9884\u5904\u7406\u6846\u67b6\uff0c\u751f\u6210\u7f16\u7801\u7ea6\u675f\u7684\u7ed3\u6784\u5316\u521d\u59cb\u6001\uff0c\u5e76\u8fdb\u884c\u4fdd\u5b88\u7684\u7535\u8def\u7ea7\u8d44\u6e90\u5206\u6790\uff08\u95e8\u6570\u91cf/\u6df1\u5ea6\uff09", "result": "\u7ea6\u675f\u611f\u77e5\u521d\u59cb\u5316\u5728\u7cbe\u786e\u8986\u76d6\u95ee\u9898\u4e2d\u964d\u4f4eoracle\u67e5\u8be2\u6b21\u6570\uff0c\u4e14\u7535\u8def\u7ea7\u8d44\u6e90\uff08\u95e8\u6570/\u6df1\u5ea6\uff09\u6548\u7387\u4f18\u4e8e\u6807\u51c6\u521d\u59cb\u5316", "conclusion": "\u8be5\u6846\u67b6\u4e3aGrover\u7b97\u6cd5\u63d0\u4f9b\u4e86\u8d44\u6e90\u66f4\u9ad8\u6548\u5b9e\u73b0\u7684\u5b9e\u7528\u57fa\u7ebf\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u7ea6\u675f\u7ec4\u5408\u4f18\u5316\u95ee\u9898"}}
{"id": "2601.17732", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.17732", "abs": "https://arxiv.org/abs/2601.17732", "authors": ["Harriet Apel", "Burak \u015eahino\u011flu"], "title": "Quantum fast-forwarding fermion-boson interactions via the polaron transform", "comment": null, "summary": "Simulating interactions between fermions and bosons is central to understanding correlated phenomena, yet these systems are inherently difficult to treat classically. Previous quantum algorithms for fermion-boson models exhibit computation costs that scale polynomially with the bosonic truncation parameter, $\u039b$. In this work we identify the efficient unitary transformation enabling fast-forwarded evolution of the fermion-boson interaction term, yielding an interaction-picture based simulation algorithm with complexity polylogarithmic in $\u039b$. We apply this transformation to explicitly construct an efficient quantum algorithm for the Hubbard-Holstein model and discuss its generalisation to other fermion-boson interacting models. This approach yields an important asymptotic improvement in the dependence on the bosonic cutoff and establishes that, for certain models, fermion-boson interactions can be simulated with resources comparable to those required for purely fermionic systems.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u578b\u91cf\u5b50\u7b97\u6cd5\uff0c\u901a\u8fc7\u9ad8\u6548\u9149\u53d8\u6362\u5b9e\u73b0\u8d39\u7c73\u5b50-\u73bb\u8272\u5b50\u76f8\u4e92\u4f5c\u7528\u9879\u7684\u5feb\u901f\u6f14\u5316\uff0c\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u4ece\u591a\u9879\u5f0f\u7ea7\u964d\u81f3\u039b\u7684\u591a\u5bf9\u6570\u7ea7\uff08polylogarithmic\uff09\uff0c\u5728Hubbard-Holstein\u6a21\u578b\u4e0a\u9a8c\u8bc1\u5e76\u63a8\u5e7f\u81f3\u5176\u4ed6\u6a21\u578b\u3002", "motivation": "\u8d39\u7c73\u5b50-\u73bb\u8272\u5b50\u76f8\u4e92\u4f5c\u7528\u4f53\u7cfb\u5bf9\u7406\u89e3\u5173\u8054\u73b0\u8c61\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7ecf\u5178\u6a21\u62df\u6781\u5176\u56f0\u96be\uff1b\u73b0\u6709\u91cf\u5b50\u7b97\u6cd5\u8ba1\u7b97\u6210\u672c\u968f\u73bb\u8272\u5b50\u622a\u65ad\u53c2\u6570\u039b\u5448\u591a\u9879\u5f0f\u589e\u957f\uff0c\u5b58\u5728\u6548\u7387\u74f6\u9888\u3002", "method": "\u8bc6\u522b\u5e76\u5e94\u7528\u9ad8\u6548\u9149\u53d8\u6362\u5b9e\u73b0\u76f8\u4e92\u4f5c\u7528\u9879\u7684\"\u5feb\u901f\u524d\u5411\u6f14\u5316\"\uff0c\u57fa\u4e8e\u76f8\u4e92\u4f5c\u7528\u56fe\u50cf\u8bbe\u8ba1\u65b0\u91cf\u5b50\u6a21\u62df\u7b97\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u5bf9\u039b\u7684\u4f9d\u8d56\u3002", "result": "\u65b0\u7b97\u6cd5\u590d\u6742\u5ea6\u4e3apolylogarithmic in \u039b\uff08\u539f\u65b9\u6cd5\u4e3apolynomial in \u039b\uff09\uff0c\u5728Hubbard-Holstein\u6a21\u578b\u4e2d\u5b9e\u73b0\u9ad8\u6548\u6a21\u62df\uff0c\u5e76\u63a8\u5e7f\u81f3\u5176\u4ed6\u8d39\u7c73\u5b50-\u73bb\u8272\u5b50\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u73bb\u8272\u5b50\u622a\u65ad\u53c2\u6570\u4f9d\u8d56\u4e0a\u5b9e\u73b0\u6e10\u8fdb\u6027\u7a81\u7834\uff0c\u8bc1\u660e\u7279\u5b9a\u6a21\u578b\u7684\u8d39\u7c73\u5b50-\u73bb\u8272\u5b50\u76f8\u4e92\u4f5c\u7528\u53ef\u7528\u63a5\u8fd1\u7eaf\u8d39\u7c73\u5b50\u7cfb\u7edf\u7684\u8d44\u6e90\u8fdb\u884c\u6a21\u62df\uff0c\u5177\u6709\u91cd\u8981\u7406\u8bba\u4ef7\u503c\u3002"}}
{"id": "2601.17757", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.17757", "abs": "https://arxiv.org/abs/2601.17757", "authors": ["Haipeng Xie", "Nobuyuki Yoshioka", "Kento Tsubouchi", "Ying Li"], "title": "Simple, Efficient, and Generic Post-Selection Decoding for qLDPC codes", "comment": null, "summary": "Quantum error correction is indispensable for scalable quantum computation. Although encoding logical qubits substantially enhances noise resilience, achieving logical error rates low enough for practical algorithms remains challenging on existing hardware. Here we introduce argument reweighting, a simple and broadly applicable post-selection decoding strategy that boosts the performance of maximum-likelihood-type decoders, including minimum-weight perfect matching and belief-propagation families. The method suppresses logical errors by performing additional decoding rounds under reweighted error models, enabling acceptance of high-confidence syndrome outcomes. Circuit-level simulations across multiple decoders and qLDPC codes show that argument reweighting substantially suppresses logical errors, requiring a rejection rate of only $1.44\\times10^{-5}$ to reduce the logical error rate by almost two orders of magnitude for the $[[144,12,12]]$ bivariate bicycle code. These results establish argument reweighting as a practical and resource-efficient approach for enhancing quantum fault tolerance.", "AI": {"tldr": "A post-selection decoding strategy called argument reweighting that significantly improves quantum error correction performance by reweighting error models during decoding, reducing logical error rates by nearly two orders of magnitude with minimal rejection.", "motivation": "Quantum error correction is essential for scalable quantum computation, but achieving sufficiently low logical error rates for practical algorithms remains challenging on existing hardware despite encoding logical qubits.", "method": "Argument reweighting - a simple, broadly applicable post-selection decoding strategy that boosts maximum-likelihood-type decoders (including minimum-weight perfect matching and belief-propagation) by performing additional decoding rounds under reweighted error models to accept high-confidence syndrome outcomes.", "result": "Circuit-level simulations show the method substantially suppresses logical errors across multiple decoders and qLDPC codes; for the [[144,12,12]] bivariate bicycle code, a rejection rate of only 1.44\u00d710\u207b\u2075 reduces logical error rate by almost two orders of magnitude.", "conclusion": "Argument reweighting is a practical and resource-efficient approach for enhancing quantum fault tolerance in quantum computing."}}
{"id": "2601.17788", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.17788", "abs": "https://arxiv.org/abs/2601.17788", "authors": ["Bo Zhang", "Yusuf Turek"], "title": "Kirkwood-Dirac Quasiprobability as a Universal Framework for Quantum Measurements Across All Regimes", "comment": "9 pages, 2 figures", "summary": "The question of when the Kirkwood-Dirac quasiprobability serves as the most appropriate description for quantum measurements has remained unresolved, particularly across different measurement strengths. While known to generate anomalous weak values in the weak measurement regime and to reduce to classical probabilities under projective measurement, the physical mechanism governing its continuous transformation has been lacking. Here we demonstrate that the KD quasiprobability provides a general framework for all measurement regimes by identifying pointer-induced decoherence as the universal mechanism controlling this transition. We show that the decoherence factor F(t) simultaneously quantifies the loss of quantum coherence and interpolates the measurement strength from weak to strong. Within this framework, the KD quasiprobability naturally deforms from its full complex form-governing weak values-to the real, non-negative Wigner formula describing projective measurements, while maintaining informational completeness throughout the transition. Our work resolves the fundamental question of the KD distribution's applicability by establishing it as the universal framework that seamlessly connects all quantum measurement regimes through a physically transparent decoherence pathway.", "AI": {"tldr": "\u8be5\u7814\u7a76\u89e3\u51b3\u4e86Kirkwood-Dirac\u51c6\u6982\u7387\u5728\u91cf\u5b50\u6d4b\u91cf\u4e2d\u7684\u9002\u7528\u6027\u95ee\u9898\uff0c\u8bc1\u5b9e\u5176\u53ef\u4f5c\u4e3a\u6240\u6709\u6d4b\u91cf\u5f3a\u5ea6\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5176\u4e2d\u6307\u9488\u8bf1\u5bfc\u9000\u76f8\u5e72\u63a7\u5236\u7740\u4ece\u5f31\u6d4b\u91cf\u5230\u5f3a\u6d4b\u91cf\u7684\u8fde\u7eed\u8f6c\u53d8\u3002", "motivation": "\u957f\u671f\u4ee5\u6765\uff0cKirkwood-Dirac\u51c6\u6982\u7387\u5728\u4f55\u65f6\u6700\u9002\u7528\u4e8e\u63cf\u8ff0\u91cf\u5b50\u6d4b\u91cf\u8fd9\u4e00\u95ee\u9898\u60ac\u800c\u672a\u51b3\uff0c\u7279\u522b\u662f\u5728\u4e0d\u540c\u6d4b\u91cf\u5f3a\u5ea6\u4e0b\u7684\u7269\u7406\u673a\u5236\u4e0d\u660e\u3002", "method": "\u63d0\u51fa\u6307\u9488\u8bf1\u5bfc\u9000\u76f8\u5e72\u662f\u63a7\u5236\u6d4b\u91cf\u5f3a\u5ea6\u8fde\u7eed\u8f6c\u53d8\u7684\u666e\u9002\u673a\u5236\uff0c\u6784\u5efaKD\u51c6\u6982\u7387\u7684\u7edf\u4e00\u6846\u67b6\u3002", "result": "\u9000\u76f8\u5e72\u56e0\u5b50F(t)\u540c\u65f6\u91cf\u5316\u76f8\u5e72\u6027\u635f\u5931\u5e76\u5185\u63d2\u6d4b\u91cf\u5f3a\u5ea6\uff0c\u4f7fKD\u51c6\u6982\u7387\u4ece\u5f31\u6d4b\u91cf\u7684\u590d\u6570\u5f62\u5f0f\u81ea\u7136\u6f14\u5316\u4e3a\u6295\u5f71\u6d4b\u91cf\u7684\u5b9e\u6570Wigner\u51fd\u6570\uff0c\u5168\u7a0b\u4fdd\u6301\u4fe1\u606f\u5b8c\u5907\u6027\u3002", "conclusion": "\u786e\u7acbKD\u5206\u5e03\u4e3a\u8fde\u63a5\u6240\u6709\u91cf\u5b50\u6d4b\u91cf\u533a\u57df\u7684\u666e\u9002\u6846\u67b6\uff0c\u901a\u8fc7\u9000\u76f8\u5e72\u8def\u5f84\u89e3\u51b3\u4e86\u5176\u9002\u7528\u6027\u7684\u6839\u672c\u95ee\u9898\u3002"}}
{"id": "2601.17804", "categories": ["quant-ph", "physics.app-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2601.17804", "abs": "https://arxiv.org/abs/2601.17804", "authors": ["Arman", "Prasanta K. Panigrahi"], "title": "Bosonic Diffusive Channel: Quantum Metrology via Finite Non-Gaussian Resource", "comment": null, "summary": "We investigate the estimation of dephasing-induced decoherence in continuous-variable quantum systems using non-Gaussian probe states. By purifying the open system, we identify optimal probes, specifically squeezed cat and symmetric squeezed compass states, via quantum Fisher information. These results are in agreement with numerical simulation. In settings where the intra-cavity field is inaccessible and standard measurements are impractical, utilizing an ancilla approach where a qubit traverses or interacts with the cavity field, leading to measurement of the qubit, hence allowing estimation of the dephasing rate via Wigner function reconstruction or less costly marginal distribution.", "AI": {"tldr": "This paper optimizes dephasing decoherence estimation in continuous-variable quantum systems using non-Gaussian probe states (squeezed cat/compass states) via quantum Fisher information, validated by simulations. An ancilla-based method enables cavity field estimation via qubit measurements.", "motivation": "To accurately estimate dephasing-induced decoherence in continuous-variable quantum systems, especially when direct cavity field measurements are impractical, by identifying optimal non-Gaussian probe states.", "method": "Purifying the open system to analyze decoherence, then using quantum Fisher information to identify optimal probes (squeezed cat states and symmetric squeezed compass states). For inaccessible cavities, an ancilla qubit traverses the cavity, enabling estimation via Wigner function reconstruction or marginal distribution measurements.", "result": "Optimal probes are squeezed cat and symmetric squeezed compass states, with results consistent with numerical simulations. The ancilla approach successfully estimates dephasing rates through indirect qubit measurements.", "conclusion": "Non-Gaussian probe states significantly enhance dephasing decoherence estimation in continuous-variable systems, and the ancilla method provides a viable alternative when direct field measurements are impossible."}}
{"id": "2601.17850", "categories": ["quant-ph", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.17850", "abs": "https://arxiv.org/abs/2601.17850", "authors": ["Andr\u00e9s F. Ducuara", "Erkka Haapasalo", "Ryo Takakura"], "title": "Multivariate R\u00e9nyi divergences characterise betting games with multiple lotteries", "comment": null, "summary": "We provide an operational interpretation of the multivariate R\u00e9nyi divergence in terms of economic-theoretic tasks based on betting, risk aversion, and multiple lotteries. We show that the multivariate R\u00e9nyi divergence $D_{\\underline\u03b1}(\\vec{P}_X)$ of probability distributions $\\vec{P}_X =(p^{(0)}_X,\\dots,p^{(d)}_X)$ and real-valued orders $\\underline\u03b1 = (\u03b1_0, \\dots, \u03b1_d)$ quantifies the economic-theoretic value that a rational agent assigns to $d$ lotteries with odds $o^{(k)}_X \\propto (p_X^{(k)})^{-1}$ ($k=1,\\dots,d$) on a random event described by $p^{(0)}_X$. In particular, when the odds are fair and the rational agent maximises over all betting strategies, the economic-theoretic value (the isoelastic certainty equivalent) that the agent assigns to the lotteries is exactly given by $w^{\\mathrm{ICE}}_{\\underline{R}}=\\exp[D_{\\underline\u03b1}(\\vec{P}_X)]$, where $\\underline{R}=(R_1,\\dots,R_d)$ is a risk-aversion vector with $R_k = 1+\u03b1_k/\u03b1_0$ being the risk-aversion parameter for lottery $k$. Furthermore, we introduce a new conditional multivariate R\u00e9nyi divergence that characterises a generalised scenario where the agent uses side information. We prove that this new quantity satisfies a data processing inequality which can be interpreted as the increment in the economic-theoretic value provided by side information; crucially, such a data processing inequality is a consequence of the agent's economic-theoretically consistent risk-averse attitude towards every lottery and vice versa. Finally, we apply these results to the resource theory of informative measurements in general probabilistic theories (GPTs). By establishing quantitative connections between information theory, physics, and economics, our framework provides a novel operational foundation for quantum state betting games with multiple lotteries in the realm of quantum resource theories.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4e3a\u591a\u5143R\u00e9nyi\u6563\u5ea6\u5efa\u7acb\u4e86\u57fa\u4e8e\u535a\u5f08\u8bba\u548c\u98ce\u9669\u89c4\u907f\u7684\u65b0\u64cd\u4f5c\u89e3\u91ca\uff0c\u8bc1\u660e\u5176\u7b49\u4e8e\u7406\u6027\u667a\u80fd\u4f53\u5728\u591a\u5f69\u7968\u6295\u6ce8\u573a\u666f\u4e2d\u7684\u7ecf\u6d4e\u4ef7\u503c\uff08\u6307\u6570\u5316\u786e\u5b9a\u6027\u7b49\u4ef7\uff09\uff0c\u5e76\u6269\u5c55\u5230\u542b\u8f85\u52a9\u4fe1\u606f\u7684\u6761\u4ef6\u6563\u5ea6\uff0c\u6700\u7ec8\u5e94\u7528\u4e8e\u91cf\u5b50\u8d44\u6e90\u7406\u8bba\u4e2d\u7684\u4fe1\u606f\u6d4b\u91cf\u95ee\u9898\u3002", "motivation": "\u5c06\u62bd\u8c61\u7684\u591a\u5143R\u00e9nyi\u6563\u5ea6\u4e0e\u4fe1\u606f\u8bba\u3001\u7ecf\u6d4e\u5b66\u5efa\u7acb\u53ef\u64cd\u4f5c\u7684\u8054\u7cfb\uff0c\u89e3\u51b3\u91cf\u5b50\u8d44\u6e90\u7406\u8bba\u4e2d\u591a\u5f69\u7968\u6295\u6ce8\u6e38\u620f\u7684\u64cd\u4f5c\u610f\u4e49\u95ee\u9898\uff0c\u4e3a\u91cf\u5b50\u4fe1\u606f\u5ea6\u91cf\u63d0\u4f9b\u7ecf\u6d4e\u5b66\u57fa\u7840\u3002", "method": "1. \u6784\u5efa\u57fa\u4e8e\u535a\u5f08\u8bba\u7684\u7ecf\u6d4e\u4efb\u52a1\u6a21\u578b\uff1a\u5b9a\u4e49\u7406\u6027\u667a\u80fd\u4f53\u5728\u591a\u5f69\u7968\u6295\u6ce8\u573a\u666f\u4e2d\u7684\u98ce\u9669\u89c4\u907f\u884c\u4e3a\uff1b2. \u5efa\u7acbR\u00e9nyi\u6563\u5ea6\u53c2\u6570\u4e0e\u98ce\u9669\u89c4\u907f\u7cfb\u6570\u7684\u6620\u5c04\uff08R_k = 1+\u03b1_k/\u03b1_0\uff09\uff1b3. \u63a8\u5bfc\u516c\u5e73\u8d54\u7387\u4e0b\u6700\u4f18\u6295\u6ce8\u7b56\u7565\u7684\u786e\u5b9a\u6027\u7b49\u4ef7\u4e0e\u6563\u5ea6\u7684\u6307\u6570\u5173\u7cfb\uff1b4. \u63d0\u51fa\u6761\u4ef6\u591a\u5143R\u00e9nyi\u6563\u5ea6\u5e76\u8bc1\u660e\u5176\u6570\u636e\u5904\u7406\u4e0d\u7b49\u5f0f\u3002", "result": "1. \u8bc1\u660e\u591a\u5143R\u00e9nyi\u6563\u5ea6D_\u03b1(P_X)\u7cbe\u786e\u91cf\u5316\u667a\u80fd\u4f53\u5bf9d\u4e2a\u5f69\u7968\u7684\u7ecf\u6d4e\u4ef7\u503c\uff0c\u4e14w^ICE_R = exp[D_\u03b1(P_X)]\uff1b2. \u65b0\u6761\u4ef6\u6563\u5ea6\u6ee1\u8db3\u6570\u636e\u5904\u7406\u4e0d\u7b49\u5f0f\uff0c\u5176\u503c\u7b49\u4e8e\u8f85\u52a9\u4fe1\u606f\u5e26\u6765\u7684\u7ecf\u6d4e\u4ef7\u503c\u589e\u91cf\uff1b3. \u8be5\u4e0d\u7b49\u5f0f\u4e0e\u98ce\u9669\u89c4\u907f\u6001\u5ea6\u4e92\u4e3a\u5145\u8981\u6761\u4ef6\uff1b4. \u6210\u529f\u5e94\u7528\u4e8e\u4e00\u822c\u6982\u7387\u7406\u8bba(GPTs)\u4e2d\u7684\u4fe1\u606f\u6d4b\u91cf\u8d44\u6e90\u7406\u8bba\u3002", "conclusion": "\u5efa\u7acb\u4e86\u4fe1\u606f\u8bba\u3001\u7ecf\u6d4e\u5b66\u4e0e\u91cf\u5b50\u7269\u7406\u7684\u5b9a\u91cf\u6865\u6881\uff0c\u4e3a\u91cf\u5b50\u8d44\u6e90\u7406\u8bba\u63d0\u4f9b\u4e86\u9996\u4e2a\u57fa\u4e8e\u535a\u5f08\u8bba\u7684\u591a\u5f69\u7968\u6295\u6ce8\u64cd\u4f5c\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u91cf\u5b50\u6001\u6295\u6ce8\u6e38\u620f\u7684\u7ecf\u6d4e\u5b66\u57fa\u7840\u95ee\u9898\uff0c\u62d3\u5c55\u4e86R\u00e9nyi\u6563\u5ea6\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2601.17856", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.17856", "abs": "https://arxiv.org/abs/2601.17856", "authors": ["S. E. Ennadifi"], "title": "On Tunneling in the Quantum Multiverse", "comment": "15 pages, 0 figure, 0 table, Hand-conducted work", "summary": "Prompted by the longstanding interpretational controversy in quantum mechanics, quantum tunneling is heuristically addressed within the Everettian quantum multiverse. In this framework, the universal wavefunction splits into decohered reflected and transmitted branches under the environmetal effect after encountring a potential barrier. The observed tunneling is then experienced by the observer located in a tunneled world. The tunneling probability and the tunneling time are investigated in terms of the tunneled world relative weights and the branching duration, respectively. The macroscopic quantum tunneling, recently honored, is also discussed and the corresponding macroscopic tunneling time is approached based on the obtained results and known data.", "AI": {"tldr": "This paper reinterprets quantum tunneling through the Many-Worlds Interpretation, where wavefunction splitting creates distinct \"tunneled\" and \"reflected\" worlds, allowing tunneling probability and time to be understood via branch weights and decoherence duration, extending to macroscopic scales.", "motivation": "The longstanding interpretational controversy in quantum mechanics surrounding quantum tunneling phenomena.", "method": "Heuristically addressing quantum tunneling within the Everettian quantum multiverse framework, where the universal wavefunction splits into decohered reflected and transmitted branches under environmental effects after encountering a potential barrier, with the observer experiencing tunneling in a tunneled world.", "result": "Investigates tunneling probability via tunneled world relative weights and tunneling time via branching duration; discusses macroscopic quantum tunneling and approaches macroscopic tunneling time using obtained results and known data.", "conclusion": "The Everettian multiverse framework offers a new perspective on quantum tunneling by interpreting it as an observer's experience within a decohered branch, enabling analysis of both microscopic and macroscopic tunneling through branch weights and decoherence timescales."}}
{"id": "2601.17876", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2601.17876", "abs": "https://arxiv.org/abs/2601.17876", "authors": ["Jie Zhao", "Zeliang Wu", "Haoran Liu", "Yueya Liu", "Xin Chen", "Xinyun Liang", "Wenfeng Huang", "Chun-Hua Yuan", "L. Q. Chen"], "title": "Coherent Amplifier-Empowered Quantum Interferometer: Preserving Sensitivity and Quantum Advantage under High Loss", "comment": "6 pages, 4 figures", "summary": "Quantum interferometers offer phase measurement capabilities that surpass the standard quantum limit (SQL), with phase sensitivity and quantum enhancement factor serving as key performance metrics. However, practical implementations face severe degradation of both metrics due to unavoidable losses, representing the foremost challenge in advancing quantum interferometry toward real-world applications. To address this challenge, we propose a coherent-amplifier-empowered quantum interferometer. The coherent amplifier dramatically suppresses the decay of both sensitivity and quantum enhancement under high-loss conditions, maintaining phase sensitivity beyond the original SQL even for losses exceeding 90%. Using an injected 4.2 dB squeezed-vacuum state in experimental demonstration, our scheme reduces the quantum enhancement degradation under 90% loss from 3.7 dB in a conventional quantum interferometer (CQI) to only 1.5 dB. More importantly, the phase sensitivity degradation under the same loss is limited to 4.0 dB, markedly outperforming the 11.2 dB degradation observed in a CQI. This improvement is enabled by the coherent amplifier's phase-sensitive photon amplification and its protection of the quantum state. This breakthrough in amplifier-empowered quantum interferometry overcomes the critical barrier to practical deployment, enabling robust quantum-enhanced measurements in lossy environments.", "AI": {"tldr": "\u63d0\u51fa\u76f8\u5e72\u653e\u5927\u5668\u589e\u5f3a\u7684\u91cf\u5b50\u5e72\u6d89\u4eea\u65b9\u6848\uff0c\u901a\u8fc7\u76f8\u4f4d\u654f\u611f\u7684\u5149\u5b50\u653e\u5927\u6291\u5236\u635f\u8017\u5bfc\u81f4\u7684\u6027\u80fd\u8870\u51cf\uff0c\u572890%\u9ad8\u635f\u8017\u4e0b\u4fdd\u6301\u8d85\u8d8a\u6807\u51c6\u91cf\u5b50\u6781\u9650\u7684\u76f8\u4f4d\u7075\u654f\u5ea6\uff0c\u89e3\u51b3\u4e86\u91cf\u5b50\u5e72\u6d89\u4eea\u5b9e\u7528\u5316\u7684\u5173\u952e\u74f6\u9888", "motivation": "\u91cf\u5b50\u5e72\u6d89\u4eea\u867d\u5177\u5907\u8d85\u8d8a\u6807\u51c6\u91cf\u5b50\u6781\u9650\u7684\u76f8\u4f4d\u6d4b\u91cf\u80fd\u529b\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u4e0d\u53ef\u907f\u514d\u7684\u5149\u5b66\u635f\u8017\u4f1a\u4e25\u91cd\u52a3\u5316\u7075\u654f\u5ea6\u548c\u91cf\u5b50\u589e\u5f3a\u56e0\u5b50\uff0c\u8fd9\u662f\u963b\u788d\u5176\u8d70\u5411\u5b9e\u9645\u5e94\u7528\u7684\u6700\u4e3b\u8981\u6311\u6218", "method": "\u5728\u91cf\u5b50\u5e72\u6d89\u4eea\u4e2d\u5f15\u5165\u76f8\u5e72\u653e\u5927\u5668\uff0c\u5229\u7528\u5176\u76f8\u4f4d\u654f\u611f\u7684\u5149\u5b50\u653e\u5927\u7279\u6027\u6765\u4fdd\u62a4\u91cf\u5b50\u6001\uff0c\u4ece\u800c\u6291\u5236\u9ad8\u635f\u8017\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u8870\u51cf", "result": "\u5b9e\u9a8c\u6ce8\u51654.2 dB\u538b\u7f29\u771f\u7a7a\u6001\u65f6\uff0c\u572890%\u635f\u8017\u4e0b\uff1a\u91cf\u5b50\u589e\u5f3a\u56e0\u5b50\u52a3\u5316\u4ece\u4f20\u7edf\u65b9\u6848\u76843.7 dB\u964d\u81f31.5 dB\uff1b\u76f8\u4f4d\u7075\u654f\u5ea6\u52a3\u5316\u4ec54.0 dB\uff0c\u8fdc\u4f18\u4e8e\u4f20\u7edf\u65b9\u6848\u768411.2 dB\uff1b\u5373\u4f7f\u635f\u8017\u8d85\u8fc790%\u4ecd\u80fd\u4fdd\u6301\u8d85\u8d8a\u6807\u51c6\u91cf\u5b50\u6781\u9650\u7684\u7075\u654f\u5ea6", "conclusion": "\u8be5\u653e\u5927\u5668\u589e\u5f3a\u65b9\u6848\u7a81\u7834\u4e86\u91cf\u5b50\u5e72\u6d89\u4eea\u5b9e\u7528\u5316\u7684\u5173\u952e\u969c\u788d\uff0c\u4e3a\u5728\u635f\u8017\u73af\u5883\u4e2d\u5b9e\u73b0\u9c81\u68d2\u7684\u91cf\u5b50\u589e\u5f3a\u6d4b\u91cf\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84"}}
{"id": "2601.17919", "categories": ["quant-ph", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2601.17919", "abs": "https://arxiv.org/abs/2601.17919", "authors": ["Arnulf J. Snedker-Nielsen", "David R. Gongora", "Magnus L. Madsen", "Christian H. Christiansen", "Eike L. Piehorsch", "Mathias \u00d8. Augustesen", "Elvedin Memisevic", "Sangeeth Kallatt", "Rodrigo A. Thomas", "Mark Kamper Svendsen", "Peter Krogstrup Jeppesen", "Marianne E. Bathen", "Lasse Vines", "Peter Granum", "Stefano Paesani"], "title": "Colour Centre Formation in Silicon-On-Insulator for On-Chip Photonic Integration", "comment": null, "summary": "Colour centres in silicon have great potential as single photon sources for quantum technologies. Some of them - like the T centre - also possess optically-active spins that enable spin-photon interfaces for generating entangled photons and multi-spin registers. This paper explores the generation of several types of colour centres in silicon for mass-manufacturable silicon-on-insulator quantum devices. We investigate how different processes in the device development affect the presence of the quantum emitters, including thermal annealing and fabrication steps for optical nanostructures. The study reveals coupled formation dynamics between different colour centres, identifies optimal parameters for annealing processes, and reports on the sensitivity to annealing duration and nanofabrication procedures for photonic integrated circuits. Furthermore, we discern stable optical signals from colour centres in silicon which have not been identified before.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u7845\u4e2d\u8272\u5fc3\u5728\u91cf\u5b50\u6280\u672f\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u5206\u6790\u9000\u706b\u548c\u7eb3\u7c73\u52a0\u5de5\u8fc7\u7a0b\u5bf9\u91cf\u5b50\u53d1\u5c04\u5668\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4e86\u6700\u4f18\u5de5\u827a\u53c2\u6570\u3001\u8272\u5fc3\u8026\u5408\u5f62\u6210\u52a8\u529b\u5b66\uff0c\u5e76\u8bc6\u522b\u51fa\u65b0\u578b\u7a33\u5b9a\u8272\u5fc3\u3002", "motivation": "\u7845\u4e2d\u7684\u8272\u5fc3\u4f5c\u4e3a\u5355\u5149\u5b50\u6e90\u548c\u81ea\u65cb-\u5149\u5b50\u63a5\u53e3\u5728\u91cf\u5b50\u6280\u672f\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u5b9e\u73b0\u4e0e\u7edd\u7f18\u4f53\u4e0a\u7845(SOI)\u91cf\u5b50\u5668\u4ef6\u7684\u5927\u89c4\u6a21\u5236\u9020\u517c\u5bb9\u3002", "method": "\u7814\u7a76\u5728\u7845\u4e2d\u751f\u6210\u591a\u79cd\u7c7b\u578b\u8272\u5fc3\uff0c\u5e76\u7cfb\u7edf\u8003\u5bdf\u4e86\u70ed\u9000\u706b\u548c\u5149\u5b66\u7eb3\u7c73\u7ed3\u6784\u5236\u9020\u6b65\u9aa4\u5bf9\u91cf\u5b50\u53d1\u5c04\u5668\u5b58\u5728\u72b6\u6001\u7684\u5f71\u54cd\u3002", "result": "\u63ed\u793a\u4e86\u4e0d\u540c\u8272\u5fc3\u4e4b\u95f4\u7684\u8026\u5408\u5f62\u6210\u52a8\u529b\u5b66\uff0c\u786e\u5b9a\u4e86\u9000\u706b\u5de5\u827a\u7684\u6700\u4f18\u53c2\u6570\uff0c\u62a5\u544a\u4e86\u8272\u5fc3\u5bf9\u9000\u706b\u65f6\u95f4\u548c\u7eb3\u7c73\u52a0\u5de5\u8fc7\u7a0b\u7684\u654f\u611f\u6027\uff0c\u5e76\u8bc6\u522b\u51fa\u7845\u4e2d\u524d\u6240\u672a\u6709\u7684\u7a33\u5b9a\u5149\u5b66\u4fe1\u53f7\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f00\u53d1\u53ef\u5927\u89c4\u6a21\u5236\u9020\u7684\u7845\u57fa\u91cf\u5b50\u5668\u4ef6\u63d0\u4f9b\u4e86\u5173\u952e\u5de5\u827a\u89c1\u89e3\uff0c\u540c\u65f6\u53d1\u73b0\u7684\u65b0\u578b\u8272\u5fc3\u4e3a\u91cf\u5b50\u6280\u672f\u53d1\u5c55\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2601.17009", "categories": ["cs.AI", "cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.17009", "abs": "https://arxiv.org/abs/2601.17009", "authors": ["Yanhua Zhao"], "title": "Online parameter estimation for the Crazyflie quadcopter through an EM algorithm", "comment": "20 pages, 37 figures", "summary": "Drones are becoming more and more popular nowadays. They are small in size, low in cost, and reliable in operation. They contain a variety of sensors and can perform a variety of flight tasks, reaching places that are difficult or inaccessible for humans. Earthquakes damage a lot of infrastructure, making it impossible for rescuers to reach some areas. But drones can help. Many amateur and professional photographers like to use drones for aerial photography. Drones play a non-negligible role in agriculture and transportation too. Drones can be used to spray pesticides, and they can also transport supplies. A quadcopter is a four-rotor drone and has been studied in this paper. In this paper, random noise is added to the quadcopter system and its effects on the drone system are studied. An extended Kalman filter has been used to estimate the state based on noisy observations from the sensor. Based on a SDE system, a linear quadratic Gaussian controller has been implemented. The expectation maximization algorithm has been applied for parameter estimation of the quadcopter. The results of offline parameter estimation and online parameter estimation are presented. The results show that the online parameter estimation has a slightly larger range of convergence values than the offline parameter estimation.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u7cfb\u7edf\u7684\u566a\u58f0\u5f71\u54cd\uff0c\u91c7\u7528\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u8fdb\u884c\u72b6\u6001\u4f30\u8ba1\uff0c\u57fa\u4e8e\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u5b9e\u73b0\u7ebf\u6027\u4e8c\u6b21\u9ad8\u65af\u63a7\u5236\uff0c\u5e76\u4f7f\u7528\u671f\u671b\u6700\u5927\u5316\u7b97\u6cd5\u8fdb\u884c\u53c2\u6570\u4f30\u8ba1\uff0c\u53d1\u73b0\u5728\u7ebf\u53c2\u6570\u4f30\u8ba1\u6bd4\u79bb\u7ebf\u4f30\u8ba1\u5177\u6709\u7a0d\u5927\u7684\u6536\u655b\u503c\u8303\u56f4\u3002", "motivation": "\u65e0\u4eba\u673a\u5728\u707e\u5bb3\u641c\u6551\u3001\u822a\u62cd\u3001\u519c\u4e1a\u690d\u4fdd\u3001\u7269\u6d41\u8fd0\u8f93\u7b49\u9886\u57df\u5e94\u7528\u4ef7\u503c\u663e\u8457\uff0c\u5c24\u5176\u5728\u5730\u9707\u540e\u57fa\u7840\u8bbe\u65bd\u635f\u6bc1\u533a\u57df\u5177\u6709\u4e0d\u53ef\u66ff\u4ee3\u7684\u4fa6\u5bdf\u4e0e\u4f5c\u4e1a\u80fd\u529b\u3002\u4e3a\u4fdd\u969c\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u53ef\u9760\u98de\u884c\uff0c\u9700\u6df1\u5165\u7814\u7a76\u968f\u673a\u566a\u58f0\u5bf9\u7cfb\u7edf\u7684\u5f71\u54cd\u53ca\u9c81\u68d2\u63a7\u5236\u65b9\u6cd5\u3002", "method": "1) \u5728\u56db\u65cb\u7ffc\u52a8\u529b\u5b66\u6a21\u578b\u4e2d\u6ce8\u5165\u968f\u673a\u566a\u58f0\uff1b2) \u91c7\u7528\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2(EKF)\u878d\u5408\u542b\u566a\u4f20\u611f\u5668\u89c2\u6d4b\u6570\u636e\u8fdb\u884c\u72b6\u6001\u4f30\u8ba1\uff1b3) \u57fa\u4e8e\u968f\u673a\u5fae\u5206\u65b9\u7a0b(SDE)\u6846\u67b6\u8bbe\u8ba1\u7ebf\u6027\u4e8c\u6b21\u9ad8\u65af(LQG)\u63a7\u5236\u5668\uff1b4) \u5e94\u7528\u671f\u671b\u6700\u5927\u5316(EM)\u7b97\u6cd5\u8fdb\u884c\u7cfb\u7edf\u53c2\u6570\u8fa8\u8bc6\uff0c\u5e76\u5bf9\u6bd4\u5206\u6790\u79bb\u7ebf\u4e0e\u5728\u7ebf\u4e24\u79cd\u4f30\u8ba1\u7b56\u7565\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u542b\u566a\u56db\u65cb\u7ffc\u7cfb\u7edf\u7684\u72b6\u6001\u4f30\u8ba1\u4e0e\u95ed\u73af\u63a7\u5236\uff0c\u83b7\u5f97\u4e86\u79bb\u7ebf\u53c2\u6570\u4f30\u8ba1\u548c\u5728\u7ebf\u53c2\u6570\u4f30\u8ba1\u4e24\u7ec4\u5b9e\u9a8c\u7ed3\u679c\u3002\u5b9a\u91cf\u5206\u6790\u8868\u660e\uff0c\u5728\u7ebf\u53c2\u6570\u4f30\u8ba1\u7684\u6536\u655b\u503c\u6ce2\u52a8\u8303\u56f4\u7565\u5927\u4e8e\u79bb\u7ebf\u4f30\u8ba1\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u9a8c\u8bc1\u4e86EKF-LQG\u6846\u67b6\u5728\u65e0\u4eba\u673a\u566a\u58f0\u6291\u5236\u4e2d\u7684\u6709\u6548\u6027\uff0cEM\u7b97\u6cd5\u53ef\u5b9e\u73b0\u51c6\u786e\u7684\u53c2\u6570\u8fa8\u8bc6\u3002\u5728\u7ebf\u4f30\u8ba1\u65b9\u6cd5\u867d\u6536\u655b\u8303\u56f4\u7a0d\u5927\uff0c\u4f46\u5177\u5907\u5b9e\u65f6\u66f4\u65b0\u80fd\u529b\uff0c\u4e3a\u65e0\u4eba\u673a\u5728\u52a8\u6001\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u7684\u81ea\u9002\u5e94\u63a7\u5236\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u589e\u5f3a\u4e86\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u4e0e\u53ef\u9760\u6027\u3002"}}
{"id": "2601.17924", "categories": ["quant-ph", "math.AP", "math.SP"], "pdf": "https://arxiv.org/pdf/2601.17924", "abs": "https://arxiv.org/abs/2601.17924", "authors": ["Marcello Malagutti", "Alberto Parmeggiani"], "title": "Perturbation Theory and the Quantum Rabi-model", "comment": "20 pages", "summary": "In the first part of the paper we study a perturbative model of the Rabi system of Quantum Optics. We therefore are able to describe, through Rellich's theory, an analytic expansion of finite families, but of any fixed length, of eigenvalues. In particular, we prove that for finite families of eigenvalues the Braak conjecture holds. In the second part we study the asymptotics of the Weyl spectral counting function of a class of systems that generalize the Quantum Rabi Model to an $N$-level atom ($N\\geq3$) with $N-1$ cavity modes of the electromagnetic field.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5fae\u6270\u7406\u8bba\u548c\u8c31\u5206\u6790\u7814\u7a76\u91cf\u5b50\u5149\u5b66\u4e2d\u7684Rabi\u7cfb\u7edf\uff0c\u8bc1\u660e\u4e86Braak\u731c\u60f3\u5728\u6709\u9650\u7279\u5f81\u503c\u65cf\u4e0a\u7684\u6210\u7acb\uff0c\u5e76\u63a8\u5e7f\u5230N\u80fd\u7ea7\u539f\u5b50\u7cfb\u7edf\u7684Weyl\u8c31\u8ba1\u6570\u51fd\u6570\u6e10\u8fd1\u6027\u7814\u7a76\u3002", "motivation": "\u7814\u7a76\u91cf\u5b50Rabi\u7cfb\u7edf\u7684\u8c31\u6027\u8d28\uff0c\u7279\u522b\u662f\u9a8c\u8bc1Braak\u731c\u60f3\uff08\u5173\u4e8e\u7279\u5f81\u503c\u89e3\u6790\u6027\u7684\u731c\u60f3\uff09\uff0c\u5e76\u5c06\u6a21\u578b\u63a8\u5e7f\u81f3\u591a\u80fd\u7ea7\u539f\u5b50\u4e0e\u591a\u8154\u6a21\u7cfb\u7edf\u7684\u8c31\u6e10\u8fd1\u884c\u4e3a\u5206\u6790\u3002", "method": "\u7b2c\u4e00\u90e8\u5206\u91c7\u7528Rellich\u7406\u8bba\u5bf9Rabi\u7cfb\u7edf\u8fdb\u884c\u5fae\u6270\u5206\u6790\uff0c\u5efa\u7acb\u7279\u5f81\u503c\u7684\u89e3\u6790\u5c55\u5f00\uff1b\u7b2c\u4e8c\u90e8\u5206\u7814\u7a76\u63a8\u5e7f\u81f3N\u80fd\u7ea7\u539f\u5b50\uff08N\u22653\uff09\u4e0eN-1\u4e2a\u7535\u78c1\u8154\u6a21\u7cfb\u7edf\u7684Weyl\u8c31\u8ba1\u6570\u51fd\u6570\u6e10\u8fd1\u6027\u6001\u3002", "result": "\u8bc1\u660e\u4e86\u6709\u9650\u7279\u5f81\u503c\u65cf\u4e0a\u7684Braak\u731c\u60f3\u6210\u7acb\uff0c\u83b7\u5f97\u4e86\u4efb\u610f\u56fa\u5b9a\u957f\u5ea6\u7279\u5f81\u503c\u65cf\u7684\u89e3\u6790\u5c55\u5f00\u5f0f\uff1b\u9488\u5bf9\u5e7f\u4e49\u91cf\u5b50Rabi\u6a21\u578b\uff0c\u5206\u6790\u4e86\u5176\u8c31\u8ba1\u6570\u51fd\u6570\u7684\u6e10\u8fd1\u5206\u5e03\u89c4\u5f8b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u5fae\u6270\u65b9\u6cd5\u786e\u7acb\u4e86Rabi\u7cfb\u7edf\u7279\u5f81\u503c\u7684\u89e3\u6790\u7ed3\u6784\uff0c\u9a8c\u8bc1\u4e86Braak\u731c\u60f3\uff0c\u5e76\u4e3a\u66f4\u590d\u6742\u7684\u591a\u4f53\u91cf\u5b50\u5149\u5b66\u7cfb\u7edf\u8c31\u5206\u6790\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u6df1\u5316\u4e86\u5bf9\u91cf\u5b50Rabi\u6a21\u578b\u53ca\u5176\u63a8\u5e7f\u7cfb\u7edf\u7684\u8c31\u6027\u8d28\u7406\u89e3\u3002"}}
{"id": "2601.17168", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.17168", "abs": "https://arxiv.org/abs/2601.17168", "authors": ["Judy Zhu", "Dhari Gandhi", "Himanshu Joshi", "Ahmad Rezaie Mianroodi", "Sedef Akinli Kocak", "Dhanesh Ramachandran"], "title": "Interpreting Agentic Systems: Beyond Model Explanations to System-Level Accountability", "comment": null, "summary": "Agentic systems have transformed how Large Language Models (LLMs) can be leveraged to create autonomous systems with goal-directed behaviors, consisting of multi-step planning and the ability to interact with different environments. These systems differ fundamentally from traditional machine learning models, both in architecture and deployment, introducing unique AI safety challenges, including goal misalignment, compounding decision errors, and coordination risks among interacting agents, that necessitate embedding interpretability and explainability by design to ensure traceability and accountability across their autonomous behaviors. Current interpretability techniques, developed primarily for static models, show limitations when applied to agentic systems. The temporal dynamics, compounding decisions, and context-dependent behaviors of agentic systems demand new analytical approaches. This paper assesses the suitability and limitations of existing interpretability methods in the context of agentic systems, identifying gaps in their capacity to provide meaningful insight into agent decision-making. We propose future directions for developing interpretability techniques specifically designed for agentic systems, pinpointing where interpretability is required to embed oversight mechanisms across the agent lifecycle from goal formation, through environmental interaction, to outcome evaluation. These advances are essential to ensure the safe and accountable deployment of agentic AI systems.", "AI": {"tldr": "This paper examines the limitations of existing interpretability methods for agentic AI systems (autonomous LLM-based systems with goal-directed behaviors) and proposes new directions for developing temporal, context-aware interpretability techniques specifically designed to address safety challenges like goal misalignment and compounding errors across the agent lifecycle.", "motivation": "Agentic systems introduce unique AI safety challenges (goal misalignment, compounding decision errors, coordination risks) that require built-in interpretability for traceability and accountability, but current static model interpretability techniques are inadequate for their temporal dynamics and context-dependent behaviors.", "method": "Assesses suitability and limitations of existing interpretability methods when applied to agentic systems, identifying specific gaps in their capacity to provide meaningful insight into agent decision-making processes.", "result": "Identifies critical gaps in current interpretability approaches and proposes future directions for developing agentic-specific interpretability techniques that can provide oversight mechanisms across the entire agent lifecycle (goal formation, environmental interaction, outcome evaluation).", "conclusion": "New interpretability methods specifically designed for agentic systems are essential to ensure their safe and accountable deployment, requiring temporal, context-aware approaches that can trace autonomous behaviors across multi-step planning and environmental interactions."}}
{"id": "2601.17926", "categories": ["quant-ph", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.17926", "abs": "https://arxiv.org/abs/2601.17926", "authors": ["Silvia N. Santalla", "Sudipto Singha Roy", "Germ\u00e1n Sierra", "Javier Rodr\u00edguez-Laguna"], "title": "The hyperlink representation of entanglement and the inclusion-exclusion principle", "comment": null, "summary": "The entanglement entropy (EE) of any bipartition of a pure state can be approximately expressed as a sum of entanglement links (ELs). In this work, we introduce their exact extension, i.e. the entanglement hyperlinks (EHLs), a type of generalized mutual informations defined through the inclusion-exclusion principle, each of which captures contributions to the multipartite entanglement that are not reducible to lower-order terms. We show that any EHL crossing a factorized partition must vanish, and that the EHLs between any set of blocks can be expressed as a sum of all the EHLs that join all of them. This last result allows us to provide an exact representation of the EE of any block of a pure state, from the sum of the EHLs which cross its boundary. In order to illustrate their rich structure, we discuss some explicit numerical examples using ground states of local Hamiltonians. The EHLs thus provide a remarkable tool to characterize multipartite entanglement in quantum information theory and quantum many-body physics.", "AI": {"tldr": "This paper introduces entanglement hyperlinks (EHLs) as exact extensions of entanglement links, defined via inclusion-exclusion principle, to capture irreducible multipartite entanglement contributions and provide an exact formula for entanglement entropy.", "motivation": "Entanglement entropy (EE) of pure states can only be approximately expressed as a sum of entanglement links, indicating a need for an exact extension that captures multipartite entanglement contributions not reducible to lower-order terms.", "method": "Generalized mutual informations are defined through the inclusion-exclusion principle, creating entanglement hyperlinks (EHLs) that quantify irreducible multipartite entanglement contributions.", "result": "(1) EHLs crossing factorized partitions must vanish; (2) EHLs between any block set equal the sum of all EHLs joining them; (3) Exact representation of EE for any pure-state block as the sum of EHLs crossing its boundary; (4) Numerical examples demonstrate rich structure in ground states of local Hamiltonians.", "conclusion": "Entanglement hyperlinks provide a powerful tool for characterizing multipartite entanglement in quantum information theory and quantum many-body physics."}}
{"id": "2601.17006", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17006", "abs": "https://arxiv.org/abs/2601.17006", "authors": ["Xuchen Li", "Jing Chen", "Xuzhao Li", "Hao Liang", "Xiaohuan Zhou", "Taifeng Wang", "Wentao Zhang"], "title": "MathMixup: Boosting LLM Mathematical Reasoning with Difficulty-Controllable Data Synthesis and Curriculum Learning", "comment": "Preprint, Under review", "summary": "In mathematical reasoning tasks, the advancement of Large Language Models (LLMs) relies heavily on high-quality training data with clearly defined and well-graded difficulty levels. However, existing data synthesis methods often suffer from limited diversity and lack precise control over problem difficulty, making them insufficient for supporting efficient training paradigms such as curriculum learning. To address these challenges, we propose MathMixup, a novel data synthesis paradigm that systematically generates high-quality, difficulty-controllable mathematical reasoning problems through hybrid and decomposed strategies. Automated self-checking and manual screening are incorporated to ensure semantic clarity and a well-structured difficulty gradient in the synthesized data. Building on this, we construct the MathMixupQA dataset and design a curriculum learning strategy that leverages these graded problems, supporting flexible integration with other datasets. Experimental results show that MathMixup and its curriculum learning strategy significantly enhance the mathematical reasoning performance of LLMs. Fine-tuned Qwen2.5-7B achieves an average score of 52.6\\% across seven mathematical benchmarks, surpassing previous state-of-the-art methods. These results fully validate the effectiveness and broad applicability of MathMixup in improving the mathematical reasoning abilities of LLMs and advancing data-centric curriculum learning.", "AI": {"tldr": "\u63d0\u51faMathMixup\u6570\u636e\u5408\u6210\u8303\u5f0f\uff0c\u901a\u8fc7\u6df7\u5408\u5206\u89e3\u7b56\u7565\u751f\u6210\u96be\u5ea6\u53ef\u63a7\u7684\u6570\u5b66\u63a8\u7406\u95ee\u9898\uff0c\u914d\u5408\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u4f7fQwen2.5-7B\u5728\u4e03\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u5f97\u5206\u8fbe52.6%\uff0c\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5", "motivation": "\u73b0\u6709\u6570\u636e\u5408\u6210\u65b9\u6cd5\u5b58\u5728\u591a\u6837\u6027\u6709\u9650\u548c\u96be\u5ea6\u63a7\u5236\u4e0d\u7cbe\u786e\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u652f\u6301\u8bfe\u7a0b\u5b66\u4e60\u7b49\u9ad8\u6548\u8bad\u7ec3\u8303\u5f0f\uff0c\u5236\u7ea6\u4e86LLM\u6570\u5b66\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347", "method": "\u63d0\u51faMathMixup\u5408\u6210\u8303\u5f0f\uff0c\u91c7\u7528\u6df7\u5408\u4e0e\u5206\u89e3\u7b56\u7565\u7cfb\u7edf\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u96be\u5ea6\u53ef\u63a7\u7684\u6570\u5b66\u95ee\u9898\uff0c\u7ed3\u5408\u81ea\u52a8\u5316\u81ea\u68c0\u548c\u4eba\u5de5\u7b5b\u9009\u786e\u4fdd\u6570\u636e\u8d28\u91cf\uff0c\u6784\u5efaMathMixupQA\u6570\u636e\u96c6\u5e76\u8bbe\u8ba1\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347LLM\u6570\u5b66\u63a8\u7406\u6027\u80fd\uff0c\u5fae\u8c03\u540e\u7684Qwen2.5-7B\u5728\u4e03\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u5f97\u520652.6%\uff0c\u8d85\u8d8a\u5148\u524d\u6700\u4f18\u65b9\u6cd5", "conclusion": "MathMixup\u8303\u5f0f\u6709\u6548\u63d0\u5347\u4e86LLM\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u63a8\u8fdb\u6570\u636e\u9a71\u52a8\u8bfe\u7a0b\u5b66\u4e60\u65b9\u9762\u7684\u6709\u6548\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027"}}
{"id": "2601.17930", "categories": ["quant-ph", "math.NA"], "pdf": "https://arxiv.org/pdf/2601.17930", "abs": "https://arxiv.org/abs/2601.17930", "authors": ["Antonio Falco", "Daniela Falco-Pomares", "Hermann G. Matthies"], "title": "A Rigorous and Self--Contained Proof of the Grover--Rudolph State Preparation Algorithm", "comment": null, "summary": "Preparing quantum states whose amplitudes encode classical probability distributions is a fundamental primitive in quantum algorithms based on amplitude encoding and amplitude estimation. Given a probability distribution $\\{p_k\\}_{k=0}^{2^n-1}$, the Grover--Rudolph procedure constructs an $n$-qubit state $\\ket\u03c8=\\sum_{k=0}^{2^n-1}\\sqrt{p_k}\\ket{k}$ by recursively applying families of controlled one-qubit rotations determined by a dyadic refinement of the target distribution. Despite its widespread use, the algorithm is often presented with informal correctness arguments and implicit conventions on the underlying dyadic tree. In this work we give a rigorous and self-contained analysis of the Grover--Rudolph construction: we formalize the dyadic probability tree, define the associated angle map via conditional masses, derive the resulting trigonometric factorizations, and prove by induction that the circuit prepares exactly the desired measurement law in the computational basis. As a complementary circuit-theoretic contribution, we show that each Grover--Rudolph stage is a uniformly controlled $\\RY$ rotation on an active register and provide an explicit ancilla-free transpilation into the gate dictionary $\\{\\RY(\\cdot),X,\\CNOT(\\cdot\\to\\cdot)\\}$ using Gray-code ladders and a Walsh--Hadamard angle transform.", "AI": {"tldr": "\u4e25\u683c\u5206\u6790Grover-Rudolph\u91cf\u5b50\u6001\u5236\u5907\u7b97\u6cd5\uff0c\u5f62\u5f0f\u5316\u5176\u4e8c\u53c9\u6811\u7ed3\u6784\u5e76\u7ed9\u51fa{RY,X,CNOT}\u95e8\u7684\u663e\u5f0f\u5b9e\u73b0\u3002", "motivation": "\u8be5\u7b97\u6cd5\u867d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u6b63\u786e\u6027\u8bba\u8bc1\u5e38\u4e3a\u975e\u6b63\u5f0f\u7684\uff0c\u5bf9\u5e95\u5c42\u4e8c\u53c9\u6811\u7684\u7ea6\u5b9a\u662f\u9690\u5f0f\u7684\u3002", "method": "\u5f62\u5f0f\u5316\u4e8c\u53c9\u6982\u7387\u6811\uff0c\u5b9a\u4e49\u6761\u4ef6\u8d28\u91cf\u89d2\u5ea6\u6620\u5c04\uff0c\u5f52\u7eb3\u8bc1\u660e\u7535\u8def\u5236\u5907\u6b63\u786e\u7684\u6d4b\u91cf\u5206\u5e03\uff1b\u8bc1\u660e\u6bcf\u9636\u6bb5\u4e3a\u5747\u5300\u63a7\u5236RY\u65cb\u8f6c\uff0c\u5e76\u7528\u683c\u96f7\u7801\u548c\u6c83\u5c14\u4ec0-\u54c8\u8fbe\u739b\u53d8\u6362\u8f6c\u8bd1\u3002", "result": "\u4e25\u683c\u8bc1\u660e\u4e86\u7b97\u6cd5\u6b63\u786e\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u65e0\u8f85\u52a9\u6bd4\u7279\u7684{RY,X,CNOT}\u95e8\u7535\u8def\u5b9e\u73b0\u3002", "conclusion": "\u4e3aGrover-Rudolph\u7b97\u6cd5\u5960\u5b9a\u4e86\u4e25\u683c\u7406\u8bba\u57fa\u7840\uff0c\u63d0\u4f9b\u4e86\u5b9e\u7528\u7535\u8def\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u5176\u5728\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7684\u53ef\u9760\u5e94\u7528\u3002"}}
{"id": "2601.17310", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17310", "abs": "https://arxiv.org/abs/2601.17310", "authors": ["Yu Akagi", "Tomohisa Seki", "Hiromasa Ito", "Toru Takiguchi", "Kazuhiko Ohe", "Yoshimasa Kawazoe"], "title": "High-Fidelity Longitudinal Patient Simulation Using Real-World Data", "comment": null, "summary": "Simulation is a powerful tool for exploring uncertainty. Its potential in clinical medicine is transformative and includes personalized treatment planning and virtual clinical trials. However, simulating patient trajectories is challenging because of complex biological and sociocultural influences. Here, we show that real-world clinical records can be leveraged to empirically model patient timelines. We developed a generative simulator model that takes a patient's history as input and synthesizes fine-grained, realistic future trajectories. The model was pretrained on more than 200 million clinical records. It produced high-fidelity future timelines, closely matching event occurrence rates, laboratory test results, and temporal dynamics in real patient future data. It also accurately estimated future event probabilities, with observed-to-expected ratios consistently near 1.0 across diverse outcomes and time horizons. Our results reveal the untapped value of real-world data in electronic health records and introduce a scalable framework for in silico modeling of clinical care.", "AI": {"tldr": "\u4e00\u4e2a\u57fa\u4e8e2\u4ebf\u591a\u6761\u4e34\u5e8a\u8bb0\u5f55\u8bad\u7ec3\u7684\u751f\u6210\u5f0fAI\u6a21\u578b\u80fd\u591f\u9ad8\u4fdd\u771f\u5730\u6a21\u62df\u60a3\u8005\u8f68\u8ff9\uff0c\u5339\u914d\u771f\u5b9e\u4e16\u754c\u6570\u636e\u6a21\u5f0f\u5e76\u51c6\u786e\u9884\u6d4b\u672a\u6765\u4e8b\u4ef6\u3002", "motivation": "\u6a21\u62df\u662f\u63a2\u7d22\u4e0d\u786e\u5b9a\u6027\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u5728\u4e34\u5e8a\u533b\u5b66\u4e2d\u5177\u6709\u53d8\u9769\u6027\u6f5c\u529b\uff0c\u5305\u62ec\u4e2a\u6027\u5316\u6cbb\u7597\u89c4\u5212\u548c\u865a\u62df\u4e34\u5e8a\u8bd5\u9a8c\uff0c\u4f46\u7531\u4e8e\u751f\u7269\u548c\u793e\u4f1a\u6587\u5316\u5f71\u54cd\u7684\u590d\u6742\u6027\uff0c\u6a21\u62df\u60a3\u8005\u8f68\u8ff9\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u751f\u6210\u5f0f\u6a21\u62df\u6a21\u578b\uff0c\u4ee5\u60a3\u8005\u5386\u53f2\u8bb0\u5f55\u4e3a\u8f93\u5165\uff0c\u5408\u6210\u7cbe\u7ec6\u4e14\u771f\u5b9e\u7684\u672a\u6765\u8f68\u8ff9\uff0c\u8be5\u6a21\u578b\u5728\u8d85\u8fc72\u4ebf\u6761\u4e34\u5e8a\u8bb0\u5f55\u4e0a\u8fdb\u884c\u4e86\u9884\u8bad\u7ec3\u3002", "result": "\u8be5\u6a21\u578b\u751f\u6210\u4e86\u9ad8\u4fdd\u771f\u7684\u672a\u6765\u65f6\u95f4\u7ebf\uff0c\u4e0e\u771f\u5b9e\u60a3\u8005\u6570\u636e\u4e2d\u7684\u4e8b\u4ef6\u53d1\u751f\u7387\u3001\u5b9e\u9a8c\u5ba4\u68c0\u67e5\u7ed3\u679c\u548c\u65f6\u95f4\u52a8\u6001\u9ad8\u5ea6\u5339\u914d\uff1b\u5728\u4e0d\u540c\u7ed3\u679c\u548c\u65f6\u95f4\u8303\u56f4\u5185\uff0c\u89c2\u5bdf\u503c\u4e0e\u9884\u671f\u503c\u7684\u6bd4\u7387\u59cb\u7ec8\u63a5\u8fd11.0\uff0c\u51c6\u786e\u4f30\u8ba1\u4e86\u672a\u6765\u4e8b\u4ef6\u6982\u7387\u3002", "conclusion": "\u63ed\u793a\u4e86\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7684\u672a\u5f00\u53d1\u4ef7\u503c\uff0c\u5e76\u4e3a\u4e34\u5e8a\u62a4\u7406\u7684\u8ba1\u7b97\u673a\u6a21\u62df\u5f15\u5165\u4e86\u53ef\u6269\u5c55\u6846\u67b6\u3002"}}
{"id": "2601.17936", "categories": ["quant-ph", "math.NA"], "pdf": "https://arxiv.org/pdf/2601.17936", "abs": "https://arxiv.org/abs/2601.17936", "authors": ["Antonio Falco", "Daniela Falco-Pomares", "Hermann G. Matthies"], "title": "Elementary Quantum Gates from Lie Group Embeddings in $U(2^n)$: Geometry, Universality, and Discretization", "comment": null, "summary": "In the standard circuit model, elementary gates are specified relative to a chosen tensor factorization and are therefore extrinsic to the ambient group $U(2^n)$. Writing $N=2^n$, we introduce an intrinsic descriptor layer in $U(N)$ by declaring as primitive the motions inside faithful embedded copies of $SU(2)$, leading to the phase-free dictionary $\\mathcal{G}^{SU}_{\\mathrm{elem}}(n)=\\bigcup_{\u03c6\\in\\Emb(SU(2),U(N))}\u03c6(SU(2))$, and we also discuss the phase-inclusive $U(2)$ variant. We show that $\\Emb(SU(2),U(N))$ decomposes into finitely many $U(N)$-homogeneous strata indexed by isotypic multiplicities, with stabilizers given by centralizers; the canonical two-level sector is organized by $\\Gr_2(\\C^N)$ up to a $PSU(2)$ gauge. Equipping $U(N)$ with the Hilbert--Schmidt bi-invariant metric, each embedded subgroup is totally geodesic. Using two-level QR/Givens factorization together with an explicit generation of diagonal tori by two-level phase rotations, we prove phase-free universality $\\langle\\mathcal{G}^{SU}_{\\mathrm{2lvl}}(n)\\rangle=SU(N)$ and hence $\\langle\\mathcal{G}^{SU}_{\\mathrm{elem}}(n)\\rangle=SU(N)$. Full universality in $U(N)$ follows by adjoining the abelian diagonal/global $U(1)$ factors (equivalently, by passing to the $U(2)$ two-level dictionary). Finally, we record a modular finite-alphabet interface by lifting Solovay--Kitaev approximation in $SU(2)$ through two-level embeddings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5b50\u7535\u8def\u7684\u5185\u5728\u63cf\u8ff0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06SU(2)\u7684\u6240\u6709\u5d4c\u5165\u4f5c\u4e3a\u539f\u59cb\u95e8\u64cd\u4f5c\uff0c\u6784\u5efa\u4e86\u65e0\u76f8\u4f4d\u8bcd\u5178\uff0c\u5e76\u8bc1\u660e\u5176\u5728SU(2^n)\u4e2d\u7684\u901a\u7528\u6027\uff0c\u540c\u65f6\u7ed9\u51fa\u4e86\u57fa\u4e8e\u6709\u9650\u5b57\u6bcd\u8868\u7684\u8fd1\u4f3c\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u7535\u8def\u6a21\u578b\u4e2d\u7684\u57fa\u672c\u95e8\u76f8\u5bf9\u4e8e\u9009\u5b9a\u7684\u5f20\u91cf\u56e0\u5b50\u5316\u662f\u5916\u5728\u7684\uff0c\u7f3a\u4e4f\u4e0eU(2^n)\u7fa4\u672c\u8eab\u7684\u5185\u5728\u8054\u7cfb\u3002\u672c\u6587\u65e8\u5728\u5efa\u7acb\u4e00\u79cd\u4e0d\u4f9d\u8d56\u5916\u90e8\u9009\u62e9\u7684\u5185\u5728\u63cf\u8ff0\u5c42\u3002", "method": "\u5b9a\u4e49N=2^n\uff0c\u6784\u9020\u65e0\u76f8\u4f4d\u8bcd\u5178G^SU_elem(n)\u4e3a\u6240\u6709SU(2)\u5230U(N)\u5d4c\u5165\u7684\u50cf\u96c6\u3002\u5229\u7528\u8868\u793a\u8bba\u5206\u6790\u5d4c\u5165\u7a7a\u95f4\u7684\u7ed3\u6784\uff0c\u91c7\u7528Hilbert-Schmidt\u53cc\u4e0d\u53d8\u5ea6\u91cf\u7814\u7a76\u51e0\u4f55\u6027\u8d28\uff0c\u8fd0\u7528\u4e24\u5c42QR/Givens\u5206\u89e3\u548c\u663e\u5f0f\u7684\u5bf9\u89d2\u73af\u9762\u751f\u6210\u65b9\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u27e8G^SU_2lvl(n)\u27e9=SU(N)\uff0c\u5373\u4e24\u5c42\u65e0\u76f8\u4f4d\u95e8\u53ef\u751f\u6210\u6574\u4e2a\u7279\u6b8a\u9149\u7fa4\uff1b\u8fdb\u800c\u27e8G^SU_elem(n)\u27e9=SU(N)\u3002\u901a\u8fc7\u9644\u52a0\u5bf9\u89d2U(1)\u76f8\u4f4d\u56e0\u5b50\u5b9e\u73b0U(N)\u7684\u5b8c\u5168\u901a\u7528\u6027\u3002\u6700\u540e\u901a\u8fc7\u5d4c\u5165\u63d0\u5347Solovay-Kitaev\u7b97\u6cd5\uff0c\u6784\u5efa\u4e86\u6a21\u5757\u5316\u6709\u9650\u5b57\u6bcd\u8868\u8fd1\u4f3c\u63a5\u53e3\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5efa\u7acb\u4e86\u57fa\u4e8e\u6240\u6709\u53ef\u80fdSU(2)\u5d4c\u5165\u7684\u91cf\u5b50\u8ba1\u7b97\u65b0\u8303\u5f0f\uff0c\u4e3a\u901a\u7528\u91cf\u5b50\u8ba1\u7b97\u63d0\u4f9b\u4e86\u66f4\u81ea\u7136\u7684\u6570\u5b66\u63cf\u8ff0\uff0c\u5e76\u7ed9\u51fa\u4e86\u5b9e\u9645\u53ef\u5b9e\u73b0\u7684\u8fd1\u4f3c\u65b9\u6848\u3002"}}
{"id": "2601.17008", "categories": ["cs.LG", "q-fin.TR"], "pdf": "https://arxiv.org/pdf/2601.17008", "abs": "https://arxiv.org/abs/2601.17008", "authors": ["Haochong Xia", "Simin Li", "Ruixiao Xu", "Zhixia Zhang", "Hongxiang Wang", "Zhiqian Liu", "Teng Yao Long", "Molei Qin", "Chuqiao Zong", "Bo An"], "title": "Bayesian Robust Financial Trading with Adversarial Synthetic Market Data", "comment": null, "summary": "Algorithmic trading relies on machine learning models to make trading decisions. Despite strong in-sample performance, these models often degrade when confronted with evolving real-world market regimes, which can shift dramatically due to macroeconomic changes-e.g., monetary policy updates or unanticipated fluctuations in participant behavior. We identify two challenges that perpetuate this mismatch: (1) insufficient robustness in existing policy against uncertainties in high-level market fluctuations, and (2) the absence of a realistic and diverse simulation environment for training, leading to policy overfitting. To address these issues, we propose a Bayesian Robust Framework that systematically integrates a macro-conditioned generative model with robust policy learning. On the data side, to generate realistic and diverse data, we propose a macro-conditioned GAN-based generator that leverages macroeconomic indicators as primary control variables, synthesizing data with faithful temporal, cross-instrument, and macro correlations. On the policy side, to learn robust policy against market fluctuations, we cast the trading process as a two-player zero-sum Bayesian Markov game, wherein an adversarial agent simulates shifting regimes by perturbing macroeconomic indicators in the macro-conditioned generator, while the trading agent-guided by a quantile belief network-maintains and updates its belief over hidden market states. The trading agent seeks a Robust Perfect Bayesian Equilibrium via Bayesian neural fictitious self-play, stabilizing learning under adversarial market perturbations. Extensive experiments on 9 financial instruments demonstrate that our framework outperforms 9 state-of-the-art baselines. In extreme events like the COVID, our method shows improved profitability and risk management, offering a reliable solution for trading under uncertain and shifting market dynamics.", "AI": {"tldr": "A Bayesian Robust Framework for algorithmic trading that combines a macro-conditioned GAN for realistic data generation with robust policy learning via a two-player Bayesian Markov game, significantly outperforming baselines especially during market crises like COVID.", "motivation": "Algorithmic trading models suffer performance degradation in evolving real-world markets due to shifting regimes from macroeconomic changes, caused by insufficient model robustness and lack of diverse training environments leading to overfitting.", "method": "Proposes a two-component framework: (1) Macro-conditioned GAN generator that uses macroeconomic indicators to synthesize realistic, diverse financial data with proper temporal, cross-instrument, and macro correlations; (2) Two-player zero-sum Bayesian Markov game where an adversarial agent simulates regime shifts by perturbing macro indicators while the trading agent maintains beliefs over hidden market states using a quantile belief network, seeking Robust Perfect Bayesian Equilibrium via Bayesian neural fictitious self-play.", "result": "Extensive experiments on 9 financial instruments show the framework outperforms 9 state-of-the-art baselines, with particularly improved profitability and risk management during extreme events like the COVID market crisis.", "conclusion": "The framework provides a reliable solution for robust algorithmic trading under uncertain and shifting market dynamics by systematically addressing both data diversity and policy robustness challenges through Bayesian game-theoretic learning."}}
{"id": "2601.17311", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17311", "abs": "https://arxiv.org/abs/2601.17311", "authors": ["Bang Liu", "Linglong Kong", "Jian Pei"], "title": "Phase Transition for Budgeted Multi-Agent Synergy", "comment": "55 pages, 12 figures", "summary": "Multi-agent systems can improve reliability, yet under a fixed inference budget they often help, saturate, or even collapse. We develop a minimal and calibratable theory that predicts these regimes from three binding constraints of modern agent stacks: finite context windows, lossy inter-agent communication, and shared failures among similar agents. Each leaf agent is summarized by a compute-performance scaling exponent $\u03b2$; communication is captured by a message-length fidelity curve $\u03b3(m)$; dependence is captured by an effective shared-error correlation $\u03c1$; and a context window $W$ imposes hard fan-in limits that make hierarchy necessary. For binary success/failure tasks with majority aggregation, we prove a sharp phase transition for deep $b$-ary trees with correlated inputs and lossy communication: a single scalar $\u03b1_\u03c1$ (combining $\u03b3(m)$, $\u03c1$, and fan-in $b$) determines whether weak signal is amplified to a nontrivial fixed point or washed out to chance. In the amplifying regime, we derive an organization exponent $s$ and show that budgeted synergy, i.e., outperforming the best single agent under the same total budget, occurs exactly when $s>\u03b2$, yielding closed-form compute allocation rules and explicit budget thresholds. We further characterize saturation via a mixing depth and provide a conservative clipped predictor that remains accurate across growth and saturation. A continuous-performance warm-up gives closed-form risks for star, chain, and tree organizations, making correlation- and communication-induced floors explicit and exposing the core design trade-offs in a smooth setting. Finally, we validate the predicted phase boundaries in controlled synthetic simulations and show how the same mechanisms explain the dominant bottlenecks reported in recent large-scale matched-budget studies of LLM agent-system scaling.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5efa\u7acb\u4e86\u4e00\u4e2a\u53ef\u6821\u51c6\u7684\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u6709\u9650\u4e0a\u4e0b\u6587\u7a97\u53e3\u3001\u6709\u635f\u901a\u4fe1\u548c\u667a\u80fd\u4f53\u95f4\u5171\u4eab\u9519\u8bef\u4e09\u4e2a\u7ea6\u675f\uff0c\u9884\u6d4b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u56fa\u5b9a\u63a8\u7406\u9884\u7b97\u4e0b\u7684\u4e09\u79cd\u884c\u4e3a\u6a21\u5f0f\uff08\u6709\u6548\u3001\u9971\u548c\u3001\u5d29\u6e83\uff09\uff0c\u5e76\u7ed9\u51fa\u4e86\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u89c4\u5219\u548c\u6027\u80fd\u76f8\u53d8\u8fb9\u754c\u3002", "motivation": "\u73b0\u4ee3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u867d\u80fd\u63d0\u5347\u53ef\u9760\u6027\uff0c\u4f46\u5728\u56fa\u5b9a\u9884\u7b97\u4e0b\u5e38\u8868\u73b0\u51fa\u4e0d\u53ef\u9884\u6d4b\u7684\u6027\u80fd\u9971\u548c\u751a\u81f3\u5d29\u6e83\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u7edf\u4e00\u7406\u8bba\u89e3\u91ca\u8fd9\u4e9b\u73b0\u8c61\uff0c\u65e0\u6cd5\u6307\u5bfc\u7cfb\u7edf\u8bbe\u8ba1\u548c\u8d44\u6e90\u5206\u914d\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5efa\u7acb\u53ef\u91cf\u5316\u7684\u9884\u6d4b\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4e09\u4e2a\u6838\u5fc3\u7ea6\u675f\u7684\u6781\u7b80\u7406\u8bba\uff1a1) \u53f6\u667a\u80fd\u4f53\u7684\u8ba1\u7b97-\u6027\u80fd\u7f29\u653e\u6307\u6570\u03b2\uff1b2) \u901a\u4fe1\u4fdd\u771f\u5ea6\u66f2\u7ebf\u03b3(m)\uff1b3) \u9519\u8bef\u76f8\u5173\u6027\u03c1\uff1b4) \u4e0a\u4e0b\u6587\u7a97\u53e3W\u51b3\u5b9a\u7684\u6247\u5165\u9650\u5236\u3002\u9488\u5bf9\u4e8c\u5206\u7c7b\u591a\u6570\u8868\u51b3\u4efb\u52a1\uff0c\u5728\u6df1\u5ea6b\u53c9\u6811\u4e0a\u5206\u6790\u6709\u635f\u901a\u4fe1\u548c\u76f8\u5173\u8f93\u5165\u4e0b\u7684\u76f8\u53d8\u884c\u4e3a\uff0c\u5e76\u63a8\u5bfc\u51fa\u7ec4\u7ec7\u6307\u6570s\u3002", "result": "1) \u53d1\u73b0\u5355\u4e00\u6807\u91cf\u03b1_\u03c1\u51b3\u5b9a\u4fe1\u53f7\u662f\u88ab\u653e\u5927\u5230\u975e\u5e73\u51e1\u56fa\u5b9a\u70b9\u8fd8\u662f\u8870\u51cf\u81f3\u968f\u673a\u6c34\u5e73\uff1b2) \u8bc1\u660e\u9884\u7b97\u534f\u540c\u6548\u5e94\uff08\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\uff09\u6070\u597d\u53d1\u751f\u5728s>\u03b2\u65f6\uff0c\u7ed9\u51fa\u95ed\u5f0f\u8d44\u6e90\u5206\u914d\u89c4\u5219\uff1b3) \u63d0\u51fa\u6df7\u5408\u6df1\u5ea6\u523b\u753b\u9971\u548c\u73b0\u8c61\uff1b4) \u4e3a\u661f\u578b\u3001\u94fe\u5f0f\u548c\u6811\u5f62\u7ed3\u6784\u63d0\u4f9b\u95ed\u5f0f\u98ce\u9669\u8868\u8fbe\u5f0f\uff1b5) \u5408\u6210\u5b9e\u9a8c\u9a8c\u8bc1\u76f8\u53d8\u8fb9\u754c\uff0c\u5e76\u4e0e\u8fd1\u671f\u5927\u89c4\u6a21LLM\u667a\u80fd\u4f53\u7814\u7a76\u4e2d\u7684\u74f6\u9888\u73b0\u8c61\u4e00\u81f4\u3002", "conclusion": "\u8be5\u7406\u8bba\u9996\u6b21\u7edf\u4e00\u89e3\u91ca\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6027\u80fd\u76f8\u53d8\u673a\u5236\uff0c\u63ed\u793a\u4e86\u901a\u4fe1\u4fdd\u771f\u5ea6\u3001\u9519\u8bef\u76f8\u5173\u6027\u548c\u7ec4\u7ec7\u5c42\u7ea7\u95f4\u7684\u6839\u672c\u6743\u8861\uff0c\u4e3a\u5728\u6709\u9650\u9884\u7b97\u4e0b\u8bbe\u8ba1\u9ad8\u6548\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u91cf\u5316\u7684\u9884\u6d4b\u5de5\u5177\u548c\u4f18\u5316\u51c6\u5219\u3002"}}
{"id": "2601.17956", "categories": ["quant-ph", "math-ph"], "pdf": "https://arxiv.org/pdf/2601.17956", "abs": "https://arxiv.org/abs/2601.17956", "authors": ["Kumar Gautam", "Akshit Dutta", "Kumar Shubham"], "title": "Quantum Radar System Using Born-Feynman path integrals approach", "comment": null, "summary": "The paper relates to a quantum radar deployment by the Born-Feynman path integrals approach based on quantum dots. The radar system comprises a quantum dot-based entangled photon generator, a transmission module, a delay line, a detection module, and a signal processing unit. The quantum dot-based entangled photon generator produces entangled photon pairs via spontaneous parametric down-conversion or stimulated emission. The signal transmission module, equipped with a microwave antenna and beamforming elements, directs the signal photon toward a target. The delay line module synchronizes the retained idler photon with the returning signal photon, preserving quantum coherence. The detection module collects the reflected signal photon and uses a cryogenically cooled superconducting nanowire single photon detector (SNSPD) for detection. Finally, the signal processing unit analyzes the quantum correlation between the scattered and idler photons to enable precise quantum state comparison.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u91cf\u5b50\u70b9\u4e0e\u73bb\u6069-\u8d39\u66fc\u8def\u5f84\u79ef\u5206\u65b9\u6cd5\u7684\u91cf\u5b50\u96f7\u8fbe\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u901a\u8fc7\u91cf\u5b50\u70b9\u4ea7\u751f\u7ea0\u7f20\u5149\u5b50\u5bf9\uff0c\u5229\u7528\u8d85\u5bfc\u7eb3\u7c73\u7ebf\u5355\u5149\u5b50\u63a2\u6d4b\u5668\u8fdb\u884c\u63a2\u6d4b\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u6563\u5c04\u5149\u5b50\u4e0e\u95f2\u7f6e\u5149\u5b50\u7684\u91cf\u5b50\u5173\u8054\u5b9e\u73b0\u7cbe\u786e\u91cf\u5b50\u6001\u6bd4\u8f83\u3002", "motivation": "\u6458\u8981\u672a\u660e\u786e\u9610\u8ff0\u7814\u7a76\u52a8\u673a\uff0c\u4f46\"\u7cbe\u786e\u91cf\u5b50\u6001\u6bd4\u8f83\"\u7684\u8868\u8ff0\u6697\u793a\u65e8\u5728\u5229\u7528\u91cf\u5b50\u7ea0\u7f20\u7279\u6027\u63d0\u5347\u96f7\u8fbe\u63a2\u6d4b\u7684\u7cbe\u5ea6\u548c\u7075\u654f\u5ea6\u3002", "method": "\u91c7\u7528\u91cf\u5b50\u70b9\u4f5c\u4e3a\u7ea0\u7f20\u5149\u5b50\u6e90\uff0c\u901a\u8fc7\u81ea\u53d1\u53c2\u91cf\u4e0b\u8f6c\u6362\u6216\u53d7\u6fc0\u53d1\u5c04\u4ea7\u751f\u7ea0\u7f20\u5149\u5b50\u5bf9\uff1b\u7cfb\u7edf\u7531\u4f20\u8f93\u6a21\u5757\uff08\u5fae\u6ce2\u5929\u7ebf\u4e0e\u6ce2\u675f\u8d4b\u5f62\u5143\u4ef6\uff09\u3001\u5ef6\u8fdf\u7ebf\u6a21\u5757\uff08\u540c\u6b65\u95f2\u7f6e\u5149\u5b50\u4e0e\u8fd4\u56de\u4fe1\u53f7\u5149\u5b50\u4ee5\u4fdd\u6301\u91cf\u5b50\u76f8\u5e72\u6027\uff09\u3001\u63a2\u6d4b\u6a21\u5757\uff08\u4f4e\u6e29\u8d85\u5bfc\u7eb3\u7c73\u7ebf\u5355\u5149\u5b50\u63a2\u6d4b\u5668\uff09\u548c\u4fe1\u53f7\u5904\u7406\u5355\u5143\uff08\u5206\u6790\u91cf\u5b50\u5173\u8054\uff09\u7ec4\u6210\u3002", "result": "\u6458\u8981\u672a\u63d0\u4f9b\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\u6216\u6027\u80fd\u6570\u636e\uff0c\u4ec5\u63cf\u8ff0\u4e86\u7cfb\u7edf\u67b6\u6784\u548c\u5de5\u4f5c\u539f\u7406\uff0c\u6700\u7ec8\u76ee\u6807\u662f\u5b9e\u73b0\u5bf9\u6563\u5c04\u5149\u5b50\u4e0e\u95f2\u7f6e\u5149\u5b50\u7684\u7cbe\u786e\u91cf\u5b50\u6001\u6bd4\u8f83\u3002", "conclusion": "\u6458\u8981\u672a\u7ed9\u51fa\u660e\u786e\u7ed3\u8bba\uff0c\u4f46\u7cfb\u7edf\u8bbe\u8ba1\u65b9\u6848\u8868\u660e\u8be5\u91cf\u5b50\u96f7\u8fbe\u67b6\u6784\u5229\u7528\u91cf\u5b50\u7ea0\u7f20\u548c\u76f8\u5e72\u6027\u4fdd\u6301\u673a\u5236\uff0c\u4e3a\u91cf\u5b50\u6001\u6bd4\u8f83\u63a2\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u6280\u672f\u5b9e\u73b0\u8def\u5f84\u3002"}}
{"id": "2601.17332", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17332", "abs": "https://arxiv.org/abs/2601.17332", "authors": ["Yicheng Tao", "Hongteng Xu"], "title": "TheoremForge: Scaling up Formal Data Synthesis with Low-Budget Agentic Workflow", "comment": null, "summary": "The high cost of agentic workflows in formal mathematics hinders large-scale data synthesis, exacerbating the scarcity of open-source corpora. To address this, we introduce \\textbf{TheoremForge}, a cost-effective formal data synthesis pipeline that decomposes the formalization process into five sub-tasks, which are \\textit{statement formalization}, \\textit{proof generation}, \\textit{premise selection}, \\textit{proof correction} and \\textit{proof sketching}. By implementing a \\textit{Decoupled Extraction Strategy}, the workflow recovers valid training signals from globally failed trajectories, effectively utilizing wasted computation. Experiments on a 2,000-problem benchmark demonstrate that TheoremForge achieves a Verified Rate of 12.6\\%, surpassing the 8.6\\% baseline, at an average cost of only \\textbf{\\$0.481} per successful trajectory using Gemini-3-Flash. Crucially, our strategy increases data yield by \\textbf{1.6$\\times$} for proof generation compared to standard filtering. These results establish TheoremForge as a scalable framework for constructing a data flywheel to train future expert models. Our code is available \\href{https://github.com/timechess/TheoremForge}{here}.", "AI": {"tldr": "TheoremForge is a cost-effective pipeline for synthesizing formal mathematics data that decomposes formalization into five sub-tasks and uses a decoupled extraction strategy to recover training signals from failed trajectories, achieving 12.6% verified rate at only $0.48 per successful trajectory.", "motivation": "The high cost of agentic workflows in formal mathematics hinders large-scale data synthesis and exacerbates the scarcity of open-source corpora.", "method": "Introduces TheoremForge, a pipeline that decomposes formalization into five sub-tasks (statement formalization, proof generation, premise selection, proof correction, and proof sketching) and implements a Decoupled Extraction Strategy to recover valid training signals from globally failed trajectories.", "result": "Achieves a 12.6% Verified Rate (surpassing the 8.6% baseline) on a 2,000-problem benchmark at an average cost of $0.481 per successful trajectory using Gemini-3-Flash, with a 1.6\u00d7 increase in data yield for proof generation compared to standard filtering.", "conclusion": "TheoremForge establishes a scalable framework for constructing a data flywheel to train future expert models in formal mathematics."}}
{"id": "2601.17960", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.17960", "abs": "https://arxiv.org/abs/2601.17960", "authors": ["Devashish Tupkary", "Shlok Nahar", "Ernest Y. -Z. Tan"], "title": "Authentication in Security Proofs for Quantum Key Distribution", "comment": null, "summary": "Quantum Key Distribution (QKD) protocols rely on authenticated classical communication. Typical QKD security proofs are carried out in an idealized setting where authentication is assumed to behave honestly: it never aborts, and all classical messages are delivered faithfully with their original timing preserved. Authenticated channels that can be constructed in practice have different properties. Most critically, such channels may abort asymmetrically, such that only the receiving party may detect an authentication failure while the sending party remains unaware. Furthermore, an adversary may delay, reorder, or block classical messages. This discrepancy renders the standard QKD security definition and existing QKD security proofs invalid in the practical authentication setting. In this work we resolve this issue. Our main result is a reduction theorem showing that, under mild and easily satisfied protocol conditions, any QKD protocol proven secure under the honest authentication setting remains secure under a practical authentication setting. This result allows all existing QKD proofs to be retroactively lifted to the practical authentication setting with a minor protocol tweak.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u7ea6\u5316\u5b9a\u7406\uff0c\u89e3\u51b3\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1(QKD)\u5728\u5b9e\u9645\u8ba4\u8bc1\u4fe1\u9053\u4e0b\u7684\u5b89\u5168\u6027\u95ee\u9898\u3002\u8bc1\u660e\u5728\u6ee1\u8db3\u6e29\u548c\u4e14\u6613\u6ee1\u8db3\u7684\u534f\u8bae\u6761\u4ef6\u65f6\uff0c\u4efb\u4f55\u5728\u7406\u60f3\u8ba4\u8bc1\u8bbe\u7f6e\u4e0b\u88ab\u8bc1\u660e\u5b89\u5168\u7684QKD\u534f\u8bae\uff0c\u5728\u9762\u5bf9\u975e\u5bf9\u79f0\u4e2d\u6b62\u3001\u6d88\u606f\u5ef6\u8fdf/\u91cd\u6392/\u963b\u65ad\u7684\u5b9e\u9645\u8ba4\u8bc1\u73af\u5883\u65f6\u4ecd\u4fdd\u6301\u5b89\u5168\uff0c\u4f7f\u73b0\u6709\u8bc1\u660e\u53ef\u901a\u8fc7\u5fae\u5c0f\u534f\u8bae\u8c03\u6574\u76f4\u63a5\u9002\u7528\u3002", "motivation": "\u6807\u51c6QKD\u5b89\u5168\u8bc1\u660e\u5047\u8bbe\u8ba4\u8bc1\u4fe1\u9053\"\u8bda\u5b9e\"\uff08\u6c38\u4e0d\u4e2d\u6b62\u3001\u6d88\u606f\u5fe0\u5b9e\u4f20\u9012\u4e14\u4fdd\u6301\u65f6\u5e8f\uff09\uff0c\u4f46\u5b9e\u9645\u8ba4\u8bc1\u4fe1\u9053\u5b58\u5728\u975e\u5bf9\u79f0\u4e2d\u6b62\uff08\u4ec5\u63a5\u6536\u65b9\u80fd\u68c0\u6d4b\u8ba4\u8bc1\u5931\u8d25\uff09\u53ca\u654c\u624b\u53ef\u5ef6\u8fdf\u3001\u91cd\u6392\u6216\u963b\u65ad\u6d88\u606f\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u73b0\u6709\u5b89\u5168\u5b9a\u4e49\u548c\u8bc1\u660e\u5728\u5b9e\u7528\u573a\u666f\u4e2d\u5931\u6548\u3002", "method": "\u63d0\u51fa\u5e76\u8bc1\u660e\u4e00\u4e2a\u7ea6\u5316\u5b9a\u7406\uff0c\u901a\u8fc7\u5c06\u5b9e\u7528\u8ba4\u8bc1\u8bbe\u7f6e\u4e0b\u7684\u6f5c\u5728\u653b\u51fb\u5f52\u7ea6\u5230\u7406\u60f3\u8ba4\u8bc1\u8bbe\u7f6e\u4e0b\u7684\u653b\u51fb\uff0c\u8bc1\u660e\u5728\u6ee1\u8db3\u7279\u5b9a\u6e29\u548c\u6761\u4ef6\u65f6\uff0c\u4e24\u79cd\u8bbe\u7f6e\u4e0b\u7684\u5b89\u5168\u6027\u7b49\u4ef7\u3002", "result": "\u83b7\u5f97\u901a\u7528\u7ea6\u5316\u5b9a\u7406\uff0c\u5141\u8bb8\u6240\u6709\u73b0\u6709QKD\u5b89\u5168\u8bc1\u660e\u901a\u8fc7\u4e00\u4e2a\u5fae\u5c0f\u7684\u534f\u8bae\u8c03\u6574\uff0c\u5373\u53ef\"\u8ffd\u6eaf\u5347\u7ea7\"\u5230\u5b9e\u7528\u8ba4\u8bc1\u8bbe\u7f6e\uff0c\u65e0\u9700\u91cd\u65b0\u8fdb\u884c\u590d\u6742\u7684\u5b89\u5168\u8bc1\u660e\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u6210\u529f\u5f25\u5408\u4e86QKD\u7406\u8bba\u5b89\u5168\u8bc1\u660e\u4e0e\u5b9e\u9645\u8ba4\u8bc1\u4fe1\u9053\u4e4b\u95f4\u7684\u5173\u952e\u9e3f\u6c9f\uff0c\u4e3aQKD\u534f\u8bae\u7684\u5b9e\u7528\u5316\u90e8\u7f72\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u969c\uff0c\u4f7f\u73b0\u6709\u5b89\u5168\u8bc1\u660e\u5177\u6709\u4e86\u5b9e\u9645\u9002\u7528\u6027\uff0c\u6781\u5927\u7b80\u5316\u4e86\u672a\u6765QKD\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u9a8c\u8bc1\u5de5\u4f5c\u3002"}}
{"id": "2601.17063", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17063", "abs": "https://arxiv.org/abs/2601.17063", "authors": ["Byeongju Kim", "Jungwan Lee", "Donghyeon Han", "Hoi-Jun Yoo", "Sangyeob Kim"], "title": "FlashMoE: Reducing SSD I/O Bottlenecks via ML-Based Cache Replacement for Mixture-of-Experts Inference on Edge Devices", "comment": null, "summary": "Recently, Mixture-of-Experts (MoE) models have gained attention for efficiently scaling large language models. Although these models are extremely large, their sparse activation enables inference to be performed by accessing only a fraction of the model at a time. This property opens the possibility of on-device inference of MoE, which was previously considered infeasible for such large models. Consequently, various systems have been proposed to leverage this sparsity and enable efficient MoE inference for edge devices. However, previous MoE inference systems like Fiddler[8] or DAOP[13] rely on DRAM-based offloading and are not suitable for memory constrained on-device environments. As recent MoE models grow to hundreds of gigabytes, RAM-offloading solutions become impractical. To address this, we propose FlashMoE, a system that offloads inactive experts to SSD, enabling efficient MoE inference under limited RAM. FlashMoE incorporates a lightweight ML-based caching strategy that adaptively combines recency and frequency signals to maximize expert reuse, significantly reducing storage I/O. In addition, we built a user-grade desktop platform to demonstrate the practicality of FlashMoE. On this real hardware setup, FlashMoE improves cache hit rate by up to 51% over well-known offloading policies such as LRU and LFU, and achieves up to 2.6x speedup compared to existing MoE inference systems.", "AI": {"tldr": "FlashMoE is a system that enables efficient Mixture-of-Experts (MoE) inference on memory-constrained devices by offloading inactive experts to SSD instead of RAM, using a lightweight ML-based caching strategy that combines recency and frequency signals to maximize expert reuse.", "motivation": "Existing MoE inference systems rely on DRAM-based offloading which becomes impractical as MoE models scale to hundreds of gigabytes, making efficient on-device inference challenging for memory-constrained edge devices.", "method": "Proposes FlashMoE, a system that offloads inactive experts to SSD with a lightweight ML-based caching strategy that adaptively combines recency and frequency signals to maximize expert reuse and reduce storage I/O.", "result": "On real hardware, FlashMoE improves cache hit rate by up to 51% over LRU/LFU policies and achieves up to 2.6x speedup compared to existing MoE inference systems.", "conclusion": "FlashMoE successfully enables practical on-device MoE inference under limited RAM by leveraging SSD offloading with intelligent caching, making large MoE models feasible for edge deployment."}}
{"id": "2601.17335", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17335", "abs": "https://arxiv.org/abs/2601.17335", "authors": ["Angshul Majumdar"], "title": "The Relativity of AGI: Distributional Axioms, Fragility, and Undecidability", "comment": null, "summary": "We study whether Artificial General Intelligence (AGI) admits a coherent theoretical definition that supports absolute claims of existence, robustness, or self-verification. We formalize AGI axiomatically as a distributional, resource-bounded semantic predicate, indexed by a task family, a task distribution, a performance functional, and explicit resource budgets. Under this framework, we derive four classes of results. First, we show that generality is inherently relational: there is no distribution-independent notion of AGI. Second, we prove non-invariance results demonstrating that arbitrarily small perturbations of the task distribution can invalidate AGI properties via cliff sets, precluding universal robustness. Third, we establish bounded transfer guarantees, ruling out unbounded generalization across task families under finite resources. Fourth, invoking Rice-style and G\u00f6del--Tarski arguments, we prove that AGI is a nontrivial semantic property and therefore cannot be soundly and completely certified by any computable procedure, including procedures implemented by the agent itself. Consequently, recursive self-improvement schemes that rely on internal self-certification of AGI are ill-posed. Taken together, our results show that strong, distribution-independent claims of AGI are not false but undefined without explicit formal indexing, and that empirical progress in AI does not imply the attainability of self-certifying general intelligence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06AGI\u5f62\u5f0f\u5316\u4e3a\u4e00\u79cd\u5206\u5e03\u5316\u3001\u8d44\u6e90\u53d7\u9650\u7684\u8bed\u4e49\u8c13\u8bcd\uff0c\u5e76\u8bc1\u660e\uff1a\u5206\u5e03\u65e0\u5173\u7684AGI\u6982\u5ff5\u662f\u672a\u5b9a\u4e49\u7684\uff0cAGI\u6027\u8d28\u4f1a\u56e0\u4efb\u52a1\u5206\u5e03\u7684\u7ec6\u5fae\u6270\u52a8\u800c\u5931\u6548\uff0c\u8de8\u4efb\u52a1\u65cf\u7684\u6cdb\u5316\u662f\u6709\u9650\u754c\u7684\uff0c\u4e14AGI\u65e0\u6cd5\u901a\u8fc7\u4efb\u4f55\u53ef\u8ba1\u7b97\u7a0b\u5e8f\uff08\u5305\u62ec\u667a\u80fd\u4f53\u81ea\u8eab\uff09\u8fdb\u884c\u5b8c\u5907\u8ba4\u8bc1\u3002", "motivation": "\u63a2\u8ba8AGI\u662f\u5426\u5b58\u5728\u4e00\u4e2a\u8fde\u8d2f\u7684\u7406\u8bba\u5b9a\u4e49\uff0c\u4ee5\u652f\u6301\u5173\u4e8e\u5b58\u5728\u6027\u3001\u9c81\u68d2\u6027\u6216\u81ea\u6211\u9a8c\u8bc1\u7684\u7edd\u5bf9\u6027\u65ad\u8a00\uff0c\u4ece\u800c\u4e3aAGI\u7684\u57fa\u7840\u7406\u8bba\u5efa\u7acb\u4e25\u683c\u7684\u6570\u5b66\u6846\u67b6\u3002", "method": "\u5c06AGI\u516c\u7406\u5316\u5730\u5f62\u5f0f\u5316\u4e3a\u5206\u5e03\u5316\u3001\u8d44\u6e90\u53d7\u9650\u7684\u8bed\u4e49\u8c13\u8bcd\uff0c\u901a\u8fc7\u4efb\u52a1\u65cf\u3001\u4efb\u52a1\u5206\u5e03\u3001\u6027\u80fd\u6cdb\u51fd\u548c\u663e\u5f0f\u8d44\u6e90\u9884\u7b97\u8fdb\u884c\u7d22\u5f15\uff1b\u8fd0\u7528Rice\u5b9a\u7406\u548cG\u00f6del-Tarski\u8bba\u8bc1\u7b49\u7406\u8bba\u5de5\u5177\u8fdb\u884c\u6570\u5b66\u8bc1\u660e\u3002", "result": "1) \u901a\u7528\u6027\u672c\u8d28\u4e0a\u662f\u5173\u7cfb\u6027\u7684\uff1a\u4e0d\u5b58\u5728\u5206\u5e03\u65e0\u5173\u7684AGI\u6982\u5ff5\uff1b2) \u975e\u4e0d\u53d8\u6027\u7ed3\u679c\uff1a\u4efb\u52a1\u5206\u5e03\u7684\u4efb\u610f\u5c0f\u6270\u52a8\u53ef\u901a\u8fc7\u60ac\u5d16\u96c6\u4f7fAGI\u6027\u8d28\u5931\u6548\uff1b3) \u6709\u9650\u8fc1\u79fb\u4fdd\u8bc1\uff1a\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u8de8\u4efb\u52a1\u65cf\u7684\u65e0\u754c\u6cdb\u5316\u88ab\u6392\u9664\uff1b4) AGI\u662f\u975e\u5e73\u51e1\u8bed\u4e49\u6027\u8d28\uff0c\u65e0\u6cd5\u88ab\u4efb\u4f55\u53ef\u8ba1\u7b97\u7a0b\u5e8f\uff08\u5305\u62ec\u667a\u80fd\u4f53\u81ea\u8eab\uff09\u5b8c\u5168\u8ba4\u8bc1\u3002", "conclusion": "\u5f3a\u5206\u5e03\u65e0\u5173\u7684AGI\u4e3b\u5f20\u5e76\u975e\u9519\u8bef\u800c\u662f\u7f3a\u4e4f\u5b9a\u4e49\uff0c\u5fc5\u987b\u663e\u5f0f\u7d22\u5f15\uff1b\u4eba\u5de5\u667a\u80fd\u7684\u5b9e\u8bc1\u8fdb\u5c55\u5e76\u4e0d\u610f\u5473\u7740\u53ef\u5b9e\u73b0\u81ea\u6211\u8ba4\u8bc1\u7684\u901a\u7528\u667a\u80fd\uff0c\u4f9d\u8d56\u5185\u90e8\u81ea\u6211\u8ba4\u8bc1\u7684\u9012\u5f52\u81ea\u6211\u6539\u8fdb\u65b9\u6848\u662f\u4e0d\u9002\u5b9a\u7684\u3002"}}
{"id": "2601.17976", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.17976", "abs": "https://arxiv.org/abs/2601.17976", "authors": ["Alexey A. Kryukov"], "title": "Quantum Paradoxes and the Quantum-Classical Transition under Unitary Measurement Dynamics with Random Hamiltonians", "comment": null, "summary": "We develop a dynamical framework for quantum measurement based on stochastic but unitary evolution in projective state space. Random Hamiltonians drawn from the Gaussian Unitary Ensemble generate stochastic unitary dynamics of the quantum state, while equivalence classes reflecting finite detector resolution define classical observables as well as classical configuration-space and phase-space submanifolds. When the evolution is constrained to the phase-space submanifold, free Schr\u00f6dinger dynamics reduces to Newtonian motion, while stochastic motion constrained to the classical configuration-space submanifold yields ordinary Brownian motion in classical space. Transition probabilities under the stochastic dynamics satisfy the Born rule, whereas the constrained classical evolution produces the normal probability distributions characteristic of classical measurements. We show that, in this setting, measurement, state reduction, and the quantum-classical transition emerge from unitary dynamics alone, without invoking nonunitary collapse or additional postulates. Entanglement and EPR correlations arise geometrically from the evolution of joint states in composite state space, preserving locality in spacetime. The framework provides a unified dynamical account of measurement and classicality compatible with the structure of quantum mechanics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u9149\u6f14\u5316\u7684\u91cf\u5b50\u6d4b\u91cf\u52a8\u529b\u5b66\u6846\u67b6\uff0c\u5229\u7528\u968f\u673a\u54c8\u5bc6\u987f\u91cf\u548c\u63a2\u6d4b\u5668\u5206\u8fa8\u7387\u7ea6\u675f\uff0c\u5728\u4e0d\u5f15\u5165\u6ce2\u51fd\u6570\u574d\u7f29\u7684\u60c5\u51b5\u4e0b\uff0c\u7edf\u4e00\u89e3\u91ca\u4e86\u91cf\u5b50\u6d4b\u91cf\u3001\u6001\u7ea6\u5316\u548c\u91cf\u5b50-\u7ecf\u5178\u8fc7\u6e21\u73b0\u8c61\u3002", "motivation": "\u89e3\u51b3\u91cf\u5b50\u6d4b\u91cf\u95ee\u9898\uff0c\u901a\u8fc7\u7eaf\u9149\u52a8\u529b\u5b66\u89e3\u91ca\u6d4b\u91cf\u3001\u6001\u7ea6\u5316\u53ca\u91cf\u5b50-\u7ecf\u5178\u8fc7\u6e21\uff0c\u907f\u514d\u975e\u9149\u574d\u7f29\u6216\u5176\u4ed6\u989d\u5916\u5047\u8bbe\u3002", "method": "\u5728\u6295\u5f71\u6001\u7a7a\u95f4\u4e2d\u6784\u5efa\u968f\u673a\u4f46\u9149\u7684\u6f14\u5316\u6846\u67b6\uff0c\u91c7\u7528\u9ad8\u65af\u9149\u7cfb\u7efc\u7684\u968f\u673a\u54c8\u5bc6\u987f\u91cf\u751f\u6210\u91cf\u5b50\u6001\u7684\u968f\u673a\u9149\u52a8\u529b\u5b66\uff0c\u5e76\u901a\u8fc7\u6709\u9650\u63a2\u6d4b\u5668\u5206\u8fa8\u7387\u7684\u7b49\u4ef7\u7c7b\u5b9a\u4e49\u7ecf\u5178\u53ef\u89c2\u6d4b\u91cf\u53ca\u7ecf\u5178\u4f4d\u5f62/\u76f8\u7a7a\u95f4\u5b50\u6d41\u5f62\u3002", "result": "\u76f8\u7a7a\u95f4\u5b50\u6d41\u5f62\u7ea6\u675f\u4e0b\u81ea\u7531\u859b\u5b9a\u8c14\u52a8\u529b\u5b66\u9000\u5316\u4e3a\u725b\u987f\u8fd0\u52a8\uff0c\u4f4d\u5f62\u7a7a\u95f4\u5b50\u6d41\u5f62\u7ea6\u675f\u4e0b\u968f\u673a\u8fd0\u52a8\u4ea7\u751f\u7ecf\u5178\u5e03\u6717\u8fd0\u52a8\uff1b\u968f\u673a\u52a8\u529b\u5b66\u7684\u8dc3\u8fc1\u6982\u7387\u6ee1\u8db3\u73bb\u6069\u89c4\u5219\uff0c\u7ea6\u675f\u7ecf\u5178\u6f14\u5316\u4ea7\u751f\u7ecf\u5178\u6d4b\u91cf\u7684\u9ad8\u65af\u5206\u5e03\uff1b\u7ea0\u7f20\u548cEPR\u5173\u8054\u4ece\u590d\u5408\u6001\u7a7a\u95f4\u7684\u51e0\u4f55\u6f14\u5316\u4e2d\u6d8c\u73b0\uff0c\u540c\u65f6\u4fdd\u6301\u65f6\u7a7a\u5b9a\u57df\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e0e\u91cf\u5b50\u529b\u5b66\u7ed3\u6784\u76f8\u5bb9\u7684\u6d4b\u91cf\u4e0e\u7ecf\u5178\u6027\u7edf\u4e00\u52a8\u529b\u5b66\u89e3\u91ca\uff0c\u8bc1\u660e\u6d4b\u91cf\u3001\u6001\u7ea6\u5316\u548c\u91cf\u5b50-\u7ecf\u5178\u8fc7\u6e21\u53ef\u5b8c\u5168\u4ece\u9149\u52a8\u529b\u5b66\u4e2d\u6d8c\u73b0\u3002"}}
{"id": "2601.17065", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.17065", "abs": "https://arxiv.org/abs/2601.17065", "authors": ["Haoxuan Li", "He Chang", "Yunshan Ma", "Yi Bin", "Yang Yang", "See-Kiong Ng", "Tat-Seng Chua"], "title": "ThinkTank-ME: A Multi-Expert Framework for Middle East Event Forecasting", "comment": null, "summary": "Event forecasting is inherently influenced by multifaceted considerations, including international relations, regional historical dynamics, and cultural contexts. However, existing LLM-based approaches employ single-model architectures that generate predictions along a singular explicit trajectory, constraining their ability to capture diverse geopolitical nuances across complex regional contexts. To address this limitation, we introduce ThinkTank-ME, a novel Think Tank framework for Middle East event forecasting that emulates collaborative expert analysis in real-world strategic decision-making. To facilitate expert specialization and rigorous evaluation, we construct POLECAT-FOR-ME, a Middle East-focused event forecasting benchmark. Experimental results demonstrate the superiority of multi-expert collaboration in handling complex temporal geopolitical forecasting tasks. The code is available at https://github.com/LuminosityX/ThinkTank-ME.", "AI": {"tldr": "\u63d0\u51faThinkTank-ME\u591a\u4e13\u5bb6\u534f\u4f5c\u6846\u67b6\u7528\u4e8e\u4e2d\u4e1c\u4e8b\u4ef6\u9884\u6d4b\uff0c\u901a\u8fc7\u6a21\u62df\u771f\u5b9e\u667a\u5e93\u5206\u6790\u6a21\u5f0f\uff0c\u514b\u670d\u5355\u6a21\u578b\u67b6\u6784\u5728\u590d\u6742\u5730\u7f18\u653f\u6cbb\u8bed\u5883\u4e0b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709LLM-based\u4e8b\u4ef6\u9884\u6d4b\u65b9\u6cd5\u91c7\u7528\u5355\u6a21\u578b\u67b6\u6784\uff0c\u751f\u6210\u5355\u4e00\u660e\u786e\u8f68\u8ff9\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u590d\u6742\u5730\u533a\u8bed\u5883\u4e0b\u7684\u591a\u5143\u5730\u7f18\u653f\u6cbb\u7ec6\u5fae\u5dee\u522b\u3002", "method": "\u63d0\u51faThinkTank-ME\u6846\u67b6\uff0c\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u6218\u7565\u51b3\u7b56\u4e2d\u7684\u534f\u4f5c\u4e13\u5bb6\u5206\u6790\uff1b\u6784\u5efa\u4e2d\u4e1c\u805a\u7126\u7684\u4e8b\u4ef6\u9884\u6d4b\u57fa\u51c6POLECAT-FOR-ME\uff0c\u4fc3\u8fdb\u4e13\u5bb6\u4e13\u4e1a\u5316\u4e0e\u4e25\u683c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u591a\u4e13\u5bb6\u534f\u4f5c\u6a21\u5f0f\u4e0b\u5904\u7406\u590d\u6742\u65f6\u5e8f\u5730\u7f18\u653f\u6cbb\u9884\u6d4b\u4efb\u52a1\u5177\u6709\u4f18\u8d8a\u6027\u3002", "conclusion": "\u591a\u4e13\u5bb6\u534f\u4f5c\u6846\u67b6\u6bd4\u5355\u6a21\u578b\u67b6\u6784\u66f4\u9002\u5408\u590d\u6742\u5730\u7f18\u653f\u6cbb\u4e8b\u4ef6\u9884\u6d4b\uff0c\u4e3a\u4e8b\u4ef6 forecasting \u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u8bba\u3002"}}
{"id": "2601.17343", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17343", "abs": "https://arxiv.org/abs/2601.17343", "authors": ["Wei Liu", "Haomei Xu", "Hongkai Liu", "Zhiying Deng", "Ruixuan Li", "Heng Huang", "Yee Whye Teh", "Wee Sun Lee"], "title": "Are We Evaluating the Edit Locality of LLM Model Editing Properly?", "comment": null, "summary": "Model editing has recently emerged as a popular paradigm for efficiently updating knowledge in LLMs. A central desideratum of updating knowledge is to balance editing efficacy, i.e., the successful injection of target knowledge, and specificity (also known as edit locality), i.e., the preservation of existing non-target knowledge. However, we find that existing specificity evaluation protocols are inadequate for this purpose. We systematically elaborated on the three fundamental issues it faces. Beyond the conceptual issues, we further empirically demonstrate that existing specificity metrics are weakly correlated with the strength of specificity regularizers. We also find that current metrics lack sufficient sensitivity, rendering them ineffective at distinguishing the specificity performance of different methods. Finally, we propose a constructive evaluation protocol. Under this protocol, the conflict between open-ended LLMs and the assumption of determined answers is eliminated, query-independent fluency biases are avoided, and the evaluation strictness can be smoothly adjusted within a near-continuous space. Experiments across various LLMs, datasets, and editing methods show that metrics derived from the proposed protocol are more sensitive to changes in the strength of specificity regularizers and exhibit strong correlation with them, enabling more fine-grained discrimination of different methods' knowledge preservation capabilities.", "AI": {"tldr": "This paper identifies fundamental flaws in current specificity evaluation metrics for LLM model editing and proposes a new evaluation protocol that is more sensitive and correlates better with specificity regularizers, enabling fine-grained discrimination between editing methods' knowledge preservation capabilities.", "motivation": "Existing specificity (edit locality) evaluation protocols for model editing are inadequate\u2014they have conceptual issues, show weak correlation with specificity regularizers, and lack sufficient sensitivity to distinguish performance differences between editing methods.", "method": "Proposes a constructive evaluation protocol that eliminates conflicts between open-ended LLM responses and determined answer assumptions, avoids query-independent fluency biases, and allows adjustable evaluation strictness within a near-continuous parameter space.", "result": "Experiments across multiple LLMs, datasets, and editing methods demonstrate that the proposed metrics are more sensitive to changes in specificity regularizer strength, show strong correlation with them, and enable better discrimination of knowledge preservation capabilities across different methods.", "conclusion": "The new evaluation protocol provides a more reliable and sensitive framework for assessing specificity in model editing, which should significantly improve the development and evaluation of future knowledge editing techniques."}}
{"id": "2601.18024", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.18024", "abs": "https://arxiv.org/abs/2601.18024", "authors": ["Peter Brearley", "Thomas Howarth"], "title": "Linear combination of unitaries with exponential convergence", "comment": null, "summary": "We present a general method for decomposing non-unitary operators into a linear combination of unitary operators, where the approximation error decays exponentially. The decomposition is based on a smooth periodic extension of the identity map via the Fourier extension method, resulting in a sine series with exponentially decaying coefficients. Rewriting the sine series in terms of complex exponentials, then evaluating it on the Hermitian and anti-Hermitian parts of a non-unitary operator, yields its approximation by a linear combination of unitaries. When implemented in a quantum circuit, the subnormalisation of the resulting block encoding scales with the double logarithm of the inverse error, substantially improving over the polynomial relationship in existing methods. For hardware or applications with a fixed error budget, we discuss a strategy to minimise subnormalisation by exploiting the overcomplete nature of the Fourier extension basis. This regularisation procedure traces an error-subnormalisation Pareto front, identifying coefficients that maximise the subnormalisation at a fixed error budget. Fourier linear combinations of unitaries thus provides an accurate and versatile framework for non-unitary quantum computing.", "AI": {"tldr": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5085\u91cc\u53f6\u6269\u5c55\u5c06\u975e\u9149\u7b97\u7b26\u5206\u89e3\u4e3a\u9149\u7b97\u7b26\u7684\u7ebf\u6027\u7ec4\u5408\uff0c\u5b9e\u73b0\u8bef\u5dee\u6307\u6570\u8870\u51cf\u548c\u5b50\u5f52\u4e00\u5316\u53cc\u5bf9\u6570\u7f29\u653e\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u591a\u9879\u5f0f\u7f29\u653e\uff0c\u4e3a\u975e\u9149\u91cf\u5b50\u8ba1\u7b97\u63d0\u4f9b\u4e86\u9ad8\u6548\u6846\u67b6\u3002", "motivation": "\u975e\u9149\u7b97\u7b26\u5728\u91cf\u5b50\u8ba1\u7b97\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7684\u5b50\u5f52\u4e00\u5316\u7f29\u653e\u4e0e\u8bef\u5dee\u5448\u591a\u9879\u5f0f\u5173\u7cfb\uff0c\u6548\u7387\u8f83\u4f4e\u3002\u672c\u6587\u65e8\u5728\u5bfb\u627e\u4e00\u79cd\u8bef\u5dee\u6307\u6570\u8870\u51cf\u4e14\u5b50\u5f52\u4e00\u5316\u7f29\u653e\u66f4\u4f18\u7684\u5206\u89e3\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u5085\u91cc\u53f6\u6269\u5c55\u65b9\u6cd5\uff0c\u901a\u8fc7\u5149\u6ed1\u5468\u671f\u5ef6\u62d3\u6052\u7b49\u6620\u5c04\uff0c\u6784\u9020\u5177\u6709\u6307\u6570\u8870\u51cf\u7cfb\u6570\u7684\u6b63\u5f26\u7ea7\u6570\uff1b\u5c06\u8be5\u7ea7\u6570\u6539\u5199\u4e3a\u590d\u6307\u6570\u5f62\u5f0f\u540e\uff0c\u4f5c\u7528\u4e8e\u975e\u9149\u7b97\u7b26\u7684\u57c3\u5c14\u7c73\u7279\u90e8\u5206\u548c\u53cd\u57c3\u5c14\u7c73\u7279\u90e8\u5206\uff0c\u5b9e\u73b0\u7528\u9149\u7b97\u7b26\u7ebf\u6027\u7ec4\u5408\u903c\u8fd1\u975e\u9149\u7b97\u7b26\u3002", "result": "\u903c\u8fd1\u8bef\u5dee\u5448\u6307\u6570\u8870\u51cf\uff1b\u5728\u91cf\u5b50\u7535\u8def\u4e2d\u5b9e\u73b0\u65f6\uff0c\u5b50\u5f52\u4e00\u5316\u7cfb\u6570\u4e0e\u9006\u8bef\u5dee\u5448\u53cc\u5bf9\u6570\u5173\u7cfb\uff08log(log(1/\u03b5))\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u591a\u9879\u5f0f\u5173\u7cfb\uff1b\u5229\u7528\u5085\u91cc\u53f6\u6269\u5c55\u57fa\u7684\u8fc7\u5b8c\u5907\u6027\uff0c\u63d0\u51fa\u4e86\u5728\u56fa\u5b9a\u8bef\u5dee\u9884\u7b97\u4e0b\u6700\u5c0f\u5316\u5b50\u5f52\u4e00\u5316\u7684\u7b56\u7565\uff0c\u5e76\u7ed8\u5236\u4e86\u8bef\u5dee-\u5b50\u5f52\u4e00\u5316\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "conclusion": "\u5085\u91cc\u53f6\u7ebf\u6027\u9149\u7b97\u7b26\u7ec4\u5408\u4e3a\u9149\u91cf\u5b50\u8ba1\u7b97\u63d0\u4f9b\u4e86\u7cbe\u786e\u4e14\u901a\u7528\u7684\u6846\u67b6\uff0c\u5728\u8bef\u5dee\u63a7\u5236\u548c\u8d44\u6e90\u6548\u7387\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2601.17069", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17069", "abs": "https://arxiv.org/abs/2601.17069", "authors": ["Shahil Shaik", "Jonathon M. Smereka", "Yue Wang"], "title": "Multi-Agent Deep Reinforcement Learning Under Constrained Communications", "comment": "21 pages, 8 figures, Under review at ICLR", "summary": "Centralized training with decentralized execution (CTDE) has been the dominant paradigm in multi-agent reinforcement learning (MARL), but its reliance on global state information during training introduces scalability, robustness, and generalization bottlenecks. Moreover, in practical scenarios such as adding/dropping teammates or facing environment dynamics that differ from the training, CTDE methods can be brittle and costly to retrain, whereas distributed approaches allow agents to adapt using only local information and peer-to-peer communication. We present a distributed MARL framework that removes the need for centralized critics or global information. Firstly, we develop a novel Distributed Graph Attention Network (D-GAT) that performs global state inference through multi-hop communication, where agents integrate neighbor features via input-dependent attention weights in a fully distributed manner. Leveraging D-GAT, we develop the distributed graph-attention MAPPO (DG-MAPPO) -- a distributed MARL framework where agents optimize local policies and value functions using local observations, multi-hop communication, and shared/averaged rewards. Empirical evaluation on the StarCraftII Multi-Agent Challenge, Google Research Football, and Multi-Agent Mujoco demonstrates that our method consistently outperforms strong CTDE baselines, achieving superior coordination across a wide range of cooperative tasks with both homogeneous and heterogeneous teams. Our distributed MARL framework provides a principled and scalable solution for robust collaboration, eliminating the need for centralized training or global observability. To the best of our knowledge, DG-MAPPO appears to be the first to fully eliminate reliance on privileged centralized information, enabling agents to learn and act solely through peer-to-peer communication.", "AI": {"tldr": "A distributed MARL framework (DG-MAPPO) using graph attention networks (D-GAT) that outperforms CTDE methods by enabling agents to learn and act solely through local observations and peer-to-peer communication without centralized critics or global state information.", "motivation": "CTDE paradigm has scalability, robustness, and generalization bottlenecks due to reliance on global state during training, and is brittle when teammates change or environments differ from training. Distributed approaches allow adaptation using only local information and peer-to-peer communication.", "method": "Developed a Distributed Graph Attention Network (D-GAT) for global state inference through multi-hop communication with input-dependent attention weights. Created DG-MAPPO where agents optimize local policies/value functions using local observations, multi-hop communication, and shared/averaged rewards, eliminating centralized critics.", "result": "Empirical evaluation on StarCraftII Multi-Agent Challenge, Google Research Football, and Multi-Agent Mujoco shows consistent outperformance over strong CTDE baselines, achieving superior coordination across cooperative tasks with both homogeneous and heterogeneous teams.", "conclusion": "The distributed MARL framework provides a principled and scalable solution for robust collaboration, eliminating the need for centralized training or global observability. DG-MAPPO is the first to fully eliminate reliance on privileged centralized information, enabling agents to learn and act solely through peer-to-peer communication."}}
{"id": "2601.17346", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17346", "abs": "https://arxiv.org/abs/2601.17346", "authors": ["Haoxin Xu", "Changyong Qi", "Tong Liu", "Bohao Zhang", "Anna He", "Bingqian Jiang", "Longwei Zheng", "Xiaoqing Gu"], "title": "Multi-Agent Learning Path Planning via LLMs", "comment": null, "summary": "The integration of large language models (LLMs) into intelligent tutoring systems offers transformative potential for personalized learning in higher education. However, most existing learning path planning approaches lack transparency, adaptability, and learner-centered explainability. To address these challenges, this study proposes a novel Multi-Agent Learning Path Planning (MALPP) framework that leverages a role- and rule-based collaboration mechanism among intelligent agents, each powered by LLMs. The framework includes three task-specific agents: a learner analytics agent, a path planning agent, and a reflection agent. These agents collaborate via structured prompts and predefined rules to analyze learning profiles, generate tailored learning paths, and iteratively refine them with interpretable feedback. Grounded in Cognitive Load Theory and Zone of Proximal Development, the system ensures that recommended paths are cognitively aligned and pedagogically meaningful. Experiments conducted on the MOOCCubeX dataset using seven LLMs show that MALPP significantly outperforms baseline models in path quality, knowledge sequence consistency, and cognitive load alignment. Ablation studies further validate the effectiveness of the collaborative mechanism and theoretical constraints. This research contributes to the development of trustworthy, explainable AI in education and demonstrates a scalable approach to learner-centered adaptive instruction powered by LLMs.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u8def\u5f84\u89c4\u5212\u6846\u67b6(MALPP)\uff0c\u901a\u8fc7\u89d2\u8272\u89c4\u5219\u534f\u4f5c\u673a\u5236\u5b9e\u73b0\u900f\u660e\u3001\u81ea\u9002\u5e94\u3001\u53ef\u89e3\u91ca\u7684\u4e2a\u6027\u5316\u6559\u5b66", "motivation": "\u73b0\u6709\u5b66\u4e60\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u7f3a\u4e4f\u900f\u660e\u5ea6\u3001\u9002\u5e94\u6027\u548c\u5b66\u4e60\u8005\u4e2d\u5fc3\u7684\u53ef\u89e3\u91ca\u6027\uff0cLLM\u5728\u6559\u80b2\u9886\u57df\u5177\u6709\u53d8\u9769\u6f5c\u529b\u4f46\u5e94\u7528\u4e0d\u8db3", "method": "\u6784\u5efa\u4e09\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff1a\u5b66\u4e60\u8005\u5206\u6790\u667a\u80fd\u4f53\u3001\u8def\u5f84\u89c4\u5212\u667a\u80fd\u4f53\u3001\u53cd\u601d\u667a\u80fd\u4f53\uff0c\u57fa\u4e8e\u8ba4\u77e5\u8d1f\u8377\u7406\u8bba\u548c\u6700\u8fd1\u53d1\u5c55\u533a\u7406\u8bba\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u548c\u9884\u5b9a\u4e49\u89c4\u5219\u534f\u4f5c", "result": "\u5728MOOCCubeX\u6570\u636e\u96c6\u4e0a\uff0cMALPP\u5728\u8def\u5f84\u8d28\u91cf\u3001\u77e5\u8bc6\u5e8f\u5217\u4e00\u81f4\u6027\u548c\u8ba4\u77e5\u8d1f\u8377\u5bf9\u9f50\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u534f\u4f5c\u673a\u5236\u548c\u7406\u8bba\u7ea6\u675f\u7684\u6709\u6548\u6027", "conclusion": "\u4e3a\u53ef\u4fe1\u53ef\u89e3\u91caAI\u6559\u80b2\u5e94\u7528\u63d0\u4f9b\u65b0\u65b9\u6848\uff0c\u5c55\u793a\u4e86LLM\u9a71\u52a8\u7684\u89c4\u6a21\u5316\u5b66\u4e60\u8005\u4e2d\u5fc3\u81ea\u9002\u5e94\u6559\u5b66\u7684\u53ef\u884c\u8def\u5f84"}}
{"id": "2601.18035", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.18035", "abs": "https://arxiv.org/abs/2601.18035", "authors": ["Devashish Tupkary", "Shlok Nahar", "Amir Arqand", "Ernest Y. -Z. Tan", "Norbert L\u00fctkenhaus"], "title": "A rigorous and complete security proof of decoy-state BB84 quantum key distribution", "comment": null, "summary": "We present a rigorous and complete security proof of the decoy-state BB84 quantum key distribution (QKD) protocol. Our analysis aims to achieve a high standard of mathematical rigour and completeness, thereby providing the necessary foundation for certification and standardization efforts. Beyond establishing the security of a specific protocol, this work develops a general and modular framework that can be readily adapted to a broad class of QKD protocols, including both prepare-and-measure and entanglement-based variants. Our framework unifies all major ingredients required for the analysis of realistic QKD protocols, including the analysis of classical authentication and classical processing, source-replacement schemes, finite-size analysis, source maps, squashing maps, and decoy-state techniques. In doing so, this work consolidates a diverse range of techniques scattered across the QKD literature into a unified formalism, representing a general and rigorous treatment of QKD security. Finally, it outlines a clear path towards incorporating practical imperfections within the same framework, thereby laying the groundwork for addressing implementation security in future analysis.", "AI": {"tldr": "\u672c\u6587\u4e3a\u8bf1\u9a97\u6001BB84\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1(QKD)\u534f\u8bae\u63d0\u4f9b\u4e86\u4e25\u683c\u4e14\u5b8c\u6574\u7684\u5b89\u5168\u6027\u8bc1\u660e\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u901a\u7528\u6a21\u5757\u5316\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u591a\u79cdQKD\u5206\u6790\u6280\u672f\uff0c\u4e3a\u534f\u8bae\u8ba4\u8bc1\u548c\u540e\u7eed\u5b9e\u73b0\u5b89\u5168\u6027\u5206\u6790\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u73b0\u6709QKD\u5b89\u5168\u8bc1\u660e\u5728\u6570\u5b66\u4e25\u8c28\u6027\u548c\u5b8c\u6574\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u96be\u4ee5\u6ee1\u8db3\u8ba4\u8bc1\u548c\u6807\u51c6\u5316\u7684\u8981\u6c42\u3002\u672c\u7814\u7a76\u65e8\u5728\u5efa\u7acb\u9ad8\u6807\u51c6\u7684\u5b89\u5168\u6027\u8bc1\u660e\uff0c\u4e3aQKD\u534f\u8bae\u7684\u8ba4\u8bc1\u548c\u6807\u51c6\u5316\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u901a\u8fc7\u5f00\u53d1\u901a\u7528\u6a21\u5757\u5316\u6846\u67b6\uff0c\u5c06\u7ecf\u5178\u8ba4\u8bc1\u3001\u7ecf\u5178\u540e\u5904\u7406\u3001\u6e90\u66ff\u6362\u65b9\u6848\u3001\u6709\u9650\u5bc6\u94a5\u5206\u6790\u3001\u6e90\u6620\u5c04\u3001\u538b\u7f29\u6620\u5c04\u53ca\u8bf1\u9a97\u6001\u6280\u672f\u7b49\u5173\u952e\u8981\u7d20\u6574\u5408\u4e3a\u7edf\u4e00\u7684\u5f62\u5f0f\u5316\u4f53\u7cfb\uff0c\u9002\u7528\u4e8eprepare-and-measure\u548centanglement-based\u4e24\u7c7bQKD\u534f\u8bae\u3002", "result": "\u6210\u529f\u5b8c\u6210\u4e86\u8bf1\u9a97\u6001BB84\u534f\u8bae\u7684\u4e25\u683c\u5b89\u5168\u6027\u8bc1\u660e\uff0c\u6784\u5efa\u4e86\u53ef\u7075\u6d3b\u9002\u914d\u591a\u79cdQKD\u53d8\u4f53\u7684\u7edf\u4e00\u5206\u6790\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u5bf9QKD\u5b89\u5168\u6027\u7684\u7cfb\u7edf\u5316\u548c\u4e25\u8c28\u5316\u5904\u7406\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c06\u5206\u6563\u7684QKD\u5206\u6790\u6280\u672f\u6574\u5408\u4e3a\u5b8c\u6574\u4f53\u7cfb\uff0c\u5e76\u6307\u660e\u4e86\u7eb3\u5165\u5b9e\u9645\u5668\u4ef6\u4e0d\u5b8c\u7f8e\u6027\u7684\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u672a\u6765\u5b9e\u73b0\u5b89\u5168\u6027\u5206\u6790\u548c\u5de5\u7a0b\u5316\u8ba4\u8bc1\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2601.17073", "categories": ["cs.LG", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.17073", "abs": "https://arxiv.org/abs/2601.17073", "authors": ["Yifei Zhang", "Meimei Liu", "Zhengwu Zhang"], "title": "Attention-Based Variational Framework for Joint and Individual Components Learning with Applications in Brain Network Analysis", "comment": null, "summary": "Brain organization is increasingly characterized through multiple imaging modalities, most notably structural connectivity (SC) and functional connectivity (FC). Integrating these inherently distinct yet complementary data sources is essential for uncovering the cross-modal patterns that drive behavioral phenotypes. However, effective integration is hindered by the high dimensionality and non-linearity of connectome data, complex non-linear SC-FC coupling, and the challenge of disentangling shared information from modality-specific variations. To address these issues, we propose the Cross-Modal Joint-Individual Variational Network (CM-JIVNet), a unified probabilistic framework designed to learn factorized latent representations from paired SC-FC datasets. Our model utilizes a multi-head attention fusion module to capture non-linear cross-modal dependencies while isolating independent, modality-specific signals. Validated on Human Connectome Project Young Adult (HCP-YA) data, CM-JIVNet demonstrates superior performance in cross-modal reconstruction and behavioral trait prediction. By effectively disentangling joint and individual feature spaces, CM-JIVNet provides a robust, interpretable, and scalable solution for large-scale multimodal brain analysis.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faCM-JIVNet\u6982\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u89e3\u51b3\u7ed3\u6784\u8fde\u63a5(SC)\u4e0e\u529f\u80fd\u8fde\u63a5(FC)\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u96be\u9898\uff0c\u5728HCP-YA\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4f18\u5f02\u7684\u8de8\u6a21\u6001\u91cd\u5efa\u4e0e\u884c\u4e3a\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u6574\u5408SC\u4e0eFC\u6570\u636e\u5bf9\u7406\u89e3\u8111\u7ec4\u7ec7\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u9ad8\u7ef4\u975e\u7ebf\u6027\u3001\u590d\u6742\u8026\u5408\u5173\u7cfb\u4ee5\u53ca\u96be\u4ee5\u5206\u79bb\u5171\u4eab\u4e0e\u7279\u5f02\u6027\u4fe1\u606f\u4e09\u5927\u6838\u5fc3\u6311\u6218\u3002", "method": "\u5f00\u53d1Cross-Modal Joint-Individual Variational Network (CM-JIVNet)\uff0c\u91c7\u7528\u591a\u5934\u90e8\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\u6355\u6349\u975e\u7ebf\u6027\u8de8\u6a21\u6001\u4f9d\u8d56\uff0c\u540c\u65f6\u5206\u79bb\u6a21\u6001\u72ec\u7acb\u4fe1\u53f7\uff0c\u5b66\u4e60\u56e0\u5b50\u5316\u6f5c\u5728\u8868\u5f81\u3002", "result": "\u57fa\u4e8eHCP-YA\u6570\u636e\u9a8c\u8bc1\uff0cCM-JIVNet\u5728\u8de8\u6a21\u6001\u91cd\u5efa\u7cbe\u5ea6\u548c\u884c\u4e3a\u7279\u5f81\u9884\u6d4b\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u8026\u8054\u5408\u4e0e\u4e2a\u4f53\u7279\u5f81\u7a7a\u95f4\u3002", "conclusion": "CM-JIVNet\u4e3a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u8111\u8fde\u63a5\u7ec4\u5206\u6790\u63d0\u4f9b\u4e86\u7a33\u5065\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u6269\u5c55\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17348", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17348", "abs": "https://arxiv.org/abs/2601.17348", "authors": ["Srikant Panda", "Sourabh Singh Yadav", "Palkesh Malviya"], "title": "Auditing Disability Representation in Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) are increasingly deployed in socially sensitive applications, yet their behavior with respect to disability remains underexplored. We study disability aware descriptions for person centric images, where models often transition from evidence grounded factual description to interpretation shift including introduction of unsupported inferences beyond observable visual evidence. To systematically analyze this phenomenon, we introduce a benchmark based on paired Neutral Prompts (NP) and Disability-Contextualised Prompts (DP) and evaluate 15 state-of-the-art open- and closed-source VLMs under a zero-shot setting across 9 disability categories. Our evaluation framework treats interpretive fidelity as core objective and combines standard text-based metrics capturing affective degradation through shifts in sentiment, social regard and response length with an LLM-as-judge protocol, validated by annotators with lived experience of disability. We find that introducing disability context consistently degrades interpretive fidelity, inducing interpretation shifts characterised by speculative inference, narrative elaboration, affective degradation and deficit oriented framing. These effects are further amplified along race and gender dimension. Finally, we demonstrate targeted prompting and preference fine-tuning effectively improves interpretive fidelity and reduces substantially interpretation shifts.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u5206\u6790\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u5728\u63cf\u8ff0\u6b8b\u969c\u4eba\u7269\u56fe\u50cf\u65f6\u7684\u89e3\u91ca\u504f\u5dee\uff0c\u53d1\u73b0\u6dfb\u52a0\u6b8b\u969c\u80cc\u666f\u4f1a\u5bfc\u81f4\u6a21\u578b\u4ece\u5ba2\u89c2\u4e8b\u5b9e\u63cf\u8ff0\u8f6c\u5411\u65e0\u6839\u636e\u63a8\u6d4b\uff0c\u51fa\u73b0\u60c5\u611f\u9000\u5316\u548c\u7f3a\u9677\u5bfc\u5411\u6846\u67b6\uff0c\u4e14\u8fd9\u79cd\u5f71\u54cd\u56e0\u79cd\u65cf\u548c\u6027\u522b\u800c\u52a0\u5267\uff0c\u4f46\u53ef\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u548c\u504f\u597d\u5fae\u8c03\u6709\u6548\u7f13\u89e3\u3002", "motivation": "\u968f\u7740\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u793e\u4f1a\u654f\u611f\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u65e5\u76ca\u589e\u591a\uff0c\u5176\u5bf9\u6b8b\u969c\u4eba\u58eb\u7684\u884c\u4e3a\u8868\u73b0\u5374\u7f3a\u4e4f\u5145\u5206\u7814\u7a76\uff0c\u6a21\u578b\u5728\u4eba\u7269\u4e2d\u5fc3\u56fe\u50cf\u63cf\u8ff0\u4e2d\u5e38\u4ece\u57fa\u4e8e\u8bc1\u636e\u7684\u4e8b\u5b9e\u63cf\u8ff0\u8f6c\u5411\u5f15\u5165\u8d85\u51fa\u89c6\u89c9\u8bc1\u636e\u7684\u65e0\u6839\u636e\u63a8\u65ad\u3002", "method": "\u6784\u5efa\u4e86\u57fa\u4e8e\u4e2d\u6027\u63d0\u793a(NP)\u4e0e\u6b8b\u969c\u60c5\u5883\u63d0\u793a(DP)\u914d\u5bf9\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u57289\u7c7b\u6b8b\u969c\u7c7b\u522b\u4e0a\u5bf915\u4e2a\u5148\u8fdb\u5f00\u6e90\u548c\u95ed\u6e90VLMs\u8fdb\u884c\u96f6\u6837\u672c\u8bc4\u4f30\uff0c\u91c7\u7528\u7ed3\u5408\u6587\u672c\u6307\u6807(\u60c5\u611f\u3001\u793e\u4f1a\u5c0a\u91cd\u3001\u54cd\u5e94\u957f\u5ea6)\u548cLLM-as-judge\u534f\u8bae(\u7ecf\u6b8b\u969c\u7ecf\u5386\u8005\u9a8c\u8bc1)\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u8861\u91cf\u89e3\u91ca\u4fdd\u771f\u5ea6\u3002", "result": "\u5f15\u5165\u6b8b\u969c\u80cc\u666f\u6301\u7eed\u964d\u4f4e\u89e3\u91ca\u4fdd\u771f\u5ea6\uff0c\u5bfc\u81f4\u63a8\u6d4b\u6027\u63a8\u65ad\u3001\u53d9\u4e8b\u9610\u8ff0\u3001\u60c5\u611f\u9000\u5316\u548c\u7f3a\u9677\u5bfc\u5411\u6846\u67b6\u7b49\u95ee\u9898\uff0c\u4e14\u8fd9\u4e9b\u5f71\u54cd\u5728\u79cd\u65cf\u548c\u6027\u522b\u7ef4\u5ea6\u4e0a\u8fdb\u4e00\u6b65\u653e\u5927\u3002", "conclusion": "\u6709\u9488\u5bf9\u6027\u7684\u63d0\u793a\u5de5\u7a0b\u548c\u504f\u597d\u5fae\u8c03\u80fd\u663e\u8457\u63d0\u9ad8\u89e3\u91ca\u4fdd\u771f\u5ea6\u5e76\u5927\u5e45\u51cf\u5c11\u89e3\u91ca\u504f\u5dee\uff0c\u4e3a\u63d0\u5347VLMs\u7684\u6b8b\u969c\u5305\u5bb9\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17426", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.17426", "abs": "https://arxiv.org/abs/2601.17426", "authors": ["Zhengqing Zang", "Yuqi Ding", "Yanmei Gu", "Changkai Song", "Zhengkai Yang", "Guoping Du", "Junbo Zhao", "Haobo Wang"], "title": "A Syllogistic Probe: Tracing the Evolution of Logic Reasoning in Large Language Models", "comment": null, "summary": "Human logic has gradually shifted from intuition-driven inference to rigorous formal systems. Motivated by recent advances in large language models (LLMs), we explore whether LLMs exhibit a similar evolution in the underlying logical framework. Using existential import as a probe, we for evaluate syllogism under traditional and modern logic. Through extensive experiments of testing SOTA LLMs on a new syllogism dataset, we have some interesting findings: (i) Model size scaling promotes the shift toward modern logic; (ii) Thinking serves as an efficient accelerator beyond parameter scaling; (iii) the Base model plays a crucial role in determining how easily and stably this shift can emerge. Beyond these core factors, we conduct additional experiments for in-depth analysis of properties of current LLMs on syllogistic reasoning.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b58\u5728\u9884\u8bbe\u8fd9\u4e00\u63a2\u9488\uff0c\u63a2\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u50cf\u4eba\u7c7b\u903b\u8f91\u4e00\u6837\u4ece\u76f4\u89c9\u63a8\u7406\u5411\u5f62\u5f0f\u5316\u7cfb\u7edf\u6f14\u8fdb\uff0c\u53d1\u73b0\u6a21\u578b\u89c4\u6a21\u3001\u601d\u7ef4\u80fd\u529b\u548c\u57fa\u5ea7\u6a21\u578b\u662f\u63a8\u52a8\u8fd9\u4e00\u8f6c\u53d8\u7684\u5173\u952e\u56e0\u7d20\u3002", "motivation": "\u53d7\u8fd1\u671f\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u5c55\u542f\u53d1\uff0c\u63a2\u7d22LLMs\u662f\u5426\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u903b\u8f91\u76f8\u4f3c\u7684\u5e95\u5c42\u903b\u8f91\u6846\u67b6\u6f14\u53d8\uff08\u4ece\u76f4\u89c9\u9a71\u52a8\u63a8\u7406\u5230\u4e25\u8c28\u5f62\u5f0f\u7cfb\u7edf\uff09\u3002", "method": "\u4ee5\u5b58\u5728\u9884\u8bbe\u4f5c\u4e3a\u63a2\u9488\uff0c\u5728\u4f20\u7edf\u548c\u73b0\u4ee3\u903b\u8f91\u6846\u67b6\u4e0b\u8bc4\u4f30\u4e09\u6bb5\u8bba\uff0c\u901a\u8fc7\u5728\u65b0\u6784\u5efa\u7684\u4e09\u6bb5\u8bba\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5SOTA\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "(i) \u6a21\u578b\u89c4\u6a21\u6269\u5c55\u4fc3\u8fdb\u5411\u73b0\u4ee3\u903b\u8f91\u7684\u8f6c\u53d8\uff1b(ii) \u601d\u7ef4\u80fd\u529b\u662f\u8d85\u8d8a\u53c2\u6570\u6269\u5c55\u7684\u9ad8\u6548\u52a0\u901f\u5668\uff1b(iii) \u57fa\u5ea7\u6a21\u578b\u5bf9\u8f6c\u53d8\u7684\u6613\u5f97\u6027\u548c\u7a33\u5b9a\u6027\u8d77\u51b3\u5b9a\u6027\u4f5c\u7528\u3002", "conclusion": "\u9664\u6838\u5fc3\u56e0\u7d20\u5916\uff0c\u8fd8\u8fdb\u884c\u4e86\u989d\u5916\u5b9e\u9a8c\u4ee5\u6df1\u5165\u5206\u6790\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u7279\u6027\u3002"}}
{"id": "2601.18060", "categories": ["quant-ph", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.18060", "abs": "https://arxiv.org/abs/2601.18060", "authors": ["Francis Boabang", "Samuel Asante Gyamerah"], "title": "Overcoming Barren Plateaus in Variational Quantum Circuits using a Two-Step Least Squares Approach", "comment": "15 pages, 2 figures", "summary": "Variational Quantum Algorithms are a vital part of quantum computing. It is a blend of quantum and classical methods for tackling tough problems in machine learning, chemistry, and combinatorial optimization. Yet as these algorithms scale up, they cannot escape the barren-plateau phenomenon. As systems grow, gradients can vanish so quickly that training deep or randomly initialized circuits becomes nearly impossible. To overcome the barren plateau problem, we introduce a two-stage optimization framework. First comes the convex initialization stage. Here, we shape the quantum energy landscape, the Hilmaton landscape, into a smooth, low-energy basin. This step makes gradients easier to spot and keeps noise from derailing the process. Once we have gotten a stable gradient flow, we move to the second stage: nonconvex refinement. In this phase, we allow the algorithm to explore different energy minima, thereby making the model more expressive. Finally, we used our two-stage solution to perform quantum cryptanalysis of the quantum key distribution protocol (i.e., BB84) to determine the optimal cloning strategies. The simulation results showed that our proposed two-stage solution outperforms its random initialization counterpart.", "AI": {"tldr": "\u9488\u5bf9\u53d8\u5206\u91cf\u5b50\u7b97\u6cd5\u4e2d\u7684\u8d2b\u7620\u9ad8\u539f\u95ee\u9898\uff0c\u63d0\u51fa\u4e24\u9636\u6bb5\u4f18\u5316\u6846\u67b6\uff08\u51f8\u521d\u59cb\u5316+\u975e\u51f8\u7ec6\u5316\uff09\uff0c\u5728BB84\u91cf\u5b50\u5bc6\u7801\u5206\u6790\u7684\u6a21\u62df\u4e2d\u4f18\u4e8e\u968f\u673a\u521d\u59cb\u5316\u65b9\u6cd5\u3002", "motivation": "\u53d8\u5206\u91cf\u5b50\u7b97\u6cd5\u662f\u91cf\u5b50\u8ba1\u7b97\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u7528\u4e8e\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u3001\u5316\u5b66\u548c\u7ec4\u5408\u4f18\u5316\u7b49\u96be\u9898\uff0c\u4f46\u5728\u6269\u5c55\u65f6\u4f1a\u906d\u9047\u8d2b\u7620\u9ad8\u539f\u73b0\u8c61\uff0c\u5bfc\u81f4\u68af\u5ea6\u6d88\u5931\uff0c\u4f7f\u5f97\u8bad\u7ec3\u6df1\u5ea6\u6216\u968f\u673a\u521d\u59cb\u5316\u7684\u7535\u8def\u53d8\u5f97\u51e0\u4e4e\u4e0d\u53ef\u80fd\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u4f18\u5316\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4e3a\u51f8\u521d\u59cb\u5316\uff0c\u5c06\u91cf\u5b50\u80fd\u91cf\u666f\u89c2\uff08\u54c8\u5bc6\u987f\u666f\u89c2\uff09\u5851\u9020\u6210\u5e73\u6ed1\u7684\u4f4e\u80fd\u91cf\u76c6\u5730\uff0c\u4fbf\u4e8e\u68af\u5ea6\u8bc6\u522b\u5e76\u907f\u514d\u566a\u58f0\u5e72\u6270\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4e3a\u975e\u51f8\u7ec6\u5316\uff0c\u5141\u8bb8\u7b97\u6cd5\u63a2\u7d22\u4e0d\u540c\u80fd\u91cf\u6781\u5c0f\u503c\u4ee5\u63d0\u9ad8\u6a21\u578b\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u5c06\u8be5\u4e24\u9636\u6bb5\u65b9\u6848\u5e94\u7528\u4e8eBB84\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\u534f\u8bae\u7684\u91cf\u5b50\u5bc6\u7801\u5206\u6790\uff0c\u4ee5\u786e\u5b9a\u6700\u4f18\u514b\u9686\u7b56\u7565\u3002\u6a21\u62df\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6848\u4f18\u4e8e\u968f\u673a\u521d\u59cb\u5316\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u4f18\u5316\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8d2b\u7620\u9ad8\u539f\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u7a33\u5b9a\u68af\u5ea6\u6d41\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u6a21\u578b\u8868\u8fbe\u80fd\u529b\uff0c\u76f8\u6bd4\u968f\u673a\u521d\u59cb\u5316\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2601.17076", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17076", "abs": "https://arxiv.org/abs/2601.17076", "authors": ["Jiajun Chen", "Yue Wu", "Kai Huang", "Wen Xi", "Yangyang Wu", "Xiaoye Miao", "Mengying Zhu", "Meng Xi", "Guanjie Cheng"], "title": "E2PL: Effective and Efficient Prompt Learning for Incomplete Multi-view Multi-Label Class Incremental Learning", "comment": "11 pages", "summary": "Multi-view multi-label classification (MvMLC) is indispensable for modern web applications aggregating information from diverse sources. However, real-world web-scale settings are rife with missing views and continuously emerging classes, which pose significant obstacles to robust learning. Prevailing methods are ill-equipped for this reality, as they either lack adaptability to new classes or incur exponential parameter growth when handling all possible missing-view patterns, severely limiting their scalability in web environments. To systematically address this gap, we formally introduce a novel task, termed \\emph{incomplete multi-view multi-label class incremental learning} (IMvMLCIL), which requires models to simultaneously address heterogeneous missing views and dynamic class expansion. To tackle this task, we propose \\textsf{E2PL}, an Effective and Efficient Prompt Learning framework for IMvMLCIL. \\textsf{E2PL} unifies two novel prompt designs: \\emph{task-tailored prompts} for class-incremental adaptation and \\emph{missing-aware prompts} for the flexible integration of arbitrary view-missing scenarios. To fundamentally address the exponential parameter explosion inherent in missing-aware prompts, we devise an \\emph{efficient prototype tensorization} module, which leverages atomic tensor decomposition to elegantly reduce the prompt parameter complexity from exponential to linear w.r.t. the number of views. We further incorporate a \\emph{dynamic contrastive learning} strategy explicitly model the complex dependencies among diverse missing-view patterns, thus enhancing the model's robustness. Extensive experiments on three benchmarks demonstrate that \\textsf{E2PL} consistently outperforms state-of-the-art methods in both effectiveness and efficiency. The codes and datasets are available at https://anonymous.4open.science/r/code-for-E2PL.", "AI": {"tldr": "\u9488\u5bf9\u73b0\u4ee3Web\u5e94\u7528\u4e2d\u591a\u89c6\u56fe\u591a\u6807\u7b7e\u5206\u7c7b\u9762\u4e34\u7684\u89c6\u56fe\u7f3a\u5931\u548c\u7c7b\u522b\u52a8\u6001\u589e\u957f\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faE2PL\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u5b9a\u5236\u63d0\u793a\u548c\u7f3a\u5931\u611f\u77e5\u63d0\u793a\uff0c\u7ed3\u5408\u539f\u578b\u5f20\u91cf\u5316\u6a21\u5757\u5c06\u53c2\u6570\u91cf\u4ece\u6307\u6570\u7ea7\u964d\u81f3\u7ebf\u6027\u7ea7\uff0c\u5e76\u91c7\u7528\u52a8\u6001\u5bf9\u6bd4\u5b66\u4e60\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684Web\u7ea7\u5e94\u7528\u573a\u666f\u666e\u904d\u5b58\u5728\u89c6\u56fe\u7f3a\u5931\u548c\u7c7b\u522b\u6301\u7eed\u65b0\u589e\u7684\u53cc\u91cd\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u7f3a\u4e4f\u5bf9\u65b0\u7c7b\u522b\u7684\u9002\u5e94\u6027\uff0c\u8981\u4e48\u5728\u5904\u7406\u6240\u6709\u53ef\u80fd\u7684\u7f3a\u5931\u89c6\u56fe\u6a21\u5f0f\u65f6\u53c2\u6570\u91cf\u5448\u6307\u6570\u7ea7\u589e\u957f\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51faE2PL\u6846\u67b6\uff1a1\uff09\u4efb\u52a1\u5b9a\u5236\u63d0\u793a\u5b9e\u73b0\u7c7b\u522b\u589e\u91cf\u9002\u5e94\uff1b2\uff09\u7f3a\u5931\u611f\u77e5\u63d0\u793a\u7075\u6d3b\u96c6\u6210\u4efb\u610f\u89c6\u56fe\u7f3a\u5931\u573a\u666f\uff1b3\uff09\u539f\u578b\u5f20\u91cf\u5316\u6a21\u5757\u901a\u8fc7\u539f\u5b50\u5f20\u91cf\u5206\u89e3\u5c06\u63d0\u793a\u53c2\u6570\u590d\u6742\u5ea6\u4ece\u6307\u6570\u7ea7\u964d\u81f3\u7ebf\u6027\u7ea7\uff1b4\uff09\u52a8\u6001\u5bf9\u6bd4\u5b66\u4e60\u663e\u5f0f\u5efa\u6a21\u4e0d\u540c\u7f3a\u5931\u89c6\u56fe\u6a21\u5f0f\u95f4\u7684\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cE2PL\u5728\u6709\u6548\u6027\u548c\u6548\u7387\u65b9\u9762\u5747\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "E2PL\u6846\u67b6\u4e3a\u4e0d\u5b8c\u5168\u591a\u89c6\u56fe\u591a\u6807\u7b7e\u7c7b\u522b\u589e\u91cf\u5b66\u4e60\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5f88\u597d\u5730\u9002\u5e94Web\u73af\u5883\u4e2d\u89c6\u56fe\u7f3a\u5931\u548c\u7c7b\u522b\u52a8\u6001\u6269\u5c55\u7684\u73b0\u5b9e\u6311\u6218\u3002"}}
{"id": "2601.17481", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17481", "abs": "https://arxiv.org/abs/2601.17481", "authors": ["Emily Broadhurst", "Tawab Safi", "Joseph Edell", "Vashisht Ganesh", "Karime Maamari"], "title": "Lattice: Generative Guardrails for Conversational Agents", "comment": null, "summary": "Conversational AI systems require guardrails to prevent harmful outputs, yet existing approaches use static rules that cannot adapt to new threats or deployment contexts. We introduce Lattice, a framework for self-constructing and continuously improving guardrails. Lattice operates in two stages: construction builds initial guardrails from labeled examples through iterative simulation and optimization; continuous improvement autonomously adapts deployed guardrails through risk assessment, adversarial testing, and consolidation. Evaluated on the ProsocialDialog dataset, Lattice achieves 91% F1 on held-out data, outperforming keyword baselines by 43pp, LlamaGuard by 25pp, and NeMo by 4pp. The continuous improvement stage achieves 7pp F1 improvement on cross-domain data through closed-loop optimization. Our framework shows that effective guardrails can be self-constructed through iterative optimization.", "AI": {"tldr": "\u63d0\u51faLattice\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8fed\u4ee3\u4f18\u5316\u5b9e\u73b0\u81ea\u6784\u5efa\u3001\u81ea\u6539\u8fdb\u7684\u5bf9\u8bddAI\u62a4\u680f\u7cfb\u7edf\uff0c\u5728ProsocialDialog\u6570\u636e\u96c6\u4e0a\u8fbe\u523091% F1\u503c\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u5bf9\u8bddAI\u62a4\u680f\u7cfb\u7edf\u4f9d\u8d56\u9759\u6001\u89c4\u5219\uff0c\u65e0\u6cd5\u9002\u5e94\u65b0\u5a01\u80c1\u6216\u90e8\u7f72\u73af\u5883\u53d8\u5316\uff0c\u7f3a\u4e4f\u81ea\u9002\u5e94\u80fd\u529b", "method": "Lattice\u6846\u67b6\u5206\u4e24\u4e2a\u9636\u6bb5\uff1a1\uff09\u6784\u5efa\u9636\u6bb5\u901a\u8fc7\u8fed\u4ee3\u6a21\u62df\u548c\u4f18\u5316\u4ece\u6807\u6ce8\u793a\u4f8b\u751f\u6210\u521d\u59cb\u62a4\u680f\uff1b2\uff09\u6301\u7eed\u6539\u8fdb\u9636\u6bb5\u901a\u8fc7\u98ce\u9669\u8bc4\u4f30\u3001\u5bf9\u6297\u6d4b\u8bd5\u548c\u6574\u5408\u5b9e\u73b0\u62a4\u680f\u81ea\u4e3b\u8fdb\u5316", "result": "\u5728ProsocialDialog\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523091% F1\u503c\uff0c\u8d85\u8d8a\u5173\u952e\u8bcd\u57fa\u7ebf43\u4e2a\u767e\u5206\u70b9\u3001LlamaGuard 25\u4e2a\u767e\u5206\u70b9\u3001NeMo 4\u4e2a\u767e\u5206\u70b9\uff1b\u6301\u7eed\u6539\u8fdb\u9636\u6bb5\u5728\u8de8\u9886\u57df\u6570\u636e\u4e0a\u8fdb\u4e00\u6b65\u63d0\u53477\u4e2a\u767e\u5206\u70b9", "conclusion": "\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u53ef\u81ea\u4e3b\u6f14\u5316\u51fa\u6709\u6548\u62a4\u680f\uff0c\u4e3aAI\u5b89\u5168\u63d0\u4f9b\u4e86\u53ef\u81ea\u9002\u5e94\u3001\u6301\u7eed\u8fdb\u5316\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.18082", "categories": ["quant-ph", "physics.chem-ph", "physics.comp-ph", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2601.18082", "abs": "https://arxiv.org/abs/2601.18082", "authors": ["Mai Nguyen Phuong Nhi", "Lan Nguyen Tran", "Le Bin Ho"], "title": "Feedback-Based Quantum Control for Safe and Synergistic Drug Combination Design", "comment": "11 pages, 7 figures", "summary": "Drug-drug interactions (DDIs) strongly affect the safety and efficacy of combination therapies. Despite the availability of large DDI databases, selecting optimal multi-drug combinations that balance safety, therapeutic benefit, and regimen size remains a challenging combinatorial optimization problem. Here, we present a quantum-control-based framework for DDI-aware drug combination optimization, in which known harmful and synergistic interactions are encoded into Ising Hamiltonians as penalties and rewards, respectively. The optimization is performed using the feedback-based quantum algorithm FALQON, a gradient-free variational approach. We study two clinically motivated tasks: the Maximum Safe Subset problem and the Synergy-Constrained Optimization problem. Numerical simulations using interaction data from Drugs.com and SYNERGxDB demonstrate efficient convergence and high-quality solutions for clinically relevant drug sets, including COVID-19 case studies.", "AI": {"tldr": "This paper proposes a quantum-control framework using FALQON algorithm to optimize drug combinations by encoding drug-drug interactions into Ising Hamiltonians, demonstrating efficient solutions for clinical tasks like maximum safe subsets and synergy-constrained optimization with real-world data.", "motivation": "Selecting optimal multi-drug combinations that balance safety, therapeutic benefit, and regimen size is a challenging combinatorial optimization problem, despite available DDI databases.", "method": "They developed a quantum-control-based framework where harmful and synergistic DDIs are encoded into Ising Hamiltonians as penalties and rewards. The optimization uses FALQON, a gradient-free variational quantum algorithm.", "result": "Numerical simulations using Drugs.com and SYNERGxDB data showed efficient convergence and high-quality solutions for clinically relevant drug sets, including COVID-19 case studies.", "conclusion": "The quantum-control framework provides an effective approach for DDI-aware drug combination optimization, potentially improving the safety and efficacy of multi-drug therapies."}}
{"id": "2601.17090", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17090", "abs": "https://arxiv.org/abs/2601.17090", "authors": ["Noam Koren", "Rafael Moschopoulos", "Kira Radinsky", "Elad Hazan"], "title": "SFO: Learning PDE Operators via Spectral Filtering", "comment": null, "summary": "Partial differential equations (PDEs) govern complex systems, yet neural operators often struggle to efficiently capture the long-range, nonlocal interactions inherent in their solution maps. We introduce Spectral Filtering Operator (SFO), a neural operator that parameterizes integral kernels using the Universal Spectral Basis (USB), a fixed, global orthonormal basis derived from the eigenmodes of the Hilbert matrix in spectral filtering theory. Motivated by our theoretical finding that the discrete Green's functions of shift-invariant PDE discretizations exhibit spatial Linear Dynamical System (LDS) structure, we prove that these kernels admit compact approximations in the USB. By learning only the spectral coefficients of rapidly decaying eigenvalues, SFO achieves a highly efficient representation. Across six benchmarks, including reaction-diffusion, fluid dynamics, and 3D electromagnetics, SFO achieves state-of-the-art accuracy, reducing error by up to 40% relative to strong baselines while using substantially fewer parameters.", "AI": {"tldr": "Spectral Filtering Operator (SFO) uses a universal spectral basis to efficiently model PDEs by learning only a few rapidly decaying spectral coefficients, achieving 40% error reduction with fewer parameters.", "motivation": "Neural operators struggle to efficiently capture long-range, nonlocal interactions inherent in PDE solution maps.", "method": "Parameterizes integral kernels using Universal Spectral Basis (USB) derived from Hilbert matrix eigenmodes, leveraging the Linear Dynamical System structure of discrete Green's functions to learn only spectral coefficients of rapidly decaying eigenvalues.", "result": "Achieved state-of-the-art accuracy across six PDE benchmarks (reaction-diffusion, fluid dynamics, 3D electromagnetics), reducing error by up to 40% while using substantially fewer parameters than baselines.", "conclusion": "SFO provides a highly efficient representation for neural operators by exploiting the compact approximability of PDE kernels in the universal spectral basis."}}
{"id": "2601.17542", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17542", "abs": "https://arxiv.org/abs/2601.17542", "authors": ["Vinoth Punniyamoorthy", "Nitin Saksena", "Srivenkateswara Reddy Sankiti", "Nachiappan Chockalingam", "Aswathnarayan Muthukrishnan Kirubakaran", "Shiva Kumar Reddy Carimireddy", "Durgaraman Maruthavanan"], "title": "Cognitive Platform Engineering for Autonomous Cloud Operations", "comment": null, "summary": "Modern DevOps practices have accelerated software delivery through automation, CI/CD pipelines, and observability tooling,but these approaches struggle to keep pace with the scale and dynamism of cloud-native systems. As telemetry volume grows and configuration drift increases, traditional, rule-driven automation often results in reactive operations, delayed remediation, and dependency on manual expertise. This paper introduces Cognitive Platform Engineering, a next-generation paradigm that integrates sensing, reasoning, and autonomous action directly into the platform lifecycle. This paper propose a four-plane reference architecture that unifies data collection, intelligent inference, policy-driven orchestration, and human experience layers within a continuous feedback loop. A prototype implementation built with Kubernetes, Terraform, Open Policy Agent, and ML-based anomaly detection demonstrates improvements in mean time to resolution, resource efficiency, and compliance. The results show that embedding intelligence into platform operations enables resilient, self-adjusting, and intent-aligned cloud environments. The paper concludes with research opportunities in reinforcement learning, explainable governance, and sustainable self-managing cloud ecosystems.", "AI": {"tldr": "\u63d0\u51fa\u8ba4\u77e5\u5e73\u53f0\u5de5\u7a0b\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u56db\u5c42\u67b6\u6784\uff08\u611f\u77e5/\u63a8\u7406/\u7f16\u6392/\u4f53\u9a8c\uff09\u5b9e\u73b0\u4e91\u5e73\u53f0\u81ea\u4e3b\u8fd0\u7ef4\uff0c\u539f\u578b\u9a8c\u8bc1\u53ef\u63d0\u5347\u6545\u969c\u6062\u590d\u901f\u5ea6\u3001\u8d44\u6e90\u6548\u7387\u548c\u5408\u89c4\u6027", "motivation": "\u4f20\u7edfDevOps\u5728\u4e91\u539f\u751f\u73af\u5883\u4e0b\u9762\u4e34 telemetry \u6570\u636e\u6fc0\u589e\u548c\u914d\u7f6e\u6f02\u79fb\u6311\u6218\uff0c\u89c4\u5219\u9a71\u52a8\u81ea\u52a8\u5316\u5bfc\u81f4\u8fd0\u7ef4\u88ab\u52a8\u3001\u4fee\u590d\u5ef6\u8fdf\uff0c\u4e9f\u9700\u66f4\u667a\u80fd\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51fa\u8ba4\u77e5\u5e73\u53f0\u5de5\u7a0b\uff08Cognitive Platform Engineering\uff09\u53ca\u56db\u5c42\u53c2\u8003\u67b6\u6784\uff1a1) \u6570\u636e\u91c7\u96c6\u5c42 2) \u667a\u80fd\u63a8\u7406\u5c42\uff08ML\u5f02\u5e38\u68c0\u6d4b\uff093) \u7b56\u7565\u7f16\u6392\u5c42\uff08OPA\uff094) \u4eba\u673a\u4f53\u9a8c\u5c42\uff0c\u5f62\u6210\u95ed\u73af\u53cd\u9988", "result": "\u57fa\u4e8eKubernetes/Terraform/OPA/ML\u7684\u539f\u578b\u663e\u793a\uff1a\u5e73\u5747\u6545\u969c\u4fee\u590d\u65f6\u95f4\uff08MTTR\uff09\u7f29\u77ed\u3001\u8d44\u6e90\u5229\u7528\u7387\u63d0\u5347\u3001\u5408\u89c4\u6027\u589e\u5f3a\uff0c\u5b9e\u73b0\u81ea\u8c03\u8282\u4e91\u5e73\u53f0", "conclusion": "\u5d4c\u5165\u667a\u80fd\u7684\u5e73\u53f0\u8fd0\u7ef4\u53ef\u6784\u5efa\u5f39\u6027\u81ea\u7ba1\u7406\u7cfb\u7edf\uff0c\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u5f3a\u5316\u5b66\u4e60\u3001\u53ef\u89e3\u91ca\u6cbb\u7406\u548c\u53ef\u6301\u7eed\u81ea\u7ef4\u62a4\u4e91\u751f\u6001"}}
{"id": "2601.18083", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.18083", "abs": "https://arxiv.org/abs/2601.18083", "authors": ["Ting-Ting Ma", "Jian Tang", "Yun-Lan Zuo", "Ran Huang. Adam Miranowicz", "Franco Nori", "Hui Jing"], "title": "Two-Polariton Blockade via Ultrastrong Light-Matter Coupling", "comment": null, "summary": "We demonstrate that a two-polariton blockade (2PB) can occur under resonant single-polariton driving in an atom-cavity system operating in the ultrastrong coupling (USC) regime-a phenomenon qualitatively distinct from, and unattainable in, both the strong and weak coupling regimes. In the USC regime, where the ratio of the atom-cavity coupling strength to the cavity resonance frequency exceeds 0.1, hybrid light-matter quasiparticles known as polaritons emerge. By employing modified second- and third-order correlation functions appropriate for the USC regime, we predict the emergence of 2PB, characterized by pronounced two-polariton bunching accompanied by suppressed three-polariton coincidences. This Letter introduces a novel route to achieving 2PB, with promising implications for the realization of multiparticle quantum light sources in the USC regime.", "AI": {"tldr": "\u5728\u539f\u5b50-\u8154\u8d85\u5f3a\u8026\u5408\u4f53\u7cfb\u4e2d\uff0c\u901a\u8fc7\u5171\u632f\u5355\u6781\u5316\u6fc0\u5143\u9a71\u52a8\u9996\u6b21\u5b9e\u73b0\u4e86\u53cc\u6781\u5316\u6fc0\u5143\u963b\u585e\u6548\u5e94\uff0c\u8be5\u6548\u5e94\u8868\u73b0\u51fa\u53cc\u7c92\u5b50\u805a\u675f\u548c\u4e09\u7c92\u5b50\u7b26\u5408\u6291\u5236\u7684\u7279\u5f81\uff0c\u4e3a\u8d85\u5f3a\u8026\u5408\u533a\u591a\u7c92\u5b50\u91cf\u5b50\u5149\u6e90\u7684\u5b9e\u73b0\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002", "motivation": "\u4f20\u7edf\u5f3a\u8026\u5408\u548c\u5f31\u8026\u5408\u4f53\u7cfb\u65e0\u6cd5\u5b9e\u73b0\u53cc\u6781\u5316\u6fc0\u5143\u963b\u585e\uff0c\u800c\u5728\u8d85\u5f3a\u8026\u5408\u533a\uff08\u8026\u5408\u5f3a\u5ea6\u4e0e\u8154\u9891\u6bd4>0.1\uff09\u5b58\u5728\u65b0\u7684\u7269\u7406\u73b0\u8c61\uff0c\u4e3a\u5f00\u53d1\u65b0\u578b\u591a\u7c92\u5b50\u91cf\u5b50\u5149\u6e90\u63d0\u4f9b\u4e86\u72ec\u7279\u673a\u4f1a\u3002", "method": "\u8fd0\u7528\u9488\u5bf9\u8d85\u5f3a\u8026\u5408\u4f53\u7cfb\u4fee\u6b63\u7684\u4e8c\u9636\u548c\u4e09\u9636\u5173\u8054\u51fd\u6570\uff0c\u7406\u8bba\u9884\u6d4b\u5728\u5171\u632f\u5355\u6781\u5316\u6fc0\u5143\u9a71\u52a8\u6761\u4ef6\u4e0b\u53cc\u6781\u5316\u6fc0\u5143\u963b\u585e\u7684\u51fa\u73b0\u3002", "result": "\u6210\u529f\u9884\u6d4b\u4e86\u53cc\u6781\u5316\u6fc0\u5143\u963b\u585e\u73b0\u8c61\uff0c\u5176\u7279\u5f81\u4e3a\u663e\u8457\u7684\u53cc\u6781\u5316\u6fc0\u5143\u805a\u675f\u6548\u5e94\u548c\u5f3a\u70c8\u6291\u5236\u7684\u4e09\u4e2a\u6781\u5316\u6fc0\u5143\u540c\u65f6\u7b26\u5408\u4e8b\u4ef6\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63ed\u793a\u4e86\u8d85\u5f3a\u8026\u5408\u4f53\u7cfb\u7684\u72ec\u7279\u91cf\u5b50\u7edf\u8ba1\u7279\u6027\uff0c\u4e3a\u8bbe\u8ba1\u57fa\u4e8e\u6781\u5316\u6fc0\u5143\u7684\u591a\u7c92\u5b50\u91cf\u5b50\u5149\u6e90\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u73b0\u65b9\u6848\u3002"}}
{"id": "2601.17091", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17091", "abs": "https://arxiv.org/abs/2601.17091", "authors": ["Ole St\u00fcven", "Keno Moenck", "Thorsten Sch\u00fcppstuhl"], "title": "CUROCKET: Optimizing ROCKET for GPU", "comment": null, "summary": "ROCKET (RandOm Convolutional KErnel Transform) is a feature extraction algorithm created for Time Series Classification (TSC), published in 2019. It applies convolution with randomly generated kernels on a time series, producing features that can be used to train a linear classifier or regressor like Ridge. At the time of publication, ROCKET was on par with the best state-of-the-art algorithms for TSC in terms of accuracy while being significantly less computationally expensive, making ROCKET a compelling algorithm for TSC. This also led to several subsequent versions, further improving accuracy and computational efficiency. The currently available ROCKET implementations are mostly bound to execution on CPU. However, convolution is a task that can be highly parallelized and is therefore suited to be executed on GPU, which speeds up the computation significantly. A key difficulty arises from the inhomogeneous kernels ROCKET uses, making standard methods for applying convolution on GPU inefficient. In this work, we propose an algorithm that is able to efficiently perform ROCKET on GPU and achieves up to 11 times higher computational efficiency per watt than ROCKET on CPU. The code for CUROCKET is available in this repository https://github.com/oleeven/CUROCKET on github.", "AI": {"tldr": "This paper presents CUROCKET, a GPU-accelerated version of the ROCKET algorithm for time series classification that handles inhomogeneous kernels efficiently, achieving up to 11x better energy efficiency than CPU-based ROCKET.", "motivation": "The original ROCKET algorithm, while accurate for time series classification, runs primarily on CPU despite convolution being highly parallelizable. Standard GPU convolution methods are inefficient for ROCKET's inhomogeneous kernels, creating a performance bottleneck that limits computational efficiency.", "method": "The authors propose a specialized GPU algorithm (CUROCKET) designed to efficiently handle the inhomogeneous kernels used in ROCKET, enabling parallelized convolution operations on GPU that would be inefficient with standard methods.", "result": "CUROCKET achieves up to 11 times higher computational efficiency per watt compared to the CPU implementation of ROCKET, demonstrating significant performance and energy advantages.", "conclusion": "The authors provide open-source code for CUROCKET at https://github.com/oleeven/CUROCKET, making the GPU-accelerated implementation publicly available for the research community."}}
{"id": "2601.17564", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17564", "abs": "https://arxiv.org/abs/2601.17564", "authors": ["Aadam", "Monu Verma", "Mohamed Abdel-Mottaleb"], "title": "JaxARC: A High-Performance JAX-based Environment for Abstraction and Reasoning Research", "comment": null, "summary": "The Abstraction and Reasoning Corpus (ARC) tests AI systems' ability to perform human-like inductive reasoning from a few demonstration pairs. Existing Gymnasium-based RL environments severely limit experimental scale due to computational bottlenecks. We present JaxARC, an open-source, high-performance RL environment for ARC implemented in JAX. Its functional, stateless architecture enables massive parallelism, achieving 38-5,439x speedup over Gymnasium at matched batch sizes, with peak throughput of 790M steps/second. JaxARC supports multiple ARC datasets, flexible action spaces, composable wrappers, and configuration-driven reproducibility, enabling large-scale RL research previously computationally infeasible. JaxARC is available at https://github.com/aadimator/JaxARC.", "AI": {"tldr": "JaxARC\u662f\u4e00\u4e2a\u57fa\u4e8eJAX\u7684\u5f00\u6e90\u9ad8\u6027\u80fd\u5f3a\u5316\u5b66\u4e60\u73af\u5883\uff0c\u7528\u4e8eARC\u4efb\u52a1\u3002\u5176\u51fd\u6570\u5f0f\u65e0\u72b6\u6001\u67b6\u6784\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u5e76\u884c\uff0c\u76f8\u6bd4Gymnasium\u83b7\u5f9738-5,439\u500d\u52a0\u901f\uff0c\u5cf0\u503c\u541e\u5410\u91cf\u8fbe7.9\u4ebf\u6b65/\u79d2\uff0c\u4f7f\u539f\u5148\u8ba1\u7b97\u4e0d\u53ef\u884c\u7684\u5927\u89c4\u6a21RL\u7814\u7a76\u6210\u4e3a\u53ef\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eGymnasium\u7684ARC\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u56e0\u8ba1\u7b97\u74f6\u9888\u4e25\u91cd\u9650\u5236\u5b9e\u9a8c\u89c4\u6a21\uff0c\u963b\u788d\u4e86AI\u7cfb\u7edf\u4eba\u7c7b\u5f0f\u5f52\u7eb3\u63a8\u7406\u80fd\u529b\u7684\u7814\u7a76\u3002", "method": "\u63d0\u51faJaxARC\uff0c\u91c7\u7528JAX\u5b9e\u73b0\u51fd\u6570\u5f0f\u65e0\u72b6\u6001\u67b6\u6784\u4ee5\u652f\u6301\u5927\u89c4\u6a21\u5e76\u884c\uff0c\u540c\u65f6\u63d0\u4f9b\u591aARC\u6570\u636e\u96c6\u652f\u6301\u3001\u7075\u6d3b\u52a8\u4f5c\u7a7a\u95f4\u3001\u53ef\u7ec4\u5408\u5305\u88c5\u5668\u548c\u914d\u7f6e\u9a71\u52a8\u7684\u590d\u73b0\u6027\u3002", "result": "\u5728\u5339\u914d\u6279\u6b21\u5927\u5c0f\u4e0b\u5b9e\u73b038-5,439\u500d\u52a0\u901f\uff0c\u5cf0\u503c\u541e\u5410\u91cf\u8fbe7.9\u4ebf\u6b65/\u79d2\uff0c\u4f7f\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u53d8\u5f97\u53ef\u884c\u3002", "conclusion": "JaxARC\u6210\u529f\u89e3\u51b3\u4e86ARC\u7814\u7a76\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u4e3aAI\u5f52\u7eb3\u63a8\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u6027\u80fd\u5de5\u5177\uff0c\u5c06\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.17093", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17093", "abs": "https://arxiv.org/abs/2601.17093", "authors": ["Olha Sirikova", "Alvin Chan"], "title": "The Triangle of Similarity: A Multi-Faceted Framework for Comparing Neural Network Representations", "comment": "Accepted to AAAI 2026 Workshop on AI for Scientific Research (AI4Research)", "summary": "Comparing neural network representations is essential for understanding and validating models in scientific applications. Existing methods, however, often provide a limited view. We propose the Triangle of Similarity, a framework that combines three complementary perspectives: static representational similarity (CKA/Procrustes), functional similarity (Linear Mode Connectivity or Predictive Similarity), and sparsity similarity (robustness under pruning). Analyzing a range of CNNs, Vision Transformers, and Vision-Language Models using both in-distribution (ImageNetV2) and out-of-distribution (CIFAR-10) testbeds, our initial findings suggest that: (1) architectural family is a primary determinant of representational similarity, forming distinct clusters; (2) CKA self-similarity and task accuracy are strongly correlated during pruning, though accuracy often degrades more sharply; and (3) for some model pairs, pruning appears to regularize representations, exposing a shared computational core. This framework offers a more holistic approach for assessing whether models have converged on similar internal mechanisms, providing a useful tool for model selection and analysis in scientific research.", "AI": {"tldr": "This paper proposes a \"Triangle of Similarity\" framework that combines three complementary perspectives\u2014static, functional, and sparsity similarity\u2014to comprehensively compare neural network representations, revealing that architectural family primarily determines similarity and pruning can expose shared computational cores.", "motivation": "Existing methods for comparing neural network representations offer limited views, hindering thorough understanding and validation of models in scientific applications.", "method": "The Triangle of Similarity framework integrates static representational similarity (CKA/Procrustes), functional similarity (Linear Mode Connectivity), and sparsity similarity (pruning robustness). The authors analyzed CNNs, Vision Transformers, and Vision-Language Models using both in-distribution (ImageNetV2) and out-of-distribution (CIFAR-10) testbeds.", "result": "Key findings include: (1) Architectural family is the primary determinant of representational similarity, forming distinct clusters; (2) CKA self-similarity and task accuracy are strongly correlated during pruning, though accuracy often degrades more sharply; (3) Pruning can regularize representations, exposing a shared computational core for some model pairs.", "conclusion": "This holistic framework provides a more comprehensive approach for assessing whether models converge on similar internal mechanisms, serving as a valuable tool for model selection and scientific analysis."}}
{"id": "2601.18164", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.18164", "abs": "https://arxiv.org/abs/2601.18164", "authors": ["Tzong-Daw Wu", "Hsi-Sheng Goan"], "title": "Quantum Recurrent Unit: A Parameter-Efficient Quantum Neural Network Component", "comment": "32 pages, 10 figures, 8 tables (including supplementary material)", "summary": "The rapid growth of modern machine learning (ML) models presents fundamental challenges in parameter efficiency and computational resource requirements. This study introduces the Quantum Recurrent Unit (QRU), a novel quantum neural network (NN) architecture specifically designed to address these challenges while remaining compatible with Noisy Intermediate-Scale Quantum (NISQ) devices. QRU leverages quantum controlled-SWAP (C-SWAP; Fredkin) gates to implement an information selection mechanism inspired by classical Gated Recurrent Units (GRUs), enabling selective processing of temporal information via quantum operations. Through its innovative recurrent architecture featuring measurement results feedforward state propagation and shared parameters across time steps, QRU achieves constant circuit depth and constant parameter count regardless of input sequence length, effectively circumventing stringent NISQ hardware constraints. We systematically validate QRU through three progressive experiments: (1) oscillatory behavior prediction, where 72-parameter QRU matches 197-parameter classical GRU performance; (2) Wisconsin Diagnostic Breast Cancer classification, where 35 parameters achieve 96.13% accuracy comparable to 167-parameter artificial NNs; and (3) MNIST handwritten digit recognition, where 132 parameters reach 98.05% accuracy, outperforming a 27,265-parameter convolutional NN. These results demonstrate that QRU consistently achieves comparable or superior performance with significantly fewer parameters than classical NNs while maintaining constant quantum circuit depth. The architecture's quantum-native design, combining C-SWAP-based information selection with novel recurrent processing, suggests QRU's potential as a fundamental building block for next-generation ML systems, offering a promising pathway toward more efficient and scalable quantum ML architectures.", "AI": {"tldr": "This paper introduces Quantum Recurrent Unit (QRU), a quantum neural network architecture using C-SWAP gates for information selection that achieves constant circuit depth and parameter count regardless of sequence length, outperforming classical models with far fewer parameters on multiple tasks.", "motivation": "The rapid growth of modern machine learning models presents fundamental challenges in parameter efficiency and computational resource requirements, especially for NISQ devices.", "method": "Introduces QRU architecture leveraging quantum C-SWAP gates to implement information selection mechanism inspired by classical GRUs, featuring measurement results feedforward state propagation and shared parameters across time steps to achieve constant circuit depth and parameter count.", "result": "QRUs achieve comparable or superior performance with significantly fewer parameters: matches classical GRU with 72 vs 197 parameters on oscillatory prediction; achieves 96.13% accuracy with 35 vs 167 parameters on breast cancer classification; reaches 98.05% accuracy with 132 vs 27,265 parameters on MNIST, while maintaining constant quantum circuit depth.", "conclusion": "QRU's quantum-native design combining C-SWAP-based information selection with recurrent processing demonstrates potential as a fundamental building block for next-generation ML systems, offering a promising pathway toward more efficient and scalable quantum ML architectures."}}
{"id": "2601.17094", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17094", "abs": "https://arxiv.org/abs/2601.17094", "authors": ["Junichiro Niimi"], "title": "Boltzmann-GPT: Bridging Energy-Based World Models and Language Generation", "comment": null, "summary": "Large Language Models (LLMs) generate fluent text, yet whether they truly understand the world or merely produce plausible language about it remains contested. We propose an architectural principle, the mouth is not the brain, that explicitly separates world models from language models. Our architecture comprises three components: a Deep Boltzmann Machine (DBM) that captures domain structure as an energy-based world model, an adapter that projects latent belief states into embedding space, and a frozen GPT-2 that provides linguistic competence without domain knowledge. We instantiate this framework in the consumer review domain using Amazon smartphone reviews. Experiments demonstrate that (1) conditioning through the world model yields significantly higher sentiment correlation, lower perplexity, and greater semantic similarity compared to prompt-based generation alone; (2) the DBM's energy function distinguishes coherent from incoherent market configurations, assigning higher energy to implausible brand-price combinations; and (3) interventions on specific attributes propagate causally to generated text with intervened outputs exhibiting distributions statistically consistent with naturally occurring samples sharing the target configuration. These findings suggest that even small-scale language models can achieve consistent, controllable generation when connected to an appropriate world model, providing empirical support for separating linguistic competence from world understanding.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\"\u5634\u4e0d\u662f\u5927\u8111\"\u7684\u67b6\u6784\u539f\u5219\uff0c\u901a\u8fc7\u6df1\u5ea6\u73bb\u5c14\u5179\u66fc\u673a\uff08DBM\uff09\u4f5c\u4e3a\u4e16\u754c\u6a21\u578b\u3001\u9002\u914d\u5668\u6295\u5f71\u4fe1\u5ff5\u72b6\u6001\u3001\u51bb\u7ed3GPT-2\u63d0\u4f9b\u8bed\u8a00\u80fd\u529b\u7684\u5206\u79bb\u5f0f\u8bbe\u8ba1\uff0c\u5728\u4e9a\u9a6c\u900a\u624b\u673a\u8bc4\u8bba\u57df\u9a8c\u8bc1\u4e86\u4e16\u754c\u6a21\u578b\u4e0e\u8bed\u8a00\u6a21\u578b\u5206\u79bb\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u53ef\u63a7\u3001\u4e00\u81f4\u7684\u6587\u672c\u751f\u6210\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u867d\u80fd\u751f\u6210\u6d41\u7545\u6587\u672c\uff0c\u4f46\u5176\u662f\u5426\u771f\u6b63\u7406\u89e3\u4e16\u754c\u4ecd\u5b58\u4e89\u8bae\u3002\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u660e\u786e\u5206\u79bb\u4e16\u754c\u7406\u89e3\u4e0e\u8bed\u8a00\u751f\u6210\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e09\u7ec4\u4ef6\u67b6\u6784\uff1a1\uff09\u6df1\u5ea6\u73bb\u5c14\u5179\u66fc\u673a\uff08DBM\uff09\u4f5c\u4e3a\u57fa\u4e8e\u80fd\u91cf\u7684\u4e16\u754c\u6a21\u578b\u6355\u83b7\u9886\u57df\u7ed3\u6784\uff1b2\uff09\u9002\u914d\u5668\u5c06\u6f5c\u5728\u4fe1\u5ff5\u72b6\u6001\u6295\u5f71\u5230\u5d4c\u5165\u7a7a\u95f4\uff1b3\uff09\u51bb\u7ed3\u7684GPT-2\u4ec5\u63d0\u4f9b\u8bed\u8a00\u80fd\u529b\u800c\u4e0d\u5305\u542b\u9886\u57df\u77e5\u8bc6\u3002\u5728\u4e9a\u9a6c\u900a\u667a\u80fd\u624b\u673a\u8bc4\u8bba\u57df\u8fdb\u884c\u5b9e\u4f8b\u5316\u9a8c\u8bc1\u3002", "result": "1\uff09\u901a\u8fc7\u4e16\u754c\u6a21\u578b\u8c03\u8282\u7684\u751f\u6210\u76f8\u6bd4\u7eaf\u63d0\u793a\u751f\u6210\uff0c\u60c5\u611f\u76f8\u5173\u6027\u663e\u8457\u66f4\u9ad8\u3001\u56f0\u60d1\u5ea6\u66f4\u4f4e\u3001\u8bed\u4e49\u76f8\u4f3c\u5ea6\u66f4\u9ad8\uff1b2\uff09DBM\u80fd\u91cf\u51fd\u6570\u80fd\u6709\u6548\u533a\u5206\u5408\u7406\u4e0e\u4e0d\u5408\u7406\u7684\u5e02\u573a\u914d\u7f6e\uff0c\u4e3a\u4e0d\u53ef\u80fd\u7684\u54c1\u724c\u4ef7\u683c\u7ec4\u5408\u5206\u914d\u66f4\u9ad8\u80fd\u91cf\uff1b3\uff09\u5bf9\u7279\u5b9a\u5c5e\u6027\u7684\u5e72\u9884\u80fd\u56e0\u679c\u6027\u5730\u4f20\u64ad\u5230\u751f\u6210\u6587\u672c\uff0c\u4e14\u5e72\u9884\u540e\u7684\u8f93\u51fa\u5206\u5e03\u4e0e\u81ea\u7136\u6837\u672c\u4e00\u81f4\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5728\u8fde\u63a5\u9002\u5f53\u4e16\u754c\u6a21\u578b\u540e\u4e5f\u80fd\u5b9e\u73b0\u4e00\u81f4\u4e14\u53ef\u63a7\u7684\u751f\u6210\uff0c\u4e3a\u5c06\u8bed\u8a00\u80fd\u529b\u4e0e\u4e16\u754c\u7406\u89e3\u5206\u79bb\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u652f\u6301\u3002"}}
{"id": "2601.17588", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17588", "abs": "https://arxiv.org/abs/2601.17588", "authors": ["Marcus Ma", "Shrikanth Narayanan"], "title": "Intelligence Requires Grounding But Not Embodiment", "comment": null, "summary": "Recent advances in LLMs have reignited scientific debate over whether embodiment is necessary for intelligence. We present the argument that intelligence requires grounding, a phenomenon entailed by embodiment, but not embodiment itself. We define intelligence as the possession of four properties -- motivation, predictive ability, understanding of causality, and learning from experience -- and argue that each can be achieved by a non-embodied, grounded agent. We use this to conclude that grounding, not embodiment, is necessary for intelligence. We then present a thought experiment of an intelligent LLM agent in a digital environment and address potential counterarguments.", "AI": {"tldr": "This paper argues that grounding (not embodiment) is necessary for intelligence, defining intelligence through four properties and proposing a non-embodied grounded LLM agent thought experiment.", "motivation": "Recent advances in LLMs have reignited scientific debate over whether embodiment is necessary for intelligence.", "method": "Defines intelligence via four properties (motivation, predictive ability, causality understanding, and learning from experience), argues each can be achieved by non-embodied grounded agents, and presents a thought experiment of an intelligent LLM in a digital environment.", "result": "All four intelligence properties can be achieved by a non-embodied, grounded agent.", "conclusion": "Grounding, not embodiment, is necessary for intelligence."}}
{"id": "2601.18290", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.18290", "abs": "https://arxiv.org/abs/2601.18290", "authors": ["Yuan-De Jin", "Zheng-Fei Ye", "Wen-Long Ma"], "title": "Resource-Efficient Noise Spectroscopy for Generic Quantum Dephasing Environments", "comment": "9 pages, 3 figures (with 11 pages, 7 figures supplement)", "summary": "We present a resource-efficient method based on repetitive weak measurements to directly measure the noise spectrum of a generic quantum environment that causes qubit phase decoherence. The weak measurement is induced by a Ramsey interferometry measurement (RIM) on the qubit and periodically applied during the free evolution of the environment. We prove that the measurement correlation of such repetitive RIMs approximately corresponds to a direct sampling of the noise correlation function, thus enabling direct noise spectroscopy of the environment. Compared to dynamical-decoupling-based noise spectroscopy, this method can efficiently measure the full noise spectrum with the detected frequency range not limited by qubit coherence time. This method is also more resource-efficient than the correlation spectroscopy, as for the same detection accuracy with $N$ sampling times, it takes total detection time $O(N)$ while the latter one takes time $O(N^2)$. We numerically demonstrate this method for both bosonic and spin baths.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.17642", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17642", "abs": "https://arxiv.org/abs/2601.17642", "authors": ["Zhihao Zhang", "Liting Huang", "Guanghao Wu", "Preslav Nakov", "Heng Ji", "Usman Naseem"], "title": "Health-ORSC-Bench: A Benchmark for Measuring Over-Refusal and Safety Completion in Health Context", "comment": "Preprint", "summary": "Safety alignment in Large Language Models is critical for healthcare; however, reliance on binary refusal boundaries often results in \\emph{over-refusal} of benign queries or \\emph{unsafe compliance} with harmful ones. While existing benchmarks measure these extremes, they fail to evaluate Safe Completion: the model's ability to maximise helpfulness on dual-use or borderline queries by providing safe, high-level guidance without crossing into actionable harm. We introduce \\textbf{Health-ORSC-Bench}, the first large-scale benchmark designed to systematically measure \\textbf{Over-Refusal} and \\textbf{Safe Completion} quality in healthcare. Comprising 31,920 benign boundary prompts across seven health categories (e.g., self-harm, medical misinformation), our framework uses an automated pipeline with human validation to test models at varying levels of intent ambiguity. We evaluate 30 state-of-the-art LLMs, including GPT-5 and Claude-4, revealing a significant tension: safety-optimised models frequently refuse up to 80\\% of \"Hard\" benign prompts, while domain-specific models often sacrifice safety for utility. Our findings demonstrate that model family and size significantly influence calibration: larger frontier models (e.g., GPT-5, Llama-4) exhibit \"safety-pessimism\" and higher over-refusal than smaller or MoE-based counterparts (e.g., Qwen-3-Next), highlighting that current LLMs struggle to balance refusal and compliance. Health-ORSC-Bench provides a rigorous standard for calibrating the next generation of medical AI assistants toward nuanced, safe, and helpful completions. The code and data will be released upon acceptance. \\textcolor{red}{Warning: Some contents may include toxic or undesired contents.}", "AI": {"tldr": "A new healthcare safety benchmark (Health-ORSC-Bench) reveals that current LLMs struggle to balance refusal and compliance - larger models like GPT-5 show excessive caution (over-refusing 80% of benign prompts) while smaller models compromise safety for utility.", "motivation": "Safety alignment in healthcare LLMs using binary refusal boundaries causes over-refusal of benign queries or unsafe compliance with harmful ones. Existing benchmarks fail to evaluate \"Safe Completion\" - the ability to provide safe, high-level guidance on dual-use/borderline queries without enabling actionable harm.", "method": "Introducing Health-ORSC-Bench, a large-scale benchmark with 31,920 benign boundary prompts across seven health categories (self-harm, medical misinformation, etc.). Uses an automated pipeline with human validation to test 30 state-of-the-art LLMs at varying levels of intent ambiguity.", "result": "Safety-optimized models refuse up to 80% of \"Hard\" benign prompts, while domain-specific models sacrifice safety for utility. Larger frontier models (GPT-5, Llama-4) exhibit \"safety-pessimism\" and higher over-refusal than smaller/MoE-based models (Qwen-3-Next).", "conclusion": "Current LLMs struggle to balance refusal and compliance in healthcare contexts. Health-ORSC-Bench provides a rigorous standard for calibrating next-generation medical AI assistants toward nuanced, safe, and helpful completions."}}
{"id": "2601.18327", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.18327", "abs": "https://arxiv.org/abs/2601.18327", "authors": ["M. Y. Abd-Rabbou", "Irfan Siddique", "Saeed Haddadi", "Cong-Feng Qiao"], "title": "Scalable Repeater Architecture for Long-Range Quantum Energy Teleportation in Gapped Systems", "comment": null, "summary": "Quantum Energy Teleportation (QET) constitutes a paradigm-shifting protocol that permits the activation of local vacuum energy through the consumption of pre-existing entanglement and classical communication. Nevertheless, the implementation of QET is severely impeded by the fundamental locality of gapped many-body systems, where the exponential clustering of ground-state correlations restricts energy extraction to microscopic scales. In this work, we address this scalability crisis within the framework of the one-dimensional anisotropic XY model. We initially provide a rigorous characterization of a monolithic measurement-induced strategy, demonstrating that while bulk projective measurements can theoretically induce long-range couplings, the approach is rendered physically untenable by exponentially diverging thermodynamic costs and vanishing success probabilities. To circumvent this impasse, we propose and analyze a hierarchical quantum repeater architecture adapted for energy teleportation. By orchestrating heralded entanglement generation, iterative entanglement purification, and nested entanglement swapping, our protocol effectively counteracts the fidelity degradation inherent in noisy quantum channels. We establish that this architecture fundamentally alters the operational resource scaling from exponential to polynomial. This proves, for the first time, the physical permissibility and computational tractability of activating vacuum energy at arbitrary distances. The significance lies not in net energy gain, but in establishing long-range QET as a viable protocol for remote quantum control and resource distribution.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u91cf\u5b50\u4e2d\u7ee7\u67b6\u6784\u89e3\u51b3\u91cf\u5b50\u80fd\u91cf\u4f20\u8f93\uff08QET\uff09\u7684\u53ef\u6269\u5c55\u6027\u5371\u673a\uff0c\u5c06\u8d44\u6e90\u6d88\u8017\u4ece\u6307\u6570\u7ea7\u964d\u81f3\u591a\u9879\u5f0f\u7ea7\uff0c\u5b9e\u73b0\u4efb\u610f\u8ddd\u79bb\u7684\u771f\u7a7a\u80fd\u91cf\u6fc0\u6d3b", "motivation": "\u73b0\u6709QET\u534f\u8bae\u53d7\u9650\u4e8e\u6709\u80fd\u9699\u591a\u4f53\u7cfb\u7edf\u7684\u5c40\u57df\u6027\uff0c\u57fa\u6001\u5173\u8054\u7684\u6307\u6570\u805a\u7c7b\u7279\u6027\u5bfc\u81f4\u80fd\u91cf\u63d0\u53d6\u4ec5\u9650\u5fae\u89c2\u5c3a\u5ea6\uff0c\u4e9f\u9700\u7a81\u7834\u8ddd\u79bb\u9650\u5236\u4ee5\u5b9e\u73b0\u8fdc\u7a0b\u91cf\u5b50\u63a7\u5236", "method": "\u57281D\u5404\u5411\u5f02\u6027XY\u6a21\u578b\u6846\u67b6\u4e0b\uff0c\u5148\u5206\u6790\u5355\u4f53\u6d4b\u91cf\u7b56\u7565\u7684\u7f3a\u9677\uff0c\u518d\u8bbe\u8ba1\u57fa\u4e8e heralded \u7ea0\u7f20\u751f\u6210\u3001\u8fed\u4ee3\u7eaf\u5316\u548c\u5d4c\u5957\u4ea4\u6362\u7684\u5206\u5c42\u91cf\u5b50\u4e2d\u7ee7\u67b6\u6784", "result": "\u65b0\u67b6\u6784\u5c06\u64cd\u4f5c\u8d44\u6e90\u6d88\u8017\u4ece\u6307\u6570\u7ea7\u4f18\u5316\u81f3\u591a\u9879\u5f0f\u7ea7\uff0c\u9996\u6b21\u8bc1\u660e\u4efb\u610f\u8ddd\u79bb\u771f\u7a7a\u80fd\u91cf\u6fc0\u6d3b\u7684\u7269\u7406\u53ef\u884c\u6027\u4e0e\u8ba1\u7b97\u53ef\u5904\u7406\u6027", "conclusion": "QET\u867d\u65e0\u51c0\u80fd\u91cf\u589e\u76ca\uff0c\u4f46\u4e3a\u8fdc\u7a0b\u91cf\u5b50\u64cd\u63a7\u4e0e\u8d44\u6e90\u5206\u914d\u63d0\u4f9b\u4e86\u53ef\u884c\u534f\u8bae\uff0c\u63a8\u52a8\u4e86\u91cf\u5b50\u80fd\u91cf\u4f20\u8f93\u7684\u5b9e\u9645\u5e94\u7528"}}
{"id": "2601.17111", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17111", "abs": "https://arxiv.org/abs/2601.17111", "authors": ["Xuan-Phi Nguyen", "Shrey Pandit", "Austin Xu", "Caiming Xiong", "Shafiq Joty"], "title": "Least-Loaded Expert Parallelism: Load Balancing An Imbalanced Mixture-of-Experts", "comment": "Preprint", "summary": "Mixture-of-Experts (MoE) models are typically pre-trained with explicit load-balancing constraints to ensure statistically balanced expert routing. Despite this, we observe that even well-trained MoE models exhibit significantly imbalanced routing. This behavior is arguably natural-and even desirable - as imbalanced routing allows models to concentrate domain-specific knowledge within a subset of experts. Expert parallelism (EP) is designed to scale MoE models by distributing experts across multiple devices, but with a less-discussed assumption of balanced routing. Under extreme imbalance, EP can funnel a disproportionate number of tokens to a small number of experts, leading to compute- and memory-bound failures on overloaded devices during post-training or inference, where explicit load balancing is often inapplicable. We propose Least-Loaded Expert Parallelism (LLEP), a novel EP algorithm that dynamically reroutes excess tokens and associated expert parameters from overloaded devices to underutilized ones. This ensures that all devices complete their workloads within the minimum collective latency while respecting memory constraints. Across different model scales, LLEP achieves up to 5x speedup and 4x reduction in peak memory usage compared to standard EP. This enables faster and higher-throughput post-training and inference, with ~1.9x faster for gpt-oss-120b. We support our method with extensive theoretical analysis and comprehensive empirical evaluations, including ablation studies. These results illuminate key trade-offs and enable a principled framework for hardware-specific hyper-parameter tuning to achieve optimal performance.", "AI": {"tldr": "\u63d0\u51faLLEP\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u91cd\u8def\u7531\u8fc7\u5ea6\u8d1f\u8f7d\u8bbe\u5907\u7684token\u548c\u4e13\u5bb6\u53c2\u6570\u5230\u7a7a\u95f2\u8bbe\u5907\uff0c\u89e3\u51b3MoE\u6a21\u578b\u63a8\u7406\u65f6\u8def\u7531\u4e0d\u5747\u8861\u5bfc\u81f4\u7684\u8ba1\u7b97\u5185\u5b58\u74f6\u9888\uff0c\u5b9e\u73b0\u6700\u9ad85\u500d\u52a0\u901f\u548c4\u500d\u5185\u5b58\u964d\u4f4e", "motivation": "MoE\u6a21\u578b\u5373\u4f7f\u9884\u8bad\u7ec3\u65f6\u52a0\u5165\u8d1f\u8f7d\u5747\u8861\u7ea6\u675f\uff0c\u4ecd\u4f1a\u8868\u73b0\u51fa\u663e\u8457\u4e0d\u5747\u8861\u7684\u8def\u7531\u884c\u4e3a\uff0c\u8fd9\u662f\u81ea\u7136\u4e14\u6709\u76ca\u7684\uff08\u53ef\u96c6\u4e2d\u9886\u57df\u77e5\u8bc6\uff09\u3002\u4f46\u4e13\u5bb6\u5e76\u884c\uff08EP\uff09\u5047\u8bbe\u5747\u8861\u8def\u7531\uff0c\u5728\u6781\u7aef\u4e0d\u5747\u8861\u65f6\u4f1a\u5bfc\u81f4\u5c11\u6570\u8bbe\u5907\u8fc7\u8f7d\uff0c\u5f15\u53d1\u8ba1\u7b97\u548c\u5185\u5b58\u74f6\u9888\uff0c\u4e14\u63a8\u7406\u9636\u6bb5\u65e0\u6cd5\u4f7f\u7528\u663e\u5f0f\u8d1f\u8f7d\u5747\u8861", "method": "Least-Loaded Expert Parallelism (LLEP)\uff1a\u4e00\u79cd\u65b0\u578bEP\u7b97\u6cd5\uff0c\u52a8\u6001\u76d1\u6d4b\u8bbe\u5907\u8d1f\u8f7d\uff0c\u5c06\u8fc7\u8f7d\u8bbe\u5907\u7684\u8d85\u989dtoken\u53ca\u76f8\u5173\u4e13\u5bb6\u53c2\u6570\u5b9e\u65f6\u91cd\u8def\u7531\u5230\u5229\u7528\u7387\u4f4e\u7684\u8bbe\u5907\u4e0a\uff0c\u786e\u4fdd\u6240\u6709\u8bbe\u5907\u5728\u6700\u5c0f\u96c6\u4f53\u5ef6\u8fdf\u5185\u5b8c\u6210\u5de5\u4f5c\u4e14\u6ee1\u8db3\u5185\u5b58\u7ea6\u675f", "result": "\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0a\uff0c\u76f8\u6bd4\u6807\u51c6EP\u5b9e\u73b0\u6700\u9ad85\u500d\u52a0\u901f\u548c4\u500d\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\u964d\u4f4e\uff1b\u5728gpt-oss-120b\u4e0a\u8fbe\u5230\u7ea61.9\u500d\u52a0\u901f\uff0c\u652f\u6301\u66f4\u5feb\u66f4\u9ad8\u541e\u5410\u7684\u540e\u8bad\u7ec3\u548c\u63a8\u7406", "conclusion": "LLEP\u4e3a\u4e0d\u5747\u8861\u8def\u7531\u4e0b\u7684MoE\u6a21\u578b\u6269\u5c55\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5e7f\u6cdb\u7684\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u8bc4\u4f30\uff08\u542b\u6d88\u878d\u7814\u7a76\uff09\uff0c\u63ed\u793a\u4e86\u5173\u952e\u6743\u8861\uff0c\u5efa\u7acb\u4e86\u786c\u4ef6\u7279\u5b9a\u8d85\u53c2\u6570\u8c03\u6574\u7684\u539f\u5219\u6027\u6846\u67b6\u4ee5\u5b9e\u73b0\u6700\u4f18\u6027\u80fd"}}
{"id": "2601.17678", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2601.17678", "abs": "https://arxiv.org/abs/2601.17678", "authors": ["Zhiyu An", "Wan Du"], "title": "DIML: Differentiable Inverse Mechanism Learning from Behaviors of Multi-Agent Learning Trajectories", "comment": null, "summary": "We study inverse mechanism learning: recovering an unknown incentive-generating mechanism from observed strategic interaction traces of self-interested learning agents. Unlike inverse game theory and multi-agent inverse reinforcement learning, which typically infer utility/reward parameters inside a structured mechanism, our target includes unstructured mechanism -- a (possibly neural) mapping from joint actions to per-agent payoffs. Unlike differentiable mechanism design, which optimizes mechanisms forward, we infer mechanisms from behavior in an observational setting. We propose DIML, a likelihood-based framework that differentiates through a model of multi-agent learning dynamics and uses the candidate mechanism to generate counterfactual payoffs needed to predict observed actions. We establish identifiability of payoff differences under a conditional logit response model and prove statistical consistency of maximum likelihood estimation under standard regularity conditions. We evaluate DIML with simulated interactions of learning agents across unstructured neural mechanisms, congestion tolling, public goods subsidies, and large-scale anonymous games. DIML reliably recovers identifiable incentive differences and supports counterfactual prediction, where its performance rivals tabular enumeration oracle in small environments and its convergence scales to large, hundred-participant environments. Code to reproduce our experiments is open-sourced.", "AI": {"tldr": "\u63d0\u51faDIML\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u89c2\u5bdf\u5230\u7684\u7b56\u7565\u4e92\u52a8\u4e2d\u6062\u590d\u672a\u77e5\u7684\u6fc0\u52b1\u751f\u6210\u673a\u5236\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u5e76\u5728\u591a\u79cd\u6e38\u620f\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982\u9006\u535a\u5f08\u8bba\u548c\u591a\u667a\u80fd\u4f53\u9006\u5f3a\u5316\u5b66\u4e60\u53ea\u80fd\u63a8\u65ad\u7ed3\u6784\u5316\u673a\u5236\u5185\u7684\u6548\u7528\u53c2\u6570\uff0c\u800c\u53ef\u5fae\u673a\u5236\u8bbe\u8ba1\u662f\u6b63\u5411\u4f18\u5316\u7684\u3002\u672c\u6587\u76ee\u6807\u662f\u63a8\u65ad\u975e\u7ed3\u6784\u5316\u673a\u5236\uff08\u5982\u795e\u7ecf\u6620\u5c04\uff09\uff0c\u4e14\u662f\u5728\u89c2\u5bdf\u6027\u8bbe\u7f6e\u4e0b\u4ece\u884c\u4e3a\u4e2d\u63a8\u65ad\u3002", "method": "DIML\u662f\u4e00\u79cd\u57fa\u4e8e\u4f3c\u7136\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u52a8\u529b\u5b66\u6a21\u578b\u8fdb\u884c\u5fae\u5206\uff0c\u5e76\u4f7f\u7528\u5019\u9009\u673a\u5236\u751f\u6210\u53cd\u4e8b\u5b9e\u6536\u76ca\u6765\u9884\u6d4b\u89c2\u5bdf\u5230\u7684\u884c\u4e3a\u3002", "result": "\u5728\u6761\u4ef6logit\u54cd\u5e94\u6a21\u578b\u4e0b\u786e\u7acb\u4e86\u6536\u76ca\u5dee\u5f02\u7684\u53ef\u8bc6\u522b\u6027\uff0c\u8bc1\u660e\u4e86\u6807\u51c6\u6b63\u5219\u6761\u4ef6\u4e0b\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u7684\u7edf\u8ba1\u4e00\u81f4\u6027\u3002\u5728\u591a\u79cd\u6a21\u62df\u73af\u5883\uff08\u795e\u7ecf\u673a\u5236\u3001\u62e5\u5835\u6536\u8d39\u3001\u516c\u5171\u54c1\u8865\u8d34\u3001\u5927\u89c4\u6a21\u533f\u540d\u6e38\u620f\uff09\u4e2d\u9a8c\u8bc1\uff0c\u80fd\u53ef\u9760\u6062\u590d\u53ef\u8bc6\u522b\u7684\u6fc0\u52b1\u5dee\u5f02\uff0c\u652f\u6301\u53cd\u4e8b\u5b9e\u9884\u6d4b\uff0c\u6027\u80fd\u5ab2\u7f8e\u5c0f\u578b\u73af\u5883\u4e2d\u7684\u8868\u683c\u679a\u4e3eoracle\uff0c\u5e76\u80fd\u6269\u5c55\u5230\u767e\u4eba\u89c4\u6a21\u73af\u5883\u3002", "conclusion": "DIML\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u9006\u673a\u5236\u5b66\u4e60\u95ee\u9898\uff0c\u517c\u5177\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u8bc1\u6027\u80fd\uff0c\u4e3a\u4ece\u6570\u636e\u4e2d\u6062\u590d\u6fc0\u52b1\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2601.18347", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.18347", "abs": "https://arxiv.org/abs/2601.18347", "authors": ["Matthieu Arnhem", "Radim Filip"], "title": "Scaling of multicopy constructive interference of Gaussian states", "comment": "16 pages, 7 figures", "summary": "Quantum technology advances crucially depend on the scaling up of essential quantum resources. Their ideal multiplexing offers more significant gains in applications; however, the scaling of the nonidentical, fragile and varying resources is neither theoretically nor experimentally known. For bosonic systems, multimode interference is an essential tool already widely exploited to develop quantum technology. Here, we analyze, predict and compare essential scaling laws for a constructive interference of multiplexed nonclassical Gaussian states carrying information by displacement with weakly fluctuating squeezing in different multimode interference architectures. The signal-to-noise ratio quantifies the increase in displacement relative to the noise. We introduce the gain-to-instability ratio to numerically estimate the effect of unexplored resource instabilities in a large scale interference scheme. The use of the gain-to-instability ratio to quantify the scaling laws opens steps for extensive theoretical investigation of other bosonic resources and follow-up feasible experimental verification necessary for further development of these platforms.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u643a\u5e26\u4f4d\u79fb\u4fe1\u606f\u7684\u5f31\u538b\u7f29\u6ce2\u52a8\u975e\u7ecf\u5178\u9ad8\u65af\u6001\u5728\u591a\u6a21\u5e72\u6d89\u4e2d\u7684\u5efa\u8bbe\u6027\u5e72\u6d89\u7f29\u653e\u89c4\u5f8b\uff0c\u63d0\u51fa\u589e\u76ca-\u4e0d\u7a33\u5b9a\u6027\u6bd4\u65b0\u6307\u6807\u91cf\u5316\u8d44\u6e90\u4e0d\u7a33\u5b9a\u6027\uff0c\u4e3a\u91cf\u5b50\u6280\u672f\u6269\u5c55\u63d0\u4f9b\u7406\u8bba\u548c\u5b9e\u9a8c\u57fa\u7840\u3002", "motivation": "\u91cf\u5b50\u6280\u672f\u8fdb\u6b65\u4f9d\u8d56\u91cf\u5b50\u8d44\u6e90\u7684\u6269\u5c55\uff0c\u4f46\u975e\u76f8\u540c\u3001\u8106\u5f31\u91cf\u5b50\u8d44\u6e90\u7684\u6269\u5c55\u673a\u5236\u5c1a\u4e0d\u660e\u786e\uff0c\u4e9f\u9700\u7406\u8bba\u548c\u5b9e\u9a8c\u7814\u7a76\u3002", "method": "\u5206\u6790\u9884\u6d4b\u5e76\u6bd4\u8f83\u4e0d\u540c\u591a\u6a21\u5e72\u6d89\u67b6\u6784\u4e2d\u590d\u7528\u975e\u7ecf\u5178\u9ad8\u65af\u6001\u7684\u5efa\u8bbe\u6027\u5e72\u6d89\u7f29\u653e\u5f8b\uff0c\u4f7f\u7528\u4fe1\u566a\u6bd4\u91cf\u5316\u4f4d\u79fb\u589e\u76ca\uff0c\u63d0\u51fa\u589e\u76ca-\u4e0d\u7a33\u5b9a\u6027\u6bd4\u6570\u503c\u8bc4\u4f30\u8d44\u6e90\u4e0d\u7a33\u5b9a\u6027\u5f71\u54cd\u3002", "result": "\u83b7\u5f97\u4e86\u591a\u6a21\u5e72\u6d89\u4e2d\u91cf\u5b50\u8d44\u6e90\u7684\u5173\u952e\u7f29\u653e\u89c4\u5f8b\uff0c\u63d0\u51fa\u7684\u589e\u76ca-\u4e0d\u7a33\u5b9a\u6027\u6bd4\u4e3a\u5927\u89c4\u6a21\u5e72\u6d89\u65b9\u6848\u4e2d\u672a\u63a2\u7d22\u7684\u8d44\u6e90\u4e0d\u7a33\u5b9a\u6027\u6548\u5e94\u63d0\u4f9b\u4e86\u6570\u503c\u4f30\u8ba1\u5de5\u5177\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u73bb\u8272\u5b50\u91cf\u5b50\u8d44\u6e90\u7684\u6269\u5c55\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5f00\u542f\u4e86\u5176\u4ed6\u73bb\u8272\u5b50\u8d44\u6e90\u7684\u7406\u8bba\u7814\u7a76\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u7684\u53ef\u80fd\uff0c\u5bf9\u91cf\u5b50\u6280\u672f\u5e73\u53f0\u53d1\u5c55\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2601.17699", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17699", "abs": "https://arxiv.org/abs/2601.17699", "authors": ["Harper Hua", "Zhen Han", "Zhengyuan Shen", "Jeremy Lee", "Patrick Guan", "Qi Zhu", "Sullam Jeoung", "Yueyan Chen", "Yunfei Bai", "Shuai Wang", "Vassilis Ioannidis", "Huzefa Rangwala"], "title": "SQL-Trail: Multi-Turn Reinforcement Learning with Interleaved Feedback for Text-to-SQL", "comment": null, "summary": "While large language models (LLMs) have substantially improved Text-to-SQL generation, a pronounced gap remains between AI systems and human experts on challenging benchmarks such as BIRD-SQL. We argue this gap stems largely from the prevailing single-pass paradigm, which lacks the iterative reasoning, schema exploration, and error-correction behaviors that humans naturally employ. To address this limitation, we introduce SQL-Trail, a multi-turn reinforcement learning (RL) agentic framework for Text-to-SQL. Rather than producing a query in one shot, SQL-Trail interacts with the database environment and uses execution feedback to iteratively refine its predictions. Our approach centers on two key ideas: (i) an adaptive turn-budget allocation mechanism that scales the agent's interaction depth to match question difficulty, and (ii) a composite reward panel that jointly incentivizes SQL correctness and efficient exploration. Across benchmarks, SQL-Trail sets a new state of the art and delivers strong data efficiency--up to 18x higher than prior single-pass RL state-of-the-art methods. Notably, our 7B and 14B models outperform substantially larger proprietary systems by 5% on average, underscoring the effectiveness of interactive, agentic workflows for robust Text-to-SQL generation.", "AI": {"tldr": "SQL-Trail\u662f\u4e00\u4e2a\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u6267\u884c\u53cd\u9988\u8fed\u4ee3\u4f18\u5316SQL\u67e5\u8be2\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u8f6e\u6b21\u9884\u7b97\u548c\u590d\u5408\u5956\u52b1\u673a\u5236\uff0c\u5728Text-to-SQL\u4efb\u52a1\u4e0a\u5b9e\u73b0\u65b0SOTA\uff0c\u6570\u636e\u6548\u7387\u63d0\u534718\u500d\uff0c\u4e14\u5c0f\u6a21\u578b\u8d85\u8d8a\u66f4\u5927\u89c4\u6a21\u7684\u5546\u4e1a\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524d\u5355\u8f6e\u751f\u6210\u8303\u5f0f\u7f3a\u4e4f\u4eba\u7c7b\u4e13\u5bb6\u6240\u5177\u5907\u7684\u8fed\u4ee3\u63a8\u7406\u3001\u6a21\u5f0f\u63a2\u7d22\u548c\u9519\u8bef\u4fee\u6b63\u80fd\u529b\uff0c\u5bfc\u81f4AI\u7cfb\u7edf\u5728BIRD-SQL\u7b49\u590d\u6742\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u4e0e\u4eba\u7c7b\u8868\u73b0\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "method": "\u63d0\u51faSQL-Trail\u591a\u8f6eRL\u6846\u67b6\uff1a(i) \u81ea\u9002\u5e94\u8f6e\u6b21\u9884\u7b97\u5206\u914d\u673a\u5236\uff0c\u6839\u636e\u95ee\u9898\u96be\u5ea6\u52a8\u6001\u8c03\u6574\u4ea4\u4e92\u6df1\u5ea6\uff1b(ii) \u590d\u5408\u5956\u52b1\u9762\u677f\uff0c\u8054\u5408\u6fc0\u52b1SQL\u6b63\u786e\u6027\u548c\u9ad8\u6548\u63a2\u7d22\uff1b\u901a\u8fc7\u4e0e\u6570\u636e\u5e93\u73af\u5883\u4ea4\u4e92\u548c\u6267\u884c\u53cd\u9988\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0SOTA\uff0c\u6570\u636e\u6548\u7387\u6bd4\u5355\u8f6eRL\u65b9\u6cd5\u63d0\u5347\u9ad8\u8fbe18\u500d\uff1b7B\u548c14B\u6a21\u578b\u5e73\u5747\u6027\u80fd\u8d85\u8d8a\u5927\u5f97\u591a\u7684\u5546\u4e1a\u7cfb\u7edf5%\u3002", "conclusion": "\u4ea4\u4e92\u5f0f\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u5bf9\u9c81\u68d2Text-to-SQL\u751f\u6210\u975e\u5e38\u6709\u6548\uff0c\u591a\u8f6e\u8fed\u4ee3\u65b9\u6cd5\u80fd\u4ee5\u66f4\u5c0f\u6a21\u578b\u5b9e\u73b0\u8d85\u8d8a\u5355\u8f6e\u8303\u5f0f\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2601.18351", "categories": ["quant-ph", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.18351", "abs": "https://arxiv.org/abs/2601.18351", "authors": ["Pranav Kulkarni", "Leo S\u00fcnkel", "Michael K\u00f6lle"], "title": "An Adaptive Purification Controller for Quantum Networks: Dynamic Protocol Selection and Multipartite Distillation", "comment": null, "summary": "Efficient entanglement distribution is the cornerstone of the Quantum Internet. However, physical link parameters such as photon loss, memory coherence time, and gate error rates fluctuate dynamically, rendering static purification strategies suboptimal. In this paper, we propose an Adaptive Purification Controller (APC) that autonomously optimizes the entanglement distillation sequence to maximize the \"goodput,\" the rate of delivered pairs meeting a strict fidelity threshold. By treating protocol selection as a resource allocation problem, the APC dynamically switches between purification depths and protocol families (e.g., BBPSSW vs. DEJMPS) to navigate the trade-off between generation rate and state quality. Using a dynamic programming planner with Pareto pruning, simulation results demonstrate that our approach eliminates the \"fidelity cliffs\" inherent in static protocols and prevents resource wastage in high-noise regimes. Furthermore, we extend the controller to heterogeneous scenarios, demonstrating robustness for both multipartite GHZ state generation and continuous variable systems using effective noiseless linear amplification models. We benchmark its computational overhead, confirming real-time feasibility with decision latencies in the millisecond range per link.", "AI": {"tldr": "\u9488\u5bf9\u91cf\u5b50\u4e92\u8054\u7f51\u4e2d\u7ea0\u7f20\u5206\u53d1\u6548\u7387\u95ee\u9898\uff0c\u63d0\u51fa\u81ea\u9002\u5e94\u7eaf\u5316\u63a7\u5236\u5668(APC)\uff0c\u901a\u8fc7\u52a8\u6001\u4f18\u5316\u84b8\u998f\u534f\u8bae\u548c\u7eaf\u5316\u6df1\u5ea6\uff0c\u6700\u5927\u5316\u597d\u541e\u5410\u91cf(goodput)\uff0c\u6d88\u9664\u4fdd\u771f\u5ea6\u60ac\u5d16\u5e76\u907f\u514d\u9ad8\u566a\u58f0\u4e0b\u7684\u8d44\u6e90\u6d6a\u8d39\u3002", "motivation": "\u9ad8\u6548\u7ea0\u7f20\u5206\u53d1\u662f\u91cf\u5b50\u4e92\u8054\u7f51\u7684\u57fa\u77f3\uff0c\u4f46\u7269\u7406\u94fe\u8def\u53c2\u6570\uff08\u5149\u5b50\u635f\u8017\u3001\u5b58\u50a8\u5668\u76f8\u5e72\u65f6\u95f4\u3001\u95e8\u8bef\u5dee\u7387\uff09\u52a8\u6001\u6ce2\u52a8\uff0c\u5bfc\u81f4\u9759\u6001\u7eaf\u5316\u7b56\u7565\u6b21\u4f18\uff0c\u9700\u5728\u52a8\u6001\u73af\u5883\u4e2d\u6700\u5927\u5316\u6ee1\u8db3\u4e25\u683c\u4fdd\u771f\u5ea6\u9608\u503c\u7684\u4ea4\u4ed8\u901f\u7387\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u7eaf\u5316\u63a7\u5236\u5668(APC)\uff0c\u5c06\u534f\u8bae\u9009\u62e9\u89c6\u4e3a\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u52a8\u6001\u5207\u6362\u7eaf\u5316\u6df1\u5ea6\u548c\u534f\u8bae\u65cf\uff08BBPSSW vs DEJMPS\uff09\uff0c\u91c7\u7528\u52a8\u6001\u89c4\u5212\u89c4\u5212\u5668\u4e0e\u5e15\u7d2f\u6258\u526a\u679d\u7b56\u7565\uff0c\u5e76\u6269\u5c55\u81f3\u5f02\u8d28\u573a\u666f\uff08\u591a\u4f53GHZ\u6001\u548c\u8fde\u7eed\u53d8\u91cf\u7cfb\u7edf\uff09\u3002", "result": "\u4eff\u771f\u8868\u660eAPC\u6d88\u9664\u4e86\u9759\u6001\u534f\u8bae\u7684\"\u4fdd\u771f\u5ea6\u60ac\u5d16\"\uff0c\u9632\u6b62\u9ad8\u566a\u58f0\u673a\u5236\u4e0b\u7684\u8d44\u6e90\u6d6a\u8d39\uff1b\u5728\u5f02\u8d28\u573a\u666f\u4e2d\u8868\u73b0\u7a33\u5065\uff1b\u51b3\u7b56\u5ef6\u8fdf\u5728\u6beb\u79d2\u7ea7\uff0c\u5177\u5907\u5b9e\u65f6\u53ef\u884c\u6027\u3002", "conclusion": "APC\u901a\u8fc7\u52a8\u6001\u81ea\u9002\u5e94\u4f18\u5316\u663e\u8457\u63d0\u5347\u91cf\u5b50\u7ea0\u7f20\u5206\u53d1\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u672a\u6765\u91cf\u5b50\u4e92\u8054\u7f51\u7684\u5b9e\u7528\u5316\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u65b9\u6848\u3002"}}
{"id": "2601.17717", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17717", "abs": "https://arxiv.org/abs/2601.17717", "authors": ["Kaituo Zhang", "Mingzhi Hu", "Hoang Anh Duy Le", "Fariha Kabir Torsha", "Zhimeng Jiang", "Minh Khai Bui", "Chia-Yuan Chang", "Yu-Neng Chuang", "Zhen Xiong", "Ying Lin", "Guanchu Wang", "Na Zou"], "title": "The LLM Data Auditor: A Metric-oriented Survey on Quality and Trustworthiness in Evaluating Synthetic Data", "comment": null, "summary": "Large Language Models (LLMs) have emerged as powerful tools for generating data across various modalities. By transforming data from a scarce resource into a controllable asset, LLMs mitigate the bottlenecks imposed by the acquisition costs of real-world data for model training, evaluation, and system iteration. However, ensuring the high quality of LLM-generated synthetic data remains a critical challenge. Existing research primarily focuses on generation methodologies, with limited direct attention to the quality of the resulting data. Furthermore, most studies are restricted to single modalities, lacking a unified perspective across different data types. To bridge this gap, we propose the \\textbf{LLM Data Auditor framework}. In this framework, we first describe how LLMs are utilized to generate data across six distinct modalities. More importantly, we systematically categorize intrinsic metrics for evaluating synthetic data from two dimensions: quality and trustworthiness. This approach shifts the focus from extrinsic evaluation, which relies on downstream task performance, to the inherent properties of the data itself. Using this evaluation system, we analyze the experimental evaluations of representative generation methods for each modality and identify substantial deficiencies in current evaluation practices. Based on these findings, we offer concrete recommendations for the community to improve the evaluation of data generation. Finally, the framework outlines methodologies for the practical application of synthetic data across different modalities.", "AI": {"tldr": "\u63d0\u51faLLM Data Auditor\u6846\u67b6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u516d\u79cd\u6a21\u6001\u7684LLM\u751f\u6210\u6570\u636e\u8d28\u91cf\uff0c\u4ece\u5916\u5728\u4efb\u52a1\u8bc4\u4f30\u8f6c\u5411\u5185\u5728\u6570\u636e\u5c5e\u6027\u8bc4\u4f30\uff0c\u5e76\u7ed9\u51fa\u6539\u8fdb\u5efa\u8bae", "motivation": "LLM\u53ef\u751f\u6210\u591a\u6a21\u6001\u6570\u636e\u7f13\u89e3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u91cd\u751f\u6210\u65b9\u6cd5\u8f7b\u6570\u636e\u8d28\u91cf\uff0c\u4e14\u591a\u4e3a\u5355\u6a21\u6001\u7f3a\u4e4f\u7edf\u4e00\u89c6\u89d2", "method": "\u6784\u5efaLLM Data Auditor\u6846\u67b6\uff0c\u63cf\u8ff0\u516d\u79cd\u6a21\u6001\u7684\u6570\u636e\u751f\u6210\u65b9\u5f0f\uff0c\u4ece\u8d28\u91cf\u548c\u53ef\u4fe1\u5ea6\u4e24\u4e2a\u7ef4\u5ea6\u7cfb\u7edf\u5316\u5185\u5728\u8bc4\u4f30\u6307\u6807", "result": "\u5206\u6790\u5404\u6a21\u6001\u4ee3\u8868\u6027\u751f\u6210\u65b9\u6cd5\u7684\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u53d1\u73b0\u5f53\u524d\u8bc4\u4f30\u5b9e\u8df5\u5b58\u5728\u91cd\u5927\u7f3a\u9677\uff0c\u5e76\u63d0\u4f9b\u5177\u4f53\u6539\u8fdb\u5efa\u8bae", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u591a\u6a21\u6001\u5408\u6210\u6570\u636e\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u8d28\u91cf\u8bc4\u4f30\u4f53\u7cfb\uff0c\u8bc6\u522b\u4e86\u8bc4\u4f30\u76f2\u533a\uff0c\u63a8\u52a8\u793e\u533a\u5efa\u7acb\u66f4\u5b8c\u5584\u7684\u751f\u6210\u6570\u636e\u8bc4\u4f30\u6807\u51c6"}}
{"id": "2601.18366", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.18366", "abs": "https://arxiv.org/abs/2601.18366", "authors": ["Celia \u00c1lvarez \u00c1lvarez", "Mariamo Mussa Juane"], "title": "Bohr's complementarity principle tested on a real quantum computer via interferometer experiments", "comment": null, "summary": "Bohr's Complementarity Principle is a core concept of quantum mechanics. In this article, an updated complementarity relation for the wave and ondulatory aspects of a quantum system is presented and discussed. Two interferometric experiments are implemented in one and two qubit circuits and executed on real hardware. The final state density matrices are reconstructed using quantum state tomography and the complementarity relation is tested via direct computation. Results of the executions are presented both graphically and with a mean squared error analysis for a better comprehension.", "AI": {"tldr": "This paper experimentally verifies an updated quantum complementarity principle through qubit-based interferometric experiments and quantum state tomography.", "motivation": "To address limitations in Bohr's original Complementarity Principle by developing and testing an updated framework for quantifying wave-particle duality in quantum systems.", "method": "Implementing two interferometric experiments in 1-qubit and 2-qubit quantum circuits on real hardware, reconstructing final state density matrices via quantum state tomography, and computing the updated complementarity relation.", "result": "The updated complementarity relation was validated through direct computation, with results presented graphically and analyzed using mean squared error (MSE) to quantify agreement with theoretical predictions.", "conclusion": "The experimental results confirm the validity of the proposed updated complementarity relation for quantum systems, demonstrating its applicability in real quantum hardware despite implementation constraints."}}
{"id": "2601.17722", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17722", "abs": "https://arxiv.org/abs/2601.17722", "authors": ["Ying Mo", "Yu Bai", "Dapeng Sun", "Yuqian Shi", "Yukai Miao", "Li Chen", "Dan Li"], "title": "EntWorld: A Holistic Environment and Benchmark for Verifiable Enterprise GUI Agents", "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have enabled agents to operate in open-ended web and operating system environments. However, existing benchmarks predominantly target consumer-oriented scenarios (e.g., e-commerce and travel booking), failing to capture the complexity and rigor of professional enterprise workflows. Enterprise systems pose distinct challenges, including high-density user interfaces, strict business logic constraints, and a strong reliance on precise, state-consistent information retrieval-settings in which current generalist agents often struggle. To address this gap, we introduce EntWorld, a large-scale benchmark consisting of 1,756 tasks across six representative enterprise domains, including customer relationship management (CRM), information technology infrastructure library (ITIL), and enterprise resource planning (ERP) systems. Unlike previous datasets that depend on fragile execution traces or extensive manual annotation, EntWorld adopts a schema-grounded task generation framework that directly reverse-engineers business logic from underlying database schemas, enabling the synthesis of realistic, long-horizon workflows. Moreover, we propose a SQL-based deterministic verification mechanism in building datasets that replaces ambiguous visual matching with rigorous state-transition validation. Experimental results demonstrate that state-of-the-art models (e.g., GPT-4.1) achieve 47.61% success rate on EntWorld, substantially lower than the human performance, highlighting a pronounced enterprise gap in current agentic capabilities and the necessity of developing domain-specific agents. We release EntWorld as a rigorous testbed to facilitate the development and evaluation of the next generation of enterprise-ready digital agents.", "AI": {"tldr": "EntWorld\u4f01\u4e1a\u5de5\u4f5c\u6d41\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793aAI\u667a\u80fd\u4f53\u5728\u4e13\u4e1a\u4f01\u4e1a\u73af\u5883\u4e2d\u7684\u6027\u80fd\u663e\u8457\u843d\u540e\u4e8e\u6d88\u8d39\u7ea7\u573a\u666f\uff0c\u5f53\u524d\u9876\u5c16\u6a21\u578b\u6210\u529f\u7387\u4ec547.61%", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u96c6\u4e2d\u5728\u7535\u5546\u3001\u65c5\u884c\u7b49\u6d88\u8d39\u573a\u666f\uff0c\u65e0\u6cd5\u53cd\u6620\u4e13\u4e1a\u4f01\u4e1a\u5de5\u4f5c\u6d41\u7684\u590d\u6742\u6027\u3002\u4f01\u4e1a\u7cfb\u7edf\u9762\u4e34\u9ad8\u5bc6\u5ea6\u7528\u6237\u754c\u9762\u3001\u4e25\u683c\u4e1a\u52a1\u903b\u8f91\u7ea6\u675f\u548c\u7cbe\u786e\u72b6\u6001\u4e00\u81f4\u6027\u4fe1\u606f\u68c0\u7d22\u7b49\u72ec\u7279\u6311\u6218\uff0c\u800c\u901a\u7528\u667a\u80fd\u4f53\u5728\u8fd9\u4e9b\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faEntWorld\u57fa\u51c6\uff0c\u5305\u542b6\u5927\u4f01\u4e1a\u9886\u57df\uff08CRM\u3001ITIL\u3001ERP\u7b49\uff09\u76841,756\u4e2a\u4efb\u52a1\u3002\u91c7\u7528\u6a21\u5f0f grounded \u7684\u4efb\u52a1\u751f\u6210\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u6570\u636e\u5e93\u6a21\u5f0f\u53cd\u63a8\u4e1a\u52a1\u903b\u8f91\u4ee5\u5408\u6210\u771f\u5b9e\u957f\u65f6\u7a0b\u5de5\u4f5c\u6d41\uff0c\u5e76\u5f15\u5165\u57fa\u4e8eSQL\u7684\u786e\u5b9a\u6027\u9a8c\u8bc1\u673a\u5236\u66ff\u4ee3\u6a21\u7cca\u7684\u89c6\u89c9\u5339\u914d\u3002", "result": "GPT-4.1\u7b49\u9876\u5c16\u6a21\u578b\u5728EntWorld\u4e0a\u6210\u529f\u7387\u4ec5\u4e3a47.61%\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u8868\u73b0\uff0c\u51f8\u663e\u4e86\u5f53\u524d\u667a\u80fd\u4f53\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u7684\u663e\u8457\u80fd\u529b\u5dee\u8ddd\u3002", "conclusion": "EntWorld\u4f5c\u4e3a\u4e25\u8c28\u6d4b\u8bd5\u5e73\u53f0\u53d1\u5e03\uff0c\u63ed\u793a\u4e86\u5f00\u53d1\u9886\u57df\u4e13\u7528\u4f01\u4e1a\u667a\u80fd\u4f53\u7684\u5fc5\u8981\u6027\uff0c\u5c06\u52a9\u529b\u4e0b\u4e00\u4ee3\u4f01\u4e1a\u7ea7\u6570\u5b57\u667a\u80fd\u4f53\u7684\u7814\u53d1\u4e0e\u8bc4\u4f30\u3002"}}
{"id": "2601.17135", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17135", "abs": "https://arxiv.org/abs/2601.17135", "authors": ["Jakob Karalus", "Friedhelm Schwenker"], "title": "ConceptACT: Episode-Level Concepts for Sample-Efficient Robotic Imitation Learning", "comment": null, "summary": "Imitation learning enables robots to acquire complex manipulation skills from human demonstrations, but current methods rely solely on low-level sensorimotor data while ignoring the rich semantic knowledge humans naturally possess about tasks. We present ConceptACT, an extension of Action Chunking with Transformers that leverages episode-level semantic concept annotations during training to improve learning efficiency. Unlike language-conditioned approaches that require semantic input at deployment, ConceptACT uses human-provided concepts (object properties, spatial relationships, task constraints) exclusively during demonstration collection, adding minimal annotation burden. We integrate concepts using a modified transformer architecture in which the final encoder layer implements concept-aware cross-attention, supervised to align with human annotations. Through experiments on two robotic manipulation tasks with logical constraints, we demonstrate that ConceptACT converges faster and achieves superior sample efficiency compared to standard ACT. Crucially, we show that architectural integration through attention mechanisms significantly outperforms naive auxiliary prediction losses or language-conditioned models. These results demonstrate that properly integrated semantic supervision provides powerful inductive biases for more efficient robot learning.", "AI": {"tldr": "ConceptACT extends Action Chunking with Transformers by incorporating semantic concept annotations during training only, using concept-aware cross-attention to improve robot imitation learning efficiency without requiring semantic input at deployment.", "motivation": "Current imitation learning methods rely solely on low-level sensorimotor data while ignoring rich human semantic knowledge about tasks, leading to inefficient learning.", "method": "ConceptACT uses human-provided semantic concepts (object properties, spatial relationships, task constraints) exclusively during demonstration collection, integrating them via modified transformer architecture with concept-aware cross-attention in the final encoder layer, supervised to align with human annotations.", "result": "Experiments on two robotic manipulation tasks show ConceptACT converges faster, achieves superior sample efficiency compared to standard ACT, and architectural integration through attention significantly outperforms naive auxiliary losses or language-conditioned models.", "conclusion": "Properly integrated semantic supervision provides powerful inductive biases for more efficient robot learning, enabling better performance without additional deployment-time annotation burden."}}
{"id": "2601.17735", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17735", "abs": "https://arxiv.org/abs/2601.17735", "authors": ["Kyungho Kim", "Geon Lee", "Juyeon Kim", "Dongwon Choi", "Shinhwan Kang", "Kijung Shin"], "title": "ReFuGe: Feature Generation for Prediction Tasks on Relational Databases with LLM Agents", "comment": "Accepted in ACM WWW 2026 (Short Paper)", "summary": "Relational databases (RDBs) play a crucial role in many real-world web applications, supporting data management across multiple interconnected tables. Beyond typical retrieval-oriented tasks, prediction tasks on RDBs have recently gained attention. In this work, we address this problem by generating informative relational features that enhance predictive performance. However, generating such features is challenging: it requires reasoning over complex schemas and exploring a combinatorially large feature space, all without explicit supervision. To address these challenges, we propose ReFuGe, an agentic framework that leverages specialized large language model agents: (1) a schema selection agent identifies the tables and columns relevant to the task, (2) a feature generation agent produces diverse candidate features from the selected schema, and (3) a feature filtering agent evaluates and retains promising features through reasoning-based and validation-based filtering. It operates within an iterative feedback loop until performance converges. Experiments on RDB benchmarks demonstrate that ReFuGe substantially improves performance on various RDB prediction tasks. Our code and datasets are available at https://github.com/K-Kyungho/REFUGE.", "AI": {"tldr": "ReFuGe is an LLM-agent framework that automatically generates relational features for RDB prediction tasks through schema selection, feature generation, and filtering in an iterative loop, significantly improving performance.", "motivation": "Relational databases are crucial for web applications, but generating informative relational features for prediction tasks is challenging due to complex schemas, combinatorially large feature spaces, and lack of explicit supervision.", "method": "ReFuGe uses three specialized LLM agents: (1) schema selection to identify relevant tables/columns, (2) feature generation to create diverse candidates, and (3) feature filtering via reasoning and validation. It operates through iterative feedback until performance converges.", "result": "Experiments on RDB benchmarks demonstrate that ReFuGe substantially improves performance across various relational database prediction tasks.", "conclusion": "The ReFuGe framework effectively automates relational feature generation, overcoming key challenges and delivering significant predictive performance gains on RDB tasks."}}
{"id": "2601.18384", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.18384", "abs": "https://arxiv.org/abs/2601.18384", "authors": ["Minjun Jeon", "Zhenyu Cai"], "title": "Quantum Error Correction on Error-mitigated Physical Qubits", "comment": "20 pages, 11 figures", "summary": "We present a general framework for applying linear quantum error mitigation (QEM) techniques directly to physical qubits within a logical qubit to suppress logical errors. By exploiting the linearity of quantum error correction (QEC), we demonstrate that any linear QEM method$\\unicode{x2014}$including probabilistic error cancellation (PEC), zero-noise extrapolation (ZNE), and symmetry verification$\\unicode{x2014}$can be integrated into the physical layer without requiring modifications to the subsequent QEC decoder. Applying this framework to memory experiments using PEC, we analytically prove and numerically verify that the leading-order contribution to the logical error can be removed, increasing the effective code distance by 2. Our simulations on repetition and rotated surface codes show that a distance-3 code with physical-level PEC achieves logical error rates lower than or similar to a distance-5 unmitigated code while using 40% and 64% fewer qubits, respectively. These results establish physical-level QEM as a widely compatible and resource-efficient strategy for enhancing logical performance in early fault-tolerant architectures.", "AI": {"tldr": "A framework applies quantum error mitigation (PEC, ZNE, symmetry verification) directly to physical qubits within logical qubits, removing leading-order logical errors to increase effective code distance by 2 and reduce qubit overhead by 40-64% while maintaining performance.", "motivation": "To develop resource-efficient strategies for suppressing logical errors in early fault-tolerant quantum computing by leveraging quantum error mitigation at the physical layer without requiring modifications to QEC decoders.", "method": "Exploits linearity of quantum error correction to integrate linear QEM techniques (PEC, ZNE, symmetry verification) directly into the physical qubit layer. Validated through analytical proofs and numerical simulations of PEC on memory experiments and surface code implementations.", "result": "Removes leading-order logical error contribution, increasing effective code distance by 2. Distance-3 codes with physical-level PEC achieve logical error rates comparable to unmitigated distance-5 codes while using 40% fewer qubits (repetition codes) and 64% fewer qubits (rotated surface codes).", "conclusion": "Physical-level QEM is a widely compatible and resource-efficient strategy that enhances logical qubit performance in early fault-tolerant architectures by reducing qubit overhead and suppressing logical errors."}}
{"id": "2601.17180", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17180", "abs": "https://arxiv.org/abs/2601.17180", "authors": ["In\u00e9s Gonzalez-Pepe", "Vinuyan Sivakolunthu", "Jacob Fortin", "Yohan Chatelain", "Tristan Glatard"], "title": "Conservative & Aggressive NaNs Accelerate U-Nets for Neuroimaging", "comment": null, "summary": "Deep learning models for neuroimaging increasingly rely on large architectures, making efficiency a persistent concern despite advances in hardware. Through an analysis of numerical uncertainty of convolutional neural networks (CNNs), we observe that many operations are applied to values dominated by numerical noise and have negligible influence on model outputs. In some models, up to two-thirds of convolution operations appear redundant. We introduce Conservative & Aggressive NaNs, two novel variants of max pooling and unpooling that identify numerically unstable voxels and replace them with NaNs, allowing subsequent layers to skip computations on irrelevant data. Both methods are implemented within PyTorch and require no architectural changes. We evaluate these approaches on four CNN models spanning neuroimaging and image classification tasks. For inputs containing at least 50% NaNs, we observe consistent runtime improvements; for data with more than two-thirds NaNs )common in several neuroimaging settings) we achieve an average inference speedup of 1.67x. Conservative NaNs reduces convolution operations by an average of 30% across models and datasets, with no measurable performance degradation, and can skip up to 64.64% of convolutions in specific layers. Aggressive NaNs can skip up to 69.30% of convolutions but may occasionally affect performance. Overall, these methods demonstrate that numerical uncertainty can be exploited to reduce redundant computation and improve inference efficiency in CNNs.", "AI": {"tldr": "This paper proposes Conservative & Aggressive NaNs, novel max pooling/unpooling variants that replace numerically unstable voxels with NaNs to skip redundant computations in CNNs, achieving 1.67x inference speedup in neuroimaging with no performance loss.", "motivation": "Large CNN architectures in neuroimaging suffer from computational inefficiency despite hardware advances, with up to two-thirds of convolution operations being redundant due to numerical noise dominating input values.", "method": "Introduces two NaN-based voxel filtering methods: Conservative NaNs (safe computation skipping) and Aggressive NaNs (maximal skipping), implemented as PyTorch-compatible pooling/unpooling layers that identify and replace numerically unstable values without architectural changes.", "result": "For neuroimaging data with >66% NaNs: 1.67x average inference speedup; Conservative NaNs skips 30% convolutions (up to 64.64% in specific layers) with zero accuracy loss, while Aggressive NaNs skips up to 69.30% but may occasionally reduce performance.", "conclusion": "Numerical uncertainty in CNNs can be systematically exploited to eliminate redundant computations, significantly improving inference efficiency in neuroimaging and general image tasks without model retraining or structural modifications."}}
{"id": "2601.17744", "categories": ["cs.AI", "cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.17744", "abs": "https://arxiv.org/abs/2601.17744", "authors": ["Amjad Fatmi"], "title": "Faramesh: A Protocol-Agnostic Execution Control Plane for Autonomous Agent Systems", "comment": "40 pages, 10 figures. Preprint. Code: https://github.com/faramesh/faramesh-core", "summary": "Autonomous agent systems increasingly trigger real-world side effects: deploying infrastructure, modifying databases, moving money, and executing workflows. Yet most agent stacks provide no mandatory execution checkpoint where organizations can deterministically permit, deny, or defer an action before it changes reality. This paper introduces Faramesh, a protocol-agnostic execution control plane that enforces execution-time authorization for agent-driven actions via a non-bypassable Action Authorization Boundary (AAB). Faramesh canonicalizes agent intent into a Canonical Action Representation (CAR), evaluates actions deterministically against policy and state, and issues a decision artifact (PERMIT/DEFER/DENY) that executors must validate prior to execution. The system is designed to be framework- and model-agnostic, supports multi-agent and multi-tenant deployments, and remains independent of transport protocols (e.g., MCP). Faramesh further provides decision-centric, append-only provenance logging keyed by canonical action hashes, enabling auditability, verification, and deterministic replay without re-running agent reasoning. We show how these primitives yield enforceable, predictable governance for autonomous execution while avoiding hidden coupling to orchestration layers or observability-only approaches.", "AI": {"tldr": "Faramesh\u662f\u4e00\u4e2a\u534f\u8bae\u65e0\u5173\u7684\u6267\u884c\u63a7\u5236\u5e73\u9762\uff0c\u901a\u8fc7\u5728\u667a\u80fd\u4f53\u884c\u52a8\u524d\u8bbe\u7f6e\u4e0d\u53ef\u7ed5\u8fc7\u7684\u6388\u6743\u8fb9\u754c\uff0c\u5c06\u610f\u56fe\u8f6c\u5316\u4e3a\u6807\u51c6\u52a8\u4f5c\u8868\u793a\u5e76\u57fa\u4e8e\u7b56\u7565\u8fdb\u884c\u786e\u5b9a\u6027\u8bc4\u4f30\uff0c\u5b9e\u73b0\u5bf9\u81ea\u4e3b\u7cfb\u7edf\u73b0\u5b9e\u4e16\u754c\u64cd\u4f5c\u7684\u53ef\u5ba1\u8ba1\u3001\u53ef\u9a8c\u8bc1\u7684\u6cbb\u7406\u63a7\u5236", "motivation": "\u81ea\u4e3b\u667a\u80fd\u4f53\u7cfb\u7edf\u65e5\u76ca\u89e6\u53d1\u771f\u5b9e\u4e16\u754c\u7684\u526f\u4f5c\u7528\uff08\u90e8\u7f72\u57fa\u7840\u8bbe\u65bd\u3001\u4fee\u6539\u6570\u636e\u5e93\u3001\u8f6c\u79fb\u8d44\u91d1\u3001\u6267\u884c\u5de5\u4f5c\u6d41\uff09\uff0c\u4f46\u73b0\u6709\u667a\u80fd\u4f53\u6808\u7f3a\u4e4f\u5728\u6539\u53d8\u73b0\u5b9e\u524d\u53ef\u786e\u5b9a\u6027\u5141\u8bb8\u3001\u62d2\u7edd\u6216\u63a8\u8fdf\u884c\u52a8\u7684\u5fc5\u8981\u6267\u884c\u68c0\u67e5\u70b9\uff0c\u5b58\u5728\u5b89\u5168\u548c\u6cbb\u7406\u98ce\u9669", "method": "\u63d0\u51faFaramesh\u7cfb\u7edf\uff1a1) \u5efa\u7acb\u4e0d\u53ef\u7ed5\u8fc7\u7684\u52a8\u4f5c\u6388\u6743\u8fb9\u754c(AAB)\uff1b2) \u5c06\u667a\u80fd\u4f53\u610f\u56fe\u89c4\u8303\u5316\u4e3a\u89c4\u8303\u52a8\u4f5c\u8868\u793a(CAR)\uff1b3) \u57fa\u4e8e\u7b56\u7565\u548c\u72b6\u6001\u8fdb\u884c\u786e\u5b9a\u6027\u8bc4\u4f30\u5e76\u751f\u6210PERMIT/DEFER/DENY\u51b3\u7b56\u5de5\u4ef6\uff1b4) \u6267\u884c\u5668\u5fc5\u987b\u9a8c\u8bc1\u51b3\u7b56\u540e\u624d\u53ef\u6267\u884c\uff1b5) \u63d0\u4f9b\u57fa\u4e8e\u89c4\u8303\u52a8\u4f5c\u54c8\u5e0c\u7684\u4e0d\u53ef\u53d8\u6eaf\u6e90\u65e5\u5fd7", "result": "\u5b9e\u73b0\u4e86\u53ef\u5f3a\u5236\u6267\u884c\u7684\u3001\u53ef\u9884\u6d4b\u7684\u81ea\u4e3b\u6267\u884c\u6cbb\u7406\u6846\u67b6\uff0c\u652f\u6301\u591a\u667a\u80fd\u4f53\u591a\u79df\u6237\u90e8\u7f72\uff0c\u72ec\u7acb\u4e8e\u4f20\u8f93\u534f\u8bae\u548c\u6a21\u578b\u6846\u67b6\uff0c\u63d0\u4f9b\u5ba1\u8ba1\u80fd\u529b\u3001\u9a8c\u8bc1\u673a\u5236\u548c\u786e\u5b9a\u6027\u91cd\u653e\u529f\u80fd\uff0c\u65e0\u9700\u91cd\u65b0\u8fd0\u884c\u667a\u80fd\u4f53\u63a8\u7406\u8fc7\u7a0b", "conclusion": "Faramesh\u901a\u8fc7\u6807\u51c6\u5316\u7684\u6388\u6743\u8fb9\u754c\u548c\u51b3\u7b56\u6eaf\u6e90\u673a\u5236\uff0c\u4e3a\u81ea\u4e3b\u667a\u80fd\u4f53\u7684\u73b0\u5b9e\u4e16\u754c\u64cd\u4f5c\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u5b89\u5168\u6cbb\u7406\u5c42\uff0c\u5728\u4fdd\u6301\u7cfb\u7edf\u7075\u6d3b\u6027\u7684\u540c\u65f6\u907f\u514d\u4e86\u4e0e\u7f16\u6392\u5c42\u7684\u9690\u5f0f\u8026\u5408\uff0c\u8d85\u8d8a\u4e86\u4ec5\u53ef\u89c2\u6d4b\u7684\u76d1\u63a7\u65b9\u6848"}}
{"id": "2601.18419", "categories": ["quant-ph", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.18419", "abs": "https://arxiv.org/abs/2601.18419", "authors": ["Michael K\u00f6lle", "Christian Reff", "Leo S\u00fcnkel", "Julian Hager", "Gerhard Stenzel", "Claudia Linnhoff-Popien"], "title": "Emergent Cooperation in Quantum Multi-Agent Reinforcement Learning Using Communication", "comment": "Accepted at IEEE ICC 2026", "summary": "Emergent cooperation in classical Multi-Agent Reinforcement Learning has gained significant attention, particularly in the context of Sequential Social Dilemmas (SSDs). While classical reinforcement learning approaches have demonstrated capability for emergent cooperation, research on extending these methods to Quantum Multi-Agent Reinforcement Learning remains limited, particularly through communication. In this paper, we apply communication approaches to quantum Q-Learning agents: the Mutual Acknowledgment Token Exchange (MATE) protocol, its extension Mutually Endorsed Distributed Incentive Acknowledgment Token Exchange (MEDIATE), the peer rewarding mechanism Gifting, and Reinforced Inter-Agent Learning (RIAL). We evaluate these approaches in three SSDs: the Iterated Prisoner's Dilemma, Iterated Stag Hunt, and Iterated Game of Chicken. Our experimental results show that approaches using MATE with temporal-difference measure (MATE\\textsubscript{TD}), AutoMATE, MEDIATE-I, and MEDIATE-S achieved high cooperation levels across all dilemmas, demonstrating that communication is a viable mechanism for fostering emergent cooperation in Quantum Multi-Agent Reinforcement Learning.", "AI": {"tldr": "This paper extends classical MARL cooperation protocols to quantum settings, showing that communication mechanisms like MATE and MEDIATE enable high cooperation in quantum agents across three social dilemmas.", "motivation": "Research on extending classical reinforcement learning cooperation methods to Quantum Multi-Agent Reinforcement Learning remains limited, particularly through communication protocols.", "method": "Applied quantum Q-Learning agents with communication protocols (MATE, MEDIATE, Gifting, RIAL) and evaluated them in three Sequential Social Dilemmas: Iterated Prisoner's Dilemma, Iterated Stag Hunt, and Iterated Game of Chicken.", "result": "Approaches using MATE_TD, AutoMATE, MEDIATE-I, and MEDIATE-S achieved high cooperation levels across all three dilemmas.", "conclusion": "Communication is a viable mechanism for fostering emergent cooperation in Quantum Multi-Agent Reinforcement Learning."}}
{"id": "2601.17183", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17183", "abs": "https://arxiv.org/abs/2601.17183", "authors": ["Farzam Asad", "Junaid Saif Khan", "Maria Tariq", "Sundus Munir", "Muhammad Adnan Khan"], "title": "Federated Proximal Optimization for Privacy-Preserving Heart Disease Prediction: A Controlled Simulation Study on Non-IID Clinical Data", "comment": "27 pages, 7 figures, 4 tables", "summary": "Healthcare institutions have access to valuable patient data that could be of great help in the development of improved diagnostic models, but privacy regulations like HIPAA and GDPR prevent hospitals from directly sharing data with one another. Federated Learning offers a way out to this problem by facilitating collaborative model training without having the raw patient data centralized. However, clinical datasets intrinsically have non-IID (non-independent and identically distributed) features brought about by demographic disparity and diversity in disease prevalence and institutional practices. This paper presents a comprehensive simulation research of Federated Proximal Optimization (FedProx) for Heart Disease prediction based on UCI Heart Disease dataset. We generate realistic non-IID data partitions by simulating four heterogeneous hospital clients from the Cleveland Clinic dataset (303 patients), by inducing statistical heterogeneity by demographic-based stratification. Our experimental results show that FedProx with proximal parameter mu=0.05 achieves 85.00% accuracy, which is better than both centralized learning (83.33%) and isolated local models (78.45% average) without revealing patient privacy. Through generous sheer ablation studies with statistical validation on 50 independent runs we demonstrate that proximal regularization is effective in curbing client drift in heterogeneous environments. This proof-of-concept research offers algorithmic insights and practical deployment guidelines for real-world federated healthcare systems, and thus, our results are directly transferable to hospital IT-administrators, implementing privacy-preserving collaborative learning.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6a21\u62df\u56db\u5bb6\u5f02\u6784\u533b\u9662\uff0c\u57fa\u4e8eUCI\u5fc3\u810f\u75c5\u6570\u636e\u96c6\u8bc4\u4f30\u8054\u90a6\u8fd1\u7aef\u4f18\u5316\u7b97\u6cd5(FedProx)\u5728\u5fc3\u810f\u75c5\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u8bc1\u660e\u8be5\u7b97\u6cd5\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\uff0c\u80fd\u6709\u6548\u89e3\u51b3\u6570\u636e\u975e\u72ec\u7acb\u540c\u5206\u5e03\u95ee\u9898\uff0c\u51c6\u786e\u7387\u8fbe85%\uff0c\u4f18\u4e8e\u96c6\u4e2d\u5b66\u4e60\u548c\u672c\u5730\u6a21\u578b\u3002", "motivation": "\u533b\u7597\u6570\u636e\u56e0HIPAA\u548cGDPR\u7b49\u9690\u79c1\u6cd5\u89c4\u65e0\u6cd5\u76f4\u63a5\u5171\u4eab\uff0c\u9650\u5236\u4e86\u8bca\u65ad\u6a21\u578b\u7684\u53d1\u5c55\u3002\u8054\u90a6\u5b66\u4e60\u867d\u80fd\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u534f\u4f5c\u8bad\u7ec3\uff0c\u4f46\u4e34\u5e8a\u6570\u636e\u7684\u975e\u72ec\u7acb\u540c\u5206\u5e03\u7279\u6027(\u7531\u4eba\u53e3\u7edf\u8ba1\u5dee\u5f02\u548c\u673a\u6784\u5b9e\u8df5\u591a\u6837\u6027\u5bfc\u81f4)\u5bfc\u81f4\u5ba2\u6237\u7aef\u6f02\u79fb\u95ee\u9898\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "\u57fa\u4e8eUCI\u5fc3\u810f\u75c5\u6570\u636e\u96c6(Cleveland Clinic, 303\u540d\u60a3\u8005)\uff0c\u901a\u8fc7\u4eba\u53e3\u7edf\u8ba1\u5206\u5c42\u6a21\u62df\u56db\u5bb6\u5f02\u6784\u533b\u9662\u5ba2\u6237\u7aef\uff0c\u521b\u5efa\u771f\u5b9e\u975eIID\u6570\u636e\u5212\u5206\u3002\u91c7\u7528FedProx\u7b97\u6cd5\uff0c\u8bbe\u7f6e\u8fd1\u7aef\u53c2\u6570mu=0.05\uff0c\u8fdb\u884c50\u6b21\u72ec\u7acb\u8fd0\u884c\u4ee5\u9a8c\u8bc1\u7edf\u8ba1\u663e\u8457\u6027\uff0c\u5bf9\u6bd4\u96c6\u4e2d\u5b66\u4e60\u3001\u672c\u5730\u5b66\u4e60\u548c\u4e0d\u540cFedProx\u914d\u7f6e\u7684\u6548\u679c\u3002", "result": "FedProx(mu=0.05)\u8fbe\u523085.00%\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u96c6\u4e2d\u5b66\u4e60(83.33%)\u548c\u672c\u5730\u6a21\u578b\u5e73\u5747\u51c6\u786e\u7387(78.45%)\u3002\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9e\u8fd1\u7aef\u6b63\u5219\u5316\u80fd\u6709\u6548\u6291\u5236\u5f02\u6784\u73af\u5883\u4e2d\u7684\u5ba2\u6237\u7aef\u6f02\u79fb\uff0c50\u6b21\u72ec\u7acb\u8fd0\u884c\u7ed3\u679c\u5177\u6709\u7edf\u8ba1\u663e\u8457\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u771f\u5b9e\u4e16\u754c\u8054\u90a6\u533b\u7597\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7b97\u6cd5\u6d1e\u89c1\u548c\u5b9e\u7528\u90e8\u7f72\u6307\u5357\uff0c\u8bc1\u660eFedProx\u80fd\u6709\u6548\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u534f\u4f5c\u5b66\u4e60\uff0c\u7ed3\u679c\u53ef\u76f4\u63a5\u5e94\u7528\u4e8e\u533b\u9662IT\u7ba1\u7406\u5458\u7684\u5b9e\u8df5\uff0c\u63a8\u52a8\u533b\u7597AI\u5728\u5408\u89c4\u6846\u67b6\u4e0b\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.18426", "categories": ["quant-ph", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.18426", "abs": "https://arxiv.org/abs/2601.18426", "authors": ["Mingyao Cui", "Qunsong Zeng", "Kaibin Huang"], "title": "A Theory of Single-Antenna Atomic Beamforming", "comment": "14 pages, 8 figures. Submitted to IEEE JSAC", "summary": "Leveraging the quantum advantages of highly excited atoms, Rydberg atomic receivers (RAREs) represent a paradigm shift in radio wave detection, offering high sensitivity and broadband reception. However, existing studies largely model RAREs as isotropic point receivers and overlook the spatial variations of atomic quantum states within vapor cells, thus inaccurately characterizing their reception patterns. To address this issue, we present a theoretical analysis of the aforementioned spatial responses of a standard local-oscillator (LO)- dressed RARE. Our results reveal that increasing the vapor-cell length produces a receive beam aligned with the LO field, with a beamwidth inversely proportional to the cell length. This finding enables atomic beamforming to enhance received signal-to-noise ratio using only a single-antenna RARE. Furthermore, we derive the achievable beamforming gain by characterizing and balancing the fundamental tradeoff between the effects of increasing the vapor cell length and the exponential power decay of laser propagating through the cell. To overcome the limitation imposed by exponential decay, we propose a novel RARE architecture termed segmental vapor cell. This architecture consists of vapor-cell segments separated by clear-air gaps, allowing the total cell length (and hence propagation loss) to remain fixed while the effective cell length increases. As a result, this segmented design expands the effective atom-field interaction area without increasing the total vapor cell length, yielding a narrower beamwidth and thus higher beamforming gain as compared with a traditional continuous vapor cell.", "AI": {"tldr": "\u9488\u5bf9\u91cc\u5fb7\u5821\u539f\u5b50\u63a5\u6536\u5668(RARE)\u7a7a\u95f4\u54cd\u5e94\u5efa\u6a21\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u672c\u6587\u7406\u8bba\u5206\u6790\u4e86LO\u9a71\u52a8RARE\u7684\u7a7a\u95f4\u7279\u6027\uff0c\u53d1\u73b0\u589e\u52a0\u6c14\u5ba4\u957f\u5ea6\u53ef\u5f62\u6210\u7a84\u6ce2\u675f\uff0c\u5e76\u63d0\u51fa\u4e86\u5206\u6bb5\u6c14\u5ba4\u7ed3\u6784\u4ee5\u514b\u670d\u6fc0\u5149\u8870\u51cf\u9650\u5236\uff0c\u5b9e\u73b0\u5355\u5929\u7ebf\u6ce2\u675f\u6210\u5f62\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5c06RARE\u7b80\u5316\u4e3a\u5404\u5411\u540c\u6027\u70b9\u63a5\u6536\u5668\uff0c\u5ffd\u7565\u4e86\u6c14\u5ba4\u5185\u91cf\u5b50\u6001\u7684\u7a7a\u95f4\u53d8\u5316\uff0c\u5bfc\u81f4\u63a5\u6536\u6a21\u5f0f\u4e0d\u51c6\u786e\u3002", "method": "\u5efa\u7acb\u6807\u51c6LO\u9a71\u52a8RARE\u7684\u7a7a\u95f4\u54cd\u5e94\u7406\u8bba\u6a21\u578b\uff0c\u5206\u6790\u6c14\u5ba4\u957f\u5ea6\u5bf9\u6ce2\u675f\u7279\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u8bbe\u8ba1\u5206\u6bb5\u6c14\u5ba4\u67b6\u6784\u3002", "result": "\u589e\u52a0\u6c14\u5ba4\u957f\u5ea6\u53ef\u4ea7\u751f\u4e0eLO\u573a\u5bf9\u9f50\u7684\u63a5\u6536\u6ce2\u675f\uff0c\u5176\u5bbd\u5ea6\u4e0e\u957f\u5ea6\u6210\u53cd\u6bd4\uff1b\u4f46\u9762\u4e34\u6fc0\u5149\u6307\u6570\u8870\u51cf\u7684\u5236\u7ea6\uff1b\u5206\u6bb5\u6c14\u5ba4\u53ef\u5728\u56fa\u5b9a\u603b\u957f\u4e0b\u589e\u52a0\u6709\u6548\u957f\u5ea6\uff0c\u83b7\u5f97\u66f4\u7a84\u6ce2\u675f\u548c\u66f4\u9ad8\u589e\u76ca\u3002", "conclusion": "\u8be5\u7814\u7a76\u5b9e\u73b0\u4e86\u5355\u5929\u7ebf\u539f\u5b50\u6ce2\u675f\u6210\u5f62\uff0c\u5206\u6bb5\u6c14\u5ba4\u8bbe\u8ba1\u6709\u6548\u7a81\u7834\u4e86\u4f20\u64ad\u635f\u8017\u9650\u5236\uff0c\u4e3a\u63d0\u5347RARE\u6027\u80fd\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848\u3002"}}
{"id": "2601.17189", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17189", "abs": "https://arxiv.org/abs/2601.17189", "authors": ["Sabrina Mokhtari", "Sara Kodeiri", "Shubhankar Mohapatra", "Florian Tramer", "Gautam Kamath"], "title": "Rethinking Benchmarks for Differentially Private Image Classification", "comment": null, "summary": "We revisit benchmarks for differentially private image classification. We suggest a comprehensive set of benchmarks, allowing researchers to evaluate techniques for differentially private machine learning in a variety of settings, including with and without additional data, in convex settings, and on a variety of qualitatively different datasets. We further test established techniques on these benchmarks in order to see which ideas remain effective in different settings. Finally, we create a publicly available leader board for the community to track progress in differentially private machine learning.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u5957\u5168\u9762\u7684\u5dee\u5206\u9690\u79c1\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4f53\u7cfb\uff0c\u6db5\u76d6\u591a\u573a\u666f\u8bbe\u7f6e\uff0c\u6d4b\u8bd5\u73b0\u6709\u6280\u672f\u6709\u6548\u6027\uff0c\u5e76\u5efa\u7acb\u516c\u5f00\u6392\u884c\u699c\u4ee5\u8ffd\u8e2a\u9886\u57df\u8fdb\u5c55\u3002", "motivation": "\u73b0\u6709\u5dee\u5206\u9690\u79c1\u673a\u5668\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e0d\u591f\u5168\u9762\uff0c\u7f3a\u4e4f\u5728\u4e0d\u540c\u573a\u666f\u4e0b\uff08\u5982\u6709\u65e0\u989d\u5916\u6570\u636e\u3001\u51f8\u8bbe\u7f6e\u3001\u4e0d\u540c\u6570\u636e\u96c6\uff09\u5bf9\u6280\u672f\u8fdb\u884c\u7cfb\u7edf\u6027\u8bc4\u4f30\u7684\u6846\u67b6\u3002", "method": "\u6784\u5efa\u6db5\u76d6\u591a\u79cd\u573a\u666f\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u5305\u62ec\u6709\u65e0\u989d\u5916\u6570\u636e\u3001\u51f8\u4f18\u5316\u8bbe\u7f6e\u548c\u4e0d\u540c\u8d28\u6027\u6570\u636e\u96c6\uff1b\u5e76\u5728\u5176\u4e0a\u6d4b\u8bd5\u73b0\u6709\u4e3b\u6d41\u6280\u672f\u4ee5\u68c0\u9a8c\u5176\u8de8\u573a\u666f\u6709\u6548\u6027\u3002", "result": "\u5efa\u7acb\u4e86\u516c\u5f00\u53ef\u7528\u7684\u793e\u533a\u6392\u884c\u699c\uff0c\u4f9b\u7814\u7a76\u4eba\u5458\u8ffd\u8e2a\u5dee\u5206\u9690\u79c1\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u3002", "conclusion": "\u901a\u8fc7\u5168\u9762\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u4e0d\u540c\u6280\u672f\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u5dee\u5f02\uff0c\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u5de5\u5177\u548c\u8fdb\u5ea6\u8ffd\u8e2a\u5e73\u53f0\uff0c\u63a8\u52a8\u5dee\u5206\u9690\u79c1\u673a\u5668\u5b66\u4e60\u7814\u7a76\u66f4\u52a0\u7cfb\u7edf\u548c\u53ef\u6bd4\u3002"}}
{"id": "2601.17789", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17789", "abs": "https://arxiv.org/abs/2601.17789", "authors": ["Yiming Su", "Kunzhao Xu", "Yanjie Gao", "Fan Yang", "Cheng Li", "Mao Yang", "Tianyin Xu"], "title": "Neuro-Symbolic Verification on Instruction Following of LLMs", "comment": null, "summary": "A fundamental problem of applying Large Language Models (LLMs) to important applications is that LLMs do not always follow instructions, and violations are often hard to observe or check. In LLM-based agentic workflows, such violations can propagate and amplify along reasoning chains, causing task failures and system incidents. This paper presents NSVIF, a neuro-symbolic framework for verifying whether an LLM's output follows the instructions used to prompt the LLM. NSVIF is a universal, general-purpose verifier; it makes no assumption about the instruction or the LLM. NSVIF formulates instruction-following verification as a constraint-satisfaction problem by modeling user instructions as constraints. NSVIF models both logical and semantic constraints; constraint solving is done by a unified solver that orchestrates logical reasoning and semantic analysis. To evaluate NSVIF, we develop VIFBENCH, a new benchmark for instruction-following verifiers with fine-grained data labels. Experiments show that NSVIF significantly outperforms LLM-based approaches and provides interpretable feedback. We also show that feedback from NSVIF helps improve LLMs' instruction-following capability without post-training.", "AI": {"tldr": "This paper proposes NSVIF, a neuro-symbolic framework that verifies LLM instruction-following by modeling instructions as logical and semantic constraints and solving them with a unified solver. It significantly outperforms LLM-based approaches, provides interpretable feedback, and can improve LLMs' instruction-following capability without post-training.", "motivation": "LLMs often fail to follow instructions, and these violations are difficult to detect. In agentic workflows, such violations can propagate and amplify along reasoning chains, causing task failures and system incidents. There is a critical need for reliable verification of instruction-following behavior.", "method": "NSVIF is a universal, general-purpose neuro-symbolic verifier that formulates instruction-following verification as a constraint-satisfaction problem. It models user instructions as both logical and semantic constraints, which are solved by a unified solver that orchestrates logical reasoning and semantic analysis. No assumptions are made about the specific instruction or LLM being verified.", "result": "Experiments on the new VIFBENCH benchmark show NSVIF significantly outperforms LLM-based approaches and provides interpretable feedback. The feedback from NSVIF also helps improve LLMs' instruction-following capability without requiring post-training.", "conclusion": "NSVIF demonstrates the effectiveness of neuro-symbolic methods for instruction-following verification. It offers a practical, interpretable solution for improving LLM reliability in real-world applications, particularly in agentic workflows where instruction violations can have cascading effects."}}
{"id": "2601.18482", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.18482", "abs": "https://arxiv.org/abs/2601.18482", "authors": ["Fu Zhang", "Yuming Zhao"], "title": "Physics-Informed Hybrid Quantum-Classical Dispatching for Large-Scale Renewable Power Systems:A Noise-Resilient Framework", "comment": null, "summary": "The integration of high-penetration renewable energy introduces significant stochasticity and non-convexity into power system dispatching, challenging the computational limits of classical optimization. While Variational Quantum Algorithms (VQAs) on Noisy Intermediate-Scale Quantum (NISQ) devices offer a promising path for combinatorial acceleration, existing approaches typically treat the power grid as a \"black box\", suffering from poor scalability (barren plateaus) and frequent violations of physical constraints. Bridging these gaps, this paper proposes a Physics-Informed Hybrid Quantum-Classical Dispatching (PI-HQCD) framework. We construct a topology-aware Hamiltonian that explicitly embeds linearized power flow equations, storage dynamics, and multi-timescale coupling directly into the quantum substrate, significantly reducing the search space dimensionality. We further derive a noise-adaptive regularization mechanism that theoretically bounds the effective Lipschitz constant of the objective function, guaranteeing convergence stability under realistic quantum measurement noise. Numerical experiments on the IEEE 39-bus benchmark and a 118-bus regional grid demonstrate that PI-HQCD achieves superior economic efficiency and higher renewable utilization compared to stochastic dual dynamic programming (SDDP). Theoretical analysis confirms that this topology-aware design leads to an O(1/N) gradient variance scaling, effectively mitigating barren plateaus and ensuring scalability for larger networks. This work establishes a rigorous paradigm for embedding engineering physics into quantum computing, paving the way for practical quantum advantage in next-generation grid operations.", "AI": {"tldr": "\u63d0\u51fa\u7269\u7406\u4fe1\u606f\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u8c03\u5ea6\u6846\u67b6PI-HQCD\uff0c\u901a\u8fc7\u62d3\u6251\u611f\u77e5\u54c8\u5bc6\u987f\u91cf\u5d4c\u5165\u7535\u7f51\u7269\u7406\u7ea6\u675f\uff0c\u663e\u8457\u7f13\u89e3 barren plateaus \u95ee\u9898\uff0c\u5e76\u5728\u7ecf\u6d4e\u6548\u7387\u548c\u53ef\u518d\u751f\u80fd\u6e90\u5229\u7528\u7387\u4e0a\u8d85\u8d8a\u7ecf\u5178SDDP\u65b9\u6cd5", "motivation": "\u9ad8\u6bd4\u4f8b\u53ef\u518d\u751f\u80fd\u6e90\u5e76\u7f51\u7ed9\u7535\u529b\u7cfb\u7edf\u8c03\u5ea6\u5e26\u6765\u968f\u673a\u6027\u548c\u975e\u51f8\u6027\u6311\u6218\uff0c\u7ecf\u5178\u4f18\u5316\u65b9\u6cd5\u8ba1\u7b97\u53d7\u9650\uff1b\u73b0\u6709\u91cf\u5b50\u7b97\u6cd5\u5c06\u7535\u7f51\u89c6\u4e3a\u9ed1\u7bb1\uff0c\u5b58\u5728\u53ef\u6269\u5c55\u6027\u5dee\uff08 barren plateaus\uff09\u548c\u7269\u7406\u7ea6\u675f\u9891\u7e41\u8fdd\u53cd\u7684\u95ee\u9898", "method": "\u6784\u5efa\u62d3\u6251\u611f\u77e5\u54c8\u5bc6\u987f\u91cf\uff0c\u5c06\u7ebf\u6027\u5316\u6f6e\u6d41\u65b9\u7a0b\u3001\u50a8\u80fd\u52a8\u6001\u548c\u591a\u65f6\u95f4\u5c3a\u5ea6\u8026\u5408\u76f4\u63a5\u5d4c\u5165\u91cf\u5b50\u57fa\u5e95\uff1b\u63a8\u5bfc\u566a\u58f0\u81ea\u9002\u5e94\u6b63\u5219\u5316\u673a\u5236\uff0c\u7406\u8bba\u4e0a\u7ea6\u675f\u76ee\u6807\u51fd\u6570\u7684Lipschitz\u5e38\u6570\uff0c\u4fdd\u8bc1\u91cf\u5b50\u6d4b\u91cf\u566a\u58f0\u4e0b\u7684\u6536\u655b\u7a33\u5b9a\u6027", "result": "\u5728IEEE 39\u8282\u70b9\u548c118\u8282\u70b9\u7cfb\u7edf\u4e0a\u7684\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0cPI-HQCD\u5728\u7ecf\u6d4e\u6027\u548c\u53ef\u518d\u751f\u80fd\u6e90\u5229\u7528\u7387\u4e0a\u4f18\u4e8e\u968f\u673a\u5bf9\u5076\u52a8\u6001\u89c4\u5212\uff08SDDP\uff09\uff1b\u7406\u8bba\u5206\u6790\u8bc1\u5b9e\u62d3\u6251\u611f\u77e5\u8bbe\u8ba1\u5b9e\u73b0O(1/N)\u68af\u5ea6\u65b9\u5dee\u7f29\u653e\uff0c\u6709\u6548\u7f13\u89e3 barren plateaus", "conclusion": "\u5efa\u7acb\u4e86\u5c06\u5de5\u7a0b\u7269\u7406\u5d4c\u5165\u91cf\u5b50\u8ba1\u7b97\u7684\u4e25\u683c\u8303\u5f0f\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u7535\u7f51\u8fd0\u884c\u4e2d\u7684\u5b9e\u7528\u91cf\u5b50\u4f18\u52bf\u94fa\u5e73\u9053\u8def"}}
{"id": "2601.17814", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17814", "abs": "https://arxiv.org/abs/2601.17814", "authors": ["Haoxuan Ma", "Guannan Lai", "Han-Jia Ye"], "title": "MMR-Bench: A Comprehensive Benchmark for Multimodal LLM Routing", "comment": null, "summary": "Multimodal large language models (MLLMs) have advanced rapidly, yet heterogeneity in architecture, alignment strategies, and efficiency means that no single model is uniformly superior across tasks. In practical deployments, workloads span lightweight OCR to complex multimodal reasoning; using one MLLM for all queries either over-provisions compute on easy instances or sacrifices accuracy on hard ones. Query-level model selection (routing) addresses this tension, but extending routing from text-only LLMs to MLLMs is nontrivial due to modality fusion, wide variation in computational cost across models, and the absence of a standardized, budget-aware evaluation. We present MMR-Bench, a unified benchmark that isolates the multimodal routing problem and enables comparison under fixed candidate sets and cost models. MMR-Bench provides (i) a controlled environment with modality-aware inputs and variable compute budgets, (ii) a broad suite of vision-language tasks covering OCR, general VQA, and multimodal math reasoning, and (iii) strong single-model reference, oracle upper bounds, and representative routing policies. Using MMR-Bench, we show that incorporating multimodal signals improves routing quality. Empirically, these cues improve the cost-accuracy frontier and enable the routed system to exceed the strongest single model's accuracy at roughly 33% of its cost. Furthermore, policies trained on a subset of models and tasks generalize zero-shot to new datasets and text-only benchmarks without retuning, establishing MMR-Bench as a foundation for studying adaptive multimodal model selection and efficient MLLM deployment. The code will be available at: https://github.com/Hunter-Wrynn/MMR-Bench.", "AI": {"tldr": "\u63d0\u51faMMR-Bench\uff0c\u9996\u4e2a\u591a\u6a21\u6001\u6a21\u578b\u8def\u7531\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u667a\u80fd\u67e5\u8be2\u5206\u914d\u5b9e\u73b0\u6210\u672c-\u7cbe\u5ea6\u4f18\u5316\uff0c\u8def\u7531\u7cfb\u7edf\u80fd\u4ee533%\u7684\u6210\u672c\u8d85\u8d8a\u6700\u5f3a\u5355\u4f53\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709MLLMs\u67b6\u6784\u5f02\u6784\u3001\u6210\u672c\u5dee\u5f02\u5927\uff0c\u5355\u4e00\u6a21\u578b\u90e8\u7f72\u8981\u4e48\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\uff0c\u8981\u4e48\u7cbe\u5ea6\u4e0d\u8db3\uff1b\u6587\u672c\u8def\u7531\u96be\u4ee5\u76f4\u63a5\u6269\u5c55\u5230\u591a\u6a21\u6001\uff0c\u7f3a\u4e4f\u6807\u51c6\u5316\u9884\u7b97\u611f\u77e5\u8bc4\u4f30\u4f53\u7cfb\u3002", "method": "\u6784\u5efa\u7edf\u4e00\u57fa\u51c6MMR-Bench\uff0c\u5305\u542b\u6a21\u6001\u611f\u77e5\u8f93\u5165\u73af\u5883\u3001\u591a\u4efb\u52a1\u5957\u4ef6\uff08OCR/VQA/\u6570\u5b66\u63a8\u7406\uff09\u53ca\u8def\u7531\u7b56\u7565\u8bc4\u4f30\u6846\u67b6\uff1b\u901a\u8fc7\u63a7\u5236\u5019\u9009\u6a21\u578b\u548c\u6210\u672c\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u6bd4\u8f83\u3002", "result": "\u591a\u6a21\u6001\u4fe1\u53f7\u663e\u8457\u63d0\u5347\u8def\u7531\u8d28\u91cf\uff0c\u4f18\u5316\u6210\u672c-\u7cbe\u5ea6\u8fb9\u754c\uff1b\u8def\u7531\u7cfb\u7edf\u53ef\u8fbe\u6700\u5f3a\u5355\u4f53\u6a21\u578b\u7cbe\u5ea6\u4f46\u4ec5\u970033%\u8ba1\u7b97\u6210\u672c\uff1b\u652f\u6301\u96f6\u6837\u672c\u8fc1\u79fb\u5230\u65b0\u6570\u636e\u96c6\u548c\u6587\u672c\u57fa\u51c6\u3002", "conclusion": "MMR-Bench\u4e3a\u81ea\u9002\u5e94\u591a\u6a21\u6001\u6a21\u578b\u9009\u62e9\u548c\u9ad8\u6548MLLM\u90e8\u7f72\u5960\u5b9a\u57fa\u7840\uff0c\u63a8\u52a8\u667a\u80fd\u8def\u7531\u7b97\u6cd5\u7814\u7a76\uff0c\u4fc3\u8fdb\u8ba1\u7b97\u8d44\u6e90\u4f18\u5316\u3002"}}
{"id": "2601.18499", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.18499", "abs": "https://arxiv.org/abs/2601.18499", "authors": ["Kratveer Singh", "Kimin Park", "Vojt\u011bch \u0160varc", "Artem Kovalenko", "Tuan Pham", "Ond\u0159ej \u010c\u00edp", "Luk\u00e1\u0161 Slodi\u010dka", "Radim Filip"], "title": "Qubit-parity interference despite unknown interaction phases", "comment": null, "summary": "Quantum interference between interacting systems is fundamental to basic science and quantum technology, but it typically requires precise control of the interaction phases of lasers or microwave generators. Can interference be observed if those interaction phases are stable but unknown, usually prohibitive for complex state without active control? Here, we answer this question by experimentally preparing a Schr\u00f6dinger-cat-like state of an internal qubit and a motional oscillator of a trapped $^{40}$Ca$^{+}$ ion, and its robustness to such uncontrolled phase. By applying alternating red and blue sideband pulses, we enforce a strict qubit-parity correlation and interference inherently insensitive to stable but unknown phases of the driving laser. For this qubit-parity interference, we use a minimal two-pulse interferometric sequence to demonstrate characteristic visibilities of $20\\%$ and $40\\%$, which approach the theoretical visibility limit, providing a scalable coherence witness without full state tomography for high-dimensional states.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5728\u56da\u7981\u79bb\u5b50\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e00\u79cd\u5bf9\u672a\u77e5\u4f46\u7a33\u5b9a\u6fc0\u5149\u76f8\u4f4d\u5177\u6709\u9c81\u68d2\u6027\u7684\u91cf\u5b50\u5e72\u6d89\uff0c\u5229\u7528\u6781\u7b80\u53cc\u8109\u51b2\u5e8f\u5217\u5236\u5907\u7c7b\u4f3c\u859b\u5b9a\u8c14\u732b\u6001\u7684\u72b6\u6001\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe40%\u7684\u5e72\u6d89\u53ef\u89c1\u5ea6\uff0c\u4e3a\u9ad8\u7ef4\u6001\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u76f8\u5e72\u6027\u89c1\u8bc1\u65b9\u6cd5\u3002", "motivation": "\u91cf\u5b50\u5e72\u6d89\u901a\u5e38\u9700\u8981\u7cbe\u786e\u63a7\u5236\u76f8\u4e92\u4f5c\u7528\u76f8\u4f4d\uff0c\u8fd9\u5728\u590d\u6742\u7cfb\u7edf\u4e2d\u5177\u6709\u6311\u6218\u6027\u3002\u8be5\u7814\u7a76\u7684\u52a8\u673a\u662f\u63a2\u7d22\u5728\u7a33\u5b9a\u4f46\u672a\u77e5\u76f8\u4f4d\u6761\u4ef6\u4e0b\u662f\u5426\u4ecd\u80fd\u89c2\u5bdf\u5230\u5e72\u6d89\u73b0\u8c61\uff0c\u8fd9\u5bf9\u57fa\u7840\u79d1\u5b66\u548c\u91cf\u5b50\u6280\u672f\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5229\u7528\u56da\u7981$^{40}$Ca$^{+}$\u79bb\u5b50\uff0c\u901a\u8fc7\u4ea4\u66ff\u65bd\u52a0\u7ea2\u5931\u8c10\u548c\u84dd\u5931\u8c10\u8fb9\u5e26\u8109\u51b2\uff0c\u5f3a\u5236\u5b9e\u73b0\u91cf\u5b50\u6bd4\u7279\u5b87\u79f0\u5173\u8054\uff0c\u4ea7\u751f\u5bf9\u672a\u77e5\u6fc0\u5149\u76f8\u4f4d\u4e0d\u654f\u611f\u7684\u5e72\u6d89\u3002\u4ed6\u4eec\u91c7\u7528\u6781\u7b80\u7684\u53cc\u8109\u51b2\u5e72\u6d89\u5e8f\u5217\u3002", "result": "\u5b9e\u9a8c\u5b9e\u73b0\u4e8620%\u548c40%\u7684\u7279\u5f81\u5e72\u6d89\u53ef\u89c1\u5ea6\uff0c\u63a5\u8fd1\u8be5\u91cf\u5b50\u6bd4\u7279\u5b87\u79f0\u5e72\u6d89\u65b9\u6848\u7684\u7406\u8bba\u53ef\u89c1\u5ea6\u6781\u9650\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u9ad8\u7ef4\u6001\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u5b8c\u5168\u5c42\u6790\u7684\u53ef\u6269\u5c55\u76f8\u5e72\u6027\u89c1\u8bc1\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5728\u76f8\u4f4d\u63a7\u5236\u53d7\u9650\u7684\u590d\u6742\u91cf\u5b50\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u9c81\u68d2\u91cf\u5b50\u5e72\u6d89\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2601.17196", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17196", "abs": "https://arxiv.org/abs/2601.17196", "authors": ["Nghia Thu Truong", "Qui Phu Pham", "Quang Nguyen", "Dung Luong", "Mai Tran"], "title": "Accelerated Sinkhorn Algorithms for Partial Optimal Transport", "comment": null, "summary": "Partial Optimal Transport (POT) addresses the problem of transporting only a fraction of the total mass between two distributions, making it suitable when marginals have unequal size or contain outliers. While Sinkhorn-based methods are widely used, their complexity bounds for POT remain suboptimal and can limit scalability. We introduce Accelerated Sinkhorn for POT (ASPOT), which integrates alternating minimization with Nesterov-style acceleration in the POT setting, yielding a complexity of $\\mathcal{O}(n^{7/3}\\varepsilon^{-5/3})$. We also show that an informed choice of the entropic parameter $\u03b3$ improves rates for the classical Sinkhorn method. Experiments on real-world applications validate our theories and demonstrate the favorable performance of our proposed methods.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u90e8\u5206\u6700\u4f18\u4f20\u8f93(POT)\u95ee\u9898\u63d0\u51fa\u52a0\u901fSinkhorn\u7b97\u6cd5(ASPOT)\uff0c\u901a\u8fc7\u7ed3\u5408\u4ea4\u66ff\u6700\u5c0f\u5316\u4e0eNesterov\u52a0\u901f\u7b56\u7565\uff0c\u5c06\u590d\u6742\u5ea6\u4f18\u5316\u81f3$\\mathcal{O}(n^{7/3}\\varepsilon^{-5/3})$\uff0c\u5e76\u6539\u8fdb\u4e86\u71b5\u53c2\u6570\u9009\u62e9\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u90e8\u5206\u6700\u4f18\u4f20\u8f93(POT)\u9002\u7528\u4e8e\u8fb9\u7f18\u5206\u5e03\u5927\u5c0f\u4e0d\u7b49\u6216\u542b\u5f02\u5e38\u503c\u7684\u573a\u666f\uff0c\u4f46\u73b0\u6709\u57fa\u4e8eSinkhorn\u7684\u65b9\u6cd5\u590d\u6742\u5ea6\u754c\u9650\u6b21\u4f18\uff0c\u9650\u5236\u4e86\u7b97\u6cd5\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e0a\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51faASPOT\u7b97\u6cd5\uff0c\u5728POT\u6846\u67b6\u4e2d\u6574\u5408\u4ea4\u66ff\u6700\u5c0f\u5316\u4e0eNesterov-style\u52a0\u901f\u6280\u672f\uff0c\u5e76\u4f18\u5316\u71b5\u53c2\u6570\u03b3\u7684\u9009\u62e9\u7b56\u7565\u3002", "result": "\u7406\u8bba\u6210\u679c\uff1a1) ASPOT\u8fbe\u5230$\\mathcal{O}(n^{7/3}\\varepsilon^{-5/3})$\u7684\u590d\u6742\u5ea6\u754c\u9650\uff1b2) \u63ed\u793a\u4e86\u71b5\u53c2\u6570\u03b3\u7684\u4f18\u5316\u9009\u62e9\u53ef\u63d0\u5347\u7ecf\u5178Sinkhorn\u65b9\u6cd5\u7684\u6536\u655b\u901f\u7387\u3002\u5b9e\u9a8c\u9a8c\u8bc1\uff1a\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aPOT\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u6c42\u89e3\u7b97\u6cd5\uff0c\u663e\u8457\u6539\u5584\u4e86Sinkhorn\u65b9\u6cd5\u5728\u90e8\u5206\u4f20\u8f93\u573a\u666f\u4e0b\u7684\u53ef\u6269\u5c55\u6027\uff0c\u517c\u5177\u7406\u8bba\u4ef7\u503c\u4e0e\u5b9e\u8df5\u610f\u4e49\u3002"}}
{"id": "2601.17826", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17826", "abs": "https://arxiv.org/abs/2601.17826", "authors": ["Siyuan Yang", "Xihan Bian", "Jiayin Tang"], "title": "RegGuard: AI-Powered Retrieval-Enhanced Assistant for Pharmaceutical Regulatory Compliance", "comment": null, "summary": "The increasing frequency and complexity of regulatory updates present a significant burden for multinational pharmaceutical companies. Compliance teams must interpret evolving rules across jurisdictions, formats, and agencies, often manually, at high cost and risk of error. We introduce RegGuard, an industrial-scale AI assistant designed to automate the interpretation of heterogeneous regulatory texts and align them with internal corporate policies. The system ingests heterogeneous document sources through a secure pipeline and enhances retrieval and generation quality with two novel components: HiSACC (Hierarchical Semantic Aggregation for Contextual Chunking) semantically segments long documents into coherent units while maintaining consistency across non-contiguous sections. ReLACE (Regulatory Listwise Adaptive Cross-Encoder for Reranking), a domain-adapted cross-encoder built on an open-source model, jointly models user queries and retrieved candidates to improve ranking relevance. Evaluations in enterprise settings demonstrate that RegGuard improves answer quality specifically in terms of relevance, groundedness, and contextual focus, while significantly mitigating hallucination risk. The system architecture is built for auditability and traceability, featuring provenance tracking, access control, and incremental indexing, making it highly responsive to evolving document sources and relevant for any domain with stringent compliance demands.", "AI": {"tldr": "\u9488\u5bf9\u8de8\u56fd\u836f\u4f01\u76d1\u7ba1\u5408\u89c4\u8d1f\u62c5\uff0c\u672c\u6587\u63d0\u51faRegGuard AI\u7cfb\u7edf\uff0c\u901a\u8fc7HiSACC\u5c42\u6b21\u8bed\u4e49\u5206\u5757\u548cReLACE\u9886\u57df\u81ea\u9002\u5e94\u91cd\u6392\u5e8f\u4e24\u5927\u521b\u65b0\uff0c\u5b9e\u73b0\u76d1\u7ba1\u6587\u672c\u81ea\u52a8\u89e3\u8bfb\uff0c\u663e\u8457\u63d0\u5347\u7b54\u6848\u8d28\u91cf\u548c\u53ef\u4fe1\u5ea6\uff0c\u964d\u4f4e\u5e7b\u89c9\u98ce\u9669\u3002", "motivation": "\u76d1\u7ba1\u66f4\u65b0\u9891\u7e41\u590d\u6742\uff0c\u8de8\u56fd\u5236\u836f\u4f01\u4e1a\u9700\u8de8\u53f8\u6cd5\u7ba1\u8f96\u533a\u3001\u683c\u5f0f\u548c\u673a\u6784\u624b\u52a8\u89e3\u8bfb\u89c4\u5219\uff0c\u6210\u672c\u9ad8\u4e14\u6613\u51fa\u9519\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u5de5\u4e1a\u7ea7AI\u52a9\u624bRegGuard\uff0c\u91c7\u7528\u5b89\u5168\u7ba1\u9053\u5904\u7406\u5f02\u6784\u6587\u6863\uff0c\u6838\u5fc3\u521b\u65b0\u5305\u62ec\uff1a1\uff09HiSACC\u901a\u8fc7\u5206\u5c42\u8bed\u4e49\u805a\u5408\u5b9e\u73b0\u957f\u6587\u6863\u8bed\u4e49\u5206\u5757\u5e76\u4fdd\u6301\u975e\u8fde\u7eed\u90e8\u5206\u4e00\u81f4\u6027\uff1b2\uff09ReLACE\u57fa\u4e8e\u5f00\u6e90\u6a21\u578b\u6784\u5efa\u9886\u57df\u81ea\u9002\u5e94\u4ea4\u53c9\u7f16\u7801\u5668\uff0c\u8054\u5408\u5efa\u6a21\u67e5\u8be2\u4e0e\u5019\u9009\u6587\u6863\u4ee5\u63d0\u5347\u6392\u5e8f\u76f8\u5173\u6027\uff1b\u7cfb\u7edf\u5177\u5907\u6eaf\u6e90\u8ffd\u8e2a\u3001\u8bbf\u95ee\u63a7\u5236\u548c\u589e\u91cf\u7d22\u5f15\u529f\u80fd\u3002", "result": "\u4f01\u4e1a\u73af\u5883\u8bc4\u4f30\u8868\u660e\uff0cRegGuard\u5728\u7b54\u6848\u76f8\u5173\u6027\u3001 groundedness\u548c\u4e0a\u4e0b\u6587\u805a\u7126\u65b9\u9762\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u5e7b\u89c9\u98ce\u9669\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u67b6\u6784\u6ce8\u91cd\u53ef\u5ba1\u8ba1\u6027\u548c\u53ef\u8ffd\u6eaf\u6027\uff0c\u80fd\u5feb\u901f\u54cd\u5e94\u6587\u6863\u66f4\u65b0\uff0c\u4e0d\u4ec5\u9002\u7528\u4e8e\u5236\u836f\u884c\u4e1a\uff0c\u4e5f\u53ef\u63a8\u5e7f\u81f3\u4efb\u4f55\u5f3a\u5408\u89c4\u8981\u6c42\u9886\u57df\u3002"}}
{"id": "2601.17204", "categories": ["cs.LG", "cs.CE"], "pdf": "https://arxiv.org/pdf/2601.17204", "abs": "https://arxiv.org/abs/2601.17204", "authors": ["Yinkai Wang", "Yan Zhou Chen", "Xiaohui Chen", "Li-Ping Liu", "Soha Hassoun"], "title": "SpecBridge: Bridging Mass Spectrometry and Molecular Representations via Cross-Modal Alignment", "comment": "preprint", "summary": "Small-molecule identification from tandem mass spectrometry (MS/MS) remains a bottleneck in untargeted settings where spectral libraries are incomplete. While deep learning offers a solution, current approaches typically fall into two extremes: explicit generative models that construct molecular graphs atom-by-atom, or joint contrastive models that learn cross-modal subspaces from scratch. We introduce SpecBridge, a novel implicit alignment framework that treats structure identification as a geometric alignment problem. SpecBridge fine-tunes a self-supervised spectral encoder (DreaMS) to project directly into the latent space of a frozen molecular foundation model (ChemBERTa), and then performs retrieval by cosine similarity to a fixed bank of precomputed molecular embeddings. Across MassSpecGym, Spectraverse, and MSnLib benchmarks, SpecBridge improves top-1 retrieval accuracy by roughly 20-25% relative to strong neural baselines, while keeping the number of trainable parameters small. These results suggest that aligning to frozen foundation models is a practical, stable alternative to designing new architectures from scratch. The code for SpecBridge is released at https://github.com/HassounLab/SpecBridge.", "AI": {"tldr": "SpecBridge\u901a\u8fc7\u5fae\u8c03\u5149\u8c31\u7f16\u7801\u5668\u4e0e\u51bb\u7ed3\u5206\u5b50\u6a21\u578b\u5bf9\u9f50\uff0c\u5b9e\u73b0\u68c0\u7d22\u51c6\u786e\u7387\u63d0\u534720-25%\uff0c\u53c2\u6570\u9ad8\u6548\u3002", "motivation": "\u7531\u4e8e\u5149\u8c31\u5e93\u4e0d\u5b8c\u6574\uff0cMS/MS\u5c0f\u5206\u5b50\u9274\u5b9a\u5b58\u5728\u74f6\u9888\uff0c\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u751f\u6210\u6a21\u578b\u6216\u4ece\u5934\u5bf9\u6bd4\u5b66\u4e60\u3002", "method": "SpecBridge\u5c06\u7ed3\u6784\u9274\u5b9a\u89c6\u4e3a\u51e0\u4f55\u5bf9\u9f50\u95ee\u9898\uff0c\u5fae\u8c03\u81ea\u76d1\u7763\u5149\u8c31\u7f16\u7801\u5668(DreaMS)\u6295\u5f71\u81f3\u51bb\u7ed3\u5206\u5b50\u6a21\u578b(ChemBERTa)\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u68c0\u7d22\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSpecBridge\u5c06top-1\u68c0\u7d22\u51c6\u786e\u7387\u76f8\u5bf9\u63d0\u534720-25%\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u3002", "conclusion": "\u4e0e\u51bb\u7ed3\u57fa\u7840\u6a21\u578b\u5bf9\u9f50\u662f\u8bbe\u8ba1\u65b0\u67b6\u6784\u7684\u53ef\u884c\u4e14\u7a33\u5b9a\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2601.17828", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17828", "abs": "https://arxiv.org/abs/2601.17828", "authors": ["Tanvi Verma", "Yang Zhou", "Rick Siow Mong Goh", "Yong Liu"], "title": "Aligning Medical Conversational AI through Online Reinforcement Learning with Information-Theoretic Rewards", "comment": null, "summary": "We present Information Gain Fine-Tuning (IGFT), a novel approach for training medical conversational AI to conduct effective patient interviews and generate comprehensive History of Present Illness (HPI) without requiring pre-collected human conversations. IGFT combines online Group Relative Policy Optimization (GRPO) with information-theoretic rewards, enabling models to learn from self-generated conversations with simulated patients. Unlike existing approaches that rely on expensive expert-annotated conversations or static datasets, our online RL framework allows models to discover effective questioning strategies through exploration. Our key innovation is an information gain reward function that tracks which clinical entities such as symptoms, temporal patterns, and medical history, are revealed during conversation. Each question's reward is computed based on its expected information gain combined with GPT-4o-mini quality assessments across dimensions including clinical relevance, patient engagement, and specificity. This hybrid approach ensures models learn to ask targeted, clinically appropriate questions that efficiently gather diagnostic information. We fine-tune two models using LoRA: Llama-3.1-8B-Instruct and DeepSeek-R1-Distill-Qwen-7B (a reasoning-optimized model). Training exclusively on Avey data containing concise HPIs, we evaluate generalization to MIMIC data with longer, more elaborate HPIs. DeepSeek-R1-Distill-Qwen-7B (IGFT) achieves F1 scores of 0.408 on Avey (10.9% improvement over base) and 0.289 on MIMIC (12.9% improvement), while Llama-3.1-8B-Instruct (IGFT) reaches 0.384 and 0.336 respectively. Both models outperform OpenAI's model on MIMIC and surpass medical domain-specific baselines like HuatuoGPT and UltraMedical, which were optimized for single-turn medical QA rather than multi-turn conversations.", "AI": {"tldr": "\u63d0\u51faIGFT\u65b9\u6cd5\uff0c\u8ba9\u533b\u7597AI\u901a\u8fc7\u81ea\u6211\u751f\u6210\u5bf9\u8bdd\u548c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u65e0\u9700\u4eba\u5de5\u6570\u636e\u5373\u53ef\u5b66\u4f1a\u6709\u6548\u95ee\u8bca\u7b56\u7565\uff0c\u5728Avey\u548cMIMIC\u6570\u636e\u96c6\u4e0aF1\u5206\u6570\u63d0\u534710-13%\uff0c\u8d85\u8d8a\u73b0\u6709\u533b\u7597\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u533b\u7597\u5bf9\u8bddAI\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u5bf9\u8bdd\u6570\u636e\uff0c\u9650\u5236\u4e86\u6a21\u578b\u63a2\u7d22\u80fd\u529b\u3002\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u9884\u6536\u96c6\u4eba\u7c7b\u5bf9\u8bdd\u5373\u53ef\u8bad\u7ec3\u9ad8\u8d28\u91cf\u95ee\u8bcaAI\u7684\u65b9\u6cd5\u3002", "method": "IGFT\u7ed3\u5408\u5728\u7ebfGRPO\u7b97\u6cd5\u4e0e\u4fe1\u606f\u8bba\u5956\u52b1\u673a\u5236\uff0c\u8ffd\u8e2a\u5bf9\u8bdd\u4e2d\u63ed\u793a\u7684\u4e34\u5e8a\u5b9e\u4f53\uff08\u75c7\u72b6\u3001\u65f6\u95f4\u6a21\u5f0f\u3001\u75c5\u53f2\uff09\u3002\u5956\u52b1\u51fd\u6570\u878d\u5408\u9884\u671f\u4fe1\u606f\u589e\u76ca\u548cGPT-4o-mini\u5bf9\u4e34\u5e8a\u76f8\u5173\u6027\u3001\u60a3\u8005\u53c2\u4e0e\u5ea6\u3001\u7279\u5f02\u6027\u7684\u8bc4\u4f30\u3002\u4f7f\u7528LoRA\u5fae\u8c03Llama-3.1-8B-Instruct\u548cDeepSeek-R1-Distill-Qwen-7B\u4e24\u4e2a\u6a21\u578b\u3002", "result": "DeepSeek-R1-Distill-Qwen-7B\u5728Avey\u4e0aF1=0.408\uff08\u8f83\u57fa\u5ea7\u6a21\u578b+10.9%\uff09\uff0c\u5728MIMIC\u4e0aF1=0.289\uff08+12.9%\uff09\uff1bLlama-3.1-8B-Instruct\u5728Avey\u4e0aF1=0.384\uff0c\u5728MIMIC\u4e0aF1=0.336\u3002\u4e24\u4e2a\u6a21\u578b\u5747\u5728MIMIC\u4e0a\u8d85\u8d8aOpenAI\u6a21\u578b\uff0c\u5e76\u4f18\u4e8eHuatuoGPT\u3001UltraMedical\u7b49\u533b\u7597\u9886\u57df\u57fa\u7ebf\u3002", "conclusion": "IGFT\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u533b\u7597\u5bf9\u8bddAI\u5bf9\u4eba\u5de5\u6570\u636e\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u901a\u8fc7\u63a2\u7d22\u5f0f\u5b66\u4e60\u5b9e\u73b0\u9ad8\u6548\u4e34\u5e8a\u4fe1\u606f\u6536\u96c6\uff0c\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5e94\u7528\u524d\u666f\uff0c\u4e3a\u591a\u8f6e\u533b\u7597\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2601.18514", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.18514", "abs": "https://arxiv.org/abs/2601.18514", "authors": ["Huili Zhang", "Yibin Guo", "Guanglei Xu", "Yulong Feng", "Jingning Zhang", "Hai-feng Yu", "S. P. Zhao"], "title": "Simultaneous determination of multiple low-lying energy levels on a superconducting quantum processor", "comment": "arXiv admin note: substantial text overlap with arXiv:2507.14880. substantial text overlap with arXiv:2507.14880. substantial text overlap with arXiv:2507.14880. substantial text overlap with arXiv:2507.14880", "summary": "Determining the ground and low-lying excited states is critical in numerous scenarios. Recent work has proposed the ancilla-entangled variational quantum eigensolver (AEVQE) that utilizes entanglement between ancilla and physical qubits to simultaneously tagert multiple low-lying energy levels. In this work, we report the experimental implementation of the AEVQE on a superconducting quantum cloud platform, demonstrating the full procedure of solving the low-lying energy levels of the H$_2$ molecule and the transverse-field Ising models (TFIMs). We obtain the potential energy curves of H$_2$ and show an indication of the ferromagnetic to paramagnetic phase transition in the TFIMs from the average absolute magnetization. Moreover, we investigate multiple factors that affect the algorithmic performance and provide a comparison with ancilla-free VQE algorithms. Our work demonstrates the experimental feasibility of the AEVQE algorithm and offers a guidance for the VQE approach in solving realistic problems on publicly-accessible quantum platforms.", "AI": {"tldr": "\u5728\u8d85\u5bfc\u91cf\u5b50\u4e91\u5e73\u53f0\u4e0a\u5b9e\u9a8c\u5b9e\u73b0AEVQE\u7b97\u6cd5\uff0c\u6210\u529f\u6c42\u89e3H\u2082\u5206\u5b50\u548c\u6a2a\u573aIsing\u6a21\u578b\u7684\u4f4e\u6fc0\u53d1\u6001\uff0c\u83b7\u5f97\u52bf\u80fd\u66f2\u7ebf\u5e76\u89c2\u5bdf\u5230\u76f8\u53d8\u8ff9\u8c61\uff0c\u9a8c\u8bc1\u4e86\u8be5\u7b97\u6cd5\u7684\u5b9e\u9a8c\u53ef\u884c\u6027\u3002", "motivation": "\u786e\u5b9a\u57fa\u6001\u548c\u4f4e\u6fc0\u53d1\u6001\u5728\u591a\u79cd\u573a\u666f\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u9ad8\u6548\u83b7\u53d6\u591a\u4e2a\u4f4e\u80fd\u7ea7\uff0c\u4e14\u9700\u5728\u771f\u5b9e\u91cf\u5b50\u786c\u4ef6\u4e0a\u9a8c\u8bc1\u7b97\u6cd5\u53ef\u884c\u6027\u3002", "method": "\u5229\u7528\u8d85\u5bfc\u91cf\u5b50\u4e91\u5e73\u53f0\u5b9e\u73b0AEVQE\u7b97\u6cd5\uff0c\u901a\u8fc7\u8f85\u52a9\u6bd4\u7279\u4e0e\u7269\u7406\u6bd4\u7279\u7ea0\u7f20\u540c\u65f6\u9776\u5411\u591a\u4e2a\u4f4e\u6fc0\u53d1\u6001\uff0c\u5728H\u2082\u5206\u5b50\u548c\u6a2a\u573aIsing\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5e76\u4e0e\u65e0\u8f85\u52a9\u6bd4\u7279VQE\u7b97\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u6210\u529f\u83b7\u5f97H\u2082\u5206\u5b50\u7684\u52bf\u80fd\u66f2\u7ebf\uff1b\u901a\u8fc7\u5e73\u5747\u7edd\u5bf9\u78c1\u5316\u5f3a\u5ea6\u89c2\u5bdf\u5230\u6a2a\u573aIsing\u6a21\u578b\u4e2d\u94c1\u78c1-\u987a\u78c1\u76f8\u53d8\u7684\u8ff9\u8c61\uff1b\u7cfb\u7edf\u7814\u7a76\u4e86\u5f71\u54cd\u7b97\u6cd5\u6027\u80fd\u7684\u591a\u4e2a\u56e0\u7d20\uff1b\u83b7\u5f97\u4e86\u4e0e\u65e0\u8f85\u52a9\u6bd4\u7279VQE\u7684\u5bf9\u6bd4\u6570\u636e\u3002", "conclusion": "\u672c\u5de5\u4f5c\u8bc1\u660e\u4e86AEVQE\u7b97\u6cd5\u5728\u771f\u5b9e\u91cf\u5b50\u786c\u4ef6\u4e0a\u7684\u5b9e\u9a8c\u53ef\u884c\u6027\uff0c\u4e3a\u5728\u516c\u5f00\u91cf\u5b50\u5e73\u53f0\u4e0a\u4f7f\u7528VQE\u65b9\u6cd5\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002"}}
{"id": "2601.18518", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.18518", "abs": "https://arxiv.org/abs/2601.18518", "authors": ["Parvendra Kumar"], "title": "Quantum Key Distribution with a Negatively Charged Quantum Dot Single-Photon Source", "comment": "8 pages, 10 figures", "summary": "Various quantum key distribution protocols require bright single-photon sources with a very low probability of multiphoton emission. In this work, we investigate single-photon generation from a negatively charged quantum dot embedded in an elliptical pillar microcavity, driven using either resonant excitation or adiabatic rapid passage (ARP). Our results show that ARP excitation significantly suppresses multiphoton emission probability and improves photon indistinguishability compared to resonant excitation. We further evaluate the secure key rate of both BB84 and twin-field quantum key distribution (TF-QKD) using quantum-dot single-photon sources and compare their performance with that of Poisson-distributed photon sources (PDS) such as weak coherent pulses and down-conversion sources. The analysis reveals that adiabatic excitation offers a modest but consistent enhancement in secure key rate relative to resonant excitation. Moreover, quantum-dot single-photon sources outperform PDS sources over short and intermediate distances; however, at longer distances, PDS sources eventually surpass quantum-dot sources in both infinite decoy-state BB84 and TF-QKD.", "AI": {"tldr": "ARP\u6fc0\u53d1\u6539\u5584\u4e86\u91cf\u5b50\u70b9\u5355\u5149\u5b50\u6e90\u8d28\u91cf\uff0c\u5728\u77ed/\u4e2d\u8ddd\u79bb\u63d0\u5347QKD\u5bc6\u94a5\u7387\uff0c\u4f46\u8fdc\u8ddd\u79bb\u65f6\u6cca\u677e\u5149\u6e90\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "QKD\u534f\u8bae\u9700\u8981\u9ad8\u4eae\u5ea6\u3001\u4f4e\u591a\u5149\u5b50\u53d1\u5c04\u6982\u7387\u7684\u5355\u5149\u5b50\u6e90\u4ee5\u4fdd\u8bc1\u5b89\u5168\u6027\u3002", "method": "\u5bf9\u6bd4\u7814\u7a76\u4e86\u5d4c\u5165\u692d\u5706\u67f1\u5fae\u8154\u7684\u8d1f\u7535\u8377\u91cf\u5b50\u70b9\u5728\u5171\u632f\u6fc0\u53d1\u548c\u7edd\u70ed\u5feb\u901f\u901a\u9053(ARP)\u6fc0\u53d1\u4e0b\u7684\u5355\u5149\u5b50\u4ea7\u751f\uff1b\u8bc4\u4f30\u4e86BB84\u548c\u53cc\u573aQKD\u7684\u5bc6\u94a5\u7387\uff0c\u5e76\u4e0e\u6cca\u677e\u5206\u5e03\u5149\u6e90\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "ARP\u663e\u8457\u6291\u5236\u591a\u5149\u5b50\u53d1\u5c04\u6982\u7387\u5e76\u63d0\u9ad8\u5149\u5b50\u4e0d\u53ef\u533a\u5206\u6027\uff1b\u91cf\u5b50\u70b9\u5149\u6e90\u5728\u77ed/\u4e2d\u8ddd\u79bb\u4f18\u4e8e\u6cca\u677e\u5149\u6e90\uff0c\u4f46\u5728\u8fdc\u8ddd\u79bb\u88ab\u53cd\u8d85\u3002", "conclusion": "ARP\u6fc0\u53d1\u65b9\u5f0f\u66f4\u4f18\uff0c\u4f7f\u91cf\u5b50\u70b9\u5149\u6e90\u5728\u4e2d\u8ddd\u79bbQKD\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\uff0c\u4f46\u8fdc\u8ddd\u79bb\u5e94\u7528\u4ecd\u9700\u6cca\u677e\u5149\u6e90\u3002"}}
{"id": "2601.17215", "categories": ["cs.LG", "hep-ex"], "pdf": "https://arxiv.org/pdf/2601.17215", "abs": "https://arxiv.org/abs/2601.17215", "authors": ["Ruoqing Zheng", "Chang Sun", "Qibin Liu", "Lauri Laatu", "Arianna Cox", "Benedikt Maier", "Alexander Tapper", "Jose G. F. Coutinho", "Wayne Luk", "Zhiqiang Que"], "title": "JetFormer: A Scalable and Efficient Transformer for Jet Tagging from Offline Analysis to FPGA Triggers", "comment": "15 pages,", "summary": "We present JetFormer, a versatile and scalable encoder-only Transformer architecture for particle jet tagging at the Large Hadron Collider (LHC). Unlike prior approaches that are often tailored to specific deployment regimes, JetFormer is designed to operate effectively across the full spectrum of jet tagging scenarios, from high-accuracy offline analysis to ultra-low-latency online triggering. The model processes variable-length sets of particle features without relying on input of explicit pairwise interactions, yet achieves competitive or superior performance compared to state-of-the-art methods. On the large-scale JetClass dataset, a large-scale JetFormer matches the accuracy of the interaction-rich ParT model (within 0.7%) while using 37.4% fewer FLOPs, demonstrating its computational efficiency and strong generalization. On benchmark HLS4ML 150P datasets, JetFormer consistently outperforms existing models such as MLPs, Deep Sets, and Interaction Networks by 3-4% in accuracy. To bridge the gap to hardware deployment, we further introduce a hardware-aware optimization pipeline based on multi-objective hyperparameter search, yielding compact variants like JetFormer-tiny suitable for FPGA-based trigger systems with sub-microsecond latency requirements. Through structured pruning and quantization, we show that JetFormer can be aggressively compressed with minimal accuracy loss. By unifying high-performance modeling and deployability within a single architectural framework, JetFormer provides a practical pathway for deploying Transformer-based jet taggers in both offline and online environments at the LHC. Code is available at https://github.com/walkieq/JetFormer.", "AI": {"tldr": "JetFormer\u662f\u4e00\u4e2a\u7528\u4e8eLHC\u7c92\u5b50\u55b7\u6ce8\u6807\u8bb0\u7684\u591a\u529f\u80fd\u53ef\u6269\u5c55\u7f16\u7801\u5668-only Transformer\u67b6\u6784\uff0c\u80fd\u5728\u5168\u573a\u666f\u4e0b\u5de5\u4f5c\uff0c\u6027\u80fd\u5ab2\u7f8eSOTA\u6a21\u578b\u4f46\u8ba1\u7b97\u6548\u7387\u9ad837.4%\uff0c\u5e76\u53ef\u538b\u7f29\u81f3\u9002\u5408FPGA\u90e8\u7f72\u7684\u8d85\u4f4e\u5ef6\u8fdf\u7248\u672c\u3002", "motivation": "\u73b0\u6709\u55b7\u6ce8\u6807\u8bb0\u65b9\u6cd5\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u90e8\u7f72\u573a\u666f\u5b9a\u5236\uff0c\u7f3a\u4e4f\u4e00\u4e2a\u80fd\u5728\u9ad8\u7cbe\u5ea6\u79bb\u7ebf\u5206\u6790\u548c\u8d85\u4f4e\u5ef6\u8fdf\u5728\u7ebf\u89e6\u53d1\u7b49\u5168\u573a\u666f\u4e0b\u7edf\u4e00\u5de5\u4f5c\u7684\u901a\u7528\u67b6\u6784\u3002", "method": "\u8bbe\u8ba1JetFormer\u7f16\u7801\u5668-only Transformer\uff0c\u76f4\u63a5\u5904\u7406\u53ef\u53d8\u957f\u5ea6\u7c92\u5b50\u7279\u5f81\u96c6\u800c\u4e0d\u4f9d\u8d56\u663e\u5f0f\u6210\u5bf9\u4ea4\u4e92\uff1b\u5f15\u5165\u786c\u4ef6\u611f\u77e5\u4f18\u5316\u7ba1\u9053\u8fdb\u884c\u591a\u76ee\u6807\u8d85\u53c2\u6570\u641c\u7d22\uff1b\u91c7\u7528\u7ed3\u6784\u5316\u526a\u679d\u548c\u91cf\u5316\u8fdb\u884c\u6a21\u578b\u538b\u7f29\u3002", "result": "\u5728JetClass\u6570\u636e\u96c6\u4e0a\uff0cJetFormer\u4ee5\u5c1137.4%\u7684\u8ba1\u7b97\u91cf\u8fbe\u5230\u4e0eParT\u6a21\u578b\u76f8\u5f53\u7cbe\u5ea6\uff08\u5dee\u8ddd0.7%\uff09\uff1b\u5728HLS4ML 150P\u6570\u636e\u96c6\u4e0a\u6bd4MLPs\u3001Deep Sets\u7b49\u6a21\u578b\u51c6\u786e\u7387\u9ad83-4%\uff1b\u53ef\u538b\u7f29\u4e3aJetFormer-tiny\u7b49\u9002\u5408FPGA\u3001\u5ef6\u8fdf\u4f4e\u4e8e\u5fae\u79d2\u7684\u7248\u672c\u3002", "conclusion": "JetFormer\u7edf\u4e00\u4e86\u9ad8\u6027\u80fd\u5efa\u6a21\u4e0e\u53ef\u90e8\u7f72\u6027\uff0c\u4e3a\u5728LHC\u79bb\u7ebf\u4e0e\u5728\u7ebf\u73af\u5883\u4e2d\u90e8\u7f72Transformer\u55b7\u6ce8\u6807\u8bb0\u5668\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2601.17897", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17897", "abs": "https://arxiv.org/abs/2601.17897", "authors": ["Jiayu Liu", "Yinhe Long", "Zhenya Huang", "Enhong Chen"], "title": "UniCog: Uncovering Cognitive Abilities of LLMs through Latent Mind Space Analysis", "comment": null, "summary": "A growing body of research suggests that the cognitive processes of large language models (LLMs) differ fundamentally from those of humans. However, existing interpretability methods remain limited in explaining how cognitive abilities are engaged during LLM reasoning. In this paper, we propose UniCog, a unified framework that analyzes LLM cognition via a latent mind space. Formulated as a latent variable model, UniCog encodes diverse abilities from dense model activations into sparse, disentangled latent dimensions. Through extensive analysis on six advanced LLMs, including DeepSeek-V3.2 and GPT-4o, we reveal a Pareto principle of LLM cognition, where a shared reasoning core is complemented by ability-specific signatures. Furthermore, we discover that reasoning failures often manifest as anomalous intensity in latent activations. These findings opens a new paradigm in LLM analysis, providing a cognition grounded view of reasoning dynamics. Finally, leveraging these insights, we introduce a latent-informed candidate prioritization strategy, which improves reasoning performance by up to 7.5% across challenging benchmarks. Our code is available at https://github.com/milksalute/unicog.", "AI": {"tldr": "Proposes UniCog, a latent variable framework that disentangles LLM cognitive abilities into sparse latent dimensions, revealing a Pareto principle of cognition and improving reasoning by 7.5%.", "motivation": "Existing interpretability methods cannot adequately explain how cognitive abilities are engaged during LLM reasoning, despite evidence that LLM cognition differs fundamentally from human cognition.", "method": "UniCog: a unified latent variable model that encodes diverse cognitive abilities from dense model activations into sparse, disentangled latent dimensions for analyzing LLM cognition via a latent mind space.", "result": "(1) Revealed a Pareto principle of LLM cognition with shared reasoning core + ability-specific signatures; (2) Found reasoning failures manifest as anomalous latent activation intensity; (3) Latent-informed candidate prioritization improved reasoning by up to 7.5% on challenging benchmarks.", "conclusion": "Opens a new cognition-grounded paradigm for LLM analysis, providing interpretable insights into reasoning dynamics and enabling practical performance improvements through latent-space analysis."}}
{"id": "2601.18534", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.18534", "abs": "https://arxiv.org/abs/2601.18534", "authors": ["Shuai Zhao", "Rong Wang", "Qi Zhao"], "title": "Certifying optimal device-independent quantum randomness in quantum networks", "comment": "14 pages,4 figures", "summary": "Bell nonlocality provides a device-independent (DI) way to certify quantum randomness, based on which true random numbers can be extracted from the observed correlations without detail characterizations on devices for quantum state preparation and measurement. However, the efficiency of current strategies for DI randomness certification is still heavily constrained when it comes to non-maximal Bell values, especially for multiple parties. Here, we present a family of multipartite Bell inequalities that allows to certify optimal quantum randomness and self-test GHZ (Greenberger-Horne-Zeilinger) states, which are inspired from the stabilizer group of the GHZ state. Due to the simple representation of stabilizer group for GHZ states, this family of Bell inequalities is of simple structure and can be easily expanded to more parties. Compared with the Mermin-type inequalities, this family of Bell inequality is more efficient in certifying quantum randomness when non-maximal Bell values achieved. Meanwhile, the general analytical upper bound for the Holevo quantity is presented, and achieves better performance compared with the MABK (Mermin-Ardehali-Belinskii-Klyshko) inequality, Parity-CHSH (Clauser-Horne-Shimony-Holt) inequality and Holz inequality at $N=3$, which is of particular interests for experimental researches on DI quantum cryptography in quantum networks.", "AI": {"tldr": "This paper presents a new family of multipartite Bell inequalities based on GHZ state stabilizers to achieve more efficient device-independent quantum randomness certification, especially for non-maximal Bell values and multi-party scenarios, outperforming existing methods like Mermin-type inequalities.", "motivation": "Current device-independent (DI) quantum randomness certification methods suffer from low efficiency when dealing with non-maximal Bell values and multiple parties, limiting practical applications in quantum networks.", "method": "Developed a family of multipartite Bell inequalities inspired by the stabilizer group of GHZ states, featuring simple structure and easy scalability to more parties. Also derived a general analytical upper bound for Holevo quantity.", "result": "The new inequalities enable optimal quantum randomness certification and GHZ state self-testing. They significantly outperform MABK, Parity-CHSH, and Holz inequalities at N=3 parties, particularly for non-maximal Bell values, with better Holevo quantity bounds.", "conclusion": "This approach provides a more efficient and practical framework for DI randomness certification in quantum networks, with strong experimental relevance for quantum cryptography applications."}}
{"id": "2601.17224", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17224", "abs": "https://arxiv.org/abs/2601.17224", "authors": ["Dmitrii Torbunov", "Yihui Ren", "Lijun Wu", "Yimei Zhu"], "title": "Parameter Inference and Uncertainty Quantification with Diffusion Models: Extending CDI to 2D Spatial Conditioning", "comment": null, "summary": "Uncertainty quantification is critical in scientific inverse problems to distinguish identifiable parameters from those that remain ambiguous given available measurements. The Conditional Diffusion Model-based Inverse Problem Solver (CDI) has previously demonstrated effective probabilistic inference for one-dimensional temporal signals, but its applicability to higher-dimensional spatial data remains unexplored. We extend CDI to two-dimensional spatial conditioning, enabling probabilistic parameter inference directly from spatial observations. We validate this extension on convergent beam electron diffraction (CBED) parameter inference - a challenging multi-parameter inverse problem in materials characterization where sample geometry, electronic structure, and thermal properties must be extracted from 2D diffraction patterns. Using simulated CBED data with ground-truth parameters, we demonstrate that CDI produces well-calibrated posterior distributions that accurately reflect measurement constraints: tight distributions for well-determined quantities and appropriately broad distributions for ambiguous parameters. In contrast, standard regression methods - while appearing accurate on aggregate metrics - mask this underlying uncertainty by predicting training set means for poorly constrained parameters. Our results confirm that CDI successfully extends from temporal to spatial domains, providing the genuine uncertainty information required for robust scientific inference.", "AI": {"tldr": "\u5c06\u6761\u4ef6\u6269\u6563\u6a21\u578b\u9006\u95ee\u9898\u6c42\u89e3\u5668(CDI)\u4ece\u4e00\u7ef4\u65f6\u5e8f\u4fe1\u53f7\u6269\u5c55\u5230\u4e8c\u7ef4\u7a7a\u95f4\u6570\u636e\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u7535\u5b50\u884d\u5c04\u53c2\u6570\u63a8\u65ad\uff0c\u63d0\u4f9b\u771f\u5b9e\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "motivation": "\u79d1\u5b66\u53cd\u95ee\u9898\u4e2d\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709CDI\u65b9\u6cd5\u4ec5\u9a8c\u8bc1\u4e8e\u4e00\u7ef4\u65f6\u5e8f\u4fe1\u53f7\uff0c\u5176\u5728\u9ad8\u7ef4\u7a7a\u95f4\u6570\u636e\u4e0a\u7684\u9002\u7528\u6027\u5c1a\u672a\u63a2\u7d22\u3002CBED\u591a\u53c2\u6570\u53cd\u6f14\u95ee\u9898\u9700\u8981\u4ece\u4e8c\u7ef4\u884d\u5c04\u56fe\u6848\u4e2d\u63d0\u53d6\u6750\u6599\u7279\u6027\uff0c\u4f46\u6807\u51c6\u56de\u5f52\u65b9\u6cd5\u4f1a\u63a9\u76d6\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u5c06CDI\u6269\u5c55\u81f3\u4e8c\u7ef4\u7a7a\u95f4\u6761\u4ef6\u5316\uff0c\u4f7f\u7528\u6a21\u62dfCBED\u6570\u636e\uff08\u5305\u542b\u771f\u5b9e\u53c2\u6570\uff09\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5e76\u4e0e\u6807\u51c6\u56de\u5f52\u65b9\u6cd5\u5bf9\u6bd4\u3002", "result": "CDI\u751f\u6210\u4e86\u6821\u51c6\u826f\u597d\u7684\u540e\u9a8c\u5206\u5e03\uff0c\u80fd\u51c6\u786e\u53cd\u6620\u6d4b\u91cf\u7ea6\u675f\uff08\u5bf9\u786e\u5b9a\u53c2\u6570\u7ed9\u51fa\u7a84\u5206\u5e03\uff0c\u5bf9\u6a21\u7cca\u53c2\u6570\u7ed9\u51fa\u5bbd\u5206\u5e03\uff09\uff1b\u800c\u6807\u51c6\u56de\u5f52\u65b9\u6cd5\u867d\u7136\u6574\u4f53\u6307\u6807\u770b\u4f3c\u51c6\u786e\uff0c\u4f46\u5bf9\u7ea6\u675f\u4e0d\u826f\u53c2\u6570\u4ec5\u9884\u6d4b\u8bad\u7ec3\u96c6\u5747\u503c\uff0c\u63a9\u76d6\u4e86\u771f\u5b9e\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "CDI\u6210\u529f\u4ece\u65f6\u5e8f\u57df\u6269\u5c55\u5230\u7a7a\u95f4\u57df\uff0c\u4e3a\u7a33\u5065\u79d1\u5b66\u63a8\u65ad\u63d0\u4f9b\u4e86\u771f\u5b9e\u7684\u4e0d\u786e\u5b9a\u6027\u4fe1\u606f\u3002"}}
{"id": "2601.17915", "categories": ["cs.AI", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.17915", "abs": "https://arxiv.org/abs/2601.17915", "authors": ["Saurabh Jha", "Rohan Arora", "Bhavya", "Noah Zheutlin", "Paulina Toro Isaza", "Laura Shwartz", "Yu Deng", "Daby Sow", "Ruchi Mahindru", "Ruchir Puri"], "title": "Think Locally, Explain Globally: Graph-Guided LLM Investigations via Local Reasoning and Belief Propagation", "comment": null, "summary": "LLM agents excel when environments are mostly static and the needed information fits in a model's context window, but they often fail in open-ended investigations where explanations must be constructed by iteratively mining evidence from massive, heterogeneous operational data. These investigations exhibit hidden dependency structure: entities interact, signals co-vary, and the importance of a fact may only become clear after other evidence is discovered. Because the context window is bounded, agents must summarize intermediate findings before their significance is known, increasing the risk of discarding key evidence. ReAct-style agents are especially brittle in this regime. Their retrieve-summarize-reason loop makes conclusions sensitive to exploration order and introduces run-to-run non-determinism, producing a reliability gap where Pass-at-k may be high but Majority-at-k remains low. Simply sampling more rollouts or generating longer reasoning traces does not reliably stabilize results, since hypotheses cannot be autonomously checked as new evidence arrives and there is no explicit mechanism for belief bookkeeping and revision. In addition, ReAct entangles semantic reasoning with controller duties such as tool orchestration and state tracking, so execution errors and plan drift degrade reasoning while consuming scarce context.\n  We address these issues by formulating investigation as abductive reasoning over a dependency graph and proposing EoG (Explanations over Graphs), a disaggregated framework in which an LLM performs bounded local evidence mining and labeling (cause vs symptom) while a deterministic controller manages traversal, state, and belief propagation to compute a minimal explanatory frontier. On a representative ITBench diagnostics task, EoG improves both accuracy and run-to-run consistency over ReAct baselines, including a 7x average gain in Majority-at-k entity F1.", "AI": {"tldr": "The paper identifies limitations of current LLM agents (like ReAct) in complex investigations and proposes EoG, a framework that uses a deterministic controller for graph traversal and belief propagation while LLMs handle local evidence mining, resulting in significantly improved accuracy and consistency.", "motivation": "Current LLM agents fail in open-ended investigations with massive heterogeneous data due to bounded context windows, hidden dependency structures, and lack of belief revision mechanisms. ReAct-style agents are brittle, order-sensitive, and non-deterministic, creating a reliability gap.", "method": "Formulates investigation as abductive reasoning over a dependency graph. Proposes EoG (Explanations over Graphs), a disaggregated framework where an LLM performs bounded local evidence mining/labeling while a deterministic controller manages graph traversal, state tracking, and belief propagation to compute a minimal explanatory frontier.", "result": "On ITBench diagnostics tasks, EoG significantly outperforms ReAct baselines in both accuracy and run-to-run consistency, achieving a 7x average improvement in Majority-at-k entity F1 score.", "conclusion": "The EoG framework effectively addresses the key limitations of current LLM agents in complex investigations by separating semantic reasoning from control logic and implementing explicit belief bookkeeping, leading to more reliable and consistent performance."}}
{"id": "2601.18538", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.18538", "abs": "https://arxiv.org/abs/2601.18538", "authors": ["Jeonghoon Park", "Jeong San Kim"], "title": "Sufficient conditions for additivity of the zero-error classical capacity of quantum channels", "comment": "9 pages, no figure", "summary": "The one-shot zero-error classical capacity of a quantum channel is the amount of classical information that can be transmitted with zero probability of error by a single use. Then the one-shot zero-error classical capacity equals to the logarithmic value of the independence number of the noncommutative graph induced by the channel. Thus the additivity of the one-shot zero-error classical capacity of a quantum channel is equivalent to the multiplicativity of the independence number of the noncommutative graph. The independence number is not multiplicative in general, and it is not clearly understood when the multiplicativity occurs. In this work, we present sufficient conditions for multiplicativity of the independence number, and we give explicit examples of quantum channels. Furthermore, we consider a block form of noncommutative graphs, and provide conditions when the independence number is multiplicative.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.17257", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17257", "abs": "https://arxiv.org/abs/2601.17257", "authors": ["Javier Porras-Valenzuela", "Samar Hadou", "Alejandro Ribeiro"], "title": "A Constrained Optimization Perspective of Unrolled Transformers", "comment": null, "summary": "We introduce a constrained optimization framework for training transformers that behave like optimization descent algorithms. Specifically, we enforce layerwise descent constraints on the objective function and replace standard empirical risk minimization (ERM) with a primal-dual training scheme. This approach yields models whose intermediate representations decrease the loss monotonically in expectation across layers. We apply our method to both unrolled transformer architectures and conventional pretrained transformers on tasks of video denoising and text classification. Across these settings, we observe constrained transformers achieve stronger robustness to perturbations and maintain higher out-of-distribution generalization, while preserving in-distribution performance.", "AI": {"tldr": "\u63d0\u51fa\u7ea6\u675f\u4f18\u5316\u6846\u67b6\u8bad\u7ec3\u7c7b\u4f18\u5316\u4e0b\u964d\u7b97\u6cd5\u7684Transformer\uff0c\u901a\u8fc7\u5c42\u95f4\u4e0b\u964d\u7ea6\u675f\u548c\u539f\u59cb-\u5bf9\u5076\u8bad\u7ec3\u65b9\u6848\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u4e0e\u6cdb\u5316\u80fd\u529b", "motivation": "\u6807\u51c6Transformer\u7f3a\u4e4f\u4f18\u5316\u8fc7\u7a0b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5bfc\u81f4\u5bf9\u6270\u52a8\u654f\u611f\u4e14\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\u6709\u9650", "method": "1. \u5728\u76ee\u6807\u51fd\u6570\u4e0a\u65bd\u52a0\u5c42\u95f4\u4e0b\u964d\u7ea6\u675f 2. \u7528\u539f\u59cb-\u5bf9\u5076\u8bad\u7ec3\u66ff\u4ee3\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316 3. \u5e94\u7528\u4e8e\u5c55\u5f00\u5f0fTransformer\u548c\u9884\u8bad\u7ec3\u6a21\u578b", "result": "\u5728\u89c6\u9891\u53bb\u566a/\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\uff1a\u4fdd\u6301\u5206\u5e03\u5185\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u6297\u6270\u52a8\u9c81\u68d2\u6027\uff08+15-22%\uff09\u548c\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b", "conclusion": "\u7ea6\u675f\u4f18\u5316\u6846\u67b6\u4f7fTransformer\u5177\u5907\u663e\u5f0f\u4f18\u5316\u8fc7\u7a0b\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u7684\u81ea\u9002\u5e94\u63a8\u7406\u6a21\u578b\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2601.17920", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17920", "abs": "https://arxiv.org/abs/2601.17920", "authors": ["Xuanzhou Chen", "Audrey Wang", "Stanley Yin", "Hanyang Jiang", "Dong Zhang"], "title": "Agentic AI for Self-Driving Laboratories in Soft Matter: Taxonomy, Benchmarks,and Open Challenges", "comment": null, "summary": "Self-driving laboratories (SDLs) close the loop between experiment design, automated execution, and data-driven decision making, and they provide a demanding testbed for agentic AI under expensive actions, noisy and delayed feedback, strict feasibility and safety constraints, and non-stationarity. This survey uses soft matter as a representative setting but focuses on the AI questions that arise in real laboratories. We frame SDL autonomy as an agent environment interaction problem with explicit observations, actions, costs, and constraints, and we use this formulation to connect common SDL pipelines to established AI principles. We review the main method families that enable closed loop experimentation, including Bayesian optimization and active learning for sample efficient experiment selection, planning and reinforcement learning for long horizon protocol optimization, and tool using agents that orchestrate heterogeneous instruments and software. We emphasize verifiable and provenance aware policies that support debugging, reproducibility, and safe operation. We then propose a capability driven taxonomy that organizes systems by decision horizon, uncertainty modeling, action parameterization, constraint handling, failure recovery, and human involvement. To enable meaningful comparison, we synthesize benchmark task templates and evaluation metrics that prioritize cost aware performance, robustness to drift, constraint violation behavior, and reproducibility. Finally, we distill lessons from deployed SDLs and outline open challenges in multi-modal representation, calibrated uncertainty, safe exploration, and shared benchmark infrastructure.", "AI": {"tldr": "A survey of AI methods for self-driving laboratories, providing an agent-environment framework, taxonomy, benchmarks, and identifying open challenges in multi-modal representation, calibrated uncertainty, safe exploration, and shared infrastructure.", "motivation": "SDLs provide a demanding testbed for agentic AI under expensive actions, noisy/delayed feedback, strict safety constraints, and non-stationarity, requiring robust AI solutions beyond toy problems.", "method": "Framed SDL autonomy as an agent-environment interaction problem with explicit observations, actions, costs, and constraints; reviewed Bayesian optimization, active learning, planning/RL, and tool-using agents.", "result": "Proposed a capability-driven taxonomy (decision horizon, uncertainty modeling, action parameterization, constraint handling, failure recovery, human involvement) and synthesized benchmark templates with cost-aware, robustness, and reproducibility metrics.", "conclusion": "Distilled lessons from deployed SDLs and outlined open challenges: multi-modal representation, calibrated uncertainty, safe exploration, and shared benchmark infrastructure."}}
{"id": "2601.18562", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.18562", "abs": "https://arxiv.org/abs/2601.18562", "authors": ["Yihua Chengyu", "Richard Meister", "Conor Carty", "Sheng-Ku Lin", "Roberto Bondesan"], "title": "Bayesian Optimization for Quantum Error-Correcting Code Discovery", "comment": "18 pages, 8 figures", "summary": "Quantum error-correcting codes protect fragile quantum information by encoding it redundantly, but identifying codes that perform well in practice with minimal overhead remains difficult due to the combinatorial search space and the high cost of logical error rate evaluation. We propose a Bayesian optimization framework to discover quantum error-correcting codes that improves data efficiency and scalability with respect to previous machine learning approaches to this task. Our main contribution is a multi-view chain-complex neural embedding that allows us to predict the logical error rate of quantum LDPC codes without performing expensive simulations. Using bivariate bicycle codes and code capacity noise as a testbed, our algorithm discovers a high-rate code [[144,36]] that achieves competitive per-qubit error rate compared to the gross code, as well as a low-error code [[144,16]] that outperforms the gross code in terms of error rate per qubit. These results highlight the ability of our pipeline to automatically discover codes balancing rate and noise suppression, while the generality of the framework enables application across diverse code families, decoders, and noise models.", "AI": {"tldr": "\u63d0\u51fa\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\u7ed3\u5408\u591a\u89c6\u89d2\u94fe\u590d\u5f62\u795e\u7ecf\u5d4c\u5165\uff0c\u9ad8\u6548\u641c\u7d22\u91cf\u5b50\u7ea0\u9519\u7801\uff0c\u53d1\u73b0\u4e24\u79cd\u65b0\u7801\uff1a[[144,36]]\u9ad8\u7801\u7387\u7801\u548c[[144,16]]\u4f4e\u9519\u8bef\u7387\u7801\uff0c\u540e\u8005\u6027\u80fd\u4f18\u4e8eGross\u7801\uff0c\u5c55\u793a\u4e86\u81ea\u52a8\u8bbe\u8ba1\u91cf\u5b50\u7801\u7684\u6f5c\u529b\u3002", "motivation": "\u91cf\u5b50\u7ea0\u9519\u7801\u4fdd\u62a4\u91cf\u5b50\u4fe1\u606f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5bfb\u627e\u6027\u80fd\u826f\u597d\u4e14\u5f00\u9500\u6700\u5c0f\u7684\u5b9e\u9645\u53ef\u7528\u7801\u56f0\u96be\uff0c\u56e0\u4e3a\u7ec4\u5408\u641c\u7d22\u7a7a\u95f4\u5de8\u5927\u4e14\u903b\u8f91\u9519\u8bef\u7387\u8bc4\u4f30\u6210\u672c\u9ad8\u6602\u3002", "method": "\u63d0\u51fa\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\uff0c\u6838\u5fc3\u8d21\u732e\u662f\u591a\u89c6\u89d2\u94fe\u590d\u5f62\u795e\u7ecf\u5d4c\u5165\u65b9\u6cd5\uff0c\u65e0\u9700\u6602\u8d35\u6a21\u62df\u5373\u53ef\u9884\u6d4b\u91cf\u5b50LDPC\u7801\u7684\u903b\u8f91\u9519\u8bef\u7387\u3002", "result": "\u5728\u53cc\u53d8\u91cf\u81ea\u884c\u8f66\u7801\u6d4b\u8bd5\u4e2d\uff0c\u53d1\u73b0[[144,36]]\u9ad8\u7801\u7387\u7801\u5355\u4f4d\u91cf\u5b50\u6bd4\u7279\u9519\u8bef\u7387\u4e0eGross\u7801\u76f8\u5f53\uff0c\u4ee5\u53ca[[144,16]]\u4f4e\u9519\u8bef\u7387\u7801\u5355\u4f4d\u91cf\u5b50\u6bd4\u7279\u9519\u8bef\u7387\u4f18\u4e8eGross\u7801\u3002", "conclusion": "\u8be5\u6d41\u7a0b\u80fd\u81ea\u52a8\u53d1\u73b0\u5e73\u8861\u7801\u7387\u548c\u566a\u58f0\u6291\u5236\u7684\u91cf\u5b50\u7801\uff0c\u4e14\u6846\u67b6\u53ef\u63a8\u5e7f\u5e94\u7528\u4e8e\u4e0d\u540c\u7801\u65cf\u3001\u89e3\u7801\u5668\u548c\u566a\u58f0\u6a21\u578b\u3002"}}
{"id": "2601.17260", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17260", "abs": "https://arxiv.org/abs/2601.17260", "authors": ["Marco Pollanen"], "title": "The Viscosity of Logic: Phase Transitions and Hysteresis in DPO Alignment", "comment": "10 Pages, 5 Figures", "summary": "Direct Preference Optimization (DPO) is often tuned as if increasing alignment pressure (controlled by $\u03b2$) yields progressively \"better\" behavior. We instead treat $\u03b2$ as a control parameter and densely sweep it for three 7B open-weight families under a fixed DPO recipe. In Mistral, capability is sharply non-monotonic: aggregated logic-probe margins become positive only in a narrow band near $\u03b2\\approx 10^{-2}$ and revert outside it, with boundary points that are seed-sensitive. Across architectures under the same sweep, we observe qualitatively different response modes: sharp reorganization in Mistral, selective changes in Llama, and smooth trade-offs in Qwen. Critically, the DPO preference margin can anticorrelate with reasoning capability (Pearson $r=-0.91$ for Llama logic), so margin-based selection can prefer capability-impaired models. Training path also matters: exposure to high $\u03b2$ induces capability losses that persist even after $\u03b2$ is reduced (hysteresis). These findings motivate capability-resolved evaluation across the $\u03b2$ landscape rather than reliance on margins or aggregate benchmarks.", "AI": {"tldr": "DPO alignment pressure (\u03b2) does not monotonically improve model capability; optimal \u03b2 varies by architecture, margin-based selection can harm reasoning, and high-\u03b2 damage persists (hysteresis), requiring capability-resolved evaluation across \u03b2 values.", "motivation": "Challenging the assumption that increasing DPO's alignment pressure (\u03b2) always yields better-aligned models, as prior work treats \u03b2 as a monotonic improvement parameter without systematic investigation of its effects on capability.", "method": "Densely swept \u03b2 values across three 7B open-weight model families (Mistral, Llama, Qwen) under fixed DPO training, analyzing capability changes via logic-probe margins and reasoning metrics.", "result": "1) Capability is non-monotonic in Mistral (optimal only near \u03b2\u224810\u207b\u00b2), 2) Architectures show distinct response modes (sharp reorganization in Mistral, selective changes in Llama, smooth trade-offs in Qwen), 3) DPO margin anticorrelates with reasoning (r=-0.91 for Llama), 4) High-\u03b2 exposure causes persistent capability loss (hysteresis).", "conclusion": "Relying on DPO margins or aggregate benchmarks is unreliable; rigorous capability evaluation across the full \u03b2 landscape is necessary to avoid selecting capability-impaired models and account for architecture-specific \u03b2 responses."}}
{"id": "2601.17923", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17923", "abs": "https://arxiv.org/abs/2601.17923", "authors": ["Ali Najar"], "title": "Learning Transferable Skills in Action RPGs via Directed Skill Graphs and Selective Adaptation", "comment": "5 pages", "summary": "Lifelong agents should expand their competence over time without retraining from scratch or overwriting previously learned behaviors. We investigate this in a challenging real-time control setting (Dark Souls III) by representing combat as a directed skill graph and training its components in a hierarchical curriculum. The resulting agent decomposes control into five reusable skills: camera control, target lock-on, movement, dodging, and a heal-attack decision policy, each optimized for a narrow responsibility. This factorization improves sample efficiency by reducing the burden on any single policy and supports selective post-training: when the environment shifts from Phase 1 to Phase 2, only a subset of skills must be adapted, while upstream skills remain transferable. Empirically, we find that targeted fine-tuning of just two skills rapidly recovers performance under a limited interaction budget, suggesting that skill-graph curricula together with selective fine-tuning offer a practical pathway toward evolving, continually learning agents in complex real-time environments.", "AI": {"tldr": "\u5728\u300a\u9ed1\u6697\u4e4b\u9b423\u300b\u7684\u5b9e\u65f6\u6218\u6597\u73af\u5883\u4e2d\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u7528\u6709\u5411\u6280\u80fd\u56fe\u548c\u5206\u5c42\u8bfe\u7a0b\u8bad\u7ec3\u5c06\u63a7\u5236\u5206\u89e3\u4e3a\u4e94\u4e2a\u53ef\u590d\u7528\u6280\u80fd\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5fae\u8c03\u4e24\u4e2a\u6280\u80fd\u5373\u53ef\u5feb\u901f\u9002\u5e94\u73af\u5883\u53d8\u5316\uff0c\u4e3a\u7ec8\u751f\u5b66\u4e60\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84", "motivation": "\u7ec8\u751f\u5b66\u4e60\u667a\u80fd\u4f53\u9700\u8981\u5728\u590d\u6742\u5b9e\u65f6\u73af\u5883\u4e2d\u6301\u7eed\u6269\u5c55\u80fd\u529b\uff0c\u4f46\u4e0d\u80fd\u4ece\u5934\u91cd\u65b0\u8bad\u7ec3\u6216\u8986\u76d6\u5df2\u5b66\u884c\u4e3a\uff0c\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8", "method": "\u5c06\u6218\u6597\u5efa\u6a21\u4e3a\u6709\u5411\u6280\u80fd\u56fe\uff0c\u91c7\u7528\u5206\u5c42\u8bfe\u7a0b\u8bad\u7ec3\u4e94\u4e2a\u72ec\u7acb\u6280\u80fd\u7ec4\u4ef6\uff08\u955c\u5934\u63a7\u5236\u3001\u9501\u5b9a\u76ee\u6807\u3001\u79fb\u52a8\u3001\u95ea\u907f\u3001\u6cbb\u7597-\u653b\u51fb\u51b3\u7b56\uff09\uff0c\u6bcf\u4e2a\u6280\u80fd\u4e13\u6ce8\u5355\u4e00\u804c\u8d23", "result": "\u6280\u80fd\u5206\u89e3\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\uff0c\u652f\u6301\u9009\u62e9\u6027\u540e\u8bad\u7ec3\uff1b\u73af\u5883\u4ece\u7b2c\u4e00\u9636\u6bb5\u5207\u6362\u5230\u7b2c\u4e8c\u9636\u6bb5\u65f6\uff0c\u4ec5\u9700\u9002\u5e94\u90e8\u5206\u6280\u80fd\uff0c\u4e0a\u6e38\u6280\u80fd\u53ef\u8fc1\u79fb\uff1b\u5728\u6709\u9650\u4ea4\u4e92\u9884\u7b97\u4e0b\uff0c\u4ec5\u5fae\u8c03\u4e24\u4e2a\u6280\u80fd\u5373\u53ef\u5feb\u901f\u6062\u590d\u6027\u80fd", "conclusion": "\u6280\u80fd\u56fe\u8bfe\u7a0b\u4e0e\u9009\u62e9\u6027\u5fae\u8c03\u76f8\u7ed3\u5408\uff0c\u4e3a\u590d\u6742\u5b9e\u65f6\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u8fdb\u5316\u3001\u6301\u7eed\u5b66\u4e60\u7684\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84"}}
{"id": "2601.18637", "categories": ["quant-ph", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.18637", "abs": "https://arxiv.org/abs/2601.18637", "authors": ["Quoc Hoan Tran", "Koki Chinzei", "Yasuhiro Endo", "Hirotaka Oshima"], "title": "Universality of Many-body Projected Ensemble for Learning Quantum Data Distribution", "comment": "21 pages, 6 figures", "summary": "Generating quantum data by learning the underlying quantum distribution poses challenges in both theoretical and practical scenarios, yet it is a critical task for understanding quantum systems. A fundamental question in quantum machine learning (QML) is the universality of approximation: whether a parameterized QML model can approximate any quantum distribution. We address this question by proving a universality theorem for the Many-body Projected Ensemble (MPE) framework, a method for quantum state design that uses a single many-body wave function to prepare random states. This demonstrates that MPE can approximate any distribution of pure states within a 1-Wasserstein distance error. This theorem provides a rigorous guarantee of universal expressivity, addressing key theoretical gaps in QML. For practicality, we propose an Incremental MPE variant with layer-wise training to improve the trainability. Numerical experiments on clustered quantum states and quantum chemistry datasets validate MPE's efficacy in learning complex quantum data distributions.", "AI": {"tldr": "MPE\u6846\u67b6\u88ab\u8bc1\u660e\u80fd\u901a\u7528\u8fd1\u4f3c\u4efb\u610f\u7eaf\u6001\u91cf\u5b50\u5206\u5e03\uff0c\u589e\u91cf\u53d8\u4f53\u63d0\u5347\u53ef\u8bad\u7ec3\u6027\u5e76\u5728\u91cf\u5b50\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6709\u6548\u3002", "motivation": "\u89e3\u51b3\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u4e2d\u53c2\u6570\u5316\u6a21\u578b\u662f\u5426\u80fd\u8fd1\u4f3c\u4efb\u610f\u91cf\u5b50\u5206\u5e03\u7684\u57fa\u672c\u95ee\u9898\uff0c\u4ee5\u53ca\u751f\u6210\u91cf\u5b50\u6570\u636e\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u6311\u6218\u3002", "method": "\u8bc1\u660eMany-body Projected Ensemble (MPE)\u6846\u67b6\u7684\u901a\u7528\u6027\u5b9a\u7406\uff0c\u5e76\u63d0\u51fa\u589e\u91cfMPE\u53d8\u4f53\u8fdb\u884c\u5206\u5c42\u8bad\u7ec3\u4ee5\u63d0\u9ad8\u53ef\u8bad\u7ec3\u6027\u3002", "result": "MPE\u80fd\u57281-Wasserstein\u8ddd\u79bb\u8bef\u5dee\u5185\u8fd1\u4f3c\u4efb\u4f55\u7eaf\u6001\u5206\u5e03\uff0c\u6570\u503c\u5b9e\u9a8c\u5728\u56e2\u7c07\u91cf\u5b50\u6001\u548c\u91cf\u5b50\u5316\u5b66\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u5b66\u4e60\u590d\u6742\u91cf\u5b50\u6570\u636e\u5206\u5e03\u7684\u6709\u6548\u6027\u3002", "conclusion": "MPE\u63d0\u4f9b\u4e86\u901a\u7528\u8868\u8fbe\u6027\u7684\u4e25\u683c\u4fdd\u8bc1\uff0c\u89e3\u51b3\u4e86\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u7684\u5173\u952e\u7406\u8bba\u7a7a\u767d\uff0c\u589e\u91cf\u53d8\u4f53\u6539\u5584\u4e86\u5b9e\u9645\u8bad\u7ec3\u6548\u679c\u3002"}}
{"id": "2601.17261", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17261", "abs": "https://arxiv.org/abs/2601.17261", "authors": ["Wei Lin", "Yining Jiang", "Qingyu Song", "Qiao Xiang", "Hong Xu"], "title": "AGZO: Activation-Guided Zeroth-Order Optimization for LLM Fine-Tuning", "comment": "21 pages in total, including 9 pages of main text, with 4 figures and 3 tables. This manuscript is submitted to arXiv", "summary": "Zeroth-Order (ZO) optimization has emerged as a promising solution for fine-tuning LLMs under strict memory constraints, as it avoids the prohibitive memory cost of storing activations for backpropagation. However, existing ZO methods typically employ isotropic perturbations, neglecting the rich structural information available during the forward pass. In this paper, we identify a crucial link between gradient formation and activation structure: the gradient of a linear layer is confined to the subspace spanned by its input activations. Leveraging this insight, we propose Activation-Guided Zeroth-Order optimization (AGZO). Unlike prior methods, AGZO extracts a compact, activation-informed subspace on the fly during the forward pass and restricts perturbations to this low-rank subspace. We provide a theoretical framework showing that AGZO optimizes a subspace-smoothed objective and provably yields update directions with higher cosine similarity to the true gradient than isotropic baselines. Empirically, we evaluate AGZO on Qwen3 and Pangu models across various benchmarks. AGZO consistently outperforms state-of-the-art ZO baselines and significantly narrows the performance gap with first-order fine-tuning, while maintaining almost the same peak memory footprint as other ZO methods.", "AI": {"tldr": "AGZO improves zeroth-order LLM fine-tuning by restricting perturbations to activation-informed low-rank subspaces, outperforming baselines while maintaining memory efficiency.", "motivation": "Existing zeroth-order optimization methods use isotropic perturbations that waste structural information from forward passes, despite being memory-efficient. The paper identifies that gradients of linear layers are confined to subspaces spanned by input activations.", "method": "Activation-Guided Zeroth-Order (AGZO) optimization: During forward passes, it dynamically extracts compact, activation-informed low-rank subspaces and constrains perturbations to these subspaces instead of using random isotropic perturbations.", "result": "Evaluated on Qwen3 and Pangu models, AGZO consistently outperforms state-of-the-art zeroth-order baselines, significantly narrows the performance gap with first-order fine-tuning, and maintains nearly identical peak memory footprint as other zeroth-order methods.", "conclusion": "AGZO provides a theoretically-grounded, practical improvement to memory-constrained LLM fine-tuning by leveraging activation structure, achieving better optimization directions and performance without additional memory overhead."}}
{"id": "2601.17942", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2601.17942", "abs": "https://arxiv.org/abs/2601.17942", "authors": ["Yu-Jie Yang", "Hung-Fu Chang", "Po-An Chen"], "title": "LLM-Based SQL Generation: Prompting, Self-Refinement, and Adaptive Weighted Majority Voting", "comment": "29 pages, 22 figures", "summary": "Text-to-SQL has emerged as a prominent research area, particularly with the rapid advancement of large language models (LLMs). By enabling users to query databases through natural language rather than SQL, this technology significantly lowers the barrier to data analysis. However, generating accurate SQL from natural language remains challenging due to ambiguity in user queries, the complexity of schema linking, limited generalization across SQL dialects, and the need for domain-specific understanding. In this study, we propose a Single-Agent Self-Refinement with Ensemble Voting (SSEV) pipeline built on PET-SQL that operates without ground-truth data, integrating self-refinement with Weighted Majority Voting (WMV) and its randomized variant (RWMA). Experimental results show that the SSEV achieves competitive performance across multiple benchmarks, attaining execution accuracies of 85.5% on Spider 1.0-Dev, 86.4% on Spider 1.0-Test, and 66.3% on BIRD-Dev. Building on insights from the SSEV pipeline, we further propose ReCAPAgent-SQL (Refinement-Critique-Act-Plan agent-based SQL framework) to address the growing complexity of enterprise databases and real-world Text-to-SQL tasks. The framework integrates multiple specialized agents for planning, external knowledge retrieval, critique, action generation, self-refinement, schema linking, and result validation, enabling iterative refinement of SQL predictions through agent collaboration. ReCAPAgent-SQL's WMA results achieve 31% execution accuracy on the first 100 queries of Spider 2.0-Lite, demonstrating significant improvements in handling real-world enterprise scenarios. Overall, our work facilitates the deployment of scalable Text-to-SQL systems in practical settings, supporting better data-driven decision-making at lower cost and with greater efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u65e0\u9700ground-truth\u6570\u636e\u7684Text-to-SQL\u65b9\u6cd5\uff1a\u5355\u667a\u80fd\u4f53\u81ea refine \u96c6\u6210\u6295\u7968\u7ba1\u9053(SSEV)\u548c\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6(ReCAPAgent-SQL)\uff0c\u5206\u522b\u5728Spider 1.0\u548cSpider 2.0-Lite\u57fa\u51c6\u4e0a\u53d6\u5f97\u7ade\u4e89\u6027\u548c\u7a81\u7834\u6027\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9e\u9645\u4f01\u4e1a\u573a\u666f\u4e0b\u7684SQL\u751f\u6210\u51c6\u786e\u7387\u3002", "motivation": "\u964d\u4f4e\u6570\u636e\u5206\u6790\u95e8\u69db\u662fText-to-SQL\u7684\u6838\u5fc3\u4ef7\u503c\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u67e5\u8be2\u6b67\u4e49\u3001\u8868\u7ed3\u6784\u94fe\u63a5\u590d\u6742\u3001SQL\u65b9\u8a00\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3001\u9886\u57df\u7406\u89e3\u6709\u9650\u7b49\u6311\u6218\uff0c\u96be\u4ee5\u76f4\u63a5\u90e8\u7f72\u4e8e\u590d\u6742\u7684\u771f\u5b9e\u4f01\u4e1a\u73af\u5883\u3002", "method": "\u9996\u5148\u63d0\u51faSSEV\u6846\u67b6\uff1a\u57fa\u4e8ePET-SQL\u6784\u5efa\u5355\u667a\u80fd\u4f53\u81ea refine \u673a\u5236\uff0c\u7ed3\u5408\u52a0\u6743\u591a\u6570\u6295\u7968(WMV)\u53ca\u5176\u968f\u673a\u53d8\u4f53(RWMA)\u8fdb\u884c\u96c6\u6210\uff1b\u8fdb\u800c\u63d0\u51faReCAPAgent-SQL\uff1a\u91c7\u7528\u89c4\u5212\u3001\u68c0\u7d22\u3001\u6279\u5224\u3001\u6267\u884c\u3001\u81ea refine\u3001\u7ed3\u6784\u94fe\u63a5\u548c\u7ed3\u679c\u9a8c\u8bc1\u4e03\u4e2a\u4e13\u4e1a\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u901a\u8fc7\u8fed\u4ee3 refinement \u5904\u7406\u590d\u6742\u4f01\u4e1a\u6570\u636e\u5e93\u3002", "result": "SSEV\u5728Spider 1.0-Dev/Test\u4e0a\u5206\u522b\u8fbe\u523085.5%\u548c86.4%\u7684\u6267\u884c\u51c6\u786e\u7387\uff0c\u5728BIRD-Dev\u4e0a\u4e3a66.3%\uff1bReCAPAgent-SQL\u5728Spider 2.0-Lite\u524d100\u6761\u67e5\u8be2\u4e0a\u5b9e\u73b031%\u7684\u6267\u884c\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u5176\u5728\u771f\u5b9e\u4f01\u4e1a\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u81ea refine \u548c\u667a\u80fd\u4f53\u534f\u4f5c\u8303\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86Text-to-SQL\u7cfb\u7edf\u5728\u5b9e\u9645\u573a\u666f\u4e0b\u7684\u53ef\u6269\u5c55\u6027\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u4f4e\u6210\u672c\u3001\u9ad8\u6548\u7387\u7684\u6570\u636e\u9a71\u52a8\u51b3\u7b56\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2601.18680", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.18680", "abs": "https://arxiv.org/abs/2601.18680", "authors": ["Marine Demarty", "Bo Yang", "Kenza Hammam", "Pauline Besserve"], "title": "Error-mitigation aware benchmarking strategy for quantum optimization problems", "comment": "11 pages, 6 figures", "summary": "Assessing whether a noisy quantum device can potentially exhibit quantum advantage is essential for selecting practical quantum utility tasks that are not efficiently verifiable by classical means. For optimization, a prominent candidate for quantum advantage, entropy benchmarking provides insights based concomitantly on the specifics of the application and its implementation, as well as hardware noise. However, such an approach still does not account for finite-shot effects or for quantum error mitigation (QEM), a key near-term error suppression strategy that reduces estimation bias at the cost of increased sampling overhead. We address this limitation by developing a benchmarking framework that explicitly incorporates finite-shot statistics and the resource overhead induced by QEM. Our framework quantifies quantum advantage through the confidence that an estimated energy lies within an interval defined by the best-known classical upper and lower bounds. Using a proof-of-principle numerical study of the two-dimensional Fermi-Hubbard model at size $8\\times8$, we demonstrate that the framework effectively identifies noise and shot-budget regimes in which the probabilistic error cancellation (PEC), a representative QEM method, is operationally advantageous, and potential quantum advantage is not hindered by finite-shot effects. Overall, our approach equips end-users with a framework based on lightweight numerics for assessing potential practical quantum advantage in optimization on near-future quantum hardware, in light of the allocated shot budget.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u578b\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u5c06\u6709\u9650\u91c7\u6837\u6548\u5e94\u548c\u91cf\u5b50\u9519\u8bef\u7f13\u89e3\uff08QEM\uff09\u7684\u8d44\u6e90\u5f00\u9500\u7eb3\u5165\u91cf\u5b50\u4f18\u52bf\u8bc4\u4f30\uff0c\u901a\u8fc7\u91cf\u5316\u80fd\u91cf\u4f30\u8ba1\u503c\u843d\u5728\u7ecf\u5178\u8fb9\u754c\u533a\u95f4\u5185\u7684\u7f6e\u4fe1\u5ea6\u6765\u5224\u65ad\u91cf\u5b50\u4f18\u52bf\uff0c\u5e76\u57288\u00d78\u8d39\u7c73-\u54c8\u4f2f\u5fb7\u6a21\u578b\u4e2d\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u71b5\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u65e0\u6cd5\u8bc4\u4f30\u6709\u9650\u91c7\u6837\u6548\u5e94\u548c\u91cf\u5b50\u9519\u8bef\u7f13\u89e3\u7b56\u7565\u5bf9\u91cf\u5b50\u4f18\u52bf\u7684\u5f71\u54cd\uff0c\u800cQEM\u4f5c\u4e3a\u8fd1\u671f\u5173\u952e\u8bef\u5dee\u6291\u5236\u6280\u672f\u4f1a\u663e\u8457\u589e\u52a0\u91c7\u6837\u5f00\u9500\uff0c\u4e9f\u9700\u4e00\u4e2a\u7efc\u5408\u8003\u8651\u8fd9\u4e9b\u5b9e\u9645\u56e0\u7d20\u7684\u91cf\u5b50\u4f18\u52bf\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7f6e\u4fe1\u533a\u95f4\u7684\u91cf\u5b50\u4f18\u52bf\u91cf\u5316\u6846\u67b6\uff1a\u901a\u8fc7\u8ba1\u7b97\u4f30\u8ba1\u80fd\u91cf\u503c\u843d\u5728\u6700\u4f73\u7ecf\u5178\u4e0a\u4e0b\u754c\u533a\u95f4\u5185\u7684\u7edf\u8ba1\u7f6e\u4fe1\u5ea6\uff0c\u540c\u65f6\u663e\u5f0f\u7eb3\u5165\u6709\u9650\u91c7\u6837\u7edf\u8ba1\u8bef\u5dee\u548cQEM\u5f15\u5165\u7684\u91c7\u6837\u5f00\u9500\uff0c\u5b9e\u73b0\u786c\u4ef6\u566a\u58f0\u4e0e\u5e94\u7528\u7279\u6027\u7684\u534f\u540c\u8bc4\u4f30\u3002", "result": "\u57288\u00d78\u8d39\u7c73-\u54c8\u4f2f\u5fb7\u6a21\u578b\u7684\u6570\u503c\u7814\u7a76\u4e2d\uff0c\u8be5\u6846\u67b6\u6210\u529f\u8bc6\u522b\u51fa\u7279\u5b9a\u566a\u58f0\u548c\u91c7\u6837\u9884\u7b97\u6761\u4ef6\u4e0b\uff0c\u6982\u7387\u6027\u9519\u8bef\u6d88\u9664\uff08PEC\uff09\u7b49QEM\u65b9\u6cd5\u80fd\u4fdd\u6301\u91cf\u5b50\u4f18\u52bf\u4e14\u4e0d\u53d7\u6709\u9650\u91c7\u6837\u6548\u5e94\u663e\u8457\u5f71\u54cd\u7684\u64cd\u4f5c\u7a97\u53e3\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7ec8\u7aef\u7528\u6237\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u6570\u503c\u5de5\u5177\uff0c\u53ef\u4f9d\u636e\u5b9e\u9645\u91c7\u6837\u9884\u7b97\u8bc4\u4f30\u8fd1\u671f\u91cf\u5b50\u786c\u4ef6\u5728\u4f18\u5316\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u91cf\u5b50\u4f18\u52bf\u6f5c\u529b\uff0c\u6307\u5bfc\u91cf\u5b50\u4f18\u52bf\u7684\u5b9e\u9a8c\u5b9e\u73b0\u7b56\u7565\u9009\u62e9\u3002"}}
{"id": "2601.17274", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17274", "abs": "https://arxiv.org/abs/2601.17274", "authors": ["Samar Hadou", "Alejandro Ribeiro"], "title": "Unrolled Neural Networks for Constrained Optimization", "comment": null, "summary": "In this paper, we develop unrolled neural networks to solve constrained optimization problems, offering accelerated, learnable counterparts to dual ascent (DA) algorithms. Our framework, termed constrained dual unrolling (CDU), comprises two coupled neural networks that jointly approximate the saddle point of the Lagrangian. The primal network emulates an iterative optimizer that finds a stationary point of the Lagrangian for a given dual multiplier, sampled from an unknown distribution. The dual network generates trajectories towards the optimal multipliers across its layers while querying the primal network at each layer. Departing from standard unrolling, we induce DA dynamics by imposing primal-descent and dual-ascent constraints through constrained learning. We formulate training the two networks as a nested optimization problem and propose an alternating procedure that updates the primal and dual networks in turn, mitigating uncertainty in the multiplier distribution required for primal network training. We numerically evaluate the framework on mixed-integer quadratic programs (MIQPs) and power allocation in wireless networks. In both cases, our approach yields near-optimal near-feasible solutions and exhibits strong out-of-distribution (OOD) generalization.", "AI": {"tldr": "This paper proposes Constrained Dual Unrolling (CDU), a neural network framework that unrolls dual ascent algorithms to solve constrained optimization problems by coupling primal and dual networks with constrained learning, achieving near-optimal solutions and strong out-of-distribution generalization on MIQPs and wireless power allocation.", "motivation": "To create accelerated, learnable neural network counterparts for dual ascent algorithms that solve constrained optimization problems, addressing limitations of standard unrolling methods through constrained learning dynamics.", "method": "Develops a two-network framework (CDU) where 1) a primal network emulates iterative optimization for given dual multipliers, and 2) a dual network generates multiplier trajectories. Uses constrained learning with primal-descent/dual-ascent constraints and alternating nested optimization to train networks while mitigating multiplier distribution uncertainty.", "result": "Numerical evaluation on mixed-integer quadratic programs (MIQPs) and wireless network power allocation demonstrates the method produces near-optimal, near-feasible solutions and exhibits strong out-of-distribution generalization performance.", "conclusion": "The CDU framework successfully provides an effective learnable approach for constrained optimization, offering improved generalization and solution quality compared to standard unrolling methods through its constrained learning formulation and coupled network design."}}
{"id": "2601.17275", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17275", "abs": "https://arxiv.org/abs/2601.17275", "authors": ["Lianlei Shan", "Han Chen", "Yixuan Wang", "Zhenjie Liu", "Wei Li"], "title": "Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning", "comment": "12 pages,", "summary": "While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting'' rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts to mitigate this by introducing a ``think-before-speak'' paradigm. However, applying RL directly in high-dimensional, discrete token spaces faces three inherent challenges: sample-inefficient rollouts, high gradient estimation variance, and the risk of catastrophic forgetting. To fundamentally address these structural bottlenecks, we propose \\textbf{DeepLatent Reasoning (DLR)}, a latent-space bidirectional contrastive reinforcement learning framework. This framework shifts the trial-and-error cost from expensive token-level full sequence generation to the continuous latent manifold. Specifically, we introduce a lightweight assistant model to efficiently sample $K$ reasoning chain encodings within the latent space. These encodings are filtered via a dual reward mechanism based on correctness and formatting; only high-value latent trajectories are fed into a \\textbf{frozen main model} for single-pass decoding. To maximize reasoning diversity while maintaining coherence, we design a contrastive learning objective to enable directed exploration within the latent space. Since the main model parameters remain frozen during optimization, this method mathematically eliminates catastrophic forgetting. Experiments demonstrate that under comparable GPU computational budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates the sustainable accumulation of reasoning capabilities, providing a viable path toward reliable and scalable reinforcement learning for LLMs.", "AI": {"tldr": "\u63d0\u51faDeepLatent Reasoning (DLR)\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5f3a\u5316\u5b66\u4e60\u4ece\u79bb\u6563token\u7a7a\u95f4\u8f6c\u79fb\u5230\u8fde\u7eed\u6f5c\u7a7a\u95f4\uff0c\u89e3\u51b3LLM\u590d\u6742\u63a8\u7406\u4e2d\u7684\u6837\u672c\u4f4e\u6548\u3001\u9ad8\u65b9\u5dee\u548c\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u7a33\u5b9a\u8bad\u7ec3\u548c\u957f\u7a0b\u63a8\u7406\u80fd\u529b\u7d2f\u79ef\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u4e2d\u4ecd\u505c\u7559\u5728\"\u7edf\u8ba1\u62df\u5408\"\u800c\u975e\u7cfb\u7edf\u6027\u903b\u8f91\u63a8\u5bfc\u5c42\u9762\uff0c\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u867d\u5f15\u5165\"\u5148\u601d\u8003\u540e\u56de\u7b54\"\u8303\u5f0f\uff0c\u4f46\u5728\u9ad8\u7ef4\u79bb\u6563token\u7a7a\u95f4\u4e2d\u9762\u4e34\u6837\u672c\u4f4e\u6548\u3001\u68af\u5ea6\u65b9\u5dee\u9ad8\u548c\u707e\u96be\u6027\u9057\u5fd8\u4e09\u5927\u74f6\u9888\u3002", "method": "\u6784\u5efa\u6f5c\u7a7a\u95f4\u53cc\u5411\u5bf9\u6bd4\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff1a\u4f7f\u7528\u8f7b\u91cf\u7ea7\u8f85\u52a9\u6a21\u578b\u5728\u6f5c\u7a7a\u95f4\u9ad8\u6548\u91c7\u6837K\u6761\u63a8\u7406\u94fe\u7f16\u7801\uff0c\u901a\u8fc7\u6b63\u786e\u6027\u548c\u683c\u5f0f\u53cc\u5956\u52b1\u673a\u5236\u7b5b\u9009\u9ad8\u8d28\u91cf\u6f5c\u8f68\u8ff9\uff0c\u8f93\u5165\u51bb\u7ed3\u7684\u4e3b\u6a21\u578b\u8fdb\u884c\u5355\u6b65\u89e3\u7801\uff1b\u8bbe\u8ba1\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u5b9e\u73b0\u6f5c\u7a7a\u95f4\u5b9a\u5411\u63a2\u7d22\uff0c\u51bb\u7ed3\u4e3b\u6a21\u578b\u53c2\u6570\u4ee5\u6570\u5b66\u4e0a\u6d88\u9664\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "\u5728\u540c\u7b49GPU\u8ba1\u7b97\u9884\u7b97\u4e0b\uff0cDLR\u5b9e\u73b0\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\u6536\u655b\u3001\u652f\u6301\u66f4\u957f\u5468\u671f\u63a8\u7406\u94fe\uff0c\u5e76\u80fd\u53ef\u6301\u7eed\u5730\u7d2f\u79ef\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6784\u5efa\u53ef\u9760\u53ef\u6269\u5c55\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u901a\u8fc7\u6f5c\u7a7a\u95f4\u4f18\u5316\u4ece\u6839\u672c\u4e0a\u89e3\u51b3\u4e86token\u7ea7RL\u7684\u7ed3\u6784\u6027\u7f3a\u9677\u3002"}}
{"id": "2601.18061", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.18061", "abs": "https://arxiv.org/abs/2601.18061", "authors": ["Kiana Jafari", "Paul Ulrich Nikolaus Rust", "Duncan Eddy", "Robbie Fraser", "Nina Vasan", "Darja Djordjevic", "Akanksha Dadlani", "Max Lamparth", "Eugenia Kim", "Mykel Kochenderfer"], "title": "Expert Evaluation and the Limits of Human Feedback in Mental Health AI Safety Testing", "comment": "17 pages, 7 pages of appendix, 21 tables", "summary": "Learning from human feedback~(LHF) assumes that expert judgments, appropriately aggregated, yield valid ground truth for training and evaluating AI systems. We tested this assumption in mental health, where high safety stakes make expert consensus essential. Three certified psychiatrists independently evaluated LLM-generated responses using a calibrated rubric. Despite similar training and shared instructions, inter-rater reliability was consistently poor ($ICC$ $0.087$--$0.295$), falling below thresholds considered acceptable for consequential assessment. Disagreement was highest on the most safety-critical items. Suicide and self-harm responses produced greater divergence than any other category, and was systematic rather than random. One factor yielded negative reliability (Krippendorff's $\u03b1= -0.203$), indicating structured disagreement worse than chance. Qualitative interviews revealed that disagreement reflects coherent but incompatible individual clinical frameworks, safety-first, engagement-centered, and culturally-informed orientations, rather than measurement error. By demonstrating that experts rely on holistic risk heuristics rather than granular factor discrimination, these findings suggest that aggregated labels function as arithmetic compromises that effectively erase grounded professional philosophies. Our results characterize expert disagreement in safety-critical AI as a sociotechnical phenomenon where professional experience introduces sophisticated layers of principled divergence. We discuss implications for reward modeling, safety classification, and evaluation benchmarks, recommending that practitioners shift from consensus-based aggregation to alignment methods that preserve and learn from expert disagreement.", "AI": {"tldr": "Expert disagreement in mental health AI evaluation is systematic and structured, not random, challenging the assumption that aggregated human feedback yields valid ground truth.", "motivation": "To test the core assumption of Learning from Human Feedback (LHF) that aggregated expert judgments provide valid ground truth, especially in high-stakes domains like mental health where safety is critical.", "method": "Three certified psychiatrists independently evaluated LLM-generated mental health responses using a calibrated rubric; inter-rater reliability was quantified, and qualitative interviews explored the reasons behind disagreement.", "result": "Inter-rater reliability was consistently poor, especially on safety-critical items like suicide/self-harm. Disagreement was systematic, reflecting incompatible clinical frameworks (safety-first, engagement-centered, culturally-informed), not measurement error.", "conclusion": "Aggregated labels erase professional philosophies; practitioners should shift from consensus-based aggregation to alignment methods that preserve and learn from principled expert disagreement."}}
{"id": "2601.17301", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17301", "abs": "https://arxiv.org/abs/2601.17301", "authors": ["Yunhui Liu", "Tieke He", "Yongchao Liu", "Can Yi", "Hong Jin", "Chuntao Hong"], "title": "Tabular Foundation Models are Strong Graph Anomaly Detectors", "comment": "Accepted by WWW 2026 (Short Paper)", "summary": "Graph anomaly detection (GAD), which aims to identify abnormal nodes that deviate from the majority, has become increasingly important in high-stakes Web domains. However, existing GAD methods follow a \"one model per dataset\" paradigm, leading to high computational costs, substantial data demands, and poor generalization when transferred to new datasets. This calls for a foundation model that enables a \"one-for-all\" GAD solution capable of detecting anomalies across diverse graphs without retraining. Yet, achieving this is challenging due to the large structural and feature heterogeneity across domains. In this paper, we propose TFM4GAD, a simple yet effective framework that adapts tabular foundation models (TFMs) for graph anomaly detection. Our key insight is that the core challenges of foundation GAD, handling heterogeneous features, generalizing across domains, and operating with scarce labels, are the exact problems that modern TFMs are designed to solve via synthetic pre-training and powerful in-context learning. The primary challenge thus becomes structural: TFMs are agnostic to graph topology. TFM4GAD bridges this gap by \"flattening\" the graph, constructing an augmented feature table that enriches raw node features with Laplacian embeddings, local and global structural characteristics, and anomaly-sensitive neighborhood aggregations. This augmented table is processed by a TFM in a fully in-context regime. Extensive experiments on multiple datasets with various TFM backbones reveal that TFM4GAD surprisingly achieves significant performance gains over specialized GAD models trained from scratch. Our work offers a new perspective and a practical paradigm for leveraging TFMs as powerful, generalist graph anomaly detectors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTFM4GAD\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u56fe\u7ed3\u6784\u5c55\u5e73\u4e3a\u589e\u5f3a\u7279\u5f81\u8868\u5e76\u9002\u914d\u8868\u683c\u57fa\u7840\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u91cd\u8bad\u7ec3\u5373\u53ef\u8de8\u591a\u4e2a\u56fe\u6570\u636e\u96c6\u68c0\u6d4b\u5f02\u5e38\u7684\u901a\u7528\u56fe\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4e13\u7528\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u56fe\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u91c7\u7528\"\u6bcf\u4e2a\u6570\u636e\u96c6\u4e00\u4e2a\u6a21\u578b\"\u7684\u8303\u5f0f\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u6570\u636e\u9700\u6c42\u5927\u3001\u8de8\u6570\u636e\u96c6\u8fc1\u79fb\u65f6\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u4e9f\u9700\u4e00\u79cd\u57fa\u7840\u6a21\u578b\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u56fe\u7ed3\u6784\u4e0a\u65e0\u9700\u91cd\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u901a\u7528\u68c0\u6d4b\u3002", "method": "TFM4GAD\u901a\u8fc7\"\u5c55\u5e73\"\u56fe\u7ed3\u6784\uff0c\u6784\u5efa\u5305\u542b\u62c9\u666e\u62c9\u65af\u5d4c\u5165\u3001\u5c40\u90e8\u4e0e\u5168\u5c40\u7ed3\u6784\u7279\u5f81\u4ee5\u53ca\u5f02\u5e38\u654f\u611f\u90bb\u57df\u805a\u5408\u7684\u589e\u5f3a\u7279\u5f81\u8868\uff0c\u5229\u7528\u8868\u683c\u57fa\u7840\u6a21\u578b\u7684\u5168\u4e0a\u4e0b\u6587\u5b66\u4e60\u673a\u5236\u8fdb\u884c\u5904\u7406\u3002", "result": "\u5728\u591a\u6570\u636e\u96c6\u548c\u591a\u79cd\u8868\u683c\u57fa\u7840\u6a21\u578b\u9aa8\u5e72\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cTFM4GAD\u76f8\u6bd4\u4ece\u96f6\u8bad\u7ec3\u7684\u4e13\u4e1a\u5316GAD\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5229\u7528\u8868\u683c\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u901a\u7528\u56fe\u5f02\u5e38\u68c0\u6d4b\u5668\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u548c\u5b9e\u7528\u8303\u5f0f\uff0c\u8bc1\u660e\u4e86\u8de8\u9886\u57df\u9002\u914d\u57fa\u7840\u6a21\u578b\u89e3\u51b3\u56fe\u95ee\u9898\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.18067", "categories": ["cs.AI", "cs.NE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.18067", "abs": "https://arxiv.org/abs/2601.18067", "authors": ["Wei-Po Hsin", "Ren-Hao Deng", "Yao-Ting Hsieh", "En-Ming Huang", "Shih-Hao Hung"], "title": "EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization", "comment": "17 pages, 6 figures, 8 tables", "summary": "Verilog's design cycle is inherently labor-intensive and necessitates extensive domain expertise. Although Large Language Models (LLMs) offer a promising pathway toward automation, their limited training data and intrinsic sequential reasoning fail to capture the strict formal logic and concurrency inherent in hardware systems. To overcome these barriers, we present EvolVE, the first framework to analyze multiple evolution strategies on chip design tasks, revealing that Monte Carlo Tree Search (MCTS) excels at maximizing functional correctness, while Idea-Guided Refinement (IGR) proves superior for optimization. We further leverage Structured Testbench Generation (STG) to accelerate the evolutionary process. To address the lack of complex optimization benchmarks, we introduce IC-RTL, targeting industry-scale problems derived from the National Integrated Circuit Contest. Evaluations establish EvolVE as the new state-of-the-art, achieving 98.1% on VerilogEval v2 and 92% on RTLLM v2. Furthermore, on the industry-scale IC-RTL suite, our framework surpasses reference implementations authored by contest participants, reducing the Power, Performance, Area (PPA) product by up to 66% in Huffman Coding and 17% in the geometric mean across all problems. The source code of the IC-RTL benchmark is available at https://github.com/weiber2002/ICRTL.", "AI": {"tldr": "This paper introduces EvolVE, a framework that combines multiple evolution strategies (MCTS for functional correctness, IGR for optimization) with Structured Testbench Generation to automate Verilog design, achieving new SOTA results and significant PPA improvements on industry-scale problems.", "motivation": "Verilog design cycles are labor-intensive and require extensive domain expertise; LLMs have limited training data and sequential reasoning that fails to capture hardware's strict formal logic and concurrency.", "method": "Proposes EvolVE framework analyzing multiple evolution strategies, finding MCTS excels at functional correctness while IGR is superior for optimization. Leverages Structured Testbench Generation (STG) to accelerate evolution and introduces IC-RTL benchmark from National Integrated Circuit Contest.", "result": "Achieves 98.1% on VerilogEval v2 and 92% on RTLLM v2. On IC-RTL suite, reduces PPA product by up to 66% for Huffman Coding and 17% geometric mean across all problems, surpassing contest participant implementations.", "conclusion": "EvolVE establishes new state-of-the-art for automated Verilog generation and optimization, significantly improving PPA metrics on industry-scale problems while providing a valuable benchmark for complex hardware design tasks."}}
{"id": "2601.18720", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.18720", "abs": "https://arxiv.org/abs/2601.18720", "authors": ["Sami Calvo"], "title": "On the Stochastic-Quantum Correspondence", "comment": "18 pages, no figures", "summary": "This paper aims to first explain, somewhat more clearly, the Stochastic-Quantum correspondence put forward in by Barandes in 2023. Specifically, the quantum-mechanical bra-ket notation is used, illuminating some results of previous results. With this, we prove the six axioms of textbook quantum mechanics from a single axiom: every physical system evolves according to a, generally indivisible, stochastic law. Afterwards, we generalise the treatment to continuous bases, which showcases a problem with them, indicating that space (and other physical variables) may be discrete in nature. Some concrete examples are also given, including the generalisation to classical and quantum fields. Then, we treat some practical issues of this new stochastic approach, regarding the solving of problems in physics, which turns out to still be most tractable in the traditional way. Finally, we explain the classical limit, where a system of many particles is found to behave classically according to Newton's second law. Along with that, we present a way of solving the measurement problem, characterising what is an environment and a measuring device and explaining how the wavefunction collapse comes about. Specifically, it is found that what distinguishes an environment is its number of degrees of freedom, while a measuring device is a low-entropy type of environment.", "AI": {"tldr": "\u672c\u6587\u5728Barandes 2023\u5e74\u968f\u673a-\u91cf\u5b50\u5bf9\u5e94\u57fa\u7840\u4e0a\uff0c\u5229\u7528\u72c4\u62c9\u514b\u7b26\u53f7\u6f84\u6e05\u7ed3\u679c\uff0c\u4ece\u5355\u4e00\u516c\u7406\uff08\u7269\u7406\u7cfb\u7edf\u9075\u5faa\u4e00\u822c\u4e0d\u53ef\u5206\u7684\u968f\u673a\u6f14\u5316\uff09\u63a8\u5bfc\u51fa\u6807\u51c6\u91cf\u5b50\u529b\u5b66\u7684\u516d\u4e2a\u516c\u7406\u3002\u63a8\u5e7f\u81f3\u8fde\u7eed\u57fa\u63ed\u793a\u4e86\u7a7a\u95f4\u53ef\u80fd\u79bb\u6563\u7684\u95ee\u9898\uff0c\u7ed9\u51fa\u7ecf\u5178/\u91cf\u5b50\u573a\u63a8\u5e7f\u5b9e\u4f8b\u3002\u8ba8\u8bba\u4e86\u5b9e\u7528\u8ba1\u7b97\u95ee\u9898\uff0c\u9610\u660e\u4e86\u591a\u7c92\u5b50\u7cfb\u7edf\u7ecf\u5178\u6781\u9650\u7b26\u5408\u725b\u987f\u7b2c\u4e8c\u5b9a\u5f8b\uff0c\u5e76\u57fa\u4e8e\u81ea\u7531\u5ea6\u6570\u91cf\u548c\u4f4e\u71b5\u7279\u6027\u533a\u5206\u73af\u5883\u4e0e\u6d4b\u91cf\u88c5\u7f6e\uff0c\u89e3\u91ca\u4e86\u6ce2\u51fd\u6570\u574d\u7f29\u673a\u5236\u3002", "motivation": "\u4e3a\u91cf\u5b50\u529b\u5b66\u5efa\u7acb\u66f4\u57fa\u7840\u7684\u968f\u673a\u7406\u8bba\u6846\u67b6\uff0c\u4ece\u5355\u4e00\u516c\u7406\u63a8\u5bfc\u6807\u51c6\u4f53\u7cfb\uff0c\u4ee5\u89e3\u51b3\u6d4b\u91cf\u95ee\u9898\u3001\u89e3\u91ca\u7ecf\u5178\u6781\u9650\uff0c\u5e76\u63a2\u8ba8\u65f6\u7a7a\u79bb\u6563\u6027\u3002", "method": "\u91c7\u7528\u7406\u8bba\u7269\u7406\u65b9\u6cd5\uff0c\u8fd0\u7528\u72c4\u62c9\u514b\u7b26\u53f7\u548c\u968f\u673a\u8fc7\u7a0b\u7406\u8bba\uff0c\u901a\u8fc7\u6570\u5b66\u63a8\u5bfc\u548c\u6982\u5ff5\u5206\u6790\uff0c\u5c06\u79bb\u6563\u7cfb\u7edf\u7684\u968f\u673a-\u91cf\u5b50\u5bf9\u5e94\u63a8\u5e7f\u81f3\u8fde\u7eed\u57fa\u548c\u573a\u8bba\u4f53\u7cfb\u3002", "result": "\u4ece\u5355\u4e00\u968f\u673a\u516c\u7406\u63a8\u5bfc\u51fa\u91cf\u5b50\u529b\u5b66\u7684\u516d\u4e2a\u6807\u51c6\u516c\u7406\uff1b\u8fde\u7eed\u57fa\u5904\u7406\u4e2d\u63ed\u793a\u51fa\u6697\u793a\u7a7a\u95f4\u79bb\u6563\u6027\u7684\u95ee\u9898\uff1b\u7ed9\u51fa\u7ecf\u5178\u548c\u91cf\u5b50\u573a\u8bba\u63a8\u5e7f\u7684\u5177\u4f53\u5b9e\u4f8b\uff1b\u9610\u660e\u591a\u7c92\u5b50\u7cfb\u7edf\u7ecf\u5178\u6781\u9650\u8d8b\u5411\u725b\u987f\u7b2c\u4e8c\u5b9a\u5f8b\uff1b\u63d0\u51fa\u4ee5\u81ea\u7531\u5ea6\u6570\u91cf\u548c\u4f4e\u71b5\u7279\u6027\u533a\u5206\u73af\u5883\u4e0e\u6d4b\u91cf\u88c5\u7f6e\uff0c\u5e76\u89e3\u91ca\u6ce2\u51fd\u6570\u574d\u7f29", "conclusion": "\u968f\u673a\u65b9\u6cd5\u4e3a\u91cf\u5b50\u529b\u5b66\u63d0\u4f9b\u4e86\u66f4\u57fa\u7840\u7684\u7406\u8bba\u57fa\u7840\uff0c\u80fd\u591f\u89e3\u91ca\u7ecf\u5178\u6781\u9650\u548c\u6d4b\u91cf\u95ee\u9898\uff0c\u6697\u793a\u65f6\u7a7a\u53ef\u80fd\u79bb\u6563\uff1b\u4f46\u5b9e\u9645\u8ba1\u7b97\u4ecd\u6700\u5b9c\u91c7\u7528\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2601.18119", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18119", "abs": "https://arxiv.org/abs/2601.18119", "authors": ["Jing Ye", "Yiwen Duan", "Yonghong Yu", "Victor Ma", "Yang Gao", "Xing Chen"], "title": "Beyond Text-to-SQL: Can LLMs Really Debug Enterprise ETL SQL?", "comment": null, "summary": "SQL is central to enterprise data engineering, yet generating fully correct SQL code in a single attempt remains difficult, even for experienced developers and advanced text-to-SQL LLMs, often requiring multiple debugging iterations. We introduce OurBench, the first benchmark for enterprise-level SQL reasoning and debugging. Our benchmark is built on two key innovations: (1) an automated construction workflow that uses reverse engineering to systematically inject realistic bugs into large-scale SQL code, enabling scalable and diverse benchmark generation; and (2) an execution-free evaluation framework tailored to enterprise settings, providing fast, accurate, and resource-efficient assessment.\n  OurBench comprises 469 OurBenchSyn queries featuring syntax errors with explicit error messages, and 516 OurBenchSem queries targeting semantic errors in which the code fails to meet user intent. The queries are highly complex, averaging over 140 lines and featuring deep and wide abstract syntax trees.\n  Evaluation of nearly 30 LLMs reveals a substantial performance gap: the best-performing model, Claude-4-Sonnet, achieves only 36.46 percent accuracy on OurBenchSyn and 32.17 percent on OurBenchSem, while most models score below 20 percent. We further explore four solution strategies, identify key challenges, and outline promising directions for enterprise SQL debugging with LLMs.", "AI": {"tldr": "The paper introduces OurBench, the first enterprise-level SQL debugging benchmark with 985 complex queries featuring syntax and semantic errors. It uses automated bug injection and execution-free evaluation, revealing that even top LLMs like Claude-4-Sonnet only achieve 32-36% accuracy, exposing a major performance gap.", "motivation": "SQL is critical for enterprise data engineering, but generating fully correct SQL code is difficult even for experienced developers and advanced LLMs, often requiring multiple debugging iterations. There is a lack of comprehensive benchmarks for evaluating SQL reasoning and debugging capabilities in enterprise settings.", "method": "The authors created OurBench with two key innovations: (1) an automated construction workflow that uses reverse engineering to systematically inject realistic bugs into large-scale SQL code for scalable benchmark generation; (2) an execution-free evaluation framework tailored to enterprise settings, providing fast, accurate, and resource-efficient assessment.", "result": "OurBench comprises 469 syntax error queries (OurBenchSyn) and 516 semantic error queries (OurBenchSem), averaging over 140 lines with deep/wide abstract syntax trees. Evaluation of nearly 30 LLMs shows a substantial performance gap: the best model (Claude-4-Sonnet) achieves only 36.46% accuracy on OurBenchSyn and 32.17% on OurBenchSem, while most models score below 20%.", "conclusion": "The significant performance gap demonstrates that current LLMs struggle with enterprise-level SQL debugging. The paper explores four solution strategies, identifies key challenges, and outlines promising research directions for improving LLM-based SQL debugging capabilities in enterprise contexts."}}
{"id": "2601.18743", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.18743", "abs": "https://arxiv.org/abs/2601.18743", "authors": ["Takeshi Kakizaki"], "title": "Approximate level-by-level maximum-likelihood decoding based on the Chase algorithm for high-rate concatenated stabilizer codes", "comment": "5 pages, 2 figure", "summary": "Fault-tolerant quantum computation (FTQC) is expected to address a wide range of computational problems. To realize large-scale FTQC, it is essential to encode logical qubits using quantum error-correcting codes. High-rate concatenated codes have recently attracted attention due to theoretical advances in fault-tolerant protocols with constant-space-overhead and polylogarithmic-time-overhead, as well as practical developments of high-rate many-hypercube codes equipped with a high-performance level-by-level minimum-distance decoder (LMDD). We propose a general, high-performance decoder for high-rate concatenated stabilizer codes that extends LMDD by leveraging the Chase algorithm to generate a suitable set of candidate errors. Our simulation results demonstrate that the proposed decoder outperforms conventional decoders for high-rate concatenated Hamming codes under bit-flip noise.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u9ad8\u7387\u7ea7\u8054\u91cf\u5b50\u7ea0\u9519\u7801\u89e3\u7801\u5668\uff0c\u901a\u8fc7Chase\u7b97\u6cd5\u6269\u5c55\u4e86\u9010\u7ea7\u6700\u5c0f\u8ddd\u79bb\u89e3\u7801\u5668(LMDD)\uff0c\u4eff\u771f\u663e\u793a\u5176\u5728\u6bd4\u7279\u7ffb\u8f6c\u566a\u58f0\u4e0b\u5bf9\u9ad8\u7387\u7ea7\u8054Hamming\u7801\u7684\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u89e3\u7801\u5668\u3002", "motivation": "\u4e3a\u5b9e\u73b0\u5927\u89c4\u6a21\u5bb9\u9519\u91cf\u5b50\u8ba1\u7b97\uff0c\u9700\u8981\u4f4e\u5f00\u9500\u7684\u91cf\u5b50\u7ea0\u9519\u7801\uff0c\u9ad8\u7387\u7ea7\u8054\u7801\u867d\u6709\u524d\u666f\u4f46\u9700\u6539\u8fdb\u89e3\u7801\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u901a\u7528\u7684\u9ad8\u6027\u80fd\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u5229\u7528Chase\u7b97\u6cd5\u751f\u6210\u5019\u9009\u9519\u8bef\u96c6\uff0c\u6269\u5c55\u4e86\u73b0\u6709\u7684\u9010\u7ea7\u6700\u5c0f\u8ddd\u79bb\u89e3\u7801\u5668(LMDD)\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u5728\u6bd4\u7279\u7ffb\u8f6c\u566a\u58f0\u6a21\u578b\u4e0b\uff0c\u8be5\u89e3\u7801\u5668\u5bf9\u9ad8\u7387\u7ea7\u8054Hamming\u7801\u7684\u89e3\u7801\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u89e3\u7801\u5668\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684Chase\u589e\u5f3a\u578bLMDD\u89e3\u7801\u5668\u63d0\u5347\u4e86\u9ad8\u7387\u7ea7\u8054\u7801\u7684\u5b9e\u7528\u6027\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u5927\u89c4\u6a21\u5bb9\u9519\u91cf\u5b50\u8ba1\u7b97\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.18123", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18123", "abs": "https://arxiv.org/abs/2601.18123", "authors": ["Muhammad Ibrahim Khan", "Bivin Pradeep", "James Brusey"], "title": "Deadline-Aware, Energy-Efficient Control of Domestic Immersion Hot Water Heaters", "comment": "Accepted at AAAI 2026", "summary": "Typical domestic immersion water heater systems are often operated continuously during winter, heating quickly rather than efficiently and ignoring predictable demand windows and ambient losses. We study deadline-aware control, where the aim is to reach a target temperature at a specified time while minimising energy consumption. We introduce an efficient Gymnasium environment that models an immersion hot water heater with first-order thermal losses and discrete on and off actions of 0 W and 6000 W applied every 120 seconds. Methods include a time-optimal bang-bang baseline, a zero-shot Monte Carlo Tree Search planner, and a Proximal Policy Optimisation policy. We report total energy consumption in watt-hours under identical physical dynamics. Across sweeps of initial temperature from 10 to 30 degrees Celsius, deadline from 30 to 90 steps, and target temperature from 40 to 80 degrees Celsius, PPO achieves the most energy-efficient performance at a 60-step horizon of 2 hours, using 3.23 kilowatt-hours, compared to 4.37 to 10.45 kilowatt-hours for bang-bang control and 4.18 to 6.46 kilowatt-hours for MCTS. This corresponds to energy savings of 26 percent at 30 steps and 69 percent at 90 steps. In a representative trajectory with a 50 kg water mass, 20 degrees Celsius ambient temperature, and a 60 degrees Celsius target, PPO consumes 54 percent less energy than bang-bang control and 33 percent less than MCTS. These results show that learned deadline-aware control reduces energy consumption under identical physical assumptions, while planners provide partial savings without training and learned policies offer near-zero inference cost once trained.", "AI": {"tldr": "This paper develops a reinforcement learning approach (PPO) for deadline-aware control of immersion water heaters, achieving 26-69% energy savings compared to traditional bang-bang control by optimizing when to heat water to reach target temperature efficiently.", "motivation": "Domestic immersion water heaters operate continuously and inefficiently in winter, heating quickly rather than efficiently while ignoring predictable demand windows and ambient thermal losses. The goal is to minimize energy consumption while ensuring water reaches a target temperature by a specified deadline.", "method": "Created a Gymnasium simulation environment modeling an immersion heater with first-order thermal losses and discrete on/off actions (0W/6000W at 120-second intervals). Compared three methods: a time-optimal bang-bang baseline, zero-shot Monte Carlo Tree Search (MCTS) planning, and a learned Proximal Policy Optimization (PPO) policy. Evaluated across parameter sweeps of initial temperature (10-30\u00b0C), deadline (30-90 steps), and target temperature (40-80\u00b0C).", "result": "PPO achieved the best energy efficiency, consuming 3.23 kWh at a 60-step (2-hour) horizon compared to 4.37-10.45 kWh for bang-bang control and 4.18-6.46 kWh for MCTS. This represents 26% energy savings at 30 steps and 69% at 90 steps versus bang-bang. In a representative test case (50 kg water, 20\u00b0C ambient, 60\u00b0C target), PPO used 54% less energy than bang-bang and 33% less than MCTS.", "conclusion": "Learned deadline-aware control policies significantly reduce energy consumption under identical physical dynamics. While MCTS planners provide partial savings without training, PPO offers superior performance with near-zero inference cost once trained, demonstrating the effectiveness of reinforcement learning for efficient thermal energy management."}}
{"id": "2601.17309", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17309", "abs": "https://arxiv.org/abs/2601.17309", "authors": ["Anagha Sabu", "Vidhya S", "Narayanan C Krishnan"], "title": "PAR: Plausibility-aware Amortized Recourse Generation", "comment": null, "summary": "Algorithmic recourse aims to recommend actionable changes to a factual's attributes that flip an unfavorable model decision while remaining realistic and feasible. We formulate recourse as a Constrained Maximum A-Posteriori (MAP) inference problem under the accepted-class data distribution seeking counterfactuals with high likelihood while respecting other recourse constraints. We present PAR, an amortized approximate inference procedure that generates highly likely recourses efficiently. Recourse likelihood is estimated directly using tractable probabilistic models that admit exact likelihood evaluation and efficient gradient propagation that is useful during training. The recourse generator is trained with the objective of maximizing the likelihood under the accepted-class distribution while minimizing the likelihood under the denied-class distribution and other losses that encode recourse constraints. Furthermore, PAR includes a neighborhood-based conditioning mechanism to promote recourse generation that is customized to a factual. We validate PAR on widely used algorithmic recourse datasets and demonstrate its efficiency in generating recourses that are valid, similar to the factual, sparse, and highly plausible, yielding superior performance over existing state-of-the-art approaches.", "AI": {"tldr": "PAR is an amortized inference method for algorithmic recourse that generates high-quality counterfactuals by maximizing accepted-class likelihood while respecting constraints, outperforming existing methods.", "motivation": "Develop realistic and feasible algorithmic recourse by formulating it as Constrained MAP inference to find high-likelihood counterfactuals under accepted-class distribution.", "method": "PAR uses tractable probabilistic models for amortized inference, trained to maximize accepted-class likelihood, minimize denied-class likelihood, and satisfy constraints, with neighborhood conditioning for customization.", "result": "PAR generates valid, similar, sparse, and plausible recourses efficiently, achieving superior performance on standard datasets.", "conclusion": "PAR successfully produces high-quality algorithmic recourses meeting multiple criteria efficiently, surpassing state-of-the-art approaches."}}
{"id": "2601.18130", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18130", "abs": "https://arxiv.org/abs/2601.18130", "authors": ["Jize Wang", "Han Wu", "Zhiyuan You", "Yiming Song", "Yijun Wang", "Zifei Shan", "Yining Li", "Songyang Zhang", "Xinyi Le", "Cailian Chen", "Xinping Guan", "Dacheng Tao"], "title": "RouteMoA: Dynamic Routing without Pre-Inference Boosts Efficient Mixture-of-Agents", "comment": null, "summary": "Mixture-of-Agents (MoA) improves LLM performance through layered collaboration, but its dense topology raises costs and latency. Existing methods employ LLM judges to filter responses, yet still require all models to perform inference before judging, failing to cut costs effectively. They also lack model selection criteria and struggle with large model pools, where full inference is costly and can exceed context limits. To address this, we propose RouteMoA, an efficient mixture-of-agents framework with dynamic routing. It employs a lightweight scorer to perform initial screening by predicting coarse-grained performance from the query, narrowing candidates to a high-potential subset without inference. A mixture of judges then refines these scores through lightweight self- and cross-assessment based on existing model outputs, providing posterior correction without additional inference. Finally, a model ranking mechanism selects models by balancing performance, cost, and latency. RouteMoA outperforms MoA across varying tasks and model pool sizes, reducing cost by 89.8% and latency by 63.6% in the large-scale model pool.", "AI": {"tldr": "RouteMoA is a dynamic routing framework for Mixture-of-Agents that uses lightweight scoring and judging to avoid full inference, reducing cost by 89.8% and latency by 63.6% while maintaining performance.", "motivation": "The dense topology of traditional Mixture-of-Agents architectures leads to high computational costs and latency. Existing methods still require all models to run inference before filtering and lack effective model selection criteria, making them inefficient for large model pools.", "method": "RouteMoA employs a three-stage approach: (1) a lightweight scorer predicts model performance from queries to narrow candidates without inference; (2) a mixture of judges refines scores through lightweight self- and cross-assessment using existing outputs; (3) a model ranking mechanism selects optimal models by balancing performance, cost, and latency.", "result": "RouteMoA outperforms traditional MoA across diverse tasks and model pool sizes, achieving 89.8% cost reduction and 63.6% latency reduction in large-scale scenarios.", "conclusion": "RouteMoA provides an efficient dynamic routing solution for MoA that dramatically reduces computational overhead while maintaining superior performance, making it practical for large-scale deployment."}}
{"id": "2601.18767", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.18767", "abs": "https://arxiv.org/abs/2601.18767", "authors": ["Martina Nibbi", "Filippo Della Chiara", "Yizhi Shen", "Aaron Szasz", "Roel Van Beeumen"], "title": "Practical block encodings of matrix polynomials that can also be trivially controlled", "comment": "23 pages, 10 figures, 3 tables", "summary": "Quantum circuits naturally implement unitary operations on input quantum states. However, non-unitary operations can also be implemented through block encodings, where additional ancilla qubits are introduced and later measured. While block encoding has a number of well-established theoretical applications, its practical implementation has been prohibitively expensive for current quantum hardware. In this paper, we present practical and explicit block encoding circuits implementing matrix polynomial transformations of a target matrix. With standard approaches, block-encoding a degree-$d$ matrix polynomial requires a circuit depth scaling as $d$ times the depth for block-encoding the original matrix alone. By leveraging the recently introduced Fast One-Qubit Controlled Select LCU (FOQCS-LCU) framework, we show that the additional circuit-depth overhead required for encoding matrix polynomials can be reduced to scale linearly in $d$ with no dependence on system size or the cost of block encoding the original matrix. Moreover, we demonstrate that the FOQCS-LCU circuits and their associated matrix polynomial transformations can be controlled with negligible overhead, enabling efficient applications such as Hadamard tests. Finally, we provide explicit circuits for representative spin models, together with detailed non-asymptotic gate counts and circuit depths.", "AI": {"tldr": "This paper introduces the FOQCS-LCU framework to efficiently implement matrix polynomial block encodings on quantum computers, reducing circuit depth overhead from O(d) to linear in polynomial degree d while maintaining practical implementability for near-term hardware.", "motivation": "Block encoding theoretically enables non-unitary operations but suffers from prohibitive circuit depth scaling (O(d)) for degree-d matrix polynomials in practical quantum hardware implementations, limiting its utility for algorithms requiring polynomial transformations.", "method": "Leveraging the Fast One-Qubit Controlled Select LCU (FOQCS-LCU) framework to construct explicit block encoding circuits for matrix polynomials, eliminating dependence on system size and original matrix encoding cost in the overhead.", "result": "Achieves circuit depth scaling linearly with polynomial degree d (vs. previous O(d) scaling), negligible control overhead for applications like Hadamard tests, and provides explicit spin model circuits with non-asymptotic gate counts.", "conclusion": "FOQCS-LCU enables practical implementation of matrix polynomial transformations on current quantum hardware by drastically reducing depth overhead, facilitating efficient quantum algorithms requiring non-unitary operations."}}
{"id": "2601.17329", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17329", "abs": "https://arxiv.org/abs/2601.17329", "authors": ["Tiejin Chen", "Xiaoou Liu", "Vishnu Nandam", "Kuan-Ru Liou", "Hua Wei"], "title": "Conformal Feedback Alignment: Quantifying Answer-Level Reliability for Robust LLM Alignment", "comment": "Accetped to Findings of EACL", "summary": "Preference-based alignment like Reinforcement Learning from Human Feedback (RLHF) learns from pairwise preferences, yet the labels are often noisy and inconsistent. Existing uncertainty-aware approaches weight preferences, but ignore a more fundamental factor: the reliability of the \\emph{answers} being compared. To address the problem, we propose Conformal Feedback Alignment (CFA), a framework that grounds preference weighting in the statistical guarantees of Conformal Prediction (CP). CFA quantifies answer-level reliability by constructing conformal prediction sets with controllable coverage and aggregates these reliabilities into principled weights for both DPO- and PPO-style training. Experiments across different datasets show that CFA improves alignment robustness and data efficiency, highlighting that modeling \\emph{answer-side} uncertainty complements preference-level weighting and yields more robust, data-efficient alignment. Codes are provided here.", "AI": {"tldr": "\u63d0\u51faCFA\u6846\u67b6\uff0c\u5229\u7528conformal prediction\u91cf\u5316\u7b54\u6848\u53ef\u9760\u6027\u5e76\u52a0\u6743\u504f\u597d\uff0c\u63d0\u5347LLM\u5bf9\u9f50\u7684\u9c81\u68d2\u6027\u548c\u6570\u636e\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u504f\u597d\u7684\u5bf9\u9f50\u65b9\u6cd5\uff08\u5982RLHF\uff09\u9762\u4e34\u6807\u7b7e\u566a\u58f0\u548c\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u73b0\u6709\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u65b9\u6cd5\u4ec5\u5bf9\u504f\u597d\u52a0\u6743\uff0c\u5374\u5ffd\u7565\u4e86\u88ab\u6bd4\u8f83\u7b54\u6848\u672c\u8eab\u7684\u53ef\u9760\u6027\u8fd9\u4e00\u66f4\u6839\u672c\u56e0\u7d20\u3002", "method": "\u63d0\u51faConformal Feedback Alignment (CFA)\u6846\u67b6\uff1a\u901a\u8fc7conformal prediction\u6784\u5efa\u53ef\u63a7\u8986\u76d6\u7387\u7684\u9884\u6d4b\u96c6\u6765\u91cf\u5316\u7b54\u6848\u7ea7\u53ef\u9760\u6027\uff0c\u5e76\u5c06\u8fd9\u4e9b\u53ef\u9760\u6027\u805a\u5408\u4e3a\u539f\u5219\u6027\u6743\u91cd\uff0c\u5e94\u7528\u4e8eDPO\u548cPPO\u5f0f\u8bad\u7ec3\u3002", "result": "\u8de8\u6570\u636e\u96c6\u5b9e\u9a8c\u8868\u660e\uff0cCFA\u63d0\u5347\u4e86alignment\u7684\u9c81\u68d2\u6027\u548c\u6570\u636e\u6548\u7387\u3002", "conclusion": "\u5efa\u6a21\u7b54\u6848\u7aef\u4e0d\u786e\u5b9a\u6027\u53ef\u8865\u5145\u504f\u597d\u7ea7\u52a0\u6743\uff0c\u5b9e\u73b0\u66f4\u9c81\u68d2\u3001\u6570\u636e\u9ad8\u6548\u7684\u5bf9\u9f50\u3002"}}
{"id": "2601.18132", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18132", "abs": "https://arxiv.org/abs/2601.18132", "authors": ["Xi Chen", "Hongru Zhou", "Huahui Yi", "Shiyu Feng", "Hanyu Zhou", "Tiancheng He", "Mingke You", "Li Wang", "Qiankun Li", "Kun Wang", "Weili Fu", "Kang Li", "Jian Li"], "title": "RareAlert: Aligning heterogeneous large language model reasoning for early rare disease risk screening", "comment": "28 page, 3 figures", "summary": "Missed and delayed diagnosis remains a major challenge in rare disease care. At the initial clinical encounters, physicians assess rare disease risk using only limited information under high uncertainty. When high-risk patients are not recognised at this stage, targeted diagnostic testing is often not initiated, resulting in missed diagnosis. Existing primary care triage processes are structurally insufficient to reliably identify patients with rare diseases at initial clinical presentation and universal screening is needed to reduce diagnostic delay. Here we present RareAlert, an early screening system which predict patient-level rare disease risk from routinely available primary-visit information. RareAlert integrates reasoning generated by ten LLMs, calibrates and weights these signals using machine learning, and distils the aligned reasoning into a single locally deployable model. To develop and evaluate RareAlert, we curated RareBench, a real-world dataset of 158,666 cases covering 33 Orphanet disease categories and more than 7,000 rare conditions, including both rare and non-rare presentations. The results showed that rare disease identification can be reconceptualised as a universal uncertainty resolution process applied to the general patient population. On an independent test set, RareAlert, a Qwen3-4B based model trained with calibrated reasoning signals, achieved an AUC of 0.917, outperforming the best machine learning ensemble and all evaluated LLMs, including GPT-5, DeepSeek-R1, Claude-3.7-Sonnet, o3-mini, Gemini-2.5-Pro, and Qwen3-235B. These findings demonstrate the diversity in LLM medical reasoning and the effectiveness of aligning such reasoning in highly uncertain clinical tasks. By incorporating calibrated reasoning into a single model, RareAlert enables accurate, privacy-preserving, and scalable rare disease risk screening suitable for large-scale local deployment.", "AI": {"tldr": "\u5f00\u53d1RareAlert\u7cfb\u7edf\uff0c\u901a\u8fc7\u96c6\u6210\u548c\u6821\u51c610\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u80fd\u4ece\u672a\u89c1\u4fe1\u606f\u4e2d\u9884\u6d4b\u7f55\u89c1\u75c5\u98ce\u9669\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u5728\u5305\u542b15.8\u4e07\u75c5\u4f8b\u7684\u6570\u636e\u96c6\u4e0aAUC\u8fbe0.917\uff0c\u6027\u80fd\u8d85\u8d8a\u5305\u62ecGPT-5\u5728\u5185\u7684\u6240\u6709\u5bf9\u6bd4\u6a21\u578b\u3002", "motivation": "\u7f55\u89c1\u75c5\u6f0f\u8bca\u548c\u5ef6\u8fdf\u8bca\u65ad\u662f\u91cd\u5927\u533b\u7597\u6311\u6218\uff0c\u73b0\u6709\u521d\u7ea7\u8bca\u7597\u5206\u8bca\u6d41\u7a0b\u65e0\u6cd5\u5728\u521d\u6b21\u5c31\u8bca\u65f6\u53ef\u9760\u8bc6\u522b\u9ad8\u98ce\u9669\u60a3\u8005\uff0c\u9700\u8981\u666e\u7b5b\u6765\u51cf\u5c11\u8bca\u65ad\u5ef6\u8fdf\u3002", "method": "\u63d0\u51faRareAlert\u7cfb\u7edf\uff0c\u6574\u540810\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u7ed3\u679c\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u8fdb\u884c\u6821\u51c6\u548c\u52a0\u6743\uff0c\u84b8\u998f\u6210\u5355\u4e2a\u53ef\u672c\u5730\u90e8\u7f72\u7684Qwen3-4B\u6a21\u578b\uff1b\u4f7f\u7528\u5305\u542b158,666\u4f8b\u75c5\u4f8b\u3001\u8986\u76d633\u4e2a\u5b64\u513f\u75be\u75c5\u7c7b\u522b\u548c7,000\u591a\u79cd\u7f55\u89c1\u75c5\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6RareBench\u8fdb\u884c\u5f00\u53d1\u8bc4\u4f30\u3002", "result": "\u5728\u72ec\u7acb\u6d4b\u8bd5\u96c6\u4e0aAUC\u8fbe\u52300.917\uff0c\u663e\u8457\u4f18\u4e8e\u6700\u4f73\u673a\u5668\u5b66\u4e60\u96c6\u6210\u6a21\u578b\u548c\u6240\u6709\u8bc4\u4f30\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5305\u62ecGPT-5\u3001DeepSeek-R1\u3001Claude-3.7-Sonnet\u7b49\uff09\uff0c\u8bc1\u660e\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u533b\u7597\u63a8\u7406\u7684\u591a\u6837\u6027\u4ee5\u53ca\u6821\u51c6\u5bf9\u9f50\u5728\u9ad8\u5ea6\u4e0d\u786e\u5b9a\u4e34\u5e8a\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "RareAlert\u5b9e\u73b0\u4e86\u51c6\u786e\u3001\u9690\u79c1\u4fdd\u62a4\u4e14\u53ef\u6269\u5c55\u7684\u7f55\u89c1\u75c5\u98ce\u9669\u7b5b\u67e5\uff0c\u9002\u5408\u5927\u89c4\u6a21\u672c\u5730\u90e8\u7f72\uff0c\u4e3a\u7f55\u89c1\u75c5\u65e9\u671f\u8bc6\u522b\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18773", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.18773", "abs": "https://arxiv.org/abs/2601.18773", "authors": ["Kaifeng Bu", "Weichen Gu", "Xiang Li"], "title": "Hamiltonian Decoded Quantum Interferometry for General Pauli Hamiltonians", "comment": "28 pages", "summary": "In this work, we study the Hamiltonian Decoded Quantum Interferometry (HDQI) for the general Hamiltonians $H=\\sum_ic_iP_i$ on an $n$-qubit system, where the coefficients $c_i\\in \\mathbb{R}$ and $P_i$ are Pauli operators. We show that, given access to an appropriate decoding oracle, there exist efficient quantum algorithms for preparing the state $\u03c1_{\\mathcal P}(H) = \\frac{\\mathcal P^2(H)}{\\text{Tr}[\\mathcal P^2(H)]}$, where $\\mathcal P(H)$ denotes the matrix function induced by a univariate polynomial $\\mathcal P(x)$. Such states can be used to approximate the Gibbs states of $H$ for suitable choices of polynomials. We further demonstrate that the proposed algorithms are robust to imperfections in the decoding procedure. Our results substantially extend the scope of HDQI beyond stabilizer-like Hamiltonians, providing a method for Gibbs-state preparation and Hamiltonian optimization in a broad class of physically and computationally relevant quantum systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u54c8\u5bc6\u987f\u89e3\u7801\u91cf\u5b50\u5e72\u6d89\u6280\u672f(HDQI)\u63a8\u5e7f\u81f3\u4e00\u822c\u54c8\u5bc6\u987f\u91cf\uff0c\u5229\u7528\u89e3\u7801\u9884\u8a00\u673a\u9ad8\u6548\u5236\u5907\u591a\u9879\u5f0f\u51fd\u6570\u6001\u4ee5\u8fd1\u4f3c\u5409\u5e03\u65af\u6001\uff0c\u5e76\u5bf9\u89e3\u7801\u8bef\u5dee\u5177\u6709\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edfHDQI\u5c40\u9650\u4e8e\u7c7b\u7a33\u5b9a\u5b50\u54c8\u5bc6\u987f\u91cf\uff0c\u65e0\u6cd5\u6ee1\u8db3\u66f4\u5e7f\u6cdb\u7269\u7406\u548c\u8ba1\u7b97\u76f8\u5173\u91cf\u5b50\u7cfb\u7edf\u7684\u9700\u6c42\uff0c\u4e9f\u9700\u6269\u5c55\u81f3\u4e00\u822c\u54c8\u5bc6\u987f\u91cf\u7684\u5409\u5e03\u65af\u6001\u5236\u5907\u65b9\u6cd5\u3002", "method": "\u9488\u5bf9n\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u4e2dH=\u03a3c_iP_i\u5f62\u5f0f\u7684\u4e00\u822c\u54c8\u5bc6\u987f\u91cf\uff0c\u5f00\u53d1\u57fa\u4e8e\u89e3\u7801\u9884\u8a00\u673a\u7684\u91cf\u5b50\u7b97\u6cd5\uff0c\u5236\u5907\u03c1_P(H) = P\u00b2(H)/Tr[P\u00b2(H)]\u6001\uff0c\u5176\u4e2dP(x)\u4e3a\u5355\u53d8\u91cf\u591a\u9879\u5f0f\u3002", "result": "\u8bc1\u660e\u5728\u5408\u9002\u89e3\u7801\u9884\u8a00\u673a\u4e0b\uff0c\u5b58\u5728\u9ad8\u6548\u7b97\u6cd5\u5236\u5907\u6b64\u7c7b\u591a\u9879\u5f0f\u6001\uff0c\u53ef\u901a\u8fc7\u9009\u62e9\u9002\u5f53\u591a\u9879\u5f0f\u903c\u8fd1\u5409\u5e03\u65af\u6001\uff1b\u7b97\u6cd5\u5bf9\u89e3\u7801\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u5b8c\u7f8e\u6027\u5177\u6709\u9c81\u68d2\u6027\uff0c\u663e\u8457\u6269\u5c55\u4e86HDQI\u7684\u9002\u7528\u8303\u56f4\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5e7f\u6cdb\u7684\u7269\u7406\u548c\u8ba1\u7b97\u76f8\u5173\u91cf\u5b50\u7cfb\u7edf\u63d0\u4f9b\u4e86\u901a\u7528\u7684\u5409\u5e03\u65af\u6001\u5236\u5907\u4e0e\u54c8\u5bc6\u987f\u4f18\u5316\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u91cf\u5b50\u5e72\u6d89\u6280\u672f\u7684\u5b9e\u7528\u5316\u53d1\u5c55\u3002"}}
{"id": "2601.17330", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17330", "abs": "https://arxiv.org/abs/2601.17330", "authors": ["Laurent Caraffa"], "title": "Thermodynamically Optimal Regularization under Information-Geometric Constraints", "comment": "7 pages, 0 figures", "summary": "Modern machine learning relies on a collection of empirically successful but theoretically heterogeneous regularization techniques, such as weight decay, dropout, and exponential moving averages. At the same time, the rapidly increasing energetic cost of training large models raises the question of whether learning algorithms approach any fundamental efficiency bound. In this work, we propose a unifying theoretical framework connecting thermodynamic optimality, information geometry, and regularization.\n  Under three explicit assumptions -- (A1) that optimality requires an intrinsic, parametrization-invariant measure of information, (A2) that belief states are modeled by maximum-entropy distributions under known constraints, and (A3) that optimal processes are quasi-static -- we prove a conditional optimality theorem. Specifically, the Fisher--Rao metric is the unique admissible geometry on belief space, and thermodynamically optimal regularization corresponds to minimizing squared Fisher--Rao distance to a reference state.\n  We derive the induced geometries for Gaussian and circular belief models, yielding hyperbolic and von Mises manifolds, respectively, and show that classical regularization schemes are structurally incapable of guaranteeing thermodynamic optimality. We introduce a notion of thermodynamic efficiency of learning and propose experimentally testable predictions. This work provides a principled geometric and thermodynamic foundation for regularization in machine learning.", "AI": {"tldr": "This paper develops a unified theoretical framework linking thermodynamic optimality, information geometry, and regularization in machine learning, proving that the Fisher-Rao metric uniquely defines optimal geometry and revealing that classical regularization methods cannot guarantee thermodynamic efficiency.", "motivation": "Modern machine learning relies on heterogeneous regularization techniques (weight decay, dropout, etc.) while facing rising energy costs in training large models, raising questions about fundamental efficiency bounds in learning algorithms.", "method": "The authors propose a theoretical framework based on three assumptions: intrinsic parametrization-invariant information measure, maximum-entropy belief distributions, and quasi-static optimal processes. They prove a conditional optimality theorem connecting Fisher-Rao metric with thermodynamic optimal regularization.", "result": "The Fisher-Rao metric is the unique admissible geometry on belief space; thermodynamically optimal regularization minimizes squared Fisher-Rao distance to a reference state. For Gaussian and circular belief models, this yields hyperbolic and von Mises manifolds. Classical regularization schemes are structurally incapable of guaranteeing thermodynamic optimality.", "conclusion": "This work establishes a principled geometric and thermodynamic foundation for regularization in machine learning and proposes experimentally testable predictions for thermodynamic efficiency of learning."}}
{"id": "2601.18137", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18137", "abs": "https://arxiv.org/abs/2601.18137", "authors": ["Yinger Zhang", "Shutong Jiang", "Renhao Li", "Jianhong Tu", "Yang Su", "Lianghao Deng", "Xudong Guo", "Chenxu Lv", "Junyang Lin"], "title": "DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints", "comment": null, "summary": "While agent evaluation has shifted toward long-horizon tasks, most benchmarks still emphasize local, step-level reasoning rather than the global constrained optimization (e.g., time and financial budgets) that demands genuine planning ability. Meanwhile, existing LLM planning benchmarks underrepresent the active information gathering and fine-grained local constraints typical of real-world settings. To address this, we introduce DeepPlanning, a challenging benchmark for practical long-horizon agent planning. It features multi-day travel planning and multi-product shopping tasks that require proactive information acquisition, local constrained reasoning, and global constrained optimization. Evaluations on DeepPlanning show that even frontier agentic LLMs struggle with these problems, highlighting the importance of reliable explicit reasoning patterns and parallel tool use for achieving better effectiveness-efficiency trade-offs. Error analysis further points to promising directions for improving agentic LLMs over long planning horizons. We open-source the code and data to support future research.", "AI": {"tldr": "The paper introduces DeepPlanning, a benchmark for evaluating long-horizon agent planning with real-world constraints, showing that current LLMs struggle with these tasks and highlighting areas for improvement.", "motivation": "Existing LLM planning benchmarks focus on local reasoning rather than global constrained optimization and lack real-world elements like active information gathering and fine-grained local constraints.", "method": "Introduced DeepPlanning benchmark with multi-day travel and multi-product shopping tasks requiring proactive information acquisition, local constrained reasoning, and global optimization.", "result": "Frontier agentic LLMs struggle on DeepPlanning; explicit reasoning and parallel tool use are important for effectiveness-efficiency trade-offs.", "conclusion": "Error analysis identifies improvement directions for long-horizon agentic LLMs, and code/data are open-sourced for future research."}}
{"id": "2601.17334", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17334", "abs": "https://arxiv.org/abs/2601.17334", "authors": ["Yufeng Huang"], "title": "Power-based Partial Attention: Bridging Linear-Complexity and Full Attention", "comment": "12 pages, 3 figures", "summary": "It is widely accepted from transformer research that \"attention is all we need\", but the amount of attention required has never been systematically quantified. Is quadratic $O(L^2)$ attention necessary, or is there a sub-quadratic attention mechanism that can achieve comparable performance? To answer this question, we introduce power-based partial attention (PPA), an attention mechanism of order $O(L^{1+p})$, where $0 \\leq p \\leq 1$, such that $p=0$ corresponds to sliding window attention with linear complexity, and $p=1$ corresponds to full attention. With this attention construction, we can explore how transformer architecture performance varies as a function of the attention scaling behavior controlled by $p$. The overall trend from our experiments shows an S-curve-like behavior where the performance transitions from sliding-window (linear-complexity) attention to full attention over a narrow window of $p$ values, and plateaus as $p$ approaches $1$. In our experiments, we show that there exists $0<p<1$ such that $O(L^{1+p})$ attention is sufficient to achieve similar results as $O(L^2)$ full attention.", "AI": {"tldr": "\u63d0\u51fa\u5e42\u6b21\u90e8\u5206\u6ce8\u610f\u529b(PPA)\u673a\u5236\uff0c\u8bc1\u660e\u57280<p<1\u65f6O(L^(1+p))\u6b21\u6ce8\u610f\u529b\u53ef\u8fbe\u5230\u4e0e\u5168\u6ce8\u610f\u529bO(L\u00b2)\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u8868\u660e\u4e8c\u6b21\u6ce8\u610f\u529b\u53ef\u80fd\u5e76\u975e\u5fc5\u9700\u3002", "motivation": "\u73b0\u6709Transformer\u7814\u7a76\u666e\u904d\u8ba4\u4e3a\"\u6ce8\u610f\u529b\u5c31\u662f\u4e00\u5207\"\uff0c\u4f46\u4ece\u672a\u7cfb\u7edf\u91cf\u5316\u6240\u9700\u6ce8\u610f\u529b\u7684\u8ba1\u7b97\u91cf\u3002\u4e8c\u6b21O(L\u00b2)\u6ce8\u610f\u529b\u662f\u5426\u5fc5\u8981\uff0c\u662f\u5426\u5b58\u5728\u53ef\u8fbe\u5230\u76f8\u5f53\u6027\u80fd\u7684\u6b21\u4e8c\u6b21\u6ce8\u610f\u529b\u673a\u5236\uff1f", "method": "\u5f15\u5165\u5e42\u6b21\u90e8\u5206\u6ce8\u610f\u529b(PPA)\uff0c\u5176\u590d\u6742\u5ea6\u4e3aO(L^(1+p))\uff0c\u5176\u4e2d0\u2264p\u22641\u3002p=0\u5bf9\u5e94\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\uff0cp=1\u5bf9\u5e94\u5168\u6ce8\u610f\u529b\u3002\u901a\u8fc7\u8c03\u8282p\u503c\u63a2\u7d22Transformer\u6027\u80fd\u968f\u6ce8\u610f\u529b\u7f29\u653e\u884c\u4e3a\u7684\u53d8\u5316\u3002", "result": "\u5b9e\u9a8c\u663e\u793aS\u578b\u66f2\u7ebf\u884c\u4e3a\uff1a\u6027\u80fd\u5728p\u7684\u7a84\u7a97\u53e3\u5185\u4ece\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u8fc7\u6e21\u5230\u5168\u6ce8\u610f\u529b\uff0c\u5e76\u5728p\u63a5\u8fd11\u65f6\u8d8b\u4e8e\u5e73\u7a33\u3002\u5b58\u57280<p<1\u4f7f\u5f97O(L^(1+p))\u6ce8\u610f\u529b\u8db3\u4ee5\u8fbe\u5230\u4e0eO(L\u00b2)\u5168\u6ce8\u610f\u529b\u76f8\u4f3c\u7684\u7ed3\u679c\u3002", "conclusion": "\u6b21\u4e8c\u6b21\u6ce8\u610f\u529b\u673a\u5236\u80fd\u591f\u8fbe\u5230\u4e0e\u5168\u4e8c\u6b21\u6ce8\u610f\u529b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u8868\u660e\u4e8c\u6b21\u6ce8\u610f\u529b\u53ef\u80fd\u5e76\u975e\u4e25\u683c\u5fc5\u9700\u3002"}}
{"id": "2601.18175", "categories": ["cs.AI", "cs.LG", "eess.SY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.18175", "abs": "https://arxiv.org/abs/2601.18175", "authors": ["Daniel Russo"], "title": "Success Conditioning as Policy Improvement: The Optimization Problem Solved by Imitating Success", "comment": null, "summary": "A widely used technique for improving policies is success conditioning, in which one collects trajectories, identifies those that achieve a desired outcome, and updates the policy to imitate the actions taken along successful trajectories. This principle appears under many names -- rejection sampling with SFT, goal-conditioned RL, Decision Transformers -- yet what optimization problem it solves, if any, has remained unclear. We prove that success conditioning exactly solves a trust-region optimization problem, maximizing policy improvement subject to a $\u03c7^2$ divergence constraint whose radius is determined automatically by the data. This yields an identity: relative policy improvement, the magnitude of policy change, and a quantity we call action-influence -- measuring how random variation in action choices affects success rates -- are exactly equal at every state. Success conditioning thus emerges as a conservative improvement operator. Exact success conditioning cannot degrade performance or induce dangerous distribution shift, but when it fails, it does so observably, by hardly changing the policy at all. We apply our theory to the common practice of return thresholding, showing this can amplify improvement, but at the cost of potential misalignment with the true objective.", "AI": {"tldr": "Success conditioning is theoretically proven to be a trust-region optimization method that safely improves policies without performance degradation, with practical implications for techniques like return thresholding.", "motivation": "Understanding what optimization problem success conditioning (a widely used technique for policy improvement that imitates successful trajectories) actually solves, despite being known by many names like rejection sampling with SFT, goal-conditioned RL, and Decision Transformers.", "method": "Proving that success conditioning exactly solves a trust-region optimization problem, maximizing policy improvement subject to a \u03c7\u00b2 divergence constraint with data-determined radius, leading to an identity connecting relative policy improvement, policy change magnitude, and action-influence.", "result": "Success conditioning emerges as a conservative improvement operator that cannot degrade performance or cause dangerous distribution shift; when it fails, it does so observably by barely changing the policy. Applied to return thresholding, it shows amplified improvement potential but with misalignment risks.", "conclusion": "Success conditioning has a solid theoretical foundation as a safe, conservative policy improvement method, and the analysis clarifies trade-offs in practical implementations."}}
{"id": "2601.18197", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18197", "abs": "https://arxiv.org/abs/2601.18197", "authors": ["Shaokang Wang", "Pei Fu", "Ruoceng Zhang", "Shaojie Zhang", "Xiuwen Xi", "Jiahui Yang", "Bin Qin", "Ying Huang", "Zhenbo Luo", "Jian Luan"], "title": "GAIA: A Data Flywheel System for Training GUI Test-Time Scaling Critic Models", "comment": null, "summary": "While Large Vision-Language Models (LVLMs) have significantly advanced GUI agents' capabilities in parsing textual instructions, interpreting screen content, and executing tasks, a critical challenge persists: the irreversibility of agent operations, where a single erroneous action can trigger catastrophic deviations. To address this, we propose the GUI Action Critic's Data Flywheel System (GAIA), a training framework that enables the models to have iterative critic capabilities, which are used to improve the Test-Time Scaling (TTS) of basic GUI agents' performance. Specifically, we train an Intuitive Critic Model (ICM) using positive and negative action examples from a base agent first. This critic evaluates the immediate correctness of the agent's intended actions, thereby selecting operations with higher success probability. Then, the initial critic guides agent actions to collect refined positive/negative samples, initiating the self-improving cycle. The augmented data then trains a second-round critic with enhanced discernment capability. We conduct experiments on various datasets and demonstrate that the proposed ICM can improve the test-time performance of various closed-source and open-source models, and the performance can be gradually improved as the data is recycled. The code and dataset will be publicly released.", "AI": {"tldr": "\u9488\u5bf9GUI agent\u64cd\u4f5c\u4e0d\u53ef\u9006\u5bfc\u81f4\u9519\u8bef\u653e\u5927\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faGAIA\u6570\u636e\u98de\u8f6e\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bad\u7ec3\u76f4\u89c9\u6279\u8bc4\u6a21\u578b(ICM)\u5b9e\u65f6\u8bc4\u4f30\u52a8\u4f5c\u6b63\u786e\u6027\uff0c\u6784\u5efa\u81ea\u6211\u6539\u8fdb\u5faa\u73af\uff0c\u5728\u5404\u7c7b\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u6d4b\u8bd5\u65f6\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(LVLMs)\u663e\u8457\u63d0\u5347\u4e86GUI agent\u7684\u80fd\u529b\uff0c\u4f46\u64cd\u4f5c\u4e0d\u53ef\u9006\u6027\u4ecd\u662f\u5173\u952e\u6311\u6218\u2014\u2014\u5355\u4e2a\u9519\u8bef\u52a8\u4f5c\u5373\u53ef\u5f15\u53d1\u707e\u96be\u6027\u504f\u5dee\uff0c\u4e9f\u9700\u63d0\u5347agent\u7684\u53ef\u9760\u6027\u548c\u6d4b\u8bd5\u65f6\u6027\u80fd\u3002", "method": "\u63d0\u51faGUI Action Critic's Data Flywheel System (GAIA)\uff0c\u5148\u57fa\u4e8e\u6b63\u8d1f\u6837\u4f8b\u8bad\u7ec3\u76f4\u89c9\u6279\u8bc4\u6a21\u578b(ICM)\u8bc4\u4f30agent\u52a8\u4f5c\u7684\u5373\u65f6\u6b63\u786e\u6027\uff0c\u7b5b\u9009\u9ad8\u6210\u529f\u7387\u64cd\u4f5c\uff1b\u518d\u7528\u521d\u59cb\u6279\u8bc4\u6a21\u578b\u6307\u5bfcagent\u6536\u96c6\u7cbe\u70bc\u6837\u672c\uff0c\u5f62\u6210\u6570\u636e\u98de\u8f6e\uff0c\u8fed\u4ee3\u8bad\u7ec3\u66f4\u5f3a\u5927\u7684\u6279\u8bc4\u6a21\u578b\u3002", "result": "\u5728\u591a\u6570\u636e\u96c6\u5b9e\u9a8c\u8868\u660e\uff0cICM\u80fd\u6709\u6548\u63d0\u5347\u95ed\u6e90\u548c\u5f00\u6e90\u6a21\u578b\u7684\u6d4b\u8bd5\u65f6\u6027\u80fd\uff0c\u4e14\u968f\u7740\u6570\u636e\u5faa\u73af\u5229\u7528\uff0c\u6027\u80fd\u53ef\u9010\u6b65\u63d0\u5347\u3002", "conclusion": "GAIA\u6846\u67b6\u901a\u8fc7\u6784\u5efa\u81ea\u6211\u6539\u8fdb\u7684\u6279\u8bc4\u6a21\u578b\u5faa\u73af\uff0c\u4e3aGUI agent\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6d4b\u8bd5\u65f6\u6027\u80fd\u63d0\u5347\u65b9\u6848\uff0c\u589e\u5f3a\u4e86\u64cd\u4f5c\u53ef\u9760\u6027\u3002"}}
{"id": "2601.17360", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.17360", "abs": "https://arxiv.org/abs/2601.17360", "authors": ["Jiankai Jin", "Xiangzheng Zhang", "Zhao Liu", "Deyue Zhang", "Quanchen Zou"], "title": "Robust Privacy: Inference-Time Privacy through Certified Robustness", "comment": null, "summary": "Machine learning systems can produce personalized outputs that allow an adversary to infer sensitive input attributes at inference time. We introduce Robust Privacy (RP), an inference-time privacy notion inspired by certified robustness: if a model's prediction is provably invariant within a radius-$R$ neighborhood around an input $x$ (e.g., under the $\\ell_2$ norm), then $x$ enjoys $R$-Robust Privacy, i.e., observing the prediction cannot distinguish $x$ from any input within distance $R$ of $x$. We further develop Attribute Privacy Enhancement (APE) to translate input-level invariance into an attribute-level privacy effect. In a controlled recommendation task where the decision depends primarily on a sensitive attribute, we show that RP expands the set of sensitive-attribute values compatible with a positive recommendation, expanding the inference interval accordingly. Finally, we empirically demonstrate that RP also mitigates model inversion attacks (MIAs) by masking fine-grained input-output dependence. Even at small noise levels ($\u03c3=0.1$), RP reduces the attack success rate (ASR) from 73% to 4% with partial model performance degradation. RP can also partially mitigate MIAs (e.g., ASR drops to 44%) with no model performance degradation.", "AI": {"tldr": "\u63d0\u51faRobust Privacy (RP)\u4f5c\u4e3a\u63a8\u7406\u65f6\u9690\u79c1\u4fdd\u62a4\u7684\u65b0\u6982\u5ff5\uff0c\u57fa\u4e8e\u8ba4\u8bc1\u9c81\u68d2\u6027\uff0c\u786e\u4fdd\u6a21\u578b\u9884\u6d4b\u5728\u8f93\u5165\u90bb\u57df\u5185\u4e0d\u53d8\uff0c\u4ece\u800c\u9632\u6b62\u63a8\u65ad\u654f\u611f\u5c5e\u6027\uff0c\u5e76\u901a\u8fc7Attribute Privacy Enhancement (APE)\u589e\u5f3a\u5c5e\u6027\u7ea7\u9690\u79c1\uff0c\u6709\u6548\u7f13\u89e3\u6a21\u578b\u53cd\u6f14\u653b\u51fb\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u7684\u4e2a\u6027\u5316\u8f93\u51fa\u53ef\u80fd\u5bfc\u81f4\u5bf9\u624b\u5728\u63a8\u7406\u65f6\u63a8\u65ad\u654f\u611f\u8f93\u5165\u5c5e\u6027\uff0c\u73b0\u6709\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u63a8\u7406\u65f6\u9690\u79c1\u4fdd\u62a4\u6982\u5ff5\u3002", "method": "\u5f15\u5165Robust Privacy (RP)\u6982\u5ff5\uff0c\u5b9a\u4e49\u6a21\u578b\u9884\u6d4b\u5728\u534a\u5f84\u4e3aR\u7684\u8f93\u5165\u90bb\u57df\u5185\uff08\u5982\u21132\u8303\u6570\uff09\u5177\u6709\u4e0d\u53d8\u6027\uff0c\u5219\u8be5\u8f93\u5165\u4eab\u6709R-\u9c81\u68d2\u9690\u79c1\uff1b\u5f00\u53d1Attribute Privacy Enhancement (APE)\u5c06\u8f93\u5165\u7ea7\u4e0d\u53d8\u6027\u8f6c\u5316\u4e3a\u5c5e\u6027\u7ea7\u9690\u79c1\u6548\u679c\uff0c\u5e76\u5728\u63a8\u8350\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u3002", "result": "\u5728\u63a8\u8350\u4efb\u52a1\u4e2d\uff0cRP\u6269\u5c55\u4e86\u4e0e\u6b63\u5411\u63a8\u8350\u517c\u5bb9\u7684\u654f\u611f\u5c5e\u6027\u503c\u8303\u56f4\uff1b\u5b9e\u8bc1\u663e\u793aRP\u663e\u8457\u7f13\u89e3\u6a21\u578b\u53cd\u6f14\u653b\u51fb\uff0c\u5c0f\u566a\u58f0\uff08\u03c3=0.1\uff09\u4e0b\u653b\u51fb\u6210\u529f\u7387\u4ece73%\u964d\u81f34%\uff0c\u4e14\u53ef\u65e0\u6027\u80fd\u635f\u5931\u5730\u5c06\u653b\u51fb\u6210\u529f\u7387\u964d\u81f344%\u3002", "conclusion": "RP\u901a\u8fc7\u5f3a\u5236\u5c40\u90e8\u8f93\u5165\u4e0d\u53d8\u6027\uff0c\u6709\u6548\u9632\u6b62\u654f\u611f\u5c5e\u6027\u63a8\u65ad\u548c\u6a21\u578b\u53cd\u6f14\u653b\u51fb\uff0c\u4e3a\u63a8\u7406\u65f6\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u4e14\u6027\u80fd\u635f\u5931\u53ef\u63a7\u3002"}}
{"id": "2601.17376", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17376", "abs": "https://arxiv.org/abs/2601.17376", "authors": ["Ruijin Hua", "Zichuan Liu", "Kun Zhang", "Yiyuan Yang"], "title": "Diversified Scaling Inference in Time Series Foundation Models", "comment": "23 pages, 16 figures, 9 tables", "summary": "The advancement of Time Series Foundation Models (TSFMs) has been driven primarily by large-scale pre-training, but inference-time compute potential remains largely untapped. This work systematically investigates two questions: how do TSFMs behave under standard sampling-based inference scaling, and can controlled sampling diversity enhance performance? We first examine the properties of TSFMs under standard sampling often fail to adhere to scaling laws due to insufficient exploration of the solution space. Building on this, we then delve into diversified inference scaling via tailored time series perturbations to expand the generative distribution's support. We theoretically analyze the diversity-fidelity trade-off and derive a critical sample threshold for diversified sampling to outperform standard sampling. Extensive experiments across various TSFMs and datasets show proper diversified inference scaling yields substantial performance gains without parameter updates, establishing inference design as a critical, compute-efficient dimension of TSFM optimization. As an application, we propose RobustMSE, a rigorous metric to quantify the headroom performance of TSFM under a fixed budget. Overall, our findings clarify these factor interactions, enabling reliable performance via diverse large-scale inference time series in parallel environments without re-training TSFMs.", "AI": {"tldr": "This paper investigates inference-time compute for Time Series Foundation Models (TSFMs), finding that standard sampling fails due to poor solution space exploration. They propose diversified inference scaling via time series perturbations, theoretically derive a critical sample threshold, and show substantial performance gains without parameter updates, establishing inference design as a compute-efficient optimization dimension.", "motivation": "The advancement of TSFMs has focused on large-scale pre-training while largely ignoring inference-time compute potential. Standard sampling-based inference scaling doesn't work well for TSFMs because they fail to explore the solution space sufficiently, violating scaling laws.", "method": "Systematically examine TSFM behavior under standard sampling, then propose diversified inference scaling using tailored time series perturbations. Theoretically analyze the diversity-fidelity trade-off and derive a critical sample threshold. Conduct extensive experiments across various TSFMs and datasets, and propose RobustMSE as a metric for performance headroom under fixed budget.", "result": "Proper diversified inference scaling yields substantial performance gains without parameter updates. The theoretical analysis provides a critical sample threshold that determines when diversified sampling outperforms standard sampling. Inference design is established as a critical, compute-efficient dimension of TSFM optimization.", "conclusion": "Findings clarify interactions between factors in TSFM inference, enabling reliable performance via diverse large-scale inference time series in parallel environments without re-training. This opens up a new dimension for optimizing TSFMs efficiently at inference time."}}
{"id": "2601.18217", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18217", "abs": "https://arxiv.org/abs/2601.18217", "authors": ["Zhihan Liu", "Lin Guan", "Yixin Nie", "Kai Zhang", "Zhuoqun Hao", "Lin Chen", "Asli Celikyilmaz", "Zhaoran Wang", "Na Zhang"], "title": "Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents", "comment": null, "summary": "Generalist LLM agents are often post-trained on a narrow set of environments but deployed across far broader, unseen domains. In this work, we investigate the challenge of agentic post-training when the eventual test domains are unknown. Specifically, we analyze which properties of reinforcement learning (RL) environments and modeling choices have the greatest influence on out-of-domain performance. First, we identify two environment axes that strongly correlate with cross-domain generalization: (i) state information richness, i.e., the amount of information for the agent to process from the state, and (ii) planning complexity, estimated via goal reachability and trajectory length under a base policy. Notably, domain realism and text-level similarity are not the primary factors; for instance, the simple grid-world domain Sokoban leads to even stronger generalization in SciWorld than the more realistic ALFWorld. Motivated by these findings, we further show that increasing state information richness alone can already effectively improve cross-domain robustness. We propose a randomization technique, which is low-overhead and broadly applicable: add small amounts of distractive goal-irrelevant features to the state to make it richer without altering the task. Beyond environment-side properties, we also examine several modeling choices: (a) SFT warmup or mid-training helps prevent catastrophic forgetting during RL but undermines generalization to domains that are not included in the mid-training datamix; and (b) turning on step-by-step thinking during RL, while not always improving in-domain performance, plays a crucial role in preserving generalization.", "AI": {"tldr": "This paper investigates how to improve LLM agents' generalization to unseen domains by analyzing RL environment properties and modeling choices, finding that state information richness and planning complexity are key factors, and proposing a simple randomization technique to enhance robustness.", "motivation": "Generalist LLM agents are trained on narrow environments but deployed across broader, unseen domains, creating a critical generalization gap that needs addressing when test domains are unknown.", "method": "The authors analyzed how different RL environment properties (state information richness, planning complexity, domain realism, text similarity) and modeling choices (SFT warmup, step-by-step thinking) affect cross-domain generalization of LLM agents.", "result": "State information richness and planning complexity strongly correlate with generalization, while domain realism and text similarity are not primary factors. Adding distractive goal-irrelevant features improves robustness, and step-by-step thinking during RL preserves generalization, whereas SFT mid-training can undermine it.", "conclusion": "To improve LLM agent generalization to unseen domains, prioritize increasing state information richness through techniques like distractive features and enable step-by-step thinking during RL, rather than relying on domain realism or SFT warmup."}}
{"id": "2601.17396", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17396", "abs": "https://arxiv.org/abs/2601.17396", "authors": ["Vashista Nobaub"], "title": "GO-OSC and VASH: Geometry-Aware Representation Learning for Early Degradation Detection in Oscillatory Systems", "comment": "21 pages, 5 figures. Includes theoretical analysis, ablation studies, and experiments on synthetic and real vibration datasets. Code available", "summary": "Early-stage degradation in oscillatory systems often manifests as geometric distortions of the dynamics, such as phase jitter, frequency drift, or loss of coherence, long before changes in signal energy are detectable. In this regime, classical energy-based diagnostics and unconstrained learned representations are structurally insensitive, leading to delayed or unstable detection. We introduce GO-OSC, a geometry-aware representation learning framework for oscillatory time series that enforces a canonical and identifiable latent parameterization, enabling stable comparison and aggregation across short, unlabeled windows. Building on this representation, we define a family of invariant linear geometric probes that target degradation-relevant directions in latent space. We provide theoretical results showing that under early phase-only degradation, energy-based statistics have zero first-order detection power, whereas geometric probes achieve strictly positive sensitivity. Our analysis characterizes when and why linear probing fails under non-identifiable representations and shows how canonicalization restores statistical detectability. Experiments on synthetic benchmarks and real vibration datasets validate the theory, demonstrating earlier detection, improved data efficiency, and robustness to operating condition changes.", "AI": {"tldr": "\u63d0\u51faGO-OSC\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u7684\u8868\u793a\u5b66\u4e60\u5b9e\u73b0\u632f\u8361\u7cfb\u7edf\u65e9\u671f\u9000\u5316\u68c0\u6d4b\uff0c\u8bc1\u660e\u80fd\u91cf\u57fa\u65b9\u6cd5\u5728\u76f8\u4f4d\u9000\u5316\u4e0b\u65e0\u6548\u800c\u51e0\u4f55\u63a2\u9488\u654f\u611f", "motivation": "\u632f\u8361\u7cfb\u7edf\u65e9\u671f\u9000\u5316\u8868\u73b0\u4e3a\u76f8\u4f4d\u6296\u52a8\u3001\u9891\u7387\u6f02\u79fb\u7b49\u51e0\u4f55\u5931\u771f\uff0c\u5728\u4fe1\u53f7\u80fd\u91cf\u53d8\u5316\u53ef\u68c0\u6d4b\u524d\u5c31\u5df2\u53d1\u751f\u3002\u4f20\u7edf\u80fd\u91cf\u57fa\u8bca\u65ad\u548c\u65e0\u7ea6\u675f\u5b66\u4e60\u8868\u793a\u5bf9\u6b64\u4e0d\u654f\u611f\uff0c\u5bfc\u81f4\u68c0\u6d4b\u5ef6\u8fdf\u6216\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51faGO-OSC\u51e0\u4f55\u611f\u77e5\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u5f3a\u5236\u89c4\u8303\u53ef\u8bc6\u522b\u7684\u6f5c\u5728\u53c2\u6570\u5316\uff1b\u6784\u5efa\u4e0d\u53d8\u7ebf\u6027\u51e0\u4f55\u63a2\u9488\u9776\u5411\u6f5c\u5728\u7a7a\u95f4\u4e2d\u9000\u5316\u76f8\u5173\u65b9\u5411\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u5728\u76f8\u4f4d\u9000\u5316\u4e0b\u80fd\u91cf\u57fa\u7edf\u8ba1\u91cf\u96f6\u9636\u68c0\u6d4b\u80fd\u529b\u4e3a\u96f6\uff0c\u800c\u51e0\u4f55\u63a2\u9488\u5177\u6709\u6b63\u654f\u611f\u6027\uff1b\u5408\u6210\u548c\u771f\u5b9e\u632f\u52a8\u6570\u636e\u96c6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u66f4\u65e9\u68c0\u6d4b\u3001\u66f4\u9ad8\u6570\u636e\u6548\u7387\u548c\u5de5\u51b5\u53d8\u5316\u9c81\u68d2\u6027\u3002", "conclusion": "\u89c4\u8303\u53ef\u8bc6\u522b\u8868\u793a\u80fd\u901a\u8fc7\u51e0\u4f55\u63a2\u9488\u5b9e\u73b0\u65e9\u671f\u9000\u5316\u68c0\u6d4b\uff0c\u89e3\u51b3\u4e86\u975e\u53ef\u8bc6\u522b\u8868\u793a\u4e0b\u7ebf\u6027\u63a2\u6d4b\u5931\u6548\u95ee\u9898\uff0c\u4e3a\u632f\u8361\u7cfb\u7edf\u5065\u5eb7\u76d1\u6d4b\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2601.18225", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18225", "abs": "https://arxiv.org/abs/2601.18225", "authors": ["Pei Wang", "Yanan Wu", "Xiaoshuai Song", "Weixun Wang", "Gengru Chen", "Zhongwen Li", "Kezhong Yan", "Ken Deng", "Qi Liu", "Shuaibing Zhao", "Shaopan Xiong", "Xuepeng Liu", "Xuefeng Chen", "Wanxi Deng", "Wenbo Su", "Bo Zheng"], "title": "ShopSimulator: Evaluating and Exploring RL-Driven LLM Agent for Shopping Assistants", "comment": null, "summary": "Large language model (LLM)-based agents are increasingly deployed in e-commerce shopping. To perform thorough, user-tailored product searches, agents should interpret personal preferences, engage in multi-turn dialogues, and ultimately retrieve and discriminate among highly similar products. However, existing research has yet to provide a unified simulation environment that consistently captures all of these aspects, and always focuses solely on evaluation benchmarks without training support. In this paper, we introduce ShopSimulator, a large-scale and challenging Chinese shopping environment. Leveraging ShopSimulator, we evaluate LLMs across diverse scenarios, finding that even the best-performing models achieve less than 40% full-success rate. Error analysis reveals that agents struggle with deep search and product selection in long trajectories, fail to balance the use of personalization cues, and to effectively engage with users. Further training exploration provides practical guidance for overcoming these weaknesses, with the combination of supervised fine-tuning (SFT) and reinforcement learning (RL) yielding significant performance improvements. Code and data will be released at https://github.com/ShopAgent-Team/ShopSimulator.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faShopSimulator\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u4e2d\u6587\u8d2d\u7269\u73af\u5883\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u3002\u7814\u7a76\u53d1\u73b0\u5373\u4f7f\u6700\u4f73\u6a21\u578b\u5b8c\u6574\u6210\u529f\u7387\u4e5f\u4f4e\u4e8e40%\uff0c\u4e3b\u8981\u5f31\u70b9\u5728\u4e8e\u957f\u8f68\u8ff9\u641c\u7d22\u3001\u4e2a\u6027\u5316\u7ebf\u7d22\u5e73\u8861\u548c\u7528\u6237\u4e92\u52a8\uff0c\u4f46\u76d1\u7763\u5fae\u8c03(SFT)\u4e0e\u5f3a\u5316\u5b66\u4e60(RL)\u7ed3\u5408\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u80fd\u5168\u9762\u6355\u6349\u7535\u5546\u8d2d\u7269\u6240\u6709\u73af\u8282\uff08\u4e2a\u6027\u5316\u504f\u597d\u7406\u89e3\u3001\u591a\u8f6e\u5bf9\u8bdd\u3001\u76f8\u4f3c\u4ea7\u54c1\u533a\u5206\uff09\u7684\u7edf\u4e00\u6a21\u62df\u73af\u5883\uff0c\u4e14\u4ec5\u5173\u6ce8\u8bc4\u4f30\u800c\u5ffd\u89c6\u8bad\u7ec3\u652f\u6301\u3002", "method": "\u5f00\u53d1\u5927\u89c4\u6a21\u4e2d\u6587\u8d2d\u7269\u73af\u5883ShopSimulator\uff1b\u5728\u591a\u79cd\u573a\u666f\u4e0b\u8bc4\u4f30LLM\uff1b\u8fdb\u884c\u9519\u8bef\u5206\u6790\uff1b\u63a2\u7d22\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u5f3a\u5316\u5b66\u4e60(RL)\u7b49\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u6700\u4f73\u6a21\u578b\u5b8c\u6574\u6210\u529f\u7387\u4e0d\u8db340%\uff1b\u8bc6\u522b\u51fa\u957f\u8f68\u8ff9\u641c\u7d22\u4e0e\u4ea7\u54c1\u9009\u62e9\u3001\u4e2a\u6027\u5316\u7ebf\u7d22\u4f7f\u7528\u5931\u8861\u3001\u7528\u6237\u4e92\u52a8\u4e0d\u8db3\u7b49\u5173\u952e\u5f31\u70b9\uff1bSFT\u4e0eRL\u7ed3\u5408\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "ShopSimulator\u4e3a\u7535\u5546LLM\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u5c40\u9650\u6027\uff0c\u5e76\u8bc1\u660e\u9002\u5f53\u7684\u8bad\u7ec3\u7b56\u7565\u53ef\u5927\u5e45\u63d0\u5347\u667a\u80fd\u4f53\u80fd\u529b\u3002"}}
{"id": "2601.17407", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17407", "abs": "https://arxiv.org/abs/2601.17407", "authors": ["Prajwal Chauhan", "Salah Eddine Choutri", "Saif Eddin Jabari"], "title": "Efficient Dilated Squeeze and Excitation Neural Operator for Differential Equations", "comment": "Accepted to Transactions on Machine Learning Research (TMLR)", "summary": "Fast and accurate surrogates for physics-driven partial differential equations (PDEs) are essential in fields such as aerodynamics, porous media design, and flow control. However, many transformer-based models and existing neural operators remain parameter-heavy, resulting in costly training and sluggish deployment. We propose D-SENO (Dilated Squeeze-Excitation Neural Operator), a lightweight operator learning framework for efficiently solving a wide range of PDEs, including airfoil potential flow, Darcy flow in porous media, pipe Poiseuille flow, and incompressible Navier Stokes vortical fields. D-SENO combines dilated convolution (DC) blocks with squeeze-and-excitation (SE) modules to jointly capture wide receptive fields and dynamics alongside channel-wise attention, enabling both accurate and efficient PDE inference. Carefully chosen dilation rates allow the receptive field to focus on critical regions, effectively modeling long-range physical dependencies. Meanwhile, the SE modules adaptively recalibrate feature channels to emphasize dynamically relevant scales. Our model achieves training speed of up to approximately $20\\times$ faster than standard transformer-based models and neural operators, while also surpassing (or matching) them in accuracy across multiple PDE benchmarks. Ablation studies show that removing the SE modules leads to a slight drop in performance.", "AI": {"tldr": "D-SENO is a lightweight neural operator combining dilated convolutions and squeeze-excitation modules that achieves 20x faster training than transformers while maintaining accuracy on various PDE problems.", "motivation": "Physics-driven PDE surrogates are crucial in aerodynamics, porous media, and flow control, but existing transformer-based and neural operator models are parameter-heavy, making training costly and deployment slow.", "method": "D-SENO integrates dilated convolution blocks with squeeze-and-excitation modules to capture wide receptive fields and channel-wise attention for solving diverse PDEs including airfoil flow, Darcy flow, Poiseuille flow, and Navier-Stokes equations.", "result": "The model achieves ~20x faster training speed compared to standard transformer-based models while matching or surpassing their accuracy across multiple PDE benchmarks; removing SE modules causes a slight performance drop.", "conclusion": "D-SENO provides an accurate and efficient framework for PDE inference, demonstrating that careful design of dilation rates and channel attention mechanisms enables lightweight yet high-performing neural operators."}}
{"id": "2601.18226", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18226", "abs": "https://arxiv.org/abs/2601.18226", "authors": ["Haotian Li", "Shijun Yang", "Weizhen Qi", "Silei Zhao", "Rui Hua", "Mingzhu Song", "Xiaojian Yang", "Chao Peng"], "title": "Yunjue Agent Tech Report: A Fully Reproducible, Zero-Start In-Situ Self-Evolving Agent System for Open-Ended Tasks", "comment": null, "summary": "Conventional agent systems often struggle in open-ended environments where task distributions continuously drift and external supervision is scarce. Their reliance on static toolsets or offline training lags behind these dynamics, leaving the system's capability boundaries rigid and unknown. To address this, we propose the In-Situ Self-Evolving paradigm. This approach treats sequential task interactions as a continuous stream of experience, enabling the system to distill short-term execution feedback into long-term, reusable capabilities without access to ground-truth labels. Within this framework, we identify tool evolution as the critical pathway for capability expansion, which provides verifiable, binary feedback signals. Within this framework, we develop Yunjue Agent, a system that iteratively synthesizes, optimizes, and reuses tools to navigate emerging challenges. To optimize evolutionary efficiency, we further introduce a Parallel Batch Evolution strategy. Empirical evaluations across five diverse benchmarks under a zero-start setting demonstrate significant performance gains over proprietary baselines. Additionally, complementary warm-start evaluations confirm that the accumulated general knowledge can be seamlessly transferred to novel domains. Finally, we propose a novel metric to monitor evolution convergence, serving as a function analogous to training loss in conventional optimization. We open-source our codebase, system traces, and evolved tools to facilitate future research in resilient, self-evolving intelligence.", "AI": {"tldr": "\u63d0\u51fa\u539f\u4f4d\u81ea\u8fdb\u5316\u8303\u5f0f\uff0c\u901a\u8fc7\u5de5\u5177\u6f14\u5316\u5b9e\u73b0\u667a\u80fd\u4f53\u5728\u5f00\u653e\u73af\u5883\u4e2d\u7684\u6301\u7eed\u5b66\u4e60\uff0cYunjue Agent\u5728\u96f6\u6837\u672c\u548c\u8fc1\u79fb\u8bbe\u7f6e\u4e0b\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5f00\u6e90\u76f8\u5173\u8d44\u6e90\u3002", "motivation": "\u4f20\u7edf\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u4efb\u52a1\u5206\u5e03\u6301\u7eed\u6f02\u79fb\u4e14\u5916\u90e8\u76d1\u7763\u7a00\u7f3a\u7684\u5f00\u653e\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5176\u4f9d\u8d56\u9759\u6001\u5de5\u5177\u96c6\u6216\u79bb\u7ebf\u8bad\u7ec3\u5bfc\u81f4\u80fd\u529b\u8fb9\u754c\u50f5\u5316\uff0c\u65e0\u6cd5\u52a8\u6001\u9002\u5e94\u65b0\u6311\u6218\u3002", "method": "\u63d0\u51fa\u539f\u4f4d\u81ea\u8fdb\u5316\u8303\u5f0f\uff0c\u5c06\u5e8f\u5217\u4efb\u52a1\u4ea4\u4e92\u89c6\u4e3a\u8fde\u7eed\u7ecf\u9a8c\u6d41\uff0c\u901a\u8fc7\u5de5\u5177\u6f14\u5316\uff08\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u4e8c\u5143\u53cd\u9988\uff09\u5c06\u77ed\u671f\u6267\u884c\u53cd\u9988\u63d0\u70bc\u4e3a\u957f\u671f\u53ef\u590d\u7528\u80fd\u529b\uff0c\u5f00\u53d1Yunjue Agent\u5e76\u5f15\u5165\u5e76\u884c\u6279\u91cf\u6f14\u5316\u7b56\u7565\u4f18\u5316\u6548\u7387\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u7684\u96f6\u542f\u52a8\u8bbe\u7f6e\u4e0b\u6027\u80fd\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\uff1b\u77e5\u8bc6\u53ef\u65e0\u7f1d\u8fc1\u79fb\u81f3\u65b0\u9886\u57df\uff1b\u63d0\u51fa\u7c7b\u4f3c\u8bad\u7ec3\u635f\u5931\u7684\u65b0\u6307\u6807\u76d1\u63a7\u6f14\u5316\u6536\u655b\u3002", "conclusion": "\u5f00\u6e90\u4ee3\u7801\u5e93\u3001\u7cfb\u7edf\u8f68\u8ff9\u548c\u6f14\u5316\u5de5\u5177\uff0c\u4fc3\u8fdb\u672a\u6765\u5bf9\u5f39\u6027\u3001\u81ea\u8fdb\u5316\u667a\u80fd\u7684\u7814\u7a76\u3002"}}
{"id": "2601.17430", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17430", "abs": "https://arxiv.org/abs/2601.17430", "authors": ["Zichuan Yang", "Yiming Xing"], "title": "Active Hypothesis Testing for Correlated Combinatorial Anomaly Detection", "comment": "47 pages, 26 figures", "summary": "We study the problem of identifying an anomalous subset of streams under correlated noise, motivated by monitoring and security in cyber-physical systems. This problem can be viewed as a form of combinatorial pure exploration, where each stream plays the role of an arm and measurements must be allocated sequentially under uncertainty. Existing combinatorial bandit and hypothesis testing methods typically assume independent observations and fail to exploit correlation for efficient measurement design. We propose ECC-AHT, an adaptive algorithm that selects continuous, constrained measurements to maximize Chernoff information between competing hypotheses, enabling active noise cancellation through differential sensing. ECC-AHT achieves optimal sample complexity guarantees and significantly outperforms state-of-the-art baselines in both synthetic and real-world correlated environments. The code is available on https://github.com/VincentdeCristo/ECC-AHT", "AI": {"tldr": "Proposes ECC-AHT, an adaptive algorithm for identifying anomalous subsets of streams under correlated noise by maximizing Chernoff information through differential sensing, achieving optimal sample complexity.", "motivation": "Monitoring and security in cyber-physical systems requires identifying anomalous streams, but existing methods assume independent observations and fail to exploit correlation for efficient measurement design.", "method": "ECC-AHT adaptively selects continuous, constrained measurements to maximize Chernoff information between competing hypotheses, enabling active noise cancellation through differential sensing.", "result": "Achieves optimal sample complexity guarantees and significantly outperforms state-of-the-art baselines in both synthetic and real-world correlated environments.", "conclusion": "The algorithm successfully addresses combinatorial pure exploration under correlated noise, providing theoretical optimality and empirical superiority for cyber-physical system monitoring."}}
{"id": "2601.17441", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17441", "abs": "https://arxiv.org/abs/2601.17441", "authors": ["Ondrej Bohdal", "Taha Ceritli", "Mete Ozay", "Jijoong Moon", "Kyeng-Hun Lee", "Hyeonmok Ko", "Umberto Michieli"], "title": "Data-driven Clustering and Merging of Adapters for On-device Large Language Models", "comment": "Accepted at ICASSP 2026", "summary": "On-device large language models commonly employ task-specific adapters (e.g., LoRAs) to deliver strong performance on downstream tasks. While storing all available adapters is impractical due to memory constraints, mobile devices typically have sufficient capacity to store a limited number of these parameters. This raises a critical challenge: how to select representative adapters that generalize well across multiple tasks - a problem that remains unexplored in existing literature. We propose a novel method D2C for adapter clustering that leverages minimal task-specific examples (e.g., 10 per task) and employs an iterative optimization process to refine cluster assignments. The adapters within each cluster are merged, creating multi-task adapters deployable on resource-constrained devices. Experimental results demonstrate that our method effectively boosts performance for considered storage budgets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faD2C\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c11\u91cf\u4efb\u52a1\u793a\u4f8b\u548c\u8fed\u4ee3\u4f18\u5316\u5bf9\u9002\u914d\u5668\u8fdb\u884c\u805a\u7c7b\u5e76\u5408\u5e76\uff0c\u751f\u6210\u53ef\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72\u7684\u591a\u4efb\u52a1\u9002\u914d\u5668\uff0c\u6709\u6548\u63d0\u5347\u5b58\u50a8\u9884\u7b97\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u8bbe\u5907\u7aef\u5927\u8bed\u8a00\u6a21\u578b\u4f7f\u7528\u4efb\u52a1\u7279\u5b9a\u9002\u914d\u5668\uff0c\u4f46\u5b58\u50a8\u6240\u6709\u9002\u914d\u5668\u4e0d\u73b0\u5b9e\uff0c\u5982\u4f55\u5728\u6709\u9650\u5185\u5b58\u4e0b\u9009\u62e9\u80fd\u6cdb\u5316\u5230\u591a\u4efb\u52a1\u7684\u4ee3\u8868\u6027\u9002\u914d\u5668\u4ecd\u662f\u672a\u63a2\u7d22\u7684\u6311\u6218\u3002", "method": "D2C\u5229\u7528\u6bcf\u4efb\u52a1\u5c11\u91cf\u793a\u4f8b\uff08\u598210\u4e2a\uff09\u548c\u8fed\u4ee3\u4f18\u5316\u8fc7\u7a0b\u8fdb\u884c\u9002\u914d\u5668\u805a\u7c7b\uff0c\u5c06\u5404\u7c07\u5185\u7684\u9002\u914d\u5668\u5408\u5e76\u521b\u5efa\u591a\u4efb\u52a1\u9002\u914d\u5668\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8003\u8651\u7684\u5b58\u50a8\u9884\u7b97\u4e0b\u6709\u6548\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "D2C\u65b9\u6cd5\u901a\u8fc7\u805a\u7c7b\u548c\u5408\u5e76\u9002\u914d\u5668\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u7684\u9002\u914d\u5668\u9009\u62e9\u95ee\u9898\uff0c\u521b\u5efa\u4e86\u53ef\u90e8\u7f72\u7684\u901a\u7528\u591a\u4efb\u52a1\u9002\u914d\u5668\u3002"}}
{"id": "2601.18308", "categories": ["cs.AI", "cs.SI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.18308", "abs": "https://arxiv.org/abs/2601.18308", "authors": ["Geunsik Lim"], "title": "A Generative AI-Driven Reliability Layer for Action-Oriented Disaster Resilience", "comment": "19 pages", "summary": "As climate-related hazards intensify, conventional early warning systems (EWS) disseminate alerts rapidly but often fail to trigger timely protective actions, leading to preventable losses and inequities. We introduce Climate RADAR (Risk-Aware, Dynamic, and Action Recommendation system), a generative AI-based reliability layer that reframes disaster communication from alerts delivered to actions executed. It integrates meteorological, hydrological, vulnerability, and social data into a composite risk index and employs guardrail-embedded large language models (LLMs) to deliver personalized recommendations across citizen, volunteer, and municipal interfaces. Evaluation through simulations, user studies, and a municipal pilot shows improved outcomes, including higher protective action execution, reduced response latency, and increased usability and trust. By combining predictive analytics, behavioral science, and responsible AI, Climate RADAR advances people-centered, transparent, and equitable early warning systems, offering practical pathways toward compliance-ready disaster resilience infrastructures.", "AI": {"tldr": "\u63d0\u51faClimate RADAR\u751f\u6210\u5f0fAI\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408\u591a\u6e90\u6570\u636e\u4e0e\u62a4\u680fLLM\uff0c\u5c06\u707e\u5bb3\u9884\u8b66\u4ece\"\u8b66\u62a5\u53d1\u9001\"\u8f6c\u53d8\u4e3a\"\u884c\u52a8\u6267\u884c\"\uff0c\u7ecf\u6a21\u62df\u3001\u7528\u6237\u7814\u7a76\u548c\u5e02\u653f\u8bd5\u70b9\u9a8c\u8bc1\u53ef\u63d0\u5347\u9632\u62a4\u884c\u52a8\u7387\u3001\u964d\u4f4e\u5ef6\u8fdf\u5e76\u589e\u5f3a\u4fe1\u4efb", "motivation": "\u4f20\u7edf\u9884\u8b66\u7cfb\u7edf\u867d\u5feb\u901f\u53d1\u5e03\u8b66\u62a5\uff0c\u4f46\u65e0\u6cd5\u6709\u6548\u89e6\u53d1\u53ca\u65f6\u9632\u62a4\u884c\u52a8\uff0c\u5bfc\u81f4\u53ef\u9884\u9632\u635f\u5931\u548c\u5e94\u5bf9\u4e0d\u5e73\u7b49\u95ee\u9898", "method": "\u6784\u5efa\u98ce\u9669\u611f\u77e5\u52a8\u6001\u63a8\u8350\u7cfb\u7edf\uff0c\u878d\u5408\u6c14\u8c61\u3001\u6c34\u6587\u3001\u8106\u5f31\u6027\u53ca\u793e\u4f1a\u6570\u636e\u751f\u6210\u590d\u5408\u98ce\u9669\u6307\u6570\uff0c\u91c7\u7528\u62a4\u680f\u5d4c\u5165\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e3a\u5e02\u6c11\u3001\u5fd7\u613f\u8005\u548c\u5e02\u653f\u90e8\u95e8\u63d0\u4f9b\u4e2a\u6027\u5316\u884c\u52a8\u5efa\u8bae\uff0c\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\u3001\u7528\u6237\u7814\u7a76\u548c\u5e02\u653f\u8bd5\u70b9\u8bc4\u4f30", "result": "\u663e\u8457\u63d0\u9ad8\u9632\u62a4\u884c\u52a8\u6267\u884c\u7387\uff0c\u7f29\u77ed\u54cd\u5e94\u5ef6\u8fdf\uff0c\u63d0\u5347\u7cfb\u7edf\u53ef\u7528\u6027\u548c\u7528\u6237\u4fe1\u4efb\u5ea6", "conclusion": "\u63a8\u52a8\u4ee5\u4eba\u4e3a\u672c\u3001\u900f\u660e\u4e14\u516c\u5e73\u7684\u9884\u8b66\u7cfb\u7edf\u53d1\u5c55\uff0c\u4e3a\u6784\u5efa\u5408\u89c4\u707e\u5bb3\u62b5\u5fa1\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u5b9e\u8df5\u8def\u5f84"}}
{"id": "2601.18353", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.18353", "abs": "https://arxiv.org/abs/2601.18353", "authors": ["Tuhin Chakrabarty", "Paramveer S. Dhillon"], "title": "Can Good Writing Be Generative? Expert-Level AI Writing Emerges through Fine-Tuning on High-Quality Books", "comment": "Proceedings of CHI 2026 Conference (To Appear)", "summary": "Creative writing has long been considered a uniquely human endeavor, requiring voice and style that machines could not replicate. This assumption is challenged by Generative AI that can emulate thousands of author styles in seconds with negligible marginal labor. To understand this better, we conducted a behavioral experiment where 28 MFA writers (experts) competed against three LLMs in emulating 50 critically acclaimed authors. Based on blind pairwise comparisons by 28 expert judges and 131 lay judges, we find that experts preferred human writing in 82.7% of cases under the in-context prompting condition but this reversed to 62% preference for AI after fine-tuning on authors' complete works. Lay judges, however, consistently preferred AI writing. Debrief interviews with expert writers revealed that their preference for AI writing triggered an identity crisis, eroding aesthetic confidence and questioning what constitutes \"good writing.\" These findings challenge discourse about AI's creative limitations and raise fundamental questions about the future of creative labor.", "AI": {"tldr": "A study shows that while expert writers initially prefer human writing over AI, this reverses after AI is fine-tuned on authors' works, causing identity crises among experts and challenging assumptions about AI's creative limitations.", "motivation": "To understand how generative AI challenges the assumption that creative writing is a uniquely human endeavor, and to explore the behavioral and psychological impact on human writers when competing with AI that can emulate literary styles.", "method": "Conducted a behavioral experiment where 28 MFA writers competed against three LLMs in emulating 50 acclaimed authors. Used blind pairwise comparisons judged by 28 expert judges and 131 lay judges, with two conditions: in-context prompting and fine-tuning on authors' complete works. Followed up with debrief interviews with expert writers.", "result": "Under in-context prompting, experts preferred human writing 82.7% of the time; after fine-tuning, experts preferred AI writing 62% of the time. Lay judges consistently preferred AI writing. Expert writers reported identity crisis, eroded aesthetic confidence, and questioned what constitutes \"good writing\".", "conclusion": "Generative AI challenges discourse about AI's creative limitations, reverses expert preferences for human writing after fine-tuning, and raises fundamental questions about the future of creative labor as AI can successfully emulate human creative styles."}}
{"id": "2601.17467", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17467", "abs": "https://arxiv.org/abs/2601.17467", "authors": ["Jianxiong Zhang", "Bing Guo", "Yuming Jiang", "Haobo Wang", "Bo An", "Xuefeng Du"], "title": "Harnessing Reasoning Trajectories for Hallucination Detection via Answer-agreement Representation Shaping", "comment": null, "summary": "Large reasoning models (LRMs) often generate long, seemingly coherent reasoning traces yet still produce incorrect answers, making hallucination detection challenging. Although trajectories contain useful signals, directly using trace text or vanilla hidden states for detection is brittle: traces vary in form and detectors can overfit to superficial patterns rather than answer validity. We introduce Answer-agreement Representation Shaping (ARS), which learns detection-friendly trace-conditioned representations by explicitly encoding answer stability. ARS generates counterfactual answers through small latent interventions, specifically, perturbing the trace-boundary embedding, and labels each perturbation by whether the resulting answer agrees with the original. It then learns representations that bring answer-agreeing states together and separate answer-disagreeing ones, exposing latent instability indicative of hallucination risk. The shaped embeddings are plug-and-play with existing embedding-based detectors and require no human annotations during training. Experiments demonstrate that ARS consistently improves detection and achieves substantial gains over strong baselines.", "AI": {"tldr": "\u63d0\u51faARS\u65b9\u6cd5\uff0c\u901a\u8fc7\u6270\u52a8\u63a8\u7406\u8fb9\u754c\u5d4c\u5165\u751f\u6210\u53cd\u4e8b\u5b9e\u7b54\u6848\uff0c\u5b66\u4e60\u80fd\u533a\u5206\u7b54\u6848\u4e00\u81f4\u6027\u7684\u8868\u793a\uff0c\u4ee5\u65e0\u76d1\u7763\u65b9\u5f0f\u63d0\u5347\u5927\u6a21\u578b\u5e7b\u89c9\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u5927\u63a8\u7406\u6a21\u578b\u5e38\u751f\u6210\u770b\u4f3c\u8fde\u8d2f\u4f46\u9519\u8bef\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u76f4\u63a5\u5229\u7528\u8f68\u8ff9\u6587\u672c\u6216\u9690\u85cf\u72b6\u6001\u8fdb\u884c\u5e7b\u89c9\u68c0\u6d4b\u6548\u679c\u4e0d\u4f73\uff0c\u6613\u53d7\u5f62\u5f0f\u53d8\u5316\u5f71\u54cd\u4e14\u4f1a\u8fc7\u62df\u5408\u8868\u9762\u6a21\u5f0f\u800c\u975e\u7b54\u6848\u6709\u6548\u6027\u3002", "method": "ARS\u901a\u8fc7\u5fae\u5c0f\u6f5c\u5728\u5e72\u9884\uff08\u6270\u52a8\u8f68\u8ff9\u8fb9\u754c\u5d4c\u5165\uff09\u751f\u6210\u53cd\u4e8b\u5b9e\u7b54\u6848\uff0c\u6839\u636e\u7b54\u6848\u662f\u5426\u4e00\u81f4\u8fdb\u884c\u6807\u6ce8\uff0c\u5b66\u4e60\u5c06\u4e00\u81f4\u7b54\u6848\u72b6\u6001\u805a\u96c6\u3001\u4e0d\u4e00\u81f4\u7b54\u6848\u72b6\u6001\u5206\u79bb\u7684\u8868\u793a\uff0c\u66b4\u9732\u6307\u793a\u5e7b\u89c9\u98ce\u9669\u7684\u6f5c\u5728\u4e0d\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eARS\u80fd\u6301\u7eed\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\uff0c\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u53d6\u5f97\u663e\u8457\u589e\u76ca\u3002", "conclusion": "ARS\u751f\u6210\u7684\u5851\u5f62\u5d4c\u5165\u53ef\u4e0e\u73b0\u6709\u57fa\u4e8e\u5d4c\u5165\u7684\u68c0\u6d4b\u5668\u5373\u63d2\u5373\u7528\uff0c\u4e14\u8bad\u7ec3\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0c\u4e3a\u5e7b\u89c9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u65e0\u76d1\u7763\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2601.18383", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18383", "abs": "https://arxiv.org/abs/2601.18383", "authors": ["Zhenyuan Guo", "Tong Chen", "Wenlong Meng", "Chen Gong", "Xin Yu", "Chengkun Wei", "Wenzhi Chen"], "title": "Dynamic Thinking-Token Selection for Efficient Reasoning in Large Reasoning Models", "comment": null, "summary": "Large Reasoning Models (LRMs) excel at solving complex problems by explicitly generating a reasoning trace before deriving the final answer. However, these extended generations incur substantial memory footprint and computational overhead, bottlenecking LRMs' efficiency. This work uses attention maps to analyze the influence of reasoning traces and uncover an interesting phenomenon: only some decision-critical tokens in a reasoning trace steer the model toward the final answer, while the remaining tokens contribute negligibly. Building on this observation, we propose Dynamic Thinking-Token Selection (DynTS). This method identifies decision-critical tokens and retains only their associated Key-Value (KV) cache states during inference, evicting the remaining redundant entries to optimize efficiency.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa Dynamic Thinking-Token Selection (DynTS) \u65b9\u6cd5\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u5206\u6790\u8bc6\u522b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u51b3\u7b56\u5173\u952e token\uff0c\u4ec5\u4fdd\u7559\u5176 KV \u7f13\u5b58\u72b6\u6001\u4ee5\u4f18\u5316\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6548\u7387\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u901a\u8fc7\u751f\u6210\u63a8\u7406\u8f68\u8ff9\u6765\u89e3\u51b3\u95ee\u9898\uff0c\u4f46\u8fd9\u4e9b\u6269\u5c55\u751f\u6210\u4f1a\u6d88\u8017\u5927\u91cf\u5185\u5b58\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u6210\u4e3a\u6548\u7387\u74f6\u9888\u3002", "method": "\u5229\u7528\u6ce8\u610f\u529b\u56fe\u5206\u6790\u63a8\u7406\u8f68\u8ff9\uff0c\u53d1\u73b0\u53ea\u6709\u90e8\u5206\u51b3\u7b56\u5173\u952e token \u4f1a\u5f71\u54cd\u6700\u7ec8\u7b54\u6848\uff0c\u636e\u6b64\u63d0\u51fa DynTS \u65b9\u6cd5\uff0c\u5728\u63a8\u7406\u65f6\u4ec5\u4fdd\u7559\u5173\u952e token \u7684 KV \u7f13\u5b58\uff0c\u5254\u9664\u5197\u4f59\u6761\u76ee\u3002", "result": "\u901a\u8fc7\u9009\u62e9\u6027\u4fdd\u7559 KV \u7f13\u5b58\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u5185\u5b58\u5360\u7528\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u4ece\u800c\u4f18\u5316\u4e86\u6a21\u578b\u6548\u7387\u3002", "conclusion": "DynTS \u65b9\u6cd5\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u8bc6\u522b\u5e76\u4fdd\u7559\u5173\u952e\u63a8\u7406 token\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u63a8\u7406\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.17473", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17473", "abs": "https://arxiv.org/abs/2601.17473", "authors": ["Manooshree Patel", "Rayna Bhattacharyya", "Thomas Lu", "Arnav Mehta", "Niels Voss", "Narges Norouzi", "Gireeja Ranade"], "title": "LeanTutor: Towards a Verified AI Mathematical Proof Tutor", "comment": "arXiv admin note: substantial text overlap with arXiv:2506.08321. substantial text overlap with arXiv:2506.08321. substantial text overlap with arXiv:2506.08321. substantial text overlap with arXiv:2506.08321", "summary": "This paper considers the development of an AI-based provably-correct mathematical proof tutor. While Large Language Models (LLMs) allow seamless communication in natural language, they are error prone. Theorem provers such as Lean allow for provable-correctness, but these are hard for students to learn. We present a proof-of-concept system (LeanTutor) by combining the complementary strengths of LLMs and theorem provers. LeanTutor is composed of three modules: (i) an autoformalizer/proof-checker, (ii) a next-step generator, and (iii) a natural language feedback generator. To evaluate the system, we introduce PeanoBench, a dataset of 371 Peano Arithmetic proofs in human-written natural language and formal language, derived from the Natural Numbers Game.", "AI": {"tldr": "\u63d0\u51faLeanTutor\u7cfb\u7edf\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u80fd\u529b\u4e0e\u5b9a\u7406\u8bc1\u660e\u5668\u7684\u53ef\u8bc1\u660e\u6b63\u786e\u6027\uff0c\u6253\u9020\u53ef\u9a8c\u8bc1\u6b63\u786e\u7684AI\u6570\u5b66\u8bc1\u660e\u8f85\u5bfc\u5de5\u5177", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u867d\u652f\u6301\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u4f46\u6613\u51fa\u9519\uff0c\u5b9a\u7406\u8bc1\u660e\u5668(\u5982Lean)\u53ef\u4fdd\u8bc1\u6b63\u786e\u6027\u4f46\u96be\u5b66\uff0c\u9700\u878d\u5408\u4e24\u8005\u4f18\u52bf\u6784\u5efa\u66f4\u53ef\u9760\u7684\u6570\u5b66\u8f85\u5bfc\u7cfb\u7edf", "method": "\u8bbe\u8ba1\u4e09\u6a21\u5757\u7cfb\u7edf\uff1a\u81ea\u52a8\u5f62\u5f0f\u5316/\u8bc1\u660e\u68c0\u67e5\u5668\u3001\u4e0b\u4e00\u6b65\u751f\u6210\u5668\u3001\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u751f\u6210\u5668\uff1b\u6784\u5efaPeanoBench\u6570\u636e\u96c6(371\u4e2a\u4eba\u5de5\u7f16\u5199\u7684\u76ae\u4e9a\u8bfa\u7b97\u672f\u8bc1\u660e)\u7528\u4e8e\u8bc4\u4f30", "result": "\u5b9e\u73b0\u4e86\u6982\u5ff5\u9a8c\u8bc1\u7cfb\u7edfLeanTutor\uff0c\u521b\u5efa\u4e86\u5305\u542b\u81ea\u7136\u8bed\u8a00\u548c\u5f62\u5f0f\u5316\u8bed\u8a00\u8bc1\u660e\u7684\u57fa\u51c6\u6570\u636e\u96c6PeanoBench", "conclusion": "\u878d\u5408LLMs\u4e0e\u5b9a\u7406\u8bc1\u660e\u5668\u662f\u5f00\u53d1\u53ef\u8bc1\u660e\u6b63\u786e\u4e14\u7528\u6237\u53cb\u597d\u7684\u6570\u5b66\u8bc1\u660e\u8f85\u5bfc\u7cfb\u7edf\u7684\u53ef\u884c\u8def\u5f84"}}
{"id": "2601.18467", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18467", "abs": "https://arxiv.org/abs/2601.18467", "authors": ["Yuhang Zhou", "Kai Zheng", "Qiguang Chen", "Mengkang Hu", "Qingfeng Sun", "Can Xu", "Jingjing Chen"], "title": "OffSeeker: Online Reinforcement Learning Is Not All You Need for Deep Research Agents", "comment": null, "summary": "Deep research agents have shown remarkable potential in handling long-horizon tasks. However, state-of-the-art performance typically relies on online reinforcement learning (RL), which is financially expensive due to extensive API calls. While offline training offers a more efficient alternative, its progress is hindered by the scarcity of high-quality research trajectories. In this paper, we demonstrate that expensive online reinforcement learning is not all you need to build powerful research agents. To bridge this gap, we introduce a fully open-source suite designed for effective offline training. Our core contributions include DeepForge, a ready-to-use task synthesis framework that generates large-scale research queries without heavy preprocessing; and a curated collection of 66k QA pairs, 33k SFT trajectories, and 21k DPO pairs. Leveraging these resources, we train OffSeeker (8B), a model developed entirely offline. Extensive evaluations across six benchmarks show that OffSeeker not only leads among similar-sized agents but also remains competitive with 30B-parameter systems trained via heavy online RL.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u79bb\u7ebf\u7684\u6df1\u5ea6\u7814\u7a76\u667a\u80fd\u4f53\u8bad\u7ec3\u65b9\u6848\uff0c\u901a\u8fc7\u5f00\u6e90\u7684DeepForge\u6846\u67b6\u548c\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u51fa8B\u53c2\u6570\u7684OffSeeker\u6a21\u578b\uff0c\u5176\u6027\u80fd\u4e0d\u4ec5\u9886\u5148\u540c\u5c3a\u5bf8\u6a21\u578b\uff0c\u8fd8\u80fd\u5ab2\u7f8e30B\u53c2\u6570\u4e14\u7ecf\u8fc7\u6602\u8d35\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524d\u9ad8\u6027\u80fd\u6df1\u5ea6\u7814\u7a76\u667a\u80fd\u4f53\u4f9d\u8d56\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u4f46API\u8c03\u7528\u6210\u672c\u9ad8\u6602\uff1b\u800c\u79bb\u7ebf\u8bad\u7ec3\u53d7\u9650\u4e8e\u9ad8\u8d28\u91cf\u7814\u7a76\u8f68\u8ff9\u6570\u636e\u7684\u7a00\u7f3a\u6027\u3002\u8bba\u6587\u65e8\u5728\u8bc1\u660e\u65e0\u9700\u6602\u8d35\u7684\u5728\u7ebfRL\u4e5f\u80fd\u6784\u5efa\u5f3a\u5927\u7684\u7814\u7a76\u667a\u80fd\u4f53\u3002", "method": "\u5f00\u53d1\u5b8c\u5168\u5f00\u6e90\u7684\u79bb\u7ebf\u8bad\u7ec3\u5957\u4ef6\uff1a1) DeepForge\u4efb\u52a1\u5408\u6210\u6846\u67b6\uff0c\u53ef\u751f\u6210\u5927\u89c4\u6a21\u7814\u7a76\u67e5\u8be2\uff1b2) \u6574\u740666k\u95ee\u7b54\u5bf9\u300133k SFT\u8f68\u8ff9\u548c21k DPO\u5bf9\u7684\u6570\u636e\u96c6\uff1b3) \u5b8c\u5168\u79bb\u7ebf\u8bad\u7ec3OffSeeker(8B)\u6a21\u578b\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cOffSeeker\u57288B\u53c2\u6570\u89c4\u6a21\u6a21\u578b\u4e2d\u8868\u73b0\u9886\u5148\uff0c\u540c\u65f6\u80fd\u4e0e30B\u53c2\u6570\u4e14\u7ecf\u8fc7\u91cd\u5ea6\u5728\u7ebfRL\u8bad\u7ec3\u7684\u6a21\u578b\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u6602\u8d35\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5e76\u975e\u6784\u5efa\u5f3a\u5927\u7814\u7a76\u667a\u80fd\u4f53\u7684\u552f\u4e00\u9014\u5f84\u3002\u901a\u8fc7\u9ad8\u8d28\u91cf\u6570\u636e\u5408\u6210\u548c\u79bb\u7ebf\u8bad\u7ec3\uff0c\u53ef\u4ee5\u5728\u663e\u8457\u964d\u4f4e\u6210\u672c\u7684\u540c\u65f6\u8fbe\u5230\u63a5\u8fd1\u5148\u8fdb\u6c34\u5e73\u7684\u6027\u80fd\uff0c\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2601.17480", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17480", "abs": "https://arxiv.org/abs/2601.17480", "authors": ["Marton Szep", "Jorge Marin Ruiz", "Georgios Kaissis", "Paulina Seidl", "R\u00fcdiger von Eisenhart-Rothe", "Florian Hinterwimmer", "Daniel Rueckert"], "title": "Unintended Memorization of Sensitive Information in Fine-Tuned Language Models", "comment": "Accepted to EACL 2026. 20 pages", "summary": "Fine-tuning Large Language Models (LLMs) on sensitive datasets carries a substantial risk of unintended memorization and leakage of Personally Identifiable Information (PII), which can violate privacy regulations and compromise individual safety. In this work, we systematically investigate a critical and underexplored vulnerability: the exposure of PII that appears only in model inputs, not in training targets. Using both synthetic and real-world datasets, we design controlled extraction probes to quantify unintended PII memorization and study how factors such as language, PII frequency, task type, and model size influence memorization behavior. We further benchmark four privacy-preserving approaches including differential privacy, machine unlearning, regularization, and preference alignment, evaluating their trade-offs between privacy and task performance. Our results show that post-training methods generally provide more consistent privacy-utility trade-offs, while differential privacy achieves strong reduction in leakage in specific settings, although it can introduce training instability. These findings highlight the persistent challenge of memorization in fine-tuned LLMs and emphasize the need for robust, scalable privacy-preserving techniques.", "AI": {"tldr": "This paper investigates how fine-tuned LLMs memorize and leak PII from input data, studies influencing factors, and benchmarks privacy-preserving methods, finding post-training approaches offer better privacy-utility trade-offs.", "motivation": "Fine-tuning LLMs on sensitive datasets risks unintended PII memorization and leakage, violating privacy regulations and compromising individual safety, particularly for PII appearing only in model inputs (not training targets).", "method": "Using synthetic and real-world datasets with controlled extraction probes to quantify PII memorization; studying factors like language, PII frequency, task type, and model size; benchmarking four privacy-preserving approaches (differential privacy, machine unlearning, regularization, preference alignment) and evaluating privacy-utility trade-offs.", "result": "Post-training methods provide more consistent privacy-utility trade-offs; differential privacy achieves strong leakage reduction in specific settings but introduces training instability.", "conclusion": "Memorization in fine-tuned LLMs remains a persistent challenge, highlighting the need for robust, scalable privacy-preserving techniques."}}
{"id": "2601.18496", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18496", "abs": "https://arxiv.org/abs/2601.18496", "authors": ["Zihan wang", "Hao Wang", "Shi Feng", "Xiaocui Yang", "Daling Wang", "Yiqun Zhang", "Jinghao Lin", "Haihua Yang", "Xiaozhong Ji"], "title": "DEEPMED: Building a Medical DeepResearch Agent via Multi-hop Med-Search Data and Turn-Controlled Agentic Training & Inference", "comment": null, "summary": "Medical reasoning models remain constrained by parametric knowledge and are thus susceptible to forgetting and hallucinations. DeepResearch (DR) models ground outputs in verifiable evidence from tools and perform strongly in general domains, but their direct transfer to medical field yields relatively limited gains. We attribute this to two gaps: task characteristic and tool-use scaling. Medical questions require evidence interpretation in a knowledge-intensive clinical context; while general DR models can retrieve information, they often lack clinical-context reasoning and thus \"find it but fail to use it,\" leaving performance limited by medical abilities. Moreover, in medical scenarios, blindly scaling tool-call can inject noisy context, derailing sensitive medical reasoning and prompting repetitive evidence-seeking along incorrect paths. Therefore, we propose DeepMed. For data, we deploy a multi-hop med-search QA synthesis method supporting the model to apply the DR paradigm in medical contexts. For training, we introduce a difficulty-aware turn-penalty to suppress excessive tool-call growth. For inference, we bring a monitor to help validate hypotheses within a controlled number of steps and avoid context rot. Overall, on seven medical benchmarks, DeepMed improves its base model by 9.79\\% on average and outperforms larger medical reasoning and DR models.", "AI": {"tldr": "The paper proposes DeepMed to address limitations of general DeepResearch models in medical domains, which suffer from lack of clinical context reasoning and excessive tool calls. By using medical-specific data synthesis, difficulty-aware training penalties, and a monitoring mechanism, DeepMed achieves 9.79% average improvement on seven medical benchmarks.", "motivation": "Medical reasoning models constrained by parametric knowledge face forgetting and hallucinations. General DeepResearch models ground outputs in evidence but transfer poorly to medical fields due to two gaps: (1) inability to interpret evidence within knowledge-intensive clinical contexts (\"find it but fail to use it\"), and (2) blind tool-call scaling injecting noisy context that derails sensitive medical reasoning.", "method": "DeepMed comprises three key components: (1) Data: multi-hop medical search QA synthesis to adapt DeepResearch paradigm for medical contexts; (2) Training: difficulty-aware turn-penalty to suppress excessive tool-call growth; (3) Inference: a monitor to validate hypotheses within controlled steps and prevent context rot.", "result": "On seven medical benchmarks, DeepMed improves its base model by 9.79% on average and outperforms larger medical reasoning and DeepResearch models.", "conclusion": "DeepMed successfully bridges the gap between general DeepResearch and medical domain requirements by incorporating clinical context understanding and controlled tool-use, demonstrating significant performance gains in medical reasoning tasks."}}
{"id": "2601.17489", "categories": ["cs.LG", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17489", "abs": "https://arxiv.org/abs/2601.17489", "authors": ["Ashutosh Bajpai", "Akshat Bhandari", "Akshay Nambi", "Tanmoy Chakraborty"], "title": "SpatialMath: Spatial Comprehension-Infused Symbolic Reasoning for Mathematical Problem-Solving", "comment": null, "summary": "Multimodal Small-to-Medium sized Language Models (MSLMs) have demonstrated strong capabilities in integrating visual and textual information but still face significant limitations in visual comprehension and mathematical reasoning, particularly in geometric problems with diverse levels of visual infusion. Current models struggle to accurately decompose intricate visual inputs and connect perception with structured reasoning, leading to suboptimal performance. To address these challenges, we propose SpatialMath, a novel Spatial Comprehension-Infused Symbolic Reasoning Framework designed to integrate spatial representations into structured symbolic reasoning chains. SpatialMath employs a specialized perception module to extract spatially-grounded representations from visual diagrams, capturing critical geometric structures and spatial relationships. These representations are then methodically infused into symbolic reasoning chains, facilitating visual comprehension-aware structured reasoning. To this end, we introduce MATHVERSE-PLUS, a novel dataset containing structured visual interpretations and step-by-step reasoning paths for vision-intensive mathematical problems. SpatialMath significantly outperforms strong multimodal baselines, achieving up to 10 percentage points improvement over supervised fine-tuning with data augmentation in vision-intensive settings. Robustness analysis reveals that enhanced spatial representations directly improve reasoning accuracy, reinforcing the need for structured perception-to-reasoning pipelines in MSLMs.", "AI": {"tldr": "SpatialMath\u662f\u4e00\u79cd\u7a7a\u95f4\u7406\u89e3\u589e\u5f3a\u7684\u7b26\u53f7\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u89c6\u89c9\u56fe\u8868\u4e2d\u63d0\u53d6\u7a7a\u95f4\u8868\u793a\u5e76\u878d\u5165\u7ed3\u6784\u5316\u7b26\u53f7\u63a8\u7406\u94fe\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u51e0\u4f55\u6570\u5b66\u95ee\u9898\u4e0a\u7684\u6027\u80fd\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe10\u4e2a\u767e\u5206\u70b9\u3002", "motivation": "\u591a\u6a21\u6001\u4e2d\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u548c\u6570\u5b66\u63a8\u7406\u65b9\u9762\u4ecd\u5b58\u5728\u663e\u8457\u5c40\u9650\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u591a\u6837\u5316\u89c6\u89c9\u6ce8\u5165\u7684\u51e0\u4f55\u95ee\u9898\u4e2d\u3002\u5f53\u524d\u6a21\u578b\u96be\u4ee5\u51c6\u786e\u5206\u89e3\u590d\u6742\u89c6\u89c9\u8f93\u5165\u5e76\u5c06\u611f\u77e5\u4e0e\u7ed3\u6784\u5316\u63a8\u7406\u8fde\u63a5\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51faSpatialMath\u6846\u67b6\uff0c\u91c7\u7528\u4e13\u7528\u611f\u77e5\u6a21\u5757\u4ece\u89c6\u89c9\u56fe\u8868\u4e2d\u63d0\u53d6\u7a7a\u95f4\u57fa\u7840\u7684\u8868\u793a\uff0c\u6355\u6349\u5173\u952e\u51e0\u4f55\u7ed3\u6784\u548c\u7a7a\u95f4\u5173\u7cfb\u3002\u8fd9\u4e9b\u8868\u793a\u88ab\u7cfb\u7edf\u5730\u878d\u5165\u7b26\u53f7\u63a8\u7406\u94fe\uff0c\u5b9e\u73b0\u89c6\u89c9\u7406\u89e3\u611f\u77e5\u7684\u7ed3\u6784\u5316\u63a8\u7406\u3002\u540c\u65f6\u5f15\u5165MATHVERSE-PLUS\u6570\u636e\u96c6\uff0c\u5305\u542b\u9762\u5411\u89c6\u89c9\u5bc6\u96c6\u578b\u6570\u5b66\u95ee\u9898\u7684\u7ed3\u6784\u5316\u89c6\u89c9\u89e3\u91ca\u548c\u9010\u6b65\u63a8\u7406\u8def\u5f84\u3002", "result": "SpatialMath\u663e\u8457\u4f18\u4e8e\u5f3a\u5927\u7684\u591a\u6a21\u6001\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u89c6\u89c9\u5bc6\u96c6\u578b\u8bbe\u7f6e\u4e0b\uff0c\u76f8\u6bd4\u6570\u636e\u589e\u5f3a\u7684\u76d1\u7763\u5fae\u8c03\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe10\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u589e\u5f3a\u7684\u7a7a\u95f4\u8868\u793a\u76f4\u63a5\u63d0\u9ad8\u4e86\u63a8\u7406\u51c6\u786e\u6027\uff0c\u5f3a\u5316\u4e86\u5728\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u4e2d\u6784\u5efa\u7ed3\u6784\u5316\u611f\u77e5-\u63a8\u7406\u7ba1\u9053\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.18554", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18554", "abs": "https://arxiv.org/abs/2601.18554", "authors": ["Alberto Purpura", "Li Wang", "Sahil Badyal", "Eugenio Beaufrand", "Adam Faulkner"], "title": "Deconstructing Instruction-Following: A New Benchmark for Granular Evaluation of Large Language Model Instruction Compliance Abilities", "comment": "Paper accepted to EACL 2026", "summary": "Reliably ensuring Large Language Models (LLMs) follow complex instructions is a critical challenge, as existing benchmarks often fail to reflect real-world use or isolate compliance from task success. We introduce MOSAIC (MOdular Synthetic Assessment of Instruction Compliance), a modular framework that uses a dynamically generated dataset with up to 20 application-oriented generation constraints to enable a granular and independent analysis of this capability. Our evaluation of five LLMs from different families based on this new benchmark demonstrates that compliance is not a monolithic capability but varies significantly with constraint type, quantity, and position. The analysis reveals model-specific weaknesses, uncovers synergistic and conflicting interactions between instructions, and identifies distinct positional biases such as primacy and recency effects. These granular insights are critical for diagnosing model failures and developing more reliable LLMs for systems that demand strict adherence to complex instructions.", "AI": {"tldr": "\u63d0\u51faMOSAIC\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u751f\u6210\u542b20\u4e2a\u7ea6\u675f\u7684\u6570\u636e\u96c6\uff0c\u5b9e\u73b0LLM\u590d\u6742\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u7684\u7ec6\u7c92\u5ea6\u72ec\u7acb\u8bc4\u4f30\uff0c\u63ed\u793a\u5408\u89c4\u6027\u56e0\u7ea6\u675f\u7c7b\u578b\u3001\u6570\u91cf\u548c\u4f4d\u7f6e\u800c\u5f02\uff0c\u5e76\u53d1\u73b0\u6a21\u578b\u7279\u5b9a\u5f31\u70b9\u53ca\u4f4d\u7f6e\u504f\u89c1\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u4f7f\u7528\u573a\u666f\uff0c\u6216\u65e0\u6cd5\u5c06\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u4e0e\u4efb\u52a1\u6210\u529f\u7387\u5206\u79bb\uff0c\u5bfc\u81f4\u96be\u4ee5\u51c6\u786e\u8bc4\u4f30LLM\u9075\u5faa\u590d\u6742\u6307\u4ee4\u7684\u53ef\u9760\u6027\u3002", "method": "\u5f00\u53d1MOSAIC\uff08\u6a21\u5757\u5316\u5408\u6210\u6307\u4ee4\u9075\u5faa\u8bc4\u4f30\uff09\u6846\u67b6\uff0c\u4f7f\u7528\u52a8\u6001\u751f\u6210\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u6700\u591a20\u4e2a\u9762\u5411\u5e94\u7528\u7684\u751f\u6210\u7ea6\u675f\uff0c\u5b9e\u73b0\u5bf9\u8be5\u80fd\u529b\u7684\u7ec6\u7c92\u5ea6\u548c\u72ec\u7acb\u5206\u6790\u3002", "result": "\u8bc4\u4f30\u4e94\u4e2a\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u7684LLM\uff0c\u53d1\u73b0\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u975e\u5355\u4e00\u7279\u6027\uff0c\u800c\u662f\u968f\u7ea6\u675f\u7c7b\u578b\u3001\u6570\u91cf\u548c\u4f4d\u7f6e\u663e\u8457\u53d8\u5316\uff1b\u63ed\u793a\u4e86\u6a21\u578b\u7279\u5b9a\u5f31\u70b9\u3001\u6307\u4ee4\u95f4\u7684\u534f\u540c\u4e0e\u51b2\u7a81\u4f5c\u7528\uff0c\u4ee5\u53ca\u9996\u56e0\u6548\u5e94\u548c\u8fd1\u56e0\u6548\u5e94\u7b49\u4f4d\u7f6e\u504f\u89c1\u3002", "conclusion": "\u8fd9\u79cd\u7ec6\u7c92\u5ea6\u6d1e\u5bdf\u5bf9\u4e8e\u8bca\u65ad\u6a21\u578b\u5931\u8d25\u3001\u5f00\u53d1\u9700\u8981\u4e25\u683c\u9075\u5b88\u590d\u6742\u6307\u4ee4\u7684\u66f4\u53ef\u9760LLM\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2601.17495", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.17495", "abs": "https://arxiv.org/abs/2601.17495", "authors": ["Ruiyu Zhang", "Lin Nie", "Wai-Fung Lam", "Qihao Wang", "Xin Zhao"], "title": "PEARL: Prototype-Enhanced Alignment for Label-Efficient Representation Learning with Deployment-Driven Insights from Digital Governance Communication Systems", "comment": "15 pages, 1 figure", "summary": "In many deployed systems, new text inputs are handled by retrieving similar past cases, for example when routing and responding to citizen messages in digital governance platforms. When these systems fail, the problem is often not the language model itself, but that the nearest neighbors in the embedding space correspond to the wrong cases. Modern machine learning systems increasingly rely on fixed, high-dimensional embeddings produced by large pretrained models and sentence encoders. In real-world deployments, labels are scarce, domains shift over time, and retraining the base encoder is expensive or infeasible. As a result, downstream performance depends heavily on embedding geometry. Yet raw embeddings are often poorly aligned with the local neighborhood structure required by nearest-neighbor retrieval, similarity search, and lightweight classifiers that operate directly on embeddings. We propose PEARL (Prototype-Enhanced Aligned Representation Learning), a label-efficient approach that uses limited supervision to softly align embeddings toward class prototypes. The method reshapes local neighborhood geometry while preserving dimensionality and avoiding aggressive projection or collapse. Its aim is to bridge the gap between purely unsupervised post-processing, which offers limited and inconsistent gains, and fully supervised projections that require substantial labeled data. We evaluate PEARL under controlled label regimes ranging from extreme label scarcity to higher-label settings. In the label-scarce condition, PEARL substantially improves local neighborhood quality, yielding 25.7% gains over raw embeddings and more than 21.1% gains relative to strong unsupervised post-processing, precisely in the regime where similarity-based systems are most brittle.", "AI": {"tldr": "PEARL\u662f\u4e00\u79cd\u6807\u7b7e\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u6709\u9650\u76d1\u7763\u5c06\u5d4c\u5165\u5411\u91cf\u8f6f\u5bf9\u9f50\u5230\u7c7b\u522b\u539f\u578b\uff0c\u4ee5\u6539\u5584\u6807\u7b7e\u7a00\u7f3a\u548c\u9886\u57df\u504f\u79fb\u573a\u666f\u4e0b\u7684\u6700\u8fd1\u90bb\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u9884\u8bad\u7ec3\u6a21\u578b\u751f\u6210\u7684\u539f\u59cb\u5d4c\u5165\u5411\u91cf\u4e0e\u6700\u8fd1\u90bb\u68c0\u7d22\u6240\u9700\u7684\u5c40\u90e8\u90bb\u57df\u7ed3\u6784\u5bf9\u9f50\u4e0d\u4f73\uff0c\u5bfc\u81f4\u7cfb\u7edf\u5931\u6548\u3002\u5b9e\u9645\u90e8\u7f72\u4e2d\u6807\u7b7e\u7a00\u7f3a\u3001\u9886\u57df\u6f02\u79fb\uff0c\u4e14\u91cd\u8bad\u7ec3\u7f16\u7801\u5668\u6210\u672c\u9ad8\u6602\uff0c\u4e0b\u6e38\u6027\u80fd\u4e25\u91cd\u4f9d\u8d56\u5d4c\u5165\u51e0\u4f55\u7ed3\u6784\u3002", "method": "PEARL\uff08\u539f\u578b\u589e\u5f3a\u5bf9\u9f50\u8868\u793a\u5b66\u4e60\uff09\u4f7f\u7528\u6709\u9650\u76d1\u7763\u5c06\u5d4c\u5165\u5411\u91cf\u8f6f\u5bf9\u9f50\u5230\u7c7b\u522b\u539f\u578b\uff0c\u91cd\u5851\u5c40\u90e8\u90bb\u57df\u51e0\u4f55\uff0c\u4fdd\u6301\u7ef4\u5ea6\u5e76\u907f\u514d\u6295\u5f71\u5d29\u6e83\uff0c\u5728\u65e0\u76d1\u7763\u540e\u5904\u7406\u548c\u5168\u76d1\u7763\u6295\u5f71\u4e4b\u95f4\u67b6\u8d77\u6865\u6881\u3002", "result": "\u5728\u6807\u7b7e\u7a00\u7f3a\u6761\u4ef6\u4e0b\uff0cPEARL\u4f7f\u5c40\u90e8\u90bb\u57df\u8d28\u91cf\u76f8\u6bd4\u539f\u59cb\u5d4c\u5165\u63d0\u534725.7%\uff0c\u76f8\u6bd4\u5f3a\u65e0\u76d1\u7763\u540e\u5904\u7406\u63d0\u5347\u8d85\u8fc721.1%\uff0c\u7279\u522b\u662f\u5728\u57fa\u4e8e\u76f8\u4f3c\u5ea6\u7684\u7cfb\u7edf\u6700\u8106\u5f31\u7684\u573a\u666f\u4e2d\u3002", "conclusion": "PEARL\u901a\u8fc7\u5c11\u91cf\u6807\u7b7e\u5373\u53ef\u663e\u8457\u6539\u5584\u5d4c\u5165\u51e0\u4f55\u7ed3\u6784\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u6700\u8fd1\u90bb\u68c0\u7d22\u8d28\u91cf\u95ee\u9898\uff0c\u65e0\u9700\u5927\u91cf\u6807\u6ce8\u6570\u636e\u6216\u6602\u8d35\u91cd\u8bad\u7ec3\u3002"}}
{"id": "2601.18588", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18588", "abs": "https://arxiv.org/abs/2601.18588", "authors": ["Xianzhe Meng", "Qiangsheng Zeng", "Ling Luo", "Qinghan Yang", "Jiarui Hao", "Wenbo Wu", "Qinyu Wang", "Rui Yin", "Lin Qi", "Renzhi Lu"], "title": "Stability as a Liability:Systematic Breakdown of Linguistic Structure in LLMs", "comment": null, "summary": "Training stability is typically regarded as a prerequisite for reliable optimization in large language models. In this work, we analyze how stabilizing training dynamics affects the induced generation distribution. We show that under standard maximum likelihood training, stable parameter trajectories lead stationary solutions to approximately minimize the forward KL divergence to the empirical distribution, while implicitly reducing generative entropy. As a consequence, the learned model can concentrate probability mass on a limited subset of empirical modes, exhibiting systematic degeneration despite smooth loss convergence. We empirically validate this effect using a controlled feedback-based training framework that stabilizes internal generation statistics, observing consistent low-entropy outputs and repetitive behavior across architectures and random seeds. It indicates that optimization stability and generative expressivity are not inherently aligned, and that stability alone is an insufficient indicator of generative quality.", "AI": {"tldr": "\u8be5\u7814\u7a76\u53d1\u73b0\u7a33\u5b9a\u8bad\u7ec3\u4f1a\u964d\u4f4e\u751f\u6210\u71b5\uff0c\u5bfc\u81f4\u6a21\u578b\u6982\u7387\u8d28\u91cf\u96c6\u4e2d\u5728\u5c11\u6570\u6a21\u5f0f\u4e0a\uff0c\u51fa\u73b0\u7cfb\u7edf\u6027\u9000\u5316\uff0c\u5c3d\u7ba1\u635f\u5931\u5e73\u6ed1\u6536\u655b\uff0c\u8868\u660e\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e0d\u8db3\u4ee5\u4fdd\u8bc1\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u63a2\u7a76\u8bad\u7ec3\u7a33\u5b9a\u6027\u5bf9\u751f\u6210\u5206\u5e03\u7684\u5f71\u54cd\uff0c\u6311\u6218\"\u7a33\u5b9a\u8bad\u7ec3\u662f\u53ef\u9760\u4f18\u5316\u524d\u63d0\"\u7684\u4f20\u7edf\u89c2\u70b9\uff0c\u63ed\u793a\u7a33\u5b9a\u6027\u4e0e\u751f\u6210\u8d28\u91cf\u4e4b\u95f4\u7684\u6f5c\u5728\u77db\u76fe\u3002", "method": "\u7406\u8bba\u5206\u6790\u8868\u660e\u7a33\u5b9a\u53c2\u6570\u8f68\u8ff9\u8fd1\u4f3c\u6700\u5c0f\u5316\u524d\u5411KL\u6563\u5ea6\uff1b\u901a\u8fc7\u53ef\u63a7\u53cd\u9988\u8bad\u7ec3\u6846\u67b6\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u89c2\u5bdf\u4e0d\u540c\u67b6\u6784\u548c\u968f\u673a\u79cd\u5b50\u4e0b\u7684\u4f4e\u71b5\u8f93\u51fa\u548c\u91cd\u590d\u884c\u4e3a\u3002", "result": "\u7a33\u5b9a\u8bad\u7ec3\u4f7f\u6a21\u578b\u9690\u5f0f\u964d\u4f4e\u751f\u6210\u71b5\uff0c\u6982\u7387\u8d28\u91cf\u96c6\u4e2d\u4e8e\u6709\u9650\u7ecf\u9a8c\u6a21\u5f0f\uff0c\u51fa\u73b0\u7cfb\u7edf\u6027\u9000\u5316\u73b0\u8c61\uff0c\u5373\u4f7f\u635f\u5931\u5e73\u6ed1\u6536\u655b\u4e5f\u65e0\u6cd5\u907f\u514d\u3002", "conclusion": "\u4f18\u5316\u7a33\u5b9a\u6027\u4e0e\u751f\u6210\u8868\u8fbe\u6027\u5e76\u975e\u5185\u5728\u4e00\u81f4\uff0c\u4ec5\u4f9d\u8d56\u7a33\u5b9a\u6027\u4f5c\u4e3a\u4f18\u5316\u8d28\u91cf\u6307\u6807\u662f\u4e0d\u5145\u5206\u7684\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6807\u51c6\u3002"}}
{"id": "2601.17512", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17512", "abs": "https://arxiv.org/abs/2601.17512", "authors": ["Yiqun Zhang", "Shenghong Cai", "Zihua Yang", "Sen Feng", "Yuzhu Ji", "Haijun Zhang"], "title": "One-Shot Federated Clustering of Non-Independent Completely Distributed Data", "comment": "This work has been accepted for publication in IEEE Internet of Things Journal", "summary": "Federated Learning (FL) that extracts data knowledge while protecting the privacy of multiple clients has achieved remarkable results in distributed privacy-preserving IoT systems, including smart traffic flow monitoring, smart grid load balancing, and so on. Since most data collected from edge devices are unlabeled, unsupervised Federated Clustering (FC) is becoming increasingly popular for exploring pattern knowledge from complex distributed data. However, due to the lack of label guidance, the common Non-Independent and Identically Distributed (Non-IID) issue of clients have greatly challenged FC by posing the following problems: How to fuse pattern knowledge (i.e., cluster distribution) from Non-IID clients; How are the cluster distributions among clients related; and How does this relationship connect with the global knowledge fusion? In this paper, a more tricky but overlooked phenomenon in Non-IID is revealed, which bottlenecks the clustering performance of the existing FC approaches. That is, different clients could fragment a cluster, and accordingly, a more generalized Non-IID concept, i.e., Non-ICD (Non-Independent Completely Distributed), is derived. To tackle the above FC challenges, a new framework named GOLD (Global Oriented Local Distribution Learning) is proposed. GOLD first finely explores the potential incomplete local cluster distributions of clients, then uploads the distribution summarization to the server for global fusion, and finally performs local cluster enhancement under the guidance of the global distribution. Extensive experiments, including significance tests, ablation studies, scalability evaluations, qualitative results, etc., have been conducted to show the superiority of GOLD.", "AI": {"tldr": "\u9488\u5bf9\u8054\u90a6\u805a\u7c7b\u4e2d\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08Non-IID\uff09\u6570\u636e\u5bfc\u81f4\u7684\u805a\u7c7b\u788e\u7247\u5316\u95ee\u9898\uff0c\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u975e\u72ec\u7acb\u5b8c\u5168\u5206\u5e03\uff08Non-ICD\uff09\u6982\u5ff5\uff0c\u5e76\u8bbe\u8ba1\u4e86GOLD\u6846\u67b6\uff0c\u901a\u8fc7\u5c40\u90e8\u805a\u7c7b\u5206\u5e03\u5b66\u4e60\u3001\u5168\u5c40\u878d\u5408\u548c\u5c40\u90e8\u589e\u5f3a\u6765\u89e3\u51b3\u8054\u90a6\u805a\u7c7b\u4e2d\u7684\u77e5\u8bc6\u878d\u5408\u6311\u6218\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u53d6\u5f97\u663e\u8457\u6210\u679c\uff0c\u4f46\u5927\u591a\u6570\u8fb9\u7f18\u8bbe\u5907\u6570\u636e\u65e0\u6807\u7b7e\uff0c\u4f7f\u5f97\u65e0\u76d1\u7763\u8054\u90a6\u805a\u7c7b\uff08FC\uff09\u8d8a\u6765\u8d8a\u53d7\u6b22\u8fce\u3002\u7136\u800c\uff0cNon-IID\u95ee\u9898\u4e25\u91cd\u6311\u6218FC\u6027\u80fd\uff0c\u7279\u522b\u662f\u5b58\u5728\u805a\u7c7b\u788e\u7247\u5316\u73b0\u8c61\uff08\u4e0d\u540c\u5ba2\u6237\u7aef\u53ef\u80fd\u5206\u5272\u540c\u4e00\u4e2a\u7c07\uff09\uff0c\u8fd9\u9650\u5236\u4e86\u73b0\u6709FC\u65b9\u6cd5\u7684\u805a\u7c7b\u6548\u679c\u3002", "method": "\u63d0\u51faGOLD\uff08Global Oriented Local Distribution Learning\uff09\u6846\u67b6\uff1a1\uff09\u7cbe\u7ec6\u63a2\u7d22\u5ba2\u6237\u7aef\u6f5c\u5728\u7684\u4e0d\u5b8c\u6574\u5c40\u90e8\u805a\u7c7b\u5206\u5e03\uff1b2\uff09\u5c06\u5206\u5e03\u6458\u8981\u4e0a\u4f20\u81f3\u670d\u52a1\u5668\u8fdb\u884c\u5168\u5c40\u878d\u5408\uff1b3\uff09\u5728\u5168\u5c40\u5206\u5e03\u6307\u5bfc\u4e0b\u8fdb\u884c\u5c40\u90e8\u805a\u7c7b\u589e\u5f3a\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff08\u663e\u8457\u6027\u68c0\u9a8c\u3001\u6d88\u878d\u7814\u7a76\u3001\u53ef\u6269\u5c55\u6027\u8bc4\u4f30\u3001\u5b9a\u6027\u7ed3\u679c\u7b49\uff09\u8bc1\u660e\u4e86GOLD\u7684\u4f18\u8d8a\u6027\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3Non-ICD\u95ee\u9898\u5e76\u63d0\u5347\u8054\u90a6\u805a\u7c7b\u6027\u80fd\u3002", "conclusion": "\u8be5\u8bba\u6587\u63ed\u793a\u4e86\u8054\u90a6\u805a\u7c7b\u4e2d\u66f4\u9690\u853d\u7684Non-ICD\u73b0\u8c61\uff0c\u63d0\u51fa\u7684GOLD\u6846\u67b6\u4e3a\u5904\u7406\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u4e0b\u7684\u8054\u90a6\u805a\u7c7b\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5168\u5c40\u5bfc\u5411\u7684\u5c40\u90e8\u5206\u5e03\u5b66\u4e60\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u77e5\u8bc6\u878d\u5408\u548c\u805a\u7c7b\u6027\u80fd\u3002"}}
{"id": "2601.18595", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18595", "abs": "https://arxiv.org/abs/2601.18595", "authors": ["Joseph Cotnareanu", "Didier Chetelat", "Yingxue Zhang", "Mark Coates"], "title": "A Balanced Neuro-Symbolic Approach for Commonsense Abductive Logic", "comment": null, "summary": "Although Large Language Models (LLMs) have demonstrated impressive formal reasoning abilities, they often break down when problems require complex proof planning. One promising approach for improving LLM reasoning abilities involves translating problems into formal logic and using a logic solver. Although off-the-shelf logic solvers are in principle substantially more efficient than LLMs at logical reasoning, they assume that all relevant facts are provided in a question and are unable to deal with missing commonsense relations. In this work, we propose a novel method that uses feedback from the logic solver to augment a logic problem with commonsense relations provided by the LLM, in an iterative manner. This involves a search procedure through potential commonsense assumptions to maximize the chance of finding useful facts while keeping cost tractable. On a collection of pure-logical reasoning datasets, from which some commonsense information has been removed, our method consistently achieves considerable improvements over existing techniques, demonstrating the value in balancing neural and symbolic elements when working in human contexts.", "AI": {"tldr": "A novel method combining LLMs and logic solvers through iterative feedback to augment logical problems with commonsense relations, improving reasoning on complex proof planning tasks.", "motivation": "LLMs struggle with complex proof planning despite good formal reasoning abilities. Logic solvers are efficient but cannot handle missing commonsense relations that are often needed in human contexts.", "method": "An iterative approach that uses logic solver feedback to guide LLMs in adding commonsense relations to logic problems, with a search procedure to find useful facts while maintaining computational tractability.", "result": "Consistent considerable improvements over existing techniques on pure-logical reasoning datasets with removed commonsense information.", "conclusion": "The method demonstrates value in balancing neural (LLM) and symbolic (logic solver) elements for reasoning in human contexts where commonsense knowledge is essential."}}
{"id": "2601.17563", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17563", "abs": "https://arxiv.org/abs/2601.17563", "authors": ["Nathan Gavenski", "Matteo Leonetti", "Odinaldo Rodrigues"], "title": "Towards Generalisable Imitation Learning Through Conditioned Transition Estimation and Online Behaviour Alignment", "comment": "The 25th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS 2026)", "summary": "State-of-the-art imitation learning from observation methods (ILfO) have recently made significant progress, but they still have some limitations: they need action-based supervised optimisation, assume that states have a single optimal action, and tend to apply teacher actions without full consideration of the actual environment state. While the truth may be out there in observed trajectories, existing methods struggle to extract it without supervision. In this work, we propose Unsupervised Imitation Learning from Observation (UfO) that addresses all of these limitations. UfO learns a policy through a two-stage process, in which the agent first obtains an approximation of the teacher's true actions in the observed state transitions, and then refines the learned policy further by adjusting agent trajectories to closely align them with the teacher's. Experiments we conducted in five widely used environments show that UfO not only outperforms the teacher and all other ILfO methods but also displays the smallest standard deviation. This reduction in standard deviation indicates better generalisation in unseen scenarios.", "AI": {"tldr": "Proposes UfO, an unsupervised imitation learning method that learns from observation without action labels using a two-stage process: first approximating teacher actions from state transitions, then refining the policy by aligning agent trajectories with the teacher. It outperforms existing methods and the teacher itself while showing better generalization.", "motivation": "Existing imitation learning from observation (ILfO) methods have three key limitations: requiring action-based supervised optimization, assuming single optimal actions per state, and applying teacher actions without considering actual environment states. They struggle to extract meaningful patterns from observed trajectories without supervision.", "method": "UfO uses a two-stage unsupervised approach: (1) approximating the teacher's true actions from observed state transitions, and (2) refining the policy by adjusting agent trajectories to closely match the teacher's demonstrated trajectories.", "result": "In five standard environments, UfO outperforms both the teacher and all compared ILfO methods, achieving the smallest standard deviation, indicating superior performance stability and better generalization to unseen scenarios.", "conclusion": "UfO successfully overcomes key limitations of existing ILfO methods by learning effectively from unsupervised observation data, demonstrating that high-quality imitation policies can be learned without action labels while achieving better generalization."}}
{"id": "2601.18608", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18608", "abs": "https://arxiv.org/abs/2601.18608", "authors": ["Fabian Fumagalli", "R. Teal Witter", "Christopher Musco"], "title": "PolySHAP: Extending KernelSHAP with Interaction-Informed Polynomial Regression", "comment": null, "summary": "Shapley values have emerged as a central game-theoretic tool in explainable AI (XAI). However, computing Shapley values exactly requires $2^d$ game evaluations for a model with $d$ features. Lundberg and Lee's KernelSHAP algorithm has emerged as a leading method for avoiding this exponential cost. KernelSHAP approximates Shapley values by approximating the game as a linear function, which is fit using a small number of game evaluations for random feature subsets.\n  In this work, we extend KernelSHAP by approximating the game via higher degree polynomials, which capture non-linear interactions between features. Our resulting PolySHAP method yields empirically better Shapley value estimates for various benchmark datasets, and we prove that these estimates are consistent.\n  Moreover, we connect our approach to paired sampling (antithetic sampling), a ubiquitous modification to KernelSHAP that improves empirical accuracy. We prove that paired sampling outputs exactly the same Shapley value approximations as second-order PolySHAP, without ever fitting a degree 2 polynomial. To the best of our knowledge, this finding provides the first strong theoretical justification for the excellent practical performance of the paired sampling heuristic.", "AI": {"tldr": "The paper extends KernelSHAP by using polynomial approximations (PolySHAP) to capture non-linear feature interactions for better Shapley value estimation, and proves paired sampling is equivalent to second-order PolySHAP, providing the first theoretical justification for this heuristic.", "motivation": "Exact Shapley value computation requires exponential cost (2^d evaluations), and while KernelSHAP avoids this with linear approximation, it cannot capture non-linear feature interactions, limiting accuracy.", "method": "Proposes PolySHAP which approximates the game function using higher-degree polynomials to model non-linear feature interactions, and theoretically connects this approach to paired sampling (antithetic sampling).", "result": "PolySHAP yields empirically better and consistent Shapley value estimates on benchmark datasets; paired sampling is mathematically equivalent to second-order PolySHAP, explaining its practical success.", "conclusion": "The work provides both an improved estimation method (PolySHAP) and the first strong theoretical foundation for the widely-used paired sampling heuristic in KernelSHAP."}}
{"id": "2601.17570", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17570", "abs": "https://arxiv.org/abs/2601.17570", "authors": ["Hadi Salloum", "Ali Jnadi", "Yaroslav Kholodov", "Alexander Gasnikov"], "title": "Quantum-Inspired Episode Selection for Monte Carlo Reinforcement Learning via QUBO Optimization", "comment": "Proceedings of Machine Learning Research tbd: 1_13, 2025 International Conference on Computational Optimization", "summary": "Monte Carlo (MC) reinforcement learning suffers from high sample complexity, especially in environments with sparse rewards, large state spaces, and correlated trajectories. We address these limitations by reformulating episode selection as a Quadratic Unconstrained Binary Optimization (QUBO) problem and solving it with quantum-inspired samplers. Our method, MC+QUBO, integrates a combinatorial filtering step into standard MC policy evaluation: from each batch of trajectories, we select a subset that maximizes cumulative reward while promoting state-space coverage. This selection is encoded as a QUBO, where linear terms favor high-reward episodes and quadratic terms penalize redundancy. We explore both Simulated Quantum Annealing (SQA) and Simulated Bifurcation (SB) as black-box solvers within this framework. Experiments in a finite-horizon GridWorld demonstrate that MC+QUBO outperforms vanilla MC in convergence speed and final policy quality, highlighting the potential of quantum-inspired optimization as a decision-making subroutine in reinforcement learning.", "AI": {"tldr": "\u63d0\u51faMC+QUBO\u65b9\u6cd5\uff0c\u5c06\u8499\u7279\u5361\u6d1b\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8f68\u8ff9\u9009\u62e9\u5efa\u6a21\u4e3aQUBO\u95ee\u9898\uff0c\u5229\u7528\u91cf\u5b50\u542f\u53d1\u91c7\u6837\u5668\uff08SQA\u548cSB\uff09\u7b5b\u9009\u9ad8\u5956\u52b1\u4e14\u591a\u6837\u5316\u7684\u8f68\u8ff9\u5b50\u96c6\uff0c\u4ece\u800c\u964d\u4f4e\u6837\u672c\u590d\u6742\u5ea6\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u8499\u7279\u5361\u6d1b\u5f3a\u5316\u5b66\u4e60\u5728\u7a00\u758f\u5956\u52b1\u3001\u5927\u72b6\u6001\u7a7a\u95f4\u53ca\u8f68\u8ff9\u76f8\u5173\u7684\u573a\u666f\u4e0b\u5b58\u5728\u6837\u672c\u590d\u6742\u5ea6\u9ad8\u7684\u56fa\u6709\u7f3a\u9677\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5e94\u7528\u6548\u7387\u3002", "method": "\u5c06episode\u9009\u62e9\u91cd\u65b0\u8868\u8ff0\u4e3a\u4e8c\u6b21\u65e0\u7ea6\u675f\u4e8c\u8fdb\u5236\u4f18\u5316\uff08QUBO\uff09\u95ee\u9898\uff1a\u7ebf\u6027\u9879\u5956\u52b1\u9ad8\u56de\u62a5\u8f68\u8ff9\uff0c\u4e8c\u6b21\u9879\u60e9\u7f5a\u5197\u4f59\u4ee5\u4fdd\u969c\u72b6\u6001\u7a7a\u95f4\u8986\u76d6\uff1b\u91c7\u7528\u6a21\u62df\u91cf\u5b50\u9000\u706b\uff08SQA\uff09\u548c\u6a21\u62df\u5206\u53c9\uff08SB\uff09\u4f5c\u4e3a\u9ed1\u76d2\u6c42\u89e3\u5668\uff0c\u5728\u6807\u51c6\u8499\u7279\u5361\u6d1b\u7b56\u7565\u8bc4\u4f30\u4e2d\u5d4c\u5165\u7ec4\u5408\u8fc7\u6ee4\u6b65\u9aa4\u3002", "result": "\u5728\u6709\u9650\u65f6\u57dfGridWorld\u5b9e\u9a8c\u4e2d\uff0cMC+QUBO\u76f8\u6bd4\u4f20\u7edf\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u7b56\u7565\u8d28\u91cf\u3002", "conclusion": "\u91cf\u5b50\u542f\u53d1\u4f18\u5316\u53ef\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u9ad8\u6548\u51b3\u7b56\u5b50\u7a0b\u5e8f\uff0c\u4e3a\u89e3\u51b3\u6837\u672c\u6548\u7387\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.17602", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17602", "abs": "https://arxiv.org/abs/2601.17602", "authors": ["Xuanzhou Chen"], "title": "Understanding Transformer Encoder-Decoder Representations through Bernoulli Dropout", "comment": null, "summary": "We study Transformer overparameterization through the lens of angular similarity in high-dimensional encoder-decoder embeddings. We apply Bernoulli dropout between the encoder and the decoder, varying the keep probability $p$ to identify a sparsity-dependent threshold above which the Top-1 prediction is preserved. Theoretically, we prove that, if the effective sparsity embeddings is sufficiently large, and thus decoder performance, remain stable under moderate coordinate dropout. Empirically, we implement the Bernoulli dropout by constructing a new Transformer model augmented with Binary Erasure Channel (BEC) and test its performance on an English-French translation task. Experimental results visualize the trends for validation accuracies and BLEU scores, both decline sharply at some threshold.", "AI": {"tldr": "\u7814\u7a76Transformer\u8fc7\u53c2\u6570\u5316\u73b0\u8c61\uff0c\u901a\u8fc7\u89d2\u76f8\u4f3c\u5ea6\u548c\u4f2f\u52aa\u5229 dropout \u53d1\u73b0\u4e86\u4e00\u4e2a\u7a00\u758f\u6027\u4e34\u754c\u9608\u503c\uff0c\u8d85\u8fc7\u8be5\u9608\u503c\u540e\u6a21\u578b\u6027\u80fd\u4f1a\u6025\u5267\u4e0b\u964d\u3002", "motivation": "\u7406\u89e3Transformer\u8fc7\u53c2\u6570\u5316\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u5728\u7a00\u758f\u6027\uff08dropout\uff09\u4e0b\u7684\u7a33\u5b9a\u6027\uff0c\u5e76\u8bc6\u522b\u6027\u80fd\u4fdd\u6301\u4e0e\u9000\u5316\u7684\u4e34\u754c\u9608\u503c\u3002", "method": "\u4ece\u89d2\u76f8\u4f3c\u5ea6\u89d2\u5ea6\u7406\u8bba\u5206\u6790\u5750\u6807dropout\u7684\u5f71\u54cd\uff0c\u5e76\u6784\u5efa\u4e00\u4e2a\u5e26\u6709\u4e8c\u5143\u64e6\u9664\u4fe1\u9053(BEC)\u7684Transformer\u6a21\u578b\uff0c\u5728\u82f1\u6cd5\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u901a\u8fc7\u6539\u53d8dropout\u6982\u7387p\u6765\u5bfb\u627e\u6027\u80fd\u9608\u503c\u3002", "result": "\u9a8c\u8bc1\u51c6\u786e\u7387\u548cBLEU\u5206\u6570\u5728\u67d0\u4e2a\u7a00\u758f\u6027\u9608\u503c\u5904\u6025\u5267\u4e0b\u964d\uff0c\u800c\u5f53\u6709\u6548\u7a00\u758f\u6027\u8db3\u591f\u5927\u65f6\uff0cTop-1\u9884\u6d4b\u5728\u9002\u5ea6\u5750\u6807dropout\u4e0b\u4fdd\u6301\u7a33\u5b9a\u3002", "conclusion": "Transformer\u5728\u9002\u5ea6dropout\u4e0b\u56e0\u8fc7\u53c2\u6570\u5316\u800c\u8868\u73b0\u7a33\u5b9a\uff0c\u4f46\u5b58\u5728\u4e00\u4e2a\u4e34\u754c\u7a00\u758f\u6027\u9608\u503c\uff0c\u8d85\u8fc7\u540e\u6027\u80fd\u4f1a\u5d29\u6e83\uff0cBEC\u589e\u5f3a\u6a21\u578b\u5b9e\u8bc1\u4e86\u8fd9\u4e00\u8d8b\u52bf\u3002"}}
{"id": "2601.18630", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.18630", "abs": "https://arxiv.org/abs/2601.18630", "authors": ["Abeer Badawi", "Md Tahmid Rahman Laskar", "Elahe Rahimi", "Sheri Grach", "Lindsay Bertrand", "Lames Danok", "Frank Rudzicz", "Jimmy Huang", "Elham Dolatabadi"], "title": "Assessing the Quality of Mental Health Support in LLM Responses through Multi-Attribute Human Evaluation", "comment": null, "summary": "The escalating global mental health crisis, marked by persistent treatment gaps, availability, and a shortage of qualified therapists, positions Large Language Models (LLMs) as a promising avenue for scalable support. While LLMs offer potential for accessible emotional assistance, their reliability, therapeutic relevance, and alignment with human standards remain challenging to address. This paper introduces a human-grounded evaluation methodology designed to assess LLM generated responses in therapeutic dialogue. Our approach involved curating a dataset of 500 mental health conversations from datasets with real-world scenario questions and evaluating the responses generated by nine diverse LLMs, including closed source and open source models. More specifically, these responses were evaluated by two psychiatric trained experts, who independently rated each on a 5 point Likert scale across a comprehensive 6 attribute rubric. This rubric captures Cognitive Support and Affective Resonance, providing a multidimensional perspective on therapeutic quality. Our analysis reveals that LLMs provide strong cognitive reliability by producing safe, coherent, and clinically appropriate information, but they demonstrate unstable affective alignment. Although closed source models (e.g., GPT-4o) offer balanced therapeutic responses, open source models show greater variability and emotional flatness. We reveal a persistent cognitive-affective gap and highlight the need for failure aware, clinically grounded evaluation frameworks that prioritize relational sensitivity alongside informational accuracy in mental health oriented LLMs. We advocate for balanced evaluation protocols with human in the loop that center on therapeutic sensitivity and provide a framework to guide the responsible design and clinical oversight of mental health oriented conversational AI.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4e13\u5bb6\u8bc4\u4f309\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u5728500\u4e2a\u5fc3\u7406\u5065\u5eb7\u5bf9\u8bdd\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0LLMs\u5177\u5907\u5f3a\u5927\u7684\u8ba4\u77e5\u53ef\u9760\u6027\uff0c\u4f46\u5728\u60c5\u611f\u5171\u9e23\u65b9\u9762\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u5b58\u5728\u8ba4\u77e5-\u60c5\u611f\u9e3f\u6c9f\uff0c\u9700\u8981\u5efa\u7acb\u4ee5\u4e34\u5e8a\u4e3a\u57fa\u7840\u3001\u91cd\u89c6\u5173\u7cfb\u654f\u611f\u6027\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u5168\u7403\u5fc3\u7406\u5065\u5eb7\u5371\u673a\u6301\u7eed\u52a0\u5267\uff0c\u5b58\u5728\u6cbb\u7597\u7f3a\u53e3\u3001\u8d44\u6e90\u53ef\u53ca\u6027\u4e0d\u8db3\u548c\u4e13\u4e1a\u6cbb\u7597\u5e08\u77ed\u7f3a\u7b49\u95ee\u9898\u3002\u5927\u8bed\u8a00\u6a21\u578b\u867d\u4e3a\u53ef\u6269\u5c55\u7684\u5fc3\u7406\u652f\u6301\u63d0\u4f9b\u4e86\u524d\u666f\uff0c\u4f46\u5176\u53ef\u9760\u6027\u3001\u6cbb\u7597\u76f8\u5173\u6027\u53ca\u4e0e\u4eba\u7c7b\u6807\u51c6\u7684\u5bf9\u9f50\u4ecd\u9762\u4e34\u6311\u6218\u3002", "method": "\u91c7\u7528\u4ee5\u4eba\u4e3a\u672c\u7684\u8bc4\u4f30\u65b9\u6cd5\uff1a\u4ece\u771f\u5b9e\u4e16\u754c\u573a\u666f\u6570\u636e\u96c6\u4e2d\u6574\u7406500\u4e2a\u5fc3\u7406\u5065\u5eb7\u5bf9\u8bdd\uff0c\u75319\u4e2a\u4e0d\u540c\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5305\u62ec\u95ed\u6e90\u548c\u5f00\u6e90\u6a21\u578b\uff09\u751f\u6210\u56de\u590d\u3002\u7531\u4e24\u540d\u53d7\u8fc7\u7cbe\u795e\u75c5\u5b66\u57f9\u8bad\u4e13\u5bb6\uff0c\u4f7f\u7528\u5305\u542b6\u4e2a\u5c5e\u6027\u7684\u8bc4\u4f30\u91cf\u8868\uff0c\u72ec\u7acb\u5bf9\u6bcf\u4e2a\u56de\u590d\u8fdb\u884c5\u70b9\u674e\u514b\u7279\u91cf\u8868\u8bc4\u5206\uff0c\u91cd\u70b9\u5173\u6ce8\u8ba4\u77e5\u652f\u6301\u548c\u60c5\u611f\u5171\u9e23\u4e24\u4e2a\u7ef4\u5ea6\u3002", "result": "LLMs\u5728\u8ba4\u77e5\u53ef\u9760\u6027\u65b9\u9762\u8868\u73b0\u5f3a\u52b2\uff0c\u80fd\u63d0\u4f9b\u5b89\u5168\u3001\u8fde\u8d2f\u4e14\u4e34\u5e8a\u9002\u5b9c\u7684\u4fe1\u606f\uff0c\u4f46\u5728\u60c5\u611f\u5bf9\u9f50\u65b9\u9762\u4e0d\u7a33\u5b9a\u3002\u95ed\u6e90\u6a21\u578b\uff08\u5982GPT-4o\uff09\u80fd\u63d0\u4f9b\u5e73\u8861\u7684\u6cbb\u7597\u6027\u56de\u590d\uff0c\u800c\u5f00\u6e90\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u5927\u7684\u53d8\u5f02\u6027\u548c\u60c5\u611f\u5e73\u6de1\u3002\u7814\u7a76\u63ed\u793a\u4e86\u6301\u7eed\u7684\u8ba4\u77e5-\u60c5\u611f\u9e3f\u6c9f\u3002", "conclusion": "\u5f3a\u8c03\u9700\u8981\u5efa\u7acb\u5177\u6709\u4e34\u5e8a\u57fa\u7840\u3001\u91cd\u89c6\u5173\u7cfb\u654f\u611f\u6027\u800c\u975e\u4ec5\u4fe1\u606f\u51c6\u786e\u6027\u7684\u8bc4\u4f30\u6846\u67b6\u3002\u5021\u5bfc\u91c7\u7528\u4eba\u673a\u534f\u540c\u7684\u5e73\u8861\u8bc4\u4f30\u534f\u8bae\uff0c\u4e3a\u5fc3\u7406\u5065\u5eb7\u5bfc\u5411\u7684\u5bf9\u8bddAI\u63d0\u4f9b\u8d1f\u8d23\u4efb\u7684\u8bbe\u8ba1\u548c\u4e34\u5e8a\u76d1\u7763\u6307\u5bfc\u6846\u67b6\u3002"}}
{"id": "2601.17607", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17607", "abs": "https://arxiv.org/abs/2601.17607", "authors": ["Daisuke Okanohara"], "title": "A Thermodynamic Theory of Learning I: Irreversible Ensemble Transport and Epistemic Costs", "comment": "9 pages. Part I of a planned series entitled \"A Thermodynamic Theory of Learning.\"", "summary": "Learning systems acquire structured internal representations from data, yet classical information-theoretic results state that deterministic transformations do not increase information. This raises a fundamental question: how can learning produce abstraction and insight without violating information-theoretic limits?\n  We argue that learning is inherently an irreversible process when performed over finite time, and that the realization of epistemic structure necessarily incurs entropy production. To formalize this perspective, we model learning as a transport process in the space of probability distributions over model configurations and introduce an epistemic free-energy framework.\n  Within this framework, we define the free-energy drop as a bookkeeping quantity that records the total reduction of epistemic free energy along a learning trajectory. This reduction decomposes into a reversible component associated with potential improvement and an irreversible component corresponding to entropy production.\n  We then derive the Epistemic Speed Limit (ESL), a finite-time inequality that lower-bounds the minimal entropy production required by any learning process to realize a given distributional transformation. This bound depends only on the Wasserstein distance between initial and final ensemble distributions and is independent of the specific learning algorithm.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5f15\u5165\u8ba4\u77e5\u81ea\u7531\u80fd\u6846\u67b6\uff0c\u5c06\u5b66\u4e60\u5efa\u6a21\u4e3a\u6982\u7387\u5206\u5e03\u7a7a\u95f4\u4e2d\u7684\u4e0d\u53ef\u9006\u8f93\u8fd0\u8fc7\u7a0b\uff0c\u63a8\u5bfc\u51fa\u8ba4\u77e5\u901f\u5ea6\u6781\u9650(ESL)\uff0c\u4ece\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u4efb\u4f55\u6709\u9650\u65f6\u95f4\u5b66\u4e60\u8fc7\u7a0b\u90fd\u5fc5\u987b\u4ea7\u751f\u71b5\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u5b66\u4e60\u5982\u4f55\u5728\u4e0d\u8fdd\u53cd\u4fe1\u606f\u8bba\u9650\u5236\u7684\u60c5\u51b5\u4e0b\u4ea7\u751f\u62bd\u8c61\u548c\u6d1e\u5bdf\u8fd9\u4e00\u6839\u672c\u95ee\u9898\u3002", "motivation": "\u7ecf\u5178\u4fe1\u606f\u8bba\u8ba4\u4e3a\u786e\u5b9a\u6027\u53d8\u6362\u4e0d\u589e\u52a0\u4fe1\u606f\uff0c\u4f46\u5b66\u4e60\u7cfb\u7edf\u5374\u80fd\u4ece\u6570\u636e\u4e2d\u83b7\u53d6\u7ed3\u6784\u5316\u8868\u5f81\u5e76\u4ea7\u751f\u62bd\u8c61\u6d1e\u5bdf\u3002\u8fd9\u4e00\u77db\u76fe\u63ed\u793a\u4e86\u73b0\u6709\u7406\u8bba\u6846\u67b6\u7684\u4e0d\u8db3\uff0c\u9700\u8981\u65b0\u7684\u89c6\u89d2\u6765\u89e3\u91ca\u5b66\u4e60\u8fc7\u7a0b\u5982\u4f55\u5728\u4e0d\u8fdd\u53cd\u4fe1\u606f\u8bba\u57fa\u672c\u9650\u5236\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u77e5\u8bc6\u83b7\u53d6\u3002", "method": "\u5c06\u5b66\u4e60\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u6a21\u578b\u914d\u7f6e\u7a7a\u95f4\u4e0a\u7684\u6982\u7387\u5206\u5e03\u8f93\u8fd0\u8fc7\u7a0b\uff0c\u6784\u5efa\u8ba4\u77e5\u81ea\u7531\u80fd\u7406\u8bba\u6846\u67b6\u3002\u5728\u8be5\u6846\u67b6\u4e0b\u5b9a\u4e49\u81ea\u7531\u80fd\u964d\u4f5c\u4e3a\u6838\u7b97\u91cf\uff0c\u5c06\u5176\u5206\u89e3\u4e3a\u4e0e\u52bf\u80fd\u6539\u8fdb\u76f8\u5173\u7684\u53ef\u9006\u5206\u91cf\u548c\u5bf9\u5e94\u71b5\u4ea7\u751f\u7684\u4e0d\u53ef\u9006\u5206\u91cf\uff0c\u5e76\u57fa\u4e8e\u6b64\u63a8\u5bfc\u51fa\u6709\u9650\u65f6\u95f4\u4e0d\u7b49\u5f0f\u3002", "result": "\u63a8\u5bfc\u51fa\u8ba4\u77e5\u901f\u5ea6\u6781\u9650(ESL)\uff0c\u8be5\u4e0d\u7b49\u5f0f\u7ed9\u51fa\u4e86\u4efb\u4f55\u5b66\u4e60\u8fc7\u7a0b\u4e3a\u5b9e\u73b0\u7279\u5b9a\u5206\u5e03\u53d8\u6362\u6240\u9700\u7684\u6700\u5c0f\u71b5\u4ea7\u751f\u4e0b\u9650\u3002\u8be5\u754c\u9650\u4ec5\u53d6\u51b3\u4e8e\u521d\u59cb\u548c\u6700\u7ec8\u7cfb\u7efc\u5206\u5e03\u4e4b\u95f4\u7684Wasserstein\u8ddd\u79bb\uff0c\u4e0e\u5177\u4f53\u5b66\u4e60\u7b97\u6cd5\u65e0\u5173\uff0c\u63ed\u793a\u4e86\u5b66\u4e60\u8fc7\u7a0b\u7684\u6839\u672c\u70ed\u529b\u5b66\u7ea6\u675f\u3002", "conclusion": "\u5b66\u4e60\u672c\u8d28\u4e0a\u662f\u6709\u9650\u65f6\u95f4\u5185\u7684\u4e0d\u53ef\u9006\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u8ba4\u77e5\u7ed3\u6784\u5fc5\u7136\u4f34\u968f\u71b5\u4ea7\u751f\u3002\u8ba4\u77e5\u81ea\u7531\u80fd\u6846\u67b6\u548cESL\u754c\u9650\u4e3a\u7406\u89e3\u5b66\u4e60\u7684\u70ed\u529b\u5b66\u4ee3\u4ef7\u63d0\u4f9b\u4e86\u666e\u9002\u7406\u8bba\uff0c\u8868\u660e\u4efb\u4f55\u5b9e\u9645\u5b66\u4e60\u7cfb\u7edf\u90fd\u9762\u4e34\u57fa\u672c\u7684\u7269\u7406\u9650\u5236\uff0c\u65e0\u6cd5\u5728\u4e0d\u4ea7\u751f\u71b5\u7684\u60c5\u51b5\u4e0b\u5b8c\u6210\u62bd\u8c61\u5316\u8fc7\u7a0b\u3002"}}
{"id": "2601.18631", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.18631", "abs": "https://arxiv.org/abs/2601.18631", "authors": ["Mingyang Song", "Haoyu Sun", "Jiawei Gu", "Linjie Li", "Luxin Xu", "Ranjay Krishna", "Yu Cheng"], "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning", "comment": "28 pages, 10 figures and 13 tables", "summary": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw.", "AI": {"tldr": "AdaReasoner\u901a\u8fc7\u5c06\u5de5\u5177\u4f7f\u7528\u5b66\u4e60\u4e3a\u901a\u7528\u63a8\u7406\u6280\u80fd\u800c\u975e\u7279\u5b9a\u5de5\u5177\u884c\u4e3a\uff0c\u5b9e\u73b0\u4e86\u89c6\u89c9\u63a8\u7406\u7684\u7a81\u7834\u3002\u5b83\u91c7\u7528\u53ef\u6269\u5c55\u6570\u636e\u7ba1\u9053\u3001Tool-GRPO\u5f3a\u5316\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u673a\u5236\uff0c\u4f7f\u6a21\u578b\u80fd\u81ea\u4e3b\u9009\u62e9\u548c\u534f\u8c03\u5de5\u5177\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8aGPT-5\uff0c\u6027\u80fd\u63d0\u5347\u8fbe24.9%\u3002", "motivation": "\u5f53\u4eba\u7c7b\u9047\u5230\u8d85\u51fa\u76f4\u63a5\u80fd\u529b\u7684\u95ee\u9898\u65f6\uff0c\u4f1a\u4f9d\u8d56\u5de5\u5177\u3002\u5bf9\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6709\u6548\u7684\u89c6\u89c9\u63a8\u7406\u5173\u952e\u5728\u4e8e\u77e5\u9053\u4f55\u65f6\u4f7f\u7528\u4f55\u79cd\u5de5\u5177\u4ee5\u53ca\u5982\u4f55\u591a\u6b65\u7ec4\u5408\u5de5\u5177\uff0c\u5c24\u5176\u662f\u5728\u9762\u5bf9\u65b0\u5de5\u5177\u6216\u65b0\u4efb\u52a1\u65f6\u3002", "method": "AdaReasoner\u901a\u8fc7\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\u5b9e\u73b0\uff1a(i)\u53ef\u6269\u5c55\u7684\u6570\u636e\u6784\u5efa\u7ba1\u9053\uff0c\u8ba9\u6a21\u578b\u63a5\u89e6\u957f\u5468\u671f\u3001\u591a\u6b65\u5de5\u5177\u4ea4\u4e92\uff1b(ii)Tool-GRPO\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u57fa\u4e8e\u6700\u7ec8\u4efb\u52a1\u6210\u529f\u4f18\u5316\u5de5\u5177\u9009\u62e9\u548c\u6392\u5e8f\uff1b(iii)\u81ea\u9002\u5e94\u5b66\u4e60\u673a\u5236\uff0c\u52a8\u6001\u8c03\u8282\u5de5\u5177\u4f7f\u7528\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAdaReasoner\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u5de5\u5177\u9002\u5e94\u548c\u6cdb\u5316\u80fd\u529b\uff1a\u5b83\u80fd\u81ea\u4e3b\u91c7\u7528\u6709\u76ca\u5de5\u5177\u3001\u6291\u5236\u65e0\u5173\u5de5\u5177\uff0c\u5e76\u6839\u636e\u4efb\u52a1\u9700\u6c42\u8c03\u6574\u5de5\u5177\u4f7f\u7528\u9891\u7387\u3002\u5728\u591a\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e73\u5747\u63d0\u53477B\u57fa\u7840\u6a21\u578b24.9%\uff0c\u5e76\u5728VSP\u548cJigsaw\u7b49\u4efb\u52a1\u4e0a\u8d85\u8d8aGPT-5\u3002", "conclusion": "\u8fd9\u4e9b\u7ec4\u4ef6\u4f7f\u6a21\u578b\u80fd\u591f\u4ece\u4efb\u52a1\u4e0a\u4e0b\u6587\u548c\u4e2d\u95f4\u7ed3\u679c\u4e2d\u63a8\u65ad\u5de5\u5177\u6548\u7528\uff0c\u5b9e\u73b0\u591a\u5de5\u5177\u534f\u8c03\u548c\u672a\u89c1\u5de5\u5177\u7684\u6cdb\u5316\uff0c\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2601.17616", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17616", "abs": "https://arxiv.org/abs/2601.17616", "authors": ["Fatema Siddika", "Md Anwar Hossen", "Tanwi Mallick", "Ali Jannesari"], "title": "Split-on-Share: Mixture of Sparse Experts for Task-Agnostic Continual Learning", "comment": "17 pages, 9 figures, 8 tables", "summary": "Continual learning in Large Language Models (LLMs) is hindered by the plasticity-stability dilemma, where acquiring new capabilities often leads to catastrophic forgetting of previous knowledge. Existing methods typically treat parameters uniformly, failing to distinguish between specific task knowledge and shared capabilities. We introduce Mixture of Sparse Experts for Task-Agnostic Continual Learning, referred to as SETA, a framework that resolves the plasticity-stability conflict by decomposing the model into modular subspaces. Unlike standard updates, where tasks compete for the same parameters, SETA separates knowledge into unique experts, designed to isolate task-specific patterns, and shared experts, responsible for capturing common features. This structure is maintained through elastic weight anchoring, which protects critical shared knowledge and enables a unified gating network to automatically retrieve the correct expert combination for each task during inference. Extensive experiments across diverse domain-specific and general benchmarks demonstrate that SETA consistently outperforms state-of-the-art parameter-efficient fine-tuning-based continual learning methods.", "AI": {"tldr": "SETAsolves catastrophic forgetting in LLMs by using Mixture of Sparse Experts to separate task-specific and shared knowledge through modular subspaces, outperforming existing parameter-efficient methods.", "motivation": "Continual learning in LLMs suffers from the plasticity-stability dilemma where acquiring new capabilities causes catastrophic forgetting of prior knowledge, and existing methods fail to distinguish between task-specific and shared parameters.", "method": "SETA decomposes models into modular subspaces with unique experts for task-specific patterns and shared experts for common features, protected by elastic weight anchoring and a unified gating network for automatic expert retrieval during inference.", "result": "Extensive experiments across diverse benchmarks demonstrate SETA consistently outperforms state-of-the-art parameter-efficient fine-tuning-based continual learning methods.", "conclusion": "The modular subspace architecture effectively resolves the plasticity-stability conflict and provides a promising framework for task-agnostic continual learning in LLMs."}}
{"id": "2601.18642", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18642", "abs": "https://arxiv.org/abs/2601.18642", "authors": ["Lei Wei", "Xu Dong", "Xiao Peng", "Niantao Xie", "Bin Wang"], "title": "FadeMem: Biologically-Inspired Forgetting for Efficient Agent Memory", "comment": null, "summary": "Large language models deployed as autonomous agents face critical memory limitations, lacking selective forgetting mechanisms that lead to either catastrophic forgetting at context boundaries or information overload within them. While human memory naturally balances retention and forgetting through adaptive decay processes, current AI systems employ binary retention strategies that preserve everything or lose it entirely. We propose FadeMem, a biologically-inspired agent memory architecture that incorporates active forgetting mechanisms mirroring human cognitive efficiency. FadeMem implements differential decay rates across a dual-layer memory hierarchy, where retention is governed by adaptive exponential decay functions modulated by semantic relevance, access frequency, and temporal patterns. Through LLM-guided conflict resolution and intelligent memory fusion, our system consolidates related information while allowing irrelevant details to fade. Experiments on Multi-Session Chat, LoCoMo, and LTI-Bench demonstrate superior multi-hop reasoning and retrieval with 45\\% storage reduction, validating the effectiveness of biologically-inspired forgetting in agent memory systems.", "AI": {"tldr": "This paper proposes FadeMem, a biologically-inspired memory architecture for LLM agents that implements active forgetting through adaptive exponential decay across a dual-layer hierarchy, achieving 45% storage reduction while improving multi-hop reasoning and retrieval performance.", "motivation": "LLM agents lack selective forgetting mechanisms, suffering from either catastrophic forgetting or information overload due to binary retention strategies, unlike human memory's adaptive decay processes that balance retention and forgetting.", "method": "FadeMem uses differential decay rates in a dual-layer memory hierarchy governed by adaptive exponential decay functions based on semantic relevance, access frequency, and temporal patterns, with LLM-guided conflict resolution and intelligent memory fusion.", "result": "Experiments on Multi-Session Chat, LoCoMo, and LTI-Bench show superior multi-hop reasoning and retrieval performance with 45% storage reduction.", "conclusion": "Biologically-inspired active forgetting mechanisms are effective for agent memory systems, improving efficiency and performance while reducing storage requirements."}}
{"id": "2601.17625", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17625", "abs": "https://arxiv.org/abs/2601.17625", "authors": ["Yuhan Xie", "Jinhan Liu", "Xiaoyong Ni", "Fei Tan", "Icare Sakr", "Thibault Collin", "Shiqi Sun", "Alejandro Rodriguez Guajardo", "Demon Fanny", "Charles-francois Vincent Latchoumane", "Henri Lorach", "Jocelyne Bloch", "Gregoire Courtine", "Mahsa Shoaran"], "title": "BrainDistill: Implantable Motor Decoding with Task-Specific Knowledge Distillation", "comment": "21 pages,7 figures", "summary": "Transformer-based neural decoders with large parameter counts, pre-trained on large-scale datasets, have recently outperformed classical machine learning models and small neural networks on brain-computer interface (BCI) tasks. However, their large parameter counts and high computational demands hinder deployment in power-constrained implantable systems. To address this challenge, we introduce BrainDistill, a novel implantable motor decoding pipeline that integrates an implantable neural decoder (IND) with a task-specific knowledge distillation (TSKD) framework. Unlike standard feature distillation methods that attempt to preserve teacher representations in full, TSKD explicitly prioritizes features critical for decoding through supervised projection. Across multiple neural datasets, IND consistently outperforms prior neural decoders on motor decoding tasks, while its TSKD-distilled variant further surpasses alternative distillation methods in few-shot calibration settings. Finally, we present a quantization-aware training scheme that enables integer-only inference with activation clipping ranges learned during training. The quantized IND enables deployment under the strict power constraints of implantable BCIs with minimal performance loss.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faBrainDistill\uff0c\u4e00\u4e2a\u7528\u4e8e\u690d\u5165\u5f0f\u8111\u673a\u63a5\u53e3\u7684\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\u84b8\u998f\u548c\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff0c\u521b\u9020\u51fa\u4f4e\u529f\u8017\u795e\u7ecf\u89e3\u7801\u5668\uff0c\u5728\u6ee1\u8db3\u529f\u7387\u7ea6\u675f\u7684\u540c\u65f6\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7528\u4e8e\u8111\u673a\u63a5\u53e3\u7684\u5927\u578b\u57fa\u4e8eTransformer\u7684\u795e\u7ecf\u89e3\u7801\u5668\u5177\u6709\u9ad8\u8ba1\u7b97\u9700\u6c42\u548c\u529f\u8017\uff0c\u4f7f\u5176\u4e0d\u9002\u5408\u5177\u6709\u4e25\u683c\u529f\u7387\u7ea6\u675f\u7684\u690d\u5165\u5f0f\u7cfb\u7edf\u3002", "method": "\u4f5c\u8005\u5f15\u5165BrainDistill\uff0c\u6574\u5408\u4e86\uff1a(1) \u690d\u5165\u5f0f\u795e\u7ecf\u89e3\u7801\u5668(IND)\uff0c(2) \u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\u84b8\u998f(TSKD)\u6846\u67b6\uff0c\u901a\u8fc7\u76d1\u7763\u6295\u5f71\u4f18\u5148\u5904\u7406\u89e3\u7801\u5173\u952e\u7279\u5f81\uff0c(3) \u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u65b9\u6848\uff0c\u5b9e\u73b0\u5177\u6709\u5b66\u4e60\u6fc0\u6d3b\u88c1\u526a\u8303\u56f4\u7684\u7eaf\u6574\u6570\u63a8\u7406\u3002", "result": "\u5728\u591a\u4e2a\u795e\u7ecf\u6570\u636e\u96c6\u4e0a\uff0cIND\u5728\u8fd0\u52a8\u89e3\u7801\u4efb\u52a1\u4e0a\u6301\u7eed\u4f18\u4e8e\u5148\u524d\u7684\u795e\u7ecf\u89e3\u7801\u5668\u3002TSKD\u84b8\u998f\u53d8\u4f53\u5728\u5c11\u6837\u672c\u6821\u51c6\u8bbe\u7f6e\u4e2d\u8d85\u8d8a\u66ff\u4ee3\u84b8\u998f\u65b9\u6cd5\u3002\u91cf\u5316\u6a21\u578b\u5728\u690d\u5165\u5f0f\u8111\u673a\u63a5\u53e3\u529f\u7387\u7ea6\u675f\u4e0b\u6210\u529f\u90e8\u7f72\uff0c\u6027\u80fd\u635f\u5931\u6781\u5c0f\u3002", "conclusion": "BrainDistill\u901a\u8fc7\u76ee\u6807\u84b8\u998f\u548c\u91cf\u5316\uff0c\u4e3a\u5728\u529f\u7387\u53d7\u9650\u7684\u690d\u5165\u5f0f\u8111\u673a\u63a5\u53e3\u7cfb\u7edf\u4e2d\u90e8\u7f72\u9ad8\u6027\u80fd\u795e\u7ecf\u89e3\u7801\u5668\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17641", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.17641", "abs": "https://arxiv.org/abs/2601.17641", "authors": ["Hao Fang", "Ryan A. Canfield", "Tomohiro Ouchi", "Beatrice Macagno", "Eli Shlizerman", "Amy L. Orsborn"], "title": "RPNT: Robust Pre-trained Neural Transformer -- A Pathway for Generalized Motor Decoding", "comment": null, "summary": "Brain decoding aims to interpret and translate neural activity into behaviors. As such, it is imperative that decoding models are able to generalize across variations, such as recordings from different brain sites, distinct sessions, different types of behavior, and a variety of subjects. Current models can only partially address these challenges and warrant the development of pretrained neural transformer models capable to adapt and generalize. In this work, we propose RPNT - Robust Pretrained Neural Transformer, designed to achieve robust generalization through pretraining, which in turn enables effective finetuning given a downstream task. In particular, RPNT unique components include 1) Multidimensional rotary positional embedding (MRoPE) to aggregate experimental metadata such as site coordinates, session name and behavior types; 2) Context-based attention mechanism via convolution kernels operating on global attention to learn local temporal structures for handling non-stationarity of neural population activity; 3) Robust self-supervised learning (SSL) objective with uniform causal masking strategies and contrastive representations. We pretrained two separate versions of RPNT on distinct datasets a) Multi-session, multi-task, and multi-subject microelectrode benchmark; b) Multi-site recordings using high-density Neuropixel 1.0 probes. The datasets include recordings from the dorsal premotor cortex (PMd) and from the primary motor cortex (M1) regions of nonhuman primates (NHPs) as they performed reaching tasks. After pretraining, we evaluated the generalization of RPNT in cross-session, cross-type, cross-subject, and cross-site downstream behavior decoding tasks. Our results show that RPNT consistently achieves and surpasses the decoding performance of existing decoding models in all tasks.", "AI": {"tldr": "The paper proposes RPNT (Robust Pretrained Neural Transformer), a novel brain decoding model that uses pretraining with specialized components (MRoPE, context-based attention, and robust SSL) to achieve robust generalization across brain sites, sessions, behavior types, and subjects, consistently outperforming existing models.", "motivation": "Current brain decoding models lack full generalization capability across variations like different brain recording sites, sessions, behavior types, and subjects, necessitating development of pretrained transformer models that can adapt and generalize effectively.", "method": "RPNT incorporates: 1) Multidimensional rotary positional embedding (MRoPE) to incorporate experimental metadata; 2) Context-based attention via convolution kernels to learn local temporal structures for handling neural non-stationarity; 3) Robust self-supervised learning with uniform causal masking and contrastive representations. The model was pretrained on two NHP datasets (microelectrode and Neuropixel recordings from PMd/M1 during reaching tasks) and evaluated on cross-session, cross-type, cross-subject, and cross-site decoding tasks.", "result": "RPNT consistently achieves and surpasses the decoding performance of existing models across all cross-domain generalization tasks tested.", "conclusion": "The proposed RPNT model successfully addresses brain decoding generalization challenges through pretraining, demonstrating superior performance and establishing an effective approach for robust neural decoding across diverse conditions."}}
{"id": "2601.18706", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18706", "abs": "https://arxiv.org/abs/2601.18706", "authors": ["Zhichao Yang", "Sepehr Janghorbani", "Dongxu Zhang", "Jun Han", "Qian Qian", "Andrew Ressler", "Gregory D. Lyng", "Sanjit Singh Batra", "Robert E. Tillman"], "title": "Health-SCORE: Towards Scalable Rubrics for Improving Health-LLMs", "comment": null, "summary": "Rubrics are essential for evaluating open-ended LLM responses, especially in safety-critical domains such as healthcare. However, creating high-quality and domain-specific rubrics typically requires significant human expertise time and development cost, making rubric-based evaluation and training difficult to scale. In this work, we introduce Health-SCORE, a generalizable and scalable rubric-based training and evaluation framework that substantially reduces rubric development costs without sacrificing performance. We show that Health-SCORE provides two practical benefits beyond standalone evaluation: it can be used as a structured reward signal to guide reinforcement learning with safety-aware supervision, and it can be incorporated directly into prompts to improve response quality through in-context learning. Across open-ended healthcare tasks, Health-SCORE achieves evaluation quality comparable to human-created rubrics while significantly lowering development effort, making rubric-based evaluation and training more scalable.", "AI": {"tldr": "Health-SCORE is a scalable framework for rubric-based LLM evaluation in healthcare that reduces development costs while maintaining quality, enabling both RL training and in-context learning improvements.", "motivation": "Creating high-quality, domain-specific rubrics for evaluating open-ended LLM responses in safety-critical domains like healthcare requires significant human expertise, time, and cost, limiting scalability.", "method": "The paper introduces Health-SCORE, a generalizable and scalable rubric-based framework that can be used as a structured reward signal for reinforcement learning and incorporated into prompts for in-context learning.", "result": "Health-SCORE achieves evaluation quality comparable to human-created rubrics across open-ended healthcare tasks while significantly reducing development effort.", "conclusion": "Health-SCORE makes rubric-based evaluation and training more scalable by substantially lowering costs without sacrificing performance."}}
{"id": "2601.17646", "categories": ["cs.LG", "math.FA", "math.OC", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.17646", "abs": "https://arxiv.org/abs/2601.17646", "authors": ["Karim Bounja", "Lahcen Laayouni", "Abdeljalil Sakat"], "title": "A Mosco sufficient condition for intrinsic stability of non-unique convex Empirical Risk Minimization", "comment": null, "summary": "Empirical risk minimization (ERM) stability is usually studied via single-valued outputs, while convex non-strict losses yield set-valued minimizers. We identify Painlev\u00e9-Kuratowski upper semicontinuity (PK-u.s.c.) as the intrinsic stability notion for the ERM solution correspondence (set-level Hadamard well-posedness) and a prerequisite to interpret stability of selections. We then characterize a minimal non-degenerate qualitative regime: Mosco-consistent perturbations and locally bounded minimizers imply PK-u.s.c., minimal-value continuity, and consistency of vanishing-gap near-minimizers. Quadratic growth yields explicit quantitative deviation bounds.", "AI": {"tldr": "Proposes Painlev\u00e9-Kuratowski upper semicontinuity (PK-u.s.c.) as the intrinsic stability notion for set-valued ERM solutions under non-strict convex losses, showing Mosco consistency and local boundedness ensure qualitative stability, while quadratic growth yields explicit quantitative error bounds.", "motivation": "Traditional ERM stability analysis focuses on single-valued outputs, but convex non-strict losses inherently produce set-valued minimizers, necessitating a rigorous set-level stability framework to interpret solution behavior under perturbations.", "method": "Identifies PK-u.s.c. as the core stability concept for ERM solution correspondences, analyzes it through Mosco-consistent perturbations and locally bounded minimizers, and leverages quadratic growth conditions to derive explicit deviation bounds.", "result": "Under minimal non-degenerate conditions (Mosco consistency + local boundedness), PK-u.s.c., minimal-value continuity, and consistency of vanishing-gap near-minimizers are achieved; quadratic growth further provides concrete quantitative deviation estimates for solutions.", "conclusion": "PK-u.s.c. is the essential foundation for interpreting set-valued ERM stability, with the established qualitative and quantitative regimes offering comprehensive guarantees for solution robustness under realistic optimization conditions."}}
{"id": "2601.18716", "categories": ["cs.AI", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2601.18716", "abs": "https://arxiv.org/abs/2601.18716", "authors": ["Naeyma N. Islam", "Thomas R. Caulfield"], "title": "Conditioned Generative Modeling of Molecular Glues: A Realistic AI Approach for Synthesizable Drug-like Molecules", "comment": "30 pages, 8 figures", "summary": "Alzheimer's disease (AD) is marked by the pathological accumulation of amyloid beta-42 (Abeta-42), contributing to synaptic dysfunction and neurodegeneration. While extracellular amyloid plaques are well-studied, increasing evidence highlights intracellular Abeta-42 as an early and toxic driver of disease progression. In this study, we present a novel, AI-assisted drug design approach to promote targeted degradation of Abeta-42 via the ubiquitin-proteasome system (UPS), using E3 ligase-directed molecular glues. We systematically evaluated the ternary complex formation potential of Abeta-42 with three E3 ligases: CRBN, VHL, and MDM2, through structure-based modeling, ADMET screening, and docking. We then developed a Ligase-Conditioned Junction Tree Variational Autoencoder (LC-JT-VAE) to generate ligase-specific small molecules, incorporating protein sequence embeddings and torsional angle-aware molecular graphs. Our results demonstrate that this generative model can produce chemically valid, novel, and target-specific molecular glues capable of facilitating Abeta-42 degradation. This integrated approach offers a promising framework for designing UPS-targeted therapies for neurodegenerative diseases.", "AI": {"tldr": "This paper develops an AI-driven drug design framework to degrade intracellular amyloid beta-42 (Abeta-42) in Alzheimer's disease by engineering E3 ligase-targeting molecular glues that activate the ubiquitin-proteasome system.", "motivation": "Intracellular Abeta-42 is identified as an early toxic driver of Alzheimer's progression, but existing therapies focus on extracellular plaques; there is a critical need for strategies that directly eliminate intracellular Abeta-42 to halt synaptic dysfunction and neurodegeneration.", "method": "Combines structure-based modeling of Abeta-42/E3 ligase (CRBN, VHL, MDM2) ternary complexes with a novel Ligase-Conditioned Junction Tree Variational Autoencoder (LC-JT-VAE) that integrates protein sequence embeddings and torsional angle-aware molecular graphs to generate target-specific molecular glues.", "result": "The LC-JT-VAE model successfully generated chemically valid, novel small molecules capable of selectively promoting Abeta-42 degradation via the ubiquitin-proteasome system, validated through computational ADMET screening and docking simulations.", "conclusion": "This AI-integrated approach provides a transformative framework for designing targeted protein degradation therapies, offering new avenues for treating neurodegenerative diseases by addressing intracellular pathogenic proteins."}}
{"id": "2601.17647", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17647", "abs": "https://arxiv.org/abs/2601.17647", "authors": ["Akila Sampath", "Vandana Janeja", "Jianwu Wang"], "title": "Time-Varying Causal Treatment for Quantifying the Causal Effect of Short-Term Variations on Arctic Sea Ice Dynamics", "comment": null, "summary": "Quantifying the causal relationship between ice melt and freshwater distribution is critical, as these complex interactions manifest as regional fluctuations in sea surface height (SSH). Leveraging SSH as a proxy for sea ice dynamics enables improved understanding of the feedback mechanisms driving polar climate change and global sea-level rise. However, conventional deep learning models often struggle with reliable treatment effect estimation in spatiotemporal settings due to unobserved confounders and the absence of physical constraints. To address these challenges, we propose the Knowledge-Guided Causal Model Variational Autoencoder (KGCM-VAE) to quantify causal mechanisms between sea ice thickness and SSH. The proposed framework integrates a velocity modulation scheme in which smoothed velocity signals are dynamically amplified via a sigmoid function governed by SSH transitions to generate physically grounded causal treatments. In addition, the model incorporates Maximum Mean Discrepancy (MMD) to balance treated and control covariate distributions in the latent space, along with a causal adjacency-constrained decoder to ensure alignment with established physical structures. Experimental results on both synthetic and real-world Arctic datasets demonstrate that KGCM-VAE achieves superior PEHE compared to state-of-the-art benchmarks. Ablation studies further confirm the effectiveness of the approach, showing that the joint application of MMD and causal adjacency constraints yields a 1.88\\% reduction in estimation error.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u77e5\u8bc6\u5f15\u5bfc\u7684\u56e0\u679c\u6a21\u578b\u53d8\u5206\u81ea\u7f16\u7801\u5668(KGCM-VAE)\uff0c\u901a\u8fc7\u878d\u5408\u7269\u7406\u7ea6\u675f\u4e0e\u56e0\u679c\u63a8\u65ad\uff0c\u91cf\u5316\u6d77\u51b0\u539a\u5ea6\u4e0e\u6d77\u5e73\u9762\u9ad8\u5ea6\u4e4b\u95f4\u7684\u56e0\u679c\u673a\u5236\uff0c\u5728\u5317\u6781\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u5f02\u8d28\u6027\u6548\u5e94\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u51b0\u878d\u5316\u4e0e\u6de1\u6c34\u5206\u5e03\u7684\u56e0\u679c\u5173\u7cfb\u5bf9\u7406\u89e3\u6781\u5730\u6c14\u5019\u53d8\u5316\u548c\u6d77\u5e73\u9762\u4e0a\u5347\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u65f6\u7a7a\u573a\u666f\u4e0b\u56e0\u672a\u89c2\u6d4b\u6df7\u6742\u56e0\u7d20\u548c\u7f3a\u4e4f\u7269\u7406\u7ea6\u675f\u800c\u96be\u4ee5\u53ef\u9760\u4f30\u8ba1\u5904\u7406\u6548\u5e94\u3002", "method": "\u63d0\u51faKGCM-VAE\u6846\u67b6\uff0c\u5305\u542b\uff1a(1)\u57fa\u4e8esigmoid\u51fd\u6570\u7684\u901f\u5ea6\u8c03\u5236\u65b9\u6848\uff0c\u6839\u636eSSH\u8fc7\u6e21\u52a8\u6001\u653e\u5927\u5e73\u6ed1\u901f\u5ea6\u4fe1\u53f7\u751f\u6210\u56e0\u679c\u5904\u7406\uff1b(2)\u6700\u5927\u5747\u503c\u5dee\u5f02(MMD)\u5e73\u8861\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5904\u7406\u7ec4\u4e0e\u63a7\u5236\u7ec4\u7684\u534f\u53d8\u91cf\u5206\u5e03\uff1b(3)\u56e0\u679c\u90bb\u63a5\u7ea6\u675f\u89e3\u7801\u5668\u786e\u4fdd\u7b26\u5408\u7269\u7406\u7ed3\u6784\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u5317\u6781\u6570\u636e\u96c6\u4e0a\uff0cKGCM-VAE\u76f8\u6bd4\u5148\u8fdb\u57fa\u51c6\u65b9\u6cd5\u83b7\u5f97\u66f4\u4f18\u7684PEHE\u6307\u6807\uff1b\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0cMMD\u4e0e\u56e0\u679c\u90bb\u63a5\u7ea6\u675f\u7684\u8054\u5408\u5e94\u7528\u4f7f\u4f30\u8ba1\u8bef\u5dee\u964d\u4f4e1.88%\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6574\u5408\u7269\u7406\u77e5\u8bc6\u4e0e\u56e0\u679c\u5efa\u6a21\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6d77\u51b0\u539a\u5ea6\u4e0e\u6d77\u5e73\u9762\u9ad8\u5ea6\u95f4\u56e0\u679c\u6548\u5e94\u7684\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u9a8c\u8bc1\u4e86\u7269\u7406\u7ea6\u675f\u4e0e\u5206\u5e03\u5e73\u8861\u7b56\u7565\u7684\u534f\u540c\u6709\u6548\u6027\u3002"}}
{"id": "2601.17654", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.17654", "abs": "https://arxiv.org/abs/2601.17654", "authors": ["Ruofan Wu", "Jae-Won Chung", "Mosharaf Chowdhury"], "title": "Kareus: Joint Reduction of Dynamic and Static Energy in Large Model Training", "comment": null, "summary": "The computing demand of AI is growing at an unprecedented rate, but energy supply is not keeping pace. As a result, energy has become an expensive, contended resource that requires explicit management and optimization. Although recent works have made significant progress in large model training optimization, they focus only on a single aspect of energy consumption: dynamic or static energy.\n  We find that fine-grained kernel scheduling and frequency scaling jointly and interdependently impact both dynamic and static energy consumption. Based on this finding, we design Kareus, a training system that pushes the time--energy tradeoff frontier by optimizing both aspects. Kareus decomposes the intractable joint optimization problem into local, partition-based subproblems. It then uses a multi-pass multi-objective optimization algorithm to find execution schedules that push the time--energy tradeoff frontier. Compared to the state of the art, Kareus reduces training energy by up to 28.3% at the same training time, or reduces training time by up to 27.5% at the same energy consumption.", "AI": {"tldr": "\u9488\u5bf9AI\u8bad\u7ec3\u80fd\u8017\u95ee\u9898\uff0cKareus\u7cfb\u7edf\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5185\u6838\u8c03\u5ea6\u4e0e\u9891\u7387\u8c03\u8282\u7684\u8054\u5408\u4f18\u5316\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u964d\u4f4e\u80fd\u801728.3%\u6216\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f427.5%\u3002", "motivation": "AI\u8ba1\u7b97\u9700\u6c42\u5feb\u901f\u589e\u957f\u800c\u80fd\u6e90\u4f9b\u5e94\u4e0d\u8db3\uff0c\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u4ec5\u5173\u6ce8\u52a8\u6001\u6216\u9759\u6001\u80fd\u8017\u5355\u4e00\u7ef4\u5ea6\uff0c\u7f3a\u4e4f\u5bf9\u4e24\u8005\u7684\u8054\u5408\u4f18\u5316\u3002", "method": "\u8bbe\u8ba1Kareus\u8bad\u7ec3\u7cfb\u7edf\uff0c\u5c06\u8054\u5408\u4f18\u5316\u95ee\u9898\u5206\u89e3\u4e3a\u57fa\u4e8e\u5206\u533a\u7684\u5b50\u95ee\u9898\uff0c\u91c7\u7528\u591a\u901a\u9053\u591a\u76ee\u6807\u4f18\u5316\u7b97\u6cd5\u5bfb\u627e\u6700\u4f18\u6267\u884c\u8c03\u5ea6\u65b9\u6848\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0cKareus\u53ef\u5728\u76f8\u540c\u8bad\u7ec3\u65f6\u95f4\u4e0b\u964d\u4f4e\u80fd\u8017\u6700\u591a28.3%\uff0c\u6216\u5728\u76f8\u540c\u80fd\u8017\u4e0b\u7f29\u77ed\u8bad\u7ec3\u65f6\u95f4\u6700\u591a27.5%\u3002", "conclusion": "Kareus\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5185\u6838\u8c03\u5ea6\u4e0e\u9891\u7387\u8c03\u8282\u7684\u534f\u540c\u4f18\u5316\uff0c\u6709\u6548\u63a8\u8fdb\u4e86AI\u8bad\u7ec3\u7684\u65f6\u95f4-\u80fd\u8017\u6743\u8861\u8fb9\u754c\u3002"}}
{"id": "2601.18744", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18744", "abs": "https://arxiv.org/abs/2601.18744", "authors": ["Fangxu Yu", "Xingang Guo", "Lingzhi Yuan", "Haoqiang Kang", "Hongyu Zhao", "Lianhui Qin", "Furong Huang", "Bin Hu", "Tianyi Zhou"], "title": "TSRBench: A Comprehensive Multi-task Multi-modal Time Series Reasoning Benchmark for Generalist Models", "comment": null, "summary": "Time series data is ubiquitous in real-world scenarios and crucial for critical applications ranging from energy management to traffic control. Consequently, the ability to reason over time series is a fundamental skill for generalist models to solve practical problems. However, this dimension is notably absent from existing benchmarks of generalist models. To bridge this gap, we introduce TSRBench, a comprehensive multi-modal benchmark designed to stress-test the full spectrum of time series reasoning capabilities. TSRBench features: i) a diverse set of 4125 problems from 14 domains, and is categorized into 4 major dimensions: Perception, Reasoning, Prediction, and Decision-Making. ii) 15 tasks from the 4 dimensions evaluating essential reasoning capabilities (e.g., numerical reasoning). Through extensive experiments, we evaluated over 30 leading proprietary and open-source LLMs, VLMs, and TSLLMs within TSRBench. Our findings reveal that: i) scaling laws hold for perception and reasoning but break down for prediction; ii) strong reasoning does not guarantee accurate context-aware forecasting, indicating a decoupling between semantic understanding and numerical prediction; and iii) despite the complementary nature of textual and visual represenations of time series as inputs, current multimodal models fail to effectively fuse them for reciprocal performance gains. TSRBench provides a standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance generalist models. Our code and dataset are available at https://tsrbench.github.io/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TSRBench\uff0c\u9996\u4e2a\u5168\u9762\u8bc4\u4f30\u901a\u7528\u6a21\u578b\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u80fd\u529b\u7684\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d64\u5927\u7ef4\u5ea615\u9879\u4efb\u52a1\uff0c\u63ed\u793a\u73b0\u6709\u6a21\u578b\u5728\u9884\u6d4b\u80fd\u529b\u548c\u591a\u6a21\u6001\u878d\u5408\u65b9\u9762\u5b58\u5728\u663e\u8457\u74f6\u9888\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5728\u80fd\u6e90\u7ba1\u7406\u3001\u4ea4\u901a\u63a7\u5236\u7b49\u5173\u952e\u5e94\u7528\u4e2d\u65e0\u5904\u4e0d\u5728\uff0c\u4f46\u73b0\u6709\u901a\u7528\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u666e\u904d\u7f3a\u4e4f\u5bf9\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u80fd\u529b\u7684\u8bc4\u4f30\uff0c\u8fd9\u4e00\u7ef4\u5ea6\u5bf9\u4e8e\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6784\u5efaTSRBench\u57fa\u51c6\uff0c\u5305\u542b14\u4e2a\u9886\u57df\u76844125\u4e2a\u95ee\u9898\uff0c\u5206\u4e3a\u611f\u77e5\u3001\u63a8\u7406\u3001\u9884\u6d4b\u548c\u51b3\u7b564\u5927\u7ef4\u5ea6\uff0c\u8bbe\u8ba115\u9879\u4efb\u52a1\u8bc4\u4f30\u6838\u5fc3\u63a8\u7406\u80fd\u529b\u3002\u5bf930\u4f59\u79cd\u4e3b\u6d41\u5927\u6a21\u578b\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u65f6\u95f4\u5e8f\u5217\u4e13\u7528\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u8bc4\u6d4b\u3002", "result": "\u53d1\u73b0\u4e09\u5927\u5173\u952e\u73b0\u8c61\uff1a1) \u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u9075\u5faa\u7f29\u653e\u5b9a\u5f8b\uff0c\u4f46\u9884\u6d4b\u80fd\u529b\u4e0d\u9075\u5faa\uff1b2) \u5f3a\u63a8\u7406\u80fd\u529b\u4e0d\u80fd\u4fdd\u8bc1\u51c6\u786e\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u9884\u6d4b\uff0c\u8bed\u4e49\u7406\u89e3\u4e0e\u6570\u503c\u9884\u6d4b\u5b58\u5728\u89e3\u8026\uff1b3) \u73b0\u6709\u591a\u6a21\u6001\u6a21\u578b\u672a\u80fd\u6709\u6548\u878d\u5408\u6587\u672c\u548c\u89c6\u89c9\u8868\u5f81\u4ee5\u83b7\u5f97\u6027\u80fd\u589e\u76ca\u3002", "conclusion": "TSRBench\u4e3a\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u5e73\u53f0\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u63a8\u8fdb\u901a\u7528\u6a21\u578b\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u6d1e\u5bdf\u548c\u65b9\u5411\u3002"}}
{"id": "2601.17667", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17667", "abs": "https://arxiv.org/abs/2601.17667", "authors": ["Pedro P. Santos", "Jacopo Silvestrin", "Alberto Sardinha", "Francisco S. Melo"], "title": "Entropic Risk-Aware Monte Carlo Tree Search", "comment": null, "summary": "We propose a provably correct Monte Carlo tree search (MCTS) algorithm for solving \\textit{risk-aware} Markov decision processes (MDPs) with \\textit{entropic risk measure} (ERM) objectives. We provide a \\textit{non-asymptotic} analysis of our proposed algorithm, showing that the algorithm: (i) is \\textit{correct} in the sense that the empirical ERM obtained at the root node converges to the optimal ERM; and (ii) enjoys \\textit{polynomial regret concentration}. Our algorithm successfully exploits the dynamic programming formulations for solving risk-aware MDPs with ERM objectives introduced by previous works in the context of an upper confidence bound-based tree search algorithm. Finally, we provide a set of illustrative experiments comparing our risk-aware MCTS method against relevant baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53ef\u8bc1\u660e\u6b63\u786e\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7b97\u6cd5\uff0c\u7528\u4e8e\u6c42\u89e3\u5177\u6709\u71b5\u98ce\u9669\u6d4b\u5ea6\u7684\u98ce\u9669\u611f\u77e5\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u63d0\u4f9b\u975e\u6e10\u8fd1\u6536\u655b\u6027\u548c\u591a\u9879\u5f0f\u9057\u61be\u754c\u4fdd\u8bc1", "motivation": "\u4f20\u7edfMDP\u4ec5\u4f18\u5316\u671f\u671b\u56de\u62a5\uff0c\u65e0\u6cd5\u523b\u753b\u98ce\u9669\u654f\u611f\u9700\u6c42\uff1b\u71b5\u98ce\u9669\u6d4b\u5ea6\u662f\u76f8\u5e72\u98ce\u9669\u5ea6\u91cf\uff0c\u4f46\u6c42\u89e3\u8ba1\u7b97\u590d\u6742\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u6709\u9650\u6837\u672c\u7406\u8bba\u4fdd\u8bc1", "method": "\u8bbe\u8ba1\u9488\u5bf9ERM\u76ee\u6807\u7684MCTS\u7b97\u6cd5\uff0c\u7ed3\u5408\u52a8\u6001\u89c4\u5212 formulation \u548cUCB\u6811\u641c\u7d22\u7b56\u7565\uff0c\u8fdb\u884c\u975e\u6e10\u8fd1\u7406\u8bba\u5206\u6790", "result": "\u7b97\u6cd5\u6b63\u786e\u6027\uff1a\u6839\u8282\u70b9\u7ecf\u9a8cERM\u6536\u655b\u5230\u6700\u4f18\u503c\uff1b\u4eab\u53d7\u591a\u9879\u5f0f\u9057\u61be\u96c6\u4e2d\u6027\uff1b\u5b9e\u9a8c\u663e\u793a\u4f18\u4e8e\u76f8\u5173\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u9996\u4e2a\u4e3a\u98ce\u9669\u611f\u77e5MDP\u63d0\u4f9b\u53ef\u8bc1\u660e\u6709\u9650\u6837\u672c\u4fdd\u8bc1\u7684MCTS\u7b97\u6cd5\uff0c\u6865\u63a5\u7406\u8bba\u4e25\u8c28\u6027\u4e0e\u5b9e\u9645\u5e94\u7528\uff0c\u4e3a\u98ce\u9669\u654f\u611f\u5e8f\u8d2f\u51b3\u7b56\u63d0\u4f9b\u53ef\u9760\u89e3\u6cd5"}}
{"id": "2601.17668", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17668", "abs": "https://arxiv.org/abs/2601.17668", "authors": ["Jang-Hyun Kim", "Dongyoon Han", "Sangdoo Yun"], "title": "Fast KVzip: Efficient and Accurate LLM Inference with Gated KV Eviction", "comment": null, "summary": "Efficient key-value (KV) cache management is crucial for the practical deployment of large language models (LLMs), yet existing compression techniques often incur a trade-off between performance degradation and computational overhead. We propose a novel gating-based KV cache eviction method for frozen-weight LLMs that achieves high compression ratios with negligible computational cost. Our approach introduces lightweight sink-attention gating modules to identify and retain critical KV pairs, and integrates seamlessly into both the prefill and decoding stages. The proposed gate training algorithm relies on forward passes of an LLM, avoiding expensive backpropagation, while achieving strong task generalization through a task-agnostic reconstruction objective. Extensive experiments across the Qwen2.5-1M, Qwen3, and Gemma3 families show that our method maintains near-lossless performance while evicting up to 70% of the KV cache. The results are consistent across a wide range of tasks, including long-context understanding, code comprehension, and mathematical reasoning, demonstrating the generality of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u95e8\u63a7\u7684KV\u7f13\u5b58\u6dd8\u6c70\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7sink-attention\u95e8\u63a7\u6a21\u5757\u8bc6\u522b\u5173\u952eKV\u5bf9\uff0c\u65e0\u9700\u53cd\u5411\u4f20\u64ad\u5373\u53ef\u5b9e\u73b0\u9ad8\u8fbe70%\u7684\u7f13\u5b58\u538b\u7f29\u7387\uff0c\u5728\u591a\u4e2aLLM\u7cfb\u5217\u548c\u4efb\u52a1\u4e0a\u4fdd\u6301\u8fd1\u4e4e\u65e0\u635f\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709KV\u7f13\u5b58\u538b\u7f29\u6280\u672f\u5728\u6027\u80fd\u9000\u5316\u4e0e\u8ba1\u7b97\u5f00\u9500\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u7ba1\u7406\u65b9\u6848\u4ee5\u652f\u6301\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u7528\u5316\u90e8\u7f72\u3002", "method": "\u9488\u5bf9\u51bb\u7ed3\u6743\u91cdLLM\u8bbe\u8ba1\u8f7b\u91cf\u7ea7sink-attention\u95e8\u63a7\u6a21\u5757\uff0c\u7528\u4e8e\u8bc6\u522b\u548c\u4fdd\u7559\u5173\u952eKV\u5bf9\uff1b\u63d0\u51fa\u4ec5\u9700\u524d\u5411\u4f20\u64ad\u7684\u8f7b\u91cf\u95e8\u8bad\u7ec3\u7b97\u6cd5\uff0c\u91c7\u7528\u4efb\u52a1\u65e0\u5173\u7684\u91cd\u5efa\u76ee\u6807\u5b9e\u73b0\u5f3a\u6cdb\u5316\u80fd\u529b\uff1b\u65e0\u7f1d\u96c6\u6210\u5230\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\u3002", "result": "\u5728Qwen2.5-1M\u3001Qwen3\u548cGemma3\u6a21\u578b\u65cf\u4e0a\u5b9e\u73b0\u9ad8\u8fbe70%\u7684KV\u7f13\u5b58\u6dd8\u6c70\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u8fd1\u4e4e\u65e0\u635f\u7684\u6027\u80fd\uff1b\u5728\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u3001\u4ee3\u7801\u7406\u89e3\u548c\u6570\u5b66\u63a8\u7406\u7b49\u4efb\u52a1\u4e0a\u8868\u73b0\u4e00\u81f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4ee5\u6781\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u5b9e\u73b0\u4e86\u9ad8\u538b\u7f29\u6bd4\uff0c\u5c55\u73b0\u51fa\u826f\u597d\u7684\u901a\u7528\u6027\u548c\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\uff0c\u4e3aLLM\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17680", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17680", "abs": "https://arxiv.org/abs/2601.17680", "authors": ["Shota Takashiro", "Takeshi Kojima", "Shohei Taniguchi", "Yusuke Iwasawa", "Yutaka Matsuo"], "title": "$\\infty$-MoE: Generalizing Mixture of Experts to Infinite Experts", "comment": "Accepted at EACL 2026 (Main)", "summary": "The Mixture of Experts (MoE) selects a few feed-forward networks (FFNs) per token, achieving an effective trade-off between computational cost and performance. In conventional MoE, each expert is treated as entirely independent, and experts are combined in a discrete space. As a result, when the number of experts increases, it becomes difficult to train each expert effectively. To stabilize training while increasing the number of experts, we propose $\\infty$-MoE that selects a portion of the parameters of large FFNs based on continuous values sampled for each token. By considering experts in a continuous space, this approach allows for an infinite number of experts while maintaining computational efficiency. Experiments show that a GPT-2 Small-based $\\infty$-MoE model, with 129M active and 186M total parameters, achieves comparable performance to a dense GPT-2 Medium with 350M parameters. Adjusting the number of sampled experts at inference time allows for a flexible trade-off between accuracy and speed, with an improvement of up to 2.5\\% in accuracy over conventional MoE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u221e-MoE\uff0c\u4e00\u79cd\u8fde\u7eed\u7a7a\u95f4\u7684\u4e13\u5bb6\u6df7\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u91c7\u6837FFN\u53c2\u6570\u800c\u975e\u9009\u62e9\u5b8c\u6574\u4e13\u5bb6\uff0c\u5b9e\u73b0\u65e0\u9650\u4e13\u5bb6\u6570\u91cf\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "\u4f20\u7edfMoE\u5c06\u4e13\u5bb6\u89c6\u4e3a\u72ec\u7acb\u79bb\u6563\u5355\u5143\uff0c\u4e13\u5bb6\u6570\u91cf\u589e\u52a0\u65f6\u8bad\u7ec3\u56f0\u96be\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8fde\u7eed\u7a7a\u95f4\u8868\u793a\u7a33\u5b9a\u8bad\u7ec3\uff0c\u540c\u65f6\u6269\u5c55\u4e13\u5bb6\u6570\u91cf", "method": "\u221e-MoE\uff1a\u57fa\u4e8e\u6bcf\u4e2atoken\u91c7\u6837\u7684\u8fde\u7eed\u503c\uff0c\u4ece\u5927\u578bFFN\u4e2d\u9009\u62e9\u90e8\u5206\u53c2\u6570\uff0c\u5c06\u4e13\u5bb6\u7f6e\u4e8e\u8fde\u7eed\u7a7a\u95f4\u4e2d\uff0c\u5b9e\u73b0\u65e0\u9650\u4e13\u5bb6\u7ec4\u5408", "result": "129M\u6fc0\u6d3b\u53c2\u6570/186M\u603b\u53c2\u6570\u7684GPT-2 Small \u221e-MoE\u6a21\u578b\u6027\u80fd\u5ab2\u7f8e350M\u53c2\u6570\u7684\u5bc6\u96c6GPT-2 Medium\uff0c\u6bd4\u4f20\u7edfMoE\u51c6\u786e\u7387\u63d0\u5347\u6700\u9ad82.5%\uff0c\u63a8\u7406\u65f6\u53ef\u7075\u6d3b\u8c03\u6574\u4e13\u5bb6\u6570\u91cf\u5e73\u8861\u7cbe\u5ea6\u4e0e\u901f\u5ea6", "conclusion": "\u8fde\u7eed\u4e13\u5bb6\u9009\u62e9\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u4e13\u5bb6\u8bad\u7ec3\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u4e0e\u6027\u80fd\u95f4\u53d6\u5f97\u66f4\u597d\u5e73\u8861\uff0c\u5e76\u63d0\u4f9b\u52a8\u6001\u63a8\u7406\u7075\u6d3b\u6027"}}
{"id": "2601.17689", "categories": ["cs.LG", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.17689", "abs": "https://arxiv.org/abs/2601.17689", "authors": ["Shanu Saklani", "Tushar M. Athawale", "Nairita Pal", "David Pugmire", "Christopher R. Johnson", "Soumya Dutta"], "title": "REV-INR: Regularized Evidential Implicit Neural Representation for Uncertainty-Aware Volume Visualization", "comment": null, "summary": "Applications of Implicit Neural Representations (INRs) have emerged as a promising deep learning approach for compactly representing large volumetric datasets. These models can act as surrogates for volume data, enabling efficient storage and on-demand reconstruction via model predictions. However, conventional deterministic INRs only provide value predictions without insights into the model's prediction uncertainty or the impact of inherent noisiness in the data. This limitation can lead to unreliable data interpretation and visualization due to prediction inaccuracies in the reconstructed volume. Identifying erroneous results extracted from model-predicted data may be infeasible, as raw data may be unavailable due to its large size. To address this challenge, we introduce REV-INR, Regularized Evidential Implicit Neural Representation, which learns to predict data values accurately along with the associated coordinate-level data uncertainty and model uncertainty using only a single forward pass of the trained REV-INR during inference. By comprehensively comparing and contrasting REV-INR with existing well-established deep uncertainty estimation methods, we show that REV-INR achieves the best volume reconstruction quality with robust data (aleatoric) and model (epistemic) uncertainty estimates using the fastest inference time. Consequently, we demonstrate that REV-INR facilitates assessment of the reliability and trustworthiness of the extracted isosurfaces and volume visualization results, enabling analyses to be solely driven by model-predicted data.", "AI": {"tldr": "REV-INR introduces a regularized evidential framework for Implicit Neural Representations (INRs) to simultaneously predict volumetric data values and coordinate-level uncertainty (both data/aleatoric and model/epistemic) in a single forward pass, enabling reliable visualization and analysis of large datasets where raw data is inaccessible.", "motivation": "Conventional deterministic INRs lack uncertainty quantification, leading to potentially unreliable reconstructions and visualizations when representing large volumetric datasets where raw data verification is infeasible due to size.", "method": "Proposes REV-INR (Regularized Evidential INR), a novel framework that integrates evidential deep learning with INRs to output both data predictions and associated uncertainties via a single inference pass, using regularization for robust uncertainty estimation.", "result": "Outperforms existing uncertainty estimation methods by achieving superior volume reconstruction quality, robust aleatoric and epistemic uncertainty estimates, and the fastest inference time simultaneously.", "conclusion": "REV-INR enables trustworthy assessment of isosurface extraction and volume visualization results solely from model-predicted data, enhancing reliability for scientific analysis of large-scale volumetric datasets."}}
{"id": "2601.17713", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17713", "abs": "https://arxiv.org/abs/2601.17713", "authors": ["Kaile Wang", "Jiannong Cao", "Yu Yang", "Xiaoyin Li", "Yinfeng Cao"], "title": "FedCCA: Client-Centric Adaptation against Data Heterogeneity in Federated Learning on IoT Devices", "comment": "Accepted by IEEE Annual Congress on Artificial Intelligence of Things (IEEE AIoT) 2025", "summary": "With the rapid development of the Internet of Things (IoT), AI model training on private data such as human sensing data is highly desired. Federated learning (FL) has emerged as a privacy-preserving distributed training framework for this purpuse. However, the data heterogeneity issue among IoT devices can significantly degrade the model performance and convergence speed in FL. Existing approaches limit in fixed client selection and aggregation on cloud server, making the privacy-preserving extraction of client-specific information during local training challenging. To this end, we propose Client-Centric Adaptation federated learning (FedCCA), an algorithm that optimally utilizes client-specific knowledge to learn a unique model for each client through selective adaptation, aiming to alleviate the influence of data heterogeneity. Specifically, FedCCA employs dynamic client selection and adaptive aggregation based on the additional client-specific encoder. To enhance multi-source knowledge transfer, we adopt an attention-based global aggregation strategy. We conducted extensive experiments on diverse datasets to assess the efficacy of FedCCA. The experimental results demonstrate that our approach exhibits a substantial performance advantage over competing baselines in addressing this specific problem.", "AI": {"tldr": "FedCCA: A client-centric federated learning algorithm that uses dynamic client selection and adaptive aggregation with client-specific encoders to tackle data heterogeneity in IoT, showing significant performance gains over existing methods.", "motivation": "IoT devices generate private sensory data requiring AI training, but federated learning suffers from data heterogeneity that degrades performance. Existing methods struggle to extract client-specific information during local training while preserving privacy.", "method": "Proposes FedCCA with dynamic client selection, adaptive aggregation using client-specific encoders, and attention-based global aggregation to learn unique models for each client through selective adaptation.", "result": "Extensive experiments on diverse datasets demonstrate substantial performance advantage over competing baselines in addressing data heterogeneity.", "conclusion": "FedCCA effectively alleviates data heterogeneity issues in federated learning for IoT by optimally utilizing client-specific knowledge through selective adaptation."}}
{"id": "2601.17761", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17761", "abs": "https://arxiv.org/abs/2601.17761", "authors": ["Dongjie Cheng", "Ruifeng Yuan", "Yongqi Li", "Runyang You", "Wenjie Wang", "Liqiang Nie", "Lei Zhang", "Wenjie Li"], "title": "AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation", "comment": null, "summary": "Real-world perception and interaction are inherently multimodal, encompassing not only language but also vision and speech, which motivates the development of \"Omni\" MLLMs that support both multimodal inputs and multimodal outputs. While a sequence of omni MLLMs has emerged, most existing systems still rely on additional expert components to achieve multimodal generation, limiting the simplicity of unified training and inference. Autoregressive (AR) modeling, with a single token stream, a single next-token objective, and a single decoder, is an elegant and scalable foundation in the text domain. Motivated by this, we present AR-Omni, a unified any-to-any model in the autoregressive paradigm without any expert decoders. AR-Omni supports autoregressive text and image generation, as well as streaming speech generation, all under a single Transformer decoder. We further address three practical issues in unified AR modeling: modality imbalance via task-aware loss reweighting, visual fidelity via a lightweight token-level perceptual alignment loss for image tokens, and stability-creativity trade-offs via a finite-state decoding mechanism. Empirically, AR-Omni achieves strong quality across three modalities while remaining real-time, achieving a 0.88 real-time factor for speech generation.", "AI": {"tldr": "\u63d0\u51faAR-Omni\uff0c\u4e00\u79cd\u7edf\u4e00\u7684\u81ea\u56de\u5f52\u5168\u6a21\u6001\u6a21\u578b\uff0c\u65e0\u9700\u4e13\u5bb6\u89e3\u7801\u5668\u5373\u53ef\u5b9e\u73b0\u6587\u672c\u3001\u56fe\u50cf\u548c\u8bed\u97f3\u7684\u5b9e\u65f6\u751f\u6210\u3002", "motivation": "\u73b0\u5b9e\u611f\u77e5\u662f\u591a\u6a21\u6001\u7684\uff0c\u4f46\u73b0\u6709\u5168\u6a21\u6001MLLMs\u4f9d\u8d56\u4e13\u5bb6\u7ec4\u4ef6\uff0c\u9650\u5236\u4e86\u7edf\u4e00\u6027\u3002\u6587\u672c\u81ea\u56de\u5f52\u5efa\u6a21\u4ee5\u5176\u7b80\u6d01\u4f18\u96c5\u7684\u53ef\u6269\u5c55\u6027\u4e3a\u591a\u6a21\u6001\u751f\u6210\u63d0\u4f9b\u4e86\u7406\u60f3\u6846\u67b6\u3002", "method": "AR-Omni\u4f7f\u7528\u5355\u4e00Transformer\u89e3\u7801\u5668\u652f\u6301\u6587\u672c\u3001\u56fe\u50cf\u548c\u6d41\u5f0f\u8bed\u97f3\u7684\u81ea\u56de\u5f52\u751f\u6210\u3002\u901a\u8fc7\u4efb\u52a1\u611f\u77e5\u635f\u5931\u91cd\u52a0\u6743\u89e3\u51b3\u6a21\u6001\u4e0d\u5e73\u8861\uff0c\u4ee4\u724c\u7ea7\u611f\u77e5\u5bf9\u9f50\u63d0\u5347\u56fe\u50cf\u4fdd\u771f\u5ea6\uff0c\u6709\u9650\u72b6\u6001\u89e3\u7801\u5e73\u8861\u7a33\u5b9a\u6027\u4e0e\u521b\u9020\u6027\u3002", "result": "\u5728\u4e09\u79cd\u6a21\u6001\u4e0a\u5b9e\u73b0\u9ad8\u8d28\u91cf\u5b9e\u65f6\u751f\u6210\uff0c\u8bed\u97f3\u5b9e\u65f6\u56e0\u5b50\u8fbe0.88\u3002", "conclusion": "\u6210\u529f\u5728\u5355\u4e00\u81ea\u56de\u5f52\u6846\u67b6\u4e2d\u7edf\u4e00\u591a\u6a21\u6001\u751f\u6210\uff0c\u65e0\u9700\u4e13\u5bb6\u89e3\u7801\u5668\uff0c\u4e3a\u5168\u6a21\u6001\u6a21\u578b\u63d0\u4f9b\u4e86\u7b80\u6d01\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17768", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.17768", "abs": "https://arxiv.org/abs/2601.17768", "authors": ["Raja Gond", "Aditya K Kamath", "Arkaprava Basu", "Ramachandran Ramjee", "Ashish Panwar"], "title": "LLM-42: Enabling Determinism in LLM Inference with Verified Speculation", "comment": "https://github.com/microsoft/llm-42", "summary": "In LLM inference, the same prompt may yield different outputs across different runs. At the system level, this non-determinism arises from floating-point non-associativity combined with dynamic batching and GPU kernels whose reduction orders vary with batch size. A straightforward way to eliminate non-determinism is to disable dynamic batching during inference, but doing so severely degrades throughput. Another approach is to make kernels batch-invariant; however, this tightly couples determinism to kernel design, requiring new implementations. This coupling also imposes fixed runtime overheads, regardless of how much of the workload actually requires determinism.\n  Inspired by ideas from speculative decoding, we present LLM-42, a scheduling-based approach to enable determinism in LLM inference. Our key observation is that if a sequence is in a consistent state, the next emitted token is likely to be consistent even with dynamic batching. Moreover, most GPU kernels use shape-consistent reductions. Leveraging these insights, LLM-42 decodes tokens using a non-deterministic fast path and enforces determinism via a lightweight verify-rollback loop. The verifier replays candidate tokens under a fixed-shape reduction schedule, commits those that are guaranteed to be consistent across runs, and rolls back those violating determinism. LLM-42 mostly re-uses existing kernels unchanged and incurs overhead only in proportion to the traffic that requires determinism.", "AI": {"tldr": "LLM-42 is a scheduling-based approach that enables deterministic LLM inference via a verify-rollback mechanism, reusing existing kernels and incurring overhead only when determinism is required.", "motivation": "LLM inference suffers from non-determinism due to floating-point non-associativity and dynamic batching, causing inconsistent outputs. Existing solutions either severely degrade throughput (disabling batching) or impose fixed overhead regardless of actual need (batch-invariant kernels).", "method": "Inspired by speculative decoding, LLM-42 uses a two-path system: a non-deterministic fast path for token decoding and a lightweight verify-rollback loop. The verifier replays candidate tokens under a fixed-shape reduction schedule, committing those guaranteed consistent across runs and rolling back inconsistent ones. It leverages that sequences in consistent states likely produce consistent next tokens and that most GPU kernels use shape-consistent reductions.", "result": "The approach mostly reuses existing kernels unchanged and incurs overhead only in proportion to the traffic that actually requires determinism, avoiding fixed runtime penalties.", "conclusion": "LLM-42 provides an efficient, practical solution for deterministic LLM inference that minimizes performance impact while ensuring consistency where needed."}}
{"id": "2601.17858", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17858", "abs": "https://arxiv.org/abs/2601.17858", "authors": ["Jiapeng Wang", "Changxin Tian", "Kunlong Chen", "Ziqi Liu", "Jiaxin Mao", "Wayne Xin Zhao", "Zhiqiang Zhang", "Jun Zhou"], "title": "MergeMix: Optimizing Mid-Training Data Mixtures via Learnable Model Merging", "comment": null, "summary": "Optimizing data mixtures is essential for unlocking the full potential of large language models (LLMs), yet identifying the optimal composition remains computationally prohibitive due to reliance on heuristic trials or expensive proxy training. To address this, we introduce \\textbf{MergeMix}, a novel approach that efficiently determines optimal data mixing ratios by repurposing model merging weights as a high-fidelity, low-cost performance proxy. By training domain-specific experts on minimal tokens and optimizing their merging weights against downstream benchmarks, MergeMix effectively optimizes the performance of data mixtures without incurring the cost of full-scale training. Extensive experiments on models with 8B and 16B parameters validate that MergeMix achieves performance comparable to or surpassing exhaustive manual tuning while drastically reducing search costs. Furthermore, MergeMix exhibits high rank consistency (Spearman $\u03c1> 0.9$) and strong cross-scale transferability, offering a scalable, automated solution for data mixture optimization.", "AI": {"tldr": "MergeMix introduces a novel method to efficiently optimize data mixing ratios for large language models by using model merging weights as a low-cost performance proxy, achieving comparable results to manual tuning with drastically reduced computational costs.", "motivation": "Current methods for optimizing data mixtures in LLMs rely on computationally expensive heuristic trials or proxy training, making the process prohibitively costly.", "method": "MergeMix trains domain-specific experts on minimal tokens and optimizes their merging weights against downstream benchmarks to serve as a high-fidelity, low-cost performance proxy for data mixture optimization.", "result": "Experiments show MergeMix achieves performance comparable to or surpassing exhaustive manual tuning while drastically reducing search costs, with high rank consistency (Spearman \u03c1>0.9) and strong cross-scale transferability.", "conclusion": "MergeMix provides a scalable, automated solution for data mixture optimization, effectively balancing performance and computational efficiency without full-scale training."}}
{"id": "2601.17883", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17883", "abs": "https://arxiv.org/abs/2601.17883", "authors": ["Dingkun Liu", "Yuheng Chen", "Zhu Chen", "Zhenyao Cui", "Yaozhi Wen", "Jiayu An", "Jingwei Luo", "Dongrui Wu"], "title": "EEG Foundation Models: Progresses, Benchmarking, and Open Problems", "comment": null, "summary": "Electroencephalography (EEG) foundation models have recently emerged as a promising paradigm for brain-computer interfaces (BCIs), aiming to learn transferable neural representations from large-scale heterogeneous recordings. Despite rapid progresses, there lacks fair and comprehensive comparisons of existing EEG foundation models, due to inconsistent pre-training objectives, preprocessing choices, and downstream evaluation protocols. This paper fills this gap. We first review 50 representative models and organize their design choices into a unified taxonomic framework including data standardization, model architectures, and self-supervised pre-training strategies. We then evaluate 12 open-source foundation models and competitive specialist baselines across 13 EEG datasets spanning nine BCI paradigms. Emphasizing real-world deployments, we consider both cross-subject generalization under a leave-one-subject-out protocol and rapid calibration under a within-subject few-shot setting. We further compare full-parameter fine-tuning with linear probing to assess the transferability of pre-trained representations, and examine the relationship between model scale and downstream performance. Our results indicate that: 1) linear probing is frequently insufficient; 2) specialist models trained from scratch remain competitive across many tasks; and, 3) larger foundation models do not necessarily yield better generalization performance under current data regimes and training practices.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e8612\u4e2a\u5f00\u6e90EEG\u57fa\u7840\u6a21\u578b\u572813\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u7ebf\u6027\u63a2\u6d4b\u4e0d\u8db3\u3001\u4e13\u5bb6\u6a21\u578b\u4ecd\u5177\u7ade\u4e89\u529b\uff0c\u4e14\u6a21\u578b\u89c4\u6a21\u4e0d\u4e00\u5b9a\u5e26\u6765\u66f4\u597d\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u9884\u8bad\u7ec3\u76ee\u6807\u3001\u9884\u5904\u7406\u548c\u8bc4\u4f30\u534f\u8bae\u4e0d\u4e00\u81f4\uff0c\u73b0\u6709EEG\u57fa\u7840\u6a21\u578b\u7f3a\u4e4f\u516c\u5e73\u5168\u9762\u7684\u6bd4\u8f83\uff0c\u9650\u5236\u4e86\u5bf9\u6a21\u578b\u771f\u5b9e\u6027\u80fd\u7684\u8bc4\u4f30\u3002", "method": "\u9996\u5148\u56de\u987e50\u4e2a\u4ee3\u8868\u6027\u6a21\u578b\u5e76\u6784\u5efa\u7edf\u4e00\u5206\u7c7b\u6846\u67b6\uff0c\u7136\u540e\u8bc4\u4f3012\u4e2a\u5f00\u6e90\u57fa\u7840\u6a21\u578b\u572813\u4e2aEEG\u6570\u636e\u96c6\u30019\u79cdBCI\u8303\u5f0f\u4e0a\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u8003\u5bdf\u8de8\u88ab\u8bd5\u6cdb\u5316\u548c\u5c11\u6837\u672c\u6821\u51c6\u80fd\u529b\uff0c\u5e76\u5bf9\u6bd4\u5168\u53c2\u6570\u5fae\u8c03\u4e0e\u7ebf\u6027\u63a2\u6d4b\u3002", "result": "1) \u7ebf\u6027\u63a2\u6d4b\u7ecf\u5e38\u4e0d\u8db3\uff1b2) \u4ece\u5934\u8bad\u7ec3\u7684\u4e13\u5bb6\u6a21\u578b\u5728\u8bb8\u591a\u4efb\u52a1\u4e0a\u4ecd\u5177\u7ade\u4e89\u529b\uff1b3) \u5728\u5f53\u524d\u6570\u636e\u548c\u8bad\u7ec3\u8303\u5f0f\u4e0b\uff0c\u66f4\u5927\u7684\u57fa\u7840\u6a21\u578b\u4e0d\u4e00\u5b9a\u5e26\u6765\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86EEG\u57fa\u7840\u6a21\u578b\u7cfb\u7edf\u6027\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u57fa\u7840\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765BCI\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2601.17910", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17910", "abs": "https://arxiv.org/abs/2601.17910", "authors": ["Aaron R. Flouro", "Shawn P. Chadwick"], "title": "Adaptive Weighting in Knowledge Distillation: An Axiomatic Framework for Multi-Scale Teacher Ensemble Optimization", "comment": "12 pages, 1 figure, 1 table", "summary": "Knowledge distillation with multiple teachers is increasingly used to improve robustness, efficiency, and safety, yet existing approaches rely largely on heuristic or implementation-specific weighting schemes. This paper develops an operator-agnostic axiomatic framework for adaptive weighting in multi-teacher knowledge distillation across three complementary scales: token, task, and context. We formalize structural conditions under which adaptive weighting operators are well-defined, admit multiple non-equivalent implementations, and can be hierarchically composed via product-structure normalization. Within this framework, we establish existence and non-uniqueness of conforming operators, characterize convergence of gradient-based optimization under standard assumptions, analyze stability and perturbation robustness, and provide an abstract formulation of safety-constrained distillation. The results decouple theoretical guarantees from specific weighting formulas, enabling principled analysis of adaptive distillation methods under heterogeneity, distribution shift, and safety constraints.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4e0e\u7b97\u5b50\u65e0\u5173\u7684\u516c\u7406\u5316\u6846\u67b6\u6765\u89e3\u51b3\u591a\u6559\u5e08\u77e5\u8bc6\u84b8\u998f\u4e2d\u7684\u81ea\u9002\u5e94\u52a0\u6743\u95ee\u9898\uff0c\u5728token\u3001\u4efb\u52a1\u548c\u4e0a\u4e0b\u6587\u4e09\u4e2a\u5c3a\u5ea6\u4e0a\u5efa\u7acb\u7406\u8bba\u57fa\u7840\uff0c\u5b9e\u73b0\u4e86\u4ece\u542f\u53d1\u5f0f\u65b9\u6cd5\u5230\u53ef\u8bc1\u660e\u7406\u8bba\u7684\u8de8\u8d8a\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u6216\u7279\u5b9a\u5b9e\u73b0\u7684\u52a0\u6743\u7b56\u7565\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7406\u8bba\u6307\u5bfc\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u5728\u5f02\u8d28\u6027\u3001\u5206\u5e03\u504f\u79fb\u548c\u5b89\u5168\u7ea6\u675f\u4e0b\u7684\u6027\u80fd\uff0c\u4e9f\u9700\u5f62\u5f0f\u5316\u6846\u67b6\u6765\u89e3\u8026\u7406\u8bba\u4fdd\u8bc1\u4e0e\u5177\u4f53\u516c\u5f0f\u3002", "method": "\u5f00\u53d1\u7b97\u5b50\u65e0\u5173\u7684\u516c\u7406\u5316\u6846\u67b6\uff0c\u5f62\u5f0f\u5316\u81ea\u9002\u5e94\u52a0\u6743\u7b97\u5b50\u7684\u7ed3\u6784\u6761\u4ef6\uff0c\u652f\u6301token/task/context\u4e09\u5c3a\u5ea6\u5206\u6790\uff0c\u901a\u8fc7\u4e58\u79ef\u7ed3\u6784\u5f52\u4e00\u5316\u5b9e\u73b0\u5c42\u6b21\u5316\u7ec4\u5408\uff0c\u5e76\u57fa\u4e8e\u6807\u51c6\u5047\u8bbe\u5206\u6790\u68af\u5ea6\u4f18\u5316\u6536\u655b\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u4e00\u81f4\u7b97\u5b50\u7684\u5b58\u5728\u6027\u4e0e\u975e\u552f\u4e00\u6027\uff0c\u5efa\u7acb\u4e86\u68af\u5ea6\u4f18\u5316\u7684\u6536\u655b\u6027\u4fdd\u8bc1\uff0c\u5206\u6790\u4e86\u7a33\u5b9a\u6027\u548c\u6270\u52a8\u9c81\u68d2\u6027\uff0c\u5e76\u7ed9\u51fa\u4e86\u5b89\u5168\u7ea6\u675f\u84b8\u998f\u7684\u62bd\u8c61\u8868\u8ff0\uff0c\u5b9e\u73b0\u4e86\u7406\u8bba\u4fdd\u8bc1\u4e0e\u5177\u4f53\u5b9e\u73b0\u7684\u5206\u79bb\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u591a\u6559\u5e08\u77e5\u8bc6\u84b8\u998f\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u5206\u6790\u5de5\u5177\uff0c\u4f7f\u7406\u8bba\u4fdd\u8bc1\u72ec\u7acb\u4e8e\u5177\u4f53\u52a0\u6743\u516c\u5f0f\uff0c\u80fd\u591f\u7cfb\u7edf\u8bc4\u4f30\u5f02\u8d28\u6027\u3001\u5206\u5e03\u504f\u79fb\u548c\u5b89\u5168\u7ea6\u675f\u7b49\u573a\u666f\u4e0b\u7684\u65b9\u6cd5\u6027\u80fd\u3002"}}
{"id": "2601.17912", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17912", "abs": "https://arxiv.org/abs/2601.17912", "authors": ["Qinyi Liu", "Mohammad Khalil", "Naman Goel"], "title": "Causal Pre-training Under the Fairness Lens: An Empirical Study of TabPFN", "comment": null, "summary": "Foundation models for tabular data, such as the Tabular Prior-data Fitted Network (TabPFN), are pre-trained on a massive number of synthetic datasets generated by structural causal models (SCM). They leverage in-context learning to offer high predictive accuracy in real-world tasks. However, the fairness properties of these foundational models, which incorporate ideas from causal reasoning during pre-training, have not yet been explored in sufficient depth. In this work, we conduct a comprehensive empirical evaluation of TabPFN and its fine-tuned variants, assessing predictive performance, fairness, and robustness across varying dataset sizes and distributional shifts. Our results reveal that while TabPFN achieves stronger predictive accuracy compared to baselines and exhibits robustness to spurious correlations, improvements in fairness are moderate and inconsistent, particularly under missing-not-at-random (MNAR) covariate shifts. These findings suggest that the causal pre-training in TabPFN is helpful but insufficient for algorithmic fairness, highlighting implications for deploying such models in practice and the need for further fairness interventions.", "AI": {"tldr": "\u8bc4\u4f30\u57fa\u4e8e\u56e0\u679c\u63a8\u7406\u9884\u8bad\u7ec3\u7684\u8868\u683c\u57fa\u7840\u6a21\u578bTabPFN\u7684\u516c\u5e73\u6027\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u9884\u6d4b\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u4f18\u79c0\uff0c\u4f46\u516c\u5e73\u6027\u6539\u5584\u6709\u9650\u4e14\u4e0d\u7a33\u5065\uff0c\u5c24\u5176\u5728MNAR\u5206\u5e03\u504f\u79fb\u4e0b\u3002", "motivation": "\u8868\u683c\u57fa\u7840\u6a21\u578b(\u5982TabPFN)\u901a\u8fc7\u56e0\u679c\u6a21\u578b\u9884\u8bad\u7ec3\u83b7\u5f97\u5f3a\u5927\u9884\u6d4b\u80fd\u529b\uff0c\u4f46\u5176\u516c\u5e73\u6027\u5c5e\u6027\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u800c\u8fd9\u5bf9\u5b9e\u9645\u5e94\u7528\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5bf9TabPFN\u53ca\u5176\u5fae\u8c03\u53d8\u4f53\u8fdb\u884c\u7efc\u5408\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u7cfb\u7edf\u6d4b\u8bd5\u4e0d\u540c\u6570\u636e\u89c4\u6a21\u548c\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9884\u6d4b\u6027\u80fd\u3001\u516c\u5e73\u6027\u548c\u9c81\u68d2\u6027\u6307\u6807\u3002", "result": "TabPFN\u9884\u6d4b\u51c6\u786e\u6027\u5f3a\u4e14\u5bf9\u865a\u5047\u76f8\u5173\u9c81\u68d2\uff0c\u4f46\u516c\u5e73\u6027\u63d0\u5347\u6709\u9650\u4e14\u4e0d\u4e00\u81f4\uff0c\u5728\u4e0d\u53ef\u968f\u673a\u7f3a\u5931(MNAR)\u534f\u53d8\u91cf\u504f\u79fb\u4e0b\u8868\u73b0\u5c24\u5176\u4e0d\u4f73\u3002", "conclusion": "\u56e0\u679c\u9884\u8bad\u7ec3\u5bf9\u7b97\u6cd5\u516c\u5e73\u6027\u6709\u5e2e\u52a9\u4f46\u4e0d\u8db3\u591f\uff0c\u5b9e\u9645\u90e8\u7f72\u9700\u989d\u5916\u516c\u5e73\u6027\u5e72\u9884\u63aa\u65bd\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u542f\u793a\u3002"}}
{"id": "2601.17916", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17916", "abs": "https://arxiv.org/abs/2601.17916", "authors": ["Jialu Tang", "Tong Xia", "Yuan Lu", "Aaqib Saeed"], "title": "UniPACT: A Multimodal Framework for Prognostic Question Answering on Raw ECG and Structured EHR", "comment": "Accepted to IEEE ICASSP 2026", "summary": "Accurate clinical prognosis requires synthesizing structured Electronic Health Records (EHRs) with real-time physiological signals like the Electrocardiogram (ECG). Large Language Models (LLMs) offer a powerful reasoning engine for this task but struggle to natively process these heterogeneous, non-textual data types. To address this, we propose UniPACT (Unified Prognostic Question Answering for Clinical Time-series), a unified framework for prognostic question answering that bridges this modality gap. UniPACT's core contribution is a structured prompting mechanism that converts numerical EHR data into semantically rich text. This textualized patient context is then fused with representations learned directly from raw ECG waveforms, enabling an LLM to reason over both modalities holistically. We evaluate UniPACT on the comprehensive MDS-ED benchmark, it achieves a state-of-the-art mean AUROC of 89.37% across a diverse set of prognostic tasks including diagnosis, deterioration, ICU admission, and mortality, outperforming specialized baselines. Further analysis demonstrates that our multimodal, multi-task approach is critical for performance and provides robustness in missing data scenarios.", "AI": {"tldr": "UniPACT\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u673a\u5236\u5c06\u6570\u503c\u578bEHR\u6570\u636e\u8f6c\u5316\u4e3a\u6587\u672c\uff0c\u5e76\u4e0eECG\u6ce2\u5f62\u8868\u5f81\u878d\u5408\uff0c\u4f7fLLM\u80fd\u7edf\u4e00\u5904\u7406\u5f02\u6784\u4e34\u5e8a\u6570\u636e\uff0c\u5728MDS-ED\u57fa\u51c6\u4e0a\u5b9e\u73b089.37%\u7684SOTA AUROC\uff0c\u663e\u8457\u63d0\u5347\u591a\u4efb\u52a1\u9884\u540e\u9884\u6d4b\u6027\u80fd", "motivation": "\u4e34\u5e8a\u9884\u540e\u9700\u6574\u5408\u7ed3\u6784\u5316\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55(EHR)\u4e0e\u5b9e\u65f6\u751f\u7406\u4fe1\u53f7(\u5982ECG)\uff0c\u4f46\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u96be\u4ee5\u539f\u751f\u5904\u7406\u6b64\u7c7b\u5f02\u6784\u975e\u6587\u672c\u6570\u636e", "method": "\u63d0\u51fa\u7edf\u4e00\u9884\u540e\u95ee\u7b54\u6846\u67b6UniPACT\uff1a1) \u7ed3\u6784\u5316\u63d0\u793a\u673a\u5236\u5c06\u6570\u503cEHR\u8f6c\u4e3a\u8bed\u4e49\u4e30\u5bcc\u7684\u6587\u672c\uff1b2) \u5c06\u6587\u672c\u5316\u60a3\u8005\u4e0a\u4e0b\u6587\u4e0e\u539f\u59cbECG\u6ce2\u5f62\u5b66\u4e60\u8868\u5f81\u878d\u5408\uff1b3) \u4f7fLLM\u80fd\u6574\u4f53\u63a8\u7406\u53cc\u6a21\u6001\u6570\u636e", "result": "\u5728MDS-ED\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4ee589.37%\u5e73\u5747AUROC\u8d85\u8d8a\u4e13\u7528\u57fa\u7ebf\uff0c\u5728\u8bca\u65ad\u3001\u75c5\u60c5\u6076\u5316\u3001ICU\u8f6c\u5165\u548c\u6b7b\u4ea1\u7b49\u591a\u6837\u9884\u540e\u4efb\u52a1\u4e2d\u5b9e\u73b0\u5f53\u524d\u6700\u4f73\u6027\u80fd", "conclusion": "\u591a\u6a21\u6001\u591a\u4efb\u52a1\u65b9\u6cd5\u5bf9\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4e14\u5728\u6570\u636e\u7f3a\u5931\u573a\u666f\u4e0b\u63d0\u4f9b\u9c81\u68d2\u6027\uff0c\u8bc1\u660e\u7edf\u4e00\u5904\u7406\u5f02\u6784\u4e34\u5e8a\u6570\u636e\u7684\u53ef\u884c\u6027"}}
{"id": "2601.17917", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17917", "abs": "https://arxiv.org/abs/2601.17917", "authors": ["Zhongyu Xiao", "Zhiwei Hao", "Jianyuan Guo", "Yong Luo", "Jia Liu", "Jie Xu", "Han Hu"], "title": "treaming-dLLM: Accelerating Diffusion LLMs via Suffix Pruning and Dynamic Decoding", "comment": "Tech report. Code is available at https://github.com/xiaoshideta/Streaming-dLLM", "summary": "Diffusion Large Language Models (dLLMs) offer a compelling paradigm for natural language generation, leveraging parallel decoding and bidirectional attention to achieve superior global coherence compared to autoregressive models. While recent works have accelerated inference via KV cache reuse or heuristic decoding, they overlook the intrinsic inefficiencies within the block-wise diffusion process. Specifically, they suffer from spatial redundancy by modeling informative-sparse suffix regions uniformly and temporal inefficiency by applying fixed denoising schedules across all the decoding process. To address this, we propose Streaming-dLLM, a training-free framework that streamlines inference across both spatial and temporal dimensions. Spatially, we introduce attenuation guided suffix modeling to approximate the full context by pruning redundant mask tokens. Temporally, we employ a dynamic confidence aware strategy with an early exit mechanism, allowing the model to skip unnecessary iterations for converged tokens. Extensive experiments show that Streaming-dLLM achieves up to 68.2X speedup while maintaining generation quality, highlighting its effectiveness in diffusion decoding. The code is available at https://github.com/xiaoshideta/Streaming-dLLM.", "AI": {"tldr": "Streaming-dLLM\u662f\u4e00\u4e2a\u65e0\u9700\u91cd\u8bad\u7ec3\u7684\u63a8\u7406\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u7ef4\u5ea6\u526a\u679d\u5197\u4f59token\u548c\u65f6\u95f4\u7ef4\u5ea6\u52a8\u6001\u63d0\u524d\u9000\u51fa\uff0c\u5c06\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe68.2\u500d\u3002", "motivation": "\u5c3d\u7ba1\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\uff08dLLMs\uff09\u51ed\u501f\u5e76\u884c\u89e3\u7801\u548c\u53cc\u5411\u6ce8\u610f\u529b\u5728\u5168\u5c40\u8fde\u8d2f\u6027\u4e0a\u4f18\u4e8e\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u4f46\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u5ffd\u89c6\u4e86\u5757\u5f0f\u6269\u6563\u8fc7\u7a0b\u5185\u5728\u7684\u4f4e\u6548\u6027\uff1a\u7a7a\u95f4\u4e0a\u5bf9\u4fe1\u606f\u7a00\u758f\u7684\u540e\u7f00\u533a\u57df\u8fdb\u884c\u5747\u5300\u5efa\u6a21\u9020\u6210\u5197\u4f59\uff0c\u65f6\u95f4\u4e0a\u4f7f\u7528\u56fa\u5b9a\u7684\u53bb\u566a\u8c03\u5ea6\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faStreaming-dLLM\uff0c\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u4ece\u7a7a\u95f4\u548c\u65f6\u95f4\u53cc\u7ef4\u5ea6\u4f18\u5316\u63a8\u7406\u3002\u7a7a\u95f4\u5c42\u9762\u91c7\u7528\u8870\u51cf\u5f15\u5bfc\u7684\u540e\u7f00\u5efa\u6a21\uff0c\u901a\u8fc7\u526a\u679d\u5197\u4f59\u63a9\u7801token\u6765\u8fd1\u4f3c\u5b8c\u6574\u4e0a\u4e0b\u6587\uff1b\u65f6\u95f4\u5c42\u9762\u91c7\u7528\u52a8\u6001\u7f6e\u4fe1\u5ea6\u611f\u77e5\u7b56\u7565\u4e0e\u63d0\u524d\u9000\u51fa\u673a\u5236\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u8df3\u8fc7\u5df2\u6536\u655btoken\u7684\u4e0d\u5fc5\u8981\u8fed\u4ee3\u3002", "result": "\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe68.2\u500d\u7684\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u89e3\u7801\u7684\u6548\u7387\u74f6\u9888\uff0c\u5728\u663e\u8457\u52a0\u901f\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u7528\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u8def\u5f84\u3002"}}
{"id": "2601.17933", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17933", "abs": "https://arxiv.org/abs/2601.17933", "authors": ["Laurent Caraffa"], "title": "Dissipative Learning: A Framework for Viable Adaptive Systems", "comment": "68 pages, 14 figures", "summary": "We propose a perspective in which learning is an intrinsically dissipative process. Forgetting and regularization are not heuristic add-ons but structural requirements for adaptive systems. Drawing on information theory, thermodynamics, and information geometry, we introduce the BEDS (Bayesian Emergent Dissipative Structures) framework, modeling learning as the evolution of compressed belief states under dissipation constraints.\n  A central contribution is the Conditional Optimality Theorem, showing that Fisher-Rao regularization measuring change via information divergence rather than Euclidean distance is the unique thermodynamically optimal regularization strategy, achieving minimal dissipation. Euclidean regularization is shown to be structurally suboptimal. The framework unifies existing methods (Ridge, SIGReg, EMA, SAC) as special cases of a single governing equation.\n  Within this view, overfitting corresponds to over-crystallization, while catastrophic forgetting reflects insufficient dissipation control. The framework distinguishes BEDS-crystallizable problems, where beliefs converge to stable equilibria, from BEDS-maintainable problems, which require continual adaptation. It extends naturally to continual and multi-agent systems, where viability, stability under adaptation and finite resources replaces asymptotic optimality as the primary criterion. Overall, this work reframes learning as maintaining viable belief states under dissipation constraints, providing a principled lens on forgetting, regularization, and stability.", "AI": {"tldr": "\u5c06\u5b66\u4e60\u91cd\u65b0\u5b9a\u4e49\u4e3a\u53d7\u8017\u6563\u7ea6\u675f\u7684\u538b\u7f29\u4fe1\u5ff5\u72b6\u6001\u6f14\u5316\u8fc7\u7a0b\uff0c\u63d0\u51faBEDS\u6846\u67b6\u8bc1\u660eFisher-Rao\u6b63\u5219\u5316\u662f\u552f\u4e00\u70ed\u529b\u5b66\u6700\u4f18\u7b56\u7565\uff0c\u7edf\u4e00\u73b0\u6709\u65b9\u6cd5\u5e76\u533a\u5206\u4e24\u7c7b\u5b66\u4e60\u95ee\u9898", "motivation": "\u6311\u6218\u4f20\u7edf\u5b66\u4e60\u89c2\uff0c\u8bba\u8bc1\u9057\u5fd8\u548c\u6b63\u5219\u5316\u662f\u81ea\u9002\u5e94\u7cfb\u7edf\u7684\u7ed3\u6784\u6027\u9700\u6c42\u800c\u975e\u542f\u53d1\u5f0f\u9644\u52a0\uff0c\u4ece\u7269\u7406\u5b66\u7b2c\u4e00\u6027\u539f\u7406\u91cd\u6784\u5b66\u4e60\u7406\u8bba", "method": "\u57fa\u4e8e\u4fe1\u606f\u8bba\u3001\u70ed\u529b\u5b66\u548c\u4fe1\u606f\u51e0\u4f55\u5b66\uff0c\u6784\u5efaBEDS\uff08\u8d1d\u53f6\u65af\u6d8c\u73b0\u8017\u6563\u7ed3\u6784\uff09\u6846\u67b6\uff0c\u5c06\u5b66\u4e60\u5efa\u6a21\u4e3a\u8017\u6563\u7ea6\u675f\u4e0b\u7684\u4fe1\u5ff5\u72b6\u6001\u6f14\u5316", "result": "\u8bc1\u660eFisher-Rao\u6b63\u5219\u5316\u662f\u552f\u4e00\u70ed\u529b\u5b66\u6700\u4f18\u7b56\u7565\uff1b\u7edf\u4e00Ridge\u3001SIGReg\u7b49\u65b9\u6cd5\u4e3a\u7279\u4f8b\uff1b\u63d0\u51fa\u8fc7\u62df\u5408=\u8fc7\u7ed3\u6676\u5316\u3001\u707e\u96be\u6027\u9057\u5fd8=\u8017\u6563\u4e0d\u8db3\u7684\u65b0\u89e3\u91ca\uff1b\u533a\u5206\u7ed3\u6676\u5316\u95ee\u9898\u4e0e\u7ef4\u6301\u6027\u95ee\u9898\u7684\u672c\u8d28\u5dee\u5f02", "conclusion": "\u5b66\u4e60\u672c\u8d28\u662f\u5728\u8017\u6563\u7ea6\u675f\u4e0b\u7ef4\u6301\u53ef\u884c\u4fe1\u5ff5\u72b6\u6001\u7684\u8fc7\u7a0b\uff0c\u7528\u7a33\u5b9a\u6027\u66ff\u4ee3\u6e10\u8fdb\u6700\u4f18\u6027\u4f5c\u4e3a\u6838\u5fc3\u5224\u636e\uff0c\u4e3a\u6301\u7eed\u5b66\u4e60\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2601.17935", "categories": ["cs.LG", "cs.CR", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.17935", "abs": "https://arxiv.org/abs/2601.17935", "authors": ["Daniel Commey", "Matilda Nkoom", "Yousef Alsenani", "Sena G. Hounsinou", "Garth V. Crosby"], "title": "FedGraph-VASP: Privacy-Preserving Federated Graph Learning with Post-Quantum Security for Cross-Institutional Anti-Money Laundering", "comment": null, "summary": "Virtual Asset Service Providers (VASPs) face a fundamental tension between regulatory compliance and user privacy when detecting cross-institutional money laundering. Current approaches require either sharing sensitive transaction data or operating in isolation, leaving critical cross-chain laundering patterns undetected. We present FedGraph-VASP, a privacy-preserving federated graph learning framework that enables collaborative anti-money laundering (AML) without exposing raw user data. Our key contribution is a Boundary Embedding Exchange protocol that shares only compressed, non-invertible graph neural network representations of boundary accounts. These exchanges are secured using post-quantum cryptography, specifically the NIST-standardized Kyber-512 key encapsulation mechanism combined with AES-256-GCM authenticated encryption. Experiments on the Elliptic Bitcoin dataset with realistic Louvain partitioning show that FedGraph-VASP achieves an F1-score of 0.508, outperforming the state-of-the-art generative baseline FedSage+ (F1 = 0.453) by 12.1 percent on binary fraud detection. We further show robustness under low-connectivity settings where generative imputation degrades performance, while approaching centralized performance (F1 = 0.620) in high-connectivity regimes. We additionally evaluate generalization on an Ethereum fraud detection dataset, where FedGraph-VASP (F1 = 0.635) is less effective under sparse cross-silo connectivity, while FedSage+ excels (F1 = 0.855), outperforming even local training (F1 = 0.785). These results highlight a topology-dependent trade-off: embedding exchange benefits connected transaction graphs, whereas generative imputation can dominate in highly modular sparse graphs. A privacy audit shows embeddings are only partially invertible (R^2 = 0.32), limiting exact feature recovery.", "AI": {"tldr": "FedGraph-VASP is a privacy-preserving federated graph learning framework that enables collaborative anti-money laundering across VASPs without exposing raw user data, using boundary embedding exchange with post-quantum cryptography.", "motivation": "VASPs face a tension between regulatory compliance and user privacy in detecting cross-institutional money laundering, as current approaches either require sharing sensitive data or operate in isolation missing cross-chain patterns.", "method": "Proposes FedGraph-VASP with a Boundary Embedding Exchange protocol that shares only compressed, non-invertible graph neural network representations of boundary accounts, secured by Kyber-512 and AES-256-GCM encryption.", "result": "Achieves F1-score of 0.508 on Bitcoin fraud detection (12.1% improvement over FedSage+), approaches centralized performance in high-connectivity regimes, but shows topology-dependent trade-offs with generative methods on sparse graphs.", "conclusion": "The framework effectively enables collaborative AML while preserving privacy, though its performance depends on transaction graph topology - embedding exchange benefits connected graphs while generative imputation excels in modular sparse graphs."}}
{"id": "2601.17958", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17958", "abs": "https://arxiv.org/abs/2601.17958", "authors": ["Ido Andrew Atad", "Itamar Zimerman", "Shahar Katz", "Lior Wolf"], "title": "TensorLens: End-to-End Transformer Analysis via High-Order Attention Tensors", "comment": "17 pages, 7 figures", "summary": "Attention matrices are fundamental to transformer research, supporting a broad range of applications including interpretability, visualization, manipulation, and distillation. Yet, most existing analyses focus on individual attention heads or layers, failing to account for the model's global behavior. While prior efforts have extended attention formulations across multiple heads via averaging and matrix multiplications or incorporated components such as normalization and FFNs, a unified and complete representation that encapsulates all transformer blocks is still lacking. We address this gap by introducing TensorLens, a novel formulation that captures the entire transformer as a single, input-dependent linear operator expressed through a high-order attention-interaction tensor. This tensor jointly encodes attention, FFNs, activations, normalizations, and residual connections, offering a theoretically coherent and expressive linear representation of the model's computation. TensorLens is theoretically grounded and our empirical validation shows that it yields richer representations than previous attention-aggregation methods. Our experiments demonstrate that the attention tensor can serve as a powerful foundation for developing tools aimed at interpretability and model understanding. Our code is attached as a supplementary.", "AI": {"tldr": "TensorLens is a novel formulation that represents the entire transformer model as a single input-dependent linear operator using a high-order attention-interaction tensor, enabling better interpretability and model understanding.", "motivation": "Existing analyses of attention matrices focus on individual heads or layers, failing to capture the transformer's global behavior. Prior work lacks a unified representation that encompasses all transformer components.", "method": "The authors introduce TensorLens, which captures the entire transformer as a single linear operator expressed through a high-order attention-interaction tensor that jointly encodes attention, FFNs, activations, normalizations, and residual connections.", "result": "Empirical validation shows TensorLens yields richer representations than previous attention-aggregation methods and serves as an effective foundation for interpretability tools.", "conclusion": "TensorLens provides a theoretically coherent and expressive linear representation of transformer computation, offering a powerful framework for advancing interpretability research and model understanding."}}
{"id": "2601.17986", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17986", "abs": "https://arxiv.org/abs/2601.17986", "authors": ["Anders Eklund"], "title": "Federated learning for unpaired multimodal data through a homogeneous transformer model", "comment": null, "summary": "Training of multimodal foundation models is currently restricted to centralized data centers containing massive, aligned datasets (e.g., image-text pairs). However, in realistic federated environments, data is often unpaired and fragmented across disjoint nodes; one node may hold sensor data, while another holds textual logs. These datasets are strictly private and share no common samples. Current federated learning (FL) methods fail in this regime, as they assume local clients possess aligned pairs or require sharing raw feature embeddings, which violates data sovereignty. We propose a novel framework to train a global multimodal transformer across decentralized nodes with disjoint modalities. We introduce a small public anchor set to align disjoint private manifolds. Using Gram matrices calculated from these public anchors, we enforce semantic alignment across modalities through centered kernel alignment without ever transmitting private samples, offering a mathematically superior privacy guarantee compared to prototype sharing. Further, we introduce a subspace-stabilized fine-tuning method to handle FL with huge transformer models. We strictly decouple domain-specific magnitude shifts from semantic direction, ensuring that nodes with varying sensor characteristics align geometrically to the global consensus. Lastly, we propose precision weighted averaging, where efficiently obtained uncertainty estimates are used to downweight uncertain nodes. This paper establishes the mathematical backbone for federated unpaired foundation models, enabling a global model to learn a unified representation of the world from fragmented, disjoint, and private data silos without requiring centralized storage or paired samples.", "AI": {"tldr": "\u63d0\u51fa\u8054\u90a6\u65e0\u914d\u5bf9\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u516c\u5171\u951a\u70b9\u96c6\u548c\u6838\u5bf9\u9f50\u5b9e\u73b0\u8de8\u8282\u70b9\u8bed\u4e49\u5bf9\u9f50\uff0c\u65e0\u9700\u5171\u4eab\u539f\u59cb\u6570\u636e\uff0c\u89e3\u51b3\u6570\u636e\u5b64\u5c9b\u548c\u9690\u79c1\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u8054\u90a6\u73af\u5883\u4e2d\u6570\u636e\u5e38\u4e3a\u672a\u914d\u5bf9\u3001\u8de8\u8282\u70b9\u788e\u7247\u5316\u4e14\u4e25\u683c\u79c1\u6709\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u914d\u5bf9\u6570\u636e\u6216\u5171\u4eab\u7279\u5f81\uff0c\u8fdd\u53cd\u6570\u636e\u4e3b\u6743\uff0c\u65e0\u6cd5\u9002\u7528\u3002", "method": "\u5f15\u5165\u5c0f\u89c4\u6a21\u516c\u5171\u951a\u70b9\u96c6\uff0c\u5229\u7528Gram\u77e9\u9635\u548c\u4e2d\u5fc3\u5316\u6838\u5bf9\u9f50\u5b9e\u73b0\u8de8\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\uff1b\u63d0\u51fa\u5b50\u7a7a\u95f4\u7a33\u5b9a\u5fae\u8c03\u89e3\u8026\u57df\u7279\u5b9a\u5e45\u5ea6\u53d8\u5316\u4e0e\u8bed\u4e49\u65b9\u5411\uff1b\u91c7\u7528\u7cbe\u5ea6\u52a0\u6743\u5e73\u5747\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u964d\u6743\u4e0d\u786e\u5b9a\u8282\u70b9\u3002", "result": "\u5efa\u7acb\u4e86\u8054\u90a6\u65e0\u914d\u5bf9\u57fa\u7840\u6a21\u578b\u7684\u6570\u5b66\u57fa\u7840\uff0c\u4f7f\u5168\u5c40\u6a21\u578b\u80fd\u4ece\u788e\u7247\u5316\u3001\u4e92\u65a5\u4e14\u79c1\u6709\u7684\u6570\u636e\u5b64\u5c9b\u4e2d\u5b66\u4e60\u7edf\u4e00\u4e16\u754c\u8868\u5f81\uff0c\u65e0\u9700\u4e2d\u5fc3\u5316\u5b58\u50a8\u6216\u914d\u5bf9\u6837\u672c\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u6570\u5b66\u4e0a\u66f4\u4f18\u7684\u9690\u79c1\u4fdd\u969c\uff0c\u5b9e\u73b0\u4e86\u5728\u4e25\u683c\u6570\u636e\u4e3b\u6743\u4e0b\u7684\u8054\u90a6\u5b66\u4e60\uff0c\u63a8\u52a8\u4e86\u9690\u79c1\u4fdd\u62a4\u4e0e\u5206\u5e03\u5f0fAI\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.17995", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17995", "abs": "https://arxiv.org/abs/2601.17995", "authors": ["Shudi Weng", "Ming Xiao", "Mikael Skoglund"], "title": "Coding-Enforced Resilient and Secure Aggregation for Hierarchical Federated Learning", "comment": null, "summary": "Hierarchical federated learning (HFL) has emerged as an effective paradigm to enhance link quality between clients and the server. However, ensuring model accuracy while preserving privacy under unreliable communication remains a key challenge in HFL, as the coordination among privacy noise can be randomly disrupted. To address this limitation, we propose a robust hierarchical secure aggregation scheme, termed H-SecCoGC, which integrates coding strategies to enforce structured aggregation. The proposed scheme not only ensures accurate global model construction under varying levels of privacy, but also avoids the partial participation issue, thereby significantly improving robustness, privacy preservation, and learning efficiency. Both theoretical analyses and experimental results demonstrate the superiority of our scheme under unreliable communication across arbitrarily strong privacy guarantees", "AI": {"tldr": "\u672c\u6587\u63d0\u51faH-SecCoGC\uff0c\u4e00\u79cd\u7528\u4e8e\u8054\u90a6\u5b66\u4e60\u7684\u9c81\u68d2\u5206\u5c42\u5b89\u5168\u805a\u5408\u65b9\u6848\uff0c\u901a\u8fc7\u7f16\u7801\u7b56\u7565\u5728\u4e0d\u53ef\u9760\u901a\u4fe1\u6761\u4ef6\u4e0b\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u548c\u9690\u79c1\u6027\u3002", "motivation": "\u5728\u5206\u5c42\u8054\u90a6\u5b66\u4e60\uff08HFL\uff09\u4e2d\uff0c\u5982\u4f55\u5728\u4e0d\u53ef\u9760\u901a\u4fe1\u60c5\u51b5\u4e0b\u786e\u4fdd\u6a21\u578b\u7cbe\u5ea6\u540c\u65f6\u4fdd\u62a4\u9690\u79c1\u4ecd\u662f\u5173\u952e\u6311\u6218\uff0c\u56e0\u4e3a\u9690\u79c1\u566a\u58f0\u7684\u534f\u8c03\u53ef\u80fd\u88ab\u968f\u673a\u7834\u574f\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u9c81\u68d2\u7684\u5206\u5c42\u5b89\u5168\u805a\u5408\u65b9\u6848H-SecCoGC\uff0c\u96c6\u6210\u7f16\u7801\u7b56\u7565\u4ee5\u5b9e\u73b0\u7ed3\u6784\u5316\u805a\u5408\u3002", "result": "\u8be5\u65b9\u6848\u5728\u4e0d\u540c\u9690\u79c1\u7ea7\u522b\u4e0b\u786e\u4fdd\u51c6\u786e\u7684\u5168\u5c40\u6a21\u578b\u6784\u5efa\uff0c\u907f\u514d\u90e8\u5206\u53c2\u4e0e\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u9c81\u68d2\u6027\u3001\u9690\u79c1\u4fdd\u62a4\u548c\u5b66\u4e60\u6548\u7387\u3002\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u7ed3\u679c\u5747\u9a8c\u8bc1\u5176\u5728\u4e0d\u53ef\u9760\u901a\u4fe1\u548c\u4efb\u610f\u5f3a\u9690\u79c1\u4fdd\u8bc1\u4e0b\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "H-SecCoGC\u65b9\u6848\u6709\u6548\u89e3\u51b3\u4e86HFL\u5728\u4e0d\u53ef\u9760\u901a\u4fe1\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u548c\u6548\u7387\u7684\u540c\u65f6\u63d0\u4f9b\u5f3a\u9690\u79c1\u4fdd\u8bc1\u3002"}}
{"id": "2601.18030", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18030", "abs": "https://arxiv.org/abs/2601.18030", "authors": ["Markus N. Rabe", "Judith Clymo", "Zheren Dong"], "title": "Spelling Bee Embeddings for Language Modeling", "comment": null, "summary": "We introduce a simple modification to the embedding layer. The key change is to infuse token embeddings with information about their spelling. Models trained with these embeddings improve not only on spelling, but also across standard benchmarks. We conduct scaling studies for models with 40M to 800M parameters, which suggest that the improvements are equivalent to needing about 8% less compute and data to achieve the same test loss.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u7684\u5d4c\u5165\u5c42\u4fee\u6539\uff0c\u901a\u8fc7\u4e3atoken\u5d4c\u5165\u6ce8\u5165\u62fc\u5199\u4fe1\u606f\uff0c\u63d0\u5347\u6a21\u578b\u5728\u62fc\u5199\u4efb\u52a1\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u6027\u80fd\uff0c\u53ef\u5b9e\u73b0\u7ea68%\u7684\u8ba1\u7b97\u548c\u6570\u636e\u8282\u7701\u3002", "motivation": "\u6807\u51c6token\u5d4c\u5165\u7f3a\u4e4f\u62fc\u5199\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5728\u62fc\u5199\u76f8\u5173\u4efb\u52a1\u53ca\u901a\u7528\u8bed\u8a00\u7406\u89e3\u4e0a\u7684\u6027\u80fd\u3002\u901a\u8fc7\u5d4c\u5165\u62fc\u5199\u7279\u5f81\uff0c\u6709\u671b\u63d0\u5347\u6a21\u578b\u6548\u7387\u548c\u8868\u73b0\u3002", "method": "\u5bf9\u5d4c\u5165\u5c42\u8fdb\u884c\u7b80\u5355\u4fee\u6539\uff0c\u6838\u5fc3\u662f\u4e3atoken\u5d4c\u5165\u6ce8\u5165\u62fc\u5199\u4fe1\u606f\u3002", "result": "1) \u6a21\u578b\u5728\u62fc\u5199\u4efb\u52a1\u548c\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5747\u6709\u63d0\u5347\uff1b2) 40M-800M\u53c2\u6570\u89c4\u6a21\u7684\u6269\u5c55\u7814\u7a76\u8868\u660e\uff0c\u8fbe\u5230\u76f8\u540c\u6d4b\u8bd5\u635f\u5931\u53ef\u51cf\u5c11\u7ea68%\u7684\u8ba1\u7b97\u548c\u6570\u636e\u9700\u6c42\u3002", "conclusion": "\u5728\u5d4c\u5165\u5c42\u4e2d\u878d\u5165\u62fc\u5199\u4fe1\u606f\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u80fd\u63d0\u5347\u6027\u80fd\u5e76\u964d\u4f4e\u8d44\u6e90\u6d88\u8017\uff0c\u5177\u6709\u663e\u8457\u7684\u6548\u7387\u4f18\u52bf\u3002"}}
{"id": "2601.18032", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2601.18032", "abs": "https://arxiv.org/abs/2601.18032", "authors": ["Brijesh FNU", "Viet Thanh Duy Nguyen", "Ashima Sharma", "Md Harun Rashid Molla", "Chengyi Xu", "Truong-Son Hy"], "title": "Multimodal Machine Learning for Soft High-k Elastomers under Data Scarcity", "comment": null, "summary": "Dielectric materials are critical building blocks for modern electronics such as sensors, actuators, and transistors. With the rapid recent advance in soft and stretchable electronics for emerging human- and robot-interfacing applications, there is a surging need for high-performance dielectric elastomers. However, it remains a grand challenge to develop soft elastomers that simultaneously possess high dielectric constants (k, related to energy storage capacity) and low Young's moduli (E, related to mechanical flexibility). While some new elastomer designs have been reported in individual (mostly one-off) studies, almost no structured dataset is currently available for dielectric elastomers that systematically encompasses their molecular sequence, dielectric, and mechanical properties. Within this context, we curate a compact, high-quality dataset of acrylate-based dielectric elastomers, one of the most widely explored elastomer backbones due to its versatile chemistry and molecular design flexibility, by screening and aggregating experimental results from the literature over the past 10 years. Building on this dataset, we propose a multimodal learning framework that leverages large-scale pretrained polymer representations from graph- and sequence-based encoders. These pretrained embeddings transfer rich chemical and structural knowledge from vast polymer corpora, enabling accurate few-shot prediction of both dielectric and mechanical properties from molecular sequences. Our results represent a new paradigm for transferring knowledge from pretrained multimodal models to overcome severe data scarcity, which can be readily translated to other polymer backbones (e.g., silicones, urethanes) and thus accelerate data-efficient discovery of soft high-k dielectric elastomers. Our source code and dataset are publicly available at https://github.com/HySonLab/Polymers", "AI": {"tldr": "Researchers developed a multimodal ML framework using pretrained polymer models to predict dielectric and mechanical properties of acrylate elastomers from molecular sequences, enabling few-shot learning despite limited data.", "motivation": "Soft electronics require dielectric elastomers with both high dielectric constants (k) and low Young's moduli (E), but no structured datasets exist linking molecular sequences to these properties, hindering systematic development.", "method": "Curated a compact dataset of acrylate-based dielectric elastomers from literature, then built a multimodal learning framework leveraging graph- and sequence-based pretrained polymer encoders for few-shot property prediction.", "result": "The model accurately predicts dielectric and mechanical properties from molecular sequences, demonstrating successful knowledge transfer from large-scale polymer corpora to overcome data scarcity.", "conclusion": "This pretrained multimodal approach provides a generalizable paradigm for accelerating discovery of high-performance dielectric elastomers across different polymer backbones."}}
{"id": "2601.18064", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.18064", "abs": "https://arxiv.org/abs/2601.18064", "authors": ["Hasi Hays"], "title": "Resonant Sparse Geometry Networks", "comment": null, "summary": "We introduce Resonant Sparse Geometry Networks (RSGN), a brain-inspired architecture with self-organizing sparse\n  hierarchical input-dependent connectivity. Unlike Transformer architectures that employ dense attention mechanisms with\n  O(n^2) computational complexity, RSGN embeds computational nodes in learned hyperbolic space where connection strength\n  decays with geodesic distance, achieving dynamic sparsity that adapts to each input. The architecture operates on two\n  distinct timescales: fast differentiable activation propagation optimized through gradient descent, and slow\n  Hebbian-inspired structural learning for connectivity adaptation through local correlation rules. We provide rigorous\n  mathematical analysis demonstrating that RSGN achieves O(n*k) computational complexity, where k << n represents the average\n  active neighborhood size. Experimental evaluation on hierarchical classification and long-range dependency tasks\n  demonstrates that RSGN achieves 96.5% accuracy on long-range dependency tasks while using approximately 15x fewer\n  parameters than standard Transformers. On challenging hierarchical classification with 20 classes, RSGN achieves 23.8%\n  accuracy (compared to 5% random baseline) with only 41,672 parameters, nearly 10x fewer than the Transformer baselines\n  which require 403,348 parameters to achieve 30.1% accuracy. Our ablation studies confirm the contribution of each architectural\n  component, with Hebbian learning providing consistent improvements. These results suggest that brain-inspired principles\n  of sparse, geometrically-organized computation offer a promising direction toward more efficient and biologically plausible\n  neural architectures.", "AI": {"tldr": "RSGN is a brain-inspired neural architecture that uses self-organizing sparse connectivity in hyperbolic space with dual-timescale learning, achieving O(n*k) complexity and using 15x fewer parameters than Transformers while maintaining high accuracy on long-range dependency and hierarchical classification tasks.", "motivation": "Transformers' dense attention mechanisms have O(n\u00b2) computational complexity, which is inefficient. The paper seeks more efficient, biologically plausible architectures inspired by the brain's sparse, hierarchical connectivity patterns.", "method": "RSGN embeds nodes in learned hyperbolic space where connection strength decays with geodesic distance. It operates on two timescales: fast gradient-based activation propagation and slow Hebbian-inspired structural learning via local correlation rules for dynamic sparsity adaptation.", "result": "RSGN achieves 96.5% accuracy on long-range dependency tasks with ~15x fewer parameters than Transformers. On 20-class hierarchical classification, it reaches 23.8% accuracy (vs 5% random) using only 41,672 parameters\u2014nearly 10x fewer than Transformers' 403,348 parameters for 30.1% accuracy. Mathematical analysis confirms O(n*k) complexity.", "conclusion": "Brain-inspired sparse, geometrically-organized computation offers a promising path toward more efficient and biologically plausible neural architectures compared to dense Transformer models."}}
{"id": "2601.18076", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18076", "abs": "https://arxiv.org/abs/2601.18076", "authors": ["Alexandra Chouldechova", "A. Feder Cooper", "Solon Barocas", "Abhinav Palia", "Dan Vann", "Hanna Wallach"], "title": "Comparison requires valid measurement: Rethinking attack success rate comparisons in AI red teaming", "comment": null, "summary": "We argue that conclusions drawn about relative system safety or attack method efficacy via AI red teaming are often not supported by evidence provided by attack success rate (ASR) comparisons. We show, through conceptual, theoretical, and empirical contributions, that many conclusions are founded on apples-to-oranges comparisons or low-validity measurements. Our arguments are grounded in asking a simple question: When can attack success rates be meaningfully compared? To answer this question, we draw on ideas from social science measurement theory and inferential statistics, which, taken together, provide a conceptual grounding for understanding when numerical values obtained through the quantification of system attributes can be meaningfully compared. Through this lens, we articulate conditions under which ASRs can and cannot be meaningfully compared. Using jailbreaking as a running example, we provide examples and extensive discussion of apples-to-oranges ASR comparisons and measurement validity challenges.", "AI": {"tldr": "This paper critiques AI red teaming practices, arguing that attack success rate (ASR) comparisons often lack validity due to inappropriate \"apples-to-oranges\" comparisons and low-validity measurements. Using social science measurement theory and inferential statistics, it establishes conditions for when ASRs can be meaningfully compared, with jailbreaking as a key example.", "motivation": "Many conclusions about AI system safety and attack method efficacy are drawn from ASR comparisons that are methodologically flawed, potentially leading to misleading claims about relative safety and vulnerability.", "method": "The authors apply concepts from social science measurement theory and inferential statistics to develop a conceptual framework for evaluating when numerical values from system attribute quantification can be meaningfully compared, combining conceptual, theoretical, and empirical analysis.", "result": "The paper identifies specific conditions under which ASRs can and cannot be meaningfully compared, and provides extensive examples of invalid comparisons in AI red teaming, demonstrating common measurement validity challenges in jailbreaking scenarios.", "conclusion": "Current ASR comparison practices in AI red teaming often lack scientific rigor; meaningful safety assessments require careful consideration of measurement validity and comparability conditions to avoid drawing unsupported conclusions about system safety."}}
{"id": "2601.18081", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18081", "abs": "https://arxiv.org/abs/2601.18081", "authors": ["Peixuan Han", "Yingjie Yu", "Jingjun Xu", "Jiaxuan You"], "title": "DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal", "comment": null, "summary": "Despite the growing adoption of large language models (LLMs) in scientific research workflows, automated support for academic rebuttal, a crucial step in academic communication and peer review, remains largely underexplored. Existing approaches typically rely on off-the-shelf LLMs or simple pipelines, which struggle with long-context understanding and often fail to produce targeted and persuasive responses. In this paper, we propose DRPG, an agentic framework for automatic academic rebuttal generation that operates through four steps: Decompose reviews into atomic concerns, Retrieve relevant evidence from the paper, Plan rebuttal strategies, and Generate responses accordingly. Notably, the Planner in DRPG reaches over 98% accuracy in identifying the most feasible rebuttal direction. Experiments on data from top-tier conferences demonstrate that DRPG significantly outperforms existing rebuttal pipelines and achieves performance beyond the average human level using only an 8B model. Our analysis further demonstrates the effectiveness of the planner design and its value in providing multi-perspective and explainable suggestions. We also showed that DRPG works well in a more complex multi-round setting. These results highlight the effectiveness of DRPG and its potential to provide high-quality rebuttal content and support the scaling of academic discussions. Codes for this work are available at https://github.com/ulab-uiuc/DRPG-RebuttalAgent.", "AI": {"tldr": "This paper proposes DRPG, an agentic framework that automatically generates academic rebuttals through four steps: decomposing reviews, retrieving evidence, planning strategies, and generating responses. Using only an 8B model, it outperforms existing methods and exceeds average human performance, with its planner achieving 98% accuracy in identifying effective rebuttal directions.", "motivation": "Automated academic rebuttal support remains underexplored despite growing LLM adoption in scientific research. Existing approaches using off-the-shelf LLMs or simple pipelines struggle with long-context understanding and fail to produce targeted, persuasive responses needed for peer review communication.", "method": "DRPG is a four-step agentic framework: (1) Decompose reviews into atomic concerns, (2) Retrieve relevant evidence from the paper, (3) Plan rebuttal strategies with a specialized Planner component, and (4) Generate targeted responses. The framework operates using an 8B parameter model.", "result": "The Planner achieves over 98% accuracy in identifying feasible rebuttal directions. DRPG significantly outperforms existing rebuttal pipelines and exceeds average human performance levels. It demonstrates effectiveness in multi-round settings and provides multi-perspective, explainable suggestions.", "conclusion": "DRPG effectively automates high-quality academic rebuttal generation, showing potential to scale academic discussions and provide valuable support for researchers navigating peer review processes. The framework's success with a relatively small 8B model suggests efficient, practical applicability."}}
{"id": "2601.18089", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18089", "abs": "https://arxiv.org/abs/2601.18089", "authors": ["Venmugil Elango", "Nidhi Bhatia", "Roger Waleffe", "Rasoul Shafipour", "Tomer Asida", "Abhinav Khattar", "Nave Assaf", "Maximilian Golub", "Joey Guman", "Tiyasa Mitra", "Ritchie Zhao", "Ritika Borkar", "Ran Zilberstein", "Mostofa Patwary", "Mohammad Shoeybi", "Bita Rouhani"], "title": "LatentMoE: Toward Optimal Accuracy per FLOP and Parameter in Mixture of Experts", "comment": null, "summary": "Mixture of Experts (MoEs) have become a central component of many state-of-the-art open-source and proprietary large language models. Despite their widespread adoption, it remains unclear how close existing MoE architectures are to optimal with respect to inference cost, as measured by accuracy per floating-point operation and per parameter. In this work, we revisit MoE design from a hardware-software co-design perspective, grounded in empirical and theoretical considerations. We characterize key performance bottlenecks across diverse deployment regimes, spanning offline high-throughput execution and online, latency-critical inference. Guided by these insights, we introduce LatentMoE, a new model architecture resulting from systematic design exploration and optimized for maximal accuracy per unit of compute. Empirical design space exploration at scales of up to 95B parameters and over a 1T-token training horizon, together with supporting theoretical analysis, shows that LatentMoE consistently outperforms standard MoE architectures in terms of accuracy per FLOP and per parameter. Given its strong performance, the LatentMoE architecture has been adopted by the flagship Nemotron-3 Super and Ultra models and scaled to substantially larger regimes, including longer token horizons and larger model sizes, as reported in Nvidia et al. (arXiv:2512.20856).", "AI": {"tldr": "This paper introduces LatentMoE, a new Mixture of Experts architecture optimized for accuracy per compute unit, which outperforms standard MoEs and has been adopted in NVIDIA's Nemotron-3 models.", "motivation": "Despite widespread adoption of MoEs in LLMs, it's unclear how close existing architectures are to optimal in terms of inference cost measured by accuracy per FLOP and per parameter.", "method": "Hardware-software co-design approach, empirical and theoretical analysis of performance bottlenecks across deployment regimes, systematic design exploration with scales up to 95B parameters and 1T-token training.", "result": "LatentMoE consistently outperforms standard MoE architectures in accuracy per FLOP and per parameter, validated through extensive empirical exploration and theoretical analysis.", "conclusion": "LatentMoE has been successfully adopted by NVIDIA's Nemotron-3 Super and Ultra flagship models and scaled to larger regimes including longer token horizons and larger model sizes."}}
{"id": "2601.18091", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18091", "abs": "https://arxiv.org/abs/2601.18091", "authors": ["Longwei Ding", "Anhao Zhao", "Fanghua Ye", "Ziyang Chen", "Xiaoyu Shen"], "title": "From LLMs to LRMs: Rethinking Pruning for Reasoning-Centric Models", "comment": "18 pages, 7 figures", "summary": "Large language models (LLMs) are increasingly costly to deploy, motivating extensive research on model pruning. However, most existing studies focus on instruction-following LLMs, leaving it unclear whether established pruning strategies transfer to reasoning-augmented models that explicitly generate long intermediate reasoning traces. In this work, we conduct a controlled study of pruning for both instruction-following ($\\textbf{LLM-instruct}$) and reasoning-augmented ($\\textbf{LLM-think}$) models. To isolate the effects of pruning, we align pruning calibration and post-pruning recovery data with each model's original training distribution, which we show yields more stable and reliable pruning behavior. We evaluate static depth pruning, static width pruning, and dynamic pruning across 17 tasks spanning classification, generation, and reasoning. Our results reveal clear paradigm-dependent differences: depth pruning outperforms width pruning on classification tasks, while width pruning is more robust for generation and reasoning. Moreover, static pruning better preserves reasoning performance, whereas dynamic pruning excels on classification and generation but remains challenging for long-chain reasoning. These findings underscore the need for pruning strategies that explicitly account for the distinct characteristics of reasoning-augmented LLMs. Our code is publicly available at https://github.com/EIT-NLP/LRM-Pruning.", "AI": {"tldr": "A controlled study reveals that pruning strategies must be tailored to model type (instruction-following vs reasoning-augmented) and task type: depth pruning works best for classification, width pruning for generation/reasoning, static pruning preserves reasoning, while dynamic pruning excels on classification/generation but struggles with long-chain reasoning.", "motivation": "Large language models are increasingly costly to deploy, but it's unclear whether established pruning strategies for instruction-following models transfer to reasoning-augmented models that generate intermediate reasoning traces.", "method": "Conducted a controlled study comparing pruning for instruction-following (LLM-instruct) and reasoning-augmented (LLM-think) models, aligning calibration/recovery data with original training distributions. Evaluated static depth pruning, static width pruning, and dynamic pruning across 17 classification, generation, and reasoning tasks.", "result": "Found clear paradigm-dependent differences: depth pruning > width pruning for classification; width pruning more robust for generation/reasoning; static pruning better preserves reasoning; dynamic pruning excels on classification/generation but remains challenging for long-chain reasoning.", "conclusion": "Pruning strategies must explicitly account for the distinct characteristics of reasoning-augmented LLMs, as one-size-fits-all approaches are ineffective."}}
{"id": "2601.18107", "categories": ["cs.LG", "cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18107", "abs": "https://arxiv.org/abs/2601.18107", "authors": ["Pedram Agand", "Mo Chen"], "title": "Beyond Static Datasets: Robust Offline Policy Optimization via Vetted Synthetic Transitions", "comment": "11 pages, 2 figures, 2 tables", "summary": "Offline Reinforcement Learning (ORL) holds immense promise for safety-critical domains like industrial robotics, where real-time environmental interaction is often prohibitive. A primary obstacle in ORL remains the distributional shift between the static dataset and the learned policy, which typically mandates high degrees of conservatism that can restrain potential policy improvements. We present MoReBRAC, a model-based framework that addresses this limitation through Uncertainty-Aware latent synthesis. Instead of relying solely on the fixed data, MoReBRAC utilizes a dual-recurrent world model to synthesize high-fidelity transitions that augment the training manifold. To ensure the reliability of this synthetic data, we implement a hierarchical uncertainty pipeline integrating Variational Autoencoder (VAE) manifold detection, model sensitivity analysis, and Monte Carlo (MC) dropout. This multi-layered filtering process guarantees that only transitions residing within high-confidence regions of the learned dynamics are utilized. Our results on D4RL Gym-MuJoCo benchmarks reveal significant performance gains, particularly in ``random'' and ``suboptimal'' data regimes. We further provide insights into the role of the VAE as a geometric anchor and discuss the distributional trade-offs encountered when learning from near-optimal datasets.", "AI": {"tldr": "MoReBRAC\u662f\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u6f5c\u5728\u7a7a\u95f4\u5408\u6210\uff0c\u7ed3\u5408\u53cc\u5faa\u73af\u4e16\u754c\u6a21\u578b\u548c\u5206\u5c42\u4e0d\u786e\u5b9a\u6027\u8fc7\u6ee4\u6765\u7f13\u89e3\u5206\u5e03\u504f\u79fb\uff0c\u5728\u968f\u673a\u548c\u6b21\u4f18\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u524d\u666f\u5e7f\u9614\uff0c\u4f46\u9759\u6001\u6570\u636e\u96c6\u4e0e\u5b66\u4e60\u7b56\u7565\u4e4b\u95f4\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\u8feb\u4f7f\u7b97\u6cd5\u91c7\u53d6\u4fdd\u5b88\u4e3b\u4e49\uff0c\u9650\u5236\u4e86\u7b56\u7565\u6539\u8fdb\u7684\u6f5c\u529b\u3002", "method": "\u8be5\u6846\u67b6\u91c7\u7528\u53cc\u5faa\u73af\u4e16\u754c\u6a21\u578b\u751f\u6210\u9ad8\u4fdd\u771f\u5408\u6210\u8f6c\u79fb\uff0c\u5e76\u901a\u8fc7\u96c6\u6210\u53d8\u5206\u81ea\u7f16\u7801\u5668\u6d41\u5f62\u68c0\u6d4b\u3001\u6a21\u578b\u654f\u611f\u6027\u5206\u6790\u548c\u8499\u7279\u5361\u6d1bDropout\u7684\u5206\u5c42\u4e0d\u786e\u5b9a\u6027\u7ba1\u9053\uff0c\u786e\u4fdd\u53ea\u4f7f\u7528\u9ad8\u7f6e\u4fe1\u5ea6\u533a\u57df\u7684\u8f6c\u6362\u3002", "result": "\u5728D4RL Gym-MuJoCo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\"\u968f\u673a\"\u548c\"\u6b21\u4f18\"\u6570\u636e regime \u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u5e76\u63ed\u793a\u4e86VAE\u4f5c\u4e3a\u51e0\u4f55\u951a\u70b9\u7684\u4f5c\u7528\u53ca\u8fd1\u4f18\u6570\u636e\u96c6\u7684\u5206\u5e03\u6743\u8861\u3002", "conclusion": "\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u5408\u6210\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u4f7f\u7b56\u7565\u80fd\u591f\u6446\u8131\u8fc7\u5ea6\u4fdd\u5b88\u7684\u9650\u5236\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u73af\u5883\u4e2d\u83b7\u5f97\u66f4\u597d\u6027\u80fd\uff0c\u5bf9\u5b9e\u9645\u5b89\u5168\u5173\u952e\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2601.18110", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18110", "abs": "https://arxiv.org/abs/2601.18110", "authors": ["Pedram Zaree", "Md Abdullah Al Mamun", "Yue Dong", "Ihsen Alouani", "Nael Abu-Ghazaleh"], "title": "AttenMIA: LLM Membership Inference Attack through Attention Signals", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed to enable or improve a multitude of real-world applications. Given the large size of their training data sets, their tendency to memorize training data raises serious privacy and intellectual property concerns. A key threat is the membership inference attack (MIA), which aims to determine whether a given sample was included in the model's training set. Existing MIAs for LLMs rely primarily on output confidence scores or embedding-based features, but these signals are often brittle, leading to limited attack success. We introduce AttenMIA, a new MIA framework that exploits self-attention patterns inside the transformer model to infer membership. Attention controls the information flow within the transformer, exposing different patterns for memorization that can be used to identify members of the dataset. Our method uses information from attention heads across layers and combines them with perturbation-based divergence metrics to train an effective MIA classifier. Using extensive experiments on open-source models including LLaMA-2, Pythia, and Opt models, we show that attention-based features consistently outperform baselines, particularly under the important low-false-positive metric (e.g., achieving up to 0.996 ROC AUC & 87.9% TPR@1%FPR on the WikiMIA-32 benchmark with Llama2-13b). We show that attention signals generalize across datasets and architectures, and provide a layer- and head-level analysis of where membership leakage is most pronounced. We also show that using AttenMIA to replace other membership inference attacks in a data extraction framework results in training data extraction attacks that outperform the state of the art. Our findings reveal that attention mechanisms, originally introduced to enhance interpretability, can inadvertently amplify privacy risks in LLMs, underscoring the need for new defenses.", "AI": {"tldr": "\u63d0\u51faAttenMIA\u6846\u67b6\uff0c\u5229\u7528Transformer\u81ea\u6ce8\u610f\u529b\u6a21\u5f0f\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u6210\u5458\u63a8\u7406\u653b\u51fb\uff0c\u5728\u4f4e\u8bef\u62a5\u7387\u6307\u6807\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u56e0\u8bad\u7ec3\u6570\u636e\u8bb0\u5fc6\u5316\u5f15\u53d1\u4e25\u91cd\u9690\u79c1\u4e0e\u77e5\u8bc6\u4ea7\u6743\u98ce\u9669\uff0c\u73b0\u6709\u6210\u5458\u63a8\u7406\u653b\u51fb\u4f9d\u8d56\u7684\u7f6e\u4fe1\u5ea6\u5206\u6570\u6216\u5d4c\u5165\u7279\u5f81\u8106\u5f31\u4e14\u6548\u679c\u6709\u9650\u3002", "method": "\u5f00\u53d1AttenMIA\u6846\u67b6\uff0c\u6316\u6398\u6a21\u578b\u5185\u90e8\u81ea\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u8de8\u5c42\u805a\u5408\u6ce8\u610f\u529b\u5934\u4fe1\u606f\u5e76\u7ed3\u5408\u6270\u52a8\u53d1\u6563\u5ea6\u5ea6\u91cf\u8bad\u7ec3MIA\u5206\u7c7b\u5668\u3002", "result": "\u5728LLaMA-2\u3001Pythia\u3001OPT\u7b49\u5f00\u6e90\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u6ce8\u610f\u529b\u7279\u5f81\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\uff0cWikiMIA-32\u57fa\u51c6\u8fbe0.996 ROC AUC\u548c87.9% TPR@1%FPR\uff1b\u6ce8\u610f\u529b\u4fe1\u53f7\u8de8\u6570\u636e\u96c6\u548c\u67b6\u6784\u6cdb\u5316\uff0c\u5e76\u63d0\u5347\u6570\u636e\u63d0\u53d6\u653b\u51fb\u81f3SOTA\u6c34\u5e73\u3002", "conclusion": "\u6ce8\u610f\u529b\u673a\u5236\u5728\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u610f\u5916\u653e\u5927\u4e86LLM\u9690\u79c1\u98ce\u9669\uff0c\u51f8\u663e\u4e86\u8bbe\u8ba1\u65b0\u9632\u5fa1\u63aa\u65bd\u7684\u7d27\u8feb\u6027\u3002"}}
{"id": "2601.18111", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18111", "abs": "https://arxiv.org/abs/2601.18111", "authors": ["Jean Kossaifi", "Nikola Kovachki", "Morteza Mardani", "Daniel Leibovici", "Suman Ravuri", "Ira Shokar", "Edoardo Calvello", "Mohammad Shoaib Abbas", "Peter Harrington", "Ashay Subramaniam", "Noah Brenowitz", "Boris Bonev", "Wonmin Byeon", "Karsten Kreis", "Dale Durran", "Arash Vahdat", "Mike Pritchard", "Jan Kautz"], "title": "Demystifying Data-Driven Probabilistic Medium-Range Weather Forecasting", "comment": null, "summary": "The recent revolution in data-driven methods for weather forecasting has lead to a fragmented landscape of complex, bespoke architectures and training strategies, obscuring the fundamental drivers of forecast accuracy. Here, we demonstrate that state-of-the-art probabilistic skill requires neither intricate architectural constraints nor specialized training heuristics. We introduce a scalable framework for learning multi-scale atmospheric dynamics by combining a directly downsampled latent space with a history-conditioned local projector that resolves high-resolution physics. We find that our framework design is robust to the choice of probabilistic estimator, seamlessly supporting stochastic interpolants, diffusion models, and CRPS-based ensemble training. Validated against the Integrated Forecasting System and the deep learning probabilistic model GenCast, our framework achieves statistically significant improvements on most of the variables. These results suggest scaling a general-purpose model is sufficient for state-of-the-art medium-range prediction, eliminating the need for tailored training recipes and proving effective across the full spectrum of probabilistic frameworks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u5929\u6c14\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u4e0b\u91c7\u6837\u6f5c\u5728\u7a7a\u95f4\u548c\u5386\u53f2\u6761\u4ef6\u5c40\u90e8\u6295\u5f71\u5668\uff0c\u5728\u591a\u79cd\u6982\u7387\u4f30\u8ba1\u5668\u4e0b\u5747\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u8bc1\u660e\u901a\u7528\u6a21\u578b\u6269\u5c55\u5373\u53ef\u5b9e\u73b0\u5148\u8fdb\u7684\u4e2d\u671f\u9884\u6d4b\uff0c\u65e0\u9700\u590d\u6742\u5b9a\u5236\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\u3002", "motivation": "\u5f53\u524d\u6570\u636e\u9a71\u52a8\u7684\u5929\u6c14\u9884\u6d4b\u65b9\u6cd5\u5b58\u5728\u788e\u7247\u5316\u95ee\u9898\uff0c\u590d\u6742\u7684\u5b9a\u5236\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\u63a9\u76d6\u4e86\u9884\u6d4b\u7cbe\u5ea6\u7684\u6839\u672c\u9a71\u52a8\u56e0\u7d20\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u7ed3\u5408\u76f4\u63a5\u4e0b\u91c7\u6837\u7684\u6f5c\u5728\u7a7a\u95f4\u4e0e\u5386\u53f2\u6761\u4ef6\u5c40\u90e8\u6295\u5f71\u5668\u6765\u5b66\u4e60\u591a\u5c3a\u5ea6\u5927\u6c14\u52a8\u529b\u5b66\uff0c\u8be5\u6846\u67b6\u8bbe\u8ba1\u5bf9\u6982\u7387\u4f30\u8ba1\u5668\u9009\u62e9\u5177\u6709\u9c81\u68d2\u6027\uff0c\u652f\u6301\u968f\u673a\u63d2\u503c\u3001\u6269\u6563\u6a21\u578b\u548cCRPS\u96c6\u6210\u8bad\u7ec3\u3002", "result": "\u5728\u5bf9\u6bd4\u96c6\u6210\u9884\u62a5\u7cfb\u7edf\u548c\u6df1\u5ea6\u5b66\u4e60\u6982\u7387\u6a21\u578bGenCast\u7684\u9a8c\u8bc1\u4e2d\uff0c\u8be5\u6846\u67b6\u5728\u5927\u591a\u6570\u53d8\u91cf\u4e0a\u5b9e\u73b0\u4e86\u7edf\u8ba1\u663e\u8457\u6027\u6539\u8fdb\u3002", "conclusion": "\u6269\u5c55\u901a\u7528\u6a21\u578b\u8db3\u4ee5\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u4e2d\u671f\u9884\u6d4b\uff0c\u6d88\u9664\u4e86\u5bf9\u5b9a\u5236\u8bad\u7ec3\u65b9\u6848\u7684\u9700\u6c42\uff0c\u5e76\u8bc1\u660e\u5728\u5168\u90e8\u6982\u7387\u6846\u67b6\u8303\u56f4\u5185\u5747\u6709\u6548\u3002"}}
{"id": "2601.18142", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18142", "abs": "https://arxiv.org/abs/2601.18142", "authors": ["Mingxu Zhang", "Huicheng Zhang", "Jiaming Ji", "Yaodong Yang", "Ying Sun"], "title": "Enhance the Safety in Reinforcement Learning by ADRC Lagrangian Methods", "comment": null, "summary": "Safe reinforcement learning (Safe RL) seeks to maximize rewards while satisfying safety constraints, typically addressed through Lagrangian-based methods. However, existing approaches, including PID and classical Lagrangian methods, suffer from oscillations and frequent safety violations due to parameter sensitivity and inherent phase lag. To address these limitations, we propose ADRC-Lagrangian methods that leverage Active Disturbance Rejection Control (ADRC) for enhanced robustness and reduced oscillations. Our unified framework encompasses classical and PID Lagrangian methods as special cases while significantly improving safety performance. Extensive experiments demonstrate that our approach reduces safety violations by up to 74%, constraint violation magnitudes by 89%, and average costs by 67\\%, establishing superior effectiveness for Safe RL in complex environments.", "AI": {"tldr": "\u63d0\u51faADRC-Lagrangian\u65b9\u6cd5\u7528\u4e8e\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u4e3b\u52a8\u6270\u52a8\u6291\u5236\u63a7\u5236\u89e3\u51b3\u4f20\u7edf\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\u7684\u632f\u8361\u548c\u9891\u7e41\u5b89\u5168\u8fdd\u53cd\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u53ef\u5927\u5e45\u964d\u4f4e\u5b89\u5168\u8fdd\u53cd\u548c\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u7684PID\u548c\u7ecf\u5178\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\u56e0\u53c2\u6570\u654f\u611f\u6027\u548c\u76f8\u4f4d\u6ede\u540e\uff0c\u5bfc\u81f4\u7b56\u7565\u632f\u8361\u548c\u9891\u7e41\u5b89\u5168\u7ea6\u675f\u8fdd\u53cd\uff0c\u96be\u4ee5\u5728\u590d\u6742\u73af\u5883\u4e2d\u7a33\u5b9a\u8fd0\u884c\u3002", "method": "\u5c06\u4e3b\u52a8\u6270\u52a8\u6291\u5236\u63a7\u5236\uff08ADRC\uff09\u5f15\u5165\u62c9\u683c\u6717\u65e5\u6846\u67b6\uff0c\u63d0\u51faADRC-Lagrangian\u65b9\u6cd5\uff0c\u6784\u5efa\u5305\u542b\u4f20\u7edf\u65b9\u6cd5\u4f5c\u4e3a\u7279\u4f8b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u589e\u5f3a\u7cfb\u7edf\u9c81\u68d2\u6027\u3002", "result": "\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u9a8c\u663e\u793a\uff1a\u5b89\u5168\u8fdd\u53cd\u6b21\u6570\u51cf\u5c1174%\uff0c\u7ea6\u675f\u8fdd\u53cd\u5e45\u5ea6\u964d\u4f4e89%\uff0c\u5e73\u5747\u6210\u672c\u4e0b\u964d67%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ADRC-Lagrangian\u65b9\u6cd5\u4e3a\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u5b89\u5168\u6027\u80fd\uff0c\u5728\u590d\u6742\u5e94\u7528\u573a\u666f\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2601.18150", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18150", "abs": "https://arxiv.org/abs/2601.18150", "authors": ["Zhaopeng Qiu", "Shuang Yu", "Jingqi Zhang", "Shuai Zhang", "Xue Huang", "Jingyi Yang", "Junjie Lai"], "title": "FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) for large language models (LLMs) is increasingly bottlenecked by rollout (generation), where long output sequence lengths make attention and KV-cache memory dominate end-to-end step time. FP8 offers an attractive lever for accelerating RL by reducing compute cost and memory traffic during rollout, but applying FP8 in RL introduces unique engineering and algorithmic challenges: policy weights change every step (requiring repeated quantization and weight synchronization into the inference engine) and low-precision rollouts can deviate from the higher-precision policy assumed by the trainer, causing train-inference mismatch and potential instability. This report presents a practical FP8 rollout stack for LLM RL, implemented in the veRL ecosystem with support for common training backends (e.g., FSDP/Megatron-LM) and inference engines (e.g., vLLM/SGLang). We (i) enable FP8 W8A8 linear-layer rollout using blockwise FP8 quantization, (ii) extend FP8 to KV-cache to remove long-context memory bottlenecks via per-step QKV scale recalibration, and (iii) mitigate mismatch using importance-sampling-based rollout correction (token-level TIS/MIS variants). Across dense and MoE models, these techniques deliver up to 44% rollout throughput gains while preserving learning behavior comparable to BF16 baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5b9e\u7528\u7684FP8 rollout\u6808\u7528\u4e8eLLM\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u5757\u7ea7\u91cf\u5316\u3001KV-cache\u4f18\u5316\u548c\u91cd\u8981\u6027\u91c7\u6837\u6821\u6b63\uff0c\u5728\u4fdd\u6301\u5b66\u4e60\u884c\u4e3a\u7a33\u5b9a\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u9ad8\u8fbe44%\u7684\u541e\u5410\u91cf\u63d0\u5347\u3002", "motivation": "LLM\u5f3a\u5316\u5b66\u4e60\u7684rollout\u9636\u6bb5\u5b58\u5728\u6027\u80fd\u74f6\u9888\uff0c\u957f\u8f93\u51fa\u5e8f\u5217\u5bfc\u81f4\u6ce8\u610f\u529b\u8ba1\u7b97\u548cKV-cache\u5185\u5b58\u5360\u7528\u4e3b\u5bfc\u7aef\u5230\u7aef\u65f6\u95f4\u3002FP8\u867d\u53ef\u52a0\u901f\u4f46\u9762\u4e34\u6743\u91cd\u6bcf\u6b65\u53d8\u5316\u9700\u91cd\u590d\u91cf\u5316\uff0c\u4ee5\u53ca\u4f4e\u7cbe\u5ea6rollout\u4e0e\u9ad8\u7cbe\u5ea6\u8bad\u7ec3\u7b56\u7565\u4e0d\u5339\u914d\u5bfc\u81f4\u7684\u4e0d\u7a33\u5b9a\u6027\u6311\u6218\u3002", "method": "1) \u91c7\u7528\u5757\u7ea7FP8\u91cf\u5316\u5b9e\u73b0W8A8\u7ebf\u6027\u5c42rollout\uff1b2) \u901a\u8fc7\u6bcf\u6b65QKV\u5c3a\u5ea6\u91cd\u6821\u51c6\u5c06FP8\u6269\u5c55\u81f3KV-cache\uff1b3) \u4f7f\u7528\u57fa\u4e8e\u91cd\u8981\u6027\u91c7\u6837\u7684rollout\u6821\u6b63(TIS/MIS)\u7f13\u89e3\u8bad\u7ec3-\u63a8\u7406\u4e0d\u5339\u914d\uff1b4) \u5728veRL\u751f\u6001\u4e2d\u5b9e\u73b0\uff0c\u652f\u6301FSDP/Megatron-LM\u7b49\u8bad\u7ec3\u540e\u7aef\u548cvLLM/SGLang\u7b49\u63a8\u7406\u5f15\u64ce\u3002", "result": "\u5728\u7a20\u5bc6\u548cMoE\u6a21\u578b\u4e0a\u5b9e\u73b0\u9ad8\u8fbe44%\u7684rollout\u541e\u5410\u91cf\u589e\u76ca\uff0c\u540c\u65f6\u5b66\u4e60\u884c\u4e3a\u4e0eBF16\u57fa\u7ebf\u76f8\u5f53\u3002", "conclusion": "\u8be5FP8 rollout\u6808\u4e3aLLM\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u52a0\u901f\u65b9\u6848\uff0c\u5728\u89e3\u51b3FP8\u5728RL\u4e2d\u7684\u5de5\u7a0b\u548c\u7b97\u6cd5\u6311\u6218\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u6548\uff0c\u5b9e\u73b0\u4e86\u541e\u5410\u91cf\u4e0e\u5b66\u4e60\u7a33\u5b9a\u6027\u7684\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2601.18171", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18171", "abs": "https://arxiv.org/abs/2601.18171", "authors": ["Yuguang Zhang", "Lijun Sheng", "Jian Liang", "Ran He"], "title": "Learning Fair Domain Adaptation with Virtual Label Distribution", "comment": "ICASSP 2026", "summary": "Unsupervised Domain Adaptation (UDA) aims to mitigate performance degradation when training and testing data are sampled from different distributions. While significant progress has been made in enhancing overall accuracy, most existing methods overlook performance disparities across categories-an issue we refer to as category fairness. Our empirical analysis reveals that UDA classifiers tend to favor certain easy categories while neglecting difficult ones. To address this, we propose Virtual Label-distribution-aware Learning (VILL), a simple yet effective framework designed to improve worst-case performance while preserving high overall accuracy. The core of VILL is an adaptive re-weighting strategy that amplifies the influence of hard-to-classify categories. Furthermore, we introduce a KL-divergence-based re-balancing strategy, which explicitly adjusts decision boundaries to enhance category fairness. Experiments on commonly used datasets demonstrate that VILL can be seamlessly integrated as a plug-and-play module into existing UDA methods, significantly improving category fairness.", "AI": {"tldr": "This paper addresses category fairness in Unsupervised Domain Adaptation by proposing VILL, a framework that uses adaptive re-weighting and KL-divergence-based rebalancing to improve worst-case performance while maintaining overall accuracy.", "motivation": "Most UDA methods focus on improving overall accuracy but neglect performance disparities across different categories, leading to classifiers that favor easy categories while performing poorly on difficult ones, creating fairness issues across categories.", "method": "The authors propose Virtual Label-distribution-aware Learning (VILL) with two strategies: (1) an adaptive re-weighting mechanism that amplifies influence of hard-to-classify categories, and (2) a KL-divergence-based re-balancing strategy that explicitly adjusts decision boundaries to enhance category fairness.", "result": "Experiments on standard datasets show VILL can be seamlessly integrated as a plug-and-play module into existing UDA methods, significantly improving category fairness while preserving high overall accuracy.", "conclusion": "VILL is a simple yet effective framework that successfully addresses the category fairness problem in UDA by improving worst-case category performance without sacrificing overall accuracy, making it a practical solution for real-world applications requiring balanced performance across categories."}}
{"id": "2601.18189", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18189", "abs": "https://arxiv.org/abs/2601.18189", "authors": ["Rui Wu", "Yongjun Li"], "title": "Smooth, Sparse, and Stable: Finite-Time Exact Skeleton Recovery via Smoothed Proximal Gradients", "comment": "20 pages, 8 figures", "summary": "Continuous optimization has significantly advanced causal discovery, yet existing methods (e.g., NOTEARS) generally guarantee only asymptotic convergence to a stationary point. This often yields dense weighted matrices that require arbitrary post-hoc thresholding to recover a DAG. This gap between continuous optimization and discrete graph structures remains a fundamental challenge. In this paper, we bridge this gap by proposing the Hybrid-Order Acyclicity Constraint (AHOC) and optimizing it via the Smoothed Proximal Gradient (SPG-AHOC). Leveraging the Manifold Identification Property of proximal algorithms, we provide a rigorous theoretical guarantee: the Finite-Time Oracle Property. We prove that under standard identifiability assumptions, SPG-AHOC recovers the exact DAG support (structure) in finite iterations, even when optimizing a smoothed approximation. This result eliminates structural ambiguity, as our algorithm returns graphs with exact zero entries without heuristic truncation. Empirically, SPG-AHOC achieves state-of-the-art accuracy and strongly corroborates the finite-time identification theory.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408\u9636\u65e0\u73af\u7ea6\u675f(AHOC)\u548c\u5149\u6ed1\u8fd1\u7aef\u68af\u5ea6\u7b97\u6cd5(SPG-AHOC)\uff0c\u5728\u6709\u9650\u6b21\u8fed\u4ee3\u5185\u7cbe\u786e\u6062\u590dDAG\u7ed3\u6784\uff0c\u65e0\u9700\u542f\u53d1\u5f0f\u622a\u65ad", "motivation": "\u73b0\u6709\u8fde\u7eed\u4f18\u5316\u65b9\u6cd5(\u5982NOTEARS)\u4ec5\u4fdd\u8bc1\u6e10\u8fd1\u6536\u655b\u5230\u9a7b\u70b9\uff0c\u4ea7\u751f\u7a20\u5bc6\u52a0\u6743\u77e9\u9635\uff0c\u9700\u4eba\u5de5\u540e\u5904\u7406\u622a\u65ad\u624d\u80fd\u6062\u590dDAG\uff0c\u5b58\u5728\u8fde\u7eed\u4f18\u5316\u4e0e\u79bb\u6563\u56fe\u7ed3\u6784\u95f4\u7684\u6839\u672c\u9e3f\u6c9f", "method": "\u63d0\u51fa\u6df7\u5408\u9636\u65e0\u73af\u7ea6\u675f(AHOC)\uff0c\u91c7\u7528\u5149\u6ed1\u8fd1\u7aef\u68af\u5ea6\u6cd5(SPG-AHOC)\u4f18\u5316\uff0c\u5229\u7528\u8fd1\u7aef\u7b97\u6cd5\u7684\u6d41\u5f62\u8bc6\u522b\u6027\u8d28\uff0c\u63d0\u4f9b\u6709\u9650\u65f6\u95f4Oracle\u6027\u8d28\u7406\u8bba\u4fdd\u8bc1", "result": "\u5728\u6807\u51c6\u53ef\u8bc6\u522b\u6027\u5047\u8bbe\u4e0b\uff0cSPG-AHOC\u5728\u6709\u9650\u6b21\u8fed\u4ee3\u5185\u7cbe\u786e\u6062\u590dDAG\u652f\u6301\u96c6\uff0c\u5373\u4f7f\u4f18\u5316\u5149\u6ed1\u8fd1\u4f3c\u4e5f\u80fd\u5f97\u5230\u7cbe\u786e\u96f6\u5143\u7d20\uff0c\u6d88\u9664\u7ed3\u6784\u6b67\u4e49", "conclusion": "\u7b97\u6cd5\u5b9e\u73b0\u65e0\u9700\u542f\u53d1\u5f0f\u622a\u65ad\uff0c\u7ecf\u9a8c\u7ed3\u679c\u8fbe\u5230SOTA\u7cbe\u5ea6\uff0c\u6709\u529b\u9a8c\u8bc1\u4e86\u6709\u9650\u65f6\u95f4\u8bc6\u522b\u7406\u8bba"}}
{"id": "2601.18200", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18200", "abs": "https://arxiv.org/abs/2601.18200", "authors": ["Chenyu Zhang", "Xinchen Lyu", "Chenshan Ren", "Shuhan Liu", "Qimei Cui", "Xiaofeng Tao"], "title": "HeterCSI: Channel-Adaptive Heterogeneous CSI Pretraining Framework for Generalized Wireless Foundation Models", "comment": "13 pages, 8 figures", "summary": "Wireless foundation models promise transformative capabilities for channel state information (CSI) processing across diverse 6G network applications, yet face fundamental challenges due to the inherent dual heterogeneity of CSI across both scale and scenario dimensions. However, current pretraining approaches either constrain inputs to fixed dimensions or isolate training by scale, limiting the generalization and scalability of wireless foundation models. In this paper, we propose HeterCSI, a channel-adaptive pretraining framework that reconciles training efficiency with robust cross-scenario generalization via a new understanding of gradient dynamics in heterogeneous CSI pretraining. Our key insight reveals that CSI scale heterogeneity primarily causes destructive gradient interference, while scenario diversity actually promotes constructive gradient alignment when properly managed. Specifically, we formulate heterogeneous CSI batch construction as a partitioning optimization problem that minimizes zero-padding overhead while preserving scenario diversity. To solve this, we develop a scale-aware adaptive batching strategy that aligns CSI samples of similar scales, and design a double-masking mechanism to isolate valid signals from padding artifacts. Extensive experiments on 12 datasets demonstrate that HeterCSI establishes a generalized foundation model without scenario-specific finetuning, achieving superior average performance over full-shot baselines. Compared to the state-of-the-art zero-shot benchmark WiFo, it reduces NMSE by 7.19 dB, 4.08 dB, and 5.27 dB for CSI reconstruction, time-domain, and frequency-domain prediction, respectively. The proposed HeterCSI framework also reduces training latency by 53% compared to existing approaches while improving generalization performance by 1.53 dB on average.", "AI": {"tldr": "\u63d0\u51faHeterCSI\uff0c\u4e00\u79cd\u4fe1\u9053\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u68af\u5ea6\u52a8\u529b\u5b66\u65b0\u7406\u89e3\u89e3\u51b3CSI\u53cc\u5f02\u6784\u6027\uff08\u5c3a\u5ea6\u4e0e\u573a\u666f\uff09\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u4e0e\u8de8\u573a\u666f\u6cdb\u5316\uff0c\u572812\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8eSOTA\u65b9\u6cd5\u3002", "motivation": "\u65e0\u7ebf\u57fa\u7840\u6a21\u578b\u57286G CSI\u5904\u7406\u4e2d\u9762\u4e34\u5c3a\u5ea6\u4e0e\u573a\u666f\u53cc\u5f02\u6784\u6027\u7684\u6839\u672c\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u6216\u9650\u5236\u8f93\u5165\u7ef4\u5ea6\u6216\u5b64\u7acb\u8bad\u7ec3\uff0c\u5bfc\u81f4\u6cdb\u5316\u6027\u4e0e\u53ef\u6269\u5c55\u6027\u53d7\u9650\u3002", "method": "\u63d0\u51faHeterCSI\u6846\u67b6\uff0c\u63ed\u793a\u5c3a\u5ea6\u5f02\u6784\u6027\u5bfc\u81f4\u7834\u574f\u6027\u68af\u5ea6\u5e72\u6270\u800c\u573a\u666f\u591a\u6837\u6027\u4fc3\u8fdb\u5efa\u8bbe\u6027\u68af\u5ea6\u5bf9\u9f50\uff1b\u6784\u5efa\u6279\u6b21\u4f18\u5316\u95ee\u9898\uff0c\u91c7\u7528\u5c3a\u5ea6\u611f\u77e5\u81ea\u9002\u5e94\u5206\u6279\u7b56\u7565\u4e0e\u53cc\u63a9\u7801\u673a\u5236\u5206\u79bb\u6709\u6548\u4fe1\u53f7\u4e0e\u586b\u5145\u4f2a\u5f71\u3002", "result": "\u572812\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u96f6\u6837\u672c\u8de8\u573a\u666f\u6cdb\u5316\uff0c\u76f8\u6bd4SOTA WiFo\u5728CSI\u91cd\u5efa\u3001\u65f6\u57df\u548c\u9891\u57df\u9884\u6d4b\u4e0a\u5206\u522b\u964d\u4f4eNMSE 7.19 dB\u30014.08 dB\u548c5.27 dB\uff1b\u8bad\u7ec3\u5ef6\u8fdf\u964d\u4f4e53%\uff0c\u5e73\u5747\u6cdb\u5316\u6027\u80fd\u63d0\u53471.53 dB\u3002", "conclusion": "HeterCSI\u901a\u8fc7\u7406\u89e3\u5f02\u6784CSI\u9884\u8bad\u7ec3\u7684\u68af\u5ea6\u52a8\u529b\u5b66\uff0c\u6210\u529f\u534f\u8c03\u8bad\u7ec3\u6548\u7387\u4e0e\u8de8\u573a\u666f\u6cdb\u5316\uff0c\u4e3a6G\u65e0\u7ebf\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18231", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18231", "abs": "https://arxiv.org/abs/2601.18231", "authors": ["Trong Khiem Tran", "Manh Cuong Dao", "Phi Le Nguyen", "Thao Nguyen Truong", "Trong Nghia Hoang"], "title": "Rethinking Cross-Modal Fine-Tuning: Optimizing the Interaction between Feature Alignment and Target Fitting", "comment": "Accepted AISTATS 20226. Preprint version", "summary": "Adapting pre-trained models to unseen feature modalities has become increasingly important due to the growing need for cross-disciplinary knowledge integration.~A key challenge here is how to align the representation of new modalities with the most relevant parts of the pre-trained model's representation space to enable accurate knowledge transfer.~This requires combining feature alignment with target fine-tuning, but uncalibrated combinations can exacerbate misalignment between the source and target feature-label structures and reduce target generalization.~Existing work however lacks a theoretical understanding of this critical interaction between feature alignment and target fitting.~To bridge this gap, we develop a principled framework that establishes a provable generalization bound on the target error, which explains the interaction between feature alignment and target fitting through a novel concept of feature-label distortion.~This bound offers actionable insights into how this interaction should be optimized for practical algorithm design. The resulting approach achieves significantly improved performance over state-of-the-art methods across a wide range of benchmark datasets.", "AI": {"tldr": "\u9488\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u8de8\u6a21\u6001\u9002\u914d\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u7279\u5f81\u6807\u7b7e\u5931\u771f\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5efa\u7acb\u53ef\u8bc1\u660e\u6cdb\u5316\u754c\uff0c\u663e\u8457\u4f18\u4e8eSOTA\u65b9\u6cd5\u3002", "motivation": "\u8de8\u5b66\u79d1\u77e5\u8bc6\u6574\u5408\u9700\u6c42\u589e\u957f\uff0c\u9700\u5c06\u9884\u8bad\u7ec3\u6a21\u578b\u9002\u914d\u5230\u672a\u89c1\u7279\u5f81\u6a21\u6001\u3002\u6838\u5fc3\u6311\u6218\u662f\u4f7f\u65b0\u6a21\u6001\u8868\u793a\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u6700\u76f8\u5173\u90e8\u5206\u5bf9\u9f50\u3002\u672a\u6821\u51c6\u7684\u7279\u5f81\u5bf9\u9f50\u4e0e\u76ee\u6807\u5fae\u8c03\u7ec4\u5408\u4f1a\u52a0\u5267\u7279\u5f81-\u6807\u7b7e\u7ed3\u6784\u9519\u914d\u5e76\u964d\u4f4e\u6cdb\u5316\u80fd\u529b\u3002\u73b0\u6709\u5de5\u4f5c\u7f3a\u4e4f\u5bf9\u8be5\u4ea4\u4e92\u4f5c\u7528\u7684\u7406\u8bba\u7406\u89e3\u3002", "method": "\u5efa\u7acb\u53ef\u8bc1\u660e\u7684\u76ee\u6807\u8bef\u5dee\u6cdb\u5316\u754c\u7684\u539f\u5219\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u7279\u5f81\u6807\u7b7e\u5931\u771f\u65b0\u6982\u5ff5\u89e3\u91ca\u7279\u5f81\u5bf9\u9f50\u4e0e\u76ee\u6807\u62df\u5408\u7684\u4ea4\u4e92\u4f5c\u7528\u3002", "result": "\u5728\u5e7f\u6cdb\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6cdb\u5316\u754c\u4e3a\u4f18\u5316\u7279\u5f81\u5bf9\u9f50\u4e0e\u76ee\u6807\u62df\u5408\u7684\u4ea4\u4e92\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u6307\u5bfc\u5b9e\u9645\u7b97\u6cd5\u8bbe\u8ba1\u3002"}}
{"id": "2601.18245", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18245", "abs": "https://arxiv.org/abs/2601.18245", "authors": ["Santanu Das", "Jatin Batra"], "title": "Tractable Gaussian Phase Retrieval with Heavy Tails and Adversarial Corruption with Near-Linear Sample Complexity", "comment": null, "summary": "Phase retrieval is the classical problem of recovering a signal $x^* \\in \\mathbb{R}^n$ from its noisy phaseless measurements $y_i = \\langle a_i, x^* \\rangle^2 + \u03b6_i$ (where $\u03b6_i$ denotes noise, and $a_i$ is the sensing vector) for $i \\in [m]$. The problem of phase retrieval has a rich history, with a variety of applications such as optics, crystallography, heteroscedastic regression, astrophysics, etc. A major consideration in algorithms for phase retrieval is robustness against measurement errors. In recent breakthroughs in algorithmic robust statistics, efficient algorithms have been developed for several parameter estimation tasks such as mean estimation, covariance estimation, robust principal component analysis (PCA), etc. in the presence of heavy-tailed noise and adversarial corruptions. In this paper, we study efficient algorithms for robust phase retrieval with heavy-tailed noise when a constant fraction of both the measurements $y_i$ and the sensing vectors $a_i$ may be arbitrarily adversarially corrupted. For this problem, Buna and Rebeschini (AISTATS 2025) very recently gave an exponential time algorithm with sample complexity $O(n \\log n)$. Their algorithm needs a robust spectral initialization, specifically, a robust estimate of the top eigenvector of a covariance matrix, which they deemed to be beyond known efficient algorithmic techniques (similar spectral initializations are a key ingredient of a large family of phase retrieval algorithms). In this work, we make a connection between robust spectral initialization and recent algorithmic advances in robust PCA, yielding the first polynomial-time algorithms for robust phase retrieval with both heavy-tailed noise and adversarial corruptions, in fact with near-linear (in $n$) sample complexity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u89e3\u51b3\u5177\u6709\u91cd\u5c3e\u566a\u58f0\u548c\u5bf9\u6297\u6027\u635f\u574f\u7684\u9c81\u68d2\u76f8\u4f4d\u6062\u590d\u95ee\u9898\uff0c\u901a\u8fc7\u5c06\u9c81\u68d2\u8c31\u521d\u59cb\u5316\u4e0e\u9c81\u68d2PCA\u7b97\u6cd5\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u7ebf\u6027\u7684\u6837\u672c\u590d\u6742\u5ea6\u3002", "motivation": "\u76f8\u4f4d\u6062\u590d\u662f\u8bb8\u591a\u79d1\u5b66\u9886\u57df\u7684\u57fa\u7840\u95ee\u9898\uff0c\u73b0\u6709\u7b97\u6cd5\u5728\u9762\u5bf9\u91cd\u5c3e\u566a\u58f0\u548c\u5bf9\u6297\u6027\u635f\u574f\u65f6\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\uff08\u9700\u8981\u6307\u6570\u65f6\u95f4\uff09\uff0c\u7279\u522b\u662f\u9c81\u68d2\u8c31\u521d\u59cb\u5316\u88ab\u8ba4\u4e3a\u662f\u8ba1\u7b97\u4e0a\u4e0d\u53ef\u884c\u7684\u5173\u952e\u74f6\u9888\u3002", "method": "\u901a\u8fc7\u5efa\u7acb\u9c81\u68d2\u8c31\u521d\u59cb\u5316\u4e0e\u8fd1\u671f\u9c81\u68d2PCA\u7b97\u6cd5\u8fdb\u5c55\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u514b\u670d\u4e86\u8c31\u521d\u59cb\u5316\u7684\u8ba1\u7b97\u74f6\u9888\u3002", "result": "\u83b7\u5f97\u4e86\u9996\u4e2a\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\uff0c\u6837\u672c\u590d\u6742\u5ea6\u63a5\u8fd1\u7ebf\u6027\uff08O(n log n)\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u4e4b\u524dBuna\u548cRebeschini\u7684\u6307\u6570\u65f6\u95f4\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u9c81\u68d2\u76f8\u4f4d\u6062\u590d\u63d0\u4f9b\u4e86\u8ba1\u7b97\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c06\u9c81\u68d2\u7edf\u8ba1\u9886\u57df\u7684\u6700\u65b0\u7b97\u6cd5\u8fdb\u5c55\u6210\u529f\u5e94\u7528\u4e8e\u7ecf\u5178\u4fe1\u53f7\u5904\u7406\u95ee\u9898\uff0c\u89e3\u51b3\u4e86\u8be5\u9886\u57df\u7684\u4e00\u4e2a\u91cd\u8981\u5f00\u653e\u6027\u95ee\u9898\u3002"}}
{"id": "2601.18255", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18255", "abs": "https://arxiv.org/abs/2601.18255", "authors": ["Fei Meng"], "title": "Beyond Retention: Orchestrating Structural Safety and Plasticity in Continual Learning for LLMs", "comment": null, "summary": "Continual learning in Large Language Models (LLMs) faces the critical challenge of balancing stability (retaining old knowledge) and plasticity (learning new tasks). While Experience Replay (ER) is a standard countermeasure against catastrophic forgetting, its impact across diverse capabilities remains underexplored. In this work, we uncover a critical dichotomy in ER's behavior: while it induces positive backward transfer on robust, unstructured tasks (e.g., boosting performance on previous NLP classification tasks through repeated rehearsal), it causes severe negative transfer on fragile, structured domains like code generation (e.g., a significant relative drop in coding accuracy). This reveals that ER trades structural integrity for broad consolidation. To address this dilemma, we propose \\textbf{Orthogonal Subspace Wake-up (OSW)}. OSW identifies essential parameter subspaces of previous tasks via a brief \"wake-up\" phase and enforces orthogonal updates for new tasks, providing a mathematically grounded \"safety guarantee\" for established knowledge structures. Empirical results across a diverse four-task sequence demonstrate that OSW uniquely succeeds in preserving fragile coding abilities where Replay fails, while simultaneously maintaining high plasticity for novel tasks. Our findings emphasize the necessity of evaluating structural safety alongside average retention in LLM continual learning.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u7ecf\u9a8c\u56de\u653e(ER)\u5728LLM\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u4e8c\u5206\u6cd5\uff1a\u5bf9\u975e\u7ed3\u6784\u5316\u4efb\u52a1\u6709\u6b63\u5411\u8fc1\u79fb\uff0c\u4f46\u5bf9\u4ee3\u7801\u751f\u6210\u7b49\u7ed3\u6784\u5316\u57df\u9020\u6210\u4e25\u91cd\u8d1f\u8fc1\u79fb\u3002\u63d0\u51fa\u7684\u6b63\u4ea4\u5b50\u7a7a\u95f4\u5524\u9192(OSW)\u65b9\u6cd5\u901a\u8fc7\u8bc6\u522b\u65e7\u4efb\u52a1\u5173\u952e\u53c2\u6570\u5b50\u7a7a\u95f4\u5e76\u5f3a\u5236\u65b0\u4efb\u52a1\u66f4\u65b0\u6b63\u4ea4\u5316\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u4e0e\u53ef\u5851\u6027\u7684\u5e73\u8861\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6301\u7eed\u5b66\u4e60\u9762\u4e34\u7a33\u5b9a\u6027(\u4fdd\u7559\u65e7\u77e5\u8bc6)\u4e0e\u53ef\u5851\u6027(\u5b66\u4e60\u65b0\u4efb\u52a1)\u7684\u6838\u5fc3\u77db\u76fe\u3002\u6807\u51c6\u65b9\u6cd5\u7ecf\u9a8c\u56de\u653e(ER)\u867d\u80fd\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\uff0c\u4f46\u5176\u5bf9\u4e0d\u540c\u80fd\u529b\u57df\u7684\u5dee\u5f02\u5316\u5f71\u54cd\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u6b63\u4ea4\u5b50\u7a7a\u95f4\u5524\u9192(OSW)\uff1a\u901a\u8fc7\u77ed\u6682\"\u5524\u9192\"\u9636\u6bb5\u8bc6\u522b\u5148\u524d\u4efb\u52a1\u7684\u5173\u952e\u53c2\u6570\u5b50\u7a7a\u95f4\uff0c\u5bf9\u65b0\u4efb\u52a1\u5f3a\u5236\u6267\u884c\u6b63\u4ea4\u66f4\u65b0\u7ea6\u675f\uff0c\u4e3a\u5df2\u5efa\u7acb\u7684\u77e5\u8bc6\u7ed3\u6784\u63d0\u4f9b\u6570\u5b66\u4fdd\u969c\u3002", "result": "\u5728\u56db\u9879\u4efb\u52a1\u5e8f\u5217\u5b9e\u9a8c\u4e2d\uff0cOSW\u6210\u529f\u4fdd\u7559\u4e86ER\u5931\u8d25\u7684\u8106\u5f31\u7f16\u7801\u80fd\u529b(\u4ee3\u7801\u751f\u6210\u51c6\u786e\u7387\u663e\u8457\u4f18\u4e8eER)\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5bf9\u65b0\u4efb\u52a1\u7684\u9ad8\u53ef\u5851\u6027\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u4e0e\u53ef\u5851\u6027\u7684\u7edf\u4e00\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03LLM\u6301\u7eed\u5b66\u4e60\u8bc4\u4f30\u5fc5\u987b\u8d85\u8d8a\u5e73\u5747\u4fdd\u7559\u7387\u6307\u6807\uff0c\u9700\u540c\u65f6\u8003\u91cf\u7ed3\u6784\u5b89\u5168\u6027\uff0c\u4e3a\u8106\u5f31\u7ed3\u6784\u5316\u77e5\u8bc6\u7684\u4fdd\u62a4\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2601.18261", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18261", "abs": "https://arxiv.org/abs/2601.18261", "authors": ["Chao-Hong Tan", "Qian Chen", "Wen Wang", "Yukun Ma", "Chong Zhang", "Chong Deng", "Qinglin Zhang", "Xiangang Li", "Jieping Ye"], "title": "FGGM: Fisher-Guided Gradient Masking for Continual Learning", "comment": "Accepted by ICASSP 2026", "summary": "Catastrophic forgetting impairs the continuous learning of large language models. We propose Fisher-Guided Gradient Masking (FGGM), a framework that mitigates this by strategically selecting parameters for updates using diagonal Fisher Information. FGGM dynamically generates binary masks with adaptive thresholds, preserving critical parameters to balance stability and plasticity without requiring historical data. Unlike magnitude-based methods such as MIGU, our approach offers a mathematically principled parameter importance estimation. On the TRACE benchmark, FGGM shows a 9.6% relative improvement in retaining general capabilities over supervised fine-tuning (SFT) and a 4.4% improvement over MIGU on TRACE tasks. Additional analysis on code generation tasks confirms FGGM's superior performance and reduced forgetting, establishing it as an effective solution.", "AI": {"tldr": "FGGM\u662f\u4e00\u79cd\u5229\u7528Fisher\u4fe1\u606f\u77e9\u9635\u52a8\u6001\u63a9\u7801\u53c2\u6570\u7684\u6846\u67b6\uff0c\u65e0\u9700\u5386\u53f2\u6570\u636e\u5373\u53ef\u7f13\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5728TRACE\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u6bd4\u76d1\u7763\u5fae\u8c03\u63d0\u53479.6%\uff0c\u6bd4MIGU\u63d0\u53474.4%\u3002", "motivation": "\u707e\u96be\u6027\u9057\u5fd8\u4f1a\u635f\u5bb3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6301\u7eed\u5b66\u4e60\u80fd\u529b\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5386\u53f2\u6570\u636e\u6216\u7f3a\u4e4f\u6570\u5b66\u539f\u7406\u652f\u6491\u3002", "method": "\u63d0\u51faFisher-Guided Gradient Masking (FGGM)\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u89d2Fisher\u4fe1\u606f\u77e9\u9635\u8bc4\u4f30\u53c2\u6570\u91cd\u8981\u6027\uff0c\u52a8\u6001\u751f\u6210\u81ea\u9002\u5e94\u9608\u503c\u7684\u4e8c\u8fdb\u5236\u63a9\u7801\u6765\u9009\u62e9\u6027\u5730\u66f4\u65b0\u53c2\u6570\uff0c\u5728\u65e0\u9700\u5386\u53f2\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5e73\u8861\u6a21\u578b\u7a33\u5b9a\u6027\u4e0e\u53ef\u5851\u6027\u3002", "result": "\u5728TRACE\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cFGGM\u76f8\u6bd4\u76d1\u7763\u5fae\u8c03(SFT)\u5728\u4fdd\u7559\u901a\u7528\u80fd\u529b\u65b9\u9762\u76f8\u5bf9\u63d0\u53479.6%\uff0c\u76f8\u6bd4MIGU\u5728TRACE\u4efb\u52a1\u4e0a\u63d0\u53474.4%\uff1b\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u5206\u6790\u4e5f\u8bc1\u5b9e\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u548c\u66f4\u5c11\u7684\u9057\u5fd8\u3002", "conclusion": "FGGM\u4e3a\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6570\u5b66\u539f\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6301\u7eed\u5b66\u4e60\u6027\u80fd\u3002"}}
{"id": "2601.18278", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18278", "abs": "https://arxiv.org/abs/2601.18278", "authors": ["Indr\u0117 \u017dliobait\u0117"], "title": "What Do Learned Models Measure?", "comment": null, "summary": "In many scientific and data-driven applications, machine learning models are increasingly used as measurement instruments, rather than merely as predictors of predefined labels. When the measurement function is learned from data, the mapping from observations to quantities is determined implicitly by the training distribution and inductive biases, allowing multiple inequivalent mappings to satisfy standard predictive evaluation criteria. We formalize learned measurement functions as a distinct focus of evaluation and introduce measurement stability, a property capturing invariance of the measured quantity across admissible realizations of the learning process and across contexts. We show that standard evaluation criteria in machine learning, including generalization error, calibration, and robustness, do not guarantee measurement stability. Through a real-world case study, we show that models with comparable predictive performance can implement systematically inequivalent measurement functions, with distribution shift providing a concrete illustration of this failure. Taken together, our results highlight a limitation of existing evaluation frameworks in settings where learned model outputs are identified as measurements, motivating the need for an additional evaluative dimension.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6307\u51fa\uff0c\u5f53\u673a\u5668\u5b66\u4e60\u6a21\u578b\u88ab\u7528\u4f5c\u6d4b\u91cf\u5de5\u5177\u800c\u975e\u4ec5\u4ec5\u662f\u9884\u6d4b\u5668\u65f6\uff0c\u6807\u51c6\u8bc4\u4f30\u6307\u6807\u65e0\u6cd5\u4fdd\u8bc1\u6d4b\u91cf\u7a33\u5b9a\u6027\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u7ef4\u5ea6", "motivation": "\u5728\u8bb8\u591a\u79d1\u5b66\u548c\u6570\u636e\u9a71\u52a8\u7684\u5e94\u7528\u4e2d\uff0c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6b63\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u7528\u4f5c\u6d4b\u91cf\u4eea\u5668\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u9884\u5b9a\u4e49\u6807\u7b7e\u7684\u9884\u6d4b\u5668\u3002\u5f53\u6d4b\u91cf\u51fd\u6570\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u65f6\uff0c\u6620\u5c04\u5173\u7cfb\u7531\u8bad\u7ec3\u5206\u5e03\u548c\u5f52\u7eb3\u504f\u5dee\u9690\u5f0f\u51b3\u5b9a\uff0c\u5bfc\u81f4\u591a\u4e2a\u4e0d\u7b49\u4ef7\u7684\u6620\u5c04\u90fd\u80fd\u6ee1\u8db3\u6807\u51c6\u9884\u6d4b\u8bc4\u4f30\u6807\u51c6\uff0c\u8fd9\u5e26\u6765\u4e86\u6f5c\u5728\u7684\u53ef\u9760\u6027\u95ee\u9898", "method": "\u5c06\u5b66\u4e60\u7684\u6d4b\u91cf\u51fd\u6570\u5f62\u5f0f\u5316\u4e3a\u72ec\u7acb\u7684\u8bc4\u4f30\u91cd\u70b9\uff0c\u5f15\u5165\u6d4b\u91cf\u7a33\u5b9a\u6027\u6982\u5ff5\uff08\u8861\u91cf\u5b66\u4e60\u8fc7\u7a0b\u548c\u4e0d\u540c\u73af\u5883\u4e0b\u6d4b\u91cf\u7ed3\u679c\u7684\u4e0d\u53d8\u6027\uff09\uff0c\u5e76\u901a\u8fc7\u771f\u5b9e\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u6807\u51c6\u8bc4\u4f30\u51c6\u5219\u7684\u5c40\u9650\u6027", "result": "\u6807\u51c6\u673a\u5668\u5b66\u4e60\u8bc4\u4f30\u6807\u51c6\uff08\u5305\u62ec\u6cdb\u5316\u8bef\u5dee\u3001\u6821\u51c6\u548c\u9c81\u68d2\u6027\uff09\u65e0\u6cd5\u4fdd\u8bc1\u6d4b\u91cf\u7a33\u5b9a\u6027\uff1b\u9884\u6d4b\u6027\u80fd\u76f8\u5f53\u7684\u6a21\u578b\u53ef\u4ee5\u5b9e\u73b0\u7cfb\u7edf\u4e0a\u4e0d\u7b49\u4ef7\u7684\u6d4b\u91cf\u51fd\u6570\uff1b\u5206\u5e03\u504f\u79fb\u662f\u8fd9\u79cd\u5931\u8d25\u7684\u5177\u4f53\u4f8b\u8bc1", "conclusion": "\u73b0\u6709\u8bc4\u4f30\u6846\u67b6\u5728\u5c06\u5b66\u4e60\u6a21\u578b\u8f93\u51fa\u4f5c\u4e3a\u6d4b\u91cf\u7684\u573a\u666f\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u589e\u52a0\u989d\u5916\u7684\u8bc4\u4f30\u7ef4\u5ea6\u6765\u786e\u4fdd\u6d4b\u91cf\u7a33\u5b9a\u6027"}}
{"id": "2601.18292", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18292", "abs": "https://arxiv.org/abs/2601.18292", "authors": ["Zhewen Tan", "Wenhan Yu", "Jianfeng Si", "Tongxin Liu", "Kaiqi Guan", "Huiyan Jin", "Jiawen Tao", "Xiaokun Yuan", "Duohe Ma", "Xiangzheng Zhang", "Tong Yang", "Lin Sun"], "title": "TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment", "comment": null, "summary": "In recent years, safety risks associated with large language models have become increasingly prominent, highlighting the urgent need to mitigate the generation of toxic and harmful content. The mainstream paradigm for LLM safety alignment typically adopts a collaborative framework involving three roles: an attacker for adversarial prompt generation, a defender for safety defense, and an evaluator for response assessment. In this paper, we propose a closed-loop reinforcement learning framework called TriPlay-RL that enables iterative and co-improving collaboration among three roles with near-zero manual annotation. Experimental results show that the attacker preserves high output diversity while achieving a 20%-50% improvement in adversarial effectiveness; the defender attains 10%-30% gains in safety performance without degrading general reasoning capability; and the evaluator continuously refines its fine-grained judgment ability through iterations, accurately distinguishing unsafe responses, simple refusals, and useful guidance. Overall, our framework establishes an efficient and scalable paradigm for LLM safety alignment, enabling continuous co-evolution within a unified learning loop.", "AI": {"tldr": "\u63d0\u51faTriPlay-RL\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2aAI\u89d2\u8272\uff08\u653b\u51fb\u8005\u3001\u9632\u5fa1\u8005\u3001\u8bc4\u4f30\u8005\uff09\u5728\u95ed\u73af\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u4e2d\u534f\u540c\u6f14\u5316\uff0c\u4ee5\u8fd1\u4e4e\u96f6\u4eba\u5de5\u6807\u6ce8\u7684\u65b9\u5f0f\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u6027\u80fd", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u98ce\u9669\u65e5\u76ca\u7a81\u51fa\uff0c\u4e9f\u9700\u7f13\u89e3\u6709\u6bd2\u6709\u5bb3\u5185\u5bb9\u751f\u6210\u3002\u73b0\u6709\u5b89\u5168\u5bf9\u9f50\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\uff0c\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u6784\u5efa\u653b\u51fb\u3001\u9632\u5fa1\u3001\u8bc4\u4f30\u4e09\u65b9\u81ea\u52a8\u5316\u534f\u4f5c\u6846\u67b6", "method": "\u63d0\u51faTriPlay-RL\u95ed\u73af\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5b9e\u73b0\u653b\u51fb\u8005\uff08\u751f\u6210\u5bf9\u6297\u6027\u63d0\u793a\uff09\u3001\u9632\u5fa1\u8005\uff08\u6784\u5efa\u5b89\u5168\u9632\u5fa1\uff09\u3001\u8bc4\u4f30\u8005\uff08\u8bc4\u4f30\u54cd\u5e94\u8d28\u91cf\uff09\u4e09\u4e2a\u89d2\u8272\u7684\u8fed\u4ee3\u534f\u540c\u8fdb\u5316\uff0c\u4ec5\u9700\u6781\u5c11\u4eba\u5de5\u6807\u6ce8", "result": "\u653b\u51fb\u8005\u5bf9\u6297\u6709\u6548\u6027\u63d0\u534720%-50%\u4e14\u4fdd\u6301\u9ad8\u591a\u6837\u6027\uff1b\u9632\u5fa1\u8005\u5b89\u5168\u6027\u80fd\u63d0\u534710%-30%\u4e14\u4e0d\u635f\u5bb3\u901a\u7528\u63a8\u7406\u80fd\u529b\uff1b\u8bc4\u4f30\u8005\u80fd\u51c6\u786e\u533a\u5206\u4e0d\u5b89\u5168\u54cd\u5e94\u3001\u7b80\u5355\u62d2\u7edd\u548c\u6709\u7528\u5f15\u5bfc", "conclusion": "\u8be5\u6846\u67b6\u5efa\u7acb\u4e86\u9ad8\u6548\u53ef\u6269\u5c55\u7684LLM\u5b89\u5168\u5bf9\u9f50\u8303\u5f0f\uff0c\u5728\u7edf\u4e00\u5b66\u4e60\u5faa\u73af\u4e2d\u5b9e\u73b0\u4e09\u65b9\u6301\u7eed\u534f\u540c\u8fdb\u5316\uff0c\u5927\u5e45\u964d\u4f4e\u4eba\u5de5\u6807\u6ce8\u6210\u672c"}}
{"id": "2601.18314", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18314", "abs": "https://arxiv.org/abs/2601.18314", "authors": ["Lina Felsner", "Sevgi G. Kafali", "Hannah Eichhorn", "Agnes A. J. Leth", "Aidas Batvinskas", "Andre Datchev", "Fabian Klemm", "Jan Aulich", "Puntika Leepagorn", "Ruben Klinger", "Daniel Rueckert", "Julia A. Schnabel"], "title": "A Master Class on Reproducibility: A Student Hackathon on Advanced MRI Reconstruction Methods", "comment": null, "summary": "We report the design, protocol, and outcomes of a student reproducibility hackathon focused on replicating the results of three influential MRI reconstruction papers: (a) MoDL, an unrolled model-based network with learned denoising; (b) HUMUS-Net, a hybrid unrolled multiscale CNN+Transformer architecture; and (c) an untrained, physics-regularized dynamic MRI method that uses a quantitative MR model for early stopping. We describe the setup of the hackathon and present reproduction outcomes alongside additional experiments, and we detail fundamental practices for building reproducible codebases.", "AI": {"tldr": "A student hackathon replicated three MRI reconstruction papers (MoDL, HUMUS-Net, and an untrained physics-regularized method), documenting outcomes and best practices for reproducible code.", "motivation": "To address the reproducibility crisis in AI research by testing whether students can replicate influential MRI reconstruction methods and identifying fundamental practices for reproducible codebases.", "method": "Organized a student hackathon where participants attempted to reproduce results from three state-of-the-art MRI reconstruction papers: MoDL (unrolled model-based network with learned denoising), HUMUS-Net (hybrid unrolled CNN+Transformer architecture), and an untrained physics-regularized dynamic MRI method using quantitative MR model for early stopping.", "result": "Reported reproduction outcomes for all three methods alongside additional experiments, and identified fundamental practices essential for building reproducible codebases.", "conclusion": "Reproducibility hackathons are effective for validating research and establishing community standards; the identified best practices should be adopted to improve reproducibility in MRI reconstruction and broader AI research fields."}}
{"id": "2601.18326", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18326", "abs": "https://arxiv.org/abs/2601.18326", "authors": ["Jie Li", "Jing Li", "Lu Lv", "Zhanyu Ju", "Fengkui Gong"], "title": "Cognitive Fusion of ZC Sequences and Time-Frequency Images for Out-of-Distribution Detection of Drone Signals", "comment": null, "summary": "We propose a drone signal out-of-distribution detection (OODD) algorithm based on the cognitive fusion of Zadoff-Chu (ZC) sequences and time-frequency images (TFI). ZC sequences are identified by analyzing the communication protocols of DJI drones, while TFI capture the time-frequency characteristics of drone signals with unknown or non-standard communication protocols. Both modalities are used jointly to enable OODD in the drone remote identification (RID) task. Specifically, ZC sequence features and TFI features are generated from the received radio frequency signals, which are then processed through dedicated feature extraction module to enhance and align them. The resultant multi-modal features undergo multi-modal feature interaction, single-modal feature fusion, and multi-modal feature fusion to produce features that integrate and complement information across modalities. Discrimination scores are computed from the fused features along both spatial and channel dimensions to capture time-frequency characteristic differences dictated by the communication protocols, and these scores will be transformed into adaptive attention weights. The weighted features are then passed through a Softmax function to produce the signal classification results. Simulation results demonstrate that the proposed algorithm outperforms existing algorithms and achieves 1.7% and 7.5% improvements in RID and OODD metrics, respectively. The proposed algorithm also performs strong robustness under varying flight conditions and across different drone types.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eZadoff-Chu\u5e8f\u5217\u548c\u65f6\u9891\u8c31\u56fe\u8ba4\u77e5\u878d\u5408\u7684\u65e0\u4eba\u673a\u4fe1\u53f7\u5206\u5e03\u5916\u68c0\u6d4b\u7b97\u6cd5\u3002\u901a\u8fc7\u591a\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u3001\u4ea4\u4e92\u4e0e\u878d\u5408\uff0c\u7ed3\u5408\u7a7a\u95f4-\u901a\u9053\u53cc\u7ef4\u5ea6\u5224\u522b\u5206\u6570\u751f\u6210\u7684\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u6743\u91cd\uff0c\u5b9e\u73b0\u4fe1\u53f7\u5206\u7c7b\u3002\u8be5\u65b9\u6cd5\u5728RID\u548cOODD\u6307\u6807\u4e0a\u5206\u522b\u63d0\u53471.7%\u548c7.5%\uff0c\u5e76\u5728\u4e0d\u540c\u98de\u884c\u6761\u4ef6\u548c\u65e0\u4eba\u673a\u7c7b\u578b\u4e0b\u5c55\u73b0\u5f3a\u9c81\u68d2\u6027\u3002", "motivation": "\u65e0\u4eba\u673a\u6570\u91cf\u6fc0\u589e\u5e26\u6765\u5b89\u5168\u9690\u60a3\uff0c\u9700\u53ef\u9760\u7684\u8fdc\u7a0b\u8bc6\u522b\u6280\u672f\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u5df2\u77e5\u6807\u51c6\u534f\u8bae\uff08\u5982DJI\u65e0\u4eba\u673a\u7684Zadoff-Chu\u5e8f\u5217\uff09\u548c\u672a\u77e5/\u975e\u6807\u51c6\u534f\u8bae\u7684\u4fe1\u53f7\u68c0\u6d4b\uff0c\u5bfc\u81f4\u5206\u5e03\u5916\u68c0\u6d4b\u6027\u80fd\u53d7\u9650\u3002\u4e9f\u9700\u4e00\u79cd\u80fd\u878d\u5408\u591a\u6a21\u6001\u4fe1\u606f\u3001\u9002\u5e94\u591a\u6837\u5316\u901a\u4fe1\u534f\u8bae\u7684\u9c81\u68d2\u68c0\u6d4b\u7b97\u6cd5\u3002", "method": "1. \u591a\u6a21\u6001\u7279\u5f81\u63d0\u53d6\uff1a\u4ece\u5c04\u9891\u4fe1\u53f7\u4e2d\u63d0\u53d6Zadoff-Chu\u5e8f\u5217\u7279\u5f81\uff08\u9488\u5bf9DJI\u534f\u8bae\uff09\u548c\u65f6\u9891\u8c31\u56fe\u7279\u5f81\uff08\u9488\u5bf9\u672a\u77e5\u534f\u8bae\uff09\uff1b2. \u7279\u5f81\u589e\u5f3a\u4e0e\u5bf9\u9f50\uff1a\u901a\u8fc7\u4e13\u7528\u6a21\u5757\u5904\u7406\u4e24\u79cd\u6a21\u6001\u7279\u5f81\uff1b3. \u591a\u5c42\u6b21\u7279\u5f81\u878d\u5408\uff1a\u4f9d\u6b21\u8fdb\u884c\u591a\u6a21\u6001\u4ea4\u4e92\u3001\u5355\u6a21\u6001\u878d\u5408\u3001\u591a\u6a21\u6001\u878d\u5408\uff0c\u5b9e\u73b0\u4fe1\u606f\u4e92\u8865\uff1b4. \u81ea\u9002\u5e94\u6ce8\u610f\u529b\u673a\u5236\uff1a\u8ba1\u7b97\u7a7a\u95f4\u548c\u901a\u9053\u7ef4\u5ea6\u7684\u5224\u522b\u5206\u6570\uff0c\u8f6c\u6362\u4e3a\u6ce8\u610f\u529b\u6743\u91cd\uff1b5. \u5206\u7c7b\u51b3\u7b56\uff1a\u52a0\u6743\u7279\u5f81\u7ecfSoftmax\u8f93\u51fa\u5206\u7c7b\u7ed3\u679c\u3002", "result": "RID\u6027\u80fd\u63d0\u53471.7%\uff0cOODD\u6027\u80fd\u63d0\u53477.5%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\uff0c\u4e14\u5728\u591a\u79cd\u98de\u884c\u6761\u4ef6\u548c\u65e0\u4eba\u673a\u7c7b\u578b\u4e0b\u4fdd\u6301\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u901a\u8fc7\u8ba4\u77e5\u878d\u5408\u534f\u8bae\u7279\u5f81\u548c\u65f6\u9891\u7279\u5f81\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u4fe1\u53f7\u5206\u5e03\u5916\u68c0\u6d4b\u96be\u9898\u3002\u591a\u5c42\u6b21\u878d\u5408\u7b56\u7565\u548c\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u4e86\u7279\u5f81\u8868\u8fbe\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u591a\u6a21\u6001\u65b9\u6cd5\u5728\u590d\u6742\u65e0\u4eba\u673a\u901a\u4fe1\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u65e0\u4eba\u673a\u76d1\u7ba1\u63d0\u4f9b\u4e86\u53ef\u9760\u6280\u672f\u652f\u6491\u3002"}}
{"id": "2601.18329", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18329", "abs": "https://arxiv.org/abs/2601.18329", "authors": ["Chuhan Feng", "Jing Li", "Jie Li", "Lu Lv", "Fengkui Gong"], "title": "Discriminability-Driven Spatial-Channel Selection with Gradient Norm for Drone Signal OOD Detection", "comment": null, "summary": "We propose a drone signal out-of-distribution (OOD) detection algorithm based on discriminability-driven spatial-channel selection with a gradient norm. Time-frequency image features are adaptively weighted along both spatial and channel dimensions by quantifying inter-class similarity and variance based on protocol-specific time-frequency characteristics. Subsequently, a gradient-norm metric is introduced to measure perturbation sensitivity for capturing the inherent instability of OOD samples, which is then fused with energy-based scores for joint inference. Simulation results demonstrate that the proposed algorithm provides superior discriminative power and robust performance via SNR and various drone types.", "AI": {"tldr": "A drone signal OOD detection algorithm using discriminability-driven spatial-channel selection and gradient norm metrics, achieving robust performance across SNR conditions and drone types.", "motivation": "Detecting out-of-distribution drone signals is challenging due to protocol-specific characteristics and instability of OOD samples, requiring discriminative feature selection and robust detection mechanisms.", "method": "Proposes discriminability-driven spatial-channel selection that adaptively weights time-frequency image features based on inter-class similarity/variance, combined with gradient-norm metrics to measure perturbation sensitivity of OOD samples, fused with energy-based scores for joint inference.", "result": "Simulation results demonstrate superior discriminative power and robust performance across varying SNR conditions and different drone types.", "conclusion": "The proposed method effectively addresses drone signal OOD detection through adaptive feature selection and gradient-based instability measurement, showing robust and discriminative detection capabilities."}}
{"id": "2601.18356", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18356", "abs": "https://arxiv.org/abs/2601.18356", "authors": ["Weiqin Yang", "Haowen Xue", "Qingyi Peng", "Hexuan Hu", "Qian Huang", "Tingbo Zhang"], "title": "Making medical vision-language models think causally across modalities with retrieval-augmented cross-modal reasoning", "comment": null, "summary": "Medical vision-language models (VLMs) achieve strong performance in diagnostic reporting and image-text alignment, yet their underlying reasoning mechanisms remain fundamentally correlational, exhibiting reliance on superficial statistical associations that fail to capture the causal pathophysiological mechanisms central to clinical decision-making. This limitation makes them fragile, prone to hallucinations, and sensitive to dataset biases. Retrieval-augmented generation (RAG) offers a partial remedy by grounding predictions in external knowledge. However, conventional RAG depends on semantic similarity, introducing new spurious correlations. We propose Multimodal Causal Retrieval-Augmented Generation, a framework that integrates causal inference principles with multimodal retrieval. It retrieves clinically relevant exemplars and causal graphs from external sources, conditioning model reasoning on counterfactual and interventional evidence rather than correlations alone. Applied to radiology report generation, diagnosis prediction, and visual question answering, it improves factual accuracy, robustness to distribution shifts, and interpretability. Our results highlight causal retrieval as a scalable path toward medical VLMs that think beyond pattern matching, enabling trustworthy multimodal reasoning in high-stakes clinical settings.", "AI": {"tldr": "\u63d0\u51fa\u591a\u6a21\u6001\u56e0\u679c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u89e3\u51b3\u533b\u7597\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ec5\u4f9d\u8d56\u76f8\u5173\u6027\u800c\u975e\u56e0\u679c\u673a\u5236\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u533b\u7597\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8bca\u65ad\u4e2d\u8868\u73b0\u4f18\u5f02\u4f46\u4f9d\u8d56\u8868\u9762\u7edf\u8ba1\u5173\u8054\u800c\u975e\u56e0\u679c\u75c5\u7406\u673a\u5236\uff0c\u5bfc\u81f4\u8106\u5f31\u6027\u3001\u5e7b\u89c9\u548c\u6570\u636e\u504f\u89c1\u95ee\u9898\uff0c\u4f20\u7edfRAG\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u6027\u4ecd\u4f1a\u5f15\u5165\u865a\u5047\u76f8\u5173\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001\u56e0\u679c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u6574\u5408\u56e0\u679c\u63a8\u65ad\u539f\u7406\u4e0e\u591a\u6a21\u6001\u68c0\u7d22\uff0c\u4ece\u5916\u90e8\u77e5\u8bc6\u5e93\u68c0\u7d22\u4e34\u5e8a\u76f8\u5173\u793a\u4f8b\u548c\u56e0\u679c\u56fe\uff0c\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u548c\u5e72\u9884\u8bc1\u636e\u800c\u975e\u5355\u7eaf\u76f8\u5173\u6027\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u5728\u653e\u5c04\u62a5\u544a\u751f\u6210\u3001\u8bca\u65ad\u9884\u6d4b\u548c\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u5bf9\u5206\u5e03\u504f\u79fb\u7684\u9c81\u68d2\u6027\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u56e0\u679c\u68c0\u7d22\u4e3a\u6784\u5efa\u53ef\u4fe1\u8d56\u7684\u533b\u7597\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u8def\u5f84\uff0c\u4f7f\u5176\u8d85\u8d8a\u6a21\u5f0f\u5339\u914d\uff0c\u5b9e\u73b0\u9ad8\u98ce\u9669\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u53ef\u9760\u591a\u6a21\u6001\u63a8\u7406\u3002"}}
{"id": "2601.18420", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18420", "abs": "https://arxiv.org/abs/2601.18420", "authors": ["Satya Prakash Dash", "Hossein Abdi", "Wei Pan", "Samuel Kaski", "Mingfei Sun"], "title": "Gradient Regularized Natural Gradients", "comment": null, "summary": "Gradient regularization (GR) has been shown to improve the generalizability of trained models. While Natural Gradient Descent has been shown to accelerate optimization in the initial phase of training, little attention has been paid to how the training dynamics of second-order optimizers can benefit from GR. In this work, we propose Gradient-Regularized Natural Gradients (GRNG), a family of scalable second-order optimizers that integrate explicit gradient regularization with natural gradient updates. Our framework provides two complementary algorithms: a frequentist variant that avoids explicit inversion of the Fisher Information Matrix (FIM) via structured approximations, and a Bayesian variant based on a Regularized-Kalman formulation that eliminates the need for FIM inversion entirely. We establish convergence guarantees for GRNG, showing that gradient regularization improves stability and enables convergence to global minima. Empirically, we demonstrate that GRNG consistently enhances both optimization speed and generalization compared to first-order methods (SGD, AdamW) and second-order baselines (K-FAC, Sophia), with strong results on vision and language benchmarks. Our findings highlight gradient regularization as a principled and practical tool to unlock the robustness of natural gradient methods for large-scale deep learning.", "AI": {"tldr": "\u63d0\u51fa\u68af\u5ea6\u6b63\u5219\u5316\u81ea\u7136\u68af\u5ea6\uff08GRNG\uff09\uff0c\u4e00\u79cd\u7ed3\u5408\u68af\u5ea6\u6b63\u5219\u5316\u4e0e\u81ea\u7136\u68af\u5ea6\u66f4\u65b0\u7684\u53ef\u6269\u5c55\u4e8c\u9636\u4f18\u5316\u5668\u5bb6\u65cf\uff0c\u901a\u8fc7\u4e24\u79cd\u53d8\u4f53\uff08\u9891\u7387\u6d3e\u7684\u7ed3\u6784\u5316\u8fd1\u4f3c\u548c\u8d1d\u53f6\u65af\u7684\u6b63\u5219\u5316\u5361\u5c14\u66fc\u6ee4\u6ce2\uff09\u907f\u514dFisher\u4fe1\u606f\u77e9\u9635\u6c42\u9006\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u57fa\u51c6\u4e0a\u4f18\u4e8eSGD\u3001AdamW\u3001K-FAC\u7b49\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u68af\u5ea6\u6b63\u5219\u5316\u80fd\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u81ea\u7136\u68af\u5ea6\u5728\u8bad\u7ec3\u521d\u671f\u80fd\u52a0\u901f\u4f18\u5316\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5f88\u5c11\u5173\u6ce8\u5982\u4f55\u5c06\u68af\u5ea6\u6b63\u5219\u5316\u4e0e\u4e8c\u9636\u4f18\u5316\u5668\u7ed3\u5408\u4ee5\u6539\u5584\u8bad\u7ec3\u52a8\u6001", "method": "\u63d0\u51faGRNG\u6846\u67b6\uff0c\u5305\u542b\u4e24\u79cd\u4e92\u8865\u7b97\u6cd5\uff1a1\uff09\u9891\u7387\u6d3e\u53d8\u4f53\u901a\u8fc7\u7ed3\u6784\u5316\u8fd1\u4f3c\u907f\u514d\u663e\u5f0fFisher\u4fe1\u606f\u77e9\u9635\u6c42\u9006\uff1b2\uff09\u8d1d\u53f6\u65af\u53d8\u4f53\u57fa\u4e8e\u6b63\u5219\u5316\u5361\u5c14\u66fc\u6ee4\u6ce2\u5b8c\u5168\u6d88\u9664\u6c42\u9006\u9700\u6c42\uff0c\u5e76\u5efa\u7acb\u6536\u655b\u6027\u4fdd\u8bc1", "result": "\u7406\u8bba\u8bc1\u660e\u68af\u5ea6\u6b63\u5219\u5316\u63d0\u5347\u7a33\u5b9a\u6027\u5e76\u786e\u4fdd\u6536\u655b\u5230\u5168\u5c40\u6781\u5c0f\u503c\uff1b\u5b9e\u8bc1\u663e\u793aGRNG\u5728\u4f18\u5316\u901f\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u6301\u7eed\u4f18\u4e8e\u4e00\u9636\u65b9\u6cd5\uff08SGD\u3001AdamW\uff09\u548c\u4e8c\u9636\u57fa\u7ebf\uff08K-FAC\u3001Sophia\uff09\uff0c\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02", "conclusion": "\u68af\u5ea6\u6b63\u5219\u5316\u662f\u89e3\u9501\u81ea\u7136\u68af\u5ea6\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u6df1\u5ea6\u5b66\u4e60\u4e2d\u9c81\u68d2\u6027\u7684\u539f\u5219\u6027\u5b9e\u7528\u5de5\u5177"}}
{"id": "2601.18447", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18447", "abs": "https://arxiv.org/abs/2601.18447", "authors": ["Jinlong Hu", "Jiacheng Liu"], "title": "GCFX: Generative Counterfactual Explanations for Deep Graph Models at the Model Level", "comment": null, "summary": "Deep graph learning models have demonstrated remarkable capabilities in processing graph-structured data and have been widely applied across various fields. However, their complex internal architectures and lack of transparency make it difficult to explain their decisions, resulting in opaque models that users find hard to understand and trust. In this paper, we explore model-level explanation techniques for deep graph learning models, aiming to provide users with a comprehensive understanding of the models' overall decision-making processes and underlying mechanisms. Specifically, we address the problem of counterfactual explanations for deep graph learning models by introducing a generative model-level counterfactual explanation approach called GCFX, which is based on deep graph generation. This approach generates a set of high-quality counterfactual explanations that reflect the model's global predictive behavior by leveraging an enhanced deep graph generation framework and a global summarization algorithm. GCFX features an architecture that combines dual encoders, structure-aware taggers, and Message Passing Neural Network decoders, enabling it to accurately learn the true latent distribution of input data and generate high-quality, closely related counterfactual examples. Subsequently, a global counterfactual summarization algorithm selects the most representative and comprehensive explanations from numerous candidate counterfactuals, providing broad insights into the model's global predictive patterns. Experiments on a synthetic dataset and several real-world datasets demonstrate that GCFX outperforms existing methods in terms of counterfactual validity and coverage while maintaining low explanation costs, thereby offering crucial support for enhancing the practicality and trustworthiness of global counterfactual explanations.", "AI": {"tldr": "A generative model-level counterfactual explanation approach (GCFX) for deep graph learning models that uses dual encoders, structure-aware taggers, and MPNN decoders to generate high-quality counterfactuals, outperforming existing methods in validity, coverage, and cost.", "motivation": "Deep graph learning models lack transparency, making them hard to understand and trust. The paper addresses the need for model-level counterfactual explanations to provide comprehensive understanding of model decision-making processes.", "method": "GCFX is a generative model-level counterfactual explanation approach based on deep graph generation. It uses an enhanced graph generation framework with dual encoders, structure-aware taggers, and MPNN decoders to learn latent data distribution and generate related counterfactuals. A global summarization algorithm selects the most representative explanations.", "result": "Experiments on synthetic and real-world datasets demonstrate GCFX outperforms existing methods in counterfactual validity and coverage while maintaining low explanation costs.", "conclusion": "GCFX enhances the practicality and trustworthiness of global counterfactual explanations for deep graph learning models."}}
{"id": "2601.18401", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18401", "abs": "https://arxiv.org/abs/2601.18401", "authors": ["Yufeng Huang"], "title": "Superlinear Multi-Step Attention", "comment": "30 pages, 6 figures", "summary": "In this paper, we propose \\textbf{Superlinear attention}, a fully trainable multi-step attention architecture that achieves subquadratic complexity for long sequences while preserving \\textbf{random context access} (a.k.a.\\ structural non-exclusion): no eligible token position is structurally excluded from being selected for attention. Superlinear attention reformulates standard causal self-attention as a multi-step search problem with $N$ steps, yielding an overall complexity of $O(L^{1+\\frac{1}{N}})$. To illustrate the architecture, we present a baseline $N=2$ implementation, which is algorithmically analogous to standard jump search. In this $O(L^{3/2})$ instantiation, the first step performs $O(L^{3/2})$ span-search to select relevant spans of the sequence, and the second step applies $O(L^{3/2})$ span-attention (standard attention restricted to the selected spans). In an upscaled $O(L^{1.54})$ configuration for robustness, we achieve an average decoding throughput of 114 tokens/sec at 1M context length and 80 tokens/sec at 10M context in our implementation on a modified 30B hybrid MoE model on a single B200 GPU. With limited training, we also obtain strong performance on the NIAH (Needle In A Haystack) task up to 256K context length, demonstrating that the routed span selection is learnable end-to-end. This paper emphasizes architectural formulation, scaling analysis, and systems feasibility, and presents initial validation; comprehensive quality evaluations across diverse long-context tasks are left to future work.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSuperlinear attention\u7684\u5168\u8bad\u7ec3\u591a\u6b65\u6ce8\u610f\u529b\u67b6\u6784\uff0c\u901a\u8fc7\u5c06\u6807\u51c6\u56e0\u679c\u81ea\u6ce8\u610f\u529b\u91cd\u6784\u4e3a\u591a\u6b65\u641c\u7d22\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u968f\u673a\u4e0a\u4e0b\u6587\u8bbf\u95ee\u80fd\u529b\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u957f\u5e8f\u5217\u7684\u4e9a\u4e8c\u6b21\u65b9\u590d\u6742\u5ea6\u3002", "motivation": "\u6807\u51c6\u56e0\u679c\u81ea\u6ce8\u610f\u529b\u5bf9\u957f\u5e8f\u5217\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u4e3a\u4e8c\u6b21\u65b9O(L\u00b2)\uff0c\u8ba1\u7b97\u5f00\u9500\u5de8\u5927\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5728\u4fdd\u6301\u4efb\u610ftoken\u4f4d\u7f6e\u90fd\u53ef\u88ab\u8bbf\u95ee\uff08\u7ed3\u6784\u975e\u6392\u4ed6\u6027\uff09\u7684\u524d\u63d0\u4e0b\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u6ce8\u610f\u529b\u673a\u5236\u3002", "method": "\u63d0\u51faSuperlinear attention\uff0c\u5c06\u56e0\u679c\u81ea\u6ce8\u610f\u529b\u91cd\u6784\u4e3aN\u6b65\u641c\u7d22\u95ee\u9898\uff0c\u6574\u4f53\u590d\u6742\u5ea6\u4e3aO(L^(1+1/N))\u3002\u5177\u4f53\u5b9e\u73b0\u4e86\u4e00\u4e2aN=2\u7684\u57fa\u7ebf\u7248\u672c\uff08\u7c7b\u4f3c\u8df3\u8f6c\u641c\u7d22\uff09\uff0c\u7b2c\u4e00\u6b65\u8fdb\u884cO(L^(3/2))\u7684\u533a\u95f4\u641c\u7d22\u9009\u62e9\u76f8\u5173\u533a\u95f4\uff0c\u7b2c\u4e8c\u6b65\u5728\u9009\u5b9a\u533a\u95f4\u5185\u8fdb\u884cO(L^(3/2))\u7684\u6807\u51c6\u6ce8\u610f\u529b\u8ba1\u7b97\u3002", "result": "\u572830B\u6df7\u5408MoE\u6a21\u578b\u4e0a\uff0c\u5b9e\u73b0\u7248\u672c\u57281M\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u8fbe\u5230114 tokens/sec\u7684\u89e3\u7801\u541e\u5410\u91cf\uff0c\u572810M\u4e0a\u4e0b\u6587\u4e0b\u8fbe\u523080 tokens/sec\u3002\u5728NIAH\u4efb\u52a1\u4e0a\uff0c\u6709\u9650\u8bad\u7ec3\u540e\u80fd\u5728256K\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u8def\u7531\u533a\u95f4\u9009\u62e9\u53ef\u901a\u8fc7\u7aef\u5230\u7aef\u5b66\u4e60\u3002", "conclusion": "\u8be5\u67b6\u6784\u5728\u7406\u8bba\u4e0a\u548c\u7cfb\u7edf\u53ef\u884c\u6027\u4e0a\u5c55\u793a\u4e86\u4f18\u52bf\uff0c\u4e3a\u957f\u5e8f\u5217\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u4f46\u5168\u9762\u7684\u8de8\u4efb\u52a1\u8d28\u91cf\u8bc4\u4f30\u7559\u5f85\u672a\u6765\u5de5\u4f5c\u3002"}}
{"id": "2601.18510", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18510", "abs": "https://arxiv.org/abs/2601.18510", "authors": ["Yibo Li", "Zijie Lin", "Ailin Deng", "Xuan Zhang", "Yufei He", "Shuo Ji", "Tri Cao", "Bryan Hooi"], "title": "Just-In-Time Reinforcement Learning: Continual Learning in LLM Agents Without Gradient Updates", "comment": null, "summary": "While Large Language Model (LLM) agents excel at general tasks, they inherently struggle with continual adaptation due to the frozen weights after deployment. Conventional reinforcement learning (RL) offers a solution but incurs prohibitive computational costs and the risk of catastrophic forgetting. We introduce Just-In-Time Reinforcement Learning (JitRL), a training-free framework that enables test-time policy optimization without any gradient updates. JitRL maintains a dynamic, non-parametric memory of experiences and retrieves relevant trajectories to estimate action advantages on-the-fly. These estimates are then used to directly modulate the LLM's output logits. We theoretically prove that this additive update rule is the exact closed-form solution to the KL-constrained policy optimization objective. Extensive experiments on WebArena and Jericho demonstrate that JitRL establishes a new state-of-the-art among training-free methods. Crucially, JitRL outperforms the performance of computationally expensive fine-tuning methods (e.g., WebRL) while reducing monetary costs by over 30 times, offering a scalable path for continual learning agents. The code is available at https://github.com/liushiliushi/JitRL.", "AI": {"tldr": "JitRL\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6d4b\u8bd5\u65f6\u7b56\u7565\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8bb0\u5fc6\u5e93\u5b9e\u65f6\u68c0\u7d22\u7ecf\u9a8c\u6765\u4f30\u8ba1\u52a8\u4f5c\u4f18\u52bf\u5e76\u76f4\u63a5\u8c03\u5236LLM\u8f93\u51fa\uff0c\u5728\u4e0d\u66f4\u65b0\u68af\u5ea6\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6301\u7eed\u9002\u5e94\uff0c\u6027\u80fd\u8d85\u8d8a\u5fae\u8c03\u65b9\u6cd5\u4e14\u6210\u672c\u964d\u4f4e30\u500d\u4ee5\u4e0a\u3002", "motivation": "\u90e8\u7f72\u540e\u7684LLM\u667a\u80fd\u4f53\u56e0\u6a21\u578b\u6743\u91cd\u51bb\u7ed3\u800c\u96be\u4ee5\u6301\u7eed\u9002\u5e94\u65b0\u73af\u5883\uff0c\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u4e14\u6613\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "\u63d0\u51fa\u8bad\u7ec3\u65e0\u5173\u7684JitRL\u6846\u67b6\uff1a\u7ef4\u62a4\u52a8\u6001\u975e\u53c2\u6570\u5316\u7ecf\u9a8c\u8bb0\u5fc6\u5e93\uff0c\u5728\u7ebf\u68c0\u7d22\u76f8\u5173\u8f68\u8ff9\u8ba1\u7b97\u52a8\u4f5c\u4f18\u52bf\uff0c\u76f4\u63a5\u8c03\u6574LLM\u8f93\u51falogits\uff0c\u7406\u8bba\u8bc1\u660e\u8be5\u52a0\u6cd5\u66f4\u65b0\u662fKL\u7ea6\u675f\u7b56\u7565\u4f18\u5316\u95ee\u9898\u7684\u7cbe\u786e\u95ed\u5f0f\u89e3\u3002", "result": "\u5728WebArena\u548cJericho\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cJitRL\u5237\u65b0\u4e86\u8bad\u7ec3\u65e0\u5173\u65b9\u6cd5\u7684SOTA\u6027\u80fd\uff0c\u6027\u80fd\u8d85\u8d8a\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u5fae\u8c03\u65b9\u6cd5\uff08\u5982WebRL\uff09\uff0c\u540c\u65f6\u5c06\u8d27\u5e01\u6210\u672c\u964d\u4f4e\u8d85\u8fc730\u500d\u3002", "conclusion": "JitRL\u4e3a\u652f\u6301\u6301\u7eed\u5b66\u4e60\u7684\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u4e14\u7ecf\u6d4e\u7684\u8def\u5f84\uff0c\u9a8c\u8bc1\u4e86\u6d4b\u8bd5\u65f6\u4f18\u5316\u66ff\u4ee3\u4f20\u7edf\u8bad\u7ec3\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2601.18409", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18409", "abs": "https://arxiv.org/abs/2601.18409", "authors": ["Aniket Sanyal", "Baraah A. M. Sidahmed", "Rebekka Burkholz", "Tatjana Chavdarova"], "title": "Frequency-Based Hyperparameter Selection in Games", "comment": null, "summary": "Learning in smooth games fundamentally differs from standard minimization due to rotational dynamics, which invalidate classical hyperparameter tuning strategies. Despite their practical importance, effective methods for tuning in games remain underexplored. A notable example is LookAhead (LA), which achieves strong empirical performance but introduces additional parameters that critically influence performance. We propose a principled approach to hyperparameter selection in games by leveraging frequency estimation of oscillatory dynamics. Specifically, we analyze oscillations both in continuous-time trajectories and through the spectrum of the discrete dynamics in the associated frequency-based space. Building on this analysis, we introduce \\emph{Modal LookAhead (MoLA)}, an extension of LA that selects the hyperparameters adaptively to a given problem. We provide convergence guarantees and demonstrate in experiments that MoLA accelerates training in both purely rotational games and mixed regimes, all with minimal computational overhead.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u6a21\u6001LookAhead (MoLA)\uff0c\u901a\u8fc7\u4f30\u8ba1\u632f\u8361\u52a8\u529b\u5b66\u9891\u7387\u5b9e\u73b0\u5e73\u6ed1\u6e38\u620f\u7684\u539f\u5219\u6027\u8d85\u53c2\u6570\u8c03\u6574\u3002MoLA\u6269\u5c55LookAhead\u5b9e\u73b0\u81ea\u9002\u5e94\u53c2\u6570\u9009\u62e9\uff0c\u63d0\u4f9b\u6536\u655b\u4fdd\u8bc1\uff0c\u5e76\u4ee5\u6700\u5c0f\u8ba1\u7b97\u5f00\u9500\u52a0\u901f\u8bad\u7ec3\u3002", "motivation": "\u5e73\u6ed1\u6e38\u620f\u4e2d\u7684\u65cb\u8f6c\u52a8\u529b\u5b66\u4f7f\u7ecf\u5178\u8d85\u53c2\u6570\u8c03\u6574\u7b56\u7565\u5931\u6548\u3002\u5c3d\u7ba1LookAhead\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5f15\u5165\u7684\u989d\u5916\u53c2\u6570\u5bf9\u6027\u80fd\u5f71\u54cd\u5173\u952e\u3002\u6709\u6548\u7684\u6e38\u620f\u8c03\u53c2\u65b9\u6cd5\u4ecd\u7f3a\u4e4f\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5229\u7528\u9891\u7387\u4f30\u8ba1\u5206\u6790\u8fde\u7eed\u65f6\u95f4\u8f68\u8ff9\u4e0e\u79bb\u6563\u52a8\u529b\u5b66\u9891\u8c31\uff0c\u63d0\u51fa\u6a21\u6001LookAhead (MoLA)\uff0c\u57fa\u4e8e\u9891\u7387\u5206\u6790\u81ea\u9002\u5e94\u9009\u62e9\u8d85\u53c2\u6570\u3002", "result": "\u63d0\u4f9b\u7406\u8bba\u6536\u655b\u4fdd\u8bc1\u3002\u5b9e\u9a8c\u8868\u660eMoLA\u5728\u7eaf\u65cb\u8f6c\u6e38\u620f\u548c\u6df7\u5408\u673a\u5236\u4e2d\u5747\u80fd\u52a0\u901f\u8bad\u7ec3\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "MoLA\u5229\u7528\u9891\u7387\u5206\u6790\u6210\u529f\u6269\u5c55LookAhead\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u8d85\u53c2\u6570\u9009\u62e9\uff0c\u4e3a\u5e73\u6ed1\u6e38\u620f\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u8c03\u53c2\u65b9\u6848\u3002"}}
{"id": "2601.18521", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18521", "abs": "https://arxiv.org/abs/2601.18521", "authors": ["Emna Boudabbous", "Mohamed Karaa", "Lokman Sboui", "Julio Montecinos", "Omar Alam"], "title": "Scalable Transit Delay Prediction at City Scale: A Systematic Approach with Multi-Resolution Feature Engineering and Deep Learning", "comment": "This manuscript is a preprint of an earlier version. A revised system-oriented version is currently under review", "summary": "Urban bus transit agencies need reliable, network-wide delay predictions to provide accurate arrival information to passengers and support real-time operational control. Accurate predictions help passengers plan their trips, reduce waiting time, and allow operations staff to adjust headways, dispatch extra vehicles, and manage disruptions. Although real-time feeds such as GTFS-Realtime (GTFS-RT) are now widely available, most existing delay prediction systems handle only a few routes, depend on hand-crafted features, and offer little guidance on how to design a scalable, reusable architecture.\n  We present a city-scale prediction pipeline that combines multi-resolution feature engineering, dimensionality reduction, and deep learning. The framework generates 1,683 spatiotemporal features by exploring 23 aggregation combinations over H3 cells, routes, segments, and temporal patterns, and compresses them into 83 components using Adaptive PCA while preserving 95% of the variance. To avoid the \"giant cluster\" problem that occurs when dense urban areas fall into a single H3 region, we introduce a hybrid H3+topology clustering method that yields 12 balanced route clusters (coefficient of variation 0.608) and enables efficient distributed training.\n  We compare five model architectures on six months of bus operations from the Soci\u00e9t\u00e9 de transport de Montr\u00e9al (STM) network in Montr\u00e9al. A global LSTM with cluster-aware features achieves the best trade-off between accuracy and efficiency, outperforming transformer models by 18 to 52% while using 275 times fewer parameters. We also report multi-level evaluation at the elementary segment, segment, and trip level with walk-forward validation and latency analysis, showing that the proposed pipeline is suitable for real-time, city-scale deployment and can be reused for other networks with limited adaptation.", "AI": {"tldr": "A city-scale bus delay prediction pipeline combining multi-resolution feature engineering, Adaptive PCA dimensionality reduction, and hybrid H3+topology clustering achieves superior performance with a global LSTM model, outperforming transformers by 18-52% while using 275x fewer parameters.", "motivation": "Urban bus transit agencies need reliable network-wide delay predictions to provide accurate passenger arrival information and support real-time operational control, but existing systems handle only few routes, rely on hand-crafted features, and lack scalable, reusable architecture guidance.", "method": "Framework generates 1,683 spatiotemporal features from 23 aggregation combinations over H3 cells, routes, segments, and temporal patterns, compresses them into 83 components via Adaptive PCA (95% variance preserved), and uses hybrid H3+topology clustering to create 12 balanced route clusters for distributed training.", "result": "Global LSTM with cluster-aware features achieved best accuracy-efficiency trade-off on six months of STM Montr\u00e9al data, outperforming transformers by 18-52% with 275x fewer parameters, validated through multi-level evaluation and latency analysis for real-time deployment.", "conclusion": "The pipeline is suitable for real-time, city-scale deployment and can be reused for other transit networks with limited adaptation."}}
{"id": "2601.18626", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.18626", "abs": "https://arxiv.org/abs/2601.18626", "authors": ["Yingxiao Huo", "Satya Prakash Dash", "Radu Stoican", "Samuel Kaski", "Mingfei Sun"], "title": "Rank-1 Approximation of Inverse Fisher for Natural Policy Gradients in Deep Reinforcement Learning", "comment": null, "summary": "Natural gradients have long been studied in deep reinforcement learning due to their fast convergence properties and covariant weight updates. However, computing natural gradients requires inversion of the Fisher Information Matrix (FIM) at each iteration, which is computationally prohibitive in nature. In this paper, we present an efficient and scalable natural policy optimization technique that leverages a rank-1 approximation to full inverse-FIM. We theoretically show that under certain conditions, a rank-1 approximation to inverse-FIM converges faster than policy gradients and, under some conditions, enjoys the same sample complexity as stochastic policy gradient methods. We benchmark our method on a diverse set of environments and show that it achieves superior performance to standard actor-critic and trust-region baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u79e9-1\u8fd1\u4f3c\u7684\u9ad8\u6548\u81ea\u7136\u68af\u5ea6\u4f18\u5316\u65b9\u6cd5\uff0c\u89e3\u51b3Fisher\u4fe1\u606f\u77e9\u9635\u6c42\u9006\u8ba1\u7b97\u74f6\u9888\uff0c\u5728\u4fdd\u6301\u7406\u8bba\u4f18\u52bf\u7684\u540c\u65f6\u5b9e\u73b0\u6bd4\u6807\u51c6\u7b56\u7565\u68af\u5ea6\u66f4\u5feb\u7684\u6536\u655b\u548c\u8d85\u8d8a\u57fa\u7ebf\u7b97\u6cd5\u7684\u6027\u80fd", "motivation": "\u81ea\u7136\u68af\u5ea6\u5728\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u5177\u6709\u6536\u655b\u5feb\u548c\u534f\u53d8\u6743\u91cd\u66f4\u65b0\u7684\u4f18\u52bf\uff0c\u4f46\u6bcf\u6b21\u8fed\u4ee3\u9700\u8ba1\u7b97Fisher\u4fe1\u606f\u77e9\u9635(FIM)\u7684\u9006\uff0c\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u96be\u4ee5\u5b9e\u7528", "method": "\u91c7\u7528\u79e9-1\u8fd1\u4f3c\u66ff\u4ee3\u5b8c\u6574FIM\u9006\u77e9\u9635\uff0c\u7406\u8bba\u8bc1\u660e\u8be5\u8fd1\u4f3c\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u6bd4\u7b56\u7565\u68af\u5ea6\u6536\u655b\u66f4\u5feb\uff0c\u4e14\u6837\u672c\u590d\u6742\u5ea6\u4e0e\u968f\u673a\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u76f8\u5f53", "result": "\u5728\u591a\u79cd\u73af\u5883\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6807\u51c6\u6f14\u5458-\u8bc4\u8bba\u5bb6(actor-critic)\u548c\u4fe1\u4efb\u57df(trust-region)\u57fa\u7ebf\u7b97\u6cd5", "conclusion": "\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u81ea\u7136\u7b56\u7565\u4f18\u5316\u6280\u672f\uff0c\u5728\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\u4fdd\u6301\u7406\u8bba\u4f18\u52bf\uff0c\u4e3a\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.18479", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18479", "abs": "https://arxiv.org/abs/2601.18479", "authors": ["Kyoleen Kwak", "Hyoseok Hwang"], "title": "Enhancing Control Policy Smoothness by Aligning Actions with Predictions from Preceding States", "comment": "Accepted at AAAI-26. 7 pages (excluding references), 3 figures", "summary": "Deep reinforcement learning has proven to be a powerful approach to solving control tasks, but its characteristic high-frequency oscillations make it difficult to apply in real-world environments. While prior methods have addressed action oscillations via architectural or loss-based methods, the latter typically depend on heuristic or synthetic definitions of state similarity to promote action consistency, which often fail to accurately reflect the underlying system dynamics. In this paper, we propose a novel loss-based method by introducing a transition-induced similar state. The transition-induced similar state is defined as the distribution of next states transitioned from the previous state. Since it utilizes only environmental feedback and actually collected data, it better captures system dynamics. Building upon this foundation, we introduce Action Smoothing by Aligning Actions with Predictions from Preceding States (ASAP), an action smoothing method that effectively mitigates action oscillations. ASAP enforces action smoothness by aligning the actions with those taken in transition-induced similar states and by penalizing second-order differences to suppress high-frequency oscillations. Experiments in Gymnasium and Isaac-Lab environments demonstrate that ASAP yields smoother control and improved policy performance over existing methods.", "AI": {"tldr": "\u9488\u5bf9\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u52a8\u4f5c\u9ad8\u9891\u632f\u8361\u95ee\u9898\uff0c\u63d0\u51faASAP\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9a\u4e49\u57fa\u4e8e\u73af\u5883\u53cd\u9988\u7684\"\u8f6c\u79fb\u8bf1\u5bfc\u76f8\u4f3c\u72b6\u6001\"\uff0c\u7ea6\u675f\u52a8\u4f5c\u4e00\u81f4\u6027\u4e0e\u60e9\u7f5a\u4e8c\u9636\u5dee\u5206\uff0c\u5b9e\u73b0\u5e73\u6ed1\u63a7\u5236\u5e76\u5728Gymnasium\u548cIsaac-Lab\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u4f18\u52bf\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u867d\u80fd\u89e3\u51b3\u590d\u6742\u63a7\u5236\u4efb\u52a1\uff0c\u4f46\u5176\u4ea7\u751f\u7684\u9ad8\u9891\u52a8\u4f5c\u632f\u8361\u96be\u4ee5\u5e94\u7528\u4e8e\u771f\u5b9e\u73af\u5883\u3002\u73b0\u6709\u57fa\u4e8e\u635f\u5931\u7684\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u6216\u4eba\u5de5\u5b9a\u4e49\u7684\u72b6\u6001\u76f8\u4f3c\u6027\uff0c\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u7cfb\u7edf\u52a8\u529b\u5b66\u7279\u6027\u3002", "method": "\u63d0\u51fa\"\u8f6c\u79fb\u8bf1\u5bfc\u76f8\u4f3c\u72b6\u6001\"\u6982\u5ff5\uff08\u4ece\u524d\u4e00\u72b6\u6001\u8f6c\u79fb\u5f97\u5230\u7684\u4e0b\u4e00\u72b6\u6001\u5206\u5e03\uff09\uff0c\u5229\u7528\u73af\u5883\u53cd\u9988\u548c\u771f\u5b9e\u6570\u636e\u5b9a\u4e49\u3002\u57fa\u4e8e\u6b64\u8bbe\u8ba1ASAP\uff08Action Smoothing by Aligning Actions with Predictions from Preceding States\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ea6\u675f\u52a8\u4f5c\u4e0e\u76f8\u4f3c\u72b6\u6001\u52a8\u4f5c\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u60e9\u7f5a\u4e8c\u9636\u5dee\u5206\u6765\u6291\u5236\u9ad8\u9891\u632f\u8361\u3002", "result": "\u5728Gymnasium\u548cIsaac-Lab\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0cASAP\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u80fd\u4ea7\u751f\u66f4\u5e73\u6ed1\u7684\u63a7\u5236\u7b56\u7565\uff0c\u5e76\u83b7\u5f97\u66f4\u597d\u7684\u7b56\u7565\u6027\u80fd\u3002", "conclusion": "ASAP\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u76f8\u4f3c\u72b6\u6001\u5b9a\u4e49\u548c\u52a8\u4f5c\u5e73\u6ed1\u7ea6\u675f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u52a8\u4f5c\u632f\u8361\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u63a7\u5236\u7b56\u7565\u7684\u5b9e\u7528\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2601.18650", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18650", "abs": "https://arxiv.org/abs/2601.18650", "authors": ["Liheng Yu", "Zhe Zhao", "Yuxuan Wang", "Pengkun Wang", "Binwu Wang", "Yang Wang"], "title": "FaLW: A Forgetting-aware Loss Reweighting for Long-tailed Unlearning", "comment": "camera-ready for iclr2026", "summary": "Machine unlearning, which aims to efficiently remove the influence of specific data from trained models, is crucial for upholding data privacy regulations like the ``right to be forgotten\". However, existing research predominantly evaluates unlearning methods on relatively balanced forget sets. This overlooks a common real-world scenario where data to be forgotten, such as a user's activity records, follows a long-tailed distribution. Our work is the first to investigate this critical research gap. We find that in such long-tailed settings, existing methods suffer from two key issues: \\textit{Heterogeneous Unlearning Deviation} and \\textit{Skewed Unlearning Deviation}. To address these challenges, we propose FaLW, a plug-and-play, instance-wise dynamic loss reweighting method. FaLW innovatively assesses the unlearning state of each sample by comparing its predictive probability to the distribution of unseen data from the same class. Based on this, it uses a forgetting-aware reweighting scheme, modulated by a balancing factor, to adaptively adjust the unlearning intensity for each sample. Extensive experiments demonstrate that FaLW achieves superior performance. Code is available at \\textbf{Supplementary Material}.", "AI": {"tldr": "\u9488\u5bf9\u957f\u5c3e\u5206\u5e03\u4e0b\u7684\u673a\u5668\u9057\u5fd8\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faFaLW\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9e\u4f8b\u7ea7\u52a8\u6001\u635f\u5931\u91cd\u52a0\u6743\u89e3\u51b3\u5f02\u8d28\u6027\u548c\u504f\u659c\u9057\u5fd8\u504f\u5dee\uff0c\u5b9e\u73b0\u9ad8\u6548\u6570\u636e\u9057\u5fd8\u3002", "motivation": "\u673a\u5668\u9057\u5fd8\u5bf9\u4e8e\u7ef4\u62a4\u6570\u636e\u9690\u79c1\u6cd5\u89c4\uff08\u5982\"\u88ab\u9057\u5fd8\u6743\"\uff09\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5728\u76f8\u5bf9\u5e73\u8861\u7684\u9057\u5fd8\u96c6\u4e0a\u8bc4\u4f30\u9057\u5fd8\u65b9\u6cd5\uff0c\u5ffd\u7565\u4e86\u73b0\u5b9e\u4e16\u754c\u4e2d\u5f85\u9057\u5fd8\u6570\u636e\uff08\u5982\u7528\u6237\u6d3b\u52a8\u8bb0\u5f55\uff09\u901a\u5e38\u5448\u957f\u5c3e\u5206\u5e03\u7684\u573a\u666f\u3002", "method": "\u63d0\u51faFaLW\uff08\u9057\u5fd8\u611f\u77e5\u635f\u5931\u91cd\u52a0\u6743\uff09\uff0c\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u5b9e\u4f8b\u7ea7\u52a8\u6001\u635f\u5931\u91cd\u52a0\u6743\u65b9\u6cd5\u3002\u5b83\u901a\u8fc7\u6bd4\u8f83\u6837\u672c\u9884\u6d4b\u6982\u7387\u4e0e\u540c\u7c7b\u672a\u89c1\u6570\u636e\u5206\u5e03\u6765\u8bc4\u4f30\u6bcf\u4e2a\u6837\u672c\u7684\u9057\u5fd8\u72b6\u6001\uff0c\u7136\u540e\u4f7f\u7528\u7531\u5e73\u8861\u56e0\u5b50\u8c03\u5236\u7684\u9057\u5fd8\u611f\u77e5\u91cd\u52a0\u6743\u65b9\u6848\uff0c\u81ea\u9002\u5e94\u8c03\u6574\u6bcf\u4e2a\u6837\u672c\u7684\u9057\u5fd8\u5f3a\u5ea6\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFaLW\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u586b\u8865\u4e86\u957f\u5c3e\u5206\u5e03\u4e0b\u673a\u5668\u9057\u5fd8\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u63d0\u51fa\u7684FaLW\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u957f\u5c3e\u573a\u666f\u4e0b\u7684\u9057\u5fd8\u504f\u5dee\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.18500", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18500", "abs": "https://arxiv.org/abs/2601.18500", "authors": ["Chen Liang", "Donghua Yang", "Yutong Wang", "Tianle Zhang", "Shenghe Zhou", "Zhiyu Liang", "Hengtong Zhang", "Hongzhi Wang", "Ziqi Li", "Xiyang Zhang", "Zheng Liang", "Yifei Li"], "title": "Nearly Optimal Bayesian Inference for Structural Missingness", "comment": null, "summary": "Structural missingness breaks 'just impute and train': values can be undefined by causal or logical constraints, and the mask may depend on observed variables, unobserved variables (MNAR), and other missingness indicators. It simultaneously brings (i) a catch-22 situation with causal loop, prediction needs the missing features, yet inferring them depends on the missingness mechanism, (ii) under MNAR, the unseen are different, the missing part can come from a shifted distribution, and (iii) plug-in imputation, a single fill-in can lock in uncertainty and yield overconfident, biased decisions. In the Bayesian view, prediction via the posterior predictive distribution integrates over the full model posterior uncertainty, rather than relying on a single point estimate. This framework decouples (i) learning an in-model missing-value posterior from (ii) label prediction by optimizing the predictive posterior distribution, enabling posterior integration. This decoupling yields an in-model almost-free-lunch: once the posterior is learned, prediction is plug-and-play while preserving uncertainty propagation. It achieves SOTA on 43 classification and 15 imputation benchmarks, with finite-sample near Bayes-optimality guarantees under our SCM prior.", "AI": {"tldr": "\u8d1d\u53f6\u65af\u6846\u67b6\u89e3\u8026\u7f3a\u5931\u503c\u63a8\u65ad\u4e0e\u9884\u6d4b\uff0c\u5b9e\u73b0\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\uff0c\u572860\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbeSOTA\u3002", "motivation": "\u7ed3\u6784\u6027\u7f3a\u5931\u5bfc\u81f4\u56e0\u679c\u5faa\u73af\u3001MNAR\u5206\u5e03\u504f\u79fb\u548c\u5355\u70b9\u63d2\u8865\u504f\u5dee\uff0c\u4f20\u7edf\"\u5148\u63d2\u8865\u518d\u8bad\u7ec3\"\u65b9\u6cd5\u5931\u6548\u3002", "method": "\u91c7\u7528\u540e\u9a8c\u9884\u6d4b\u5206\u5e03\u79ef\u5206\u66ff\u4ee3\u5355\u70b9\u4f30\u8ba1\uff0c\u5c06\u7f3a\u5931\u503c\u540e\u9a8c\u5b66\u4e60\u4e0e\u6807\u7b7e\u9884\u6d4b\u89e3\u8026\uff0c\u4f18\u5316\u9884\u6d4b\u540e\u9a8c\u5206\u5e03\u3002", "result": "\u572843\u4e2a\u5206\u7c7b\u548c15\u4e2a\u63d2\u8865\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u5148\u9a8c\u4e0b\u5177\u6709\u6709\u9650\u6837\u672c\u8fd1\u8d1d\u53f6\u65af\u6700\u4f18\u6027\u4fdd\u8bc1\u3002", "conclusion": "\u5b9e\u73b0\"\u6a21\u578b\u5185\u514d\u8d39\u5348\u9910\"\uff1a\u4e00\u65e6\u5b66\u4e60\u5230\u540e\u9a8c\uff0c\u9884\u6d4b\u5373\u53ef\u5373\u63d2\u5373\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\uff0c\u6709\u6548\u89e3\u51b3\u7ed3\u6784\u6027\u7f3a\u5931\u7684\u4e09\u5927\u6311\u6218\u3002"}}
{"id": "2601.18675", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18675", "abs": "https://arxiv.org/abs/2601.18675", "authors": ["Aditya Kumar", "Mario A. Cypko", "Oliver Amft"], "title": "Learning temporal embeddings from electronic health records of chronic kidney disease patients", "comment": "7 pages, 3 figures, 3 tables. The paper has been submitted to IEEE EMBC 2026 and copyright might be transferred without notice", "summary": "We investigate whether temporal embedding models trained on longitudinal electronic health records can learn clinically meaningful representations without compromising predictive performance, and how architectural choices affect embedding quality. Model-guided medicine requires representations that capture disease dynamics while remaining transparent and task agnostic, whereas most clinical prediction models are optimised for a single task. Representation learning facilitates learning embeddings that generalise across downstream tasks, and recurrent architectures are well-suited for modelling temporal structure in observational clinical data. Using the MIMIC-IV dataset, we study patients with chronic kidney disease (CKD) and compare three recurrent architectures: a vanilla LSTM, an attention-augmented LSTM, and a time-aware LSTM (T-LSTM). All models are trained both as embedding models and as direct end-to-end predictors. Embedding quality is evaluated via CKD stage clustering and in-ICU mortality prediction. The T-LSTM produces more structured embeddings, achieving a lower Davies-Bouldin Index (DBI = 9.91) and higher CKD stage classification accuracy (0.74) than the vanilla LSTM (DBI = 15.85, accuracy = 0.63) and attention-augmented LSTM (DBI = 20.72, accuracy = 0.67). For in-ICU mortality prediction, embedding models consistently outperform end-to-end predictors, improving accuracy from 0.72-0.75 to 0.82-0.83, which indicates that learning embeddings as an intermediate step is more effective than direct end-to-end learning.", "AI": {"tldr": "Time-aware LSTM (T-LSTM) generates superior clinical embeddings from EHR data, outperforming vanilla and attention-augmented LSTMs. Embedding-based models achieve higher ICU mortality prediction accuracy (0.82-0.83) than end-to-end predictors (0.72-0.75).", "motivation": "Develop clinically meaningful, task-agnostic temporal representations from longitudinal EHRs that capture disease dynamics without sacrificing predictive performance, enabling model-guided medicine rather than single-task optimization.", "method": "Studied CKD patients in MIMIC-IV using three recurrent architectures (vanilla LSTM, attention-augmented LSTM, T-LSTM), trained as both embedding models and end-to-end predictors. Evaluated embedding quality via CKD stage clustering and ICU mortality prediction.", "result": "T-LSTM produced most structured embeddings (DBI=9.91, CKD accuracy=0.74), surpassing vanilla LSTM (DBI=15.85, accuracy=0.63) and attention-augmented LSTM (DBI=20.72, accuracy=0.67). Embedding models consistently outperformed end-to-end predictors for mortality prediction (0.82-0.83 vs 0.72-0.75 accuracy).", "conclusion": "Learning embeddings as an intermediate step is more effective than direct end-to-end learning for clinical prediction tasks. T-LSTM architecture is optimal for capturing temporal disease dynamics in EHR data while maintaining transparency and generalizability."}}
{"id": "2601.18509", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18509", "abs": "https://arxiv.org/abs/2601.18509", "authors": ["Andro Sabashvili"], "title": "Conformal Prediction Algorithms for Time Series Forecasting: Methods and Benchmark", "comment": null, "summary": "Reliable uncertainty quantification is of critical importance in time series forecasting, yet traditional methods often rely on restrictive distributional assumptions. Conformal prediction (CP) has emerged as a promising distribution-free framework for generating prediction intervals with rigorous theoretical guarantees. However, applying CP to sequential data presents a primary challenge: the temporal dependencies inherent in time series fundamentally violate the core assumption of data exchangeability, upon which standard CP guarantees are built. This review critically examines the main categories of algorithmic solutions designed to address this conflict. We survey and benchmark methods that relax the exchangeability assumption, those that redefine the data unit to be a collection of independent time series, approaches that explicitly model the dynamics of the prediction residuals, and online learning algorithms that adapt to distribution shifts to maintain long-run coverage. By synthesizing these approaches, we highlight computational efficiency and practical performance on real-world data.", "AI": {"tldr": "This review paper analyzes conformal prediction methods for time series forecasting, focusing on solutions that address the violation of exchangeability assumption due to temporal dependencies, and benchmarks four categories of algorithms for uncertainty quantification.", "motivation": "Traditional uncertainty quantification methods in time series forecasting rely on restrictive distributional assumptions, while conformal prediction (CP) offers distribution-free guarantees but requires data exchangeability - an assumption violated by temporal dependencies in sequential data.", "method": "The paper surveys and benchmarks four main algorithmic categories: (1) methods relaxing exchangeability, (2) approaches redefining data units as independent time series collections, (3) methods explicitly modeling prediction residual dynamics, and (4) online learning algorithms adapting to distribution shifts.", "result": "The review synthesizes these approaches and evaluates their computational efficiency and practical performance on real-world data, providing a comprehensive comparison of CP adaptations for time series.", "conclusion": "The paper highlights that adapting conformal prediction to time series requires careful consideration of temporal dependencies, and the surveyed methods offer different trade-offs between theoretical guarantees, computational efficiency, and real-world performance."}}
{"id": "2601.18681", "categories": ["cs.LG", "cs.AI", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.18681", "abs": "https://arxiv.org/abs/2601.18681", "authors": ["Yilie Huang", "Wenpin Tang", "Xunyu Zhou"], "title": "ART for Diffusion Sampling: A Reinforcement Learning Approach to Timestep Schedule", "comment": "17 pages, 7 figures", "summary": "We consider time discretization for score-based diffusion models to generate samples from a learned reverse-time dynamic on a finite grid. Uniform and hand-crafted grids can be suboptimal given a budget on the number of time steps. We introduce Adaptive Reparameterized Time (ART) that controls the clock speed of a reparameterized time variable, leading to a time change and uneven timesteps along the sampling trajectory while preserving the terminal time. The objective is to minimize the aggregate error arising from the discretized Euler scheme. We derive a randomized control companion, ART-RL, and formulate time change as a continuous-time reinforcement learning (RL) problem with Gaussian policies. We then prove that solving ART-RL recovers the optimal ART schedule, which in turn enables practical actor--critic updates to learn the latter in a data-driven way. Empirically, based on the official EDM pipeline, ART-RL improves Fr\u00e9chet Inception Distance on CIFAR-10 over a wide range of budgets and transfers to AFHQv2, FFHQ, and ImageNet without the need of retraining.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u91cd\u53c2\u6570\u5316\u65f6\u95f4(ART)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6269\u6563\u6a21\u578b\u7684\u65f6\u95f4\u6b65\u8c03\u5ea6\uff0c\u5728\u6709\u9650\u9884\u7b97\u4e0b\u63d0\u5347\u91c7\u6837\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u91c7\u7528\u5747\u5300\u6216\u624b\u5de5\u8bbe\u8ba1\u7684\u65f6\u95f4\u7f51\u683c\u8fdb\u884c\u79bb\u6563\u5316\uff0c\u5728\u7ed9\u5b9a\u6b65\u6570\u9884\u7b97\u4e0b\u5e76\u975e\u6700\u4f18\uff0c\u5bfc\u81f4\u91c7\u6837\u6548\u7387\u4f4e\u4e0b\u3002", "method": "1) \u63d0\u51faART\u63a7\u5236\u91cd\u53c2\u6570\u5316\u65f6\u95f4\u53d8\u91cf\u7684\"\u65f6\u949f\u901f\u5ea6\"\uff0c\u4ea7\u751f\u975e\u5747\u5300\u65f6\u95f4\u6b65\uff1b2) \u5efa\u7acbART-RL\u6846\u67b6\uff0c\u5c06\u65f6\u95f4\u8c03\u5ea6\u5efa\u6a21\u4e3a\u8fde\u7eed\u65f6\u95f4\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff1b3) \u8bc1\u660e\u6c42\u89e3ART-RL\u53ef\u6062\u590d\u6700\u4f18ART\u8c03\u5ea6\uff1b4) \u4f7f\u7528\u6f14\u5458-\u8bc4\u8bba\u5bb6\u7b97\u6cd5\u6570\u636e\u9a71\u52a8\u5730\u5b66\u4e60\u8c03\u5ea6\u7b56\u7565\u3002", "result": "\u5728EDM pipeline\u4e0a\uff0cART-RL\u5728CIFAR-10\u4e0a\u663e\u8457\u63d0\u5347FID\u6307\u6807\uff0c\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u8fc1\u79fb\u5230AFHQv2\u3001FFHQ\u548cImageNet\u3002", "conclusion": "ART-RL\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u5b66\u4e60\u6700\u4f18\u65f6\u95f4\u8c03\u5ea6\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u6269\u6563\u6a21\u578b\u7684\u91c7\u6837\u6548\u7387\u548c\u8d28\u91cf\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.18702", "categories": ["cs.LG", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2601.18702", "abs": "https://arxiv.org/abs/2601.18702", "authors": ["Hansheng Ren"], "title": "From Fuzzy to Exact: The Halo Architecture for Infinite-Depth Reasoning via Rational Arithmetic", "comment": "8 pages, 6 figures. Submitted to UAI 2026", "summary": "Current paradigms in Deep Learning prioritize computational throughput over numerical precision, relying on the assumption that intelligence emerges from statistical correlation at scale. In this paper, we challenge this orthodoxy. We propose the Exactness Hypothesis: that General Intelligence (AGI), specifically high-order causal inference, requires a computational substrate capable of Arbitrary Precision Arithmetic. We argue that the \"hallucinations\" and logical incoherence seen in current Large Language Models (LLMs) are artifacts of IEEE 754 floating-point approximation errors accumulating over deep compositional functions. To mitigate this, we introduce the Halo Architecture, a paradigm shift to Rational Arithmetic ($\\mathbb{Q}$) supported by a novel Exact Inference Unit (EIU). Empirical validation on the Huginn-0125 prototype demonstrates that while 600B-parameter scale BF16 baselines collapse in chaotic systems, Halo maintains zero numerical divergence indefinitely. This work establishes exact arithmetic as a prerequisite for reducing logical uncertainty in System 2 AGI.", "AI": {"tldr": "\u6311\u6218\u6df1\u5ea6\u5b66\u4e60\u91cd\u89c6\u541e\u5410\u91cf\u800c\u975e\u7cbe\u5ea6\u7684\u8303\u5f0f\uff0c\u63d0\u51fa\u7cbe\u786e\u6027\u5047\u8bf4\uff1a\u901a\u7528\u4eba\u5de5\u667a\u80fd\u9700\u8981\u4efb\u610f\u7cbe\u5ea6\u7b97\u672f\uff0c\u5e76\u5f15\u5165Halo\u67b6\u6784\u4f7f\u7528\u6709\u7406\u6570\u8fd0\u7b97\u6765\u6d88\u9664\u6570\u503c\u8bef\u5dee", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u201c\u5e7b\u89c9\u201d\u548c\u903b\u8f91\u4e0d\u8fde\u8d2f\u95ee\u9898\u6e90\u4e8eIEEE 754\u6d6e\u70b9\u8fd1\u4f3c\u8bef\u5dee\u5728\u6df1\u5ea6\u7ec4\u5408\u51fd\u6570\u4e2d\u7684\u7d2f\u79ef", "method": "\u63d0\u51faHalo\u67b6\u6784\uff0c\u91c7\u7528\u6709\u7406\u6570\u8fd0\u7b97\uff08Q\uff09\u548c\u652f\u6301\u7cbe\u786e\u63a8\u7406\u5355\u5143\uff08EIU\uff09", "result": "Huginn-0125\u539f\u578b\u9a8c\u8bc1\u663e\u793a\uff0c6000\u4ebf\u53c2\u6570BF16\u57fa\u7ebf\u5728\u6df7\u6c8c\u7cfb\u7edf\u4e2d\u5d29\u6e83\u65f6\uff0cHalo\u80fd\u65e0\u9650\u671f\u4fdd\u6301\u96f6\u6570\u503c\u53d1\u6563", "conclusion": "\u7cbe\u786e\u7b97\u672f\u662f\u51cf\u5c11\u7cfb\u7edf2\u901a\u7528\u4eba\u5de5\u667a\u80fd\u903b\u8f91\u4e0d\u786e\u5b9a\u6027\u7684\u5148\u51b3\u6761\u4ef6"}}
{"id": "2601.18751", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18751", "abs": "https://arxiv.org/abs/2601.18751", "authors": ["Seyed Amir Hosseini", "Maryam Abdolali", "Amirhosein Tavakkoli", "Fardin Ayar", "Ehsan Javanmardi", "Manabu Tsukada", "Mahdi Javanmardi"], "title": "Trust, Don't Trust, or Flip: Robust Preference-Based Reinforcement Learning with Multi-Expert Feedback", "comment": "Equal contribution: Seyed Amir Hosseini and Maryam Abdolali. Corresponding author: Maryam Abdolali (maryam.abdolali@kntu.ac.ir)", "summary": "Preference-based reinforcement learning (PBRL) offers a promising alternative to explicit reward engineering by learning from pairwise trajectory comparisons. However, real-world preference data often comes from heterogeneous annotators with varying reliability; some accurate, some noisy, and some systematically adversarial. Existing PBRL methods either treat all feedback equally or attempt to filter out unreliable sources, but both approaches fail when faced with adversarial annotators who systematically provide incorrect preferences. We introduce TriTrust-PBRL (TTP), a unified framework that jointly learns a shared reward model and expert-specific trust parameters from multi-expert preference feedback. The key insight is that trust parameters naturally evolve during gradient-based optimization to be positive (trust), near zero (ignore), or negative (flip), enabling the model to automatically invert adversarial preferences and recover useful signal rather than merely discarding corrupted feedback. We provide theoretical analysis establishing identifiability guarantees and detailed gradient analysis that explains how expert separation emerges naturally during training without explicit supervision. Empirically, we evaluate TTP on four diverse domains spanning manipulation tasks (MetaWorld) and locomotion (DM Control) under various corruption scenarios. TTP achieves state-of-the-art robustness, maintaining near-oracle performance under adversarial corruption while standard PBRL methods fail catastrophically. Notably, TTP outperforms existing baselines by successfully learning from mixed expert pools containing both reliable and adversarial annotators, all while requiring no expert features beyond identification indices and integrating seamlessly with existing PBRL pipelines.", "AI": {"tldr": "\u63d0\u51faTriTrust-PBRL\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u5171\u4eab\u5956\u52b1\u6a21\u578b\u548c\u4e13\u5bb6\u4fe1\u4efb\u53c2\u6570\uff0c\u81ea\u52a8\u8bc6\u522b\u5e76\u53cd\u8f6c\u5bf9\u6297\u6027\u6807\u6ce8\u8005\u504f\u597d\uff0c\u5728\u5b58\u5728\u4e0d\u53ef\u9760\u548c\u5bf9\u6297\u6027\u6807\u6ce8\u8005\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9c81\u68d2\u504f\u597d\u5f3a\u5316\u5b66\u4e60\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u504f\u597d\u6570\u636e\u6765\u81ea\u5f02\u6784\u6807\u6ce8\u8005\uff0c\u5305\u542b\u51c6\u786e\u3001\u566a\u58f0\u548c\u7cfb\u7edf\u6027\u5bf9\u6297\u6027\u53cd\u9988\u3002\u73b0\u6709PBRL\u65b9\u6cd5\u8981\u4e48\u5e73\u7b49\u5bf9\u5f85\u6240\u6709\u53cd\u9988\uff0c\u8981\u4e48\u8fc7\u6ee4\u4e0d\u53ef\u9760\u6e90\uff0c\u4f46\u9762\u5bf9\u5bf9\u6297\u6027\u6807\u6ce8\u8005\u65f6\u4f1a\u5931\u6548\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "TriTrust-PBRL (TTP) \u6846\u67b6\u8054\u5408\u5b66\u4e60\u5171\u4eab\u5956\u52b1\u51fd\u6570\u548c\u4e13\u5bb6\u7279\u5b9a\u4fe1\u4efb\u53c2\u6570\u3002\u4fe1\u4efb\u53c2\u6570\u5728\u68af\u5ea6\u4f18\u5316\u4e2d\u81ea\u7136\u6f14\u5316\u4e3a\u6b63\u503c\uff08\u4fe1\u4efb\uff09\u3001\u8fd1\u96f6\u503c\uff08\u5ffd\u7565\uff09\u6216\u8d1f\u503c\uff08\u53cd\u8f6c\uff09\uff0c\u4f7f\u6a21\u578b\u80fd\u81ea\u52a8\u53cd\u8f6c\u5bf9\u6297\u6027\u504f\u597d\u800c\u975e\u7b80\u5355\u4e22\u5f03\u3002\u63d0\u4f9b\u53ef\u8bc6\u522b\u6027\u4fdd\u8bc1\u548c\u68af\u5ea6\u5206\u6790\u3002", "result": "\u5728MetaWorld\u64cd\u4f5c\u548cDM Control\u8fd0\u52a8\u4efb\u52a1\u4e0a\uff0cTTP\u5728\u5bf9\u6297\u6027\u8150\u8680\u4e0b\u4fdd\u6301\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u6807\u51c6PBRL\u65b9\u6cd5\u3002\u80fd\u6210\u529f\u4ece\u5305\u542b\u53ef\u9760\u548c\u5bf9\u6297\u6027\u6807\u6ce8\u8005\u7684\u6df7\u5408\u4e13\u5bb6\u6c60\u4e2d\u5b66\u4e60\uff0c\u65e0\u9700\u989d\u5916\u4e13\u5bb6\u7279\u5f81\uff0c\u4e14\u4e0e\u73b0\u6709PBRL\u6d41\u7a0b\u65e0\u7f1d\u96c6\u6210\u3002", "conclusion": "TTP\u4e3a\u5904\u7406\u5f02\u6784\u6807\u6ce8\u8005\u504f\u597d\u6570\u636e\u63d0\u4f9b\u4e86\u7edf\u4e00\u9c81\u68d2\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u4fe1\u4efb\u5efa\u6a21\u5b9e\u73b0\u4e86\u5bf9\u6297\u6027\u504f\u597d\u53cd\u8f6c\uff0c\u5728\u65e0\u9700\u4e13\u5bb6\u7279\u5f81\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u4e1a\u754c\u6700\u4f73\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.18524", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18524", "abs": "https://arxiv.org/abs/2601.18524", "authors": ["Yongqi Jin", "Yecheng Wang", "Jun-jie Wang", "Rong Zhu", "Guolin Ke", "Weinan E"], "title": "From Human Labels to Literature: Semi-Supervised Learning of NMR Chemical Shifts at Scale", "comment": null, "summary": "Accurate prediction of nuclear magnetic resonance (NMR) chemical shifts is fundamental to spectral analysis and molecular structure elucidation, yet existing machine learning methods rely on limited, labor-intensive atom-assigned datasets. We propose a semi-supervised framework that learns NMR chemical shifts from millions of literature-extracted spectra without explicit atom-level assignments, integrating a small amount of labeled data with large-scale unassigned spectra. We formulate chemical shift prediction from literature spectra as a permutation-invariant set supervision problem, and show that under commonly satisfied conditions on the loss function, optimal bipartite matching reduces to a sorting-based loss, enabling stable large-scale semi-supervised training beyond traditional curated datasets. Our models achieve substantially improved accuracy and robustness over state-of-the-art methods and exhibit stronger generalization on significantly larger and more diverse molecular datasets. Moreover, by incorporating solvent information at scale, our approach captures systematic solvent effects across common NMR solvents for the first time. Overall, our results demonstrate that large-scale unlabeled spectra mined from the literature can serve as a practical and effective data source for training NMR shift models, suggesting a broader role of literature-derived, weakly structured data in data-centric AI for science.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u534a\u76d1\u7763\u6846\u67b6\uff0c\u5229\u7528\u6570\u767e\u4e07\u6587\u732e\u63d0\u53d6\u7684\u672a\u6807\u8bb0NMR\u5149\u8c31\u548c\u5c11\u91cf\u6807\u8bb0\u6570\u636e\u9884\u6d4b\u5316\u5b66\u4f4d\u79fb\uff0c\u901a\u8fc7\u6392\u5e8f\u635f\u5931\u5b9e\u73b0\u5927\u89c4\u6a21\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u9996\u6b21\u6355\u83b7\u7cfb\u7edf\u6027\u6eb6\u5242\u6548\u5e94\u3002", "motivation": "\u73b0\u6709NMR\u5316\u5b66\u4f4d\u79fb\u9884\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u6709\u9650\u4e14\u6807\u6ce8\u6210\u672c\u9ad8\u7684\u539f\u5b50\u7ea7\u6807\u8bb0\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5c06\u5316\u5b66\u4f4d\u79fb\u9884\u6d4b\u5efa\u6a21\u4e3a\u7f6e\u6362\u4e0d\u53d8\u96c6\u5408\u76d1\u7763\u95ee\u9898\uff0c\u5229\u7528\u6587\u732e\u4e2d\u5927\u89c4\u6a21\u672a\u6807\u8bb0\u5149\u8c31\uff0c\u5728\u7279\u5b9a\u635f\u5931\u6761\u4ef6\u4e0b\u5c06\u6700\u4f18\u4e8c\u5206\u5339\u914d\u7b80\u5316\u4e3a\u6392\u5e8f\u635f\u5931\uff0c\u5b9e\u73b0\u7a33\u5b9a\u534a\u76d1\u7763\u8bad\u7ec3\uff0c\u5e76\u6574\u5408\u6eb6\u5242\u4fe1\u606f\u3002", "result": "\u6a21\u578b\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u66f4\u5927\u66f4\u590d\u6742\u7684\u5206\u5b50\u6570\u636e\u96c6\u4e0a\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u8de8\u5e38\u89c1NMR\u6eb6\u5242\u7684\u7cfb\u7edf\u6027\u6eb6\u5242\u6548\u5e94\u6355\u83b7\u3002", "conclusion": "\u6587\u732e\u6316\u6398\u7684\u5927\u89c4\u6a21\u672a\u6807\u8bb0\u5149\u8c31\u53ef\u4f5c\u4e3aNMR\u4f4d\u79fb\u6a21\u578b\u8bad\u7ec3\u7684\u6709\u6548\u6570\u636e\u6e90\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u79d1\u5b66AI\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u8868\u660e\u5f31\u7ed3\u6784\u5316\u6587\u732e\u6570\u636e\u5728\u79d1\u5b66AI\u4e2d\u5177\u6709\u5e7f\u9614\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2601.18753", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18753", "abs": "https://arxiv.org/abs/2601.18753", "authors": ["Xinyue Zeng", "Junhong Lin", "Yujun Yan", "Feng Guo", "Liang Shi", "Jun Wu", "Dawei Zhou"], "title": "HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs", "comment": "Have been accepted by ICLR'26", "summary": "The reliability of Large Language Models (LLMs) in high-stakes domains such as healthcare, law, and scientific discovery is often compromised by hallucinations. These failures typically stem from two sources: data-driven hallucinations and reasoning-driven hallucinations. However, existing detection methods usually address only one source and rely on task-specific heuristics, limiting their generalization to complex scenarios. To overcome these limitations, we introduce the Hallucination Risk Bound, a unified theoretical framework that formally decomposes hallucination risk into data-driven and reasoning-driven components, linked respectively to training-time mismatches and inference-time instabilities. This provides a principled foundation for analyzing how hallucinations emerge and evolve. Building on this foundation, we introduce HalluGuard, an NTK-based score that leverages the induced geometry and captured representations of the NTK to jointly identify data-driven and reasoning-driven hallucinations. We evaluate HalluGuard on 10 diverse benchmarks, 11 competitive baselines, and 9 popular LLM backbones, consistently achieving state-of-the-art performance in detecting diverse forms of LLM hallucinations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e7b\u89c9\u98ce\u9669\u8fb9\u754c\u7edf\u4e00\u7406\u8bba\u6846\u67b6\u548c\u57fa\u4e8eNTK\u7684HalluGuard\u68c0\u6d4b\u65b9\u6cd5\uff0c\u53ef\u8054\u5408\u8bc6\u522b\u6570\u636e\u9a71\u52a8\u548c\u63a8\u7406\u9a71\u52a8\u4e24\u7c7b\u5e7b\u89c9\uff0c\u572810\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0SOTA\u6027\u80fd", "motivation": "\u73b0\u6709\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\u4ec5\u9488\u5bf9\u5355\u4e00\u6765\u6e90\u4e14\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u542f\u53d1\u5f0f\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u533b\u7597\u3001\u6cd5\u5f8b\u7b49\u9ad8\u590d\u6742\u5ea6\u3001\u9ad8\u98ce\u9669\u573a\u666f", "method": "1) \u5e7b\u89c9\u98ce\u9669\u8fb9\u754c\u7406\u8bba\uff1a\u5c06\u5e7b\u89c9\u98ce\u9669\u5206\u89e3\u4e3a\u8bad\u7ec3\u4e0d\u5339\u914d\uff08\u6570\u636e\u9a71\u52a8\uff09\u548c\u63a8\u7406\u4e0d\u7a33\u5b9a\uff08\u63a8\u7406\u9a71\u52a8\uff09\u4e24\u90e8\u5206\uff1b2) HalluGuard\uff1a\u5229\u7528\u795e\u7ecf\u6b63\u5207\u6838\u7684\u51e0\u4f55\u7ed3\u6784\u548c\u8868\u793a\u7279\u5f81\u8054\u5408\u8bc6\u522b\u4e24\u7c7b\u5e7b\u89c9", "result": "\u572810\u4e2a\u591a\u6837\u5316\u57fa\u51c6\u300111\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u548c9\u4e2a\u4e3b\u6d41LLM\u4e0a\u5747\u53d6\u5f97\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u6cdb\u5316\u80fd\u529b", "conclusion": "\u8be5\u7406\u8bba\u6846\u67b6\u4e3a\u5206\u6790\u5e7b\u89c9\u4ea7\u751f\u4e0e\u6f14\u5316\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u57fa\u7840\uff0cNTK-based\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u4e24\u7c7b\u5e7b\u89c9\u7684\u8054\u5408\u68c0\u6d4b"}}
{"id": "2601.18525", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18525", "abs": "https://arxiv.org/abs/2601.18525", "authors": ["Eleonora Grassucci", "Giordano Cicchetti", "Emanuele Frasca", "Aurelio Uncini", "Danilo Comminiello"], "title": "Closing the Modality Gap Aligns Group-Wise Semantics", "comment": "ICLR 2026", "summary": "In multimodal learning, CLIP has been recognized as the \\textit{de facto} method for learning a shared latent space across multiple modalities, placing similar representations close to each other and moving them away from dissimilar ones. Although CLIP-based losses effectively align modalities at the semantic level, the resulting latent spaces often remain only partially shared, revealing a structural mismatch known as the modality gap. While the necessity of addressing this phenomenon remains debated, particularly given its limited impact on instance-wise tasks (e.g., retrieval), we prove that its influence is instead strongly pronounced in group-level tasks (e.g., clustering). To support this claim, we introduce a novel method designed to consistently reduce this discrepancy in two-modal settings, with a straightforward extension to the general $n$-modal case. Through our extensive evaluation, we demonstrate our novel insight: while reducing the gap provides only marginal or inconsistent improvements in traditional instance-wise tasks, it significantly enhances group-wise tasks. These findings may reshape our understanding of the modality gap, highlighting its key role in improving performance on tasks requiring semantic grouping.", "AI": {"tldr": "CLIP\u5b58\u5728\u6a21\u6001\u9e3f\u6c9f\u95ee\u9898\uff0c\u65b0\u7814\u7a76\u53d1\u73b0\u5176\u5bf9\u805a\u7c7b\u7b49\u5206\u7ec4\u4efb\u52a1\u5f71\u54cd\u5de8\u5927\uff0c\u800c\u5bf9\u68c0\u7d22\u7b49\u5b9e\u4f8b\u4efb\u52a1\u5f71\u54cd\u6709\u9650\uff1b\u63d0\u51fa\u7684\u95f4\u9699\u7f29\u51cf\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u5206\u7ec4\u6027\u80fd", "motivation": "\u63a2\u7a76CLIP\u6a21\u6001\u9e3f\u6c9f\u5bf9\u4e0d\u540c\u4efb\u52a1\u7684\u5f71\u54cd\u5dee\u5f02\uff0c\u89e3\u51b3\u5176\u5728\u5206\u7ec4\u4efb\u52a1\u4e2d\u88ab\u5ffd\u89c6\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u5f00\u53d1\u4e00\u81f4\u7684\u95f4\u9699\u7f29\u51cf\u65b9\u6cd5", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u6a21\u6001\u573a\u666f\u4e0b\u6301\u7eed\u7f29\u51cf\u6a21\u6001\u9e3f\u6c9f\u7684\u65b9\u6cd5\uff0c\u53ef\u81ea\u7136\u6269\u5c55\u5230n\u6a21\u6001\u901a\u7528\u60c5\u51b5", "result": "\u4f20\u7edf\u5b9e\u4f8b\u7ea7\u4efb\u52a1\uff08\u68c0\u7d22\uff09\u4ec5\u83b7\u8fb9\u9645\u6539\u8fdb\uff0c\u800c\u7ec4\u7ea7\u4efb\u52a1\uff08\u805a\u7c7b\uff09\u6027\u80fd\u663e\u8457\u63d0\u5347", "conclusion": "\u6a21\u6001\u9e3f\u6c9f\u5bf9\u8bed\u4e49\u5206\u7ec4\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u8be5\u53d1\u73b0\u91cd\u5851\u4e86\u5bf9\u8be5\u73b0\u8c61\u7684\u7406\u89e3\uff0c\u5f3a\u8c03\u5176\u5728\u5206\u7ec4\u4efb\u52a1\u4e2d\u7684\u5173\u952e\u4f5c\u7528"}}
{"id": "2601.18777", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18777", "abs": "https://arxiv.org/abs/2601.18777", "authors": ["Abhishek Divekar", "Anirban Majumder"], "title": "PRECISE: Reducing the Bias of LLM Evaluations Using Prediction-Powered Ranking Estimation", "comment": "Accepted at AAAI 2026 - Innovative Applications of AI (IAAI-26)", "summary": "Evaluating the quality of search, ranking and RAG systems traditionally requires a significant number of human relevance annotations. In recent times, several deployed systems have explored the usage of Large Language Models (LLMs) as automated judges for this task while their inherent biases prevent direct use for metric estimation. We present a statistical framework extending Prediction-Powered Inference (PPI) that combines minimal human annotations with LLM judgments to produce reliable estimates of metrics which require sub-instance annotations. Our method requires as few as 100 human-annotated queries and 10,000 unlabeled examples, reducing annotation requirements significantly compared to traditional approaches. We formulate our proposed framework (PRECISE) for inference of relevance uplift for an LLM-based query reformulation application, extending PPI to sub-instance annotations at the query-document level. By reformulating the metric-integration space, we reduced the computational complexity from O(2^|C|) to O(2^K), where |C| represents corpus size (in order of millions). Detailed experiments across prominent retrieval datasets demonstrate that our method reduces the variance of estimates for the business-critical Precision@K metric, while effectively correcting for LLM bias in low-resource settings.", "AI": {"tldr": "PRECISE combines minimal human annotations (100 queries) with LLM judgments to evaluate search systems reliably while correcting bias, cutting annotation needs dramatically.", "motivation": "Evaluating search/ranking/RAG systems traditionally requires massive human annotation efforts, and while LLMs offer automation, their biases prevent direct use for accurate metric estimation.", "method": "Extended PPI framework (PRECISE) that integrates few human annotations with many LLM judgments, reformulating metric-integration to reduce complexity from O(2^|C|) to O(2^K).", "result": "Achieves reliable Precision@K estimates with 100 human queries and 10,000 unlabeled examples, reducing variance and correcting LLM bias across retrieval datasets.", "conclusion": "Enables cost-effective, reliable evaluation of retrieval systems by combining limited human oversight with automated LLM judgments at scale."}}
{"id": "2601.18546", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18546", "abs": "https://arxiv.org/abs/2601.18546", "authors": ["Arash Jamshidi", "Katsiaryna Haitsiukevich", "Kai Puolam\u00e4ki"], "title": "Information Hidden in Gradients of Regression with Target Noise", "comment": null, "summary": "Second-order information -- such as curvature or data covariance -- is critical for optimisation, diagnostics, and robustness. However, in many modern settings, only the gradients are observable. We show that the gradients alone can reveal the Hessian, equalling the data covariance $\u03a3$ for the linear regression. Our key insight is a simple variance calibration: injecting Gaussian noise so that the total target noise variance equals the batch size ensures that the empirical gradient covariance closely approximates the Hessian, even when evaluated far from the optimum. We provide non-asymptotic operator-norm guarantees under sub-Gaussian inputs. We also show that without such calibration, recovery can fail by an $\u03a9(1)$ factor. The proposed method is practical (a \"set target-noise variance to $n$\" rule) and robust (variance $\\mathcal{O}(n)$ suffices to recover $\u03a3$ up to scale). Applications include preconditioning for faster optimisation, adversarial risk estimation, and gradient-only training, for example, in distributed systems. We support our theoretical results with experiments on synthetic and real data.", "AI": {"tldr": "\u63d0\u51fa\u65b9\u5dee\u6821\u51c6\u65b9\u6cd5\uff0c\u4ec5\u7528\u68af\u5ea6\u5373\u53ef\u6062\u590dHessian\u77e9\u9635\uff0c\u566a\u58f0\u65b9\u5dee\u8bbe\u4e3a\u6279\u5927\u5c0fn\uff0c\u5177\u6709\u975e\u6e10\u8fd1\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e94\u7528\u4e8e\u4f18\u5316\u548c\u9c81\u68d2\u6027\u5206\u6790\u3002", "motivation": "\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u4e2d\u5e38\u4ec5\u80fd\u83b7\u5f97\u68af\u5ea6\uff0c\u4f46Hessian/\u6570\u636e\u534f\u65b9\u5dee\u5bf9\u4f18\u5316\u3001\u8bca\u65ad\u548c\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\u3002\u76f4\u63a5\u83b7\u53d6\u4e8c\u9636\u4fe1\u606f\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u4ece\u68af\u5ea6\u4e2d\u63a8\u65ad\u3002", "method": "\u6ce8\u5165\u9ad8\u65af\u566a\u58f0\u4f7f\u603b\u76ee\u6807\u566a\u58f0\u65b9\u5dee\u7b49\u4e8e\u6279\u5927\u5c0f\uff0c\u7ecf\u9a8c\u68af\u5ea6\u534f\u65b9\u5dee\u5373\u53ef\u903c\u8fd1Hessian\u3002\u7406\u8bba\u5206\u6790\u5728\u4e9a\u9ad8\u65af\u8f93\u5165\u4e0b\u7684\u975e\u6e10\u8fd1\u7b97\u5b50\u8303\u6570\u8bef\u5dee\u754c\uff0c\u5e76\u8bc1\u660e\u65e0\u6821\u51c6\u4f1a\u5931\u8d25\u3002", "result": "\u7406\u8bba\uff1a\u68af\u5ea6\u534f\u65b9\u5dee\u2248Hessian\uff0c\u63d0\u4f9b\u975e\u6e10\u8fd1\u8bef\u5dee\u754c\uff0c\u6821\u51c6\u5fc5\u8981\u4e14\u5145\u5206\u3002\u5b9e\u8df5\uff1a\u65b9\u6cd5\u7b80\u5355\u9c81\u68d2\uff0cO(n)\u65b9\u5dee\u5373\u53ef\u6062\u590d\u03a3\u81f3\u5c3a\u5ea6\uff0c\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "conclusion": "\u5b9e\u73b0\u4e86\u4ece\u4e00\u9636\u68af\u5ea6\u6709\u6548\u63d0\u53d6\u4e8c\u9636\u4fe1\u606f\uff0c\u4e3a\u9884\u6761\u4ef6\u4f18\u5316\u3001\u5bf9\u6297\u98ce\u9669\u4f30\u8ba1\u3001\u5206\u5e03\u5f0f\u7eaf\u68af\u5ea6\u8bad\u7ec3\u7b49\u5e94\u7528\u63d0\u4f9b\u65b0\u5de5\u5177\uff0c\u517c\u5177\u7406\u8bba\u548c\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.18779", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18779", "abs": "https://arxiv.org/abs/2601.18779", "authors": ["Yuxiao Qu", "Amrith Setlur", "Virginia Smith", "Ruslan Salakhutdinov", "Aviral Kumar"], "title": "POPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration", "comment": null, "summary": "Reinforcement learning (RL) has improved the reasoning abilities of large language models (LLMs), yet state-of-the-art methods still fail to learn on many training problems. On hard problems, on-policy RL rarely explores even a single correct rollout, yielding zero reward and no learning signal for driving improvement. We find that natural solutions to remedy this exploration problem from classical RL, such as entropy bonuses, more permissive clipping of the importance ratio, or direct optimization of pass@k objectives, do not resolve this issue and often destabilize optimization without improving solvability. A natural alternative is to leverage transfer from easier problems. However, we show that mixing easy and hard problems during RL training is counterproductive due to ray interference, where optimization focuses on already-solvable problems in a way that actively inhibits progress on harder ones. To address this challenge, we introduce Privileged On-Policy Exploration (POPE), an approach that leverages human- or other oracle solutions as privileged information to guide exploration on hard problems, unlike methods that use oracle solutions as training targets (e.g., off-policy RL methods or warmstarting from SFT). POPE augments hard problems with prefixes of oracle solutions, enabling RL to obtain non-zero rewards during guided rollouts. Crucially, the resulting behaviors transfer back to the original, unguided problems through a synergy between instruction-following and reasoning. Empirically, POPE expands the set of solvable problems and substantially improves performance on challenging reasoning benchmarks.", "AI": {"tldr": "POPE is a reinforcement learning method that uses oracle solution prefixes as privileged information to guide exploration on hard problems, enabling LLMs to get non-zero rewards and learn effectively, while avoiding the issues of ray interference from mixing easy/hard problems.", "motivation": "On-policy RL for LLMs struggles with hard problems due to zero-reward rollouts and lack of learning signal. Classical RL fixes don't work, and mixing easy/hard problems causes \"ray interference\" that inhibits progress on hard problems.", "method": "Privileged On-Policy Exploration (POPE) augments hard problems with oracle solution prefixes to guide rollouts and obtain non-zero rewards. Unlike off-policy methods, it uses privileged information only for exploration, not training targets.", "result": "POPE expands the set of solvable problems and substantially improves performance on challenging reasoning benchmarks by enabling effective exploration and transferring learned behaviors back to unguided problems.", "conclusion": "POPE successfully addresses the exploration-exploitation challenge in RL for LLMs by leveraging privileged information without suffering from ray interference, demonstrating that guided exploration can substantially improve solvability on hard reasoning tasks."}}
{"id": "2601.18564", "categories": ["cs.LG", "cs.CV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.18564", "abs": "https://arxiv.org/abs/2601.18564", "authors": ["Chong Hyun Lee", "Kibae Lee", "Hyun Hee Yim"], "title": "An Unsupervised Tensor-Based Domain Alignment", "comment": "5 pages, 5 figures", "summary": "We propose a tensor-based domain alignment (DA) algorithm designed to align source and target tensors within an invariant subspace through the use of alignment matrices. These matrices along with the subspace undergo iterative optimization of which constraint is on oblique manifold, which offers greater flexibility and adaptability compared to the traditional Stiefel manifold. Moreover, regularization terms defined to preserve the variance of both source and target tensors, ensures robust performance. Our framework is versatile, effectively generalizing existing tensor-based DA methods as special cases. Through extensive experiments, we demonstrate that our approach not only enhances DA conversion speed but also significantly boosts classification accuracy. This positions our method as superior to current state-of-the-art techniques, making it a preferable choice for complex domain adaptation tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f20\u91cf\u7684\u57df\u5bf9\u9f50\u7b97\u6cd5\uff0c\u901a\u8fc7\u5728\u659c\u6d41\u5f62\u4e0a\u8fed\u4ee3\u4f18\u5316\u5bf9\u9f50\u77e9\u9635\u548c\u4e0d\u53d8\u5b50\u7a7a\u95f4\uff0c\u5e76\u5f15\u5165\u4fdd\u6301\u65b9\u5dee\u7684\u89c4\u6574\u9879\uff0c\u5b9e\u73b0\u66f4\u5feb\u901f\u3001\u66f4\u51c6\u786e\u7684\u57df\u9002\u5e94\u3002", "motivation": "\u4f20\u7edfStiefel\u6d41\u5f62\u7ea6\u675f\u7075\u6d3b\u6027\u4e0d\u8db3\uff0c\u96be\u4ee5\u6709\u6548\u5e94\u5bf9\u590d\u6742\u57df\u9002\u5e94\u4efb\u52a1\u7684\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1\u5f20\u91cf\u57df\u5bf9\u9f50\u7b97\u6cd5\uff0c\u5229\u7528\u5bf9\u9f50\u77e9\u9635\u5728\u4e0d\u53d8\u5b50\u7a7a\u95f4\u5185\u914d\u51c6\u6e90\u548c\u76ee\u6807\u5f20\u91cf\uff0c\u5728\u659c\u6d41\u5f62\u4e0a\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\uff0c\u5e76\u5b9a\u4e49\u4fdd\u6301\u65b9\u5dee\u7684\u89c4\u6574\u9879\u786e\u4fdd\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u57df\u5bf9\u9f50\u8f6c\u6362\u901f\u5ea6\uff0c\u8fd8\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u5177\u6709\u901a\u7528\u6027\uff0c\u53ef\u5c06\u73b0\u6709\u5f20\u91cf\u57df\u5bf9\u9f50\u65b9\u6cd5\u6cdb\u5316\u4e3a\u7279\u4f8b\uff0c\u662f\u590d\u6742\u57df\u9002\u5e94\u4efb\u52a1\u7684\u4f18\u9009\u65b9\u6848\u3002"}}
{"id": "2601.18783", "categories": ["cs.LG", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.18783", "abs": "https://arxiv.org/abs/2601.18783", "authors": ["Deepthi Pathare", "Leo Laine", "Morteza Haghir Chehreghani"], "title": "Multi-Objective Reinforcement Learning for Efficient Tactical Decision Making for Trucks in Highway Traffic", "comment": null, "summary": "Balancing safety, efficiency, and operational costs in highway driving poses a challenging decision-making problem for heavy-duty vehicles. A central difficulty is that conventional scalar reward formulations, obtained by aggregating these competing objectives, often obscure the structure of their trade-offs. We present a Proximal Policy Optimization based multi-objective reinforcement learning framework that learns a continuous set of policies explicitly representing these trade-offs and evaluates it on a scalable simulation platform for tactical decision making in trucks. The proposed approach learns a continuous set of Pareto-optimal policies that capture the trade-offs among three conflicting objectives: safety, quantified in terms of collisions and successful completion; energy efficiency and time efficiency, quantified using energy cost and driver cost, respectively. The resulting Pareto frontier is smooth and interpretable, enabling flexibility in choosing driving behavior along different conflicting objectives. This framework allows seamless transitions between different driving policies without retraining, yielding a robust and adaptive decision-making strategy for autonomous trucking applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u7684\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4e3a\u91cd\u578b\u5361\u8f66\u5728\u9ad8\u901f\u516c\u8def\u573a\u666f\u4e0b\u5b66\u4e60\u8fde\u7eed\u5e15\u7d2f\u6258\u6700\u4f18\u7b56\u7565\u96c6\uff0c\u5b9e\u73b0\u5b89\u5168\u6027\u3001\u80fd\u6548\u548c\u65f6\u95f4\u6210\u672c\u7684\u5e73\u8861\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u7075\u6d3b\u5207\u6362\u9a7e\u9a76\u7b56\u7565\u3002", "motivation": "\u91cd\u578b\u8f66\u8f86\u5728\u9ad8\u901f\u516c\u8def\u9a7e\u9a76\u4e2d\u9700\u8981\u5e73\u8861\u5b89\u5168\u6027\u3001\u6548\u7387\u548c\u8fd0\u8425\u6210\u672c\u7b49\u591a\u4e2a\u51b2\u7a81\u76ee\u6807\uff0c\u4f46\u4f20\u7edf\u7684\u6807\u91cf\u5956\u52b1\u51fd\u6570\u4f1a\u63a9\u76d6\u8fd9\u4e9b\u76ee\u6807\u95f4\u7684\u6743\u8861\u7ed3\u6784\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u7684\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u53ef\u6269\u5c55\u7684\u5361\u8f66\u6218\u672f\u51b3\u7b56\u4eff\u771f\u5e73\u53f0\u4e0a\uff0c\u5b66\u4e60\u80fd\u660e\u786e\u8868\u793a\u76ee\u6807\u6743\u8861\u7684\u8fde\u7eed\u7b56\u7565\u96c6\u5408\u3002", "result": "\u5b66\u4e60\u5230\u5305\u542b\u5b89\u5168\u6027\uff08\u78b0\u649e\u4e0e\u4efb\u52a1\u5b8c\u6210\uff09\u3001\u80fd\u6548\uff08\u80fd\u8017\u6210\u672c\uff09\u548c\u65f6\u95f4\u6548\u7387\uff08\u53f8\u673a\u6210\u672c\uff09\u4e09\u4e2a\u51b2\u7a81\u76ee\u6807\u7684\u8fde\u7eed\u5e15\u7d2f\u6258\u6700\u4f18\u7b56\u7565\u96c6\uff0c\u5f62\u6210\u5e73\u6ed1\u53ef\u89e3\u91ca\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u5728\u4e0d\u540c\u9a7e\u9a76\u7b56\u7565\u95f4\u65e0\u7f1d\u5207\u6362\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5361\u8f66\u63d0\u4f9b\u4e86\u9c81\u68d2\u81ea\u9002\u5e94\u7684\u51b3\u7b56\u7b56\u7565\u3002"}}
{"id": "2601.18580", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18580", "abs": "https://arxiv.org/abs/2601.18580", "authors": ["Vincenzo De Paola", "Mirco Mutti", "Riccardo Zamboni", "Marcello Restelli"], "title": "K-Myriad: Jump-starting reinforcement learning with unsupervised parallel agents", "comment": null, "summary": "Parallelization in Reinforcement Learning is typically employed to speed up the training of a single policy, where multiple workers collect experience from an identical sampling distribution. This common design limits the potential of parallelization by neglecting the advantages of diverse exploration strategies. We propose K-Myriad, a scalable and unsupervised method that maximizes the collective state entropy induced by a population of parallel policies. By cultivating a portfolio of specialized exploration strategies, K-Myriad provides a robust initialization for Reinforcement Learning, leading to both higher training efficiency and the discovery of heterogeneous solutions. Experiments on high-dimensional continuous control tasks, with large-scale parallelization, demonstrate that K-Myriad can learn a broad set of distinct policies, highlighting its effectiveness for collective exploration and paving the way towards novel parallelization strategies.", "AI": {"tldr": "K-Myriad is a scalable unsupervised method that uses parallel policies to maximize collective state entropy, creating diverse exploration strategies for better RL initialization and heterogeneous solutions.", "motivation": "Traditional parallel RL uses identical workers for single-policy speedup, ignoring the potential of diverse exploration strategies among parallel agents.", "method": "Proposes K-Myriad to maximize collective state entropy from a population of parallel policies, cultivating specialized exploration strategies without supervision.", "result": "Demonstrates higher training efficiency and discovery of heterogeneous solutions on high-dimensional continuous control tasks with large-scale parallelization.", "conclusion": "K-Myriad effectively enables collective exploration by learning diverse policies, advancing novel parallelization strategies in RL."}}
{"id": "2601.18795", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18795", "abs": "https://arxiv.org/abs/2601.18795", "authors": ["Amrith Setlur", "Zijian Wang", "Andrew Cohen", "Paria Rashidinejad", "Sang Michael Xie"], "title": "Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes", "comment": null, "summary": "Typical reinforcement learning (RL) methods for LLM reasoning waste compute on hard problems, where correct on-policy traces are rare, policy gradients vanish, and learning stalls. To bootstrap more efficient RL, we consider reusing old sampling FLOPs (from prior inference or RL training) in the form of off-policy traces. Standard off-policy methods supervise against off-policy data, causing instabilities during RL optimization. We introduce PrefixRL, where we condition on the prefix of successful off-policy traces and run on-policy RL to complete them, side-stepping off-policy instabilities. PrefixRL boosts the learning signal on hard problems by modulating the difficulty of the problem through the off-policy prefix length. We prove that the PrefixRL objective is not only consistent with the standard RL objective but also more sample efficient. Empirically, we discover back-generalization: training only on prefixed problems generalizes to out-of-distribution unprefixed performance, with learned strategies often differing from those in the prefix. In our experiments, we source the off-policy traces by rejection sampling with the base model, creating a self-improvement loop. On hard reasoning problems, PrefixRL reaches the same training reward 2x faster than the strongest baseline (SFT on off-policy data then RL), even after accounting for the compute spent on the initial rejection sampling, and increases the final reward by 3x. The gains transfer to held-out benchmarks, and PrefixRL is still effective when off-policy traces are derived from a different model family, validating its flexibility in practical settings.", "AI": {"tldr": "PrefixRL is a novel reinforcement learning method that efficiently bootstraps LLM reasoning by conditioning on prefixes of successful off-policy traces, avoiding instability while achieving 2x faster training and 3x higher final rewards on hard problems.", "motivation": "Standard RL for LLM reasoning is inefficient on hard problems due to rare correct traces, vanishing gradients, and stalled learning; reusing off-policy traces from prior computation could improve efficiency but causes optimization instability.", "method": "PrefixRL conditions on prefixes of successful off-policy traces and runs on-policy RL to complete them, modulating problem difficulty by prefix length; theoretically proven consistent with standard RL and more sample-efficient.", "result": "Achieves 2x faster training to same reward and 3x higher final reward vs. strongest baseline; discovers back-generalization where prefixed training transfers to unprefixed OOD problems; effective across model families.", "conclusion": "PrefixRL successfully leverages off-policy computation for stable, efficient RL on hard reasoning problems with strong generalization and practical flexibility."}}
{"id": "2601.18604", "categories": ["cs.LG", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2601.18604", "abs": "https://arxiv.org/abs/2601.18604", "authors": ["Zhiwei Zheng", "Kevin Bryson"], "title": "LaCoGSEA: Unsupervised deep learning for pathway analysis via latent correlation", "comment": null, "summary": "Motivation: Pathway enrichment analysis is widely used to interpret gene expression data. Standard approaches, such as GSEA, rely on predefined phenotypic labels and pairwise comparisons, which limits their applicability in unsupervised settings. Existing unsupervised extensions, including single-sample methods, provide pathway-level summaries but primarily capture linear relationships and do not explicitly model gene-pathway associations. More recently, deep learning models have been explored to capture non-linear transcriptomic structure. However, their interpretation has typically relied on generic explainable AI (XAI) techniques designed for feature-level attribution. As these methods are not designed for pathway-level interpretation in unsupervised transcriptomic analyses, their effectiveness in this setting remains limited.\n  Results: To bridge this gap, we introduce LaCoGSEA (Latent Correlation GSEA), an unsupervised framework that integrates deep representation learning with robust pathway statistics. LaCoGSEA employs an autoencoder to capture non-linear manifolds and proposes a global gene-latent correlation metric as a proxy for differential expression, generating dense gene rankings without prior labels. We demonstrate that LaCoGSEA offers three key advantages: (i) it achieves improved clustering performance in distinguishing cancer subtypes compared to existing unsupervised baselines; (ii) it recovers a broader range of biologically meaningful pathways at higher ranks compared with linear dimensionality reduction and gradient-based XAI methods; and (iii) it maintains high robustness and consistency across varying experimental protocols and dataset sizes. Overall, LaCoGSEA provides state-of-the-art performance in unsupervised pathway enrichment analysis.\n  Availability and implementation: https://github.com/willyzzz/LaCoGSEA", "AI": {"tldr": "LaCoGSEA is an unsupervised pathway enrichment framework that combines autoencoders with a novel gene-latent correlation metric to identify biologically meaningful pathways without needing phenotypic labels, outperforming existing methods in clustering, pathway recovery, and robustness.", "motivation": "Standard pathway enrichment analysis methods require predefined phenotypic labels and pairwise comparisons, limiting their use in unsupervised scenarios. Existing unsupervised extensions primarily capture linear relationships and don't explicitly model gene-pathway associations. While deep learning models can capture non-linear transcriptomic structure, their interpretation relies on generic explainable AI techniques not designed for pathway-level interpretation in unsupervised transcriptomic analyses.", "method": "LaCoGSEA (Latent Correlation GSEA) integrates deep representation learning with robust pathway statistics. It employs an autoencoder to capture non-linear manifolds in gene expression data and proposes a global gene-latent correlation metric as a proxy for differential expression to generate dense gene rankings without prior labels.", "result": "LaCoGSEA demonstrates three key advantages: (i) improved clustering performance in distinguishing cancer subtypes compared to existing unsupervised baselines; (ii) recovery of a broader range of biologically meaningful pathways at higher ranks compared with linear dimensionality reduction and gradient-based XAI methods; and (iii) high robustness and consistency across varying experimental protocols and dataset sizes, achieving state-of-the-art performance in unsupervised pathway enrichment analysis.", "conclusion": "LaCoGSEA provides a superior unsupervised framework for pathway enrichment analysis by effectively capturing non-linear transcriptomic structure and explicitly modeling gene-pathway associations through a novel latent correlation approach, offering improved performance, biological relevance, and robustness over existing methods."}}
{"id": "2601.18615", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18615", "abs": "https://arxiv.org/abs/2601.18615", "authors": ["Ramiro Valdes Jara", "Adam Meyers"], "title": "Geometry-Free Conditional Diffusion Modeling for Solving the Inverse Electrocardiography Problem", "comment": null, "summary": "This paper proposes a data-driven model for solving the inverse problem of electrocardiography, the mathematical problem that forms the basis of electrocardiographic imaging (ECGI). We present a conditional diffusion framework that learns a probabilistic mapping from noisy body surface signals to heart surface electric potentials. The proposed approach leverages the generative nature of diffusion models to capture the non-unique and underdetermined nature of the ECGI inverse problem, enabling probabilistic sampling of multiple reconstructions rather than a single deterministic estimate. Unlike traditional methods, the proposed framework is geometry-free and purely data-driven, alleviating the need for patient-specific mesh construction. We evaluate the method on a real ECGI dataset and compare it against strong deterministic baselines, including a convolutional neural network, long short-term memory network, and transformer-based model. The results demonstrate that the proposed diffusion approach achieves improved reconstruction accuracy, highlighting the potential of diffusion models as a robust tool for noninvasive cardiac electrophysiology imaging.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u51e0\u4f55\u65e0\u5173\u3001\u6570\u636e\u9a71\u52a8\u7684\u9006\u95ee\u9898\u6c42\u89e3\u6846\u67b6\uff0c\u7528\u4e8e\u5fc3\u7535\u56fe\u6210\u50cf(ECGI)\uff0c\u53ef\u6982\u7387\u6027\u5730\u751f\u6210\u591a\u4e2a\u91cd\u5efa\u7ed3\u679c\u800c\u975e\u5355\u4e00\u786e\u5b9a\u4f30\u8ba1\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u4f18\u4e8eCNN/LSTM/Transformer\u7b49\u786e\u5b9a\u6027\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "ECGI\u9006\u95ee\u9898\u662f\u6b20\u5b9a\u7684\u3001\u975e\u552f\u4e00\u6027\u7684\u6570\u5b66\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u60a3\u8005\u7279\u5f02\u6027\u7f51\u683c\u6784\u5efa\uff0c\u4e14\u73b0\u6709\u786e\u5b9a\u6027\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u89e3\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u91c7\u7528\u6761\u4ef6\u6269\u6563\u6846\u67b6\u5b66\u4e60\u4ece\u566a\u58f0\u4f53\u8868\u4fe1\u53f7\u5230\u5fc3\u8868\u7535\u4f4d\u7684\u6982\u7387\u6620\u5c04\uff0c\u5b9e\u73b0\u51e0\u4f55\u65e0\u5173\u3001\u7eaf\u6570\u636e\u9a71\u52a8\u7684\u5efa\u6a21\uff0c\u652f\u6301\u591a\u91cd\u5efa\u7ed3\u679c\u91c7\u6837\u3002", "result": "\u5728\u771f\u5b9eECGI\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4CNN\u3001LSTM\u548cTransformer\u7b49\u5f3a\u786e\u5b9a\u6027\u57fa\u7ebf\uff0c\u8be5\u6269\u6563\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u91cd\u5efa\u7cbe\u5ea6\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u65e0\u521b\u5fc3\u810f\u7535\u751f\u7406\u6210\u50cf\u7684\u9c81\u68d2\u5de5\u5177\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4e3aECGI\u63d0\u4f9b\u4e86\u65b0\u7684\u6982\u7387\u6027\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18620", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18620", "abs": "https://arxiv.org/abs/2601.18620", "authors": ["Panagiotis Lymperopoulos", "Abhiramon Rajasekharan", "Ian Berlot-Attwell", "St\u00e9phane Aroca-Ouellette", "Kaheer Suleman"], "title": "CASSANDRA: Programmatic and Probabilistic Learning and Inference for Stochastic World Modeling", "comment": "28 pages, 2 figures", "summary": "Building world models is essential for planning in real-world domains such as businesses. Since such domains have rich semantics, we can leverage world knowledge to effectively model complex action effects and causal relationships from limited data. In this work, we propose CASSANDRA, a neurosymbolic world modeling approach that leverages an LLM as a knowledge prior to construct lightweight transition models for planning. CASSANDRA integrates two components: (1) LLM-synthesized code to model deterministic features, and (2) LLM-guided structure learning of a probabilistic graphical model to capture causal relationships among stochastic variables. We evaluate CASSANDRA in (i) a small-scale coffee-shop simulator and (ii) a complex theme park business simulator, where we demonstrate significant improvements in transition prediction and planning over baselines.", "AI": {"tldr": "CASSANDRA is a neurosymbolic world modeling approach that uses LLMs as knowledge priors to build lightweight transition models for planning in business domains, combining LLM-synthesized code for deterministic features with LLM-guided probabilistic graphical models for causal relationships, showing significant improvements over baselines in coffee-shop and theme park simulators.", "motivation": "Building world models for real-world planning is challenging due to rich semantics and limited data; traditional methods struggle to model complex action effects and causal relationships, but leveraging world knowledge from LLMs can overcome these limitations.", "method": "CASSANDRA integrates two components: (1) LLM-synthesized code to model deterministic features, and (2) LLM-guided structure learning of a probabilistic graphical model to capture causal relationships among stochastic variables, combining symbolic reasoning with neural networks.", "result": "Evaluated on a small-scale coffee-shop simulator and a complex theme park business simulator, CASSANDRA demonstrates significant improvements in transition prediction and planning performance over baseline methods.", "conclusion": "The neurosymbolic approach effectively leverages LLM knowledge priors to build lightweight, accurate world models for planning in complex domains, showing promise for real-world business applications."}}
{"id": "2601.18640", "categories": ["cs.LG", "q-bio.MN"], "pdf": "https://arxiv.org/pdf/2601.18640", "abs": "https://arxiv.org/abs/2601.18640", "authors": ["Zhiwei Zheng", "Kevin Bryson"], "title": "TwinPurify: Purifying gene expression data to reveal tumor-intrinsic transcriptional programs via self-supervised learning", "comment": null, "summary": "Advances in single-cell and spatial transcriptomic technologies have transformed tumor ecosystem profiling at cellular resolution. However, large scale studies on patient cohorts continue to rely on bulk transcriptomic data, where variation in tumor purity obscures tumor-intrinsic transcriptional signals and constrains downstream discovery. Many deconvolution methods report strong performance on synthetic bulk mixtures but fail to generalize to real patient cohorts because of unmodeled biological and technical variation.\n  Here, we introduce TwinPurify, a representation learning framework that adapts the Barlow Twins self-supervised objective, representing a fundamental departure from the deconvolution paradigm. Rather than resolving the bulk mixture into discrete cell-type fractions, TwinPurify instead learns continuous, high-dimensional tumor embeddings by leveraging adjacent-normal profiles within the same cohort as \"background\" guidance, enabling the disentanglement of tumor-specific signals without relying on any external reference.\n  Benchmarked against multiple large cancer cohorts across RNA-seq and microarray platforms, TwinPurify outperforms conventional representation learning baselines like auto-encoders in recovering tumor-intrinsic and immune signals. The purified embeddings improve molecular subtype and grade classification, enhance survival model concordance, and uncover biologically meaningful pathway activities compared to raw bulk profiles. By providing a transferable framework for decontaminating bulk transcriptomics, TwinPurify extends the utility of existing clinical datasets for molecular discovery.", "AI": {"tldr": "TwinPurify uses self-supervised learning (Barlow Twins) with adjacent-normal samples as guidance to learn tumor-specific embeddings from bulk transcriptomics, outperforming traditional deconvolution methods and improving downstream analysis without needing external references.", "motivation": "\u5355\u7ec6\u80de\u548c\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u6280\u672f\u867d\u7136\u5148\u8fdb\uff0c\u4f46\u5927\u89c4\u6a21\u4e34\u5e8a\u7814\u7a76\u4ecd\u4f9d\u8d56\u6279\u91cf\u8f6c\u5f55\u7ec4\u6570\u636e\u3002\u7136\u800c\uff0c\u80bf\u7624\u7eaf\u5ea6\u7684\u53d8\u5316\u63a9\u76d6\u4e86\u80bf\u7624\u5185\u5728\u7684\u8f6c\u5f55\u4fe1\u53f7\uff0c\u9650\u5236\u4e86\u4e0b\u6e38\u53d1\u73b0\u3002\u8bb8\u591a\u53cd\u5377\u79ef\u65b9\u6cd5\u5728\u5408\u6210\u6570\u636e\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7531\u4e8e\u672a\u5efa\u6a21\u7684\u751f\u7269\u548c\u6280\u672f\u53d8\u5f02\uff0c\u65e0\u6cd5\u63a8\u5e7f\u5230\u771f\u5b9e\u60a3\u8005\u961f\u5217\u3002", "method": "TwinPurify\u91c7\u7528Barlow Twins\u81ea\u76d1\u7763\u5b66\u4e60\u76ee\u6807\uff0c\u5b66\u4e60\u8fde\u7eed\u7684\u9ad8\u7ef4\u80bf\u7624\u5d4c\u5165\u8868\u793a\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u540c\u961f\u5217\u4e2d\u7684\u764c\u65c1\u6b63\u5e38\u6837\u672c\u4f5c\u4e3a\"\u80cc\u666f\"\u6307\u5bfc\uff0c\u65e0\u9700\u5916\u90e8\u53c2\u8003\u5373\u53ef\u89e3\u79bb\u80bf\u7624\u7279\u5f02\u6027\u4fe1\u53f7\uff0c\u4ece\u6839\u672c\u4e0a\u4e0d\u540c\u4e8e\u4f20\u7edf\u7684\u53cd\u5377\u79ef\u8303\u5f0f\u3002", "result": "\u5728\u591a\u4e2a\u5927\u578b\u764c\u75c7\u961f\u5217\u548c\u4e0d\u540c\u5e73\u53f0\uff08RNA-seq\u548c\u82af\u7247\uff09\u4e0a\uff0cTwinPurify\u5728\u6062\u590d\u80bf\u7624\u5185\u5728\u4fe1\u53f7\u548c\u514d\u75ab\u4fe1\u53f7\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff08\u5982\u81ea\u7f16\u7801\u5668\uff09\u3002\u7eaf\u5316\u7684\u5d4c\u5165\u8868\u793a\u6539\u5584\u4e86\u5206\u5b50\u4e9a\u578b\u548c\u5206\u7ea7\u5206\u7c7b\uff0c\u589e\u5f3a\u4e86\u751f\u5b58\u6a21\u578b\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u66f4\u591a\u751f\u7269\u5b66\u610f\u4e49\u7684\u901a\u8def\u6d3b\u6027\u3002", "conclusion": "TwinPurify\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u8fc1\u79fb\u7684\u6846\u67b6\uff0c\u7528\u4e8e\"\u51c0\u5316\"\u6279\u91cf\u8f6c\u5f55\u7ec4\u6570\u636e\uff0c\u6269\u5c55\u4e86\u73b0\u6709\u4e34\u5e8a\u6570\u636e\u96c6\u5728\u5206\u5b50\u53d1\u73b0\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2601.18676", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18676", "abs": "https://arxiv.org/abs/2601.18676", "authors": ["Miles Martinez", "Alex H. Williams"], "title": "Quasi Monte Carlo methods enable extremely low-dimensional deep generative models", "comment": null, "summary": "This paper introduces quasi-Monte Carlo latent variable models (QLVMs): a class of deep generative models that are specialized for finding extremely low-dimensional and interpretable embeddings of high-dimensional datasets. Unlike standard approaches, which rely on a learned encoder and variational lower bounds, QLVMs directly approximate the marginal likelihood by randomized quasi-Monte Carlo integration. While this brute force approach has drawbacks in higher-dimensional spaces, we find that it excels in fitting one, two, and three dimensional deep latent variable models. Empirical results on a range of datasets show that QLVMs consistently outperform conventional variational autoencoders (VAEs) and importance weighted autoencoders (IWAEs) with matched latent dimensionality. The resulting embeddings enable transparent visualization and post hoc analyses such as nonparametric density estimation, clustering, and geodesic path computation, which are nontrivial to validate in higher-dimensional spaces. While our approach is compute-intensive and struggles to generate fine-scale details in complex datasets, it offers a compelling solution for applications prioritizing interpretability and latent space analysis.", "AI": {"tldr": "\u63d0\u51fa\u51c6\u8499\u7279\u5361\u6d1b\u6f5c\u53d8\u91cf\u6a21\u578b(QLVMs)\uff0c\u901a\u8fc7\u968f\u673a\u51c6\u8499\u7279\u5361\u6d1b\u79ef\u5206\u76f4\u63a5\u8fd1\u4f3c\u8fb9\u9645\u4f3c\u7136\uff0c\u57281-3\u7ef4\u6f5c\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u9ad8\u7ef4\u6570\u636e\u7684\u53ef\u89e3\u91ca\u5d4c\u5165\uff0c\u6027\u80fd\u4f18\u4e8eVAEs/IWAEs\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "motivation": "\u6807\u51c6\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u4f9d\u8d56\u7f16\u7801\u5668\u548c\u53d8\u5206\u4e0b\u754c\uff0c\u96be\u4ee5\u83b7\u5f97\u6781\u4f4e\u7ef4\u4e14\u53ef\u89e3\u91ca\u7684\u6f5c\u7a7a\u95f4\u3002QLVMs\u65e8\u5728\u901a\u8fc7\u76f4\u63a5\u4f18\u5316\u8fb9\u9645\u4f3c\u7136\uff0c\u5b9e\u73b0\u900f\u660e\u53ef\u89c6\u5316\u548c\u540e\u9a8c\u5206\u6790\uff0c\u89e3\u51b3\u9ad8\u7ef4\u6570\u636e\u4f4e\u7ef4\u5d4c\u5165\u7684\u53ef\u89e3\u91ca\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u968f\u673a\u51c6\u8499\u7279\u5361\u6d1b\u79ef\u5206\u76f4\u63a5\u8fd1\u4f3c\u8fb9\u9645\u4f3c\u7136\uff0c\u907f\u514d\u53d8\u5206\u4e0b\u754c\u548c\u7f16\u7801\u5668\u5b66\u4e60\u7684\u9650\u5236\uff0c\u4e13\u6ce8\u4e8e1-3\u7ef4\u6f5c\u7a7a\u95f4\u5efa\u6a21\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cQLVMs\u5728\u5339\u914d\u6f5c\u7ef4\u5ea6\u4e0b\u6301\u7eed\u4f18\u4e8eVAEs\u548cIWAEs\uff0c\u652f\u6301\u975e\u53c2\u6570\u5bc6\u5ea6\u4f30\u8ba1\u3001\u805a\u7c7b\u548c\u6d4b\u5730\u7ebf\u8def\u5f84\u8ba1\u7b97\u7b49\u53ef\u89e3\u91ca\u5206\u6790\u3002", "conclusion": "QLVMs\u4e3a\u91cd\u89c6\u53ef\u89e3\u91ca\u6027\u548c\u6f5c\u7a7a\u95f4\u5206\u6790\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u867d\u8ba1\u7b97\u5bc6\u96c6\u4e14\u5728\u590d\u6742\u6570\u636e\u4e0a\u96be\u4ee5\u6355\u6349\u7ec6\u8282\uff0c\u4f46\u5728\u4f4e\u7ef4\u573a\u666f\u4e0b\u4f18\u52bf\u663e\u8457\u3002"}}
{"id": "2601.18678", "categories": ["cs.LG", "cs.CV", "cs.HC", "math.DG"], "pdf": "https://arxiv.org/pdf/2601.18678", "abs": "https://arxiv.org/abs/2601.18678", "authors": ["Eslam Zaher", "Maciej Trzaskowski", "Quan Nguyen", "Fred Roosta"], "title": "Counterfactual Explanations on Robust Perceptual Geodesics", "comment": "Accepted at ICLR 2026", "summary": "Latent-space optimization methods for counterfactual explanations - framed as minimal semantic perturbations that change model predictions - inherit the ambiguity of Wachter et al.'s objective: the choice of distance metric dictates whether perturbations are meaningful or adversarial. Existing approaches adopt flat or misaligned geometries, leading to off-manifold artifacts, semantic drift, or adversarial collapse. We introduce Perceptual Counterfactual Geodesics (PCG), a method that constructs counterfactuals by tracing geodesics under a perceptually Riemannian metric induced from robust vision features. This geometry aligns with human perception and penalizes brittle directions, enabling smooth, on-manifold, semantically valid transitions. Experiments on three vision datasets show that PCG outperforms baselines and reveals failure modes hidden under standard metrics.", "AI": {"tldr": "PCG\u65b9\u6cd5\u901a\u8fc7\u611f\u77e5\u9ece\u66fc\u51e0\u4f55\u6784\u5efa\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u5b9e\u73b0\u8bed\u4e49\u6709\u6548\u4e14\u6d41\u5f62\u4e0a\u7684\u5e73\u6ed1\u8fc7\u6e21\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\u56e0\u8ddd\u79bb\u5ea6\u91cf\u9009\u62e9\u5bfc\u81f4\u6b67\u4e49\uff0c\u4ea7\u751f\u975e\u8bed\u4e49\u6216\u5bf9\u6297\u6027\u6270\u52a8\uff0c\u5b58\u5728\u79bb\u6d41\u5f62\u4f2a\u5f71\u3001\u8bed\u4e49\u6f02\u79fb\u6216\u5bf9\u6297\u6027\u5d29\u6e83\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u611f\u77e5\u53cd\u4e8b\u5b9e\u6d4b\u5730\u7ebf\uff08PCG\uff09\uff0c\u5229\u7528\u9c81\u68d2\u89c6\u89c9\u7279\u5f81\u8bf1\u5bfc\u7684\u611f\u77e5\u9ece\u66fc\u5ea6\u91cf\u6765\u8ffd\u8e2a\u6d4b\u5730\u7ebf\u6784\u5efa\u53cd\u4e8b\u5b9e\u3002", "result": "\u5728\u4e09\u4e2a\u89c6\u89c9\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPCG\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u6807\u51c6\u5ea6\u91cf\u4e0b\u9690\u85cf\u7684\u9519\u8bef\u6a21\u5f0f\u3002", "conclusion": "PCG\u901a\u8fc7\u4eba\u7c7b\u611f\u77e5\u5bf9\u9f50\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u60e9\u7f5a\u8106\u5f31\u65b9\u5411\uff0c\u5b9e\u73b0\u5e73\u6ed1\u3001\u6d41\u5f62\u4e0a\u3001\u8bed\u4e49\u6709\u6548\u7684\u8f6c\u6362\u3002"}}
{"id": "2601.18696", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18696", "abs": "https://arxiv.org/abs/2601.18696", "authors": ["Paul Whitten", "Francis Wolff", "Chris Papachristou"], "title": "Explainability Methods for Hardware Trojan Detection: A Systematic Comparison", "comment": null, "summary": "Hardware trojan detection requires accurate identification and interpretable explanations for security engineers to validate and act on results. This work compares three explainability categories for gate-level trojan detection on the Trust-Hub benchmark: (1) domain-aware property-based analysis of 31 circuit-specific features from gate fanin patterns, flip-flop distances, and I/O connectivity; (2) case-based reasoning using k-nearest neighbors for precedent-based explanations; and (3) model-agnostic feature attribution (LIME, SHAP, gradient).\n  Results show different advantages per approach. Property-based analysis provides explanations through circuit concepts like \"high fanin complexity near outputs indicates potential triggers.\" Case-based reasoning achieves 97.4% correspondence between predictions and training exemplars, offering justifications grounded in precedent. LIME and SHAP provide feature attributions with strong inter-method correlation (r=0.94, p<0.001) but lack circuit-level context for validation.\n  XGBoost classification achieves 46.15% precision and 52.17% recall on 11,392 test samples, a 9-fold precision improvement over prior work (Hasegawa et al.: 5.13%) while reducing false positive rates from 5.6% to 0.25%. Gradient-based attribution runs 481 times faster than SHAP but provides similar domain-opaque insights.\n  This work demonstrates that property-based and case-based approaches offer domain alignment and precedent-based interpretability compared to generic feature rankings, with implications for XAI deployment where practitioners must validate ML predictions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6bd4\u8f83\u4e86\u4e09\u79cd\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff08\u9886\u57df\u611f\u77e5\u7684\u5c5e\u6027\u5206\u6790\u3001\u57fa\u4e8e\u6848\u4f8b\u7684\u63a8\u7406\u548c\u6a21\u578b\u65e0\u5173\u7684\u7279\u5f81\u5f52\u56e0\uff09\u7528\u4e8e\u95e8\u7ea7\u786c\u4ef6\u6728\u9a6c\u68c0\u6d4b\uff0c\u53d1\u73b0\u9886\u57df\u5bf9\u9f50\u7684\u65b9\u6cd5\u6bd4\u901a\u7528\u7279\u5f81\u6392\u5e8f\u66f4\u9002\u5408\u5b89\u5168\u5de5\u7a0b\u5e08\u9a8c\u8bc1ML\u9884\u6d4b\u7ed3\u679c\u3002", "motivation": "\u786c\u4ef6\u6728\u9a6c\u68c0\u6d4b\u4e0d\u4ec5\u9700\u8981\u51c6\u786e\u7684\u8bc6\u522b\uff0c\u8fd8\u9700\u8981\u53ef\u89e3\u91ca\u7684\u89e3\u91ca\uff0c\u4ee5\u4fbf\u5b89\u5168\u5de5\u7a0b\u5e08\u9a8c\u8bc1\u548c\u54cd\u5e94\u7ed3\u679c\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u7535\u8def\u7ea7\u4e0a\u4e0b\u6587\u7684\u7406\u89e3\uff0c\u96be\u4ee5\u8ba9\u9886\u57df\u4e13\u5bb6\u4fe1\u4efb\u548c\u9a8c\u8bc1\u3002", "method": "\u5728Trust-Hub\u57fa\u51c6\u4e0a\u6bd4\u8f83\u4e09\u7c7b\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff1a1) \u57fa\u4e8e31\u4e2a\u7535\u8def\u7279\u5b9a\u5c5e\u6027\uff08\u6247\u5165\u6a21\u5f0f\u3001\u89e6\u53d1\u5668\u8ddd\u79bb\u3001I/O\u8fde\u63a5\uff09\u7684\u9886\u57df\u611f\u77e5\u5206\u6790\uff1b2) \u4f7f\u7528k\u8fd1\u90bb\u7684\u57fa\u4e8e\u6848\u4f8b\u63a8\u7406\uff1b3) \u6a21\u578b\u65e0\u5173\u7684\u7279\u5f81\u5f52\u56e0\uff08LIME\u3001SHAP\u3001\u68af\u5ea6\uff09\u3002\u4f7f\u7528XGBoost\u5206\u7c7b\u5668\u8fdb\u884c\u68c0\u6d4b\u3002", "result": "XGBoost\u572811,392\u4e2a\u6d4b\u8bd5\u6837\u672c\u4e0a\u8fbe\u523046.15%\u7cbe\u786e\u7387\u548c52.17%\u53ec\u56de\u7387\uff0c\u7cbe\u786e\u7387\u8f83\u5148\u524d\u5de5\u4f5c\u63d0\u53479\u500d\uff0c\u5047\u9633\u6027\u7387\u4ece5.6%\u964d\u81f30.25%\u3002\u6848\u4f8b\u63a8\u7406\u4e0e\u8bad\u7ec3\u6837\u672c\u7684\u9884\u6d4b\u5bf9\u5e94\u7387\u8fbe97.4%\u3002LIME\u548cSHAP\u65b9\u6cd5\u95f4\u76f8\u5173\u6027\u9ad8(r=0.94)\u4f46\u7f3a\u4e4f\u7535\u8def\u7ea7\u4e0a\u4e0b\u6587\u3002\u68af\u5ea6\u5f52\u56e0\u6bd4SHAP\u5feb481\u500d\u4f46\u63d0\u4f9b\u7c7b\u4f3c\u7684\u4e0d\u900f\u660e\u6d1e\u5bdf\u3002", "conclusion": "\u4e0e\u901a\u7528\u7279\u5f81\u6392\u5e8f\u76f8\u6bd4\uff0c\u57fa\u4e8e\u5c5e\u6027\u548c\u6848\u4f8b\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u9886\u57df\u5bf9\u9f50\u548c\u57fa\u4e8e\u5148\u4f8b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u8fd9\u5bf9\u9700\u8981\u9a8c\u8bc1ML\u9884\u6d4b\u7684XAI\u90e8\u7f72\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u9886\u57df\u4e13\u5bb6\u66f4\u503e\u5411\u4e8e\u53ef\u7406\u89e3\u3001\u53ef\u9a8c\u8bc1\u7684\u89e3\u91ca\u65b9\u5f0f\u3002"}}
{"id": "2601.18699", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18699", "abs": "https://arxiv.org/abs/2601.18699", "authors": ["Olaf Yunus Laitinen Imanov"], "title": "Mechanistic Analysis of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning", "comment": "16 pages, 16 figures (6 main + 10 supplementary)", "summary": "Large language models exhibit remarkable performance across diverse tasks through pre-training and fine-tuning paradigms. However, continual fine-tuning on sequential tasks induces catastrophic forgetting, where newly acquired knowledge interferes with previously learned capabilities. Despite widespread observations of this phenomenon, the mechanistic understanding remains limited. Here, we present a comprehensive mechanistic analysis of catastrophic forgetting in transformer-based LLMs during sequential fine-tuning. Through systematic experiments across multiple model scales (109B to 400B total parameters) and task sequences, we identify three primary mechanisms driving forgetting: gradient interference in attention weights, representational drift in intermediate layers, and loss landscape flattening. We demonstrate that forgetting severity correlates strongly with task similarity (Pearson r = 0.87) and gradient alignment metrics. Our analysis reveals that approximately 15 to 23 percent of attention heads undergo severe disruption during fine-tuning, with lower layers showing greater susceptibility. These findings establish mechanistic foundations for developing targeted mitigation strategies in continual learning systems.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5728\u8fde\u7eed\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u673a\u7406\u5206\u6790\uff0c\u8bc6\u522b\u51fa\u4e09\u79cd\u5173\u952e\u673a\u5236\uff0c\u5e76\u53d1\u73b0\u9057\u5fd8\u7a0b\u5ea6\u4e0e\u4efb\u52a1\u76f8\u4f3c\u5ea6\u53ca\u68af\u5ea6\u5bf9\u9f50\u5b58\u5728\u5f3a\u76f8\u5173\u6027\u3002", "motivation": "\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5728\u8fde\u7eed\u5fae\u8c03\u65f6\u4f1a\u51fa\u73b0\u707e\u96be\u6027\u9057\u5fd8\u73b0\u8c61\uff0c\u4f46\u5176\u80cc\u540e\u7684\u673a\u7406\u7406\u89e3\u6709\u9650\uff0c\u8fd9\u9650\u5236\u4e86\u6709\u6548\u7f13\u89e3\u7b56\u7565\u7684\u5f00\u53d1\u3002", "method": "\u901a\u8fc7\u5bf9109B\u81f3400B\u53c2\u6570\u7684Transformer\u6a21\u578b\u5728\u591a\u4efb\u52a1\u5e8f\u5217\u4e0a\u8fdb\u884c\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u5206\u6790\u68af\u5ea6\u548c\u8868\u5f81\u53d8\u5316\u6765\u63ed\u793a\u9057\u5fd8\u673a\u5236\u3002", "result": "\u8bc6\u522b\u51fa\u6ce8\u610f\u529b\u6743\u91cd\u4e2d\u7684\u68af\u5ea6\u5e72\u6270\u3001\u4e2d\u95f4\u5c42\u7684\u8868\u5f81\u6f02\u79fb\u548c\u635f\u5931\u666f\u89c2\u5e73\u5766\u5316\u4e09\u79cd\u4e3b\u8981\u673a\u5236\uff1b\u9057\u5fd8\u4e25\u91cd\u7a0b\u5ea6\u4e0e\u4efb\u52a1\u76f8\u4f3c\u5ea6\u5f3a\u76f8\u5173\uff08Pearson r=0.87\uff09\uff1b\u7ea615-23%\u7684\u6ce8\u610f\u529b\u5934\u53d7\u5230\u4e25\u91cd\u7834\u574f\uff0c\u4e14\u5e95\u5c42\u66f4\u6613\u53d7\u5f71\u54cd\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u5f00\u53d1\u6301\u7eed\u5b66\u4e60\u7cfb\u7edf\u7684\u9488\u5bf9\u6027\u7f13\u89e3\u7b56\u7565\u5960\u5b9a\u4e86\u673a\u7406\u57fa\u7840\u3002"}}
{"id": "2601.18728", "categories": ["cs.LG", "math.DG", "math.OC", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.18728", "abs": "https://arxiv.org/abs/2601.18728", "authors": ["Willem Diepeveen", "Oscar Leong"], "title": "Riemannian AmbientFlow: Towards Simultaneous Manifold Learning and Generative Modeling from Corrupted Data", "comment": null, "summary": "Modern generative modeling methods have demonstrated strong performance in learning complex data distributions from clean samples. In many scientific and imaging applications, however, clean samples are unavailable, and only noisy or linearly corrupted measurements can be observed. Moreover, latent structures, such as manifold geometries, present in the data are important to extract for further downstream scientific analysis. In this work, we introduce Riemannian AmbientFlow, a framework for simultaneously learning a probabilistic generative model and the underlying, nonlinear data manifold directly from corrupted observations. Building on the variational inference framework of AmbientFlow, our approach incorporates data-driven Riemannian geometry induced by normalizing flows, enabling the extraction of manifold structure through pullback metrics and Riemannian Autoencoders. We establish theoretical guarantees showing that, under appropriate geometric regularization and measurement conditions, the learned model recovers the underlying data distribution up to a controllable error and yields a smooth, bi-Lipschitz manifold parametrization. We further show that the resulting smooth decoder can serve as a principled generative prior for inverse problems with recovery guarantees. We empirically validate our approach on low-dimensional synthetic manifolds and on MNIST.", "AI": {"tldr": "\u63d0\u51faRiemannian AmbientFlow\u6846\u67b6\uff0c\u4ece\u542b\u566a\u89c2\u6d4b\u4e2d\u540c\u65f6\u5b66\u4e60\u6982\u7387\u751f\u6210\u6a21\u578b\u548c\u975e\u7ebf\u6027\u6570\u636e\u6d41\u5f62\uff0c\u5177\u5907\u7406\u8bba\u4fdd\u8bc1\u5e76\u5728\u5408\u6210\u6570\u636e\u548cMNIST\u4e0a\u9a8c\u8bc1\u6709\u6548\u3002", "motivation": "\u79d1\u5b66\u548c\u6210\u50cf\u5e94\u7528\u4e2d\u901a\u5e38\u65e0\u6cd5\u83b7\u5f97\u5e72\u51c0\u6837\u672c\uff0c\u53ea\u80fd\u89c2\u6d4b\u5230\u542b\u566a\u6216\u7ebf\u6027\u9000\u5316\u6570\u636e\uff0c\u800c\u6570\u636e\u4e2d\u6f5c\u5728\u7684\u4f4e\u7ef4\u6d41\u5f62\u7ed3\u6784\u5bf9\u4e0b\u6e38\u5206\u6790\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u4ece\u9000\u5316\u6570\u636e\u4e2d\u540c\u65f6\u5b66\u4e60\u751f\u6210\u6a21\u578b\u548c\u6d41\u5f62\u51e0\u4f55\u3002", "method": "\u57fa\u4e8eAmbientFlow\u53d8\u5206\u63a8\u65ad\u6846\u67b6\uff0c\u5f15\u5165\u5f52\u4e00\u5316\u6d41\u9a71\u52a8\u7684\u6570\u636e\u9a71\u52a8\u9ece\u66fc\u51e0\u4f55\uff0c\u5229\u7528\u62c9\u56de\u5ea6\u91cf\u548c\u9ece\u66fc\u81ea\u7f16\u7801\u5668\u63d0\u53d6\u6d41\u5f62\u7ed3\u6784\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u5728\u9002\u5f53\u7684\u51e0\u4f55\u6b63\u5219\u5316\u548c\u6d4b\u91cf\u6761\u4ef6\u4e0b\uff0c\u53ef\u6062\u590d\u5e95\u5c42\u5206\u5e03\u81f3\u53ef\u63a7\u8bef\u5dee\uff0c\u5e76\u83b7\u5f97\u5149\u6ed1\u53ccLipschitz\u6d41\u5f62\u53c2\u6570\u5316\uff1b\u5149\u6ed1\u89e3\u7801\u5668\u53ef\u4f5c\u4e3a\u9006\u95ee\u9898\u7684\u751f\u6210\u5148\u9a8c\u5e76\u63d0\u4f9b\u6062\u590d\u4fdd\u8bc1\uff1b\u5728\u4f4e\u7ef4\u5408\u6210\u6d41\u5f62\u548cMNIST\u4e0a\u9a8c\u8bc1\u6709\u6548\u3002", "conclusion": "Riemannian AmbientFlow\u6210\u529f\u5b9e\u73b0\u4e86\u4ece\u542b\u566a\u89c2\u6d4b\u4e2d\u8054\u5408\u5b66\u4e60\u751f\u6210\u6a21\u578b\u548c\u6f5c\u5728\u6d41\u5f62\uff0c\u517c\u5177\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.18734", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18734", "abs": "https://arxiv.org/abs/2601.18734", "authors": ["Siyan Zhao", "Zhihui Xie", "Mengchen Liu", "Jing Huang", "Guan Pang", "Feiyu Chen", "Aditya Grover"], "title": "Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models", "comment": "13 pages", "summary": "Knowledge distillation improves large language model (LLM) reasoning by compressing the knowledge of a teacher LLM to train smaller LLMs. On-policy distillation advances this approach by having the student sample its own trajectories while a teacher LLM provides dense token-level supervision, addressing the distribution mismatch between training and inference in off-policy distillation methods. However, on-policy distillation typically requires a separate, often larger, teacher LLM and does not explicitly leverage ground-truth solutions available in reasoning datasets. Inspired by the intuition that a sufficiently capable LLM can rationalize external privileged reasoning traces and teach its weaker self (i.e., the version without access to privileged information), we introduce On-Policy Self-Distillation (OPSD), a framework where a single model acts as both teacher and student by conditioning on different contexts. The teacher policy conditions on privileged information (e.g., verified reasoning traces) while the student policy sees only the question; training minimizes the per-token divergence between these distributions over the student's own rollouts. We demonstrate the efficacy of our method on multiple mathematical reasoning benchmarks, achieving 4-8x token efficiency compared to reinforcement learning methods such as GRPO and superior performance over off-policy distillation methods.", "AI": {"tldr": "Proposes On-Policy Self-Distillation (OPSD), where a single LLM acts as both teacher (with privileged reasoning traces) and student (without traces) to eliminate distribution mismatch and improve token efficiency in reasoning tasks.", "motivation": "Existing on-policy distillation requires separate teacher models and fails to leverage ground-truth solutions, causing distribution mismatch and inefficiency; OPSD addresses this by enabling self-distillation using privileged information within one model.", "method": "Single model conditions on different contexts: teacher policy accesses verified reasoning traces (privileged info), student policy sees only questions; training minimizes per-token divergence between these policies over student's own rollouts.", "result": "Achieves 4-8x higher token efficiency than RL methods (e.g., GRPO) and outperforms off-policy distillation on mathematical reasoning benchmarks.", "conclusion": "OPSD eliminates need for external teachers and effectively utilizes ground-truth data, setting new efficiency and performance standards for LLM reasoning distillation."}}
{"id": "2601.18736", "categories": ["cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.18736", "abs": "https://arxiv.org/abs/2601.18736", "authors": ["Jake Lyon", "Ehsan Saeedizade", "Shamik Sengupta"], "title": "Benchmarking Machine Learning Models for IoT Malware Detection under Data Scarcity and Drift", "comment": null, "summary": "The rapid expansion of the Internet of Things (IoT) in domains such as smart cities, transportation, and industrial systems has heightened the urgency of addressing their security vulnerabilities. IoT devices often operate under limited computational resources, lack robust physical safeguards, and are deployed in heterogeneous and dynamic networks, making them prime targets for cyberattacks and malware applications. Machine learning (ML) offers a promising approach to automated malware detection and classification, but practical deployment requires models that are both effective and lightweight. The goal of this study is to investigate the effectiveness of four supervised learning models (Random Forest, LightGBM, Logistic Regression, and a Multi-Layer Perceptron) for malware detection and classification using the IoT-23 dataset. We evaluate model performance in both binary and multiclass classification tasks, assess sensitivity to training data volume, and analyze temporal robustness to simulate deployment in evolving threat landscapes. Our results show that tree-based models achieve high accuracy and generalization, even with limited training data, while performance deteriorates over time as malware diversity increases. These findings underscore the importance of adaptive, resource-efficient ML models for securing IoT systems in real-world environments.", "AI": {"tldr": "This study evaluates four machine learning models for IoT malware detection using the IoT-23 dataset, finding that tree-based models (Random Forest and LightGBM) perform best even with limited data, but their effectiveness declines over time as malware evolves, emphasizing the need for adaptive, lightweight security solutions in resource-constrained IoT environments.", "motivation": "IoT devices in smart cities, transportation, and industrial systems face urgent security vulnerabilities due to limited computational resources, lack of physical safeguards, and deployment in heterogeneous networks, making them prime targets for malware; while ML offers automated detection, practical deployment requires both effective and lightweight models.", "method": "Investigated four supervised learning models (Random Forest, LightGBM, Logistic Regression, Multi-Layer Perceptron) using the IoT-23 dataset; evaluated binary and multiclass classification performance, sensitivity to training data volume, and temporal robustness to simulate evolving threat landscapes.", "result": "Tree-based models achieved high accuracy and generalization even with limited training data; however, all models showed performance deterioration over time as malware diversity increased.", "conclusion": "Adaptive, resource-efficient ML models are crucial for securing IoT systems in real-world environments where threats continuously evolve and computational resources are constrained."}}
{"id": "2601.18760", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18760", "abs": "https://arxiv.org/abs/2601.18760", "authors": ["Henry Bell", "Lara Neubauer da Costa Schertel", "Bochu Ding", "Brandon Fain"], "title": "Beyond Preferences: Learning Alignment Principles Grounded in Human Reasons and Values", "comment": null, "summary": "A crucial consideration when developing and deploying Large Language Models (LLMs) is the human values to which these models are aligned. In the constitutional framework of alignment models are aligned to a set of principles (the constitution) specified in natural language. However, it is unclear how to fairly determine this constitution with widespread stakeholder input. In this work we propose Grounded Constitutional AI (GCAI), a unified framework for generating constitutions of principles that are representative of both users' general expectations toward AI (general principles) and their interaction-time preferences (contextual principles). We extend the Inverse Constitutional AI (ICAI) approach to generate contextual principles from human preference annotation data by leveraging human-provided \\textit{reasons} for their preferences. We supplement these contextual principles with general principles surfaced from user statements of \\textit{values} regarding AI. We show that a constitution generated by GCAI is preferred by humans over one generated through ICAI both personally, and for widespread use in governing AI behavior. Additionally participants consider the GCAI constitution to be more morally grounded, coherent, and pluralistic.", "AI": {"tldr": "\u63d0\u51faGrounded Constitutional AI (GCAI)\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u7528\u6237\u7684\u4e00\u822c\u4ef7\u503c\u89c2\u58f0\u660e\u548c\u4ea4\u4e92\u504f\u597d\u7406\u7531\uff0c\u751f\u6210\u66f4\u53d7\u4eba\u7c7b\u8ba4\u53ef\u7684AI\u5baa\u6cd5\u539f\u5219\uff0c\u4f18\u4e8e\u73b0\u6709ICAI\u65b9\u6cd5\u3002", "motivation": "\u5f00\u53d1\u90e8\u7f72\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u9700\u8981\u5bf9\u9f50\u4eba\u7c7b\u4ef7\u503c\u89c2\uff0c\u4f46\u5982\u4f55\u516c\u5e73\u5730\u901a\u8fc7\u5e7f\u6cdb\u5229\u76ca\u76f8\u5173\u8005\u8f93\u5165\u6765\u786e\u5b9a\u5baa\u6cd5\u539f\u5219\u4ecd\u4e0d\u660e\u786e\u3002", "method": "\u63d0\u51faGCAI\u6846\u67b6\uff0c\u6269\u5c55ICAI\u65b9\u6cd5\uff1a\u5229\u7528\u4eba\u7c7b\u504f\u597d\u6807\u6ce8\u6570\u636e\u4e2d\u7684\u7406\u7531\u751f\u6210\u60c5\u5883\u539f\u5219\uff0c\u5e76\u8865\u5145\u6765\u81ea\u7528\u6237AI\u4ef7\u503c\u89c2\u58f0\u660e\u7684\u901a\u7528\u539f\u5219\uff0c\u7ed3\u5408\u4e24\u8005\u751f\u6210\u5baa\u6cd5\u3002", "result": "GCAI\u751f\u6210\u7684\u5baa\u6cd5\u5728\u4e2a\u4eba\u4f7f\u7528\u548c\u5e7f\u6cdb\u5e94\u7528\u65b9\u9762\u5747\u4f18\u4e8eICAI\uff0c\u88ab\u53c2\u4e0e\u8005\u8ba4\u4e3a\u66f4\u5177\u9053\u5fb7\u57fa\u7840\u3001\u8fde\u8d2f\u6027\u548c\u591a\u5143\u6027\u3002", "conclusion": "GCAI\u4e3a\u751f\u6210AI\u5baa\u6cd5\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\uff0c\u80fd\u66f4\u597d\u4ee3\u8868\u7528\u6237\u671f\u671b\u548c\u504f\u597d\uff0c\u5b9e\u73b0\u66f4\u516c\u5e73\u3001\u66f4\u53d7\u8ba4\u53ef\u7684AI\u5bf9\u9f50\u3002"}}
{"id": "2601.18778", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18778", "abs": "https://arxiv.org/abs/2601.18778", "authors": ["Shobhita Sundaram", "John Quan", "Ariel Kwiatkowski", "Kartik Ahuja", "Yann Ollivier", "Julia Kempe"], "title": "Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability", "comment": null, "summary": "Can a model learn to escape its own learning plateau? Reinforcement learning methods for finetuning large reasoning models stall on datasets with low initial success rates, and thus little training signal. We investigate a fundamental question: Can a pretrained LLM leverage latent knowledge to generate an automated curriculum for problems it cannot solve? To explore this, we design SOAR: A self-improvement framework designed to surface these pedagogical signals through meta-RL. A teacher copy of the model proposes synthetic problems for a student copy, and is rewarded with its improvement on a small subset of hard problems. Critically, SOAR grounds the curriculum in measured student progress rather than intrinsic proxy rewards. Our study on the hardest subsets of mathematical benchmarks (0/128 success) reveals three core findings. First, we show that it is possible to realize bi-level meta-RL that unlocks learning under sparse, binary rewards by sharpening a latent capacity of pretrained models to generate useful stepping stones. Second, grounded rewards outperform intrinsic reward schemes used in prior LLM self-play, reliably avoiding the instability and diversity collapse modes they typically exhibit. Third, analyzing the generated questions reveals that structural quality and well-posedness are more critical for learning progress than solution correctness. Our results suggest that the ability to generate useful stepping stones does not require the preexisting ability to actually solve the hard problems, paving a principled path to escape reasoning plateaus without additional curated data.", "AI": {"tldr": "SOAR: A meta-RL framework where a teacher LLM generates synthetic problems as curricula for a student LLM, rewarded by actual student progress on hard problems, escaping reasoning plateaus without needing to solve them first.", "motivation": "Reinforcement learning stalls on low-success-rate datasets with sparse binary rewards; can pretrained LLMs leverage latent knowledge to self-generate curricula for unsolvable problems?", "method": "SOAR uses bi-level meta-RL: a teacher model proposes synthetic problems for a student model, with teacher rewards based on measured student improvement on hard problem subsets, grounding curriculum in actual progress rather than intrinsic proxies.", "result": "(1) Bi-level meta-RL unlocks learning under sparse rewards by sharpening latent stepping-stone generation capacity; (2) Grounded rewards outperform intrinsic schemes, avoiding instability and diversity collapse; (3) Question structural quality matters more than solution correctness for learning progress.", "conclusion": "Models can generate useful stepping stones without preexisting ability to solve hard problems, providing a principled path to escape reasoning plateaus without additional curated data."}}
