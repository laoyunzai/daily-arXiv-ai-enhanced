<div id=toc></div>

# Table of Contents

- [cs.CC](#cs.CC) [Total: 1]
- [quant-ph](#quant-ph) [Total: 4]
- [cs.AI](#cs.AI) [Total: 11]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 5]
- [cs.LG](#cs.LG) [Total: 9]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 2]
- [nlin.CD](#nlin.CD) [Total: 1]
- [nlin.AO](#nlin.AO) [Total: 1]


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [1] [Polynomial Identity Testing and Reconstruction for Depth-4 Powering Circuits of High Degree](https://arxiv.org/abs/2602.20832)
*Amir Shpilka,Yann Tal*

Main category: cs.CC

TL;DR: This paper presents the first polynomial-time deterministic algorithms for polynomial identity testing (PIT) and reconstruction of depth-4 arithmetic circuits (Σ^{[r]}∧^{[d]}Σ^{[s]}Π^{[δ]}), handling unbounded top fan-in and improving prior work that required non-degeneracy assumptions.


<details>
  <summary>Details</summary>
Motivation: Depth-4 arithmetic circuits are fundamental in algebraic complexity and appear in tensor decomposition/machine learning tasks, but existing deterministic PIT/reconstruction algorithms were limited to bounded top fan-in or relied on non-constructive/non-deterministic methods. This work addresses the critical gap in deterministic, efficient solutions for practical circuit classes with polynomially large parameters.

Method: PIT uses the Mason-Stothers theorem (ABC theorem for function fields) to construct explicit hitting sets by proving linear independence of high-degree sparse polynomial powers after projection. Reconstruction combines this with Wronskian-based differential operators, kernel analysis, and a robust Klivans-Spielman hitting set to recover circuit structure.

Result: 1) Deterministic PIT algorithm with hitting set size O(r⁴ s⁴ n² d δ³) when d > r². 2) Reconstruction algorithm running in poly(n,s,d) time when d = Ω(r⁴δ), tolerating polynomially large r and δ. Both hold over characteristic zero/sufficiently large fields.

Conclusion: This resolves a major open problem by providing the first deterministic polynomial-time solutions for depth-4 powering circuits with unbounded top fan-in, advancing the state-of-the-art in derandomization and explicit circuit reconstruction for algebraic models relevant to machine learning.

Abstract: We study deterministic polynomial identity testing (PIT) and reconstruction algorithms for depth-$4$ arithmetic circuits of the form \[ Σ^{[r]}\!\wedge^{[d]}\!Σ^{[s]}\!Π^{[δ]}. \] This model generalizes Waring decompositions and diagonal circuits, and captures sums of powers of low-degree sparse polynomials. Specifically, each circuit computes a sum of $r$ terms, where each term is a $d$-th power of an $s$-sparse polynomial of degree $δ$. This model also includes algebraic representations that arise in tensor decomposition and moment-based learning tasks such as mixture models and subspace learning.
  We give deterministic worst-case algorithms for PIT and reconstruction in this model. Our PIT construction applies when $d>r^2$ and yields explicit hitting sets of size $O(r^4 s^4 n^2 d δ^3)$. The reconstruction algorithm runs in time $\textrm{poly}(n,s,d)$ under the condition $d=Ω(r^4δ)$, and in particular it tolerates polynomially large top fan-in $r$ and bottom degree $δ$.
  Both results hold over fields of characteristic zero and over fields of sufficiently large characteristic. These algorithms provide the first polynomial-time deterministic solutions for depth-$4$ powering circuits with unbounded top fan-in. In particular, the reconstruction result improves upon previous work which required non-degeneracy or average-case assumptions.
  The PIT construction relies on the ABC theorem for function fields (Mason-Stothers theorem), which ensures linear independence of high-degree powers of sparse polynomials after a suitable projection. The reconstruction algorithm combines this with Wronskian-based differential operators, structural properties of their kernels, and a robust version of the Klivans-Spielman hitting set.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [2] [A Symplectic Proof of the Quantum Singleton Bound](https://arxiv.org/abs/2602.20186)
*Frederick Dehmel,Shilun Li*

Main category: quant-ph

TL;DR: 该论文提出了量子Singleton界的辛线性代数证明方法及Lean4形式化验证，通过辛向量空间模型和基于距离的擦除可纠正性，推导出稳定子码的参数约束关系。


<details>
  <summary>Details</summary>
Motivation: 传统熵基证明方法分析复杂且不易于形式化验证，本文旨在提供一种更纯粹的代数证明框架，突出量子纠错码的底层代数结构，同时满足计算机辅助验证的需求。

Method: 采用有限维辛向量空间描述泡利算子，结合距离基擦除可纠正性与清洁引理，运用辛稳定子框架内的维数计数方法。

Result: 成功推导出[[n, k, d]]稳定子量子纠错码的量子Singleton界：k + 2(d - 1) ≤ n，并完成了该证明的Lean4形式化。

Conclusion: 辛线性代数方法有效剥离了分析工具，揭示了界定的纯代数本质，证明了该途径对形式化验证的优越性，为量子纠错码的理论研究提供了新范式。

Abstract: We present a symplectic linear-algebraic proof of the Quantum Singleton Bound for stabiliser quantum error-correcting codes together with a Lean4 formalisation of the linear-algebraic argument. The proof is formulated in the language of finite-dimensional symplectic vector spaces modelling Pauli operators and relies on distance-based erasure correctability and the cleaning lemma. Using a dimension-counting argument within the symplectic stabiliser framework, we derive the bound \( k + 2(d - 1) \le n \) for any [[n, k, d]] stabiliser code. This approach isolates the algebraic structure underlying the bound and avoids the heavier analytic machinery that appears in entropy-based proofs, while remaining well-suited to formal verification.

</details>


### [3] [Quantum Simulations for Extreme Ultraviolet Photolithography](https://arxiv.org/abs/2602.20234)
*Tyler D. Kharazi,Stepan Fomichev,Shu Kanno,Takao Kobayashi,Juan Miguel Arrazola,Qi Gao,Torin F. Stetina*

Main category: quant-ph

TL;DR: 开发量子算法模拟极紫外光刻过程，以解决半导体制造中的模糊限制。


<details>
  <summary>Details</summary>
Motivation: 极紫外光刻因92 eV光子吸收导致的“模糊”而受限；经典方法计算效率低下，需量子模拟提供精确数据。

Method: 提出两种量子算法：一是用于光吸收截面的相干时域光谱算法，二是利用实时动力学计算光电子能谱的一阶量化平面波模拟。

Result: 资源估算：小实例需约200逻辑量子比特和10^9非Clifford门；高级光发射算法需≥10^13门和数千量子比特。

Conclusion: 高保真量子模拟是克服半导体小型化中电子模糊瓶颈的关键。

Abstract: Extreme Ultraviolet (EUV) lithography is the state-of-the-art process in semiconductor fabrication, yet its spatial resolution is fundamentally limited by the ``blur'' originating from absorption of photons at 92 eV, which induce physical and chemical changes in the photoresist via excited state processes and electron cascades. Accurate modeling of these phenomena requires precise ab initio data for high-energy decay channels, specifically photoabsorption and photoelectron emission. These are computationally difficult for classical methods due to prohibitive scaling in simulating electron dynamics, or due to the inability to resolve the ionization continuum in an efficient manner. In this work, we present quantum simulation algorithms to compute these key observables. First, we introduce a coherent time-domain spectroscopy algorithm optimized to resolve the photoabsorption cross-section at the 92 eV operating frequency. Second, we develop a first-quantized plane-wave simulation to compute the photoelectron kinetic energy spectrum, utilizing real-time dynamics and energy windowing to treat bound and delocalized scattering states on equal footing. Additionally, we provide logical resource estimation for a model photoresist monomer, 4-iodo-2-methylphenol (IMePh), and demonstrate that 92 eV absorption sensitivity can be resolved using roughly $200$ logical qubits and $10^{9}$ total non-Clifford gates per circuit with approximately $10^3$ shots for the smallest instance. The more sophisticated photoemission algorithm that models the continuum explicitly, incurs gate costs of $\geq 10^{13}$ total non-Clifford gates per circuit, $10^4$ shots, and requires a few thousand logical qubits. These results establish high-fidelity quantum simulations as a key component to parameterize the multi-scale macroscopic models required to overcome the electron blur bottleneck in semiconductor miniaturization.

</details>


### [4] [Proof of a finite threshold for the union-find decoder](https://arxiv.org/abs/2602.20238)
*Satoshi Yoshida,Ethan Lake,Hayata Yamasaki*

Main category: quant-ph

TL;DR: Proves union-find (UF) decoder achieves rigorous finite error threshold for surface code under circuit-level stochastic errors, enabling practical fault-tolerant quantum computation.


<details>
  <summary>Details</summary>
Motivation: While the UF decoder shows strong empirical performance for surface codes, its lack of rigorous threshold proof leaves uncertainty about fault tolerance beyond simulated scenarios, hindering theoretical validation for real-world quantum computers.

Method: Developed a refined error-clustering framework that extends prior cellular-automaton analysis techniques, demonstrating that error clusters can be separated by larger buffers to enable analytical control over UF decoder behavior.

Result: Established: (1) Rigorous finite threshold for UF decoder under circuit-level local stochastic errors; (2) Quasi-polylogarithmic upper bound on parallel UF decoder runtime; (3) Extended finite threshold proof to simpler greedy decoder.

Conclusion: Provides theoretical foundation for UF-based decoders in fault-tolerant quantum computers, offering a unified analytical framework that bridges practical decoder performance with rigorous fault-tolerance guarantees.

Abstract: Fast decoders that achieve strong error suppression are essential for fault-tolerant quantum computation (FTQC) from both practical and theoretical perspectives. The union-find (UF) decoder for the surface code is widely regarded as a promising candidate, offering almost-linear time complexity and favorable empirical error suppression supported by numerical evidence. However, the lack of a rigorous threshold theorem has left open whether the UF decoder can achieve fault tolerance beyond the error models and parameter regimes tested in numerical simulations. Here, we provide a rigorous proof of a finite threshold for the UF decoder on the surface code under the circuit-level local stochastic error model. To this end, we develop a refined error-clustering framework that extends techniques previously used to analyze cellular-automaton and renormalization-group decoders, by showing that error clusters can be separated by substantially larger buffers, thereby enabling analytical control over the behavior of the UF decoder. Using this guarantee, we further prove a quasi-polylogarithmic upper bound on the average runtime of a parallel UF decoder in terms of the code size. We also show that this framework yields a finite threshold for the greedy decoder, a simpler decoder with lower complexity but weaker empirical error suppression. These results provide a solid theoretical foundation for the practical use of UF-based decoders in the development of fault-tolerant quantum computers, while offering a unified framework for studying fault tolerance across these practical decoders.

</details>


### [5] [Time Crystals as Passively Protected Oscillating Qubits](https://arxiv.org/abs/2602.20269)
*Mert Esencan,A. I. Lvovsky,Berislav Buča*

Main category: quant-ph

TL;DR: 该论文提出利用耗散时间晶体动力学实现动态量子比特的无源纠错，通过驱动-耗散玻色系统在Bose-Hubbard dimer中编码持续振荡的量子信息，在热力学极限下通过宇称对称性破缺和Liouvillian谱相变实现噪声环境下的量子信息被动保护


<details>
  <summary>Details</summary>
Motivation: 现有量子无源纠错方案仅适用于静态量子存储器，难以保护动态演化的量子比特，亟需解决开放量子系统中动态量子信息的退相干问题

Method: 在驱动-耗散玻色系统中构建Bose-Hubbard dimer模型，利用强宇称对称性产生简并稳态，通过热力学极限打破对称性形成非稳态持续振荡，并分析Liouvillian谱的相变行为

Result: 当驱动强度超过临界点时，非稳态在Liouvillian谱相变后形成无噪声子系统，可抵抗全局损耗、全局退相位和相位扰动，使振荡量子比特的量子信息被被动保护

Conclusion: 耗散时间晶体动力学为动态量子信息提供新型无源保护机制，实现了自稳定振荡量子比特，为开放量子系统的量子计算开辟了新途径

Abstract: Protecting information against decoherence in open quantum systems remains a central challenge for quantum computing. In particular, passive error correction schemes have so far been limited to static memories rather than dynamical qubits. We demonstrate that a driven-dissipative bosonic system can encode a persistently oscillating qubit within a noiseless subsystem, realized explicitly in the Bose-Hubbard dimer (BHD). The strong parity symmetry of the model leads to degenerate stationary states. This symmetry is further broken into non-stationary states in the thermodynamic limit, which exhibit persistent oscillations. As the driving force increases, the Liouvillian spectrum of these states features a phase transition. Above the transition point, the non-stationary state encodes quantum information, preserving it in a noiseless subsystem. In addition to global loss that affects both bosonic modes identically, we further add global dephasing and show that the oscillating qubit is preserved. Finally, in order to gain additional physical insight, we study the effect of phase perturbation to both modes and observe that likewise they are passively protected, returning approximately to their initial configurations. These results establish dissipative time-crystalline dynamics as a mechanism for passive protection of dynamical quantum information, enabling autonomously stabilized oscillating qubits.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [Multilevel Determinants of Overweight and Obesity Among U.S. Children Aged 10-17: Comparative Evaluation of Statistical and Machine Learning Approaches Using the 2021 National Survey of Children's Health](https://arxiv.org/abs/2602.20303)
*Joyanta Jyoti Mondal*

Main category: cs.AI

TL;DR: 该研究分析18,792名美国青少年数据，比较统计学与机器学习模型预测超重/肥胖的效果，发现复杂模型提升有限，且种族与贫困群体的预测差异持续存在，呼吁改进数据质量与公平性监测而非追求算法复杂度


<details>
  <summary>Details</summary>
Motivation: 美国青少年超重肥胖问题严峻，但多层面行为、家庭及社区因素的联合预测结构尚未明确，且不同算法在预测性能与群体公平性上的差异需系统评估

Method: 基于2021年美国儿童健康调查数据，采用逻辑回归、随机森林、XGBoost等7类模型，通过AUC、F1值等指标评估预测性能，并检验种族与贫困亚组的差异

Result: 模型区分度0.66-0.79，逻辑回归与梯度提升树表现最均衡；提升类与深度学习仅轻微改善召回率与F1，但无模型全面占优；所有算法均存在种族与贫困群体的性能差异

Conclusion: 增加模型复杂度对预测效果提升有限，需优先提升数据质量与公平性监测，而非依赖复杂算法，以解决健康差异问题

Abstract: Background: Childhood and adolescent overweight and obesity remain major public health concerns in the United States and are shaped by behavioral, household, and community factors. Their joint predictive structure at the population level remains incompletely characterized. Objectives: The study aims to identify multilevel predictors of overweight and obesity among U.S. adolescents and compare the predictive performance, calibration, and subgroup equity of statistical, machine-learning, and deep-learning models. Data and Methods: We analyze 18,792 children aged 10-17 years from the 2021 National Survey of Children's Health. Overweight/obesity is defined using BMI categories. Predictors included diet, physical activity, sleep, parental stress, socioeconomic conditions, adverse experiences, and neighborhood characteristics. Models include logistic regression, random forest, gradient boosting, XGBoost, LightGBM, multilayer perceptron, and TabNet. Performance is evaluated using AUC, accuracy, precision, recall, F1 score, and Brier score. Results: Discrimination range from 0.66 to 0.79. Logistic regression, gradient boosting, and MLP showed the most stable balance of discrimination and calibration. Boosting and deep learning modestly improve recall and F1 score. No model was uniformly superior. Performance disparities across race and poverty groups persist across algorithms. Conclusion: Increased model complexity yields limited gains over logistic regression. Predictors consistently span behavioral, household, and neighborhood domains. Persistent subgroup disparities indicate the need for improved data quality and equity-focused surveillance rather than greater algorithmic complexity.

</details>


### [7] [DMCD: Semantic-Statistical Framework for Causal Discovery](https://arxiv.org/abs/2602.20333)
*Samarth KaPatel,Sofia Nikiforova,Giacinto Paolo Saggese,Paul Smith*

Main category: cs.AI

TL;DR: Proposes DMCD, a two-phase causal discovery framework combining LLM-based semantic drafting from metadata with statistical validation, achieving leading performance on real-world benchmarks.


<details>
  <summary>Details</summary>
Motivation: To leverage rich variable metadata through semantic reasoning for causal discovery, addressing limitations of purely statistical methods by incorporating domain knowledge.

Method: Two-phase approach: Phase I uses LLM to generate a sparse draft DAG from variable metadata; Phase II audits and refines this draft via conditional independence testing with targeted edge revisions based on detected discrepancies.

Result: Achieves competitive or leading performance across three real-world benchmarks (industrial engineering, environmental monitoring, IT systems), with particularly large gains in recall and F1 score. Ablation studies confirm improvements come from semantic reasoning rather than memorization.

Conclusion: Combining semantic priors from LLM with principled statistical verification yields a high-performing and practically effective approach to causal structure learning.

Abstract: We present DMCD (DataMap Causal Discovery), a two-phase causal discovery framework that integrates LLM-based semantic drafting from variable metadata with statistical validation on observational data. In Phase I, a large language model proposes a sparse draft DAG, serving as a semantically informed prior over the space of possible causal structures. In Phase II, this draft is audited and refined via conditional independence testing, with detected discrepancies guiding targeted edge revisions.
  We evaluate our approach on three metadata-rich real-world benchmarks spanning industrial engineering, environmental monitoring, and IT systems analysis. Across these datasets, DMCD achieves competitive or leading performance against diverse causal discovery baselines, with particularly large gains in recall and F1 score. Probing and ablation experiments suggest that these improvements arise from semantic reasoning over metadata rather than memorization of benchmark graphs. Overall, our results demonstrate that combining semantic priors with principled statistical verification yields a high-performing and practically effective approach to causal structure learning.

</details>


### [8] [Diffusion Modulation via Environment Mechanism Modeling for Planning](https://arxiv.org/abs/2602.20422)
*Hanping Zhang,Yuhong Guo*

Main category: cs.AI

TL;DR: 提出DMEMM方法，通过建模环境机制（状态转移与奖励函数）调制扩散模型训练，解决离线强化学习中轨迹生成与真实环境不一致的问题，实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在离线强化学习轨迹生成中忽略状态转移一致性，导致生成轨迹与真实环境机制存在显著偏差

Method: 提出Diffusion Modulation via Environment Mechanism Modeling (DMEMM)，在扩散模型训练中显式融入状态转移动态和奖励函数等关键环境机制进行调制

Result: 实验表明该方法在离线强化学习规划任务上达到当前最优性能水平

Conclusion: 通过环境机制建模调制扩散过程可有效提升轨迹生成与真实环境的一致性，为离线RL规划提供新范式

Abstract: Diffusion models have shown promising capabilities in trajectory generation for planning in offline reinforcement learning (RL). However, conventional diffusion-based planning methods often fail to account for the fact that generating trajectories in RL requires unique consistency between transitions to ensure coherence in real environments. This oversight can result in considerable discrepancies between the generated trajectories and the underlying mechanisms of a real environment. To address this problem, we propose a novel diffusion-based planning method, termed as Diffusion Modulation via Environment Mechanism Modeling (DMEMM). DMEMM modulates diffusion model training by incorporating key RL environment mechanisms, particularly transition dynamics and reward functions. Experimental results demonstrate that DMEMM achieves state-of-the-art performance for planning with offline reinforcement learning.

</details>


### [9] [Implicit Intelligence -- Evaluating Agents on What Users Don't Say](https://arxiv.org/abs/2602.20424)
*Ved Sirdeshmukh,Marc Wetter*

Main category: cs.AI

TL;DR: A new evaluation framework reveals AI agents struggle with implicit reasoning, with top models passing only 48.3% of scenarios that require inferring unstated constraints beyond literal instructions.


<details>
  <summary>Details</summary>
Motivation: Current AI benchmarks only test explicit instruction-following, failing to evaluate whether agents can reason about implicit requirements like accessibility, privacy, catastrophic risks, and contextual constraints that are fundamental to natural human communication.

Method: Developed Implicit Intelligence framework paired with Agent-as-a-World (AaW) harness, featuring 205 interactive scenarios defined in human-readable YAML files where user requests appear simple but solutions require discovering hidden complexity through environmental exploration.

Result: Evaluated 16 frontier and open-weight models, finding that even the best-performing model achieved only a 48.3% scenario pass rate, revealing significant performance gaps.

Conclusion: Substantial room for improvement exists in bridging the gap between literal instruction-following and human-like contextual reasoning, as current AI agents struggle to become genuine goal-fulfillers in underspecified real-world scenarios.

Abstract: Real-world requests to AI agents are fundamentally underspecified. Natural human communication relies on shared context and unstated constraints that speakers expect listeners to infer. Current agentic benchmarks test explicit instruction-following but fail to evaluate whether agents can reason about implicit requirements spanning accessibility needs, privacy boundaries, catastrophic risks, and contextual constraints. We present Implicit Intelligence, an evaluation framework testing whether AI agents can move beyond prompt-following to become genuine goal-fulfillers, paired with Agent-as-a-World (AaW), a harness where interactive worlds are defined in human-readable YAML files and simulated by language models. Our scenarios feature apparent simplicity in user requests, hidden complexity in correct solutions, and discoverability of constraints through environmental exploration. Evaluating 16 frontier and open-weight models across 205 scenarios, we find that even the best-performing model achieves only 48.3% scenario pass rate, revealing substantial room for improvement in bridging the gap between literal instruction-following and human-like contextual reasoning.

</details>


### [10] [Learning to Rewrite Tool Descriptions for Reliable LLM-Agent Tool Use](https://arxiv.org/abs/2602.20426)
*Ruocheng Guo,Kaiwen Dong,Xiang Gao,Kamalika Das*

Main category: cs.AI

TL;DR: 提出Trace-Free+框架解决工具接口瓶颈问题，通过课程学习从轨迹丰富场景迁移到无轨迹部署，并构建大规模高质量工具接口数据集，在多个基准测试中实现未见工具的稳定提升和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有研究过度关注智能体微调，忽略了工具接口质量瓶颈问题。传统方法依赖执行轨迹数据，在冷启动或隐私限制场景下不可用，且工具独立优化限制泛化能力。

Method: 1) 提出课程学习框架Trace-Free+，实现从轨迹丰富到无轨迹场景的监督迁移；2) 构建结构化流程生成的大规模高质量工具接口数据集；3) 抽象可复用的接口使用模式和工具调用结果。

Result: 在StableToolBench和RestBench上：1) 对未见工具保持稳定提升；2) 展现强跨领域泛化能力；3) 候选工具扩展至100+时仍保持鲁棒性。

Conclusion: 工具接口优化是智能体微调的有效补充方案，Trace-Free+框架证明无需执行轨迹也能实现高质量工具接口学习，具有实际部署价值。

Abstract: The performance of LLM-based agents depends not only on the agent itself but also on the quality of the tool interfaces it consumes. While prior work has focused heavily on agent fine-tuning, tool interfaces-including natural language descriptions and parameter schemas-remain largely human-oriented and often become a bottleneck, especially when agents must select from large candidate tool sets. Existing approaches to improving tool interfaces rely on execution traces, which are frequently unavailable in cold-start or privacy-constrained settings, and typically optimize each tool independently, limiting scalability and generalization to unseen tools. We propose Trace-Free+, a curriculum learning framework that progressively transfers supervision from trace-rich settings to trace-free deployment, encouraging the model to abstract reusable interface-usage patterns and tool usage outcomes. To support this approach, we construct a large-scale dataset of high-quality tool interfaces using a structured workflow over a diverse collection of tools. Experiments on StableToolBench and RestBench show consistent gains on unseen tools, strong cross-domain generalization, and robustness as the number of candidate tools scales to over 100, demonstrating that tool interface optimization is a practical and deployable complement to agent fine-tuning.

</details>


### [11] [ActionEngine: From Reactive to Programmatic GUI Agents via State Machine Memory](https://arxiv.org/abs/2602.20502)
*Hongbin Zhong,Fazle Faisal,Luis França,Tanakorn Leesatapornwongsa,Adriana Szekeres,Kexin Rong,Suman Nath*

Main category: cs.AI

TL;DR: 提出ActionEngine框架，通过双智能体架构（离线构建GUI状态机的爬取智能体+在线生成执行程序的执行智能体）实现程序化规划，替代传统视觉模型的逐步推理，显著提升GUI自动化任务的效率和准确率


<details>
  <summary>Details</summary>
Motivation: 现有GUI智能体采用视觉模型逐步推理执行，存在成本高、延迟大（随推理步骤线性增长）且缺乏页面历史记忆导致准确率低的问题

Method: 1. 设计双智能体架构：Crawling Agent通过离线探索构建可更新的GUI状态机记忆；Execution Agent基于记忆合成可执行的Python程序进行在线任务执行<br>2. 引入视觉重接地机制：执行失败时修复动作并更新记忆，应对界面变化

Result: 在WebArena的Reddit任务中达到95%成功率（最强基线仅66%），平均仅需1次LLM调用，成本降低11.8倍，端到端延迟降低2倍

Conclusion: 通过全局程序化规划+爬虫验证动作模板+节点级执行与局部修复的结合，实现了可扩展且可靠的GUI交互，为自动化任务提供了高效新范式

Abstract: Existing Graphical User Interface (GUI) agents operate through step-by-step calls to vision language models--taking a screenshot, reasoning about the next action, executing it, then repeating on the new page--resulting in high costs and latency that scale with the number of reasoning steps, and limited accuracy due to no persistent memory of previously visited pages.
  We propose ActionEngine, a training-free framework that transitions from reactive execution to programmatic planning through a novel two-agent architecture: a Crawling Agent that constructs an updatable state-machine memory of the GUIs through offline exploration, and an Execution Agent that leverages this memory to synthesize complete, executable Python programs for online task execution.
  To ensure robustness against evolving interfaces, execution failures trigger a vision-based re-grounding fallback that repairs the failed action and updates the memory.
  This design drastically improves both efficiency and accuracy: on Reddit tasks from the WebArena benchmark, our agent achieves 95% task success with on average a single LLM call, compared to 66% for the strongest vision-only baseline, while reducing cost by 11.8x and end-to-end latency by 2x.
  Together, these components yield scalable and reliable GUI interaction by combining global programmatic planning, crawler-validated action templates, and node-level execution with localized validation and repair.

</details>


### [12] [CausalReasoningBenchmark: A Real-World Benchmark for Disentangled Evaluation of Causal Identification and Estimation](https://arxiv.org/abs/2602.20571)
*Ayush Sawarni,Jiyuan Tan,Vasilis Syrgkanis*

Main category: cs.AI

TL;DR: 现有因果推断基准将识别与估计混为一谈，本文提出CausalReasoningBenchmark，要求模型分别输出结构化识别规范和数值估计结果。测试发现：LLM高层策略识别率达84%，但完整识别规范正确率仅30%，瓶颈在于研究设计的细节而非计算。


<details>
  <summary>Details</summary>
Motivation: 现有自动化因果推断基准仅评估单一数值输出（如平均处理效应），将因果分析的两个关键步骤——识别（制定研究设计）与估计（数值实现）——混为一谈，难以诊断系统具体失败原因。

Method: 构建包含173个查询、138个真实数据集的基准测试集（源自85篇论文和4本教科书），要求系统必须输出：(i) 结构化识别规范（策略、处理变量、结果变量、控制变量及设计细节）；(ii) 点估计值和标准误。两部分独立评分以实现精细化诊断。

Result: 对先进LLM的基线测试显示：高层策略识别正确率84%，但完整识别规范正确率骤降至30%，表明当前瓶颈在于研究设计的细微细节，而非数值计算能力。

Conclusion: CausalReasoningBenchmark能有效区分因果推理失败与数值执行错误，揭示现有模型在识别细节上的严重不足，为开发更稳健的自动化因果推断系统提供了诊断工具。该基准已在Hugging Face开源。

Abstract: Many benchmarks for automated causal inference evaluate a system's performance based on a single numerical output, such as an Average Treatment Effect (ATE). This approach conflates two distinct steps in causal analysis: identification-formulating a valid research design under stated assumptions-and estimation-implementing that design numerically on finite data. We introduce CausalReasoningBenchmark, a benchmark of 173 queries across 138 real-world datasets, curated from 85 peer-reviewed research papers and four widely-used causal-inference textbooks. For each query a system must produce (i) a structured identification specification that names the strategy, the treatment, outcome, and control variables, and all design-specific elements, and (ii) a point estimate with a standard error. By scoring these two components separately, our benchmark enables granular diagnosis: it distinguishes failures in causal reasoning from errors in numerical execution. Baseline results with a state-of-the-art LLM show that, while the model correctly identifies the high-level strategy in 84 % of cases, full identification-specification correctness drops to only 30 %, revealing that the bottleneck lies in the nuanced details of research design rather than in computation. CausalReasoningBenchmark is publicly available on Hugging Face and is designed to foster the development of more robust automated causal-inference systems.

</details>


### [13] [Buffer Matters: Unleashing the Power of Off-Policy Reinforcement Learning in Large Language Model Reasoning](https://arxiv.org/abs/2602.20722)
*Xu Wan,Yansheng Wang,Wenqi Huang,Mingyang Sun*

Main category: cs.AI

TL;DR: BAPO是一种离策略RLVR框架，通过动态选择训练批次和重评估困难样本来提高LLM后训练的数据效率，在多项任务上比GRPO平均提升12.5%。


<details>
  <summary>Details</summary>
Motivation: 传统的同策略RLVR框架存在经验浪费和奖励同质化问题，阻碍了大语言模型后训练在困难样本上的学习效率。

Method: 提出批适应策略优化(BAPO)，一种离策略RLVR框架，通过重评估历史困难样本并复用高质量样本来动态选择训练批次，同时保证策略改进的下界。

Result: 在数学、规划和视觉推理任务上，BAPO相比GRPO平均提升12.5%，且能解决40.7%的基础模型始终无法解决的问题。

Conclusion: BAPO有效提高了大语言模型后训练的数据效率和性能。

Abstract: Traditional on-policy Reinforcement Learning with Verifiable Rewards (RLVR) frameworks suffer from experience waste and reward homogeneity, which directly hinders learning efficiency on difficult samples during large language models post-training. In this paper, we introduce Batch Adaptation Policy Optimization (BAPO), an off-policy RLVR framework to improve the data efficiency in large language models post-training. It dynamically selects training batches by re-evaluating historically difficult samples and reusing high-quality ones, while holding a lower bound guarantee for policy improvement. Extensive experiments further demonstrate that BAPO achieves an average 12.5% improvement over GRPO across mathematics, planning, and visual reasoning tasks. Crucially, BAPO successfully resolves 40.7% of problems that base models consistently fail to solve.

</details>


### [14] [CHESS: Context-aware Hierarchical Efficient Semantic Selection for Long-Context LLM Inference](https://arxiv.org/abs/2602.20732)
*Chao Fei,Guozhong Li,Chenxi Liu,Panos Kalnis*

Main category: cs.AI

TL;DR: CHESS是一种算法-系统协同设计的KV缓存管理系统，通过上下文感知的分层选择策略动态重建相干上下文，在仅使用1% KV缓存的情况下保持高质量推理，实现最高4.56倍吞吐量提升


<details>
  <summary>Details</summary>
Motivation: 长上下文大语言模型推理时面临KV缓存瓶颈，现有剪枝方法缺乏上下文感知，忽略token的逐步相关性和局部语义，导致质量下降；且不规则的内存访问和选择开销限制了实际加速效果

Method: 算法-系统协同设计：算法层面采用上下文感知的分层选择策略，为当前解码动态重建相干上下文；系统层面使用粗粒度选择消除昂贵数据移动，充分实现理论稀疏性的实际加速

Result: 在仅使用1% KV缓存的情况下超越全KV质量，实现最高4.56倍吞吐量提升，保持低延迟稳定推理，且持续优于其他强基线方法

Conclusion: CHESS通过算法-系统协同设计有效解决了长上下文LLM的KV缓存约束问题，在保持推理质量的同时实现了显著的实践性能提升

Abstract: Long-context LLMs demand accurate inference at low latency, yet decoding becomes primarily constrained by KV cache as context grows. Prior pruning methods are largely context-agnostic: their token selection ignores step-wise relevance and local semantics, which undermines quality. Moreover, their irregular accesses and selection overheads yield only limited wall-clock speedups. To address this, we propose \textbf{CHESS}, an \textit{algorithm-system co-design} KV-cache management system. Algorithmically, CHESS introduces a context-aware, hierarchical selection policy that dynamically reconstructs a coherent context for the current decoding. System-wise, coarse granularity selection eliminates expensive data movement, fully realizing practical acceleration from theoretical sparsity. Extensive evaluations demonstrate that CHESS surpasses Full-KV quality using only \textbf{1\%} of the KV cache, delivers low-latency stable inference with up to \textbf{4.56$\times$} higher throughput, and consistently outperforms other strong baselines. Code is available at \href{https://anonymous.4open.science/r/CHESS-9958/}{https://anonymous.4open.science/r/CHESS/}.

</details>


### [15] [PyVision-RL: Forging Open Agentic Vision Models via RL](https://arxiv.org/abs/2602.20739)
*Shitian Zhao,Shaoheng Lin,Ming Li,Haoquan Zhang,Wenshuo Peng,Kaipeng Zhang,Chen Wei*

Main category: cs.AI

TL;DR: 提出PyVision-RL框架解决多模态智能体强化学习中的交互崩溃问题，通过过采样-过滤-排序策略和累积工具奖励保持多轮推理，并开发支持按需抽帧的视频理解模型PyVision-Video


<details>
  <summary>Details</summary>
Motivation: 多模态智能体在强化学习中会出现交互崩溃现象，即模型减少工具使用和多轮推理，限制了智能体行为的收益。需要一种稳定训练并维持交互的方法。

Method: 提出PyVision-RL框架，结合过采样-过滤-排序的rollout策略和累积工具奖励来防止崩溃；针对视频推理，采用按需上下文构建，在推理过程中选择性地采样任务相关帧以减少视觉token使用。

Result: 实验显示该框架性能强劲且效率提升，证明了维持交互和按需视觉处理对可扩展多模态智能体的重要性。

Conclusion: 持续交互和按需视觉处理是构建可扩展多模态智能体的关键，PyVision-RL框架有效解决了交互崩溃问题。

Abstract: Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.

</details>


### [16] [Aletheia tackles FirstProof autonomously](https://arxiv.org/abs/2602.21201)
*Tony Feng,Junehyuk Jung,Sang-hyun Kim,Carlo Pagano,Sergei Gukov,Chiang-Chiang Tsai,David Woodruff,Adel Javanmard,Aryan Mokhtari,Dawsen Hwang,Yuri Chervonyi,Jonathan N. Lee,Garrett Bingham,Trieu H. Trinh,Vahab Mirrokni,Quoc V. Le,Thang Luong*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We report the performance of Aletheia (Feng et al., 2026b), a mathematics research agent powered by Gemini 3 Deep Think, on the inaugural FirstProof challenge. Within the allowed timeframe of the challenge, Aletheia autonomously solved 6 problems (2, 5, 7, 8, 9, 10) out of 10 according to majority expert assessments; we note that experts were not unanimous on Problem 8 (only). For full transparency, we explain our interpretation of FirstProof and disclose details about our experiments as well as our evaluation. Raw prompts and outputs are available at https://github.com/google-deepmind/superhuman/tree/main/aletheia.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [17] [Elliptic mirror of the quantum Hall effect](https://arxiv.org/abs/2602.20174)
*C. A. Lütken*

Main category: cond-mat.str-el

TL;DR: 提出环面σ模型，用全纯模对称性统一描述整数量子霍尔效应和分数量子霍尔效应，通过镜像对称映射到可解的椭圆模型，其临界指数与数值模拟结果高度吻合


<details>
  <summary>Details</summary>
Motivation: 通过全纯模对称性自动统一整数和分数量子霍尔效应，解决传统理论中两者分离的问题，并探索量子临界点的普适性规律

Method: 利用镜像对称将复杂的环面σ模型映射为可处理的椭圆模型，通过拓扑卷绕数描述拓扑保护，并对比30年实验/数值数据验证几何标度流

Result: 1. 模模型预测的标度流几何与实验数据高度一致，包含多个量子临界点位置；2. 临界退局域化指数ν_tor=2.605与Chalker-Coddington模型的数值结果ν_num=2.607±0.004惊人吻合；3. 实验值ν_exp=2.3±0.2存在差异

Conclusion: 环面σ模型与数值模型很可能属于同一普适类，但实验差异需通过改进有限尺寸标度实验验证；若证实，该框架将为量子霍尔临界现象提供统一理论基础

Abstract: Toroidal sigma models of magneto-transport are analyzed, in which integer and fractional quantum Hall effects automatically are unified by a {holomorphic modular symmetry}. By exploiting a quantum equivalence called \emph{mirror symmetry}, these models are mapped to tractable mirror models (also elliptic), in which topological protection is provided by more familiar winding numbers. Phase diagrams and scaling properties of elliptic models are compared to some of the experimental and numerical data accumulated over the past three decades. The geometry of scaling flows extracted from quantum Hall experiments is in good agreement with modular predictions, including the location of many quantum critical points. One conspicuous model %(arguably the simplest and most natural one) has a critical delocalization exponent $ν_{\rm tor} = 18 \ln 2 /(π^2 G^4) = 2.6051\dots$ ($G$ is Gauss' constant) that is in excellent agreement with the value $ν_{\rm num} = 2.607\pm\,.004$ calculated in the numerical Chalker-Coddington model, suggesting that these models are in the same universality class. The real delocalization exponent may be disentangled from other scaling exponents in finite size scaling experiments, giving an experimental value $ν_{\rm exp} = 2.3\pm 0.2$. The modular model suggests how these theoretical and experimental results may be reconciled, but in order to determine if these theoretical models really are in the quantum Hall universality class, improved finite size scaling experiments are urgently needed.

</details>


### [18] [Real-space construction and classification for time-reversal symmetric crystalline superconductors in 2D interacting fermionic systems](https://arxiv.org/abs/2602.20560)
*Yi-Ming Liu,Wei-Qiang Chen,Zheng-Cheng Gu*

Main category: cond-mat.str-el

TL;DR: 该论文系统地分类了二维相互作用费米子系统中具有时间反演和晶体对称性的拓扑超导体，发现了无法在非相互作用或玻色子系统中实现的新型本征相互作用拓扑超导相。


<details>
  <summary>Details</summary>
Motivation: 实际超导材料中普遍存在晶体对称性和时间反演对称性，但对于具有这些对称性的相互作用费米子系统，其拓扑分类尚不完整。

Method: 采用显式的实空间构造方法，对二维相互作用费米子系统中的时间反演对称保护晶体拓扑超导体进行系统分类。

Result: 在C4×Z2^T、D4×Z2^T等对称性保护下，发现具有Z4分类的本征相互作用相；这些相可通过装饰化的1D费米子拓扑相（双马约拉纳链）构建；扩展至p4、p4m、p4g、pm等壁纸群；这些相可演生出具有角零能模的高阶拓扑相；并验证了晶体等效原理。

Conclusion: 该工作完善了相互作用拓扑超导体的分类体系，揭示了与自由费米子或玻色子系统本质不同的新型拓扑相，为量子材料研究提供了理论基础。

Abstract: Crystalline symmetry and time-reversal symmetry are commonly present in real superconducting materials. However, the topological classification of systems respecting these symmetries, particularly for interacting fermions, remains incomplete. In this work, we systematically classify time-reversal symmetry-protected crystalline topological superconductors in two-dimensional interacting fermionic systems using an explicit real-space construction. Among the resulting phases, we identify intrinsically interacting fermionic topological superconductors, i.e., phases that cannot be realized in either free-fermion or interacting bosonic systems. For spinless fermions with protecting symmetry group $C_4 \times Z_2^T$ or $D_4 \times Z_2^T$ (plus fermion parity), the intrinsic sector has a $Z_4$ classification. The corresponding root phases generating this $Z_4$ classification admit a transparent real-space construction in terms of decorated 1D blocks. These blocks are 1D fermionic symmetry-protected topological (FSPT) phases, realizable as double Majorana chains. We further find the corresponding $Z_4$ spinless intrinsic phases for wallpaper groups $p4$, $p4m$, and $p4g$. We also find an additional $Z_2$ intrinsically interacting phase for spinless fermions with wallpaper group $pm$, which is absent with the corresponding point-group symmetry alone. Moreover, these intrinsic phases naturally give rise to higher-order FSPT phases that support corner zero modes. Finally, we verify the crystalline equivalence principle for generic 2D interacting FSPT systems with both crystalline and internal symmetries.

</details>


### [19] [Kondo breakdown as an entanglement transition driven by continuous measurement](https://arxiv.org/abs/2602.20600)
*Debraj Debata,Abhirup Mukherjee,Siddhartha Lal*

Main category: cond-mat.str-el

TL;DR: Reformulates Kondo breakdown as a measurement-driven entanglement transition using non-perturbative URG, revealing a phase diagram with Kondo-screened and polarized phases separated by a novel non-Fermi liquid critical point.


<details>
  <summary>Details</summary>
Motivation: To understand how a local magnetic field disrupts Kondo screening by framing it as a continuous quantum measurement process that drives an entanglement transition between impurity and fermionic environment.

Method: Non-perturbative Unitary Renormalization Group (URG) approach to derive coupled RG flow equations for Kondo exchange coupling and local magnetic field, analyzing fixed points and phase boundaries.

Result: Identifies two distinct low-energy phases: a Kondo-screened phase (entangled singlet state) and a polarized local-moment phase (suppressed entanglement), with a critical transition point linked to a non-Fermi liquid; characterizes signatures via spectral functions and impurity thermalization.

Conclusion: Demonstrates that decoherence from the local field acts as a measurement driving an entanglement transition, providing new insights into quantum measurement dynamics in strongly correlated electron systems.

Abstract: We study the breakdown of Kondo screening by a local magnetic field from the perspective of a measurement-driven entanglement transition in a monitored quantum system. Here, the Kondo coupling leads to the growth in entanglement of an impurity spin with it's fermionic environment, while the local field plays the role of a continuous observer. Using a non-perturbative Unitary Renormalization Group (URG) approach, we derive coupled renormalization-group flow equations for the Kondo exchange and the local field, and obtain a field-dependent RG phase diagram. The RG flows separate a low-energy Kondo-screened phase, where the impurity is absorbed into the Fermi sea and forms an entangled singlet with the conduction bath, from a polarized local-moment phase in which screening is frustrated and impurity-bath entanglement is suppressed. We identify the fixed-point Hamiltonians governing the two phases and the critical regime, and relate the transition to the emergence of a novel non-Fermi liquid. Various impurity signatures such as the spectral function and thermalisation of impurity observables are used to characterise this entanglement transition. These results offer insight into the interplay of decoherence and measurement in governing the dynamics of a prototypical quantum system.

</details>


### [20] [Parameterizing DFT+U+V from Hybrid Functionals: A Wannier-Function-Based Approach for Strongly Correlated Materials](https://arxiv.org/abs/2602.20814)
*Dmitry M. Korotin,Anna A. Anisimova,Vladimir I. Anisimov*

Main category: cond-mat.str-el

TL;DR: 提出一种从杂化泛函计算中通过Wannier函数投影参数化DFT+U+V的方法，通过构建共同的局域Wannier基组并最小化哈密顿量失配来确定有效Hubbard参数，以较低成本精确重现杂化泛函的电子结构


<details>
  <summary>Details</summary>
Motivation: 解决强关联材料电子结构计算中杂化泛函精度高但计算成本昂贵，而传统DFT计算效率却精度不足的问题，寻求一种兼顾精度与效率的参数化方案

Method: 构建适用于DFT和杂化泛函计算的公共局域Wannier基组，通过最小化关联子空间内哈密顿量失配来确定在位U和位间V的Hubbard相互作用参数

Result: 在MgO、NiO和V₂O₅三种不同电子特性的氧化物体系中验证成功，该方法能以较低计算成本精确重现杂化泛函的带隙、态密度和磁矩等关键电子性质

Conclusion: 建立的参数化流程实现了计算效率与精度的良好平衡，为强关联材料的结构优化和多体计算提供了实用解决方案

Abstract: We present an approach to parameterize DFT+$U$+$V$ from hybrid-functional calculations using Wannier-function projections. The method constructs a common localized Wannier basis for both semilocal DFT and hybrid-functional calculations, then determines effective on-site ($U$) and intersite ($V$) Hubbard parameters by minimizing the Hamiltonian mismatch within the correlated subspace. This procedure yields interaction parameters that reproduce the hybrid-functional electronic structure at a fraction of the computational cost and allow efficient structural relaxations and further many-body calculations. We validate the workflow on three oxide systems with different electronic characters: MgO (wide-gap insulator), NiO (antiferromagnetic charge-transfer insulator), and V$_2$O$_5$ (d$^0$ transition-metal oxide). In all cases, the mapped DFT+$U$+$V$ parameters reproduce hybrid-functional band gaps, densities of states, and magnetic moments and improve upon semilocal DFT while maintaining computational efficiency.

</details>


### [21] [Entanglement Properties of the One-Dimensional Dimerized Fermi-Hubbard Model](https://arxiv.org/abs/2602.20990)
*Min-Chul Cha,Hoon Beom Kwon,Ji-Woo Lee,Myung-Hoon Chung*

Main category: cond-mat.str-el

TL;DR: The study distinguishes two distinct insulating phases in the 1D dimerized Fermi-Hubbard model using entanglement properties: a 1/2-filling phase with a charge gap from band structure, and a 3/4-filling Mott phase with an interaction-driven gap.


<details>
  <summary>Details</summary>
Motivation: To characterize and differentiate quantum phases in the dimerized Fermi-Hubbard model, particularly identifying distinct mechanisms behind insulating behaviors at different electron fillings using entanglement properties as a diagnostic tool.

Method: Employed a matrix-product-state (MPS) approach to compute the ground state, analyzing entanglement spectra and half-chain entanglement entropy scaling to probe phase characteristics.

Result: Identified three phases: two insulating phases (1/2-filling with interaction-enhanced band gap; 3/4-filling with Mott gap from interactions) and a metallic phase. Entanglement entropy scaling and spectrum distribution revealed fundamental differences between the two insulating phases.

Conclusion: Entanglement properties effectively distinguish quantum phases with different insulating mechanisms, demonstrating that entanglement spectra serve as a powerful tool for characterizing many-body states beyond conventional order parameters.

Abstract: We study the entanglement properties of the one-dimensional dimerized Fermi-Hubbard model. Using a matrix-product-state approach, we compute the ground state and identify two insulating phases at 1/2- and 3/4-filling, along with a metallic phase, whose mechanisms can be characterized by their entanglement spectra. Our findings indicate that the two insulating phases are distinct, implying that the phase at 1/2-filling has a charge gap arising from the band gap, which is enhanced by repulsive interactions, while the phase at 3/4-filling exhibits a Mott gap resulting from particle interactions. This difference between the two insulating phases is reflected in the scaling properties of the half-chain entanglement entropy and the distribution of the entanglement spectrum.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [22] [MoBiQuant: Mixture-of-Bits Quantization for Token-Adaptive Elastic LLMs](https://arxiv.org/abs/2602.20191)
*Dongwei Wang,Jinhee Kim,Seokho Han,Denis Gudovskiy,Yohei Nakata,Tomoyuki Okuno,KhayTze Peong,Kang Eun Jeon,Jong Hwan Ko,Yiran Chen,Huanrui Yang*

Main category: cs.LG

TL;DR: MoBiQuant是一种新颖的混合比特量化框架，通过基于token敏感度动态调整权重精度，实现弹性LLM推理，无需重复校准即可在不同精度间平滑切换。


<details>
  <summary>Details</summary>
Motivation: 云和边缘设备的计算资源动态变化需要弹性LLM部署，但现有量化校准参数与特定精度绑定，导致弹性精度校准和运行时精度切换面临挑战。精度相关的异常值迁移现象造成token级敏感度差异，是问题的根源。

Method: 提出MoBiQuant框架，包含两个核心组件：1) 多对一递归残差量化，通过迭代方式重建更高精度权重；2) token感知路由器，根据token敏感度动态选择残差位切片数量，实现基于token敏感度的自适应精度调整。

Result: 实验表明MoBiQuant具备强弹性：在LLaMA3-8B模型上，无需重复校准即可匹配比特级特定校准的PTQ性能，同时实现平滑精度切换，并改善了token异常值分布的泛化能力。

Conclusion: MoBiQuant成功解决了弹性LLM部署中的精度切换难题，为资源受限环境下动态调整模型推理精度提供了高效可行的解决方案。

Abstract: Changing runtime complexity on cloud and edge devices necessitates elastic large language model (LLM) deployment, where an LLM can be inferred with various quantization precisions based on available computational resources. However, it has been observed that the calibration parameters for quantization are typically linked to specific precisions, which presents challenges during elastic-precision calibration and precision switching at runtime. In this work, we attribute the source of varying calibration parameters to the varying token-level sensitivity caused by a precision-dependent outlier migration phenomenon.Motivated by this observation, we propose \texttt{MoBiQuant}, a novel Mixture-of-Bits quantization framework that adjusts weight precision for elastic LLM inference based on token sensitivity. Specifically, we propose the many-in-one recursive residual quantization that can iteratively reconstruct higher-precision weights and the token-aware router to dynamically select the number of residual bit slices. MoBiQuant enables smooth precision switching while improving generalization for the distribution of token outliers. Experimental results demonstrate that MoBiQuant exhibits strong elasticity, enabling it to match the performance of bit-specific calibrated PTQ on LLaMA3-8B without repeated calibration.

</details>


### [23] [FedAvg-Based CTMC Hazard Model for Federated Bridge Deterioration Assessment](https://arxiv.org/abs/2602.20194)
*Takato Yasuno*

Main category: cs.LG

TL;DR: 提出一种联邦学习框架，用于在不共享原始桥梁检查记录的情况下，协同估计连续时间马尔可夫链（CTMC）桥梁劣化风险模型，使各市政部门能在保护数据主权的同时获得全局基准参数，实现基于证据的寿命周期规划。


<details>
  <summary>Details</summary>
Motivation: 桥梁定期检查记录包含敏感的公共基础设施信息，在现有数据治理约束下，跨组织数据共享难以实现。需要一种协作建模方法，在不泄露原始数据的前提下，共同构建桥梁劣化预测基准模型。

Method: 采用联邦学习框架，各用户在本地用mini-batch随机梯度下降训练三个劣化方向（Good→Minor, Good→Severe, Minor→Severe）的对数线性CTMC风险模型，协变量包括桥梁年龄、海岸线距离和桥面面积；每轮通信仅上传12维伪梯度向量，服务器端采用带动量和梯度裁剪的样本加权联邦平均（FedAvg）进行聚合；实验使用基于已知真实参数生成的合成数据，评估联邦收敛行为。

Result: 模拟结果显示，在异构用户场景下，平均负对数似然持续收敛，聚合梯度范数随用户规模增加而减小；联邦更新机制形成天然参与激励：用户注册本地数据到共享技术平台，可定期获得全局基准参数，这些参数无法从本地数据单独获得。

Conclusion: 该联邦框架成功实现了在不转移原始数据的情况下协作训练桥梁劣化模型，在保护数据主权的同时提供共享基准，激励用户参与，为基础设施寿命周期规划提供了基于证据的决策支持。

Abstract: Bridge periodic inspection records contain sensitive information about public infrastructure, making cross-organizational data sharing impractical under existing data governance constraints. We propose a federated framework for estimating a Continuous-Time Markov Chain (CTMC) hazard model of bridge deterioration, enabling municipalities to collaboratively train a shared benchmark model without transferring raw inspection records. Each User holds local inspection data and trains a log-linear hazard model over three deterioration-direction transitions -- Good$\to$Minor, Good$\to$Severe, and Minor$\to$Severe -- with covariates for bridge age, coastline distance, and deck area. Local optimization is performed via mini-batch stochastic gradient descent on the CTMC log-likelihood, and only a 12-dimensional pseudo-gradient vector is uploaded to a central server per communication round. The server aggregates User updates using sample-weighted Federated Averaging (FedAvg) with momentum and gradient clipping. All experiments in this paper are conducted on fully synthetic data generated from a known ground-truth parameter set with region-specific heterogeneity, enabling controlled evaluation of federated convergence behaviour. Simulation results across heterogeneous Users show consistent convergence of the average negative log-likelihood, with the aggregated gradient norm decreasing as User scale increases. Furthermore, the federated update mechanism provides a natural participation incentive: Users who register their local inspection datasets on a shared technical-standard platform receive in return the periodically updated global benchmark parameters -- information that cannot be obtained from local data alone -- thereby enabling evidence-based life-cycle planning without surrendering data sovereignty.

</details>


### [24] [Controllable Exploration in Hybrid-Policy RLVR for Multi-Modal Reasoning](https://arxiv.org/abs/2602.20197)
*Zhuoxu Huang,Mengxi Jia,Hao Sun,Xuelong Li,Jungong Han*

Main category: cs.LG

TL;DR: 针对MLLM强化学习中的熵崩溃和策略退化问题，提出CalibRL混合策略框架，通过分布感知优势加权和专家引导的可控探索机制，在八个基准测试中实现稳定改进。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在强化学习训练中面临状态空间巨大、奖励稀疏导致的熵崩溃、策略退化及次优行为过度利用等问题，现有探索策略效率低下，需要保持生产性随机性。

Method: CalibRL框架包含两个核心机制：1）分布感知优势加权，按群体稀有度缩放更新以校准分布并保留探索；2）不对称LeakyReLU激活函数，以专家知识为校准基准调节过度自信更新，同时保持纠正方向。

Result: 在八个领域内外基准测试中表现持续改进，通过在线采样估计策略分布，有效缓解模型策略与专家轨迹间的分布不匹配，实现探索与利用的稳定平衡。

Conclusion: 该可控混合策略RLVR训练方法显著提升了MLLM推理能力，为强化学习探索问题提供了有效解决方案，代码已开源。

Abstract: Reinforcement Learning with verifiable rewards (RLVR) has emerged as a primary learning paradigm for enhancing the reasoning capabilities of multi-modal large language models (MLLMs). However, during RL training, the enormous state space of MLLM and sparse rewards often leads to entropy collapse, policy degradation, or over-exploitation of suboptimal behaviors. This necessitates an exploration strategy that maintains productive stochasticity while avoiding the drawbacks of uncontrolled random sampling, yielding inefficient exploration. In this paper, we propose CalibRL, a hybrid-policy RLVR framework that supports controllable exploration with expert guidance, enabled by two key mechanisms. First, a distribution-aware advantage weighting scales updates by group rareness to calibrate the distribution, therefore preserving exploration. Meanwhile, the asymmetric activation function (LeakyReLU) leverages the expert knowledge as a calibration baseline to moderate overconfident updates while preserving their corrective direction. CalibRL increases policy entropy in a guided manner and clarifies the target distribution by estimating the on-policy distribution through online sampling. Updates are driven by these informative behaviors, avoiding convergence to erroneous patterns. Importantly, these designs help alleviate the distributional mismatch between the model's policy and expert trajectories, thereby achieving a more stable balance between exploration and exploitation. Extensive experiments across eight benchmarks, including both in-domain and out-of-domain settings, demonstrate consistent improvements, validating the effectiveness of our controllable hybrid-policy RLVR training. Code is available at https://github.com/zhh6425/CalibRL.

</details>


### [25] [IMOVNO+: A Regional Partitioning and Meta-Heuristic Ensemble Framework for Imbalanced Multi-Class Learning](https://arxiv.org/abs/2602.20199)
*Soufiane Bacha,Laouni Djafri,Sahraoui Dhelim,Huansheng Ning*

Main category: cs.LG

TL;DR: 提出IMOVNO+框架解决多分类任务中的类别不平衡、重叠与噪声问题，通过数据级（条件概率分区+智能过采样）和算法级（集成剪枝）优化显著提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现有方法在多分类场景下对类别不平衡/重叠/噪声问题处理不足：二值化方法忽略全局类间依赖，传统聚类无法捕捉分布形态，集成学习难以融合弱分类器导致泛化性差

Method: 双层框架：1) 数据层：用条件概率量化样本信息量→划分核心/重叠/噪声区域→Z-score与大跳间隙距离去噪→多正则化过采样控制合成样本 proximity；2) 算法层：元启发式剪枝集成分类器削弱弱学习器影响

Result: 35个数据集测试显示：多分类任务G-mean提升37-57%、F1-score 25-44%、精确率25-39%、召回率26-43%；二分类任务性能提升14-39%且接近100%准确率

Conclusion: IMOVNO+有效应对数据稀缺与不平衡问题，通过提升数据质量与算法鲁棒性实现跨任务泛化性能突破

Abstract: Class imbalance, overlap, and noise degrade data quality, reduce model reliability, and limit generalization. Although widely studied in binary classification, these issues remain underexplored in multi-class settings, where complex inter-class relationships make minority-majority structures unclear and traditional clustering fails to capture distribution shape. Approaches that rely only on geometric distances risk removing informative samples and generating low-quality synthetic data, while binarization approaches treat imbalance locally and ignore global inter-class dependencies. At the algorithmic level, ensembles struggle to integrate weak classifiers, leading to limited robustness. This paper proposes IMOVNO+ (IMbalance-OVerlap-NOise+ Algorithm-Level Optimization), a two-level framework designed to jointly enhance data quality and algorithmic robustness for binary and multi-class tasks. At the data level, first, conditional probability is used to quantify the informativeness of each sample. Second, the dataset is partitioned into core, overlapping, and noisy regions. Third, an overlapping-cleaning algorithm is introduced that combines Z-score metrics with a big-jump gap distance. Fourth, a smart oversampling algorithm based on multi-regularization controls synthetic sample proximity, preventing new overlaps. At the algorithmic level, a meta-heuristic prunes ensemble classifiers to reduce weak-learner influence. IMOVNO+ was evaluated on 35 datasets (13 multi-class, 22 binary). Results show consistent superiority over state-of-the-art methods, approaching 100% in several cases. For multi-class data, IMOVNO+ achieves gains of 37-57% in G-mean, 25-44% in F1-score, 25-39% in precision, and 26-43% in recall. In binary tasks, it attains near-perfect performance with improvements of 14-39%. The framework handles data scarcity and imbalance from collection and privacy limits.

</details>


### [26] [Multimodal Crystal Flow: Any-to-Any Modality Generation for Unified Crystal Modeling](https://arxiv.org/abs/2602.20210)
*Kiyoung Seong,Sungsoo Ahn,Sehui Han,Changyoung Park*

Main category: cs.LG

TL;DR: 提出MCFlow统一框架，用独立时间变量的多模态流模型实现晶体结构预测和生成任务，在标准Transformer中引入成分对称性感知的原子排序和层次置换增强，在MP-20和MPTS-52基准测试中达到与专用模型相当的性能


<details>
  <summary>Details</summary>
Motivation: 现有晶体生成模型任务特定化，缺乏跨任务共享表示的统一框架

Method: 设计多模态晶体流模型，通过原子类型和结构独立时间变量实现多任务推理轨迹；在Transformer中引入成分对称性感知的原子排序与层次置换增强，注入结晶学先验

Result: 在MP-20和MPTS-52基准测试上，MCFlow在多个晶体生成任务中达到与专用基线模型相当的性能水平

Conclusion: MCFlow成功建立了统一的多模态生成框架，证明了跨晶体任务共享表示的可行性，为材料设计提供了更灵活的基础模型

Abstract: Crystal modeling spans a family of conditional and unconditional generation tasks across different modalities, including crystal structure prediction (CSP) and \emph{de novo} generation (DNG). While recent deep generative models have shown promising performance, they remain largely task-specific, lacking a unified framework that shares crystal representations across different generation tasks. To address this limitation, we propose \emph{Multimodal Crystal Flow (MCFlow)}, a unified multimodal flow model that realizes multiple crystal generation tasks as distinct inference trajectories via independent time variables for atom types and crystal structures. To enable multimodal flow in a standard transformer model, we introduce a composition- and symmetry-aware atom ordering with hierarchical permutation augmentation, injecting strong compositional and crystallographic priors without explicit structural templates. Experiments on the MP-20 and MPTS-52 benchmarks show that MCFlow achieves competitive performance against task-specific baselines across multiple crystal generation tasks.

</details>


### [27] [Exploring Anti-Aging Literature via ConvexTopics and Large Language Models](https://arxiv.org/abs/2602.20224)
*Lana E. Yeganova,Won G. Kim,Shubo Tian,Natalie Xie,Donald C. Comeau,W. John Wilbur,Zhiyong Lu*

Main category: cs.LG

TL;DR: 提出一种基于凸优化的聚类算法，通过选择数据范例生成稳定且可解释的生物医学主题，在1.2万篇衰老相关文献中验证效果优于K-means、LDA等方法


<details>
  <summary>Details</summary>
Motivation: 生物医学文献快速扩张导致知识组织与趋势检测困难，现有K-means、LDA等方法对初始化敏感易陷入局部最优，影响结果可重复性

Method: 重新构建凸优化聚类算法，通过选择数据范例生成细粒度主题，确保全局最优解

Result: 在1.2万篇PubMed衰老文献中识别出分子机制、膳食补充剂、运动、肠道菌群等可解释主题，经医学专家验证有效性

Conclusion: 该方法具有优越的可重复性和可解释性，为开发可扩展的Web端知识发现工具奠定基础

Abstract: The rapid expansion of biomedical publications creates challenges for organizing knowledge and detecting emerging trends, underscoring the need for scalable and interpretable methods. Common clustering and topic modeling approaches such as K-means or LDA remain sensitive to initialization and prone to local optima, limiting reproducibility and evaluation. We propose a reformulation of a convex optimization based clustering algorithm that produces stable, fine-grained topics by selecting exemplars from the data and guaranteeing a global optimum. Applied to about 12,000 PubMed articles on aging and longevity, our method uncovers topics validated by medical experts. It yields interpretable topics spanning from molecular mechanisms to dietary supplements, physical activity, and gut microbiota. The method performs favorably, and most importantly, its reproducibility and interpretability distinguish it from common clustering approaches, including K-means, LDA, and BERTopic. This work provides a basis for developing scalable, web-accessible tools for knowledge discovery.

</details>


### [28] [Quantitative Approximation Rates for Group Equivariant Learning](https://arxiv.org/abs/2602.20370)
*Jonathan W. Siegel,Snir Hordan,Hannah Lawrence,Ali Syed,Nadav Dym*

Main category: cs.LG

TL;DR: This paper derives quantitative approximation rates for group-equivariant neural networks, proving they are as expressive as standard ReLU MLPs for learning equivariant functions, thus showing that hard-coding symmetries doesn't sacrifice approximation power.


<details>
  <summary>Details</summary>
Motivation: While universal approximation is established for both standard and equivariant networks, quantitative rates are known only for standard ReLU networks on α-Hölder functions. For equivariant models, such rates are scarce despite much interest, creating a gap in theoretical understanding.

Method: The authors derive quantitative approximation rates for several prominent architectures: Deep Sets (permutation-invariant), Sumformer/Transformer (permutation-equivariant), frame-averaged networks (invariant to permutations and rigid motions), and bi-Lipschitz invariant models.

Result: They prove that equally-sized ReLU MLPs and equivariant architectures have equal expressive power over equivariant functions, meaning hard-coding equivariance doesn't reduce approximation capability.

Conclusion: Hard-coding equivariance priors into neural networks does not result in a loss of expressivity or approximation power, validating the theoretical foundation of equivariant architectures.

Abstract: The universal approximation theorem establishes that neural networks can approximate any continuous function on a compact set. Later works in approximation theory provide quantitative approximation rates for ReLU networks on the class of $α$-Hölder functions $f: [0,1]^N \to \mathbb{R}$. The goal of this paper is to provide similar quantitative approximation results in the context of group equivariant learning, where the learned $α$-Hölder function is known to obey certain group symmetries. While there has been much interest in the literature in understanding the universal approximation properties of equivariant models, very few quantitative approximation results are known for equivariant models.
  In this paper, we bridge this gap by deriving quantitative approximation rates for several prominent group-equivariant and invariant architectures. The architectures that we consider include: the permutation-invariant Deep Sets architecture; the permutation-equivariant Sumformer and Transformer architectures; joint invariance to permutations and rigid motions using invariant networks based on frame averaging; and general bi-Lipschitz invariant models. Overall, we show that equally-sized ReLU MLPs and equivariant architectures are equally expressive over equivariant functions. Thus, hard-coding equivariance does not result in a loss of expressivity or approximation power in these models.

</details>


### [29] [Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs](https://arxiv.org/abs/2602.21198)
*Yining Hong,Huang Huang,Manling Li,Li Fei-Fei,Jiajun Wu,Yejin Choi*

Main category: cs.LG

TL;DR: 本文提出反思性测试时规划(RTTP)，通过三种反思模式（行动中的反思、行动后的反思和回顾性反思）解决具身大语言模型无法从错误中学习的问题，在两项基准测试中显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前具身大语言模型赋予机器人高层任务推理能力，但缺乏反思机制，导致错误重复发生而非积累为经验。受人类反思实践者启发，旨在让机器人从独立试验转向经验累积。

Method: 提出反思性测试时规划框架，包含：1) 行动中的反思—通过测试时扩展生成并评估多个候选动作；2) 行动后的反思—基于外部反馈更新反思模型和动作策略；3) 回顾性反思—利用后见之明重新评估早期决策并分配长期信用。

Result: 在Long-Horizon Household和MuJoCo Cupboard Fitting基准测试中性能显著提升，消融实验验证了不同反思模式的互补作用，真实机器人试验展示了通过反思实现行为修正。

Conclusion: 该方法成功使机器人具备反思能力，将独立试验转化为可累积的学习经验，为具身智能体的持续学习提供了有效框架。

Abstract: Embodied LLMs endow robots with high-level task reasoning, but they cannot reflect on what went wrong or why, turning deployment into a sequence of independent trials where mistakes repeat rather than accumulate into experience. Drawing upon human reflective practitioners, we introduce Reflective Test-Time Planning, which integrates two modes of reflection: \textit{reflection-in-action}, where the agent uses test-time scaling to generate and score multiple candidate actions using internal reflections before execution; and \textit{reflection-on-action}, which uses test-time training to update both its internal reflection model and its action policy based on external reflections after execution. We also include retrospective reflection, allowing the agent to re-evaluate earlier decisions and perform model updates with hindsight for proper long-horizon credit assignment. Experiments on our newly-designed Long-Horizon Household benchmark and MuJoCo Cupboard Fitting benchmark show significant gains over baseline models, with ablative studies validating the complementary roles of reflection-in-action and reflection-on-action. Qualitative analyses, including real-robot trials, highlight behavioral correction through reflection.

</details>


### [30] [Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking](https://arxiv.org/abs/2602.21196)
*Ravi Ghadia,Maksim Abraham,Sergei Vorobyov,Max Ryabinin*

Main category: cs.LG

TL;DR: UPipe introduces fine-grained attention head-level chunking to reduce activation memory by 87.5% in 32B Transformers, enabling 5M token context length for Llama3-8B on single 8×H100 node while maintaining training speed.


<details>
  <summary>Details</summary>
Motivation: Current context parallelism methods (e.g., Ring Attention, DeepSpeed Ulysses) prioritize scaling over context dimension but neglect memory efficiency, limiting supported sequence lengths; advanced techniques like activation offloading extend context length at the cost of training throughput.

Method: Proposes UPipe, a context parallelism technique that performs fine-grained chunking at the attention head level to significantly reduce self-attention activation memory usage.

Result: Reduces intermediate tensor memory in attention layers by up to 87.5% for 32B Transformers; supports 5M token context length when training Llama3-8B on a single 8×H100 node, improving prior methods by over 25% while matching their training speed.

Conclusion: UPipe breaks the activation memory barrier for long-sequence processing, enabling much longer context lengths without sacrificing training throughput.

Abstract: Efficiently processing long sequences with Transformer models usually requires splitting the computations across accelerators via context parallelism. The dominant approaches in this family of methods, such as Ring Attention or DeepSpeed Ulysses, enable scaling over the context dimension but do not focus on memory efficiency, which limits the sequence lengths they can support. More advanced techniques, such as Fully Pipelined Distributed Transformer or activation offloading, can further extend the possible context length at the cost of training throughput. In this paper, we present UPipe, a simple yet effective context parallelism technique that performs fine-grained chunking at the attention head level. This technique significantly reduces the activation memory usage of self-attention, breaking the activation memory barrier and unlocking much longer context lengths. Our approach reduces intermediate tensor memory usage in the attention layer by as much as 87.5$\%$ for 32B Transformers, while matching previous context parallelism techniques in terms of training speed. UPipe can support the context length of 5M tokens when training Llama3-8B on a single 8$\times$H100 node, improving upon prior methods by over 25$\%$.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [31] [Construction of a Neural Network with Temperature-Dependent Recall Patterns](https://arxiv.org/abs/2602.20620)
*Munetaka Sasaki*

Main category: cond-mat.dis-nn

TL;DR: 提出一个温度依赖的记忆模型，通过在全连接图和稀疏图中嵌入不同模式，实现温度调控的模式回忆切换，并观察到一级相变现象。


<details>
  <summary>Details</summary>
Motivation: 实现温度变化导致回忆模式改变的行为，利用不同图结构对热涨落的抵抗能力差异来构建温度敏感的记忆系统。

Method: 将两种模式分别嵌入全连接图和稀疏图，通过调节权重比例；采用平衡蒙特卡洛模拟观察温度依赖的回忆变化，并用退火模拟研究低温下的动力学行为。

Result: 模拟证实温度依赖的模式回忆变化确实存在；系统在模式切换时经历一级相变；退火模拟显示低温下自由能垒过高会导致无法回忆稀疏图模式。

Conclusion: 该模型成功展示了温度调控的模式回忆功能及其相变特性，同时揭示了自由能垒对低温记忆提取的限制。

Abstract: We present a simple model that recalls two different patterns depending on the temperature. To realize a change in recall pattern due to temperature change, we embed two patterns to different graphs: the first pattern into a fully connected graph and the second pattern into a sparse graph. Because a fully connected graph is more resistant to thermal fluctuations than a sparse graph, we can realize a change in recall pattern by tuning relative weights of the two patterns properly. We demonstrate by equilibrium Monte-Carlo simulations that such a temperature-dependent change in recall patterns does occur in our model. Simulation results strongly indicate that the system undergoes a first-order phase transition when the change in recall patterns occurs. It is also demonstrated by annealing simulations that the system fails to recall the pattern embedded in the sparse graph at low temperatures if the free-energy barrier is too high to overcome within the given simulation timescale.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [32] [Spectral Decimation of Quantum Many-Body Hamiltonians](https://arxiv.org/abs/2602.20256)
*Feng He,Arthur Hutsalyuk,Giuseppe Mussardo,Andrea Stampiggi*

Main category: cond-mat.stat-mech

TL;DR: 提出谱抽取法这一新理论工具，通过分析量子多体系统的混合谱来量化涌现对称性，能有效区分混沌动力学、统计混合和涌现可积性。


<details>
  <summary>Details</summary>
Motivation: 开发系统性的谱抽取理论，为统计混合谱中的涌现对称性提供定量探针。

Method: 基于统计混合的解析描述，推导特征对称扇区(CSS)的显式表达式，应用于希尔伯特空间碎裂和MBL两种典型体系。

Result: CSS维度是对称扇区的加权平均；在碎裂体系中重现混合预测；在无序海森堡链中发现CSS收缩揭示可积性涌现，并提出特征对称熵(CSE)作为有限尺寸标度观测量。

Conclusion: 谱抽取法是一种可控、无偏且计算廉价的诊断工具，可揭示多体谱中的隐藏结构。

Abstract: We develop a systematic theory of spectral decimation for quantum many-body Hamiltonians and show that it provides a quantitative probe of emergent symmetries in statistically mixed spectra. Building on an analytical description of statistical mixtures, we derive an explicit expression for the size of a characteristic symmetry sector (CSS), defined as the largest subsequence of levels exhibiting non-Poissonian correlations. The CSS dimension is shown to be the size-biased average of the underlying symmetry sectors, establishing a direct link between spectral statistics and Hilbert-space structure. We apply this framework to two paradigmatic settings: Hilbert-space fragmentation and disorder-induced many-body localization (MBL). In fragmented systems, the CSS reproduces the mixture prediction and isolates correlated subsectors even when the full spectrum appears nearly Poissonian. In the disordered Heisenberg chain, spectral decimation reveals the gradual emergence of integrability through a shrinking CSS, whose statistics exhibit signatures consistent with local integrals of motion. We introduce a characteristic symmetry entropy (CSE) as a finite-size scaling observable and extract, within accessible system sizes, the crossover exponents. Our results establish spectral decimation as a controlled, unbiased and computationally inexpensive diagnostic of hidden structure in many-body spectra, capable of distinguishing between chaotic dynamics, statistical mixtures, and emergent integrability.

</details>


### [33] [Hyperuniformity in active fluids reshape nucleation and capillary-wave dynamics](https://arxiv.org/abs/2602.20308)
*Raphaël Maire*

Main category: cond-mat.stat-mech

TL;DR: 研究非平衡超均匀流体中的成核现象，发现其由非平衡准势能而非可逆功控制，成核概率不再分离为表面和体积贡献，且毛细波导致详细平衡破缺。


<details>
  <summary>Details</summary>
Motivation: 典型活性流体中的成核常呈现类平衡特征，但当大尺度涨落被强烈抑制时会出现显著偏离，需探究非平衡超均匀流体中的成核机制。

Method: 将完整密度场动力学投影到相关集体变量上，分析非平衡超均匀流体的成核过程。

Result: 1. 成核由非平衡准势能而非可逆功控制；2. 因超均匀涨落减弱，成核概率不再分离为表面和体积贡献；3. 毛细波导致非互易动力学驱动的详细平衡破缺。

Conclusion: 建立的理论框架可推广至常规活性流体，用于识别非平衡特征。

Abstract: While nucleation in typical active and driven fluids often appears equilibrium-like, striking departures emerge when large-scale fluctuations are strongly suppressed. Here, we investigate nucleation in nonequilibrium hyperuniform fluids by projecting the full density-field dynamics onto relevant collective variables. We demonstrate that nucleation is governed by a nonequilibrium quasi-potential rather than the reversible work of formation. Surprisingly, because of the reduced hyperuniform fluctuations, the nucleation probability no longer separates into the usual surface and volume contributions. Furthermore, accounting for capillary waves reveals a clear breakdown of detailed balance driven by nonreciprocal dynamics. More broadly, our framework can be readily extended to identify nonequilibrium signatures in conventional active fluids.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [34] [Geometric investigation of chaos unfolding in Hamiltonian systems](https://arxiv.org/abs/2602.20682)
*L. Salasnich,F. Sattin*

Main category: nlin.CD

TL;DR: 该研究重新审视哈密顿动力学中混沌的几何方法，发现混沌轨道中相邻轨迹的指数发散并非连续发生，而是在每个转折点处呈现离散的突变式增长，这一现象可通过参数共振理论（特别是马丢方程）解释。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为混沌系统中轨迹发散是连续指数过程，但本研究旨在通过雅可比-列维-奇维塔方程（JLCE）揭示其更精细的离散机制。

Method: 采用解析与数值相结合的方法：对低维动力系统进行数值模拟，分析轨迹在能量允许区域边界处（转折点）的散射行为，并关联参数共振理论（马丢方程）。

Result: 发现混沌轨道的轨迹分离在转折点处发生急剧、离散的跳跃式增长，而非连续指数发散；混沌特性由边界散射细节决定。

Conclusion: 混沌的本质源于边界散射的离散动力学过程，参数共振理论（马丢方程）为此提供了理论框架，修正了传统连续发散模型。

Abstract: In this work we revisit the geometric approach to chaos in Hamiltonian dynamics, by means of the Jacobi-Levi-Civita equation (JLCE). We inspect numerically two low-dimensional dynamical systems; show that, along chaotic orbits, the exponential divergence between nearby trajectories quantified by the JLCE does not unfold in a continuous manner, rather is closer to a multiplicative discrete process: in correspondence of each turning point, where the trajectory bounces away from the boundary of the energetically allowed region, the relative separation increases sharply and abruptly. We highlight through analytical and numerical arguments that the chaotic rather than regular nature of the trajectory is determined by the details of the scattering with the boundary, and interpret these results in terms of parametric resonance theory, and specifically the Mathieu equation.

</details>


<div id='nlin.AO'></div>

# nlin.AO [[Back]](#toc)

### [35] [Half a century of the theory of synchronization](https://arxiv.org/abs/2602.20505)
*Yoshiki Kuramoto*

Main category: nlin.AO

TL;DR: 该综述回顾了耦合振荡器与同步理论过去50年的发展，重点介绍作者三项核心贡献：Kuramoto-Sivashinsky方程（时空混沌）、Kuramoto模型（同步相变）及嵌合体态发现，强调这些成果均源于复杂Ginzburg-Landau方程的相位约化方法，并总结动力学约化方法对该领域的关键作用。


<details>
  <summary>Details</summary>
Motivation: 系统梳理耦合振荡器与同步理论近50年的演进脉络，通过个人代表性工作阐明该领域的关键突破与发展逻辑，强调数学方法（如相位约化）在推动学科深度探索中的核心地位。

Method: 以复杂Ginzburg-Landau方程（1974年与合作者从反应扩散模型导出）为理论基础，采用相位约化、中心流形约化等动力学约化方法，构建可解析求解的简化模型。

Result: 1) 导出描述时空混沌的Kuramoto-Sivashinsky方程；2) 建立可解析求解的同步相变Kuramoto模型；3) 发现非局域耦合振荡器中相干-非相干共存的嵌合体态；4) 阐明上述成果对后续研究的深远影响。

Conclusion: 相位约化等动力学约化方法是揭示耦合系统复杂行为本质的核心工具，其跨尺度建模思想将持续推动非线性动力学与同步理论的深化发展。

Abstract: This review offers a retrospective of the development of the theory of coupled oscillators and synchronization over the past half century. Among the various works made by myself during this period, the following three specific works will be focused on, serving as some key points to illustrate the field's evolution. They are the derivation of (1) a simple partial differential equation exhibiting spatio-tempoeral chaos (Kuramoto-Sivashinsky equaiton), (2) a solvable mathematical model describing synchronization phase transition (Kuramoto model), and the discovery of (3) coexistence of coherence and incoherence in nonlocally coupled oscillators (chimera states). It is emphasized that all these works resulted fron the phase reduction of the complex Ginzburg-Landau equation (or its variants), the equation which was derived with a coworker in 1974 from a certain reaction-diffusion model. A quick overview will also be made on how the above three works influenced the subsequent development of the field of coupled oscillators and synchronization. Finally, a few comments will be made on how the methods of dynamical reduction, such as the center-manifold reduction and phase reduction, are crucial for exploring this field in depth. This article is a largely faithful reproduction of the content presented in my award lecture.

</details>
