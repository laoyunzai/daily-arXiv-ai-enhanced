<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 57]
- [cs.LG](#cs.LG) [Total: 133]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 9]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [quant-ph](#quant-ph) [Total: 62]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 6]
- [nlin.AO](#nlin.AO) [Total: 3]
- [physics.data-an](#physics.data-an) [Total: 1]
- [nlin.CD](#nlin.CD) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Online parameter estimation for the Crazyflie quadcopter through an EM algorithm](https://arxiv.org/abs/2601.17009)
*Yanhua Zhao*

Main category: cs.AI

TL;DR: 本文研究四旋翼无人机系统的噪声影响，采用扩展卡尔曼滤波进行状态估计，基于随机微分方程实现线性二次高斯控制，并使用期望最大化算法进行参数估计，发现在线参数估计比离线估计具有稍大的收敛值范围。


<details>
  <summary>Details</summary>
Motivation: 无人机在灾害搜救、航拍、农业植保、物流运输等领域应用价值显著，尤其在地震后基础设施损毁区域具有不可替代的侦察与作业能力。为保障四旋翼无人机在复杂环境下的可靠飞行，需深入研究随机噪声对系统的影响及鲁棒控制方法。

Method: 1) 在四旋翼动力学模型中注入随机噪声；2) 采用扩展卡尔曼滤波(EKF)融合含噪传感器观测数据进行状态估计；3) 基于随机微分方程(SDE)框架设计线性二次高斯(LQG)控制器；4) 应用期望最大化(EM)算法进行系统参数辨识，并对比分析离线与在线两种估计策略。

Result: 成功实现了含噪四旋翼系统的状态估计与闭环控制，获得了离线参数估计和在线参数估计两组实验结果。定量分析表明，在线参数估计的收敛值波动范围略大于离线估计方法。

Conclusion: 研究验证了EKF-LQG框架在无人机噪声抑制中的有效性，EM算法可实现准确的参数辨识。在线估计方法虽收敛范围稍大，但具备实时更新能力，为无人机在动态不确定环境下的自适应控制提供了可行方案，增强了系统的鲁棒性与可靠性。

Abstract: Drones are becoming more and more popular nowadays. They are small in size, low in cost, and reliable in operation. They contain a variety of sensors and can perform a variety of flight tasks, reaching places that are difficult or inaccessible for humans. Earthquakes damage a lot of infrastructure, making it impossible for rescuers to reach some areas. But drones can help. Many amateur and professional photographers like to use drones for aerial photography. Drones play a non-negligible role in agriculture and transportation too. Drones can be used to spray pesticides, and they can also transport supplies. A quadcopter is a four-rotor drone and has been studied in this paper. In this paper, random noise is added to the quadcopter system and its effects on the drone system are studied. An extended Kalman filter has been used to estimate the state based on noisy observations from the sensor. Based on a SDE system, a linear quadratic Gaussian controller has been implemented. The expectation maximization algorithm has been applied for parameter estimation of the quadcopter. The results of offline parameter estimation and online parameter estimation are presented. The results show that the online parameter estimation has a slightly larger range of convergence values than the offline parameter estimation.

</details>


### [2] [Interpreting Agentic Systems: Beyond Model Explanations to System-Level Accountability](https://arxiv.org/abs/2601.17168)
*Judy Zhu,Dhari Gandhi,Himanshu Joshi,Ahmad Rezaie Mianroodi,Sedef Akinli Kocak,Dhanesh Ramachandran*

Main category: cs.AI

TL;DR: This paper examines the limitations of existing interpretability methods for agentic AI systems (autonomous LLM-based systems with goal-directed behaviors) and proposes new directions for developing temporal, context-aware interpretability techniques specifically designed to address safety challenges like goal misalignment and compounding errors across the agent lifecycle.


<details>
  <summary>Details</summary>
Motivation: Agentic systems introduce unique AI safety challenges (goal misalignment, compounding decision errors, coordination risks) that require built-in interpretability for traceability and accountability, but current static model interpretability techniques are inadequate for their temporal dynamics and context-dependent behaviors.

Method: Assesses suitability and limitations of existing interpretability methods when applied to agentic systems, identifying specific gaps in their capacity to provide meaningful insight into agent decision-making processes.

Result: Identifies critical gaps in current interpretability approaches and proposes future directions for developing agentic-specific interpretability techniques that can provide oversight mechanisms across the entire agent lifecycle (goal formation, environmental interaction, outcome evaluation).

Conclusion: New interpretability methods specifically designed for agentic systems are essential to ensure their safe and accountable deployment, requiring temporal, context-aware approaches that can trace autonomous behaviors across multi-step planning and environmental interactions.

Abstract: Agentic systems have transformed how Large Language Models (LLMs) can be leveraged to create autonomous systems with goal-directed behaviors, consisting of multi-step planning and the ability to interact with different environments. These systems differ fundamentally from traditional machine learning models, both in architecture and deployment, introducing unique AI safety challenges, including goal misalignment, compounding decision errors, and coordination risks among interacting agents, that necessitate embedding interpretability and explainability by design to ensure traceability and accountability across their autonomous behaviors. Current interpretability techniques, developed primarily for static models, show limitations when applied to agentic systems. The temporal dynamics, compounding decisions, and context-dependent behaviors of agentic systems demand new analytical approaches. This paper assesses the suitability and limitations of existing interpretability methods in the context of agentic systems, identifying gaps in their capacity to provide meaningful insight into agent decision-making. We propose future directions for developing interpretability techniques specifically designed for agentic systems, pinpointing where interpretability is required to embed oversight mechanisms across the agent lifecycle from goal formation, through environmental interaction, to outcome evaluation. These advances are essential to ensure the safe and accountable deployment of agentic AI systems.

</details>


### [3] [High-Fidelity Longitudinal Patient Simulation Using Real-World Data](https://arxiv.org/abs/2601.17310)
*Yu Akagi,Tomohisa Seki,Hiromasa Ito,Toru Takiguchi,Kazuhiko Ohe,Yoshimasa Kawazoe*

Main category: cs.AI

TL;DR: 一个基于2亿多条临床记录训练的生成式AI模型能够高保真地模拟患者轨迹，匹配真实世界数据模式并准确预测未来事件。


<details>
  <summary>Details</summary>
Motivation: 模拟是探索不确定性的强大工具，在临床医学中具有变革性潜力，包括个性化治疗规划和虚拟临床试验，但由于生物和社会文化影响的复杂性，模拟患者轨迹具有挑战性。

Method: 开发了一个生成式模拟模型，以患者历史记录为输入，合成精细且真实的未来轨迹，该模型在超过2亿条临床记录上进行了预训练。

Result: 该模型生成了高保真的未来时间线，与真实患者数据中的事件发生率、实验室检查结果和时间动态高度匹配；在不同结果和时间范围内，观察值与预期值的比率始终接近1.0，准确估计了未来事件概率。

Conclusion: 揭示了电子健康记录中真实世界数据的未开发价值，并为临床护理的计算机模拟引入了可扩展框架。

Abstract: Simulation is a powerful tool for exploring uncertainty. Its potential in clinical medicine is transformative and includes personalized treatment planning and virtual clinical trials. However, simulating patient trajectories is challenging because of complex biological and sociocultural influences. Here, we show that real-world clinical records can be leveraged to empirically model patient timelines. We developed a generative simulator model that takes a patient's history as input and synthesizes fine-grained, realistic future trajectories. The model was pretrained on more than 200 million clinical records. It produced high-fidelity future timelines, closely matching event occurrence rates, laboratory test results, and temporal dynamics in real patient future data. It also accurately estimated future event probabilities, with observed-to-expected ratios consistently near 1.0 across diverse outcomes and time horizons. Our results reveal the untapped value of real-world data in electronic health records and introduce a scalable framework for in silico modeling of clinical care.

</details>


### [4] [Phase Transition for Budgeted Multi-Agent Synergy](https://arxiv.org/abs/2601.17311)
*Bang Liu,Linglong Kong,Jian Pei*

Main category: cs.AI

TL;DR: 该论文建立了一个可校准的理论框架，通过有限上下文窗口、有损通信和智能体间共享错误三个约束，预测多智能体系统在固定推理预算下的三种行为模式（有效、饱和、崩溃），并给出了计算资源分配规则和性能相变边界。


<details>
  <summary>Details</summary>
Motivation: 现代多智能体系统虽能提升可靠性，但在固定预算下常表现出不可预测的性能饱和甚至崩溃。现有研究缺乏统一理论解释这些现象，无法指导系统设计和资源分配。本文旨在填补这一空白，建立可量化的预测框架。

Method: 提出基于三个核心约束的极简理论：1) 叶智能体的计算-性能缩放指数β；2) 通信保真度曲线γ(m)；3) 错误相关性ρ；4) 上下文窗口W决定的扇入限制。针对二分类多数表决任务，在深度b叉树上分析有损通信和相关输入下的相变行为，并推导出组织指数s。

Result: 1) 发现单一标量α_ρ决定信号是被放大到非平凡固定点还是衰减至随机水平；2) 证明预算协同效应（优于单智能体）恰好发生在s>β时，给出闭式资源分配规则；3) 提出混合深度刻画饱和现象；4) 为星型、链式和树形结构提供闭式风险表达式；5) 合成实验验证相变边界，并与近期大规模LLM智能体研究中的瓶颈现象一致。

Conclusion: 该理论首次统一解释了多智能体系统的性能相变机制，揭示了通信保真度、错误相关性和组织层级间的根本权衡，为在有限预算下设计高效智能体系统提供了可量化的预测工具和优化准则。

Abstract: Multi-agent systems can improve reliability, yet under a fixed inference budget they often help, saturate, or even collapse. We develop a minimal and calibratable theory that predicts these regimes from three binding constraints of modern agent stacks: finite context windows, lossy inter-agent communication, and shared failures among similar agents. Each leaf agent is summarized by a compute-performance scaling exponent $β$; communication is captured by a message-length fidelity curve $γ(m)$; dependence is captured by an effective shared-error correlation $ρ$; and a context window $W$ imposes hard fan-in limits that make hierarchy necessary. For binary success/failure tasks with majority aggregation, we prove a sharp phase transition for deep $b$-ary trees with correlated inputs and lossy communication: a single scalar $α_ρ$ (combining $γ(m)$, $ρ$, and fan-in $b$) determines whether weak signal is amplified to a nontrivial fixed point or washed out to chance. In the amplifying regime, we derive an organization exponent $s$ and show that budgeted synergy, i.e., outperforming the best single agent under the same total budget, occurs exactly when $s>β$, yielding closed-form compute allocation rules and explicit budget thresholds. We further characterize saturation via a mixing depth and provide a conservative clipped predictor that remains accurate across growth and saturation. A continuous-performance warm-up gives closed-form risks for star, chain, and tree organizations, making correlation- and communication-induced floors explicit and exposing the core design trade-offs in a smooth setting. Finally, we validate the predicted phase boundaries in controlled synthetic simulations and show how the same mechanisms explain the dominant bottlenecks reported in recent large-scale matched-budget studies of LLM agent-system scaling.

</details>


### [5] [TheoremForge: Scaling up Formal Data Synthesis with Low-Budget Agentic Workflow](https://arxiv.org/abs/2601.17332)
*Yicheng Tao,Hongteng Xu*

Main category: cs.AI

TL;DR: TheoremForge is a cost-effective pipeline for synthesizing formal mathematics data that decomposes formalization into five sub-tasks and uses a decoupled extraction strategy to recover training signals from failed trajectories, achieving 12.6% verified rate at only $0.48 per successful trajectory.


<details>
  <summary>Details</summary>
Motivation: The high cost of agentic workflows in formal mathematics hinders large-scale data synthesis and exacerbates the scarcity of open-source corpora.

Method: Introduces TheoremForge, a pipeline that decomposes formalization into five sub-tasks (statement formalization, proof generation, premise selection, proof correction, and proof sketching) and implements a Decoupled Extraction Strategy to recover valid training signals from globally failed trajectories.

Result: Achieves a 12.6% Verified Rate (surpassing the 8.6% baseline) on a 2,000-problem benchmark at an average cost of $0.481 per successful trajectory using Gemini-3-Flash, with a 1.6× increase in data yield for proof generation compared to standard filtering.

Conclusion: TheoremForge establishes a scalable framework for constructing a data flywheel to train future expert models in formal mathematics.

Abstract: The high cost of agentic workflows in formal mathematics hinders large-scale data synthesis, exacerbating the scarcity of open-source corpora. To address this, we introduce \textbf{TheoremForge}, a cost-effective formal data synthesis pipeline that decomposes the formalization process into five sub-tasks, which are \textit{statement formalization}, \textit{proof generation}, \textit{premise selection}, \textit{proof correction} and \textit{proof sketching}. By implementing a \textit{Decoupled Extraction Strategy}, the workflow recovers valid training signals from globally failed trajectories, effectively utilizing wasted computation. Experiments on a 2,000-problem benchmark demonstrate that TheoremForge achieves a Verified Rate of 12.6\%, surpassing the 8.6\% baseline, at an average cost of only \textbf{\$0.481} per successful trajectory using Gemini-3-Flash. Crucially, our strategy increases data yield by \textbf{1.6$\times$} for proof generation compared to standard filtering. These results establish TheoremForge as a scalable framework for constructing a data flywheel to train future expert models. Our code is available \href{https://github.com/timechess/TheoremForge}{here}.

</details>


### [6] [The Relativity of AGI: Distributional Axioms, Fragility, and Undecidability](https://arxiv.org/abs/2601.17335)
*Angshul Majumdar*

Main category: cs.AI

TL;DR: 该论文将AGI形式化为一种分布化、资源受限的语义谓词，并证明：分布无关的AGI概念是未定义的，AGI性质会因任务分布的细微扰动而失效，跨任务族的泛化是有限界的，且AGI无法通过任何可计算程序（包括智能体自身）进行完备认证。


<details>
  <summary>Details</summary>
Motivation: 探讨AGI是否存在一个连贯的理论定义，以支持关于存在性、鲁棒性或自我验证的绝对性断言，从而为AGI的基础理论建立严格的数学框架。

Method: 将AGI公理化地形式化为分布化、资源受限的语义谓词，通过任务族、任务分布、性能泛函和显式资源预算进行索引；运用Rice定理和Gödel-Tarski论证等理论工具进行数学证明。

Result: 1) 通用性本质上是关系性的：不存在分布无关的AGI概念；2) 非不变性结果：任务分布的任意小扰动可通过悬崖集使AGI性质失效；3) 有限迁移保证：在有限资源下跨任务族的无界泛化被排除；4) AGI是非平凡语义性质，无法被任何可计算程序（包括智能体自身）完全认证。

Conclusion: 强分布无关的AGI主张并非错误而是缺乏定义，必须显式索引；人工智能的实证进展并不意味着可实现自我认证的通用智能，依赖内部自我认证的递归自我改进方案是不适定的。

Abstract: We study whether Artificial General Intelligence (AGI) admits a coherent theoretical definition that supports absolute claims of existence, robustness, or self-verification. We formalize AGI axiomatically as a distributional, resource-bounded semantic predicate, indexed by a task family, a task distribution, a performance functional, and explicit resource budgets. Under this framework, we derive four classes of results. First, we show that generality is inherently relational: there is no distribution-independent notion of AGI. Second, we prove non-invariance results demonstrating that arbitrarily small perturbations of the task distribution can invalidate AGI properties via cliff sets, precluding universal robustness. Third, we establish bounded transfer guarantees, ruling out unbounded generalization across task families under finite resources. Fourth, invoking Rice-style and Gödel--Tarski arguments, we prove that AGI is a nontrivial semantic property and therefore cannot be soundly and completely certified by any computable procedure, including procedures implemented by the agent itself. Consequently, recursive self-improvement schemes that rely on internal self-certification of AGI are ill-posed. Taken together, our results show that strong, distribution-independent claims of AGI are not false but undefined without explicit formal indexing, and that empirical progress in AI does not imply the attainability of self-certifying general intelligence.

</details>


### [7] [Are We Evaluating the Edit Locality of LLM Model Editing Properly?](https://arxiv.org/abs/2601.17343)
*Wei Liu,Haomei Xu,Hongkai Liu,Zhiying Deng,Ruixuan Li,Heng Huang,Yee Whye Teh,Wee Sun Lee*

Main category: cs.AI

TL;DR: This paper identifies fundamental flaws in current specificity evaluation metrics for LLM model editing and proposes a new evaluation protocol that is more sensitive and correlates better with specificity regularizers, enabling fine-grained discrimination between editing methods' knowledge preservation capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing specificity (edit locality) evaluation protocols for model editing are inadequate—they have conceptual issues, show weak correlation with specificity regularizers, and lack sufficient sensitivity to distinguish performance differences between editing methods.

Method: Proposes a constructive evaluation protocol that eliminates conflicts between open-ended LLM responses and determined answer assumptions, avoids query-independent fluency biases, and allows adjustable evaluation strictness within a near-continuous parameter space.

Result: Experiments across multiple LLMs, datasets, and editing methods demonstrate that the proposed metrics are more sensitive to changes in specificity regularizer strength, show strong correlation with them, and enable better discrimination of knowledge preservation capabilities across different methods.

Conclusion: The new evaluation protocol provides a more reliable and sensitive framework for assessing specificity in model editing, which should significantly improve the development and evaluation of future knowledge editing techniques.

Abstract: Model editing has recently emerged as a popular paradigm for efficiently updating knowledge in LLMs. A central desideratum of updating knowledge is to balance editing efficacy, i.e., the successful injection of target knowledge, and specificity (also known as edit locality), i.e., the preservation of existing non-target knowledge. However, we find that existing specificity evaluation protocols are inadequate for this purpose. We systematically elaborated on the three fundamental issues it faces. Beyond the conceptual issues, we further empirically demonstrate that existing specificity metrics are weakly correlated with the strength of specificity regularizers. We also find that current metrics lack sufficient sensitivity, rendering them ineffective at distinguishing the specificity performance of different methods. Finally, we propose a constructive evaluation protocol. Under this protocol, the conflict between open-ended LLMs and the assumption of determined answers is eliminated, query-independent fluency biases are avoided, and the evaluation strictness can be smoothly adjusted within a near-continuous space. Experiments across various LLMs, datasets, and editing methods show that metrics derived from the proposed protocol are more sensitive to changes in the strength of specificity regularizers and exhibit strong correlation with them, enabling more fine-grained discrimination of different methods' knowledge preservation capabilities.

</details>


### [8] [Multi-Agent Learning Path Planning via LLMs](https://arxiv.org/abs/2601.17346)
*Haoxin Xu,Changyong Qi,Tong Liu,Bohao Zhang,Anna He,Bingqian Jiang,Longwei Zheng,Xiaoqing Gu*

Main category: cs.AI

TL;DR: 提出基于LLM的多智能体学习路径规划框架(MALPP)，通过角色规则协作机制实现透明、自适应、可解释的个性化教学


<details>
  <summary>Details</summary>
Motivation: 现有学习路径规划方法缺乏透明度、适应性和学习者中心的可解释性，LLM在教育领域具有变革潜力但应用不足

Method: 构建三智能体协作框架：学习者分析智能体、路径规划智能体、反思智能体，基于认知负荷理论和最近发展区理论，通过结构化提示和预定义规则协作

Result: 在MOOCCubeX数据集上，MALPP在路径质量、知识序列一致性和认知负荷对齐方面显著优于基线模型，消融实验验证协作机制和理论约束的有效性

Conclusion: 为可信可解释AI教育应用提供新方案，展示了LLM驱动的规模化学习者中心自适应教学的可行路径

Abstract: The integration of large language models (LLMs) into intelligent tutoring systems offers transformative potential for personalized learning in higher education. However, most existing learning path planning approaches lack transparency, adaptability, and learner-centered explainability. To address these challenges, this study proposes a novel Multi-Agent Learning Path Planning (MALPP) framework that leverages a role- and rule-based collaboration mechanism among intelligent agents, each powered by LLMs. The framework includes three task-specific agents: a learner analytics agent, a path planning agent, and a reflection agent. These agents collaborate via structured prompts and predefined rules to analyze learning profiles, generate tailored learning paths, and iteratively refine them with interpretable feedback. Grounded in Cognitive Load Theory and Zone of Proximal Development, the system ensures that recommended paths are cognitively aligned and pedagogically meaningful. Experiments conducted on the MOOCCubeX dataset using seven LLMs show that MALPP significantly outperforms baseline models in path quality, knowledge sequence consistency, and cognitive load alignment. Ablation studies further validate the effectiveness of the collaborative mechanism and theoretical constraints. This research contributes to the development of trustworthy, explainable AI in education and demonstrates a scalable approach to learner-centered adaptive instruction powered by LLMs.

</details>


### [9] [Auditing Disability Representation in Vision-Language Models](https://arxiv.org/abs/2601.17348)
*Srikant Panda,Sourabh Singh Yadav,Palkesh Malviya*

Main category: cs.AI

TL;DR: 本研究系统分析了视觉语言模型(VLMs)在描述残障人物图像时的解释偏差，发现添加残障背景会导致模型从客观事实描述转向无根据推测，出现情感退化和缺陷导向框架，且这种影响因种族和性别而加剧，但可通过提示工程和偏好微调有效缓解。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型在社会敏感应用中的部署日益增多，其对残障人士的行为表现却缺乏充分研究，模型在人物中心图像描述中常从基于证据的事实描述转向引入超出视觉证据的无根据推断。

Method: 构建了基于中性提示(NP)与残障情境提示(DP)配对的基准测试，在9类残障类别上对15个先进开源和闭源VLMs进行零样本评估，采用结合文本指标(情感、社会尊重、响应长度)和LLM-as-judge协议(经残障经历者验证)的评估框架来衡量解释保真度。

Result: 引入残障背景持续降低解释保真度，导致推测性推断、叙事阐述、情感退化和缺陷导向框架等问题，且这些影响在种族和性别维度上进一步放大。

Conclusion: 有针对性的提示工程和偏好微调能显著提高解释保真度并大幅减少解释偏差，为提升VLMs的残障包容性提供了有效解决方案。

Abstract: Vision-language models (VLMs) are increasingly deployed in socially sensitive applications, yet their behavior with respect to disability remains underexplored. We study disability aware descriptions for person centric images, where models often transition from evidence grounded factual description to interpretation shift including introduction of unsupported inferences beyond observable visual evidence. To systematically analyze this phenomenon, we introduce a benchmark based on paired Neutral Prompts (NP) and Disability-Contextualised Prompts (DP) and evaluate 15 state-of-the-art open- and closed-source VLMs under a zero-shot setting across 9 disability categories. Our evaluation framework treats interpretive fidelity as core objective and combines standard text-based metrics capturing affective degradation through shifts in sentiment, social regard and response length with an LLM-as-judge protocol, validated by annotators with lived experience of disability. We find that introducing disability context consistently degrades interpretive fidelity, inducing interpretation shifts characterised by speculative inference, narrative elaboration, affective degradation and deficit oriented framing. These effects are further amplified along race and gender dimension. Finally, we demonstrate targeted prompting and preference fine-tuning effectively improves interpretive fidelity and reduces substantially interpretation shifts.

</details>


### [10] [A Syllogistic Probe: Tracing the Evolution of Logic Reasoning in Large Language Models](https://arxiv.org/abs/2601.17426)
*Zhengqing Zang,Yuqi Ding,Yanmei Gu,Changkai Song,Zhengkai Yang,Guoping Du,Junbo Zhao,Haobo Wang*

Main category: cs.AI

TL;DR: 本文通过存在预设这一探针，探究大语言模型是否像人类逻辑一样从直觉推理向形式化系统演进，发现模型规模、思维能力和基座模型是推动这一转变的关键因素。


<details>
  <summary>Details</summary>
Motivation: 受近期大语言模型进展启发，探索LLMs是否表现出与人类逻辑相似的底层逻辑框架演变（从直觉驱动推理到严谨形式系统）。

Method: 以存在预设作为探针，在传统和现代逻辑框架下评估三段论，通过在新构建的三段论数据集上测试SOTA大语言模型进行实验。

Result: (i) 模型规模扩展促进向现代逻辑的转变；(ii) 思维能力是超越参数扩展的高效加速器；(iii) 基座模型对转变的易得性和稳定性起决定性作用。

Conclusion: 除核心因素外，还进行了额外实验以深入分析当前大语言模型在推理能力方面的特性。

Abstract: Human logic has gradually shifted from intuition-driven inference to rigorous formal systems. Motivated by recent advances in large language models (LLMs), we explore whether LLMs exhibit a similar evolution in the underlying logical framework. Using existential import as a probe, we for evaluate syllogism under traditional and modern logic. Through extensive experiments of testing SOTA LLMs on a new syllogism dataset, we have some interesting findings: (i) Model size scaling promotes the shift toward modern logic; (ii) Thinking serves as an efficient accelerator beyond parameter scaling; (iii) the Base model plays a crucial role in determining how easily and stably this shift can emerge. Beyond these core factors, we conduct additional experiments for in-depth analysis of properties of current LLMs on syllogistic reasoning.

</details>


### [11] [Lattice: Generative Guardrails for Conversational Agents](https://arxiv.org/abs/2601.17481)
*Emily Broadhurst,Tawab Safi,Joseph Edell,Vashisht Ganesh,Karime Maamari*

Main category: cs.AI

TL;DR: 提出Lattice框架，通过两阶段迭代优化实现自构建、自改进的对话AI护栏系统，在ProsocialDialog数据集上达到91% F1值，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有对话AI护栏系统依赖静态规则，无法适应新威胁或部署环境变化，缺乏自适应能力

Method: Lattice框架分两个阶段：1）构建阶段通过迭代模拟和优化从标注示例生成初始护栏；2）持续改进阶段通过风险评估、对抗测试和整合实现护栏自主进化

Result: 在ProsocialDialog测试集上达到91% F1值，超越关键词基线43个百分点、LlamaGuard 25个百分点、NeMo 4个百分点；持续改进阶段在跨领域数据上进一步提升7个百分点

Conclusion: 通过迭代优化可自主演化出有效护栏，为AI安全提供了可自适应、持续进化的解决方案

Abstract: Conversational AI systems require guardrails to prevent harmful outputs, yet existing approaches use static rules that cannot adapt to new threats or deployment contexts. We introduce Lattice, a framework for self-constructing and continuously improving guardrails. Lattice operates in two stages: construction builds initial guardrails from labeled examples through iterative simulation and optimization; continuous improvement autonomously adapts deployed guardrails through risk assessment, adversarial testing, and consolidation. Evaluated on the ProsocialDialog dataset, Lattice achieves 91% F1 on held-out data, outperforming keyword baselines by 43pp, LlamaGuard by 25pp, and NeMo by 4pp. The continuous improvement stage achieves 7pp F1 improvement on cross-domain data through closed-loop optimization. Our framework shows that effective guardrails can be self-constructed through iterative optimization.

</details>


### [12] [Cognitive Platform Engineering for Autonomous Cloud Operations](https://arxiv.org/abs/2601.17542)
*Vinoth Punniyamoorthy,Nitin Saksena,Srivenkateswara Reddy Sankiti,Nachiappan Chockalingam,Aswathnarayan Muthukrishnan Kirubakaran,Shiva Kumar Reddy Carimireddy,Durgaraman Maruthavanan*

Main category: cs.AI

TL;DR: 提出认知平台工程新范式，通过四层架构（感知/推理/编排/体验）实现云平台自主运维，原型验证可提升故障恢复速度、资源效率和合规性


<details>
  <summary>Details</summary>
Motivation: 传统DevOps在云原生环境下面临 telemetry 数据激增和配置漂移挑战，规则驱动自动化导致运维被动、修复延迟，亟需更智能的解决方案

Method: 提出认知平台工程（Cognitive Platform Engineering）及四层参考架构：1) 数据采集层 2) 智能推理层（ML异常检测）3) 策略编排层（OPA）4) 人机体验层，形成闭环反馈

Result: 基于Kubernetes/Terraform/OPA/ML的原型显示：平均故障修复时间（MTTR）缩短、资源利用率提升、合规性增强，实现自调节云平台

Conclusion: 嵌入智能的平台运维可构建弹性自管理系统，未来研究方向包括强化学习、可解释治理和可持续自维护云生态

Abstract: Modern DevOps practices have accelerated software delivery through automation, CI/CD pipelines, and observability tooling,but these approaches struggle to keep pace with the scale and dynamism of cloud-native systems. As telemetry volume grows and configuration drift increases, traditional, rule-driven automation often results in reactive operations, delayed remediation, and dependency on manual expertise. This paper introduces Cognitive Platform Engineering, a next-generation paradigm that integrates sensing, reasoning, and autonomous action directly into the platform lifecycle. This paper propose a four-plane reference architecture that unifies data collection, intelligent inference, policy-driven orchestration, and human experience layers within a continuous feedback loop. A prototype implementation built with Kubernetes, Terraform, Open Policy Agent, and ML-based anomaly detection demonstrates improvements in mean time to resolution, resource efficiency, and compliance. The results show that embedding intelligence into platform operations enables resilient, self-adjusting, and intent-aligned cloud environments. The paper concludes with research opportunities in reinforcement learning, explainable governance, and sustainable self-managing cloud ecosystems.

</details>


### [13] [JaxARC: A High-Performance JAX-based Environment for Abstraction and Reasoning Research](https://arxiv.org/abs/2601.17564)
*Aadam,Monu Verma,Mohamed Abdel-Mottaleb*

Main category: cs.AI

TL;DR: JaxARC是一个基于JAX的开源高性能强化学习环境，用于ARC任务。其函数式无状态架构实现了大规模并行，相比Gymnasium获得38-5,439倍加速，峰值吞吐量达7.9亿步/秒，使原先计算不可行的大规模RL研究成为可能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Gymnasium的ARC强化学习环境因计算瓶颈严重限制实验规模，阻碍了AI系统人类式归纳推理能力的研究。

Method: 提出JaxARC，采用JAX实现函数式无状态架构以支持大规模并行，同时提供多ARC数据集支持、灵活动作空间、可组合包装器和配置驱动的复现性。

Result: 在匹配批次大小下实现38-5,439倍加速，峰值吞吐量达7.9亿步/秒，使大规模强化学习研究变得可行。

Conclusion: JaxARC成功解决了ARC研究的计算瓶颈，为AI归纳推理研究提供了高性能工具，将推动该领域的发展。

Abstract: The Abstraction and Reasoning Corpus (ARC) tests AI systems' ability to perform human-like inductive reasoning from a few demonstration pairs. Existing Gymnasium-based RL environments severely limit experimental scale due to computational bottlenecks. We present JaxARC, an open-source, high-performance RL environment for ARC implemented in JAX. Its functional, stateless architecture enables massive parallelism, achieving 38-5,439x speedup over Gymnasium at matched batch sizes, with peak throughput of 790M steps/second. JaxARC supports multiple ARC datasets, flexible action spaces, composable wrappers, and configuration-driven reproducibility, enabling large-scale RL research previously computationally infeasible. JaxARC is available at https://github.com/aadimator/JaxARC.

</details>


### [14] [Intelligence Requires Grounding But Not Embodiment](https://arxiv.org/abs/2601.17588)
*Marcus Ma,Shrikanth Narayanan*

Main category: cs.AI

TL;DR: This paper argues that grounding (not embodiment) is necessary for intelligence, defining intelligence through four properties and proposing a non-embodied grounded LLM agent thought experiment.


<details>
  <summary>Details</summary>
Motivation: Recent advances in LLMs have reignited scientific debate over whether embodiment is necessary for intelligence.

Method: Defines intelligence via four properties (motivation, predictive ability, causality understanding, and learning from experience), argues each can be achieved by non-embodied grounded agents, and presents a thought experiment of an intelligent LLM in a digital environment.

Result: All four intelligence properties can be achieved by a non-embodied, grounded agent.

Conclusion: Grounding, not embodiment, is necessary for intelligence.

Abstract: Recent advances in LLMs have reignited scientific debate over whether embodiment is necessary for intelligence. We present the argument that intelligence requires grounding, a phenomenon entailed by embodiment, but not embodiment itself. We define intelligence as the possession of four properties -- motivation, predictive ability, understanding of causality, and learning from experience -- and argue that each can be achieved by a non-embodied, grounded agent. We use this to conclude that grounding, not embodiment, is necessary for intelligence. We then present a thought experiment of an intelligent LLM agent in a digital environment and address potential counterarguments.

</details>


### [15] [Health-ORSC-Bench: A Benchmark for Measuring Over-Refusal and Safety Completion in Health Context](https://arxiv.org/abs/2601.17642)
*Zhihao Zhang,Liting Huang,Guanghao Wu,Preslav Nakov,Heng Ji,Usman Naseem*

Main category: cs.AI

TL;DR: A new healthcare safety benchmark (Health-ORSC-Bench) reveals that current LLMs struggle to balance refusal and compliance - larger models like GPT-5 show excessive caution (over-refusing 80% of benign prompts) while smaller models compromise safety for utility.


<details>
  <summary>Details</summary>
Motivation: Safety alignment in healthcare LLMs using binary refusal boundaries causes over-refusal of benign queries or unsafe compliance with harmful ones. Existing benchmarks fail to evaluate "Safe Completion" - the ability to provide safe, high-level guidance on dual-use/borderline queries without enabling actionable harm.

Method: Introducing Health-ORSC-Bench, a large-scale benchmark with 31,920 benign boundary prompts across seven health categories (self-harm, medical misinformation, etc.). Uses an automated pipeline with human validation to test 30 state-of-the-art LLMs at varying levels of intent ambiguity.

Result: Safety-optimized models refuse up to 80% of "Hard" benign prompts, while domain-specific models sacrifice safety for utility. Larger frontier models (GPT-5, Llama-4) exhibit "safety-pessimism" and higher over-refusal than smaller/MoE-based models (Qwen-3-Next).

Conclusion: Current LLMs struggle to balance refusal and compliance in healthcare contexts. Health-ORSC-Bench provides a rigorous standard for calibrating next-generation medical AI assistants toward nuanced, safe, and helpful completions.

Abstract: Safety alignment in Large Language Models is critical for healthcare; however, reliance on binary refusal boundaries often results in \emph{over-refusal} of benign queries or \emph{unsafe compliance} with harmful ones. While existing benchmarks measure these extremes, they fail to evaluate Safe Completion: the model's ability to maximise helpfulness on dual-use or borderline queries by providing safe, high-level guidance without crossing into actionable harm. We introduce \textbf{Health-ORSC-Bench}, the first large-scale benchmark designed to systematically measure \textbf{Over-Refusal} and \textbf{Safe Completion} quality in healthcare. Comprising 31,920 benign boundary prompts across seven health categories (e.g., self-harm, medical misinformation), our framework uses an automated pipeline with human validation to test models at varying levels of intent ambiguity. We evaluate 30 state-of-the-art LLMs, including GPT-5 and Claude-4, revealing a significant tension: safety-optimised models frequently refuse up to 80\% of "Hard" benign prompts, while domain-specific models often sacrifice safety for utility. Our findings demonstrate that model family and size significantly influence calibration: larger frontier models (e.g., GPT-5, Llama-4) exhibit "safety-pessimism" and higher over-refusal than smaller or MoE-based counterparts (e.g., Qwen-3-Next), highlighting that current LLMs struggle to balance refusal and compliance. Health-ORSC-Bench provides a rigorous standard for calibrating the next generation of medical AI assistants toward nuanced, safe, and helpful completions. The code and data will be released upon acceptance. \textcolor{red}{Warning: Some contents may include toxic or undesired contents.}

</details>


### [16] [DIML: Differentiable Inverse Mechanism Learning from Behaviors of Multi-Agent Learning Trajectories](https://arxiv.org/abs/2601.17678)
*Zhiyu An,Wan Du*

Main category: cs.AI

TL;DR: 提出DIML框架，用于从观察到的策略互动中恢复未知的激励生成机制，具有理论保证并在多种游戏环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法如逆博弈论和多智能体逆强化学习只能推断结构化机制内的效用参数，而可微机制设计是正向优化的。本文目标是推断非结构化机制（如神经映射），且是在观察性设置下从行为中推断。

Method: DIML是一种基于似然的框架，通过多智能体学习动力学模型进行微分，并使用候选机制生成反事实收益来预测观察到的行为。

Result: 在条件logit响应模型下确立了收益差异的可识别性，证明了标准正则条件下最大似然估计的统计一致性。在多种模拟环境（神经机制、拥堵收费、公共品补贴、大规模匿名游戏）中验证，能可靠恢复可识别的激励差异，支持反事实预测，性能媲美小型环境中的表格枚举oracle，并能扩展到百人规模环境。

Conclusion: DIML框架有效解决了逆机制学习问题，兼具理论保证和实证性能，为从数据中恢复激励机制提供了新工具。

Abstract: We study inverse mechanism learning: recovering an unknown incentive-generating mechanism from observed strategic interaction traces of self-interested learning agents. Unlike inverse game theory and multi-agent inverse reinforcement learning, which typically infer utility/reward parameters inside a structured mechanism, our target includes unstructured mechanism -- a (possibly neural) mapping from joint actions to per-agent payoffs. Unlike differentiable mechanism design, which optimizes mechanisms forward, we infer mechanisms from behavior in an observational setting. We propose DIML, a likelihood-based framework that differentiates through a model of multi-agent learning dynamics and uses the candidate mechanism to generate counterfactual payoffs needed to predict observed actions. We establish identifiability of payoff differences under a conditional logit response model and prove statistical consistency of maximum likelihood estimation under standard regularity conditions. We evaluate DIML with simulated interactions of learning agents across unstructured neural mechanisms, congestion tolling, public goods subsidies, and large-scale anonymous games. DIML reliably recovers identifiable incentive differences and supports counterfactual prediction, where its performance rivals tabular enumeration oracle in small environments and its convergence scales to large, hundred-participant environments. Code to reproduce our experiments is open-sourced.

</details>


### [17] [SQL-Trail: Multi-Turn Reinforcement Learning with Interleaved Feedback for Text-to-SQL](https://arxiv.org/abs/2601.17699)
*Harper Hua,Zhen Han,Zhengyuan Shen,Jeremy Lee,Patrick Guan,Qi Zhu,Sullam Jeoung,Yueyan Chen,Yunfei Bai,Shuai Wang,Vassilis Ioannidis,Huzefa Rangwala*

Main category: cs.AI

TL;DR: SQL-Trail是一个多轮强化学习智能体框架，通过执行反馈迭代优化SQL查询，采用自适应轮次预算和复合奖励机制，在Text-to-SQL任务上实现新SOTA，数据效率提升18倍，且小模型超越更大规模的商业系统。


<details>
  <summary>Details</summary>
Motivation: 当前单轮生成范式缺乏人类专家所具备的迭代推理、模式探索和错误修正能力，导致AI系统在BIRD-SQL等复杂基准测试上与人类表现存在显著差距。

Method: 提出SQL-Trail多轮RL框架：(i) 自适应轮次预算分配机制，根据问题难度动态调整交互深度；(ii) 复合奖励面板，联合激励SQL正确性和高效探索；通过与数据库环境交互和执行反馈进行迭代优化。

Result: 在多项基准测试中达到新SOTA，数据效率比单轮RL方法提升高达18倍；7B和14B模型平均性能超越大得多的商业系统5%。

Conclusion: 交互式智能体工作流对鲁棒Text-to-SQL生成非常有效，多轮迭代方法能以更小模型实现超越单轮范式的性能表现。

Abstract: While large language models (LLMs) have substantially improved Text-to-SQL generation, a pronounced gap remains between AI systems and human experts on challenging benchmarks such as BIRD-SQL. We argue this gap stems largely from the prevailing single-pass paradigm, which lacks the iterative reasoning, schema exploration, and error-correction behaviors that humans naturally employ. To address this limitation, we introduce SQL-Trail, a multi-turn reinforcement learning (RL) agentic framework for Text-to-SQL. Rather than producing a query in one shot, SQL-Trail interacts with the database environment and uses execution feedback to iteratively refine its predictions. Our approach centers on two key ideas: (i) an adaptive turn-budget allocation mechanism that scales the agent's interaction depth to match question difficulty, and (ii) a composite reward panel that jointly incentivizes SQL correctness and efficient exploration. Across benchmarks, SQL-Trail sets a new state of the art and delivers strong data efficiency--up to 18x higher than prior single-pass RL state-of-the-art methods. Notably, our 7B and 14B models outperform substantially larger proprietary systems by 5% on average, underscoring the effectiveness of interactive, agentic workflows for robust Text-to-SQL generation.

</details>


### [18] [The LLM Data Auditor: A Metric-oriented Survey on Quality and Trustworthiness in Evaluating Synthetic Data](https://arxiv.org/abs/2601.17717)
*Kaituo Zhang,Mingzhi Hu,Hoang Anh Duy Le,Fariha Kabir Torsha,Zhimeng Jiang,Minh Khai Bui,Chia-Yuan Chang,Yu-Neng Chuang,Zhen Xiong,Ying Lin,Guanchu Wang,Na Zou*

Main category: cs.AI

TL;DR: 提出LLM Data Auditor框架，系统评估六种模态的LLM生成数据质量，从外在任务评估转向内在数据属性评估，并给出改进建议


<details>
  <summary>Details</summary>
Motivation: LLM可生成多模态数据缓解数据稀缺问题，但现有研究重生成方法轻数据质量，且多为单模态缺乏统一视角

Method: 构建LLM Data Auditor框架，描述六种模态的数据生成方式，从质量和可信度两个维度系统化内在评估指标

Result: 分析各模态代表性生成方法的实验评估，发现当前评估实践存在重大缺陷，并提供具体改进建议

Conclusion: 该框架为多模态合成数据提供了系统化质量评估体系，识别了评估盲区，推动社区建立更完善的生成数据评估标准

Abstract: Large Language Models (LLMs) have emerged as powerful tools for generating data across various modalities. By transforming data from a scarce resource into a controllable asset, LLMs mitigate the bottlenecks imposed by the acquisition costs of real-world data for model training, evaluation, and system iteration. However, ensuring the high quality of LLM-generated synthetic data remains a critical challenge. Existing research primarily focuses on generation methodologies, with limited direct attention to the quality of the resulting data. Furthermore, most studies are restricted to single modalities, lacking a unified perspective across different data types. To bridge this gap, we propose the \textbf{LLM Data Auditor framework}. In this framework, we first describe how LLMs are utilized to generate data across six distinct modalities. More importantly, we systematically categorize intrinsic metrics for evaluating synthetic data from two dimensions: quality and trustworthiness. This approach shifts the focus from extrinsic evaluation, which relies on downstream task performance, to the inherent properties of the data itself. Using this evaluation system, we analyze the experimental evaluations of representative generation methods for each modality and identify substantial deficiencies in current evaluation practices. Based on these findings, we offer concrete recommendations for the community to improve the evaluation of data generation. Finally, the framework outlines methodologies for the practical application of synthetic data across different modalities.

</details>


### [19] [EntWorld: A Holistic Environment and Benchmark for Verifiable Enterprise GUI Agents](https://arxiv.org/abs/2601.17722)
*Ying Mo,Yu Bai,Dapeng Sun,Yuqian Shi,Yukai Miao,Li Chen,Dan Li*

Main category: cs.AI

TL;DR: EntWorld企业工作流基准测试揭示AI智能体在专业企业环境中的性能显著落后于消费级场景，当前顶尖模型成功率仅47.61%


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型基准测试主要集中在电商、旅行等消费场景，无法反映专业企业工作流的复杂性。企业系统面临高密度用户界面、严格业务逻辑约束和精确状态一致性信息检索等独特挑战，而通用智能体在这些场景中表现不佳。

Method: 提出EntWorld基准，包含6大企业领域（CRM、ITIL、ERP等）的1,756个任务。采用模式 grounded 的任务生成框架，直接从数据库模式反推业务逻辑以合成真实长时程工作流，并引入基于SQL的确定性验证机制替代模糊的视觉匹配。

Result: GPT-4.1等顶尖模型在EntWorld上成功率仅为47.61%，远低于人类表现，凸显了当前智能体在企业环境中的显著能力差距。

Conclusion: EntWorld作为严谨测试平台发布，揭示了开发领域专用企业智能体的必要性，将助力下一代企业级数字智能体的研发与评估。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have enabled agents to operate in open-ended web and operating system environments. However, existing benchmarks predominantly target consumer-oriented scenarios (e.g., e-commerce and travel booking), failing to capture the complexity and rigor of professional enterprise workflows. Enterprise systems pose distinct challenges, including high-density user interfaces, strict business logic constraints, and a strong reliance on precise, state-consistent information retrieval-settings in which current generalist agents often struggle. To address this gap, we introduce EntWorld, a large-scale benchmark consisting of 1,756 tasks across six representative enterprise domains, including customer relationship management (CRM), information technology infrastructure library (ITIL), and enterprise resource planning (ERP) systems. Unlike previous datasets that depend on fragile execution traces or extensive manual annotation, EntWorld adopts a schema-grounded task generation framework that directly reverse-engineers business logic from underlying database schemas, enabling the synthesis of realistic, long-horizon workflows. Moreover, we propose a SQL-based deterministic verification mechanism in building datasets that replaces ambiguous visual matching with rigorous state-transition validation. Experimental results demonstrate that state-of-the-art models (e.g., GPT-4.1) achieve 47.61% success rate on EntWorld, substantially lower than the human performance, highlighting a pronounced enterprise gap in current agentic capabilities and the necessity of developing domain-specific agents. We release EntWorld as a rigorous testbed to facilitate the development and evaluation of the next generation of enterprise-ready digital agents.

</details>


### [20] [ReFuGe: Feature Generation for Prediction Tasks on Relational Databases with LLM Agents](https://arxiv.org/abs/2601.17735)
*Kyungho Kim,Geon Lee,Juyeon Kim,Dongwon Choi,Shinhwan Kang,Kijung Shin*

Main category: cs.AI

TL;DR: ReFuGe is an LLM-agent framework that automatically generates relational features for RDB prediction tasks through schema selection, feature generation, and filtering in an iterative loop, significantly improving performance.


<details>
  <summary>Details</summary>
Motivation: Relational databases are crucial for web applications, but generating informative relational features for prediction tasks is challenging due to complex schemas, combinatorially large feature spaces, and lack of explicit supervision.

Method: ReFuGe uses three specialized LLM agents: (1) schema selection to identify relevant tables/columns, (2) feature generation to create diverse candidates, and (3) feature filtering via reasoning and validation. It operates through iterative feedback until performance converges.

Result: Experiments on RDB benchmarks demonstrate that ReFuGe substantially improves performance across various relational database prediction tasks.

Conclusion: The ReFuGe framework effectively automates relational feature generation, overcoming key challenges and delivering significant predictive performance gains on RDB tasks.

Abstract: Relational databases (RDBs) play a crucial role in many real-world web applications, supporting data management across multiple interconnected tables. Beyond typical retrieval-oriented tasks, prediction tasks on RDBs have recently gained attention. In this work, we address this problem by generating informative relational features that enhance predictive performance. However, generating such features is challenging: it requires reasoning over complex schemas and exploring a combinatorially large feature space, all without explicit supervision. To address these challenges, we propose ReFuGe, an agentic framework that leverages specialized large language model agents: (1) a schema selection agent identifies the tables and columns relevant to the task, (2) a feature generation agent produces diverse candidate features from the selected schema, and (3) a feature filtering agent evaluates and retains promising features through reasoning-based and validation-based filtering. It operates within an iterative feedback loop until performance converges. Experiments on RDB benchmarks demonstrate that ReFuGe substantially improves performance on various RDB prediction tasks. Our code and datasets are available at https://github.com/K-Kyungho/REFUGE.

</details>


### [21] [Faramesh: A Protocol-Agnostic Execution Control Plane for Autonomous Agent Systems](https://arxiv.org/abs/2601.17744)
*Amjad Fatmi*

Main category: cs.AI

TL;DR: Faramesh是一个协议无关的执行控制平面，通过在智能体行动前设置不可绕过的授权边界，将意图转化为标准动作表示并基于策略进行确定性评估，实现对自主系统现实世界操作的可审计、可验证的治理控制


<details>
  <summary>Details</summary>
Motivation: 自主智能体系统日益触发真实世界的副作用（部署基础设施、修改数据库、转移资金、执行工作流），但现有智能体栈缺乏在改变现实前可确定性允许、拒绝或推迟行动的必要执行检查点，存在安全和治理风险

Method: 提出Faramesh系统：1) 建立不可绕过的动作授权边界(AAB)；2) 将智能体意图规范化为规范动作表示(CAR)；3) 基于策略和状态进行确定性评估并生成PERMIT/DEFER/DENY决策工件；4) 执行器必须验证决策后才可执行；5) 提供基于规范动作哈希的不可变溯源日志

Result: 实现了可强制执行的、可预测的自主执行治理框架，支持多智能体多租户部署，独立于传输协议和模型框架，提供审计能力、验证机制和确定性重放功能，无需重新运行智能体推理过程

Conclusion: Faramesh通过标准化的授权边界和决策溯源机制，为自主智能体的现实世界操作提供了必要的安全治理层，在保持系统灵活性的同时避免了与编排层的隐式耦合，超越了仅可观测的监控方案

Abstract: Autonomous agent systems increasingly trigger real-world side effects: deploying infrastructure, modifying databases, moving money, and executing workflows. Yet most agent stacks provide no mandatory execution checkpoint where organizations can deterministically permit, deny, or defer an action before it changes reality. This paper introduces Faramesh, a protocol-agnostic execution control plane that enforces execution-time authorization for agent-driven actions via a non-bypassable Action Authorization Boundary (AAB). Faramesh canonicalizes agent intent into a Canonical Action Representation (CAR), evaluates actions deterministically against policy and state, and issues a decision artifact (PERMIT/DEFER/DENY) that executors must validate prior to execution. The system is designed to be framework- and model-agnostic, supports multi-agent and multi-tenant deployments, and remains independent of transport protocols (e.g., MCP). Faramesh further provides decision-centric, append-only provenance logging keyed by canonical action hashes, enabling auditability, verification, and deterministic replay without re-running agent reasoning. We show how these primitives yield enforceable, predictable governance for autonomous execution while avoiding hidden coupling to orchestration layers or observability-only approaches.

</details>


### [22] [Neuro-Symbolic Verification on Instruction Following of LLMs](https://arxiv.org/abs/2601.17789)
*Yiming Su,Kunzhao Xu,Yanjie Gao,Fan Yang,Cheng Li,Mao Yang,Tianyin Xu*

Main category: cs.AI

TL;DR: This paper proposes NSVIF, a neuro-symbolic framework that verifies LLM instruction-following by modeling instructions as logical and semantic constraints and solving them with a unified solver. It significantly outperforms LLM-based approaches, provides interpretable feedback, and can improve LLMs' instruction-following capability without post-training.


<details>
  <summary>Details</summary>
Motivation: LLMs often fail to follow instructions, and these violations are difficult to detect. In agentic workflows, such violations can propagate and amplify along reasoning chains, causing task failures and system incidents. There is a critical need for reliable verification of instruction-following behavior.

Method: NSVIF is a universal, general-purpose neuro-symbolic verifier that formulates instruction-following verification as a constraint-satisfaction problem. It models user instructions as both logical and semantic constraints, which are solved by a unified solver that orchestrates logical reasoning and semantic analysis. No assumptions are made about the specific instruction or LLM being verified.

Result: Experiments on the new VIFBENCH benchmark show NSVIF significantly outperforms LLM-based approaches and provides interpretable feedback. The feedback from NSVIF also helps improve LLMs' instruction-following capability without requiring post-training.

Conclusion: NSVIF demonstrates the effectiveness of neuro-symbolic methods for instruction-following verification. It offers a practical, interpretable solution for improving LLM reliability in real-world applications, particularly in agentic workflows where instruction violations can have cascading effects.

Abstract: A fundamental problem of applying Large Language Models (LLMs) to important applications is that LLMs do not always follow instructions, and violations are often hard to observe or check. In LLM-based agentic workflows, such violations can propagate and amplify along reasoning chains, causing task failures and system incidents. This paper presents NSVIF, a neuro-symbolic framework for verifying whether an LLM's output follows the instructions used to prompt the LLM. NSVIF is a universal, general-purpose verifier; it makes no assumption about the instruction or the LLM. NSVIF formulates instruction-following verification as a constraint-satisfaction problem by modeling user instructions as constraints. NSVIF models both logical and semantic constraints; constraint solving is done by a unified solver that orchestrates logical reasoning and semantic analysis. To evaluate NSVIF, we develop VIFBENCH, a new benchmark for instruction-following verifiers with fine-grained data labels. Experiments show that NSVIF significantly outperforms LLM-based approaches and provides interpretable feedback. We also show that feedback from NSVIF helps improve LLMs' instruction-following capability without post-training.

</details>


### [23] [MMR-Bench: A Comprehensive Benchmark for Multimodal LLM Routing](https://arxiv.org/abs/2601.17814)
*Haoxuan Ma,Guannan Lai,Han-Jia Ye*

Main category: cs.AI

TL;DR: 提出MMR-Bench，首个多模态模型路由基准测试，通过智能查询分配实现成本-精度优化，路由系统能以33%的成本超越最强单体模型精度。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs架构异构、成本差异大，单一模型部署要么计算资源浪费，要么精度不足；文本路由难以直接扩展到多模态，缺乏标准化预算感知评估体系。

Method: 构建统一基准MMR-Bench，包含模态感知输入环境、多任务套件（OCR/VQA/数学推理）及路由策略评估框架；通过控制候选模型和成本模型进行系统比较。

Result: 多模态信号显著提升路由质量，优化成本-精度边界；路由系统可达最强单体模型精度但仅需33%计算成本；支持零样本迁移到新数据集和文本基准。

Conclusion: MMR-Bench为自适应多模态模型选择和高效MLLM部署奠定基础，推动智能路由算法研究，促进计算资源优化。

Abstract: Multimodal large language models (MLLMs) have advanced rapidly, yet heterogeneity in architecture, alignment strategies, and efficiency means that no single model is uniformly superior across tasks. In practical deployments, workloads span lightweight OCR to complex multimodal reasoning; using one MLLM for all queries either over-provisions compute on easy instances or sacrifices accuracy on hard ones. Query-level model selection (routing) addresses this tension, but extending routing from text-only LLMs to MLLMs is nontrivial due to modality fusion, wide variation in computational cost across models, and the absence of a standardized, budget-aware evaluation. We present MMR-Bench, a unified benchmark that isolates the multimodal routing problem and enables comparison under fixed candidate sets and cost models. MMR-Bench provides (i) a controlled environment with modality-aware inputs and variable compute budgets, (ii) a broad suite of vision-language tasks covering OCR, general VQA, and multimodal math reasoning, and (iii) strong single-model reference, oracle upper bounds, and representative routing policies. Using MMR-Bench, we show that incorporating multimodal signals improves routing quality. Empirically, these cues improve the cost-accuracy frontier and enable the routed system to exceed the strongest single model's accuracy at roughly 33% of its cost. Furthermore, policies trained on a subset of models and tasks generalize zero-shot to new datasets and text-only benchmarks without retuning, establishing MMR-Bench as a foundation for studying adaptive multimodal model selection and efficient MLLM deployment. The code will be available at: https://github.com/Hunter-Wrynn/MMR-Bench.

</details>


### [24] [RegGuard: AI-Powered Retrieval-Enhanced Assistant for Pharmaceutical Regulatory Compliance](https://arxiv.org/abs/2601.17826)
*Siyuan Yang,Xihan Bian,Jiayin Tang*

Main category: cs.AI

TL;DR: 针对跨国药企监管合规负担，本文提出RegGuard AI系统，通过HiSACC层次语义分块和ReLACE领域自适应重排序两大创新，实现监管文本自动解读，显著提升答案质量和可信度，降低幻觉风险。


<details>
  <summary>Details</summary>
Motivation: 监管更新频繁复杂，跨国制药企业需跨司法管辖区、格式和机构手动解读规则，成本高且易出错，亟需自动化解决方案。

Method: 提出工业级AI助手RegGuard，采用安全管道处理异构文档，核心创新包括：1）HiSACC通过分层语义聚合实现长文档语义分块并保持非连续部分一致性；2）ReLACE基于开源模型构建领域自适应交叉编码器，联合建模查询与候选文档以提升排序相关性；系统具备溯源追踪、访问控制和增量索引功能。

Result: 企业环境评估表明，RegGuard在答案相关性、 groundedness和上下文聚焦方面显著提升，同时大幅降低幻觉风险。

Conclusion: 该系统架构注重可审计性和可追溯性，能快速响应文档更新，不仅适用于制药行业，也可推广至任何强合规要求领域。

Abstract: The increasing frequency and complexity of regulatory updates present a significant burden for multinational pharmaceutical companies. Compliance teams must interpret evolving rules across jurisdictions, formats, and agencies, often manually, at high cost and risk of error. We introduce RegGuard, an industrial-scale AI assistant designed to automate the interpretation of heterogeneous regulatory texts and align them with internal corporate policies. The system ingests heterogeneous document sources through a secure pipeline and enhances retrieval and generation quality with two novel components: HiSACC (Hierarchical Semantic Aggregation for Contextual Chunking) semantically segments long documents into coherent units while maintaining consistency across non-contiguous sections. ReLACE (Regulatory Listwise Adaptive Cross-Encoder for Reranking), a domain-adapted cross-encoder built on an open-source model, jointly models user queries and retrieved candidates to improve ranking relevance. Evaluations in enterprise settings demonstrate that RegGuard improves answer quality specifically in terms of relevance, groundedness, and contextual focus, while significantly mitigating hallucination risk. The system architecture is built for auditability and traceability, featuring provenance tracking, access control, and incremental indexing, making it highly responsive to evolving document sources and relevant for any domain with stringent compliance demands.

</details>


### [25] [Aligning Medical Conversational AI through Online Reinforcement Learning with Information-Theoretic Rewards](https://arxiv.org/abs/2601.17828)
*Tanvi Verma,Yang Zhou,Rick Siow Mong Goh,Yong Liu*

Main category: cs.AI

TL;DR: 提出IGFT方法，让医疗AI通过自我生成对话和在线强化学习，无需人工数据即可学会有效问诊策略，在Avey和MIMIC数据集上F1分数提升10-13%，超越现有医疗基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有医疗对话AI依赖昂贵的人工标注对话数据，限制了模型探索能力。需要开发一种无需预收集人类对话即可训练高质量问诊AI的方法。

Method: IGFT结合在线GRPO算法与信息论奖励机制，追踪对话中揭示的临床实体（症状、时间模式、病史）。奖励函数融合预期信息增益和GPT-4o-mini对临床相关性、患者参与度、特异性的评估。使用LoRA微调Llama-3.1-8B-Instruct和DeepSeek-R1-Distill-Qwen-7B两个模型。

Result: DeepSeek-R1-Distill-Qwen-7B在Avey上F1=0.408（较基座模型+10.9%），在MIMIC上F1=0.289（+12.9%）；Llama-3.1-8B-Instruct在Avey上F1=0.384，在MIMIC上F1=0.336。两个模型均在MIMIC上超越OpenAI模型，并优于HuatuoGPT、UltraMedical等医疗领域基线。

Conclusion: IGFT框架成功解决了医疗对话AI对人工数据的依赖问题，通过探索式学习实现高效临床信息收集，展现出良好的泛化能力和应用前景，为多轮医疗对话系统提供了新的技术路径。

Abstract: We present Information Gain Fine-Tuning (IGFT), a novel approach for training medical conversational AI to conduct effective patient interviews and generate comprehensive History of Present Illness (HPI) without requiring pre-collected human conversations. IGFT combines online Group Relative Policy Optimization (GRPO) with information-theoretic rewards, enabling models to learn from self-generated conversations with simulated patients. Unlike existing approaches that rely on expensive expert-annotated conversations or static datasets, our online RL framework allows models to discover effective questioning strategies through exploration. Our key innovation is an information gain reward function that tracks which clinical entities such as symptoms, temporal patterns, and medical history, are revealed during conversation. Each question's reward is computed based on its expected information gain combined with GPT-4o-mini quality assessments across dimensions including clinical relevance, patient engagement, and specificity. This hybrid approach ensures models learn to ask targeted, clinically appropriate questions that efficiently gather diagnostic information. We fine-tune two models using LoRA: Llama-3.1-8B-Instruct and DeepSeek-R1-Distill-Qwen-7B (a reasoning-optimized model). Training exclusively on Avey data containing concise HPIs, we evaluate generalization to MIMIC data with longer, more elaborate HPIs. DeepSeek-R1-Distill-Qwen-7B (IGFT) achieves F1 scores of 0.408 on Avey (10.9% improvement over base) and 0.289 on MIMIC (12.9% improvement), while Llama-3.1-8B-Instruct (IGFT) reaches 0.384 and 0.336 respectively. Both models outperform OpenAI's model on MIMIC and surpass medical domain-specific baselines like HuatuoGPT and UltraMedical, which were optimized for single-turn medical QA rather than multi-turn conversations.

</details>


### [26] [UniCog: Uncovering Cognitive Abilities of LLMs through Latent Mind Space Analysis](https://arxiv.org/abs/2601.17897)
*Jiayu Liu,Yinhe Long,Zhenya Huang,Enhong Chen*

Main category: cs.AI

TL;DR: Proposes UniCog, a latent variable framework that disentangles LLM cognitive abilities into sparse latent dimensions, revealing a Pareto principle of cognition and improving reasoning by 7.5%.


<details>
  <summary>Details</summary>
Motivation: Existing interpretability methods cannot adequately explain how cognitive abilities are engaged during LLM reasoning, despite evidence that LLM cognition differs fundamentally from human cognition.

Method: UniCog: a unified latent variable model that encodes diverse cognitive abilities from dense model activations into sparse, disentangled latent dimensions for analyzing LLM cognition via a latent mind space.

Result: (1) Revealed a Pareto principle of LLM cognition with shared reasoning core + ability-specific signatures; (2) Found reasoning failures manifest as anomalous latent activation intensity; (3) Latent-informed candidate prioritization improved reasoning by up to 7.5% on challenging benchmarks.

Conclusion: Opens a new cognition-grounded paradigm for LLM analysis, providing interpretable insights into reasoning dynamics and enabling practical performance improvements through latent-space analysis.

Abstract: A growing body of research suggests that the cognitive processes of large language models (LLMs) differ fundamentally from those of humans. However, existing interpretability methods remain limited in explaining how cognitive abilities are engaged during LLM reasoning. In this paper, we propose UniCog, a unified framework that analyzes LLM cognition via a latent mind space. Formulated as a latent variable model, UniCog encodes diverse abilities from dense model activations into sparse, disentangled latent dimensions. Through extensive analysis on six advanced LLMs, including DeepSeek-V3.2 and GPT-4o, we reveal a Pareto principle of LLM cognition, where a shared reasoning core is complemented by ability-specific signatures. Furthermore, we discover that reasoning failures often manifest as anomalous intensity in latent activations. These findings opens a new paradigm in LLM analysis, providing a cognition grounded view of reasoning dynamics. Finally, leveraging these insights, we introduce a latent-informed candidate prioritization strategy, which improves reasoning performance by up to 7.5% across challenging benchmarks. Our code is available at https://github.com/milksalute/unicog.

</details>


### [27] [Think Locally, Explain Globally: Graph-Guided LLM Investigations via Local Reasoning and Belief Propagation](https://arxiv.org/abs/2601.17915)
*Saurabh Jha,Rohan Arora,Bhavya,Noah Zheutlin,Paulina Toro Isaza,Laura Shwartz,Yu Deng,Daby Sow,Ruchi Mahindru,Ruchir Puri*

Main category: cs.AI

TL;DR: The paper identifies limitations of current LLM agents (like ReAct) in complex investigations and proposes EoG, a framework that uses a deterministic controller for graph traversal and belief propagation while LLMs handle local evidence mining, resulting in significantly improved accuracy and consistency.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents fail in open-ended investigations with massive heterogeneous data due to bounded context windows, hidden dependency structures, and lack of belief revision mechanisms. ReAct-style agents are brittle, order-sensitive, and non-deterministic, creating a reliability gap.

Method: Formulates investigation as abductive reasoning over a dependency graph. Proposes EoG (Explanations over Graphs), a disaggregated framework where an LLM performs bounded local evidence mining/labeling while a deterministic controller manages graph traversal, state tracking, and belief propagation to compute a minimal explanatory frontier.

Result: On ITBench diagnostics tasks, EoG significantly outperforms ReAct baselines in both accuracy and run-to-run consistency, achieving a 7x average improvement in Majority-at-k entity F1 score.

Conclusion: The EoG framework effectively addresses the key limitations of current LLM agents in complex investigations by separating semantic reasoning from control logic and implementing explicit belief bookkeeping, leading to more reliable and consistent performance.

Abstract: LLM agents excel when environments are mostly static and the needed information fits in a model's context window, but they often fail in open-ended investigations where explanations must be constructed by iteratively mining evidence from massive, heterogeneous operational data. These investigations exhibit hidden dependency structure: entities interact, signals co-vary, and the importance of a fact may only become clear after other evidence is discovered. Because the context window is bounded, agents must summarize intermediate findings before their significance is known, increasing the risk of discarding key evidence. ReAct-style agents are especially brittle in this regime. Their retrieve-summarize-reason loop makes conclusions sensitive to exploration order and introduces run-to-run non-determinism, producing a reliability gap where Pass-at-k may be high but Majority-at-k remains low. Simply sampling more rollouts or generating longer reasoning traces does not reliably stabilize results, since hypotheses cannot be autonomously checked as new evidence arrives and there is no explicit mechanism for belief bookkeeping and revision. In addition, ReAct entangles semantic reasoning with controller duties such as tool orchestration and state tracking, so execution errors and plan drift degrade reasoning while consuming scarce context.
  We address these issues by formulating investigation as abductive reasoning over a dependency graph and proposing EoG (Explanations over Graphs), a disaggregated framework in which an LLM performs bounded local evidence mining and labeling (cause vs symptom) while a deterministic controller manages traversal, state, and belief propagation to compute a minimal explanatory frontier. On a representative ITBench diagnostics task, EoG improves both accuracy and run-to-run consistency over ReAct baselines, including a 7x average gain in Majority-at-k entity F1.

</details>


### [28] [Agentic AI for Self-Driving Laboratories in Soft Matter: Taxonomy, Benchmarks,and Open Challenges](https://arxiv.org/abs/2601.17920)
*Xuanzhou Chen,Audrey Wang,Stanley Yin,Hanyang Jiang,Dong Zhang*

Main category: cs.AI

TL;DR: A survey of AI methods for self-driving laboratories, providing an agent-environment framework, taxonomy, benchmarks, and identifying open challenges in multi-modal representation, calibrated uncertainty, safe exploration, and shared infrastructure.


<details>
  <summary>Details</summary>
Motivation: SDLs provide a demanding testbed for agentic AI under expensive actions, noisy/delayed feedback, strict safety constraints, and non-stationarity, requiring robust AI solutions beyond toy problems.

Method: Framed SDL autonomy as an agent-environment interaction problem with explicit observations, actions, costs, and constraints; reviewed Bayesian optimization, active learning, planning/RL, and tool-using agents.

Result: Proposed a capability-driven taxonomy (decision horizon, uncertainty modeling, action parameterization, constraint handling, failure recovery, human involvement) and synthesized benchmark templates with cost-aware, robustness, and reproducibility metrics.

Conclusion: Distilled lessons from deployed SDLs and outlined open challenges: multi-modal representation, calibrated uncertainty, safe exploration, and shared benchmark infrastructure.

Abstract: Self-driving laboratories (SDLs) close the loop between experiment design, automated execution, and data-driven decision making, and they provide a demanding testbed for agentic AI under expensive actions, noisy and delayed feedback, strict feasibility and safety constraints, and non-stationarity. This survey uses soft matter as a representative setting but focuses on the AI questions that arise in real laboratories. We frame SDL autonomy as an agent environment interaction problem with explicit observations, actions, costs, and constraints, and we use this formulation to connect common SDL pipelines to established AI principles. We review the main method families that enable closed loop experimentation, including Bayesian optimization and active learning for sample efficient experiment selection, planning and reinforcement learning for long horizon protocol optimization, and tool using agents that orchestrate heterogeneous instruments and software. We emphasize verifiable and provenance aware policies that support debugging, reproducibility, and safe operation. We then propose a capability driven taxonomy that organizes systems by decision horizon, uncertainty modeling, action parameterization, constraint handling, failure recovery, and human involvement. To enable meaningful comparison, we synthesize benchmark task templates and evaluation metrics that prioritize cost aware performance, robustness to drift, constraint violation behavior, and reproducibility. Finally, we distill lessons from deployed SDLs and outline open challenges in multi-modal representation, calibrated uncertainty, safe exploration, and shared benchmark infrastructure.

</details>


### [29] [Learning Transferable Skills in Action RPGs via Directed Skill Graphs and Selective Adaptation](https://arxiv.org/abs/2601.17923)
*Ali Najar*

Main category: cs.AI

TL;DR: 在《黑暗之魂3》的实时战斗环境中，研究者提出用有向技能图和分层课程训练将控制分解为五个可复用技能，通过选择性微调两个技能即可快速适应环境变化，为终生学习智能体提供了实用路径


<details>
  <summary>Details</summary>
Motivation: 终生学习智能体需要在复杂实时环境中持续扩展能力，但不能从头重新训练或覆盖已学行为，避免灾难性遗忘

Method: 将战斗建模为有向技能图，采用分层课程训练五个独立技能组件（镜头控制、锁定目标、移动、闪避、治疗-攻击决策），每个技能专注单一职责

Result: 技能分解提升了样本效率，支持选择性后训练；环境从第一阶段切换到第二阶段时，仅需适应部分技能，上游技能可迁移；在有限交互预算下，仅微调两个技能即可快速恢复性能

Conclusion: 技能图课程与选择性微调相结合，为复杂实时环境中实现可进化、持续学习的智能体提供了实用路径

Abstract: Lifelong agents should expand their competence over time without retraining from scratch or overwriting previously learned behaviors. We investigate this in a challenging real-time control setting (Dark Souls III) by representing combat as a directed skill graph and training its components in a hierarchical curriculum. The resulting agent decomposes control into five reusable skills: camera control, target lock-on, movement, dodging, and a heal-attack decision policy, each optimized for a narrow responsibility. This factorization improves sample efficiency by reducing the burden on any single policy and supports selective post-training: when the environment shifts from Phase 1 to Phase 2, only a subset of skills must be adapted, while upstream skills remain transferable. Empirically, we find that targeted fine-tuning of just two skills rapidly recovers performance under a limited interaction budget, suggesting that skill-graph curricula together with selective fine-tuning offer a practical pathway toward evolving, continually learning agents in complex real-time environments.

</details>


### [30] [LLM-Based SQL Generation: Prompting, Self-Refinement, and Adaptive Weighted Majority Voting](https://arxiv.org/abs/2601.17942)
*Yu-Jie Yang,Hung-Fu Chang,Po-An Chen*

Main category: cs.AI

TL;DR: 本文提出两种无需ground-truth数据的Text-to-SQL方法：单智能体自 refine 集成投票管道(SSEV)和多智能体协作框架(ReCAPAgent-SQL)，分别在Spider 1.0和Spider 2.0-Lite基准上取得竞争性和突破性性能，显著提升了实际企业场景下的SQL生成准确率。


<details>
  <summary>Details</summary>
Motivation: 降低数据分析门槛是Text-to-SQL的核心价值，但现有方法面临查询歧义、表结构链接复杂、SQL方言泛化能力不足、领域理解有限等挑战，难以直接部署于复杂的真实企业环境。

Method: 首先提出SSEV框架：基于PET-SQL构建单智能体自 refine 机制，结合加权多数投票(WMV)及其随机变体(RWMA)进行集成；进而提出ReCAPAgent-SQL：采用规划、检索、批判、执行、自 refine、结构链接和结果验证七个专业智能体协作，通过迭代 refinement 处理复杂企业数据库。

Result: SSEV在Spider 1.0-Dev/Test上分别达到85.5%和86.4%的执行准确率，在BIRD-Dev上为66.3%；ReCAPAgent-SQL在Spider 2.0-Lite前100条查询上实现31%的执行准确率，证明其在真实企业场景中的有效性。

Conclusion: 该工作通过自 refine 和智能体协作范式，显著提升了Text-to-SQL系统在实际场景下的可扩展性和准确性，为低成本、高效率的数据驱动决策提供了可行路径。

Abstract: Text-to-SQL has emerged as a prominent research area, particularly with the rapid advancement of large language models (LLMs). By enabling users to query databases through natural language rather than SQL, this technology significantly lowers the barrier to data analysis. However, generating accurate SQL from natural language remains challenging due to ambiguity in user queries, the complexity of schema linking, limited generalization across SQL dialects, and the need for domain-specific understanding. In this study, we propose a Single-Agent Self-Refinement with Ensemble Voting (SSEV) pipeline built on PET-SQL that operates without ground-truth data, integrating self-refinement with Weighted Majority Voting (WMV) and its randomized variant (RWMA). Experimental results show that the SSEV achieves competitive performance across multiple benchmarks, attaining execution accuracies of 85.5% on Spider 1.0-Dev, 86.4% on Spider 1.0-Test, and 66.3% on BIRD-Dev. Building on insights from the SSEV pipeline, we further propose ReCAPAgent-SQL (Refinement-Critique-Act-Plan agent-based SQL framework) to address the growing complexity of enterprise databases and real-world Text-to-SQL tasks. The framework integrates multiple specialized agents for planning, external knowledge retrieval, critique, action generation, self-refinement, schema linking, and result validation, enabling iterative refinement of SQL predictions through agent collaboration. ReCAPAgent-SQL's WMA results achieve 31% execution accuracy on the first 100 queries of Spider 2.0-Lite, demonstrating significant improvements in handling real-world enterprise scenarios. Overall, our work facilitates the deployment of scalable Text-to-SQL systems in practical settings, supporting better data-driven decision-making at lower cost and with greater efficiency.

</details>


### [31] [Expert Evaluation and the Limits of Human Feedback in Mental Health AI Safety Testing](https://arxiv.org/abs/2601.18061)
*Kiana Jafari,Paul Ulrich Nikolaus Rust,Duncan Eddy,Robbie Fraser,Nina Vasan,Darja Djordjevic,Akanksha Dadlani,Max Lamparth,Eugenia Kim,Mykel Kochenderfer*

Main category: cs.AI

TL;DR: Expert disagreement in mental health AI evaluation is systematic and structured, not random, challenging the assumption that aggregated human feedback yields valid ground truth.


<details>
  <summary>Details</summary>
Motivation: To test the core assumption of Learning from Human Feedback (LHF) that aggregated expert judgments provide valid ground truth, especially in high-stakes domains like mental health where safety is critical.

Method: Three certified psychiatrists independently evaluated LLM-generated mental health responses using a calibrated rubric; inter-rater reliability was quantified, and qualitative interviews explored the reasons behind disagreement.

Result: Inter-rater reliability was consistently poor, especially on safety-critical items like suicide/self-harm. Disagreement was systematic, reflecting incompatible clinical frameworks (safety-first, engagement-centered, culturally-informed), not measurement error.

Conclusion: Aggregated labels erase professional philosophies; practitioners should shift from consensus-based aggregation to alignment methods that preserve and learn from principled expert disagreement.

Abstract: Learning from human feedback~(LHF) assumes that expert judgments, appropriately aggregated, yield valid ground truth for training and evaluating AI systems. We tested this assumption in mental health, where high safety stakes make expert consensus essential. Three certified psychiatrists independently evaluated LLM-generated responses using a calibrated rubric. Despite similar training and shared instructions, inter-rater reliability was consistently poor ($ICC$ $0.087$--$0.295$), falling below thresholds considered acceptable for consequential assessment. Disagreement was highest on the most safety-critical items. Suicide and self-harm responses produced greater divergence than any other category, and was systematic rather than random. One factor yielded negative reliability (Krippendorff's $α= -0.203$), indicating structured disagreement worse than chance. Qualitative interviews revealed that disagreement reflects coherent but incompatible individual clinical frameworks, safety-first, engagement-centered, and culturally-informed orientations, rather than measurement error. By demonstrating that experts rely on holistic risk heuristics rather than granular factor discrimination, these findings suggest that aggregated labels function as arithmetic compromises that effectively erase grounded professional philosophies. Our results characterize expert disagreement in safety-critical AI as a sociotechnical phenomenon where professional experience introduces sophisticated layers of principled divergence. We discuss implications for reward modeling, safety classification, and evaluation benchmarks, recommending that practitioners shift from consensus-based aggregation to alignment methods that preserve and learn from expert disagreement.

</details>


### [32] [EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization](https://arxiv.org/abs/2601.18067)
*Wei-Po Hsin,Ren-Hao Deng,Yao-Ting Hsieh,En-Ming Huang,Shih-Hao Hung*

Main category: cs.AI

TL;DR: This paper introduces EvolVE, a framework that combines multiple evolution strategies (MCTS for functional correctness, IGR for optimization) with Structured Testbench Generation to automate Verilog design, achieving new SOTA results and significant PPA improvements on industry-scale problems.


<details>
  <summary>Details</summary>
Motivation: Verilog design cycles are labor-intensive and require extensive domain expertise; LLMs have limited training data and sequential reasoning that fails to capture hardware's strict formal logic and concurrency.

Method: Proposes EvolVE framework analyzing multiple evolution strategies, finding MCTS excels at functional correctness while IGR is superior for optimization. Leverages Structured Testbench Generation (STG) to accelerate evolution and introduces IC-RTL benchmark from National Integrated Circuit Contest.

Result: Achieves 98.1% on VerilogEval v2 and 92% on RTLLM v2. On IC-RTL suite, reduces PPA product by up to 66% for Huffman Coding and 17% geometric mean across all problems, surpassing contest participant implementations.

Conclusion: EvolVE establishes new state-of-the-art for automated Verilog generation and optimization, significantly improving PPA metrics on industry-scale problems while providing a valuable benchmark for complex hardware design tasks.

Abstract: Verilog's design cycle is inherently labor-intensive and necessitates extensive domain expertise. Although Large Language Models (LLMs) offer a promising pathway toward automation, their limited training data and intrinsic sequential reasoning fail to capture the strict formal logic and concurrency inherent in hardware systems. To overcome these barriers, we present EvolVE, the first framework to analyze multiple evolution strategies on chip design tasks, revealing that Monte Carlo Tree Search (MCTS) excels at maximizing functional correctness, while Idea-Guided Refinement (IGR) proves superior for optimization. We further leverage Structured Testbench Generation (STG) to accelerate the evolutionary process. To address the lack of complex optimization benchmarks, we introduce IC-RTL, targeting industry-scale problems derived from the National Integrated Circuit Contest. Evaluations establish EvolVE as the new state-of-the-art, achieving 98.1% on VerilogEval v2 and 92% on RTLLM v2. Furthermore, on the industry-scale IC-RTL suite, our framework surpasses reference implementations authored by contest participants, reducing the Power, Performance, Area (PPA) product by up to 66% in Huffman Coding and 17% in the geometric mean across all problems. The source code of the IC-RTL benchmark is available at https://github.com/weiber2002/ICRTL.

</details>


### [33] [Beyond Text-to-SQL: Can LLMs Really Debug Enterprise ETL SQL?](https://arxiv.org/abs/2601.18119)
*Jing Ye,Yiwen Duan,Yonghong Yu,Victor Ma,Yang Gao,Xing Chen*

Main category: cs.AI

TL;DR: The paper introduces OurBench, the first enterprise-level SQL debugging benchmark with 985 complex queries featuring syntax and semantic errors. It uses automated bug injection and execution-free evaluation, revealing that even top LLMs like Claude-4-Sonnet only achieve 32-36% accuracy, exposing a major performance gap.


<details>
  <summary>Details</summary>
Motivation: SQL is critical for enterprise data engineering, but generating fully correct SQL code is difficult even for experienced developers and advanced LLMs, often requiring multiple debugging iterations. There is a lack of comprehensive benchmarks for evaluating SQL reasoning and debugging capabilities in enterprise settings.

Method: The authors created OurBench with two key innovations: (1) an automated construction workflow that uses reverse engineering to systematically inject realistic bugs into large-scale SQL code for scalable benchmark generation; (2) an execution-free evaluation framework tailored to enterprise settings, providing fast, accurate, and resource-efficient assessment.

Result: OurBench comprises 469 syntax error queries (OurBenchSyn) and 516 semantic error queries (OurBenchSem), averaging over 140 lines with deep/wide abstract syntax trees. Evaluation of nearly 30 LLMs shows a substantial performance gap: the best model (Claude-4-Sonnet) achieves only 36.46% accuracy on OurBenchSyn and 32.17% on OurBenchSem, while most models score below 20%.

Conclusion: The significant performance gap demonstrates that current LLMs struggle with enterprise-level SQL debugging. The paper explores four solution strategies, identifies key challenges, and outlines promising research directions for improving LLM-based SQL debugging capabilities in enterprise contexts.

Abstract: SQL is central to enterprise data engineering, yet generating fully correct SQL code in a single attempt remains difficult, even for experienced developers and advanced text-to-SQL LLMs, often requiring multiple debugging iterations. We introduce OurBench, the first benchmark for enterprise-level SQL reasoning and debugging. Our benchmark is built on two key innovations: (1) an automated construction workflow that uses reverse engineering to systematically inject realistic bugs into large-scale SQL code, enabling scalable and diverse benchmark generation; and (2) an execution-free evaluation framework tailored to enterprise settings, providing fast, accurate, and resource-efficient assessment.
  OurBench comprises 469 OurBenchSyn queries featuring syntax errors with explicit error messages, and 516 OurBenchSem queries targeting semantic errors in which the code fails to meet user intent. The queries are highly complex, averaging over 140 lines and featuring deep and wide abstract syntax trees.
  Evaluation of nearly 30 LLMs reveals a substantial performance gap: the best-performing model, Claude-4-Sonnet, achieves only 36.46 percent accuracy on OurBenchSyn and 32.17 percent on OurBenchSem, while most models score below 20 percent. We further explore four solution strategies, identify key challenges, and outline promising directions for enterprise SQL debugging with LLMs.

</details>


### [34] [Deadline-Aware, Energy-Efficient Control of Domestic Immersion Hot Water Heaters](https://arxiv.org/abs/2601.18123)
*Muhammad Ibrahim Khan,Bivin Pradeep,James Brusey*

Main category: cs.AI

TL;DR: This paper develops a reinforcement learning approach (PPO) for deadline-aware control of immersion water heaters, achieving 26-69% energy savings compared to traditional bang-bang control by optimizing when to heat water to reach target temperature efficiently.


<details>
  <summary>Details</summary>
Motivation: Domestic immersion water heaters operate continuously and inefficiently in winter, heating quickly rather than efficiently while ignoring predictable demand windows and ambient thermal losses. The goal is to minimize energy consumption while ensuring water reaches a target temperature by a specified deadline.

Method: Created a Gymnasium simulation environment modeling an immersion heater with first-order thermal losses and discrete on/off actions (0W/6000W at 120-second intervals). Compared three methods: a time-optimal bang-bang baseline, zero-shot Monte Carlo Tree Search (MCTS) planning, and a learned Proximal Policy Optimization (PPO) policy. Evaluated across parameter sweeps of initial temperature (10-30°C), deadline (30-90 steps), and target temperature (40-80°C).

Result: PPO achieved the best energy efficiency, consuming 3.23 kWh at a 60-step (2-hour) horizon compared to 4.37-10.45 kWh for bang-bang control and 4.18-6.46 kWh for MCTS. This represents 26% energy savings at 30 steps and 69% at 90 steps versus bang-bang. In a representative test case (50 kg water, 20°C ambient, 60°C target), PPO used 54% less energy than bang-bang and 33% less than MCTS.

Conclusion: Learned deadline-aware control policies significantly reduce energy consumption under identical physical dynamics. While MCTS planners provide partial savings without training, PPO offers superior performance with near-zero inference cost once trained, demonstrating the effectiveness of reinforcement learning for efficient thermal energy management.

Abstract: Typical domestic immersion water heater systems are often operated continuously during winter, heating quickly rather than efficiently and ignoring predictable demand windows and ambient losses. We study deadline-aware control, where the aim is to reach a target temperature at a specified time while minimising energy consumption. We introduce an efficient Gymnasium environment that models an immersion hot water heater with first-order thermal losses and discrete on and off actions of 0 W and 6000 W applied every 120 seconds. Methods include a time-optimal bang-bang baseline, a zero-shot Monte Carlo Tree Search planner, and a Proximal Policy Optimisation policy. We report total energy consumption in watt-hours under identical physical dynamics. Across sweeps of initial temperature from 10 to 30 degrees Celsius, deadline from 30 to 90 steps, and target temperature from 40 to 80 degrees Celsius, PPO achieves the most energy-efficient performance at a 60-step horizon of 2 hours, using 3.23 kilowatt-hours, compared to 4.37 to 10.45 kilowatt-hours for bang-bang control and 4.18 to 6.46 kilowatt-hours for MCTS. This corresponds to energy savings of 26 percent at 30 steps and 69 percent at 90 steps. In a representative trajectory with a 50 kg water mass, 20 degrees Celsius ambient temperature, and a 60 degrees Celsius target, PPO consumes 54 percent less energy than bang-bang control and 33 percent less than MCTS. These results show that learned deadline-aware control reduces energy consumption under identical physical assumptions, while planners provide partial savings without training and learned policies offer near-zero inference cost once trained.

</details>


### [35] [RouteMoA: Dynamic Routing without Pre-Inference Boosts Efficient Mixture-of-Agents](https://arxiv.org/abs/2601.18130)
*Jize Wang,Han Wu,Zhiyuan You,Yiming Song,Yijun Wang,Zifei Shan,Yining Li,Songyang Zhang,Xinyi Le,Cailian Chen,Xinping Guan,Dacheng Tao*

Main category: cs.AI

TL;DR: RouteMoA is a dynamic routing framework for Mixture-of-Agents that uses lightweight scoring and judging to avoid full inference, reducing cost by 89.8% and latency by 63.6% while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: The dense topology of traditional Mixture-of-Agents architectures leads to high computational costs and latency. Existing methods still require all models to run inference before filtering and lack effective model selection criteria, making them inefficient for large model pools.

Method: RouteMoA employs a three-stage approach: (1) a lightweight scorer predicts model performance from queries to narrow candidates without inference; (2) a mixture of judges refines scores through lightweight self- and cross-assessment using existing outputs; (3) a model ranking mechanism selects optimal models by balancing performance, cost, and latency.

Result: RouteMoA outperforms traditional MoA across diverse tasks and model pool sizes, achieving 89.8% cost reduction and 63.6% latency reduction in large-scale scenarios.

Conclusion: RouteMoA provides an efficient dynamic routing solution for MoA that dramatically reduces computational overhead while maintaining superior performance, making it practical for large-scale deployment.

Abstract: Mixture-of-Agents (MoA) improves LLM performance through layered collaboration, but its dense topology raises costs and latency. Existing methods employ LLM judges to filter responses, yet still require all models to perform inference before judging, failing to cut costs effectively. They also lack model selection criteria and struggle with large model pools, where full inference is costly and can exceed context limits. To address this, we propose RouteMoA, an efficient mixture-of-agents framework with dynamic routing. It employs a lightweight scorer to perform initial screening by predicting coarse-grained performance from the query, narrowing candidates to a high-potential subset without inference. A mixture of judges then refines these scores through lightweight self- and cross-assessment based on existing model outputs, providing posterior correction without additional inference. Finally, a model ranking mechanism selects models by balancing performance, cost, and latency. RouteMoA outperforms MoA across varying tasks and model pool sizes, reducing cost by 89.8% and latency by 63.6% in the large-scale model pool.

</details>


### [36] [RareAlert: Aligning heterogeneous large language model reasoning for early rare disease risk screening](https://arxiv.org/abs/2601.18132)
*Xi Chen,Hongru Zhou,Huahui Yi,Shiyu Feng,Hanyu Zhou,Tiancheng He,Mingke You,Li Wang,Qiankun Li,Kun Wang,Weili Fu,Kang Li,Jian Li*

Main category: cs.AI

TL;DR: 开发RareAlert系统，通过集成和校准10个大语言模型的推理，构建了一个能从未见信息中预测罕见病风险的轻量级模型，在包含15.8万病例的数据集上AUC达0.917，性能超越包括GPT-5在内的所有对比模型。


<details>
  <summary>Details</summary>
Motivation: 罕见病漏诊和延迟诊断是重大医疗挑战，现有初级诊疗分诊流程无法在初次就诊时可靠识别高风险患者，需要普筛来减少诊断延迟。

Method: 提出RareAlert系统，整合10个大语言模型的推理结果，通过机器学习进行校准和加权，蒸馏成单个可本地部署的Qwen3-4B模型；使用包含158,666例病例、覆盖33个孤儿疾病类别和7,000多种罕见病的真实世界数据集RareBench进行开发评估。

Result: 在独立测试集上AUC达到0.917，显著优于最佳机器学习集成模型和所有评估的大语言模型（包括GPT-5、DeepSeek-R1、Claude-3.7-Sonnet等），证明了大语言模型医疗推理的多样性以及校准对齐在高度不确定临床任务中的有效性。

Conclusion: RareAlert实现了准确、隐私保护且可扩展的罕见病风险筛查，适合大规模本地部署，为罕见病早期识别提供了可行解决方案。

Abstract: Missed and delayed diagnosis remains a major challenge in rare disease care. At the initial clinical encounters, physicians assess rare disease risk using only limited information under high uncertainty. When high-risk patients are not recognised at this stage, targeted diagnostic testing is often not initiated, resulting in missed diagnosis. Existing primary care triage processes are structurally insufficient to reliably identify patients with rare diseases at initial clinical presentation and universal screening is needed to reduce diagnostic delay. Here we present RareAlert, an early screening system which predict patient-level rare disease risk from routinely available primary-visit information. RareAlert integrates reasoning generated by ten LLMs, calibrates and weights these signals using machine learning, and distils the aligned reasoning into a single locally deployable model. To develop and evaluate RareAlert, we curated RareBench, a real-world dataset of 158,666 cases covering 33 Orphanet disease categories and more than 7,000 rare conditions, including both rare and non-rare presentations. The results showed that rare disease identification can be reconceptualised as a universal uncertainty resolution process applied to the general patient population. On an independent test set, RareAlert, a Qwen3-4B based model trained with calibrated reasoning signals, achieved an AUC of 0.917, outperforming the best machine learning ensemble and all evaluated LLMs, including GPT-5, DeepSeek-R1, Claude-3.7-Sonnet, o3-mini, Gemini-2.5-Pro, and Qwen3-235B. These findings demonstrate the diversity in LLM medical reasoning and the effectiveness of aligning such reasoning in highly uncertain clinical tasks. By incorporating calibrated reasoning into a single model, RareAlert enables accurate, privacy-preserving, and scalable rare disease risk screening suitable for large-scale local deployment.

</details>


### [37] [DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints](https://arxiv.org/abs/2601.18137)
*Yinger Zhang,Shutong Jiang,Renhao Li,Jianhong Tu,Yang Su,Lianghao Deng,Xudong Guo,Chenxu Lv,Junyang Lin*

Main category: cs.AI

TL;DR: The paper introduces DeepPlanning, a benchmark for evaluating long-horizon agent planning with real-world constraints, showing that current LLMs struggle with these tasks and highlighting areas for improvement.


<details>
  <summary>Details</summary>
Motivation: Existing LLM planning benchmarks focus on local reasoning rather than global constrained optimization and lack real-world elements like active information gathering and fine-grained local constraints.

Method: Introduced DeepPlanning benchmark with multi-day travel and multi-product shopping tasks requiring proactive information acquisition, local constrained reasoning, and global optimization.

Result: Frontier agentic LLMs struggle on DeepPlanning; explicit reasoning and parallel tool use are important for effectiveness-efficiency trade-offs.

Conclusion: Error analysis identifies improvement directions for long-horizon agentic LLMs, and code/data are open-sourced for future research.

Abstract: While agent evaluation has shifted toward long-horizon tasks, most benchmarks still emphasize local, step-level reasoning rather than the global constrained optimization (e.g., time and financial budgets) that demands genuine planning ability. Meanwhile, existing LLM planning benchmarks underrepresent the active information gathering and fine-grained local constraints typical of real-world settings. To address this, we introduce DeepPlanning, a challenging benchmark for practical long-horizon agent planning. It features multi-day travel planning and multi-product shopping tasks that require proactive information acquisition, local constrained reasoning, and global constrained optimization. Evaluations on DeepPlanning show that even frontier agentic LLMs struggle with these problems, highlighting the importance of reliable explicit reasoning patterns and parallel tool use for achieving better effectiveness-efficiency trade-offs. Error analysis further points to promising directions for improving agentic LLMs over long planning horizons. We open-source the code and data to support future research.

</details>


### [38] [Success Conditioning as Policy Improvement: The Optimization Problem Solved by Imitating Success](https://arxiv.org/abs/2601.18175)
*Daniel Russo*

Main category: cs.AI

TL;DR: Success conditioning is theoretically proven to be a trust-region optimization method that safely improves policies without performance degradation, with practical implications for techniques like return thresholding.


<details>
  <summary>Details</summary>
Motivation: Understanding what optimization problem success conditioning (a widely used technique for policy improvement that imitates successful trajectories) actually solves, despite being known by many names like rejection sampling with SFT, goal-conditioned RL, and Decision Transformers.

Method: Proving that success conditioning exactly solves a trust-region optimization problem, maximizing policy improvement subject to a χ² divergence constraint with data-determined radius, leading to an identity connecting relative policy improvement, policy change magnitude, and action-influence.

Result: Success conditioning emerges as a conservative improvement operator that cannot degrade performance or cause dangerous distribution shift; when it fails, it does so observably by barely changing the policy. Applied to return thresholding, it shows amplified improvement potential but with misalignment risks.

Conclusion: Success conditioning has a solid theoretical foundation as a safe, conservative policy improvement method, and the analysis clarifies trade-offs in practical implementations.

Abstract: A widely used technique for improving policies is success conditioning, in which one collects trajectories, identifies those that achieve a desired outcome, and updates the policy to imitate the actions taken along successful trajectories. This principle appears under many names -- rejection sampling with SFT, goal-conditioned RL, Decision Transformers -- yet what optimization problem it solves, if any, has remained unclear. We prove that success conditioning exactly solves a trust-region optimization problem, maximizing policy improvement subject to a $χ^2$ divergence constraint whose radius is determined automatically by the data. This yields an identity: relative policy improvement, the magnitude of policy change, and a quantity we call action-influence -- measuring how random variation in action choices affects success rates -- are exactly equal at every state. Success conditioning thus emerges as a conservative improvement operator. Exact success conditioning cannot degrade performance or induce dangerous distribution shift, but when it fails, it does so observably, by hardly changing the policy at all. We apply our theory to the common practice of return thresholding, showing this can amplify improvement, but at the cost of potential misalignment with the true objective.

</details>


### [39] [GAIA: A Data Flywheel System for Training GUI Test-Time Scaling Critic Models](https://arxiv.org/abs/2601.18197)
*Shaokang Wang,Pei Fu,Ruoceng Zhang,Shaojie Zhang,Xiuwen Xi,Jiahui Yang,Bin Qin,Ying Huang,Zhenbo Luo,Jian Luan*

Main category: cs.AI

TL;DR: 针对GUI agent操作不可逆导致错误放大的问题，本文提出GAIA数据飞轮系统，通过训练直觉批评模型(ICM)实时评估动作正确性，构建自我改进循环，在各类模型和数据集上显著提升测试时性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大视觉语言模型(LVLMs)显著提升了GUI agent的能力，但操作不可逆性仍是关键挑战——单个错误动作即可引发灾难性偏差，亟需提升agent的可靠性和测试时性能。

Method: 提出GUI Action Critic's Data Flywheel System (GAIA)，先基于正负样例训练直觉批评模型(ICM)评估agent动作的即时正确性，筛选高成功率操作；再用初始批评模型指导agent收集精炼样本，形成数据飞轮，迭代训练更强大的批评模型。

Result: 在多数据集实验表明，ICM能有效提升闭源和开源模型的测试时性能，且随着数据循环利用，性能可逐步提升。

Conclusion: GAIA框架通过构建自我改进的批评模型循环，为GUI agent提供了有效的测试时性能提升方案，增强了操作可靠性。

Abstract: While Large Vision-Language Models (LVLMs) have significantly advanced GUI agents' capabilities in parsing textual instructions, interpreting screen content, and executing tasks, a critical challenge persists: the irreversibility of agent operations, where a single erroneous action can trigger catastrophic deviations. To address this, we propose the GUI Action Critic's Data Flywheel System (GAIA), a training framework that enables the models to have iterative critic capabilities, which are used to improve the Test-Time Scaling (TTS) of basic GUI agents' performance. Specifically, we train an Intuitive Critic Model (ICM) using positive and negative action examples from a base agent first. This critic evaluates the immediate correctness of the agent's intended actions, thereby selecting operations with higher success probability. Then, the initial critic guides agent actions to collect refined positive/negative samples, initiating the self-improving cycle. The augmented data then trains a second-round critic with enhanced discernment capability. We conduct experiments on various datasets and demonstrate that the proposed ICM can improve the test-time performance of various closed-source and open-source models, and the performance can be gradually improved as the data is recycled. The code and dataset will be publicly released.

</details>


### [40] [Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents](https://arxiv.org/abs/2601.18217)
*Zhihan Liu,Lin Guan,Yixin Nie,Kai Zhang,Zhuoqun Hao,Lin Chen,Asli Celikyilmaz,Zhaoran Wang,Na Zhang*

Main category: cs.AI

TL;DR: This paper investigates how to improve LLM agents' generalization to unseen domains by analyzing RL environment properties and modeling choices, finding that state information richness and planning complexity are key factors, and proposing a simple randomization technique to enhance robustness.


<details>
  <summary>Details</summary>
Motivation: Generalist LLM agents are trained on narrow environments but deployed across broader, unseen domains, creating a critical generalization gap that needs addressing when test domains are unknown.

Method: The authors analyzed how different RL environment properties (state information richness, planning complexity, domain realism, text similarity) and modeling choices (SFT warmup, step-by-step thinking) affect cross-domain generalization of LLM agents.

Result: State information richness and planning complexity strongly correlate with generalization, while domain realism and text similarity are not primary factors. Adding distractive goal-irrelevant features improves robustness, and step-by-step thinking during RL preserves generalization, whereas SFT mid-training can undermine it.

Conclusion: To improve LLM agent generalization to unseen domains, prioritize increasing state information richness through techniques like distractive features and enable step-by-step thinking during RL, rather than relying on domain realism or SFT warmup.

Abstract: Generalist LLM agents are often post-trained on a narrow set of environments but deployed across far broader, unseen domains. In this work, we investigate the challenge of agentic post-training when the eventual test domains are unknown. Specifically, we analyze which properties of reinforcement learning (RL) environments and modeling choices have the greatest influence on out-of-domain performance. First, we identify two environment axes that strongly correlate with cross-domain generalization: (i) state information richness, i.e., the amount of information for the agent to process from the state, and (ii) planning complexity, estimated via goal reachability and trajectory length under a base policy. Notably, domain realism and text-level similarity are not the primary factors; for instance, the simple grid-world domain Sokoban leads to even stronger generalization in SciWorld than the more realistic ALFWorld. Motivated by these findings, we further show that increasing state information richness alone can already effectively improve cross-domain robustness. We propose a randomization technique, which is low-overhead and broadly applicable: add small amounts of distractive goal-irrelevant features to the state to make it richer without altering the task. Beyond environment-side properties, we also examine several modeling choices: (a) SFT warmup or mid-training helps prevent catastrophic forgetting during RL but undermines generalization to domains that are not included in the mid-training datamix; and (b) turning on step-by-step thinking during RL, while not always improving in-domain performance, plays a crucial role in preserving generalization.

</details>


### [41] [ShopSimulator: Evaluating and Exploring RL-Driven LLM Agent for Shopping Assistants](https://arxiv.org/abs/2601.18225)
*Pei Wang,Yanan Wu,Xiaoshuai Song,Weixun Wang,Gengru Chen,Zhongwen Li,Kezhong Yan,Ken Deng,Qi Liu,Shuaibing Zhao,Shaopan Xiong,Xuepeng Liu,Xuefeng Chen,Wanxi Deng,Wenbo Su,Bo Zheng*

Main category: cs.AI

TL;DR: 本文提出ShopSimulator，一个大规模中文购物环境，用于评估LLM智能体。研究发现即使最佳模型完整成功率也低于40%，主要弱点在于长轨迹搜索、个性化线索平衡和用户互动，但监督微调(SFT)与强化学习(RL)结合可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏能全面捕捉电商购物所有环节（个性化偏好理解、多轮对话、相似产品区分）的统一模拟环境，且仅关注评估而忽视训练支持。

Method: 开发大规模中文购物环境ShopSimulator；在多种场景下评估LLM；进行错误分析；探索监督微调(SFT)和强化学习(RL)等训练方法。

Result: 最佳模型完整成功率不足40%；识别出长轨迹搜索与产品选择、个性化线索使用失衡、用户互动不足等关键弱点；SFT与RL结合带来显著性能提升。

Conclusion: ShopSimulator为电商LLM智能体提供了重要基准，揭示了当前局限性，并证明适当的训练策略可大幅提升智能体能力。

Abstract: Large language model (LLM)-based agents are increasingly deployed in e-commerce shopping. To perform thorough, user-tailored product searches, agents should interpret personal preferences, engage in multi-turn dialogues, and ultimately retrieve and discriminate among highly similar products. However, existing research has yet to provide a unified simulation environment that consistently captures all of these aspects, and always focuses solely on evaluation benchmarks without training support. In this paper, we introduce ShopSimulator, a large-scale and challenging Chinese shopping environment. Leveraging ShopSimulator, we evaluate LLMs across diverse scenarios, finding that even the best-performing models achieve less than 40% full-success rate. Error analysis reveals that agents struggle with deep search and product selection in long trajectories, fail to balance the use of personalization cues, and to effectively engage with users. Further training exploration provides practical guidance for overcoming these weaknesses, with the combination of supervised fine-tuning (SFT) and reinforcement learning (RL) yielding significant performance improvements. Code and data will be released at https://github.com/ShopAgent-Team/ShopSimulator.

</details>


### [42] [Yunjue Agent Tech Report: A Fully Reproducible, Zero-Start In-Situ Self-Evolving Agent System for Open-Ended Tasks](https://arxiv.org/abs/2601.18226)
*Haotian Li,Shijun Yang,Weizhen Qi,Silei Zhao,Rui Hua,Mingzhu Song,Xiaojian Yang,Chao Peng*

Main category: cs.AI

TL;DR: 提出原位自进化范式，通过工具演化实现智能体在开放环境中的持续学习，Yunjue Agent在零样本和迁移设置下均表现优异，并开源相关资源。


<details>
  <summary>Details</summary>
Motivation: 传统智能体系统在任务分布持续漂移且外部监督稀缺的开放环境中表现不佳，其依赖静态工具集或离线训练导致能力边界僵化，无法动态适应新挑战。

Method: 提出原位自进化范式，将序列任务交互视为连续经验流，通过工具演化（提供可验证二元反馈）将短期执行反馈提炼为长期可复用能力，开发Yunjue Agent并引入并行批量演化策略优化效率。

Result: 在五个基准测试的零启动设置下性能显著超越现有基线；知识可无缝迁移至新领域；提出类似训练损失的新指标监控演化收敛。

Conclusion: 开源代码库、系统轨迹和演化工具，促进未来对弹性、自进化智能的研究。

Abstract: Conventional agent systems often struggle in open-ended environments where task distributions continuously drift and external supervision is scarce. Their reliance on static toolsets or offline training lags behind these dynamics, leaving the system's capability boundaries rigid and unknown. To address this, we propose the In-Situ Self-Evolving paradigm. This approach treats sequential task interactions as a continuous stream of experience, enabling the system to distill short-term execution feedback into long-term, reusable capabilities without access to ground-truth labels. Within this framework, we identify tool evolution as the critical pathway for capability expansion, which provides verifiable, binary feedback signals. Within this framework, we develop Yunjue Agent, a system that iteratively synthesizes, optimizes, and reuses tools to navigate emerging challenges. To optimize evolutionary efficiency, we further introduce a Parallel Batch Evolution strategy. Empirical evaluations across five diverse benchmarks under a zero-start setting demonstrate significant performance gains over proprietary baselines. Additionally, complementary warm-start evaluations confirm that the accumulated general knowledge can be seamlessly transferred to novel domains. Finally, we propose a novel metric to monitor evolution convergence, serving as a function analogous to training loss in conventional optimization. We open-source our codebase, system traces, and evolved tools to facilitate future research in resilient, self-evolving intelligence.

</details>


### [43] [A Generative AI-Driven Reliability Layer for Action-Oriented Disaster Resilience](https://arxiv.org/abs/2601.18308)
*Geunsik Lim*

Main category: cs.AI

TL;DR: 提出Climate RADAR生成式AI系统，通过整合多源数据与护栏LLM，将灾害预警从"警报发送"转变为"行动执行"，经模拟、用户研究和市政试点验证可提升防护行动率、降低延迟并增强信任


<details>
  <summary>Details</summary>
Motivation: 传统预警系统虽快速发布警报，但无法有效触发及时防护行动，导致可预防损失和应对不平等问题

Method: 构建风险感知动态推荐系统，融合气象、水文、脆弱性及社会数据生成复合风险指数，采用护栏嵌入的大型语言模型为市民、志愿者和市政部门提供个性化行动建议，通过模拟实验、用户研究和市政试点评估

Result: 显著提高防护行动执行率，缩短响应延迟，提升系统可用性和用户信任度

Conclusion: 推动以人为本、透明且公平的预警系统发展，为构建合规灾害抵御基础设施提供实践路径

Abstract: As climate-related hazards intensify, conventional early warning systems (EWS) disseminate alerts rapidly but often fail to trigger timely protective actions, leading to preventable losses and inequities. We introduce Climate RADAR (Risk-Aware, Dynamic, and Action Recommendation system), a generative AI-based reliability layer that reframes disaster communication from alerts delivered to actions executed. It integrates meteorological, hydrological, vulnerability, and social data into a composite risk index and employs guardrail-embedded large language models (LLMs) to deliver personalized recommendations across citizen, volunteer, and municipal interfaces. Evaluation through simulations, user studies, and a municipal pilot shows improved outcomes, including higher protective action execution, reduced response latency, and increased usability and trust. By combining predictive analytics, behavioral science, and responsible AI, Climate RADAR advances people-centered, transparent, and equitable early warning systems, offering practical pathways toward compliance-ready disaster resilience infrastructures.

</details>


### [44] [Can Good Writing Be Generative? Expert-Level AI Writing Emerges through Fine-Tuning on High-Quality Books](https://arxiv.org/abs/2601.18353)
*Tuhin Chakrabarty,Paramveer S. Dhillon*

Main category: cs.AI

TL;DR: A study shows that while expert writers initially prefer human writing over AI, this reverses after AI is fine-tuned on authors' works, causing identity crises among experts and challenging assumptions about AI's creative limitations.


<details>
  <summary>Details</summary>
Motivation: To understand how generative AI challenges the assumption that creative writing is a uniquely human endeavor, and to explore the behavioral and psychological impact on human writers when competing with AI that can emulate literary styles.

Method: Conducted a behavioral experiment where 28 MFA writers competed against three LLMs in emulating 50 acclaimed authors. Used blind pairwise comparisons judged by 28 expert judges and 131 lay judges, with two conditions: in-context prompting and fine-tuning on authors' complete works. Followed up with debrief interviews with expert writers.

Result: Under in-context prompting, experts preferred human writing 82.7% of the time; after fine-tuning, experts preferred AI writing 62% of the time. Lay judges consistently preferred AI writing. Expert writers reported identity crisis, eroded aesthetic confidence, and questioned what constitutes "good writing".

Conclusion: Generative AI challenges discourse about AI's creative limitations, reverses expert preferences for human writing after fine-tuning, and raises fundamental questions about the future of creative labor as AI can successfully emulate human creative styles.

Abstract: Creative writing has long been considered a uniquely human endeavor, requiring voice and style that machines could not replicate. This assumption is challenged by Generative AI that can emulate thousands of author styles in seconds with negligible marginal labor. To understand this better, we conducted a behavioral experiment where 28 MFA writers (experts) competed against three LLMs in emulating 50 critically acclaimed authors. Based on blind pairwise comparisons by 28 expert judges and 131 lay judges, we find that experts preferred human writing in 82.7% of cases under the in-context prompting condition but this reversed to 62% preference for AI after fine-tuning on authors' complete works. Lay judges, however, consistently preferred AI writing. Debrief interviews with expert writers revealed that their preference for AI writing triggered an identity crisis, eroding aesthetic confidence and questioning what constitutes "good writing." These findings challenge discourse about AI's creative limitations and raise fundamental questions about the future of creative labor.

</details>


### [45] [Dynamic Thinking-Token Selection for Efficient Reasoning in Large Reasoning Models](https://arxiv.org/abs/2601.18383)
*Zhenyuan Guo,Tong Chen,Wenlong Meng,Chen Gong,Xin Yu,Chengkun Wei,Wenzhi Chen*

Main category: cs.AI

TL;DR: 该论文提出 Dynamic Thinking-Token Selection (DynTS) 方法，通过注意力分析识别推理过程中的决策关键 token，仅保留其 KV 缓存状态以优化大型推理模型的效率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过生成推理轨迹来解决问题，但这些扩展生成会消耗大量内存和计算资源，成为效率瓶颈。

Method: 利用注意力图分析推理轨迹，发现只有部分决策关键 token 会影响最终答案，据此提出 DynTS 方法，在推理时仅保留关键 token 的 KV 缓存，剔除冗余条目。

Result: 通过选择性保留 KV 缓存，显著降低了推理过程中的内存占用和计算开销，从而优化了模型效率。

Conclusion: DynTS 方法通过注意力机制识别并保留关键推理 token，有效提升了大型推理模型的推理效率，为可扩展的推理模型提供了新思路。

Abstract: Large Reasoning Models (LRMs) excel at solving complex problems by explicitly generating a reasoning trace before deriving the final answer. However, these extended generations incur substantial memory footprint and computational overhead, bottlenecking LRMs' efficiency. This work uses attention maps to analyze the influence of reasoning traces and uncover an interesting phenomenon: only some decision-critical tokens in a reasoning trace steer the model toward the final answer, while the remaining tokens contribute negligibly. Building on this observation, we propose Dynamic Thinking-Token Selection (DynTS). This method identifies decision-critical tokens and retains only their associated Key-Value (KV) cache states during inference, evicting the remaining redundant entries to optimize efficiency.

</details>


### [46] [OffSeeker: Online Reinforcement Learning Is Not All You Need for Deep Research Agents](https://arxiv.org/abs/2601.18467)
*Yuhang Zhou,Kai Zheng,Qiguang Chen,Mengkang Hu,Qingfeng Sun,Can Xu,Jingjing Chen*

Main category: cs.AI

TL;DR: 该论文提出了一种完全离线的深度研究智能体训练方案，通过开源的DeepForge框架和大规模数据集，训练出8B参数的OffSeeker模型，其性能不仅领先同尺寸模型，还能媲美30B参数且经过昂贵在线强化学习的系统。


<details>
  <summary>Details</summary>
Motivation: 当前高性能深度研究智能体依赖在线强化学习，但API调用成本高昂；而离线训练受限于高质量研究轨迹数据的稀缺性。论文旨在证明无需昂贵的在线RL也能构建强大的研究智能体。

Method: 开发完全开源的离线训练套件：1) DeepForge任务合成框架，可生成大规模研究查询；2) 整理66k问答对、33k SFT轨迹和21k DPO对的数据集；3) 完全离线训练OffSeeker(8B)模型。

Result: 在六个基准测试上，OffSeeker在8B参数规模模型中表现领先，同时能与30B参数且经过重度在线RL训练的模型保持竞争力。

Conclusion: 昂贵的在线强化学习并非构建强大研究智能体的唯一途径。通过高质量数据合成和离线训练，可以在显著降低成本的同时达到接近先进水平的性能，为研究社区提供了更高效的替代方案。

Abstract: Deep research agents have shown remarkable potential in handling long-horizon tasks. However, state-of-the-art performance typically relies on online reinforcement learning (RL), which is financially expensive due to extensive API calls. While offline training offers a more efficient alternative, its progress is hindered by the scarcity of high-quality research trajectories. In this paper, we demonstrate that expensive online reinforcement learning is not all you need to build powerful research agents. To bridge this gap, we introduce a fully open-source suite designed for effective offline training. Our core contributions include DeepForge, a ready-to-use task synthesis framework that generates large-scale research queries without heavy preprocessing; and a curated collection of 66k QA pairs, 33k SFT trajectories, and 21k DPO pairs. Leveraging these resources, we train OffSeeker (8B), a model developed entirely offline. Extensive evaluations across six benchmarks show that OffSeeker not only leads among similar-sized agents but also remains competitive with 30B-parameter systems trained via heavy online RL.

</details>


### [47] [DEEPMED: Building a Medical DeepResearch Agent via Multi-hop Med-Search Data and Turn-Controlled Agentic Training & Inference](https://arxiv.org/abs/2601.18496)
*Zihan wang,Hao Wang,Shi Feng,Xiaocui Yang,Daling Wang,Yiqun Zhang,Jinghao Lin,Haihua Yang,Xiaozhong Ji*

Main category: cs.AI

TL;DR: The paper proposes DeepMed to address limitations of general DeepResearch models in medical domains, which suffer from lack of clinical context reasoning and excessive tool calls. By using medical-specific data synthesis, difficulty-aware training penalties, and a monitoring mechanism, DeepMed achieves 9.79% average improvement on seven medical benchmarks.


<details>
  <summary>Details</summary>
Motivation: Medical reasoning models constrained by parametric knowledge face forgetting and hallucinations. General DeepResearch models ground outputs in evidence but transfer poorly to medical fields due to two gaps: (1) inability to interpret evidence within knowledge-intensive clinical contexts ("find it but fail to use it"), and (2) blind tool-call scaling injecting noisy context that derails sensitive medical reasoning.

Method: DeepMed comprises three key components: (1) Data: multi-hop medical search QA synthesis to adapt DeepResearch paradigm for medical contexts; (2) Training: difficulty-aware turn-penalty to suppress excessive tool-call growth; (3) Inference: a monitor to validate hypotheses within controlled steps and prevent context rot.

Result: On seven medical benchmarks, DeepMed improves its base model by 9.79% on average and outperforms larger medical reasoning and DeepResearch models.

Conclusion: DeepMed successfully bridges the gap between general DeepResearch and medical domain requirements by incorporating clinical context understanding and controlled tool-use, demonstrating significant performance gains in medical reasoning tasks.

Abstract: Medical reasoning models remain constrained by parametric knowledge and are thus susceptible to forgetting and hallucinations. DeepResearch (DR) models ground outputs in verifiable evidence from tools and perform strongly in general domains, but their direct transfer to medical field yields relatively limited gains. We attribute this to two gaps: task characteristic and tool-use scaling. Medical questions require evidence interpretation in a knowledge-intensive clinical context; while general DR models can retrieve information, they often lack clinical-context reasoning and thus "find it but fail to use it," leaving performance limited by medical abilities. Moreover, in medical scenarios, blindly scaling tool-call can inject noisy context, derailing sensitive medical reasoning and prompting repetitive evidence-seeking along incorrect paths. Therefore, we propose DeepMed. For data, we deploy a multi-hop med-search QA synthesis method supporting the model to apply the DR paradigm in medical contexts. For training, we introduce a difficulty-aware turn-penalty to suppress excessive tool-call growth. For inference, we bring a monitor to help validate hypotheses within a controlled number of steps and avoid context rot. Overall, on seven medical benchmarks, DeepMed improves its base model by 9.79\% on average and outperforms larger medical reasoning and DR models.

</details>


### [48] [Deconstructing Instruction-Following: A New Benchmark for Granular Evaluation of Large Language Model Instruction Compliance Abilities](https://arxiv.org/abs/2601.18554)
*Alberto Purpura,Li Wang,Sahil Badyal,Eugenio Beaufrand,Adam Faulkner*

Main category: cs.AI

TL;DR: 提出MOSAIC框架，通过动态生成含20个约束的数据集，实现LLM复杂指令遵循能力的细粒度独立评估，揭示合规性因约束类型、数量和位置而异，并发现模型特定弱点及位置偏见。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法反映真实世界使用场景，或无法将指令遵循能力与任务成功率分离，导致难以准确评估LLM遵循复杂指令的可靠性。

Method: 开发MOSAIC（模块化合成指令遵循评估）框架，使用动态生成的数据集，包含最多20个面向应用的生成约束，实现对该能力的细粒度和独立分析。

Result: 评估五个不同模型家族的LLM，发现指令遵循能力非单一特性，而是随约束类型、数量和位置显著变化；揭示了模型特定弱点、指令间的协同与冲突作用，以及首因效应和近因效应等位置偏见。

Conclusion: 这种细粒度洞察对于诊断模型失败、开发需要严格遵守复杂指令的更可靠LLM系统至关重要。

Abstract: Reliably ensuring Large Language Models (LLMs) follow complex instructions is a critical challenge, as existing benchmarks often fail to reflect real-world use or isolate compliance from task success. We introduce MOSAIC (MOdular Synthetic Assessment of Instruction Compliance), a modular framework that uses a dynamically generated dataset with up to 20 application-oriented generation constraints to enable a granular and independent analysis of this capability. Our evaluation of five LLMs from different families based on this new benchmark demonstrates that compliance is not a monolithic capability but varies significantly with constraint type, quantity, and position. The analysis reveals model-specific weaknesses, uncovers synergistic and conflicting interactions between instructions, and identifies distinct positional biases such as primacy and recency effects. These granular insights are critical for diagnosing model failures and developing more reliable LLMs for systems that demand strict adherence to complex instructions.

</details>


### [49] [Stability as a Liability:Systematic Breakdown of Linguistic Structure in LLMs](https://arxiv.org/abs/2601.18588)
*Xianzhe Meng,Qiangsheng Zeng,Ling Luo,Qinghan Yang,Jiarui Hao,Wenbo Wu,Qinyu Wang,Rui Yin,Lin Qi,Renzhi Lu*

Main category: cs.AI

TL;DR: 该研究发现稳定训练会降低生成熵，导致模型概率质量集中在少数模式上，出现系统性退化，尽管损失平滑收敛，表明训练稳定性不足以保证生成质量。


<details>
  <summary>Details</summary>
Motivation: 探究训练稳定性对生成分布的影响，挑战"稳定训练是可靠优化前提"的传统观点，揭示稳定性与生成质量之间的潜在矛盾。

Method: 理论分析表明稳定参数轨迹近似最小化前向KL散度；通过可控反馈训练框架实证验证，观察不同架构和随机种子下的低熵输出和重复行为。

Result: 稳定训练使模型隐式降低生成熵，概率质量集中于有限经验模式，出现系统性退化现象，即使损失平滑收敛也无法避免。

Conclusion: 优化稳定性与生成表达性并非内在一致，仅依赖稳定性作为优化质量指标是不充分的，需要更全面的评估标准。

Abstract: Training stability is typically regarded as a prerequisite for reliable optimization in large language models. In this work, we analyze how stabilizing training dynamics affects the induced generation distribution. We show that under standard maximum likelihood training, stable parameter trajectories lead stationary solutions to approximately minimize the forward KL divergence to the empirical distribution, while implicitly reducing generative entropy. As a consequence, the learned model can concentrate probability mass on a limited subset of empirical modes, exhibiting systematic degeneration despite smooth loss convergence. We empirically validate this effect using a controlled feedback-based training framework that stabilizes internal generation statistics, observing consistent low-entropy outputs and repetitive behavior across architectures and random seeds. It indicates that optimization stability and generative expressivity are not inherently aligned, and that stability alone is an insufficient indicator of generative quality.

</details>


### [50] [A Balanced Neuro-Symbolic Approach for Commonsense Abductive Logic](https://arxiv.org/abs/2601.18595)
*Joseph Cotnareanu,Didier Chetelat,Yingxue Zhang,Mark Coates*

Main category: cs.AI

TL;DR: A novel method combining LLMs and logic solvers through iterative feedback to augment logical problems with commonsense relations, improving reasoning on complex proof planning tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex proof planning despite good formal reasoning abilities. Logic solvers are efficient but cannot handle missing commonsense relations that are often needed in human contexts.

Method: An iterative approach that uses logic solver feedback to guide LLMs in adding commonsense relations to logic problems, with a search procedure to find useful facts while maintaining computational tractability.

Result: Consistent considerable improvements over existing techniques on pure-logical reasoning datasets with removed commonsense information.

Conclusion: The method demonstrates value in balancing neural (LLM) and symbolic (logic solver) elements for reasoning in human contexts where commonsense knowledge is essential.

Abstract: Although Large Language Models (LLMs) have demonstrated impressive formal reasoning abilities, they often break down when problems require complex proof planning. One promising approach for improving LLM reasoning abilities involves translating problems into formal logic and using a logic solver. Although off-the-shelf logic solvers are in principle substantially more efficient than LLMs at logical reasoning, they assume that all relevant facts are provided in a question and are unable to deal with missing commonsense relations. In this work, we propose a novel method that uses feedback from the logic solver to augment a logic problem with commonsense relations provided by the LLM, in an iterative manner. This involves a search procedure through potential commonsense assumptions to maximize the chance of finding useful facts while keeping cost tractable. On a collection of pure-logical reasoning datasets, from which some commonsense information has been removed, our method consistently achieves considerable improvements over existing techniques, demonstrating the value in balancing neural and symbolic elements when working in human contexts.

</details>


### [51] [PolySHAP: Extending KernelSHAP with Interaction-Informed Polynomial Regression](https://arxiv.org/abs/2601.18608)
*Fabian Fumagalli,R. Teal Witter,Christopher Musco*

Main category: cs.AI

TL;DR: The paper extends KernelSHAP by using polynomial approximations (PolySHAP) to capture non-linear feature interactions for better Shapley value estimation, and proves paired sampling is equivalent to second-order PolySHAP, providing the first theoretical justification for this heuristic.


<details>
  <summary>Details</summary>
Motivation: Exact Shapley value computation requires exponential cost (2^d evaluations), and while KernelSHAP avoids this with linear approximation, it cannot capture non-linear feature interactions, limiting accuracy.

Method: Proposes PolySHAP which approximates the game function using higher-degree polynomials to model non-linear feature interactions, and theoretically connects this approach to paired sampling (antithetic sampling).

Result: PolySHAP yields empirically better and consistent Shapley value estimates on benchmark datasets; paired sampling is mathematically equivalent to second-order PolySHAP, explaining its practical success.

Conclusion: The work provides both an improved estimation method (PolySHAP) and the first strong theoretical foundation for the widely-used paired sampling heuristic in KernelSHAP.

Abstract: Shapley values have emerged as a central game-theoretic tool in explainable AI (XAI). However, computing Shapley values exactly requires $2^d$ game evaluations for a model with $d$ features. Lundberg and Lee's KernelSHAP algorithm has emerged as a leading method for avoiding this exponential cost. KernelSHAP approximates Shapley values by approximating the game as a linear function, which is fit using a small number of game evaluations for random feature subsets.
  In this work, we extend KernelSHAP by approximating the game via higher degree polynomials, which capture non-linear interactions between features. Our resulting PolySHAP method yields empirically better Shapley value estimates for various benchmark datasets, and we prove that these estimates are consistent.
  Moreover, we connect our approach to paired sampling (antithetic sampling), a ubiquitous modification to KernelSHAP that improves empirical accuracy. We prove that paired sampling outputs exactly the same Shapley value approximations as second-order PolySHAP, without ever fitting a degree 2 polynomial. To the best of our knowledge, this finding provides the first strong theoretical justification for the excellent practical performance of the paired sampling heuristic.

</details>


### [52] [Assessing the Quality of Mental Health Support in LLM Responses through Multi-Attribute Human Evaluation](https://arxiv.org/abs/2601.18630)
*Abeer Badawi,Md Tahmid Rahman Laskar,Elahe Rahimi,Sheri Grach,Lindsay Bertrand,Lames Danok,Frank Rudzicz,Jimmy Huang,Elham Dolatabadi*

Main category: cs.AI

TL;DR: 该研究通过专家评估9个大语言模型在500个心理健康对话中的表现，发现LLMs具备强大的认知可靠性，但在情感共鸣方面表现不稳定，存在认知-情感鸿沟，需要建立以临床为基础、重视关系敏感性的评估框架。


<details>
  <summary>Details</summary>
Motivation: 全球心理健康危机持续加剧，存在治疗缺口、资源可及性不足和专业治疗师短缺等问题。大语言模型虽为可扩展的心理支持提供了前景，但其可靠性、治疗相关性及与人类标准的对齐仍面临挑战。

Method: 采用以人为本的评估方法：从真实世界场景数据集中整理500个心理健康对话，由9个不同的大语言模型（包括闭源和开源模型）生成回复。由两名受过精神病学培训专家，使用包含6个属性的评估量表，独立对每个回复进行5点李克特量表评分，重点关注认知支持和情感共鸣两个维度。

Result: LLMs在认知可靠性方面表现强劲，能提供安全、连贯且临床适宜的信息，但在情感对齐方面不稳定。闭源模型（如GPT-4o）能提供平衡的治疗性回复，而开源模型表现出更大的变异性和情感平淡。研究揭示了持续的认知-情感鸿沟。

Conclusion: 强调需要建立具有临床基础、重视关系敏感性而非仅信息准确性的评估框架。倡导采用人机协同的平衡评估协议，为心理健康导向的对话AI提供负责任的设计和临床监督指导框架。

Abstract: The escalating global mental health crisis, marked by persistent treatment gaps, availability, and a shortage of qualified therapists, positions Large Language Models (LLMs) as a promising avenue for scalable support. While LLMs offer potential for accessible emotional assistance, their reliability, therapeutic relevance, and alignment with human standards remain challenging to address. This paper introduces a human-grounded evaluation methodology designed to assess LLM generated responses in therapeutic dialogue. Our approach involved curating a dataset of 500 mental health conversations from datasets with real-world scenario questions and evaluating the responses generated by nine diverse LLMs, including closed source and open source models. More specifically, these responses were evaluated by two psychiatric trained experts, who independently rated each on a 5 point Likert scale across a comprehensive 6 attribute rubric. This rubric captures Cognitive Support and Affective Resonance, providing a multidimensional perspective on therapeutic quality. Our analysis reveals that LLMs provide strong cognitive reliability by producing safe, coherent, and clinically appropriate information, but they demonstrate unstable affective alignment. Although closed source models (e.g., GPT-4o) offer balanced therapeutic responses, open source models show greater variability and emotional flatness. We reveal a persistent cognitive-affective gap and highlight the need for failure aware, clinically grounded evaluation frameworks that prioritize relational sensitivity alongside informational accuracy in mental health oriented LLMs. We advocate for balanced evaluation protocols with human in the loop that center on therapeutic sensitivity and provide a framework to guide the responsible design and clinical oversight of mental health oriented conversational AI.

</details>


### [53] [AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning](https://arxiv.org/abs/2601.18631)
*Mingyang Song,Haoyu Sun,Jiawei Gu,Linjie Li,Luxin Xu,Ranjay Krishna,Yu Cheng*

Main category: cs.AI

TL;DR: AdaReasoner通过将工具使用学习为通用推理技能而非特定工具行为，实现了视觉推理的突破。它采用可扩展数据管道、Tool-GRPO强化学习和自适应机制，使模型能自主选择和协调工具，在多个基准测试中超越GPT-5，性能提升达24.9%。


<details>
  <summary>Details</summary>
Motivation: 当人类遇到超出直接能力的问题时，会依赖工具。对于多模态大语言模型，有效的视觉推理关键在于知道何时使用何种工具以及如何多步组合工具，尤其是在面对新工具或新任务时。

Method: AdaReasoner通过三个核心组件实现：(i)可扩展的数据构建管道，让模型接触长周期、多步工具交互；(ii)Tool-GRPO强化学习算法，基于最终任务成功优化工具选择和排序；(iii)自适应学习机制，动态调节工具使用。

Result: 实验表明，AdaReasoner展现出强大的工具适应和泛化能力：它能自主采用有益工具、抑制无关工具，并根据任务需求调整工具使用频率。在多个挑战性基准测试中达到最先进性能，平均提升7B基础模型24.9%，并在VSP和Jigsaw等任务上超越GPT-5。

Conclusion: 这些组件使模型能够从任务上下文和中间结果中推断工具效用，实现多工具协调和未见工具的泛化，为多模态推理提供了新范式。

Abstract: When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw.

</details>


### [54] [FadeMem: Biologically-Inspired Forgetting for Efficient Agent Memory](https://arxiv.org/abs/2601.18642)
*Lei Wei,Xu Dong,Xiao Peng,Niantao Xie,Bin Wang*

Main category: cs.AI

TL;DR: This paper proposes FadeMem, a biologically-inspired memory architecture for LLM agents that implements active forgetting through adaptive exponential decay across a dual-layer hierarchy, achieving 45% storage reduction while improving multi-hop reasoning and retrieval performance.


<details>
  <summary>Details</summary>
Motivation: LLM agents lack selective forgetting mechanisms, suffering from either catastrophic forgetting or information overload due to binary retention strategies, unlike human memory's adaptive decay processes that balance retention and forgetting.

Method: FadeMem uses differential decay rates in a dual-layer memory hierarchy governed by adaptive exponential decay functions based on semantic relevance, access frequency, and temporal patterns, with LLM-guided conflict resolution and intelligent memory fusion.

Result: Experiments on Multi-Session Chat, LoCoMo, and LTI-Bench show superior multi-hop reasoning and retrieval performance with 45% storage reduction.

Conclusion: Biologically-inspired active forgetting mechanisms are effective for agent memory systems, improving efficiency and performance while reducing storage requirements.

Abstract: Large language models deployed as autonomous agents face critical memory limitations, lacking selective forgetting mechanisms that lead to either catastrophic forgetting at context boundaries or information overload within them. While human memory naturally balances retention and forgetting through adaptive decay processes, current AI systems employ binary retention strategies that preserve everything or lose it entirely. We propose FadeMem, a biologically-inspired agent memory architecture that incorporates active forgetting mechanisms mirroring human cognitive efficiency. FadeMem implements differential decay rates across a dual-layer memory hierarchy, where retention is governed by adaptive exponential decay functions modulated by semantic relevance, access frequency, and temporal patterns. Through LLM-guided conflict resolution and intelligent memory fusion, our system consolidates related information while allowing irrelevant details to fade. Experiments on Multi-Session Chat, LoCoMo, and LTI-Bench demonstrate superior multi-hop reasoning and retrieval with 45\% storage reduction, validating the effectiveness of biologically-inspired forgetting in agent memory systems.

</details>


### [55] [Health-SCORE: Towards Scalable Rubrics for Improving Health-LLMs](https://arxiv.org/abs/2601.18706)
*Zhichao Yang,Sepehr Janghorbani,Dongxu Zhang,Jun Han,Qian Qian,Andrew Ressler,Gregory D. Lyng,Sanjit Singh Batra,Robert E. Tillman*

Main category: cs.AI

TL;DR: Health-SCORE is a scalable framework for rubric-based LLM evaluation in healthcare that reduces development costs while maintaining quality, enabling both RL training and in-context learning improvements.


<details>
  <summary>Details</summary>
Motivation: Creating high-quality, domain-specific rubrics for evaluating open-ended LLM responses in safety-critical domains like healthcare requires significant human expertise, time, and cost, limiting scalability.

Method: The paper introduces Health-SCORE, a generalizable and scalable rubric-based framework that can be used as a structured reward signal for reinforcement learning and incorporated into prompts for in-context learning.

Result: Health-SCORE achieves evaluation quality comparable to human-created rubrics across open-ended healthcare tasks while significantly reducing development effort.

Conclusion: Health-SCORE makes rubric-based evaluation and training more scalable by substantially lowering costs without sacrificing performance.

Abstract: Rubrics are essential for evaluating open-ended LLM responses, especially in safety-critical domains such as healthcare. However, creating high-quality and domain-specific rubrics typically requires significant human expertise time and development cost, making rubric-based evaluation and training difficult to scale. In this work, we introduce Health-SCORE, a generalizable and scalable rubric-based training and evaluation framework that substantially reduces rubric development costs without sacrificing performance. We show that Health-SCORE provides two practical benefits beyond standalone evaluation: it can be used as a structured reward signal to guide reinforcement learning with safety-aware supervision, and it can be incorporated directly into prompts to improve response quality through in-context learning. Across open-ended healthcare tasks, Health-SCORE achieves evaluation quality comparable to human-created rubrics while significantly lowering development effort, making rubric-based evaluation and training more scalable.

</details>


### [56] [Conditioned Generative Modeling of Molecular Glues: A Realistic AI Approach for Synthesizable Drug-like Molecules](https://arxiv.org/abs/2601.18716)
*Naeyma N. Islam,Thomas R. Caulfield*

Main category: cs.AI

TL;DR: This paper develops an AI-driven drug design framework to degrade intracellular amyloid beta-42 (Abeta-42) in Alzheimer's disease by engineering E3 ligase-targeting molecular glues that activate the ubiquitin-proteasome system.


<details>
  <summary>Details</summary>
Motivation: Intracellular Abeta-42 is identified as an early toxic driver of Alzheimer's progression, but existing therapies focus on extracellular plaques; there is a critical need for strategies that directly eliminate intracellular Abeta-42 to halt synaptic dysfunction and neurodegeneration.

Method: Combines structure-based modeling of Abeta-42/E3 ligase (CRBN, VHL, MDM2) ternary complexes with a novel Ligase-Conditioned Junction Tree Variational Autoencoder (LC-JT-VAE) that integrates protein sequence embeddings and torsional angle-aware molecular graphs to generate target-specific molecular glues.

Result: The LC-JT-VAE model successfully generated chemically valid, novel small molecules capable of selectively promoting Abeta-42 degradation via the ubiquitin-proteasome system, validated through computational ADMET screening and docking simulations.

Conclusion: This AI-integrated approach provides a transformative framework for designing targeted protein degradation therapies, offering new avenues for treating neurodegenerative diseases by addressing intracellular pathogenic proteins.

Abstract: Alzheimer's disease (AD) is marked by the pathological accumulation of amyloid beta-42 (Abeta-42), contributing to synaptic dysfunction and neurodegeneration. While extracellular amyloid plaques are well-studied, increasing evidence highlights intracellular Abeta-42 as an early and toxic driver of disease progression. In this study, we present a novel, AI-assisted drug design approach to promote targeted degradation of Abeta-42 via the ubiquitin-proteasome system (UPS), using E3 ligase-directed molecular glues. We systematically evaluated the ternary complex formation potential of Abeta-42 with three E3 ligases: CRBN, VHL, and MDM2, through structure-based modeling, ADMET screening, and docking. We then developed a Ligase-Conditioned Junction Tree Variational Autoencoder (LC-JT-VAE) to generate ligase-specific small molecules, incorporating protein sequence embeddings and torsional angle-aware molecular graphs. Our results demonstrate that this generative model can produce chemically valid, novel, and target-specific molecular glues capable of facilitating Abeta-42 degradation. This integrated approach offers a promising framework for designing UPS-targeted therapies for neurodegenerative diseases.

</details>


### [57] [TSRBench: A Comprehensive Multi-task Multi-modal Time Series Reasoning Benchmark for Generalist Models](https://arxiv.org/abs/2601.18744)
*Fangxu Yu,Xingang Guo,Lingzhi Yuan,Haoqiang Kang,Hongyu Zhao,Lianhui Qin,Furong Huang,Bin Hu,Tianyi Zhou*

Main category: cs.AI

TL;DR: 本文提出了TSRBench，首个全面评估通用模型时间序列推理能力的多模态基准测试，涵盖4大维度15项任务，揭示现有模型在预测能力和多模态融合方面存在显著瓶颈。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据在能源管理、交通控制等关键应用中无处不在，但现有通用模型基准测试普遍缺乏对时间序列推理能力的评估，这一维度对于解决实际问题至关重要。

Method: 构建TSRBench基准，包含14个领域的4125个问题，分为感知、推理、预测和决策4大维度，设计15项任务评估核心推理能力。对30余种主流大模型、视觉语言模型和时间序列专用模型进行系统评测。

Result: 发现三大关键现象：1) 感知和推理能力遵循缩放定律，但预测能力不遵循；2) 强推理能力不能保证准确的上下文感知预测，语义理解与数值预测存在解耦；3) 现有多模态模型未能有效融合文本和视觉表征以获得性能增益。

Conclusion: TSRBench为时间序列推理提供了标准化评估平台，揭示了当前模型的局限性，为推进通用模型发展提供了重要洞察和方向。

Abstract: Time series data is ubiquitous in real-world scenarios and crucial for critical applications ranging from energy management to traffic control. Consequently, the ability to reason over time series is a fundamental skill for generalist models to solve practical problems. However, this dimension is notably absent from existing benchmarks of generalist models. To bridge this gap, we introduce TSRBench, a comprehensive multi-modal benchmark designed to stress-test the full spectrum of time series reasoning capabilities. TSRBench features: i) a diverse set of 4125 problems from 14 domains, and is categorized into 4 major dimensions: Perception, Reasoning, Prediction, and Decision-Making. ii) 15 tasks from the 4 dimensions evaluating essential reasoning capabilities (e.g., numerical reasoning). Through extensive experiments, we evaluated over 30 leading proprietary and open-source LLMs, VLMs, and TSLLMs within TSRBench. Our findings reveal that: i) scaling laws hold for perception and reasoning but break down for prediction; ii) strong reasoning does not guarantee accurate context-aware forecasting, indicating a decoupling between semantic understanding and numerical prediction; and iii) despite the complementary nature of textual and visual represenations of time series as inputs, current multimodal models fail to effectively fuse them for reciprocal performance gains. TSRBench provides a standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance generalist models. Our code and dataset are available at https://tsrbench.github.io/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [58] [MathMixup: Boosting LLM Mathematical Reasoning with Difficulty-Controllable Data Synthesis and Curriculum Learning](https://arxiv.org/abs/2601.17006)
*Xuchen Li,Jing Chen,Xuzhao Li,Hao Liang,Xiaohuan Zhou,Taifeng Wang,Wentao Zhang*

Main category: cs.LG

TL;DR: 提出MathMixup数据合成范式，通过混合分解策略生成难度可控的数学推理问题，配合课程学习策略使Qwen2.5-7B在七个数学基准测试中平均得分达52.6%，超越现有最优方法


<details>
  <summary>Details</summary>
Motivation: 现有数据合成方法存在多样性有限和难度控制不精确的问题，难以支持课程学习等高效训练范式，制约了LLM数学推理能力的提升

Method: 提出MathMixup合成范式，采用混合与分解策略系统生成高质量、难度可控的数学问题，结合自动化自检和人工筛选确保数据质量，构建MathMixupQA数据集并设计课程学习策略

Result: 实验表明该方法显著提升LLM数学推理性能，微调后的Qwen2.5-7B在七个数学基准测试中平均得分52.6%，超越先前最优方法

Conclusion: MathMixup范式有效提升了LLM数学推理能力，验证了其在推进数据驱动课程学习方面的有效性和广泛适用性

Abstract: In mathematical reasoning tasks, the advancement of Large Language Models (LLMs) relies heavily on high-quality training data with clearly defined and well-graded difficulty levels. However, existing data synthesis methods often suffer from limited diversity and lack precise control over problem difficulty, making them insufficient for supporting efficient training paradigms such as curriculum learning. To address these challenges, we propose MathMixup, a novel data synthesis paradigm that systematically generates high-quality, difficulty-controllable mathematical reasoning problems through hybrid and decomposed strategies. Automated self-checking and manual screening are incorporated to ensure semantic clarity and a well-structured difficulty gradient in the synthesized data. Building on this, we construct the MathMixupQA dataset and design a curriculum learning strategy that leverages these graded problems, supporting flexible integration with other datasets. Experimental results show that MathMixup and its curriculum learning strategy significantly enhance the mathematical reasoning performance of LLMs. Fine-tuned Qwen2.5-7B achieves an average score of 52.6\% across seven mathematical benchmarks, surpassing previous state-of-the-art methods. These results fully validate the effectiveness and broad applicability of MathMixup in improving the mathematical reasoning abilities of LLMs and advancing data-centric curriculum learning.

</details>


### [59] [Bayesian Robust Financial Trading with Adversarial Synthetic Market Data](https://arxiv.org/abs/2601.17008)
*Haochong Xia,Simin Li,Ruixiao Xu,Zhixia Zhang,Hongxiang Wang,Zhiqian Liu,Teng Yao Long,Molei Qin,Chuqiao Zong,Bo An*

Main category: cs.LG

TL;DR: A Bayesian Robust Framework for algorithmic trading that combines a macro-conditioned GAN for realistic data generation with robust policy learning via a two-player Bayesian Markov game, significantly outperforming baselines especially during market crises like COVID.


<details>
  <summary>Details</summary>
Motivation: Algorithmic trading models suffer performance degradation in evolving real-world markets due to shifting regimes from macroeconomic changes, caused by insufficient model robustness and lack of diverse training environments leading to overfitting.

Method: Proposes a two-component framework: (1) Macro-conditioned GAN generator that uses macroeconomic indicators to synthesize realistic, diverse financial data with proper temporal, cross-instrument, and macro correlations; (2) Two-player zero-sum Bayesian Markov game where an adversarial agent simulates regime shifts by perturbing macro indicators while the trading agent maintains beliefs over hidden market states using a quantile belief network, seeking Robust Perfect Bayesian Equilibrium via Bayesian neural fictitious self-play.

Result: Extensive experiments on 9 financial instruments show the framework outperforms 9 state-of-the-art baselines, with particularly improved profitability and risk management during extreme events like the COVID market crisis.

Conclusion: The framework provides a reliable solution for robust algorithmic trading under uncertain and shifting market dynamics by systematically addressing both data diversity and policy robustness challenges through Bayesian game-theoretic learning.

Abstract: Algorithmic trading relies on machine learning models to make trading decisions. Despite strong in-sample performance, these models often degrade when confronted with evolving real-world market regimes, which can shift dramatically due to macroeconomic changes-e.g., monetary policy updates or unanticipated fluctuations in participant behavior. We identify two challenges that perpetuate this mismatch: (1) insufficient robustness in existing policy against uncertainties in high-level market fluctuations, and (2) the absence of a realistic and diverse simulation environment for training, leading to policy overfitting. To address these issues, we propose a Bayesian Robust Framework that systematically integrates a macro-conditioned generative model with robust policy learning. On the data side, to generate realistic and diverse data, we propose a macro-conditioned GAN-based generator that leverages macroeconomic indicators as primary control variables, synthesizing data with faithful temporal, cross-instrument, and macro correlations. On the policy side, to learn robust policy against market fluctuations, we cast the trading process as a two-player zero-sum Bayesian Markov game, wherein an adversarial agent simulates shifting regimes by perturbing macroeconomic indicators in the macro-conditioned generator, while the trading agent-guided by a quantile belief network-maintains and updates its belief over hidden market states. The trading agent seeks a Robust Perfect Bayesian Equilibrium via Bayesian neural fictitious self-play, stabilizing learning under adversarial market perturbations. Extensive experiments on 9 financial instruments demonstrate that our framework outperforms 9 state-of-the-art baselines. In extreme events like the COVID, our method shows improved profitability and risk management, offering a reliable solution for trading under uncertain and shifting market dynamics.

</details>


### [60] [FlashMoE: Reducing SSD I/O Bottlenecks via ML-Based Cache Replacement for Mixture-of-Experts Inference on Edge Devices](https://arxiv.org/abs/2601.17063)
*Byeongju Kim,Jungwan Lee,Donghyeon Han,Hoi-Jun Yoo,Sangyeob Kim*

Main category: cs.LG

TL;DR: FlashMoE is a system that enables efficient Mixture-of-Experts (MoE) inference on memory-constrained devices by offloading inactive experts to SSD instead of RAM, using a lightweight ML-based caching strategy that combines recency and frequency signals to maximize expert reuse.


<details>
  <summary>Details</summary>
Motivation: Existing MoE inference systems rely on DRAM-based offloading which becomes impractical as MoE models scale to hundreds of gigabytes, making efficient on-device inference challenging for memory-constrained edge devices.

Method: Proposes FlashMoE, a system that offloads inactive experts to SSD with a lightweight ML-based caching strategy that adaptively combines recency and frequency signals to maximize expert reuse and reduce storage I/O.

Result: On real hardware, FlashMoE improves cache hit rate by up to 51% over LRU/LFU policies and achieves up to 2.6x speedup compared to existing MoE inference systems.

Conclusion: FlashMoE successfully enables practical on-device MoE inference under limited RAM by leveraging SSD offloading with intelligent caching, making large MoE models feasible for edge deployment.

Abstract: Recently, Mixture-of-Experts (MoE) models have gained attention for efficiently scaling large language models. Although these models are extremely large, their sparse activation enables inference to be performed by accessing only a fraction of the model at a time. This property opens the possibility of on-device inference of MoE, which was previously considered infeasible for such large models. Consequently, various systems have been proposed to leverage this sparsity and enable efficient MoE inference for edge devices. However, previous MoE inference systems like Fiddler[8] or DAOP[13] rely on DRAM-based offloading and are not suitable for memory constrained on-device environments. As recent MoE models grow to hundreds of gigabytes, RAM-offloading solutions become impractical. To address this, we propose FlashMoE, a system that offloads inactive experts to SSD, enabling efficient MoE inference under limited RAM. FlashMoE incorporates a lightweight ML-based caching strategy that adaptively combines recency and frequency signals to maximize expert reuse, significantly reducing storage I/O. In addition, we built a user-grade desktop platform to demonstrate the practicality of FlashMoE. On this real hardware setup, FlashMoE improves cache hit rate by up to 51% over well-known offloading policies such as LRU and LFU, and achieves up to 2.6x speedup compared to existing MoE inference systems.

</details>


### [61] [ThinkTank-ME: A Multi-Expert Framework for Middle East Event Forecasting](https://arxiv.org/abs/2601.17065)
*Haoxuan Li,He Chang,Yunshan Ma,Yi Bin,Yang Yang,See-Kiong Ng,Tat-Seng Chua*

Main category: cs.LG

TL;DR: 提出ThinkTank-ME多专家协作框架用于中东事件预测，通过模拟真实智库分析模式，克服单模型架构在复杂地缘政治语境下的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-based事件预测方法采用单模型架构，生成单一明确轨迹，无法充分捕捉复杂地区语境下的多元地缘政治细微差别。

Method: 提出ThinkTank-ME框架，模拟真实世界战略决策中的协作专家分析；构建中东聚焦的事件预测基准POLECAT-FOR-ME，促进专家专业化与严格评估。

Result: 实验结果表明，在多专家协作模式下处理复杂时序地缘政治预测任务具有优越性。

Conclusion: 多专家协作框架比单模型架构更适合复杂地缘政治事件预测，为事件 forecasting 提供了新的方法论。

Abstract: Event forecasting is inherently influenced by multifaceted considerations, including international relations, regional historical dynamics, and cultural contexts. However, existing LLM-based approaches employ single-model architectures that generate predictions along a singular explicit trajectory, constraining their ability to capture diverse geopolitical nuances across complex regional contexts. To address this limitation, we introduce ThinkTank-ME, a novel Think Tank framework for Middle East event forecasting that emulates collaborative expert analysis in real-world strategic decision-making. To facilitate expert specialization and rigorous evaluation, we construct POLECAT-FOR-ME, a Middle East-focused event forecasting benchmark. Experimental results demonstrate the superiority of multi-expert collaboration in handling complex temporal geopolitical forecasting tasks. The code is available at https://github.com/LuminosityX/ThinkTank-ME.

</details>


### [62] [Multi-Agent Deep Reinforcement Learning Under Constrained Communications](https://arxiv.org/abs/2601.17069)
*Shahil Shaik,Jonathon M. Smereka,Yue Wang*

Main category: cs.LG

TL;DR: A distributed MARL framework (DG-MAPPO) using graph attention networks (D-GAT) that outperforms CTDE methods by enabling agents to learn and act solely through local observations and peer-to-peer communication without centralized critics or global state information.


<details>
  <summary>Details</summary>
Motivation: CTDE paradigm has scalability, robustness, and generalization bottlenecks due to reliance on global state during training, and is brittle when teammates change or environments differ from training. Distributed approaches allow adaptation using only local information and peer-to-peer communication.

Method: Developed a Distributed Graph Attention Network (D-GAT) for global state inference through multi-hop communication with input-dependent attention weights. Created DG-MAPPO where agents optimize local policies/value functions using local observations, multi-hop communication, and shared/averaged rewards, eliminating centralized critics.

Result: Empirical evaluation on StarCraftII Multi-Agent Challenge, Google Research Football, and Multi-Agent Mujoco shows consistent outperformance over strong CTDE baselines, achieving superior coordination across cooperative tasks with both homogeneous and heterogeneous teams.

Conclusion: The distributed MARL framework provides a principled and scalable solution for robust collaboration, eliminating the need for centralized training or global observability. DG-MAPPO is the first to fully eliminate reliance on privileged centralized information, enabling agents to learn and act solely through peer-to-peer communication.

Abstract: Centralized training with decentralized execution (CTDE) has been the dominant paradigm in multi-agent reinforcement learning (MARL), but its reliance on global state information during training introduces scalability, robustness, and generalization bottlenecks. Moreover, in practical scenarios such as adding/dropping teammates or facing environment dynamics that differ from the training, CTDE methods can be brittle and costly to retrain, whereas distributed approaches allow agents to adapt using only local information and peer-to-peer communication. We present a distributed MARL framework that removes the need for centralized critics or global information. Firstly, we develop a novel Distributed Graph Attention Network (D-GAT) that performs global state inference through multi-hop communication, where agents integrate neighbor features via input-dependent attention weights in a fully distributed manner. Leveraging D-GAT, we develop the distributed graph-attention MAPPO (DG-MAPPO) -- a distributed MARL framework where agents optimize local policies and value functions using local observations, multi-hop communication, and shared/averaged rewards. Empirical evaluation on the StarCraftII Multi-Agent Challenge, Google Research Football, and Multi-Agent Mujoco demonstrates that our method consistently outperforms strong CTDE baselines, achieving superior coordination across a wide range of cooperative tasks with both homogeneous and heterogeneous teams. Our distributed MARL framework provides a principled and scalable solution for robust collaboration, eliminating the need for centralized training or global observability. To the best of our knowledge, DG-MAPPO appears to be the first to fully eliminate reliance on privileged centralized information, enabling agents to learn and act solely through peer-to-peer communication.

</details>


### [63] [Attention-Based Variational Framework for Joint and Individual Components Learning with Applications in Brain Network Analysis](https://arxiv.org/abs/2601.17073)
*Yifei Zhang,Meimei Liu,Zhengwu Zhang*

Main category: cs.LG

TL;DR: 本研究提出CM-JIVNet概率框架，通过多头注意力机制解决结构连接(SC)与功能连接(FC)多模态数据融合难题，在HCP-YA数据集上实现优异的跨模态重建与行为预测性能。


<details>
  <summary>Details</summary>
Motivation: 整合SC与FC数据对理解脑组织至关重要，但面临高维非线性、复杂耦合关系以及难以分离共享与特异性信息三大核心挑战。

Method: 开发Cross-Modal Joint-Individual Variational Network (CM-JIVNet)，采用多头部注意力融合模块捕捉非线性跨模态依赖，同时分离模态独立信号，学习因子化潜在表征。

Result: 基于HCP-YA数据验证，CM-JIVNet在跨模态重建精度和行为特征预测性能上显著优于现有方法，成功解耦联合与个体特征空间。

Conclusion: CM-JIVNet为大规模多模态脑连接组分析提供了稳健、可解释且可扩展的统一解决方案。

Abstract: Brain organization is increasingly characterized through multiple imaging modalities, most notably structural connectivity (SC) and functional connectivity (FC). Integrating these inherently distinct yet complementary data sources is essential for uncovering the cross-modal patterns that drive behavioral phenotypes. However, effective integration is hindered by the high dimensionality and non-linearity of connectome data, complex non-linear SC-FC coupling, and the challenge of disentangling shared information from modality-specific variations. To address these issues, we propose the Cross-Modal Joint-Individual Variational Network (CM-JIVNet), a unified probabilistic framework designed to learn factorized latent representations from paired SC-FC datasets. Our model utilizes a multi-head attention fusion module to capture non-linear cross-modal dependencies while isolating independent, modality-specific signals. Validated on Human Connectome Project Young Adult (HCP-YA) data, CM-JIVNet demonstrates superior performance in cross-modal reconstruction and behavioral trait prediction. By effectively disentangling joint and individual feature spaces, CM-JIVNet provides a robust, interpretable, and scalable solution for large-scale multimodal brain analysis.

</details>


### [64] [E2PL: Effective and Efficient Prompt Learning for Incomplete Multi-view Multi-Label Class Incremental Learning](https://arxiv.org/abs/2601.17076)
*Jiajun Chen,Yue Wu,Kai Huang,Wen Xi,Yangyang Wu,Xiaoye Miao,Mengying Zhu,Meng Xi,Guanjie Cheng*

Main category: cs.LG

TL;DR: 针对现代Web应用中多视图多标签分类面临的视图缺失和类别动态增长问题，本文提出E2PL框架，通过任务定制提示和缺失感知提示，结合原型张量化模块将参数量从指数级降至线性级，并采用动态对比学习提升模型鲁棒性，在三个基准测试中均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界的Web级应用场景普遍存在视图缺失和类别持续新增的双重挑战，现有方法要么缺乏对新类别的适应性，要么在处理所有可能的缺失视图模式时参数量呈指数级增长，严重限制了可扩展性。

Method: 提出E2PL框架：1）任务定制提示实现类别增量适应；2）缺失感知提示灵活集成任意视图缺失场景；3）原型张量化模块通过原子张量分解将提示参数复杂度从指数级降至线性级；4）动态对比学习显式建模不同缺失视图模式间的复杂依赖关系。

Result: 在三个基准数据集上的大量实验表明，E2PL在有效性和效率方面均持续优于现有最先进方法。

Conclusion: E2PL框架为不完全多视图多标签类别增量学习任务提供了有效且高效的解决方案，能够很好地适应Web环境中视图缺失和类别动态扩展的现实挑战。

Abstract: Multi-view multi-label classification (MvMLC) is indispensable for modern web applications aggregating information from diverse sources. However, real-world web-scale settings are rife with missing views and continuously emerging classes, which pose significant obstacles to robust learning. Prevailing methods are ill-equipped for this reality, as they either lack adaptability to new classes or incur exponential parameter growth when handling all possible missing-view patterns, severely limiting their scalability in web environments. To systematically address this gap, we formally introduce a novel task, termed \emph{incomplete multi-view multi-label class incremental learning} (IMvMLCIL), which requires models to simultaneously address heterogeneous missing views and dynamic class expansion. To tackle this task, we propose \textsf{E2PL}, an Effective and Efficient Prompt Learning framework for IMvMLCIL. \textsf{E2PL} unifies two novel prompt designs: \emph{task-tailored prompts} for class-incremental adaptation and \emph{missing-aware prompts} for the flexible integration of arbitrary view-missing scenarios. To fundamentally address the exponential parameter explosion inherent in missing-aware prompts, we devise an \emph{efficient prototype tensorization} module, which leverages atomic tensor decomposition to elegantly reduce the prompt parameter complexity from exponential to linear w.r.t. the number of views. We further incorporate a \emph{dynamic contrastive learning} strategy explicitly model the complex dependencies among diverse missing-view patterns, thus enhancing the model's robustness. Extensive experiments on three benchmarks demonstrate that \textsf{E2PL} consistently outperforms state-of-the-art methods in both effectiveness and efficiency. The codes and datasets are available at https://anonymous.4open.science/r/code-for-E2PL.

</details>


### [65] [SFO: Learning PDE Operators via Spectral Filtering](https://arxiv.org/abs/2601.17090)
*Noam Koren,Rafael Moschopoulos,Kira Radinsky,Elad Hazan*

Main category: cs.LG

TL;DR: Spectral Filtering Operator (SFO) uses a universal spectral basis to efficiently model PDEs by learning only a few rapidly decaying spectral coefficients, achieving 40% error reduction with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Neural operators struggle to efficiently capture long-range, nonlocal interactions inherent in PDE solution maps.

Method: Parameterizes integral kernels using Universal Spectral Basis (USB) derived from Hilbert matrix eigenmodes, leveraging the Linear Dynamical System structure of discrete Green's functions to learn only spectral coefficients of rapidly decaying eigenvalues.

Result: Achieved state-of-the-art accuracy across six PDE benchmarks (reaction-diffusion, fluid dynamics, 3D electromagnetics), reducing error by up to 40% while using substantially fewer parameters than baselines.

Conclusion: SFO provides a highly efficient representation for neural operators by exploiting the compact approximability of PDE kernels in the universal spectral basis.

Abstract: Partial differential equations (PDEs) govern complex systems, yet neural operators often struggle to efficiently capture the long-range, nonlocal interactions inherent in their solution maps. We introduce Spectral Filtering Operator (SFO), a neural operator that parameterizes integral kernels using the Universal Spectral Basis (USB), a fixed, global orthonormal basis derived from the eigenmodes of the Hilbert matrix in spectral filtering theory. Motivated by our theoretical finding that the discrete Green's functions of shift-invariant PDE discretizations exhibit spatial Linear Dynamical System (LDS) structure, we prove that these kernels admit compact approximations in the USB. By learning only the spectral coefficients of rapidly decaying eigenvalues, SFO achieves a highly efficient representation. Across six benchmarks, including reaction-diffusion, fluid dynamics, and 3D electromagnetics, SFO achieves state-of-the-art accuracy, reducing error by up to 40% relative to strong baselines while using substantially fewer parameters.

</details>


### [66] [CUROCKET: Optimizing ROCKET for GPU](https://arxiv.org/abs/2601.17091)
*Ole Stüven,Keno Moenck,Thorsten Schüppstuhl*

Main category: cs.LG

TL;DR: This paper presents CUROCKET, a GPU-accelerated version of the ROCKET algorithm for time series classification that handles inhomogeneous kernels efficiently, achieving up to 11x better energy efficiency than CPU-based ROCKET.


<details>
  <summary>Details</summary>
Motivation: The original ROCKET algorithm, while accurate for time series classification, runs primarily on CPU despite convolution being highly parallelizable. Standard GPU convolution methods are inefficient for ROCKET's inhomogeneous kernels, creating a performance bottleneck that limits computational efficiency.

Method: The authors propose a specialized GPU algorithm (CUROCKET) designed to efficiently handle the inhomogeneous kernels used in ROCKET, enabling parallelized convolution operations on GPU that would be inefficient with standard methods.

Result: CUROCKET achieves up to 11 times higher computational efficiency per watt compared to the CPU implementation of ROCKET, demonstrating significant performance and energy advantages.

Conclusion: The authors provide open-source code for CUROCKET at https://github.com/oleeven/CUROCKET, making the GPU-accelerated implementation publicly available for the research community.

Abstract: ROCKET (RandOm Convolutional KErnel Transform) is a feature extraction algorithm created for Time Series Classification (TSC), published in 2019. It applies convolution with randomly generated kernels on a time series, producing features that can be used to train a linear classifier or regressor like Ridge. At the time of publication, ROCKET was on par with the best state-of-the-art algorithms for TSC in terms of accuracy while being significantly less computationally expensive, making ROCKET a compelling algorithm for TSC. This also led to several subsequent versions, further improving accuracy and computational efficiency. The currently available ROCKET implementations are mostly bound to execution on CPU. However, convolution is a task that can be highly parallelized and is therefore suited to be executed on GPU, which speeds up the computation significantly. A key difficulty arises from the inhomogeneous kernels ROCKET uses, making standard methods for applying convolution on GPU inefficient. In this work, we propose an algorithm that is able to efficiently perform ROCKET on GPU and achieves up to 11 times higher computational efficiency per watt than ROCKET on CPU. The code for CUROCKET is available in this repository https://github.com/oleeven/CUROCKET on github.

</details>


### [67] [The Triangle of Similarity: A Multi-Faceted Framework for Comparing Neural Network Representations](https://arxiv.org/abs/2601.17093)
*Olha Sirikova,Alvin Chan*

Main category: cs.LG

TL;DR: This paper proposes a "Triangle of Similarity" framework that combines three complementary perspectives—static, functional, and sparsity similarity—to comprehensively compare neural network representations, revealing that architectural family primarily determines similarity and pruning can expose shared computational cores.


<details>
  <summary>Details</summary>
Motivation: Existing methods for comparing neural network representations offer limited views, hindering thorough understanding and validation of models in scientific applications.

Method: The Triangle of Similarity framework integrates static representational similarity (CKA/Procrustes), functional similarity (Linear Mode Connectivity), and sparsity similarity (pruning robustness). The authors analyzed CNNs, Vision Transformers, and Vision-Language Models using both in-distribution (ImageNetV2) and out-of-distribution (CIFAR-10) testbeds.

Result: Key findings include: (1) Architectural family is the primary determinant of representational similarity, forming distinct clusters; (2) CKA self-similarity and task accuracy are strongly correlated during pruning, though accuracy often degrades more sharply; (3) Pruning can regularize representations, exposing a shared computational core for some model pairs.

Conclusion: This holistic framework provides a more comprehensive approach for assessing whether models converge on similar internal mechanisms, serving as a valuable tool for model selection and scientific analysis.

Abstract: Comparing neural network representations is essential for understanding and validating models in scientific applications. Existing methods, however, often provide a limited view. We propose the Triangle of Similarity, a framework that combines three complementary perspectives: static representational similarity (CKA/Procrustes), functional similarity (Linear Mode Connectivity or Predictive Similarity), and sparsity similarity (robustness under pruning). Analyzing a range of CNNs, Vision Transformers, and Vision-Language Models using both in-distribution (ImageNetV2) and out-of-distribution (CIFAR-10) testbeds, our initial findings suggest that: (1) architectural family is a primary determinant of representational similarity, forming distinct clusters; (2) CKA self-similarity and task accuracy are strongly correlated during pruning, though accuracy often degrades more sharply; and (3) for some model pairs, pruning appears to regularize representations, exposing a shared computational core. This framework offers a more holistic approach for assessing whether models have converged on similar internal mechanisms, providing a useful tool for model selection and analysis in scientific research.

</details>


### [68] [Boltzmann-GPT: Bridging Energy-Based World Models and Language Generation](https://arxiv.org/abs/2601.17094)
*Junichiro Niimi*

Main category: cs.LG

TL;DR: 该论文提出"嘴不是大脑"的架构原则，通过深度玻尔兹曼机（DBM）作为世界模型、适配器投影信念状态、冻结GPT-2提供语言能力的分离式设计，在亚马逊手机评论域验证了世界模型与语言模型分离的有效性，实现了更可控、一致的文本生成。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）虽能生成流畅文本，但其是否真正理解世界仍存争议。现有方法未能明确分离世界理解与语言生成能力。

Method: 提出三组件架构：1）深度玻尔兹曼机（DBM）作为基于能量的世界模型捕获领域结构；2）适配器将潜在信念状态投影到嵌入空间；3）冻结的GPT-2仅提供语言能力而不包含领域知识。在亚马逊智能手机评论域进行实例化验证。

Result: 1）通过世界模型调节的生成相比纯提示生成，情感相关性显著更高、困惑度更低、语义相似度更高；2）DBM能量函数能有效区分合理与不合理的市场配置，为不可能的品牌价格组合分配更高能量；3）对特定属性的干预能因果性地传播到生成文本，且干预后的输出分布与自然样本一致。

Conclusion: 研究表明，即使小规模语言模型在连接适当世界模型后也能实现一致且可控的生成，为将语言能力与世界理解分离提供了实证支持。

Abstract: Large Language Models (LLMs) generate fluent text, yet whether they truly understand the world or merely produce plausible language about it remains contested. We propose an architectural principle, the mouth is not the brain, that explicitly separates world models from language models. Our architecture comprises three components: a Deep Boltzmann Machine (DBM) that captures domain structure as an energy-based world model, an adapter that projects latent belief states into embedding space, and a frozen GPT-2 that provides linguistic competence without domain knowledge. We instantiate this framework in the consumer review domain using Amazon smartphone reviews. Experiments demonstrate that (1) conditioning through the world model yields significantly higher sentiment correlation, lower perplexity, and greater semantic similarity compared to prompt-based generation alone; (2) the DBM's energy function distinguishes coherent from incoherent market configurations, assigning higher energy to implausible brand-price combinations; and (3) interventions on specific attributes propagate causally to generated text with intervened outputs exhibiting distributions statistically consistent with naturally occurring samples sharing the target configuration. These findings suggest that even small-scale language models can achieve consistent, controllable generation when connected to an appropriate world model, providing empirical support for separating linguistic competence from world understanding.

</details>


### [69] [Least-Loaded Expert Parallelism: Load Balancing An Imbalanced Mixture-of-Experts](https://arxiv.org/abs/2601.17111)
*Xuan-Phi Nguyen,Shrey Pandit,Austin Xu,Caiming Xiong,Shafiq Joty*

Main category: cs.LG

TL;DR: 提出LLEP算法，通过动态重路由过度负载设备的token和专家参数到空闲设备，解决MoE模型推理时路由不均衡导致的计算内存瓶颈，实现最高5倍加速和4倍内存降低


<details>
  <summary>Details</summary>
Motivation: MoE模型即使预训练时加入负载均衡约束，仍会表现出显著不均衡的路由行为，这是自然且有益的（可集中领域知识）。但专家并行（EP）假设均衡路由，在极端不均衡时会导致少数设备过载，引发计算和内存瓶颈，且推理阶段无法使用显式负载均衡

Method: Least-Loaded Expert Parallelism (LLEP)：一种新型EP算法，动态监测设备负载，将过载设备的超额token及相关专家参数实时重路由到利用率低的设备上，确保所有设备在最小集体延迟内完成工作且满足内存约束

Result: 在不同模型规模上，相比标准EP实现最高5倍加速和4倍峰值内存使用降低；在gpt-oss-120b上达到约1.9倍加速，支持更快更高吞吐的后训练和推理

Conclusion: LLEP为不均衡路由下的MoE模型扩展提供了有效解决方案，通过广泛的理论分析和实证评估（含消融研究），揭示了关键权衡，建立了硬件特定超参数调整的原则性框架以实现最优性能

Abstract: Mixture-of-Experts (MoE) models are typically pre-trained with explicit load-balancing constraints to ensure statistically balanced expert routing. Despite this, we observe that even well-trained MoE models exhibit significantly imbalanced routing. This behavior is arguably natural-and even desirable - as imbalanced routing allows models to concentrate domain-specific knowledge within a subset of experts. Expert parallelism (EP) is designed to scale MoE models by distributing experts across multiple devices, but with a less-discussed assumption of balanced routing. Under extreme imbalance, EP can funnel a disproportionate number of tokens to a small number of experts, leading to compute- and memory-bound failures on overloaded devices during post-training or inference, where explicit load balancing is often inapplicable. We propose Least-Loaded Expert Parallelism (LLEP), a novel EP algorithm that dynamically reroutes excess tokens and associated expert parameters from overloaded devices to underutilized ones. This ensures that all devices complete their workloads within the minimum collective latency while respecting memory constraints. Across different model scales, LLEP achieves up to 5x speedup and 4x reduction in peak memory usage compared to standard EP. This enables faster and higher-throughput post-training and inference, with ~1.9x faster for gpt-oss-120b. We support our method with extensive theoretical analysis and comprehensive empirical evaluations, including ablation studies. These results illuminate key trade-offs and enable a principled framework for hardware-specific hyper-parameter tuning to achieve optimal performance.

</details>


### [70] [ConceptACT: Episode-Level Concepts for Sample-Efficient Robotic Imitation Learning](https://arxiv.org/abs/2601.17135)
*Jakob Karalus,Friedhelm Schwenker*

Main category: cs.LG

TL;DR: ConceptACT extends Action Chunking with Transformers by incorporating semantic concept annotations during training only, using concept-aware cross-attention to improve robot imitation learning efficiency without requiring semantic input at deployment.


<details>
  <summary>Details</summary>
Motivation: Current imitation learning methods rely solely on low-level sensorimotor data while ignoring rich human semantic knowledge about tasks, leading to inefficient learning.

Method: ConceptACT uses human-provided semantic concepts (object properties, spatial relationships, task constraints) exclusively during demonstration collection, integrating them via modified transformer architecture with concept-aware cross-attention in the final encoder layer, supervised to align with human annotations.

Result: Experiments on two robotic manipulation tasks show ConceptACT converges faster, achieves superior sample efficiency compared to standard ACT, and architectural integration through attention significantly outperforms naive auxiliary losses or language-conditioned models.

Conclusion: Properly integrated semantic supervision provides powerful inductive biases for more efficient robot learning, enabling better performance without additional deployment-time annotation burden.

Abstract: Imitation learning enables robots to acquire complex manipulation skills from human demonstrations, but current methods rely solely on low-level sensorimotor data while ignoring the rich semantic knowledge humans naturally possess about tasks. We present ConceptACT, an extension of Action Chunking with Transformers that leverages episode-level semantic concept annotations during training to improve learning efficiency. Unlike language-conditioned approaches that require semantic input at deployment, ConceptACT uses human-provided concepts (object properties, spatial relationships, task constraints) exclusively during demonstration collection, adding minimal annotation burden. We integrate concepts using a modified transformer architecture in which the final encoder layer implements concept-aware cross-attention, supervised to align with human annotations. Through experiments on two robotic manipulation tasks with logical constraints, we demonstrate that ConceptACT converges faster and achieves superior sample efficiency compared to standard ACT. Crucially, we show that architectural integration through attention mechanisms significantly outperforms naive auxiliary prediction losses or language-conditioned models. These results demonstrate that properly integrated semantic supervision provides powerful inductive biases for more efficient robot learning.

</details>


### [71] [Conservative & Aggressive NaNs Accelerate U-Nets for Neuroimaging](https://arxiv.org/abs/2601.17180)
*Inés Gonzalez-Pepe,Vinuyan Sivakolunthu,Jacob Fortin,Yohan Chatelain,Tristan Glatard*

Main category: cs.LG

TL;DR: This paper proposes Conservative & Aggressive NaNs, novel max pooling/unpooling variants that replace numerically unstable voxels with NaNs to skip redundant computations in CNNs, achieving 1.67x inference speedup in neuroimaging with no performance loss.


<details>
  <summary>Details</summary>
Motivation: Large CNN architectures in neuroimaging suffer from computational inefficiency despite hardware advances, with up to two-thirds of convolution operations being redundant due to numerical noise dominating input values.

Method: Introduces two NaN-based voxel filtering methods: Conservative NaNs (safe computation skipping) and Aggressive NaNs (maximal skipping), implemented as PyTorch-compatible pooling/unpooling layers that identify and replace numerically unstable values without architectural changes.

Result: For neuroimaging data with >66% NaNs: 1.67x average inference speedup; Conservative NaNs skips 30% convolutions (up to 64.64% in specific layers) with zero accuracy loss, while Aggressive NaNs skips up to 69.30% but may occasionally reduce performance.

Conclusion: Numerical uncertainty in CNNs can be systematically exploited to eliminate redundant computations, significantly improving inference efficiency in neuroimaging and general image tasks without model retraining or structural modifications.

Abstract: Deep learning models for neuroimaging increasingly rely on large architectures, making efficiency a persistent concern despite advances in hardware. Through an analysis of numerical uncertainty of convolutional neural networks (CNNs), we observe that many operations are applied to values dominated by numerical noise and have negligible influence on model outputs. In some models, up to two-thirds of convolution operations appear redundant. We introduce Conservative & Aggressive NaNs, two novel variants of max pooling and unpooling that identify numerically unstable voxels and replace them with NaNs, allowing subsequent layers to skip computations on irrelevant data. Both methods are implemented within PyTorch and require no architectural changes. We evaluate these approaches on four CNN models spanning neuroimaging and image classification tasks. For inputs containing at least 50% NaNs, we observe consistent runtime improvements; for data with more than two-thirds NaNs )common in several neuroimaging settings) we achieve an average inference speedup of 1.67x. Conservative NaNs reduces convolution operations by an average of 30% across models and datasets, with no measurable performance degradation, and can skip up to 64.64% of convolutions in specific layers. Aggressive NaNs can skip up to 69.30% of convolutions but may occasionally affect performance. Overall, these methods demonstrate that numerical uncertainty can be exploited to reduce redundant computation and improve inference efficiency in CNNs.

</details>


### [72] [Federated Proximal Optimization for Privacy-Preserving Heart Disease Prediction: A Controlled Simulation Study on Non-IID Clinical Data](https://arxiv.org/abs/2601.17183)
*Farzam Asad,Junaid Saif Khan,Maria Tariq,Sundus Munir,Muhammad Adnan Khan*

Main category: cs.LG

TL;DR: 本研究通过模拟四家异构医院，基于UCI心脏病数据集评估联邦近端优化算法(FedProx)在心脏病预测中的表现，证明该算法在保护隐私的同时，能有效解决数据非独立同分布问题，准确率达85%，优于集中学习和本地模型。


<details>
  <summary>Details</summary>
Motivation: 医疗数据因HIPAA和GDPR等隐私法规无法直接共享，限制了诊断模型的发展。联邦学习虽能实现隐私保护的协作训练，但临床数据的非独立同分布特性(由人口统计差异和机构实践多样性导致)导致客户端漂移问题，影响模型性能。

Method: 基于UCI心脏病数据集(Cleveland Clinic, 303名患者)，通过人口统计分层模拟四家异构医院客户端，创建真实非IID数据划分。采用FedProx算法，设置近端参数mu=0.05，进行50次独立运行以验证统计显著性，对比集中学习、本地学习和不同FedProx配置的效果。

Result: FedProx(mu=0.05)达到85.00%准确率，显著优于集中学习(83.33%)和本地模型平均准确率(78.45%)。消融实验证实近端正则化能有效抑制异构环境中的客户端漂移，50次独立运行结果具有统计显著性。

Conclusion: 本研究为真实世界联邦医疗系统提供了算法洞见和实用部署指南，证明FedProx能有效实现隐私保护的协作学习，结果可直接应用于医院IT管理员的实践，推动医疗AI在合规框架下的发展。

Abstract: Healthcare institutions have access to valuable patient data that could be of great help in the development of improved diagnostic models, but privacy regulations like HIPAA and GDPR prevent hospitals from directly sharing data with one another. Federated Learning offers a way out to this problem by facilitating collaborative model training without having the raw patient data centralized. However, clinical datasets intrinsically have non-IID (non-independent and identically distributed) features brought about by demographic disparity and diversity in disease prevalence and institutional practices. This paper presents a comprehensive simulation research of Federated Proximal Optimization (FedProx) for Heart Disease prediction based on UCI Heart Disease dataset. We generate realistic non-IID data partitions by simulating four heterogeneous hospital clients from the Cleveland Clinic dataset (303 patients), by inducing statistical heterogeneity by demographic-based stratification. Our experimental results show that FedProx with proximal parameter mu=0.05 achieves 85.00% accuracy, which is better than both centralized learning (83.33%) and isolated local models (78.45% average) without revealing patient privacy. Through generous sheer ablation studies with statistical validation on 50 independent runs we demonstrate that proximal regularization is effective in curbing client drift in heterogeneous environments. This proof-of-concept research offers algorithmic insights and practical deployment guidelines for real-world federated healthcare systems, and thus, our results are directly transferable to hospital IT-administrators, implementing privacy-preserving collaborative learning.

</details>


### [73] [Rethinking Benchmarks for Differentially Private Image Classification](https://arxiv.org/abs/2601.17189)
*Sabrina Mokhtari,Sara Kodeiri,Shubhankar Mohapatra,Florian Tramer,Gautam Kamath*

Main category: cs.LG

TL;DR: 提出一套全面的差分隐私图像分类基准测试体系，涵盖多场景设置，测试现有技术有效性，并建立公开排行榜以追踪领域进展。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私机器学习基准测试不够全面，缺乏在不同场景下（如有无额外数据、凸设置、不同数据集）对技术进行系统性评估的框架。

Method: 构建涵盖多种场景的综合基准测试集，包括有无额外数据、凸优化设置和不同质性数据集；并在其上测试现有主流技术以检验其跨场景有效性。

Result: 建立了公开可用的社区排行榜，供研究人员追踪差分隐私机器学习领域的最新进展。

Conclusion: 通过全面基准测试揭示了不同技术在不同场景下的有效性差异，为社区提供了标准化的评估工具和进度追踪平台，推动差分隐私机器学习研究更加系统和可比。

Abstract: We revisit benchmarks for differentially private image classification. We suggest a comprehensive set of benchmarks, allowing researchers to evaluate techniques for differentially private machine learning in a variety of settings, including with and without additional data, in convex settings, and on a variety of qualitatively different datasets. We further test established techniques on these benchmarks in order to see which ideas remain effective in different settings. Finally, we create a publicly available leader board for the community to track progress in differentially private machine learning.

</details>


### [74] [Accelerated Sinkhorn Algorithms for Partial Optimal Transport](https://arxiv.org/abs/2601.17196)
*Nghia Thu Truong,Qui Phu Pham,Quang Nguyen,Dung Luong,Mai Tran*

Main category: cs.LG

TL;DR: 本文针对部分最优传输(POT)问题提出加速Sinkhorn算法(ASPOT)，通过结合交替最小化与Nesterov加速策略，将复杂度优化至$\mathcal{O}(n^{7/3}\varepsilon^{-5/3})$，并改进了熵参数选择，实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 部分最优传输(POT)适用于边缘分布大小不等或含异常值的场景，但现有基于Sinkhorn的方法复杂度界限次优，限制了算法在大规模问题上的可扩展性。

Method: 提出ASPOT算法，在POT框架中整合交替最小化与Nesterov-style加速技术，并优化熵参数γ的选择策略。

Result: 理论成果：1) ASPOT达到$\mathcal{O}(n^{7/3}\varepsilon^{-5/3})$的复杂度界限；2) 揭示了熵参数γ的优化选择可提升经典Sinkhorn方法的收敛速率。实验验证：在真实世界应用中表现出良好的性能。

Conclusion: 该研究为POT问题提供了更高效的求解算法，显著改善了Sinkhorn方法在部分传输场景下的可扩展性，兼具理论价值与实践意义。

Abstract: Partial Optimal Transport (POT) addresses the problem of transporting only a fraction of the total mass between two distributions, making it suitable when marginals have unequal size or contain outliers. While Sinkhorn-based methods are widely used, their complexity bounds for POT remain suboptimal and can limit scalability. We introduce Accelerated Sinkhorn for POT (ASPOT), which integrates alternating minimization with Nesterov-style acceleration in the POT setting, yielding a complexity of $\mathcal{O}(n^{7/3}\varepsilon^{-5/3})$. We also show that an informed choice of the entropic parameter $γ$ improves rates for the classical Sinkhorn method. Experiments on real-world applications validate our theories and demonstrate the favorable performance of our proposed methods.

</details>


### [75] [SpecBridge: Bridging Mass Spectrometry and Molecular Representations via Cross-Modal Alignment](https://arxiv.org/abs/2601.17204)
*Yinkai Wang,Yan Zhou Chen,Xiaohui Chen,Li-Ping Liu,Soha Hassoun*

Main category: cs.LG

TL;DR: SpecBridge通过微调光谱编码器与冻结分子模型对齐，实现检索准确率提升20-25%，参数高效。


<details>
  <summary>Details</summary>
Motivation: 由于光谱库不完整，MS/MS小分子鉴定存在瓶颈，现有方法局限于生成模型或从头对比学习。

Method: SpecBridge将结构鉴定视为几何对齐问题，微调自监督光谱编码器(DreaMS)投影至冻结分子模型(ChemBERTa)的潜在空间，通过余弦相似度检索。

Result: 在三个基准测试中，SpecBridge将top-1检索准确率相对提升20-25%，同时保持较少可训练参数。

Conclusion: 与冻结基础模型对齐是设计新架构的可行且稳定的替代方案。

Abstract: Small-molecule identification from tandem mass spectrometry (MS/MS) remains a bottleneck in untargeted settings where spectral libraries are incomplete. While deep learning offers a solution, current approaches typically fall into two extremes: explicit generative models that construct molecular graphs atom-by-atom, or joint contrastive models that learn cross-modal subspaces from scratch. We introduce SpecBridge, a novel implicit alignment framework that treats structure identification as a geometric alignment problem. SpecBridge fine-tunes a self-supervised spectral encoder (DreaMS) to project directly into the latent space of a frozen molecular foundation model (ChemBERTa), and then performs retrieval by cosine similarity to a fixed bank of precomputed molecular embeddings. Across MassSpecGym, Spectraverse, and MSnLib benchmarks, SpecBridge improves top-1 retrieval accuracy by roughly 20-25% relative to strong neural baselines, while keeping the number of trainable parameters small. These results suggest that aligning to frozen foundation models is a practical, stable alternative to designing new architectures from scratch. The code for SpecBridge is released at https://github.com/HassounLab/SpecBridge.

</details>


### [76] [JetFormer: A Scalable and Efficient Transformer for Jet Tagging from Offline Analysis to FPGA Triggers](https://arxiv.org/abs/2601.17215)
*Ruoqing Zheng,Chang Sun,Qibin Liu,Lauri Laatu,Arianna Cox,Benedikt Maier,Alexander Tapper,Jose G. F. Coutinho,Wayne Luk,Zhiqiang Que*

Main category: cs.LG

TL;DR: JetFormer是一个用于LHC粒子喷注标记的多功能可扩展编码器-only Transformer架构，能在全场景下工作，性能媲美SOTA模型但计算效率高37.4%，并可压缩至适合FPGA部署的超低延迟版本。


<details>
  <summary>Details</summary>
Motivation: 现有喷注标记方法通常针对特定部署场景定制，缺乏一个能在高精度离线分析和超低延迟在线触发等全场景下统一工作的通用架构。

Method: 设计JetFormer编码器-only Transformer，直接处理可变长度粒子特征集而不依赖显式成对交互；引入硬件感知优化管道进行多目标超参数搜索；采用结构化剪枝和量化进行模型压缩。

Result: 在JetClass数据集上，JetFormer以少37.4%的计算量达到与ParT模型相当精度（差距0.7%）；在HLS4ML 150P数据集上比MLPs、Deep Sets等模型准确率高3-4%；可压缩为JetFormer-tiny等适合FPGA、延迟低于微秒的版本。

Conclusion: JetFormer统一了高性能建模与可部署性，为在LHC离线与在线环境中部署Transformer喷注标记器提供了实用路径。

Abstract: We present JetFormer, a versatile and scalable encoder-only Transformer architecture for particle jet tagging at the Large Hadron Collider (LHC). Unlike prior approaches that are often tailored to specific deployment regimes, JetFormer is designed to operate effectively across the full spectrum of jet tagging scenarios, from high-accuracy offline analysis to ultra-low-latency online triggering. The model processes variable-length sets of particle features without relying on input of explicit pairwise interactions, yet achieves competitive or superior performance compared to state-of-the-art methods. On the large-scale JetClass dataset, a large-scale JetFormer matches the accuracy of the interaction-rich ParT model (within 0.7%) while using 37.4% fewer FLOPs, demonstrating its computational efficiency and strong generalization. On benchmark HLS4ML 150P datasets, JetFormer consistently outperforms existing models such as MLPs, Deep Sets, and Interaction Networks by 3-4% in accuracy. To bridge the gap to hardware deployment, we further introduce a hardware-aware optimization pipeline based on multi-objective hyperparameter search, yielding compact variants like JetFormer-tiny suitable for FPGA-based trigger systems with sub-microsecond latency requirements. Through structured pruning and quantization, we show that JetFormer can be aggressively compressed with minimal accuracy loss. By unifying high-performance modeling and deployability within a single architectural framework, JetFormer provides a practical pathway for deploying Transformer-based jet taggers in both offline and online environments at the LHC. Code is available at https://github.com/walkieq/JetFormer.

</details>


### [77] [Parameter Inference and Uncertainty Quantification with Diffusion Models: Extending CDI to 2D Spatial Conditioning](https://arxiv.org/abs/2601.17224)
*Dmitrii Torbunov,Yihui Ren,Lijun Wu,Yimei Zhu*

Main category: cs.LG

TL;DR: 将条件扩散模型逆问题求解器(CDI)从一维时序信号扩展到二维空间数据，成功应用于电子衍射参数推断，提供真实不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 科学反问题中不确定性量化至关重要，但现有CDI方法仅验证于一维时序信号，其在高维空间数据上的适用性尚未探索。CBED多参数反演问题需要从二维衍射图案中提取材料特性，但标准回归方法会掩盖不确定性。

Method: 将CDI扩展至二维空间条件化，使用模拟CBED数据（包含真实参数）进行验证，并与标准回归方法对比。

Result: CDI生成了校准良好的后验分布，能准确反映测量约束（对确定参数给出窄分布，对模糊参数给出宽分布）；而标准回归方法虽然整体指标看似准确，但对约束不良参数仅预测训练集均值，掩盖了真实不确定性。

Conclusion: CDI成功从时序域扩展到空间域，为稳健科学推断提供了真实的不确定性信息。

Abstract: Uncertainty quantification is critical in scientific inverse problems to distinguish identifiable parameters from those that remain ambiguous given available measurements. The Conditional Diffusion Model-based Inverse Problem Solver (CDI) has previously demonstrated effective probabilistic inference for one-dimensional temporal signals, but its applicability to higher-dimensional spatial data remains unexplored. We extend CDI to two-dimensional spatial conditioning, enabling probabilistic parameter inference directly from spatial observations. We validate this extension on convergent beam electron diffraction (CBED) parameter inference - a challenging multi-parameter inverse problem in materials characterization where sample geometry, electronic structure, and thermal properties must be extracted from 2D diffraction patterns. Using simulated CBED data with ground-truth parameters, we demonstrate that CDI produces well-calibrated posterior distributions that accurately reflect measurement constraints: tight distributions for well-determined quantities and appropriately broad distributions for ambiguous parameters. In contrast, standard regression methods - while appearing accurate on aggregate metrics - mask this underlying uncertainty by predicting training set means for poorly constrained parameters. Our results confirm that CDI successfully extends from temporal to spatial domains, providing the genuine uncertainty information required for robust scientific inference.

</details>


### [78] [A Constrained Optimization Perspective of Unrolled Transformers](https://arxiv.org/abs/2601.17257)
*Javier Porras-Valenzuela,Samar Hadou,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 提出约束优化框架训练类优化下降算法的Transformer，通过层间下降约束和原始-对偶训练方案提升模型鲁棒性与泛化能力


<details>
  <summary>Details</summary>
Motivation: 标准Transformer缺乏优化过程的可解释性，导致对扰动敏感且分布外泛化能力有限

Method: 1. 在目标函数上施加层间下降约束 2. 用原始-对偶训练替代经验风险最小化 3. 应用于展开式Transformer和预训练模型

Result: 在视频去噪/文本分类任务中：保持分布内性能的同时，显著提升对抗扰动鲁棒性（+15-22%）和分布外泛化能力

Conclusion: 约束优化框架使Transformer具备显式优化过程，为构建更可靠的自适应推理模型提供新范式

Abstract: We introduce a constrained optimization framework for training transformers that behave like optimization descent algorithms. Specifically, we enforce layerwise descent constraints on the objective function and replace standard empirical risk minimization (ERM) with a primal-dual training scheme. This approach yields models whose intermediate representations decrease the loss monotonically in expectation across layers. We apply our method to both unrolled transformer architectures and conventional pretrained transformers on tasks of video denoising and text classification. Across these settings, we observe constrained transformers achieve stronger robustness to perturbations and maintain higher out-of-distribution generalization, while preserving in-distribution performance.

</details>


### [79] [The Viscosity of Logic: Phase Transitions and Hysteresis in DPO Alignment](https://arxiv.org/abs/2601.17260)
*Marco Pollanen*

Main category: cs.LG

TL;DR: DPO alignment pressure (β) does not monotonically improve model capability; optimal β varies by architecture, margin-based selection can harm reasoning, and high-β damage persists (hysteresis), requiring capability-resolved evaluation across β values.


<details>
  <summary>Details</summary>
Motivation: Challenging the assumption that increasing DPO's alignment pressure (β) always yields better-aligned models, as prior work treats β as a monotonic improvement parameter without systematic investigation of its effects on capability.

Method: Densely swept β values across three 7B open-weight model families (Mistral, Llama, Qwen) under fixed DPO training, analyzing capability changes via logic-probe margins and reasoning metrics.

Result: 1) Capability is non-monotonic in Mistral (optimal only near β≈10⁻²), 2) Architectures show distinct response modes (sharp reorganization in Mistral, selective changes in Llama, smooth trade-offs in Qwen), 3) DPO margin anticorrelates with reasoning (r=-0.91 for Llama), 4) High-β exposure causes persistent capability loss (hysteresis).

Conclusion: Relying on DPO margins or aggregate benchmarks is unreliable; rigorous capability evaluation across the full β landscape is necessary to avoid selecting capability-impaired models and account for architecture-specific β responses.

Abstract: Direct Preference Optimization (DPO) is often tuned as if increasing alignment pressure (controlled by $β$) yields progressively "better" behavior. We instead treat $β$ as a control parameter and densely sweep it for three 7B open-weight families under a fixed DPO recipe. In Mistral, capability is sharply non-monotonic: aggregated logic-probe margins become positive only in a narrow band near $β\approx 10^{-2}$ and revert outside it, with boundary points that are seed-sensitive. Across architectures under the same sweep, we observe qualitatively different response modes: sharp reorganization in Mistral, selective changes in Llama, and smooth trade-offs in Qwen. Critically, the DPO preference margin can anticorrelate with reasoning capability (Pearson $r=-0.91$ for Llama logic), so margin-based selection can prefer capability-impaired models. Training path also matters: exposure to high $β$ induces capability losses that persist even after $β$ is reduced (hysteresis). These findings motivate capability-resolved evaluation across the $β$ landscape rather than reliance on margins or aggregate benchmarks.

</details>


### [80] [AGZO: Activation-Guided Zeroth-Order Optimization for LLM Fine-Tuning](https://arxiv.org/abs/2601.17261)
*Wei Lin,Yining Jiang,Qingyu Song,Qiao Xiang,Hong Xu*

Main category: cs.LG

TL;DR: AGZO improves zeroth-order LLM fine-tuning by restricting perturbations to activation-informed low-rank subspaces, outperforming baselines while maintaining memory efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing zeroth-order optimization methods use isotropic perturbations that waste structural information from forward passes, despite being memory-efficient. The paper identifies that gradients of linear layers are confined to subspaces spanned by input activations.

Method: Activation-Guided Zeroth-Order (AGZO) optimization: During forward passes, it dynamically extracts compact, activation-informed low-rank subspaces and constrains perturbations to these subspaces instead of using random isotropic perturbations.

Result: Evaluated on Qwen3 and Pangu models, AGZO consistently outperforms state-of-the-art zeroth-order baselines, significantly narrows the performance gap with first-order fine-tuning, and maintains nearly identical peak memory footprint as other zeroth-order methods.

Conclusion: AGZO provides a theoretically-grounded, practical improvement to memory-constrained LLM fine-tuning by leveraging activation structure, achieving better optimization directions and performance without additional memory overhead.

Abstract: Zeroth-Order (ZO) optimization has emerged as a promising solution for fine-tuning LLMs under strict memory constraints, as it avoids the prohibitive memory cost of storing activations for backpropagation. However, existing ZO methods typically employ isotropic perturbations, neglecting the rich structural information available during the forward pass. In this paper, we identify a crucial link between gradient formation and activation structure: the gradient of a linear layer is confined to the subspace spanned by its input activations. Leveraging this insight, we propose Activation-Guided Zeroth-Order optimization (AGZO). Unlike prior methods, AGZO extracts a compact, activation-informed subspace on the fly during the forward pass and restricts perturbations to this low-rank subspace. We provide a theoretical framework showing that AGZO optimizes a subspace-smoothed objective and provably yields update directions with higher cosine similarity to the true gradient than isotropic baselines. Empirically, we evaluate AGZO on Qwen3 and Pangu models across various benchmarks. AGZO consistently outperforms state-of-the-art ZO baselines and significantly narrows the performance gap with first-order fine-tuning, while maintaining almost the same peak memory footprint as other ZO methods.

</details>


### [81] [Unrolled Neural Networks for Constrained Optimization](https://arxiv.org/abs/2601.17274)
*Samar Hadou,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: This paper proposes Constrained Dual Unrolling (CDU), a neural network framework that unrolls dual ascent algorithms to solve constrained optimization problems by coupling primal and dual networks with constrained learning, achieving near-optimal solutions and strong out-of-distribution generalization on MIQPs and wireless power allocation.


<details>
  <summary>Details</summary>
Motivation: To create accelerated, learnable neural network counterparts for dual ascent algorithms that solve constrained optimization problems, addressing limitations of standard unrolling methods through constrained learning dynamics.

Method: Develops a two-network framework (CDU) where 1) a primal network emulates iterative optimization for given dual multipliers, and 2) a dual network generates multiplier trajectories. Uses constrained learning with primal-descent/dual-ascent constraints and alternating nested optimization to train networks while mitigating multiplier distribution uncertainty.

Result: Numerical evaluation on mixed-integer quadratic programs (MIQPs) and wireless network power allocation demonstrates the method produces near-optimal, near-feasible solutions and exhibits strong out-of-distribution generalization performance.

Conclusion: The CDU framework successfully provides an effective learnable approach for constrained optimization, offering improved generalization and solution quality compared to standard unrolling methods through its constrained learning formulation and coupled network design.

Abstract: In this paper, we develop unrolled neural networks to solve constrained optimization problems, offering accelerated, learnable counterparts to dual ascent (DA) algorithms. Our framework, termed constrained dual unrolling (CDU), comprises two coupled neural networks that jointly approximate the saddle point of the Lagrangian. The primal network emulates an iterative optimizer that finds a stationary point of the Lagrangian for a given dual multiplier, sampled from an unknown distribution. The dual network generates trajectories towards the optimal multipliers across its layers while querying the primal network at each layer. Departing from standard unrolling, we induce DA dynamics by imposing primal-descent and dual-ascent constraints through constrained learning. We formulate training the two networks as a nested optimization problem and propose an alternating procedure that updates the primal and dual networks in turn, mitigating uncertainty in the multiplier distribution required for primal network training. We numerically evaluate the framework on mixed-integer quadratic programs (MIQPs) and power allocation in wireless networks. In both cases, our approach yields near-optimal near-feasible solutions and exhibits strong out-of-distribution (OOD) generalization.

</details>


### [82] [Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning](https://arxiv.org/abs/2601.17275)
*Lianlei Shan,Han Chen,Yixuan Wang,Zhenjie Liu,Wei Li*

Main category: cs.LG

TL;DR: 提出DeepLatent Reasoning (DLR)框架，通过将强化学习从离散token空间转移到连续潜空间，解决LLM复杂推理中的样本低效、高方差和灾难性遗忘问题，实现更稳定训练和长程推理能力累积。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂多步推理任务中仍停留在"统计拟合"而非系统性逻辑推导层面，传统强化学习虽引入"先思考后回答"范式，但在高维离散token空间中面临样本低效、梯度方差高和灾难性遗忘三大瓶颈。

Method: 构建潜空间双向对比强化学习框架：使用轻量级辅助模型在潜空间高效采样K条推理链编码，通过正确性和格式双奖励机制筛选高质量潜轨迹，输入冻结的主模型进行单步解码；设计对比学习目标实现潜空间定向探索，冻结主模型参数以数学上消除灾难性遗忘。

Result: 在同等GPU计算预算下，DLR实现更稳定的训练收敛、支持更长周期推理链，并能可持续地累积推理能力。

Conclusion: 该方法为构建可靠可扩展的大语言模型强化学习提供了可行路径，通过潜空间优化从根本上解决了token级RL的结构性缺陷。

Abstract: While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting'' rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts to mitigate this by introducing a ``think-before-speak'' paradigm. However, applying RL directly in high-dimensional, discrete token spaces faces three inherent challenges: sample-inefficient rollouts, high gradient estimation variance, and the risk of catastrophic forgetting. To fundamentally address these structural bottlenecks, we propose \textbf{DeepLatent Reasoning (DLR)}, a latent-space bidirectional contrastive reinforcement learning framework. This framework shifts the trial-and-error cost from expensive token-level full sequence generation to the continuous latent manifold. Specifically, we introduce a lightweight assistant model to efficiently sample $K$ reasoning chain encodings within the latent space. These encodings are filtered via a dual reward mechanism based on correctness and formatting; only high-value latent trajectories are fed into a \textbf{frozen main model} for single-pass decoding. To maximize reasoning diversity while maintaining coherence, we design a contrastive learning objective to enable directed exploration within the latent space. Since the main model parameters remain frozen during optimization, this method mathematically eliminates catastrophic forgetting. Experiments demonstrate that under comparable GPU computational budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates the sustainable accumulation of reasoning capabilities, providing a viable path toward reliable and scalable reinforcement learning for LLMs.

</details>


### [83] [Tabular Foundation Models are Strong Graph Anomaly Detectors](https://arxiv.org/abs/2601.17301)
*Yunhui Liu,Tieke He,Yongchao Liu,Can Yi,Hong Jin,Chuntao Hong*

Main category: cs.LG

TL;DR: 本文提出TFM4GAD框架，通过将图结构展平为增强特征表并适配表格基础模型，实现了无需重训练即可跨多个图数据集检测异常的通用图异常检测方法，性能显著优于专用模型。


<details>
  <summary>Details</summary>
Motivation: 现有图异常检测方法采用"每个数据集一个模型"的范式，导致计算成本高、数据需求大、跨数据集迁移时泛化能力差。亟需一种基础模型方案，能够在不同图结构上无需重训练即可实现通用检测。

Method: TFM4GAD通过"展平"图结构，构建包含拉普拉斯嵌入、局部与全局结构特征以及异常敏感邻域聚合的增强特征表，利用表格基础模型的全上下文学习机制进行处理。

Result: 在多数据集和多种表格基础模型骨干上的广泛实验表明，TFM4GAD相比从零训练的专业化GAD模型取得了显著的性能提升。

Conclusion: 该研究为利用表格基础模型作为通用图异常检测器提供了新视角和实用范式，证明了跨领域适配基础模型解决图问题的有效性。

Abstract: Graph anomaly detection (GAD), which aims to identify abnormal nodes that deviate from the majority, has become increasingly important in high-stakes Web domains. However, existing GAD methods follow a "one model per dataset" paradigm, leading to high computational costs, substantial data demands, and poor generalization when transferred to new datasets. This calls for a foundation model that enables a "one-for-all" GAD solution capable of detecting anomalies across diverse graphs without retraining. Yet, achieving this is challenging due to the large structural and feature heterogeneity across domains. In this paper, we propose TFM4GAD, a simple yet effective framework that adapts tabular foundation models (TFMs) for graph anomaly detection. Our key insight is that the core challenges of foundation GAD, handling heterogeneous features, generalizing across domains, and operating with scarce labels, are the exact problems that modern TFMs are designed to solve via synthetic pre-training and powerful in-context learning. The primary challenge thus becomes structural: TFMs are agnostic to graph topology. TFM4GAD bridges this gap by "flattening" the graph, constructing an augmented feature table that enriches raw node features with Laplacian embeddings, local and global structural characteristics, and anomaly-sensitive neighborhood aggregations. This augmented table is processed by a TFM in a fully in-context regime. Extensive experiments on multiple datasets with various TFM backbones reveal that TFM4GAD surprisingly achieves significant performance gains over specialized GAD models trained from scratch. Our work offers a new perspective and a practical paradigm for leveraging TFMs as powerful, generalist graph anomaly detectors.

</details>


### [84] [PAR: Plausibility-aware Amortized Recourse Generation](https://arxiv.org/abs/2601.17309)
*Anagha Sabu,Vidhya S,Narayanan C Krishnan*

Main category: cs.LG

TL;DR: PAR is an amortized inference method for algorithmic recourse that generates high-quality counterfactuals by maximizing accepted-class likelihood while respecting constraints, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Develop realistic and feasible algorithmic recourse by formulating it as Constrained MAP inference to find high-likelihood counterfactuals under accepted-class distribution.

Method: PAR uses tractable probabilistic models for amortized inference, trained to maximize accepted-class likelihood, minimize denied-class likelihood, and satisfy constraints, with neighborhood conditioning for customization.

Result: PAR generates valid, similar, sparse, and plausible recourses efficiently, achieving superior performance on standard datasets.

Conclusion: PAR successfully produces high-quality algorithmic recourses meeting multiple criteria efficiently, surpassing state-of-the-art approaches.

Abstract: Algorithmic recourse aims to recommend actionable changes to a factual's attributes that flip an unfavorable model decision while remaining realistic and feasible. We formulate recourse as a Constrained Maximum A-Posteriori (MAP) inference problem under the accepted-class data distribution seeking counterfactuals with high likelihood while respecting other recourse constraints. We present PAR, an amortized approximate inference procedure that generates highly likely recourses efficiently. Recourse likelihood is estimated directly using tractable probabilistic models that admit exact likelihood evaluation and efficient gradient propagation that is useful during training. The recourse generator is trained with the objective of maximizing the likelihood under the accepted-class distribution while minimizing the likelihood under the denied-class distribution and other losses that encode recourse constraints. Furthermore, PAR includes a neighborhood-based conditioning mechanism to promote recourse generation that is customized to a factual. We validate PAR on widely used algorithmic recourse datasets and demonstrate its efficiency in generating recourses that are valid, similar to the factual, sparse, and highly plausible, yielding superior performance over existing state-of-the-art approaches.

</details>


### [85] [Conformal Feedback Alignment: Quantifying Answer-Level Reliability for Robust LLM Alignment](https://arxiv.org/abs/2601.17329)
*Tiejin Chen,Xiaoou Liu,Vishnu Nandam,Kuan-Ru Liou,Hua Wei*

Main category: cs.LG

TL;DR: 提出CFA框架，利用conformal prediction量化答案可靠性并加权偏好，提升LLM对齐的鲁棒性和数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于偏好的对齐方法（如RLHF）面临标签噪声和不一致问题，现有不确定性感知方法仅对偏好加权，却忽略了被比较答案本身的可靠性这一更根本因素。

Method: 提出Conformal Feedback Alignment (CFA)框架：通过conformal prediction构建可控覆盖率的预测集来量化答案级可靠性，并将这些可靠性聚合为原则性权重，应用于DPO和PPO式训练。

Result: 跨数据集实验表明，CFA提升了alignment的鲁棒性和数据效率。

Conclusion: 建模答案端不确定性可补充偏好级加权，实现更鲁棒、数据高效的对齐。

Abstract: Preference-based alignment like Reinforcement Learning from Human Feedback (RLHF) learns from pairwise preferences, yet the labels are often noisy and inconsistent. Existing uncertainty-aware approaches weight preferences, but ignore a more fundamental factor: the reliability of the \emph{answers} being compared. To address the problem, we propose Conformal Feedback Alignment (CFA), a framework that grounds preference weighting in the statistical guarantees of Conformal Prediction (CP). CFA quantifies answer-level reliability by constructing conformal prediction sets with controllable coverage and aggregates these reliabilities into principled weights for both DPO- and PPO-style training. Experiments across different datasets show that CFA improves alignment robustness and data efficiency, highlighting that modeling \emph{answer-side} uncertainty complements preference-level weighting and yields more robust, data-efficient alignment. Codes are provided here.

</details>


### [86] [Thermodynamically Optimal Regularization under Information-Geometric Constraints](https://arxiv.org/abs/2601.17330)
*Laurent Caraffa*

Main category: cs.LG

TL;DR: This paper develops a unified theoretical framework linking thermodynamic optimality, information geometry, and regularization in machine learning, proving that the Fisher-Rao metric uniquely defines optimal geometry and revealing that classical regularization methods cannot guarantee thermodynamic efficiency.


<details>
  <summary>Details</summary>
Motivation: Modern machine learning relies on heterogeneous regularization techniques (weight decay, dropout, etc.) while facing rising energy costs in training large models, raising questions about fundamental efficiency bounds in learning algorithms.

Method: The authors propose a theoretical framework based on three assumptions: intrinsic parametrization-invariant information measure, maximum-entropy belief distributions, and quasi-static optimal processes. They prove a conditional optimality theorem connecting Fisher-Rao metric with thermodynamic optimal regularization.

Result: The Fisher-Rao metric is the unique admissible geometry on belief space; thermodynamically optimal regularization minimizes squared Fisher-Rao distance to a reference state. For Gaussian and circular belief models, this yields hyperbolic and von Mises manifolds. Classical regularization schemes are structurally incapable of guaranteeing thermodynamic optimality.

Conclusion: This work establishes a principled geometric and thermodynamic foundation for regularization in machine learning and proposes experimentally testable predictions for thermodynamic efficiency of learning.

Abstract: Modern machine learning relies on a collection of empirically successful but theoretically heterogeneous regularization techniques, such as weight decay, dropout, and exponential moving averages. At the same time, the rapidly increasing energetic cost of training large models raises the question of whether learning algorithms approach any fundamental efficiency bound. In this work, we propose a unifying theoretical framework connecting thermodynamic optimality, information geometry, and regularization.
  Under three explicit assumptions -- (A1) that optimality requires an intrinsic, parametrization-invariant measure of information, (A2) that belief states are modeled by maximum-entropy distributions under known constraints, and (A3) that optimal processes are quasi-static -- we prove a conditional optimality theorem. Specifically, the Fisher--Rao metric is the unique admissible geometry on belief space, and thermodynamically optimal regularization corresponds to minimizing squared Fisher--Rao distance to a reference state.
  We derive the induced geometries for Gaussian and circular belief models, yielding hyperbolic and von Mises manifolds, respectively, and show that classical regularization schemes are structurally incapable of guaranteeing thermodynamic optimality. We introduce a notion of thermodynamic efficiency of learning and propose experimentally testable predictions. This work provides a principled geometric and thermodynamic foundation for regularization in machine learning.

</details>


### [87] [Power-based Partial Attention: Bridging Linear-Complexity and Full Attention](https://arxiv.org/abs/2601.17334)
*Yufeng Huang*

Main category: cs.LG

TL;DR: 提出幂次部分注意力(PPA)机制，证明在0<p<1时O(L^(1+p))次注意力可达到与全注意力O(L²)相当的性能，表明二次注意力可能并非必需。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer研究普遍认为"注意力就是一切"，但从未系统量化所需注意力的计算量。二次O(L²)注意力是否必要，是否存在可达到相当性能的次二次注意力机制？

Method: 引入幂次部分注意力(PPA)，其复杂度为O(L^(1+p))，其中0≤p≤1。p=0对应线性复杂度的滑动窗口注意力，p=1对应全注意力。通过调节p值探索Transformer性能随注意力缩放行为的变化。

Result: 实验显示S型曲线行为：性能在p的窄窗口内从滑动窗口注意力过渡到全注意力，并在p接近1时趋于平稳。存在0<p<1使得O(L^(1+p))注意力足以达到与O(L²)全注意力相似的结果。

Conclusion: 次二次注意力机制能够达到与全二次注意力相当的性能，表明二次注意力可能并非严格必需。

Abstract: It is widely accepted from transformer research that "attention is all we need", but the amount of attention required has never been systematically quantified. Is quadratic $O(L^2)$ attention necessary, or is there a sub-quadratic attention mechanism that can achieve comparable performance? To answer this question, we introduce power-based partial attention (PPA), an attention mechanism of order $O(L^{1+p})$, where $0 \leq p \leq 1$, such that $p=0$ corresponds to sliding window attention with linear complexity, and $p=1$ corresponds to full attention. With this attention construction, we can explore how transformer architecture performance varies as a function of the attention scaling behavior controlled by $p$. The overall trend from our experiments shows an S-curve-like behavior where the performance transitions from sliding-window (linear-complexity) attention to full attention over a narrow window of $p$ values, and plateaus as $p$ approaches $1$. In our experiments, we show that there exists $0<p<1$ such that $O(L^{1+p})$ attention is sufficient to achieve similar results as $O(L^2)$ full attention.

</details>


### [88] [Robust Privacy: Inference-Time Privacy through Certified Robustness](https://arxiv.org/abs/2601.17360)
*Jiankai Jin,Xiangzheng Zhang,Zhao Liu,Deyue Zhang,Quanchen Zou*

Main category: cs.LG

TL;DR: 提出Robust Privacy (RP)作为推理时隐私保护的新概念，基于认证鲁棒性，确保模型预测在输入邻域内不变，从而防止推断敏感属性，并通过Attribute Privacy Enhancement (APE)增强属性级隐私，有效缓解模型反演攻击。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统的个性化输出可能导致对手在推理时推断敏感输入属性，现有隐私保护方法存在不足，因此需要新的推理时隐私保护概念。

Method: 引入Robust Privacy (RP)概念，定义模型预测在半径为R的输入邻域内（如ℓ2范数）具有不变性，则该输入享有R-鲁棒隐私；开发Attribute Privacy Enhancement (APE)将输入级不变性转化为属性级隐私效果，并在推荐任务中验证。

Result: 在推荐任务中，RP扩展了与正向推荐兼容的敏感属性值范围；实证显示RP显著缓解模型反演攻击，小噪声（σ=0.1）下攻击成功率从73%降至4%，且可无性能损失地将攻击成功率降至44%。

Conclusion: RP通过强制局部输入不变性，有效防止敏感属性推断和模型反演攻击，为推理时隐私保护提供了可行方案，且性能损失可控。

Abstract: Machine learning systems can produce personalized outputs that allow an adversary to infer sensitive input attributes at inference time. We introduce Robust Privacy (RP), an inference-time privacy notion inspired by certified robustness: if a model's prediction is provably invariant within a radius-$R$ neighborhood around an input $x$ (e.g., under the $\ell_2$ norm), then $x$ enjoys $R$-Robust Privacy, i.e., observing the prediction cannot distinguish $x$ from any input within distance $R$ of $x$. We further develop Attribute Privacy Enhancement (APE) to translate input-level invariance into an attribute-level privacy effect. In a controlled recommendation task where the decision depends primarily on a sensitive attribute, we show that RP expands the set of sensitive-attribute values compatible with a positive recommendation, expanding the inference interval accordingly. Finally, we empirically demonstrate that RP also mitigates model inversion attacks (MIAs) by masking fine-grained input-output dependence. Even at small noise levels ($σ=0.1$), RP reduces the attack success rate (ASR) from 73% to 4% with partial model performance degradation. RP can also partially mitigate MIAs (e.g., ASR drops to 44%) with no model performance degradation.

</details>


### [89] [Diversified Scaling Inference in Time Series Foundation Models](https://arxiv.org/abs/2601.17376)
*Ruijin Hua,Zichuan Liu,Kun Zhang,Yiyuan Yang*

Main category: cs.LG

TL;DR: This paper investigates inference-time compute for Time Series Foundation Models (TSFMs), finding that standard sampling fails due to poor solution space exploration. They propose diversified inference scaling via time series perturbations, theoretically derive a critical sample threshold, and show substantial performance gains without parameter updates, establishing inference design as a compute-efficient optimization dimension.


<details>
  <summary>Details</summary>
Motivation: The advancement of TSFMs has focused on large-scale pre-training while largely ignoring inference-time compute potential. Standard sampling-based inference scaling doesn't work well for TSFMs because they fail to explore the solution space sufficiently, violating scaling laws.

Method: Systematically examine TSFM behavior under standard sampling, then propose diversified inference scaling using tailored time series perturbations. Theoretically analyze the diversity-fidelity trade-off and derive a critical sample threshold. Conduct extensive experiments across various TSFMs and datasets, and propose RobustMSE as a metric for performance headroom under fixed budget.

Result: Proper diversified inference scaling yields substantial performance gains without parameter updates. The theoretical analysis provides a critical sample threshold that determines when diversified sampling outperforms standard sampling. Inference design is established as a critical, compute-efficient dimension of TSFM optimization.

Conclusion: Findings clarify interactions between factors in TSFM inference, enabling reliable performance via diverse large-scale inference time series in parallel environments without re-training. This opens up a new dimension for optimizing TSFMs efficiently at inference time.

Abstract: The advancement of Time Series Foundation Models (TSFMs) has been driven primarily by large-scale pre-training, but inference-time compute potential remains largely untapped. This work systematically investigates two questions: how do TSFMs behave under standard sampling-based inference scaling, and can controlled sampling diversity enhance performance? We first examine the properties of TSFMs under standard sampling often fail to adhere to scaling laws due to insufficient exploration of the solution space. Building on this, we then delve into diversified inference scaling via tailored time series perturbations to expand the generative distribution's support. We theoretically analyze the diversity-fidelity trade-off and derive a critical sample threshold for diversified sampling to outperform standard sampling. Extensive experiments across various TSFMs and datasets show proper diversified inference scaling yields substantial performance gains without parameter updates, establishing inference design as a critical, compute-efficient dimension of TSFM optimization. As an application, we propose RobustMSE, a rigorous metric to quantify the headroom performance of TSFM under a fixed budget. Overall, our findings clarify these factor interactions, enabling reliable performance via diverse large-scale inference time series in parallel environments without re-training TSFMs.

</details>


### [90] [GO-OSC and VASH: Geometry-Aware Representation Learning for Early Degradation Detection in Oscillatory Systems](https://arxiv.org/abs/2601.17396)
*Vashista Nobaub*

Main category: cs.LG

TL;DR: 提出GO-OSC框架，通过几何感知的表示学习实现振荡系统早期退化检测，证明能量基方法在相位退化下无效而几何探针敏感


<details>
  <summary>Details</summary>
Motivation: 振荡系统早期退化表现为相位抖动、频率漂移等几何失真，在信号能量变化可检测前就已发生。传统能量基诊断和无约束学习表示对此不敏感，导致检测延迟或不稳定。

Method: 提出GO-OSC几何感知表示学习框架，强制规范可识别的潜在参数化；构建不变线性几何探针靶向潜在空间中退化相关方向。

Result: 理论证明在相位退化下能量基统计量零阶检测能力为零，而几何探针具有正敏感性；合成和真实振动数据集实验验证了更早检测、更高数据效率和工况变化鲁棒性。

Conclusion: 规范可识别表示能通过几何探针实现早期退化检测，解决了非可识别表示下线性探测失效问题，为振荡系统健康监测提供新范式。

Abstract: Early-stage degradation in oscillatory systems often manifests as geometric distortions of the dynamics, such as phase jitter, frequency drift, or loss of coherence, long before changes in signal energy are detectable. In this regime, classical energy-based diagnostics and unconstrained learned representations are structurally insensitive, leading to delayed or unstable detection. We introduce GO-OSC, a geometry-aware representation learning framework for oscillatory time series that enforces a canonical and identifiable latent parameterization, enabling stable comparison and aggregation across short, unlabeled windows. Building on this representation, we define a family of invariant linear geometric probes that target degradation-relevant directions in latent space. We provide theoretical results showing that under early phase-only degradation, energy-based statistics have zero first-order detection power, whereas geometric probes achieve strictly positive sensitivity. Our analysis characterizes when and why linear probing fails under non-identifiable representations and shows how canonicalization restores statistical detectability. Experiments on synthetic benchmarks and real vibration datasets validate the theory, demonstrating earlier detection, improved data efficiency, and robustness to operating condition changes.

</details>


### [91] [Efficient Dilated Squeeze and Excitation Neural Operator for Differential Equations](https://arxiv.org/abs/2601.17407)
*Prajwal Chauhan,Salah Eddine Choutri,Saif Eddin Jabari*

Main category: cs.LG

TL;DR: D-SENO is a lightweight neural operator combining dilated convolutions and squeeze-excitation modules that achieves 20x faster training than transformers while maintaining accuracy on various PDE problems.


<details>
  <summary>Details</summary>
Motivation: Physics-driven PDE surrogates are crucial in aerodynamics, porous media, and flow control, but existing transformer-based and neural operator models are parameter-heavy, making training costly and deployment slow.

Method: D-SENO integrates dilated convolution blocks with squeeze-and-excitation modules to capture wide receptive fields and channel-wise attention for solving diverse PDEs including airfoil flow, Darcy flow, Poiseuille flow, and Navier-Stokes equations.

Result: The model achieves ~20x faster training speed compared to standard transformer-based models while matching or surpassing their accuracy across multiple PDE benchmarks; removing SE modules causes a slight performance drop.

Conclusion: D-SENO provides an accurate and efficient framework for PDE inference, demonstrating that careful design of dilation rates and channel attention mechanisms enables lightweight yet high-performing neural operators.

Abstract: Fast and accurate surrogates for physics-driven partial differential equations (PDEs) are essential in fields such as aerodynamics, porous media design, and flow control. However, many transformer-based models and existing neural operators remain parameter-heavy, resulting in costly training and sluggish deployment. We propose D-SENO (Dilated Squeeze-Excitation Neural Operator), a lightweight operator learning framework for efficiently solving a wide range of PDEs, including airfoil potential flow, Darcy flow in porous media, pipe Poiseuille flow, and incompressible Navier Stokes vortical fields. D-SENO combines dilated convolution (DC) blocks with squeeze-and-excitation (SE) modules to jointly capture wide receptive fields and dynamics alongside channel-wise attention, enabling both accurate and efficient PDE inference. Carefully chosen dilation rates allow the receptive field to focus on critical regions, effectively modeling long-range physical dependencies. Meanwhile, the SE modules adaptively recalibrate feature channels to emphasize dynamically relevant scales. Our model achieves training speed of up to approximately $20\times$ faster than standard transformer-based models and neural operators, while also surpassing (or matching) them in accuracy across multiple PDE benchmarks. Ablation studies show that removing the SE modules leads to a slight drop in performance.

</details>


### [92] [Active Hypothesis Testing for Correlated Combinatorial Anomaly Detection](https://arxiv.org/abs/2601.17430)
*Zichuan Yang,Yiming Xing*

Main category: cs.LG

TL;DR: Proposes ECC-AHT, an adaptive algorithm for identifying anomalous subsets of streams under correlated noise by maximizing Chernoff information through differential sensing, achieving optimal sample complexity.


<details>
  <summary>Details</summary>
Motivation: Monitoring and security in cyber-physical systems requires identifying anomalous streams, but existing methods assume independent observations and fail to exploit correlation for efficient measurement design.

Method: ECC-AHT adaptively selects continuous, constrained measurements to maximize Chernoff information between competing hypotheses, enabling active noise cancellation through differential sensing.

Result: Achieves optimal sample complexity guarantees and significantly outperforms state-of-the-art baselines in both synthetic and real-world correlated environments.

Conclusion: The algorithm successfully addresses combinatorial pure exploration under correlated noise, providing theoretical optimality and empirical superiority for cyber-physical system monitoring.

Abstract: We study the problem of identifying an anomalous subset of streams under correlated noise, motivated by monitoring and security in cyber-physical systems. This problem can be viewed as a form of combinatorial pure exploration, where each stream plays the role of an arm and measurements must be allocated sequentially under uncertainty. Existing combinatorial bandit and hypothesis testing methods typically assume independent observations and fail to exploit correlation for efficient measurement design. We propose ECC-AHT, an adaptive algorithm that selects continuous, constrained measurements to maximize Chernoff information between competing hypotheses, enabling active noise cancellation through differential sensing. ECC-AHT achieves optimal sample complexity guarantees and significantly outperforms state-of-the-art baselines in both synthetic and real-world correlated environments. The code is available on https://github.com/VincentdeCristo/ECC-AHT

</details>


### [93] [Data-driven Clustering and Merging of Adapters for On-device Large Language Models](https://arxiv.org/abs/2601.17441)
*Ondrej Bohdal,Taha Ceritli,Mete Ozay,Jijoong Moon,Kyeng-Hun Lee,Hyeonmok Ko,Umberto Michieli*

Main category: cs.LG

TL;DR: 本文提出D2C方法，通过少量任务示例和迭代优化对适配器进行聚类并合并，生成可在资源受限设备上部署的多任务适配器，有效提升存储预算下的性能。


<details>
  <summary>Details</summary>
Motivation: 设备端大语言模型使用任务特定适配器，但存储所有适配器不现实，如何在有限内存下选择能泛化到多任务的代表性适配器仍是未探索的挑战。

Method: D2C利用每任务少量示例（如10个）和迭代优化过程进行适配器聚类，将各簇内的适配器合并创建多任务适配器。

Result: 实验结果表明，该方法在考虑的存储预算下有效提升了性能。

Conclusion: D2C方法通过聚类和合并适配器，成功解决了资源受限设备的适配器选择问题，创建了可部署的通用多任务适配器。

Abstract: On-device large language models commonly employ task-specific adapters (e.g., LoRAs) to deliver strong performance on downstream tasks. While storing all available adapters is impractical due to memory constraints, mobile devices typically have sufficient capacity to store a limited number of these parameters. This raises a critical challenge: how to select representative adapters that generalize well across multiple tasks - a problem that remains unexplored in existing literature. We propose a novel method D2C for adapter clustering that leverages minimal task-specific examples (e.g., 10 per task) and employs an iterative optimization process to refine cluster assignments. The adapters within each cluster are merged, creating multi-task adapters deployable on resource-constrained devices. Experimental results demonstrate that our method effectively boosts performance for considered storage budgets.

</details>


### [94] [Harnessing Reasoning Trajectories for Hallucination Detection via Answer-agreement Representation Shaping](https://arxiv.org/abs/2601.17467)
*Jianxiong Zhang,Bing Guo,Yuming Jiang,Haobo Wang,Bo An,Xuefeng Du*

Main category: cs.LG

TL;DR: 提出ARS方法，通过扰动推理边界嵌入生成反事实答案，学习能区分答案一致性的表示，以无监督方式提升大模型幻觉检测效果。


<details>
  <summary>Details</summary>
Motivation: 大推理模型常生成看似连贯但错误的推理轨迹，直接利用轨迹文本或隐藏状态进行幻觉检测效果不佳，易受形式变化影响且会过拟合表面模式而非答案有效性。

Method: ARS通过微小潜在干预（扰动轨迹边界嵌入）生成反事实答案，根据答案是否一致进行标注，学习将一致答案状态聚集、不一致答案状态分离的表示，暴露指示幻觉风险的潜在不稳定性。

Result: 实验表明ARS能持续提升检测性能，相比强基线取得显著增益。

Conclusion: ARS生成的塑形嵌入可与现有基于嵌入的检测器即插即用，且训练无需人工标注，为幻觉检测提供了有效的无监督表示学习方法。

Abstract: Large reasoning models (LRMs) often generate long, seemingly coherent reasoning traces yet still produce incorrect answers, making hallucination detection challenging. Although trajectories contain useful signals, directly using trace text or vanilla hidden states for detection is brittle: traces vary in form and detectors can overfit to superficial patterns rather than answer validity. We introduce Answer-agreement Representation Shaping (ARS), which learns detection-friendly trace-conditioned representations by explicitly encoding answer stability. ARS generates counterfactual answers through small latent interventions, specifically, perturbing the trace-boundary embedding, and labels each perturbation by whether the resulting answer agrees with the original. It then learns representations that bring answer-agreeing states together and separate answer-disagreeing ones, exposing latent instability indicative of hallucination risk. The shaped embeddings are plug-and-play with existing embedding-based detectors and require no human annotations during training. Experiments demonstrate that ARS consistently improves detection and achieves substantial gains over strong baselines.

</details>


### [95] [LeanTutor: Towards a Verified AI Mathematical Proof Tutor](https://arxiv.org/abs/2601.17473)
*Manooshree Patel,Rayna Bhattacharyya,Thomas Lu,Arnav Mehta,Niels Voss,Narges Norouzi,Gireeja Ranade*

Main category: cs.LG

TL;DR: 提出LeanTutor系统，结合大型语言模型的自然语言交互能力与定理证明器的可证明正确性，打造可验证正确的AI数学证明辅导工具


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLMs)虽支持自然语言交互但易出错，定理证明器(如Lean)可保证正确性但难学，需融合两者优势构建更可靠的数学辅导系统

Method: 设计三模块系统：自动形式化/证明检查器、下一步生成器、自然语言反馈生成器；构建PeanoBench数据集(371个人工编写的皮亚诺算术证明)用于评估

Result: 实现了概念验证系统LeanTutor，创建了包含自然语言和形式化语言证明的基准数据集PeanoBench

Conclusion: 融合LLMs与定理证明器是开发可证明正确且用户友好的数学证明辅导系统的可行路径

Abstract: This paper considers the development of an AI-based provably-correct mathematical proof tutor. While Large Language Models (LLMs) allow seamless communication in natural language, they are error prone. Theorem provers such as Lean allow for provable-correctness, but these are hard for students to learn. We present a proof-of-concept system (LeanTutor) by combining the complementary strengths of LLMs and theorem provers. LeanTutor is composed of three modules: (i) an autoformalizer/proof-checker, (ii) a next-step generator, and (iii) a natural language feedback generator. To evaluate the system, we introduce PeanoBench, a dataset of 371 Peano Arithmetic proofs in human-written natural language and formal language, derived from the Natural Numbers Game.

</details>


### [96] [Unintended Memorization of Sensitive Information in Fine-Tuned Language Models](https://arxiv.org/abs/2601.17480)
*Marton Szep,Jorge Marin Ruiz,Georgios Kaissis,Paulina Seidl,Rüdiger von Eisenhart-Rothe,Florian Hinterwimmer,Daniel Rueckert*

Main category: cs.LG

TL;DR: This paper investigates how fine-tuned LLMs memorize and leak PII from input data, studies influencing factors, and benchmarks privacy-preserving methods, finding post-training approaches offer better privacy-utility trade-offs.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning LLMs on sensitive datasets risks unintended PII memorization and leakage, violating privacy regulations and compromising individual safety, particularly for PII appearing only in model inputs (not training targets).

Method: Using synthetic and real-world datasets with controlled extraction probes to quantify PII memorization; studying factors like language, PII frequency, task type, and model size; benchmarking four privacy-preserving approaches (differential privacy, machine unlearning, regularization, preference alignment) and evaluating privacy-utility trade-offs.

Result: Post-training methods provide more consistent privacy-utility trade-offs; differential privacy achieves strong leakage reduction in specific settings but introduces training instability.

Conclusion: Memorization in fine-tuned LLMs remains a persistent challenge, highlighting the need for robust, scalable privacy-preserving techniques.

Abstract: Fine-tuning Large Language Models (LLMs) on sensitive datasets carries a substantial risk of unintended memorization and leakage of Personally Identifiable Information (PII), which can violate privacy regulations and compromise individual safety. In this work, we systematically investigate a critical and underexplored vulnerability: the exposure of PII that appears only in model inputs, not in training targets. Using both synthetic and real-world datasets, we design controlled extraction probes to quantify unintended PII memorization and study how factors such as language, PII frequency, task type, and model size influence memorization behavior. We further benchmark four privacy-preserving approaches including differential privacy, machine unlearning, regularization, and preference alignment, evaluating their trade-offs between privacy and task performance. Our results show that post-training methods generally provide more consistent privacy-utility trade-offs, while differential privacy achieves strong reduction in leakage in specific settings, although it can introduce training instability. These findings highlight the persistent challenge of memorization in fine-tuned LLMs and emphasize the need for robust, scalable privacy-preserving techniques.

</details>


### [97] [SpatialMath: Spatial Comprehension-Infused Symbolic Reasoning for Mathematical Problem-Solving](https://arxiv.org/abs/2601.17489)
*Ashutosh Bajpai,Akshat Bhandari,Akshay Nambi,Tanmoy Chakraborty*

Main category: cs.LG

TL;DR: SpatialMath是一种空间理解增强的符号推理框架，通过从视觉图表中提取空间表示并融入结构化符号推理链，显著提升多模态语言模型在几何数学问题上的性能，性能提升高达10个百分点。


<details>
  <summary>Details</summary>
Motivation: 多模态中小型语言模型在视觉理解和数学推理方面仍存在显著局限，特别是在具有多样化视觉注入的几何问题中。当前模型难以准确分解复杂视觉输入并将感知与结构化推理连接，导致性能不佳。

Method: 提出SpatialMath框架，采用专用感知模块从视觉图表中提取空间基础的表示，捕捉关键几何结构和空间关系。这些表示被系统地融入符号推理链，实现视觉理解感知的结构化推理。同时引入MATHVERSE-PLUS数据集，包含面向视觉密集型数学问题的结构化视觉解释和逐步推理路径。

Result: SpatialMath显著优于强大的多模态基线模型，在视觉密集型设置下，相比数据增强的监督微调性能提升高达10个百分点。

Conclusion: 增强的空间表示直接提高了推理准确性，强化了在多模态语言模型中构建结构化感知-推理管道的重要性。

Abstract: Multimodal Small-to-Medium sized Language Models (MSLMs) have demonstrated strong capabilities in integrating visual and textual information but still face significant limitations in visual comprehension and mathematical reasoning, particularly in geometric problems with diverse levels of visual infusion. Current models struggle to accurately decompose intricate visual inputs and connect perception with structured reasoning, leading to suboptimal performance. To address these challenges, we propose SpatialMath, a novel Spatial Comprehension-Infused Symbolic Reasoning Framework designed to integrate spatial representations into structured symbolic reasoning chains. SpatialMath employs a specialized perception module to extract spatially-grounded representations from visual diagrams, capturing critical geometric structures and spatial relationships. These representations are then methodically infused into symbolic reasoning chains, facilitating visual comprehension-aware structured reasoning. To this end, we introduce MATHVERSE-PLUS, a novel dataset containing structured visual interpretations and step-by-step reasoning paths for vision-intensive mathematical problems. SpatialMath significantly outperforms strong multimodal baselines, achieving up to 10 percentage points improvement over supervised fine-tuning with data augmentation in vision-intensive settings. Robustness analysis reveals that enhanced spatial representations directly improve reasoning accuracy, reinforcing the need for structured perception-to-reasoning pipelines in MSLMs.

</details>


### [98] [PEARL: Prototype-Enhanced Alignment for Label-Efficient Representation Learning with Deployment-Driven Insights from Digital Governance Communication Systems](https://arxiv.org/abs/2601.17495)
*Ruiyu Zhang,Lin Nie,Wai-Fung Lam,Qihao Wang,Xin Zhao*

Main category: cs.LG

TL;DR: PEARL是一种标签高效的方法，利用有限监督将嵌入向量软对齐到类别原型，以改善标签稀缺和领域偏移场景下的最近邻检索性能。


<details>
  <summary>Details</summary>
Motivation: 预训练模型生成的原始嵌入向量与最近邻检索所需的局部邻域结构对齐不佳，导致系统失效。实际部署中标签稀缺、领域漂移，且重训练编码器成本高昂，下游性能严重依赖嵌入几何结构。

Method: PEARL（原型增强对齐表示学习）使用有限监督将嵌入向量软对齐到类别原型，重塑局部邻域几何，保持维度并避免投影崩溃，在无监督后处理和全监督投影之间架起桥梁。

Result: 在标签稀缺条件下，PEARL使局部邻域质量相比原始嵌入提升25.7%，相比强无监督后处理提升超过21.1%，特别是在基于相似度的系统最脆弱的场景中。

Conclusion: PEARL通过少量标签即可显著改善嵌入几何结构，有效解决了实际部署中的最近邻检索质量问题，无需大量标注数据或昂贵重训练。

Abstract: In many deployed systems, new text inputs are handled by retrieving similar past cases, for example when routing and responding to citizen messages in digital governance platforms. When these systems fail, the problem is often not the language model itself, but that the nearest neighbors in the embedding space correspond to the wrong cases. Modern machine learning systems increasingly rely on fixed, high-dimensional embeddings produced by large pretrained models and sentence encoders. In real-world deployments, labels are scarce, domains shift over time, and retraining the base encoder is expensive or infeasible. As a result, downstream performance depends heavily on embedding geometry. Yet raw embeddings are often poorly aligned with the local neighborhood structure required by nearest-neighbor retrieval, similarity search, and lightweight classifiers that operate directly on embeddings. We propose PEARL (Prototype-Enhanced Aligned Representation Learning), a label-efficient approach that uses limited supervision to softly align embeddings toward class prototypes. The method reshapes local neighborhood geometry while preserving dimensionality and avoiding aggressive projection or collapse. Its aim is to bridge the gap between purely unsupervised post-processing, which offers limited and inconsistent gains, and fully supervised projections that require substantial labeled data. We evaluate PEARL under controlled label regimes ranging from extreme label scarcity to higher-label settings. In the label-scarce condition, PEARL substantially improves local neighborhood quality, yielding 25.7% gains over raw embeddings and more than 21.1% gains relative to strong unsupervised post-processing, precisely in the regime where similarity-based systems are most brittle.

</details>


### [99] [One-Shot Federated Clustering of Non-Independent Completely Distributed Data](https://arxiv.org/abs/2601.17512)
*Yiqun Zhang,Shenghong Cai,Zihua Yang,Sen Feng,Yuzhu Ji,Haijun Zhang*

Main category: cs.LG

TL;DR: 针对联邦聚类中非独立同分布（Non-IID）数据导致的聚类碎片化问题，该论文提出了一种新的非独立完全分布（Non-ICD）概念，并设计了GOLD框架，通过局部聚类分布学习、全局融合和局部增强来解决联邦聚类中的知识融合挑战。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在隐私保护方面取得显著成果，但大多数边缘设备数据无标签，使得无监督联邦聚类（FC）越来越受欢迎。然而，Non-IID问题严重挑战FC性能，特别是存在聚类碎片化现象（不同客户端可能分割同一个簇），这限制了现有FC方法的聚类效果。

Method: 提出GOLD（Global Oriented Local Distribution Learning）框架：1）精细探索客户端潜在的不完整局部聚类分布；2）将分布摘要上传至服务器进行全局融合；3）在全局分布指导下进行局部聚类增强。

Result: 通过大量实验（显著性检验、消融研究、可扩展性评估、定性结果等）证明了GOLD的优越性，能够有效解决Non-ICD问题并提升联邦聚类性能。

Conclusion: 该论文揭示了联邦聚类中更隐蔽的Non-ICD现象，提出的GOLD框架为处理非独立同分布数据下的联邦聚类问题提供了有效解决方案，通过全局导向的局部分布学习方法实现了更好的知识融合和聚类性能。

Abstract: Federated Learning (FL) that extracts data knowledge while protecting the privacy of multiple clients has achieved remarkable results in distributed privacy-preserving IoT systems, including smart traffic flow monitoring, smart grid load balancing, and so on. Since most data collected from edge devices are unlabeled, unsupervised Federated Clustering (FC) is becoming increasingly popular for exploring pattern knowledge from complex distributed data. However, due to the lack of label guidance, the common Non-Independent and Identically Distributed (Non-IID) issue of clients have greatly challenged FC by posing the following problems: How to fuse pattern knowledge (i.e., cluster distribution) from Non-IID clients; How are the cluster distributions among clients related; and How does this relationship connect with the global knowledge fusion? In this paper, a more tricky but overlooked phenomenon in Non-IID is revealed, which bottlenecks the clustering performance of the existing FC approaches. That is, different clients could fragment a cluster, and accordingly, a more generalized Non-IID concept, i.e., Non-ICD (Non-Independent Completely Distributed), is derived. To tackle the above FC challenges, a new framework named GOLD (Global Oriented Local Distribution Learning) is proposed. GOLD first finely explores the potential incomplete local cluster distributions of clients, then uploads the distribution summarization to the server for global fusion, and finally performs local cluster enhancement under the guidance of the global distribution. Extensive experiments, including significance tests, ablation studies, scalability evaluations, qualitative results, etc., have been conducted to show the superiority of GOLD.

</details>


### [100] [Towards Generalisable Imitation Learning Through Conditioned Transition Estimation and Online Behaviour Alignment](https://arxiv.org/abs/2601.17563)
*Nathan Gavenski,Matteo Leonetti,Odinaldo Rodrigues*

Main category: cs.LG

TL;DR: Proposes UfO, an unsupervised imitation learning method that learns from observation without action labels using a two-stage process: first approximating teacher actions from state transitions, then refining the policy by aligning agent trajectories with the teacher. It outperforms existing methods and the teacher itself while showing better generalization.


<details>
  <summary>Details</summary>
Motivation: Existing imitation learning from observation (ILfO) methods have three key limitations: requiring action-based supervised optimization, assuming single optimal actions per state, and applying teacher actions without considering actual environment states. They struggle to extract meaningful patterns from observed trajectories without supervision.

Method: UfO uses a two-stage unsupervised approach: (1) approximating the teacher's true actions from observed state transitions, and (2) refining the policy by adjusting agent trajectories to closely match the teacher's demonstrated trajectories.

Result: In five standard environments, UfO outperforms both the teacher and all compared ILfO methods, achieving the smallest standard deviation, indicating superior performance stability and better generalization to unseen scenarios.

Conclusion: UfO successfully overcomes key limitations of existing ILfO methods by learning effectively from unsupervised observation data, demonstrating that high-quality imitation policies can be learned without action labels while achieving better generalization.

Abstract: State-of-the-art imitation learning from observation methods (ILfO) have recently made significant progress, but they still have some limitations: they need action-based supervised optimisation, assume that states have a single optimal action, and tend to apply teacher actions without full consideration of the actual environment state. While the truth may be out there in observed trajectories, existing methods struggle to extract it without supervision. In this work, we propose Unsupervised Imitation Learning from Observation (UfO) that addresses all of these limitations. UfO learns a policy through a two-stage process, in which the agent first obtains an approximation of the teacher's true actions in the observed state transitions, and then refines the learned policy further by adjusting agent trajectories to closely align them with the teacher's. Experiments we conducted in five widely used environments show that UfO not only outperforms the teacher and all other ILfO methods but also displays the smallest standard deviation. This reduction in standard deviation indicates better generalisation in unseen scenarios.

</details>


### [101] [Quantum-Inspired Episode Selection for Monte Carlo Reinforcement Learning via QUBO Optimization](https://arxiv.org/abs/2601.17570)
*Hadi Salloum,Ali Jnadi,Yaroslav Kholodov,Alexander Gasnikov*

Main category: cs.LG

TL;DR: 提出MC+QUBO方法，将蒙特卡洛强化学习中的轨迹选择建模为QUBO问题，利用量子启发采样器（SQA和SB）筛选高奖励且多样化的轨迹子集，从而降低样本复杂度并提升性能。


<details>
  <summary>Details</summary>
Motivation: 蒙特卡洛强化学习在稀疏奖励、大状态空间及轨迹相关的场景下存在样本复杂度高的固有缺陷，限制了其在复杂环境中的应用效率。

Method: 将episode选择重新表述为二次无约束二进制优化（QUBO）问题：线性项奖励高回报轨迹，二次项惩罚冗余以保障状态空间覆盖；采用模拟量子退火（SQA）和模拟分叉（SB）作为黑盒求解器，在标准蒙特卡洛策略评估中嵌入组合过滤步骤。

Result: 在有限时域GridWorld实验中，MC+QUBO相比传统蒙特卡洛方法显著提升了收敛速度和最终策略质量。

Conclusion: 量子启发优化可作为强化学习中的高效决策子程序，为解决样本效率问题提供了新思路。

Abstract: Monte Carlo (MC) reinforcement learning suffers from high sample complexity, especially in environments with sparse rewards, large state spaces, and correlated trajectories. We address these limitations by reformulating episode selection as a Quadratic Unconstrained Binary Optimization (QUBO) problem and solving it with quantum-inspired samplers. Our method, MC+QUBO, integrates a combinatorial filtering step into standard MC policy evaluation: from each batch of trajectories, we select a subset that maximizes cumulative reward while promoting state-space coverage. This selection is encoded as a QUBO, where linear terms favor high-reward episodes and quadratic terms penalize redundancy. We explore both Simulated Quantum Annealing (SQA) and Simulated Bifurcation (SB) as black-box solvers within this framework. Experiments in a finite-horizon GridWorld demonstrate that MC+QUBO outperforms vanilla MC in convergence speed and final policy quality, highlighting the potential of quantum-inspired optimization as a decision-making subroutine in reinforcement learning.

</details>


### [102] [Understanding Transformer Encoder-Decoder Representations through Bernoulli Dropout](https://arxiv.org/abs/2601.17602)
*Xuanzhou Chen*

Main category: cs.LG

TL;DR: 研究Transformer过参数化现象，通过角相似度和伯努利 dropout 发现了一个稀疏性临界阈值，超过该阈值后模型性能会急剧下降。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer过参数化如何影响模型在稀疏性（dropout）下的稳定性，并识别性能保持与退化的临界阈值。

Method: 从角相似度角度理论分析坐标dropout的影响，并构建一个带有二元擦除信道(BEC)的Transformer模型，在英法翻译任务上通过改变dropout概率p来寻找性能阈值。

Result: 验证准确率和BLEU分数在某个稀疏性阈值处急剧下降，而当有效稀疏性足够大时，Top-1预测在适度坐标dropout下保持稳定。

Conclusion: Transformer在适度dropout下因过参数化而表现稳定，但存在一个临界稀疏性阈值，超过后性能会崩溃，BEC增强模型实证了这一趋势。

Abstract: We study Transformer overparameterization through the lens of angular similarity in high-dimensional encoder-decoder embeddings. We apply Bernoulli dropout between the encoder and the decoder, varying the keep probability $p$ to identify a sparsity-dependent threshold above which the Top-1 prediction is preserved. Theoretically, we prove that, if the effective sparsity embeddings is sufficiently large, and thus decoder performance, remain stable under moderate coordinate dropout. Empirically, we implement the Bernoulli dropout by constructing a new Transformer model augmented with Binary Erasure Channel (BEC) and test its performance on an English-French translation task. Experimental results visualize the trends for validation accuracies and BLEU scores, both decline sharply at some threshold.

</details>


### [103] [A Thermodynamic Theory of Learning I: Irreversible Ensemble Transport and Epistemic Costs](https://arxiv.org/abs/2601.17607)
*Daisuke Okanohara*

Main category: cs.LG

TL;DR: 该论文通过引入认知自由能框架，将学习建模为概率分布空间中的不可逆输运过程，推导出认知速度极限(ESL)，从理论上证明了任何有限时间学习过程都必须产生熵，从而解决了学习如何在不违反信息论限制的情况下产生抽象和洞察这一根本问题。


<details>
  <summary>Details</summary>
Motivation: 经典信息论认为确定性变换不增加信息，但学习系统却能从数据中获取结构化表征并产生抽象洞察。这一矛盾揭示了现有理论框架的不足，需要新的视角来解释学习过程如何在不违反信息论基本限制的前提下实现知识获取。

Method: 将学习过程建模为模型配置空间上的概率分布输运过程，构建认知自由能理论框架。在该框架下定义自由能降作为核算量，将其分解为与势能改进相关的可逆分量和对应熵产生的不可逆分量，并基于此推导出有限时间不等式。

Result: 推导出认知速度极限(ESL)，该不等式给出了任何学习过程为实现特定分布变换所需的最小熵产生下限。该界限仅取决于初始和最终系综分布之间的Wasserstein距离，与具体学习算法无关，揭示了学习过程的根本热力学约束。

Conclusion: 学习本质上是有限时间内的不可逆过程，实现认知结构必然伴随熵产生。认知自由能框架和ESL界限为理解学习的热力学代价提供了普适理论，表明任何实际学习系统都面临基本的物理限制，无法在不产生熵的情况下完成抽象化过程。

Abstract: Learning systems acquire structured internal representations from data, yet classical information-theoretic results state that deterministic transformations do not increase information. This raises a fundamental question: how can learning produce abstraction and insight without violating information-theoretic limits?
  We argue that learning is inherently an irreversible process when performed over finite time, and that the realization of epistemic structure necessarily incurs entropy production. To formalize this perspective, we model learning as a transport process in the space of probability distributions over model configurations and introduce an epistemic free-energy framework.
  Within this framework, we define the free-energy drop as a bookkeeping quantity that records the total reduction of epistemic free energy along a learning trajectory. This reduction decomposes into a reversible component associated with potential improvement and an irreversible component corresponding to entropy production.
  We then derive the Epistemic Speed Limit (ESL), a finite-time inequality that lower-bounds the minimal entropy production required by any learning process to realize a given distributional transformation. This bound depends only on the Wasserstein distance between initial and final ensemble distributions and is independent of the specific learning algorithm.

</details>


### [104] [Split-on-Share: Mixture of Sparse Experts for Task-Agnostic Continual Learning](https://arxiv.org/abs/2601.17616)
*Fatema Siddika,Md Anwar Hossen,Tanwi Mallick,Ali Jannesari*

Main category: cs.LG

TL;DR: SETAsolves catastrophic forgetting in LLMs by using Mixture of Sparse Experts to separate task-specific and shared knowledge through modular subspaces, outperforming existing parameter-efficient methods.


<details>
  <summary>Details</summary>
Motivation: Continual learning in LLMs suffers from the plasticity-stability dilemma where acquiring new capabilities causes catastrophic forgetting of prior knowledge, and existing methods fail to distinguish between task-specific and shared parameters.

Method: SETA decomposes models into modular subspaces with unique experts for task-specific patterns and shared experts for common features, protected by elastic weight anchoring and a unified gating network for automatic expert retrieval during inference.

Result: Extensive experiments across diverse benchmarks demonstrate SETA consistently outperforms state-of-the-art parameter-efficient fine-tuning-based continual learning methods.

Conclusion: The modular subspace architecture effectively resolves the plasticity-stability conflict and provides a promising framework for task-agnostic continual learning in LLMs.

Abstract: Continual learning in Large Language Models (LLMs) is hindered by the plasticity-stability dilemma, where acquiring new capabilities often leads to catastrophic forgetting of previous knowledge. Existing methods typically treat parameters uniformly, failing to distinguish between specific task knowledge and shared capabilities. We introduce Mixture of Sparse Experts for Task-Agnostic Continual Learning, referred to as SETA, a framework that resolves the plasticity-stability conflict by decomposing the model into modular subspaces. Unlike standard updates, where tasks compete for the same parameters, SETA separates knowledge into unique experts, designed to isolate task-specific patterns, and shared experts, responsible for capturing common features. This structure is maintained through elastic weight anchoring, which protects critical shared knowledge and enables a unified gating network to automatically retrieve the correct expert combination for each task during inference. Extensive experiments across diverse domain-specific and general benchmarks demonstrate that SETA consistently outperforms state-of-the-art parameter-efficient fine-tuning-based continual learning methods.

</details>


### [105] [BrainDistill: Implantable Motor Decoding with Task-Specific Knowledge Distillation](https://arxiv.org/abs/2601.17625)
*Yuhan Xie,Jinhan Liu,Xiaoyong Ni,Fei Tan,Icare Sakr,Thibault Collin,Shiqi Sun,Alejandro Rodriguez Guajardo,Demon Fanny,Charles-francois Vincent Latchoumane,Henri Lorach,Jocelyne Bloch,Gregoire Courtine,Mahsa Shoaran*

Main category: cs.LG

TL;DR: 该论文提出BrainDistill，一个用于植入式脑机接口的流水线，通过任务特定知识蒸馏和量化感知训练，创造出低功耗神经解码器，在满足功率约束的同时性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 用于脑机接口的大型基于Transformer的神经解码器具有高计算需求和功耗，使其不适合具有严格功率约束的植入式系统。

Method: 作者引入BrainDistill，整合了：(1) 植入式神经解码器(IND)，(2) 任务特定知识蒸馏(TSKD)框架，通过监督投影优先处理解码关键特征，(3) 量化感知训练方案，实现具有学习激活裁剪范围的纯整数推理。

Result: 在多个神经数据集上，IND在运动解码任务上持续优于先前的神经解码器。TSKD蒸馏变体在少样本校准设置中超越替代蒸馏方法。量化模型在植入式脑机接口功率约束下成功部署，性能损失极小。

Conclusion: BrainDistill通过目标蒸馏和量化，为在功率受限的植入式脑机接口系统中部署高性能神经解码器提供了有效解决方案。

Abstract: Transformer-based neural decoders with large parameter counts, pre-trained on large-scale datasets, have recently outperformed classical machine learning models and small neural networks on brain-computer interface (BCI) tasks. However, their large parameter counts and high computational demands hinder deployment in power-constrained implantable systems. To address this challenge, we introduce BrainDistill, a novel implantable motor decoding pipeline that integrates an implantable neural decoder (IND) with a task-specific knowledge distillation (TSKD) framework. Unlike standard feature distillation methods that attempt to preserve teacher representations in full, TSKD explicitly prioritizes features critical for decoding through supervised projection. Across multiple neural datasets, IND consistently outperforms prior neural decoders on motor decoding tasks, while its TSKD-distilled variant further surpasses alternative distillation methods in few-shot calibration settings. Finally, we present a quantization-aware training scheme that enables integer-only inference with activation clipping ranges learned during training. The quantized IND enables deployment under the strict power constraints of implantable BCIs with minimal performance loss.

</details>


### [106] [RPNT: Robust Pre-trained Neural Transformer -- A Pathway for Generalized Motor Decoding](https://arxiv.org/abs/2601.17641)
*Hao Fang,Ryan A. Canfield,Tomohiro Ouchi,Beatrice Macagno,Eli Shlizerman,Amy L. Orsborn*

Main category: cs.LG

TL;DR: The paper proposes RPNT (Robust Pretrained Neural Transformer), a novel brain decoding model that uses pretraining with specialized components (MRoPE, context-based attention, and robust SSL) to achieve robust generalization across brain sites, sessions, behavior types, and subjects, consistently outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Current brain decoding models lack full generalization capability across variations like different brain recording sites, sessions, behavior types, and subjects, necessitating development of pretrained transformer models that can adapt and generalize effectively.

Method: RPNT incorporates: 1) Multidimensional rotary positional embedding (MRoPE) to incorporate experimental metadata; 2) Context-based attention via convolution kernels to learn local temporal structures for handling neural non-stationarity; 3) Robust self-supervised learning with uniform causal masking and contrastive representations. The model was pretrained on two NHP datasets (microelectrode and Neuropixel recordings from PMd/M1 during reaching tasks) and evaluated on cross-session, cross-type, cross-subject, and cross-site decoding tasks.

Result: RPNT consistently achieves and surpasses the decoding performance of existing models across all cross-domain generalization tasks tested.

Conclusion: The proposed RPNT model successfully addresses brain decoding generalization challenges through pretraining, demonstrating superior performance and establishing an effective approach for robust neural decoding across diverse conditions.

Abstract: Brain decoding aims to interpret and translate neural activity into behaviors. As such, it is imperative that decoding models are able to generalize across variations, such as recordings from different brain sites, distinct sessions, different types of behavior, and a variety of subjects. Current models can only partially address these challenges and warrant the development of pretrained neural transformer models capable to adapt and generalize. In this work, we propose RPNT - Robust Pretrained Neural Transformer, designed to achieve robust generalization through pretraining, which in turn enables effective finetuning given a downstream task. In particular, RPNT unique components include 1) Multidimensional rotary positional embedding (MRoPE) to aggregate experimental metadata such as site coordinates, session name and behavior types; 2) Context-based attention mechanism via convolution kernels operating on global attention to learn local temporal structures for handling non-stationarity of neural population activity; 3) Robust self-supervised learning (SSL) objective with uniform causal masking strategies and contrastive representations. We pretrained two separate versions of RPNT on distinct datasets a) Multi-session, multi-task, and multi-subject microelectrode benchmark; b) Multi-site recordings using high-density Neuropixel 1.0 probes. The datasets include recordings from the dorsal premotor cortex (PMd) and from the primary motor cortex (M1) regions of nonhuman primates (NHPs) as they performed reaching tasks. After pretraining, we evaluated the generalization of RPNT in cross-session, cross-type, cross-subject, and cross-site downstream behavior decoding tasks. Our results show that RPNT consistently achieves and surpasses the decoding performance of existing decoding models in all tasks.

</details>


### [107] [A Mosco sufficient condition for intrinsic stability of non-unique convex Empirical Risk Minimization](https://arxiv.org/abs/2601.17646)
*Karim Bounja,Lahcen Laayouni,Abdeljalil Sakat*

Main category: cs.LG

TL;DR: Proposes Painlevé-Kuratowski upper semicontinuity (PK-u.s.c.) as the intrinsic stability notion for set-valued ERM solutions under non-strict convex losses, showing Mosco consistency and local boundedness ensure qualitative stability, while quadratic growth yields explicit quantitative error bounds.


<details>
  <summary>Details</summary>
Motivation: Traditional ERM stability analysis focuses on single-valued outputs, but convex non-strict losses inherently produce set-valued minimizers, necessitating a rigorous set-level stability framework to interpret solution behavior under perturbations.

Method: Identifies PK-u.s.c. as the core stability concept for ERM solution correspondences, analyzes it through Mosco-consistent perturbations and locally bounded minimizers, and leverages quadratic growth conditions to derive explicit deviation bounds.

Result: Under minimal non-degenerate conditions (Mosco consistency + local boundedness), PK-u.s.c., minimal-value continuity, and consistency of vanishing-gap near-minimizers are achieved; quadratic growth further provides concrete quantitative deviation estimates for solutions.

Conclusion: PK-u.s.c. is the essential foundation for interpreting set-valued ERM stability, with the established qualitative and quantitative regimes offering comprehensive guarantees for solution robustness under realistic optimization conditions.

Abstract: Empirical risk minimization (ERM) stability is usually studied via single-valued outputs, while convex non-strict losses yield set-valued minimizers. We identify Painlevé-Kuratowski upper semicontinuity (PK-u.s.c.) as the intrinsic stability notion for the ERM solution correspondence (set-level Hadamard well-posedness) and a prerequisite to interpret stability of selections. We then characterize a minimal non-degenerate qualitative regime: Mosco-consistent perturbations and locally bounded minimizers imply PK-u.s.c., minimal-value continuity, and consistency of vanishing-gap near-minimizers. Quadratic growth yields explicit quantitative deviation bounds.

</details>


### [108] [Time-Varying Causal Treatment for Quantifying the Causal Effect of Short-Term Variations on Arctic Sea Ice Dynamics](https://arxiv.org/abs/2601.17647)
*Akila Sampath,Vandana Janeja,Jianwu Wang*

Main category: cs.LG

TL;DR: 本研究提出知识引导的因果模型变分自编码器(KGCM-VAE)，通过融合物理约束与因果推断，量化海冰厚度与海平面高度之间的因果机制，在北极数据集上实现了更优的异质性效应估计精度。


<details>
  <summary>Details</summary>
Motivation: 冰融化与淡水分布的因果关系对理解极地气候变化和海平面上升至关重要，但现有深度学习模型在时空场景下因未观测混杂因素和缺乏物理约束而难以可靠估计处理效应。

Method: 提出KGCM-VAE框架，包含：(1)基于sigmoid函数的速度调制方案，根据SSH过渡动态放大平滑速度信号生成因果处理；(2)最大均值差异(MMD)平衡潜在空间中处理组与控制组的协变量分布；(3)因果邻接约束解码器确保符合物理结构。

Result: 在合成和真实北极数据集上，KGCM-VAE相比先进基准方法获得更优的PEHE指标；消融研究表明，MMD与因果邻接约束的联合应用使估计误差降低1.88%。

Conclusion: 该框架通过整合物理知识与因果建模，有效提升了海冰厚度与海平面高度间因果效应的估计精度，验证了物理约束与分布平衡策略的协同有效性。

Abstract: Quantifying the causal relationship between ice melt and freshwater distribution is critical, as these complex interactions manifest as regional fluctuations in sea surface height (SSH). Leveraging SSH as a proxy for sea ice dynamics enables improved understanding of the feedback mechanisms driving polar climate change and global sea-level rise. However, conventional deep learning models often struggle with reliable treatment effect estimation in spatiotemporal settings due to unobserved confounders and the absence of physical constraints. To address these challenges, we propose the Knowledge-Guided Causal Model Variational Autoencoder (KGCM-VAE) to quantify causal mechanisms between sea ice thickness and SSH. The proposed framework integrates a velocity modulation scheme in which smoothed velocity signals are dynamically amplified via a sigmoid function governed by SSH transitions to generate physically grounded causal treatments. In addition, the model incorporates Maximum Mean Discrepancy (MMD) to balance treated and control covariate distributions in the latent space, along with a causal adjacency-constrained decoder to ensure alignment with established physical structures. Experimental results on both synthetic and real-world Arctic datasets demonstrate that KGCM-VAE achieves superior PEHE compared to state-of-the-art benchmarks. Ablation studies further confirm the effectiveness of the approach, showing that the joint application of MMD and causal adjacency constraints yields a 1.88\% reduction in estimation error.

</details>


### [109] [Kareus: Joint Reduction of Dynamic and Static Energy in Large Model Training](https://arxiv.org/abs/2601.17654)
*Ruofan Wu,Jae-Won Chung,Mosharaf Chowdhury*

Main category: cs.LG

TL;DR: 针对AI训练能耗问题，Kareus系统通过细粒度内核调度与频率调节的联合优化，相比现有方法降低能耗28.3%或减少训练时间27.5%。


<details>
  <summary>Details</summary>
Motivation: AI计算需求快速增长而能源供应不足，现有优化方法仅关注动态或静态能耗单一维度，缺乏对两者的联合优化。

Method: 设计Kareus训练系统，将联合优化问题分解为基于分区的子问题，采用多通道多目标优化算法寻找最优执行调度方案。

Result: 相比现有最优方法，Kareus可在相同训练时间下降低能耗最多28.3%，或在相同能耗下缩短训练时间最多27.5%。

Conclusion: Kareus通过细粒度内核调度与频率调节的协同优化，有效推进了AI训练的时间-能耗权衡边界。

Abstract: The computing demand of AI is growing at an unprecedented rate, but energy supply is not keeping pace. As a result, energy has become an expensive, contended resource that requires explicit management and optimization. Although recent works have made significant progress in large model training optimization, they focus only on a single aspect of energy consumption: dynamic or static energy.
  We find that fine-grained kernel scheduling and frequency scaling jointly and interdependently impact both dynamic and static energy consumption. Based on this finding, we design Kareus, a training system that pushes the time--energy tradeoff frontier by optimizing both aspects. Kareus decomposes the intractable joint optimization problem into local, partition-based subproblems. It then uses a multi-pass multi-objective optimization algorithm to find execution schedules that push the time--energy tradeoff frontier. Compared to the state of the art, Kareus reduces training energy by up to 28.3% at the same training time, or reduces training time by up to 27.5% at the same energy consumption.

</details>


### [110] [Entropic Risk-Aware Monte Carlo Tree Search](https://arxiv.org/abs/2601.17667)
*Pedro P. Santos,Jacopo Silvestrin,Alberto Sardinha,Francisco S. Melo*

Main category: cs.LG

TL;DR: 提出一种可证明正确的蒙特卡洛树搜索算法，用于求解具有熵风险测度的风险感知马尔可夫决策过程，提供非渐近收敛性和多项式遗憾界保证


<details>
  <summary>Details</summary>
Motivation: 传统MDP仅优化期望回报，无法刻画风险敏感需求；熵风险测度是相干风险度量，但求解计算复杂，现有方法缺乏有限样本理论保证

Method: 设计针对ERM目标的MCTS算法，结合动态规划 formulation 和UCB树搜索策略，进行非渐近理论分析

Result: 算法正确性：根节点经验ERM收敛到最优值；享受多项式遗憾集中性；实验显示优于相关基线方法

Conclusion: 首个为风险感知MDP提供可证明有限样本保证的MCTS算法，桥接理论严谨性与实际应用，为风险敏感序贯决策提供可靠解法

Abstract: We propose a provably correct Monte Carlo tree search (MCTS) algorithm for solving \textit{risk-aware} Markov decision processes (MDPs) with \textit{entropic risk measure} (ERM) objectives. We provide a \textit{non-asymptotic} analysis of our proposed algorithm, showing that the algorithm: (i) is \textit{correct} in the sense that the empirical ERM obtained at the root node converges to the optimal ERM; and (ii) enjoys \textit{polynomial regret concentration}. Our algorithm successfully exploits the dynamic programming formulations for solving risk-aware MDPs with ERM objectives introduced by previous works in the context of an upper confidence bound-based tree search algorithm. Finally, we provide a set of illustrative experiments comparing our risk-aware MCTS method against relevant baselines.

</details>


### [111] [Fast KVzip: Efficient and Accurate LLM Inference with Gated KV Eviction](https://arxiv.org/abs/2601.17668)
*Jang-Hyun Kim,Dongyoon Han,Sangdoo Yun*

Main category: cs.LG

TL;DR: 提出一种基于门控的KV缓存淘汰方法，通过轻量级sink-attention门控模块识别关键KV对，无需反向传播即可实现高达70%的缓存压缩率，在多个LLM系列和任务上保持近乎无损的性能。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存压缩技术在性能退化与计算开销之间存在权衡，亟需一种高效管理方案以支持大语言模型的实用化部署。

Method: 针对冻结权重LLM设计轻量级sink-attention门控模块，用于识别和保留关键KV对；提出仅需前向传播的轻量门训练算法，采用任务无关的重建目标实现强泛化能力；无缝集成到预填充和解码阶段。

Result: 在Qwen2.5-1M、Qwen3和Gemma3模型族上实现高达70%的KV缓存淘汰率，同时保持近乎无损的性能；在长上下文理解、代码理解和数学推理等任务上表现一致。

Conclusion: 该方法以极低的计算成本实现了高压缩比，展现出良好的通用性和任务泛化能力，为LLM的高效部署提供了有效解决方案。

Abstract: Efficient key-value (KV) cache management is crucial for the practical deployment of large language models (LLMs), yet existing compression techniques often incur a trade-off between performance degradation and computational overhead. We propose a novel gating-based KV cache eviction method for frozen-weight LLMs that achieves high compression ratios with negligible computational cost. Our approach introduces lightweight sink-attention gating modules to identify and retain critical KV pairs, and integrates seamlessly into both the prefill and decoding stages. The proposed gate training algorithm relies on forward passes of an LLM, avoiding expensive backpropagation, while achieving strong task generalization through a task-agnostic reconstruction objective. Extensive experiments across the Qwen2.5-1M, Qwen3, and Gemma3 families show that our method maintains near-lossless performance while evicting up to 70% of the KV cache. The results are consistent across a wide range of tasks, including long-context understanding, code comprehension, and mathematical reasoning, demonstrating the generality of our approach.

</details>


### [112] [$\infty$-MoE: Generalizing Mixture of Experts to Infinite Experts](https://arxiv.org/abs/2601.17680)
*Shota Takashiro,Takeshi Kojima,Shohei Taniguchi,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.LG

TL;DR: 本文提出∞-MoE，一种连续空间的专家混合方法，通过采样FFN参数而非选择完整专家，实现无限专家数量，在保持计算效率的同时提升模型性能


<details>
  <summary>Details</summary>
Motivation: 传统MoE将专家视为独立离散单元，专家数量增加时训练困难。本文旨在通过连续空间表示稳定训练，同时扩展专家数量

Method: ∞-MoE：基于每个token采样的连续值，从大型FFN中选择部分参数，将专家置于连续空间中，实现无限专家组合

Result: 129M激活参数/186M总参数的GPT-2 Small ∞-MoE模型性能媲美350M参数的密集GPT-2 Medium，比传统MoE准确率提升最高2.5%，推理时可灵活调整专家数量平衡精度与速度

Conclusion: 连续专家选择机制有效解决了大规模专家训练稳定性问题，在计算效率与性能间取得更好平衡，并提供动态推理灵活性

Abstract: The Mixture of Experts (MoE) selects a few feed-forward networks (FFNs) per token, achieving an effective trade-off between computational cost and performance. In conventional MoE, each expert is treated as entirely independent, and experts are combined in a discrete space. As a result, when the number of experts increases, it becomes difficult to train each expert effectively. To stabilize training while increasing the number of experts, we propose $\infty$-MoE that selects a portion of the parameters of large FFNs based on continuous values sampled for each token. By considering experts in a continuous space, this approach allows for an infinite number of experts while maintaining computational efficiency. Experiments show that a GPT-2 Small-based $\infty$-MoE model, with 129M active and 186M total parameters, achieves comparable performance to a dense GPT-2 Medium with 350M parameters. Adjusting the number of sampled experts at inference time allows for a flexible trade-off between accuracy and speed, with an improvement of up to 2.5\% in accuracy over conventional MoE.

</details>


### [113] [REV-INR: Regularized Evidential Implicit Neural Representation for Uncertainty-Aware Volume Visualization](https://arxiv.org/abs/2601.17689)
*Shanu Saklani,Tushar M. Athawale,Nairita Pal,David Pugmire,Christopher R. Johnson,Soumya Dutta*

Main category: cs.LG

TL;DR: REV-INR introduces a regularized evidential framework for Implicit Neural Representations (INRs) to simultaneously predict volumetric data values and coordinate-level uncertainty (both data/aleatoric and model/epistemic) in a single forward pass, enabling reliable visualization and analysis of large datasets where raw data is inaccessible.


<details>
  <summary>Details</summary>
Motivation: Conventional deterministic INRs lack uncertainty quantification, leading to potentially unreliable reconstructions and visualizations when representing large volumetric datasets where raw data verification is infeasible due to size.

Method: Proposes REV-INR (Regularized Evidential INR), a novel framework that integrates evidential deep learning with INRs to output both data predictions and associated uncertainties via a single inference pass, using regularization for robust uncertainty estimation.

Result: Outperforms existing uncertainty estimation methods by achieving superior volume reconstruction quality, robust aleatoric and epistemic uncertainty estimates, and the fastest inference time simultaneously.

Conclusion: REV-INR enables trustworthy assessment of isosurface extraction and volume visualization results solely from model-predicted data, enhancing reliability for scientific analysis of large-scale volumetric datasets.

Abstract: Applications of Implicit Neural Representations (INRs) have emerged as a promising deep learning approach for compactly representing large volumetric datasets. These models can act as surrogates for volume data, enabling efficient storage and on-demand reconstruction via model predictions. However, conventional deterministic INRs only provide value predictions without insights into the model's prediction uncertainty or the impact of inherent noisiness in the data. This limitation can lead to unreliable data interpretation and visualization due to prediction inaccuracies in the reconstructed volume. Identifying erroneous results extracted from model-predicted data may be infeasible, as raw data may be unavailable due to its large size. To address this challenge, we introduce REV-INR, Regularized Evidential Implicit Neural Representation, which learns to predict data values accurately along with the associated coordinate-level data uncertainty and model uncertainty using only a single forward pass of the trained REV-INR during inference. By comprehensively comparing and contrasting REV-INR with existing well-established deep uncertainty estimation methods, we show that REV-INR achieves the best volume reconstruction quality with robust data (aleatoric) and model (epistemic) uncertainty estimates using the fastest inference time. Consequently, we demonstrate that REV-INR facilitates assessment of the reliability and trustworthiness of the extracted isosurfaces and volume visualization results, enabling analyses to be solely driven by model-predicted data.

</details>


### [114] [FedCCA: Client-Centric Adaptation against Data Heterogeneity in Federated Learning on IoT Devices](https://arxiv.org/abs/2601.17713)
*Kaile Wang,Jiannong Cao,Yu Yang,Xiaoyin Li,Yinfeng Cao*

Main category: cs.LG

TL;DR: FedCCA: A client-centric federated learning algorithm that uses dynamic client selection and adaptive aggregation with client-specific encoders to tackle data heterogeneity in IoT, showing significant performance gains over existing methods.


<details>
  <summary>Details</summary>
Motivation: IoT devices generate private sensory data requiring AI training, but federated learning suffers from data heterogeneity that degrades performance. Existing methods struggle to extract client-specific information during local training while preserving privacy.

Method: Proposes FedCCA with dynamic client selection, adaptive aggregation using client-specific encoders, and attention-based global aggregation to learn unique models for each client through selective adaptation.

Result: Extensive experiments on diverse datasets demonstrate substantial performance advantage over competing baselines in addressing data heterogeneity.

Conclusion: FedCCA effectively alleviates data heterogeneity issues in federated learning for IoT by optimally utilizing client-specific knowledge through selective adaptation.

Abstract: With the rapid development of the Internet of Things (IoT), AI model training on private data such as human sensing data is highly desired. Federated learning (FL) has emerged as a privacy-preserving distributed training framework for this purpuse. However, the data heterogeneity issue among IoT devices can significantly degrade the model performance and convergence speed in FL. Existing approaches limit in fixed client selection and aggregation on cloud server, making the privacy-preserving extraction of client-specific information during local training challenging. To this end, we propose Client-Centric Adaptation federated learning (FedCCA), an algorithm that optimally utilizes client-specific knowledge to learn a unique model for each client through selective adaptation, aiming to alleviate the influence of data heterogeneity. Specifically, FedCCA employs dynamic client selection and adaptive aggregation based on the additional client-specific encoder. To enhance multi-source knowledge transfer, we adopt an attention-based global aggregation strategy. We conducted extensive experiments on diverse datasets to assess the efficacy of FedCCA. The experimental results demonstrate that our approach exhibits a substantial performance advantage over competing baselines in addressing this specific problem.

</details>


### [115] [AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation](https://arxiv.org/abs/2601.17761)
*Dongjie Cheng,Ruifeng Yuan,Yongqi Li,Runyang You,Wenjie Wang,Liqiang Nie,Lei Zhang,Wenjie Li*

Main category: cs.LG

TL;DR: 提出AR-Omni，一种统一的自回归全模态模型，无需专家解码器即可实现文本、图像和语音的实时生成。


<details>
  <summary>Details</summary>
Motivation: 现实感知是多模态的，但现有全模态MLLMs依赖专家组件，限制了统一性。文本自回归建模以其简洁优雅的可扩展性为多模态生成提供了理想框架。

Method: AR-Omni使用单一Transformer解码器支持文本、图像和流式语音的自回归生成。通过任务感知损失重加权解决模态不平衡，令牌级感知对齐提升图像保真度，有限状态解码平衡稳定性与创造性。

Result: 在三种模态上实现高质量实时生成，语音实时因子达0.88。

Conclusion: 成功在单一自回归框架中统一多模态生成，无需专家解码器，为全模态模型提供了简洁可扩展的解决方案。

Abstract: Real-world perception and interaction are inherently multimodal, encompassing not only language but also vision and speech, which motivates the development of "Omni" MLLMs that support both multimodal inputs and multimodal outputs. While a sequence of omni MLLMs has emerged, most existing systems still rely on additional expert components to achieve multimodal generation, limiting the simplicity of unified training and inference. Autoregressive (AR) modeling, with a single token stream, a single next-token objective, and a single decoder, is an elegant and scalable foundation in the text domain. Motivated by this, we present AR-Omni, a unified any-to-any model in the autoregressive paradigm without any expert decoders. AR-Omni supports autoregressive text and image generation, as well as streaming speech generation, all under a single Transformer decoder. We further address three practical issues in unified AR modeling: modality imbalance via task-aware loss reweighting, visual fidelity via a lightweight token-level perceptual alignment loss for image tokens, and stability-creativity trade-offs via a finite-state decoding mechanism. Empirically, AR-Omni achieves strong quality across three modalities while remaining real-time, achieving a 0.88 real-time factor for speech generation.

</details>


### [116] [LLM-42: Enabling Determinism in LLM Inference with Verified Speculation](https://arxiv.org/abs/2601.17768)
*Raja Gond,Aditya K Kamath,Arkaprava Basu,Ramachandran Ramjee,Ashish Panwar*

Main category: cs.LG

TL;DR: LLM-42 is a scheduling-based approach that enables deterministic LLM inference via a verify-rollback mechanism, reusing existing kernels and incurring overhead only when determinism is required.


<details>
  <summary>Details</summary>
Motivation: LLM inference suffers from non-determinism due to floating-point non-associativity and dynamic batching, causing inconsistent outputs. Existing solutions either severely degrade throughput (disabling batching) or impose fixed overhead regardless of actual need (batch-invariant kernels).

Method: Inspired by speculative decoding, LLM-42 uses a two-path system: a non-deterministic fast path for token decoding and a lightweight verify-rollback loop. The verifier replays candidate tokens under a fixed-shape reduction schedule, committing those guaranteed consistent across runs and rolling back inconsistent ones. It leverages that sequences in consistent states likely produce consistent next tokens and that most GPU kernels use shape-consistent reductions.

Result: The approach mostly reuses existing kernels unchanged and incurs overhead only in proportion to the traffic that actually requires determinism, avoiding fixed runtime penalties.

Conclusion: LLM-42 provides an efficient, practical solution for deterministic LLM inference that minimizes performance impact while ensuring consistency where needed.

Abstract: In LLM inference, the same prompt may yield different outputs across different runs. At the system level, this non-determinism arises from floating-point non-associativity combined with dynamic batching and GPU kernels whose reduction orders vary with batch size. A straightforward way to eliminate non-determinism is to disable dynamic batching during inference, but doing so severely degrades throughput. Another approach is to make kernels batch-invariant; however, this tightly couples determinism to kernel design, requiring new implementations. This coupling also imposes fixed runtime overheads, regardless of how much of the workload actually requires determinism.
  Inspired by ideas from speculative decoding, we present LLM-42, a scheduling-based approach to enable determinism in LLM inference. Our key observation is that if a sequence is in a consistent state, the next emitted token is likely to be consistent even with dynamic batching. Moreover, most GPU kernels use shape-consistent reductions. Leveraging these insights, LLM-42 decodes tokens using a non-deterministic fast path and enforces determinism via a lightweight verify-rollback loop. The verifier replays candidate tokens under a fixed-shape reduction schedule, commits those that are guaranteed to be consistent across runs, and rolls back those violating determinism. LLM-42 mostly re-uses existing kernels unchanged and incurs overhead only in proportion to the traffic that requires determinism.

</details>


### [117] [MergeMix: Optimizing Mid-Training Data Mixtures via Learnable Model Merging](https://arxiv.org/abs/2601.17858)
*Jiapeng Wang,Changxin Tian,Kunlong Chen,Ziqi Liu,Jiaxin Mao,Wayne Xin Zhao,Zhiqiang Zhang,Jun Zhou*

Main category: cs.LG

TL;DR: MergeMix introduces a novel method to efficiently optimize data mixing ratios for large language models by using model merging weights as a low-cost performance proxy, achieving comparable results to manual tuning with drastically reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: Current methods for optimizing data mixtures in LLMs rely on computationally expensive heuristic trials or proxy training, making the process prohibitively costly.

Method: MergeMix trains domain-specific experts on minimal tokens and optimizes their merging weights against downstream benchmarks to serve as a high-fidelity, low-cost performance proxy for data mixture optimization.

Result: Experiments show MergeMix achieves performance comparable to or surpassing exhaustive manual tuning while drastically reducing search costs, with high rank consistency (Spearman ρ>0.9) and strong cross-scale transferability.

Conclusion: MergeMix provides a scalable, automated solution for data mixture optimization, effectively balancing performance and computational efficiency without full-scale training.

Abstract: Optimizing data mixtures is essential for unlocking the full potential of large language models (LLMs), yet identifying the optimal composition remains computationally prohibitive due to reliance on heuristic trials or expensive proxy training. To address this, we introduce \textbf{MergeMix}, a novel approach that efficiently determines optimal data mixing ratios by repurposing model merging weights as a high-fidelity, low-cost performance proxy. By training domain-specific experts on minimal tokens and optimizing their merging weights against downstream benchmarks, MergeMix effectively optimizes the performance of data mixtures without incurring the cost of full-scale training. Extensive experiments on models with 8B and 16B parameters validate that MergeMix achieves performance comparable to or surpassing exhaustive manual tuning while drastically reducing search costs. Furthermore, MergeMix exhibits high rank consistency (Spearman $ρ> 0.9$) and strong cross-scale transferability, offering a scalable, automated solution for data mixture optimization.

</details>


### [118] [EEG Foundation Models: Progresses, Benchmarking, and Open Problems](https://arxiv.org/abs/2601.17883)
*Dingkun Liu,Yuheng Chen,Zhu Chen,Zhenyao Cui,Yaozhi Wen,Jiayu An,Jingwei Luo,Dongrui Wu*

Main category: cs.LG

TL;DR: 本研究系统评估了12个开源EEG基础模型在13个数据集上的表现，发现线性探测不足、专家模型仍具竞争力，且模型规模不一定带来更好性能。


<details>
  <summary>Details</summary>
Motivation: 由于预训练目标、预处理和评估协议不一致，现有EEG基础模型缺乏公平全面的比较，限制了对模型真实性能的评估。

Method: 首先回顾50个代表性模型并构建统一分类框架，然后评估12个开源基础模型在13个EEG数据集、9种BCI范式上的表现，重点考察跨被试泛化和少样本校准能力，并对比全参数微调与线性探测。

Result: 1) 线性探测经常不足；2) 从头训练的专家模型在许多任务上仍具竞争力；3) 在当前数据和训练范式下，更大的基础模型不一定带来更好的泛化性能。

Conclusion: 该研究填补了EEG基础模型系统性评估的空白，揭示了当前基础模型的局限性，为未来BCI研究提供了重要参考。

Abstract: Electroencephalography (EEG) foundation models have recently emerged as a promising paradigm for brain-computer interfaces (BCIs), aiming to learn transferable neural representations from large-scale heterogeneous recordings. Despite rapid progresses, there lacks fair and comprehensive comparisons of existing EEG foundation models, due to inconsistent pre-training objectives, preprocessing choices, and downstream evaluation protocols. This paper fills this gap. We first review 50 representative models and organize their design choices into a unified taxonomic framework including data standardization, model architectures, and self-supervised pre-training strategies. We then evaluate 12 open-source foundation models and competitive specialist baselines across 13 EEG datasets spanning nine BCI paradigms. Emphasizing real-world deployments, we consider both cross-subject generalization under a leave-one-subject-out protocol and rapid calibration under a within-subject few-shot setting. We further compare full-parameter fine-tuning with linear probing to assess the transferability of pre-trained representations, and examine the relationship between model scale and downstream performance. Our results indicate that: 1) linear probing is frequently insufficient; 2) specialist models trained from scratch remain competitive across many tasks; and, 3) larger foundation models do not necessarily yield better generalization performance under current data regimes and training practices.

</details>


### [119] [Adaptive Weighting in Knowledge Distillation: An Axiomatic Framework for Multi-Scale Teacher Ensemble Optimization](https://arxiv.org/abs/2601.17910)
*Aaron R. Flouro,Shawn P. Chadwick*

Main category: cs.LG

TL;DR: 提出一个与算子无关的公理化框架来解决多教师知识蒸馏中的自适应加权问题，在token、任务和上下文三个尺度上建立理论基础，实现了从启发式方法到可证明理论的跨越。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖启发式或特定实现的加权策略，缺乏统一理论指导，无法保证在异质性、分布偏移和安全约束下的性能，亟需形式化框架来解耦理论保证与具体公式。

Method: 开发算子无关的公理化框架，形式化自适应加权算子的结构条件，支持token/task/context三尺度分析，通过乘积结构归一化实现层次化组合，并基于标准假设分析梯度优化收敛性。

Result: 证明了一致算子的存在性与非唯一性，建立了梯度优化的收敛性保证，分析了稳定性和扰动鲁棒性，并给出了安全约束蒸馏的抽象表述，实现了理论保证与具体实现的分离。

Conclusion: 该框架为多教师知识蒸馏提供了原则性分析工具，使理论保证独立于具体加权公式，能够系统评估异质性、分布偏移和安全约束等场景下的方法性能。

Abstract: Knowledge distillation with multiple teachers is increasingly used to improve robustness, efficiency, and safety, yet existing approaches rely largely on heuristic or implementation-specific weighting schemes. This paper develops an operator-agnostic axiomatic framework for adaptive weighting in multi-teacher knowledge distillation across three complementary scales: token, task, and context. We formalize structural conditions under which adaptive weighting operators are well-defined, admit multiple non-equivalent implementations, and can be hierarchically composed via product-structure normalization. Within this framework, we establish existence and non-uniqueness of conforming operators, characterize convergence of gradient-based optimization under standard assumptions, analyze stability and perturbation robustness, and provide an abstract formulation of safety-constrained distillation. The results decouple theoretical guarantees from specific weighting formulas, enabling principled analysis of adaptive distillation methods under heterogeneity, distribution shift, and safety constraints.

</details>


### [120] [Causal Pre-training Under the Fairness Lens: An Empirical Study of TabPFN](https://arxiv.org/abs/2601.17912)
*Qinyi Liu,Mohammad Khalil,Naman Goel*

Main category: cs.LG

TL;DR: 评估基于因果推理预训练的表格基础模型TabPFN的公平性表现，发现其预测准确性和鲁棒性优秀，但公平性改善有限且不稳健，尤其在MNAR分布偏移下。


<details>
  <summary>Details</summary>
Motivation: 表格基础模型(如TabPFN)通过因果模型预训练获得强大预测能力，但其公平性属性尚未被充分探索，而这对实际应用部署至关重要。

Method: 对TabPFN及其微调变体进行综合实证评估，系统测试不同数据规模和分布偏移下的预测性能、公平性和鲁棒性指标。

Result: TabPFN预测准确性强且对虚假相关鲁棒，但公平性提升有限且不一致，在不可随机缺失(MNAR)协变量偏移下表现尤其不佳。

Conclusion: 因果预训练对算法公平性有帮助但不足够，实际部署需额外公平性干预措施，为后续研究方向提供启示。

Abstract: Foundation models for tabular data, such as the Tabular Prior-data Fitted Network (TabPFN), are pre-trained on a massive number of synthetic datasets generated by structural causal models (SCM). They leverage in-context learning to offer high predictive accuracy in real-world tasks. However, the fairness properties of these foundational models, which incorporate ideas from causal reasoning during pre-training, have not yet been explored in sufficient depth. In this work, we conduct a comprehensive empirical evaluation of TabPFN and its fine-tuned variants, assessing predictive performance, fairness, and robustness across varying dataset sizes and distributional shifts. Our results reveal that while TabPFN achieves stronger predictive accuracy compared to baselines and exhibits robustness to spurious correlations, improvements in fairness are moderate and inconsistent, particularly under missing-not-at-random (MNAR) covariate shifts. These findings suggest that the causal pre-training in TabPFN is helpful but insufficient for algorithmic fairness, highlighting implications for deploying such models in practice and the need for further fairness interventions.

</details>


### [121] [UniPACT: A Multimodal Framework for Prognostic Question Answering on Raw ECG and Structured EHR](https://arxiv.org/abs/2601.17916)
*Jialu Tang,Tong Xia,Yuan Lu,Aaqib Saeed*

Main category: cs.LG

TL;DR: UniPACT框架通过结构化提示机制将数值型EHR数据转化为文本，并与ECG波形表征融合，使LLM能统一处理异构临床数据，在MDS-ED基准上实现89.37%的SOTA AUROC，显著提升多任务预后预测性能


<details>
  <summary>Details</summary>
Motivation: 临床预后需整合结构化电子健康记录(EHR)与实时生理信号(如ECG)，但大型语言模型(LLM)难以原生处理此类异构非文本数据

Method: 提出统一预后问答框架UniPACT：1) 结构化提示机制将数值EHR转为语义丰富的文本；2) 将文本化患者上下文与原始ECG波形学习表征融合；3) 使LLM能整体推理双模态数据

Result: 在MDS-ED基准测试中，以89.37%平均AUROC超越专用基线，在诊断、病情恶化、ICU转入和死亡等多样预后任务中实现当前最佳性能

Conclusion: 多模态多任务方法对性能至关重要，且在数据缺失场景下提供鲁棒性，证明统一处理异构临床数据的可行性

Abstract: Accurate clinical prognosis requires synthesizing structured Electronic Health Records (EHRs) with real-time physiological signals like the Electrocardiogram (ECG). Large Language Models (LLMs) offer a powerful reasoning engine for this task but struggle to natively process these heterogeneous, non-textual data types. To address this, we propose UniPACT (Unified Prognostic Question Answering for Clinical Time-series), a unified framework for prognostic question answering that bridges this modality gap. UniPACT's core contribution is a structured prompting mechanism that converts numerical EHR data into semantically rich text. This textualized patient context is then fused with representations learned directly from raw ECG waveforms, enabling an LLM to reason over both modalities holistically. We evaluate UniPACT on the comprehensive MDS-ED benchmark, it achieves a state-of-the-art mean AUROC of 89.37% across a diverse set of prognostic tasks including diagnosis, deterioration, ICU admission, and mortality, outperforming specialized baselines. Further analysis demonstrates that our multimodal, multi-task approach is critical for performance and provides robustness in missing data scenarios.

</details>


### [122] [treaming-dLLM: Accelerating Diffusion LLMs via Suffix Pruning and Dynamic Decoding](https://arxiv.org/abs/2601.17917)
*Zhongyu Xiao,Zhiwei Hao,Jianyuan Guo,Yong Luo,Jia Liu,Jie Xu,Han Hu*

Main category: cs.LG

TL;DR: Streaming-dLLM是一个无需重训练的推理优化框架，通过空间维度剪枝冗余token和时间维度动态提前退出，将扩散大语言模型推理速度提升高达68.2倍。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散大语言模型（dLLMs）凭借并行解码和双向注意力在全局连贯性上优于自回归模型，但现有加速方法忽视了块式扩散过程内在的低效性：空间上对信息稀疏的后缀区域进行均匀建模造成冗余，时间上使用固定的去噪调度导致效率低下。

Method: 提出Streaming-dLLM，一个无需训练的框架，从空间和时间双维度优化推理。空间层面采用衰减引导的后缀建模，通过剪枝冗余掩码token来近似完整上下文；时间层面采用动态置信度感知策略与提前退出机制，使模型能够跳过已收敛token的不必要迭代。

Result: 在保持生成质量的前提下，实现了高达68.2倍的推理速度提升。

Conclusion: 该框架有效解决了扩散解码的效率瓶颈，在显著加速的同时保持了生成质量，为扩散语言模型的实用化提供了有效路径。

Abstract: Diffusion Large Language Models (dLLMs) offer a compelling paradigm for natural language generation, leveraging parallel decoding and bidirectional attention to achieve superior global coherence compared to autoregressive models. While recent works have accelerated inference via KV cache reuse or heuristic decoding, they overlook the intrinsic inefficiencies within the block-wise diffusion process. Specifically, they suffer from spatial redundancy by modeling informative-sparse suffix regions uniformly and temporal inefficiency by applying fixed denoising schedules across all the decoding process. To address this, we propose Streaming-dLLM, a training-free framework that streamlines inference across both spatial and temporal dimensions. Spatially, we introduce attenuation guided suffix modeling to approximate the full context by pruning redundant mask tokens. Temporally, we employ a dynamic confidence aware strategy with an early exit mechanism, allowing the model to skip unnecessary iterations for converged tokens. Extensive experiments show that Streaming-dLLM achieves up to 68.2X speedup while maintaining generation quality, highlighting its effectiveness in diffusion decoding. The code is available at https://github.com/xiaoshideta/Streaming-dLLM.

</details>


### [123] [Dissipative Learning: A Framework for Viable Adaptive Systems](https://arxiv.org/abs/2601.17933)
*Laurent Caraffa*

Main category: cs.LG

TL;DR: 将学习重新定义为受耗散约束的压缩信念状态演化过程，提出BEDS框架证明Fisher-Rao正则化是唯一热力学最优策略，统一现有方法并区分两类学习问题


<details>
  <summary>Details</summary>
Motivation: 挑战传统学习观，论证遗忘和正则化是自适应系统的结构性需求而非启发式附加，从物理学第一性原理重构学习理论

Method: 基于信息论、热力学和信息几何学，构建BEDS（贝叶斯涌现耗散结构）框架，将学习建模为耗散约束下的信念状态演化

Result: 证明Fisher-Rao正则化是唯一热力学最优策略；统一Ridge、SIGReg等方法为特例；提出过拟合=过结晶化、灾难性遗忘=耗散不足的新解释；区分结晶化问题与维持性问题的本质差异

Conclusion: 学习本质是在耗散约束下维持可行信念状态的过程，用稳定性替代渐进最优性作为核心判据，为持续学习和多智能体系统提供新范式

Abstract: We propose a perspective in which learning is an intrinsically dissipative process. Forgetting and regularization are not heuristic add-ons but structural requirements for adaptive systems. Drawing on information theory, thermodynamics, and information geometry, we introduce the BEDS (Bayesian Emergent Dissipative Structures) framework, modeling learning as the evolution of compressed belief states under dissipation constraints.
  A central contribution is the Conditional Optimality Theorem, showing that Fisher-Rao regularization measuring change via information divergence rather than Euclidean distance is the unique thermodynamically optimal regularization strategy, achieving minimal dissipation. Euclidean regularization is shown to be structurally suboptimal. The framework unifies existing methods (Ridge, SIGReg, EMA, SAC) as special cases of a single governing equation.
  Within this view, overfitting corresponds to over-crystallization, while catastrophic forgetting reflects insufficient dissipation control. The framework distinguishes BEDS-crystallizable problems, where beliefs converge to stable equilibria, from BEDS-maintainable problems, which require continual adaptation. It extends naturally to continual and multi-agent systems, where viability, stability under adaptation and finite resources replaces asymptotic optimality as the primary criterion. Overall, this work reframes learning as maintaining viable belief states under dissipation constraints, providing a principled lens on forgetting, regularization, and stability.

</details>


### [124] [FedGraph-VASP: Privacy-Preserving Federated Graph Learning with Post-Quantum Security for Cross-Institutional Anti-Money Laundering](https://arxiv.org/abs/2601.17935)
*Daniel Commey,Matilda Nkoom,Yousef Alsenani,Sena G. Hounsinou,Garth V. Crosby*

Main category: cs.LG

TL;DR: FedGraph-VASP is a privacy-preserving federated graph learning framework that enables collaborative anti-money laundering across VASPs without exposing raw user data, using boundary embedding exchange with post-quantum cryptography.


<details>
  <summary>Details</summary>
Motivation: VASPs face a tension between regulatory compliance and user privacy in detecting cross-institutional money laundering, as current approaches either require sharing sensitive data or operate in isolation missing cross-chain patterns.

Method: Proposes FedGraph-VASP with a Boundary Embedding Exchange protocol that shares only compressed, non-invertible graph neural network representations of boundary accounts, secured by Kyber-512 and AES-256-GCM encryption.

Result: Achieves F1-score of 0.508 on Bitcoin fraud detection (12.1% improvement over FedSage+), approaches centralized performance in high-connectivity regimes, but shows topology-dependent trade-offs with generative methods on sparse graphs.

Conclusion: The framework effectively enables collaborative AML while preserving privacy, though its performance depends on transaction graph topology - embedding exchange benefits connected graphs while generative imputation excels in modular sparse graphs.

Abstract: Virtual Asset Service Providers (VASPs) face a fundamental tension between regulatory compliance and user privacy when detecting cross-institutional money laundering. Current approaches require either sharing sensitive transaction data or operating in isolation, leaving critical cross-chain laundering patterns undetected. We present FedGraph-VASP, a privacy-preserving federated graph learning framework that enables collaborative anti-money laundering (AML) without exposing raw user data. Our key contribution is a Boundary Embedding Exchange protocol that shares only compressed, non-invertible graph neural network representations of boundary accounts. These exchanges are secured using post-quantum cryptography, specifically the NIST-standardized Kyber-512 key encapsulation mechanism combined with AES-256-GCM authenticated encryption. Experiments on the Elliptic Bitcoin dataset with realistic Louvain partitioning show that FedGraph-VASP achieves an F1-score of 0.508, outperforming the state-of-the-art generative baseline FedSage+ (F1 = 0.453) by 12.1 percent on binary fraud detection. We further show robustness under low-connectivity settings where generative imputation degrades performance, while approaching centralized performance (F1 = 0.620) in high-connectivity regimes. We additionally evaluate generalization on an Ethereum fraud detection dataset, where FedGraph-VASP (F1 = 0.635) is less effective under sparse cross-silo connectivity, while FedSage+ excels (F1 = 0.855), outperforming even local training (F1 = 0.785). These results highlight a topology-dependent trade-off: embedding exchange benefits connected transaction graphs, whereas generative imputation can dominate in highly modular sparse graphs. A privacy audit shows embeddings are only partially invertible (R^2 = 0.32), limiting exact feature recovery.

</details>


### [125] [TensorLens: End-to-End Transformer Analysis via High-Order Attention Tensors](https://arxiv.org/abs/2601.17958)
*Ido Andrew Atad,Itamar Zimerman,Shahar Katz,Lior Wolf*

Main category: cs.LG

TL;DR: TensorLens is a novel formulation that represents the entire transformer model as a single input-dependent linear operator using a high-order attention-interaction tensor, enabling better interpretability and model understanding.


<details>
  <summary>Details</summary>
Motivation: Existing analyses of attention matrices focus on individual heads or layers, failing to capture the transformer's global behavior. Prior work lacks a unified representation that encompasses all transformer components.

Method: The authors introduce TensorLens, which captures the entire transformer as a single linear operator expressed through a high-order attention-interaction tensor that jointly encodes attention, FFNs, activations, normalizations, and residual connections.

Result: Empirical validation shows TensorLens yields richer representations than previous attention-aggregation methods and serves as an effective foundation for interpretability tools.

Conclusion: TensorLens provides a theoretically coherent and expressive linear representation of transformer computation, offering a powerful framework for advancing interpretability research and model understanding.

Abstract: Attention matrices are fundamental to transformer research, supporting a broad range of applications including interpretability, visualization, manipulation, and distillation. Yet, most existing analyses focus on individual attention heads or layers, failing to account for the model's global behavior. While prior efforts have extended attention formulations across multiple heads via averaging and matrix multiplications or incorporated components such as normalization and FFNs, a unified and complete representation that encapsulates all transformer blocks is still lacking. We address this gap by introducing TensorLens, a novel formulation that captures the entire transformer as a single, input-dependent linear operator expressed through a high-order attention-interaction tensor. This tensor jointly encodes attention, FFNs, activations, normalizations, and residual connections, offering a theoretically coherent and expressive linear representation of the model's computation. TensorLens is theoretically grounded and our empirical validation shows that it yields richer representations than previous attention-aggregation methods. Our experiments demonstrate that the attention tensor can serve as a powerful foundation for developing tools aimed at interpretability and model understanding. Our code is attached as a supplementary.

</details>


### [126] [Federated learning for unpaired multimodal data through a homogeneous transformer model](https://arxiv.org/abs/2601.17986)
*Anders Eklund*

Main category: cs.LG

TL;DR: 提出联邦无配对多模态基础模型训练框架，通过公共锚点集和核对齐实现跨节点语义对齐，无需共享原始数据，解决数据孤岛和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 现实联邦环境中数据常为未配对、跨节点碎片化且严格私有，现有方法依赖配对数据或共享特征，违反数据主权，无法适用。

Method: 引入小规模公共锚点集，利用Gram矩阵和中心化核对齐实现跨模态语义对齐；提出子空间稳定微调解耦域特定幅度变化与语义方向；采用精度加权平均通过不确定性估计降权不确定节点。

Result: 建立了联邦无配对基础模型的数学基础，使全局模型能从碎片化、互斥且私有的数据孤岛中学习统一世界表征，无需中心化存储或配对样本。

Conclusion: 该框架为多模态基础模型提供了数学上更优的隐私保障，实现了在严格数据主权下的联邦学习，推动了隐私保护与分布式AI的发展。

Abstract: Training of multimodal foundation models is currently restricted to centralized data centers containing massive, aligned datasets (e.g., image-text pairs). However, in realistic federated environments, data is often unpaired and fragmented across disjoint nodes; one node may hold sensor data, while another holds textual logs. These datasets are strictly private and share no common samples. Current federated learning (FL) methods fail in this regime, as they assume local clients possess aligned pairs or require sharing raw feature embeddings, which violates data sovereignty. We propose a novel framework to train a global multimodal transformer across decentralized nodes with disjoint modalities. We introduce a small public anchor set to align disjoint private manifolds. Using Gram matrices calculated from these public anchors, we enforce semantic alignment across modalities through centered kernel alignment without ever transmitting private samples, offering a mathematically superior privacy guarantee compared to prototype sharing. Further, we introduce a subspace-stabilized fine-tuning method to handle FL with huge transformer models. We strictly decouple domain-specific magnitude shifts from semantic direction, ensuring that nodes with varying sensor characteristics align geometrically to the global consensus. Lastly, we propose precision weighted averaging, where efficiently obtained uncertainty estimates are used to downweight uncertain nodes. This paper establishes the mathematical backbone for federated unpaired foundation models, enabling a global model to learn a unified representation of the world from fragmented, disjoint, and private data silos without requiring centralized storage or paired samples.

</details>


### [127] [Coding-Enforced Resilient and Secure Aggregation for Hierarchical Federated Learning](https://arxiv.org/abs/2601.17995)
*Shudi Weng,Ming Xiao,Mikael Skoglund*

Main category: cs.LG

TL;DR: 本文提出H-SecCoGC，一种用于联邦学习的鲁棒分层安全聚合方案，通过编码策略在不可靠通信条件下保持模型精度和隐私性。


<details>
  <summary>Details</summary>
Motivation: 在分层联邦学习（HFL）中，如何在不可靠通信情况下确保模型精度同时保护隐私仍是关键挑战，因为隐私噪声的协调可能被随机破坏。

Method: 提出一种鲁棒的分层安全聚合方案H-SecCoGC，集成编码策略以实现结构化聚合。

Result: 该方案在不同隐私级别下确保准确的全局模型构建，避免部分参与问题，显著提升鲁棒性、隐私保护和学习效率。理论分析和实验结果均验证其在不可靠通信和任意强隐私保证下的优越性。

Conclusion: H-SecCoGC方案有效解决了HFL在不可靠通信下的局限性，在保持模型精度和效率的同时提供强隐私保证。

Abstract: Hierarchical federated learning (HFL) has emerged as an effective paradigm to enhance link quality between clients and the server. However, ensuring model accuracy while preserving privacy under unreliable communication remains a key challenge in HFL, as the coordination among privacy noise can be randomly disrupted. To address this limitation, we propose a robust hierarchical secure aggregation scheme, termed H-SecCoGC, which integrates coding strategies to enforce structured aggregation. The proposed scheme not only ensures accurate global model construction under varying levels of privacy, but also avoids the partial participation issue, thereby significantly improving robustness, privacy preservation, and learning efficiency. Both theoretical analyses and experimental results demonstrate the superiority of our scheme under unreliable communication across arbitrarily strong privacy guarantees

</details>


### [128] [Spelling Bee Embeddings for Language Modeling](https://arxiv.org/abs/2601.18030)
*Markus N. Rabe,Judith Clymo,Zheren Dong*

Main category: cs.LG

TL;DR: 提出一种简单的嵌入层修改，通过为token嵌入注入拼写信息，提升模型在拼写任务和基准测试上的性能，可实现约8%的计算和数据节省。


<details>
  <summary>Details</summary>
Motivation: 标准token嵌入缺乏拼写信息，限制了模型在拼写相关任务及通用语言理解上的性能。通过嵌入拼写特征，有望提升模型效率和表现。

Method: 对嵌入层进行简单修改，核心是为token嵌入注入拼写信息。

Result: 1) 模型在拼写任务和标准基准测试上均有提升；2) 40M-800M参数规模的扩展研究表明，达到相同测试损失可减少约8%的计算和数据需求。

Conclusion: 在嵌入层中融入拼写信息是一种简单有效的改进方法，能提升性能并降低资源消耗，具有显著的效率优势。

Abstract: We introduce a simple modification to the embedding layer. The key change is to infuse token embeddings with information about their spelling. Models trained with these embeddings improve not only on spelling, but also across standard benchmarks. We conduct scaling studies for models with 40M to 800M parameters, which suggest that the improvements are equivalent to needing about 8% less compute and data to achieve the same test loss.

</details>


### [129] [Multimodal Machine Learning for Soft High-k Elastomers under Data Scarcity](https://arxiv.org/abs/2601.18032)
*Brijesh FNU,Viet Thanh Duy Nguyen,Ashima Sharma,Md Harun Rashid Molla,Chengyi Xu,Truong-Son Hy*

Main category: cs.LG

TL;DR: Researchers developed a multimodal ML framework using pretrained polymer models to predict dielectric and mechanical properties of acrylate elastomers from molecular sequences, enabling few-shot learning despite limited data.


<details>
  <summary>Details</summary>
Motivation: Soft electronics require dielectric elastomers with both high dielectric constants (k) and low Young's moduli (E), but no structured datasets exist linking molecular sequences to these properties, hindering systematic development.

Method: Curated a compact dataset of acrylate-based dielectric elastomers from literature, then built a multimodal learning framework leveraging graph- and sequence-based pretrained polymer encoders for few-shot property prediction.

Result: The model accurately predicts dielectric and mechanical properties from molecular sequences, demonstrating successful knowledge transfer from large-scale polymer corpora to overcome data scarcity.

Conclusion: This pretrained multimodal approach provides a generalizable paradigm for accelerating discovery of high-performance dielectric elastomers across different polymer backbones.

Abstract: Dielectric materials are critical building blocks for modern electronics such as sensors, actuators, and transistors. With the rapid recent advance in soft and stretchable electronics for emerging human- and robot-interfacing applications, there is a surging need for high-performance dielectric elastomers. However, it remains a grand challenge to develop soft elastomers that simultaneously possess high dielectric constants (k, related to energy storage capacity) and low Young's moduli (E, related to mechanical flexibility). While some new elastomer designs have been reported in individual (mostly one-off) studies, almost no structured dataset is currently available for dielectric elastomers that systematically encompasses their molecular sequence, dielectric, and mechanical properties. Within this context, we curate a compact, high-quality dataset of acrylate-based dielectric elastomers, one of the most widely explored elastomer backbones due to its versatile chemistry and molecular design flexibility, by screening and aggregating experimental results from the literature over the past 10 years. Building on this dataset, we propose a multimodal learning framework that leverages large-scale pretrained polymer representations from graph- and sequence-based encoders. These pretrained embeddings transfer rich chemical and structural knowledge from vast polymer corpora, enabling accurate few-shot prediction of both dielectric and mechanical properties from molecular sequences. Our results represent a new paradigm for transferring knowledge from pretrained multimodal models to overcome severe data scarcity, which can be readily translated to other polymer backbones (e.g., silicones, urethanes) and thus accelerate data-efficient discovery of soft high-k dielectric elastomers. Our source code and dataset are publicly available at https://github.com/HySonLab/Polymers

</details>


### [130] [Resonant Sparse Geometry Networks](https://arxiv.org/abs/2601.18064)
*Hasi Hays*

Main category: cs.LG

TL;DR: RSGN is a brain-inspired neural architecture that uses self-organizing sparse connectivity in hyperbolic space with dual-timescale learning, achieving O(n*k) complexity and using 15x fewer parameters than Transformers while maintaining high accuracy on long-range dependency and hierarchical classification tasks.


<details>
  <summary>Details</summary>
Motivation: Transformers' dense attention mechanisms have O(n²) computational complexity, which is inefficient. The paper seeks more efficient, biologically plausible architectures inspired by the brain's sparse, hierarchical connectivity patterns.

Method: RSGN embeds nodes in learned hyperbolic space where connection strength decays with geodesic distance. It operates on two timescales: fast gradient-based activation propagation and slow Hebbian-inspired structural learning via local correlation rules for dynamic sparsity adaptation.

Result: RSGN achieves 96.5% accuracy on long-range dependency tasks with ~15x fewer parameters than Transformers. On 20-class hierarchical classification, it reaches 23.8% accuracy (vs 5% random) using only 41,672 parameters—nearly 10x fewer than Transformers' 403,348 parameters for 30.1% accuracy. Mathematical analysis confirms O(n*k) complexity.

Conclusion: Brain-inspired sparse, geometrically-organized computation offers a promising path toward more efficient and biologically plausible neural architectures compared to dense Transformer models.

Abstract: We introduce Resonant Sparse Geometry Networks (RSGN), a brain-inspired architecture with self-organizing sparse
  hierarchical input-dependent connectivity. Unlike Transformer architectures that employ dense attention mechanisms with
  O(n^2) computational complexity, RSGN embeds computational nodes in learned hyperbolic space where connection strength
  decays with geodesic distance, achieving dynamic sparsity that adapts to each input. The architecture operates on two
  distinct timescales: fast differentiable activation propagation optimized through gradient descent, and slow
  Hebbian-inspired structural learning for connectivity adaptation through local correlation rules. We provide rigorous
  mathematical analysis demonstrating that RSGN achieves O(n*k) computational complexity, where k << n represents the average
  active neighborhood size. Experimental evaluation on hierarchical classification and long-range dependency tasks
  demonstrates that RSGN achieves 96.5% accuracy on long-range dependency tasks while using approximately 15x fewer
  parameters than standard Transformers. On challenging hierarchical classification with 20 classes, RSGN achieves 23.8%
  accuracy (compared to 5% random baseline) with only 41,672 parameters, nearly 10x fewer than the Transformer baselines
  which require 403,348 parameters to achieve 30.1% accuracy. Our ablation studies confirm the contribution of each architectural
  component, with Hebbian learning providing consistent improvements. These results suggest that brain-inspired principles
  of sparse, geometrically-organized computation offer a promising direction toward more efficient and biologically plausible
  neural architectures.

</details>


### [131] [Comparison requires valid measurement: Rethinking attack success rate comparisons in AI red teaming](https://arxiv.org/abs/2601.18076)
*Alexandra Chouldechova,A. Feder Cooper,Solon Barocas,Abhinav Palia,Dan Vann,Hanna Wallach*

Main category: cs.LG

TL;DR: This paper critiques AI red teaming practices, arguing that attack success rate (ASR) comparisons often lack validity due to inappropriate "apples-to-oranges" comparisons and low-validity measurements. Using social science measurement theory and inferential statistics, it establishes conditions for when ASRs can be meaningfully compared, with jailbreaking as a key example.


<details>
  <summary>Details</summary>
Motivation: Many conclusions about AI system safety and attack method efficacy are drawn from ASR comparisons that are methodologically flawed, potentially leading to misleading claims about relative safety and vulnerability.

Method: The authors apply concepts from social science measurement theory and inferential statistics to develop a conceptual framework for evaluating when numerical values from system attribute quantification can be meaningfully compared, combining conceptual, theoretical, and empirical analysis.

Result: The paper identifies specific conditions under which ASRs can and cannot be meaningfully compared, and provides extensive examples of invalid comparisons in AI red teaming, demonstrating common measurement validity challenges in jailbreaking scenarios.

Conclusion: Current ASR comparison practices in AI red teaming often lack scientific rigor; meaningful safety assessments require careful consideration of measurement validity and comparability conditions to avoid drawing unsupported conclusions about system safety.

Abstract: We argue that conclusions drawn about relative system safety or attack method efficacy via AI red teaming are often not supported by evidence provided by attack success rate (ASR) comparisons. We show, through conceptual, theoretical, and empirical contributions, that many conclusions are founded on apples-to-oranges comparisons or low-validity measurements. Our arguments are grounded in asking a simple question: When can attack success rates be meaningfully compared? To answer this question, we draw on ideas from social science measurement theory and inferential statistics, which, taken together, provide a conceptual grounding for understanding when numerical values obtained through the quantification of system attributes can be meaningfully compared. Through this lens, we articulate conditions under which ASRs can and cannot be meaningfully compared. Using jailbreaking as a running example, we provide examples and extensive discussion of apples-to-oranges ASR comparisons and measurement validity challenges.

</details>


### [132] [DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal](https://arxiv.org/abs/2601.18081)
*Peixuan Han,Yingjie Yu,Jingjun Xu,Jiaxuan You*

Main category: cs.LG

TL;DR: This paper proposes DRPG, an agentic framework that automatically generates academic rebuttals through four steps: decomposing reviews, retrieving evidence, planning strategies, and generating responses. Using only an 8B model, it outperforms existing methods and exceeds average human performance, with its planner achieving 98% accuracy in identifying effective rebuttal directions.


<details>
  <summary>Details</summary>
Motivation: Automated academic rebuttal support remains underexplored despite growing LLM adoption in scientific research. Existing approaches using off-the-shelf LLMs or simple pipelines struggle with long-context understanding and fail to produce targeted, persuasive responses needed for peer review communication.

Method: DRPG is a four-step agentic framework: (1) Decompose reviews into atomic concerns, (2) Retrieve relevant evidence from the paper, (3) Plan rebuttal strategies with a specialized Planner component, and (4) Generate targeted responses. The framework operates using an 8B parameter model.

Result: The Planner achieves over 98% accuracy in identifying feasible rebuttal directions. DRPG significantly outperforms existing rebuttal pipelines and exceeds average human performance levels. It demonstrates effectiveness in multi-round settings and provides multi-perspective, explainable suggestions.

Conclusion: DRPG effectively automates high-quality academic rebuttal generation, showing potential to scale academic discussions and provide valuable support for researchers navigating peer review processes. The framework's success with a relatively small 8B model suggests efficient, practical applicability.

Abstract: Despite the growing adoption of large language models (LLMs) in scientific research workflows, automated support for academic rebuttal, a crucial step in academic communication and peer review, remains largely underexplored. Existing approaches typically rely on off-the-shelf LLMs or simple pipelines, which struggle with long-context understanding and often fail to produce targeted and persuasive responses. In this paper, we propose DRPG, an agentic framework for automatic academic rebuttal generation that operates through four steps: Decompose reviews into atomic concerns, Retrieve relevant evidence from the paper, Plan rebuttal strategies, and Generate responses accordingly. Notably, the Planner in DRPG reaches over 98% accuracy in identifying the most feasible rebuttal direction. Experiments on data from top-tier conferences demonstrate that DRPG significantly outperforms existing rebuttal pipelines and achieves performance beyond the average human level using only an 8B model. Our analysis further demonstrates the effectiveness of the planner design and its value in providing multi-perspective and explainable suggestions. We also showed that DRPG works well in a more complex multi-round setting. These results highlight the effectiveness of DRPG and its potential to provide high-quality rebuttal content and support the scaling of academic discussions. Codes for this work are available at https://github.com/ulab-uiuc/DRPG-RebuttalAgent.

</details>


### [133] [LatentMoE: Toward Optimal Accuracy per FLOP and Parameter in Mixture of Experts](https://arxiv.org/abs/2601.18089)
*Venmugil Elango,Nidhi Bhatia,Roger Waleffe,Rasoul Shafipour,Tomer Asida,Abhinav Khattar,Nave Assaf,Maximilian Golub,Joey Guman,Tiyasa Mitra,Ritchie Zhao,Ritika Borkar,Ran Zilberstein,Mostofa Patwary,Mohammad Shoeybi,Bita Rouhani*

Main category: cs.LG

TL;DR: This paper introduces LatentMoE, a new Mixture of Experts architecture optimized for accuracy per compute unit, which outperforms standard MoEs and has been adopted in NVIDIA's Nemotron-3 models.


<details>
  <summary>Details</summary>
Motivation: Despite widespread adoption of MoEs in LLMs, it's unclear how close existing architectures are to optimal in terms of inference cost measured by accuracy per FLOP and per parameter.

Method: Hardware-software co-design approach, empirical and theoretical analysis of performance bottlenecks across deployment regimes, systematic design exploration with scales up to 95B parameters and 1T-token training.

Result: LatentMoE consistently outperforms standard MoE architectures in accuracy per FLOP and per parameter, validated through extensive empirical exploration and theoretical analysis.

Conclusion: LatentMoE has been successfully adopted by NVIDIA's Nemotron-3 Super and Ultra flagship models and scaled to larger regimes including longer token horizons and larger model sizes.

Abstract: Mixture of Experts (MoEs) have become a central component of many state-of-the-art open-source and proprietary large language models. Despite their widespread adoption, it remains unclear how close existing MoE architectures are to optimal with respect to inference cost, as measured by accuracy per floating-point operation and per parameter. In this work, we revisit MoE design from a hardware-software co-design perspective, grounded in empirical and theoretical considerations. We characterize key performance bottlenecks across diverse deployment regimes, spanning offline high-throughput execution and online, latency-critical inference. Guided by these insights, we introduce LatentMoE, a new model architecture resulting from systematic design exploration and optimized for maximal accuracy per unit of compute. Empirical design space exploration at scales of up to 95B parameters and over a 1T-token training horizon, together with supporting theoretical analysis, shows that LatentMoE consistently outperforms standard MoE architectures in terms of accuracy per FLOP and per parameter. Given its strong performance, the LatentMoE architecture has been adopted by the flagship Nemotron-3 Super and Ultra models and scaled to substantially larger regimes, including longer token horizons and larger model sizes, as reported in Nvidia et al. (arXiv:2512.20856).

</details>


### [134] [From LLMs to LRMs: Rethinking Pruning for Reasoning-Centric Models](https://arxiv.org/abs/2601.18091)
*Longwei Ding,Anhao Zhao,Fanghua Ye,Ziyang Chen,Xiaoyu Shen*

Main category: cs.LG

TL;DR: A controlled study reveals that pruning strategies must be tailored to model type (instruction-following vs reasoning-augmented) and task type: depth pruning works best for classification, width pruning for generation/reasoning, static pruning preserves reasoning, while dynamic pruning excels on classification/generation but struggles with long-chain reasoning.


<details>
  <summary>Details</summary>
Motivation: Large language models are increasingly costly to deploy, but it's unclear whether established pruning strategies for instruction-following models transfer to reasoning-augmented models that generate intermediate reasoning traces.

Method: Conducted a controlled study comparing pruning for instruction-following (LLM-instruct) and reasoning-augmented (LLM-think) models, aligning calibration/recovery data with original training distributions. Evaluated static depth pruning, static width pruning, and dynamic pruning across 17 classification, generation, and reasoning tasks.

Result: Found clear paradigm-dependent differences: depth pruning > width pruning for classification; width pruning more robust for generation/reasoning; static pruning better preserves reasoning; dynamic pruning excels on classification/generation but remains challenging for long-chain reasoning.

Conclusion: Pruning strategies must explicitly account for the distinct characteristics of reasoning-augmented LLMs, as one-size-fits-all approaches are ineffective.

Abstract: Large language models (LLMs) are increasingly costly to deploy, motivating extensive research on model pruning. However, most existing studies focus on instruction-following LLMs, leaving it unclear whether established pruning strategies transfer to reasoning-augmented models that explicitly generate long intermediate reasoning traces. In this work, we conduct a controlled study of pruning for both instruction-following ($\textbf{LLM-instruct}$) and reasoning-augmented ($\textbf{LLM-think}$) models. To isolate the effects of pruning, we align pruning calibration and post-pruning recovery data with each model's original training distribution, which we show yields more stable and reliable pruning behavior. We evaluate static depth pruning, static width pruning, and dynamic pruning across 17 tasks spanning classification, generation, and reasoning. Our results reveal clear paradigm-dependent differences: depth pruning outperforms width pruning on classification tasks, while width pruning is more robust for generation and reasoning. Moreover, static pruning better preserves reasoning performance, whereas dynamic pruning excels on classification and generation but remains challenging for long-chain reasoning. These findings underscore the need for pruning strategies that explicitly account for the distinct characteristics of reasoning-augmented LLMs. Our code is publicly available at https://github.com/EIT-NLP/LRM-Pruning.

</details>


### [135] [Beyond Static Datasets: Robust Offline Policy Optimization via Vetted Synthetic Transitions](https://arxiv.org/abs/2601.18107)
*Pedram Agand,Mo Chen*

Main category: cs.LG

TL;DR: MoReBRAC是一种基于模型的离线强化学习框架，通过不确定性感知的潜在空间合成，结合双循环世界模型和分层不确定性过滤来缓解分布偏移，在随机和次优数据上表现出显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在安全关键领域前景广阔，但静态数据集与学习策略之间的分布偏移问题迫使算法采取保守主义，限制了策略改进的潜力。

Method: 该框架采用双循环世界模型生成高保真合成转移，并通过集成变分自编码器流形检测、模型敏感性分析和蒙特卡洛Dropout的分层不确定性管道，确保只使用高置信度区域的转换。

Result: 在D4RL Gym-MuJoCo基准测试中取得显著性能提升，特别是在"随机"和"次优"数据 regime 中表现突出，并揭示了VAE作为几何锚点的作用及近优数据集的分布权衡。

Conclusion: 这种不确定性感知合成方法有效解决了离线强化学习中的分布偏移问题，使策略能够摆脱过度保守的限制，在具有挑战性的数据环境中获得更好性能，对实际安全关键应用具有重要意义。

Abstract: Offline Reinforcement Learning (ORL) holds immense promise for safety-critical domains like industrial robotics, where real-time environmental interaction is often prohibitive. A primary obstacle in ORL remains the distributional shift between the static dataset and the learned policy, which typically mandates high degrees of conservatism that can restrain potential policy improvements. We present MoReBRAC, a model-based framework that addresses this limitation through Uncertainty-Aware latent synthesis. Instead of relying solely on the fixed data, MoReBRAC utilizes a dual-recurrent world model to synthesize high-fidelity transitions that augment the training manifold. To ensure the reliability of this synthetic data, we implement a hierarchical uncertainty pipeline integrating Variational Autoencoder (VAE) manifold detection, model sensitivity analysis, and Monte Carlo (MC) dropout. This multi-layered filtering process guarantees that only transitions residing within high-confidence regions of the learned dynamics are utilized. Our results on D4RL Gym-MuJoCo benchmarks reveal significant performance gains, particularly in ``random'' and ``suboptimal'' data regimes. We further provide insights into the role of the VAE as a geometric anchor and discuss the distributional trade-offs encountered when learning from near-optimal datasets.

</details>


### [136] [AttenMIA: LLM Membership Inference Attack through Attention Signals](https://arxiv.org/abs/2601.18110)
*Pedram Zaree,Md Abdullah Al Mamun,Yue Dong,Ihsen Alouani,Nael Abu-Ghazaleh*

Main category: cs.LG

TL;DR: 提出AttenMIA框架，利用Transformer自注意力模式实现更有效的成员推理攻击，在低误报率指标上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型因训练数据记忆化引发严重隐私与知识产权风险，现有成员推理攻击依赖的置信度分数或嵌入特征脆弱且效果有限。

Method: 开发AttenMIA框架，挖掘模型内部自注意力模式，跨层聚合注意力头信息并结合扰动发散度度量训练MIA分类器。

Result: 在LLaMA-2、Pythia、OPT等开源模型上验证，注意力特征持续优于基线，WikiMIA-32基准达0.996 ROC AUC和87.9% TPR@1%FPR；注意力信号跨数据集和架构泛化，并提升数据提取攻击至SOTA水平。

Conclusion: 注意力机制在提升可解释性的同时意外放大了LLM隐私风险，凸显了设计新防御措施的紧迫性。

Abstract: Large Language Models (LLMs) are increasingly deployed to enable or improve a multitude of real-world applications. Given the large size of their training data sets, their tendency to memorize training data raises serious privacy and intellectual property concerns. A key threat is the membership inference attack (MIA), which aims to determine whether a given sample was included in the model's training set. Existing MIAs for LLMs rely primarily on output confidence scores or embedding-based features, but these signals are often brittle, leading to limited attack success. We introduce AttenMIA, a new MIA framework that exploits self-attention patterns inside the transformer model to infer membership. Attention controls the information flow within the transformer, exposing different patterns for memorization that can be used to identify members of the dataset. Our method uses information from attention heads across layers and combines them with perturbation-based divergence metrics to train an effective MIA classifier. Using extensive experiments on open-source models including LLaMA-2, Pythia, and Opt models, we show that attention-based features consistently outperform baselines, particularly under the important low-false-positive metric (e.g., achieving up to 0.996 ROC AUC & 87.9% TPR@1%FPR on the WikiMIA-32 benchmark with Llama2-13b). We show that attention signals generalize across datasets and architectures, and provide a layer- and head-level analysis of where membership leakage is most pronounced. We also show that using AttenMIA to replace other membership inference attacks in a data extraction framework results in training data extraction attacks that outperform the state of the art. Our findings reveal that attention mechanisms, originally introduced to enhance interpretability, can inadvertently amplify privacy risks in LLMs, underscoring the need for new defenses.

</details>


### [137] [Demystifying Data-Driven Probabilistic Medium-Range Weather Forecasting](https://arxiv.org/abs/2601.18111)
*Jean Kossaifi,Nikola Kovachki,Morteza Mardani,Daniel Leibovici,Suman Ravuri,Ira Shokar,Edoardo Calvello,Mohammad Shoaib Abbas,Peter Harrington,Ashay Subramaniam,Noah Brenowitz,Boris Bonev,Wonmin Byeon,Karsten Kreis,Dale Durran,Arash Vahdat,Mike Pritchard,Jan Kautz*

Main category: cs.LG

TL;DR: 提出一个可扩展的天气预测框架，通过结合下采样潜在空间和历史条件局部投影器，在多种概率估计器下均取得显著改进，证明通用模型扩展即可实现先进的中期预测，无需复杂定制架构和训练策略。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动的天气预测方法存在碎片化问题，复杂的定制架构和训练策略掩盖了预测精度的根本驱动因素。

Method: 引入一个可扩展框架，结合直接下采样的潜在空间与历史条件局部投影器来学习多尺度大气动力学，该框架设计对概率估计器选择具有鲁棒性，支持随机插值、扩散模型和CRPS集成训练。

Result: 在对比集成预报系统和深度学习概率模型GenCast的验证中，该框架在大多数变量上实现了统计显著性改进。

Conclusion: 扩展通用模型足以实现最先进的中期预测，消除了对定制训练方案的需求，并证明在全部概率框架范围内均有效。

Abstract: The recent revolution in data-driven methods for weather forecasting has lead to a fragmented landscape of complex, bespoke architectures and training strategies, obscuring the fundamental drivers of forecast accuracy. Here, we demonstrate that state-of-the-art probabilistic skill requires neither intricate architectural constraints nor specialized training heuristics. We introduce a scalable framework for learning multi-scale atmospheric dynamics by combining a directly downsampled latent space with a history-conditioned local projector that resolves high-resolution physics. We find that our framework design is robust to the choice of probabilistic estimator, seamlessly supporting stochastic interpolants, diffusion models, and CRPS-based ensemble training. Validated against the Integrated Forecasting System and the deep learning probabilistic model GenCast, our framework achieves statistically significant improvements on most of the variables. These results suggest scaling a general-purpose model is sufficient for state-of-the-art medium-range prediction, eliminating the need for tailored training recipes and proving effective across the full spectrum of probabilistic frameworks.

</details>


### [138] [Enhance the Safety in Reinforcement Learning by ADRC Lagrangian Methods](https://arxiv.org/abs/2601.18142)
*Mingxu Zhang,Huicheng Zhang,Jiaming Ji,Yaodong Yang,Ying Sun*

Main category: cs.LG

TL;DR: 提出ADRC-Lagrangian方法用于安全强化学习，通过主动扰动抑制控制解决传统拉格朗日方法的振荡和频繁安全违反问题，实验证明可大幅降低安全违反和成本。


<details>
  <summary>Details</summary>
Motivation: 现有安全强化学习的PID和经典拉格朗日方法因参数敏感性和相位滞后，导致策略振荡和频繁安全约束违反，难以在复杂环境中稳定运行。

Method: 将主动扰动抑制控制（ADRC）引入拉格朗日框架，提出ADRC-Lagrangian方法，构建包含传统方法作为特例的统一框架，增强系统鲁棒性。

Result: 在复杂环境中实验显示：安全违反次数减少74%，约束违反幅度降低89%，平均成本下降67%，显著优于现有方法。

Conclusion: ADRC-Lagrangian方法为安全强化学习提供了更稳健的解决方案，大幅提升了安全性能，在复杂应用场景中具有显著优势。

Abstract: Safe reinforcement learning (Safe RL) seeks to maximize rewards while satisfying safety constraints, typically addressed through Lagrangian-based methods. However, existing approaches, including PID and classical Lagrangian methods, suffer from oscillations and frequent safety violations due to parameter sensitivity and inherent phase lag. To address these limitations, we propose ADRC-Lagrangian methods that leverage Active Disturbance Rejection Control (ADRC) for enhanced robustness and reduced oscillations. Our unified framework encompasses classical and PID Lagrangian methods as special cases while significantly improving safety performance. Extensive experiments demonstrate that our approach reduces safety violations by up to 74%, constraint violation magnitudes by 89%, and average costs by 67\%, establishing superior effectiveness for Safe RL in complex environments.

</details>


### [139] [FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning](https://arxiv.org/abs/2601.18150)
*Zhaopeng Qiu,Shuang Yu,Jingqi Zhang,Shuai Zhang,Xue Huang,Jingyi Yang,Junjie Lai*

Main category: cs.LG

TL;DR: 提出一个实用的FP8 rollout栈用于LLM强化学习，通过块级量化、KV-cache优化和重要性采样校正，在保持学习行为稳定的前提下实现高达44%的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: LLM强化学习的rollout阶段存在性能瓶颈，长输出序列导致注意力计算和KV-cache内存占用主导端到端时间。FP8虽可加速但面临权重每步变化需重复量化，以及低精度rollout与高精度训练策略不匹配导致的不稳定性挑战。

Method: 1) 采用块级FP8量化实现W8A8线性层rollout；2) 通过每步QKV尺度重校准将FP8扩展至KV-cache；3) 使用基于重要性采样的rollout校正(TIS/MIS)缓解训练-推理不匹配；4) 在veRL生态中实现，支持FSDP/Megatron-LM等训练后端和vLLM/SGLang等推理引擎。

Result: 在稠密和MoE模型上实现高达44%的rollout吞吐量增益，同时学习行为与BF16基线相当。

Conclusion: 该FP8 rollout栈为LLM强化学习提供了实用的加速方案，在解决FP8在RL中的工程和算法挑战方面取得了显著成效，实现了吞吐量与学习稳定性的良好平衡。

Abstract: Reinforcement learning (RL) for large language models (LLMs) is increasingly bottlenecked by rollout (generation), where long output sequence lengths make attention and KV-cache memory dominate end-to-end step time. FP8 offers an attractive lever for accelerating RL by reducing compute cost and memory traffic during rollout, but applying FP8 in RL introduces unique engineering and algorithmic challenges: policy weights change every step (requiring repeated quantization and weight synchronization into the inference engine) and low-precision rollouts can deviate from the higher-precision policy assumed by the trainer, causing train-inference mismatch and potential instability. This report presents a practical FP8 rollout stack for LLM RL, implemented in the veRL ecosystem with support for common training backends (e.g., FSDP/Megatron-LM) and inference engines (e.g., vLLM/SGLang). We (i) enable FP8 W8A8 linear-layer rollout using blockwise FP8 quantization, (ii) extend FP8 to KV-cache to remove long-context memory bottlenecks via per-step QKV scale recalibration, and (iii) mitigate mismatch using importance-sampling-based rollout correction (token-level TIS/MIS variants). Across dense and MoE models, these techniques deliver up to 44% rollout throughput gains while preserving learning behavior comparable to BF16 baselines.

</details>


### [140] [Learning Fair Domain Adaptation with Virtual Label Distribution](https://arxiv.org/abs/2601.18171)
*Yuguang Zhang,Lijun Sheng,Jian Liang,Ran He*

Main category: cs.LG

TL;DR: This paper addresses category fairness in Unsupervised Domain Adaptation by proposing VILL, a framework that uses adaptive re-weighting and KL-divergence-based rebalancing to improve worst-case performance while maintaining overall accuracy.


<details>
  <summary>Details</summary>
Motivation: Most UDA methods focus on improving overall accuracy but neglect performance disparities across different categories, leading to classifiers that favor easy categories while performing poorly on difficult ones, creating fairness issues across categories.

Method: The authors propose Virtual Label-distribution-aware Learning (VILL) with two strategies: (1) an adaptive re-weighting mechanism that amplifies influence of hard-to-classify categories, and (2) a KL-divergence-based re-balancing strategy that explicitly adjusts decision boundaries to enhance category fairness.

Result: Experiments on standard datasets show VILL can be seamlessly integrated as a plug-and-play module into existing UDA methods, significantly improving category fairness while preserving high overall accuracy.

Conclusion: VILL is a simple yet effective framework that successfully addresses the category fairness problem in UDA by improving worst-case category performance without sacrificing overall accuracy, making it a practical solution for real-world applications requiring balanced performance across categories.

Abstract: Unsupervised Domain Adaptation (UDA) aims to mitigate performance degradation when training and testing data are sampled from different distributions. While significant progress has been made in enhancing overall accuracy, most existing methods overlook performance disparities across categories-an issue we refer to as category fairness. Our empirical analysis reveals that UDA classifiers tend to favor certain easy categories while neglecting difficult ones. To address this, we propose Virtual Label-distribution-aware Learning (VILL), a simple yet effective framework designed to improve worst-case performance while preserving high overall accuracy. The core of VILL is an adaptive re-weighting strategy that amplifies the influence of hard-to-classify categories. Furthermore, we introduce a KL-divergence-based re-balancing strategy, which explicitly adjusts decision boundaries to enhance category fairness. Experiments on commonly used datasets demonstrate that VILL can be seamlessly integrated as a plug-and-play module into existing UDA methods, significantly improving category fairness.

</details>


### [141] [Smooth, Sparse, and Stable: Finite-Time Exact Skeleton Recovery via Smoothed Proximal Gradients](https://arxiv.org/abs/2601.18189)
*Rui Wu,Yongjun Li*

Main category: cs.LG

TL;DR: 提出混合阶无环约束(AHOC)和光滑近端梯度算法(SPG-AHOC)，在有限次迭代内精确恢复DAG结构，无需启发式截断


<details>
  <summary>Details</summary>
Motivation: 现有连续优化方法(如NOTEARS)仅保证渐近收敛到驻点，产生稠密加权矩阵，需人工后处理截断才能恢复DAG，存在连续优化与离散图结构间的根本鸿沟

Method: 提出混合阶无环约束(AHOC)，采用光滑近端梯度法(SPG-AHOC)优化，利用近端算法的流形识别性质，提供有限时间Oracle性质理论保证

Result: 在标准可识别性假设下，SPG-AHOC在有限次迭代内精确恢复DAG支持集，即使优化光滑近似也能得到精确零元素，消除结构歧义

Conclusion: 算法实现无需启发式截断，经验结果达到SOTA精度，有力验证了有限时间识别理论

Abstract: Continuous optimization has significantly advanced causal discovery, yet existing methods (e.g., NOTEARS) generally guarantee only asymptotic convergence to a stationary point. This often yields dense weighted matrices that require arbitrary post-hoc thresholding to recover a DAG. This gap between continuous optimization and discrete graph structures remains a fundamental challenge. In this paper, we bridge this gap by proposing the Hybrid-Order Acyclicity Constraint (AHOC) and optimizing it via the Smoothed Proximal Gradient (SPG-AHOC). Leveraging the Manifold Identification Property of proximal algorithms, we provide a rigorous theoretical guarantee: the Finite-Time Oracle Property. We prove that under standard identifiability assumptions, SPG-AHOC recovers the exact DAG support (structure) in finite iterations, even when optimizing a smoothed approximation. This result eliminates structural ambiguity, as our algorithm returns graphs with exact zero entries without heuristic truncation. Empirically, SPG-AHOC achieves state-of-the-art accuracy and strongly corroborates the finite-time identification theory.

</details>


### [142] [HeterCSI: Channel-Adaptive Heterogeneous CSI Pretraining Framework for Generalized Wireless Foundation Models](https://arxiv.org/abs/2601.18200)
*Chenyu Zhang,Xinchen Lyu,Chenshan Ren,Shuhan Liu,Qimei Cui,Xiaofeng Tao*

Main category: cs.LG

TL;DR: 提出HeterCSI，一种信道自适应预训练框架，通过梯度动力学新理解解决CSI双异构性（尺度与场景）问题，实现高效训练与跨场景泛化，在12个数据集上优于SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 无线基础模型在6G CSI处理中面临尺度与场景双异构性的根本挑战，现有方法或限制输入维度或孤立训练，导致泛化性与可扩展性受限。

Method: 提出HeterCSI框架，揭示尺度异构性导致破坏性梯度干扰而场景多样性促进建设性梯度对齐；构建批次优化问题，采用尺度感知自适应分批策略与双掩码机制分离有效信号与填充伪影。

Result: 在12个数据集上实现零样本跨场景泛化，相比SOTA WiFo在CSI重建、时域和频域预测上分别降低NMSE 7.19 dB、4.08 dB和5.27 dB；训练延迟降低53%，平均泛化性能提升1.53 dB。

Conclusion: HeterCSI通过理解异构CSI预训练的梯度动力学，成功协调训练效率与跨场景泛化，为6G无线基础模型提供了可扩展解决方案。

Abstract: Wireless foundation models promise transformative capabilities for channel state information (CSI) processing across diverse 6G network applications, yet face fundamental challenges due to the inherent dual heterogeneity of CSI across both scale and scenario dimensions. However, current pretraining approaches either constrain inputs to fixed dimensions or isolate training by scale, limiting the generalization and scalability of wireless foundation models. In this paper, we propose HeterCSI, a channel-adaptive pretraining framework that reconciles training efficiency with robust cross-scenario generalization via a new understanding of gradient dynamics in heterogeneous CSI pretraining. Our key insight reveals that CSI scale heterogeneity primarily causes destructive gradient interference, while scenario diversity actually promotes constructive gradient alignment when properly managed. Specifically, we formulate heterogeneous CSI batch construction as a partitioning optimization problem that minimizes zero-padding overhead while preserving scenario diversity. To solve this, we develop a scale-aware adaptive batching strategy that aligns CSI samples of similar scales, and design a double-masking mechanism to isolate valid signals from padding artifacts. Extensive experiments on 12 datasets demonstrate that HeterCSI establishes a generalized foundation model without scenario-specific finetuning, achieving superior average performance over full-shot baselines. Compared to the state-of-the-art zero-shot benchmark WiFo, it reduces NMSE by 7.19 dB, 4.08 dB, and 5.27 dB for CSI reconstruction, time-domain, and frequency-domain prediction, respectively. The proposed HeterCSI framework also reduces training latency by 53% compared to existing approaches while improving generalization performance by 1.53 dB on average.

</details>


### [143] [Rethinking Cross-Modal Fine-Tuning: Optimizing the Interaction between Feature Alignment and Target Fitting](https://arxiv.org/abs/2601.18231)
*Trong Khiem Tran,Manh Cuong Dao,Phi Le Nguyen,Thao Nguyen Truong,Trong Nghia Hoang*

Main category: cs.LG

TL;DR: 针对预训练模型跨模态适配问题，提出基于特征标签失真的理论框架，建立可证明泛化界，显著优于SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 跨学科知识整合需求增长，需将预训练模型适配到未见特征模态。核心挑战是使新模态表示与预训练模型最相关部分对齐。未校准的特征对齐与目标微调组合会加剧特征-标签结构错配并降低泛化能力。现有工作缺乏对该交互作用的理论理解。

Method: 建立可证明的目标误差泛化界的原则性框架，通过特征标签失真新概念解释特征对齐与目标拟合的交互作用。

Result: 在广泛的基准数据集上，所提方法显著优于现有最先进方法。

Conclusion: 该泛化界为优化特征对齐与目标拟合的交互提供了可操作的见解，指导实际算法设计。

Abstract: Adapting pre-trained models to unseen feature modalities has become increasingly important due to the growing need for cross-disciplinary knowledge integration.~A key challenge here is how to align the representation of new modalities with the most relevant parts of the pre-trained model's representation space to enable accurate knowledge transfer.~This requires combining feature alignment with target fine-tuning, but uncalibrated combinations can exacerbate misalignment between the source and target feature-label structures and reduce target generalization.~Existing work however lacks a theoretical understanding of this critical interaction between feature alignment and target fitting.~To bridge this gap, we develop a principled framework that establishes a provable generalization bound on the target error, which explains the interaction between feature alignment and target fitting through a novel concept of feature-label distortion.~This bound offers actionable insights into how this interaction should be optimized for practical algorithm design. The resulting approach achieves significantly improved performance over state-of-the-art methods across a wide range of benchmark datasets.

</details>


### [144] [Tractable Gaussian Phase Retrieval with Heavy Tails and Adversarial Corruption with Near-Linear Sample Complexity](https://arxiv.org/abs/2601.18245)
*Santanu Das,Jatin Batra*

Main category: cs.LG

TL;DR: 提出了首个多项式时间算法解决具有重尾噪声和对抗性损坏的鲁棒相位恢复问题，通过将鲁棒谱初始化与鲁棒PCA算法结合，实现了接近线性的样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 相位恢复是许多科学领域的基础问题，现有算法在面对重尾噪声和对抗性损坏时计算效率低下（需要指数时间），特别是鲁棒谱初始化被认为是计算上不可行的关键瓶颈。

Method: 通过建立鲁棒谱初始化与近期鲁棒PCA算法进展之间的联系，克服了谱初始化的计算瓶颈。

Result: 获得了首个多项式时间算法，样本复杂度接近线性（O(n log n)），显著优于之前Buna和Rebeschini的指数时间算法。

Conclusion: 该工作为鲁棒相位恢复提供了计算高效的解决方案，将鲁棒统计领域的最新算法进展成功应用于经典信号处理问题，解决了该领域的一个重要开放性问题。

Abstract: Phase retrieval is the classical problem of recovering a signal $x^* \in \mathbb{R}^n$ from its noisy phaseless measurements $y_i = \langle a_i, x^* \rangle^2 + ζ_i$ (where $ζ_i$ denotes noise, and $a_i$ is the sensing vector) for $i \in [m]$. The problem of phase retrieval has a rich history, with a variety of applications such as optics, crystallography, heteroscedastic regression, astrophysics, etc. A major consideration in algorithms for phase retrieval is robustness against measurement errors. In recent breakthroughs in algorithmic robust statistics, efficient algorithms have been developed for several parameter estimation tasks such as mean estimation, covariance estimation, robust principal component analysis (PCA), etc. in the presence of heavy-tailed noise and adversarial corruptions. In this paper, we study efficient algorithms for robust phase retrieval with heavy-tailed noise when a constant fraction of both the measurements $y_i$ and the sensing vectors $a_i$ may be arbitrarily adversarially corrupted. For this problem, Buna and Rebeschini (AISTATS 2025) very recently gave an exponential time algorithm with sample complexity $O(n \log n)$. Their algorithm needs a robust spectral initialization, specifically, a robust estimate of the top eigenvector of a covariance matrix, which they deemed to be beyond known efficient algorithmic techniques (similar spectral initializations are a key ingredient of a large family of phase retrieval algorithms). In this work, we make a connection between robust spectral initialization and recent algorithmic advances in robust PCA, yielding the first polynomial-time algorithms for robust phase retrieval with both heavy-tailed noise and adversarial corruptions, in fact with near-linear (in $n$) sample complexity.

</details>


### [145] [Beyond Retention: Orchestrating Structural Safety and Plasticity in Continual Learning for LLMs](https://arxiv.org/abs/2601.18255)
*Fei Meng*

Main category: cs.LG

TL;DR: 本文揭示了经验回放(ER)在LLM持续学习中的关键二分法：对非结构化任务有正向迁移，但对代码生成等结构化域造成严重负迁移。提出的正交子空间唤醒(OSW)方法通过识别旧任务关键参数子空间并强制新任务更新正交化，实现了稳定与可塑性的平衡。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型持续学习面临稳定性(保留旧知识)与可塑性(学习新任务)的核心矛盾。标准方法经验回放(ER)虽能缓解灾难性遗忘，但其对不同能力域的差异化影响尚未被充分探索。

Method: 提出正交子空间唤醒(OSW)：通过短暂"唤醒"阶段识别先前任务的关键参数子空间，对新任务强制执行正交更新约束，为已建立的知识结构提供数学保障。

Result: 在四项任务序列实验中，OSW成功保留了ER失败的脆弱编码能力(代码生成准确率显著优于ER)，同时保持了对新任务的高可塑性，实现了稳定与可塑性的统一。

Conclusion: 研究强调LLM持续学习评估必须超越平均保留率指标，需同时考量结构安全性，为脆弱结构化知识的保护提供了新范式。

Abstract: Continual learning in Large Language Models (LLMs) faces the critical challenge of balancing stability (retaining old knowledge) and plasticity (learning new tasks). While Experience Replay (ER) is a standard countermeasure against catastrophic forgetting, its impact across diverse capabilities remains underexplored. In this work, we uncover a critical dichotomy in ER's behavior: while it induces positive backward transfer on robust, unstructured tasks (e.g., boosting performance on previous NLP classification tasks through repeated rehearsal), it causes severe negative transfer on fragile, structured domains like code generation (e.g., a significant relative drop in coding accuracy). This reveals that ER trades structural integrity for broad consolidation. To address this dilemma, we propose \textbf{Orthogonal Subspace Wake-up (OSW)}. OSW identifies essential parameter subspaces of previous tasks via a brief "wake-up" phase and enforces orthogonal updates for new tasks, providing a mathematically grounded "safety guarantee" for established knowledge structures. Empirical results across a diverse four-task sequence demonstrate that OSW uniquely succeeds in preserving fragile coding abilities where Replay fails, while simultaneously maintaining high plasticity for novel tasks. Our findings emphasize the necessity of evaluating structural safety alongside average retention in LLM continual learning.

</details>


### [146] [FGGM: Fisher-Guided Gradient Masking for Continual Learning](https://arxiv.org/abs/2601.18261)
*Chao-Hong Tan,Qian Chen,Wen Wang,Yukun Ma,Chong Zhang,Chong Deng,Qinglin Zhang,Xiangang Li,Jieping Ye*

Main category: cs.LG

TL;DR: FGGM是一种利用Fisher信息矩阵动态掩码参数的框架，无需历史数据即可缓解大语言模型的灾难性遗忘，在TRACE基准测试上比监督微调提升9.6%，比MIGU提升4.4%。


<details>
  <summary>Details</summary>
Motivation: 灾难性遗忘会损害大语言模型的持续学习能力，现有方法需要历史数据或缺乏数学原理支撑。

Method: 提出Fisher-Guided Gradient Masking (FGGM)框架，通过对角Fisher信息矩阵评估参数重要性，动态生成自适应阈值的二进制掩码来选择性地更新参数，在无需历史数据的情况下平衡模型稳定性与可塑性。

Result: 在TRACE基准测试上，FGGM相比监督微调(SFT)在保留通用能力方面相对提升9.6%，相比MIGU在TRACE任务上提升4.4%；代码生成任务分析也证实了其优越性能和更少的遗忘。

Conclusion: FGGM为缓解灾难性遗忘提供了有效的数学原理解决方案，显著提升了持续学习性能。

Abstract: Catastrophic forgetting impairs the continuous learning of large language models. We propose Fisher-Guided Gradient Masking (FGGM), a framework that mitigates this by strategically selecting parameters for updates using diagonal Fisher Information. FGGM dynamically generates binary masks with adaptive thresholds, preserving critical parameters to balance stability and plasticity without requiring historical data. Unlike magnitude-based methods such as MIGU, our approach offers a mathematically principled parameter importance estimation. On the TRACE benchmark, FGGM shows a 9.6% relative improvement in retaining general capabilities over supervised fine-tuning (SFT) and a 4.4% improvement over MIGU on TRACE tasks. Additional analysis on code generation tasks confirms FGGM's superior performance and reduced forgetting, establishing it as an effective solution.

</details>


### [147] [What Do Learned Models Measure?](https://arxiv.org/abs/2601.18278)
*Indrė Žliobaitė*

Main category: cs.LG

TL;DR: 该论文指出，当机器学习模型被用作测量工具而非仅仅是预测器时，标准评估指标无法保证测量稳定性，需要新的评估维度


<details>
  <summary>Details</summary>
Motivation: 在许多科学和数据驱动的应用中，机器学习模型正越来越多地被用作测量仪器，而不仅仅是预定义标签的预测器。当测量函数从数据中学习时，映射关系由训练分布和归纳偏差隐式决定，导致多个不等价的映射都能满足标准预测评估标准，这带来了潜在的可靠性问题

Method: 将学习的测量函数形式化为独立的评估重点，引入测量稳定性概念（衡量学习过程和不同环境下测量结果的不变性），并通过真实案例研究验证标准评估准则的局限性

Result: 标准机器学习评估标准（包括泛化误差、校准和鲁棒性）无法保证测量稳定性；预测性能相当的模型可以实现系统上不等价的测量函数；分布偏移是这种失败的具体例证

Conclusion: 现有评估框架在将学习模型输出作为测量的场景中存在局限性，需要增加额外的评估维度来确保测量稳定性

Abstract: In many scientific and data-driven applications, machine learning models are increasingly used as measurement instruments, rather than merely as predictors of predefined labels. When the measurement function is learned from data, the mapping from observations to quantities is determined implicitly by the training distribution and inductive biases, allowing multiple inequivalent mappings to satisfy standard predictive evaluation criteria. We formalize learned measurement functions as a distinct focus of evaluation and introduce measurement stability, a property capturing invariance of the measured quantity across admissible realizations of the learning process and across contexts. We show that standard evaluation criteria in machine learning, including generalization error, calibration, and robustness, do not guarantee measurement stability. Through a real-world case study, we show that models with comparable predictive performance can implement systematically inequivalent measurement functions, with distribution shift providing a concrete illustration of this failure. Taken together, our results highlight a limitation of existing evaluation frameworks in settings where learned model outputs are identified as measurements, motivating the need for an additional evaluative dimension.

</details>


### [148] [TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment](https://arxiv.org/abs/2601.18292)
*Zhewen Tan,Wenhan Yu,Jianfeng Si,Tongxin Liu,Kaiqi Guan,Huiyan Jin,Jiawen Tao,Xiaokun Yuan,Duohe Ma,Xiangzheng Zhang,Tong Yang,Lin Sun*

Main category: cs.LG

TL;DR: 提出TriPlay-RL框架，通过三个AI角色（攻击者、防御者、评估者）在闭环强化学习系统中协同演化，以近乎零人工标注的方式显著提升大语言模型的安全对齐性能


<details>
  <summary>Details</summary>
Motivation: 大语言模型安全风险日益突出，亟需缓解有毒有害内容生成。现有安全对齐依赖人工标注，效率低下，需要构建攻击、防御、评估三方自动化协作框架

Method: 提出TriPlay-RL闭环强化学习框架，实现攻击者（生成对抗性提示）、防御者（构建安全防御）、评估者（评估响应质量）三个角色的迭代协同进化，仅需极少人工标注

Result: 攻击者对抗有效性提升20%-50%且保持高多样性；防御者安全性能提升10%-30%且不损害通用推理能力；评估者能准确区分不安全响应、简单拒绝和有用引导

Conclusion: 该框架建立了高效可扩展的LLM安全对齐范式，在统一学习循环中实现三方持续协同进化，大幅降低人工标注成本

Abstract: In recent years, safety risks associated with large language models have become increasingly prominent, highlighting the urgent need to mitigate the generation of toxic and harmful content. The mainstream paradigm for LLM safety alignment typically adopts a collaborative framework involving three roles: an attacker for adversarial prompt generation, a defender for safety defense, and an evaluator for response assessment. In this paper, we propose a closed-loop reinforcement learning framework called TriPlay-RL that enables iterative and co-improving collaboration among three roles with near-zero manual annotation. Experimental results show that the attacker preserves high output diversity while achieving a 20%-50% improvement in adversarial effectiveness; the defender attains 10%-30% gains in safety performance without degrading general reasoning capability; and the evaluator continuously refines its fine-grained judgment ability through iterations, accurately distinguishing unsafe responses, simple refusals, and useful guidance. Overall, our framework establishes an efficient and scalable paradigm for LLM safety alignment, enabling continuous co-evolution within a unified learning loop.

</details>


### [149] [A Master Class on Reproducibility: A Student Hackathon on Advanced MRI Reconstruction Methods](https://arxiv.org/abs/2601.18314)
*Lina Felsner,Sevgi G. Kafali,Hannah Eichhorn,Agnes A. J. Leth,Aidas Batvinskas,Andre Datchev,Fabian Klemm,Jan Aulich,Puntika Leepagorn,Ruben Klinger,Daniel Rueckert,Julia A. Schnabel*

Main category: cs.LG

TL;DR: A student hackathon replicated three MRI reconstruction papers (MoDL, HUMUS-Net, and an untrained physics-regularized method), documenting outcomes and best practices for reproducible code.


<details>
  <summary>Details</summary>
Motivation: To address the reproducibility crisis in AI research by testing whether students can replicate influential MRI reconstruction methods and identifying fundamental practices for reproducible codebases.

Method: Organized a student hackathon where participants attempted to reproduce results from three state-of-the-art MRI reconstruction papers: MoDL (unrolled model-based network with learned denoising), HUMUS-Net (hybrid unrolled CNN+Transformer architecture), and an untrained physics-regularized dynamic MRI method using quantitative MR model for early stopping.

Result: Reported reproduction outcomes for all three methods alongside additional experiments, and identified fundamental practices essential for building reproducible codebases.

Conclusion: Reproducibility hackathons are effective for validating research and establishing community standards; the identified best practices should be adopted to improve reproducibility in MRI reconstruction and broader AI research fields.

Abstract: We report the design, protocol, and outcomes of a student reproducibility hackathon focused on replicating the results of three influential MRI reconstruction papers: (a) MoDL, an unrolled model-based network with learned denoising; (b) HUMUS-Net, a hybrid unrolled multiscale CNN+Transformer architecture; and (c) an untrained, physics-regularized dynamic MRI method that uses a quantitative MR model for early stopping. We describe the setup of the hackathon and present reproduction outcomes alongside additional experiments, and we detail fundamental practices for building reproducible codebases.

</details>


### [150] [Cognitive Fusion of ZC Sequences and Time-Frequency Images for Out-of-Distribution Detection of Drone Signals](https://arxiv.org/abs/2601.18326)
*Jie Li,Jing Li,Lu Lv,Zhanyu Ju,Fengkui Gong*

Main category: cs.LG

TL;DR: 提出一种基于Zadoff-Chu序列和时频谱图认知融合的无人机信号分布外检测算法。通过多模态特征提取、交互与融合，结合空间-通道双维度判别分数生成的自适应注意力权重，实现信号分类。该方法在RID和OODD指标上分别提升1.7%和7.5%，并在不同飞行条件和无人机类型下展现强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 无人机数量激增带来安全隐患，需可靠的远程识别技术。现有方法难以兼顾已知标准协议（如DJI无人机的Zadoff-Chu序列）和未知/非标准协议的信号检测，导致分布外检测性能受限。亟需一种能融合多模态信息、适应多样化通信协议的鲁棒检测算法。

Method: 1. 多模态特征提取：从射频信号中提取Zadoff-Chu序列特征（针对DJI协议）和时频谱图特征（针对未知协议）；2. 特征增强与对齐：通过专用模块处理两种模态特征；3. 多层次特征融合：依次进行多模态交互、单模态融合、多模态融合，实现信息互补；4. 自适应注意力机制：计算空间和通道维度的判别分数，转换为注意力权重；5. 分类决策：加权特征经Softmax输出分类结果。

Result: RID性能提升1.7%，OODD性能提升7.5%，显著优于现有算法，且在多种飞行条件和无人机类型下保持强鲁棒性。

Conclusion: 该算法通过认知融合协议特征和时频特征，有效解决了无人机信号分布外检测难题。多层次融合策略和自适应注意力机制增强了特征表达能力，验证了多模态方法在复杂无人机通信环境中的有效性，为无人机监管提供了可靠技术支撑。

Abstract: We propose a drone signal out-of-distribution detection (OODD) algorithm based on the cognitive fusion of Zadoff-Chu (ZC) sequences and time-frequency images (TFI). ZC sequences are identified by analyzing the communication protocols of DJI drones, while TFI capture the time-frequency characteristics of drone signals with unknown or non-standard communication protocols. Both modalities are used jointly to enable OODD in the drone remote identification (RID) task. Specifically, ZC sequence features and TFI features are generated from the received radio frequency signals, which are then processed through dedicated feature extraction module to enhance and align them. The resultant multi-modal features undergo multi-modal feature interaction, single-modal feature fusion, and multi-modal feature fusion to produce features that integrate and complement information across modalities. Discrimination scores are computed from the fused features along both spatial and channel dimensions to capture time-frequency characteristic differences dictated by the communication protocols, and these scores will be transformed into adaptive attention weights. The weighted features are then passed through a Softmax function to produce the signal classification results. Simulation results demonstrate that the proposed algorithm outperforms existing algorithms and achieves 1.7% and 7.5% improvements in RID and OODD metrics, respectively. The proposed algorithm also performs strong robustness under varying flight conditions and across different drone types.

</details>


### [151] [Discriminability-Driven Spatial-Channel Selection with Gradient Norm for Drone Signal OOD Detection](https://arxiv.org/abs/2601.18329)
*Chuhan Feng,Jing Li,Jie Li,Lu Lv,Fengkui Gong*

Main category: cs.LG

TL;DR: A drone signal OOD detection algorithm using discriminability-driven spatial-channel selection and gradient norm metrics, achieving robust performance across SNR conditions and drone types.


<details>
  <summary>Details</summary>
Motivation: Detecting out-of-distribution drone signals is challenging due to protocol-specific characteristics and instability of OOD samples, requiring discriminative feature selection and robust detection mechanisms.

Method: Proposes discriminability-driven spatial-channel selection that adaptively weights time-frequency image features based on inter-class similarity/variance, combined with gradient-norm metrics to measure perturbation sensitivity of OOD samples, fused with energy-based scores for joint inference.

Result: Simulation results demonstrate superior discriminative power and robust performance across varying SNR conditions and different drone types.

Conclusion: The proposed method effectively addresses drone signal OOD detection through adaptive feature selection and gradient-based instability measurement, showing robust and discriminative detection capabilities.

Abstract: We propose a drone signal out-of-distribution (OOD) detection algorithm based on discriminability-driven spatial-channel selection with a gradient norm. Time-frequency image features are adaptively weighted along both spatial and channel dimensions by quantifying inter-class similarity and variance based on protocol-specific time-frequency characteristics. Subsequently, a gradient-norm metric is introduced to measure perturbation sensitivity for capturing the inherent instability of OOD samples, which is then fused with energy-based scores for joint inference. Simulation results demonstrate that the proposed algorithm provides superior discriminative power and robust performance via SNR and various drone types.

</details>


### [152] [Making medical vision-language models think causally across modalities with retrieval-augmented cross-modal reasoning](https://arxiv.org/abs/2601.18356)
*Weiqin Yang,Haowen Xue,Qingyi Peng,Hexuan Hu,Qian Huang,Tingbo Zhang*

Main category: cs.LG

TL;DR: 提出多模态因果检索增强生成框架，解决医疗视觉语言模型仅依赖相关性而非因果机制的问题，提升事实准确性、鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 医疗视觉语言模型在诊断中表现优异但依赖表面统计关联而非因果病理机制，导致脆弱性、幻觉和数据偏见问题，传统RAG基于语义相似性仍会引入虚假相关。

Method: 提出多模态因果检索增强生成框架，整合因果推断原理与多模态检索，从外部知识库检索临床相关示例和因果图，基于反事实和干预证据而非单纯相关性进行推理。

Result: 在放射报告生成、诊断预测和视觉问答任务中，该方法提高了事实准确性、对分布偏移的鲁棒性和模型可解释性。

Conclusion: 因果检索为构建可信赖的医疗视觉语言模型提供了可扩展路径，使其超越模式匹配，实现高风险临床环境中的可靠多模态推理。

Abstract: Medical vision-language models (VLMs) achieve strong performance in diagnostic reporting and image-text alignment, yet their underlying reasoning mechanisms remain fundamentally correlational, exhibiting reliance on superficial statistical associations that fail to capture the causal pathophysiological mechanisms central to clinical decision-making. This limitation makes them fragile, prone to hallucinations, and sensitive to dataset biases. Retrieval-augmented generation (RAG) offers a partial remedy by grounding predictions in external knowledge. However, conventional RAG depends on semantic similarity, introducing new spurious correlations. We propose Multimodal Causal Retrieval-Augmented Generation, a framework that integrates causal inference principles with multimodal retrieval. It retrieves clinically relevant exemplars and causal graphs from external sources, conditioning model reasoning on counterfactual and interventional evidence rather than correlations alone. Applied to radiology report generation, diagnosis prediction, and visual question answering, it improves factual accuracy, robustness to distribution shifts, and interpretability. Our results highlight causal retrieval as a scalable path toward medical VLMs that think beyond pattern matching, enabling trustworthy multimodal reasoning in high-stakes clinical settings.

</details>


### [153] [Gradient Regularized Natural Gradients](https://arxiv.org/abs/2601.18420)
*Satya Prakash Dash,Hossein Abdi,Wei Pan,Samuel Kaski,Mingfei Sun*

Main category: cs.LG

TL;DR: 提出梯度正则化自然梯度（GRNG），一种结合梯度正则化与自然梯度更新的可扩展二阶优化器家族，通过两种变体（频率派的结构化近似和贝叶斯的正则化卡尔曼滤波）避免Fisher信息矩阵求逆，实验证明其在视觉和语言基准上优于SGD、AdamW、K-FAC等基线方法


<details>
  <summary>Details</summary>
Motivation: 梯度正则化能提升模型泛化能力，自然梯度在训练初期能加速优化，但现有研究很少关注如何将梯度正则化与二阶优化器结合以改善训练动态

Method: 提出GRNG框架，包含两种互补算法：1）频率派变体通过结构化近似避免显式Fisher信息矩阵求逆；2）贝叶斯变体基于正则化卡尔曼滤波完全消除求逆需求，并建立收敛性保证

Result: 理论证明梯度正则化提升稳定性并确保收敛到全局极小值；实证显示GRNG在优化速度和泛化能力上持续优于一阶方法（SGD、AdamW）和二阶基线（K-FAC、Sophia），在视觉和语言基准测试中表现优异

Conclusion: 梯度正则化是解锁自然梯度方法在大规模深度学习中鲁棒性的原则性实用工具

Abstract: Gradient regularization (GR) has been shown to improve the generalizability of trained models. While Natural Gradient Descent has been shown to accelerate optimization in the initial phase of training, little attention has been paid to how the training dynamics of second-order optimizers can benefit from GR. In this work, we propose Gradient-Regularized Natural Gradients (GRNG), a family of scalable second-order optimizers that integrate explicit gradient regularization with natural gradient updates. Our framework provides two complementary algorithms: a frequentist variant that avoids explicit inversion of the Fisher Information Matrix (FIM) via structured approximations, and a Bayesian variant based on a Regularized-Kalman formulation that eliminates the need for FIM inversion entirely. We establish convergence guarantees for GRNG, showing that gradient regularization improves stability and enables convergence to global minima. Empirically, we demonstrate that GRNG consistently enhances both optimization speed and generalization compared to first-order methods (SGD, AdamW) and second-order baselines (K-FAC, Sophia), with strong results on vision and language benchmarks. Our findings highlight gradient regularization as a principled and practical tool to unlock the robustness of natural gradient methods for large-scale deep learning.

</details>


### [154] [GCFX: Generative Counterfactual Explanations for Deep Graph Models at the Model Level](https://arxiv.org/abs/2601.18447)
*Jinlong Hu,Jiacheng Liu*

Main category: cs.LG

TL;DR: A generative model-level counterfactual explanation approach (GCFX) for deep graph learning models that uses dual encoders, structure-aware taggers, and MPNN decoders to generate high-quality counterfactuals, outperforming existing methods in validity, coverage, and cost.


<details>
  <summary>Details</summary>
Motivation: Deep graph learning models lack transparency, making them hard to understand and trust. The paper addresses the need for model-level counterfactual explanations to provide comprehensive understanding of model decision-making processes.

Method: GCFX is a generative model-level counterfactual explanation approach based on deep graph generation. It uses an enhanced graph generation framework with dual encoders, structure-aware taggers, and MPNN decoders to learn latent data distribution and generate related counterfactuals. A global summarization algorithm selects the most representative explanations.

Result: Experiments on synthetic and real-world datasets demonstrate GCFX outperforms existing methods in counterfactual validity and coverage while maintaining low explanation costs.

Conclusion: GCFX enhances the practicality and trustworthiness of global counterfactual explanations for deep graph learning models.

Abstract: Deep graph learning models have demonstrated remarkable capabilities in processing graph-structured data and have been widely applied across various fields. However, their complex internal architectures and lack of transparency make it difficult to explain their decisions, resulting in opaque models that users find hard to understand and trust. In this paper, we explore model-level explanation techniques for deep graph learning models, aiming to provide users with a comprehensive understanding of the models' overall decision-making processes and underlying mechanisms. Specifically, we address the problem of counterfactual explanations for deep graph learning models by introducing a generative model-level counterfactual explanation approach called GCFX, which is based on deep graph generation. This approach generates a set of high-quality counterfactual explanations that reflect the model's global predictive behavior by leveraging an enhanced deep graph generation framework and a global summarization algorithm. GCFX features an architecture that combines dual encoders, structure-aware taggers, and Message Passing Neural Network decoders, enabling it to accurately learn the true latent distribution of input data and generate high-quality, closely related counterfactual examples. Subsequently, a global counterfactual summarization algorithm selects the most representative and comprehensive explanations from numerous candidate counterfactuals, providing broad insights into the model's global predictive patterns. Experiments on a synthetic dataset and several real-world datasets demonstrate that GCFX outperforms existing methods in terms of counterfactual validity and coverage while maintaining low explanation costs, thereby offering crucial support for enhancing the practicality and trustworthiness of global counterfactual explanations.

</details>


### [155] [Superlinear Multi-Step Attention](https://arxiv.org/abs/2601.18401)
*Yufeng Huang*

Main category: cs.LG

TL;DR: 本文提出了一种名为Superlinear attention的全训练多步注意力架构，通过将标准因果自注意力重构为多步搜索问题，在保持随机上下文访问能力的同时，实现了长序列的亚二次方复杂度。


<details>
  <summary>Details</summary>
Motivation: 标准因果自注意力对长序列的计算复杂度为二次方O(L²)，计算开销巨大。需要一种能在保持任意token位置都可被访问（结构非排他性）的前提下，降低计算复杂度的注意力机制。

Method: 提出Superlinear attention，将因果自注意力重构为N步搜索问题，整体复杂度为O(L^(1+1/N))。具体实现了一个N=2的基线版本（类似跳转搜索），第一步进行O(L^(3/2))的区间搜索选择相关区间，第二步在选定区间内进行O(L^(3/2))的标准注意力计算。

Result: 在30B混合MoE模型上，实现版本在1M上下文长度下达到114 tokens/sec的解码吞吐量，在10M上下文下达到80 tokens/sec。在NIAH任务上，有限训练后能在256K上下文长度下表现出色，证明了路由区间选择可通过端到端学习。

Conclusion: 该架构在理论上和系统可行性上展示了优势，为长序列处理提供了新思路，但全面的跨任务质量评估留待未来工作。

Abstract: In this paper, we propose \textbf{Superlinear attention}, a fully trainable multi-step attention architecture that achieves subquadratic complexity for long sequences while preserving \textbf{random context access} (a.k.a.\ structural non-exclusion): no eligible token position is structurally excluded from being selected for attention. Superlinear attention reformulates standard causal self-attention as a multi-step search problem with $N$ steps, yielding an overall complexity of $O(L^{1+\frac{1}{N}})$. To illustrate the architecture, we present a baseline $N=2$ implementation, which is algorithmically analogous to standard jump search. In this $O(L^{3/2})$ instantiation, the first step performs $O(L^{3/2})$ span-search to select relevant spans of the sequence, and the second step applies $O(L^{3/2})$ span-attention (standard attention restricted to the selected spans). In an upscaled $O(L^{1.54})$ configuration for robustness, we achieve an average decoding throughput of 114 tokens/sec at 1M context length and 80 tokens/sec at 10M context in our implementation on a modified 30B hybrid MoE model on a single B200 GPU. With limited training, we also obtain strong performance on the NIAH (Needle In A Haystack) task up to 256K context length, demonstrating that the routed span selection is learnable end-to-end. This paper emphasizes architectural formulation, scaling analysis, and systems feasibility, and presents initial validation; comprehensive quality evaluations across diverse long-context tasks are left to future work.

</details>


### [156] [Just-In-Time Reinforcement Learning: Continual Learning in LLM Agents Without Gradient Updates](https://arxiv.org/abs/2601.18510)
*Yibo Li,Zijie Lin,Ailin Deng,Xuan Zhang,Yufei He,Shuo Ji,Tri Cao,Bryan Hooi*

Main category: cs.LG

TL;DR: JitRL是一种无需训练的测试时策略优化框架，通过动态记忆库实时检索经验来估计动作优势并直接调制LLM输出，在不更新梯度的情况下实现持续适应，性能超越微调方法且成本降低30倍以上。


<details>
  <summary>Details</summary>
Motivation: 部署后的LLM智能体因模型权重冻结而难以持续适应新环境，传统强化学习方法计算成本高昂且易导致灾难性遗忘。

Method: 提出训练无关的JitRL框架：维护动态非参数化经验记忆库，在线检索相关轨迹计算动作优势，直接调整LLM输出logits，理论证明该加法更新是KL约束策略优化问题的精确闭式解。

Result: 在WebArena和Jericho基准测试中，JitRL刷新了训练无关方法的SOTA性能，性能超越计算成本高昂的微调方法（如WebRL），同时将货币成本降低超过30倍。

Conclusion: JitRL为支持持续学习的智能体提供了一种可扩展、高效且经济的路径，验证了测试时优化替代传统训练的可行性。

Abstract: While Large Language Model (LLM) agents excel at general tasks, they inherently struggle with continual adaptation due to the frozen weights after deployment. Conventional reinforcement learning (RL) offers a solution but incurs prohibitive computational costs and the risk of catastrophic forgetting. We introduce Just-In-Time Reinforcement Learning (JitRL), a training-free framework that enables test-time policy optimization without any gradient updates. JitRL maintains a dynamic, non-parametric memory of experiences and retrieves relevant trajectories to estimate action advantages on-the-fly. These estimates are then used to directly modulate the LLM's output logits. We theoretically prove that this additive update rule is the exact closed-form solution to the KL-constrained policy optimization objective. Extensive experiments on WebArena and Jericho demonstrate that JitRL establishes a new state-of-the-art among training-free methods. Crucially, JitRL outperforms the performance of computationally expensive fine-tuning methods (e.g., WebRL) while reducing monetary costs by over 30 times, offering a scalable path for continual learning agents. The code is available at https://github.com/liushiliushi/JitRL.

</details>


### [157] [Frequency-Based Hyperparameter Selection in Games](https://arxiv.org/abs/2601.18409)
*Aniket Sanyal,Baraah A. M. Sidahmed,Rebekka Burkholz,Tatjana Chavdarova*

Main category: cs.LG

TL;DR: 本文提出模态LookAhead (MoLA)，通过估计振荡动力学频率实现平滑游戏的原则性超参数调整。MoLA扩展LookAhead实现自适应参数选择，提供收敛保证，并以最小计算开销加速训练。


<details>
  <summary>Details</summary>
Motivation: 平滑游戏中的旋转动力学使经典超参数调整策略失效。尽管LookAhead表现优异，但其引入的额外参数对性能影响关键。有效的游戏调参方法仍缺乏充分探索。

Method: 利用频率估计分析连续时间轨迹与离散动力学频谱，提出模态LookAhead (MoLA)，基于频率分析自适应选择超参数。

Result: 提供理论收敛保证。实验表明MoLA在纯旋转游戏和混合机制中均能加速训练，且计算开销极小。

Conclusion: MoLA利用频率分析成功扩展LookAhead，实现自适应超参数选择，为平滑游戏提供了原则性调参方案。

Abstract: Learning in smooth games fundamentally differs from standard minimization due to rotational dynamics, which invalidate classical hyperparameter tuning strategies. Despite their practical importance, effective methods for tuning in games remain underexplored. A notable example is LookAhead (LA), which achieves strong empirical performance but introduces additional parameters that critically influence performance. We propose a principled approach to hyperparameter selection in games by leveraging frequency estimation of oscillatory dynamics. Specifically, we analyze oscillations both in continuous-time trajectories and through the spectrum of the discrete dynamics in the associated frequency-based space. Building on this analysis, we introduce \emph{Modal LookAhead (MoLA)}, an extension of LA that selects the hyperparameters adaptively to a given problem. We provide convergence guarantees and demonstrate in experiments that MoLA accelerates training in both purely rotational games and mixed regimes, all with minimal computational overhead.

</details>


### [158] [Scalable Transit Delay Prediction at City Scale: A Systematic Approach with Multi-Resolution Feature Engineering and Deep Learning](https://arxiv.org/abs/2601.18521)
*Emna Boudabbous,Mohamed Karaa,Lokman Sboui,Julio Montecinos,Omar Alam*

Main category: cs.LG

TL;DR: A city-scale bus delay prediction pipeline combining multi-resolution feature engineering, Adaptive PCA dimensionality reduction, and hybrid H3+topology clustering achieves superior performance with a global LSTM model, outperforming transformers by 18-52% while using 275x fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Urban bus transit agencies need reliable network-wide delay predictions to provide accurate passenger arrival information and support real-time operational control, but existing systems handle only few routes, rely on hand-crafted features, and lack scalable, reusable architecture guidance.

Method: Framework generates 1,683 spatiotemporal features from 23 aggregation combinations over H3 cells, routes, segments, and temporal patterns, compresses them into 83 components via Adaptive PCA (95% variance preserved), and uses hybrid H3+topology clustering to create 12 balanced route clusters for distributed training.

Result: Global LSTM with cluster-aware features achieved best accuracy-efficiency trade-off on six months of STM Montréal data, outperforming transformers by 18-52% with 275x fewer parameters, validated through multi-level evaluation and latency analysis for real-time deployment.

Conclusion: The pipeline is suitable for real-time, city-scale deployment and can be reused for other transit networks with limited adaptation.

Abstract: Urban bus transit agencies need reliable, network-wide delay predictions to provide accurate arrival information to passengers and support real-time operational control. Accurate predictions help passengers plan their trips, reduce waiting time, and allow operations staff to adjust headways, dispatch extra vehicles, and manage disruptions. Although real-time feeds such as GTFS-Realtime (GTFS-RT) are now widely available, most existing delay prediction systems handle only a few routes, depend on hand-crafted features, and offer little guidance on how to design a scalable, reusable architecture.
  We present a city-scale prediction pipeline that combines multi-resolution feature engineering, dimensionality reduction, and deep learning. The framework generates 1,683 spatiotemporal features by exploring 23 aggregation combinations over H3 cells, routes, segments, and temporal patterns, and compresses them into 83 components using Adaptive PCA while preserving 95% of the variance. To avoid the "giant cluster" problem that occurs when dense urban areas fall into a single H3 region, we introduce a hybrid H3+topology clustering method that yields 12 balanced route clusters (coefficient of variation 0.608) and enables efficient distributed training.
  We compare five model architectures on six months of bus operations from the Société de transport de Montréal (STM) network in Montréal. A global LSTM with cluster-aware features achieves the best trade-off between accuracy and efficiency, outperforming transformer models by 18 to 52% while using 275 times fewer parameters. We also report multi-level evaluation at the elementary segment, segment, and trip level with walk-forward validation and latency analysis, showing that the proposed pipeline is suitable for real-time, city-scale deployment and can be reused for other networks with limited adaptation.

</details>


### [159] [Rank-1 Approximation of Inverse Fisher for Natural Policy Gradients in Deep Reinforcement Learning](https://arxiv.org/abs/2601.18626)
*Yingxiao Huo,Satya Prakash Dash,Radu Stoican,Samuel Kaski,Mingfei Sun*

Main category: cs.LG

TL;DR: 提出一种基于秩-1近似的高效自然梯度优化方法，解决Fisher信息矩阵求逆计算瓶颈，在保持理论优势的同时实现比标准策略梯度更快的收敛和超越基线算法的性能


<details>
  <summary>Details</summary>
Motivation: 自然梯度在深度强化学习中具有收敛快和协变权重更新的优势，但每次迭代需计算Fisher信息矩阵(FIM)的逆，计算成本过高难以实用

Method: 采用秩-1近似替代完整FIM逆矩阵，理论证明该近似在特定条件下比策略梯度收敛更快，且样本复杂度与随机策略梯度方法相当

Result: 在多种环境基准测试中，该方法优于标准演员-评论家(actor-critic)和信任域(trust-region)基线算法

Conclusion: 实现了可扩展的自然策略优化技术，在降低计算复杂度的同时保持理论优势，为大规模强化学习应用提供实用解决方案

Abstract: Natural gradients have long been studied in deep reinforcement learning due to their fast convergence properties and covariant weight updates. However, computing natural gradients requires inversion of the Fisher Information Matrix (FIM) at each iteration, which is computationally prohibitive in nature. In this paper, we present an efficient and scalable natural policy optimization technique that leverages a rank-1 approximation to full inverse-FIM. We theoretically show that under certain conditions, a rank-1 approximation to inverse-FIM converges faster than policy gradients and, under some conditions, enjoys the same sample complexity as stochastic policy gradient methods. We benchmark our method on a diverse set of environments and show that it achieves superior performance to standard actor-critic and trust-region baselines.

</details>


### [160] [Enhancing Control Policy Smoothness by Aligning Actions with Predictions from Preceding States](https://arxiv.org/abs/2601.18479)
*Kyoleen Kwak,Hyoseok Hwang*

Main category: cs.LG

TL;DR: 针对深度强化学习动作高频振荡问题，提出ASAP方法，通过定义基于环境反馈的"转移诱导相似状态"，约束动作一致性与惩罚二阶差分，实现平滑控制并在Gymnasium和Isaac-Lab环境中验证了其性能优势。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习虽能解决复杂控制任务，但其产生的高频动作振荡难以应用于真实环境。现有基于损失的方法依赖启发式或人工定义的状态相似性，无法准确反映系统动力学特性。

Method: 提出"转移诱导相似状态"概念（从前一状态转移得到的下一状态分布），利用环境反馈和真实数据定义。基于此设计ASAP（Action Smoothing by Aligning Actions with Predictions from Preceding States）方法，通过约束动作与相似状态动作的一致性，并惩罚二阶差分来抑制高频振荡。

Result: 在Gymnasium和Isaac-Lab环境中的实验表明，ASAP相比现有方法能产生更平滑的控制策略，并获得更好的策略性能。

Conclusion: ASAP通过数据驱动的相似状态定义和动作平滑约束，有效解决了深度强化学习的动作振荡问题，提升了控制策略的实用性和性能。

Abstract: Deep reinforcement learning has proven to be a powerful approach to solving control tasks, but its characteristic high-frequency oscillations make it difficult to apply in real-world environments. While prior methods have addressed action oscillations via architectural or loss-based methods, the latter typically depend on heuristic or synthetic definitions of state similarity to promote action consistency, which often fail to accurately reflect the underlying system dynamics. In this paper, we propose a novel loss-based method by introducing a transition-induced similar state. The transition-induced similar state is defined as the distribution of next states transitioned from the previous state. Since it utilizes only environmental feedback and actually collected data, it better captures system dynamics. Building upon this foundation, we introduce Action Smoothing by Aligning Actions with Predictions from Preceding States (ASAP), an action smoothing method that effectively mitigates action oscillations. ASAP enforces action smoothness by aligning the actions with those taken in transition-induced similar states and by penalizing second-order differences to suppress high-frequency oscillations. Experiments in Gymnasium and Isaac-Lab environments demonstrate that ASAP yields smoother control and improved policy performance over existing methods.

</details>


### [161] [FaLW: A Forgetting-aware Loss Reweighting for Long-tailed Unlearning](https://arxiv.org/abs/2601.18650)
*Liheng Yu,Zhe Zhao,Yuxuan Wang,Pengkun Wang,Binwu Wang,Yang Wang*

Main category: cs.LG

TL;DR: 针对长尾分布下的机器遗忘问题，本文提出FaLW方法，通过实例级动态损失重加权解决异质性和偏斜遗忘偏差，实现高效数据遗忘。


<details>
  <summary>Details</summary>
Motivation: 机器遗忘对于维护数据隐私法规（如"被遗忘权"）至关重要。然而，现有研究主要在相对平衡的遗忘集上评估遗忘方法，忽略了现实世界中待遗忘数据（如用户活动记录）通常呈长尾分布的场景。

Method: 提出FaLW（遗忘感知损失重加权），一种即插即用的实例级动态损失重加权方法。它通过比较样本预测概率与同类未见数据分布来评估每个样本的遗忘状态，然后使用由平衡因子调制的遗忘感知重加权方案，自适应调整每个样本的遗忘强度。

Result: 大量实验表明，FaLW实现了卓越的性能。

Conclusion: 该研究首次填补了长尾分布下机器遗忘的研究空白，提出的FaLW方法能有效解决长尾场景下的遗忘偏差问题，为实际应用中保护用户隐私提供了新思路。

Abstract: Machine unlearning, which aims to efficiently remove the influence of specific data from trained models, is crucial for upholding data privacy regulations like the ``right to be forgotten". However, existing research predominantly evaluates unlearning methods on relatively balanced forget sets. This overlooks a common real-world scenario where data to be forgotten, such as a user's activity records, follows a long-tailed distribution. Our work is the first to investigate this critical research gap. We find that in such long-tailed settings, existing methods suffer from two key issues: \textit{Heterogeneous Unlearning Deviation} and \textit{Skewed Unlearning Deviation}. To address these challenges, we propose FaLW, a plug-and-play, instance-wise dynamic loss reweighting method. FaLW innovatively assesses the unlearning state of each sample by comparing its predictive probability to the distribution of unseen data from the same class. Based on this, it uses a forgetting-aware reweighting scheme, modulated by a balancing factor, to adaptively adjust the unlearning intensity for each sample. Extensive experiments demonstrate that FaLW achieves superior performance. Code is available at \textbf{Supplementary Material}.

</details>


### [162] [Nearly Optimal Bayesian Inference for Structural Missingness](https://arxiv.org/abs/2601.18500)
*Chen Liang,Donghua Yang,Yutong Wang,Tianle Zhang,Shenghe Zhou,Zhiyu Liang,Hengtong Zhang,Hongzhi Wang,Ziqi Li,Xiyang Zhang,Zheng Liang,Yifei Li*

Main category: cs.LG

TL;DR: 贝叶斯框架解耦缺失值推断与预测，实现不确定性传播，在60个基准测试中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 结构性缺失导致因果循环、MNAR分布偏移和单点插补偏差，传统"先插补再训练"方法失效。

Method: 采用后验预测分布积分替代单点估计，将缺失值后验学习与标签预测解耦，优化预测后验分布。

Result: 在43个分类和15个插补基准测试中达到最先进性能，在结构因果模型先验下具有有限样本近贝叶斯最优性保证。

Conclusion: 实现"模型内免费午餐"：一旦学习到后验，预测即可即插即用，同时保持不确定性传播，有效解决结构性缺失的三大挑战。

Abstract: Structural missingness breaks 'just impute and train': values can be undefined by causal or logical constraints, and the mask may depend on observed variables, unobserved variables (MNAR), and other missingness indicators. It simultaneously brings (i) a catch-22 situation with causal loop, prediction needs the missing features, yet inferring them depends on the missingness mechanism, (ii) under MNAR, the unseen are different, the missing part can come from a shifted distribution, and (iii) plug-in imputation, a single fill-in can lock in uncertainty and yield overconfident, biased decisions. In the Bayesian view, prediction via the posterior predictive distribution integrates over the full model posterior uncertainty, rather than relying on a single point estimate. This framework decouples (i) learning an in-model missing-value posterior from (ii) label prediction by optimizing the predictive posterior distribution, enabling posterior integration. This decoupling yields an in-model almost-free-lunch: once the posterior is learned, prediction is plug-and-play while preserving uncertainty propagation. It achieves SOTA on 43 classification and 15 imputation benchmarks, with finite-sample near Bayes-optimality guarantees under our SCM prior.

</details>


### [163] [Learning temporal embeddings from electronic health records of chronic kidney disease patients](https://arxiv.org/abs/2601.18675)
*Aditya Kumar,Mario A. Cypko,Oliver Amft*

Main category: cs.LG

TL;DR: Time-aware LSTM (T-LSTM) generates superior clinical embeddings from EHR data, outperforming vanilla and attention-augmented LSTMs. Embedding-based models achieve higher ICU mortality prediction accuracy (0.82-0.83) than end-to-end predictors (0.72-0.75).


<details>
  <summary>Details</summary>
Motivation: Develop clinically meaningful, task-agnostic temporal representations from longitudinal EHRs that capture disease dynamics without sacrificing predictive performance, enabling model-guided medicine rather than single-task optimization.

Method: Studied CKD patients in MIMIC-IV using three recurrent architectures (vanilla LSTM, attention-augmented LSTM, T-LSTM), trained as both embedding models and end-to-end predictors. Evaluated embedding quality via CKD stage clustering and ICU mortality prediction.

Result: T-LSTM produced most structured embeddings (DBI=9.91, CKD accuracy=0.74), surpassing vanilla LSTM (DBI=15.85, accuracy=0.63) and attention-augmented LSTM (DBI=20.72, accuracy=0.67). Embedding models consistently outperformed end-to-end predictors for mortality prediction (0.82-0.83 vs 0.72-0.75 accuracy).

Conclusion: Learning embeddings as an intermediate step is more effective than direct end-to-end learning for clinical prediction tasks. T-LSTM architecture is optimal for capturing temporal disease dynamics in EHR data while maintaining transparency and generalizability.

Abstract: We investigate whether temporal embedding models trained on longitudinal electronic health records can learn clinically meaningful representations without compromising predictive performance, and how architectural choices affect embedding quality. Model-guided medicine requires representations that capture disease dynamics while remaining transparent and task agnostic, whereas most clinical prediction models are optimised for a single task. Representation learning facilitates learning embeddings that generalise across downstream tasks, and recurrent architectures are well-suited for modelling temporal structure in observational clinical data. Using the MIMIC-IV dataset, we study patients with chronic kidney disease (CKD) and compare three recurrent architectures: a vanilla LSTM, an attention-augmented LSTM, and a time-aware LSTM (T-LSTM). All models are trained both as embedding models and as direct end-to-end predictors. Embedding quality is evaluated via CKD stage clustering and in-ICU mortality prediction. The T-LSTM produces more structured embeddings, achieving a lower Davies-Bouldin Index (DBI = 9.91) and higher CKD stage classification accuracy (0.74) than the vanilla LSTM (DBI = 15.85, accuracy = 0.63) and attention-augmented LSTM (DBI = 20.72, accuracy = 0.67). For in-ICU mortality prediction, embedding models consistently outperform end-to-end predictors, improving accuracy from 0.72-0.75 to 0.82-0.83, which indicates that learning embeddings as an intermediate step is more effective than direct end-to-end learning.

</details>


### [164] [Conformal Prediction Algorithms for Time Series Forecasting: Methods and Benchmark](https://arxiv.org/abs/2601.18509)
*Andro Sabashvili*

Main category: cs.LG

TL;DR: This review paper analyzes conformal prediction methods for time series forecasting, focusing on solutions that address the violation of exchangeability assumption due to temporal dependencies, and benchmarks four categories of algorithms for uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Traditional uncertainty quantification methods in time series forecasting rely on restrictive distributional assumptions, while conformal prediction (CP) offers distribution-free guarantees but requires data exchangeability - an assumption violated by temporal dependencies in sequential data.

Method: The paper surveys and benchmarks four main algorithmic categories: (1) methods relaxing exchangeability, (2) approaches redefining data units as independent time series collections, (3) methods explicitly modeling prediction residual dynamics, and (4) online learning algorithms adapting to distribution shifts.

Result: The review synthesizes these approaches and evaluates their computational efficiency and practical performance on real-world data, providing a comprehensive comparison of CP adaptations for time series.

Conclusion: The paper highlights that adapting conformal prediction to time series requires careful consideration of temporal dependencies, and the surveyed methods offer different trade-offs between theoretical guarantees, computational efficiency, and real-world performance.

Abstract: Reliable uncertainty quantification is of critical importance in time series forecasting, yet traditional methods often rely on restrictive distributional assumptions. Conformal prediction (CP) has emerged as a promising distribution-free framework for generating prediction intervals with rigorous theoretical guarantees. However, applying CP to sequential data presents a primary challenge: the temporal dependencies inherent in time series fundamentally violate the core assumption of data exchangeability, upon which standard CP guarantees are built. This review critically examines the main categories of algorithmic solutions designed to address this conflict. We survey and benchmark methods that relax the exchangeability assumption, those that redefine the data unit to be a collection of independent time series, approaches that explicitly model the dynamics of the prediction residuals, and online learning algorithms that adapt to distribution shifts to maintain long-run coverage. By synthesizing these approaches, we highlight computational efficiency and practical performance on real-world data.

</details>


### [165] [ART for Diffusion Sampling: A Reinforcement Learning Approach to Timestep Schedule](https://arxiv.org/abs/2601.18681)
*Yilie Huang,Wenpin Tang,Xunyu Zhou*

Main category: cs.LG

TL;DR: 提出自适应重参数化时间(ART)方法，通过强化学习优化扩散模型的时间步调度，在有限预算下提升采样质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型采用均匀或手工设计的时间网格进行离散化，在给定步数预算下并非最优，导致采样效率低下。

Method: 1) 提出ART控制重参数化时间变量的"时钟速度"，产生非均匀时间步；2) 建立ART-RL框架，将时间调度建模为连续时间强化学习问题；3) 证明求解ART-RL可恢复最优ART调度；4) 使用演员-评论家算法数据驱动地学习调度策略。

Result: 在EDM pipeline上，ART-RL在CIFAR-10上显著提升FID指标，且无需重新训练即可迁移到AFHQv2、FFHQ和ImageNet。

Conclusion: ART-RL通过数据驱动学习最优时间调度，有效提高了扩散模型的采样效率和质量，具有良好的泛化能力。

Abstract: We consider time discretization for score-based diffusion models to generate samples from a learned reverse-time dynamic on a finite grid. Uniform and hand-crafted grids can be suboptimal given a budget on the number of time steps. We introduce Adaptive Reparameterized Time (ART) that controls the clock speed of a reparameterized time variable, leading to a time change and uneven timesteps along the sampling trajectory while preserving the terminal time. The objective is to minimize the aggregate error arising from the discretized Euler scheme. We derive a randomized control companion, ART-RL, and formulate time change as a continuous-time reinforcement learning (RL) problem with Gaussian policies. We then prove that solving ART-RL recovers the optimal ART schedule, which in turn enables practical actor--critic updates to learn the latter in a data-driven way. Empirically, based on the official EDM pipeline, ART-RL improves Fréchet Inception Distance on CIFAR-10 over a wide range of budgets and transfers to AFHQv2, FFHQ, and ImageNet without the need of retraining.

</details>


### [166] [From Fuzzy to Exact: The Halo Architecture for Infinite-Depth Reasoning via Rational Arithmetic](https://arxiv.org/abs/2601.18702)
*Hansheng Ren*

Main category: cs.LG

TL;DR: 挑战深度学习重视吞吐量而非精度的范式，提出精确性假说：通用人工智能需要任意精度算术，并引入Halo架构使用有理数运算来消除数值误差


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的“幻觉”和逻辑不连贯问题源于IEEE 754浮点近似误差在深度组合函数中的累积

Method: 提出Halo架构，采用有理数运算（Q）和支持精确推理单元（EIU）

Result: Huginn-0125原型验证显示，6000亿参数BF16基线在混沌系统中崩溃时，Halo能无限期保持零数值发散

Conclusion: 精确算术是减少系统2通用人工智能逻辑不确定性的先决条件

Abstract: Current paradigms in Deep Learning prioritize computational throughput over numerical precision, relying on the assumption that intelligence emerges from statistical correlation at scale. In this paper, we challenge this orthodoxy. We propose the Exactness Hypothesis: that General Intelligence (AGI), specifically high-order causal inference, requires a computational substrate capable of Arbitrary Precision Arithmetic. We argue that the "hallucinations" and logical incoherence seen in current Large Language Models (LLMs) are artifacts of IEEE 754 floating-point approximation errors accumulating over deep compositional functions. To mitigate this, we introduce the Halo Architecture, a paradigm shift to Rational Arithmetic ($\mathbb{Q}$) supported by a novel Exact Inference Unit (EIU). Empirical validation on the Huginn-0125 prototype demonstrates that while 600B-parameter scale BF16 baselines collapse in chaotic systems, Halo maintains zero numerical divergence indefinitely. This work establishes exact arithmetic as a prerequisite for reducing logical uncertainty in System 2 AGI.

</details>


### [167] [Trust, Don't Trust, or Flip: Robust Preference-Based Reinforcement Learning with Multi-Expert Feedback](https://arxiv.org/abs/2601.18751)
*Seyed Amir Hosseini,Maryam Abdolali,Amirhosein Tavakkoli,Fardin Ayar,Ehsan Javanmardi,Manabu Tsukada,Mahdi Javanmardi*

Main category: cs.LG

TL;DR: 提出TriTrust-PBRL框架，通过联合学习共享奖励模型和专家信任参数，自动识别并反转对抗性标注者偏好，在存在不可靠和对抗性标注者的情况下实现鲁棒偏好强化学习。


<details>
  <summary>Details</summary>
Motivation: 现实世界偏好数据来自异构标注者，包含准确、噪声和系统性对抗性反馈。现有PBRL方法要么平等对待所有反馈，要么过滤不可靠源，但面对对抗性标注者时会失效，需要更鲁棒的解决方案。

Method: TriTrust-PBRL (TTP) 框架联合学习共享奖励函数和专家特定信任参数。信任参数在梯度优化中自然演化为正值（信任）、近零值（忽略）或负值（反转），使模型能自动反转对抗性偏好而非简单丢弃。提供可识别性保证和梯度分析。

Result: 在MetaWorld操作和DM Control运动任务上，TTP在对抗性腐蚀下保持接近最优性能，显著优于标准PBRL方法。能成功从包含可靠和对抗性标注者的混合专家池中学习，无需额外专家特征，且与现有PBRL流程无缝集成。

Conclusion: TTP为处理异构标注者偏好数据提供了统一鲁棒框架，通过自动信任建模实现了对抗性偏好反转，在无需专家特征的情况下达到业界最佳鲁棒性。

Abstract: Preference-based reinforcement learning (PBRL) offers a promising alternative to explicit reward engineering by learning from pairwise trajectory comparisons. However, real-world preference data often comes from heterogeneous annotators with varying reliability; some accurate, some noisy, and some systematically adversarial. Existing PBRL methods either treat all feedback equally or attempt to filter out unreliable sources, but both approaches fail when faced with adversarial annotators who systematically provide incorrect preferences. We introduce TriTrust-PBRL (TTP), a unified framework that jointly learns a shared reward model and expert-specific trust parameters from multi-expert preference feedback. The key insight is that trust parameters naturally evolve during gradient-based optimization to be positive (trust), near zero (ignore), or negative (flip), enabling the model to automatically invert adversarial preferences and recover useful signal rather than merely discarding corrupted feedback. We provide theoretical analysis establishing identifiability guarantees and detailed gradient analysis that explains how expert separation emerges naturally during training without explicit supervision. Empirically, we evaluate TTP on four diverse domains spanning manipulation tasks (MetaWorld) and locomotion (DM Control) under various corruption scenarios. TTP achieves state-of-the-art robustness, maintaining near-oracle performance under adversarial corruption while standard PBRL methods fail catastrophically. Notably, TTP outperforms existing baselines by successfully learning from mixed expert pools containing both reliable and adversarial annotators, all while requiring no expert features beyond identification indices and integrating seamlessly with existing PBRL pipelines.

</details>


### [168] [From Human Labels to Literature: Semi-Supervised Learning of NMR Chemical Shifts at Scale](https://arxiv.org/abs/2601.18524)
*Yongqi Jin,Yecheng Wang,Jun-jie Wang,Rong Zhu,Guolin Ke,Weinan E*

Main category: cs.LG

TL;DR: 提出一种半监督框架，利用数百万文献提取的未标记NMR光谱和少量标记数据预测化学位移，通过排序损失实现大规模训练，显著提升准确性和鲁棒性，并首次捕获系统性溶剂效应。


<details>
  <summary>Details</summary>
Motivation: 现有NMR化学位移预测方法依赖有限且标注成本高的原子级标记数据集，限制了模型性能和泛化能力。

Method: 将化学位移预测建模为置换不变集合监督问题，利用文献中大规模未标记光谱，在特定损失条件下将最优二分匹配简化为排序损失，实现稳定半监督训练，并整合溶剂信息。

Result: 模型精度和鲁棒性显著优于现有方法，在更大更复杂的分子数据集上泛化能力更强，首次实现了跨常见NMR溶剂的系统性溶剂效应捕获。

Conclusion: 文献挖掘的大规模未标记光谱可作为NMR位移模型训练的有效数据源，为数据驱动的科学AI研究提供了新范式，表明弱结构化文献数据在科学AI中具有广阔应用前景。

Abstract: Accurate prediction of nuclear magnetic resonance (NMR) chemical shifts is fundamental to spectral analysis and molecular structure elucidation, yet existing machine learning methods rely on limited, labor-intensive atom-assigned datasets. We propose a semi-supervised framework that learns NMR chemical shifts from millions of literature-extracted spectra without explicit atom-level assignments, integrating a small amount of labeled data with large-scale unassigned spectra. We formulate chemical shift prediction from literature spectra as a permutation-invariant set supervision problem, and show that under commonly satisfied conditions on the loss function, optimal bipartite matching reduces to a sorting-based loss, enabling stable large-scale semi-supervised training beyond traditional curated datasets. Our models achieve substantially improved accuracy and robustness over state-of-the-art methods and exhibit stronger generalization on significantly larger and more diverse molecular datasets. Moreover, by incorporating solvent information at scale, our approach captures systematic solvent effects across common NMR solvents for the first time. Overall, our results demonstrate that large-scale unlabeled spectra mined from the literature can serve as a practical and effective data source for training NMR shift models, suggesting a broader role of literature-derived, weakly structured data in data-centric AI for science.

</details>


### [169] [HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs](https://arxiv.org/abs/2601.18753)
*Xinyue Zeng,Junhong Lin,Yujun Yan,Feng Guo,Liang Shi,Jun Wu,Dawei Zhou*

Main category: cs.LG

TL;DR: 本文提出幻觉风险边界统一理论框架和基于NTK的HalluGuard检测方法，可联合识别数据驱动和推理驱动两类幻觉，在10个基准测试中实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法仅针对单一来源且依赖任务特定启发式，难以泛化到医疗、法律等高复杂度、高风险场景

Method: 1) 幻觉风险边界理论：将幻觉风险分解为训练不匹配（数据驱动）和推理不稳定（推理驱动）两部分；2) HalluGuard：利用神经正切核的几何结构和表示特征联合识别两类幻觉

Result: 在10个多样化基准、11个基线方法和9个主流LLM上均取得最先进性能，展现出强大泛化能力

Conclusion: 该理论框架为分析幻觉产生与演化提供了原则性基础，NTK-based方法有效解决了现有技术的局限性，实现了两类幻觉的联合检测

Abstract: The reliability of Large Language Models (LLMs) in high-stakes domains such as healthcare, law, and scientific discovery is often compromised by hallucinations. These failures typically stem from two sources: data-driven hallucinations and reasoning-driven hallucinations. However, existing detection methods usually address only one source and rely on task-specific heuristics, limiting their generalization to complex scenarios. To overcome these limitations, we introduce the Hallucination Risk Bound, a unified theoretical framework that formally decomposes hallucination risk into data-driven and reasoning-driven components, linked respectively to training-time mismatches and inference-time instabilities. This provides a principled foundation for analyzing how hallucinations emerge and evolve. Building on this foundation, we introduce HalluGuard, an NTK-based score that leverages the induced geometry and captured representations of the NTK to jointly identify data-driven and reasoning-driven hallucinations. We evaluate HalluGuard on 10 diverse benchmarks, 11 competitive baselines, and 9 popular LLM backbones, consistently achieving state-of-the-art performance in detecting diverse forms of LLM hallucinations.

</details>


### [170] [Closing the Modality Gap Aligns Group-Wise Semantics](https://arxiv.org/abs/2601.18525)
*Eleonora Grassucci,Giordano Cicchetti,Emanuele Frasca,Aurelio Uncini,Danilo Comminiello*

Main category: cs.LG

TL;DR: CLIP存在模态鸿沟问题，新研究发现其对聚类等分组任务影响巨大，而对检索等实例任务影响有限；提出的间隙缩减方法显著提升分组性能


<details>
  <summary>Details</summary>
Motivation: 探究CLIP模态鸿沟对不同任务的影响差异，解决其在分组任务中被忽视的关键作用，并开发一致的间隙缩减方法

Method: 提出一种新颖的两模态场景下持续缩减模态鸿沟的方法，可自然扩展到n模态通用情况

Result: 传统实例级任务（检索）仅获边际改进，而组级任务（聚类）性能显著提升

Conclusion: 模态鸿沟对语义分组任务至关重要，该发现重塑了对该现象的理解，强调其在分组任务中的关键作用

Abstract: In multimodal learning, CLIP has been recognized as the \textit{de facto} method for learning a shared latent space across multiple modalities, placing similar representations close to each other and moving them away from dissimilar ones. Although CLIP-based losses effectively align modalities at the semantic level, the resulting latent spaces often remain only partially shared, revealing a structural mismatch known as the modality gap. While the necessity of addressing this phenomenon remains debated, particularly given its limited impact on instance-wise tasks (e.g., retrieval), we prove that its influence is instead strongly pronounced in group-level tasks (e.g., clustering). To support this claim, we introduce a novel method designed to consistently reduce this discrepancy in two-modal settings, with a straightforward extension to the general $n$-modal case. Through our extensive evaluation, we demonstrate our novel insight: while reducing the gap provides only marginal or inconsistent improvements in traditional instance-wise tasks, it significantly enhances group-wise tasks. These findings may reshape our understanding of the modality gap, highlighting its key role in improving performance on tasks requiring semantic grouping.

</details>


### [171] [PRECISE: Reducing the Bias of LLM Evaluations Using Prediction-Powered Ranking Estimation](https://arxiv.org/abs/2601.18777)
*Abhishek Divekar,Anirban Majumder*

Main category: cs.LG

TL;DR: PRECISE combines minimal human annotations (100 queries) with LLM judgments to evaluate search systems reliably while correcting bias, cutting annotation needs dramatically.


<details>
  <summary>Details</summary>
Motivation: Evaluating search/ranking/RAG systems traditionally requires massive human annotation efforts, and while LLMs offer automation, their biases prevent direct use for accurate metric estimation.

Method: Extended PPI framework (PRECISE) that integrates few human annotations with many LLM judgments, reformulating metric-integration to reduce complexity from O(2^|C|) to O(2^K).

Result: Achieves reliable Precision@K estimates with 100 human queries and 10,000 unlabeled examples, reducing variance and correcting LLM bias across retrieval datasets.

Conclusion: Enables cost-effective, reliable evaluation of retrieval systems by combining limited human oversight with automated LLM judgments at scale.

Abstract: Evaluating the quality of search, ranking and RAG systems traditionally requires a significant number of human relevance annotations. In recent times, several deployed systems have explored the usage of Large Language Models (LLMs) as automated judges for this task while their inherent biases prevent direct use for metric estimation. We present a statistical framework extending Prediction-Powered Inference (PPI) that combines minimal human annotations with LLM judgments to produce reliable estimates of metrics which require sub-instance annotations. Our method requires as few as 100 human-annotated queries and 10,000 unlabeled examples, reducing annotation requirements significantly compared to traditional approaches. We formulate our proposed framework (PRECISE) for inference of relevance uplift for an LLM-based query reformulation application, extending PPI to sub-instance annotations at the query-document level. By reformulating the metric-integration space, we reduced the computational complexity from O(2^|C|) to O(2^K), where |C| represents corpus size (in order of millions). Detailed experiments across prominent retrieval datasets demonstrate that our method reduces the variance of estimates for the business-critical Precision@K metric, while effectively correcting for LLM bias in low-resource settings.

</details>


### [172] [Information Hidden in Gradients of Regression with Target Noise](https://arxiv.org/abs/2601.18546)
*Arash Jamshidi,Katsiaryna Haitsiukevich,Kai Puolamäki*

Main category: cs.LG

TL;DR: 提出方差校准方法，仅用梯度即可恢复Hessian矩阵，噪声方差设为批大小n，具有非渐近理论保证，应用于优化和鲁棒性分析。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习中常仅能获得梯度，但Hessian/数据协方差对优化、诊断和鲁棒性至关重要。直接获取二阶信息计算成本高，需从梯度中推断。

Method: 注入高斯噪声使总目标噪声方差等于批大小，经验梯度协方差即可逼近Hessian。理论分析在亚高斯输入下的非渐近算子范数误差界，并证明无校准会失败。

Result: 理论：梯度协方差≈Hessian，提供非渐近误差界，校准必要且充分。实践：方法简单鲁棒，O(n)方差即可恢复Σ至尺度，合成和真实数据实验验证。

Conclusion: 实现了从一阶梯度有效提取二阶信息，为预条件优化、对抗风险估计、分布式纯梯度训练等应用提供新工具，兼具理论和实用价值。

Abstract: Second-order information -- such as curvature or data covariance -- is critical for optimisation, diagnostics, and robustness. However, in many modern settings, only the gradients are observable. We show that the gradients alone can reveal the Hessian, equalling the data covariance $Σ$ for the linear regression. Our key insight is a simple variance calibration: injecting Gaussian noise so that the total target noise variance equals the batch size ensures that the empirical gradient covariance closely approximates the Hessian, even when evaluated far from the optimum. We provide non-asymptotic operator-norm guarantees under sub-Gaussian inputs. We also show that without such calibration, recovery can fail by an $Ω(1)$ factor. The proposed method is practical (a "set target-noise variance to $n$" rule) and robust (variance $\mathcal{O}(n)$ suffices to recover $Σ$ up to scale). Applications include preconditioning for faster optimisation, adversarial risk estimation, and gradient-only training, for example, in distributed systems. We support our theoretical results with experiments on synthetic and real data.

</details>


### [173] [POPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration](https://arxiv.org/abs/2601.18779)
*Yuxiao Qu,Amrith Setlur,Virginia Smith,Ruslan Salakhutdinov,Aviral Kumar*

Main category: cs.LG

TL;DR: POPE is a reinforcement learning method that uses oracle solution prefixes as privileged information to guide exploration on hard problems, enabling LLMs to get non-zero rewards and learn effectively, while avoiding the issues of ray interference from mixing easy/hard problems.


<details>
  <summary>Details</summary>
Motivation: On-policy RL for LLMs struggles with hard problems due to zero-reward rollouts and lack of learning signal. Classical RL fixes don't work, and mixing easy/hard problems causes "ray interference" that inhibits progress on hard problems.

Method: Privileged On-Policy Exploration (POPE) augments hard problems with oracle solution prefixes to guide rollouts and obtain non-zero rewards. Unlike off-policy methods, it uses privileged information only for exploration, not training targets.

Result: POPE expands the set of solvable problems and substantially improves performance on challenging reasoning benchmarks by enabling effective exploration and transferring learned behaviors back to unguided problems.

Conclusion: POPE successfully addresses the exploration-exploitation challenge in RL for LLMs by leveraging privileged information without suffering from ray interference, demonstrating that guided exploration can substantially improve solvability on hard reasoning tasks.

Abstract: Reinforcement learning (RL) has improved the reasoning abilities of large language models (LLMs), yet state-of-the-art methods still fail to learn on many training problems. On hard problems, on-policy RL rarely explores even a single correct rollout, yielding zero reward and no learning signal for driving improvement. We find that natural solutions to remedy this exploration problem from classical RL, such as entropy bonuses, more permissive clipping of the importance ratio, or direct optimization of pass@k objectives, do not resolve this issue and often destabilize optimization without improving solvability. A natural alternative is to leverage transfer from easier problems. However, we show that mixing easy and hard problems during RL training is counterproductive due to ray interference, where optimization focuses on already-solvable problems in a way that actively inhibits progress on harder ones. To address this challenge, we introduce Privileged On-Policy Exploration (POPE), an approach that leverages human- or other oracle solutions as privileged information to guide exploration on hard problems, unlike methods that use oracle solutions as training targets (e.g., off-policy RL methods or warmstarting from SFT). POPE augments hard problems with prefixes of oracle solutions, enabling RL to obtain non-zero rewards during guided rollouts. Crucially, the resulting behaviors transfer back to the original, unguided problems through a synergy between instruction-following and reasoning. Empirically, POPE expands the set of solvable problems and substantially improves performance on challenging reasoning benchmarks.

</details>


### [174] [An Unsupervised Tensor-Based Domain Alignment](https://arxiv.org/abs/2601.18564)
*Chong Hyun Lee,Kibae Lee,Hyun Hee Yim*

Main category: cs.LG

TL;DR: 提出一种基于张量的域对齐算法，通过在斜流形上迭代优化对齐矩阵和不变子空间，并引入保持方差的规整项，实现更快速、更准确的域适应。


<details>
  <summary>Details</summary>
Motivation: 传统Stiefel流形约束灵活性不足，难以有效应对复杂域适应任务的需求。

Method: 设计张量域对齐算法，利用对齐矩阵在不变子空间内配准源和目标张量，在斜流形上进行迭代优化，并定义保持方差的规整项确保鲁棒性。

Result: 实验表明该方法不仅提升了域对齐转换速度，还显著提高了分类准确率，优于现有先进方法。

Conclusion: 该框架具有通用性，可将现有张量域对齐方法泛化为特例，是复杂域适应任务的优选方案。

Abstract: We propose a tensor-based domain alignment (DA) algorithm designed to align source and target tensors within an invariant subspace through the use of alignment matrices. These matrices along with the subspace undergo iterative optimization of which constraint is on oblique manifold, which offers greater flexibility and adaptability compared to the traditional Stiefel manifold. Moreover, regularization terms defined to preserve the variance of both source and target tensors, ensures robust performance. Our framework is versatile, effectively generalizing existing tensor-based DA methods as special cases. Through extensive experiments, we demonstrate that our approach not only enhances DA conversion speed but also significantly boosts classification accuracy. This positions our method as superior to current state-of-the-art techniques, making it a preferable choice for complex domain adaptation tasks.

</details>


### [175] [Multi-Objective Reinforcement Learning for Efficient Tactical Decision Making for Trucks in Highway Traffic](https://arxiv.org/abs/2601.18783)
*Deepthi Pathare,Leo Laine,Morteza Haghir Chehreghani*

Main category: cs.LG

TL;DR: 提出一种基于近端策略优化的多目标强化学习框架，为重型卡车在高速公路场景下学习连续帕累托最优策略集，实现安全性、能效和时间成本的平衡，无需重新训练即可灵活切换驾驶策略。


<details>
  <summary>Details</summary>
Motivation: 重型车辆在高速公路驾驶中需要平衡安全性、效率和运营成本等多个冲突目标，但传统的标量奖励函数会掩盖这些目标间的权衡结构。

Method: 采用基于近端策略优化（PPO）的多目标强化学习方法，在可扩展的卡车战术决策仿真平台上，学习能明确表示目标权衡的连续策略集合。

Result: 学习到包含安全性（碰撞与任务完成）、能效（能耗成本）和时间效率（司机成本）三个冲突目标的连续帕累托最优策略集，形成平滑可解释的帕累托前沿。

Conclusion: 该框架可在不同驾驶策略间无缝切换且无需重新训练，为自动驾驶卡车提供了鲁棒自适应的决策策略。

Abstract: Balancing safety, efficiency, and operational costs in highway driving poses a challenging decision-making problem for heavy-duty vehicles. A central difficulty is that conventional scalar reward formulations, obtained by aggregating these competing objectives, often obscure the structure of their trade-offs. We present a Proximal Policy Optimization based multi-objective reinforcement learning framework that learns a continuous set of policies explicitly representing these trade-offs and evaluates it on a scalable simulation platform for tactical decision making in trucks. The proposed approach learns a continuous set of Pareto-optimal policies that capture the trade-offs among three conflicting objectives: safety, quantified in terms of collisions and successful completion; energy efficiency and time efficiency, quantified using energy cost and driver cost, respectively. The resulting Pareto frontier is smooth and interpretable, enabling flexibility in choosing driving behavior along different conflicting objectives. This framework allows seamless transitions between different driving policies without retraining, yielding a robust and adaptive decision-making strategy for autonomous trucking applications.

</details>


### [176] [K-Myriad: Jump-starting reinforcement learning with unsupervised parallel agents](https://arxiv.org/abs/2601.18580)
*Vincenzo De Paola,Mirco Mutti,Riccardo Zamboni,Marcello Restelli*

Main category: cs.LG

TL;DR: K-Myriad is a scalable unsupervised method that uses parallel policies to maximize collective state entropy, creating diverse exploration strategies for better RL initialization and heterogeneous solutions.


<details>
  <summary>Details</summary>
Motivation: Traditional parallel RL uses identical workers for single-policy speedup, ignoring the potential of diverse exploration strategies among parallel agents.

Method: Proposes K-Myriad to maximize collective state entropy from a population of parallel policies, cultivating specialized exploration strategies without supervision.

Result: Demonstrates higher training efficiency and discovery of heterogeneous solutions on high-dimensional continuous control tasks with large-scale parallelization.

Conclusion: K-Myriad effectively enables collective exploration by learning diverse policies, advancing novel parallelization strategies in RL.

Abstract: Parallelization in Reinforcement Learning is typically employed to speed up the training of a single policy, where multiple workers collect experience from an identical sampling distribution. This common design limits the potential of parallelization by neglecting the advantages of diverse exploration strategies. We propose K-Myriad, a scalable and unsupervised method that maximizes the collective state entropy induced by a population of parallel policies. By cultivating a portfolio of specialized exploration strategies, K-Myriad provides a robust initialization for Reinforcement Learning, leading to both higher training efficiency and the discovery of heterogeneous solutions. Experiments on high-dimensional continuous control tasks, with large-scale parallelization, demonstrate that K-Myriad can learn a broad set of distinct policies, highlighting its effectiveness for collective exploration and paving the way towards novel parallelization strategies.

</details>


### [177] [Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes](https://arxiv.org/abs/2601.18795)
*Amrith Setlur,Zijian Wang,Andrew Cohen,Paria Rashidinejad,Sang Michael Xie*

Main category: cs.LG

TL;DR: PrefixRL is a novel reinforcement learning method that efficiently bootstraps LLM reasoning by conditioning on prefixes of successful off-policy traces, avoiding instability while achieving 2x faster training and 3x higher final rewards on hard problems.


<details>
  <summary>Details</summary>
Motivation: Standard RL for LLM reasoning is inefficient on hard problems due to rare correct traces, vanishing gradients, and stalled learning; reusing off-policy traces from prior computation could improve efficiency but causes optimization instability.

Method: PrefixRL conditions on prefixes of successful off-policy traces and runs on-policy RL to complete them, modulating problem difficulty by prefix length; theoretically proven consistent with standard RL and more sample-efficient.

Result: Achieves 2x faster training to same reward and 3x higher final reward vs. strongest baseline; discovers back-generalization where prefixed training transfers to unprefixed OOD problems; effective across model families.

Conclusion: PrefixRL successfully leverages off-policy computation for stable, efficient RL on hard reasoning problems with strong generalization and practical flexibility.

Abstract: Typical reinforcement learning (RL) methods for LLM reasoning waste compute on hard problems, where correct on-policy traces are rare, policy gradients vanish, and learning stalls. To bootstrap more efficient RL, we consider reusing old sampling FLOPs (from prior inference or RL training) in the form of off-policy traces. Standard off-policy methods supervise against off-policy data, causing instabilities during RL optimization. We introduce PrefixRL, where we condition on the prefix of successful off-policy traces and run on-policy RL to complete them, side-stepping off-policy instabilities. PrefixRL boosts the learning signal on hard problems by modulating the difficulty of the problem through the off-policy prefix length. We prove that the PrefixRL objective is not only consistent with the standard RL objective but also more sample efficient. Empirically, we discover back-generalization: training only on prefixed problems generalizes to out-of-distribution unprefixed performance, with learned strategies often differing from those in the prefix. In our experiments, we source the off-policy traces by rejection sampling with the base model, creating a self-improvement loop. On hard reasoning problems, PrefixRL reaches the same training reward 2x faster than the strongest baseline (SFT on off-policy data then RL), even after accounting for the compute spent on the initial rejection sampling, and increases the final reward by 3x. The gains transfer to held-out benchmarks, and PrefixRL is still effective when off-policy traces are derived from a different model family, validating its flexibility in practical settings.

</details>


### [178] [LaCoGSEA: Unsupervised deep learning for pathway analysis via latent correlation](https://arxiv.org/abs/2601.18604)
*Zhiwei Zheng,Kevin Bryson*

Main category: cs.LG

TL;DR: LaCoGSEA is an unsupervised pathway enrichment framework that combines autoencoders with a novel gene-latent correlation metric to identify biologically meaningful pathways without needing phenotypic labels, outperforming existing methods in clustering, pathway recovery, and robustness.


<details>
  <summary>Details</summary>
Motivation: Standard pathway enrichment analysis methods require predefined phenotypic labels and pairwise comparisons, limiting their use in unsupervised scenarios. Existing unsupervised extensions primarily capture linear relationships and don't explicitly model gene-pathway associations. While deep learning models can capture non-linear transcriptomic structure, their interpretation relies on generic explainable AI techniques not designed for pathway-level interpretation in unsupervised transcriptomic analyses.

Method: LaCoGSEA (Latent Correlation GSEA) integrates deep representation learning with robust pathway statistics. It employs an autoencoder to capture non-linear manifolds in gene expression data and proposes a global gene-latent correlation metric as a proxy for differential expression to generate dense gene rankings without prior labels.

Result: LaCoGSEA demonstrates three key advantages: (i) improved clustering performance in distinguishing cancer subtypes compared to existing unsupervised baselines; (ii) recovery of a broader range of biologically meaningful pathways at higher ranks compared with linear dimensionality reduction and gradient-based XAI methods; and (iii) high robustness and consistency across varying experimental protocols and dataset sizes, achieving state-of-the-art performance in unsupervised pathway enrichment analysis.

Conclusion: LaCoGSEA provides a superior unsupervised framework for pathway enrichment analysis by effectively capturing non-linear transcriptomic structure and explicitly modeling gene-pathway associations through a novel latent correlation approach, offering improved performance, biological relevance, and robustness over existing methods.

Abstract: Motivation: Pathway enrichment analysis is widely used to interpret gene expression data. Standard approaches, such as GSEA, rely on predefined phenotypic labels and pairwise comparisons, which limits their applicability in unsupervised settings. Existing unsupervised extensions, including single-sample methods, provide pathway-level summaries but primarily capture linear relationships and do not explicitly model gene-pathway associations. More recently, deep learning models have been explored to capture non-linear transcriptomic structure. However, their interpretation has typically relied on generic explainable AI (XAI) techniques designed for feature-level attribution. As these methods are not designed for pathway-level interpretation in unsupervised transcriptomic analyses, their effectiveness in this setting remains limited.
  Results: To bridge this gap, we introduce LaCoGSEA (Latent Correlation GSEA), an unsupervised framework that integrates deep representation learning with robust pathway statistics. LaCoGSEA employs an autoencoder to capture non-linear manifolds and proposes a global gene-latent correlation metric as a proxy for differential expression, generating dense gene rankings without prior labels. We demonstrate that LaCoGSEA offers three key advantages: (i) it achieves improved clustering performance in distinguishing cancer subtypes compared to existing unsupervised baselines; (ii) it recovers a broader range of biologically meaningful pathways at higher ranks compared with linear dimensionality reduction and gradient-based XAI methods; and (iii) it maintains high robustness and consistency across varying experimental protocols and dataset sizes. Overall, LaCoGSEA provides state-of-the-art performance in unsupervised pathway enrichment analysis.
  Availability and implementation: https://github.com/willyzzz/LaCoGSEA

</details>


### [179] [Geometry-Free Conditional Diffusion Modeling for Solving the Inverse Electrocardiography Problem](https://arxiv.org/abs/2601.18615)
*Ramiro Valdes Jara,Adam Meyers*

Main category: cs.LG

TL;DR: 提出一种基于条件扩散模型的几何无关、数据驱动的逆问题求解框架，用于心电图成像(ECGI)，可概率性地生成多个重建结果而非单一确定估计，在真实数据集上优于CNN/LSTM/Transformer等确定性基线方法。


<details>
  <summary>Details</summary>
Motivation: ECGI逆问题是欠定的、非唯一性的数学问题，传统方法依赖患者特异性网格构建，且现有确定性方法无法捕捉解的不确定性。

Method: 采用条件扩散框架学习从噪声体表信号到心表电位的概率映射，实现几何无关、纯数据驱动的建模，支持多重建结果采样。

Result: 在真实ECGI数据集上评估，相比CNN、LSTM和Transformer等强确定性基线，该扩散方法实现了更高的重建精度。

Conclusion: 扩散模型作为无创心脏电生理成像的鲁棒工具具有巨大潜力，为ECGI提供了新的概率性解决方案。

Abstract: This paper proposes a data-driven model for solving the inverse problem of electrocardiography, the mathematical problem that forms the basis of electrocardiographic imaging (ECGI). We present a conditional diffusion framework that learns a probabilistic mapping from noisy body surface signals to heart surface electric potentials. The proposed approach leverages the generative nature of diffusion models to capture the non-unique and underdetermined nature of the ECGI inverse problem, enabling probabilistic sampling of multiple reconstructions rather than a single deterministic estimate. Unlike traditional methods, the proposed framework is geometry-free and purely data-driven, alleviating the need for patient-specific mesh construction. We evaluate the method on a real ECGI dataset and compare it against strong deterministic baselines, including a convolutional neural network, long short-term memory network, and transformer-based model. The results demonstrate that the proposed diffusion approach achieves improved reconstruction accuracy, highlighting the potential of diffusion models as a robust tool for noninvasive cardiac electrophysiology imaging.

</details>


### [180] [CASSANDRA: Programmatic and Probabilistic Learning and Inference for Stochastic World Modeling](https://arxiv.org/abs/2601.18620)
*Panagiotis Lymperopoulos,Abhiramon Rajasekharan,Ian Berlot-Attwell,Stéphane Aroca-Ouellette,Kaheer Suleman*

Main category: cs.LG

TL;DR: CASSANDRA is a neurosymbolic world modeling approach that uses LLMs as knowledge priors to build lightweight transition models for planning in business domains, combining LLM-synthesized code for deterministic features with LLM-guided probabilistic graphical models for causal relationships, showing significant improvements over baselines in coffee-shop and theme park simulators.


<details>
  <summary>Details</summary>
Motivation: Building world models for real-world planning is challenging due to rich semantics and limited data; traditional methods struggle to model complex action effects and causal relationships, but leveraging world knowledge from LLMs can overcome these limitations.

Method: CASSANDRA integrates two components: (1) LLM-synthesized code to model deterministic features, and (2) LLM-guided structure learning of a probabilistic graphical model to capture causal relationships among stochastic variables, combining symbolic reasoning with neural networks.

Result: Evaluated on a small-scale coffee-shop simulator and a complex theme park business simulator, CASSANDRA demonstrates significant improvements in transition prediction and planning performance over baseline methods.

Conclusion: The neurosymbolic approach effectively leverages LLM knowledge priors to build lightweight, accurate world models for planning in complex domains, showing promise for real-world business applications.

Abstract: Building world models is essential for planning in real-world domains such as businesses. Since such domains have rich semantics, we can leverage world knowledge to effectively model complex action effects and causal relationships from limited data. In this work, we propose CASSANDRA, a neurosymbolic world modeling approach that leverages an LLM as a knowledge prior to construct lightweight transition models for planning. CASSANDRA integrates two components: (1) LLM-synthesized code to model deterministic features, and (2) LLM-guided structure learning of a probabilistic graphical model to capture causal relationships among stochastic variables. We evaluate CASSANDRA in (i) a small-scale coffee-shop simulator and (ii) a complex theme park business simulator, where we demonstrate significant improvements in transition prediction and planning over baselines.

</details>


### [181] [TwinPurify: Purifying gene expression data to reveal tumor-intrinsic transcriptional programs via self-supervised learning](https://arxiv.org/abs/2601.18640)
*Zhiwei Zheng,Kevin Bryson*

Main category: cs.LG

TL;DR: TwinPurify uses self-supervised learning (Barlow Twins) with adjacent-normal samples as guidance to learn tumor-specific embeddings from bulk transcriptomics, outperforming traditional deconvolution methods and improving downstream analysis without needing external references.


<details>
  <summary>Details</summary>
Motivation: 单细胞和空间转录组技术虽然先进，但大规模临床研究仍依赖批量转录组数据。然而，肿瘤纯度的变化掩盖了肿瘤内在的转录信号，限制了下游发现。许多反卷积方法在合成数据上表现良好，但由于未建模的生物和技术变异，无法推广到真实患者队列。

Method: TwinPurify采用Barlow Twins自监督学习目标，学习连续的高维肿瘤嵌入表示。该方法利用同队列中的癌旁正常样本作为"背景"指导，无需外部参考即可解离肿瘤特异性信号，从根本上不同于传统的反卷积范式。

Result: 在多个大型癌症队列和不同平台（RNA-seq和芯片）上，TwinPurify在恢复肿瘤内在信号和免疫信号方面优于传统表示学习方法（如自编码器）。纯化的嵌入表示改善了分子亚型和分级分类，增强了生存模型的一致性，并揭示了更多生物学意义的通路活性。

Conclusion: TwinPurify提供了一个可迁移的框架，用于"净化"批量转录组数据，扩展了现有临床数据集在分子发现中的实用性。

Abstract: Advances in single-cell and spatial transcriptomic technologies have transformed tumor ecosystem profiling at cellular resolution. However, large scale studies on patient cohorts continue to rely on bulk transcriptomic data, where variation in tumor purity obscures tumor-intrinsic transcriptional signals and constrains downstream discovery. Many deconvolution methods report strong performance on synthetic bulk mixtures but fail to generalize to real patient cohorts because of unmodeled biological and technical variation.
  Here, we introduce TwinPurify, a representation learning framework that adapts the Barlow Twins self-supervised objective, representing a fundamental departure from the deconvolution paradigm. Rather than resolving the bulk mixture into discrete cell-type fractions, TwinPurify instead learns continuous, high-dimensional tumor embeddings by leveraging adjacent-normal profiles within the same cohort as "background" guidance, enabling the disentanglement of tumor-specific signals without relying on any external reference.
  Benchmarked against multiple large cancer cohorts across RNA-seq and microarray platforms, TwinPurify outperforms conventional representation learning baselines like auto-encoders in recovering tumor-intrinsic and immune signals. The purified embeddings improve molecular subtype and grade classification, enhance survival model concordance, and uncover biologically meaningful pathway activities compared to raw bulk profiles. By providing a transferable framework for decontaminating bulk transcriptomics, TwinPurify extends the utility of existing clinical datasets for molecular discovery.

</details>


### [182] [Quasi Monte Carlo methods enable extremely low-dimensional deep generative models](https://arxiv.org/abs/2601.18676)
*Miles Martinez,Alex H. Williams*

Main category: cs.LG

TL;DR: 提出准蒙特卡洛潜变量模型(QLVMs)，通过随机准蒙特卡洛积分直接近似边际似然，在1-3维潜空间中实现高维数据的可解释嵌入，性能优于VAEs/IWAEs但计算成本高。


<details>
  <summary>Details</summary>
Motivation: 标准深度生成模型依赖编码器和变分下界，难以获得极低维且可解释的潜空间。QLVMs旨在通过直接优化边际似然，实现透明可视化和后验分析，解决高维数据低维嵌入的可解释性问题。

Method: 采用随机准蒙特卡洛积分直接近似边际似然，避免变分下界和编码器学习的限制，专注于1-3维潜空间建模。

Result: 在多个数据集上，QLVMs在匹配潜维度下持续优于VAEs和IWAEs，支持非参数密度估计、聚类和测地线路径计算等可解释分析。

Conclusion: QLVMs为重视可解释性和潜空间分析的应用提供了可行方案，虽计算密集且在复杂数据上难以捕捉细节，但在低维场景下优势显著。

Abstract: This paper introduces quasi-Monte Carlo latent variable models (QLVMs): a class of deep generative models that are specialized for finding extremely low-dimensional and interpretable embeddings of high-dimensional datasets. Unlike standard approaches, which rely on a learned encoder and variational lower bounds, QLVMs directly approximate the marginal likelihood by randomized quasi-Monte Carlo integration. While this brute force approach has drawbacks in higher-dimensional spaces, we find that it excels in fitting one, two, and three dimensional deep latent variable models. Empirical results on a range of datasets show that QLVMs consistently outperform conventional variational autoencoders (VAEs) and importance weighted autoencoders (IWAEs) with matched latent dimensionality. The resulting embeddings enable transparent visualization and post hoc analyses such as nonparametric density estimation, clustering, and geodesic path computation, which are nontrivial to validate in higher-dimensional spaces. While our approach is compute-intensive and struggles to generate fine-scale details in complex datasets, it offers a compelling solution for applications prioritizing interpretability and latent space analysis.

</details>


### [183] [Counterfactual Explanations on Robust Perceptual Geodesics](https://arxiv.org/abs/2601.18678)
*Eslam Zaher,Maciej Trzaskowski,Quan Nguyen,Fred Roosta*

Main category: cs.LG

TL;DR: PCG方法通过感知黎曼几何构建反事实解释，实现语义有效且流形上的平滑过渡，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法因距离度量选择导致歧义，产生非语义或对抗性扰动，存在离流形伪影、语义漂移或对抗性崩溃问题。

Method: 提出感知反事实测地线（PCG），利用鲁棒视觉特征诱导的感知黎曼度量来追踪测地线构建反事实。

Result: 在三个视觉数据集上的实验表明，PCG优于基线方法，并揭示了标准度量下隐藏的错误模式。

Conclusion: PCG通过人类感知对齐的几何结构，惩罚脆弱方向，实现平滑、流形上、语义有效的转换。

Abstract: Latent-space optimization methods for counterfactual explanations - framed as minimal semantic perturbations that change model predictions - inherit the ambiguity of Wachter et al.'s objective: the choice of distance metric dictates whether perturbations are meaningful or adversarial. Existing approaches adopt flat or misaligned geometries, leading to off-manifold artifacts, semantic drift, or adversarial collapse. We introduce Perceptual Counterfactual Geodesics (PCG), a method that constructs counterfactuals by tracing geodesics under a perceptually Riemannian metric induced from robust vision features. This geometry aligns with human perception and penalizes brittle directions, enabling smooth, on-manifold, semantically valid transitions. Experiments on three vision datasets show that PCG outperforms baselines and reveals failure modes hidden under standard metrics.

</details>


### [184] [Explainability Methods for Hardware Trojan Detection: A Systematic Comparison](https://arxiv.org/abs/2601.18696)
*Paul Whitten,Francis Wolff,Chris Papachristou*

Main category: cs.LG

TL;DR: 该论文比较了三种可解释性方法（领域感知的属性分析、基于案例的推理和模型无关的特征归因）用于门级硬件木马检测，发现领域对齐的方法比通用特征排序更适合安全工程师验证ML预测结果。


<details>
  <summary>Details</summary>
Motivation: 硬件木马检测不仅需要准确的识别，还需要可解释的解释，以便安全工程师验证和响应结果。现有方法缺乏对电路级上下文的理解，难以让领域专家信任和验证。

Method: 在Trust-Hub基准上比较三类可解释性方法：1) 基于31个电路特定属性（扇入模式、触发器距离、I/O连接）的领域感知分析；2) 使用k近邻的基于案例推理；3) 模型无关的特征归因（LIME、SHAP、梯度）。使用XGBoost分类器进行检测。

Result: XGBoost在11,392个测试样本上达到46.15%精确率和52.17%召回率，精确率较先前工作提升9倍，假阳性率从5.6%降至0.25%。案例推理与训练样本的预测对应率达97.4%。LIME和SHAP方法间相关性高(r=0.94)但缺乏电路级上下文。梯度归因比SHAP快481倍但提供类似的不透明洞察。

Conclusion: 与通用特征排序相比，基于属性和案例的方法提供了更好的领域对齐和基于先例的可解释性，这对需要验证ML预测的XAI部署具有重要意义。领域专家更倾向于可理解、可验证的解释方式。

Abstract: Hardware trojan detection requires accurate identification and interpretable explanations for security engineers to validate and act on results. This work compares three explainability categories for gate-level trojan detection on the Trust-Hub benchmark: (1) domain-aware property-based analysis of 31 circuit-specific features from gate fanin patterns, flip-flop distances, and I/O connectivity; (2) case-based reasoning using k-nearest neighbors for precedent-based explanations; and (3) model-agnostic feature attribution (LIME, SHAP, gradient).
  Results show different advantages per approach. Property-based analysis provides explanations through circuit concepts like "high fanin complexity near outputs indicates potential triggers." Case-based reasoning achieves 97.4% correspondence between predictions and training exemplars, offering justifications grounded in precedent. LIME and SHAP provide feature attributions with strong inter-method correlation (r=0.94, p<0.001) but lack circuit-level context for validation.
  XGBoost classification achieves 46.15% precision and 52.17% recall on 11,392 test samples, a 9-fold precision improvement over prior work (Hasegawa et al.: 5.13%) while reducing false positive rates from 5.6% to 0.25%. Gradient-based attribution runs 481 times faster than SHAP but provides similar domain-opaque insights.
  This work demonstrates that property-based and case-based approaches offer domain alignment and precedent-based interpretability compared to generic feature rankings, with implications for XAI deployment where practitioners must validate ML predictions.

</details>


### [185] [Mechanistic Analysis of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning](https://arxiv.org/abs/2601.18699)
*Olaf Yunus Laitinen Imanov*

Main category: cs.LG

TL;DR: 本文对大规模语言模型在连续微调过程中的灾难性遗忘进行了系统性机理分析，识别出三种关键机制，并发现遗忘程度与任务相似度及梯度对齐存在强相关性。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型在连续微调时会出现灾难性遗忘现象，但其背后的机理理解有限，这限制了有效缓解策略的开发。

Method: 通过对109B至400B参数的Transformer模型在多任务序列上进行系统实验，分析梯度和表征变化来揭示遗忘机制。

Result: 识别出注意力权重中的梯度干扰、中间层的表征漂移和损失景观平坦化三种主要机制；遗忘严重程度与任务相似度强相关（Pearson r=0.87）；约15-23%的注意力头受到严重破坏，且底层更易受影响。

Conclusion: 这些发现为开发持续学习系统的针对性缓解策略奠定了机理基础。

Abstract: Large language models exhibit remarkable performance across diverse tasks through pre-training and fine-tuning paradigms. However, continual fine-tuning on sequential tasks induces catastrophic forgetting, where newly acquired knowledge interferes with previously learned capabilities. Despite widespread observations of this phenomenon, the mechanistic understanding remains limited. Here, we present a comprehensive mechanistic analysis of catastrophic forgetting in transformer-based LLMs during sequential fine-tuning. Through systematic experiments across multiple model scales (109B to 400B total parameters) and task sequences, we identify three primary mechanisms driving forgetting: gradient interference in attention weights, representational drift in intermediate layers, and loss landscape flattening. We demonstrate that forgetting severity correlates strongly with task similarity (Pearson r = 0.87) and gradient alignment metrics. Our analysis reveals that approximately 15 to 23 percent of attention heads undergo severe disruption during fine-tuning, with lower layers showing greater susceptibility. These findings establish mechanistic foundations for developing targeted mitigation strategies in continual learning systems.

</details>


### [186] [Riemannian AmbientFlow: Towards Simultaneous Manifold Learning and Generative Modeling from Corrupted Data](https://arxiv.org/abs/2601.18728)
*Willem Diepeveen,Oscar Leong*

Main category: cs.LG

TL;DR: 提出Riemannian AmbientFlow框架，从含噪观测中同时学习概率生成模型和非线性数据流形，具备理论保证并在合成数据和MNIST上验证有效。


<details>
  <summary>Details</summary>
Motivation: 科学和成像应用中通常无法获得干净样本，只能观测到含噪或线性退化数据，而数据中潜在的低维流形结构对下游分析至关重要，需要从退化数据中同时学习生成模型和流形几何。

Method: 基于AmbientFlow变分推断框架，引入归一化流驱动的数据驱动黎曼几何，利用拉回度量和黎曼自编码器提取流形结构。

Result: 理论证明在适当的几何正则化和测量条件下，可恢复底层分布至可控误差，并获得光滑双Lipschitz流形参数化；光滑解码器可作为逆问题的生成先验并提供恢复保证；在低维合成流形和MNIST上验证有效。

Conclusion: Riemannian AmbientFlow成功实现了从含噪观测中联合学习生成模型和潜在流形，兼具理论保证和实际应用价值。

Abstract: Modern generative modeling methods have demonstrated strong performance in learning complex data distributions from clean samples. In many scientific and imaging applications, however, clean samples are unavailable, and only noisy or linearly corrupted measurements can be observed. Moreover, latent structures, such as manifold geometries, present in the data are important to extract for further downstream scientific analysis. In this work, we introduce Riemannian AmbientFlow, a framework for simultaneously learning a probabilistic generative model and the underlying, nonlinear data manifold directly from corrupted observations. Building on the variational inference framework of AmbientFlow, our approach incorporates data-driven Riemannian geometry induced by normalizing flows, enabling the extraction of manifold structure through pullback metrics and Riemannian Autoencoders. We establish theoretical guarantees showing that, under appropriate geometric regularization and measurement conditions, the learned model recovers the underlying data distribution up to a controllable error and yields a smooth, bi-Lipschitz manifold parametrization. We further show that the resulting smooth decoder can serve as a principled generative prior for inverse problems with recovery guarantees. We empirically validate our approach on low-dimensional synthetic manifolds and on MNIST.

</details>


### [187] [Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models](https://arxiv.org/abs/2601.18734)
*Siyan Zhao,Zhihui Xie,Mengchen Liu,Jing Huang,Guan Pang,Feiyu Chen,Aditya Grover*

Main category: cs.LG

TL;DR: Proposes On-Policy Self-Distillation (OPSD), where a single LLM acts as both teacher (with privileged reasoning traces) and student (without traces) to eliminate distribution mismatch and improve token efficiency in reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing on-policy distillation requires separate teacher models and fails to leverage ground-truth solutions, causing distribution mismatch and inefficiency; OPSD addresses this by enabling self-distillation using privileged information within one model.

Method: Single model conditions on different contexts: teacher policy accesses verified reasoning traces (privileged info), student policy sees only questions; training minimizes per-token divergence between these policies over student's own rollouts.

Result: Achieves 4-8x higher token efficiency than RL methods (e.g., GRPO) and outperforms off-policy distillation on mathematical reasoning benchmarks.

Conclusion: OPSD eliminates need for external teachers and effectively utilizes ground-truth data, setting new efficiency and performance standards for LLM reasoning distillation.

Abstract: Knowledge distillation improves large language model (LLM) reasoning by compressing the knowledge of a teacher LLM to train smaller LLMs. On-policy distillation advances this approach by having the student sample its own trajectories while a teacher LLM provides dense token-level supervision, addressing the distribution mismatch between training and inference in off-policy distillation methods. However, on-policy distillation typically requires a separate, often larger, teacher LLM and does not explicitly leverage ground-truth solutions available in reasoning datasets. Inspired by the intuition that a sufficiently capable LLM can rationalize external privileged reasoning traces and teach its weaker self (i.e., the version without access to privileged information), we introduce On-Policy Self-Distillation (OPSD), a framework where a single model acts as both teacher and student by conditioning on different contexts. The teacher policy conditions on privileged information (e.g., verified reasoning traces) while the student policy sees only the question; training minimizes the per-token divergence between these distributions over the student's own rollouts. We demonstrate the efficacy of our method on multiple mathematical reasoning benchmarks, achieving 4-8x token efficiency compared to reinforcement learning methods such as GRPO and superior performance over off-policy distillation methods.

</details>


### [188] [Benchmarking Machine Learning Models for IoT Malware Detection under Data Scarcity and Drift](https://arxiv.org/abs/2601.18736)
*Jake Lyon,Ehsan Saeedizade,Shamik Sengupta*

Main category: cs.LG

TL;DR: This study evaluates four machine learning models for IoT malware detection using the IoT-23 dataset, finding that tree-based models (Random Forest and LightGBM) perform best even with limited data, but their effectiveness declines over time as malware evolves, emphasizing the need for adaptive, lightweight security solutions in resource-constrained IoT environments.


<details>
  <summary>Details</summary>
Motivation: IoT devices in smart cities, transportation, and industrial systems face urgent security vulnerabilities due to limited computational resources, lack of physical safeguards, and deployment in heterogeneous networks, making them prime targets for malware; while ML offers automated detection, practical deployment requires both effective and lightweight models.

Method: Investigated four supervised learning models (Random Forest, LightGBM, Logistic Regression, Multi-Layer Perceptron) using the IoT-23 dataset; evaluated binary and multiclass classification performance, sensitivity to training data volume, and temporal robustness to simulate evolving threat landscapes.

Result: Tree-based models achieved high accuracy and generalization even with limited training data; however, all models showed performance deterioration over time as malware diversity increased.

Conclusion: Adaptive, resource-efficient ML models are crucial for securing IoT systems in real-world environments where threats continuously evolve and computational resources are constrained.

Abstract: The rapid expansion of the Internet of Things (IoT) in domains such as smart cities, transportation, and industrial systems has heightened the urgency of addressing their security vulnerabilities. IoT devices often operate under limited computational resources, lack robust physical safeguards, and are deployed in heterogeneous and dynamic networks, making them prime targets for cyberattacks and malware applications. Machine learning (ML) offers a promising approach to automated malware detection and classification, but practical deployment requires models that are both effective and lightweight. The goal of this study is to investigate the effectiveness of four supervised learning models (Random Forest, LightGBM, Logistic Regression, and a Multi-Layer Perceptron) for malware detection and classification using the IoT-23 dataset. We evaluate model performance in both binary and multiclass classification tasks, assess sensitivity to training data volume, and analyze temporal robustness to simulate deployment in evolving threat landscapes. Our results show that tree-based models achieve high accuracy and generalization, even with limited training data, while performance deteriorates over time as malware diversity increases. These findings underscore the importance of adaptive, resource-efficient ML models for securing IoT systems in real-world environments.

</details>


### [189] [Beyond Preferences: Learning Alignment Principles Grounded in Human Reasons and Values](https://arxiv.org/abs/2601.18760)
*Henry Bell,Lara Neubauer da Costa Schertel,Bochu Ding,Brandon Fain*

Main category: cs.LG

TL;DR: 提出Grounded Constitutional AI (GCAI)框架，通过结合用户的一般价值观声明和交互偏好理由，生成更受人类认可的AI宪法原则，优于现有ICAI方法。


<details>
  <summary>Details</summary>
Motivation: 开发部署大语言模型时需要对齐人类价值观，但如何公平地通过广泛利益相关者输入来确定宪法原则仍不明确。

Method: 提出GCAI框架，扩展ICAI方法：利用人类偏好标注数据中的理由生成情境原则，并补充来自用户AI价值观声明的通用原则，结合两者生成宪法。

Result: GCAI生成的宪法在个人使用和广泛应用方面均优于ICAI，被参与者认为更具道德基础、连贯性和多元性。

Conclusion: GCAI为生成AI宪法提供了统一框架，能更好代表用户期望和偏好，实现更公平、更受认可的AI对齐。

Abstract: A crucial consideration when developing and deploying Large Language Models (LLMs) is the human values to which these models are aligned. In the constitutional framework of alignment models are aligned to a set of principles (the constitution) specified in natural language. However, it is unclear how to fairly determine this constitution with widespread stakeholder input. In this work we propose Grounded Constitutional AI (GCAI), a unified framework for generating constitutions of principles that are representative of both users' general expectations toward AI (general principles) and their interaction-time preferences (contextual principles). We extend the Inverse Constitutional AI (ICAI) approach to generate contextual principles from human preference annotation data by leveraging human-provided \textit{reasons} for their preferences. We supplement these contextual principles with general principles surfaced from user statements of \textit{values} regarding AI. We show that a constitution generated by GCAI is preferred by humans over one generated through ICAI both personally, and for widespread use in governing AI behavior. Additionally participants consider the GCAI constitution to be more morally grounded, coherent, and pluralistic.

</details>


### [190] [Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability](https://arxiv.org/abs/2601.18778)
*Shobhita Sundaram,John Quan,Ariel Kwiatkowski,Kartik Ahuja,Yann Ollivier,Julia Kempe*

Main category: cs.LG

TL;DR: SOAR: A meta-RL framework where a teacher LLM generates synthetic problems as curricula for a student LLM, rewarded by actual student progress on hard problems, escaping reasoning plateaus without needing to solve them first.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning stalls on low-success-rate datasets with sparse binary rewards; can pretrained LLMs leverage latent knowledge to self-generate curricula for unsolvable problems?

Method: SOAR uses bi-level meta-RL: a teacher model proposes synthetic problems for a student model, with teacher rewards based on measured student improvement on hard problem subsets, grounding curriculum in actual progress rather than intrinsic proxies.

Result: (1) Bi-level meta-RL unlocks learning under sparse rewards by sharpening latent stepping-stone generation capacity; (2) Grounded rewards outperform intrinsic schemes, avoiding instability and diversity collapse; (3) Question structural quality matters more than solution correctness for learning progress.

Conclusion: Models can generate useful stepping stones without preexisting ability to solve hard problems, providing a principled path to escape reasoning plateaus without additional curated data.

Abstract: Can a model learn to escape its own learning plateau? Reinforcement learning methods for finetuning large reasoning models stall on datasets with low initial success rates, and thus little training signal. We investigate a fundamental question: Can a pretrained LLM leverage latent knowledge to generate an automated curriculum for problems it cannot solve? To explore this, we design SOAR: A self-improvement framework designed to surface these pedagogical signals through meta-RL. A teacher copy of the model proposes synthetic problems for a student copy, and is rewarded with its improvement on a small subset of hard problems. Critically, SOAR grounds the curriculum in measured student progress rather than intrinsic proxy rewards. Our study on the hardest subsets of mathematical benchmarks (0/128 success) reveals three core findings. First, we show that it is possible to realize bi-level meta-RL that unlocks learning under sparse, binary rewards by sharpening a latent capacity of pretrained models to generate useful stepping stones. Second, grounded rewards outperform intrinsic reward schemes used in prior LLM self-play, reliably avoiding the instability and diversity collapse modes they typically exhibit. Third, analyzing the generated questions reveals that structural quality and well-posedness are more critical for learning progress than solution correctness. Our results suggest that the ability to generate useful stepping stones does not require the preexisting ability to actually solve the hard problems, paving a principled path to escape reasoning plateaus without additional curated data.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [191] [Ferrichiral skyrmions with sublattice-resolved chirality in extended Kitaev model in triangular lattice](https://arxiv.org/abs/2601.17121)
*Bogeng Wen,Jiefu Cen,Hae-Young Kee*

Main category: cond-mat.str-el

TL;DR: 在三角晶格扩展Kitaev模型中，研究人员通过经典蒙特卡洛模拟发现了一种无需外磁场或Dzyaloshinskii-Moriya相互作用的"铁手性斯格明子相"，该相在零温和较宽参数范围内稳定，揭示了纯阻挫交换相互作用驱动的非传统斯格明子物理机制。


<details>
  <summary>Details</summary>
Motivation: 传统斯格明子相通常依赖外磁场或Dzyaloshinskii-Moriya相互作用稳定。本研究旨在探索纯由阻挫交换相互作用驱动的拓扑磁结构，特别是在Kitaev模型框架下寻找不依赖外部条件的斯格明子相，以深化对量子阻挫系统中新奇拓扑物态的理解。

Method: 采用经典蒙特卡洛模拟方法，系统研究三角晶格上扩展Kitaev模型在特定参数极限下的相行为，重点分析对称非对角键依赖相互作用和海森堡相互作用映射到XXZ模型的行为，并结合Kitaev相互作用的影响。

Result: 在先前识别的Z₂涡旋区内发现了一种铁手性斯格明子相，其特征是三个亚格子中两个携带单位斯格明子电荷，第三个保持非手性。该相在零温度、无外磁场和无Dzyaloshinskii-Moriya相互作用的条件下稳定存在，且能在较宽参数窗口和相对较高温度下保持。

Conclusion: 该研究揭示了由阻挫交换相互作用纯驱动的非传统斯格明子形成路径，表明传统XXZ磁体材料由于具有相同自旋-轨道耦合机制，很可能也存在有限Kitaev相互作用和铁手性态，为未来在相关材料体系中探索拓扑磁结构提供了新方向。

Abstract: We study an extended Kitaev model on the triangular lattice in a limit where the symmetric off-diagonal bond-dependent and Heisenberg interactions together map onto an XXZ model, in addition to the Kitaev interaction. Within the previously identified $\mathbb{Z}_2$ vortex regime, we uncover a ferrichiral skyrmion phase characterized by a sublattice-resolved scalar chirality: two of the three sublattices carry unit skyrmion charge, while the third remains nonchiral. Using classical Monte Carlo simulations, we show that this ferrichiral skyrmion phase emerges at zero temperature and in the absence of both an external magnetic field and Dzyaloshinskii-Moriya interactions, in sharp contrast to conventional skyrmion-hosting systems. The phase is stable over a wide parameter window and persists to relatively high temperatures. Our results reveal an unconventional route to skyrmion physics driven purely by frustrated exchange interactions and highlight the emergence of rich topological structures. Since both XXZ anisotropy and Kitaev interactions originate from the same spin-orbit-coupling mechanism, materials traditionally classified as XXZ magnets are expected to host finite Kitaev interactions as well. The potential for ferrichirality in these systems therefore warrants further investigation.

</details>


### [192] [Quantum Hyperuniformity and Quantum Weight](https://arxiv.org/abs/2601.18331)
*Junmo Jeon,Shiro Sakai*

Main category: cond-mat.str-el

TL;DR: Extending hyperuniformity to quantum fluctuations in electron systems creates a framework for identifying quantum phase transitions and gap structures via "quantum weight," distinguishing different phases (gapped, gapless, localized-critical-extended) as demonstrated with the Aubry-André model.


<details>
  <summary>Details</summary>
Motivation: To generalize hyperuniformity from classical to quantum systems for characterizing quantum phase transitions and underlying gap structures in electron systems, especially in aperiodic/disordered environments.

Method: Analyzing long-wavelength fluctuations of many-body ground states through the charge-density structure factor, incorporating intrinsic quantum fluctuations into hyperuniformity theory, and exemplifying with the Aubry-André model.

Result: Different quantum phases (gapped, gapless, localized-critical-extended) are sharply distinguished by quantum hyperuniformity classes; critical points exhibit anomalous scaling from multifractal wave functions; in gapped phases, quantum weight quantitatively measures gap size via universal power-law scaling.

Conclusion: Quantum hyperuniformity serves as a direct fingerprint of quantum criticality and a practical probe for quantum phase transitions in aperiodic electron systems, providing a powerful new framework for characterizing quantum phases.

Abstract: Extending hyperuniformity from classical to quantum fluctuations in electron systems yields a framework that identifies quantum phase transitions and reveals underlying gap structures through the quantum weight. We study long-wavelength fluctuations of many-body ground states through the charge-density structure factor by incorporating intrinsic quantum fluctuations into hyperuniformity. Although charge fluctuations at zero temperature are generally suppressed by particle-number conservation, their long-wavelength scaling reveals distinct universal behaviors that define quantum hyperuniformity classes. By exemplifying the Aubry-Andre model, we find that gapped, gapless, and localized-critical-extended phases are sharply distinguished by the quantum hyperuniformity classes. Notably, at the critical point, multifractal wave functions generate anomalous scaling behavior. We further show that, in quantum-hyperuniform gapped phases, the quantum weight provides a quantitative measure of the gap size through a universal power-law scaling. Along with classical hyperuniformity, quantum hyperuniformity serves a direct fingerprint of quantum criticality and a practical probe of quantum phase transitions in aperiodic electron systems.

</details>


### [193] [Emergent Random Spin Singlets in Disordered Spin-1/2 perovskite BaCu$_{1/3}$Ta$_{2/3}$O$_3$](https://arxiv.org/abs/2601.17455)
*Sagar Mahapatra,Francesco De Angelis,Dibyata Rout,Priyanshi Tiwari,Martin Etter,Edmund Welter,M. P. Saravanan,Rajeev Rawat,Satoshi Nishimoto,Carlo Meneghini,Surjeet Singh*

Main category: cond-mat.str-el

TL;DR: 研究无序钙钛矿BaCu$_{1/3}$Ta$_{2/3}$O$_3$发现其在极低温下无磁序，但存在宽而有限界的交换耦合分布，揭示了一种新型受限随机量子基态


<details>
  <summary>Details</summary>
Motivation: 探索具有本征受限随机性的无序量子磁体基态，挑战传统无限随机性固定点理论描述的随机单态相

Method: 利用同步辐射X射线衍射和X射线吸收光谱表征局域无序结构，分析低温磁性与热力学行为

Result: 观察到Cu/Ta随机占位导致的结构受限磁交换路径，0.1K以下无磁序或自旋冻结，呈现宽但非奇异的交换耦合分布P(J)

Conclusion: 实验证实了交换随机性宽而本征受限的量子基态存在，突破了传统无限随机性固定点理论框架

Abstract: We investigate the disordered perovskite BaCu$_{1/3}$Ta$_{2/3}$O$_3$, where Cu (spin-1/2) and Ta randomly occupy a pseudo-cubic lattice. Synchrotron X-ray diffraction and X-ray absorption spectroscopy establish the local nature of the disorder, revealing the presence of structurally constrained magnetic exchange paths. No magnetic ordering or spin freezing is observed down to 0.1 K. The low-temperature magnetic and thermodynamic behavior is captured by a broad but non-singular distribution $P(J)$ of exchange couplings $J$. These results open the possibility of realizing a disordered quantum ground state where the exchange randomness is broad yet intrinsically bounded, departing from the conventional infinite-randomness fixed point driven random-singlet phase.

</details>


### [194] [Tree tensor network solver for real-time quantum impurity dynamics](https://arxiv.org/abs/2601.17718)
*Bo Zhan,Jia-Lin Chen,Zhen Fan,Tao Xiang*

Main category: cond-mat.str-el

TL;DR: A tree tensor network (TTN) impurity solver using Cayley tree decomposition enables efficient, accurate real-time simulations of quantum impurity models with reduced entanglement and lower computational cost compared to conventional chain-based methods.


<details>
  <summary>Details</summary>
Motivation: To enable highly efficient and accurate real-time simulations of quantum impurity models by capturing the multiscale entanglement structure of impurity-bath systems, addressing limitations of conventional chain-based mappings.

Method: Decompose a noninteracting bath Hamiltonian into a Cayley tree to create a tensor network representation that naturally captures multiscale entanglement structure.

Result: Substantial entanglement reduction allows accurate ground-state and long-time dynamics at lower bond dimensions; benchmark calculations on single-impurity Anderson model show enhanced resolution of real-frequency spectral functions without analytic continuation.

Conclusion: The TTN solver provides a balanced, scale-uniform description of impurity physics and offers a versatile approach for real-time dynamical mean-field theory and quantum impurity model applications.

Abstract: We introduce a tree tensor network (TTN) impurity solver that enables highly efficient and accurate real-time simulations of quantum impurity models. By decomposing a noninteracting bath Hamiltonian into a Cayley tree, the method provides a tensor network representation that naturally captures the multiscale entanglement structure intrinsic to impurity-bath systems. This geometry differs from conventional chain-based mappings and yields a substantial reduction of entanglement, allowing accurate ground-state properties and long-time dynamics to be captured at significantly lower bond dimensions. Benchmark calculations for the single-impurity Anderson model demonstrate that the TTN solver achieves markedly enhanced resolution of real-frequency spectral functions, without invoking analytic continuation. This impurity solver provides a balanced, scale-uniform description of impurity physics and offers a versatile approach for real-time dynamical mean-field theory and related applications involving quantum impurity models.

</details>


### [195] [Toward Scalable Normalizing Flows for the Hubbard Model](https://arxiv.org/abs/2601.18273)
*Janik Kreit,Andrea Bulgarelli,Lena Funcke,Thomas Luu,Dominic Schuh,Simran Singh,Lorenzo Verzichelli*

Main category: cond-mat.str-el

TL;DR: 本研究探索将正规化流模拟扩展到更大的晶格尺寸和更低温度，并比较了随机正规化流与非平衡马尔可夫链蒙特卡洛方法在费米子系统上的扩展行为。


<details>
  <summary>Details</summary>
Motivation: 正规化流已证明能够学习哈伯德模型的玻尔兹曼分布，为凝聚态物理中的生成建模开辟了新途径。本研究旨在将此类模拟扩展到更具挑战性的物理参数区间（更大晶格、更低温度），同时提高稳定性和计算效率。

Method: 通过分析实现扩展所需的步骤，并对两种方法——随机正规化流和非平衡马尔可夫链蒙特卡洛——在哈伯德费米子系统上的扩展行为进行系统性比较。

Result: 摘要未给出具体数值结果，仅表明呈现了两种方法的扩展行为分析。

Conclusion: 摘要未明确陈述结论，但工作暗示理解扩展行为对于将这些方法推广到更大、更具物理意义系统至关重要。

Abstract: Normalizing flows have recently demonstrated the ability to learn the Boltzmann distribution of the Hubbard model, opening new avenues for generative modeling in condensed matter physics. In this work, we investigate the steps required to extend such simulations to larger lattice sizes and lower temperatures, with a focus on enhancing stability and efficiency. Additionally, we present the scaling behavior of stochastic normalizing flows and non-equilibrium Markov chain Monte Carlo methods for this fermionic system.

</details>


### [196] [Magnetic Signatures of a Putative Fractional Topological Insulator in Twisted MoTe2](https://arxiv.org/abs/2601.18508)
*Yiping Wang,Gillian E. Minarik,Weijie Li,Yves Kwan,Shuai Yuan,Eric Anderson,Chaowei Hu,Julian Ingham,Jeongheon Choe,Takashi Taniguchi,Kenji Watanabe,Xavier Roy,Jiun-Haw Chu,Raquel Queiroz,James C. Hone,N. Regnault,Xiaodong Xu,Xiaoyang Zhu*

Main category: cond-mat.str-el

TL;DR: This paper reports experimental evidence of a fractional topological insulator with time-reversal symmetry at filling factor v = -4/3 in twisted MoTe2 bilayers, showing unique vanishing magnetization and antiferromagnetic-like behavior under out-of-plane magnetic fields, which disappears at slightly different twist angles.


<details>
  <summary>Details</summary>
Motivation: To investigate exotic quantum states arising from electronic correlation, topology, and time-reversal-symmetry (TRS), specifically the nature of a correlated state at v = -4/3 in twisted MoTe2 bilayers that is twice the filling factor of the known v = -2/3 fractional Chern insulator, and to determine if it represents a predicted TRS-preserving fractional topological insulator.

Method: Employed pump-probe circular dichroism (CD) measurements on twisted MoTe2 bilayers with twist angles of 3.9° and 3.7°, supported by theoretical calculations using an interacting continuum model of tMoTe2.

Result: The v = -4/3 state exhibits vanishing magnetization (m = 0) in finite windows of out-of-plane magnetic field below ~2-4 mT, and a first-order phase transition to ±m states at higher fields. This out-of-plane antiferromagnetic-like behavior is unique to v = -4/3 and disappears at twist angles of 4.0° and 3.3°. The magnetic signature matches predictions for a fractional topological insulator with TRS comprising two copies of -2/3 FCIs with opposite chiralities.

Conclusion: The work identifies a candidate fractional topological insulator with time-reversal symmetry at v = -4/3 in twisted MoTe2 bilayers at twist angles of approximately 3.7-3.9°.

Abstract: The interplay among electronic correlation, topology, and time-reversal-symmetry (TRS) often leads to exotic quantum states of matter. Primary examples include the recently realized fractional Chern insulators (FCIs) in twisted MoTe2 bilayers (tMoTe2) and multilayer graphene aligned with hBN, where TRS is broken in partially filled flat moire Chern bands. Among the FCIs in tMoTe2, the most robust is at a hole filling of v = -2/3 per moire unit cell. Interestingly, transient optical sensing and more recent transport measurements revealed a correlated state at v = -4/3, twice the filling factor for the v = -2/3 FCI. Here, employing pump-probe circular dichroism (CD) measurements on tMoTe2 with twist angles = 3.9 degree and 3.7 degree, we find that the v = -4/3 state exhibits vanishing magnetization (m = 0) in finite windows of out-of-plane magnetic field less than ~2-4 mT, and a first order phase transition to + - m states at higher fields. This out-of-plane antiferromagnetic (AFM) like behavior is notably absent for all other correlated states and disappears for the v = -4/3 state at higher or lower twist angles = 4.0 degree and 3.3 degree. The observed magnetic signature at v = -4/3 is consistent with a predicted fractional topological insulator (FTI) with TRS, consisting of two copies of -2/3 FCIs with opposite chiralities. We support these findings with calculations in the interacting continuum model of tMoTe2. Our work presents a candidate for fractional topological insulators with TRS.

</details>


### [197] [Edge States Effects in Quantum Work Statistics](https://arxiv.org/abs/2601.18614)
*Moallison F. Cavalcante*

Main category: cond-mat.str-el

TL;DR: 该研究通过量子杂质模型和局域淬火协议，精确计算了系统在非平衡态下的量子功分布，揭示了边界边缘态对功分布的显著影响及其在低能和高能区的特征指纹。


<details>
  <summary>Details</summary>
Motivation: 量化通过局域控制访问边界相所需的能量代价，探索边缘态与量子功统计的关联。

Method: 采用解析可解的量子杂质模型，设计局域淬火协议驱动系统远离平衡态，并精确计算量子功分布。

Result: 边缘态显著改变量子功分布，在低能阈值和高能区呈现可识别的特征指纹。

Conclusion: 量子功分布可作为探测边界相和边缘态的有效非平衡诊断工具。

Abstract: Motivated by the objective of quantifying the energetic cost of accessing boundary phases through local control, we investigate here a simple, analytically tractable quantum impurity model. This model exhibits a rich boundary phase diagram, characterized by phases with different numbers of edge states. By considering a local quench protocol that drives the system out of equilibrium, we calculate exactly the resulting quantum work distribution across these phases. Our results show that the presence of edge states strongly alters this distribution. In particular, we analytically determine key fingerprints of these states both near the low-energy threshold and in the high-energy region.

</details>


### [198] [Multi-target density matrix renormalization group for 3D CFTs on the fuzzy sphere](https://arxiv.org/abs/2601.18648)
*Jin-Xiang Hao,Zheng Zhu,Yang Qi*

Main category: cond-mat.str-el

TL;DR: 结合模糊球正则化与多目标DMRG算法，突破精确对角化限制，在更大体系尺寸下研究3D共形场论，显著提升与bootstrap基准的吻合度


<details>
  <summary>Details</summary>
Motivation: 模糊球正则化为研究三维共形场论提供了强大框架，但受限于精确对角化方法可处理的体系尺寸

Method: 将模糊球正则化与多目标密度矩阵重正化群(DMRG)算法结合，研究球形最低朗道能级上的三维伊辛模型

Result: 成功计算了比以往精确对角化更大体系尺寸下的24个低能态，在临界点提取了六个初级算符的标度维数，结果与bootstrap基准的吻合度显著优于小尺寸下的精确对角化结果，并能高效计算大体系中的多个激发态

Conclusion: 模糊球正则化与先进DMRG技术相结合，为三维共形场论的高精度物理研究建立了强大且普适的框架

Abstract: The fuzzy sphere regularization provides a powerful framework for studying three-dimensional (3D) conformal field theories (CFTs) by mapping them onto numerically tractable lattice models on the spherical lowest Landau level. However, the system sizes accessible to this method have been limited by the exact diagonalization (ED). In this work, we transcend this limitation by combining the fuzzy sphere regularization with a sophisticated multi-target density matrix renormalization group (DMRG) algorithm. Focusing on the 3D Ising-type model on the spherical lowest Landau level, we calculate the 24 low-lying energies at a larger system size than previously feasible with ED. At criticality, we extract the scaling dimensions of six primary operators, and the results show significantly improved agreement with bootstrap benchmarks compared to previous ED results at smaller sizes. Our approach allows us to efficiently target multiple excited states in larger systems beyond the reach of exact diagonalization. This study establishes the fuzzy sphere regularization combined with advanced DMRG techniques as a powerful and general framework for precision physics in 3D CFTs.

</details>


### [199] [Quantum skyrmions in the antiferromagnetic triangular lattice](https://arxiv.org/abs/2601.18737)
*Inés Corte,Federico Holik,Lorena Rebón,Flavia A. Gómez Albarracín*

Main category: cond-mat.str-el

TL;DR: Researchers used DMRG simulations to discover stable quantum antiferromagnetic skyrmions in a spin-1/2 triangular lattice model, demonstrating their potential for spintronics applications.


<details>
  <summary>Details</summary>
Motivation: To investigate quantum analogues of antiferromagnetic skyrmions, which are theoretically promising for data storage due to lacking transverse deflection, but have not been previously studied despite existing research on quantum ferromagnetic skyrmions.

Method: Density matrix renormalization group (DMRG) simulations of an antiferromagnetic quantum spin-1/2 Heisenberg model with Dzyaloshinskii-Moriya interactions on a triangular lattice, with analysis via magnetization profiles, spin structure factors, and quantum entanglement measures.

Result: The DMRG calculations reveal that three-sublattice quantum antiferromagnetic skyrmion textures emerge as stable ground states across a broad range of applied magnetic fields.

Conclusion: This work establishes the existence of quantum antiferromagnetic skyrmions, providing a foundation for developing quantum spintronic devices based on topological spin textures.

Abstract: Magnetic skyrmions are topological quasiparticles potentially useful for memory and computing devices. Antiferromagnetic (AF) skyrmions present no transverse deflection, making them suitable candidates for data storage applications. After the discovery of skyrmions with length scales comparable to the lattice constant, several works presented quantum analogues of classical ferromagnetic skyrmions in spin systems. However, studies about quantum analogues of AF skyrmions are still lacking. Here, we explore the phases of the AF quantum spin-1/2 Heisenberg model with Dzyaloshinskii-Moriya interactions on the triangular lattice using the density matrix renormalization group (DMRG) algorithm. We study the magnetization profile, spin structure factor and quantum entanglement of the resulting ground states to characterize the corresponding phases and signal the emergence of quantum AF skyrmions. Our results support that three-sublattice quantum antiferromagnetic skyrmion textures are stabilized in a wide range of magnetic fields.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [200] [The dimensionality of the Hopfield model](https://arxiv.org/abs/2601.17427)
*Cristopher Erazo,Santiago Acevedo,Alessandro Ingrosso*

Main category: cond-mat.dis-nn

TL;DR: 该研究利用专为二进制数据设计的二元本征维度(BID)几何测度分析Hopfield模型，发现BID在不同相中呈现不同的标度行为，并与重叠分布存在直接关联，为理解自旋系统提供了新的几何视角。


<details>
  <summary>Details</summary>
Motivation: 由于有限尺寸效应会干扰自旋玻璃序参量(q)的准确数值估计，本研究旨在应用专为二进制数据设计的二元本征维度(BID)这一几何测度，来分析在统计力学、机器学习和神经科学中具有范式意义的Hopfield模型，以克服传统方法的局限性。

Method: 采用二元本征维度(BID)这一几何测度，对Hopfield自旋系统进行定量分析。

Result: 研究发现，在自旋间关联较小的检索相和顺磁相中，BID随系统尺寸线性缩放；而在整个自旋玻璃相中，BID呈现次线性缩放，揭示了其关联结构。此外，研究建立了BID与重叠分布之间的直接关系，发现了状态空间几何结构与标准自旋序参量之间的新联系。

Conclusion: BID能够有效表征Hopfield模型的相和相变，并揭示状态空间几何结构与自旋序参量之间的深刻联系，为分析二进制数据系统的几何特性提供了新工具。

Abstract: We use the Binary Intrinsic Dimension (BID), a geometrical measure designed for binary data, to analyze the Hopfield model, a paradigmatic spin system from statistical mechanics, machine learning and neuroscience. The BID allows us to characterize the phases and transitions of this system, and moreover it is robust against finite-size effects that interfere with the correct numerical estimation of the spin-glass order parameter ($q$). We observe that the BID scales linearly with system size in the retrieval and paramagnetic phases, where the correlations between spins are small, and exhibits sublinear scaling in the whole spin-glass phase, highlighting its correlated structure. Furthermore, we establish a direct relationship between the BID and the overlap distribution, unveiling a novel connection between the geometry of the state-space and standard spin order parameters.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [201] [The Double Covariance Model: A Stochastic Reconstruction of Quantum Entangled States via Interplay of Micro-Macro Time Scales](https://arxiv.org/abs/2601.17070)
*Andrei Khrennikov*

Main category: quant-ph

TL;DR: 该论文提出一个数学框架，证明任何复合量子系统的密度算子均可通过两个经典随机过程的相关性导出，用量子态作为经典概率空间的四阶矩结构来重构量子理论。


<details>
  <summary>Details</summary>
Motivation: 探索量子纠缠的经典起源，建立量子理论与经典随机过程之间的桥梁，挑战量子关联必须非经典的传统认知。

Method: 采用双时间尺度（微观/宏观）方案：微观时间描述子系统随机波动X(t)/Y(t)，宏观时间涌现量子关联；提出双协方差模型(DCM)，将量子态定义为底层经典概率空间的四阶矩结构。

Result: 证明任意复合系统密度算子ρ_AB均可由两个经典随机过程的相关性精确重构，且DCM成功复现量子理论核心特性（如纠缠）。

Conclusion: 量子纠缠可能源于经典随机过程的高阶统计特性，为量子基础提供新视角：量子理论或可视为经典概率论的扩展而非颠覆。

Abstract: This article presents a concrete mathematical framework for the generation of entangled quantum states from classical stochastic processes. We demonstrate that any density operator $ρ_{AB}$ of a composite system can be derived from the correlations between two underlying stochastic processes, $X(t)$ and $Y(t)$, representing the random fluctuations of its subsystems. This construction utilizes a two-scale temporal scheme - micro and macro time - where quantum correlations emerge as macro-correlations derived from underlying micro-correlations. We propose the Double Covariance Model (DCM), which reproduces the fundamental properties of quantum theory by treating the quantum state as the fourth-order moment structure of an underlying classical probability space.

</details>


### [202] [A pedagogical derivation of the first-order effective Hamiltonian for the two-mode Jaynes-Cummings model](https://arxiv.org/abs/2601.17208)
*Alejandro R. Urzúa*

Main category: quant-ph

TL;DR: This paper provides a clear, teaching-focused derivation of the effective Hamiltonian for a two-mode quantum system, revealing how atom-field interactions create a beam-splitter effect between light modes.


<details>
  <summary>Details</summary>
Motivation: To make advanced effective Hamiltonian methods in multimode light-matter interactions accessible for teaching and learning by emphasizing physical insight over complex mathematics.

Method: Uses a perturbative unitary transformation to eliminate nonresonant terms in the two-mode Jaynes-Cummings model, followed by diagonalization via geometric rotation in bosonic mode space.

Result: Derives an atom-induced effective beam-splitter interaction between field modes, with dispersive frequency shifts that are interpretable through a simple geometric rotation.

Conclusion: Successfully demonstrates a transparent pedagogical approach to effective Hamiltonians, enhancing understanding of dispersive regime dynamics for educational purposes in quantum optics.

Abstract: This work presents a pedagogical and self-contained derivation of the first-order effective Hamiltonian for the two-mode Jaynes-Cummings model in the dispersive regime. A perturbative unitary transformation removes nonresonant atom-field terms, revealing dispersive frequency shifts leading to an atom-induced effective beam-splitter interaction between the field modes. The resulting Hamiltonian is diagonalized through a simple geometric rotation in the two-mode bosonic space, providing a transparent interpretation of the underlying dynamics. The exposition emphasized clarity and physical insight, making effective Hamiltonian methods accessible for teaching and learning in multimode light-matter interactions.

</details>


### [203] [The Universe as a Detector: A Quantum Filtering Formulation of the Diósi-Penrose Model](https://arxiv.org/abs/2601.17384)
*John Gough,Dylon Rees*

Main category: quant-ph

TL;DR: 该论文针对Diósi-Penrose问题提出了一种基于时空零差探测量子滤波的替代方案，通过开放量子随机模型中的输出正交分量连续统，建立了量子Kushner-Stratonovich方程，避免了预设背景引力涨落的传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统Diósi-Penrose模型依赖预设的背景引力涨落来解释波函数坍缩，这一假设存在理论不确定性。作者旨在从量子测量和滤波理论的基本原理出发，寻找一种不依赖引力涨落假设的替代性理论框架。

Method: 基于开放量子随机系统理论，采用时空零差探测技术对输出正交分量的连续统进行量子滤波，推导出描述波函数连续坍缩的量子Kushner-Stratonovich方程。

Result: 成功构建了与传统Diósi-Penrose模型平行的量子滤波表述，得到了相应的量子Kushner-Stratonovich方程，并将该模型与量子退相干理论中的连续坍缩模型建立了联系。

Conclusion: 该研究为Diósi-Penrose问题提供了不依赖引力涨落假设的量子滤波理论框架，丰富了波函数坍缩机制的理论体系，并与现有量子退相干理论形成了良好衔接。

Abstract: We consider the Diósi-Penrose problem but rather than postulating background gravitational fluctuations, we instead consider the quantum filter that arises from space-time homodyning the continuum of output quadrature described in the open quantum stochastic model presented here. This is described by a quantum Kushner-Stratonovich equation, typical of the form appearing in continuous-time collapse of the wave-function models in Quantum Decoherence Theory

</details>


### [204] [Non-Markovian Decoherence Times in Finite-Memory Environments](https://arxiv.org/abs/2601.17394)
*Ramandeep Dewan*

Main category: quant-ph

TL;DR: 该研究表明，有限记忆环境中退相干在短时呈二次增长而非指数增长，退相干时间按关联时间的平方根缩放，挑战了标准的马尔可夫模型。


<details>
  <summary>Details</summary>
Motivation: 传统马尔可夫主方程描述退相干时假设环境记忆消失，预测相干性指数抑制，但这对应于奇异极限。对于具有有限记忆时间的实际环境，这种描述可能不准确。

Method: 提出一般性时间非局域退相干泛函，仅由环境力关联函数决定。马尔可夫动力学作为极限情况恢复。对高斯、软幂律和Ornstein-Uhlenbeck环境进行解析分析，采用伪模映射进行精确数值模拟，并分析纯度和冯诺依曼熵动力学。

Result: 发现退相干泛函在短时呈二次增长，与模型无关。操作定义的退相干时间（不假设指数衰减）按环境关联时间的平方根缩放。Ornstein-Uhlenbeck情形下可得到精确解析闭包。仅在无记忆极限下才出现指数退相干。抑制特定相干元素不一定与不可逆熵产生一致。

Conclusion: 非马尔可夫效应从根本上改变了退相干行为。环境关联时间是可从动力学数据中提取的操作参数。需要区分相干性衰减与可观测量子特性损失。

Abstract: Decoherence is often modeled using Markovian master equations that predict exponential suppression of coherence and are frequently used as effective bounds on quantum behavior in complex environments. Such descriptions, however, correspond to the singular physical limit of vanishing environmental memory. Here we formulate decoherence using a general time-nonlocal decoherence functional determined solely by the environmental force correlation function, with Markovian dynamics recovered explicitly as a limiting case.
  For arbitrary stationary environments with finite temporal correlations, we show that the decoherence functional exhibits quadratic short-time growth that is model-independent within the finite-memory class considered. Consequently, the decoherence time defined operationally-without assuming exponential decay-scales as the square root of the environmental correlation time, independent of the detailed form of the bath correlation kernel.
  These results are illustrated analytically for Gaussian-correlated, soft power-law, and Ornstein-Uhlenbeck environments. In the Ornstein-Uhlenbeck case, the non-Markovian dynamics admit an exact analytical closure, yielding a closed evolution equation for the coherence. Exact numerical simulations based on a pseudomode mapping confirm the predicted scaling and show that exponential decoherence emerges only in the memoryless limit.
  Beyond coherence decay, we distinguish decoherence rates from observable loss of quantum signatures by analyzing purity and von Neumann entropy dynamics. We show that suppression of a specific coherence element need not coincide with irreversible entropy production. Finally, we introduce an inferred-memory perspective in which the environmental correlation time is treated as an operationally extractable parameter from dynamical data.

</details>


### [205] [Qhronology: A Python package for studying quantum models of closed timelike curves](https://arxiv.org/abs/2601.17459)
*Lachlan G. Bishop*

Main category: quant-ph

TL;DR: This paper introduces Qhronology, a Python-based scientific computing package for simulating quantum models of closed timelike curves (CTCs) and general quantum information processing.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive computational framework for studying quantum theories of antichronological time travel and analyzing quantum resolutions to temporal paradoxes, while also serving as a complete quantum circuit simulator.

Method: Developing a Python package that implements quantum circuit simulation with both numerical and symbolic capabilities, specifically designed to handle CTC quantum models and temporal paradox calculations.

Result: The paper presents Qhronology's design philosophy, architecture, basic usage, demonstrates its capabilities through various examples, and provides empirical benchmarking of its circuit simulation performance.

Conclusion: Qhronology successfully bridges quantum computing and theoretical time travel research, offering researchers a versatile tool for both specialized CTC studies and general quantum algorithm development.

Abstract: Qhronology is a novel scientific-computing package for studying quantum models of closed timelike curves (CTCs) and simulating general quantum information processing and computation. Written in Python, the program provides a comprehensive framework for analyzing quantum theories of antichronological time travel, including functionality to calculate quantum resolutions to temporal paradoxes. It also operates as a complete quantum circuit simulator, enabling the examination of quantum algorithms and protocols in both numerical and symbolic capacities. In this paper, we formally introduce Qhronology, beginning with discussion on aspects of its design philosophy and architecture. An overview of its basic usage is then presented, along with a collection of examples demonstrating its various capabilities within a variety of distinct contexts. Lastly, the performance of the package's circuit simulation component is characterized by way of some simple empirical benchmarking.

</details>


### [206] [Quantum-Inspired Algorithms beyond Unitary Circuits: the Laplace Transform](https://arxiv.org/abs/2601.17724)
*Noufal Jaseem,Sergi Ramos-Calderer,Gauthameshwar S.,Dingzu Wang,José Ignacio Latorre,Dario Poletti*

Main category: quant-ph

TL;DR: This paper introduces a tensor-network-based quantum-inspired algorithm to efficiently compute the non-unitary discrete Laplace transform on classical hardware, achieving significant speedups for large-scale data (up to 2^30 inputs) via MPO compression and a novel decomposition combining damping and Fourier transforms.


<details>
  <summary>Details</summary>
Motivation: Quantum algorithms typically rely on unitary operations, but many important transforms (like the discrete Laplace transform) are non-unitary and aperiodic. Existing quantum circuit models cannot efficiently handle such operations, limiting practical applications. This work aims to bridge this gap by leveraging tensor networks' flexibility to accommodate non-unitary maps for acceleration.

Method: The approach encodes input signals across two paired n-qubit registers, decomposes the Laplace transform into a compressed non-unitary Damping Transform followed by a Quantum Fourier Transform (QFT), and represents the entire operation as a single matrix-product operator (MPO). Critical to the method is aggressive MPO compression to low bond dimension, enabling efficient execution on classical hardware.

Result: Simulations successfully processed datasets up to N=2^30 input points (yielding 2^60 output points), demonstrating that low bond dimensions in the MPO representation enable substantial computational acceleration while maintaining controllable accuracy. The method also allows precise pole identification within the transform.

Conclusion: By extending quantum-inspired algorithms beyond unitarity through tensor-network flexibility and MPO compression, this work achieves practical speedups for non-unitary transforms like the discrete Laplace transform on classical hardware, opening avenues for efficient large-scale signal processing and mathematical computations.

Abstract: Quantum-inspired algorithms can deliver substantial speedups over classical state-of-the-art methods by executing quantum algorithms with tensor networks on conventional hardware. Unlike circuit models restricted to unitary gates, tensor networks naturally accommodate non-unitary maps. This flexibility lets us design quantum-inspired methods that start from a quantum algorithmic structure, yet go beyond unitarity to achieve speedups. Here we introduce a tensor-network approach to compute the discrete Laplace transform, a non-unitary, aperiodic transform (in contrast to the Fourier transform). We encode a length-$N$ signal on two paired $n$-qubit registers and decompose the overall map into a non-unitary exponential Damping Transform followed by a Quantum Fourier Transform, both compressed in a single matrix-product operator. This decomposition admits strong MPO compression to low bond dimension resulting in significant acceleration. We demonstrate simulations up to $N=2^{30}$ input data points, with up to $2^{60}$ output data points, and quantify how bond dimension controls runtime and accuracy, including precise and efficient pole identification.

</details>


### [207] [Bayesian quantum sensing using graybox machine learning](https://arxiv.org/abs/2601.17465)
*Akram Youssry,Stefan Todd,Patrick Murton,Muhammad Junaid Arshad,Alberto Peruzzo,Cristian Bonato*

Main category: quant-ph

TL;DR: 该论文提出了一种灰箱建模策略，将基于物理的模型与数据驱动方法相结合，显著提升了量子传感器的性能，并在单自旋量子传感器上验证了其在磁场估计中的有效性。


<details>
  <summary>Details</summary>
Motivation: 量子传感器在空间分辨率和灵敏度方面具有显著优势，但其实际性能常受到噪声、不完美的态制备和非理想控制场等未建模效应的限制。

Method: 研究团队首次在固态开放量子系统中实验实施了灰箱建模策略，将基于物理的系统模型与实验缺陷的数据驱动描述相结合，利用约10,000个训练数据点，通过贝叶斯推理进行磁场估计。

Result: 灰箱模型比纯物理模型的均方误差提高了几个数量级。

Conclusion: 该方法可广泛应用于各种量子传感平台，对于实时自适应协议尤其有价值，因为在这些协议中模型不准确会导致次优控制和性能下降。

Abstract: Quantum sensors offer significant advantages over classical devices in spatial resolution and sensitivity, enabling transformative applications across materials science, healthcare, and beyond. Their practical performance, however, is often constrained by unmodelled effects, including noise, imperfect state preparation, and non-ideal control fields.
  In this work, we report the first experimental implementation of a graybox modelling strategy for a solid-state open quantum system. The graybox framework integrates a physics-based system model with a data-driven description of experimental imperfections, achieving higher fidelity than purely analytical (whitebox) approaches while requiring fewer training resources than fully deep-learning models. We experimentally validate the method on the task of estimating a static magnetic field using a single-spin quantum sensor, performing Bayesian inference with a graybox model trained on prior experimental data. Using roughly 10,000 training datapoints, the graybox model yields several orders of magnitude improvement in mean squared error over the corresponding physics-only model. These results are broadly applicable to a wide range of quantum sensing platforms, not limited to single-spin systems, and are particularly valuable for real-time adaptive protocols, where model inaccuracies can otherwise lead to suboptimal control and degraded performance.

</details>


### [208] [Are Quantum Voting Protocols Practical?](https://arxiv.org/abs/2601.17514)
*Nitin Jha,Abhishek Parakh*

Main category: quant-ph

TL;DR: 本文综述量子投票协议的实际可行性，分析其理论基础、主要架构类型、关键实施挑战（损耗、噪声、可扩展性、抗胁迫），并探讨近期在小规模选举中的部署前景。


<details>
  <summary>Details</summary>
Motivation: 传统投票依赖计算复杂性，量子投票利用量子力学特性（如不可克隆定理）实现选票保密和公开可验证。本文旨在系统评估这些协议的实际可行性。

Method: 1) 阐述叠加、不可克隆、纠缠等量子原理；2) 建立统一威胁模型；3) 综述三类协议：纠缠中心化计票、自计票公开验证、最小化权威认证；4) 评估技术挑战；5) 讨论部署场景。

Result: 识别出损耗、噪声、设备缺陷、可扩展性和抗胁迫等核心挑战，指出短期内仅适用于小规模选举。

Conclusion: 量子投票虽具理论优势，但实际应用面临重大技术障碍。近期仅可能在参与方较少的小规模选举中部署，大规模应用需技术突破。

Abstract: Quantum voting protocols aim to offer ballot secrecy and publicly verifiable tallies using physical guarantees from quantum mechanics, rather than relying solely on computational hardness. This article surveys whether such quantum voting protocols are practical. We begin by outlining core mathematical ideas such as the superposition principle, the no-cloning theorem, and quantum entanglement. We then define a common system and threat model, identifying key actors, trust assumptions, and security goals. Representative protocol families are reviewed, including entanglement-based schemes with central tallying, self-tallying designs that enable public verification, and authority-minimized approaches that certify untrusted devices through observable correlations. Finally, we evaluate implementation challenges, including loss, noise, device imperfections, scalability, and coercion resistance, and discuss realistic near-term deployment scenarios for small-scale elections.

</details>


### [209] [Quantum Phase Transitions in the Transverse-Field Ising Model: A Comparative Study of Exact, Variational, and Hardware-Based Approaches](https://arxiv.org/abs/2601.17515)
*Rudraksh Sharma*

Main category: quant-ph

TL;DR: 该论文通过精确对角化、变分量子本征求解器(VQE)模拟和真实量子处理器实验，研究了四自旋一维横场伊辛模型的基态性质与量子临界动力学，发现浅层变分电路能可靠计算基态能量，但磁序参数和相关函数观测受噪声影响显著，揭示了当前含噪声中等规模量子(NISQ)系统在模拟量子临界现象时的能力与局限。


<details>
  <summary>Details</summary>
Motivation: 评估含噪声中等规模量子(NISQ)设备在模拟量子临界现象中的可行性，为未来量子硬件和算法发展提供基准测试。

Method: 结合精确对角化(基准)、变分量子本征求解器(VQE)模拟和资源高效的批处理协议，在IQM Garnet量子处理器上模拟四自旋横场伊辛模型，计算基态能量、磁序参数和相关函数。

Result: 浅层变分电路在全参数空间内能可靠捕获基态能量；但磁序参数和相关函数敏感量受噪声影响显著，硬件实验显示临界交叉区域出现强烈展宽，与长程关联的噪声衰减一致。

Conclusion: 当前NISQ系统可部分模拟量子临界现象，但噪声严重限制了对关联敏感量的精确测量；该工作为量子硬件优化和算法开发提供了关键基准，指明了未来改进方向。

Abstract: The quantum phase transitions provide a paradigm for studying collective quantum phenomena that are a result of competing non-commuting interactions. This paper will study the ground state properties and quantum critical dynamics of the one-dimensional transverse field Ising model through a combined perspective that includes exact diagonalisation, variational quantum eigensolver (VQE) simulations, and simulations on realistic physical quantum devices. We focus on a lattice of four spins, where we calculate the ground-state energies, magnetic order parameters and correlation functions at uniformly applied conditions, which is repeated by all systems. Precise diagonalisation provides both a benchmark, which is symmetry-conserving, and a depth-two, physics inspired variational approximation, which provides simulations accessible to hardware. The circuits that have been optimised identically are then placed on the IQM Garnet quantum processor, using a resource-efficient batched protocol. We find that the ground-state energies of shallow variational circuits are reliably captured by the circuit over the entire parameter space; the magnetic arrangement parameters and observables sensitive to correlation signal significantly more noise. The error analysis of quantitative analysis reveals a strong broadening of critical crossover on hardware, which is consistent with the noise attenuation of long-range correlations. These findings highlight the current capabilities as well as the fundamental limitations of noisy intermediate-scale quantum systems in modelling quantum critical phenomena as a benchmark to future enhancements in obtaining quantum hardware and quantum algorithms development.

</details>


### [210] [Autonomous phonon maser in levitated spin-mechanics](https://arxiv.org/abs/2601.17552)
*Mohamed Hatifi*

Main category: quant-ph

TL;DR: 该研究演示了悬浮纳米金刚石中的单个氮空位中心在微波和光泵浦下可作为反转增益介质，实现自主 phonon maser 的稳定输出，并揭示了其阈值特性。


<details>
  <summary>Details</summary>
Motivation: 悬浮纳米金刚石中的氮空位中心提供了超低音机械模式及可调的耗散与自旋反作用，探索其实现声子 maser 对于量子传感、基础物理和量子信息处理具有重要价值。

Method: 采用微波 dressing 和光泵浦技术，在分离时间尺度 regime 下通过绝热 elimination 方法推导出具有失谐依赖跃迁率的约化机械主方程，并通过完整主方程数值模拟验证理论预测。

Result: 研究表明仅需百分之一量级的 dressed-basis 反转即可达到阈值，小信号增益可比本征机械损耗高出数个量级，全数值模拟证实了阈值以上的自持振荡和符合 Maxwell-Bloch 理论的饱和行为。

Conclusion: 悬浮 NV 系统能够高效地将自旋布居反转转化为机械振荡，为实现量子极限放大器、高灵敏度传感器以及探索量子非线性力学动力学提供了可行平台。

Abstract: Levitated nanodiamonds hosting a single nitrogen-vacancy (NV) center provide an ultra-low-frequency mechanical mode with widely tunable dissipation and spin backaction under microwave dressing and optical pumping. We demonstrate that the driven NV spin can be tuned to act as an inverted gain medium for the center-of-mass motion, thereby stabilizing an autonomous phonon maser. In the separation-of-timescales regime where spin dynamics is fast, adiabatic elimination yields a reduced mechanical master equation with closed-form, detuning-dependent transition rates and a sharp threshold given by the sign change of the phonon-number damping. For representative levitated-NV parameters, we find that a percent-level dressed-basis inversion is sufficient to reach the threshold, and the small-signal gain can exceed the intrinsic mechanical loss by orders of magnitude. Full master-equation simulations confirm above-threshold self-oscillation and a phase-diffusing, coherent steady state, whose saturation follows the Maxwell-Bloch prediction.

</details>


### [211] [Holstein Primakoff spin codes for local and collective noise](https://arxiv.org/abs/2601.17592)
*Sivaprasad Omanakuttan,Tyler Thurtell,Andrew K. Forbes,Vikas Buchemmavari,Ben Q. Baragiola*

Main category: quant-ph

TL;DR: 提出Holstein-Primakoff自旋码框架，将玻色码映射到对称自旋系综，实现抗集体和局域噪声的量子纠错，无需测量即可将局域错误转化为可纠正的集体错误


<details>
  <summary>Details</summary>
Motivation: 现有量子纠错码依赖局域控制和稳定子测量，难以在集体相互作用主导的系统中实现，需要适配集体噪声场景的新编码方案

Method: 基于Holstein-Primakoff近似，建立从连续变量玻色码到置换对称自旋系综的映射框架，构建HP自旋码

Result: HP码对集体噪声和局域自旋噪声均具鲁棒性，并提出无测量的局域错误恢复协议，可将局域噪声转化为可纠正的集体自旋错误

Conclusion: 该框架为集体相互作用系统提供了实用的纠错方案，降低了测量难度，拓展了玻色码在自旋系统中的应用

Abstract: Quantum error correction is essential for fault-tolerant quantum computation, yet most existing codes rely on local control and stabilizer measurements that are difficult to implement in systems dominated by collective interactions. Inspired by spin-GKP codes in PhysRevA.108.022428, we develop a general framework for Holstein-Primakoff spin codes, which maps continuous-variable bosonic codes onto permutation-symmetric spin ensembles via the Holstein-Primakoff approximation. We show that HP codes are robust to both collective and local-spin noise and propose an explicit measurement-free local error recovery procedure to map local noise into correctable collective-spin errors.

</details>


### [212] [Generalized Aharonov-Bohm Effect](https://arxiv.org/abs/2601.17659)
*Shan Gao*

Main category: quant-ph

TL;DR: 本研究利用WKB方法揭示了时间依赖磁通量下Aharonov-Bohm效应的新行为：在准静态近似下，圆形路径的相位移动与时间平均磁通成正比，而非圆形路径则表现出磁通历史和几何路径的混合依赖性，澄清了规范势与感应电场的作用机制。


<details>
  <summary>Details</summary>
Motivation: 虽然Aharonov-Bohm效应在静态磁通下已得到充分验证，但其在时变磁通条件下的行为仍是一个悬而未决且有争议的问题，亟需理论澄清。

Method: 采用WKB近似方法，推导时间依赖磁矢势下的Aharonov-Bohm相位移动，并验证规范选择与麦克斯韦方程的一致性。

Result: 对于准静态下的圆形路径，AB相位移动Δφ_AB = (1/T)∫₀ᵀ eΦ(t)dt，总相位移动（含动力学贡献）等于eΦ(0)；非圆形路径的相位移动同时依赖于磁通历史和路径几何，体现规范势与感应电场的混合作用；推广到非零外磁场时，圆形路径的相位移动与时间平均包围磁通成正比，总相位仅取决于初始磁通eΦ_enc(R,0)，而一般路径受洛伦兹力影响产生额外路径依赖性。

Conclusion: 该研究明确了时变条件下规范依赖势与感应场在广义AB效应中的作用，为量子技术中的相位调控提供了新的理论见解，并深化了对AB效应局域与非局域解释的理解。

Abstract: The Aharonov-Bohm (AB) effect highlights the fundamental role of electromagnetic potentials in quantum mechanics, manifesting as a phase shift for a charged particle in field-free regions. While well-established for static magnetic fluxes, the effect's behavior under time-varying fluxes remains an open and debated question. Employing the WKB method, we derive the AB phase shift for a time-dependent magnetic vector potential, demonstrating that for circular paths in the quasistatic regime, it is proportional to the time-averaged enclosed magnetic flux, \(Δφ_{\rm AB} = \frac{1}{T} \int_0^T e Φ(t) \, dt\), with the total phase shift, including kinetic contributions, equaling \(e Φ(0)\). For non-circular paths, the phase shift depends on both the flux history and path geometry, revealing the effect's hybrid nature involving gauge potentials and induced electric fields. We verify the consistency of our gauge choice with Maxwell's equations and discuss the implications for local versus nonlocal interpretations of the AB effect. We also generalize the results to scenarios with nonzero external magnetic fields, where the enclosed flux is through the actual electron paths, and for circular paths of radius $R$, the AB phase shift is also proportional to the time average of the enclosed flux \(Φ_{\rm enc}(R,t)\), with the total phase shift depending only on the initial enclosed flux \(e Φ_{\rm enc}(R,0)\); for general non-circular paths, the external magnetic field affects trajectories and phase accumulation through the Lorentz force, leading to additional path dependence. These findings clarify the role of gauge-dependent potentials and induced fields in the generalized AB effect, offering new theoretical insights and potential applications in quantum technologies.

</details>


### [213] [From Joint to Single-System Psi-Onticity Without Preparation Independence](https://arxiv.org/abs/2601.17662)
*Shan Gao*

Main category: quant-ph

TL;DR: 论文揭示：PBR定理对复合系统ψ-本体性的证明，无需PIP假设即可通过量子力学张量积结构直接推出子系统ψ-本体性，从而堵住了ψ-认知模型的漏洞。


<details>
  <summary>Details</summary>
Motivation: 挑战"拒绝制备独立性假设(PIP)就能保持ψ-认知模型可能性"的主流观点，指出该理解存在根本性不完整，旨在消除PBR定理中这一长期存在的理论漏洞。

Method: 采用理论推导方法，基于量子力学的张量积结构，从复合系统产品态的ψ-本体性直接逻辑推出子系统ψ-本体性，绕过PIP假设。

Result: 发现子系统ψ-本体性不依赖于PIP，而是直接源于量子力学的基本结构，从而无需任何额外辅助假设即可确立。

Conclusion: 该发现简化了PBR定理的假设体系，彻底关闭了通过拒绝PIP来维护ψ-认知模型的路径，为ψ-本体论提供了更稳固的概念基础。

Abstract: The Pusey-Barrett-Rudolph (PBR) theorem establishes $ψ$-onticity for individual quantum systems, but its standard formulation relies on the Preparation Independence Postulate (PIP). This has led to a prevalent view that rejecting PIP leaves open the possibility of $ψ$-epistemic models for individual systems. In this work, we show that this understanding is incomplete: once the PBR theorem establishes $ψ$-onticity for composite systems prepared in product states, the $ψ$-onticity of the individual subsystems follows directly from the tensor-product structure of quantum mechanics, without invoking PIP or any further auxiliary assumptions. This result removes a key auxiliary assumption from the PBR theorem, closes a persistent loophole for preserving $ψ$-epistemic models, and strengthens the conceptual foundations of $ψ$-ontology.

</details>


### [214] [Comment on "Aharonov-Bohm Phase is Locally Generated Like All Other Quantum Phases"](https://arxiv.org/abs/2601.17665)
*Shan Gao*

Main category: quant-ph

TL;DR: 本文评论并反驳了Marletto和Vedral关于Aharonov-Bohm相由纠缠局部介导的主张，通过数学分析指出其模型的错误，并重申该相位于传统量子电动力学中由矢量势A驱动，而非纠缠所致。


<details>
  <summary>Details</summary>
Motivation: 纠正对Aharonov-Bohm效应的误解，挑战原论文中"纠缠是AB相因果机制"的观点，维护量子电动力学中矢量势A的核心地位。

Method: 对原模型进行批判性数学分析，检验其基于场的能量表达式，验证规范独立性声明，并与标准量子电动力学预测对比。

Result: 发现原模型存在数学缺陷：前置因子错误导致Coulomb规范下出现+qv·A_s而非正确的-qv·A_s；该等价性仅在静态Coulomb规范下近似成立，不适用于时变场或其他规范；非闭合路径的AB相具有规范依赖性。

Conclusion: AB相并非由纠缠产生，而是源于带电粒子与矢量势A的相互作用；纠缠仅是量子电动力学的副产品，不起因果作用；传统的半经典解释得到确认。

Abstract: Marletto and Vedral [Phys. Rev. Lett. 125, 040401 (2020)] propose that the Aharonov-Bohm (AB) phase is locally mediated by entanglement between a charged particle and the quantized electromagnetic field, asserting gauge independence for non-closed paths. In this Comment, we critically analyze their model and demonstrate that the AB phase arises from the interaction with the vector potential \(\mathbf{A}\), not from entanglement, which is a byproduct of the quantum electrodynamics (QED) framework. We show that their field-based energy formulation, intended to reflect local electromagnetic interactions, is mathematically flawed due to an incorrect prefactor and yields \( +q \mathbf{v} \cdot \mathbf{A}_{\mathbf{s}} \) in the Coulomb gauge, conflicting with QED's \( -q \mathbf{v} \cdot \mathbf{A}_{\mathbf{s}} \). This equivalence to \( q \mathbf{v} \cdot \mathbf{A}_{\mathbf{s}} \) holds only approximately in the Coulomb gauge under static conditions, failing for time-dependent fields and other gauges, undermining their claim of a gauge-independent local mechanism. Furthermore, we confirm that the AB phase is gauge-dependent for non-closed paths, contradicting their assertion. Our analysis reaffirms the conventional explanation in the semi-classical picture, where the AB phase is driven by the vector potential \(\mathbf{A}\), with entanglement playing no causal role in its generation.

</details>


### [215] [Efficient Trotter-Suzuki Schemes for Long-time Quantum Dynamics](https://arxiv.org/abs/2601.18756)
*Marko Maležič,Johann Ostmeyer*

Main category: quant-ph

TL;DR: The paper presents a framework for optimizing high-order Trotter-Suzuki decompositions by parameter optimization, discovering new 4th and 6th order schemes that significantly outperform traditional low-order methods in simulating long-time many-body system dynamics.


<details>
  <summary>Details</summary>
Motivation: Low-order Trotter-Suzuki decompositions have rapidly growing errors that limit long-time simulations of many-body systems, despite being straightforward to implement. There's a need for more efficient high-order schemes.

Method: The authors developed a framework that identifies the structure of Trotter-Suzuki schemes and directly optimizes their parameters over a high-dimensional space to construct efficient high-order decompositions.

Result: Discovered new schemes with significantly improved efficiency over traditional Suzuki and Yoshida constructions; recommended two novel schemes at 4th and 6th order; demonstrated effectiveness on Heisenberg model and quantum harmonic oscillator; outperformed low-order schemes like Leapfrog even with large time steps; found that decompositions with more uniform coefficients feature improved error accumulation.

Conclusion: The proposed optimization framework successfully creates highly efficient high-order Trotter-Suzuki schemes, particularly those with uniform coefficients, making long-time many-body dynamics simulations more feasible with reduced error accumulation.

Abstract: Accurately simulating long-time dynamics of many-body systems is a challenge in both classical and quantum computing due to the accumulation of Trotter errors. While low-order Trotter-Suzuki decompositions are straightforward to implement, their rapidly growing error limits access to long-time observables. We present a framework for constructing efficient high-order Trotter-Suzuki schemes by identifying their structure and directly optimizing their parameters over a high-dimensional space. This method enables the discovery of new schemes with significantly improved efficiency compared to traditional constructions, such as those by Suzuki and Yoshida. Based on the theoretical efficiency and practical performance, we recommend two novel highly efficient schemes at $4^{\textrm{th}}$ and $6^{\textrm{th}}$ order. We also demonstrate the effectiveness of these decompositions on the Heisenberg model and the quantum harmonic oscillator, and find that for a fixed final time they perform better across the computational cost. Even when using large time steps, they surpass established low-order schemes like the Leapfrog. Finally, we investigate the in-practice performance of different Trotter schemes and find the decompositions with more uniform coefficients tend to feature improved error accumulation over long times. We have included this observation into our choice of recommended schemes.

</details>


### [216] [Nontrivial bounds on extractable energy in quantum energy teleportation for gapped manybody systems with a unique ground state](https://arxiv.org/abs/2601.18718)
*Taisanul Haque*

Main category: quant-ph

TL;DR: This paper proves that energy extraction in quantum energy teleportation decays exponentially with distance between measurement and operation regions in gapped lattice systems, establishing a fundamental spatial limitation.


<details>
  <summary>Details</summary>
Motivation: Understanding fundamental limits of energy manipulation in quantum systems is crucial for quantum information processing and thermodynamics. Quantum energy teleportation (QET) protocols require quantification of how far energy can be effectively extracted, which this work addresses for realistic lattice systems.

Method: The authors employ a nonperturbative approach using the variational characterization of ground states combined with exponential clustering properties guaranteed by the spectral gap. They assume mild regularity conditions on the Hamiltonian and uniform operator-norm bounds on local measurements.

Result: They establish a universal bound: |E_A-E_B| ≤ C e^(-μd), where d is the distance between regions A (measurement) and B (unitary operations), and C, μ are positive constants determined by the spectral gap, interaction range, and local operator norms. This bound is explicit up to model-dependent constants.

Conclusion: The work demonstrates that quantum energy teleportation faces an intrinsic exponential distance limitation in gapped systems, providing fundamental constraints for protocol design and revealing that long-range energy extraction becomes exponentially suppressed, which has significant implications for quantum thermodynamic device engineering.

Abstract: We establish a universal, exponentially decaying upper bound on the average energy that can be extracted in quantum energy teleportation (QET) protocols executed on finite-range gapped lattice systems possessing a unique ground state. Under mild regularity assumptions on the Hamiltonian and uniform operator-norm bounds on the local measurement operators, there exist positive constants $C$ and $μ$ (determined by the spectral gap, interaction range and local operator norms) such that for any local measurement performed in a region $A$ and any outcome-dependent local unitaries implemented in a disjoint region $B$ separated by distance $d=\operatorname{dist}(A,B)$ one has $|E_A-E_B|\le C\,e^{-μd}.$ The bound is nonperturbative, explicit up to model-dependent constants, and follows from the variational characterization of the ground state combined with exponential clustering implied by the spectral gap.

</details>


### [217] [Realisation of Protected Cat Qutrit via Engineered Quantum Tunnelling](https://arxiv.org/abs/2601.17675)
*Sangil Kwon,Daisuke Hoshi,Toshiaki Nagase,Daichi Sugiyama,Hiroto Mukai,Kengo Takemura,Rintaro Kojima,Yu Zhou,Shohei Watabe,Fumiki Yoshihara,Jaw-Shen Tsai*

Main category: quant-ph

TL;DR: Researchers implemented a three-photon Kerr parametric oscillator (KPO) as a protected qutrit, demonstrating quantum coherence through Rabi oscillations and Wigner function measurements, observing breathing dynamics that reveal protection mechanisms, and identifying ways to improve photon occupation suppression.


<details>
  <summary>Details</summary>
Motivation: To create protected qubits/qutrits with biased-noise properties using engineered quantum tunnelling in phase space, specifically exploring multi-photon transitions combined with Kerr nonlinearity as an alternative to conventional qubit platforms.

Method: Implemented a three-photon Kerr parametric oscillator (KPO) system and conducted experiments including three-photon Rabi oscillations and direct Wigner function measurements to characterize quantum coherence and state properties.

Result: Demonstrated quantum coherence with three-photon Rabi oscillations, observed three-component cat-like states via Wigner functions, discovered breathing-like phase space dynamics from qutrit-excited state interference (frequency matching energy gap as protection hallmark), and identified higher-order pump terms as the main mechanism limiting photon occupation.

Conclusion: The three-photon KPO shows basic quantum properties suitable for protected qutrit operation; mitigating identified pump terms is necessary to maximize protection, establishing this as a viable first step toward an alternative qutrit platform.

Abstract: Engineering quantum tunnelling in phase space has emerged as a viable method for creating a protected qubit with biased-noise properties. A promising approach is to combine a Kerr nonlinearity with multi-photon transitions, resulting in a system known as a Kerr parametric oscillator (KPO). In this work, we implement a three-photon KPO and explore its potential as a protected qutrit. We confirm quantum coherence by demonstrating three-photon Rabi oscillations and performing direct Wigner function measurements that reveal three-component cat-like states. We observe breathing-like dynamics in phase space, arising from exotic temporal interference between the qutrit and excited states. The frequency of this interference corresponds to the energy gap between the qutrit and excited manifolds, thereby providing an experimental hallmark of qutrit space protection. We also identify a higher-order pump term as the main mechanism suppressing photon occupation; mitigating this term is necessary to maximize protection. Our findings elucidate the basic quantum properties of the three-photon KPO and establish the first step toward its use as an alternative qutrit platform.

</details>


### [218] [Exponential Quantum Speedup on Structured Hard Instances of Maximum Independent Set](https://arxiv.org/abs/2601.17686)
*Vicky Choi*

Main category: quant-ph

TL;DR: 本文提出一种非stoquastic绝热量子算法，在结构化最大独立集问题上实现指数级加速，利用XX-driver产生量子干涉绕过隧穿，并提供可在当前量子计算机上验证的小规模模型。


<details>
  <summary>Details</summary>
Motivation: 为实际相关的组合优化问题建立量子加速是量子计算领域的核心挑战。

Method: 识别一类结构化的经典困难最大独立集实例，设计并分析利用该结构的非stoquastic绝热量子优化算法。

Result: 该算法在多项式时间内运行，相对于横场量子退火和经典最优求解器实现指数级加速。关键机制是非stoquastic XX-driver通过符号生成量子干涉创建平滑演化路径以绕过隧穿，并提供了可扩展的小规模模型用于实验验证。

Conclusion: 非stoquastic驱动通过量子干涉实现量子加速，提供了一种不太可能存在高效经典模拟的量子优势机制，并为当前量子计算机的验证提供了具体模型。

Abstract: Establishing quantum speedup for computationally hard problems of practical relevance, particularly combinatorial optimization problems, remains a central challenge in quantum computation. In this work, we identify a structurally defined family of classically hard maximum independent set (MIS) instances, and design and analyze a non-stoquastic adiabatic quantum optimization algorithm that exploits this structure. The algorithm runs in polynomial time and achieves an exponential speedup over both transverse-field quantum annealing and state-of-the-art classical solvers on these instances, under assumptions supported by analytical and numerical evidence. We identify the essential quantum mechanism enabling the speedup as the use of a non-stoquastic XX-driver to access a larger sign-structured admissible subspace beyond the stoquastic regime, which allows sign-generating quantum interference to create smooth evolution paths that bypass tunneling. This identifies a distinctive quantum mechanism underlying the speedup and explains why no efficient classical analogue is likely to exist. In addition, our analysis produces scalable small-scale models, derived from our structural reduction, that capture the essential dynamics of the algorithm. These models provide a concrete opportunity for verification of the quantum advantage mechanism on currently available universal quantum computers.

</details>


### [219] [Traversability dynamics of minimal Sachdev-Ye-Kitaev Wormhole-inspired teleportation protocol with a parity-time ($\mathcal{PT}$)-symmetric non-Hermitian deformation](https://arxiv.org/abs/2601.17688)
*Sudhanva Joshi,Sunil Kumar Mishra*

Main category: quant-ph

TL;DR: 本研究探讨PT对称非幺正形变对基于SYK模型的全息隐形传态中虫洞可穿越性的影响。通过引入增益/损耗，发现谱奇异点驱动的相变，PT破缺相可指数放大信号并保持因果性。临界阈值呈对数正态分布，深处存在纯化效应，实现近完美保真度。


<details>
  <summary>Details</summary>
Motivation: 探索利用非厄米拓扑增强全息量子通信的鲁棒性机制，解决量子多体系统中信号放大与噪声问题，并研究虫洞可穿越性的调控方式。

Method: 采用耦合Sachdev-Ye-Kitaev系统作为全息隐形传态模型，制备于热场双态浴中，向边界哈密顿量引入平衡的增益和损耗项，分析有效哈密顿量的能谱演化、奇异点和非幺正动力学行为，并进行无序实现的统计研究。

Result: 1. 在边界哈密顿量中引入平衡增益/损耗后，系统出现由谱奇异点驱动的非厄米相变，实能量本征值在奇异点处合并为复共轭对；2. PT破缺相起到放大器作用，使隐形传态信号指数增长，同时保持虫洞可穿越的因果时间窗；3. 临界非厄米阈值γ_c服从对数正态分布，表明相变对SYK谱微观能级间距高度敏感；4. 在PT破缺相深处观察到"纯化"效应，该传态通道可作为纠缠蒸馏器，对后选择态实现近完美保真度。

Conclusion: 非厄米拓扑结构可用于增强全息量子通信，为含噪、最小量子多体系统中的信号放大提供鲁棒机制，这对量子信息处理和量子引力研究具有重要意义。

Abstract: Holography-inspired teleportation has recently emerged as a significant area of research in quantum many-body systems. In this work, we investigate the effects of $\mathcal{PT}$ symmetric non-unitary deformations on the traversability of the wormhole-inspired teleportation protocol modeled by coupled Sachdev-Ye-Kitaev systems prepared in a Thermofield Double state bath. By introducing balanced gain and loss terms to the boundary Hamiltonians, we identify a phase transition driven by spectral exceptional points, where the real energy eigenvalues of the effective Hamiltonian coalesce and bifurcate into complex conjugate pairs. We demonstrate that the $\mathcal{PT}$-broken phase acts as an amplifier, enabling exponential growth in the norm of the teleported signal while preserving the causal time window for the wormhole's traversability. A statistical study of disorder realizations reveals that the critical non-Hermiticity threshold $γ_c$ follows a log-normal distribution, reflecting the sensitivity of the transition to the microscopic level spacing of the chaotic SYK spectrum. Furthermore, we observe a ``Purification" effect deep in the broken phase, where the teleportation channel acts as an entanglement distiller, yielding near-perfect teleportation fidelity for post-selected states. Our results suggest that the non-Hermitian topology can be harnessed to enhance holographic quantum communication, providing a robust mechanism for signal amplification in noisy, minimal quantum many-body systems.

</details>


### [220] [Reducing Circuit Resources in Grover's Algorithm via Constraint-Aware Initialization](https://arxiv.org/abs/2601.17725)
*Eunok Bae,Jeonghyeon Shin,Minjin Choi*

Main category: quant-ph

TL;DR: 提出约束感知初始化框架，通过结构化初始态减少Grover算法搜索空间，在精确覆盖问题中验证其资源效率优于标准均匀初始化


<details>
  <summary>Details</summary>
Motivation: Grover算法虽提供二次加速，但组合优化问题的大搜索空间仍存挑战；通过初始态编码约束可缩小有效搜索空间，需平衡查询复杂度与电路开销

Method: 建立线性约束的预处理框架，生成编码约束的结构化初始态，并进行保守的电路级资源分析（门数量/深度）

Result: 约束感知初始化在精确覆盖问题中降低oracle查询次数，且电路级资源（门数/深度）效率优于标准初始化

Conclusion: 该框架为Grover算法提供了资源更高效实现的实用基线，尤其适用于约束组合优化问题

Abstract: Grover's search algorithm provides a quadratic speedup over classical brute-force search in terms of query complexity and is widely used as a versatile subroutine in numerous quantum algorithms, including those for combinatorial problems with large search spaces. For such problems, it is natural to reduce the effective search space by incorporating problem constraints at the initialization step, which in Grover's algorithm can be achieved by preparing structured initial states that encode constraint information. In this work, we present a systematic framework with a simple preprocessing procedure for constraint-aware initialization in Grover's algorithm, focusing on problems with linear constraints. While such structured initial states can reduce the number of oracle queries required to obtain a solution, their preparation incurs additional circuit-level costs. We therefore offer a conservative circuit-level resource analysis, showing that the resulting constraint-aware initialization can improve resource efficiency in terms of gate counts and circuit depth. The validity of the framework is further demonstrated numerically using the exact-cover problem. Overall, our results indicate that this approach serves as a practical baseline for achieving more resource-efficient implementations of Grover's algorithm compared to the standard uniform initialization.

</details>


### [221] [Quantum fast-forwarding fermion-boson interactions via the polaron transform](https://arxiv.org/abs/2601.17732)
*Harriet Apel,Burak Şahinoğlu*

Main category: quant-ph

TL;DR: 提出新型量子算法，通过高效酉变换实现费米子-玻色子相互作用项的快速演化，将计算复杂度从多项式级降至Λ的多对数级（polylogarithmic），在Hubbard-Holstein模型上验证并推广至其他模型。


<details>
  <summary>Details</summary>
Motivation: 费米子-玻色子相互作用体系对理解关联现象至关重要，但经典模拟极其困难；现有量子算法计算成本随玻色子截断参数Λ呈多项式增长，存在效率瓶颈。

Method: 识别并应用高效酉变换实现相互作用项的"快速前向演化"，基于相互作用图像设计新量子模拟算法，显著降低对Λ的依赖。

Result: 新算法复杂度为polylogarithmic in Λ（原方法为polynomial in Λ），在Hubbard-Holstein模型中实现高效模拟，并推广至其他费米子-玻色子模型。

Conclusion: 该方法在玻色子截断参数依赖上实现渐进性突破，证明特定模型的费米子-玻色子相互作用可用接近纯费米子系统的资源进行模拟，具有重要理论价值。

Abstract: Simulating interactions between fermions and bosons is central to understanding correlated phenomena, yet these systems are inherently difficult to treat classically. Previous quantum algorithms for fermion-boson models exhibit computation costs that scale polynomially with the bosonic truncation parameter, $Λ$. In this work we identify the efficient unitary transformation enabling fast-forwarded evolution of the fermion-boson interaction term, yielding an interaction-picture based simulation algorithm with complexity polylogarithmic in $Λ$. We apply this transformation to explicitly construct an efficient quantum algorithm for the Hubbard-Holstein model and discuss its generalisation to other fermion-boson interacting models. This approach yields an important asymptotic improvement in the dependence on the bosonic cutoff and establishes that, for certain models, fermion-boson interactions can be simulated with resources comparable to those required for purely fermionic systems.

</details>


### [222] [Simple, Efficient, and Generic Post-Selection Decoding for qLDPC codes](https://arxiv.org/abs/2601.17757)
*Haipeng Xie,Nobuyuki Yoshioka,Kento Tsubouchi,Ying Li*

Main category: quant-ph

TL;DR: A post-selection decoding strategy called argument reweighting that significantly improves quantum error correction performance by reweighting error models during decoding, reducing logical error rates by nearly two orders of magnitude with minimal rejection.


<details>
  <summary>Details</summary>
Motivation: Quantum error correction is essential for scalable quantum computation, but achieving sufficiently low logical error rates for practical algorithms remains challenging on existing hardware despite encoding logical qubits.

Method: Argument reweighting - a simple, broadly applicable post-selection decoding strategy that boosts maximum-likelihood-type decoders (including minimum-weight perfect matching and belief-propagation) by performing additional decoding rounds under reweighted error models to accept high-confidence syndrome outcomes.

Result: Circuit-level simulations show the method substantially suppresses logical errors across multiple decoders and qLDPC codes; for the [[144,12,12]] bivariate bicycle code, a rejection rate of only 1.44×10⁻⁵ reduces logical error rate by almost two orders of magnitude.

Conclusion: Argument reweighting is a practical and resource-efficient approach for enhancing quantum fault tolerance in quantum computing.

Abstract: Quantum error correction is indispensable for scalable quantum computation. Although encoding logical qubits substantially enhances noise resilience, achieving logical error rates low enough for practical algorithms remains challenging on existing hardware. Here we introduce argument reweighting, a simple and broadly applicable post-selection decoding strategy that boosts the performance of maximum-likelihood-type decoders, including minimum-weight perfect matching and belief-propagation families. The method suppresses logical errors by performing additional decoding rounds under reweighted error models, enabling acceptance of high-confidence syndrome outcomes. Circuit-level simulations across multiple decoders and qLDPC codes show that argument reweighting substantially suppresses logical errors, requiring a rejection rate of only $1.44\times10^{-5}$ to reduce the logical error rate by almost two orders of magnitude for the $[[144,12,12]]$ bivariate bicycle code. These results establish argument reweighting as a practical and resource-efficient approach for enhancing quantum fault tolerance.

</details>


### [223] [Kirkwood-Dirac Quasiprobability as a Universal Framework for Quantum Measurements Across All Regimes](https://arxiv.org/abs/2601.17788)
*Bo Zhang,Yusuf Turek*

Main category: quant-ph

TL;DR: 该研究解决了Kirkwood-Dirac准概率在量子测量中的适用性问题，证实其可作为所有测量强度的统一框架，其中指针诱导退相干控制着从弱测量到强测量的连续转变。


<details>
  <summary>Details</summary>
Motivation: 长期以来，Kirkwood-Dirac准概率在何时最适用于描述量子测量这一问题悬而未决，特别是在不同测量强度下的物理机制不明。

Method: 提出指针诱导退相干是控制测量强度连续转变的普适机制，构建KD准概率的统一框架。

Result: 退相干因子F(t)同时量化相干性损失并内插测量强度，使KD准概率从弱测量的复数形式自然演化为投影测量的实数Wigner函数，全程保持信息完备性。

Conclusion: 确立KD分布为连接所有量子测量区域的普适框架，通过退相干路径解决了其适用性的根本问题。

Abstract: The question of when the Kirkwood-Dirac quasiprobability serves as the most appropriate description for quantum measurements has remained unresolved, particularly across different measurement strengths. While known to generate anomalous weak values in the weak measurement regime and to reduce to classical probabilities under projective measurement, the physical mechanism governing its continuous transformation has been lacking. Here we demonstrate that the KD quasiprobability provides a general framework for all measurement regimes by identifying pointer-induced decoherence as the universal mechanism controlling this transition. We show that the decoherence factor F(t) simultaneously quantifies the loss of quantum coherence and interpolates the measurement strength from weak to strong. Within this framework, the KD quasiprobability naturally deforms from its full complex form-governing weak values-to the real, non-negative Wigner formula describing projective measurements, while maintaining informational completeness throughout the transition. Our work resolves the fundamental question of the KD distribution's applicability by establishing it as the universal framework that seamlessly connects all quantum measurement regimes through a physically transparent decoherence pathway.

</details>


### [224] [Bosonic Diffusive Channel: Quantum Metrology via Finite Non-Gaussian Resource](https://arxiv.org/abs/2601.17804)
*Arman,Prasanta K. Panigrahi*

Main category: quant-ph

TL;DR: This paper optimizes dephasing decoherence estimation in continuous-variable quantum systems using non-Gaussian probe states (squeezed cat/compass states) via quantum Fisher information, validated by simulations. An ancilla-based method enables cavity field estimation via qubit measurements.


<details>
  <summary>Details</summary>
Motivation: To accurately estimate dephasing-induced decoherence in continuous-variable quantum systems, especially when direct cavity field measurements are impractical, by identifying optimal non-Gaussian probe states.

Method: Purifying the open system to analyze decoherence, then using quantum Fisher information to identify optimal probes (squeezed cat states and symmetric squeezed compass states). For inaccessible cavities, an ancilla qubit traverses the cavity, enabling estimation via Wigner function reconstruction or marginal distribution measurements.

Result: Optimal probes are squeezed cat and symmetric squeezed compass states, with results consistent with numerical simulations. The ancilla approach successfully estimates dephasing rates through indirect qubit measurements.

Conclusion: Non-Gaussian probe states significantly enhance dephasing decoherence estimation in continuous-variable systems, and the ancilla method provides a viable alternative when direct field measurements are impossible.

Abstract: We investigate the estimation of dephasing-induced decoherence in continuous-variable quantum systems using non-Gaussian probe states. By purifying the open system, we identify optimal probes, specifically squeezed cat and symmetric squeezed compass states, via quantum Fisher information. These results are in agreement with numerical simulation. In settings where the intra-cavity field is inaccessible and standard measurements are impractical, utilizing an ancilla approach where a qubit traverses or interacts with the cavity field, leading to measurement of the qubit, hence allowing estimation of the dephasing rate via Wigner function reconstruction or less costly marginal distribution.

</details>


### [225] [Multivariate Rényi divergences characterise betting games with multiple lotteries](https://arxiv.org/abs/2601.17850)
*Andrés F. Ducuara,Erkka Haapasalo,Ryo Takakura*

Main category: quant-ph

TL;DR: 该论文为多元Rényi散度建立了基于博弈论和风险规避的新操作解释，证明其等于理性智能体在多彩票投注场景中的经济价值（指数化确定性等价），并扩展到含辅助信息的条件散度，最终应用于量子资源理论中的信息测量问题。


<details>
  <summary>Details</summary>
Motivation: 将抽象的多元Rényi散度与信息论、经济学建立可操作的联系，解决量子资源理论中多彩票投注游戏的操作意义问题，为量子信息度量提供经济学基础。

Method: 1. 构建基于博弈论的经济任务模型：定义理性智能体在多彩票投注场景中的风险规避行为；2. 建立Rényi散度参数与风险规避系数的映射（R_k = 1+α_k/α_0）；3. 推导公平赔率下最优投注策略的确定性等价与散度的指数关系；4. 提出条件多元Rényi散度并证明其数据处理不等式。

Result: 1. 证明多元Rényi散度D_α(P_X)精确量化智能体对d个彩票的经济价值，且w^ICE_R = exp[D_α(P_X)]；2. 新条件散度满足数据处理不等式，其值等于辅助信息带来的经济价值增量；3. 该不等式与风险规避态度互为充要条件；4. 成功应用于一般概率理论(GPTs)中的信息测量资源理论。

Conclusion: 建立了信息论、经济学与量子物理的定量桥梁，为量子资源理论提供了首个基于博弈论的多彩票投注操作框架，解决了量子态投注游戏的经济学基础问题，拓展了Rényi散度的应用范围。

Abstract: We provide an operational interpretation of the multivariate Rényi divergence in terms of economic-theoretic tasks based on betting, risk aversion, and multiple lotteries. We show that the multivariate Rényi divergence $D_{\underlineα}(\vec{P}_X)$ of probability distributions $\vec{P}_X =(p^{(0)}_X,\dots,p^{(d)}_X)$ and real-valued orders $\underlineα = (α_0, \dots, α_d)$ quantifies the economic-theoretic value that a rational agent assigns to $d$ lotteries with odds $o^{(k)}_X \propto (p_X^{(k)})^{-1}$ ($k=1,\dots,d$) on a random event described by $p^{(0)}_X$. In particular, when the odds are fair and the rational agent maximises over all betting strategies, the economic-theoretic value (the isoelastic certainty equivalent) that the agent assigns to the lotteries is exactly given by $w^{\mathrm{ICE}}_{\underline{R}}=\exp[D_{\underlineα}(\vec{P}_X)]$, where $\underline{R}=(R_1,\dots,R_d)$ is a risk-aversion vector with $R_k = 1+α_k/α_0$ being the risk-aversion parameter for lottery $k$. Furthermore, we introduce a new conditional multivariate Rényi divergence that characterises a generalised scenario where the agent uses side information. We prove that this new quantity satisfies a data processing inequality which can be interpreted as the increment in the economic-theoretic value provided by side information; crucially, such a data processing inequality is a consequence of the agent's economic-theoretically consistent risk-averse attitude towards every lottery and vice versa. Finally, we apply these results to the resource theory of informative measurements in general probabilistic theories (GPTs). By establishing quantitative connections between information theory, physics, and economics, our framework provides a novel operational foundation for quantum state betting games with multiple lotteries in the realm of quantum resource theories.

</details>


### [226] [On Tunneling in the Quantum Multiverse](https://arxiv.org/abs/2601.17856)
*S. E. Ennadifi*

Main category: quant-ph

TL;DR: This paper reinterprets quantum tunneling through the Many-Worlds Interpretation, where wavefunction splitting creates distinct "tunneled" and "reflected" worlds, allowing tunneling probability and time to be understood via branch weights and decoherence duration, extending to macroscopic scales.


<details>
  <summary>Details</summary>
Motivation: The longstanding interpretational controversy in quantum mechanics surrounding quantum tunneling phenomena.

Method: Heuristically addressing quantum tunneling within the Everettian quantum multiverse framework, where the universal wavefunction splits into decohered reflected and transmitted branches under environmental effects after encountering a potential barrier, with the observer experiencing tunneling in a tunneled world.

Result: Investigates tunneling probability via tunneled world relative weights and tunneling time via branching duration; discusses macroscopic quantum tunneling and approaches macroscopic tunneling time using obtained results and known data.

Conclusion: The Everettian multiverse framework offers a new perspective on quantum tunneling by interpreting it as an observer's experience within a decohered branch, enabling analysis of both microscopic and macroscopic tunneling through branch weights and decoherence timescales.

Abstract: Prompted by the longstanding interpretational controversy in quantum mechanics, quantum tunneling is heuristically addressed within the Everettian quantum multiverse. In this framework, the universal wavefunction splits into decohered reflected and transmitted branches under the environmetal effect after encountring a potential barrier. The observed tunneling is then experienced by the observer located in a tunneled world. The tunneling probability and the tunneling time are investigated in terms of the tunneled world relative weights and the branching duration, respectively. The macroscopic quantum tunneling, recently honored, is also discussed and the corresponding macroscopic tunneling time is approached based on the obtained results and known data.

</details>


### [227] [Coherent Amplifier-Empowered Quantum Interferometer: Preserving Sensitivity and Quantum Advantage under High Loss](https://arxiv.org/abs/2601.17876)
*Jie Zhao,Zeliang Wu,Haoran Liu,Yueya Liu,Xin Chen,Xinyun Liang,Wenfeng Huang,Chun-Hua Yuan,L. Q. Chen*

Main category: quant-ph

TL;DR: 提出相干放大器增强的量子干涉仪方案，通过相位敏感的光子放大抑制损耗导致的性能衰减，在90%高损耗下保持超越标准量子极限的相位灵敏度，解决了量子干涉仪实用化的关键瓶颈


<details>
  <summary>Details</summary>
Motivation: 量子干涉仪虽具备超越标准量子极限的相位测量能力，但实际应用中不可避免的光学损耗会严重劣化灵敏度和量子增强因子，这是阻碍其走向实际应用的最主要挑战

Method: 在量子干涉仪中引入相干放大器，利用其相位敏感的光子放大特性来保护量子态，从而抑制高损耗条件下的性能衰减

Result: 实验注入4.2 dB压缩真空态时，在90%损耗下：量子增强因子劣化从传统方案的3.7 dB降至1.5 dB；相位灵敏度劣化仅4.0 dB，远优于传统方案的11.2 dB；即使损耗超过90%仍能保持超越标准量子极限的灵敏度

Conclusion: 该放大器增强方案突破了量子干涉仪实用化的关键障碍，为在损耗环境中实现鲁棒的量子增强测量提供了可行路径

Abstract: Quantum interferometers offer phase measurement capabilities that surpass the standard quantum limit (SQL), with phase sensitivity and quantum enhancement factor serving as key performance metrics. However, practical implementations face severe degradation of both metrics due to unavoidable losses, representing the foremost challenge in advancing quantum interferometry toward real-world applications. To address this challenge, we propose a coherent-amplifier-empowered quantum interferometer. The coherent amplifier dramatically suppresses the decay of both sensitivity and quantum enhancement under high-loss conditions, maintaining phase sensitivity beyond the original SQL even for losses exceeding 90%. Using an injected 4.2 dB squeezed-vacuum state in experimental demonstration, our scheme reduces the quantum enhancement degradation under 90% loss from 3.7 dB in a conventional quantum interferometer (CQI) to only 1.5 dB. More importantly, the phase sensitivity degradation under the same loss is limited to 4.0 dB, markedly outperforming the 11.2 dB degradation observed in a CQI. This improvement is enabled by the coherent amplifier's phase-sensitive photon amplification and its protection of the quantum state. This breakthrough in amplifier-empowered quantum interferometry overcomes the critical barrier to practical deployment, enabling robust quantum-enhanced measurements in lossy environments.

</details>


### [228] [Colour Centre Formation in Silicon-On-Insulator for On-Chip Photonic Integration](https://arxiv.org/abs/2601.17919)
*Arnulf J. Snedker-Nielsen,David R. Gongora,Magnus L. Madsen,Christian H. Christiansen,Eike L. Piehorsch,Mathias Ø. Augustesen,Elvedin Memisevic,Sangeeth Kallatt,Rodrigo A. Thomas,Mark Kamper Svendsen,Peter Krogstrup Jeppesen,Marianne E. Bathen,Lasse Vines,Peter Granum,Stefano Paesani*

Main category: quant-ph

TL;DR: 该研究探索了硅中色心在量子技术中的应用，通过分析退火和纳米加工过程对量子发射器的影响，发现了最优工艺参数、色心耦合形成动力学，并识别出新型稳定色心。


<details>
  <summary>Details</summary>
Motivation: 硅中的色心作为单光子源和自旋-光子接口在量子技术中具有巨大潜力，但需要实现与绝缘体上硅(SOI)量子器件的大规模制造兼容。

Method: 研究在硅中生成多种类型色心，并系统考察了热退火和光学纳米结构制造步骤对量子发射器存在状态的影响。

Result: 揭示了不同色心之间的耦合形成动力学，确定了退火工艺的最优参数，报告了色心对退火时间和纳米加工过程的敏感性，并识别出硅中前所未有的稳定光学信号。

Conclusion: 该研究为开发可大规模制造的硅基量子器件提供了关键工艺见解，同时发现的新型色心为量子技术发展开辟了新方向。

Abstract: Colour centres in silicon have great potential as single photon sources for quantum technologies. Some of them - like the T centre - also possess optically-active spins that enable spin-photon interfaces for generating entangled photons and multi-spin registers. This paper explores the generation of several types of colour centres in silicon for mass-manufacturable silicon-on-insulator quantum devices. We investigate how different processes in the device development affect the presence of the quantum emitters, including thermal annealing and fabrication steps for optical nanostructures. The study reveals coupled formation dynamics between different colour centres, identifies optimal parameters for annealing processes, and reports on the sensitivity to annealing duration and nanofabrication procedures for photonic integrated circuits. Furthermore, we discern stable optical signals from colour centres in silicon which have not been identified before.

</details>


### [229] [Perturbation Theory and the Quantum Rabi-model](https://arxiv.org/abs/2601.17924)
*Marcello Malagutti,Alberto Parmeggiani*

Main category: quant-ph

TL;DR: 该论文通过微扰理论和谱分析研究量子光学中的Rabi系统，证明了Braak猜想在有限特征值族上的成立，并推广到N能级原子系统的Weyl谱计数函数渐近性研究。


<details>
  <summary>Details</summary>
Motivation: 研究量子Rabi系统的谱性质，特别是验证Braak猜想（关于特征值解析性的猜想），并将模型推广至多能级原子与多腔模系统的谱渐近行为分析。

Method: 第一部分采用Rellich理论对Rabi系统进行微扰分析，建立特征值的解析展开；第二部分研究推广至N能级原子（N≥3）与N-1个电磁腔模系统的Weyl谱计数函数渐近性态。

Result: 证明了有限特征值族上的Braak猜想成立，获得了任意固定长度特征值族的解析展开式；针对广义量子Rabi模型，分析了其谱计数函数的渐近分布规律。

Conclusion: 该工作通过微扰方法确立了Rabi系统特征值的解析结构，验证了Braak猜想，并为更复杂的多体量子光学系统谱分析提供了理论基础，深化了对量子Rabi模型及其推广系统的谱性质理解。

Abstract: In the first part of the paper we study a perturbative model of the Rabi system of Quantum Optics. We therefore are able to describe, through Rellich's theory, an analytic expansion of finite families, but of any fixed length, of eigenvalues. In particular, we prove that for finite families of eigenvalues the Braak conjecture holds. In the second part we study the asymptotics of the Weyl spectral counting function of a class of systems that generalize the Quantum Rabi Model to an $N$-level atom ($N\geq3$) with $N-1$ cavity modes of the electromagnetic field.

</details>


### [230] [The hyperlink representation of entanglement and the inclusion-exclusion principle](https://arxiv.org/abs/2601.17926)
*Silvia N. Santalla,Sudipto Singha Roy,Germán Sierra,Javier Rodríguez-Laguna*

Main category: quant-ph

TL;DR: This paper introduces entanglement hyperlinks (EHLs) as exact extensions of entanglement links, defined via inclusion-exclusion principle, to capture irreducible multipartite entanglement contributions and provide an exact formula for entanglement entropy.


<details>
  <summary>Details</summary>
Motivation: Entanglement entropy (EE) of pure states can only be approximately expressed as a sum of entanglement links, indicating a need for an exact extension that captures multipartite entanglement contributions not reducible to lower-order terms.

Method: Generalized mutual informations are defined through the inclusion-exclusion principle, creating entanglement hyperlinks (EHLs) that quantify irreducible multipartite entanglement contributions.

Result: (1) EHLs crossing factorized partitions must vanish; (2) EHLs between any block set equal the sum of all EHLs joining them; (3) Exact representation of EE for any pure-state block as the sum of EHLs crossing its boundary; (4) Numerical examples demonstrate rich structure in ground states of local Hamiltonians.

Conclusion: Entanglement hyperlinks provide a powerful tool for characterizing multipartite entanglement in quantum information theory and quantum many-body physics.

Abstract: The entanglement entropy (EE) of any bipartition of a pure state can be approximately expressed as a sum of entanglement links (ELs). In this work, we introduce their exact extension, i.e. the entanglement hyperlinks (EHLs), a type of generalized mutual informations defined through the inclusion-exclusion principle, each of which captures contributions to the multipartite entanglement that are not reducible to lower-order terms. We show that any EHL crossing a factorized partition must vanish, and that the EHLs between any set of blocks can be expressed as a sum of all the EHLs that join all of them. This last result allows us to provide an exact representation of the EE of any block of a pure state, from the sum of the EHLs which cross its boundary. In order to illustrate their rich structure, we discuss some explicit numerical examples using ground states of local Hamiltonians. The EHLs thus provide a remarkable tool to characterize multipartite entanglement in quantum information theory and quantum many-body physics.

</details>


### [231] [A Rigorous and Self--Contained Proof of the Grover--Rudolph State Preparation Algorithm](https://arxiv.org/abs/2601.17930)
*Antonio Falco,Daniela Falco-Pomares,Hermann G. Matthies*

Main category: quant-ph

TL;DR: 严格分析Grover-Rudolph量子态制备算法，形式化其二叉树结构并给出{RY,X,CNOT}门的显式实现。


<details>
  <summary>Details</summary>
Motivation: 该算法虽被广泛应用，但正确性论证常为非正式的，对底层二叉树的约定是隐式的。

Method: 形式化二叉概率树，定义条件质量角度映射，归纳证明电路制备正确的测量分布；证明每阶段为均匀控制RY旋转，并用格雷码和沃尔什-哈达玛变换转译。

Result: 严格证明了算法正确性，并提供了无辅助比特的{RY,X,CNOT}门电路实现。

Conclusion: 为Grover-Rudolph算法奠定了严格理论基础，提供了实用电路方案，有助于其在量子计算中的可靠应用。

Abstract: Preparing quantum states whose amplitudes encode classical probability distributions is a fundamental primitive in quantum algorithms based on amplitude encoding and amplitude estimation. Given a probability distribution $\{p_k\}_{k=0}^{2^n-1}$, the Grover--Rudolph procedure constructs an $n$-qubit state $\ketψ=\sum_{k=0}^{2^n-1}\sqrt{p_k}\ket{k}$ by recursively applying families of controlled one-qubit rotations determined by a dyadic refinement of the target distribution. Despite its widespread use, the algorithm is often presented with informal correctness arguments and implicit conventions on the underlying dyadic tree. In this work we give a rigorous and self-contained analysis of the Grover--Rudolph construction: we formalize the dyadic probability tree, define the associated angle map via conditional masses, derive the resulting trigonometric factorizations, and prove by induction that the circuit prepares exactly the desired measurement law in the computational basis. As a complementary circuit-theoretic contribution, we show that each Grover--Rudolph stage is a uniformly controlled $\RY$ rotation on an active register and provide an explicit ancilla-free transpilation into the gate dictionary $\{\RY(\cdot),X,\CNOT(\cdot\to\cdot)\}$ using Gray-code ladders and a Walsh--Hadamard angle transform.

</details>


### [232] [Elementary Quantum Gates from Lie Group Embeddings in $U(2^n)$: Geometry, Universality, and Discretization](https://arxiv.org/abs/2601.17936)
*Antonio Falco,Daniela Falco-Pomares,Hermann G. Matthies*

Main category: quant-ph

TL;DR: 本文提出了一种量子电路的内在描述方法，通过将SU(2)的所有嵌入作为原始门操作，构建了无相位词典，并证明其在SU(2^n)中的通用性，同时给出了基于有限字母表的近似方案。


<details>
  <summary>Details</summary>
Motivation: 传统电路模型中的基本门相对于选定的张量因子化是外在的，缺乏与U(2^n)群本身的内在联系。本文旨在建立一种不依赖外部选择的内在描述层。

Method: 定义N=2^n，构造无相位词典G^SU_elem(n)为所有SU(2)到U(N)嵌入的像集。利用表示论分析嵌入空间的结构，采用Hilbert-Schmidt双不变度量研究几何性质，运用两层QR/Givens分解和显式的对角环面生成方法。

Result: 证明了⟨G^SU_2lvl(n)⟩=SU(N)，即两层无相位门可生成整个特殊酉群；进而⟨G^SU_elem(n)⟩=SU(N)。通过附加对角U(1)相位因子实现U(N)的完全通用性。最后通过嵌入提升Solovay-Kitaev算法，构建了模块化有限字母表近似接口。

Conclusion: 该工作建立了基于所有可能SU(2)嵌入的量子计算新范式，为通用量子计算提供了更自然的数学描述，并给出了实际可实现的近似方案。

Abstract: In the standard circuit model, elementary gates are specified relative to a chosen tensor factorization and are therefore extrinsic to the ambient group $U(2^n)$. Writing $N=2^n$, we introduce an intrinsic descriptor layer in $U(N)$ by declaring as primitive the motions inside faithful embedded copies of $SU(2)$, leading to the phase-free dictionary $\mathcal{G}^{SU}_{\mathrm{elem}}(n)=\bigcup_{φ\in\Emb(SU(2),U(N))}φ(SU(2))$, and we also discuss the phase-inclusive $U(2)$ variant. We show that $\Emb(SU(2),U(N))$ decomposes into finitely many $U(N)$-homogeneous strata indexed by isotypic multiplicities, with stabilizers given by centralizers; the canonical two-level sector is organized by $\Gr_2(\C^N)$ up to a $PSU(2)$ gauge. Equipping $U(N)$ with the Hilbert--Schmidt bi-invariant metric, each embedded subgroup is totally geodesic. Using two-level QR/Givens factorization together with an explicit generation of diagonal tori by two-level phase rotations, we prove phase-free universality $\langle\mathcal{G}^{SU}_{\mathrm{2lvl}}(n)\rangle=SU(N)$ and hence $\langle\mathcal{G}^{SU}_{\mathrm{elem}}(n)\rangle=SU(N)$. Full universality in $U(N)$ follows by adjoining the abelian diagonal/global $U(1)$ factors (equivalently, by passing to the $U(2)$ two-level dictionary). Finally, we record a modular finite-alphabet interface by lifting Solovay--Kitaev approximation in $SU(2)$ through two-level embeddings.

</details>


### [233] [Quantum Radar System Using Born-Feynman path integrals approach](https://arxiv.org/abs/2601.17956)
*Kumar Gautam,Akshit Dutta,Kumar Shubham*

Main category: quant-ph

TL;DR: 本文提出一种基于量子点与玻恩-费曼路径积分方法的量子雷达系统，该系统通过量子点产生纠缠光子对，利用超导纳米线单光子探测器进行探测，并通过分析散射光子与闲置光子的量子关联实现精确量子态比较。


<details>
  <summary>Details</summary>
Motivation: 摘要未明确阐述研究动机，但"精确量子态比较"的表述暗示旨在利用量子纠缠特性提升雷达探测的精度和灵敏度。

Method: 采用量子点作为纠缠光子源，通过自发参量下转换或受激发射产生纠缠光子对；系统由传输模块（微波天线与波束赋形元件）、延迟线模块（同步闲置光子与返回信号光子以保持量子相干性）、探测模块（低温超导纳米线单光子探测器）和信号处理单元（分析量子关联）组成。

Result: 摘要未提供具体实验结果或性能数据，仅描述了系统架构和工作原理，最终目标是实现对散射光子与闲置光子的精确量子态比较。

Conclusion: 摘要未给出明确结论，但系统设计方案表明该量子雷达架构利用量子纠缠和相干性保持机制，为量子态比较探测提供了一种可行的技术实现路径。

Abstract: The paper relates to a quantum radar deployment by the Born-Feynman path integrals approach based on quantum dots. The radar system comprises a quantum dot-based entangled photon generator, a transmission module, a delay line, a detection module, and a signal processing unit. The quantum dot-based entangled photon generator produces entangled photon pairs via spontaneous parametric down-conversion or stimulated emission. The signal transmission module, equipped with a microwave antenna and beamforming elements, directs the signal photon toward a target. The delay line module synchronizes the retained idler photon with the returning signal photon, preserving quantum coherence. The detection module collects the reflected signal photon and uses a cryogenically cooled superconducting nanowire single photon detector (SNSPD) for detection. Finally, the signal processing unit analyzes the quantum correlation between the scattered and idler photons to enable precise quantum state comparison.

</details>


### [234] [Authentication in Security Proofs for Quantum Key Distribution](https://arxiv.org/abs/2601.17960)
*Devashish Tupkary,Shlok Nahar,Ernest Y. -Z. Tan*

Main category: quant-ph

TL;DR: 该论文提出一个约化定理，解决量子密钥分发(QKD)在实际认证信道下的安全性问题。证明在满足温和且易满足的协议条件时，任何在理想认证设置下被证明安全的QKD协议，在面对非对称中止、消息延迟/重排/阻断的实际认证环境时仍保持安全，使现有证明可通过微小协议调整直接适用。


<details>
  <summary>Details</summary>
Motivation: 标准QKD安全证明假设认证信道"诚实"（永不中止、消息忠实传递且保持时序），但实际认证信道存在非对称中止（仅接收方能检测认证失败）及敌手可延迟、重排或阻断消息等问题，导致现有安全定义和证明在实用场景中失效。

Method: 提出并证明一个约化定理，通过将实用认证设置下的潜在攻击归约到理想认证设置下的攻击，证明在满足特定温和条件时，两种设置下的安全性等价。

Result: 获得通用约化定理，允许所有现有QKD安全证明通过一个微小的协议调整，即可"追溯升级"到实用认证设置，无需重新进行复杂的安全证明。

Conclusion: 该工作成功弥合了QKD理论安全证明与实际认证信道之间的关键鸿沟，为QKD协议的实用化部署提供了理论保障，使现有安全证明具有了实际适用性，极大简化了未来QKD系统的安全性验证工作。

Abstract: Quantum Key Distribution (QKD) protocols rely on authenticated classical communication. Typical QKD security proofs are carried out in an idealized setting where authentication is assumed to behave honestly: it never aborts, and all classical messages are delivered faithfully with their original timing preserved. Authenticated channels that can be constructed in practice have different properties. Most critically, such channels may abort asymmetrically, such that only the receiving party may detect an authentication failure while the sending party remains unaware. Furthermore, an adversary may delay, reorder, or block classical messages. This discrepancy renders the standard QKD security definition and existing QKD security proofs invalid in the practical authentication setting. In this work we resolve this issue. Our main result is a reduction theorem showing that, under mild and easily satisfied protocol conditions, any QKD protocol proven secure under the honest authentication setting remains secure under a practical authentication setting. This result allows all existing QKD proofs to be retroactively lifted to the practical authentication setting with a minor protocol tweak.

</details>


### [235] [Quantum Paradoxes and the Quantum-Classical Transition under Unitary Measurement Dynamics with Random Hamiltonians](https://arxiv.org/abs/2601.17976)
*Alexey A. Kryukov*

Main category: quant-ph

TL;DR: 该论文提出了一种基于随机酉演化的量子测量动力学框架，利用随机哈密顿量和探测器分辨率约束，在不引入波函数坍缩的情况下，统一解释了量子测量、态约化和量子-经典过渡现象。


<details>
  <summary>Details</summary>
Motivation: 解决量子测量问题，通过纯酉动力学解释测量、态约化及量子-经典过渡，避免非酉坍缩或其他额外假设。

Method: 在投影态空间中构建随机但酉的演化框架，采用高斯酉系综的随机哈密顿量生成量子态的随机酉动力学，并通过有限探测器分辨率的等价类定义经典可观测量及经典位形/相空间子流形。

Result: 相空间子流形约束下自由薛定谔动力学退化为牛顿运动，位形空间子流形约束下随机运动产生经典布朗运动；随机动力学的跃迁概率满足玻恩规则，约束经典演化产生经典测量的高斯分布；纠缠和EPR关联从复合态空间的几何演化中涌现，同时保持时空定域性。

Conclusion: 该框架提供了与量子力学结构相容的测量与经典性统一动力学解释，证明测量、态约化和量子-经典过渡可完全从酉动力学中涌现。

Abstract: We develop a dynamical framework for quantum measurement based on stochastic but unitary evolution in projective state space. Random Hamiltonians drawn from the Gaussian Unitary Ensemble generate stochastic unitary dynamics of the quantum state, while equivalence classes reflecting finite detector resolution define classical observables as well as classical configuration-space and phase-space submanifolds. When the evolution is constrained to the phase-space submanifold, free Schrödinger dynamics reduces to Newtonian motion, while stochastic motion constrained to the classical configuration-space submanifold yields ordinary Brownian motion in classical space. Transition probabilities under the stochastic dynamics satisfy the Born rule, whereas the constrained classical evolution produces the normal probability distributions characteristic of classical measurements. We show that, in this setting, measurement, state reduction, and the quantum-classical transition emerge from unitary dynamics alone, without invoking nonunitary collapse or additional postulates. Entanglement and EPR correlations arise geometrically from the evolution of joint states in composite state space, preserving locality in spacetime. The framework provides a unified dynamical account of measurement and classicality compatible with the structure of quantum mechanics.

</details>


### [236] [Linear combination of unitaries with exponential convergence](https://arxiv.org/abs/2601.18024)
*Peter Brearley,Thomas Howarth*

Main category: quant-ph

TL;DR: 该方法通过傅里叶扩展将非酉算符分解为酉算符的线性组合，实现误差指数衰减和子归一化双对数缩放，优于现有方法的多项式缩放，为非酉量子计算提供了高效框架。


<details>
  <summary>Details</summary>
Motivation: 非酉算符在量子计算中具有重要应用，但现有方法的子归一化缩放与误差呈多项式关系，效率较低。本文旨在寻找一种误差指数衰减且子归一化缩放更优的分解方法。

Method: 基于傅里叶扩展方法，通过光滑周期延拓恒等映射，构造具有指数衰减系数的正弦级数；将该级数改写为复指数形式后，作用于非酉算符的埃尔米特部分和反埃尔米特部分，实现用酉算符线性组合逼近非酉算符。

Result: 逼近误差呈指数衰减；在量子电路中实现时，子归一化系数与逆误差呈双对数关系（log(log(1/ε))），显著优于现有方法的多项式关系；利用傅里叶扩展基的过完备性，提出了在固定误差预算下最小化子归一化的策略，并绘制了误差-子归一化帕累托前沿。

Conclusion: 傅里叶线性酉算符组合为酉量子计算提供了精确且通用的框架，在误差控制和资源效率方面具有显著优势。

Abstract: We present a general method for decomposing non-unitary operators into a linear combination of unitary operators, where the approximation error decays exponentially. The decomposition is based on a smooth periodic extension of the identity map via the Fourier extension method, resulting in a sine series with exponentially decaying coefficients. Rewriting the sine series in terms of complex exponentials, then evaluating it on the Hermitian and anti-Hermitian parts of a non-unitary operator, yields its approximation by a linear combination of unitaries. When implemented in a quantum circuit, the subnormalisation of the resulting block encoding scales with the double logarithm of the inverse error, substantially improving over the polynomial relationship in existing methods. For hardware or applications with a fixed error budget, we discuss a strategy to minimise subnormalisation by exploiting the overcomplete nature of the Fourier extension basis. This regularisation procedure traces an error-subnormalisation Pareto front, identifying coefficients that maximise the subnormalisation at a fixed error budget. Fourier linear combinations of unitaries thus provides an accurate and versatile framework for non-unitary quantum computing.

</details>


### [237] [A rigorous and complete security proof of decoy-state BB84 quantum key distribution](https://arxiv.org/abs/2601.18035)
*Devashish Tupkary,Shlok Nahar,Amir Arqand,Ernest Y. -Z. Tan,Norbert Lütkenhaus*

Main category: quant-ph

TL;DR: 本文为诱骗态BB84量子密钥分发(QKD)协议提供了严格且完整的安全性证明，并开发了一种通用模块化框架，统一了多种QKD分析技术，为协议认证和后续实现安全性分析奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有QKD安全证明在数学严谨性和完整性方面存在不足，难以满足认证和标准化的要求。本研究旨在建立高标准的安全性证明，为QKD协议的认证和标准化提供理论基础。

Method: 通过开发通用模块化框架，将经典认证、经典后处理、源替换方案、有限密钥分析、源映射、压缩映射及诱骗态技术等关键要素整合为统一的形式化体系，适用于prepare-and-measure和entanglement-based两类QKD协议。

Result: 成功完成了诱骗态BB84协议的严格安全性证明，构建了可灵活适配多种QKD变体的统一分析框架，实现了对QKD安全性的系统化和严谨化处理。

Conclusion: 该研究将分散的QKD分析技术整合为完整体系，并指明了纳入实际器件不完美性的研究方向，为未来实现安全性分析和工程化认证奠定了坚实基础。

Abstract: We present a rigorous and complete security proof of the decoy-state BB84 quantum key distribution (QKD) protocol. Our analysis aims to achieve a high standard of mathematical rigour and completeness, thereby providing the necessary foundation for certification and standardization efforts. Beyond establishing the security of a specific protocol, this work develops a general and modular framework that can be readily adapted to a broad class of QKD protocols, including both prepare-and-measure and entanglement-based variants. Our framework unifies all major ingredients required for the analysis of realistic QKD protocols, including the analysis of classical authentication and classical processing, source-replacement schemes, finite-size analysis, source maps, squashing maps, and decoy-state techniques. In doing so, this work consolidates a diverse range of techniques scattered across the QKD literature into a unified formalism, representing a general and rigorous treatment of QKD security. Finally, it outlines a clear path towards incorporating practical imperfections within the same framework, thereby laying the groundwork for addressing implementation security in future analysis.

</details>


### [238] [Overcoming Barren Plateaus in Variational Quantum Circuits using a Two-Step Least Squares Approach](https://arxiv.org/abs/2601.18060)
*Francis Boabang,Samuel Asante Gyamerah*

Main category: quant-ph

TL;DR: 针对变分量子算法中的贫瘠高原问题，提出两阶段优化框架（凸初始化+非凸细化），在BB84量子密码分析的模拟中优于随机初始化方法。


<details>
  <summary>Details</summary>
Motivation: 变分量子算法是量子计算的重要组成部分，用于解决机器学习、化学和组合优化等难题，但在扩展时会遭遇贫瘠高原现象，导致梯度消失，使得训练深度或随机初始化的电路变得几乎不可能。

Method: 提出两阶段优化框架：第一阶段为凸初始化，将量子能量景观（哈密顿景观）塑造成平滑的低能量盆地，便于梯度识别并避免噪声干扰；第二阶段为非凸细化，允许算法探索不同能量极小值以提高模型表达能力。

Result: 将该两阶段方案应用于BB84量子密钥分发协议的量子密码分析，以确定最优克隆策略。模拟结果表明，该方案优于随机初始化方法。

Conclusion: 所提出的两阶段优化框架有效解决了贫瘠高原问题，在保持稳定梯度流的同时提高了模型表达能力，相比随机初始化表现出更好的性能。

Abstract: Variational Quantum Algorithms are a vital part of quantum computing. It is a blend of quantum and classical methods for tackling tough problems in machine learning, chemistry, and combinatorial optimization. Yet as these algorithms scale up, they cannot escape the barren-plateau phenomenon. As systems grow, gradients can vanish so quickly that training deep or randomly initialized circuits becomes nearly impossible. To overcome the barren plateau problem, we introduce a two-stage optimization framework. First comes the convex initialization stage. Here, we shape the quantum energy landscape, the Hilmaton landscape, into a smooth, low-energy basin. This step makes gradients easier to spot and keeps noise from derailing the process. Once we have gotten a stable gradient flow, we move to the second stage: nonconvex refinement. In this phase, we allow the algorithm to explore different energy minima, thereby making the model more expressive. Finally, we used our two-stage solution to perform quantum cryptanalysis of the quantum key distribution protocol (i.e., BB84) to determine the optimal cloning strategies. The simulation results showed that our proposed two-stage solution outperforms its random initialization counterpart.

</details>


### [239] [Feedback-Based Quantum Control for Safe and Synergistic Drug Combination Design](https://arxiv.org/abs/2601.18082)
*Mai Nguyen Phuong Nhi,Lan Nguyen Tran,Le Bin Ho*

Main category: quant-ph

TL;DR: This paper proposes a quantum-control framework using FALQON algorithm to optimize drug combinations by encoding drug-drug interactions into Ising Hamiltonians, demonstrating efficient solutions for clinical tasks like maximum safe subsets and synergy-constrained optimization with real-world data.


<details>
  <summary>Details</summary>
Motivation: Selecting optimal multi-drug combinations that balance safety, therapeutic benefit, and regimen size is a challenging combinatorial optimization problem, despite available DDI databases.

Method: They developed a quantum-control-based framework where harmful and synergistic DDIs are encoded into Ising Hamiltonians as penalties and rewards. The optimization uses FALQON, a gradient-free variational quantum algorithm.

Result: Numerical simulations using Drugs.com and SYNERGxDB data showed efficient convergence and high-quality solutions for clinically relevant drug sets, including COVID-19 case studies.

Conclusion: The quantum-control framework provides an effective approach for DDI-aware drug combination optimization, potentially improving the safety and efficacy of multi-drug therapies.

Abstract: Drug-drug interactions (DDIs) strongly affect the safety and efficacy of combination therapies. Despite the availability of large DDI databases, selecting optimal multi-drug combinations that balance safety, therapeutic benefit, and regimen size remains a challenging combinatorial optimization problem. Here, we present a quantum-control-based framework for DDI-aware drug combination optimization, in which known harmful and synergistic interactions are encoded into Ising Hamiltonians as penalties and rewards, respectively. The optimization is performed using the feedback-based quantum algorithm FALQON, a gradient-free variational approach. We study two clinically motivated tasks: the Maximum Safe Subset problem and the Synergy-Constrained Optimization problem. Numerical simulations using interaction data from Drugs.com and SYNERGxDB demonstrate efficient convergence and high-quality solutions for clinically relevant drug sets, including COVID-19 case studies.

</details>


### [240] [Two-Polariton Blockade via Ultrastrong Light-Matter Coupling](https://arxiv.org/abs/2601.18083)
*Ting-Ting Ma,Jian Tang,Yun-Lan Zuo,Ran Huang. Adam Miranowicz,Franco Nori,Hui Jing*

Main category: quant-ph

TL;DR: 在原子-腔超强耦合体系中，通过共振单极化激元驱动首次实现了双极化激元阻塞效应，该效应表现出双粒子聚束和三粒子符合抑制的特征，为超强耦合区多粒子量子光源的实现开辟了新途径。


<details>
  <summary>Details</summary>
Motivation: 传统强耦合和弱耦合体系无法实现双极化激元阻塞，而在超强耦合区（耦合强度与腔频比>0.1）存在新的物理现象，为开发新型多粒子量子光源提供了独特机会。

Method: 运用针对超强耦合体系修正的二阶和三阶关联函数，理论预测在共振单极化激元驱动条件下双极化激元阻塞的出现。

Result: 成功预测了双极化激元阻塞现象，其特征为显著的双极化激元聚束效应和强烈抑制的三个极化激元同时符合事件。

Conclusion: 该工作揭示了超强耦合体系的独特量子统计特性，为设计基于极化激元的多粒子量子光源提供了新的理论框架和实现方案。

Abstract: We demonstrate that a two-polariton blockade (2PB) can occur under resonant single-polariton driving in an atom-cavity system operating in the ultrastrong coupling (USC) regime-a phenomenon qualitatively distinct from, and unattainable in, both the strong and weak coupling regimes. In the USC regime, where the ratio of the atom-cavity coupling strength to the cavity resonance frequency exceeds 0.1, hybrid light-matter quasiparticles known as polaritons emerge. By employing modified second- and third-order correlation functions appropriate for the USC regime, we predict the emergence of 2PB, characterized by pronounced two-polariton bunching accompanied by suppressed three-polariton coincidences. This Letter introduces a novel route to achieving 2PB, with promising implications for the realization of multiparticle quantum light sources in the USC regime.

</details>


### [241] [Quantum Recurrent Unit: A Parameter-Efficient Quantum Neural Network Component](https://arxiv.org/abs/2601.18164)
*Tzong-Daw Wu,Hsi-Sheng Goan*

Main category: quant-ph

TL;DR: This paper introduces Quantum Recurrent Unit (QRU), a quantum neural network architecture using C-SWAP gates for information selection that achieves constant circuit depth and parameter count regardless of sequence length, outperforming classical models with far fewer parameters on multiple tasks.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of modern machine learning models presents fundamental challenges in parameter efficiency and computational resource requirements, especially for NISQ devices.

Method: Introduces QRU architecture leveraging quantum C-SWAP gates to implement information selection mechanism inspired by classical GRUs, featuring measurement results feedforward state propagation and shared parameters across time steps to achieve constant circuit depth and parameter count.

Result: QRUs achieve comparable or superior performance with significantly fewer parameters: matches classical GRU with 72 vs 197 parameters on oscillatory prediction; achieves 96.13% accuracy with 35 vs 167 parameters on breast cancer classification; reaches 98.05% accuracy with 132 vs 27,265 parameters on MNIST, while maintaining constant quantum circuit depth.

Conclusion: QRU's quantum-native design combining C-SWAP-based information selection with recurrent processing demonstrates potential as a fundamental building block for next-generation ML systems, offering a promising pathway toward more efficient and scalable quantum ML architectures.

Abstract: The rapid growth of modern machine learning (ML) models presents fundamental challenges in parameter efficiency and computational resource requirements. This study introduces the Quantum Recurrent Unit (QRU), a novel quantum neural network (NN) architecture specifically designed to address these challenges while remaining compatible with Noisy Intermediate-Scale Quantum (NISQ) devices. QRU leverages quantum controlled-SWAP (C-SWAP; Fredkin) gates to implement an information selection mechanism inspired by classical Gated Recurrent Units (GRUs), enabling selective processing of temporal information via quantum operations. Through its innovative recurrent architecture featuring measurement results feedforward state propagation and shared parameters across time steps, QRU achieves constant circuit depth and constant parameter count regardless of input sequence length, effectively circumventing stringent NISQ hardware constraints. We systematically validate QRU through three progressive experiments: (1) oscillatory behavior prediction, where 72-parameter QRU matches 197-parameter classical GRU performance; (2) Wisconsin Diagnostic Breast Cancer classification, where 35 parameters achieve 96.13% accuracy comparable to 167-parameter artificial NNs; and (3) MNIST handwritten digit recognition, where 132 parameters reach 98.05% accuracy, outperforming a 27,265-parameter convolutional NN. These results demonstrate that QRU consistently achieves comparable or superior performance with significantly fewer parameters than classical NNs while maintaining constant quantum circuit depth. The architecture's quantum-native design, combining C-SWAP-based information selection with novel recurrent processing, suggests QRU's potential as a fundamental building block for next-generation ML systems, offering a promising pathway toward more efficient and scalable quantum ML architectures.

</details>


### [242] [Resource-Efficient Noise Spectroscopy for Generic Quantum Dephasing Environments](https://arxiv.org/abs/2601.18290)
*Yuan-De Jin,Zheng-Fei Ye,Wen-Long Ma*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present a resource-efficient method based on repetitive weak measurements to directly measure the noise spectrum of a generic quantum environment that causes qubit phase decoherence. The weak measurement is induced by a Ramsey interferometry measurement (RIM) on the qubit and periodically applied during the free evolution of the environment. We prove that the measurement correlation of such repetitive RIMs approximately corresponds to a direct sampling of the noise correlation function, thus enabling direct noise spectroscopy of the environment. Compared to dynamical-decoupling-based noise spectroscopy, this method can efficiently measure the full noise spectrum with the detected frequency range not limited by qubit coherence time. This method is also more resource-efficient than the correlation spectroscopy, as for the same detection accuracy with $N$ sampling times, it takes total detection time $O(N)$ while the latter one takes time $O(N^2)$. We numerically demonstrate this method for both bosonic and spin baths.

</details>


### [243] [Scalable Repeater Architecture for Long-Range Quantum Energy Teleportation in Gapped Systems](https://arxiv.org/abs/2601.18327)
*M. Y. Abd-Rabbou,Irfan Siddique,Saeed Haddadi,Cong-Feng Qiao*

Main category: quant-ph

TL;DR: 提出分层量子中继架构解决量子能量传输（QET）的可扩展性危机，将资源消耗从指数级降至多项式级，实现任意距离的真空能量激活


<details>
  <summary>Details</summary>
Motivation: 现有QET协议受限于有能隙多体系统的局域性，基态关联的指数聚类特性导致能量提取仅限微观尺度，亟需突破距离限制以实现远程量子控制

Method: 在1D各向异性XY模型框架下，先分析单体测量策略的缺陷，再设计基于 heralded 纠缠生成、迭代纯化和嵌套交换的分层量子中继架构

Result: 新架构将操作资源消耗从指数级优化至多项式级，首次证明任意距离真空能量激活的物理可行性与计算可处理性

Conclusion: QET虽无净能量增益，但为远程量子操控与资源分配提供了可行协议，推动了量子能量传输的实际应用

Abstract: Quantum Energy Teleportation (QET) constitutes a paradigm-shifting protocol that permits the activation of local vacuum energy through the consumption of pre-existing entanglement and classical communication. Nevertheless, the implementation of QET is severely impeded by the fundamental locality of gapped many-body systems, where the exponential clustering of ground-state correlations restricts energy extraction to microscopic scales. In this work, we address this scalability crisis within the framework of the one-dimensional anisotropic XY model. We initially provide a rigorous characterization of a monolithic measurement-induced strategy, demonstrating that while bulk projective measurements can theoretically induce long-range couplings, the approach is rendered physically untenable by exponentially diverging thermodynamic costs and vanishing success probabilities. To circumvent this impasse, we propose and analyze a hierarchical quantum repeater architecture adapted for energy teleportation. By orchestrating heralded entanglement generation, iterative entanglement purification, and nested entanglement swapping, our protocol effectively counteracts the fidelity degradation inherent in noisy quantum channels. We establish that this architecture fundamentally alters the operational resource scaling from exponential to polynomial. This proves, for the first time, the physical permissibility and computational tractability of activating vacuum energy at arbitrary distances. The significance lies not in net energy gain, but in establishing long-range QET as a viable protocol for remote quantum control and resource distribution.

</details>


### [244] [Scaling of multicopy constructive interference of Gaussian states](https://arxiv.org/abs/2601.18347)
*Matthieu Arnhem,Radim Filip*

Main category: quant-ph

TL;DR: 研究分析了携带位移信息的弱压缩波动非经典高斯态在多模干涉中的建设性干涉缩放规律，提出增益-不稳定性比新指标量化资源不稳定性，为量子技术扩展提供理论和实验基础。


<details>
  <summary>Details</summary>
Motivation: 量子技术进步依赖量子资源的扩展，但非相同、脆弱量子资源的扩展机制尚不明确，亟需理论和实验研究。

Method: 分析预测并比较不同多模干涉架构中复用非经典高斯态的建设性干涉缩放律，使用信噪比量化位移增益，提出增益-不稳定性比数值评估资源不稳定性影响。

Result: 获得了多模干涉中量子资源的关键缩放规律，提出的增益-不稳定性比为大规模干涉方案中未探索的资源不稳定性效应提供了数值估计工具。

Conclusion: 该工作为玻色子量子资源的扩展奠定了理论基础，开启了其他玻色子资源的理论研究和实验验证的可能，对量子技术平台发展具有重要意义。

Abstract: Quantum technology advances crucially depend on the scaling up of essential quantum resources. Their ideal multiplexing offers more significant gains in applications; however, the scaling of the nonidentical, fragile and varying resources is neither theoretically nor experimentally known. For bosonic systems, multimode interference is an essential tool already widely exploited to develop quantum technology. Here, we analyze, predict and compare essential scaling laws for a constructive interference of multiplexed nonclassical Gaussian states carrying information by displacement with weakly fluctuating squeezing in different multimode interference architectures. The signal-to-noise ratio quantifies the increase in displacement relative to the noise. We introduce the gain-to-instability ratio to numerically estimate the effect of unexplored resource instabilities in a large scale interference scheme. The use of the gain-to-instability ratio to quantify the scaling laws opens steps for extensive theoretical investigation of other bosonic resources and follow-up feasible experimental verification necessary for further development of these platforms.

</details>


### [245] [An Adaptive Purification Controller for Quantum Networks: Dynamic Protocol Selection and Multipartite Distillation](https://arxiv.org/abs/2601.18351)
*Pranav Kulkarni,Leo Sünkel,Michael Kölle*

Main category: quant-ph

TL;DR: 针对量子互联网中纠缠分发效率问题，提出自适应纯化控制器(APC)，通过动态优化蒸馏协议和纯化深度，最大化好吞吐量(goodput)，消除保真度悬崖并避免高噪声下的资源浪费。


<details>
  <summary>Details</summary>
Motivation: 高效纠缠分发是量子互联网的基石，但物理链路参数（光子损耗、存储器相干时间、门误差率）动态波动，导致静态纯化策略次优，需在动态环境中最大化满足严格保真度阈值的交付速率。

Method: 提出自适应纯化控制器(APC)，将协议选择视为资源分配问题，动态切换纯化深度和协议族（BBPSSW vs DEJMPS），采用动态规划规划器与帕累托剪枝策略，并扩展至异质场景（多体GHZ态和连续变量系统）。

Result: 仿真表明APC消除了静态协议的"保真度悬崖"，防止高噪声机制下的资源浪费；在异质场景中表现稳健；决策延迟在毫秒级，具备实时可行性。

Conclusion: APC通过动态自适应优化显著提升量子纠缠分发的效率和鲁棒性，为未来量子互联网的实用化部署提供了可行的技术方案。

Abstract: Efficient entanglement distribution is the cornerstone of the Quantum Internet. However, physical link parameters such as photon loss, memory coherence time, and gate error rates fluctuate dynamically, rendering static purification strategies suboptimal. In this paper, we propose an Adaptive Purification Controller (APC) that autonomously optimizes the entanglement distillation sequence to maximize the "goodput," the rate of delivered pairs meeting a strict fidelity threshold. By treating protocol selection as a resource allocation problem, the APC dynamically switches between purification depths and protocol families (e.g., BBPSSW vs. DEJMPS) to navigate the trade-off between generation rate and state quality. Using a dynamic programming planner with Pareto pruning, simulation results demonstrate that our approach eliminates the "fidelity cliffs" inherent in static protocols and prevents resource wastage in high-noise regimes. Furthermore, we extend the controller to heterogeneous scenarios, demonstrating robustness for both multipartite GHZ state generation and continuous variable systems using effective noiseless linear amplification models. We benchmark its computational overhead, confirming real-time feasibility with decision latencies in the millisecond range per link.

</details>


### [246] [Bohr's complementarity principle tested on a real quantum computer via interferometer experiments](https://arxiv.org/abs/2601.18366)
*Celia Álvarez Álvarez,Mariamo Mussa Juane*

Main category: quant-ph

TL;DR: This paper experimentally verifies an updated quantum complementarity principle through qubit-based interferometric experiments and quantum state tomography.


<details>
  <summary>Details</summary>
Motivation: To address limitations in Bohr's original Complementarity Principle by developing and testing an updated framework for quantifying wave-particle duality in quantum systems.

Method: Implementing two interferometric experiments in 1-qubit and 2-qubit quantum circuits on real hardware, reconstructing final state density matrices via quantum state tomography, and computing the updated complementarity relation.

Result: The updated complementarity relation was validated through direct computation, with results presented graphically and analyzed using mean squared error (MSE) to quantify agreement with theoretical predictions.

Conclusion: The experimental results confirm the validity of the proposed updated complementarity relation for quantum systems, demonstrating its applicability in real quantum hardware despite implementation constraints.

Abstract: Bohr's Complementarity Principle is a core concept of quantum mechanics. In this article, an updated complementarity relation for the wave and ondulatory aspects of a quantum system is presented and discussed. Two interferometric experiments are implemented in one and two qubit circuits and executed on real hardware. The final state density matrices are reconstructed using quantum state tomography and the complementarity relation is tested via direct computation. Results of the executions are presented both graphically and with a mean squared error analysis for a better comprehension.

</details>


### [247] [Quantum Error Correction on Error-mitigated Physical Qubits](https://arxiv.org/abs/2601.18384)
*Minjun Jeon,Zhenyu Cai*

Main category: quant-ph

TL;DR: A framework applies quantum error mitigation (PEC, ZNE, symmetry verification) directly to physical qubits within logical qubits, removing leading-order logical errors to increase effective code distance by 2 and reduce qubit overhead by 40-64% while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To develop resource-efficient strategies for suppressing logical errors in early fault-tolerant quantum computing by leveraging quantum error mitigation at the physical layer without requiring modifications to QEC decoders.

Method: Exploits linearity of quantum error correction to integrate linear QEM techniques (PEC, ZNE, symmetry verification) directly into the physical qubit layer. Validated through analytical proofs and numerical simulations of PEC on memory experiments and surface code implementations.

Result: Removes leading-order logical error contribution, increasing effective code distance by 2. Distance-3 codes with physical-level PEC achieve logical error rates comparable to unmitigated distance-5 codes while using 40% fewer qubits (repetition codes) and 64% fewer qubits (rotated surface codes).

Conclusion: Physical-level QEM is a widely compatible and resource-efficient strategy that enhances logical qubit performance in early fault-tolerant architectures by reducing qubit overhead and suppressing logical errors.

Abstract: We present a general framework for applying linear quantum error mitigation (QEM) techniques directly to physical qubits within a logical qubit to suppress logical errors. By exploiting the linearity of quantum error correction (QEC), we demonstrate that any linear QEM method$\unicode{x2014}$including probabilistic error cancellation (PEC), zero-noise extrapolation (ZNE), and symmetry verification$\unicode{x2014}$can be integrated into the physical layer without requiring modifications to the subsequent QEC decoder. Applying this framework to memory experiments using PEC, we analytically prove and numerically verify that the leading-order contribution to the logical error can be removed, increasing the effective code distance by 2. Our simulations on repetition and rotated surface codes show that a distance-3 code with physical-level PEC achieves logical error rates lower than or similar to a distance-5 unmitigated code while using 40% and 64% fewer qubits, respectively. These results establish physical-level QEM as a widely compatible and resource-efficient strategy for enhancing logical performance in early fault-tolerant architectures.

</details>


### [248] [Emergent Cooperation in Quantum Multi-Agent Reinforcement Learning Using Communication](https://arxiv.org/abs/2601.18419)
*Michael Kölle,Christian Reff,Leo Sünkel,Julian Hager,Gerhard Stenzel,Claudia Linnhoff-Popien*

Main category: quant-ph

TL;DR: This paper extends classical MARL cooperation protocols to quantum settings, showing that communication mechanisms like MATE and MEDIATE enable high cooperation in quantum agents across three social dilemmas.


<details>
  <summary>Details</summary>
Motivation: Research on extending classical reinforcement learning cooperation methods to Quantum Multi-Agent Reinforcement Learning remains limited, particularly through communication protocols.

Method: Applied quantum Q-Learning agents with communication protocols (MATE, MEDIATE, Gifting, RIAL) and evaluated them in three Sequential Social Dilemmas: Iterated Prisoner's Dilemma, Iterated Stag Hunt, and Iterated Game of Chicken.

Result: Approaches using MATE_TD, AutoMATE, MEDIATE-I, and MEDIATE-S achieved high cooperation levels across all three dilemmas.

Conclusion: Communication is a viable mechanism for fostering emergent cooperation in Quantum Multi-Agent Reinforcement Learning.

Abstract: Emergent cooperation in classical Multi-Agent Reinforcement Learning has gained significant attention, particularly in the context of Sequential Social Dilemmas (SSDs). While classical reinforcement learning approaches have demonstrated capability for emergent cooperation, research on extending these methods to Quantum Multi-Agent Reinforcement Learning remains limited, particularly through communication. In this paper, we apply communication approaches to quantum Q-Learning agents: the Mutual Acknowledgment Token Exchange (MATE) protocol, its extension Mutually Endorsed Distributed Incentive Acknowledgment Token Exchange (MEDIATE), the peer rewarding mechanism Gifting, and Reinforced Inter-Agent Learning (RIAL). We evaluate these approaches in three SSDs: the Iterated Prisoner's Dilemma, Iterated Stag Hunt, and Iterated Game of Chicken. Our experimental results show that approaches using MATE with temporal-difference measure (MATE\textsubscript{TD}), AutoMATE, MEDIATE-I, and MEDIATE-S achieved high cooperation levels across all dilemmas, demonstrating that communication is a viable mechanism for fostering emergent cooperation in Quantum Multi-Agent Reinforcement Learning.

</details>


### [249] [A Theory of Single-Antenna Atomic Beamforming](https://arxiv.org/abs/2601.18426)
*Mingyao Cui,Qunsong Zeng,Kaibin Huang*

Main category: quant-ph

TL;DR: 针对里德堡原子接收器(RARE)空间响应建模不足的问题，本文理论分析了LO驱动RARE的空间特性，发现增加气室长度可形成窄波束，并提出了分段气室结构以克服激光衰减限制，实现单天线波束成形。


<details>
  <summary>Details</summary>
Motivation: 现有研究将RARE简化为各向同性点接收器，忽略了气室内量子态的空间变化，导致接收模式不准确。

Method: 建立标准LO驱动RARE的空间响应理论模型，分析气室长度对波束特性的影响，并设计分段气室架构。

Result: 增加气室长度可产生与LO场对齐的接收波束，其宽度与长度成反比；但面临激光指数衰减的制约；分段气室可在固定总长下增加有效长度，获得更窄波束和更高增益。

Conclusion: 该研究实现了单天线原子波束成形，分段气室设计有效突破了传播损耗限制，为提升RARE性能提供了新方案。

Abstract: Leveraging the quantum advantages of highly excited atoms, Rydberg atomic receivers (RAREs) represent a paradigm shift in radio wave detection, offering high sensitivity and broadband reception. However, existing studies largely model RAREs as isotropic point receivers and overlook the spatial variations of atomic quantum states within vapor cells, thus inaccurately characterizing their reception patterns. To address this issue, we present a theoretical analysis of the aforementioned spatial responses of a standard local-oscillator (LO)- dressed RARE. Our results reveal that increasing the vapor-cell length produces a receive beam aligned with the LO field, with a beamwidth inversely proportional to the cell length. This finding enables atomic beamforming to enhance received signal-to-noise ratio using only a single-antenna RARE. Furthermore, we derive the achievable beamforming gain by characterizing and balancing the fundamental tradeoff between the effects of increasing the vapor cell length and the exponential power decay of laser propagating through the cell. To overcome the limitation imposed by exponential decay, we propose a novel RARE architecture termed segmental vapor cell. This architecture consists of vapor-cell segments separated by clear-air gaps, allowing the total cell length (and hence propagation loss) to remain fixed while the effective cell length increases. As a result, this segmented design expands the effective atom-field interaction area without increasing the total vapor cell length, yielding a narrower beamwidth and thus higher beamforming gain as compared with a traditional continuous vapor cell.

</details>


### [250] [Physics-Informed Hybrid Quantum-Classical Dispatching for Large-Scale Renewable Power Systems:A Noise-Resilient Framework](https://arxiv.org/abs/2601.18482)
*Fu Zhang,Yuming Zhao*

Main category: quant-ph

TL;DR: 提出物理信息混合量子-经典调度框架PI-HQCD，通过拓扑感知哈密顿量嵌入电网物理约束，显著缓解 barren plateaus 问题，并在经济效率和可再生能源利用率上超越经典SDDP方法


<details>
  <summary>Details</summary>
Motivation: 高比例可再生能源并网给电力系统调度带来随机性和非凸性挑战，经典优化方法计算受限；现有量子算法将电网视为黑箱，存在可扩展性差（ barren plateaus）和物理约束频繁违反的问题

Method: 构建拓扑感知哈密顿量，将线性化潮流方程、储能动态和多时间尺度耦合直接嵌入量子基底；推导噪声自适应正则化机制，理论上约束目标函数的Lipschitz常数，保证量子测量噪声下的收敛稳定性

Result: 在IEEE 39节点和118节点系统上的数值实验表明，PI-HQCD在经济性和可再生能源利用率上优于随机对偶动态规划（SDDP）；理论分析证实拓扑感知设计实现O(1/N)梯度方差缩放，有效缓解 barren plateaus

Conclusion: 建立了将工程物理嵌入量子计算的严格范式，为下一代电网运行中的实用量子优势铺平道路

Abstract: The integration of high-penetration renewable energy introduces significant stochasticity and non-convexity into power system dispatching, challenging the computational limits of classical optimization. While Variational Quantum Algorithms (VQAs) on Noisy Intermediate-Scale Quantum (NISQ) devices offer a promising path for combinatorial acceleration, existing approaches typically treat the power grid as a "black box", suffering from poor scalability (barren plateaus) and frequent violations of physical constraints. Bridging these gaps, this paper proposes a Physics-Informed Hybrid Quantum-Classical Dispatching (PI-HQCD) framework. We construct a topology-aware Hamiltonian that explicitly embeds linearized power flow equations, storage dynamics, and multi-timescale coupling directly into the quantum substrate, significantly reducing the search space dimensionality. We further derive a noise-adaptive regularization mechanism that theoretically bounds the effective Lipschitz constant of the objective function, guaranteeing convergence stability under realistic quantum measurement noise. Numerical experiments on the IEEE 39-bus benchmark and a 118-bus regional grid demonstrate that PI-HQCD achieves superior economic efficiency and higher renewable utilization compared to stochastic dual dynamic programming (SDDP). Theoretical analysis confirms that this topology-aware design leads to an O(1/N) gradient variance scaling, effectively mitigating barren plateaus and ensuring scalability for larger networks. This work establishes a rigorous paradigm for embedding engineering physics into quantum computing, paving the way for practical quantum advantage in next-generation grid operations.

</details>


### [251] [Qubit-parity interference despite unknown interaction phases](https://arxiv.org/abs/2601.18499)
*Kratveer Singh,Kimin Park,Vojtěch Švarc,Artem Kovalenko,Tuan Pham,Ondřej Číp,Lukáš Slodička,Radim Filip*

Main category: quant-ph

TL;DR: 该论文通过在囚禁离子系统中实现一种对未知但稳定激光相位具有鲁棒性的量子干涉，利用极简双脉冲序列制备类似薛定谔猫态的状态，实现了高达40%的干涉可见度，为高维态提供了一种可扩展的相干性见证方法。


<details>
  <summary>Details</summary>
Motivation: 量子干涉通常需要精确控制相互作用相位，这在复杂系统中具有挑战性。该研究的动机是探索在稳定但未知相位条件下是否仍能观察到干涉现象，这对基础科学和量子技术具有重要意义。

Method: 研究人员利用囚禁$^{40}$Ca$^{+}$离子，通过交替施加红失谐和蓝失谐边带脉冲，强制实现量子比特宇称关联，产生对未知激光相位不敏感的干涉。他们采用极简的双脉冲干涉序列。

Result: 实验实现了20%和40%的特征干涉可见度，接近该量子比特宇称干涉方案的理论可见度极限。

Conclusion: 该工作为高维态提供了一种无需完全层析的可扩展相干性见证方法，展示了在相位控制受限的复杂量子系统中实现鲁棒量子干涉的可行性。

Abstract: Quantum interference between interacting systems is fundamental to basic science and quantum technology, but it typically requires precise control of the interaction phases of lasers or microwave generators. Can interference be observed if those interaction phases are stable but unknown, usually prohibitive for complex state without active control? Here, we answer this question by experimentally preparing a Schrödinger-cat-like state of an internal qubit and a motional oscillator of a trapped $^{40}$Ca$^{+}$ ion, and its robustness to such uncontrolled phase. By applying alternating red and blue sideband pulses, we enforce a strict qubit-parity correlation and interference inherently insensitive to stable but unknown phases of the driving laser. For this qubit-parity interference, we use a minimal two-pulse interferometric sequence to demonstrate characteristic visibilities of $20\%$ and $40\%$, which approach the theoretical visibility limit, providing a scalable coherence witness without full state tomography for high-dimensional states.

</details>


### [252] [Simultaneous determination of multiple low-lying energy levels on a superconducting quantum processor](https://arxiv.org/abs/2601.18514)
*Huili Zhang,Yibin Guo,Guanglei Xu,Yulong Feng,Jingning Zhang,Hai-feng Yu,S. P. Zhao*

Main category: quant-ph

TL;DR: 在超导量子云平台上实验实现AEVQE算法，成功求解H₂分子和横场Ising模型的低激发态，获得势能曲线并观察到相变迹象，验证了该算法的实验可行性。


<details>
  <summary>Details</summary>
Motivation: 确定基态和低激发态在多种场景中至关重要，但传统方法难以同时高效获取多个低能级，且需在真实量子硬件上验证算法可行性。

Method: 利用超导量子云平台实现AEVQE算法，通过辅助比特与物理比特纠缠同时靶向多个低激发态，在H₂分子和横场Ising模型上进行实验验证，并与无辅助比特VQE算法进行对比分析。

Result: 成功获得H₂分子的势能曲线；通过平均绝对磁化强度观察到横场Ising模型中铁磁-顺磁相变的迹象；系统研究了影响算法性能的多个因素；获得了与无辅助比特VQE的对比数据。

Conclusion: 本工作证明了AEVQE算法在真实量子硬件上的实验可行性，为在公开量子平台上使用VQE方法解决实际问题提供了重要指导。

Abstract: Determining the ground and low-lying excited states is critical in numerous scenarios. Recent work has proposed the ancilla-entangled variational quantum eigensolver (AEVQE) that utilizes entanglement between ancilla and physical qubits to simultaneously tagert multiple low-lying energy levels. In this work, we report the experimental implementation of the AEVQE on a superconducting quantum cloud platform, demonstrating the full procedure of solving the low-lying energy levels of the H$_2$ molecule and the transverse-field Ising models (TFIMs). We obtain the potential energy curves of H$_2$ and show an indication of the ferromagnetic to paramagnetic phase transition in the TFIMs from the average absolute magnetization. Moreover, we investigate multiple factors that affect the algorithmic performance and provide a comparison with ancilla-free VQE algorithms. Our work demonstrates the experimental feasibility of the AEVQE algorithm and offers a guidance for the VQE approach in solving realistic problems on publicly-accessible quantum platforms.

</details>


### [253] [Quantum Key Distribution with a Negatively Charged Quantum Dot Single-Photon Source](https://arxiv.org/abs/2601.18518)
*Parvendra Kumar*

Main category: quant-ph

TL;DR: ARP激发改善了量子点单光子源质量，在短/中距离提升QKD密钥率，但远距离时泊松光源表现更优。


<details>
  <summary>Details</summary>
Motivation: QKD协议需要高亮度、低多光子发射概率的单光子源以保证安全性。

Method: 对比研究了嵌入椭圆柱微腔的负电荷量子点在共振激发和绝热快速通道(ARP)激发下的单光子产生；评估了BB84和双场QKD的密钥率，并与泊松分布光源进行比较。

Result: ARP显著抑制多光子发射概率并提高光子不可区分性；量子点光源在短/中距离优于泊松光源，但在远距离被反超。

Conclusion: ARP激发方式更优，使量子点光源在中距离QKD中具有实用价值，但远距离应用仍需泊松光源。

Abstract: Various quantum key distribution protocols require bright single-photon sources with a very low probability of multiphoton emission. In this work, we investigate single-photon generation from a negatively charged quantum dot embedded in an elliptical pillar microcavity, driven using either resonant excitation or adiabatic rapid passage (ARP). Our results show that ARP excitation significantly suppresses multiphoton emission probability and improves photon indistinguishability compared to resonant excitation. We further evaluate the secure key rate of both BB84 and twin-field quantum key distribution (TF-QKD) using quantum-dot single-photon sources and compare their performance with that of Poisson-distributed photon sources (PDS) such as weak coherent pulses and down-conversion sources. The analysis reveals that adiabatic excitation offers a modest but consistent enhancement in secure key rate relative to resonant excitation. Moreover, quantum-dot single-photon sources outperform PDS sources over short and intermediate distances; however, at longer distances, PDS sources eventually surpass quantum-dot sources in both infinite decoy-state BB84 and TF-QKD.

</details>


### [254] [Certifying optimal device-independent quantum randomness in quantum networks](https://arxiv.org/abs/2601.18534)
*Shuai Zhao,Rong Wang,Qi Zhao*

Main category: quant-ph

TL;DR: This paper presents a new family of multipartite Bell inequalities based on GHZ state stabilizers to achieve more efficient device-independent quantum randomness certification, especially for non-maximal Bell values and multi-party scenarios, outperforming existing methods like Mermin-type inequalities.


<details>
  <summary>Details</summary>
Motivation: Current device-independent (DI) quantum randomness certification methods suffer from low efficiency when dealing with non-maximal Bell values and multiple parties, limiting practical applications in quantum networks.

Method: Developed a family of multipartite Bell inequalities inspired by the stabilizer group of GHZ states, featuring simple structure and easy scalability to more parties. Also derived a general analytical upper bound for Holevo quantity.

Result: The new inequalities enable optimal quantum randomness certification and GHZ state self-testing. They significantly outperform MABK, Parity-CHSH, and Holz inequalities at N=3 parties, particularly for non-maximal Bell values, with better Holevo quantity bounds.

Conclusion: This approach provides a more efficient and practical framework for DI randomness certification in quantum networks, with strong experimental relevance for quantum cryptography applications.

Abstract: Bell nonlocality provides a device-independent (DI) way to certify quantum randomness, based on which true random numbers can be extracted from the observed correlations without detail characterizations on devices for quantum state preparation and measurement. However, the efficiency of current strategies for DI randomness certification is still heavily constrained when it comes to non-maximal Bell values, especially for multiple parties. Here, we present a family of multipartite Bell inequalities that allows to certify optimal quantum randomness and self-test GHZ (Greenberger-Horne-Zeilinger) states, which are inspired from the stabilizer group of the GHZ state. Due to the simple representation of stabilizer group for GHZ states, this family of Bell inequalities is of simple structure and can be easily expanded to more parties. Compared with the Mermin-type inequalities, this family of Bell inequality is more efficient in certifying quantum randomness when non-maximal Bell values achieved. Meanwhile, the general analytical upper bound for the Holevo quantity is presented, and achieves better performance compared with the MABK (Mermin-Ardehali-Belinskii-Klyshko) inequality, Parity-CHSH (Clauser-Horne-Shimony-Holt) inequality and Holz inequality at $N=3$, which is of particular interests for experimental researches on DI quantum cryptography in quantum networks.

</details>


### [255] [Sufficient conditions for additivity of the zero-error classical capacity of quantum channels](https://arxiv.org/abs/2601.18538)
*Jeonghoon Park,Jeong San Kim*

Main category: quant-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The one-shot zero-error classical capacity of a quantum channel is the amount of classical information that can be transmitted with zero probability of error by a single use. Then the one-shot zero-error classical capacity equals to the logarithmic value of the independence number of the noncommutative graph induced by the channel. Thus the additivity of the one-shot zero-error classical capacity of a quantum channel is equivalent to the multiplicativity of the independence number of the noncommutative graph. The independence number is not multiplicative in general, and it is not clearly understood when the multiplicativity occurs. In this work, we present sufficient conditions for multiplicativity of the independence number, and we give explicit examples of quantum channels. Furthermore, we consider a block form of noncommutative graphs, and provide conditions when the independence number is multiplicative.

</details>


### [256] [Bayesian Optimization for Quantum Error-Correcting Code Discovery](https://arxiv.org/abs/2601.18562)
*Yihua Chengyu,Richard Meister,Conor Carty,Sheng-Ku Lin,Roberto Bondesan*

Main category: quant-ph

TL;DR: 提出贝叶斯优化框架结合多视角链复形神经嵌入，高效搜索量子纠错码，发现两种新码：[[144,36]]高码率码和[[144,16]]低错误率码，后者性能优于Gross码，展示了自动设计量子码的潜力。


<details>
  <summary>Details</summary>
Motivation: 量子纠错码保护量子信息至关重要，但寻找性能良好且开销最小的实际可用码困难，因为组合搜索空间巨大且逻辑错误率评估成本高昂。

Method: 提出贝叶斯优化框架，核心贡献是多视角链复形神经嵌入方法，无需昂贵模拟即可预测量子LDPC码的逻辑错误率。

Result: 在双变量自行车码测试中，发现[[144,36]]高码率码单位量子比特错误率与Gross码相当，以及[[144,16]]低错误率码单位量子比特错误率优于Gross码。

Conclusion: 该流程能自动发现平衡码率和噪声抑制的量子码，且框架可推广应用于不同码族、解码器和噪声模型。

Abstract: Quantum error-correcting codes protect fragile quantum information by encoding it redundantly, but identifying codes that perform well in practice with minimal overhead remains difficult due to the combinatorial search space and the high cost of logical error rate evaluation. We propose a Bayesian optimization framework to discover quantum error-correcting codes that improves data efficiency and scalability with respect to previous machine learning approaches to this task. Our main contribution is a multi-view chain-complex neural embedding that allows us to predict the logical error rate of quantum LDPC codes without performing expensive simulations. Using bivariate bicycle codes and code capacity noise as a testbed, our algorithm discovers a high-rate code [[144,36]] that achieves competitive per-qubit error rate compared to the gross code, as well as a low-error code [[144,16]] that outperforms the gross code in terms of error rate per qubit. These results highlight the ability of our pipeline to automatically discover codes balancing rate and noise suppression, while the generality of the framework enables application across diverse code families, decoders, and noise models.

</details>


### [257] [Universality of Many-body Projected Ensemble for Learning Quantum Data Distribution](https://arxiv.org/abs/2601.18637)
*Quoc Hoan Tran,Koki Chinzei,Yasuhiro Endo,Hirotaka Oshima*

Main category: quant-ph

TL;DR: MPE框架被证明能通用近似任意纯态量子分布，增量变体提升可训练性并在量子数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: 解决量子机器学习中参数化模型是否能近似任意量子分布的基本问题，以及生成量子数据的理论和实践挑战。

Method: 证明Many-body Projected Ensemble (MPE)框架的通用性定理，并提出增量MPE变体进行分层训练以提高可训练性。

Result: MPE能在1-Wasserstein距离误差内近似任何纯态分布，数值实验在团簇量子态和量子化学数据集上验证了其学习复杂量子数据分布的有效性。

Conclusion: MPE提供了通用表达性的严格保证，解决了量子机器学习的关键理论空白，增量变体改善了实际训练效果。

Abstract: Generating quantum data by learning the underlying quantum distribution poses challenges in both theoretical and practical scenarios, yet it is a critical task for understanding quantum systems. A fundamental question in quantum machine learning (QML) is the universality of approximation: whether a parameterized QML model can approximate any quantum distribution. We address this question by proving a universality theorem for the Many-body Projected Ensemble (MPE) framework, a method for quantum state design that uses a single many-body wave function to prepare random states. This demonstrates that MPE can approximate any distribution of pure states within a 1-Wasserstein distance error. This theorem provides a rigorous guarantee of universal expressivity, addressing key theoretical gaps in QML. For practicality, we propose an Incremental MPE variant with layer-wise training to improve the trainability. Numerical experiments on clustered quantum states and quantum chemistry datasets validate MPE's efficacy in learning complex quantum data distributions.

</details>


### [258] [Error-mitigation aware benchmarking strategy for quantum optimization problems](https://arxiv.org/abs/2601.18680)
*Marine Demarty,Bo Yang,Kenza Hammam,Pauline Besserve*

Main category: quant-ph

TL;DR: 该论文开发了一个新型基准测试框架，将有限采样效应和量子错误缓解（QEM）的资源开销纳入量子优势评估，通过量化能量估计值落在经典边界区间内的置信度来判断量子优势，并在8×8费米-哈伯德模型中验证了框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有熵基准测试方法无法评估有限采样效应和量子错误缓解策略对量子优势的影响，而QEM作为近期关键误差抑制技术会显著增加采样开销，亟需一个综合考虑这些实际因素的量子优势评估框架。

Method: 提出基于置信区间的量子优势量化框架：通过计算估计能量值落在最佳经典上下界区间内的统计置信度，同时显式纳入有限采样统计误差和QEM引入的采样开销，实现硬件噪声与应用特性的协同评估。

Result: 在8×8费米-哈伯德模型的数值研究中，该框架成功识别出特定噪声和采样预算条件下，概率性错误消除（PEC）等QEM方法能保持量子优势且不受有限采样效应显著影响的操作窗口。

Conclusion: 该框架为终端用户提供了轻量级数值工具，可依据实际采样预算评估近期量子硬件在优化任务中的实用量子优势潜力，指导量子优势的实验实现策略选择。

Abstract: Assessing whether a noisy quantum device can potentially exhibit quantum advantage is essential for selecting practical quantum utility tasks that are not efficiently verifiable by classical means. For optimization, a prominent candidate for quantum advantage, entropy benchmarking provides insights based concomitantly on the specifics of the application and its implementation, as well as hardware noise. However, such an approach still does not account for finite-shot effects or for quantum error mitigation (QEM), a key near-term error suppression strategy that reduces estimation bias at the cost of increased sampling overhead. We address this limitation by developing a benchmarking framework that explicitly incorporates finite-shot statistics and the resource overhead induced by QEM. Our framework quantifies quantum advantage through the confidence that an estimated energy lies within an interval defined by the best-known classical upper and lower bounds. Using a proof-of-principle numerical study of the two-dimensional Fermi-Hubbard model at size $8\times8$, we demonstrate that the framework effectively identifies noise and shot-budget regimes in which the probabilistic error cancellation (PEC), a representative QEM method, is operationally advantageous, and potential quantum advantage is not hindered by finite-shot effects. Overall, our approach equips end-users with a framework based on lightweight numerics for assessing potential practical quantum advantage in optimization on near-future quantum hardware, in light of the allocated shot budget.

</details>


### [259] [On the Stochastic-Quantum Correspondence](https://arxiv.org/abs/2601.18720)
*Sami Calvo*

Main category: quant-ph

TL;DR: 本文在Barandes 2023年随机-量子对应基础上，利用狄拉克符号澄清结果，从单一公理（物理系统遵循一般不可分的随机演化）推导出标准量子力学的六个公理。推广至连续基揭示了空间可能离散的问题，给出经典/量子场推广实例。讨论了实用计算问题，阐明了多粒子系统经典极限符合牛顿第二定律，并基于自由度数量和低熵特性区分环境与测量装置，解释了波函数坍缩机制。


<details>
  <summary>Details</summary>
Motivation: 为量子力学建立更基础的随机理论框架，从单一公理推导标准体系，以解决测量问题、解释经典极限，并探讨时空离散性。

Method: 采用理论物理方法，运用狄拉克符号和随机过程理论，通过数学推导和概念分析，将离散系统的随机-量子对应推广至连续基和场论体系。

Result: 从单一随机公理推导出量子力学的六个标准公理；连续基处理中揭示出暗示空间离散性的问题；给出经典和量子场论推广的具体实例；阐明多粒子系统经典极限趋向牛顿第二定律；提出以自由度数量和低熵特性区分环境与测量装置，并解释波函数坍缩

Conclusion: 随机方法为量子力学提供了更基础的理论基础，能够解释经典极限和测量问题，暗示时空可能离散；但实际计算仍最宜采用传统方法。

Abstract: This paper aims to first explain, somewhat more clearly, the Stochastic-Quantum correspondence put forward in by Barandes in 2023. Specifically, the quantum-mechanical bra-ket notation is used, illuminating some results of previous results. With this, we prove the six axioms of textbook quantum mechanics from a single axiom: every physical system evolves according to a, generally indivisible, stochastic law. Afterwards, we generalise the treatment to continuous bases, which showcases a problem with them, indicating that space (and other physical variables) may be discrete in nature. Some concrete examples are also given, including the generalisation to classical and quantum fields. Then, we treat some practical issues of this new stochastic approach, regarding the solving of problems in physics, which turns out to still be most tractable in the traditional way. Finally, we explain the classical limit, where a system of many particles is found to behave classically according to Newton's second law. Along with that, we present a way of solving the measurement problem, characterising what is an environment and a measuring device and explaining how the wavefunction collapse comes about. Specifically, it is found that what distinguishes an environment is its number of degrees of freedom, while a measuring device is a low-entropy type of environment.

</details>


### [260] [Approximate level-by-level maximum-likelihood decoding based on the Chase algorithm for high-rate concatenated stabilizer codes](https://arxiv.org/abs/2601.18743)
*Takeshi Kakizaki*

Main category: quant-ph

TL;DR: 提出一种新的高率级联量子纠错码解码器，通过Chase算法扩展了逐级最小距离解码器(LMDD)，仿真显示其在比特翻转噪声下对高率级联Hamming码的性能优于传统解码器。


<details>
  <summary>Details</summary>
Motivation: 为实现大规模容错量子计算，需要低开销的量子纠错码，高率级联码虽有前景但需改进解码性能。

Method: 提出一种通用的高性能解码器，通过利用Chase算法生成候选错误集，扩展了现有的逐级最小距离解码器(LMDD)。

Result: 仿真结果表明，在比特翻转噪声模型下，该解码器对高率级联Hamming码的解码性能优于传统解码器。

Conclusion: 所提出的Chase增强型LMDD解码器提升了高率级联码的实用性，有助于推动大规模容错量子计算的发展。

Abstract: Fault-tolerant quantum computation (FTQC) is expected to address a wide range of computational problems. To realize large-scale FTQC, it is essential to encode logical qubits using quantum error-correcting codes. High-rate concatenated codes have recently attracted attention due to theoretical advances in fault-tolerant protocols with constant-space-overhead and polylogarithmic-time-overhead, as well as practical developments of high-rate many-hypercube codes equipped with a high-performance level-by-level minimum-distance decoder (LMDD). We propose a general, high-performance decoder for high-rate concatenated stabilizer codes that extends LMDD by leveraging the Chase algorithm to generate a suitable set of candidate errors. Our simulation results demonstrate that the proposed decoder outperforms conventional decoders for high-rate concatenated Hamming codes under bit-flip noise.

</details>


### [261] [Practical block encodings of matrix polynomials that can also be trivially controlled](https://arxiv.org/abs/2601.18767)
*Martina Nibbi,Filippo Della Chiara,Yizhi Shen,Aaron Szasz,Roel Van Beeumen*

Main category: quant-ph

TL;DR: This paper introduces the FOQCS-LCU framework to efficiently implement matrix polynomial block encodings on quantum computers, reducing circuit depth overhead from O(d) to linear in polynomial degree d while maintaining practical implementability for near-term hardware.


<details>
  <summary>Details</summary>
Motivation: Block encoding theoretically enables non-unitary operations but suffers from prohibitive circuit depth scaling (O(d)) for degree-d matrix polynomials in practical quantum hardware implementations, limiting its utility for algorithms requiring polynomial transformations.

Method: Leveraging the Fast One-Qubit Controlled Select LCU (FOQCS-LCU) framework to construct explicit block encoding circuits for matrix polynomials, eliminating dependence on system size and original matrix encoding cost in the overhead.

Result: Achieves circuit depth scaling linearly with polynomial degree d (vs. previous O(d) scaling), negligible control overhead for applications like Hadamard tests, and provides explicit spin model circuits with non-asymptotic gate counts.

Conclusion: FOQCS-LCU enables practical implementation of matrix polynomial transformations on current quantum hardware by drastically reducing depth overhead, facilitating efficient quantum algorithms requiring non-unitary operations.

Abstract: Quantum circuits naturally implement unitary operations on input quantum states. However, non-unitary operations can also be implemented through block encodings, where additional ancilla qubits are introduced and later measured. While block encoding has a number of well-established theoretical applications, its practical implementation has been prohibitively expensive for current quantum hardware. In this paper, we present practical and explicit block encoding circuits implementing matrix polynomial transformations of a target matrix. With standard approaches, block-encoding a degree-$d$ matrix polynomial requires a circuit depth scaling as $d$ times the depth for block-encoding the original matrix alone. By leveraging the recently introduced Fast One-Qubit Controlled Select LCU (FOQCS-LCU) framework, we show that the additional circuit-depth overhead required for encoding matrix polynomials can be reduced to scale linearly in $d$ with no dependence on system size or the cost of block encoding the original matrix. Moreover, we demonstrate that the FOQCS-LCU circuits and their associated matrix polynomial transformations can be controlled with negligible overhead, enabling efficient applications such as Hadamard tests. Finally, we provide explicit circuits for representative spin models, together with detailed non-asymptotic gate counts and circuit depths.

</details>


### [262] [Hamiltonian Decoded Quantum Interferometry for General Pauli Hamiltonians](https://arxiv.org/abs/2601.18773)
*Kaifeng Bu,Weichen Gu,Xiang Li*

Main category: quant-ph

TL;DR: 该研究将哈密顿解码量子干涉技术(HDQI)推广至一般哈密顿量，利用解码预言机高效制备多项式函数态以近似吉布斯态，并对解码误差具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统HDQI局限于类稳定子哈密顿量，无法满足更广泛物理和计算相关量子系统的需求，亟需扩展至一般哈密顿量的吉布斯态制备方法。

Method: 针对n量子比特系统中H=Σc_iP_i形式的一般哈密顿量，开发基于解码预言机的量子算法，制备ρ_P(H) = P²(H)/Tr[P²(H)]态，其中P(x)为单变量多项式。

Result: 证明在合适解码预言机下，存在高效算法制备此类多项式态，可通过选择适当多项式逼近吉布斯态；算法对解码过程中的不完美性具有鲁棒性，显著扩展了HDQI的适用范围。

Conclusion: 该工作为广泛的物理和计算相关量子系统提供了通用的吉布斯态制备与哈密顿优化方法，推动了量子干涉技术的实用化发展。

Abstract: In this work, we study the Hamiltonian Decoded Quantum Interferometry (HDQI) for the general Hamiltonians $H=\sum_ic_iP_i$ on an $n$-qubit system, where the coefficients $c_i\in \mathbb{R}$ and $P_i$ are Pauli operators. We show that, given access to an appropriate decoding oracle, there exist efficient quantum algorithms for preparing the state $ρ_{\mathcal P}(H) = \frac{\mathcal P^2(H)}{\text{Tr}[\mathcal P^2(H)]}$, where $\mathcal P(H)$ denotes the matrix function induced by a univariate polynomial $\mathcal P(x)$. Such states can be used to approximate the Gibbs states of $H$ for suitable choices of polynomials. We further demonstrate that the proposed algorithms are robust to imperfections in the decoding procedure. Our results substantially extend the scope of HDQI beyond stabilizer-like Hamiltonians, providing a method for Gibbs-state preparation and Hamiltonian optimization in a broad class of physically and computationally relevant quantum systems.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [263] [Entropic Efficiency of Bayesian Inference Protocols](https://arxiv.org/abs/2601.17282)
*Nathan Shettell,Alexia Auffèves*

Main category: cond-mat.stat-mech

TL;DR: The paper defines inferential efficiency as information gain per memory erasure cost to benchmark sequential versus parallel measurement strategies, finding they achieve equal minimal thermodynamic cost when correlations are fully exploited, but parallel processing excels when correlations remain hidden.


<details>
  <summary>Details</summary>
Motivation: Inference underlies scientific discovery, machine learning, and decision-making, yet lacks a complete physical understanding of its thermodynamic costs and optimal memory architectures for efficient information processing.

Method: Defines an inferential efficiency metric (information gain divided by cumulative memory erasure cost) and benchmarks two limiting measurement paradigms: sequential (iterative memory use) and parallel (simultaneous memory exploitation) to analyze how correlations affect minimal erasure costs.

Result: When all system-memory correlations are exploited, both paradigms achieve identical minimal erasure costs even with noise; when correlations are unexploited, parallel paradigm outperforms sequential due to hidden memory degrees of freedom; minimal cost reflects temporal correlations in sequential and spatial correlations in parallel architectures.

Conclusion: Provides a quantitative, physically grounded framework for comparing inference strategies, determining their efficiency, and linking target information gains to minimal entropic costs, revealing when parallel processing offers thermodynamic advantages.

Abstract: Inference is a versatile tool that underlies scientific discovery, machine learning, and everyday decision-making: it describes how an agent updates a probability distribution as partial information is acquired from multiple measurements, reducing ignorance about a system's latent state. We define an inferential efficiency as the ratio of information gain to cumulative memory erasure cost, with inefficiency arising from unexploited correlations between the measured system and memories, and/or between memories and environment (noise). Using this efficiency, we benchmark two limiting measurement paradigms: sequential, in which the same memory is exploited iteratively, and parallel, in which many memories are exploited simultaneously. In both cases, the minimal erasure cost reflects correlations across memories: temporal in sequential, spatial in parallel. Remarkably, when all system-memory correlations are exploited for inference, both paradigms attain the same minimal erasure cost, even in the presence of noise. Conversely, the parallel paradigm performs better in the presence of unexploited correlations, stemming from hidden memories' degrees of freedom. This approach provides a quantitative, physically grounded criterion to compare inference strategies, determine their efficiency, and link target information gains to their minimal entropic cost.

</details>


### [264] [A Local Structural Basis to Resolve Amorphous Ices](https://arxiv.org/abs/2601.17488)
*Quinn M. Gallagher,Ryan J. Szukalo,Nicolas Giovambattista,Pablo G. Debenedetti,Michael A. Webb*

Main category: cond-mat.stat-mech

TL;DR: This paper develops a probabilistic data-driven framework to identify local structural variables distinguishing low-density and high-density amorphous ices, revealing that phase identity is encoded in the first coordination shell and transitions occur via simple redistribution without intermediates, exhibiting first-order-like behavior.


<details>
  <summary>Details</summary>
Motivation: Amorphous phases lack ordered structures like unit cells, making their differentiation challenging. While ordered phases are distinguished by symmetry, the microscopic basis for distinguishing amorphous phases remains unclear, necessitating new methods to identify their distinguishing local structural features.

Method: A new probabilistic data-driven framework is applied to molecular simulation data of water to identify local collective variables that discriminate LDA and HDA phases and characterize their pressure-induced transitions, analyzing robustness across different force fields.

Result: Local density descriptors distinguish LDA/HDA; phase identity is encoded within the first coordination shell; LDA transitions to HDA via simple redistribution of environments without intermediates, showing first-order-like behavior unlike gradual transitions in metallic glasses; findings are robust across force fields.

Conclusion: Amorphous phases can be characterized by local structural features in the first coordination shell, providing a general framework for systems lacking obvious distinguishing features, and reveals fundamental differences in phase transition mechanisms between amorphous materials.

Abstract: Phases with distinct thermodynamic properties must differ in their underlying distributions of microscopic structures. While ordered phases are readily distinguished by unit cells and space groups, the local structural basis differentiating amorphous phases is less apparent. Here, using a new probabilistic data-driven framework applied to molecular simulation data on water, we identify local collective variables that discriminate low-density and high-density amorphous (LDA and HDA) ices and characterize pressure-induced transitions between these phases. As expected, descriptors related to local density capably distinguish LDA and HDA; however, phase identity is surprisingly encoded within the first coordination shell. Furthermore, LDA transitions to HDA by a simple redistribution of LDA- and HDA-like environments with no evident intermediate structures, in accordance with a first-order-like transition that contrasts with the gradual evolution observed in other amorphous systems such as metallic glasses. These findings are robust across force fields, which themselves exhibit structural differences, and exemplify how other systems lacking obvious distinguishing features can be characterized.

</details>


### [265] [Wigner distribution, Wigner entropy and Quantum Refrigerator of a One-Dimensional Off-diagonal Quasicrystal](https://arxiv.org/abs/2601.17691)
*Shan Suo,Ao Zhou,Yanting Chen,Shujie Cheng,Gao Xianlong*

Main category: cond-mat.stat-mech

TL;DR: 研究具有双重准周期调制非对角准晶体的局域化现象，通过分形维数和维格纳分析绘制相图，揭示量子热机/制冷机模式。


<details>
  <summary>Details</summary>
Motivation: 探究多重调制准晶体的局域化相变机制，拓展准周期系统在量子热力学中的应用潜力。

Method: 通过分形维数分析绘制退局域化-局域化相图，利用维格纳分布区分退局域化/局域化态，采用维格纳熵分离扩展相、临界相和局域化相。

Result: 获得退局域化-局域化相图；维格纳分布可区分退局域化与局域化态；维格纳熵可分离扩展、临界和局域化相；局域化态促进量子加热模式出现，同时产生制冷模式。

Conclusion: 深化了对局域化现象的理解，拓展了准周期系统在量子热力学中的应用。

Abstract: We investigate an off-diagonal quasicrystal featuring simultaneous off-diagonal and diagonal quasiperiodic modulations. By analyzing the fractal dimension, we map out the delocalization-localization phase diagram. We demonstrate that delocalized and localized states can be distinguished via the Wigner distribution, while extended, critical, and localized phases are separated using the Wigner entropy. Furthermore, we explore the quantum thermodynamic properties, revealing that localized states facilitate the emergence of a quantum heater mode, alongside the appearance of a refrigerator mode. These findings enhance our understanding of localization phenomena and expand the thermodynamic applications of quasiperiodic systems.

</details>


### [266] [Non-equilibrium symmetry of cyclic first-passage times](https://arxiv.org/abs/2601.18136)
*Daniel Maria Busiello,Shiling Liang,Simone Pigolotti*

Main category: cond-mat.stat-mech

TL;DR: 研究发现小物理系统中任意循环的首次通过时间之和在热力学平衡时与方向无关，非平衡时则满足涨落定理，揭示了一种结合时间反演和轨迹交换的新对称性，并将循环熵产生与系统整体熵产生关联。


<details>
  <summary>Details</summary>
Motivation: 探索随机系统特别是非平衡物理中循环首次通过时间的对称性规律，理解熵产生在局部循环与整体系统之间的关系。

Method: 采用首次通过时间分析、大偏差理论，以及结合时间反演和轨迹片段交换的对称性论证方法。

Result: 1) 平衡态下，顺时针和逆时针探索循环的首次通过时间总和服从相同概率分布；2) 非平衡态下，两个方向的分布满足详细的涨落定理；3) 发现了一种新的轨迹对称性；4) 建立了循环熵产生与系统整体熵产生的理论联系。

Conclusion: 该研究揭示的随机系统新对称性在非平衡物理中具有广泛的适用潜力。

Abstract: We study the sum of first passage times along an arbitrary cycle made up of N>2 states of a small physical system. We show that, if the system is at thermodynamic equilibrium, this sum follows the same probability distribution regardless of whether the cycle is explored clockwise or counterclockwise. Out of equilibrium, the distributions of clockwise and counterclockwise cyclic first passage times are related by a detailed fluctuation theorem. This result descends from a symmetry of clockwise and counterclockwise trajectories, which combines time reversal with swapping portions of the trajectories. We then relate the entropy produced along the cycle with the entropy production of the whole system using large deviation theory. Our results reveal a novel symmetry in stochastic systems, of potential broad applicability in non-equilibrium physics.

</details>


### [267] [Separating Energy and Entropy Contributions to the Hexatic-Liquid Transitions in Two-Dimensional Repulsive Systems](https://arxiv.org/abs/2601.18215)
*Yan-Wei Li,Rui Ding,Wen-Hao Ma*

Main category: cond-mat.stat-mech

TL;DR: 揭示了二维熔化路径由能量（凸性）与熵（凹性）的竞争决定：熵主导时为一阶相变，否则为连续相变，建立了热力学量曲率作为理解二维熔化的基本原理。


<details>
  <summary>Details</summary>
Motivation: 二维熔化研究中，系统在势能和细节影响下可发生一阶或连续六边形-液体相变，但其根本热力学起源尚不明确。

Method: 通过分解三种典型排斥系统的亥姆霍兹自由能，分析能量与熵的贡献曲率，并进一步分离振动熵和构型熵的作用。

Result: 能量贡献始终使自由能呈凸性，熵贡献呈凹性；熵凹性主导时发生一阶相变，否则为连续相变。构型熵曲率从凸（一阶）到凹（连续）的转变与缺陷增殖的香农熵一致，且零温下一阶相变因熵效应消失而转为连续。

Conclusion: 不同热力学量的曲率是决定二维熔化本质的根本原理，阐明了能量-熵竞争对相变路径的普适调控机制。

Abstract: Over the past decades, research on two-dimensional melting has established that both first-order and continuous hexatic-liquid transitions can occur, influenced by various factors in the potential energy and system details. The fundamental thermodynamic origins of this sensitivity remains elusive. Here, by decomposing the Helmholtz free energy across three representative repulsive systems, we reveal a universal competition between energy and entropy that dictates the melting pathway. The energetic contribution consistently imparts convexity to the free energy, whereas entropy imparts concavity. A first-order transition occurs when concave entropy dominates; otherwise, the transition is continuous. Further decomposition shows that vibrational entropy drives the concave total entropic curvature, while the configurational entropy's curvature switches from convex (first-order) to concave (continuous), mirroring defect proliferation measured by Shannon entropy. The convexity of the energy is dominated by the inherent potential, with minimal vibrational influence. Finally, we predict and verify that the first-order transition becomes continuous at zero temperature, where entropic effects vanish. Our work establishes the curvature of different thermodynamic quantities as a fundamental principle for understanding the nature of two-dimensional melting.

</details>


### [268] [Measurement induced faster symmetry restoration in quantum trajectories](https://arxiv.org/abs/2601.18458)
*Katha Ganguly,Bijay Kumar Agarwalla*

Main category: cond-mat.stat-mech

TL;DR: 本文利用连续测量的反作用作为资源，通过结合U(1)保持的幺正演化，实现全局U(1)对称性恢复。研究发现，在全局监测下，包含远距离电荷区的叠加态比近距离区的态恢复更快，且该现象在不同测量协议下具有普适性。局部监测可进一步加速某些在全局监测下恢复缓慢的态的对称性恢复。


<details>
  <summary>Details</summary>
Motivation: 连续测量会导致量子轨迹和测量反作用。本工作的动机是将这种测量反作用作为一种资源，用于恢复全局U(1)对称性。

Method: 将连续测量与U(1)保持的幺正演化相结合，从U(1)对称性破缺的初始态出发，模拟由全局和局部观测量连续测量产生的量子轨迹。

Result: 在全局监测下，包含远距离电荷区叠加的态比近距离区的态更快恢复对称性；该行为在不同测量协议下具有普适性；局部监测能进一步加速某些在全局监测下恢复缓慢的态的对称性恢复。

Conclusion: 测量反作用可以有效地用于恢复U(1)对称性，且通过选择合适的测量方式（全局或局部）可以优化恢复速度。

Abstract: Continuous measurement of quantum systems provides a standard route to quantum trajectories through the successive acquisition of information which further results in measurement back-action. In this work, we harness this back-action as a resource for global $U(1)$ symmetry restoration where continuous measurement is combined with a $U(1)$-preserving unitary evolution. Starting from a $U(1)$ symmetry-broken initial state, we simulate quantum trajectories generated by continuous measurements of both global and local observables. We show that under global monitoring, states containing superpositions of distant charge sectors restore symmetry faster than those involving nearby sectors. We establish the universality of this behavior across different measurement protocols. Finally, we demonstrate that local monitoring can further accelerate symmetry restoration for certain states that relax slowly under global monitoring.

</details>


<div id='nlin.AO'></div>

# nlin.AO [[Back]](#toc)

### [269] [From microscopic social force models to macroscopic continuum models for pedestrian flow](https://arxiv.org/abs/2601.17304)
*Liangze Yang,Hui Yu,Jie Du*

Main category: nlin.AO

TL;DR: This paper bridges microscopic and macroscopic pedestrian flow models by deriving mesoscopic kinetic equations from a social force model, then developing continuum macroscopic models through varying interaction forces, with numerical validation showing consistent behavioral predictions across scales.


<details>
  <summary>Details</summary>
Motivation: Existing pedestrian flow models operate at isolated scales (microscopic for individual interactions, macroscopic for global dynamics), with no established framework connecting these levels; this gap limits comprehensive analysis of crowd behavior.

Method: Derived mesoscopic kinetic equations from a reactive optimal-route microscopic social force model, then systematically varied interaction force parameters to generate multiple macroscopic continuum models, followed by comparative numerical simulations.

Result: Successfully linked three modeling scales (micro-meso-macro) and demonstrated that the derived continuum models accurately reproduced key behavioral patterns of the original social force model under diverse scenarios.

Conclusion: The proposed multiscale framework establishes a rigorous mathematical connection between individual-level interactions and population-level dynamics, enabling more robust and versatile pedestrian flow analysis for complex crowd systems.

Abstract: The pedestrian flow is one of the most complex systems, involving large populations of interacting agents. Models at microscopic and macroscopic scales offer different advantages for studying related problems. In general, microscopic models can describe interaction forces at the individual level. Macroscopic models, on the other hand, provide analytical insights into global interactions and long-term overall dynamics, along with efficient numerical simulations and predictions. However, the relationship between models at different scales has rarely been explored. In this study, based on the original microscopic social force model with a reactive optimal route choice strategy, we first derive kinetic equations at the mesoscopic level. By varying the interaction force in different scenarios, we then derive several continuum models at the macroscopic level. Finally, numerical examples are given to evaluate the behaviors of the social force model and our continuum models.

</details>


### [270] [Neural-Inspired Multi-Agent Molecular Communication Networks for Collective Intelligence](https://arxiv.org/abs/2601.18018)
*Boran A. Kilic,Ozgur B. Akan*

Main category: nlin.AO

TL;DR: This paper proposes a collective intelligence approach for Molecular Communication in IoBNT, using simple nanomachines with Greenberg-Hastings cellular automata. It shows that the system achieves maximum information processing at a critical phase transition point (edge of chaos), validated through mean-field analysis and simulations.


<details>
  <summary>Details</summary>
Motivation: Current MC research relies on overly complex, energy-intensive individual nanomachines that are impractical for real-world IoBNT applications due to severe energy and processing constraints. There's a need for simpler, more scalable solutions.

Method: The authors propose a decentralized architecture where simple nanomachines interact through a diffusive medium using a threshold-based firing mechanism modeled by Greenberg-Hastings cellular automata. They employ mean-field analysis to derive fixed-point equations for steady-state populations and validate these theoretical predictions against stochastic simulations.

Result: The network exhibits a second-order phase transition at a specific activation threshold. Both pairwise and collective mutual information peak precisely at this critical point, demonstrating that the system maximizes information propagation and processing capacity at the edge of chaos.

Conclusion: Collective intelligence through simple nanomachines can enable efficient molecular communication for IoBNT, with optimal information processing occurring at the critical phase transition point, offering a biologically-inspired paradigm shift.

Abstract: Molecular Communication (MC) is a pivotal enabler for the Internet of Bio-Nano Things (IoBNT). However, current research often relies on super-capable individual agents with complex transceiver architectures that defy the energy and processing constraints of realistic nanomachines. This paper proposes a paradigm shift towards collective intelligence, inspired by the cortical networks of the biological brain. We introduce a decentralized network architecture where simple nanomachines interact via a diffusive medium using a threshold-based firing mechanism modeled by Greenberg-Hastings (GH) cellular automata. We derive fixed-point equations for steady-state populations via mean-field analysis and validate them against stochastic simulations. We demonstrate that the network undergoes a second-order phase transition at a specific activation threshold. Crucially, we show that both pairwise and collective mutual information peak exactly at this critical transition point, confirming that the system maximizes information propagation and processing capacity at the edge of chaos.

</details>


### [271] [Order Out of Noise and Disorder: Fate of the Frustrated Manifold](https://arxiv.org/abs/2601.18653)
*Igor Halperin*

Main category: nlin.AO

TL;DR: 研究受挫布朗粒子在弯曲曲面上的动力学，发现无序可诱导维度降低和对称性破缺，从而形成有序结构。


<details>
  <summary>Details</summary>
Motivation: 探索如何在弯曲空间中通过几何和拓扑结构将随机性转化为有序性，为无序系统产生涌现空间秩序提供几何框架，并与传统需要外部驱动或精细调节的非线性自组织系统形成对比。

Method: 采用朗之万动力学方法研究N个布朗粒子在紧凑二维黎曼流形上的行为，粒子通过测地距离线性势相互作用，并引入淬火随机耦合。在球面S²、环面T²和有界圆柱S¹×[0,H]三种几何结构上进行模拟。

Result: 发现玻璃化弛豫驱动粒子从二维分布转变为准一维结构：球面上形成带状结构，环面上形成环状结构，圆柱上形成局域化团簇。对称性破缺模式取决于拓扑：SO(3)→SO(2)（球面）、SO(2)×SO(2)→SO(2)×Z₂（环面）、SO(2)→Z₂（圆柱）。对称性破缺方向通过A型扩散型Nambu-Goldstone动力学缓慢演化。

Conclusion: 几何和拓扑结构本身就能将随机性引导成有序，无需外部驱动或精细调节的非线性机制。该研究建立了无序在弯曲空间产生涌现空间秩序的几何框架，并与自旋玻璃理论、量子场论、天体物理结构形成等领域建立了联系。

Abstract: We study Langevin dynamics of $N$ Brownian particles on compact two-dimensional Riemannian manifolds, interacting through pairwise potentials linear in geodesic distance with quenched random couplings. These \emph{frustrated Brownian particles} experience the competing demands of random attractive and repulsive interactions while confined to curved surfaces. We consider three geometries: the sphere $S^2$, torus $T^2$ (closed manifolds), and bounded cylinder $S^1 \times [0,H]$ (manifold with boundary). Our central finding is disorder-induced dimension reduction accompanied by spontaneous breaking of rotational symmetry: order emerges from the combination of two sources of randomness (thermal noise and quenched disorder), with the manifold topology determining the character of the emerging structures. Glassy relaxation drives particles from 2D distributions to quasi-1D structures, specifically bands on $S^2$, rings on $T^2$, and localized clusters on the cylinder. The symmetry breaking pattern depends on topology: SO(3)$\to$SO(2) on the sphere, SO(2)$\times$SO(2)$\to$SO(2)$\times\mathbb{Z}_2$ on the torus, and SO(2)$\to\mathbb{Z}_2$ on the cylinder. Unlike conventional spontaneous symmetry breaking, the symmetry-breaking direction is not frozen but evolves slowly via thermal noise through type-A diffusive Nambu-Goldstone dynamics, while the reduced-dimensional structure persists. Unlike conventional self-organizing systems that require external driving or fine-tuned nonlinearities, our model demonstrates that geometry and topology alone can channel randomness into order. We discuss connections to spin glass theory, quantum field theory, astrophysical structure formation, and self-organizing systems, providing a geometric framework for understanding how disorder generates emergent spatial order on curved spaces.

</details>


<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [272] [McSAS3: improved Monte Carlo small-angle scattering analysis software for dilute and dense scatterers](https://arxiv.org/abs/2601.18659)
*Brian Richard Pauw,Ingo Breßler*

Main category: physics.data-an

TL;DR: McSAS3是新一代小角散射分析软件，采用蒙特卡洛方法实现无模型参数分布拟合，支持自动化流程和单/批量数据处理，配备用户友好的图形界面


<details>
  <summary>Details</summary>
Motivation: 原始McSAS软件需要升级以适应自动化数据处理流程的需求，同时保持对单/批量散射数据的处理能力

Method: 采用蒙特卡洛优化算法进行小角散射数据拟合，通过图形界面(McSAS3GUI)生成和测试配置文件，支持机器专用模板扩展

Result: 能够高精度拟合各类实际散射图案，输出绝对体积刻度数据下的无模型参数分布（通常为体积加权尺寸分布）

Conclusion: McSAS3成功实现了自动化集成与交互式使用的双重目标，其蒙特卡洛方法提供了可靠的参数分布分析能力，GUI显著降低了使用门槛

Abstract: McSAS3 is the refactored successor to the original McSAS Monte Carlo small-angle scattering analysis software. It is intended to be integrated in automated data processing pipelines, but can also be used to process individual (batches of) scattering data.
  McSAS3 comes with a graphical user interface (McSAS3GUI), complete with guides, examples and videos. McSAS3GUI will help to generate and test the three configuration files that McSAS3 needs for data read-in, Monte Carlo optimization and histogramming. The user interface can also be used to process individual files or batches, and can be augmented with machine-specific use templates.
  The Monte Carlo (MC) approach is able to fit most practical scattering patterns extremely well, resulting in form-free model parameter distributions. Theoretically, these can be distributions on any model parameter, but in practice the MC-optimized parameter is usually a (volume-weighted) size distribution, in absolute volume fraction for absolute-scaled data.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [273] [Finite-scale geometric invariants for chaotic and weakly chaotic dynamics](https://arxiv.org/abs/2601.17370)
*Vinesh Vijayan*

Main category: nlin.CD

TL;DR: 提出有限尺度几何观测量，量化耗散系统中局部集合的时间演化增长率；在一致双曲系统中收敛于与柯尔莫哥洛夫-西奈熵相关的平台，在非一致双曲系统中衰减至零，为混沌动力学提供有限尺度表征


<details>
  <summary>Details</summary>
Motivation: 传统混沌量化依赖渐近不变量（如熵）和符号动力学/马尔可夫划分，难以描述有限尺度、瞬态混沌及开放间歇系统；需建立无需先验划分的几何化有限尺度观测方法

Method: 定义有限时间和分辨率下的几何观测量，通过局部集合的演化增长率捕捉动力学行为；避免使用符号动力学或马尔可夫划分

Result: 1. 一致双曲系统：观测量收敛至分辨率依赖的平台，其对数标度系数等于柯尔莫哥洛夫-西奈熵；2. 仅双曲系统：观测量衰减至零，反映无熵产生；3. 开放间歇系统：仍保持有限定义，揭示瞬态弱混沌的有限尺度特征；4. 数值验证：应用于Hénon映射和Feigenbaum点

Conclusion: 该观测量实现了混沌动力学的有限尺度几何表征，与经典熵理论一致（在适用场景），并扩展至经典方法失效的开放系统，为瞬态混沌分析提供新工具

Abstract: We introduce a finite scale geometric observable that quantifies the growth rate of localized sets under time evolution in dissipative dynamical systems. Defined at finite time and resolution without reference to symbolic dynamics or Markov partitions this observable converges, in uniformly hyperbolic systems, to a resolution dependent plateau whose logarithmic scaling coefficient equals the Kolmogorov Sinai entropy. In merely hyperbolic systems, it decays to zero, reflecting the absence of entropy production, while remaining well defined at finite scales. Numerical results for the Henon map and Feigenbaum point illustrate these behaviors. Our findings yield a finite scale geometric characterization of chaotic dynamics, consistent with classical entropy theory where applicable. We further demonstrate that the observable remains well defined in open intermittent systems, where trajectories escape and classical asymptotic invariants fail, revealing finite scale signatures of transient weak chaos.

</details>
